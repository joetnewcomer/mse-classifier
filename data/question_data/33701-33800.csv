,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Choosing a 5 member team out of 12 girls and 10 boys,Choosing a 5 member team out of 12 girls and 10 boys,,"We must choose a 5-member team from 12 girls and 10 boys. How many ways are there to make the choice so that there are no more than 3 boys on the team? The correct answer is $\binom{22}{5} - \binom{12}{1} \binom{10}{4} - \binom{10}{5}$. I understand the $\binom{22}{5}$ part, but where I am confused at is the other two parts. I do not know how to get those parts. Can anyone help me understand how to get to the solution?","We must choose a 5-member team from 12 girls and 10 boys. How many ways are there to make the choice so that there are no more than 3 boys on the team? The correct answer is $\binom{22}{5} - \binom{12}{1} \binom{10}{4} - \binom{10}{5}$. I understand the $\binom{22}{5}$ part, but where I am confused at is the other two parts. I do not know how to get those parts. Can anyone help me understand how to get to the solution?",,"['probability', 'combinatorics', 'binomial-coefficients']"
1,Lower bound on $P(X>\lambda)$ where $X$ is Gaussian.,Lower bound on  where  is Gaussian.,P(X>\lambda) X,"Suppose X is a 0 mean Gaussian random variable with variance 1.   I'm trying to find a lower bound on $P(X>\lambda)$.   Specifically I'd like to derive a lower bound of the form $c e^{-C\lambda^2}$ for positive constants $c,C$.  I know there exists several upper bounds of this form.   But I am looking for a lower bound.","Suppose X is a 0 mean Gaussian random variable with variance 1.   I'm trying to find a lower bound on $P(X>\lambda)$.   Specifically I'd like to derive a lower bound of the form $c e^{-C\lambda^2}$ for positive constants $c,C$.  I know there exists several upper bounds of this form.   But I am looking for a lower bound.",,['probability']
2,What's the probability that I will earn \$25?,What's the probability that I will earn \$25?,,"I go to a casino with \$100. At the casino, I play a game in which I get \$1 if I win, and lose \$1 if I lose. The probability of me winning is $\frac{1}{4}$, and I must either win or lose every time I play this game. I will keep playing this game until I either earn \$25 or lose all my money. What's the probability that I will earn \$25? I originally thought that the answer was $\frac{1}{4^{25}}$ because I must win a net 25 times. However, I realized that the problem was far more complicated because I could win and lose many times. Furthermore, if I go broke, I must stop playing. What tools in probability can I latch off of to solve this problem?","I go to a casino with \$100. At the casino, I play a game in which I get \$1 if I win, and lose \$1 if I lose. The probability of me winning is $\frac{1}{4}$, and I must either win or lose every time I play this game. I will keep playing this game until I either earn \$25 or lose all my money. What's the probability that I will earn \$25? I originally thought that the answer was $\frac{1}{4^{25}}$ because I must win a net 25 times. However, I realized that the problem was far more complicated because I could win and lose many times. Furthermore, if I go broke, I must stop playing. What tools in probability can I latch off of to solve this problem?",,['probability']
3,Probability distribution for the perimeter and area of triangle with fixed circumscribed radius,Probability distribution for the perimeter and area of triangle with fixed circumscribed radius,,"Given a circle with radius R = 1, I'm trying to find either the probability distribution function or the density function for the space of triangle, which is randomly selected on this circle. The same task is for perimeter function of this triangle. The only thing I've understood is the following. If we fix some point R on the circle, then angles ROA, ROB, ROC (counterclockwise) are uniformly distributed on [0; 2 * Pi]. I've tried expressing the space and the perimeter through those angles, but still had no success. I would appreciate any help, really. I've tried to solve this problem for three weeks, and it seems to me that soon those triangles and circles will begin to come into my night dreams. Thanks.","Given a circle with radius R = 1, I'm trying to find either the probability distribution function or the density function for the space of triangle, which is randomly selected on this circle. The same task is for perimeter function of this triangle. The only thing I've understood is the following. If we fix some point R on the circle, then angles ROA, ROB, ROC (counterclockwise) are uniformly distributed on [0; 2 * Pi]. I've tried expressing the space and the perimeter through those angles, but still had no success. I would appreciate any help, really. I've tried to solve this problem for three weeks, and it seems to me that soon those triangles and circles will begin to come into my night dreams. Thanks.",,"['probability', 'probability-theory']"
4,Probability of picking elements randomly without frequent repeats,Probability of picking elements randomly without frequent repeats,,"If I have a set S that has n unique elements, and I pick from it such that each element has an equal probability of being chosen, how many elements should S be so that the probability of seeing any elements repeated within a small subset of drawn elements (let's say, 5) is low (like less than .5)? To put it another way, if I am using a random number generator to pick 5 or so integers from 1 to n, how large should n be to ensure the probability of seeing the same number come up twice is less than .5? Sorry if my phrasing is confusing, I'm not a mathematician. I'm interested in this because it will simplify some code I'm working on by a lot.","If I have a set S that has n unique elements, and I pick from it such that each element has an equal probability of being chosen, how many elements should S be so that the probability of seeing any elements repeated within a small subset of drawn elements (let's say, 5) is low (like less than .5)? To put it another way, if I am using a random number generator to pick 5 or so integers from 1 to n, how large should n be to ensure the probability of seeing the same number come up twice is less than .5? Sorry if my phrasing is confusing, I'm not a mathematician. I'm interested in this because it will simplify some code I'm working on by a lot.",,"['probability', 'combinatorics']"
5,Probability and the Symmetric Group,Probability and the Symmetric Group,,"Let $p_{n,k}$ be the probability that a random permutation from $S_n$, the symmetric group of order $n!$, has exactly $k$ fixed points.  I am trying to compute $\lim _{n\to \infty}p_{n,k}$.  After playing with it a bit, I am relatively confident the limit is $\frac{1}{k!}$, although I have yet to come up with a proof. Here's what I've tried so far.  If $a_{n,k}$ is the number of elements of $S_n$ with $k$ fixed points, then we have that $$ a_{n,k}=\binom{n}{k}a_{n-k,0} $$ That is, there are $\binom{n}{k}$ ways to choose $k$ fixed points, and after those are chosen, there are $a_{n-k,0}$ to permute the remaining $n-k$ elements with $0$ fixed points.  After computing $a_{n,0}$ by hand for small values of $n$, I was able to look up the recursion formula $a_n=na_{n-1}+(-1)^n$ from oeis.org.  Using this, I was able to come up with my ""guess"" of $\frac{1}{k!}$, but this direction doesn't seem to be leading me towards finding a proof. Any ideas how to proceed?  Group theory is not my strong point, so I would not be surprised if there is a result that makes this problem almost trivial.  A pointer to any such results would be excellent.","Let $p_{n,k}$ be the probability that a random permutation from $S_n$, the symmetric group of order $n!$, has exactly $k$ fixed points.  I am trying to compute $\lim _{n\to \infty}p_{n,k}$.  After playing with it a bit, I am relatively confident the limit is $\frac{1}{k!}$, although I have yet to come up with a proof. Here's what I've tried so far.  If $a_{n,k}$ is the number of elements of $S_n$ with $k$ fixed points, then we have that $$ a_{n,k}=\binom{n}{k}a_{n-k,0} $$ That is, there are $\binom{n}{k}$ ways to choose $k$ fixed points, and after those are chosen, there are $a_{n-k,0}$ to permute the remaining $n-k$ elements with $0$ fixed points.  After computing $a_{n,0}$ by hand for small values of $n$, I was able to look up the recursion formula $a_n=na_{n-1}+(-1)^n$ from oeis.org.  Using this, I was able to come up with my ""guess"" of $\frac{1}{k!}$, but this direction doesn't seem to be leading me towards finding a proof. Any ideas how to proceed?  Group theory is not my strong point, so I would not be surprised if there is a result that makes this problem almost trivial.  A pointer to any such results would be excellent.",,"['probability', 'group-theory']"
6,a sequential game of dice,a sequential game of dice,,"consider the following game: 10 dice are tossed and those showing 3 are more are retained. [those showing 2 or less are discarded.] the remaining dice are tossed again and those showing 4 or more are retained. finally, the remaining dice are tossed once more and those showing 5 or more are retained.  if $Z$ denotes the number of dice retained after the third round of play, what is the probability that $Z = 0$?","consider the following game: 10 dice are tossed and those showing 3 are more are retained. [those showing 2 or less are discarded.] the remaining dice are tossed again and those showing 4 or more are retained. finally, the remaining dice are tossed once more and those showing 5 or more are retained.  if $Z$ denotes the number of dice retained after the third round of play, what is the probability that $Z = 0$?",,"['probability', 'recreational-mathematics', 'dice']"
7,Given the maximum likelihood function- estimate the value of the parameter,Given the maximum likelihood function- estimate the value of the parameter,,"Lets  say I have the pdf and maximum likelihood function: $ f_X(x) = \begin{cases}  \frac{\alpha \beta^\alpha}{x^{\alpha+1}}, & x > \beta, \\ 0, & x \leq \beta. \end{cases} $ $ \begin{aligned} & l(\alpha, \beta) = \log L(\alpha, \beta) = n \log \alpha + n\alpha \log \beta - (\alpha + 1)\sum_{i=1}^{n} \log x_i \end{aligned} $ In the solution they answered: \begin{aligned} & \text{To determine the maximum likelihood estimation of } \beta, \text{ we note that the only term containing } \beta \text{ is } n\alpha \log \beta. \text{ It is monotonic and increasing in } \beta, \text{ meaning we maximize } l, \text{ and thus } L, \text{ by choosing } \beta \text{ as large as possible. The constraint we must consider is that } x_i \geq \beta \text{ for each } i = 1, \ldots, n. \text{ Therefore, we set } \hat{\beta} = \min_{1 \leq i \leq n} x_i \text{ to maximize } l \text{ with respect to } \beta. \end{aligned} I don't really understand the explanation at all especiall anything after the ""The constraint we must consider is that...."". I'd really appreciate it if someone could explain it in easier terms.","Lets  say I have the pdf and maximum likelihood function: In the solution they answered: I don't really understand the explanation at all especiall anything after the ""The constraint we must consider is that...."". I'd really appreciate it if someone could explain it in easier terms.","
f_X(x) = \begin{cases} 
\frac{\alpha \beta^\alpha}{x^{\alpha+1}}, & x > \beta, \\
0, & x \leq \beta.
\end{cases}
 
\begin{aligned}
& l(\alpha, \beta) = \log L(\alpha, \beta) = n \log \alpha + n\alpha \log \beta - (\alpha + 1)\sum_{i=1}^{n} \log x_i
\end{aligned}
 \begin{aligned}
& \text{To determine the maximum likelihood estimation of } \beta, \text{ we note that the only term containing } \beta \text{ is } n\alpha \log \beta. \text{ It is monotonic and increasing in } \beta, \text{ meaning we maximize } l, \text{ and thus } L, \text{ by choosing } \beta \text{ as large as possible. The constraint we must consider is that } x_i \geq \beta \text{ for each } i = 1, \ldots, n. \text{ Therefore, we set } \hat{\beta} = \min_{1 \leq i \leq n} x_i \text{ to maximize } l \text{ with respect to } \beta.
\end{aligned}","['probability', 'probability-theory', 'statistics', 'statistical-inference', 'maximum-likelihood']"
8,Want an example of uncorrelated but dependent joint Bernoulli example [closed],Want an example of uncorrelated but dependent joint Bernoulli example [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 27 days ago . Improve this question Can anyone give an example (joint probability table) that two Bernoulli variables are uncorrelated but not independent?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 27 days ago . Improve this question Can anyone give an example (joint probability table) that two Bernoulli variables are uncorrelated but not independent?",,"['probability', 'bernoulli-numbers']"
9,Expected Number of Flips and Probability in a Coin Toss Experiment,Expected Number of Flips and Probability in a Coin Toss Experiment,,"I'm currently studying probability and I've come across a problem that I'm finding quite challenging. I would appreciate any help or guidance. Problem Statement: A fair coin is continually flipped until both heads and tails have appeared. I need to find: (a) The expected number of flips (b) The probability that the last flip lands on heads My Attempt: For part (a), I understand that the expected value is the long-run average or mean of a random variable. Since the coin is fair, the probability of getting heads or tails is 0.5. However, I'm not sure how to apply this concept when the coin is flipped until both heads and tails have appeared. For part (b), I'm a bit confused. My initial thought was that since the coin is fair, the probability that the last flip lands on heads would be 0.5. But I'm not sure if this is correct because the experiment doesn't stop until both heads and tails have appeared. Background: I'm an undergraduate student majoring in Mathematics. I've taken courses in Calculus and Linear Algebra, and I'm currently taking a course in Probability and Statistics. I'm familiar with the basics of probability, but this problem seems to involve concepts that I haven't fully grasped yet. I found this problem in my textbook (unfortunately, I don't have the name of the book right now), in the chapter on expected values. I've tried to solve it using the concepts explained in the book, but I'm stuck. I would really appreciate it if someone could explain the solution in a way that a beginner in probability could understand. Thank you in advance for your help!","I'm currently studying probability and I've come across a problem that I'm finding quite challenging. I would appreciate any help or guidance. Problem Statement: A fair coin is continually flipped until both heads and tails have appeared. I need to find: (a) The expected number of flips (b) The probability that the last flip lands on heads My Attempt: For part (a), I understand that the expected value is the long-run average or mean of a random variable. Since the coin is fair, the probability of getting heads or tails is 0.5. However, I'm not sure how to apply this concept when the coin is flipped until both heads and tails have appeared. For part (b), I'm a bit confused. My initial thought was that since the coin is fair, the probability that the last flip lands on heads would be 0.5. But I'm not sure if this is correct because the experiment doesn't stop until both heads and tails have appeared. Background: I'm an undergraduate student majoring in Mathematics. I've taken courses in Calculus and Linear Algebra, and I'm currently taking a course in Probability and Statistics. I'm familiar with the basics of probability, but this problem seems to involve concepts that I haven't fully grasped yet. I found this problem in my textbook (unfortunately, I don't have the name of the book right now), in the chapter on expected values. I've tried to solve it using the concepts explained in the book, but I'm stuck. I would really appreciate it if someone could explain the solution in a way that a beginner in probability could understand. Thank you in advance for your help!",,"['probability', 'probability-theory', 'probability-distributions', 'expected-value']"
10,Q. 18 from A first course in probability by Sheldon Ross,Q. 18 from A first course in probability by Sheldon Ross,,"Each of 20 families selected to take part in a treasure hunt consist of a mother, father, son, and daughter. Assuming that they look for the treasure in pairs that are randomly chosen from the 80 participating individuals and that each pair has the same probability of finding the treasure, calculate the probability that the pair that finds the treasure includes a mother but not her daughter. Solution I thought Sample space: $80\times79$ As there are 20 mothers so we select and each have one daughter so event will have $20\times78$ samples. This gives answer $0.247$ . But answer in book is $0.3734$ .","Each of 20 families selected to take part in a treasure hunt consist of a mother, father, son, and daughter. Assuming that they look for the treasure in pairs that are randomly chosen from the 80 participating individuals and that each pair has the same probability of finding the treasure, calculate the probability that the pair that finds the treasure includes a mother but not her daughter. Solution I thought Sample space: As there are 20 mothers so we select and each have one daughter so event will have samples. This gives answer . But answer in book is .",80\times79 20\times78 0.247 0.3734,['probability']
11,On asymptotics of certain sums of multinomial coefficients,On asymptotics of certain sums of multinomial coefficients,,"Given positive integers $n$ and $k$ , set $$ S_{n,k}=\sum_{\substack{a_1+a_2+\dots+a_k=2n\\ a_i \in 2\mathbb{N},\,i=1,\ldots,k}}\frac{(2n)!}{a_1!a_2!\dots a_k!}, $$ where $2\mathbb{N}=\{0,2,4,\ldots\}$ . According to the answers of Special sum of multinomial coefficients! there is no ""nice"" closed form expression for $S_{n,k}$ . My question is: How can one find the asymptotics of $S_{n,k}$ for fixed $n$ when $k \rightarrow \infty$ ? My thoughts so far: It is mentioned in the link above that the expression for $S_{n,k}$ resembles Sterling numbers of the second kind, so perhaps some approximation results for those numbers may be relevant. Also, I did some numerical experimentation that seems to suggest that the naive guess $S_{n,k}\sim C_n \cdot k^n$ (where $C_n>0$ depends on $n$ only) is plausible.","Given positive integers and , set where . According to the answers of Special sum of multinomial coefficients! there is no ""nice"" closed form expression for . My question is: How can one find the asymptotics of for fixed when ? My thoughts so far: It is mentioned in the link above that the expression for resembles Sterling numbers of the second kind, so perhaps some approximation results for those numbers may be relevant. Also, I did some numerical experimentation that seems to suggest that the naive guess (where depends on only) is plausible.","n k  S_{n,k}=\sum_{\substack{a_1+a_2+\dots+a_k=2n\\ a_i \in 2\mathbb{N},\,i=1,\ldots,k}}\frac{(2n)!}{a_1!a_2!\dots a_k!},
 2\mathbb{N}=\{0,2,4,\ldots\} S_{n,k} S_{n,k} n k \rightarrow \infty S_{n,k} S_{n,k}\sim C_n \cdot k^n C_n>0 n","['probability', 'combinatorics', 'asymptotics', 'multinomial-coefficients', 'multinomial-distribution']"
12,Mean value of the smallest selected number if we keep selecting numbers uniformly from 0 and 1 as long as they are decreasing,Mean value of the smallest selected number if we keep selecting numbers uniformly from 0 and 1 as long as they are decreasing,,"I come across a problem that interests me a lot: Select numbers uniformly distributed between 0 and 1, one after one, as long as they keep decreasing: stop selecting when you obtain a number that is greater than the previous one you selected. Q: What is the average value of the smallest number you have selected? It is not that hard to show that the average number to select is e, but what is the average smallest value when stopping?","I come across a problem that interests me a lot: Select numbers uniformly distributed between 0 and 1, one after one, as long as they keep decreasing: stop selecting when you obtain a number that is greater than the previous one you selected. Q: What is the average value of the smallest number you have selected? It is not that hard to show that the average number to select is e, but what is the average smallest value when stopping?",,"['probability', 'combinatorics', 'statistics', 'expected-value']"
13,"Dice, conditional probability - where am I wrong?","Dice, conditional probability - where am I wrong?",,"There is a problem which I solved wrong. I know the correct solution from the book, but I cannot find the gap in my reasoning. The problem is: three fair dice are rolled at the same time. What is the probability of getting at least one ""1"" upon condition that at least one die shows a ""6""? The solution in the book is quite complicated, with a result of 30/91. My idea - before checking the book solution - was: let's just throw out the die that rolled a ""6"", now we have two dice and are looking for the probability of getting at least one ""1"" from them. Now this is fairly easy, it's 1-(5/6)^2 = 11/36. WRONG. Yes I know this is wrong. I went as far as write a program that simulated 10,000,000 rollings of three dice, and the relevant percentage worked out very close to 30/91, the result in the book, not my result. But where did I go wrong?","There is a problem which I solved wrong. I know the correct solution from the book, but I cannot find the gap in my reasoning. The problem is: three fair dice are rolled at the same time. What is the probability of getting at least one ""1"" upon condition that at least one die shows a ""6""? The solution in the book is quite complicated, with a result of 30/91. My idea - before checking the book solution - was: let's just throw out the die that rolled a ""6"", now we have two dice and are looking for the probability of getting at least one ""1"" from them. Now this is fairly easy, it's 1-(5/6)^2 = 11/36. WRONG. Yes I know this is wrong. I went as far as write a program that simulated 10,000,000 rollings of three dice, and the relevant percentage worked out very close to 30/91, the result in the book, not my result. But where did I go wrong?",,"['probability', 'conditional-probability', 'dice']"
14,Probability theory problem - the order of drawing tickets doesn't matter,Probability theory problem - the order of drawing tickets doesn't matter,,"I found this problem in a textbook. It is given right after the theory about the Bayes' rule and the total probability rule. Problem: We have an urn with $N$ lottery tickets of which $M \le N $ are winning tickets. $K$ persons $K \le N$ take turns drawing tickets from the urn in order. Each person draws one ticket. Prove that each person (no matter of his order number) has a probability of $M/N$ for drawing a winning ticket. I can prove this statement for persons $1$ and $2$ using the total probability law, but I cannot quite formalize the proof for the $K$ -th person. I have the feeling that the total probability law has to be used here. I have this approach in mind which I am not sure if it's rigorous enough. Here it is: Obviously the probability of each ticket being a winning ticket is $M/N$ . Let's suppose it's person # $K$ 's turn to draw and he draws some ticket $A$ . Now we define these events: $H_1$ : ticket A is a winning ticket $H_2$ : ticket A is not a winning ticket $B$ : person # $K$ has drawn a winning ticket Using the total probability law we get: $P(B) = P(H_1) P(B|H_1) + P(H_2) P(B|H_2) = ( M/N ) \cdot 1 + ((N-M)/N) \cdot 0 = M/N$ But this solution is weird to me because I feel like I am already assuming what I need to prove. I don't know if this approach is valid, is it? If it's not, how can this problem be solved more rigorously? And finally, I was also thinking of another approach: some sort of induction by K. But it didn't lead me anywhere (at least for now). So... is the above approach valid and if not, what is the best way to solve this problem rigorously (without using any complex apparatus of course, because this problem is in the very beginning of the textbook, only basic things are known so far) ?","I found this problem in a textbook. It is given right after the theory about the Bayes' rule and the total probability rule. Problem: We have an urn with lottery tickets of which are winning tickets. persons take turns drawing tickets from the urn in order. Each person draws one ticket. Prove that each person (no matter of his order number) has a probability of for drawing a winning ticket. I can prove this statement for persons and using the total probability law, but I cannot quite formalize the proof for the -th person. I have the feeling that the total probability law has to be used here. I have this approach in mind which I am not sure if it's rigorous enough. Here it is: Obviously the probability of each ticket being a winning ticket is . Let's suppose it's person # 's turn to draw and he draws some ticket . Now we define these events: : ticket A is a winning ticket : ticket A is not a winning ticket : person # has drawn a winning ticket Using the total probability law we get: But this solution is weird to me because I feel like I am already assuming what I need to prove. I don't know if this approach is valid, is it? If it's not, how can this problem be solved more rigorously? And finally, I was also thinking of another approach: some sort of induction by K. But it didn't lead me anywhere (at least for now). So... is the above approach valid and if not, what is the best way to solve this problem rigorously (without using any complex apparatus of course, because this problem is in the very beginning of the textbook, only basic things are known so far) ?",N M \le N  K K \le N M/N 1 2 K M/N K A H_1 H_2 B K P(B) = P(H_1) P(B|H_1) + P(H_2) P(B|H_2) = ( M/N ) \cdot 1 + ((N-M)/N) \cdot 0 = M/N,"['probability', 'probability-theory', 'solution-verification']"
15,Expectation of the number of men seated between two specific women,Expectation of the number of men seated between two specific women,,"I’ve been having a hard time answering this question (I’m bad at combinatorics) - i’d like it if you could help! The Question $m \geq 1$ men and $n \geq 2$ women sit randomly on a bench with $m+n$ places, two of these women are Hilla and Nikki. What is the expectation of the number of men sitting between Hilla and Nikki? So far my thinking was to look at particular cases and then move on to a generalized expression I can calculate. I think that for $i$ men between Hilla and Nikki I can look at the permutations as $(n+m-i-2+1)! = (m+n-1-i)!$ “outside” permutations times “inner” permutation of the Hilla/Nikki area. $(m)(m-1)(m-2)\dots (m-i+1) = \frac{m!}{(m-i)!}$ for picking i men since order matters, and multiply it by $2$ since I can switch Hilla and Nikki around. So overall the expectation is $$E(M)=\sum_{i=1}^m i \cdot 2ֿֿ\cdot \frac{m!}{(m-i)!} \cdot (m+n-1-i) ! \cdot  \frac{1}{(n+m)!}$$ but I think I didn’t account for the possibilities of women sitting between them. Overall this seems like a really complicated approach.","I’ve been having a hard time answering this question (I’m bad at combinatorics) - i’d like it if you could help! The Question men and women sit randomly on a bench with places, two of these women are Hilla and Nikki. What is the expectation of the number of men sitting between Hilla and Nikki? So far my thinking was to look at particular cases and then move on to a generalized expression I can calculate. I think that for men between Hilla and Nikki I can look at the permutations as “outside” permutations times “inner” permutation of the Hilla/Nikki area. for picking i men since order matters, and multiply it by since I can switch Hilla and Nikki around. So overall the expectation is but I think I didn’t account for the possibilities of women sitting between them. Overall this seems like a really complicated approach.",m \geq 1 n \geq 2 m+n i (n+m-i-2+1)! = (m+n-1-i)! (m)(m-1)(m-2)\dots (m-i+1) = \frac{m!}{(m-i)!} 2 E(M)=\sum_{i=1}^m i \cdot 2ֿֿ\cdot \frac{m!}{(m-i)!} \cdot (m+n-1-i) ! \cdot  \frac{1}{(n+m)!},"['probability', 'combinatorics', 'expected-value']"
16,Sample means and confidence intervals,Sample means and confidence intervals,,"For a population, where its parameters are unknown, why is it that if we take samples, those samples create a distribution that resembles that of a Normal Distribution? What if the original population distribution is not normal, why don't the samples create a distribution that is similar to the population, because they should have similar features to the population?","For a population, where its parameters are unknown, why is it that if we take samples, those samples create a distribution that resembles that of a Normal Distribution? What if the original population distribution is not normal, why don't the samples create a distribution that is similar to the population, because they should have similar features to the population?",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution']"
17,Probability of having exactly one empty container,Probability of having exactly one empty container,,What's the probability that if we distribute N letters among N drawers and presume that every single configuration is equally likely (saturation is at least 0 and at most N) then a single drawer will be found empty? ////////////////////// My progress so far: Total number of cases: $N^{N}$ Reasoning: It's the implication of the saturation condition. ///////////////// The number of favorable configurations: $2 \cdot \binom{N}{2}\cdot (N - 2)!$ Reasoning: I can choose $2$ drawers out of $N$ drawers in $\binom{N}{2}$ ways. These two drawers serve as the empty one and the one containing two letters. The factor $2$ is to consider the relative location of the two drawers. $(N - 2)!$ is to take the permutation of the remaining drawers into account. /////////////////// I feel like something is mistaken. Question: What's the route of correctness?,What's the probability that if we distribute N letters among N drawers and presume that every single configuration is equally likely (saturation is at least 0 and at most N) then a single drawer will be found empty? ////////////////////// My progress so far: Total number of cases: Reasoning: It's the implication of the saturation condition. ///////////////// The number of favorable configurations: Reasoning: I can choose drawers out of drawers in ways. These two drawers serve as the empty one and the one containing two letters. The factor is to consider the relative location of the two drawers. is to take the permutation of the remaining drawers into account. /////////////////// I feel like something is mistaken. Question: What's the route of correctness?,N^{N} 2 \cdot \binom{N}{2}\cdot (N - 2)! 2 N \binom{N}{2} 2 (N - 2)!,"['probability', 'combinatorics']"
18,Probability that second of $20$ watches is broken given $2$ are broken and after discarding first based on $80\%$-correct expert advice? [closed],Probability that second of  watches is broken given  are broken and after discarding first based on -correct expert advice? [closed],20 2 80\%,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question The problem goes this way. There are $20$ watches, $2$ of which are broken. And there is an expert who can say if the watch is broken. But his verdict is only correct with probability $0.8$ - in both cases, if the watch is broken and if it is not. So, then I choose one watch, show it to the expert and he says: it's broken. I choose then another watch of these $20$ . What is the probability that this second watch is broken? Thanks in advance. PS. I dont really understand how this information can help, but let it be. My math background: PhD, but it is 20 years ago, so I'm not an active mathematician anymore. The source of the problem: a student who i know. What I tried: naturally, Bayes theorem. But I still cannot solve it. PPS. I found the solution. It goes without Bayes but with the law of total probability. Its rather technical.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question The problem goes this way. There are watches, of which are broken. And there is an expert who can say if the watch is broken. But his verdict is only correct with probability - in both cases, if the watch is broken and if it is not. So, then I choose one watch, show it to the expert and he says: it's broken. I choose then another watch of these . What is the probability that this second watch is broken? Thanks in advance. PS. I dont really understand how this information can help, but let it be. My math background: PhD, but it is 20 years ago, so I'm not an active mathematician anymore. The source of the problem: a student who i know. What I tried: naturally, Bayes theorem. But I still cannot solve it. PPS. I found the solution. It goes without Bayes but with the law of total probability. Its rather technical.",20 2 0.8 20,"['probability', 'conditional-probability']"
19,Probability that the first and fourth balls are red when drawing 4 balls.,Probability that the first and fourth balls are red when drawing 4 balls.,,"Sorry, couldn't fit the entire question into the title. Question: A box contains 15 identical balls except that 10 are red and 5 are black. Four balls are drawn successively and without replacement. Calculate the probability that the first and fourth balls are red . My attempt: Probability = $$1*2C0 + 1*2C1 + 1*2C2  \over 4C0 + 4C1 + 4C2 + 4C3 + 4C4 $$ My idea is that no. of ways to make first and fourth balls = 1, and we have 2 balls left which can either have red or black colors. However, my textbook answer was: $$10P2*13P2\over15P4$$ Which I don't get at all; why would you use permutations when you have identical balls ? Wouldn't that mess things up? Thanks in advance.","Sorry, couldn't fit the entire question into the title. Question: A box contains 15 identical balls except that 10 are red and 5 are black. Four balls are drawn successively and without replacement. Calculate the probability that the first and fourth balls are red . My attempt: Probability = My idea is that no. of ways to make first and fourth balls = 1, and we have 2 balls left which can either have red or black colors. However, my textbook answer was: Which I don't get at all; why would you use permutations when you have identical balls ? Wouldn't that mess things up? Thanks in advance.",1*2C0 + 1*2C1 + 1*2C2  \over 4C0 + 4C1 + 4C2 + 4C3 + 4C4  10P2*13P2\over15P4,"['probability', 'combinatorics']"
20,How do you read the summa symbol with no superscript and the Real numbers subscript?,How do you read the summa symbol with no superscript and the Real numbers subscript?,,"How do you read $\int_\mathbb R f(x) dx$ ? I'm doing continuous random variables in probability, for context, so the whole thing is: $\int_\mathbb R f(x) dx = \int^{\infty}_{-\infty} f(x) dx = 1 $ And I get the idea that it's just saying that the total area under the curve is 1, but how do I say the first part in English? ""The integral over all the real values in the domain of f?""","How do you read ? I'm doing continuous random variables in probability, for context, so the whole thing is: And I get the idea that it's just saying that the total area under the curve is 1, but how do I say the first part in English? ""The integral over all the real values in the domain of f?""",\int_\mathbb R f(x) dx \int_\mathbb R f(x) dx = \int^{\infty}_{-\infty} f(x) dx = 1 ,"['probability', 'integration']"
21,Stuck on a probability question about pairs of socks,Stuck on a probability question about pairs of socks,,"The question is as follows: You have 10 pairs of socks (i.e., 20 socks in total) with each pair in a different color.  You put all the socks into the washing machine but it “eats” four of the 20 socks at random. What  is  the  expected  number  of  complete  pairs  left  in  the  washing machine? I approached this question like this: There are 3 possible outcomes: 8 pairs of socks survive 7 pairs of socks survive 6 pairs of socks survive I then tried to find the probability of each of these events occurring. The ways that 8 pairs of socks survive is: Eating 2 different colour socks then eating their pairs Eating first sock, then eating it's pair, Eating the second sock, then eating its pair P(8 pairs) = $1*\frac{18}{19}*\frac{2}{18}*\frac{1}{17}\\+1*\frac{1}{19}*1*\frac{1}{17}$ The ways that 7 pairs of socks survive is: Eating three socks of different colours, then eating any one of their pairs Eating two socks of different colours, then eating any one of their pairs, then drawing another sock of a different colour Eating any sock, eating another sock of the same colour, then eating two other socks of different colours P(7 socks)= $1*\frac{18}{19}*\frac{16}{18}*\frac{3}{17}\\+1*\frac{18}{19}*\frac{2}{18}*\frac{16}{17}\\+1*\frac{1}{19}*1*\frac{16}{17}$ The ways that 6 pairs of socks survive is: Eating 4 socks of different colours P(6 pairs)= $1*\frac{18}{19}*\frac{16}{18}*\frac{14}{17}$ I am confident that my solution is incorrect. First of all, the probabilities sum to $\frac{2906}{2907}$ when it should sum to 1. Secondly, I simulated the process in the question on my computer and the probabilities that I obtained were: P(8 pairs)= $0.0269986$ P(7 pairs)= $0.4319539$ P(6 pairs)= $0.5410475$ Compared to my answers which were: P(8 pairs)= $0.0093$ P(7 pairs)= $0.2972$ P(6 pairs)= $0.6935$ Would there be another simpler way of solving the question?","The question is as follows: You have 10 pairs of socks (i.e., 20 socks in total) with each pair in a different color.  You put all the socks into the washing machine but it “eats” four of the 20 socks at random. What  is  the  expected  number  of  complete  pairs  left  in  the  washing machine? I approached this question like this: There are 3 possible outcomes: 8 pairs of socks survive 7 pairs of socks survive 6 pairs of socks survive I then tried to find the probability of each of these events occurring. The ways that 8 pairs of socks survive is: Eating 2 different colour socks then eating their pairs Eating first sock, then eating it's pair, Eating the second sock, then eating its pair P(8 pairs) = The ways that 7 pairs of socks survive is: Eating three socks of different colours, then eating any one of their pairs Eating two socks of different colours, then eating any one of their pairs, then drawing another sock of a different colour Eating any sock, eating another sock of the same colour, then eating two other socks of different colours P(7 socks)= The ways that 6 pairs of socks survive is: Eating 4 socks of different colours P(6 pairs)= I am confident that my solution is incorrect. First of all, the probabilities sum to when it should sum to 1. Secondly, I simulated the process in the question on my computer and the probabilities that I obtained were: P(8 pairs)= P(7 pairs)= P(6 pairs)= Compared to my answers which were: P(8 pairs)= P(7 pairs)= P(6 pairs)= Would there be another simpler way of solving the question?",1*\frac{18}{19}*\frac{2}{18}*\frac{1}{17}\\+1*\frac{1}{19}*1*\frac{1}{17} 1*\frac{18}{19}*\frac{16}{18}*\frac{3}{17}\\+1*\frac{18}{19}*\frac{2}{18}*\frac{16}{17}\\+1*\frac{1}{19}*1*\frac{16}{17} 1*\frac{18}{19}*\frac{16}{18}*\frac{14}{17} \frac{2906}{2907} 0.0269986 0.4319539 0.5410475 0.0093 0.2972 0.6935,"['probability', 'combinatorics']"
22,Xor of two binary random variables,Xor of two binary random variables,,"Let $X,Y\in\{0,1\}$ be two dependent binary random variables such that $\Pr[X=0]=\frac{1}{2}+\alpha$ and $\Pr[Y=0]=\frac{1}{2}+\beta$ for $\alpha,\beta\geq 0$ . My question is how to get a lower bound of $\Pr[X\oplus Y=0]$ . Here $X\oplus Y$ is the xor of two binary variables $X,Y$ . In the case that they are independent, $\Pr[X\oplus Y=0]=(1/2+\alpha)(1/2+\beta)+(1/2-\alpha)(1/2-\beta)=1/2+2\alpha\beta$ . When they are dependent, I have $\Pr[X\oplus Y=0]\geq \Pr[X=0,Y=0]\geq 1-(1/2-\alpha)-(1/2-\beta)=\alpha+\beta$ . However, the bound seems weak. I suspect one can get a lower bound greater than $1/2$ as in the independent case, but a counterexample would also be appreciated.","Let be two dependent binary random variables such that and for . My question is how to get a lower bound of . Here is the xor of two binary variables . In the case that they are independent, . When they are dependent, I have . However, the bound seems weak. I suspect one can get a lower bound greater than as in the independent case, but a counterexample would also be appreciated.","X,Y\in\{0,1\} \Pr[X=0]=\frac{1}{2}+\alpha \Pr[Y=0]=\frac{1}{2}+\beta \alpha,\beta\geq 0 \Pr[X\oplus Y=0] X\oplus Y X,Y \Pr[X\oplus Y=0]=(1/2+\alpha)(1/2+\beta)+(1/2-\alpha)(1/2-\beta)=1/2+2\alpha\beta \Pr[X\oplus Y=0]\geq \Pr[X=0,Y=0]\geq 1-(1/2-\alpha)-(1/2-\beta)=\alpha+\beta 1/2","['probability', 'random-variables', 'upper-lower-bounds']"
23,Probability of selecting a poker hand,Probability of selecting a poker hand,,"I am trying to solve a probability problem about five-card poker hand. I have access to the answer which is different from what I had come up with. The question is: What is the probability that a five-card poker hand has exactly two cards of same value, but no other cards duplicated? My answer to this question was as follows: $\binom{13}{1} \binom{4}{2} \binom{48}{1}\binom{44}{1} \binom{40}{1}$ . Which means: First select a card number then select its two suits ie. $\binom{13}{1} \binom{4}{2}$ . These will be the two cards of same value. Select three other cards which are not duplicate as: $\binom{48}{1}\binom{44}{1} \binom{40}{1}$ . The correct answer doesn't match my answer. This answer is provided in book AOPS and is as: $\binom{13}{1} \binom{4}{2}\binom{12}{3}\binom{4}{1}\binom{4}{1}\binom{4}{1}$ . So question is, what am I doing wrong? Thanks","I am trying to solve a probability problem about five-card poker hand. I have access to the answer which is different from what I had come up with. The question is: What is the probability that a five-card poker hand has exactly two cards of same value, but no other cards duplicated? My answer to this question was as follows: . Which means: First select a card number then select its two suits ie. . These will be the two cards of same value. Select three other cards which are not duplicate as: . The correct answer doesn't match my answer. This answer is provided in book AOPS and is as: . So question is, what am I doing wrong? Thanks",\binom{13}{1} \binom{4}{2} \binom{48}{1}\binom{44}{1} \binom{40}{1} \binom{13}{1} \binom{4}{2} \binom{48}{1}\binom{44}{1} \binom{40}{1} \binom{13}{1} \binom{4}{2}\binom{12}{3}\binom{4}{1}\binom{4}{1}\binom{4}{1},"['probability', 'combinatorics']"
24,Can An irreducible Markov chain with infinitely many states be a positive recurrent chain or periodic chain?,Can An irreducible Markov chain with infinitely many states be a positive recurrent chain or periodic chain?,,"I understand that any irreducible finite Markov chain is necessarily positive recurrent, however can this be the case for a Markov chain with infinitely many states? Similarly for periodicity? There also seems to be a link between the 2, either both statements are true or false.","I understand that any irreducible finite Markov chain is necessarily positive recurrent, however can this be the case for a Markov chain with infinitely many states? Similarly for periodicity? There also seems to be a link between the 2, either both statements are true or false.",,"['probability', 'probability-theory', 'stochastic-processes', 'markov-chains']"
25,"What's the distribution of $xy+xz+yz$ where $x,y,z $ are independent standard normal?",What's the distribution of  where  are independent standard normal?,"xy+xz+yz x,y,z ","We know the product of two independent Normal random variables has a normal product distribution, or Variance Gamma distribution if they are correlated. But, what if there are three Normal random variables? So, here is the question: Suppose $x,y,z$ are three independent normal random variables ( $x, y, z\sim N(0,1)$ ), what's the distribution of $xy+xz+yz$ ?","We know the product of two independent Normal random variables has a normal product distribution, or Variance Gamma distribution if they are correlated. But, what if there are three Normal random variables? So, here is the question: Suppose are three independent normal random variables ( ), what's the distribution of ?","x,y,z x, y, z\sim N(0,1) xy+xz+yz","['probability', 'probability-distributions', 'normal-distribution']"
26,Solution verification: Calculating expected value of sum of three unusual dice,Solution verification: Calculating expected value of sum of three unusual dice,,"Problem: We have three fair dice. Their numbering is unusual, the $6$ numbers on the respective dice are: $$\text{Die}\,\,\#1:\,1,3,5,7,9,10,\quad\text{Die}\,\,\#2:\,1,2,2,3,3,3,\quad\text{Die}\,\,\#3:\,2,2,4,4,4,4.$$ We roll all three of these dice, and denote by $X$ the sum of the three numbers that are showing. Find the expected value $X$ . My Attempt: I have a hunch that the indicator approach might somehow work here but I do not see how to execute it. Therefore, I will attempt to solve the problem the hard way, by finding the probability mass function of $X$ . Note that $X$ takes values in the set $A=\{4,5,6,7,8,9,10,11,12,13,14,15,16,17\}.$ Now we have the following calculations using the independence of the die rolls and some basic counting rules $$P(X=4)=\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{2}{6^3},\, \\P(X=5)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{4}{6^3},\, \\P(X=6)=\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{12}{6^3},$$ $$P(X=7)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=8)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3},$$ $$P(X=9)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=10)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3},$$ $$P(X=11)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=12)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3},$$ $$P(X=13)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{14}{6^3},$$ $$P(X=14)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{26}{6^3},$$ $$P(X=15)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}=\frac{18}{6^3},$$ $$P(X=16)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}=\frac{20}{6^3},\, \\P(X=17)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}=\frac{12}{6^3}.$$ One can see that $\sum_{k=4}^{17}P(X=k)=1$ , so we have a legitimate probability mass function. Now let us calculate the expectation of $X$ . Using the definition we have $$E[X]=\sum_{k=4}^{17}k P(X=k)=\frac{1}{6^3}[4\cdot2+5\cdot4+6\cdot12+7\cdot12+8\cdot24+9\cdot12+10\cdot24+11\cdot12+12\cdot24+13\cdot14+14\cdot26+15\cdot18+16\cdot20+17\cdot12]=11.5$$ Could anybody please give me some feedback about my approach? If my exposition is unclear, please let me know and I will do my best to improve it. In addition, if anyone has a simpler way of tackling the problem, any hints on how to go about such a method would be much appreciated. Thank you very much for your time and feedback.","Problem: We have three fair dice. Their numbering is unusual, the numbers on the respective dice are: We roll all three of these dice, and denote by the sum of the three numbers that are showing. Find the expected value . My Attempt: I have a hunch that the indicator approach might somehow work here but I do not see how to execute it. Therefore, I will attempt to solve the problem the hard way, by finding the probability mass function of . Note that takes values in the set Now we have the following calculations using the independence of the die rolls and some basic counting rules One can see that , so we have a legitimate probability mass function. Now let us calculate the expectation of . Using the definition we have Could anybody please give me some feedback about my approach? If my exposition is unclear, please let me know and I will do my best to improve it. In addition, if anyone has a simpler way of tackling the problem, any hints on how to go about such a method would be much appreciated. Thank you very much for your time and feedback.","6 \text{Die}\,\,\#1:\,1,3,5,7,9,10,\quad\text{Die}\,\,\#2:\,1,2,2,3,3,3,\quad\text{Die}\,\,\#3:\,2,2,4,4,4,4. X X X X A=\{4,5,6,7,8,9,10,11,12,13,14,15,16,17\}. P(X=4)=\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{2}{6^3},\, \\P(X=5)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{4}{6^3},\, \\P(X=6)=\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{12}{6^3}, P(X=7)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=8)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3}, P(X=9)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=10)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3}, P(X=11)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=12)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3}, P(X=13)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{14}{6^3}, P(X=14)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{26}{6^3}, P(X=15)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}=\frac{18}{6^3}, P(X=16)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}=\frac{20}{6^3},\, \\P(X=17)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}=\frac{12}{6^3}. \sum_{k=4}^{17}P(X=k)=1 X E[X]=\sum_{k=4}^{17}k P(X=k)=\frac{1}{6^3}[4\cdot2+5\cdot4+6\cdot12+7\cdot12+8\cdot24+9\cdot12+10\cdot24+11\cdot12+12\cdot24+13\cdot14+14\cdot26+15\cdot18+16\cdot20+17\cdot12]=11.5","['probability', 'solution-verification', 'expected-value']"
27,"Estimate expected payoff of rolling a dice, with choice of rolling up to $50$ times.","Estimate expected payoff of rolling a dice, with choice of rolling up to  times.",50,"This is an extended question of the classical rolling dice and give face value question. You roll a dice, and you'll be paid by face value. If you're not satisfied, you can roll again. You are allowed $k$ rolls. In the old question, if you are allowed two rolls, then the expected payoff is $E[\text{payoff}] = 4.25$ . If  you are allowed $3$ rolls, the expected payoff is $E[\text{payoff}] = 4.67$ . If you can roll up to $50$ times, you can calculate the payoff using the formula and get $E = 5.999762$ , notice that after $5^\text{th}$ roll, your expected payoff will be greater than $5$ , so you'll only stop once you roll $6$ . So my question here is, without exact calculation(using geometric process), how would you estimate how many $9$ s are there in the answer? Or another way to ask will be, is the expected payoff bigger than $5.9$ ? bigger than $5.99$ ? etc.","This is an extended question of the classical rolling dice and give face value question. You roll a dice, and you'll be paid by face value. If you're not satisfied, you can roll again. You are allowed rolls. In the old question, if you are allowed two rolls, then the expected payoff is . If  you are allowed rolls, the expected payoff is . If you can roll up to times, you can calculate the payoff using the formula and get , notice that after roll, your expected payoff will be greater than , so you'll only stop once you roll . So my question here is, without exact calculation(using geometric process), how would you estimate how many s are there in the answer? Or another way to ask will be, is the expected payoff bigger than ? bigger than ? etc.",k E[\text{payoff}] = 4.25 3 E[\text{payoff}] = 4.67 50 E = 5.999762 5^\text{th} 5 6 9 5.9 5.99,"['probability', 'expected-value', 'dice']"
28,Expectation of product of quadratic forms in Gaussian distribution,Expectation of product of quadratic forms in Gaussian distribution,,"Let $x \in \mathbb{R}^n$ be a random vector with i.i.d. entries distributed as $\mathcal{N}(0, 1)$ .  Let $A, B$ be two $n \times n$ symmetric matrices.  I would like to find $\mathbb{E} (x^T A x) (x^T B x)$ .",Let be a random vector with i.i.d. entries distributed as .  Let be two symmetric matrices.  I would like to find .,"x \in \mathbb{R}^n \mathcal{N}(0, 1) A, B n \times n \mathbb{E} (x^T A x) (x^T B x)","['probability', 'probability-distributions', 'normal-distribution', 'expected-value']"
29,Intuition on probability of drawing two aces given that the first draw is an ace.,Intuition on probability of drawing two aces given that the first draw is an ace.,,"I would like to know why my reasoning is wrong on this problem. Given that the first draw is an ace, I think there would be 3/51 chances of getting two aces after I know that the first one is an ace. It seems that the probability of this event is 1/33. Where is my reasoning incorrect? Edit : The problem is as follows. I have a standard deck of cards and I draw $2$ cards at random. I would like to compute the probability $ P( \text{Having two aces} | \text{ Have one ace} )$ and $P( \text{Having two aces} | \text{Having one ace of spades} )$ . It seems, based on these lectures from Harvard: https://www.youtube.com/watch?v=JzDvVgNDxo8&list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo&index=5 at $9:30$ mark, that $$ P( \text{Having two aces} | \text{ Have one ace} ) = 1/33 $$ while $$ P( \text{Having two aces} | \text{Having one ace of spades} ) = 1/17 $$ I do not understand why my combinatorics is incorrect, even though I think I understand his solution.","I would like to know why my reasoning is wrong on this problem. Given that the first draw is an ace, I think there would be 3/51 chances of getting two aces after I know that the first one is an ace. It seems that the probability of this event is 1/33. Where is my reasoning incorrect? Edit : The problem is as follows. I have a standard deck of cards and I draw cards at random. I would like to compute the probability and . It seems, based on these lectures from Harvard: https://www.youtube.com/watch?v=JzDvVgNDxo8&list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo&index=5 at mark, that while I do not understand why my combinatorics is incorrect, even though I think I understand his solution.","2 
P( \text{Having two aces} | \text{ Have one ace} ) P( \text{Having two aces} | \text{Having one ace of spades} ) 9:30 
P( \text{Having two aces} | \text{ Have one ace} ) = 1/33
 
P( \text{Having two aces} | \text{Having one ace of spades} ) = 1/17
","['probability', 'combinatorics', 'conditional-probability']"
30,Radon–Nikodym Derivative and Bayes' Theorem,Radon–Nikodym Derivative and Bayes' Theorem,,"Theorem 1.3.1. (Bayes' theorem): Suppose that $X$ has a parametric family $\mathcal{P}_0$ of distributions with parameter space $\Omega$ .   Suppose that $P_\theta \ll \nu$ for all $\theta \in \Omega$ , and let $f_{X\mid\Theta}(x\mid\theta)$ be the conditional density (with respect to $\nu$ ) of $X$ given $\Theta = \theta$ .   Let $\mu_\Theta$ be the prior distribution of $\Theta$ .   Let $\mu_{\Theta\mid X}(\cdot \mid x)$ denote the conditional distribution of $\Theta$ given $X = x$ .   Then $\mu_{\Theta\mid X} \ll \mu_\Theta$ , a.s. with respect to the marginal of $X$ , and the Radon–Nikodym derivative is $$ \frac{\mathrm d\mu_{\Theta\mid X}}{\mathrm d\mu_\Theta}(\theta \mid x) = \frac{f_{X\mid \Theta}(x\mid \theta)}{\int_\Omega f_{X\mid\Theta}(x\mid t) \, \mathrm d\mu_\Theta(t)} $$ for those $x$ such that the denominator is neither $0$ nor infinite.   The prior predictive probability of the set of $x$ values such that the denominator is $0$ or infinite is $0$ , hence the posterior can be defined arbitrarily for such $x$ values. I tried to derive the right hand side of the Radon–Nikodym derivative above but I got different result, here is my attempt: \begin{equation} \label{eq1} \begin{split} \frac{\mathrm d\mu_{\Theta\mid X}}{\mathrm d\mu_\Theta}(\theta \mid x) &= f_{\Theta\mid X}(\theta\mid x) \mathrm \space \space \space[1]\\ &=\frac{f_{X\mid \Theta}(x\mid \theta) \cdot f_{\Theta}(\theta)}{f_X(x)}\\ &=\frac{f_{X\mid \Theta}(x\mid \theta) \cdot f_{\Theta}(\theta)}{\int_\Omega f_{X\mid\Theta}(x\mid t) \, \cdot f_{\Theta}(t) \space \mathrm dt}\\ &=\frac{f_{X\mid \Theta}(x\mid \theta) \cdot f_{\Theta}(\theta)}{\int_\Omega f_{X\mid\Theta}(x\mid t) \, \mathrm d\mu_\Theta(t)} \end{split} \end{equation} but now, where does $f_{\Theta}(\theta)$ go? for $[1]$ see slide $10$ of the following document: http://mlg.eng.cam.ac.uk/mlss09/mlss_slides/Orbanz_1.pdf Thanks in advance.","Theorem 1.3.1. (Bayes' theorem): Suppose that has a parametric family of distributions with parameter space .   Suppose that for all , and let be the conditional density (with respect to ) of given .   Let be the prior distribution of .   Let denote the conditional distribution of given .   Then , a.s. with respect to the marginal of , and the Radon–Nikodym derivative is for those such that the denominator is neither nor infinite.   The prior predictive probability of the set of values such that the denominator is or infinite is , hence the posterior can be defined arbitrarily for such values. I tried to derive the right hand side of the Radon–Nikodym derivative above but I got different result, here is my attempt: but now, where does go? for see slide of the following document: http://mlg.eng.cam.ac.uk/mlss09/mlss_slides/Orbanz_1.pdf Thanks in advance.","X \mathcal{P}_0 \Omega P_\theta \ll \nu \theta \in \Omega f_{X\mid\Theta}(x\mid\theta) \nu X \Theta = \theta \mu_\Theta \Theta \mu_{\Theta\mid X}(\cdot \mid x) \Theta X = x \mu_{\Theta\mid X} \ll \mu_\Theta X 
\frac{\mathrm d\mu_{\Theta\mid X}}{\mathrm d\mu_\Theta}(\theta \mid x)
= \frac{f_{X\mid \Theta}(x\mid \theta)}{\int_\Omega f_{X\mid\Theta}(x\mid t) \, \mathrm d\mu_\Theta(t)}
 x 0 x 0 0 x \begin{equation} \label{eq1}
\begin{split}
\frac{\mathrm d\mu_{\Theta\mid X}}{\mathrm d\mu_\Theta}(\theta \mid x) &= f_{\Theta\mid X}(\theta\mid x) \mathrm \space \space \space[1]\\
&=\frac{f_{X\mid \Theta}(x\mid \theta) \cdot f_{\Theta}(\theta)}{f_X(x)}\\
&=\frac{f_{X\mid \Theta}(x\mid \theta) \cdot f_{\Theta}(\theta)}{\int_\Omega f_{X\mid\Theta}(x\mid t) \, \cdot f_{\Theta}(t) \space \mathrm dt}\\
&=\frac{f_{X\mid \Theta}(x\mid \theta) \cdot f_{\Theta}(\theta)}{\int_\Omega f_{X\mid\Theta}(x\mid t) \, \mathrm d\mu_\Theta(t)}
\end{split}
\end{equation} f_{\Theta}(\theta) [1] 10","['probability', 'probability-theory', 'measure-theory']"
31,Find the unique distribution of a random variable knowing the moments of the random variable,Find the unique distribution of a random variable knowing the moments of the random variable,,"This problem comes from Allan Gut's 'An Intermediate Course in Probability', but I cannot solve the problem. The random variable $X$ has the property that $$EX^{n}=\frac{2^{n}}{n+1}, \quad n = 1,2, ...$$ Find some (in fact, the unique) distribution of X having these moments. I know that the moment-generating function (MGF) can be expressed as $$\psi_{X}(t)= E\, e^{tX} = 1 + \sum_{k=1}^{\infty}\frac{t^{k}}{k!}EX^{k}.$$ I tried to use this expression to find an expression of the MGF that I can use to identify the distribution: $$\psi_{X}(t) = 1 + \sum_{k = 1}^{\infty}\frac{t^{k}}{k!}\frac{2^{k}}{k+1} = 1 + \sum_{k = 1}^{\infty}\frac{(2t)^{k}}{k!(k+1)}$$ $$ = 1 + \sum_{k = 1}^{\infty}\frac{(2t)^{k+1}}{2t(k+1)!} = 1 + \frac{1}{2t}\sum_{k = 0}^{\infty} \frac{(2t)^{k}}{k!} = 1+\frac{1}{2t}e^{2t}$$ But this is not an MGF I recognize, so I'm not sure how to proceed.","This problem comes from Allan Gut's 'An Intermediate Course in Probability', but I cannot solve the problem. The random variable has the property that Find some (in fact, the unique) distribution of X having these moments. I know that the moment-generating function (MGF) can be expressed as I tried to use this expression to find an expression of the MGF that I can use to identify the distribution: But this is not an MGF I recognize, so I'm not sure how to proceed.","X EX^{n}=\frac{2^{n}}{n+1}, \quad n = 1,2, ... \psi_{X}(t)= E\, e^{tX} = 1 + \sum_{k=1}^{\infty}\frac{t^{k}}{k!}EX^{k}. \psi_{X}(t) = 1 + \sum_{k = 1}^{\infty}\frac{t^{k}}{k!}\frac{2^{k}}{k+1} = 1 + \sum_{k = 1}^{\infty}\frac{(2t)^{k}}{k!(k+1)}  = 1 + \sum_{k = 1}^{\infty}\frac{(2t)^{k+1}}{2t(k+1)!} = 1 + \frac{1}{2t}\sum_{k = 0}^{\infty} \frac{(2t)^{k}}{k!} = 1+\frac{1}{2t}e^{2t}","['probability', 'probability-theory', 'probability-distributions']"
32,What is the probability that two cards drawn from a deck are both face cards and at least one is red?,What is the probability that two cards drawn from a deck are both face cards and at least one is red?,,"Two cards are drawn from a deck of cards. What is the probability that they are both face cards and at least one is red? Assume that there are $52$ cards and without replacement. I have two different methods, and they got different solutions. The second method below is the same as the teacher's answer: $0.0385$ , but the first is not. What is wrong with my first solution? Method 1: $\frac{12\cdot11}{52\cdot51}\cdot\frac{3}{4}=0.0373$ . I got this because I calculated the probability that I get two face cards, and then I multiplied by $\frac {3}{4}$ because there is $\frac {3}{4}$ chance that I get at least $1$ red. Method 2: $\frac{\binom{12}{2}}{\binom{52}{2}}-\frac{\binom{6}{2}}{\binom{52}{2}}=0.0385$ . The first fraction is the probability that both are face cards. The second fraction is the probability that they are both face cards and both black.","Two cards are drawn from a deck of cards. What is the probability that they are both face cards and at least one is red? Assume that there are cards and without replacement. I have two different methods, and they got different solutions. The second method below is the same as the teacher's answer: , but the first is not. What is wrong with my first solution? Method 1: . I got this because I calculated the probability that I get two face cards, and then I multiplied by because there is chance that I get at least red. Method 2: . The first fraction is the probability that both are face cards. The second fraction is the probability that they are both face cards and both black.",52 0.0385 \frac{12\cdot11}{52\cdot51}\cdot\frac{3}{4}=0.0373 \frac {3}{4} \frac {3}{4} 1 \frac{\binom{12}{2}}{\binom{52}{2}}-\frac{\binom{6}{2}}{\binom{52}{2}}=0.0385,"['probability', 'card-games']"
33,Does the uniform distribution minimize the expected value in the coupon collector's problem?,Does the uniform distribution minimize the expected value in the coupon collector's problem?,,"Given a discrete probability distribution with $n$ possible outcomes and the task of repeatedly sampling until getting each outcome at least once, does the uniform distribution give the smallest expected value for the total number of samples?","Given a discrete probability distribution with $n$ possible outcomes and the task of repeatedly sampling until getting each outcome at least once, does the uniform distribution give the smallest expected value for the total number of samples?",,"['probability', 'optimization', 'coupon-collector']"
34,"Policy Gradients, Log Trick, Expectations, Softmax Policy","Policy Gradients, Log Trick, Expectations, Softmax Policy",,"In so-called ""policy based"" game playing algoritms (reinforcement learning in particular), there is a policy function $\pi_\theta(s,a)$ that gives the probability of action given state of the game and hidden parameter $\theta$, as shown in pg. 16 of lecture notes below. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf In order to improve the score (let me drop the underscript $\theta$ from now on), instead of $\nabla \pi$ one usually tries to compute $\nabla \log \pi(s,a)$ based on this trick $$  \nabla \pi(s,a) = \pi(s,a) \frac{\nabla \pi(s,a)}{\pi(s,a)} $$ $$ = \pi(s,a)\nabla \log \pi(s,a) $$ Now the author above calls $\nabla \log \pi(s,a)$ the score function and claims ""he can compute it easier because the expectation of it is easy"". As an example he picks softmax as $\pi$ which is, $$ \pi(s,a) \propto e^{\phi(s,a)^T\theta} $$ and jumps to score function $$ \nabla \log \pi(s,a) = \phi(s,a) - E[\phi(s,\cdot)] $$ I don't understand how the derivation could reach this statement. After some laboring, I have $h(s,a,\theta) = \phi(s,a)^T\theta$ $$  \pi_\theta(s,a) = \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} } $$ The gradient of the log $$  \nabla_\theta \log \pi_\theta =  \nabla_\theta \log \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} } $$ $$ = \nabla_\theta \big[ \log e^{h(s,a,\theta)} - \log \sum_b e^{h(s,b,\theta)}\big]$$ because $$ \log(\frac{x}{y}) = \log x - \log y $$ We continue $$ = \nabla_\theta \big[ h(s,a,\theta) - \log \sum_b e^{h(s,b,\theta)} \big]$$ $$  = \phi(s,a) - \sum_b h(s,b,\theta)\frac{e^{h(s,b,\theta)}}{\sum_b e^{h(s,b,\theta)}}  $$ $$  = \phi(s,a) - \sum_b h(s,b,\theta) \pi_\theta(b,s) $$ The last term can be seen as expectation. Would this work? I am not sure of that last derivative of log of sum.","In so-called ""policy based"" game playing algoritms (reinforcement learning in particular), there is a policy function $\pi_\theta(s,a)$ that gives the probability of action given state of the game and hidden parameter $\theta$, as shown in pg. 16 of lecture notes below. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf In order to improve the score (let me drop the underscript $\theta$ from now on), instead of $\nabla \pi$ one usually tries to compute $\nabla \log \pi(s,a)$ based on this trick $$  \nabla \pi(s,a) = \pi(s,a) \frac{\nabla \pi(s,a)}{\pi(s,a)} $$ $$ = \pi(s,a)\nabla \log \pi(s,a) $$ Now the author above calls $\nabla \log \pi(s,a)$ the score function and claims ""he can compute it easier because the expectation of it is easy"". As an example he picks softmax as $\pi$ which is, $$ \pi(s,a) \propto e^{\phi(s,a)^T\theta} $$ and jumps to score function $$ \nabla \log \pi(s,a) = \phi(s,a) - E[\phi(s,\cdot)] $$ I don't understand how the derivation could reach this statement. After some laboring, I have $h(s,a,\theta) = \phi(s,a)^T\theta$ $$  \pi_\theta(s,a) = \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} } $$ The gradient of the log $$  \nabla_\theta \log \pi_\theta =  \nabla_\theta \log \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} } $$ $$ = \nabla_\theta \big[ \log e^{h(s,a,\theta)} - \log \sum_b e^{h(s,b,\theta)}\big]$$ because $$ \log(\frac{x}{y}) = \log x - \log y $$ We continue $$ = \nabla_\theta \big[ h(s,a,\theta) - \log \sum_b e^{h(s,b,\theta)} \big]$$ $$  = \phi(s,a) - \sum_b h(s,b,\theta)\frac{e^{h(s,b,\theta)}}{\sum_b e^{h(s,b,\theta)}}  $$ $$  = \phi(s,a) - \sum_b h(s,b,\theta) \pi_\theta(b,s) $$ The last term can be seen as expectation. Would this work? I am not sure of that last derivative of log of sum.",,"['probability', 'multivariable-calculus']"
35,Probability of eighth best reaching the semifinals,Probability of eighth best reaching the semifinals,,"$16$ players $P_1, P_2, ..... P_{16}$ take part in a tennis knockout tournament. The order of the matches is chosen in random. Lower suffix player is better than higher suffix, the better wins. What is the probability that the eighth best reaches the semifinals? Basically I think that we just have to keep selecting only those players who are below $P_8$. For the first round the probability of choosing a player below $P_8$ would be $\frac{8}{15}$. But then I find it difficult to arrange the players who got selected in the next round.","$16$ players $P_1, P_2, ..... P_{16}$ take part in a tennis knockout tournament. The order of the matches is chosen in random. Lower suffix player is better than higher suffix, the better wins. What is the probability that the eighth best reaches the semifinals? Basically I think that we just have to keep selecting only those players who are below $P_8$. For the first round the probability of choosing a player below $P_8$ would be $\frac{8}{15}$. But then I find it difficult to arrange the players who got selected in the next round.",,['probability']
36,Which one of these options is false?,Which one of these options is false?,,"Given two independent events $A$ and $B$, with given conditions: $0 \lt P(A) , P(B) <1 $. Which one of the following options is/are false? $A$ and $B’$ are independent. $A’$ and $B’$ are independent. $P(A|B) = P(A|B’)$ For any event c, with $0 \lt P(c) \lt 1$, $P(AB|c)= P(A|c)\cdot P(B|c)$ Here is what I tried: A and B are independent iff:  $P(A \cap B)$ $=$ $P(A)\cdot P(B)$ Now, we have : $P(A) = P(A \cap B) + P(A \cap B')$ So, $ P(A \cap B')$ $=$ $P(A) - P(A \cap B)$ $=$ $P(A) - P(A)\cdot P(B)$ $=$ $[1-P(B)]\cdot P(A)$ $=$ $P(A)\cdot P(B')$ Thus, 1 is true. We know that, $P(A’ \cap B’) =P(A \cup B )’$ $ =1 - P(A \cup B)$ $  =1 - P(A) - P(B) + P( A \cap B)$ $  =1 - P(A) - P(B) + P(A)\cdot P(B)$ $  = [1-P(A)] \cdot [1-P(B)]$ $  =P(A’)P(B’) $ Thus, 2 is also true. By conditional probability,  $P(A | B)$ $=$ $\frac{P(A \cap B)} {P(B)}$ $=$ $\frac{P(A)\cdot P(B)}{P(B)}$ $=$ $P(A) $ And $P(A | B') $ $ =$ $\frac{P(A \cap B')}{P(B')}$ $=$ $\frac{P(A)\cdot P(B')}{P(B')}$ $=$ $P(A) $ The problem is with 4. I tried to disprove it, by finding a counter-example, and I couldn't. What is the correct answer?","Given two independent events $A$ and $B$, with given conditions: $0 \lt P(A) , P(B) <1 $. Which one of the following options is/are false? $A$ and $B’$ are independent. $A’$ and $B’$ are independent. $P(A|B) = P(A|B’)$ For any event c, with $0 \lt P(c) \lt 1$, $P(AB|c)= P(A|c)\cdot P(B|c)$ Here is what I tried: A and B are independent iff:  $P(A \cap B)$ $=$ $P(A)\cdot P(B)$ Now, we have : $P(A) = P(A \cap B) + P(A \cap B')$ So, $ P(A \cap B')$ $=$ $P(A) - P(A \cap B)$ $=$ $P(A) - P(A)\cdot P(B)$ $=$ $[1-P(B)]\cdot P(A)$ $=$ $P(A)\cdot P(B')$ Thus, 1 is true. We know that, $P(A’ \cap B’) =P(A \cup B )’$ $ =1 - P(A \cup B)$ $  =1 - P(A) - P(B) + P( A \cap B)$ $  =1 - P(A) - P(B) + P(A)\cdot P(B)$ $  = [1-P(A)] \cdot [1-P(B)]$ $  =P(A’)P(B’) $ Thus, 2 is also true. By conditional probability,  $P(A | B)$ $=$ $\frac{P(A \cap B)} {P(B)}$ $=$ $\frac{P(A)\cdot P(B)}{P(B)}$ $=$ $P(A) $ And $P(A | B') $ $ =$ $\frac{P(A \cap B')}{P(B')}$ $=$ $\frac{P(A)\cdot P(B')}{P(B')}$ $=$ $P(A) $ The problem is with 4. I tried to disprove it, by finding a counter-example, and I couldn't. What is the correct answer?",,['probability']
37,How to compute $E[\log(X)]$ when $X$ follows a beta distribution?,How to compute  when  follows a beta distribution?,E[\log(X)] X,"Given a Beta variable $X \sim B(\alpha\ge 2,\beta)$ , how do I compute the expectation of its logarithm $E[\log(X)]$ ? This is deemed ""obvious"" on MO , but I see no easy way to compute $\int_0^1 x^{\alpha-1}(1-x)^{\beta-1}\log x \; dx$ . Differentiating Beta function $B(\alpha,\beta)=\int_0^1 x^{\alpha-1}(1-x)^{\beta-1}dx$ by $\alpha$ results in Digamma function - is this really the way to go? Thanks.","Given a Beta variable , how do I compute the expectation of its logarithm ? This is deemed ""obvious"" on MO , but I see no easy way to compute . Differentiating Beta function by results in Digamma function - is this really the way to go? Thanks.","X \sim B(\alpha\ge 2,\beta) E[\log(X)] \int_0^1 x^{\alpha-1}(1-x)^{\beta-1}\log x \; dx B(\alpha,\beta)=\int_0^1 x^{\alpha-1}(1-x)^{\beta-1}dx \alpha",['probability']
38,Probability of having $n$ dice equal to or greater than $x$ when $m$ dice are rolled,Probability of having  dice equal to or greater than  when  dice are rolled,n x m,"I am trying to figure out how to do the math for something like this. The scenario: $n = 4$ $x = 7$ (using d10s, where 0 = 10) $m = 8$ In words, if I roll 8d10 (eight 10-sided dice) what is the probability of having four dice greater than or equal to 7 (where 0 is the greateest [10]) I have seen a lot of sites which will do the calculation where $n = 1$, but I want to make $n$ a variable. I know the probability of a 'successful' role is 4/10. I know that I could write out all the possible combinations and count the ones that meet the criteria, but I'm sure this can be done with math. How would you calculate this?","I am trying to figure out how to do the math for something like this. The scenario: $n = 4$ $x = 7$ (using d10s, where 0 = 10) $m = 8$ In words, if I roll 8d10 (eight 10-sided dice) what is the probability of having four dice greater than or equal to 7 (where 0 is the greateest [10]) I have seen a lot of sites which will do the calculation where $n = 1$, but I want to make $n$ a variable. I know the probability of a 'successful' role is 4/10. I know that I could write out all the possible combinations and count the ones that meet the criteria, but I'm sure this can be done with math. How would you calculate this?",,['probability']
39,Quadratic variation of the sum,Quadratic variation of the sum,,Let $X_t$ and $Y_t$ be two independent martingales whose quadratic variations are given by $\langle X\rangle_t$ and $\langle Y\rangle_t$ respectively... If we define $Z_t=a X_t+ bY_t$ can we say anything about the quadratic variation of $Z_t$ ? $a$ and $b$ are two constants.,Let and be two independent martingales whose quadratic variations are given by and respectively... If we define can we say anything about the quadratic variation of ? and are two constants.,X_t Y_t \langle X\rangle_t \langle Y\rangle_t Z_t=a X_t+ bY_t Z_t a b,"['probability', 'stochastic-processes', 'quadratic-variation']"
40,Interesting probability question - husband and wife committee variation,Interesting probability question - husband and wife committee variation,,"Twenty husbands and wives (ten couples) are randomly divided into two groups. What is the probability that at exactly 4 wives are in the same group as their husbands? Attempt: There are $\binom{40}{2}=780$ numbers of posible committees by dividing the couples into two groups. The way I thought about this problem was to have a bag filled with two colored balls both numbered 1-10 and drawing them out one by one. If there are exactly 4 couples together in a group that means there must be 1 male female pair in a committee with a different partner, or with the ball example two different colored balls with different numbers. So the number of ways of having a committee with exactly 4 couples is the number of pairs $$(1,2), \dots (1,10), (2,3), \dots (2,10), (3,4), \dots (3,10), \dots (9,10) = \sum_{i=1}^9 i = 45$$ therefore the probability is $45/780.$ Can anyone confirm this answer? I think it is wrong and I also think there is a much more elegant solution.","Twenty husbands and wives (ten couples) are randomly divided into two groups. What is the probability that at exactly 4 wives are in the same group as their husbands? Attempt: There are $\binom{40}{2}=780$ numbers of posible committees by dividing the couples into two groups. The way I thought about this problem was to have a bag filled with two colored balls both numbered 1-10 and drawing them out one by one. If there are exactly 4 couples together in a group that means there must be 1 male female pair in a committee with a different partner, or with the ball example two different colored balls with different numbers. So the number of ways of having a committee with exactly 4 couples is the number of pairs $$(1,2), \dots (1,10), (2,3), \dots (2,10), (3,4), \dots (3,10), \dots (9,10) = \sum_{i=1}^9 i = 45$$ therefore the probability is $45/780.$ Can anyone confirm this answer? I think it is wrong and I also think there is a much more elegant solution.",,"['probability', 'combinatorics', 'probability-theory', 'permutations']"
41,Monkey typing on 29 letter keyboard.,Monkey typing on 29 letter keyboard.,,"This monkey is driving me a little crazy. I think he should get fired - it's not nice. Here is the information.  A monkey is typing on a 29 letter keyboard. He is writing a word that is 5 letters long. How many words can the monkey write? 29*29*29*29*29 What is the probability the monkey starts with the letter H? 1/29 What is the probability the monkey writes the word ""YASOV""? 1/29*29*29*29*29 What is the probability the word contains ""H"" once? I don't understand the last one. My best bet would have been 29*29*29*29*1/29^5. The mindset being that you can press every key (4 times), and then press H once. Or maybe even 29*29*29*29*29-29*29*29*29*28. Doing a baby calculation with a keyboard of ABC, and the monkey typing a word of two letters, containing the word ""A"" only once I get 4/9. I just scribbled every possibility and found the answer, so I assume: 2*2/3*3. I don't see the connection to the 5 in the answer. I also don't understand why 28 (or 2 for that matter), as 28 would mean 28 possibilities. Wouldn't that be the possibility of not getting H? Every possibility except H? The correct solution to the real answer is 28^4*5/29^5.","This monkey is driving me a little crazy. I think he should get fired - it's not nice. Here is the information.  A monkey is typing on a 29 letter keyboard. He is writing a word that is 5 letters long. How many words can the monkey write? 29*29*29*29*29 What is the probability the monkey starts with the letter H? 1/29 What is the probability the monkey writes the word ""YASOV""? 1/29*29*29*29*29 What is the probability the word contains ""H"" once? I don't understand the last one. My best bet would have been 29*29*29*29*1/29^5. The mindset being that you can press every key (4 times), and then press H once. Or maybe even 29*29*29*29*29-29*29*29*29*28. Doing a baby calculation with a keyboard of ABC, and the monkey typing a word of two letters, containing the word ""A"" only once I get 4/9. I just scribbled every possibility and found the answer, so I assume: 2*2/3*3. I don't see the connection to the 5 in the answer. I also don't understand why 28 (or 2 for that matter), as 28 would mean 28 possibilities. Wouldn't that be the possibility of not getting H? Every possibility except H? The correct solution to the real answer is 28^4*5/29^5.",,"['probability', 'combinatorics']"
42,"If X, Y, Z are independent random variables, then X + Y, Z are independent random variables. [duplicate]","If X, Y, Z are independent random variables, then X + Y, Z are independent random variables. [duplicate]",,"This question already has an answer here : $X$,$Y$,$Z$ mutually independent implies $X+Y$ independent of $Z$ (1 answer) Closed 8 years ago . I found the same question ( X,Y,Z are mutually independent random variables. Is X and Y+Z independent? here), but the answer uses characteristic functions and fourier inversion theorem, but this is exercise in chapter long before characteristic functions.","This question already has an answer here : $X$,$Y$,$Z$ mutually independent implies $X+Y$ independent of $Z$ (1 answer) Closed 8 years ago . I found the same question ( X,Y,Z are mutually independent random variables. Is X and Y+Z independent? here), but the answer uses characteristic functions and fourier inversion theorem, but this is exercise in chapter long before characteristic functions.",,"['probability', 'probability-theory', 'independence']"
43,What is the probability of selecting five of the winning balls and one of the supplementary balls?,What is the probability of selecting five of the winning balls and one of the supplementary balls?,,"So I'm just doing a bit of probability questions and wanted to make sure I got it right. I have $50$ balls numbered $1-50$, and we pick $6$ winning balls and $2$ supplementary without replacement. So the chance to get the $6$ winning balls would simply be: $$\frac{6}{50} \cdot \frac{5}{49} \cdot \frac{4}{48} \cdot \frac{3}{47} \cdot \frac{2}{46} \cdot \frac{1}{45} = \frac{1}{15890700}$$ and for $5$ winning balls it would be, same process as above: $$\frac{3}{1059380}$$ Now the part that confuses me is $5$ winning, $1$ supplementary. Would this be given by: $$\frac{6}{50} \cdot \frac{5}{49} \cdot \frac{4}{48} \cdot \frac{3}{47} \cdot \frac{2}{46} \cdot \frac{44}{45} \cdot \frac{2}{44} = \frac{1}{7945350}?$$ Can someone please check if I did this right, I have a sense that I did not, but not sure where I went wrong.","So I'm just doing a bit of probability questions and wanted to make sure I got it right. I have $50$ balls numbered $1-50$, and we pick $6$ winning balls and $2$ supplementary without replacement. So the chance to get the $6$ winning balls would simply be: $$\frac{6}{50} \cdot \frac{5}{49} \cdot \frac{4}{48} \cdot \frac{3}{47} \cdot \frac{2}{46} \cdot \frac{1}{45} = \frac{1}{15890700}$$ and for $5$ winning balls it would be, same process as above: $$\frac{3}{1059380}$$ Now the part that confuses me is $5$ winning, $1$ supplementary. Would this be given by: $$\frac{6}{50} \cdot \frac{5}{49} \cdot \frac{4}{48} \cdot \frac{3}{47} \cdot \frac{2}{46} \cdot \frac{44}{45} \cdot \frac{2}{44} = \frac{1}{7945350}?$$ Can someone please check if I did this right, I have a sense that I did not, but not sure where I went wrong.",,['probability']
44,"$3$ identical Dice are tossed simultaneously, The probability that all dice shows same number.","identical Dice are tossed simultaneously, The probability that all dice shows same number.",3,"If $3$ identical Dice are tossed simultaneously, The find probability that all dice shows same number. $\bf{My\; Try::}$ Let $A$ be the event in which upper face of all dice shows same number and $S$ be the sample space Now Here we have $3$ identical dice. So $x_{1}$ be the number of times in which dice shows number $1$ on upper face. Similarly $x_{2}$ be the number of times in which dice shows number $2$ on upper face. ........... ........... ........... $x_{6}$ be the number of times in which dice shows number $6$ on upper face. So here $x_{1}+x_{2}+x_{3}+x_{4}+x_{5}+x_{6} = 3,$ Where $x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}\geq 0$ So we get $\displaystyle n(S) = \binom{8}{2}=56$ and $n(A) = 6$ So required probability $\displaystyle P(A) = \frac{6}{56}$ although i have solve that question using that post How many are the possible outcomes from throwing $n$ (identical) dice but i did not understand what is the difference between identical dice and simple dice. and why answer can not be equal to $\displaystyle \frac{6}{256}$ bcz whether dice are identical or not total number of possible outcome will remain same.)(Its my assumption.) plz explain me, Thanks","If identical Dice are tossed simultaneously, The find probability that all dice shows same number. Let be the event in which upper face of all dice shows same number and be the sample space Now Here we have identical dice. So be the number of times in which dice shows number on upper face. Similarly be the number of times in which dice shows number on upper face. ........... ........... ........... be the number of times in which dice shows number on upper face. So here Where So we get and So required probability although i have solve that question using that post How many are the possible outcomes from throwing $n$ (identical) dice but i did not understand what is the difference between identical dice and simple dice. and why answer can not be equal to bcz whether dice are identical or not total number of possible outcome will remain same.)(Its my assumption.) plz explain me, Thanks","3 \bf{My\; Try::} A S 3 x_{1} 1 x_{2} 2 x_{6} 6 x_{1}+x_{2}+x_{3}+x_{4}+x_{5}+x_{6} = 3, x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}\geq 0 \displaystyle n(S) = \binom{8}{2}=56 n(A) = 6 \displaystyle P(A) = \frac{6}{56} \displaystyle \frac{6}{256}",['probability']
45,"Roll a die until, for the first time, the same side shows up two times in a row. Let $X$ be the number of rolls needed. compute $\mathbb{E}(X)$.","Roll a die until, for the first time, the same side shows up two times in a row. Let  be the number of rolls needed. compute .",X \mathbb{E}(X),"I'm having trouble with solving this problem: Roll a die until, for the first time, the same side shows up two times in a row. Let $X$ be the number of rolls needed. compute $\mathbb{E}(X)$. I know that the answer to this is $\mathbb{E}(X) = 7$, but I don't know why. Could you explain this to me? Thanks in advance!","I'm having trouble with solving this problem: Roll a die until, for the first time, the same side shows up two times in a row. Let $X$ be the number of rolls needed. compute $\mathbb{E}(X)$. I know that the answer to this is $\mathbb{E}(X) = 7$, but I don't know why. Could you explain this to me? Thanks in advance!",,"['probability', 'probability-theory', 'random-variables']"
46,Find the probability that the final score is 4 in a dice game with two throws,Find the probability that the final score is 4 in a dice game with two throws,,"A game uses an unbiased die with faces numbered 1 to 6. The die is thrown once. If it shows 4 or 5 or 6 then this number is the final score. If it shows 1 or 2 or 3 then die is thrown again and the final score is the sum of the numbers shown on both throws. i. Find the probability that the final score is 4. ii. Given the die is thrown only once, find the probability that the final score is 4. iii. Given the die is thrown twice, find the probability that the final score is 4. I have managed to solve part i and ii and would share the solution below. I am unable to work out part iii and would appreciate help. My solution: i. $P(4) + P(1,3) + P(2,2) + P(3,1) = \dfrac{1}{6} + \dfrac{1}{36} \cdot 3 = \dfrac{1}{4}$ ii. We know that the die is thrown only once and that can happen only when the score is 4, 5 or 6. The sample space is 3. So $P(4)$ now is $1/3$. iii. My guess: $P(1,3) + P(2,2) + P(3,1) = 1/12$ --> Apparently this answer is incorrect and I really can't work out why this is wrong and what the correct approach would be. Reference: OCR Jan 2009 Probability & Statistics 1 (4732)","A game uses an unbiased die with faces numbered 1 to 6. The die is thrown once. If it shows 4 or 5 or 6 then this number is the final score. If it shows 1 or 2 or 3 then die is thrown again and the final score is the sum of the numbers shown on both throws. i. Find the probability that the final score is 4. ii. Given the die is thrown only once, find the probability that the final score is 4. iii. Given the die is thrown twice, find the probability that the final score is 4. I have managed to solve part i and ii and would share the solution below. I am unable to work out part iii and would appreciate help. My solution: i. $P(4) + P(1,3) + P(2,2) + P(3,1) = \dfrac{1}{6} + \dfrac{1}{36} \cdot 3 = \dfrac{1}{4}$ ii. We know that the die is thrown only once and that can happen only when the score is 4, 5 or 6. The sample space is 3. So $P(4)$ now is $1/3$. iii. My guess: $P(1,3) + P(2,2) + P(3,1) = 1/12$ --> Apparently this answer is incorrect and I really can't work out why this is wrong and what the correct approach would be. Reference: OCR Jan 2009 Probability & Statistics 1 (4732)",,"['probability', 'dice']"
47,infinite monkey problem - probability of an infinite sequence containing an infinite sequence [duplicate],infinite monkey problem - probability of an infinite sequence containing an infinite sequence [duplicate],,"This question already has answers here : Given an infinite number of monkeys and an infinite amount of time, would one of them write Hamlet? (13 answers) Closed 9 years ago . Note: This question is specifically about when the infinite monkey theorem is extended to reproducing an infinite sequence (as oppose to a finite one) I was browsing wikipedia, and came across the infinite monkey problem. I understand that if Random keys are selected for an infinite period of time, eventually any finite combination of characters will eventually be produced. For example, a complete copy of Hamlet. However, I am confused about what happens when this is extended to infinity. According to wikipedia: If the monkey's allotted length of text is infinite, the chance of typing only the digits of pi is $0$, which is just as possible as typing nothing but Gs (also probability $0$). http://en.wikipedia.org/wiki/Infinite_monkey_theorem#Almost_surely However, this suggests that the probability of typing any combination of characters (that are infinitely long) is also $0$. However, the probability that a combination is typed is 1. However the sum of all possible events appears to be $0$, not $1$ (as the probability of any combination of infinite length being typed such as pi is $0$ as $0+0+0+0+...+0=0$ I'm not sure if I have just misunderstood this problem or if I have made a false assumption. It might have something to do with concept of ""Almost certainly"", but I was a bit intimidated by the wikipedia article.","This question already has answers here : Given an infinite number of monkeys and an infinite amount of time, would one of them write Hamlet? (13 answers) Closed 9 years ago . Note: This question is specifically about when the infinite monkey theorem is extended to reproducing an infinite sequence (as oppose to a finite one) I was browsing wikipedia, and came across the infinite monkey problem. I understand that if Random keys are selected for an infinite period of time, eventually any finite combination of characters will eventually be produced. For example, a complete copy of Hamlet. However, I am confused about what happens when this is extended to infinity. According to wikipedia: If the monkey's allotted length of text is infinite, the chance of typing only the digits of pi is $0$, which is just as possible as typing nothing but Gs (also probability $0$). http://en.wikipedia.org/wiki/Infinite_monkey_theorem#Almost_surely However, this suggests that the probability of typing any combination of characters (that are infinitely long) is also $0$. However, the probability that a combination is typed is 1. However the sum of all possible events appears to be $0$, not $1$ (as the probability of any combination of infinite length being typed such as pi is $0$ as $0+0+0+0+...+0=0$ I'm not sure if I have just misunderstood this problem or if I have made a false assumption. It might have something to do with concept of ""Almost certainly"", but I was a bit intimidated by the wikipedia article.",,"['probability', 'infinity']"
48,Standard deviation formula confusion,Standard deviation formula confusion,,"I'm having trouble understanding the formula for standard deviation . I know how to calculate standard deviation,  but I cant understand some parts of the formula . I'll give you an example  $$\sigma=\sqrt{\frac 1N \sum_{i=1}^N(x_i-\mu)^2}$$ Say we have a bunch of numbers like $9, 2, 5, 4, 15$ This part of the formula says subtract the Mean and square the result $(x_i-\mu)^2$ The mean is 7 and when I subtract and square I get 4,  25,  4, 9, 64 This is where I get stuck - I know I have to add up all the values then divide by how many $\displaystyle \frac 1N \sum_{i=1}^N$ but how does this part of the formula say that? I know the sigma means add up but what does the $N$ on top of sigma mean? what does the $i=1$ at the bottom of sigma mean ?","I'm having trouble understanding the formula for standard deviation . I know how to calculate standard deviation,  but I cant understand some parts of the formula . I'll give you an example  $$\sigma=\sqrt{\frac 1N \sum_{i=1}^N(x_i-\mu)^2}$$ Say we have a bunch of numbers like $9, 2, 5, 4, 15$ This part of the formula says subtract the Mean and square the result $(x_i-\mu)^2$ The mean is 7 and when I subtract and square I get 4,  25,  4, 9, 64 This is where I get stuck - I know I have to add up all the values then divide by how many $\displaystyle \frac 1N \sum_{i=1}^N$ but how does this part of the formula say that? I know the sigma means add up but what does the $N$ on top of sigma mean? what does the $i=1$ at the bottom of sigma mean ?",,"['probability', 'statistics']"
49,How to comprehend $E(X) = \int_0^\infty {P(X > x)dx} $ and $E(X) = \sum\limits_{n = 1}^\infty {P\{ X \ge n\} }$ for positive variable $X$?,How to comprehend  and  for positive variable ?,E(X) = \int_0^\infty {P(X > x)dx}  E(X) = \sum\limits_{n = 1}^\infty {P\{ X \ge n\} } X,"Suppose $X$ is an integrable, positive random variable. Then, if $X$ is arithmetic, we have $E(X) = \sum\limits_{n = 1}^\infty  {P\{ X \ge n\} }$ and if $X$ is continuous, we have $E(X) = \int_0^\infty  {P(X > x)dx} $. How to understand these two formulas intuitively?","Suppose $X$ is an integrable, positive random variable. Then, if $X$ is arithmetic, we have $E(X) = \sum\limits_{n = 1}^\infty  {P\{ X \ge n\} }$ and if $X$ is continuous, we have $E(X) = \int_0^\infty  {P(X > x)dx} $. How to understand these two formulas intuitively?",,"['probability', 'probability-theory', 'expectation']"
50,Combinatoric Solution To The Birthday Paradox,Combinatoric Solution To The Birthday Paradox,,"I attempted the following solution to the birthday ""paradox"" problem. It is not correct, but I'd like to know where I went wrong. Where $P(N)$ is the probability of any two people in a group of $N$ people having the same birthday, I consider the first few values. For two people, the probability that they share a birthday is simply $1/365$, not counting leap years. For three people, it is the probability of every combination of two of them ""ored"" together, which is simply the sum of the probabilities of every combination of two people. Thus, $$ P(2)=P(AB)=\frac{1}{365} $$ $$ P(3)=P(AB)+P(AC)+P(BC)=3\times P(2) $$ $$ P(4)=P(AB)+P(AC)+P(AD)+P(BC)+P(BD)+P(CD)=6\times P(2) $$ Where $P(XY)$ is used to denote the probability of persons $X$ and $Y$ sharing a birthday. You can see pretty clearly that the coefficients are binomial. $$ P(N)=\binom{N}{2}\times P(2)=\frac{N!}{2!(N-2)!}\cdot\frac{1}{365}=\frac{N(N-1)}{730} $$ Now according to the pidgeonhole principal, we should have $P(366)=1$, which this expression clearly violates (instead it gives $P(366)=183$). So clearly I'm doing something wrong.","I attempted the following solution to the birthday ""paradox"" problem. It is not correct, but I'd like to know where I went wrong. Where $P(N)$ is the probability of any two people in a group of $N$ people having the same birthday, I consider the first few values. For two people, the probability that they share a birthday is simply $1/365$, not counting leap years. For three people, it is the probability of every combination of two of them ""ored"" together, which is simply the sum of the probabilities of every combination of two people. Thus, $$ P(2)=P(AB)=\frac{1}{365} $$ $$ P(3)=P(AB)+P(AC)+P(BC)=3\times P(2) $$ $$ P(4)=P(AB)+P(AC)+P(AD)+P(BC)+P(BD)+P(CD)=6\times P(2) $$ Where $P(XY)$ is used to denote the probability of persons $X$ and $Y$ sharing a birthday. You can see pretty clearly that the coefficients are binomial. $$ P(N)=\binom{N}{2}\times P(2)=\frac{N!}{2!(N-2)!}\cdot\frac{1}{365}=\frac{N(N-1)}{730} $$ Now according to the pidgeonhole principal, we should have $P(366)=1$, which this expression clearly violates (instead it gives $P(366)=183$). So clearly I'm doing something wrong.",,"['probability', 'combinatorics']"
51,Card Matching: expected value of correctly predicted cards with partial feedback,Card Matching: expected value of correctly predicted cards with partial feedback,,"A standard deck of cards is shuffled, and the cards are dealt face down one by one. Just after each card is dealt, you name any card (as your prediction). Let $X$ be the number of cards you predict correctly. (b) Now suppose that you get partial feedback: after each prediction, you are told immediately whether or not it is right (but without the card being revealed). Suppose you use the following strategy: keep saying a specific card’s name until you hear that you are correct. Then keep saying a different card’s name until you hear that you are correct (if ever). Continue in this way, naming the same card over and over again until you are correct and then switching to a new card, until the deck runs out. Find the expected value of $X$ , and show that it is very close to $e − 1$ . Can anybody give me a hint what the indicator random variable should be? If I let it refer to the $j^{th}$ card in the deck, it's too complicated and I don't know how to compute its expected value. If I let it refer to the $j^{th}$ card that I say, I don't know if I will ever say it.","A standard deck of cards is shuffled, and the cards are dealt face down one by one. Just after each card is dealt, you name any card (as your prediction). Let be the number of cards you predict correctly. (b) Now suppose that you get partial feedback: after each prediction, you are told immediately whether or not it is right (but without the card being revealed). Suppose you use the following strategy: keep saying a specific card’s name until you hear that you are correct. Then keep saying a different card’s name until you hear that you are correct (if ever). Continue in this way, naming the same card over and over again until you are correct and then switching to a new card, until the deck runs out. Find the expected value of , and show that it is very close to . Can anybody give me a hint what the indicator random variable should be? If I let it refer to the card in the deck, it's too complicated and I don't know how to compute its expected value. If I let it refer to the card that I say, I don't know if I will ever say it.",X X e − 1 j^{th} j^{th},"['probability', 'card-games']"
52,"What is the expected value of $\min\{|X|,|Y|\}/\max\{|X|,|Y|\}$ assuming $X$ and $Y$ are independent?",What is the expected value of  assuming  and  are independent?,"\min\{|X|,|Y|\}/\max\{|X|,|Y|\} X Y","So I need to compute $$E\left[\frac{\min\{|X|,|Y|\}}{\max\{|X|,|Y|\}}\right]$$ given $X,Y \sim$ Normal$(0,1)$ and independent. What I am having trouble seeing is whether $\min\{|X|,|Y|\}$ and $\max\{|X|,|Y|\}$ are independent. If so I can factor the expectation into $$E\left[\min\{|X|,|Y|\}\right]\cdot E\left[\frac{1}{\max\{|X|,|Y|\}}\right]$$ how I should find the expected value of $1/\max\{|X|,|Y|\}$.","So I need to compute $$E\left[\frac{\min\{|X|,|Y|\}}{\max\{|X|,|Y|\}}\right]$$ given $X,Y \sim$ Normal$(0,1)$ and independent. What I am having trouble seeing is whether $\min\{|X|,|Y|\}$ and $\max\{|X|,|Y|\}$ are independent. If so I can factor the expectation into $$E\left[\min\{|X|,|Y|\}\right]\cdot E\left[\frac{1}{\max\{|X|,|Y|\}}\right]$$ how I should find the expected value of $1/\max\{|X|,|Y|\}$.",,"['probability', 'statistics', 'probability-distributions']"
53,Probability: $n$ balls into $n$ holes with exactly one hole remaining empty [duplicate],Probability:  balls into  holes with exactly one hole remaining empty [duplicate],n n,"This question already has answers here : What is the correct way to think about this yet another balls/boxes problem? (4 answers) Closed 9 years ago . The question is: n balls are distributed into n holes at random. What is the probability that exactly one hole remains empty. I came up with $$P\left(A\right)=\frac{\:\dbinom{n\:}{1}\:\dbinom{n\:-1}{1}\:\left(n-2\right)!}{n^n}$$ but was told I’m way way off. Yes, I do realize I’m awful at probability. Thanks for the help.","This question already has answers here : What is the correct way to think about this yet another balls/boxes problem? (4 answers) Closed 9 years ago . The question is: n balls are distributed into n holes at random. What is the probability that exactly one hole remains empty. I came up with $$P\left(A\right)=\frac{\:\dbinom{n\:}{1}\:\dbinom{n\:-1}{1}\:\left(n-2\right)!}{n^n}$$ but was told I’m way way off. Yes, I do realize I’m awful at probability. Thanks for the help.",,['probability']
54,Probability problem in networking.,Probability problem in networking.,,"Consider the following statement from Computer Networking A Top-Down Approach textbook, With packet switching, the probability that a user is active is $0.1$. If there are $35$ users, the probability that there are $11$ or more simultaneously active users is approximately $0.0004$. $P(11$ or more$) = P(11) + P(12) + \dots + P(35)\\ =1-(P(1) + P(2) + \dots + P(10)) = 1.1111111111111119\times 10^{-11$}$ Something is wrong with my thinking, can someone point it out?","Consider the following statement from Computer Networking A Top-Down Approach textbook, With packet switching, the probability that a user is active is $0.1$. If there are $35$ users, the probability that there are $11$ or more simultaneously active users is approximately $0.0004$. $P(11$ or more$) = P(11) + P(12) + \dots + P(35)\\ =1-(P(1) + P(2) + \dots + P(10)) = 1.1111111111111119\times 10^{-11$}$ Something is wrong with my thinking, can someone point it out?",,[]
55,A year having more than one Friday the 13th?,A year having more than one Friday the 13th?,,"We know that every year has at least one Friday the 13th . What about two Friday the 13ths, in a year? What is the probability of a year having two Friday the 13ths? (interesting subquestion here is if this probability is the same for leap and non-leap years) Is it possible that a year has three Friday the 13ths? (after posting the question I discovered that it is, as said in comments) This web page on wolfram MathWorld claims that a year has 1.72 Friday the 13ths on average. It also gives this table:","We know that every year has at least one Friday the 13th . What about two Friday the 13ths, in a year? What is the probability of a year having two Friday the 13ths? (interesting subquestion here is if this probability is the same for leap and non-leap years) Is it possible that a year has three Friday the 13ths? (after posting the question I discovered that it is, as said in comments) This web page on wolfram MathWorld claims that a year has 1.72 Friday the 13ths on average. It also gives this table:",,"['probability', 'calendar-computations']"
56,Moment Generating Function of Gaussian Distribution,Moment Generating Function of Gaussian Distribution,,"Derive from first principles, the moment generating function of a Gaussian Distribution with $$PDF= \dfrac{1}{\sqrt{2\pi \sigma^2}}e^{-(x- \mu)^2/2\sigma^2}$$ MY ATTEMPT MGF= E[$e^{tx}$]= $$\int_{-\infty}^{\infty} e^{tx} \dfrac{1}{\sqrt{2\pi \sigma^2}}e^{-(x- \mu)^2/2\sigma^2}dx$$ Now to perform a substitution; z= $\dfrac{(x- \mu)}{\sigma}$ $x=z\sigma + \mu$ $dx=\sigma \ \ du$ $$\int_{-\infty}^{\infty} e^{tz\sigma}e^{t\mu} \dfrac{1}{\sqrt{2\pi }}e^{-z^2/2}dz$$ which can be rewritten as ; $$e^{t\mu}\int_{-\infty}^{\infty} e^{tz\sigma} \dfrac{1}{\sqrt{2\pi }}e^{-z^2/2}dz \ \ \ \ \ \ (*)$$ I have also noted that for the standard gaussian distribution the moment generating function is as follows; MGF=E[$e^{tx}$]=$$\int_{-\infty}^{\infty} e^{tx} \dfrac{1}{\sqrt{2\pi }}e^{-x^2/2}dx=e^{t^2/2}$$ Now what Im having trouble with is combining these two facts..... I know the CORRECT ANSWER I SHOULD GET ; $$MGF \ \ = \ \ e^{\mu t}e^{\sigma^2 t^2 /2}$$ Now I can rewrite (*) as ; $$e^{t\mu}e^{ \sigma}\int_{-\infty}^{\infty} e^{tz} \dfrac{1}{\sqrt{2\pi }}e^{-z^2/2}dz$$ and this equals (by comparison to standard gaussian); $$e^{t\mu}e^{ \sigma}e^{t^2/2}$$ but it seems ive lost a $\sigma $ somewhere?!","Derive from first principles, the moment generating function of a Gaussian Distribution with $$PDF= \dfrac{1}{\sqrt{2\pi \sigma^2}}e^{-(x- \mu)^2/2\sigma^2}$$ MY ATTEMPT MGF= E[$e^{tx}$]= $$\int_{-\infty}^{\infty} e^{tx} \dfrac{1}{\sqrt{2\pi \sigma^2}}e^{-(x- \mu)^2/2\sigma^2}dx$$ Now to perform a substitution; z= $\dfrac{(x- \mu)}{\sigma}$ $x=z\sigma + \mu$ $dx=\sigma \ \ du$ $$\int_{-\infty}^{\infty} e^{tz\sigma}e^{t\mu} \dfrac{1}{\sqrt{2\pi }}e^{-z^2/2}dz$$ which can be rewritten as ; $$e^{t\mu}\int_{-\infty}^{\infty} e^{tz\sigma} \dfrac{1}{\sqrt{2\pi }}e^{-z^2/2}dz \ \ \ \ \ \ (*)$$ I have also noted that for the standard gaussian distribution the moment generating function is as follows; MGF=E[$e^{tx}$]=$$\int_{-\infty}^{\infty} e^{tx} \dfrac{1}{\sqrt{2\pi }}e^{-x^2/2}dx=e^{t^2/2}$$ Now what Im having trouble with is combining these two facts..... I know the CORRECT ANSWER I SHOULD GET ; $$MGF \ \ = \ \ e^{\mu t}e^{\sigma^2 t^2 /2}$$ Now I can rewrite (*) as ; $$e^{t\mu}e^{ \sigma}\int_{-\infty}^{\infty} e^{tz} \dfrac{1}{\sqrt{2\pi }}e^{-z^2/2}dz$$ and this equals (by comparison to standard gaussian); $$e^{t\mu}e^{ \sigma}e^{t^2/2}$$ but it seems ive lost a $\sigma $ somewhere?!",,"['probability', 'probability-distributions', 'normal-distribution', 'moment-generating-functions']"
57,Probability of even number of successes in a series of independent trials,Probability of even number of successes in a series of independent trials,,Consider a series of independent trials at each of which there is a success of a failure with probabilities $p$ and $1-p$ respectively. I am finding it difficult to derive the probability of even number of successes occurring at the nth trial. Any assistance will be much appreciated. Thanks,Consider a series of independent trials at each of which there is a success of a failure with probabilities $p$ and $1-p$ respectively. I am finding it difficult to derive the probability of even number of successes occurring at the nth trial. Any assistance will be much appreciated. Thanks,,"['probability', 'generating-functions']"
58,A list of different measures of distance/difference/dissimilarities/similarity of two probability distributions? [duplicate],A list of different measures of distance/difference/dissimilarities/similarity of two probability distributions? [duplicate],,"This question already has answers here : Prokhorov metric vs. total variation norm (3 answers) Closed 6 years ago . I wanted to know more about the different methods for comparing the similarities of two probability distributions P and Q. I wanted a list of the different methods that exist for comparing probability distributions. For example, the two that I am aware that exist are: KL-Divergence EMD (earths mover's distance) I was wondering if people knew about more different measures and if they could provide maybe a good reference for learning about it. Also, on top of providing the distance measure that you are suggesting, if you could provide a brief intuition on it before I research it more, it could be very helpful!","This question already has answers here : Prokhorov metric vs. total variation norm (3 answers) Closed 6 years ago . I wanted to know more about the different methods for comparing the similarities of two probability distributions P and Q. I wanted a list of the different methods that exist for comparing probability distributions. For example, the two that I am aware that exist are: KL-Divergence EMD (earths mover's distance) I was wondering if people knew about more different measures and if they could provide maybe a good reference for learning about it. Also, on top of providing the distance measure that you are suggesting, if you could provide a brief intuition on it before I research it more, it could be very helpful!",,"['probability', 'probability-theory', 'reference-request']"
59,What are the exact odds of getting a perfect NCAA bracket?,What are the exact odds of getting a perfect NCAA bracket?,,"With the NCAA March Madness Finals nearing, I thought it'd be appropriate to ask this. From everything that I've read and heard online, there seems to be varying opinions on the exact odds of getting a perfect NCAA bracket, especially from different sources.  That seems certainly strange, because I had originally thought there'd be only one way to calculate the odds of that. For example, this link seems to suggest the odds are 9.2 quintillion. Which seems odd that, compared with this , suggesting it's around 4 quadrillion.  Which leads me to being somewhat confused as to the exact number for the odds. What I'm far more interested is how do I calculate the chances of getting a perfect bracket? With 68 possible winners, I had originally thought it'd be a simple 68! , but I thought since there were a total possible of 68 slots, it could be 68 ^ 68 ?  Is my thinking off, or am I somewhere in the right court?","With the NCAA March Madness Finals nearing, I thought it'd be appropriate to ask this. From everything that I've read and heard online, there seems to be varying opinions on the exact odds of getting a perfect NCAA bracket, especially from different sources.  That seems certainly strange, because I had originally thought there'd be only one way to calculate the odds of that. For example, this link seems to suggest the odds are 9.2 quintillion. Which seems odd that, compared with this , suggesting it's around 4 quadrillion.  Which leads me to being somewhat confused as to the exact number for the odds. What I'm far more interested is how do I calculate the chances of getting a perfect bracket? With 68 possible winners, I had originally thought it'd be a simple 68! , but I thought since there were a total possible of 68 slots, it could be 68 ^ 68 ?  Is my thinking off, or am I somewhere in the right court?",,['probability']
60,Convolution of continuous and discrete distributions,Convolution of continuous and discrete distributions,,"Assume we have two random variables $X$ and $Y$, such that $X \sim P(x)$ and $Y \sim G(y)$. We ask, what is the distribution of $Z = X+Y$. If both of the distributions of $X$ and $Y$ are discrete, the distribution of $Z$ is given by the convolution of the two, ie. $$F(z) = \sum_{x \in \mathbb Z} P(x) G(z-x).$$ Similarly, if the distributions are continuous, the convolution is $$F(z) = \int_{-\infty}^{\infty} P(x) G(z-x) dx.$$ Now, what if other, say $P(x)$ is discrete and the other continuous? Take as an example probably the simplest one, $X \sim \text{Bernoulli}(1/2;x)$ and $Y \sim U(0,1)$. If one thinks of this, $Z$ has probability $1/2$ of being between zero and 1 and same probability of being between one and two. Thus one could say immediately that $Z \sim U(0,2).$ The way I have seen this done elsewhere involves the use of a distribution called Dirac delta function $\delta(x)$, which is zero everywhere else except at zero but the integral over all reals is $1$. Using this, we can make $\text{Bernoulli}(1/2;x)$ a continuous distribution, namely $$\text{DeltaBernoulli(1/2;x)} \sim \text{Bernoulli}(1/2;x)\delta(x)+\text{Bernoulli}(1/2;x)\delta(x-1).$$ (Subquestion: is this convolution of the two distributions or something else?) Using this, lets perform the convolution $$M(z) = \int_{-\infty}^{\infty} 1*\text{PDF}(\text{DeltaBernoulli}(1/2;z-x)) dx = \begin{cases}    1/2 & 0\le z< 1 \vee 1 < z \le 2 \\    1       &z = 1 \\    0   &\text{elsewhere}   \end{cases}  $$ This is almost $U(0,2)$ and in any case has no real differences to it. This method seems a tad clumsy, especially if one would want to convolute, say, normal distribution and Poisson distribution. Are there other ways to do this?","Assume we have two random variables $X$ and $Y$, such that $X \sim P(x)$ and $Y \sim G(y)$. We ask, what is the distribution of $Z = X+Y$. If both of the distributions of $X$ and $Y$ are discrete, the distribution of $Z$ is given by the convolution of the two, ie. $$F(z) = \sum_{x \in \mathbb Z} P(x) G(z-x).$$ Similarly, if the distributions are continuous, the convolution is $$F(z) = \int_{-\infty}^{\infty} P(x) G(z-x) dx.$$ Now, what if other, say $P(x)$ is discrete and the other continuous? Take as an example probably the simplest one, $X \sim \text{Bernoulli}(1/2;x)$ and $Y \sim U(0,1)$. If one thinks of this, $Z$ has probability $1/2$ of being between zero and 1 and same probability of being between one and two. Thus one could say immediately that $Z \sim U(0,2).$ The way I have seen this done elsewhere involves the use of a distribution called Dirac delta function $\delta(x)$, which is zero everywhere else except at zero but the integral over all reals is $1$. Using this, we can make $\text{Bernoulli}(1/2;x)$ a continuous distribution, namely $$\text{DeltaBernoulli(1/2;x)} \sim \text{Bernoulli}(1/2;x)\delta(x)+\text{Bernoulli}(1/2;x)\delta(x-1).$$ (Subquestion: is this convolution of the two distributions or something else?) Using this, lets perform the convolution $$M(z) = \int_{-\infty}^{\infty} 1*\text{PDF}(\text{DeltaBernoulli}(1/2;z-x)) dx = \begin{cases}    1/2 & 0\le z< 1 \vee 1 < z \le 2 \\    1       &z = 1 \\    0   &\text{elsewhere}   \end{cases}  $$ This is almost $U(0,2)$ and in any case has no real differences to it. This method seems a tad clumsy, especially if one would want to convolute, say, normal distribution and Poisson distribution. Are there other ways to do this?",,"['probability', 'probability-distributions']"
61,"Probability of winning the game ""1-2-3""","Probability of winning the game ""1-2-3""",,"Ok, game is as follow, with spanish cards (you can do it with poker cards using the As as a 1) You shuffle, put the deck face bottom, and start turning the cards one by one, saying a number each time you turn a card around ---> 1, 2, 3; 1, 2, 3; etc. If when you say 1 a 1 comes out, you lose, same with 2 and 3. If you finish the deck without losing, you win. I know some basics of probabilities, but is there a way to calculate the probability of winning the game, given a random shuffled deck?","Ok, game is as follow, with spanish cards (you can do it with poker cards using the As as a 1) You shuffle, put the deck face bottom, and start turning the cards one by one, saying a number each time you turn a card around ---> 1, 2, 3; 1, 2, 3; etc. If when you say 1 a 1 comes out, you lose, same with 2 and 3. If you finish the deck without losing, you win. I know some basics of probabilities, but is there a way to calculate the probability of winning the game, given a random shuffled deck?",,"['probability', 'card-games']"
62,covariance of integral of Brownian,covariance of integral of Brownian,,"What is the covariance of the process $X(t) = \int_0^t B(u)\,du$ where $B$ is a standard Brownian motion? i.e., I wish to find $E[X(t)X(s)]$, for $0<s<t<\infty$. Any ideas? Thanks you very much for your help!","What is the covariance of the process $X(t) = \int_0^t B(u)\,du$ where $B$ is a standard Brownian motion? i.e., I wish to find $E[X(t)X(s)]$, for $0<s<t<\infty$. Any ideas? Thanks you very much for your help!",,"['probability', 'stochastic-processes', 'stochastic-calculus']"
63,Confusion with event notation and usage in probability,Confusion with event notation and usage in probability,,"In $P(E_1, E_2)$, the comma is read as an and . However, consider a random variable with $\Omega=$ {1,2,3,4,5,6} and $E_1$={1,2,3}, $E_2=${3,4,5,6} Am I right in assuming that a comma in the event and sample space definitions means xor ? Therefore, can you conclude that $P(E_1)+P(E_2)=1$? EDIT: Would $P(E_1)=P(1,2,3)$ would mean 'probability of 1 xor 2 xor 3' or  'probability of 1 and 2 and 3'. The latter would obviously be 0 since mutually exclusive events can't be realized simultaneously.","In $P(E_1, E_2)$, the comma is read as an and . However, consider a random variable with $\Omega=$ {1,2,3,4,5,6} and $E_1$={1,2,3}, $E_2=${3,4,5,6} Am I right in assuming that a comma in the event and sample space definitions means xor ? Therefore, can you conclude that $P(E_1)+P(E_2)=1$? EDIT: Would $P(E_1)=P(1,2,3)$ would mean 'probability of 1 xor 2 xor 3' or  'probability of 1 and 2 and 3'. The latter would obviously be 0 since mutually exclusive events can't be realized simultaneously.",,['probability']
64,Exponential Distribution - memoryless,Exponential Distribution - memoryless,,"A post office has 2 clerks. Alice enters the post office while 2 other customers, Bob and Claire, are being served by the 2 clerks. She is next in line. Assume that the time a clerk spends serving a customer has the Exponential(x) distribution. (a) What is the probability that Alice is the last of the 3 customers to be done being served Hint: no integrals are needed. (b) What is the expected total time that Alice needs to spend at the post office? I understand that no matter who leaves the office first, the remainer has the same expo(x) like Alice, but don't have intuitive thinking why the answer for the 1st is 1/2.","A post office has 2 clerks. Alice enters the post office while 2 other customers, Bob and Claire, are being served by the 2 clerks. She is next in line. Assume that the time a clerk spends serving a customer has the Exponential(x) distribution. (a) What is the probability that Alice is the last of the 3 customers to be done being served Hint: no integrals are needed. (b) What is the expected total time that Alice needs to spend at the post office? I understand that no matter who leaves the office first, the remainer has the same expo(x) like Alice, but don't have intuitive thinking why the answer for the 1st is 1/2.",,['probability']
65,an exercise book for probability theory recommendation request,an exercise book for probability theory recommendation request,,"I'm looking for a good exercise book for probability theory, preferably at least partially with solutions to it. I want it to be detailed, not trivial, providing me solid fundamentals in the topic to be developed in the future. I'd wish it to be more of a ""applied"" technical university approach than the highly ""abstract"" one tailored for pure mathematics student at a university, however it need not to be so. The topics I would like it to cover are more or less like in the part one of the book ""Probability, Random Variables and Stochastic Processes"" by Papoulis and Pillai ( table of contents ). Thank you in advance.","I'm looking for a good exercise book for probability theory, preferably at least partially with solutions to it. I want it to be detailed, not trivial, providing me solid fundamentals in the topic to be developed in the future. I'd wish it to be more of a ""applied"" technical university approach than the highly ""abstract"" one tailored for pure mathematics student at a university, however it need not to be so. The topics I would like it to cover are more or less like in the part one of the book ""Probability, Random Variables and Stochastic Processes"" by Papoulis and Pillai ( table of contents ). Thank you in advance.",,"['probability', 'reference-request']"
66,Using Chebyshev's inequality to obtain lower bounds,Using Chebyshev's inequality to obtain lower bounds,,"I need help with a question I found in Master Stats. I'm unaware of Chebyshev's inequality hence I can't do this question, can anyone help. Q) A company produces planks whose length is a random variable of mean 2.5m and standard deviation 0.1m. Use Chebyshev's inequality to obtain a lower bound on the probability that the length of planks does not differ more than 0.5m from the mean length. Thanks in advance","I need help with a question I found in Master Stats. I'm unaware of Chebyshev's inequality hence I can't do this question, can anyone help. Q) A company produces planks whose length is a random variable of mean 2.5m and standard deviation 0.1m. Use Chebyshev's inequality to obtain a lower bound on the probability that the length of planks does not differ more than 0.5m from the mean length. Thanks in advance",,['probability']
67,Question about the independence definition.,Question about the independence definition.,,"Why does the independence definition requires that every subfamily of events $A_1,A_2,\ldots,A_n$ satisfies $P(A_{i1}\cap \cdots \cap A_{ik})=\prod_j P(A_{ij})$ where $i_1 < i_2 < \cdots < i_n$ and $j < n$. My doubt arose from this: Suppose $A_1,A_2$ and $A_3$ such as $P(A_1\cap A_2\cap A_3)=P(A_1)P(A_2)P(A_3)$. Then $$P(A_1\cap A_2)=P(A_1\cap A_2 \cap A_3) + P(A_1\cap A_2 \cap A_3^c)$$ $$=P(A_1)P(A_2)(P(A_3)+P(A_3^c))=P(A_1)P(A_2).$$ So it seems to me that if $P(A_1\cap A_2\cap A_3)=P(A_1)P(A_2)P(A_3)$ then $P(A_i\cap A_j)=P(A_i)P(A_j)$, i.e., the biggest collection independence implies the smaller ones. Why am I wrong? The calculations seems right to me, maybe my conclusion from it are wrong?","Why does the independence definition requires that every subfamily of events $A_1,A_2,\ldots,A_n$ satisfies $P(A_{i1}\cap \cdots \cap A_{ik})=\prod_j P(A_{ij})$ where $i_1 < i_2 < \cdots < i_n$ and $j < n$. My doubt arose from this: Suppose $A_1,A_2$ and $A_3$ such as $P(A_1\cap A_2\cap A_3)=P(A_1)P(A_2)P(A_3)$. Then $$P(A_1\cap A_2)=P(A_1\cap A_2 \cap A_3) + P(A_1\cap A_2 \cap A_3^c)$$ $$=P(A_1)P(A_2)(P(A_3)+P(A_3^c))=P(A_1)P(A_2).$$ So it seems to me that if $P(A_1\cap A_2\cap A_3)=P(A_1)P(A_2)P(A_3)$ then $P(A_i\cap A_j)=P(A_i)P(A_j)$, i.e., the biggest collection independence implies the smaller ones. Why am I wrong? The calculations seems right to me, maybe my conclusion from it are wrong?",,['probability']
68,Probability that two cards are black and one is red,Probability that two cards are black and one is red,,"If I draw three cards at random (without replacement) from a standard 52-card deck, what is the probability that two of the cards will be black and one of them will be red? Thanks!","If I draw three cards at random (without replacement) from a standard 52-card deck, what is the probability that two of the cards will be black and one of them will be red? Thanks!",,['probability']
69,Probability Term for something that defies the odds.,Probability Term for something that defies the odds.,,"I'm not a mathematician; I just wandered over here from Writers SE and am hoping you guys can help. I'm writing a novel in which the theme is characters beating the odds. (It's a future dystopia, the government is super-powerful, etc. - the characters aren't doing any actual math). I'm looking for a term for a character who beats the odds. (I'm looking for gambling terms AND probability terms). So, going in, probability said that x was very likely to happen. But then y happened, instead.  Is there a term for y, after the event is over? (look at me trying to be all math-y - hopefully someone can edit this to be more coherent). In gambling terms, I'm looking for a word that would mean the dark horse or underdog AFTER he's won. Maybe math doesn't personalize things so much, and there just isn't a term, but if there is one, I'd love to know about it! Thanks!","I'm not a mathematician; I just wandered over here from Writers SE and am hoping you guys can help. I'm writing a novel in which the theme is characters beating the odds. (It's a future dystopia, the government is super-powerful, etc. - the characters aren't doing any actual math). I'm looking for a term for a character who beats the odds. (I'm looking for gambling terms AND probability terms). So, going in, probability said that x was very likely to happen. But then y happened, instead.  Is there a term for y, after the event is over? (look at me trying to be all math-y - hopefully someone can edit this to be more coherent). In gambling terms, I'm looking for a word that would mean the dark horse or underdog AFTER he's won. Maybe math doesn't personalize things so much, and there just isn't a term, but if there is one, I'd love to know about it! Thanks!",,"['probability', 'terminology']"
70,Poisson Processes-Waiting Times,Poisson Processes-Waiting Times,,"Phone calls arrive at a telephone exchange at an average rate of $1$ hit per minute ($\lambda=1$). Let $T_4$ be the time in minutes when the fourth hit occurs after the counter is switched on. Find $\mathbb{P}(3\le T_4\le 5)$. So to do this we calculate $\mathbb{P}(T_4\le 5)-\mathbb{P}(T_4<3)$. I calculate $\mathbb{P}(T_4\le 5)=1-\mathrm{e}^{-5(118/3)}$, using the Poisson process $1-\mathbb{P}(\mathrm{Pois}(5)\le 3)$. But the problem is I'm not sure how to compute $\mathbb{P}(T_4<3)$, namely that the fourth arrives in less than 3 minutes. I know that it will be another Poisson with mean $3$, but I'm confused on the sign of the inequality and number of terms. Thanks!!","Phone calls arrive at a telephone exchange at an average rate of $1$ hit per minute ($\lambda=1$). Let $T_4$ be the time in minutes when the fourth hit occurs after the counter is switched on. Find $\mathbb{P}(3\le T_4\le 5)$. So to do this we calculate $\mathbb{P}(T_4\le 5)-\mathbb{P}(T_4<3)$. I calculate $\mathbb{P}(T_4\le 5)=1-\mathrm{e}^{-5(118/3)}$, using the Poisson process $1-\mathbb{P}(\mathrm{Pois}(5)\le 3)$. But the problem is I'm not sure how to compute $\mathbb{P}(T_4<3)$, namely that the fourth arrives in less than 3 minutes. I know that it will be another Poisson with mean $3$, but I'm confused on the sign of the inequality and number of terms. Thanks!!",,"['probability', 'statistics', 'probability-distributions']"
71,How to obtain the Standard Deviation of a ratio of independent binomial random variables?,How to obtain the Standard Deviation of a ratio of independent binomial random variables?,,"X and Y are 2 independent binomial random variables with parameters (n,p) and (m,q) respectively. (trials, probability parameter)","X and Y are 2 independent binomial random variables with parameters (n,p) and (m,q) respectively. (trials, probability parameter)",,"['probability', 'probability-theory', 'probability-distributions']"
72,An alternative solution to a probability problem,An alternative solution to a probability problem,,"The text states: There are 3 seniors and 15 juniors in Mrs. Gillis’s math class. Three   students are chosen at random from the class. What is the probability   that the group consists of a senior and two juniors? I answered this correctly by using Combinations to calculate the number of elements in the sample space (816) and the number of possible combinations of one senior and two juniors (315), arriving at a probability of 38.6%, which is the answer the text gives. However it seems like another way to answer the question is to calculate the probability of selecting 1 senior out of 3 (3/18) and multiply by the probability of picking 1 junior out of the 17 remaining students (15/17) and multiply that by the probability of choosing another junior out of the remaining 16 students (14/16). Yet this results in a different answer: 12.86% What have I missed?","The text states: There are 3 seniors and 15 juniors in Mrs. Gillis’s math class. Three   students are chosen at random from the class. What is the probability   that the group consists of a senior and two juniors? I answered this correctly by using Combinations to calculate the number of elements in the sample space (816) and the number of possible combinations of one senior and two juniors (315), arriving at a probability of 38.6%, which is the answer the text gives. However it seems like another way to answer the question is to calculate the probability of selecting 1 senior out of 3 (3/18) and multiply by the probability of picking 1 junior out of the 17 remaining students (15/17) and multiply that by the probability of choosing another junior out of the remaining 16 students (14/16). Yet this results in a different answer: 12.86% What have I missed?",,['probability']
73,what is wrong with this reasoning?,what is wrong with this reasoning?,,"14% of people are left handed, if we want to find the chances of a brother and a sister both being left handed, we might be tempted to multiply .14 by .14. What is wrong with this reasoning when trying to find out the probability of both of them being left handed?","14% of people are left handed, if we want to find the chances of a brother and a sister both being left handed, we might be tempted to multiply .14 by .14. What is wrong with this reasoning when trying to find out the probability of both of them being left handed?",,[]
74,2 slightly different situations in which 2 coins are tossed. Does the knowledge of an observer effect the probabilities of the outcomes?,2 slightly different situations in which 2 coins are tossed. Does the knowledge of an observer effect the probabilities of the outcomes?,,"Situation A: Once only, I toss 2 identical fair coins and don't look at the outcomes. A truthful observer looks at one of the coins and tells me that at least one of the coins is a head. Situation B: Once only, I toss 2 identical fair coins and don't look at the outcomes. A truthful observer looks at both of the coins and tells me that at least one of the coins is a head. In A, what is the probability that there are 2 heads? In B, what is the probability that there are 2 heads? Aren't both probabilities 1/2? EDIT Let me refine the question. The agreement I have with the observer is this: 1) I will toss 2 identical coins. 2) In situation A a third party will cover the coins with a cloth. The observer will look only at the outcome of the coin that comes to rest nearest him and report it to me. What is the probability that the second outcome will be the same as the first? 3) In situation B the observer will look at both outcomes and report only the state (heads or tails) of the coin that came to rest nearest him. What is the probability that the second outcome will be the same as the first? 2nd EDIT Changing 3) above to: 3) In situation B the observer will look at both outcomes, choose one of them, and truthfully report, ""There is at least one heads"", or ""There is at least one tails"", whichever is the case. What is the probability that the second outcome (that of the other coin) will be the same as the first?","Situation A: Once only, I toss 2 identical fair coins and don't look at the outcomes. A truthful observer looks at one of the coins and tells me that at least one of the coins is a head. Situation B: Once only, I toss 2 identical fair coins and don't look at the outcomes. A truthful observer looks at both of the coins and tells me that at least one of the coins is a head. In A, what is the probability that there are 2 heads? In B, what is the probability that there are 2 heads? Aren't both probabilities 1/2? EDIT Let me refine the question. The agreement I have with the observer is this: 1) I will toss 2 identical coins. 2) In situation A a third party will cover the coins with a cloth. The observer will look only at the outcome of the coin that comes to rest nearest him and report it to me. What is the probability that the second outcome will be the same as the first? 3) In situation B the observer will look at both outcomes and report only the state (heads or tails) of the coin that came to rest nearest him. What is the probability that the second outcome will be the same as the first? 2nd EDIT Changing 3) above to: 3) In situation B the observer will look at both outcomes, choose one of them, and truthfully report, ""There is at least one heads"", or ""There is at least one tails"", whichever is the case. What is the probability that the second outcome (that of the other coin) will be the same as the first?",,['probability']
75,probability of getting 50 heads from tossing a coin 100 times,probability of getting 50 heads from tossing a coin 100 times,,"folks, i am new to this forum and not a math expert. so please bear with me if am asking silly questions. The question is ""probability of getting 50 heads from tossing a coin 100 times"". So the answer for this is, I guess, ${100 \choose 50} (2 ^{-100})$. So all am trying to get is easier way to calculate ${100 \choose 50}$, or another approach to the parent problem only. Thanks all, appreciate that. raj","folks, i am new to this forum and not a math expert. so please bear with me if am asking silly questions. The question is ""probability of getting 50 heads from tossing a coin 100 times"". So the answer for this is, I guess, ${100 \choose 50} (2 ^{-100})$. So all am trying to get is easier way to calculate ${100 \choose 50}$, or another approach to the parent problem only. Thanks all, appreciate that. raj",,['probability']
76,If sums of Normal Distributions are Normal :Why are the weighted sums of Normal Distributions non-Normal?,If sums of Normal Distributions are Normal :Why are the weighted sums of Normal Distributions non-Normal?,,"I posted this question here Do 2 Normal Distributions always produce another Normal Distribution? where it was suggested to me that Characteristic Functions can be used to show that the distribution for the sums of independent Normal Distributions are still Normal. I had originally tried to prove this using Convolutions, but I think this is easier to show using Characteristic Functions: Define two random variables $X$ and $Y$ with probability distributions $f(x)$ and $f(y)$ . Assuming that we can determine the Characteristic Functions for both of these Random Variables: $$ \phi(t) = E[e^{itX}] $$ Then the sum of these two random variables $Z = X+Y$ has the following Characteristic Function $$ \phi_Z(t) = \phi_X(t) \cdot \phi_Y(t) $$ Using this logic, it seems much easier to prove that the distribution for the sums of two Normal Distributions is still Normal: $$ \text{Characteristic Function of a Normal Distribution:} \quad \phi(t) = e^{iμt - \frac{1}{2}σ^2t^2} $$ The Characteristic Functions of both Random Variables are: $$ \phi_X(t) = e^{iμ1t - \frac{1}{2}σ1^2t^2} $$ $$ \phi_Y(t) = e^{iμ2t - \frac{1}{2}σ2^2t^2} $$ We can then use this to find out the Characteristic Function of the sum $Z$ : $$ \phi_Z(t) = \phi_X(t) \cdot \phi_Y(t) = e^{iμ1t - \frac{1}{2}σ1^2t^2} \cdot e^{iμ2t - \frac{1}{2}σ2^2t^2} $$ $$ \phi_Z(t) = e^{i(μ1+μ2)t - \frac{1}{2}(σ1^2+σ2^2)t^2} $$ Using the property that Characteristic Functions are unique, we can recognize that $Z$ is basically a Normal Distribution with $\mu = \mu_1 + \mu_2$ and $\sigma^2 = \sigma_1^2 + \sigma_2^2$ . This leads me to my actual question: Are the weighted sums of Normal Distributions also Normally Distributed? I have read about Finite Mixture Models (e.g. https://en.wikipedia.org/wiki/Mixture_model ) in which a probability distribution is defined as follows: $$ f(x) = w1*\frac{1}{\sqrt{2\pi\sigma1^2}}e^{-\frac{(x-μ1)^2}{2\sigma1^2}} + w2*\frac{1}{\sqrt{2\pi\sigma2^2}}e^{-\frac{(x-μ2)^2}{2\sigma2^2}} + ... + wk*\frac{1}{\sqrt{2\pi\sigma_k^2}}e^{-\frac{(x-μ_k)^2}{2\sigma_k^2}} $$ $$ \sum_{i=1}^{n} w_i = 1 $$ $$ 0 \leq w_i \leq 1 \quad \text{for all } i $$ My initial thought was that if the sums of Normal Distributions are Normal, then shouldn't the Weighted Sums of Normal Distributions also be Normal? However on the other hand, I have seen visual representations of Finite Mixtures that don't look very Normal. I tried to write the sum of Characteristic Functions (e.g. start with only two distributions), I got: $$ \phi_{mixture}(t) = w1*\phi_{1}(t) + w2*\phi_{2}(t) $$ $$ \phi_{mixture}(t) = w1*e^{iμ1t - \frac{1}{2}σ^2t^2} + w2*e^{iμ2t - \frac{1}{2}σ^2t^2} $$ I am not sure if this result is Normal or Non-Normal. Can someone please help me out here? Is the above equation the Characteristic Function of a Normal Distribution?","I posted this question here Do 2 Normal Distributions always produce another Normal Distribution? where it was suggested to me that Characteristic Functions can be used to show that the distribution for the sums of independent Normal Distributions are still Normal. I had originally tried to prove this using Convolutions, but I think this is easier to show using Characteristic Functions: Define two random variables and with probability distributions and . Assuming that we can determine the Characteristic Functions for both of these Random Variables: Then the sum of these two random variables has the following Characteristic Function Using this logic, it seems much easier to prove that the distribution for the sums of two Normal Distributions is still Normal: The Characteristic Functions of both Random Variables are: We can then use this to find out the Characteristic Function of the sum : Using the property that Characteristic Functions are unique, we can recognize that is basically a Normal Distribution with and . This leads me to my actual question: Are the weighted sums of Normal Distributions also Normally Distributed? I have read about Finite Mixture Models (e.g. https://en.wikipedia.org/wiki/Mixture_model ) in which a probability distribution is defined as follows: My initial thought was that if the sums of Normal Distributions are Normal, then shouldn't the Weighted Sums of Normal Distributions also be Normal? However on the other hand, I have seen visual representations of Finite Mixtures that don't look very Normal. I tried to write the sum of Characteristic Functions (e.g. start with only two distributions), I got: I am not sure if this result is Normal or Non-Normal. Can someone please help me out here? Is the above equation the Characteristic Function of a Normal Distribution?","X Y f(x) f(y) 
\phi(t) = E[e^{itX}]
 Z = X+Y 
\phi_Z(t) = \phi_X(t) \cdot \phi_Y(t)
 
\text{Characteristic Function of a Normal Distribution:} \quad \phi(t) = e^{iμt - \frac{1}{2}σ^2t^2}
 
\phi_X(t) = e^{iμ1t - \frac{1}{2}σ1^2t^2}
 
\phi_Y(t) = e^{iμ2t - \frac{1}{2}σ2^2t^2}
 Z 
\phi_Z(t) = \phi_X(t) \cdot \phi_Y(t) = e^{iμ1t - \frac{1}{2}σ1^2t^2} \cdot e^{iμ2t - \frac{1}{2}σ2^2t^2}
 
\phi_Z(t) = e^{i(μ1+μ2)t - \frac{1}{2}(σ1^2+σ2^2)t^2}
 Z \mu = \mu_1 + \mu_2 \sigma^2 = \sigma_1^2 + \sigma_2^2 
f(x) = w1*\frac{1}{\sqrt{2\pi\sigma1^2}}e^{-\frac{(x-μ1)^2}{2\sigma1^2}} + w2*\frac{1}{\sqrt{2\pi\sigma2^2}}e^{-\frac{(x-μ2)^2}{2\sigma2^2}} + ... + wk*\frac{1}{\sqrt{2\pi\sigma_k^2}}e^{-\frac{(x-μ_k)^2}{2\sigma_k^2}}
 
\sum_{i=1}^{n} w_i = 1
 
0 \leq w_i \leq 1 \quad \text{for all } i
 
\phi_{mixture}(t) = w1*\phi_{1}(t) + w2*\phi_{2}(t)
 
\phi_{mixture}(t) = w1*e^{iμ1t - \frac{1}{2}σ^2t^2} + w2*e^{iμ2t - \frac{1}{2}σ^2t^2}
",['probability']
77,Probability with Permutations and Combinations,Probability with Permutations and Combinations,,"Consider the following problem. Consider a random arrangement of the letters in $GOOSE$ . What is the probability the arrangement is $OSGOE$ . Now, we know, that the probability is the number of favourable outcomes over the total number of outcomes. If we consider the number of arrangements of $GOOSE$ , we get $\frac{5!}{2!}=60$ . Hence the probability should be $\frac{1}{60}$ . However if approach this differently, we get a probability of $\frac{2}{5!}=\frac{1}{60}$ . It seems, whether we treat the $OO$ as distinct $O$ , or whether we treat the $OO$ as a repeat, the the result is the same. However, sometimes this isn't the case. How do we know which one to use? Generally, do we use permutations, when calculating probabilities, as even though two cases may appear the same, both can occur probabilistically, and we shouldn't divide by repeats? Eg. Consider arranging 3 red flower and 6 yellow flowers in a row. Find the probability that the 3 red roses are not separated. Here, does it matter whether we treat the flowers as distinct, or whether we treat the red flowers all the same, and all the yellows all the same. Do both produce the same answers? Find the probability that no two red roses are next to each other. Which one do I use here? Whether I should treat the objects as distinct or the same is confusing me.","Consider the following problem. Consider a random arrangement of the letters in . What is the probability the arrangement is . Now, we know, that the probability is the number of favourable outcomes over the total number of outcomes. If we consider the number of arrangements of , we get . Hence the probability should be . However if approach this differently, we get a probability of . It seems, whether we treat the as distinct , or whether we treat the as a repeat, the the result is the same. However, sometimes this isn't the case. How do we know which one to use? Generally, do we use permutations, when calculating probabilities, as even though two cases may appear the same, both can occur probabilistically, and we shouldn't divide by repeats? Eg. Consider arranging 3 red flower and 6 yellow flowers in a row. Find the probability that the 3 red roses are not separated. Here, does it matter whether we treat the flowers as distinct, or whether we treat the red flowers all the same, and all the yellows all the same. Do both produce the same answers? Find the probability that no two red roses are next to each other. Which one do I use here? Whether I should treat the objects as distinct or the same is confusing me.",GOOSE OSGOE GOOSE \frac{5!}{2!}=60 \frac{1}{60} \frac{2}{5!}=\frac{1}{60} OO O OO,"['probability', 'combinatorics', 'permutations', 'combinations']"
78,Expected value of number of specific cards in starting hands in a card game,Expected value of number of specific cards in starting hands in a card game,,"I don't understand what I found when calculating the expected value of a card game. A deck contains 40 cards. 8 of them are red cards and 32 of them are blue cards. At the start of the game, 5 cards are drawn to be the starting hand. The question is to find the expected value of drawing red cards in the starting hand. d = number of cards in deck r = number of red cards in deck b = number of blue cards in deck s = number of starting hands n = number of red cards drawn in starting hand I believe the probability drawing n red cards in starting hand is something like this, $P_n = \frac{C_n^r×C_{s-n}^{b}}{C_s^{d}}$ I believe the expected value of number of red cards drawn in starting hand should be like (and given that r>s), $$E(n) = \sum_{n=0}^s n×P_n$$ So r=8 in this case, E(n) should be 1 if calculated correctly After that, I modify the deck, so there will be 16 red cards and 24 blue cards. And the deck size remains unchanged. So r=16 and the returned value of E(n) is 2 if I calculated correctly. At this moment, I noticed that to calculate E(n), I can just simply calculate this, $\frac{r} {d}×s$ I don't understand why this is possible. I tried to simplify the expression of the equation from my original calculation but I failed. Could anyone kindly explain to me what happened? Edit: Thank you very much for the first 2 responses, but I would like to ask if there is a way to simplify this, $\sum_{n=0}^s n × \frac{C_n^r×C_{s-n}^{b}}{C_s^{d}}$ (where r>s) Into this, $\frac{r} {d}×s$","I don't understand what I found when calculating the expected value of a card game. A deck contains 40 cards. 8 of them are red cards and 32 of them are blue cards. At the start of the game, 5 cards are drawn to be the starting hand. The question is to find the expected value of drawing red cards in the starting hand. d = number of cards in deck r = number of red cards in deck b = number of blue cards in deck s = number of starting hands n = number of red cards drawn in starting hand I believe the probability drawing n red cards in starting hand is something like this, I believe the expected value of number of red cards drawn in starting hand should be like (and given that r>s), So r=8 in this case, E(n) should be 1 if calculated correctly After that, I modify the deck, so there will be 16 red cards and 24 blue cards. And the deck size remains unchanged. So r=16 and the returned value of E(n) is 2 if I calculated correctly. At this moment, I noticed that to calculate E(n), I can just simply calculate this, I don't understand why this is possible. I tried to simplify the expression of the equation from my original calculation but I failed. Could anyone kindly explain to me what happened? Edit: Thank you very much for the first 2 responses, but I would like to ask if there is a way to simplify this, (where r>s) Into this,",P_n = \frac{C_n^r×C_{s-n}^{b}}{C_s^{d}} E(n) = \sum_{n=0}^s n×P_n \frac{r} {d}×s \sum_{n=0}^s n × \frac{C_n^r×C_{s-n}^{b}}{C_s^{d}} \frac{r} {d}×s,"['probability', 'expected-value', 'card-games']"
79,What’s the probability for this problem?,What’s the probability for this problem?,,in class we were working on a question regarding our seating plan. The question was: we are a class of 32 students sitting in pairs of 2. What is the probability of one or more pairs existing in an original seating plan and after shuffling all students around? I made a small simulation in php that gives me an approximation of 40.3% but I don’t know how to calculate it. If anybody is interested in the simulation and wants to look for an error there please say so and I’ll add it later,in class we were working on a question regarding our seating plan. The question was: we are a class of 32 students sitting in pairs of 2. What is the probability of one or more pairs existing in an original seating plan and after shuffling all students around? I made a small simulation in php that gives me an approximation of 40.3% but I don’t know how to calculate it. If anybody is interested in the simulation and wants to look for an error there please say so and I’ll add it later,,['probability']
80,Probability 5-card hand contains exactly 2 pairs,Probability 5-card hand contains exactly 2 pairs,,"What is the probability that a five-card poker hand contains two pairs (that is, two of each of two different kinds and a fifth card of a third kind)? The textbook answer is 198/4165 But why doesn't it work that: Since we know the first card can be any card, it will be 52/52, the next card must be one of the same kind which is just 3/51, since there's 4 cards of a kind and one has already been picked. Then repeat for the next card excluding the remaining 2 cards of the last kind, 48/50, and the next being one of the same here 3/49. The last card being any other card 44/48 Multiply them all together (52 * 3 * 48 * 3 * 44) / (52 * 51 * 50 * 49 * 48) * 5!, the possible permutations  = 1584/4165, why does this process not work when the logic seems reasonable?","What is the probability that a five-card poker hand contains two pairs (that is, two of each of two different kinds and a fifth card of a third kind)? The textbook answer is 198/4165 But why doesn't it work that: Since we know the first card can be any card, it will be 52/52, the next card must be one of the same kind which is just 3/51, since there's 4 cards of a kind and one has already been picked. Then repeat for the next card excluding the remaining 2 cards of the last kind, 48/50, and the next being one of the same here 3/49. The last card being any other card 44/48 Multiply them all together (52 * 3 * 48 * 3 * 44) / (52 * 51 * 50 * 49 * 48) * 5!, the possible permutations  = 1584/4165, why does this process not work when the logic seems reasonable?",,"['probability', 'combinatorics']"
81,What is the stationary distribution of this Markov chain?,What is the stationary distribution of this Markov chain?,,A Markov chain is shown in the figure above. I am writing the transition matrix as: $$P =	\begin{bmatrix} 0.6 & 0.4 & 0 & 0\\ 0 & 0 & 0.5 & 0.5\\ 0 & 0 & 0.25 & 0.75\\ 0.7 & 0.3 & 0 & 0 \end{bmatrix}$$ I using given formula to find stationary distribution: $$AP=A$$ where $A=[w_1 \; w_2\; w_3\; w_4]$ is the stationary distribution. The answer for stationary distribution is given as: $$w_1=\frac{4}{12} \quad w_2=\frac{3}{12} \quad w_3=\frac{3}{12} \quad w_4=\frac{2}{12}$$ which I can easily check the formula $$AP=A$$ is not satisfying. Where am I going wrong?,A Markov chain is shown in the figure above. I am writing the transition matrix as: I using given formula to find stationary distribution: where is the stationary distribution. The answer for stationary distribution is given as: which I can easily check the formula is not satisfying. Where am I going wrong?,"P =	\begin{bmatrix}
0.6 & 0.4 & 0 & 0\\
0 & 0 & 0.5 & 0.5\\
0 & 0 & 0.25 & 0.75\\
0.7 & 0.3 & 0 & 0
\end{bmatrix} AP=A A=[w_1 \; w_2\; w_3\; w_4] w_1=\frac{4}{12} \quad w_2=\frac{3}{12} \quad w_3=\frac{3}{12} \quad w_4=\frac{2}{12} AP=A","['probability', 'stochastic-processes', 'markov-chains', 'entropy']"
82,Probability of Adjacent Birthdays,Probability of Adjacent Birthdays,,"Recall the birthday problem, where only 23 people are required for a >50% chance that at least two share the same birthday. What is the new probability if we want at least two people out of twenty-three to share the same birthday or have adjacent birthdays? January 1 and December 31 are considered adjacent, and we don't consider February 29. Similar to the birthday problem, I thought it would be easier to consider the probability that nobody has matching or adjacent birthdays, but as I was setting up the products, I realized that there are different amounts of available dates, depending on when the birthdays are. For ease of writing, let the dates be represented as 1, 2, 3, 4, ..., 365. Say the first person's birthday is 1. If the second person's birthday is 3 or 364, then there are 360 remaining choices for the third birthday, but if the second person's birthday is not in (364, 365, 1, 2, 3), then there are 359 remaining choices for the third person's birthday. Considering all these possibilities for 23 people is tedious, however. I also tried approaching this from a stars and bars perspective. Arrange 23 bars in a circle, put one star automatically between each bar, leaving 339 possible dates to be inserted. However, this also is difficult because there's an ordering. If three bars are Mar 1, Mar 14, and Mar 20, you can't stick Aug 5 in there. So, I was at a loss to figure it out intuitively. On the other hand, I generated a pattern by starting small (5 days and 2 people, for example), and finding a function out of OEIS. The answer to the problem with 365 days and 23 people is $$1 - \frac{23!}{365^{23}} \left( \binom{343}{23} - \binom{341}{21} \right) \approx 0.8879096$$ Let $r(n,k)$ be the rising factorial, $n(n+1)(n+2)...(n+k-1)$ . The answer can also be written as $$1 - \frac{23!}{365^{23}} \left( r(321, 23) - r(321, 21) \right) \approx 0.8879096$$ The $321$ comes from $365 - 2(23-1)$ , which is due to the pattern I found. My question is, what's the intuitive way to approach this problem, or how do I actually derive the formula to get my answer?","Recall the birthday problem, where only 23 people are required for a >50% chance that at least two share the same birthday. What is the new probability if we want at least two people out of twenty-three to share the same birthday or have adjacent birthdays? January 1 and December 31 are considered adjacent, and we don't consider February 29. Similar to the birthday problem, I thought it would be easier to consider the probability that nobody has matching or adjacent birthdays, but as I was setting up the products, I realized that there are different amounts of available dates, depending on when the birthdays are. For ease of writing, let the dates be represented as 1, 2, 3, 4, ..., 365. Say the first person's birthday is 1. If the second person's birthday is 3 or 364, then there are 360 remaining choices for the third birthday, but if the second person's birthday is not in (364, 365, 1, 2, 3), then there are 359 remaining choices for the third person's birthday. Considering all these possibilities for 23 people is tedious, however. I also tried approaching this from a stars and bars perspective. Arrange 23 bars in a circle, put one star automatically between each bar, leaving 339 possible dates to be inserted. However, this also is difficult because there's an ordering. If three bars are Mar 1, Mar 14, and Mar 20, you can't stick Aug 5 in there. So, I was at a loss to figure it out intuitively. On the other hand, I generated a pattern by starting small (5 days and 2 people, for example), and finding a function out of OEIS. The answer to the problem with 365 days and 23 people is Let be the rising factorial, . The answer can also be written as The comes from , which is due to the pattern I found. My question is, what's the intuitive way to approach this problem, or how do I actually derive the formula to get my answer?","1 - \frac{23!}{365^{23}} \left( \binom{343}{23} - \binom{341}{21} \right) \approx 0.8879096 r(n,k) n(n+1)(n+2)...(n+k-1) 1 - \frac{23!}{365^{23}} \left( r(321, 23) - r(321, 21) \right) \approx 0.8879096 321 365 - 2(23-1)","['probability', 'combinatorics', 'birthday']"
83,"If someone wins 3000 rounds of a game out of 100000 numbered rounds of a game, what is the expected no. of last-3-digits of a round that they solved?","If someone wins 3000 rounds of a game out of 100000 numbered rounds of a game, what is the expected no. of last-3-digits of a round that they solved?",,"I participate in r/picturegame on Reddit. Each round is numbered from Round 0 (1 actually, but let's say it was 0) up to Round 111400 at the moment. Suppose the top player of this game had won 3000 rounds out of all the rounds up to and including round 99999 and has been playing since the beginning (this is roughly true and keeps the numbers convenient I think.) Let's also suppose that he isn't especially trying to win rounds of a certain number, and the round numbers don't follow any particular pattern relative to his sleep schedule. (This is also realistic.) Now let's take the last 3 digits of each round that he won, so round 89345 would be ""345"", round 40080 would be ""080"" and (for the sake of argument) round 66 would be ""066"". Out of the 3000 rounds that he won, what is the expected number of round-endings, from ""000"" to ""999"", that are not represented amongst those rounds?","I participate in r/picturegame on Reddit. Each round is numbered from Round 0 (1 actually, but let's say it was 0) up to Round 111400 at the moment. Suppose the top player of this game had won 3000 rounds out of all the rounds up to and including round 99999 and has been playing since the beginning (this is roughly true and keeps the numbers convenient I think.) Let's also suppose that he isn't especially trying to win rounds of a certain number, and the round numbers don't follow any particular pattern relative to his sleep schedule. (This is also realistic.) Now let's take the last 3 digits of each round that he won, so round 89345 would be ""345"", round 40080 would be ""080"" and (for the sake of argument) round 66 would be ""066"". Out of the 3000 rounds that he won, what is the expected number of round-endings, from ""000"" to ""999"", that are not represented amongst those rounds?",,"['probability', 'statistics', 'random']"
84,Probability of pair of gloves selection,Probability of pair of gloves selection,,"In his wardrobe, Fred has a total of ten pairs of gloves. He had to pack his suitcase before a business meeting, and he chooses eight gloves without looking at them. We assume that any set of eight gloves has an equal chance of being chosen. I am told to calculate the likelihood that these 8 gloves do not contain any matching pairs, i.e. that no two (left and right) gloves are from the same pair. This is what I came up with, that is, the probability of success for each choice: $$\frac{20}{20}×\frac{18}{19}×\frac{16}{18}×...×\frac{6}{13}=\frac{384}{4199}≈0.09145$$ At first, I was a little confused by the wording but I believe this seems about right. Is there an alternative way to get the desired probability, e.g. with $1-...$ ? Thanks in advance for any feedback.","In his wardrobe, Fred has a total of ten pairs of gloves. He had to pack his suitcase before a business meeting, and he chooses eight gloves without looking at them. We assume that any set of eight gloves has an equal chance of being chosen. I am told to calculate the likelihood that these 8 gloves do not contain any matching pairs, i.e. that no two (left and right) gloves are from the same pair. This is what I came up with, that is, the probability of success for each choice: At first, I was a little confused by the wording but I believe this seems about right. Is there an alternative way to get the desired probability, e.g. with ? Thanks in advance for any feedback.",\frac{20}{20}×\frac{18}{19}×\frac{16}{18}×...×\frac{6}{13}=\frac{384}{4199}≈0.09145 1-...,"['probability', 'combinatorics']"
85,Probability of A winning the game [closed],Probability of A winning the game [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The following question is from problem 30 of the 2021 level 11-12 Mathematical Kangaroo: A certain game is won when one player gets 3 points ahead. Two players A and B are playing the game and at a particular point, A is 1 point ahead. Each player has an equal probability of winning each point. What is the probability that A wins the game? Apparently, the solution is 2/3, but I don’t quite know how the solution is gotten. Any help will be much appreciated!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The following question is from problem 30 of the 2021 level 11-12 Mathematical Kangaroo: A certain game is won when one player gets 3 points ahead. Two players A and B are playing the game and at a particular point, A is 1 point ahead. Each player has an equal probability of winning each point. What is the probability that A wins the game? Apparently, the solution is 2/3, but I don’t quite know how the solution is gotten. Any help will be much appreciated!",,"['probability', 'probability-theory', 'puzzle']"
86,Probability $3$ integers $\le n$ are side lengths of triangle,Probability  integers  are side lengths of triangle,3 \le n,"Here is a question from my probability textbook: Three different persons have each to name an integer not greater than $n$ . Find the chance that the integers named will be such that every two are together greater than the third. Here's what I did. After computing the cases up to $n = 7$ (which I'm not typing out here due to being too lazy), I was able to observe that we have the recursion $$p_1 = 1, \quad p_n = {{(n-1)^3 p_{n-1} + {{3n(n-1)}\over2} + 1}\over{n^3}}$$ However, I don't know how to solve it. Can anyone help me? Edit: I bountied the question. I'd like to see a complete self-contained solution solving the recurrence I give without reference to external sources such as OEIS, Wikipedia, etc.","Here is a question from my probability textbook: Three different persons have each to name an integer not greater than . Find the chance that the integers named will be such that every two are together greater than the third. Here's what I did. After computing the cases up to (which I'm not typing out here due to being too lazy), I was able to observe that we have the recursion However, I don't know how to solve it. Can anyone help me? Edit: I bountied the question. I'd like to see a complete self-contained solution solving the recurrence I give without reference to external sources such as OEIS, Wikipedia, etc.","n n = 7 p_1 = 1, \quad p_n = {{(n-1)^3 p_{n-1} + {{3n(n-1)}\over2} + 1}\over{n^3}}","['probability', 'combinatorics', 'recurrence-relations', 'contest-math']"
87,Solving the integral $\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)$,Solving the integral,\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right),"I want to solve the following integral $$ \int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right) $$ with $a\in\mathbb{R}$ and $t\in\mathbb{R}^+$ . I have tried some substitutions and the most promising are $u=\sqrt{t-s} \ \left(du=-\frac{1}{2\sqrt{t-s}} \  ds\right)$ so that the integral becomes \begin{align} \int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right) &=-2\int_{\sqrt{t}}^{0}du\ \sqrt{s}\operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)\\ &=\color{orange}{2\int^{\sqrt{t}}_{0}du\ \sqrt{t-u^2}\operatorname{erfc}\left(\frac{a}{\sqrt{t-u^2}}\right).} \end{align} $v=\frac{1}{\sqrt{s}} \ \left(dv = -\frac{1}{2s^{3/2}} \ \ \ ds \right)$ which yields \begin{align} \int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right) &=2\int_{\tfrac{1}{\sqrt{t}}}^{\infty} dv \ v^{3/2} \frac{1}{v\sqrt{t-\tfrac{1}{v^2}}} \ \operatorname{erfc}\left(av\right)\\ &=\color{orange}{2\int_{\tfrac{1}{\sqrt{t}}}^{\infty} dv \ \frac{v^{3/2}}{\sqrt{v^2t-1}}  \ \operatorname{erfc}\left(av\right)} \end{align} Furthermore, I have looked at the table of integrals [ 1 ], [ 2 ] and [ 3 ]. However, so far, no luck. Is there someone who knows a way forward?","I want to solve the following integral with and . I have tried some substitutions and the most promising are so that the integral becomes which yields Furthermore, I have looked at the table of integrals [ 1 ], [ 2 ] and [ 3 ]. However, so far, no luck. Is there someone who knows a way forward?","
\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)
 a\in\mathbb{R} t\in\mathbb{R}^+ u=\sqrt{t-s} \ \left(du=-\frac{1}{2\sqrt{t-s}} \  ds\right) \begin{align}
\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)
&=-2\int_{\sqrt{t}}^{0}du\ \sqrt{s}\operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)\\
&=\color{orange}{2\int^{\sqrt{t}}_{0}du\ \sqrt{t-u^2}\operatorname{erfc}\left(\frac{a}{\sqrt{t-u^2}}\right).}
\end{align} v=\frac{1}{\sqrt{s}} \ \left(dv = -\frac{1}{2s^{3/2}} \ \ \ ds \right) \begin{align}
\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)
&=2\int_{\tfrac{1}{\sqrt{t}}}^{\infty} dv \ v^{3/2} \frac{1}{v\sqrt{t-\tfrac{1}{v^2}}} \ \operatorname{erfc}\left(av\right)\\
&=\color{orange}{2\int_{\tfrac{1}{\sqrt{t}}}^{\infty} dv \ \frac{v^{3/2}}{\sqrt{v^2t-1}}  \ \operatorname{erfc}\left(av\right)}
\end{align}","['calculus', 'probability', 'integration', 'definite-integrals', 'error-function']"
88,"What is the probability that the sequence “broken, good, good, broken, good” occurs in the next $5$ bottles?","What is the probability that the sequence “broken, good, good, broken, good” occurs in the next  bottles?",5,"I have a probability problem. I think I know how to solve $A$ , but $B$ is still a mystery to me. If anyone could help me I appreciate a lot. Return bottles are constantly returned to the lemonade factory. There is a probability of $18\%$ of a bottle to be broken. This probably occurs independently. A) what is the probability that the sequence “broken, good, good, broken, good” occurs in the next $5$ bottles? B) what is the probability of the $10$ first bottles of the next $147$ bottles being good? For $A$ I think is $0.18\times(1-0.18)\times(1-0.18)\times0.18\times(1-0.18)$ . And, for $B$ , it seems a negative binomial probability, but I am not sure. Thank you very much","I have a probability problem. I think I know how to solve , but is still a mystery to me. If anyone could help me I appreciate a lot. Return bottles are constantly returned to the lemonade factory. There is a probability of of a bottle to be broken. This probably occurs independently. A) what is the probability that the sequence “broken, good, good, broken, good” occurs in the next bottles? B) what is the probability of the first bottles of the next bottles being good? For I think is . And, for , it seems a negative binomial probability, but I am not sure. Thank you very much",A B 18\% 5 10 147 A 0.18\times(1-0.18)\times(1-0.18)\times0.18\times(1-0.18) B,"['probability', 'probability-theory', 'probability-distributions']"
89,Probability of Sum being $4$ with unfair dice,Probability of Sum being  with unfair dice,4,"You roll two six-sided dice.  Die $1$ is fair.  Die $2$ is unfair such that the probability of rolling an odd number is $2/3$ and the probability of rolling an even number is $1/3$ , though the probability rolling of each odd number is the same, and the probability of rolling each even number is the same.  What is the probability of the sum of both dice adding up to $4$ ? My answer: I wrote out the combinations to sum $4$ between both dies: (F1, U3), (F2, U2), (F3, U1) [F = Fair die, U = Unfair die]. My thinking is we need to find the OR probability of these pairs: P((F1 and U3) or (F2 and U2) or (F3 and U1)): $$(\frac16 \times \frac23) + (\frac16 \times \frac12) + (\frac16 \times \frac23) = 0.30555555555.$$ Is this correct?","You roll two six-sided dice.  Die is fair.  Die is unfair such that the probability of rolling an odd number is and the probability of rolling an even number is , though the probability rolling of each odd number is the same, and the probability of rolling each even number is the same.  What is the probability of the sum of both dice adding up to ? My answer: I wrote out the combinations to sum between both dies: (F1, U3), (F2, U2), (F3, U1) [F = Fair die, U = Unfair die]. My thinking is we need to find the OR probability of these pairs: P((F1 and U3) or (F2 and U2) or (F3 and U1)): Is this correct?",1 2 2/3 1/3 4 4 (\frac16 \times \frac23) + (\frac16 \times \frac12) + (\frac16 \times \frac23) = 0.30555555555.,"['probability', 'discrete-mathematics']"
90,Showing $E(X^n) = n!a$ using induction. [closed],Showing  using induction. [closed],E(X^n) = n!a,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $X$ be a random variable with $a = E(X)$ , and suppose $X$ has the same distribution as $U(Y + Z)$ , where $U$ is uniformly distributed over $(0, 1)$ , $U, Y, Z$ are independent and $X, Y, Z$ have the same distribution. How can we prove using induction that $E(X^n) = n!a^n$ for $n = 1, 2,...$ . Any hint would be appreciated.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let be a random variable with , and suppose has the same distribution as , where is uniformly distributed over , are independent and have the same distribution. How can we prove using induction that for . Any hint would be appreciated.","X a = E(X) X U(Y + Z) U (0, 1) U, Y, Z X, Y, Z E(X^n) = n!a^n n = 1, 2,...","['probability', 'probability-theory', 'induction', 'independence']"
91,"Uniformly at random, break a unit stick in two places. What is the probability that the smallest piece is $\leq 1/5$?","Uniformly at random, break a unit stick in two places. What is the probability that the smallest piece is ?",\leq 1/5,"I was asked this in an interview and wasn't sure how to solve it: Consider a stick of length $1$ . Select two points independently and uniformly at random on the stick. Break the stick at these two points, resulting in $3$ smaller pieces. What is the probability that the smallest of these pieces is $\leq 1/5$ ? For starters, I noted that the three sections must have lengths: $$ \begin{aligned} \ell_1 &= \max(X,Y) \\ \ell_2 &= \max(X,Y) - \min(X,Y) \\ \ell_3 &= 1 - \max(X,Y) \end{aligned} $$ Let $\ell_{\texttt{min}}$ , $\ell_{\texttt{mid}}$ , and $\ell_{\texttt{max}}$ represent the smallest, middle, and largest stick lengths. So clearly we want to compute $$ \mathbb{P}(\ell_{\texttt{min}} \leq 1/5) $$ However, I wasn't sure how to move forward from here. I assume I need to frame the problem such that I can perform integration by computing an area on a unit square.","I was asked this in an interview and wasn't sure how to solve it: Consider a stick of length . Select two points independently and uniformly at random on the stick. Break the stick at these two points, resulting in smaller pieces. What is the probability that the smallest of these pieces is ? For starters, I noted that the three sections must have lengths: Let , , and represent the smallest, middle, and largest stick lengths. So clearly we want to compute However, I wasn't sure how to move forward from here. I assume I need to frame the problem such that I can perform integration by computing an area on a unit square.","1 3 \leq 1/5 
\begin{aligned}
\ell_1 &= \max(X,Y)
\\
\ell_2 &= \max(X,Y) - \min(X,Y)
\\
\ell_3 &= 1 - \max(X,Y)
\end{aligned}
 \ell_{\texttt{min}} \ell_{\texttt{mid}} \ell_{\texttt{max}} 
\mathbb{P}(\ell_{\texttt{min}} \leq 1/5)
",['probability']
92,Bagel probability intuition; unordered vs ordered selection,Bagel probability intuition; unordered vs ordered selection,,"I was solving the following problem: The bagel shop serves 4 types of bagels; plain, poppy seed, asiago, and everything. If 6 bagels were randomly selected for a breakfast meeting, what is the probability that no poppy seed bagels were selected? My thought processes was; if I were to reach in and grab a bagel, there is a $\frac{3}{4}$ chance that it is not a poppy seed bagel. If I repeated this experiment 6 times, then I would have a $\left(\frac{3}{4}\right)^6 \approx 18\%$ chance of not drawing a poppy seed bagel. However, when reviewing the solution it was calculated as $$\frac{\text{Number of unordered sets not containing poppy seed}}{\text{Total number of unordered sets}}$$ or $$\frac{\binom{6+3-1}{6}}{\binom{6+4-1}{6}} \approx 33\%$$ I understand intuitively why the solution makes sense, calculating the fraction of the number of unordered sets that do not contain poppy seed, my question is Why does my intuition fail me in the first case?","I was solving the following problem: The bagel shop serves 4 types of bagels; plain, poppy seed, asiago, and everything. If 6 bagels were randomly selected for a breakfast meeting, what is the probability that no poppy seed bagels were selected? My thought processes was; if I were to reach in and grab a bagel, there is a chance that it is not a poppy seed bagel. If I repeated this experiment 6 times, then I would have a chance of not drawing a poppy seed bagel. However, when reviewing the solution it was calculated as or I understand intuitively why the solution makes sense, calculating the fraction of the number of unordered sets that do not contain poppy seed, my question is Why does my intuition fail me in the first case?",\frac{3}{4} \left(\frac{3}{4}\right)^6 \approx 18\% \frac{\text{Number of unordered sets not containing poppy seed}}{\text{Total number of unordered sets}} \frac{\binom{6+3-1}{6}}{\binom{6+4-1}{6}} \approx 33\%,"['probability', 'combinatorics', 'discrete-mathematics', 'permutations', 'intuition']"
93,What is the opposite of a null set?,What is the opposite of a null set?,,"In probability theory (and in measure theory), a null set is a set with measure $0$ . Is there a term for the opposite of this, ie. a set whose complement is a null set, or (equivalently, if we restrict ourselves to a probability space) a set with measure $1$ ? Events with probability $1$ can be described as ""almost certain,"" ""almost sure,"" etc., but I'm looking for a word to describe the set itself. Would something like ""full set"" be a suitable phrasing? Or is there already a convention for this?","In probability theory (and in measure theory), a null set is a set with measure . Is there a term for the opposite of this, ie. a set whose complement is a null set, or (equivalently, if we restrict ourselves to a probability space) a set with measure ? Events with probability can be described as ""almost certain,"" ""almost sure,"" etc., but I'm looking for a word to describe the set itself. Would something like ""full set"" be a suitable phrasing? Or is there already a convention for this?",0 1 1,"['probability', 'probability-theory', 'measure-theory', 'terminology']"
94,"Finding density of $U = \frac{X}{X + Y}$ for $X, \ Y $ ~ $\text{Exp}(\lambda)$ i.i.d [duplicate]",Finding density of  for  ~  i.i.d [duplicate],"U = \frac{X}{X + Y} X, \ Y  \text{Exp}(\lambda)","This question already has answers here : X,Y are independent exponentially distributed then what is the distribution of X/(X+Y) (3 answers) Closed 3 years ago . Problem: Given $X, Y$ ~ $\text{Exp}(\lambda)$ i.i.d, find $f_U, \ F_U$ for $U := \frac{X}{X + Y}$ . My approach: For a fixed $u > 0$ , parameterize $\{ (x,y) | \frac{x}{x + y} = u \}$ = $\{ (x,y) | y = \frac{x  (1 - u)}{u} \}$ = $\{ (x,\frac{x  (1 - u)}{u}) | x \geq 0\}$ ( $x \geq 0$ holds by $X$ ~ $\text{Exp}(\lambda)$ ). Then, one can compute: $$\int_0^{+\infty}f_X(x)  f_Y\left(\frac{x (1 - u)}{u}\right) \mathrm{d}x = \int_0^{+\infty} \lambda  e^{-\lambda x}  \lambda  e^{-\lambda \frac{x  (1 - u)}{u}} \mathrm{d}x = \lambda^2 \int_0^{+\infty} e^{-\lambda x  \frac{1}{u}} \mathrm{d}x = \\ \lambda^2  \left(-\frac{u}{\lambda}  e^{-\lambda x  \frac{1}{u}} \biggr{\rvert}_0^{+\infty}\right) = \lambda^2  \left(\frac{u}{\lambda}\right) = \lambda u $$ My problem: Given those computations, I arrived at the conclusion $f_U(u) = \lambda u$ . Although Wolfram Alpha agrees with my computations, the master solution does not, as according to it $F_U (u) = u$ (and therefore $f_U = 1$ ). It'd be great to get some help on where I went wrong. Given that Wolfram Alpha indicates correct computations, I believe my mistake to be conceptual. On a general note: How would you rate my approach; are there better ways to tackle such problems?","This question already has answers here : X,Y are independent exponentially distributed then what is the distribution of X/(X+Y) (3 answers) Closed 3 years ago . Problem: Given ~ i.i.d, find for . My approach: For a fixed , parameterize = = ( holds by ~ ). Then, one can compute: My problem: Given those computations, I arrived at the conclusion . Although Wolfram Alpha agrees with my computations, the master solution does not, as according to it (and therefore ). It'd be great to get some help on where I went wrong. Given that Wolfram Alpha indicates correct computations, I believe my mistake to be conceptual. On a general note: How would you rate my approach; are there better ways to tackle such problems?","X, Y \text{Exp}(\lambda) f_U, \ F_U U := \frac{X}{X + Y} u > 0 \{ (x,y) | \frac{x}{x + y} = u \} \{ (x,y) | y = \frac{x  (1 - u)}{u} \} \{ (x,\frac{x  (1 - u)}{u}) | x \geq 0\} x \geq 0 X \text{Exp}(\lambda) \int_0^{+\infty}f_X(x)  f_Y\left(\frac{x (1 - u)}{u}\right) \mathrm{d}x = \int_0^{+\infty} \lambda  e^{-\lambda x}  \lambda  e^{-\lambda \frac{x  (1 - u)}{u}} \mathrm{d}x = \lambda^2 \int_0^{+\infty} e^{-\lambda x  \frac{1}{u}} \mathrm{d}x = \\ \lambda^2  \left(-\frac{u}{\lambda}  e^{-\lambda x  \frac{1}{u}} \biggr{\rvert}_0^{+\infty}\right) = \lambda^2  \left(\frac{u}{\lambda}\right) = \lambda u  f_U(u) = \lambda u F_U (u) = u f_U = 1","['probability', 'integration', 'probability-distributions', 'exponential-distribution']"
95,Characteristic Function as a Fourier Transform,Characteristic Function as a Fourier Transform,,"The fourier transform of a function is defined to be: $$\hat{f}(\omega)=\int_{R}e^{-it\omega}f(t)dt$$ which I understand that essentially $e^{-it\omega}$ controls the frequency at  which our function $f(t)$ is wrapped around the unit circle in the complex plane, with $f(t)$ dictating the radius of the polar graph for a given $t$ . That is, if my frequency is $10$ then every rotation around the unit circle traverses $\frac{1}{10}$ seconds of my graph $f(t)$ in 10 rotations I have covered 10 seconds of my function $f(t)$ . The fourier transform outputs the central mass for a given frequency across all $t$ of our polar graph. For  wave functions this  intuitively makes  sense but what does the fourier transform for our probability density function tell us? How do I interpret frequency with respect to a non-wave function?","The fourier transform of a function is defined to be: which I understand that essentially controls the frequency at  which our function is wrapped around the unit circle in the complex plane, with dictating the radius of the polar graph for a given . That is, if my frequency is then every rotation around the unit circle traverses seconds of my graph in 10 rotations I have covered 10 seconds of my function . The fourier transform outputs the central mass for a given frequency across all of our polar graph. For  wave functions this  intuitively makes  sense but what does the fourier transform for our probability density function tell us? How do I interpret frequency with respect to a non-wave function?",\hat{f}(\omega)=\int_{R}e^{-it\omega}f(t)dt e^{-it\omega} f(t) f(t) t 10 \frac{1}{10} f(t) f(t) t,"['probability', 'probability-theory', 'fourier-analysis', 'characteristic-functions']"
96,Given $\mathbb E(5X+2)=12$ and $\mathbb E(X|Y)=Y^3$ compute $\mathbb E(Y^3)$,Given  and  compute,\mathbb E(5X+2)=12 \mathbb E(X|Y)=Y^3 \mathbb E(Y^3),"Given $\mathbb E(5X+2)=12$ and $\mathbb E(X|Y)=Y^3$ compute $\mathbb E(Y^3)$ . I've been trying this a million different ways and can't seem to reach a final answer, I would love any suggestions!","Given and compute . I've been trying this a million different ways and can't seem to reach a final answer, I would love any suggestions!",\mathbb E(5X+2)=12 \mathbb E(X|Y)=Y^3 \mathbb E(Y^3),"['probability', 'probability-theory', 'conditional-expectation', 'conditional-probability', 'expected-value']"
97,Uniform distribution variable in the Newton symbol,Uniform distribution variable in the Newton symbol,,"The following question is from actuarial exam. Let $N$ be uniformly distributed on $\{0,1,2,...,19\}$ . Compute $$\mathbb{E}\sum_{k=0}^{N}{N-k \choose k}(-1)^k$$ I started $$\mathbb{E}\sum_{k=0}^{N}{N-k \choose k}(-1)^k=\frac{1}{20}\sum_{n=0}^{19}\mathbb{E}\left(\sum_{k=0}^{N}{N-k \choose k}(-1)^k|N=n\right)=\frac{1}{20}\mathbb{E}\sum_{n=0}^{19}\sum_{k=0}^{n}{n-k \choose k}(-1)^k$$ Here I changed summation variable and the order of summation a couple of times but it didn't work: $$\frac{1}{20}\mathbb{E}\sum_{n=0}^{19}\sum_{k=0}^{n}{n-k \choose k}(-1)^k=\frac{1}{20}\mathbb{E}\sum_{n=0}^{19}\sum_{l=0}^{n}{l \choose n-l}(-1)^{n-l}=\frac{1}{20}\mathbb{E}\sum_{l=0}^{19}\sum_{n=l}^{19}{l \choose n-l}(-1)^{n-l}=\frac{1}{20}\mathbb{E}\sum_{l=0}^{19}\sum_{m=0}^{19-l}{l \choose m}(-1)^{m}$$ The last sum is equal $0$ if $l\leq 9$ . This is because $$\sum_{k=0}^{n}{n \choose k}(-1)^k=0.$$ Hence $$\frac{1}{20}\mathbb{E}\sum_{l=0}^{19}\sum_{m=0}^{19-l}{l \choose m}(-1)^{m}=\frac{1}{20}\mathbb{E}\sum_{l=10}^{19}\sum_{m=0}^{19-l}{l \choose m}(-1)^{m}$$ I also tried to count this sum by setting consecutive $N=0,1,2,3,4...$ but I coudn't find any regularity. Please help",The following question is from actuarial exam. Let be uniformly distributed on . Compute I started Here I changed summation variable and the order of summation a couple of times but it didn't work: The last sum is equal if . This is because Hence I also tried to count this sum by setting consecutive but I coudn't find any regularity. Please help,"N \{0,1,2,...,19\} \mathbb{E}\sum_{k=0}^{N}{N-k \choose k}(-1)^k \mathbb{E}\sum_{k=0}^{N}{N-k \choose k}(-1)^k=\frac{1}{20}\sum_{n=0}^{19}\mathbb{E}\left(\sum_{k=0}^{N}{N-k \choose k}(-1)^k|N=n\right)=\frac{1}{20}\mathbb{E}\sum_{n=0}^{19}\sum_{k=0}^{n}{n-k \choose k}(-1)^k \frac{1}{20}\mathbb{E}\sum_{n=0}^{19}\sum_{k=0}^{n}{n-k \choose k}(-1)^k=\frac{1}{20}\mathbb{E}\sum_{n=0}^{19}\sum_{l=0}^{n}{l \choose n-l}(-1)^{n-l}=\frac{1}{20}\mathbb{E}\sum_{l=0}^{19}\sum_{n=l}^{19}{l \choose n-l}(-1)^{n-l}=\frac{1}{20}\mathbb{E}\sum_{l=0}^{19}\sum_{m=0}^{19-l}{l \choose m}(-1)^{m} 0 l\leq 9 \sum_{k=0}^{n}{n \choose k}(-1)^k=0. \frac{1}{20}\mathbb{E}\sum_{l=0}^{19}\sum_{m=0}^{19-l}{l \choose m}(-1)^{m}=\frac{1}{20}\mathbb{E}\sum_{l=10}^{19}\sum_{m=0}^{19-l}{l \choose m}(-1)^{m} N=0,1,2,3,4...","['probability', 'binomial-coefficients', 'uniform-distribution', 'expected-value']"
98,Calculate lottery's second prize using combination - lottery probability question.,Calculate lottery's second prize using combination - lottery probability question.,,"Assume a lottery game of the following rules: Picking your numbers : Pick a total of 6 different numbers from the lot containing 42 numbers (1 to 42). Draw : Draw 7 balls, with no repetition(i.e: ball drawn is not put back in the lot) from the lot labeled from 1 to 42. Results : If the first 6 balls drawn matches your own 6 numbers (order doesn't matter) : Jackpot . If 5 of the first 6 balls drawn matches 5 of your numbers (order doesn't matter) and the 7th drawn ball matches your 6th number: second prize . If 5 of the first 6 balls drawn matches 5 of your numbers and nothing else matches: third prize . I'll end it here for not having many other prizes. If I want to check my chance of winning the jackpot, it's pretty straightforward and looks like a combination $C(42,6)$ , so it should be: $$     \frac{42\cdot41\cdot40\cdot39\cdot38\cdot37}{6!} = 5,245,786.  $$ So my chance of getting the jackpot is $(\frac{1}{5,245,786})$ For the third prize it's also a straightforward combination $C(42,5)$ , it's equal to: $$     \frac{42\cdot41\cdot40\cdot39\cdot38}{5!} = 850,668.  $$ So third prize probability is equal to $\left(\frac{1}{850,668}\right)$ Now I am being stumbled on how to calculate the 2nd prize probability. My memories from school are not helping me enough to get my answer. I know that it should be between the two numbers I got there, however any calculations I am making end ups with a probability much higher than the first prize's. Could you please verify that my 1st and 3rd prize probabilities are well calculated and help me calculate the 2nd prize probability?","Assume a lottery game of the following rules: Picking your numbers : Pick a total of 6 different numbers from the lot containing 42 numbers (1 to 42). Draw : Draw 7 balls, with no repetition(i.e: ball drawn is not put back in the lot) from the lot labeled from 1 to 42. Results : If the first 6 balls drawn matches your own 6 numbers (order doesn't matter) : Jackpot . If 5 of the first 6 balls drawn matches 5 of your numbers (order doesn't matter) and the 7th drawn ball matches your 6th number: second prize . If 5 of the first 6 balls drawn matches 5 of your numbers and nothing else matches: third prize . I'll end it here for not having many other prizes. If I want to check my chance of winning the jackpot, it's pretty straightforward and looks like a combination , so it should be: So my chance of getting the jackpot is For the third prize it's also a straightforward combination , it's equal to: So third prize probability is equal to Now I am being stumbled on how to calculate the 2nd prize probability. My memories from school are not helping me enough to get my answer. I know that it should be between the two numbers I got there, however any calculations I am making end ups with a probability much higher than the first prize's. Could you please verify that my 1st and 3rd prize probabilities are well calculated and help me calculate the 2nd prize probability?","C(42,6) 
    \frac{42\cdot41\cdot40\cdot39\cdot38\cdot37}{6!} = 5,245,786.
  (\frac{1}{5,245,786}) C(42,5) 
    \frac{42\cdot41\cdot40\cdot39\cdot38}{5!} = 850,668.
  \left(\frac{1}{850,668}\right)","['probability', 'combinations', 'lotteries']"
99,Proving formula sum of product of binomial coefficients,Proving formula sum of product of binomial coefficients,,"I have to proof the following formula \begin{align} \sum_{k=0}^{n/2} {n\choose2k} {2k\choose k} 2^{n-2k} = {2n\choose n} \end{align} I tried to use the fact that ${2n\choose n} = \sum_{k=0}^{n} {n\choose k}^2$, but I don't get any conclusion. Any suggestions? Thanks in advance!","I have to proof the following formula \begin{align} \sum_{k=0}^{n/2} {n\choose2k} {2k\choose k} 2^{n-2k} = {2n\choose n} \end{align} I tried to use the fact that ${2n\choose n} = \sum_{k=0}^{n} {n\choose k}^2$, but I don't get any conclusion. Any suggestions? Thanks in advance!",,"['probability', 'combinatorics', 'binomial-coefficients', 'combinatorial-proofs']"
