,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Preserving independence of random variables,Preserving independence of random variables,,"Suppose I have three random variables, $X,Y,Z$ with $X$ independent of $Z$, $Y$ independent of $Z$. Which transformation can I apply to $X,Y$ to that the result is again a random variable independent of $Z$? Or better, for which $f(x,y)$ is $f(X,Y)$ independent of $Z$? For example $f(x,y) = xy$ does not work because in general $XY$ is not independent of $Z$. Is there a general result?","Suppose I have three random variables, $X,Y,Z$ with $X$ independent of $Z$, $Y$ independent of $Z$. Which transformation can I apply to $X,Y$ to that the result is again a random variable independent of $Z$? Or better, for which $f(x,y)$ is $f(X,Y)$ independent of $Z$? For example $f(x,y) = xy$ does not work because in general $XY$ is not independent of $Z$. Is there a general result?",,"['probability', 'probability-theory', 'random-variables', 'independence']"
1,Modeling with Markov Chains and one-step analysis,Modeling with Markov Chains and one-step analysis,,I have set up the following model: Let $X_n$ be the number of heads in the $n$-th toss and $P(X_0=0)=1$. I can calculate the transition matrix $P$. Define  $$ T=\min\{n\geq 0\mid X_n=5\}. $$ Then $P(X=1)=P(X_{T-1}=4)$. Noting that $X_n$ is a Markov chain and defining $$ u_i=P(X_{T-1}=4\mid X_0=i) $$ we have $$ u_i=\sum_{j=0}^5P(X_{T-1}=4\mid X_1=j)\cdot P_{ij}. $$ I ended up with $u_0=0$ which seems nonsense. [Edited:]What is wrong with my model? Would anybody come up with a better one? [Added:] Here is possibly where I get the calculation wrong: $$ P(X_{T-1}=4\mid X_1=i)=u_i $$ and $u_5=0$. [Last Update:] What is wrong is $$ P(X_{T-1}=4\mid X_1=4)=u_4. $$ One should get $P(X_{T-1}=4\mid X_1=4)=1$ instead.,I have set up the following model: Let $X_n$ be the number of heads in the $n$-th toss and $P(X_0=0)=1$. I can calculate the transition matrix $P$. Define  $$ T=\min\{n\geq 0\mid X_n=5\}. $$ Then $P(X=1)=P(X_{T-1}=4)$. Noting that $X_n$ is a Markov chain and defining $$ u_i=P(X_{T-1}=4\mid X_0=i) $$ we have $$ u_i=\sum_{j=0}^5P(X_{T-1}=4\mid X_1=j)\cdot P_{ij}. $$ I ended up with $u_0=0$ which seems nonsense. [Edited:]What is wrong with my model? Would anybody come up with a better one? [Added:] Here is possibly where I get the calculation wrong: $$ P(X_{T-1}=4\mid X_1=i)=u_i $$ and $u_5=0$. [Last Update:] What is wrong is $$ P(X_{T-1}=4\mid X_1=4)=u_4. $$ One should get $P(X_{T-1}=4\mid X_1=4)=1$ instead.,,['probability']
2,How to numerically test a limsup? (Example : numerical simulation of the law of iterated logarithm),How to numerically test a limsup? (Example : numerical simulation of the law of iterated logarithm),,"I have a random walk $S_n$ (the increments are Bernoulli $\pm 1$ with probability $1/2$ each). I'd like to test numerically the Law of iterated logarithm : $$\limsup_{n \rightarrow \infty} \underbrace{\frac{S_n}{\sqrt{2 n (\log \log n)}}}_{Y_n} = 1, \qquad \rm{a.s.}$$ My attemps have failed (see this question ) since, when you do a numerical simulation, you can never evaluate this quantity that would be required for the $\limsup$ evaluation (because the computer memory is not infinite...): $$Z_k=\sup_{\ell \geq k}Y_{\ell}$$ but only: $$Y_{k,n}=\max_{k\leq \ell \leq n}Y_\ell$$ Question: how can you do a simulation that showcases that the $\limsup$ is $1$ ? (and have a plot showing a convergence to 1, in the contrary of this failed attempt ). Sidenote: in my case, the increments are not exactly independent, but close to it. I'd like to numerically test if a law-of-iterated-logarithm-like result holds. But for now, I would already be more than happy if I could get a numerical evidence of the standard law in the standard case where increments are independent. Sidenote2: code for failed attempt: import numpy as np import matplotlib.pyplot as plt N = 10*1000*1000 B = 2 * np.random.binomial(1, 0.5, N) - 1       # N independent +1/-1 each of them with probability 1/2 B = np.cumsum(B)                                # random walk plt.plot(B); plt.show() C = B / np.sqrt(2 * np.arange(N) * np.log(np.log(np.arange(N)))) M = np.maximum.accumulate(C[::-1])[::-1]        # limsup, see http://stackoverflow.com/questions/35149843/running-max-limsup-in-numpy-what-optimization plt.plot(M); plt.show()","I have a random walk (the increments are Bernoulli with probability each). I'd like to test numerically the Law of iterated logarithm : My attemps have failed (see this question ) since, when you do a numerical simulation, you can never evaluate this quantity that would be required for the evaluation (because the computer memory is not infinite...): but only: Question: how can you do a simulation that showcases that the is ? (and have a plot showing a convergence to 1, in the contrary of this failed attempt ). Sidenote: in my case, the increments are not exactly independent, but close to it. I'd like to numerically test if a law-of-iterated-logarithm-like result holds. But for now, I would already be more than happy if I could get a numerical evidence of the standard law in the standard case where increments are independent. Sidenote2: code for failed attempt: import numpy as np import matplotlib.pyplot as plt N = 10*1000*1000 B = 2 * np.random.binomial(1, 0.5, N) - 1       # N independent +1/-1 each of them with probability 1/2 B = np.cumsum(B)                                # random walk plt.plot(B); plt.show() C = B / np.sqrt(2 * np.arange(N) * np.log(np.log(np.arange(N)))) M = np.maximum.accumulate(C[::-1])[::-1]        # limsup, see http://stackoverflow.com/questions/35149843/running-max-limsup-in-numpy-what-optimization plt.plot(M); plt.show()","S_n \pm 1 1/2 \limsup_{n \rightarrow \infty} \underbrace{\frac{S_n}{\sqrt{2 n (\log \log n)}}}_{Y_n} = 1, \qquad \rm{a.s.} \limsup Z_k=\sup_{\ell \geq k}Y_{\ell} Y_{k,n}=\max_{k\leq \ell \leq n}Y_\ell \limsup 1","['probability', 'probability-theory', 'random-variables', 'random-walk', 'simulation']"
3,Law of the Iterated Logarithm and Stopping Times,Law of the Iterated Logarithm and Stopping Times,,"Let $(X_1,X_2,...)$ be i.i.d random variables with mean  $0$ and variance $1$. By the Law of the Iterated Logarithm, for all $\epsilon >0$, \begin{equation} P\left[ \frac{1}{t}\sum_{i=1}^{t}X_i \geq (1+\epsilon)\sqrt{\frac{2 \log \log t}{t}}\text{ i.o.}\right] =0 \end{equation} Let  \begin{equation} Y :=\left\{ \frac{1}{t}\sum_{i=1}^{t}X_i<(1+\epsilon)\sqrt{\frac{2 \log \log t}{t}}\text{ for all }t \geq 3 , \;\epsilon >0\right\}  \end{equation} be the set of sample paths such that $\frac{1}{t}\sum_{i=1}^{t-1}X_i$ never exceeds $(1+\epsilon)\sqrt{\frac{2 \log \log t}{t}}$ for all $\epsilon > 0$. I am trying to show that $P(Y) >0$. Here is an equivalent formulation which may be useful. Let $I_t = I\{A_t\}$ denote the indicator rv for the event $A_t$, where $A_t$ is the event corresponding to  $$\frac{1}{t}\sum_{i=1}^{t}X_i \geq (1+\epsilon)\sqrt{\frac{2 \log \log t}{t}} $$ and let $N = \sum_{t=1}^{\infty} I_t$ denote the total number of the events to occur. Then this is equivalent to showing $P(N=0)>0$.","Let $(X_1,X_2,...)$ be i.i.d random variables with mean  $0$ and variance $1$. By the Law of the Iterated Logarithm, for all $\epsilon >0$, \begin{equation} P\left[ \frac{1}{t}\sum_{i=1}^{t}X_i \geq (1+\epsilon)\sqrt{\frac{2 \log \log t}{t}}\text{ i.o.}\right] =0 \end{equation} Let  \begin{equation} Y :=\left\{ \frac{1}{t}\sum_{i=1}^{t}X_i<(1+\epsilon)\sqrt{\frac{2 \log \log t}{t}}\text{ for all }t \geq 3 , \;\epsilon >0\right\}  \end{equation} be the set of sample paths such that $\frac{1}{t}\sum_{i=1}^{t-1}X_i$ never exceeds $(1+\epsilon)\sqrt{\frac{2 \log \log t}{t}}$ for all $\epsilon > 0$. I am trying to show that $P(Y) >0$. Here is an equivalent formulation which may be useful. Let $I_t = I\{A_t\}$ denote the indicator rv for the event $A_t$, where $A_t$ is the event corresponding to  $$\frac{1}{t}\sum_{i=1}^{t}X_i \geq (1+\epsilon)\sqrt{\frac{2 \log \log t}{t}} $$ and let $N = \sum_{t=1}^{\infty} I_t$ denote the total number of the events to occur. Then this is equivalent to showing $P(N=0)>0$.",,['probability']
4,What is a good book/online source about the subject of reliability? [closed],What is a good book/online source about the subject of reliability? [closed],,Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 8 years ago . Improve this question I am particularly interested about Reliability function $R(t)$ Failure function $h(t)$ Renewal process Maintained systems Nonhomogenius Poisson process and reliability growth Thank you.,Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 8 years ago . Improve this question I am particularly interested about Reliability function $R(t)$ Failure function $h(t)$ Renewal process Maintained systems Nonhomogenius Poisson process and reliability growth Thank you.,,"['probability', 'reference-request']"
5,"Characterization of point process, given the number of points","Characterization of point process, given the number of points",,"For a point process with independent and identically distributed (i.i.d) inter-renewals, with distribution $p(x)$, we observed $N$ points on $[0,T]$. What is the joint probability distribution function of random variables $x_1,x_2,\cdots,x_n$? Obviously, $x_1+x_2+\cdots+x_n=T$ For a Poisson point process, it is simply the probability distribution function of the event that we put $N$ points independently and randomly with respect to uniform distribution on $[0,T]$. For a renewal point process, where the inter-renewal times are i.i.d with distribution $p(x)$ the distribution function is: \begin{align} &F_{X_1,\dots,X_n}\left(t_1,\cdots,t_2\right)\\&=\mathbb{P}\left(x_1\leq t_1,\cdots,x_{n-1}\leq t_{n-1}|\{\sum_{i=1}^{n-1}x_i<T\} \cap \{\sum_{i=1}^{n}x_i>T\} \right)\\ &=\frac{\mathbb{P}\left(x_1\leq t_1,\cdots,x_{n-1}\leq t_{n-1}\cap\{\sum_{i=1}^{n-1}x_i<T\} \cap \{\sum_{i=1}^{n}x_i>T\} \right)}{\mathbb{P}\left(\{\sum_{i=1}^{n-1}x_i<T\} \cap \{\sum_{i=1}^{n}x_i>T\} \right)}\\ &=\frac{1}{c}{\mathbb{P}\left(x_1\leq t_1,\cdots,x_{n-1}\leq t_{n-1}\cap\{\sum_{i=1}^{n-1}x_i<T\} \cap \{\sum_{i=1}^{n}x_i>T\} \right)}\\ &=\frac{1}{c}\int_{x_1=0}^{min(T,t_1)}\int_{x_2=0}^{min(T-x_1,t_2)}\cdots\int_{x_n=T-x_1-x_2\cdots-x_{n-1}}^{\infty} p(x_1)p(x_2) \cdots p(x_n) d_{x_1} \cdots d_{x_n} \end{align} To obtain the pdf, I have to take derivatives with respect to $t_1,\cdots,t_n$. I don't think it is going to be a clean result. Any idea?","For a point process with independent and identically distributed (i.i.d) inter-renewals, with distribution $p(x)$, we observed $N$ points on $[0,T]$. What is the joint probability distribution function of random variables $x_1,x_2,\cdots,x_n$? Obviously, $x_1+x_2+\cdots+x_n=T$ For a Poisson point process, it is simply the probability distribution function of the event that we put $N$ points independently and randomly with respect to uniform distribution on $[0,T]$. For a renewal point process, where the inter-renewal times are i.i.d with distribution $p(x)$ the distribution function is: \begin{align} &F_{X_1,\dots,X_n}\left(t_1,\cdots,t_2\right)\\&=\mathbb{P}\left(x_1\leq t_1,\cdots,x_{n-1}\leq t_{n-1}|\{\sum_{i=1}^{n-1}x_i<T\} \cap \{\sum_{i=1}^{n}x_i>T\} \right)\\ &=\frac{\mathbb{P}\left(x_1\leq t_1,\cdots,x_{n-1}\leq t_{n-1}\cap\{\sum_{i=1}^{n-1}x_i<T\} \cap \{\sum_{i=1}^{n}x_i>T\} \right)}{\mathbb{P}\left(\{\sum_{i=1}^{n-1}x_i<T\} \cap \{\sum_{i=1}^{n}x_i>T\} \right)}\\ &=\frac{1}{c}{\mathbb{P}\left(x_1\leq t_1,\cdots,x_{n-1}\leq t_{n-1}\cap\{\sum_{i=1}^{n-1}x_i<T\} \cap \{\sum_{i=1}^{n}x_i>T\} \right)}\\ &=\frac{1}{c}\int_{x_1=0}^{min(T,t_1)}\int_{x_2=0}^{min(T-x_1,t_2)}\cdots\int_{x_n=T-x_1-x_2\cdots-x_{n-1}}^{\infty} p(x_1)p(x_2) \cdots p(x_n) d_{x_1} \cdots d_{x_n} \end{align} To obtain the pdf, I have to take derivatives with respect to $t_1,\cdots,t_n$. I don't think it is going to be a clean result. Any idea?",,"['probability', 'probability-theory', 'probability-distributions', 'stochastic-processes', 'stochastic-calculus']"
6,How to prove this inequality (already verified by numerical simulation)?,How to prove this inequality (already verified by numerical simulation)?,,"I have a conjecture which has been verified extensively by simulation. The conjecture is as follows: $\forall t \in [0, 1], \alpha \in [0,1]$, and positive real sequences $\{p\}_{i:1,\dots,n}, $, $\{q\}_{i:1,\dots,n}, $, it holds that $$\sum_{i,j} (t p_i p_j + \bar{t} q_i q_j)^{1+\alpha} \geq \sum_{i,j} (t p_i + \bar{t} q_i)^{1+\alpha} (t p_j + \bar{t} q_j)^{1+\alpha},$$ where $\bar{t} = 1-t$. For the case $\alpha = 0$ and $\alpha = 1$, it's easy to prove the conjecture, by expanding the terms and using convexity and Jensen's inequality. For example, when $\alpha = 0$, one can use the fact that $(\sum_i x_i)^2$ is convex. However, I don't have any clue of how to prove it for the more general case where $\alpha \in (0,1)$. Does anyone have any idea of how to prove the general case (possibly by leveraging the convexity of the terms)? Thanks in advance!","I have a conjecture which has been verified extensively by simulation. The conjecture is as follows: $\forall t \in [0, 1], \alpha \in [0,1]$, and positive real sequences $\{p\}_{i:1,\dots,n}, $, $\{q\}_{i:1,\dots,n}, $, it holds that $$\sum_{i,j} (t p_i p_j + \bar{t} q_i q_j)^{1+\alpha} \geq \sum_{i,j} (t p_i + \bar{t} q_i)^{1+\alpha} (t p_j + \bar{t} q_j)^{1+\alpha},$$ where $\bar{t} = 1-t$. For the case $\alpha = 0$ and $\alpha = 1$, it's easy to prove the conjecture, by expanding the terms and using convexity and Jensen's inequality. For example, when $\alpha = 0$, one can use the fact that $(\sum_i x_i)^2$ is convex. However, I don't have any clue of how to prove it for the more general case where $\alpha \in (0,1)$. Does anyone have any idea of how to prove the general case (possibly by leveraging the convexity of the terms)? Thanks in advance!",,"['probability', 'inequality', 'convex-analysis', 'information-theory']"
7,Is this some entropy I haven't heard of?,Is this some entropy I haven't heard of?,,For a discrete finite probability distribution $p(s)$ the function $$\sum\limits_s p(s)\log ^2 p(s)$$ looks like the Shannon entropy but has a square on the $\log$. Is there a name for this? Or it is meaningless?,For a discrete finite probability distribution $p(s)$ the function $$\sum\limits_s p(s)\log ^2 p(s)$$ looks like the Shannon entropy but has a square on the $\log$. Is there a name for this? Or it is meaningless?,,"['probability', 'probability-distributions', 'entropy']"
8,Four of a kind -dilemma,Four of a kind -dilemma,,"CONDITIONS Imagine, that we have five standard 52-card decks . We will now mix all these five decks into a one ( afterwards ""big deck"" , 260 cards in total ). Now our task is to turn all the cards open, one card at a time, until we can form "" four of a kind "" from these cards that have been turned open ( ""dealt"" ). QUESTION: What is the maximum number of these cards to be dealt (in worst case scenario), where we still can not form any "" four of a kind "" that consists of: 1) any four cards that have the same numeric value (example ""8♥ 8♥ 8♣ 8♣"" or ""J♥ J♣ J♣ J♣""); 2) four completely suited cards (example ""K♥ K♥ K♥ K♥"" or ""7♣ 7♣ 7♣ 7♣""). 3) four completely offsuited cards (example ""K♥ K♦ K♣ K♠"" or ""7♣ 7♥ 7♦ 7♠""). The suite of those cards that doesn't form a four of a kind is irrelevant (they can be both suited or offsuited ). My anwsers 1) any four of a kind We simply run three of a kinds for all 13 different numeric values (A, 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K and suit doesn't matter). We now get a total of 39 cards. $$ (3 \cdot 13)  = 39 $$ The 40th card to be dealt needs to gives us one four of a kind no matter what card it is. So my anwser is 39 (40th card will give us four of a kind - suited or offsuited ). Note that the number of decks used is irrelevant for the assigment (assuming that we use at least one complete deck of cards). 2) four of a kind - suited Here we need to avoid any four of a kind that consists of the same suit. What were gonna do is to deal three cards from all suits per numeric value (A, 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K) - three of each, because the whole idea is to avoid the 4th. Now we have a total of 12 cards per numeric value like this A♥ A♥ A♥, A♦ A♦ A♦, A♣ A♣ A♣, A♠ A♠ A♠ and 2♥ 2♥ 2♥, 2♦ 2♦ 2♦, 2♣ 2♣ 2♣, 2♠ 2♠ 2♠ ect. As there are 13 different numeric values (A, 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K), there is a maximum of $12 * 13 = 156$ different cards without any four of a kind: same suit . Let's deal these cards described above and see where we stand. The next one, let's make it a six of clubs (6♠), is the fourth six of clubs dealt, so the result is 156. No matter what the card to be dealt is, it will inevitably be the 4th of that specific suite and value. Since the 157th card dealt will always give us four of a kind that consists of four same suit and value , we have now reached the maximum. Note that for this task there must be at least four decks used. Any added deck after four doesn't effect at all; the limit comes from the fact that there are only 13 different numeric values per deck. 3) four of a kind - offsuited We deal all cards from all 5 decks, except one suit (in this example we exclude spade ♠): A♥ A♥ A♥ A♥ A♥, A♦ A♦ A♦ A♦ A♦, A♣ A♣ A♣ A♣ A♣ and 2♥ 2♥ 2♥ 2♥ 2♥, 2♦ 2♦ 2♦ 2♦ 2♦, 2♣ 2♣ 2♣ 2♣ 2♣ ect., so we got a total of 15 individual cards for each numeric value. As we have 13 different numeric values in five decks, and all the spades will be taken out of the deck, we are left with a total of 195 cards. There is also an opposite way to do this. First let's calculate $$ X \cdot Y  = 65 $$ Where X is the number of spades per deck ( 13 ) and Y is the number of the decks used ( 5 ). We now substitute 65 from original 260 cards and our anwser is 195. We can also do this also by $$ (A \cdot B) \cdot C = 195 $$ where A is number of one suit per deck, B is number of suits minus one suit  (not including one suit) and C is the number of decks used. The anwser to this is 195 cards, that still won't make any offsuited four of a kind . Changing the number of decks used makes a big difference in the math (four decks will do $(13 * 3) * 4 = 156$ for example). Here is the previous equation again: $$ (A + B + C) \cdot X  = 195 $$ Where A is number hearts per value in 5 decks (5), B is number of diamonds per value in 5 decks (5), C is number of clubs per value in 5 decks (5) and Y is number of different numeric values in a card deck (13). Given that we now have used all hearts , diamonds and clubs in the deck, the next card dealt, 196th in order, must be spade, because there are no other cards left in our big deck. The 196th card creates the first four of a kind specified in article 3) . Note that the more decks we use, the more cards of the same suite we can deal. If we use only one deck, the result is the same as in article 1) : 39 cards - 40th will make the first four of a kind, offsuited of course up to three decks, because there just cannot be four ace of spades in three standard 52-card decks. Am I right with these? P.S. a lot of edits are made to improve the structure and readability of the question.","CONDITIONS Imagine, that we have five standard 52-card decks . We will now mix all these five decks into a one ( afterwards ""big deck"" , 260 cards in total ). Now our task is to turn all the cards open, one card at a time, until we can form "" four of a kind "" from these cards that have been turned open ( ""dealt"" ). QUESTION: What is the maximum number of these cards to be dealt (in worst case scenario), where we still can not form any "" four of a kind "" that consists of: 1) any four cards that have the same numeric value (example ""8♥ 8♥ 8♣ 8♣"" or ""J♥ J♣ J♣ J♣""); 2) four completely suited cards (example ""K♥ K♥ K♥ K♥"" or ""7♣ 7♣ 7♣ 7♣""). 3) four completely offsuited cards (example ""K♥ K♦ K♣ K♠"" or ""7♣ 7♥ 7♦ 7♠""). The suite of those cards that doesn't form a four of a kind is irrelevant (they can be both suited or offsuited ). My anwsers 1) any four of a kind We simply run three of a kinds for all 13 different numeric values (A, 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K and suit doesn't matter). We now get a total of 39 cards. $$ (3 \cdot 13)  = 39 $$ The 40th card to be dealt needs to gives us one four of a kind no matter what card it is. So my anwser is 39 (40th card will give us four of a kind - suited or offsuited ). Note that the number of decks used is irrelevant for the assigment (assuming that we use at least one complete deck of cards). 2) four of a kind - suited Here we need to avoid any four of a kind that consists of the same suit. What were gonna do is to deal three cards from all suits per numeric value (A, 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K) - three of each, because the whole idea is to avoid the 4th. Now we have a total of 12 cards per numeric value like this A♥ A♥ A♥, A♦ A♦ A♦, A♣ A♣ A♣, A♠ A♠ A♠ and 2♥ 2♥ 2♥, 2♦ 2♦ 2♦, 2♣ 2♣ 2♣, 2♠ 2♠ 2♠ ect. As there are 13 different numeric values (A, 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K), there is a maximum of $12 * 13 = 156$ different cards without any four of a kind: same suit . Let's deal these cards described above and see where we stand. The next one, let's make it a six of clubs (6♠), is the fourth six of clubs dealt, so the result is 156. No matter what the card to be dealt is, it will inevitably be the 4th of that specific suite and value. Since the 157th card dealt will always give us four of a kind that consists of four same suit and value , we have now reached the maximum. Note that for this task there must be at least four decks used. Any added deck after four doesn't effect at all; the limit comes from the fact that there are only 13 different numeric values per deck. 3) four of a kind - offsuited We deal all cards from all 5 decks, except one suit (in this example we exclude spade ♠): A♥ A♥ A♥ A♥ A♥, A♦ A♦ A♦ A♦ A♦, A♣ A♣ A♣ A♣ A♣ and 2♥ 2♥ 2♥ 2♥ 2♥, 2♦ 2♦ 2♦ 2♦ 2♦, 2♣ 2♣ 2♣ 2♣ 2♣ ect., so we got a total of 15 individual cards for each numeric value. As we have 13 different numeric values in five decks, and all the spades will be taken out of the deck, we are left with a total of 195 cards. There is also an opposite way to do this. First let's calculate $$ X \cdot Y  = 65 $$ Where X is the number of spades per deck ( 13 ) and Y is the number of the decks used ( 5 ). We now substitute 65 from original 260 cards and our anwser is 195. We can also do this also by $$ (A \cdot B) \cdot C = 195 $$ where A is number of one suit per deck, B is number of suits minus one suit  (not including one suit) and C is the number of decks used. The anwser to this is 195 cards, that still won't make any offsuited four of a kind . Changing the number of decks used makes a big difference in the math (four decks will do $(13 * 3) * 4 = 156$ for example). Here is the previous equation again: $$ (A + B + C) \cdot X  = 195 $$ Where A is number hearts per value in 5 decks (5), B is number of diamonds per value in 5 decks (5), C is number of clubs per value in 5 decks (5) and Y is number of different numeric values in a card deck (13). Given that we now have used all hearts , diamonds and clubs in the deck, the next card dealt, 196th in order, must be spade, because there are no other cards left in our big deck. The 196th card creates the first four of a kind specified in article 3) . Note that the more decks we use, the more cards of the same suite we can deal. If we use only one deck, the result is the same as in article 1) : 39 cards - 40th will make the first four of a kind, offsuited of course up to three decks, because there just cannot be four ace of spades in three standard 52-card decks. Am I right with these? P.S. a lot of edits are made to improve the structure and readability of the question.",,"['probability', 'card-games']"
9,math problem probability,math problem probability,,"Let $\{X_n\}$ be a sequence of independent identically distributed random variables with mean $\mu$ and finite variance. Prove that: $$T_n= \binom{n}{2}^{-1} \sum_{1\leq i< j\leq n} {X_{i}X_{j}} \overset{P}{\rightarrow} \mu ^2 \text{ when } n \rightarrow \infty. $$ Indication: Prove the quadratic mean convergence. I try to do this: $$\lim_{n\rightarrow \infty }E[(T_{n} - \mu ^2)^2] = 0?$$ I have calculated the mean of $T_{n}$ and I think $E(T_{n})=\mu^2$ so, $$E[(T_{n} - \mu ^2)^2]=E(T_{n}^2)-E(T_{n})^2 = \mu^4-(\mu^2)^2=0. $$ The problem is that I don't use the fact $n\rightarrow \infty $. Thank you!!!!!","Let $\{X_n\}$ be a sequence of independent identically distributed random variables with mean $\mu$ and finite variance. Prove that: $$T_n= \binom{n}{2}^{-1} \sum_{1\leq i< j\leq n} {X_{i}X_{j}} \overset{P}{\rightarrow} \mu ^2 \text{ when } n \rightarrow \infty. $$ Indication: Prove the quadratic mean convergence. I try to do this: $$\lim_{n\rightarrow \infty }E[(T_{n} - \mu ^2)^2] = 0?$$ I have calculated the mean of $T_{n}$ and I think $E(T_{n})=\mu^2$ so, $$E[(T_{n} - \mu ^2)^2]=E(T_{n}^2)-E(T_{n})^2 = \mu^4-(\mu^2)^2=0. $$ The problem is that I don't use the fact $n\rightarrow \infty $. Thank you!!!!!",,"['probability', 'probability-theory']"
10,Distribution of monochromatic edge counts in 2-colored square lattices,Distribution of monochromatic edge counts in 2-colored square lattices,,"The question is how to compute in general the following: given integers $n$ and $k$, how many 2-colored $n \times n$ lattices have $k$ monochromatic edges. There are no diagonal edges. Observations : I noticed that when you do a plot of the distribution (normalized so that it's a probability distribution) for $n=10$, you get something which is approximately Gaussian in $k$. I thought that it might have some relationship with the Central Limit Theorem, which is used to explain why Gaussians show up in so many places. The CLT says that the probability distribution of the average of a large number of IID random variables is Gaussian. The random variables in this case act as indicators of whether an edge is monochromatic or not. This then suggests a binomial approximation where the Bernoulli trials correspond to checking if an edge is monochromatic or not. I tried this out and it works well. The insight from these approximations is that whether some edge is monochromatic or not is largely independent of other edges. Of course they actually are dependent so these approximations can't be perfect. I imagine, and I haven't had a chance to test this, that the graphs of log probabilities of the actual probability distribution and its binomial approximation diverge as you go along the tails. So can anyone give an exact formula?","The question is how to compute in general the following: given integers $n$ and $k$, how many 2-colored $n \times n$ lattices have $k$ monochromatic edges. There are no diagonal edges. Observations : I noticed that when you do a plot of the distribution (normalized so that it's a probability distribution) for $n=10$, you get something which is approximately Gaussian in $k$. I thought that it might have some relationship with the Central Limit Theorem, which is used to explain why Gaussians show up in so many places. The CLT says that the probability distribution of the average of a large number of IID random variables is Gaussian. The random variables in this case act as indicators of whether an edge is monochromatic or not. This then suggests a binomial approximation where the Bernoulli trials correspond to checking if an edge is monochromatic or not. I tried this out and it works well. The insight from these approximations is that whether some edge is monochromatic or not is largely independent of other edges. Of course they actually are dependent so these approximations can't be perfect. I imagine, and I haven't had a chance to test this, that the graphs of log probabilities of the actual probability distribution and its binomial approximation diverge as you go along the tails. So can anyone give an exact formula?",,"['probability', 'combinatorics', 'probability-distributions']"
11,probability on countable infinite sets,probability on countable infinite sets,,"My question relates to probabilities on countable infinite sets.  For example, what is the probability of choosing an even number from the positive integers.  Believe it or not I am interested in this question from a practical standpoint.  I am writing a paper on Boltzmann's brains (Brains that occur spontaneously from the vacuum in De Sitter space). Specifically the problem is, is it more likely that I am a Boltzmann brain or a regular brain if space is infinite.  There are of course no good calculations about what the odds of a Boltzmann brain identical to mine are vs. the odds of an identical brain evolving on a twin earth (my money would be that a copy on a twin earth is much more likely).  But would the specific odds matter if space is infinite and there are an infinite number of both, regardless of which is more common?  I know the answer would of course depend on your philosophy of probability (frequency, Bayesian, etc.) so I would be interested in the answer for each of the main theories in probability.","My question relates to probabilities on countable infinite sets.  For example, what is the probability of choosing an even number from the positive integers.  Believe it or not I am interested in this question from a practical standpoint.  I am writing a paper on Boltzmann's brains (Brains that occur spontaneously from the vacuum in De Sitter space). Specifically the problem is, is it more likely that I am a Boltzmann brain or a regular brain if space is infinite.  There are of course no good calculations about what the odds of a Boltzmann brain identical to mine are vs. the odds of an identical brain evolving on a twin earth (my money would be that a copy on a twin earth is much more likely).  But would the specific odds matter if space is infinite and there are an infinite number of both, regardless of which is more common?  I know the answer would of course depend on your philosophy of probability (frequency, Bayesian, etc.) so I would be interested in the answer for each of the main theories in probability.",,"['probability', 'infinity']"
12,Expected value of best possible earnings in a fair betting game.,Expected value of best possible earnings in a fair betting game.,,"You are playing a fair betting game. i.e. Every round you win 1 dollar with probability 0.5 or lose 1 dollar with probability 0.5. You play a total of $T$ rounds in a game. Suppose in hindsight, you always left the game when you had the best possible earnings for that game, then what are your expected earnings? The answer to this is O($\sqrt{T}$). I still can't figure out why. Any suggestions?","You are playing a fair betting game. i.e. Every round you win 1 dollar with probability 0.5 or lose 1 dollar with probability 0.5. You play a total of $T$ rounds in a game. Suppose in hindsight, you always left the game when you had the best possible earnings for that game, then what are your expected earnings? The answer to this is O($\sqrt{T}$). I still can't figure out why. Any suggestions?",,"['probability', 'computational-complexity']"
13,"Does $\Bbb E[X|Z]=\Bbb E[Y|Z]$ if $X,Y$ are identically distributed random variable?",Does  if  are identically distributed random variable?,"\Bbb E[X|Z]=\Bbb E[Y|Z] X,Y","Does $\Bbb E[X|Z]=\Bbb E[Y|Z]$ if $X,Y$ are identically distributed random variable, where $Z$ is a third random variable? Thank you!","Does $\Bbb E[X|Z]=\Bbb E[Y|Z]$ if $X,Y$ are identically distributed random variable, where $Z$ is a third random variable? Thank you!",,['probability']
14,Show the number of triangles in a random graph follows Poisson,Show the number of triangles in a random graph follows Poisson,,"Define a random graph $\Bbb G(n,p)$ as a graph with $n$ vertices where an edge between two vertices occur with probability $p$. Let $X$ be the number of triangles in the random graph, show that if $p=c/n$ ($c$ is a non-negative constant), then $X$ asymptotically follows Poisson distribution as $n \to \infty$. Can anyone provide some help? Thank you!","Define a random graph $\Bbb G(n,p)$ as a graph with $n$ vertices where an edge between two vertices occur with probability $p$. Let $X$ be the number of triangles in the random graph, show that if $p=c/n$ ($c$ is a non-negative constant), then $X$ asymptotically follows Poisson distribution as $n \to \infty$. Can anyone provide some help? Thank you!",,"['probability', 'graph-theory']"
15,Bernstein-type inequality for heavy-tailed random variables,Bernstein-type inequality for heavy-tailed random variables,,"It is known that for independent sub-exponential random variables, the following Bernstein-type inequality holds: \begin{align} \mathbb{P}\biggl(\biggl|  \sum_{i=1}^N a_i X_i\biggr| >t \biggr) \leq 2 \exp\left[ -c\min \left(\frac{t^2}{K^2 \| \vec{a}\|_2^2}, \frac{t}{ K \| \vec{a}\|_{\infty}} \right)\right], \end{align} where $K = \max \| X_1\|_{\psi_1}$ and $\vec{a}\in\mathbb{R}^N$. I wonder if similar concentration inequality holds for heavy-tailed random variables where $X_i$ satisfies $\mathbb{P}(X_i > t) \leq C\exp(c t^{-\alpha}) $ for $\alpha\in(0,1)$.","It is known that for independent sub-exponential random variables, the following Bernstein-type inequality holds: \begin{align} \mathbb{P}\biggl(\biggl|  \sum_{i=1}^N a_i X_i\biggr| >t \biggr) \leq 2 \exp\left[ -c\min \left(\frac{t^2}{K^2 \| \vec{a}\|_2^2}, \frac{t}{ K \| \vec{a}\|_{\infty}} \right)\right], \end{align} where $K = \max \| X_1\|_{\psi_1}$ and $\vec{a}\in\mathbb{R}^N$. I wonder if similar concentration inequality holds for heavy-tailed random variables where $X_i$ satisfies $\mathbb{P}(X_i > t) \leq C\exp(c t^{-\alpha}) $ for $\alpha\in(0,1)$.",,"['probability', 'statistics', 'concentration-of-measure']"
16,Applying Kelly Criterion to profit/loss bet,Applying Kelly Criterion to profit/loss bet,,"In a financial derivative trading situation, there are two outcomes to a bet (win/lose), but I don't necessarily lose my entire stake if I lose the bet, because I can buy my way out of the bet, taking a net loss. Nonetheless, I have to bet the entire stake in order to play. However, if I win the bet, I receive my stake plus a fraction as a gain. For example, let's say: my bankroll is \$100 if I win, I will gain 5% of my stake if I lose, I will lose 30% of my stake I expect to win 91% of the time What I want to determine is how much of my bankroll to put at stake. According the Wikipedia article, I should be using this formula: $$ f*=(\frac pa)-(\frac qb) $$ where: p = probability of winning q = probability of losing ($\equiv 1-p$) a = amount of loss b = amount of gain f* = percentage of bankroll to bet However, when I do the math: $$ f*=(\frac{.91}{.3})-(\frac{.09}{.05})={3.0\overline3}-{1.8}={1.2\overline3} $$ That's obviously very wrong. I'm expecting a number where $0 \leq f* \leq 1$. A colleague provided a function that he claims is a Kelly formula, and it gives values that I would expect, but I don't understand how he got the formula he uses. $$ k*=p-\frac q{(\frac ba)} $$ The math works closer to what I would expect: $$ k*={0.91}-\frac {0.09}{(\frac {.05}{.3})}={0.91}-\frac {0.09}{.1\overline6}={0.91}-{.54}={.37} $$ So I would put 37% of my bankroll at stake, which seems about right. What I don't understand is how my colleague's math works to the Kelly Criterion, or how I misapplied the formula from Wikipedia.","In a financial derivative trading situation, there are two outcomes to a bet (win/lose), but I don't necessarily lose my entire stake if I lose the bet, because I can buy my way out of the bet, taking a net loss. Nonetheless, I have to bet the entire stake in order to play. However, if I win the bet, I receive my stake plus a fraction as a gain. For example, let's say: my bankroll is \$100 if I win, I will gain 5% of my stake if I lose, I will lose 30% of my stake I expect to win 91% of the time What I want to determine is how much of my bankroll to put at stake. According the Wikipedia article, I should be using this formula: $$ f*=(\frac pa)-(\frac qb) $$ where: p = probability of winning q = probability of losing ($\equiv 1-p$) a = amount of loss b = amount of gain f* = percentage of bankroll to bet However, when I do the math: $$ f*=(\frac{.91}{.3})-(\frac{.09}{.05})={3.0\overline3}-{1.8}={1.2\overline3} $$ That's obviously very wrong. I'm expecting a number where $0 \leq f* \leq 1$. A colleague provided a function that he claims is a Kelly formula, and it gives values that I would expect, but I don't understand how he got the formula he uses. $$ k*=p-\frac q{(\frac ba)} $$ The math works closer to what I would expect: $$ k*={0.91}-\frac {0.09}{(\frac {.05}{.3})}={0.91}-\frac {0.09}{.1\overline6}={0.91}-{.54}={.37} $$ So I would put 37% of my bankroll at stake, which seems about right. What I don't understand is how my colleague's math works to the Kelly Criterion, or how I misapplied the formula from Wikipedia.",,"['probability', 'gambling']"
17,"How to prove that $cov(f(X_1, X_2, \ldots ,X_n), g(X_1, X_2, \ldots ,X_n)) \geq 0$ for $X_1, \ldots, X_n$ independent and $f,g$ increasing?",How to prove that  for  independent and  increasing?,"cov(f(X_1, X_2, \ldots ,X_n), g(X_1, X_2, \ldots ,X_n)) \geq 0 X_1, \ldots, X_n f,g","I read in a talk that a consequence of the FKG inequality is that: $$ cov(f(X_1, X_2, \ldots ,X_n), g(X_1, X_2, \ldots ,X_n)) \geq 0 $$  for $X_1, X_2, \ldots ,X_n$ independent and $f,g$ increasing in each coordinate, that is, in example, $f(X_1, X_2, \ldots ,X_n)$ is increasing in $X_1$ for fixed $X_2, \ldots, X_n$. However, I read in a paper that this can be proved using a bounding theorem, ie., Chernoff, Chebychev, etc. Does anyone have any idea how to do this It states that one can prove the two variable case and then use induction, but I am failing how to see this is proven in even the two variable case. Thanks!","I read in a talk that a consequence of the FKG inequality is that: $$ cov(f(X_1, X_2, \ldots ,X_n), g(X_1, X_2, \ldots ,X_n)) \geq 0 $$  for $X_1, X_2, \ldots ,X_n$ independent and $f,g$ increasing in each coordinate, that is, in example, $f(X_1, X_2, \ldots ,X_n)$ is increasing in $X_1$ for fixed $X_2, \ldots, X_n$. However, I read in a paper that this can be proved using a bounding theorem, ie., Chernoff, Chebychev, etc. Does anyone have any idea how to do this It states that one can prove the two variable case and then use induction, but I am failing how to see this is proven in even the two variable case. Thanks!",,"['probability', 'probability-theory', 'statistics']"
18,Distribution of sum of $n$ i.i.d. symmetric Pareto distributed random variables,Distribution of sum of  i.i.d. symmetric Pareto distributed random variables,n,"Let $X$ be a random variable which follows the symmetric Pareto distribution . For a fix, real parameter set $\alpha > 0$ and $L>0$, its PDF is defined as $$ p_X(x) =    \left\{   \begin{array}{ll}     \frac{1}{2}\alpha L^\alpha |x|^{-\alpha-1}  & |x| \geq L \\     0 & \mbox{otherwise.}   \end{array}   \right. $$ If possible, I would like to derive PDF $p_S(x)$, where $S = \sum_{i=1}^n X_i$. Each $X_i$ is i.i.d. according to the PDF given above. I'm particularly interested in the tail distribution $\overline{F}_S(x) = Pr(S > x)$. My approach is to calculate the $n$-th power of the CF $\varphi_X(\omega)$ and calculate its Fourier transform to obtain $p_S(x)$. However, I'm uncertain if this procedure is valid here. Is it? Do any restrictions emerge inevitably w.r.t. $\alpha$? I observe some ""unhandy terms"". Is it possible to obtain $p_S(x)$ analytically and in closed form? First of all, the CF of a (one sided) Pareto distribution is given by $\varphi_Y(\omega) = \alpha L^\alpha (-i\omega)^\alpha\Gamma(-\alpha, -i\omega L)$. Is the upper incomplete gamma function well defined for a real negative exponent and imaginary integral bounds? I have \begin{eqnarray} \varphi_S(\omega) &=& \frac{1}{2^n}(\varphi_Y(\omega) + \varphi_Y(-\omega))^n\\ &=& \left(\alpha L^\alpha\right)^n|\omega|^{\alpha n}|\Gamma(-\alpha, i\omega L)|^{n} \cos \left( \arg \left( \Gamma(-\alpha, i\omega L)\right)+ \sigma(\omega)\frac{\alpha\pi}{2}\right)\;, \end{eqnarray} where $\sigma(\cdot)$ is the signum function. Alternatively, I have \begin{eqnarray} \varphi_S(\omega) &=& \left( \alpha L^\alpha \int\limits_{L}^\infty x^{-\alpha-1} \cos \left(\omega x \right) \,\mathrm{d}x \right)^n. \end{eqnarray} Does this help in any way to get $p_S(x)$ or $\overline{F}_S(x)$? How could I proceed? P.S.: In my case, n is a power of 2. If the inversion has no chance to succeed for a fix $n$, what would be an appropriate approach to investigate $\lim_{n \to \infty} \overline{F}_S(x)$?","Let $X$ be a random variable which follows the symmetric Pareto distribution . For a fix, real parameter set $\alpha > 0$ and $L>0$, its PDF is defined as $$ p_X(x) =    \left\{   \begin{array}{ll}     \frac{1}{2}\alpha L^\alpha |x|^{-\alpha-1}  & |x| \geq L \\     0 & \mbox{otherwise.}   \end{array}   \right. $$ If possible, I would like to derive PDF $p_S(x)$, where $S = \sum_{i=1}^n X_i$. Each $X_i$ is i.i.d. according to the PDF given above. I'm particularly interested in the tail distribution $\overline{F}_S(x) = Pr(S > x)$. My approach is to calculate the $n$-th power of the CF $\varphi_X(\omega)$ and calculate its Fourier transform to obtain $p_S(x)$. However, I'm uncertain if this procedure is valid here. Is it? Do any restrictions emerge inevitably w.r.t. $\alpha$? I observe some ""unhandy terms"". Is it possible to obtain $p_S(x)$ analytically and in closed form? First of all, the CF of a (one sided) Pareto distribution is given by $\varphi_Y(\omega) = \alpha L^\alpha (-i\omega)^\alpha\Gamma(-\alpha, -i\omega L)$. Is the upper incomplete gamma function well defined for a real negative exponent and imaginary integral bounds? I have \begin{eqnarray} \varphi_S(\omega) &=& \frac{1}{2^n}(\varphi_Y(\omega) + \varphi_Y(-\omega))^n\\ &=& \left(\alpha L^\alpha\right)^n|\omega|^{\alpha n}|\Gamma(-\alpha, i\omega L)|^{n} \cos \left( \arg \left( \Gamma(-\alpha, i\omega L)\right)+ \sigma(\omega)\frac{\alpha\pi}{2}\right)\;, \end{eqnarray} where $\sigma(\cdot)$ is the signum function. Alternatively, I have \begin{eqnarray} \varphi_S(\omega) &=& \left( \alpha L^\alpha \int\limits_{L}^\infty x^{-\alpha-1} \cos \left(\omega x \right) \,\mathrm{d}x \right)^n. \end{eqnarray} Does this help in any way to get $p_S(x)$ or $\overline{F}_S(x)$? How could I proceed? P.S.: In my case, n is a power of 2. If the inversion has no chance to succeed for a fix $n$, what would be an appropriate approach to investigate $\lim_{n \to \infty} \overline{F}_S(x)$?",,"['probability', 'probability-distributions', 'problem-solving', 'stochastic-integrals', 'stochastic-analysis']"
19,A time reversible Markov chain problem on urns,A time reversible Markov chain problem on urns,,"Question: (Ross Probability Models , Ch. 4, Ex. 70) A total of $m$ white balls and $m$ black balls are distributed into two urns such that each urn contains $m$ balls. At each stage, a ball is selected from either urn, and the selected balls are switched. Let $X_n$ denote the number of black balls in the first urn after the $n$th switch. Give (a) the transition probabilities, (b) find/guess the limiting probabilities of the chain without computation, and (c) actually compute the limiting probabilities and show that the chain is time reversible. Problem: I don't know how to reason through (b), and I can't find a simple form solution for (c). Attempt: I will first do part (a). Let $P$ be the transition matrix and $\pi$ the limiting probabilities vector. Then $$P_{ii}=2\left(\frac{i}{m}\right)\left(\frac{m-i}{m}\right)$$ Since we choose $i$ blacks from the first urn and $m-i$ blacks from the second, or vice versa regarding whites. $$P_{i,i+1}=\left(\frac{m-i}{m}\right)^2$$ Since we choose from the $m-i$ whites from the first urn and $m-i$ blacks from the second. Similarly, then, $$P_{i,i-1}=\left(\frac{i}{m}\right)^2$$ Noting that $P_{i,i-1}+P_{ii}+P_{i,i+1}=1$, all other entries in $P$ for each $i$ is $0$. Now I will solve (c). The chain is time reversible because for any states $i,j$, it's not possible to witness $i$ followed by $j$ twice without witnessing $j$ followed by $i$ someplace in between. Thus, we have $$\pi_iP_{i,i-1}=\pi_{i-1}P_{i-1,i}\Longrightarrow\pi_i=\pi_{i-1}\frac{P_{i-1,i}}{P_{i,i-1}}=\pi_{i-1}\left(\frac{m-i+1}{i}\right)^2$$ Then $$\pi_i=\pi_0\prod_{j=1}^i\left(\frac{m-j+1}{j}\right)^2$$ And $$\sum_{k=0}^m\pi_k=\pi_0\sum_{k=0}^m\prod_{j=1}^k\left(\frac{m-j+1}{j}\right)^2=1$$ Which gives me a very messy solution for $\pi_i$ for all $i$. Unfortunately, I can't think of any way to simplify this with the squared term, but for various reasons I suspect that the solution has a nice representation. Update: We have that $$\pi_0\sum_{k=0}^m\prod_{j=1}^k\left(\frac{m-j+1}{j}\right)^2=\pi_0\sum_{k=0}^m{m\choose k}^2=\pi_0\sum_{k=0}^m{m\choose k}{m\choose{m-k}}=\pi_0{{2m}\choose m}=1$$ Or $$\pi_0=\frac1{{2m}\choose m}$$ And $$\pi_i=\pi_0\prod_{j=1}^i\left(\frac{m-j+1}{j}\right)^2=\pi_0{m\choose i}^2=\pi_0{m\choose i}{m\choose{m-i}}=\frac{{m\choose i}{m\choose{m-i}}}{{2m}\choose m}$$","Question: (Ross Probability Models , Ch. 4, Ex. 70) A total of $m$ white balls and $m$ black balls are distributed into two urns such that each urn contains $m$ balls. At each stage, a ball is selected from either urn, and the selected balls are switched. Let $X_n$ denote the number of black balls in the first urn after the $n$th switch. Give (a) the transition probabilities, (b) find/guess the limiting probabilities of the chain without computation, and (c) actually compute the limiting probabilities and show that the chain is time reversible. Problem: I don't know how to reason through (b), and I can't find a simple form solution for (c). Attempt: I will first do part (a). Let $P$ be the transition matrix and $\pi$ the limiting probabilities vector. Then $$P_{ii}=2\left(\frac{i}{m}\right)\left(\frac{m-i}{m}\right)$$ Since we choose $i$ blacks from the first urn and $m-i$ blacks from the second, or vice versa regarding whites. $$P_{i,i+1}=\left(\frac{m-i}{m}\right)^2$$ Since we choose from the $m-i$ whites from the first urn and $m-i$ blacks from the second. Similarly, then, $$P_{i,i-1}=\left(\frac{i}{m}\right)^2$$ Noting that $P_{i,i-1}+P_{ii}+P_{i,i+1}=1$, all other entries in $P$ for each $i$ is $0$. Now I will solve (c). The chain is time reversible because for any states $i,j$, it's not possible to witness $i$ followed by $j$ twice without witnessing $j$ followed by $i$ someplace in between. Thus, we have $$\pi_iP_{i,i-1}=\pi_{i-1}P_{i-1,i}\Longrightarrow\pi_i=\pi_{i-1}\frac{P_{i-1,i}}{P_{i,i-1}}=\pi_{i-1}\left(\frac{m-i+1}{i}\right)^2$$ Then $$\pi_i=\pi_0\prod_{j=1}^i\left(\frac{m-j+1}{j}\right)^2$$ And $$\sum_{k=0}^m\pi_k=\pi_0\sum_{k=0}^m\prod_{j=1}^k\left(\frac{m-j+1}{j}\right)^2=1$$ Which gives me a very messy solution for $\pi_i$ for all $i$. Unfortunately, I can't think of any way to simplify this with the squared term, but for various reasons I suspect that the solution has a nice representation. Update: We have that $$\pi_0\sum_{k=0}^m\prod_{j=1}^k\left(\frac{m-j+1}{j}\right)^2=\pi_0\sum_{k=0}^m{m\choose k}^2=\pi_0\sum_{k=0}^m{m\choose k}{m\choose{m-k}}=\pi_0{{2m}\choose m}=1$$ Or $$\pi_0=\frac1{{2m}\choose m}$$ And $$\pi_i=\pi_0\prod_{j=1}^i\left(\frac{m-j+1}{j}\right)^2=\pi_0{m\choose i}^2=\pi_0{m\choose i}{m\choose{m-i}}=\frac{{m\choose i}{m\choose{m-i}}}{{2m}\choose m}$$",,"['probability', 'stochastic-processes', 'markov-chains']"
20,Hypergeometric Probability,Hypergeometric Probability,,"I am trying to figure this problem out and it just makes no sense to me.  The questions is as follows: A box contains 5 Green balls and 5 Orange Balls.  Five balls are taken at random without replacement. What is the probability that two green balls and 3 orange balls are selected? Now working through the problem manually I come up with this: (5/10)(4/9)(3/8)(5/7)(4/6) = 0.039 But using hypergeometric rules the best I understand I come up with this: ((5,2)(5,0))/(10,2) = .222 and ((5,3)(5,0))/(10,3) = .0833 .222*.083 = .0185 What am I not understanding here.  My confusion is in the fact that we are selecting both green and orange and not just looking for all orange or all green","I am trying to figure this problem out and it just makes no sense to me.  The questions is as follows: A box contains 5 Green balls and 5 Orange Balls.  Five balls are taken at random without replacement. What is the probability that two green balls and 3 orange balls are selected? Now working through the problem manually I come up with this: (5/10)(4/9)(3/8)(5/7)(4/6) = 0.039 But using hypergeometric rules the best I understand I come up with this: ((5,2)(5,0))/(10,2) = .222 and ((5,3)(5,0))/(10,3) = .0833 .222*.083 = .0185 What am I not understanding here.  My confusion is in the fact that we are selecting both green and orange and not just looking for all orange or all green",,"['probability', 'statistics']"
21,"Brownian motion, quadratic variation, existence of partitions?","Brownian motion, quadratic variation, existence of partitions?",,"Let $A_t$ be a standard Brownian motion. Where can I find a reference to/can anybody supply a proof of the fact that with probability $1$ there exists a sequence of partitions $\{t_{k, n} : k = 1, \dots, k_n\}$ $$0 = t_{0, n} < t_{1, n} < \dots < t_{k_n, n} = 1,$$with$$\lim_{n \to \infty} \max \{t_{j, n} - t_{j - 1, n} : j = 1, \dots, k_n\} = 0$$and$$\liminf_{ n \to \infty} \sum_{j=1}^{k_n} (A(t_{j, n}) - A(t_{j-1, n}))^2 > 1?$$All the sources I have been looking at have just been assuming the existence of this sequence? It's not very clear to me how this is clear. Thanks in advance!","Let $A_t$ be a standard Brownian motion. Where can I find a reference to/can anybody supply a proof of the fact that with probability $1$ there exists a sequence of partitions $\{t_{k, n} : k = 1, \dots, k_n\}$ $$0 = t_{0, n} < t_{1, n} < \dots < t_{k_n, n} = 1,$$with$$\lim_{n \to \infty} \max \{t_{j, n} - t_{j - 1, n} : j = 1, \dots, k_n\} = 0$$and$$\liminf_{ n \to \infty} \sum_{j=1}^{k_n} (A(t_{j, n}) - A(t_{j-1, n}))^2 > 1?$$All the sources I have been looking at have just been assuming the existence of this sequence? It's not very clear to me how this is clear. Thanks in advance!",,"['real-analysis', 'probability']"
22,Probability distribution derived from gamma function - does it have a name?,Probability distribution derived from gamma function - does it have a name?,,"Consider the complex gamma function, denoted by $\Gamma(\sigma+it)$. Now, let's fix $\sigma$ and let t vary. Then consider the following expression: $$|\Gamma(\sigma+it)|^2$$ For any choice of $\sigma$ such that $\Gamma(\sigma)$ isn't a pole, this will appear to be (almost) a two-sided probability density function, save for that it isn't normalized. It decays around as quickly as $e^{-|t|}$, somewhat resembles the function $e^{-\sqrt{1+t^2}}$, and appears related to the hyperbolic distribution. Is there a precise closed-form expression for this function in terms of elementary or Louvillian functions? Is there a name for this probability distribution (assuming it's normalized)?","Consider the complex gamma function, denoted by $\Gamma(\sigma+it)$. Now, let's fix $\sigma$ and let t vary. Then consider the following expression: $$|\Gamma(\sigma+it)|^2$$ For any choice of $\sigma$ such that $\Gamma(\sigma)$ isn't a pole, this will appear to be (almost) a two-sided probability density function, save for that it isn't normalized. It decays around as quickly as $e^{-|t|}$, somewhat resembles the function $e^{-\sqrt{1+t^2}}$, and appears related to the hyperbolic distribution. Is there a precise closed-form expression for this function in terms of elementary or Louvillian functions? Is there a name for this probability distribution (assuming it's normalized)?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'gamma-function']"
23,What is the probability this Markov chain does not reach state $r$?,What is the probability this Markov chain does not reach state ?,r,"Consider a random walk on the non-negative integers. You start at $0$, and in each step you either move $1$ higher, or $2$ lower (but can't go below $0$). The direction is picked w.p. $1/2$ independently at any step. From $0$ you can either stay at $0$, or go to $1$. From $1$ you flip a coin and go either to $0$ or to $2$. From $i>1$, you go to $i-2$ or to $i+1$. What is the probability that during $n$ steps, the process never reaches state $r$? If this does not have a simple form (as a function of $n,r$), can we derive an upper bound for it? This seems to be solvable by a recurrence formula, but I couldn't reach an explicit expression.","Consider a random walk on the non-negative integers. You start at $0$, and in each step you either move $1$ higher, or $2$ lower (but can't go below $0$). The direction is picked w.p. $1/2$ independently at any step. From $0$ you can either stay at $0$, or go to $1$. From $1$ you flip a coin and go either to $0$ or to $2$. From $i>1$, you go to $i-2$ or to $i+1$. What is the probability that during $n$ steps, the process never reaches state $r$? If this does not have a simple form (as a function of $n,r$), can we derive an upper bound for it? This seems to be solvable by a recurrence formula, but I couldn't reach an explicit expression.",,"['probability', 'stochastic-processes', 'markov-chains', 'markov-process']"
24,"Given two sets of $100$ samples of $10$ items from a $1000$ item set, what is probability that the two sets have non-empty intersection","Given two sets of  samples of  items from a  item set, what is probability that the two sets have non-empty intersection",100 10 1000,"Suppose two people go grocery shopping $100$ times each. Each time, they pick $10$ items randomly from the $1000$ items at the store. As a result, each person has $100$ randomly chosen baskets of $10$ items. What is the probability that, by the end, both people have one identical randomly chosen basket? This problem is straightforward if I know that the $100$ random baskets are unique (in which case you can calculate the probability as having person two pick one of the $100$ random baskets person one chose in $100$ tries), but I don't know how to account for the fact that each person could have fewer than $100$ unique random baskets. As I write this question, I realized it might be possible to write the probability as a sum over the number of unique random baskets person one chose.","Suppose two people go grocery shopping times each. Each time, they pick items randomly from the items at the store. As a result, each person has randomly chosen baskets of items. What is the probability that, by the end, both people have one identical randomly chosen basket? This problem is straightforward if I know that the random baskets are unique (in which case you can calculate the probability as having person two pick one of the random baskets person one chose in tries), but I don't know how to account for the fact that each person could have fewer than unique random baskets. As I write this question, I realized it might be possible to write the probability as a sum over the number of unique random baskets person one chose.",100 10 1000 100 10 100 100 100 100,[]
25,Zero conditional entropy,Zero conditional entropy,,"This question is related to this math.se question but I need a bit more guidance. For two discrete random variables $X,Y$ we define their  conditional entropy to be $$H(X|Y) = -\sum_{y \in Y} Pr[Y = y] ( \sum_{x \in X} Pr[X = x | Y = y] \log_2{Pr[X=x|Y=y]}).$$ I would like to show that $H(X|Y) = 0$ if and only if $X = f(Y)$ for some function $Y.$ Can someone help me with this? I don't know how the fact that $X,Y$ are related by a function implies $H(X|Y) = 0$ and vice versa.","This question is related to this math.se question but I need a bit more guidance. For two discrete random variables $X,Y$ we define their  conditional entropy to be $$H(X|Y) = -\sum_{y \in Y} Pr[Y = y] ( \sum_{x \in X} Pr[X = x | Y = y] \log_2{Pr[X=x|Y=y]}).$$ I would like to show that $H(X|Y) = 0$ if and only if $X = f(Y)$ for some function $Y.$ Can someone help me with this? I don't know how the fact that $X,Y$ are related by a function implies $H(X|Y) = 0$ and vice versa.",,"['probability', 'probability-theory', 'random-variables', 'entropy']"
26,Swapping the integrand and variables being integrated,Swapping the integrand and variables being integrated,,"I am reading a paper in which the following integral is solved, but I am not sure how to derive the answer myself.  The integral is: $$\int_{F_{x-1}}^{F_x} du {\int_{G_{y-1}}^{G_y}{dv}\  C^*(u,v)} \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  [1] $$ I know that the answer simplifies to: $$ \frac{f_X g_Y}{4} * \{ C^*(F_{X}, G_{Y}) + C^*(F_{X}, G_{Y-1}) + C^*(F_{X-1}, G_{Y}) + C^*(F_{X-1}, G_{Y-1}) \} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [2]$$ The original integral is,  $$\int_{F_{x-1}}^{F_x}{\int_{G_{y-1}}^{G_y}{C^*(u,v)}\ du\ dv } \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  [3] $$ which I think is supposed to be the same as Equation [1]. What I do know for a fact is the following: $$\int_{F_{x-1}}^{F_x} du {\int_{G_{y-1}}^{G_y}{dv}} = (F(x)-F(x-1))\times(G(y)-G(y-1)) = f_X g_Y \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [4]$$ From Equation [4], I can kind of see where the answer is coming from, but obviously I cannot just pull out $C^*(u,v)$ from the integral itself?  I am beginning to think that the author rewrote Equation [3] in the form of Equation [1] deliberately, but I am unsure as to why?  It seems that the function to be integrated, $C^*(u,v)$ is swapped with the variables to be integrated but I haven't seen this form of an integral before, so I'm not 100% sure what I'm working with. Something else that I tried is the following: $$ \int^2_1 x dx = 2 - 1/2 = 3/2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [5]$$ $$ \int^2_1 dx\  x =  \frac{x|^2_1 * (1 + 2)}{2} = 3/2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [6]$$ But really, I'm not sure where the divide by 2 (I just put it in there to make the answers equal) in Equation 6 comes from, or whether I just got lucky somehow and this is not a rule in general? Any help is greatly appreciated! Edit #1: All, so I investigated the simple example that I tried further and I think I have a solution.  Generalizing Equation's 5 and 6, what I want to see is when the following is true: $$ \int_a^b{f(x)dx} = F(b)-F(a) \stackrel{?}{=} \int_a^b{dx\ f(x)} = \frac{(b-a)*(f(a)+f(b))}{2} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [7] $$ Lets have a look at this geometrically ... The blue line represents a general function $f(x)$.  What we see here is the black hatched area represents the value of $\int_a^b{f(x) dx}$ while the red hatched area represents the value of $\frac{(b-a)(f(a)+f(b))}{2}$.  So now from this, lets think about when these two values will be equal.  For this, refer to Figure 2 below. We can see that the red-hatched area and the black-hatched area will be equal if the areas of the orange triangle's are equivalent.  This means that the LHS and RHS of Equation 7 are equivalent if that condition holds true.  I think that means that we can say in general that Equation 7 is true if the function being integrated is linear in the region of integration.  If the function is not linear in the region of integration, then if the area's (not necessarily traingular any longer) are equivalent then Equation 7 still holds.  I think the derivation then for Equation [2] is a 2-D extension of this principle.  Anybody have thoughts on this?","I am reading a paper in which the following integral is solved, but I am not sure how to derive the answer myself.  The integral is: $$\int_{F_{x-1}}^{F_x} du {\int_{G_{y-1}}^{G_y}{dv}\  C^*(u,v)} \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  [1] $$ I know that the answer simplifies to: $$ \frac{f_X g_Y}{4} * \{ C^*(F_{X}, G_{Y}) + C^*(F_{X}, G_{Y-1}) + C^*(F_{X-1}, G_{Y}) + C^*(F_{X-1}, G_{Y-1}) \} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [2]$$ The original integral is,  $$\int_{F_{x-1}}^{F_x}{\int_{G_{y-1}}^{G_y}{C^*(u,v)}\ du\ dv } \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  [3] $$ which I think is supposed to be the same as Equation [1]. What I do know for a fact is the following: $$\int_{F_{x-1}}^{F_x} du {\int_{G_{y-1}}^{G_y}{dv}} = (F(x)-F(x-1))\times(G(y)-G(y-1)) = f_X g_Y \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [4]$$ From Equation [4], I can kind of see where the answer is coming from, but obviously I cannot just pull out $C^*(u,v)$ from the integral itself?  I am beginning to think that the author rewrote Equation [3] in the form of Equation [1] deliberately, but I am unsure as to why?  It seems that the function to be integrated, $C^*(u,v)$ is swapped with the variables to be integrated but I haven't seen this form of an integral before, so I'm not 100% sure what I'm working with. Something else that I tried is the following: $$ \int^2_1 x dx = 2 - 1/2 = 3/2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [5]$$ $$ \int^2_1 dx\  x =  \frac{x|^2_1 * (1 + 2)}{2} = 3/2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [6]$$ But really, I'm not sure where the divide by 2 (I just put it in there to make the answers equal) in Equation 6 comes from, or whether I just got lucky somehow and this is not a rule in general? Any help is greatly appreciated! Edit #1: All, so I investigated the simple example that I tried further and I think I have a solution.  Generalizing Equation's 5 and 6, what I want to see is when the following is true: $$ \int_a^b{f(x)dx} = F(b)-F(a) \stackrel{?}{=} \int_a^b{dx\ f(x)} = \frac{(b-a)*(f(a)+f(b))}{2} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [7] $$ Lets have a look at this geometrically ... The blue line represents a general function $f(x)$.  What we see here is the black hatched area represents the value of $\int_a^b{f(x) dx}$ while the red hatched area represents the value of $\frac{(b-a)(f(a)+f(b))}{2}$.  So now from this, lets think about when these two values will be equal.  For this, refer to Figure 2 below. We can see that the red-hatched area and the black-hatched area will be equal if the areas of the orange triangle's are equivalent.  This means that the LHS and RHS of Equation 7 are equivalent if that condition holds true.  I think that means that we can say in general that Equation 7 is true if the function being integrated is linear in the region of integration.  If the function is not linear in the region of integration, then if the area's (not necessarily traingular any longer) are equivalent then Equation 7 still holds.  I think the derivation then for Equation [2] is a 2-D extension of this principle.  Anybody have thoughts on this?",,"['probability', 'integration', 'definite-integrals']"
27,Prove that if $E[X|\sigma(Y)] = Y$ and $E[Y|\sigma(X)] = X$ then $X = Y$ almost surely. [duplicate],Prove that if  and  then  almost surely. [duplicate],E[X|\sigma(Y)] = Y E[Y|\sigma(X)] = X X = Y,"This question already has answers here : Question about Conditional Expectation (1 answer) If $E[X|Y]=Y$ almost surely and $E[Y|X]=X$ almost surely then $X=Y$ almost surely (4 answers) Closed 8 years ago . Prove that if  $E[X|\sigma(Y)] = Y$ and $E[Y|\sigma(X)] = X$ then $X = Y$ almost surely. This is my idea: By assumption, $Y = E[X|\sigma(Y)] = E\left\lbrack E[Y|\sigma(X)]|\sigma(Y)\right\rbrack$, and I would like to show that this equals $E[Y|\sigma(X)]$ which equals $X$.  If I can show that $E[Y|\sigma(X)]$ is $\sigma(Y)$ measurable, then the proof is finished.  But I am not sure how to do this. Any help would be greatly appreciated.","This question already has answers here : Question about Conditional Expectation (1 answer) If $E[X|Y]=Y$ almost surely and $E[Y|X]=X$ almost surely then $X=Y$ almost surely (4 answers) Closed 8 years ago . Prove that if  $E[X|\sigma(Y)] = Y$ and $E[Y|\sigma(X)] = X$ then $X = Y$ almost surely. This is my idea: By assumption, $Y = E[X|\sigma(Y)] = E\left\lbrack E[Y|\sigma(X)]|\sigma(Y)\right\rbrack$, and I would like to show that this equals $E[Y|\sigma(X)]$ which equals $X$.  If I can show that $E[Y|\sigma(X)]$ is $\sigma(Y)$ measurable, then the proof is finished.  But I am not sure how to do this. Any help would be greatly appreciated.",,['probability']
28,Properties of an MLE based on likelihood constructed from both PDF and CDF,Properties of an MLE based on likelihood constructed from both PDF and CDF,,"For continuous RV the likelihood function is (typically) given by a product of PDFs, i.e. $$L(\theta; x_1,x_2, ..., x_n) = \prod_{i=1}^n f(x_i\mid \theta) $$ However, in survival analysis with censored outcomes the likelihood is given by a product of both PDFs (for non-censored observations) and CDFs (for right-, left-, and interval-censored data), i.e. \begin{align}&L(\theta; x_1,x_2, ..., x_n) \\=\, &\prod_{i\in unc.}f(x_i\mid\theta)\prod_{i\in l.c.}F(x_i\mid\theta)\prod_{i\in r.c.}\left(1-F(x_i\mid\theta)\right)\prod_{i\in i.c.}\left(F(x_{i,r}\mid\theta)-F(x_{i,l}|\theta)\right)\end{align} Since the range of PDFs is not bounded by $1$ (e.g. the pdf of the Weibull distribution with the shape parameter $k < 1$ goes to infinity as $x$ goes to $0$) and CDFs clearly are, it would seem that an MLE constructed in the latter way would result in parameters that “favour” non-censored values.  How does this affect all those “nice” properties of an MLE like consistency, efficiency, etc?","For continuous RV the likelihood function is (typically) given by a product of PDFs, i.e. $$L(\theta; x_1,x_2, ..., x_n) = \prod_{i=1}^n f(x_i\mid \theta) $$ However, in survival analysis with censored outcomes the likelihood is given by a product of both PDFs (for non-censored observations) and CDFs (for right-, left-, and interval-censored data), i.e. \begin{align}&L(\theta; x_1,x_2, ..., x_n) \\=\, &\prod_{i\in unc.}f(x_i\mid\theta)\prod_{i\in l.c.}F(x_i\mid\theta)\prod_{i\in r.c.}\left(1-F(x_i\mid\theta)\right)\prod_{i\in i.c.}\left(F(x_{i,r}\mid\theta)-F(x_{i,l}|\theta)\right)\end{align} Since the range of PDFs is not bounded by $1$ (e.g. the pdf of the Weibull distribution with the shape parameter $k < 1$ goes to infinity as $x$ goes to $0$) and CDFs clearly are, it would seem that an MLE constructed in the latter way would result in parameters that “favour” non-censored values.  How does this affect all those “nice” properties of an MLE like consistency, efficiency, etc?",,"['probability', 'parameter-estimation']"
29,"Finding a hidden ""heavy"" subset of random variables.","Finding a hidden ""heavy"" subset of random variables.",,"Let $X_1,\dots, X_n$ be independent non-negative random variables (with finite expectation and variance), and $0 < m < n$ be a fixed integer such that there exists a subset $S\subseteq [n]$ of size $\lvert S\rvert =m$ with $\sum_{k\in S} \mathbb{E}[X_k] \geq 100$; and $\sum_{k\notin S} \mathbb{E}[X_k] \leq 1$ and $\sum_{k\in S} \operatorname{Var}[X_k] \leq \frac{1}{100} \left(\sum_{k\in S} \mathbb{E}[X_k]\right)^2$; and $\sum_{k\notin S} \operatorname{Var}[X_k] \leq \frac{1}{100}$. In other words, there is an unknown subset of elements that, altogether, are ""heavy"", and its complement is ""light."" If I define $T$ as the indices of the $2m$ top values among $X_1,\dots, X_n$, what is the probability that $\sum_{[n]\setminus T} \mathbb{E}[X_i] < 2$? (where the probability is taken over the realization of the $X_i$'s). In other terms, what is the probability that we removed (almost) all the weight of the""heavy set""? If $m=1$, then this is clear that is happens with high probability by Chebyshev's inequality: if $S=\{i\}$, then the probability that $X_i$ is less than $50$ is at most $\frac{1}{25}$, and similarly the probability that the sum of all other elements (so a fortiori any of them) is greater than 2 is at most $\frac{1}{100}$, so that overall $X_i$ is the greatest element (and therefore among the two top elements) with probability at least $19/20$. Is there a way to generalize this argument to $m \geq 2$?","Let $X_1,\dots, X_n$ be independent non-negative random variables (with finite expectation and variance), and $0 < m < n$ be a fixed integer such that there exists a subset $S\subseteq [n]$ of size $\lvert S\rvert =m$ with $\sum_{k\in S} \mathbb{E}[X_k] \geq 100$; and $\sum_{k\notin S} \mathbb{E}[X_k] \leq 1$ and $\sum_{k\in S} \operatorname{Var}[X_k] \leq \frac{1}{100} \left(\sum_{k\in S} \mathbb{E}[X_k]\right)^2$; and $\sum_{k\notin S} \operatorname{Var}[X_k] \leq \frac{1}{100}$. In other words, there is an unknown subset of elements that, altogether, are ""heavy"", and its complement is ""light."" If I define $T$ as the indices of the $2m$ top values among $X_1,\dots, X_n$, what is the probability that $\sum_{[n]\setminus T} \mathbb{E}[X_i] < 2$? (where the probability is taken over the realization of the $X_i$'s). In other terms, what is the probability that we removed (almost) all the weight of the""heavy set""? If $m=1$, then this is clear that is happens with high probability by Chebyshev's inequality: if $S=\{i\}$, then the probability that $X_i$ is less than $50$ is at most $\frac{1}{25}$, and similarly the probability that the sum of all other elements (so a fortiori any of them) is greater than 2 is at most $\frac{1}{100}$, so that overall $X_i$ is the greatest element (and therefore among the two top elements) with probability at least $19/20$. Is there a way to generalize this argument to $m \geq 2$?",,"['probability', 'statistics', 'order-statistics']"
30,Uniform continuous distribution for cycles.,Uniform continuous distribution for cycles.,,Let there be $n$ people standing in a circle and holding hands with probability $p$. What is the expectation value $E(X)$ for the number of 'chains' when $p=.5$? For what $p$ is $E(X)$ largest? Edit: a chain is defined by two or more people next to one another holding hands.,Let there be $n$ people standing in a circle and holding hands with probability $p$. What is the expectation value $E(X)$ for the number of 'chains' when $p=.5$? For what $p$ is $E(X)$ largest? Edit: a chain is defined by two or more people next to one another holding hands.,,"['probability', 'probability-theory', 'expectation', 'conditional-expectation']"
31,The law of large numbers - limits of $\max$ vs $\max$ of a limit.,The law of large numbers - limits of  vs  of a limit.,\max \max,"Assume that $X_{1,1}, \dots , X_{1,n}, X_{2,1},\dots, X_{2,n}, \dots ,X_{n,1}, \dots , X_{n,n}$ are i.i.d. random variables, and that $\mathbb EX_{i,j}$ exists and is finite. From the strong law of large numbers we have  $$\max_i\left\{\lim_{n\to\infty}\left\{\frac{1}{n}\sum_{j=1}^{n}X_{i,j}\right\}\right\} \overset{a.s.}{=} \mathbb EX_{i,j} ,$$ because $\max_i$ is redundant. However, do we have $$\lim_{n\to\infty}\left\{\max_i\left\{\frac{1}{n}\sum_{j=1}^{n}X_{i,j}\right\}\right\}  \overset{a.s.}{=} \mathbb EX_{i,j} ? $$  I.e., can  we change the order of $\max$ and $\lim$ without affecting the result?","Assume that $X_{1,1}, \dots , X_{1,n}, X_{2,1},\dots, X_{2,n}, \dots ,X_{n,1}, \dots , X_{n,n}$ are i.i.d. random variables, and that $\mathbb EX_{i,j}$ exists and is finite. From the strong law of large numbers we have  $$\max_i\left\{\lim_{n\to\infty}\left\{\frac{1}{n}\sum_{j=1}^{n}X_{i,j}\right\}\right\} \overset{a.s.}{=} \mathbb EX_{i,j} ,$$ because $\max_i$ is redundant. However, do we have $$\lim_{n\to\infty}\left\{\max_i\left\{\frac{1}{n}\sum_{j=1}^{n}X_{i,j}\right\}\right\}  \overset{a.s.}{=} \mathbb EX_{i,j} ? $$  I.e., can  we change the order of $\max$ and $\lim$ without affecting the result?",,"['probability', 'probability-limit-theorems', 'law-of-large-numbers']"
32,Arc Length of largest arc when $n$ points are chosen at random on the circumference of the unit circle.,Arc Length of largest arc when  points are chosen at random on the circumference of the unit circle.,n,"I am currently doing a little self-study of A Probability Path by Sidney Resnick, and am having trouble with the following problem: Points are chosen at random on the circumference of the unit circle. $Y_n$ is the arc length of the largest arc not containing any points when $n$ points are chosen. Show $Y_n \to 0$ almost surely. So far my strategy is the following: Since $\{Y_n\}$ is a monotone (decreasing) sequence, convergence almost surely is equivalent to convergence in probability, so it suffices to show that the expected value $E(Y_n) \to 0$. A naive guess is that $E(Y_n) = \frac{2\pi}{n}$. We can write  \begin{equation} E(Y_n) = \int_0^{2\pi}P(Y_n > l)\,d l. \end{equation} and \begin{equation} P(Y_n > l) = \begin{cases} 1, & \text{if } l < \frac{2\pi}{n},\\ \text{??} & \text{otherwise}. \end{cases} \end{equation} How do we determine $P(Y_n > l)$? Or is there another way to proceed?","I am currently doing a little self-study of A Probability Path by Sidney Resnick, and am having trouble with the following problem: Points are chosen at random on the circumference of the unit circle. $Y_n$ is the arc length of the largest arc not containing any points when $n$ points are chosen. Show $Y_n \to 0$ almost surely. So far my strategy is the following: Since $\{Y_n\}$ is a monotone (decreasing) sequence, convergence almost surely is equivalent to convergence in probability, so it suffices to show that the expected value $E(Y_n) \to 0$. A naive guess is that $E(Y_n) = \frac{2\pi}{n}$. We can write  \begin{equation} E(Y_n) = \int_0^{2\pi}P(Y_n > l)\,d l. \end{equation} and \begin{equation} P(Y_n > l) = \begin{cases} 1, & \text{if } l < \frac{2\pi}{n},\\ \text{??} & \text{otherwise}. \end{cases} \end{equation} How do we determine $P(Y_n > l)$? Or is there another way to proceed?",,"['probability', 'probability-theory']"
33,Brownian motion and associated martingales,Brownian motion and associated martingales,,"Under the Wiener measure $\Bbb{W}$ the process $x(t)$ is a brownian motion. This means that $\Bbb{E}[{x(t)-x(s)\mid \mathcal{F}_s}]=0$. Let $P$ be a measure in $C([0,\infty),\Bbb{R}^d)$ such that for every $\theta \in \Bbb{R}^d$ $$X_\theta(t):=\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{1}{2} t |\theta|^2\bigg\}$$ is a martingale. Then $P = \Bbb{W}$ is the Wiener measure. Let $\sigma$ be a simmetric matrix: Assume now we define $$\tilde{P}(x(t_1)\in A_1, \ldots x(t_k) \in A_k) := \Bbb{W}(\sigma(x(t_1)-x) \in A_1,\ldots ,\sigma(x(t_k)-x) \in A_k)$$ Let $a = \sigma \sigma^T$ Is it true that $$Y_\theta(t):=\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{d}{2} t \langle\theta,a\theta \rangle^2\bigg\} $$ Is a martingale? Attempt: $\langle\theta,\sigma(x_t-x)\rangle = \sum_{i,j}\theta_i \sigma_{i,j}(x(t)-x)_j$ We could use Ito calculus to conclude that \begin{align*} \exp\{\langle\theta,\sigma(x_t-x\rangle)\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \partial_i\langle\theta\sigma(x(s)-x)\rangle \exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\ &+ \sum_{j = 1}^d \frac{1}{2}\int_0^t \partial_{i}\exp\{\langle\theta,\sigma(x_s-x)\rangle\} \partial_i \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds  \end{align*} Since $\partial_j \sum_{i,j}\theta_i \sigma_{i,j}(x(t)-x)_j = \sum_{i}\theta_i \sigma_{i,j}$ \begin{align*} \exp\{\langle\theta,\sigma(x_t-x)\rangle\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \sum_{i}\theta_i \sigma_{i,j}\exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\ &+ \sum_{j = 1}^d \frac{1}{2}\int_0^t (\sum_{i}\theta_i \sigma_{i,j})(\sum_{i}\theta_i \sigma_{i,j})\exp\{\langle\theta,\sigma(x_s-x)\rangle\} \partial_i \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds  \end{align*} Now note that $(\sum_{i}\theta_i \sigma_{i,j})(\sum_{i}\theta_i \sigma_{i,j}) = \sum_{i,j}\theta_i a_{i,j}\theta_j= \langle \theta, a\theta \rangle$ So we conclude \begin{align*} \exp\{\langle\theta,\sigma(x_t-x)\rangle\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \sum_{i}\theta_i \sigma_{i,j}\exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\ &+  \frac{d}{2}\int_0^t \langle \theta, a\theta \rangle \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds  \end{align*} Now it suffices to note that once $\phi(t)$ is a continuous bounded (on intervals) martingale and $\psi(t)$ is a continuous function of bounded variation such that $\Bbb{E}[\|\psi \|_[0,t]]<\infty$ (the total variation on the interval $[0,t]$ has finite expectation) then  $$\phi(t)\psi(t)- \int_0^t \phi(s)\,d\psi(s) $$ is also a martingale. Take \begin{align*}\phi(t)&=\exp\{\langle\theta,\sigma(x_t)-x\rangle\} -   \frac{d}{2}\int_0^t \langle \theta, a\theta \rangle \exp\{\langle\theta,\sigma(x_s)-x\rangle\} \, ds \\ \psi(t) &= \exp{-\frac{d}{2} t \langle \theta, a\theta \rangle }  \end{align*} So in a sense, the result is already proved. Question: is there a different approach to this question? I was thinking something along the following lines: $$\tilde{P}(x(t_1)\in A_1, \ldots x(t_k) \in A_k) := \Bbb{W}(\sigma(x(t_1)-x) \in A_1,\ldots ,\sigma(x(t_k)-x) \in A_k)$$ Implies that  $$\Bbb{E}_{\tilde{P}}[f(x_t)] = \Bbb{E}_{\Bbb{W}}[f(\sigma(x_t-x))]  $$ then the condition  $$\Bbb{E}_{\Bbb{W}}\bigg[\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{d}{2} t |\theta|^2\bigg\}\bigg]=1$$ Should imply that  $$\Bbb{E}_{\tilde{P}}\bigg[\exp\bigg\{ \langle\theta,\sigma(x(t)-x)\rangle - \frac{1}{2} t \langle\theta, a\theta\rangle\bigg\}\bigg]=1$$ But I don't see it","Under the Wiener measure $\Bbb{W}$ the process $x(t)$ is a brownian motion. This means that $\Bbb{E}[{x(t)-x(s)\mid \mathcal{F}_s}]=0$. Let $P$ be a measure in $C([0,\infty),\Bbb{R}^d)$ such that for every $\theta \in \Bbb{R}^d$ $$X_\theta(t):=\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{1}{2} t |\theta|^2\bigg\}$$ is a martingale. Then $P = \Bbb{W}$ is the Wiener measure. Let $\sigma$ be a simmetric matrix: Assume now we define $$\tilde{P}(x(t_1)\in A_1, \ldots x(t_k) \in A_k) := \Bbb{W}(\sigma(x(t_1)-x) \in A_1,\ldots ,\sigma(x(t_k)-x) \in A_k)$$ Let $a = \sigma \sigma^T$ Is it true that $$Y_\theta(t):=\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{d}{2} t \langle\theta,a\theta \rangle^2\bigg\} $$ Is a martingale? Attempt: $\langle\theta,\sigma(x_t-x)\rangle = \sum_{i,j}\theta_i \sigma_{i,j}(x(t)-x)_j$ We could use Ito calculus to conclude that \begin{align*} \exp\{\langle\theta,\sigma(x_t-x\rangle)\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \partial_i\langle\theta\sigma(x(s)-x)\rangle \exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\ &+ \sum_{j = 1}^d \frac{1}{2}\int_0^t \partial_{i}\exp\{\langle\theta,\sigma(x_s-x)\rangle\} \partial_i \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds  \end{align*} Since $\partial_j \sum_{i,j}\theta_i \sigma_{i,j}(x(t)-x)_j = \sum_{i}\theta_i \sigma_{i,j}$ \begin{align*} \exp\{\langle\theta,\sigma(x_t-x)\rangle\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \sum_{i}\theta_i \sigma_{i,j}\exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\ &+ \sum_{j = 1}^d \frac{1}{2}\int_0^t (\sum_{i}\theta_i \sigma_{i,j})(\sum_{i}\theta_i \sigma_{i,j})\exp\{\langle\theta,\sigma(x_s-x)\rangle\} \partial_i \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds  \end{align*} Now note that $(\sum_{i}\theta_i \sigma_{i,j})(\sum_{i}\theta_i \sigma_{i,j}) = \sum_{i,j}\theta_i a_{i,j}\theta_j= \langle \theta, a\theta \rangle$ So we conclude \begin{align*} \exp\{\langle\theta,\sigma(x_t-x)\rangle\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \sum_{i}\theta_i \sigma_{i,j}\exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\ &+  \frac{d}{2}\int_0^t \langle \theta, a\theta \rangle \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds  \end{align*} Now it suffices to note that once $\phi(t)$ is a continuous bounded (on intervals) martingale and $\psi(t)$ is a continuous function of bounded variation such that $\Bbb{E}[\|\psi \|_[0,t]]<\infty$ (the total variation on the interval $[0,t]$ has finite expectation) then  $$\phi(t)\psi(t)- \int_0^t \phi(s)\,d\psi(s) $$ is also a martingale. Take \begin{align*}\phi(t)&=\exp\{\langle\theta,\sigma(x_t)-x\rangle\} -   \frac{d}{2}\int_0^t \langle \theta, a\theta \rangle \exp\{\langle\theta,\sigma(x_s)-x\rangle\} \, ds \\ \psi(t) &= \exp{-\frac{d}{2} t \langle \theta, a\theta \rangle }  \end{align*} So in a sense, the result is already proved. Question: is there a different approach to this question? I was thinking something along the following lines: $$\tilde{P}(x(t_1)\in A_1, \ldots x(t_k) \in A_k) := \Bbb{W}(\sigma(x(t_1)-x) \in A_1,\ldots ,\sigma(x(t_k)-x) \in A_k)$$ Implies that  $$\Bbb{E}_{\tilde{P}}[f(x_t)] = \Bbb{E}_{\Bbb{W}}[f(\sigma(x_t-x))]  $$ then the condition  $$\Bbb{E}_{\Bbb{W}}\bigg[\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{d}{2} t |\theta|^2\bigg\}\bigg]=1$$ Should imply that  $$\Bbb{E}_{\tilde{P}}\bigg[\exp\bigg\{ \langle\theta,\sigma(x(t)-x)\rangle - \frac{1}{2} t \langle\theta, a\theta\rangle\bigg\}\bigg]=1$$ But I don't see it",,"['probability', 'stochastic-processes', 'martingales']"
34,Application Problem: Conditioning Poisson Process,Application Problem: Conditioning Poisson Process,,"I am trying to solve the following application problem: There are $n$ components with independent lifetimes which are such that component $i$ functions for an exponential time with rate $\lambda_i$. Suppose that all components are initially in use and remain so until they fail. Find the probability that component 1 is the second component to fail. I know I need to condition on which component fails first, but I don't know how to set up this condition in order to compute the probability in question.","I am trying to solve the following application problem: There are $n$ components with independent lifetimes which are such that component $i$ functions for an exponential time with rate $\lambda_i$. Suppose that all components are initially in use and remain so until they fail. Find the probability that component 1 is the second component to fail. I know I need to condition on which component fails first, but I don't know how to set up this condition in order to compute the probability in question.",,"['probability', 'probability-distributions', 'poisson-distribution']"
35,Borel-Cantelli lemma: non-negative iid random variables,Borel-Cantelli lemma: non-negative iid random variables,,"I came across a claim in a paper on branching processes which says that the following is an immediate consequence of the B-C lemmas: Let $X, X_1, X_2, \ldots$ be nonnegative iid random variables. Then $\limsup_{n \to \infty} X_n/n = 0$ if $EX<\infty$, and $\limsup_{n \to \infty} X_n/n = \infty$ if $EX=\infty$. So to apply the BC lemmas to these, I want to essentially show that $$(1) \; \textrm{If } EX<\infty, \textrm{ then } P(\limsup \{X_n/n > \epsilon\}) = 0 \quad \forall \epsilon>0$$ $$(2) \; \textrm{If } EX=\infty, \textrm{ then } P(\limsup \{X_n/n > \delta\}) = 1 \quad \forall \delta>0$$ But I keep getting stuck. For example if I want to apply the first BC lemma to (1), then using Markov's inequality only gives $P(X_n > n\epsilon) < EX/n\epsilon$, which isn't summable. Am I missing something right under my nose?","I came across a claim in a paper on branching processes which says that the following is an immediate consequence of the B-C lemmas: Let $X, X_1, X_2, \ldots$ be nonnegative iid random variables. Then $\limsup_{n \to \infty} X_n/n = 0$ if $EX<\infty$, and $\limsup_{n \to \infty} X_n/n = \infty$ if $EX=\infty$. So to apply the BC lemmas to these, I want to essentially show that $$(1) \; \textrm{If } EX<\infty, \textrm{ then } P(\limsup \{X_n/n > \epsilon\}) = 0 \quad \forall \epsilon>0$$ $$(2) \; \textrm{If } EX=\infty, \textrm{ then } P(\limsup \{X_n/n > \delta\}) = 1 \quad \forall \delta>0$$ But I keep getting stuck. For example if I want to apply the first BC lemma to (1), then using Markov's inequality only gives $P(X_n > n\epsilon) < EX/n\epsilon$, which isn't summable. Am I missing something right under my nose?",,"['real-analysis', 'probability']"
36,"What are the odds that the pattern, win lose lose, will happen 23 times in a row (69 rounds)?","What are the odds that the pattern, win lose lose, will happen 23 times in a row (69 rounds)?",,"In a game of 50/50 (this example could be a coin flip). Before any flips, What are the odds that the pattern win lose lose (heads tails tails if your choice was heads each time.) will happen 23 times in a row (69 rounds)? What about after the chain starts? After the initial wll, what do the odds become for the next ""Set"" of wll conditions? And the third, up to the 23 occurrence? Again, keeping with your initial choice (option) each time. To put a twist on the question, how does the solution change if the odds are based off the roulette ""50/50"" options? (With pesky zeros added, or coin landing on its edge) Would that change the odds since it's not about the number but rather pattern if the option won or lost? Can this also be broken down to what are the odds for each round (or flip) for round 1 to be a winner round 2 will lose round 3 will lose, (rnd 4 will win 5 will lose 6 will also lose etc to rnd 69?) wllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwll I have seen similar questions about odds and statistics asked, and one of those formulas may work for this riddle, but my knowledge of mathematics are limited. I greatly appreciate the helpfulness of the stack network communities and the cumulative knowledge we share. https://stats.stackexchange.com/questions/12174/time-taken-to-hit-a-pattern-of-heads-and-tails-in-a-series-of-coin-tosses is an enlightening read. The following Stack Exchange links are very informative on the subject as well. What is the probability of a coin landing tails 7 times in a row in a series of 150 coin flips? Measuring Roulette sequence probability Roulette outcome probability If I flip a coin $100$ times, why do the results trend towards $50-50$? Roulette Conditional Probability Roulette betting system probability Simple coin flip probability Probability of $5$ fair coin flips having strictly more heads than $4$ fair coin flips","In a game of 50/50 (this example could be a coin flip). Before any flips, What are the odds that the pattern win lose lose (heads tails tails if your choice was heads each time.) will happen 23 times in a row (69 rounds)? What about after the chain starts? After the initial wll, what do the odds become for the next ""Set"" of wll conditions? And the third, up to the 23 occurrence? Again, keeping with your initial choice (option) each time. To put a twist on the question, how does the solution change if the odds are based off the roulette ""50/50"" options? (With pesky zeros added, or coin landing on its edge) Would that change the odds since it's not about the number but rather pattern if the option won or lost? Can this also be broken down to what are the odds for each round (or flip) for round 1 to be a winner round 2 will lose round 3 will lose, (rnd 4 will win 5 will lose 6 will also lose etc to rnd 69?) wllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwllwll I have seen similar questions about odds and statistics asked, and one of those formulas may work for this riddle, but my knowledge of mathematics are limited. I greatly appreciate the helpfulness of the stack network communities and the cumulative knowledge we share. https://stats.stackexchange.com/questions/12174/time-taken-to-hit-a-pattern-of-heads-and-tails-in-a-series-of-coin-tosses is an enlightening read. The following Stack Exchange links are very informative on the subject as well. What is the probability of a coin landing tails 7 times in a row in a series of 150 coin flips? Measuring Roulette sequence probability Roulette outcome probability If I flip a coin $100$ times, why do the results trend towards $50-50$? Roulette Conditional Probability Roulette betting system probability Simple coin flip probability Probability of $5$ fair coin flips having strictly more heads than $4$ fair coin flips",,['probability']
37,Properties Least Mean Fourth Error,Properties Least Mean Fourth Error,,"I am interested in whether a quantity  \begin{align*} E[(X-E[X|Y])^4] \end{align*} has been studied in the literature before. I am not even sure if ""least mean fourth error"" is a correct name, since $g(Y)=E[X|Y]$ might not be the best estimator for the $\inf_{g(y)} E[(X-g(Y))^4]$. However, I am intersted in $E[(X-E[X|Y])^4]$ rather then $\inf_{g(y)} E[(X-g(Y))^4]$. Specifically, I am interested in the case when  \begin{align*} Y=X+Z \end{align*} where $X$ is zero mean and unit variance and $Z \sim \mathcal{N}(0,1)$ and independent of $X$. Here are some bounds that I was able to come up with: Lower Bound: For example we can related it to MMSE via Jensen's inequality \begin{align*} E[(X-E[X|Y])^4] \ge E^2[(X-E[X|Y])^2]=MMSE^2 \end{align*} Upper Bound 1: Also, using Minkowski inequality and assuming $E[X^4]$ exists we can have  \begin{align*} E[(X-E[X|Y])^4]& \le (E[X^4]^{1/4}+E[E^4[X|Y]]^{1/4})^4 \\ &\le 2^4 E[X^4] \end{align*} Upper bound 2: Observe that  \begin{align*} (X-E[X|Y])=-(Z-E[Z|Y]) \end{align*} So,  \begin{align*} E[(X-E[X|Y])^4]=E(Z-E[Z|Y])^4] \le 2^4 E[Z^4]. \end{align*} The nice thing about upper bound 2 is that it does not require any assumptions about $X$. Moreover, since $Z$ is Gaussian $E[Z^4]$ is well defined. Need Help with Improve the upper bounds I have. For example, can we get rid of factor $ 2^4$? Can we say that over all random variables with  $E[X^2]\le 1 $ Gaussian $X$ maximizes $E[(X-E[X|Y])^4]$. This is true for $E[(X-E[X|Y])^2]$ and I don't feel that $E[(X-E[X|Y])^4]$ is very different. Any references on quantities $E[(X-E[X|Y])^4]$ or $\inf_{g(y)} E[(X-g(Y))^4]$ Thanks a lot for any hoel","I am interested in whether a quantity  \begin{align*} E[(X-E[X|Y])^4] \end{align*} has been studied in the literature before. I am not even sure if ""least mean fourth error"" is a correct name, since $g(Y)=E[X|Y]$ might not be the best estimator for the $\inf_{g(y)} E[(X-g(Y))^4]$. However, I am intersted in $E[(X-E[X|Y])^4]$ rather then $\inf_{g(y)} E[(X-g(Y))^4]$. Specifically, I am interested in the case when  \begin{align*} Y=X+Z \end{align*} where $X$ is zero mean and unit variance and $Z \sim \mathcal{N}(0,1)$ and independent of $X$. Here are some bounds that I was able to come up with: Lower Bound: For example we can related it to MMSE via Jensen's inequality \begin{align*} E[(X-E[X|Y])^4] \ge E^2[(X-E[X|Y])^2]=MMSE^2 \end{align*} Upper Bound 1: Also, using Minkowski inequality and assuming $E[X^4]$ exists we can have  \begin{align*} E[(X-E[X|Y])^4]& \le (E[X^4]^{1/4}+E[E^4[X|Y]]^{1/4})^4 \\ &\le 2^4 E[X^4] \end{align*} Upper bound 2: Observe that  \begin{align*} (X-E[X|Y])=-(Z-E[Z|Y]) \end{align*} So,  \begin{align*} E[(X-E[X|Y])^4]=E(Z-E[Z|Y])^4] \le 2^4 E[Z^4]. \end{align*} The nice thing about upper bound 2 is that it does not require any assumptions about $X$. Moreover, since $Z$ is Gaussian $E[Z^4]$ is well defined. Need Help with Improve the upper bounds I have. For example, can we get rid of factor $ 2^4$? Can we say that over all random variables with  $E[X^2]\le 1 $ Gaussian $X$ maximizes $E[(X-E[X|Y])^4]$. This is true for $E[(X-E[X|Y])^2]$ and I don't feel that $E[(X-E[X|Y])^4]$ is very different. Any references on quantities $E[(X-E[X|Y])^4]$ or $\inf_{g(y)} E[(X-g(Y))^4]$ Thanks a lot for any hoel",,"['probability', 'probability-theory', 'reference-request', 'conditional-expectation', 'mean-square-error']"
38,Calculating $ \mathbb E \left[e^{-\mu W_T } 1_\left( {\min W_t \leq a} \right) \right]$ for a Wiener process,Calculating  for a Wiener process, \mathbb E \left[e^{-\mu W_T } 1_\left( {\min W_t \leq a} \right) \right],"Let $W_t$ be a standard Wiener process, $a$ some real number, and $\chi (x)$ the indicator function. I am trying to calculate the following expectation: $$ \mathbb E \left[e^{-\mu W_T } \chi \left( {\min_{0 \leq t \leq T} W_t \leq a} \right) \right] $$ I believe I have to use the reflection principle in some way. I have tried reasoning that conditioned on that $W_t$ hits $a$ at some point prior to $T$, the distribution of $W_T$ has to be centred around $a$. However, its variance is going to be $T-\tau$ where $\tau = \inf \lbrace s:W_s \leq a \rbrace$, so we get something like $$ = \mathbb E \left[ \mathbb E \left[ e^{-\mu W_T } \chi \left( {\min_{0 \leq t \leq T} W_t \leq a} \right) \bigg|  \min_{0 \leq t \leq T} W_t \leq a\right] \right]\\ = \mathbb E \left[ \mathbb E \left[ e^{-\mu W_T }  \bigg|  \min_{0 \leq t \leq T} W_t \leq a\right] \right] $$ and following the said reasoning the inner expectation is $$ \int_{\mathbb R} e^{-\mu x} \frac{1}{\sqrt{2 \pi}(T-\tau)}\exp{\left(\frac{-(x-a)^2}{2(T-\tau)} \right)}dx $$ and the one would have to find the distribution of $\tau$ to calculate the next expectation etc. So even if this were to be correct, it looks quite messy. I suspect there is supposed to be a simpler way, still involving the reflection principle? Any help highly appreciated.","Let $W_t$ be a standard Wiener process, $a$ some real number, and $\chi (x)$ the indicator function. I am trying to calculate the following expectation: $$ \mathbb E \left[e^{-\mu W_T } \chi \left( {\min_{0 \leq t \leq T} W_t \leq a} \right) \right] $$ I believe I have to use the reflection principle in some way. I have tried reasoning that conditioned on that $W_t$ hits $a$ at some point prior to $T$, the distribution of $W_T$ has to be centred around $a$. However, its variance is going to be $T-\tau$ where $\tau = \inf \lbrace s:W_s \leq a \rbrace$, so we get something like $$ = \mathbb E \left[ \mathbb E \left[ e^{-\mu W_T } \chi \left( {\min_{0 \leq t \leq T} W_t \leq a} \right) \bigg|  \min_{0 \leq t \leq T} W_t \leq a\right] \right]\\ = \mathbb E \left[ \mathbb E \left[ e^{-\mu W_T }  \bigg|  \min_{0 \leq t \leq T} W_t \leq a\right] \right] $$ and following the said reasoning the inner expectation is $$ \int_{\mathbb R} e^{-\mu x} \frac{1}{\sqrt{2 \pi}(T-\tau)}\exp{\left(\frac{-(x-a)^2}{2(T-\tau)} \right)}dx $$ and the one would have to find the distribution of $\tau$ to calculate the next expectation etc. So even if this were to be correct, it looks quite messy. I suspect there is supposed to be a simpler way, still involving the reflection principle? Any help highly appreciated.",,"['probability', 'stochastic-processes']"
39,Problems with this reasoning (Gambling),Problems with this reasoning (Gambling),,"Some mate of mine is some casino lover, and he usually says something like this to justify his hobby. ""Let's suppose we have a game, in which I gamble something, and if I win, I receive the double, and if I lose I don't receive anything. So I gamble 1 dollar. If I win, I stop gambling. If I lose, I gamble 2 dollars. If I win now, I stop gambling, but if I lose, I gamble 4 dollars, and so. Then in the end, when I win I will always have won 1 dollar"". I see a few problems with that reasoning. What if he keeps losing until he does not have more money? What does he do then? Another way for me to think it is, let's suppose that the game is not fair, so he has a negative expected value for this game. Independently of how he gambles, he'll tend to lose, and if the only way for it to happen is just not winning even once, it will happen eventually. So, is there any other problem with that reasoning? Sorry, I don't know which tag to use here, I think probability may not be too appropiate.","Some mate of mine is some casino lover, and he usually says something like this to justify his hobby. ""Let's suppose we have a game, in which I gamble something, and if I win, I receive the double, and if I lose I don't receive anything. So I gamble 1 dollar. If I win, I stop gambling. If I lose, I gamble 2 dollars. If I win now, I stop gambling, but if I lose, I gamble 4 dollars, and so. Then in the end, when I win I will always have won 1 dollar"". I see a few problems with that reasoning. What if he keeps losing until he does not have more money? What does he do then? Another way for me to think it is, let's suppose that the game is not fair, so he has a negative expected value for this game. Independently of how he gambles, he'll tend to lose, and if the only way for it to happen is just not winning even once, it will happen eventually. So, is there any other problem with that reasoning? Sorry, I don't know which tag to use here, I think probability may not be too appropiate.",,['probability']
40,Chernoff-like bound for small intervals in tail distribution,Chernoff-like bound for small intervals in tail distribution,,"I am searching for a Chernoff-like bound that controls the probability of small intervals in the tail distribution. More specifically, let $X_1, \ldots, X_n$ be independent random variables with values in $[0,1]$. Let $X := \sum X_i$ and $\mu := \mathbb E[X]$. My conjecture is that, for $\lambda \in [0,\sqrt{\mu}]$, an inequality like the following holds: $$\mathbb P[ \mu + \lambda \sqrt{\mu} \leq X \leq \mu + \lambda\sqrt{\mu} + 1] \leq \frac{e^{-\lambda^2/3}}{\sqrt{\pi \mu}}.$$ (I'll be happy if you need to include diverse absolute constants in the right hand side, even in the exponential... Also, note that I've chosen the width of the interval to be large enough to avoid discretization issues.) Here's why I think this inequality holds. First, it can be proved that $Var X \leq \mu$, with equality in the extreme case where $X$ is a Poisson distribution (and thus, intuitively, a sum of infinitely many independent random variables). When $X$ is a Poisson distribution, the inequality does hold. Moreover, by analogy with the normal distribution, my intuition is that, when $\lambda \geq 1$, because the Poisson distribution maximizes the variance, it also maximizes the left-hand side above (up to discretization artefacts). Indeed, the probability density function $f_\sigma(x)$ of $\mathcal N(0,\sigma^2)$ for $\sigma \in [0,1]$ and restricted to $x \geq 1$ is increasing in $\sigma$. Any help is welcome. Thank you! EDIT: It seems that, using a non-uniform Berry-Esseen bound, one can get a bound of $O \left( \frac{1}{\lambda^3 \sqrt{\mu}} \right)$. This does not seem tight to me though.","I am searching for a Chernoff-like bound that controls the probability of small intervals in the tail distribution. More specifically, let $X_1, \ldots, X_n$ be independent random variables with values in $[0,1]$. Let $X := \sum X_i$ and $\mu := \mathbb E[X]$. My conjecture is that, for $\lambda \in [0,\sqrt{\mu}]$, an inequality like the following holds: $$\mathbb P[ \mu + \lambda \sqrt{\mu} \leq X \leq \mu + \lambda\sqrt{\mu} + 1] \leq \frac{e^{-\lambda^2/3}}{\sqrt{\pi \mu}}.$$ (I'll be happy if you need to include diverse absolute constants in the right hand side, even in the exponential... Also, note that I've chosen the width of the interval to be large enough to avoid discretization issues.) Here's why I think this inequality holds. First, it can be proved that $Var X \leq \mu$, with equality in the extreme case where $X$ is a Poisson distribution (and thus, intuitively, a sum of infinitely many independent random variables). When $X$ is a Poisson distribution, the inequality does hold. Moreover, by analogy with the normal distribution, my intuition is that, when $\lambda \geq 1$, because the Poisson distribution maximizes the variance, it also maximizes the left-hand side above (up to discretization artefacts). Indeed, the probability density function $f_\sigma(x)$ of $\mathcal N(0,\sigma^2)$ for $\sigma \in [0,1]$ and restricted to $x \geq 1$ is increasing in $\sigma$. Any help is welcome. Thank you! EDIT: It seems that, using a non-uniform Berry-Esseen bound, one can get a bound of $O \left( \frac{1}{\lambda^3 \sqrt{\mu}} \right)$. This does not seem tight to me though.",,"['probability', 'inequality']"
41,"Does a function $f(x,y)$ exist that $\int_0^{1-x}f(x,y)dy=1$ and $\int_0^{1-y}f(x,y)dx=1$?",Does a function  exist that  and ?,"f(x,y) \int_0^{1-x}f(x,y)dy=1 \int_0^{1-y}f(x,y)dx=1","Suppose function $f(x,y)$ is defined on triangle $x\ge 0, y\ge 0, x+y\le 1$. Does such a function exist that $\int_0^{1-x}f(x,y)dy=1$ and $\int_0^{1-y}f(x,y)dx=1$ for any $x\in (0,1)$ and $y\in (0,1)$? Actually, I'm trying to find a probability distribution that both marginal distributions are uniform.","Suppose function $f(x,y)$ is defined on triangle $x\ge 0, y\ge 0, x+y\le 1$. Does such a function exist that $\int_0^{1-x}f(x,y)dy=1$ and $\int_0^{1-y}f(x,y)dx=1$ for any $x\in (0,1)$ and $y\in (0,1)$? Actually, I'm trying to find a probability distribution that both marginal distributions are uniform.",,"['calculus', 'probability', 'functional-analysis']"
42,How to prove that the module of characteristic function is less than one,How to prove that the module of characteristic function is less than one,,"I would like to know if my resolution is right... I want prove that $|\varphi(t)| = \vert\mathbb{E}[e^{i t X}]\vert\leq 1$ , $\forall t \in \mathbb{R}$ . $\it{proof:}$ First, note that $|\mathbb{E}[e^{i t X}]| \leq \mathbb{E}[|e^{i t X}|]$ . But, on the other side, we know that $|e^{i t X}| = |\cos(tX)+ i \sin(tX)| = 1$ . Then, $\mathbb{E}[|e^{i t X}|] = {\displaystyle\int_{\Omega}1dP} = P(\Omega) = 1$ . So, $|\varphi(t)| = \vert\mathbb{E}[e^{i t X}]\vert\leq  \mathbb{E}[|e^{i t X}|] = 1$","I would like to know if my resolution is right... I want prove that , . First, note that . But, on the other side, we know that . Then, . So,",|\varphi(t)| = \vert\mathbb{E}[e^{i t X}]\vert\leq 1 \forall t \in \mathbb{R} \it{proof:} |\mathbb{E}[e^{i t X}]| \leq \mathbb{E}[|e^{i t X}|] |e^{i t X}| = |\cos(tX)+ i \sin(tX)| = 1 \mathbb{E}[|e^{i t X}|] = {\displaystyle\int_{\Omega}1dP} = P(\Omega) = 1 |\varphi(t)| = \vert\mathbb{E}[e^{i t X}]\vert\leq  \mathbb{E}[|e^{i t X}|] = 1,"['probability', 'characteristic-functions']"
43,How many possible shuffles can be won perfectly?,How many possible shuffles can be won perfectly?,,"It is known that the possible shuffles of a deck of cards is $52!$, or ~$80658175170943878571660636856403766975289505440883277824000000000000$ different combinations. I have become aware of a game known as Idiot's Delight. The game is played in this manner: A deck is shuffled, and held in your left hand, face down. You begin pulling cards from the back of the deck, flipping them face up on top of the deck. You continue this process. When you have at least 4 cards, begin looking at the last 4 cards. If the first and last cards of the last four cards are the same suit, you may remove the two cards between them. If the first and last cards of the last four cards are the same value, you may remove all four cards. If the last four cards do not satisfy these conditions, continue drawing from the back of the deck, until you run out of face-down cards. Look at this example [Face Down Cards][$A$ ♠][$2$ ♣][$3$ ♦][$4$ ♠] Here, the $2$♣ and $3$♦ can be removed, and then play continues. You can win a game by correctly ""matching"" all the cards, or removing all cards from the face down section and containing no cards face up. This forces the last play to have the first and fourth card match in number. Correct me if I'm wrong, but I believe that this game is possible to be won, albeit if difficult. Assuming it is possible to be won, how would I calculate how many potential shuffles out of $52!$ would be possible to be won, if the game is played perfectly, and how could I design a proof for this? Perfect play, for the sake of this question, encompasses not knowing the order of the deck, but making the decision to remove cards whenever there is a possibility of such. EDIT: Assume for the sake of this question that a shuffle is a completely random selection of one of $52!$ permutations.","It is known that the possible shuffles of a deck of cards is $52!$, or ~$80658175170943878571660636856403766975289505440883277824000000000000$ different combinations. I have become aware of a game known as Idiot's Delight. The game is played in this manner: A deck is shuffled, and held in your left hand, face down. You begin pulling cards from the back of the deck, flipping them face up on top of the deck. You continue this process. When you have at least 4 cards, begin looking at the last 4 cards. If the first and last cards of the last four cards are the same suit, you may remove the two cards between them. If the first and last cards of the last four cards are the same value, you may remove all four cards. If the last four cards do not satisfy these conditions, continue drawing from the back of the deck, until you run out of face-down cards. Look at this example [Face Down Cards][$A$ ♠][$2$ ♣][$3$ ♦][$4$ ♠] Here, the $2$♣ and $3$♦ can be removed, and then play continues. You can win a game by correctly ""matching"" all the cards, or removing all cards from the face down section and containing no cards face up. This forces the last play to have the first and fourth card match in number. Correct me if I'm wrong, but I believe that this game is possible to be won, albeit if difficult. Assuming it is possible to be won, how would I calculate how many potential shuffles out of $52!$ would be possible to be won, if the game is played perfectly, and how could I design a proof for this? Perfect play, for the sake of this question, encompasses not knowing the order of the deck, but making the decision to remove cards whenever there is a possibility of such. EDIT: Assume for the sake of this question that a shuffle is a completely random selection of one of $52!$ permutations.",,"['probability', 'sequences-and-series', 'card-games']"
44,Seven-Card Stud with Random Hand Selection,Seven-Card Stud with Random Hand Selection,,"I was recently confronted with a number—$2727707$, actually—that started a short train of thought while I was placed on hold.  (This seems to happen quite often: both the observation of unusual numbers, and also being placed on hold.)  Suppose you had a seven-card poker hand, consisting of all sevens, the deuces of spades and hearts, and the ten of diamonds. Obviously, if you were playing ordinary stud, your best hand is the sevens and the ten.  But suppose once all hands are dealt, the actual five-card combinations for the sake of comparison are selected randomly (with each combination having an equal probability).  It is a simple, if somewhat tedious, matter to determine what the odds are that the combination in this case would be four of a kind, full house, three of a kind, two pair.  (The hand cannot be a straight or a flush, and must at least be two pair.) However, the general case is a bit more involved, in large part because the ranking of poker hands is somewhat irregular, involving a mix of sets of cards of the same rank, sets of cards of the same suit, rank sequences, and combinations of these.  With that in mind: Given two seven-card hands, is there any general method (that avoids brute force enumeration) for determining which one is likely to win a head-to-head matchup between randomly-selected five-card combinations? In the event that there is no such general method, is there a method by which it could be quickly determined, without brute force enumeration, that a hand dominates another (i.e., that any five-card combinations of one beats any five-card combination of the other)?","I was recently confronted with a number—$2727707$, actually—that started a short train of thought while I was placed on hold.  (This seems to happen quite often: both the observation of unusual numbers, and also being placed on hold.)  Suppose you had a seven-card poker hand, consisting of all sevens, the deuces of spades and hearts, and the ten of diamonds. Obviously, if you were playing ordinary stud, your best hand is the sevens and the ten.  But suppose once all hands are dealt, the actual five-card combinations for the sake of comparison are selected randomly (with each combination having an equal probability).  It is a simple, if somewhat tedious, matter to determine what the odds are that the combination in this case would be four of a kind, full house, three of a kind, two pair.  (The hand cannot be a straight or a flush, and must at least be two pair.) However, the general case is a bit more involved, in large part because the ranking of poker hands is somewhat irregular, involving a mix of sets of cards of the same rank, sets of cards of the same suit, rank sequences, and combinations of these.  With that in mind: Given two seven-card hands, is there any general method (that avoids brute force enumeration) for determining which one is likely to win a head-to-head matchup between randomly-selected five-card combinations? In the event that there is no such general method, is there a method by which it could be quickly determined, without brute force enumeration, that a hand dominates another (i.e., that any five-card combinations of one beats any five-card combination of the other)?",,"['probability', 'card-games']"
45,Probabilistic Logic,Probabilistic Logic,,"I was wondering if there is any system of logic that has been worked out that explicitly uses probabilistic notions at its foundation.  It would include ideas like, as a first principle, all statements are not be determined as true or false, but instead should be quantified with a number between 0 and 1.  So it simply avoids the law of the excluded middle.  I presume it might take Bayesian inference as a first principle as a way to define implication, i.e. the measure of truth of the statement $Y \implies X$ is given by $$ P(X|Y) = \frac{P(X, Y)}{P(Y)} $$ These ideas are used throughout probability but I am looking for a system where they have been axiomatized.","I was wondering if there is any system of logic that has been worked out that explicitly uses probabilistic notions at its foundation.  It would include ideas like, as a first principle, all statements are not be determined as true or false, but instead should be quantified with a number between 0 and 1.  So it simply avoids the law of the excluded middle.  I presume it might take Bayesian inference as a first principle as a way to define implication, i.e. the measure of truth of the statement is given by These ideas are used throughout probability but I am looking for a system where they have been axiomatized.","Y \implies X  P(X|Y) = \frac{P(X, Y)}{P(Y)} ","['probability', 'logic']"
46,Computing MMSE and conditional expectation,Computing MMSE and conditional expectation,,"Suppose we have three independent, zero mean, finite variance random variables $V,W,Z$ and where $W,Z$ are Gaussian random variables.  These random variables form a new random variable $Y$ \begin{align*} Y=V+W+Z \end{align*} I want to compute the following quantity \begin{align*} E[W(V-E[V|Y])] \end{align*} I have the following conjecture \begin{align*} &E[W(V-E[V|Y])]\\ &=E[(W+Z)(V-E[V|Y])]\sqrt{\frac{E[W^2]}{E[W^2]+E[Z^2]}} \text{ Problematic step}\\ &=E[(Y-V)(V-E[V|Y])]\sqrt{\frac{E[W^2]}{E[W^2]+E[Z^2]}}\\ &=E[(V-E[V|Y])^2]\sqrt{\frac{E[W^2]}{E[W^2]+E[Z^2]}} \text{ by orthogonality principle} \end{align*} In the  'Problematic Step' I am not completly sure I can add $Z$ and normalize. Is that step fine if yes how to argue it? For example, in the case when $V$ is Gaussian this is easy to verify. {Possible Approach:} I was thinking of using the following approach: In really we have to show that  \begin{align*} E[W E[V|Y]]= \alpha E[(W+Z) E[V|Y]] \end{align*} where the conjecture is that $\alpha=\sqrt{\frac{E[W^2]}{E[W^2]+E[Z^2]}}$. So, we really have to show that \begin{align*} &\int \int w E[V|Y=y] f_{W,Y} (w,y) dw dy\\ &= \alpha  \int \int \int (w+z) E[V|Y=y] f_{Z,W,Y} (z,w,y) dw dy dz\\ &= \alpha   \int \int s E[V|Y=y] f_{Y,S} (y,s) ds dy \end{align*} where $S=W+Z$. but how to show the last step?? The good thing is that $S=W+Z$ is Gaussian so maybe it will make things easeir. Thank you","Suppose we have three independent, zero mean, finite variance random variables $V,W,Z$ and where $W,Z$ are Gaussian random variables.  These random variables form a new random variable $Y$ \begin{align*} Y=V+W+Z \end{align*} I want to compute the following quantity \begin{align*} E[W(V-E[V|Y])] \end{align*} I have the following conjecture \begin{align*} &E[W(V-E[V|Y])]\\ &=E[(W+Z)(V-E[V|Y])]\sqrt{\frac{E[W^2]}{E[W^2]+E[Z^2]}} \text{ Problematic step}\\ &=E[(Y-V)(V-E[V|Y])]\sqrt{\frac{E[W^2]}{E[W^2]+E[Z^2]}}\\ &=E[(V-E[V|Y])^2]\sqrt{\frac{E[W^2]}{E[W^2]+E[Z^2]}} \text{ by orthogonality principle} \end{align*} In the  'Problematic Step' I am not completly sure I can add $Z$ and normalize. Is that step fine if yes how to argue it? For example, in the case when $V$ is Gaussian this is easy to verify. {Possible Approach:} I was thinking of using the following approach: In really we have to show that  \begin{align*} E[W E[V|Y]]= \alpha E[(W+Z) E[V|Y]] \end{align*} where the conjecture is that $\alpha=\sqrt{\frac{E[W^2]}{E[W^2]+E[Z^2]}}$. So, we really have to show that \begin{align*} &\int \int w E[V|Y=y] f_{W,Y} (w,y) dw dy\\ &= \alpha  \int \int \int (w+z) E[V|Y=y] f_{Z,W,Y} (z,w,y) dw dy dz\\ &= \alpha   \int \int s E[V|Y=y] f_{Y,S} (y,s) ds dy \end{align*} where $S=W+Z$. but how to show the last step?? The good thing is that $S=W+Z$ is Gaussian so maybe it will make things easeir. Thank you",,"['real-analysis', 'probability', 'probability-theory', 'expectation', 'conditional-expectation']"
47,shifted exponential distribution with inter-arrival time,shifted exponential distribution with inter-arrival time,,"Given that time interval $T^*$ in seconds between certain events has a negative exponential distribution. The instrument cannot detect intervals which are less than $\delta$ seconds. Let $T_1, ..., T_n$ be a sample of independent intervals measyred by the instrument. The distribution of one of those observation $T_i$ is the conditional distribution of $T^*$ given that $T^*>\delta$ In this question, if I want to find the probability density function of $T_i$, should I consider the shifted exponential distribution such that: $$f_T(t) = \begin{cases} \lambda e^{-\lambda (t- \delta)} & t>\delta, \\ 0 & otherwise \end{cases}$$ with $E(T)=\delta + \frac{1}{\lambda}$ and $Var(T) = \frac{1}{\lambda ^2}$ Thank you","Given that time interval $T^*$ in seconds between certain events has a negative exponential distribution. The instrument cannot detect intervals which are less than $\delta$ seconds. Let $T_1, ..., T_n$ be a sample of independent intervals measyred by the instrument. The distribution of one of those observation $T_i$ is the conditional distribution of $T^*$ given that $T^*>\delta$ In this question, if I want to find the probability density function of $T_i$, should I consider the shifted exponential distribution such that: $$f_T(t) = \begin{cases} \lambda e^{-\lambda (t- \delta)} & t>\delta, \\ 0 & otherwise \end{cases}$$ with $E(T)=\delta + \frac{1}{\lambda}$ and $Var(T) = \frac{1}{\lambda ^2}$ Thank you",,"['probability', 'statistics', 'probability-distributions']"
48,"PMF for K, the number of trails up to, but not including, the second success","PMF for K, the number of trails up to, but not including, the second success",,"I'm taking an MIT OCW course on Probability. Question: Al performs an experiment comprising a series of independent trials. On each trial, he simultaneously flips a set of three fair coins. Whenever all three coins land on the same side in any given trial, Al calls the trial a success. Find the PMF for K, the number of trials up to, but not including, the second success. My solution: Success occurs only when we get 3 heads or 3 tails $P(success) = 1/4$ In $k$ trails, we will 1 success, so the PMF is - $PMF = {{k}\choose{1}} * 1/4 * (3/4)^{k-1}$ Solution Given: $PMF = {{k}\choose{1}} * (1/4)^2 * (3/4)^{k-1}$ Can anyone explain what is wrong in my answer? Link to the solution: 2 (b)","I'm taking an MIT OCW course on Probability. Question: Al performs an experiment comprising a series of independent trials. On each trial, he simultaneously flips a set of three fair coins. Whenever all three coins land on the same side in any given trial, Al calls the trial a success. Find the PMF for K, the number of trials up to, but not including, the second success. My solution: Success occurs only when we get 3 heads or 3 tails $P(success) = 1/4$ In $k$ trails, we will 1 success, so the PMF is - $PMF = {{k}\choose{1}} * 1/4 * (3/4)^{k-1}$ Solution Given: $PMF = {{k}\choose{1}} * (1/4)^2 * (3/4)^{k-1}$ Can anyone explain what is wrong in my answer? Link to the solution: 2 (b)",,"['probability', 'probability-theory', 'geometric-probability']"
49,On the probability of singular matrices containing whole numbers,On the probability of singular matrices containing whole numbers,,"Today in class, my teacher was teaching determinants. He gave us problems to solve of various kinds, including various row-column operations and determinants properties. But one thing that remained common was the fact that in every other problem the value of the determinant was coming out to be $0$ . It was so very frequent that the two rows came out to be common, one row/column came out to be zero and finally the answer was zero. This motivated me to put forward a question which my teacher said was too hard for my fellow classmates. Please help me with this. Consider we have $9$ random integers chosen from the set $[0,n]$ in order. Then find the probability that in their given order these integers forms a $3 \times 3$ matrix which is singular. That is, say we choose $x_1, x_2, x_3, \dotsc, x_9$ , then find the probability that $$ \begin{vmatrix} x_1 & x_2 & x_3 \\ x_4 & x_5 & x_6 \\  x_7 & x_8 & x_9 \end{vmatrix} $$ is equal to $0$ .","Today in class, my teacher was teaching determinants. He gave us problems to solve of various kinds, including various row-column operations and determinants properties. But one thing that remained common was the fact that in every other problem the value of the determinant was coming out to be . It was so very frequent that the two rows came out to be common, one row/column came out to be zero and finally the answer was zero. This motivated me to put forward a question which my teacher said was too hard for my fellow classmates. Please help me with this. Consider we have random integers chosen from the set in order. Then find the probability that in their given order these integers forms a matrix which is singular. That is, say we choose , then find the probability that is equal to .","0 9 [0,n] 3 \times 3 x_1, x_2, x_3, \dotsc, x_9 
\begin{vmatrix}
x_1 & x_2 & x_3 \\
x_4 & x_5 & x_6 \\ 
x_7 & x_8 & x_9
\end{vmatrix}
 0","['probability', 'matrices', 'determinant', 'random-matrices']"
50,Number of solutions of an equation,Number of solutions of an equation,,"Fix a vector $x\in\{0,1\}^n$, and let $a$ be a random vector in $\mathbb{Z}^n_q$ for some prime $q$. Consider $y=ax$, and $S=\{x'\mid ax'=y\}$. I want to compute the probability that $\lvert S \rvert\geq k$ for some $k$ (when the probability is over the possible values of $a$) and I'm not sure how. If $a=0$ for example, we would have $2^{n}$ solutions, while if $a=(1,2,...,2^n)$ $x$ must be unique. But I'm not sure how to characterize in general the values of $a$ for which there are many solutions (or a few solutions).","Fix a vector $x\in\{0,1\}^n$, and let $a$ be a random vector in $\mathbb{Z}^n_q$ for some prime $q$. Consider $y=ax$, and $S=\{x'\mid ax'=y\}$. I want to compute the probability that $\lvert S \rvert\geq k$ for some $k$ (when the probability is over the possible values of $a$) and I'm not sure how. If $a=0$ for example, we would have $2^{n}$ solutions, while if $a=(1,2,...,2^n)$ $x$ must be unique. But I'm not sure how to characterize in general the values of $a$ for which there are many solutions (or a few solutions).",,"['linear-algebra', 'probability', 'matrix-equations']"
51,Random Variables and Statistic,Random Variables and Statistic,,"I'm studying Statistical Inference by Casella and I'm confused with the definitions of random variable & statistic. So let we have the probability space  $(\Omega, F, P)$ where $\Omega$ is the sample space, $F$ is the $\sigma-algebra$ and $P$ is the probability function. Then we define ""a"" random variable, a function that maps every element in the sample space to a real number in the interval $[0,1]$. After that we define a statistic; Let $X_1, X_2, .., X_n$ be a random sample of size n from a population and let $T(x_1, x_2, .., x_n)$ be a function whose domain includes the sample space of $(X_1, X_2, .., X_n)$. Then the random variable/vector   $T(X_1, X_2, .., X_n)$  is called a statistic. This is the part where I stuck. I think that we can define lots of different random variables on a probability space and then define different distribution functions. But what is the relation between our sample space, random variables and ""a"" statistic?","I'm studying Statistical Inference by Casella and I'm confused with the definitions of random variable & statistic. So let we have the probability space  $(\Omega, F, P)$ where $\Omega$ is the sample space, $F$ is the $\sigma-algebra$ and $P$ is the probability function. Then we define ""a"" random variable, a function that maps every element in the sample space to a real number in the interval $[0,1]$. After that we define a statistic; Let $X_1, X_2, .., X_n$ be a random sample of size n from a population and let $T(x_1, x_2, .., x_n)$ be a function whose domain includes the sample space of $(X_1, X_2, .., X_n)$. Then the random variable/vector   $T(X_1, X_2, .., X_n)$  is called a statistic. This is the part where I stuck. I think that we can define lots of different random variables on a probability space and then define different distribution functions. But what is the relation between our sample space, random variables and ""a"" statistic?",,"['probability', 'statistics', 'probability-theory', 'statistical-inference', 'sampling']"
52,Probability of turning a 3x3 Rubik's Cube 50 random turns and solving,Probability of turning a 3x3 Rubik's Cube 50 random turns and solving,,"So I am working with my secondary Algebra II class on permutations, combinations, and probability and had an idea for an interesting problem that I hope can work for their skill level.  We have been able to establish that the total number of permutations for a 3x3 Rubik's Cube is $$4.3252 \cdot 10^{19}$$ which they are good with and seem to understand the reasoning behind.  Now, I would like to give them a problem something like ""What is the probability of solving a 3x3 Rubik's Cube by making 50 random turns?  Assume that it is fully scrambled prior to your first turn.""  The problem is...I don't know how to solve the question I just asked!  I'm trained in Computer Science so I am familiar with combinatorics/probability but I'm not really sure how to tackle this problem. Part of me feels like it is way more advanced than I am anticipating but, again, I really am stuck on how to go about solving it and even if the solution is more advanced than is appropriate for a high school Algebra II class I am hoping to be able to glean atleast some of the reasoning out to show them an interesting application of what we have been learning. inb4 this should be on MESE.  The question is not about how to teach this problem but literally how to actually solve the problem","So I am working with my secondary Algebra II class on permutations, combinations, and probability and had an idea for an interesting problem that I hope can work for their skill level.  We have been able to establish that the total number of permutations for a 3x3 Rubik's Cube is $$4.3252 \cdot 10^{19}$$ which they are good with and seem to understand the reasoning behind.  Now, I would like to give them a problem something like ""What is the probability of solving a 3x3 Rubik's Cube by making 50 random turns?  Assume that it is fully scrambled prior to your first turn.""  The problem is...I don't know how to solve the question I just asked!  I'm trained in Computer Science so I am familiar with combinatorics/probability but I'm not really sure how to tackle this problem. Part of me feels like it is way more advanced than I am anticipating but, again, I really am stuck on how to go about solving it and even if the solution is more advanced than is appropriate for a high school Algebra II class I am hoping to be able to glean atleast some of the reasoning out to show them an interesting application of what we have been learning. inb4 this should be on MESE.  The question is not about how to teach this problem but literally how to actually solve the problem",,"['probability', 'combinatorics']"
53,Probability of another 3 integers with same sum and product as the first 3 integers,Probability of another 3 integers with same sum and product as the first 3 integers,,"Let us suppose $3$ integers are selected at random from a large range, say $$-1000\leq x\leq y\leq z\leq 1000$$ Now, we define the sum and product: $$\begin{align*}s&=x+y+z \\p&=xyz\end{align*}$$ ($s$ and $p$ will not be equal in most cases, sorry for the confusion) What is the probability that there exists another solution for $(x,y,z)$ that satisfies above 3 equations? (reordering of x, y and z not allowed) My friend gave me this question, and I have no idea where to start. If we limit ourselves to positive integers, is there a unique solution, or not?","Let us suppose $3$ integers are selected at random from a large range, say $$-1000\leq x\leq y\leq z\leq 1000$$ Now, we define the sum and product: $$\begin{align*}s&=x+y+z \\p&=xyz\end{align*}$$ ($s$ and $p$ will not be equal in most cases, sorry for the confusion) What is the probability that there exists another solution for $(x,y,z)$ that satisfies above 3 equations? (reordering of x, y and z not allowed) My friend gave me this question, and I have no idea where to start. If we limit ourselves to positive integers, is there a unique solution, or not?",,"['probability', 'algebra-precalculus', 'probability-theory', 'diophantine-equations']"
54,Expectation of couples surviving after some time.,Expectation of couples surviving after some time.,,"There are $2m$ persons forming $m$ couples who live together at a given time. Suppose that at some later time, the probability of each person being alive is $p$, independently of other persons. At that later time, let $A$ be the number of persons that are alive and let $S$ be the number of couples in which both partners are alive. For any number of total surviving persons $a$, find $E[S∣A=a]$. I don't have the answer to the problem. Here is my proposed solution, can anyone verify whether it is correct. Let $$(S| A= a) = (S_1| A = a) + (S_2| A = a) + ... + (S_m | A = a)$$ Here $(S_i| A = a)$ is a indicator variable indicating whether both members in a couple are alive or not. So the probability of $p(S_i | A = a)$ is given by $$\frac{p(S_i \cap A = a)}{p(A = a)}$$ where the numerator is given by $p^2 {n - 2 \choose a-2}p^{a-2}(1-p)^{n-2-(a-2)}$ and denominator is given by ${n \choose a}p^a(1-p)^{n-a}$. Therefore $E[S_i | A = a]$ is now known to us, and by symmetry all the indicator variables will have same expectation. So we have the final answer as $m \cdot \frac{p(S_i \cap A = a)}{p(A = a)}$. If I am wrong, point out from the specific place where I went wrong. Thanks!","There are $2m$ persons forming $m$ couples who live together at a given time. Suppose that at some later time, the probability of each person being alive is $p$, independently of other persons. At that later time, let $A$ be the number of persons that are alive and let $S$ be the number of couples in which both partners are alive. For any number of total surviving persons $a$, find $E[S∣A=a]$. I don't have the answer to the problem. Here is my proposed solution, can anyone verify whether it is correct. Let $$(S| A= a) = (S_1| A = a) + (S_2| A = a) + ... + (S_m | A = a)$$ Here $(S_i| A = a)$ is a indicator variable indicating whether both members in a couple are alive or not. So the probability of $p(S_i | A = a)$ is given by $$\frac{p(S_i \cap A = a)}{p(A = a)}$$ where the numerator is given by $p^2 {n - 2 \choose a-2}p^{a-2}(1-p)^{n-2-(a-2)}$ and denominator is given by ${n \choose a}p^a(1-p)^{n-a}$. Therefore $E[S_i | A = a]$ is now known to us, and by symmetry all the indicator variables will have same expectation. So we have the final answer as $m \cdot \frac{p(S_i \cap A = a)}{p(A = a)}$. If I am wrong, point out from the specific place where I went wrong. Thanks!",,"['probability', 'expectation']"
55,spectral norm of a sparse Gaussian matrix,spectral norm of a sparse Gaussian matrix,,"Suppose $G$ is an $m \times n$ matrix such that each entry of $G$ is a standard normal variable. We know that the spectral norm of $G$ scales as $\sqrt(m) + \sqrt(n)$. Now, given a set of indices $S$ suppose we construct a new matrix $A$ such that $A_{ij} = G_{ij}$ if $(i,j) \in S$, and 0 otherwise. Can we show that the spectral norm of $A$ is upper bounded by the spectral norm of $G$?","Suppose $G$ is an $m \times n$ matrix such that each entry of $G$ is a standard normal variable. We know that the spectral norm of $G$ scales as $\sqrt(m) + \sqrt(n)$. Now, given a set of indices $S$ suppose we construct a new matrix $A$ such that $A_{ij} = G_{ij}$ if $(i,j) \in S$, and 0 otherwise. Can we show that the spectral norm of $A$ is upper bounded by the spectral norm of $G$?",,"['probability', 'matrices', 'random']"
56,Conditional distribution of sum of binomial variables given the number of variables with non-zero values,Conditional distribution of sum of binomial variables given the number of variables with non-zero values,,"Let $T$ be a set of $M$ sets of size $(N / M)$ whose values are restricted to $\{0, 1\}$ where $M \leq N$ and $N \equiv 0 \pmod{M}.$ Each value is either $0$ or $1$ with some probability p Let $F(T)$ be the number of sets in $T$ whose subsets contain one or more entries of $1$. Let $G(T)$ be the sum of values over every set in $T$. Given $M$, $N$, and $F(T)$, but without knowing the exact contents of $T$, what is the probability distribution for $G(T)$ ? ex. With all values known: $$p = 0.5$$ $$N = 6$$ $$M = 3$$ $$T = \{\{0, 0\}, \{1, 1\}, \{0, 1\}\}$$ $$F(T) = 2 $$ $$G(T) = 3$$ What I want to solve for: $$p = 0.5$$ $$N = 6$$ $$M = 3$$ $$T = \ ???$$ $$F(T) = 2$$ What is the probability that $G(T) = X$ where $0 \leq X \leq N$ ?","Let $T$ be a set of $M$ sets of size $(N / M)$ whose values are restricted to $\{0, 1\}$ where $M \leq N$ and $N \equiv 0 \pmod{M}.$ Each value is either $0$ or $1$ with some probability p Let $F(T)$ be the number of sets in $T$ whose subsets contain one or more entries of $1$. Let $G(T)$ be the sum of values over every set in $T$. Given $M$, $N$, and $F(T)$, but without knowing the exact contents of $T$, what is the probability distribution for $G(T)$ ? ex. With all values known: $$p = 0.5$$ $$N = 6$$ $$M = 3$$ $$T = \{\{0, 0\}, \{1, 1\}, \{0, 1\}\}$$ $$F(T) = 2 $$ $$G(T) = 3$$ What I want to solve for: $$p = 0.5$$ $$N = 6$$ $$M = 3$$ $$T = \ ???$$ $$F(T) = 2$$ What is the probability that $G(T) = X$ where $0 \leq X \leq N$ ?",,"['probability', 'discrete-mathematics']"
57,Weighted sum of large numbers,Weighted sum of large numbers,,"From the law of large numbers, if $X_1,X_2,...X_N$ are i.i.d random variable, then we have $$\lim_{N \rightarrow \infty} \frac{1}{N} (\Sigma_1^N X_i)=\mu$$ where $\mu$ is the mean of $X_i$. What I want to ask is if there is a infinite sequence $a_i$. Can we express the value of $\lim_{N \rightarrow \infty} \frac{1}{N} (\Sigma_1^N a_iX_i)$ with $a_i$?","From the law of large numbers, if $X_1,X_2,...X_N$ are i.i.d random variable, then we have $$\lim_{N \rightarrow \infty} \frac{1}{N} (\Sigma_1^N X_i)=\mu$$ where $\mu$ is the mean of $X_i$. What I want to ask is if there is a infinite sequence $a_i$. Can we express the value of $\lim_{N \rightarrow \infty} \frac{1}{N} (\Sigma_1^N a_iX_i)$ with $a_i$?",,"['real-analysis', 'probability']"
58,If $E(|X|\log|X|)<\infty$ then is $E\left[\frac{|S_n|}{n}\ \log\left(\frac{|S_n|}{n}\right)\right]<\infty$?,If  then is ?,E(|X|\log|X|)<\infty E\left[\frac{|S_n|}{n}\ \log\left(\frac{|S_n|}{n}\right)\right]<\infty,"I am trying to finish a homework problem in my probability class. I think I am at the end of my problem if I can show that  $$E(|X|\log|X|)<\infty$$ implies that  $$E\left[\frac{|S_n|}{n}\ \log^+\left(\frac{|S_n|}{n}\right)\right]<M$$ For all $n$, where $S_n$ is the sum of $n$ i.i.d. random variables with distribution of $X$. $\log^+x=\max(\log x,0)$. I am not sure if this is true but I feel like it should be. Any ideas or help would be appreciated.","I am trying to finish a homework problem in my probability class. I think I am at the end of my problem if I can show that  $$E(|X|\log|X|)<\infty$$ implies that  $$E\left[\frac{|S_n|}{n}\ \log^+\left(\frac{|S_n|}{n}\right)\right]<M$$ For all $n$, where $S_n$ is the sum of $n$ i.i.d. random variables with distribution of $X$. $\log^+x=\max(\log x,0)$. I am not sure if this is true but I feel like it should be. Any ideas or help would be appreciated.",,"['probability', 'integration', 'measure-theory', 'probability-theory', 'probability-distributions']"
59,An integral with respect to the Haar measure on a unitary group,An integral with respect to the Haar measure on a unitary group,,"Let $A,D\in \mathbb{C}^{n \times n}$ be diagonal matrices. I need to calculate  $$\int_{U(n)}\det{(A-HDH^\dagger)}\,\mathrm{d}H$$  where $dH$ is the unit invariant Haar measure on the group of unitary matrices and $H^\dagger$ is the conjugate transpose of $H$. (If $A=I$ this is very easy to solve, but I want the answer for $A\neq I$ in terms of $A$ and $D$.)","Let $A,D\in \mathbb{C}^{n \times n}$ be diagonal matrices. I need to calculate  $$\int_{U(n)}\det{(A-HDH^\dagger)}\,\mathrm{d}H$$  where $dH$ is the unit invariant Haar measure on the group of unitary matrices and $H^\dagger$ is the conjugate transpose of $H$. (If $A=I$ this is very easy to solve, but I want the answer for $A\neq I$ in terms of $A$ and $D$.)",,"['probability', 'statistics', 'algebraic-geometry', 'differential-geometry', 'random-matrices']"
60,When is the probability of countable union equal to the limit of probabilities of finite unions?,When is the probability of countable union equal to the limit of probabilities of finite unions?,,"Lets say there are arbitrary sequence of sets $A_i$. When does the following below equation hold?, i.e., what specific properties of $A_i$ would make it invalid $$P\left(\lim_{n \to \infty} \bigcup_{i=1}^{n} A_i\right) = \lim_{n \to \infty}P\left(\bigcup_{i=1}^{n} A_i\right)$$","Lets say there are arbitrary sequence of sets $A_i$. When does the following below equation hold?, i.e., what specific properties of $A_i$ would make it invalid $$P\left(\lim_{n \to \infty} \bigcup_{i=1}^{n} A_i\right) = \lim_{n \to \infty}P\left(\bigcup_{i=1}^{n} A_i\right)$$",,"['probability', 'limits']"
61,Using Jensen's inequality to prove the Cauchy distribution has no mean,Using Jensen's inequality to prove the Cauchy distribution has no mean,,"I can see that there is no mean because  $\int x / \pi(1+x^{2})$ does not converge from -inf to inf. But my prof hinted at using Jensen's inequality for the proof. $$f(E(X)) \le E(f(X))$$ How can I use this? We have not learned Cauchy principal value, so that is not applicable.","I can see that there is no mean because  $\int x / \pi(1+x^{2})$ does not converge from -inf to inf. But my prof hinted at using Jensen's inequality for the proof. $$f(E(X)) \le E(f(X))$$ How can I use this? We have not learned Cauchy principal value, so that is not applicable.",,"['probability', 'statistics', 'probability-distributions', 'expectation']"
62,Convergence in distribution of stochastic equation solutions,Convergence in distribution of stochastic equation solutions,,"I'm studying from Kurtz's book ""Markov Processes Characterization and convergence"" and  I have a question about the convergence of processes in $\mathbb{Z}^d$ that are solution of some equation. (see chapter 6 section 4). Let $X^n$ be the solution of the equation $$X^n(t) = X(0) + \sum_{l\in \mathbb{Z}^d}l\cdot N_l\left(\int_0^t \beta^n_l(X^n(s))ds \right)$$ where $\beta^n_l \colon \mathbb{Z}^d \to [0,\infty)$, $N_l$ are independent real valued poisson processes with rate parameter $1$ and $X(0)$ is a random variable independent from $(N_l)$. Suppose that $\beta^n_l \stackrel{n\to \infty}{\longrightarrow} \beta_l$ pointwise and that $(\beta^n_l)_n$ is uniformly bounded. I want to prove that $$X^n \Rightarrow X \quad \text{ in}\quad D_{\mathbb{Z}^d}[0,\infty)$$ where $X$ is the solution of  $$X(t) = X(0) + \sum_{l\in \mathbb{Z}^d}l\cdot N_l\left(\int_0^t \beta_l(X(s))ds \right)$$ The most common approach to do this is to prove tension of the family $X^n$ and then prove that every limit point is $X$. For the tension part the only thing that I could manage to do is to prove tension of every process $R^n_l(t)= N_l\left(\int_0^t \beta_l^n(X^n(s))ds\right)$. This is easy because $$w'(R_l^n,\delta,T) \leq w'(N_l,\delta\cdot C_l, T\cdot C_l )$$ where $C_l$ is a uniform bound of the functions $(\beta_l^n)_n$ But with this I don't really have tension of the whole process, and if I had it I don't know how to prove that the unique limit point is $X$. In chapter 6 section 1 of the book there is a theorem that proves convergence of solution of a similar equation but with only one element on its right side. Any help will be appreciated.","I'm studying from Kurtz's book ""Markov Processes Characterization and convergence"" and  I have a question about the convergence of processes in $\mathbb{Z}^d$ that are solution of some equation. (see chapter 6 section 4). Let $X^n$ be the solution of the equation $$X^n(t) = X(0) + \sum_{l\in \mathbb{Z}^d}l\cdot N_l\left(\int_0^t \beta^n_l(X^n(s))ds \right)$$ where $\beta^n_l \colon \mathbb{Z}^d \to [0,\infty)$, $N_l$ are independent real valued poisson processes with rate parameter $1$ and $X(0)$ is a random variable independent from $(N_l)$. Suppose that $\beta^n_l \stackrel{n\to \infty}{\longrightarrow} \beta_l$ pointwise and that $(\beta^n_l)_n$ is uniformly bounded. I want to prove that $$X^n \Rightarrow X \quad \text{ in}\quad D_{\mathbb{Z}^d}[0,\infty)$$ where $X$ is the solution of  $$X(t) = X(0) + \sum_{l\in \mathbb{Z}^d}l\cdot N_l\left(\int_0^t \beta_l(X(s))ds \right)$$ The most common approach to do this is to prove tension of the family $X^n$ and then prove that every limit point is $X$. For the tension part the only thing that I could manage to do is to prove tension of every process $R^n_l(t)= N_l\left(\int_0^t \beta_l^n(X^n(s))ds\right)$. This is easy because $$w'(R_l^n,\delta,T) \leq w'(N_l,\delta\cdot C_l, T\cdot C_l )$$ where $C_l$ is a uniform bound of the functions $(\beta_l^n)_n$ But with this I don't really have tension of the whole process, and if I had it I don't know how to prove that the unique limit point is $X$. In chapter 6 section 1 of the book there is a theorem that proves convergence of solution of a similar equation but with only one element on its right side. Any help will be appreciated.",,"['probability', 'stochastic-processes']"
63,Limit of $\frac{1}{n}\log({n\choose np})$ without using Stirling's formula,Limit of  without using Stirling's formula,\frac{1}{n}\log({n\choose np}),"I am trying to evaluate the following limit: $$ \forall p\in(0,1) ,\lim_{n\rightarrow \infty} \frac{1}{n}\log{n\choose \lfloor np \rfloor} =H(p),$$ where $\lfloor x\rfloor$ means the greatest integer less than $x$ and $H(p)=-p\log(p)-q\log(q),q+p=1$ is the entropy of Bernoulli(p). I know it could be evaluated by Stirling's formula, but the exercise asks not to use it. One hint is using $$\forall \epsilon >0,\lim_{n\rightarrow \infty}P(|p^{\frac{S_n}{n}}q^{1-\frac{S_n}{n}} -e^{-H(p)}|>\epsilon)=0,$$ where $S_n=\sum_{i=1}^{n}X_i \;\text{and}\; X_i \;\text {iid, Bernoulli(p)}$. This follows from strong law of large number. Since $S_n\sim B(n,p)$, we could have  a term involving ${n\choose \lfloor np \rfloor}p^{\lfloor  np\rfloor}q^{n-\lfloor np \rfloor}$ in computing $P(|p^{\frac{S_n}{n}}q^{1-\frac{S_n}{n}} -e^{-H(p)}|<\epsilon)$, but then how to proceed? Any help would be appreciated!","I am trying to evaluate the following limit: $$ \forall p\in(0,1) ,\lim_{n\rightarrow \infty} \frac{1}{n}\log{n\choose \lfloor np \rfloor} =H(p),$$ where $\lfloor x\rfloor$ means the greatest integer less than $x$ and $H(p)=-p\log(p)-q\log(q),q+p=1$ is the entropy of Bernoulli(p). I know it could be evaluated by Stirling's formula, but the exercise asks not to use it. One hint is using $$\forall \epsilon >0,\lim_{n\rightarrow \infty}P(|p^{\frac{S_n}{n}}q^{1-\frac{S_n}{n}} -e^{-H(p)}|>\epsilon)=0,$$ where $S_n=\sum_{i=1}^{n}X_i \;\text{and}\; X_i \;\text {iid, Bernoulli(p)}$. This follows from strong law of large number. Since $S_n\sim B(n,p)$, we could have  a term involving ${n\choose \lfloor np \rfloor}p^{\lfloor  np\rfloor}q^{n-\lfloor np \rfloor}$ in computing $P(|p^{\frac{S_n}{n}}q^{1-\frac{S_n}{n}} -e^{-H(p)}|<\epsilon)$, but then how to proceed? Any help would be appreciated!",,"['probability', 'combinatorics', 'binomial-coefficients', 'factorial', 'entropy']"
64,Estimating the support of a probability density function,Estimating the support of a probability density function,,"The inverse moment problem deals with the reconstruction of a probability density function (PDF) of a random variable (RV) by means of its statistical moments. In the special case of the Hausdorff moment problem, where the PDF has a finite support, how could one estimate this support using the moments? Note that, without loss of generality, the Hausdorff moment problem is usually solved for the standard support of $[0,1]$. But we must know what is the support of our PDF before transforming the RV to the mentioned standard support. A related paper: http://www.sciencedirect.com/science/article/pii/S0167715208000539","The inverse moment problem deals with the reconstruction of a probability density function (PDF) of a random variable (RV) by means of its statistical moments. In the special case of the Hausdorff moment problem, where the PDF has a finite support, how could one estimate this support using the moments? Note that, without loss of generality, the Hausdorff moment problem is usually solved for the standard support of $[0,1]$. But we must know what is the support of our PDF before transforming the RV to the mentioned standard support. A related paper: http://www.sciencedirect.com/science/article/pii/S0167715208000539",,"['probability', 'probability-theory', 'probability-distributions', 'random-variables', 'inverse-problems']"
65,Cumulative distribution function - why is it so important?,Cumulative distribution function - why is it so important?,,"Why CDF is so imporant that it's defined before probability density function in most textbooks? What makes it so important? Was it defined in that way just for practical reasons, or there is a deeper, mathematical reason of how it's defined?","Why CDF is so imporant that it's defined before probability density function in most textbooks? What makes it so important? Was it defined in that way just for practical reasons, or there is a deeper, mathematical reason of how it's defined?",,"['probability', 'probability-theory']"
66,Finding $L^1$ centers of sets of probability distributions,Finding  centers of sets of probability distributions,L^1,"Let $\mathcal{P}^n = \{ x \in \mathbb{R}^n : x \geq 0, \sum x = 1\}$. Suppose I have $p_1, \ldots, p_m \in \mathcal{P}^n$. I want to find an $L^1$ center for these points. i.e. $q \in \mathcal{P}^n$ such that $\sum\limits_{j \leq n, k \leq m} |q_j - (p_k)_j|$ is minimized. Such a point obviously exists because $\mathcal{P}^n$ is compact, but I'd like to actually find one. Any ideas how? I'm happy with an algorithm that would let me calculate the answer numerically as I don't really expect there's a closed form. Approximate answers are OK too. Notes: If we drop the constraint that $\sum q = 1$ then letting $q_j$ be a median of $\{ (p_k)_j : k \leq m\}$ works, but in general this will not sum to 1. If we instead want the $L^2$ center then $\frac{1}{m} \sum p_j$ works, but this will generally not be an $L^1$ center. Any answer to this is presumably non-unique. It would be nice if there was some good ""canonical"" choice of center (e.g. one that minimizes the $L^2$ distance amongst the $L^1$ centers), but I don't really care that much. Answers which generalize to infinite measure spaces would be great, but I'm not holding out much hope.","Let $\mathcal{P}^n = \{ x \in \mathbb{R}^n : x \geq 0, \sum x = 1\}$. Suppose I have $p_1, \ldots, p_m \in \mathcal{P}^n$. I want to find an $L^1$ center for these points. i.e. $q \in \mathcal{P}^n$ such that $\sum\limits_{j \leq n, k \leq m} |q_j - (p_k)_j|$ is minimized. Such a point obviously exists because $\mathcal{P}^n$ is compact, but I'd like to actually find one. Any ideas how? I'm happy with an algorithm that would let me calculate the answer numerically as I don't really expect there's a closed form. Approximate answers are OK too. Notes: If we drop the constraint that $\sum q = 1$ then letting $q_j$ be a median of $\{ (p_k)_j : k \leq m\}$ works, but in general this will not sum to 1. If we instead want the $L^2$ center then $\frac{1}{m} \sum p_j$ works, but this will generally not be an $L^1$ center. Any answer to this is presumably non-unique. It would be nice if there was some good ""canonical"" choice of center (e.g. one that minimizes the $L^2$ distance amongst the $L^1$ centers), but I don't really care that much. Answers which generalize to infinite measure spaces would be great, but I'm not holding out much hope.",,"['probability', 'optimization', 'normed-spaces']"
67,finding n in binomial distribution,finding n in binomial distribution,,"A player tosses a coin $2n$ times. The probability of head is $0.48$.  The player wins if he gets head more than $n$ times. But he can choose the number $n$ before the game. What $n$ should he choose to increase the probability of winning, and what will be the probability? Does someone have ideas? As I understand it is a binomial distribution, and we need to find $n$ to maximize  $$\operatorname{Pr}(n<X\le 2n) = \sum\limits_{k = n + 1}^{2n} \binom{2n}{k}p^{k}q^{2n-k}$$ but this sum is not differentiable.","A player tosses a coin $2n$ times. The probability of head is $0.48$.  The player wins if he gets head more than $n$ times. But he can choose the number $n$ before the game. What $n$ should he choose to increase the probability of winning, and what will be the probability? Does someone have ideas? As I understand it is a binomial distribution, and we need to find $n$ to maximize  $$\operatorname{Pr}(n<X\le 2n) = \sum\limits_{k = n + 1}^{2n} \binom{2n}{k}p^{k}q^{2n-k}$$ but this sum is not differentiable.",,"['probability', 'probability-theory']"
68,Rao-Cramer lower bound regularity condition and dominated convergence,Rao-Cramer lower bound regularity condition and dominated convergence,,"Let $(\mathcal{X}, \mathcal{F}, (\mathbb{P}_\vartheta)_{\vartheta \in \Theta})$ be a statistical model dominated by a sigma-finite measure $\mu$ with Likelihood-function $L(\vartheta, x)$ which is assumed to be continuously differentiable in $\vartheta$. One regularity condition, used in the proof of the Rao-Cramer lower bound in our lecture is: $\int_{\mathcal{X}} \nabla_{\vartheta} L(\vartheta, x) \mu(dx) = \nabla_{\vartheta}\int_{\mathcal{X}} L(\vartheta, x) \mu(dx) \;\;\;\;\;\; (1) $ It was argued that that this condition holds due to the dominated convergence theorem if every $\vartheta_0 \in \Theta$ has a neighborhood $U_{\vartheta_0} \subseteq \Theta$ such that $\int_{\mathcal{X}} sup_{\vartheta \in U_{\vartheta_0}} \left[\nabla_{\vartheta} L(\vartheta, x) \right] \mu(dx) < \infty\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(2)$ I tried to understand the argument by considering a one-dimensional $\vartheta$ first. If the dominated convergence theorem is to be applied, taking the derivative has to be expressed as the limit of the difference quotient: $\int_{\mathcal{X}} sup_{n \in \mathbb{N}} \frac{L(\vartheta + \frac{1}{n}, x)-L(\vartheta, x)}{\frac{1}{n}} \mu(dx)<\infty \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(3)$ But i don't understand how i get from here to (a one-dimensional version of) equation (2). Can somebody provide the missing steps? Edit:  Ok i got it. In equation (3) the mean value theorem alows to replace the difference quotient with the derivative w.r.t. $\vartheta$ evaluated at some $\vartheta^*\;  \in\; [\vartheta, \vartheta+\frac{1}{n}]$. Since we need to consider an endpiece of the difference-quotient only, $n$ can be chosen arbitrarily large, especially such that $[\vartheta, \vartheta+\frac{1}{n}] \subseteq \Theta$.","Let $(\mathcal{X}, \mathcal{F}, (\mathbb{P}_\vartheta)_{\vartheta \in \Theta})$ be a statistical model dominated by a sigma-finite measure $\mu$ with Likelihood-function $L(\vartheta, x)$ which is assumed to be continuously differentiable in $\vartheta$. One regularity condition, used in the proof of the Rao-Cramer lower bound in our lecture is: $\int_{\mathcal{X}} \nabla_{\vartheta} L(\vartheta, x) \mu(dx) = \nabla_{\vartheta}\int_{\mathcal{X}} L(\vartheta, x) \mu(dx) \;\;\;\;\;\; (1) $ It was argued that that this condition holds due to the dominated convergence theorem if every $\vartheta_0 \in \Theta$ has a neighborhood $U_{\vartheta_0} \subseteq \Theta$ such that $\int_{\mathcal{X}} sup_{\vartheta \in U_{\vartheta_0}} \left[\nabla_{\vartheta} L(\vartheta, x) \right] \mu(dx) < \infty\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(2)$ I tried to understand the argument by considering a one-dimensional $\vartheta$ first. If the dominated convergence theorem is to be applied, taking the derivative has to be expressed as the limit of the difference quotient: $\int_{\mathcal{X}} sup_{n \in \mathbb{N}} \frac{L(\vartheta + \frac{1}{n}, x)-L(\vartheta, x)}{\frac{1}{n}} \mu(dx)<\infty \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(3)$ But i don't understand how i get from here to (a one-dimensional version of) equation (2). Can somebody provide the missing steps? Edit:  Ok i got it. In equation (3) the mean value theorem alows to replace the difference quotient with the derivative w.r.t. $\vartheta$ evaluated at some $\vartheta^*\;  \in\; [\vartheta, \vartheta+\frac{1}{n}]$. Since we need to consider an endpiece of the difference-quotient only, $n$ can be chosen arbitrarily large, especially such that $[\vartheta, \vartheta+\frac{1}{n}] \subseteq \Theta$.",,"['probability', 'statistics', 'random-variables', 'estimation']"
69,When will this generalized binomial model generate an exchangeable sequence?,When will this generalized binomial model generate an exchangeable sequence?,,"Start with a generalized binomial model $$P(X_{n+1}=1\mid \mathcal{F}_n)=\theta_n+ n^{-1} d_n \sum_{i=1}^n X_i$$ $$P(X_{n+1}=1)=p_{n+1}=\theta_n + n^{-1}d_n \sum_{i=1}^n p_i$$ $$P(X_1 = 1)= \theta_0$$ With $0\leq \theta_n+ d_n <1$. This generates a random sequence $X=(X_1,X_2,\dots,X_N)$. I'd like to find appropriate conditions on $(\theta_n)$ and $(d_n)$ so that $X$ is exchangeable. What kind of techniques can I use to figure this out for a general $N$? I'd start by testing a hypothesis that we need $(\theta_n),(d_n)$ constant sequences such that $p_m=p = \frac{\theta_n}{1-d_n}=\frac{\theta}{1-d}$ for all $m$, but I can't think of an efficient way to check this beyond some brute force that seems like it will quickly get unwieldy. And within this case, $d=0$ would obviously give exchangeability, but I'd like something nontrivial. I'd be interested in exchangeability results for other generalizations which allow for dependence between trials as well.","Start with a generalized binomial model $$P(X_{n+1}=1\mid \mathcal{F}_n)=\theta_n+ n^{-1} d_n \sum_{i=1}^n X_i$$ $$P(X_{n+1}=1)=p_{n+1}=\theta_n + n^{-1}d_n \sum_{i=1}^n p_i$$ $$P(X_1 = 1)= \theta_0$$ With $0\leq \theta_n+ d_n <1$. This generates a random sequence $X=(X_1,X_2,\dots,X_N)$. I'd like to find appropriate conditions on $(\theta_n)$ and $(d_n)$ so that $X$ is exchangeable. What kind of techniques can I use to figure this out for a general $N$? I'd start by testing a hypothesis that we need $(\theta_n),(d_n)$ constant sequences such that $p_m=p = \frac{\theta_n}{1-d_n}=\frac{\theta}{1-d}$ for all $m$, but I can't think of an efficient way to check this beyond some brute force that seems like it will quickly get unwieldy. And within this case, $d=0$ would obviously give exchangeability, but I'd like something nontrivial. I'd be interested in exchangeability results for other generalizations which allow for dependence between trials as well.",,"['probability', 'probability-theory', 'probability-distributions']"
70,Find the chance that $a^3 + b^3 \equiv 0 (\mod 3)$,Find the chance that,a^3 + b^3 \equiv 0 (\mod 3),"We are given set of integer numbers $\{1,2, \dots N\}$. $N \ge 3$ Then perform a drawing with replacement of two elements $a$ and $b$. Problem is to find the probability of following statement holding true: $a^3 + b^3 \equiv 0\space (\mod 3)$ Solution draft I believe Little Fermat theorem will be of help here, $ a^3 \equiv a \space(\mod 3)$. From there we conclude that either $a$ and $b$ both must be divisible by 3, or one must have residue 1 and other must have 2. Total amount of possible $a$ and $b$ pairs is $n^2$. Case when $3|a$ and $3|b$ is achievable for ${\lfloor \frac n 3 \rfloor}^2$ pairs. Last case holds for the same amount of pairs. Hence, answer would be $\frac {2{\lfloor \frac n 3 \rfloor}^2} {n^2} = \frac 2 9$ Is that a correct answer and is that legal to drop lowerbound around $\frac n 3$ here? Thanks! Edit Solution above is wrong. Suppose, $N=4$ than (3,3) (2,1) (2,4) (1,2) (4,2) fits, hence $P= \frac 5 {16}$. I would appreciate some suggestions. Edit 2 $$     P = \begin{cases}     \frac 1 3 & if N \equiv 0 (\mod 3 )\\     \frac {{\lfloor \frac n 3 \rfloor}^2 + 2 \cdot \lfloor \frac n 3 \rfloor \lceil \frac n 3 \rceil}{n^2} & N \equiv 1 (\mod 3 )\\ \frac {{\lfloor \frac n 3 \rfloor}^2 + 2 \cdot \lceil \frac n 3 \rceil \lceil \frac n 3 \rceil}{n^2} & N \equiv 2 (\mod 3 ) \end{cases}$$","We are given set of integer numbers $\{1,2, \dots N\}$. $N \ge 3$ Then perform a drawing with replacement of two elements $a$ and $b$. Problem is to find the probability of following statement holding true: $a^3 + b^3 \equiv 0\space (\mod 3)$ Solution draft I believe Little Fermat theorem will be of help here, $ a^3 \equiv a \space(\mod 3)$. From there we conclude that either $a$ and $b$ both must be divisible by 3, or one must have residue 1 and other must have 2. Total amount of possible $a$ and $b$ pairs is $n^2$. Case when $3|a$ and $3|b$ is achievable for ${\lfloor \frac n 3 \rfloor}^2$ pairs. Last case holds for the same amount of pairs. Hence, answer would be $\frac {2{\lfloor \frac n 3 \rfloor}^2} {n^2} = \frac 2 9$ Is that a correct answer and is that legal to drop lowerbound around $\frac n 3$ here? Thanks! Edit Solution above is wrong. Suppose, $N=4$ than (3,3) (2,1) (2,4) (1,2) (4,2) fits, hence $P= \frac 5 {16}$. I would appreciate some suggestions. Edit 2 $$     P = \begin{cases}     \frac 1 3 & if N \equiv 0 (\mod 3 )\\     \frac {{\lfloor \frac n 3 \rfloor}^2 + 2 \cdot \lfloor \frac n 3 \rfloor \lceil \frac n 3 \rceil}{n^2} & N \equiv 1 (\mod 3 )\\ \frac {{\lfloor \frac n 3 \rfloor}^2 + 2 \cdot \lceil \frac n 3 \rceil \lceil \frac n 3 \rceil}{n^2} & N \equiv 2 (\mod 3 ) \end{cases}$$",,"['probability', 'combinatorics', 'elementary-number-theory', 'proof-verification']"
71,Does this non-negative non-increasing function eventually attain $0$,Does this non-negative non-increasing function eventually attain,0,"Let $\phi(z): \mathbb{R}\rightarrow [0,B]$, with $B>0$,  be a non-negative and non-increasing  function such that $\phi(0) = B$ and   \begin{align} \phi(z) = \max(0, E[\phi(z+X)]+a\mu - c), \end{align}   where $X$ is a random variable with mean $E[X]=\mu>0$ such that $c - a\mu=\epsilon > 0$ and finite second moment. Show that $\phi(z_0) = 0$ for some sufficiently large $z_0$. I came across this argument in a paper by Thomas Ferguson about optimal stopping rules [1]: Suppose $\phi(z)>0$ for any $z$, then $E[\phi(z+X)] > c - a\mu = \epsilon > 0$, which since $\phi$ is non-increasing implies $\phi(z)>\epsilon$, which in turn implies that $E[\phi(z+X)]>2\epsilon$ for sufficiently large $z$, etc. Eventually this would imply that $E[\phi(z+X)]>B$, a contradiction. However, it is not clear to me, how one can infer that $E[\phi(z+X)]> \epsilon$ implies $\phi(z)>\epsilon$ when there are no additional constraints on $X$ such as boundedness. [1] Thomas S. Ferguson, ""Some Time-invariant Stopping Rule Problems"".","Let $\phi(z): \mathbb{R}\rightarrow [0,B]$, with $B>0$,  be a non-negative and non-increasing  function such that $\phi(0) = B$ and   \begin{align} \phi(z) = \max(0, E[\phi(z+X)]+a\mu - c), \end{align}   where $X$ is a random variable with mean $E[X]=\mu>0$ such that $c - a\mu=\epsilon > 0$ and finite second moment. Show that $\phi(z_0) = 0$ for some sufficiently large $z_0$. I came across this argument in a paper by Thomas Ferguson about optimal stopping rules [1]: Suppose $\phi(z)>0$ for any $z$, then $E[\phi(z+X)] > c - a\mu = \epsilon > 0$, which since $\phi$ is non-increasing implies $\phi(z)>\epsilon$, which in turn implies that $E[\phi(z+X)]>2\epsilon$ for sufficiently large $z$, etc. Eventually this would imply that $E[\phi(z+X)]>B$, a contradiction. However, it is not clear to me, how one can infer that $E[\phi(z+X)]> \epsilon$ implies $\phi(z)>\epsilon$ when there are no additional constraints on $X$ such as boundedness. [1] Thomas S. Ferguson, ""Some Time-invariant Stopping Rule Problems"".",,"['real-analysis', 'probability']"
72,"Let $X$ be a continuous random variable with cdf $F$. Show that $Y = F(X)$ has uniform $(0,1)$ distribution and therefore $X = F^{−1}(Y)$",Let  be a continuous random variable with cdf . Show that  has uniform  distribution and therefore,"X F Y = F(X) (0,1) X = F^{−1}(Y)","Let $X$ be a continuous random variable with cdf $F$. Show that $Y = F(X)$ has uniform $(0,1)$ distribution and therefore $X = F^{−1}(Y)$. My Sol: $P(Y \leq y ) = P(F(X) \leq y) = P\left(F^{-1}(F(X)) \leq F^{-1}(y)\right) = P(X \leq F^{-1}(y)) = F(F^{-1}(y)) = y$ so at the endpoints we have that $P(Y \leq y) = 1$ for $y \geq 1$ and $P(Y \leq y) = 0$ for $y\leq0$ therefore showing $Y= F(X)$ has uniform $(0,1)$ distribution and therefore $X = F^{-1}(Y)$.","Let $X$ be a continuous random variable with cdf $F$. Show that $Y = F(X)$ has uniform $(0,1)$ distribution and therefore $X = F^{−1}(Y)$. My Sol: $P(Y \leq y ) = P(F(X) \leq y) = P\left(F^{-1}(F(X)) \leq F^{-1}(y)\right) = P(X \leq F^{-1}(y)) = F(F^{-1}(y)) = y$ so at the endpoints we have that $P(Y \leq y) = 1$ for $y \geq 1$ and $P(Y \leq y) = 0$ for $y\leq0$ therefore showing $Y= F(X)$ has uniform $(0,1)$ distribution and therefore $X = F^{-1}(Y)$.",,"['probability', 'statistics', 'probability-distributions', 'random-variables', 'uniform-distribution']"
73,On the average length of the Steiner net for $n$ randomly chosen points in the unit square,On the average length of the Steiner net for  randomly chosen points in the unit square,n,"$n$ points are randomly chosen in the unit square with respect to the uniform measure. What is the average length $L$ of the associated Steiner net (tree of minimum length through each of the $n$ points)? Heuristics show that we may expect something like $C\cdot\sqrt{n}$: a sub-optimal net is achieved by considering a square spiral with width $\approx\frac{1}{\sqrt{n}}$ and connecting every point with the closest point of the spiral. Another natural approach is to consider the boundary of the convex envelope, remove it, consider the boundary of the convex envelope of the remaining points and so on. Divide-and-conquer approaches looks interesting, too. However, despite the fact that $L\ll\sqrt{n}$ is straightforward to prove, I have not managed to prove that $L\geq D\cdot\sqrt{n}$, neither $$ L\sim C\sqrt{n} $$ with an explicit constant $C$. Update : By the asymptotics on the EMST by Micheal Steele and the fact that the Steiner net is longer than a constant times the length of the minimum spanning tree, we have the double inequality $$ A\sqrt{n} \leq L \leq B\sqrt{n}$$ with $B\leq\frac{5}{2}$ by my previous construction. It remains to prove that: $$ L\sim C\sqrt{n}$$ for some explicit constant $C$.","$n$ points are randomly chosen in the unit square with respect to the uniform measure. What is the average length $L$ of the associated Steiner net (tree of minimum length through each of the $n$ points)? Heuristics show that we may expect something like $C\cdot\sqrt{n}$: a sub-optimal net is achieved by considering a square spiral with width $\approx\frac{1}{\sqrt{n}}$ and connecting every point with the closest point of the spiral. Another natural approach is to consider the boundary of the convex envelope, remove it, consider the boundary of the convex envelope of the remaining points and so on. Divide-and-conquer approaches looks interesting, too. However, despite the fact that $L\ll\sqrt{n}$ is straightforward to prove, I have not managed to prove that $L\geq D\cdot\sqrt{n}$, neither $$ L\sim C\sqrt{n} $$ with an explicit constant $C$. Update : By the asymptotics on the EMST by Micheal Steele and the fact that the Steiner net is longer than a constant times the length of the minimum spanning tree, we have the double inequality $$ A\sqrt{n} \leq L \leq B\sqrt{n}$$ with $B\leq\frac{5}{2}$ by my previous construction. It remains to prove that: $$ L\sim C\sqrt{n}$$ for some explicit constant $C$.",,"['probability', 'geometry', 'inequality']"
74,sum of dependent bernoulli random variables is poisson?,sum of dependent bernoulli random variables is poisson?,,"Suppose you have a sequence of bernoulli variables, $X_n$ with $\frac{1}{n} = \mathbb{P}(X_n = 1) = 1 - \mathbb{P}(X_n = 0)$ and let $S_n = \sum_{k=1}^{n} X_k X_{k+1} $. The goal is that $S = \lim_{n \rightarrow \infty} S_n$ is Poisson distributed with parameter 1. It's just dependent enough so that we can't apply the central limit theorem for triangular arrays. A friend and I were asked to prove this and we've been racking our heads against the wall with this. Initially, I thought it might be false, but I computed the variance and some magical cancellation happens so that it gives you the right thing. I have no idea how to proceed and most arguments I try end up becoming too complicated. Is there something obvious that I'm missing? All help is appreciated!","Suppose you have a sequence of bernoulli variables, $X_n$ with $\frac{1}{n} = \mathbb{P}(X_n = 1) = 1 - \mathbb{P}(X_n = 0)$ and let $S_n = \sum_{k=1}^{n} X_k X_{k+1} $. The goal is that $S = \lim_{n \rightarrow \infty} S_n$ is Poisson distributed with parameter 1. It's just dependent enough so that we can't apply the central limit theorem for triangular arrays. A friend and I were asked to prove this and we've been racking our heads against the wall with this. Initially, I thought it might be false, but I computed the variance and some magical cancellation happens so that it gives you the right thing. I have no idea how to proceed and most arguments I try end up becoming too complicated. Is there something obvious that I'm missing? All help is appreciated!",,"['probability', 'poisson-distribution']"
75,A question related to cardinality and probability,A question related to cardinality and probability,,"I have a question attached related to both probability and cardinality. Let me know if my formulation of the problem is non-rigorous or confusing. Any proof or suggestions are appreciated.Thank you all. The question follows. Consider a set $I$ consists of $N$ incidents. $I=\{i_{1},i_{2},...,i_{k},...i_{N}\}$ Each incident has a probability to happen, i.e. incident $i_{k}$ happens with the probability $r_{k}$. Without loss of generality, we assume $r_{1}\geq r_{2}\geq ... \geq r_{k}\geq ... \geq r_{N}$ Given a constant $n<N$, we can have set $I_{1}=\{i_{1},i_{2},...,i_{n}\}$. Apparently, $|I_{1}|=n$ and $I_{1}\subset I$. Define a mapping $I\to S$ with $S=\{s_{1},s_{2},...,s_{k},...s_{N}\}$ subject to $ s_{k} = \left\{ \begin{array}{ccc} 1 &\mbox{ (Pr=$r_{k}$)} \\ 0 &\mbox{ (Pr=$1-r_{k}$)} \\ \end{array} \right. $ Pick out the incidents with correspond $s$ being 1 to form the set $I_{2}$ , i.e. $I_{2}=\{i_{m_{1}},i_{m_{2}},...,i_{m_{M}}\} \quad \mbox{and} \quad s_{m_{k}}=1 \quad k=1,2,...,M $ Apparently, $|I_{2}|=M$ and $I_{2}\subset I$. Note that there could be $I_{2}\ne I_{1}$ and $|I_{2}| \ne |I_{1}|$. The question is, If we have two set $A$ and $B$ with following assumptions: (1)$ A\subset I$ and $B\subset I$ (2)$|A|=|B|=n$ (3)$ |A \cap I_{1}| \geq |B \cap I_{1}| $ Is the following statement true? $ E(|A \cap I_{2}|) \geq E(|B \cap I_{2}|) $ where $E$ means expected value. If this is true, how to prove it? If not, how to prove it’s not true?","I have a question attached related to both probability and cardinality. Let me know if my formulation of the problem is non-rigorous or confusing. Any proof or suggestions are appreciated.Thank you all. The question follows. Consider a set $I$ consists of $N$ incidents. $I=\{i_{1},i_{2},...,i_{k},...i_{N}\}$ Each incident has a probability to happen, i.e. incident $i_{k}$ happens with the probability $r_{k}$. Without loss of generality, we assume $r_{1}\geq r_{2}\geq ... \geq r_{k}\geq ... \geq r_{N}$ Given a constant $n<N$, we can have set $I_{1}=\{i_{1},i_{2},...,i_{n}\}$. Apparently, $|I_{1}|=n$ and $I_{1}\subset I$. Define a mapping $I\to S$ with $S=\{s_{1},s_{2},...,s_{k},...s_{N}\}$ subject to $ s_{k} = \left\{ \begin{array}{ccc} 1 &\mbox{ (Pr=$r_{k}$)} \\ 0 &\mbox{ (Pr=$1-r_{k}$)} \\ \end{array} \right. $ Pick out the incidents with correspond $s$ being 1 to form the set $I_{2}$ , i.e. $I_{2}=\{i_{m_{1}},i_{m_{2}},...,i_{m_{M}}\} \quad \mbox{and} \quad s_{m_{k}}=1 \quad k=1,2,...,M $ Apparently, $|I_{2}|=M$ and $I_{2}\subset I$. Note that there could be $I_{2}\ne I_{1}$ and $|I_{2}| \ne |I_{1}|$. The question is, If we have two set $A$ and $B$ with following assumptions: (1)$ A\subset I$ and $B\subset I$ (2)$|A|=|B|=n$ (3)$ |A \cap I_{1}| \geq |B \cap I_{1}| $ Is the following statement true? $ E(|A \cap I_{2}|) \geq E(|B \cap I_{2}|) $ where $E$ means expected value. If this is true, how to prove it? If not, how to prove it’s not true?",,['probability']
76,the continuity theorem with respect to Laplace transform,the continuity theorem with respect to Laplace transform,,"Let $(\mu_n)_{n\in\mathbb{N}}$ be a sequence of probability measures on $\mathbb{N}$, such that the Laplace transform $\phi_n(\lambda)=\int e^{-\lambda x}\mu_n(dx)$ converges pointwise to a limit $\phi(\lambda)=\int e^{-\lambda x}\mu(dx)$ for some probability measure $\mu$ and $\lambda$ in some non-empty interval $(a,b)$. Prove that $\mu_n$ converges weakly to $\mu$. Hint: Fix $\lambda_0\in(a,b)$ and rewrite $\phi_n$ as Laplace transform of the new probability measure $\eta_n(dx)=e^{-\lambda_0x}\frac{\mu_n(dx)}{\phi_n(\lambda_0)}$, with $\eta$ defined similarly. Show that $\eta_n$ is tight and converges weakly to $\eta$ and then deduce that $\mu_n$ converges weakly to $\mu$. This is the continuity theorem with respect to Laplace transform. I have found it in ""An introduction to Probability"" by Feller. But it doesn't provide proofs clearly. Does anyone see the proof of the the continuity theorem with respect to Laplace transform? Can you recommend me the source about this?","Let $(\mu_n)_{n\in\mathbb{N}}$ be a sequence of probability measures on $\mathbb{N}$, such that the Laplace transform $\phi_n(\lambda)=\int e^{-\lambda x}\mu_n(dx)$ converges pointwise to a limit $\phi(\lambda)=\int e^{-\lambda x}\mu(dx)$ for some probability measure $\mu$ and $\lambda$ in some non-empty interval $(a,b)$. Prove that $\mu_n$ converges weakly to $\mu$. Hint: Fix $\lambda_0\in(a,b)$ and rewrite $\phi_n$ as Laplace transform of the new probability measure $\eta_n(dx)=e^{-\lambda_0x}\frac{\mu_n(dx)}{\phi_n(\lambda_0)}$, with $\eta$ defined similarly. Show that $\eta_n$ is tight and converges weakly to $\eta$ and then deduce that $\mu_n$ converges weakly to $\mu$. This is the continuity theorem with respect to Laplace transform. I have found it in ""An introduction to Probability"" by Feller. But it doesn't provide proofs clearly. Does anyone see the proof of the the continuity theorem with respect to Laplace transform? Can you recommend me the source about this?",,"['real-analysis', 'probability', 'probability-theory', 'laplace-transform']"
77,6-digit password - a special decoding method,6-digit password - a special decoding method,,"Consider the situation of decoding a 6-digit password that consists of the symbols A to Z and 0 to 9, where all possible combinations are tried randomly and uniformly. Consider the following decoding method: At first a combination is chosen randomly and uniformly. At the next trial a digit from this combination is chosen uniformly at random and its entry is substituted by a uniformly randomly chosen element from $\left\{A,...,Z,0,...,9\right\}$. This procedure is repeated until the password is found. (a) What is the probability that the correct password will never be entered? (b) What is the probability that eventually the same combination will be entered two consecutive times? I already asked how to get the anwers to (a) and (b) without using the here mentioned special decoding method and I got great help, see Probability concerning a 6-digit password . Now I have to answer (a) and (b) using the decoding method and again I have enormous problems! Combinatorical thoughts are not my favourite business. Nevertheless I tried to find the probabilities in an analog way as it was shown to me in the linked thread. Additionally, I wonder if this task now maybe has something to do with Markov chains because the lecture this task is from is about Markov chains. I think there are (at least) the two following ways to understand the described decoding strategy, which sense is meant? Sense 1 We choose (randomly and uniformly) one of the $36^6$ possible combinations. If it is the right, we stop. Otherwise we then choose (randomly and uniformly) one of the 6 digits and substitute it (randomly and uniformly) by a symbol out of the alphabet. Then we choose (randomly and uniformly) one of the remaining 5 digits and subtitute it and so on until we substituted all the 6 digits. If this was the right password, we stop. If not, we again start substituting the 6 digits (the digits of the combination that we chosed at the beginning, that is, we stick to this combination). Sense 2 Same as above with the difference that after we substituted all 6 digits and saw that it is the wrong password, we choose (randomly and uniformly) another combination out of the $36^6$ possibilities (it can be the same as before) and then substitute the digits of this new combination. I think that sense 1 is meant and thus I considered the task in this sense. (a) Anyway, my result here is $0$, because as far as I see the probability not to have reached the right password after n passages is $$ \left(1-\frac{1}{36^6}\right)\cdot\left(1-\prod_{k=1}^6\frac{1}{k\cdot 36}\right)^n $$ and this tends to $0$ as $n\to\infty$. Remark : If we decode without this special method (see the linked thread) then the probability of (a) is 0, too. (b) The probability that we have eventually one pair of consecutive equal guesses is - to my results -  $$ \sum_{n=0}^{\infty}\left(1-\frac{1}{36^6}\right)\cdot\left(\prod_{k=1}^6\frac{35}{k\cdot 36}\right)\cdot\left(\prod_{k=1}^6\frac{34}{k\cdot 36}\right)^n\left(\prod_{k=1}^6\frac{1}{k\cdot 36}\right) $$ Edit I think my last result for (b) was not correct, I think instead it has to be $$ \sum_{n=0}^{\infty}\left(1-\frac{1}{36^6}\right)\cdot\left(1-\prod_{k=1}^k\frac{1}{k\cdot 36}\right)\cdot\left(1-\prod_{k=1}^{6}\frac{2}{k\cdot 36}\right)^n\cdot\left(\prod_{k=1}^{6}\frac{1}{k\cdot 36}\right). $$ If I know compute the geometrical series and use that $1-\prod_{k=1}^{6}\frac{1}{k\cdot 36}\approx 1$, then I get that (with $p:=\frac{1}{36^6}$) the probability of (b) is $$ \approx \left(\frac{1}{2}\right)^6\cdot (1-p) $$ Remark : When decoding without this method (see the linked thread) then the probability of (b) is $\frac{1}{2}\cdot (1-p)$. That is, using the method, if my result is correct, we have a much smaller probability for (b). So this decoding method is more efficient. Would be great to get a feedback from you to know if I am right. And, as mentioned, I am interested to know if the task has something to do with the context of Markov chains. Ciao & greetings Salamo","Consider the situation of decoding a 6-digit password that consists of the symbols A to Z and 0 to 9, where all possible combinations are tried randomly and uniformly. Consider the following decoding method: At first a combination is chosen randomly and uniformly. At the next trial a digit from this combination is chosen uniformly at random and its entry is substituted by a uniformly randomly chosen element from $\left\{A,...,Z,0,...,9\right\}$. This procedure is repeated until the password is found. (a) What is the probability that the correct password will never be entered? (b) What is the probability that eventually the same combination will be entered two consecutive times? I already asked how to get the anwers to (a) and (b) without using the here mentioned special decoding method and I got great help, see Probability concerning a 6-digit password . Now I have to answer (a) and (b) using the decoding method and again I have enormous problems! Combinatorical thoughts are not my favourite business. Nevertheless I tried to find the probabilities in an analog way as it was shown to me in the linked thread. Additionally, I wonder if this task now maybe has something to do with Markov chains because the lecture this task is from is about Markov chains. I think there are (at least) the two following ways to understand the described decoding strategy, which sense is meant? Sense 1 We choose (randomly and uniformly) one of the $36^6$ possible combinations. If it is the right, we stop. Otherwise we then choose (randomly and uniformly) one of the 6 digits and substitute it (randomly and uniformly) by a symbol out of the alphabet. Then we choose (randomly and uniformly) one of the remaining 5 digits and subtitute it and so on until we substituted all the 6 digits. If this was the right password, we stop. If not, we again start substituting the 6 digits (the digits of the combination that we chosed at the beginning, that is, we stick to this combination). Sense 2 Same as above with the difference that after we substituted all 6 digits and saw that it is the wrong password, we choose (randomly and uniformly) another combination out of the $36^6$ possibilities (it can be the same as before) and then substitute the digits of this new combination. I think that sense 1 is meant and thus I considered the task in this sense. (a) Anyway, my result here is $0$, because as far as I see the probability not to have reached the right password after n passages is $$ \left(1-\frac{1}{36^6}\right)\cdot\left(1-\prod_{k=1}^6\frac{1}{k\cdot 36}\right)^n $$ and this tends to $0$ as $n\to\infty$. Remark : If we decode without this special method (see the linked thread) then the probability of (a) is 0, too. (b) The probability that we have eventually one pair of consecutive equal guesses is - to my results -  $$ \sum_{n=0}^{\infty}\left(1-\frac{1}{36^6}\right)\cdot\left(\prod_{k=1}^6\frac{35}{k\cdot 36}\right)\cdot\left(\prod_{k=1}^6\frac{34}{k\cdot 36}\right)^n\left(\prod_{k=1}^6\frac{1}{k\cdot 36}\right) $$ Edit I think my last result for (b) was not correct, I think instead it has to be $$ \sum_{n=0}^{\infty}\left(1-\frac{1}{36^6}\right)\cdot\left(1-\prod_{k=1}^k\frac{1}{k\cdot 36}\right)\cdot\left(1-\prod_{k=1}^{6}\frac{2}{k\cdot 36}\right)^n\cdot\left(\prod_{k=1}^{6}\frac{1}{k\cdot 36}\right). $$ If I know compute the geometrical series and use that $1-\prod_{k=1}^{6}\frac{1}{k\cdot 36}\approx 1$, then I get that (with $p:=\frac{1}{36^6}$) the probability of (b) is $$ \approx \left(\frac{1}{2}\right)^6\cdot (1-p) $$ Remark : When decoding without this method (see the linked thread) then the probability of (b) is $\frac{1}{2}\cdot (1-p)$. That is, using the method, if my result is correct, we have a much smaller probability for (b). So this decoding method is more efficient. Would be great to get a feedback from you to know if I am right. And, as mentioned, I am interested to know if the task has something to do with the context of Markov chains. Ciao & greetings Salamo",,"['probability', 'combinatorics', 'stochastic-processes', 'proof-verification', 'markov-chains']"
78,How to model this easy problem as sum of indicator random variables in order to apply Chernoff bound,How to model this easy problem as sum of indicator random variables in order to apply Chernoff bound,,"Do you have an idea how I could model the following process somehow as a sum of independent indicator random variables? I have given a grid of size $n \times n$ for $n \rightarrow \infty$. Now I color each point of this grid uniformly at random with one out of $k$ colors, where $k=\mathcal{O}(1)$. I am interested in the probability that none of the $3 \times 3$-subgrids in this grid are monochromatic (monochromatic=all 9 points have the same color) and should show this probability is at most $e^{- \Omega(n^2)}$. My task is explicitly to model this such that I can use Chernoff bounds.  Chernoff bounds, in the context we had it, can be applied if we have a sum $X:=\sum_{i=1}^m X_i$ of independent indicator random variables $X_i \sim Be(p_i)$. My problem is that clearly the different $3 \times 3$ grids are not independent, so the easy approach to state $X=\sum_{s \in S} X_s$ where $S$ is the set of all $3\times 3$ subgrids and $X_s$ is $1$ if $s$ is a monochromatic grid, and calculate $P[X=0]$ does not work. Would be very happy about any hint.  Thank you very much!","Do you have an idea how I could model the following process somehow as a sum of independent indicator random variables? I have given a grid of size $n \times n$ for $n \rightarrow \infty$. Now I color each point of this grid uniformly at random with one out of $k$ colors, where $k=\mathcal{O}(1)$. I am interested in the probability that none of the $3 \times 3$-subgrids in this grid are monochromatic (monochromatic=all 9 points have the same color) and should show this probability is at most $e^{- \Omega(n^2)}$. My task is explicitly to model this such that I can use Chernoff bounds.  Chernoff bounds, in the context we had it, can be applied if we have a sum $X:=\sum_{i=1}^m X_i$ of independent indicator random variables $X_i \sim Be(p_i)$. My problem is that clearly the different $3 \times 3$ grids are not independent, so the easy approach to state $X=\sum_{s \in S} X_s$ where $S$ is the set of all $3\times 3$ subgrids and $X_s$ is $1$ if $s$ is a monochromatic grid, and calculate $P[X=0]$ does not work. Would be very happy about any hint.  Thank you very much!",,"['probability', 'probability-theory', 'probability-distributions', 'expectation', 'distribution-tails']"
79,Estimating parameters for a binomial,Estimating parameters for a binomial,,"First of all I'd like to precise that I'm not an expert of the subject. Suppose to have two random variables $X$ and $Y$ that are binomial, respectively $X\sim B(n_1,p)$ and $Y\sim B(n_2,p),$ note here that $p$ is the same. I know that $Z=X+Y \sim B(n_1+n_2,p).$ Let $\{x_1,\ldots,x_k\}$  be a sample for $X$ and $\{y_1,\ldots,y_k\}$ be a sample for $Y$, is there a standard method for estimating $n=n_1+n_2$ and $p$? This is what I have done: take the ""new sample"" for $Z$ given by $\{x_1+y_1,\ldots, x_k+y_k\}$, using the Likelihood Estimator, obtain an estimation for $n$ and $p$, use he Fisher information in order to obtain the error over $n$ and $p$. The method seems to work, but I have still some doubts. Let $S_k$ the group of permutation over $k$ elements. For every $\sigma\in S_k$ we can consider the ""sample"" given by $\{x_1+y_{\sigma(1)},\dots, x_k+y_{\sigma(k)}\}.$ Applying the Likelihood Estimator for each one of the ""new samples"" (there are $k!$ different sums) we obtain different estimation for $n$ and $p$. What is the meaning of this? It can be used for calculating the error for $n$?","First of all I'd like to precise that I'm not an expert of the subject. Suppose to have two random variables $X$ and $Y$ that are binomial, respectively $X\sim B(n_1,p)$ and $Y\sim B(n_2,p),$ note here that $p$ is the same. I know that $Z=X+Y \sim B(n_1+n_2,p).$ Let $\{x_1,\ldots,x_k\}$  be a sample for $X$ and $\{y_1,\ldots,y_k\}$ be a sample for $Y$, is there a standard method for estimating $n=n_1+n_2$ and $p$? This is what I have done: take the ""new sample"" for $Z$ given by $\{x_1+y_1,\ldots, x_k+y_k\}$, using the Likelihood Estimator, obtain an estimation for $n$ and $p$, use he Fisher information in order to obtain the error over $n$ and $p$. The method seems to work, but I have still some doubts. Let $S_k$ the group of permutation over $k$ elements. For every $\sigma\in S_k$ we can consider the ""sample"" given by $\{x_1+y_{\sigma(1)},\dots, x_k+y_{\sigma(k)}\}.$ Applying the Likelihood Estimator for each one of the ""new samples"" (there are $k!$ different sums) we obtain different estimation for $n$ and $p$. What is the meaning of this? It can be used for calculating the error for $n$?",,"['probability', 'statistics']"
80,Strange Consequences of Large Cardinals in Probability,Strange Consequences of Large Cardinals in Probability,,"Large cardinal axioms are very strong hypothesizes and as any other strong hypothesis they have many strange consequences in mathematics. On the other hand we know that if we bring even the least infinity ($\aleph_0$) into the story, there will be many strange theorems in probability which are challenging to our intuition. (Recall the well-known example of a monkey who is typing randomly and can type entire masterworks of human literature if he has an infinite time). I wonder what would happen if we work with large cardinalities rather than $\aleph_0$ in probability? In fact there are some connections between large cardinals and probability. Probably the most well known is the problem of extending Lebesgue measure to all sets of reals which is equivalent to the existence of a real valued measurable cardinal $\leq 2^{\aleph0}$. Note that a real valued measurable cardinal $\kappa$ by definition is an uncountable cardinal in which there is a $\kappa$-additive probability measure on the power set of $\kappa$ which vanishes on singletons. Question: What are examples of theorems in probability theory which are related to large cardinals or have a large cardinal strength? Is there any example of a philosophically strange probabilistic phenomena which happens in the real world if we assume existence of large cardinals? I mean something like the example of a monkey who can type all texts of human literature if we assume existence of $\aleph_0$ as the weakest large cardinal assumption.","Large cardinal axioms are very strong hypothesizes and as any other strong hypothesis they have many strange consequences in mathematics. On the other hand we know that if we bring even the least infinity ($\aleph_0$) into the story, there will be many strange theorems in probability which are challenging to our intuition. (Recall the well-known example of a monkey who is typing randomly and can type entire masterworks of human literature if he has an infinite time). I wonder what would happen if we work with large cardinalities rather than $\aleph_0$ in probability? In fact there are some connections between large cardinals and probability. Probably the most well known is the problem of extending Lebesgue measure to all sets of reals which is equivalent to the existence of a real valued measurable cardinal $\leq 2^{\aleph0}$. Note that a real valued measurable cardinal $\kappa$ by definition is an uncountable cardinal in which there is a $\kappa$-additive probability measure on the power set of $\kappa$ which vanishes on singletons. Question: What are examples of theorems in probability theory which are related to large cardinals or have a large cardinal strength? Is there any example of a philosophically strange probabilistic phenomena which happens in the real world if we assume existence of large cardinals? I mean something like the example of a monkey who can type all texts of human literature if we assume existence of $\aleph_0$ as the weakest large cardinal assumption.",,"['probability', 'reference-request']"
81,Distribution of functions of uniform random variables,Distribution of functions of uniform random variables,,"Given these two independent and uniform distributed random variables, $$X \sim U[-\pi,\pi]$$ and $$Y \sim U[-\pi,\pi]$$ What is the distribution of $$\sin(X)$$ and $$\sin (Y)$$ and the distribution of $$\sin(X)-\sin(Y)$$ Thanks","Given these two independent and uniform distributed random variables, $$X \sim U[-\pi,\pi]$$ and $$Y \sim U[-\pi,\pi]$$ What is the distribution of $$\sin(X)$$ and $$\sin (Y)$$ and the distribution of $$\sin(X)-\sin(Y)$$ Thanks",,"['probability', 'probability-theory', 'probability-distributions', 'conditional-probability']"
82,Partition in graph connecting itself and other half,Partition in graph connecting itself and other half,,"Let $G=(V,E)$ be a graph with $n$ vertices and minimum degree $\delta>10$. Prove that there is a partition of $V$ into two disjoint subsets $A$ and $B$ so that $|A|\leq O(\dfrac{n\ln\delta}{\delta})$, and each vertex of $B$ has $\geq 1$ neighbor in $A$ and $\geq 1$ neighbor in $B$. [Source: The probabilistic method, Alon and Spencer] I would like to set up a probabilistic argument here, but not sure how to start.","Let $G=(V,E)$ be a graph with $n$ vertices and minimum degree $\delta>10$. Prove that there is a partition of $V$ into two disjoint subsets $A$ and $B$ so that $|A|\leq O(\dfrac{n\ln\delta}{\delta})$, and each vertex of $B$ has $\geq 1$ neighbor in $A$ and $\geq 1$ neighbor in $B$. [Source: The probabilistic method, Alon and Spencer] I would like to set up a probabilistic argument here, but not sure how to start.",,"['probability', 'combinatorics', 'graph-theory']"
83,When can I leave the absolute value from Chebyshev's inequality?,When can I leave the absolute value from Chebyshev's inequality?,,"I have a positive random variable whose distribution is unknown, but its mean is $10$. I have to find an estimation of its variance, given that $Pr(X\ge9$)=0.9980 I thought of Chebyshev's inequality: $Pr(|X-10|\geq -1)\leq\frac{D^2X}{1}=0.9980$ But can I leave the absolute value?","I have a positive random variable whose distribution is unknown, but its mean is $10$. I have to find an estimation of its variance, given that $Pr(X\ge9$)=0.9980 I thought of Chebyshev's inequality: $Pr(|X-10|\geq -1)\leq\frac{D^2X}{1}=0.9980$ But can I leave the absolute value?",,"['probability', 'random-variables']"
84,Problem in conditional expectation,Problem in conditional expectation,,"I came across this problem recently. Let $X$ be a non-negative random variable on $(\Omega,\mathscr{F},\mathbf{P})$ and let $\mathscr{G} \subseteq \mathscr{F}$ be a sub-sigma-algebra. 1) Show that $X>0 \implies \mathrm{E}[X|\mathscr{G}]>0$ almost surely. 2) Show that $\{\mathrm{E}[X|\mathscr{G}]>0\}$ is the smallest $\mathscr{G}$ measurable event that contains the event $\{{X>0}\}$ almost surely. Now 1) can be proved defining $A=\{Y:\mathrm{E}[X|\mathscr{G}]\le0\}$. Then by the tower property $\mathrm{E}[X\mathbb{1}_A(Y)]=\mathrm{E}[\mathrm{E}(X|Y)\mathbb{1}_A(Y)]$. So $\mathrm{E}(\mathbb{1}_A(Y))=0$ a.s. so $\mathrm{E}(X|Y))>0$ a.s. Could anyone show how to get the second part?","I came across this problem recently. Let $X$ be a non-negative random variable on $(\Omega,\mathscr{F},\mathbf{P})$ and let $\mathscr{G} \subseteq \mathscr{F}$ be a sub-sigma-algebra. 1) Show that $X>0 \implies \mathrm{E}[X|\mathscr{G}]>0$ almost surely. 2) Show that $\{\mathrm{E}[X|\mathscr{G}]>0\}$ is the smallest $\mathscr{G}$ measurable event that contains the event $\{{X>0}\}$ almost surely. Now 1) can be proved defining $A=\{Y:\mathrm{E}[X|\mathscr{G}]\le0\}$. Then by the tower property $\mathrm{E}[X\mathbb{1}_A(Y)]=\mathrm{E}[\mathrm{E}(X|Y)\mathbb{1}_A(Y)]$. So $\mathrm{E}(\mathbb{1}_A(Y))=0$ a.s. so $\mathrm{E}(X|Y))>0$ a.s. Could anyone show how to get the second part?",,"['probability', 'conditional-expectation']"
85,Extinction probabilities of binomial tends to Poisson distribution,Extinction probabilities of binomial tends to Poisson distribution,,"I am stuck on exercise 11.2 From Grimmett's  probability on graphs. Here is a link to the pdf on his website. Consider a branching process whose family-sizes have the binomial distribution bin$(n, \frac{\lambda}{n})$. Show that the extinction probability converges to $\eta(\lambda)$ as $n \rightarrow \infty$, where $\eta(\lambda)$ is the extinction probability of a branching process with family-sizes distributed as $\text{Po}(\lambda)$. To solve this exercise, we use a theorem from his other book, which states that if you have a branching process whose family sizes are $X$ distributed then the extinction probability of this branching process is the smallest non-negative fixed point of the equation $s = G(s)$ where $G$ is the probability generating function $G(s) = \sum_{k = 0}^\infty \mathbb{P}(X = k)s^k$. My next step was computing these probability generating functions for $\text{bin}(n,\lambda)$, these are $G_n(s) = (1+\frac{\lambda( s -1)}{n})^n$. When the family sizes are Poisson distributed with parameter $\lambda$ we get $G(s) = e^{\lambda(s-1)}$. and so clearly $G_n$ converges pointwise to $G$. This however is not enough, as we want to show that the extinction probabilities also converge, so we have $s_n$ extinction probabilities of the $n$-th process, i.e. $s_n$ is the smallest non-negative solution to $s = G_n(s)$, and we want to show that this converges the smallest non-negative solution of $s = G(s)$. I hoped I could use some theorem which states that if you have pointwise convergence then the fixed points also converge, but this is false, see this counterexample . During writing this I thought of an attempt to solve this using Hurwitz theorem, namely, show that the smallest non-negative fixed point of $G(s)$ is smaller than 1, then show that the function $G'$ is complex differentiable with non-zero derrivative, use inverse function theorem, get an open neighbourhood of our fixed point, find sequence of fixed points which converge to the smallest non-negative fixed point $\eta(\lambda)$ of $G$. Here I am stuck trying to show that $s_n$ are the smallest non-negative fixed points of $G_n$. How to proceed?","I am stuck on exercise 11.2 From Grimmett's  probability on graphs. Here is a link to the pdf on his website. Consider a branching process whose family-sizes have the binomial distribution bin$(n, \frac{\lambda}{n})$. Show that the extinction probability converges to $\eta(\lambda)$ as $n \rightarrow \infty$, where $\eta(\lambda)$ is the extinction probability of a branching process with family-sizes distributed as $\text{Po}(\lambda)$. To solve this exercise, we use a theorem from his other book, which states that if you have a branching process whose family sizes are $X$ distributed then the extinction probability of this branching process is the smallest non-negative fixed point of the equation $s = G(s)$ where $G$ is the probability generating function $G(s) = \sum_{k = 0}^\infty \mathbb{P}(X = k)s^k$. My next step was computing these probability generating functions for $\text{bin}(n,\lambda)$, these are $G_n(s) = (1+\frac{\lambda( s -1)}{n})^n$. When the family sizes are Poisson distributed with parameter $\lambda$ we get $G(s) = e^{\lambda(s-1)}$. and so clearly $G_n$ converges pointwise to $G$. This however is not enough, as we want to show that the extinction probabilities also converge, so we have $s_n$ extinction probabilities of the $n$-th process, i.e. $s_n$ is the smallest non-negative solution to $s = G_n(s)$, and we want to show that this converges the smallest non-negative solution of $s = G(s)$. I hoped I could use some theorem which states that if you have pointwise convergence then the fixed points also converge, but this is false, see this counterexample . During writing this I thought of an attempt to solve this using Hurwitz theorem, namely, show that the smallest non-negative fixed point of $G(s)$ is smaller than 1, then show that the function $G'$ is complex differentiable with non-zero derrivative, use inverse function theorem, get an open neighbourhood of our fixed point, find sequence of fixed points which converge to the smallest non-negative fixed point $\eta(\lambda)$ of $G$. Here I am stuck trying to show that $s_n$ are the smallest non-negative fixed points of $G_n$. How to proceed?",,"['probability-theory', 'generating-functions', 'fixed-point-theorems']"
86,Fix point theorem for measures? metric on space of measures?,Fix point theorem for measures? metric on space of measures?,,"I have the following problem: I consider a probability space $(\Omega, \mathcal{F}, \mu)$ where $\Omega= C_0([0,1])$ (space of continuous functions on $[0,1]$ starting from 0), $\mathcal{F}$ is a $\sigma$ algebra on $\Omega$ (for instance the one generated by point-evaluations) and $\mu$ a probability measure on $(C_0([0,1]),\mathcal{F})$. Denote by $\mathcal{M}(\Omega)$ the space of probability measures on $(\Omega, \mathcal{F})$. I define a mapping from $\mathcal{M}(\Omega)$ onto itself by $$\mu \mapsto P^\mu = \int f d\mu$$ where $f$ (btw $f$ is not trivial, so that the mapping is not trivial) is fixed and it depends also on $\mu$ in fact. I can give more details if necessary. I'm very interested in finding the measure $\mu^\ast$ such that $\mu^\ast = P^{\mu^\ast}$ if it exists and the first naiv thought was a ""fix point theorem"" but then the question is... 1) What topology/metric should I use in $\mathcal{M}(\Omega)$ so that it makes sense to consider this operator? 2) Having found a metric $d$, and proving $d(P^\mu, P^{\mu'})\leq d(\mu,\mu')$, would this imply there is $\mu^\ast$ such that $\mu^\ast = P^{\mu^\ast}$? (Banach fix point theorem) In conclusion, I'd like to find a fix point of this mapping :( Is there any idea or standard trick? Thank you very much for your kind help!","I have the following problem: I consider a probability space $(\Omega, \mathcal{F}, \mu)$ where $\Omega= C_0([0,1])$ (space of continuous functions on $[0,1]$ starting from 0), $\mathcal{F}$ is a $\sigma$ algebra on $\Omega$ (for instance the one generated by point-evaluations) and $\mu$ a probability measure on $(C_0([0,1]),\mathcal{F})$. Denote by $\mathcal{M}(\Omega)$ the space of probability measures on $(\Omega, \mathcal{F})$. I define a mapping from $\mathcal{M}(\Omega)$ onto itself by $$\mu \mapsto P^\mu = \int f d\mu$$ where $f$ (btw $f$ is not trivial, so that the mapping is not trivial) is fixed and it depends also on $\mu$ in fact. I can give more details if necessary. I'm very interested in finding the measure $\mu^\ast$ such that $\mu^\ast = P^{\mu^\ast}$ if it exists and the first naiv thought was a ""fix point theorem"" but then the question is... 1) What topology/metric should I use in $\mathcal{M}(\Omega)$ so that it makes sense to consider this operator? 2) Having found a metric $d$, and proving $d(P^\mu, P^{\mu'})\leq d(\mu,\mu')$, would this imply there is $\mu^\ast$ such that $\mu^\ast = P^{\mu^\ast}$? (Banach fix point theorem) In conclusion, I'd like to find a fix point of this mapping :( Is there any idea or standard trick? Thank you very much for your kind help!",,"['real-analysis', 'probability', 'general-topology', 'functional-analysis', 'measure-theory']"
87,Probability help! Am I even doing this right?,Probability help! Am I even doing this right?,,"I am really bad with probability, so I just want some explanations and help with this problem (and probably many more to come!) and I also want to know if I am on the right track. Thank you! Lyme disease is a disease carried by ticks, which can be transmitted to humans by tick bites. Suppose the probability of contracting the disease is 1/100 for each tick bite. a. What is the probability that you will not get the disease when bitten once? I know that the probability of NOT getting the disease is 99/100, so if it is one time you get bitten, then the probability that you will not get the disease when bitten once IS 99/100, right? Since it is only one time. b. What is the probability that you will not get the disease from your first tick bite, and will the it from your second tick bite? I did (probability of NOT getting the disease) x (probability of getting the disease). So I got (99/100) x (1/100) = 99/10000 as the answer. Am I correct? I think the events are independent from each other.","I am really bad with probability, so I just want some explanations and help with this problem (and probably many more to come!) and I also want to know if I am on the right track. Thank you! Lyme disease is a disease carried by ticks, which can be transmitted to humans by tick bites. Suppose the probability of contracting the disease is 1/100 for each tick bite. a. What is the probability that you will not get the disease when bitten once? I know that the probability of NOT getting the disease is 99/100, so if it is one time you get bitten, then the probability that you will not get the disease when bitten once IS 99/100, right? Since it is only one time. b. What is the probability that you will not get the disease from your first tick bite, and will the it from your second tick bite? I did (probability of NOT getting the disease) x (probability of getting the disease). So I got (99/100) x (1/100) = 99/10000 as the answer. Am I correct? I think the events are independent from each other.",,"['probability', 'statistics']"
88,Probabilistic counting inequality,Probabilistic counting inequality,,"I am reading a proof involving the existence of a property in a tournament (a directed complete graph). To make the proof work, we need to have $n^ke^{-n/2^k}<1$. Here $n$ is the order of the tournament and $k$ is the order of some sub tournament. The proof goes on to say that if $n>k^22^{k+1}$, then $n^ke^{-n/2^k}<1$, but I am not sure how to derive this bound on $n$? I have that $$\begin{align*} n^ke^{-n/2^k}&<1\\ e^{-n/2^k}&<n^{-k}\\ -\frac{n}{2^k}&<-k\ln{n}\\ \frac{n}{\ln{n}}&>k2^k \end{align*}$$ Since $\ln{x}$ is an increasing function and $n\ge 2$ we have $$ \frac{n}{\ln{2}}\ge\frac{n}{\ln{n}}>k2^k $$ which yields $$ n>k2^k\ln{2}. $$ So I am guessing that since $2>\ln{2}$ and $k>1$, we have $n>k^22^{k+1}>k2^{k+1}>k2^k\ln{2}.$ Why not state the lower version of the bound, $k2^k\ln{2}$, instead of the seemingly contrived higher bound, $k^22^{k+1}$?","I am reading a proof involving the existence of a property in a tournament (a directed complete graph). To make the proof work, we need to have $n^ke^{-n/2^k}<1$. Here $n$ is the order of the tournament and $k$ is the order of some sub tournament. The proof goes on to say that if $n>k^22^{k+1}$, then $n^ke^{-n/2^k}<1$, but I am not sure how to derive this bound on $n$? I have that $$\begin{align*} n^ke^{-n/2^k}&<1\\ e^{-n/2^k}&<n^{-k}\\ -\frac{n}{2^k}&<-k\ln{n}\\ \frac{n}{\ln{n}}&>k2^k \end{align*}$$ Since $\ln{x}$ is an increasing function and $n\ge 2$ we have $$ \frac{n}{\ln{2}}\ge\frac{n}{\ln{n}}>k2^k $$ which yields $$ n>k2^k\ln{2}. $$ So I am guessing that since $2>\ln{2}$ and $k>1$, we have $n>k^22^{k+1}>k2^{k+1}>k2^k\ln{2}.$ Why not state the lower version of the bound, $k2^k\ln{2}$, instead of the seemingly contrived higher bound, $k^22^{k+1}$?",,"['probability', 'combinatorics', 'graph-theory']"
89,Generalization of the Glivenko-Cantelli Theorem,Generalization of the Glivenko-Cantelli Theorem,,"The classic Glivenko-Cantelli Theorem states that $$ \sup_{t}|F_{n}(t) - F(t)| \longrightarrow_{a.s.} 0 $$ where $F_{n}(t)$ is the empirical cdf. Looking at the proof of the theorem, it seems to me like it only uses the fact that $F_{n}(x) \longrightarrow_{a.s.} F(x)$ for each fixed $x$, and that  both $F_{n}(x)$ and $F(x)$ are monotonic with $0\leq F(x) \leq 1$ and $0 \leq F_{n}(x) \leq 1$. My first question is that if, in general, we have an estimated function $G_{n}(t)$ such that $G_{n}(t) \longrightarrow_{a.s.} G(t)$ for each fixed $t$ with  $G(t)$ monotonic and $|G(t)| \leq K$, is it true that $$ \sup_{t}|G_{n}(t) - G(t)| \longrightarrow_{a.s.} 0? $$ Another question I have is if the above can be generalized to some extent. That is, if, we have an estimated function $G_{n}(t)$ such that $G_{n}(t) \longrightarrow_{a.s.} G(t)$ for each fixed $t$ with  $G(t)$ continuous and $|G(t)| \leq K$ for $t \in [a,b]$, is it true that $$ \sup_{t \in [a,b]}|G_{n}(t) - G(t)| \longrightarrow_{a.s.} 0? $$ where $a < b$ are finite numbers.","The classic Glivenko-Cantelli Theorem states that $$ \sup_{t}|F_{n}(t) - F(t)| \longrightarrow_{a.s.} 0 $$ where $F_{n}(t)$ is the empirical cdf. Looking at the proof of the theorem, it seems to me like it only uses the fact that $F_{n}(x) \longrightarrow_{a.s.} F(x)$ for each fixed $x$, and that  both $F_{n}(x)$ and $F(x)$ are monotonic with $0\leq F(x) \leq 1$ and $0 \leq F_{n}(x) \leq 1$. My first question is that if, in general, we have an estimated function $G_{n}(t)$ such that $G_{n}(t) \longrightarrow_{a.s.} G(t)$ for each fixed $t$ with  $G(t)$ monotonic and $|G(t)| \leq K$, is it true that $$ \sup_{t}|G_{n}(t) - G(t)| \longrightarrow_{a.s.} 0? $$ Another question I have is if the above can be generalized to some extent. That is, if, we have an estimated function $G_{n}(t)$ such that $G_{n}(t) \longrightarrow_{a.s.} G(t)$ for each fixed $t$ with  $G(t)$ continuous and $|G(t)| \leq K$ for $t \in [a,b]$, is it true that $$ \sup_{t \in [a,b]}|G_{n}(t) - G(t)| \longrightarrow_{a.s.} 0? $$ where $a < b$ are finite numbers.",,"['probability', 'statistics', 'probability-theory']"
90,On a problem of sphere-packing for Reed-Solomon codes,On a problem of sphere-packing for Reed-Solomon codes,,"Suppose we have an $[n, k+1, n-k]$ Reed Solomon code $\mathcal C$ over $\mathbb F_q$, where $n-k=d$ is the minimum distance, and suppose that $d=2t+1$. We know that for every $r \in \mathbb F_q^n$ the ball  $$ B(r,t):=\left\{x \in \mathbb F_q^n \;|\; d(x,r) \leq t \right\}$$ centered in $r$ with radius $t$ contains at most one codeword $c\in \mathcal C$.  Obviously also the sphere $$ S(r,t):=\left\{x \in \mathbb F_q^n \;|\; d(x,r) = t \right\}$$ has the same property. ( $d(-,-)$ is the Hamming distance). Now fix an integer $e$ with $t<e<2t$ I have three questions. Q1) I would like to know (or to estimate) the probability that, given an $r \in \mathbb F_q^n$, we have $$ \left|B(r,e) \cap \mathcal C\right| \geq 2.$$ In particular I am interested in the case when $e=t+1$ Q2)  I would like to know (or to estimate) the probability that, given an $r \in \mathbb F_q^n$, we have $$ \left|S(r,e) \cap \mathcal C\right| =1.$$ In particular I' d like to compute it when $e=t+1$ Q3) I want to estimate the expected value of :   $$ \left|B(r,t+1)\cap \mathcal C\right|,\;\;\; \mbox{ and } $$   $$ \left|S(r,t+1) \cap \mathcal C\right|$$ Thanks for your help!","Suppose we have an $[n, k+1, n-k]$ Reed Solomon code $\mathcal C$ over $\mathbb F_q$, where $n-k=d$ is the minimum distance, and suppose that $d=2t+1$. We know that for every $r \in \mathbb F_q^n$ the ball  $$ B(r,t):=\left\{x \in \mathbb F_q^n \;|\; d(x,r) \leq t \right\}$$ centered in $r$ with radius $t$ contains at most one codeword $c\in \mathcal C$.  Obviously also the sphere $$ S(r,t):=\left\{x \in \mathbb F_q^n \;|\; d(x,r) = t \right\}$$ has the same property. ( $d(-,-)$ is the Hamming distance). Now fix an integer $e$ with $t<e<2t$ I have three questions. Q1) I would like to know (or to estimate) the probability that, given an $r \in \mathbb F_q^n$, we have $$ \left|B(r,e) \cap \mathcal C\right| \geq 2.$$ In particular I am interested in the case when $e=t+1$ Q2)  I would like to know (or to estimate) the probability that, given an $r \in \mathbb F_q^n$, we have $$ \left|S(r,e) \cap \mathcal C\right| =1.$$ In particular I' d like to compute it when $e=t+1$ Q3) I want to estimate the expected value of :   $$ \left|B(r,t+1)\cap \mathcal C\right|,\;\;\; \mbox{ and } $$   $$ \left|S(r,t+1) \cap \mathcal C\right|$$ Thanks for your help!",,"['probability', 'combinatorics', 'coding-theory']"
91,Expected number of subtree removal in a tree.,Expected number of subtree removal in a tree.,,"I was solving this problem. In a gist the problem is as follows: You are given a rooted tree. On each step you choose a node randomly and remove the subtree rooted by that node and the node itself, until all of them have been removed (that is root has been chosen). Find the expected number of steps in this process. In the editorial it is written that the direct removal of the node $i$ (that node $i$ has been chosen, not its ancestors) is $\frac{1}{\text{Depth[i]}}$. Intuitively I realise that if some node has more ancestors then the probability that its ancestor is chosen (hence the node $i$ got removed) is more than the node itself. Hence, more ancestors implies lesser the probability of direct removal. But how the probability is exactly equal to $\frac{1}{\text{Depth[i]}}$, that I couldn't understand. Please help.","I was solving this problem. In a gist the problem is as follows: You are given a rooted tree. On each step you choose a node randomly and remove the subtree rooted by that node and the node itself, until all of them have been removed (that is root has been chosen). Find the expected number of steps in this process. In the editorial it is written that the direct removal of the node $i$ (that node $i$ has been chosen, not its ancestors) is $\frac{1}{\text{Depth[i]}}$. Intuitively I realise that if some node has more ancestors then the probability that its ancestor is chosen (hence the node $i$ got removed) is more than the node itself. Hence, more ancestors implies lesser the probability of direct removal. But how the probability is exactly equal to $\frac{1}{\text{Depth[i]}}$, that I couldn't understand. Please help.",,"['probability', 'expectation', 'trees']"
92,Approximation for the convolution of normal and lognormal distributions,Approximation for the convolution of normal and lognormal distributions,,"$$X \sim \ln\mathcal{N}(\mu_X,\,\sigma_X)$$ $$Y \sim \mathcal{N}(0,\,1)$$ $$Z = X + Y$$ I want to find the probability density functions and cumulative distribution functions of $Z$. As the below is not integrable, I assume I can only find an approximation. $$\int_{-\infty }^{\infty } \frac{e^{-\frac{\left(\log (x)-\mu _X\right){}^2}{2 \sigma _X^2}}}{\sqrt{2 \pi } \sigma _X x} \frac{e^{-\frac{\left(z-x\right){}^2}{2}}}{\sqrt{2 \pi }} \, dx$$ What approaches are there to finding an approximation? I've tried using Mathematica to integrate the first few terms of the Taylor series about $x=0$, but it returns $0$. What else can I try?","$$X \sim \ln\mathcal{N}(\mu_X,\,\sigma_X)$$ $$Y \sim \mathcal{N}(0,\,1)$$ $$Z = X + Y$$ I want to find the probability density functions and cumulative distribution functions of $Z$. As the below is not integrable, I assume I can only find an approximation. $$\int_{-\infty }^{\infty } \frac{e^{-\frac{\left(\log (x)-\mu _X\right){}^2}{2 \sigma _X^2}}}{\sqrt{2 \pi } \sigma _X x} \frac{e^{-\frac{\left(z-x\right){}^2}{2}}}{\sqrt{2 \pi }} \, dx$$ What approaches are there to finding an approximation? I've tried using Mathematica to integrate the first few terms of the Taylor series about $x=0$, but it returns $0$. What else can I try?",,"['probability', 'probability-theory', 'probability-distributions', 'logarithms', 'normal-distribution']"
93,Probability of Sum of Independent Events Exceeding a Value,Probability of Sum of Independent Events Exceeding a Value,,"Suppose I have $n$ random number generators.  Once an hour, on the hour, each one generates a random real number $x_k$ such that $0 \le x_k \lt \infty$. Each generator produces its values according to its own independent probability distribution function $f_k()$, which is a known function.  For example, one generator might follow an exponential distribution, another might follow a normal distribution, etc. Let $X = \sum\limits_{k=1}^n x_k$ for all of the number generators in any one hour. Given $y$ such that $0 \le y \lt 1$ (a probability), I need to find a value $z$ such that $P(X \le z) = y$. Basically, I need to be able to do something like find the value that $X$ will be less than or equal to 50% of the time. I apologize if I've gotten any of the notation wrong, I'm actually a software engineer so I know some things about math but not others.  I know enough about probability to express the problem above, but I don't even know where to begin in terms of solving it. Any help, or even suggested readings would be much appreciated.","Suppose I have $n$ random number generators.  Once an hour, on the hour, each one generates a random real number $x_k$ such that $0 \le x_k \lt \infty$. Each generator produces its values according to its own independent probability distribution function $f_k()$, which is a known function.  For example, one generator might follow an exponential distribution, another might follow a normal distribution, etc. Let $X = \sum\limits_{k=1}^n x_k$ for all of the number generators in any one hour. Given $y$ such that $0 \le y \lt 1$ (a probability), I need to find a value $z$ such that $P(X \le z) = y$. Basically, I need to be able to do something like find the value that $X$ will be less than or equal to 50% of the time. I apologize if I've gotten any of the notation wrong, I'm actually a software engineer so I know some things about math but not others.  I know enough about probability to express the problem above, but I don't even know where to begin in terms of solving it. Any help, or even suggested readings would be much appreciated.",,"['probability', 'soft-question']"
94,Derive a procedure to select one of the 2 options with equal probability when we are not using a fair coin.,Derive a procedure to select one of the 2 options with equal probability when we are not using a fair coin.,,"Derive a procedure to select one of the 2 options with equal probability when we are not using a fair coin. $P(\text{H}) = p$. $P(\text{T}) = 1 - p = q$. I came up with the following two-roll outcomes: $P(\text{HH}) = p^2$. $P(\text{HT}) = pq$. Let the event $\text{HT}$ be $A$. $P(\text{TH}) = qp$. Let the event $\text{TH}$ be $B$. $P(\text{TT}) = q^2$. If $\text{HH}$ or $\text{TT}$ is hit we repeat the toss until $A$ or $B$. So \begin{align} P(A) & = pq + (p^2 + q^2)pq + (p^2 + q^2)^2pq + \ldots \\ & = pq(1 + (p^2 + q^2) + (p^2 + q^2)^2 + \ldots) \\ & = pq\left(\frac{1}{1 - (p^2 + q^2)}\right) \\ & = \frac{p(1 - p)}{2pq} = \frac{p(1 - p)}{2p(1 - p)} = \frac 12. \end{align} Now we want to solve for a different number other than 1/2, i.e., 1/3 or 3/4 where some solution is done in a fixed amount of N flips. How do we find a formula for this?","Derive a procedure to select one of the 2 options with equal probability when we are not using a fair coin. $P(\text{H}) = p$. $P(\text{T}) = 1 - p = q$. I came up with the following two-roll outcomes: $P(\text{HH}) = p^2$. $P(\text{HT}) = pq$. Let the event $\text{HT}$ be $A$. $P(\text{TH}) = qp$. Let the event $\text{TH}$ be $B$. $P(\text{TT}) = q^2$. If $\text{HH}$ or $\text{TT}$ is hit we repeat the toss until $A$ or $B$. So \begin{align} P(A) & = pq + (p^2 + q^2)pq + (p^2 + q^2)^2pq + \ldots \\ & = pq(1 + (p^2 + q^2) + (p^2 + q^2)^2 + \ldots) \\ & = pq\left(\frac{1}{1 - (p^2 + q^2)}\right) \\ & = \frac{p(1 - p)}{2pq} = \frac{p(1 - p)}{2p(1 - p)} = \frac 12. \end{align} Now we want to solve for a different number other than 1/2, i.e., 1/3 or 3/4 where some solution is done in a fixed amount of N flips. How do we find a formula for this?",,"['probability', 'combinatorics', 'discrete-mathematics']"
95,Random $0-1$ matrices,Random  matrices,0-1,"I'm working my way through the Oxford notes in Probabilistic Combinatorics and came across this question in one of the question sheets; I'd like to stress that this is not my homework: I'm simply working through the notes for my own pleasure. For $n, r ∈ \mathbb{N}$, $1 < r < n$, let $z(r, n)$ be the largest possible number of $0$ entries in an $n × n$ matrix which has no $r × r$ submatrix whose entries are all $0$. (Here a submatrix is obtained by selecting any $r$ rows and any $r$ columns; the rows/columns need not be consecutive.) Consider a random matrix in which each entry is $0$ with probability $p$ and $1$ with probability $1−p$, independently. Deduce that $z(r, n) > pn^{2}-p^{r^{2}}n^{2r}.$ My solution: Let $[n]_{2}$ denote the set of all $n \times n$ matrices, with only $0-1$ entries. For each $\sigma \in [n]_{2}$ let the random variable $X(\sigma)$ denote the number of $0$ entries in $\sigma$ and the random variable $Y(\sigma)$ denote the number of $r \times r$ submatrices filled with entirely with $0$'s. It follows that $\mathbb{E}(X)=pn^{{2}}$ and $\mathbb{E}(Y)=\binom{n}{r}^{2}p^{r^{2}}<n^{2r}p^{r^{2}}$. Further more consider the random variable $X-Y$ for which $\mathbb{E}(X-Y)>pn^{2}-n^{2r}p^{r^{2}}$. It follows that there exists a $\sigma \in [n]_{2}$ such that $X(\sigma)-Y(\sigma)>pn^{2}-n^{2r}p^{r^{2}}$. Given such a $\sigma$ construct $\sigma' \in [n]_{2}$ as follows. For each of the all $0$, $r \times r$ sub matrices of $\sigma$ remove at random a single $0$ from each to ensure $\sigma'$ has no $r \times r$ such sub-matrices. We have removed at most $\binom{n}{r}^{2}<n^{2r}$ such zero's and so we can conclude that $\sigma'$ has at least $pn^{2}-p^{r^{2}}n^{2r}$ zeros thus we can conclude that $z(n,r)>pn^{2}-p^{r^{2}}n^{2r}$. I'd appreciate any comments on the validity of the proof.","I'm working my way through the Oxford notes in Probabilistic Combinatorics and came across this question in one of the question sheets; I'd like to stress that this is not my homework: I'm simply working through the notes for my own pleasure. For $n, r ∈ \mathbb{N}$, $1 < r < n$, let $z(r, n)$ be the largest possible number of $0$ entries in an $n × n$ matrix which has no $r × r$ submatrix whose entries are all $0$. (Here a submatrix is obtained by selecting any $r$ rows and any $r$ columns; the rows/columns need not be consecutive.) Consider a random matrix in which each entry is $0$ with probability $p$ and $1$ with probability $1−p$, independently. Deduce that $z(r, n) > pn^{2}-p^{r^{2}}n^{2r}.$ My solution: Let $[n]_{2}$ denote the set of all $n \times n$ matrices, with only $0-1$ entries. For each $\sigma \in [n]_{2}$ let the random variable $X(\sigma)$ denote the number of $0$ entries in $\sigma$ and the random variable $Y(\sigma)$ denote the number of $r \times r$ submatrices filled with entirely with $0$'s. It follows that $\mathbb{E}(X)=pn^{{2}}$ and $\mathbb{E}(Y)=\binom{n}{r}^{2}p^{r^{2}}<n^{2r}p^{r^{2}}$. Further more consider the random variable $X-Y$ for which $\mathbb{E}(X-Y)>pn^{2}-n^{2r}p^{r^{2}}$. It follows that there exists a $\sigma \in [n]_{2}$ such that $X(\sigma)-Y(\sigma)>pn^{2}-n^{2r}p^{r^{2}}$. Given such a $\sigma$ construct $\sigma' \in [n]_{2}$ as follows. For each of the all $0$, $r \times r$ sub matrices of $\sigma$ remove at random a single $0$ from each to ensure $\sigma'$ has no $r \times r$ such sub-matrices. We have removed at most $\binom{n}{r}^{2}<n^{2r}$ such zero's and so we can conclude that $\sigma'$ has at least $pn^{2}-p^{r^{2}}n^{2r}$ zeros thus we can conclude that $z(n,r)>pn^{2}-p^{r^{2}}n^{2r}$. I'd appreciate any comments on the validity of the proof.",,"['probability', 'combinatorics', 'probability-theory']"
96,Asymptotic Bounds for the distribution of $f_n(X_n)$.,Asymptotic Bounds for the distribution of .,f_n(X_n),"Let $\{X_n\}_{n \in \mathbb{N}}$ be a sequence of $\mathbb{R}^{k}$-valued random variables defined on some probability space $(\Omega, \mathcal{F}, \mathbb{P})$ converging almost surely to $X$. Suppose there is a sequence of functions $\{f_n: \mathbb{R}^{k} \rightarrow \mathbb{R}\}_{n \in \mathbb{N}}$ such that for every converging sequence $\{h_n\}$ in $\mathbb{R}^{k}$, with limit $h$, we have: $$-\infty<g_1(h) \leq \liminf_{n \rightarrow \infty} f_n(h_n) < \limsup_{n \rightarrow \infty} f_n(h_n) \leq g_2(h)<\infty$$ for some continuous functions $g_1,g_2: \mathbb{R}^{k} \rightarrow \mathbb{R}$. Question: Is there anything clever that we can say about: $$\liminf_{n \rightarrow \infty} \mathbb{P} \{ \omega \in \Omega \: |\: f_n(X_n(\omega)) \leq c)\} \text{ and } \limsup_{n \rightarrow \infty} \mathbb{P} \{ \omega \in \Omega \: |\: f_n(X_n(\omega)) \leq c)\}, c \in \mathbb{R}$$ What I have in mind is that for a fixed $\omega$, there is $N({\omega},\epsilon)$ such that for $n>N(\omega,\epsilon)$: $$ f_n(X_n(\omega)) \leq c \implies \liminf_{n \rightarrow \infty} f_n(X_n(\omega)) < c + \epsilon \implies g_1(X(\omega)) \leq c+\epsilon.  $$ If $N(\omega,\epsilon)$ were only a function of $\epsilon$, we could say that for all $\epsilon>0$: $$ \limsup_{n \rightarrow \infty} \mathbb{P} \{ \omega \in \Omega \: |\: f_n(X_n(\omega)) \leq c)\}\leq \mathbb{P}\{\omega \in \Omega \: | \: g_1(X(\omega))\leq c+\epsilon\}. $$ However, I don't think one can ignore the dependence on $\omega$. If that is the case, I am not sure about how to bound the asymptotic distribution of $f_n(X_n)$. Let me know if this makes sense. Thanks!","Let $\{X_n\}_{n \in \mathbb{N}}$ be a sequence of $\mathbb{R}^{k}$-valued random variables defined on some probability space $(\Omega, \mathcal{F}, \mathbb{P})$ converging almost surely to $X$. Suppose there is a sequence of functions $\{f_n: \mathbb{R}^{k} \rightarrow \mathbb{R}\}_{n \in \mathbb{N}}$ such that for every converging sequence $\{h_n\}$ in $\mathbb{R}^{k}$, with limit $h$, we have: $$-\infty<g_1(h) \leq \liminf_{n \rightarrow \infty} f_n(h_n) < \limsup_{n \rightarrow \infty} f_n(h_n) \leq g_2(h)<\infty$$ for some continuous functions $g_1,g_2: \mathbb{R}^{k} \rightarrow \mathbb{R}$. Question: Is there anything clever that we can say about: $$\liminf_{n \rightarrow \infty} \mathbb{P} \{ \omega \in \Omega \: |\: f_n(X_n(\omega)) \leq c)\} \text{ and } \limsup_{n \rightarrow \infty} \mathbb{P} \{ \omega \in \Omega \: |\: f_n(X_n(\omega)) \leq c)\}, c \in \mathbb{R}$$ What I have in mind is that for a fixed $\omega$, there is $N({\omega},\epsilon)$ such that for $n>N(\omega,\epsilon)$: $$ f_n(X_n(\omega)) \leq c \implies \liminf_{n \rightarrow \infty} f_n(X_n(\omega)) < c + \epsilon \implies g_1(X(\omega)) \leq c+\epsilon.  $$ If $N(\omega,\epsilon)$ were only a function of $\epsilon$, we could say that for all $\epsilon>0$: $$ \limsup_{n \rightarrow \infty} \mathbb{P} \{ \omega \in \Omega \: |\: f_n(X_n(\omega)) \leq c)\}\leq \mathbb{P}\{\omega \in \Omega \: | \: g_1(X(\omega))\leq c+\epsilon\}. $$ However, I don't think one can ignore the dependence on $\omega$. If that is the case, I am not sure about how to bound the asymptotic distribution of $f_n(X_n)$. Let me know if this makes sense. Thanks!",,"['probability', 'statistics', 'measure-theory', 'probability-theory', 'convergence-divergence']"
97,Approximate the probability,Approximate the probability,,"An immortal snail is at one end of a perfect rubber band with a length of 1km. Every day, it travels 10cm in a random direction, forwards or backwards on the rubber band. Every night, the rubber band gets stretched uniformly by 1km. As an example, during the first day the snail might advance to x=10cm, then the rubber band gets stretched by a factor of 2, so the snail is now at x=20cm on a rubber band of 2km. The question: Approximate the probability that it will reach the other side at some point (better approximations are obviously preferred, but any bounds are acceptable as long as they are found by doing something interesting)","An immortal snail is at one end of a perfect rubber band with a length of 1km. Every day, it travels 10cm in a random direction, forwards or backwards on the rubber band. Every night, the rubber band gets stretched uniformly by 1km. As an example, during the first day the snail might advance to x=10cm, then the rubber band gets stretched by a factor of 2, so the snail is now at x=20cm on a rubber band of 2km. The question: Approximate the probability that it will reach the other side at some point (better approximations are obviously preferred, but any bounds are acceptable as long as they are found by doing something interesting)",,['probability']
98,Definition of conditional expectation,Definition of conditional expectation,,"We have the following definition for a conditional expectation: Let $X: \Omega \rightarrow \mathbb{R}$ be a randomn variable on ($\Omega, \Sigma, \mathbb{P})$. Let $F \subset \Sigma$ be a sub $\sigma-$ algebra, then $Z$ is called a conditional expectation of $X $ under $F$, if $Z$ is $F$ measurable and $\mathbb{E}(X \chi_C) = \mathbb{E}(Z \chi_C)$ for all $C \in F$. What I don't understand here is: What is wrong with $Z=X$? In this case $X$ is $F$ measurable as $X$ is $\Sigma$-measurable and we certainly have $\mathbb{E}(X \chi_C) = \mathbb{E}(X \chi_C)$, so somehow I don't trust this definition. Did I copy anything wrong from the blackboard?","We have the following definition for a conditional expectation: Let $X: \Omega \rightarrow \mathbb{R}$ be a randomn variable on ($\Omega, \Sigma, \mathbb{P})$. Let $F \subset \Sigma$ be a sub $\sigma-$ algebra, then $Z$ is called a conditional expectation of $X $ under $F$, if $Z$ is $F$ measurable and $\mathbb{E}(X \chi_C) = \mathbb{E}(Z \chi_C)$ for all $C \in F$. What I don't understand here is: What is wrong with $Z=X$? In this case $X$ is $F$ measurable as $X$ is $\Sigma$-measurable and we certainly have $\mathbb{E}(X \chi_C) = \mathbb{E}(X \chi_C)$, so somehow I don't trust this definition. Did I copy anything wrong from the blackboard?",,['probability']
99,Sum of poisson random variables,Sum of poisson random variables,,"Let $N, X_1, \dots , X_n$ be independent random variables. $N \sim P(\lambda) \quad (\text{Poisson distribution})$, while $X_k \sim B(p)$ (Bernoulli) Let us consider the ""random"" sum $S = X_1 + \dots + X_N$, which is random also because of the index $N$ (e.g. if $N=0$ then $ S = 0$, if $N = 3$ then $ S = X_1 + X_2 + X_3$, and so forth) Question 1 (theoretical question) How do I prove that $S$ and $N - S$ are random variables? Question 2 (more pratical) I've determined that $S \sim P(\lambda p)$. How do I calculate the distribution of $N - S$? They are both Poisson, but they are not independent (it suffice to notice that $N=0 \implies S = 0$) What I've tried is:  $$P(N-S = k) = \sum_{j=0}^\infty P(S=j)\cdot P(N=j+k)$$ But I got stuck at $$P(N-S = k) = e^{-\lambda (p+1)} \lambda^k \sum_{j=0}^\infty \frac{\left(\lambda^2p\right)^j}{j!(\ j+k)!}$$ I don't know how to simplify that. And I'm not even that sure that the way I'm doing is right. Any help is greatly appreciated! :)","Let $N, X_1, \dots , X_n$ be independent random variables. $N \sim P(\lambda) \quad (\text{Poisson distribution})$, while $X_k \sim B(p)$ (Bernoulli) Let us consider the ""random"" sum $S = X_1 + \dots + X_N$, which is random also because of the index $N$ (e.g. if $N=0$ then $ S = 0$, if $N = 3$ then $ S = X_1 + X_2 + X_3$, and so forth) Question 1 (theoretical question) How do I prove that $S$ and $N - S$ are random variables? Question 2 (more pratical) I've determined that $S \sim P(\lambda p)$. How do I calculate the distribution of $N - S$? They are both Poisson, but they are not independent (it suffice to notice that $N=0 \implies S = 0$) What I've tried is:  $$P(N-S = k) = \sum_{j=0}^\infty P(S=j)\cdot P(N=j+k)$$ But I got stuck at $$P(N-S = k) = e^{-\lambda (p+1)} \lambda^k \sum_{j=0}^\infty \frac{\left(\lambda^2p\right)^j}{j!(\ j+k)!}$$ I don't know how to simplify that. And I'm not even that sure that the way I'm doing is right. Any help is greatly appreciated! :)",,"['probability', 'sequences-and-series', 'probability-distributions', 'random-variables']"
