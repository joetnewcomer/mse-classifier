,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding upper bound for $C^1$ function,Finding upper bound for  function,C^1,"Suppose $\phi: \mathbb R^m\to \mathbb R^m$ is $C^1$ and that $s \in \mathbb R^n$. Let $J\phi(x)$ denote the Jacobian of $\phi$ at $x$, which we assume to be invertible for any $x\in \mathbb R^m$. Now, for distinct $a,b\in \mathbb R^m$ set $F(a,b)= \frac{ \mathbf{\phi}({a})-\mathbf{\phi}({b})-J\mathbf{\phi}({b})({a}-{b})}{\|{a}-{b}\|}$. Consider the coordinate maps $F=(F_1, \ldots, F_m)$. I want to show that $|F_i(a,b)|\leq \frac{K}{2m}$ for each $i=1,  \ldots, m$ on some neighborhood of $s$. I think that $|F_i(a,b)|=\frac{\Big |\phi_i(a)-\phi_i(b)-\Big(\sum_{i=1}^m (a_i-b_i)\frac{\partial \phi_1}{\partial x_i}\Big )\Big|}{\|a-b\|}$, where $\phi$ is written in coordinates as $(\phi_1, \ldots, \phi_n)$ and $\frac{\partial \phi_1}{\partial x_i}$ denotes the partial of $\phi_1$ with respect to the $i$-th variable $x_i$. I'd appreciate if someone could explain the proof. UPDATE: Is there something confusing or that doesn't make sense about my question? I'm a bit surprised that it has received no answers, since there very well may be a simple answer I am missing.","Suppose $\phi: \mathbb R^m\to \mathbb R^m$ is $C^1$ and that $s \in \mathbb R^n$. Let $J\phi(x)$ denote the Jacobian of $\phi$ at $x$, which we assume to be invertible for any $x\in \mathbb R^m$. Now, for distinct $a,b\in \mathbb R^m$ set $F(a,b)= \frac{ \mathbf{\phi}({a})-\mathbf{\phi}({b})-J\mathbf{\phi}({b})({a}-{b})}{\|{a}-{b}\|}$. Consider the coordinate maps $F=(F_1, \ldots, F_m)$. I want to show that $|F_i(a,b)|\leq \frac{K}{2m}$ for each $i=1,  \ldots, m$ on some neighborhood of $s$. I think that $|F_i(a,b)|=\frac{\Big |\phi_i(a)-\phi_i(b)-\Big(\sum_{i=1}^m (a_i-b_i)\frac{\partial \phi_1}{\partial x_i}\Big )\Big|}{\|a-b\|}$, where $\phi$ is written in coordinates as $(\phi_1, \ldots, \phi_n)$ and $\frac{\partial \phi_1}{\partial x_i}$ denotes the partial of $\phi_1$ with respect to the $i$-th variable $x_i$. I'd appreciate if someone could explain the proof. UPDATE: Is there something confusing or that doesn't make sense about my question? I'm a bit surprised that it has received no answers, since there very well may be a simple answer I am missing.",,"['real-analysis', 'multivariable-calculus']"
1,Local minimum and maximum of two variable implicit function,Local minimum and maximum of two variable implicit function,,"What is local minimum and maximum of implicit function $F(x,y)=x^4+y^4−x^2 −y^2=0$? I calculated first and second derivative: $$\frac{\partial F(x, y(x))}{\partial x}=4x^3+4y^3y'-2x-2yy'$$ $$\frac{\partial F^2(x, y(x))}{\partial x^2}=12x^2+12y^2(y')^2+12y^3y''-2(y')^2-2yy''-2$$ $$y'=\frac{2x-4x^3}{4y^3-2y}$$ $$y''=\frac{(y')^2-6x^2-6y^2(y')^2+1}{6y^3-y}$$ Then I don't know how to continue. How to find stationary points? How to decide if stationary point is local minimum or maximum?","What is local minimum and maximum of implicit function $F(x,y)=x^4+y^4−x^2 −y^2=0$? I calculated first and second derivative: $$\frac{\partial F(x, y(x))}{\partial x}=4x^3+4y^3y'-2x-2yy'$$ $$\frac{\partial F^2(x, y(x))}{\partial x^2}=12x^2+12y^2(y')^2+12y^3y''-2(y')^2-2yy''-2$$ $$y'=\frac{2x-4x^3}{4y^3-2y}$$ $$y''=\frac{(y')^2-6x^2-6y^2(y')^2+1}{6y^3-y}$$ Then I don't know how to continue. How to find stationary points? How to decide if stationary point is local minimum or maximum?",,"['calculus', 'multivariable-calculus']"
2,Is there an equivalent to the Intermediate Value Theorem for $\mathbb{R}^2$ scalar fields?,Is there an equivalent to the Intermediate Value Theorem for  scalar fields?,\mathbb{R}^2,"I am to prove some sort of mean value theorem for double integrals. That is, if $f: R \subset \mathbb{R}^2 \rightarrow \mathbb{R}$ is continuous on some rectangle $R$, then there exists $c \in R$ such that $\iint_R f\, dA = f(c) \mu(R)$, where $\mu(R)$ is the area of the rectangle $R$. My only idea so far is to prove this theorem the same way I proved it for real valued functions of real variables: using the Intermediate Value Theorem thanks to the hypothesis that $f$ is continuous. However I am unsure if such theorem exists for several variables. Is there an equivalent to it?","I am to prove some sort of mean value theorem for double integrals. That is, if $f: R \subset \mathbb{R}^2 \rightarrow \mathbb{R}$ is continuous on some rectangle $R$, then there exists $c \in R$ such that $\iint_R f\, dA = f(c) \mu(R)$, where $\mu(R)$ is the area of the rectangle $R$. My only idea so far is to prove this theorem the same way I proved it for real valued functions of real variables: using the Intermediate Value Theorem thanks to the hypothesis that $f$ is continuous. However I am unsure if such theorem exists for several variables. Is there an equivalent to it?",,"['calculus', 'multivariable-calculus']"
3,Second order sufficiency test for multivariable function,Second order sufficiency test for multivariable function,,"Question : Suppose $f$ is a $\mathcal{C}^2$ function and $x^*$ is a point of its domain at which $\nabla f\left( {{x^*}} \right)d \geq 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ for every non-zero feasible direction. Is $x^*$ necessarily a local minimum? Basically the question is asking whether the above two conditions are sufficient. My work so far: If $\exists $ a feasible direction $d\in \mathbb{R}^n$ at $x^*$ such that $ \nabla f(x^*)d<0 $ then $\exists \epsilon >0 $ such that for $|\alpha| < \epsilon$ we have $ x(\alpha) = x^* + \alpha d \in D $. Then, $$ f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( \alpha  \right)$$ $$ f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \alpha \nabla f\left( {{x^*}} \right)d + o\left( \alpha  \right) $$ where for small $\alpha$ $$ \alpha \nabla f\left( {{x^*}} \right)d + o\left( \alpha  \right) < 0$$ since $\alpha \geq 0 $ and $\nabla f\left( {{x^*}} \right)d < 0$. This implies $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) < 0$ contradicting that $x^*$ is a minimum.  Now I claim if $\nabla f\left( {{x^*}} \right)d = 0$ then,  $${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$$ To see why note,  $$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + \frac{1}{2}{\left( {x\left( \alpha  \right) - {x^*}} \right)^T}{\nabla ^2}f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( {{\alpha ^2}} \right) $$ which simplifies to $$f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) = \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right)$$ Note that $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) < 0$ for small $\alpha$ if ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d < 0 $ which would contradict that $x^*$ is a minimum. Hence we must have $${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$$ Claim :If $\nabla f\left( {{x^*}} \right)d > 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ then $x^*$ is a necessarily local minimum. To see why, note that: $$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + \frac{1}{2}{\left( {x\left( \alpha  \right) - {x^*}} \right)^T}{\nabla ^2}f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( {{\alpha ^2}} \right)$$ simplifies to,  $$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \alpha \nabla f\left( {{x^*}} \right)d + \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right)$$ $$ \Rightarrow f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) = \alpha \nabla f\left( {{x^*}} \right)d + \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right) $$ We have $\alpha>0$ by definition of feasible direction. Moreovoer  $\nabla f\left( {{x^*}} \right)d > 0$ and  ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ and since these two terms dominate, we always have $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) > 0$. Since $d$ is an arbitrary feasible direction, we see that moving away from $x^*$ only increases the value of $f$ which implies that $x^*$ is indeed the minimum. We can relax the condition $\nabla f\left( {{x^*}} \right)d > 0$ to $\nabla f\left( {{x^*}} \right)d \geq 0$ and the above will still hold. To see why ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ guarantees sufficiency note that if we only require ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$ the $o\left(\alpha^2\right) $ term could make the LHS $<0$ when both $\nabla f\left( {{x^*}} \right)d = 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d = 0$. However for the case when only $\nabla f\left( {{x^*}} \right)d \geq 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ the whole term still remains positive. Hence the conditions are sufficient. Apparently this proof is wrong and the two conditions in the question are not sufficient conditions. It has something to do with non-convex domains where a ""nonlinear motion"" may take you to a minimum. But I am having trouble wrapping my head around this. Does anyone know of a counter-example or a proof that the above are not sufficient conditions?","Question : Suppose $f$ is a $\mathcal{C}^2$ function and $x^*$ is a point of its domain at which $\nabla f\left( {{x^*}} \right)d \geq 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ for every non-zero feasible direction. Is $x^*$ necessarily a local minimum? Basically the question is asking whether the above two conditions are sufficient. My work so far: If $\exists $ a feasible direction $d\in \mathbb{R}^n$ at $x^*$ such that $ \nabla f(x^*)d<0 $ then $\exists \epsilon >0 $ such that for $|\alpha| < \epsilon$ we have $ x(\alpha) = x^* + \alpha d \in D $. Then, $$ f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( \alpha  \right)$$ $$ f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \alpha \nabla f\left( {{x^*}} \right)d + o\left( \alpha  \right) $$ where for small $\alpha$ $$ \alpha \nabla f\left( {{x^*}} \right)d + o\left( \alpha  \right) < 0$$ since $\alpha \geq 0 $ and $\nabla f\left( {{x^*}} \right)d < 0$. This implies $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) < 0$ contradicting that $x^*$ is a minimum.  Now I claim if $\nabla f\left( {{x^*}} \right)d = 0$ then,  $${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$$ To see why note,  $$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + \frac{1}{2}{\left( {x\left( \alpha  \right) - {x^*}} \right)^T}{\nabla ^2}f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( {{\alpha ^2}} \right) $$ which simplifies to $$f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) = \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right)$$ Note that $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) < 0$ for small $\alpha$ if ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d < 0 $ which would contradict that $x^*$ is a minimum. Hence we must have $${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$$ Claim :If $\nabla f\left( {{x^*}} \right)d > 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ then $x^*$ is a necessarily local minimum. To see why, note that: $$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + \frac{1}{2}{\left( {x\left( \alpha  \right) - {x^*}} \right)^T}{\nabla ^2}f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( {{\alpha ^2}} \right)$$ simplifies to,  $$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \alpha \nabla f\left( {{x^*}} \right)d + \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right)$$ $$ \Rightarrow f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) = \alpha \nabla f\left( {{x^*}} \right)d + \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right) $$ We have $\alpha>0$ by definition of feasible direction. Moreovoer  $\nabla f\left( {{x^*}} \right)d > 0$ and  ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ and since these two terms dominate, we always have $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) > 0$. Since $d$ is an arbitrary feasible direction, we see that moving away from $x^*$ only increases the value of $f$ which implies that $x^*$ is indeed the minimum. We can relax the condition $\nabla f\left( {{x^*}} \right)d > 0$ to $\nabla f\left( {{x^*}} \right)d \geq 0$ and the above will still hold. To see why ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ guarantees sufficiency note that if we only require ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$ the $o\left(\alpha^2\right) $ term could make the LHS $<0$ when both $\nabla f\left( {{x^*}} \right)d = 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d = 0$. However for the case when only $\nabla f\left( {{x^*}} \right)d \geq 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ the whole term still remains positive. Hence the conditions are sufficient. Apparently this proof is wrong and the two conditions in the question are not sufficient conditions. It has something to do with non-convex domains where a ""nonlinear motion"" may take you to a minimum. But I am having trouble wrapping my head around this. Does anyone know of a counter-example or a proof that the above are not sufficient conditions?",,"['multivariable-calculus', 'optimization']"
4,"Function totally differentiable in $(0,0)$",Function totally differentiable in,"(0,0)","I asked this question here but didn't have an account yet and I can't comment yet on another question. So please forgive me for asking again. Consider the following function: $$f:\mathbb R^2\rightarrow\mathbb R,(x,y)\mapsto\begin{cases} x,&y=0 \\ y,&x=0\\0,&\text{else}\end{cases}$$ So far I have, that this function is continuous in $(0,0)$, as for every sequence $(x_n,y_n)\rightarrow (0,0)$ we have $$f(x_n,y_n)=\begin{cases} x_n,& y_n=0 \\ y_n,&x_n=0 \\ 0,&\text{else}\end{cases} \rightarrow 0.$$ Further I have calculated all directional derivatives: If $(v_1,v_2)\in\mathbb R^2\setminus\{0\}$ with $v_1\cdot v_2\neq 0$, we have $\frac{1}{t}f(tv_1,tv_2)=0.$ For $v_1=0,v_2\neq 0$ we have $\frac{1}{t}f(tv_1,tv_2)=\frac{1}{t}\cdot tv_2=v_2$ and for $v_1\neq 0, v_2=0$ we get $\frac{1}{t}f(tv_1,tv_2)=v_1$. Thus we get for the partial derivatives: $$\frac{\partial f}{\partial x}(0,0)=1,\quad \frac{\partial f}{\partial y}(0,0)=1.$$ I now want to know, if this function is totally differentiable in $(0,0)$. The partial derivatives are not continuous in $(0,0)$, so I can't use that to say that the function is totally differentiable. But as $f$ is continuous in $(0,0)$ I can't rule out that the function is not totally differentiable. I know that by definition $f$ is totally differentiable in $(0,0)$ if there exists a Matrix $T$ and a function $\varphi$ with $$f(x,y)=f(0,0)+T\cdot \left((x,y)-(0,0)\right)+\Vert (x,y)-(0,0)\Vert\cdot\varphi(x,y)$$ but I don't know how to calculate $T$ and $\varphi$. Regarding the comment if I'm sure that the partial derivatives are not continuous: For $(x,y)\neq (0,0)$ I get $\frac{\partial f}{\partial x}(x,y)=\begin{cases} 1, & y=0\\0,&\text{else}\end{cases}$ which seems to me not continuous in $(0,0)$. Or am I wrong here? Regarding the answer by H. H. Rugh: what notation do you mean? And how do you get to that equation?","I asked this question here but didn't have an account yet and I can't comment yet on another question. So please forgive me for asking again. Consider the following function: $$f:\mathbb R^2\rightarrow\mathbb R,(x,y)\mapsto\begin{cases} x,&y=0 \\ y,&x=0\\0,&\text{else}\end{cases}$$ So far I have, that this function is continuous in $(0,0)$, as for every sequence $(x_n,y_n)\rightarrow (0,0)$ we have $$f(x_n,y_n)=\begin{cases} x_n,& y_n=0 \\ y_n,&x_n=0 \\ 0,&\text{else}\end{cases} \rightarrow 0.$$ Further I have calculated all directional derivatives: If $(v_1,v_2)\in\mathbb R^2\setminus\{0\}$ with $v_1\cdot v_2\neq 0$, we have $\frac{1}{t}f(tv_1,tv_2)=0.$ For $v_1=0,v_2\neq 0$ we have $\frac{1}{t}f(tv_1,tv_2)=\frac{1}{t}\cdot tv_2=v_2$ and for $v_1\neq 0, v_2=0$ we get $\frac{1}{t}f(tv_1,tv_2)=v_1$. Thus we get for the partial derivatives: $$\frac{\partial f}{\partial x}(0,0)=1,\quad \frac{\partial f}{\partial y}(0,0)=1.$$ I now want to know, if this function is totally differentiable in $(0,0)$. The partial derivatives are not continuous in $(0,0)$, so I can't use that to say that the function is totally differentiable. But as $f$ is continuous in $(0,0)$ I can't rule out that the function is not totally differentiable. I know that by definition $f$ is totally differentiable in $(0,0)$ if there exists a Matrix $T$ and a function $\varphi$ with $$f(x,y)=f(0,0)+T\cdot \left((x,y)-(0,0)\right)+\Vert (x,y)-(0,0)\Vert\cdot\varphi(x,y)$$ but I don't know how to calculate $T$ and $\varphi$. Regarding the comment if I'm sure that the partial derivatives are not continuous: For $(x,y)\neq (0,0)$ I get $\frac{\partial f}{\partial x}(x,y)=\begin{cases} 1, & y=0\\0,&\text{else}\end{cases}$ which seems to me not continuous in $(0,0)$. Or am I wrong here? Regarding the answer by H. H. Rugh: what notation do you mean? And how do you get to that equation?",,['multivariable-calculus']
5,Prove the following identity for the Apéry constant,Prove the following identity for the Apéry constant,,"Perhaps this kind of integral is well knonw, or can be easily deduced from other. I don't know it but I would like to see the computation of this to refresh the computation of iterated integrals. I was inspired to change the factors of those integrals in the Wikipedia Page for Apéry 's constant . Firht I tried modify those integrals writting some factors $\sqrt{x}$. After with factors $1-x^2$ and computing with the online calculator of Wolfram Alpha one find with the code $\displaystyle{\int_0^1\int_0^1\int_0^1\int_0^1 (1-y^2)(1-t^2)/(1-xyz) \,dx\,dy\,dz\,dt}$ an integral involving the cited constant. I am assuming that these families were well known , because was easy to find those identities by similarity, when one modify such integrands as I've said. See also this MathWorld Page for the Apéry's constant , truly those formulas and identites are more complicated than my question, thus the kind of integral that Question. Show us how to prove    $$\int_0^1\int_0^1\int_0^1\int_0^1\frac{(1-y^2)(1-t^2)}{1-xyz}dxdydzdt=\frac{1}{36}(24\zeta(3)+9-2\pi^2).$$ Thanks in advance. Since the idea was easy, see comments for other example, it is easy that it was in the literature. Also you are welcome to answer the following optional question, Question (Optional) Can you find a integral now involving at least a factor $\sqrt{\text{variable}}$ of previous variables $x,y,z,\ldots$, to get an identity, as the previous, for $\zeta(3)$?","Perhaps this kind of integral is well knonw, or can be easily deduced from other. I don't know it but I would like to see the computation of this to refresh the computation of iterated integrals. I was inspired to change the factors of those integrals in the Wikipedia Page for Apéry 's constant . Firht I tried modify those integrals writting some factors $\sqrt{x}$. After with factors $1-x^2$ and computing with the online calculator of Wolfram Alpha one find with the code $\displaystyle{\int_0^1\int_0^1\int_0^1\int_0^1 (1-y^2)(1-t^2)/(1-xyz) \,dx\,dy\,dz\,dt}$ an integral involving the cited constant. I am assuming that these families were well known , because was easy to find those identities by similarity, when one modify such integrands as I've said. See also this MathWorld Page for the Apéry's constant , truly those formulas and identites are more complicated than my question, thus the kind of integral that Question. Show us how to prove    $$\int_0^1\int_0^1\int_0^1\int_0^1\frac{(1-y^2)(1-t^2)}{1-xyz}dxdydzdt=\frac{1}{36}(24\zeta(3)+9-2\pi^2).$$ Thanks in advance. Since the idea was easy, see comments for other example, it is easy that it was in the literature. Also you are welcome to answer the following optional question, Question (Optional) Can you find a integral now involving at least a factor $\sqrt{\text{variable}}$ of previous variables $x,y,z,\ldots$, to get an identity, as the previous, for $\zeta(3)$?",,['integration']
6,A Lagrange multiplier,A Lagrange multiplier,,"Given the function $$f(x,y,z)=xyz$$ subject to the constraint $$g(x,y,z)=\frac1x+\frac1y+\frac1z-1=0,$$ the method Lagrange multipliers shows that the point $(3,3,3)$ is an extremum of $f(x,y)$ satisfying $g(x,y,z)=0$. This point gives $f(3,3,3)=27$. But notice also that $f(4,4,2)=32$ and $f(1,-1,1)=-1$, both points satisfying the equation $g(4,4,2)=g(1,-1,1)=0$. But does this not tell me that $(3,3,3)$ is not an extremum? What is wrong with the above?","Given the function $$f(x,y,z)=xyz$$ subject to the constraint $$g(x,y,z)=\frac1x+\frac1y+\frac1z-1=0,$$ the method Lagrange multipliers shows that the point $(3,3,3)$ is an extremum of $f(x,y)$ satisfying $g(x,y,z)=0$. This point gives $f(3,3,3)=27$. But notice also that $f(4,4,2)=32$ and $f(1,-1,1)=-1$, both points satisfying the equation $g(4,4,2)=g(1,-1,1)=0$. But does this not tell me that $(3,3,3)$ is not an extremum? What is wrong with the above?",,"['multivariable-calculus', 'lagrange-multiplier']"
7,One-variable continuity of one partial derivative implies differentiability?,One-variable continuity of one partial derivative implies differentiability?,,"I am interested in a particular instacne of the phenomena ""Partial derivatives + (A certain degree of) continuity"" implies differentiablilty. My case assumes less regularity than usual: Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}, (x_0,y_0) \in \mathbb{R}^2$. Assume that the partial derivatives of $f$ with respect to $x$ and $y$ exist at the point $(x_0,y_0)$ and one of them exists and continuous w.r.t to the other variable (for instance the function $y \mapsto\frac{\partial{f}}{\partial x}(x_0,y)$ is continuous at the point $y_0$). Is it true that $f$ is differentiable at $(x_0,y_0)$? Note: It is known that if both partial derivatives exists, and one of them is continuous (as a functions of two variables) then $f$ is differentiable. (For a proof see here ). However, the proof uses: 1) The existence of $\frac{\partial{f}}{\partial x}$ on some ball around $(x_0,y_0)$ (I assume only $\frac{\partial{f}}{\partial x}$ exists on $\{x_0\} \times (y_0-\epsilon,y_0+\epsilon)$). 2) The continuity of $(x,y) \mapsto \frac{\partial{f}}{\partial x}(x,y)$ at $(0,0)$. (I assume only continuity of $\frac{\partial{f}}{\partial x}(x_0,y)$).","I am interested in a particular instacne of the phenomena ""Partial derivatives + (A certain degree of) continuity"" implies differentiablilty. My case assumes less regularity than usual: Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}, (x_0,y_0) \in \mathbb{R}^2$. Assume that the partial derivatives of $f$ with respect to $x$ and $y$ exist at the point $(x_0,y_0)$ and one of them exists and continuous w.r.t to the other variable (for instance the function $y \mapsto\frac{\partial{f}}{\partial x}(x_0,y)$ is continuous at the point $y_0$). Is it true that $f$ is differentiable at $(x_0,y_0)$? Note: It is known that if both partial derivatives exists, and one of them is continuous (as a functions of two variables) then $f$ is differentiable. (For a proof see here ). However, the proof uses: 1) The existence of $\frac{\partial{f}}{\partial x}$ on some ball around $(x_0,y_0)$ (I assume only $\frac{\partial{f}}{\partial x}$ exists on $\{x_0\} \times (y_0-\epsilon,y_0+\epsilon)$). 2) The continuity of $(x,y) \mapsto \frac{\partial{f}}{\partial x}(x,y)$ at $(0,0)$. (I assume only continuity of $\frac{\partial{f}}{\partial x}(x_0,y)$).",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
8,$\frac {\partial}{\partial t}T$ vs $\frac d{dt} T$.,vs .,\frac {\partial}{\partial t}T \frac d{dt} T,"Suppose we have a function $T_1=F(x,y,t)$. Now suppose that $x=g(t),y=h(t)$, so we have a new $T_2=F(x(t),y(t),t)$, so then we have that $\frac \partial{\partial t} T_2=F_t$ and $\frac d{dt}T_2=F_x x_t+F_yy_t+F_t$. My question is, if we let $x,y$ and $t$ be independant variables, does the symbol $\frac d{dt}T_1$ (if we use the same idea as before, we should have that $\frac d{dt}T_1=\frac {\partial}{\partial t}T_1$, is this true?) have any meaning?","Suppose we have a function $T_1=F(x,y,t)$. Now suppose that $x=g(t),y=h(t)$, so we have a new $T_2=F(x(t),y(t),t)$, so then we have that $\frac \partial{\partial t} T_2=F_t$ and $\frac d{dt}T_2=F_x x_t+F_yy_t+F_t$. My question is, if we let $x,y$ and $t$ be independant variables, does the symbol $\frac d{dt}T_1$ (if we use the same idea as before, we should have that $\frac d{dt}T_1=\frac {\partial}{\partial t}T_1$, is this true?) have any meaning?",,"['multivariable-calculus', 'derivatives', 'notation', 'partial-derivative']"
9,Directional derivative understanding,Directional derivative understanding,,"[Beginning multivariable question.] I have just been introduced to a theorem that says $$D_uf(x)=\nabla f(x)\cdot u.$$ So in the two-dimensional case, $$\nabla f(x,y)= \langle f_x(x,y),f_y(x,y)\rangle \cdot \langle a,b\rangle$$ I don't really understand this. It seems to me that $f_x$ could be 0 and $f_y$ could be 0, there could still be a nonzero derivative in the direction halfway between the $x$ and $y$ axes. My intuition is that such a situation would violate the theorem -- so I must be misunderstanding something. Where am I going wrong?","[Beginning multivariable question.] I have just been introduced to a theorem that says $$D_uf(x)=\nabla f(x)\cdot u.$$ So in the two-dimensional case, $$\nabla f(x,y)= \langle f_x(x,y),f_y(x,y)\rangle \cdot \langle a,b\rangle$$ I don't really understand this. It seems to me that $f_x$ could be 0 and $f_y$ could be 0, there could still be a nonzero derivative in the direction halfway between the $x$ and $y$ axes. My intuition is that such a situation would violate the theorem -- so I must be misunderstanding something. Where am I going wrong?",,"['multivariable-calculus', 'derivatives']"
10,Polar coordinates integration,Polar coordinates integration,,"Compute the following integrals  over $R$ $f(x,y)\,dx\,dy$ over the area $R$ where: $f(x, y) = x$  and $R$ is given by $0 ≤ r ≤ \cos θ$ and $f(x, y) = x$. I understand polar coordinates is probably the most suitable. We can convert $f$ into  $r^2\cos(\theta) \,dr\,d\theta$. The bounds for the $r$ variable is $0$ to $\cos(\theta)$. I'm not too sure how to get the bounds for the theta variable.  My first guess is that it's from $0$ to $2\pi$ (simply because whenever I did integration with polar coordinates, it was always integrating a whole circle, so from $0$ to $2\pi$. I'm not too sure what to do in this case). But in the solutions, they are integrating theta from $\frac{-\pi}{2}$ to $\frac{\pi}{2}$. I'm having a hard time understanding why.","Compute the following integrals  over $R$ $f(x,y)\,dx\,dy$ over the area $R$ where: $f(x, y) = x$  and $R$ is given by $0 ≤ r ≤ \cos θ$ and $f(x, y) = x$. I understand polar coordinates is probably the most suitable. We can convert $f$ into  $r^2\cos(\theta) \,dr\,d\theta$. The bounds for the $r$ variable is $0$ to $\cos(\theta)$. I'm not too sure how to get the bounds for the theta variable.  My first guess is that it's from $0$ to $2\pi$ (simply because whenever I did integration with polar coordinates, it was always integrating a whole circle, so from $0$ to $2\pi$. I'm not too sure what to do in this case). But in the solutions, they are integrating theta from $\frac{-\pi}{2}$ to $\frac{\pi}{2}$. I'm having a hard time understanding why.",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
11,Monotonic Function Optimization on Convex Constraint Region,Monotonic Function Optimization on Convex Constraint Region,,"So I have the following function, which I want to maximize: $$f(x_1,...,x_n) = \sum_{i=1}^n\alpha_i\sqrt{x_i}$$ (where all $\alpha_i$ are positive), subjected to the following equality and inequality constraints: $$\sum_{i=1}^nx_i = B$$ $$l_i \leq x_i \leq u_i$$ for $l_i,u_i,B$ all positive. I want to say that there's a nice geometric way to think about this.  In particular, invoking the coordinate transformation $$x_i\mapsto {v_i\over \alpha_i^2} $$, we have $$f(v_1,...,v_n) = \sum_{i=1}^n\sqrt{v_i}$$ and constraints $$\sum_{i=1}^n{v_i\over \alpha_i^2}$$ $$l_i \leq {v_i \over \alpha_i^2} \leq u_i$$ In a global sense, given a fixed sum, $S$, of all the $v_i$ we know from the triangle inequality that the optimal solution is $v_i = {S\over n}$ for all $i$.  Is there a way to apply this knowledge of the global solution to finding one within our constraint region (which is the intersection of a hyperplane and a polytope).  If there's no easy way to divine a closed form solution, might there be an algorithm suited to this kind of optimization? Gradient ascent is applicable, but it seems to be too ignorant of what I want to call the nice-ness of the objective function and constraint region. I don't have much background by way of numerical analysis, so I'd really appreciate any insight. Edit: I should probably point out that I will eventually be attempting to code this algorithm, so solutions of exponential or worse order are scary to me. Edit2: I realize this question isn't the most interesting numerical exercise in the world, so I've attached a modest bounty.","So I have the following function, which I want to maximize: $$f(x_1,...,x_n) = \sum_{i=1}^n\alpha_i\sqrt{x_i}$$ (where all $\alpha_i$ are positive), subjected to the following equality and inequality constraints: $$\sum_{i=1}^nx_i = B$$ $$l_i \leq x_i \leq u_i$$ for $l_i,u_i,B$ all positive. I want to say that there's a nice geometric way to think about this.  In particular, invoking the coordinate transformation $$x_i\mapsto {v_i\over \alpha_i^2} $$, we have $$f(v_1,...,v_n) = \sum_{i=1}^n\sqrt{v_i}$$ and constraints $$\sum_{i=1}^n{v_i\over \alpha_i^2}$$ $$l_i \leq {v_i \over \alpha_i^2} \leq u_i$$ In a global sense, given a fixed sum, $S$, of all the $v_i$ we know from the triangle inequality that the optimal solution is $v_i = {S\over n}$ for all $i$.  Is there a way to apply this knowledge of the global solution to finding one within our constraint region (which is the intersection of a hyperplane and a polytope).  If there's no easy way to divine a closed form solution, might there be an algorithm suited to this kind of optimization? Gradient ascent is applicable, but it seems to be too ignorant of what I want to call the nice-ness of the objective function and constraint region. I don't have much background by way of numerical analysis, so I'd really appreciate any insight. Edit: I should probably point out that I will eventually be attempting to code this algorithm, so solutions of exponential or worse order are scary to me. Edit2: I realize this question isn't the most interesting numerical exercise in the world, so I've attached a modest bounty.",,"['multivariable-calculus', 'optimization', 'algorithms', 'nonlinear-optimization', 'numerical-optimization']"
12,Computing the integral of a differential form in $\mathbb{R}^{2}$.,Computing the integral of a differential form in .,\mathbb{R}^{2},"Let $D$ be the disk \begin{equation} D=\{(x,y)\in\mathbb{R}^{2}\:|\:x^{2}+y^{2}\leq 1\}, \end{equation} which is easily verified to be a compact $2$-differentiable manifold with boundary. Let $d\omega$ be the $2$-differential form on $D$ given by \begin{equation} d\omega=(1-x^{2})\:dx\wedge dy. \end{equation} I want to compute the following integral \begin{equation} \int_{D}d\omega \end{equation} How can I do that? I don't really understand how to parametrize $D$ in order to keep the standard orientation of $\mathbb{R}^{2}$, which is given by the canonical basis ordered as $[e_{1},\:e_{2}]$. Can someone help me? Thanks.","Let $D$ be the disk \begin{equation} D=\{(x,y)\in\mathbb{R}^{2}\:|\:x^{2}+y^{2}\leq 1\}, \end{equation} which is easily verified to be a compact $2$-differentiable manifold with boundary. Let $d\omega$ be the $2$-differential form on $D$ given by \begin{equation} d\omega=(1-x^{2})\:dx\wedge dy. \end{equation} I want to compute the following integral \begin{equation} \int_{D}d\omega \end{equation} How can I do that? I don't really understand how to parametrize $D$ in order to keep the standard orientation of $\mathbb{R}^{2}$, which is given by the canonical basis ordered as $[e_{1},\:e_{2}]$. Can someone help me? Thanks.",,"['integration', 'multivariable-calculus', 'differential-geometry', 'differential-forms']"
13,Geometric Median Problem with a twist,Geometric Median Problem with a twist,,"Given two vectors $x, y$ in $\mathbb{R}^n$ and scalar $\alpha$, what is the value of $\alpha$ that minimizes $||\alpha x - y ||_1$? Give an algorithm to find the minimum. I've tried couple of examples by hand but I am not getting anywhere. Either it's the value of $|\alpha x_i - y_i|$ that dominates the function or the median. Is there a systematic way to get the solution?","Given two vectors $x, y$ in $\mathbb{R}^n$ and scalar $\alpha$, what is the value of $\alpha$ that minimizes $||\alpha x - y ||_1$? Give an algorithm to find the minimum. I've tried couple of examples by hand but I am not getting anywhere. Either it's the value of $|\alpha x_i - y_i|$ that dominates the function or the median. Is there a systematic way to get the solution?",,"['multivariable-calculus', 'optimization', 'algorithms', 'convex-optimization', 'median']"
14,Why is the Lagrange Multipliers Theorem not working?,Why is the Lagrange Multipliers Theorem not working?,,"Consider the function $h: K \to \mathbb{R}$ where $K := \{x \in \mathbb{R}^3:x,y,z \geq 0, x+2y+3z\leq 6\}$. $h$ is defined as: $$ h(x) = xe^{(x+2y+3z)} $$ Find the supremum and the infimum of $h$. Well, it seems to be a fairly easy task, right? By The Weierstrass Extreme Value Theorem there must be both maximum and minimum. Let's find out if we can find any extremum inside $K$. Looking for extrema inside $K$: $$\nabla h(x)=\left((x+1)e^{(x+2y+3z)},2xe^{(x+2y+3z)},3xe^{(x+2y+3z)}\right)$$ As we can see, the gradient of the function $h$ will never be $0$, otherwise $x$ would have to be $-1$ and $0$ at the same time. Plot twist: Actually, there is an extremum inside $K$ - the function $h$ takes its lowest value at points $(0,y,z)$ where $0 \leq2y+3z\leq 6$. Let's examine the borders of $K$, shall we? The border set $M$ is defined as $M:=\{x\in \mathbb{R}:F(x)=c\}$ where $F(x)=x+2y+3z$ and $c=6$. Gradient of $F$ is: $$ \nabla F(x)=(1,2,3) $$ So, by The Lagrange Multipliers Theorem we have the following system of equations: $$ \begin{cases} (x+1)e^{(x+2y+3z)} = \lambda\\  2xe^{(x+2y+3z)} = 2\lambda\\ 3xe^{(x+2y+3z)} = 3\lambda\\ x+2y+3z=6 \end{cases} $$ And again there are no solutions. But hey! Wait! Plot twist once again: $h$ has a maximum at point $x=(6,0,0)$. I'm starting to realise that I should have be more careful with defining the set $M$ for the Langrange Multipliers. The function $F$ itself is not enough since I'm lacking the information that $x,y,z\geq 0$. Is it the only thing I am doing wrong? Could you please show me a hint or two to make me understand my mistakes?","Consider the function $h: K \to \mathbb{R}$ where $K := \{x \in \mathbb{R}^3:x,y,z \geq 0, x+2y+3z\leq 6\}$. $h$ is defined as: $$ h(x) = xe^{(x+2y+3z)} $$ Find the supremum and the infimum of $h$. Well, it seems to be a fairly easy task, right? By The Weierstrass Extreme Value Theorem there must be both maximum and minimum. Let's find out if we can find any extremum inside $K$. Looking for extrema inside $K$: $$\nabla h(x)=\left((x+1)e^{(x+2y+3z)},2xe^{(x+2y+3z)},3xe^{(x+2y+3z)}\right)$$ As we can see, the gradient of the function $h$ will never be $0$, otherwise $x$ would have to be $-1$ and $0$ at the same time. Plot twist: Actually, there is an extremum inside $K$ - the function $h$ takes its lowest value at points $(0,y,z)$ where $0 \leq2y+3z\leq 6$. Let's examine the borders of $K$, shall we? The border set $M$ is defined as $M:=\{x\in \mathbb{R}:F(x)=c\}$ where $F(x)=x+2y+3z$ and $c=6$. Gradient of $F$ is: $$ \nabla F(x)=(1,2,3) $$ So, by The Lagrange Multipliers Theorem we have the following system of equations: $$ \begin{cases} (x+1)e^{(x+2y+3z)} = \lambda\\  2xe^{(x+2y+3z)} = 2\lambda\\ 3xe^{(x+2y+3z)} = 3\lambda\\ x+2y+3z=6 \end{cases} $$ And again there are no solutions. But hey! Wait! Plot twist once again: $h$ has a maximum at point $x=(6,0,0)$. I'm starting to realise that I should have be more careful with defining the set $M$ for the Langrange Multipliers. The function $F$ itself is not enough since I'm lacking the information that $x,y,z\geq 0$. Is it the only thing I am doing wrong? Could you please show me a hint or two to make me understand my mistakes?",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
15,Evaluate $\iint_{R}(x^2+y^2)dxdy$,Evaluate,\iint_{R}(x^2+y^2)dxdy,"$$\iint_{R}(x^2+y^2)dxdy$$ $$0\leq r\leq 2 \,\, ,\frac{\pi}{4}\leq \theta\leq\frac{3\pi}{4}$$ My attempt : Jacobian=r $$=\iint_{R}(x^2+y^2)dxdy$$ $$x:=r\cos \theta \,\,\,,y:=r\cos \theta$$ $$\sqrt{x^2+y^2}=r$$ $$\int_{\theta=\pi/4}^{\theta=3\pi/4}\bigg[\int_{r=0}^{r=2}\bigg(r^2\bigg)dr\bigg]d\theta$$ $$.....=\boxed{\frac{4\pi}{3}}$$ Is it correct? >","$$\iint_{R}(x^2+y^2)dxdy$$ $$0\leq r\leq 2 \,\, ,\frac{\pi}{4}\leq \theta\leq\frac{3\pi}{4}$$ My attempt : Jacobian=r $$=\iint_{R}(x^2+y^2)dxdy$$ $$x:=r\cos \theta \,\,\,,y:=r\cos \theta$$ $$\sqrt{x^2+y^2}=r$$ $$\int_{\theta=\pi/4}^{\theta=3\pi/4}\bigg[\int_{r=0}^{r=2}\bigg(r^2\bigg)dr\bigg]d\theta$$ $$.....=\boxed{\frac{4\pi}{3}}$$ Is it correct? >",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'proof-verification']"
16,What can we conclude about the function $f$?,What can we conclude about the function ?,f,"Let $f$ be a scalar field, $f:\mathbb R^n\to\mathbb R$ . Suppose there is an $n$ -ball $B(a;r)$ centered at $a$ with radius $r$ and a fixed vector $y\in\mathbb R^n$ such that $f'(x;y)=0$ for every $x\in B(a;r)$ . What can you conclude about $f$ ? This problem is from Apostol Calculus Volume 2. I have just begun the subject. However, I did try to work out as follows: It is evident, from Mean Value Theorem, that for any $x\in B(a;r)$ , we have $f(x+y)-f(x)=f'(x+\theta y;y)=0$ , supposing that $x+y\in B(a;r)$ and hence $x+\theta y\in B(a;r)$ if $0<\theta<1$ . (Here's my first question: can we assume that just because $x\in B(a;r)$ that $x+y\in B(a;r)$ also?) Anyway this gives that $f(x+y)=f(x)$ for each $x\in B(a;r)$ . I am not sure what I am supposed to conclude from here. I tried to interpret it by drawing an open ball around $a$ and taking a few points $x_1,x_2,x_3$ inside the ball, I tried to plot $x_i+y$ (in parallel straight lines), $i=1,2,3$ but I couldn't understand if this is anything significant. Any help will be appreciated.","Let be a scalar field, . Suppose there is an -ball centered at with radius and a fixed vector such that for every . What can you conclude about ? This problem is from Apostol Calculus Volume 2. I have just begun the subject. However, I did try to work out as follows: It is evident, from Mean Value Theorem, that for any , we have , supposing that and hence if . (Here's my first question: can we assume that just because that also?) Anyway this gives that for each . I am not sure what I am supposed to conclude from here. I tried to interpret it by drawing an open ball around and taking a few points inside the ball, I tried to plot (in parallel straight lines), but I couldn't understand if this is anything significant. Any help will be appreciated.","f f:\mathbb R^n\to\mathbb R n B(a;r) a r y\in\mathbb R^n f'(x;y)=0 x\in B(a;r) f x\in B(a;r) f(x+y)-f(x)=f'(x+\theta y;y)=0 x+y\in B(a;r) x+\theta y\in B(a;r) 0<\theta<1 x\in B(a;r) x+y\in B(a;r) f(x+y)=f(x) x\in B(a;r) a x_1,x_2,x_3 x_i+y i=1,2,3","['multivariable-calculus', 'self-learning']"
17,Integral of a function with an exponentiated inner product,Integral of a function with an exponentiated inner product,,"Let $f:\Bbb R^n\to \Bbb R^n$ be a continuous function such that $\int_{\Bbb R^n}|f(x)|dx\lt\infty$. Let $A$ be a real $n\times n$ invertible matrix and for $x,y\in\Bbb R^n$, let $\langle x,y\rangle$ denote the standard inner product in $\Bbb R^n$. Then $$\int_{\Bbb R^n}f(Ax)e^{i\langle y,x\rangle}\,dx\overset{?}{=}\  \begin{array}{l} (1)\ \int_{\Bbb R^n} f(x) e^{i\langle(A^{-1})^T y,x\rangle} \frac{dx}{|\det A|} \\ (2)\  \int_{\Bbb R^n} f(x) e^{i\langle A^T y,x\rangle} \frac{dx}{|\det A|}\\ (3)\ \int_{\Bbb R^n}f(x)e^{i\langle(A^T)^{-1} y,x\rangle}\,dx \\ (4)\ \int_{\Bbb R^n}f(x)e^{i\langle A^{-1} y,x\rangle}\frac {dx}{|\det A|} \end{array}$$ Attempt: Let $Ax=X\Rightarrow x=A^{-1}X$, then $dx=\frac {dX}{|\det A|}$. Using all these, we have: $\int_{\Bbb R^n}f(Ax)e^{i\langle y,x\rangle}dx=\int_{\Bbb R^n}f(X)e^{i\langle y,A^{-1}X\rangle}\frac{dX}{|\det A|}=\int_{\Bbb R^n}f(X)e^{i\langle (A^{-1})^Ty,X\rangle}\frac{dX}{|\det A|}$. Now if we again put $x=X\Rightarrow dx=dX$, then we get the last expression as, $\int_{\Bbb R^n}f(x)e^{i\langle(A^{-1})^T y,x\rangle}\frac {dx}{|\det A|}$. That means 1) is true.","Let $f:\Bbb R^n\to \Bbb R^n$ be a continuous function such that $\int_{\Bbb R^n}|f(x)|dx\lt\infty$. Let $A$ be a real $n\times n$ invertible matrix and for $x,y\in\Bbb R^n$, let $\langle x,y\rangle$ denote the standard inner product in $\Bbb R^n$. Then $$\int_{\Bbb R^n}f(Ax)e^{i\langle y,x\rangle}\,dx\overset{?}{=}\  \begin{array}{l} (1)\ \int_{\Bbb R^n} f(x) e^{i\langle(A^{-1})^T y,x\rangle} \frac{dx}{|\det A|} \\ (2)\  \int_{\Bbb R^n} f(x) e^{i\langle A^T y,x\rangle} \frac{dx}{|\det A|}\\ (3)\ \int_{\Bbb R^n}f(x)e^{i\langle(A^T)^{-1} y,x\rangle}\,dx \\ (4)\ \int_{\Bbb R^n}f(x)e^{i\langle A^{-1} y,x\rangle}\frac {dx}{|\det A|} \end{array}$$ Attempt: Let $Ax=X\Rightarrow x=A^{-1}X$, then $dx=\frac {dX}{|\det A|}$. Using all these, we have: $\int_{\Bbb R^n}f(Ax)e^{i\langle y,x\rangle}dx=\int_{\Bbb R^n}f(X)e^{i\langle y,A^{-1}X\rangle}\frac{dX}{|\det A|}=\int_{\Bbb R^n}f(X)e^{i\langle (A^{-1})^Ty,X\rangle}\frac{dX}{|\det A|}$. Now if we again put $x=X\Rightarrow dx=dX$, then we get the last expression as, $\int_{\Bbb R^n}f(x)e^{i\langle(A^{-1})^T y,x\rangle}\frac {dx}{|\det A|}$. That means 1) is true.",,"['real-analysis', 'multivariable-calculus', 'inner-products', 'matrix-calculus']"
18,Multivariable calculus chain rule for weak derivatives,Multivariable calculus chain rule for weak derivatives,,"Let $g:(0,1) \rightarrow \mathbb R^n$ be absolutely continuous, $F \in W^{1,2} (\mathbb R^n).$ Is it true that a.e. it holds $$ \dfrac{dF(g(t))}{dt} = \nabla F(g(t)) \cdot g'(t) \quad ? $$ What I want to prove is that $$ \int_0^1 F(g(t)) \phi'(t) \ dt = - \int_0^1 \nabla F(g(t)) \cdot g'(t) \phi(t) \ dt $$ for every $\phi \in C_c^{\infty}((0,1))$. My idea would be to use some sort of change of variable formula to obtain the thesis as in \begin{align} \int_0^1 F(g(t)) \phi'(t) \ dt \stackrel{*}{=} &~ \int_{\mathbb R^n} F(x) \phi'(g^{-1}(x)) (g^{-1})'(x) \cdot dx \\ = &~ \int_{\mathbb R^n} F(x) \nabla(\phi(g^{-1}(x))) \cdot dx \\ = &~ - \int_{\mathbb R^n} \nabla F(x)  \phi(g^{-1}(x)) \cdot dx \\ = &~ - \int_{\mathbb R^n} \nabla F(g(t)) \cdot g'(t)  \phi(t) \ dt, \end{align} but equality in $*$ would require $g$ to be a $C^1$-diffeomorphism or similar hypothesis. How could I prove the result for an absolutely continuous $g$?","Let $g:(0,1) \rightarrow \mathbb R^n$ be absolutely continuous, $F \in W^{1,2} (\mathbb R^n).$ Is it true that a.e. it holds $$ \dfrac{dF(g(t))}{dt} = \nabla F(g(t)) \cdot g'(t) \quad ? $$ What I want to prove is that $$ \int_0^1 F(g(t)) \phi'(t) \ dt = - \int_0^1 \nabla F(g(t)) \cdot g'(t) \phi(t) \ dt $$ for every $\phi \in C_c^{\infty}((0,1))$. My idea would be to use some sort of change of variable formula to obtain the thesis as in \begin{align} \int_0^1 F(g(t)) \phi'(t) \ dt \stackrel{*}{=} &~ \int_{\mathbb R^n} F(x) \phi'(g^{-1}(x)) (g^{-1})'(x) \cdot dx \\ = &~ \int_{\mathbb R^n} F(x) \nabla(\phi(g^{-1}(x))) \cdot dx \\ = &~ - \int_{\mathbb R^n} \nabla F(x)  \phi(g^{-1}(x)) \cdot dx \\ = &~ - \int_{\mathbb R^n} \nabla F(g(t)) \cdot g'(t)  \phi(t) \ dt, \end{align} but equality in $*$ would require $g$ to be a $C^1$-diffeomorphism or similar hypothesis. How could I prove the result for an absolutely continuous $g$?",,"['real-analysis', 'multivariable-calculus', 'reference-request', 'sobolev-spaces']"
19,Differentiation under integral sign (arctan-function),Differentiation under integral sign (arctan-function),,I have the integral $$ F(s) = \int_{0}^{\infty} \frac{\arctan(sx)}{x(1+x^2)} dx$$ and am supposed to solve it by finding $F'(s)$. So we get  $$ F'(s) = \int_{0}^{\infty} \frac{\partial F}{\partial s} \frac{\arctan(sx)}{x(1+x^2)} dx =...=  \int_{0}^{\infty} \frac{1}{1+s^2x^2+x^2+s^2x^4} dx $$ and I don't see how I can solve this integral. Should I try another approach?,I have the integral $$ F(s) = \int_{0}^{\infty} \frac{\arctan(sx)}{x(1+x^2)} dx$$ and am supposed to solve it by finding $F'(s)$. So we get  $$ F'(s) = \int_{0}^{\infty} \frac{\partial F}{\partial s} \frac{\arctan(sx)}{x(1+x^2)} dx =...=  \int_{0}^{\infty} \frac{1}{1+s^2x^2+x^2+s^2x^4} dx $$ and I don't see how I can solve this integral. Should I try another approach?,,['multivariable-calculus']
20,Simple limit in multi variable,Simple limit in multi variable,,"For $x=(x_1,x_2,x_3)$, determine the limit $$\lim_{x\to 0} \frac{\sin|x|^2}{|x|^2+x_1x_2x_3}. $$ I want to use that $\lim_{x\to 0} \frac{sin|x|^2}{|x|^2} = 1$ but I can't see how to do that. Any hints?","For $x=(x_1,x_2,x_3)$, determine the limit $$\lim_{x\to 0} \frac{\sin|x|^2}{|x|^2+x_1x_2x_3}. $$ I want to use that $\lim_{x\to 0} \frac{sin|x|^2}{|x|^2} = 1$ but I can't see how to do that. Any hints?",,['multivariable-calculus']
21,Volume between sphere and cylinder with different centers,Volume between sphere and cylinder with different centers,,I am working on a tumor model and need to calculate the volume enclosed between the sphere given by $$(x-d)^2+y^2+z^2=r^2$$ and the cylinder given by $$x^2+y^2=R^2.$$ I have worked it out by using surfaces of revolution but this is tedious and required numerous cases. When I try to use cylindrical coordinates I end up the integral $$\int^{2\pi}_0 \int^R_0 \int^{\sqrt{r^2-R^2+d^2-2dR\cos (\theta )}}_{-\sqrt{r^2-R^2+d^2-2dR\cos (\theta )}} R~dz~dR~d\theta$$ which won't compute. I suspect either I am making an error in my transformation to cylindrical coordinates or a different method is needed.,I am working on a tumor model and need to calculate the volume enclosed between the sphere given by $$(x-d)^2+y^2+z^2=r^2$$ and the cylinder given by $$x^2+y^2=R^2.$$ I have worked it out by using surfaces of revolution but this is tedious and required numerous cases. When I try to use cylindrical coordinates I end up the integral $$\int^{2\pi}_0 \int^R_0 \int^{\sqrt{r^2-R^2+d^2-2dR\cos (\theta )}}_{-\sqrt{r^2-R^2+d^2-2dR\cos (\theta )}} R~dz~dR~d\theta$$ which won't compute. I suspect either I am making an error in my transformation to cylindrical coordinates or a different method is needed.,,['multivariable-calculus']
22,Checking if vector field is conservative,Checking if vector field is conservative,,"If I can find a potential function for a vector field, does that necessarily mean that the vector field is conservative? For example if I had a vector field that is not defined on the x-axis and I am able to find a potential function that is defined everywhere but the x-axis, is the field conservative? I know this is true when the vector field is undefined at finitely many points, but not too sure when there are infinitely many undefined points. Edit: Another question I have is; Suppose a vector field has divergence $= 0 $ everywhere but undefined at the origin and am  trying to find the outward flux. to use the divergence theorem I have to do $$\int\int\int_{V-\{0\}} \text{div} \textbf F \space \text{d}V = \int\int_{\partial V} (\textbf {F $\cdot$ n})\text{d}S + \int\int_{B\epsilon} (\textbf {F $\cdot$ n})\text{d}S = 0  $$ $$\int\int_{\partial V} (\textbf {F $\cdot$ n})\text{d}S = -\int\int_{B\epsilon} (\textbf {F $\cdot$ n})\text{d}S$$ Where $B\epsilon$ is the ball with radius $\epsilon$ around the origin When I solve for $\int\int_{\partial V} (\textbf {F $\cdot$ n})\text{d}S$ I will get an answer depending on epsilon(if it doesn't get cancelled out). If epsilon remains, would I take the limit as epsilon goes to zero, or would I keep my answer with respect to epsilon?","If I can find a potential function for a vector field, does that necessarily mean that the vector field is conservative? For example if I had a vector field that is not defined on the x-axis and I am able to find a potential function that is defined everywhere but the x-axis, is the field conservative? I know this is true when the vector field is undefined at finitely many points, but not too sure when there are infinitely many undefined points. Edit: Another question I have is; Suppose a vector field has divergence $= 0 $ everywhere but undefined at the origin and am  trying to find the outward flux. to use the divergence theorem I have to do $$\int\int\int_{V-\{0\}} \text{div} \textbf F \space \text{d}V = \int\int_{\partial V} (\textbf {F $\cdot$ n})\text{d}S + \int\int_{B\epsilon} (\textbf {F $\cdot$ n})\text{d}S = 0  $$ $$\int\int_{\partial V} (\textbf {F $\cdot$ n})\text{d}S = -\int\int_{B\epsilon} (\textbf {F $\cdot$ n})\text{d}S$$ Where $B\epsilon$ is the ball with radius $\epsilon$ around the origin When I solve for $\int\int_{\partial V} (\textbf {F $\cdot$ n})\text{d}S$ I will get an answer depending on epsilon(if it doesn't get cancelled out). If epsilon remains, would I take the limit as epsilon goes to zero, or would I keep my answer with respect to epsilon?",,['multivariable-calculus']
23,Suspecting that some calculus is wrong in my book,Suspecting that some calculus is wrong in my book,,"Here is something which I am skeptical that it might be wrong: $V$ is a two-variable $C^{\infty}$ function such that $V(t,w) = e^{-rt} F(w)$, for some one-variable $C^{\infty}$ function $F$. Let $z = V_w (t,w)$ and define a function $J$ such that  $$ J(t,z) = V(t,w) - wz.$$ Then it asserts that $J_t = V_t$. (But I get the following: $$J(t,z) = V(t,w) - w V_w (t,w) = e^{-rt} ( F(w) - wF'(w) ),$$ hence $$J_t= -r e^{-rt} ( F(w) - wF'(w) )   \neq V_t.)$$ Can anyone tell me which is right?","Here is something which I am skeptical that it might be wrong: $V$ is a two-variable $C^{\infty}$ function such that $V(t,w) = e^{-rt} F(w)$, for some one-variable $C^{\infty}$ function $F$. Let $z = V_w (t,w)$ and define a function $J$ such that  $$ J(t,z) = V(t,w) - wz.$$ Then it asserts that $J_t = V_t$. (But I get the following: $$J(t,z) = V(t,w) - w V_w (t,w) = e^{-rt} ( F(w) - wF'(w) ),$$ hence $$J_t= -r e^{-rt} ( F(w) - wF'(w) )   \neq V_t.)$$ Can anyone tell me which is right?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
24,"If $x$ is a boundary of $S$ in $\mathbb{R}$, then $x$ must contain both interior points and exterior points of $S$","If  is a boundary of  in , then  must contain both interior points and exterior points of",x S \mathbb{R} x S,"Above is the statement that I am given to prove or disprove. I think it is false. For $Q$ a rational number, there is no interior point nor exterior point. so every point in $Q$ is boundary point, but every ball of any point in $Q$ does not contain both interior and exterior points of $S$. Is it valid counter-example to it? And I am wondering if $Q$ consists of $\textrm{int} S + \textrm{ext} S + \textrm{boundary} S$, where $S$ is subset of $Q$. It is true for $R$, but not sure whether it still hold for $Q$.","Above is the statement that I am given to prove or disprove. I think it is false. For $Q$ a rational number, there is no interior point nor exterior point. so every point in $Q$ is boundary point, but every ball of any point in $Q$ does not contain both interior and exterior points of $S$. Is it valid counter-example to it? And I am wondering if $Q$ consists of $\textrm{int} S + \textrm{ext} S + \textrm{boundary} S$, where $S$ is subset of $Q$. It is true for $R$, but not sure whether it still hold for $Q$.",,['multivariable-calculus']
25,Let $C$ be the curve of intersection of the plane $x+y-z=0$ with ellipsoid $\frac{x^2}4+\frac{y^2}5+\frac{z^2}{25}=1$.,Let  be the curve of intersection of the plane  with ellipsoid .,C x+y-z=0 \frac{x^2}4+\frac{y^2}5+\frac{z^2}{25}=1,"Let $C$ be the curve of intersection of the plane $x+y-z=0$ and the ellipsoid $$\frac{x^2}4+\frac{y^2}5+\frac{z^2}{25}=1$$ Find the points on $C$ which are farthest and nearest from the origin When dealing with constraints I tried to consider the function $$F(x,y,z)=x^2+y^2+z^2-\lambda(x+y-z)-\mu\left(\frac{x^2}4+\frac{y^2}5+\frac{z^2}{25}-1\right)$$ However, I cannot solve this equation after differentiating respect to $x,y,z$ because it yields three equations with no common solution. The system of equations are $$2x=\lambda+\frac{\mu x}{2}$$ $$2y=\lambda+\frac{2\mu y}{5}$$ $$2z=-\lambda+\frac{2\mu z}{25}$$ How would I approach this problem, thanks.","Let be the curve of intersection of the plane and the ellipsoid Find the points on which are farthest and nearest from the origin When dealing with constraints I tried to consider the function However, I cannot solve this equation after differentiating respect to because it yields three equations with no common solution. The system of equations are How would I approach this problem, thanks.","C x+y-z=0 \frac{x^2}4+\frac{y^2}5+\frac{z^2}{25}=1 C F(x,y,z)=x^2+y^2+z^2-\lambda(x+y-z)-\mu\left(\frac{x^2}4+\frac{y^2}5+\frac{z^2}{25}-1\right) x,y,z 2x=\lambda+\frac{\mu x}{2} 2y=\lambda+\frac{2\mu y}{5} 2z=-\lambda+\frac{2\mu z}{25}","['real-analysis', 'multivariable-calculus', 'lagrange-multiplier']"
26,How to use Stokes Theorem to evaluate $\int_{S} \text{curl} F\cdot d\mathbf{S}$,How to use Stokes Theorem to evaluate,\int_{S} \text{curl} F\cdot d\mathbf{S},"Let F = $( yz, 0, x)$ and $S$ is the portion of the plane ${x\over2} + {y\over3} + z = 1$ where $x, y, z \ge 0$, oriented with an upward pointing normal then prove: $$\int_{S} \text{curl} F\cdot d\mathbf{S} = -1 = \int_{\partial S} F\cdot d\mathbf{s}$$ I figured out that $\text{curl} F = (0, y-1, -z)$ then I think the parametrization of the plane is $(x, y, 1- {x\over2} -{y\over3})\implies n = (\frac 12, \frac13,1)$. So $$\int_{S} \text{curl} F\cdot d\mathbf{S} = \int_{S} \left(0, y-1, -1 + {x\over2} +{y\over3}\right)\cdot\left(\frac 12, \frac 13,1\right)dA$$ I don't know how to find the boundary of the plane and I'm not sure if I did right? Thanks for your helping.","Let F = $( yz, 0, x)$ and $S$ is the portion of the plane ${x\over2} + {y\over3} + z = 1$ where $x, y, z \ge 0$, oriented with an upward pointing normal then prove: $$\int_{S} \text{curl} F\cdot d\mathbf{S} = -1 = \int_{\partial S} F\cdot d\mathbf{s}$$ I figured out that $\text{curl} F = (0, y-1, -z)$ then I think the parametrization of the plane is $(x, y, 1- {x\over2} -{y\over3})\implies n = (\frac 12, \frac13,1)$. So $$\int_{S} \text{curl} F\cdot d\mathbf{S} = \int_{S} \left(0, y-1, -1 + {x\over2} +{y\over3}\right)\cdot\left(\frac 12, \frac 13,1\right)dA$$ I don't know how to find the boundary of the plane and I'm not sure if I did right? Thanks for your helping.",,"['integration', 'multivariable-calculus', 'definite-integrals']"
27,Find all points on conic section such that the normal vector to conic section is parallel to y-axis.,Find all points on conic section such that the normal vector to conic section is parallel to y-axis.,,"The conic section k: $x^2+3y^2-2x+6y-8=0$. Find all points such that the normal vector of the conic section in these points is parallel to y-axis. My approach: Find the gradient of the conic section: $F(x,y) = x^2+3y^2-2x+6y$ $\nabla F(x,y) = <2x-2,6y+6>$ The y-axis vector is $(0,1)$. Find such constants that the gradient of the conic section is scalar multiple of y-axis: $2x-2=0 \rightarrow x=1$ $6y+6=1 \rightarrow y=-5/6$ The point $[1,-5/6]$ however isn't the correct answer. If I consider the points separately, then for $x$ I get points $[1,1], [1,-3]$ which is a correct answer but I'm still left with $y=-5/6$ for which I get valid $x$'s but the points are not correct (in fact their are perpendicular to the y-axis and not parallel. Where do I make mistake? Why am I getting the extra $y=-5/6$?","The conic section k: $x^2+3y^2-2x+6y-8=0$. Find all points such that the normal vector of the conic section in these points is parallel to y-axis. My approach: Find the gradient of the conic section: $F(x,y) = x^2+3y^2-2x+6y$ $\nabla F(x,y) = <2x-2,6y+6>$ The y-axis vector is $(0,1)$. Find such constants that the gradient of the conic section is scalar multiple of y-axis: $2x-2=0 \rightarrow x=1$ $6y+6=1 \rightarrow y=-5/6$ The point $[1,-5/6]$ however isn't the correct answer. If I consider the points separately, then for $x$ I get points $[1,1], [1,-3]$ which is a correct answer but I'm still left with $y=-5/6$ for which I get valid $x$'s but the points are not correct (in fact their are perpendicular to the y-axis and not parallel. Where do I make mistake? Why am I getting the extra $y=-5/6$?",,['multivariable-calculus']
28,A Book recommendation for double Integrals?,A Book recommendation for double Integrals?,,"I have a really hard time learning Double Integrals, which I attempted to understand when I first saw the use of polar co-ordinates for Integrals. So my goal is to learn double Integrals and also learn how polar co ordinates is being applied. I'm looking for a book that goes step by step, and which teaches me things like how $dx dy$ changes to $r dr d\theta$, (this is only one example)  etc (everything). In short I'm looking for something like ""Double Integrals with polar co-ordinates for dummies"" Please help me. Thanks.","I have a really hard time learning Double Integrals, which I attempted to understand when I first saw the use of polar co-ordinates for Integrals. So my goal is to learn double Integrals and also learn how polar co ordinates is being applied. I'm looking for a book that goes step by step, and which teaches me things like how $dx dy$ changes to $r dr d\theta$, (this is only one example)  etc (everything). In short I'm looking for something like ""Double Integrals with polar co-ordinates for dummies"" Please help me. Thanks.",,"['calculus', 'integration', 'multivariable-calculus', 'soft-question', 'book-recommendation']"
29,"Gradient, towards a maximum or minimum?","Gradient, towards a maximum or minimum?",,"I have a question on the gradient of a mutlivariable function. For a function $f: \mathbb{R}^{n} \to \mathbb{R}$, the gradient is given as  $$ [\frac{\partial f}{\partial x_1} \cdots \frac{\partial f}{\partial x_n}]$$ From the discussion I have read, I agree with the notion that the gradient is exactly the direction of steepest ascent. To motivate my question, I consider a gradient descent algorithm.  The gradient descent algorithm aims to pick parameters $\vec{\theta} = [\theta_1, \cdots, \theta_n]$ such that the following cost function is minimized. $$J(\theta) = \sum_{i=1}^{m} (h_{\theta}(x_i) - y_i)^2$$ where $h_{\theta}$ is some function parametrized by $\theta$. The algorithm works by updating $\theta_{j}$ to $\theta_{j}'$ for all $j$ using the following update rule.  $$ \theta_{j}' = \theta_{j} - \alpha \frac{\partial J(\theta)}{\partial \theta_{j}}$$ for some constant $\alpha$. By the definition of gradient, our algorithm gets closer and closer to the minimum each time. I get that the gradient is the direction for the greatest rate of change in the function (this comes from the cosine argument linked below), but that could mean both increase or decrease? Why is it that the slope of the gradient is always pointing to a local maximum and not a minimum? Why is gradient the direction of steepest ascent?","I have a question on the gradient of a mutlivariable function. For a function $f: \mathbb{R}^{n} \to \mathbb{R}$, the gradient is given as  $$ [\frac{\partial f}{\partial x_1} \cdots \frac{\partial f}{\partial x_n}]$$ From the discussion I have read, I agree with the notion that the gradient is exactly the direction of steepest ascent. To motivate my question, I consider a gradient descent algorithm.  The gradient descent algorithm aims to pick parameters $\vec{\theta} = [\theta_1, \cdots, \theta_n]$ such that the following cost function is minimized. $$J(\theta) = \sum_{i=1}^{m} (h_{\theta}(x_i) - y_i)^2$$ where $h_{\theta}$ is some function parametrized by $\theta$. The algorithm works by updating $\theta_{j}$ to $\theta_{j}'$ for all $j$ using the following update rule.  $$ \theta_{j}' = \theta_{j} - \alpha \frac{\partial J(\theta)}{\partial \theta_{j}}$$ for some constant $\alpha$. By the definition of gradient, our algorithm gets closer and closer to the minimum each time. I get that the gradient is the direction for the greatest rate of change in the function (this comes from the cosine argument linked below), but that could mean both increase or decrease? Why is it that the slope of the gradient is always pointing to a local maximum and not a minimum? Why is gradient the direction of steepest ascent?",,['multivariable-calculus']
30,Integration by Change of Variable,Integration by Change of Variable,,"By using change of variable, $$x+y=(\surd2)u \text { and } y-x=(\surd2)v$$ Evaluate $$I=\iint(y-x)^2e^{-(x+y)^2}dv\,du$$ with $R$ bounded by $x=0,y=0,x+y=1$ After changing of variable, I get $$\int_0^{1/\surd2}\int_{-u}^u2v^2e^{-2u^2}dv\,du=\frac{4}{3}\int_0^{1/\surd2}u^3e^{-2u^2}\,du$$ I cannot solve the equation after that part. Help me check which part I made mistake. Thank you.","By using change of variable, $$x+y=(\surd2)u \text { and } y-x=(\surd2)v$$ Evaluate $$I=\iint(y-x)^2e^{-(x+y)^2}dv\,du$$ with $R$ bounded by $x=0,y=0,x+y=1$ After changing of variable, I get $$\int_0^{1/\surd2}\int_{-u}^u2v^2e^{-2u^2}dv\,du=\frac{4}{3}\int_0^{1/\surd2}u^3e^{-2u^2}\,du$$ I cannot solve the equation after that part. Help me check which part I made mistake. Thank you.",,['multivariable-calculus']
31,"uniform continuity, differentials","uniform continuity, differentials",,"Let $\{f_n\}_{n=1}^\infty$ be a sequence in $C^1(U)$ where $U \subset \mathbb{R}^d$ is open. Suppose $f_n \to f$ uniformly on compact subsets of $U$. Assume further that $df_n \to A$ in the same sense where $A$ is a function on $U$ taking values in the $d\times d $ matrices. How would I go about proving that $f \in C^1(U)$ and that $df = A$ in $U$? I know that limits of uniformly convergent sequences of continuous functions are continuous. For the derivatives, should I use the definition of the linearization with the error $r(x, h)$?","Let $\{f_n\}_{n=1}^\infty$ be a sequence in $C^1(U)$ where $U \subset \mathbb{R}^d$ is open. Suppose $f_n \to f$ uniformly on compact subsets of $U$. Assume further that $df_n \to A$ in the same sense where $A$ is a function on $U$ taking values in the $d\times d $ matrices. How would I go about proving that $f \in C^1(U)$ and that $df = A$ in $U$? I know that limits of uniformly convergent sequences of continuous functions are continuous. For the derivatives, should I use the definition of the linearization with the error $r(x, h)$?",,"['linear-algebra', 'multivariable-calculus']"
32,Line integral of second kind over a circle: $\int \frac{xdy - ydx}{x^2+y^2}$,Line integral of second kind over a circle:,\int \frac{xdy - ydx}{x^2+y^2},"I've just get stuck with some task of line integral: $$\int \frac{xdy - ydx}{x^2+y^2}\quad \text{ over} \ x^2+y^2=R^2$$ I understand that I need to use polar coordinates, and I have such thing: $$x = r \cos\theta\quad dx = -r \sin\theta d\theta$$ $$y = r \sin\theta\quad dy = r  \cos\theta d\theta$$ Then I put it in a task example and get: $$\int (r^2 \cos^2\theta+r^2\sin^2\theta)d\theta/r^2 = \int d\theta$$ But from what to what should I integrate and is the result is right?","I've just get stuck with some task of line integral: $$\int \frac{xdy - ydx}{x^2+y^2}\quad \text{ over} \ x^2+y^2=R^2$$ I understand that I need to use polar coordinates, and I have such thing: $$x = r \cos\theta\quad dx = -r \sin\theta d\theta$$ $$y = r \sin\theta\quad dy = r  \cos\theta d\theta$$ Then I put it in a task example and get: $$\int (r^2 \cos^2\theta+r^2\sin^2\theta)d\theta/r^2 = \int d\theta$$ But from what to what should I integrate and is the result is right?",,"['integration', 'multivariable-calculus', 'line-integrals']"
33,Change of Variables: which order of integration limits?,Change of Variables: which order of integration limits?,,"An exercise in Larson/Edwards Calculus (10th ed.) asks the reader to evaluate the double integral $$\int_R\int 4(x^2+y^2)\,\mathrm{d}A$$ using a given change of variables $$x=\frac{1}{2}(u+v)\mathrm{,}\quad y=\frac{1}{2}(u-v)$$ over the region pictured below. Since, with the given change of variables, $\,v=x-y\,$ and $\,u=x+y$, it looks like my $v$ limits of integration are going to be $-1$ and $1$, and my $u$ limits of integration are going to be $-1$ and $1$ as well.  My question is: how do I know which number should be the upper limit of integration and which number should be the lower limit of integration? Going off the picture, it looks like I should be integrating from $v=1$ to $v=-1$, and from $u=-1$ to $u=1$, so my integral would look like this: $$\int_{1}^{-1}\int_{-1}^{1}f(u,v)\,\mathrm{d}u\,\mathrm{d}v$$ This gives me the opposite of the correct answer (the correct answer multiplied by $-1$). I know I get the correct answer by integrating $$\int_{-1}^{1}\int_{-1}^{1}f(u,v)\,\mathrm{d}u\,\mathrm{d}v$$ instead, but I don't see any reason why I ought to put the limits in that order. I suppose it might always be correct to choose the least of the two values to be the lower limit and the greatest of the two values to be the upper limit (thought I'm not sure), but even if that were so I still wouldn't really understand why that's the case. Edit: I tried writing the original integral without changing the variables, like this: $$\int_{-1}^{0}\int_{-x-1}^{x+1}g(x,y)\,\mathrm{d}y\,\mathrm{d}x+\int_0^1\int_{x-1}^{-x+1}g(x,y)\,\mathrm{d}y\,\mathrm{d}x$$ and then manipulating the inequalities $$-x-1\leq y\leq x+1\quad\mathrm{and}\quad x-1\leq y \leq -x+1$$ to find $-1\leq x+y\mathrm{,}$ $-1\leq x-y\mathrm{,}$ $x-y\leq 1\mathrm{,}$ and $x+y\leq 1$. Substituting $v$ for $x-y$ and $u$ for $x+y$, I get $-1\leq v\leq 1$ and $-1\leq u\leq 1$. Basing my limits of integration off these inequalities seems to work, but I'm not sure this procedure for finding limits of integration works in general.","An exercise in Larson/Edwards Calculus (10th ed.) asks the reader to evaluate the double integral $$\int_R\int 4(x^2+y^2)\,\mathrm{d}A$$ using a given change of variables $$x=\frac{1}{2}(u+v)\mathrm{,}\quad y=\frac{1}{2}(u-v)$$ over the region pictured below. Since, with the given change of variables, $\,v=x-y\,$ and $\,u=x+y$, it looks like my $v$ limits of integration are going to be $-1$ and $1$, and my $u$ limits of integration are going to be $-1$ and $1$ as well.  My question is: how do I know which number should be the upper limit of integration and which number should be the lower limit of integration? Going off the picture, it looks like I should be integrating from $v=1$ to $v=-1$, and from $u=-1$ to $u=1$, so my integral would look like this: $$\int_{1}^{-1}\int_{-1}^{1}f(u,v)\,\mathrm{d}u\,\mathrm{d}v$$ This gives me the opposite of the correct answer (the correct answer multiplied by $-1$). I know I get the correct answer by integrating $$\int_{-1}^{1}\int_{-1}^{1}f(u,v)\,\mathrm{d}u\,\mathrm{d}v$$ instead, but I don't see any reason why I ought to put the limits in that order. I suppose it might always be correct to choose the least of the two values to be the lower limit and the greatest of the two values to be the upper limit (thought I'm not sure), but even if that were so I still wouldn't really understand why that's the case. Edit: I tried writing the original integral without changing the variables, like this: $$\int_{-1}^{0}\int_{-x-1}^{x+1}g(x,y)\,\mathrm{d}y\,\mathrm{d}x+\int_0^1\int_{x-1}^{-x+1}g(x,y)\,\mathrm{d}y\,\mathrm{d}x$$ and then manipulating the inequalities $$-x-1\leq y\leq x+1\quad\mathrm{and}\quad x-1\leq y \leq -x+1$$ to find $-1\leq x+y\mathrm{,}$ $-1\leq x-y\mathrm{,}$ $x-y\leq 1\mathrm{,}$ and $x+y\leq 1$. Substituting $v$ for $x-y$ and $u$ for $x+y$, I get $-1\leq v\leq 1$ and $-1\leq u\leq 1$. Basing my limits of integration off these inequalities seems to work, but I'm not sure this procedure for finding limits of integration works in general.",,['multivariable-calculus']
34,How to use Lagrange Multiplier in this question?,How to use Lagrange Multiplier in this question?,,"I have to find absolute maximum and minimum values of $f(x,y)$ = $4x^{2} + 9y^{2} -8x - 12y + 4 $ over rectangle in first quadrant bounded by lines $x=2 , y=3$   and coordinate axes I have checked interior points for maxima and minima .But for points on the boundary i need to check now .I can also substitute values in original function of two variables and reduce it into single variable and check extremum on boundary . But im interested to use lagrange multipliers in this case . Can anyone help me with that ? Thanks in advance","I have to find absolute maximum and minimum values of $f(x,y)$ = $4x^{2} + 9y^{2} -8x - 12y + 4 $ over rectangle in first quadrant bounded by lines $x=2 , y=3$   and coordinate axes I have checked interior points for maxima and minima .But for points on the boundary i need to check now .I can also substitute values in original function of two variables and reduce it into single variable and check extremum on boundary . But im interested to use lagrange multipliers in this case . Can anyone help me with that ? Thanks in advance",,"['multivariable-calculus', 'lagrange-multiplier']"
35,Double integral change of variables,Double integral change of variables,,"Using an appropriate change of variables, evaluate $$ \iint_{B}\exp\left(\,{y - x \over y+x}\,\right)\,{\rm d}x\,{\rm d}y $$  Where $B$ is the interior of the triangle with vertices at $\left(\, 0,0\,\right), \left(\, 0,1\,\right)\ \mbox{and}\ \left(\, 1,0\,\right)$. Attempt: I used $u = y - x\,,\ v = y + x$ as my new variables. I also found that the old bounds were $0\ \leq\ x\ \leq\ 1$ and $0\ \leq\ y\ \leq\ 1 - x$. However, I don't understand how to put my bounds in terms of $u$ and $v$. I tried setting $y = u + x = v - x$, then solving for $x$ to get $x = \left(\, v - u\,\right)/2$. I substituted that into the bound for $x$ and got $0\ \leq\ v\ \leq\ 2 + u$. Is that correct ?. How do I get the constant bounds for $u$ ?. Thank you!","Using an appropriate change of variables, evaluate $$ \iint_{B}\exp\left(\,{y - x \over y+x}\,\right)\,{\rm d}x\,{\rm d}y $$  Where $B$ is the interior of the triangle with vertices at $\left(\, 0,0\,\right), \left(\, 0,1\,\right)\ \mbox{and}\ \left(\, 1,0\,\right)$. Attempt: I used $u = y - x\,,\ v = y + x$ as my new variables. I also found that the old bounds were $0\ \leq\ x\ \leq\ 1$ and $0\ \leq\ y\ \leq\ 1 - x$. However, I don't understand how to put my bounds in terms of $u$ and $v$. I tried setting $y = u + x = v - x$, then solving for $x$ to get $x = \left(\, v - u\,\right)/2$. I substituted that into the bound for $x$ and got $0\ \leq\ v\ \leq\ 2 + u$. Is that correct ?. How do I get the constant bounds for $u$ ?. Thank you!",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
36,Reverse Mean Value Theorem for convex functions,Reverse Mean Value Theorem for convex functions,,"1)By ""reverse"" MVT I mean: eg. in d=1, for any $x\in \mathbb{R}$ there exists $a,b$ containing x s.t. for $f\in C^{1}$ we have $f'(x)=\frac{f(b)-f(a)}{b-a}$. This fails for all functions that cross the real line eg. $f(x)=x^{3}$ fails at $x=0$. However, I think if the function is also convex and strictly positive, or convace and strictly negative, it works. Can we weaken it to locally convex? 2)How about some general formula connecting derivative with it's antiderivative i.e. for any $x\in \mathbb{R}$ there exists $a,b$ s.t. for $f\in C^{1}$ we have $f'(x)=G(f(a),f(b),r_{i})$, where $r_{i}\in \mathbb{R}$ are real numbers. Thanks","1)By ""reverse"" MVT I mean: eg. in d=1, for any $x\in \mathbb{R}$ there exists $a,b$ containing x s.t. for $f\in C^{1}$ we have $f'(x)=\frac{f(b)-f(a)}{b-a}$. This fails for all functions that cross the real line eg. $f(x)=x^{3}$ fails at $x=0$. However, I think if the function is also convex and strictly positive, or convace and strictly negative, it works. Can we weaken it to locally convex? 2)How about some general formula connecting derivative with it's antiderivative i.e. for any $x\in \mathbb{R}$ there exists $a,b$ s.t. for $f\in C^{1}$ we have $f'(x)=G(f(a),f(b),r_{i})$, where $r_{i}\in \mathbb{R}$ are real numbers. Thanks",,"['calculus', 'multivariable-calculus', 'convex-analysis']"
37,doubt regarding Definition of differentiability .,doubt regarding Definition of differentiability .,,"The definition of a differentiable function is as follows: A function $f:A\to Y$ is said to be differentiable at a $\in A$ if there is a linear map $T\in L(X,Y), $ such that : $$\text{lim}_{~r\to 0}\frac{\|f(a+r)-f(a)-Tr\|}{\|r\|}=0$$ If such a $T:X\to Y$ exists ,then it is unique.It is denoted as derivative of $f$ at $a\in A$ and is denoted as $f'(a)$ .Note that $f'(a):X\to Y$ is a linear operator and its value at $x\in X$ is written as $f'(a)(x)\in Y$ I can't understand if we map all values from $X\to Y$ using $f'(a)$ what do we get... I know it is $f'(a)(x)\in Y$ at a particular $x\in X$ ..but what are we doing with this mapping.. Kindly help me..","The definition of a differentiable function is as follows: A function is said to be differentiable at a if there is a linear map such that : If such a exists ,then it is unique.It is denoted as derivative of at and is denoted as .Note that is a linear operator and its value at is written as I can't understand if we map all values from using what do we get... I know it is at a particular ..but what are we doing with this mapping.. Kindly help me..","f:A\to Y \in A T\in L(X,Y),  \text{lim}_{~r\to 0}\frac{\|f(a+r)-f(a)-Tr\|}{\|r\|}=0 T:X\to Y f a\in A f'(a) f'(a):X\to Y x\in X f'(a)(x)\in Y X\to Y f'(a) f'(a)(x)\in Y x\in X","['multivariable-calculus', 'derivatives']"
38,The derivative as a linear transform,The derivative as a linear transform,,"I'm having trouble wrapping my head around thinking about the derivative as a linear transform. Here is an example I came up with to try and understand it better. Let $f: \mathbb{R}^2 \to \mathbb{R}^3$ be defined by $f(x,y) = (x+y,xy,x^2-y^2)$ Then the derivative of $f$ at $\vec{x_0} = \left[\matrix{    x_0 \cr    y_0 \cr }\right]$ is given by: $f'_\vec{x_0} = \left[\begin{array}{*{20}{c}} {1}&{1}\\ {y_0}&{x_0}\\ {2x_0}&{-2y_0}\\ \end{array}\right] $ This means that $f'_\vec{x_0}$ is the linear transformation that takes a vector $\vec{v} \in \mathbb{R}^2$ and sends it to $f'_\vec{x_0}\vec{v} \in \mathbb{R}^3$. Here are my questions: How do I interpret the derivative in higher dimensions? In one dimension we can think of the derivative as the slope of a tangent line. Is there a similar notion for higher dimensions? I'm looking at the above matrix and trying to attach some meaning to it, but I can't come up with anything. What does  $f'_\vec{x_0}\vec{v}$ represent? Is it just the derivative of $f$ at $\vec{x_0}$ in the direction of $\vec{v}$? Do the components of this vector represent how fast $f$ is changing along each axis?","I'm having trouble wrapping my head around thinking about the derivative as a linear transform. Here is an example I came up with to try and understand it better. Let $f: \mathbb{R}^2 \to \mathbb{R}^3$ be defined by $f(x,y) = (x+y,xy,x^2-y^2)$ Then the derivative of $f$ at $\vec{x_0} = \left[\matrix{    x_0 \cr    y_0 \cr }\right]$ is given by: $f'_\vec{x_0} = \left[\begin{array}{*{20}{c}} {1}&{1}\\ {y_0}&{x_0}\\ {2x_0}&{-2y_0}\\ \end{array}\right] $ This means that $f'_\vec{x_0}$ is the linear transformation that takes a vector $\vec{v} \in \mathbb{R}^2$ and sends it to $f'_\vec{x_0}\vec{v} \in \mathbb{R}^3$. Here are my questions: How do I interpret the derivative in higher dimensions? In one dimension we can think of the derivative as the slope of a tangent line. Is there a similar notion for higher dimensions? I'm looking at the above matrix and trying to attach some meaning to it, but I can't come up with anything. What does  $f'_\vec{x_0}\vec{v}$ represent? Is it just the derivative of $f$ at $\vec{x_0}$ in the direction of $\vec{v}$? Do the components of this vector represent how fast $f$ is changing along each axis?",,"['real-analysis', 'multivariable-calculus']"
39,Proving the relation: $∇(\mathbf{u}·\mathbf{v})=(\mathbf{v}·∇)\mathbf{u}+(\mathbf{u}·∇)\mathbf{v}+\mathbf{v}×(∇×\mathbf{u})+\mathbf{u}×(∇×\mathbf{v})$,Proving the relation:,∇(\mathbf{u}·\mathbf{v})=(\mathbf{v}·∇)\mathbf{u}+(\mathbf{u}·∇)\mathbf{v}+\mathbf{v}×(∇×\mathbf{u})+\mathbf{u}×(∇×\mathbf{v}),"I have to prove the following relation. I am looking for a solution beyond the obvious brute force method of considering $\mathbf{u}=u_1\mathbf{i}+u_2\mathbf{j}+u_3\mathbf{k}\;and\;\mathbf{v}=v_1\mathbf{i}+v_2\mathbf{j}+v_3\mathbf{k}$ putting in these values and simply showing that the two sides come to the same result through a long and tedious calculation. Any thoughts? $$∇(\mathbf{u}·\mathbf{v})=(\mathbf{v}·∇)\mathbf{u}+(\mathbf{u}·∇)\mathbf{v}+\mathbf{v}×(∇×\mathbf{u})+\mathbf{u}×(∇×\mathbf{v})\;where\;\mathbf{u},\mathbf{v}\;are\;vectors$$ P.S: I was not very sure whether I have added the correct tags or not. Feel free to add or remove tags as you deem appropriate.","I have to prove the following relation. I am looking for a solution beyond the obvious brute force method of considering $\mathbf{u}=u_1\mathbf{i}+u_2\mathbf{j}+u_3\mathbf{k}\;and\;\mathbf{v}=v_1\mathbf{i}+v_2\mathbf{j}+v_3\mathbf{k}$ putting in these values and simply showing that the two sides come to the same result through a long and tedious calculation. Any thoughts? $$∇(\mathbf{u}·\mathbf{v})=(\mathbf{v}·∇)\mathbf{u}+(\mathbf{u}·∇)\mathbf{v}+\mathbf{v}×(∇×\mathbf{u})+\mathbf{u}×(∇×\mathbf{v})\;where\;\mathbf{u},\mathbf{v}\;are\;vectors$$ P.S: I was not very sure whether I have added the correct tags or not. Feel free to add or remove tags as you deem appropriate.",,"['multivariable-calculus', 'vector-analysis']"
40,Area and volume relation (multivariable calculus problem),Area and volume relation (multivariable calculus problem),,"Let $D \subset R^3$ a region over the plane $z=0$, if $C$ is the cone of base $D$ and vertex at $(0,0,1)$, show that $Vol(C)=\dfrac{1}{3}A(D)$, where $A(D)$ is the area of the region $D$. First I thought of applying Gauss (divergence) theorem over the region $W$, where $W$ is the region enclosed by the cone and the function $F=(x,y,z)$, in that case, we would have $$\iiint_W div(F)dV=\iint_{\partial W} F.dS.$$ We have $$\iiint_W div(F)dV=3\iiint_W 1dV=3Vol(C).$$ So I've tried to show $$\iint_{\partial W} F.dS=A(D)$$ but I couldn't. I've parametrized the cone ($\partial W$) by $T(u,v)=(u\cos(v),u\sin(v),u)$, then $T_u \times T_v=(-u\cos(v),-u\sin(v),u)$, so $$\iint_{\partial W} F.dS=\iint_{[0,1]\times [0,2\pi]} (u\cos(v),u\sin(v),u).(-u\cos(v),-u\sin(v),u)dudv=-u^2+u^2=0$$ Clearly the area of$D$ is not $0$. I don't know what I am doing wrong, I would appreciate corrections and suggestions.","Let $D \subset R^3$ a region over the plane $z=0$, if $C$ is the cone of base $D$ and vertex at $(0,0,1)$, show that $Vol(C)=\dfrac{1}{3}A(D)$, where $A(D)$ is the area of the region $D$. First I thought of applying Gauss (divergence) theorem over the region $W$, where $W$ is the region enclosed by the cone and the function $F=(x,y,z)$, in that case, we would have $$\iiint_W div(F)dV=\iint_{\partial W} F.dS.$$ We have $$\iiint_W div(F)dV=3\iiint_W 1dV=3Vol(C).$$ So I've tried to show $$\iint_{\partial W} F.dS=A(D)$$ but I couldn't. I've parametrized the cone ($\partial W$) by $T(u,v)=(u\cos(v),u\sin(v),u)$, then $T_u \times T_v=(-u\cos(v),-u\sin(v),u)$, so $$\iint_{\partial W} F.dS=\iint_{[0,1]\times [0,2\pi]} (u\cos(v),u\sin(v),u).(-u\cos(v),-u\sin(v),u)dudv=-u^2+u^2=0$$ Clearly the area of$D$ is not $0$. I don't know what I am doing wrong, I would appreciate corrections and suggestions.",,"['integration', 'multivariable-calculus']"
41,how to prove gradients vectors are the same in polar and cartesian coordinates.,how to prove gradients vectors are the same in polar and cartesian coordinates.,,"Suppose $T=T(r,\theta)=G(x,y)$ How do you prove $\nabla T(r,\theta)=\nabla G(x,y)$? I can think of some arguments in favor of this equality, but I want an actual proof or a very good intuitive argument. My arguments in favor go something like this: -Gradient vectors should be the same because if my directional derivative is taken parallel to the gradient vector then I get its maximum/minimum value and if these two gradient vectors are the same then everything will be consistent . Thanks.","Suppose $T=T(r,\theta)=G(x,y)$ How do you prove $\nabla T(r,\theta)=\nabla G(x,y)$? I can think of some arguments in favor of this equality, but I want an actual proof or a very good intuitive argument. My arguments in favor go something like this: -Gradient vectors should be the same because if my directional derivative is taken parallel to the gradient vector then I get its maximum/minimum value and if these two gradient vectors are the same then everything will be consistent . Thanks.",,"['multivariable-calculus', 'vector-analysis']"
42,Finding partial derivatives of a given function,Finding partial derivatives of a given function,,"I must calculate $\displaystyle \frac{\partial{f}}{\partial{x}}$ and $\displaystyle\frac{\partial{f}}{\partial{y}}$ from $f(x,y)=\displaystyle\frac{4 \cdot \pi^2 \cdot x}{y^2}$. $$\displaystyle \frac{\partial{f}}{\partial{x}}=\frac{4 \cdot \pi^2}{y^2}$$ $$\displaystyle \frac{\partial{f}}{\partial{y}}=\frac{-8 \cdot \pi^2 \cdot x}{y^3}$$  Is it correct? Thanks in advance!","I must calculate $\displaystyle \frac{\partial{f}}{\partial{x}}$ and $\displaystyle\frac{\partial{f}}{\partial{y}}$ from $f(x,y)=\displaystyle\frac{4 \cdot \pi^2 \cdot x}{y^2}$. $$\displaystyle \frac{\partial{f}}{\partial{x}}=\frac{4 \cdot \pi^2}{y^2}$$ $$\displaystyle \frac{\partial{f}}{\partial{y}}=\frac{-8 \cdot \pi^2 \cdot x}{y^3}$$  Is it correct? Thanks in advance!",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
43,Poisson equation on a square,Poisson equation on a square,,"Studying PDEs from the notes of my professor, and there's a part I don't understand about seeking a solution for the Poisson equation on a square. Let's start from the beginning though. We want to solve the problem  $$\begin{cases}  &\nabla^2u=F(x,y) \\   &u(x,0)=a(x) \\   &u(\pi,y)=b(y) \\   &u(x,\pi)=c(x) \\   &u(0,y)=d(y)  \end{cases}$$ By linearity, we can solve 5 problems distinctly. Among these 5 problems, there's the ""seeking a solution for the nonhomogeneous equation"". At one point, it says on the notes: We can write $F(x,y)$ as $\sum F_n(y)X_n(x)$, where $X_n(x)$ satisfies the equation $X_n''(x)=-\lambda X_n(x)$. I know he wants to solve it using Lagrange's method, but I don't quite understand why $F(x,y)$ can be written that way. Anyone?","Studying PDEs from the notes of my professor, and there's a part I don't understand about seeking a solution for the Poisson equation on a square. Let's start from the beginning though. We want to solve the problem  $$\begin{cases}  &\nabla^2u=F(x,y) \\   &u(x,0)=a(x) \\   &u(\pi,y)=b(y) \\   &u(x,\pi)=c(x) \\   &u(0,y)=d(y)  \end{cases}$$ By linearity, we can solve 5 problems distinctly. Among these 5 problems, there's the ""seeking a solution for the nonhomogeneous equation"". At one point, it says on the notes: We can write $F(x,y)$ as $\sum F_n(y)X_n(x)$, where $X_n(x)$ satisfies the equation $X_n''(x)=-\lambda X_n(x)$. I know he wants to solve it using Lagrange's method, but I don't quite understand why $F(x,y)$ can be written that way. Anyone?",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'poissons-equation']"
44,approximate this fancy looking double integral,approximate this fancy looking double integral,,"$$\int_{0}^{2\pi} \int_{0}^{1}r^5\sin^22\theta\left(1-r^2 \right)^2\sqrt{1+\left(1+ \cos^2\theta \right)36r^2  }\hspace{1mm}drd\theta$$ I tried integrating myself, spent many hours but could not figure out really anything, even used wolfram, it says time up!","$$\int_{0}^{2\pi} \int_{0}^{1}r^5\sin^22\theta\left(1-r^2 \right)^2\sqrt{1+\left(1+ \cos^2\theta \right)36r^2  }\hspace{1mm}drd\theta$$ I tried integrating myself, spent many hours but could not figure out really anything, even used wolfram, it says time up!",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'polar-coordinates']"
45,Verify that $\nabla(A\cdot B) = (B\cdot\nabla)A + (A\cdot\nabla)B + B\times(\nabla\times A) + A\times(\nabla\times B)$,Verify that,\nabla(A\cdot B) = (B\cdot\nabla)A + (A\cdot\nabla)B + B\times(\nabla\times A) + A\times(\nabla\times B),"I'm trying to verify the following identity $$\nabla(\textbf{A}\cdot\textbf{B}) = (\textbf{B}\cdot\nabla)\textbf{A} + (\textbf{A}\cdot\nabla)\textbf{B} + \textbf{B}\times(\nabla\times\textbf{A}) + \textbf{A}\times(\nabla\times\textbf{B})$$ To make this, I'm using the $BAC-CAB$ expansion of the triple vector product. Then, I get these two equations $ \textbf{A}\times(\nabla\times\textbf{B}) =  \nabla(\textbf{A}\cdot\textbf{B}) - (\textbf{A}\cdot\nabla)\textbf{B}$ and $ \textbf{B}\times(\nabla\times\textbf{A}) =  \nabla(\textbf{A}\cdot\textbf{B}) - (\textbf{B}\cdot\nabla)\textbf{A}$. Adding these equations I obtained that $$ 2\nabla(\textbf{A}\cdot\textbf{B}) = (\textbf{B}\cdot\nabla)\textbf{A} + (\textbf{A}\cdot\nabla)\textbf{B} + \textbf{B}\times(\nabla\times\textbf{A}) + \textbf{A}\times(\nabla\times\textbf{B}) $$ What am I doing wrong?","I'm trying to verify the following identity $$\nabla(\textbf{A}\cdot\textbf{B}) = (\textbf{B}\cdot\nabla)\textbf{A} + (\textbf{A}\cdot\nabla)\textbf{B} + \textbf{B}\times(\nabla\times\textbf{A}) + \textbf{A}\times(\nabla\times\textbf{B})$$ To make this, I'm using the $BAC-CAB$ expansion of the triple vector product. Then, I get these two equations $ \textbf{A}\times(\nabla\times\textbf{B}) =  \nabla(\textbf{A}\cdot\textbf{B}) - (\textbf{A}\cdot\nabla)\textbf{B}$ and $ \textbf{B}\times(\nabla\times\textbf{A}) =  \nabla(\textbf{A}\cdot\textbf{B}) - (\textbf{B}\cdot\nabla)\textbf{A}$. Adding these equations I obtained that $$ 2\nabla(\textbf{A}\cdot\textbf{B}) = (\textbf{B}\cdot\nabla)\textbf{A} + (\textbf{A}\cdot\nabla)\textbf{B} + \textbf{B}\times(\nabla\times\textbf{A}) + \textbf{A}\times(\nabla\times\textbf{B}) $$ What am I doing wrong?",,"['multivariable-calculus', 'physics']"
46,Parametrizing a 3D surface,Parametrizing a 3D surface,,"Find a parametrization of the surface $x^3 + 3xy + z^2 = 2$, $z > 0$, and use it to find the tangent plane at $x = 1$, $y = \dfrac{1}{3}$, $z = 0$. I know how to find the tangent plane once I have the parametrization - it's the first part that's troubling me. I started by solving for $z$, which gave me the parametrization $(u, \, v, \, \sqrt{2-u^3-3uv})$. Then the partial derivative w.r.t. $u$ is $\left( 1, \, 0, \, \dfrac{-3(u^2+v)}{2 \sqrt{2-u^3 - 3uv}} \right)$. But when I plug in $u = 1$ and $y = \dfrac{1}{3}$, I get a discontinuity. Not really sure what else to try. Maybe cylindrical coordinates would work better?","Find a parametrization of the surface $x^3 + 3xy + z^2 = 2$, $z > 0$, and use it to find the tangent plane at $x = 1$, $y = \dfrac{1}{3}$, $z = 0$. I know how to find the tangent plane once I have the parametrization - it's the first part that's troubling me. I started by solving for $z$, which gave me the parametrization $(u, \, v, \, \sqrt{2-u^3-3uv})$. Then the partial derivative w.r.t. $u$ is $\left( 1, \, 0, \, \dfrac{-3(u^2+v)}{2 \sqrt{2-u^3 - 3uv}} \right)$. But when I plug in $u = 1$ and $y = \dfrac{1}{3}$, I get a discontinuity. Not really sure what else to try. Maybe cylindrical coordinates would work better?",,"['multivariable-calculus', 'parametric']"
47,Why this $C^1$ function is onto?,Why this  function is onto?,C^1,Let $f:\mathbb R^n\to \mathbb R^m $ is class $C^1$ Also $f^{-1}(B)$ is bounded whenever $B$ is bounded and  $\nabla f_i(x)$ are linearly independent for each $x$. Then $f$ is onto. Why? I have no idea to explain this.,Let $f:\mathbb R^n\to \mathbb R^m $ is class $C^1$ Also $f^{-1}(B)$ is bounded whenever $B$ is bounded and  $\nabla f_i(x)$ are linearly independent for each $x$. Then $f$ is onto. Why? I have no idea to explain this.,,['multivariable-calculus']
48,Double integrals in polar coordinates,Double integrals in polar coordinates,,"Determine the domain of $$\!\!\!\!\!\!\!{\small D \equiv \left\{\left(x,y\right) \in \mathbb{R}^{2}\ {\large\mid}\ x \in \left[\,{-\,\frac{1}{\,\sqrt{\,{2}\,}\,}, \frac{1}{\,\sqrt{\,{2}\,}\,}}\,\right],\ y \in \left[\,{\left\vert\,{x}\,\right\vert, \,\sqrt{\,{1 - x^{2}}\,}\,}\,\right]\right\}} $$ in polar coordinates and draw it. Also how would you integrate $$\int\int_D \frac{1}{1+x^2 + y^2}dA$$ which is i guess $$\int_{-\frac{1}{\sqrt{2}}}^{\frac{1}{\sqrt{2}}}\int_{|x|}^{\sqrt{1-x^2}} \frac{1}{1+x^2 + y^2}dydx$$ I guess in the integral you can use the polar coordinates $$\int\int_D \frac{1}{1+r^2\cos^2(\phi) + r^2\sin^2(\phi)}rdrd\phi$$ $$\int\int_D \frac{r}{1+r^2}drd\phi$$ $$\int_{\frac{1}{4\pi}}^{\frac{3}{4\pi}}\int_0^1 \frac{r}{1+r^2}drd\phi=\int_{\frac{1}{4\pi}}^{\frac{3}{4\pi}}\left(\frac 12 \ln(1+1^2)-\frac 12\ln(1+0^2) \right)d\phi$$ $$\int_{\frac{1}{4\pi}}^{\frac{3}{4\pi}}\frac{\ln{2}}{2}d\phi=\left(\frac{3}{4\pi}\frac{\ln{2}}{2}-\frac{1}{4\pi}\frac{\ln{2}}{2} \right)=\frac{\pi}{4}\ln{2}$$ Did I get it right?",Determine the domain of in polar coordinates and draw it. Also how would you integrate which is i guess I guess in the integral you can use the polar coordinates Did I get it right?,"\!\!\!\!\!\!\!{\small
D \equiv
\left\{\left(x,y\right) \in \mathbb{R}^{2}\
{\large\mid}\
x \in \left[\,{-\,\frac{1}{\,\sqrt{\,{2}\,}\,},
\frac{1}{\,\sqrt{\,{2}\,}\,}}\,\right],\
y \in \left[\,{\left\vert\,{x}\,\right\vert,
\,\sqrt{\,{1 - x^{2}}\,}\,}\,\right]\right\}}
 \int\int_D \frac{1}{1+x^2 + y^2}dA \int_{-\frac{1}{\sqrt{2}}}^{\frac{1}{\sqrt{2}}}\int_{|x|}^{\sqrt{1-x^2}} \frac{1}{1+x^2 + y^2}dydx \int\int_D \frac{1}{1+r^2\cos^2(\phi) + r^2\sin^2(\phi)}rdrd\phi \int\int_D \frac{r}{1+r^2}drd\phi \int_{\frac{1}{4\pi}}^{\frac{3}{4\pi}}\int_0^1 \frac{r}{1+r^2}drd\phi=\int_{\frac{1}{4\pi}}^{\frac{3}{4\pi}}\left(\frac 12 \ln(1+1^2)-\frac 12\ln(1+0^2) \right)d\phi \int_{\frac{1}{4\pi}}^{\frac{3}{4\pi}}\frac{\ln{2}}{2}d\phi=\left(\frac{3}{4\pi}\frac{\ln{2}}{2}-\frac{1}{4\pi}\frac{\ln{2}}{2} \right)=\frac{\pi}{4}\ln{2}","['integration', 'multivariable-calculus']"
49,Triple integral over a sphere with parameter $2n$?,Triple integral over a sphere with parameter ?,2n,"I need to integrate $x^{2n}+y^{2n}+z^{2n}$ over a sphere of equation $x²+y²+z²=1$. I have thought of changing the coordinates from cartesian to spherical but I don't know how to deal with the integrant in this case. Or maybe should I divide the integral in three parts? Or even use the vector field $r=(x,y,z)$, but then how? Any help would be amazing there. Thank you.","I need to integrate $x^{2n}+y^{2n}+z^{2n}$ over a sphere of equation $x²+y²+z²=1$. I have thought of changing the coordinates from cartesian to spherical but I don't know how to deal with the integrant in this case. Or maybe should I divide the integral in three parts? Or even use the vector field $r=(x,y,z)$, but then how? Any help would be amazing there. Thank you.",,"['multivariable-calculus', 'definite-integrals', 'spherical-coordinates']"
50,Integral curves of $X = z \dfrac{\partial}{\partial \theta} - \sin \theta \dfrac{\partial}{\partial z}$ on a cylinder,Integral curves of  on a cylinder,X = z \dfrac{\partial}{\partial \theta} - \sin \theta \dfrac{\partial}{\partial z},"Consider coordinates $(\theta, z)$ on $S^1 \times \mathbb R$, and a vector field $$X = z \dfrac{\partial}{\partial \theta} - \sin \theta \dfrac{\partial}{\partial z}.$$ Show that the integral curve of $X$ through $(\pi/2,0)$ defines a compact submanifold, and find another point s.t. the passing integral curve is not compact. Hint: calculate $H = \dfrac{z^2}{2}-\cos \theta$ on the integral curves. 1st attempt. By brute force one obtains for an integral curve $\alpha_t = (\alpha^1_t, \alpha^2_t)$ the system $$ \begin{cases} \dot \alpha^1 = \alpha^2\\ \dot \alpha^2 = - \sin \alpha^1 \end{cases}$$ the second equation being not integrable with elementary functions. However maybe one could deduce some properties of the solution? 2nd attempt. A quick calculation shows that $H$ is costant along integral curves. Therefore $z$ is limited on the integral curve, so the curve lies on a limited cilinder. But how do I know that it is compact (and moreover the curve is injective with non null derivative)?","Consider coordinates $(\theta, z)$ on $S^1 \times \mathbb R$, and a vector field $$X = z \dfrac{\partial}{\partial \theta} - \sin \theta \dfrac{\partial}{\partial z}.$$ Show that the integral curve of $X$ through $(\pi/2,0)$ defines a compact submanifold, and find another point s.t. the passing integral curve is not compact. Hint: calculate $H = \dfrac{z^2}{2}-\cos \theta$ on the integral curves. 1st attempt. By brute force one obtains for an integral curve $\alpha_t = (\alpha^1_t, \alpha^2_t)$ the system $$ \begin{cases} \dot \alpha^1 = \alpha^2\\ \dot \alpha^2 = - \sin \alpha^1 \end{cases}$$ the second equation being not integrable with elementary functions. However maybe one could deduce some properties of the solution? 2nd attempt. A quick calculation shows that $H$ is costant along integral curves. Therefore $z$ is limited on the integral curve, so the curve lies on a limited cilinder. But how do I know that it is compact (and moreover the curve is injective with non null derivative)?",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'vector-fields']"
51,Find maximum of a double integral over a region,Find maximum of a double integral over a region,,"I have a region given by $$R = |{ax}|+|{by}| \le 1$$ and $$f(x,y) = \iint\limits_{R}{(ax-by)^2 \ \cdot \ (3ab^3+12a^3b-6a^3b^2) \ \cdot \ \sin^2({\pi ax + \pi by}})dxdy$$ I need to find the values of $a$ and $b$ that maximize $f$ and I have no idea where to start.","I have a region given by $$R = |{ax}|+|{by}| \le 1$$ and $$f(x,y) = \iint\limits_{R}{(ax-by)^2 \ \cdot \ (3ab^3+12a^3b-6a^3b^2) \ \cdot \ \sin^2({\pi ax + \pi by}})dxdy$$ I need to find the values of $a$ and $b$ that maximize $f$ and I have no idea where to start.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
52,Surface integral- getting different result using two methods,Surface integral- getting different result using two methods,,"I'm doing my homework and I came to conclusion I'm not sure is right. I need to find $$\iint_S x dydz+y^2dxdz+z^2dxdy$$ where $S$ is outer side of surface $x=z^2$, and $1\le y \le3$ and $x\le9$. Now, since for calculating first part of integral ($xdydz$) I need to project surface on plane $x=0$ I will have integral equal $0$. And I get same conclusion for two other parts of integral. Am I right? In the end integral equals $0$? UPDATE: But using divergence theorem I get: $I=\int_1^3 dy \int_{-3}^3 dz \int_{z^2}^9 (1+2y+2z) dx = 360$. I'm not sure which part I'm doing wrong?","I'm doing my homework and I came to conclusion I'm not sure is right. I need to find $$\iint_S x dydz+y^2dxdz+z^2dxdy$$ where $S$ is outer side of surface $x=z^2$, and $1\le y \le3$ and $x\le9$. Now, since for calculating first part of integral ($xdydz$) I need to project surface on plane $x=0$ I will have integral equal $0$. And I get same conclusion for two other parts of integral. Am I right? In the end integral equals $0$? UPDATE: But using divergence theorem I get: $I=\int_1^3 dy \int_{-3}^3 dz \int_{z^2}^9 (1+2y+2z) dx = 360$. I'm not sure which part I'm doing wrong?",,"['integration', 'multivariable-calculus']"
53,"Prove $f(x,y) = xy/(x^2 + y^2)$ is continuous everywhere except $(0,0).$",Prove  is continuous everywhere except,"f(x,y) = xy/(x^2 + y^2) (0,0).","I'd just like to ask you if my proof here is valid. I'll provide you with the method I used and if it seems ok let me know! If not, explanations would be helpful! My main approach to this question involves using the theorem that if functions $g$ and $h$ are both continuous, then their product and their sum are also continuous. I begun by splitting $f$ into $(g)(1/h)$, where $g(x,y) = xy$ and $h(x,y) = x^2 + y^2.$ Then, I again split $g$ into $2$ functions $p(x) = x$ and $q(x) = y$ and claimed that since they were both polynomials of order 1, they must be continuous everywhere. (Is it ok to just say this? It's a theorem in my book.) Hence, their product $xy$ must be continuous. Similarly, the sum of $x^2$ and $y^2$ (two polynomials of order $2$) must also be continuous. Now, we are left with $f(x,y) =$ the product of $g$ and the inverse of $h$. The inverse of $h$ is again continuous everywhere, except $(0,0)$ as $h(0,0)$ is undefined. Thus, $f(x,y)$ is continuous everywhere, except $(0,0)$. Thanks for reading!","I'd just like to ask you if my proof here is valid. I'll provide you with the method I used and if it seems ok let me know! If not, explanations would be helpful! My main approach to this question involves using the theorem that if functions $g$ and $h$ are both continuous, then their product and their sum are also continuous. I begun by splitting $f$ into $(g)(1/h)$, where $g(x,y) = xy$ and $h(x,y) = x^2 + y^2.$ Then, I again split $g$ into $2$ functions $p(x) = x$ and $q(x) = y$ and claimed that since they were both polynomials of order 1, they must be continuous everywhere. (Is it ok to just say this? It's a theorem in my book.) Hence, their product $xy$ must be continuous. Similarly, the sum of $x^2$ and $y^2$ (two polynomials of order $2$) must also be continuous. Now, we are left with $f(x,y) =$ the product of $g$ and the inverse of $h$. The inverse of $h$ is again continuous everywhere, except $(0,0)$ as $h(0,0)$ is undefined. Thus, $f(x,y)$ is continuous everywhere, except $(0,0)$. Thanks for reading!",,"['functions', 'multivariable-calculus']"
54,Calculate flux through a surface,Calculate flux through a surface,,"Part of the surface, S,  is: $z=x^2+y^2$ above the disk $ \ x^2+y^2 = 1 \ $ oriented in the $\vec k$ direction. I need to set up an integrated integral to calculate the flux of $\vec F = yz\vec i+xz\vec j-y^2\vec k$ through S. I am wanting to make sure I am setting up the flux integral properly before I begin to calculate it. $$\int_S \vec F \cdot dA = \int_S \vec F(x,y,f(x,y)) \cdot dA $$ First I found dA: $$dA = (-f_x\vec i-f_y\vec j+\vec k)d xd y=(-2x\vec i-2y\vec j+\vec k)d xd y$$ Then found $\vec F(x,y,f(x,y))$: $$\vec F(x,y,f(x,y)) = yz\vec i+xz\vec j-y^2\vec k=y(x^2+y^2)\vec i +x(x^2+y^2)\vec j-y^2\vec k$$ Then I changed to polar coordinates: $$dA=(-2r\cos\theta\vec i-2r\sin\theta\vec j+\vec k)rd rd \theta$$ $$\vec F(r,\theta)=r^3\sin\theta\vec i +r^3\cos\theta\vec j-r^2\sin^2\theta\vec k$$ Did the dot product of the two vectors obtaining: $$(-4r^4\cos\theta \sin\theta-r^2\sin^2\theta)$$ Thus,  $$\int_{\theta=0}^{\theta=2\pi}\int_{r=0}^{r=1} (-4r^4\cos\theta \sin\theta-r^2\sin^2\theta)r dr d\theta$$ Does this seem right? I'm working off the example in the book, and as usually not very helpful with intermediate steps.","Part of the surface, S,  is: $z=x^2+y^2$ above the disk $ \ x^2+y^2 = 1 \ $ oriented in the $\vec k$ direction. I need to set up an integrated integral to calculate the flux of $\vec F = yz\vec i+xz\vec j-y^2\vec k$ through S. I am wanting to make sure I am setting up the flux integral properly before I begin to calculate it. $$\int_S \vec F \cdot dA = \int_S \vec F(x,y,f(x,y)) \cdot dA $$ First I found dA: $$dA = (-f_x\vec i-f_y\vec j+\vec k)d xd y=(-2x\vec i-2y\vec j+\vec k)d xd y$$ Then found $\vec F(x,y,f(x,y))$: $$\vec F(x,y,f(x,y)) = yz\vec i+xz\vec j-y^2\vec k=y(x^2+y^2)\vec i +x(x^2+y^2)\vec j-y^2\vec k$$ Then I changed to polar coordinates: $$dA=(-2r\cos\theta\vec i-2r\sin\theta\vec j+\vec k)rd rd \theta$$ $$\vec F(r,\theta)=r^3\sin\theta\vec i +r^3\cos\theta\vec j-r^2\sin^2\theta\vec k$$ Did the dot product of the two vectors obtaining: $$(-4r^4\cos\theta \sin\theta-r^2\sin^2\theta)$$ Thus,  $$\int_{\theta=0}^{\theta=2\pi}\int_{r=0}^{r=1} (-4r^4\cos\theta \sin\theta-r^2\sin^2\theta)r dr d\theta$$ Does this seem right? I'm working off the example in the book, and as usually not very helpful with intermediate steps.",,['multivariable-calculus']
55,"Show that $\sum_{cyc} J(x,J(y,z))=0$.",Show that .,"\sum_{cyc} J(x,J(y,z))=0","Let $x,y,z$ be functions of $(u,v)$ and $J$ be the Jacobian matrix. Show that $\sum_{cyc} J(x,J(y,z))=0$ . I expanded the thing and realized that the first term in the sum is $x_u(J(y,z_v)+J(y_v,z))-x_v (J(y,z_u)+J(y_u,z))$ , but I don't know how to carry on then. Furthermore, I suspect that this has something to do with the jacobi identity , but I don't know are they related. Thanks.","Let be functions of and be the Jacobian matrix. Show that . I expanded the thing and realized that the first term in the sum is , but I don't know how to carry on then. Furthermore, I suspect that this has something to do with the jacobi identity , but I don't know are they related. Thanks.","x,y,z (u,v) J \sum_{cyc} J(x,J(y,z))=0 x_u(J(y,z_v)+J(y_v,z))-x_v (J(y,z_u)+J(y_u,z))","['multivariable-calculus', 'partial-derivative']"
56,"$\displaystyle f(xy,z-2x)=0$ satisfies $x \dfrac{\partial{z}}{\partial{x}}-y \dfrac{\partial{z}}{\partial{y}}=2x$",satisfies,"\displaystyle f(xy,z-2x)=0 x \dfrac{\partial{z}}{\partial{x}}-y \dfrac{\partial{z}}{\partial{y}}=2x","Show that $\displaystyle f(xy,z-2x)=0$ satisfies under certain conditions, the equation $x \dfrac{\partial{z}}{\partial{x}}-y \dfrac{\partial{z}}{\partial{y}}=2x$. What are these conditions? Attempt: $\beta=z-2x$ $x \dfrac{\partial{\beta}}{\partial{x}}=x \dfrac{\partial{z}}{\partial{x}}-2x$ $y \dfrac{\partial{\beta}}{\partial{y}}=y \dfrac{\partial{z}}{\partial{y}}$ using the given equation we get $x \dfrac{\partial{\beta}}{\partial{x}}=y \dfrac{\partial{\beta}}{\partial{y}}$","Show that $\displaystyle f(xy,z-2x)=0$ satisfies under certain conditions, the equation $x \dfrac{\partial{z}}{\partial{x}}-y \dfrac{\partial{z}}{\partial{y}}=2x$. What are these conditions? Attempt: $\beta=z-2x$ $x \dfrac{\partial{\beta}}{\partial{x}}=x \dfrac{\partial{z}}{\partial{x}}-2x$ $y \dfrac{\partial{\beta}}{\partial{y}}=y \dfrac{\partial{z}}{\partial{y}}$ using the given equation we get $x \dfrac{\partial{\beta}}{\partial{x}}=y \dfrac{\partial{\beta}}{\partial{y}}$",,"['calculus', 'multivariable-calculus']"
57,Multi-Variable Function continuously differentiable if partial derivatives exist continuosly,Multi-Variable Function continuously differentiable if partial derivatives exist continuosly,,"I am wondering about a detail in the proof of the equivalence of a function being continuously differentiable and having continuous partial derivatives. The theorem is found in Rudin's Principles of Mathematical analysis and goes as follows: Suppose $\textbf{f}$ maps an open set $E$ of $\mathbb{R}^n$ into $\mathbb{R}^m$. Then $\textbf{f}\in\mathcal{C}^1(E)$ if and only if the partial derivatives $D_jf_i$ exist and are continuous on $E$ for $1\leq i\leq m$ and $1\leq j\leq n$. For the ""if"" direction of the proof, he claims it suffices to consider the $m=1$ case. My question is why does that suffice?","I am wondering about a detail in the proof of the equivalence of a function being continuously differentiable and having continuous partial derivatives. The theorem is found in Rudin's Principles of Mathematical analysis and goes as follows: Suppose $\textbf{f}$ maps an open set $E$ of $\mathbb{R}^n$ into $\mathbb{R}^m$. Then $\textbf{f}\in\mathcal{C}^1(E)$ if and only if the partial derivatives $D_jf_i$ exist and are continuous on $E$ for $1\leq i\leq m$ and $1\leq j\leq n$. For the ""if"" direction of the proof, he claims it suffices to consider the $m=1$ case. My question is why does that suffice?",,"['multivariable-calculus', 'derivatives']"
58,Geometric Interpretation of Jacobi identity for cross product,Geometric Interpretation of Jacobi identity for cross product,,"Is there a geometric ""reason"" for the Jacobi identity for cross products? Some geometric equality of some area ...? All proofs I know work by some form of linear algebra (or use the interpretation as a Lie algebra ...)","Is there a geometric ""reason"" for the Jacobi identity for cross products? Some geometric equality of some area ...? All proofs I know work by some form of linear algebra (or use the interpretation as a Lie algebra ...)",,"['linear-algebra', 'multivariable-calculus', 'analytic-geometry', 'cross-product']"
59,Inverse function theorem: how show $F \in C^k \Rightarrow F^{-1} \in C^k$ with this method?,Inverse function theorem: how show  with this method?,F \in C^k \Rightarrow F^{-1} \in C^k,"I am reading the proof in Buck's Advanced Calculus of the inverse function theorem, on p. 359. The way he proves it is to show that $(DF)_{p_0}^{-1}$ satisfies $$F^{-1}(p_0 + h) - F^{-1}(p_0) = (DF)_{p_0}^{-1}(h) + o(h).$$ Therefore $(DF)_{p_0}^{-1}$ must be the differential of $F^{-1}$ at the point $p_0$, and since $(DF)^{-1}_p$ is a rational function of the entries of $(DF)_p$ (with denominator = the Jacobian, which is nonzero), $F^{-1}$ must be $C^1$, since $F$ is assumed to be $C^1$. My Question: This has only shown that $$F \in C^1 \Rightarrow F^{-1}\in C^1.$$ It has not shown the result that if $F$ is $C^n$, then $F^{-1}$ is $C^n$. If we want to maintain the structure of this proof, is it easy enough to get that extra result? Or is this stronger result more easily shown with another style of proof altogether? (I am aware there are other methods, based on the contraction principle, or a fixed point theorem.)","I am reading the proof in Buck's Advanced Calculus of the inverse function theorem, on p. 359. The way he proves it is to show that $(DF)_{p_0}^{-1}$ satisfies $$F^{-1}(p_0 + h) - F^{-1}(p_0) = (DF)_{p_0}^{-1}(h) + o(h).$$ Therefore $(DF)_{p_0}^{-1}$ must be the differential of $F^{-1}$ at the point $p_0$, and since $(DF)^{-1}_p$ is a rational function of the entries of $(DF)_p$ (with denominator = the Jacobian, which is nonzero), $F^{-1}$ must be $C^1$, since $F$ is assumed to be $C^1$. My Question: This has only shown that $$F \in C^1 \Rightarrow F^{-1}\in C^1.$$ It has not shown the result that if $F$ is $C^n$, then $F^{-1}$ is $C^n$. If we want to maintain the structure of this proof, is it easy enough to get that extra result? Or is this stronger result more easily shown with another style of proof altogether? (I am aware there are other methods, based on the contraction principle, or a fixed point theorem.)",,['multivariable-calculus']
60,"Given $a\in\mathbb{R}^2\backslash X$ and $v\in\mathbb{R}^2$, $\exists\delta$ such that $t\in[0,\delta) \Rightarrow a+tv\in \mathbb{R}^2\backslash X$.","Given  and ,  such that .","a\in\mathbb{R}^2\backslash X v\in\mathbb{R}^2 \exists\delta t\in[0,\delta) \Rightarrow a+tv\in \mathbb{R}^2\backslash X","Let $X=\{(x,y)\in\mathbb{R}^2;\;x>0 \;\text{and}\;x^2\leq y\leq2x^2\}$. Prove that for all $(a,v)\in(\mathbb{R^2}\backslash X)\times \mathbb{R}^2$ there exists $\delta>0$ such that $$0\leq t <\delta \Rightarrow a+tv\in \mathbb{R}^2\backslash X$$ This problem is in the section of differentiable functions of my book. Thanks.","Let $X=\{(x,y)\in\mathbb{R}^2;\;x>0 \;\text{and}\;x^2\leq y\leq2x^2\}$. Prove that for all $(a,v)\in(\mathbb{R^2}\backslash X)\times \mathbb{R}^2$ there exists $\delta>0$ such that $$0\leq t <\delta \Rightarrow a+tv\in \mathbb{R}^2\backslash X$$ This problem is in the section of differentiable functions of my book. Thanks.",,['multivariable-calculus']
61,change of variables using a substitution,change of variables using a substitution,,"Let $D$ be the triangle with vertices $(0,0),(1,0)$ and $(0,1)$. Evaluate $$\iint_D \exp\left( \frac{y-x}{y+x} \right) \,dx\,dy$$  by making the substitution $u=y-x$ and $v=y+x$ My attempt Finding the domain, $D$ $$ (x,y) \rightarrow(u,v) $$ $$ (0,0) \rightarrow(0,0) $$ $$ (0,1) \rightarrow(1,1) $$$$ (1,0) \rightarrow(-1,1) $$ Thus the domain is $-1 \le u \le1$ and $0 \le v \le1$ and the jacobian is $-2$ and so the integral I get is $$ \int_0^1 \int_{-1}^1 -2e^{\frac{u}{v}} \,du\,dv   $$ this is awfully too hard to integrate and i can't figure out where i have gone wrong","Let $D$ be the triangle with vertices $(0,0),(1,0)$ and $(0,1)$. Evaluate $$\iint_D \exp\left( \frac{y-x}{y+x} \right) \,dx\,dy$$  by making the substitution $u=y-x$ and $v=y+x$ My attempt Finding the domain, $D$ $$ (x,y) \rightarrow(u,v) $$ $$ (0,0) \rightarrow(0,0) $$ $$ (0,1) \rightarrow(1,1) $$$$ (1,0) \rightarrow(-1,1) $$ Thus the domain is $-1 \le u \le1$ and $0 \le v \le1$ and the jacobian is $-2$ and so the integral I get is $$ \int_0^1 \int_{-1}^1 -2e^{\frac{u}{v}} \,du\,dv   $$ this is awfully too hard to integrate and i can't figure out where i have gone wrong",,"['integration', 'multivariable-calculus']"
62,"curl of what yields $(0,s^{-1},0)$ in cylindrical coordinates?",curl of what yields  in cylindrical coordinates?,"(0,s^{-1},0)","In cylindrical coordinates $(s,\theta,z)$, what function $\mathbf{A}$ has the property $$\nabla\times \mathbf{A} = (0, \frac{1}{s} , 0) $$ I know generally that $$\nabla\times \mathbf{A} = \left(\frac{1}{s}\frac{\partial A_3}{\partial \theta}-\frac{\partial A_2}{\partial z}, \frac{\partial A_1}{\partial z}-\frac{\partial A_3}{\partial s} , \frac{1}{s} \left( \frac{\partial}{\partial z}(s\cdot A_2) - \frac{\partial A_1}{\partial \theta}\right)\right) $$ Is there some better way of solving for $\mathbf{A}$ other than a lot of ugly equations?","In cylindrical coordinates $(s,\theta,z)$, what function $\mathbf{A}$ has the property $$\nabla\times \mathbf{A} = (0, \frac{1}{s} , 0) $$ I know generally that $$\nabla\times \mathbf{A} = \left(\frac{1}{s}\frac{\partial A_3}{\partial \theta}-\frac{\partial A_2}{\partial z}, \frac{\partial A_1}{\partial z}-\frac{\partial A_3}{\partial s} , \frac{1}{s} \left( \frac{\partial}{\partial z}(s\cdot A_2) - \frac{\partial A_1}{\partial \theta}\right)\right) $$ Is there some better way of solving for $\mathbf{A}$ other than a lot of ugly equations?",,"['multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
63,Can I prove continuity of a function of two variables in this way?,Can I prove continuity of a function of two variables in this way?,,"Common approach in handling functions of two variables is to express this function in polar coordinate system. For example, in the classic example $$f(x,y)=\left\{\begin{array}{lr}\frac{xy}{x^2+y^2} & (x,y)\neq (0,0)\\0 & \text{otherwise}\end{array}\right.$$ say $x=r\cos(\phi)$ and $y=\sin(\phi)$, and then we can write $f(r, \phi)=\frac{r^2\sin(\phi)\cos(\phi)}{r^2(\cos^2(\phi) + \sin^2(\phi))}=\frac{1}{2}\sin(2\phi)$. Now if we let $r\rightarrow 0$, then it and, for example, $\phi=\frac{\pi}{4}$, the $$\lim_{r\rightarrow 0}f(r, \frac{\pi}{4})=\frac{1}{2}\neq 0$$ so $f$ isn't continuous in $(0,0)$. Now I am given a function $$f(x,y)=(x+y)\exp(xy)$$Since this function is composition of continuous functions, it must be continuous, but I wanted to ask in common: earlier I used polar coordinates to tell that the function isn't continuous, so can I use the same approach to say that function is continuous, that is: $$\lim_{r\rightarrow0}f(r,\phi)=\lim_{r\rightarrow0}r(\cos(\phi)+\sin(\phi))\exp(r^2\sin(\phi)\cos(\phi))=0$$ so function must be continuous, or is it not enough? Thank you in advance!","Common approach in handling functions of two variables is to express this function in polar coordinate system. For example, in the classic example $$f(x,y)=\left\{\begin{array}{lr}\frac{xy}{x^2+y^2} & (x,y)\neq (0,0)\\0 & \text{otherwise}\end{array}\right.$$ say $x=r\cos(\phi)$ and $y=\sin(\phi)$, and then we can write $f(r, \phi)=\frac{r^2\sin(\phi)\cos(\phi)}{r^2(\cos^2(\phi) + \sin^2(\phi))}=\frac{1}{2}\sin(2\phi)$. Now if we let $r\rightarrow 0$, then it and, for example, $\phi=\frac{\pi}{4}$, the $$\lim_{r\rightarrow 0}f(r, \frac{\pi}{4})=\frac{1}{2}\neq 0$$ so $f$ isn't continuous in $(0,0)$. Now I am given a function $$f(x,y)=(x+y)\exp(xy)$$Since this function is composition of continuous functions, it must be continuous, but I wanted to ask in common: earlier I used polar coordinates to tell that the function isn't continuous, so can I use the same approach to say that function is continuous, that is: $$\lim_{r\rightarrow0}f(r,\phi)=\lim_{r\rightarrow0}r(\cos(\phi)+\sin(\phi))\exp(r^2\sin(\phi)\cos(\phi))=0$$ so function must be continuous, or is it not enough? Thank you in advance!",,['multivariable-calculus']
64,Simple Partial Derivative Chain Rule Question,Simple Partial Derivative Chain Rule Question,,"Say $f: \mathbb{R}^n \rightarrow \mathbb{R}$ by some $f(x_1,x_2,\cdots,x_n)$. Further suppose that each $x_i: \mathbb{R}^m \rightarrow \mathbb{R}$ by some $x_i=x_i(\eta_1,\eta_2,\cdots,\eta_m)$. Does it then follow that $$\frac{\partial f}{\partial \eta_i} = \sum_{k=1}^{n} \frac{\partial f}{\partial x_k}\frac{\partial x_k}{\partial \eta_i},$$ assuming all of $\frac{\partial f}{\partial x_k}$ and $\frac{\partial x_k}{\partial \eta_i}$ exist on some subset of $\mathbb{R}^n$ and $\mathbb{R}^m$ respectively? I don't have much formal education in way of partial derivatives and aside from the fact of treating variables that we aren't taking the derivative with respect to to be constants. Really, my goal is to be able to successfully take some PDE like: $$u_{xx}+u_{yy}=0$$ with $u=u(x,y)$ and doing some coordinate change $\xi = x+y$ and $\eta=x-y$ and doing the change of variable to the homogenous equation. What do people call this? Doing a change of variable to the PDE or equation? If this is correct I will be able to move forward in at least being able to do the chain rule I need to for my class.","Say $f: \mathbb{R}^n \rightarrow \mathbb{R}$ by some $f(x_1,x_2,\cdots,x_n)$. Further suppose that each $x_i: \mathbb{R}^m \rightarrow \mathbb{R}$ by some $x_i=x_i(\eta_1,\eta_2,\cdots,\eta_m)$. Does it then follow that $$\frac{\partial f}{\partial \eta_i} = \sum_{k=1}^{n} \frac{\partial f}{\partial x_k}\frac{\partial x_k}{\partial \eta_i},$$ assuming all of $\frac{\partial f}{\partial x_k}$ and $\frac{\partial x_k}{\partial \eta_i}$ exist on some subset of $\mathbb{R}^n$ and $\mathbb{R}^m$ respectively? I don't have much formal education in way of partial derivatives and aside from the fact of treating variables that we aren't taking the derivative with respect to to be constants. Really, my goal is to be able to successfully take some PDE like: $$u_{xx}+u_{yy}=0$$ with $u=u(x,y)$ and doing some coordinate change $\xi = x+y$ and $\eta=x-y$ and doing the change of variable to the homogenous equation. What do people call this? Doing a change of variable to the PDE or equation? If this is correct I will be able to move forward in at least being able to do the chain rule I need to for my class.",,['multivariable-calculus']
65,How to Handle Two-Center Bipolar Coordinates?,How to Handle Two-Center Bipolar Coordinates?,,"In my problem, I want to integrate a $2D$ function $f(x,y)$ which explicitly depends on the vector $ \vec{r}_1=\vec{r}-\vec{R}_1 $ and $\vec{r}_2=\vec{r}-\vec{R}_2$, where $\vec{R}_1=(a,0)$ and $\vec{R}_2=(-a,0)$ are two fixed points. I found that the two-center bipolar coordinate maybe helpful which you can find some information about it here in Wikipedia . However, I am not clear how to apply the integration exactly. The question is general (and maybe stupid): Since the two coordinate $r_1$ and $r_2$ are not orthogonal, would there be any problem when doing the change of variables $(x,y)$ to $(r_1,r_2)$? Furthermore, how to express differential operators, like gradient, curl, in the new coordinates? Any useful reference are welcome.","In my problem, I want to integrate a $2D$ function $f(x,y)$ which explicitly depends on the vector $ \vec{r}_1=\vec{r}-\vec{R}_1 $ and $\vec{r}_2=\vec{r}-\vec{R}_2$, where $\vec{R}_1=(a,0)$ and $\vec{R}_2=(-a,0)$ are two fixed points. I found that the two-center bipolar coordinate maybe helpful which you can find some information about it here in Wikipedia . However, I am not clear how to apply the integration exactly. The question is general (and maybe stupid): Since the two coordinate $r_1$ and $r_2$ are not orthogonal, would there be any problem when doing the change of variables $(x,y)$ to $(r_1,r_2)$? Furthermore, how to express differential operators, like gradient, curl, in the new coordinates? Any useful reference are welcome.",,"['integration', 'multivariable-calculus', 'coordinate-systems']"
66,"Why isn't the union of the coordinate axes a manifold, using the formal definition of a manifold?","Why isn't the union of the coordinate axes a manifold, using the formal definition of a manifold?",,"Let me call $M$ a manifold iff locally it is given as the graph of a $C^1$ function. Then without appealing to ""remove a point"" type arguments, why isn't the union of the $x$ and $y$ coordinate axes (the solution set of $xy=0$) a manifold? That is, why isn't this locally a graph around the origin?","Let me call $M$ a manifold iff locally it is given as the graph of a $C^1$ function. Then without appealing to ""remove a point"" type arguments, why isn't the union of the $x$ and $y$ coordinate axes (the solution set of $xy=0$) a manifold? That is, why isn't this locally a graph around the origin?",,['multivariable-calculus']
67,Parametric curve of intersection - line integral with respect to arc length,Parametric curve of intersection - line integral with respect to arc length,,"This comes from Apostol's Calculus, Vol. II, Section 10.9 #14: A uniform wire has the shape of that portion of the curve of intersecion of the two surfaces $x^2+y^2=z^2$ and $y^2=x$ connecting the points $(0,0,0)$ and $(1,1,\sqrt 2)$. Find the $z$-coordinate of its centroid. The $z$-coordinate of the centroid is defined as $\dfrac {\int_C z\, \mathrm ds}{\int_C \mathrm ds}$, where $s(t)$ is the arc length. (That is, if $\vec\alpha(t)$ is a parametrization of $C$, $s(t)=\int \lVert \vec \alpha \,' (t) \rVert \mathrm d t$.) One valid parametrization of the curve is $$\vec \alpha(t)=\left(t^2,\, t,\, t\sqrt {t^2+1}\right)\,,$$ however this becomes quite difficult to work with. In fact, Mathematica can only numerically integrate $\int_C \mathrm ds= \int_0^1 \lVert \vec \alpha\, '(t) \rVert\, \mathrm dt$. I thought perhaps using a substitution like $t=\tan(\theta)$ would be helpful, but obviously if this was viable Mathematica would have performed this simple substitution in the first place. My question is, specifically for this problem, is there a nicer parametrization that I am missing? Hints appreciated . I have tried solving $x$ and $y$ in terms of $z$ and using the resulting parametrization, but it is no better. In case it is helpful, the book's answer is $$\frac {600-36\sqrt 2 - 49 \log(9-4\sqrt 2)}{64[6\sqrt 2 + \log (3+2\sqrt 2)]}\approx0.747018$$  While Mathematica returns $0.710276$ using my parametrization, so perhaps there is a mistake (either with my parametrization, the answer in the book, or the question in the book).","This comes from Apostol's Calculus, Vol. II, Section 10.9 #14: A uniform wire has the shape of that portion of the curve of intersecion of the two surfaces $x^2+y^2=z^2$ and $y^2=x$ connecting the points $(0,0,0)$ and $(1,1,\sqrt 2)$. Find the $z$-coordinate of its centroid. The $z$-coordinate of the centroid is defined as $\dfrac {\int_C z\, \mathrm ds}{\int_C \mathrm ds}$, where $s(t)$ is the arc length. (That is, if $\vec\alpha(t)$ is a parametrization of $C$, $s(t)=\int \lVert \vec \alpha \,' (t) \rVert \mathrm d t$.) One valid parametrization of the curve is $$\vec \alpha(t)=\left(t^2,\, t,\, t\sqrt {t^2+1}\right)\,,$$ however this becomes quite difficult to work with. In fact, Mathematica can only numerically integrate $\int_C \mathrm ds= \int_0^1 \lVert \vec \alpha\, '(t) \rVert\, \mathrm dt$. I thought perhaps using a substitution like $t=\tan(\theta)$ would be helpful, but obviously if this was viable Mathematica would have performed this simple substitution in the first place. My question is, specifically for this problem, is there a nicer parametrization that I am missing? Hints appreciated . I have tried solving $x$ and $y$ in terms of $z$ and using the resulting parametrization, but it is no better. In case it is helpful, the book's answer is $$\frac {600-36\sqrt 2 - 49 \log(9-4\sqrt 2)}{64[6\sqrt 2 + \log (3+2\sqrt 2)]}\approx0.747018$$  While Mathematica returns $0.710276$ using my parametrization, so perhaps there is a mistake (either with my parametrization, the answer in the book, or the question in the book).",,"['multivariable-calculus', 'physics', 'vector-analysis']"
68,What is the volume of this ring-like solid?,What is the volume of this ring-like solid?,,"We're learning about triple integrals and such in class. Here's one of the problems I'm working on: A cylindrical drill with radius 3 is used to bore a hole through the center of a sphere of radius 5. Find the volume of the ring shaped solid that remains. Now here's what I'm thinking: Triple integrate this: $r \,dr\, d\theta\, dz$ Bounds for $r$: $3$ to $5$ Bounds for $\theta$: $0$ to $2\pi$ Bounds for $z$ : ...$0$ to $5$? (times $2$? I mean, it's $-5$ to $5$...) However, maybe the bounds for $r$ should depend on $z$. That would make sense... right? I wish I knew when to have the bounds depend on another variable and such. I think this one would though, because the ""radius"" would shrink as you increased (or decreased) z. By how much though? When $z$ is $0$, $r$ is $5$. When $z$ is $1$, $r$ is $5$, so the new distance from the $z$-axis would be $\sqrt{1 + 5^2}$. Right...? I'd sure appreciate some pointers! I'll respond as quickly as possible.","We're learning about triple integrals and such in class. Here's one of the problems I'm working on: A cylindrical drill with radius 3 is used to bore a hole through the center of a sphere of radius 5. Find the volume of the ring shaped solid that remains. Now here's what I'm thinking: Triple integrate this: $r \,dr\, d\theta\, dz$ Bounds for $r$: $3$ to $5$ Bounds for $\theta$: $0$ to $2\pi$ Bounds for $z$ : ...$0$ to $5$? (times $2$? I mean, it's $-5$ to $5$...) However, maybe the bounds for $r$ should depend on $z$. That would make sense... right? I wish I knew when to have the bounds depend on another variable and such. I think this one would though, because the ""radius"" would shrink as you increased (or decreased) z. By how much though? When $z$ is $0$, $r$ is $5$. When $z$ is $1$, $r$ is $5$, so the new distance from the $z$-axis would be $\sqrt{1 + 5^2}$. Right...? I'd sure appreciate some pointers! I'll respond as quickly as possible.",,['multivariable-calculus']
69,evaluate the volume of solid,evaluate the volume of solid,,"Consider the paraboloid $(\mathcal{P}): z=x^2+y^2$ and the plane $(\mathcal{Q}): 2x+2y+z=2$ . Let $\mathcal{S}$ be the solid region bounded above by $(\mathcal{Q})$ and below by $(\mathcal{P})$ . Find the volume of the solid region $\mathcal{S}$ . After sketching the plane will cut the paraboloid and obtain the shadow in the $xy$ -plane as a circle of equation $$ R: (x+1)^2+(y+1)^2=4. $$ Now to find the volume, I travel first in the $z$ -direction and get $$ V(\mathcal{S})=\iint_{R}\int_{x^2+y^2}^{2-2x-2y}dz dA=\iint_{R}\left(2-2x-2y-x^2-y^2\right)dA. $$ Now to evaluate the double integral over the shadow $R$ , we may use cartesian coordinates and try to evaluate it, but the calculation are very hard. Is it possible to find the volume for example using cylindrical or spherical coordinates? How we can do it? Based on the comment below, $r^2=(x+1)^2+(y+1)^2$ , the volume could be $$ V(\mathcal{S})=\int_{0}^{2\pi} \int_{0}^2\left[-r^2+4\right]r dr d\theta. $$ Is it correct in this way to solve it?","Consider the paraboloid and the plane . Let be the solid region bounded above by and below by . Find the volume of the solid region . After sketching the plane will cut the paraboloid and obtain the shadow in the -plane as a circle of equation Now to find the volume, I travel first in the -direction and get Now to evaluate the double integral over the shadow , we may use cartesian coordinates and try to evaluate it, but the calculation are very hard. Is it possible to find the volume for example using cylindrical or spherical coordinates? How we can do it? Based on the comment below, , the volume could be Is it correct in this way to solve it?","(\mathcal{P}): z=x^2+y^2 (\mathcal{Q}): 2x+2y+z=2 \mathcal{S} (\mathcal{Q}) (\mathcal{P}) \mathcal{S} xy 
R: (x+1)^2+(y+1)^2=4.
 z 
V(\mathcal{S})=\iint_{R}\int_{x^2+y^2}^{2-2x-2y}dz dA=\iint_{R}\left(2-2x-2y-x^2-y^2\right)dA.
 R r^2=(x+1)^2+(y+1)^2 
V(\mathcal{S})=\int_{0}^{2\pi} \int_{0}^2\left[-r^2+4\right]r dr d\theta.
","['integration', 'multivariable-calculus', 'multiple-integral', 'spherical-coordinates', 'cylindrical-coordinates']"
70,Seeking Efficient Solution Strategies for a Triple Integral,Seeking Efficient Solution Strategies for a Triple Integral,,"I'm attempting to evaluate the following triple integral: $$ \int_{-1}^1 \int_{-1}^1 \int_0^{\frac{4}{\sqrt{2}}\big(1+yz-|y+z|\big)} \sqrt{2x^2+(y-z)^2} \, dx \, dy \, dz $$ I initially attempted a direct solution, and while the inner integral has an antiderivative, the upper limit poses challenges due to its complexity. Integrating the resulting expression with this limit inflates the integral. Subsequently, I explored a suitable transformation but haven't identified an effective one. I'm seeking guidance on potential strategies or clever approaches to solve this integral more efficiently. Your insights and assistance would be greatly appreciated.","I'm attempting to evaluate the following triple integral: I initially attempted a direct solution, and while the inner integral has an antiderivative, the upper limit poses challenges due to its complexity. Integrating the resulting expression with this limit inflates the integral. Subsequently, I explored a suitable transformation but haven't identified an effective one. I'm seeking guidance on potential strategies or clever approaches to solve this integral more efficiently. Your insights and assistance would be greatly appreciated.","
\int_{-1}^1 \int_{-1}^1 \int_0^{\frac{4}{\sqrt{2}}\big(1+yz-|y+z|\big)} \sqrt{2x^2+(y-z)^2} \, dx \, dy \, dz
","['integration', 'multivariable-calculus', 'definite-integrals']"
71,What are the prerequisites to study Calculus on Manifolds.,What are the prerequisites to study Calculus on Manifolds.,,"I'm a bachelor degree math student who is interested in differential geometry and topology and doing my way through it. Recently I discovered the world of Calculus on Manifolds and I wish to know what are the prerequisists to studying it. I know that, besides a lot of Topology, Analysis and Linear Algebra, I need also some of Tensor Algebra (I don't know if it's the correct therm for it) and learning about the Wedge Product, Exterior Derivative and etc. However, I don't really know to where to get started. I want to know if there is a recomendation to what subjects to studied first and some good references. I apologize if the english isn't proper or it was already been asked before.","I'm a bachelor degree math student who is interested in differential geometry and topology and doing my way through it. Recently I discovered the world of Calculus on Manifolds and I wish to know what are the prerequisists to studying it. I know that, besides a lot of Topology, Analysis and Linear Algebra, I need also some of Tensor Algebra (I don't know if it's the correct therm for it) and learning about the Wedge Product, Exterior Derivative and etc. However, I don't really know to where to get started. I want to know if there is a recomendation to what subjects to studied first and some good references. I apologize if the english isn't proper or it was already been asked before.",,"['multivariable-calculus', 'reference-request', 'manifolds', 'differential-topology']"
72,Finding Lamé coefficients (scale factors) for the curves,Finding Lamé coefficients (scale factors) for the curves,,"I am having trouble finding the Lamé coefficients for the following curves. I have specified all the necessary theory, however I am not sure how to practically compute it. $$ \begin{cases}  x = \frac{1}{2}(v_1^2 - v_2^2), & v_1 = \text{const} \\  y = v_1v_2, & v_2 = \text{const}  \end{cases} $$ Here is some background: A point $P$ in $3$ -D space (or its $\mathbf{r}$ ) can be defined using Cartesian coordinates $(x,y,z)$ by $\mathbf{r} = x \mathbf{e}_x + y\mathbf{e}_y + z\mathbf{e}_z$ , where $\mathbf{e}_x,\mathbf{e}_y, \mathbf{e}_z$ are the standard basis vectors. In the Cartesian system, the standard basis vectors can be derived from the derivative of the location of point $P$ with respect to the local coordinate $$ \mathbf{e}_{x} = \frac{\partial\mathbf{r}}{\partial x}; \quad \mathbf{e}_{y} = \frac{\partial\mathbf{r}}{\partial y}; \quad  \mathbf{e}_{z} = \frac{\partial\mathbf{r}}{\partial z}. $$ Applying the same derivatives to the curvilinear system locally at point $P$ defines the natural basis vectors: $$ \mathbf{h}_1 = \frac{\partial\mathbf{r}}{\partial q^1}; \quad  \mathbf{h}_2 = \frac{\partial\mathbf{r}}{\partial q^2}; \quad  \mathbf{h}_3 = \frac{\partial\mathbf{r}}{\partial q^3}. $$ we define the Lamé coefficients (after Gabriel Lamé) by $$ h_1 = |\mathbf{h}_1|; \quad  h_2 = |\mathbf{h}_2|; \quad  h_3 = |\mathbf{h}_3|. $$ Additional note: Lamé  coefficients are also known as scale factors.","I am having trouble finding the Lamé coefficients for the following curves. I have specified all the necessary theory, however I am not sure how to practically compute it. Here is some background: A point in -D space (or its ) can be defined using Cartesian coordinates by , where are the standard basis vectors. In the Cartesian system, the standard basis vectors can be derived from the derivative of the location of point with respect to the local coordinate Applying the same derivatives to the curvilinear system locally at point defines the natural basis vectors: we define the Lamé coefficients (after Gabriel Lamé) by Additional note: Lamé  coefficients are also known as scale factors.","
\begin{cases} 
x = \frac{1}{2}(v_1^2 - v_2^2), & v_1 = \text{const} \\ 
y = v_1v_2, & v_2 = \text{const} 
\end{cases}
 P 3 \mathbf{r} (x,y,z) \mathbf{r} = x \mathbf{e}_x + y\mathbf{e}_y + z\mathbf{e}_z \mathbf{e}_x,\mathbf{e}_y, \mathbf{e}_z P 
\mathbf{e}_{x} = \frac{\partial\mathbf{r}}{\partial x}; \quad
\mathbf{e}_{y} = \frac{\partial\mathbf{r}}{\partial y}; \quad 
\mathbf{e}_{z} = \frac{\partial\mathbf{r}}{\partial z}.
 P 
\mathbf{h}_1 = \frac{\partial\mathbf{r}}{\partial q^1}; \quad 
\mathbf{h}_2 = \frac{\partial\mathbf{r}}{\partial q^2}; \quad 
\mathbf{h}_3 = \frac{\partial\mathbf{r}}{\partial q^3}.
 
h_1 = |\mathbf{h}_1|; \quad 
h_2 = |\mathbf{h}_2|; \quad 
h_3 = |\mathbf{h}_3|.
","['calculus', 'multivariable-calculus', 'systems-of-equations', 'coordinate-systems', 'curvilinear-coordinates']"
73,"Am I using the condition ""$f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$"" correctly (Calculus on Manifolds by Michael Spivak)","Am I using the condition "" is bounded in some open set around each point of "" correctly (Calculus on Manifolds by Michael Spivak)",f:A\to\mathbb{R} A,"I am reading ""Calculus on Manifolds"" by Michael Spivak. An open cover $\mathcal{O}$ of an open set $A\subset\mathbb{R}^n$ is admissible if each $U\in\mathcal{O}$ is contained in $A$ . If $\Phi$ is subordinate to $\mathcal{O}$ , $f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$ , $\{x:f\text{ is discontinuous at }x\}$ has measure $0$ , then each $\int_A \varphi\cdot |f|$ exists. The author wrote $\int_A \varphi\cdot |f|$ exists without a proof. So I forced to check $\int_A \varphi\cdot |f|$ exists. At first, it was necessary to consider why the above conditions are required. One of the above conditions is "" $f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$ "". I am new to such a property of functions. So, I want you to check if I am using the condition "" $f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$ "" correctly. I used the following theorems to check $\int_A \varphi\cdot |f|$ exists. 3-8 Theorem. Let $A$ be a closed rectangle and $f:A\to\mathbb{R}$ a bounded function. Let $B=\{x:f\text{ is not continuous at }x\}$ . Then $f$ is integrable if and only if $B$ is a set of measure $0$ . 3-11 Theorem. Let $A\subset \Bbb R^n$ and let $\mathcal{O}$ be an open cover of $A$ . Then there is a collection $\Phi$ of $C^\infty$ functions $\varphi$ defined in an open set containing $A$ , with the following properties: (1). For each $x \in A$ we have $0 \leq \varphi(x) \leq 1$ . (2). For each $x \in A$ there is an open set $V$ containing $x$ such that all but finitely many $\varphi \in \Phi$ are $0$ on $V$ . (3). For each $x \in A$ we have $\sum_{\varphi \in \Phi}\varphi(x)=1$ (by (2) for each $x$ their sum is finite in some open set containing $x$ ). (4). For each $\varphi \in \Phi$ there is an open set $U$ in $\mathcal{O}$ such that $\varphi = 0$ outside of some closed set contained in $U$ . (A collection $\Phi$ satisfying (1) to (3) is called a $C^\infty$ partiion of unity for $A$ . If $\Phi$ also satisfies (4), it is said to be subordinate to the cover $\mathcal{O}$ . In this chapter we will only use continuity of the functions $\varphi$ .) Even if we change ""closed"" in (4) with ""compact"", Theorem 3-11 still holds. About this, please see https://math.stackexchange.com/a/243671/1226161 . Let $\varphi\in\Phi$ . Then by (4), there is an open set $U\in\mathcal{O}$ such that $\varphi=0$ outside of some compact set $C$ contained in $U$ . Since $\mathcal{O}$ is admissible, $C\subset U\subset A$ . Since $\varphi$ is continuous, $\varphi(x)=0$ for $x$ on the boundary of $C$ . Let $a$ be an any point on the boundary of $C$ . For any positive $\varepsilon$ , there exists a neighborhood $V_1$ of $a$ such that $|\varphi(x)|=|\varphi(x)-\varphi(a)|<\varepsilon$ for any $x\in V_1$ since $\varphi$ is continuous at $a$ and $\varphi(a)=0$ . For any positive $\varepsilon$ , there exists a neighborhood $V_2$ of $a$ and $M$ such that $||f|(x)|<M$ for any $x\in V_2$ since $|f|$ is bounded in some open set around each point of $A$ . Then, $|\varphi\cdot |f|(x)-\varphi\cdot |f|(a)|=|\varphi(x) |f|(x)|=|\varphi(x)|\cdot |f(x)|<\varepsilon\cdot M$ for any $x$ in some neighborhoood of $a$ . So, $\varphi\cdot |f|$ is continuous at each $a$ on the boundary of $C$ . Since $\varphi\cdot |f|(x) = 0$ if $x\in A-C$ , so $\int_A \varphi\cdot |f|=\int_C \varphi\cdot |f|$ . Let $B$ be a closed rectangle such that $C\subset B$ . Let $g:B\to\mathbb{R}$ be a function such that $g(x) = \varphi\cdot |f|(x)$ if $x\in C$ and $g(x) = 0$ if $x\in B-C$ . Then, $g$ is continuous at each point $a$ in $B-C$ and $g$ is continuous at each point $a$ on the boundary of $C$ and $\{x\in C:\varphi\cdot |f|\text{ is discontinuous at }x\}\subset \{x\in A:f\text{ is discontinuous at }x\}$ has measure $0$ . So, $\int_C \varphi\cdot |f|$ exists. So, $\int_A \varphi\cdot |f|$ exists. I wrote $\int_A \varphi\cdot |f|$ as if I knew what $\int_A \varphi\cdot |f|$ means. But we don't know what $\int_A \varphi\cdot |f|$ means if $A$ is not bounded. So, I want to defne as follows: An open cover $\mathcal{O}$ of an open set $A\subset\mathbb{R}^n$ is admissible if each $U\in\mathcal{O}$ is contained in $A$ . Let $\mathcal{O}$ be admissible. Let $\Phi$ be subordinate to $\mathcal{O}$ . By (4) in Theorem 3-11, there is an open set $U\in\mathcal{O}$ such that $\varphi=0$ outside of some compact set $C_{\varphi}$ contained in $U$ . Since $\mathcal{O}$ is admissible, $C_{\varphi}\subset U\subset A$ . If $f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$ , $\{x:f\text{ is discontinuous at }x\}$ has measure $0$ , then each $\int_{C_{\varphi}} \varphi\cdot |f|$ exists. The value of $\int_{C_{\varphi}} \varphi\cdot |f|$ does not depend on the choice of $C_{\varphi}$ . We define $\int_A \varphi\cdot |f|:=\int_{C_{\varphi}} \varphi\cdot |f|$ .","I am reading ""Calculus on Manifolds"" by Michael Spivak. An open cover of an open set is admissible if each is contained in . If is subordinate to , is bounded in some open set around each point of , has measure , then each exists. The author wrote exists without a proof. So I forced to check exists. At first, it was necessary to consider why the above conditions are required. One of the above conditions is "" is bounded in some open set around each point of "". I am new to such a property of functions. So, I want you to check if I am using the condition "" is bounded in some open set around each point of "" correctly. I used the following theorems to check exists. 3-8 Theorem. Let be a closed rectangle and a bounded function. Let . Then is integrable if and only if is a set of measure . 3-11 Theorem. Let and let be an open cover of . Then there is a collection of functions defined in an open set containing , with the following properties: (1). For each we have . (2). For each there is an open set containing such that all but finitely many are on . (3). For each we have (by (2) for each their sum is finite in some open set containing ). (4). For each there is an open set in such that outside of some closed set contained in . (A collection satisfying (1) to (3) is called a partiion of unity for . If also satisfies (4), it is said to be subordinate to the cover . In this chapter we will only use continuity of the functions .) Even if we change ""closed"" in (4) with ""compact"", Theorem 3-11 still holds. About this, please see https://math.stackexchange.com/a/243671/1226161 . Let . Then by (4), there is an open set such that outside of some compact set contained in . Since is admissible, . Since is continuous, for on the boundary of . Let be an any point on the boundary of . For any positive , there exists a neighborhood of such that for any since is continuous at and . For any positive , there exists a neighborhood of and such that for any since is bounded in some open set around each point of . Then, for any in some neighborhoood of . So, is continuous at each on the boundary of . Since if , so . Let be a closed rectangle such that . Let be a function such that if and if . Then, is continuous at each point in and is continuous at each point on the boundary of and has measure . So, exists. So, exists. I wrote as if I knew what means. But we don't know what means if is not bounded. So, I want to defne as follows: An open cover of an open set is admissible if each is contained in . Let be admissible. Let be subordinate to . By (4) in Theorem 3-11, there is an open set such that outside of some compact set contained in . Since is admissible, . If is bounded in some open set around each point of , has measure , then each exists. The value of does not depend on the choice of . We define .",\mathcal{O} A\subset\mathbb{R}^n U\in\mathcal{O} A \Phi \mathcal{O} f:A\to\mathbb{R} A \{x:f\text{ is discontinuous at }x\} 0 \int_A \varphi\cdot |f| \int_A \varphi\cdot |f| \int_A \varphi\cdot |f| f:A\to\mathbb{R} A f:A\to\mathbb{R} A \int_A \varphi\cdot |f| A f:A\to\mathbb{R} B=\{x:f\text{ is not continuous at }x\} f B 0 A\subset \Bbb R^n \mathcal{O} A \Phi C^\infty \varphi A x \in A 0 \leq \varphi(x) \leq 1 x \in A V x \varphi \in \Phi 0 V x \in A \sum_{\varphi \in \Phi}\varphi(x)=1 x x \varphi \in \Phi U \mathcal{O} \varphi = 0 U \Phi C^\infty A \Phi \mathcal{O} \varphi \varphi\in\Phi U\in\mathcal{O} \varphi=0 C U \mathcal{O} C\subset U\subset A \varphi \varphi(x)=0 x C a C \varepsilon V_1 a |\varphi(x)|=|\varphi(x)-\varphi(a)|<\varepsilon x\in V_1 \varphi a \varphi(a)=0 \varepsilon V_2 a M ||f|(x)|<M x\in V_2 |f| A |\varphi\cdot |f|(x)-\varphi\cdot |f|(a)|=|\varphi(x) |f|(x)|=|\varphi(x)|\cdot |f(x)|<\varepsilon\cdot M x a \varphi\cdot |f| a C \varphi\cdot |f|(x) = 0 x\in A-C \int_A \varphi\cdot |f|=\int_C \varphi\cdot |f| B C\subset B g:B\to\mathbb{R} g(x) = \varphi\cdot |f|(x) x\in C g(x) = 0 x\in B-C g a B-C g a C \{x\in C:\varphi\cdot |f|\text{ is discontinuous at }x\}\subset \{x\in A:f\text{ is discontinuous at }x\} 0 \int_C \varphi\cdot |f| \int_A \varphi\cdot |f| \int_A \varphi\cdot |f| \int_A \varphi\cdot |f| \int_A \varphi\cdot |f| A \mathcal{O} A\subset\mathbb{R}^n U\in\mathcal{O} A \mathcal{O} \Phi \mathcal{O} U\in\mathcal{O} \varphi=0 C_{\varphi} U \mathcal{O} C_{\varphi}\subset U\subset A f:A\to\mathbb{R} A \{x:f\text{ is discontinuous at }x\} 0 \int_{C_{\varphi}} \varphi\cdot |f| \int_{C_{\varphi}} \varphi\cdot |f| C_{\varphi} \int_A \varphi\cdot |f|:=\int_{C_{\varphi}} \varphi\cdot |f|,"['multivariable-calculus', 'solution-verification', 'improper-integrals']"
74,Second order derivative of matrix functions,Second order derivative of matrix functions,,"Let $A, B : \text{Sym}(d) \rightarrow \text{Sym}(d) $ , where $\text{Sym}(d)$ is the set of $d \times d$ symmetric matrices. I am interested in computing the second order differential of $F(X) = A(X) B(X)$ , more precisely $D^2 F(X)[U, U]$ for some $U \in \text{Sym}(d)$ . I am currently using the following formulas $$ DF(X)[U] = DA(X)[U] \; B(X) + A(X) \; DB(X)[U] $$ $$ D^2 F(X)[U, U] = D^2A(X)[U, U] \; B(X) + 2 DA(X)[U] \; DB(X)[U] + A(X) \; D^2 B(X)[U,U] $$ However, I am not completely confident in them (especially since I am not familiar with tensors). I would appreciate if you could help me with a reference for these.","Let , where is the set of symmetric matrices. I am interested in computing the second order differential of , more precisely for some . I am currently using the following formulas However, I am not completely confident in them (especially since I am not familiar with tensors). I would appreciate if you could help me with a reference for these.","A, B : \text{Sym}(d) \rightarrow \text{Sym}(d)  \text{Sym}(d) d \times d F(X) = A(X) B(X) D^2 F(X)[U, U] U \in \text{Sym}(d)  DF(X)[U] = DA(X)[U] \; B(X) + A(X) \; DB(X)[U]   D^2 F(X)[U, U] = D^2A(X)[U, U] \; B(X) + 2 DA(X)[U] \; DB(X)[U] + A(X) \; D^2 B(X)[U,U] ","['multivariable-calculus', 'derivatives', 'machine-learning']"
75,Is the Local Average of a Continuous Multivariable Function Differentiable?,Is the Local Average of a Continuous Multivariable Function Differentiable?,,"Suppose we have a continuous $f:\mathbb{R}\to\mathbb{R}$ . It is an immediate corollary of the Leibniz integral rule that $f^*:x\mapsto\int_{x-\frac{1}{2}}^{x+\frac{1}{2}}f(t)\ dt$ , the ""local"" average of $f$ on the interval of length one centred at $x$ , is a continuously differentiable function in $x$ . I was wondering how far we can get extending this result into higher dimensions. Specifically, let $f:\mathbb{R}^n\to\mathbb{R}^m$ be continuous. For $x\in\mathbb{R}^n$ , let $B_r(x)$ be the closed ball of radius $r$ around $x$ . Then, is the function $f^*:x\to\int_{B_{\frac{1}{2}}(x)}f(t)\ dt$ , where the integral is the standard integral for vector-valued functions, a continuously differentiable function? Could we say, integrate over a square instead of a circle?","Suppose we have a continuous . It is an immediate corollary of the Leibniz integral rule that , the ""local"" average of on the interval of length one centred at , is a continuously differentiable function in . I was wondering how far we can get extending this result into higher dimensions. Specifically, let be continuous. For , let be the closed ball of radius around . Then, is the function , where the integral is the standard integral for vector-valued functions, a continuously differentiable function? Could we say, integrate over a square instead of a circle?",f:\mathbb{R}\to\mathbb{R} f^*:x\mapsto\int_{x-\frac{1}{2}}^{x+\frac{1}{2}}f(t)\ dt f x x f:\mathbb{R}^n\to\mathbb{R}^m x\in\mathbb{R}^n B_r(x) r x f^*:x\to\int_{B_{\frac{1}{2}}(x)}f(t)\ dt,"['real-analysis', 'multivariable-calculus', 'derivatives', 'continuity', 'leibniz-integral-rule']"
76,Self-similar solutions for a particular parabolic system,Self-similar solutions for a particular parabolic system,,"Consider the parabolic system \begin{align}    \begin{cases}    u_t - \Delta\Big((a_1 + a_{11} u + a_{12} v) u\Big) = 0, & t >0, \ x \in \mathbb R^n \\    v_t - \Delta\Big((a_2 + a_{22} v + a_{21} u) v\Big) = 0, & t >0, \ x \in \mathbb R^n     \end{cases} \end{align} where $a_1,a_2,a_{11},a_{22},a_{12},a_{21}$ are non-negative constants and the unknowns are $u: (0,\infty)\times \mathbb R^n \to \mathbb R$ and $v: (0,\infty)\times \mathbb R^n \to \mathbb R$ . Main Question: Formally, can we compute a (possibly radial) self-similar solution, that is a solution of this system the form $$\left(\frac{1}{t^{\gamma}} u(x/t^{\alpha}), \ \frac{1}{t^{\kappa}} v(x/t^{\beta})\right)$$ for suitable $\alpha$ , $\beta$ , $\gamma$ , $\kappa$ ? Subquestion: Partial results assuming something (reasonable) extra on the coefficients are also very welcome. Remark 1. The computation that I'm looking for in the system above is very classical in the particular case of the heat equation $u_t -\Delta u = 0$ (i.e. $a_{11} = a_{12} = a_{2} = a_{22} = a_{21} = 0$ ). For your convenience, I'm typing it up below (following what can be found in Evans' book): Let us look for a solution of the form \begin{equation*} u(x, t)=\frac{1}{t^{\alpha}} v\left(\frac{x}{t^{\beta}}\right) \quad\left(x \in \mathbb{R}^{n}, t>0\right) \end{equation*} where the constants $\alpha, \beta$ and the function $v: \mathbb{R}^{n} \rightarrow \mathbb{R}$ must be found. Plugging this into the heat equation, we arrive at \begin{equation*} \alpha t^{-(\alpha+1)} v(y)+\beta t^{-(\alpha+1)} y \cdot D v(y)+t^{-(\alpha+2 \beta)} \Delta v(y)=0 \end{equation*} for $y:=t^{-\beta} x$ . In order to transform this into an expression involving the variable $y$ alone, we take $\beta=\frac{1}{2}$ , so that the equation reduces to \begin{equation*} \alpha v+\frac{1}{2} y \cdot D v+\Delta v=0 \end{equation*} Assuming also that $v$ is radial -- that is, $v(y)=w(|y|)$ for some $w: \mathbb{R} \rightarrow \mathbb{R}$ , we get \begin{equation*} \alpha w+\frac{1}{2} r w^{\prime}+w^{\prime \prime}+\frac{n-1}{r} w^{\prime}=0 \end{equation*} for $r=|y|, '=\frac{d}{d r}$ . Setting $\alpha=\frac{n}{2}$ , we get \begin{equation*} \left(r^{n-1} w^{\prime}\right)^{\prime}+\frac{1}{2}\left(r^{n} w\right)^{\prime}=0 \end{equation*} Thus \begin{equation*} r^{n-1} w^{\prime}+\frac{1}{2} r^{n} w=a \end{equation*} for some constant $a$ . Assuming $\lim _{r \rightarrow \infty} w, w^{\prime}=0$ , we conclude $a=0$ ; whence \begin{equation*} w^{\prime}=-\frac{1}{2} r w \end{equation*} But then for some constant $b$ \begin{equation}  w=b e^{-\frac{r^{2}}{4}} \end{equation} In conclusion, we arrive at the expected Gaussian profile $$\frac{b}{t^{n / 2}} e^{-\frac{|x|}{4 t}}$$ Remark 2 For the same question for the Porous Medium Equation $u_t - \Delta u^m = 0$ , see Section 4.4 Source-type solutions. Selfsimilarity from page 69 of https://verso.mat.uam.es/~juanluis.vazquez/BKPME2006six.pdf","Consider the parabolic system where are non-negative constants and the unknowns are and . Main Question: Formally, can we compute a (possibly radial) self-similar solution, that is a solution of this system the form for suitable , , , ? Subquestion: Partial results assuming something (reasonable) extra on the coefficients are also very welcome. Remark 1. The computation that I'm looking for in the system above is very classical in the particular case of the heat equation (i.e. ). For your convenience, I'm typing it up below (following what can be found in Evans' book): Let us look for a solution of the form where the constants and the function must be found. Plugging this into the heat equation, we arrive at for . In order to transform this into an expression involving the variable alone, we take , so that the equation reduces to Assuming also that is radial -- that is, for some , we get for . Setting , we get Thus for some constant . Assuming , we conclude ; whence But then for some constant In conclusion, we arrive at the expected Gaussian profile Remark 2 For the same question for the Porous Medium Equation , see Section 4.4 Source-type solutions. Selfsimilarity from page 69 of https://verso.mat.uam.es/~juanluis.vazquez/BKPME2006six.pdf","\begin{align}
   \begin{cases}
   u_t - \Delta\Big((a_1 + a_{11} u + a_{12} v) u\Big) = 0, & t >0, \ x \in \mathbb R^n \\
   v_t - \Delta\Big((a_2 + a_{22} v + a_{21} u) v\Big) = 0, & t >0, \ x \in \mathbb R^n 
   \end{cases}
\end{align} a_1,a_2,a_{11},a_{22},a_{12},a_{21} u: (0,\infty)\times \mathbb R^n \to \mathbb R v: (0,\infty)\times \mathbb R^n \to \mathbb R \left(\frac{1}{t^{\gamma}} u(x/t^{\alpha}), \ \frac{1}{t^{\kappa}} v(x/t^{\beta})\right) \alpha \beta \gamma \kappa u_t -\Delta u = 0 a_{11} = a_{12} = a_{2} = a_{22} = a_{21} = 0 \begin{equation*}
u(x, t)=\frac{1}{t^{\alpha}} v\left(\frac{x}{t^{\beta}}\right) \quad\left(x \in \mathbb{R}^{n}, t>0\right)
\end{equation*} \alpha, \beta v: \mathbb{R}^{n} \rightarrow \mathbb{R} \begin{equation*}
\alpha t^{-(\alpha+1)} v(y)+\beta t^{-(\alpha+1)} y \cdot D v(y)+t^{-(\alpha+2 \beta)} \Delta v(y)=0
\end{equation*} y:=t^{-\beta} x y \beta=\frac{1}{2} \begin{equation*}
\alpha v+\frac{1}{2} y \cdot D v+\Delta v=0
\end{equation*} v v(y)=w(|y|) w: \mathbb{R} \rightarrow \mathbb{R} \begin{equation*}
\alpha w+\frac{1}{2} r w^{\prime}+w^{\prime \prime}+\frac{n-1}{r} w^{\prime}=0
\end{equation*} r=|y|, '=\frac{d}{d r} \alpha=\frac{n}{2} \begin{equation*}
\left(r^{n-1} w^{\prime}\right)^{\prime}+\frac{1}{2}\left(r^{n} w\right)^{\prime}=0
\end{equation*} \begin{equation*}
r^{n-1} w^{\prime}+\frac{1}{2} r^{n} w=a
\end{equation*} a \lim _{r \rightarrow \infty} w, w^{\prime}=0 a=0 \begin{equation*}
w^{\prime}=-\frac{1}{2} r w
\end{equation*} b \begin{equation} 
w=b e^{-\frac{r^{2}}{4}}
\end{equation} \frac{b}{t^{n / 2}} e^{-\frac{|x|}{4 t}} u_t - \Delta u^m = 0","['calculus', 'multivariable-calculus', 'partial-differential-equations', 'reference-request', 'heat-equation']"
77,Is it possible to derive the Jacobian from the chain rule for multivariable functions?,Is it possible to derive the Jacobian from the chain rule for multivariable functions?,,"In the single variable calculus, when doing a u substitution, we have $\qquad\begin{align}\int_a^b{f'(x)}\,\mathrm dx &= \int_a^b{(h \circ g)'(x)\,\mathrm dx}\\& = \int_a^b{(h' \circ g)(x)\,g'(x)\,\mathrm dx} \\&= (h \circ g)(x) \Big|_a^b \\&= h(g(b)) - h(g(a)) \\&= \int_{g(a)}^{g(b)}{h'(u)\,\mathrm du}\end{align}$ Here, there's a nice connection between the chain rule and the Jacobian, where the $g'(x)dx$ term comes out of the wash without any appeals to geometric reasoning, like is often the case when introducing the Jacobian term for the multivariable change of variables.  Is it possible to make an analogous derivation for the multivariable case? What confuses me is that the multivariable chain rule has the form of a sum, but the Jacobian term has the form of a difference, so at first glance it seems not possible.","In the single variable calculus, when doing a u substitution, we have Here, there's a nice connection between the chain rule and the Jacobian, where the term comes out of the wash without any appeals to geometric reasoning, like is often the case when introducing the Jacobian term for the multivariable change of variables.  Is it possible to make an analogous derivation for the multivariable case? What confuses me is that the multivariable chain rule has the form of a sum, but the Jacobian term has the form of a difference, so at first glance it seems not possible.","\qquad\begin{align}\int_a^b{f'(x)}\,\mathrm dx &= \int_a^b{(h \circ g)'(x)\,\mathrm dx}\\& = \int_a^b{(h' \circ g)(x)\,g'(x)\,\mathrm dx} \\&= (h \circ g)(x) \Big|_a^b \\&= h(g(b)) - h(g(a)) \\&= \int_{g(a)}^{g(b)}{h'(u)\,\mathrm du}\end{align} g'(x)dx","['multivariable-calculus', 'chain-rule', 'jacobian']"
78,Multinomial theorem with multivariate terms?,Multinomial theorem with multivariate terms?,,"Let $S=\{a,b,c,d,...\}$ . Let $P_n=(abc+abd+acd+...+ab+ac+ad+...a+b+c+d...)^n$ . In addition, there's the condition that for all variables, $x^n=x$ (maybe it'll be easier without this?).  Is there something like the multinomial theorem to expand out $P_n$ ? Edit: Here's what I have so far: For any $n$ , the unique terms in the expanded product is the set of terms $a^{k_1}b^{k_2}c^{k_3}...$ , such that $k_1,k_2,k_3,... \le n$ (you can have at most $n$ multiplicands in the product), and that $n \le k_1+k_2+k_3+... \le 3n$ (at minimum, there are $n$ length-1 multiplicands in the product, and at most, there are $n$ length-3 multiplicands in the product). For any such term $a^{k_1}b^{k_2}c^{k_3}...$ , to find the number of terms that make it up, there must be $k_1$ terms that have $a$ , $k_2$ terms that includes $b$ , $k_3$ that includes c, and so on. Edit 2: I realize that this can be done recursively --- for a given term, remove one of the original terms ( $abc, abd, ...$ ), and find the composition of that smaller term,","Let . Let . In addition, there's the condition that for all variables, (maybe it'll be easier without this?).  Is there something like the multinomial theorem to expand out ? Edit: Here's what I have so far: For any , the unique terms in the expanded product is the set of terms , such that (you can have at most multiplicands in the product), and that (at minimum, there are length-1 multiplicands in the product, and at most, there are length-3 multiplicands in the product). For any such term , to find the number of terms that make it up, there must be terms that have , terms that includes , that includes c, and so on. Edit 2: I realize that this can be done recursively --- for a given term, remove one of the original terms ( ), and find the composition of that smaller term,","S=\{a,b,c,d,...\} P_n=(abc+abd+acd+...+ab+ac+ad+...a+b+c+d...)^n x^n=x P_n n a^{k_1}b^{k_2}c^{k_3}... k_1,k_2,k_3,... \le n n n \le k_1+k_2+k_3+... \le 3n n n a^{k_1}b^{k_2}c^{k_3}... k_1 a k_2 b k_3 abc, abd, ...","['multivariable-calculus', 'multinomial-theorem']"
79,Differential $1$-form and proof of an open disc and open circular annulus not being diffeomorphic,Differential -form and proof of an open disc and open circular annulus not being diffeomorphic,1,"There is one example in my script about an application of a differential $1$ form in proving some subsets of $\Bbb R^2$ aren't diffeomorphic. As far as I've understood the explanation, we used a $C^2$ diffeomorphism and I don't understand how it's non-existence implies the non-existanece of a $C^1$ diffeomorphism. Here it goes, verbatim from Croatian: Using differential $1$ - froms, we can show certain subsets of $\Bbb R^2$ aren't diffeomorphic. $\underline{\boldsymbol{\text{example } 17.33}:}$ For $c\in\Bbb R^2,$ and $0<r<s$ let's denote by $B(c,r)$ and $B(c,r,s)$ the open disc centered at $c$ of radius $r$ and open circular annulus centered at $c$ with the inner radius $r$ and outer radius $s$ respectively. Since $B(c,r)$ is convex (and in particular, it's a 'star-like' set), every closed differential $1$ -form of class $C^1$ on $B(c,r)$ is exact. On the other hand, there are closed differential $1$ -forms on $B(c,r,s)$ which aren't exact. Namely, for $c=(c_1,c_2),$ let $\omega_c$ denote the angular form centered at $c$ : $$\omega_c=\frac{-(y-c_2)dx+(x-c_1)dy}{(x-c_1)^2+(y-c_2)^2}.$$ Then, $\omega_c$ is a well defined $1$ -form on $B(c,r,s)\subset\Bbb R^2\setminus\{c\}$ and $d\omega_c=0,$ (that is, $\omega_c$ is closed). On the other hand, for $p=\frac{r+s}2$ let $\gamma:[0,2\pi]\to B(c,r,s)$ be a parametrization of the circle $\{(x,y)\in\Bbb R^2\mid\|x-c\|=p\}\subset B(c,r,s)$ defined by $\gamma(t)=(c_1+p\cos t,c_2+p\sin t).$ Then, $\begin{aligned}&\color{white}=\int_\gamma\omega_c\\&=\int_0^{2\pi}\frac{-p\sin t(-p\sin t)+p\cos t(p\cos t)}{(p\cos t)^2+(p\sin t)^2}dt\\&=\int_0^{2\pi}dt=2\pi\end{aligned}$ Suppose there exists a $C^2$ diffeomorphism $\boldsymbol F:B(c,r)\to B(c,r,s).$ Then, it would hold, $$\int_{\boldsymbol F^{-1}\circ\gamma} \boldsymbol F^*(\omega_c)=\int_\gamma\omega_c=2\pi\tag 1$$ Since the pullback and the differential commute, $\boldsymbol F^*(\omega_c)$ a is closed $1$ -form on $B(c,r)$ and hence exact. Since $F^{-1}\circ\gamma$ is a closed path in $B(c,r),$ $$\int_{\boldsymbol F^{-1}\circ\gamma}\boldsymbol F^*(\omega_c)=0,$$ which is a contradition with $(1)$ . In a lemma where we've proven that the pullback and the differential of a $1$ -form commute, there was an assumption on $\boldsymbol F$ being a $C^2$ diffeomorphism and in the proof by brute-force computation, we recalled the Schwarz theorem which requires continuous second derivatives. I believe, for the same reason, $\boldsymbol F$ is said to be of class $C^2$ in the above example. However, I'm missing why this example proves the two sets aren't diffeomorphic. I thought we should prove there isn't a $C^1$ -diffeomorphism $\boldsymbol F:B(c,r)\to B(c,r,s),$ and, indeed, I found, only the text, of the same task in an old exam where it is explicitly stated one should prove there isn't a $C^1$ diffeomorphism. I see the converse is true by contrapositive, but I can't grasp how we know there isn't a $C^1$ -diffeomorphism based on the non-existence of a $C^2$ -diffeomorphism. Is there a fallacy in this approach, or does the pullback not require a $C^2$ -diffeomorphism $\boldsymbol F$ in order to commute with the differential? $\boldsymbol{\text{ P. S}:}$ I tried to summarize the material, but we have been warned some of the terminology we use is not conventional around the world, so feel free to let me know if I should clarify anything regarding the terminology or my background on the matter.","There is one example in my script about an application of a differential form in proving some subsets of aren't diffeomorphic. As far as I've understood the explanation, we used a diffeomorphism and I don't understand how it's non-existence implies the non-existanece of a diffeomorphism. Here it goes, verbatim from Croatian: Using differential - froms, we can show certain subsets of aren't diffeomorphic. For and let's denote by and the open disc centered at of radius and open circular annulus centered at with the inner radius and outer radius respectively. Since is convex (and in particular, it's a 'star-like' set), every closed differential -form of class on is exact. On the other hand, there are closed differential -forms on which aren't exact. Namely, for let denote the angular form centered at : Then, is a well defined -form on and (that is, is closed). On the other hand, for let be a parametrization of the circle defined by Then, Suppose there exists a diffeomorphism Then, it would hold, Since the pullback and the differential commute, a is closed -form on and hence exact. Since is a closed path in which is a contradition with . In a lemma where we've proven that the pullback and the differential of a -form commute, there was an assumption on being a diffeomorphism and in the proof by brute-force computation, we recalled the Schwarz theorem which requires continuous second derivatives. I believe, for the same reason, is said to be of class in the above example. However, I'm missing why this example proves the two sets aren't diffeomorphic. I thought we should prove there isn't a -diffeomorphism and, indeed, I found, only the text, of the same task in an old exam where it is explicitly stated one should prove there isn't a diffeomorphism. I see the converse is true by contrapositive, but I can't grasp how we know there isn't a -diffeomorphism based on the non-existence of a -diffeomorphism. Is there a fallacy in this approach, or does the pullback not require a -diffeomorphism in order to commute with the differential? I tried to summarize the material, but we have been warned some of the terminology we use is not conventional around the world, so feel free to let me know if I should clarify anything regarding the terminology or my background on the matter.","1 \Bbb R^2 C^2 C^1 1 \Bbb R^2 \underline{\boldsymbol{\text{example } 17.33}:} c\in\Bbb R^2, 0<r<s B(c,r) B(c,r,s) c r c r s B(c,r) 1 C^1 B(c,r) 1 B(c,r,s) c=(c_1,c_2), \omega_c c \omega_c=\frac{-(y-c_2)dx+(x-c_1)dy}{(x-c_1)^2+(y-c_2)^2}. \omega_c 1 B(c,r,s)\subset\Bbb R^2\setminus\{c\} d\omega_c=0, \omega_c p=\frac{r+s}2 \gamma:[0,2\pi]\to B(c,r,s) \{(x,y)\in\Bbb R^2\mid\|x-c\|=p\}\subset B(c,r,s) \gamma(t)=(c_1+p\cos t,c_2+p\sin t). \begin{aligned}&\color{white}=\int_\gamma\omega_c\\&=\int_0^{2\pi}\frac{-p\sin t(-p\sin t)+p\cos t(p\cos t)}{(p\cos t)^2+(p\sin t)^2}dt\\&=\int_0^{2\pi}dt=2\pi\end{aligned} C^2 \boldsymbol F:B(c,r)\to B(c,r,s). \int_{\boldsymbol F^{-1}\circ\gamma} \boldsymbol F^*(\omega_c)=\int_\gamma\omega_c=2\pi\tag 1 \boldsymbol F^*(\omega_c) 1 B(c,r) F^{-1}\circ\gamma B(c,r), \int_{\boldsymbol F^{-1}\circ\gamma}\boldsymbol F^*(\omega_c)=0, (1) 1 \boldsymbol F C^2 \boldsymbol F C^2 C^1 \boldsymbol F:B(c,r)\to B(c,r,s), C^1 C^1 C^2 C^2 \boldsymbol F \boldsymbol{\text{ P. S}:}","['integration', 'multivariable-calculus', 'proof-writing', 'vector-analysis']"
80,Very fast but inaccurate estimations of multivariate Gaussian integral over a hypercube,Very fast but inaccurate estimations of multivariate Gaussian integral over a hypercube,,"$\def\Z{\mathbb{Z}}\def\R{\mathbb{R}}\def\A{\mathcal{A}}\def\N{\mathcal{N}}$ I'm working on 4D positive real values, i.e. $\R^4_{\geq 0}$ , where it is gridized with hypercubes of side length $a > 0$ . So, an index vector $I = [i ~~ j ~~ k ~~ \ell]^T$ uniquely represents the region $$\A_I := \{ x \in \R^4_{\geq 0} ~~|~~ a I_n \leq x_n < a (I_n+1),~~ n = 1,2,3,4 \}$$ where $x_n$ and $I_n$ represents the $n$ th entity of $x$ and $I$ respectively. I have a multivariate Gaussian distribution in $\R^4$ , which is $N(m,P)$ , i.e. with mean $m$ and covariance matrix $P$ where $m \in \R^4$ and $P\in\R^{4\times 4}$ is symmetric. Let $m \in \A_I$ for some $I$ and $\N_I := \{J \in \Z^4 ~~|~~ I_n-1 \leq J_n \leq I_n+1\}$ be the index set that represents the immediate neighbours of $\A_I$ (including itself). For each $J \in \N_I$ I want to estimate the Gaussian integral over $\A_J$ , i.e. $$\alpha_J := |2 \pi P|^{-1/2} \int_{\A_J} \exp\left(-\frac{1}{2} (x-m)^T P^{-1} (x-m)\right) dx $$ where $|\cdot|$ represents the determinant. So, I need to estimate $3^4=81$ numbers very fast, e.g. under 1 milliseconds on a typical mid-range laptop if possible. The estimations do not need to be very accurate though, just rough values. One method that comes to mind is to calculate the integrand for the vertices and midpoint of regions, and use trapezoidal integration. This would require the calculation of the integrand $4^4+3^4=337$ times, which might be too much. Another method could be to calculate the integrand for only the midpoints and use them as estimations or maybe use some sort of interpolation and averaging centered around $m$ . Although both methods can run very fast, I'm not sure they would create relevant estimations especially when $m$ is close to some boundary between regions. Is there a name for these kind of problems? Is there very fast and robust algorithms (produces relevant results) suited to this problem? Note: The values of $m$ and $P$ are not known beforehand. So, a precomputation table cannot be used (I guess). But if the values in a table for a specific $P$ can be used by adjusting the values for the incoming $P$ , that would be ok.","I'm working on 4D positive real values, i.e. , where it is gridized with hypercubes of side length . So, an index vector uniquely represents the region where and represents the th entity of and respectively. I have a multivariate Gaussian distribution in , which is , i.e. with mean and covariance matrix where and is symmetric. Let for some and be the index set that represents the immediate neighbours of (including itself). For each I want to estimate the Gaussian integral over , i.e. where represents the determinant. So, I need to estimate numbers very fast, e.g. under 1 milliseconds on a typical mid-range laptop if possible. The estimations do not need to be very accurate though, just rough values. One method that comes to mind is to calculate the integrand for the vertices and midpoint of regions, and use trapezoidal integration. This would require the calculation of the integrand times, which might be too much. Another method could be to calculate the integrand for only the midpoints and use them as estimations or maybe use some sort of interpolation and averaging centered around . Although both methods can run very fast, I'm not sure they would create relevant estimations especially when is close to some boundary between regions. Is there a name for these kind of problems? Is there very fast and robust algorithms (produces relevant results) suited to this problem? Note: The values of and are not known beforehand. So, a precomputation table cannot be used (I guess). But if the values in a table for a specific can be used by adjusting the values for the incoming , that would be ok.","\def\Z{\mathbb{Z}}\def\R{\mathbb{R}}\def\A{\mathcal{A}}\def\N{\mathcal{N}} \R^4_{\geq 0} a > 0 I = [i ~~ j ~~ k ~~ \ell]^T \A_I := \{ x \in \R^4_{\geq 0} ~~|~~ a I_n \leq x_n < a (I_n+1),~~ n = 1,2,3,4 \} x_n I_n n x I \R^4 N(m,P) m P m \in \R^4 P\in\R^{4\times 4} m \in \A_I I \N_I := \{J \in \Z^4 ~~|~~ I_n-1 \leq J_n \leq I_n+1\} \A_I J \in \N_I \A_J \alpha_J := |2 \pi P|^{-1/2} \int_{\A_J} \exp\left(-\frac{1}{2} (x-m)^T P^{-1} (x-m)\right) dx  |\cdot| 3^4=81 4^4+3^4=337 m m m P P P","['multivariable-calculus', 'gaussian-integral']"
81,Deriving boundary conditions for Faraday's law and Ampére's law by letting the width $\delta$ approach zero,Deriving boundary conditions for Faraday's law and Ampére's law by letting the width  approach zero,\delta,"I am currently studying the textbook Physics of Photonic Devices , second edition, by Shun Lien Chuang. Section 2.1.1 Maxwell's Equations in MKS Units says the following: The well-known Maxwell's equations in MKS (meter, kilogram, and second) units are written as $$\nabla \times \mathbf{E} = - \dfrac{\partial}{\partial{t}}\mathbf{B} \ \ \ \ \text{Faraday's law} \tag{2.1.1}$$ $$\nabla \times \mathbf{H} = \mathbf{J} + \dfrac{\partial{\mathbf{D}}}{\partial{t}} \ \ \ \ \text{Ampére's law} \tag{2.1.2}$$ $$\nabla \cdot \mathbf{D} = \rho \ \ \ \ \text{Gauss's law} \tag{2.1.3}$$ $$\nabla \cdot \mathbf{B} = 0 \ \ \ \ \text{Gauss's law} \tag{2.1.4}$$ where $\mathbf{E}$ is the electric field (V/m), $\mathbf{H}$ is the magnetic field (A/m), $\mathbf{D}$ is the electric displacement flux density (C/m $^2$ ), and $\mathbf{B}$ is the magnetic flux density (Vs/m $^2$ or Webers/m $^2$ ). The two source terms, the charge density $\rho$ (C/m $^3$ ) and the current density $\mathbf{J}$ (A/m $^2$ ), are related by the continuity equation $$\nabla \cdot \mathbf{J} + \dfrac{\partial}{\partial{t}} \rho = 0 \tag{2.1.5}$$ Section 2.1.2 Boundary Conditions then says the following: By applying the first two Maxwell's equations over a small rectangular surface with a width $\delta$ (dashed line in Fig. 2.1a) across the interface of a boundary and using Stokes' theorem between a line integral over a contour $C$ and the surface $S$ enclosed by the contour $$\oint_C \mathbf{E} \cdot d \mathscr{l} = \int_S \nabla \times \mathbf{E} \cdot \mathbf{\hat{n}} \ dS = - \dfrac{d}{dt} \int_S \mathbf{B} \cdot \mathbf{\hat{n}} \ dS \tag{2.1.9a}$$ $$\oint_C \mathbf{H} \cdot d \mathscr{l} = \int_S \nabla \times \mathbf{H} \cdot \mathbf{\hat{n}} \ dS = \int_S \mathbf{J} \cdot \mathbf{\hat{n}} \ dS + \dfrac{d}{dt} \int_S \mathbf{D} \cdot \mathbf{\hat{n}} \ dS, \tag{2.1.9b}$$ the following boundary conditions can be derived by letting the width $\delta$ approach zero: $$\mathbf{\hat{n}} \times (\mathbf{E}_1 - \mathbf{E}_2) = 0 \tag{2.1.10}$$ $$\mathbf{\hat{n}} \times (\mathbf{H}_1 - \mathbf{H}_2) = \mathbf{J}_s, \tag{2.1.11}$$ where $\mathbf{J}_s(= \lim\limits_{\mathbf{J} \to \infty, \ \delta \to 0} \mathbf{J} \delta)$ is the surface current density (A/m). Note that the unit normal vector $\hat{n}$ points from medium 2 to medium 1. How does letting $\delta$ approach zero get us 2.1.10 and 2.1.11? And why do we have $\mathbf{E}_1 - \mathbf{E}_2$ and $\mathbf{H}_1 - \mathbf{H}_2$ instead of $\mathbf{E}_2 - \mathbf{E}_1$ and $\mathbf{H}_2 - \mathbf{H}_1$ ?","I am currently studying the textbook Physics of Photonic Devices , second edition, by Shun Lien Chuang. Section 2.1.1 Maxwell's Equations in MKS Units says the following: The well-known Maxwell's equations in MKS (meter, kilogram, and second) units are written as where is the electric field (V/m), is the magnetic field (A/m), is the electric displacement flux density (C/m ), and is the magnetic flux density (Vs/m or Webers/m ). The two source terms, the charge density (C/m ) and the current density (A/m ), are related by the continuity equation Section 2.1.2 Boundary Conditions then says the following: By applying the first two Maxwell's equations over a small rectangular surface with a width (dashed line in Fig. 2.1a) across the interface of a boundary and using Stokes' theorem between a line integral over a contour and the surface enclosed by the contour the following boundary conditions can be derived by letting the width approach zero: where is the surface current density (A/m). Note that the unit normal vector points from medium 2 to medium 1. How does letting approach zero get us 2.1.10 and 2.1.11? And why do we have and instead of and ?","\nabla \times \mathbf{E} = - \dfrac{\partial}{\partial{t}}\mathbf{B} \ \ \ \ \text{Faraday's law} \tag{2.1.1} \nabla \times \mathbf{H} = \mathbf{J} + \dfrac{\partial{\mathbf{D}}}{\partial{t}} \ \ \ \ \text{Ampére's law} \tag{2.1.2} \nabla \cdot \mathbf{D} = \rho \ \ \ \ \text{Gauss's law} \tag{2.1.3} \nabla \cdot \mathbf{B} = 0 \ \ \ \ \text{Gauss's law} \tag{2.1.4} \mathbf{E} \mathbf{H} \mathbf{D} ^2 \mathbf{B} ^2 ^2 \rho ^3 \mathbf{J} ^2 \nabla \cdot \mathbf{J} + \dfrac{\partial}{\partial{t}} \rho = 0 \tag{2.1.5} \delta C S \oint_C \mathbf{E} \cdot d \mathscr{l} = \int_S \nabla \times \mathbf{E} \cdot \mathbf{\hat{n}} \ dS = - \dfrac{d}{dt} \int_S \mathbf{B} \cdot \mathbf{\hat{n}} \ dS \tag{2.1.9a} \oint_C \mathbf{H} \cdot d \mathscr{l} = \int_S \nabla \times \mathbf{H} \cdot \mathbf{\hat{n}} \ dS = \int_S \mathbf{J} \cdot \mathbf{\hat{n}} \ dS + \dfrac{d}{dt} \int_S \mathbf{D} \cdot \mathbf{\hat{n}} \ dS, \tag{2.1.9b} \delta \mathbf{\hat{n}} \times (\mathbf{E}_1 - \mathbf{E}_2) = 0 \tag{2.1.10} \mathbf{\hat{n}} \times (\mathbf{H}_1 - \mathbf{H}_2) = \mathbf{J}_s, \tag{2.1.11} \mathbf{J}_s(= \lim\limits_{\mathbf{J} \to \infty, \ \delta \to 0} \mathbf{J} \delta) \hat{n} \delta \mathbf{E}_1 - \mathbf{E}_2 \mathbf{H}_1 - \mathbf{H}_2 \mathbf{E}_2 - \mathbf{E}_1 \mathbf{H}_2 - \mathbf{H}_1","['multivariable-calculus', 'physics', 'stokes-theorem']"
82,"Solution verification of flux integral $\iint_DF\cdot\hat{n}dS$ with $F=(2x,y,z)$",Solution verification of flux integral  with,"\iint_DF\cdot\hat{n}dS F=(2x,y,z)","Compute the flux of $F=(2x,y,z)$ through the surface $$ r = u^2v\,\hat{\imath} + uv^2\,\hat{\jmath} + v^3\,\hat{k}, \quad 0\leq u \leq 1, \quad 0 \leq v \leq 1 $$ My approach: is it correct?",Compute the flux of through the surface My approach: is it correct?,"F=(2x,y,z) 
r = u^2v\,\hat{\imath} + uv^2\,\hat{\jmath} + v^3\,\hat{k},
\quad 0\leq u \leq 1, \quad 0 \leq v \leq 1
","['multivariable-calculus', 'solution-verification', 'vector-analysis', 'surface-integrals']"
83,Apostol's Calculus II vs. Mathematical Analysis books,Apostol's Calculus II vs. Mathematical Analysis books,,"I'm a second year math undergraduate and I'm looking for a book for the three analysis courses I'm taking this year: Differentiation of Multivariable functions, Integration of Multivariable functions and Power Series and Lebesgue Integral. I have read almost all Spivak's Calculus and some of Apostol's Calculus book and done plenty of the exercises in both books as last year I took a Calculus course that covered most of the topics of single-variable calculus. I've also studied the basics of metric spaces (using Kaplansky's Set Theory and Metric Spaces as a reference) and some topology using Mendelson's introduction to topology (I'll be taking this semester a course on General Topology too). With this background, which book should I get? I was thinking as the two main options either Calculus II (Apostol) with a supplementary book on Lebesgue integral or Mathematical Analysis (Apostol). Is there any other book suitable for my courses? Are there any Dover books (or similar) related that could be useful as a supplementary material (as I've already found for other topics) ? I appreciate suggestions and comments about the books I've already mentioned. Edit (i): I've also seen a bit about Spivak's Calculus in Manifolds. Can it be suitable for studying multivariable calculus?","I'm a second year math undergraduate and I'm looking for a book for the three analysis courses I'm taking this year: Differentiation of Multivariable functions, Integration of Multivariable functions and Power Series and Lebesgue Integral. I have read almost all Spivak's Calculus and some of Apostol's Calculus book and done plenty of the exercises in both books as last year I took a Calculus course that covered most of the topics of single-variable calculus. I've also studied the basics of metric spaces (using Kaplansky's Set Theory and Metric Spaces as a reference) and some topology using Mendelson's introduction to topology (I'll be taking this semester a course on General Topology too). With this background, which book should I get? I was thinking as the two main options either Calculus II (Apostol) with a supplementary book on Lebesgue integral or Mathematical Analysis (Apostol). Is there any other book suitable for my courses? Are there any Dover books (or similar) related that could be useful as a supplementary material (as I've already found for other topics) ? I appreciate suggestions and comments about the books I've already mentioned. Edit (i): I've also seen a bit about Spivak's Calculus in Manifolds. Can it be suitable for studying multivariable calculus?",,"['real-analysis', 'multivariable-calculus', 'reference-request', 'book-recommendation', 'advice']"
84,time-dependent inflection points,time-dependent inflection points,,"For a function $u:\mathbb{R}\times [0,\infty)\rightarrow\mathbb{R}$ , let $\bar{x}$ be an inflection point of $u_0(x):=u(x,0)$ to the right of its maximum. Further, let the inflection point evolve with time $t\mapsto\bar{x}(t)$ such that $\bar{x}(0)=\bar{x}$ . Surely we then have $(u_0)_{xx}(\bar{x}(0))=u_{xx}(\bar{x}(0),0)=0$ , but why is it true that $u_{xx}(\bar{x}(t),t)=0$ for all $t\geq 0$ ? I'm assuming this follows since $(\bar{x}(t),t)$ is also an inflection point of $u$ , but I can't see why this is true. For reference, I'm trying to understand the proof of Lemma 6 from this paper: https://arxiv.org/pdf/1707.09000.pdf . It is in this proof that I encountered the claim above. I'm pretty confused about the entire idea of a time-dependent inflection point, and in particular, about going from an inflection point of a function of a single variable, i.e. an inflection point of $u_0(x)$ , to an inflection point of the two-variable function $u$ . I would really appreciate a clear, pedagogical answer if possible.. Thanks in advance.","For a function , let be an inflection point of to the right of its maximum. Further, let the inflection point evolve with time such that . Surely we then have , but why is it true that for all ? I'm assuming this follows since is also an inflection point of , but I can't see why this is true. For reference, I'm trying to understand the proof of Lemma 6 from this paper: https://arxiv.org/pdf/1707.09000.pdf . It is in this proof that I encountered the claim above. I'm pretty confused about the entire idea of a time-dependent inflection point, and in particular, about going from an inflection point of a function of a single variable, i.e. an inflection point of , to an inflection point of the two-variable function . I would really appreciate a clear, pedagogical answer if possible.. Thanks in advance.","u:\mathbb{R}\times [0,\infty)\rightarrow\mathbb{R} \bar{x} u_0(x):=u(x,0) t\mapsto\bar{x}(t) \bar{x}(0)=\bar{x} (u_0)_{xx}(\bar{x}(0))=u_{xx}(\bar{x}(0),0)=0 u_{xx}(\bar{x}(t),t)=0 t\geq 0 (\bar{x}(t),t) u u_0(x) u","['multivariable-calculus', 'partial-differential-equations']"
85,A complete proof of the Lagrange multipliers theorem using the implicit function theorem,A complete proof of the Lagrange multipliers theorem using the implicit function theorem,,"While searching on MSE, I couldn't find a complete rigorous proof the method of Lagrange multipliers using the implicit function theorem. I tried to write a complete proof myself below, but am not sure about some details, mainly the reordering of coordinates part. Any advice is very appreciated. Theorem. Let $f:\mathbb{R}^d\to\mathbb{R}$ and $h:\mathbb{R}^d\to\mathbb{R}^n$ be continuously differentiable functions, $C\in\mathbb{R}^n$ , and $M=\{h=C\}$ . Assume that $\text{rank } h'(x)=n$ for all $x\in M$ . If $f$ attains a constrained local extremum at $a$ , subject to the constraint $h(a)=C$ , then there exists $\lambda_1,\dots,\lambda_n\in\mathbb{R}$ such that $$\nabla f(a)=\lambda_1\nabla h_1(a)+\dots+\lambda_n\nabla h_n(a), \quad \quad (1)$$ where $\nabla f(a)=[\frac{\partial f}{\partial x_1} (a),\dots,\frac{\partial f}{\partial x_d} (a)]^\top$ and $\nabla h_i(a)=[\frac{\partial h_i}{\partial x_1} (a),\dots,\frac{\partial h_i}{\partial x_d} (a)]^\top$ for $i=1,\dots,n$ . Proof. Suppose $f$ attains a contrained local extremum at $a$ . First note that $h'(a)=[\nabla h_1(a),\dots, \nabla h_n(a)]^\top$ is $n\times d$ , and so $\text{rank } h'(x)=n$ implies that $n\leq d$ and that $h'(a)$ has $n$ linearly independent columns. If $n=d$ , then $h'(a)$ is invertible, and we can write $f'(a)=\Lambda h'(a)$ with $\Lambda:=f'(a)[h'(a)]^{-1}:=[\lambda_1,\dots,\lambda_n]$ . Transposing gives $(1)$ . Hence assume WLOG that $n<d$ . Next we claim that we can assume WLOG that the first $n$ columns of $h'(a)$ are linearly independent. Otherwise there exists a permutation $\pi:\{1,\dots,d\}\to \{1,\dots,d\} $ that reorders the columns of $h'(a)$ so that it holds. Define $f^*:\mathbb{R}^d\to\mathbb{R}$ and $h^*:\mathbb{R}^d\to\mathbb{R}^n$ by $$f^*(x_1,\dots,x_d)=f(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}), \quad h^*(x_1,\dots,x_d)=h(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)})$$ Then we have $$\frac{\partial f^*}{\partial x_j}(x_1,\dots,x_d)=\frac{\partial f}{\partial x_{\pi(j)}}(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}) \quad 1\leq j\leq d,$$ $$\frac{\partial h_i^*}{\partial x_j}(x_1,\dots,x_d)=\frac{\partial h_i}{\partial x_{\pi(j)}}(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}) \quad 1\leq i\leq n, \quad 1\leq j\leq d$$ and so the continuous differentiability of $f,h$ implies the continuous differentiability of $f^*,h^*$ . Let $a^*=(a_{\pi(1)},\dots,a_{\pi(n)})$ . Then $$\frac{\partial f^*}{\partial x_j}(a^*)=\frac{\partial f}{\partial x_{\pi(j)}}(a) \quad 1\leq j\leq d,$$ $$\frac{\partial h_i^*}{\partial x_j}(a^*)=\frac{\partial h_i}{\partial x_{\pi(j)}}(a)  \quad 1\leq i\leq n, \quad 1\leq j\leq d$$ It follows that the columns of $f^{*'}(a^*)$ and $h^{*'}(a^*)$ are those of $f'(a)$ and $h'(a)$ permuted according to $\pi$ . Moreover we see that $f^*$ attains a  constrained local extrumum at $a^*$ , subject to the constraint $h^*(a^*)=C$ . Since condition $(1)$ for $f,h$ at $a$ is equivalent to condition $(1)$ for $f^*,h^*$ at $a^*$ , we have reduced the problem to the case where the first $n$ columns of $h'(a)$ are linearly independent. As a last reduction, we may assume WLOG that $C=0$ , for otherwise we can replace $h$ by $h-C$ . Let $m=d-n$ and denote points in $\mathbb{R}^d=\mathbb{R}^{n+m}$ by $(x,y)$ with $x\in\mathbb{R}^n$ and $y\in\mathbb{R}^m$ . Then $h$ satisfies  the conditions of the implicit function theorem at the point $a=(a_x,a_y)$ , as found in Rudin PMA Theorem 9.27 for example. Therefore, there exists open sets $U\subset \mathbb{R}^{n+m}$ and $W\subset \mathbb R^m$ with $(a_x,a_y)\in U$ and $a_y\in W$ , and a continuously differentiable function $g:W\to\mathbb R^n$ such that $g(a_y)=a_x$ and $h(g(y),y)=0$ for all $y\in W$ . In particular we have $$\{(g(y),y):y\in W\}\subset M \quad \quad (2)$$ Define the function $F:W\to \mathbb R$ by $F(y)=f(g(y),y)$ . This functions is differentiable on $W$ , and the chain rule gives $F'(y)=f'(g(y),y)\begin{bmatrix} g'(y)\\ I_m \end{bmatrix}$ . Since $a_y\in W$ and $g$ is continuous at $a_y$ , it follows from $(2)$ that $F$ has an unconstrained local extremum at $a_y$ . For if there exists an $r>0$ such that $f|_{M\cap B_r(a)}$ has an unconstrained extremum at $a$ , then by choosing $\delta>0$ sufficiently small we obtain $(g(y),y)\in M\cap B_r(a)$ whenever $y\in W\cap B_{\delta}(a_y)$ . Using the fact that the derivative must vanish at a local extremum, and partitioning $f'(a)=[f_x'(a) \quad f_y'(a)]$ , we get $$0=F'(a_y)=[f_x'(a) \quad f_y'(a)]\begin{bmatrix} g'(a_y)\\ I_m \end{bmatrix}=f_x'(a)g'(a_y)+ f_y'(a)  \quad \quad (3)$$ Next consider the function $H:W\to\mathbb R^{n+m}$ defined by $H(y)=h(g(y),y)$ . This functions is differentiable on $W$ , and the chain rule gives $H'(y)=h'(g(y),y)\begin{bmatrix} g'(y)\\ I_m \end{bmatrix}$ . Moreover we have $H=0$ by definition of $g$ , and so in particular $$0=H'(a_y)=[h_x'(a) \quad h_y'(a)]\begin{bmatrix} g'(a_y)\\ I_m \end{bmatrix}=h_x'(a)g'(a_y)+ h_y'(a) \quad \quad (4)$$ where we partioned $h'(a)=[h_x'(a) \quad h_y'(a)]$ . Since the first $n$ columns of $h'(a)$ are linearly independent, $h_x'(a)$ is invertible, and from $(4)$ we get $$g'(a_y)=-[h_x'(a)]^{-1}h_y'(a) \quad \quad (5)$$ Substituting $(5)$ in $(3)$ and solving for $f_y'(a) $ we obtain $$f_y'(a) =\Lambda h_y'(a) \quad \quad (6)$$ where $\Lambda:=f_x'(a)[h_x'(a)]^{-1}:=[\lambda_1,\dots,\lambda_n]$ . But we also have $$f'_x(a)=f'_x(a)[h_x'(a)]^{-1}h_x'(a)=\Lambda h_x'(a) \quad \quad (7)$$ so combining $(6)$ and $(7)$ gives $$f'(a)=[f'_x(a) \quad f'_y(a)]=\Lambda [h_x'(a)\quad  h_y'(a)]=\Lambda h'(a)$$ Transposing gives $(1)$ , as desired.","While searching on MSE, I couldn't find a complete rigorous proof the method of Lagrange multipliers using the implicit function theorem. I tried to write a complete proof myself below, but am not sure about some details, mainly the reordering of coordinates part. Any advice is very appreciated. Theorem. Let and be continuously differentiable functions, , and . Assume that for all . If attains a constrained local extremum at , subject to the constraint , then there exists such that where and for . Proof. Suppose attains a contrained local extremum at . First note that is , and so implies that and that has linearly independent columns. If , then is invertible, and we can write with . Transposing gives . Hence assume WLOG that . Next we claim that we can assume WLOG that the first columns of are linearly independent. Otherwise there exists a permutation that reorders the columns of so that it holds. Define and by Then we have and so the continuous differentiability of implies the continuous differentiability of . Let . Then It follows that the columns of and are those of and permuted according to . Moreover we see that attains a  constrained local extrumum at , subject to the constraint . Since condition for at is equivalent to condition for at , we have reduced the problem to the case where the first columns of are linearly independent. As a last reduction, we may assume WLOG that , for otherwise we can replace by . Let and denote points in by with and . Then satisfies  the conditions of the implicit function theorem at the point , as found in Rudin PMA Theorem 9.27 for example. Therefore, there exists open sets and with and , and a continuously differentiable function such that and for all . In particular we have Define the function by . This functions is differentiable on , and the chain rule gives . Since and is continuous at , it follows from that has an unconstrained local extremum at . For if there exists an such that has an unconstrained extremum at , then by choosing sufficiently small we obtain whenever . Using the fact that the derivative must vanish at a local extremum, and partitioning , we get Next consider the function defined by . This functions is differentiable on , and the chain rule gives . Moreover we have by definition of , and so in particular where we partioned . Since the first columns of are linearly independent, is invertible, and from we get Substituting in and solving for we obtain where . But we also have so combining and gives Transposing gives , as desired.","f:\mathbb{R}^d\to\mathbb{R} h:\mathbb{R}^d\to\mathbb{R}^n C\in\mathbb{R}^n M=\{h=C\} \text{rank } h'(x)=n x\in M f a h(a)=C \lambda_1,\dots,\lambda_n\in\mathbb{R} \nabla f(a)=\lambda_1\nabla h_1(a)+\dots+\lambda_n\nabla h_n(a), \quad \quad (1) \nabla f(a)=[\frac{\partial f}{\partial x_1} (a),\dots,\frac{\partial f}{\partial x_d} (a)]^\top \nabla h_i(a)=[\frac{\partial h_i}{\partial x_1} (a),\dots,\frac{\partial h_i}{\partial x_d} (a)]^\top i=1,\dots,n f a h'(a)=[\nabla h_1(a),\dots, \nabla h_n(a)]^\top n\times d \text{rank } h'(x)=n n\leq d h'(a) n n=d h'(a) f'(a)=\Lambda h'(a) \Lambda:=f'(a)[h'(a)]^{-1}:=[\lambda_1,\dots,\lambda_n] (1) n<d n h'(a) \pi:\{1,\dots,d\}\to \{1,\dots,d\}  h'(a) f^*:\mathbb{R}^d\to\mathbb{R} h^*:\mathbb{R}^d\to\mathbb{R}^n f^*(x_1,\dots,x_d)=f(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}), \quad h^*(x_1,\dots,x_d)=h(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}) \frac{\partial f^*}{\partial x_j}(x_1,\dots,x_d)=\frac{\partial f}{\partial x_{\pi(j)}}(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}) \quad 1\leq j\leq d, \frac{\partial h_i^*}{\partial x_j}(x_1,\dots,x_d)=\frac{\partial h_i}{\partial x_{\pi(j)}}(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}) \quad 1\leq i\leq n, \quad 1\leq j\leq d f,h f^*,h^* a^*=(a_{\pi(1)},\dots,a_{\pi(n)}) \frac{\partial f^*}{\partial x_j}(a^*)=\frac{\partial f}{\partial x_{\pi(j)}}(a) \quad 1\leq j\leq d, \frac{\partial h_i^*}{\partial x_j}(a^*)=\frac{\partial h_i}{\partial x_{\pi(j)}}(a)  \quad 1\leq i\leq n, \quad 1\leq j\leq d f^{*'}(a^*) h^{*'}(a^*) f'(a) h'(a) \pi f^* a^* h^*(a^*)=C (1) f,h a (1) f^*,h^* a^* n h'(a) C=0 h h-C m=d-n \mathbb{R}^d=\mathbb{R}^{n+m} (x,y) x\in\mathbb{R}^n y\in\mathbb{R}^m h a=(a_x,a_y) U\subset \mathbb{R}^{n+m} W\subset \mathbb R^m (a_x,a_y)\in U a_y\in W g:W\to\mathbb R^n g(a_y)=a_x h(g(y),y)=0 y\in W \{(g(y),y):y\in W\}\subset M \quad \quad (2) F:W\to \mathbb R F(y)=f(g(y),y) W F'(y)=f'(g(y),y)\begin{bmatrix} g'(y)\\ I_m \end{bmatrix} a_y\in W g a_y (2) F a_y r>0 f|_{M\cap B_r(a)} a \delta>0 (g(y),y)\in M\cap B_r(a) y\in W\cap B_{\delta}(a_y) f'(a)=[f_x'(a) \quad f_y'(a)] 0=F'(a_y)=[f_x'(a) \quad f_y'(a)]\begin{bmatrix} g'(a_y)\\ I_m \end{bmatrix}=f_x'(a)g'(a_y)+ f_y'(a)  \quad \quad (3) H:W\to\mathbb R^{n+m} H(y)=h(g(y),y) W H'(y)=h'(g(y),y)\begin{bmatrix} g'(y)\\ I_m \end{bmatrix} H=0 g 0=H'(a_y)=[h_x'(a) \quad h_y'(a)]\begin{bmatrix} g'(a_y)\\ I_m \end{bmatrix}=h_x'(a)g'(a_y)+ h_y'(a) \quad \quad (4) h'(a)=[h_x'(a) \quad h_y'(a)] n h'(a) h_x'(a) (4) g'(a_y)=-[h_x'(a)]^{-1}h_y'(a) \quad \quad (5) (5) (3) f_y'(a)  f_y'(a) =\Lambda h_y'(a) \quad \quad (6) \Lambda:=f_x'(a)[h_x'(a)]^{-1}:=[\lambda_1,\dots,\lambda_n] f'_x(a)=f'_x(a)[h_x'(a)]^{-1}h_x'(a)=\Lambda h_x'(a) \quad \quad (7) (6) (7) f'(a)=[f'_x(a) \quad f'_y(a)]=\Lambda [h_x'(a)\quad  h_y'(a)]=\Lambda h'(a) (1)","['real-analysis', 'multivariable-calculus', 'derivatives', 'optimization', 'lagrange-multiplier']"
86,"Is $f(x,y)\in L^1([0,1]\times[0,1])$?",Is ?,"f(x,y)\in L^1([0,1]\times[0,1])","I have to study the Lebesgue integrability of $f(x,y)=\dfrac{x-y}{(x+y)^3}$ on $Q=[0,1]^2\subseteq\mathbb R^2$ . The sign of $f$ is variable in $Q$ , so I studied $|f(x,y)|=\dfrac{|x-y|}{(x+y)^3}$ in order to apply Tonelli's theorem (because $|f|$ is positive and measurable in $Q$ and I can evaluate the iterated integrals). $$\iint_{Q}|f(x,y)|dxdy=\iint_{[0,1]^2}|f(x,y)|dxdy=\int_0^1dy\int_0^1\dfrac{|x-y|}{(x+y)^3}dx$$ with the change of variable $\begin{cases}u=x-y\\v=x+y \end{cases}\implies\begin{cases}x=\dfrac{u+v}{2} \\y=\dfrac{v-u}{2}\end{cases}$ I have $|\det(J)|=\dfrac{1}{4}$ , with $\begin{cases}0<u+v<2\\0<v-u<2 \end{cases}$ as new set  of integration $\tilde Q$ in $(u,v)$ -plane. So we obtain $$\dfrac{1}{4}\Bigg(\int_{-1}^0|u|du\int_{-u}^{2+u}\dfrac{dv}{v^3}+\int_{0}^1|u|du\int_{u}^{2-u}\dfrac{dv}{v^3}\Bigg)=$$ $$=\dfrac{1}{4}\Bigg(\int_{-1}^0\dfrac{u}{2(2+u)^2}-\dfrac{1}{2u}du+\int_{0}^1\dfrac{-u}{2(2-u)^2}+\dfrac{1}{2u}du\Bigg)$$ and I think that this shows that $f\notin L^1([0,1]^2)$ .","I have to study the Lebesgue integrability of on . The sign of is variable in , so I studied in order to apply Tonelli's theorem (because is positive and measurable in and I can evaluate the iterated integrals). with the change of variable I have , with as new set  of integration in -plane. So we obtain and I think that this shows that .","f(x,y)=\dfrac{x-y}{(x+y)^3} Q=[0,1]^2\subseteq\mathbb R^2 f Q |f(x,y)|=\dfrac{|x-y|}{(x+y)^3} |f| Q \iint_{Q}|f(x,y)|dxdy=\iint_{[0,1]^2}|f(x,y)|dxdy=\int_0^1dy\int_0^1\dfrac{|x-y|}{(x+y)^3}dx \begin{cases}u=x-y\\v=x+y \end{cases}\implies\begin{cases}x=\dfrac{u+v}{2} \\y=\dfrac{v-u}{2}\end{cases} |\det(J)|=\dfrac{1}{4} \begin{cases}0<u+v<2\\0<v-u<2 \end{cases} \tilde Q (u,v) \dfrac{1}{4}\Bigg(\int_{-1}^0|u|du\int_{-u}^{2+u}\dfrac{dv}{v^3}+\int_{0}^1|u|du\int_{u}^{2-u}\dfrac{dv}{v^3}\Bigg)= =\dfrac{1}{4}\Bigg(\int_{-1}^0\dfrac{u}{2(2+u)^2}-\dfrac{1}{2u}du+\int_{0}^1\dfrac{-u}{2(2-u)^2}+\dfrac{1}{2u}du\Bigg) f\notin L^1([0,1]^2)","['multivariable-calculus', 'solution-verification']"
87,Constant Rank Theorem without using the Inverse Function Theorem,Constant Rank Theorem without using the Inverse Function Theorem,,"Background Scanning Wikipedia's article on the Implicit Function Theorem I came across the following comment in the ""See Also"" section: Both the implicit function theorem and the inverse function theorem can be seen as special cases of the constant rank theorem. The article on the Inverse Function Theorem makes a similar claim (see here ): The inverse function theorem (and the implicit function theorem) can be seen as a special case of the constant rank theorem, (...) ${}^\text{[11]}$ . Question 1: Is this implying that the Constant Rank Theorem can be proved without using the Inverse Function Theorem? Main question If the answer to question 1 is ""yes"" then Question 2: How can you proof the Constant Rank Theorem without using either the Inverse- or the Implicit Function Theorem? For the sake of completeness, the Constant Rank Theorem roughly asserts that if the rank of a $C^r$ function $f:\mathbb{R}^m\to\mathbb{R}^n$ is constant $k$ on an open neighbourhood $U$ of $x_0$ , then there are $C^r$ diffeomorphisms $\alpha,\beta$ such that $$\alpha \circ f \circ\beta (x_1,...,x_m) = (x_1,...,x_k,0,...,0)$$ near $x_0$ . (See for instance John M. Lee, Introduction to Smooth Manifolds , p. 81.) Wikipedia's reference Wikipedia's reference for the second claim is the following: [11]: Boothby, William M. (1986). An Introduction to Differentiable Manifolds and Riemannian Geometry . pp. 46-50. As you can see from this screenshot , Boothby uses the Inverse Function Theorem in his proof of the Constant Rank Theorem.","Background Scanning Wikipedia's article on the Implicit Function Theorem I came across the following comment in the ""See Also"" section: Both the implicit function theorem and the inverse function theorem can be seen as special cases of the constant rank theorem. The article on the Inverse Function Theorem makes a similar claim (see here ): The inverse function theorem (and the implicit function theorem) can be seen as a special case of the constant rank theorem, (...) . Question 1: Is this implying that the Constant Rank Theorem can be proved without using the Inverse Function Theorem? Main question If the answer to question 1 is ""yes"" then Question 2: How can you proof the Constant Rank Theorem without using either the Inverse- or the Implicit Function Theorem? For the sake of completeness, the Constant Rank Theorem roughly asserts that if the rank of a function is constant on an open neighbourhood of , then there are diffeomorphisms such that near . (See for instance John M. Lee, Introduction to Smooth Manifolds , p. 81.) Wikipedia's reference Wikipedia's reference for the second claim is the following: [11]: Boothby, William M. (1986). An Introduction to Differentiable Manifolds and Riemannian Geometry . pp. 46-50. As you can see from this screenshot , Boothby uses the Inverse Function Theorem in his proof of the Constant Rank Theorem.","{}^\text{[11]} C^r f:\mathbb{R}^m\to\mathbb{R}^n k U x_0 C^r \alpha,\beta \alpha \circ f \circ\beta (x_1,...,x_m) = (x_1,...,x_k,0,...,0) x_0","['real-analysis', 'multivariable-calculus', 'differential-geometry']"
88,Calculate or evaluate double trigonometric integral,Calculate or evaluate double trigonometric integral,,"I would like to calculate: $$\int_{0}^{\pi} \left( \int_{0}^{\pi} \frac{\sin\varphi}{1+\left(\cos2\theta-\cos\varphi \right)^2} \cos\left(j\varphi\right) d\varphi \right) \cos\left(2^i \theta\right)d\theta$$ where $i,j\in\mathbb N$ . I tried with integration by parts for: $$\int_{0}^{\pi} \frac{\sin\varphi}{1+\left(\cos2\theta-\cos\varphi \right)^2} \cos\left(j\varphi\right) d\varphi $$ but I came up with nothing. Maybe it is unmanageable. What do you think?",I would like to calculate: where . I tried with integration by parts for: but I came up with nothing. Maybe it is unmanageable. What do you think?,"\int_{0}^{\pi} \left( \int_{0}^{\pi} \frac{\sin\varphi}{1+\left(\cos2\theta-\cos\varphi \right)^2} \cos\left(j\varphi\right) d\varphi \right) \cos\left(2^i \theta\right)d\theta i,j\in\mathbb N \int_{0}^{\pi} \frac{\sin\varphi}{1+\left(\cos2\theta-\cos\varphi \right)^2} \cos\left(j\varphi\right) d\varphi ","['calculus', 'integration', 'multivariable-calculus']"
89,Given a directional derivative to find a point,Given a directional derivative to find a point,,"Let $ f(x,y,z)$ be differentiable, and assume that $$ f(x, y, x^2 + y) = 3x - y $$ (for all values of $x,y$ ). Also, given the direct-derivative of the point $A=(0, 12, 12)$ and the direction vector is $(1, 0, 1)$ is equal to $3$ , I need to find the gradient of $f$ at $A$ . I have no idea how to even start, because $f$ is not given explicitly, only the derivative — but again, not explicitly for $f(x,y,z)$ , but for this weird combination $f(x, y, x^2 + y)$ . What should I do?","Let be differentiable, and assume that (for all values of ). Also, given the direct-derivative of the point and the direction vector is is equal to , I need to find the gradient of at . I have no idea how to even start, because is not given explicitly, only the derivative — but again, not explicitly for , but for this weird combination . What should I do?"," f(x,y,z)  f(x, y, x^2 + y) = 3x - y  x,y A=(0, 12, 12) (1, 0, 1) 3 f A f f(x,y,z) f(x, y, x^2 + y)",[]
90,Weak formulation of $\Delta^{2}u=f$ with boundary conditions $u=\Delta u =0$.,Weak formulation of  with boundary conditions .,\Delta^{2}u=f u=\Delta u =0,"Let $U\subset\mathbb{R}^{n}$ be an open subset (with sufficiently smooth boundary) and consider the boundary value problem $$\begin{cases}\Delta^{2}u=f \ \text{on $U$} \\ u=\Delta u=0 \ \text{on $\partial U$} \end{cases}.$$ To find a unique weak solution, I want to apply the Riesz representation theorem or Lax-Milgram theorem to an appropriate Sobolev space. My problem is finding this appropriate Sobolev space, that is, I'm trying to find the weak formulation of the above problem. I multiplied the above equation with a sufficiently smooth test function $v$ and found that \begin{align*} \int_{U}\Delta^{2}u \cdot v&=\int_{\partial U}\frac{\partial\Delta u}{\partial n}\cdot v-\int_{\partial U}\Delta u\cdot\frac{\partial v}{\partial n}+\int_{U}\Delta u\cdot\Delta v\\ &=\int_{\partial U}\frac{\partial\Delta u}{\partial n}\cdot v+\int_{U}\Delta u\cdot\Delta v. \end{align*} I have seen that $H^{2}(U)\cap H_{0}^{1}(U)$ is the Sobolev space we are looking for, but how do I deduce that? Is there a general approach? Of course we need $v\in H^{2}(U)$ in order to apply $\Delta$ to it. Furthermore, it looks like we are forcing the integral on the boundary to be zero by requiring $v=0$ on $\partial U$ , i.e. $v\in H_{0}^{1}(U)$ . Also, in order to apply the above mentioned theorems, one needs $u$ and $v$ to lie in the same Sobolev space. But if $u\in H^{2}(U)\cap H_{0}^{1}(U)$ is a weak solution, then we do not have $\Delta u=0$ on $\partial U$ .","Let be an open subset (with sufficiently smooth boundary) and consider the boundary value problem To find a unique weak solution, I want to apply the Riesz representation theorem or Lax-Milgram theorem to an appropriate Sobolev space. My problem is finding this appropriate Sobolev space, that is, I'm trying to find the weak formulation of the above problem. I multiplied the above equation with a sufficiently smooth test function and found that I have seen that is the Sobolev space we are looking for, but how do I deduce that? Is there a general approach? Of course we need in order to apply to it. Furthermore, it looks like we are forcing the integral on the boundary to be zero by requiring on , i.e. . Also, in order to apply the above mentioned theorems, one needs and to lie in the same Sobolev space. But if is a weak solution, then we do not have on .","U\subset\mathbb{R}^{n} \begin{cases}\Delta^{2}u=f \ \text{on U} \\ u=\Delta u=0 \ \text{on \partial U} \end{cases}. v \begin{align*}
\int_{U}\Delta^{2}u \cdot v&=\int_{\partial U}\frac{\partial\Delta u}{\partial n}\cdot v-\int_{\partial U}\Delta u\cdot\frac{\partial v}{\partial n}+\int_{U}\Delta u\cdot\Delta v\\
&=\int_{\partial U}\frac{\partial\Delta u}{\partial n}\cdot v+\int_{U}\Delta u\cdot\Delta v.
\end{align*} H^{2}(U)\cap H_{0}^{1}(U) v\in H^{2}(U) \Delta v=0 \partial U v\in H_{0}^{1}(U) u v u\in H^{2}(U)\cap H_{0}^{1}(U) \Delta u=0 \partial U","['multivariable-calculus', 'partial-differential-equations', 'sobolev-spaces', 'boundary-value-problem', 'weak-derivatives']"
91,"Evans Partial Differential Equations, chapter 7 exercise 1 (uniqueness of a regular solution to the heat equation with Neumann boundary conditions)","Evans Partial Differential Equations, chapter 7 exercise 1 (uniqueness of a regular solution to the heat equation with Neumann boundary conditions)",,"I would like to know how to solve the first exercise of chapter 7 of Evans' Partial Differential Equations , second edition. The problem goes like this: Let $U\subset\mathbb{R}^n$ be an open and bounded set, with smooth boundary, and let $T>0$ . Prove that there is at most one smooth solution of this initial/boundary value problem for the heat equation with Neumann boundary conditions $$\begin{cases}u_t-\Delta u=f&\text{in }U_T\\ \frac{\partial u}{\partial\nu}=0&\text{in }\partial U\times[0,T]\\ u=g&\text{in }U\times\{t=0\} \end{cases}$$ This is what I've got so far: Suppose $u$ and $v$ are two regular solutions of the given problem. Then $u-v$ is a regular solution to the problem $$\begin{cases}u_t-\Delta u=0&\text{in }U_T\\ \frac{\partial u}{\partial\nu}=0&\text{in }\partial U\times[0,T]\\ u=0&\text{in }U\times\{t=0\} \end{cases}$$ If $u$ is a solution to the last problem, on the one hand, we have that $$\begin{aligned} \int_{U\times(0,T)}\Delta udx&=\int_0^T\left(\int_U\Delta u(x,\tau)dx\right)d\tau&\text{ (Fubini's theorem)}\\ &=\int_0^T\left(\int_{\partial U}\frac{\partial u}{\partial\nu}(x,\tau)dS\right)d\tau&\text{ (Green's formula)}\\ &=\int_{\partial U\times(0,T)}\frac{\partial u}{\partial\nu}dS&\text{ (Fubini's theorem)}\\ &=0 \end{aligned}$$ Because, by hypothesis, we know that $\partial u/\partial\nu$ is identically equal to zero in $\partial U\times[0,T]$ . While on the other hand $$\int_{U\times(0,T)}u_tdx=\int_{U\times(0,T)}\Delta udx=0\Rightarrow u_t=0\text{ in }U\times(0,T)\Rightarrow u\text{ is constant in }t\in (0,T)$$ Since $u=0$ in $U\times\{t=0\}$ and we are supposing this is a regular solution, by continuity we can conclude that $u$ is identically zero in $U\times [0,T]$ . But recall that the difference of two regular solutions of the first problem is a solution to the second, so if there is a regular solution to the original problem, it must be unique. What do you think? Are there flaws in my proof?  Is there another method to prove this result? Thanks in advance for your help.","I would like to know how to solve the first exercise of chapter 7 of Evans' Partial Differential Equations , second edition. The problem goes like this: Let be an open and bounded set, with smooth boundary, and let . Prove that there is at most one smooth solution of this initial/boundary value problem for the heat equation with Neumann boundary conditions This is what I've got so far: Suppose and are two regular solutions of the given problem. Then is a regular solution to the problem If is a solution to the last problem, on the one hand, we have that Because, by hypothesis, we know that is identically equal to zero in . While on the other hand Since in and we are supposing this is a regular solution, by continuity we can conclude that is identically zero in . But recall that the difference of two regular solutions of the first problem is a solution to the second, so if there is a regular solution to the original problem, it must be unique. What do you think? Are there flaws in my proof?  Is there another method to prove this result? Thanks in advance for your help.","U\subset\mathbb{R}^n T>0 \begin{cases}u_t-\Delta u=f&\text{in }U_T\\
\frac{\partial u}{\partial\nu}=0&\text{in }\partial U\times[0,T]\\
u=g&\text{in }U\times\{t=0\}
\end{cases} u v u-v \begin{cases}u_t-\Delta u=0&\text{in }U_T\\
\frac{\partial u}{\partial\nu}=0&\text{in }\partial U\times[0,T]\\
u=0&\text{in }U\times\{t=0\}
\end{cases} u \begin{aligned}
\int_{U\times(0,T)}\Delta udx&=\int_0^T\left(\int_U\Delta u(x,\tau)dx\right)d\tau&\text{ (Fubini's theorem)}\\
&=\int_0^T\left(\int_{\partial U}\frac{\partial u}{\partial\nu}(x,\tau)dS\right)d\tau&\text{ (Green's formula)}\\
&=\int_{\partial U\times(0,T)}\frac{\partial u}{\partial\nu}dS&\text{ (Fubini's theorem)}\\
&=0
\end{aligned} \partial u/\partial\nu \partial U\times[0,T] \int_{U\times(0,T)}u_tdx=\int_{U\times(0,T)}\Delta udx=0\Rightarrow u_t=0\text{ in }U\times(0,T)\Rightarrow u\text{ is constant in }t\in (0,T) u=0 U\times\{t=0\} u U\times [0,T]","['integration', 'multivariable-calculus', 'partial-differential-equations', 'solution-verification', 'parabolic-pde']"
92,Mixed partial derivatives of planar functions converging to delta distribution,Mixed partial derivatives of planar functions converging to delta distribution,,"Given a sequence $(f_k)_{k\in\mathbb{N}}\subset C^2(\mathbb{R}^2)$ of strictly positive functions $f_k\equiv f_k(x,y)$ with $\|f_k(x,\cdot)\|_{L^1}=1$ for all $x\in\mathbb{R}$ , such that for each $x\in\mathbb{R}$ we have $$\tag{1} \lim_{k\rightarrow\infty} f_k(x,\cdot) = \delta_x \qquad\text{ in the distributional sense},$$ with $\delta_x$ the delta distribution at $x$ . (The 'heat kernel' $f_k:=\frac{k}{4\pi}e^{-k(x-y)^2/4}$ is an example of such a sequence.) Question: Is the above enough, I wonder, to infer that for infinitely many $k\in\mathbb{N}$ , the mixed derivatives $\partial_y\partial_x\log(f_k)$ vanish (almost) nowhere on the diagonal $\Delta:=\{(x,x)\mid x\in\mathbb{R}\}$ ? Intuition : Let $k\in\mathbb{N}$ and $x_0\in\mathbb{R}$ be fixed, and $\delta>0$ be small. Considering the restriction of $f_k$ to the square $\mathcal{R}:= B_\delta(x_0)\times B_\delta(x_0)$ with $y$ -sections $\mathcal{R}_y:= B_\delta(x_0)\times\{y\}$ , we by $(1)$ find the functions $\phi_k^y := \left.f_k\right|_{\mathcal{R}_y}$ to 'bulk' increasingly (with $k$ ) at the point $(y,y)$ and 'flatten sharply' on $\mathcal{R}_y\setminus\{(y,y)\}$ . Consequently, the (monotonic) transformations $\varphi_k^y:=\log(\phi^y_k)$ show a 'rapid decay (as $k\rightarrow\infty$ ) below zero' on $\mathcal{R}_y\setminus\{(y,y)\}$ . This suggests that (for $k$ large enough) we have $\psi_k^y:= \left.\partial_x(\varphi_k^y)\right|_{x=x_0}>0$ for $y>x_0$ , and $\psi_k^y<0$ for $y<x_0$ , so that $\left.\partial_y[\partial_x\log(f_k)]\right|_{(x,y)=(x_0, x_0)} = \partial_y(\left.\psi_k^y)\right|_{y=x_0} > 0$ , provided that $(1)$ guarantees that $\lim_{h\rightarrow 0^+}\frac{\psi_k^{x_0+h} - \psi_k^{x_0-h}}{2h}>0.$ Do you see a way to put this intuition into a rigorous proof? (Or is it wrong altogether and the claim doesn't hold?)","Given a sequence of strictly positive functions with for all , such that for each we have with the delta distribution at . (The 'heat kernel' is an example of such a sequence.) Question: Is the above enough, I wonder, to infer that for infinitely many , the mixed derivatives vanish (almost) nowhere on the diagonal ? Intuition : Let and be fixed, and be small. Considering the restriction of to the square with -sections , we by find the functions to 'bulk' increasingly (with ) at the point and 'flatten sharply' on . Consequently, the (monotonic) transformations show a 'rapid decay (as ) below zero' on . This suggests that (for large enough) we have for , and for , so that , provided that guarantees that Do you see a way to put this intuition into a rigorous proof? (Or is it wrong altogether and the claim doesn't hold?)","(f_k)_{k\in\mathbb{N}}\subset C^2(\mathbb{R}^2) f_k\equiv f_k(x,y) \|f_k(x,\cdot)\|_{L^1}=1 x\in\mathbb{R} x\in\mathbb{R} \tag{1} \lim_{k\rightarrow\infty} f_k(x,\cdot) = \delta_x \qquad\text{ in the distributional sense}, \delta_x x f_k:=\frac{k}{4\pi}e^{-k(x-y)^2/4} k\in\mathbb{N} \partial_y\partial_x\log(f_k) \Delta:=\{(x,x)\mid x\in\mathbb{R}\} k\in\mathbb{N} x_0\in\mathbb{R} \delta>0 f_k \mathcal{R}:= B_\delta(x_0)\times B_\delta(x_0) y \mathcal{R}_y:= B_\delta(x_0)\times\{y\} (1) \phi_k^y := \left.f_k\right|_{\mathcal{R}_y} k (y,y) \mathcal{R}_y\setminus\{(y,y)\} \varphi_k^y:=\log(\phi^y_k) k\rightarrow\infty \mathcal{R}_y\setminus\{(y,y)\} k \psi_k^y:= \left.\partial_x(\varphi_k^y)\right|_{x=x_0}>0 y>x_0 \psi_k^y<0 y<x_0 \left.\partial_y[\partial_x\log(f_k)]\right|_{(x,y)=(x_0, x_0)} = \partial_y(\left.\psi_k^y)\right|_{y=x_0} > 0 (1) \lim_{h\rightarrow 0^+}\frac{\psi_k^{x_0+h} - \psi_k^{x_0-h}}{2h}>0.","['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'distribution-theory']"
93,The Euler-Lagrange equation,The Euler-Lagrange equation,,"Here's an (edited) excerpt from my Calculus textbook. I've highlighted the part I didn't understand in boldface, so could anyone explain those bits? Let us consider the integral $$ I=\int_{a}^{b} F\left(y, y^{\prime}, x\right) d x $$ where $a, b$ and the form of the function $F$ are fixed by given considerations. The curve $y(x)$ is to be chosen so as to make stationary the value of $I$ . Let us suppose that $y(x)$ is the function required to make $I$ stationary and consider making the replacement $$ y(x) \rightarrow y(x)+\alpha \eta(x) $$ where the parameter $\alpha$ is small and $\eta(x)$ is a nicely behaved arbitrary function. For the value of $I$ to be stationary with respect to these variations, we require $$ \left.\frac{d I}{d \alpha}\right|_{\alpha=0}=0 \quad \text { for all } \eta(x) $$ Expanding as a Taylor series in $\alpha$ , we obtain $$ \begin{aligned} I(y, \alpha) &=\int_{a}^{b} F\left(y+\alpha \eta, y^{\prime}+\alpha \eta^{\prime}, x\right) d x \\ &=\int_{a}^{b} F\left(y, y^{\prime}, x\right) d x+\int_{a}^{b}\left(\frac{\partial F}{\partial y} \alpha \eta+\frac{\partial F}{\partial y^{\prime}} \alpha \eta^{\prime}\right) d x+\mathrm{O}\left(\alpha^{2}\right) \end{aligned} $$ Thus, $$ \left.\frac{d I}{d \alpha}\right|_{\alpha=0}= \delta I=\int_{a}^{b}\left(\frac{\partial F}{\partial y} \eta+\frac{\partial F}{\partial y^{\prime}} \eta^{\prime}\right) d x=0. $$ where $\delta I$ denotes the first-order variation in the value of $I$ due to the variation (22.2) in the function $y(x) .$ Integrating the second term by parts this becomes $$ \left[\eta \frac{\partial F}{\partial y^{\prime}}\right]_{a}^{b}+\int_{a}^{b}\left[\frac{\partial F}{\partial y}-\frac{d}{d x}\left(\frac{\partial F}{\partial y^{\prime}}\right)\right] \eta(x) d x=0. $$ Now, if we demand that the lower end-point $a$ is fixed, while we allow variation of the end-point $b$ along the curve $h(x, y)=0$ , (Why/how is this?) we obtain through a similar analysis as above that the variation in the value of $I$ due to the arbitrary variation is given to first order by $$ \delta I=\left[\frac{\partial F}{\partial y^{\prime}} \eta\right]_{a}^{b}+\int_{a}^{b}\left(\frac{\partial F}{\partial y}-\frac{d}{d x} \frac{\partial F}{\partial y^{\prime}}\right) \eta d x+F(b) \Delta x $$ where $\Delta x$ is the displacement in the $x$ -direction of the upper end-point,  and $F(b)$ is the value of $F$ at $x=b .$ We of course require the displacement $\Delta x$ to be small. We can show that $\Delta y=\eta(b)+y^{\prime}(b) \Delta x .$ Since the upper end-point must lie on $h(x, y)=0$ (Why/how is this?) we also require that, at $x=b$ , $$ \frac{\partial h}{\partial x} \Delta x+\frac{\partial h}{\partial y} \Delta y=0. $$","Here's an (edited) excerpt from my Calculus textbook. I've highlighted the part I didn't understand in boldface, so could anyone explain those bits? Let us consider the integral where and the form of the function are fixed by given considerations. The curve is to be chosen so as to make stationary the value of . Let us suppose that is the function required to make stationary and consider making the replacement where the parameter is small and is a nicely behaved arbitrary function. For the value of to be stationary with respect to these variations, we require Expanding as a Taylor series in , we obtain Thus, where denotes the first-order variation in the value of due to the variation (22.2) in the function Integrating the second term by parts this becomes Now, if we demand that the lower end-point is fixed, while we allow variation of the end-point along the curve , (Why/how is this?) we obtain through a similar analysis as above that the variation in the value of due to the arbitrary variation is given to first order by where is the displacement in the -direction of the upper end-point,  and is the value of at We of course require the displacement to be small. We can show that Since the upper end-point must lie on (Why/how is this?) we also require that, at ,","
I=\int_{a}^{b} F\left(y, y^{\prime}, x\right) d x
 a, b F y(x) I y(x) I 
y(x) \rightarrow y(x)+\alpha \eta(x)
 \alpha \eta(x) I 
\left.\frac{d I}{d \alpha}\right|_{\alpha=0}=0 \quad \text { for all } \eta(x)
 \alpha 
\begin{aligned}
I(y, \alpha) &=\int_{a}^{b} F\left(y+\alpha \eta, y^{\prime}+\alpha \eta^{\prime}, x\right) d x \\
&=\int_{a}^{b} F\left(y, y^{\prime}, x\right) d x+\int_{a}^{b}\left(\frac{\partial F}{\partial y} \alpha \eta+\frac{\partial F}{\partial y^{\prime}} \alpha \eta^{\prime}\right) d x+\mathrm{O}\left(\alpha^{2}\right)
\end{aligned}
 
\left.\frac{d I}{d \alpha}\right|_{\alpha=0}= \delta I=\int_{a}^{b}\left(\frac{\partial F}{\partial y} \eta+\frac{\partial F}{\partial y^{\prime}} \eta^{\prime}\right) d x=0.
 \delta I I y(x) . 
\left[\eta \frac{\partial F}{\partial y^{\prime}}\right]_{a}^{b}+\int_{a}^{b}\left[\frac{\partial F}{\partial y}-\frac{d}{d x}\left(\frac{\partial F}{\partial y^{\prime}}\right)\right] \eta(x) d x=0.
 a b h(x, y)=0 I 
\delta I=\left[\frac{\partial F}{\partial y^{\prime}} \eta\right]_{a}^{b}+\int_{a}^{b}\left(\frac{\partial F}{\partial y}-\frac{d}{d x} \frac{\partial F}{\partial y^{\prime}}\right) \eta d x+F(b) \Delta x
 \Delta x x F(b) F x=b . \Delta x \Delta y=\eta(b)+y^{\prime}(b) \Delta x . h(x, y)=0 x=b 
\frac{\partial h}{\partial x} \Delta x+\frac{\partial h}{\partial y} \Delta y=0.
","['multivariable-calculus', 'calculus-of-variations', 'euler-lagrange-equation']"
94,"If $f: \mathbb{R}^2 \to \mathbb{R}$ is a continuous function with zero integral over every rectangle of area $1,$ then $f = 0.$",If  is a continuous function with zero integral over every rectangle of area  then,"f: \mathbb{R}^2 \to \mathbb{R} 1, f = 0.","If $f: \mathbb{R}^2 \to \mathbb{R}$ is a continuous function with zero integral over every rectangle of area $1,$ prove $f = 0.$ Using induction and a shifting argument for the base case, I managed to show that if $ABCD$ is a rectangle with integer area, then $$f(A) + f(C) = f(B) + f(D) \, \, (*)$$ I think this almost solves the problem, I just need to draw the right diagram now. Unfortunately, I'm having difficulties coming up with a diagram that works. I drew a $2 \times 2$ square with vertices $A, B, C, D, E, F, G, H, I,$ applied $(*)$ on all sorts of squares within this square, summed the results, and then tried to cancel as many terms on both sides as possible. However, no matter which sums I took, I would always end up with either $0 = 0$ or a trivial result of the form $(*).$ How do I find the right diagram? I tried drawing $3 \times 3, \sqrt{2} \times \sqrt{2}$ and even $\sqrt{3} \times \sqrt{3}$ squares, but ran into the same problem (at one point, I thought I had proven that $f(x,y) = f(x+\sqrt{3},y) = f(x+\sqrt{2},y),$ which would show $f$ is constant after employing the density of $\{a\sqrt{2}+b\sqrt{3} : a, b \in \mathbb{Z}\}$ in $\mathbb{R},$ but it turned out I made a mistake when adding terms in both cases). Any ideas?","If is a continuous function with zero integral over every rectangle of area prove Using induction and a shifting argument for the base case, I managed to show that if is a rectangle with integer area, then I think this almost solves the problem, I just need to draw the right diagram now. Unfortunately, I'm having difficulties coming up with a diagram that works. I drew a square with vertices applied on all sorts of squares within this square, summed the results, and then tried to cancel as many terms on both sides as possible. However, no matter which sums I took, I would always end up with either or a trivial result of the form How do I find the right diagram? I tried drawing and even squares, but ran into the same problem (at one point, I thought I had proven that which would show is constant after employing the density of in but it turned out I made a mistake when adding terms in both cases). Any ideas?","f: \mathbb{R}^2 \to \mathbb{R} 1, f = 0. ABCD f(A) + f(C) = f(B) + f(D) \, \, (*) 2 \times 2 A, B, C, D, E, F, G, H, I, (*) 0 = 0 (*). 3 \times 3, \sqrt{2} \times \sqrt{2} \sqrt{3} \times \sqrt{3} f(x,y) = f(x+\sqrt{3},y) = f(x+\sqrt{2},y), f \{a\sqrt{2}+b\sqrt{3} : a, b \in \mathbb{Z}\} \mathbb{R},","['real-analysis', 'integration', 'multivariable-calculus']"
95,Conditions for Fubini's theorem in simple terms,Conditions for Fubini's theorem in simple terms,,"Could someone explain the conditions required to satisfy Fubini's theorem in layman's terms, without going into measure theory? I'm a high school student trying to gain a simple understanding of when Fubini's theorem doesn't apply, but everything I've found online is in terms of measure spaces. Would it be possible to explain the conditions in terms of continuity and boundedness over the region of integration?","Could someone explain the conditions required to satisfy Fubini's theorem in layman's terms, without going into measure theory? I'm a high school student trying to gain a simple understanding of when Fubini's theorem doesn't apply, but everything I've found online is in terms of measure spaces. Would it be possible to explain the conditions in terms of continuity and boundedness over the region of integration?",,"['integration', 'multivariable-calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
96,"Question about Lagrange multipliers, optimization problems and KKT-points.","Question about Lagrange multipliers, optimization problems and KKT-points.",,"I am having some difficulties with optimization problems with inequality constraints. In general the problems I am given will look something like this: $$\min f(x,y,z) \\ \text{s.t.} \space \space \space g(x,y,z) \le0 \\ \space   \space \space \space h(x,y,z)=0$$ I would usually start by writing down the lagrange function $\mathcal L$ , where $\lambda$ , $\mu$ are the corresponding Lagrange multipliers. $$\mathcal L(x,y,z,\lambda,\mu)=f(x,y,z)+\lambda (h(x,y,z))+\mu(g(x,y,z))$$ In order to find the KKT-points I would solve: $$\nabla \mathcal L=0$$ which will give me one or many KKT points. Here is the problem I am having: This method seems like I am always treating the inequality as an equality. Do I ever treat the ""strictly less"" case? Sometimes I see people (lectures or youtube videos I have watched) ""ignore"" the inequality constraint, solve the problem and then checking if the inequality is satisfied at the resulting points. Why does one do that? If I am getting a negative lagrange multiplier for the inequality constraint, how do I proceed? Does it just mean there are no points that satisfy that constraint? Is there some sort of ""road map"" or strategy when dealing with problems like this (optimization with one inequality and eqality constraint)?","I am having some difficulties with optimization problems with inequality constraints. In general the problems I am given will look something like this: I would usually start by writing down the lagrange function , where , are the corresponding Lagrange multipliers. In order to find the KKT-points I would solve: which will give me one or many KKT points. Here is the problem I am having: This method seems like I am always treating the inequality as an equality. Do I ever treat the ""strictly less"" case? Sometimes I see people (lectures or youtube videos I have watched) ""ignore"" the inequality constraint, solve the problem and then checking if the inequality is satisfied at the resulting points. Why does one do that? If I am getting a negative lagrange multiplier for the inequality constraint, how do I proceed? Does it just mean there are no points that satisfy that constraint? Is there some sort of ""road map"" or strategy when dealing with problems like this (optimization with one inequality and eqality constraint)?","\min f(x,y,z) \\ \text{s.t.} \space \space \space g(x,y,z) \le0 \\ \space 
 \space \space \space h(x,y,z)=0 \mathcal L \lambda \mu \mathcal L(x,y,z,\lambda,\mu)=f(x,y,z)+\lambda (h(x,y,z))+\mu(g(x,y,z)) \nabla \mathcal L=0","['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'karush-kuhn-tucker']"
97,Is each mixed partial derivatives at one point independent of the order of differentiation ？,Is each mixed partial derivatives at one point independent of the order of differentiation ？,,"$\left(\textit{A.C.CLAIRAUT}\right)$ Suppose that $U$ is an open connected set in $\mathbf{R}^n,$ that $x_0\in U,$ and that $f：U\rightarrow \mathbf{R}.$ Its mixed second partial derivatives $f_{ji},f_{ij} (1\leq i<j \leq n) $ exist on $U$ . If $f_{ji},f_{ij}$ are continuous at $x=x_0$ , then $f_{ji}(x_0)=f_{ij}(x_0).$ By the Clairaut's theorem,the following proposition can be easily proved applying the induction method. $\textbf{Proposition}$ $U$ is an open connected set in $\mathbf{R}^n.$ If $f：U\rightarrow \mathbf{R}$ all of whose partial derivatives up to $k$ are $\underline{\text{ defined and continuous in } U }$$(i.e.f\in C^{k}(U))$ , then for any fixed $r (2\leq r\leq k),$ the value $\partial_{i_1\cdots i_r}f(x)$ of the partial derivative remains the same for any permutation of the  indices $i_1\cdots i_r (1\leq i_{1}，\cdots，i_{r}\leq n).$ $\textbf{My Question:}$ Now we consider an elementary question,slightly modify the proposition'condition：replacing by "" If $f：U\rightarrow \mathbf{R}$ all of whose partial derivatives up to order $k$ are $\underline{\text{defined in } U\text{ and continuous at } x_{0}\in U.}$ "" Whether we also get that the value of $\partial_{i_1\cdots i_r}f(x)$ at $x=x_{0}$ is independent of the order $i_1\cdots i_r  (1\leq i_{1}，\cdots，i_{r}\leq n)$ , for any fixed $r (2\leq r\leq k)$ ？When $k>2$ ,the conclusion will not holds (I think so).But I need some counterexamples to verify！","Suppose that is an open connected set in that and that Its mixed second partial derivatives exist on . If are continuous at , then By the Clairaut's theorem,the following proposition can be easily proved applying the induction method. is an open connected set in If all of whose partial derivatives up to are , then for any fixed the value of the partial derivative remains the same for any permutation of the  indices Now we consider an elementary question,slightly modify the proposition'condition：replacing by "" If all of whose partial derivatives up to order are "" Whether we also get that the value of at is independent of the order , for any fixed ？When ,the conclusion will not holds (I think so).But I need some counterexamples to verify！","\left(\textit{A.C.CLAIRAUT}\right) U \mathbf{R}^n, x_0\in U, f：U\rightarrow \mathbf{R}. f_{ji},f_{ij} (1\leq i<j \leq n)  U f_{ji},f_{ij} x=x_0 f_{ji}(x_0)=f_{ij}(x_0). \textbf{Proposition} U \mathbf{R}^n. f：U\rightarrow \mathbf{R} k \underline{\text{ defined and continuous in } U }(i.e.f\in C^{k}(U)) r (2\leq r\leq k), \partial_{i_1\cdots i_r}f(x) i_1\cdots i_r (1\leq i_{1}，\cdots，i_{r}\leq n). \textbf{My Question:} f：U\rightarrow \mathbf{R} k \underline{\text{defined in } U\text{ and continuous at } x_{0}\in U.} \partial_{i_1\cdots i_r}f(x) x=x_{0} i_1\cdots i_r  (1\leq i_{1}，\cdots，i_{r}\leq n) r (2\leq r\leq k) k>2","['real-analysis', 'calculus', 'multivariable-calculus']"
98,Intermediate value theorem for functions $f:\mathbb R \to \mathbb R^2 $,Intermediate value theorem for functions,f:\mathbb R \to \mathbb R^2 ,"Let $f:[a,b]\subset \mathbb R\to\mathbb R^2$ be a continuous function on $[a,b]$ , and differentiable on $(a,b)$ , such that $f´(t)\neq 0 $ for all $t\in(a,b)$ . Prove that there exists $\xi \in (a,b) $ and $\lambda\in\mathbb R$ such that: $$f(b)-f(a)= \lambda f´(\xi)$$ What I have so far is that by applying the one dimensional case, i get that: $$f(b)-f(a)=(b-a)(f_1´(\xi_1), f_2´(\xi_2))$$ For some $\xi_1,\ \xi_2 \in \mathbb R$ . Thus one way to prove it would be by showing that $\xi_1=\xi_2$ , and this is where I´ve no clue how to proceed.","Let be a continuous function on , and differentiable on , such that for all . Prove that there exists and such that: What I have so far is that by applying the one dimensional case, i get that: For some . Thus one way to prove it would be by showing that , and this is where I´ve no clue how to proceed.","f:[a,b]\subset \mathbb R\to\mathbb R^2 [a,b] (a,b) f´(t)\neq 0  t\in(a,b) \xi \in (a,b)  \lambda\in\mathbb R f(b)-f(a)= \lambda f´(\xi) f(b)-f(a)=(b-a)(f_1´(\xi_1), f_2´(\xi_2)) \xi_1,\ \xi_2 \in \mathbb R \xi_1=\xi_2","['real-analysis', 'calculus', 'multivariable-calculus']"
99,Alternative definitions for Completeness in $\mathbb{R}^n$,Alternative definitions for Completeness in,\mathbb{R}^n,"I'm reading my multivariable calculus lecture notes and have some questions $\dots$ Here are some Theorems mentioned in the notes: $$\boxed{\begin{align}\text{MCT (Monotone Convergence Theorem)}\\ \text{BST (Bounded Squence Theorem)}\end{align}}$$ Consider the case in $\mathbb{R}$ we have: $\text{Every bounded sequence in $\mathbb{R}$ has a subsequence that converges to a limit.}\tag*{BST}$ $\text{Every non-decreasing sequence of real numbers}\tag*{MCT}\\ \text{that is bounded above converges to a limit.}$ $\text{Any non-empty set of real numbers} \\\text{that has an upper bound must }\\\text{have a least upper bound in real numbers.}\tag*{Dedekind Completeness}$ You can check that these are all actually equivalences, in other words, that you can prove the completeness of $\mathbb{R}$ (the least upper bound property) from the MCT and that you can prove the MCT from the BST: $$\text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness}$$ Hence in $\mathbb{R}$ the notion of completeness is equivalent to the Bounded Sequence Theorem. Finally notice that the statement of the Bounded Sequence Theorem no longer requires the definition of a least upper bound (or an order between vectors) and has a generalization to the Bounded Sequence Theorem in $\mathbb{R}^n$ as above. This allows us to define completeness of $\mathbb{R}^n$ in terms of the BST. The analogue of the completeness axiom in higher dimensions thus becomes: $\mathbb{R}^n$ is complete if Every bounded sequence in $\mathbb{R}^n$ has a convergent subsequence. $\dots$ We could also show that completeness is equivalent to the $\underline{\text{Intermediate Value Theorem}}$ , or to the statement that $\underline{\text{Every absolutely convergent sequence converges}}$ . I think it's saying since for $\mathbb{R}$ we have Dedekind Completeness (the least upper bound property) , which is hard to extend to $\mathbb{R}^n$ , but BST $\Leftrightarrow$ Completeness, so we can take BST as an alternative definition for Completeness. I believe $\text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness}$ actually means: $$(\text{BST}\Leftrightarrow \text{MCT})\land(\text{BST}\Leftrightarrow\text{Completeness})\land(\text{MCT}\Leftrightarrow\text{Completeness})$$ And the note also listed the proof outline for $\mathbb{R}$ which might be: (Any cycle would work) $$(\text{BST}\Rightarrow\text{MCT})\land (\text{MCT}\Rightarrow\text{Completeness})\land(\text{Completeness}\Rightarrow\text{BST})$$ I'm not sure why it's not taking Cauchy Completeness for $\mathbb{R}^n$ , since $$\text{Cauchy Completeness}\Leftrightarrow\text{Dedekind Completeness}$$ Is MCT also an alternative definition for Completeness $?$ How does $\text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness}$ generalise to $\mathbb{R}^n?$ My attempts: $\text{Every bounded sequence in $\mathbb{R}^n$ has a subsequence that converges to a limit.}\tag*{BST}$ $\text{Every non-decreasing sequence in $\mathbb{R}^n$}\tag*{MCT}\\ \text{that is bounded above converges to a limit.}$ $\text{Every Cauchy sequence in $\mathbb{R}^n$ converges}\tag*{Cauchy Completeness}$ In order to prove $\text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness}$ , maybe take the following proof outline: $$(\text{BST}\Rightarrow\text{Completeness})\land (\text{Completeness}\Rightarrow\text{MCT})\land(\text{MCT}\Rightarrow\text{BST})$$ For BST $\Rightarrow$ Completeness: First prove two lemmas $1.$ Every Cauchy sequence are bounded in $\mathbb{R}^n$ $2.$ Every Cauchy sequence in $\mathbb{R}^n$ has a convergent subsequce are convergent. Then we take arbitrary Cauchy sequence, apply lemma $1$ that it's bounded, then apply BST so it has a subsequence that converges to a limit, finally apply lemma $2$ have it converges, hence Completeness hold. Some details for each Lemma: Lemma $1$ : Cauchy sequence in $\mathbb{R}^n$ is bounded Proof. Assume $\{a_j\}_{j=1}^\infty$ is a Cauchy sequence $$\forall\varepsilon>0,\exists N\in\mathbb{N},s.t.\forall j,k\in\mathbb{N},(j,k\ge N\rightarrow\Vert a_j-a_k\Vert <\varepsilon)$$ Show it’s bounded $$\exists r>0,s.t.\forall j\in\mathbb{N},\Vert a_j\Vert <r$$ Let $S:=\{d\in\mathbb{R}:\exists i\in [1,k],s.t.\Vert 0-a_i\Vert= d\}$ Also $M:=\max(S)$ And $r:=M+\varepsilon$ $\underline{\text{Case} 1:j\le k}$ Then we have $$j\in[1,k]\Rightarrow \Vert 0-a_j\Vert \le M$$ $$\Rightarrow \Vert a_j\Vert =\Vert 0-a_j\Vert < M+\varepsilon=r$$ $$\Rightarrow \boxed{\Vert a_j\Vert <r}$$ $\underline{\text{Case} 2:j>k}$ Since $k\in[1,k]$ we have $$a_k\le M\Rightarrow\Vert 0-a_k\Vert \le M$$ And by assumption $$\Vert aj-a_k\Vert <\varepsilon$$ Together with Triangle inequality of norm in $\mathbb{R}^n$ implies $$\Vert a_j\Vert =\Vert 0-a_j\Vert \le\Vert 0-a_k\Vert +\Vert a_k-a_j\Vert <M+\varepsilon=r$$ $$\Rightarrow\boxed{\Vert a_j\Vert <r} \tag*{$\square$}$$ Lemma $2$ : If Any Cauchy sequence in $\mathbb{R}^n$ has a convergent subsequce then that Cauchy sequence is convergent. Proof. Assume $\{a_j\}_{j=1}^\infty$ is a Cauchy sequence in $\mathbb{R}^n$ that has a convergent subsequence $$\forall\varepsilon>0,\exists N\in\mathbb{N},s.t.\forall j,k\in\mathbb{N},(j,k\ge N\rightarrow\Vert a_j-a_k\Vert <\varepsilon)$$ $$\wedge\forall\varepsilon>0,\exists J> 0,s.t.\forall i\in\mathbb{N}(i\ge J\rightarrow\Vert a_{j_i}−L\Vert <\varepsilon)$$ Show $\{a_j\}_{j=1}^\infty$ converges to the same point. $$\forall\varepsilon>0,\exists K> 0,s.t.\forall j\in\mathbb{N}(j\ge K\rightarrow\Vert a_{j}−L\Vert <\varepsilon)$$ By rearrange the assumption we have $$\forall\frac{\varepsilon}{2}>0,\exists N\in\mathbb{N},s.t.\forall j,i\in\mathbb{N},(j,i\ge N\rightarrow\Vert a_j-a_{j_i}\Vert < \frac{\varepsilon}{2})$$ $$\wedge\forall\frac{\varepsilon}{2}>0,\exists J> 0,s.t.\forall i\in\mathbb{N}(i\ge J\rightarrow\Vert a_{j_i}−L\Vert <\frac{\varepsilon}{2})$$ Let $K=\max\{N,J\}$ , so we can use both assumptions By Triangle inequality of norm in $\mathbb{R}^n$ implies $$\Vert a_j-L\Vert =\Vert a_j-a_{j_i}+a_{j_i}-L\Vert \le\Vert a_j-a_{j_i}\Vert +\Vert a_{j_i}-L\Vert <\varepsilon$$ $$\Rightarrow \boxed{\Vert a_j-L\Vert <\varepsilon}\tag*{$\square$}$$ Is the idea correct $?$ Thanks for your help.","I'm reading my multivariable calculus lecture notes and have some questions Here are some Theorems mentioned in the notes: Consider the case in we have: You can check that these are all actually equivalences, in other words, that you can prove the completeness of (the least upper bound property) from the MCT and that you can prove the MCT from the BST: Hence in the notion of completeness is equivalent to the Bounded Sequence Theorem. Finally notice that the statement of the Bounded Sequence Theorem no longer requires the definition of a least upper bound (or an order between vectors) and has a generalization to the Bounded Sequence Theorem in as above. This allows us to define completeness of in terms of the BST. The analogue of the completeness axiom in higher dimensions thus becomes: is complete if Every bounded sequence in has a convergent subsequence. We could also show that completeness is equivalent to the , or to the statement that . I think it's saying since for we have Dedekind Completeness (the least upper bound property) , which is hard to extend to , but BST Completeness, so we can take BST as an alternative definition for Completeness. I believe actually means: And the note also listed the proof outline for which might be: (Any cycle would work) I'm not sure why it's not taking Cauchy Completeness for , since Is MCT also an alternative definition for Completeness How does generalise to My attempts: In order to prove , maybe take the following proof outline: For BST Completeness: First prove two lemmas Every Cauchy sequence are bounded in Every Cauchy sequence in has a convergent subsequce are convergent. Then we take arbitrary Cauchy sequence, apply lemma that it's bounded, then apply BST so it has a subsequence that converges to a limit, finally apply lemma have it converges, hence Completeness hold. Some details for each Lemma: Lemma : Cauchy sequence in is bounded Proof. Assume is a Cauchy sequence Show it’s bounded Let Also And Then we have Since we have And by assumption Together with Triangle inequality of norm in implies Lemma : If Any Cauchy sequence in has a convergent subsequce then that Cauchy sequence is convergent. Proof. Assume is a Cauchy sequence in that has a convergent subsequence Show converges to the same point. By rearrange the assumption we have Let , so we can use both assumptions By Triangle inequality of norm in implies Is the idea correct Thanks for your help.","\dots \boxed{\begin{align}\text{MCT (Monotone Convergence Theorem)}\\
\text{BST (Bounded Squence Theorem)}\end{align}} \mathbb{R} \text{Every bounded sequence in \mathbb{R} has a subsequence that converges to a limit.}\tag*{BST} \text{Every non-decreasing sequence of real numbers}\tag*{MCT}\\
\text{that is bounded above converges to a limit.} \text{Any non-empty set of real numbers} \\\text{that has an upper bound must }\\\text{have a least upper bound in real numbers.}\tag*{Dedekind Completeness} \mathbb{R} \text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness} \mathbb{R} \mathbb{R}^n \mathbb{R}^n \mathbb{R}^n \mathbb{R}^n \dots \underline{\text{Intermediate Value Theorem}} \underline{\text{Every absolutely convergent sequence converges}} \mathbb{R} \mathbb{R}^n \Leftrightarrow \text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness} (\text{BST}\Leftrightarrow \text{MCT})\land(\text{BST}\Leftrightarrow\text{Completeness})\land(\text{MCT}\Leftrightarrow\text{Completeness}) \mathbb{R} (\text{BST}\Rightarrow\text{MCT})\land (\text{MCT}\Rightarrow\text{Completeness})\land(\text{Completeness}\Rightarrow\text{BST}) \mathbb{R}^n \text{Cauchy Completeness}\Leftrightarrow\text{Dedekind Completeness} ? \text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness} \mathbb{R}^n? \text{Every bounded sequence in \mathbb{R}^n has a subsequence that converges to a limit.}\tag*{BST} \text{Every non-decreasing sequence in \mathbb{R}^n}\tag*{MCT}\\
\text{that is bounded above converges to a limit.} \text{Every Cauchy sequence in \mathbb{R}^n converges}\tag*{Cauchy Completeness} \text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness} (\text{BST}\Rightarrow\text{Completeness})\land (\text{Completeness}\Rightarrow\text{MCT})\land(\text{MCT}\Rightarrow\text{BST}) \Rightarrow 1. \mathbb{R}^n 2. \mathbb{R}^n 1 2 1 \mathbb{R}^n \{a_j\}_{j=1}^\infty \forall\varepsilon>0,\exists N\in\mathbb{N},s.t.\forall j,k\in\mathbb{N},(j,k\ge N\rightarrow\Vert a_j-a_k\Vert <\varepsilon) \exists r>0,s.t.\forall j\in\mathbb{N},\Vert a_j\Vert <r S:=\{d\in\mathbb{R}:\exists i\in [1,k],s.t.\Vert 0-a_i\Vert= d\} M:=\max(S) r:=M+\varepsilon \underline{\text{Case} 1:j\le k} j\in[1,k]\Rightarrow \Vert 0-a_j\Vert \le M \Rightarrow \Vert a_j\Vert =\Vert 0-a_j\Vert < M+\varepsilon=r \Rightarrow \boxed{\Vert a_j\Vert <r} \underline{\text{Case} 2:j>k} k\in[1,k] a_k\le M\Rightarrow\Vert 0-a_k\Vert \le M \Vert aj-a_k\Vert <\varepsilon \mathbb{R}^n \Vert a_j\Vert =\Vert 0-a_j\Vert \le\Vert 0-a_k\Vert +\Vert a_k-a_j\Vert <M+\varepsilon=r \Rightarrow\boxed{\Vert a_j\Vert <r} \tag*{\square} 2 \mathbb{R}^n \{a_j\}_{j=1}^\infty \mathbb{R}^n \forall\varepsilon>0,\exists N\in\mathbb{N},s.t.\forall j,k\in\mathbb{N},(j,k\ge N\rightarrow\Vert a_j-a_k\Vert <\varepsilon) \wedge\forall\varepsilon>0,\exists J> 0,s.t.\forall i\in\mathbb{N}(i\ge J\rightarrow\Vert a_{j_i}−L\Vert <\varepsilon) \{a_j\}_{j=1}^\infty \forall\varepsilon>0,\exists K> 0,s.t.\forall j\in\mathbb{N}(j\ge K\rightarrow\Vert a_{j}−L\Vert <\varepsilon) \forall\frac{\varepsilon}{2}>0,\exists N\in\mathbb{N},s.t.\forall j,i\in\mathbb{N},(j,i\ge N\rightarrow\Vert a_j-a_{j_i}\Vert < \frac{\varepsilon}{2}) \wedge\forall\frac{\varepsilon}{2}>0,\exists J> 0,s.t.\forall i\in\mathbb{N}(i\ge J\rightarrow\Vert a_{j_i}−L\Vert <\frac{\varepsilon}{2}) K=\max\{N,J\} \mathbb{R}^n \Vert a_j-L\Vert =\Vert a_j-a_{j_i}+a_{j_i}-L\Vert \le\Vert a_j-a_{j_i}\Vert +\Vert a_{j_i}-L\Vert <\varepsilon \Rightarrow \boxed{\Vert a_j-L\Vert <\varepsilon}\tag*{\square} ?","['real-analysis', 'general-topology', 'multivariable-calculus', 'proof-verification', 'definition']"
