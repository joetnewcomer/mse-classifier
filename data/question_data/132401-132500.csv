,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Differential equation trouble,Differential equation trouble,,"I am trying to solve the following differential equation: $$\frac{\mathrm{d}y}{\mathrm{d}x}=2(2x+y)^2$$ If we make the substitution $z=2x+y$, then we get: $$\frac{\mathrm{d}z}{\mathrm{d}x}=2+\frac{\mathrm{d}y}{\mathrm{d}x}=2+2z^{2}$$ This is a separable ODE, and so we get $$\frac{\mathrm{d}z}{2+2z^{2}}=\mathrm{d}x \implies \int\frac{\mathrm{d}z}{2+2z^{2}}=\int\mathrm{d}x \implies \frac{1}{2}\tan^{-1}(z)=x+C$$ Rearranging we get: $$z=\tan(2x+C) \implies y=\tan(2x+C)-2x$$ However plugging the ODE into Mathematica gives: $$y(x)=\frac{2}{4Ce^{4ix}-i}-2x-i$$ And I can't see any way to reconcile these two results? Have I made an assumption that I'm unaware of at some point throughout my solution, or have I done something completely wrong? P.S: Code for Mathematica: FullSimplify[DSolve[y'[x] == 2 (2 x + y[x])^2, y[x], x]]","I am trying to solve the following differential equation: $$\frac{\mathrm{d}y}{\mathrm{d}x}=2(2x+y)^2$$ If we make the substitution $z=2x+y$, then we get: $$\frac{\mathrm{d}z}{\mathrm{d}x}=2+\frac{\mathrm{d}y}{\mathrm{d}x}=2+2z^{2}$$ This is a separable ODE, and so we get $$\frac{\mathrm{d}z}{2+2z^{2}}=\mathrm{d}x \implies \int\frac{\mathrm{d}z}{2+2z^{2}}=\int\mathrm{d}x \implies \frac{1}{2}\tan^{-1}(z)=x+C$$ Rearranging we get: $$z=\tan(2x+C) \implies y=\tan(2x+C)-2x$$ However plugging the ODE into Mathematica gives: $$y(x)=\frac{2}{4Ce^{4ix}-i}-2x-i$$ And I can't see any way to reconcile these two results? Have I made an assumption that I'm unaware of at some point throughout my solution, or have I done something completely wrong? P.S: Code for Mathematica: FullSimplify[DSolve[y'[x] == 2 (2 x + y[x])^2, y[x], x]]",,"['calculus', 'ordinary-differential-equations', 'trigonometry']"
1,How to reduce higher order linear ODE to a system of first order ODE?,How to reduce higher order linear ODE to a system of first order ODE?,,"Is there any general and systematic way of reducing the higher order linear ODE to a system of first order ODE? For example, assume we have $a_3x^{(3)}+a_2x^{(2)}+a_1x^{(1)}+a_0x=0$, then how do we convert this into matrix form(a system of first order ODE). And after we solve the system of equation, how to combine them into our final solution $x(t)$? Thanks for helping me out!","Is there any general and systematic way of reducing the higher order linear ODE to a system of first order ODE? For example, assume we have $a_3x^{(3)}+a_2x^{(2)}+a_1x^{(1)}+a_0x=0$, then how do we convert this into matrix form(a system of first order ODE). And after we solve the system of equation, how to combine them into our final solution $x(t)$? Thanks for helping me out!",,['ordinary-differential-equations']
2,Schwartz kernel theorem in the case the distributions are induced by smooth functions..,Schwartz kernel theorem in the case the distributions are induced by smooth functions..,,"how can I show that if $A:C^\infty(\mathbb T^n)\rightarrow C^\infty(\mathbb T^n)$ is a continuous linear operators then there is a unique linear and continuous operator $K_A: C^\infty(\mathbb T^n\times \mathbb T^n)\rightarrow \mathbb C$ such that, $$\langle A\varphi, \psi\rangle=\langle K_A, \psi\otimes \varphi\rangle.$$ Remember that if $u\in C^\infty(\mathbb T^n)$ then $u$ induces a distribution by the pairing: $$\langle u, \phi\rangle=\int_{\mathbb T^n} u(x)\phi(x)\ dx.$$ Indeed, I don't know if this result holds, that would be a version of the Schwartz kernel theorem in the case the distributions are induced by smooth functions... Can anyone help me...","how can I show that if $A:C^\infty(\mathbb T^n)\rightarrow C^\infty(\mathbb T^n)$ is a continuous linear operators then there is a unique linear and continuous operator $K_A: C^\infty(\mathbb T^n\times \mathbb T^n)\rightarrow \mathbb C$ such that, $$\langle A\varphi, \psi\rangle=\langle K_A, \psi\otimes \varphi\rangle.$$ Remember that if $u\in C^\infty(\mathbb T^n)$ then $u$ induces a distribution by the pairing: $$\langle u, \phi\rangle=\int_{\mathbb T^n} u(x)\phi(x)\ dx.$$ Indeed, I don't know if this result holds, that would be a version of the Schwartz kernel theorem in the case the distributions are induced by smooth functions... Can anyone help me...",,"['analysis', 'ordinary-differential-equations', 'distribution-theory']"
3,$\omega$- and $\alpha$-limit sets,- and -limit sets,\omega \alpha,"I'm trying to identify the $\omega$-limit sets and $\alpha$-limit sets of $r' = r-r^2$, and $\theta' = 1$. I note that the $\omega$-limit sets are essentially sinks and sources are $\alpha$-limit sets, but am confused on the procedure of such. Also, how can we use this concept to tackle: $r' = r^3 - 3r^2 + 2r$, and $\theta' = 1$","I'm trying to identify the $\omega$-limit sets and $\alpha$-limit sets of $r' = r-r^2$, and $\theta' = 1$. I note that the $\omega$-limit sets are essentially sinks and sources are $\alpha$-limit sets, but am confused on the procedure of such. Also, how can we use this concept to tackle: $r' = r^3 - 3r^2 + 2r$, and $\theta' = 1$",,['ordinary-differential-equations']
4,Convert $\frac{d^2y}{dx^2}+x^2y=0$ to Bessel equivalent and show that its solution is $\sqrt x(AJ_{1/4}+BJ_{-1/4})$,Convert  to Bessel equivalent and show that its solution is,\frac{d^2y}{dx^2}+x^2y=0 \sqrt x(AJ_{1/4}+BJ_{-1/4}),"I have been following the thread "" Convert Airy's Equation $y''-xy=0$  to Bessel equation $$t^2u''+tu'+(t^2-c^2)u$$ "" but I can't join the dots to a solve similar equation $y''+x^2y=0$ so as to obtain a solution of the form $$\sqrt x\left(AJ_\frac{1}{4}+BJ_{-\frac{1}{4}}\right)$$ I actually get an equation that looks this way $$t^2\frac{du}{dt}+t\frac{du}{dt}+(t^2+\frac{5}{64})u$$ The above equation can not yield the desired solution. Please help me to clearly see this. Thank you.","I have been following the thread "" Convert Airy's Equation $y''-xy=0$  to Bessel equation $$t^2u''+tu'+(t^2-c^2)u$$ "" but I can't join the dots to a solve similar equation $y''+x^2y=0$ so as to obtain a solution of the form $$\sqrt x\left(AJ_\frac{1}{4}+BJ_{-\frac{1}{4}}\right)$$ I actually get an equation that looks this way $$t^2\frac{du}{dt}+t\frac{du}{dt}+(t^2+\frac{5}{64})u$$ The above equation can not yield the desired solution. Please help me to clearly see this. Thank you.",,['ordinary-differential-equations']
5,On the differential equation $y''+y=0$,On the differential equation,y''+y=0,"Consider the differential equation $$\frac{d^{2}y}{dx^{2}}+y=0$$ with initial conditions $y(0)=0$ and $y'(0)=1$. The solution is well known - $y=\sin(x)$. I know how to derive this solution, since the given equation is a linear differential equation with constant coefficients, and characteristic equation $z^{2}+1=0$. I also know that this identity, combined with the initial conditions, allows us to compute $y^{(n)}(0)$ and thus the Maclaurin series of $y$, which coincides with the Maclaurin series of $\sin(x)$.  Neither of these proofs appear to use any property of $\sin(x)$ other than its oscillating derivatives. Does there exist a proof of the solution to this equation which uses some other properties to $\sin(x)$? If not, is there a way of visualising it, considering the connection of $\sin$ to the unit circle?","Consider the differential equation $$\frac{d^{2}y}{dx^{2}}+y=0$$ with initial conditions $y(0)=0$ and $y'(0)=1$. The solution is well known - $y=\sin(x)$. I know how to derive this solution, since the given equation is a linear differential equation with constant coefficients, and characteristic equation $z^{2}+1=0$. I also know that this identity, combined with the initial conditions, allows us to compute $y^{(n)}(0)$ and thus the Maclaurin series of $y$, which coincides with the Maclaurin series of $\sin(x)$.  Neither of these proofs appear to use any property of $\sin(x)$ other than its oscillating derivatives. Does there exist a proof of the solution to this equation which uses some other properties to $\sin(x)$? If not, is there a way of visualising it, considering the connection of $\sin$ to the unit circle?",,"['ordinary-differential-equations', 'trigonometry', 'intuition', 'alternative-proof']"
6,How to create a simple differential equation,How to create a simple differential equation,,"I am doing numerical analysis where we work with differential equations but I have never had any classes on differential equations. It seems you can get by in an introductory numerical analysis course with just knowing what a differential equation is an how the initial value problem solving process works. So I know a few process for solving them numerically but I have never learned how to solve them mathematically. Anyway, I would like to know how to 'create' differential equation for testing my numerical approximation algorithms against, but as I have never learned how to deal with them mathematically I don't know the process for creating them. Can someone show me a step by step process for creating simple differential equations? If I have $y = 5x^2 +c$, $c$ being some constant I can make a differential equation of the form - $\frac{dy}{dx} = 10x$ But I want to have differential equations where the $\frac{dy}{dx}$ is dependent on not just $x$, but on $x$ and $y$. What is the process for doing this? Edit To clarify, I am looking to create simple equations of the form $\frac{dy}{dx} = f(x,y)$, $y(0) = y_0$ with solutions $y$ that are also relatively simple. This is so I can then write out the first few iterations of RK methods by hand to see how it all works.","I am doing numerical analysis where we work with differential equations but I have never had any classes on differential equations. It seems you can get by in an introductory numerical analysis course with just knowing what a differential equation is an how the initial value problem solving process works. So I know a few process for solving them numerically but I have never learned how to solve them mathematically. Anyway, I would like to know how to 'create' differential equation for testing my numerical approximation algorithms against, but as I have never learned how to deal with them mathematically I don't know the process for creating them. Can someone show me a step by step process for creating simple differential equations? If I have $y = 5x^2 +c$, $c$ being some constant I can make a differential equation of the form - $\frac{dy}{dx} = 10x$ But I want to have differential equations where the $\frac{dy}{dx}$ is dependent on not just $x$, but on $x$ and $y$. What is the process for doing this? Edit To clarify, I am looking to create simple equations of the form $\frac{dy}{dx} = f(x,y)$, $y(0) = y_0$ with solutions $y$ that are also relatively simple. This is so I can then write out the first few iterations of RK methods by hand to see how it all works.",,['ordinary-differential-equations']
7,"How to solve the differential equation $y'=3y^{\frac{2}{3}}$, $y(0)=0$","How to solve the differential equation ,",y'=3y^{\frac{2}{3}} y(0)=0,I found the ODE $\displaystyle y'=3y^{\frac{2}{3}}$ under the assumption of $y(0)=0$ somewhere and tried to solve it. I think there are infinitely many solutions to the problem but couldn't find more than $y=x^3$ and $y=0$. Can you verify that there are infinitely many solutions or that there aren't? Thanks for the help!,I found the ODE $\displaystyle y'=3y^{\frac{2}{3}}$ under the assumption of $y(0)=0$ somewhere and tried to solve it. I think there are infinitely many solutions to the problem but couldn't find more than $y=x^3$ and $y=0$. Can you verify that there are infinitely many solutions or that there aren't? Thanks for the help!,,['ordinary-differential-equations']
8,Linear Two-Point Boundary Value Problem (BVP),Linear Two-Point Boundary Value Problem (BVP),,"I ran across the following. In the text I was reading, it was left unproved. Can anyone help me see why it's true? If the linear two-point boundary value problem    \begin{cases}  y''=p(x)y'+q(x)y+r(x)\\ y(a)=A\\ y(b)=B  \end{cases}    satisfies $p(x),q(x),r(x)$ are continuous on $[a,b]$ $q(x)>0$ on $[a,b]$ then the problem as a unique solution. Thanks!","I ran across the following. In the text I was reading, it was left unproved. Can anyone help me see why it's true? If the linear two-point boundary value problem    \begin{cases}  y''=p(x)y'+q(x)y+r(x)\\ y(a)=A\\ y(b)=B  \end{cases}    satisfies $p(x),q(x),r(x)$ are continuous on $[a,b]$ $q(x)>0$ on $[a,b]$ then the problem as a unique solution. Thanks!",,['ordinary-differential-equations']
9,Comparison theorem for systems of ODE,Comparison theorem for systems of ODE,,"Let vector-function $x(t)$ satisfy a differential equation $$   \dot x = f(x), $$ and a vector-function $y($t) satisfy a differential inequality $$    \dot y \leq f(y) $$ with starting positions $y(0) < x(0)$. If a function $f(x)$ satisfies the property:  $$   f_{i}(x_1+\alpha_1,\ldots,x_{i-1}+\alpha_{i-1},x_i,x_{i+1}+\alpha_{i+1},\ldots,x_{n}+\alpha_{n}) \geq f_{i}(x_1,\ldots,x_n) $$ for any $\alpha_{1} \geq 0, \ldots, \alpha_{n}  \geq 0$ (i.e. it is quasimonotone), then $y(t) \leq x(t)$ for any $t>0$. Function $f(x)$ is smooth. Is there a name for such theorem? Please help me to proof it or give me a reference.","Let vector-function $x(t)$ satisfy a differential equation $$   \dot x = f(x), $$ and a vector-function $y($t) satisfy a differential inequality $$    \dot y \leq f(y) $$ with starting positions $y(0) < x(0)$. If a function $f(x)$ satisfies the property:  $$   f_{i}(x_1+\alpha_1,\ldots,x_{i-1}+\alpha_{i-1},x_i,x_{i+1}+\alpha_{i+1},\ldots,x_{n}+\alpha_{n}) \geq f_{i}(x_1,\ldots,x_n) $$ for any $\alpha_{1} \geq 0, \ldots, \alpha_{n}  \geq 0$ (i.e. it is quasimonotone), then $y(t) \leq x(t)$ for any $t>0$. Function $f(x)$ is smooth. Is there a name for such theorem? Please help me to proof it or give me a reference.",,"['reference-request', 'ordinary-differential-equations']"
10,Parametric equations for elliptical orbits,Parametric equations for elliptical orbits,,"(Editing because I left out the actual question part. Oops!) Given a situation like a comet going round the sun in an elliptical orbit, what would parametric equations for the comet's path look like? It'd be extra rad if it was expressed in terms of polar coordinates with the sun(one of the foci) at the origin. Even just pointers on a better way to approach the problem would be cool. Here's how I approached it but it didn't get me to a solution: The acceleration at any point is $$\begin{eqnarray*} F &=& \frac{Gm_{sun}m_{comet}}{r^2} \\ r &=& \|\vec{x}\|\\ \vec{a} &=& -\frac{F}{m_{comet}}\frac{\vec{x}}{\|\vec{x}\|}\\ &=& \frac{Gm_{sun}}{\|\vec{x}\|^2}\frac{\vec{x}}{\|\vec{x}\|} \\ &=& \frac{Gm_{sun}\vec{x}}{\|\vec{x}\|^3}\\ \end{eqnarray*}$$ and of course acceleration is the second derivative of the position, so $$\begin{eqnarray*} \vec{x} &=& \iint \frac{Gm_{sun}\vec{x}}{\|\vec{x}\|^3} dt\,dt\\ &=& Gm_{sun}\iint \frac{\vec{x}}{\|\vec{x}\|^3} dt\,dt \end{eqnarray*}$$ but I have no idea what do with integrating a norm like that, so breaking the vector down: $$\begin{eqnarray*} \vec{x} &=& \left \langle x,y \right \rangle \\ \vec{a} &=& \left \langle x'',y'' \right \rangle \\ \|\vec{x}\| &=& \left ( x^2+y^2 \right )^\frac{1}{2} \\ \|\vec{x}\|^3 &=& \left ( x^2+y^2 \right )^\frac{3}{2} \\ \frac{1}{\|\vec{x}\|^3} &=& \left ( x^2+y^2 \right )^{-\frac{3}{2}} \\ \left \langle x'',y'' \right \rangle&=& Gm_{sun} \left \langle \frac{x}{\|\vec{x}\|^3}, \frac{y}{\|\vec{x}\|^3} \right \rangle\\ \left \langle x'',y'' \right \rangle&=& Gm_{sun} \left \langle x\left ( x^2+y^2 \right )^{-\frac{3}{2}}, y\left ( x^2+y^2 \right )^{-\frac{3}{2}} \right \rangle \end{eqnarray*}$$ which gives me a system of differential equations I could write like  $$\begin{eqnarray*} {\partial^2\over\partial t^2} x(t) &=& Gm_{sun}x(t)\left ( x(t)^2+y(t)^2 \right )^{-\frac{3}{2}} \\ {\partial^2\over\partial t^2} y(t) &=& Gm_{sun}y(t)\left ( x(t)^2+y(t)^2 \right )^{-\frac{3}{2}} \end{eqnarray*}$$ And I really don't know what to do with differential equations like those except take guesses at the form x(t) and y(t) take. Kepler's laws of planetary motion and the fact that it is known to be an ellipse and might be able to add useful constraints, but so far I haven't been able to really make progress using them myself. Anyway, I'm sure two-body gravity problems like this are extremely well-understood by now, so I'm sure somebody worked this out already if I could just find the right words to type into Google :)","(Editing because I left out the actual question part. Oops!) Given a situation like a comet going round the sun in an elliptical orbit, what would parametric equations for the comet's path look like? It'd be extra rad if it was expressed in terms of polar coordinates with the sun(one of the foci) at the origin. Even just pointers on a better way to approach the problem would be cool. Here's how I approached it but it didn't get me to a solution: The acceleration at any point is $$\begin{eqnarray*} F &=& \frac{Gm_{sun}m_{comet}}{r^2} \\ r &=& \|\vec{x}\|\\ \vec{a} &=& -\frac{F}{m_{comet}}\frac{\vec{x}}{\|\vec{x}\|}\\ &=& \frac{Gm_{sun}}{\|\vec{x}\|^2}\frac{\vec{x}}{\|\vec{x}\|} \\ &=& \frac{Gm_{sun}\vec{x}}{\|\vec{x}\|^3}\\ \end{eqnarray*}$$ and of course acceleration is the second derivative of the position, so $$\begin{eqnarray*} \vec{x} &=& \iint \frac{Gm_{sun}\vec{x}}{\|\vec{x}\|^3} dt\,dt\\ &=& Gm_{sun}\iint \frac{\vec{x}}{\|\vec{x}\|^3} dt\,dt \end{eqnarray*}$$ but I have no idea what do with integrating a norm like that, so breaking the vector down: $$\begin{eqnarray*} \vec{x} &=& \left \langle x,y \right \rangle \\ \vec{a} &=& \left \langle x'',y'' \right \rangle \\ \|\vec{x}\| &=& \left ( x^2+y^2 \right )^\frac{1}{2} \\ \|\vec{x}\|^3 &=& \left ( x^2+y^2 \right )^\frac{3}{2} \\ \frac{1}{\|\vec{x}\|^3} &=& \left ( x^2+y^2 \right )^{-\frac{3}{2}} \\ \left \langle x'',y'' \right \rangle&=& Gm_{sun} \left \langle \frac{x}{\|\vec{x}\|^3}, \frac{y}{\|\vec{x}\|^3} \right \rangle\\ \left \langle x'',y'' \right \rangle&=& Gm_{sun} \left \langle x\left ( x^2+y^2 \right )^{-\frac{3}{2}}, y\left ( x^2+y^2 \right )^{-\frac{3}{2}} \right \rangle \end{eqnarray*}$$ which gives me a system of differential equations I could write like  $$\begin{eqnarray*} {\partial^2\over\partial t^2} x(t) &=& Gm_{sun}x(t)\left ( x(t)^2+y(t)^2 \right )^{-\frac{3}{2}} \\ {\partial^2\over\partial t^2} y(t) &=& Gm_{sun}y(t)\left ( x(t)^2+y(t)^2 \right )^{-\frac{3}{2}} \end{eqnarray*}$$ And I really don't know what to do with differential equations like those except take guesses at the form x(t) and y(t) take. Kepler's laws of planetary motion and the fact that it is known to be an ellipse and might be able to add useful constraints, but so far I haven't been able to really make progress using them myself. Anyway, I'm sure two-body gravity problems like this are extremely well-understood by now, so I'm sure somebody worked this out already if I could just find the right words to type into Google :)",,['ordinary-differential-equations']
11,Regarding Ladder Operators and Quantum Harmonic Oscillators,Regarding Ladder Operators and Quantum Harmonic Oscillators,,"When dealing with the Quantum Harmonic Oscillator Operator $H=-\frac{d^{2}}{dx^{2}}+x^{2}$, there is the approach of using the Ladder Operator: Suppose that are two operators $L^{+}$ and $L^{-}$ and define $f_{n}$ such that $$L^{+}(f_{n})=\sqrt{n+1}f_{n+1}$$ $$L^{-}(f_{n})=\sqrt{n}f_{n-1}$$ then $$L^{+}L^{-}(f_{n})=L^{+}(\sqrt{n}f_{n-1})=nf_{n}$$ $$L^{-}L^{+}(f_{n})=L^{-}(\sqrt{n+1}f_{n+1})=(n+1)f_{n}$$ Thus $$L^{+}L^{-}+L^{-}L^{+}=(2n+1)f_{n}$$ Now since $$H=-\frac{d^{2}}{dx^{2}}+x^{2}=\left(x+\frac{d}{dx}\right)\left(x-\frac{d}{dx}\right)$$ $$=\frac{1}{2}\left[\left(x+\frac{d}{dx}\right)\left(x-\frac{d}{dx}\right)+\left(x+\frac{d}{dx}\right)\left(x-\frac{d}{dx}\right)\right]$$ $$=\frac{1}{\sqrt{2}}\left(x+\frac{d}{dx}\right)\frac{1}{\sqrt{2}}\left(x-\frac{d}{dx}\right)+\frac{1}{\sqrt{2}}\left(x-\frac{d}{dx}\right)\frac{1}{\sqrt{2}}\left(x+\frac{d}{dx}\right)$$ If we let $$L^{+}=\frac{1}{\sqrt{2}}\left(x+\frac{d}{dx}\right)\quad\text{and}\quad L^{-}=\frac{1}{\sqrt{2}}\left(x-\frac{d}{dx}\right)$$ then $$H=L^{+}L^{-}+L^{-}L^{+}$$ Since $$L^{+}(f_{n})=\sqrt{n+1}f_{n+1}\implies$$ $$f_{n}=\sqrt{n!}L^{n}(f_{0})$$ Thus if we can a $f_{0}$ such that $$f_{n}=\sqrt{n!}L^{n}(f_{0})\implies L^{-}(f_{n})=\sqrt{n}f_{n-1}\tag{1}$$ would be satisfied, then we have found a set of eigenfunctions for H, which is $f_{n}$ and the corresponding eigenvalue is 2n+1. Now my instructor says that if we let $f_{0}$ be in the kernel of $L^-$, meaning that: $$L^{-1}f_{0}=0\tag{2},$$ then (1) would be true. Specifically, this means that $$f_{0}(x)=e^{-\frac{1}{2}x^{2}}.$$ As it is already verified (and commonly used in quantum mechanics) that this $f_{0}(x)$ does satisfy (1). But I can't really see how does (2) leads to (1). Is this choice $f_0$ really a lucky guess or actually a choice the generally works for all operators that can be expressed as $H=L^{+}L^{-}+L^{-}L^{+}$? A different way of asking this question is: are the ladder operator really just operators that tells us how to get from one state to another, or it is actually something that would be used as a general method of solving some differential equations?","When dealing with the Quantum Harmonic Oscillator Operator $H=-\frac{d^{2}}{dx^{2}}+x^{2}$, there is the approach of using the Ladder Operator: Suppose that are two operators $L^{+}$ and $L^{-}$ and define $f_{n}$ such that $$L^{+}(f_{n})=\sqrt{n+1}f_{n+1}$$ $$L^{-}(f_{n})=\sqrt{n}f_{n-1}$$ then $$L^{+}L^{-}(f_{n})=L^{+}(\sqrt{n}f_{n-1})=nf_{n}$$ $$L^{-}L^{+}(f_{n})=L^{-}(\sqrt{n+1}f_{n+1})=(n+1)f_{n}$$ Thus $$L^{+}L^{-}+L^{-}L^{+}=(2n+1)f_{n}$$ Now since $$H=-\frac{d^{2}}{dx^{2}}+x^{2}=\left(x+\frac{d}{dx}\right)\left(x-\frac{d}{dx}\right)$$ $$=\frac{1}{2}\left[\left(x+\frac{d}{dx}\right)\left(x-\frac{d}{dx}\right)+\left(x+\frac{d}{dx}\right)\left(x-\frac{d}{dx}\right)\right]$$ $$=\frac{1}{\sqrt{2}}\left(x+\frac{d}{dx}\right)\frac{1}{\sqrt{2}}\left(x-\frac{d}{dx}\right)+\frac{1}{\sqrt{2}}\left(x-\frac{d}{dx}\right)\frac{1}{\sqrt{2}}\left(x+\frac{d}{dx}\right)$$ If we let $$L^{+}=\frac{1}{\sqrt{2}}\left(x+\frac{d}{dx}\right)\quad\text{and}\quad L^{-}=\frac{1}{\sqrt{2}}\left(x-\frac{d}{dx}\right)$$ then $$H=L^{+}L^{-}+L^{-}L^{+}$$ Since $$L^{+}(f_{n})=\sqrt{n+1}f_{n+1}\implies$$ $$f_{n}=\sqrt{n!}L^{n}(f_{0})$$ Thus if we can a $f_{0}$ such that $$f_{n}=\sqrt{n!}L^{n}(f_{0})\implies L^{-}(f_{n})=\sqrt{n}f_{n-1}\tag{1}$$ would be satisfied, then we have found a set of eigenfunctions for H, which is $f_{n}$ and the corresponding eigenvalue is 2n+1. Now my instructor says that if we let $f_{0}$ be in the kernel of $L^-$, meaning that: $$L^{-1}f_{0}=0\tag{2},$$ then (1) would be true. Specifically, this means that $$f_{0}(x)=e^{-\frac{1}{2}x^{2}}.$$ As it is already verified (and commonly used in quantum mechanics) that this $f_{0}(x)$ does satisfy (1). But I can't really see how does (2) leads to (1). Is this choice $f_0$ really a lucky guess or actually a choice the generally works for all operators that can be expressed as $H=L^{+}L^{-}+L^{-}L^{+}$? A different way of asking this question is: are the ladder operator really just operators that tells us how to get from one state to another, or it is actually something that would be used as a general method of solving some differential equations?",,"['ordinary-differential-equations', 'operator-algebras', 'quantum-mechanics']"
12,numerical integration of equations of motion of large system of particles with lubrication forces,numerical integration of equations of motion of large system of particles with lubrication forces,,"I have a large system of solid particles moving in the liquid. I use traditional Newtonian equations of motion for the particles. There are many different interaction forces between particles and the most severe are ""lubrication forces"". These are forces which dependent on the distance between the two particles and their velocities and the force goes to infinity when the particles get into touch. Which integration technique should I use to have as few time steps while stable? I have implemented Runge-Kutta-Fehlberg method with adaptive time step following this article: http://www.trentfguidry.net/post/2009/10/09/Runge-Kutta-Fehlberg.aspx but it produces a large number of steps even if I allow for large errors. I compare it with and Eulerian method where I estimate the future step length based on the allowed force changes. Such a dummy method produces less steps than the Runge-Kutta-Fehlberg and is stable but still is slow... Any suggestions are appreciated! PS: For those who are familiar with it. I use Lattice Boltzmann as fluid dynamics solver and Immersed Boundary Method for particles representation.","I have a large system of solid particles moving in the liquid. I use traditional Newtonian equations of motion for the particles. There are many different interaction forces between particles and the most severe are ""lubrication forces"". These are forces which dependent on the distance between the two particles and their velocities and the force goes to infinity when the particles get into touch. Which integration technique should I use to have as few time steps while stable? I have implemented Runge-Kutta-Fehlberg method with adaptive time step following this article: http://www.trentfguidry.net/post/2009/10/09/Runge-Kutta-Fehlberg.aspx but it produces a large number of steps even if I allow for large errors. I compare it with and Eulerian method where I estimate the future step length based on the allowed force changes. Such a dummy method produces less steps than the Runge-Kutta-Fehlberg and is stable but still is slow... Any suggestions are appreciated! PS: For those who are familiar with it. I use Lattice Boltzmann as fluid dynamics solver and Immersed Boundary Method for particles representation.",,"['ordinary-differential-equations', 'numerical-methods']"
13,Constant area of triangle from tangent line and axes,Constant area of triangle from tangent line and axes,,"Let $f:(0,+\infty)\to (0,+\infty)$ be a differentiable function such that $f'(x)\not=0$ with the following property: the area of the triangle formed by the tangent line of $C_f$ at a point $M(x_0,f(x_0))$ , $x'x$ and $y'y$ is constant and independent of $x_0\in (0,+\infty)$ . Find all functions $f$ with the above properties. I know that $f(x)=\frac{1}{x}, x>0$ satisfies the above conditions. Attempt Let $\varepsilon_{x_0}:\ y-f(x_0)=f'(x_0)(x-x_0)$ be the equation of a tangent line of $C_f$ at a point $M(x_0,f(x_0))$ and $A\bigg(\frac{x_0f'(x_0)-f(x_0)}{f'(x_0)},0\bigg)$ , $B(0,-x_0f'(x_0)+f(x_0))$ the intersection points of $\varepsilon_{x_0}$ with the axes. Then $(OAB)=c=\text{constant}\Leftrightarrow  (x_0f'(x_0)-f(x_0))^2=-2cf'(x_0)$ . Questions Is there any reference of the above problem in the literature? Is there an easy way to solve the differential equation $(xf'(x)-f(x))^2=-2cf'(x)$ ?","Let be a differentiable function such that with the following property: the area of the triangle formed by the tangent line of at a point , and is constant and independent of . Find all functions with the above properties. I know that satisfies the above conditions. Attempt Let be the equation of a tangent line of at a point and , the intersection points of with the axes. Then . Questions Is there any reference of the above problem in the literature? Is there an easy way to solve the differential equation ?","f:(0,+\infty)\to (0,+\infty) f'(x)\not=0 C_f M(x_0,f(x_0)) x'x y'y x_0\in (0,+\infty) f f(x)=\frac{1}{x}, x>0 \varepsilon_{x_0}:\ y-f(x_0)=f'(x_0)(x-x_0) C_f M(x_0,f(x_0)) A\bigg(\frac{x_0f'(x_0)-f(x_0)}{f'(x_0)},0\bigg) B(0,-x_0f'(x_0)+f(x_0)) \varepsilon_{x_0} (OAB)=c=\text{constant}\Leftrightarrow  (x_0f'(x_0)-f(x_0))^2=-2cf'(x_0) (xf'(x)-f(x))^2=-2cf'(x)","['calculus', 'ordinary-differential-equations', 'derivatives', 'area', 'tangent-line']"
14,"What exactly should the solution to $y^\prime = (1 - 2x)y^2, \ y(0) = -\frac{1}{12}$ be?",What exactly should the solution to  be?,"y^\prime = (1 - 2x)y^2, \ y(0) = -\frac{1}{12}","(Hey everyone, I'm new to Stack Exchange and this is actually my first question, so if I'm doing anything inappropriate regarding asking questions, please point that out for me.) So I've been taking an ODE course lately, and I encountered a question in my homework, which is basically an Initial Value Problem that goes by $$y^\prime = (1 - 2x)y^2, \ y(0) = -\frac{1}{12}.$$ This is a pretty standatd IVP, so I just attempted by dividing both sides by $y^2$ (while noting the trivial solution $y = 0$ ): $$\frac{dy}{y^2} = (1-2x)dx$$ Integrating both sides gives me $$-\frac{1}{y} = -x^2 + x + C$$ Plugging in the initial conditions gives $C = 12$ , so after doing some algebraic manipulations, the final answer I obtained is $y = 0$ and $y = \frac{1}{x^2 - x -12}.$ But here's the thing: the solution given by my teacher is $$y = \frac{1}{x^2 - x - 12}, \ -3 < x < 4,$$ about which there are several things I'm not so sure. First, I don't really understand why the trivial solution is missing in the suggested solution. Also, I'm not sure whether it's necessary at all to include the domain of the function in the answer (since the solutions to problems as such on textbooks don't seem to bother including the domains), although in this case since the polynomial with $x$ terms is in the denominator, there is certain constraints on the interval on which $y$ is defined--which brings out my third confusion: why is the domain here not simply $x \neq -3, x \neq 4$ , but an open interval $(-3, 4)$ ? One possible reasoning I came up with is that, given the initial point $(0, -\frac{1}{12})$ , we only care about the part of the function close to that point. But if the answer is given solely by the suggested solution, isn't the trivial solution completely ignored? My concerns here are really just about subtlties, but as a beginner in ODE, I find these minor issues especially convoluted and needs clarifications, so I'd be appreciative if anyone can help with this! $\textbf{EDIT}$ : $y^2$ is not a trivial solution in this case, as it does not satisfy the initial condition, so I've made a dumb mistake...It is better though, to explicitly write down the domain of the function (see Anne's comment below). Thank you guys for helping!","(Hey everyone, I'm new to Stack Exchange and this is actually my first question, so if I'm doing anything inappropriate regarding asking questions, please point that out for me.) So I've been taking an ODE course lately, and I encountered a question in my homework, which is basically an Initial Value Problem that goes by This is a pretty standatd IVP, so I just attempted by dividing both sides by (while noting the trivial solution ): Integrating both sides gives me Plugging in the initial conditions gives , so after doing some algebraic manipulations, the final answer I obtained is and But here's the thing: the solution given by my teacher is about which there are several things I'm not so sure. First, I don't really understand why the trivial solution is missing in the suggested solution. Also, I'm not sure whether it's necessary at all to include the domain of the function in the answer (since the solutions to problems as such on textbooks don't seem to bother including the domains), although in this case since the polynomial with terms is in the denominator, there is certain constraints on the interval on which is defined--which brings out my third confusion: why is the domain here not simply , but an open interval ? One possible reasoning I came up with is that, given the initial point , we only care about the part of the function close to that point. But if the answer is given solely by the suggested solution, isn't the trivial solution completely ignored? My concerns here are really just about subtlties, but as a beginner in ODE, I find these minor issues especially convoluted and needs clarifications, so I'd be appreciative if anyone can help with this! : is not a trivial solution in this case, as it does not satisfy the initial condition, so I've made a dumb mistake...It is better though, to explicitly write down the domain of the function (see Anne's comment below). Thank you guys for helping!","y^\prime = (1 - 2x)y^2, \ y(0) = -\frac{1}{12}. y^2 y = 0 \frac{dy}{y^2} = (1-2x)dx -\frac{1}{y} = -x^2 + x + C C = 12 y = 0 y = \frac{1}{x^2 - x -12}. y = \frac{1}{x^2 - x - 12}, \ -3 < x < 4, x y x \neq -3, x \neq 4 (-3, 4) (0, -\frac{1}{12}) \textbf{EDIT} y^2","['ordinary-differential-equations', 'initial-value-problems']"
15,Green function jump conditions for second order differential equation,Green function jump conditions for second order differential equation,,"I been trying to find the Green's function for a particular problem. For the equation $$  q(x) \frac{\mathrm{d}^2u(x)}{\mathrm{d}x^2} + p(x) u(x) =0 \tag{1} $$ where $q(x)$ and $p(x)$ are some functions of $x$ . I have solved these and have two linearly independent results. $$ u(x) = c_1 u_1(x) + c_2 u_2(x) $$ with $c_1, c_2$ being arbitrary constants and both $u_1, u_2$ solve equation (1). Now for the boundary conditions. At $x\to +\infty$ we require that the solution goes to zero, only $u_2$ satisfies, another condition is at $x\to1_+$ . Only $u_1$ satisfies this. Thus our Green's function can be written as $$ G(x,y) = c_1 \mathcal{H}(y-x) u_1(x) + c_2 \mathcal{H}(x-y) u_2(x). $$ Where $\mathcal{H}$ is the Heaviside step function and $y>1$ . Now the Green's function must be continous at $x = y$ , thus we have condition for (for example) $c_1$ . But what is the other condition? Does the condition $$ \lim_{x\to y_-} \frac{\partial G(x,y) }{\partial x} -\lim_{x\to y_+} \frac{\partial G(x,y) }{\partial x} = 1 $$ hold even for our special ODE (1)?","I been trying to find the Green's function for a particular problem. For the equation where and are some functions of . I have solved these and have two linearly independent results. with being arbitrary constants and both solve equation (1). Now for the boundary conditions. At we require that the solution goes to zero, only satisfies, another condition is at . Only satisfies this. Thus our Green's function can be written as Where is the Heaviside step function and . Now the Green's function must be continous at , thus we have condition for (for example) . But what is the other condition? Does the condition hold even for our special ODE (1)?"," 
q(x) \frac{\mathrm{d}^2u(x)}{\mathrm{d}x^2} + p(x) u(x) =0 \tag{1}
 q(x) p(x) x 
u(x) = c_1 u_1(x) + c_2 u_2(x)
 c_1, c_2 u_1, u_2 x\to +\infty u_2 x\to1_+ u_1 
G(x,y) = c_1 \mathcal{H}(y-x) u_1(x) + c_2 \mathcal{H}(x-y) u_2(x).
 \mathcal{H} y>1 x = y c_1 
\lim_{x\to y_-} \frac{\partial G(x,y) }{\partial x} -\lim_{x\to y_+} \frac{\partial G(x,y) }{\partial x} = 1
","['ordinary-differential-equations', 'greens-function']"
16,Proving that the origin is unstable in a dynamical system,Proving that the origin is unstable in a dynamical system,,"Prove that the origin is an unstable equilibrium point for the system $$\begin{align*}\dot{x}&=x^3+yx^2\\\dot{y}&=-y+x^2\end{align*}$$ I've already tried to linearize the system in order to use the Hartman–Grobman theorem, but the eigenvalues of the respective Jacobian matrix are $0$ and $-1$ ; non of them are positive, then Lyapunov's indirect method doesn't work either. Do you have any idea of another theorem that allow us to conclude this?","Prove that the origin is an unstable equilibrium point for the system I've already tried to linearize the system in order to use the Hartman–Grobman theorem, but the eigenvalues of the respective Jacobian matrix are and ; non of them are positive, then Lyapunov's indirect method doesn't work either. Do you have any idea of another theorem that allow us to conclude this?",\begin{align*}\dot{x}&=x^3+yx^2\\\dot{y}&=-y+x^2\end{align*} 0 -1,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory', 'lyapunov-functions']"
17,How to prove that a conserved quantity does not exist in the system $\ddot{x}=-kx-c\dot{x}$,How to prove that a conserved quantity does not exist in the system,\ddot{x}=-kx-c\dot{x},"I would like to know how to prove that there is no conserved quantity in a system. Let $k$ and $c$ be real numbers. Below is the ODE for the damped harmonic oscillator. $$\ddot{x}=-kx-c\dot{x}$$ From a physics point of view, I would expect that no conserved quantities exist in this system, since energy is not conserved in it. The definition of a conservative quantity is a non-constant function $f:\mathbb{R}^2\to\mathbb{R}$ that satisfies $\frac{\mathrm{d}f(x(t),y(t))}{\mathrm{d}t}=0$ for all solutions $(x,y)$ of the following system. $$ \begin{aligned} \dot{x}&=y\\ \dot{y}&=-kx-cy \end{aligned}$$","I would like to know how to prove that there is no conserved quantity in a system. Let and be real numbers. Below is the ODE for the damped harmonic oscillator. From a physics point of view, I would expect that no conserved quantities exist in this system, since energy is not conserved in it. The definition of a conservative quantity is a non-constant function that satisfies for all solutions of the following system.","k c \ddot{x}=-kx-c\dot{x} f:\mathbb{R}^2\to\mathbb{R} \frac{\mathrm{d}f(x(t),y(t))}{\mathrm{d}t}=0 (x,y) 
\begin{aligned}
\dot{x}&=y\\
\dot{y}&=-kx-cy
\end{aligned}","['real-analysis', 'calculus']"
18,Conflicting results in finding orthogonal curves,Conflicting results in finding orthogonal curves,,"I'm told to find the set of orthogonal curves to the curve of equation $y=cx^2$ . Using implicit differentiation, $\frac{dy}{dx}=2cx$ . Then, the desired curves obey the differential equation $\frac{dy}{dx}=-\frac{1}{2cx}$ . Integrating, we find the curves $y=-\frac{1}{2c}\log{|x|}+K$ , for some real constant $K$ . The thing is in some solution I found, they rewrite $2cx$ as $\frac{2y}{x}$ , because $c=\frac{y}{x^2}$ , and in doing so they get a different differential equation with different curves $\left(y=\pm \sqrt{K-\frac{1}{2}x^2}\right)$ . Which are clearly different curves. Plotting them on geogebra makes it seem like mine is wrong. What error did I make?","I'm told to find the set of orthogonal curves to the curve of equation . Using implicit differentiation, . Then, the desired curves obey the differential equation . Integrating, we find the curves , for some real constant . The thing is in some solution I found, they rewrite as , because , and in doing so they get a different differential equation with different curves . Which are clearly different curves. Plotting them on geogebra makes it seem like mine is wrong. What error did I make?",y=cx^2 \frac{dy}{dx}=2cx \frac{dy}{dx}=-\frac{1}{2cx} y=-\frac{1}{2c}\log{|x|}+K K 2cx \frac{2y}{x} c=\frac{y}{x^2} \left(y=\pm \sqrt{K-\frac{1}{2}x^2}\right),['ordinary-differential-equations']
19,Why can't I have $\dot{x}(t) < 0$?,Why can't I have ?,\dot{x}(t) < 0,"I'm in the very beginning of ""Ordinary Differential Equations and Dynamical Systems"" by Gerald Teschl and on page 9 he starts with differential equations of the form: $$\dot{x} = f(x), \quad x(0) = x_0, \quad f \in C(\mathbb{R})$$ To solve these he quickly goes to: $$\int_{0}^t\frac{\dot{x}(s)ds}{f(x(s))} = t$$ Letting $F(x) =\int_{x_0}^x\frac{dy}{f(y)}$ he points out that any solution must satisfy that $F(x(t)) = t$ . So, since $F(x)$ is is strictly monotone in some neighborhood of $x_0$ , any solution $\phi$ will need to satisfy $$\phi(t) = F^{-1}(t), \phi(0) = F^{-1}(0) = x_0$$ Assuming, WLOG, $f(x_0)>0$ we can find a maximal interval $(x_1, x_2) \ni x_0$ where $f$ is positive in this interval as well. So we can define: $$T_+ = \lim_{x \uparrow x_2} F(x) \in (0, +\infty]$$ $$T_- = \lim_{x \downarrow x_2} F(x) \in [-\infty, 0)$$ So, $\phi \in C^1((T_-, T_+))$ . So far, so good. But, where I get confused is the following statement. Teschl states that: In particular, $\phi$ is defined for all $t > 0$ if and only if: $$T_+ = \int_{x_0}^{x_2}\frac{dy}{f(y)} = +\infty$$ This is what I don't understand. I understand the ""if"" direction - if the integral evaluates to infinity, then the approaches $x_2$ but keeps slowing down so that it never actually reaches $x_2$ . But, the ""only if"" direction confuses me. $x_2$ is just the maximal point to the right of $x_0$ where the velocity stays positive (since it started out positive). To me, this indicates that if the velocity starts out positive, it cannot every become negative, but that doesn't seem right. Can anyone help me understand why this must be true?","I'm in the very beginning of ""Ordinary Differential Equations and Dynamical Systems"" by Gerald Teschl and on page 9 he starts with differential equations of the form: To solve these he quickly goes to: Letting he points out that any solution must satisfy that . So, since is is strictly monotone in some neighborhood of , any solution will need to satisfy Assuming, WLOG, we can find a maximal interval where is positive in this interval as well. So we can define: So, . So far, so good. But, where I get confused is the following statement. Teschl states that: In particular, is defined for all if and only if: This is what I don't understand. I understand the ""if"" direction - if the integral evaluates to infinity, then the approaches but keeps slowing down so that it never actually reaches . But, the ""only if"" direction confuses me. is just the maximal point to the right of where the velocity stays positive (since it started out positive). To me, this indicates that if the velocity starts out positive, it cannot every become negative, but that doesn't seem right. Can anyone help me understand why this must be true?","\dot{x} = f(x), \quad x(0) = x_0, \quad f \in C(\mathbb{R}) \int_{0}^t\frac{\dot{x}(s)ds}{f(x(s))} = t F(x) =\int_{x_0}^x\frac{dy}{f(y)} F(x(t)) = t F(x) x_0 \phi \phi(t) = F^{-1}(t), \phi(0) = F^{-1}(0) = x_0 f(x_0)>0 (x_1, x_2) \ni x_0 f T_+ = \lim_{x \uparrow x_2} F(x) \in (0, +\infty] T_- = \lim_{x \downarrow x_2} F(x) \in [-\infty, 0) \phi \in C^1((T_-, T_+)) \phi t > 0 T_+ = \int_{x_0}^{x_2}\frac{dy}{f(y)} = +\infty x_2 x_2 x_2 x_0",['ordinary-differential-equations']
20,How do we solve Burgers' equation using the method of characteristics?,How do we solve Burgers' equation using the method of characteristics?,,"I want to solve \begin{align}&\forall(t,x)\in(0,\infty)\times\mathbb R:\left(\frac{\partial u}{\partial t}+u\frac{\partial u}{\partial x}\right)(t,x)=0;\tag1\\&\forall x\in\mathbb R:u(0,x)=u_0(x)\tag2,\end{align} where $\Omega:=(0,\infty)\times\mathbb R$ and $u_0\in C^1(\mathbb R)$ , using the method of characteristics . Assume that $u_0'\le0$ and $$u_0(x)=\left.\begin{cases}1&\text{, if }x\le0\\0&\text{, if }x\ge1\end{cases}\right\}\;\;\;\text{for all }x\in\mathbb R\tag3.$$ As usual, we consider a path $\gamma$ with $\gamma'=(1,\underbrace{u\circ\gamma}_{=:\:z})$ , since then $(1)$ implies $z'=0$ and hence $z=c_1$ for some $c_1\in\mathbb R$ , which in turn impleis $\gamma=(c_2+t,c_3+c_1t)$ for some $c_2,c_3\in\mathbb R$ . Fix $(t_0,x_0)\in(0,\infty)\times\mathbb R$ . Since we are interested in the value $u(t_0,x_0)$ , we choose $c_2:=0$ and $c_3:=x_0-c_1t_0$ to obtain $$u(t_0,x_0)=z(t_0)=z(0)=u_0(x_0-c_1t_0)\tag4.$$ Question 1 : So, is this the whole story? Do we obtain different values for the solution of $(1)$ by varying $c_1$ ? And does this that $(1)$ and $(2)$ together do not admit a unique solution? (And do they admit a solution at all?) Question 2 : By $(3)$ , we see that if $c_1\ge\frac{x_0}{t_0}$ , then $u(t_0,x_0)=1$ ad if $c_1\le\frac{x_0-1}{t_0}$ , then $u(t_0,x_0)=0$ , but what can we infer from that? Does this somehow yield a contradiction for general $t_0$ ? Or, phrased differently, does this imply that we won't be able to obtain a ""global"" solution, but only a solution up to some finite time $T>0$ ?","I want to solve where and , using the method of characteristics . Assume that and As usual, we consider a path with , since then implies and hence for some , which in turn impleis for some . Fix . Since we are interested in the value , we choose and to obtain Question 1 : So, is this the whole story? Do we obtain different values for the solution of by varying ? And does this that and together do not admit a unique solution? (And do they admit a solution at all?) Question 2 : By , we see that if , then ad if , then , but what can we infer from that? Does this somehow yield a contradiction for general ? Or, phrased differently, does this imply that we won't be able to obtain a ""global"" solution, but only a solution up to some finite time ?","\begin{align}&\forall(t,x)\in(0,\infty)\times\mathbb R:\left(\frac{\partial u}{\partial t}+u\frac{\partial u}{\partial x}\right)(t,x)=0;\tag1\\&\forall x\in\mathbb R:u(0,x)=u_0(x)\tag2,\end{align} \Omega:=(0,\infty)\times\mathbb R u_0\in C^1(\mathbb R) u_0'\le0 u_0(x)=\left.\begin{cases}1&\text{, if }x\le0\\0&\text{, if }x\ge1\end{cases}\right\}\;\;\;\text{for all }x\in\mathbb R\tag3. \gamma \gamma'=(1,\underbrace{u\circ\gamma}_{=:\:z}) (1) z'=0 z=c_1 c_1\in\mathbb R \gamma=(c_2+t,c_3+c_1t) c_2,c_3\in\mathbb R (t_0,x_0)\in(0,\infty)\times\mathbb R u(t_0,x_0) c_2:=0 c_3:=x_0-c_1t_0 u(t_0,x_0)=z(t_0)=z(0)=u_0(x_0-c_1t_0)\tag4. (1) c_1 (1) (2) (3) c_1\ge\frac{x_0}{t_0} u(t_0,x_0)=1 c_1\le\frac{x_0-1}{t_0} u(t_0,x_0)=0 t_0 T>0","['ordinary-differential-equations', 'partial-differential-equations', 'fluid-dynamics', 'characteristics']"
21,Optimal control input for nonlinear dynamics,Optimal control input for nonlinear dynamics,,"This is from Chapter 1 of the book, ""Robust Nonlinear Control Design"" by Freeman and Kokotovic. Consider the system $$\dot{x} = -x^3 + u + wx,$$ where $u$ is an unconstrained input and $w$ is known to take values in the interval $[-1,1]$ . If we consider the cost functional, $$ J = \int_0^{\infty} (x^2 + u^2) dt, $$ then the optimal feedback law that minimizes the cost functional (in the worst case) is claimed to be given by $$ u = x^3 - x - x\sqrt{x^4 -2x^2 + 2}. $$ Question: How is the optimal feedback law obtained? Attempt #1: I tried using Pontryagin Maximum Principle, and I got the following. The Hamiltonian is given as $$ H = x^2 + u^2 + \lambda(-x^3 + u + wx). $$ By taking derivatives with respect to $x$ and $u$ and solving them, $$ \dot{\lambda} = -\frac{\partial H}{\partial x} = -2x+ 3\lambda x^2 -\lambda wx \Rightarrow \lambda = ke^{(3x^2-w)t}+\frac{2x}{w-3x^2}, $$ $$ \frac{\partial H}{\partial u} = 0 \Rightarrow u = -\frac{\lambda}{2}. $$ Substituting $\lambda$ in the first expression into the second expression, $$ u = -\frac{k}{2}e^{(3x^2-w)t}+\frac{2x}{w-3x^2} $$ The initial or final conditions for the ODE in $\lambda$ are not stated so I cannot solve for $k$ , but the form of the solution that I obtained is quite different from what is given. Does anyone has any ideas or alternative approaches? Attempt #2 (added on Jan 5 2021): I also tried to work directly with the given optimal control input, $u$ and the nonlinear dynamics. Substituting $u$ into the nonlinear dynamics $\dot{x}$ and using the worst-case $w$ of $1$ gives $$ \dot{x}^2 = x^2 (x^4 - 2x^2 +2) = x^6 - 2x^4 + 2x^2 $$ Also note that we have the following expression for $u$ from the nonlinear dynamics, $$ \dot{x} = -x^3 + u + x \Rightarrow u = \dot{x} + x^3 - x  $$ This is independent of the given optimal control input. With this expression of $u$ , if $\dot{x}$ can be assumed to be $0$ (may not be a valid assumption), then the following can be said of the cost functional, $$ \begin{aligned} J = \int_0^{\infty} (x^2 + u^2) dt &= \int_0^{\infty} (x^2 + (\dot{x} + x^3 - x)^2) dt\\ &= \int_0^{\infty} (x^2 + x^6 - 2x^4 + x^2) dt\\ &= \int_0^{\infty} (x^6 - 2x^4 + 2x^2) dt\\ &= \int_0^{\infty} \dot{x}^2 dt = 0,  \end{aligned} $$ which is the minimum, since the cost functional is nonnegative.","This is from Chapter 1 of the book, ""Robust Nonlinear Control Design"" by Freeman and Kokotovic. Consider the system where is an unconstrained input and is known to take values in the interval . If we consider the cost functional, then the optimal feedback law that minimizes the cost functional (in the worst case) is claimed to be given by Question: How is the optimal feedback law obtained? Attempt #1: I tried using Pontryagin Maximum Principle, and I got the following. The Hamiltonian is given as By taking derivatives with respect to and and solving them, Substituting in the first expression into the second expression, The initial or final conditions for the ODE in are not stated so I cannot solve for , but the form of the solution that I obtained is quite different from what is given. Does anyone has any ideas or alternative approaches? Attempt #2 (added on Jan 5 2021): I also tried to work directly with the given optimal control input, and the nonlinear dynamics. Substituting into the nonlinear dynamics and using the worst-case of gives Also note that we have the following expression for from the nonlinear dynamics, This is independent of the given optimal control input. With this expression of , if can be assumed to be (may not be a valid assumption), then the following can be said of the cost functional, which is the minimum, since the cost functional is nonnegative.","\dot{x} = -x^3 + u + wx, u w [-1,1] 
J = \int_0^{\infty} (x^2 + u^2) dt,
 
u = x^3 - x - x\sqrt{x^4 -2x^2 + 2}.
 
H = x^2 + u^2 + \lambda(-x^3 + u + wx).
 x u 
\dot{\lambda} = -\frac{\partial H}{\partial x} = -2x+ 3\lambda x^2 -\lambda wx \Rightarrow \lambda = ke^{(3x^2-w)t}+\frac{2x}{w-3x^2},
 
\frac{\partial H}{\partial u} = 0 \Rightarrow u = -\frac{\lambda}{2}.
 \lambda 
u = -\frac{k}{2}e^{(3x^2-w)t}+\frac{2x}{w-3x^2}
 \lambda k u u \dot{x} w 1 
\dot{x}^2 = x^2 (x^4 - 2x^2 +2) = x^6 - 2x^4 + 2x^2
 u 
\dot{x} = -x^3 + u + x \Rightarrow u = \dot{x} + x^3 - x 
 u \dot{x} 0 
\begin{aligned}
J = \int_0^{\infty} (x^2 + u^2) dt &= \int_0^{\infty} (x^2 + (\dot{x} + x^3 - x)^2) dt\\
&= \int_0^{\infty} (x^2 + x^6 - 2x^4 + x^2) dt\\
&= \int_0^{\infty} (x^6 - 2x^4 + 2x^2) dt\\
&= \int_0^{\infty} \dot{x}^2 dt = 0, 
\end{aligned}
","['ordinary-differential-equations', 'control-theory', 'optimal-control', 'maximum-principle']"
22,Solving heat equation using Fourier-series with non-homogeneous assymetric bondary conditions,Solving heat equation using Fourier-series with non-homogeneous assymetric bondary conditions,,"Problem I am having trouble with finding a solution which satisfies the boundary and initial conditions to this PDE: $$\frac{\partial u}{\partial t} = \frac{\partial ^2 u}{\partial^2x}$$ where $u=u(x,t)$ , $0 \leq x \leq L$ with boundary & initial conditions: BC1: $u(x=0,t>0)=T_f$ ; BC2: $u(x=L,t>0)=T_i$ ; IC: $u(x,t=0)=f(x)$ Related but different posts Motivation on Using Fourier Series to Solve Heat Equation : the answer to this uses BCs: $u(x=0,t)=u(x=L,t)=0 \forall t$ which is not the same as my BCs Solve Heat Equation using Fourier Transform (non homogeneous) : solving a modified version of the heat equation, Dirichlet BC Solving the heat equation using Fourier series : relies on the same source as I do ( wikipedia ), but it does not advance the simpler version of the problem outlined there, and I am attempting to do it here. non homogeneous heat equation? : different IC, not much elaborated What I do get Following the strategy outlined here , I do separation of variables: $$u(x,t) = X(x)T(t)$$ The PDE becomes: $$\frac{T'}{\alpha T}=\frac{X''}{X}$$ LHS only time dependence, RHS only x dependence, so they must be equal to a  constant. Let this constant be $-\lambda$ (with $\lambda > 0$ so we get an exponential decay rather than growth for the temporal equation). Temporal eq: $$T'=-\lambda \alpha T$$ which implies: $$T(t)=A e^{-\lambda \alpha t}$$ Spatial eq: $$X''+\lambda X = 0$$ having a solution: $$X(x) = B e^{\sqrt{-\lambda}x}+Ce^{-\sqrt{-\lambda}x}$$ which, since $\lambda>0$ , can be rewritten to: $$X(x) = B \sin(\sqrt{\lambda}x) + C \cos(\sqrt{\lambda}x)$$ Where issues begin If we had $u(x=0,L;t) = 0$ , I would be confident to use these to determine $\lambda$ & C to be: $$\lambda = \frac{n^2 \pi^2}{L^2}$$ $$C=0$$ Then, proceed by setting $A=1$ , so we have: $$u(x,t) = \sum_{n=0}^{\infty}B_{n} \sin\left(\frac{n\pi}{L}x\right) e^{-\frac{n^2 \pi^2}{L^2}\alpha t}$$ Then, determine $B_n$ s using orthogonality of the different frequency sine functions: $$B_n = \frac{2}{L}\int_0^Lf(x)\sin\left(\frac{n\pi}{L}x\right)dx$$ How do I attempt to tackle them Despite my IC is not $u(x=0,L;t) = 0$ , I have some hope for this path. Now my solution is: $$u(x,t) = e^{-\frac{n^2 \pi^2}{L^2}\alpha t} \sum_{n=0}^{\infty}\frac{2}{L}\left(\int_0^Lf(x)\sin\left(\frac{n\pi}{L}x\right)dx\right)\sin\left(\frac{n\pi}{L}x\right)$$ Which I believe satisfies my IC but not my BCs. Don't worry, lets add to $X(x)$ a line which make it satisfy the BCs as well (which are: $u(x=0,t>0)=T_f$ & $u(x=L,t>0)=T_i$ ). Let's call this $X_p$ : $$X_p(x) = \frac{T_i-T_f}{L}x$$ Now if I just add this to the previously found $X$ , I will obviously screw up the $B_n$ s, which were calculated such a way that the weighted sum of sines will give me $f(x)$ . If I just add $X_p$ , the weighted sum of sines and $X_p$ will give me $f(x)+X_p$ . Lets subtract $X_p$ from $f(x)$ when calculating the coefficeients of the sines, that way the weighted sum of the sines and $X_p$ will give me $f(x)-X_p+X_p = f(x)$ in $t=0$ , which is good, and in $t>0$ I still satisfy my BCs because I have added $X_p$ to the general solution. Where I arrive So my final result is: $$u(x,t) = e^{-\frac{n^2 \pi^2}{L^2}\alpha t} \left(\sum_{n=0}^{\infty}\frac{2}{L}\left(\int_0^L\left(f(x) - \frac{T_i-T_f}{L}x\right)\sin\left(\frac{n\pi}{L}x\right)dx\right)\sin\left(\frac{n\pi}{L}x\right)+\frac{T_i-T_f}{L}x\right)$$ Question Is this a right way of obtaining the solution to the equation, or is it completely off track? Is there a name for the different steps I am using? If there is a standard way of solving these kind of equations which is not this way, I'd like to know. (My guess would be that the $X_p$ is some kind of ""particular solution"", but I have used that term in different context, and my memory regarding terminology is pretty short.)","Problem I am having trouble with finding a solution which satisfies the boundary and initial conditions to this PDE: where , with boundary & initial conditions: BC1: ; BC2: ; IC: Related but different posts Motivation on Using Fourier Series to Solve Heat Equation : the answer to this uses BCs: which is not the same as my BCs Solve Heat Equation using Fourier Transform (non homogeneous) : solving a modified version of the heat equation, Dirichlet BC Solving the heat equation using Fourier series : relies on the same source as I do ( wikipedia ), but it does not advance the simpler version of the problem outlined there, and I am attempting to do it here. non homogeneous heat equation? : different IC, not much elaborated What I do get Following the strategy outlined here , I do separation of variables: The PDE becomes: LHS only time dependence, RHS only x dependence, so they must be equal to a  constant. Let this constant be (with so we get an exponential decay rather than growth for the temporal equation). Temporal eq: which implies: Spatial eq: having a solution: which, since , can be rewritten to: Where issues begin If we had , I would be confident to use these to determine & C to be: Then, proceed by setting , so we have: Then, determine s using orthogonality of the different frequency sine functions: How do I attempt to tackle them Despite my IC is not , I have some hope for this path. Now my solution is: Which I believe satisfies my IC but not my BCs. Don't worry, lets add to a line which make it satisfy the BCs as well (which are: & ). Let's call this : Now if I just add this to the previously found , I will obviously screw up the s, which were calculated such a way that the weighted sum of sines will give me . If I just add , the weighted sum of sines and will give me . Lets subtract from when calculating the coefficeients of the sines, that way the weighted sum of the sines and will give me in , which is good, and in I still satisfy my BCs because I have added to the general solution. Where I arrive So my final result is: Question Is this a right way of obtaining the solution to the equation, or is it completely off track? Is there a name for the different steps I am using? If there is a standard way of solving these kind of equations which is not this way, I'd like to know. (My guess would be that the is some kind of ""particular solution"", but I have used that term in different context, and my memory regarding terminology is pretty short.)","\frac{\partial u}{\partial t} = \frac{\partial ^2 u}{\partial^2x} u=u(x,t) 0 \leq x \leq L u(x=0,t>0)=T_f u(x=L,t>0)=T_i u(x,t=0)=f(x) u(x=0,t)=u(x=L,t)=0 \forall t u(x,t) = X(x)T(t) \frac{T'}{\alpha T}=\frac{X''}{X} -\lambda \lambda > 0 T'=-\lambda \alpha T T(t)=A e^{-\lambda \alpha t} X''+\lambda X = 0 X(x) = B e^{\sqrt{-\lambda}x}+Ce^{-\sqrt{-\lambda}x} \lambda>0 X(x) = B \sin(\sqrt{\lambda}x) + C \cos(\sqrt{\lambda}x) u(x=0,L;t) = 0 \lambda \lambda = \frac{n^2 \pi^2}{L^2} C=0 A=1 u(x,t) = \sum_{n=0}^{\infty}B_{n} \sin\left(\frac{n\pi}{L}x\right) e^{-\frac{n^2 \pi^2}{L^2}\alpha t} B_n B_n = \frac{2}{L}\int_0^Lf(x)\sin\left(\frac{n\pi}{L}x\right)dx u(x=0,L;t) = 0 u(x,t) = e^{-\frac{n^2 \pi^2}{L^2}\alpha t} \sum_{n=0}^{\infty}\frac{2}{L}\left(\int_0^Lf(x)\sin\left(\frac{n\pi}{L}x\right)dx\right)\sin\left(\frac{n\pi}{L}x\right) X(x) u(x=0,t>0)=T_f u(x=L,t>0)=T_i X_p X_p(x) = \frac{T_i-T_f}{L}x X B_n f(x) X_p X_p f(x)+X_p X_p f(x) X_p f(x)-X_p+X_p = f(x) t=0 t>0 X_p u(x,t) = e^{-\frac{n^2 \pi^2}{L^2}\alpha t} \left(\sum_{n=0}^{\infty}\frac{2}{L}\left(\int_0^L\left(f(x) - \frac{T_i-T_f}{L}x\right)\sin\left(\frac{n\pi}{L}x\right)dx\right)\sin\left(\frac{n\pi}{L}x\right)+\frac{T_i-T_f}{L}x\right) X_p","['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'fourier-series', 'linear-pde']"
23,"Does there exist $f(x)$ such that $\int_0^x f(t) \, dt +x=1$?",Does there exist  such that ?,"f(x) \int_0^x f(t) \, dt +x=1","Does there exist $f(x)$ (continuous) such that $\int_0^x f(t) \, dt +x=1$ ? My guess is that it doesn't since setting $x=0$ we obtain $0=1$ . Is this assertion correct?",Does there exist (continuous) such that ? My guess is that it doesn't since setting we obtain . Is this assertion correct?,"f(x) \int_0^x f(t) \, dt +x=1 x=0 0=1",['calculus']
24,Property of solution to initial value problem,Property of solution to initial value problem,,"Let $f\colon [0,\infty) \to [0, \infty)$ be a continuous function such that $\int_0^\infty f(t) \, dt = \infty$ . Assume that $y \colon [0, \infty) \to \mathbb R$ is a solution to the initial value problem $$\begin{cases}y'' + f(t) y = 0 \\ y(0) = 1 \end{cases}. $$ I want to show that $y$ has infinitely many zeros that do not accumulate anywhere and at each zero the derivative of $y$ is non-vanishing. Furthermore, between the zeros $y$ is either positive and concave or negative and convex. Thoughts so far: I could not come up with something smart that uses the integral assumption on $f$ . Clearly the theorem holds for constant $f$ , e.g. $f = 1$ . Then the solutions are of the form $y(t) = A\sin(t) + B\cos(t)$ and satisfy the desired properties. Plotting the solution for other choices of $f$ give similar trigonometric patterns of the solution, but I could not make it further.","Let be a continuous function such that . Assume that is a solution to the initial value problem I want to show that has infinitely many zeros that do not accumulate anywhere and at each zero the derivative of is non-vanishing. Furthermore, between the zeros is either positive and concave or negative and convex. Thoughts so far: I could not come up with something smart that uses the integral assumption on . Clearly the theorem holds for constant , e.g. . Then the solutions are of the form and satisfy the desired properties. Plotting the solution for other choices of give similar trigonometric patterns of the solution, but I could not make it further.","f\colon [0,\infty) \to [0, \infty) \int_0^\infty f(t) \, dt = \infty y \colon [0, \infty) \to \mathbb R \begin{cases}y'' + f(t) y = 0 \\ y(0) = 1 \end{cases}.  y y y f f f = 1 y(t) = A\sin(t) + B\cos(t) f","['real-analysis', 'ordinary-differential-equations', 'analysis', 'derivatives']"
25,How to solve this relatively simple non-linear ODE?,How to solve this relatively simple non-linear ODE?,,"I'm having troubles in simplifying a differential equation to find its solutions.  Consider this ODE: $$     \frac{1}{r} \, \frac{d}{dr} \Bigl( \frac{r}{B} \, \frac{d B}{dr} \Bigr) = k \, B^2, \tag{1} $$ where $k$ is just a real constant and $B(r)$ is the function to be found.  I already know one solution of this equation (it was found by guessing, trial and errors): $$ B(r) = \frac{\beta}{1 + \lambda r^2}, \tag{2} $$ where $\beta$ and $\lambda = k \beta^2 / 4$ are arbitrary constants.  My goal is to transform (1) into an integral so I could find back the solution (2).  I'm yet unable to find a change of variable that makes that equation solvable analyticaly.  I tried $u = 1/B$ , and $B = e^{\phi(r)}$ (and few other trials).  Any idea how to solve (1)?","I'm having troubles in simplifying a differential equation to find its solutions.  Consider this ODE: where is just a real constant and is the function to be found.  I already know one solution of this equation (it was found by guessing, trial and errors): where and are arbitrary constants.  My goal is to transform (1) into an integral so I could find back the solution (2).  I'm yet unable to find a change of variable that makes that equation solvable analyticaly.  I tried , and (and few other trials).  Any idea how to solve (1)?","
    \frac{1}{r} \, \frac{d}{dr} \Bigl( \frac{r}{B} \, \frac{d B}{dr} \Bigr) = k \, B^2, \tag{1}
 k B(r) 
B(r) = \frac{\beta}{1 + \lambda r^2}, \tag{2}
 \beta \lambda = k \beta^2 / 4 u = 1/B B = e^{\phi(r)}","['ordinary-differential-equations', 'functions', 'solution-verification']"
26,Can convolution be expressed as a differential equation?,Can convolution be expressed as a differential equation?,,"The integral equation for (causal) convolution is given by $$y(t) = \int_{-\infty}^{t} K(t - \tau) x(\tau) d\tau$$ Can one write an equivalent differential equation for general well-behaved kernel $K$ ? Since convolution with an exponential kernel is a solution to a linear ODE with with time-dependent input (e.g. see 2nd example here ), the transformation is possible for some kernels. If it turns out that there are well-behaved kernels for which the transformation is not possible, it would be great to address the question on determining the set of kernels for which the transformation is possible. EDIT : I have found a related post , where respondents argue that this operation is indeed impossible for general kernels. I will comment further when I have fully understood if the outlined arguments apply to my case, where causality is enforced by the limits of integration.","The integral equation for (causal) convolution is given by Can one write an equivalent differential equation for general well-behaved kernel ? Since convolution with an exponential kernel is a solution to a linear ODE with with time-dependent input (e.g. see 2nd example here ), the transformation is possible for some kernels. If it turns out that there are well-behaved kernels for which the transformation is not possible, it would be great to address the question on determining the set of kernels for which the transformation is possible. EDIT : I have found a related post , where respondents argue that this operation is indeed impossible for general kernels. I will comment further when I have fully understood if the outlined arguments apply to my case, where causality is enforced by the limits of integration.",y(t) = \int_{-\infty}^{t} K(t - \tau) x(\tau) d\tau K,"['calculus', 'ordinary-differential-equations', 'convolution']"
27,Closed form of recurrence relation $a_k=(n+k)a_{k-1}-ka_{k-2}$,Closed form of recurrence relation,a_k=(n+k)a_{k-1}-ka_{k-2},"The goal is to derive a closed form expression of $(a_k)_{k\ge1}$ such that $$a_k=(n+k)a_{k-1}-ka_{k-2}$$ for all positive integers $n$ , given the values of $a_0$ and $a_{-1}$ . Considering the generating function $A(x)=\sum\limits_{k\ge0}a_kx^k$ , we have \begin{align}A(x)&=a_0+a_1x+n\sum_{k\ge2}a_{k-1}x^k+\sum_{k\ge2}k(a_{k-1}-a_{k-2})x^k\\&=a_0+a_1x+nx(A(x)-a_0)+x\frac d{dx}\sum_{k\ge2}(a_{k-1}-a_{k-2})x^k\\&=a_0+a_1x-na_0x+nxA(x)+x\frac d{dx}(x(A(x)-a_0)-x^2A(x))\\&=a_0+a_1x-na_0x+nxA(x)+x[A(x)-a_0+xA'(x)-2xA(x)-x^2A'(x)]\end{align} which gives the first-order linear ODE $$x^2(x-1)A'(x)+(2x^2-(n+1)x+1)A(x)=a_0+(a_1-(n+1)a_0)x,$$ but this does not provide a closed form of $A(x)$ for all positive $n$ . Are there any other approaches that can be used to solve this recurrence relation?","The goal is to derive a closed form expression of such that for all positive integers , given the values of and . Considering the generating function , we have which gives the first-order linear ODE but this does not provide a closed form of for all positive . Are there any other approaches that can be used to solve this recurrence relation?","(a_k)_{k\ge1} a_k=(n+k)a_{k-1}-ka_{k-2} n a_0 a_{-1} A(x)=\sum\limits_{k\ge0}a_kx^k \begin{align}A(x)&=a_0+a_1x+n\sum_{k\ge2}a_{k-1}x^k+\sum_{k\ge2}k(a_{k-1}-a_{k-2})x^k\\&=a_0+a_1x+nx(A(x)-a_0)+x\frac d{dx}\sum_{k\ge2}(a_{k-1}-a_{k-2})x^k\\&=a_0+a_1x-na_0x+nxA(x)+x\frac d{dx}(x(A(x)-a_0)-x^2A(x))\\&=a_0+a_1x-na_0x+nxA(x)+x[A(x)-a_0+xA'(x)-2xA(x)-x^2A'(x)]\end{align} x^2(x-1)A'(x)+(2x^2-(n+1)x+1)A(x)=a_0+(a_1-(n+1)a_0)x, A(x) n","['ordinary-differential-equations', 'recurrence-relations', 'generating-functions', 'closed-form']"
28,Concluding that sine and cosine are $2\pi$ periodic from definitions,Concluding that sine and cosine are  periodic from definitions,2\pi,"Let $(f,g)$ be a pair of real-valued $C^1$ functions on $\mathbb{R}$ satisfying $$\forall x\in\mathbb{R}\left(f'(x)=g(x)\quad\text{and}\quad g'(x)=-f(x)\right)$$ $$f(0)=0\quad\text{and}\quad g(0)=1$$ Then it is pretty immediate that $f$ and $g$ are both $C^\infty$ on $\mathbb{R}$ , this determines a power series which determines uniqueness of the pair. A cute argument (without power series) shows that these functions  satisfy $f(x)^2+g(x)^2=1$ . Here's my question: how do we deduce that $f(x)$ and $g(x)$ are $2\pi$ periodic? I understand that we could have started the entire axiomatic system by defining $f(x)$ and $g(x)$ in terms of triangles, but since we already have uniqueness from this setup I wonder how we deduce periodicity from here.","Let be a pair of real-valued functions on satisfying Then it is pretty immediate that and are both on , this determines a power series which determines uniqueness of the pair. A cute argument (without power series) shows that these functions  satisfy . Here's my question: how do we deduce that and are periodic? I understand that we could have started the entire axiomatic system by defining and in terms of triangles, but since we already have uniqueness from this setup I wonder how we deduce periodicity from here.","(f,g) C^1 \mathbb{R} \forall x\in\mathbb{R}\left(f'(x)=g(x)\quad\text{and}\quad g'(x)=-f(x)\right) f(0)=0\quad\text{and}\quad g(0)=1 f g C^\infty \mathbb{R} f(x)^2+g(x)^2=1 f(x) g(x) 2\pi f(x) g(x)","['ordinary-differential-equations', 'trigonometry']"
29,Solving a 2nd order DE (non-constant coefficients),Solving a 2nd order DE (non-constant coefficients),,"I am having trouble solving the following differential equation: $$y''+(x^2-1)y'+2xy=0$$ To start off I employed the power-series method, so let $$y=\sum_{n=0}^{\infty}a_nx^n$$ $$y'=\sum_{n=1}^{\infty}na_nx^{n-1}$$ $$y''=\sum_{n=2}^{\infty}n(n-1)a_nx^{n-2}$$ Next I plugged my power-series into my original DE: $$\sum_{n=2}^{\infty}n(n-1)a_nx^{n-2}+\sum_{n=1}^{\infty}na_nx^{n+1}-\sum_{n=1}^{\infty}na_nx^{n-1}+\sum_{n=0}^{\infty}2a_nx^{n+1}=0$$ When I matched the powers of $x$ I ended up with the following equation: $$\sum_{n=0}^{\infty}(n+2)(n+1)a_{n+2}x^n+\sum_{n=2}^{\infty}(n-1)a_{n-1}x^n-\sum_{n=0}^{\infty}(n+1)a_{n+1}x^n+\sum_{n=1}^{\infty}2a_{n-1}x^n=0$$ Next I tried to match the indeces, so naturally I took out the first 2 terms from $\sum_{n=0}^{\infty}(n+2)(n+1)a_{n+2}x^n$ , the first 2 terms from $\sum_{n=0}^{\infty}(n+1)a_{n+1}x^n$ , and the first term from $\sum_{n=1}^{\infty}2a_{n-1}x^n$ . At this point I end up with 2 recurrence relations: $$2a_2+6a_3x-a_0-2a_2x+2a_0x=0$$ $$(n+2)(n+1)a_{n+2}+(n-1)a_{n-1}-(n+1)a_{n+1}+2a_{n-1}=0$$ And here I am completely lost, normally what I would do is solve for one of the first $a_n$ terms but since I've got $x$ 's in the first recurrence I can't really see how that would work. Perhaps there is a much simpler method that I can use? This is just practice for my ODE final exam.","I am having trouble solving the following differential equation: To start off I employed the power-series method, so let Next I plugged my power-series into my original DE: When I matched the powers of I ended up with the following equation: Next I tried to match the indeces, so naturally I took out the first 2 terms from , the first 2 terms from , and the first term from . At this point I end up with 2 recurrence relations: And here I am completely lost, normally what I would do is solve for one of the first terms but since I've got 's in the first recurrence I can't really see how that would work. Perhaps there is a much simpler method that I can use? This is just practice for my ODE final exam.",y''+(x^2-1)y'+2xy=0 y=\sum_{n=0}^{\infty}a_nx^n y'=\sum_{n=1}^{\infty}na_nx^{n-1} y''=\sum_{n=2}^{\infty}n(n-1)a_nx^{n-2} \sum_{n=2}^{\infty}n(n-1)a_nx^{n-2}+\sum_{n=1}^{\infty}na_nx^{n+1}-\sum_{n=1}^{\infty}na_nx^{n-1}+\sum_{n=0}^{\infty}2a_nx^{n+1}=0 x \sum_{n=0}^{\infty}(n+2)(n+1)a_{n+2}x^n+\sum_{n=2}^{\infty}(n-1)a_{n-1}x^n-\sum_{n=0}^{\infty}(n+1)a_{n+1}x^n+\sum_{n=1}^{\infty}2a_{n-1}x^n=0 \sum_{n=0}^{\infty}(n+2)(n+1)a_{n+2}x^n \sum_{n=0}^{\infty}(n+1)a_{n+1}x^n \sum_{n=1}^{\infty}2a_{n-1}x^n 2a_2+6a_3x-a_0-2a_2x+2a_0x=0 (n+2)(n+1)a_{n+2}+(n-1)a_{n-1}-(n+1)a_{n+1}+2a_{n-1}=0 a_n x,"['ordinary-differential-equations', 'power-series']"
30,Newton's Law of cooling problem when the thermometer brought indoors,Newton's Law of cooling problem when the thermometer brought indoors,,"At $2$ PM a thermometer reading $80^{\circ}$ F is taken outside. Where the air temperature is $20^{\circ}$ F. At $2:03$ PM the temperature reading yielded by the thermometer is $42^{\circ}$ F. Later, the thermometer brought inside. Where the air temperature is at $80^{\circ}$ F. At $2:10$ PM the reading is $71^{\circ}$ F. When was the thermometer brought indoors $?$ $1$ st part, Using cooling law, $$T=(T_0-T_m)e^{-kt}+T_m \quad T_0=\text{initial temp.}\quad T_m=\text{medium/Air temp.}$$ \begin{align} 42&=(80-20)e^{-k\times3}+20\\ k&=-\frac{1}{3}\ln\left(\frac{11}{30}\right)\\ T(t)&=60e^{\frac{1}{3}\ln\left(\frac{11}{30}\right)t}+20 \end{align} $2$ nd part, Now let after $x$ min the thermometer brought indoors then $T(x)=60e^{\frac{1}{3}\ln\left(\frac{11}{30}\right)x}+20$ which will be $T_0$ for second part where, $$T(t)=T_m-(T_m-T_0)e^{-kt}\quad\text{Using warming Law}$$ \begin{align} T(?)=80-\left(100-60e^{\frac{1}{3}\ln\left(\frac{11}{30}\right)x}\right)e^{-k\times?} \end{align} Now I am lost how to processed. Any help will be appreciated.","At PM a thermometer reading F is taken outside. Where the air temperature is F. At PM the temperature reading yielded by the thermometer is F. Later, the thermometer brought inside. Where the air temperature is at F. At PM the reading is F. When was the thermometer brought indoors st part, Using cooling law, nd part, Now let after min the thermometer brought indoors then which will be for second part where, Now I am lost how to processed. Any help will be appreciated.","2 80^{\circ} 20^{\circ} 2:03 42^{\circ} 80^{\circ} 2:10 71^{\circ} ? 1 T=(T_0-T_m)e^{-kt}+T_m \quad T_0=\text{initial temp.}\quad T_m=\text{medium/Air temp.} \begin{align}
42&=(80-20)e^{-k\times3}+20\\
k&=-\frac{1}{3}\ln\left(\frac{11}{30}\right)\\
T(t)&=60e^{\frac{1}{3}\ln\left(\frac{11}{30}\right)t}+20
\end{align} 2 x T(x)=60e^{\frac{1}{3}\ln\left(\frac{11}{30}\right)x}+20 T_0 T(t)=T_m-(T_m-T_0)e^{-kt}\quad\text{Using warming Law} \begin{align}
T(?)=80-\left(100-60e^{\frac{1}{3}\ln\left(\frac{11}{30}\right)x}\right)e^{-k\times?}
\end{align}","['ordinary-differential-equations', 'initial-value-problems']"
31,How to solve given differential equation $(x^3y^3+x^2y^2+xy+1)ydx+(x^3y^3-x^2y^2-xy+1)xdy=0$?,How to solve given differential equation ?,(x^3y^3+x^2y^2+xy+1)ydx+(x^3y^3-x^2y^2-xy+1)xdy=0,The equation to solve is: $$(x^3y^3+x^2y^2+xy+1)ydx+(x^3y^3-x^2y^2-xy+1)xdy=0$$ I tried putting $xy=t$ but that just gave me this: $$\frac{t^3-t^2-t+1}{t^3+t^2+t+1}dt=\frac{dx}{x}$$ I suppose there must be some clever factoring involved somewhere but I can't see it so can someone guide me on how to advance or perhaps suggest an alternate method?,The equation to solve is: I tried putting but that just gave me this: I suppose there must be some clever factoring involved somewhere but I can't see it so can someone guide me on how to advance or perhaps suggest an alternate method?,(x^3y^3+x^2y^2+xy+1)ydx+(x^3y^3-x^2y^2-xy+1)xdy=0 xy=t \frac{t^3-t^2-t+1}{t^3+t^2+t+1}dt=\frac{dx}{x},['ordinary-differential-equations']
32,PDE as a system of ODEs,PDE as a system of ODEs,,It has been said that a PDE is like an infinite dimensional system of ODEs. How can one see this by a clear example? Can any PDE be transformed into an infinite dimensional system of ODEs?,It has been said that a PDE is like an infinite dimensional system of ODEs. How can one see this by a clear example? Can any PDE be transformed into an infinite dimensional system of ODEs?,,"['ordinary-differential-equations', 'partial-differential-equations', 'systems-of-equations', 'dynamical-systems']"
33,How to solve this D.E $y''(\frac{x}{2})+y'(\frac{x}{2})+y(x)=x$,How to solve this D.E,y''(\frac{x}{2})+y'(\frac{x}{2})+y(x)=x,I know how to slove $y''(x)+y'(x)+y(x)=x$ But I couldn't solve this $$y''(\frac{x}{2})+y'(\frac{x}{2})+y(x)=x$$ any hint to help me? Thanls,I know how to slove But I couldn't solve this any hint to help me? Thanls,y''(x)+y'(x)+y(x)=x y''(\frac{x}{2})+y'(\frac{x}{2})+y(x)=x,['ordinary-differential-equations']
34,Solution of second ordinary equation,Solution of second ordinary equation,,"i have the following question. Let $\phi_1$ and $\phi_2$ fundamental system solutions on an interval $I$ for the second order equation $$ y''+a(x)y= 0. $$ Prove that there exists fundamental system solutions $\{y_1,y_2\}$ such that the Wronksian $W[y_1,y_2]$ satisfies $W[y_1,y_2]=1$ . So, I know that $\{y_1,y_2\}$ is a system of fundamental solution means that any solution $y$ of edo is written: $y(x)= c_1 y_1(x)+ c_2 y_2(x)$ where $c_1$ and $c_2$ arbitrary contacts. Then $W[y_1,y_2]= y_1(x)y'_2(x)-y_2(x)y_1'(x)$ . But I don't know how to resolve the question and what's utility of $\phi_1$ and $\phi_2$ . Thank's in advance to the help.","i have the following question. Let and fundamental system solutions on an interval for the second order equation Prove that there exists fundamental system solutions such that the Wronksian satisfies . So, I know that is a system of fundamental solution means that any solution of edo is written: where and arbitrary contacts. Then . But I don't know how to resolve the question and what's utility of and . Thank's in advance to the help.","\phi_1 \phi_2 I 
y''+a(x)y= 0.
 \{y_1,y_2\} W[y_1,y_2] W[y_1,y_2]=1 \{y_1,y_2\} y y(x)= c_1 y_1(x)+ c_2 y_2(x) c_1 c_2 W[y_1,y_2]= y_1(x)y'_2(x)-y_2(x)y_1'(x) \phi_1 \phi_2","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
35,What are autonomous and non-autonomous systems??,What are autonomous and non-autonomous systems??,,What are autonomous and non-autonomous systems and how are they different from each other. What are the differences between the types of systems they are describing? Do both autonomous and non-autonomous system describe dynamical systems or are dynamical systems something different?,What are autonomous and non-autonomous systems and how are they different from each other. What are the differences between the types of systems they are describing? Do both autonomous and non-autonomous system describe dynamical systems or are dynamical systems something different?,,"['ordinary-differential-equations', 'multivariable-calculus', 'dynamical-systems']"
36,What is the difference between a derivative and a differential equation?,What is the difference between a derivative and a differential equation?,,"If function $y=x^2$ , then the derivative of $y$ is $2x$ . We write the derivative as either $f'(x)=2x$ or $\frac{\text dy}{\text dx}=2x$ . In this case $\frac{\text dy}{\text dx}=2x$ is also a differential equation. But if we have the function $y=ax^2+bx+c$ then in this case we will have $\frac{\text dy}{\text dx}= 2ax+b$ . Is $\frac{\text dy}{\text dx}= 2ax+b$ also considered as a differential equation or we will have to remove all arbitrary constants from the function to call it a differential equation?","If function , then the derivative of is . We write the derivative as either or . In this case is also a differential equation. But if we have the function then in this case we will have . Is also considered as a differential equation or we will have to remove all arbitrary constants from the function to call it a differential equation?",y=x^2 y 2x f'(x)=2x \frac{\text dy}{\text dx}=2x \frac{\text dy}{\text dx}=2x y=ax^2+bx+c \frac{\text dy}{\text dx}= 2ax+b \frac{\text dy}{\text dx}= 2ax+b,"['calculus', 'ordinary-differential-equations', 'derivatives']"
37,Finding approximation to stable manifold of saddle point,Finding approximation to stable manifold of saddle point,,"I am stuck on the following exercise from Strogatz' book on dynamical systems (exercise 6.1.14). Consider the system $\dot{x} = x+e^{-y}, \dot{y} = -y$ . This system   has a single fixed point, $(-1, 0)$ . This is a saddle point. The   unstable manifold is $y=0$ , but the stable manifold is some non-linear   curve. Let $(x, y)$ be a point on the stable manifold close to $(-1, > 0)$ and define $u = x + 1$ . Write the stable manifold as $y=a_1u +a_2u^2 + O(u^3)$ . To determine the coefficients, derive two   expressions for $dy/du$ and equate them. I have, as a first try for an equation, simply differentiated $y$ wrt $u$ : $\frac{dy}{du} = a_1 + 2a_2u + O(u^2)$ , where I'm a bit uncertain about the $O(u^2)$ , but I suspect we can leave that out anyway, since we're approximating. I'm not sure how we'd find a second equation. I've found a post that answers the same question , but it ends up with a line, rather than the non-linear curve that is pictured in the book: Furthermore, I don't see why they have $\frac{dy}{du} = \frac{\dot{y}}{\dot{u}}$ or how they calculated the Taylor approximation for $\dot{u}$ . Basically, I'm a bit lost, could someone perhaps give me some hints?","I am stuck on the following exercise from Strogatz' book on dynamical systems (exercise 6.1.14). Consider the system . This system   has a single fixed point, . This is a saddle point. The   unstable manifold is , but the stable manifold is some non-linear   curve. Let be a point on the stable manifold close to and define . Write the stable manifold as . To determine the coefficients, derive two   expressions for and equate them. I have, as a first try for an equation, simply differentiated wrt : , where I'm a bit uncertain about the , but I suspect we can leave that out anyway, since we're approximating. I'm not sure how we'd find a second equation. I've found a post that answers the same question , but it ends up with a line, rather than the non-linear curve that is pictured in the book: Furthermore, I don't see why they have or how they calculated the Taylor approximation for . Basically, I'm a bit lost, could someone perhaps give me some hints?","\dot{x} = x+e^{-y}, \dot{y} = -y (-1, 0) y=0 (x, y) (-1,
> 0) u = x + 1 y=a_1u +a_2u^2 + O(u^3) dy/du y u \frac{dy}{du} = a_1 + 2a_2u + O(u^2) O(u^2) \frac{dy}{du} = \frac{\dot{y}}{\dot{u}} \dot{u}","['ordinary-differential-equations', 'taylor-expansion', 'dynamical-systems']"
38,How to solve an ODE of this form $\dot{P}(t)=A^TP(t)+P(t)A$?,How to solve an ODE of this form ?,\dot{P}(t)=A^TP(t)+P(t)A,"Background If $\dot{x}(t)=A \,x(t)$ , then we know the solution is $x(t)=e^{At}x(0)$ . Question Now let $\dot{P}(t)=A^TP(t)+P(t)A$ , what is $P(t)$ ? Attempt If we take $P(t)$ as common factor (I'm not sure I'm doing this correctly though) ,then we have: $$ \dot{P}(t)=A^TP(t)+P(t)A=(A^T+A)P(t) $$ and using the same rationale in the background section, we have $$ P(t)=e^{(A^T+A) t}P(0)=e^{A^T t}P(0)e^{At} $$ Is this a correct solution? Note I know the answer is $P(t)=e^{A^T t}P(0)e^{At}$ but I'm not sure how to find it.","Background If , then we know the solution is . Question Now let , what is ? Attempt If we take as common factor (I'm not sure I'm doing this correctly though) ,then we have: and using the same rationale in the background section, we have Is this a correct solution? Note I know the answer is but I'm not sure how to find it.","\dot{x}(t)=A \,x(t) x(t)=e^{At}x(0) \dot{P}(t)=A^TP(t)+P(t)A P(t) P(t) 
\dot{P}(t)=A^TP(t)+P(t)A=(A^T+A)P(t)
 
P(t)=e^{(A^T+A) t}P(0)=e^{A^T t}P(0)e^{At}
 P(t)=e^{A^T t}P(0)e^{At}","['ordinary-differential-equations', 'analysis', 'lyapunov-functions']"
39,What's an example where Lyapunov fails to find the bounds of stability,What's an example where Lyapunov fails to find the bounds of stability,,"In linear control theory, a system is stable if and only if if satisfies the Routh–Hurwitz stability criterion, so we can use this to solve for the limits of stability. E.g. you can find the maximum gain that will allow for a stable system. However, for nonlinear systems, if we can find a candidate function that satisfies the Lyapunov conditions, the system is stable, but the failure of a candidate function to satisfy the conditions doesn't mean it's unstable. What is an example where one can find a Lyapunov candidate function that proves stability for a range of parameters, but where the system is still stable outside that range? I.e. where the stability bounds suggested by the Lyapunov function aren't the actual stability bounds?","In linear control theory, a system is stable if and only if if satisfies the Routh–Hurwitz stability criterion, so we can use this to solve for the limits of stability. E.g. you can find the maximum gain that will allow for a stable system. However, for nonlinear systems, if we can find a candidate function that satisfies the Lyapunov conditions, the system is stable, but the failure of a candidate function to satisfy the conditions doesn't mean it's unstable. What is an example where one can find a Lyapunov candidate function that proves stability for a range of parameters, but where the system is still stable outside that range? I.e. where the stability bounds suggested by the Lyapunov function aren't the actual stability bounds?",,"['ordinary-differential-equations', 'control-theory', 'stability-in-odes', 'stability-theory', 'lyapunov-functions']"
40,"Find order, degree & linearity of ordinary D.E.","Find order, degree & linearity of ordinary D.E.",,"Given an ordinary differential equation (O.D.E.): $$y\left(\frac{d^5y}{dx^5}\right)^2+x^3\left(\frac{d^3y}{dx^3}\right)^3+x\left(\frac{dy}{dx}\right)=y^2\sin(x^2)$$ Which of the following options is correct for given O.D.E.? a) It is a fifth order linear O.D.E. of degree 2 b) It is a fifth order non-linear O.D.E. of degree 3 c) It is a fifth order non-linear O.D.E. of degree 2 d) None My try: Given D.E. $y\left(\frac{d^5y}{dx^5}\right)^2+x^3\left(\frac{d^3y}{dx^3}\right)^3+x\left(\frac{dy}{dx}\right)=y^2\sin(x^2)$ The order of O.D.E.=highest order=5 The degree of O.D.E.=degree of highest order derivative =2 Since $y$ is multiplied with $\left(\frac{d^5y}{dx^5}\right)$ or $y$ has a power $2$ so it is a non-linear O.D.E. Thus, My answer becomes (c) but I am not sure if i am right. Please tell me if I am wrong or right & correct me if i am wrong & give explanation. Thank you.","Given an ordinary differential equation (O.D.E.): Which of the following options is correct for given O.D.E.? a) It is a fifth order linear O.D.E. of degree 2 b) It is a fifth order non-linear O.D.E. of degree 3 c) It is a fifth order non-linear O.D.E. of degree 2 d) None My try: Given D.E. The order of O.D.E.=highest order=5 The degree of O.D.E.=degree of highest order derivative =2 Since is multiplied with or has a power so it is a non-linear O.D.E. Thus, My answer becomes (c) but I am not sure if i am right. Please tell me if I am wrong or right & correct me if i am wrong & give explanation. Thank you.",y\left(\frac{d^5y}{dx^5}\right)^2+x^3\left(\frac{d^3y}{dx^3}\right)^3+x\left(\frac{dy}{dx}\right)=y^2\sin(x^2) y\left(\frac{d^5y}{dx^5}\right)^2+x^3\left(\frac{d^3y}{dx^3}\right)^3+x\left(\frac{dy}{dx}\right)=y^2\sin(x^2) y \left(\frac{d^5y}{dx^5}\right) y 2,"['calculus', 'ordinary-differential-equations']"
41,How to interpret $\frac{f(x)}{f'(x)}$ being a decreasing function?,How to interpret  being a decreasing function?,\frac{f(x)}{f'(x)},"I have a model where $f(x)$ is a decreasing, convex function from mapping $[0,\infty)$ to $[0,1]$. How do I interpret a condition on $f(x)$ that the function $\frac{f(x)}{f'(x)}$ is also decreasing in $x$? Is there a family of functions that I need to consider in this case?","I have a model where $f(x)$ is a decreasing, convex function from mapping $[0,\infty)$ to $[0,1]$. How do I interpret a condition on $f(x)$ that the function $\frac{f(x)}{f'(x)}$ is also decreasing in $x$? Is there a family of functions that I need to consider in this case?",,"['real-analysis', 'ordinary-differential-equations', 'derivatives']"
42,Change to polar coordinate in ODEs,Change to polar coordinate in ODEs,,"Consider a planar ODE (e.g., $\dot{x}=f_1(x,y),\dot{y}=f_2(x,y)$). According to page $68$ of ODE with Applications by Chicone , a change to polar coordinates (i.e., $x=r\cos(\theta),y=r\sin(\theta)$) in this system introduces a singularity on the line $\{(r,\theta): r=0 \}$. Furthermore, if the original ODE has a rest point at the origin, this singularity is removable. Why is this true? What is the intuition? How can we prove this result? Thank you.","Consider a planar ODE (e.g., $\dot{x}=f_1(x,y),\dot{y}=f_2(x,y)$). According to page $68$ of ODE with Applications by Chicone , a change to polar coordinates (i.e., $x=r\cos(\theta),y=r\sin(\theta)$) in this system introduces a singularity on the line $\{(r,\theta): r=0 \}$. Furthermore, if the original ODE has a rest point at the origin, this singularity is removable. Why is this true? What is the intuition? How can we prove this result? Thank you.",,"['real-analysis', 'ordinary-differential-equations', 'differential-geometry', 'polar-coordinates', 'control-theory']"
43,Understanding solution of PDE using method: separation of variables.,Understanding solution of PDE using method: separation of variables.,,"Could someone please help me to understand the doubts I have about the solution of this pde problem and to check the things that I've added to the solution? Oscillations of the beam are described by equation $u_{tt}+Ku_{xxxx}=0, \ with\ 0\le x\le L,K>0$. If both ends clamped, then the boundary conditions are $u(0,t)=u_x(0,t)=0\\u(L,t)=u_x(L,t)=0.$ Use separation of variables to find eigenvalues and their corresponding eigenfunctions. Solution. First off consider the change of coordinates $[0, L]\ to \ [-L,L],L=1/2.$ Separating variables $u(x,t)=X(x)T(t)$ we arrive to $T''+\lambda T=0$ and $X^{iv}-\frac{\lambda}{k}X=0 $. If $w^4=\lambda/k$ then $$X^{(iv)}=w^4X\\X(L)=X'(L)=0\\X(-L)=X'(-L)=0\\T''+Kw^4T=0$$ w>0 Questions I understand the change from $[0, L]\ to \ [-L,L],L=1/2$, but why use it in the solution? Why not to use the original interval? From comments, Leucippus mentioned that conditions for $T$ are missing, how do I find such conditions? I don't understand the '..up to a constant factor $C=1$,..' in (4.17) red square, why to take $C=1$ ? Finally in (4.19) and (4.20) we arrive to the solutions $X(t)$ in which I don't see what is the eigenvalue (In my book eigenvalue was given like $\beta=\frac{\pi x}{l}$ and it came from applying the conditions). I think it's $w$ but how do I get $w$ to have the form $\lambda_n=\frac{n\pi }{l}$? I also think the solution is not complete, we also need solution of $T''+Kw^4T=0$ am I right? Could someone help me to understand please? Now suppose eigenvalue $\lambda=0$, then $kw^4=0.$ $$\implies X^{iv}=0,\ T''=0$$ Applying condition $x=0$ in the solution gives $X(x)=A$ and $T(t)=B$, thus $\lambda=0 $ is an eigenvalue and the eigenfunction is any constant. Now suppose eigenvalue $\lambda<0$, then $kw^4<0.$ I won't continue here since I don't know If I'm doing everything right or wrong. Note: An alternative solution is very welcome especially if it's easier than mine :) Thank you.","Could someone please help me to understand the doubts I have about the solution of this pde problem and to check the things that I've added to the solution? Oscillations of the beam are described by equation $u_{tt}+Ku_{xxxx}=0, \ with\ 0\le x\le L,K>0$. If both ends clamped, then the boundary conditions are $u(0,t)=u_x(0,t)=0\\u(L,t)=u_x(L,t)=0.$ Use separation of variables to find eigenvalues and their corresponding eigenfunctions. Solution. First off consider the change of coordinates $[0, L]\ to \ [-L,L],L=1/2.$ Separating variables $u(x,t)=X(x)T(t)$ we arrive to $T''+\lambda T=0$ and $X^{iv}-\frac{\lambda}{k}X=0 $. If $w^4=\lambda/k$ then $$X^{(iv)}=w^4X\\X(L)=X'(L)=0\\X(-L)=X'(-L)=0\\T''+Kw^4T=0$$ w>0 Questions I understand the change from $[0, L]\ to \ [-L,L],L=1/2$, but why use it in the solution? Why not to use the original interval? From comments, Leucippus mentioned that conditions for $T$ are missing, how do I find such conditions? I don't understand the '..up to a constant factor $C=1$,..' in (4.17) red square, why to take $C=1$ ? Finally in (4.19) and (4.20) we arrive to the solutions $X(t)$ in which I don't see what is the eigenvalue (In my book eigenvalue was given like $\beta=\frac{\pi x}{l}$ and it came from applying the conditions). I think it's $w$ but how do I get $w$ to have the form $\lambda_n=\frac{n\pi }{l}$? I also think the solution is not complete, we also need solution of $T''+Kw^4T=0$ am I right? Could someone help me to understand please? Now suppose eigenvalue $\lambda=0$, then $kw^4=0.$ $$\implies X^{iv}=0,\ T''=0$$ Applying condition $x=0$ in the solution gives $X(x)=A$ and $T(t)=B$, thus $\lambda=0 $ is an eigenvalue and the eigenfunction is any constant. Now suppose eigenvalue $\lambda<0$, then $kw^4<0.$ I won't continue here since I don't know If I'm doing everything right or wrong. Note: An alternative solution is very welcome especially if it's easier than mine :) Thank you.",,"['ordinary-differential-equations', 'proof-verification']"
44,"""Beats"" via trig identity or something?","""Beats"" via trig identity or something?",,"Planning on talking about resonance in DE. The solution to the IVP $$y''+y=\cos(t),\quad y(0)=y'(0)=0$$is $$y=\frac12 t\sin(t).$$Resonance, great. Now what if the forcing function has almost the resonant frequency? If $\alpha^2\ne1$ the solution to $$y''+y=\cos(\alpha t),\quad y(0)=y'(0)=0$$is $$y=\frac1{1-\alpha^2}(\cos(\alpha t)-\cos(t)).$$It's pleasant to note that this is $\sim\frac12 t\sin(t)$ as $t\to0$, and that for every $t$ it tends to $\frac12 t\sin(t)$ as $\alpha\to1$. For a fixed $\alpha$ close to $1$ it's clear if you think about it that $\cos(\alpha t)-\cos(t)$ exhibits ""beats"": there are intervals where it's close to $2\cos(\alpha t)$ and intervals where it's close to $0$. Alas when I say ""you"" here it's also clear that I mean you , not my DE students. Hence the question: Is there a clever way to  write $\cos(\alpha t)-\cos(t)$ so that the beats are evident? Like it's clear that $\cos(\epsilon t)\cos(t)$ shows beats - something analogous to that? Of course $$e^{i\alpha t}-e^{it}=e^{it}(e^{i(\alpha-1)t}-1),$$but I'd rather avoid complex numbers here and sadly the real part of a product is not the product of the real parts. I could use that, or just the sum formula for the cosine, to get a sum of two products, each of which is the sort of thing I want. But writing it as just the product of something with frequency $1$ and something with low frequency would be so much nicer. This must be well studied by people who study such things...","Planning on talking about resonance in DE. The solution to the IVP $$y''+y=\cos(t),\quad y(0)=y'(0)=0$$is $$y=\frac12 t\sin(t).$$Resonance, great. Now what if the forcing function has almost the resonant frequency? If $\alpha^2\ne1$ the solution to $$y''+y=\cos(\alpha t),\quad y(0)=y'(0)=0$$is $$y=\frac1{1-\alpha^2}(\cos(\alpha t)-\cos(t)).$$It's pleasant to note that this is $\sim\frac12 t\sin(t)$ as $t\to0$, and that for every $t$ it tends to $\frac12 t\sin(t)$ as $\alpha\to1$. For a fixed $\alpha$ close to $1$ it's clear if you think about it that $\cos(\alpha t)-\cos(t)$ exhibits ""beats"": there are intervals where it's close to $2\cos(\alpha t)$ and intervals where it's close to $0$. Alas when I say ""you"" here it's also clear that I mean you , not my DE students. Hence the question: Is there a clever way to  write $\cos(\alpha t)-\cos(t)$ so that the beats are evident? Like it's clear that $\cos(\epsilon t)\cos(t)$ shows beats - something analogous to that? Of course $$e^{i\alpha t}-e^{it}=e^{it}(e^{i(\alpha-1)t}-1),$$but I'd rather avoid complex numbers here and sadly the real part of a product is not the product of the real parts. I could use that, or just the sum formula for the cosine, to get a sum of two products, each of which is the sort of thing I want. But writing it as just the product of something with frequency $1$ and something with low frequency would be so much nicer. This must be well studied by people who study such things...",,"['ordinary-differential-equations', 'almost-periodic-functions']"
45,Using bounds for an analytically intractable differential equations problem,Using bounds for an analytically intractable differential equations problem,,"Exercise 2.8.6 from Strogatz's Nonlinear Dynamics and Chaos with Applications asks us to consider the initial value problem $\dot x = x+e^{-x}$, $x(0)=0$ $\left(\dot x\, \text{means}\, \frac{dx}{dt}\right)$, which cannot be solved analytically. Part (b) of the exercise states as follows: Using some analytical arguments, obtain rigorous bounds for the value of $x$ at $t=1$. In other words, prove that $a<x(1)<b$ for $a$, $b$ to be determined. By being clever, try to make $a$ and $b$ as close together as possible. I was given a hint from my professor that, in other words, we want to approximate $$\psi(x) \leq x+ e^{-x} \leq \phi(x) $$ for $\phi(x),\,\psi(x) \geq 0$, and that essentially what we want to do is solve the following three IVPs: $$(1) \qquad\frac{dx}{dt} = \psi(x) \\ \qquad x(t_{0})=x_{0}, $$ $$ (2) \qquad  \frac{dx}{dt} = x+e^{-x} \\ \qquad x(t_{0}) = x_{0},$$ and  $$ (3) \qquad \frac{dx}{dt} = \phi(x) \\ \qquad x(t_{0})=x_{0} .$$ Then, if we call the solution to $(1)$ $a(t)$, the solution to $(2)$ $x(t)$, and the solution to $(3)$ $b(t)$, the problem boils down to showing that $$a(1) \leq x(1) \leq b(1). $$ In that regard, I am very close to finding $b(1)$. To get there, I considered the Taylor expansion for $$e^{-x} = \sum_{n=0}^{\infty} (-1)^{n} \frac{x^{n}}{n!} =1-x + \frac{x^{2}}{2} - \frac{x^{3}}{3!}+ \cdots$$ so that $x+e^{-x} = x + \sum_{n=0}^{\infty} (-1)^{n}\frac{x^{n}}{n!}.$ As a bound, I decided to show that $x+e^{-x} \leq 1 + \frac{x^{2}}{2}$ $\forall x \geq 0.$ To that effect, I let $h(x) = x + e^{-x} - 1 - \frac{x^{2}}{2}$, and then I considered $h^{\prime}(x) = 1 - e^{-x} - x$ and $h^{\prime \prime}(x) = e^{-x} - 1$. Since $h^{\prime \prime}(x) > 0$ for $x < 0$, $h^{\prime}(x)$ is increasing on $(-\infty, 0)$. Since $h^{\prime \prime}(x) < 0$ for $x > 0$, $h^{\prime}(x)$ is decreasing on $(0, \infty)$. But, $h^{\prime}(x) = 0$ for $x = 0$, so $h^{\prime}(x) < 0$ for $x<0$ and $h^{\prime}(x) < 0 $ for $x > 0$ as well - the only time $h^{\prime}(x)$ is not negative is when $x=0$, and there it is simply $=0$. Therefore, $h$ is everywhere decreasing, and since $h(x)=0$, $h > 0$ for $x<0$ and $h < 0$ for $x > 0$, so we have that $h(x) = x + e^{-x} - 1 - \frac{x^{2}}{2} \leq 0$ for $x \geq 0$, which implies that $x + e^{-x} \leq 1 + \frac{x^{2}}{2} $, as desired. So, then I let $\phi(x) = 1 - \frac{x^{2}}{2}$, and I need to solve Equation $(3)$ above. Using separation of variables, I have that $\frac{dx}{dt} = 1 - \frac{x^{2}}{2}$ becomes $\displaystyle \int \frac{1}{\displaystyle 1 - \left(\frac{x}{\sqrt{2}} \right)^{2}} dx = \int dt$, which comes out to be  $$\ln \left(1 + \frac{x}{\sqrt{2}} \right) - \ln \left(1 - \frac{x}{\sqrt{2}} \right) = \sqrt{2}t + C, $$ And then, exponentiating both sides, we get $$\frac{1+\frac{x}{\sqrt{2}}}{1-\frac{x}{\sqrt{2}}}= Ce^{\sqrt{2}t} $$ From here, I don't know what to do . The problem tells us what the initial condition when $t=0$ is - it tells us that $x(0) = 0$, but it doesn't tell us what it is for $t=1$. And since we're trying to find bounds for $x(1)$, I'm assuming that for Equation (3) (actually in all three of the equations), we want $t_{0} = 1$, right? The only problem is that I can't figure out what $x_{0}$ is in that case, so how do I solve for $C$? Also, if you could help me out on the left bound (i.e., solving for $\psi(x)$), I would appreciate that very much! Thank you ahead of time for your help.","Exercise 2.8.6 from Strogatz's Nonlinear Dynamics and Chaos with Applications asks us to consider the initial value problem $\dot x = x+e^{-x}$, $x(0)=0$ $\left(\dot x\, \text{means}\, \frac{dx}{dt}\right)$, which cannot be solved analytically. Part (b) of the exercise states as follows: Using some analytical arguments, obtain rigorous bounds for the value of $x$ at $t=1$. In other words, prove that $a<x(1)<b$ for $a$, $b$ to be determined. By being clever, try to make $a$ and $b$ as close together as possible. I was given a hint from my professor that, in other words, we want to approximate $$\psi(x) \leq x+ e^{-x} \leq \phi(x) $$ for $\phi(x),\,\psi(x) \geq 0$, and that essentially what we want to do is solve the following three IVPs: $$(1) \qquad\frac{dx}{dt} = \psi(x) \\ \qquad x(t_{0})=x_{0}, $$ $$ (2) \qquad  \frac{dx}{dt} = x+e^{-x} \\ \qquad x(t_{0}) = x_{0},$$ and  $$ (3) \qquad \frac{dx}{dt} = \phi(x) \\ \qquad x(t_{0})=x_{0} .$$ Then, if we call the solution to $(1)$ $a(t)$, the solution to $(2)$ $x(t)$, and the solution to $(3)$ $b(t)$, the problem boils down to showing that $$a(1) \leq x(1) \leq b(1). $$ In that regard, I am very close to finding $b(1)$. To get there, I considered the Taylor expansion for $$e^{-x} = \sum_{n=0}^{\infty} (-1)^{n} \frac{x^{n}}{n!} =1-x + \frac{x^{2}}{2} - \frac{x^{3}}{3!}+ \cdots$$ so that $x+e^{-x} = x + \sum_{n=0}^{\infty} (-1)^{n}\frac{x^{n}}{n!}.$ As a bound, I decided to show that $x+e^{-x} \leq 1 + \frac{x^{2}}{2}$ $\forall x \geq 0.$ To that effect, I let $h(x) = x + e^{-x} - 1 - \frac{x^{2}}{2}$, and then I considered $h^{\prime}(x) = 1 - e^{-x} - x$ and $h^{\prime \prime}(x) = e^{-x} - 1$. Since $h^{\prime \prime}(x) > 0$ for $x < 0$, $h^{\prime}(x)$ is increasing on $(-\infty, 0)$. Since $h^{\prime \prime}(x) < 0$ for $x > 0$, $h^{\prime}(x)$ is decreasing on $(0, \infty)$. But, $h^{\prime}(x) = 0$ for $x = 0$, so $h^{\prime}(x) < 0$ for $x<0$ and $h^{\prime}(x) < 0 $ for $x > 0$ as well - the only time $h^{\prime}(x)$ is not negative is when $x=0$, and there it is simply $=0$. Therefore, $h$ is everywhere decreasing, and since $h(x)=0$, $h > 0$ for $x<0$ and $h < 0$ for $x > 0$, so we have that $h(x) = x + e^{-x} - 1 - \frac{x^{2}}{2} \leq 0$ for $x \geq 0$, which implies that $x + e^{-x} \leq 1 + \frac{x^{2}}{2} $, as desired. So, then I let $\phi(x) = 1 - \frac{x^{2}}{2}$, and I need to solve Equation $(3)$ above. Using separation of variables, I have that $\frac{dx}{dt} = 1 - \frac{x^{2}}{2}$ becomes $\displaystyle \int \frac{1}{\displaystyle 1 - \left(\frac{x}{\sqrt{2}} \right)^{2}} dx = \int dt$, which comes out to be  $$\ln \left(1 + \frac{x}{\sqrt{2}} \right) - \ln \left(1 - \frac{x}{\sqrt{2}} \right) = \sqrt{2}t + C, $$ And then, exponentiating both sides, we get $$\frac{1+\frac{x}{\sqrt{2}}}{1-\frac{x}{\sqrt{2}}}= Ce^{\sqrt{2}t} $$ From here, I don't know what to do . The problem tells us what the initial condition when $t=0$ is - it tells us that $x(0) = 0$, but it doesn't tell us what it is for $t=1$. And since we're trying to find bounds for $x(1)$, I'm assuming that for Equation (3) (actually in all three of the equations), we want $t_{0} = 1$, right? The only problem is that I can't figure out what $x_{0}$ is in that case, so how do I solve for $C$? Also, if you could help me out on the left bound (i.e., solving for $\psi(x)$), I would appreciate that very much! Thank you ahead of time for your help.",,['ordinary-differential-equations']
46,Why this field with non-zero curl has closed orbit?,Why this field with non-zero curl has closed orbit?,,"I am working on a dynamic system: $$\dot{r}=F(r,\phi)=v \cos{\phi}$$ $$\dot{\phi}=G(r,\phi)=\frac{f(r,\phi)}{v \cos{\phi}}-\frac{v \sin{\phi}}{r}$$ where $f(r,\phi)=0.5 r^5 (\pi/2-\phi)$ The out-of-plane component of the curl $F_{\phi}-G_r$ of this system is not generally zero. But the numerical integrated trajectory from this is closed. Sample trajectory picture: Initial condition: $r(0)=0.9, \phi(0)=\pi/2$ How is this happening? Is it because there are infinite number of singularity (though removable) at $\phi=\pi/2$. I'd really appreciate your help!","I am working on a dynamic system: $$\dot{r}=F(r,\phi)=v \cos{\phi}$$ $$\dot{\phi}=G(r,\phi)=\frac{f(r,\phi)}{v \cos{\phi}}-\frac{v \sin{\phi}}{r}$$ where $f(r,\phi)=0.5 r^5 (\pi/2-\phi)$ The out-of-plane component of the curl $F_{\phi}-G_r$ of this system is not generally zero. But the numerical integrated trajectory from this is closed. Sample trajectory picture: Initial condition: $r(0)=0.9, \phi(0)=\pi/2$ How is this happening? Is it because there are infinite number of singularity (though removable) at $\phi=\pi/2$. I'd really appreciate your help!",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
47,A first order non-homogeneous initial value problem,A first order non-homogeneous initial value problem,,"Does $\frac{\text{d}y}{\text{d}x}=-y+\sin(x^2)$ has a unique and well defined solution in $y\in[0,\infty)$ for any $y(0)\in\mathbb{R}$? For a unique solution to exists, do we need $\sin(x^2)$ to be locally Lebesgue integrable on $[0,\infty)$?","Does $\frac{\text{d}y}{\text{d}x}=-y+\sin(x^2)$ has a unique and well defined solution in $y\in[0,\infty)$ for any $y(0)\in\mathbb{R}$? For a unique solution to exists, do we need $\sin(x^2)$ to be locally Lebesgue integrable on $[0,\infty)$?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'initial-value-problems']"
48,Integral of Vector Equations of Motion,Integral of Vector Equations of Motion,,"I have a problem in my textbook, it states ""At time t=o, vectors $E=E_o$ and $B=B_o$, where the unit vectors $E_o$ and $B_o$ are fixed and orthogonal. The equations of motion are $\frac{dE}{dt}=E_o+B\times E_o$ $\frac{dB}{dt}=B_o+E\times B_o$ Find E and B at a general time t, showing that after a long time the directions of E and B have almost interchanged."" At first instinct I want to integrate E and B w.r.t. t. However, I'm not sure how to do this if E depends on B and B depends on E, while they both depend on t. I just need help starting the question, once I have direction I'm sure I can finish as well answer the second part.","I have a problem in my textbook, it states ""At time t=o, vectors $E=E_o$ and $B=B_o$, where the unit vectors $E_o$ and $B_o$ are fixed and orthogonal. The equations of motion are $\frac{dE}{dt}=E_o+B\times E_o$ $\frac{dB}{dt}=B_o+E\times B_o$ Find E and B at a general time t, showing that after a long time the directions of E and B have almost interchanged."" At first instinct I want to integrate E and B w.r.t. t. However, I'm not sure how to do this if E depends on B and B depends on E, while they both depend on t. I just need help starting the question, once I have direction I'm sure I can finish as well answer the second part.",,['ordinary-differential-equations']
49,Asymptotic behavior of solutions to laguerre's differential equation,Asymptotic behavior of solutions to laguerre's differential equation,,"The Frobenius solution to the homogenous DE (Laguerre's): $\\$ $$xg'' + (2n+2 -x)g' + (\lambda - n -1)g = 0$$ is given by $$g(x) = \sum_{k = 0}^\infty a_k \,x^{n+k}$$ where the coefficients are generated by the recurrence: $$a_0 = 1, \qquad \frac{a_{k+1}}{a_k} = \frac{n+k+1 - \lambda}{2(n+k+1)(k+1)}$$ $\\$ The series truncates for certain integer values of $\lambda$. Multiple texts mention that for non-integer values of $\lambda$ the solution grows on the order of $e^x$ at infinity. Unfortunately, this statement is always prefaced by ""it can be shown that .."". I can't seem to find a source that demonstrates this. I was hoping someone could point me in the right direction.","The Frobenius solution to the homogenous DE (Laguerre's): $\\$ $$xg'' + (2n+2 -x)g' + (\lambda - n -1)g = 0$$ is given by $$g(x) = \sum_{k = 0}^\infty a_k \,x^{n+k}$$ where the coefficients are generated by the recurrence: $$a_0 = 1, \qquad \frac{a_{k+1}}{a_k} = \frac{n+k+1 - \lambda}{2(n+k+1)(k+1)}$$ $\\$ The series truncates for certain integer values of $\lambda$. Multiple texts mention that for non-integer values of $\lambda$ the solution grows on the order of $e^x$ at infinity. Unfortunately, this statement is always prefaced by ""it can be shown that .."". I can't seem to find a source that demonstrates this. I was hoping someone could point me in the right direction.",,"['ordinary-differential-equations', 'asymptotics', 'analyticity']"
50,Problem on the domain of the solution of a differential equation,Problem on the domain of the solution of a differential equation,,"Let $f:[0,\alpha]\to\mathbb{R}$ be a solution of the Cauchy problem: $\begin{cases} f'(t)=(f(t))^2+t \\ f(0)=0 \end{cases} $ The question is: prove that $\alpha<3$. It is clear that the problem admits a unique solution locally on some $[-\epsilon,\epsilon]$, since $(f)^2+t$ is $C^1$ in $(f,t)$, and we have $f(t)>0$ for all $t>0$, looking at the sign of $f'$. But now I don't know how to proceed. Thank you all!","Let $f:[0,\alpha]\to\mathbb{R}$ be a solution of the Cauchy problem: $\begin{cases} f'(t)=(f(t))^2+t \\ f(0)=0 \end{cases} $ The question is: prove that $\alpha<3$. It is clear that the problem admits a unique solution locally on some $[-\epsilon,\epsilon]$, since $(f)^2+t$ is $C^1$ in $(f,t)$, and we have $f(t)>0$ for all $t>0$, looking at the sign of $f'$. But now I don't know how to proceed. Thank you all!",,"['analysis', 'ordinary-differential-equations', 'cauchy-problem']"
51,differential equation with reciprocal and a linear function in the derivative,differential equation with reciprocal and a linear function in the derivative,,"My question is how to solve $$ \frac{dx(at+b)}{dt} =\frac{c}{x(t)} $$ I'm aware that when $a=1, b = 0$, the solution to this problem is $x(t)=\sqrt{2(ct+k)}$ But in general, does this differential equation have an solution when $a \neq 1$ and $b \neq 0$?","My question is how to solve $$ \frac{dx(at+b)}{dt} =\frac{c}{x(t)} $$ I'm aware that when $a=1, b = 0$, the solution to this problem is $x(t)=\sqrt{2(ct+k)}$ But in general, does this differential equation have an solution when $a \neq 1$ and $b \neq 0$?",,['ordinary-differential-equations']
52,Diffeq question: method of undetermined coefficients,Diffeq question: method of undetermined coefficients,,"I have a question here from Differential Equations. We are learning how to solve second order nonhomogenous equations using the method of undetermined coefficients. The equation at hand is $$y''+4y=3\sin(2t)$$  I understand that the solution to the corresponding homogenous equation is $$y_c(t)=c_1\cos(2t)+c_2\sin(2t)$$ And I understand that to solve the equation using this method, I have to find an equation with ""undetermined coefficients"" that I then add to $y_c(t)$. But what form does this part of the equation take? p.s. The answer in the back of the book is $y=2\cos2t-(1/8)\sin2t-(3/4)t\cos2t$. The initial conditions were $y(0)=2, y'(0)=-1$, but I don't need help with initial conditions.","I have a question here from Differential Equations. We are learning how to solve second order nonhomogenous equations using the method of undetermined coefficients. The equation at hand is $$y''+4y=3\sin(2t)$$  I understand that the solution to the corresponding homogenous equation is $$y_c(t)=c_1\cos(2t)+c_2\sin(2t)$$ And I understand that to solve the equation using this method, I have to find an equation with ""undetermined coefficients"" that I then add to $y_c(t)$. But what form does this part of the equation take? p.s. The answer in the back of the book is $y=2\cos2t-(1/8)\sin2t-(3/4)t\cos2t$. The initial conditions were $y(0)=2, y'(0)=-1$, but I don't need help with initial conditions.",,['ordinary-differential-equations']
53,Optimization problem: The curve with the minimum time to get through a pile of quicksand - Calculus of Variations,Optimization problem: The curve with the minimum time to get through a pile of quicksand - Calculus of Variations,,"Suppose we have a function for the velocity given by $v(r,\theta)=r$ , or in Cartesian form $v(x,y)=\sqrt{x^2+y^2}$ . As we can see below, as we get closer to the origin $(0,0)$ , the velocity decreases. I've found it easy to visualize the field as being some form of quicksand where it is harder to move through as you approach the origin. This is demonstrated below by a plot I've made using Wolfram Mathematica: What I am trying to do: Find the two functions for $y(x)$ which would minimize the time taken to get from point $A(-1,0)$ and $B(1,0)$ . I deduced that the fastest path cannot be the straight line directly from $A$ to $B$ , since it would require an infinite time to get through the origin. A guess for the two curves is shown by the $\color{#0050B0}{\text{dark blue}}$ and the $\color{#00AAAA}{\text{light blue}}$ curves I've made. I'm almost certain they would be symmetrical. I've first guessed that the optimized curve would be similar to an ellipse, however I hesitated after I've plotted this. I've done some research on the problem and figured it may be similar to the derivation of the Brachistochrone curve , using the Euler-Lagrange equations. I am new to the Calculus of Variations, so here is the working I've done so far. We have: $$dt=\frac{ds}{v} \Rightarrow dt=\frac{\sqrt{dx^2+dy^2}}{\sqrt{x^2+y^2}} \Rightarrow dt=\frac{\sqrt{r^2+\left(\frac{dr}{d\theta}\right)^2}}{r}~d\theta$$ On the third step I converted it to polar coordinates. Adding integration signs: $$\int_{0}^{T}~dt=\int_{\theta_1}^{\theta_2}\frac{\sqrt{r^2+\left(\frac{dr}{d\theta}\right)^2}}{r}~d\theta$$ $$T=\int_{\theta_1}^{\theta_2} \sqrt{1+\frac{(r')^2}{r^2}}~d\theta$$ Where $T$ is the total time taken to get from $A$ to $B$ . I thought of using the following Euler-Lagrange Equation: $$\frac{d}{d\theta}\left(\frac{\partial L}{\partial r'}\right)=\frac{\partial L}{\partial r} \tag{1}$$ For the functional: $$L(\theta,r,r')=\sqrt{1+\frac{(r')^2}{r^2}}$$ Evaluating partial derivatives: $$\frac{dL}{dr}=-\frac{(r')^2}{r^3\sqrt{\frac{(r')^2}{r^2}+1}}=-\frac{(r')^2}{r^2\sqrt{(r')^2+r^2}}$$ $$\frac{dL}{dr'}=\frac{r'}{r^2\sqrt{\frac{(r')^2}{r^2}+1}}=\frac{r'}{r\sqrt{(r')^2+r^2}}$$ Substituting into $(1)$ : $$\frac{d}{d\theta}\left(\frac{r'}{r\sqrt{(r')^2+r^2}}\right)=-\frac{(r')^2}{r^2\sqrt{(r')^2+r^2}}$$ I integrated both sides with respect to $\theta$ and obtained: $$\frac{r'}{r\sqrt{(r')^2+r^2}}=-\frac{(r')^2\theta}{r^2\sqrt{(r')^2+r^2}}+C \tag{2}$$ Now, I realize I must solve this differential equation. I've tried simplifying it to obtain: $$r\frac{dr}{d\theta}=-\left(\frac{dr}{d\theta}\right)^2\theta+Cr^2\sqrt{\left(\frac{dr}{d\theta}\right)^2+r^2} \tag{3}$$ However, I think I've hit a dead end. I'm not certain that it is solvable in terms of elementary functions. Both Mathematica and Wolfram|Alpha have not given me a solution to this differential equation. To conclude, I would like some guidance on how to continue solving the differential equation, assuming I have done the calculation and methodology correctly so far. If I have not done the correct methodology, I would appreciate some guidance on how to proceed with the problem.","Suppose we have a function for the velocity given by , or in Cartesian form . As we can see below, as we get closer to the origin , the velocity decreases. I've found it easy to visualize the field as being some form of quicksand where it is harder to move through as you approach the origin. This is demonstrated below by a plot I've made using Wolfram Mathematica: What I am trying to do: Find the two functions for which would minimize the time taken to get from point and . I deduced that the fastest path cannot be the straight line directly from to , since it would require an infinite time to get through the origin. A guess for the two curves is shown by the and the curves I've made. I'm almost certain they would be symmetrical. I've first guessed that the optimized curve would be similar to an ellipse, however I hesitated after I've plotted this. I've done some research on the problem and figured it may be similar to the derivation of the Brachistochrone curve , using the Euler-Lagrange equations. I am new to the Calculus of Variations, so here is the working I've done so far. We have: On the third step I converted it to polar coordinates. Adding integration signs: Where is the total time taken to get from to . I thought of using the following Euler-Lagrange Equation: For the functional: Evaluating partial derivatives: Substituting into : I integrated both sides with respect to and obtained: Now, I realize I must solve this differential equation. I've tried simplifying it to obtain: However, I think I've hit a dead end. I'm not certain that it is solvable in terms of elementary functions. Both Mathematica and Wolfram|Alpha have not given me a solution to this differential equation. To conclude, I would like some guidance on how to continue solving the differential equation, assuming I have done the calculation and methodology correctly so far. If I have not done the correct methodology, I would appreciate some guidance on how to proceed with the problem.","v(r,\theta)=r v(x,y)=\sqrt{x^2+y^2} (0,0) y(x) A(-1,0) B(1,0) A B \color{#0050B0}{\text{dark blue}} \color{#00AAAA}{\text{light blue}} dt=\frac{ds}{v} \Rightarrow dt=\frac{\sqrt{dx^2+dy^2}}{\sqrt{x^2+y^2}} \Rightarrow dt=\frac{\sqrt{r^2+\left(\frac{dr}{d\theta}\right)^2}}{r}~d\theta \int_{0}^{T}~dt=\int_{\theta_1}^{\theta_2}\frac{\sqrt{r^2+\left(\frac{dr}{d\theta}\right)^2}}{r}~d\theta T=\int_{\theta_1}^{\theta_2} \sqrt{1+\frac{(r')^2}{r^2}}~d\theta T A B \frac{d}{d\theta}\left(\frac{\partial L}{\partial r'}\right)=\frac{\partial L}{\partial r} \tag{1} L(\theta,r,r')=\sqrt{1+\frac{(r')^2}{r^2}} \frac{dL}{dr}=-\frac{(r')^2}{r^3\sqrt{\frac{(r')^2}{r^2}+1}}=-\frac{(r')^2}{r^2\sqrt{(r')^2+r^2}} \frac{dL}{dr'}=\frac{r'}{r^2\sqrt{\frac{(r')^2}{r^2}+1}}=\frac{r'}{r\sqrt{(r')^2+r^2}} (1) \frac{d}{d\theta}\left(\frac{r'}{r\sqrt{(r')^2+r^2}}\right)=-\frac{(r')^2}{r^2\sqrt{(r')^2+r^2}} \theta \frac{r'}{r\sqrt{(r')^2+r^2}}=-\frac{(r')^2\theta}{r^2\sqrt{(r')^2+r^2}}+C \tag{2} r\frac{dr}{d\theta}=-\left(\frac{dr}{d\theta}\right)^2\theta+Cr^2\sqrt{\left(\frac{dr}{d\theta}\right)^2+r^2} \tag{3}","['ordinary-differential-equations', 'multivariable-calculus', 'optimization', 'calculus-of-variations']"
54,Differential equation combined with functional equation which looks simple,Differential equation combined with functional equation which looks simple,,"The equation is $$f'(x)=cf(x/2).$$ The problem emerges when I'm trying to deal with a partial differntial equation $$u_{t}(x,t)=u_{xx}(x,t/2).$$ Either using separation of variables: $$u(x,t)=X(x)T(t)\rightarrow X(x)T'(t)=X''(x)T(t/2)$$ so that $$X''(x)-cX(x)=0, T'(t)-cT(t/2)=0$$ or doing Fourier transform: $$\hat{u}_t(s,t)+s^2\hat{u}(s,t/2)=0$$ eventually reduces the problem to solving $$f'(x)=cf(x/2).$$ But the equation does not seem easy to solve. I can only prove the existence of solutions using the Euler method. The numerical simulation looks like The other function plotted for comparison is the exponential function. Does a solution of closed form exist to the equation? If not, how do we solve the original PDE?","The equation is The problem emerges when I'm trying to deal with a partial differntial equation Either using separation of variables: so that or doing Fourier transform: eventually reduces the problem to solving But the equation does not seem easy to solve. I can only prove the existence of solutions using the Euler method. The numerical simulation looks like The other function plotted for comparison is the exponential function. Does a solution of closed form exist to the equation? If not, how do we solve the original PDE?","f'(x)=cf(x/2). u_{t}(x,t)=u_{xx}(x,t/2). u(x,t)=X(x)T(t)\rightarrow X(x)T'(t)=X''(x)T(t/2) X''(x)-cX(x)=0, T'(t)-cT(t/2)=0 \hat{u}_t(s,t)+s^2\hat{u}(s,t/2)=0 f'(x)=cf(x/2).","['ordinary-differential-equations', 'functional-equations']"
55,Flow on compact manifold,Flow on compact manifold,,"These questions seem simple, and but I have not found the answer on the web (I have no mathematician in my neighborhood). Does a continuous injective function from $E$ to $E$ have to be surjective, where $E$ is either the $n$-sphere, the $n$-torus, or more generally a compact manifold without boundary? (I have found simple counterexamples in case the manifold is not compact, or has a boundary) The case $E=S^1$ is solved in ""An injective continuous map on the unit sphere is a homeomorphism"", but I cannot generalize the argument. The initial motivation of my question comes from a flow defined on a sphere (or torus), which is injective. And I need it to be surjective as well for it to define a new coordinate system on the sphere. Thanks in advance!","These questions seem simple, and but I have not found the answer on the web (I have no mathematician in my neighborhood). Does a continuous injective function from $E$ to $E$ have to be surjective, where $E$ is either the $n$-sphere, the $n$-torus, or more generally a compact manifold without boundary? (I have found simple counterexamples in case the manifold is not compact, or has a boundary) The case $E=S^1$ is solved in ""An injective continuous map on the unit sphere is a homeomorphism"", but I cannot generalize the argument. The initial motivation of my question comes from a flow defined on a sphere (or torus), which is injective. And I need it to be surjective as well for it to define a new coordinate system on the sphere. Thanks in advance!",,"['ordinary-differential-equations', 'manifolds']"
56,Find general solution for the differential equation $x^3y^{'''}+x^2y^{''}+3xy^{'}-8y=0$,Find general solution for the differential equation,x^3y^{'''}+x^2y^{''}+3xy^{'}-8y=0,"Find general solution for the differential equation $x^3y^{'''}+x^2y^{''}+3xy^{'}-8y=0$ This is the Euler differential equation which can be solved by substitution $x=e^t$. I don't understand the following differential relations: $$xy^{'}=x\frac{dy}{dx}=\frac{dy}{dt} $$ $$x^2y^{''}=x^2\frac{d^2y}{dx^2}=\frac{d}{dt}\left(\frac{d}{dt}-1\right)y $$ $$x^3y^{'''}=x^3\frac{d^3y}{dx^3}=\frac{d}{dt}\left(\frac{d}{dt}-1\right)\left(\frac{d}{dt}-2\right)y $$ How to evaluate these relations? From here, it is easy to solve the equation, which is homogeneous with constant coefficients. General solution is $y=c_1e^{2\ln x}+c_2e^{-i2\ln x}+c_3e^{i2\ln x}$","Find general solution for the differential equation $x^3y^{'''}+x^2y^{''}+3xy^{'}-8y=0$ This is the Euler differential equation which can be solved by substitution $x=e^t$. I don't understand the following differential relations: $$xy^{'}=x\frac{dy}{dx}=\frac{dy}{dt} $$ $$x^2y^{''}=x^2\frac{d^2y}{dx^2}=\frac{d}{dt}\left(\frac{d}{dt}-1\right)y $$ $$x^3y^{'''}=x^3\frac{d^3y}{dx^3}=\frac{d}{dt}\left(\frac{d}{dt}-1\right)\left(\frac{d}{dt}-2\right)y $$ How to evaluate these relations? From here, it is easy to solve the equation, which is homogeneous with constant coefficients. General solution is $y=c_1e^{2\ln x}+c_2e^{-i2\ln x}+c_3e^{i2\ln x}$",,['ordinary-differential-equations']
57,Find complete integral of $(y-x)(qy-px) = (p-q)^{2}$,Find complete integral of,(y-x)(qy-px) = (p-q)^{2},"Find complete integral for partial differential equation $(y-x)(qy-px) = (p-q)^{2}$ where  $p={ \partial  z \over \partial x},q={ \partial  z \over \partial y}$. My attempt: The given equation is  f(x,y,z,p,q) = $(y-x)(qy-px) - (p-q)^{2}$ The Charpit's auxilary equation will be given by $${dp \over 2px-(p+q)y}={dq \over 2qy-(p+q)x}={dx \over {2(p-q)-x2+xy}}={dy \over {-2(p-q)+xy-y2}}$$  From here I tried to proceed but no solvable fraction turned out.","Find complete integral for partial differential equation $(y-x)(qy-px) = (p-q)^{2}$ where  $p={ \partial  z \over \partial x},q={ \partial  z \over \partial y}$. My attempt: The given equation is  f(x,y,z,p,q) = $(y-x)(qy-px) - (p-q)^{2}$ The Charpit's auxilary equation will be given by $${dp \over 2px-(p+q)y}={dq \over 2qy-(p+q)x}={dx \over {2(p-q)-x2+xy}}={dy \over {-2(p-q)+xy-y2}}$$  From here I tried to proceed but no solvable fraction turned out.",,"['ordinary-differential-equations', 'partial-differential-equations', 'characteristics']"
58,ODEs are invariant under the given Lie groups?,ODEs are invariant under the given Lie groups?,,"$\frac{dy}{dx} = \frac{x^{2}y}{x^{3}+xy+y^2}$ is invariant under $(x,y) \mapsto (\frac{x}{1+\varepsilon y},\frac{y}{1+\varepsilon y})$ I can't make both sides equal when I have a variable depends on two variables I use $\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{D}y}{\mathrm{D}x}$ ($D=$ total derivative), but I can't make both equal!! How to do that? I want to mention that this example is arbitrary I can not make any invariant when any variable equal any combination of $x$ and $y$.","$\frac{dy}{dx} = \frac{x^{2}y}{x^{3}+xy+y^2}$ is invariant under $(x,y) \mapsto (\frac{x}{1+\varepsilon y},\frac{y}{1+\varepsilon y})$ I can't make both sides equal when I have a variable depends on two variables I use $\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{D}y}{\mathrm{D}x}$ ($D=$ total derivative), but I can't make both equal!! How to do that? I want to mention that this example is arbitrary I can not make any invariant when any variable equal any combination of $x$ and $y$.",,"['ordinary-differential-equations', 'lie-groups', 'symmetric-groups', 'symmetry']"
59,"find an approximate solution, up to the order of epsilon","find an approximate solution, up to the order of epsilon",,"The question is to find an approximate solution, up to the order of epsilon of following problem. $$y'' + y+\epsilon y^3 = 0$$ $$y(0) = a$$ $$y'(0) = 0$$ I tried to solve the given problem using perturbation theory. $$y(t) = y_0(t) + \epsilon y_1(t) + \epsilon^2 y_2(t) + \cdots$$ $$t = w(\epsilon)t = (1 + \epsilon w_1 + \epsilon^2 w_2 + \cdots )t$$ However, i failed to find an appropriate approximate solution... help me!!","The question is to find an approximate solution, up to the order of epsilon of following problem. $$y'' + y+\epsilon y^3 = 0$$ $$y(0) = a$$ $$y'(0) = 0$$ I tried to solve the given problem using perturbation theory. $$y(t) = y_0(t) + \epsilon y_1(t) + \epsilon^2 y_2(t) + \cdots$$ $$t = w(\epsilon)t = (1 + \epsilon w_1 + \epsilon^2 w_2 + \cdots )t$$ However, i failed to find an appropriate approximate solution... help me!!",,"['ordinary-differential-equations', 'approximation', 'perturbation-theory']"
60,Transformation of second order ODE,Transformation of second order ODE,,"I am a beginner for a ODE theory. There is a ODE: $$ x^{2\beta+2} \frac{d^2 V}{dx^2}+x\frac{dV}{dx}+bV=0. $$ In the paper, the authors introduce some transformation $$ V(S)=S^{1/2+\beta}e^{A(x)}w(x) $$ where $w(x)$ satisfies the Whittaker's equation. My question is how to find such a transformation.  Is there a rule to convert from second-order ODE to Whittaker's equation?? Thanks in advance.","I am a beginner for a ODE theory. There is a ODE: $$ x^{2\beta+2} \frac{d^2 V}{dx^2}+x\frac{dV}{dx}+bV=0. $$ In the paper, the authors introduce some transformation $$ V(S)=S^{1/2+\beta}e^{A(x)}w(x) $$ where $w(x)$ satisfies the Whittaker's equation. My question is how to find such a transformation.  Is there a rule to convert from second-order ODE to Whittaker's equation?? Thanks in advance.",,"['ordinary-differential-equations', 'partial-differential-equations', 'special-functions', 'transformation', 'hypergeometric-function']"
61,How to solve this DE?,How to solve this DE?,,"Consider the ordinary differential equation $$y''=xyy'$$ I'm pretty stumped, so any tips on how to proceed? It seems fairly simple but I'm drawing a blank.","Consider the ordinary differential equation $$y''=xyy'$$ I'm pretty stumped, so any tips on how to proceed? It seems fairly simple but I'm drawing a blank.",,['ordinary-differential-equations']
62,stability of equilibria for $n$-dimensional nonlinear systems of differential equations: examples,stability of equilibria for -dimensional nonlinear systems of differential equations: examples,n,"I'm currently self-studying dynamical systems. I'm trying to summarize what can be said about the stability of equilibrium points for an $n$-dimensional non-linear system of differential equations: $\frac{d}{dt}\bf{x} = \bf{f}(\bf{x})$ Let's say I already have found the equilibria $\bf{x}^{*}$ (for which $\bf{f}(\bf{x}^{*}) = \bf{0}$). Can someone point me to a reference which will help me classify the stability types of these equilibria (whether they are saddle points, centres, sources, sinks, etc.). Specifically, I want to know what can be said by using linearizations (ie. with Jacobians) - I realize that what can be said using just this method is probably quite limited. I know that the Hartman-Grobman theorem is helpful. The way I understand it, this theorem says that if the Jacobian of the system evaluated at $\bf{x}^{*}$ has eigenvalues $\{ \lambda_{j} \}_{j=1}^{n}$ such that $\text{Re}(\lambda_{j})\neq0$ for all $j$, then: if all eigenvalues are negative then it is asymptotically stable (sink) if all eigenvalues are positive then it is unstable (source) if there is a mix of positive and negative eigenvalues, then its a saddle point Is this correct? Can you say anything else if $\text{Re}(\lambda_{j})=0$ for one or more $j$? Every reference I come across deals with $two$-dimensional systems pretty heavily, and then omits a detailed discussion on higher-dimensional systems. What I am really interested in is a document which could list me some examples dealing with the stability of systems that have dimension $higher$ than two. Can someone help me out?","I'm currently self-studying dynamical systems. I'm trying to summarize what can be said about the stability of equilibrium points for an $n$-dimensional non-linear system of differential equations: $\frac{d}{dt}\bf{x} = \bf{f}(\bf{x})$ Let's say I already have found the equilibria $\bf{x}^{*}$ (for which $\bf{f}(\bf{x}^{*}) = \bf{0}$). Can someone point me to a reference which will help me classify the stability types of these equilibria (whether they are saddle points, centres, sources, sinks, etc.). Specifically, I want to know what can be said by using linearizations (ie. with Jacobians) - I realize that what can be said using just this method is probably quite limited. I know that the Hartman-Grobman theorem is helpful. The way I understand it, this theorem says that if the Jacobian of the system evaluated at $\bf{x}^{*}$ has eigenvalues $\{ \lambda_{j} \}_{j=1}^{n}$ such that $\text{Re}(\lambda_{j})\neq0$ for all $j$, then: if all eigenvalues are negative then it is asymptotically stable (sink) if all eigenvalues are positive then it is unstable (source) if there is a mix of positive and negative eigenvalues, then its a saddle point Is this correct? Can you say anything else if $\text{Re}(\lambda_{j})=0$ for one or more $j$? Every reference I come across deals with $two$-dimensional systems pretty heavily, and then omits a detailed discussion on higher-dimensional systems. What I am really interested in is a document which could list me some examples dealing with the stability of systems that have dimension $higher$ than two. Can someone help me out?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
63,Help solving a first order non-linear differential equation derived from the navier-stokes equation,Help solving a first order non-linear differential equation derived from the navier-stokes equation,,"I am an engineer studying an unsteady-state flow through a pipe.  The transient Bernoulli equation of this system, which I picked up from here ( http://higheredbcs.wiley.com/legacy/college/fox/0471742996/webpdf/ch06.pdf )  yields this differential equation: $$\frac{dv(t)}{dt}+av^2(t)+bP(t)=c$$ This is a first order non-linear differential equation.  Unfortunately, I have no experience solving non-linear differential equations.  From the research I have done, this type of equation looks similar to the Ricatti equation.  Is there a closed form solution to the above equation?  How can I solve this equation?  I'm interested in getting a function that shows how the pressure or velocity of the system decays with time. FYI:  v(t) is velocity,  P(t) is pressure,  (a,b,c) are constants.  Thank You all.","I am an engineer studying an unsteady-state flow through a pipe.  The transient Bernoulli equation of this system, which I picked up from here ( http://higheredbcs.wiley.com/legacy/college/fox/0471742996/webpdf/ch06.pdf )  yields this differential equation: $$\frac{dv(t)}{dt}+av^2(t)+bP(t)=c$$ This is a first order non-linear differential equation.  Unfortunately, I have no experience solving non-linear differential equations.  From the research I have done, this type of equation looks similar to the Ricatti equation.  Is there a closed form solution to the above equation?  How can I solve this equation?  I'm interested in getting a function that shows how the pressure or velocity of the system decays with time. FYI:  v(t) is velocity,  P(t) is pressure,  (a,b,c) are constants.  Thank You all.",,"['ordinary-differential-equations', 'fluid-dynamics']"
64,Why does $\frac{1}{r}\frac{dr}{d\theta} = \cot \psi$?,Why does ?,\frac{1}{r}\frac{dr}{d\theta} = \cot \psi,"In the discussion of linear fractional equations in Birkhoff and Rota's Ordinary Differential Equations , the authors assert that if we convert a DE of the form $y' = F\left(\frac{y}{x}\right)$ to polar coordinates, then we have \begin{align} \frac{1}{r}\frac{dr}{d\theta} = \cot \psi, \end{align} where $\psi = \gamma - \theta$, with $\gamma$ being the tangent direction and $\theta$ the radial direction.  I'm afraid this has me entirely buffaloed -- why on earth is this true?  I have neither an analytic nor geometric intuition as to how this could possibly be.  I'm sure there's some elementaryish fact about $\frac{dr}{d\theta}$ which makes the answer obvious, but I have no clue what said fact is. I do note that  \begin{align} \frac{dy}{d\theta} &= r\cos\theta + \frac{dr}{d\theta}\sin\theta\Rightarrow\\ \frac{dr}{d\theta} & = \frac{dy}{d\theta}\csc\theta - r\cot\theta\Rightarrow\\ \frac{1}{r}\frac{dr}{d\theta} & = \frac{1}{y}\frac{dy}{d\theta} - \cot\theta,  \end{align} but this is as close as I can come to getting a cotangent anywhere near the expression (and, of course, $\theta \neq \psi$ in general). What gives?","In the discussion of linear fractional equations in Birkhoff and Rota's Ordinary Differential Equations , the authors assert that if we convert a DE of the form $y' = F\left(\frac{y}{x}\right)$ to polar coordinates, then we have \begin{align} \frac{1}{r}\frac{dr}{d\theta} = \cot \psi, \end{align} where $\psi = \gamma - \theta$, with $\gamma$ being the tangent direction and $\theta$ the radial direction.  I'm afraid this has me entirely buffaloed -- why on earth is this true?  I have neither an analytic nor geometric intuition as to how this could possibly be.  I'm sure there's some elementaryish fact about $\frac{dr}{d\theta}$ which makes the answer obvious, but I have no clue what said fact is. I do note that  \begin{align} \frac{dy}{d\theta} &= r\cos\theta + \frac{dr}{d\theta}\sin\theta\Rightarrow\\ \frac{dr}{d\theta} & = \frac{dy}{d\theta}\csc\theta - r\cot\theta\Rightarrow\\ \frac{1}{r}\frac{dr}{d\theta} & = \frac{1}{y}\frac{dy}{d\theta} - \cot\theta,  \end{align} but this is as close as I can come to getting a cotangent anywhere near the expression (and, of course, $\theta \neq \psi$ in general). What gives?",,"['calculus', 'ordinary-differential-equations']"
65,Why does my particular solution to $y''-y'-2y=2e^{-t}$ via variation of parameters not match that by undetermined coefficients?,Why does my particular solution to  via variation of parameters not match that by undetermined coefficients?,y''-y'-2y=2e^{-t},"The particular solution to $y''-y'-2y=2e^{-t}$ found by the method of undetermined coefficients is $Y(t)=-\frac{2}{3}te^{-t}$ However, when I find the particular solution via the method of variation of parameters I obtain a different expression.  I would really appreciate someone to point out where my calculations go awry. First I find the fundamental set of solutions for the analogous homogeneous equation $y''-y'-2y=0$ which are $y_1(t)=e^{-t}$ and $y_2(t)=e^{2t}$.  Then I use these to find the Wronskian $W(y_1,y_2)(t)=3e^t$.  Then I plug these values into the following equation to get the particular solution: $$Y(t)=-y_1(t)\int \frac{y_2(t)g(t)}{W(y_1,y_2)(t)}dt+y_2(t)\int\frac{y_1(t)g(t)}{W(y_1,y_2)(t)}dt$$ Where $g(t)=2e^{-t}$ is the non-homogeneous term. $$Y(t)=-e^{-t}\int\frac{e^{2t}2e^{-t}}{3e^t}dt+e^{2t}\int\frac{e^{-t}2e^{-t}}{3e^t}dt$$ $$=-e^{-t}\int\frac{2e^t}{3e^t}dt+e^{2t}\int\frac{2e^{-2t}}{3e^t}dt$$ $$=-e^{-t}\int\frac23dt+e^{2t}\int\frac23e^{-3t}dt$$ Finally giving: $$Y(t)=-\frac23te^{-t}-\frac29e^{-t}$$ So why doesn't this particular solution match the one obtained from undetermined coefficients?","The particular solution to $y''-y'-2y=2e^{-t}$ found by the method of undetermined coefficients is $Y(t)=-\frac{2}{3}te^{-t}$ However, when I find the particular solution via the method of variation of parameters I obtain a different expression.  I would really appreciate someone to point out where my calculations go awry. First I find the fundamental set of solutions for the analogous homogeneous equation $y''-y'-2y=0$ which are $y_1(t)=e^{-t}$ and $y_2(t)=e^{2t}$.  Then I use these to find the Wronskian $W(y_1,y_2)(t)=3e^t$.  Then I plug these values into the following equation to get the particular solution: $$Y(t)=-y_1(t)\int \frac{y_2(t)g(t)}{W(y_1,y_2)(t)}dt+y_2(t)\int\frac{y_1(t)g(t)}{W(y_1,y_2)(t)}dt$$ Where $g(t)=2e^{-t}$ is the non-homogeneous term. $$Y(t)=-e^{-t}\int\frac{e^{2t}2e^{-t}}{3e^t}dt+e^{2t}\int\frac{e^{-t}2e^{-t}}{3e^t}dt$$ $$=-e^{-t}\int\frac{2e^t}{3e^t}dt+e^{2t}\int\frac{2e^{-2t}}{3e^t}dt$$ $$=-e^{-t}\int\frac23dt+e^{2t}\int\frac23e^{-3t}dt$$ Finally giving: $$Y(t)=-\frac23te^{-t}-\frac29e^{-t}$$ So why doesn't this particular solution match the one obtained from undetermined coefficients?",,['ordinary-differential-equations']
66,Find two linearly independent solutions of the differential equation $(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3}$,Find two linearly independent solutions of the differential equation,(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3},"I want to find two linearly independent solutions of the differential equation $$(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3}$$ Previously I have seen that the following holds  for the differential equation $y''+ \frac{1}{x}y'-\frac{1}{x^2}y=0, x>0$ : We are looking for solutions of the differential equation of the form $x^r$ . Then the function $x^r$ is a solution of the differential equation at $(0,+\infty)$ if: $$r(r-1)x^{r-2}+ \frac{1}{x} r x^{r-1}- \frac{1}{x^2}x^r=0 \forall x >0 \Rightarrow r=1 \text{ or } r=-1$$ So, the functions $y_1(x)=x, y_2= \frac{1}{x}$ are solutions of the differential equation and it also holds that they are linearly indepedent since $W(y_1, y_2) \neq 0$ For this differential equation $$(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3}$$ I thought the following: $(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3} \Rightarrow  y''+ \frac{3}{3x-1}y'-\frac{9}{(3x-1)^2}y=0$ We are looking for solutions of the differential equation of the form $\left( x- \frac{1}{3}\right)^r$ . Then the function $\left( x- \frac{1}{3}\right)^r$ is a solution of the differential equation at $( \frac{1}{3},+\infty)$ if: $$r(r-1) \left( x- \frac{1}{3}\right)^{r-2}+ \frac{1}{ \frac{3x-1}{3}} r \left( \frac{3x-1}{3}\right)^{r-1}- \frac{9}{(3x-1)^2} \left( x- \frac{1}{3}\right)=0 \Rightarrow \dots \Rightarrow r= \pm 1$$ Therefore, the functions $z_1(x)=x- \frac{1}{3}, z_2(x)=\frac{1}{x- \frac{1}{3}}$ are solutions of the differential equation at $\left( \frac{1}{3}, +\infty\right)$ . $$z_1(x) z_2'(x)-z_1'(x) z_2(x)=\frac{-2}{x- \frac{1}{3}} \neq 0$$ So, $z_1, z_2$ are linearly independent solutions of the differential equation. Thus, the general solution of $y''+ \frac{3}{3x-1}y'-\frac{9}{(3x-1)^2}y=0$ is of the form: $$c_1 \left( x- \frac{1}{3} \right)+ c_2 \left( \frac{1}{x- \frac{1}{3}}\right) | c_1, c_2 \in \mathbb{R}, x> \frac{1}{3}$$ EDIT : We set $t=x-\frac{1}{3}$ and we have: $$\frac{dy}{dt}=\frac{dy}{dx} \frac{dx}{dt}=\frac{dy}{dx}$$ $$\frac{d^2y}{dt^2}=\frac{d}{dt} \left( \frac{dy}{dt}\right)=\frac{d}{dt} \left( \frac{dy}{dx} \right)=\frac{dx}{dt} \frac{d}{dx} \left( \frac{dy}{dx} \right)=\frac{d^2y}{dx^2}$$ $$y''(x)+ \frac{1}{x-\frac{1}{3}}y'(x)-\frac{1}{\left( x-\frac{1}{3}\right)^2}y(x)=0 \\ \Rightarrow y''(t)+\frac{1}{t}y'(t)-\frac{1}{t^2}y(t)$$ Two linearly independent solutions are $y_1(t)=t$ and $y_2(t)=\frac{1}{t}, y \in (0,+\infty)$ . Thus, two linearly independent solutions of $y''+\frac{1}{x-\frac{1}{3}}y'-\frac{1}{\left( x-\frac{1}{3} \right)^2}y=0, x> \frac{1}{3}$ are $y_1(x)=x-\frac{1}{3}, y_2(x)=\frac{1}{x-\frac{1}{3}}$ Is it right or have I done something wrong?","I want to find two linearly independent solutions of the differential equation Previously I have seen that the following holds  for the differential equation : We are looking for solutions of the differential equation of the form . Then the function is a solution of the differential equation at if: So, the functions are solutions of the differential equation and it also holds that they are linearly indepedent since For this differential equation I thought the following: We are looking for solutions of the differential equation of the form . Then the function is a solution of the differential equation at if: Therefore, the functions are solutions of the differential equation at . So, are linearly independent solutions of the differential equation. Thus, the general solution of is of the form: EDIT : We set and we have: Two linearly independent solutions are and . Thus, two linearly independent solutions of are Is it right or have I done something wrong?","(3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3} y''+ \frac{1}{x}y'-\frac{1}{x^2}y=0, x>0 x^r x^r (0,+\infty) r(r-1)x^{r-2}+ \frac{1}{x} r x^{r-1}- \frac{1}{x^2}x^r=0 \forall x >0 \Rightarrow r=1 \text{ or } r=-1 y_1(x)=x, y_2= \frac{1}{x} W(y_1, y_2) \neq 0 (3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3} (3x-1)^2 y''+(9x-3)y'-9y=0 \text{ for } x> \frac{1}{3} \Rightarrow  y''+ \frac{3}{3x-1}y'-\frac{9}{(3x-1)^2}y=0 \left( x- \frac{1}{3}\right)^r \left( x- \frac{1}{3}\right)^r ( \frac{1}{3},+\infty) r(r-1) \left( x- \frac{1}{3}\right)^{r-2}+ \frac{1}{ \frac{3x-1}{3}} r \left( \frac{3x-1}{3}\right)^{r-1}- \frac{9}{(3x-1)^2} \left( x- \frac{1}{3}\right)=0 \Rightarrow \dots \Rightarrow r= \pm 1 z_1(x)=x- \frac{1}{3}, z_2(x)=\frac{1}{x- \frac{1}{3}} \left( \frac{1}{3}, +\infty\right) z_1(x) z_2'(x)-z_1'(x) z_2(x)=\frac{-2}{x- \frac{1}{3}} \neq 0 z_1, z_2 y''+ \frac{3}{3x-1}y'-\frac{9}{(3x-1)^2}y=0 c_1 \left( x- \frac{1}{3} \right)+ c_2 \left( \frac{1}{x- \frac{1}{3}}\right) | c_1, c_2 \in \mathbb{R}, x> \frac{1}{3} t=x-\frac{1}{3} \frac{dy}{dt}=\frac{dy}{dx} \frac{dx}{dt}=\frac{dy}{dx} \frac{d^2y}{dt^2}=\frac{d}{dt} \left( \frac{dy}{dt}\right)=\frac{d}{dt} \left( \frac{dy}{dx} \right)=\frac{dx}{dt} \frac{d}{dx} \left( \frac{dy}{dx} \right)=\frac{d^2y}{dx^2} y''(x)+ \frac{1}{x-\frac{1}{3}}y'(x)-\frac{1}{\left( x-\frac{1}{3}\right)^2}y(x)=0 \\ \Rightarrow y''(t)+\frac{1}{t}y'(t)-\frac{1}{t^2}y(t) y_1(t)=t y_2(t)=\frac{1}{t}, y \in (0,+\infty) y''+\frac{1}{x-\frac{1}{3}}y'-\frac{1}{\left( x-\frac{1}{3} \right)^2}y=0, x> \frac{1}{3} y_1(x)=x-\frac{1}{3}, y_2(x)=\frac{1}{x-\frac{1}{3}}",['ordinary-differential-equations']
67,Differential Equations Constant,Differential Equations Constant,,"The function $y(x)$ satisfies the linear equation $$y'' + p(x)y' + q(x)y = 0.$$ The Wronskian $W(x)$ of two independent solutions, denoted $y_1(x)$ and $y_2(x)$ , is defined to be $$W(x) = \left|\begin{array}{ccc} y_1 & y_2 \\ y_1' & y_2' \end{array}  \right|.$$ Let $y_1(x)$ be given. Use the Wronskian to determine a first-order inhomogeneous differential equation for $y_2(x)$ . Hence, show that $$y_2(x) = y_1(x) \int_{x_0}^x \frac {W(t)}{y_1(t)^2} \mathrm d t . $$ I get by expanding the determinant and using an integrating factor $$ \frac {y_2(x)} {y_1(x)} =  \int \frac {W(x)}{y_1(x)^2} \mathrm d x $$ hence $$ \frac {y_2(x)} {y_1(x)} -  \frac {y_2(x_0)} {y_1(x_0)} =  \int_{x_0}^x \frac {W(t)}{y_1(t)^2} \mathrm d t . $$ But why can I just decide $y_2(x_0)=0$ if $x_0$ is arbitrary? And if it's for a specific value of $x_0$ (the question is rather vague about this...) then how do I know that in general there exists $x_0$ such that $y_2(x_0)=0$ ?","The function satisfies the linear equation The Wronskian of two independent solutions, denoted and , is defined to be Let be given. Use the Wronskian to determine a first-order inhomogeneous differential equation for . Hence, show that I get by expanding the determinant and using an integrating factor hence But why can I just decide if is arbitrary? And if it's for a specific value of (the question is rather vague about this...) then how do I know that in general there exists such that ?",y(x) y'' + p(x)y' + q(x)y = 0. W(x) y_1(x) y_2(x) W(x) = \left|\begin{array}{ccc} y_1 & y_2 \\ y_1' & y_2' \end{array}  \right|. y_1(x) y_2(x) y_2(x) = y_1(x) \int_{x_0}^x \frac {W(t)}{y_1(t)^2} \mathrm d t .   \frac {y_2(x)} {y_1(x)} =  \int \frac {W(x)}{y_1(x)^2} \mathrm d x   \frac {y_2(x)} {y_1(x)} -  \frac {y_2(x_0)} {y_1(x_0)} =  \int_{x_0}^x \frac {W(t)}{y_1(t)^2} \mathrm d t .  y_2(x_0)=0 x_0 x_0 x_0 y_2(x_0)=0,['ordinary-differential-equations']
68,Picard Iterates Converge Uniformly,Picard Iterates Converge Uniformly,,"I have a homework question that asks to show that the Picard iterates $$ \phi_{n+1}(t) = \int_0^t 1 + \phi_n^2(s) \, ds, \quad \quad \phi_0(t) = 0 $$ converge uniformly on any compact interval $[-r, r] \subseteq (- \pi/2, \pi/2)$. I was thinking about stating the fact that those Picard iterates represent the initial value problem $$ x' = 1 + x^2 = f(x), \quad \quad x(0) = 0, $$ whose solution is of course $x(t) = \tan(t)$.  That is, $$ \phi_{n+1}(t) = \int_0^t f(\phi_n(s)) \, ds, \quad \quad \phi_0(t) = 0. $$ Then use the fact that $f$ is Lipschitz on any compact interval to get a bound $$ \| \phi_{n+1} - \phi_n \| \leq K \| \phi_n - \phi_{n-1} \| $$ for some constant $K$ (where $\| \cdot \|$ is the sup norm).  Then I could bound $ \| \phi_n - \phi_m \|$ and show that $\{ \phi_i \}$ is a Cauchy sequence and I'd be done.  However, I think $K$ has to be less than $1$ for that argument to work and I don't think I can show that.  Also, that argument doesn't take into account the restriction $[-r, r] \subseteq (- \pi/2, \pi/2)$, only the fact that $f$ is Lipschitz on any compact interval. I'd really appreciate a push in the right direction.","I have a homework question that asks to show that the Picard iterates $$ \phi_{n+1}(t) = \int_0^t 1 + \phi_n^2(s) \, ds, \quad \quad \phi_0(t) = 0 $$ converge uniformly on any compact interval $[-r, r] \subseteq (- \pi/2, \pi/2)$. I was thinking about stating the fact that those Picard iterates represent the initial value problem $$ x' = 1 + x^2 = f(x), \quad \quad x(0) = 0, $$ whose solution is of course $x(t) = \tan(t)$.  That is, $$ \phi_{n+1}(t) = \int_0^t f(\phi_n(s)) \, ds, \quad \quad \phi_0(t) = 0. $$ Then use the fact that $f$ is Lipschitz on any compact interval to get a bound $$ \| \phi_{n+1} - \phi_n \| \leq K \| \phi_n - \phi_{n-1} \| $$ for some constant $K$ (where $\| \cdot \|$ is the sup norm).  Then I could bound $ \| \phi_n - \phi_m \|$ and show that $\{ \phi_i \}$ is a Cauchy sequence and I'd be done.  However, I think $K$ has to be less than $1$ for that argument to work and I don't think I can show that.  Also, that argument doesn't take into account the restriction $[-r, r] \subseteq (- \pi/2, \pi/2)$, only the fact that $f$ is Lipschitz on any compact interval. I'd really appreciate a push in the right direction.",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'convergence-divergence', 'lipschitz-functions']"
69,A funtion equation $2f(x)+f''(x)=xf'(x)$,A funtion equation,2f(x)+f''(x)=xf'(x),"Suppose $f(x)\in C^2(\mathbb R)$ solves the equation $$ 2f(x)+f''(x)=xf'(x),\forall x\in\mathbb R $$ Assume that $(f(0),f'(0))\neq (0,0)$. Prove that $(f(x),f'(x))$ is unbounded.","Suppose $f(x)\in C^2(\mathbb R)$ solves the equation $$ 2f(x)+f''(x)=xf'(x),\forall x\in\mathbb R $$ Assume that $(f(0),f'(0))\neq (0,0)$. Prove that $(f(x),f'(x))$ is unbounded.",,['ordinary-differential-equations']
70,What is the purpose of studying Sturm-Louville eigenvalue problem?,What is the purpose of studying Sturm-Louville eigenvalue problem?,,"After a cursory read on the SL eigenvalue problem, I did not immediately feel enlightened and failed find much usefulness except for knowing that SL generalizes a broader class of differential equations. Firstly, knowing a DE is Sturm-Louville system doesn't automatically produce a solution. Also, it does not give you the form of the eigenvalues. I would think these are the most important aspect of studying DE. What is done however, is to first solve the DE and then recognize it is a SL DE and hence satisfies a bunch of useful properties such as minimum eigenvalue, etc. But even knowing all this, I have not been able to solve DE faster knowing that it is a SL DE or it satisfies a myriad of properties. Why do we characterize an ODE as SL problem at all?","After a cursory read on the SL eigenvalue problem, I did not immediately feel enlightened and failed find much usefulness except for knowing that SL generalizes a broader class of differential equations. Firstly, knowing a DE is Sturm-Louville system doesn't automatically produce a solution. Also, it does not give you the form of the eigenvalues. I would think these are the most important aspect of studying DE. What is done however, is to first solve the DE and then recognize it is a SL DE and hence satisfies a bunch of useful properties such as minimum eigenvalue, etc. But even knowing all this, I have not been able to solve DE faster knowing that it is a SL DE or it satisfies a myriad of properties. Why do we characterize an ODE as SL problem at all?",,"['ordinary-differential-equations', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'eigenfunctions']"
71,I have a special solution for the Lane-Emden equation. Can I use it to find the general solution?,I have a special solution for the Lane-Emden equation. Can I use it to find the general solution?,,"The general Lane-Emden equation is $$\ddot{y}+\frac{2\dot{y}}{x}+y^N=0$$ where $y(0)=1$ and $\dot{y}(0)=0$.  If we eliminate the requirement that $y(0)=1$ there is a special solution for all real values of N, excluding N=1. $$y=\bigg(\frac{xi}{\sqrt{\beta^2 +\beta}}\bigg)^\beta$$  where $\beta=\frac{2}{1-N}$. Is there some way to use this special solution to find the general solution?  It would be of great interest to astrophysicists. The equation is invariant to the Lie group $G(x,y)=(\lambda x,\lambda^\beta y)\lambda_o=0$.  When you apply this group to the DEQ you find that $\beta -2=N\beta$, or $\beta=\frac{2}{1-N}$  Its invariants (polynomial group stabilizers in the kernel of the map) are $\eta=\frac{\ddot{y}}{x^{\beta -2}}$, $\nu=\frac{\dot{y}}{x^{\beta-1}}$ and $\mu=\frac{y}{x^\beta}$.  The Lane-Emden equation expressed as invariants is $\eta+2\nu+\mu^N=0$. $$ \frac{d\nu}{d\mu}=\frac{x\frac{d\nu}{dx}}{x\frac{d\mu}{dx}}=\frac{\eta-(\beta -1)\nu}{\nu-\beta\mu} $$ Since $\eta=-\mu^N-2\nu$, this becomes $$ (\mu^N+(1+\beta)\nu)d\mu+(\nu-\beta\mu)d\nu=0 $$ Solve this and you've got it. The above special solution was found using the Lie algebra (also in the kernel of the map) between the invariants at singlarities, saddle points and along separatrices: $\eta=\beta(\beta -1)\mu$ and $\nu=\beta\mu$.","The general Lane-Emden equation is $$\ddot{y}+\frac{2\dot{y}}{x}+y^N=0$$ where $y(0)=1$ and $\dot{y}(0)=0$.  If we eliminate the requirement that $y(0)=1$ there is a special solution for all real values of N, excluding N=1. $$y=\bigg(\frac{xi}{\sqrt{\beta^2 +\beta}}\bigg)^\beta$$  where $\beta=\frac{2}{1-N}$. Is there some way to use this special solution to find the general solution?  It would be of great interest to astrophysicists. The equation is invariant to the Lie group $G(x,y)=(\lambda x,\lambda^\beta y)\lambda_o=0$.  When you apply this group to the DEQ you find that $\beta -2=N\beta$, or $\beta=\frac{2}{1-N}$  Its invariants (polynomial group stabilizers in the kernel of the map) are $\eta=\frac{\ddot{y}}{x^{\beta -2}}$, $\nu=\frac{\dot{y}}{x^{\beta-1}}$ and $\mu=\frac{y}{x^\beta}$.  The Lane-Emden equation expressed as invariants is $\eta+2\nu+\mu^N=0$. $$ \frac{d\nu}{d\mu}=\frac{x\frac{d\nu}{dx}}{x\frac{d\mu}{dx}}=\frac{\eta-(\beta -1)\nu}{\nu-\beta\mu} $$ Since $\eta=-\mu^N-2\nu$, this becomes $$ (\mu^N+(1+\beta)\nu)d\mu+(\nu-\beta\mu)d\nu=0 $$ Solve this and you've got it. The above special solution was found using the Lie algebra (also in the kernel of the map) between the invariants at singlarities, saddle points and along separatrices: $\eta=\beta(\beta -1)\mu$ and $\nu=\beta\mu$.",,['ordinary-differential-equations']
72,Under what conditions can a function $ y: \mathbb{R} \to \mathbb{R} $ be expressed as $ \dfrac{z'}{z} $?,Under what conditions can a function  be expressed as ?, y: \mathbb{R} \to \mathbb{R}   \dfrac{z'}{z} ,"Can an arbitrary function $ y: \mathbb{R} \to \mathbb{R} $ always be expressed as $ \dfrac{z'}{z} $ for some differentiable function $ z: \mathbb{R} \to \mathbb{R} $, or are additional conditions on $ y $ needed for this to be true? This question was inspired by reading a writeup about differential equations, where the change of variables $ y = \dfrac{z'}{z} $ is made.","Can an arbitrary function $ y: \mathbb{R} \to \mathbb{R} $ always be expressed as $ \dfrac{z'}{z} $ for some differentiable function $ z: \mathbb{R} \to \mathbb{R} $, or are additional conditions on $ y $ needed for this to be true? This question was inspired by reading a writeup about differential equations, where the change of variables $ y = \dfrac{z'}{z} $ is made.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'derivatives']"
73,How find this ODE solution $f''(x)=f(x)(1+2\tan^2{x})$,How find this ODE solution,f''(x)=f(x)(1+2\tan^2{x}),Question: Find the ODE solution:   $$f''(x)=f(x)(1+2\tan^2{x})$$ such $f(0)=0$ My idea: let $y=f(x)$  then $$y''=y(1+2\tan^2{x})$$ $$\Longrightarrow \dfrac{y''}{y}=1+2\tan^2{x}$$ and I found  use wolf : http://www.wolframalpha.com/input/?i=y%27%27%3Dy%281%2B2tan%5E2x%29&dataset= Now How can you find this solution? by hand? Thank you,Question: Find the ODE solution:   $$f''(x)=f(x)(1+2\tan^2{x})$$ such $f(0)=0$ My idea: let $y=f(x)$  then $$y''=y(1+2\tan^2{x})$$ $$\Longrightarrow \dfrac{y''}{y}=1+2\tan^2{x}$$ and I found  use wolf : http://www.wolframalpha.com/input/?i=y%27%27%3Dy%281%2B2tan%5E2x%29&dataset= Now How can you find this solution? by hand? Thank you,,['ordinary-differential-equations']
74,Inverse Function Differential Equation [duplicate],Inverse Function Differential Equation [duplicate],,"This question already has an answer here : Inverse of a bijection f is equal to its derivative (1 answer) Closed 10 years ago . For the differential equation $$\frac{d}{dx}[y(x)]=y^{(-1)}(x)$$ where $y^{(-1)}(x)$ is the inverse of $y(x)$, find y(x). I gave up on finding the solution analytically pretty quickly and decided that a numerical approach might be more effective. But, I'm not sure that this problem can be solved, even numerically; an Euler or Runge-Kutta type method will not work because to find the value of $y^{(-1)}(a)$, one must first know the value of $y(b)$, where $a$ is not necessarily equal to $b$. Sort of like trying to solve $\frac{d}{dx}[y(x)]=y(x+1)$, I don't know of any numerical approaches that can handle a problem of that type. If anyone has any ideas on how this might be solved (or proven unsolvable), they would be appreciated. Thanks!","This question already has an answer here : Inverse of a bijection f is equal to its derivative (1 answer) Closed 10 years ago . For the differential equation $$\frac{d}{dx}[y(x)]=y^{(-1)}(x)$$ where $y^{(-1)}(x)$ is the inverse of $y(x)$, find y(x). I gave up on finding the solution analytically pretty quickly and decided that a numerical approach might be more effective. But, I'm not sure that this problem can be solved, even numerically; an Euler or Runge-Kutta type method will not work because to find the value of $y^{(-1)}(a)$, one must first know the value of $y(b)$, where $a$ is not necessarily equal to $b$. Sort of like trying to solve $\frac{d}{dx}[y(x)]=y(x+1)$, I don't know of any numerical approaches that can handle a problem of that type. If anyone has any ideas on how this might be solved (or proven unsolvable), they would be appreciated. Thanks!",,"['ordinary-differential-equations', 'numerical-methods', 'inverse', 'functional-equations']"
75,What is the meaning of $\;x\;dx=y\;dy\;$?,What is the meaning of ?,\;x\;dx=y\;dy\;,I know that $x > y$ and $\;x\;dx=y\;dy.\;$ Can someone explain to me what is the meaning of this and how are x and y related?,I know that $x > y$ and $\;x\;dx=y\;dy.\;$ Can someone explain to me what is the meaning of this and how are x and y related?,,['ordinary-differential-equations']
76,Fixed points of a dynamical system,Fixed points of a dynamical system,,"My understanding was that a fixed point of a dynamical system $\dot{x} = f(x)$ is a point $x$ such that $f(x) = 0$ and that if an (autonomous) system starts in a fixed point it will stay there. But for instance the system defined by $\dot{x} = 2x^{\frac{1}{2}}$ with initial condition $x(0) = 0$ has as a solution $x(t) = t^{2}$, which definitely does not stay at 0, although $2x^{\frac{1}{2}} = 0$ when $x = 0$. so where is my thinking going wrong?","My understanding was that a fixed point of a dynamical system $\dot{x} = f(x)$ is a point $x$ such that $f(x) = 0$ and that if an (autonomous) system starts in a fixed point it will stay there. But for instance the system defined by $\dot{x} = 2x^{\frac{1}{2}}$ with initial condition $x(0) = 0$ has as a solution $x(t) = t^{2}$, which definitely does not stay at 0, although $2x^{\frac{1}{2}} = 0$ when $x = 0$. so where is my thinking going wrong?",,"['ordinary-differential-equations', 'dynamical-systems']"
77,"Solution of $y'+2y=g(t)$, $y(0)=0$, for a given $g(t)$ having a jump discontinuity","Solution of , , for a given  having a jump discontinuity",y'+2y=g(t) y(0)=0 g(t),"The differential equations book that I'm reading states the following: Linear differential equations sometimes occur in which one or both of the functions $p$ and $g$ have jump discontinuities. If $t_0$ is such a point of discontinuity, then it is necessary to solve the equation separately for $t < t_0$ and $t > t_0$. Afterward, the two solutions are matched so that $y$ is continuous at $t_0$; this is accomplished by a proper choice of the arbitrary constants. Then it asks to solve the following initial value problem: $y' + 2y = g(t),\ y(0) = 0$ where $g(t)=\begin{cases} 1 & \text{ if $0 \leq t \leq 1$} \\ 0 & \text{ if $t > 1$}\end{cases}$ For $0 \leq t \leq 1$: $$y' + 2y = 1$$ The integrating factor is $e^{2t}$: $$(ye^{2t})' = e^{2t}$$ $$ye^{2t} = \dfrac{e^{2t}}{2} + C_1$$ Since the interval $0 \leq t \leq 1$ contains the initial point $t = 0$, the constant $C_1$ is determined by the initial condition $y(0) = 0$: $0e^{0} = \dfrac{e^{0}}{2} + C_1$. So, $C=-\dfrac{1}{2}$ Therefore, the solution for this interval is $y = \dfrac{1}{2}\left(1-e^{-2t}\right)$. For $t > 1$: $$y' + 2y = 0$$ The integrating factor is $e^{2t}$: $$(ye^{2t})' = 0$$ $$ye^{2t} = C_2$$ $$y = C_2e^{-2t}$$ The above solution should be ""matched"" with the previous one at $t=1$. So, setting $t = 1$ and equalling both solutions gives the value of $C_2$: $$\dfrac{1}{2}\left(1-e^{-2}\right) = C_2e^{-2}$$ $$C_2 = \dfrac{1}{2}(e^2-1)$$ So, the second solution becomes $y = \dfrac{1}{2}(e^2-1)e^{-2t}$ Now, a continuous function can be created, joining these two solutions gives the general solution: $$y(t)=\begin{cases} \dfrac{1}{2}(1-e^{-2t}) & \text{ if $0 \leq t \leq 1$} \\ \dfrac{1}{2}(e^2-1)e^{-2t} & \text{ if $t > 1$}\end{cases}$$ This attempt is correct, because this is the answer given by the book. But, although the function above is continuous at $t=1$, it is not differentiable, since the derivative of $y(t)$ is not continuous at $t=1$ (there is a ""kink"" at $y(1)$). So, why is the function above a solution for $t\geq 0$? Shouldn't a solution be differentiable everywhere?","The differential equations book that I'm reading states the following: Linear differential equations sometimes occur in which one or both of the functions $p$ and $g$ have jump discontinuities. If $t_0$ is such a point of discontinuity, then it is necessary to solve the equation separately for $t < t_0$ and $t > t_0$. Afterward, the two solutions are matched so that $y$ is continuous at $t_0$; this is accomplished by a proper choice of the arbitrary constants. Then it asks to solve the following initial value problem: $y' + 2y = g(t),\ y(0) = 0$ where $g(t)=\begin{cases} 1 & \text{ if $0 \leq t \leq 1$} \\ 0 & \text{ if $t > 1$}\end{cases}$ For $0 \leq t \leq 1$: $$y' + 2y = 1$$ The integrating factor is $e^{2t}$: $$(ye^{2t})' = e^{2t}$$ $$ye^{2t} = \dfrac{e^{2t}}{2} + C_1$$ Since the interval $0 \leq t \leq 1$ contains the initial point $t = 0$, the constant $C_1$ is determined by the initial condition $y(0) = 0$: $0e^{0} = \dfrac{e^{0}}{2} + C_1$. So, $C=-\dfrac{1}{2}$ Therefore, the solution for this interval is $y = \dfrac{1}{2}\left(1-e^{-2t}\right)$. For $t > 1$: $$y' + 2y = 0$$ The integrating factor is $e^{2t}$: $$(ye^{2t})' = 0$$ $$ye^{2t} = C_2$$ $$y = C_2e^{-2t}$$ The above solution should be ""matched"" with the previous one at $t=1$. So, setting $t = 1$ and equalling both solutions gives the value of $C_2$: $$\dfrac{1}{2}\left(1-e^{-2}\right) = C_2e^{-2}$$ $$C_2 = \dfrac{1}{2}(e^2-1)$$ So, the second solution becomes $y = \dfrac{1}{2}(e^2-1)e^{-2t}$ Now, a continuous function can be created, joining these two solutions gives the general solution: $$y(t)=\begin{cases} \dfrac{1}{2}(1-e^{-2t}) & \text{ if $0 \leq t \leq 1$} \\ \dfrac{1}{2}(e^2-1)e^{-2t} & \text{ if $t > 1$}\end{cases}$$ This attempt is correct, because this is the answer given by the book. But, although the function above is continuous at $t=1$, it is not differentiable, since the derivative of $y(t)$ is not continuous at $t=1$ (there is a ""kink"" at $y(1)$). So, why is the function above a solution for $t\geq 0$? Shouldn't a solution be differentiable everywhere?",,"['ordinary-differential-equations', 'continuity']"
78,What's the difference between $\frac{dy}{dx}$ and $dy$?,What's the difference between  and ?,\frac{dy}{dx} dy,"Ok, so I was doing a substitution problem and I realized that $dy = u\ dx + x\ du$ and not $\frac{dy}{dx} = u\ dx + x\ du$ and I was wondering what the difference was between those two. My first guess would be that $\frac{dy}{dx}$ means differential of $y$ with respect to $x$, and $dy$ would be differential of $y$, but I don't know what that implies exactly. Can you provide examples so I can wrap my head around that concept? If $\frac{dy}{dx}$ = integral of $3x$, what would $y$ be?","Ok, so I was doing a substitution problem and I realized that $dy = u\ dx + x\ du$ and not $\frac{dy}{dx} = u\ dx + x\ du$ and I was wondering what the difference was between those two. My first guess would be that $\frac{dy}{dx}$ means differential of $y$ with respect to $x$, and $dy$ would be differential of $y$, but I don't know what that implies exactly. Can you provide examples so I can wrap my head around that concept? If $\frac{dy}{dx}$ = integral of $3x$, what would $y$ be?",,['ordinary-differential-equations']
79,Lipschitz Continuity,Lipschitz Continuity,,"I have a quick question on how to interpret Lipschitz Continuity rather than normal continuity. What is the exact difference between the two, because I can't really find one. And is it more general than normal continuity or is it a more strict kind of continuity?","I have a quick question on how to interpret Lipschitz Continuity rather than normal continuity. What is the exact difference between the two, because I can't really find one. And is it more general than normal continuity or is it a more strict kind of continuity?",,['ordinary-differential-equations']
80,response of unit step input in harmonically oscillating system,response of unit step input in harmonically oscillating system,,"As far as I've understood or misunderstood in constant coefficient second order differential equation  $$\frac{d^2y}{dt^2} + b \frac{dy}{dt} +cy = ef(t)$$ $b$, $c$ being constants, $f(t)$ the input to the system, $y$ being response of the system. Let, $$\frac{d^2y}{dt^2}  +\omega_0^2y = u(t)k$$ such that $k$ is non zero be a system and $u(t)$ a unit step function. How to physically visualize this system(or input to the system), isn't the $u(t)$ like applying constant force to the system? Will it not bring the system to halt after certain time?  Like if we keep poking a mass-spring system with constant force only in one direction?? But the solution seems different . Please Help me to clear this simple misconception. Thanks you!!","As far as I've understood or misunderstood in constant coefficient second order differential equation  $$\frac{d^2y}{dt^2} + b \frac{dy}{dt} +cy = ef(t)$$ $b$, $c$ being constants, $f(t)$ the input to the system, $y$ being response of the system. Let, $$\frac{d^2y}{dt^2}  +\omega_0^2y = u(t)k$$ such that $k$ is non zero be a system and $u(t)$ a unit step function. How to physically visualize this system(or input to the system), isn't the $u(t)$ like applying constant force to the system? Will it not bring the system to halt after certain time?  Like if we keep poking a mass-spring system with constant force only in one direction?? But the solution seems different . Please Help me to clear this simple misconception. Thanks you!!",,['ordinary-differential-equations']
81,Guessing the form of solutions when solving differential equations,Guessing the form of solutions when solving differential equations,,"When solving differential equations, it seems like we often have to guess the form of the solution beforehand. How does one know what to try? It seems natural to the authors of textbooks to try say a power series solution or some other form, but I don't quite understand how they know. Also, there tends to also be the method of guessing a form of the solution then verifying later that it is valid, but won't we risk unwittingly compromise generality? Thanks.","When solving differential equations, it seems like we often have to guess the form of the solution beforehand. How does one know what to try? It seems natural to the authors of textbooks to try say a power series solution or some other form, but I don't quite understand how they know. Also, there tends to also be the method of guessing a form of the solution then verifying later that it is valid, but won't we risk unwittingly compromise generality? Thanks.",,['ordinary-differential-equations']
82,How to solve the differential equation?,How to solve the differential equation?,,"How can the following differential equation can be solved? $$ \frac{dy}{dt}=3+e^{-t} -\frac{1}{2}y $$ I proceeded by by rearranging the equation as follows $$ \frac{dy}{dt}+\frac{1}{2}y=3+e^{-t}  $$My idea was to make the LHS a derivatives of two variables so that it could be integrated. But apparently I could not do that.  How should i proceed now? Your help is much appreciated.Thankyou.","How can the following differential equation can be solved? $$ \frac{dy}{dt}=3+e^{-t} -\frac{1}{2}y $$ I proceeded by by rearranging the equation as follows $$ \frac{dy}{dt}+\frac{1}{2}y=3+e^{-t}  $$My idea was to make the LHS a derivatives of two variables so that it could be integrated. But apparently I could not do that.  How should i proceed now? Your help is much appreciated.Thankyou.",,"['calculus', 'ordinary-differential-equations']"
83,The WKB method: motivation,The WKB method: motivation,,"Given a differential equation of the form $\epsilon \frac{d^ny}{dx^n} + \sum_{k=0}^{n-1} a_k(x)\frac{d^ky}{dx^k}=0$ Then the WKB method says to choose the ansatz $y\sim exp({\frac{i\phi(x)}{\epsilon}})A(x,\epsilon)$ where $A(x,\epsilon) = \sum_{k=0}^{\infty} A_k(x)\epsilon^n$ I wondered what is the motivation for this educated guess. Thanks in advance.","Given a differential equation of the form $\epsilon \frac{d^ny}{dx^n} + \sum_{k=0}^{n-1} a_k(x)\frac{d^ky}{dx^k}=0$ Then the WKB method says to choose the ansatz $y\sim exp({\frac{i\phi(x)}{\epsilon}})A(x,\epsilon)$ where $A(x,\epsilon) = \sum_{k=0}^{\infty} A_k(x)\epsilon^n$ I wondered what is the motivation for this educated guess. Thanks in advance.",,"['ordinary-differential-equations', 'perturbation-theory']"
84,Solution to a nonlinear ODE,Solution to a nonlinear ODE,,"$$ a_1 y'''''+ a_2 y'''+\left(a_3 + y^2 \right) y' = 0 $$ where $a_1, a_2, a_3$ are constants with $a_1>0$ and $a_2,a_3 \in \mathbb{R}$ . Is there a general solution $y(x)$ to the above differential equation? I am aware that there is an easy solution to the linearized version of the above equation $(a_1 y'''''+ a_2 y'''+ a_3 y' = 0)$ and want to know if there is a solution to the nonlinear equation too. The equation arises in a mechanics problem. We have an unevenly pre-stretched ribbon (narrow plate). The constants $a_1,a_2$ depend on prestretch, and $a_3$ is a regularizing parameter. We are interested in the mechanics of twisting this pre-stretched ribbon. The mechanical system can be seen in figure-1 of this arxiv paper","where are constants with and . Is there a general solution to the above differential equation? I am aware that there is an easy solution to the linearized version of the above equation and want to know if there is a solution to the nonlinear equation too. The equation arises in a mechanics problem. We have an unevenly pre-stretched ribbon (narrow plate). The constants depend on prestretch, and is a regularizing parameter. We are interested in the mechanics of twisting this pre-stretched ribbon. The mechanical system can be seen in figure-1 of this arxiv paper"," a_1 y'''''+ a_2 y'''+\left(a_3 + y^2 \right) y' = 0  a_1, a_2, a_3 a_1>0 a_2,a_3 \in \mathbb{R} y(x) (a_1 y'''''+ a_2 y'''+ a_3 y' = 0) a_1,a_2 a_3","['ordinary-differential-equations', 'asymptotics', 'nonlinear-system', 'perturbation-theory']"
85,"Among the curves whose all tangents pass through the origin, find the one that passes through point $(a,b)$.","Among the curves whose all tangents pass through the origin, find the one that passes through point .","(a,b)","Among the curves whose all tangents pass through the origin, find the one that passes through point $(a,b)$ . Here is my solution but my answer seems incorrect. Let $f(x)$ be the function of the curve. At $(t, f(t))$ , the function $y=f(x)$ has a tangent line $y=f'(t)(x-t)+f(t)$ . Since the tangent line passes through the origin, we get $0=f'(t)(-t)+f(t)$ $tf'(t)=f(t)$ , which can be written as the differential equation $xy'=y$ . After solving the differential equation, I got the family of curve $y = Cx$ , which is are composed of straight lines passing through the origin. I am stuck here and don't know what the next step should be. Please feel free to share your thoughts. Thank you in advance.","Among the curves whose all tangents pass through the origin, find the one that passes through point . Here is my solution but my answer seems incorrect. Let be the function of the curve. At , the function has a tangent line . Since the tangent line passes through the origin, we get , which can be written as the differential equation . After solving the differential equation, I got the family of curve , which is are composed of straight lines passing through the origin. I am stuck here and don't know what the next step should be. Please feel free to share your thoughts. Thank you in advance.","(a,b) f(x) (t, f(t)) y=f(x) y=f'(t)(x-t)+f(t) 0=f'(t)(-t)+f(t) tf'(t)=f(t) xy'=y y = Cx","['calculus', 'ordinary-differential-equations', 'differential']"
86,How to solve a second order ODE with Dirac delta inhomogeneity?,How to solve a second order ODE with Dirac delta inhomogeneity?,,"I am working with the following second order ODE \begin{equation} y''(x) - q² y(x) = F \delta(x - x_0) \end{equation} on the domain $-L/2$ to $+L/2$ without specifying any boundary conditions for now. The solution to this problem I found in the literature is \begin{equation} y(x) = A \cosh(qx) + B \sinh(qx) + \frac{F}{2q} e^{q|x-x_0|} . \end{equation} However, I can not reconstruct this result. I am not sure how to split the solution into two parts before and after the Dirac spike. Here is how far I have come: The solution to the homogeneous equation can be obtained from the Ansatz \begin{equation} y(x) = A e^{-qx} + B e^{qx} . \end{equation} For the non-homogeneous equation I try variation of parameters starting from \begin{equation} y(x) = C(x) e^{-qx} + D(x) e^{qx} . \end{equation} Plugging this Ansatz into the ODE and introducing the constrain $C'(x) e^{-qx} + D'(x) e^{qx} = 0$ allows me to solve for the coefficients $C'(x)$ and $D'(x)$ \begin{equation} C'(x) = - \frac{F}{2q} \delta(x - x_0) e^{qx} \quad \text{and} \quad D'(x) = \frac{F}{2q} \delta(x - x_0) e^{-qx} . \end{equation} Integrating these equations then gives \begin{equation} C(x_0) = - \frac{F}{2q} e^{qx_0} \quad \text{and} \quad D(x_0) = \frac{F}{2q} e^{-qx_0}, \end{equation} which do not depend on $x$ anymore, but only $x_0$ . This can now be used to obtain the full solution to the ODE \begin{equation} y(x) = A e^{-qx} + B e^{qx} + \frac{F}{2q} \left( e^{q(x - x_0)} - e^{q(x_0 - x)} \right) . \end{equation} I think the homogeneous part of mine is somewhat similar to the solution from the literature. I am more concerned what I missed in constructing the last term. Does anyone have any ideas on this?","I am working with the following second order ODE on the domain to without specifying any boundary conditions for now. The solution to this problem I found in the literature is However, I can not reconstruct this result. I am not sure how to split the solution into two parts before and after the Dirac spike. Here is how far I have come: The solution to the homogeneous equation can be obtained from the Ansatz For the non-homogeneous equation I try variation of parameters starting from Plugging this Ansatz into the ODE and introducing the constrain allows me to solve for the coefficients and Integrating these equations then gives which do not depend on anymore, but only . This can now be used to obtain the full solution to the ODE I think the homogeneous part of mine is somewhat similar to the solution from the literature. I am more concerned what I missed in constructing the last term. Does anyone have any ideas on this?","\begin{equation}
y''(x) - q² y(x) = F \delta(x - x_0)
\end{equation} -L/2 +L/2 \begin{equation}
y(x) = A \cosh(qx) + B \sinh(qx) + \frac{F}{2q} e^{q|x-x_0|} .
\end{equation} \begin{equation}
y(x) = A e^{-qx} + B e^{qx} .
\end{equation} \begin{equation}
y(x) = C(x) e^{-qx} + D(x) e^{qx} .
\end{equation} C'(x) e^{-qx} + D'(x) e^{qx} = 0 C'(x) D'(x) \begin{equation}
C'(x) = - \frac{F}{2q} \delta(x - x_0) e^{qx} \quad \text{and} \quad D'(x) = \frac{F}{2q} \delta(x - x_0) e^{-qx} .
\end{equation} \begin{equation}
C(x_0) = - \frac{F}{2q} e^{qx_0} \quad \text{and} \quad D(x_0) = \frac{F}{2q} e^{-qx_0},
\end{equation} x x_0 \begin{equation}
y(x) = A e^{-qx} + B e^{qx} + \frac{F}{2q} \left( e^{q(x - x_0)} - e^{q(x_0 - x)} \right) .
\end{equation}","['ordinary-differential-equations', 'dirac-delta']"
87,Will squaring both sides of the ode change its type?,Will squaring both sides of the ode change its type?,,"Given the ode \begin{equation} y^{\prime}=\sqrt{1+x+y}\tag{1} \end{equation} The question is, is this a D'Alembert's ode or not? As is known, D'Alembert's has the form Wikipedia \begin{equation} y=xf\left(  p\right)  +g\left(  p\right)  \tag{2} \end{equation} Where $p\equiv\frac{dy}{dx}=y^{\prime}$ . As it stands (1) is not of the form (2). But after squaring both sides of (1) the ode becomes \begin{align} \left(  y^{\prime}\right)  ^{2}  & =1+x+y\nonumber\\ y  & =-x-1+\left(  y^{\prime}\right)  ^{2}\tag{3}\\ & =xf\left(  p\right)  +g\left(  p\right)  \nonumber \end{align} Where \begin{align*} f\left(  p\right)    & =-1\\ g\left(  p\right)    & =-1+p^{2}% \end{align*} Now ode (3) is D'Alembert's. But this is after squaring both sides of (1). So the question is, is it mathematically correct to say (1) is D'Alembert's ode or not? Maple agrees and says that (1) is D'Alembert's ode. So it must have squared both sides. restart; ode:=diff(y(x),x)=sqrt(1+x+y(x)); DEtools:-odeadvisor(ode); gives [[_homogeneous, `class C`], _dAlembert] But some might not agree with Maple here. I am not sure if one is allowed to do that, or if one must only apply the form comparison on the original ode without squaring it. Hence my question. I am well aware that squaring the ode and solving the squared ode will introduce extraneous solutions to the original ode , and one must therefore verify that each solution generated satisfies the original ode, else removed as  extraneous. But that is not my question. My question is on the type of the original ode itself. In particular , I am not sure now if squaring both sides of the ode will generate an ode with all of its solutions that fail to verify the original ode. I do not have such an example myself of such case, but this does not mean such case does not exist. In the example given in this question at the top, squaring the ode generates two solutions, one of which is extraneous and can be discarded, but the second one does verify the original ode. On a side note, without squaring the above ode and solving it as d'Alembert type, I now have no idea how to solve the original ode as is, as I do not know what type it is, and hence do not know what method to apply to it. Maple says the above ode is also of type _homogeneous, class C . But I looked at Maple's website , and read the description, but did not follow it and still have no idea still what it means as there is no actual reference outside Maple that says what _homogeneous, class C ode type is.","Given the ode The question is, is this a D'Alembert's ode or not? As is known, D'Alembert's has the form Wikipedia Where . As it stands (1) is not of the form (2). But after squaring both sides of (1) the ode becomes Where Now ode (3) is D'Alembert's. But this is after squaring both sides of (1). So the question is, is it mathematically correct to say (1) is D'Alembert's ode or not? Maple agrees and says that (1) is D'Alembert's ode. So it must have squared both sides. restart; ode:=diff(y(x),x)=sqrt(1+x+y(x)); DEtools:-odeadvisor(ode); gives [[_homogeneous, `class C`], _dAlembert] But some might not agree with Maple here. I am not sure if one is allowed to do that, or if one must only apply the form comparison on the original ode without squaring it. Hence my question. I am well aware that squaring the ode and solving the squared ode will introduce extraneous solutions to the original ode , and one must therefore verify that each solution generated satisfies the original ode, else removed as  extraneous. But that is not my question. My question is on the type of the original ode itself. In particular , I am not sure now if squaring both sides of the ode will generate an ode with all of its solutions that fail to verify the original ode. I do not have such an example myself of such case, but this does not mean such case does not exist. In the example given in this question at the top, squaring the ode generates two solutions, one of which is extraneous and can be discarded, but the second one does verify the original ode. On a side note, without squaring the above ode and solving it as d'Alembert type, I now have no idea how to solve the original ode as is, as I do not know what type it is, and hence do not know what method to apply to it. Maple says the above ode is also of type _homogeneous, class C . But I looked at Maple's website , and read the description, but did not follow it and still have no idea still what it means as there is no actual reference outside Maple that says what _homogeneous, class C ode type is.","\begin{equation}
y^{\prime}=\sqrt{1+x+y}\tag{1}
\end{equation} \begin{equation}
y=xf\left(  p\right)  +g\left(  p\right)  \tag{2}
\end{equation} p\equiv\frac{dy}{dx}=y^{\prime} \begin{align}
\left(  y^{\prime}\right)  ^{2}  & =1+x+y\nonumber\\
y  & =-x-1+\left(  y^{\prime}\right)  ^{2}\tag{3}\\
& =xf\left(  p\right)  +g\left(  p\right)  \nonumber
\end{align} \begin{align*}
f\left(  p\right)    & =-1\\
g\left(  p\right)    & =-1+p^{2}%
\end{align*}",['ordinary-differential-equations']
88,Solutions of a Linear Differential Equation in a Banach Algebra,Solutions of a Linear Differential Equation in a Banach Algebra,,"Assume $A$ is a real Banach algebra (which need not necessarily be commutative or finite-dimensional) with unit and the function $f: \mathbb{R}\to A$ satisfies the differential equation $$\frac{df\left( t \right)}{dt}=f\left( t \right)\cdot s\left( t \right)$$ for all $t\in \mathbb{R}$ with a given continuous function $s: \mathbb{R}\to A$ . How can you prove that $f\left( t \right)$ is invertible for all $t\in \mathbb{R}$ if there is (at least) one $t_{0}\in \mathbb{R}$ for which $f\left( t_{0} \right)$ is invertible? In case the Banach algebra is just $\mathbb{R}$ everybody knows the answer: the only function $f:\mathbb{R}\to \mathbb{R}$ which then satisfies the differential equation and $f\left( t_{0} \right)=r_{0}$ with a given $r_{0}\in \mathbb{R}$ is defined by $$f\left( t \right)=r_{0}\cdot \exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right)$$ for all $t\in \mathbb{R}$ and thus the assertion ensues immediately. Virtually the same reasoning is valid if the Banach algebra is commutative (no matter whether it is finite-dimensional), only the real exponential function has to be replaced by the exponential on the Banach algebra which is defined by the power series $$\exp\left( a \right)=\sum_{i=0}^{\infty}\frac{1}{i!}\cdot a^{i}$$ which is convergent for all $a\in A$ , and once again, the uniquely determined function $f:A\to A$ which satisfies the differential equation and $f\left( t_{0} \right)=r_{0}$ with a given $r_{0}\in A$ is defined by $f\left( t \right)=r_{0}\cdot \exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right)$ for all $t\in \mathbb{R}$ , where it is well known how to define this integral over the continuous function $s$ with values in $A$ . So, if $f\left( t_{0} \right)=r_{0}$ is invertible then $f\left( t \right)$ is invertible for all $t\in \mathbb{R}$ as this is true for $\exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right)$ . Unfortunately this does not work if the Banach algebra A is not commutative because then a solution of the differential equation cannot be given explicitly using the exponential function on A.","Assume is a real Banach algebra (which need not necessarily be commutative or finite-dimensional) with unit and the function satisfies the differential equation for all with a given continuous function . How can you prove that is invertible for all if there is (at least) one for which is invertible? In case the Banach algebra is just everybody knows the answer: the only function which then satisfies the differential equation and with a given is defined by for all and thus the assertion ensues immediately. Virtually the same reasoning is valid if the Banach algebra is commutative (no matter whether it is finite-dimensional), only the real exponential function has to be replaced by the exponential on the Banach algebra which is defined by the power series which is convergent for all , and once again, the uniquely determined function which satisfies the differential equation and with a given is defined by for all , where it is well known how to define this integral over the continuous function with values in . So, if is invertible then is invertible for all as this is true for . Unfortunately this does not work if the Banach algebra A is not commutative because then a solution of the differential equation cannot be given explicitly using the exponential function on A.",A f: \mathbb{R}\to A \frac{df\left( t \right)}{dt}=f\left( t \right)\cdot s\left( t \right) t\in \mathbb{R} s: \mathbb{R}\to A f\left( t \right) t\in \mathbb{R} t_{0}\in \mathbb{R} f\left( t_{0} \right) \mathbb{R} f:\mathbb{R}\to \mathbb{R} f\left( t_{0} \right)=r_{0} r_{0}\in \mathbb{R} f\left( t \right)=r_{0}\cdot \exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right) t\in \mathbb{R} \exp\left( a \right)=\sum_{i=0}^{\infty}\frac{1}{i!}\cdot a^{i} a\in A f:A\to A f\left( t_{0} \right)=r_{0} r_{0}\in A f\left( t \right)=r_{0}\cdot \exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right) t\in \mathbb{R} s A f\left( t_{0} \right)=r_{0} f\left( t \right) t\in \mathbb{R} \exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right),"['ordinary-differential-equations', 'banach-algebras']"
89,How can I solve the differential equation $f'(x) + f\left( x^2\right) =0$?,How can I solve the differential equation ?,f'(x) + f\left( x^2\right) =0,"I was looking at this AOPS thread which dealt with the following question: Find all functions $f:(1, \infty) \to \mathbb{R}$ that satisfy $$f(x) -f(y) = (y-x)f(xy)  $$ I attempted to solve (at least part of) this question, and in one of my attempts I encountered a path that seemed promising, but I couldn't manage to finish the solution. Here's what I did: Rewriting the functional equation as $$ \frac{f(x) - f(y)}{x-y} = -f(xy) $$ the LHS is the slope of a secant line. This makes me want to take the limit $y\to x$ on both sides to get some derivatives. So assuming $f$ is differentiable on the domain of interest, this gives $$ f'(x) = -f\left(x^2\right) \tag{1} $$ as a differential equation whose solutions form a subset of differentiable solutions to our original functional equation. As shown in the AOPS thread, the family of solutions to the original problem is indeed $f(x) = \frac{c}{x}$ , with $c$ some constant, and this family of functions do satisfy that $\frac{\mathrm{d}}{\mathrm{d} x}\frac{c}{x} = -\frac{c}{x^2}$ as the differential equation suggests, so all seems good up to this point. Even though using the ansatz $f(x) = \frac{c}{x}$ to verify the differential equation $(1)$ does work, I don't know how to solve the differential equation directly. I thought about introducing a substitution of the form $u(x) = x^2$ , but then the LHS of $(1)$ would end up with things like $f'(\sqrt{u})$ which again doesn't seem very useful. Is it possible to solve the differential equation $(1)$ directly? And if the answer is yes, what is the procedure with which you can go about solving a differential equation like this? Thank you!","I was looking at this AOPS thread which dealt with the following question: Find all functions that satisfy I attempted to solve (at least part of) this question, and in one of my attempts I encountered a path that seemed promising, but I couldn't manage to finish the solution. Here's what I did: Rewriting the functional equation as the LHS is the slope of a secant line. This makes me want to take the limit on both sides to get some derivatives. So assuming is differentiable on the domain of interest, this gives as a differential equation whose solutions form a subset of differentiable solutions to our original functional equation. As shown in the AOPS thread, the family of solutions to the original problem is indeed , with some constant, and this family of functions do satisfy that as the differential equation suggests, so all seems good up to this point. Even though using the ansatz to verify the differential equation does work, I don't know how to solve the differential equation directly. I thought about introducing a substitution of the form , but then the LHS of would end up with things like which again doesn't seem very useful. Is it possible to solve the differential equation directly? And if the answer is yes, what is the procedure with which you can go about solving a differential equation like this? Thank you!","f:(1, \infty) \to \mathbb{R} f(x) -f(y) = (y-x)f(xy)   
\frac{f(x) - f(y)}{x-y} = -f(xy)
 y\to x f 
f'(x) = -f\left(x^2\right) \tag{1}
 f(x) = \frac{c}{x} c \frac{\mathrm{d}}{\mathrm{d} x}\frac{c}{x} = -\frac{c}{x^2} f(x) = \frac{c}{x} (1) u(x) = x^2 (1) f'(\sqrt{u}) (1)","['calculus', 'ordinary-differential-equations', 'contest-math', 'functional-equations']"
90,Non-standard calculus of variations solution leads to contradiction,Non-standard calculus of variations solution leads to contradiction,,"I've been given the calculus of variations problem of extremizing $$I=\int_a^bf(x,y,y')dx=\int_a^by\sqrt{1+y'^2}dx$$ where of course $y$ is a function of $x$ . I came up with what I thought was a pretty sharp solution method that got around nonlinearities in the application of the Euler-Lagrange equation, but it leads to an apparent contradiction: Since there is no direct $x$ dependence in the integrand, the Hamiltonian/first-integral   must be constant for the extremizing $y$ , i.e. $$f-y'\frac{\partial f}{\partial y'}=c_1\Leftrightarrow y\sqrt{1+y'^2}-\frac{yy'^2}{\sqrt{1+y'^2}}=c_1\Leftrightarrow y+yy'^2-yy'^2=c_1\sqrt{1+y'^2}$$ $$\Leftrightarrow y^2=c_1^2(1+y'^2)\Leftrightarrow 1+y'^2= \frac{y^2}{c_1^2}.\tag{1}$$ Extremal $y$ must also satisfy the normal Euler-Lagrange equations $$\frac{\partial f}{\partial y}=\frac{d}{dx}\frac{\partial f}{\partial y'}\Leftrightarrow \sqrt{1+y'^2}=\frac{d}{dx}\left[ \frac{yy'}{\sqrt{1+y'^2}} \right]$$ $$\Leftrightarrow \sqrt{1+y'^2}=\frac{y'^2}{\sqrt{1+y'^2}}+yy''\left(\frac{1}{\sqrt{1+y'^2}}- \frac{y'^2}{(1+y'^2)^{3/2}} \right)$$ $$\Leftrightarrow (1+y'^2)^2=y'^2+y'^4+yy''+yy'^2y''-yy'^2y''$$ $$\Leftrightarrow y'^4+2y'^2+1=y'^2+y'^4+yy''$$ $$\Leftrightarrow y'^2-yy''+1=0.\tag{2}$$ Substituting the first-integral expression for $1+y'^2$ , $$\frac{y^2}{c_1^2}=yy''\Leftrightarrow y''-\frac{y}{c_1^2}=0.\tag{3}$$ However, the solution to this linear ODE is not a solution to the Euler-Lagrange ODE. The only way I can see that the argument is unjustified is if $c_1=0$ and therefore $y=0$ , but boundary conditions were given in the problem that this solution cannot satisfy. I've even written out proofs of the E-L equations and Hamiltonian conservation that don't appear to make any unjustified assumptions. Any ideas?","I've been given the calculus of variations problem of extremizing where of course is a function of . I came up with what I thought was a pretty sharp solution method that got around nonlinearities in the application of the Euler-Lagrange equation, but it leads to an apparent contradiction: Since there is no direct dependence in the integrand, the Hamiltonian/first-integral   must be constant for the extremizing , i.e. Extremal must also satisfy the normal Euler-Lagrange equations Substituting the first-integral expression for , However, the solution to this linear ODE is not a solution to the Euler-Lagrange ODE. The only way I can see that the argument is unjustified is if and therefore , but boundary conditions were given in the problem that this solution cannot satisfy. I've even written out proofs of the E-L equations and Hamiltonian conservation that don't appear to make any unjustified assumptions. Any ideas?","I=\int_a^bf(x,y,y')dx=\int_a^by\sqrt{1+y'^2}dx y x x y f-y'\frac{\partial f}{\partial y'}=c_1\Leftrightarrow y\sqrt{1+y'^2}-\frac{yy'^2}{\sqrt{1+y'^2}}=c_1\Leftrightarrow y+yy'^2-yy'^2=c_1\sqrt{1+y'^2} \Leftrightarrow y^2=c_1^2(1+y'^2)\Leftrightarrow 1+y'^2= \frac{y^2}{c_1^2}.\tag{1} y \frac{\partial f}{\partial y}=\frac{d}{dx}\frac{\partial f}{\partial y'}\Leftrightarrow \sqrt{1+y'^2}=\frac{d}{dx}\left[ \frac{yy'}{\sqrt{1+y'^2}} \right] \Leftrightarrow \sqrt{1+y'^2}=\frac{y'^2}{\sqrt{1+y'^2}}+yy''\left(\frac{1}{\sqrt{1+y'^2}}- \frac{y'^2}{(1+y'^2)^{3/2}} \right) \Leftrightarrow (1+y'^2)^2=y'^2+y'^4+yy''+yy'^2y''-yy'^2y'' \Leftrightarrow y'^4+2y'^2+1=y'^2+y'^4+yy'' \Leftrightarrow y'^2-yy''+1=0.\tag{2} 1+y'^2 \frac{y^2}{c_1^2}=yy''\Leftrightarrow y''-\frac{y}{c_1^2}=0.\tag{3} c_1=0 y=0","['ordinary-differential-equations', 'calculus-of-variations', 'classical-mechanics']"
91,Solving a nonlinear PDE,Solving a nonlinear PDE,,"I wish to solve the following PDE: $$ u_t - (u^{2})_{xx}=0 $$ with some boundary conditions which right now is not of much relevance apart from the fact that $u(t,x)$ is a piecewise continuous function and $u_x(t,0)=0$ . I have used the following substitution: $u(t,x)=f(\eta)t^{-\frac{1}{3}}$ and $\eta=xt^{-\frac{1}{3}}$ which reduces the above equation to a single variable of the form: $$ \partial_{\eta}\left ( 2ff_\eta + \frac{\eta f}{3} \right )=0 $$ Now, I am very confused about how to proceed with this reduced form to solve the above equation and find the analytical solution of $f(\eta)$ thereby finding the analytical solution of $u(t,x)$ . Help of any sort is deeply appreciated. Thanks in advance.","I wish to solve the following PDE: with some boundary conditions which right now is not of much relevance apart from the fact that is a piecewise continuous function and . I have used the following substitution: and which reduces the above equation to a single variable of the form: Now, I am very confused about how to proceed with this reduced form to solve the above equation and find the analytical solution of thereby finding the analytical solution of . Help of any sort is deeply appreciated. Thanks in advance.","
u_t - (u^{2})_{xx}=0
 u(t,x) u_x(t,0)=0 u(t,x)=f(\eta)t^{-\frac{1}{3}} \eta=xt^{-\frac{1}{3}} 
\partial_{\eta}\left ( 2ff_\eta + \frac{\eta f}{3} \right )=0
 f(\eta) u(t,x)","['ordinary-differential-equations', 'partial-differential-equations']"
92,"How ""practical"" is the Laplace transform method for constant coefficient ODE?","How ""practical"" is the Laplace transform method for constant coefficient ODE?",,"I just finished teaching a chapter on using Laplace transform to solve constant coefficient second order linear differential equations. I touted how amazing the method was because it incorporates the initial data from the start, works for strange forcing terms, and reduces the problem of solving an ODE to computing the unit impulse response $e(t)$ (by taking the easy inverse Laplace of the reciprocal of the characteristic function) and the convolution $e*g$ , where $g$ is the forcing term--notice how we do not need to compute Laplace of $g$ . I know theoretically this is significant, and that for discontinuous and non-standard forcing terms this is one of the best methods. However, I was left with the feeling that if we cannot really calculate the convolution (closed form) then this is not as impressive after all! So: How commonly is this method actually used in practice for solving ODE -- say by engineers? Are there ways to compute the convolution for a considerably large collection of pairs of functions? If we cannot find convolution in closed form, is this method used to produce numerical solutions, e.g., by estimating the integral in the definition of the convolution? References will be appreciated (over heuristics)!","I just finished teaching a chapter on using Laplace transform to solve constant coefficient second order linear differential equations. I touted how amazing the method was because it incorporates the initial data from the start, works for strange forcing terms, and reduces the problem of solving an ODE to computing the unit impulse response (by taking the easy inverse Laplace of the reciprocal of the characteristic function) and the convolution , where is the forcing term--notice how we do not need to compute Laplace of . I know theoretically this is significant, and that for discontinuous and non-standard forcing terms this is one of the best methods. However, I was left with the feeling that if we cannot really calculate the convolution (closed form) then this is not as impressive after all! So: How commonly is this method actually used in practice for solving ODE -- say by engineers? Are there ways to compute the convolution for a considerably large collection of pairs of functions? If we cannot find convolution in closed form, is this method used to produce numerical solutions, e.g., by estimating the integral in the definition of the convolution? References will be appreciated (over heuristics)!",e(t) e*g g g,"['ordinary-differential-equations', 'laplace-transform', 'convolution', 'inverse-laplace']"
93,ODE solution is monotonic with respect to initial value?,ODE solution is monotonic with respect to initial value?,,"Consider the ODE $$\frac{dy}{dt}=f(y,t), \quad y(0)=a,$$ where we may assume that $f$ is Lipschitz continuous and there is a unique solution $y(t)$ on $[0,2]$ . My questions is following: Is $y(1)$ a monotone (more precisely, increasing) function of $a$ ? I believe the claim holds true because the direction fields never cross, and thus if a solution curve $y_1(t)$ is above $y_2(t)$ at $t=0$ , then it will always be above $y_2$ . If you look at a simple example like logistic equation, you'll see that $y(1)$ is indeed increasing with respect to $y(0)$ . The continuous dependence on initial data has been well-studied but I can not find any result or even discussion on the monotonicity.","Consider the ODE where we may assume that is Lipschitz continuous and there is a unique solution on . My questions is following: Is a monotone (more precisely, increasing) function of ? I believe the claim holds true because the direction fields never cross, and thus if a solution curve is above at , then it will always be above . If you look at a simple example like logistic equation, you'll see that is indeed increasing with respect to . The continuous dependence on initial data has been well-studied but I can not find any result or even discussion on the monotonicity.","\frac{dy}{dt}=f(y,t), \quad y(0)=a, f y(t) [0,2] y(1) a y_1(t) y_2(t) t=0 y_2 y(1) y(0)","['ordinary-differential-equations', 'dynamical-systems', 'initial-value-problems']"
94,A differential equation with a Leibniz's formula,A differential equation with a Leibniz's formula,,"I found this exercise and I don't know how to solve the first question. Solve this differential equation : $$\sum_{k=0}^{n} \binom{n}{k}y^{(k)}=0$$ Deduce that for all $j<n$ : $$\sum_{k=0}^{n} \binom{n}{k} \binom{k}{j}(-1)^k=0$$ For the first part, I don't know how to start, it looks like a Leibniz formula for the product of functions $y$ and a function with all its derivatives equal to one .... which is strange. I also thought about the superposition principle, but I don't know if it's a good idea because it would impliy using differential equations of different orders. So,if you could give me some tips ^^ Thanks in adavance","I found this exercise and I don't know how to solve the first question. Solve this differential equation : Deduce that for all : For the first part, I don't know how to start, it looks like a Leibniz formula for the product of functions and a function with all its derivatives equal to one .... which is strange. I also thought about the superposition principle, but I don't know if it's a good idea because it would impliy using differential equations of different orders. So,if you could give me some tips ^^ Thanks in adavance",\sum_{k=0}^{n} \binom{n}{k}y^{(k)}=0 j<n \sum_{k=0}^{n} \binom{n}{k} \binom{k}{j}(-1)^k=0 y,"['ordinary-differential-equations', 'derivatives']"
95,"How comes that integrating factor = $1/(x\,y)$",How comes that integrating factor =,"1/(x\,y)","I'm just curious how the integrating factor m(x,y) = $\frac{1}{x\,y}$ of $\frac{\mathrm{d}}{\mathrm{d}x}f(x,y) = x+y-\frac{x^2}{y}\,y'(x)$ was determined. It was just given. When I was introduced to this topic this week, my fist impression was that integrating factor just can depend on x or y. Because determinations like $m(x) = \exp(\int \alpha(x)\mathrm{d}x)$ whereas $\alpha(x) = \frac{\partial_y\partial_x(f(x,y))-\partial_x\partial_y(f(x,y))}{\partial_xf(x,y)}$ or such just seem to work this way. So loosely speaking: Are there other methods?","I'm just curious how the integrating factor m(x,y) = of was determined. It was just given. When I was introduced to this topic this week, my fist impression was that integrating factor just can depend on x or y. Because determinations like whereas or such just seem to work this way. So loosely speaking: Are there other methods?","\frac{1}{x\,y} \frac{\mathrm{d}}{\mathrm{d}x}f(x,y) = x+y-\frac{x^2}{y}\,y'(x) m(x) = \exp(\int \alpha(x)\mathrm{d}x) \alpha(x) = \frac{\partial_y\partial_x(f(x,y))-\partial_x\partial_y(f(x,y))}{\partial_xf(x,y)}",['ordinary-differential-equations']
96,Global stability vs Global asymptotic stability,Global stability vs Global asymptotic stability,,"After trying to get my head around this for an embarrassingly long time, I think I need some help... We defined local (lyapunov) stability and asymptotic stability the following way: An equilibrium $y^*$ of $\dot{y} = f(y)$ is called stable, if for each $\varepsilon$ -neighbourhood $B_\varepsilon (y^*)$ there exists a $\delta$ -neighbourhood $B_\delta(y^*)$ such that $$y_0 \in B_\delta(y^*) \implies y(t) \in B_\varepsilon (y^*) \forall t\geq t_0$$ asymptotically stable, if $y^*$ is stable and there exist a $\mu$ -neighbourhood $B_{\mu} (y^*)$ such that $$y_0 \in B_{\mu} (y^*) \implies \lim_{t \to \infty} y(t, y_0) = y^*$$ Ok so up to here both defintions make total sense to me. Now here comes my trouble: Later in the lecture we define ""global stability"" just with the following sentence: ""An equilibrium is called globally stable, if it is stable for (almost) all initial conditions, not just some which are close to the equilibrium $y^*$ ."" We don't introduce global asymptotic stability at all. But doesn't this definition of global stability imply $\lim_{t\to\infty} y(t, y_0) = y^* $ for all $y_0$ ? We also use this to prove global stability once. But wouldn't this be the definition of global asymptotic stability? What is the difference between the two? We go on to Lyapunov functions and mention there that under certain conditions you get global stability while if additionally $\dot V =0$ you get global asymptotic stability. This course isn't really about stability analysis so we didn't go into depth at all, or provided any proofs but I would really like to understand the difference between global stability and global asymptotic stability. I've read everything on google and found nothing, so I probably don't see something extremely trivial. Any help is appreciated!","After trying to get my head around this for an embarrassingly long time, I think I need some help... We defined local (lyapunov) stability and asymptotic stability the following way: An equilibrium of is called stable, if for each -neighbourhood there exists a -neighbourhood such that asymptotically stable, if is stable and there exist a -neighbourhood such that Ok so up to here both defintions make total sense to me. Now here comes my trouble: Later in the lecture we define ""global stability"" just with the following sentence: ""An equilibrium is called globally stable, if it is stable for (almost) all initial conditions, not just some which are close to the equilibrium ."" We don't introduce global asymptotic stability at all. But doesn't this definition of global stability imply for all ? We also use this to prove global stability once. But wouldn't this be the definition of global asymptotic stability? What is the difference between the two? We go on to Lyapunov functions and mention there that under certain conditions you get global stability while if additionally you get global asymptotic stability. This course isn't really about stability analysis so we didn't go into depth at all, or provided any proofs but I would really like to understand the difference between global stability and global asymptotic stability. I've read everything on google and found nothing, so I probably don't see something extremely trivial. Any help is appreciated!","y^* \dot{y} = f(y) \varepsilon B_\varepsilon (y^*) \delta B_\delta(y^*) y_0 \in B_\delta(y^*) \implies y(t) \in B_\varepsilon (y^*) \forall t\geq t_0 y^* \mu B_{\mu} (y^*) y_0 \in B_{\mu} (y^*) \implies \lim_{t \to \infty} y(t, y_0) = y^* y^* \lim_{t\to\infty} y(t, y_0) = y^*  y_0 \dot V =0","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'lyapunov-functions']"
97,"Why does the Wronskian satisfy $W(yy_1,\ldots,yy_n)=y^n W(y_1,\ldots,y_n)$?",Why does the Wronskian satisfy ?,"W(yy_1,\ldots,yy_n)=y^n W(y_1,\ldots,y_n)","The Wronskian of smooth functions $y_1,\ldots,y_n$ is defined by the determinant $$W(y_1,\ldots,y_n):=\det \left(y_i ^{(j)}\right).$$ It can be verified by a messy induction that the Wronskian satisfies the identity $$W(yy_1,\ldots,yy_n)=y^n W(y_1,\ldots,y_n)$$ for every smooth function $y$ . What is a conceptual proof of this fact? Why should we ""expect"" the result to be true? Note that it is not just multiplying each column by the same scalar. There are many cross-terms coming from the Leibnitz rule for derivatives. For example, $n=2$ is the assertion $$yy_1(yy_2)'-yy_2(yy_1)'=y^2(y_1y_2'-y_2y_1').$$ The Wronskian has intuitive meaning as the volume spanned by solutions to an ODE of order $n$ after converting it to a system of first-order equations. The identity is true for all functions that are differentiable $n-1$ times, but giving a proof just in the case that they solve an ODE will also be great.","The Wronskian of smooth functions is defined by the determinant It can be verified by a messy induction that the Wronskian satisfies the identity for every smooth function . What is a conceptual proof of this fact? Why should we ""expect"" the result to be true? Note that it is not just multiplying each column by the same scalar. There are many cross-terms coming from the Leibnitz rule for derivatives. For example, is the assertion The Wronskian has intuitive meaning as the volume spanned by solutions to an ODE of order after converting it to a system of first-order equations. The identity is true for all functions that are differentiable times, but giving a proof just in the case that they solve an ODE will also be great.","y_1,\ldots,y_n W(y_1,\ldots,y_n):=\det \left(y_i ^{(j)}\right). W(yy_1,\ldots,yy_n)=y^n W(y_1,\ldots,y_n) y n=2 yy_1(yy_2)'-yy_2(yy_1)'=y^2(y_1y_2'-y_2y_1'). n n-1","['ordinary-differential-equations', 'determinant', 'wronskian']"
98,How to solve the following initial value problem?,How to solve the following initial value problem?,,"Let $f:\mathbb{R}\to \mathbb{R}$ be a  Lipschitz function such that $f(x)=0$ if and only if $x=\pm n^2$ where $n\in \mathbb{N}$ . Consider the initial value problem $$y'(t)=f(y(t)),\ y(0)=y_0$$ Then which of the following are true? $y$ is a monotone function for all $y_0\in \mathbb{R}$ . For any $y_0\in \mathbb{R}$ , there exists $M_{y_0}>0$ such that $|y(t)|\leq M_{y_0}$ for all $t\in \mathbb{R}$ . there exists a $y_0\in \mathbb{R}$ , such that the corresponding solution $y$ is unbounded $\sup_{t, s\in \mathbb{R}}|y(t)-y(s)|=2n+1$ if $y_0\in (n^2, (n+1)^2), \ n\geq 1$ I was trying it by considering a linear function on subintervals $(n^2, (n+1)^2)$ and $0$ on end points. But not able to conclude anything. Plese help.","Let be a  Lipschitz function such that if and only if where . Consider the initial value problem Then which of the following are true? is a monotone function for all . For any , there exists such that for all . there exists a , such that the corresponding solution is unbounded if I was trying it by considering a linear function on subintervals and on end points. But not able to conclude anything. Plese help.","f:\mathbb{R}\to \mathbb{R} f(x)=0 x=\pm n^2 n\in \mathbb{N} y'(t)=f(y(t)),\ y(0)=y_0 y y_0\in \mathbb{R} y_0\in \mathbb{R} M_{y_0}>0 |y(t)|\leq M_{y_0} t\in \mathbb{R} y_0\in \mathbb{R} y \sup_{t, s\in \mathbb{R}}|y(t)-y(s)|=2n+1 y_0\in (n^2, (n+1)^2), \ n\geq 1 (n^2, (n+1)^2) 0","['ordinary-differential-equations', 'initial-value-problems']"
99,Why do we need both Divergence and Curl to define a vector field?,Why do we need both Divergence and Curl to define a vector field?,,"I was reading Classical Electrodynamics by J.D.Jacskon (section 1.5) where he said: Perhaps some readers know that a vector field can be specified almost completely if its divergence and curl are given everywhere in space with a comment (almost)-Up to the gradient of a scalar function that satisfies the Laplace equation. why is this so if we want to know $\vec{E}$ in 3-D space we really just want to find out is $E_x,E_y$ and $E_z$ (in cartesian coordinates) which are just 3 scalar functions. If we know the curl is (let's say) zero, that is $$\nabla \times \vec{E}=0$$ in cartesian coordinates that gives us three equations $$\frac{\partial E_z}{\partial y}-\frac{\partial E_y}{\partial z}=0$$ $$\frac{\partial E_x}{\partial z}-\frac{\partial E_z}{\partial x}=0$$ $$\frac{\partial E_y}{\partial x}-\frac{\partial E_x}{\partial y}=0$$ these are three equations for 3 unknown quantities, that should give us enough* information about these three functions, it obviously isn't so hence we need to know the divergence as well, my question is why? is there some theorem that proves this (I suspect there is $:)$ ) this is obviously a mathematical question although it is physics motivated so I hope it is ok that I post it here. (*up to maybe a constant because all the equations involve first derivatives, I can live with that)","I was reading Classical Electrodynamics by J.D.Jacskon (section 1.5) where he said: Perhaps some readers know that a vector field can be specified almost completely if its divergence and curl are given everywhere in space with a comment (almost)-Up to the gradient of a scalar function that satisfies the Laplace equation. why is this so if we want to know in 3-D space we really just want to find out is and (in cartesian coordinates) which are just 3 scalar functions. If we know the curl is (let's say) zero, that is in cartesian coordinates that gives us three equations these are three equations for 3 unknown quantities, that should give us enough* information about these three functions, it obviously isn't so hence we need to know the divergence as well, my question is why? is there some theorem that proves this (I suspect there is ) this is obviously a mathematical question although it is physics motivated so I hope it is ok that I post it here. (*up to maybe a constant because all the equations involve first derivatives, I can live with that)","\vec{E} E_x,E_y E_z \nabla \times \vec{E}=0 \frac{\partial E_z}{\partial y}-\frac{\partial E_y}{\partial z}=0 \frac{\partial E_x}{\partial z}-\frac{\partial E_z}{\partial x}=0 \frac{\partial E_y}{\partial x}-\frac{\partial E_x}{\partial y}=0 :)","['ordinary-differential-equations', 'vector-fields', 'electromagnetism']"
