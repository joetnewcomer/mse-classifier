,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Tractrix & limits,Tractrix & limits,,"I'm in calculus 1 and I'm having quite a bit of trouble with this problem. Any advice as to how to obtain a solution or anything would be much appreciated. Thanks! To determine the points on the curve, we need to calculate the intersection point of adjacent threads (the red lines), then determine the limit of that intersection point as the end points of the threads approach zero. Tractrix 1 This is the situation shown in the example above: a given thread runs from the point with coördinates $(r,0)$ to the point with coördinates $(0,1-r)$, $0 ≤ r ≤ 1$.  In other words, the sum of the $x$ coördinate on the $x$-axis at the lower end of the thread and the $y$-coördinate on the $y$-axis at the upper end of the thread is one. To determine a point on the tangent curve, consider two adjacent threads: one runs from $(r,0)$ to $(0,1-r)$; the other runs from $(r+h,0)$ to $(0,1-(r+h))$.  These lines intersect at a point $(x_{r+h},y_{r+h})$. The point on the curve is $(x_r,y_r)=\lim\limits_{h\to 0}  (x_{r+h},y_{r+h})$. Write the equation for the line through the points $(r,0)$ and $(0,1-r)$. Write the equation for the line through the points $(r+h,0)$ and $(0,1-(r+h))$. Calculate the point $(x_{r+h},y_{r+h})$ where these two lines intersect. Calculate $\lim\limits_{h\to 0}  (x_{r+h},y_{r+h})$. Use the point just determined to write the equation for the curve that passes through all such points.  (Note: the equation should be symmetric in $x$ and $y$.)","I'm in calculus 1 and I'm having quite a bit of trouble with this problem. Any advice as to how to obtain a solution or anything would be much appreciated. Thanks! To determine the points on the curve, we need to calculate the intersection point of adjacent threads (the red lines), then determine the limit of that intersection point as the end points of the threads approach zero. Tractrix 1 This is the situation shown in the example above: a given thread runs from the point with coördinates $(r,0)$ to the point with coördinates $(0,1-r)$, $0 ≤ r ≤ 1$.  In other words, the sum of the $x$ coördinate on the $x$-axis at the lower end of the thread and the $y$-coördinate on the $y$-axis at the upper end of the thread is one. To determine a point on the tangent curve, consider two adjacent threads: one runs from $(r,0)$ to $(0,1-r)$; the other runs from $(r+h,0)$ to $(0,1-(r+h))$.  These lines intersect at a point $(x_{r+h},y_{r+h})$. The point on the curve is $(x_r,y_r)=\lim\limits_{h\to 0}  (x_{r+h},y_{r+h})$. Write the equation for the line through the points $(r,0)$ and $(0,1-r)$. Write the equation for the line through the points $(r+h,0)$ and $(0,1-(r+h))$. Calculate the point $(x_{r+h},y_{r+h})$ where these two lines intersect. Calculate $\lim\limits_{h\to 0}  (x_{r+h},y_{r+h})$. Use the point just determined to write the equation for the curve that passes through all such points.  (Note: the equation should be symmetric in $x$ and $y$.)",,"['calculus', 'limits', 'conic-sections', 'plane-curves', 'envelope']"
1,How does one evaluate $\lim\limits _{n\to \infty }\left(\prod_{x=2}^{n}\frac{x^3-1}{x^3+1}\right)$?,How does one evaluate ?,\lim\limits _{n\to \infty }\left(\prod_{x=2}^{n}\frac{x^3-1}{x^3+1}\right),I tried this form:  $$\lim_{n\to+\infty}\left(\prod_{x=2}^{n}\frac{\left(x-1\right)\left(x^2+x+1\right)}{\left(x+1\right)\left(x^2-x+1\right)}\right)$$ but it doesn't ring any bell.,I tried this form:  $$\lim_{n\to+\infty}\left(\prod_{x=2}^{n}\frac{\left(x-1\right)\left(x^2+x+1\right)}{\left(x+1\right)\left(x^2-x+1\right)}\right)$$ but it doesn't ring any bell.,,"['calculus', 'sequences-and-series', 'limits']"
2,"$\lim_{x\to 1; x\in (0,\infty)-\{1\}} \frac{x^{\alpha} -1}{x-1}, \alpha\in\mathbb{R}$",,"\lim_{x\to 1; x\in (0,\infty)-\{1\}} \frac{x^{\alpha} -1}{x-1}, \alpha\in\mathbb{R}","I'm trying to evaluate this limit (taken from T.Tao's Analysis 1 book) without logarithms and without knowing that the function $f:(0,\infty)\to\mathbb{R}, f(x):=x^{\alpha}, \alpha\in\mathbb{R}$ is differentiable on $(0,\infty)$ (infact the book asks to use this limit to show that $f$ is differentiable with limit $f'(x)=\alpha x^{\alpha -1}$. Now, I've tried to use the fact that $x^{\alpha}, \alpha\in\mathbb{R}$ is defined as $x^{\alpha}:=\lim_{n\to\infty} x^{q_n}$ where $(q_n)_{n=1}^\infty$ is any sequence of rational numbers converging to $\alpha$ (hence a bounded sequence) and the fact (that I've already proved) that $\lim_{x\to 1; x\in (0,\infty)-\{1\}} \frac{x^q-1}{x-1}=q\ \forall q\in\mathbb{Q}$ to use the squeeze theorem somehow but I haven't gotten anywhere so far. Any hints? Best regards, lorenzo.","I'm trying to evaluate this limit (taken from T.Tao's Analysis 1 book) without logarithms and without knowing that the function $f:(0,\infty)\to\mathbb{R}, f(x):=x^{\alpha}, \alpha\in\mathbb{R}$ is differentiable on $(0,\infty)$ (infact the book asks to use this limit to show that $f$ is differentiable with limit $f'(x)=\alpha x^{\alpha -1}$. Now, I've tried to use the fact that $x^{\alpha}, \alpha\in\mathbb{R}$ is defined as $x^{\alpha}:=\lim_{n\to\infty} x^{q_n}$ where $(q_n)_{n=1}^\infty$ is any sequence of rational numbers converging to $\alpha$ (hence a bounded sequence) and the fact (that I've already proved) that $\lim_{x\to 1; x\in (0,\infty)-\{1\}} \frac{x^q-1}{x-1}=q\ \forall q\in\mathbb{Q}$ to use the squeeze theorem somehow but I haven't gotten anywhere so far. Any hints? Best regards, lorenzo.",,"['real-analysis', 'limits', 'derivatives']"
3,"With the condition $\lim_{x\to \infty}(f(x+a)-f(x))=0$, how to construct $h(x)$?","With the condition , how to construct ?",\lim_{x\to \infty}(f(x+a)-f(x))=0 h(x),"Assume $f(x)\in C[0,+\infty)$，and for all $a\geqslant 0$, we have \begin{align*} \lim_{x\to \infty}(f(x+a)-f(x))=0 \tag{*}. \end{align*} Prove that there exists $g(x)\in C[0,+\infty)$ and $h(x)\in C^1[0,+\infty)$ such that $f(x)=g(x)+h(x)$, and such that they satisfy \begin{align*} \lim_{x\to \infty}g(x)=0,~~\lim_{x\to \infty}h'(x)=0. \end{align*} My thought is let $h(x)=\frac1 a\int_x^{x+a}f(t)\,dt$, then it is easy to see $\lim_{x\to \infty}h'(x)=0$, but I can't explain that $\lim_{x\to \infty}g(x)=\lim_{x\to \infty}f(x)-h(x)=0$.  It seems we should try proving  $\lim_{x\to +\infty}f(x)$ exists by using the condition of (*), but I'm not sure whether it's true or false.","Assume $f(x)\in C[0,+\infty)$，and for all $a\geqslant 0$, we have \begin{align*} \lim_{x\to \infty}(f(x+a)-f(x))=0 \tag{*}. \end{align*} Prove that there exists $g(x)\in C[0,+\infty)$ and $h(x)\in C^1[0,+\infty)$ such that $f(x)=g(x)+h(x)$, and such that they satisfy \begin{align*} \lim_{x\to \infty}g(x)=0,~~\lim_{x\to \infty}h'(x)=0. \end{align*} My thought is let $h(x)=\frac1 a\int_x^{x+a}f(t)\,dt$, then it is easy to see $\lim_{x\to \infty}h'(x)=0$, but I can't explain that $\lim_{x\to \infty}g(x)=\lim_{x\to \infty}f(x)-h(x)=0$.  It seems we should try proving  $\lim_{x\to +\infty}f(x)$ exists by using the condition of (*), but I'm not sure whether it's true or false.",,"['limits', 'derivatives']"
4,Show that $\sum\limits_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i}=2n+\frac23+o(1)$,Show that,\sum\limits_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i}=2n+\frac23+o(1),"When modeling the average queue length of an M/D/1/K queue, we encountered the expression $$A_n=\sum_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i}$$ Empirically, $$A_n\approx2n + \frac{2}{3}$$ for large $n$, and at least for $n > 5$. For instance, $$A_6=12.66666714138\qquad A_{10}=20.66666666648$$ These values are too neat to be a mere coincidence, but we are not able to explain them. Mathematically, the result to be proven is $$\lim_{n\to\infty}\ (A_n-2n)=\frac23$$ that is, $$\lim_{n\to\infty} \left( \sum_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i} - 2n\right) = \frac{2}{3}$$ Any pointers on how to prove this would be greatly appreciated.","When modeling the average queue length of an M/D/1/K queue, we encountered the expression $$A_n=\sum_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i}$$ Empirically, $$A_n\approx2n + \frac{2}{3}$$ for large $n$, and at least for $n > 5$. For instance, $$A_6=12.66666714138\qquad A_{10}=20.66666666648$$ These values are too neat to be a mere coincidence, but we are not able to explain them. Mathematically, the result to be proven is $$\lim_{n\to\infty}\ (A_n-2n)=\frac23$$ that is, $$\lim_{n\to\infty} \left( \sum_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i} - 2n\right) = \frac{2}{3}$$ Any pointers on how to prove this would be greatly appreciated.",,"['real-analysis', 'limits', 'summation']"
5,Why are we allowed to put conditions on $\epsilon$ in this limit proof?,Why are we allowed to put conditions on  in this limit proof?,\epsilon,"Let $$X_n=\begin{cases} 1/n, &\text{ with prob } 1-1/n\\ n^2, &\text{ with prob } 1/n.\end{cases}.$$ $$Y_{n}=\begin{cases} 0, &\text{ with prob } 1-1/n\\ n, &\text{ with prob } 1/n.\end{cases}.$$ We want to show that $X_n \rightarrow 0$ in probability, which means showing that $\lim_{n\rightarrow\infty}P(|X_n-0|>\epsilon)=0$ for all $\epsilon>0$. And similarly for showing $Y_n \rightarrow 0$ in probability. The solutions I've seen to these questions go as follows: Whenever $\epsilon>1/n$, we have $0\leq P(|X_n-0|>\epsilon)=P(X_n>1/n)=P(X_n=n^2)=1/n$. The result follows by letting $n\rightarrow \infty$ and using the Sandwich theorem. And for the second one ( https://youtu.be/x4q6H6lxFFE?t=594 ): Whenever $\epsilon<n$, we have $0\leq P(|Y_n-0|\geq \epsilon)=1/n$. The result follows by letting $n\rightarrow \infty$ and using the Sandwich theorem. Simple enough, but why are we allowed to put conditions on $\epsilon$? I thought $\epsilon$ was just any positive number. Also even though the examples are really similar, in one case it's $\epsilon>1/n$ and in the other it's $\epsilon<n$. Is that just because one definition allows for $> \epsilon$ and the other is $\geq\epsilon$? Edit: On second glance I'm confused as to why the second proof has $\epsilon<n$? Shouldn't it just be $\epsilon>0$ as then $P(|Y_n|>\epsilon)=P(Y_n>0)=P(Y_n=n)=1/n$?","Let $$X_n=\begin{cases} 1/n, &\text{ with prob } 1-1/n\\ n^2, &\text{ with prob } 1/n.\end{cases}.$$ $$Y_{n}=\begin{cases} 0, &\text{ with prob } 1-1/n\\ n, &\text{ with prob } 1/n.\end{cases}.$$ We want to show that $X_n \rightarrow 0$ in probability, which means showing that $\lim_{n\rightarrow\infty}P(|X_n-0|>\epsilon)=0$ for all $\epsilon>0$. And similarly for showing $Y_n \rightarrow 0$ in probability. The solutions I've seen to these questions go as follows: Whenever $\epsilon>1/n$, we have $0\leq P(|X_n-0|>\epsilon)=P(X_n>1/n)=P(X_n=n^2)=1/n$. The result follows by letting $n\rightarrow \infty$ and using the Sandwich theorem. And for the second one ( https://youtu.be/x4q6H6lxFFE?t=594 ): Whenever $\epsilon<n$, we have $0\leq P(|Y_n-0|\geq \epsilon)=1/n$. The result follows by letting $n\rightarrow \infty$ and using the Sandwich theorem. Simple enough, but why are we allowed to put conditions on $\epsilon$? I thought $\epsilon$ was just any positive number. Also even though the examples are really similar, in one case it's $\epsilon>1/n$ and in the other it's $\epsilon<n$. Is that just because one definition allows for $> \epsilon$ and the other is $\geq\epsilon$? Edit: On second glance I'm confused as to why the second proof has $\epsilon<n$? Shouldn't it just be $\epsilon>0$ as then $P(|Y_n|>\epsilon)=P(Y_n>0)=P(Y_n=n)=1/n$?",,"['probability', 'limits', 'probability-limit-theorems']"
6,"If the $n^{th}$ partial sum of a series $\sum a_n$ is $S_n = \frac{n-1}{n+1}$, find $a_n$ and the sum.","If the  partial sum of a series  is , find  and the sum.",n^{th} \sum a_n S_n = \frac{n-1}{n+1} a_n,"If the $n^{th}$ partial sum of a series $\sum a_n$ is $S_n = \dfrac{n-1}{n+1}$, find $a_n$ and the sum. By definition,  the sum of the series is the $\lim n\to\infty$ of it's $n^{th}$ partial sum. $$\text{ Sum = } \lim_{n\to\infty} = \dfrac{n-1}{n+1} = 1$$ I am asked to find $a_n$. How do I do this? What is the procedure?","If the $n^{th}$ partial sum of a series $\sum a_n$ is $S_n = \dfrac{n-1}{n+1}$, find $a_n$ and the sum. By definition,  the sum of the series is the $\lim n\to\infty$ of it's $n^{th}$ partial sum. $$\text{ Sum = } \lim_{n\to\infty} = \dfrac{n-1}{n+1} = 1$$ I am asked to find $a_n$. How do I do this? What is the procedure?",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
7,How to evaluate the following limits?,How to evaluate the following limits?,,"I was reading a proof on the evaluation of $\int_0^\infty e^{-x^2}\ dx$ without advanced techniques and stumbled upon two limits that I can't seem to crack: $$\lim_{m\to\infty}\left(\sqrt{m}\cdot\prod_{n=1}^m\frac{2n}{2n+1}\right)=\frac{\sqrt{\pi}}2$$ $$\lim_{m\to\infty}\left(\sqrt{m}\cdot\prod_{n=2}^m\frac{2n-3}{2n-2}\right)=\frac1{\sqrt{\pi}}$$ The proof does not go into detail on how these limits were obtained, and since I wanted to understand it completely, I thought this would be the best place to ask. I have not been exposed to infinite products (only summations) and therefore I do not know which rules to apply (I feel as if they are quite similar?). In both cases, I see that an indeterminate form $0\cdot\infty$ presents its self, therefore I am guessing Hospital would be a nice approach? Any help  is appreciated! Also, my calculus book does not tackle infinite products, any suggestions on books that might give me a general outlook on the subject?","I was reading a proof on the evaluation of $\int_0^\infty e^{-x^2}\ dx$ without advanced techniques and stumbled upon two limits that I can't seem to crack: $$\lim_{m\to\infty}\left(\sqrt{m}\cdot\prod_{n=1}^m\frac{2n}{2n+1}\right)=\frac{\sqrt{\pi}}2$$ $$\lim_{m\to\infty}\left(\sqrt{m}\cdot\prod_{n=2}^m\frac{2n-3}{2n-2}\right)=\frac1{\sqrt{\pi}}$$ The proof does not go into detail on how these limits were obtained, and since I wanted to understand it completely, I thought this would be the best place to ask. I have not been exposed to infinite products (only summations) and therefore I do not know which rules to apply (I feel as if they are quite similar?). In both cases, I see that an indeterminate form $0\cdot\infty$ presents its self, therefore I am guessing Hospital would be a nice approach? Any help  is appreciated! Also, my calculus book does not tackle infinite products, any suggestions on books that might give me a general outlook on the subject?",,['limits']
8,Limits with L'Hopital - guessing and confirm: $\lim _{x \to 0^+} (1+x)^{\large\frac1{\sin x}}$,Limits with L'Hopital - guessing and confirm:,\lim _{x \to 0^+} (1+x)^{\large\frac1{\sin x}},With L'Hopital I have$$\lim _{x \to 0^+} (1+x)^{\large\frac1{\sin x}}=\lim _{x \to 0_+} e^{\ln{(1+x)^\frac{1}{\sin x}}}=\lim _{x \to 0_+} e^{\ln(1+x)}\cdot\frac{1}{\sin x}$$ $$=\lim _{x \to 0_+} \frac{\ln(1+x)}{\sin x}=\lim _{x \to 0_+}\frac{\frac{1}{1+x}}{\cos x}=1$$ I just want someone to confirm it is the limit and I want to ask too if there is any method that I can guess the limit without actually calculating it? Thanks in advance!,With L'Hopital I have$$\lim _{x \to 0^+} (1+x)^{\large\frac1{\sin x}}=\lim _{x \to 0_+} e^{\ln{(1+x)^\frac{1}{\sin x}}}=\lim _{x \to 0_+} e^{\ln(1+x)}\cdot\frac{1}{\sin x}$$ $$=\lim _{x \to 0_+} \frac{\ln(1+x)}{\sin x}=\lim _{x \to 0_+}\frac{\frac{1}{1+x}}{\cos x}=1$$ I just want someone to confirm it is the limit and I want to ask too if there is any method that I can guess the limit without actually calculating it? Thanks in advance!,,"['calculus', 'limits']"
9,prove that $\lim_{x \to 1}\frac{x^{2}-x+1}{x+1}=\frac{1}{2}$,prove that,\lim_{x \to 1}\frac{x^{2}-x+1}{x+1}=\frac{1}{2},Please check my proof Let $\epsilon >0$ and $\delta >0$ $$0<x<\delta \rightarrow \frac{x^{2}-x+1}{x+1}-\frac{1}{2}<\epsilon $$ $$\frac{2x^{2}-2x+2-x+1}{2x+2}<\epsilon $$ $$\frac{2x^{2}-3x+1}{2x+2}<\epsilon $$ since   $\frac{2x^2-3x+1}{2x+2} <\frac{2x^{2}-3x+2}{2}$ then $\frac{2x^{2}-3x+1}{2}< \epsilon $ $2x^{2}-3x+1<2\epsilon $ choose $2\epsilon =\delta $ then $\frac{2x^{2}-3x+1}{2}<\frac{2\epsilon }{2}=\epsilon $ by transitivity of inequality $\frac{2x^{2}-3x+1}{2x+2}<\epsilon $,Please check my proof Let $\epsilon >0$ and $\delta >0$ $$0<x<\delta \rightarrow \frac{x^{2}-x+1}{x+1}-\frac{1}{2}<\epsilon $$ $$\frac{2x^{2}-2x+2-x+1}{2x+2}<\epsilon $$ $$\frac{2x^{2}-3x+1}{2x+2}<\epsilon $$ since   $\frac{2x^2-3x+1}{2x+2} <\frac{2x^{2}-3x+2}{2}$ then $\frac{2x^{2}-3x+1}{2}< \epsilon $ $2x^{2}-3x+1<2\epsilon $ choose $2\epsilon =\delta $ then $\frac{2x^{2}-3x+1}{2}<\frac{2\epsilon }{2}=\epsilon $ by transitivity of inequality $\frac{2x^{2}-3x+1}{2x+2}<\epsilon $,,"['real-analysis', 'limits', 'proof-verification']"
10,Limit problem with sequences [closed],Limit problem with sequences [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let us consider the sequence $(a_n)_{n \ge 1}$ as follows: $$a_n= \log_{2n}(2n+1)\log_{2n+2}(2n+3)\log_{2n+4}(2n+5) \dots\log_{4n^2}(4n^2+1).$$ Compute the limit of this sequence.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let us consider the sequence $(a_n)_{n \ge 1}$ as follows: $$a_n= \log_{2n}(2n+1)\log_{2n+2}(2n+3)\log_{2n+4}(2n+5) \dots\log_{4n^2}(4n^2+1).$$ Compute the limit of this sequence.",,"['sequences-and-series', 'limits']"
11,Solve for $k$ such that $f$ is a real valued continuous function,Solve for  such that  is a real valued continuous function,k f,"Find a non-zero value for the constant $k$ such that $f$ defined as below is continuous at $x = 0$. $$f(x) =  \begin{cases} \frac{\tan(kx)}{x}, \hspace{0.3cm}x< 0 \\ 3x + 2k^2, \hspace{0.3cm}x\geq0 \end{cases}$$ My attempt: $$\lim_{x\rightarrow0^{-}}\frac{\tan(kx)}{ x} = \lim_{x \rightarrow 0^{-}} \frac{\sin(kx)}{ x\cos(kx)} = \lim_{x \rightarrow 0^{-}} \frac{k\sin(kx)}{(kx)}\frac{1}{\cos(kx)} = k\lim_{x \rightarrow 0^{-}} \frac{1}{\cos(kx)}  = k$$ $$\lim_{x \rightarrow 0^{+}}3x+2k^2 = 2k^2$$ For continuity we must have that the limit on the right must be equal to the limit on the left, i.e, $k = 2k^2$, so $$k(2k-1) = 0$$ Therefore, a non-zero value for constant $k$ such that $f$ is continuous on $x=0$ is $k = 1/2$. I am not fully confident in my solution. Could someone tell me if i went down the right path and why? Or maybe tell me I am entirely wrong and lend a hand?","Find a non-zero value for the constant $k$ such that $f$ defined as below is continuous at $x = 0$. $$f(x) =  \begin{cases} \frac{\tan(kx)}{x}, \hspace{0.3cm}x< 0 \\ 3x + 2k^2, \hspace{0.3cm}x\geq0 \end{cases}$$ My attempt: $$\lim_{x\rightarrow0^{-}}\frac{\tan(kx)}{ x} = \lim_{x \rightarrow 0^{-}} \frac{\sin(kx)}{ x\cos(kx)} = \lim_{x \rightarrow 0^{-}} \frac{k\sin(kx)}{(kx)}\frac{1}{\cos(kx)} = k\lim_{x \rightarrow 0^{-}} \frac{1}{\cos(kx)}  = k$$ $$\lim_{x \rightarrow 0^{+}}3x+2k^2 = 2k^2$$ For continuity we must have that the limit on the right must be equal to the limit on the left, i.e, $k = 2k^2$, so $$k(2k-1) = 0$$ Therefore, a non-zero value for constant $k$ such that $f$ is continuous on $x=0$ is $k = 1/2$. I am not fully confident in my solution. Could someone tell me if i went down the right path and why? Or maybe tell me I am entirely wrong and lend a hand?",,"['calculus', 'limits', 'continuity']"
12,Limit of $(1/n)^{(1/n!)}$ as $n \to \infty$,Limit of  as,(1/n)^{(1/n!)} n \to \infty,A computer algebra system told me that \begin{equation} \lim_{n \to \infty} \left( \frac{1}{n} \right)^{1/n!} = 1 \end{equation} How can I show this? I tried applying the exponential and logarithm to see that this is equal to \begin{equation} \exp \lim_{n \to \infty} \left( \frac{1}{n!} \log \frac{1}{n} \right) \end{equation} But I'm not sure how to proceed.,A computer algebra system told me that \begin{equation} \lim_{n \to \infty} \left( \frac{1}{n} \right)^{1/n!} = 1 \end{equation} How can I show this? I tried applying the exponential and logarithm to see that this is equal to \begin{equation} \exp \lim_{n \to \infty} \left( \frac{1}{n!} \log \frac{1}{n} \right) \end{equation} But I'm not sure how to proceed.,,['limits']
13,How to evaluate $\lim_{n\rightarrow \infty} \sqrt{n}(\frac{e^{rt/n+\sigma\sqrt{t/n}}-1}{e^{2\sigma\sqrt{t/n}}-1}-\frac{1}{2})$?,How to evaluate ?,\lim_{n\rightarrow \infty} \sqrt{n}(\frac{e^{rt/n+\sigma\sqrt{t/n}}-1}{e^{2\sigma\sqrt{t/n}}-1}-\frac{1}{2}),"I know it evaluates to $\frac{rt-\frac{1}{2}\sigma^2t}{2\sigma\sqrt{t}}$ but how to get there is the problem. Using L'Hôpital you find $$\lim_{n\rightarrow\infty}\frac{e^{rt/n+\sigma\sqrt{t/n}}-1}{e^{2\sigma\sqrt{t/n}}-1} = \frac{1}{2}$$But L'Hôpital doesn't work on the whole thing. The only limit calculator that could figure it out was wolfram but that couldn't give me the steps to get there. using Taylor series expansion gives me:$$\lim_{n\to\infty}\sqrt{n}(\frac{\sum_{k=0}^{\infty}(rt/n+\sigma\sqrt{t/n})-1}{\sum_{k=0}^{\infty}(2\sigma\sqrt{t/n})^k-1}-\frac{1}{2})$$But I fail to see how to go further from here, any help would be appreciated","I know it evaluates to $\frac{rt-\frac{1}{2}\sigma^2t}{2\sigma\sqrt{t}}$ but how to get there is the problem. Using L'Hôpital you find $$\lim_{n\rightarrow\infty}\frac{e^{rt/n+\sigma\sqrt{t/n}}-1}{e^{2\sigma\sqrt{t/n}}-1} = \frac{1}{2}$$But L'Hôpital doesn't work on the whole thing. The only limit calculator that could figure it out was wolfram but that couldn't give me the steps to get there. using Taylor series expansion gives me:$$\lim_{n\to\infty}\sqrt{n}(\frac{\sum_{k=0}^{\infty}(rt/n+\sigma\sqrt{t/n})-1}{\sum_{k=0}^{\infty}(2\sigma\sqrt{t/n})^k-1}-\frac{1}{2})$$But I fail to see how to go further from here, any help would be appreciated",,"['limits', 'taylor-expansion', 'limits-without-lhopital']"
14,"$a_n := 1/\sqrt{n} $. Show that the sequence is bounded, monotone decreasing...",". Show that the sequence is bounded, monotone decreasing...",a_n := 1/\sqrt{n} ,"I'm given the following: $(a_n)_{n\in\mathbb{N}} $ is a real sequence defined as $a_n := 1/\sqrt{n} $ for $n \in \mathbb{N}$. i) Show that $(a_n)_{n\in\mathbb{N}} $ is bounded from above and below, and give the explicit boundaries. ii) Show that, $(a_n)_{n\in\mathbb{N}} $, is monotonely decreasing. iii) Give $\lim_{n\to\infty} a_n  $ Now I understand all of this, Im just not sure how to write a good mathematical proof for it. Basically my idea is like this... Since n can only be a positive integer, when n = 1, it will basically be  $1/\sqrt{1} = 1$. When n > 1, then the fraction will become smaller, and thats why the above boundary is 1. As n approaches inifinity, then the fraction $1/\sqrt{n}$ will approach 0. So the lower boundary is 0. The function is monotone decreasing since ($1/\sqrt{n}) \geq (1/\sqrt{n+1})$. And $\lim_{n\to\infty} a_n = 0 $ This is all clear to me, but I just feel like if I write it like this, it is not really a mathematical proof and I am not sure how I can formulate this as a proper valid proof.","I'm given the following: $(a_n)_{n\in\mathbb{N}} $ is a real sequence defined as $a_n := 1/\sqrt{n} $ for $n \in \mathbb{N}$. i) Show that $(a_n)_{n\in\mathbb{N}} $ is bounded from above and below, and give the explicit boundaries. ii) Show that, $(a_n)_{n\in\mathbb{N}} $, is monotonely decreasing. iii) Give $\lim_{n\to\infty} a_n  $ Now I understand all of this, Im just not sure how to write a good mathematical proof for it. Basically my idea is like this... Since n can only be a positive integer, when n = 1, it will basically be  $1/\sqrt{1} = 1$. When n > 1, then the fraction will become smaller, and thats why the above boundary is 1. As n approaches inifinity, then the fraction $1/\sqrt{n}$ will approach 0. So the lower boundary is 0. The function is monotone decreasing since ($1/\sqrt{n}) \geq (1/\sqrt{n+1})$. And $\lim_{n\to\infty} a_n = 0 $ This is all clear to me, but I just feel like if I write it like this, it is not really a mathematical proof and I am not sure how I can formulate this as a proper valid proof.",,"['calculus', 'sequences-and-series', 'limits', 'proof-writing']"
15,Series shown up in Quantum Mechanics,Series shown up in Quantum Mechanics,,"Here is the series, by some concepts in quantum mechanics, it should be 1, however, I don't know how to prove it. Could someone show me the proof? Thanks. $$\lim_{k\to \infty}\sum_{n=1}^{k} \frac{4}{n^2\pi^2}(1-\cos\frac{n\pi}{2})^2 $$","Here is the series, by some concepts in quantum mechanics, it should be 1, however, I don't know how to prove it. Could someone show me the proof? Thanks. $$\lim_{k\to \infty}\sum_{n=1}^{k} \frac{4}{n^2\pi^2}(1-\cos\frac{n\pi}{2})^2 $$",,"['calculus', 'sequences-and-series', 'limits']"
16,Definitions 4.32 and 4.33 and Theorem 4.34 in Baby Rudin: Are these proofs the limit statements based on these definitions correct?,Definitions 4.32 and 4.33 and Theorem 4.34 in Baby Rudin: Are these proofs the limit statements based on these definitions correct?,,"Here's Theorem 4.34 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $f$ and $g$ be defined on $E \subset \mathbb{R}$ . Suppose $$ f(t) \to A, \ \ \ g(t) \to B, \ \mbox{ as } \ t \to x.$$ Then (a) $f(t) \to A^\prime$ implies $A^\prime = A$ . (b) $\left( f+g \right)(t) \to A+B$ . (c) $\left( fg \right)(t) \to AB$ . (d) $\left( f/g \right)(t) \to A/B$ . provided the right members of (b), (c), and (d) are defined. Note that $\infty - \infty$ , $0 \cdot \infty$ , $\infty / \infty$ , $A/0$ are not defined. Now here is Definition 4.33 in Rudin: Let $f$ be a real function defined on $E \subset \mathbb{R}$ . We say that $$ f(t) \to A \ \mbox{ as } \ t \to x,$$ where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$ , $t \neq x$ . And, finally here is Definition 4.32: For any real $c$ , the set of real numbers $x$ such that $x > c$ is called a neighborhood of $+\infty$ and is written $(c, +\infty)$ . Similarly, the set $(-\infty, c)$ is a neighborhood of $-\infty$ . How to prove Theorem 4.34 using Definitions 4.33 and 4.32? My effort: Theorem 4.34(a): If $f(t) \to A$ and $f(t) \to A^\prime$ as $t \to x$ , then, for every neighborhood $U$ of $A$ , we can find a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$ , $t \neq x$ , and for every neighborhood $U^\prime$ of $A^\prime$ , we can find a neighborhood $V^\prime$ of $x$ such that $V^\prime \cap E$ is not empty, and such that $f(t) \in U^\prime$ for all $t \in V^\prime \cap E$ , $t \neq x$ . Now if $A \neq A^\prime$ , then we can find neighborhoods $U$ and $U^\prime$ of $A$ and $A^\prime$ , respectively, such that $U \cap U^\prime$ is empty. Now $V \cap V^\prime$ is not empty and is a neighborhood of $x$ . What next? Theorem 4.34(b), (c), and (d): If $f(t) \to A$ and $g(t) \to B$ as $t \to x$ , then, for every neighborhood $U$ of $A$ and for every neighborhood $V$ of $B$ , we can find neighborhoods $W_1$ and $W_2$ , respectively, of $x$ such that $W_1 \cap E$ and $W_2 \cap E$ are not empty, and such that $f(t) \in U$ for all $t \in W_1 \cap E$ , $t \neq x$ , and such that $g(t) \in V$ for all $t \in W_2 \cap E$ , $t \neq x$ . What next?","Here's Theorem 4.34 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let and be defined on . Suppose Then (a) implies . (b) . (c) . (d) . provided the right members of (b), (c), and (d) are defined. Note that , , , are not defined. Now here is Definition 4.33 in Rudin: Let be a real function defined on . We say that where and are in the extended real number system, if for every neighborhood of there is a neighborhood of such that is not empty, and such that for all , . And, finally here is Definition 4.32: For any real , the set of real numbers such that is called a neighborhood of and is written . Similarly, the set is a neighborhood of . How to prove Theorem 4.34 using Definitions 4.33 and 4.32? My effort: Theorem 4.34(a): If and as , then, for every neighborhood of , we can find a neighborhood of such that is not empty, and such that for all , , and for every neighborhood of , we can find a neighborhood of such that is not empty, and such that for all , . Now if , then we can find neighborhoods and of and , respectively, such that is empty. Now is not empty and is a neighborhood of . What next? Theorem 4.34(b), (c), and (d): If and as , then, for every neighborhood of and for every neighborhood of , we can find neighborhoods and , respectively, of such that and are not empty, and such that for all , , and such that for all , . What next?","f g E \subset \mathbb{R}  f(t) \to A, \ \ \ g(t) \to B, \ \mbox{ as } \ t \to x. f(t) \to A^\prime A^\prime = A \left( f+g \right)(t) \to A+B \left( fg \right)(t) \to AB \left( f/g \right)(t) \to A/B \infty - \infty 0 \cdot \infty \infty / \infty A/0 f E \subset \mathbb{R}  f(t) \to A \ \mbox{ as } \ t \to x, A x U A V x V \cap E f(t) \in U t \in V \cap E t \neq x c x x > c +\infty (c, +\infty) (-\infty, c) -\infty f(t) \to A f(t) \to A^\prime t \to x U A V x V \cap E f(t) \in U t \in V \cap E t \neq x U^\prime A^\prime V^\prime x V^\prime \cap E f(t) \in U^\prime t \in V^\prime \cap E t \neq x A \neq A^\prime U U^\prime A A^\prime U \cap U^\prime V \cap V^\prime x f(t) \to A g(t) \to B t \to x U A V B W_1 W_2 x W_1 \cap E W_2 \cap E f(t) \in U t \in W_1 \cap E t \neq x g(t) \in V t \in W_2 \cap E t \neq x","['calculus', 'real-analysis', 'analysis', 'limits']"
17,The $\frac{\sin x}x$ limit and the floor function,The  limit and the floor function,\frac{\sin x}x,"$$\lim_{x\to0}\left\lfloor\frac{\sin x}x\right\rfloor=\,?$$ I think it should be 1, but in my class notes it's given as 0. I don't understand, because the limit without the floor brackets is 1, yet $\lfloor1\rfloor=1$.","$$\lim_{x\to0}\left\lfloor\frac{\sin x}x\right\rfloor=\,?$$ I think it should be 1, but in my class notes it's given as 0. I don't understand, because the limit without the floor brackets is 1, yet $\lfloor1\rfloor=1$.",,"['limits', 'limits-without-lhopital']"
18,Prove that $\lim_{x\to \frac{\pi}{2}}\sin{\frac{x}{2}} \cdot [\sin{x}] = 0$,Prove that,\lim_{x\to \frac{\pi}{2}}\sin{\frac{x}{2}} \cdot [\sin{x}] = 0,"How can I prove: $$\lim_{x\to \frac{\pi}{2}}\sin{\frac{x}{2}} \cdot [\sin{x}] = 0$$ I know that for every $0<x<\pi$, where $x\neq \pi/2$,  $[\sin{x}]=0$ How can I go from here?","How can I prove: $$\lim_{x\to \frac{\pi}{2}}\sin{\frac{x}{2}} \cdot [\sin{x}] = 0$$ I know that for every $0<x<\pi$, where $x\neq \pi/2$,  $[\sin{x}]=0$ How can I go from here?",,[]
19,strategies to find explicit formulae for series,strategies to find explicit formulae for series,,"I have been manipulating a certain series for several hours without finding any pattern. Hence I am wondering what some of the better strategies are to find patterns and thus an explicit formula for a series. Among the things I have tried so far are: looking for a common difference between terms looking for a common ratio between terms reversing the order of the terms and summing them up, to check whether the result will be the same for all terms bringing the terms to a common denominator and looking for a obvious pattern in the numerator I had no luck with any of these and others. The series btw. is $\sum_{k = 1}^n\frac{k - 1}{k(k + 1)(k + 2)}$ This is oen of the things I tried: $\begin{align*}                     S_n & = \frac{0}{6} + \frac{1}{24} + \frac{2}{60} + \frac{3}{120} + \frac{4}{210} + \frac{6}{336}\\                         & = \frac{0}{6} + \frac{1}{24} + \frac{1}{30} + \frac{1}{40} + \frac{1}{52,5} + \frac{1}{56}\\                         & = \frac{0}{1680} + \frac{70}{1680} + \frac{56}{1680} + \frac{42}{1680} + \frac{32}{1680} + \frac{30}{1680}\\         a_n - a_{n+1} : &  -\frac{70}{1680}; \frac{14}{1680}; \frac{14}{1680}; \frac{10}{1680}; \frac{8}{1680};  \end{align*}$ The differences between the terms get ever smaller and the sum approaches $.25$, but any internal pattern remains hidden after the things I tried. So, are there a number of useful methods to uncover patterns in series?","I have been manipulating a certain series for several hours without finding any pattern. Hence I am wondering what some of the better strategies are to find patterns and thus an explicit formula for a series. Among the things I have tried so far are: looking for a common difference between terms looking for a common ratio between terms reversing the order of the terms and summing them up, to check whether the result will be the same for all terms bringing the terms to a common denominator and looking for a obvious pattern in the numerator I had no luck with any of these and others. The series btw. is $\sum_{k = 1}^n\frac{k - 1}{k(k + 1)(k + 2)}$ This is oen of the things I tried: $\begin{align*}                     S_n & = \frac{0}{6} + \frac{1}{24} + \frac{2}{60} + \frac{3}{120} + \frac{4}{210} + \frac{6}{336}\\                         & = \frac{0}{6} + \frac{1}{24} + \frac{1}{30} + \frac{1}{40} + \frac{1}{52,5} + \frac{1}{56}\\                         & = \frac{0}{1680} + \frac{70}{1680} + \frac{56}{1680} + \frac{42}{1680} + \frac{32}{1680} + \frac{30}{1680}\\         a_n - a_{n+1} : &  -\frac{70}{1680}; \frac{14}{1680}; \frac{14}{1680}; \frac{10}{1680}; \frac{8}{1680};  \end{align*}$ The differences between the terms get ever smaller and the sum approaches $.25$, but any internal pattern remains hidden after the things I tried. So, are there a number of useful methods to uncover patterns in series?",,"['sequences-and-series', 'limits', 'closed-form']"
20,Proving that a limit of a piecewise constant function does not exist using $\epsilon$-$\delta$,Proving that a limit of a piecewise constant function does not exist using -,\epsilon \delta,"How would one go about proving that the limit of $$f:[0,1)\cup(1,2]\to\{1,2\}\\ 		x\mapsto 		\begin{cases} 			1&\text{if }x\in[0,1)\\ 			2&\text{if }x\in(1,2] 		\end{cases}$$ as $x\to 1$ does not exist? I attempted to proceed by contradiction. Suppose $\displaystyle\lim_{x\to 1} f(x)=L$ exists. Then $\forall\,\epsilon>0$, $\exists\,\delta>0$ such that  $$0 < |x-1|<\delta\Longrightarrow |f(x)-L|<\epsilon$$ Now I know that it is impossible to find an appropriate $\delta$ if we take $\epsilon=\frac{1}{3}$ for example, since the leap $f$ at $x=1$ is of length $1$. So so far I wish to show that $$0 < |x-1|<\delta\Longrightarrow |f(x)-L|<\frac{1}{3}$$ is impossible. But how can I proceed from here? I have to substitute $f(x)$ somehow, but I'm not sure how to go about it. Also, note that I do not wish to involve Left-hand/Right-hand limits at this point (we haven't covered those yet).","How would one go about proving that the limit of $$f:[0,1)\cup(1,2]\to\{1,2\}\\ 		x\mapsto 		\begin{cases} 			1&\text{if }x\in[0,1)\\ 			2&\text{if }x\in(1,2] 		\end{cases}$$ as $x\to 1$ does not exist? I attempted to proceed by contradiction. Suppose $\displaystyle\lim_{x\to 1} f(x)=L$ exists. Then $\forall\,\epsilon>0$, $\exists\,\delta>0$ such that  $$0 < |x-1|<\delta\Longrightarrow |f(x)-L|<\epsilon$$ Now I know that it is impossible to find an appropriate $\delta$ if we take $\epsilon=\frac{1}{3}$ for example, since the leap $f$ at $x=1$ is of length $1$. So so far I wish to show that $$0 < |x-1|<\delta\Longrightarrow |f(x)-L|<\frac{1}{3}$$ is impossible. But how can I proceed from here? I have to substitute $f(x)$ somehow, but I'm not sure how to go about it. Also, note that I do not wish to involve Left-hand/Right-hand limits at this point (we haven't covered those yet).",,"['real-analysis', 'limits', 'epsilon-delta']"
21,Find the value of the limit as n approaches infinity,Find the value of the limit as n approaches infinity,,"Find the limit as $n$ approaches infinity: $\lim_{n\to \infty}$$\sum_{i=1}^n  (\frac{1}{\sqrt{n}+\frac{i}{\sqrt{n}}})^2$ I can't seem to be able to figure out how to solve these with the i in the denominator, using the wolfram calculator I got an advanced answer with some gamma notation in it that makes no sense to me. How do I solve it without the gamma notation. I've gotten it down to limit as $n$ approaches infinity: $\lim_{n\to \infty}$$\sum_{i=1}^n  (\frac{n}{n^2+2in+i^2})$ I am only in calculus 1 and am looking for a way to seperate the function to where i can apply the sigma notation to each part of the function using the formulas for the sums of powers","Find the limit as $n$ approaches infinity: $\lim_{n\to \infty}$$\sum_{i=1}^n  (\frac{1}{\sqrt{n}+\frac{i}{\sqrt{n}}})^2$ I can't seem to be able to figure out how to solve these with the i in the denominator, using the wolfram calculator I got an advanced answer with some gamma notation in it that makes no sense to me. How do I solve it without the gamma notation. I've gotten it down to limit as $n$ approaches infinity: $\lim_{n\to \infty}$$\sum_{i=1}^n  (\frac{n}{n^2+2in+i^2})$ I am only in calculus 1 and am looking for a way to seperate the function to where i can apply the sigma notation to each part of the function using the formulas for the sums of powers",,"['calculus', 'sequences-and-series', 'limits']"
22,Equivalent norms and rate of convergence,Equivalent norms and rate of convergence,,"Suppose that in a finite-dimensional normed space we have defined two (equivalent) norms $\|\cdot\|_1$ and $\|\cdot\|_2$. Is it true that if  a sequence $(x_n)_{n \geq 1}$ converges to some point $x$ at a rate $n$ in the norm $\|\cdot\|_1$ (in the sense that $\|x_n-x\|_1 \sim \frac{1}{n}$), then we also have the same rate of convergence for the second norm, i.e. $\|x_n-x\|_2 \sim \frac{C}{n}$ for some constant $C$ ? So far, I've been only able to deduce that $$C_1 \leq \lim_{n \to \infty} n\|x_n-x\|_2 \leq C_2$$ for some positive constants $C_1$ and $C_2$ by using the definition of equivalent norms. How can I prove that the limit actually exists and equals some constant $C$ ? Perhaps it is obvious, but I don't see it !","Suppose that in a finite-dimensional normed space we have defined two (equivalent) norms $\|\cdot\|_1$ and $\|\cdot\|_2$. Is it true that if  a sequence $(x_n)_{n \geq 1}$ converges to some point $x$ at a rate $n$ in the norm $\|\cdot\|_1$ (in the sense that $\|x_n-x\|_1 \sim \frac{1}{n}$), then we also have the same rate of convergence for the second norm, i.e. $\|x_n-x\|_2 \sim \frac{C}{n}$ for some constant $C$ ? So far, I've been only able to deduce that $$C_1 \leq \lim_{n \to \infty} n\|x_n-x\|_2 \leq C_2$$ for some positive constants $C_1$ and $C_2$ by using the definition of equivalent norms. How can I prove that the limit actually exists and equals some constant $C$ ? Perhaps it is obvious, but I don't see it !",,"['limits', 'convergence-divergence', 'normed-spaces']"
23,Problem with limit proof $lim(|x_n+y_n|-|x_n-y_n|)$,Problem with limit proof,lim(|x_n+y_n|-|x_n-y_n|),I have: $$\lim_{n\to \infty}(|x_n+y_n|-|x_n-y_n|)=+\infty$$ Need to prove: $$\lim_{n\to \infty}|x_n|=\lim_{n\to \infty}|y_n|=\lim_{n\to \infty}x_ny_n=+\infty$$ I can prove $\lim_{n\to \infty}|x_n|=\lim_{n\to \infty}|y_n|=+\infty$ but I don't know what to do with other part. I can notice that $\lim_{n\to \infty}|x_n||y_n|=\lim_{n\to \infty}|y_nx_n|=+\infty$ but it's not what I exactly need because $\lim_{n\to \infty}|y_nx_n|=+\infty \Rightarrow \lim_{n\to \infty}y_nx_n=\infty $. Maybe there is some possibility to prove that $ x_n$ and $y_n$ have the same sign.,I have: $$\lim_{n\to \infty}(|x_n+y_n|-|x_n-y_n|)=+\infty$$ Need to prove: $$\lim_{n\to \infty}|x_n|=\lim_{n\to \infty}|y_n|=\lim_{n\to \infty}x_ny_n=+\infty$$ I can prove $\lim_{n\to \infty}|x_n|=\lim_{n\to \infty}|y_n|=+\infty$ but I don't know what to do with other part. I can notice that $\lim_{n\to \infty}|x_n||y_n|=\lim_{n\to \infty}|y_nx_n|=+\infty$ but it's not what I exactly need because $\lim_{n\to \infty}|y_nx_n|=+\infty \Rightarrow \lim_{n\to \infty}y_nx_n=\infty $. Maybe there is some possibility to prove that $ x_n$ and $y_n$ have the same sign.,,['limits']
24,Product of $\limsup_{x\to\infty} x_n $,Product of,\limsup_{x\to\infty} x_n ,Given $x_n > 0$ and $$\limsup_{n\to\infty} x_n \cdot \limsup_{n\to\infty} \frac{1}{x_n} = 1$$ Does this mean that $x_n$ converges?,Given $x_n > 0$ and $$\limsup_{n\to\infty} x_n \cdot \limsup_{n\to\infty} \frac{1}{x_n} = 1$$ Does this mean that $x_n$ converges?,,"['sequences-and-series', 'limits', 'convergence-divergence', 'limsup-and-liminf']"
25,Limit square roots of polynomials,Limit square roots of polynomials,,"I am trying to find $\lim \limits_{n \to \infty} {\sqrt{n^3+1}-n\sqrt{n} \over \sqrt{n^2+1}-n}$. I rewrite the fraction as $${(\sqrt{n^3+1}-n\sqrt{n})(\sqrt{n^3+1}+n\sqrt{n}) \over (\sqrt{n^2+1}-n)(\sqrt{n^3+1}+n\sqrt{n})} = {1 \over (\sqrt{n^2+1}-n)(\sqrt{n^3+1}+n\sqrt{n})}$$ I notice $$\sqrt{n^2+1}-n > 0$$ so the denominator is growing to infinity while the whole limit is $0$. In the material I'm covering, the properties of polynomials haven't been discussed (but the properties of the square root have been). Is there a more basic way to conclude about the denominator going to infinity?","I am trying to find $\lim \limits_{n \to \infty} {\sqrt{n^3+1}-n\sqrt{n} \over \sqrt{n^2+1}-n}$. I rewrite the fraction as $${(\sqrt{n^3+1}-n\sqrt{n})(\sqrt{n^3+1}+n\sqrt{n}) \over (\sqrt{n^2+1}-n)(\sqrt{n^3+1}+n\sqrt{n})} = {1 \over (\sqrt{n^2+1}-n)(\sqrt{n^3+1}+n\sqrt{n})}$$ I notice $$\sqrt{n^2+1}-n > 0$$ so the denominator is growing to infinity while the whole limit is $0$. In the material I'm covering, the properties of polynomials haven't been discussed (but the properties of the square root have been). Is there a more basic way to conclude about the denominator going to infinity?",,['limits']
26,Is the cusp of $\sqrt{\left|x-\frac 1 2\right|}$ differentiable?,Is the cusp of  differentiable?,\sqrt{\left|x-\frac 1 2\right|},"Consider the vector space $$\mathcal V=\left\{f:[0,1]\to\mathbb R\ \left|\ \sup_{x\neq y}\frac{|f(x)-f(y)|}{\sqrt{|x-y|}}\lt \infty \ \text{ and }\ f\left(\frac 1 2\right)=0 \right. \right\}$$   Show that the function $f(x)=\sqrt{\left|x-\frac 12\right|}$ belongs to $\mathcal V$. When $x\neq y$, $\ \sup_{x\neq y}\frac{f(x)-f(y)}{\sqrt{x-y}}=\frac{\mathrm d f}{\mathrm d x}\cdot\sqrt{x-y},\ $ and since $f$ is differentiable for $x\in[0,\frac 12)\cup(\frac 12,1]$, the only point to investigate is $x=\frac 12$. I don't see why the property would be satisfied at $x=\frac 12$ because I think the derivative of $f$ tends to $\pm$ infinity and doesn't exist. I thought that solving  $$\lim_{x\to\frac 12}\lim_{y\to \frac 12}\frac{\left|\sqrt{\left|x-\frac 12\right|}-\sqrt{\left|y-\frac 12\right|}\right|}{\sqrt{|x-y|}}$$  or even $$\lim_{x\to\frac 12^+}\lim_{y\to \frac 12^-}\frac{\left|\sqrt{x-\frac 12}-\sqrt{\frac 12-y}\right|}{\sqrt{x-y}}$$ would help to prove that the supremum is real.","Consider the vector space $$\mathcal V=\left\{f:[0,1]\to\mathbb R\ \left|\ \sup_{x\neq y}\frac{|f(x)-f(y)|}{\sqrt{|x-y|}}\lt \infty \ \text{ and }\ f\left(\frac 1 2\right)=0 \right. \right\}$$   Show that the function $f(x)=\sqrt{\left|x-\frac 12\right|}$ belongs to $\mathcal V$. When $x\neq y$, $\ \sup_{x\neq y}\frac{f(x)-f(y)}{\sqrt{x-y}}=\frac{\mathrm d f}{\mathrm d x}\cdot\sqrt{x-y},\ $ and since $f$ is differentiable for $x\in[0,\frac 12)\cup(\frac 12,1]$, the only point to investigate is $x=\frac 12$. I don't see why the property would be satisfied at $x=\frac 12$ because I think the derivative of $f$ tends to $\pm$ infinity and doesn't exist. I thought that solving  $$\lim_{x\to\frac 12}\lim_{y\to \frac 12}\frac{\left|\sqrt{\left|x-\frac 12\right|}-\sqrt{\left|y-\frac 12\right|}\right|}{\sqrt{|x-y|}}$$  or even $$\lim_{x\to\frac 12^+}\lim_{y\to \frac 12^-}\frac{\left|\sqrt{x-\frac 12}-\sqrt{\frac 12-y}\right|}{\sqrt{x-y}}$$ would help to prove that the supremum is real.",,"['limits', 'normed-spaces', 'holder-spaces']"
27,Why does $\lim_{k \rightarrow \infty} \lvert k^2 \sin (k^4) \rvert = 0$?,Why does ?,\lim_{k \rightarrow \infty} \lvert k^2 \sin (k^4) \rvert = 0,"For context: the question was if $x^2 y^2 \sin(\frac{1}{x^4}) + 2$ converges on $2$ for $x \rightarrow 0$. My initial assumption was that this is false because if I use the sequence $(x_k, y_k) = (\frac{1}{k}, k^2)$ this works out as $\mathop{\lim}\limits_{k \rightarrow \infty} \lvert k^2 \sin(k^4) \rvert \overset{?}{=} 0$, which intuitively seems to diverge. Yet the correct answer to the question was that it does indeed converge, and Wolfram Alpha also says so. But plotting the sequence with GeoGebra for the first 1000 elements shows a somewhat ""random"", yet growing sequence 1 . So why does $\lvert k^2 \sin(k^4) \rvert$ sequence converge? 1 Using the command Sequence[(k, abs(k² sin(k⁴))), k, 1, 1000]","For context: the question was if $x^2 y^2 \sin(\frac{1}{x^4}) + 2$ converges on $2$ for $x \rightarrow 0$. My initial assumption was that this is false because if I use the sequence $(x_k, y_k) = (\frac{1}{k}, k^2)$ this works out as $\mathop{\lim}\limits_{k \rightarrow \infty} \lvert k^2 \sin(k^4) \rvert \overset{?}{=} 0$, which intuitively seems to diverge. Yet the correct answer to the question was that it does indeed converge, and Wolfram Alpha also says so. But plotting the sequence with GeoGebra for the first 1000 elements shows a somewhat ""random"", yet growing sequence 1 . So why does $\lvert k^2 \sin(k^4) \rvert$ sequence converge? 1 Using the command Sequence[(k, abs(k² sin(k⁴))), k, 1, 1000]",,"['calculus', 'limits']"
28,Show using the formal definition of a limit that $\lim_{x\to4} \frac{x-4}{2-\sqrt x} =-4$ [closed],Show using the formal definition of a limit that  [closed],\lim_{x\to4} \frac{x-4}{2-\sqrt x} =-4,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Utilize the formal definition of a limit to prove: $$\lim_{x\to4} \frac{x-4}{2-\sqrt x} =-4$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Utilize the formal definition of a limit to prove: $$\lim_{x\to4} \frac{x-4}{2-\sqrt x} =-4$$",,['calculus']
29,Raabe's and Schlömilch's tests for limits,Raabe's and Schlömilch's tests for limits,,"This is problem 2.53 in Sohrab's Basic Real Analysis Problem First the Raabe's test as it has been formulated in the book. Corollary 2.3.31 Let $(x_n)$ be a sequence of positive numbers. Then $\sum x_n$ converges if $x_{n+1}/x_n\leq 1-r/n$ is ultimately true for some $r>1$. Problem 2.53 Show that Raabe's Test (Corollary 2.3.31) implies the following one (which is due to Schlömilch): Let $x_n > 0\;\forall n\in\mathbb{N}$. Then $\sum x_n$ converges if $n\log(x_n/x_{n+1})\geq r$ is ultimately true for some $r > 1$. Hint: Use the inequalities $x/(1+x)\leq \log(1+x)\leq x$ for all $x>-1$. Question The idea is to prove: Schlömilch's premises $\implies$ Raabe's premises $\implies$ convergence. BUT I'm only able to prove: Raabe's premises $\implies$ Schlömilch's premises $\implies$ convergence: $$\frac{r}{n}\leq 1-\frac{x_{n+1}}{x_n}\leq \log\frac{x_n}{x_{n+1}}$$ where, with $y:=\frac{x_{n}}{x_{n+1}}-1$, we have used $y/(1+y)\leq \log(1+y)$. It seems to me that the Raabe's is a more general result, therefore it is not possible to show that Schlömilch's premises $\implies$ Raabe's premises. And this is the question: Is my thought correct or otherwise where is the error in my reasoning?","This is problem 2.53 in Sohrab's Basic Real Analysis Problem First the Raabe's test as it has been formulated in the book. Corollary 2.3.31 Let $(x_n)$ be a sequence of positive numbers. Then $\sum x_n$ converges if $x_{n+1}/x_n\leq 1-r/n$ is ultimately true for some $r>1$. Problem 2.53 Show that Raabe's Test (Corollary 2.3.31) implies the following one (which is due to Schlömilch): Let $x_n > 0\;\forall n\in\mathbb{N}$. Then $\sum x_n$ converges if $n\log(x_n/x_{n+1})\geq r$ is ultimately true for some $r > 1$. Hint: Use the inequalities $x/(1+x)\leq \log(1+x)\leq x$ for all $x>-1$. Question The idea is to prove: Schlömilch's premises $\implies$ Raabe's premises $\implies$ convergence. BUT I'm only able to prove: Raabe's premises $\implies$ Schlömilch's premises $\implies$ convergence: $$\frac{r}{n}\leq 1-\frac{x_{n+1}}{x_n}\leq \log\frac{x_n}{x_{n+1}}$$ where, with $y:=\frac{x_{n}}{x_{n+1}}-1$, we have used $y/(1+y)\leq \log(1+y)$. It seems to me that the Raabe's is a more general result, therefore it is not possible to show that Schlömilch's premises $\implies$ Raabe's premises. And this is the question: Is my thought correct or otherwise where is the error in my reasoning?",,"['calculus', 'sequences-and-series', 'limits']"
30,"What is the rigorous formal definition of the ""limit position"" of a line with variable parameters","What is the rigorous formal definition of the ""limit position"" of a line with variable parameters",,"My question is taken from here (it was asked long time ago without satisfactory answer, so I feel like it is necessary to raise it again), do Carmo defines the notion of strong and weak tangent by using the notion of ""limit position"" without defining it. (Weak tangent) $\alpha: I \to \Bbb R^3$ has a weak tangent at $t_0 \in I$ , if the line determined by $\alpha(t_0 + h)$ and $\alpha(t_0)$ has a limit position when $h \to 0$ . (Strong tangent) $\alpha: I \to \Bbb R^3$ has a strong tangent at $t_0 \in I$ , if the line determined by $\alpha(t_0 + h)$ and $\alpha(t_0 + k)$ has a limit position when $h \to 0$ and $k \to 0$ . I think we can define it by the included angle between the variable line and the limit line but this can't rule out the case when they are parallel to each other with a positive distance. Does anyone have an elegant definition for the ""limit position""?","My question is taken from here (it was asked long time ago without satisfactory answer, so I feel like it is necessary to raise it again), do Carmo defines the notion of strong and weak tangent by using the notion of ""limit position"" without defining it. (Weak tangent) has a weak tangent at , if the line determined by and has a limit position when . (Strong tangent) has a strong tangent at , if the line determined by and has a limit position when and . I think we can define it by the included angle between the variable line and the limit line but this can't rule out the case when they are parallel to each other with a positive distance. Does anyone have an elegant definition for the ""limit position""?",\alpha: I \to \Bbb R^3 t_0 \in I \alpha(t_0 + h) \alpha(t_0) h \to 0 \alpha: I \to \Bbb R^3 t_0 \in I \alpha(t_0 + h) \alpha(t_0 + k) h \to 0 k \to 0,"['limits', 'differential-geometry']"
31,Calculate the limits of the sequence $\frac{2}{n+2}$ from first principles,Calculate the limits of the sequence  from first principles,\frac{2}{n+2},i know that it can then be simplified by saying $\frac{2}{n+2}$ < $\frac{2}{n}$ but then would it just continue as normal saying that you would then choose an integer $N(\epsilon)$ within the set of natural numbers such that $$N(\epsilon) > \frac{2}{\epsilon} $$ possible by Archimedes principle. if someone could talk me through this it would be much appreciated.,i know that it can then be simplified by saying $\frac{2}{n+2}$ < $\frac{2}{n}$ but then would it just continue as normal saying that you would then choose an integer $N(\epsilon)$ within the set of natural numbers such that $$N(\epsilon) > \frac{2}{\epsilon} $$ possible by Archimedes principle. if someone could talk me through this it would be much appreciated.,,"['calculus', 'sequences-and-series', 'limits', 'epsilon-delta']"
32,What does x* mean in the integral definition?,What does x* mean in the integral definition?,,"I was looking for the sum definition of limit and came across this diagram, but I don't know what the x*I means. I know it's different from x though. Thanks in advance!","I was looking for the sum definition of limit and came across this diagram, but I don't know what the x*I means. I know it's different from x though. Thanks in advance!",,"['limits', 'definite-integrals']"
33,"$\lim_{k \rightarrow \infty} \sup_{n \ge 1} a_{n, k} = \sup_{n \ge 1} \lim_{k \to \infty} a_{n,k}$?",?,"\lim_{k \rightarrow \infty} \sup_{n \ge 1} a_{n, k} = \sup_{n \ge 1} \lim_{k \to \infty} a_{n,k}","This is important for me and somehow I can't think straight now so I need some help. Suppose $a_{n,k}$ is a real double sequence and it is bounded. Is it possible to prove the claim $$ \lim_{k \rightarrow \infty} \sup_{n \ge 1} a_{n, k} = \sup_{n \ge 1} \lim_{k \to \infty} a_{n,k}$$ Thanks!","This is important for me and somehow I can't think straight now so I need some help. Suppose $a_{n,k}$ is a real double sequence and it is bounded. Is it possible to prove the claim $$ \lim_{k \rightarrow \infty} \sup_{n \ge 1} a_{n, k} = \sup_{n \ge 1} \lim_{k \to \infty} a_{n,k}$$ Thanks!",,"['real-analysis', 'limits', 'supremum-and-infimum']"
34,Does $\lim_{q\to \infty }f(q)$ has sense where $q\in \mathbb Q$?,Does  has sense where ?,\lim_{q\to \infty }f(q) q\in \mathbb Q,"I wrote in a previous exam $$\lim_{\underset{q\in \mathbb Q}{q\to \infty} }f(q)=1\neq 0 =\lim_{\underset{r\in \mathbb R\backslash \mathbb Q}{r\to \infty }}f(r),$$ but my teacher told me that such limit has no sense, but I don't understand why. Is that really have no sense ? And if yes, why ?","I wrote in a previous exam $$\lim_{\underset{q\in \mathbb Q}{q\to \infty} }f(q)=1\neq 0 =\lim_{\underset{r\in \mathbb R\backslash \mathbb Q}{r\to \infty }}f(r),$$ but my teacher told me that such limit has no sense, but I don't understand why. Is that really have no sense ? And if yes, why ?",,"['real-analysis', 'limits', 'notation']"
35,How to prove it using $\epsilon$-$\delta$ defination of limit?,How to prove it using - defination of limit?,\epsilon \delta,"The question is : Show that, $$\lim_{x \to 0} \frac {\sin \frac {1} {x}} {\sin \frac {1} {x}}$$ does not exist. How to solve it by the defination of limit? Can it be solved using sequential criterion? Please help me. Thank you in advance.","The question is : Show that, $$\lim_{x \to 0} \frac {\sin \frac {1} {x}} {\sin \frac {1} {x}}$$ does not exist. How to solve it by the defination of limit? Can it be solved using sequential criterion? Please help me. Thank you in advance.",,[]
36,Doubt regarding a limit which is related to MVT,Doubt regarding a limit which is related to MVT,,"Let the function $f(x)$ be differentiable  and $f'(x)$ be continuous in $\left(-\infty,\infty \right)$ with $f'(2)=14$ then evaluate the limit $$\lim_{x\to 0}\frac{f(2+\sin x)-f(2+x\cos x)}{x-\sin x}$$ My attempt: $\lim_{x\to 0}\frac{f(2+\sin x)-f(2+x\cos x)}{x-\sin x}$ $\lim_{x\to 0}\left(\frac{f(2+\sin x)-f(2+x\cos x)}{\sin x-x\cos x}\right)$ $\frac{\sin x-x\cos x}{x-\sin x}$$=\left(f'(2)\right)(2)=28$ Is the method used here correct","Let the function $f(x)$ be differentiable  and $f'(x)$ be continuous in $\left(-\infty,\infty \right)$ with $f'(2)=14$ then evaluate the limit $$\lim_{x\to 0}\frac{f(2+\sin x)-f(2+x\cos x)}{x-\sin x}$$ My attempt: $\lim_{x\to 0}\frac{f(2+\sin x)-f(2+x\cos x)}{x-\sin x}$ $\lim_{x\to 0}\left(\frac{f(2+\sin x)-f(2+x\cos x)}{\sin x-x\cos x}\right)$ $\frac{\sin x-x\cos x}{x-\sin x}$$=\left(f'(2)\right)(2)=28$ Is the method used here correct",,"['calculus', 'limits', 'proof-verification']"
37,A limit using the Euler number: $\lim_{n\rightarrow\infty}\frac{n!}{(n-i)!}\left(\frac{c}{n}\right)^{n-i}$,A limit using the Euler number:,\lim_{n\rightarrow\infty}\frac{n!}{(n-i)!}\left(\frac{c}{n}\right)^{n-i},What is answer of this limit and how can I get it? $c$ and $i$ are constants. $$\lim_{n\rightarrow\infty}\frac{n!}{(n-i)!}\left(\frac{c}{n}\right)^{n-i}$$ I guess it will envolve some Neper/the Euler number $e$. I tried to rearrange terms of factorial and exponents in a good way but I couldn't make any conclusion so I think couldn't find the nice shape of writing this expression.,What is answer of this limit and how can I get it? $c$ and $i$ are constants. $$\lim_{n\rightarrow\infty}\frac{n!}{(n-i)!}\left(\frac{c}{n}\right)^{n-i}$$ I guess it will envolve some Neper/the Euler number $e$. I tried to rearrange terms of factorial and exponents in a good way but I couldn't make any conclusion so I think couldn't find the nice shape of writing this expression.,,"['calculus', 'limits', 'convergence-divergence', 'closed-form']"
38,Function with countably many points of discontinuity,Function with countably many points of discontinuity,,"Aside from rigor, is this proof correct? Claim. Let $f$ be a function defined on $[0, 1]$ such that $\lim\limits_{y\to a} f(y)$ exists for all $a \in [0, 1]$. Then for any $\epsilon > 0$ there are only finitely many points $a \in [0, 1]$ with $$|\lim\limits_{y\to a} f(y) - f(a)| > \epsilon.$$ Proof. Suppose that there are infinitely many such points $a.$ Then by the Bolzano-Weierstrass Theorem, these points have a limit $x \in [0, 1].$ Let $$L := \lim\limits_{y \to x} f(y) = \lim\limits_{a\to x} f(a).$$ The condition $$|\lim\limits_{y\to a} f(y) - f(a)| > \epsilon$$ means that for $y$ close to $a$, $f(y)$ is far from $f(a).$ Similarly $\lim\limits_{a \to x} f(a) = L$ means that for $a$ close to $x,$ $f(a)$ is close to $L.$ Together this means that for $y$ close to $x$, $f(y)$ is far from $L$, but this contradicts the fact that $L = \lim\limits_{y \to x} f(y),$ i.e. for $y$ close to $x$, $f(y)$ is close to $L.$","Aside from rigor, is this proof correct? Claim. Let $f$ be a function defined on $[0, 1]$ such that $\lim\limits_{y\to a} f(y)$ exists for all $a \in [0, 1]$. Then for any $\epsilon > 0$ there are only finitely many points $a \in [0, 1]$ with $$|\lim\limits_{y\to a} f(y) - f(a)| > \epsilon.$$ Proof. Suppose that there are infinitely many such points $a.$ Then by the Bolzano-Weierstrass Theorem, these points have a limit $x \in [0, 1].$ Let $$L := \lim\limits_{y \to x} f(y) = \lim\limits_{a\to x} f(a).$$ The condition $$|\lim\limits_{y\to a} f(y) - f(a)| > \epsilon$$ means that for $y$ close to $a$, $f(y)$ is far from $f(a).$ Similarly $\lim\limits_{a \to x} f(a) = L$ means that for $a$ close to $x,$ $f(a)$ is close to $L.$ Together this means that for $y$ close to $x$, $f(y)$ is far from $L$, but this contradicts the fact that $L = \lim\limits_{y \to x} f(y),$ i.e. for $y$ close to $x$, $f(y)$ is close to $L.$",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
39,"Prove that $\lim_{(x,y)\to(1,1)} \frac {x}{y}=1$ by epsilon delta",Prove that  by epsilon delta,"\lim_{(x,y)\to(1,1)} \frac {x}{y}=1","How can I prove that  $$\lim_{(x,y)\to(1,1)} \frac {x}{y}=1$$ By epsilon delta? I am trying and I am stuck: Proof: Suppose $\epsilon >0$ we want to construct $\delta = \delta (\epsilon ) $ such that $|x/y-1|<\epsilon $ whenever $|x-1|<\delta$ and $|y-1|<\delta$. But $\vert \frac{x}{y}-1 \vert =\vert \frac {x-y}{y}\vert\leq\frac {|x-1|+|y-1|}{|y|}<\frac {2\delta}{|y-1|}<\frac {2\delta}{\delta}=2$","How can I prove that  $$\lim_{(x,y)\to(1,1)} \frac {x}{y}=1$$ By epsilon delta? I am trying and I am stuck: Proof: Suppose $\epsilon >0$ we want to construct $\delta = \delta (\epsilon ) $ such that $|x/y-1|<\epsilon $ whenever $|x-1|<\delta$ and $|y-1|<\delta$. But $\vert \frac{x}{y}-1 \vert =\vert \frac {x-y}{y}\vert\leq\frac {|x-1|+|y-1|}{|y|}<\frac {2\delta}{|y-1|}<\frac {2\delta}{\delta}=2$",,"['calculus', 'limits', 'multivariable-calculus', 'epsilon-delta']"
40,Convergence of a series with terms in-between harmonic and geometric,Convergence of a series with terms in-between harmonic and geometric,,"Let $\alpha \in (0,1)$, and let $K \in \mathbb N$. Consider the infinite series as a function a $K$: $$f(K) = \frac{K^\alpha}{(1+K^\alpha)} + \frac{K^\alpha (K+1)^\alpha}{(1+K^\alpha)(1+(K+1)^\alpha)} + \frac{K^\alpha (K+1)^\alpha (K+2)^\alpha}{(1+K^\alpha)(1+(K+1)^\alpha)(1+ (K+2)^\alpha} + \dots $$ My questions are: Is $f(K)$ finite? Is $\lim_{K \to \infty} f(K)$ finite? Note that when $\alpha = 0$, the series always converges to $1$. When $\alpha = 1$, it always diverges. So the question boils down to: what happens when $\alpha \in (0,1)$?","Let $\alpha \in (0,1)$, and let $K \in \mathbb N$. Consider the infinite series as a function a $K$: $$f(K) = \frac{K^\alpha}{(1+K^\alpha)} + \frac{K^\alpha (K+1)^\alpha}{(1+K^\alpha)(1+(K+1)^\alpha)} + \frac{K^\alpha (K+1)^\alpha (K+2)^\alpha}{(1+K^\alpha)(1+(K+1)^\alpha)(1+ (K+2)^\alpha} + \dots $$ My questions are: Is $f(K)$ finite? Is $\lim_{K \to \infty} f(K)$ finite? Note that when $\alpha = 0$, the series always converges to $1$. When $\alpha = 1$, it always diverges. So the question boils down to: what happens when $\alpha \in (0,1)$?",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
41,$\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n}$ converges for all $a\in \mathbb{R}$,converges for all,\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n} a\in \mathbb{R},"$$\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n}$$ Can I just see this series as a geometric series? Since $c = \frac{1}{1+a^2}<1$, we can see this as the geometric series: $$\sum_{n=0}^{\infty}bc^n = \sum_{n=0}^{\infty}a^2\left(\frac{1}{1+a^2}\right)^n$$ that converges because $c<1$. The sum of this series is: $$S_n = b(c^0+c^1+c^2+\cdots c^n)\rightarrow cS_n = b(c^1+c^2 + c^3 + \cdots + c^{n}+c^{n+1})\rightarrow $$$$cS_n - S_n = b(c^{n+1}-1)\rightarrow S_n(c-1) = b(c^{n+1}-1)\rightarrow S_n = b\frac{c^{n+1}-1}{c-1}$$ $$S = \lim S_n = b\frac{1}{1-c}$$ So $$\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n} = b\frac{1}{1-c} = a^2\frac{1}{1-\frac{1}{1+a^2}} =$$","$$\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n}$$ Can I just see this series as a geometric series? Since $c = \frac{1}{1+a^2}<1$, we can see this as the geometric series: $$\sum_{n=0}^{\infty}bc^n = \sum_{n=0}^{\infty}a^2\left(\frac{1}{1+a^2}\right)^n$$ that converges because $c<1$. The sum of this series is: $$S_n = b(c^0+c^1+c^2+\cdots c^n)\rightarrow cS_n = b(c^1+c^2 + c^3 + \cdots + c^{n}+c^{n+1})\rightarrow $$$$cS_n - S_n = b(c^{n+1}-1)\rightarrow S_n(c-1) = b(c^{n+1}-1)\rightarrow S_n = b\frac{c^{n+1}-1}{c-1}$$ $$S = \lim S_n = b\frac{1}{1-c}$$ So $$\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n} = b\frac{1}{1-c} = a^2\frac{1}{1-\frac{1}{1+a^2}} =$$",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
42,"If $\lim_{x\to a} g(x) = M$, show that there exists a number $\delta > 0$ such that $0 < |x - a| < \delta \Rightarrow |g(x)| < 1 + |M|.$","If , show that there exists a number  such that",\lim_{x\to a} g(x) = M \delta > 0 0 < |x - a| < \delta \Rightarrow |g(x)| < 1 + |M|.,"The hint given was to take $\epsilon = 1$ for the formal definition of a limit. Doing this means that $|g(x) - M| < 1.$ Using the triangle inequality, you get $$|g(x)| = |(g(x) - M) + M| \le |g(x) - M| + |M| < 1 + |M|.$$ It also appears that the same method can be applied for $0 <\,\epsilon < 1.$ For $\epsilon = 0.5$, you end up getting $|g(x)| < 0.5 + |M| < 1 + |M|.$ How can the question be answered for $\epsilon > 1$?","The hint given was to take $\epsilon = 1$ for the formal definition of a limit. Doing this means that $|g(x) - M| < 1.$ Using the triangle inequality, you get $$|g(x)| = |(g(x) - M) + M| \le |g(x) - M| + |M| < 1 + |M|.$$ It also appears that the same method can be applied for $0 <\,\epsilon < 1.$ For $\epsilon = 0.5$, you end up getting $|g(x)| < 0.5 + |M| < 1 + |M|.$ How can the question be answered for $\epsilon > 1$?",,"['limits', 'proof-verification', 'definition']"
43,Is this estimate true or not true?,Is this estimate true or not true?,,"Let $\varepsilon>0$. Let $\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ the standard normal density function. Then  $$\lim_{\varepsilon\to 0}\int_0^1 \frac{1}{\sqrt{x}}\left[ \varphi\left(\frac{\sqrt{x}-\varepsilon}{\sqrt{x}}\right)-\varphi\left(\frac{\sqrt{x}+\varepsilon}{\sqrt{x}}\right)\right]dx=0.$$ The question I have is about the modulus of continuity. I would like to know if $$\left|\int_0^1 \frac{1}{\sqrt{x}}\left[ \varphi\left(\frac{\sqrt{x}-\varepsilon}{\sqrt{x}}\right)-\varphi\left(\frac{\sqrt{x}+\varepsilon}{\sqrt{x}}\right)\right]dx\right|\leq C\varepsilon,$$ for some universal constant $C>0$. Observe that pulling the absolute value inside the integral and using the Lipschitz property on $\varphi$ is too much, since $\frac{1}{x}$ is not integrable. Thanks!!","Let $\varepsilon>0$. Let $\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ the standard normal density function. Then  $$\lim_{\varepsilon\to 0}\int_0^1 \frac{1}{\sqrt{x}}\left[ \varphi\left(\frac{\sqrt{x}-\varepsilon}{\sqrt{x}}\right)-\varphi\left(\frac{\sqrt{x}+\varepsilon}{\sqrt{x}}\right)\right]dx=0.$$ The question I have is about the modulus of continuity. I would like to know if $$\left|\int_0^1 \frac{1}{\sqrt{x}}\left[ \varphi\left(\frac{\sqrt{x}-\varepsilon}{\sqrt{x}}\right)-\varphi\left(\frac{\sqrt{x}+\varepsilon}{\sqrt{x}}\right)\right]dx\right|\leq C\varepsilon,$$ for some universal constant $C>0$. Observe that pulling the absolute value inside the integral and using the Lipschitz property on $\varphi$ is too much, since $\frac{1}{x}$ is not integrable. Thanks!!",,"['calculus', 'real-analysis', 'limits', 'inequality', 'estimation']"
44,"Finding an $N$, such that $\forall n \geq N$...","Finding an , such that ...",N \forall n \geq N,"Ok, so I am stuck regarding the following question; Let $\epsilon > 0$ be a positive real number. Find a natural number $N \in$ $\mathbb{N}$ such that $$ |a_n| =\left|\frac{n + \frac{1}{n}}{n+1} - 1\right| < \epsilon, \forall n \geq \mathbb{N}$$ Now, my thinking when I'm posed with a question like this is to try and rearrange to get $n > f(\epsilon)$, then one can easily deduce what the value of $N$ should be, for instance by taking the integer part of $f(\epsilon)$ and adding $1$, which ensures that it is an integer and also ensures that it is greater or equal to the original $f(\epsilon)$. Now, with an expression like this, I cannot see how to possibly rearrange it to get the required expression, but I have been told that if I find an $f(n) \geq |a_n|$, then I can use the $N$ I get from this as the $N$ for the original expression. This intuitively makes sense as this $f(n)$ is larger, and so you'd need a larger $N$ for it to be smaller than $\epsilon$, and so this 'larger $N$' would also work for the original expression. But I run in to trouble with finding this 'nice $f(n)$', I mean can I take any larger $f(n)$? For example $|a_n| \leq \left|\frac{n(n+1)}{n+1} - 1\right| = |n - 1|$. Because $n^2 \geq n \enspace,    \forall n \geq 1$ and $n \geq \frac{1}{n} \enspace\forall n \geq 1$. But this then gives $n < \epsilon + 1$ which is nonsense so I'm not sure if my method is correct or not.","Ok, so I am stuck regarding the following question; Let $\epsilon > 0$ be a positive real number. Find a natural number $N \in$ $\mathbb{N}$ such that $$ |a_n| =\left|\frac{n + \frac{1}{n}}{n+1} - 1\right| < \epsilon, \forall n \geq \mathbb{N}$$ Now, my thinking when I'm posed with a question like this is to try and rearrange to get $n > f(\epsilon)$, then one can easily deduce what the value of $N$ should be, for instance by taking the integer part of $f(\epsilon)$ and adding $1$, which ensures that it is an integer and also ensures that it is greater or equal to the original $f(\epsilon)$. Now, with an expression like this, I cannot see how to possibly rearrange it to get the required expression, but I have been told that if I find an $f(n) \geq |a_n|$, then I can use the $N$ I get from this as the $N$ for the original expression. This intuitively makes sense as this $f(n)$ is larger, and so you'd need a larger $N$ for it to be smaller than $\epsilon$, and so this 'larger $N$' would also work for the original expression. But I run in to trouble with finding this 'nice $f(n)$', I mean can I take any larger $f(n)$? For example $|a_n| \leq \left|\frac{n(n+1)}{n+1} - 1\right| = |n - 1|$. Because $n^2 \geq n \enspace,    \forall n \geq 1$ and $n \geq \frac{1}{n} \enspace\forall n \geq 1$. But this then gives $n < \epsilon + 1$ which is nonsense so I'm not sure if my method is correct or not.",,"['sequences-and-series', 'limits']"
45,How do I calculate this limit when two terms tend to infinity at similar rates,How do I calculate this limit when two terms tend to infinity at similar rates,,"In a particular problem that I am currently trying to solve, I have the following expression (this is not the entire expression, I have included only the terms involving $a_1$ and $b_1$), $\lim_{(a_1,b_1)\to (\infty,\infty)} \frac{(a_1+a_2+p)!}{(a_1 + p)!}\frac{(b_1+q)^{a_1 +p}}{(b_1+b_2+q)^{a_1+a_2+p}}$ [equation 1] $a_1$ and $b_1$ tend to infinity such that $a_1/b_1 \rightarrow c_0$, i.e., the ratio of $a_1$ and $b_1$ tends to a fixed value. (It is known that $a_2$, $p$ and $q$ do not tend to infinity) This is what I did so far. I used Stirling's approximation for the factorials, the limit reduces to, $\lim_{(a_1,b_1)\to (\infty,\infty)} \left[\sqrt{\frac{a_1+a_2+p}{a_1 +p}}\left(\frac{a_1+a_2+p}{b_1+b_2+q}\right)^{a_1+a_2+p} \left(\frac{b_1+q}{a_1+p}\right)^{a_1+p} e^{-a_2}\right]$ I then, made these approximations, $(a_1+a_2+p) \rightarrow a_1$, $(b_1+b_2+q) \rightarrow b_1$, $(a_1+p) \rightarrow a_1$ and $(b_1+q) \rightarrow b_1$. With these approximations, the limit is calculated as, $\lim_{(a_1,b_1)\to (\infty,\infty)} \left(\frac{a_1}{b_1}\right)^{a_2}e^{-a_2} = c_0^{a_2}e^{-a_2}$ [equation 2] I want to know if this is a correct way to approximating the limit. I doubt my solution because the simulation results in values that can be obtained if I approximate [equation 1] as $c_0^{a_2}e^{-c_0}$ . This differs from [equation 2] in only one term but creates a huge difference. Is there any problem in my the way I approximate  the limit? How do I solve this? Any help is appreciated. Thanks in advance","In a particular problem that I am currently trying to solve, I have the following expression (this is not the entire expression, I have included only the terms involving $a_1$ and $b_1$), $\lim_{(a_1,b_1)\to (\infty,\infty)} \frac{(a_1+a_2+p)!}{(a_1 + p)!}\frac{(b_1+q)^{a_1 +p}}{(b_1+b_2+q)^{a_1+a_2+p}}$ [equation 1] $a_1$ and $b_1$ tend to infinity such that $a_1/b_1 \rightarrow c_0$, i.e., the ratio of $a_1$ and $b_1$ tends to a fixed value. (It is known that $a_2$, $p$ and $q$ do not tend to infinity) This is what I did so far. I used Stirling's approximation for the factorials, the limit reduces to, $\lim_{(a_1,b_1)\to (\infty,\infty)} \left[\sqrt{\frac{a_1+a_2+p}{a_1 +p}}\left(\frac{a_1+a_2+p}{b_1+b_2+q}\right)^{a_1+a_2+p} \left(\frac{b_1+q}{a_1+p}\right)^{a_1+p} e^{-a_2}\right]$ I then, made these approximations, $(a_1+a_2+p) \rightarrow a_1$, $(b_1+b_2+q) \rightarrow b_1$, $(a_1+p) \rightarrow a_1$ and $(b_1+q) \rightarrow b_1$. With these approximations, the limit is calculated as, $\lim_{(a_1,b_1)\to (\infty,\infty)} \left(\frac{a_1}{b_1}\right)^{a_2}e^{-a_2} = c_0^{a_2}e^{-a_2}$ [equation 2] I want to know if this is a correct way to approximating the limit. I doubt my solution because the simulation results in values that can be obtained if I approximate [equation 1] as $c_0^{a_2}e^{-c_0}$ . This differs from [equation 2] in only one term but creates a huge difference. Is there any problem in my the way I approximate  the limit? How do I solve this? Any help is appreciated. Thanks in advance",,"['limits', 'approximation', 'infinity']"
46,Limit of $n-1$ measure of the boundary of a sphere,Limit of  measure of the boundary of a sphere,n-1,"The measure of a sphere of radius $R$ centered in $0_{\mathbb{R}^n}$ in $\mathbb{R}^n$ is \begin{array}{l l}\int_{B_0(R)}dx_1\ldots dx_n & =\int_0^R\rho^{n-1}d\rho \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^{n-1}{\varphi_1}d\varphi_1\ldots\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos{\varphi_{n-1}}d\varphi_{n-1}\int_{0}^{2\pi} d\theta \\ & =\omega_n \int_0^R\rho^{n-1} d\rho  \end{array} where $\omega_n$ is the $n-1$ measure of the boundary of the sphere. We can calculate the integral of the function $e^{-||x||^2}$ over $\mathbb R^n$ as $$\int_{\mathbb{R}^n}e^{-||x||^2}dx_1\ldots dx_n=\int_{\mathbb R^n}e^{-x_1^2-\ldots-x_1^n}dx_1\ldots dx_n=\int_{-\infty}^{+\infty}e^{-x_1^2}dx_1\cdot \ldots \cdot \int_{-\infty}^{+\infty}e^{-x_n^2}dx_n=\pi^{\frac{n}{2}} $$ This integral can also be calculated by observing that $e^{-||x||^2}$ is a radial function: in fact $$\int_{\mathbb R^n}e^{-||x||^2}=\omega_n\int_{\mathbb R^{n}} e^{-\rho^2}\rho^{n-1}d\rho\stackrel{\rho^2=t}{=}\frac{\omega_n}{2}\int_{\mathbb R^n}e^{-t}t^{\frac{n}{2}-1}dt=\frac 12 \omega_n \Gamma\left (\frac{n}{2}\right )$$ This yields the equality $$\omega_n=\frac{2\pi^{\frac{n}{2}}}{\Gamma\left (\frac{n}{2}\right )}=\frac{\pi^{\frac {n-1}{2}}2^{\frac{n+1}{2}}}{n!!}$$ Clearly, we have  $$\lim_{n \to \infty}\omega_n=0$$ How can we interpret such result in a geometrical way? Is there any intuitive explanation of this fact?","The measure of a sphere of radius $R$ centered in $0_{\mathbb{R}^n}$ in $\mathbb{R}^n$ is \begin{array}{l l}\int_{B_0(R)}dx_1\ldots dx_n & =\int_0^R\rho^{n-1}d\rho \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^{n-1}{\varphi_1}d\varphi_1\ldots\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos{\varphi_{n-1}}d\varphi_{n-1}\int_{0}^{2\pi} d\theta \\ & =\omega_n \int_0^R\rho^{n-1} d\rho  \end{array} where $\omega_n$ is the $n-1$ measure of the boundary of the sphere. We can calculate the integral of the function $e^{-||x||^2}$ over $\mathbb R^n$ as $$\int_{\mathbb{R}^n}e^{-||x||^2}dx_1\ldots dx_n=\int_{\mathbb R^n}e^{-x_1^2-\ldots-x_1^n}dx_1\ldots dx_n=\int_{-\infty}^{+\infty}e^{-x_1^2}dx_1\cdot \ldots \cdot \int_{-\infty}^{+\infty}e^{-x_n^2}dx_n=\pi^{\frac{n}{2}} $$ This integral can also be calculated by observing that $e^{-||x||^2}$ is a radial function: in fact $$\int_{\mathbb R^n}e^{-||x||^2}=\omega_n\int_{\mathbb R^{n}} e^{-\rho^2}\rho^{n-1}d\rho\stackrel{\rho^2=t}{=}\frac{\omega_n}{2}\int_{\mathbb R^n}e^{-t}t^{\frac{n}{2}-1}dt=\frac 12 \omega_n \Gamma\left (\frac{n}{2}\right )$$ This yields the equality $$\omega_n=\frac{2\pi^{\frac{n}{2}}}{\Gamma\left (\frac{n}{2}\right )}=\frac{\pi^{\frac {n-1}{2}}2^{\frac{n+1}{2}}}{n!!}$$ Clearly, we have  $$\lim_{n \to \infty}\omega_n=0$$ How can we interpret such result in a geometrical way? Is there any intuitive explanation of this fact?",,"['integration', 'limits', 'measure-theory', 'intuition', 'spheres']"
47,Prove that $\lim_{\ r\ \to \ \infty} \dfrac{r! r^x}{x(x+1)(x+2) \dots (x+r)} = \int_{0}^{\infty} t^{x-1} e^{-t} dt $,Prove that,\lim_{\ r\ \to \ \infty} \dfrac{r! r^x}{x(x+1)(x+2) \dots (x+r)} = \int_{0}^{\infty} t^{x-1} e^{-t} dt ,"From Havil & Dyson, ""Gamma: Exploring Euler's Constant"", section 6.1 I can't prove the following Euler's theorem : ... on 13 October 1729, Euler had already proposed to Goldbach the    definition $$\Gamma (x) = \lim_{r\ \to \ \infty} \Gamma_r (x) $$ where $$\Gamma_r (x) = \dfrac{r! r^x}{x(x+1)(x+2) \dots (x+r)}$$ [which is valid whenever $x$ is not zero or negative integer]. After 1 hours and 40 minutes of attempt I can't turn $\lim_{r\ \to \ \infty} \Gamma_r (x)$ to the following definition of $\Gamma (x)$ that I know, i.e. $$\Gamma (x) = \int_{0}^{\infty} t^{x-1} e^{-t} dt, \ \ \ x>0. $$ So why $$\lim_{r\ \to \ \infty} \dfrac{r^x}{x(1+ \frac{x}{1})(1+\frac{x}{2}) \dots (1+\frac{x}{r})} = \int_{0}^{\infty} t^{x-1} e^{-t} dt , \ \ \ x>0 \ \text{?} $$ EDIT : For the special case when $x$ is a positive integer we will have the problem of showing $$\lim_{r\ \to \ \infty} \dfrac{r!r^n}{(n+r)!} =1 $$ to be valid.","From Havil & Dyson, ""Gamma: Exploring Euler's Constant"", section 6.1 I can't prove the following Euler's theorem : ... on 13 October 1729, Euler had already proposed to Goldbach the    definition $$\Gamma (x) = \lim_{r\ \to \ \infty} \Gamma_r (x) $$ where $$\Gamma_r (x) = \dfrac{r! r^x}{x(x+1)(x+2) \dots (x+r)}$$ [which is valid whenever $x$ is not zero or negative integer]. After 1 hours and 40 minutes of attempt I can't turn $\lim_{r\ \to \ \infty} \Gamma_r (x)$ to the following definition of $\Gamma (x)$ that I know, i.e. $$\Gamma (x) = \int_{0}^{\infty} t^{x-1} e^{-t} dt, \ \ \ x>0. $$ So why $$\lim_{r\ \to \ \infty} \dfrac{r^x}{x(1+ \frac{x}{1})(1+\frac{x}{2}) \dots (1+\frac{x}{r})} = \int_{0}^{\infty} t^{x-1} e^{-t} dt , \ \ \ x>0 \ \text{?} $$ EDIT : For the special case when $x$ is a positive integer we will have the problem of showing $$\lim_{r\ \to \ \infty} \dfrac{r!r^n}{(n+r)!} =1 $$ to be valid.",,"['limits', 'improper-integrals']"
48,Rate of convergence of Cesàro means,Rate of convergence of Cesàro means,,"For a sequence $a_n = O(n^{-1/2})$ as $n\to\infty$, consider the corresponding Cesàro means $b_n = \frac{1}{n} \sum_{j=1}^n a_j$. Is it possible to derive the rate of convergence for the sequence $b_n$? What about the general case $a_n = O(c_n)$?","For a sequence $a_n = O(n^{-1/2})$ as $n\to\infty$, consider the corresponding Cesàro means $b_n = \frac{1}{n} \sum_{j=1}^n a_j$. Is it possible to derive the rate of convergence for the sequence $b_n$? What about the general case $a_n = O(c_n)$?",,"['sequences-and-series', 'limits', 'convergence-divergence', 'cesaro-summable']"
49,Finding the limit as $n \to \infty $ of $n\ln\left(1+\frac{\ x}{n^2}\right)$,Finding the limit as  of,n \to \infty  n\ln\left(1+\frac{\ x}{n^2}\right),Find $$\lim_{n\to \infty} n\ln\left(1+\frac{\ x}{n^2}\right)$$ My attempt: $\lim_{n\to \infty} n \left[\ln\left(\frac{\ n^2 +x}{n^2}\right)\right]$ = $\lim_{n\to \infty} n [\ln (n^2 +x) - \ln(n^2)]$ But I'm not sure how to get this out of indeterminate form.,Find $$\lim_{n\to \infty} n\ln\left(1+\frac{\ x}{n^2}\right)$$ My attempt: $\lim_{n\to \infty} n \left[\ln\left(\frac{\ n^2 +x}{n^2}\right)\right]$ = $\lim_{n\to \infty} n [\ln (n^2 +x) - \ln(n^2)]$ But I'm not sure how to get this out of indeterminate form.,,"['calculus', 'real-analysis', 'limits', 'logarithms', 'indeterminate-forms']"
50,Limit to find convergence of $\int_1^\infty \frac{\arctan x}{x^2} ~dx$,Limit to find convergence of,\int_1^\infty \frac{\arctan x}{x^2} ~dx,"Show the integral is convergent and find the value it converges to. $$\int_1^\infty \frac{\arctan x}{x^2} ~dx$$ I have found the indefinite integral to be $$-\frac{\arctan x}{x} + \ln|x| -\frac{1}{2}\ln(x^2+1)$$ When I take the limit as t goes to $\infty$, I end up having to deal with this $$\lim_{t\to \infty}\left(-\frac{\arctan x}{x} + \ln|x| -\frac{1}{2}\ln(x^2+1)\right)$$ I know the first term goes to 0, but how do I find the limit of the rest of it? I tried combining the two terms to see if I could use L'Hospital's rule, but I couldn't get anywhere. I got to this and was not sure how to proceed. $$\lim_{t\to \infty}\left(\ln \left(\frac{|t|}{\sqrt{t^2+1}}\right)\right)$$","Show the integral is convergent and find the value it converges to. $$\int_1^\infty \frac{\arctan x}{x^2} ~dx$$ I have found the indefinite integral to be $$-\frac{\arctan x}{x} + \ln|x| -\frac{1}{2}\ln(x^2+1)$$ When I take the limit as t goes to $\infty$, I end up having to deal with this $$\lim_{t\to \infty}\left(-\frac{\arctan x}{x} + \ln|x| -\frac{1}{2}\ln(x^2+1)\right)$$ I know the first term goes to 0, but how do I find the limit of the rest of it? I tried combining the two terms to see if I could use L'Hospital's rule, but I couldn't get anywhere. I got to this and was not sure how to proceed. $$\lim_{t\to \infty}\left(\ln \left(\frac{|t|}{\sqrt{t^2+1}}\right)\right)$$",,"['calculus', 'real-analysis', 'limits', 'logarithms', 'improper-integrals']"
51,Prove using the formal definition of a limit that,Prove using the formal definition of a limit that,,"How would I go about proving this limit? $\lim_{x\to\infty}\frac{1}{x^4+x^2+5}=0$ so far I have: $|f(x) -L|< ϵ$ wherever $x > N$ $|\frac{1}{x^4+x^2+5} - 0| < ϵ $ wherever $x > N$ $|\frac{1}{x^4+x^2+5}| < ϵ  ,\; x > ∞$, assuming, $x > 0$ taking the absolute value $\frac{1}{x^4+x^2+5} < ϵ$ ${x^4+x^2+5}$ > $\frac{1}{ϵ}$ i am not sure whether this is correct, but what would be the next step?","How would I go about proving this limit? $\lim_{x\to\infty}\frac{1}{x^4+x^2+5}=0$ so far I have: $|f(x) -L|< ϵ$ wherever $x > N$ $|\frac{1}{x^4+x^2+5} - 0| < ϵ $ wherever $x > N$ $|\frac{1}{x^4+x^2+5}| < ϵ  ,\; x > ∞$, assuming, $x > 0$ taking the absolute value $\frac{1}{x^4+x^2+5} < ϵ$ ${x^4+x^2+5}$ > $\frac{1}{ϵ}$ i am not sure whether this is correct, but what would be the next step?",,"['real-analysis', 'limits']"
52,How to choose $\epsilon$ for limit of a variable?,How to choose  for limit of a variable?,\epsilon,"A constant number $a$ is said to be the limit of a variable $x$, if   for every preassigned arbitrarily small positive number $\epsilon$ it   is possible to indicate a value of the variable $x$ such that all   subsequent values of the variable will satisfy the inequality   $|x-a|<\epsilon$ Does this mean that $\epsilon$ will change according to the problem under consideration? For example, if the problem needs a variable $x$ in the range of $[0,2]$, then $\epsilon$ can be $0.1$ or $0.01$ but if the problem needs a variable $x$ in the range of $[0,2000]$, then $\epsilon$ can be $1$ or $2$ I am confused by the word ""preassigned"", can I just replace ""every preassigned"" with ""any"" ?","A constant number $a$ is said to be the limit of a variable $x$, if   for every preassigned arbitrarily small positive number $\epsilon$ it   is possible to indicate a value of the variable $x$ such that all   subsequent values of the variable will satisfy the inequality   $|x-a|<\epsilon$ Does this mean that $\epsilon$ will change according to the problem under consideration? For example, if the problem needs a variable $x$ in the range of $[0,2]$, then $\epsilon$ can be $0.1$ or $0.01$ but if the problem needs a variable $x$ in the range of $[0,2000]$, then $\epsilon$ can be $1$ or $2$ I am confused by the word ""preassigned"", can I just replace ""every preassigned"" with ""any"" ?",,"['calculus', 'limits']"
53,Can this be proven like this? (Limit Theory),Can this be proven like this? (Limit Theory),,"I had this question in mind when I was revisiting my earlier question: Getting stuck on an Analysis Question - Limit Theory . Now that I know $\lim\limits_{n\to\infty} \sqrt[n]{X_1\cdot X_2\cdot \cdot \cdot X_n} = a$ if $\lim\limits_{n\to\infty}X_n = a$ and $X_n > 0, a>0$. I want to see if it is okay to prove this proposition if $a=0$. My intuitive idea is this: For all positive $\epsilon$, there is some $N$, if $n \geq N, |x_n|< \epsilon$. Thus, somehow (though I don't know how to work out the details), $\lim\limits_{n\to\infty} \sqrt[n]{X_N\cdot X_{N+1}\cdot \cdot \cdot X_n} < \lim\limits_{n\to\infty} \epsilon^{\frac{n-N+1}{n}} = \epsilon$ And the previous terms ${X_1\cdot X_2\cdot \cdot \cdot X_{N-1}}$ are real. Hence, we can say $\lim\limits_{n\to\infty} \sqrt[n]{X_1\cdot X_2\cdot \cdot \cdot X_{N-1}} = 1$. Thus, establishing specific $N_1$ and $N_2$ respectively, can we conclude that $\lim\limits_{n\to\infty} \sqrt[n]{X_1\cdot X_2\cdot \cdot \cdot X_n} = 0 < \epsilon$ ? (I wish I could figure this out myself but I failed. Thanks for your help!)","I had this question in mind when I was revisiting my earlier question: Getting stuck on an Analysis Question - Limit Theory . Now that I know $\lim\limits_{n\to\infty} \sqrt[n]{X_1\cdot X_2\cdot \cdot \cdot X_n} = a$ if $\lim\limits_{n\to\infty}X_n = a$ and $X_n > 0, a>0$. I want to see if it is okay to prove this proposition if $a=0$. My intuitive idea is this: For all positive $\epsilon$, there is some $N$, if $n \geq N, |x_n|< \epsilon$. Thus, somehow (though I don't know how to work out the details), $\lim\limits_{n\to\infty} \sqrt[n]{X_N\cdot X_{N+1}\cdot \cdot \cdot X_n} < \lim\limits_{n\to\infty} \epsilon^{\frac{n-N+1}{n}} = \epsilon$ And the previous terms ${X_1\cdot X_2\cdot \cdot \cdot X_{N-1}}$ are real. Hence, we can say $\lim\limits_{n\to\infty} \sqrt[n]{X_1\cdot X_2\cdot \cdot \cdot X_{N-1}} = 1$. Thus, establishing specific $N_1$ and $N_2$ respectively, can we conclude that $\lim\limits_{n\to\infty} \sqrt[n]{X_1\cdot X_2\cdot \cdot \cdot X_n} = 0 < \epsilon$ ? (I wish I could figure this out myself but I failed. Thanks for your help!)",,"['real-analysis', 'analysis', 'limits']"
54,"Finding $\lim_{(x,y)\to (0,1)} \frac {xy - x } {x^2 + y^2 - 2y + 1} $",Finding,"\lim_{(x,y)\to (0,1)} \frac {xy - x } {x^2 + y^2 - 2y + 1} ","I'm trying to approach it using polar coordinates, but am not sure how to handle it because $r = 1$ instead of $r = 0$. Any help would be appreciated.","I'm trying to approach it using polar coordinates, but am not sure how to handle it because $r = 1$ instead of $r = 0$. Any help would be appreciated.",,"['limits', 'multivariable-calculus']"
55,Limit of function of hyperbolic,Limit of function of hyperbolic,,How can I - without using derivatives - find the limit of the function $f(x)=\frac{1}{\cosh(x)}+\log \left(\frac{\cosh(x)}{1+\cosh(x)} \right)$ as $x \to \infty$ and as $x \to -\infty$? We know that $\cosh(x) \to \infty$ as $x \to \pm \infty$ thus $\frac{1}{\cosh(x)} \to 0$ as $x \to \pm \infty$. And I imagine that $\frac{\cosh(x)}{1+\cosh(x)} \to 1$ as $x \to \pm \infty$ thus $\log\left(\frac{\cosh(x)}{1+\cosh(x)}\right) \to 0$ as $x \to \pm \infty$. Is this approach sufficiently formal? Any help is appreciated.,How can I - without using derivatives - find the limit of the function $f(x)=\frac{1}{\cosh(x)}+\log \left(\frac{\cosh(x)}{1+\cosh(x)} \right)$ as $x \to \infty$ and as $x \to -\infty$? We know that $\cosh(x) \to \infty$ as $x \to \pm \infty$ thus $\frac{1}{\cosh(x)} \to 0$ as $x \to \pm \infty$. And I imagine that $\frac{\cosh(x)}{1+\cosh(x)} \to 1$ as $x \to \pm \infty$ thus $\log\left(\frac{\cosh(x)}{1+\cosh(x)}\right) \to 0$ as $x \to \pm \infty$. Is this approach sufficiently formal? Any help is appreciated.,,"['limits', 'logarithms', 'hyperbolic-functions']"
56,limit of integral function [duplicate],limit of integral function [duplicate],,"This question already has answers here : Prove that $\lim_{a \to 0^{+}} \int_{0}^{a} \frac{1}{\sqrt{\cos(x)-\cos(a)}} \;dx=\frac{\pi}{\sqrt{2}}$ (2 answers) Closed 8 years ago . As part of an investigation, a student of mine needed to evaluate the limit,  $$\lim_{\beta\to 0+}\int_0^\beta\frac{1}{\sqrt{\cos\theta-\cos\beta}}\,d\theta.$$ Mathematica gives the answer as $\frac{\pi}{\sqrt2}$, but I couldn't show her a nice way of proving this. Is there any way to do this that a (bright) high school student might understand? Or does it necessarily involve an excursion into the land of elliptic integrals? For interest's sake: the limit arose when she was trying to see how well the circle approximates the tautochrone .","This question already has answers here : Prove that $\lim_{a \to 0^{+}} \int_{0}^{a} \frac{1}{\sqrt{\cos(x)-\cos(a)}} \;dx=\frac{\pi}{\sqrt{2}}$ (2 answers) Closed 8 years ago . As part of an investigation, a student of mine needed to evaluate the limit,  $$\lim_{\beta\to 0+}\int_0^\beta\frac{1}{\sqrt{\cos\theta-\cos\beta}}\,d\theta.$$ Mathematica gives the answer as $\frac{\pi}{\sqrt2}$, but I couldn't show her a nice way of proving this. Is there any way to do this that a (bright) high school student might understand? Or does it necessarily involve an excursion into the land of elliptic integrals? For interest's sake: the limit arose when she was trying to see how well the circle approximates the tautochrone .",,"['calculus', 'integration', 'limits']"
57,Find $\lim_{n\rightarrow\infty}\left(1-(1-\exp(tn^{-\frac{1}{v}}))^v\right)^n$,Find,\lim_{n\rightarrow\infty}\left(1-(1-\exp(tn^{-\frac{1}{v}}))^v\right)^n,"Find the limit as $n\rightarrow\infty$ of $\left(1-(1-\exp(tn^{-\frac{1}{v}}))^v\right)^n$, where $t\in(-\infty,0)$, and $v\in(0,1)$. Remarks: A non-trivial limit does exist! - verified numerically. I would like to use a similar idea to $\lim_{n\rightarrow\infty}\left(1-\frac{t}{n}\right)^n=\exp(-t)$. This standard result can be proved, for example, by taking the logarithm and using l'Hopitals rule. The method does not seem to work in this case however due to problems differentiating.","Find the limit as $n\rightarrow\infty$ of $\left(1-(1-\exp(tn^{-\frac{1}{v}}))^v\right)^n$, where $t\in(-\infty,0)$, and $v\in(0,1)$. Remarks: A non-trivial limit does exist! - verified numerically. I would like to use a similar idea to $\lim_{n\rightarrow\infty}\left(1-\frac{t}{n}\right)^n=\exp(-t)$. This standard result can be proved, for example, by taking the logarithm and using l'Hopitals rule. The method does not seem to work in this case however due to problems differentiating.",,['limits']
58,Why is the sequence $u_N = \inf\{s_n : n \gt N\}$ increasing?,Why is the sequence  increasing?,u_N = \inf\{s_n : n \gt N\},"A question in my book I am studying says to let $s_n$ and $t_n$ be sequences and suppose there exists $N_0$ such that $s_n \le t_n$ for all $n \gt N_0$. Show $\lim \inf s_n \le \lim \inf t_n$ and $\lim \sup s_n \le \lim \sup t_n$. The hint for the problem  in the back of my book says to let $u_N = \inf \{s_n : n \gt N\}$ and $w_N = \inf \{t_n : n \gt N\}$. Then it says that $u_N$ and $w_N$ are increasing sequences and $u_N \le w_N$ for all $N \gt N_0$. What I don't understand is how we can assume that $u_N$ and $w_N$ are increasing sequences. From my understanding, since we don't explicitly know what $s_n$ and $t_n$, we cannot know if these are increasing or decreasing right? Also would they still be increasing if it was defined instead as $u_N = \sup\{s_n : n \gt N\}$?","A question in my book I am studying says to let $s_n$ and $t_n$ be sequences and suppose there exists $N_0$ such that $s_n \le t_n$ for all $n \gt N_0$. Show $\lim \inf s_n \le \lim \inf t_n$ and $\lim \sup s_n \le \lim \sup t_n$. The hint for the problem  in the back of my book says to let $u_N = \inf \{s_n : n \gt N\}$ and $w_N = \inf \{t_n : n \gt N\}$. Then it says that $u_N$ and $w_N$ are increasing sequences and $u_N \le w_N$ for all $N \gt N_0$. What I don't understand is how we can assume that $u_N$ and $w_N$ are increasing sequences. From my understanding, since we don't explicitly know what $s_n$ and $t_n$, we cannot know if these are increasing or decreasing right? Also would they still be increasing if it was defined instead as $u_N = \sup\{s_n : n \gt N\}$?",,"['sequences-and-series', 'limits', 'supremum-and-infimum']"
59,Trigonometric integrals and limits,Trigonometric integrals and limits,,"Show $$\lim_{N\to\infty}g_N(\theta_N)=2\int^\pi_0\frac{\sin x}{x}dx-\pi,$$ where $$g_N(\theta_N)=\int_0^{\theta_N}\frac{\sin[(N+1/2)x]}{\sin(x/2)}dx-\pi,$$ $$\theta_N=\frac{\pi}{N+1/2},$$ and $$g_N(x)=2\sum_{n=1}^\infty\frac{\sin nx}{n}-(\pi-x).$$ So far I have: $$\lim_{N\to\infty}g_N(\theta_N)=\lim_{N\to\infty}\int_0^{\theta_N}\frac{\sin[(N+1/2)x]}{2\sin(x/2)}dx-\pi$$ Let $u=(N+1/2)x$, and $du=N+1/2$. $$\lim_{N\to\infty}\int^\pi_0\frac{\sin(u)\cdot(N+1/2)}{2\sin(\frac{u}{N+1/2})}du$$ But the integral is no simpler than when I started. How would I go about showing this? (This question is part of a series of questions demonstrating Gibbs phenomenon)","Show $$\lim_{N\to\infty}g_N(\theta_N)=2\int^\pi_0\frac{\sin x}{x}dx-\pi,$$ where $$g_N(\theta_N)=\int_0^{\theta_N}\frac{\sin[(N+1/2)x]}{\sin(x/2)}dx-\pi,$$ $$\theta_N=\frac{\pi}{N+1/2},$$ and $$g_N(x)=2\sum_{n=1}^\infty\frac{\sin nx}{n}-(\pi-x).$$ So far I have: $$\lim_{N\to\infty}g_N(\theta_N)=\lim_{N\to\infty}\int_0^{\theta_N}\frac{\sin[(N+1/2)x]}{2\sin(x/2)}dx-\pi$$ Let $u=(N+1/2)x$, and $du=N+1/2$. $$\lim_{N\to\infty}\int^\pi_0\frac{\sin(u)\cdot(N+1/2)}{2\sin(\frac{u}{N+1/2})}du$$ But the integral is no simpler than when I started. How would I go about showing this? (This question is part of a series of questions demonstrating Gibbs phenomenon)",,"['integration', 'limits', 'trigonometry', 'fourier-series']"
60,"Is this limit finite, or infinite?","Is this limit finite, or infinite?",,"Is $$\lim_{x\uparrow 1}\left(\frac{1-x}{x}\max\{nx^n|n\in\mathbb{N}\} \right)$$ infinite, or finite? ($\mathbb{N}$ is the set of the natural numbers). According to Mathematica, it looks like converging to between 0.3 and 0.4... Thanks for your help!","Is $$\lim_{x\uparrow 1}\left(\frac{1-x}{x}\max\{nx^n|n\in\mathbb{N}\} \right)$$ infinite, or finite? ($\mathbb{N}$ is the set of the natural numbers). According to Mathematica, it looks like converging to between 0.3 and 0.4... Thanks for your help!",,"['calculus', 'sequences-and-series', 'limits', 'optimization']"
61,$\int_{-1}^1\frac1f=\infty$ iff $\int_{-1}^1(u_n')^2f\to0$,iff,\int_{-1}^1\frac1f=\infty \int_{-1}^1(u_n')^2f\to0,"Let $f$ be a continuous function on $[-1,1]$ such that $f(x\neq0)>0,f(0)=0$. How can I show that $\int_{-1}^1\frac{1}{f(t)}dt=\infty$ iff there exists a sequence of functions $u_n$, $C^1$ on $[-1,1]$ such that $\int_{-1}^1 (u_n'(t))^2f(t)dt\underset{{n\infty}}\to0$ and $(u_n)$ converges pointwise to the sign function on $[-1,1]$ ? The sign function is defined by $sgn(x<0)=-1,sgn(0)=0,sgn(x>0)=1$. By $C^1$ I mean derivable with a continuous derivative. Neither way of the equivalence seems easy, and other than having confirmed that the above works both ways on easy cases (such as $f$ being $|x|$), I haven't really made any progress. What is more since the convergence is only pointwise in the reverse way, it tells us very little on the derivative of $u_n$.","Let $f$ be a continuous function on $[-1,1]$ such that $f(x\neq0)>0,f(0)=0$. How can I show that $\int_{-1}^1\frac{1}{f(t)}dt=\infty$ iff there exists a sequence of functions $u_n$, $C^1$ on $[-1,1]$ such that $\int_{-1}^1 (u_n'(t))^2f(t)dt\underset{{n\infty}}\to0$ and $(u_n)$ converges pointwise to the sign function on $[-1,1]$ ? The sign function is defined by $sgn(x<0)=-1,sgn(0)=0,sgn(x>0)=1$. By $C^1$ I mean derivable with a continuous derivative. Neither way of the equivalence seems easy, and other than having confirmed that the above works both ways on easy cases (such as $f$ being $|x|$), I haven't really made any progress. What is more since the convergence is only pointwise in the reverse way, it tells us very little on the derivative of $u_n$.",,"['calculus', 'integration', 'limits']"
62,Could I do this to an infinite series?,Could I do this to an infinite series?,,If a had two series like so: $$\lim_{n\rightarrow \infty} \sum^{n}_{i=1} i + \sum^{\infty}_{k=1} k $$ Is it logical for me to say: $$\lim_{n\rightarrow \infty} \sum^{n}_{i=1} i = \sum^{\infty}_{i=1} i $$ Therefore: $$\lim_{n\rightarrow \infty} \sum^{n}_{i=1} i + \sum^{\infty}_{k=1} k $$ $$=$$ $$ \sum^{\infty}_{k=1} k + \sum^{\infty}_{k=1} k $$ $$=$$ $$ \sum^{\infty}_{k=1} 2k$$ Is this wrong?,If a had two series like so: $$\lim_{n\rightarrow \infty} \sum^{n}_{i=1} i + \sum^{\infty}_{k=1} k $$ Is it logical for me to say: $$\lim_{n\rightarrow \infty} \sum^{n}_{i=1} i = \sum^{\infty}_{i=1} i $$ Therefore: $$\lim_{n\rightarrow \infty} \sum^{n}_{i=1} i + \sum^{\infty}_{k=1} k $$ $$=$$ $$ \sum^{\infty}_{k=1} k + \sum^{\infty}_{k=1} k $$ $$=$$ $$ \sum^{\infty}_{k=1} 2k$$ Is this wrong?,,"['sequences-and-series', 'limits']"
63,"Show, with the definition, that $\lim_\limits{ (x,y) \to (0,0)} x\sin\frac{1}{y} + y\sin\frac{1}{x}$ exist","Show, with the definition, that  exist","\lim_\limits{ (x,y) \to (0,0)} x\sin\frac{1}{y} + y\sin\frac{1}{x}","Show, with the definition, that $\lim_\limits{ (x,y) \to (0,0)} x\sin\frac{1}{y} + y\sin\frac{1}{x} $ exist; $(x,y) \in \mathbb{R^2}-\{(0,0)\}$. I think the limit is zero because for $||(x,y)|| < \delta$, $$||x\sin\frac{1}{y} + y\sin\frac{1}{x}|| \leq ||(x,y)|| \cdot||(\sin\frac{1}{y},\sin\frac{1}{x})|| \leq \sqrt{2} ||(x,y)||< \sqrt{2}\delta.$$  It is sufficient to define that $\sqrt{2}\delta = \epsilon$ I am not certain of what I did so far. Is there anyone who can give me a hint to solve the problem?","Show, with the definition, that $\lim_\limits{ (x,y) \to (0,0)} x\sin\frac{1}{y} + y\sin\frac{1}{x} $ exist; $(x,y) \in \mathbb{R^2}-\{(0,0)\}$. I think the limit is zero because for $||(x,y)|| < \delta$, $$||x\sin\frac{1}{y} + y\sin\frac{1}{x}|| \leq ||(x,y)|| \cdot||(\sin\frac{1}{y},\sin\frac{1}{x})|| \leq \sqrt{2} ||(x,y)||< \sqrt{2}\delta.$$  It is sufficient to define that $\sqrt{2}\delta = \epsilon$ I am not certain of what I did so far. Is there anyone who can give me a hint to solve the problem?",,['limits']
64,How to evaluate $\lim _{x\to \infty }\left(1+2x\sqrt{x}\right)^{2/\ln x}$?,How to evaluate ?,\lim _{x\to \infty }\left(1+2x\sqrt{x}\right)^{2/\ln x},"I have a problem with this limit, can you please show me a way to solve it without L'Hôpital's rule? $$\lim _{x\to \infty}\left(1+2x\sqrt{x}\right)^{\frac{2}{\ln x}}$$ This is my solution (it's correct, but I'd like to see another way, which doesn't use L'Hôpital's rule): $$\lim _{x\to \infty}\left(e^{\frac{2}{\ln x}\ln\left(1+2x\sqrt{x}\right)}\right)$$ $$\frac{2\ln\left(1+2x\sqrt{x}\right)}{\ln x}$$ L'Hôpital: $$\lim _{x\to \infty}\left(\frac{2\ln\left(1+2x\sqrt{x}\right)}{\ln x}\right)=\lim _{x\to \infty}\left(\frac{\frac{6\sqrt{x}}{2x^{\frac{3}{2}}+1}}{\frac{1}{x}}\right)=\lim _{x\to \infty}\left(\frac{6x^{\frac{3}{2}}}{2x^{\frac{3}{2}}+1}\right)$$ Again if you want: $$=\lim_{x\to \infty}\left(\frac{9\sqrt{x}}{3\sqrt{x}}\right)=3=\color{red}{e^3}$$ Are there other ways to solve it?","I have a problem with this limit, can you please show me a way to solve it without L'Hôpital's rule? $$\lim _{x\to \infty}\left(1+2x\sqrt{x}\right)^{\frac{2}{\ln x}}$$ This is my solution (it's correct, but I'd like to see another way, which doesn't use L'Hôpital's rule): $$\lim _{x\to \infty}\left(e^{\frac{2}{\ln x}\ln\left(1+2x\sqrt{x}\right)}\right)$$ $$\frac{2\ln\left(1+2x\sqrt{x}\right)}{\ln x}$$ L'Hôpital: $$\lim _{x\to \infty}\left(\frac{2\ln\left(1+2x\sqrt{x}\right)}{\ln x}\right)=\lim _{x\to \infty}\left(\frac{\frac{6\sqrt{x}}{2x^{\frac{3}{2}}+1}}{\frac{1}{x}}\right)=\lim _{x\to \infty}\left(\frac{6x^{\frac{3}{2}}}{2x^{\frac{3}{2}}+1}\right)$$ Again if you want: $$=\lim_{x\to \infty}\left(\frac{9\sqrt{x}}{3\sqrt{x}}\right)=3=\color{red}{e^3}$$ Are there other ways to solve it?",,"['calculus', 'real-analysis', 'limits', 'limits-without-lhopital']"
65,asymptotic behavior of the two sequences defining exponential function,asymptotic behavior of the two sequences defining exponential function,,"There are two definitions of exponential function: $$e^x=\lim_{n\to\infty} S_n=\lim_{n\to \infty} a_n \text{       ,}$$ where $$S_n=1+x+\frac{x^2}{2!}+\dots+\frac{x^n}{n!}$$ and $$a_n=(1+\frac{x}{n})^n \text{   .}$$ Since the two sequence have the same limit, I guess they are somehow related and reflect different aspects of $e^x$. So my first question is: are there any relationships between $S_n$ and $a_n$? My other questions come from the following observations: When x is positive, obviously $S_n$ is increasing. Is $a_n$ also increasing? When x is negative, $S_n$ goes up and down since it keeps adding numbers of alternating signs as as $n$ grows. Eventually $S_n$ ""squeezes"" to its limit. What about the behavior of $a_n$ in this case? When $n$ is smaller than $|x|$, I can see that $a_n$ changes signs very often. When $n$ is large, $a_n$ is always positive, and is $a_n$ increasing when $n$ is large?","There are two definitions of exponential function: $$e^x=\lim_{n\to\infty} S_n=\lim_{n\to \infty} a_n \text{       ,}$$ where $$S_n=1+x+\frac{x^2}{2!}+\dots+\frac{x^n}{n!}$$ and $$a_n=(1+\frac{x}{n})^n \text{   .}$$ Since the two sequence have the same limit, I guess they are somehow related and reflect different aspects of $e^x$. So my first question is: are there any relationships between $S_n$ and $a_n$? My other questions come from the following observations: When x is positive, obviously $S_n$ is increasing. Is $a_n$ also increasing? When x is negative, $S_n$ goes up and down since it keeps adding numbers of alternating signs as as $n$ grows. Eventually $S_n$ ""squeezes"" to its limit. What about the behavior of $a_n$ in this case? When $n$ is smaller than $|x|$, I can see that $a_n$ changes signs very often. When $n$ is large, $a_n$ is always positive, and is $a_n$ increasing when $n$ is large?",,"['real-analysis', 'limits', 'exponential-function']"
66,Applications of Dominated/Monotone convergence theorem,Applications of Dominated/Monotone convergence theorem,,"Consider a measure $\mu$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ where $\mathcal{B}(\mathbb{R})$ is the Borel $\sigma$-algebra on $\mathbb{R}$. Consider the function $f: [0,\infty)\rightarrow \{1,0\}$, $f(u)=1_{[u=0]}$ where $$ 1_{[u=0]}= \begin{cases} 1 \text{ if $u=0$}\\ 0 \text{ otherwise} \end{cases} $$ Consider the function $g: [0,\infty)\rightarrow [0,\infty)$, $g(u)=u$. Statement : (a) it is possible to construct a sequence of measurable continuous functions $\{f_m(\cdot)\}_m$ such that $\lim_{m \rightarrow \infty}f_m(u)=f(u)$ $\forall u$, and $1\geq f_m(u)\geq f_{m+1}(u)$ $\forall u,m$ (hence, we can apply dominated convergence theorem). (b) it is possible to construct a sequence of measurable continuous functions $\{g_m(\cdot)\}_m$ such that $\lim_{m \rightarrow \infty}g_m(u)=g(u) \forall u$ and $0\leq g_m(u)\leq g_{m+1}(u)$ $\forall u,m$ (hence, we can apply monotone convergence theorem). (from van der Vaart ""Asymptotic Statistics"" proof Lemma 6.4 p.89) . Question : on which result this statement is based? My attempt : I know the following two results but I'm not sure whether they fit for the statement above (1) Consider $h: \mathbb{R}\rightarrow [0,\infty)$. Construct the partition of $[0,\infty)$ in $2^{2m}+1$ intervals of length $2^{-m}$. Let $I_{m.k}$ be the $k$-th interval. Define $h_m(u)=\sum_{k=1}^{2^{2m}+1}\frac{k-1}{2^m}1_{[f^{-1}(I_{m,k})]}$. Then $\lim_{m \rightarrow \infty}h_m(u)=h(u)$ $\forall u$. (2) Consider an open set $G \in \mathcal{B}(\mathbb{R})$. Then, there exists a sequence of functions $h_m(\cdot)$ such that $0\leq h_m(u)\leq h_{m+1}(u)$ $\forall m,u$, $\lim_{m \rightarrow \infty}h_m(u)=1_{[u \in G]}$ $\forall u$ (implying that $h_m(u) \leq 1$ $\forall m,u$).","Consider a measure $\mu$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ where $\mathcal{B}(\mathbb{R})$ is the Borel $\sigma$-algebra on $\mathbb{R}$. Consider the function $f: [0,\infty)\rightarrow \{1,0\}$, $f(u)=1_{[u=0]}$ where $$ 1_{[u=0]}= \begin{cases} 1 \text{ if $u=0$}\\ 0 \text{ otherwise} \end{cases} $$ Consider the function $g: [0,\infty)\rightarrow [0,\infty)$, $g(u)=u$. Statement : (a) it is possible to construct a sequence of measurable continuous functions $\{f_m(\cdot)\}_m$ such that $\lim_{m \rightarrow \infty}f_m(u)=f(u)$ $\forall u$, and $1\geq f_m(u)\geq f_{m+1}(u)$ $\forall u,m$ (hence, we can apply dominated convergence theorem). (b) it is possible to construct a sequence of measurable continuous functions $\{g_m(\cdot)\}_m$ such that $\lim_{m \rightarrow \infty}g_m(u)=g(u) \forall u$ and $0\leq g_m(u)\leq g_{m+1}(u)$ $\forall u,m$ (hence, we can apply monotone convergence theorem). (from van der Vaart ""Asymptotic Statistics"" proof Lemma 6.4 p.89) . Question : on which result this statement is based? My attempt : I know the following two results but I'm not sure whether they fit for the statement above (1) Consider $h: \mathbb{R}\rightarrow [0,\infty)$. Construct the partition of $[0,\infty)$ in $2^{2m}+1$ intervals of length $2^{-m}$. Let $I_{m.k}$ be the $k$-th interval. Define $h_m(u)=\sum_{k=1}^{2^{2m}+1}\frac{k-1}{2^m}1_{[f^{-1}(I_{m,k})]}$. Then $\lim_{m \rightarrow \infty}h_m(u)=h(u)$ $\forall u$. (2) Consider an open set $G \in \mathcal{B}(\mathbb{R})$. Then, there exists a sequence of functions $h_m(\cdot)$ such that $0\leq h_m(u)\leq h_{m+1}(u)$ $\forall m,u$, $\lim_{m \rightarrow \infty}h_m(u)=1_{[u \in G]}$ $\forall u$ (implying that $h_m(u) \leq 1$ $\forall m,u$).",,"['sequences-and-series', 'limits', 'measure-theory', 'lebesgue-integral']"
67,Calculus - limit of a function: $\lim\limits_{x \to {\pi \over 3}} {\sin (x-{\pi \over 3})\over {1 - 2\cos x}}$,Calculus - limit of a function:,\lim\limits_{x \to {\pi \over 3}} {\sin (x-{\pi \over 3})\over {1 - 2\cos x}},"How do you compute the following limit without using the l'Hopital rule?  If you were allowed to use it, it becomes easy and the result is $\sqrt{3}\over 3$ but without it, I am not sure how to proceed. $$\lim_{x \to {\pi \over 3}} {\sin (x-{\pi \over 3})\over {1 - 2\cos x}}$$","How do you compute the following limit without using the l'Hopital rule?  If you were allowed to use it, it becomes easy and the result is $\sqrt{3}\over 3$ but without it, I am not sure how to proceed. $$\lim_{x \to {\pi \over 3}} {\sin (x-{\pi \over 3})\over {1 - 2\cos x}}$$",,"['calculus', 'limits', 'trigonometry', 'limits-without-lhopital']"
68,Showing properties of a space using dense subsets (soft),Showing properties of a space using dense subsets (soft),,"I'm noticing a lot of times during my functional analysis course, that I'm missing some calculus basics (2 years passed since my last class covering this stuff): Especially when working with Lebesgue- and Sobolev-Spaces, one often withdraws to dense subsets of those spaces in order to prove properties of the spaces itself. I try to formulate my problem as abstract as possible: Imagine there is an abstract normed space $(X,\|\cdot\|)$ and some dense subset $Y\subset X$. First of all, dense means, that I can approximate each object $x\in X$ by a sequence $\{x_n\}$ of objects in $Y$. This means that: $$x_n\overset{n\to\infty}\to x\,\text{in }\|\cdot\|\quad\Leftrightarrow\quad\lim_{n\to\infty}\|x_n-x\| = 0$$ Imagine further, that we want to show a certain property for all objects $x\in X$. Often we would then just consider the approximating sequence of objects in $Y$, and for the objects in $Y$ the property is easily shown (most of the times). In an exam I would now just write down that the property - because of density - also holds in the limit. Anyway, I'd be cheating on myself If I claim understand why exactly density is enough for this. Another point that's unclear to me is, if this procedure (of saying that it has to hold in the limit because of density) is always valid in the above abstract setting, or if there is some other assumption that has to hold. Maybe there is someone who can enlighten me a bit. EDIT: A concrete example When proving the Poincaré-inequality $\|u\|_{L^p(U)}\leq C\|Du\|_{L^p(U)}$ for $U\subset\mathbb{R}^n$, and e.g. $u\in W^{1,p}_0(U)$, one just restricts oneself to a function $v\in C^\infty_0(U)$, for which the argument can be shown. Then I have here in my notes that after taking a series of $C^\infty_0$ functions $\varphi_n(x)$ that converge to $u$ in $W^{1,1}_0(U)$: [...] We showed the claim for every $\varphi_n$. By density it has to hold also in the limit.","I'm noticing a lot of times during my functional analysis course, that I'm missing some calculus basics (2 years passed since my last class covering this stuff): Especially when working with Lebesgue- and Sobolev-Spaces, one often withdraws to dense subsets of those spaces in order to prove properties of the spaces itself. I try to formulate my problem as abstract as possible: Imagine there is an abstract normed space $(X,\|\cdot\|)$ and some dense subset $Y\subset X$. First of all, dense means, that I can approximate each object $x\in X$ by a sequence $\{x_n\}$ of objects in $Y$. This means that: $$x_n\overset{n\to\infty}\to x\,\text{in }\|\cdot\|\quad\Leftrightarrow\quad\lim_{n\to\infty}\|x_n-x\| = 0$$ Imagine further, that we want to show a certain property for all objects $x\in X$. Often we would then just consider the approximating sequence of objects in $Y$, and for the objects in $Y$ the property is easily shown (most of the times). In an exam I would now just write down that the property - because of density - also holds in the limit. Anyway, I'd be cheating on myself If I claim understand why exactly density is enough for this. Another point that's unclear to me is, if this procedure (of saying that it has to hold in the limit because of density) is always valid in the above abstract setting, or if there is some other assumption that has to hold. Maybe there is someone who can enlighten me a bit. EDIT: A concrete example When proving the Poincaré-inequality $\|u\|_{L^p(U)}\leq C\|Du\|_{L^p(U)}$ for $U\subset\mathbb{R}^n$, and e.g. $u\in W^{1,p}_0(U)$, one just restricts oneself to a function $v\in C^\infty_0(U)$, for which the argument can be shown. Then I have here in my notes that after taking a series of $C^\infty_0$ functions $\varphi_n(x)$ that converge to $u$ in $W^{1,1}_0(U)$: [...] We showed the claim for every $\varphi_n$. By density it has to hold also in the limit.",,"['functional-analysis', 'limits', 'lebesgue-integral', 'sobolev-spaces']"
69,"Hints to compute if exists $\lim_{n\to\infty}\sum_{k=1}^n\sigma(k^2)/\sum_{k=1}^n\sigma(k)$, which $\sigma(n)=\sum_{d\mid n}d$, and other question","Hints to compute if exists , which , and other question",\lim_{n\to\infty}\sum_{k=1}^n\sigma(k^2)/\sum_{k=1}^n\sigma(k) \sigma(n)=\sum_{d\mid n}d,"I would like receive hints at least for one of the following problems, these are going from experiments. Can you provide to me hints for at least one of the following problems? I will try put the answer this week. If you want provide hints for one of the problems and solve the another, too is welcome. My goal is learn and made useful post for this site: A) Prove or refute that $\exists$ a real $\delta$ such that $$\lim_{n\to\infty}\sum_{k=1}^{n}\sigma(k)-\left(\frac{n}{rad(n)}\right)^{2+\delta}\sum_{k=1}^{n}\mu(k)k=0,$$   where $\mu(m)$ is Mobius function , $\sigma(m)$ is the sum of divisors function and $rad(m)$ is defined by $rad(1)=1$ and if $m>1$ by the product of distinct primes dividing $m$, $\prod_{p\mid m}p$ (for example $rad(12)=6$). I believe that previous exercise is more difficult than this B) Compute, if exists,    $$\lim_{n\to\infty}\frac{\sum_{k=1}^n\sigma(k^2)}{\sum_{k=1}^n\sigma(k)}.$$ I believe that B) is more easy and useful currently to me. I know Bachmann's theorem about the average order of the sum of divisor function (page 60 in Apostol, Introduction to Analytic Number Theory), an how it was proved. If you want say an hint about the numerator in the limit of B), then I will try it. Thanks in advance. I excuse that this post is tagged as experimental mathematics, since I've used my computer to claim this questions, I don't know if these are in the literature.","I would like receive hints at least for one of the following problems, these are going from experiments. Can you provide to me hints for at least one of the following problems? I will try put the answer this week. If you want provide hints for one of the problems and solve the another, too is welcome. My goal is learn and made useful post for this site: A) Prove or refute that $\exists$ a real $\delta$ such that $$\lim_{n\to\infty}\sum_{k=1}^{n}\sigma(k)-\left(\frac{n}{rad(n)}\right)^{2+\delta}\sum_{k=1}^{n}\mu(k)k=0,$$   where $\mu(m)$ is Mobius function , $\sigma(m)$ is the sum of divisors function and $rad(m)$ is defined by $rad(1)=1$ and if $m>1$ by the product of distinct primes dividing $m$, $\prod_{p\mid m}p$ (for example $rad(12)=6$). I believe that previous exercise is more difficult than this B) Compute, if exists,    $$\lim_{n\to\infty}\frac{\sum_{k=1}^n\sigma(k^2)}{\sum_{k=1}^n\sigma(k)}.$$ I believe that B) is more easy and useful currently to me. I know Bachmann's theorem about the average order of the sum of divisor function (page 60 in Apostol, Introduction to Analytic Number Theory), an how it was proved. If you want say an hint about the numerator in the limit of B), then I will try it. Thanks in advance. I excuse that this post is tagged as experimental mathematics, since I've used my computer to claim this questions, I don't know if these are in the literature.",,"['limits', 'analytic-number-theory']"
70,Evaluating $\lim_{x\to 0}\frac{\sin(x)\arcsin(x)-x^2}{x^6}$ Step by Step Using L' Hopital Rule,Evaluating  Step by Step Using L' Hopital Rule,\lim_{x\to 0}\frac{\sin(x)\arcsin(x)-x^2}{x^6},The limit to be found is $$ \lim_{x\to 0}\frac{\sin(x)\arcsin(x)-x^2}{x^6}$$ I've tried l'hopital rule but it gets really messy. I've also tried splitting it into 2 limits but that doesn't work. I can't think of any meaningful substitution either. PS: I know the answer $1/18$ but I'm interested in the method. Thank you.,The limit to be found is $$ \lim_{x\to 0}\frac{\sin(x)\arcsin(x)-x^2}{x^6}$$ I've tried l'hopital rule but it gets really messy. I've also tried splitting it into 2 limits but that doesn't work. I can't think of any meaningful substitution either. PS: I know the answer $1/18$ but I'm interested in the method. Thank you.,,"['calculus', 'limits']"
71,How to Evaluate $\lim_{x\to0}\frac{(1-x)^{1/3}-1}{4^x-3^x}$? [closed],How to Evaluate ? [closed],\lim_{x\to0}\frac{(1-x)^{1/3}-1}{4^x-3^x},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to find this limit without using L'Hospital rule $$\lim_{x\to0}\frac{(1-x)^{1/3}-1}{4^x-3^x}$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to find this limit without using L'Hospital rule $$\lim_{x\to0}\frac{(1-x)^{1/3}-1}{4^x-3^x}$$",,"['limits', 'limits-without-lhopital']"
72,Checking Continuity in General.,Checking Continuity in General.,,"Suppose you have a given  Function on $\mathbb R^n$.And you want to check the continuity on a Point.How can you be sure that you checked all possible ways to approach that point$?$That method is useful for proving it does not exist but to prove it does exist you need a more general method.But all methods for continuity have ""for all ..."" in them so how can you check ""for all..."" (sequences ,Open sets,approaches..) whatever definition you wanna use.","Suppose you have a given  Function on $\mathbb R^n$.And you want to check the continuity on a Point.How can you be sure that you checked all possible ways to approach that point$?$That method is useful for proving it does not exist but to prove it does exist you need a more general method.But all methods for continuity have ""for all ..."" in them so how can you check ""for all..."" (sequences ,Open sets,approaches..) whatever definition you wanna use.",,"['real-analysis', 'analysis', 'limits', 'multivariable-calculus', 'continuity']"
73,How do you prove indeterminate form using epsilon and delta?,How do you prove indeterminate form using epsilon and delta?,,"The question, for instance, is proving $$\lim_{x\to\infty}\frac{x}{x+1}=1$$ This is my answer, which is likely incorrect. $$\forall\epsilon>0, \exists M \in \mathbb{R}$$  such that $$ x>M \Rightarrow \left|\frac{x}{x+1}-1\right|<\epsilon$$ $$\left|\frac{x}{x+1}-1\right|<\epsilon \iff \frac{1}{|x+1|}<\epsilon \iff |x+1|>\frac1\epsilon$$ I then get $x>\frac{1-\epsilon}{\epsilon}$ Picking $M=\frac{1-\epsilon}{\epsilon}$,  I get $$x>M=\frac{1-\epsilon}{\epsilon}\Rightarrow x>\frac1\epsilon - 1 \Rightarrow \cdots \Rightarrow \left|\frac{x}{x+1}-1\right|<\epsilon$$ Could anybody please point out which part should I add or fix?","The question, for instance, is proving $$\lim_{x\to\infty}\frac{x}{x+1}=1$$ This is my answer, which is likely incorrect. $$\forall\epsilon>0, \exists M \in \mathbb{R}$$  such that $$ x>M \Rightarrow \left|\frac{x}{x+1}-1\right|<\epsilon$$ $$\left|\frac{x}{x+1}-1\right|<\epsilon \iff \frac{1}{|x+1|}<\epsilon \iff |x+1|>\frac1\epsilon$$ I then get $x>\frac{1-\epsilon}{\epsilon}$ Picking $M=\frac{1-\epsilon}{\epsilon}$,  I get $$x>M=\frac{1-\epsilon}{\epsilon}\Rightarrow x>\frac1\epsilon - 1 \Rightarrow \cdots \Rightarrow \left|\frac{x}{x+1}-1\right|<\epsilon$$ Could anybody please point out which part should I add or fix?",,"['calculus', 'limits', 'epsilon-delta', 'indeterminate-forms']"
74,Limits and infinity in a succession,Limits and infinity in a succession,,"Apologies for this rather basic question. I am preparing the entry exam for university without the help of a teacher and occasionally get stuck on seemingly simple things. I have been all over the internet and cannot find out how to solve this limit: \begin{align*} \quad \lim_{n \to \infty}\frac{3^n+2^n}{5^n+3}\\  \end{align*} Does changing it to \begin{align*} \quad \lim_{n \to \infty}\frac{3^n}{5^n+3} + \lim_{n \to \infty}\frac{2^n}{5^n+3}\\  \end{align*} would help in this case? Or can I do \begin{align*} \quad \lim_{n \to \infty}\frac{1+\frac{2^n}{3^n}}{1+\frac{3}{5^n}}\\  \end{align*} I would be very, very grateful if anyone could take the time and show me step by step how this is done.","Apologies for this rather basic question. I am preparing the entry exam for university without the help of a teacher and occasionally get stuck on seemingly simple things. I have been all over the internet and cannot find out how to solve this limit: \begin{align*} \quad \lim_{n \to \infty}\frac{3^n+2^n}{5^n+3}\\  \end{align*} Does changing it to \begin{align*} \quad \lim_{n \to \infty}\frac{3^n}{5^n+3} + \lim_{n \to \infty}\frac{2^n}{5^n+3}\\  \end{align*} would help in this case? Or can I do \begin{align*} \quad \lim_{n \to \infty}\frac{1+\frac{2^n}{3^n}}{1+\frac{3}{5^n}}\\  \end{align*} I would be very, very grateful if anyone could take the time and show me step by step how this is done.",,"['limits', 'infinity']"
75,"What is $ \lim_{(x,y) \rightarrow (0,0)} \frac{x^3-y^3}{x^2-y^2} $",What is," \lim_{(x,y) \rightarrow (0,0)} \frac{x^3-y^3}{x^2-y^2} ","Find the following limit  $$ \lim_{(x,y) \rightarrow (0,0)} \frac{x^3-y^3}{x^2-y^2}$$ This question has been bugging me for some time. Couldn't find it anywhere on the Internet","Find the following limit  $$ \lim_{(x,y) \rightarrow (0,0)} \frac{x^3-y^3}{x^2-y^2}$$ This question has been bugging me for some time. Couldn't find it anywhere on the Internet",,"['limits', 'multivariable-calculus']"
76,Is $\log^2n = O(n)$ or $n = O(\log^2n)$ true?,Is  or  true?,\log^2n = O(n) n = O(\log^2n),"I'm trying to figure out if: 1) $\log^2n = O(n)$ and 2) $ n = O(\log^2n)$ are true or if one or both are false. So far I've concluded that both are false because if $n = 8$ for the first one, then $\log^2 8 = O(8)$ which is false since it simplifies to $9 = O(8)$ which does not belong to $O(n)$. For the second one, I believe it to be false as well because if $n = 1024$ or (some other big number), you get $1024 = O(\log^2 1024)$ which simplifies to $1024 = O(100)$. And $1024$ does not belong to $O(100)$. Am I right or is one of these true? Thanks.","I'm trying to figure out if: 1) $\log^2n = O(n)$ and 2) $ n = O(\log^2n)$ are true or if one or both are false. So far I've concluded that both are false because if $n = 8$ for the first one, then $\log^2 8 = O(8)$ which is false since it simplifies to $9 = O(8)$ which does not belong to $O(n)$. For the second one, I believe it to be false as well because if $n = 1024$ or (some other big number), you get $1024 = O(\log^2 1024)$ which simplifies to $1024 = O(100)$. And $1024$ does not belong to $O(100)$. Am I right or is one of these true? Thanks.",,"['calculus', 'limits', 'asymptotics']"
77,Existence of a limit [closed],Existence of a limit [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove if the following is correct or not: If $$\lim _{x\to x_0}f(x) = L \text{ and } \lim_{x\to x_1}g(x) = x_0,$$ then $$\lim_{x\to x_1} f(g(x))= L.$$ So, I guess this can be solved either by proving it or find a example that contradicts the above, so this statement is wrong.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove if the following is correct or not: If $$\lim _{x\to x_0}f(x) = L \text{ and } \lim_{x\to x_1}g(x) = x_0,$$ then $$\lim_{x\to x_1} f(g(x))= L.$$ So, I guess this can be solved either by proving it or find a example that contradicts the above, so this statement is wrong.",,['limits']
78,Proof of uniform continuity of with sequences of functions,Proof of uniform continuity of with sequences of functions,,"Let ${f_n}$ be a sequence of continuous functions such that $f_n \rightarrow f$ uniformly on $R$. Suppose that $x_n \rightarrow x_0$. Prove that $$\lim_{n\to\infty} f_n(x_n)=f(x_0)$$ I really don't know where to start on this, can someone help me prove this? Thank you!","Let ${f_n}$ be a sequence of continuous functions such that $f_n \rightarrow f$ uniformly on $R$. Suppose that $x_n \rightarrow x_0$. Prove that $$\lim_{n\to\infty} f_n(x_n)=f(x_0)$$ I really don't know where to start on this, can someone help me prove this? Thank you!",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
79,Find and prove the limit of $X_n=$ $\frac {n^{100}}{1.01^n}$,Find and prove the limit of,X_n= \frac {n^{100}}{1.01^n},"I have to find and prove the limit of the sequence $X_n=$ $\frac {n^{100}}{1.01^n}$ What is the easier way? I tried to use Bernoulli's inequality to say lim$\frac {n^{100}}{1.01^n}$ = lim$\frac {n^{100}}{(1+1/10)^n}$ and $ (1+1/10)^n \geq 1+n(1/10).$ But I could get anything.  I think another way is to use the Squeeze Theorem but I have not could find the correct sequences. Any Ideas? I only can use the definition, Squeeze Theorem, the Bernoulli's inequality or using operations to reduce the sequence.","I have to find and prove the limit of the sequence $X_n=$ $\frac {n^{100}}{1.01^n}$ What is the easier way? I tried to use Bernoulli's inequality to say lim$\frac {n^{100}}{1.01^n}$ = lim$\frac {n^{100}}{(1+1/10)^n}$ and $ (1+1/10)^n \geq 1+n(1/10).$ But I could get anything.  I think another way is to use the Squeeze Theorem but I have not could find the correct sequences. Any Ideas? I only can use the definition, Squeeze Theorem, the Bernoulli's inequality or using operations to reduce the sequence.",,"['calculus', 'sequences-and-series', 'limits']"
80,Completion of a metric space $\mathbb{R}$ with a special metric,Completion of a metric space  with a special metric,\mathbb{R},"Let $(\mathbb{R},d)$ be a metric space. Define $d(x,y) = |\tan^{-1} x - \tan^{-1} y|$. I want to prove that $(\mathbb{R},d)$ is isometric to $(\{\tan^{-1} x, x \in \mathbb{R}\}, \rho)$ where $\rho$ is a Euclidean distance. Define $f: (\mathbb{R},d) \to (\{\tan^{-1} x, x \in \mathbb{R}\}, \rho), f(x) = \tan^{-1} x$. Obviously, the function is a bijection, and it preserves metric: $d(x,y) = |\tan^{-1} x - \tan^{-1} y| = \rho(f(x),f(y))$. So the metric spaces are isometric. We know that $(\{\tan^{-1} x, x \in \mathbb{R}\}, \rho)$ is incomplete, since the sequence $x_n = \tan^{-1} n$ is Cauchy, but it converges to $\frac{\pi}{2}$ in $(\mathbb{R},\rho)$, since  $\frac{\pi}{2} \notin \{\tan^{-1} x, x \in \mathbb{R}\}$, it is incomplete. So since spaces are isometric, $(\mathbb{R},d)$ is also incomplete. I assume the completion of $(\{\tan^{-1} x, x \in \mathbb{R}\}, \rho)$ will be $(\{\tan^{-1} x, x \in \mathbb{R}\} \cup \{-\frac{\pi}{2}, \frac{\pi}{2}\}, \rho)$. What will be completion of $(\mathbb{R},d)$ then? And, please, tell me if any one my statements are wrong.","Let $(\mathbb{R},d)$ be a metric space. Define $d(x,y) = |\tan^{-1} x - \tan^{-1} y|$. I want to prove that $(\mathbb{R},d)$ is isometric to $(\{\tan^{-1} x, x \in \mathbb{R}\}, \rho)$ where $\rho$ is a Euclidean distance. Define $f: (\mathbb{R},d) \to (\{\tan^{-1} x, x \in \mathbb{R}\}, \rho), f(x) = \tan^{-1} x$. Obviously, the function is a bijection, and it preserves metric: $d(x,y) = |\tan^{-1} x - \tan^{-1} y| = \rho(f(x),f(y))$. So the metric spaces are isometric. We know that $(\{\tan^{-1} x, x \in \mathbb{R}\}, \rho)$ is incomplete, since the sequence $x_n = \tan^{-1} n$ is Cauchy, but it converges to $\frac{\pi}{2}$ in $(\mathbb{R},\rho)$, since  $\frac{\pi}{2} \notin \{\tan^{-1} x, x \in \mathbb{R}\}$, it is incomplete. So since spaces are isometric, $(\mathbb{R},d)$ is also incomplete. I assume the completion of $(\{\tan^{-1} x, x \in \mathbb{R}\}, \rho)$ will be $(\{\tan^{-1} x, x \in \mathbb{R}\} \cup \{-\frac{\pi}{2}, \frac{\pi}{2}\}, \rho)$. What will be completion of $(\mathbb{R},d)$ then? And, please, tell me if any one my statements are wrong.",,"['real-analysis', 'limits', 'metric-spaces', 'cauchy-sequences']"
81,My incorrect approach solving this limit. What am I missing?,My incorrect approach solving this limit. What am I missing?,,"I'm doing this limit: $$-1/2\lim_{n\to\infty}\,{\frac {n\left( 2\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}n-1-2\,n \right)}{n+1} } $$ I arrived to this from a bigger expression, and arrived at this point, according to maple, the limit is correctly done. The result (again, according to maple) is $-1/2$, so $$\lim_{n\to\infty}\,{\frac {n\left( 2\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}n-1-2\,n \right)}{n+1} }=1.$$ But I've done it several times, and I have obtained $-1$ every time. Here's one of my tryings: $$\lim_{n\to\infty}\,{\frac {n\left( 2\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}n-1-2\,n \right)}{n+1} }$$ We have that $$\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}\to e^2e^{-2}=1$$ So $$\lim_{n\to\infty}\,{\frac {n\left( 2\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}n-1-2\,n \right)}{n+1} }=\lim_{n\to\infty}\,{\frac {n\left( 2n-1-2\,n \right)}{n+1} }=\lim_{n\to\infty}\,{\frac {\left( -n \right)}{n+1} }=-1$$ It's obvious that I'm doing a mistake in one of those steps, but I'm not being able to find what it is. Edit: this is from my analysis course, so I can't use Taylor to solve this limit.","I'm doing this limit: $$-1/2\lim_{n\to\infty}\,{\frac {n\left( 2\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}n-1-2\,n \right)}{n+1} } $$ I arrived to this from a bigger expression, and arrived at this point, according to maple, the limit is correctly done. The result (again, according to maple) is $-1/2$, so $$\lim_{n\to\infty}\,{\frac {n\left( 2\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}n-1-2\,n \right)}{n+1} }=1.$$ But I've done it several times, and I have obtained $-1$ every time. Here's one of my tryings: $$\lim_{n\to\infty}\,{\frac {n\left( 2\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}n-1-2\,n \right)}{n+1} }$$ We have that $$\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}\to e^2e^{-2}=1$$ So $$\lim_{n\to\infty}\,{\frac {n\left( 2\,{{\rm e}^{2}} \left( {\frac {n+1}{n}}  \right) ^{-2\,n}n-1-2\,n \right)}{n+1} }=\lim_{n\to\infty}\,{\frac {n\left( 2n-1-2\,n \right)}{n+1} }=\lim_{n\to\infty}\,{\frac {\left( -n \right)}{n+1} }=-1$$ It's obvious that I'm doing a mistake in one of those steps, but I'm not being able to find what it is. Edit: this is from my analysis course, so I can't use Taylor to solve this limit.",,"['calculus', 'sequences-and-series', 'limits']"
82,Clarify books proof limit of $\frac{1}{x}$ diverging at $0$,Clarify books proof limit of  diverging at,\frac{1}{x} 0,"My book shows that the limit diverges at $0$ obviously. How they do it, though, is a bit baffling to me. Define $f:(0,1)\to \mathbb{R}$ by $f(x)=\frac{1}{x}$. $0$ is an accumulation point so we may inquire as to the existence of the limit there. Let $L$ by the limit, and choose $\epsilon>0$ such that $L+\epsilon>0$. Now, if $0<x<\frac{1}{L+\epsilon}$, then $L+\epsilon<\frac{1}{x}$; hence $|f(x)-L|<\epsilon$. Thus it is impossible to find a $\delta >0$ to fulfill the requirement of the definition. So I understand if $0<x<\frac{1}{L+\epsilon}$ is true then we have it but how can we guarantee this is the case? I need some further clarification of the details. Is there another way beginning from  $0<|x|<\delta$ implies $\left| \frac{1}{x}-L\right| < \epsilon$? Thanks for your help!","My book shows that the limit diverges at $0$ obviously. How they do it, though, is a bit baffling to me. Define $f:(0,1)\to \mathbb{R}$ by $f(x)=\frac{1}{x}$. $0$ is an accumulation point so we may inquire as to the existence of the limit there. Let $L$ by the limit, and choose $\epsilon>0$ such that $L+\epsilon>0$. Now, if $0<x<\frac{1}{L+\epsilon}$, then $L+\epsilon<\frac{1}{x}$; hence $|f(x)-L|<\epsilon$. Thus it is impossible to find a $\delta >0$ to fulfill the requirement of the definition. So I understand if $0<x<\frac{1}{L+\epsilon}$ is true then we have it but how can we guarantee this is the case? I need some further clarification of the details. Is there another way beginning from  $0<|x|<\delta$ implies $\left| \frac{1}{x}-L\right| < \epsilon$? Thanks for your help!",,"['real-analysis', 'analysis', 'limits', 'epsilon-delta']"
83,Difficult (?) limit involving exponentials,Difficult (?) limit involving exponentials,,I wish to show that $$ \lim_{n\to \infty} \left(2e^{\frac{it}{\sqrt{n}}} - e^{\frac{2it}{\sqrt{n}}}\right)^n \to e^{t^2}$$ Taylor expanding the inner expression yields $$ 1 + \frac{2-2^2}{2!}\left(\frac{it}{\sqrt{n}}\right)^2 + \cdots + \frac{2-2^k}{k!}\left(\frac{it}{\sqrt{n}}\right)^k + \cdots $$ however I'm not sure this is the way to go? I don't really know how to attack this one. Hints are most appreciated!,I wish to show that $$ \lim_{n\to \infty} \left(2e^{\frac{it}{\sqrt{n}}} - e^{\frac{2it}{\sqrt{n}}}\right)^n \to e^{t^2}$$ Taylor expanding the inner expression yields $$ 1 + \frac{2-2^2}{2!}\left(\frac{it}{\sqrt{n}}\right)^2 + \cdots + \frac{2-2^k}{k!}\left(\frac{it}{\sqrt{n}}\right)^k + \cdots $$ however I'm not sure this is the way to go? I don't really know how to attack this one. Hints are most appreciated!,,"['limits', 'exponential-function']"
84,Is it bad practice to use informal arguments when evaluating limits?,Is it bad practice to use informal arguments when evaluating limits?,,"I often see slightly informal ways of evaluating limits. For example: $$\lim_{x\to\infty} \frac{1+\frac{\sin(x)}{x}+\frac{x}{e^x}}{3+\frac{1}{x}} = \frac{1+0+0}{3+0} =\frac{1}{3}$$ I think this is not a proof, rather a bit of an sketchy argument. Is it bad practice to evaluate limits like this? Or is it okay?","I often see slightly informal ways of evaluating limits. For example: $$\lim_{x\to\infty} \frac{1+\frac{\sin(x)}{x}+\frac{x}{e^x}}{3+\frac{1}{x}} = \frac{1+0+0}{3+0} =\frac{1}{3}$$ I think this is not a proof, rather a bit of an sketchy argument. Is it bad practice to evaluate limits like this? Or is it okay?",,"['calculus', 'limits', 'soft-question']"
85,Easier method to prove limit by epsilon delta definition. [duplicate],Easier method to prove limit by epsilon delta definition. [duplicate],,"This question already has an answer here : Precise definition of limits and $\lim_{x\to-1}\frac1{\sqrt{x^2+3}}=\frac12$ (1 answer) Closed 8 years ago . supposed I wish to prove this limit via the epsilon delta definition of limits: $$\lim_{x \to -1}\frac{1}{\sqrt{x^2+3}}=\frac{1}{2}.$$ This means I have to show that: $$\bigg|\frac{1}{\sqrt{x^2+3}} - \frac{1}{2}\bigg| < \epsilon$$  whenever $0<\big|x +1\big|<\delta$. How can I proceed to choose $\delta$ in a way such that my proof will be simple? So far I have tried to work ""backwards"" but ended up all messy because of the squareroot. Any hint or help would be appreciated. Thanks!","This question already has an answer here : Precise definition of limits and $\lim_{x\to-1}\frac1{\sqrt{x^2+3}}=\frac12$ (1 answer) Closed 8 years ago . supposed I wish to prove this limit via the epsilon delta definition of limits: $$\lim_{x \to -1}\frac{1}{\sqrt{x^2+3}}=\frac{1}{2}.$$ This means I have to show that: $$\bigg|\frac{1}{\sqrt{x^2+3}} - \frac{1}{2}\bigg| < \epsilon$$  whenever $0<\big|x +1\big|<\delta$. How can I proceed to choose $\delta$ in a way such that my proof will be simple? So far I have tried to work ""backwards"" but ended up all messy because of the squareroot. Any hint or help would be appreciated. Thanks!",,"['calculus', 'limits', 'epsilon-delta']"
86,Right hand Limit of the Greatest integer function under the $\sin $ function,Right hand Limit of the Greatest integer function under the  function,\sin ,"Find the right hand limit of the given function $$\lim_{x\to 0^+}\frac{\sin [x]}{[x]}$$,Where $[.]$ denotes greatest integer function. My Attempt: I just expanded the $\sin $ function then divided it by $[x]$ Then taken the limit and found the limit as $1$, But I am not sure about my solution. Please someone help me. Thank you.","Find the right hand limit of the given function $$\lim_{x\to 0^+}\frac{\sin [x]}{[x]}$$,Where $[.]$ denotes greatest integer function. My Attempt: I just expanded the $\sin $ function then divided it by $[x]$ Then taken the limit and found the limit as $1$, But I am not sure about my solution. Please someone help me. Thank you.",,"['limits', 'continuity', 'limits-without-lhopital']"
87,strictly convex functions and limits,strictly convex functions and limits,,"Suppose I have a strictly convex function $f(x)$ for $x\geq 0$, with $f(0) = 0$, $f'(0) =0$ and $f''(0) >0$.  Is it obvious that $f$ must be superlinear as $x\to +\infty$?  Alternatively, how can I conclude that \begin{equation} \lim_{x\to \infty}\frac{f(x)}{x} = +\infty \end{equation}","Suppose I have a strictly convex function $f(x)$ for $x\geq 0$, with $f(0) = 0$, $f'(0) =0$ and $f''(0) >0$.  Is it obvious that $f$ must be superlinear as $x\to +\infty$?  Alternatively, how can I conclude that \begin{equation} \lim_{x\to \infty}\frac{f(x)}{x} = +\infty \end{equation}",,"['limits', 'convex-analysis']"
88,Evaluate $\lim\limits_{\alpha \to \infty} e^{-t\sqrt{\alpha}}(1-\frac{t}{\sqrt{\alpha}})^{-\alpha}$ [duplicate],Evaluate  [duplicate],\lim\limits_{\alpha \to \infty} e^{-t\sqrt{\alpha}}(1-\frac{t}{\sqrt{\alpha}})^{-\alpha},"This question already has an answer here : Proof for $e^z = \lim \limits_{x \rightarrow \infty} \left( 1 + \frac{z}{x} \right)^x$ (1 answer) Closed 6 years ago . How does one show $$\lim_{\alpha \to \infty} e^{-t\sqrt{\alpha}}\left(1-\frac{t}{\sqrt{\alpha}}\right)^{-\alpha} = e^{t^2 / 2}?$$ Not homework, this is from this proof that the gamma distribution has a limiting distribution of the standard normal as $\alpha \to \infty$. It suggests using numerical techniques to find the limit of the above, but I would like to know if there is a good way to solve this manually. I tried L'Hopital's rule but got $e^t$, which is obviously incorrect.","This question already has an answer here : Proof for $e^z = \lim \limits_{x \rightarrow \infty} \left( 1 + \frac{z}{x} \right)^x$ (1 answer) Closed 6 years ago . How does one show $$\lim_{\alpha \to \infty} e^{-t\sqrt{\alpha}}\left(1-\frac{t}{\sqrt{\alpha}}\right)^{-\alpha} = e^{t^2 / 2}?$$ Not homework, this is from this proof that the gamma distribution has a limiting distribution of the standard normal as $\alpha \to \infty$. It suggests using numerical techniques to find the limit of the above, but I would like to know if there is a good way to solve this manually. I tried L'Hopital's rule but got $e^t$, which is obviously incorrect.",,"['calculus', 'probability', 'limits', 'moment-generating-functions', 'gamma-distribution']"
89,Limit(s) of a Sequence from the decimal expansion of $\pi$,Limit(s) of a Sequence from the decimal expansion of,\pi,"I found a statement in a book concerning the decimal expansion of $\pi$ that I do not really understand. The statement is my problem number 2 , where problem number 1 really looks like a reference request. Here there is the setting. Take $\pi$, and construct a sequence $x_1 := 0.1$, $x_2 := 0.41$, $x_3  := 0.592$, etc, where for example the first element of the sequence is    the first digit of the decimal expansion of $\pi$, and the second element of the sequence corresponds to the second and the third digit of the decimal expansion of $\pi$. 1. Apparently, it has been proved somewhere that this sequence converges at least to a limit point. I would like to know if somebody knows about this result. 2. I do not really get how this sequence, if it converge (as it has been proved), can converge to more than one point . Is there somebody who can clarify point 2? Thank you in advance.","I found a statement in a book concerning the decimal expansion of $\pi$ that I do not really understand. The statement is my problem number 2 , where problem number 1 really looks like a reference request. Here there is the setting. Take $\pi$, and construct a sequence $x_1 := 0.1$, $x_2 := 0.41$, $x_3  := 0.592$, etc, where for example the first element of the sequence is    the first digit of the decimal expansion of $\pi$, and the second element of the sequence corresponds to the second and the third digit of the decimal expansion of $\pi$. 1. Apparently, it has been proved somewhere that this sequence converges at least to a limit point. I would like to know if somebody knows about this result. 2. I do not really get how this sequence, if it converge (as it has been proved), can converge to more than one point . Is there somebody who can clarify point 2? Thank you in advance.",,"['sequences-and-series', 'limits', 'reference-request', 'soft-question', 'pi']"
90,Show that $n^n<(n!)^2$,Show that,n^n<(n!)^2,I want to show that $\lim\limits_{n \to \infty}\frac{n^n}{(n!)^2}=0$ But I have absolutely no idea besides that $\frac{n^n}{(n!)^2}=\frac{n}{1}\cdot \frac{n}{2}\cdot ...\cdot \frac{n}{(n-1)^2}\cdot \frac{n}{n^2}$ Help me please.,I want to show that $\lim\limits_{n \to \infty}\frac{n^n}{(n!)^2}=0$ But I have absolutely no idea besides that $\frac{n^n}{(n!)^2}=\frac{n}{1}\cdot \frac{n}{2}\cdot ...\cdot \frac{n}{(n-1)^2}\cdot \frac{n}{n^2}$ Help me please.,,['limits']
91,"For fixed $x \geq 0$, find $\lim\limits_{n\to\infty}1-\left(\frac{n-\lambda}{n}\right)^{nx}$","For fixed , find",x \geq 0 \lim\limits_{n\to\infty}1-\left(\frac{n-\lambda}{n}\right)^{nx},"For fixed $x \geq 0$, find $\lim\limits_{n\to\infty}1-\left(\frac{n-\lambda}{n}\right)^{nx}.$ Clearly, the object of interest is $\lim\limits_{n\to\infty}(\frac{n-\lambda}{n})^{nx}=\lim\limits_{n\to\infty}(1-\frac{\lambda}{n})^{nx}$. This closely resembles $\lim\limits_{n\to\infty}(1-\frac{1}{n})^n=\frac{1}{e}$. Otherwise, I am lost.","For fixed $x \geq 0$, find $\lim\limits_{n\to\infty}1-\left(\frac{n-\lambda}{n}\right)^{nx}.$ Clearly, the object of interest is $\lim\limits_{n\to\infty}(\frac{n-\lambda}{n})^{nx}=\lim\limits_{n\to\infty}(1-\frac{\lambda}{n})^{nx}$. This closely resembles $\lim\limits_{n\to\infty}(1-\frac{1}{n})^n=\frac{1}{e}$. Otherwise, I am lost.",,"['real-analysis', 'limits', 'exponential-function']"
92,Prove $\lim_{x\to 0^+}{\frac{x^3}{|x|}} = 0$,Prove,\lim_{x\to 0^+}{\frac{x^3}{|x|}} = 0,"Prove $\lim_{x\to 0^+}{\frac{x^3}{|x|}} = 0$ My work: Because we are approaching $0$ from the right side, we can drop the absolute value since $x > 0$ $\forall x \in (0,+\infty)$ so we get: $f(x) = x^2$. Obviously, if we just plugged in $0$ we would see the $\lim_{x\to 0^+} {x^2} = 0$ but I want to prove this using the following definition: Let $f$ be a function defined on a subset $S$ of $\mathbb{R}$, let $a$ be a real number that is the limit of some sequence in $S$, and let $L$ be a real number. then $\lim_{x\to a^S}{f(x)} = L$ if and only if for each $\epsilon > 0$ there exists $\delta >0$ such that $x \in S$ and $|x-a| < \delta$ imply $|f(x) - L| < \epsilon$ Using this definition, we find that in order for this limit to exist, we must have that $x_0 \in S$  and $|x-0| < \delta \implies \left|\frac{x^3}{|x|} - L\right| < \epsilon$ However, I have no idea how to continue!","Prove $\lim_{x\to 0^+}{\frac{x^3}{|x|}} = 0$ My work: Because we are approaching $0$ from the right side, we can drop the absolute value since $x > 0$ $\forall x \in (0,+\infty)$ so we get: $f(x) = x^2$. Obviously, if we just plugged in $0$ we would see the $\lim_{x\to 0^+} {x^2} = 0$ but I want to prove this using the following definition: Let $f$ be a function defined on a subset $S$ of $\mathbb{R}$, let $a$ be a real number that is the limit of some sequence in $S$, and let $L$ be a real number. then $\lim_{x\to a^S}{f(x)} = L$ if and only if for each $\epsilon > 0$ there exists $\delta >0$ such that $x \in S$ and $|x-a| < \delta$ imply $|f(x) - L| < \epsilon$ Using this definition, we find that in order for this limit to exist, we must have that $x_0 \in S$  and $|x-0| < \delta \implies \left|\frac{x^3}{|x|} - L\right| < \epsilon$ However, I have no idea how to continue!",,"['real-analysis', 'limits']"
93,length of the curve $y=x^n$ in the unit square,length of the curve  in the unit square,y=x^n,"Let $l_n$ be the length of the curve $y=x^n$ in $[0,1]\times[0,1]$. Then obviously $\lim_{n\to\infty}l_n = 2$. What about $\lim_{n\to\infty}(n(2-l_n))$ ? The formula $l_n = \int_0^1\sqrt{1+n^2x^{2n-2}}\: dx$ contains (probably) an nonelementary integral...","Let $l_n$ be the length of the curve $y=x^n$ in $[0,1]\times[0,1]$. Then obviously $\lim_{n\to\infty}l_n = 2$. What about $\lim_{n\to\infty}(n(2-l_n))$ ? The formula $l_n = \int_0^1\sqrt{1+n^2x^{2n-2}}\: dx$ contains (probably) an nonelementary integral...",,"['real-analysis', 'geometry', 'limits', 'asymptotics', 'plane-curves']"
94,When is the function Continuous?,When is the function Continuous?,,"In my assignment I have to determine when is the function continuous. This is the function: \begin{equation} g(x) = \begin{cases} \left\lfloor  {\sin\frac{1}{x}}\right\rfloor&\text{if} \space x \ne 0 \\ 0 & \text{ if }x=0 \end{cases} \end{equation} I have to find when it is continuous on the interval $(\frac{1}{2\pi},\infty)$ I have proved that the function is not continuous when $x \to \frac {1}{\pi}$ and when $x\to\frac{2}{\pi}$ However, I suspect there are more that these values. How can I find where is the function continuous? Thanks.","In my assignment I have to determine when is the function continuous. This is the function: \begin{equation} g(x) = \begin{cases} \left\lfloor  {\sin\frac{1}{x}}\right\rfloor&\text{if} \space x \ne 0 \\ 0 & \text{ if }x=0 \end{cases} \end{equation} I have to find when it is continuous on the interval $(\frac{1}{2\pi},\infty)$ I have proved that the function is not continuous when $x \to \frac {1}{\pi}$ and when $x\to\frac{2}{\pi}$ However, I suspect there are more that these values. How can I find where is the function continuous? Thanks.",,"['calculus', 'limits', 'functions', 'continuity']"
95,Compute the limit of a matrix,Compute the limit of a matrix,,"We need to compute the limit of a sequence as $x \rightarrow \infty$ Using matrix-matrix multiplication we can define power as $A^p=A*\cdots*A$, $p$ times. We need to compute the limit of $A^p$ as $p \rightarrow \infty$ Now my question is, is it fine to compute some of them and telling that it will approach $0$ or not? Like for example starting from computing $A^2$, then $A^4$, $\ldots,A^{20}$. Is this good for showing a limit or not?","We need to compute the limit of a sequence as $x \rightarrow \infty$ Using matrix-matrix multiplication we can define power as $A^p=A*\cdots*A$, $p$ times. We need to compute the limit of $A^p$ as $p \rightarrow \infty$ Now my question is, is it fine to compute some of them and telling that it will approach $0$ or not? Like for example starting from computing $A^2$, then $A^4$, $\ldots,A^{20}$. Is this good for showing a limit or not?",,"['linear-algebra', 'matrices', 'limits']"
96,Limit of the expression,Limit of the expression,,I want to find following limit  $$\lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=1}^{\infty}|\ln n-\ln k|\left(1-\frac{1}{n}\right)^k=?$$ To use computer programs is also allowed. Thanks for your helps...,I want to find following limit  $$\lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=1}^{\infty}|\ln n-\ln k|\left(1-\frac{1}{n}\right)^k=?$$ To use computer programs is also allowed. Thanks for your helps...,,"['calculus', 'probability', 'sequences-and-series', 'limits']"
97,How can I evaluate the following limit-integral combination?,How can I evaluate the following limit-integral combination?,,Can you give me some hint on how to show that $$\lim_{y\to0^+}\frac{\int_0^\infty \exp(-y\cosh (x))\text dx}{\log y}=-1?$$ I tried to delimit from above and from below the function $x\mapsto−y\cosh(x)$ with some simple functions (simple means for which I can explicitly evaluate the integral by anti-differentiation) whose integrals from 0 to ∞ are asymptotic to $-\log y$ as $y$ approaches $0+$. But I failed in finding them.,Can you give me some hint on how to show that $$\lim_{y\to0^+}\frac{\int_0^\infty \exp(-y\cosh (x))\text dx}{\log y}=-1?$$ I tried to delimit from above and from below the function $x\mapsto−y\cosh(x)$ with some simple functions (simple means for which I can explicitly evaluate the integral by anti-differentiation) whose integrals from 0 to ∞ are asymptotic to $-\log y$ as $y$ approaches $0+$. But I failed in finding them.,,"['limits', 'improper-integrals', 'bessel-functions']"
98,Limit with polylog,Limit with polylog,,How do you show the following limit?  $$\lim_{x\to\infty} x\log(-e^x + 1)+\operatorname{Li}_2(e^x)-\frac12x^2=\frac{\pi^2}3$$ Where $\operatorname{Li}_n(x)$ is the polylogarithm. This question is inspired by a thread in the sagemath mailinglist.,How do you show the following limit?  $$\lim_{x\to\infty} x\log(-e^x + 1)+\operatorname{Li}_2(e^x)-\frac12x^2=\frac{\pi^2}3$$ Where $\operatorname{Li}_n(x)$ is the polylogarithm. This question is inspired by a thread in the sagemath mailinglist.,,"['calculus', 'limits', 'asymptotics', 'polylogarithm']"
99,Radius and interval of convergence of $\sum_{n=1}^{\infty}(-1)^n\frac{x^{2n}}{(2n)!}$ by root and ratio test are different?,Radius and interval of convergence of  by root and ratio test are different?,\sum_{n=1}^{\infty}(-1)^n\frac{x^{2n}}{(2n)!},$$\sum_{n=1}^{\infty}(-1)^n\frac{x^{2n}}{(2n)!}$$ By using ratio test $$\lim_{n\to\infty}\frac{x^{2(n+1)}}{(2(n+1))!}\frac{(2n)!}{x^{2n}}=\lim_{n\to\infty}\frac{x^2}{(2n+2)(2n+1)}=0$$ By using root test $$\lim_{n\to\infty}\sqrt[n]{\frac{x^{2n}}{(2n)!}}=\lim_{n\to\infty}\frac{x^2}{{(2n)!}^{\frac1n}} = x^2$$ By using root test then the series convergent only when $0<x<1$ while with ratio test x can be any real number. What is the mistake I made here. I assume that (I can't prove it's true) $\lim_{n\to\infty}{{(2n)!}^{\frac1n}}=1$. It is the part that I can't make sure as same as $\lim_{n\to\infty}\frac{n!}{n^n}=0$.,$$\sum_{n=1}^{\infty}(-1)^n\frac{x^{2n}}{(2n)!}$$ By using ratio test $$\lim_{n\to\infty}\frac{x^{2(n+1)}}{(2(n+1))!}\frac{(2n)!}{x^{2n}}=\lim_{n\to\infty}\frac{x^2}{(2n+2)(2n+1)}=0$$ By using root test $$\lim_{n\to\infty}\sqrt[n]{\frac{x^{2n}}{(2n)!}}=\lim_{n\to\infty}\frac{x^2}{{(2n)!}^{\frac1n}} = x^2$$ By using root test then the series convergent only when $0<x<1$ while with ratio test x can be any real number. What is the mistake I made here. I assume that (I can't prove it's true) $\lim_{n\to\infty}{{(2n)!}^{\frac1n}}=1$. It is the part that I can't make sure as same as $\lim_{n\to\infty}\frac{n!}{n^n}=0$.,,"['sequences-and-series', 'limits']"
