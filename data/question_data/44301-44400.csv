,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,the action of a Lie algebra on itself,the action of a Lie algebra on itself,,"Currently I've read the first 5, almost 6 chapters of Serre's ""Lie algebras and Lie groups"". Let $L$ be a Lie algebra, and suppose $L$ is given as a subalgebra of $End(V)$, where $V$ is some finite dimensional vector space over an alg. closed field $k$. Now, as usual one can identify $End(V) = V\otimes V^*$, where the isomorphism is given on simple tensors by sending $v\otimes w$ to the endomorphism $\psi_{v\otimes w} : z\mapsto w(z)\cdot v$. Thus, $L$ is a subalgebra of $V\otimes V^*$, and it acts on $V\otimes V^*$ through its action on $V$ and hence on $V^*$, and hence on $V\otimes V^*$. I was trying to convince myself that through this action, $L$ is stable under the action of itself. A priori, there are a number of ways one might view $L\subset End(V)$ acting on $V^*$. For example, at first I thought the right action is just by ""precomposition"", ie given $f\in V^*$ and $\varphi\in End(V)$, $\varphi f = f\circ\varphi$. However, this turns out to be wrong. The right one is given by: $$\varphi.f := - f\circ\varphi$$ Secondly, given an action of $\varphi$ on $V$, and on $W$, there is a natural action of $\varphi$ on $V\otimes W$ given on simple tensors by $\varphi(v\otimes w) = \varphi(v)\otimes\varphi(w)$, but again this turns out to be the wrong action. The right one in our context is instead $$\varphi.(v\otimes w) = \varphi.v\otimes w + v\otimes\varphi.w$$ In the case $W = V^*$, if one views $v\otimes w$ as the endomorphism $\psi_{v\otimes w} : z\mapsto w(z)\cdot v$, then we get: $$\varphi.(v\otimes w) = \varphi\circ \psi_{v\otimes w} + \psi_{v\otimes\varphi.w} = \varphi\circ\psi_{v\otimes w} - \psi_{v\otimes w}\circ\varphi$$ Thus, at last, if $x,\varphi\in End(V)$, then we get $$\varphi.x = \varphi\circ x + x\circ\varphi = [\varphi,x]$$ Thus the action of $\varphi\in End(V)$ on $End(V)$ is just the adjoint action. In particular, $L$ is invariant under $L$. I suppose my question is - ""Why?"" As a novice to Lie algebras, this all seems rather strange to me. Why ""should"" $L$ leave itself invariant, viewed as a subspace of $End(V) = V\otimes V^*$? Intuitively, if $L$ is the tangent space of a certain Lie group, what is the meaning of this action of $L$ on itself? In the case $L = End(V)$, what do the other ""natural"" actions of $End(V)$ on itself by left composition, right composition, or both, mean in the Lie group context? I suppose I'd welcome some examples that might shed light on how to think of these computations. I get the feeling that a lot of this will become clear once I get into the Lie group parts of Serre's book, so I almost considered not posting this question, but I'm posting it anyway, partially just as a reminder to myself to continue to think about these questions as I continue reading.","Currently I've read the first 5, almost 6 chapters of Serre's ""Lie algebras and Lie groups"". Let $L$ be a Lie algebra, and suppose $L$ is given as a subalgebra of $End(V)$, where $V$ is some finite dimensional vector space over an alg. closed field $k$. Now, as usual one can identify $End(V) = V\otimes V^*$, where the isomorphism is given on simple tensors by sending $v\otimes w$ to the endomorphism $\psi_{v\otimes w} : z\mapsto w(z)\cdot v$. Thus, $L$ is a subalgebra of $V\otimes V^*$, and it acts on $V\otimes V^*$ through its action on $V$ and hence on $V^*$, and hence on $V\otimes V^*$. I was trying to convince myself that through this action, $L$ is stable under the action of itself. A priori, there are a number of ways one might view $L\subset End(V)$ acting on $V^*$. For example, at first I thought the right action is just by ""precomposition"", ie given $f\in V^*$ and $\varphi\in End(V)$, $\varphi f = f\circ\varphi$. However, this turns out to be wrong. The right one is given by: $$\varphi.f := - f\circ\varphi$$ Secondly, given an action of $\varphi$ on $V$, and on $W$, there is a natural action of $\varphi$ on $V\otimes W$ given on simple tensors by $\varphi(v\otimes w) = \varphi(v)\otimes\varphi(w)$, but again this turns out to be the wrong action. The right one in our context is instead $$\varphi.(v\otimes w) = \varphi.v\otimes w + v\otimes\varphi.w$$ In the case $W = V^*$, if one views $v\otimes w$ as the endomorphism $\psi_{v\otimes w} : z\mapsto w(z)\cdot v$, then we get: $$\varphi.(v\otimes w) = \varphi\circ \psi_{v\otimes w} + \psi_{v\otimes\varphi.w} = \varphi\circ\psi_{v\otimes w} - \psi_{v\otimes w}\circ\varphi$$ Thus, at last, if $x,\varphi\in End(V)$, then we get $$\varphi.x = \varphi\circ x + x\circ\varphi = [\varphi,x]$$ Thus the action of $\varphi\in End(V)$ on $End(V)$ is just the adjoint action. In particular, $L$ is invariant under $L$. I suppose my question is - ""Why?"" As a novice to Lie algebras, this all seems rather strange to me. Why ""should"" $L$ leave itself invariant, viewed as a subspace of $End(V) = V\otimes V^*$? Intuitively, if $L$ is the tangent space of a certain Lie group, what is the meaning of this action of $L$ on itself? In the case $L = End(V)$, what do the other ""natural"" actions of $End(V)$ on itself by left composition, right composition, or both, mean in the Lie group context? I suppose I'd welcome some examples that might shed light on how to think of these computations. I get the feeling that a lot of this will become clear once I get into the Lie group parts of Serre's book, so I almost considered not posting this question, but I'm posting it anyway, partially just as a reminder to myself to continue to think about these questions as I continue reading.",,"['abstract-algebra', 'algebraic-geometry', 'lie-groups', 'lie-algebras']"
1,Difficult ring-theory problem,Difficult ring-theory problem,,"Let $(R,+,\cdot)$ be a ring with at least 2 elements. If we know that $R$ is not a field and $x^2=x$ for any $x \in R$, where $x$ is not invertible, prove that: a) $a+x$ is not invertible, $\forall a,x\in R$, where $a$ is invertible and $x$ is not invertible, $x \neq0$ b)$x^2=x, \forall x\in R$ My solution, which is not correct: a) Let $U(R)$ be the group of invertible elements from $R$. Obviously, if $a \in R$ is invertible, then $a^{-1}$ is invertible, too. Also,  for any $t \in U(R)$ and for any $s \in R-U(R)$, we have that $t \cdot s \in R-U(R)$. $(1)$ We suppose $a+x \in U(R)$. $a+x=a \cdot(1+a^{-1}x)$ Hence $1+a^{-1}x \in U(R)$, which means that $a^{-1}x \in U(R)$, contradiction with $(1)$. So our supposition is false. It follows that $a+x \in R-U(R)$. (This is wrong, take $\mathbb{Z}_{6}$ for instance) b) Let $y \in R-U(R), y\neq0$. From a), we have that $1+y$ is not invertible. Then $(1+y)^2=1+y\Leftrightarrow 2y=0$. Let $a$ be an invertible element. $(a+y)^2=a+y$ $ \Rightarrow a^2+ay+ya+y^2=a+y$ $ \Rightarrow a^2+ay+ya+=a$ I thought that $R$ should be commutative, hence $ay+ya=ya+ya=2ya=2y\cdot a=0\cdot a=0$ . So, $a^2=a$, and thus we obtain that $a=1$ is the only invertible element. Knowing that $x^2=x, \forall x \in R-U(R)$ and that $1^2=1$, we obtained that $x^2=x, \forall x \in R$","Let $(R,+,\cdot)$ be a ring with at least 2 elements. If we know that $R$ is not a field and $x^2=x$ for any $x \in R$, where $x$ is not invertible, prove that: a) $a+x$ is not invertible, $\forall a,x\in R$, where $a$ is invertible and $x$ is not invertible, $x \neq0$ b)$x^2=x, \forall x\in R$ My solution, which is not correct: a) Let $U(R)$ be the group of invertible elements from $R$. Obviously, if $a \in R$ is invertible, then $a^{-1}$ is invertible, too. Also,  for any $t \in U(R)$ and for any $s \in R-U(R)$, we have that $t \cdot s \in R-U(R)$. $(1)$ We suppose $a+x \in U(R)$. $a+x=a \cdot(1+a^{-1}x)$ Hence $1+a^{-1}x \in U(R)$, which means that $a^{-1}x \in U(R)$, contradiction with $(1)$. So our supposition is false. It follows that $a+x \in R-U(R)$. (This is wrong, take $\mathbb{Z}_{6}$ for instance) b) Let $y \in R-U(R), y\neq0$. From a), we have that $1+y$ is not invertible. Then $(1+y)^2=1+y\Leftrightarrow 2y=0$. Let $a$ be an invertible element. $(a+y)^2=a+y$ $ \Rightarrow a^2+ay+ya+y^2=a+y$ $ \Rightarrow a^2+ay+ya+=a$ I thought that $R$ should be commutative, hence $ay+ya=ya+ya=2ya=2y\cdot a=0\cdot a=0$ . So, $a^2=a$, and thus we obtain that $a=1$ is the only invertible element. Knowing that $x^2=x, \forall x \in R-U(R)$ and that $1^2=1$, we obtained that $x^2=x, \forall x \in R$",,"['abstract-algebra', 'ring-theory']"
2,A short exact sequence with $M=M_1 \oplus M_2$ that does not split,A short exact sequence with  that does not split,M=M_1 \oplus M_2,"A sequence of $R$-modules of the form   $$0 \to M_1 \stackrel{f}{\to} M \stackrel{g}{\to} M_2 \to 0$$   is called a short exact sequence (ses) if $f$ is injective, $g$ is surjective and $\operatorname{Im} f = \operatorname{Ker} g$. A short exact sequence is said to split if there exist a $R$-homomorphism $h: M_2 \to M$ such that $g\circ h = \operatorname{Id}_{M_2}$. It is well known that if a short exact sequence split then $M \cong M_1 \oplus M_2$. What I am interested in is the converse. I know that the converse is not true. That is, there exist a short exact sequence with $M \cong M_1 \oplus M_2$ which does not split. I wish to construct such a counter example. Consider $R=\mathbb{Z}$, $M_1=\mathbb{Z}/2\mathbb{Z}$ and $M'=\mathbb{Z}/4\mathbb{Z}$. Suppose we have a $\mathbb{Z}$-module $N$ such that $M_1\oplus N \cong N$ and $M' \oplus N \cong N$. Define $M= M'\oplus N$, and $M_2=M_1\oplus N$. Then $$M_1\oplus M_2 \cong M_1 \oplus N \cong N \cong M' \oplus N = M$$ Then consider the sequence $$0 \to M_1 \stackrel{f}{\to} M \stackrel{g}{\to} M_2 \to 0$$ where $f:M_1\to M=M''\oplus N$ is defined as $f(\bar{1})=(\bar{2},0)$ and $g: M \to M_2$ as $g(\bar{x},n)=(\bar{x},n)$. Then clearly $f$ is injective and $g$ is surjective and $\operatorname{Im} f = \operatorname{Ker} g$. Suppose the ses splits. Then $h :M_2 \to M$ is such that $g\circ h =\operatorname{Id}_{M_2}$. But then $$h(\bar{1},0) \in  g^{-1}(\bar{1},0)=\{\bar{1},0),(\bar{3},0)\}$$ But $2(\bar{1},0)=(\bar{2},0)$ and $2(\bar{3},0)=(\bar{2},0)$ in $M$. Thus $$h(\bar{0},0)=2h(\bar{1},0)=(\bar{2},0)$$ which contradicts the fact that $h$ is a homomorphism. Thus the ses does not splits. The only thing that remains to show the existence of such an $N$. However, I am no able to show that such an $N$ does exists. Any help/ suggestions.","A sequence of $R$-modules of the form   $$0 \to M_1 \stackrel{f}{\to} M \stackrel{g}{\to} M_2 \to 0$$   is called a short exact sequence (ses) if $f$ is injective, $g$ is surjective and $\operatorname{Im} f = \operatorname{Ker} g$. A short exact sequence is said to split if there exist a $R$-homomorphism $h: M_2 \to M$ such that $g\circ h = \operatorname{Id}_{M_2}$. It is well known that if a short exact sequence split then $M \cong M_1 \oplus M_2$. What I am interested in is the converse. I know that the converse is not true. That is, there exist a short exact sequence with $M \cong M_1 \oplus M_2$ which does not split. I wish to construct such a counter example. Consider $R=\mathbb{Z}$, $M_1=\mathbb{Z}/2\mathbb{Z}$ and $M'=\mathbb{Z}/4\mathbb{Z}$. Suppose we have a $\mathbb{Z}$-module $N$ such that $M_1\oplus N \cong N$ and $M' \oplus N \cong N$. Define $M= M'\oplus N$, and $M_2=M_1\oplus N$. Then $$M_1\oplus M_2 \cong M_1 \oplus N \cong N \cong M' \oplus N = M$$ Then consider the sequence $$0 \to M_1 \stackrel{f}{\to} M \stackrel{g}{\to} M_2 \to 0$$ where $f:M_1\to M=M''\oplus N$ is defined as $f(\bar{1})=(\bar{2},0)$ and $g: M \to M_2$ as $g(\bar{x},n)=(\bar{x},n)$. Then clearly $f$ is injective and $g$ is surjective and $\operatorname{Im} f = \operatorname{Ker} g$. Suppose the ses splits. Then $h :M_2 \to M$ is such that $g\circ h =\operatorname{Id}_{M_2}$. But then $$h(\bar{1},0) \in  g^{-1}(\bar{1},0)=\{\bar{1},0),(\bar{3},0)\}$$ But $2(\bar{1},0)=(\bar{2},0)$ and $2(\bar{3},0)=(\bar{2},0)$ in $M$. Thus $$h(\bar{0},0)=2h(\bar{1},0)=(\bar{2},0)$$ which contradicts the fact that $h$ is a homomorphism. Thus the ses does not splits. The only thing that remains to show the existence of such an $N$. However, I am no able to show that such an $N$ does exists. Any help/ suggestions.",,"['abstract-algebra', 'modules', 'exact-sequence']"
3,Representation theory: An origin story.,Representation theory: An origin story.,,"I read that: Dedekind made the observation: taking the multiplication table of a finite group $G$ and turning it into a matrix $X_G$ by replacing each entry $g$ of the table by a variable $x_g$. The determinant of $X_G$ factors into a product of irreducible polynomials, each of which occurs with multiplicity equal to its degree. For a second I couldn't think of what 'multiplicity equal to its degree meant' and then I realised that they must mean I have $f_1^{a_1}f_2^{a_2}f_3^{a_3}$ where $f_i$ are irreducible, and each have degree $a_i$. So I went to check this in an easy case, to make sure I understood, and I see that for $\Bbb Z_3$: $$\begin{matrix}&0&1&2\\0&0&1&2\\1&1&2&0\\2&2&0&1\end{matrix} \mapsto \begin{bmatrix}x_0&x_1&x_2\\x_1&x_2&x_0\\x_2&x_0&x_1\end{bmatrix}$$ Where the determinant of this is $$-x_0^3-x_1^3-x_2^3+3x_0x_1x_2=-(x_0+x_1+x_2)(x_0^2+x_1^2+x_2^2-x_0x_1-x_0x_2-x_1x_2)$$ which I believe are irreducible. But the latter term has degree $2$ and multiplicity $1$. What has gone wrong?","I read that: Dedekind made the observation: taking the multiplication table of a finite group $G$ and turning it into a matrix $X_G$ by replacing each entry $g$ of the table by a variable $x_g$. The determinant of $X_G$ factors into a product of irreducible polynomials, each of which occurs with multiplicity equal to its degree. For a second I couldn't think of what 'multiplicity equal to its degree meant' and then I realised that they must mean I have $f_1^{a_1}f_2^{a_2}f_3^{a_3}$ where $f_i$ are irreducible, and each have degree $a_i$. So I went to check this in an easy case, to make sure I understood, and I see that for $\Bbb Z_3$: $$\begin{matrix}&0&1&2\\0&0&1&2\\1&1&2&0\\2&2&0&1\end{matrix} \mapsto \begin{bmatrix}x_0&x_1&x_2\\x_1&x_2&x_0\\x_2&x_0&x_1\end{bmatrix}$$ Where the determinant of this is $$-x_0^3-x_1^3-x_2^3+3x_0x_1x_2=-(x_0+x_1+x_2)(x_0^2+x_1^2+x_2^2-x_0x_1-x_0x_2-x_1x_2)$$ which I believe are irreducible. But the latter term has degree $2$ and multiplicity $1$. What has gone wrong?",,"['abstract-algebra', 'matrices', 'representation-theory']"
4,"how many ""pure ternary"" operators are there?","how many ""pure ternary"" operators are there?",,"Pure ternary operator is a function $T:A\times A\times A \to A$ such that for every pair of binary operators $f,g:A\times A \to A$ and for each of the following inequalities $f(g(x,y),z) \ne T(x,y,z)$ $f(x,g(y,z)) \ne T(x,y,z)$ there is a triplet $x,y,z\in A$ for which the inequality holds Basically I cannot replace T with two binary operators in a simple composition. Such ternary functions exist (for the set of size two): $$T(1,0,0)=1  \quad T(0,0,0)=0 \quad T(0,1,0)=1 \quad T(1,0,1)=1  \quad T(0,1,1)=0 \\ T(1,1,1)=0\;or\; 1 \quad T(1,1,0) = 0 \quad T(0,0,1)=1 \quad $$ Lets rule out the possibility the inner binary function is applied on the left two elements. Then the outer function has for the zero as its right argument two possible outcomes, so the inner function has to map the left(1,0) and (0,1) to the same element. But if that is true, than (1,0,1) and (0,1,1) needs to be mapped on the same element, but they are not. The possibility of the inner function being applied on the right two elements can't happen neither. (0,1,0) and (0,1,1) have different images, but (0,1,0) and (0,0,1) have the same image, so the right (1,0) and (0,1) has to be mapped identically. That would imply (1,1,0) and (1,0,1) have the same image, but they do not. So no matter what operators I choose, I cannot use them in ""simple"" composition to obtain this ternary operator. Hope my proof is correct. And such a way of constructing the ""pure ternary"" function could be (I think) applied for larger sets as well. I tried to google more information about it, but I was unsuccessful. Mostly I ended it up on topic related to Are all n-ary operators simply compositions of binary operators? . But I am interested only in this kind of composition. So my question: Is there a way to calculate (without going through all possibilities) the number of ""pure ternary"" operators on a set of size $n$? I really do not know how to approach this. EDIT: Example of such a operator is incorrect, it works for a different definition. Please I will try to make correction, but I need some time, I was playing with for too long and lost it. EDIT2: Based on Bram28 comment I changed the definition. I have been actually working with this definition anyway. This more strict version should be feasible at least on constructing the examples.","Pure ternary operator is a function $T:A\times A\times A \to A$ such that for every pair of binary operators $f,g:A\times A \to A$ and for each of the following inequalities $f(g(x,y),z) \ne T(x,y,z)$ $f(x,g(y,z)) \ne T(x,y,z)$ there is a triplet $x,y,z\in A$ for which the inequality holds Basically I cannot replace T with two binary operators in a simple composition. Such ternary functions exist (for the set of size two): $$T(1,0,0)=1  \quad T(0,0,0)=0 \quad T(0,1,0)=1 \quad T(1,0,1)=1  \quad T(0,1,1)=0 \\ T(1,1,1)=0\;or\; 1 \quad T(1,1,0) = 0 \quad T(0,0,1)=1 \quad $$ Lets rule out the possibility the inner binary function is applied on the left two elements. Then the outer function has for the zero as its right argument two possible outcomes, so the inner function has to map the left(1,0) and (0,1) to the same element. But if that is true, than (1,0,1) and (0,1,1) needs to be mapped on the same element, but they are not. The possibility of the inner function being applied on the right two elements can't happen neither. (0,1,0) and (0,1,1) have different images, but (0,1,0) and (0,0,1) have the same image, so the right (1,0) and (0,1) has to be mapped identically. That would imply (1,1,0) and (1,0,1) have the same image, but they do not. So no matter what operators I choose, I cannot use them in ""simple"" composition to obtain this ternary operator. Hope my proof is correct. And such a way of constructing the ""pure ternary"" function could be (I think) applied for larger sets as well. I tried to google more information about it, but I was unsuccessful. Mostly I ended it up on topic related to Are all n-ary operators simply compositions of binary operators? . But I am interested only in this kind of composition. So my question: Is there a way to calculate (without going through all possibilities) the number of ""pure ternary"" operators on a set of size $n$? I really do not know how to approach this. EDIT: Example of such a operator is incorrect, it works for a different definition. Please I will try to make correction, but I need some time, I was playing with for too long and lost it. EDIT2: Based on Bram28 comment I changed the definition. I have been actually working with this definition anyway. This more strict version should be feasible at least on constructing the examples.",,"['abstract-algebra', 'combinatorics']"
5,Connections between finite and Lie groups in Fulton and Harris,Connections between finite and Lie groups in Fulton and Harris,,"In Representation Theory: A First Course by William Fulton and Joe Harris, the following appears on the first page about Lie Groups From a naive point of view, Lie groups seem to stand at the opposite end of the spectrum of groups from finite $\text{ones}^1$ with the superscript referencing the following note $^1$ In spite of this, there are deep, if only partially understood, relations between finite and Lie groups, extending even to their simple group classifications. Would someone shed a little more light on this? Specifically: Why is it naive to think of Lie Groups as being at the opposite end of the spectrum of groups from finite groups? What are the relations between finite and Lie groups that the authors are referencing? In what way are these relations only partially understood?","In Representation Theory: A First Course by William Fulton and Joe Harris, the following appears on the first page about Lie Groups From a naive point of view, Lie groups seem to stand at the opposite end of the spectrum of groups from finite $\text{ones}^1$ with the superscript referencing the following note $^1$ In spite of this, there are deep, if only partially understood, relations between finite and Lie groups, extending even to their simple group classifications. Would someone shed a little more light on this? Specifically: Why is it naive to think of Lie Groups as being at the opposite end of the spectrum of groups from finite groups? What are the relations between finite and Lie groups that the authors are referencing? In what way are these relations only partially understood?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'representation-theory', 'lie-groups']"
6,Examples of proper loops in $\mathbb{R}$,Examples of proper loops in,\mathbb{R},"A loop $(L, \cdot)$ is a binary structure that satisfies every group axiom except for the associative property. A loop which is not a group is called a proper loop . A topological loop $(L,\cdot)$ is a topological space which is also a loop such that   ""$\cdot$"" and the inverse operations are continuous. In the literature there are many examples of finite proper loops, but I couldn't find any example on $\mathbb{R}$. So my question is What are some examples of proper topological loop structures on $\mathbb{R}$? The operations here need to be continuous with respect to the usual topology of $\mathbb{R}$.  If the loop  happens to be commutative, even better. I am just trying to picture them, because a continuous group structure  in $\mathbb{R}$ is, basically, just the addition and there is nothing counterintuitive about it. However, for continuous loops defined on $\mathbb{R}$ there is a very deep theory behind their classification, so examples of them could be very enlightening. Edit No2: As Eric noticed some of the efforts made towards the classification of continuous loops may provide us with examples. Here is a collection of the most relevant results, which were found in Chapter 18 of the book ""Loops in Group and Lie Theory"" , by Nagy and Strambach. (Thm. 18.18) A topological loop   on $\mathbb{R}$ is a proper loop if and only if the group $G$ generated by its translations is not locally compact ($G$ is equipped with the Arens topology). (by Hoffman, pg. 243) A monassociative (a special type of loop) loop on $\mathbb{R}$ is either a Lie group, or the union of two one-parameter semigroups meeting in the unit $e$, each being isomorphic to the semigroup of positive real numbers with respect to the addition. The first one is impressive, but difficult for me to work with. The second one is very promising to provide an example. So $L=A\cup B$, $A\cap B=\{e\}$ and each component is just the $[0,\infty)$. What boggles me here is that is that we don't seem to know enough to be able to reconstruct the operation ""$\cdot$"" on $L=A\cup B$ from the available data. For $x, y\in A$, we know what $x\cdot y$ is equal to, it's just $f(x)+f(y)$ where $f:(A, \cdot) \rightarrow [0,\infty)$ is our isomorphism. Similarly, for $x, y\in B$.   But for pairs $(a,b)$ such that $a\in A$ and $b\in B$, what can we tell for their product $a\cdot b$?","A loop $(L, \cdot)$ is a binary structure that satisfies every group axiom except for the associative property. A loop which is not a group is called a proper loop . A topological loop $(L,\cdot)$ is a topological space which is also a loop such that   ""$\cdot$"" and the inverse operations are continuous. In the literature there are many examples of finite proper loops, but I couldn't find any example on $\mathbb{R}$. So my question is What are some examples of proper topological loop structures on $\mathbb{R}$? The operations here need to be continuous with respect to the usual topology of $\mathbb{R}$.  If the loop  happens to be commutative, even better. I am just trying to picture them, because a continuous group structure  in $\mathbb{R}$ is, basically, just the addition and there is nothing counterintuitive about it. However, for continuous loops defined on $\mathbb{R}$ there is a very deep theory behind their classification, so examples of them could be very enlightening. Edit No2: As Eric noticed some of the efforts made towards the classification of continuous loops may provide us with examples. Here is a collection of the most relevant results, which were found in Chapter 18 of the book ""Loops in Group and Lie Theory"" , by Nagy and Strambach. (Thm. 18.18) A topological loop   on $\mathbb{R}$ is a proper loop if and only if the group $G$ generated by its translations is not locally compact ($G$ is equipped with the Arens topology). (by Hoffman, pg. 243) A monassociative (a special type of loop) loop on $\mathbb{R}$ is either a Lie group, or the union of two one-parameter semigroups meeting in the unit $e$, each being isomorphic to the semigroup of positive real numbers with respect to the addition. The first one is impressive, but difficult for me to work with. The second one is very promising to provide an example. So $L=A\cup B$, $A\cap B=\{e\}$ and each component is just the $[0,\infty)$. What boggles me here is that is that we don't seem to know enough to be able to reconstruct the operation ""$\cdot$"" on $L=A\cup B$ from the available data. For $x, y\in A$, we know what $x\cdot y$ is equal to, it's just $f(x)+f(y)$ where $f:(A, \cdot) \rightarrow [0,\infty)$ is our isomorphism. Similarly, for $x, y\in B$.   But for pairs $(a,b)$ such that $a\in A$ and $b\in B$, what can we tell for their product $a\cdot b$?",,"['abstract-algebra', 'examples-counterexamples', 'quasigroups', 'grouplike-elements']"
7,How can $G$ a simple group always be isomorphic to $Z_p$ for some prime $p$?,How can  a simple group always be isomorphic to  for some prime ?,G Z_p p,"Is it not a necessary condition that the order of two groups must be equal for them to be isomorphic? Does ""$G$ is a simple group of odd order"" somehow imply ""$|G|$ is prime""?"" If not, I don't see how it can't be the case that $|G|$ is some odd composite number, so it couldn't be isomorphic to any $Z_p$ where $p$ is prime.","Is it not a necessary condition that the order of two groups must be equal for them to be isomorphic? Does ""$G$ is a simple group of odd order"" somehow imply ""$|G|$ is prime""?"" If not, I don't see how it can't be the case that $|G|$ is some odd composite number, so it couldn't be isomorphic to any $Z_p$ where $p$ is prime.",,['abstract-algebra']
8,Valuations of integer valued polynomials,Valuations of integer valued polynomials,,"Consider the ring $R=\text{Int}(\mathbb Z):=\{p(x)\in \mathbb Q[x]\ |\ p(n)\in \mathbb Z, \forall n\in \mathbb Z \}$. Let $K$ denote the fraction field of $R$. Fix an $a\in \mathbb Z$ and let $P$ be the prime ideal defined by   $P:=\{q(x)\in R\ | \ q(a)\equiv 0 (\text{mod } p)\}$ and let $R_P$ denote the localization of $R$ at $P$. Let $\Gamma$ denote the totally ordered group $\mathbb Z \times \mathbb Z$, where the operation is componentwise addition and the order is the lexicographic one. Find a surjective valuation $v:K\rightarrow \Gamma$ such that its corresponding valuation ring is precisely $R_P$. Now when trying to find $v\left(\frac{f(x)}{g(x)}\right)=(c_1,c_2)\in \mathbb Z\times \mathbb Z$, I thought of using a similar expression as for the $p$-adic valuation on $\mathbb Q$, namely define $c_2=e_p(f(a))-e_p(g(a))$ for one of the components, however the issue with that is that it yields a positive value even when $\frac{f(x)}{g(x)}$ is not in $R_P$, namely say if $e_p(f(a))=2, e_p(g(a))=1$. I have been trying to find some nice functions $P(X,Y)$ so that $P(X_1+X_2,Y_1+Y_2)=P(X_1,Y_1)+P(X_2,Y_2)$ which would help here, but none of them were surjective on $\mathbb Z$.","Consider the ring $R=\text{Int}(\mathbb Z):=\{p(x)\in \mathbb Q[x]\ |\ p(n)\in \mathbb Z, \forall n\in \mathbb Z \}$. Let $K$ denote the fraction field of $R$. Fix an $a\in \mathbb Z$ and let $P$ be the prime ideal defined by   $P:=\{q(x)\in R\ | \ q(a)\equiv 0 (\text{mod } p)\}$ and let $R_P$ denote the localization of $R$ at $P$. Let $\Gamma$ denote the totally ordered group $\mathbb Z \times \mathbb Z$, where the operation is componentwise addition and the order is the lexicographic one. Find a surjective valuation $v:K\rightarrow \Gamma$ such that its corresponding valuation ring is precisely $R_P$. Now when trying to find $v\left(\frac{f(x)}{g(x)}\right)=(c_1,c_2)\in \mathbb Z\times \mathbb Z$, I thought of using a similar expression as for the $p$-adic valuation on $\mathbb Q$, namely define $c_2=e_p(f(a))-e_p(g(a))$ for one of the components, however the issue with that is that it yields a positive value even when $\frac{f(x)}{g(x)}$ is not in $R_P$, namely say if $e_p(f(a))=2, e_p(g(a))=1$. I have been trying to find some nice functions $P(X,Y)$ so that $P(X_1+X_2,Y_1+Y_2)=P(X_1,Y_1)+P(X_2,Y_2)$ which would help here, but none of them were surjective on $\mathbb Z$.",,"['abstract-algebra', 'commutative-algebra', 'valuation-theory', 'integer-valued-polynomials']"
9,Fundamental theorem of finite abelian groups proof,Fundamental theorem of finite abelian groups proof,,"Can someone give a proof of the ""technical part"" of the following proof? One direction seems pretty trivial, but the other I am having trouble with. Why does it matter that the subgroup $H$ we choose is maximal?","Can someone give a proof of the ""technical part"" of the following proof? One direction seems pretty trivial, but the other I am having trouble with. Why does it matter that the subgroup $H$ we choose is maximal?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
10,Find an example of degree-100 extension of $\Bbb Q(\zeta_5)$ and $\Bbb Q(\sqrt[3]{2})$.,Find an example of degree-100 extension of  and .,\Bbb Q(\zeta_5) \Bbb Q(\sqrt[3]{2}),"I am trying to find an example of degree-100 extension of $\Bbb Q(\zeta_5)$ and an example of degree-100 extension of $\Bbb Q(\sqrt[3]{2})$. For the example of degree-100 extension of $\Bbb Q(\sqrt[3]{2}),$ I suspect that $\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3})$ is an example. Since $3$ and $100$ are co-prime, that means I can prove $[\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3}):\Bbb Q]$ is at least $300$ and at most $300.$  Therefore $[\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3}):\Bbb Q]=300.$ Then by the tower law, problem solved. For the example of degree-100 extension of $\Bbb Q(\zeta_5)$, I really don't know that how to find (and prove) such an example. Note: I haven't learnt Galois Theory. So please don't use that.Thanks so much.","I am trying to find an example of degree-100 extension of $\Bbb Q(\zeta_5)$ and an example of degree-100 extension of $\Bbb Q(\sqrt[3]{2})$. For the example of degree-100 extension of $\Bbb Q(\sqrt[3]{2}),$ I suspect that $\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3})$ is an example. Since $3$ and $100$ are co-prime, that means I can prove $[\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3}):\Bbb Q]$ is at least $300$ and at most $300.$  Therefore $[\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3}):\Bbb Q]=300.$ Then by the tower law, problem solved. For the example of degree-100 extension of $\Bbb Q(\zeta_5)$, I really don't know that how to find (and prove) such an example. Note: I haven't learnt Galois Theory. So please don't use that.Thanks so much.",,"['abstract-algebra', 'field-theory', 'extension-field', 'irreducible-polynomials']"
11,Simple Modules and Ideals,Simple Modules and Ideals,,"It is pretty routine to show that every simple left $R$-module is cyclically generated by any nonzero element and that these simple modules are precisely of the form $R/I$, where $I$ is some maximal ideal. My question is what happens when you start to 'stitch' these together: let $S_1,S_2$ are distinct (in that they are not isomorphic) simple left $R$-modules. Then these are of the form $R/I, R/J$, where $I,J$ are maximal left ideals, respectively. My thought it is that $S_1 \oplus S_2 \cong R/(I \cap J)$. This makes since in that if $R$ were semisimple, then it is a sum of simple modules and its jacobson radical (the intersection of all maximal left ideals) is zero. Then extending my idea above, $I \cap J$ does seem to be the correct ideal. But I am unable to prove this. My idea was to define a surjective map $\phi: R \to R/I \times R/J$ and use the first isomorphism theorem but the only obvious map is $r \mapsto (r,r)$ and its not clear that is surjective. [For a bit I thought it wouldn't be but perhaps it, non-obviously is?] I could go the other way, $\psi: R/I \times R/J \to R/(I \cap J)$ via $(r,s) \mapsto rs+I\cap J$ (easy enough to show this is well-defined since this vanishes on $I \cap J$). This map is pretty clearly surjective. But injective? If $(r,s)$ were to map to zero, then $rs \in I \cap J$. But beyond that I'm not sure where to go with this. If the ring were commutative, I would have more since maximal implies prime and this would be something. (since this doesn't work, I'm really out of ideas) Any ideas on how to prove this or can $S_1 \oplus S_2$ not be represented this way?","It is pretty routine to show that every simple left $R$-module is cyclically generated by any nonzero element and that these simple modules are precisely of the form $R/I$, where $I$ is some maximal ideal. My question is what happens when you start to 'stitch' these together: let $S_1,S_2$ are distinct (in that they are not isomorphic) simple left $R$-modules. Then these are of the form $R/I, R/J$, where $I,J$ are maximal left ideals, respectively. My thought it is that $S_1 \oplus S_2 \cong R/(I \cap J)$. This makes since in that if $R$ were semisimple, then it is a sum of simple modules and its jacobson radical (the intersection of all maximal left ideals) is zero. Then extending my idea above, $I \cap J$ does seem to be the correct ideal. But I am unable to prove this. My idea was to define a surjective map $\phi: R \to R/I \times R/J$ and use the first isomorphism theorem but the only obvious map is $r \mapsto (r,r)$ and its not clear that is surjective. [For a bit I thought it wouldn't be but perhaps it, non-obviously is?] I could go the other way, $\psi: R/I \times R/J \to R/(I \cap J)$ via $(r,s) \mapsto rs+I\cap J$ (easy enough to show this is well-defined since this vanishes on $I \cap J$). This map is pretty clearly surjective. But injective? If $(r,s)$ were to map to zero, then $rs \in I \cap J$. But beyond that I'm not sure where to go with this. If the ring were commutative, I would have more since maximal implies prime and this would be something. (since this doesn't work, I'm really out of ideas) Any ideas on how to prove this or can $S_1 \oplus S_2$ not be represented this way?",,"['abstract-algebra', 'ring-theory', 'modules']"
12,Extension of divisible fields,Extension of divisible fields,,"Assume that $F$ is an infinite subfield of a field $K$ such that its multiplicative group, $F^\times$, is divisible. Also, $a\in K$ and $[F(a):F]<\infty$. Can we conclude that the  multiplicative group of $F(a)$ is a divisible group? For example the root closure of $\mathbb Q$ in $\mathbb C$, is a divisible field. Can we obtain that any finite extension of this field is a divisible field?","Assume that $F$ is an infinite subfield of a field $K$ such that its multiplicative group, $F^\times$, is divisible. Also, $a\in K$ and $[F(a):F]<\infty$. Can we conclude that the  multiplicative group of $F(a)$ is a divisible group? For example the root closure of $\mathbb Q$ in $\mathbb C$, is a divisible field. Can we obtain that any finite extension of this field is a divisible field?",,"['abstract-algebra', 'group-theory', 'field-theory', 'divisible-groups']"
13,How can i prove that the finite extension field of real number is itself or the field which is isomorphic to complex number?,How can i prove that the finite extension field of real number is itself or the field which is isomorphic to complex number?,,"How can i prove that the finite extension field of real number is itself or the field which is isomorphic to complex number ? In deed, this example is included in Fraleght . Abstract Algebra text. I did try the followings:  $\mathbb{R}$ is real number. Then $\mathbb{C}$ is explassd as the smallest extension field including $ \mathbb{R} \cup ${$i$} How about considering this set . Let set $\mathbb{H}$ is the smallest field including  $\mathbb{R} \cup${$i,j,k$} where $i, j, k$ are called Hamilton number or quaternion their square are equal to $-1$. Firstly, I do know that this set is a ring. But i check that this set is a field. Of course, $\mathbb{H}$ may be not a field. Becasue, if that is true, then The Fraleght text book is wrong. However, I would like to know the specific reasons and Example's solution . Please help me to get this.","How can i prove that the finite extension field of real number is itself or the field which is isomorphic to complex number ? In deed, this example is included in Fraleght . Abstract Algebra text. I did try the followings:  $\mathbb{R}$ is real number. Then $\mathbb{C}$ is explassd as the smallest extension field including $ \mathbb{R} \cup ${$i$} How about considering this set . Let set $\mathbb{H}$ is the smallest field including  $\mathbb{R} \cup${$i,j,k$} where $i, j, k$ are called Hamilton number or quaternion their square are equal to $-1$. Firstly, I do know that this set is a ring. But i check that this set is a field. Of course, $\mathbb{H}$ may be not a field. Becasue, if that is true, then The Fraleght text book is wrong. However, I would like to know the specific reasons and Example's solution . Please help me to get this.",,['abstract-algebra']
14,Can group actions be generalized to encompass a large class of algebraic objects?,Can group actions be generalized to encompass a large class of algebraic objects?,,"Vector spaces can be thought of as a sort of canonical field action on an Abelian group, likewise modules can be thought of as the action of a ring on a group. 1. How far can this be generalized? For example, can a ring be thought of as the action of a group on its set endowed with a different group structure? Can we think of free vector spaces as the action of a field on a set? I know that a group is the group action of itself on itself (I think). Perhaps better phrased: (Rephrased) Which algebraic objects can not be thought of as the action of one type of algebraic structure on another algebraic structure? This question might be too broad or general as written, in which case an answer to the question: (Alternate) Is there a formal/rigorous notion of ""field action"" generalizing ""group action"" such that vector spaces are exactly the action of a field on an Abelian group? would suffice. Note: This question seems related: 'Free Vector Space' and 'Vector Space' When I say ""action"" I guess I mean just a functions from one space onto another. I might also have something in mind like the notion of group object from category theory, but I am not aware of a notion of ""ring objects"" or ""field objects"" in category theory, so such a characterization might be lacking.","Vector spaces can be thought of as a sort of canonical field action on an Abelian group, likewise modules can be thought of as the action of a ring on a group. 1. How far can this be generalized? For example, can a ring be thought of as the action of a group on its set endowed with a different group structure? Can we think of free vector spaces as the action of a field on a set? I know that a group is the group action of itself on itself (I think). Perhaps better phrased: (Rephrased) Which algebraic objects can not be thought of as the action of one type of algebraic structure on another algebraic structure? This question might be too broad or general as written, in which case an answer to the question: (Alternate) Is there a formal/rigorous notion of ""field action"" generalizing ""group action"" such that vector spaces are exactly the action of a field on an Abelian group? would suffice. Note: This question seems related: 'Free Vector Space' and 'Vector Space' When I say ""action"" I guess I mean just a functions from one space onto another. I might also have something in mind like the notion of group object from category theory, but I am not aware of a notion of ""ring objects"" or ""field objects"" in category theory, so such a characterization might be lacking.",,"['abstract-algebra', 'soft-question']"
15,Decomposition of a module over an integral domain,Decomposition of a module over an integral domain,,"If $M$ is an $R$-module over an integral domain $R$, then must it be true that $M\cong \mathrm{Tor}(M) \oplus M/\mathrm{Tor}(M)$? I am interested in the case where $M$ has finite rank $n$, if that simplifies things. $M$ having rank $n$ means that $M$ has a maximal $R$-linearly independent set of size $n$.","If $M$ is an $R$-module over an integral domain $R$, then must it be true that $M\cong \mathrm{Tor}(M) \oplus M/\mathrm{Tor}(M)$? I am interested in the case where $M$ has finite rank $n$, if that simplifies things. $M$ having rank $n$ means that $M$ has a maximal $R$-linearly independent set of size $n$.",,"['abstract-algebra', 'ring-theory', 'modules']"
16,"Galois group of $\Bbb Q(\{\sqrt[2^n]{2}, \zeta_{2^n} \;:\; n \geq 1 \})$ over $\Bbb Q$",Galois group of  over,"\Bbb Q(\{\sqrt[2^n]{2}, \zeta_{2^n} \;:\; n \geq 1 \}) \Bbb Q","Let $K_n = \Bbb Q(\sqrt[2^n]{2}, \zeta_{2^n})$ be a Galois extension of $\Bbb Q$ (where $ \zeta_{2^n}=e^{2\pi i / 2^n}$), and let $K$ be the compositum of all the $K_n$'s. It is a Galois extension of $\Bbb Q$. Notice that $m ≤ n \implies K_m \subset K_n$. What does the Galois group of $K/\Bbb Q$ look like? According to the proposition $1.1.$ of this document, $\text{Gal}(K/\Bbb Q)$ is the inverse limit of $\text{Gal}(L/\Bbb Q)$ where $L/\Bbb Q$ is a finite Galois extension such that $L \subseteq K$. In particular, we have to consider $L=K_n$, which has Galois group isomorphic to the ""affine group"" $\Bbb Z/2^n\Bbb Z \rtimes (\Bbb Z/2^n\Bbb Z)^{\times}$ (the holomorph of $\Bbb Z/2^n\Bbb Z$). What other subextensions should I consider? Moreover, I have some trouble as for understanding the inverse limit of all these Galois groups... Some related questions are: (1) , (2) , (3) . Here is a question with a similar infinite extension. Any help would be highly appreciated. Thank you in advance!","Let $K_n = \Bbb Q(\sqrt[2^n]{2}, \zeta_{2^n})$ be a Galois extension of $\Bbb Q$ (where $ \zeta_{2^n}=e^{2\pi i / 2^n}$), and let $K$ be the compositum of all the $K_n$'s. It is a Galois extension of $\Bbb Q$. Notice that $m ≤ n \implies K_m \subset K_n$. What does the Galois group of $K/\Bbb Q$ look like? According to the proposition $1.1.$ of this document, $\text{Gal}(K/\Bbb Q)$ is the inverse limit of $\text{Gal}(L/\Bbb Q)$ where $L/\Bbb Q$ is a finite Galois extension such that $L \subseteq K$. In particular, we have to consider $L=K_n$, which has Galois group isomorphic to the ""affine group"" $\Bbb Z/2^n\Bbb Z \rtimes (\Bbb Z/2^n\Bbb Z)^{\times}$ (the holomorph of $\Bbb Z/2^n\Bbb Z$). What other subextensions should I consider? Moreover, I have some trouble as for understanding the inverse limit of all these Galois groups... Some related questions are: (1) , (2) , (3) . Here is a question with a similar infinite extension. Any help would be highly appreciated. Thank you in advance!",,"['abstract-algebra', 'field-theory', 'galois-theory', 'profinite-groups']"
17,If $B$ is an ideal of $A$ then $B[x]$ is an ideal of $A[x]$ - what's wrong with my proof?,If  is an ideal of  then  is an ideal of  - what's wrong with my proof?,B A B[x] A[x],"This is exercise E.2 from chapter 24 of Pinter's A Book of Abstract Algebra : If $B$ is an ideal of $A$, $B[x]$ is not necessarily an ideal of $A[x]$. Give an example to prove this contention. It seems pretty easy to me to construct a proof that $B[x]$ is indeed an ideal of $A[x]$, so I would like to know what's wrong with it: First we want to show that $B[x]$ is a subgroup of $A[x]$ under addition: Let $p, q \in B[x]$. To calculate $p+q$ we simply add the corresponding coefficients. Since the coefficients are in $B$ and $B$ is a subgroup, the coefficients of $p+q$ belong to $B$ and so $p+q\in B[x]$. Let $p \in B[x]$. Again, since $B$ is a subgroup, $-p$ is in $B[x]$. Now I show that, given any $p \in B[x]$ and $r \in A[x]$, $pr$ and $rp$ are in $B[x]$. The coefficients of $pr$ are given by $$(pr)_i = \sum_{j+k=i} p_j r_k = \sum_{j=0}^i p_j r_{i-j}$$ Each term of the sum is a product of some $p_j$, which is in $B$, and some element of $A$. Since $B$ is an ideal, all $p_j r_k$ are in $B$, and so is the sum; therefore, $pr \in B[x]$. The same argument works for $rp$. I seem to have proved that $B[x]$ is an ideal of $A[x]$. Where have I gone wrong?","This is exercise E.2 from chapter 24 of Pinter's A Book of Abstract Algebra : If $B$ is an ideal of $A$, $B[x]$ is not necessarily an ideal of $A[x]$. Give an example to prove this contention. It seems pretty easy to me to construct a proof that $B[x]$ is indeed an ideal of $A[x]$, so I would like to know what's wrong with it: First we want to show that $B[x]$ is a subgroup of $A[x]$ under addition: Let $p, q \in B[x]$. To calculate $p+q$ we simply add the corresponding coefficients. Since the coefficients are in $B$ and $B$ is a subgroup, the coefficients of $p+q$ belong to $B$ and so $p+q\in B[x]$. Let $p \in B[x]$. Again, since $B$ is a subgroup, $-p$ is in $B[x]$. Now I show that, given any $p \in B[x]$ and $r \in A[x]$, $pr$ and $rp$ are in $B[x]$. The coefficients of $pr$ are given by $$(pr)_i = \sum_{j+k=i} p_j r_k = \sum_{j=0}^i p_j r_{i-j}$$ Each term of the sum is a product of some $p_j$, which is in $B$, and some element of $A$. Since $B$ is an ideal, all $p_j r_k$ are in $B$, and so is the sum; therefore, $pr \in B[x]$. The same argument works for $rp$. I seem to have proved that $B[x]$ is an ideal of $A[x]$. Where have I gone wrong?",,"['abstract-algebra', 'polynomials', 'ring-theory', 'ideals']"
18,Is every unramified extension of DVRs simple?,Is every unramified extension of DVRs simple?,,"Let $A$ be a discrete valuation ring with maximal ideal $\mathfrak{m}$, fraction field $K$, and $L$ a finite separable extension of $K$ degree $n$, unramified w.r.t. $A$. Let $B$ be the integral closure of $A$ in $L$. Is it true that $B$ has the form $A[x]/(f)$ for some $f\in A[x]$? Here's what I have so far: Certainly $B$ is finite etale over $A$ of degree $n$, and if $\alpha\in B$ is a generator of $L$ over $K$ with monic minimal polynomial $f\in A[x]$, then $A[x]/(f)$ is finite flat over $A$ of degree $n$, so if $A[x]/(f)$ is etale over $A$, then the natural injection $A[x]/(f)\hookrightarrow B$ would have to be etale of rank 1, hence an isomorphism. Thus, it suffices to show that $A[x]/(f)$ is etale over $A$, or equivalently, that the image $\overline{f}$ of $f$ in $(A/\mathfrak{m})[x]$ is a separable polynomial. Another question: is the unramified condition necessary? What's an example of a finite non-simple extension of DVR's?","Let $A$ be a discrete valuation ring with maximal ideal $\mathfrak{m}$, fraction field $K$, and $L$ a finite separable extension of $K$ degree $n$, unramified w.r.t. $A$. Let $B$ be the integral closure of $A$ in $L$. Is it true that $B$ has the form $A[x]/(f)$ for some $f\in A[x]$? Here's what I have so far: Certainly $B$ is finite etale over $A$ of degree $n$, and if $\alpha\in B$ is a generator of $L$ over $K$ with monic minimal polynomial $f\in A[x]$, then $A[x]/(f)$ is finite flat over $A$ of degree $n$, so if $A[x]/(f)$ is etale over $A$, then the natural injection $A[x]/(f)\hookrightarrow B$ would have to be etale of rank 1, hence an isomorphism. Thus, it suffices to show that $A[x]/(f)$ is etale over $A$, or equivalently, that the image $\overline{f}$ of $f$ in $(A/\mathfrak{m})[x]$ is a separable polynomial. Another question: is the unramified condition necessary? What's an example of a finite non-simple extension of DVR's?",,"['abstract-algebra', 'number-theory', 'algebraic-geometry', 'commutative-algebra']"
19,When is a 5th degree polynomial with at least 1 non-real root solvable by radicals?,When is a 5th degree polynomial with at least 1 non-real root solvable by radicals?,,"Let $f(X)$ be an irreducible polynomial of degree 5 with coefficents in the field of rational numbers $\mathbb{Q}$. Assume that $f$ has at least one non-real root in the complex field $\mathbb{C}$. Assume further that the discriminant of $f$ is a square in $\mathbb{Q}$. Let $r$ be a root of $f$, and let $K$ be the field $\mathbb{Q}(r)$, so that $f$ factors in $K[X]$ as $$f = (X − r)g$$ with $g$ of degree 4. Prove that $f$ is solvable by radicals if and only if $g$ is reducible in $K[X]$. My thoughts: I know that since the discriminant of $f$ is a square in $\mathbb{Q}$, $G\subset A_5,$ and since $f$ is irreducible, $G$ must be transitive. So we must have that either $G\cong A_5$ or $G\cong D_5$ (the dihedral group of order 10). Moreover, since char $\mathbb{Q}=0,$ we know that $f$ is solvable by radicals if and only if $G$ is solvable. Since $A_5$ is simple and non-abelian, $A_5$ is not solvable. On the other hand, $D_5$ is solvable: $D_5$ has an element $r$ with $r^5=e$, and $[D_5 : \left<r\right>]=2,$ so $\left<r\right>\triangleleft D_5.$ Now $$D_5\triangleright\left<r\right>\triangleright\{e\}.$$  So it suffices to show that $g$ is reducible in $K[X]$ if and only if $G\cong D_5.$ If $f$ has exactly 2 non-real roots, then the only nontrivial $\mathbb{Q}$-automorphism is complex conjugation, so $G$ contains a transposition, which implies $G\cong S_5$. Thus, $f$ must have exactly one real root and 4 non-real roots. This is where I'm stuck. How can we relate the reducibility of $g$ in $K[X]$ to the Galois group of $f$?","Let $f(X)$ be an irreducible polynomial of degree 5 with coefficents in the field of rational numbers $\mathbb{Q}$. Assume that $f$ has at least one non-real root in the complex field $\mathbb{C}$. Assume further that the discriminant of $f$ is a square in $\mathbb{Q}$. Let $r$ be a root of $f$, and let $K$ be the field $\mathbb{Q}(r)$, so that $f$ factors in $K[X]$ as $$f = (X − r)g$$ with $g$ of degree 4. Prove that $f$ is solvable by radicals if and only if $g$ is reducible in $K[X]$. My thoughts: I know that since the discriminant of $f$ is a square in $\mathbb{Q}$, $G\subset A_5,$ and since $f$ is irreducible, $G$ must be transitive. So we must have that either $G\cong A_5$ or $G\cong D_5$ (the dihedral group of order 10). Moreover, since char $\mathbb{Q}=0,$ we know that $f$ is solvable by radicals if and only if $G$ is solvable. Since $A_5$ is simple and non-abelian, $A_5$ is not solvable. On the other hand, $D_5$ is solvable: $D_5$ has an element $r$ with $r^5=e$, and $[D_5 : \left<r\right>]=2,$ so $\left<r\right>\triangleleft D_5.$ Now $$D_5\triangleright\left<r\right>\triangleright\{e\}.$$  So it suffices to show that $g$ is reducible in $K[X]$ if and only if $G\cong D_5.$ If $f$ has exactly 2 non-real roots, then the only nontrivial $\mathbb{Q}$-automorphism is complex conjugation, so $G$ contains a transposition, which implies $G\cong S_5$. Thus, $f$ must have exactly one real root and 4 non-real roots. This is where I'm stuck. How can we relate the reducibility of $g$ in $K[X]$ to the Galois group of $f$?",,"['abstract-algebra', 'polynomials', 'galois-theory', 'solvable-groups', 'quintics']"
20,Counter-examples of Galois Correspondence,Counter-examples of Galois Correspondence,,What are some examples of a separable field extension $L/K$ and a subgroup $H$ of $\text{Aut}(L/K)$ such that $\text{Aut}(L/L^H) \neq H$? Here $L^H$ means the fixed field of $H$.,What are some examples of a separable field extension $L/K$ and a subgroup $H$ of $\text{Aut}(L/K)$ such that $\text{Aut}(L/L^H) \neq H$? Here $L^H$ means the fixed field of $H$.,,"['abstract-algebra', 'field-theory', 'galois-theory']"
21,"If $f^*:\mathrm{Hom}(H, R) \to \mathrm{Hom}(G, R)$ is an iso for all $R$, is $f: G\to H$ an iso?","If  is an iso for all , is  an iso?","f^*:\mathrm{Hom}(H, R) \to \mathrm{Hom}(G, R) R f: G\to H","Let $G$, $H$ be abelian groups and $f: G\to H$ a homomorphism. Assume that $f^*: \mathrm{Hom}(H, R) \to \mathrm{Hom}(G, R)$ (as morphisms of abelian groups, taking $R$ with its additive group structure) is an isomorphism for all commutative rings (with 1) $R$. Is then $f$ an isomorphism as well? The question is motivated by a question on homological algebra, see here .","Let $G$, $H$ be abelian groups and $f: G\to H$ a homomorphism. Assume that $f^*: \mathrm{Hom}(H, R) \to \mathrm{Hom}(G, R)$ (as morphisms of abelian groups, taking $R$ with its additive group structure) is an isomorphism for all commutative rings (with 1) $R$. Is then $f$ an isomorphism as well? The question is motivated by a question on homological algebra, see here .",,['abstract-algebra']
22,Decompose $V \otimes V \otimes V$ into irreducible representations of $SL_2(\mathbb{R})$,Decompose  into irreducible representations of,V \otimes V \otimes V SL_2(\mathbb{R}),"Let $V=\mathbb{C^2}$ be the standard representation of $SL_2(\mathbb{R})$ Decompose $V \otimes V \otimes V$ into irreducible representations of $SL_2(\mathbb{R})$ I will just consider $SL_2(\mathbb{C})$ since there is a 1:1 correspondence between $SL_2(\mathbb{R})$ and $SL_2(\mathbb{C})$ Weight vectors of $V$ are $v_1$ and $v_{-1}$ with weights 1 and -1 respectively Weight vectors of $V \otimes V \otimes V$: Weight 3: $v_1 \otimes v_1 \otimes v_1$ Weight 1: $v_1 \otimes v_1 \otimes v_{-1}$, $v_1 \otimes v_{-1} \otimes v_1$ and $v_{-1} \otimes v_1 \otimes v_1$ Weight -1: $v_1 \otimes v_{-1} \otimes v_{-1}$, $v_{-1}\otimes v_{-1} \otimes v_1$ and $v_{-1} \otimes v_1 \otimes v_{-1}$ Weight -3: $v_{-1} \otimes v_{-1} \otimes v_{-1}$ How do I use this information to find the irreducible representations?","Let $V=\mathbb{C^2}$ be the standard representation of $SL_2(\mathbb{R})$ Decompose $V \otimes V \otimes V$ into irreducible representations of $SL_2(\mathbb{R})$ I will just consider $SL_2(\mathbb{C})$ since there is a 1:1 correspondence between $SL_2(\mathbb{R})$ and $SL_2(\mathbb{C})$ Weight vectors of $V$ are $v_1$ and $v_{-1}$ with weights 1 and -1 respectively Weight vectors of $V \otimes V \otimes V$: Weight 3: $v_1 \otimes v_1 \otimes v_1$ Weight 1: $v_1 \otimes v_1 \otimes v_{-1}$, $v_1 \otimes v_{-1} \otimes v_1$ and $v_{-1} \otimes v_1 \otimes v_1$ Weight -1: $v_1 \otimes v_{-1} \otimes v_{-1}$, $v_{-1}\otimes v_{-1} \otimes v_1$ and $v_{-1} \otimes v_1 \otimes v_{-1}$ Weight -3: $v_{-1} \otimes v_{-1} \otimes v_{-1}$ How do I use this information to find the irreducible representations?",,"['abstract-algebra', 'vector-spaces', 'representation-theory', 'lie-groups']"
23,Finding the kernel of an epimorphism onto $S_3$?,Finding the kernel of an epimorphism onto ?,S_3,"Let $\Lambda$ denote the group with presentation $\langle a,b \mid   abab^{-1}a^{-1}b^{-1}\rangle$. We define the following epimorphism   from $\Lambda$ onto $S_3$: $\theta: \Lambda(a,b) \rightarrow S_3$ using $a \mapsto (12)$ and $b  \mapsto (23)$. We have $(12)(23)(23)^{-1}(12)^{-1}(23)^{-1}=e$   thus indicating that our homomorphism factors through the quotient   modulo the normal subgroup $H$ of $F$ generated by   $abab^{-1}a^{-1}b^{-1}$ and, by definition, $\Lambda=F/H$. Find an   explicit, finite presentation for the kernel, $\kappa$, of the   epimorphism I know the kernel of $\theta$ is the set of all elements of $\Lambda$ that are mapped to $e\in S_3$ and is a normal subgroup of $\Lambda$. I'm a little new to finding kernels so I'm not sure how to proceed. What's a good way to find the kernel of this epimorphism?","Let $\Lambda$ denote the group with presentation $\langle a,b \mid   abab^{-1}a^{-1}b^{-1}\rangle$. We define the following epimorphism   from $\Lambda$ onto $S_3$: $\theta: \Lambda(a,b) \rightarrow S_3$ using $a \mapsto (12)$ and $b  \mapsto (23)$. We have $(12)(23)(23)^{-1}(12)^{-1}(23)^{-1}=e$   thus indicating that our homomorphism factors through the quotient   modulo the normal subgroup $H$ of $F$ generated by   $abab^{-1}a^{-1}b^{-1}$ and, by definition, $\Lambda=F/H$. Find an   explicit, finite presentation for the kernel, $\kappa$, of the   epimorphism I know the kernel of $\theta$ is the set of all elements of $\Lambda$ that are mapped to $e\in S_3$ and is a normal subgroup of $\Lambda$. I'm a little new to finding kernels so I'm not sure how to proceed. What's a good way to find the kernel of this epimorphism?",,"['abstract-algebra', 'group-theory']"
24,Showing reducibility of a polynomial in a Discrete Valuation Ring,Showing reducibility of a polynomial in a Discrete Valuation Ring,,Let $R$ be a complete discrete valuation ring with uniformiser $\pi$. I would like to show that a polynomial $f$ in $R[X]$ is reducible. Does it suffice to show that $f$ is reducible in $\frac{R}{\pi^i}[X]$ for all $i\in\mathbb{Z}_{\geq{1}}$? Thoughts: I think it does because $R$ complete means it is the inverse limit of the  $\frac{R}{\pi^i}$'s. Moreover each factorisation of $f$ in $\frac{R}{\pi^i}[X]$ passes down to $j\leq i$. But I kind of need to pass down from $i$ equal to infinity to make this work. I feel like I've seen something like this before somewhere else as well where it was made to work but I can't remember the argument... The example I'm interested in is when $R$ is a $\mathbb{Z}_p$ and $\pi=p$ where $p$ is a rational prime.,Let $R$ be a complete discrete valuation ring with uniformiser $\pi$. I would like to show that a polynomial $f$ in $R[X]$ is reducible. Does it suffice to show that $f$ is reducible in $\frac{R}{\pi^i}[X]$ for all $i\in\mathbb{Z}_{\geq{1}}$? Thoughts: I think it does because $R$ complete means it is the inverse limit of the  $\frac{R}{\pi^i}$'s. Moreover each factorisation of $f$ in $\frac{R}{\pi^i}[X]$ passes down to $j\leq i$. But I kind of need to pass down from $i$ equal to infinity to make this work. I feel like I've seen something like this before somewhere else as well where it was made to work but I can't remember the argument... The example I'm interested in is when $R$ is a $\mathbb{Z}_p$ and $\pi=p$ where $p$ is a rational prime.,,"['abstract-algebra', 'polynomials', 'ring-theory', 'algebraic-number-theory', 'dedekind-domain']"
25,Variant of Nakayama's lemma,Variant of Nakayama's lemma,,"I am trying to prove that if $M$ is an $R$-module, with $R$ complete w.r.t. an ideal $\mathfrak{m}$, and $M$ is separated ($\cap_k \mathfrak{m}^k M=0$) and the images of $m_1,\dots,m_n$ generate $M/\mathfrak{m} M$, then $m_1,\dots,m_n$ generate $M$. This appears as Exercise 7.2 in Eisenbud 's Commutative Algebra text. I am pretty stuck and would appreciate some hints.","I am trying to prove that if $M$ is an $R$-module, with $R$ complete w.r.t. an ideal $\mathfrak{m}$, and $M$ is separated ($\cap_k \mathfrak{m}^k M=0$) and the images of $m_1,\dots,m_n$ generate $M/\mathfrak{m} M$, then $m_1,\dots,m_n$ generate $M$. This appears as Exercise 7.2 in Eisenbud 's Commutative Algebra text. I am pretty stuck and would appreciate some hints.",,"['abstract-algebra', 'commutative-algebra', 'modules']"
26,Composition series for Verma modules.,Composition series for Verma modules.,,"Let $L$ a Lie Algebra. I need prove that that every Verma module $\Delta(\lambda)$ admits a composition series, i.e a series of submodules with simple factors.  I found a proof that is quite short in this scripts , at Proposition 5.5. At the end of the proof, when builds a concrete series, the term $M_i$ is given and is considered $M_i$ a maximal sub-module of $M$. I am not too much sure how it is obvious that such a maximal sub-module exists. I thought to two ways to proceed, the first is to consider that $M$ is a sub-module of a Verma module, that the weights of $M$ are bounded by above and deduce (in someway, if it is true) that there is a finite number of maximal weights of $M$, say $\mu_1 \dots \mu_n $. Then since $\text{dim}$ $ M_{\mu_i} < \infty$ we can choose a basis $B_i$ of $M_{\mu_i}$. If $M$ is generated by all $v \in B_i$ for all $i$, then we can conclude using the theory of finitely generated modules (in particular, this question ). The second way is to consider a maximal weight $\mu$ of $M$ and $v \in M_\mu$, then try to prove that $M'=\sum N $, where the sum is over all the submodules of $M'$ that don't contain $v$, is maximal in $M$. Then, how can I prove that the factors (and their occurrence) are independent on the choice of the Jordan-Holder chain?","Let $L$ a Lie Algebra. I need prove that that every Verma module $\Delta(\lambda)$ admits a composition series, i.e a series of submodules with simple factors.  I found a proof that is quite short in this scripts , at Proposition 5.5. At the end of the proof, when builds a concrete series, the term $M_i$ is given and is considered $M_i$ a maximal sub-module of $M$. I am not too much sure how it is obvious that such a maximal sub-module exists. I thought to two ways to proceed, the first is to consider that $M$ is a sub-module of a Verma module, that the weights of $M$ are bounded by above and deduce (in someway, if it is true) that there is a finite number of maximal weights of $M$, say $\mu_1 \dots \mu_n $. Then since $\text{dim}$ $ M_{\mu_i} < \infty$ we can choose a basis $B_i$ of $M_{\mu_i}$. If $M$ is generated by all $v \in B_i$ for all $i$, then we can conclude using the theory of finitely generated modules (in particular, this question ). The second way is to consider a maximal weight $\mu$ of $M$ and $v \in M_\mu$, then try to prove that $M'=\sum N $, where the sum is over all the submodules of $M'$ that don't contain $v$, is maximal in $M$. Then, how can I prove that the factors (and their occurrence) are independent on the choice of the Jordan-Holder chain?",,"['abstract-algebra', 'modules', 'representation-theory', 'lie-algebras', 'verma-modules']"
27,Is it true that $(R\times S)[G]\cong R[G]\times S[G]$?,Is it true that ?,(R\times S)[G]\cong R[G]\times S[G],"I know for two groups $G, H$ (not necessarily finite) we have $R[G\times H]\cong (R[G])[H]$, but I was wondering if we had a similar statement for rings $R,\,S$. In other words, if $R,\,S$ are two (possibly noncommutative rings), is it true that $(R\times S)[G]\cong R[G]\times S[G]$?","I know for two groups $G, H$ (not necessarily finite) we have $R[G\times H]\cong (R[G])[H]$, but I was wondering if we had a similar statement for rings $R,\,S$. In other words, if $R,\,S$ are two (possibly noncommutative rings), is it true that $(R\times S)[G]\cong R[G]\times S[G]$?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'noncommutative-algebra', 'group-rings']"
28,Are tensor products cancellative?,Are tensor products cancellative?,,"Let $k$ be a field, $F \subseteq k$ a subfield, and $A_0$ a finitely generated $F$-algebra.  Then $k \otimes_F A_0$ is a finitely generated $k$-algebra.  If $B_0$ is another finitely generated $F$-algebra, and there exists some $k$-algebra isomorphism $\phi:k \otimes_F A_0 \rightarrow k \otimes_F B_0$, do $A_0$ and $B_0$ have to be isomorphic as $F$-algebras?  Or at least $F$-vector spaces? I know that if $\phi$ were induced by some existing homomorphism $A_0 \rightarrow B_0$, then the claim would follow from faithful flatness.  If $\phi$ is any old isomorphism, I'm not so sure.  What do you think?","Let $k$ be a field, $F \subseteq k$ a subfield, and $A_0$ a finitely generated $F$-algebra.  Then $k \otimes_F A_0$ is a finitely generated $k$-algebra.  If $B_0$ is another finitely generated $F$-algebra, and there exists some $k$-algebra isomorphism $\phi:k \otimes_F A_0 \rightarrow k \otimes_F B_0$, do $A_0$ and $B_0$ have to be isomorphic as $F$-algebras?  Or at least $F$-vector spaces? I know that if $\phi$ were induced by some existing homomorphism $A_0 \rightarrow B_0$, then the claim would follow from faithful flatness.  If $\phi$ is any old isomorphism, I'm not so sure.  What do you think?",,"['abstract-algebra', 'commutative-algebra']"
29,"Orbits of the $\text{SL}(n,\mathcal{O}_K)$-action on $\mathbb{P}^{n-1}(K)$ for a number field $K$.",Orbits of the -action on  for a number field .,"\text{SL}(n,\mathcal{O}_K) \mathbb{P}^{n-1}(K) K","I was reading some notes of Keith Conrad where he proves that the number of orbits of the $\text{SL}(2,\mathcal{O}_K)$-action on $\mathbb{P}^{1}(K)$ for a number field $K$ is precisely the class number of $K$. I am wondering if there is any kind of ""higher-order"" arithmetic information found in looking at the number of orbits of the $\text{SL}(n,\mathcal{O}_K)$-action on $\mathbb{P}^{n-1}(K)$ for $n>2$ (or even on higher Grassmannians $\text{Gr}(r,K^{n})$, but let's not get too crazy for now). As a first question, will these numbers even be finite for all $n$? For $K=\mathbb{Q}$, I believe one can use a generalized Euclidean algorithm to show that the action above is transitive for all $n$, at least for the projective spaces $\mathbb{P}^{n-1}(\mathbb{Q})$, but I'm not sure if one can adapt this argument to work even for $\mathcal{O}_K$ that are UFDs but not Euclidean. Does anyone know of any references on this question?","I was reading some notes of Keith Conrad where he proves that the number of orbits of the $\text{SL}(2,\mathcal{O}_K)$-action on $\mathbb{P}^{1}(K)$ for a number field $K$ is precisely the class number of $K$. I am wondering if there is any kind of ""higher-order"" arithmetic information found in looking at the number of orbits of the $\text{SL}(n,\mathcal{O}_K)$-action on $\mathbb{P}^{n-1}(K)$ for $n>2$ (or even on higher Grassmannians $\text{Gr}(r,K^{n})$, but let's not get too crazy for now). As a first question, will these numbers even be finite for all $n$? For $K=\mathbb{Q}$, I believe one can use a generalized Euclidean algorithm to show that the action above is transitive for all $n$, at least for the projective spaces $\mathbb{P}^{n-1}(\mathbb{Q})$, but I'm not sure if one can adapt this argument to work even for $\mathcal{O}_K$ that are UFDs but not Euclidean. Does anyone know of any references on this question?",,"['abstract-algebra', 'matrices', 'number-theory', 'algebraic-number-theory']"
30,the unit group of an infinite field cannot be cyclic [duplicate],the unit group of an infinite field cannot be cyclic [duplicate],,"This question already has answers here : Why must a field whose a group of units is cyclic be finite? (4 answers) Closed 8 years ago . It is well-known that the unit group of a finite field is a finite cyclic group. But for infinite fields, e.g., $\mathbb{Q}$ or $\mathbb{R}$, the unit groups are not cyclic. I heard this fact in my class and I'm trying to prove it. Assuming the unit group is infinite cyclic with the generator $a$, I tried to show some element, $\frac{1}{a+1}$ or $\frac{1}{a^2+1}$ cannot be a power of $a$, but it doesn't seem hopeful. Could someone help me?","This question already has answers here : Why must a field whose a group of units is cyclic be finite? (4 answers) Closed 8 years ago . It is well-known that the unit group of a finite field is a finite cyclic group. But for infinite fields, e.g., $\mathbb{Q}$ or $\mathbb{R}$, the unit groups are not cyclic. I heard this fact in my class and I'm trying to prove it. Assuming the unit group is infinite cyclic with the generator $a$, I tried to show some element, $\frac{1}{a+1}$ or $\frac{1}{a^2+1}$ cannot be a power of $a$, but it doesn't seem hopeful. Could someone help me?",,['abstract-algebra']
31,How to find automorphism of a particular order.,How to find automorphism of a particular order.,,"Given a finite  group  if  the  automorphism group  is  known  is  it  possible  to  write  down  all  the  automorphisms  with  respective  orders? For  example say the  group $Z_{p^{2}}$  has  automorphism  group isomorphic  to  $Z_{p(p-1)}$ and I  have  to  find  an automorphism  of  order $p$, $p-1$. Now I know  that one automorphism  of  order $p$  is $y \mapsto y^{p+1}$ . But  how  to  actually find  them  out  for  this  other  groups?","Given a finite  group  if  the  automorphism group  is  known  is  it  possible  to  write  down  all  the  automorphisms  with  respective  orders? For  example say the  group $Z_{p^{2}}$  has  automorphism  group isomorphic  to  $Z_{p(p-1)}$ and I  have  to  find  an automorphism  of  order $p$, $p-1$. Now I know  that one automorphism  of  order $p$  is $y \mapsto y^{p+1}$ . But  how  to  actually find  them  out  for  this  other  groups?",,['abstract-algebra']
32,What exactly is Hensel doing for us in this result?,What exactly is Hensel doing for us in this result?,,"I'm reading a paper where the author appeals to Hensel's lemma, but it is not clear to me quite how it is meant to be applied (or, for that matter, which version!). My commutative algebra background is unfortunately not as good as I'd like, and I can't find references between plain old Hensel's lemma for $p$-adics and Henselian rings. The setup is this. I have a split exact sequence of (unital, and otherwise nice) rings $$ A \stackrel{r}{\to} B \to 0 $$ where $N = ker(r)$ is a nil-ideal, and $s$ is a section of $r$. Let $\Sigma$ be a set of primes (in the end it will probably just be a single prime), $\mathbb{Z}_\Sigma = \{a/b\in\mathbb{Q} \mid p\not| b,\ \forall p\in\Sigma\}$ and $R_\Sigma = R\otimes_\mathbb{Z} \mathbb{Z}_\Sigma$ for $R$ a ring (sim. for an ideal). Then we still have $N_\Sigma$ a nil-ideal, $r_\Sigma$ is still onto. Let $U_\Sigma = r_\Sigma^{-1}(1) \subset A_\Sigma$. We also have that $a\in A_\Sigma$ is a unit if and only if $r_\Sigma(a)$ is a unit. Now comes the bit I don't understand. The author claims, using Hensel, that given a unit $n\in \mathbb{Z}_\Sigma^\times$ with $n\gt 0$ and some $a\in U_\Sigma$, there is a unique $b\in U_\Sigma$ such that $b^n = a$. I can think of several relevant points, but I can't connect them up into a proof in my mind. As $N$ is a nil-ideal, so $A$ should be complete in the $N$-adic topology. Since $a\in U_\Sigma$, I'm lifting the solution $1$ to $x^n - 1=0$ in $B_\Sigma$ through the various projections $A_\Sigma/N_\Sigma^{k+1} \to A_\Sigma/N_\Sigma^k$. The rings $A$ and $B$ are quite nice ($B$ is isomorphic to $\mathbb{Z}^m$, for instance, but $A$ can be more complicated), so I can probably assume they're Noetherian... Is the proof there under my nose? Or is a bit more subtle?","I'm reading a paper where the author appeals to Hensel's lemma, but it is not clear to me quite how it is meant to be applied (or, for that matter, which version!). My commutative algebra background is unfortunately not as good as I'd like, and I can't find references between plain old Hensel's lemma for $p$-adics and Henselian rings. The setup is this. I have a split exact sequence of (unital, and otherwise nice) rings $$ A \stackrel{r}{\to} B \to 0 $$ where $N = ker(r)$ is a nil-ideal, and $s$ is a section of $r$. Let $\Sigma$ be a set of primes (in the end it will probably just be a single prime), $\mathbb{Z}_\Sigma = \{a/b\in\mathbb{Q} \mid p\not| b,\ \forall p\in\Sigma\}$ and $R_\Sigma = R\otimes_\mathbb{Z} \mathbb{Z}_\Sigma$ for $R$ a ring (sim. for an ideal). Then we still have $N_\Sigma$ a nil-ideal, $r_\Sigma$ is still onto. Let $U_\Sigma = r_\Sigma^{-1}(1) \subset A_\Sigma$. We also have that $a\in A_\Sigma$ is a unit if and only if $r_\Sigma(a)$ is a unit. Now comes the bit I don't understand. The author claims, using Hensel, that given a unit $n\in \mathbb{Z}_\Sigma^\times$ with $n\gt 0$ and some $a\in U_\Sigma$, there is a unique $b\in U_\Sigma$ such that $b^n = a$. I can think of several relevant points, but I can't connect them up into a proof in my mind. As $N$ is a nil-ideal, so $A$ should be complete in the $N$-adic topology. Since $a\in U_\Sigma$, I'm lifting the solution $1$ to $x^n - 1=0$ in $B_\Sigma$ through the various projections $A_\Sigma/N_\Sigma^{k+1} \to A_\Sigma/N_\Sigma^k$. The rings $A$ and $B$ are quite nice ($B$ is isomorphic to $\mathbb{Z}^m$, for instance, but $A$ can be more complicated), so I can probably assume they're Noetherian... Is the proof there under my nose? Or is a bit more subtle?",,"['abstract-algebra', 'ring-theory', 'hensels-lemma']"
33,Under what conditions does $M \oplus A \cong M \oplus B$ imply $A \cong B$?,Under what conditions does  imply ?,M \oplus A \cong M \oplus B A \cong B,"This question is fairly general (I'm actually interested in a more specific setting, which I'll mention later), and I've found similar questions/answers on here but they don't seem to answer the following: Let $R$ be a ring. Are there any simple conditions on $R$-modules $M, A$ and $B$ to ensure that $M \oplus A \cong M \oplus B$ implies $A \cong B$? This is obviously not true in general: a simple counterexample is given by $ M= \bigoplus_{n \in \mathbb{N}} \mathbb{Z}, A = \mathbb{Z}, B = 0 $. In the more specific setting that I'm interested in, $R$ is noetherian, each module is finitely generated, reflexive and satisfies $\text{Ext}_R^n(M,R) = 0$ for $n \geqslant 1$ (or replacing $M$ with $A$ or $B$), and $A$ is projective. In this case, do we have the desired result?","This question is fairly general (I'm actually interested in a more specific setting, which I'll mention later), and I've found similar questions/answers on here but they don't seem to answer the following: Let $R$ be a ring. Are there any simple conditions on $R$-modules $M, A$ and $B$ to ensure that $M \oplus A \cong M \oplus B$ implies $A \cong B$? This is obviously not true in general: a simple counterexample is given by $ M= \bigoplus_{n \in \mathbb{N}} \mathbb{Z}, A = \mathbb{Z}, B = 0 $. In the more specific setting that I'm interested in, $R$ is noetherian, each module is finitely generated, reflexive and satisfies $\text{Ext}_R^n(M,R) = 0$ for $n \geqslant 1$ (or replacing $M$ with $A$ or $B$), and $A$ is projective. In this case, do we have the desired result?",,"['abstract-algebra', 'ring-theory', 'modules', 'homological-algebra']"
34,Transcendental Extensions. $F(\alpha)$ isomorphic to $F(x)$,Transcendental Extensions.  isomorphic to,F(\alpha) F(x),"Let $E$ be an extension field of $F$ and $\alpha \in E$. Then $\alpha$ is transcendental over $F$ if and only if $F(\alpha)$ is isomorphic to $F(x)$, the field of fractions of $F[x]$. This was a theorem in an abstract algebra textbook with a very brief proof. Can someone please explain why this theorem holds? I'm having difficulty grasping the concepts at hand. Thanks!","Let $E$ be an extension field of $F$ and $\alpha \in E$. Then $\alpha$ is transcendental over $F$ if and only if $F(\alpha)$ is isomorphic to $F(x)$, the field of fractions of $F[x]$. This was a theorem in an abstract algebra textbook with a very brief proof. Can someone please explain why this theorem holds? I'm having difficulty grasping the concepts at hand. Thanks!",,['abstract-algebra']
35,Alternative way to count the number of solutions to the equation $x^2 + y^2 = -1$ over $\Bbb Z /p$,Alternative way to count the number of solutions to the equation  over,x^2 + y^2 = -1 \Bbb Z /p,"$x^2 + y^2 = -1$ is a weird equation because it has no solutions over $\Bbb R$. I want to count the number of solutions it has over $\Bbb Z / p$ where $p$ is prime. If $p = 2$ then it has $p$ solutions. This is to do with the fact that squaring is a field automorphism. If $p \equiv 1 \pmod{4}$ then there is an $i$ such that $i^2 = -1$ so $$x^2 + y^2 = -1 \implies \left({x \over i}\right)^2 + \left({y \over i}\right)^2 = 1 \implies \left({x \over i} + y \right)\left({x \over i} - y \right) = 1$$ which has $p - 1$ solutions. If $p \equiv 3 \pmod{4}$ then the situation is more complicated. The thing I noticed is that $A = \{x \mid x^2 + y^2 = -1\}$ and $B = \{x \mid y^2 - x^2 = 1\}$ form a partition of $\Bbb Z/p$. Reason being that $x \not\in A \implies (-1 - x^2 \mid p) = -1 \implies (1 + x^2 \mid p) = 1 \implies (\exists y)\,1+x^2 = y^2 \implies x \in B$, and vice versa. Also notice that $A \cap B = \emptyset$. So we get $|A| = |\Bbb Z / p| - |B| = p - |B|$. To determine $|B|$, use the fact that for every $(x,y)$ for which $y^2 - x^2 = 1$, $(x,-y)$ also satisfies the equation, so $|B|$ is the number of solutions to $y^2 - x^2 = 1$ divided by $2$, which is $\frac{p-1}{2} \therefore \,|A| = {p + 1 \over 2}$. Now it's easy to see that the number of solutions to $x^2 + y^2 = -1$ is $2|A|$ which is $p+1$. Any quicker method?","$x^2 + y^2 = -1$ is a weird equation because it has no solutions over $\Bbb R$. I want to count the number of solutions it has over $\Bbb Z / p$ where $p$ is prime. If $p = 2$ then it has $p$ solutions. This is to do with the fact that squaring is a field automorphism. If $p \equiv 1 \pmod{4}$ then there is an $i$ such that $i^2 = -1$ so $$x^2 + y^2 = -1 \implies \left({x \over i}\right)^2 + \left({y \over i}\right)^2 = 1 \implies \left({x \over i} + y \right)\left({x \over i} - y \right) = 1$$ which has $p - 1$ solutions. If $p \equiv 3 \pmod{4}$ then the situation is more complicated. The thing I noticed is that $A = \{x \mid x^2 + y^2 = -1\}$ and $B = \{x \mid y^2 - x^2 = 1\}$ form a partition of $\Bbb Z/p$. Reason being that $x \not\in A \implies (-1 - x^2 \mid p) = -1 \implies (1 + x^2 \mid p) = 1 \implies (\exists y)\,1+x^2 = y^2 \implies x \in B$, and vice versa. Also notice that $A \cap B = \emptyset$. So we get $|A| = |\Bbb Z / p| - |B| = p - |B|$. To determine $|B|$, use the fact that for every $(x,y)$ for which $y^2 - x^2 = 1$, $(x,-y)$ also satisfies the equation, so $|B|$ is the number of solutions to $y^2 - x^2 = 1$ divided by $2$, which is $\frac{p-1}{2} \therefore \,|A| = {p + 1 \over 2}$. Now it's easy to see that the number of solutions to $x^2 + y^2 = -1$ is $2|A|$ which is $p+1$. Any quicker method?",,"['abstract-algebra', 'elementary-number-theory', 'finite-fields', 'alternative-proof']"
36,Induction of an irreducible group representation,Induction of an irreducible group representation,,"I'm having some trouble finding the answer to the following question. Any ideas on how to get started? Let $H$ be a subgroup of a group $G$ and let $U_{1}$, ...,$U_{k}$ be the irreducible representations of $G$. Further, assume that $\psi_{1}$, ..., $\psi_{k}$ are the ($H$-)characters of the restrictions $Res(U_{1})$, ..., $Res(U_{k})$. If $V$ is an irreducible $H$-representation, give a formula to compute the decomposition $Ind($V$) = U^{\oplus e_{1}}_{1} \oplus U^{\oplus e_{2}}_{2} \oplus ... \oplus U^{\oplus e_{k}}_{k}$.","I'm having some trouble finding the answer to the following question. Any ideas on how to get started? Let $H$ be a subgroup of a group $G$ and let $U_{1}$, ...,$U_{k}$ be the irreducible representations of $G$. Further, assume that $\psi_{1}$, ..., $\psi_{k}$ are the ($H$-)characters of the restrictions $Res(U_{1})$, ..., $Res(U_{k})$. If $V$ is an irreducible $H$-representation, give a formula to compute the decomposition $Ind($V$) = U^{\oplus e_{1}}_{1} \oplus U^{\oplus e_{2}}_{2} \oplus ... \oplus U^{\oplus e_{k}}_{k}$.",,"['abstract-algebra', 'group-theory', 'representation-theory']"
37,If $H \leq G$ and $[G:H]! \leq |G|$ then $G$ is not simple,If  and  then  is not simple,H \leq G [G:H]! \leq |G| G,"I'm looking for verification: My claim: If $G$ is a finite group and $H$ is a (proper)subgroup of index $k>1$, where $k! \leq |G|$, then $G$ is not simple. Proof: Consider the set of left cosets of $G$ by $H$: $$G/H = \{H,g_1H,\dots,g_kH\}$$ Then $\phi: G \rightarrow S_{G/H}$ is a homomorphism where we define: $$\phi(g)[g_iH] = g g_iH$$ ($S_{G/H}$ is the group of permutations on elements of $G/H$ and of course $S_{G/H} \cong S_k$.) Also $\ker \phi \neq G$ because $\phi(g_1)$ is clearly not the identity in $S_{G/H}$. Because $\phi$ is a homomorphism, $$ G/\ker\phi \cong \text{im}\phi$$ Then $|\text{im} \phi| \leq |S_k| = k!$ and if $k! \leq |G|$ then $|\ker\phi| > 1$ and $\ker\phi$ is a non-trivial normal subgroup of $G$. Example: If a group of order $36$ has a subgroup of index $3$ or $4$ then the group is not simple.","I'm looking for verification: My claim: If $G$ is a finite group and $H$ is a (proper)subgroup of index $k>1$, where $k! \leq |G|$, then $G$ is not simple. Proof: Consider the set of left cosets of $G$ by $H$: $$G/H = \{H,g_1H,\dots,g_kH\}$$ Then $\phi: G \rightarrow S_{G/H}$ is a homomorphism where we define: $$\phi(g)[g_iH] = g g_iH$$ ($S_{G/H}$ is the group of permutations on elements of $G/H$ and of course $S_{G/H} \cong S_k$.) Also $\ker \phi \neq G$ because $\phi(g_1)$ is clearly not the identity in $S_{G/H}$. Because $\phi$ is a homomorphism, $$ G/\ker\phi \cong \text{im}\phi$$ Then $|\text{im} \phi| \leq |S_k| = k!$ and if $k! \leq |G|$ then $|\ker\phi| > 1$ and $\ker\phi$ is a non-trivial normal subgroup of $G$. Example: If a group of order $36$ has a subgroup of index $3$ or $4$ then the group is not simple.",,"['abstract-algebra', 'group-theory', 'proof-verification', 'finite-groups', 'simple-groups']"
38,Show $x^p-t$ has no root in the field $\mathbb{F}_p(t)$,Show  has no root in the field,x^p-t \mathbb{F}_p(t),"I don't think I fully understand. Let's say there is a root $x_0 \in K=\mathbb{F}_p(t)$, where $p$ is a prime number. Then $x_0 = \frac{P(t)}{Q(t)}$ for some polynomials $P,Q \in \mathbb{F}_p[t]$. We can assume $\gcd(P,Q)=1$ and $x_0^p-t= \frac{(P(t))^p}{(Q(t))^p}-t= \frac{P(t)^p-tQ(t)^p}{Q(t)^p} =0$, so coefficients of $P(t)^p, tQ(t)^p$ must be identical, which contradicts $\gcd(P,Q)=1$, hence such $x_0$ does not exist and the polynomial has no root in $K$. am I about right? anyway, I would appreciate an explanation about $\mathbb{F}_p(t)$, what is $t$? what is the meaning of a variable which does not belong to any specific ""world""? I cannot use $t$ as if it was a member of $\mathbb{F}_p$ and hence cannot assume $t^{p-1} = 1 \pmod p$...","I don't think I fully understand. Let's say there is a root $x_0 \in K=\mathbb{F}_p(t)$, where $p$ is a prime number. Then $x_0 = \frac{P(t)}{Q(t)}$ for some polynomials $P,Q \in \mathbb{F}_p[t]$. We can assume $\gcd(P,Q)=1$ and $x_0^p-t= \frac{(P(t))^p}{(Q(t))^p}-t= \frac{P(t)^p-tQ(t)^p}{Q(t)^p} =0$, so coefficients of $P(t)^p, tQ(t)^p$ must be identical, which contradicts $\gcd(P,Q)=1$, hence such $x_0$ does not exist and the polynomial has no root in $K$. am I about right? anyway, I would appreciate an explanation about $\mathbb{F}_p(t)$, what is $t$? what is the meaning of a variable which does not belong to any specific ""world""? I cannot use $t$ as if it was a member of $\mathbb{F}_p$ and hence cannot assume $t^{p-1} = 1 \pmod p$...",,"['abstract-algebra', 'field-theory', 'extension-field', 'irreducible-polynomials']"
39,An element of $f$ of a function field such that $P$ is the only pole of $f$.,An element of  of a function field such that  is the only pole of .,f P f,"Let $F$ be a function field in one variable over a field $k$. Let $S$ be a nonempty finite subset of all places of $F$. Prove that if $P \in S$, there is an element $f$ of $F$ such that $P$ is the only pole of $f$.","Let $F$ be a function field in one variable over a field $k$. Let $S$ be a nonempty finite subset of all places of $F$. Prove that if $P \in S$, there is an element $f$ of $F$ such that $P$ is the only pole of $f$.",,['abstract-algebra']
40,Find kernel generators for ring maps,Find kernel generators for ring maps,,"This is the textbook question: Q: Find generators for the kernels of the following maps: $\mathbb{R}[x,y] \to \mathbb{R}$ defined by $f(x,y) \rightsquigarrow f(0,0)$ $\mathbb{R}[x] \to \mathbb{C}$ defined by $f(x) \rightsquigarrow f(2+ i)$ $\mathbb{Z}[x] \to \mathbb{R}$ defined by $f(x) \rightsquigarrow f(1+\sqrt{2})$ $\mathbb{Z}[x] \to \mathbb{C}$ defined by $x \rightsquigarrow \sqrt{2}+\sqrt{3}$ $\mathbb{C}[x,y,z] \to \mathbb{C}[t]$ defined by $x \rightsquigarrow t, y \rightsquigarrow t^2, z \rightsquigarrow t^3$ My work on the first three: Any polynomial that satisfies $f(0,0)=0$ will be in the kernel. Intuitively, the two polynomials $f(x,y)=x$ and $f(x,y)=y$ should generate this. How can I prove that? The kernel will have root $(2+i)$ and the conjugate $(2-i)$ which multiply to $(x-(2+i))(x-(2-i)) = x^2-4x+5$. The coefficients are real and the polynomial is irreducible in $\mathbb{R}$. That polynomial is clearly in the kernel. How can I show that it generates the kernel? I find polynomial $f(x) = x^2-2x-1$ that is in the kernel and is irreducible in $\mathbb{Z}$. How can I show that it generates the kernel?","This is the textbook question: Q: Find generators for the kernels of the following maps: $\mathbb{R}[x,y] \to \mathbb{R}$ defined by $f(x,y) \rightsquigarrow f(0,0)$ $\mathbb{R}[x] \to \mathbb{C}$ defined by $f(x) \rightsquigarrow f(2+ i)$ $\mathbb{Z}[x] \to \mathbb{R}$ defined by $f(x) \rightsquigarrow f(1+\sqrt{2})$ $\mathbb{Z}[x] \to \mathbb{C}$ defined by $x \rightsquigarrow \sqrt{2}+\sqrt{3}$ $\mathbb{C}[x,y,z] \to \mathbb{C}[t]$ defined by $x \rightsquigarrow t, y \rightsquigarrow t^2, z \rightsquigarrow t^3$ My work on the first three: Any polynomial that satisfies $f(0,0)=0$ will be in the kernel. Intuitively, the two polynomials $f(x,y)=x$ and $f(x,y)=y$ should generate this. How can I prove that? The kernel will have root $(2+i)$ and the conjugate $(2-i)$ which multiply to $(x-(2+i))(x-(2-i)) = x^2-4x+5$. The coefficients are real and the polynomial is irreducible in $\mathbb{R}$. That polynomial is clearly in the kernel. How can I show that it generates the kernel? I find polynomial $f(x) = x^2-2x-1$ that is in the kernel and is irreducible in $\mathbb{Z}$. How can I show that it generates the kernel?",,"['abstract-algebra', 'ring-theory']"
41,Let $S$ be a normal p-subgroup of a finite group $G$. Prove that $S \subseteq P$ for every Sylow p-subgroup $P$ of $G$,Let  be a normal p-subgroup of a finite group . Prove that  for every Sylow p-subgroup  of,S G S \subseteq P P G,"Let $S$ be a normal p-subgroup of a finite group $G$. Prove that $S \subseteq P$ for every Sylow p-subgroup $P$ of $G$. Now, I know that this involves the Sylow Theorems, of course. This is very new to me but my thinking so far is this. S is a normal p-subgroup of a finite group G, so we can say that S has some order of a power of p, $|S|=p^k$ where p is prime and S is a subgroup of some Sylow p-subgroup $K$ (by the second Sylow Th). If I let $P$ be any other Sylow p-subroup, we must show $S \subseteq P.$ Using the third Sylow Theorem, we can state that $K$ and $P$ are conjugates. Thus, there exists $x \in G$ s.t. $xKx^{-1}=P$ Thus, $xSx^{-1} \subseteq P$. But, since S is normal in G, we have $xSx^{-1}=S$. Please let me know if this is sound theory and if not, please offer corrections.","Let $S$ be a normal p-subgroup of a finite group $G$. Prove that $S \subseteq P$ for every Sylow p-subgroup $P$ of $G$. Now, I know that this involves the Sylow Theorems, of course. This is very new to me but my thinking so far is this. S is a normal p-subgroup of a finite group G, so we can say that S has some order of a power of p, $|S|=p^k$ where p is prime and S is a subgroup of some Sylow p-subgroup $K$ (by the second Sylow Th). If I let $P$ be any other Sylow p-subroup, we must show $S \subseteq P.$ Using the third Sylow Theorem, we can state that $K$ and $P$ are conjugates. Thus, there exists $x \in G$ s.t. $xKx^{-1}=P$ Thus, $xSx^{-1} \subseteq P$. But, since S is normal in G, we have $xSx^{-1}=S$. Please let me know if this is sound theory and if not, please offer corrections.",,"['abstract-algebra', 'group-theory', 'sylow-theory']"
42,"If $F$ is a free group then $g^2=h^2$ implies $g=h$ for $h,g\in F$",If  is a free group then  implies  for,"F g^2=h^2 g=h h,g\in F","If $F$ is a free group then $g^2=h^2$ implies $g=h$ for $h,g\in F$. I've been trying to prove this given the definition of a free group $F$: given group $F$ and subset $X\subseteq F$, $F$ is free over $X$ if for any group $G$ and function $\theta: X \to G$, there exists a unique homomorphism $\alpha:F\to G$ such that $\alpha(x)=\theta(x)$ for all $x\in X$. The definition doesn't leave me much to work with: I've attempted to define a function $\theta:X\to F$ and then use the definition to take the given $\alpha$ and somehow arrive at $h=g$ but I've been unsuccessful.","If $F$ is a free group then $g^2=h^2$ implies $g=h$ for $h,g\in F$. I've been trying to prove this given the definition of a free group $F$: given group $F$ and subset $X\subseteq F$, $F$ is free over $X$ if for any group $G$ and function $\theta: X \to G$, there exists a unique homomorphism $\alpha:F\to G$ such that $\alpha(x)=\theta(x)$ for all $x\in X$. The definition doesn't leave me much to work with: I've attempted to define a function $\theta:X\to F$ and then use the definition to take the given $\alpha$ and somehow arrive at $h=g$ but I've been unsuccessful.",,"['abstract-algebra', 'group-theory', 'free-groups']"
43,$n-1$ dimensional permutation module for $S_n$,dimensional permutation module for,n-1 S_n,"Say $n \ge 5$. Let $P$ be the $(n-1)$ dimensional permutation module for $S_n$, i.e. the permutation representation on $\{(x_1, \dots, x_n) \in {\bf C}^n: \sum x_i  = 0\}$. Prove that: $\wedge^2P$ is always a irreducible $S_n$-module; $\text{Sym}^2P$ is always isomorphic to the sum of the trivial representation, $P$ and an irreducible $S_n$-module. Is there a way to do this problem without using something ""overpowered"" such as Young tableaus?","Say $n \ge 5$. Let $P$ be the $(n-1)$ dimensional permutation module for $S_n$, i.e. the permutation representation on $\{(x_1, \dots, x_n) \in {\bf C}^n: \sum x_i  = 0\}$. Prove that: $\wedge^2P$ is always a irreducible $S_n$-module; $\text{Sym}^2P$ is always isomorphic to the sum of the trivial representation, $P$ and an irreducible $S_n$-module. Is there a way to do this problem without using something ""overpowered"" such as Young tableaus?",,['abstract-algebra']
44,the Zassenhaus /Baker–Campbell–Hausdorff formula for cosine.,the Zassenhaus /Baker–Campbell–Hausdorff formula for cosine.,,"This question concerns the expansion of non-commutative algebra $[X,Y] \neq 0$ for two operators $X,Y$. One can think of $X$ and $Y$ as some matrices. If $[X,Y] = 0$, we have $$e^{t(X+Y)}= e^{tX}~  e^{tY}$$ If $[X,Y] \neq 0$, We know the Zassenhaus formula or the Baker–Campbell–Hausdorff formula: $$e^{t(X+Y)}= e^{tX}~  e^{tY} ~e^{-\frac{t^2}{2} [X,Y]} ~ e^{\frac{t^3}{6}(2[Y,[X,Y]]+ [X,[X,Y]] )} ~ e^{\frac{-t^4}{24}([[[X,Y],X],X] + 3[[[X,Y],X],Y] + 3[[[X,Y],Y],Y]) } \cdots$$ This expresses $e^{t(X+Y)}$ in terms of $e^{tX}$ and $e^{tY}$, and their further commutators $[X,Y]$. Question: Do we have a similar form for $\cos(A+B)$ when $[A,B]=C \neq 0$? (We may take $[C,A]=[C,B]=0$ for the simplest case to extract the first order term.) If $[A,B]=0$, we have   $$\cos(A+B)=\cos(A)\cos(B) -\sin(A)\sin(B).$$ If $[A,B]=C \neq 0$, do we have some similar expression like the Zassenhaus formula or the Baker–Campbell–Hausdorff formula:   $$\cos(A+B)=\cos(A)\cos(B) \dots-\sin(A)\sin(B) \dots+ \dots$$ Can we express $\cos(A+B)$ in terms of $\cos(A)$,$\cos(B)$,$\sin(A)$,$\sin(B)$ and some function of $C$?","This question concerns the expansion of non-commutative algebra $[X,Y] \neq 0$ for two operators $X,Y$. One can think of $X$ and $Y$ as some matrices. If $[X,Y] = 0$, we have $$e^{t(X+Y)}= e^{tX}~  e^{tY}$$ If $[X,Y] \neq 0$, We know the Zassenhaus formula or the Baker–Campbell–Hausdorff formula: $$e^{t(X+Y)}= e^{tX}~  e^{tY} ~e^{-\frac{t^2}{2} [X,Y]} ~ e^{\frac{t^3}{6}(2[Y,[X,Y]]+ [X,[X,Y]] )} ~ e^{\frac{-t^4}{24}([[[X,Y],X],X] + 3[[[X,Y],X],Y] + 3[[[X,Y],Y],Y]) } \cdots$$ This expresses $e^{t(X+Y)}$ in terms of $e^{tX}$ and $e^{tY}$, and their further commutators $[X,Y]$. Question: Do we have a similar form for $\cos(A+B)$ when $[A,B]=C \neq 0$? (We may take $[C,A]=[C,B]=0$ for the simplest case to extract the first order term.) If $[A,B]=0$, we have   $$\cos(A+B)=\cos(A)\cos(B) -\sin(A)\sin(B).$$ If $[A,B]=C \neq 0$, do we have some similar expression like the Zassenhaus formula or the Baker–Campbell–Hausdorff formula:   $$\cos(A+B)=\cos(A)\cos(B) \dots-\sin(A)\sin(B) \dots+ \dots$$ Can we express $\cos(A+B)$ in terms of $\cos(A)$,$\cos(B)$,$\sin(A)$,$\sin(B)$ and some function of $C$?",,"['abstract-algebra', 'sequences-and-series', 'taylor-expansion', 'noncommutative-algebra']"
45,Proving a subring of $\mathbb{Q}$ containing $\mathbb{Z}$ is a PID,Proving a subring of  containing  is a PID,\mathbb{Q} \mathbb{Z},"Let $S$ be a subring of $\mathbb{Q}$ containing $\mathbb{Z}$. Prove that it is a principal ideal domain. So here is what I tried. Take any ideal $I\subset S$. Take any two elements, say $a=p/q, b=r/s$ in $I$. Rationalize denominators (we can do that since $\mathbb{Z}\subset S$. Now we have $aqs=p=K(r)=K(bqs)+R$, that is, just apply the Euclidean Algorithm. Now, $a-Kb=R/qs\in I$, so take the set of all such remainders $R/qs$. There are two possibilities. Either this set has a smallest positive element, or it does not. Suppose it has a smallest such element, call it $x=m/n$. Let $a=p/q\in I$. We show $a=lx$. For suppose not. Rationalizing denominators with $n, q>1$, $p=Lm+Z$, so then $Z<m$ so $Z/qn<m/n=x$, giving a contradiction. Now though, I am stuck on the case where there is no such smallest element :( Any thoughts? EDIT: For those marking it as a duplicate: The first answer in the linked question, as mentioned in the comments, feels somewhat unnatural and requires all this extra machinery. On the other hand, the second answer given is incomplete: The poster says ""Now show that the ideal generated by $t=\frac{t}{1}$ in R is the ideal you started out with."" But if $\frac{r}{s}\in I$, then there exists $k$ such that $r=kt$, so $\frac{r}{s}=\frac{k}{s}\cdot \frac{t}{1}$, but $\frac{k}{s}$ is not necessarily in $S$.","Let $S$ be a subring of $\mathbb{Q}$ containing $\mathbb{Z}$. Prove that it is a principal ideal domain. So here is what I tried. Take any ideal $I\subset S$. Take any two elements, say $a=p/q, b=r/s$ in $I$. Rationalize denominators (we can do that since $\mathbb{Z}\subset S$. Now we have $aqs=p=K(r)=K(bqs)+R$, that is, just apply the Euclidean Algorithm. Now, $a-Kb=R/qs\in I$, so take the set of all such remainders $R/qs$. There are two possibilities. Either this set has a smallest positive element, or it does not. Suppose it has a smallest such element, call it $x=m/n$. Let $a=p/q\in I$. We show $a=lx$. For suppose not. Rationalizing denominators with $n, q>1$, $p=Lm+Z$, so then $Z<m$ so $Z/qn<m/n=x$, giving a contradiction. Now though, I am stuck on the case where there is no such smallest element :( Any thoughts? EDIT: For those marking it as a duplicate: The first answer in the linked question, as mentioned in the comments, feels somewhat unnatural and requires all this extra machinery. On the other hand, the second answer given is incomplete: The poster says ""Now show that the ideal generated by $t=\frac{t}{1}$ in R is the ideal you started out with."" But if $\frac{r}{s}\in I$, then there exists $k$ such that $r=kt$, so $\frac{r}{s}=\frac{k}{s}\cdot \frac{t}{1}$, but $\frac{k}{s}$ is not necessarily in $S$.",,"['abstract-algebra', 'ring-theory', 'principal-ideal-domains']"
46,Equivalent conditions of a Galois extension (Exercise VI.4 in Lang's Algebra),Equivalent conditions of a Galois extension (Exercise VI.4 in Lang's Algebra),,"let $k$ be a field of characteristic $\neq 2$. Let $c\in k, c\notin k^2$. Let $F=k(\sqrt{c})$ . Let $\alpha=a+b\sqrt{c}$ with $a,b\in k$ not both $a,b=0$. Let $E=F(\sqrt{\alpha})$. Prove that the following are equivalent : $E$ is Galois over $k$. $E=F(\sqrt{\alpha'})$ where $\alpha'=a-b\sqrt{c}$. Either $\alpha\alpha'\in k^2$ or $c\alpha \alpha'\in k^2$. Show that when these conditions are satisfied, then $E$ is cyclic over $k$ of degree $4$ iff $c\alpha\alpha'\in k^2$. What i have tried so far is : $E=F(\sqrt{a+b\sqrt{c}})=k(\sqrt{c},\sqrt{a+b\sqrt{c}})=k(\sqrt{a+b\sqrt{c}})$ As $\sqrt{a-b\sqrt{c}}$ is another root  of minimal polynomial of $\sqrt{a+b\sqrt{c}}$ and as $E/k$ is galois we see that $E=F(\sqrt{a-b\sqrt{c}})$ (adjoining any root should give same extension).. So,  first statement implies second.. Suppose $\alpha\alpha'\in k^2$ i.e., $a^2-b^2c\in k^2$ then i can write $\sqrt{a+b\sqrt{c}}=\sqrt{m}+\sqrt{n}$ with $m,n\in k$. To be precise, $m=\frac{1}{2}(a+\sqrt{a^2-b^2c})$ and $n=\frac{1}{2}(a-\sqrt{a^2-b^2c})$ Then i have $E=k(\sqrt{a+b\sqrt{c}})=k(\sqrt{m}+\sqrt{n})=k(\sqrt{m},\sqrt{n})$ We have a result that says $k(\sqrt{m},\sqrt{n})$ is of degree $4$ if $mn $  is not a square in $k$.. In this case we have $mn=c$ which is not a square in $k$ so $E$ is of degree $4$ over $k$... More over i can see in this case we have galois group to be $\mathbb{Z}_2\times \mathbb{Z}_2$. So,  i have proved that if $c\alpha\alpha'\notin k^2$ then Galois group is not cyclic..  As there are only two possible groups of order $4$ i have prove that $E$ is cyclic over $k$ of degree $4$ iff $c\alpha\alpha'\in k^2$. I have no idea how do you proceed proving second statement implies the third and third implies first.. Please give some hints..","let $k$ be a field of characteristic $\neq 2$. Let $c\in k, c\notin k^2$. Let $F=k(\sqrt{c})$ . Let $\alpha=a+b\sqrt{c}$ with $a,b\in k$ not both $a,b=0$. Let $E=F(\sqrt{\alpha})$. Prove that the following are equivalent : $E$ is Galois over $k$. $E=F(\sqrt{\alpha'})$ where $\alpha'=a-b\sqrt{c}$. Either $\alpha\alpha'\in k^2$ or $c\alpha \alpha'\in k^2$. Show that when these conditions are satisfied, then $E$ is cyclic over $k$ of degree $4$ iff $c\alpha\alpha'\in k^2$. What i have tried so far is : $E=F(\sqrt{a+b\sqrt{c}})=k(\sqrt{c},\sqrt{a+b\sqrt{c}})=k(\sqrt{a+b\sqrt{c}})$ As $\sqrt{a-b\sqrt{c}}$ is another root  of minimal polynomial of $\sqrt{a+b\sqrt{c}}$ and as $E/k$ is galois we see that $E=F(\sqrt{a-b\sqrt{c}})$ (adjoining any root should give same extension).. So,  first statement implies second.. Suppose $\alpha\alpha'\in k^2$ i.e., $a^2-b^2c\in k^2$ then i can write $\sqrt{a+b\sqrt{c}}=\sqrt{m}+\sqrt{n}$ with $m,n\in k$. To be precise, $m=\frac{1}{2}(a+\sqrt{a^2-b^2c})$ and $n=\frac{1}{2}(a-\sqrt{a^2-b^2c})$ Then i have $E=k(\sqrt{a+b\sqrt{c}})=k(\sqrt{m}+\sqrt{n})=k(\sqrt{m},\sqrt{n})$ We have a result that says $k(\sqrt{m},\sqrt{n})$ is of degree $4$ if $mn $  is not a square in $k$.. In this case we have $mn=c$ which is not a square in $k$ so $E$ is of degree $4$ over $k$... More over i can see in this case we have galois group to be $\mathbb{Z}_2\times \mathbb{Z}_2$. So,  i have proved that if $c\alpha\alpha'\notin k^2$ then Galois group is not cyclic..  As there are only two possible groups of order $4$ i have prove that $E$ is cyclic over $k$ of degree $4$ iff $c\alpha\alpha'\in k^2$. I have no idea how do you proceed proving second statement implies the third and third implies first.. Please give some hints..",,['abstract-algebra']
47,Prove that the kernel of a group homomorphism $\phi$ is a subgroup and that $\phi$ is injective,Prove that the kernel of a group homomorphism  is a subgroup and that  is injective,\phi \phi,"I am solving the following exercise: Let $\phi : G_1 \rightarrow G_2$ be a homomorphism (where $G_1$ and   $G_2$ are groups) and $\ker \phi := \{ g \in G_1 \mid \phi(g) = e \}$ now I have to prove that a) $\ker \phi$ is a subgroup of $G_1$, b) $\phi$ is injective if and only $\ker \phi = \{ e \}$ My Problem: Until yet we have not really covered the topic of homomorphism between groups in our lectures. Anyhow I looked it up on wikipedia and found the definition for a subgroup as the following: $(U, \circ)$ is a subgroup of $(G, \circ)$ if $U$ is not an empty set. Therefore: $a,b \in U \Rightarrow a \circ b \in U$ $a \in U \Rightarrow a^{-1} \in U$ $a,b \in U \Rightarrow a \circ b^{-1} \in U$ so I began to work with these definitions. I somehow managed to prove what I'm supposed to but I'm not sure if I did it the right way. I would be very thankful about some additional words to my attempt and also corrections. Thank you a lot in advance. My Attempt: a) $\ker \phi$ is a subgroup of $G_1$. So we can take two elements $x,y \in \ker \phi$ which are $x := \phi(g_1) = e $ and $y := \phi(g_2) = e $ and show that $x^{-1}$ and $x \circ y$ $\in$ $\ker \phi$. Since $x\circ x^{-1} \in \ker \phi$ we can say: $x\circ x^{-1} = e \ \Leftrightarrow  \ \overbrace{\phi(g_1)}^{= \ e} \circ x^{-1} = e \ \Rightarrow \ x^{-1} = e \ \Rightarrow \ x^{-1} \in \ker \phi$. It must also be true that $x \circ y \in \ker \phi$ this is easily shown by: $x \circ y \ \Leftrightarrow \ \overbrace{\phi(g_1)}^{= \ e} \circ \overbrace{\phi(g_2)}^{= \ e} = e \ \Rightarrow \ x \circ y \in \ker\phi$ b) To show that $\phi$ is injective when $ \ker\phi = \{ e_{G_1} \}$ we must show that $ \ker\phi = \{ e_{G_1} \}$ has only one fiber which then has to be $\phi^{-1}(e_{G_2})$. So we can take two elements $g_1,g_2 \in G_1$ and if $\phi(g_1) = \phi(g_2) = e \ \Rightarrow \ g_1 = g_2$ we can state that $\phi$ with $\ker \phi = \{e\}$ is injective.","I am solving the following exercise: Let $\phi : G_1 \rightarrow G_2$ be a homomorphism (where $G_1$ and   $G_2$ are groups) and $\ker \phi := \{ g \in G_1 \mid \phi(g) = e \}$ now I have to prove that a) $\ker \phi$ is a subgroup of $G_1$, b) $\phi$ is injective if and only $\ker \phi = \{ e \}$ My Problem: Until yet we have not really covered the topic of homomorphism between groups in our lectures. Anyhow I looked it up on wikipedia and found the definition for a subgroup as the following: $(U, \circ)$ is a subgroup of $(G, \circ)$ if $U$ is not an empty set. Therefore: $a,b \in U \Rightarrow a \circ b \in U$ $a \in U \Rightarrow a^{-1} \in U$ $a,b \in U \Rightarrow a \circ b^{-1} \in U$ so I began to work with these definitions. I somehow managed to prove what I'm supposed to but I'm not sure if I did it the right way. I would be very thankful about some additional words to my attempt and also corrections. Thank you a lot in advance. My Attempt: a) $\ker \phi$ is a subgroup of $G_1$. So we can take two elements $x,y \in \ker \phi$ which are $x := \phi(g_1) = e $ and $y := \phi(g_2) = e $ and show that $x^{-1}$ and $x \circ y$ $\in$ $\ker \phi$. Since $x\circ x^{-1} \in \ker \phi$ we can say: $x\circ x^{-1} = e \ \Leftrightarrow  \ \overbrace{\phi(g_1)}^{= \ e} \circ x^{-1} = e \ \Rightarrow \ x^{-1} = e \ \Rightarrow \ x^{-1} \in \ker \phi$. It must also be true that $x \circ y \in \ker \phi$ this is easily shown by: $x \circ y \ \Leftrightarrow \ \overbrace{\phi(g_1)}^{= \ e} \circ \overbrace{\phi(g_2)}^{= \ e} = e \ \Rightarrow \ x \circ y \in \ker\phi$ b) To show that $\phi$ is injective when $ \ker\phi = \{ e_{G_1} \}$ we must show that $ \ker\phi = \{ e_{G_1} \}$ has only one fiber which then has to be $\phi^{-1}(e_{G_2})$. So we can take two elements $g_1,g_2 \in G_1$ and if $\phi(g_1) = \phi(g_2) = e \ \Rightarrow \ g_1 = g_2$ we can state that $\phi$ with $\ker \phi = \{e\}$ is injective.",,"['group-theory', 'abstract-algebra']"
48,Let $G$ be a finite group. Show that $G$ is isomorphic to a subgroup of $S_n$ (symmetric group).,Let  be a finite group. Show that  is isomorphic to a subgroup of  (symmetric group).,G G S_n,"I am solving a geometry exercise in which I have to proof the following: Let $G$ be a finite group with $n=\vert G \vert$ elements. Show that $G$ is isomorph to a subset of $S_n$ (symmertric group). Problem: Since we only had 4 geometry lectures we have not covered a lot. We get an exercise sheet every 2nd week and have lectures every week. We have not covered cayley's theorem yet. Is this theorem necessary to solve the exercise? I looked up cayley's theorem and its proof in M.Artin - Algebra and Fraleigh's First Course in Abstract Algebra . I tried it on my own but I would be glad if you could correct or enhance my 'proof'. My Attempt :  Let $|G|=n$ . We want $|G|$ to me isomorph to $S_n$. So we can number the elements of $G$: $ \begin{array}{ccc} 1 & 2 & \ldots & n\\ a_1 & a_2 & \ldots & a_n \end{array}  $ we can now choose $x\in G$ and do a left multiplication, which leads to the following $\Rightarrow$ $ \begin{array}{ccc} 1 & 2 & \ldots & n\\ x \cdot a_1 & x \cdot a_2 & \ldots &  x \cdot a_n \end{array}  $ so we can say therefore the elements will be newly arranged in $G$ after the multiplication and because we numbered the elements of $G$ we can 'look up' where they have been before. That actually means: $x \cdot a_i = a_j$ so therefore x sends element $a$ from place $i$ to place $j$ (permutation). So we define the following definition: $\lambda_x=\begin{cases} \{1,2,\ldots,n\} \rightarrow \{1,2,\ldots,n\} \\ \lambda_x(i) \quad \qquad \mapsto \ j \end{cases}$ so therefore we can write: $ \begin{array}{ccc} 1 & 2 & \ldots & n\\ a_{\lambda_x(1)} & a_{\lambda_x(2)} & \ldots & a_{\lambda_x(n)} \end{array}  $ now we can state a new function: $\phi=\begin{cases} G \quad \rightarrow \quad S_n \\ \phi(\lambda(x)) \mapsto \lambda_x \end{cases}$ which basically means: we link the permutation that the left multiplication by x does to its x. Now we have to proof the isomorphism between $G$ and $S_n$ therefore it needs to be bijective and homomorphic. Well $\phi$ needs to be a group homomorphism and bijective which means: homomorphism: Let $\lambda(i),\lambda(j) \in G$ , $(G, \star)$ and $(S_n, *)$ be the two groups then: $\phi(\lambda(i) \star \lambda(j))= \phi(\lambda(i)) * \phi(\lambda(j)) = \lambda_i * \lambda_j$ bijective: injective: $\forall \lambda(i),\lambda(j) \in G\ , \ \phi(\lambda(i)) = \phi(\lambda(j)) \Rightarrow \lambda_i=\lambda_j$ surjective: $ \forall \lambda_x \in S_n \ , \ \exists \lambda(x) \in G : \phi(\lambda(x)) = \lambda_x \qquad  \qquad \qquad \qquad \qquad \qquad \qquad \qquad \Box$","I am solving a geometry exercise in which I have to proof the following: Let $G$ be a finite group with $n=\vert G \vert$ elements. Show that $G$ is isomorph to a subset of $S_n$ (symmertric group). Problem: Since we only had 4 geometry lectures we have not covered a lot. We get an exercise sheet every 2nd week and have lectures every week. We have not covered cayley's theorem yet. Is this theorem necessary to solve the exercise? I looked up cayley's theorem and its proof in M.Artin - Algebra and Fraleigh's First Course in Abstract Algebra . I tried it on my own but I would be glad if you could correct or enhance my 'proof'. My Attempt :  Let $|G|=n$ . We want $|G|$ to me isomorph to $S_n$. So we can number the elements of $G$: $ \begin{array}{ccc} 1 & 2 & \ldots & n\\ a_1 & a_2 & \ldots & a_n \end{array}  $ we can now choose $x\in G$ and do a left multiplication, which leads to the following $\Rightarrow$ $ \begin{array}{ccc} 1 & 2 & \ldots & n\\ x \cdot a_1 & x \cdot a_2 & \ldots &  x \cdot a_n \end{array}  $ so we can say therefore the elements will be newly arranged in $G$ after the multiplication and because we numbered the elements of $G$ we can 'look up' where they have been before. That actually means: $x \cdot a_i = a_j$ so therefore x sends element $a$ from place $i$ to place $j$ (permutation). So we define the following definition: $\lambda_x=\begin{cases} \{1,2,\ldots,n\} \rightarrow \{1,2,\ldots,n\} \\ \lambda_x(i) \quad \qquad \mapsto \ j \end{cases}$ so therefore we can write: $ \begin{array}{ccc} 1 & 2 & \ldots & n\\ a_{\lambda_x(1)} & a_{\lambda_x(2)} & \ldots & a_{\lambda_x(n)} \end{array}  $ now we can state a new function: $\phi=\begin{cases} G \quad \rightarrow \quad S_n \\ \phi(\lambda(x)) \mapsto \lambda_x \end{cases}$ which basically means: we link the permutation that the left multiplication by x does to its x. Now we have to proof the isomorphism between $G$ and $S_n$ therefore it needs to be bijective and homomorphic. Well $\phi$ needs to be a group homomorphism and bijective which means: homomorphism: Let $\lambda(i),\lambda(j) \in G$ , $(G, \star)$ and $(S_n, *)$ be the two groups then: $\phi(\lambda(i) \star \lambda(j))= \phi(\lambda(i)) * \phi(\lambda(j)) = \lambda_i * \lambda_j$ bijective: injective: $\forall \lambda(i),\lambda(j) \in G\ , \ \phi(\lambda(i)) = \phi(\lambda(j)) \Rightarrow \lambda_i=\lambda_j$ surjective: $ \forall \lambda_x \in S_n \ , \ \exists \lambda(x) \in G : \phi(\lambda(x)) = \lambda_x \qquad  \qquad \qquad \qquad \qquad \qquad \qquad \qquad \Box$",,"['abstract-algebra', 'group-theory', 'group-isomorphism']"
49,Fields extensions over isomorphic fields of different degrees,Fields extensions over isomorphic fields of different degrees,,"What are the simplest examples of situations where in a field $F$ there are two subfields $L_1$ and $L_2$ such that extensions $F/L_1$ and $F/L_2$ are finite, degrees are different $$ [F:L_1] \neq [F:L_2], $$ but fields $L_1$ and $L_2$ are isomorphic as abstract fields.","What are the simplest examples of situations where in a field $F$ there are two subfields $L_1$ and $L_2$ such that extensions $F/L_1$ and $F/L_2$ are finite, degrees are different $$ [F:L_1] \neq [F:L_2], $$ but fields $L_1$ and $L_2$ are isomorphic as abstract fields.",,"['abstract-algebra', 'field-theory', 'extension-field']"
50,Basis-free and noncommutative versions of the two-polynomials-over-ring problem (McCoy theorem etc.),Basis-free and noncommutative versions of the two-polynomials-over-ring problem (McCoy theorem etc.),,"There is a rather canonical bunch of exercises in commutative algebra which tend to come up time and again on math.stackexchange: recently in #948010 and #83121 , formerly in #227787 and #413788 , and in many other places, such as Messing/Reiner arXiv:1209.6307v2 . Probably its most well-known appearance is as Exercise 2 in Chapter 1 of Atiyah/Macdonald's ""Introduction to Commutative Algebra"". Let me rephrase that exercise: Let $A$ be a commutative ring, and let $A\left[x\right]$ be the ring of polynomials in one variable $x$ over $A$ . Let $f \in A\left[x\right]$ . (a) Show that $f$ is a unit in the ring $A\left[x\right]$ if and only if the constant coefficient of $f$ is a unit and all other coefficients are nilpotent. (b) Show that $f$ is nilpotent if and only if all coefficients of $f$ are nilpotent. (c) Show that $f$ is a non-zero-divisor in $A\left[x\right]$ if and only if every $a \in A$ satisfying $af = 0$ satisfies $a = 0$ . (We say that an element $u$ of a commutative ring $B$ is a non-zero-divisor if and only if every $v \in B$ satisfying $uv = 0$ satisfies $v = 0$ .) (d) We say that a polynomial in $A\left[x\right]$ is primitive if and only if $1$ is an $A$ -linear combination of its coefficients. Show that for any two polynomials $f$ and $g$ in $A\left[x\right]$ , the product $fg$ is primitive if and only if $f$ and $g$ are primitive. Notice that what I call (c) is the contrapositive of Atiyah/Macdonald's Exercise 2 (c) , as the notion of a non-zero-divisor is the correct constructive way to formalize statements about zero-divisors. The exercise is followed by an Exercise 3 which asks for generalizations of all of these results to multivariate polynomial rings $A\left[x_1, x_2, ..., x_n\right]$ ; I believe these can be done by induction over $n$ (though I have not really checked). Solutions of the problem given in literature are usually not constructive per se, but can often be rewritten in constructive terms. $\newcommand\Sym{\operatorname{Sym}}$ A question I have posed to myself long ago, but never had the time to seriously think about, is the following: A polynomial ring is a particular case of a symmetric algebra. What happens if we blindly generalize the exercise to symmetric algebras in general? Let $A$ be a commutative ring, and let $V$ be an $A$ -module. Let $\Sym V$ denote the symmetric algebra of $V$ over $A$ . Let $f \in \Sym V$ . (Sa) Prove or disprove that $f$ is a unit in the ring $\Sym V$ if and only if the $0$ -th homogeneous component of $f$ is a unit and all other homogeneous components are nilpotent. (Sb) Prove that $f$ is nilpotent if and only if all homogeneous components of $f$ are nilpotent. [This one is actually true by an easy induction argument.] (Sc) Prove or disprove that $f$ is a non-zero-divisor in $\Sym V$ if and only if every $a \in A$ satisfying $af = 0$ satisfies $a = 0$ . (Sd) We say that an element $h$ of $\Sym V$ is primitive if and only if $F\left(h\right) = 1$ for some $F \in \left(\Sym V\right)^\ast$ (linear dual). Prove or disprove that for any two elements $f$ and $g$ of $\Sym V$ , the product $fg$ is primitive if and only if $f$ and $g$ are primitive. These generalizations of the polynomial-ring exercise are by no means the only ones, the right ones or the canonical ones. I had to generalize the notion of a coefficient differently for (Sa) and for (Sd) to not get something obviously stupid, and I would not be totally surprised if the result is still wrong. Another direction to generalize things in is that of noncommutative polynomials. There is no difference between commutative polynomial rings $A\left[x\right]$ and noncommutative polynomial rings $A\left<x\right>$ in one variable, so let us state the question in multiple variables: Let $A$ be a commutative ring, and let $n \in \mathbb{N}$ . Let $A\left<x_1, x_2, ..., x_n\right>$ be the ring of noncommutative polynomials in the variables $x_1, x_2, ..., x_n$ over $A$ . (This is the monoid ring of the free monoid generated by $x_1, x_2, ..., x_n$ .) Let $f \in A\left<x_1, x_2, ..., x_n\right>$ . (Na) Prove or disprove that $f$ is a unit in the ring $A\left<x_1, x_2, ..., x_n\right>$ if and only if the constant coefficient of $f$ is a unit in $A$ and all other coefficients are nilpotent. (Nb) Prove or disprove that $f$ is nilpotent if and only if all coefficients of $f$ are nilpotent. (Nc) Prove or disprove that $f$ is a left non-zero-divisor in $A\left<x_1, x_2, ..., x_n\right>$ if and only if every $a \in A$ satisfying $af = 0$ satisfies $a = 0$ . (We say that an element $u$ of a ring $B$ is a left non-zero-divisor if and only if every $v \in B$ satisfying $uv = 0$ satisfies $v = 0$ .) (Nd) We say that a noncommutative polynomial in $A\left<x_1, x_2, ..., x_n\right>$ is primitive if and only if $1$ is an $A$ -linear combination of its coefficients. Prove or disprove that for any two noncommutative polynomials $f$ and $g$ in $A\left<x_1, x_2, ..., x_n\right>$ , the product $fg$ is primitive if and only if $f$ and $g$ are primitive. Finally, the two generalizations can be combined into one that concerns the tensor algebra. (This is not to say that it will entail the symmetric-algebra version as a corollary.) I'll leave stating the conjectures to the reader, as this post is long enough.","There is a rather canonical bunch of exercises in commutative algebra which tend to come up time and again on math.stackexchange: recently in #948010 and #83121 , formerly in #227787 and #413788 , and in many other places, such as Messing/Reiner arXiv:1209.6307v2 . Probably its most well-known appearance is as Exercise 2 in Chapter 1 of Atiyah/Macdonald's ""Introduction to Commutative Algebra"". Let me rephrase that exercise: Let be a commutative ring, and let be the ring of polynomials in one variable over . Let . (a) Show that is a unit in the ring if and only if the constant coefficient of is a unit and all other coefficients are nilpotent. (b) Show that is nilpotent if and only if all coefficients of are nilpotent. (c) Show that is a non-zero-divisor in if and only if every satisfying satisfies . (We say that an element of a commutative ring is a non-zero-divisor if and only if every satisfying satisfies .) (d) We say that a polynomial in is primitive if and only if is an -linear combination of its coefficients. Show that for any two polynomials and in , the product is primitive if and only if and are primitive. Notice that what I call (c) is the contrapositive of Atiyah/Macdonald's Exercise 2 (c) , as the notion of a non-zero-divisor is the correct constructive way to formalize statements about zero-divisors. The exercise is followed by an Exercise 3 which asks for generalizations of all of these results to multivariate polynomial rings ; I believe these can be done by induction over (though I have not really checked). Solutions of the problem given in literature are usually not constructive per se, but can often be rewritten in constructive terms. A question I have posed to myself long ago, but never had the time to seriously think about, is the following: A polynomial ring is a particular case of a symmetric algebra. What happens if we blindly generalize the exercise to symmetric algebras in general? Let be a commutative ring, and let be an -module. Let denote the symmetric algebra of over . Let . (Sa) Prove or disprove that is a unit in the ring if and only if the -th homogeneous component of is a unit and all other homogeneous components are nilpotent. (Sb) Prove that is nilpotent if and only if all homogeneous components of are nilpotent. [This one is actually true by an easy induction argument.] (Sc) Prove or disprove that is a non-zero-divisor in if and only if every satisfying satisfies . (Sd) We say that an element of is primitive if and only if for some (linear dual). Prove or disprove that for any two elements and of , the product is primitive if and only if and are primitive. These generalizations of the polynomial-ring exercise are by no means the only ones, the right ones or the canonical ones. I had to generalize the notion of a coefficient differently for (Sa) and for (Sd) to not get something obviously stupid, and I would not be totally surprised if the result is still wrong. Another direction to generalize things in is that of noncommutative polynomials. There is no difference between commutative polynomial rings and noncommutative polynomial rings in one variable, so let us state the question in multiple variables: Let be a commutative ring, and let . Let be the ring of noncommutative polynomials in the variables over . (This is the monoid ring of the free monoid generated by .) Let . (Na) Prove or disprove that is a unit in the ring if and only if the constant coefficient of is a unit in and all other coefficients are nilpotent. (Nb) Prove or disprove that is nilpotent if and only if all coefficients of are nilpotent. (Nc) Prove or disprove that is a left non-zero-divisor in if and only if every satisfying satisfies . (We say that an element of a ring is a left non-zero-divisor if and only if every satisfying satisfies .) (Nd) We say that a noncommutative polynomial in is primitive if and only if is an -linear combination of its coefficients. Prove or disprove that for any two noncommutative polynomials and in , the product is primitive if and only if and are primitive. Finally, the two generalizations can be combined into one that concerns the tensor algebra. (This is not to say that it will entail the symmetric-algebra version as a corollary.) I'll leave stating the conjectures to the reader, as this post is long enough.","A A\left[x\right] x A f \in A\left[x\right] f A\left[x\right] f f f f A\left[x\right] a \in A af = 0 a = 0 u B v \in B uv = 0 v = 0 A\left[x\right] 1 A f g A\left[x\right] fg f g A\left[x_1, x_2, ..., x_n\right] n \newcommand\Sym{\operatorname{Sym}} A V A \Sym V V A f \in \Sym V f \Sym V 0 f f f f \Sym V a \in A af = 0 a = 0 h \Sym V F\left(h\right) = 1 F \in \left(\Sym V\right)^\ast f g \Sym V fg f g A\left[x\right] A\left<x\right> A n \in \mathbb{N} A\left<x_1, x_2, ..., x_n\right> x_1, x_2, ..., x_n A x_1, x_2, ..., x_n f \in A\left<x_1, x_2, ..., x_n\right> f A\left<x_1, x_2, ..., x_n\right> f A f f f A\left<x_1, x_2, ..., x_n\right> a \in A af = 0 a = 0 u B v \in B uv = 0 v = 0 A\left<x_1, x_2, ..., x_n\right> 1 A f g A\left<x_1, x_2, ..., x_n\right> fg f g","['abstract-algebra', 'polynomials', 'commutative-algebra', 'noncommutative-algebra']"
51,First Isomorphism Theorem to identify a quotient,First Isomorphism Theorem to identify a quotient,,"I understand the notion of quotient groups quite well (I think), but I'm struggling a little bit with the following problem: Let $G$ denote the group of 2x2 invertible real upper triangular matrices, and $H\vartriangleleft G$ the subgroup with $a_{11}=a_{22}$. Identify the quotient group $G/H$ up to isomorphism using the First Isomorphism Theorem. The theorem gives a way to find the quotient once a homomorphism $\varphi:G\to G'$ with $G'$ some unknown group and $\ker\varphi=H$ has been found. Is there any systematic way to find $\varphi$ and/or $G'$ without blindly groping around in the dark and trying test cases?","I understand the notion of quotient groups quite well (I think), but I'm struggling a little bit with the following problem: Let $G$ denote the group of 2x2 invertible real upper triangular matrices, and $H\vartriangleleft G$ the subgroup with $a_{11}=a_{22}$. Identify the quotient group $G/H$ up to isomorphism using the First Isomorphism Theorem. The theorem gives a way to find the quotient once a homomorphism $\varphi:G\to G'$ with $G'$ some unknown group and $\ker\varphi=H$ has been found. Is there any systematic way to find $\varphi$ and/or $G'$ without blindly groping around in the dark and trying test cases?",,['abstract-algebra']
52,Self studying higher mathematics?,Self studying higher mathematics?,,I'm fairly well-versed in calculus but I would like to explore beyond calculus. I have looked into the basics of some topics in higher mathematics such as group theory and abstract algebra and they intrigue me. I am wondering if there are any recommended methods or resources I should use to learn more about these topics. And is there any recommended starting point?,I'm fairly well-versed in calculus but I would like to explore beyond calculus. I have looked into the basics of some topics in higher mathematics such as group theory and abstract algebra and they intrigue me. I am wondering if there are any recommended methods or resources I should use to learn more about these topics. And is there any recommended starting point?,,"['abstract-algebra', 'group-theory', 'soft-question']"
53,About automorphisms of commutative semigroups,About automorphisms of commutative semigroups,,"Suppose that $M$ is a commutative monoid and that the product $P$ of $M$ and the nonnegative integers $\mathbb{N}$ with addition has no nontrivial automorphisms. The set $S$ of pairs $(m,n)$ in $P$ with $n>0$ is closed under addition. Can $S$ have a  nontrivial automorphism?","Suppose that $M$ is a commutative monoid and that the product $P$ of $M$ and the nonnegative integers $\mathbb{N}$ with addition has no nontrivial automorphisms. The set $S$ of pairs $(m,n)$ in $P$ with $n>0$ is closed under addition. Can $S$ have a  nontrivial automorphism?",,"['abstract-algebra', 'semigroups']"
54,"Prove that $k(\alpha+\beta)=k(\alpha,\beta)$",Prove that,"k(\alpha+\beta)=k(\alpha,\beta)","I am trying to solve the following problem: Let $k$ be a finite field and let $k(\alpha,\beta)/k$ be finite. If $k(\alpha)\cap k(\beta)=k$, prove that $k(\alpha,\beta)=k(\alpha+\beta)$. What I already know: $k(\alpha+\beta)\subset k(\alpha,\beta)$ $k(\alpha,\beta)/k$ is a Galois extension. $[k(\alpha,\beta):k]=[k(\alpha):k][k(\beta):k]$. Some thoughts in mind: Prove that $\alpha\in k(\alpha+\beta)$ or $\beta\in k(\alpha+\beta)$. Prove that $[k(\alpha,\beta):k(\alpha+\beta)]=1$ Try to express $[k(\alpha+\beta):k]$ in terms of $[k(\alpha):k]$ and $[k(\beta):k]$. Try to use some Fundamental Theorem of Galois Theory. However, I still have no idea how to move on. Any help?","I am trying to solve the following problem: Let $k$ be a finite field and let $k(\alpha,\beta)/k$ be finite. If $k(\alpha)\cap k(\beta)=k$, prove that $k(\alpha,\beta)=k(\alpha+\beta)$. What I already know: $k(\alpha+\beta)\subset k(\alpha,\beta)$ $k(\alpha,\beta)/k$ is a Galois extension. $[k(\alpha,\beta):k]=[k(\alpha):k][k(\beta):k]$. Some thoughts in mind: Prove that $\alpha\in k(\alpha+\beta)$ or $\beta\in k(\alpha+\beta)$. Prove that $[k(\alpha,\beta):k(\alpha+\beta)]=1$ Try to express $[k(\alpha+\beta):k]$ in terms of $[k(\alpha):k]$ and $[k(\beta):k]$. Try to use some Fundamental Theorem of Galois Theory. However, I still have no idea how to move on. Any help?",,"['abstract-algebra', 'field-theory', 'galois-theory']"
55,Projective modules over $kG$ equivalent to injective.,Projective modules over  equivalent to injective.,kG,"Let $k$ be a field and $G$ is finite group. I want to prove that a $kG$ module $P$ is projective iff it's injective. I proved that if module is projective then it's injective. 1) $kG$ is injective because $Hom_{kG}(M,kG)=Hom_{k}(M,k)$. So every free $kG$ module is injective. 2) Every projective module is a free summand of free, so it's injective. But I don't know how can I prove that injective modules are projective","Let $k$ be a field and $G$ is finite group. I want to prove that a $kG$ module $P$ is projective iff it's injective. I proved that if module is projective then it's injective. 1) $kG$ is injective because $Hom_{kG}(M,kG)=Hom_{k}(M,k)$. So every free $kG$ module is injective. 2) Every projective module is a free summand of free, so it's injective. But I don't know how can I prove that injective modules are projective",,"['abstract-algebra', 'homological-algebra', 'projective-module', 'injective-module', 'group-rings']"
56,"Equivalence of definitions for ""normal extension"" and how to lift isomorphisms to them","Equivalence of definitions for ""normal extension"" and how to lift isomorphisms to them",,"Briefly: I want to prove that these two definitions for ""normal extension"" are equivalent: ""$K$ is a splitting field for a collection of polynomials in $F[x]$"" vs. ""Every irreducible polynomial in $F[x]$ with a root in $K$ splits completely in $K$"". But I don't have much machinery for working with splitting fields of multiple (much less infinite) polynomials. My proof involved a use of well-ordering and transfinite induction that I'm not that confident about. Is there an easier way to prove this? Less briefly: In our abstract algebra class, we were asked to prove the following theorem: Problem: Let $K$ be a finite extension of $F$. Prove that $K$ is a splitting field over $F$ if   and only if every irreducible polynomial in $F[x]$ that has a root in $K$ splits   completely in $K[x]$. I tried to prove this without the restriction on finiteness, because clearly I don't have anything better to do with my time. The backward direction was unchanged, but my proof for the forward direction relied on the ability to ""lift"" isomorphisms to arbitrary splitting fields, i.e., Theorem: Let $\sigma : F \to F'$ be an isomorphism, and let $S \subseteq F[x]$ be a collection of polynomials. If $K$ is the splitting field of $S$ over $F$ and $K'$ is the splitting field of $\sigma(S)$ over $F'$, then there is an isomorphism $\tau : K \to K'$ such that $\tau$ restricted to $F$ is exactly $\sigma$. We have already proven that we can do this for the splitting field of a single polynomial, but induction can only get me so far (finite collections). So I tried to use transfinite induction. Here's a rough sketch of my proof (I wrote it out in full on the problem set, so I can fill in any details if need be): By the Axiom of Choice, we can well-order $S = (p_i)_{i \in I}$ by some index set $I$. Let $F_0 = F$, and for each $i \in I$, let $F_i$ be the splitting field of $p_i$ over $\bigcup_{j < i} F_j$. Let $K = \bigcup_{i \in I} K_i$. [Proof that $K$ is a splitting field of $S$ over $F$] Do the same for $F'$. Let $\sigma_0 = \sigma$. For every successor ordinal $i + 1$, let $\sigma_{i + 1} : F_{i+1} \to F'_{i+1}$ be a lift of $\sigma_i$. For every limit ordinal $i$, we have to take an additional step. For all $j < i$, union together all $F_j$ to get $L_i$, and then we can define $\varphi : L_i \to L'_i$ as a 'union' of all the $\sigma_j$. Now, we let $\sigma_i$ be the lift of $\varphi$ to $F_i$ and $F'_i$. Lastly, we union together all the $\sigma_i$ to get a $\tau : K \to K'$, and show it is an isomorphism, and that when restricted to $F$, it is exactly $\sigma$. Here's a diagram I included, but I'm not sure how much that helps. This feels pretty cumbersome, but it also proves many results together (including the existence of algebraic closures!). Is there a cleaner way to prove the theorem? In particular, I'm much more comfortable with Zorn's Lemma than transfinite induction. (I suspect it requires Choice, in one form or another). And if the theorem is still just has hard, is there an easier way to prove the actual problem? The only proofs I've seen seem to take for granted that we can lift isomorphisms to algebraic closures, which seems to be just as hard as what I'm trying to prove.","Briefly: I want to prove that these two definitions for ""normal extension"" are equivalent: ""$K$ is a splitting field for a collection of polynomials in $F[x]$"" vs. ""Every irreducible polynomial in $F[x]$ with a root in $K$ splits completely in $K$"". But I don't have much machinery for working with splitting fields of multiple (much less infinite) polynomials. My proof involved a use of well-ordering and transfinite induction that I'm not that confident about. Is there an easier way to prove this? Less briefly: In our abstract algebra class, we were asked to prove the following theorem: Problem: Let $K$ be a finite extension of $F$. Prove that $K$ is a splitting field over $F$ if   and only if every irreducible polynomial in $F[x]$ that has a root in $K$ splits   completely in $K[x]$. I tried to prove this without the restriction on finiteness, because clearly I don't have anything better to do with my time. The backward direction was unchanged, but my proof for the forward direction relied on the ability to ""lift"" isomorphisms to arbitrary splitting fields, i.e., Theorem: Let $\sigma : F \to F'$ be an isomorphism, and let $S \subseteq F[x]$ be a collection of polynomials. If $K$ is the splitting field of $S$ over $F$ and $K'$ is the splitting field of $\sigma(S)$ over $F'$, then there is an isomorphism $\tau : K \to K'$ such that $\tau$ restricted to $F$ is exactly $\sigma$. We have already proven that we can do this for the splitting field of a single polynomial, but induction can only get me so far (finite collections). So I tried to use transfinite induction. Here's a rough sketch of my proof (I wrote it out in full on the problem set, so I can fill in any details if need be): By the Axiom of Choice, we can well-order $S = (p_i)_{i \in I}$ by some index set $I$. Let $F_0 = F$, and for each $i \in I$, let $F_i$ be the splitting field of $p_i$ over $\bigcup_{j < i} F_j$. Let $K = \bigcup_{i \in I} K_i$. [Proof that $K$ is a splitting field of $S$ over $F$] Do the same for $F'$. Let $\sigma_0 = \sigma$. For every successor ordinal $i + 1$, let $\sigma_{i + 1} : F_{i+1} \to F'_{i+1}$ be a lift of $\sigma_i$. For every limit ordinal $i$, we have to take an additional step. For all $j < i$, union together all $F_j$ to get $L_i$, and then we can define $\varphi : L_i \to L'_i$ as a 'union' of all the $\sigma_j$. Now, we let $\sigma_i$ be the lift of $\varphi$ to $F_i$ and $F'_i$. Lastly, we union together all the $\sigma_i$ to get a $\tau : K \to K'$, and show it is an isomorphism, and that when restricted to $F$, it is exactly $\sigma$. Here's a diagram I included, but I'm not sure how much that helps. This feels pretty cumbersome, but it also proves many results together (including the existence of algebraic closures!). Is there a cleaner way to prove the theorem? In particular, I'm much more comfortable with Zorn's Lemma than transfinite induction. (I suspect it requires Choice, in one form or another). And if the theorem is still just has hard, is there an easier way to prove the actual problem? The only proofs I've seen seem to take for granted that we can lift isomorphisms to algebraic closures, which seems to be just as hard as what I'm trying to prove.",,"['abstract-algebra', 'field-theory', 'extension-field', 'alternative-proof']"
57,Is every subgroup of the product of two cyclic groups is again a product of two cyclic groups?,Is every subgroup of the product of two cyclic groups is again a product of two cyclic groups?,,"Well, this is my question. Is every subgroup of the product of two cyclic groups is again a product of two cyclic groups (maybe one being trivial)? Thanks!","Well, this is my question. Is every subgroup of the product of two cyclic groups is again a product of two cyclic groups (maybe one being trivial)? Thanks!",,"['abstract-algebra', 'group-theory']"
58,"If $G$ is a finite union of some of its abelian subgroups, then the index of the center of the group is finite","If  is a finite union of some of its abelian subgroups, then the index of the center of the group is finite",G,"If $G$ is a finite union of some of its abelian subgroups, then the index of the center of the group is finite Would I not simply state that by Lagrange's theorem, $Z(G)$ can divide into the abelian subgroups, and the abelian subgroups can divide into $G$? This solution seems too obvious and intuitively wrong. Also I don't know how to approach the question if $G$ was infinite, thank you!","If $G$ is a finite union of some of its abelian subgroups, then the index of the center of the group is finite Would I not simply state that by Lagrange's theorem, $Z(G)$ can divide into the abelian subgroups, and the abelian subgroups can divide into $G$? This solution seems too obvious and intuitively wrong. Also I don't know how to approach the question if $G$ was infinite, thank you!",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
59,Localization of Coordinate Rings: $\mathbb C[V_f] = \mathbb C[V]_f$.,Localization of Coordinate Rings: .,\mathbb C[V_f] = \mathbb C[V]_f,"Let $V\subseteq\mathbb C^n$ be an irreducible affine variety, then the coordinate ring $$\mathbb C[V] = \mathbb C[x_1,\dots,x_n]\big/\mathbf I(V)$$ is an integral domain. Let $f\in\mathbb C[V]\setminus\{0\}$, then we can define the localization $$ \mathbb C[V]_f = \left\{\,g\big/f^\ell \in \mathbb C(V)\,\big|\, g\in\mathbb C[V], \ell\ge 0\,\right\}, $$ where $\mathbb C(V)$ denotes the field of fractions of $\mathbb C[V]$. I want to proof that $\mathbb C[V]_f$ is the coordinate ring of the principal open subset $$V_f = \left\{\,p\in V\,\big|\, f(p)\neq 0\,\right\}.$$ We can see $V_f$ as an affine variety by identifying it with $$\widetilde{V_f} = \mathbf V(\mathbf I(V)+\langle gy-1\rangle) \subseteq \mathbb C^n\times \mathbb C,$$ where $y$ is $(n+1)$th coordinate, $g\in\mathbb C[x_1,\dots,x_n]$ represents $f\in\mathbb C[V]$ and the projection $\mathbb C^n\times \mathbb C\to\mathbb C^n$ maps $\widetilde{V_f}$ bijectively onto $V_f$. Then \begin{align} \mathbb C[V_f] &\cong \mathbb C[\widetilde{V_f}] = \mathbb C[x_1,\dots,x_n,y]\big/(\mathbf I(V)+\langle gy-1\rangle)\\ &\cong \mathbb C\left[x_1,\dots,x_n,1\big/g\right]\big/\mathbf I(V)\\ &\cong \left(\mathbb C[V]\right)\left[1\big/f\right] \cong \mathbb C[V]_f. \end{align} Is this reasoning correct? I'm not sure everything done in the last chain of isomorphisms is rigorous.","Let $V\subseteq\mathbb C^n$ be an irreducible affine variety, then the coordinate ring $$\mathbb C[V] = \mathbb C[x_1,\dots,x_n]\big/\mathbf I(V)$$ is an integral domain. Let $f\in\mathbb C[V]\setminus\{0\}$, then we can define the localization $$ \mathbb C[V]_f = \left\{\,g\big/f^\ell \in \mathbb C(V)\,\big|\, g\in\mathbb C[V], \ell\ge 0\,\right\}, $$ where $\mathbb C(V)$ denotes the field of fractions of $\mathbb C[V]$. I want to proof that $\mathbb C[V]_f$ is the coordinate ring of the principal open subset $$V_f = \left\{\,p\in V\,\big|\, f(p)\neq 0\,\right\}.$$ We can see $V_f$ as an affine variety by identifying it with $$\widetilde{V_f} = \mathbf V(\mathbf I(V)+\langle gy-1\rangle) \subseteq \mathbb C^n\times \mathbb C,$$ where $y$ is $(n+1)$th coordinate, $g\in\mathbb C[x_1,\dots,x_n]$ represents $f\in\mathbb C[V]$ and the projection $\mathbb C^n\times \mathbb C\to\mathbb C^n$ maps $\widetilde{V_f}$ bijectively onto $V_f$. Then \begin{align} \mathbb C[V_f] &\cong \mathbb C[\widetilde{V_f}] = \mathbb C[x_1,\dots,x_n,y]\big/(\mathbf I(V)+\langle gy-1\rangle)\\ &\cong \mathbb C\left[x_1,\dots,x_n,1\big/g\right]\big/\mathbf I(V)\\ &\cong \left(\mathbb C[V]\right)\left[1\big/f\right] \cong \mathbb C[V]_f. \end{align} Is this reasoning correct? I'm not sure everything done in the last chain of isomorphisms is rigorous.",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'ring-theory', 'proof-verification']"
60,"Isomorphism between Möbius transformations and $SL(2,\mathbb{C})/\mathbb{Z}_2$",Isomorphism between Möbius transformations and,"SL(2,\mathbb{C})/\mathbb{Z}_2","This is a problem from A Course in Modern Mathematical Physics by Peter Szekeres. Here's the quote to the problem I'm solving: Show that the map $\mu$ from $SL(2,\mathbb{C})$ to the Möbius group is a homomorphism, and that the kernel of this homomorphism is $\{I;-I\}$; i.e the Möbius group is isomorphic to  $SL(2,\mathbb{C})/\mathbb{Z}_2$. What I did was to define $$\mu:\left( \begin{array}{ccc} a & b \\ c & d \end{array} \right)\rightarrow m(z)=\frac{az+b}{cz+d}$$ where $ad-bc=1$. By taking the image of two unimodular matrix product gives us the composition of two Möbius transformations which is exactly the result we expect, it is easy to see why $\{I;-I\}$ (identity matrix and the ""negative"" identity matrix) is the kernel of this homomorphism, by the theorem that goes: If $\varphi: G\rightarrow G'$ is a homomorphism then the factor group $G/\ker(\varphi)$ is isomorphic with the image subgroup $im(\varphi)\subseteq G' $ I can prove that the group of the Möbius transformations is isomorphic to $SL(2,\mathbb{C})/\{I;-I\}$, can't find a way to prove that it is isomorphic to $SL(2,\mathbb{C})/\mathbb{Z}_2$ because i thought that you could only factor a group by a normal subgroup of that group, I don't know if it is possible to factor it by a group wich is isomorphic the normal subgroup of that group. If that were the case, I could prove that $\{I;-I\}$ is isomorphic to $\mathbb{Z}_2$ and then prove that $SL(2,\mathbb{C})/\mathbb{Z}_2 \cong SL(2,\mathbb{C})/\{I;-I\}$ which by transitivity should be isomorphic to the group os Möbius transformations. If this is not posible, i don't see how $\mathbb{Z}_2$ is a normal subgroup of $SL(2,\mathbb{C})$.","This is a problem from A Course in Modern Mathematical Physics by Peter Szekeres. Here's the quote to the problem I'm solving: Show that the map $\mu$ from $SL(2,\mathbb{C})$ to the Möbius group is a homomorphism, and that the kernel of this homomorphism is $\{I;-I\}$; i.e the Möbius group is isomorphic to  $SL(2,\mathbb{C})/\mathbb{Z}_2$. What I did was to define $$\mu:\left( \begin{array}{ccc} a & b \\ c & d \end{array} \right)\rightarrow m(z)=\frac{az+b}{cz+d}$$ where $ad-bc=1$. By taking the image of two unimodular matrix product gives us the composition of two Möbius transformations which is exactly the result we expect, it is easy to see why $\{I;-I\}$ (identity matrix and the ""negative"" identity matrix) is the kernel of this homomorphism, by the theorem that goes: If $\varphi: G\rightarrow G'$ is a homomorphism then the factor group $G/\ker(\varphi)$ is isomorphic with the image subgroup $im(\varphi)\subseteq G' $ I can prove that the group of the Möbius transformations is isomorphic to $SL(2,\mathbb{C})/\{I;-I\}$, can't find a way to prove that it is isomorphic to $SL(2,\mathbb{C})/\mathbb{Z}_2$ because i thought that you could only factor a group by a normal subgroup of that group, I don't know if it is possible to factor it by a group wich is isomorphic the normal subgroup of that group. If that were the case, I could prove that $\{I;-I\}$ is isomorphic to $\mathbb{Z}_2$ and then prove that $SL(2,\mathbb{C})/\mathbb{Z}_2 \cong SL(2,\mathbb{C})/\{I;-I\}$ which by transitivity should be isomorphic to the group os Möbius transformations. If this is not posible, i don't see how $\mathbb{Z}_2$ is a normal subgroup of $SL(2,\mathbb{C})$.",,"['abstract-algebra', 'group-theory', 'linear-groups']"
61,"What is the ""opposite"" of a forgetful functor?","What is the ""opposite"" of a forgetful functor?",,"Consider a category $C$ and a monoid $M$.  Consider a functor $F:C\to M$. It maps the objects of $C$ into the only object of $M$. But I don't want it to map every morphism of $C$ into the identity on $M$.  If $f$ is a morphism in $C$, I would like in general $F(f)$ to be non-trivial. Intuitively, this functor forgets only the underlying graph, and preserves (some of) the operations. In a way, it is the opposite of a forgetful functor. Does a functor with such properties have a name?","Consider a category $C$ and a monoid $M$.  Consider a functor $F:C\to M$. It maps the objects of $C$ into the only object of $M$. But I don't want it to map every morphism of $C$ into the identity on $M$.  If $f$ is a morphism in $C$, I would like in general $F(f)$ to be non-trivial. Intuitively, this functor forgets only the underlying graph, and preserves (some of) the operations. In a way, it is the opposite of a forgetful functor. Does a functor with such properties have a name?",,"['abstract-algebra', 'category-theory', 'terminology', 'monoid', 'monoidal-categories']"
62,How to show that any field extension $K/\mathbb{Q}$ of degree 4 that is not Galois has a quadratic extension $L$ that is Galois over $\mathbb{Q}$.,How to show that any field extension  of degree 4 that is not Galois has a quadratic extension  that is Galois over .,K/\mathbb{Q} L \mathbb{Q},"$\newcommand{\Q}{\mathbb{Q}}$Let $K/\Q$ be a field extension of degree $4$ that is not Galois. How to show that there exists an extension $L\supseteq K$ such that $[L:K]=2$ and $L/\Q$ is Galois? I know the example of $\Q(\sqrt[4]{2})$ which is not Galois but is contained in the splitting field of $x^4-2$ which is Galois and of degree $8$, and I am trying to generalize this. But I am not even sure if we can write $K=\Q(\alpha)$ for some $\alpha$. Anyway, if this is the case, then the splitting field $L$ of the minimal polynomial of $\alpha$ would be Galois and of degree $8$, $12$ or $24$ since $\mathrm{Gal}(L/\Q)$ would be a subgroup of $S_4$. But how to rule out $12$ and $24$?","$\newcommand{\Q}{\mathbb{Q}}$Let $K/\Q$ be a field extension of degree $4$ that is not Galois. How to show that there exists an extension $L\supseteq K$ such that $[L:K]=2$ and $L/\Q$ is Galois? I know the example of $\Q(\sqrt[4]{2})$ which is not Galois but is contained in the splitting field of $x^4-2$ which is Galois and of degree $8$, and I am trying to generalize this. But I am not even sure if we can write $K=\Q(\alpha)$ for some $\alpha$. Anyway, if this is the case, then the splitting field $L$ of the minimal polynomial of $\alpha$ would be Galois and of degree $8$, $12$ or $24$ since $\mathrm{Gal}(L/\Q)$ would be a subgroup of $S_4$. But how to rule out $12$ and $24$?",,"['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
63,Algebraic and Galois Extension is a Splitting Field of some set.,Algebraic and Galois Extension is a Splitting Field of some set.,,"This is taken from the book Algebra by Thomas W. Hungerford ; Theorem. Let $K$ be an extension of $F$. The following are equivalent: $K$ is algebraic and Galois over $F$ . $K$ is separable over $F$ and $K$ is a splitting field over $F$ of a set $S$ of polynomials in $F[x]$. $K$ is the splitting field over $F$ of a set $T$ of separable polynomials in $F[x]$. Proof. (1)$\implies$(2),(3) Let $u\in K$ and let $f(x)\in F[x]$ be the monic irreducible polynomial of $u$. Let $u=u_1,\ldots,u_r$ be the distinct roots of $f$ in $K$; then $r\leq n=\deg(f)$. If $\tau\in\mathrm{Aut}_F(K)$, then $\tau$ permutes the $u_i$. So the coefficients of the polynomial $g(x) = (x-u_1)(x-u_2)\cdots(x-u_r)$ are fixed by all $\tau\in\mathrm{Aut}_F(K)$, and therefore $g(x)\in F[x]$ (since the extension is Galois, so the fixed field of $\mathrm{Aut}_F(K)$ is $F$). Since $u$ is a root of $g$, then $f(x)|g(x)$. Therefore, $n=\deg(f)\leq \deg(g) = r \leq n$, so $\deg(g)=n$. Thus, $f$ has $n$ distinct roots in $K$, so $u$ is separable over $F$. Now let $\{u_i\}_{i\in I}$ be a basis for $K$ over $F$; for each $i\in I$ let $f_i\in F[x]$ be the monic irreducible of $u_i$. Then $K$ is the splitting field over $F$ of $S=\{f_i\}_{i\in I}$, and each $f_i$ is separable. This establishes (2) and (3). What I don't understand is why $K$ is the splitting field of $S=\{f_i\}_{i\in I}$. In order to $K$ to be the splitting field of the said set, $K$ needs to be equal to $F(X)$ (according to the definition of the book) where $X = \{v | v$ is a root of $f_i$ and $i \in I\} $ It can be easily seen that $F(X) \subseteq K$. How do I show the other side?","This is taken from the book Algebra by Thomas W. Hungerford ; Theorem. Let $K$ be an extension of $F$. The following are equivalent: $K$ is algebraic and Galois over $F$ . $K$ is separable over $F$ and $K$ is a splitting field over $F$ of a set $S$ of polynomials in $F[x]$. $K$ is the splitting field over $F$ of a set $T$ of separable polynomials in $F[x]$. Proof. (1)$\implies$(2),(3) Let $u\in K$ and let $f(x)\in F[x]$ be the monic irreducible polynomial of $u$. Let $u=u_1,\ldots,u_r$ be the distinct roots of $f$ in $K$; then $r\leq n=\deg(f)$. If $\tau\in\mathrm{Aut}_F(K)$, then $\tau$ permutes the $u_i$. So the coefficients of the polynomial $g(x) = (x-u_1)(x-u_2)\cdots(x-u_r)$ are fixed by all $\tau\in\mathrm{Aut}_F(K)$, and therefore $g(x)\in F[x]$ (since the extension is Galois, so the fixed field of $\mathrm{Aut}_F(K)$ is $F$). Since $u$ is a root of $g$, then $f(x)|g(x)$. Therefore, $n=\deg(f)\leq \deg(g) = r \leq n$, so $\deg(g)=n$. Thus, $f$ has $n$ distinct roots in $K$, so $u$ is separable over $F$. Now let $\{u_i\}_{i\in I}$ be a basis for $K$ over $F$; for each $i\in I$ let $f_i\in F[x]$ be the monic irreducible of $u_i$. Then $K$ is the splitting field over $F$ of $S=\{f_i\}_{i\in I}$, and each $f_i$ is separable. This establishes (2) and (3). What I don't understand is why $K$ is the splitting field of $S=\{f_i\}_{i\in I}$. In order to $K$ to be the splitting field of the said set, $K$ needs to be equal to $F(X)$ (according to the definition of the book) where $X = \{v | v$ is a root of $f_i$ and $i \in I\} $ It can be easily seen that $F(X) \subseteq K$. How do I show the other side?",,"['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
64,"No field extension is ""degree 4 away from an algebraic closure""","No field extension is ""degree 4 away from an algebraic closure""",,"Question: Suppose $[L:K]=4$ , $\operatorname{char}K \neq 2$ and $L$ is algebraically closed. Show that there is an intermediate field $M$ such that $[L:M]=2$ and $X^2 + 1$ splits over $M$ . Show that this leads to a contradiction. I have successfully found such $M$ . Would somebody please give me some hints to the last part?","Question: Suppose , and is algebraically closed. Show that there is an intermediate field such that and splits over . Show that this leads to a contradiction. I have successfully found such . Would somebody please give me some hints to the last part?",[L:K]=4 \operatorname{char}K \neq 2 L M [L:M]=2 X^2 + 1 M M,['abstract-algebra']
65,group of elements whose orders are a power of some integer,group of elements whose orders are a power of some integer,,"I am stuck with the following problem, from C. Pinter's ""A Book of Abstract Algebra"", p. 153 ex. C 6: Let $G$ be an abelian group, and $H_p$ the subset of $G$ such that the order of every $x \in H_p$ is a power of $p$. Prove that $H_p$ is a subgroup of $G$, and that $G/H_p$ has no elements whose order is a non-zero power of $p$. So, to prove that $H_p$ is a subgroup of $G$, I need to show (among others) that it is closed under the group operation. Let $a$, $b \in H_p$, by hypothesis $\mathrm{ord}(a) = p^m, \mathrm{ord}(b) = p^n$ for some integers $m, n$. Now  consider $(ab)^{p^{mn}}$: since $G$ is abelian, this is equal to $a^{p^{mn}}b^{p^{mn}}$, which is in turn equal to $a^{{(p^m)}^n}b^{{(p^n)}^m}$, which is equal to the identity $e$, since $p^m = \mathrm{ord}(a)$ and $p^n = \mathrm{ord}(b)$. In other words, $(ab)^{p^{mn}} = e$, hence $\mathrm{ord}(ab)$ divides $p^{mn}$. Now here is where I am puzzled: nothing is assumed about $p$. For if $p$ were assumed to be prime (as the choice of the letter ""p"" seems to indicate), it would indeed seem to follow that $\mathrm{ord}(ab)$ must itself be a power of $p$. Conversely, there seem to be counterexamples when $p$ is not prime, e.g. if we take $G = (\mathbb{Z}_{12},+)$ and set $p = 4$, then $H_4$ contains $3$ ($\mathrm{ord}(3) = 4$), but $3 + 3 = 6$ has order $2$, which is not a power of $4$. In other words, $H_4$ does not appear to be closed under the group operation ($+$, in this case). Any comments? Thanks very much.","I am stuck with the following problem, from C. Pinter's ""A Book of Abstract Algebra"", p. 153 ex. C 6: Let $G$ be an abelian group, and $H_p$ the subset of $G$ such that the order of every $x \in H_p$ is a power of $p$. Prove that $H_p$ is a subgroup of $G$, and that $G/H_p$ has no elements whose order is a non-zero power of $p$. So, to prove that $H_p$ is a subgroup of $G$, I need to show (among others) that it is closed under the group operation. Let $a$, $b \in H_p$, by hypothesis $\mathrm{ord}(a) = p^m, \mathrm{ord}(b) = p^n$ for some integers $m, n$. Now  consider $(ab)^{p^{mn}}$: since $G$ is abelian, this is equal to $a^{p^{mn}}b^{p^{mn}}$, which is in turn equal to $a^{{(p^m)}^n}b^{{(p^n)}^m}$, which is equal to the identity $e$, since $p^m = \mathrm{ord}(a)$ and $p^n = \mathrm{ord}(b)$. In other words, $(ab)^{p^{mn}} = e$, hence $\mathrm{ord}(ab)$ divides $p^{mn}$. Now here is where I am puzzled: nothing is assumed about $p$. For if $p$ were assumed to be prime (as the choice of the letter ""p"" seems to indicate), it would indeed seem to follow that $\mathrm{ord}(ab)$ must itself be a power of $p$. Conversely, there seem to be counterexamples when $p$ is not prime, e.g. if we take $G = (\mathbb{Z}_{12},+)$ and set $p = 4$, then $H_4$ contains $3$ ($\mathrm{ord}(3) = 4$), but $3 + 3 = 6$ has order $2$, which is not a power of $4$. In other words, $H_4$ does not appear to be closed under the group operation ($+$, in this case). Any comments? Thanks very much.",,"['abstract-algebra', 'group-theory']"
66,"$ \Bbb Q [ \sqrt{2} + \sqrt{3} ] = \Bbb Q [ \sqrt{2} , \sqrt{3} ] $ [duplicate]",[duplicate]," \Bbb Q [ \sqrt{2} + \sqrt{3} ] = \Bbb Q [ \sqrt{2} , \sqrt{3} ] ","This question already has answers here : Is $\mathbb{Q}(\sqrt{2}, \sqrt{3}) = \mathbb{Q}(\sqrt{2}+\sqrt{3})$? (6 answers) Closed 10 years ago . Prove, that $ \Bbb Q [ \sqrt{2} + \sqrt{3} ] = \Bbb Q [  \sqrt{2} , \sqrt{3} ]  $ I don't know the definition of  $\Bbb Q [  \sqrt{2} , \sqrt{3} ]$, can anyone help me with this?","This question already has answers here : Is $\mathbb{Q}(\sqrt{2}, \sqrt{3}) = \mathbb{Q}(\sqrt{2}+\sqrt{3})$? (6 answers) Closed 10 years ago . Prove, that $ \Bbb Q [ \sqrt{2} + \sqrt{3} ] = \Bbb Q [  \sqrt{2} , \sqrt{3} ]  $ I don't know the definition of  $\Bbb Q [  \sqrt{2} , \sqrt{3} ]$, can anyone help me with this?",,['abstract-algebra']
67,Use Sylow Theory to show existence of certain subgroup,Use Sylow Theory to show existence of certain subgroup,,"I am studying for a qualifying exam but have gotten stuck on this problem: Let $G$ be a finite group, $S$ a Sylow $2$-subgroup of $G$, $T\leq S$ with $|S/T| = 2$, and $g\in G$ with $|g|=2$. Suppose that $hgh^{-1} \not \in T$ for all $h\in G$. Show there exists a subgroup $H$ of $G$ with $|G/H| = 2$ and $g\not\in H$. I have considered the action of $G$ on $G/T$ by left multiplication. If we let $|G| = 2^km$, where $2\not \mid m$, then $|T| = 2^{k-1}$ and so $|G/T| = 2m$. We are looking for a subgroup of order $2^{k-1}m$. Then I looked at the kernel of the action, since $g$ is not in the kernel. However, the kernel is not a large enough subgroup. Any suggestions for this problem would be appreciated!","I am studying for a qualifying exam but have gotten stuck on this problem: Let $G$ be a finite group, $S$ a Sylow $2$-subgroup of $G$, $T\leq S$ with $|S/T| = 2$, and $g\in G$ with $|g|=2$. Suppose that $hgh^{-1} \not \in T$ for all $h\in G$. Show there exists a subgroup $H$ of $G$ with $|G/H| = 2$ and $g\not\in H$. I have considered the action of $G$ on $G/T$ by left multiplication. If we let $|G| = 2^km$, where $2\not \mid m$, then $|T| = 2^{k-1}$ and so $|G/T| = 2m$. We are looking for a subgroup of order $2^{k-1}m$. Then I looked at the kernel of the action, since $g$ is not in the kernel. However, the kernel is not a large enough subgroup. Any suggestions for this problem would be appreciated!",,"['abstract-algebra', 'finite-groups', 'sylow-theory']"
68,$M\times N$ Doesn’t Have a Module Structure,Doesn’t Have a Module Structure,M\times N,"In Keith Conrad's notes (page 4) is written: For two $R-$modules $M$ and $N$ , $M\oplus N$ and $M\times N$ are the   same sets, but $M\oplus N$ is an $R-$module and $M\times N$ doesn’t   have a module structure. Out of context this phrase seems strange. I think of $\oplus_{i\in I}M_i$ and $\prod_{i\in I}M_i$ as a sets of collections $\{(m_i)_{i\in I}\}$ with term-wise operations (a difference only in case infinite $I$). Or as a universal objects in category of $R-$modules. In both cases $\prod_{i\in I}M_i$ is a module (as I think). I get the impression that we ""make believe"" that $M\times N$ doesn’t have a module structure. It seems to me that Keith Conrad considers $""\oplus""$ (and further $""\otimes""$) be a module (object of category $R-$mod), but $""\times""$ be only a set of variables for multi-linear functions. Why? Further written: ... addition on $R$ is linear function $R\oplus R\to R$, but addition   on R is not a bilinear function $R\times R\to R$, as we saw above.   Multiplication as a function $R\times R\to R$ is bilinear, but as a   function $R\oplus R\to R$ it is not linear. It strange question but why $""\oplus""$ has a natural addition and $""\times""$ has a natural multiplication? Sorry for such vague questions, I hope you understand what confuses me.","In Keith Conrad's notes (page 4) is written: For two $R-$modules $M$ and $N$ , $M\oplus N$ and $M\times N$ are the   same sets, but $M\oplus N$ is an $R-$module and $M\times N$ doesn’t   have a module structure. Out of context this phrase seems strange. I think of $\oplus_{i\in I}M_i$ and $\prod_{i\in I}M_i$ as a sets of collections $\{(m_i)_{i\in I}\}$ with term-wise operations (a difference only in case infinite $I$). Or as a universal objects in category of $R-$modules. In both cases $\prod_{i\in I}M_i$ is a module (as I think). I get the impression that we ""make believe"" that $M\times N$ doesn’t have a module structure. It seems to me that Keith Conrad considers $""\oplus""$ (and further $""\otimes""$) be a module (object of category $R-$mod), but $""\times""$ be only a set of variables for multi-linear functions. Why? Further written: ... addition on $R$ is linear function $R\oplus R\to R$, but addition   on R is not a bilinear function $R\times R\to R$, as we saw above.   Multiplication as a function $R\times R\to R$ is bilinear, but as a   function $R\oplus R\to R$ it is not linear. It strange question but why $""\oplus""$ has a natural addition and $""\times""$ has a natural multiplication? Sorry for such vague questions, I hope you understand what confuses me.",,"['abstract-algebra', 'category-theory', 'modules']"
69,Kummer extensions,Kummer extensions,,"I want to prove the following: If $F$ contains all $n$-roots of unity and $\operatorname{char}F$ not divides $n$ then $K:=F(\sqrt[n]{a_1},\sqrt[n]{a_2},\ldots,\sqrt[n]{a_m})/F$ is a Galois abelian extension. Hint: Prove that $G:=Gal(K/F)\cong \left<a_1 F^{\times n},\ldots,a_m F^{\times n}\right>\leq  F^\times/ F^{\times n}$, where $ F^{\times n}=\{x^n: x\in F^\times\}$. Attempt: I show that $K/F$ is galois,  $F^{\times n}$ subgroup, but I tried (and I can't) to prove hint for $m=1$ considering the map $\psi\colon \left<a F^{\times n}\right>\to G=F(\sqrt[n]{a})/F$ given by $a^kF^{\times n}\to\sigma_k$ where $\sigma_k$ is defined by $\sigma_k(\sqrt[n]{a})=\zeta_n^{mk}\sqrt[n]{a}$ where $m$ is the order of $aF^{\times n}$. I proved that $\psi$ is an homomorphism but I don't know how to prove that such $\sigma_k$ is an element of $G$, I tried using the basic theorems of radical extensions. I am almost sure that such a map is the desired isomorphism since I proved with a lot of examples.","I want to prove the following: If $F$ contains all $n$-roots of unity and $\operatorname{char}F$ not divides $n$ then $K:=F(\sqrt[n]{a_1},\sqrt[n]{a_2},\ldots,\sqrt[n]{a_m})/F$ is a Galois abelian extension. Hint: Prove that $G:=Gal(K/F)\cong \left<a_1 F^{\times n},\ldots,a_m F^{\times n}\right>\leq  F^\times/ F^{\times n}$, where $ F^{\times n}=\{x^n: x\in F^\times\}$. Attempt: I show that $K/F$ is galois,  $F^{\times n}$ subgroup, but I tried (and I can't) to prove hint for $m=1$ considering the map $\psi\colon \left<a F^{\times n}\right>\to G=F(\sqrt[n]{a})/F$ given by $a^kF^{\times n}\to\sigma_k$ where $\sigma_k$ is defined by $\sigma_k(\sqrt[n]{a})=\zeta_n^{mk}\sqrt[n]{a}$ where $m$ is the order of $aF^{\times n}$. I proved that $\psi$ is an homomorphism but I don't know how to prove that such $\sigma_k$ is an element of $G$, I tried using the basic theorems of radical extensions. I am almost sure that such a map is the desired isomorphism since I proved with a lot of examples.",,"['abstract-algebra', 'galois-theory', 'radicals']"
70,Number of solutions to $x^2+y^2=1$ in a finite field?,Number of solutions to  in a finite field?,x^2+y^2=1,"This question is related to a simple case of a previous question : How many solutions are there for   $$ x^2+y^2=1 $$   in a finite field $F_q$? The answer of the this question would give the order of the orthogonal group $O_2({F_q})$. But I don't see a way how I can count the solutions. [Partial work.] Things might boil down to count $$ t=\left\{\frac{u}{u^2+v^2}\mid u,v\in F_q,\ \ u^2+v^2\not=0\right\} $$ which is what I learned from a post here . Let $k$ be the number of solutions to $u^2+v^2=0$. Then $q^2-k$ gives the number of possible denominators.","This question is related to a simple case of a previous question : How many solutions are there for   $$ x^2+y^2=1 $$   in a finite field $F_q$? The answer of the this question would give the order of the orthogonal group $O_2({F_q})$. But I don't see a way how I can count the solutions. [Partial work.] Things might boil down to count $$ t=\left\{\frac{u}{u^2+v^2}\mid u,v\in F_q,\ \ u^2+v^2\not=0\right\} $$ which is what I learned from a post here . Let $k$ be the number of solutions to $u^2+v^2=0$. Then $q^2-k$ gives the number of possible denominators.",,['abstract-algebra']
71,Reference request for Lorentz group and unitary representations,Reference request for Lorentz group and unitary representations,,"More precisely, I often read or listen that Lorentz group has not (non trivial) unitary finite dimensional irreducible representations because it is not compact. Now, I know that the ""converse"" part of this theorem is part of the statement of Peter-Weyl theorem (i.e. if $G$ is a compact group on a Hilbert space $V$ and $\phi$ is a unitary representation of $G$, then $V$ is the orthogonal sum of finite dimensional irreducible invariant subspaces), but I don't know which is the theorem that states the preceding claim. I really appreciate proofs, references or suggestions for readings! (keep in mind that at the present day I never studied represention theory, but I need some deeper notion because I'm a physicist with the aim to be a mathematical physicist and the current duty to study QFT!) Thank you in advance.","More precisely, I often read or listen that Lorentz group has not (non trivial) unitary finite dimensional irreducible representations because it is not compact. Now, I know that the ""converse"" part of this theorem is part of the statement of Peter-Weyl theorem (i.e. if $G$ is a compact group on a Hilbert space $V$ and $\phi$ is a unitary representation of $G$, then $V$ is the orthogonal sum of finite dimensional irreducible invariant subspaces), but I don't know which is the theorem that states the preceding claim. I really appreciate proofs, references or suggestions for readings! (keep in mind that at the present day I never studied represention theory, but I need some deeper notion because I'm a physicist with the aim to be a mathematical physicist and the current duty to study QFT!) Thank you in advance.",,"['abstract-algebra', 'group-theory', 'reference-request', 'representation-theory', 'mathematical-physics']"
72,Question on irreducible representation of a Banach algebra,Question on irreducible representation of a Banach algebra,,"Let $\mathcal A$ be a Banach algebra over $\mathbb{C}$, $\mathcal X$ a irreducible left $\mathcal A$-module. If $x,y \in \mathcal X$ are linearly independent, there exists an element $a\in\mathcal A$ such that $ax=x$ and $ay=0$. Is it true? If it is true, how to prove? Thanks a lot.","Let $\mathcal A$ be a Banach algebra over $\mathbb{C}$, $\mathcal X$ a irreducible left $\mathcal A$-module. If $x,y \in \mathcal X$ are linearly independent, there exists an element $a\in\mathcal A$ such that $ax=x$ and $ay=0$. Is it true? If it is true, how to prove? Thanks a lot.",,"['abstract-algebra', 'representation-theory', 'operator-algebras']"
73,let $H\subset G$ with $|G:H|=n$ then $\exists~K\leq H$ with $K\unlhd G$ such that $|G:K|\leq n!$ (Dummit Fooote 4.2.8),let  with  then  with  such that  (Dummit Fooote 4.2.8),H\subset G |G:H|=n \exists~K\leq H K\unlhd G |G:K|\leq n!,"Question is to prove that For $H\subset G$ with $|G:H|=n$, $\exists~K\leq H$ with $K\unlhd G$ such that $|G:K|\leq n!$ What i have done so far is that : $H$ be a subgroup of index $n$ in $G$ and let $\{g_i :1\leq i\leq n\}$ be its coset representatives.\ Consider the action of  $G$ on set of left cosets $G\times \{g_i H:1\leq i \leq n\}\rightarrow  \{g_i H:1\leq i \leq n\}$. In  other words we have the action  $G\times \{1,2,3,...n\}\rightarrow \{1,2,3,...n\}$. For each $g\in G$ acting on $\{1,2,3,...n\}$ we have image in $\{1,2,3,...n\}$. So,each $g\in G$ gives a permutation in $\{1,2,3,...n\}$ So, we have $G\rightarrow S_n$ a homomorphism. As $\eta : G\rightarrow S_n$ is a homomorphism, $Ker(\eta)$ would be a normal subgroup of $G$ and by Isomorphism theorem we have $G/Ker(\eta)$ is isomorphis to subgroup of $S_n$. Set $K=Ker(\eta)$, we see that $K\leq H$ and $G/K\cong M$ where $M\leq S_n$. As $|S_n|=n!$ we see that $|G/K|\leq n!$ and so, $|G:K|\leq n!$. Infact $|G:K|$ divides $n!$ which is not asked to prove in the Question. So, I am wondering whether my approach is fine or i have just proved something more. Please look at this as just a proof verification Question. Thank You.","Question is to prove that For $H\subset G$ with $|G:H|=n$, $\exists~K\leq H$ with $K\unlhd G$ such that $|G:K|\leq n!$ What i have done so far is that : $H$ be a subgroup of index $n$ in $G$ and let $\{g_i :1\leq i\leq n\}$ be its coset representatives.\ Consider the action of  $G$ on set of left cosets $G\times \{g_i H:1\leq i \leq n\}\rightarrow  \{g_i H:1\leq i \leq n\}$. In  other words we have the action  $G\times \{1,2,3,...n\}\rightarrow \{1,2,3,...n\}$. For each $g\in G$ acting on $\{1,2,3,...n\}$ we have image in $\{1,2,3,...n\}$. So,each $g\in G$ gives a permutation in $\{1,2,3,...n\}$ So, we have $G\rightarrow S_n$ a homomorphism. As $\eta : G\rightarrow S_n$ is a homomorphism, $Ker(\eta)$ would be a normal subgroup of $G$ and by Isomorphism theorem we have $G/Ker(\eta)$ is isomorphis to subgroup of $S_n$. Set $K=Ker(\eta)$, we see that $K\leq H$ and $G/K\cong M$ where $M\leq S_n$. As $|S_n|=n!$ we see that $|G/K|\leq n!$ and so, $|G:K|\leq n!$. Infact $|G:K|$ divides $n!$ which is not asked to prove in the Question. So, I am wondering whether my approach is fine or i have just proved something more. Please look at this as just a proof verification Question. Thank You.",,['abstract-algebra']
74,What is this Weierstrass' proof of uniqueness of $\mathbb{R}$ and $\mathbb{C}$ algebras?,What is this Weierstrass' proof of uniqueness of  and  algebras?,\mathbb{R} \mathbb{C},"I'm reading Derbyshire's Unknown Quantity . It's an interesting exercise to enumerate and classify all possible algebras. Your results will depend on what you are willing to allow. The narrowest case is that of commutative, associative, finite-dimensional algebras over (that is, having their scalars taken from) the field of real numbers $\mathbb{R}$ and with no divisors of zero. There are just two such algebras: $\mathbb{R}$ and $\mathbb{C}$, a thing proved by Weierstrass in 1864. What is this proof? I've googled Weierstrass algebra proof but found mainly the Stone-Weierstrass Theorem , which I'm not sure if this is the proof.","I'm reading Derbyshire's Unknown Quantity . It's an interesting exercise to enumerate and classify all possible algebras. Your results will depend on what you are willing to allow. The narrowest case is that of commutative, associative, finite-dimensional algebras over (that is, having their scalars taken from) the field of real numbers $\mathbb{R}$ and with no divisors of zero. There are just two such algebras: $\mathbb{R}$ and $\mathbb{C}$, a thing proved by Weierstrass in 1864. What is this proof? I've googled Weierstrass algebra proof but found mainly the Stone-Weierstrass Theorem , which I'm not sure if this is the proof.",,['abstract-algebra']
75,A subset of a field that is a subfield,A subset of a field that is a subfield,,"It can be verified that the following assertion is true: a subset $S$ of a field $F$ is a subfield if $S$ contains the additive and multiplicative identities 0 and 1, if $S$ is closed under addition, multiplication, additive inverses, and $S-\{0\}$ is closed under multiplicative inverses.  An exercise asks to show that the condition $0,1 \in S$ can be replaced by the condition that ''$S$ contains at least two elements''.  The hint given is ''Consider $ax=a$.'' Suppose $S$ contains at least two distinct elements, say $a,b$.  By the hypotheses, $-a \in S$, so $a+(-a) =0 \in S$.  At least one of $a,b$ is nonzero, say $a \ne 0$. Then its inverse $a^{-1} \in S$ and so the product $a a^{-1}=1 \in S$.  Thus, $0,1 \in S$.  This solves the exercise.  My question is whether there's another solution that uses the hint of considering $ax=a$.","It can be verified that the following assertion is true: a subset $S$ of a field $F$ is a subfield if $S$ contains the additive and multiplicative identities 0 and 1, if $S$ is closed under addition, multiplication, additive inverses, and $S-\{0\}$ is closed under multiplicative inverses.  An exercise asks to show that the condition $0,1 \in S$ can be replaced by the condition that ''$S$ contains at least two elements''.  The hint given is ''Consider $ax=a$.'' Suppose $S$ contains at least two distinct elements, say $a,b$.  By the hypotheses, $-a \in S$, so $a+(-a) =0 \in S$.  At least one of $a,b$ is nonzero, say $a \ne 0$. Then its inverse $a^{-1} \in S$ and so the product $a a^{-1}=1 \in S$.  Thus, $0,1 \in S$.  This solves the exercise.  My question is whether there's another solution that uses the hint of considering $ax=a$.",,"['abstract-algebra', 'commutative-algebra', 'field-theory']"
76,Isomorphism of complements in semi-direct products,Isomorphism of complements in semi-direct products,,"Suppose $G$ is a finite group with normal subgroups $M,N$ and subgroups $H,K$ such that $M \cong N$, $MH=NK=G$, and $M \cap H = N \cap K = 1$. Is it the case that $H \cong K$? Clearly $H \cong G/M$ and $K \cong G/N$, so this is similar to Isomorphic quotient groups but of course the examples there are not semi-direct products. I assumed counterexamples would be plentiful, but unless I made a mistake, there are no examples with |G| ≤ 300. This question was motivated by Tobias's remarks in his question How to determine if two semidirect products are isomorphic?","Suppose $G$ is a finite group with normal subgroups $M,N$ and subgroups $H,K$ such that $M \cong N$, $MH=NK=G$, and $M \cap H = N \cap K = 1$. Is it the case that $H \cong K$? Clearly $H \cong G/M$ and $K \cong G/N$, so this is similar to Isomorphic quotient groups but of course the examples there are not semi-direct products. I assumed counterexamples would be plentiful, but unless I made a mistake, there are no examples with |G| ≤ 300. This question was motivated by Tobias's remarks in his question How to determine if two semidirect products are isomorphic?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
77,General proof that a product of nonzero homogeneous polynomials is nonzero (under certain conditions).,General proof that a product of nonzero homogeneous polynomials is nonzero (under certain conditions).,,"Background, Notation, Definitions : Given a set $X$, I define the set $M(X)$ of monomials with $X$-indeterminates to be the set of elements of $\omega^X$ having finite support. Given $m_0,m_1\in M(X)$, I define the operation $*$ on $M(X)$ by $$(m_0*m_1)(x):=m_0(x)+_\omega m_1(x).$$ $\langle M(X),*\rangle$ is then a commutative, cancellative monoid, with the zero element of $\omega^X$ as the identity. Given a ring $R$, it is then natural to define the set $R[X]$ of polynomials with $R$-coefficients and $X$-indeterminates to be the set of elements of $R^{M(X)}$ having finite support. We define the addition and multiplication operations $\oplus$ and $\odot$ on $R[X]$ in terms of the addition and multiplication operations $+$ and $\cdot$ on $R$ as follows: $$(p_0\oplus p_1)(m):=p_0(m)+p_1(m)$$ $$(p_0\odot p_1)(m):=\underset{m_0*m_1=m}{\sum_{m_0,m_1\in M(X)}}p_0(m_0)\cdot p_1(m_1).$$ Then $\langle R[X],\oplus,\odot\rangle$ is a ring. It will be commutative when $R$ is, with unity when $R$ has one. I define the function $\deg:M(X)\to\omega$ by $$\deg(m):=\sum_{x\in X}m(x),$$  and the function $\sigma:\bigl(R[X]\smallsetminus\{0_{R[X]}\}\bigr)\to\omega$ by $$\sigma(p):=\max\{\deg(m):m\in M(X),p(m)\ne0_R\}.$$ It is readily seen that $\deg(m_0*m_1)=\deg(m_0)+_\omega\deg(m_1)$ and that $\sigma(p_0\odot p_1)\le\sigma(p_0)+_\omega\sigma(p_1)$ whenever $p_0,p_1,p_0\odot p_1\ne 0_{R[X]}$. I define the set $H(R,X)$ of homogeneous polynomials with $R$-coefficients and $X$-indeterminates to be the set of all $p\in R[X]\smallsetminus\{0_{R[X]}\}$ such that $$\sigma(p)=\min\{\deg(m):m\in M(X),p(m)\ne0_R\}.$$ It is readily seen that $H(R,X)\cup\{0_{R[X]}\}$ is a sub-semigroup of $\langle R[X],\odot\rangle$ (a sub-monoid if $R$ is unital). The Actual Question : It seems clear to me that $R$ has the zero product property ($a\cdot b=0_R$ implies $a=0_R$ or $b=0_R$) if and only if $\langle H(R,X),\odot\rangle$ is a semigroup. In that case, the restriction of $\sigma$ to $H(R,X)$ should be a semigroup homomorphism--that is, $\sigma(h_0\odot h_1)=\sigma(h_0)+_\omega\sigma(h_1)$. Unfortunately, I have been banging my head against the wall trying to prove these for some time now. In particular, I'm having trouble showing that whenever $R$ has the zero product property, then $h_0\odot h_1\ne0_{R[X]}$ whenever $h_0,h_1\in H(R,X)$. I've tried to proceed by induction on the cardinalities of the supports of $h_0,h_1$, but I can't figure out how to make the induction step click. Any suggestions, hints, or nice proofs of this?","Background, Notation, Definitions : Given a set $X$, I define the set $M(X)$ of monomials with $X$-indeterminates to be the set of elements of $\omega^X$ having finite support. Given $m_0,m_1\in M(X)$, I define the operation $*$ on $M(X)$ by $$(m_0*m_1)(x):=m_0(x)+_\omega m_1(x).$$ $\langle M(X),*\rangle$ is then a commutative, cancellative monoid, with the zero element of $\omega^X$ as the identity. Given a ring $R$, it is then natural to define the set $R[X]$ of polynomials with $R$-coefficients and $X$-indeterminates to be the set of elements of $R^{M(X)}$ having finite support. We define the addition and multiplication operations $\oplus$ and $\odot$ on $R[X]$ in terms of the addition and multiplication operations $+$ and $\cdot$ on $R$ as follows: $$(p_0\oplus p_1)(m):=p_0(m)+p_1(m)$$ $$(p_0\odot p_1)(m):=\underset{m_0*m_1=m}{\sum_{m_0,m_1\in M(X)}}p_0(m_0)\cdot p_1(m_1).$$ Then $\langle R[X],\oplus,\odot\rangle$ is a ring. It will be commutative when $R$ is, with unity when $R$ has one. I define the function $\deg:M(X)\to\omega$ by $$\deg(m):=\sum_{x\in X}m(x),$$  and the function $\sigma:\bigl(R[X]\smallsetminus\{0_{R[X]}\}\bigr)\to\omega$ by $$\sigma(p):=\max\{\deg(m):m\in M(X),p(m)\ne0_R\}.$$ It is readily seen that $\deg(m_0*m_1)=\deg(m_0)+_\omega\deg(m_1)$ and that $\sigma(p_0\odot p_1)\le\sigma(p_0)+_\omega\sigma(p_1)$ whenever $p_0,p_1,p_0\odot p_1\ne 0_{R[X]}$. I define the set $H(R,X)$ of homogeneous polynomials with $R$-coefficients and $X$-indeterminates to be the set of all $p\in R[X]\smallsetminus\{0_{R[X]}\}$ such that $$\sigma(p)=\min\{\deg(m):m\in M(X),p(m)\ne0_R\}.$$ It is readily seen that $H(R,X)\cup\{0_{R[X]}\}$ is a sub-semigroup of $\langle R[X],\odot\rangle$ (a sub-monoid if $R$ is unital). The Actual Question : It seems clear to me that $R$ has the zero product property ($a\cdot b=0_R$ implies $a=0_R$ or $b=0_R$) if and only if $\langle H(R,X),\odot\rangle$ is a semigroup. In that case, the restriction of $\sigma$ to $H(R,X)$ should be a semigroup homomorphism--that is, $\sigma(h_0\odot h_1)=\sigma(h_0)+_\omega\sigma(h_1)$. Unfortunately, I have been banging my head against the wall trying to prove these for some time now. In particular, I'm having trouble showing that whenever $R$ has the zero product property, then $h_0\odot h_1\ne0_{R[X]}$ whenever $h_0,h_1\in H(R,X)$. I've tried to proceed by induction on the cardinalities of the supports of $h_0,h_1$, but I can't figure out how to make the induction step click. Any suggestions, hints, or nice proofs of this?",,"['abstract-algebra', 'group-theory', 'polynomials', 'ring-theory']"
78,Abstract algebra polynomial problem,Abstract algebra polynomial problem,,"Let $F$ be any field and $a,b\in F,\,\,a\neq b$. Find the greatest common divisor of $f(x) = x + a$ and $g(x) = x + b$. Since the degree of both is $1$, the gcd is $1$ or $f(x)$ or $g(x)$, since $a\neq b$. So $\gcd(f(x),g(x))=1$. Am I right for the answer and proving?","Let $F$ be any field and $a,b\in F,\,\,a\neq b$. Find the greatest common divisor of $f(x) = x + a$ and $g(x) = x + b$. Since the degree of both is $1$, the gcd is $1$ or $f(x)$ or $g(x)$, since $a\neq b$. So $\gcd(f(x),g(x))=1$. Am I right for the answer and proving?",,['abstract-algebra']
79,Groups between a group and its profinite completion,Groups between a group and its profinite completion,,"If $G$ and $H$ are finitely generated residually finite groups such that $G\leq H\leq \hat G$, where $\hat G$ denotes the profinite completion of $G$, does it follow that $$\hat H \cong \hat G$$ Thanks!","If $G$ and $H$ are finitely generated residually finite groups such that $G\leq H\leq \hat G$, where $\hat G$ denotes the profinite completion of $G$, does it follow that $$\hat H \cong \hat G$$ Thanks!",,"['abstract-algebra', 'group-theory']"
80,Injective hull and some Hom,Injective hull and some Hom,,"Let $R$ be a commutative ring with unit. Suppose $P\in Spec(R)$ and let $E=E(R/P)$ be the injective hull of $R/P$. What can we say about $Hom_R(R/P, E)$. We know that $R/m\cong Hom_R(R/m, E)$, where $m$ is a maximal ideal of $R$.","Let $R$ be a commutative ring with unit. Suppose $P\in Spec(R)$ and let $E=E(R/P)$ be the injective hull of $R/P$. What can we say about $Hom_R(R/P, E)$. We know that $R/m\cong Hom_R(R/m, E)$, where $m$ is a maximal ideal of $R$.",,"['abstract-algebra', 'commutative-algebra', 'modules', 'homological-algebra']"
81,Showing that $\mathbb{F}_p(x)$ is an infinite field of finite characteristic,Showing that  is an infinite field of finite characteristic,\mathbb{F}_p(x),"Let $F := \mathbb{F}_p(x)$, the field of rational functions in one variable over the prime field $\mathbb{F}_p$. How can we show that $F$ is an infinite field of finite characteristic? Thoughts so far $F$ is clearly infinite since (for example) $1,x,x^2,x^3 \ldots$ etc. are all contained in $F$. Suppose that $f \in F$. Then $f=\frac{p_1(x)}{p_2(x)}$ with $p_1,p_2$ having coefficients in $\mathbb{F}_p$. I'm not sure how we can show that $\text{char}F=p$ though.","Let $F := \mathbb{F}_p(x)$, the field of rational functions in one variable over the prime field $\mathbb{F}_p$. How can we show that $F$ is an infinite field of finite characteristic? Thoughts so far $F$ is clearly infinite since (for example) $1,x,x^2,x^3 \ldots$ etc. are all contained in $F$. Suppose that $f \in F$. Then $f=\frac{p_1(x)}{p_2(x)}$ with $p_1,p_2$ having coefficients in $\mathbb{F}_p$. I'm not sure how we can show that $\text{char}F=p$ though.",,"['abstract-algebra', 'field-theory']"
82,Trace and Norm of a separable extension.,Trace and Norm of a separable extension.,,"If $L | K$ is a separable extension and $\sigma : L \rightarrow \bar K$ varies over the different $K$-embeddings of $L$ into an algebraic closure $\bar K$ of $K$, then how to prove that          $$f_x(t) = \Pi (t - \sigma(x))?$$ $f_x(t)$ is the characteristic polynomial of the linear transformation $T_x:L \rightarrow L$ where $T_x(a)=xa$","If $L | K$ is a separable extension and $\sigma : L \rightarrow \bar K$ varies over the different $K$-embeddings of $L$ into an algebraic closure $\bar K$ of $K$, then how to prove that          $$f_x(t) = \Pi (t - \sigma(x))?$$ $f_x(t)$ is the characteristic polynomial of the linear transformation $T_x:L \rightarrow L$ where $T_x(a)=xa$",,"['abstract-algebra', 'field-theory', 'algebraic-number-theory', 'galois-theory']"
83,Aut($G$) $\simeq$ Aut($M$) $ \times$ Aut($N$),Aut()  Aut()  Aut(),G \simeq M  \times N,"A question in group theory: Let $ G = M \times N $ be the direct product of $ 2 $ normal subgroups. If $( | M | , | N | ) = 1 $ then Aut($G$) $\simeq$ Aut($M$) $ \times$ Aut($N$). I proved that Aut($M$) $ \times$ Aut($N$) $\le$ Aut($G$), but I can't prove the other inclusion. Any hint ?","A question in group theory: Let $ G = M \times N $ be the direct product of $ 2 $ normal subgroups. If $( | M | , | N | ) = 1 $ then Aut($G$) $\simeq$ Aut($M$) $ \times$ Aut($N$). I proved that Aut($M$) $ \times$ Aut($N$) $\le$ Aut($G$), but I can't prove the other inclusion. Any hint ?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
84,A question about invariant factors of finitely generated modules over a PID.,A question about invariant factors of finitely generated modules over a PID.,,"A Theorem in our textbook says: If R is a PID, then every finitely generated torision R-module M is a direct sum of cyclic modules $$M= R/(c_1) \bigoplus R/(c_2) \bigoplus \cdots \bigoplus R/(c_t)$$ where $t \geq 1$ and $c_1 | c_2 | \cdots | c_t$. If we want to classify groups of order 32 and put them in invariant factor form then the abelian groups of order 32 are: $Z_{32}$ $Z_{16} \bigoplus Z_2$ $Z_8 \bigoplus Z_4$ $Z_8 \bigoplus Z_2 \bigoplus Z_2$ $Z_4 \bigoplus Z_2 \bigoplus Z_2 \bigoplus Z_2$ $Z_2 \bigoplus Z_2 \bigoplus Z_2 \bigoplus Z_2 \bigoplus Z_2$ My question is: The theorem only tells us that ""M is a direct sum of cyclic modules"". But how do we know what those cyclic modules are?","A Theorem in our textbook says: If R is a PID, then every finitely generated torision R-module M is a direct sum of cyclic modules $$M= R/(c_1) \bigoplus R/(c_2) \bigoplus \cdots \bigoplus R/(c_t)$$ where $t \geq 1$ and $c_1 | c_2 | \cdots | c_t$. If we want to classify groups of order 32 and put them in invariant factor form then the abelian groups of order 32 are: $Z_{32}$ $Z_{16} \bigoplus Z_2$ $Z_8 \bigoplus Z_4$ $Z_8 \bigoplus Z_2 \bigoplus Z_2$ $Z_4 \bigoplus Z_2 \bigoplus Z_2 \bigoplus Z_2$ $Z_2 \bigoplus Z_2 \bigoplus Z_2 \bigoplus Z_2 \bigoplus Z_2$ My question is: The theorem only tells us that ""M is a direct sum of cyclic modules"". But how do we know what those cyclic modules are?",,['abstract-algebra']
85,Ring Automorphisms and Roots of Monic Polynomials,Ring Automorphisms and Roots of Monic Polynomials,,"I have the following problem: Let $R$ be a commutative ring with identity and $\phi: R \rightarrow R$ a ring automorphism.  If $F=\lbrace r\in R | \phi(r)=r \rbrace$, show that $\phi^2$ being the identity map implies each element of $R$ is the root of a monic polynomial of degree two in $F[x]$. I've tried constructing the polynomial explicitly, but I haven't had any luck.  I've considered trying to use an argument using indices, but I'm not sure how to go about it.","I have the following problem: Let $R$ be a commutative ring with identity and $\phi: R \rightarrow R$ a ring automorphism.  If $F=\lbrace r\in R | \phi(r)=r \rbrace$, show that $\phi^2$ being the identity map implies each element of $R$ is the root of a monic polynomial of degree two in $F[x]$. I've tried constructing the polynomial explicitly, but I haven't had any luck.  I've considered trying to use an argument using indices, but I'm not sure how to go about it.",,['abstract-algebra']
86,One to one correspondence of ideals in $R$ and $S^{-1}R$?,One to one correspondence of ideals in  and ?,R S^{-1}R,"I proved the following statement, but I am very unsure that it is correct, since this proposition is not stated in my books for general ideals but only for prime ideals. Please point out where the mistake is if it is incorrect. ($I^e$ denotes the extended ideal of I which also equals $S^{-1}I$, $J^c$ denotes the contraction of $J$) Proposition: Let R be a ring and let S be an multiplicative set. The ideals of $S^{-1}R$ are in one to one correspondence with the ideals I of R which do not meet S. The correspondence is $I \Leftrightarrow  S^{-1}I$. Under this corresponcence prime ideals correspond to prime ideals. Proof: Let $I$ be an ideal of $R$ which do not meet $S$. For any $\frac{a}{b}, \frac{c}{d} \in S^{-1}I$ and any $\frac{r}{s} \in  S^{-1}R $,  $\frac{a}{b}\frac{r}{s}=\frac{ar}{bs}$ is in $ S^{-1}I$ since $ar\in I$ and $cs\in S$. Futhermore,  $\frac{a}{b}+ \frac{c}{d}= \frac{da+bc}{db}$ is an element in $ S^{-1}I$ by the same argument. Converserly, let $J$ be any ideal of $S^{-1}R$, if $\frac{x}{s}\in J$ then $\frac{x}{1}\in J$, hence $x \in J^c$, (with respect to the homomorphism $\Phi: R \rightarrow S^{-1}R$, given by $\Phi(x)=\frac{x}{1}$), hence $J\subseteq J^{ce}$, further it is easy to check that $J^{ce}\subseteq J$ for any homomorphism. Hence $J=J^{ce}$. Since $J^{c}=I$ for some ideal $I$ of $R$ we have that $J=S^{-1}I$. Furthermore if $I$ is prime in$R$ it is easy to see that $S^{-1}I$ is prime in $S^{-1}R$. Converserly, if $J$ is any prime ideal $J \subset S^{-1}R$ then $J=S^{-1}I$ some ideal $I$ of $R$, if $ac\in I$ then by construction $\frac{ac}{bd} \in S^{-1}I$. Since $J$ is prime this imply that $\frac{a}{b}$ or $\frac{c}{d}$ is in $J$. This imply that $a$ or $c$ is in $I$","I proved the following statement, but I am very unsure that it is correct, since this proposition is not stated in my books for general ideals but only for prime ideals. Please point out where the mistake is if it is incorrect. ($I^e$ denotes the extended ideal of I which also equals $S^{-1}I$, $J^c$ denotes the contraction of $J$) Proposition: Let R be a ring and let S be an multiplicative set. The ideals of $S^{-1}R$ are in one to one correspondence with the ideals I of R which do not meet S. The correspondence is $I \Leftrightarrow  S^{-1}I$. Under this corresponcence prime ideals correspond to prime ideals. Proof: Let $I$ be an ideal of $R$ which do not meet $S$. For any $\frac{a}{b}, \frac{c}{d} \in S^{-1}I$ and any $\frac{r}{s} \in  S^{-1}R $,  $\frac{a}{b}\frac{r}{s}=\frac{ar}{bs}$ is in $ S^{-1}I$ since $ar\in I$ and $cs\in S$. Futhermore,  $\frac{a}{b}+ \frac{c}{d}= \frac{da+bc}{db}$ is an element in $ S^{-1}I$ by the same argument. Converserly, let $J$ be any ideal of $S^{-1}R$, if $\frac{x}{s}\in J$ then $\frac{x}{1}\in J$, hence $x \in J^c$, (with respect to the homomorphism $\Phi: R \rightarrow S^{-1}R$, given by $\Phi(x)=\frac{x}{1}$), hence $J\subseteq J^{ce}$, further it is easy to check that $J^{ce}\subseteq J$ for any homomorphism. Hence $J=J^{ce}$. Since $J^{c}=I$ for some ideal $I$ of $R$ we have that $J=S^{-1}I$. Furthermore if $I$ is prime in$R$ it is easy to see that $S^{-1}I$ is prime in $S^{-1}R$. Converserly, if $J$ is any prime ideal $J \subset S^{-1}R$ then $J=S^{-1}I$ some ideal $I$ of $R$, if $ac\in I$ then by construction $\frac{ac}{bd} \in S^{-1}I$. Since $J$ is prime this imply that $\frac{a}{b}$ or $\frac{c}{d}$ is in $J$. This imply that $a$ or $c$ is in $I$",,"['abstract-algebra', 'commutative-algebra', 'ring-theory']"
87,Topologically nilpotent elements of a linearly topologized ring,Topologically nilpotent elements of a linearly topologized ring,,"In what follows all rings are commutative topological rings. An element $x$ of a ring is called topologically nilpotent if $lim_{n\rightarrow \infty} x^n = 0$. If a ring $A$ has a fundamental system of neighborhoods of $0$ consiting of ideals, $A$ is called linearly topologized. An open ideal $I$ of a linearly topologized ring is called an ideal of definition if, for every neighborhood $V$ of $0$, there exists an integer $n > 0$ such that $I^n \subset V$. The following proposition is Lemma 7.1.3 of Grothendieck's EGA I, Ch. 0. Proposition Suppose a linearly topologized ring $A$ has an ideal of definition $I$. Suppose $x$ mod $I$ is nilpotent in $A/I$. Then $x$ is topologically nilpotent. The proof of EGA is as follows. Let $V$ be a neighborhood of $0$. There exists an integer $n > 0$ such that $I^n \subset V$. If $x^m \in I$, $x^{mq} \in V$ for all integer $q \ge n$. Hence $x$ is topologically nilpotent. My question Why $x$ is topologically nilpotent?","In what follows all rings are commutative topological rings. An element $x$ of a ring is called topologically nilpotent if $lim_{n\rightarrow \infty} x^n = 0$. If a ring $A$ has a fundamental system of neighborhoods of $0$ consiting of ideals, $A$ is called linearly topologized. An open ideal $I$ of a linearly topologized ring is called an ideal of definition if, for every neighborhood $V$ of $0$, there exists an integer $n > 0$ such that $I^n \subset V$. The following proposition is Lemma 7.1.3 of Grothendieck's EGA I, Ch. 0. Proposition Suppose a linearly topologized ring $A$ has an ideal of definition $I$. Suppose $x$ mod $I$ is nilpotent in $A/I$. Then $x$ is topologically nilpotent. The proof of EGA is as follows. Let $V$ be a neighborhood of $0$. There exists an integer $n > 0$ such that $I^n \subset V$. If $x^m \in I$, $x^{mq} \in V$ for all integer $q \ge n$. Hence $x$ is topologically nilpotent. My question Why $x$ is topologically nilpotent?",,"['abstract-algebra', 'commutative-algebra']"
88,root of $x^3+x+1$ over $\mathbb{F}_5$,root of  over,x^3+x+1 \mathbb{F}_5,"What is the simplest radical expression of some root $a \in \overline{\mathbb{F}_5}$ of the polynomial $x^3+x+1  \in \mathbb{F}_5[x]$? I wonder if one can simplify the general formulas in this special case. I think $\sqrt[3]{\sqrt{2} - 3} - \sqrt[3]{\sqrt{2} + 3}$ is ok for the beginning, but this minus in the middle and the nested roots make it rather complicated.","What is the simplest radical expression of some root $a \in \overline{\mathbb{F}_5}$ of the polynomial $x^3+x+1  \in \mathbb{F}_5[x]$? I wonder if one can simplify the general formulas in this special case. I think $\sqrt[3]{\sqrt{2} - 3} - \sqrt[3]{\sqrt{2} + 3}$ is ok for the beginning, but this minus in the middle and the nested roots make it rather complicated.",,"['abstract-algebra', 'polynomials', 'finite-fields']"
89,Listing down the Galois group,Listing down the Galois group,,"This is a common exercise: Sketch the lattice of subfields of $F = \mathbb{Q} ( \mathbb{e^{\frac{2 \pi i}{p}}})$ be a cyclotomic extension over $\mathbb{Q}$ (where $p$ is an odd prime). It got me wondering, what's the easiest way of writing/listing down the elements of the Galois group $Aut_{\mathbb{Q}} F$?","This is a common exercise: Sketch the lattice of subfields of $F = \mathbb{Q} ( \mathbb{e^{\frac{2 \pi i}{p}}})$ be a cyclotomic extension over $\mathbb{Q}$ (where $p$ is an odd prime). It got me wondering, what's the easiest way of writing/listing down the elements of the Galois group $Aut_{\mathbb{Q}} F$?",,['abstract-algebra']
90,Algebras over a field are flat - category theoretic proof?,Algebras over a field are flat - category theoretic proof?,,"Let $k$ be a field. Assume that you already know that the category $\mathrm{Alg}(k)$ of $k$-algebras (everything here is commutative and unital) has a coproduct $\sqcup$. But you don't know that this actually comes from the tensor product of vector spaces over $k$. You just know the universal property of $A \sqcup B$ (aka $A \otimes_k B$). From this you can deduce: $\sqcup$ is commutative and associative up to natural isomorphisms $(A/I) \sqcup B \cong (A \sqcup B) / \langle i_A(I) \rangle$ $A \sqcup k[x_1,\dotsc,x_n] \cong A[x_1,\dotsc,x_n]$ $A \sqcup -$ commutes with colimits In particular, we can compute the tensor product of arbitrary algebras using presentations: $$k[\{x_i\}]/I \sqcup k[\{y_j\}]/J \cong k[\{x_i\},\{y_j\}]/\langle I,J \rangle$$ Question. How can we prove that for every injective homomorphism $\phi : A \to B$ of $k$-algebras the induced homomorphism $\phi \sqcup \mathrm{id} : A \sqcup C \to B \sqcup C$ is also injective for every $k$-algebra $C$? For example, this is clear when $C$ is a polynomial algebra over $k$. In general, $C$ is free as a module over $k$, but we cannot use the isomorphism $C \cong k^{(I)}$ since this leaves the category of $k$-algebras. Background: I assist a lecture where the students have just learned what the tensor product of algebras is, without knowing the tensor product of modules. Now they have to believe somehow some of the well-known properties, because they are usually proven with the help of the tensor product of modules. But perhaps we can do it with algebras alone. Since the lecture is about elementary algebraic geometry, you may assume that $k$ is algebraically closed and some basic results about affine varieties (but not about their fiber products ;)). Appendix: Further properties which follow from the universal property: 1) Let us denote the coproduct inclusions by $i_A : A \to A \sqcup B$ and $i_B : B \to A \sqcup B$. It is easy to see that $\otimes : A \times B \to A \sqcup B, (a,b) \mapsto i_A(a) \cdot i_B(b)$ is $k$-bilinear and that the span of the image generates $A \sqcup B$ (since the image satisfies the same universal property). 2) When $A,B \neq 0$, then we also have $A \otimes_k B \neq 0$. Geometrically: The fiber product of two non-empty schemes is non-empty. Proof: Since the zero algebra only maps to the zero algebra, we may replace $A,B$ by quotients. In particular, we may assume that $A,B$ are field extensions of $k$. Let $P$ be a transzendence basis of $A/k$ and $Q$ one of $B/k$. Let $M$ be a set containing disjoint copies of $P$ and $Q$ and let $C$ be the algebraic closure of $k(M)$. Then there are maps $A \to C$ and $B \to C$, which induce by the universal property a map $A \sqcup B \to C$. Since $C \neq 0$, we also have $A \sqcup B \neq 0$.","Let $k$ be a field. Assume that you already know that the category $\mathrm{Alg}(k)$ of $k$-algebras (everything here is commutative and unital) has a coproduct $\sqcup$. But you don't know that this actually comes from the tensor product of vector spaces over $k$. You just know the universal property of $A \sqcup B$ (aka $A \otimes_k B$). From this you can deduce: $\sqcup$ is commutative and associative up to natural isomorphisms $(A/I) \sqcup B \cong (A \sqcup B) / \langle i_A(I) \rangle$ $A \sqcup k[x_1,\dotsc,x_n] \cong A[x_1,\dotsc,x_n]$ $A \sqcup -$ commutes with colimits In particular, we can compute the tensor product of arbitrary algebras using presentations: $$k[\{x_i\}]/I \sqcup k[\{y_j\}]/J \cong k[\{x_i\},\{y_j\}]/\langle I,J \rangle$$ Question. How can we prove that for every injective homomorphism $\phi : A \to B$ of $k$-algebras the induced homomorphism $\phi \sqcup \mathrm{id} : A \sqcup C \to B \sqcup C$ is also injective for every $k$-algebra $C$? For example, this is clear when $C$ is a polynomial algebra over $k$. In general, $C$ is free as a module over $k$, but we cannot use the isomorphism $C \cong k^{(I)}$ since this leaves the category of $k$-algebras. Background: I assist a lecture where the students have just learned what the tensor product of algebras is, without knowing the tensor product of modules. Now they have to believe somehow some of the well-known properties, because they are usually proven with the help of the tensor product of modules. But perhaps we can do it with algebras alone. Since the lecture is about elementary algebraic geometry, you may assume that $k$ is algebraically closed and some basic results about affine varieties (but not about their fiber products ;)). Appendix: Further properties which follow from the universal property: 1) Let us denote the coproduct inclusions by $i_A : A \to A \sqcup B$ and $i_B : B \to A \sqcup B$. It is easy to see that $\otimes : A \times B \to A \sqcup B, (a,b) \mapsto i_A(a) \cdot i_B(b)$ is $k$-bilinear and that the span of the image generates $A \sqcup B$ (since the image satisfies the same universal property). 2) When $A,B \neq 0$, then we also have $A \otimes_k B \neq 0$. Geometrically: The fiber product of two non-empty schemes is non-empty. Proof: Since the zero algebra only maps to the zero algebra, we may replace $A,B$ by quotients. In particular, we may assume that $A,B$ are field extensions of $k$. Let $P$ be a transzendence basis of $A/k$ and $Q$ one of $B/k$. Let $M$ be a set containing disjoint copies of $P$ and $Q$ and let $C$ be the algebraic closure of $k(M)$. Then there are maps $A \to C$ and $B \to C$, which induce by the universal property a map $A \sqcup B \to C$. Since $C \neq 0$, we also have $A \sqcup B \neq 0$.",,"['abstract-algebra', 'category-theory', 'tensor-products']"
91,Showing a degree formula $\dim_{\mathbb{C}} R^{2} / L$,Showing a degree formula,\dim_{\mathbb{C}} R^{2} / L,"If $a,b,c,d$ are in $R=\mathbb{C}[t]$ and  $ad-bc \ne 0$, $L= R(a,b)+R(c,d)$ in $R^{2}$. I want to show that $\dim_{\mathbb{C}}R^{2}/L = \deg(ad-bc)$. In a previous theorem it was shown that : $\dim_{\mathbb{C}}R /tR = \deg(t)$. So this can be used, if it helps. Does anybody see how to do this? Please, do tell me the right path.","If $a,b,c,d$ are in $R=\mathbb{C}[t]$ and  $ad-bc \ne 0$, $L= R(a,b)+R(c,d)$ in $R^{2}$. I want to show that $\dim_{\mathbb{C}}R^{2}/L = \deg(ad-bc)$. In a previous theorem it was shown that : $\dim_{\mathbb{C}}R /tR = \deg(t)$. So this can be used, if it helps. Does anybody see how to do this? Please, do tell me the right path.",,"['abstract-algebra', 'commutative-algebra', 'ring-theory']"
92,How do you pronounce $\mathbb{Q}[\sqrt{2}]$?,How do you pronounce ?,\mathbb{Q}[\sqrt{2}],"Is there a standard term for $\mathbb{Q}[\sqrt{2}]$? I say it as ""Q adjoin root two"".","Is there a standard term for $\mathbb{Q}[\sqrt{2}]$? I say it as ""Q adjoin root two"".",,"['abstract-algebra', 'notation']"
93,Supplementary exercises for Herstein's Noncommutative Rings,Supplementary exercises for Herstein's Noncommutative Rings,,"I've been studying from the book Noncommutative Rings by Herstein (not as a part of some official course), but unfortunately it doesn't contain any exercises apart from a few simple ones in the body. I was wondering if anyone had any suggestions on where I could find suitable exercises (since from what I've seen, this material is covered quite differently in different books, so it may not necessarily be possible to just take any other book on noncommutative algebra and use its exercises). Any help will be appreciated.","I've been studying from the book Noncommutative Rings by Herstein (not as a part of some official course), but unfortunately it doesn't contain any exercises apart from a few simple ones in the body. I was wondering if anyone had any suggestions on where I could find suitable exercises (since from what I've seen, this material is covered quite differently in different books, so it may not necessarily be possible to just take any other book on noncommutative algebra and use its exercises). Any help will be appreciated.",,"['abstract-algebra', 'reference-request', 'self-learning', 'noncommutative-algebra']"
94,Algebra structure of tensor product of two Galois extensions,Algebra structure of tensor product of two Galois extensions,,"Sorry if this question is too basic. It is from Fröhlich and Taylor's ""Algebraic Number Theory"". Let $E/F$ be a finite Galois extension of fields, with $G=Gal(E/F)$, and let $K$ and $L$ be two subfields of $E$, containing $F$, such that $K/F$ and $L/F$ are both Galois. Let $M=Gal(E/K)$ and $N=Gal(E/L)$ be normal subgroups of $G$. Suppose ${\gamma_1,\ldots,\gamma_n}$ is a transversal for $MN$ in $G$, with $n=[G:MN]$. If $C$ is the compositum $KL$ in $E$, how can I show the map $$ k\otimes l\mapsto (k^{\gamma_1}l,\ldots,k^{\gamma_n}l) $$ induces an isomorphism between $K\otimes_F L$ and $\prod_{i=1}^n C$? It is clear to me that this map is an $F$-algebra homomorphism, and that they both have the same dimension over $F$.  Thus surjectivity, or injectivity, would be enough.  I have not been able to figure out what the idempotents of $\prod_{i=1}^n C$ should look like in $K\otimes_F L$, so I have not been able to show surjectivity.  Meanwhile, I think injectivity should be easier to show, because if we have $$ k_1\otimes l_1 + \cdots + k_m\otimes l_m\mapsto 0,$$ then we get a system of equations $$ k_1l_1 + \cdots + k_ml_m=0$$ $$ \cdots$$ $$ k_1^{\gamma_n}l_1 + \cdots k_m^{\gamma_n}l_m=0.$$ Summing up the columns, I get $$ \sum_{i=1}^m(\sum_{j=1}^n k_i^{\gamma_j})l_i=0 $$ and all this is happening in $L$.  But I can't seem to finish this argument.  Any help would be greatly appreciated.","Sorry if this question is too basic. It is from Fröhlich and Taylor's ""Algebraic Number Theory"". Let $E/F$ be a finite Galois extension of fields, with $G=Gal(E/F)$, and let $K$ and $L$ be two subfields of $E$, containing $F$, such that $K/F$ and $L/F$ are both Galois. Let $M=Gal(E/K)$ and $N=Gal(E/L)$ be normal subgroups of $G$. Suppose ${\gamma_1,\ldots,\gamma_n}$ is a transversal for $MN$ in $G$, with $n=[G:MN]$. If $C$ is the compositum $KL$ in $E$, how can I show the map $$ k\otimes l\mapsto (k^{\gamma_1}l,\ldots,k^{\gamma_n}l) $$ induces an isomorphism between $K\otimes_F L$ and $\prod_{i=1}^n C$? It is clear to me that this map is an $F$-algebra homomorphism, and that they both have the same dimension over $F$.  Thus surjectivity, or injectivity, would be enough.  I have not been able to figure out what the idempotents of $\prod_{i=1}^n C$ should look like in $K\otimes_F L$, so I have not been able to show surjectivity.  Meanwhile, I think injectivity should be easier to show, because if we have $$ k_1\otimes l_1 + \cdots + k_m\otimes l_m\mapsto 0,$$ then we get a system of equations $$ k_1l_1 + \cdots + k_ml_m=0$$ $$ \cdots$$ $$ k_1^{\gamma_n}l_1 + \cdots k_m^{\gamma_n}l_m=0.$$ Summing up the columns, I get $$ \sum_{i=1}^m(\sum_{j=1}^n k_i^{\gamma_j})l_i=0 $$ and all this is happening in $L$.  But I can't seem to finish this argument.  Any help would be greatly appreciated.",,"['number-theory', 'abstract-algebra', 'field-theory']"
95,Inner product of signatures of piecewise linear paths,Inner product of signatures of piecewise linear paths,,"It is a well-know observation that, given two points $x_1,x_2 \in \mathbb{R}^d$ , the path signature associated to their linear interpolation is given by the tensor exponential. Precisely, if $\Delta x$ denotes the linear interpolation $x_1 \rightarrow x_2$ , then $$S(\Delta x) = \left(1 , \Delta x, \frac{\Delta x^{\otimes 2}}{2!}, \frac{\Delta x^{\otimes 3}}{3!}, ... \right) =: \exp_\otimes(\Delta x) \in T((\mathbb{R}^d)),$$ where $S(\cdot)$ denotes the signature map and $T((\mathbb{R}^d))$ is the extended tensor algebra. Actually, the signature takes value in a subset $T^1$ of $T((\mathbb{R}^d))$ which can be endowed with an inner product, and thus becomes a Hilbert space if one takes the appropriate completions. I refer to this work for a proper definition, but I assume most readers interested in this post are familiar with this. Now, instead of considering just a line segment, let us consider a piecewise linear path. Specifically, given a finite number of points $x_1,...,x_n$ in $\mathbb{R}^d$ , we consider the path corresponding to successive linear interpolations, i.e. $x_1 \rightarrow x_2$ concatenated with $x_2 \rightarrow x_3$ , which, in its turn, we concatenate with $x_3 \rightarrow x_4$ , and so on. Let us denote this piecewise linear path by $\textbf{x}$ . Thanks to Chen's identity, we know that the signature of $\textbf{x}$ is given by multiplying in the extended tensor algebra the signatures associated to each interpolation, i.e. $$S(\textbf{x}) = \exp_\otimes(\Delta_1 x) \ \otimes \ \exp_\otimes(\Delta_2 x) \ \otimes \ ... \ \otimes \exp_\otimes(\Delta_{n-1} x),$$ where $\Delta_i x$ denotes the interpolated path $x_i \rightarrow x_{i+1}$ , and $\otimes$ is the multiplication defined in $T((\mathbb{R}^d))$ . Lastly, let $\textbf{y}$ denote some other piecewise linear interpolation of $y_1,...,y_n\in \mathbb{R}^d$ , and let $\langle \cdot, \cdot \rangle_{T^1}$ denote the aforementioned inner product. My question is: Can we express $\langle S(\textbf{x}), S(\textbf{y}) \rangle_{T^1}$ via the simpler inner products $\langle \exp_\otimes(\Delta_i x), \exp_\otimes(\Delta_j y) \rangle_{T^1}$ with $i,j \in \{1,...,n-1\}$ ?","It is a well-know observation that, given two points , the path signature associated to their linear interpolation is given by the tensor exponential. Precisely, if denotes the linear interpolation , then where denotes the signature map and is the extended tensor algebra. Actually, the signature takes value in a subset of which can be endowed with an inner product, and thus becomes a Hilbert space if one takes the appropriate completions. I refer to this work for a proper definition, but I assume most readers interested in this post are familiar with this. Now, instead of considering just a line segment, let us consider a piecewise linear path. Specifically, given a finite number of points in , we consider the path corresponding to successive linear interpolations, i.e. concatenated with , which, in its turn, we concatenate with , and so on. Let us denote this piecewise linear path by . Thanks to Chen's identity, we know that the signature of is given by multiplying in the extended tensor algebra the signatures associated to each interpolation, i.e. where denotes the interpolated path , and is the multiplication defined in . Lastly, let denote some other piecewise linear interpolation of , and let denote the aforementioned inner product. My question is: Can we express via the simpler inner products with ?","x_1,x_2 \in \mathbb{R}^d \Delta x x_1 \rightarrow x_2 S(\Delta x) = \left(1 , \Delta x, \frac{\Delta x^{\otimes 2}}{2!}, \frac{\Delta x^{\otimes 3}}{3!}, ... \right) =: \exp_\otimes(\Delta x) \in T((\mathbb{R}^d)), S(\cdot) T((\mathbb{R}^d)) T^1 T((\mathbb{R}^d)) x_1,...,x_n \mathbb{R}^d x_1 \rightarrow x_2 x_2 \rightarrow x_3 x_3 \rightarrow x_4 \textbf{x} \textbf{x} S(\textbf{x}) = \exp_\otimes(\Delta_1 x) \ \otimes \ \exp_\otimes(\Delta_2 x) \ \otimes \ ... \ \otimes \exp_\otimes(\Delta_{n-1} x), \Delta_i x x_i \rightarrow x_{i+1} \otimes T((\mathbb{R}^d)) \textbf{y} y_1,...,y_n\in \mathbb{R}^d \langle \cdot, \cdot \rangle_{T^1} \langle S(\textbf{x}), S(\textbf{y}) \rangle_{T^1} \langle \exp_\otimes(\Delta_i x), \exp_\otimes(\Delta_j y) \rangle_{T^1} i,j \in \{1,...,n-1\}","['abstract-algebra', 'tensor-products', 'rough-path-theory']"
96,Is it possible to recover the Cartan-Leray Spectral Sequence for Group Cohomology from the Leray Spectral Sequence for Sheaf Cohomology?,Is it possible to recover the Cartan-Leray Spectral Sequence for Group Cohomology from the Leray Spectral Sequence for Sheaf Cohomology?,,"Let $G$ be a discrete group acting freely and cellularily on a CW-complex $X$ . I am interested in the Cartan-Leray spectral sequence from Eilenberg and Cartan's Homological Algebra, Theorem XVI.8.4, which goes $$E^2_{p,q}=H^p(G,H^q(X;M))\Rightarrow H^{p+q}(X/G;M)\,,$$ where $M$ is a left $G$ -module intepreted as a local coefficient system on $X$ rsp. $X/G$ . The proof presented there is quite explicit, and I was wondering if there is a more abstract way to prove this using the Leray spectral sequence : For a continous map $f:A\to B$ and a sheaf $\mathcal F$ on $A$ , there is a spectral sequence $$E^2_{p,q}=H^p(B;R^q(f_\ast)(\mathcal F))\Rightarrow H^{p+q}(A;\mathcal F)\,.$$ My idea was to construct a map $f:X/G\to K(G,1)$ , where $K(G,1)$ is an Eilenberg-MacLane complex for $G$ , and set $\mathcal F:=\underline{M}_{X/G}$ . Then $H^{p+q}(A;\mathcal F)=H^{p+q}(X/G;M)$ , which is good. But $$H^p(B;R^q(f_\ast)(\mathcal F))=H^p(K(G,1);\operatorname{Sheafification of}H^q(f^{-1}(-);M))\,,$$ of which I am not sure why it should equal $H^p(K(G,1);\underline{H^q(X;M)}_{K(G,1)})=H^p(G,H^q(X;M))$ . Is there a way to make this work? Did I go wrong somewhere along?","Let be a discrete group acting freely and cellularily on a CW-complex . I am interested in the Cartan-Leray spectral sequence from Eilenberg and Cartan's Homological Algebra, Theorem XVI.8.4, which goes where is a left -module intepreted as a local coefficient system on rsp. . The proof presented there is quite explicit, and I was wondering if there is a more abstract way to prove this using the Leray spectral sequence : For a continous map and a sheaf on , there is a spectral sequence My idea was to construct a map , where is an Eilenberg-MacLane complex for , and set . Then , which is good. But of which I am not sure why it should equal . Is there a way to make this work? Did I go wrong somewhere along?","G X E^2_{p,q}=H^p(G,H^q(X;M))\Rightarrow H^{p+q}(X/G;M)\,, M G X X/G f:A\to B \mathcal F A E^2_{p,q}=H^p(B;R^q(f_\ast)(\mathcal F))\Rightarrow H^{p+q}(A;\mathcal F)\,. f:X/G\to K(G,1) K(G,1) G \mathcal F:=\underline{M}_{X/G} H^{p+q}(A;\mathcal F)=H^{p+q}(X/G;M) H^p(B;R^q(f_\ast)(\mathcal F))=H^p(K(G,1);\operatorname{Sheafification of}H^q(f^{-1}(-);M))\,, H^p(K(G,1);\underline{H^q(X;M)}_{K(G,1)})=H^p(G,H^q(X;M))","['abstract-algebra', 'group-cohomology', 'sheaf-cohomology', 'spectral-sequences', 'eilenberg-maclane-spaces']"
97,Localization of a differential ring,Localization of a differential ring,,"I am reading Asymptotic Differential Algebra and Model Theory of Transseries and on p. 200 there is a claim/explanation of Localization: $R$ is a differential ring and $A$ its multiplicative subset, $0 \notin A$ . Then there is a unique derivation on $A^{-1} R$ making $A^{-1} R$ into a differential ring where the natural map $r \mapsto r / 1: R \rightarrow A^{-1} R$ is a morphism of differential rings; it is given by $$ (r / a)^{\prime}=\left(r^{\prime} a-r a^{\prime}\right) / a^2 > \quad \text { for } r \in R, a \in A, $$ and we always consider $A^{-1} R$ as a differential ring in this way. In particular, if $R$ is a differential integral domain (that is, a differential ring whose underlying ring is an integral domain), then the derivation $\partial$ of $R$ extends uniquely to a derivation on the fraction field of $R$ . My question is why is the last sentence true? I cannot come up witha proof for it. EDIT I'm also trying to see why the given derivation actually meets the criteria stated in the first sentence. The only thing that springs to my mind is universal property of commutative rings (that they use earlier in the book for localization), namely that for every ring morphism $\phi: A \rightarrow B$ into a commutative ring $B$ with $\phi(S) \subseteq B^{\times}$ there is a unique ring morphism $\phi^{\prime}: S^{-1} A \rightarrow B$ such that $\phi=\phi^{\prime} \circ \iota$ , but I don't see how I could apply it here sicne $R$ a differential algebra.","I am reading Asymptotic Differential Algebra and Model Theory of Transseries and on p. 200 there is a claim/explanation of Localization: is a differential ring and its multiplicative subset, . Then there is a unique derivation on making into a differential ring where the natural map is a morphism of differential rings; it is given by and we always consider as a differential ring in this way. In particular, if is a differential integral domain (that is, a differential ring whose underlying ring is an integral domain), then the derivation of extends uniquely to a derivation on the fraction field of . My question is why is the last sentence true? I cannot come up witha proof for it. EDIT I'm also trying to see why the given derivation actually meets the criteria stated in the first sentence. The only thing that springs to my mind is universal property of commutative rings (that they use earlier in the book for localization), namely that for every ring morphism into a commutative ring with there is a unique ring morphism such that , but I don't see how I could apply it here sicne a differential algebra.","R A 0 \notin A A^{-1} R A^{-1} R r \mapsto r / 1: R
\rightarrow A^{-1} R  (r / a)^{\prime}=\left(r^{\prime} a-r a^{\prime}\right) / a^2
> \quad \text { for } r \in R, a \in A,  A^{-1} R R \partial R R \phi: A \rightarrow B B \phi(S) \subseteq B^{\times} \phi^{\prime}: S^{-1} A \rightarrow B \phi=\phi^{\prime} \circ \iota R","['abstract-algebra', 'group-theory', 'ring-theory', 'field-theory', 'localization']"
98,Problem with Veblen's proof for the transcendence of $\pi$,Problem with Veblen's proof for the transcendence of,\pi,"I'm trying to understand the following proof , (no need to read all of it), but there's one point where I'm stuck. The proof is by contradiction, so they use the definition of an algebraic number (a is algebraic if there exists a polynomial in the field extension such that $f(a)=0$ : "" $x_1,...,x_n$ are algebraic, so they are the roots of an equation $$f(x) = a_0+a_1x+...+a_nx^n$$ with integral coeffiecients, $a_0\neq0, a_n\neq0$ ."" Yet, on the last page the Girard–Newton formulas are used: ""But from Newton's formulas $$S_1+a_1=0, S_2+a_1S_1+2a_2+0,...$$ it follows that $S_1,S_2,...,S_{s-p}$ are whole numbers. Where from my own calculations based on Wikipedia (so not sure if this is correct), these formulas are given by $$\sum^k_{i=1} (-1)^{i-1}a_{n-k+i}S_i = ka_{n-k}$$ for $1 \leq k \leq n$ . Meaning that we would get $a_nS_1 + a_1 = 0$ etc. So if $a_n$ isn't 1, if $f$ isn't monic, $S_i$ wouldn't be an integer. Why do we suddenly assume $f$ to be monic, since this doesn't follow from the definition of an algebraic number. Is the proof wrong? What am I missing? This has been messing with me for days, thank you in advance. Edit: This is guessing work, but I think we may assume $f$ is monic from the start. By looking at the proof for the transcendence of $e$ , this is obvious.But I think there might be a way to choose $x_1,...,x_n$ as roots for $f$ such that it is monic and $$c+\sum^{n}_{i=1} e^{x_i} = 0.$$ Edit²: The formulas should be as follow: $a_nS_1 + a_{n-1} = 0, S_2 = S_1^2-2\frac{a_{n-2}}{a_n}$ etc","I'm trying to understand the following proof , (no need to read all of it), but there's one point where I'm stuck. The proof is by contradiction, so they use the definition of an algebraic number (a is algebraic if there exists a polynomial in the field extension such that : "" are algebraic, so they are the roots of an equation with integral coeffiecients, ."" Yet, on the last page the Girard–Newton formulas are used: ""But from Newton's formulas it follows that are whole numbers. Where from my own calculations based on Wikipedia (so not sure if this is correct), these formulas are given by for . Meaning that we would get etc. So if isn't 1, if isn't monic, wouldn't be an integer. Why do we suddenly assume to be monic, since this doesn't follow from the definition of an algebraic number. Is the proof wrong? What am I missing? This has been messing with me for days, thank you in advance. Edit: This is guessing work, but I think we may assume is monic from the start. By looking at the proof for the transcendence of , this is obvious.But I think there might be a way to choose as roots for such that it is monic and Edit²: The formulas should be as follow: etc","f(a)=0 x_1,...,x_n f(x) = a_0+a_1x+...+a_nx^n a_0\neq0, a_n\neq0 S_1+a_1=0, S_2+a_1S_1+2a_2+0,... S_1,S_2,...,S_{s-p} \sum^k_{i=1} (-1)^{i-1}a_{n-k+i}S_i = ka_{n-k} 1 \leq k \leq n a_nS_1 + a_1 = 0 a_n f S_i f f e x_1,...,x_n f c+\sum^{n}_{i=1} e^{x_i} = 0. a_nS_1 + a_{n-1} = 0, S_2 = S_1^2-2\frac{a_{n-2}}{a_n}","['abstract-algebra', 'number-theory', 'symmetric-polynomials', 'transcendence-theory']"
99,Property of Fitting subgroup: Let $G$ be a finite group and $C:= C_G(F(G))$. Then $O_p(C/C\cap F(G))=1$ for every prime $p$.,Property of Fitting subgroup: Let  be a finite group and . Then  for every prime .,G C:= C_G(F(G)) O_p(C/C\cap F(G))=1 p,"I'm trying to understand the proof of the following which is stated in Kurzweil and Stellmacher: Let $G$ be a finite group and $C:= C_G(F(G))$ . Then $$O_p(C/C\cap F(G))=1$$ for every prime $p$ . Here, $F(G)$ is the Fitting subgroup of $G$ , i.e., the product of all nilpotent normal subgroups of $G$ and $O_p(H)$ denotes the intersection of all Sylow $p$ -subgroups of a group $H$ . Also, a group is defined to be nilpotent if all its subgroups are subnormal. My attempt: Let $H=C\cap F(G)$ and let $\pi:C\to C/H$ be the quotient map. Let $P=\pi^{-1}(O_p(C/H))$ . If $P\subseteq H$ , then $\pi(P)= 1$ and so $O_p(C/H)=1$ . Therefore it suffices to show that $P\subseteq H$ . As $P\subseteq C$ , we are only left to show that $P\subseteq F(G)$ . As the $F(G)$ is the largest normal nilpotent subgroup of $G$ , it suffices to show that $P$ is normal in $G$ and $P$ is nilpotent. Normal: As $C$ is a normal subgroup of $G$ , so if $P$ is a characteristic subgroup of $C$ , then $P$ is also a normal subgroup of $G$ ......? Nilpotent: As subgroups of nilpotent groups are nilpotent, we only need to show that $C$ is nilpotent. However, as $H\subseteq Z(C)$ , we know that $C$ is nilpotent if and only if $C/H$ is nilpotent..... This is where I'm stuck. I don't know how to show that $P$ is a characteristic subgroup of $C$ and how to show that $C/H$ is nilpotent. Edit: Okay, I made some attempt on the nilpotent part: $Z(C)$ is a characteristic subgroup of $C$ and $C$ is a normal subgroup of $G$ , so $Z(C)$ is a normal subgroup of $G$ . Also, $Z(C)$ is nilpotent. Therefore $Z(C)\subseteq F(G)$ . Of course, $Z(C)\subseteq C$ . It follows that $H=Z(C)$ . So, we need to show that $C/Z(C)$ is nilpotent.... I don't know if this is any help.","I'm trying to understand the proof of the following which is stated in Kurzweil and Stellmacher: Let be a finite group and . Then for every prime . Here, is the Fitting subgroup of , i.e., the product of all nilpotent normal subgroups of and denotes the intersection of all Sylow -subgroups of a group . Also, a group is defined to be nilpotent if all its subgroups are subnormal. My attempt: Let and let be the quotient map. Let . If , then and so . Therefore it suffices to show that . As , we are only left to show that . As the is the largest normal nilpotent subgroup of , it suffices to show that is normal in and is nilpotent. Normal: As is a normal subgroup of , so if is a characteristic subgroup of , then is also a normal subgroup of ......? Nilpotent: As subgroups of nilpotent groups are nilpotent, we only need to show that is nilpotent. However, as , we know that is nilpotent if and only if is nilpotent..... This is where I'm stuck. I don't know how to show that is a characteristic subgroup of and how to show that is nilpotent. Edit: Okay, I made some attempt on the nilpotent part: is a characteristic subgroup of and is a normal subgroup of , so is a normal subgroup of . Also, is nilpotent. Therefore . Of course, . It follows that . So, we need to show that is nilpotent.... I don't know if this is any help.",G C:= C_G(F(G)) O_p(C/C\cap F(G))=1 p F(G) G G O_p(H) p H H=C\cap F(G) \pi:C\to C/H P=\pi^{-1}(O_p(C/H)) P\subseteq H \pi(P)= 1 O_p(C/H)=1 P\subseteq H P\subseteq C P\subseteq F(G) F(G) G P G P C G P C P G C H\subseteq Z(C) C C/H P C C/H Z(C) C C G Z(C) G Z(C) Z(C)\subseteq F(G) Z(C)\subseteq C H=Z(C) C/Z(C),"['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory', 'nilpotent-groups']"
