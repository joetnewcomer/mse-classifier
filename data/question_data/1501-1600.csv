,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Limit Supremum and Infimum. Struggling the concept,Limit Supremum and Infimum. Struggling the concept,,"I am struggling to understand what is the limit supremum/infimum. I've been told that it is not the same thing as ""the limit of a supremum of a set"" (which makes sense since the supremum/infimum is usually a number). I've consulted with two Analysis books, but none of them seem to be able to convey it what they are trying to say. I got an example in my notebook that may clarify my confusion Ex. Consider $\left \{-200,100,1,2,-1,2,-1,1,2,-1  \right \}$ Then let $v_k = \sup \left \{a_n : n \geq k \right \}$ and $\limsup_{n\to\infty} a_n= \lim_{k\to\infty} v_k=2$ and $\liminf_{n\to\infty} a_n=-200$ Can someone explain to me the reasoning (without omitting any details) for the answers? I think I got a feeling for the liminf, but not limsup","I am struggling to understand what is the limit supremum/infimum. I've been told that it is not the same thing as ""the limit of a supremum of a set"" (which makes sense since the supremum/infimum is usually a number). I've consulted with two Analysis books, but none of them seem to be able to convey it what they are trying to say. I got an example in my notebook that may clarify my confusion Ex. Consider Then let and and Can someone explain to me the reasoning (without omitting any details) for the answers? I think I got a feeling for the liminf, but not limsup","\left \{-200,100,1,2,-1,2,-1,1,2,-1  \right \} v_k = \sup \left \{a_n : n \geq k \right \} \limsup_{n\to\infty} a_n= \lim_{k\to\infty} v_k=2 \liminf_{n\to\infty} a_n=-200","['real-analysis', 'sequences-and-series', 'limsup-and-liminf']"
1,Importance of Least Upper Bound Property of $\mathbb{R}$,Importance of Least Upper Bound Property of,\mathbb{R},"My professor asserts that the Least Upper Bound Property of $\mathbb{R}$ (Completeness Axiom) is the most essential piece in the study of real analysis. He says that almost every theorem in calculus/analysis relies directly upon on this Property. I know that the Archimedian property of $\mathbb{R}$ directly uses the property for the proof, but I'm trying to think of other major theorems that use the property directly. Do you think he means consequences of the property? Because then the gates are wide open... Does anyone know of any other theorems that use the properly directly? Sorry this is more of a general question","My professor asserts that the Least Upper Bound Property of $\mathbb{R}$ (Completeness Axiom) is the most essential piece in the study of real analysis. He says that almost every theorem in calculus/analysis relies directly upon on this Property. I know that the Archimedian property of $\mathbb{R}$ directly uses the property for the proof, but I'm trying to think of other major theorems that use the property directly. Do you think he means consequences of the property? Because then the gates are wide open... Does anyone know of any other theorems that use the properly directly? Sorry this is more of a general question",,"['real-analysis', 'analysis']"
2,Methods to solve $\int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx$,Methods to solve,\int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx,"I have a feel this will be a duplicate question. I have had a look around and couldn't find it, so please advise if so. Here I wish to address the definite integral: \begin{equation}  I = \int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx \end{equation} I have solved it using Feynman's Trick, however I feel it's limited and am hoping to find other methods to solve. Without using Residues, what are some other approaches to this integral? My method: \begin{equation}  I(t) = \int_{0}^{\infty} \frac{e^{-tx^2}}{x^2 + 1}\:dx \end{equation} Here $I = I(1)$ and $I(0) = \frac{\pi}{2}$ . Take the derivative under the curve with respect to ' $t$ ' to achieve: \begin{align}  I'(t) &= \int_{0}^{\infty} \frac{-x^2e^{-tx^2}}{x^2 + 1}\:dx = -\int_{0}^{\infty} \frac{x^2e^{-tx^2}}{x^2 + 1}\:dx \\ &= -\left[\int_{0}^{\infty} \frac{\left(x^2 + 1 - 1\right)e^{-tx^2}}{x^2 + 1}\:dx \right] \\ &= -\int_{0}^{\infty} e^{-tx^2}\:dx  + \int_{0}^{\infty} \frac{e^{-tx^2}}{x^2 + 1}\:dx \\ &= -\frac{\sqrt{\pi}}{2}\frac{1}{\sqrt{t}} + I(t) \end{align} And so we arrive at the differential equation: \begin{equation}  I'(t) - I(t) = -\frac{\sqrt{\pi}}{2}\frac{1}{\sqrt{t}} \end{equation} Which yields the solution: \begin{equation}  I(t) = \frac{\pi}{2}e^t\operatorname{erfc}\left(t\right) \end{equation} Thus, \begin{equation}  I = I(1) \int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx =  \frac{\pi}{2}e\operatorname{erfc}(1) \end{equation} Addendum: Using the exact method I've employed, you can extend the above integral into a more genealised form: \begin{equation}  I = \int_{0}^{\infty} \frac{e^{-kx^2}}{x^2 + 1}\:dx =  \frac{\pi}{2}e^k\operatorname{erfc}(\sqrt{k}) \end{equation} Addendum 2: Whilst we are genealising: \begin{equation}  I = \int_{0}^{\infty} \frac{e^{-kx^2}}{ax^2 + b}\:dx =  \frac{\pi}{2b}e^\Phi\operatorname{erfc}(\sqrt{\Phi}) \end{equation} Where $\Phi = \frac{kb}{a}$ and $a,b,k \in \mathbb{R}^{+}$","I have a feel this will be a duplicate question. I have had a look around and couldn't find it, so please advise if so. Here I wish to address the definite integral: I have solved it using Feynman's Trick, however I feel it's limited and am hoping to find other methods to solve. Without using Residues, what are some other approaches to this integral? My method: Here and . Take the derivative under the curve with respect to ' ' to achieve: And so we arrive at the differential equation: Which yields the solution: Thus, Addendum: Using the exact method I've employed, you can extend the above integral into a more genealised form: Addendum 2: Whilst we are genealising: Where and","\begin{equation}
 I = \int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx
\end{equation} \begin{equation}
 I(t) = \int_{0}^{\infty} \frac{e^{-tx^2}}{x^2 + 1}\:dx
\end{equation} I = I(1) I(0) = \frac{\pi}{2} t \begin{align}
 I'(t) &= \int_{0}^{\infty} \frac{-x^2e^{-tx^2}}{x^2 + 1}\:dx = -\int_{0}^{\infty} \frac{x^2e^{-tx^2}}{x^2 + 1}\:dx \\
&= -\left[\int_{0}^{\infty} \frac{\left(x^2 + 1 - 1\right)e^{-tx^2}}{x^2 + 1}\:dx \right] \\
&= -\int_{0}^{\infty} e^{-tx^2}\:dx  + \int_{0}^{\infty} \frac{e^{-tx^2}}{x^2 + 1}\:dx \\
&= -\frac{\sqrt{\pi}}{2}\frac{1}{\sqrt{t}} + I(t)
\end{align} \begin{equation}
 I'(t) - I(t) = -\frac{\sqrt{\pi}}{2}\frac{1}{\sqrt{t}}
\end{equation} \begin{equation}
 I(t) = \frac{\pi}{2}e^t\operatorname{erfc}\left(t\right)
\end{equation} \begin{equation}
 I = I(1) \int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx =  \frac{\pi}{2}e\operatorname{erfc}(1)
\end{equation} \begin{equation}
 I = \int_{0}^{\infty} \frac{e^{-kx^2}}{x^2 + 1}\:dx =  \frac{\pi}{2}e^k\operatorname{erfc}(\sqrt{k})
\end{equation} \begin{equation}
 I = \int_{0}^{\infty} \frac{e^{-kx^2}}{ax^2 + b}\:dx =  \frac{\pi}{2b}e^\Phi\operatorname{erfc}(\sqrt{\Phi})
\end{equation} \Phi = \frac{kb}{a} a,b,k \in \mathbb{R}^{+}",['real-analysis']
3,"Why is one proof for Cauchy-Schwarz inequality easy, but directly it is hard?","Why is one proof for Cauchy-Schwarz inequality easy, but directly it is hard?",,"Let's say you are in $\mathbb{R}^n$ and you define the norm as $||x||=\sqrt{x_1^2+x_2^2...+x_n^2}$. This we recognize as the usual norm from the inner product: $||x|| = \sqrt{\langle x, x \rangle}$, where $\langle x, y \rangle = x_1 y_1 + x_2 y_2+ \cdots + x_n y_n$. It is easy to check that this satisfies all the axioms for an inner product. Then we may define orthogonality as a zero inner product, and we get the Pythagorean theorem, we define projection, and then the proof for Cauchy–Schwarz is pretty straight forward. But now comes my problem. Lets say you do not want to go through the inner product, but you still want to prove Cauchy–Schwarz. When you do not have an inner product, Cauchy–Schwarz do not make much sense, but I want the part where we have replaced the inner-product part. I mean, Cauchy–Schwarz says: $|\langle x, y \rangle| \le ||x|| \cdot ||y||$. This equation makes sense even without inner-products for our case: $\left| x_1 y_1 + x_2 y_2 + \cdots + x_n y_n \right| \le \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} \cdot \sqrt{y_1^2 + y_2^2 + \cdots + y_n^2}$ However I am not able to prove this inequality. For me, it is easier going through the inner product for proving this, but I want to be able to prove this inequality directly, how am I supposed to do that? That is, my problem is proving that $\left| x_1 y_1 + x_2 y_2 + \cdots + x_n y_n \right| \le \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} \cdot \sqrt{y_1^2 + y_2^2 + \cdots + y_n^2}$ without going through the inner product. Is this hard? Would you say it is easier defining the inner-product and proving it in that way? It seems weird that it should be easier to define a lot of new terms just to prove an inequality.","Let's say you are in $\mathbb{R}^n$ and you define the norm as $||x||=\sqrt{x_1^2+x_2^2...+x_n^2}$. This we recognize as the usual norm from the inner product: $||x|| = \sqrt{\langle x, x \rangle}$, where $\langle x, y \rangle = x_1 y_1 + x_2 y_2+ \cdots + x_n y_n$. It is easy to check that this satisfies all the axioms for an inner product. Then we may define orthogonality as a zero inner product, and we get the Pythagorean theorem, we define projection, and then the proof for Cauchy–Schwarz is pretty straight forward. But now comes my problem. Lets say you do not want to go through the inner product, but you still want to prove Cauchy–Schwarz. When you do not have an inner product, Cauchy–Schwarz do not make much sense, but I want the part where we have replaced the inner-product part. I mean, Cauchy–Schwarz says: $|\langle x, y \rangle| \le ||x|| \cdot ||y||$. This equation makes sense even without inner-products for our case: $\left| x_1 y_1 + x_2 y_2 + \cdots + x_n y_n \right| \le \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} \cdot \sqrt{y_1^2 + y_2^2 + \cdots + y_n^2}$ However I am not able to prove this inequality. For me, it is easier going through the inner product for proving this, but I want to be able to prove this inequality directly, how am I supposed to do that? That is, my problem is proving that $\left| x_1 y_1 + x_2 y_2 + \cdots + x_n y_n \right| \le \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} \cdot \sqrt{y_1^2 + y_2^2 + \cdots + y_n^2}$ without going through the inner product. Is this hard? Would you say it is easier defining the inner-product and proving it in that way? It seems weird that it should be easier to define a lot of new terms just to prove an inequality.",,"['real-analysis', 'vector-spaces', 'normed-spaces']"
4,Alternative solutions to $\lim_{n\to\infty} \frac{1}{\sqrt{n}}\int_{ 1/{\sqrt{n}}}^{1}\frac{\ln(1+x)}{x^3}\mathrm{d}x$,Alternative solutions to,\lim_{n\to\infty} \frac{1}{\sqrt{n}}\int_{ 1/{\sqrt{n}}}^{1}\frac{\ln(1+x)}{x^3}\mathrm{d}x,"Here is a limit that can be computed directly by performing the integration and then taking the limit, but the way is rather ugly. What else can we do? Might we avoid the integration?  $$\lim_{n\to\infty} \frac{1}{\sqrt{n}}\int_{ 1/{\sqrt{n}}}^{1}\frac{\ln(1+x)}{x^3}\mathrm{d}x$$","Here is a limit that can be computed directly by performing the integration and then taking the limit, but the way is rather ugly. What else can we do? Might we avoid the integration?  $$\lim_{n\to\infty} \frac{1}{\sqrt{n}}\int_{ 1/{\sqrt{n}}}^{1}\frac{\ln(1+x)}{x^3}\mathrm{d}x$$",,"['calculus', 'real-analysis', 'limits', 'definite-integrals']"
5,All solutions of $f(x)f(-x)=1$,All solutions of,f(x)f(-x)=1,"What are all the solutions of the functional equation $$f(x)f(-x)=1\,?$$ This one is trivial: $$f(x)=e^{cx},$$ as it is implied (for example) by the fundamental property of exponentials, namely $e^a e^b=e^{a+b}$ . But there is another solution: $$f(x)=\frac{c+x}{c-x}.$$ Are there any more solutions? How can I be sure?","What are all the solutions of the functional equation This one is trivial: as it is implied (for example) by the fundamental property of exponentials, namely . But there is another solution: Are there any more solutions? How can I be sure?","f(x)f(-x)=1\,? f(x)=e^{cx}, e^a e^b=e^{a+b} f(x)=\frac{c+x}{c-x}.","['real-analysis', 'functions', 'functional-equations']"
6,Can the product of a sequence of numbers between 0 and 1 converge to positive?,Can the product of a sequence of numbers between 0 and 1 converge to positive?,,"Let $x_n \in (0,1)$, is it possible that $\prod_{n=1}^\infty x_n >0$ ? I think it isn't, because such small numbers multiplied together will become smaller and smaller, but I am not sure if there is a positive lower bound for the product. Thanks!","Let $x_n \in (0,1)$, is it possible that $\prod_{n=1}^\infty x_n >0$ ? I think it isn't, because such small numbers multiplied together will become smaller and smaller, but I am not sure if there is a positive lower bound for the product. Thanks!",,['real-analysis']
7,Finding $\int^1_0 \frac{\log(1+x)}{x}dx$ without series expansion,Finding  without series expansion,\int^1_0 \frac{\log(1+x)}{x}dx,I was trying to evaluate  $$\int^1_0 \frac{\log(1+x)}{x}dx.$$ I expanded $\log(1+x) $  as $x -\frac{x^2}{2}... $ and got the answer. I would like to know if there is any way to do it without series expanding.,I was trying to evaluate  $$\int^1_0 \frac{\log(1+x)}{x}dx.$$ I expanded $\log(1+x) $  as $x -\frac{x^2}{2}... $ and got the answer. I would like to know if there is any way to do it without series expanding.,,"['real-analysis', 'calculus', 'integration', 'improper-integrals']"
8,Find exact value of $\sum_{n=1}^{\infty} \frac{1}{2^{n}} \tan \left( \frac{\pi}{2^{n+1}} \right)$,Find exact value of,\sum_{n=1}^{\infty} \frac{1}{2^{n}} \tan \left( \frac{\pi}{2^{n+1}} \right),"How can I compute the series $$\sum_{n=1}^{\infty} \frac{1}{2^{n}} \tan \left( \frac{\pi}{2^{n+1}} \right)$$ I just guess using half-angle formula to compute this series, but I can't do any approaches. How should I do to solve this infinite sum?","How can I compute the series I just guess using half-angle formula to compute this series, but I can't do any approaches. How should I do to solve this infinite sum?",\sum_{n=1}^{\infty} \frac{1}{2^{n}} \tan \left( \frac{\pi}{2^{n+1}} \right),"['real-analysis', 'calculus', 'sequences-and-series']"
9,"Evaluate $\int^1_0 \log^2(1-x) \log^2(x) \, dx$",Evaluate,"\int^1_0 \log^2(1-x) \log^2(x) \, dx","I have no idea where to even start. WolframAlpha cant compute it either. $$\int^1_0 \log^2(1-x) \log^2(x) \, dx$$ I think it can be done with series, but I am not sure, can someone help a little so I can get a start?? Thanks!","I have no idea where to even start. WolframAlpha cant compute it either. $$\int^1_0 \log^2(1-x) \log^2(x) \, dx$$ I think it can be done with series, but I am not sure, can someone help a little so I can get a start?? Thanks!",,"['real-analysis', 'integration', 'logarithms', 'improper-integrals', 'gamma-function']"
10,"If two measures agree on generating sets, do they agree on all measurable sets?","If two measures agree on generating sets, do they agree on all measurable sets?",,"Here's the problem that insprired my question: Suppose $X$ is the set of real numbers, $\mathcal B$ is the Borel $\sigma$-algebra, and $m$ and $n$ are two measures on $(X, \mathcal B)$ such that $m((a,b)) = n((a,b)) < \infty$ whenever $-\infty < a< b < \infty$. Prove that $m(A) = n(A)$ whenever $A \in \mathcal B$. When I look at this, I want to say that this problem is essentially trivial, but I can't convince myself that it actually is.  I want to say that since these measures agree on sets that generate $\mathcal B$, and every $\mathcal B$-measurable set can be formed by taking countable intersections and unions of these types of sets, then the result follows from the fact that these are both measures. So my questions are: Can we actually say that every Borel measurable set can be written as a countable union or intersection of these finite open intervals? Does the result immediately follow from properties of measures?","Here's the problem that insprired my question: Suppose $X$ is the set of real numbers, $\mathcal B$ is the Borel $\sigma$-algebra, and $m$ and $n$ are two measures on $(X, \mathcal B)$ such that $m((a,b)) = n((a,b)) < \infty$ whenever $-\infty < a< b < \infty$. Prove that $m(A) = n(A)$ whenever $A \in \mathcal B$. When I look at this, I want to say that this problem is essentially trivial, but I can't convince myself that it actually is.  I want to say that since these measures agree on sets that generate $\mathcal B$, and every $\mathcal B$-measurable set can be formed by taking countable intersections and unions of these types of sets, then the result follows from the fact that these are both measures. So my questions are: Can we actually say that every Borel measurable set can be written as a countable union or intersection of these finite open intervals? Does the result immediately follow from properties of measures?",,"['real-analysis', 'measure-theory']"
11,Proving that $S=\{\frac{1}{n}:n\in\mathbb{Z}\}\cup\{0\}$ is compact using the open cover definition,Proving that  is compact using the open cover definition,S=\{\frac{1}{n}:n\in\mathbb{Z}\}\cup\{0\},"Let $S=\{\frac{1}{n}:n\in\mathbb{Z}\}\cup\{0\}$ be a subset of $\mathbb{R}$. I have to prove using the open cover definition that this is compact. Could you help me, please?","Let $S=\{\frac{1}{n}:n\in\mathbb{Z}\}\cup\{0\}$ be a subset of $\mathbb{R}$. I have to prove using the open cover definition that this is compact. Could you help me, please?",,"['real-analysis', 'general-topology', 'analysis', 'compactness']"
12,The other ways to calculate $\int_0^1\frac{\ln(1-x^2)}{x}dx$,The other ways to calculate,\int_0^1\frac{\ln(1-x^2)}{x}dx,"Prove that   $$\int_0^1\frac{\ln(1-x^2)}{x}dx=-\frac{\pi^2}{12}$$ without using series expansion. An easy way to calculate the above integral is using series expansion. Here is an example \begin{align} \int_0^1\frac{\ln(1-x^2)}{x}dx&=-\int_0^1\frac{1}{x}\sum_{n=0}^\infty\frac{x^{2n}}{n} dx\\ &=-\sum_{n=0}^\infty\frac{1}{n}\int_0^1x^{2n-1}dx\\ &=-\frac{1}{2}\sum_{n=0}^\infty\frac{1}{n^2}\\ &=-\frac{\pi^2}{12} \end{align} I am wondering, are there other ways to calculate the integral without using series expansion of its integrand? Any method is welcome. Thank you. (>‿◠)✌","Prove that   $$\int_0^1\frac{\ln(1-x^2)}{x}dx=-\frac{\pi^2}{12}$$ without using series expansion. An easy way to calculate the above integral is using series expansion. Here is an example \begin{align} \int_0^1\frac{\ln(1-x^2)}{x}dx&=-\int_0^1\frac{1}{x}\sum_{n=0}^\infty\frac{x^{2n}}{n} dx\\ &=-\sum_{n=0}^\infty\frac{1}{n}\int_0^1x^{2n-1}dx\\ &=-\frac{1}{2}\sum_{n=0}^\infty\frac{1}{n^2}\\ &=-\frac{\pi^2}{12} \end{align} I am wondering, are there other ways to calculate the integral without using series expansion of its integrand? Any method is welcome. Thank you. (>‿◠)✌",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
13,Prove $e^{x+y}=e^{x}e^{y}$ by using Exponential Series,Prove  by using Exponential Series,e^{x+y}=e^{x}e^{y},"In order to show $e^{x+y}=e^{x}e^{y}$ by using Exponential Series, I got the following: $$e^{x}e^{y}=\Big(\sum_{n=0}^{\infty}{x^n \over n!}\Big)\cdot \Big(\sum_{n=0}^{\infty}{y^n \over n!}\Big)=\sum_{n=0}^{\infty}\sum_{k=0}^n{x^ky^n \over {k!n!}}$$ But, where should I go next to get $e^{x+y}=\sum_{n=0}^{\infty}{(x+y)^n \over n!}$. Thanks in advance.","In order to show $e^{x+y}=e^{x}e^{y}$ by using Exponential Series, I got the following: $$e^{x}e^{y}=\Big(\sum_{n=0}^{\infty}{x^n \over n!}\Big)\cdot \Big(\sum_{n=0}^{\infty}{y^n \over n!}\Big)=\sum_{n=0}^{\infty}\sum_{k=0}^n{x^ky^n \over {k!n!}}$$ But, where should I go next to get $e^{x+y}=\sum_{n=0}^{\infty}{(x+y)^n \over n!}$. Thanks in advance.",,"['real-analysis', 'exponential-function']"
14,Proving that $\sin x \ge \frac{x}{x+1}$,Proving that,\sin x \ge \frac{x}{x+1},"Prove that $$ \sin x \ge \frac{x}{x+1}, \space \space\forall x \in \left[0, \frac{\pi}{2}\right]$$","Prove that $$ \sin x \ge \frac{x}{x+1}, \space \space\forall x \in \left[0, \frac{\pi}{2}\right]$$",,"['calculus', 'real-analysis', 'trigonometry', 'inequality']"
15,Erroneous proof that derivative of function is always continuous,Erroneous proof that derivative of function is always continuous,,"Suppose that $f:\mathbb{R}\to \mathbb{R}$ is differentiable on the entire real line. (It is not difficult to give an example when $f'(x)$ is not continuous.) Find the flaw in the following ""proof"" that $f'(x)$ is continuous. Proof: Let $x_0$ be an arbitrary point on $\mathbb{R}$ and $f'(x_0)$ the derivative of $f$ at the point $x_0$ . By definition of the derivative and Lagrange's(aka MVT) theorem $$f'(x_0)=\lim \limits_{x\to x_0}\dfrac{f(x)-f(x_0)}{x-x_0}=\lim \limits_{x\to x_0}f'(\xi)=\lim \limits_{\xi \to x_0}f'(\xi), \qquad \qquad (*)$$ where $\xi$ is a point between $x_0$ and $x$ and therefore tends to $x_0$ as $x\to x_0$ . I have some difficulties to point out to the mistake in this ""proof"". But here is I was thinking. The first equality in $(*)$ is just the definition of the derivative. The second equality follows from MVT. I do not think that the third equality is true. So basically we need to show the following: If $\lim \limits_{x\to x_0}f'(\xi)=f'(x_0)$ then $\lim \limits_{\xi\to  x_0}f'(\xi)=f'(x_0)$ I was trying to show it via $\varepsilon-\delta$ formalist and the fact that for any $x\in \mathbb{R}$ so that $x\neq x_0$ the $\xi$ is in between $x$ and $x_0$ but I failed. Can anyone help me please? Am I right that the third equality in $(*)$ is not valid. And where is the flaw in this ""proof""? EDIT: Suppose that $\lim \limits_{x\to x_0}f'(g(x))=f'(x_0)$ , where $g:\mathbb{R}\to \mathbb{R}$ defined by $$g(x)= \begin{cases} \xi_x, & \text{if } x\neq x_0, \\ x_0, & \text{if } x=x_0, \end{cases}$$ and $g(x)$ has Intermediate Value property (every value between $g(x)$ and $x_0$ is taken somewhere between $x$ and $x_0$ by $g$ ). Then $$\lim \limits_{y\to x_0}f'(y)=f'(x_0).$$ Remark: I am trying to prove it in a rigorous way via $\varepsilon-\delta$ formalism but I have some issues. My approach: Let $\varepsilon>0$ be given then $\exists \delta>0$ such that $\forall x\in \mathbb{R} (0<|x-x_0|<\delta \Rightarrow |f'(g(x))-f'(x_0)|<\varepsilon)$ . I have no idea how to invoke the Intermediate value property to obtain the desired result.","Suppose that is differentiable on the entire real line. (It is not difficult to give an example when is not continuous.) Find the flaw in the following ""proof"" that is continuous. Proof: Let be an arbitrary point on and the derivative of at the point . By definition of the derivative and Lagrange's(aka MVT) theorem where is a point between and and therefore tends to as . I have some difficulties to point out to the mistake in this ""proof"". But here is I was thinking. The first equality in is just the definition of the derivative. The second equality follows from MVT. I do not think that the third equality is true. So basically we need to show the following: If then I was trying to show it via formalist and the fact that for any so that the is in between and but I failed. Can anyone help me please? Am I right that the third equality in is not valid. And where is the flaw in this ""proof""? EDIT: Suppose that , where defined by and has Intermediate Value property (every value between and is taken somewhere between and by ). Then Remark: I am trying to prove it in a rigorous way via formalism but I have some issues. My approach: Let be given then such that . I have no idea how to invoke the Intermediate value property to obtain the desired result.","f:\mathbb{R}\to \mathbb{R} f'(x) f'(x) x_0 \mathbb{R} f'(x_0) f x_0 f'(x_0)=\lim \limits_{x\to x_0}\dfrac{f(x)-f(x_0)}{x-x_0}=\lim \limits_{x\to x_0}f'(\xi)=\lim \limits_{\xi \to x_0}f'(\xi), \qquad \qquad (*) \xi x_0 x x_0 x\to x_0 (*) \lim \limits_{x\to x_0}f'(\xi)=f'(x_0) \lim \limits_{\xi\to
 x_0}f'(\xi)=f'(x_0) \varepsilon-\delta x\in \mathbb{R} x\neq x_0 \xi x x_0 (*) \lim \limits_{x\to x_0}f'(g(x))=f'(x_0) g:\mathbb{R}\to \mathbb{R} g(x)=
\begin{cases}
\xi_x, & \text{if } x\neq x_0, \\
x_0, & \text{if } x=x_0,
\end{cases} g(x) g(x) x_0 x x_0 g \lim \limits_{y\to x_0}f'(y)=f'(x_0). \varepsilon-\delta \varepsilon>0 \exists \delta>0 \forall x\in \mathbb{R} (0<|x-x_0|<\delta \Rightarrow |f'(g(x))-f'(x_0)|<\varepsilon)","['real-analysis', 'fake-proofs']"
16,Double Euler sum $ \sum_{k\geq 1} \frac{H_k^{(2)} H_k}{k^3} $,Double Euler sum, \sum_{k\geq 1} \frac{H_k^{(2)} H_k}{k^3} ,I proved the following result $$\displaystyle \sum_{k\geq 1}  \frac{H_k^{(2)} H_k}{k^3} =- \frac{97}{12} \zeta(6)+\frac{7}{4}\zeta(4)\zeta(2) + \frac{5}{2}\zeta(3)^2+\frac{2}{3}\zeta(2)^3$$ After consideration of powers of polylogarithms. You can refer to the following thread . My question is : are there any papers in the literature which dealt with that result? Are my evaluations worth publishing ?,I proved the following result $$\displaystyle \sum_{k\geq 1}  \frac{H_k^{(2)} H_k}{k^3} =- \frac{97}{12} \zeta(6)+\frac{7}{4}\zeta(4)\zeta(2) + \frac{5}{2}\zeta(3)^2+\frac{2}{3}\zeta(2)^3$$ After consideration of powers of polylogarithms. You can refer to the following thread . My question is : are there any papers in the literature which dealt with that result? Are my evaluations worth publishing ?,,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'harmonic-numbers', 'polylogarithm']"
17,3rd iterate of a continuous function equals identity function,3rd iterate of a continuous function equals identity function,,"If $ f: \mathbb{R} \to \mathbb{R} $ is continuous, and $\forall x \in \mathbb{R} :\;(f \circ f \circ f)(x) = x  $, show that $ f(x) = x $. The condition that $f$ is continuous on $\mathbb{R}$ is crucial for the proof. I can find functions such as $\displaystyle\frac{x-3}{x+1}$ that satisfies  $ (f \circ f \circ f)(x) = x $. I have tried to negate the conclusion to see if there's a contradiction, but got stuck.","If $ f: \mathbb{R} \to \mathbb{R} $ is continuous, and $\forall x \in \mathbb{R} :\;(f \circ f \circ f)(x) = x  $, show that $ f(x) = x $. The condition that $f$ is continuous on $\mathbb{R}$ is crucial for the proof. I can find functions such as $\displaystyle\frac{x-3}{x+1}$ that satisfies  $ (f \circ f \circ f)(x) = x $. I have tried to negate the conclusion to see if there's a contradiction, but got stuck.",,"['calculus', 'real-analysis', 'functional-equations', 'function-and-relation-composition']"
18,What is the best way to define the diameter of the empty subset of a metric space?,What is the best way to define the diameter of the empty subset of a metric space?,,"This question is related to Why are metric spaces non-empty? .  I think that a metric space should allowed to be empty, and many authorities, including Rudin, agree with me.  That way, any subset of a metric space is a metric space, you don't have to make an exception for $\varnothing$, and you can ascribe certain properties to $\varnothing$, such as, $\varnothing$ is compact and connected. By definition of diameter of a metric space, the diameter of $\varnothing$ should be $-\infty$, since the $\sup$ of the empty set is $-\infty$.  This ""feels wrong"", since the diameter is a measure of the ""size"" of a metric space or a subset thereof, and its seems like the diameter of $\varnothing$ ought to be zero. I guess I really have two questions: If you allow the diameter of the empty set to be $-\infty$, does it lead to problems?  For example, $-\infty$ plus any real number is $-\infty$, and I could imagine how that might lead to a problem, but I haven't seen an actual situtation where that happens. In practice, what do expert analysts (such as Rudin, Folland, Royden, etc.) use for the diameter of $\varnothing$?","This question is related to Why are metric spaces non-empty? .  I think that a metric space should allowed to be empty, and many authorities, including Rudin, agree with me.  That way, any subset of a metric space is a metric space, you don't have to make an exception for $\varnothing$, and you can ascribe certain properties to $\varnothing$, such as, $\varnothing$ is compact and connected. By definition of diameter of a metric space, the diameter of $\varnothing$ should be $-\infty$, since the $\sup$ of the empty set is $-\infty$.  This ""feels wrong"", since the diameter is a measure of the ""size"" of a metric space or a subset thereof, and its seems like the diameter of $\varnothing$ ought to be zero. I guess I really have two questions: If you allow the diameter of the empty set to be $-\infty$, does it lead to problems?  For example, $-\infty$ plus any real number is $-\infty$, and I could imagine how that might lead to a problem, but I haven't seen an actual situtation where that happens. In practice, what do expert analysts (such as Rudin, Folland, Royden, etc.) use for the diameter of $\varnothing$?",,"['real-analysis', 'analysis', 'reference-request', 'metric-spaces']"
19,Nowhere monotonic continuous function,Nowhere monotonic continuous function,,Does there exist a nowhere monotonic continuous function from some open subset of $\mathbb{R}$ to $\mathbb{R}$? Some nowhere differentiable function sort of object?,Does there exist a nowhere monotonic continuous function from some open subset of $\mathbb{R}$ to $\mathbb{R}$? Some nowhere differentiable function sort of object?,,"['real-analysis', 'general-topology']"
20,Pair of functions with same values and same derivatives at distinct points,Pair of functions with same values and same derivatives at distinct points,,"Let $f,g:[0,1]\to \mathbb{R}$ be two functions of class $C^1$ such that $f'(x)>0$ and $g'(x)>0$ for all $x\in [0,1]$ . Assume that $$ f(0) = g(0) \qquad \text{and} \qquad f(1) = g(1). $$ Show that there exist $x,y\in[0,1]$ such that $$ f(x) = g(y) \qquad \text{and} \qquad f'(x) = g'(y). $$ I made the following attempts (non of them worked): First, define a function $h:[0,1]\times [0,1]\to \mathbb{R}$ given by $$ h(x,y) = [f(x)-g(y)]^2 + [f'(x)-g'(y)]^2 $$ and try to show that $h(x,y) = 0$ for some $(x,y)$ . Then I defined $k:[0,1]\times [0,1] \to \mathbb{R}$ by $$ k(x,y) = \frac{1}{2}[f(x)-f(y)]^2 $$ and tried to show that at some point $(x,y)$ the divergence of $k$ is zero, because in this case we have that $$ 0 = \frac{\partial k}{\partial x}(x,y) + \frac{\partial k}{\partial y}(x,y) = (f(x)-g(y))f'(x)-(f(x)-f(y))g'(y) = (f(x)-f(y))(f'(x)-g'(y)) $$ but it didn't work either. May be I'm not following the right way to solve this exercise.","Let be two functions of class such that and for all . Assume that Show that there exist such that I made the following attempts (non of them worked): First, define a function given by and try to show that for some . Then I defined by and tried to show that at some point the divergence of is zero, because in this case we have that but it didn't work either. May be I'm not following the right way to solve this exercise.","f,g:[0,1]\to \mathbb{R} C^1 f'(x)>0 g'(x)>0 x\in [0,1] 
f(0) = g(0) \qquad \text{and} \qquad f(1) = g(1).
 x,y\in[0,1] 
f(x) = g(y) \qquad \text{and} \qquad f'(x) = g'(y).
 h:[0,1]\times [0,1]\to \mathbb{R} 
h(x,y) = [f(x)-g(y)]^2 + [f'(x)-g'(y)]^2
 h(x,y) = 0 (x,y) k:[0,1]\times [0,1] \to \mathbb{R} 
k(x,y) = \frac{1}{2}[f(x)-f(y)]^2
 (x,y) k 
0 = \frac{\partial k}{\partial x}(x,y) + \frac{\partial k}{\partial y}(x,y) = (f(x)-g(y))f'(x)-(f(x)-f(y))g'(y) = (f(x)-f(y))(f'(x)-g'(y))
","['real-analysis', 'calculus', 'derivatives']"
21,How to check if a function is convex,How to check if a function is convex,,"According to a calculus book I have been reading, we call a function $g(x)$ a convex function if $$g(\lambda x +(1-\lambda)y) \leq \lambda g(x) +(1-\lambda)g(y)$$ , for all $x,y$ and $0<\lambda<1$ . But if I have to check if a given function is convex or not,this definition seems hard and impractical to use. So,my question is, is there any easier way of checking convexity of a function and if there is,then why it is equivalent to this defiinition. Thanks in advance!","According to a calculus book I have been reading, we call a function a convex function if , for all and . But if I have to check if a given function is convex or not,this definition seems hard and impractical to use. So,my question is, is there any easier way of checking convexity of a function and if there is,then why it is equivalent to this defiinition. Thanks in advance!","g(x) g(\lambda x +(1-\lambda)y) \leq \lambda g(x) +(1-\lambda)g(y) x,y 0<\lambda<1","['real-analysis', 'calculus', 'functions', 'self-learning']"
22,Is there a slowest divergent function?,Is there a slowest divergent function?,,"So I've been playing around with some functions for a while, and started wondering about a slowest divergent function(as in $\lim_{x\to\infty} f(x)\to\infty$) and so I searched around for an answer. I can see that there are ways to construct a new function that is necessarily diverging slower than the original one. But then it struck me that it really feels like properties of an open set where there are no smallest element in $(0,\infty)$. So the question is this, is it possible to define recursively a function $f(x)$ such that $\lim_{x\to\infty} f(x)\to\infty$ and  $$ (\forall g(x) \neq f(x), \lim_{x\to\infty} g(x)\to\infty) \lim_{x\to\infty} {g(x)\over f(x)}\to\infty $$ For an example of a function defined recursively, consider $$ f(x)={x^{1\over f(x)}\over ln(x)} $$ which I have no idea how it behaves. The reason I'm stressing the recursion is beacuse despite having no smallest element on $(0,\infty)$ , elements can get arbitrarily close to the endpoints, and to do that with a function, I'm guessing recursion is the way to go.","So I've been playing around with some functions for a while, and started wondering about a slowest divergent function(as in $\lim_{x\to\infty} f(x)\to\infty$) and so I searched around for an answer. I can see that there are ways to construct a new function that is necessarily diverging slower than the original one. But then it struck me that it really feels like properties of an open set where there are no smallest element in $(0,\infty)$. So the question is this, is it possible to define recursively a function $f(x)$ such that $\lim_{x\to\infty} f(x)\to\infty$ and  $$ (\forall g(x) \neq f(x), \lim_{x\to\infty} g(x)\to\infty) \lim_{x\to\infty} {g(x)\over f(x)}\to\infty $$ For an example of a function defined recursively, consider $$ f(x)={x^{1\over f(x)}\over ln(x)} $$ which I have no idea how it behaves. The reason I'm stressing the recursion is beacuse despite having no smallest element on $(0,\infty)$ , elements can get arbitrarily close to the endpoints, and to do that with a function, I'm guessing recursion is the way to go.",,"['calculus', 'real-analysis', 'recursion']"
23,Derivative of Linear Map,Derivative of Linear Map,,"I'm reading Allan Pollack's Differential Topology and got stuck on this argument: In the second paragraph of page 9, section 1.2 he said ""Note that if $f:U\to \mathbf{R^m}$ is itself a linear map $L$, then $df_x=L$ for all $x\in U$. In particular, the derivative of the inclusion map of $U$ into $\mathbf{R^n}$ at any point $x\in U$ is the identity transformation of $\mathbf{R^n}$."" I don't quite understand his first sentence. How could $f=L$ and $L$ linear imply $df_x=L$? A counter example is $f(x)=x$ linear, then $f'(x)=1 \neq L$. Thanks a lot for everyone's help!","I'm reading Allan Pollack's Differential Topology and got stuck on this argument: In the second paragraph of page 9, section 1.2 he said ""Note that if $f:U\to \mathbf{R^m}$ is itself a linear map $L$, then $df_x=L$ for all $x\in U$. In particular, the derivative of the inclusion map of $U$ into $\mathbf{R^n}$ at any point $x\in U$ is the identity transformation of $\mathbf{R^n}$."" I don't quite understand his first sentence. How could $f=L$ and $L$ linear imply $df_x=L$? A counter example is $f(x)=x$ linear, then $f'(x)=1 \neq L$. Thanks a lot for everyone's help!",,"['calculus', 'real-analysis', 'linear-algebra', 'general-topology']"
24,Archimedean property,Archimedean property,,"I've been studying the axiomatic definition of the real numbers, and there's one thing I'm not entirely sure about. I think I've understood that the Archimedean axiom is added in order to discard ordered complete fields containing infinitesimals like the hyperreal numbers. Additionally, this property clearly cannot be derived solely from the axioms of ordered field and completeness, since $^*\mathbb{R}$ and $\mathbb{R}$ are two complete ordered fields, two models of the axioms, one of them Archimedean and the other non-Archimedean. Are these ideas correct? Thanks.","I've been studying the axiomatic definition of the real numbers, and there's one thing I'm not entirely sure about. I think I've understood that the Archimedean axiom is added in order to discard ordered complete fields containing infinitesimals like the hyperreal numbers. Additionally, this property clearly cannot be derived solely from the axioms of ordered field and completeness, since $^*\mathbb{R}$ and $\mathbb{R}$ are two complete ordered fields, two models of the axioms, one of them Archimedean and the other non-Archimedean. Are these ideas correct? Thanks.",,['real-analysis']
25,"Why haven't mathematicians come up with an efficient way of writing “sufficiently”, e.g. “for $n$ sufficiently large”","Why haven't mathematicians come up with an efficient way of writing “sufficiently”, e.g. “for  sufficiently large”",n,"Consider a typical proof in an introductory analysis course: Claim: Let $(x_n)_\mathbb{N}$ and $(y_n)_\mathbb{N}$ be convergent sequences in $\mathbb{R}$ (or $\mathbb{C}$ ) and let $x,y$ be their respective limits. Then $(x_n+y_n)_\mathbb{N}$ is convergent and its limit is $x+y$ . Proof: Let $\varepsilon >0$ . There exists $n_1$ resp. $n_2$ such that $$\forall n \geq n_1, |x_n-x| < \varepsilon/2$$ resp. $$\forall n \geq n_2, |y_n - y| < \varepsilon/2.$$ Let $n_0 = \max(n_1,n_2).$ The triangle inequality implies that $$\forall n \geq n_0, |(x_n + y_n) - (x+y)| \leq |x_n - x| + |y_n - y| < \varepsilon/2 + \varepsilon/2 = \varepsilon.$$ This proves the claim. As a first-year student, this is a proof structure that comes up a lot . And yet, a significant portion of it seems redundant. Namely, the actual value of $n_0$ that I chose is of almost no significance. I could just as well have chosen $n_1 + n_2$ or $\max(n_1,n_2)+52.$ The only thing that's important is that $n_0$ be greater than both $n_1$ and $n_2$ , which is necessarily possible due to the fact that $\mathbb{N}$ is totally-ordered and not bounded above. This remark has led me to come up with a notation which I use extensively in my notes and saves me vast amounts of ink. This notation is the following: I define the notation $\mathbb{N}^\infty$ to mean “any set of the form $\mathbb{N}\setminus \left\{0,\ldots,n_0\right\}$ where $n_0 \in \mathbb{N}$ . (The $\infty$ -symbol is supposed to symbolise “sufficiently close to infinity”.) Like little-oh and big-oh notation, $\mathbb{N}^\infty$ does not refer to a specific object but rather a generic object with a certain property. However, $\mathbb{N}^\infty$ sets have the following useful property: any finite intersection of $\mathbb{N}^\infty$ sets is $\mathbb{N}^\infty$ . (This is kind of like how any finite sum of $o(f)$ functions is $o(f).$ ) The last property has the following consequence: Let $P_1,\ldots,P_k$ be predicates on $\mathbb{N}.$ Suppose that for all $i=1,\ldots,k$ we have $$\forall n \in \mathbb{N}^\infty,P_i(n) \textrm{ is true}.$$ Then $$\forall n \in \mathbb{N}^\infty, (P_1(n)\wedge\ldots\wedge \ P_k(n)) \textrm{ is true}$$ This is just a fancy way of saying “If, in a finite set of predicates, each predicate is true for sufficiently large $n$ , then for sufficiently large $n$ , each predicate is simultaneously true.” Note that this fails if the number of predicates is infinite. Using this notation, the definition of the limit can be written as follows: We say that $(x_n)_\mathbb{N}$ tends to some number $x$ iff for all $\varepsilon >0,$ $$\forall n \in \mathbb{N}^\infty, |x_n - x| < \varepsilon.$$ Using the property that I just stated, proof I gave above can also be rewritten: Proof: Let $\varepsilon >0$ . Then $$\forall n \in \mathbb{N}^\infty,|x_n - x| < \varepsilon/2$$ and $$\forall n \in \mathbb{N}^\infty,|y_n - y| < \varepsilon/2$$ hence by the triangle inequality, $$\forall n \in \mathbb{N}^\infty,|(x_n+y_n) - (x+y)| < \varepsilon.$$ Not only is this version more concise, but in my opinion it is better from a pedagogical point of view. When a student unfamiliar with analysis reads the first version (see above), there is some chance that we will be side-winded by the construction of $n_0$ (which as I said bears little to no importance), and he will be detracted from the actual crux of the proof which is the use of the triangle inequality. On the other, if the same student reads the second version, assuming that he understands the notation, he won't be side-winded by information that is not strictly necessary to his conceptual understanding of the proof. Finally, and perhaps most importantly, there is no loss in rigour in using the $\mathbb{N}^\infty$ notation provided that the “rules of the game” are well-understood. In a similar vein, for functional limits I use the notation $I^a$ (where $I$ is an interval and $a$ is in the closure of $I$ ) to signify “the intersection of $I$ with some open interval centred around $a$ ”. Here, the $a$ in the exponent is intended to symbolise “sufficiently close to $a$ ”. We again have the property that any finite intersection of $I^a$ sets is $I^a$ . In a way that is similar to the above, this notation allows us to simplify definitions and proofs in a way that is in my opinion non-negligeable and pedagogically fruitful. Finally, I would like to ask: Since the notions of “sufficiently large” and “sufficiently close to” are so ubiquitous in analysis, why haven't mathematicians come up with a way to convey them efficiently?","Consider a typical proof in an introductory analysis course: Claim: Let and be convergent sequences in (or ) and let be their respective limits. Then is convergent and its limit is . Proof: Let . There exists resp. such that resp. Let The triangle inequality implies that This proves the claim. As a first-year student, this is a proof structure that comes up a lot . And yet, a significant portion of it seems redundant. Namely, the actual value of that I chose is of almost no significance. I could just as well have chosen or The only thing that's important is that be greater than both and , which is necessarily possible due to the fact that is totally-ordered and not bounded above. This remark has led me to come up with a notation which I use extensively in my notes and saves me vast amounts of ink. This notation is the following: I define the notation to mean “any set of the form where . (The -symbol is supposed to symbolise “sufficiently close to infinity”.) Like little-oh and big-oh notation, does not refer to a specific object but rather a generic object with a certain property. However, sets have the following useful property: any finite intersection of sets is . (This is kind of like how any finite sum of functions is ) The last property has the following consequence: Let be predicates on Suppose that for all we have Then This is just a fancy way of saying “If, in a finite set of predicates, each predicate is true for sufficiently large , then for sufficiently large , each predicate is simultaneously true.” Note that this fails if the number of predicates is infinite. Using this notation, the definition of the limit can be written as follows: We say that tends to some number iff for all Using the property that I just stated, proof I gave above can also be rewritten: Proof: Let . Then and hence by the triangle inequality, Not only is this version more concise, but in my opinion it is better from a pedagogical point of view. When a student unfamiliar with analysis reads the first version (see above), there is some chance that we will be side-winded by the construction of (which as I said bears little to no importance), and he will be detracted from the actual crux of the proof which is the use of the triangle inequality. On the other, if the same student reads the second version, assuming that he understands the notation, he won't be side-winded by information that is not strictly necessary to his conceptual understanding of the proof. Finally, and perhaps most importantly, there is no loss in rigour in using the notation provided that the “rules of the game” are well-understood. In a similar vein, for functional limits I use the notation (where is an interval and is in the closure of ) to signify “the intersection of with some open interval centred around ”. Here, the in the exponent is intended to symbolise “sufficiently close to ”. We again have the property that any finite intersection of sets is . In a way that is similar to the above, this notation allows us to simplify definitions and proofs in a way that is in my opinion non-negligeable and pedagogically fruitful. Finally, I would like to ask: Since the notions of “sufficiently large” and “sufficiently close to” are so ubiquitous in analysis, why haven't mathematicians come up with a way to convey them efficiently?","(x_n)_\mathbb{N} (y_n)_\mathbb{N} \mathbb{R} \mathbb{C} x,y (x_n+y_n)_\mathbb{N} x+y \varepsilon >0 n_1 n_2 \forall n \geq n_1, |x_n-x| < \varepsilon/2 \forall n \geq n_2, |y_n - y| < \varepsilon/2. n_0 = \max(n_1,n_2). \forall n \geq n_0, |(x_n + y_n) - (x+y)| \leq |x_n - x| + |y_n - y| < \varepsilon/2 + \varepsilon/2 = \varepsilon. n_0 n_1 + n_2 \max(n_1,n_2)+52. n_0 n_1 n_2 \mathbb{N} \mathbb{N}^\infty \mathbb{N}\setminus \left\{0,\ldots,n_0\right\} n_0 \in \mathbb{N} \infty \mathbb{N}^\infty \mathbb{N}^\infty \mathbb{N}^\infty \mathbb{N}^\infty o(f) o(f). P_1,\ldots,P_k \mathbb{N}. i=1,\ldots,k \forall n \in \mathbb{N}^\infty,P_i(n) \textrm{ is true}. \forall n \in \mathbb{N}^\infty, (P_1(n)\wedge\ldots\wedge \ P_k(n)) \textrm{ is true} n n (x_n)_\mathbb{N} x \varepsilon >0, \forall n \in \mathbb{N}^\infty, |x_n - x| < \varepsilon. \varepsilon >0 \forall n \in \mathbb{N}^\infty,|x_n - x| < \varepsilon/2 \forall n \in \mathbb{N}^\infty,|y_n - y| < \varepsilon/2 \forall n \in \mathbb{N}^\infty,|(x_n+y_n) - (x+y)| < \varepsilon. n_0 \mathbb{N}^\infty I^a I a I I a a a I^a I^a","['real-analysis', 'soft-question', 'education']"
26,Definition of regularity in PDE theory,Definition of regularity in PDE theory,,"I'm studying a course in PDE theory and we talked a lot about ""regularity"". However, we never gave a precise definition about what ""regularity of weak solutions"" is. Also a search on the web didn't clarify for me what regularity exactly is. Somehow I think that we want to find out how smoothly the weak solutions of a PDE behave, i.e. if a solution is $C^\infty$ for instance. My question is: Can someone pin down what people think about, when they talk about ""regularity of weak solutions""? Thanks!","I'm studying a course in PDE theory and we talked a lot about ""regularity"". However, we never gave a precise definition about what ""regularity of weak solutions"" is. Also a search on the web didn't clarify for me what regularity exactly is. Somehow I think that we want to find out how smoothly the weak solutions of a PDE behave, i.e. if a solution is $C^\infty$ for instance. My question is: Can someone pin down what people think about, when they talk about ""regularity of weak solutions""? Thanks!",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'soft-question']"
27,"Prove that $\int_0^{\pi/2}\ln^2(\cos x)\,dx=\frac{\pi}{2}\ln^2 2+\frac{\pi^3}{24}$",Prove that,"\int_0^{\pi/2}\ln^2(\cos x)\,dx=\frac{\pi}{2}\ln^2 2+\frac{\pi^3}{24}","Prove that \begin{equation} \int_0^{\pi/2}\ln^2(\cos x)\,dx=\frac{\pi}{2}\ln^2 2+\frac{\pi^3}{24} \end{equation} I tried to use by parts method and ended with \begin{equation} \int \ln^2(\cos x)\,dx=x\ln^2(\cos x)+2\int x\ln(\cos x)\tan x\,dx \end{equation} The latter integral seems hard to evaluate. Could anyone here please help me to prove it preferably with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you. Addendum: I also found this nice closed-form \begin{equation} -\int_0^{\pi/2}\ln^3(\cos x)\,dx=\frac{\pi}{2}\ln^3 2+\frac{\pi^3}{8}\ln 2 +\frac{3\pi}{4}\zeta(3) \end{equation} I hope someone here also help me to prove it. (>‿◠)✌","Prove that \begin{equation} \int_0^{\pi/2}\ln^2(\cos x)\,dx=\frac{\pi}{2}\ln^2 2+\frac{\pi^3}{24} \end{equation} I tried to use by parts method and ended with \begin{equation} \int \ln^2(\cos x)\,dx=x\ln^2(\cos x)+2\int x\ln(\cos x)\tan x\,dx \end{equation} The latter integral seems hard to evaluate. Could anyone here please help me to prove it preferably with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you. Addendum: I also found this nice closed-form \begin{equation} -\int_0^{\pi/2}\ln^3(\cos x)\,dx=\frac{\pi}{2}\ln^3 2+\frac{\pi^3}{8}\ln 2 +\frac{3\pi}{4}\zeta(3) \end{equation} I hope someone here also help me to prove it. (>‿◠)✌",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
28,The Cantor ternary set is totally disconnected,The Cantor ternary set is totally disconnected,,"A set $S$ in a metric space $X$ is called totally disconnected if for any distinct $x,y\in S$, there exists separated sets $A$ and $B$ with $x\in A$, $y\in B$ and $S=A \cup B$. Let $C=\bigcap_{n=1}^\infty C_n$ be the Cantor ternary set. Given $x,y \in C$ with $x\lt y$, set $\epsilon=y-x$. For each $n\in$ N , $C_n$ consists of a finite union of closed intervals. Explain why there must exist an N large enough so that it is impossible for $x$ and $y$ both to belong to the same closed interval in $C_N$. I know that the Cantor set is constructed by removing the middle open thirds for each n. And each $C_n$ has $2^n$ closed intervals. As you go on, the closed sets get significantly small, so it's safe to assume that for some N, $x$ and $y$ will be ""separated"" into two different closed intervals. I'm not sure how to show this formally though.","A set $S$ in a metric space $X$ is called totally disconnected if for any distinct $x,y\in S$, there exists separated sets $A$ and $B$ with $x\in A$, $y\in B$ and $S=A \cup B$. Let $C=\bigcap_{n=1}^\infty C_n$ be the Cantor ternary set. Given $x,y \in C$ with $x\lt y$, set $\epsilon=y-x$. For each $n\in$ N , $C_n$ consists of a finite union of closed intervals. Explain why there must exist an N large enough so that it is impossible for $x$ and $y$ both to belong to the same closed interval in $C_N$. I know that the Cantor set is constructed by removing the middle open thirds for each n. And each $C_n$ has $2^n$ closed intervals. As you go on, the closed sets get significantly small, so it's safe to assume that for some N, $x$ and $y$ will be ""separated"" into two different closed intervals. I'm not sure how to show this formally though.",,"['real-analysis', 'general-topology']"
29,Sufficient condition for convergence of a real sequence [duplicate],Sufficient condition for convergence of a real sequence [duplicate],,"This question already has answers here : Every subsequence of $x_n$ has a further subsequence which converges to $x$. Then the sequence $x_n$ converges to $x$. (4 answers) Closed 6 years ago . Let $(x_n)$ be a sequence of real numbers. Prove that if there exists $x$ such that every subsequence $(x_{n_k})$ of $(x_n)$ has a convergent (sub-)subsequence $(x_{n_{k_l}})$ to $x$, then the original sequence $(x_n)$ itself converges to $x$ . Thanks for any help.","This question already has answers here : Every subsequence of $x_n$ has a further subsequence which converges to $x$. Then the sequence $x_n$ converges to $x$. (4 answers) Closed 6 years ago . Let $(x_n)$ be a sequence of real numbers. Prove that if there exists $x$ such that every subsequence $(x_{n_k})$ of $(x_n)$ has a convergent (sub-)subsequence $(x_{n_{k_l}})$ to $x$, then the original sequence $(x_n)$ itself converges to $x$ . Thanks for any help.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
30,How is a singular continuous measure defined?,How is a singular continuous measure defined?,,"On a measurable space, how is a measure being singular continuous relative to another defined? I searched on the internet and in some books to no avail and it mostly appears in a special case - the Lebesgue measure space $\mathbb{R}$. Do you know if singular continuous measures can be generalized to a more general measure space than Lebesgue measure space $\mathbb{R}$? In particular, can it be defined on any measure space, as hinted by the Wiki article I linked below? The purpose of knowing the answers to previous questions is that I would like to know to what extent the decomposition of a singular measure into a discrete measure and a singular continuous measure still exist, all wrt a refrence measure? Thanks and regards! PS : In case you may wonder, I encounter this concept from Wikipedia (feel it somehow sloppy though): Given $μ$ and $ν$ two σ-finite signed measures on a measurable space   $(Ω,Σ)$, there exist two $σ$-finite signed measures $ν_0$ and $ν_1$   such that: $\nu=\nu_0+\nu_1\,$ $\nu_0\ll\mu$ (that is, $ν_0$ is absolutely continuous with respect to $μ$) $\nu_1\perp\mu$ (that is, $ν_1$ and $μ$ are singular). The decomposition of the singular part can refined: $$      \, \nu = \nu_{\mathrm{cont}} + \nu_{\mathrm{sing}} + \nu_{\mathrm{pp}} $$ where $\nu_{\mathrm{cont}}$ is the absolutely continuous part $\nu_{\mathrm{sing}}$ is the singular continuous part $\nu_{\mathrm{pp}}$ is the pure point part (a discrete measure).","On a measurable space, how is a measure being singular continuous relative to another defined? I searched on the internet and in some books to no avail and it mostly appears in a special case - the Lebesgue measure space $\mathbb{R}$. Do you know if singular continuous measures can be generalized to a more general measure space than Lebesgue measure space $\mathbb{R}$? In particular, can it be defined on any measure space, as hinted by the Wiki article I linked below? The purpose of knowing the answers to previous questions is that I would like to know to what extent the decomposition of a singular measure into a discrete measure and a singular continuous measure still exist, all wrt a refrence measure? Thanks and regards! PS : In case you may wonder, I encounter this concept from Wikipedia (feel it somehow sloppy though): Given $μ$ and $ν$ two σ-finite signed measures on a measurable space   $(Ω,Σ)$, there exist two $σ$-finite signed measures $ν_0$ and $ν_1$   such that: $\nu=\nu_0+\nu_1\,$ $\nu_0\ll\mu$ (that is, $ν_0$ is absolutely continuous with respect to $μ$) $\nu_1\perp\mu$ (that is, $ν_1$ and $μ$ are singular). The decomposition of the singular part can refined: $$      \, \nu = \nu_{\mathrm{cont}} + \nu_{\mathrm{sing}} + \nu_{\mathrm{pp}} $$ where $\nu_{\mathrm{cont}}$ is the absolutely continuous part $\nu_{\mathrm{sing}}$ is the singular continuous part $\nu_{\mathrm{pp}}$ is the pure point part (a discrete measure).",,"['real-analysis', 'measure-theory', 'reference-request', 'singular-measures']"
31,Sum of open/closed/compact sets in $\mathbb{R}^n$ open/closed/compact,Sum of open/closed/compact sets in  open/closed/compact,\mathbb{R}^n,"I know that the following exercise you can find on internet maybe with solution too, but I want to know, if my ""solutions"" are correct. Let $X,Y\subset \mathbb{R}^n$, $X+Y=\{x+y;x\in X, y\in Y\}$. Prove it or find a counterexample. 1) If X, Y open, then X+Y is open 2) If X, Y closed, then X+Y is closed 3) If X, Y compact, then X+Y is compact My ideas: 1) is true. Could you have a look if my solution is correct? My try: We know that for all $x\in X \exists \epsilon >0: B_{\epsilon}(x)=\{b\in X; \|b-x\|<\epsilon\}\subset X$, and for all $y\in Y \exists \epsilon' >0: B_{\epsilon'}(x)=\{b'\in X; \|b'-x\|<\epsilon'\}\subset Y$, because $X$ and $Y$ are open. Now: $\|z-(x+y)\|=\|z-x-y\|=\|z'-z'+z-x-y\|=\|z'-x+z-z'-y\|\le\|z'-x\|+\|z-z'-y\|<\epsilon+\epsilon'$. Define $\eta =\epsilon +\epsilon'$. We have: for $x+y\in X+Y \exists \eta >0, \eta =\epsilon +\epsilon': B_{\eta}(x+y)=\{z\in X+Y; \|z-(x+y)\|<\eta\}\subset X+Y$. I'm not sure, if this is correct. If my ""solution"" is false, could you help to correct? 2) First I said that this is true, but after googeling I found out that you can find a counterexample. Could you help me to find the mistake of my ""solution""? My try: For every sequence $(x_n)\subseteq X$ such that $x_n\to x_0$, $x_0\in \mathbb{R}^n$, it is $x_0\in X$, because X is closed. For every sequence $(y_n)\subseteq Y$, such that $y_n\to y_0$, $y_0\in \mathbb{R}^n$, it is $y_0\in Y$, because Y is closed. Let $(z_n)\subseteq X+Y$ be a sequence, $z_n=x_n+y_n$ for every $n\in\mathbb{N}$ and let $z_n\to z_0 \in \mathbb{R}^n$. But it is $z_n=x_n+y_n\to x_0+y_0$ and by the uniqueness of limits $z_0=x_0+y_0\in X+Y$, because X and Y are closed. But my try has to be wrong I think, because you find counterexamples for 2). And I don't find the mistake :(, could you help me? 3) Is it correct? I would say yes. Maybe I can prove it with a continuous function and $X+Y$ as it's compact image. Regards","I know that the following exercise you can find on internet maybe with solution too, but I want to know, if my ""solutions"" are correct. Let $X,Y\subset \mathbb{R}^n$, $X+Y=\{x+y;x\in X, y\in Y\}$. Prove it or find a counterexample. 1) If X, Y open, then X+Y is open 2) If X, Y closed, then X+Y is closed 3) If X, Y compact, then X+Y is compact My ideas: 1) is true. Could you have a look if my solution is correct? My try: We know that for all $x\in X \exists \epsilon >0: B_{\epsilon}(x)=\{b\in X; \|b-x\|<\epsilon\}\subset X$, and for all $y\in Y \exists \epsilon' >0: B_{\epsilon'}(x)=\{b'\in X; \|b'-x\|<\epsilon'\}\subset Y$, because $X$ and $Y$ are open. Now: $\|z-(x+y)\|=\|z-x-y\|=\|z'-z'+z-x-y\|=\|z'-x+z-z'-y\|\le\|z'-x\|+\|z-z'-y\|<\epsilon+\epsilon'$. Define $\eta =\epsilon +\epsilon'$. We have: for $x+y\in X+Y \exists \eta >0, \eta =\epsilon +\epsilon': B_{\eta}(x+y)=\{z\in X+Y; \|z-(x+y)\|<\eta\}\subset X+Y$. I'm not sure, if this is correct. If my ""solution"" is false, could you help to correct? 2) First I said that this is true, but after googeling I found out that you can find a counterexample. Could you help me to find the mistake of my ""solution""? My try: For every sequence $(x_n)\subseteq X$ such that $x_n\to x_0$, $x_0\in \mathbb{R}^n$, it is $x_0\in X$, because X is closed. For every sequence $(y_n)\subseteq Y$, such that $y_n\to y_0$, $y_0\in \mathbb{R}^n$, it is $y_0\in Y$, because Y is closed. Let $(z_n)\subseteq X+Y$ be a sequence, $z_n=x_n+y_n$ for every $n\in\mathbb{N}$ and let $z_n\to z_0 \in \mathbb{R}^n$. But it is $z_n=x_n+y_n\to x_0+y_0$ and by the uniqueness of limits $z_0=x_0+y_0\in X+Y$, because X and Y are closed. But my try has to be wrong I think, because you find counterexamples for 2). And I don't find the mistake :(, could you help me? 3) Is it correct? I would say yes. Maybe I can prove it with a continuous function and $X+Y$ as it's compact image. Regards",,"['real-analysis', 'general-topology']"
32,Evaluation of $\prod_{n=1}^\infty e\left(\frac{n}{n+1}\right)^{n}\sqrt{\frac{n}{n+1}}$,Evaluation of,\prod_{n=1}^\infty e\left(\frac{n}{n+1}\right)^{n}\sqrt{\frac{n}{n+1}},During my calculations I ended up with the following product: $$P=\prod_{n=1}^\infty e\left(\frac{n}{n+1}\right)^{n}\sqrt{\frac{n}{n+1}}$$ I tried to express it in terms of series by taking the logarithm  $$S=\ln P=\sum_{n=1}^\infty \ln\left(e\left(\frac{n}{n+1}\right)^{n}\sqrt{\frac{n}{n+1}}\right)$$ but I also got stuck. Numerical calculation suggests that it is equal to $$P\stackrel{?}=\frac{\sqrt{2\pi}}{e}$$ but I am not able to prove the conjecture. Any idea about how to evaluate the product? Any help would be appreciated. Thanks in advance.,During my calculations I ended up with the following product: $$P=\prod_{n=1}^\infty e\left(\frac{n}{n+1}\right)^{n}\sqrt{\frac{n}{n+1}}$$ I tried to express it in terms of series by taking the logarithm  $$S=\ln P=\sum_{n=1}^\infty \ln\left(e\left(\frac{n}{n+1}\right)^{n}\sqrt{\frac{n}{n+1}}\right)$$ but I also got stuck. Numerical calculation suggests that it is equal to $$P\stackrel{?}=\frac{\sqrt{2\pi}}{e}$$ but I am not able to prove the conjecture. Any idea about how to evaluate the product? Any help would be appreciated. Thanks in advance.,,"['calculus', 'real-analysis', 'sequences-and-series', 'products', 'infinite-product']"
33,What's the fastest way to tell if a function is uniformly continuous or not?,What's the fastest way to tell if a function is uniformly continuous or not?,,"I have my real analysis final tomorrow and there are multiple choice questions.  I'm wondering about a fast way to tell if a function is uniformly continuous or not.  I know and understand the definition of uniform continuity, and I understand its difference from continuity, but I'm realizing I don't really know how to tell if a function is uniformly continuous or not (on a given interval or on R). One of my classmates suggested that a function is NOT uniformly continuous if its derivative diverges in the given interval.  Is this true?  Can I just think of the graph of the function and if its slope does not eventually settle to some point, is it not uniformly continuous? Thanks in advance for any help regarding how to approach these kinds of questions!","I have my real analysis final tomorrow and there are multiple choice questions.  I'm wondering about a fast way to tell if a function is uniformly continuous or not.  I know and understand the definition of uniform continuity, and I understand its difference from continuity, but I'm realizing I don't really know how to tell if a function is uniformly continuous or not (on a given interval or on R). One of my classmates suggested that a function is NOT uniformly continuous if its derivative diverges in the given interval.  Is this true?  Can I just think of the graph of the function and if its slope does not eventually settle to some point, is it not uniformly continuous? Thanks in advance for any help regarding how to approach these kinds of questions!",,['real-analysis']
34,"Prove: $\int_0^{\infty} \frac{\ln{(1+x)}\arctan{(\sqrt{x})}}{4+x^2} \, \mathrm{d}x = \frac{\pi}{2} \arctan{\left(\frac{1}{2}\right)} \ln{5}$",Prove:,"\int_0^{\infty} \frac{\ln{(1+x)}\arctan{(\sqrt{x})}}{4+x^2} \, \mathrm{d}x = \frac{\pi}{2} \arctan{\left(\frac{1}{2}\right)} \ln{5}","Prove: $$\int_0^{\infty} \frac{\ln{(1+x)}\arctan{(\sqrt{x})}}{4+x^2} \, \mathrm{d}x = \frac{\pi}{2} \arctan{\left(\frac{1}{2}\right)} \ln{5}$$ This might be a repeat question (I couldnt find a question of this here).  If im being honest I dont know the first step really...  Maybe a clever integration by parts, substitution, differentiation under integral sign, power series, or contour?  If someone could give advice.","Prove: This might be a repeat question (I couldnt find a question of this here).  If im being honest I dont know the first step really...  Maybe a clever integration by parts, substitution, differentiation under integral sign, power series, or contour?  If someone could give advice.","\int_0^{\infty} \frac{\ln{(1+x)}\arctan{(\sqrt{x})}}{4+x^2} \, \mathrm{d}x = \frac{\pi}{2} \arctan{\left(\frac{1}{2}\right)} \ln{5}","['real-analysis', 'calculus']"
35,Integral $\int_1^{\sqrt{2}}\frac{1}{x}\ln\left(\frac{2-2x^2+x^4}{2x-2x^2+x^3}\right)dx$,Integral,\int_1^{\sqrt{2}}\frac{1}{x}\ln\left(\frac{2-2x^2+x^4}{2x-2x^2+x^3}\right)dx,"Calculate the following integral: \begin{equation} \int_1^{\sqrt{2}}\frac{1}{x}\ln\left(\frac{2-2x^2+x^4}{2x-2x^2+x^3}\right)dx \end{equation} I am having trouble to calculate the integral. I tried to use by parts method but it didn't help. Wolfram Alpha gives me $0$ as the answer but I don't know how to get it. I also tried to search the similar question here and I got this: $$I=\int_{-1}^1\frac1x\sqrt{\frac{1+x}{1-x}}\ln\left(\frac{2\,x^2+2\,x+1}{2\,x^2-2\,x+1}\right)\ \mathrm dx,$$ yet it didn't help much. Besides, I don't understand the answers there. Could anyone here please help me to calculate the integral preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.","Calculate the following integral: \begin{equation} \int_1^{\sqrt{2}}\frac{1}{x}\ln\left(\frac{2-2x^2+x^4}{2x-2x^2+x^3}\right)dx \end{equation} I am having trouble to calculate the integral. I tried to use by parts method but it didn't help. Wolfram Alpha gives me $0$ as the answer but I don't know how to get it. I also tried to search the similar question here and I got this: $$I=\int_{-1}^1\frac1x\sqrt{\frac{1+x}{1-x}}\ln\left(\frac{2\,x^2+2\,x+1}{2\,x^2-2\,x+1}\right)\ \mathrm dx,$$ yet it didn't help much. Besides, I don't understand the answers there. Could anyone here please help me to calculate the integral preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
36,Stein's lemma condition,Stein's lemma condition,,"(Apologies if I break some conventions, this is my first time posting!) I am working on proving Stein's characterization of the Normal distribution: for Z $\sim N(0,1)$ and some differentiable function $f$ with $E[|f'(Z)|] < \infty$, $$E[Zf(Z)] = E[f'(Z)]$$ Writing the LHS expression in integral form and integrating by parts, I eventually obtain: $$E[Zf(Z)] = \frac{1}{\sqrt{2\pi}} \left[ -f(z) \cdot \exp \left\{ \frac{-z^2}{2} \right\} \right] \Bigg|_{-\infty}^{\infty} + E[f'(Z)]$$ Now I need to show that the first expression on the right hand size is zero. Intuitively, this seems clear because of the exponential term, but I am having trouble explicitly applying the condition on $f'$ to prove this rigorously. Any ideas?","(Apologies if I break some conventions, this is my first time posting!) I am working on proving Stein's characterization of the Normal distribution: for Z $\sim N(0,1)$ and some differentiable function $f$ with $E[|f'(Z)|] < \infty$, $$E[Zf(Z)] = E[f'(Z)]$$ Writing the LHS expression in integral form and integrating by parts, I eventually obtain: $$E[Zf(Z)] = \frac{1}{\sqrt{2\pi}} \left[ -f(z) \cdot \exp \left\{ \frac{-z^2}{2} \right\} \right] \Bigg|_{-\infty}^{\infty} + E[f'(Z)]$$ Now I need to show that the first expression on the right hand size is zero. Intuitively, this seems clear because of the exponential term, but I am having trouble explicitly applying the condition on $f'$ to prove this rigorously. Any ideas?",,"['real-analysis', 'probability', 'statistics']"
37,Is $f(x)=\sin(x^2)$ periodic?,Is  periodic?,f(x)=\sin(x^2),"Is the function $f:\Bbb R \rightarrow \Bbb R$ defined as $f(x)=\sin(x^2)$, for all $x\in\Bbb R$, periodic? Here's my attempt to solve this: Let's assume that it is periodic. For a function to be periodic, it must satisfy $f(x)=f(T+x)$ for all $x\in\Bbb R$, so it must satisfy the relation for $x=0$ as well. So we get that $T^2=k\pi \iff T=\sqrt{k\pi}$, $k\in\Bbb N$ (since $T$ must be positive, we remove the $-\sqrt{k\pi}$ solution). So what now? I tried taking $x=\sqrt\pi$ and using the $T$ I found, and I get this: $$ \sin\pi=\sin(T+\sqrt\pi)\iff-1=\sin(\pi(\sqrt k+1)^2)\iff k+2\sqrt k+1=3/2+l  $$ Is this enough for contradiction? The left side of equation is sometimes irrational and gets rational only when $k$ is perfect square, which doesn't happen periodic, while the right hand side is always rational. Or I'm still missing some steps? Thanks.","Is the function $f:\Bbb R \rightarrow \Bbb R$ defined as $f(x)=\sin(x^2)$, for all $x\in\Bbb R$, periodic? Here's my attempt to solve this: Let's assume that it is periodic. For a function to be periodic, it must satisfy $f(x)=f(T+x)$ for all $x\in\Bbb R$, so it must satisfy the relation for $x=0$ as well. So we get that $T^2=k\pi \iff T=\sqrt{k\pi}$, $k\in\Bbb N$ (since $T$ must be positive, we remove the $-\sqrt{k\pi}$ solution). So what now? I tried taking $x=\sqrt\pi$ and using the $T$ I found, and I get this: $$ \sin\pi=\sin(T+\sqrt\pi)\iff-1=\sin(\pi(\sqrt k+1)^2)\iff k+2\sqrt k+1=3/2+l  $$ Is this enough for contradiction? The left side of equation is sometimes irrational and gets rational only when $k$ is perfect square, which doesn't happen periodic, while the right hand side is always rational. Or I'm still missing some steps? Thanks.",,"['real-analysis', 'trigonometry', 'periodic-functions']"
38,Value of $\lim_{n\to \infty}\frac{1^n+2^n+\cdots+(n-1)^n}{n^n}$,Value of,\lim_{n\to \infty}\frac{1^n+2^n+\cdots+(n-1)^n}{n^n},"I remember that a couple of years ago a friend showed me and some other people the following expression: $$\lim_{n\to \infty}\frac{1^n+2^n+\cdots+(n-1)^n}{n^n}.$$ As shown below, I can prove that this limit exists by the monotone convergence theorem. I also remember that my friend gave a very dubious ""proof"" that the value of the limit is $\frac{1}{e-1}$. I cannot remember the details of the proof, but I am fairly certain that it made the common error of treating $n$ as a variable in some places at some times and as a constant in other places at other times. Nevertheless, numerical analysis suggests that the value my friend gave was correct, even if his methods were flawed. My question is then: What is the value of this limit and how do we prove it rigorously? (Also, for bonus points, What might my friend's original proof have been and what exactly was his error, if any? ) I give my convergence proof below in two parts. In both parts, I define the sequence $a_n$ by $a_n=\frac{1^n+2^n+\cdots+(n-1)^n}{n^n}$ for all integers $n\ge 2$. First, I prove that $a_n$ is bounded above by $1$. Second, I prove that $a_n$ is increasing. (1) The sequence $a_n$ satisfies $a_n<1$ for all $n\ge 2$. Note that $a_n<1$ is equivalent to $1^n+2^n+\cdots+(n-1)^n<n^n$. I prove this second statement by induction. Observe that $1^2=1<4=2^2$. Now suppose that $1^n+2^n+\cdots+(n-1)^n<n^n$ for some integer $n\ge 2$. Then $$1^{n+1}+2^{n+1}+\cdots+(n-1)^{n+1}+n^{n+1}\le(n-1)(1^n+2^n+\cdots+(n-1)^n)+n^{n+1}<(n-1)n^n+n^{n+1}<(n+1)n^n+n^{n+1}\le n^{n+1}+(n+1)n^n+\binom{n+1}{2}n^{n-1}+\cdots+1=(n+1)^{n+1}.$$ (2) The sequence $a_n$ is increasing for all $n\ge 2$. We must first prove the following preliminary proposition. (I'm not sure if ""lemma"" is appropriate for this.) (2a) For all integers $n\ge 2$ and $2\le k\le n$, $\left(\frac{k-1}{k}\right)^n\le\left(\frac{k}{k+1}\right)^{n+1}$. We observe that $k^2-1\le kn$, so upon division by $k(k^2-1)$, we get $\frac{1}{k}\le\frac{n}{k^2-1}$. By Bernoulli's Inequality , we may find: $$\frac{k+1}{k}\le 1+\frac{n}{k^2-1}\le\left(1+\frac{1}{k^2-1}\right)^n=\left(\frac{k^2}{k^2-1}\right)^n.$$ A little multiplication and we arrive at $\left(\frac{k-1}{k}\right)^n\le\left(\frac{k}{k+1}\right)^{n+1}$. We may now first apply this to see that $\left(\frac{n-1}{n}\right)^n\le\left(\frac{n}{n+1}\right)^{n+1}$. Then we suppose that for some integer $2\le k\le n$, we have $\left(\frac{k}{n}\right)^n\le\left(\frac{k+1}{n+1}\right)^{n+1}$. Then: $$\left(\frac{k-1}{n}\right)^n=\left(\frac{k}{n}\right)^n\left(\frac{k-1}{k}\right)^n\le\left(\frac{k+1}{n+1}\right)^{n+1}\left(\frac{k}{k+1}\right)^{n+1}=\left(\frac{k}{n+1}\right)^{n+1}.$$ By backwards (finite) induction from $n$, we have that $\left(\frac{k}{n}\right)^n\le\left(\frac{k+1}{n+1}\right)^{n+1}$ for all integers $1\le k\le n$, so: $$a_n=\left(\frac{1}{n}\right)^n+\left(\frac{2}{n}\right)^n+\cdots+\left(\frac{n-1}{n}\right)^n\le\left(\frac{2}{n+1}\right)^{n+1}+\left(\frac{3}{n+1}\right)^{n+1}+\cdots+\left(\frac{n}{n+1}\right)^{n+1}<\left(\frac{1}{n+1}\right)^{n+1}+\left(\frac{2}{n+1}\right)^{n+1}+\left(\frac{3}{n+1}\right)^{n+1}+\cdots+\left(\frac{n}{n+1}\right)^{n+1}=a_{n+1}.$$ (In fact, this proves that $a_n$ is strictly increasing.) By the monotone convergence theorem, $a_n$ converges. I should note that I am not especially well-practiced in proving these sorts of inequalities, so I may have given a significantly more complicated proof than necessary. If this is the case, feel free to explain in a comment or in your answer. I'd love to get a better grip on these inequalities in addition to finding out what the limit is. Thanks!","I remember that a couple of years ago a friend showed me and some other people the following expression: $$\lim_{n\to \infty}\frac{1^n+2^n+\cdots+(n-1)^n}{n^n}.$$ As shown below, I can prove that this limit exists by the monotone convergence theorem. I also remember that my friend gave a very dubious ""proof"" that the value of the limit is $\frac{1}{e-1}$. I cannot remember the details of the proof, but I am fairly certain that it made the common error of treating $n$ as a variable in some places at some times and as a constant in other places at other times. Nevertheless, numerical analysis suggests that the value my friend gave was correct, even if his methods were flawed. My question is then: What is the value of this limit and how do we prove it rigorously? (Also, for bonus points, What might my friend's original proof have been and what exactly was his error, if any? ) I give my convergence proof below in two parts. In both parts, I define the sequence $a_n$ by $a_n=\frac{1^n+2^n+\cdots+(n-1)^n}{n^n}$ for all integers $n\ge 2$. First, I prove that $a_n$ is bounded above by $1$. Second, I prove that $a_n$ is increasing. (1) The sequence $a_n$ satisfies $a_n<1$ for all $n\ge 2$. Note that $a_n<1$ is equivalent to $1^n+2^n+\cdots+(n-1)^n<n^n$. I prove this second statement by induction. Observe that $1^2=1<4=2^2$. Now suppose that $1^n+2^n+\cdots+(n-1)^n<n^n$ for some integer $n\ge 2$. Then $$1^{n+1}+2^{n+1}+\cdots+(n-1)^{n+1}+n^{n+1}\le(n-1)(1^n+2^n+\cdots+(n-1)^n)+n^{n+1}<(n-1)n^n+n^{n+1}<(n+1)n^n+n^{n+1}\le n^{n+1}+(n+1)n^n+\binom{n+1}{2}n^{n-1}+\cdots+1=(n+1)^{n+1}.$$ (2) The sequence $a_n$ is increasing for all $n\ge 2$. We must first prove the following preliminary proposition. (I'm not sure if ""lemma"" is appropriate for this.) (2a) For all integers $n\ge 2$ and $2\le k\le n$, $\left(\frac{k-1}{k}\right)^n\le\left(\frac{k}{k+1}\right)^{n+1}$. We observe that $k^2-1\le kn$, so upon division by $k(k^2-1)$, we get $\frac{1}{k}\le\frac{n}{k^2-1}$. By Bernoulli's Inequality , we may find: $$\frac{k+1}{k}\le 1+\frac{n}{k^2-1}\le\left(1+\frac{1}{k^2-1}\right)^n=\left(\frac{k^2}{k^2-1}\right)^n.$$ A little multiplication and we arrive at $\left(\frac{k-1}{k}\right)^n\le\left(\frac{k}{k+1}\right)^{n+1}$. We may now first apply this to see that $\left(\frac{n-1}{n}\right)^n\le\left(\frac{n}{n+1}\right)^{n+1}$. Then we suppose that for some integer $2\le k\le n$, we have $\left(\frac{k}{n}\right)^n\le\left(\frac{k+1}{n+1}\right)^{n+1}$. Then: $$\left(\frac{k-1}{n}\right)^n=\left(\frac{k}{n}\right)^n\left(\frac{k-1}{k}\right)^n\le\left(\frac{k+1}{n+1}\right)^{n+1}\left(\frac{k}{k+1}\right)^{n+1}=\left(\frac{k}{n+1}\right)^{n+1}.$$ By backwards (finite) induction from $n$, we have that $\left(\frac{k}{n}\right)^n\le\left(\frac{k+1}{n+1}\right)^{n+1}$ for all integers $1\le k\le n$, so: $$a_n=\left(\frac{1}{n}\right)^n+\left(\frac{2}{n}\right)^n+\cdots+\left(\frac{n-1}{n}\right)^n\le\left(\frac{2}{n+1}\right)^{n+1}+\left(\frac{3}{n+1}\right)^{n+1}+\cdots+\left(\frac{n}{n+1}\right)^{n+1}<\left(\frac{1}{n+1}\right)^{n+1}+\left(\frac{2}{n+1}\right)^{n+1}+\left(\frac{3}{n+1}\right)^{n+1}+\cdots+\left(\frac{n}{n+1}\right)^{n+1}=a_{n+1}.$$ (In fact, this proves that $a_n$ is strictly increasing.) By the monotone convergence theorem, $a_n$ converges. I should note that I am not especially well-practiced in proving these sorts of inequalities, so I may have given a significantly more complicated proof than necessary. If this is the case, feel free to explain in a comment or in your answer. I'd love to get a better grip on these inequalities in addition to finding out what the limit is. Thanks!",,"['real-analysis', 'sequences-and-series', 'limits']"
39,Asymptotic rate of decay of the integrals,Asymptotic rate of decay of the integrals,,"For each $n$ define the sequence $y_n= \int_{0}^\infty (1-e^{-\frac{x^2}{4}})^n e^{-x}dx$ . By dominated convergence theorem, it is clear that $y_n$ converges to $0$ . I am interested in finding the rate at which $y_n$ decays. Mainly I am interested to know if the decay is exponential in $n$ or not i.e $y_n$ is roughly $c^n$ for some n ? I have encountered this in a physics problem. I am unsure how to study the asymptotic behaviour. Any help is appreciated.","For each define the sequence . By dominated convergence theorem, it is clear that converges to . I am interested in finding the rate at which decays. Mainly I am interested to know if the decay is exponential in or not i.e is roughly for some n ? I have encountered this in a physics problem. I am unsure how to study the asymptotic behaviour. Any help is appreciated.",n y_n= \int_{0}^\infty (1-e^{-\frac{x^2}{4}})^n e^{-x}dx y_n 0 y_n n y_n c^n,"['real-analysis', 'measure-theory', 'definite-integrals', 'asymptotics', 'mathematical-physics']"
40,A proof that the Cantor set is Perfect,A proof that the Cantor set is Perfect,,"I found in a book a proof that the Cantor Set $\Delta$ is perfect, however I would like to know if ""my proof"" does the job in the same way. Theorem : The Cantor Set $\Delta$ is perfect. Proof: Let $x \in \Delta$ and fix $\epsilon > 0$. Then, we can take a $n_0 = n$ sufficiently large to have $\epsilon > 1/3^{n_0}$.   Thus, the interval $[a, b]$ where $x$ lies is a subset of $B_\epsilon > (x)$. Hence, by iterating the construction of the Cantor set for $N >  n_0$, we have intervals of length $1/3^N$ all included in $B_\epsilon  (x)$, but with only one of those intervals such that $x$ lies within. The intution behind the proof was that we should prove that for every $x$, if $x \in \Delta$, then for every $\epsilon >0$, $B_\epsilon (x) \setminus \{x\} \cap \Delta \neq \varnothing$. Now, I do not particularly like my reference to the $[a, b]$ interval that is not mentioned before. Moreover, here – by choosing a closed interval – I am trying to address all at once the case in which $x$ is an endpoint of one of the closed intervals that form $\Delta$. Finally, I did not close the proof with a statement like ""Thus, there are infinitely many points that differ from $x$ and that lie within $B_\epsilon (x)$. In the end, I am not completely sure if this can be considered a proof or not. The intuition is correct (I am kind of positive about it), but I am not sure if I was actually able to write down my intuition in a good way. As always any feedback is more than welcome. Thank you!","I found in a book a proof that the Cantor Set $\Delta$ is perfect, however I would like to know if ""my proof"" does the job in the same way. Theorem : The Cantor Set $\Delta$ is perfect. Proof: Let $x \in \Delta$ and fix $\epsilon > 0$. Then, we can take a $n_0 = n$ sufficiently large to have $\epsilon > 1/3^{n_0}$.   Thus, the interval $[a, b]$ where $x$ lies is a subset of $B_\epsilon > (x)$. Hence, by iterating the construction of the Cantor set for $N >  n_0$, we have intervals of length $1/3^N$ all included in $B_\epsilon  (x)$, but with only one of those intervals such that $x$ lies within. The intution behind the proof was that we should prove that for every $x$, if $x \in \Delta$, then for every $\epsilon >0$, $B_\epsilon (x) \setminus \{x\} \cap \Delta \neq \varnothing$. Now, I do not particularly like my reference to the $[a, b]$ interval that is not mentioned before. Moreover, here – by choosing a closed interval – I am trying to address all at once the case in which $x$ is an endpoint of one of the closed intervals that form $\Delta$. Finally, I did not close the proof with a statement like ""Thus, there are infinitely many points that differ from $x$ and that lie within $B_\epsilon (x)$. In the end, I am not completely sure if this can be considered a proof or not. The intuition is correct (I am kind of positive about it), but I am not sure if I was actually able to write down my intuition in a good way. As always any feedback is more than welcome. Thank you!",,"['real-analysis', 'proof-verification', 'proof-writing', 'cantor-set']"
41,Why study metric spaces?,Why study metric spaces?,,"Most universities have a 3rd year undergraduate analysis course in which metric spaces are studied in depth (compactness, completeness, connectedness, etc...). However, in practice it seems that most of these metric spaces are normed vector spaces. Why not just cover normed vector spaces instead of metric spaces? Even if we lose some generality, normed vector spaces feel more natural and interesting, in my opinion, at least.","Most universities have a 3rd year undergraduate analysis course in which metric spaces are studied in depth (compactness, completeness, connectedness, etc...). However, in practice it seems that most of these metric spaces are normed vector spaces. Why not just cover normed vector spaces instead of metric spaces? Even if we lose some generality, normed vector spaces feel more natural and interesting, in my opinion, at least.",,"['real-analysis', 'metric-spaces', 'education']"
42,Asymptotic behaviour of a multiple integral on the unit hypercube,Asymptotic behaviour of a multiple integral on the unit hypercube,,"A few days ago I found an interesting limit on the ""problems blackboard"" of my University: $$\lim_{n\to +\infty}\int_{(0,1)^n}\frac{\sum_{j=1}^n x_j^2}{\sum_{j=1}^n x_j}d\mu = 1.$$ The correct claim, however, is: $$\lim_{n\to +\infty}\int_{(0,1)^n}\frac{\sum_{j=1}^n x_j^2}{\sum_{j=1}^n x_j}d\mu = \frac{2}{3}.$$ In fact, following @Tetis' approach we have: $$I_n = \int_{(0,1)^n}\frac{\sum_{j=1}^n y_j^2}{\sum_{j=1}^{n}y_j}d\mu = \int_{(-1/2,1/2)^n}\frac{\frac{1}{2}+\frac{2}{n}\sum_{j=1}^n x_j^2+\frac{2}{n}\sum_{j=1}^n x_j}{1+\frac{2}{n}\sum_{j=1}^n x_j}d\mu,$$ now setting $x_j=-z_j$ and summing the two integrals $$I_n = \int_{(-1/2,1/2)^n}\frac{\frac{1}{2}+\frac{2}{n}\sum_{j=1}^n x_j^2+\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}{1-\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}d\mu$$ follows, so: $$ I_n-\frac{2}{3}=\int_{(-1/2,1/2)^n} \left(-\frac{1}{2}+\frac{2}{n}\sum_{j=1}^n x_j^2\right)\frac{\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}{1-\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}d\mu < 0,$$ $$ \left| I_n-\frac{2}{3}\right|\leq\int_{\sum x_i^2\leq\frac{n}{4}}\left(\frac{1}{2}-\frac{2}{n}\sum_{j=1}^n x_j^2\right)\frac{\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}{1-\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}d\mu,$$ $$ \frac{2}{3}-I_n\leq \frac{n^{n/2}}{2^{n+1}} \int_{\sum x_i^2\leq 1}\left(1-\sum_{j=1}^{n}x_j^2\right)\frac{x_1^2}{1-x_1^2}d\mu.$$ The last bound, anyway, is too crude, since the RHS is $$ \Theta\left(\left(\sqrt{\frac{e\pi}{2}}\right)^n\frac{\log n}{n^{3/2}}\right).$$ My question now is: what is the asymptotic behaviour of $I_n$? A second one is: can we prove $I_n\geq\frac{2}{3}-\frac{C}{n}$, for a suitable positive constant $C$, without the Central Limit Theorem? UPDATE: After a few I came out with a proof of my own. The challenge is now to give the first three terms of the asymptotics, and possibly a continued fraction expansion for $I_n$.","A few days ago I found an interesting limit on the ""problems blackboard"" of my University: $$\lim_{n\to +\infty}\int_{(0,1)^n}\frac{\sum_{j=1}^n x_j^2}{\sum_{j=1}^n x_j}d\mu = 1.$$ The correct claim, however, is: $$\lim_{n\to +\infty}\int_{(0,1)^n}\frac{\sum_{j=1}^n x_j^2}{\sum_{j=1}^n x_j}d\mu = \frac{2}{3}.$$ In fact, following @Tetis' approach we have: $$I_n = \int_{(0,1)^n}\frac{\sum_{j=1}^n y_j^2}{\sum_{j=1}^{n}y_j}d\mu = \int_{(-1/2,1/2)^n}\frac{\frac{1}{2}+\frac{2}{n}\sum_{j=1}^n x_j^2+\frac{2}{n}\sum_{j=1}^n x_j}{1+\frac{2}{n}\sum_{j=1}^n x_j}d\mu,$$ now setting $x_j=-z_j$ and summing the two integrals $$I_n = \int_{(-1/2,1/2)^n}\frac{\frac{1}{2}+\frac{2}{n}\sum_{j=1}^n x_j^2+\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}{1-\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}d\mu$$ follows, so: $$ I_n-\frac{2}{3}=\int_{(-1/2,1/2)^n} \left(-\frac{1}{2}+\frac{2}{n}\sum_{j=1}^n x_j^2\right)\frac{\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}{1-\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}d\mu < 0,$$ $$ \left| I_n-\frac{2}{3}\right|\leq\int_{\sum x_i^2\leq\frac{n}{4}}\left(\frac{1}{2}-\frac{2}{n}\sum_{j=1}^n x_j^2\right)\frac{\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}{1-\frac{4}{n^2}\left(\sum_{j=1}^n x_j\right)^2}d\mu,$$ $$ \frac{2}{3}-I_n\leq \frac{n^{n/2}}{2^{n+1}} \int_{\sum x_i^2\leq 1}\left(1-\sum_{j=1}^{n}x_j^2\right)\frac{x_1^2}{1-x_1^2}d\mu.$$ The last bound, anyway, is too crude, since the RHS is $$ \Theta\left(\left(\sqrt{\frac{e\pi}{2}}\right)^n\frac{\log n}{n^{3/2}}\right).$$ My question now is: what is the asymptotic behaviour of $I_n$? A second one is: can we prove $I_n\geq\frac{2}{3}-\frac{C}{n}$, for a suitable positive constant $C$, without the Central Limit Theorem? UPDATE: After a few I came out with a proof of my own. The challenge is now to give the first three terms of the asymptotics, and possibly a continued fraction expansion for $I_n$.",,"['real-analysis', 'inequality', 'asymptotics', 'integral-inequality']"
43,"Evaluating $\int_1^2\frac{\arctan(x+1)}{x}\,dx$",Evaluating,"\int_1^2\frac{\arctan(x+1)}{x}\,dx","Evaluate the following integral $$\int_1^2\frac{\arctan(x+1)}{x}\,dx$$ with $0\leq\arctan(x)<\pi/2$ for $0\leq x<\infty.$ I proceeded the following way $$\begin{aligned} &\int_1^2\frac{\arctan(x+1)}{x}\,dx\to {\small{\begin{bmatrix}&u=x+1&\\&du=dx&\end{bmatrix}}} \to\int_2^3\frac{\arctan(u)}{u-1}\,du=\\ &\ln(2)\arctan(3)-\int_2^3\frac{\ln(u-1)}{u^2+1}\,du\to {\small{\begin{bmatrix}&u=\tan(\theta)&\\&du=\sec^2(\theta)d\theta&\end{bmatrix}}} \to\\ &\ln(2)\arctan(3)-\int_\alpha^\beta\ln\left(\tan(\theta)-1\right)\,d\theta=\ln(2)\arctan(3)-\int_\alpha^\beta\ln\left(\sin(\theta)-\cos(\theta)\right)\,d\theta+\\ &+\int_\alpha^\beta\ln\left(\cos(\theta)\right)\,d\theta. \end{aligned}$$ But $$\int_\alpha^\beta\ln\left(\sin(\theta)-\cos(\theta)\right)\,d\theta\to {\small{\begin{bmatrix}&\theta=s+3\pi/4&\\&d\theta=ds&\end{bmatrix}}} \to\int_{\alpha-3\pi/4}^{\beta-3\pi/4}\ln\left(\sqrt2\cos(s)\right)\,ds$$ so $$\begin{aligned}\int_1^2\frac{\arctan(x+1)}{x}\,dx&=\ln(2)\arctan(3)+\ln(\sqrt{2})(\alpha-\beta)\\ &\phantom{aaaaa}-\int_{\alpha-3\pi/4}^{\beta-3\pi/4}\ln\left(\cos(s)\right)\,ds+\int_\alpha^\beta\ln\left(\cos(s)\right)\,ds. \end{aligned}$$ Here $\alpha=\arctan(2)$ and $\beta=\arctan(3)$ . The problem here is that I am not able to find a way to cancel the last two integrals on the RHS of the latter equality. ADDENDUM Using Mathematica 11.3 I found that the answers is $\frac{3}{8} \pi  \ln(2)\approx0.81659478386385079894.$ In my equality, if we assume the integrals that involve cosines cancel, we have that the result of the integral is $\frac{1}{2} \ln (2) \left(\arctan(2)-\arctan(3)\right)+\ln (2) \arctan(3)\approx 0.81659478386385079894$ . Which are exactly equal up to $20$ decimal places! How would I go about canceling the integrals involving cosines (if they actually do cancel)?","Evaluate the following integral with for I proceeded the following way But so Here and . The problem here is that I am not able to find a way to cancel the last two integrals on the RHS of the latter equality. ADDENDUM Using Mathematica 11.3 I found that the answers is In my equality, if we assume the integrals that involve cosines cancel, we have that the result of the integral is . Which are exactly equal up to decimal places! How would I go about canceling the integrals involving cosines (if they actually do cancel)?","\int_1^2\frac{\arctan(x+1)}{x}\,dx 0\leq\arctan(x)<\pi/2 0\leq x<\infty. \begin{aligned}
&\int_1^2\frac{\arctan(x+1)}{x}\,dx\to {\small{\begin{bmatrix}&u=x+1&\\&du=dx&\end{bmatrix}}}
\to\int_2^3\frac{\arctan(u)}{u-1}\,du=\\
&\ln(2)\arctan(3)-\int_2^3\frac{\ln(u-1)}{u^2+1}\,du\to {\small{\begin{bmatrix}&u=\tan(\theta)&\\&du=\sec^2(\theta)d\theta&\end{bmatrix}}}
\to\\
&\ln(2)\arctan(3)-\int_\alpha^\beta\ln\left(\tan(\theta)-1\right)\,d\theta=\ln(2)\arctan(3)-\int_\alpha^\beta\ln\left(\sin(\theta)-\cos(\theta)\right)\,d\theta+\\
&+\int_\alpha^\beta\ln\left(\cos(\theta)\right)\,d\theta.
\end{aligned} \int_\alpha^\beta\ln\left(\sin(\theta)-\cos(\theta)\right)\,d\theta\to {\small{\begin{bmatrix}&\theta=s+3\pi/4&\\&d\theta=ds&\end{bmatrix}}}
\to\int_{\alpha-3\pi/4}^{\beta-3\pi/4}\ln\left(\sqrt2\cos(s)\right)\,ds \begin{aligned}\int_1^2\frac{\arctan(x+1)}{x}\,dx&=\ln(2)\arctan(3)+\ln(\sqrt{2})(\alpha-\beta)\\
&\phantom{aaaaa}-\int_{\alpha-3\pi/4}^{\beta-3\pi/4}\ln\left(\cos(s)\right)\,ds+\int_\alpha^\beta\ln\left(\cos(s)\right)\,ds.
\end{aligned} \alpha=\arctan(2) \beta=\arctan(3) \frac{3}{8} \pi  \ln(2)\approx0.81659478386385079894. \frac{1}{2} \ln (2) \left(\arctan(2)-\arctan(3)\right)+\ln (2) \arctan(3)\approx 0.81659478386385079894 20","['real-analysis', 'integration', 'trigonometry', 'definite-integrals', 'logarithms']"
44,Prove that $\sin n\theta=n\sin \theta-\frac{n(n^2-1)}{3!}\sin^3\theta+\frac{n(n^2-1)(n^2-3^2)}{5!}\sin^5\theta+\cdots$,Prove that,\sin n\theta=n\sin \theta-\frac{n(n^2-1)}{3!}\sin^3\theta+\frac{n(n^2-1)(n^2-3^2)}{5!}\sin^5\theta+\cdots,"Prove that    $$\sin n\theta=n\sin \theta-\frac{n(n^2-1)}{3!}\sin^3\theta+\frac{n(n^2-1)(n^2-3^2)}{5!}\sin^5\theta+-\cdots$$ If I am not mistaken, this identity was either proven by Newton or known to him, so if possible I would really like to see the way he approached it, though any solution will suffice. My brief efforts involved induction on $n$ which failed since I ended up with having to manipulate $\sin( n+1)\theta=\sin( n\theta +\theta)=\sin n\theta \cos\theta+\cos n\theta\sin\theta $, which involves $\cos n\theta$. I tried the ""familiar"" method of expansion of $$\sin n\theta=n\theta-\frac{(n\theta)^3}{3!}+\frac{(n\theta)^5}{5!}-+\cdots$$ but this only made it more complicated$$\sin n\theta=n\theta-\frac{(n\theta)^3}{3!}+\frac{(n\theta)^5}{5!}-+\cdots=\\n\Big(\theta-\frac{\theta^3}{3!}+\frac{\theta^5}{5!}-+\cdots\Big)-\frac{n(n^2-1)}{3!}\Big(\theta-\frac{\theta^3}{3!}+\frac{\theta^5}{5!}-+\cdots\Big)^3+-\cdots$$ Another attempt would be to use the identity $$\sin n\theta=\frac{1}{2i}(e^{in\theta}-e^{-in\theta})$$ though this was obviously not known to Newton. In any case, any thougths, ideas are welcome.. EDIT A thorough answer has been provided below, but since it  involves the use of complex numbers, I deem that the search for another answer, one based solely on the mathematical knowledge up to Newton's time is open. After a bit more research, it appears that Newton came up with the formula after reading a book by Vieta, but I have been unable to gather further info on whether the formula was known to Vieta as well.","Prove that    $$\sin n\theta=n\sin \theta-\frac{n(n^2-1)}{3!}\sin^3\theta+\frac{n(n^2-1)(n^2-3^2)}{5!}\sin^5\theta+-\cdots$$ If I am not mistaken, this identity was either proven by Newton or known to him, so if possible I would really like to see the way he approached it, though any solution will suffice. My brief efforts involved induction on $n$ which failed since I ended up with having to manipulate $\sin( n+1)\theta=\sin( n\theta +\theta)=\sin n\theta \cos\theta+\cos n\theta\sin\theta $, which involves $\cos n\theta$. I tried the ""familiar"" method of expansion of $$\sin n\theta=n\theta-\frac{(n\theta)^3}{3!}+\frac{(n\theta)^5}{5!}-+\cdots$$ but this only made it more complicated$$\sin n\theta=n\theta-\frac{(n\theta)^3}{3!}+\frac{(n\theta)^5}{5!}-+\cdots=\\n\Big(\theta-\frac{\theta^3}{3!}+\frac{\theta^5}{5!}-+\cdots\Big)-\frac{n(n^2-1)}{3!}\Big(\theta-\frac{\theta^3}{3!}+\frac{\theta^5}{5!}-+\cdots\Big)^3+-\cdots$$ Another attempt would be to use the identity $$\sin n\theta=\frac{1}{2i}(e^{in\theta}-e^{-in\theta})$$ though this was obviously not known to Newton. In any case, any thougths, ideas are welcome.. EDIT A thorough answer has been provided below, but since it  involves the use of complex numbers, I deem that the search for another answer, one based solely on the mathematical knowledge up to Newton's time is open. After a bit more research, it appears that Newton came up with the formula after reading a book by Vieta, but I have been unable to gather further info on whether the formula was known to Vieta as well.",,"['calculus', 'real-analysis', 'sequences-and-series', 'trigonometry', 'reference-request']"
45,Closed form of $\int_0^{\infty} \frac{\log(x)}{\cosh(x) \sec(x)- \tan(x)} \ dx$,Closed form of,\int_0^{\infty} \frac{\log(x)}{\cosh(x) \sec(x)- \tan(x)} \ dx,What real analysis tools would you recommend me for getting the closed form of the integral below? $$\int_0^{\infty} \frac{\log(x)}{\cosh(x) \sec(x)- \tan(x)} \ dx$$,What real analysis tools would you recommend me for getting the closed form of the integral below? $$\int_0^{\infty} \frac{\log(x)}{\cosh(x) \sec(x)- \tan(x)} \ dx$$,,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
46,Proof that $\mathbb{Q}$ is dense in $\mathbb{R}$,Proof that  is dense in,\mathbb{Q} \mathbb{R},"I'm looking at a proof that $\mathbb{Q}$ is dense in $\mathbb{R}$, using only the Archimedean Property of $\mathbb{R}$ and basic properties of ordered fields. One step asserts that for any $n \in \mathbb{N}$, $x \in \mathbb{R}$, there is an integer $m$ such that $m - 1 \leq nx < m$.  Why is this true?  (Ideally, this fact can be shown using only the Archimedean property of $\mathbb{R}$ and basic properties of ordered fields...)","I'm looking at a proof that $\mathbb{Q}$ is dense in $\mathbb{R}$, using only the Archimedean Property of $\mathbb{R}$ and basic properties of ordered fields. One step asserts that for any $n \in \mathbb{N}$, $x \in \mathbb{R}$, there is an integer $m$ such that $m - 1 \leq nx < m$.  Why is this true?  (Ideally, this fact can be shown using only the Archimedean property of $\mathbb{R}$ and basic properties of ordered fields...)",,['real-analysis']
47,Can the plane be covered by open disjoint one dimensional intervals?,Can the plane be covered by open disjoint one dimensional intervals?,,"I remember I heard this question a while ago but have never figured it out. Let an open interval in $\mathbb{R}$ of the form $(a,b)$ . Now, imaging placing it in $\mathbb{R}^2$ , anywhere we want while keeping its length but perhaps rotating it if we want. Essentially this creates a line segment without its start/endpoints parametrized by $P(1-t)+Qt$ for $0<t<1$ where $P$ and $Q$ are its start and end points and with length $||P-Q||=b-a$ . As an example, if we take the unit interval $(0,1)$ , we can place it as it is in $\mathbb{R}^2$ as $(0,1)\times \{0\}$ or we can rotate it as $\{1\}\times (0,1)$ etc. I hope it is clear what I mean. My question is the following. Can we cover the entire $\mathbb{R}^2$ with disjoint such ""open intervals"" or if you prefer, line segments of finite length without their start/endpoints? I don't remember if the initial question I read was using only unit intervals, or of any length $(a,b)$ and I don't know if that plays a role in the answer but as a gut feeling, I doubt it does. My impression is that the answer is negative and we can't cover the plane in such a manner, or if it is possible it would use some weird abstract construction perhaps involving AC. Any help is appreciated! Edit: It seems that I could have phrased my wording a bit better although that was my last paragraph in the post above that I wasn't certain if it mattered whether we need to have fixed length intervals or not. Apparently it does, it is impossible with using only fixed length intervals but it is possible if we allow intervals of different lengths. Proof of impossibility in one case given in a paper given in the answers, and a construction in the other case is given by some answers and also found in that paper itself. Thanks everyone!","I remember I heard this question a while ago but have never figured it out. Let an open interval in of the form . Now, imaging placing it in , anywhere we want while keeping its length but perhaps rotating it if we want. Essentially this creates a line segment without its start/endpoints parametrized by for where and are its start and end points and with length . As an example, if we take the unit interval , we can place it as it is in as or we can rotate it as etc. I hope it is clear what I mean. My question is the following. Can we cover the entire with disjoint such ""open intervals"" or if you prefer, line segments of finite length without their start/endpoints? I don't remember if the initial question I read was using only unit intervals, or of any length and I don't know if that plays a role in the answer but as a gut feeling, I doubt it does. My impression is that the answer is negative and we can't cover the plane in such a manner, or if it is possible it would use some weird abstract construction perhaps involving AC. Any help is appreciated! Edit: It seems that I could have phrased my wording a bit better although that was my last paragraph in the post above that I wasn't certain if it mattered whether we need to have fixed length intervals or not. Apparently it does, it is impossible with using only fixed length intervals but it is possible if we allow intervals of different lengths. Proof of impossibility in one case given in a paper given in the answers, and a construction in the other case is given by some answers and also found in that paper itself. Thanks everyone!","\mathbb{R} (a,b) \mathbb{R}^2 P(1-t)+Qt 0<t<1 P Q ||P-Q||=b-a (0,1) \mathbb{R}^2 (0,1)\times \{0\} \{1\}\times (0,1) \mathbb{R}^2 (a,b)","['real-analysis', 'analysis']"
48,$f$ such that $\sum a_n$ converges $\implies \sum f(a_n)$ converges,such that  converges  converges,f \sum a_n \implies \sum f(a_n),"Let $f:\mathbb R\to \mathbb R$ be such that for every sequence $(a_n)$ of real numbers, $\sum a_n$ converges $\implies \sum f(a_n)$ converges Prove there is some open neighborhood $V$ , such that $0\in V$ and $f$ restricted to $V$ is a linear function. This was given to a friend at his oral exam this week. He told me the problem is hard. I tried it myself, but I haven't made any significant progress.","Let be such that for every sequence of real numbers, converges converges Prove there is some open neighborhood , such that and restricted to is a linear function. This was given to a friend at his oral exam this week. He told me the problem is hard. I tried it myself, but I haven't made any significant progress.",f:\mathbb R\to \mathbb R (a_n) \sum a_n \implies \sum f(a_n) V 0\in V f V,"['real-analysis', 'sequences-and-series']"
49,Is the Dirac delta a function?,Is the Dirac delta a function?,,Is Dirac delta a function? What is its contribution to analysis? What I know about it: It is infinite at 0 and 0 everywhere else. Its integration is 1 and I know how does it come.,Is Dirac delta a function? What is its contribution to analysis? What I know about it: It is infinite at 0 and 0 everywhere else. Its integration is 1 and I know how does it come.,,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'distribution-theory']"
50,A function with countable discontinuities is Borel measurable.,A function with countable discontinuities is Borel measurable.,,"Let $f:[a,b] \to \mathbb{R}$ be bounded with countable discontinuities. Show that $f$ is Borel-measurable. One solution uses the fact that A function on a compact interval [a, b] is Riemann integrable if and only if it is bounded and continuous almost everywhere. But is it possible to solve this problem similarly to finite discontinuity cases? In other words, $\{f>t\}$ can be decomposed something like $\bigcup_n \{f_n>t\}\cup\{\text{discontinuous points}\}$? If the discontinuites were finite, then just I can order them  so it was possible. But I don't know similar method is possible in countable cases. (It will be good then we don't need the condition $f$ is bounded.)","Let $f:[a,b] \to \mathbb{R}$ be bounded with countable discontinuities. Show that $f$ is Borel-measurable. One solution uses the fact that A function on a compact interval [a, b] is Riemann integrable if and only if it is bounded and continuous almost everywhere. But is it possible to solve this problem similarly to finite discontinuity cases? In other words, $\{f>t\}$ can be decomposed something like $\bigcup_n \{f_n>t\}\cup\{\text{discontinuous points}\}$? If the discontinuites were finite, then just I can order them  so it was possible. But I don't know similar method is possible in countable cases. (It will be good then we don't need the condition $f$ is bounded.)",,['real-analysis']
51,"Can a norm take infinite value? For example, $\|\cdot \|_1$?","Can a norm take infinite value? For example, ?",\|\cdot \|_1,"A definition for norm from Wikipedia says Given a vector space $V$ over a subfield $F$ of the complex numbers, a norm on $V$ is a function $p: V → \mathbb{R}$ with the following properties: For all $a ∈ F$ and all $u, v ∈ V$, $p(av) = |a| p(v)$, (positive homogeneity or positive scalability). $p(u + v) ≤ p(u) + p(v)$ (triangle inequality or subadditivity). If $p(v) = 0$ then $v$ is the zero vector (separates points). Can a norm take value $+\infty$? I think topology and convergence are what I had in mind. If we modify the definition of a norm to allow it take $\infty$,  in such a generalized norm space, does it induce a topology, so that we can talk about convergence relative to the generalized norm being equivalent to convergence relative the induced topology? My question comes from an example: can $\|\cdot \|_1$ be defined on all measurable functions which are allowed to have infinite integrals not just finite integrals? Thanks!","A definition for norm from Wikipedia says Given a vector space $V$ over a subfield $F$ of the complex numbers, a norm on $V$ is a function $p: V → \mathbb{R}$ with the following properties: For all $a ∈ F$ and all $u, v ∈ V$, $p(av) = |a| p(v)$, (positive homogeneity or positive scalability). $p(u + v) ≤ p(u) + p(v)$ (triangle inequality or subadditivity). If $p(v) = 0$ then $v$ is the zero vector (separates points). Can a norm take value $+\infty$? I think topology and convergence are what I had in mind. If we modify the definition of a norm to allow it take $\infty$,  in such a generalized norm space, does it induce a topology, so that we can talk about convergence relative to the generalized norm being equivalent to convergence relative the induced topology? My question comes from an example: can $\|\cdot \|_1$ be defined on all measurable functions which are allowed to have infinite integrals not just finite integrals? Thanks!",,"['real-analysis', 'normed-spaces']"
52,Why does the Continuum Hypothesis make an ideal measure on $\mathbb R$ impossible?,Why does the Continuum Hypothesis make an ideal measure on  impossible?,\mathbb R,"On the page 43 of Real Analysis by H.L. Royden (1st Edition) we read: ""(Ideally) we should like $m$ (the measure function) to have the following properties: $m(E)$ is defined for each subset $E$ of real numbers. For an interval $I$, $m(I) = l(I)$ (the length of $I$). If $\{E_n\}$ is a sequence of disjoint sets (for which $m$ is defined), $m(\bigcup E_n)= \sum m (E_n)$."" Then at the end of page 44 we read : ""If we assume the Continuum Hypothesis (that every non countable set of real numbers can be put in one to one correspondence with the set of all real numbers) then such a measure is impossible,"" and no more explanation was given. Now assuming the Continuum Hypothesis I am not able to see why such a measure is not possible.  Would you be kind enough to help me?","On the page 43 of Real Analysis by H.L. Royden (1st Edition) we read: ""(Ideally) we should like $m$ (the measure function) to have the following properties: $m(E)$ is defined for each subset $E$ of real numbers. For an interval $I$, $m(I) = l(I)$ (the length of $I$). If $\{E_n\}$ is a sequence of disjoint sets (for which $m$ is defined), $m(\bigcup E_n)= \sum m (E_n)$."" Then at the end of page 44 we read : ""If we assume the Continuum Hypothesis (that every non countable set of real numbers can be put in one to one correspondence with the set of all real numbers) then such a measure is impossible,"" and no more explanation was given. Now assuming the Continuum Hypothesis I am not able to see why such a measure is not possible.  Would you be kind enough to help me?",,"['real-analysis', 'measure-theory', 'examples-counterexamples']"
53,Close-form for integral $T(n)=\int_0^{\pi/2}\frac{1}{1+\sin^n(x)}dx$,Close-form for integral,T(n)=\int_0^{\pi/2}\frac{1}{1+\sin^n(x)}dx,"Context: I have recently become interested in integrals of the form $$T(n)=\int_0^{\pi/2}\frac{dx}{1+\sin(x)^n},$$ and I conjecture that $T(n)$ has a closed form evaluation for all $n\in\Bbb Z_{\ge0}$ , or at least all non-negative even integers $n$ . Trivially, one has $T(0)=\pi/4$ . Less trivially, there is $T(1)=1$ (easy with $t=\tan(x/2)$ ), and $T(2)=\frac\pi{2\sqrt{2}}$ as seen here . The integral $J=T(3)$ , however, is not so easy. We see that this integral is $$3J=\int_0^{\pi/2}\frac{dx}{1+\sin x}-\int_0^{\pi/2}\frac{\sin (x)dx}{\sin(x)^2-\sin(x)+1}+2\int_0^{\pi/2}\frac{dx}{\sin(x)^2-\sin(x)+1}.$$ The first integral is easy, and we get $$3J=1-J_1+J_2.$$ The next integral is $$J_1=\int_0^{\pi/2}\frac{\sin (x)dx}{\sin(x)^2-\sin(x)+1},$$ which is, from $t=\tan(x/2)$ , $$J_1=2\int_0^1\frac{\frac{2t}{1+t^2}}{(\frac{2t}{1+t^2})^2-\frac{2t}{1+t^2}+1}\frac{dt}{1+t^2},$$ which is the awful $$J_1=4\int_0^1\frac{tdt}{t^4-2t^3+6t^2-2t+1}.$$ I found that nothing but brute force could tackle this integral, so I used partial fractions and got $$J_1=4\sum_{a\in A}\frac{1}{f'(a)}\left(1+a\log(1-a)-a\log(-a)\right)$$ where $$A=\left\{\frac12\left(1+p_1i\sqrt3+p_2\sqrt{-6+2p_3i\sqrt3}\right):p_1,p_2,p_3\in\{-1,1\}\right\}$$ is the set of roots of the polynomial $f(z)=z^4-2z^3+6z^2-2z+1,$ and $\log(z)$ is the complex logarithm. However, the remaining integral is a little worse. We have from the substitution $t=\tan(x/2)$ the awful $$J_2=4\int_0^1\frac{1+t^2}{1-2t+5t^2-2t^3}dt.$$ We can do the same sort of trick here as with the last integral and get $$J_2=4\sum_{b\in B}\frac1{g'(b)}\int_0^1\frac{t^2+1}{t-b}dt$$ where $$B=\{z\in\Bbb C: 1-2z+5z^2-2z^3=0\}$$ is the set of roots of the polynomial $g(z)=1-2z+5z^2-2z^3$ . These roots do indeed have explicit evaluations . The integral in the summation is easy enough to calculate, but I'm not going to, as we already see that the integral has a closed form. Next up, Wolfram evaluates $$T(4)=\int_0^1\frac{dx}{1+\sin(x)^4}=\frac\pi4\sqrt{1+\sqrt2},$$ as well as $$T(6)=\int_0^{\pi/2}\frac{dx}{1+\sin(x)^6}=\frac{\pi}{12}(\sqrt{2}+2\sqrt{3}),$$ which is here . In fact, we may evaluate $T(2n)$ in terms of hypergeometric functions, which may have a general closed form. We do so by noting that $$\frac{1}{1+\sin(x)^{2n}}=\sum_{k\ge0}(-1)^k\sin(x)^{2nk}$$ so that $$T(2n)=\sum_{k\ge0}(-1)^k\int_0^{\pi/2}\sin(x)^{2nk}dx=\frac{\sqrt\pi}{2}\sum_{k\ge0}(-1)^k\frac{\Gamma(nk+\tfrac12)}{(nk)!}.$$ This is $$T(2n)=\frac\pi2\,_{n}F_{n-1}\left(1-\tfrac{1}{2n},A_n;B_n;-1\right)$$ where $$\begin{align} A_n&=\left\{\frac{2r+1}{2n}:0\le r\in\Bbb Z\le n-2\right\}\\ B_n&=\left\{\frac{r}{n}:1\le r\in\Bbb Z\le n-1\right\}. \end{align}$$ Whether or not this hypergeometric has a closed form I am unsure, but it looks simple enough to be evaluated exactly. Questions: Can $T(n)$ be computed in closed form of all $n$ ? If not, when can it be computed in closed form? And at the very least, what is $T(5)$ ? It seems to be very nasty. Thanks!","Context: I have recently become interested in integrals of the form and I conjecture that has a closed form evaluation for all , or at least all non-negative even integers . Trivially, one has . Less trivially, there is (easy with ), and as seen here . The integral , however, is not so easy. We see that this integral is The first integral is easy, and we get The next integral is which is, from , which is the awful I found that nothing but brute force could tackle this integral, so I used partial fractions and got where is the set of roots of the polynomial and is the complex logarithm. However, the remaining integral is a little worse. We have from the substitution the awful We can do the same sort of trick here as with the last integral and get where is the set of roots of the polynomial . These roots do indeed have explicit evaluations . The integral in the summation is easy enough to calculate, but I'm not going to, as we already see that the integral has a closed form. Next up, Wolfram evaluates as well as which is here . In fact, we may evaluate in terms of hypergeometric functions, which may have a general closed form. We do so by noting that so that This is where Whether or not this hypergeometric has a closed form I am unsure, but it looks simple enough to be evaluated exactly. Questions: Can be computed in closed form of all ? If not, when can it be computed in closed form? And at the very least, what is ? It seems to be very nasty. Thanks!","T(n)=\int_0^{\pi/2}\frac{dx}{1+\sin(x)^n}, T(n) n\in\Bbb Z_{\ge0} n T(0)=\pi/4 T(1)=1 t=\tan(x/2) T(2)=\frac\pi{2\sqrt{2}} J=T(3) 3J=\int_0^{\pi/2}\frac{dx}{1+\sin x}-\int_0^{\pi/2}\frac{\sin (x)dx}{\sin(x)^2-\sin(x)+1}+2\int_0^{\pi/2}\frac{dx}{\sin(x)^2-\sin(x)+1}. 3J=1-J_1+J_2. J_1=\int_0^{\pi/2}\frac{\sin (x)dx}{\sin(x)^2-\sin(x)+1}, t=\tan(x/2) J_1=2\int_0^1\frac{\frac{2t}{1+t^2}}{(\frac{2t}{1+t^2})^2-\frac{2t}{1+t^2}+1}\frac{dt}{1+t^2}, J_1=4\int_0^1\frac{tdt}{t^4-2t^3+6t^2-2t+1}. J_1=4\sum_{a\in A}\frac{1}{f'(a)}\left(1+a\log(1-a)-a\log(-a)\right) A=\left\{\frac12\left(1+p_1i\sqrt3+p_2\sqrt{-6+2p_3i\sqrt3}\right):p_1,p_2,p_3\in\{-1,1\}\right\} f(z)=z^4-2z^3+6z^2-2z+1, \log(z) t=\tan(x/2) J_2=4\int_0^1\frac{1+t^2}{1-2t+5t^2-2t^3}dt. J_2=4\sum_{b\in B}\frac1{g'(b)}\int_0^1\frac{t^2+1}{t-b}dt B=\{z\in\Bbb C: 1-2z+5z^2-2z^3=0\} g(z)=1-2z+5z^2-2z^3 T(4)=\int_0^1\frac{dx}{1+\sin(x)^4}=\frac\pi4\sqrt{1+\sqrt2}, T(6)=\int_0^{\pi/2}\frac{dx}{1+\sin(x)^6}=\frac{\pi}{12}(\sqrt{2}+2\sqrt{3}), T(2n) \frac{1}{1+\sin(x)^{2n}}=\sum_{k\ge0}(-1)^k\sin(x)^{2nk} T(2n)=\sum_{k\ge0}(-1)^k\int_0^{\pi/2}\sin(x)^{2nk}dx=\frac{\sqrt\pi}{2}\sum_{k\ge0}(-1)^k\frac{\Gamma(nk+\tfrac12)}{(nk)!}. T(2n)=\frac\pi2\,_{n}F_{n-1}\left(1-\tfrac{1}{2n},A_n;B_n;-1\right) \begin{align}
A_n&=\left\{\frac{2r+1}{2n}:0\le r\in\Bbb Z\le n-2\right\}\\
B_n&=\left\{\frac{r}{n}:1\le r\in\Bbb Z\le n-1\right\}.
\end{align} T(n) n T(5)","['real-analysis', 'integration', 'definite-integrals', 'closed-form', 'trigonometric-integrals']"
54,"Maximum of $\int_a^b \frac{f(x)}{x}\,\mathrm dx$",Maximum of,"\int_a^b \frac{f(x)}{x}\,\mathrm dx","Let $b>a>0$ and $M>0$ be fixed. Let $F$ be the set of all functions $f:[a,b]\to[-M,M]$ such that $$\int_a^bf(x)\,\mathrm dx=0.$$ Find $$\max_{f\in F}\int_a^b\frac{f(x)}x\,\mathrm dx.$$ I tried to use the CBS inequality for integrals for the functions $(f(x))^2$ and $\dfrac{1}{x^2}$ , but when I tried to find the equality case, the function $f(x)=\dfrac{k}{x}$ I got did not satisfy $\displaystyle\int_a^bf(x)\,\mathrm dx=0$ . So the maximum must be lower than what I found. But I do not know anything about the continuity of $f$ and I don't know how to continue.","Let and be fixed. Let be the set of all functions such that Find I tried to use the CBS inequality for integrals for the functions and , but when I tried to find the equality case, the function I got did not satisfy . So the maximum must be lower than what I found. But I do not know anything about the continuity of and I don't know how to continue.","b>a>0 M>0 F f:[a,b]\to[-M,M] \int_a^bf(x)\,\mathrm dx=0. \max_{f\in F}\int_a^b\frac{f(x)}x\,\mathrm dx. (f(x))^2 \dfrac{1}{x^2} f(x)=\dfrac{k}{x} \displaystyle\int_a^bf(x)\,\mathrm dx=0 f","['real-analysis', 'inequality', 'definite-integrals']"
55,"Convexity of $x\left(1+\frac1x\right)^x,\ x\ge 0$",Convexity of,"x\left(1+\frac1x\right)^x,\ x\ge 0","This may turn out to be really simple but I do not see a quick way to the proof. How would one show $\displaystyle x\Big(1+\frac1x\Big)^x,\ x\ge 0$ is convex? I derived the second derivative. It has a negative term. I suppose I could combine certain terms to make the negativeness disappear. But I hope there is a really clever way to see it right away.","This may turn out to be really simple but I do not see a quick way to the proof. How would one show $\displaystyle x\Big(1+\frac1x\Big)^x,\ x\ge 0$ is convex? I derived the second derivative. It has a negative term. I suppose I could combine certain terms to make the negativeness disappear. But I hope there is a really clever way to see it right away.",,"['calculus', 'real-analysis', 'inequality', 'convex-analysis', 'exponential-function']"
56,Does having a minimum along all lines imply that the function has a minimum?,Does having a minimum along all lines imply that the function has a minimum?,,"Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$, with $f \in C^k$, $k \geq 2$. Suppose that $f$ has a local minimum at the origin along all lines. That is, for all $(x, y) \in \mathbb{R}^2$, the function $g_{x, y}(t) = f(tx, ty)$ has a local minimum at $t=0$. Does it then follow that $f$ has a local minimum at the origin? I suppose I need to show that the Hessian of $f$ is positive definite, but I'm not sure how.","Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$, with $f \in C^k$, $k \geq 2$. Suppose that $f$ has a local minimum at the origin along all lines. That is, for all $(x, y) \in \mathbb{R}^2$, the function $g_{x, y}(t) = f(tx, ty)$ has a local minimum at $t=0$. Does it then follow that $f$ has a local minimum at the origin? I suppose I need to show that the Hessian of $f$ is positive definite, but I'm not sure how.",,['real-analysis']
57,"Prove that to each $\epsilon >0$, there exists a $\delta >0$ so that the Lebesgue integral...","Prove that to each , there exists a  so that the Lebesgue integral...",\epsilon >0 \delta >0,"Suppose $f$ is in $L^1$ space of $\mu$, where $\mu$ is the Lebesgue measure. Prove that to each $\epsilon >0$, there exists a $\delta >0$ so that the Lebesgue integral of the absolute value of $f$ is less than $\epsilon$ (over the set $E$) whenever the measure of $E$ is less than $\delta$. I know that the integral of $f$ is finite. So for arbitrary $f$ we can find the integral of $f$ to be less than some $\epsilon$, but how would I connect that with the measure of $E$ less than $\delta$. Any help would be welcome.","Suppose $f$ is in $L^1$ space of $\mu$, where $\mu$ is the Lebesgue measure. Prove that to each $\epsilon >0$, there exists a $\delta >0$ so that the Lebesgue integral of the absolute value of $f$ is less than $\epsilon$ (over the set $E$) whenever the measure of $E$ is less than $\delta$. I know that the integral of $f$ is finite. So for arbitrary $f$ we can find the integral of $f$ to be less than some $\epsilon$, but how would I connect that with the measure of $E$ less than $\delta$. Any help would be welcome.",,"['real-analysis', 'analysis']"
58,"Rationals of the form $\frac{p}{q}$ where $p,q$ are primes in $[a,b]$",Rationals of the form  where  are primes in,"\frac{p}{q} p,q [a,b]","Consider the closed interval $[0,1]$, there is $\frac{2}{3} \in [0,1]$ where $p=2$ and $q=3$. Similarly consider $[2,3]$, one can have $\frac{5}{2} \in [2,3]$ where $p=5$ and $q=2$. Does every interval of the form $[a,b]$, where $a,b \in \mathbb{R}$ contain a rational of this kind. If yes, how can we prove it?","Consider the closed interval $[0,1]$, there is $\frac{2}{3} \in [0,1]$ where $p=2$ and $q=3$. Similarly consider $[2,3]$, one can have $\frac{5}{2} \in [2,3]$ where $p=5$ and $q=2$. Does every interval of the form $[a,b]$, where $a,b \in \mathbb{R}$ contain a rational of this kind. If yes, how can we prove it?",,"['number-theory', 'real-analysis']"
59,Can a function be smooth at a single point?,Can a function be smooth at a single point?,,"I saw a thread ( Find a function smooth at one isolated point ) in which it is asked whether or not it is possible for a function to be smooth at a point, but not smooth on a deleted neighbourhood  of said point. The thread is closed with an accepted answer despite the answer seeming to be incorrect, so I would like to re-ask the question here. I have read (here: Example of a function continuous at only one point. ) that $$f=\begin{cases}x\,\mathrm{if}\,x\in\mathbb{Q}\\ 0\,\mathrm{if}\,x\in\mathbb{R}\setminus\mathbb{Q} \end{cases}$$ is only continuous at $x=0$ . If it is continuous at $x=0$ , then maybe $$f=\begin{cases}\sum_{k=1}^{\infty}x^k\,\mathrm{if}\,x\in\mathbb{Q}\\ 0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\mathrm{if}\,x\in\mathbb{R}\setminus\mathbb{Q} \end{cases}$$ is smooth at $x=0$ and nowhere else (does this work?).","I saw a thread ( Find a function smooth at one isolated point ) in which it is asked whether or not it is possible for a function to be smooth at a point, but not smooth on a deleted neighbourhood  of said point. The thread is closed with an accepted answer despite the answer seeming to be incorrect, so I would like to re-ask the question here. I have read (here: Example of a function continuous at only one point. ) that is only continuous at . If it is continuous at , then maybe is smooth at and nowhere else (does this work?).","f=\begin{cases}x\,\mathrm{if}\,x\in\mathbb{Q}\\
0\,\mathrm{if}\,x\in\mathbb{R}\setminus\mathbb{Q}
\end{cases} x=0 x=0 f=\begin{cases}\sum_{k=1}^{\infty}x^k\,\mathrm{if}\,x\in\mathbb{Q}\\
0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\mathrm{if}\,x\in\mathbb{R}\setminus\mathbb{Q}
\end{cases} x=0","['real-analysis', 'derivatives', 'smooth-functions']"
60,Proving that the tangent to a convex function is always below the function [duplicate],Proving that the tangent to a convex function is always below the function [duplicate],,"This question already has answers here : Tangent line of a convex function (4 answers) Closed 4 years ago . Consider a real-valued convex function $f$ defined on an open interval $(a,b) \subset \mathbb{R}$ . Let $x,y \in (a,b)$ .  I want to prove that \begin{equation} f((1-\lambda)x + \lambda y) \leq (1-\lambda)f(x) + \lambda f(y) , \lambda \in [0,1]\implies f(y) + f'(y)(x-y) \leq f(x) \end{equation} No other assumptions (regarding the second derivatives) is allowed.  While both facts are really obvious and clear, I am unable to prove this without making additional assumptions.","This question already has answers here : Tangent line of a convex function (4 answers) Closed 4 years ago . Consider a real-valued convex function defined on an open interval . Let .  I want to prove that No other assumptions (regarding the second derivatives) is allowed.  While both facts are really obvious and clear, I am unable to prove this without making additional assumptions.","f (a,b) \subset \mathbb{R} x,y \in (a,b) \begin{equation}
f((1-\lambda)x + \lambda y) \leq (1-\lambda)f(x) + \lambda f(y) , \lambda \in [0,1]\implies f(y) + f'(y)(x-y) \leq f(x)
\end{equation}","['real-analysis', 'functions', 'convex-analysis']"
61,Does $\sqrt{a+b} \le \sqrt a + \sqrt b$ hold for all positive real numbers a and b?,Does  hold for all positive real numbers a and b?,\sqrt{a+b} \le \sqrt a + \sqrt b,"I thought of this a while ago, but can't make up a proof or a counterexample. Does anyone know more about this? $$\sqrt{a+b}  \le \sqrt a + \sqrt b , \forall a,b \in \mathbb R_+$$ Moreover, what happens with more variables? Say: $$\sqrt{x_1+x_2+...+x_n} \le \sqrt x_1 +\sqrt x_2 + ... + \sqrt x_n $$ with $ x_i \in\mathbb R_+ \forall i \in \{1,2,...,n\} $ Or when $i = \mathbb N$ PS: I tried looking on the internet for this but I don't know how I would call this.","I thought of this a while ago, but can't make up a proof or a counterexample. Does anyone know more about this? $$\sqrt{a+b}  \le \sqrt a + \sqrt b , \forall a,b \in \mathbb R_+$$ Moreover, what happens with more variables? Say: $$\sqrt{x_1+x_2+...+x_n} \le \sqrt x_1 +\sqrt x_2 + ... + \sqrt x_n $$ with $ x_i \in\mathbb R_+ \forall i \in \{1,2,...,n\} $ Or when $i = \mathbb N$ PS: I tried looking on the internet for this but I don't know how I would call this.",,"['real-analysis', 'inequality']"
62,Supremum of the Difference of Two Functions,Supremum of the Difference of Two Functions,,"Given two real-valued functions $f$ and $g$, is it true that $\sup(f-g) \geq \sup(f) - \sup(g)$","Given two real-valued functions $f$ and $g$, is it true that $\sup(f-g) \geq \sup(f) - \sup(g)$",,['real-analysis']
63,Existence of a limit associated to an almost subadditive sequence,Existence of a limit associated to an almost subadditive sequence,,"I want to prove that a sequence lives in a specific interval; I did prove that lives in a bigger interval, but not in the one I want. Let $ a_n $ a sequence such that for any n,m $$a_n + a_m -1 < a_{n+m} < a_n + a_m +1  $$   Prove that the sequence  $ (a_n) / n $ converges ( to $L$)  and then prove that    $$ a_n  \in \left(nL - 1,nL+1 \right) $$ I proved everything, but I proved that lies in $$ \left( nL - 2,nL+2 \right) $$ I can´t bound it more.","I want to prove that a sequence lives in a specific interval; I did prove that lives in a bigger interval, but not in the one I want. Let $ a_n $ a sequence such that for any n,m $$a_n + a_m -1 < a_{n+m} < a_n + a_m +1  $$   Prove that the sequence  $ (a_n) / n $ converges ( to $L$)  and then prove that    $$ a_n  \in \left(nL - 1,nL+1 \right) $$ I proved everything, but I proved that lies in $$ \left( nL - 2,nL+2 \right) $$ I can´t bound it more.",,"['real-analysis', 'sequences-and-series']"
64,Incorrect reasoning during Taylor series derivation?,Incorrect reasoning during Taylor series derivation?,,"I want to derive the Taylor series approximation of a function $f(x)$ at a point $p$ using the following reasoning, but ""my"" Taylor series formula misses the inverse factorial scaling of individual terms. Why? Is my reasoning missing some steps? Are my assumptions incorrect? The goal is to build a local approximation of a function $f(x)$ at point $p$ using incremental changes of functions. Our first local approximation of $f(x)$ will be the constant function $f_{0}(x) = f(p)$ . This is a very crude approximation, which we seek to incrementally improve by incorporating our knowledge of the first derivative $f'(x)$ . The numerical value of the first derivative $f'(x)$ tells us by how much does the value of function $f$ change if we move one unit from the point $x$ in the direction of the $X$ axis (but this is just a local linear approximation, which is valid only near the point $x$ , not necessarily 1 unit away from the point $x$ ). We can therefore improve our first guess $f_{0}$ to $f_{1}(x) = f(p) + (x - p)f'(p)$ . The term $(x - p)f'(p)$ uses the local measure of change $f'(p)$ to construct a linear function $(x - p)f'(p)$ , which represents an incremental offset that improves the approximation provided by $f_{0}$ . $(x - p)f'(p)$ is only a linear function, so to further improve our approximation error $\vert f(x) - f_{1}(x)\vert$ , we can use higher-order derivative $f''(x)$ to construct another linear approximation of how does the function $f'(x)$ change: $f_{2}(x) = f(p) + (x - p) \left( f'(p) + (x - p) f''(p) \right) $ . In general, we can keep recursively improving the linear approximations by adding incremental correction terms for lower-order derivatives, e.g. adding the offset $(x - p) f^{(n)}(p)$ that will bring the value of $f^{(n-1)}(p)$ closer to the exact value $f^{(n-1)}(x)$ : $$ \begin{aligned} \Delta x &= x - p\\ f_{n}(x) &= f(p) + \underbrace{\Delta x \left( f'(p) + \underbrace{\Delta x \left(f''(p) + \underbrace{\Delta x \left(f'''(p) + \cdots\right)}_\text{incremental correction for $f''(p)$} \right)}_\text{incremental correction for $f'(p)$} \right)}_\text{incremental correction for $f(p)$}\\ f_{n}(x) &= f(p) + f'(p)\Delta x + f''(p)\Delta x^2 + f'''(p) \Delta x^3 + \cdots \end{aligned} $$ Yet the correct Taylor expansion looks like this: $$ f_{T}(x) = f(p) + \frac{f'(p)}{1!}\Delta x + \frac{f''(p)}{2!}\Delta x^2 + \frac{f'''(p)}{3!} \Delta x^3 + \cdots $$ Why is my reasoning incorrect? Why is ""my"" recursive formula not an optimal function approximation when compared to the equivalent Taylor expansion? And how can I change my reasoning to arrive at the ""correct"" Taylor polynomials that include inverse factorials? A side-question: seeking a clearer geometrical understanding, what does the term $\frac{1}{n!}$ scale? Does it scale the term $f^{(n)}(p)$ or does it scale $\Delta x^n$ ?","I want to derive the Taylor series approximation of a function at a point using the following reasoning, but ""my"" Taylor series formula misses the inverse factorial scaling of individual terms. Why? Is my reasoning missing some steps? Are my assumptions incorrect? The goal is to build a local approximation of a function at point using incremental changes of functions. Our first local approximation of will be the constant function . This is a very crude approximation, which we seek to incrementally improve by incorporating our knowledge of the first derivative . The numerical value of the first derivative tells us by how much does the value of function change if we move one unit from the point in the direction of the axis (but this is just a local linear approximation, which is valid only near the point , not necessarily 1 unit away from the point ). We can therefore improve our first guess to . The term uses the local measure of change to construct a linear function , which represents an incremental offset that improves the approximation provided by . is only a linear function, so to further improve our approximation error , we can use higher-order derivative to construct another linear approximation of how does the function change: . In general, we can keep recursively improving the linear approximations by adding incremental correction terms for lower-order derivatives, e.g. adding the offset that will bring the value of closer to the exact value : Yet the correct Taylor expansion looks like this: Why is my reasoning incorrect? Why is ""my"" recursive formula not an optimal function approximation when compared to the equivalent Taylor expansion? And how can I change my reasoning to arrive at the ""correct"" Taylor polynomials that include inverse factorials? A side-question: seeking a clearer geometrical understanding, what does the term scale? Does it scale the term or does it scale ?","f(x) p f(x) p f(x) f_{0}(x) = f(p) f'(x) f'(x) f x X x x f_{0} f_{1}(x) = f(p) + (x - p)f'(p) (x - p)f'(p) f'(p) (x - p)f'(p) f_{0} (x - p)f'(p) \vert f(x) - f_{1}(x)\vert f''(x) f'(x) f_{2}(x) = f(p) + (x - p) \left( f'(p) + (x - p) f''(p) \right)  (x - p) f^{(n)}(p) f^{(n-1)}(p) f^{(n-1)}(x) 
\begin{aligned}
\Delta x &= x - p\\
f_{n}(x) &= f(p) + \underbrace{\Delta x \left( f'(p) + \underbrace{\Delta x \left(f''(p) + \underbrace{\Delta x \left(f'''(p) + \cdots\right)}_\text{incremental correction for f''(p)} \right)}_\text{incremental correction for f'(p)} \right)}_\text{incremental correction for f(p)}\\
f_{n}(x) &= f(p) + f'(p)\Delta x + f''(p)\Delta x^2 + f'''(p) \Delta x^3 + \cdots
\end{aligned}
  f_{T}(x) = f(p) + \frac{f'(p)}{1!}\Delta x + \frac{f''(p)}{2!}\Delta x^2 + \frac{f'''(p)}{3!} \Delta x^3 + \cdots  \frac{1}{n!} f^{(n)}(p) \Delta x^n","['real-analysis', 'calculus', 'taylor-expansion', 'approximation']"
65,"Subgroups of $(\mathbb R, +)$ are either dense or cyclic.",Subgroups of  are either dense or cyclic.,"(\mathbb R, +)","I was trying to prove that any subgroup of $(\mathbb R, +)$ is either dense in $\mathbb R$ or is a cyclic subgroup of $(\mathbb R, +)$. Thanks in advance for any help.","I was trying to prove that any subgroup of $(\mathbb R, +)$ is either dense in $\mathbb R$ or is a cyclic subgroup of $(\mathbb R, +)$. Thanks in advance for any help.",,['real-analysis']
66,"$f(x)=ax+b$ for some $a,b\in\mathbb{Q}$ if $f(\mathbb{Q})\subset\mathbb{Q}$ and $f(\mathbb{R-Q})\subseteq \mathbb{R}-\mathbb{Q}$",for some  if  and,"f(x)=ax+b a,b\in\mathbb{Q} f(\mathbb{Q})\subset\mathbb{Q} f(\mathbb{R-Q})\subseteq \mathbb{R}-\mathbb{Q}","let $f$ be a polynomial function such that $f(\mathbb{Q})\subset \mathbb{Q}$ and $f(\mathbb{R}-\mathbb{Q}) \subset \mathbb{R}- \mathbb{Q}$ . Prove that $f(x)=ax+b$ for some $a,b\in\mathbb{Q}$ My approach : I started by defining $g(x)=f(x)-f(0)$ . Now $g(x)$ satisfies the properties $g(\mathbb{Q})\subset \mathbb{Q}$ ,  $g(\mathbb{R}-\mathbb{Q})\subset\mathbb{R}-\mathbb{Q}$ and $g(0)=0$ . $g$ is a polynomial and $g(0)=0$ . $0$ is root of $g$ and hence $g(x)=x h(x)$ for some polynomial $h(x)$ . It will be enough if one can prove that $h(x)$ is constant . I'm stuck here . Any form help will be highly appreciated.","let $f$ be a polynomial function such that $f(\mathbb{Q})\subset \mathbb{Q}$ and $f(\mathbb{R}-\mathbb{Q}) \subset \mathbb{R}- \mathbb{Q}$ . Prove that $f(x)=ax+b$ for some $a,b\in\mathbb{Q}$ My approach : I started by defining $g(x)=f(x)-f(0)$ . Now $g(x)$ satisfies the properties $g(\mathbb{Q})\subset \mathbb{Q}$ ,  $g(\mathbb{R}-\mathbb{Q})\subset\mathbb{R}-\mathbb{Q}$ and $g(0)=0$ . $g$ is a polynomial and $g(0)=0$ . $0$ is root of $g$ and hence $g(x)=x h(x)$ for some polynomial $h(x)$ . It will be enough if one can prove that $h(x)$ is constant . I'm stuck here . Any form help will be highly appreciated.",,"['real-analysis', 'general-topology', 'analysis']"
67,$e^{\left(\pi^{(e^\pi)}\right)}\;$ or $\;\pi^{\left(e^{(\pi^e)}\right)}$. Which one is greater than the other?,or . Which one is greater than the other?,e^{\left(\pi^{(e^\pi)}\right)}\; \;\pi^{\left(e^{(\pi^e)}\right)},$\newcommand{\bigxl}[1]{\mathopen{\displaystyle#1}} \newcommand{\bigxr}[1]{\mathclose{\displaystyle#1}} $ $$\large e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\quad\text{or}\quad\pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ Which one is greater? Effort. I know that $$e^\pi\ge \pi^e$$ Then $$\pi^{(e^\pi)}\ge e^{(\pi^e)}$$ But I can't say $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\le \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ or $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\ge \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$,$\newcommand{\bigxl}[1]{\mathopen{\displaystyle#1}} \newcommand{\bigxr}[1]{\mathclose{\displaystyle#1}} $ $$\large e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\quad\text{or}\quad\pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ Which one is greater? Effort. I know that $$e^\pi\ge \pi^e$$ Then $$\pi^{(e^\pi)}\ge e^{(\pi^e)}$$ But I can't say $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\le \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ or $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\ge \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$,,"['real-analysis', 'inequality', 'exponential-function', 'pi']"
68,Evaluate $ \int_{0}^{1} \log\left(\frac{x^2-2x-4}{x^2+2x-4}\right) \frac{\mathrm{d}x}{\sqrt{1-x^2}} $,Evaluate, \int_{0}^{1} \log\left(\frac{x^2-2x-4}{x^2+2x-4}\right) \frac{\mathrm{d}x}{\sqrt{1-x^2}} ,"Evaluate : $$ \int_{0}^{1} \log\left(\dfrac{x^2-2x-4}{x^2+2x-4}\right) \dfrac{\mathrm{d}x}{\sqrt{1-x^2}} $$ Introduction : I have a friend on another math platform who proposed a summation question and he has a good reputation of posting legitimate questions. I worked it out to another equivalent form i.e, the above integral. Here's my work :- We start with  $\displaystyle \sum_{n=0}^\infty L_{2n+1} x^n = \dfrac{x+1}{x^2-3x + 1}  $. Replacing $x$ with $x^2$ , we get $$ \sum_{n=0}^\infty L_{2n+1} x^{2n} = \dfrac{x^2+1}{x^4-3x^2+1} $$ Integrate: $$ \sum_{n=0}^\infty \dfrac{L_{2n+1} x^{2n+1}}{2n+1} = \underbrace{\int \dfrac{x^2+1}{x^4-3x^2+1} \,\mathrm{d}x}_{:= I} $$ Then, $\displaystyle I = \int \dfrac{ 1+(1/x^2)}{x^2 + 1/x^2 - 3} \, \mathrm{d}x $. Let $t = x - \dfrac1x  \Rightarrow \left( 1 + \dfrac1{x^2} \right) \, \mathrm{d}x = \mathrm{d}t $. Then $\displaystyle I = \int \dfrac{\mathrm{d}t}{t^2-1} = \dfrac12 \log \left | \dfrac{t-1}{t+1} \right | = \dfrac12 \log \left | \dfrac{x^2-x-1}{x^2+x-1} \right | $. $$ \begin{eqnarray} S & := & \sum_{n=0}^\infty \dfrac{ L_{2n+1}}{(2n+1)^2 \binom{2n}n } = \int_0^1 \sum_{n=0}^\infty \dfrac{ L_{2n+1}}{2n+1} (x-x^2)^n \, \mathrm{d}x  \qquad \left(\text{ Because }\dfrac1{(2n+1) \binom{2n}n} = \operatorname{B}(n+1,n+1) = \int_{0}^{1}  x^n(1-x)^n \mathrm{d}x\right) \\ &=& \int_0^1 \dfrac1{2\sqrt{x-x^2} } \log \left | \dfrac{x -x^2 - \sqrt{x-x^2} - 1}{x -x^2 + \sqrt{x-x^2} - 1} \right | \, \mathrm{d}x \\ &=& \int_0^1 f(x)\, \mathrm{d}x  \end{eqnarray} $$ Note that $f(1-x) = f(x) $, so $\displaystyle S =2  \int_0^{1/2} f(x) \, \mathrm{d}x = 2 \int_0^{1/2} f\left( \dfrac12 - x\right) \, \mathrm{d}x $, and so $$ S = \int_0^{1/2} \dfrac1{\sqrt{\frac14 - x^2}} \log \left |\dfrac{a^2-a-1}{a^2+a-1} \right| \, \mathrm{d}x $$ where $a = \sqrt{\dfrac14 - x^2} $. Substitute $x = \dfrac12 \cos (\theta) $ and simplify: $$ S = \int_0^{\pi/2} \log \left | \dfrac{ \cos^2 -2 \cos \theta - 4}{\cos^2 + 2\cos\theta - 4} \right | \, \mathrm{d}x = \int_0^1 \log \left ( \dfrac{x^2-2x-4}{x^2+2x-4} \right) \dfrac{\mathrm{d}x}{\sqrt{1-x^2}} \\ \vdots $$ Closed Form : Recently, the same question was posted on M.S.E. albeit in a different form by another friend of mine here . That integral is obtained from this by applying Integration By Parts. Mr. Jack D'Aurizio also gave a closed form in terms of Imaginary part of Dilogarithms, specifically, $$I = -2 \ \Im \left[\text{Li}_2\left[i\left(1-\sqrt{5}+\sqrt{5-2 \sqrt{5}}\right)\right]+\text{Li}_2\left[i\left(1+\sqrt{5}-\sqrt{5+2 \sqrt{5}}\right)\right] \right]$$ However, there is a more elementary closed form that exists for the question (as evident from the original question) in terms of natural logarithm and Catalan's constant. All solutions are greatly appreciated.","Evaluate : $$ \int_{0}^{1} \log\left(\dfrac{x^2-2x-4}{x^2+2x-4}\right) \dfrac{\mathrm{d}x}{\sqrt{1-x^2}} $$ Introduction : I have a friend on another math platform who proposed a summation question and he has a good reputation of posting legitimate questions. I worked it out to another equivalent form i.e, the above integral. Here's my work :- We start with  $\displaystyle \sum_{n=0}^\infty L_{2n+1} x^n = \dfrac{x+1}{x^2-3x + 1}  $. Replacing $x$ with $x^2$ , we get $$ \sum_{n=0}^\infty L_{2n+1} x^{2n} = \dfrac{x^2+1}{x^4-3x^2+1} $$ Integrate: $$ \sum_{n=0}^\infty \dfrac{L_{2n+1} x^{2n+1}}{2n+1} = \underbrace{\int \dfrac{x^2+1}{x^4-3x^2+1} \,\mathrm{d}x}_{:= I} $$ Then, $\displaystyle I = \int \dfrac{ 1+(1/x^2)}{x^2 + 1/x^2 - 3} \, \mathrm{d}x $. Let $t = x - \dfrac1x  \Rightarrow \left( 1 + \dfrac1{x^2} \right) \, \mathrm{d}x = \mathrm{d}t $. Then $\displaystyle I = \int \dfrac{\mathrm{d}t}{t^2-1} = \dfrac12 \log \left | \dfrac{t-1}{t+1} \right | = \dfrac12 \log \left | \dfrac{x^2-x-1}{x^2+x-1} \right | $. $$ \begin{eqnarray} S & := & \sum_{n=0}^\infty \dfrac{ L_{2n+1}}{(2n+1)^2 \binom{2n}n } = \int_0^1 \sum_{n=0}^\infty \dfrac{ L_{2n+1}}{2n+1} (x-x^2)^n \, \mathrm{d}x  \qquad \left(\text{ Because }\dfrac1{(2n+1) \binom{2n}n} = \operatorname{B}(n+1,n+1) = \int_{0}^{1}  x^n(1-x)^n \mathrm{d}x\right) \\ &=& \int_0^1 \dfrac1{2\sqrt{x-x^2} } \log \left | \dfrac{x -x^2 - \sqrt{x-x^2} - 1}{x -x^2 + \sqrt{x-x^2} - 1} \right | \, \mathrm{d}x \\ &=& \int_0^1 f(x)\, \mathrm{d}x  \end{eqnarray} $$ Note that $f(1-x) = f(x) $, so $\displaystyle S =2  \int_0^{1/2} f(x) \, \mathrm{d}x = 2 \int_0^{1/2} f\left( \dfrac12 - x\right) \, \mathrm{d}x $, and so $$ S = \int_0^{1/2} \dfrac1{\sqrt{\frac14 - x^2}} \log \left |\dfrac{a^2-a-1}{a^2+a-1} \right| \, \mathrm{d}x $$ where $a = \sqrt{\dfrac14 - x^2} $. Substitute $x = \dfrac12 \cos (\theta) $ and simplify: $$ S = \int_0^{\pi/2} \log \left | \dfrac{ \cos^2 -2 \cos \theta - 4}{\cos^2 + 2\cos\theta - 4} \right | \, \mathrm{d}x = \int_0^1 \log \left ( \dfrac{x^2-2x-4}{x^2+2x-4} \right) \dfrac{\mathrm{d}x}{\sqrt{1-x^2}} \\ \vdots $$ Closed Form : Recently, the same question was posted on M.S.E. albeit in a different form by another friend of mine here . That integral is obtained from this by applying Integration By Parts. Mr. Jack D'Aurizio also gave a closed form in terms of Imaginary part of Dilogarithms, specifically, $$I = -2 \ \Im \left[\text{Li}_2\left[i\left(1-\sqrt{5}+\sqrt{5-2 \sqrt{5}}\right)\right]+\text{Li}_2\left[i\left(1+\sqrt{5}-\sqrt{5+2 \sqrt{5}}\right)\right] \right]$$ However, there is a more elementary closed form that exists for the question (as evident from the original question) in terms of natural logarithm and Catalan's constant. All solutions are greatly appreciated.",,"['real-analysis', 'integration', 'sequences-and-series', 'definite-integrals', 'polylogarithm']"
69,Exotic bijection from $\mathbb R$ to $\mathbb R$,Exotic bijection from  to,\mathbb R \mathbb R,"Clearly there is no continuous bijections $f,g~:~\mathbb R \to \mathbb R$ such that $fg$ is a bijection from $\mathbb R$ to $\mathbb R$. If we omit the continuity assumption, is there such an example ? Notes: to follow from Dustan's comments: Notes:  By definition $fg~:~x \mapsto f(x)\times g(x)$ and not $f\circ g$. If there were continuous bijections just look at the limits of $f$ and $g$ at $+\infty$ and $-\infty$ to conclude that $fg$ can't be a bijection","Clearly there is no continuous bijections $f,g~:~\mathbb R \to \mathbb R$ such that $fg$ is a bijection from $\mathbb R$ to $\mathbb R$. If we omit the continuity assumption, is there such an example ? Notes: to follow from Dustan's comments: Notes:  By definition $fg~:~x \mapsto f(x)\times g(x)$ and not $f\circ g$. If there were continuous bijections just look at the limits of $f$ and $g$ at $+\infty$ and $-\infty$ to conclude that $fg$ can't be a bijection",,['real-analysis']
70,A limit coming from high school,A limit coming from high school,,"I'm sure you guys can do this in many ways, using integrals, Taylor series, but I need a high school way, like the use of a simple squeeze theorem. Can we get such a way? $$\lim_{n\to\infty}\left(1+\log\left(1+\frac{1}{n^2}\right)\right)\left(1+\log\left(1+\frac{2}{n^2}\right)\right)\cdots\left(1+\log\left(1+\frac{n}{n^2}\right)\right)$$ EDIT : The proof I need I plan to present to some kids, so everything should be clear for them.","I'm sure you guys can do this in many ways, using integrals, Taylor series, but I need a high school way, like the use of a simple squeeze theorem. Can we get such a way? $$\lim_{n\to\infty}\left(1+\log\left(1+\frac{1}{n^2}\right)\right)\left(1+\log\left(1+\frac{2}{n^2}\right)\right)\cdots\left(1+\log\left(1+\frac{n}{n^2}\right)\right)$$ EDIT : The proof I need I plan to present to some kids, so everything should be clear for them.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
71,Product of sets and supremum,Product of sets and supremum,,"Let $A$ and $B$ be nonempty sets of positive real numbers that are bounded above. Also let $AB = \{ab: a \in A, b \in B \}$. Prove that $AB$ is bounded above and $\sup(AB) = (\sup A) (\sup B)$. So $\sup A$ and $\sup B$ exist by completeness. An upper bound for $AB$ is $(\sup A)(\sup B)$. Let $\alpha = \sup A$ and $\beta = \sup B$. We want to show that if $c$ is an upper bound for $AB$ then $\alpha \beta \leq c$. For $a \in A$, $ab \leq c$ for all $b \in B$. So $c/b$ is an upper bound for $A$. Thus $\alpha \leq c/b$. It follows that $\alpha \beta \leq c$. Is this correct?","Let $A$ and $B$ be nonempty sets of positive real numbers that are bounded above. Also let $AB = \{ab: a \in A, b \in B \}$. Prove that $AB$ is bounded above and $\sup(AB) = (\sup A) (\sup B)$. So $\sup A$ and $\sup B$ exist by completeness. An upper bound for $AB$ is $(\sup A)(\sup B)$. Let $\alpha = \sup A$ and $\beta = \sup B$. We want to show that if $c$ is an upper bound for $AB$ then $\alpha \beta \leq c$. For $a \in A$, $ab \leq c$ for all $b \in B$. So $c/b$ is an upper bound for $A$. Thus $\alpha \leq c/b$. It follows that $\alpha \beta \leq c$. Is this correct?",,"['real-analysis', 'products', 'supremum-and-infimum']"
72,Does the Riemann integral come from a measure?,Does the Riemann integral come from a measure?,,"Can we approach the Riemann integral with measure theory? That is: can we find a measure $\mu$ defined on a $\sigma$-algebra of $\mathbb{R}$ such that a function is $\mu$-integrable if and only if it is Riemann integrable, and that the integral $\int f d\mu$ is equal to the corresponding Riemann integral. If so can we extend this to improper Riemann integrals? What about Riemann integration in $\mathbb{R}^n$?","Can we approach the Riemann integral with measure theory? That is: can we find a measure $\mu$ defined on a $\sigma$-algebra of $\mathbb{R}$ such that a function is $\mu$-integrable if and only if it is Riemann integrable, and that the integral $\int f d\mu$ is equal to the corresponding Riemann integral. If so can we extend this to improper Riemann integrals? What about Riemann integration in $\mathbb{R}^n$?",,"['real-analysis', 'measure-theory']"
73,Convergence in $L^p$ of $f_n$ implies convergence in $L^1$ of $|f_n|^p$ and $f_n^p$,Convergence in  of  implies convergence in  of  and,L^p f_n L^1 |f_n|^p f_n^p,"I have the following question about $L^p$ spaces: Suppose that $f,f_1,f_2,\ldots$ are functions in $L^p$ for some $p \geq 1$ and the sequence converges in $L^p$ to $f$ , i.e. $||f_n-f||_p \to 0$ . Does this imply that the sequence $|f_1|^p,|f_2|^p,\ldots$ converges in $L^1$ to $|f|^p$ ? Is it also true if we remove the absolute value? That is, the sequence $f_1^p,f_2^p,...$ converges to $f^p$ , in case $p$ doesn't create problem with power (for example $p=3$ ). I am using necessary and sufficient condition from the Vitali's convergence theorem and it seems to work. Do you have some advices or some references for this result? Many thanks","I have the following question about spaces: Suppose that are functions in for some and the sequence converges in to , i.e. . Does this imply that the sequence converges in to ? Is it also true if we remove the absolute value? That is, the sequence converges to , in case doesn't create problem with power (for example ). I am using necessary and sufficient condition from the Vitali's convergence theorem and it seems to work. Do you have some advices or some references for this result? Many thanks","L^p f,f_1,f_2,\ldots L^p p \geq 1 L^p f ||f_n-f||_p \to 0 |f_1|^p,|f_2|^p,\ldots L^1 |f|^p f_1^p,f_2^p,... f^p p p=3","['real-analysis', 'measure-theory']"
74,"If $f(x) + f'(x) + f''(x) \to A$ as $x \to \infty$, then show that $f(x) \to A$ as $x \to \infty$","If  as , then show that  as",f(x) + f'(x) + f''(x) \to A x \to \infty f(x) \to A x \to \infty,"This problem is an extension to the simpler problem which deals with $f(x) + f'(x) \to A$ as $x \to \infty$ (see problem 2 on my blog ). If $f$ is twice continuously differentiable in some interval $(a, \infty)$ and $f(x) + f'(x) + f''(x) \to A$ as $x \to \infty$ then show that $f(x) \to A$ as $x \to \infty$. However, the approach based on considering sign of $f'(x)$ for large $x$ (which applies to the simpler problem in the blog) does not seem to apply here. Any hints on this problem? I believe that a similar generalization concerning expression $\sum\limits_{k = 0}^{n}f^{(k)}(x) \to A$ is also true, but I don't have a clue to prove the general result.","This problem is an extension to the simpler problem which deals with $f(x) + f'(x) \to A$ as $x \to \infty$ (see problem 2 on my blog ). If $f$ is twice continuously differentiable in some interval $(a, \infty)$ and $f(x) + f'(x) + f''(x) \to A$ as $x \to \infty$ then show that $f(x) \to A$ as $x \to \infty$. However, the approach based on considering sign of $f'(x)$ for large $x$ (which applies to the simpler problem in the blog) does not seem to apply here. Any hints on this problem? I believe that a similar generalization concerning expression $\sum\limits_{k = 0}^{n}f^{(k)}(x) \to A$ is also true, but I don't have a clue to prove the general result.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'limits', 'convergence-divergence']"
75,What is a good upper bound $n^n(n-1)^{n-1}\ldots2^21^1$?,What is a good upper bound ?,n^n(n-1)^{n-1}\ldots2^21^1,"Given an integer $n \ge 1$, I'd like to have a not-very-loose upper bound for the integer $$u(n) := \Pi_{k=1}^n k^k =  n^n(n-1)^{(n-1)}\ldots2^21^1.$$ It's easy see that, $u(n) \le n^{n(n+1)/2}$, but this is not very interesting. Update We have $u(n) \le e^{\left(\frac{1}{2}n(n+1)\log\left(\frac{2n + 1}{3}\right)\right)}$, and we can't really do much better! Indeed, using Euler-Maclaurin , we have $ \log(u(n)) = \int_2^nx\log x dx = \frac{1}{4}n^2(2\log(n) - 1) - 2\log(2) + \frac{1}{4} + \text{error terms}$, which is comparable to the bound $\log(u(n)) \le \frac{1}{2}n(n+1)\log\left(\frac{2n + 1}{3}\right)$ in the accepted answer (see below). In particular, we can conclude that accepted answer's bound is tight!","Given an integer $n \ge 1$, I'd like to have a not-very-loose upper bound for the integer $$u(n) := \Pi_{k=1}^n k^k =  n^n(n-1)^{(n-1)}\ldots2^21^1.$$ It's easy see that, $u(n) \le n^{n(n+1)/2}$, but this is not very interesting. Update We have $u(n) \le e^{\left(\frac{1}{2}n(n+1)\log\left(\frac{2n + 1}{3}\right)\right)}$, and we can't really do much better! Indeed, using Euler-Maclaurin , we have $ \log(u(n)) = \int_2^nx\log x dx = \frac{1}{4}n^2(2\log(n) - 1) - 2\log(2) + \frac{1}{4} + \text{error terms}$, which is comparable to the bound $\log(u(n)) \le \frac{1}{2}n(n+1)\log\left(\frac{2n + 1}{3}\right)$ in the accepted answer (see below). In particular, we can conclude that accepted answer's bound is tight!",,"['real-analysis', 'combinatorics', 'elementary-number-theory', 'inequality', 'products']"
76,Discontinuities of the derivative of a differentiable function on closed interval,Discontinuities of the derivative of a differentiable function on closed interval,,"I have a question about the corollary to theorem 5.12 in Rudin's Principles of Mathematical Analysis (page 108): Suppose $f$ is a real differentiable function on $[a,b]$ and suppose $f'(a)< \lambda < f'(b)$ then there is a point $x \in (a,b)$ such that $f'(x) = \lambda$ Corollary : If $f$ is differentiable on $[a,b]$ then $f'$ cannot have any simple discontinuities on $[a,b]$ . Can someone help me to show how he uses the result in the ""main theorem"" in the corollary? (There are two cases of simple discontinuities $f(x+) = f(x-) \neq f(x)$ and $f(x +) \neq f(x-)$","I have a question about the corollary to theorem 5.12 in Rudin's Principles of Mathematical Analysis (page 108): Suppose is a real differentiable function on and suppose then there is a point such that Corollary : If is differentiable on then cannot have any simple discontinuities on . Can someone help me to show how he uses the result in the ""main theorem"" in the corollary? (There are two cases of simple discontinuities and","f [a,b] f'(a)< \lambda < f'(b) x \in (a,b) f'(x) = \lambda f [a,b] f' [a,b] f(x+) = f(x-) \neq f(x) f(x +) \neq f(x-)",['real-analysis']
77,Is there a bijection of the natural numbers which swaps $\frac{1}{n}$-summable subsets with $\frac{1}{\sqrt{n}}$-summable subsets?,Is there a bijection of the natural numbers which swaps -summable subsets with -summable subsets?,\frac{1}{n} \frac{1}{\sqrt{n}},"Let me start with a precise statement of the question.  For a subset $A\subseteq \mathbb{N}$ and a series of positive real numbers $\sum_{n=0}^\infty a_n$ , I'll use the notation $\sum_A a_n$ as a shorthand for $\sum_{n\in A} a_n$ . Is there a bijection $f:\mathbb{N}\rightarrow \mathbb{N}$ with the property that for every $A\subseteq \mathbb{N}$ , $\sum_A \frac{1}{n}$ converges iff $\sum_{f(A)} \frac{1}{\sqrt{n}}$ converges? Background : Fix a series of positive terms $\sum_{n=0}^\infty a_n$ .  Given a subset $A\subseteq \mathbb{N}$ , call it $a_n$ -small if $\sum_A a_n$ converges.  The following proposition is easy to prove: Proposition :  The set $\{A\subseteq \mathbb{N}: A\text{ is } a_n\text{-small}\}\cup \{\mathbb{N}\}$ is a topology of closed sets on $\mathbb{N}$ . I will use the notation $(\mathbb{N},a_n)$ to refer to this topology.  Then one can ask how topological properties of $(\mathbb{N},a_n)$ are related to series properties of $\sum a_n$ .  For example, I can show: Proposition :  The following are equivalent. $\sum a_n$ converges. $(\mathbb{N},a_n)$ is discrete. $(\mathbb{N},a_n)$ is disconnected. $(\mathbb{N},a_n)$ is Hausdorff And there are other nice things.  For example, $(\mathbb{N},a_n)$ is compact iff $(\mathbb{N},a_n)$ is cofinite iff $\liminf a_n > 0$ . With this language, my question can be reformulated as... Are $\left(\mathbb{N}, \frac{1}{n}\right)$ and $\left(\mathbb{N}, \frac{1}{\sqrt{n}}\right)$ homeomorphic? I have made very little progress on this.  Of course, the identity function $i:(\mathbb{N}, \frac{1}{\sqrt{n}})\rightarrow (\mathbb{N}, \frac{1}{n})$ is a continuous bijection, but the inverse map is not continuous.  Also, if there is such a bijection, there there is such a bijection with agrees with $i$ on any preassigned finite set. Edit I thought I add in a slightly suprising (to me, at least) example when things work out to be homeomorphic. Begin with a convergent series $\sum c_n$ . and divergent series $\sum a_n$ with $\lim a_n = 0$ .  Create a new series $b_n$ using all the terms of $c_n$ and $a_n$ (in whatever order you wish).  Then $(\mathbb{N}, a_n)$ and $(\mathbb{N}, b_n)$ are homeomorphic.  The idea is that since $\lim a_n = 0$ , there is a convergent infinite subseries $\sum_A a_n$ .  Then we use a bijection which with $A$ and $A\cup \{\text{indices of }c_n\}$ to ""squeeze"" the $c_n$ in without changing the topology.  Of course, I am glossing over many details, but I can include them if desired.","Let me start with a precise statement of the question.  For a subset and a series of positive real numbers , I'll use the notation as a shorthand for . Is there a bijection with the property that for every , converges iff converges? Background : Fix a series of positive terms .  Given a subset , call it -small if converges.  The following proposition is easy to prove: Proposition :  The set is a topology of closed sets on . I will use the notation to refer to this topology.  Then one can ask how topological properties of are related to series properties of .  For example, I can show: Proposition :  The following are equivalent. converges. is discrete. is disconnected. is Hausdorff And there are other nice things.  For example, is compact iff is cofinite iff . With this language, my question can be reformulated as... Are and homeomorphic? I have made very little progress on this.  Of course, the identity function is a continuous bijection, but the inverse map is not continuous.  Also, if there is such a bijection, there there is such a bijection with agrees with on any preassigned finite set. Edit I thought I add in a slightly suprising (to me, at least) example when things work out to be homeomorphic. Begin with a convergent series . and divergent series with .  Create a new series using all the terms of and (in whatever order you wish).  Then and are homeomorphic.  The idea is that since , there is a convergent infinite subseries .  Then we use a bijection which with and to ""squeeze"" the in without changing the topology.  Of course, I am glossing over many details, but I can include them if desired.","A\subseteq \mathbb{N} \sum_{n=0}^\infty a_n \sum_A a_n \sum_{n\in A} a_n f:\mathbb{N}\rightarrow \mathbb{N} A\subseteq \mathbb{N} \sum_A \frac{1}{n} \sum_{f(A)} \frac{1}{\sqrt{n}} \sum_{n=0}^\infty a_n A\subseteq \mathbb{N} a_n \sum_A a_n \{A\subseteq \mathbb{N}: A\text{ is } a_n\text{-small}\}\cup \{\mathbb{N}\} \mathbb{N} (\mathbb{N},a_n) (\mathbb{N},a_n) \sum a_n \sum a_n (\mathbb{N},a_n) (\mathbb{N},a_n) (\mathbb{N},a_n) (\mathbb{N},a_n) (\mathbb{N},a_n) \liminf a_n > 0 \left(\mathbb{N}, \frac{1}{n}\right) \left(\mathbb{N}, \frac{1}{\sqrt{n}}\right) i:(\mathbb{N}, \frac{1}{\sqrt{n}})\rightarrow (\mathbb{N}, \frac{1}{n}) i \sum c_n \sum a_n \lim a_n = 0 b_n c_n a_n (\mathbb{N}, a_n) (\mathbb{N}, b_n) \lim a_n = 0 \sum_A a_n A A\cup \{\text{indices of }c_n\} c_n","['real-analysis', 'sequences-and-series', 'general-topology']"
78,Product rule of weak derivatives,Product rule of weak derivatives,,"I am working on proving the following proposition: If $u,v\in {W^1(\Omega)}$ and $uv,uDv+vDu\in L^1_{\operatorname{loc}}(\Omega)$, then we have the product formula $$D(uv)=uDv+vDu.$$ The definition I use for weak derivative: A function $u\in L^1_{loc}(\Omega)$ has a $\alpha-$th weak derivative in $\Omega$ if there is a function $v\in L^1_{\operatorname{loc}}(\Omega)$ with: $$\int_{\Omega}uD^\alpha\phi=(-1)^{|\alpha|}\int_{\Omega}v\phi dx\quad \forall\,\phi\in C^\infty_0(\Omega)$$ $u\in W^1(\Omega)$ means it has first weak derivative. I searched on the internet. Most of them just proved the case of $u\in W^1(\Omega)$ and $v\in C^1(\Omega)$. This is the first step of the proving. I want to prove the general case by approximation. Namely, let $v_\epsilon$ denote the regularization (or mollification) of $v\in W^1(\Omega)$, then it is true $$D(uv_\epsilon)=v_\epsilon Du+uDv_\epsilon$$ or equivalently $$\int_{\Omega}uv_\epsilon D\phi=-\int_{\Omega}(v_\epsilon Du+uDv_\epsilon)\phi dx\quad \forall\,\phi\in C^\infty_0(\Omega)$$ I am trying to prove $$\int_{\Omega}uv_\epsilon D\phi\to \int_{\Omega}uvD\phi\text{ as }\epsilon\to 0$$ $$\int_{\Omega}(v_\epsilon Du+uDv_\epsilon)\phi dx\to \int_{\Omega}(v Du+uDv)\phi dx\text{ as }\epsilon\to 0$$ But I can not accomplish, even for the first limit. Can anyone give me some idea? Maybe I am heading a wrong direction. But I am quite convinced that the mollification is so good that this must be true. On the other hand, this formula is very elementary in Sobolev space. Probably you can prove this proposition by distribution theory(or generalized function), however, it sounds cheating for me somehow, because here our definition has noting with distribution.","I am working on proving the following proposition: If $u,v\in {W^1(\Omega)}$ and $uv,uDv+vDu\in L^1_{\operatorname{loc}}(\Omega)$, then we have the product formula $$D(uv)=uDv+vDu.$$ The definition I use for weak derivative: A function $u\in L^1_{loc}(\Omega)$ has a $\alpha-$th weak derivative in $\Omega$ if there is a function $v\in L^1_{\operatorname{loc}}(\Omega)$ with: $$\int_{\Omega}uD^\alpha\phi=(-1)^{|\alpha|}\int_{\Omega}v\phi dx\quad \forall\,\phi\in C^\infty_0(\Omega)$$ $u\in W^1(\Omega)$ means it has first weak derivative. I searched on the internet. Most of them just proved the case of $u\in W^1(\Omega)$ and $v\in C^1(\Omega)$. This is the first step of the proving. I want to prove the general case by approximation. Namely, let $v_\epsilon$ denote the regularization (or mollification) of $v\in W^1(\Omega)$, then it is true $$D(uv_\epsilon)=v_\epsilon Du+uDv_\epsilon$$ or equivalently $$\int_{\Omega}uv_\epsilon D\phi=-\int_{\Omega}(v_\epsilon Du+uDv_\epsilon)\phi dx\quad \forall\,\phi\in C^\infty_0(\Omega)$$ I am trying to prove $$\int_{\Omega}uv_\epsilon D\phi\to \int_{\Omega}uvD\phi\text{ as }\epsilon\to 0$$ $$\int_{\Omega}(v_\epsilon Du+uDv_\epsilon)\phi dx\to \int_{\Omega}(v Du+uDv)\phi dx\text{ as }\epsilon\to 0$$ But I can not accomplish, even for the first limit. Can anyone give me some idea? Maybe I am heading a wrong direction. But I am quite convinced that the mollification is so good that this must be true. On the other hand, this formula is very elementary in Sobolev space. Probably you can prove this proposition by distribution theory(or generalized function), however, it sounds cheating for me somehow, because here our definition has noting with distribution.",,"['real-analysis', 'partial-differential-equations', 'sobolev-spaces', 'weak-derivatives']"
79,Sobolev spaces fourier norm equivalence,Sobolev spaces fourier norm equivalence,,"I am reading about Sobolev spaces and I have a question regarding Sobolev spaces and the Fourier transform. So by defining the Fourier transform, $F(\cdot)$ as an isometry we get $||f||_{L^2}=||\widehat{f}||_{L^2}$ and $\widehat{D^\alpha f}=\xi^\alpha\widehat{f}$. So one can define the norm in $H^k$, $$||f||_{H^k}=\left(\int(1+|\xi|^2)^k|\widehat{f(\xi)}|^2d\xi\right)^{\frac{1}{2}}$$ then he goes on and says that the proof that this is an equivalent norm is an easy exercise and is ommited. So I tried to prove it but I got stuck very quickly. The proof boils down to having to show that the functions $$F(\xi)=(1+|\xi|^2)^k$$ and $$S(\xi)=\sum_{|\alpha|\le k}|\xi^\alpha|^2$$ with $\xi\in \mathbb{R}^n$ bound each other. The one directions is more or less clear since $S$ includes the terms of $F$, but I cannot see why the reverse is true.","I am reading about Sobolev spaces and I have a question regarding Sobolev spaces and the Fourier transform. So by defining the Fourier transform, $F(\cdot)$ as an isometry we get $||f||_{L^2}=||\widehat{f}||_{L^2}$ and $\widehat{D^\alpha f}=\xi^\alpha\widehat{f}$. So one can define the norm in $H^k$, $$||f||_{H^k}=\left(\int(1+|\xi|^2)^k|\widehat{f(\xi)}|^2d\xi\right)^{\frac{1}{2}}$$ then he goes on and says that the proof that this is an equivalent norm is an easy exercise and is ommited. So I tried to prove it but I got stuck very quickly. The proof boils down to having to show that the functions $$F(\xi)=(1+|\xi|^2)^k$$ and $$S(\xi)=\sum_{|\alpha|\le k}|\xi^\alpha|^2$$ with $\xi\in \mathbb{R}^n$ bound each other. The one directions is more or less clear since $S$ includes the terms of $F$, but I cannot see why the reverse is true.",,"['real-analysis', 'functional-analysis', 'sobolev-spaces']"
80,$e$ as the limit of a sequence,as the limit of a sequence,e,"One of the standard definitions of $e$ is as $$\lim_{n\rightarrow\infty}\left(1 + \frac{1}{n}\right)^n$$ But in all cases I've seen this limit, it is proven as a limit of the sequence $\Big\{\big(1 + \frac{1}{n}\big)^n\Big\}$, which seems to cover the limit for only $n$ as an integer. Now my question is whether this sequence limit is equivalent to a normal limit for which $n$ can be any real number. I can think of cases which this isn't generally true; $$\lim_{n\rightarrow\infty}\ \sin(n\pi)$$ comes readily to mind, for which the limit as a sequence is simply $0$ but as a general limit, it is undefined. The limit for $e$ is used exactly as if it were a normal limit, which leads me to believe it is equivalent. Are there conditions for which the limit of a sequence and the corresponding function are identical?","One of the standard definitions of $e$ is as $$\lim_{n\rightarrow\infty}\left(1 + \frac{1}{n}\right)^n$$ But in all cases I've seen this limit, it is proven as a limit of the sequence $\Big\{\big(1 + \frac{1}{n}\big)^n\Big\}$, which seems to cover the limit for only $n$ as an integer. Now my question is whether this sequence limit is equivalent to a normal limit for which $n$ can be any real number. I can think of cases which this isn't generally true; $$\lim_{n\rightarrow\infty}\ \sin(n\pi)$$ comes readily to mind, for which the limit as a sequence is simply $0$ but as a general limit, it is undefined. The limit for $e$ is used exactly as if it were a normal limit, which leads me to believe it is equivalent. Are there conditions for which the limit of a sequence and the corresponding function are identical?",,"['calculus', 'real-analysis']"
81,non-trivial result about integrals due Gromov,non-trivial result about integrals due Gromov,,"Gromov proved that if $$ f,g:\left[ {a,b} \right] \to R $$ are integrable functions, such that the function $$ t \to \frac{{f\left( t \right)}} {{g\left( t \right)}} $$ is also integrable, and decreasing.  Then the function $$ r \to \frac{{\int\limits_a^r {f\left( t \right)dt} }} {{\int\limits_a^r {g\left( t \right)dt} }} $$ is decreasing. I could not proved, and I could not find a proof )=","Gromov proved that if $$ f,g:\left[ {a,b} \right] \to R $$ are integrable functions, such that the function $$ t \to \frac{{f\left( t \right)}} {{g\left( t \right)}} $$ is also integrable, and decreasing.  Then the function $$ r \to \frac{{\int\limits_a^r {f\left( t \right)dt} }} {{\int\limits_a^r {g\left( t \right)dt} }} $$ is decreasing. I could not proved, and I could not find a proof )=",,"['calculus', 'real-analysis']"
82,Measure theory convention that $\infty \cdot 0 = 0$,Measure theory convention that,\infty \cdot 0 = 0,"In the preface of Terry Tao's notes on measure theory he states that in the extended real number setting we adopt the convention that $\infty \cdot 0 = 0 \cdot \infty = 0.$ He explains that it's a useful convention which makes it natural to define integration from below (e.g. integrals as supremums of nonnegative simple functions). He seems to be saying, ""Let's define it this way, because it makes our lives simpler."" Is more justification not needed? This 'fact' helped me during my midterm today in showing that $\mu(E \times \mathbb{R})=0$ for any measure-zero set $E$, but I felt sleazy using it. Any thoughts (philosophical or otherwise) on why or how we can do this and still have self respect as mathematicians?","In the preface of Terry Tao's notes on measure theory he states that in the extended real number setting we adopt the convention that $\infty \cdot 0 = 0 \cdot \infty = 0.$ He explains that it's a useful convention which makes it natural to define integration from below (e.g. integrals as supremums of nonnegative simple functions). He seems to be saying, ""Let's define it this way, because it makes our lives simpler."" Is more justification not needed? This 'fact' helped me during my midterm today in showing that $\mu(E \times \mathbb{R})=0$ for any measure-zero set $E$, but I felt sleazy using it. Any thoughts (philosophical or otherwise) on why or how we can do this and still have self respect as mathematicians?",,"['real-analysis', 'measure-theory']"
83,"Given $n+1$ points, bound the product of the distances from one of them","Given  points, bound the product of the distances from one of them",n+1,"We have $n+1$ real numbers $x_1,\cdots,x_{n+1}$ such that $-1\leq x_i\leq 1$ for all $1\leq i\leq n+1$ . I am wondering whether the following fact is true: There exists some $j$ such that $\prod_{\substack{i=1\\i\neq j}}^{n+1}\left |x_j-x_i\right |\leq \frac{n+1}{2^{n-1}}$ . Some base steps are easy. Assume that this holds for $n-1$ . Applying this inductive hypothesis to $\left\{x_i:i\in\mathbb{N}_{\leq n+1}\right\}\setminus\left\{x_j\right\}$ for each $j\in\mathbb{N}_{\leq n+1}$ , we conclude that for each $j\in\mathbb{N}_{\leq n+1}$ there exists $h\left(j\right)\in\mathbb{N}_{\leq n+1}\setminus\left\{j\right\}$ such that $\prod_{\substack{i=1\\i\neq j\\i\neq h\left(j\right)}}^{n+1}\left |x_{h\left(j\right)}-x_i\right |\leq\frac{n}{2^{n-2}}$ . Now, if for some $j\in\mathbb{N}_{\leq n+1}$ we have $\left |x_{h\left(j\right)}-x_j\right |\leq \frac{1}{2}$ , then $\prod_{\substack{i=1\\i\neq h\left(j\right)}}^{n+1}\left |x_{h\left(j\right)}-x_i\right |\leq \frac{1}{2}\cdot\frac{n}{2^{n-2}}\leq \frac{1+\frac{1}{n}}{2}\frac{n}{2^{n-2}}=\frac{n+1}{2^{n-1}}$ , which gives what we want. But what about the case in which $\left |x_{h\left(j\right)}-x_j\right |>\frac{1}{2}$ for all $j\in\mathbb{N}_{\leq n+1}$ ? Is there any other way?","We have real numbers such that for all . I am wondering whether the following fact is true: There exists some such that . Some base steps are easy. Assume that this holds for . Applying this inductive hypothesis to for each , we conclude that for each there exists such that . Now, if for some we have , then , which gives what we want. But what about the case in which for all ? Is there any other way?","n+1 x_1,\cdots,x_{n+1} -1\leq x_i\leq 1 1\leq i\leq n+1 j \prod_{\substack{i=1\\i\neq j}}^{n+1}\left |x_j-x_i\right |\leq \frac{n+1}{2^{n-1}} n-1 \left\{x_i:i\in\mathbb{N}_{\leq n+1}\right\}\setminus\left\{x_j\right\} j\in\mathbb{N}_{\leq n+1} j\in\mathbb{N}_{\leq n+1} h\left(j\right)\in\mathbb{N}_{\leq n+1}\setminus\left\{j\right\} \prod_{\substack{i=1\\i\neq j\\i\neq h\left(j\right)}}^{n+1}\left |x_{h\left(j\right)}-x_i\right |\leq\frac{n}{2^{n-2}} j\in\mathbb{N}_{\leq n+1} \left |x_{h\left(j\right)}-x_j\right |\leq \frac{1}{2} \prod_{\substack{i=1\\i\neq h\left(j\right)}}^{n+1}\left |x_{h\left(j\right)}-x_i\right |\leq \frac{1}{2}\cdot\frac{n}{2^{n-2}}\leq \frac{1+\frac{1}{n}}{2}\frac{n}{2^{n-2}}=\frac{n+1}{2^{n-1}} \left |x_{h\left(j\right)}-x_j\right |>\frac{1}{2} j\in\mathbb{N}_{\leq n+1}","['real-analysis', 'optimization', 'real-numbers', 'upper-lower-bounds']"
84,"If $f[A]$ and $A$ intersect for (most) $A \subset \mathbb{R}$, then how far is $f$ from the identity?","If  and  intersect for (most) , then how far is  from the identity?",f[A] A A \subset \mathbb{R} f,"$\textbf{Definition}$ : We say $f:\mathbb{R} \to \mathbb{R}$ is intersecting if for every nonempty $A \subset \mathbb{R}$ , $f[A] \cap A \neq \varnothing$ . There is only one intersecting function: the identity. The reason for this is that $f[\{x\}] \cap \{x\} \neq \varnothing$ forces $f(x)=x$ . If we impose restrictions on the subsets $A$ we consider (say, for instance, we rule out singletons), must $f$ still be the identity? Let's try this. We can first introduce some definitions. $\textbf{Definition:}$ For any cardinal $\ell$ , we say $f:\mathbb{R} \to \mathbb{R}$ is $\ell$ - intersecting if for every nonempty $A \subset \mathbb{R}$ with $|A| \geq \ell$ , $f[A] \cap A \neq \varnothing$ . $\textbf{Definition:}$ For any $f:\mathbb{R} \to \mathbb{R}$ , its deviation is the cardinality of the set $\{x \in \mathbb{R} : f(x) \neq x\}$ . I'm going to write down some results for the $2$ -intersecting case. Suppose $f$ has the property that for every $A \subset \mathbb{R}$ with $|A| \geq 2$ that $f[A] \cap A \neq \varnothing$ . There indeed exists a non-identity example here, one with deviation $3$ , in fact. Let $f(x) = x$ for $x \in \mathbb{R} \setminus \{1,2,3\}$ and let $f(1)=2, f(2)=3, f(3)=1$ . Is there a $2$ -intersecting $f$ with deviation $\geq 4$ ? No. The argument for this is combinatorial. Suppose an $f$ with this property existed, and let $A=\{x_1, x_2, x_3, x_4\}$ be a set of four elements with $f(x_i) \neq x_i$ for each $x_i \in A$ . We first note that $f$ restricted to $A$ defines a function $f_{A}:A \to A$ . To justify this claim, we can argue by contradiction. Suppose for some $x_i \in A$ that $f(x_i) = y \notin A$ . Let $x_j, x_k$ be distinct elements in $A$ , also both distinct from $x_i$ . Then since $f[\{x_i, x_j\}] \cap \{x_i, x_j\} \neq \varnothing$ , and $f[\{x_i, x_k\}] \cap \{x_i, x_k\} \neq \varnothing$ , it's easy to see this implies $f(x_k) = f(x_j) = x_i$ , which implies $f[\{x_k, x_j\}] \cap \{x_k, x_j\} = \varnothing$ , contradiction. We also note the restricted function $f_{A}$ is injective. To see this, suppose, say, $f(x_i) = x_{m}$ and $f(x_j) = x_{m}$ , and each of $i,j, m$ are distinct. Then $f[\{x_i, x_j\}] \cap \{x_i, x_j\} = \varnothing$ , contradiction. Since $A$ is finite, this implies it's bijective too. Hence $f_{A}$ corresponds to a permutation in $S_4$ without fixed points. It cannot have two cycles, since if $x_j, x_i$ are in distinct two-cycles, then $f[\{x_i, x_j\}] \cap \{x_i, x_j\} = \varnothing$ . Hence it's cyclic. But if it's cyclic, then $f[\{x_{i_2}, x_{i_4}\}] \cap \{x_{i_2}, x_{i_4}\} = \varnothing$ , where $x_{i_2}, x_{i_4}$ are respectively the second and fourth elements of the cycle. This establishes the contradiction. What eludes me is the general case. The combinatorics seem to get harder if $\ell \geq 3$ . $\textbf{Problem 1:}$ The finite case. Let $\ell$ be a finite cardinal (i.e., a positive integer). Then is it true that any $\ell$ -intersecting map $f:\mathbb{R} \to \mathbb{R}$ has finite deviation? If this is true (which I suspect), is there a closed form for the largest possible deviation in terms of $\ell$ ? $\textbf{Problem 2:}$ The infinite case. What is the maximum deviation of an $\aleph_{0}$ -intersecting map $f:\mathbb{R} \to \mathbb{R}$ ? What is the maximum deviation of a $\mathfrak{c}$ -intersecting map $f:\mathbb{R} \to \mathbb{R}$ ? In particular, are there such functions with infinite deviation? You will notice that we don't really use any of the analytic or algebraic structure of $\mathbb{R}$ here, so really these notions can be generalized to functions $f:X \to Y$ for arbitrary sets $X,Y$ . We could, however, ask different kinds of questions similar to the ones described above which make use of the structure of $\mathbb{R}$ . Instead of considering subsets $A$ with sufficiently large cardinality, we could alternatively consider subsets $A$ which are (nondegenerate) intervals, as has been suggested in the comments, or possibly subsets which are nonempty open sets. In a broad sense, we're interested in finding ""highly non-identity"" functions $f$ satisfying $f[A] \cap A \neq \varnothing$ for $A$ in some 'large' collection of subsets of $\mathbb{R}$ . If you have the solution to a different problem, but one which is similar in the broad sense described above, you're free to share it.",": We say is intersecting if for every nonempty , . There is only one intersecting function: the identity. The reason for this is that forces . If we impose restrictions on the subsets we consider (say, for instance, we rule out singletons), must still be the identity? Let's try this. We can first introduce some definitions. For any cardinal , we say is - intersecting if for every nonempty with , . For any , its deviation is the cardinality of the set . I'm going to write down some results for the -intersecting case. Suppose has the property that for every with that . There indeed exists a non-identity example here, one with deviation , in fact. Let for and let . Is there a -intersecting with deviation ? No. The argument for this is combinatorial. Suppose an with this property existed, and let be a set of four elements with for each . We first note that restricted to defines a function . To justify this claim, we can argue by contradiction. Suppose for some that . Let be distinct elements in , also both distinct from . Then since , and , it's easy to see this implies , which implies , contradiction. We also note the restricted function is injective. To see this, suppose, say, and , and each of are distinct. Then , contradiction. Since is finite, this implies it's bijective too. Hence corresponds to a permutation in without fixed points. It cannot have two cycles, since if are in distinct two-cycles, then . Hence it's cyclic. But if it's cyclic, then , where are respectively the second and fourth elements of the cycle. This establishes the contradiction. What eludes me is the general case. The combinatorics seem to get harder if . The finite case. Let be a finite cardinal (i.e., a positive integer). Then is it true that any -intersecting map has finite deviation? If this is true (which I suspect), is there a closed form for the largest possible deviation in terms of ? The infinite case. What is the maximum deviation of an -intersecting map ? What is the maximum deviation of a -intersecting map ? In particular, are there such functions with infinite deviation? You will notice that we don't really use any of the analytic or algebraic structure of here, so really these notions can be generalized to functions for arbitrary sets . We could, however, ask different kinds of questions similar to the ones described above which make use of the structure of . Instead of considering subsets with sufficiently large cardinality, we could alternatively consider subsets which are (nondegenerate) intervals, as has been suggested in the comments, or possibly subsets which are nonempty open sets. In a broad sense, we're interested in finding ""highly non-identity"" functions satisfying for in some 'large' collection of subsets of . If you have the solution to a different problem, but one which is similar in the broad sense described above, you're free to share it.","\textbf{Definition} f:\mathbb{R} \to \mathbb{R} A \subset \mathbb{R} f[A] \cap A \neq \varnothing f[\{x\}] \cap \{x\} \neq \varnothing f(x)=x A f \textbf{Definition:} \ell f:\mathbb{R} \to \mathbb{R} \ell A \subset \mathbb{R} |A| \geq \ell f[A] \cap A \neq \varnothing \textbf{Definition:} f:\mathbb{R} \to \mathbb{R} \{x \in \mathbb{R} : f(x) \neq x\} 2 f A \subset \mathbb{R} |A| \geq 2 f[A] \cap A \neq \varnothing 3 f(x) = x x \in \mathbb{R} \setminus \{1,2,3\} f(1)=2, f(2)=3, f(3)=1 2 f \geq 4 f A=\{x_1, x_2, x_3, x_4\} f(x_i) \neq x_i x_i \in A f A f_{A}:A \to A x_i \in A f(x_i) = y \notin A x_j, x_k A x_i f[\{x_i, x_j\}] \cap \{x_i, x_j\} \neq \varnothing f[\{x_i, x_k\}] \cap \{x_i, x_k\} \neq \varnothing f(x_k) = f(x_j) = x_i f[\{x_k, x_j\}] \cap \{x_k, x_j\} = \varnothing f_{A} f(x_i) = x_{m} f(x_j) = x_{m} i,j, m f[\{x_i, x_j\}] \cap \{x_i, x_j\} = \varnothing A f_{A} S_4 x_j, x_i f[\{x_i, x_j\}] \cap \{x_i, x_j\} = \varnothing f[\{x_{i_2}, x_{i_4}\}] \cap \{x_{i_2}, x_{i_4}\} = \varnothing x_{i_2}, x_{i_4} \ell \geq 3 \textbf{Problem 1:} \ell \ell f:\mathbb{R} \to \mathbb{R} \ell \textbf{Problem 2:} \aleph_{0} f:\mathbb{R} \to \mathbb{R} \mathfrak{c} f:\mathbb{R} \to \mathbb{R} \mathbb{R} f:X \to Y X,Y \mathbb{R} A A f f[A] \cap A \neq \varnothing A \mathbb{R}","['real-analysis', 'combinatorics', 'elementary-set-theory']"
85,An integral inequality (one variable),An integral inequality (one variable),,"Anyone has an idea to prove the following inequality? Let $g:\left(0,1\right)\rightarrow\mathbb{R}$ be twice differentiable and $r\in\left(0,1\right)$ such that  $$ r\left(g""\left(x\right)+\dfrac{g'\left(x\right)}{x}\right)\geq\left(g'\left(x\right)\right)^{2},\forall x\in\left(0,1\right). $$ Prove that  $$ \left(\intop_{0}^{1}e^{-g\left(x\right)}xdx\right)\left(\intop_{0}^{1}e^{g\left(x\right)}xdx\right)\leq\dfrac{1}{4\left(1-r\right)},\quad\quad {(\star)} $$ provided the LHS is finite. Further comment: This inequality comes from a contest for undergraduate students in my university. If we take the function  $$ g\left(x\right)=-r\ln\left(-\ln\left(x\right)\right) $$  then we have  $$ r\left(g""\left(x\right)+\dfrac{g'\left(x\right)}{x}\right)=\left(g'\left(x\right)\right)^{2},\forall x\in\left(0,1\right). $$ By using Mathematica, we can see that for this function $$ \left(\intop_{0}^{1}e^{-g\left(x\right)}xdx\right)\left(\intop_{0}^{1}e^{g\left(x\right)}xdx\right)=\dfrac{1}{4}\Gamma\left(1-r\right)\Gamma\left(1+r\right). $$ Moreover, we also can check that  $$ \dfrac{1}{4}\Gamma\left(1-r\right)\Gamma\left(1+r\right)\leq\dfrac{1}{4\left(1-r\right)},\forall r\in\left(0,1\right). $$","Anyone has an idea to prove the following inequality? Let $g:\left(0,1\right)\rightarrow\mathbb{R}$ be twice differentiable and $r\in\left(0,1\right)$ such that  $$ r\left(g""\left(x\right)+\dfrac{g'\left(x\right)}{x}\right)\geq\left(g'\left(x\right)\right)^{2},\forall x\in\left(0,1\right). $$ Prove that  $$ \left(\intop_{0}^{1}e^{-g\left(x\right)}xdx\right)\left(\intop_{0}^{1}e^{g\left(x\right)}xdx\right)\leq\dfrac{1}{4\left(1-r\right)},\quad\quad {(\star)} $$ provided the LHS is finite. Further comment: This inequality comes from a contest for undergraduate students in my university. If we take the function  $$ g\left(x\right)=-r\ln\left(-\ln\left(x\right)\right) $$  then we have  $$ r\left(g""\left(x\right)+\dfrac{g'\left(x\right)}{x}\right)=\left(g'\left(x\right)\right)^{2},\forall x\in\left(0,1\right). $$ By using Mathematica, we can see that for this function $$ \left(\intop_{0}^{1}e^{-g\left(x\right)}xdx\right)\left(\intop_{0}^{1}e^{g\left(x\right)}xdx\right)=\dfrac{1}{4}\Gamma\left(1-r\right)\Gamma\left(1+r\right). $$ Moreover, we also can check that  $$ \dfrac{1}{4}\Gamma\left(1-r\right)\Gamma\left(1+r\right)\leq\dfrac{1}{4\left(1-r\right)},\forall r\in\left(0,1\right). $$",,"['real-analysis', 'inequality', 'integral-inequality']"
86,Holder's inequality for infinite products,Holder's inequality for infinite products,,"In analysis, Holder's inequality says that if we have a sequence $p_1, p_2, \ldots, p_n$ of real numbers in $[1,\infty]$ such that $\sum_{i=1}^n \frac{1}{p_i} = \frac{1}{r}$, and a sequence of measurable functions $f_1, f_2, \ldots, f_n$, then letting $f = f_1 f_2 \cdots f_n$, we have the inequality \begin{equation} \lVert f \rVert_r \leq \lVert f_1 \rVert_{p_1} \lVert f_2 \rVert_{p_2} \cdots \lVert f_n \rVert_{p_n}. \end{equation} In particular, if $f_i \in L^{p_i}(X,\mu)$ for all $i$, then $f \in L^r(X,\mu)$. I'm looking for a generalization of this inequality to infinite products.  That is, suppose we have an infinite sequence $(p_i)_{i \in \mathbb{N}}$ of real numbers in $[1,\infty]$ such that $\sum_{i=1}^\infty \frac{1}{p_i} = \frac{1}{r}$ and an infinite sequence of measurable functions $(f_i)_{i \in \mathbb{N}}$.  Suppose moreover that the function $f(x) = \lim_{n \to \infty} \prod_{i=1}^n f_i(x)$ exists for almost every $x$.  Under what conditions can I assert that \begin{equation} \lVert f \rVert_r \leq \liminf_{n \to \infty} \prod_{i=1}^n \lVert f_i \rVert_{p_i} ? \end{equation} It seems to me that this might follow automatically from the first version of Holder's theorem I quoted above, but I'm a little uncomfortable with taking the limits.  Is there anything I should watch out for? Thanks in advance!","In analysis, Holder's inequality says that if we have a sequence $p_1, p_2, \ldots, p_n$ of real numbers in $[1,\infty]$ such that $\sum_{i=1}^n \frac{1}{p_i} = \frac{1}{r}$, and a sequence of measurable functions $f_1, f_2, \ldots, f_n$, then letting $f = f_1 f_2 \cdots f_n$, we have the inequality \begin{equation} \lVert f \rVert_r \leq \lVert f_1 \rVert_{p_1} \lVert f_2 \rVert_{p_2} \cdots \lVert f_n \rVert_{p_n}. \end{equation} In particular, if $f_i \in L^{p_i}(X,\mu)$ for all $i$, then $f \in L^r(X,\mu)$. I'm looking for a generalization of this inequality to infinite products.  That is, suppose we have an infinite sequence $(p_i)_{i \in \mathbb{N}}$ of real numbers in $[1,\infty]$ such that $\sum_{i=1}^\infty \frac{1}{p_i} = \frac{1}{r}$ and an infinite sequence of measurable functions $(f_i)_{i \in \mathbb{N}}$.  Suppose moreover that the function $f(x) = \lim_{n \to \infty} \prod_{i=1}^n f_i(x)$ exists for almost every $x$.  Under what conditions can I assert that \begin{equation} \lVert f \rVert_r \leq \liminf_{n \to \infty} \prod_{i=1}^n \lVert f_i \rVert_{p_i} ? \end{equation} It seems to me that this might follow automatically from the first version of Holder's theorem I quoted above, but I'm a little uncomfortable with taking the limits.  Is there anything I should watch out for? Thanks in advance!",,"['real-analysis', 'inequality', 'lebesgue-integral', 'integral-inequality']"
87,Formalizing Those Readings of Leibniz Notation that Don't Appeal to Infinitesimals/Differentials,Formalizing Those Readings of Leibniz Notation that Don't Appeal to Infinitesimals/Differentials,,"[Disclaimer: I've studied a lot of logic but never been good at analysis, so that's the angle I'm coming from below] In my attempt to find a precise version of the 'definitions' usually given when first introducing Leibniz notation in single or multivariable calculus or analysis, wherein there is no appeal to differentials or infinitesimals,  I've discovered that I'm confused about a few interrelated low-level issues around formalization and notation, etc.  I don't know which questions are the more basic ones here, so I'll just ask them as go along explaining what I think I do understand about proposed formal definitions of the sort in question. I'm concerned with both the $\frac{dy}{dx}$ notation and the ' $\frac{\partial y}{\partial x}$ ' notation for partial derivatives, but just the real-valued case. Since I suspect that my confusions stem from use-mention confusions, and the confusion of functions, variables, and their values, I will use, and assume familiarity with lambda notation, metavariables, and quasi-quotation throughout.  Where not stated, $\ulcorner\lambda x.\phi\urcorner$ refers to the function on the largest real domain on which $\phi$ is a real number. Assume only definitions for real variables are sought after below. Anyway, on with the definitions: These short articles by the author Thurston (the first five results) all give roughly the same formal definition of Leibniz notation: http://scholar.google.ca/scholar?hl=en&as_sdt=0,5&q=thurston+leibniz Whereas the top results for this search are by the author Harrison, and each give one of a few slight variants on a different definition: http://www.google.ca/search?q=%22leibniz+notation%22+%22lambda+term%22&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-GB:official&client=firefox-a Here is my understanding of their definitions: HARRISON: $\ulcorner\frac{d\phi}{d\psi}\urcorner$ is a shorthand for $\ulcorner D(\lambda\psi.\phi)(\psi)\urcorner$ i.e. "" $\frac{dy}{dx}$ "" stands for "" $D(\lambda x.y)(x)$ "" This means that: $\psi$ must be a variable of the underlying logic (and it has a free and a bound occurence here) and $\phi$ must be a string such that $\ulcorner\forall x,\phi=f(x)\urcorner$ is true for some function $f:S\to R$ where $S\subseteq R$ . Q1. does anyone have a simpler way to state the restriction on $\phi$ ? Q1.1 what is the proper name for the sort of string $\phi$ must be? THURSTON: $\ulcorner\frac{d\phi}{d\psi}\urcorner$ is a shorthand for $\ulcorner\frac{\psi'}{\phi'}\urcorner$ i.e. "" $\frac{dy}{dx}$ "" is short for "" $\frac{y'}{x'}$ "" This means that $\phi$ and $\psi$ must be names of functions from the reals to the reals. Q1.2 Are there other formal definitions in the literature I should compare to these? I haven't found any yet... Harrison's defintion does not return a value because a free variable is uninstntiated, in the same sense in which wffs which are not sentences do not return a truth value. Thurston's version, however returns a function. For instance, $\frac{df}{dx}=f'(x)$ for Harrison, $\frac{df}{dx}=f'$ for Thurston More concretely $\frac{d(x^2+x)}{dx}=$ $2x+1$ for Harrison $\lambda x.2x+1$ for Thurston Q2 is the value ' $2x+1$ ' which contains a free variable being returned, or are we implicitly quantifying over $x$ , or, let's call it ' $\xi$ ' for the purposes of quasi quotation, and saying: $\forall\xi,\ulcorner 2\xi+1\urcorner$ refers to (not 'is') the value returned? However, consider the following 'typical' calculus problem: "" $y = f(x)$ $x = g(u)$ $g(x) = x^3 - 7$ find $\frac{df}{dx}$ "" Thurston gets us $\frac{df}{dx} = \lambda x.\frac{1}{3x^2}$ This is undefined on Harrison's approach, since $x$ is not a variable of the logic, but the name of a function. Q3 should I be considering the possibility that the logic allows for variables ranging over functions? And if we pretended that it was we'd still get $\frac{df}{dx} = D(\lambda x.f)(x)$ but this is mal-formed since $\lambda x$ needs something like ' $f(x)$ ' rather than ' $f$ ' as input. And if we added a case to Harrison's definition to append ' $(x)$ ' or the like when it's missing, we'd still get $\frac{df}{dx} = 1$ , which is not equal to the result we got with Thurston's definition--but more strikingly, the function we evaluated to get 1 was, unike the $f'$ vs $f'(x)$ case above, not even the same function as was returned by Thurston's definition. Q4 What should I conclude from the fact that these definitions diverge in this manner? Now consider: "" $y = f(x)$ $f(x) = x^9$ find $\frac{dy}{dx}$ "" On Harrison's account, we could view $y$ as a metavariable, so that $f(x)$ is placed substitutionally into the defining string, but I have a feeling that is not the right way to understand it. However, if $y$ is merely a variable of the logic, it is a free variable in the result, and we end up with one free variable too many.. On Thurston's account, $y$ must be the name of a function, but "" $y = f(x)$ "" sets $y$ equal to an expression with a free variable, not equal to the name of a function Q5 should i view statements like "" $y=f(x)$ "" as involving a supressed "" $(x)$ "" and "" $\forall$ "" so that we get "" $\forall x,y(x)=f(x)$ "" ? Or should I see $y$ as a metavariable? Or, should I imagine the logic extended to allow some new syntactic category of 'dependent' variables, while thinking of the usual variables in the logic as 'independent' variables--i.e. those whose value does not depend on others? I think I am very confused about what happens when one variable depends on another. Q6 On a related note, I saw a passage recently that spoke in terms like "" $x(u)$ is the inverse function of $u(x)$ ""--how should this be understood more precisely? I've come to discover that I don't understand expressions of this sort at all! Q7 Does either of these definitions clearly capture 'practice' better than another? Q8 How should similar attempts be made for the 'del' notation for patial derivatives? Q9 Can someone give me an example of where $\frac{d}{dx}$ and $\frac{\partial}{\partial x}$ return different values on the same input?  If I'm not mistaken, in some formalizations this never happens, and in other formalizations it does--I think Harrison's would not allow for this since it just returns an 'expression' rather than one of the various functions that can be formed by an expression when you apply a lambda operator to it. I started also trying to read this article on revising patial derivative notation: [I've hit my link limit as I'm new here, but google ""revised notation for partial derivatives""  (with quotes).  It's by WC Hassenpflug] but I got stuck on the sentence: ""If we have a function $u = f(x,y)$ and the transformation $y=g(x\cup)$ is made, then it is not clear whether $\frac{\partial u}{dx}$ means $\frac{\partial u}{dx}\vert y$ or $\frac{\partial y}{dx}\vert n$ Can someone explain that one to me? Q10 This all bears some superficial similarity to the relationship between so-called 'random variables', which are actually functions, and what are called 'variables' in the underlying logic--this has also confused me, and I see many operations done in text on random variables where the operators have only been defined for values in the random variable's domain, and not on functions. Can anyone comment on this? I would be nice if I could dismiss two long-standing confusions with one stone :p","[Disclaimer: I've studied a lot of logic but never been good at analysis, so that's the angle I'm coming from below] In my attempt to find a precise version of the 'definitions' usually given when first introducing Leibniz notation in single or multivariable calculus or analysis, wherein there is no appeal to differentials or infinitesimals,  I've discovered that I'm confused about a few interrelated low-level issues around formalization and notation, etc.  I don't know which questions are the more basic ones here, so I'll just ask them as go along explaining what I think I do understand about proposed formal definitions of the sort in question. I'm concerned with both the notation and the ' ' notation for partial derivatives, but just the real-valued case. Since I suspect that my confusions stem from use-mention confusions, and the confusion of functions, variables, and their values, I will use, and assume familiarity with lambda notation, metavariables, and quasi-quotation throughout.  Where not stated, refers to the function on the largest real domain on which is a real number. Assume only definitions for real variables are sought after below. Anyway, on with the definitions: These short articles by the author Thurston (the first five results) all give roughly the same formal definition of Leibniz notation: http://scholar.google.ca/scholar?hl=en&as_sdt=0,5&q=thurston+leibniz Whereas the top results for this search are by the author Harrison, and each give one of a few slight variants on a different definition: http://www.google.ca/search?q=%22leibniz+notation%22+%22lambda+term%22&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-GB:official&client=firefox-a Here is my understanding of their definitions: HARRISON: is a shorthand for i.e. "" "" stands for "" "" This means that: must be a variable of the underlying logic (and it has a free and a bound occurence here) and must be a string such that is true for some function where . Q1. does anyone have a simpler way to state the restriction on ? Q1.1 what is the proper name for the sort of string must be? THURSTON: is a shorthand for i.e. "" "" is short for "" "" This means that and must be names of functions from the reals to the reals. Q1.2 Are there other formal definitions in the literature I should compare to these? I haven't found any yet... Harrison's defintion does not return a value because a free variable is uninstntiated, in the same sense in which wffs which are not sentences do not return a truth value. Thurston's version, however returns a function. For instance, for Harrison, for Thurston More concretely for Harrison for Thurston Q2 is the value ' ' which contains a free variable being returned, or are we implicitly quantifying over , or, let's call it ' ' for the purposes of quasi quotation, and saying: refers to (not 'is') the value returned? However, consider the following 'typical' calculus problem: "" find "" Thurston gets us This is undefined on Harrison's approach, since is not a variable of the logic, but the name of a function. Q3 should I be considering the possibility that the logic allows for variables ranging over functions? And if we pretended that it was we'd still get but this is mal-formed since needs something like ' ' rather than ' ' as input. And if we added a case to Harrison's definition to append ' ' or the like when it's missing, we'd still get , which is not equal to the result we got with Thurston's definition--but more strikingly, the function we evaluated to get 1 was, unike the vs case above, not even the same function as was returned by Thurston's definition. Q4 What should I conclude from the fact that these definitions diverge in this manner? Now consider: "" find "" On Harrison's account, we could view as a metavariable, so that is placed substitutionally into the defining string, but I have a feeling that is not the right way to understand it. However, if is merely a variable of the logic, it is a free variable in the result, and we end up with one free variable too many.. On Thurston's account, must be the name of a function, but "" "" sets equal to an expression with a free variable, not equal to the name of a function Q5 should i view statements like "" "" as involving a supressed "" "" and "" "" so that we get "" "" ? Or should I see as a metavariable? Or, should I imagine the logic extended to allow some new syntactic category of 'dependent' variables, while thinking of the usual variables in the logic as 'independent' variables--i.e. those whose value does not depend on others? I think I am very confused about what happens when one variable depends on another. Q6 On a related note, I saw a passage recently that spoke in terms like "" is the inverse function of ""--how should this be understood more precisely? I've come to discover that I don't understand expressions of this sort at all! Q7 Does either of these definitions clearly capture 'practice' better than another? Q8 How should similar attempts be made for the 'del' notation for patial derivatives? Q9 Can someone give me an example of where and return different values on the same input?  If I'm not mistaken, in some formalizations this never happens, and in other formalizations it does--I think Harrison's would not allow for this since it just returns an 'expression' rather than one of the various functions that can be formed by an expression when you apply a lambda operator to it. I started also trying to read this article on revising patial derivative notation: [I've hit my link limit as I'm new here, but google ""revised notation for partial derivatives""  (with quotes).  It's by WC Hassenpflug] but I got stuck on the sentence: ""If we have a function and the transformation is made, then it is not clear whether means or Can someone explain that one to me? Q10 This all bears some superficial similarity to the relationship between so-called 'random variables', which are actually functions, and what are called 'variables' in the underlying logic--this has also confused me, and I see many operations done in text on random variables where the operators have only been defined for values in the random variable's domain, and not on functions. Can anyone comment on this? I would be nice if I could dismiss two long-standing confusions with one stone :p","\frac{dy}{dx} \frac{\partial y}{\partial x} \ulcorner\lambda x.\phi\urcorner \phi \ulcorner\frac{d\phi}{d\psi}\urcorner \ulcorner D(\lambda\psi.\phi)(\psi)\urcorner \frac{dy}{dx} D(\lambda x.y)(x) \psi \phi \ulcorner\forall x,\phi=f(x)\urcorner f:S\to R S\subseteq R \phi \phi \ulcorner\frac{d\phi}{d\psi}\urcorner \ulcorner\frac{\psi'}{\phi'}\urcorner \frac{dy}{dx} \frac{y'}{x'} \phi \psi \frac{df}{dx}=f'(x) \frac{df}{dx}=f' \frac{d(x^2+x)}{dx}= 2x+1 \lambda x.2x+1 2x+1 x \xi \forall\xi,\ulcorner 2\xi+1\urcorner y = f(x) x = g(u) g(x) = x^3 - 7 \frac{df}{dx} \frac{df}{dx} = \lambda x.\frac{1}{3x^2} x \frac{df}{dx} = D(\lambda x.f)(x) \lambda x f(x) f (x) \frac{df}{dx} = 1 f' f'(x) y = f(x) f(x) = x^9 \frac{dy}{dx} y f(x) y y y = f(x) y y=f(x) (x) \forall \forall x,y(x)=f(x) y x(u) u(x) \frac{d}{dx} \frac{\partial}{\partial x} u = f(x,y) y=g(x\cup) \frac{\partial u}{dx} \frac{\partial u}{dx}\vert y \frac{\partial y}{dx}\vert n","['calculus', 'real-analysis', 'logic', 'notation', 'definition']"
88,"If $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is a polynomial on every line, is it a polynomial itself? [duplicate]","If  is a polynomial on every line, is it a polynomial itself? [duplicate]",f:\mathbb{R}^2\rightarrow\mathbb{R},"This question already has answers here : Real polynomial in two variables (3 answers) Closed last year . The question: If $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is a function such that for every ""linear"" function $\varphi:\mathbb{R}\rightarrow\mathbb{R}^2$ (i.e. an affine line in $\mathbb{R}^2$ ) we have that $f\circ\varphi$ is a polynomial, then is $f$ a polynomial itself? Discussion (only for curious readers): What I have already proved is that if $f\circ\varphi$ is a polynomial of degree at most $n$ then $f$ is a polynomial of degree at most $n$ too. Although, I just proved it for $n=2$ and assumed that the similar construction exists and can be made for higher $n$ . The proof goes like this: Substract from $f$ the polynomial that is at the line $x=0$ . For example, if at $x=0$ we have the polynomial $ay^2+by+c$ then substract that from $f$ . So, if $f=x+xy+y^2$ , then after substracting we are left with $x+xy$ . We have that this new $f$ is a quadratic polynomial iff the old $f$ was. Do that for $y=0$ too. Now the new $f$ is equal to $0$ on both axes. Substract now from $f$ the function $f(1,1)\cdot xy$ . The new $f$ is zero on the axes and on the line $x+y=2$ because it is zero at $3$ points on it: $(0,2)$ , $(1,1)$ and $(2,0)$ . Now every line (that is not parallel to these $3$ nor is concurrent with two of them) intersects all $3$ of them (at $3$ different points) so $f$ is zero on them too. These lines pass through each point, so the new $f$ is $0$ everywhere, and that is equivalent to the original $f$ being a quadratic form. My idea for my original question was that there will be simply too many lines for $f$ to not be a polynomial, i.e. if the number of lines at which $f$ is a polynomial of the degree exactly $n$ is too big (in some sense) then $f$ must be a polynomial of degree $n$ too. I thought maybe some argument similar to the one in the spoiler would work, but I couldn't find it.","This question already has answers here : Real polynomial in two variables (3 answers) Closed last year . The question: If is a function such that for every ""linear"" function (i.e. an affine line in ) we have that is a polynomial, then is a polynomial itself? Discussion (only for curious readers): What I have already proved is that if is a polynomial of degree at most then is a polynomial of degree at most too. Although, I just proved it for and assumed that the similar construction exists and can be made for higher . The proof goes like this: Substract from the polynomial that is at the line . For example, if at we have the polynomial then substract that from . So, if , then after substracting we are left with . We have that this new is a quadratic polynomial iff the old was. Do that for too. Now the new is equal to on both axes. Substract now from the function . The new is zero on the axes and on the line because it is zero at points on it: , and . Now every line (that is not parallel to these nor is concurrent with two of them) intersects all of them (at different points) so is zero on them too. These lines pass through each point, so the new is everywhere, and that is equivalent to the original being a quadratic form. My idea for my original question was that there will be simply too many lines for to not be a polynomial, i.e. if the number of lines at which is a polynomial of the degree exactly is too big (in some sense) then must be a polynomial of degree too. I thought maybe some argument similar to the one in the spoiler would work, but I couldn't find it.","f:\mathbb{R}^2\rightarrow\mathbb{R} \varphi:\mathbb{R}\rightarrow\mathbb{R}^2 \mathbb{R}^2 f\circ\varphi f f\circ\varphi n f n n=2 n f x=0 x=0 ay^2+by+c f f=x+xy+y^2 x+xy f f y=0 f 0 f f(1,1)\cdot xy f x+y=2 3 (0,2) (1,1) (2,0) 3 3 3 f f 0 f f f n f n","['real-analysis', 'polynomials']"
89,why is $ 2 = \frac{5}{1+\frac{8}{4+\frac{11}{7 + \frac{14}{10 + \dots}}} } $,why is, 2 = \frac{5}{1+\frac{8}{4+\frac{11}{7 + \frac{14}{10 + \dots}}} } ,"Why is $ 2 = \cfrac{5}{1+\cfrac{8}{4+\cfrac{11}{7 + \cfrac{14}{10 + \ddots}}} } $ where the sequences $5,8,11,14,\dots$ and $1,4,7,10,\dots$ are of the form $5 + 3 n$ and $1 + 3n$ . (This converges on both even and uneven iterates) I was surprised this is an integer. Maybe it would help to rewrite this generalized continued fraction into a ""normal"" simple continued fraction. But I believe that would give us coefficients that generalized the double factorial to a sort of "" triple factorial "" meaning $ f(n) = n \cdot f(n-3) \cdot f(n-6) \cdot f(n-9) \cdots $ and I have almost no skills or understanding of those. Maybe some transformation formula's make this easy, but Im not seeing it.","Why is where the sequences and are of the form and . (This converges on both even and uneven iterates) I was surprised this is an integer. Maybe it would help to rewrite this generalized continued fraction into a ""normal"" simple continued fraction. But I believe that would give us coefficients that generalized the double factorial to a sort of "" triple factorial "" meaning and I have almost no skills or understanding of those. Maybe some transformation formula's make this easy, but Im not seeing it."," 2 = \cfrac{5}{1+\cfrac{8}{4+\cfrac{11}{7 + \cfrac{14}{10 + \ddots}}} }  5,8,11,14,\dots 1,4,7,10,\dots 5 + 3 n 1 + 3n  f(n) = n \cdot f(n-3) \cdot f(n-6) \cdot f(n-9) \cdots ","['real-analysis', 'continued-fractions']"
90,Integral over domain of infinite tetration of x over extended domain from 0 to $\sqrt[e]e$. Possible $\int_{e^{-e}}^{e^\frac1e} x^{x^{…}}dx$ solution.,Integral over domain of infinite tetration of x over extended domain from 0 to . Possible  solution.,\sqrt[e]e \int_{e^{-e}}^{e^\frac1e} x^{x^{…}}dx,"I have been trying to find an interesting constant over the domain of the infinite tetration of x and have just almost figured out the area with a non integral infinite sum representation. Just one constant is in my way. D denotes the domain . This question is different from this one as it has the full domain such that the imaginary part is $0$ and not for a single point. Here is a demo of my expansion. Here is my source for the product logarithm/W-Lambert function series. My inspiration for the series is here . Finally here is a graph of the constant. My work is as follows. I used a bit of software to help with the evaluation at the end. Here is data about the generalized incomplete gamma function used here. $$ \mathrm{G=\int_D x^{x^{x^…}}dx=\int_D {^\infty x}\, dx=\int_D-\frac{W(-ln(x))}{ln(x)}dx= 1.265188689361227081430914184615901039501069191363542653701819999950085943915822836313002058708863484…\implies G+\int_0^{\sqrt[-e]e }\frac {W(-ln(x))}{ln(x)}dx= \int_{\sqrt[-e]e}^{\sqrt[e]e}\frac {W(-ln(x))}{ln(x)}dx= \sum_{n=1}^\infty\frac{n^{n-1}}{n!} \int_{\sqrt[-e]e}^{\sqrt[e]e} ln^{n-1}(x)dx= \sum_{n=1}^\infty\frac{(-n)^{n-1}}{n!}Γ\left(n,-\frac 1e,\frac 1e\right)=\sum_{n=1}^\infty\frac{(-n)^{n-1}}{n}Q\left(n,-\frac 1e, \frac 1e\right)= 0.886369135921835965080748…=G-0.378819553439391116350165…} $$ I have also found the amazing result of being able to integrate the $\mathrm{x^{th}}$ root of x using a theorem on the integral of an inverse function . Here is my work. $$\mathrm{\int_{eW\left(\frac 1e\right)}^e \left(x^\frac 1x=\sqrt[x]x\right)dx+\int_{e^{-\frac 1e}}^{e^\frac 1e} {^\infty x}\,dx=e^{1+\frac1e}-e^{1-\frac1e} W\left(\frac 1e\right)=e^{1-\frac1e}\left(e^\frac2e-W\left(\frac 1e\right)\right)=e^{1+\frac1e}\left(1-e^{-\frac2e}W\left(\frac 1e\right)\right)\implies \int_{eW\left(\frac 1e\right)}^e x^\frac 1xdx= e^{1+\frac1e}-e^{1-\frac1e} W\left(\frac 1e\right)-\sum_{n=1}^\infty\frac{(-n)^{n-1}}{n}Q\left(n,-\frac 1e, \frac 1e\right)}$$ In order to find an exact form of G, I need to find the following. The other form uses the following identity here : $$\mathrm{I= \int_0^ {e^{-\frac 1e}}  x^{x^{x^…}}dx=\int_0^{e^{-\frac 1e}} {^\infty x}\, dx=\int_0^ {e^{-\frac 1e}} -\frac{W(-ln(x))}{ln(x)}dx=e^{1-\frac1e}W\left(\frac1e\right)-\int_0^ {eW\left(\frac1e\right)} \sqrt[x]x dx=0.378819553439391116350165…}$$ The previous result is proof of the following. I guess this link here is not accurate anymore. I will give an example if wanted of this result. Using another Wikipedia theorem proves that: $$\mathrm{\int x^{\frac1x}dx=x^{\frac1x+1}+\sum_{n=1}^\infty (-1)^nn^{n-2} Q\left(n,-\frac{ln(x)}{x}\right)+C,eW\left(\frac1e\right)\le x\le e}$$ more info on this result I found this series based on this answer from @mathphile which does not give the right result as the n=0 term diverges and even trying to use the lower integration bound as $e^{-e}$ still gives the wrong result. The user’s answer would have cracked the question. How do I evaluate this integral? A closed form is wanted, but optional. Please give me any hints as the already used series expansion for this other integral is not in the interval of convergence. This is the main constant that I need to find. Also, please correct me and give me feedback! Note: These Taylor series expansions were found: Taylor Series Unfortunately, it seems like there may be uneven radiuses of convergence which may make us need to have multiple series. Also see nth derivative of xth root of x at x=1 OEIS which has an actual formula. We are so close to the answer.","I have been trying to find an interesting constant over the domain of the infinite tetration of x and have just almost figured out the area with a non integral infinite sum representation. Just one constant is in my way. D denotes the domain . This question is different from this one as it has the full domain such that the imaginary part is and not for a single point. Here is a demo of my expansion. Here is my source for the product logarithm/W-Lambert function series. My inspiration for the series is here . Finally here is a graph of the constant. My work is as follows. I used a bit of software to help with the evaluation at the end. Here is data about the generalized incomplete gamma function used here. I have also found the amazing result of being able to integrate the root of x using a theorem on the integral of an inverse function . Here is my work. In order to find an exact form of G, I need to find the following. The other form uses the following identity here : The previous result is proof of the following. I guess this link here is not accurate anymore. I will give an example if wanted of this result. Using another Wikipedia theorem proves that: more info on this result I found this series based on this answer from @mathphile which does not give the right result as the n=0 term diverges and even trying to use the lower integration bound as still gives the wrong result. The user’s answer would have cracked the question. How do I evaluate this integral? A closed form is wanted, but optional. Please give me any hints as the already used series expansion for this other integral is not in the interval of convergence. This is the main constant that I need to find. Also, please correct me and give me feedback! Note: These Taylor series expansions were found: Taylor Series Unfortunately, it seems like there may be uneven radiuses of convergence which may make us need to have multiple series. Also see nth derivative of xth root of x at x=1 OEIS which has an actual formula. We are so close to the answer.","0 
\mathrm{G=\int_D x^{x^{x^…}}dx=\int_D {^\infty x}\, dx=\int_D-\frac{W(-ln(x))}{ln(x)}dx= 1.265188689361227081430914184615901039501069191363542653701819999950085943915822836313002058708863484…\implies G+\int_0^{\sqrt[-e]e }\frac {W(-ln(x))}{ln(x)}dx= \int_{\sqrt[-e]e}^{\sqrt[e]e}\frac {W(-ln(x))}{ln(x)}dx= \sum_{n=1}^\infty\frac{n^{n-1}}{n!} \int_{\sqrt[-e]e}^{\sqrt[e]e} ln^{n-1}(x)dx= \sum_{n=1}^\infty\frac{(-n)^{n-1}}{n!}Γ\left(n,-\frac 1e,\frac 1e\right)=\sum_{n=1}^\infty\frac{(-n)^{n-1}}{n}Q\left(n,-\frac 1e, \frac 1e\right)= 0.886369135921835965080748…=G-0.378819553439391116350165…}
 \mathrm{x^{th}} \mathrm{\int_{eW\left(\frac 1e\right)}^e \left(x^\frac 1x=\sqrt[x]x\right)dx+\int_{e^{-\frac 1e}}^{e^\frac 1e} {^\infty x}\,dx=e^{1+\frac1e}-e^{1-\frac1e} W\left(\frac 1e\right)=e^{1-\frac1e}\left(e^\frac2e-W\left(\frac 1e\right)\right)=e^{1+\frac1e}\left(1-e^{-\frac2e}W\left(\frac 1e\right)\right)\implies \int_{eW\left(\frac 1e\right)}^e x^\frac 1xdx= e^{1+\frac1e}-e^{1-\frac1e} W\left(\frac 1e\right)-\sum_{n=1}^\infty\frac{(-n)^{n-1}}{n}Q\left(n,-\frac 1e, \frac 1e\right)} \mathrm{I= \int_0^ {e^{-\frac 1e}}  x^{x^{x^…}}dx=\int_0^{e^{-\frac 1e}} {^\infty x}\, dx=\int_0^ {e^{-\frac 1e}} -\frac{W(-ln(x))}{ln(x)}dx=e^{1-\frac1e}W\left(\frac1e\right)-\int_0^ {eW\left(\frac1e\right)} \sqrt[x]x dx=0.378819553439391116350165…} \mathrm{\int x^{\frac1x}dx=x^{\frac1x+1}+\sum_{n=1}^\infty (-1)^nn^{n-2} Q\left(n,-\frac{ln(x)}{x}\right)+C,eW\left(\frac1e\right)\le x\le e} e^{-e}","['real-analysis', 'definite-integrals', 'constants', 'tetration', 'power-towers']"
91,Differentiable bijection $f:\mathbb{R} \to \mathbb{R}$ with nonzero derivative whose inverse is not differentiable,Differentiable bijection  with nonzero derivative whose inverse is not differentiable,f:\mathbb{R} \to \mathbb{R},"I had an exam today, and I was asked about the inverse function theorem, and the exact conditions and statement (as stated in Mathematical Analysis by VA Zorich): Let $X, Y \subset \mathbb{R}$ be open sets and let the functions $f: X \to Y$ and $f^{-1}: Y \to X$ be mutually inverse and continuous at points $x_{0} \in X$ and $y_{0}=f(x_{0})$, respectively. If $f$ is differentiable at $x_{0}$ and $f'(x_{0}) \neq 0$, then $f^{-1}$ is also differentiable at $y_{0}$  and its derivative is $$(f^{-1})'(y_{0})=(f'(x_{0}))^{-1}=\frac{1}{f'(x_{0})}.$$ Then I was asked to come up with an example which would show that the condition that $f^{-1}$ be continuous at $y_{0}$ is not redundant, i.e. that there exists a differentiable bijection $f: \mathbb{R} \to \mathbb{R}$ such that $f'(x) \neq 0$ for all $x \in \mathbb{R}$, but whose inverse $f^{-1}$ is non-differentiable at some point $y_{0} \in \mathbb{R}$ (my professor stated that both the domain and range are $\mathbb{R}$, but I'd be open to any example whose domain/range are open subsets of $\mathbb{R}$). I couldn't come up with an answer on the spot, so I was left with a homework assignment, which I'm shamelessly asking for help with here. Here are some examples that almost, but don't quite fit the bill: $f(x)=x^3, f:\mathbb{R} \to \mathbb{R}$, whose inverse $f^{-1}(y)=\sqrt[3]y$ is not differentiable at $0$, but the problem is that $f'(0)=0$. If $f'(x) \neq 0$ were removed as a conditions, many counterexamples could be easily found, because any bijection $f$ whose derivative is zero at $x$ implies that $f^{-1}$ is not differentiable at $f(x)$. An example which is possibly closer to what I'm looking for is the one found in the answer of Functions which are Continuous, but not Bicontinuous , which fits the bill completely except for the domain/range, because $f^{-1}$ is not continuous at 1, let alone differentiable, but its domain is not an open or connected set, so it wouldn't be easy to extend its domain to $\mathbb{R}$ and still keep all of its properties. I'm aware that the inverse $f^{-1}$ of a continuous bijection $f$ defined on an interval ($\mathbb{R}$ in this case) is also continuous, so the ""pathological inverse"" that I'm looking for is continuous (unlike example 2), but not differentiable (i.e. ""spiky"") at a point, even though $f$ isn't ""spiky"" anywhere.","I had an exam today, and I was asked about the inverse function theorem, and the exact conditions and statement (as stated in Mathematical Analysis by VA Zorich): Let $X, Y \subset \mathbb{R}$ be open sets and let the functions $f: X \to Y$ and $f^{-1}: Y \to X$ be mutually inverse and continuous at points $x_{0} \in X$ and $y_{0}=f(x_{0})$, respectively. If $f$ is differentiable at $x_{0}$ and $f'(x_{0}) \neq 0$, then $f^{-1}$ is also differentiable at $y_{0}$  and its derivative is $$(f^{-1})'(y_{0})=(f'(x_{0}))^{-1}=\frac{1}{f'(x_{0})}.$$ Then I was asked to come up with an example which would show that the condition that $f^{-1}$ be continuous at $y_{0}$ is not redundant, i.e. that there exists a differentiable bijection $f: \mathbb{R} \to \mathbb{R}$ such that $f'(x) \neq 0$ for all $x \in \mathbb{R}$, but whose inverse $f^{-1}$ is non-differentiable at some point $y_{0} \in \mathbb{R}$ (my professor stated that both the domain and range are $\mathbb{R}$, but I'd be open to any example whose domain/range are open subsets of $\mathbb{R}$). I couldn't come up with an answer on the spot, so I was left with a homework assignment, which I'm shamelessly asking for help with here. Here are some examples that almost, but don't quite fit the bill: $f(x)=x^3, f:\mathbb{R} \to \mathbb{R}$, whose inverse $f^{-1}(y)=\sqrt[3]y$ is not differentiable at $0$, but the problem is that $f'(0)=0$. If $f'(x) \neq 0$ were removed as a conditions, many counterexamples could be easily found, because any bijection $f$ whose derivative is zero at $x$ implies that $f^{-1}$ is not differentiable at $f(x)$. An example which is possibly closer to what I'm looking for is the one found in the answer of Functions which are Continuous, but not Bicontinuous , which fits the bill completely except for the domain/range, because $f^{-1}$ is not continuous at 1, let alone differentiable, but its domain is not an open or connected set, so it wouldn't be easy to extend its domain to $\mathbb{R}$ and still keep all of its properties. I'm aware that the inverse $f^{-1}$ of a continuous bijection $f$ defined on an interval ($\mathbb{R}$ in this case) is also continuous, so the ""pathological inverse"" that I'm looking for is continuous (unlike example 2), but not differentiable (i.e. ""spiky"") at a point, even though $f$ isn't ""spiky"" anywhere.",,"['real-analysis', 'examples-counterexamples', 'inverse-function', 'inverse-function-theorem']"
92,Convex function can be written as supremum of some affine functions,Convex function can be written as supremum of some affine functions,,"Let $\phi: \mathbb{R} \to \mathbb{R}$ be a convex function. Prove that $\phi$ can be written as the supremum of some affine functions $\alpha$, in the sense that $\phi(x) = \sup_\alpha \alpha(x)$ for every $x$, where each $\alpha$ is defined by$$\alpha: x \mapsto a_\alpha x + b_\alpha$$for some $a_\alpha$ and $b_\alpha$. My progress is as follows. I can show that if $\phi$ is convex and $x \in \mathbb{R}$, there exists a real number $c$ such that$$\phi(y) \ge \phi(x) + c(y - x)$$for all $y \in \mathbb{R}$. But I am at a loss on how to continue, how to finish. Could aybody help?","Let $\phi: \mathbb{R} \to \mathbb{R}$ be a convex function. Prove that $\phi$ can be written as the supremum of some affine functions $\alpha$, in the sense that $\phi(x) = \sup_\alpha \alpha(x)$ for every $x$, where each $\alpha$ is defined by$$\alpha: x \mapsto a_\alpha x + b_\alpha$$for some $a_\alpha$ and $b_\alpha$. My progress is as follows. I can show that if $\phi$ is convex and $x \in \mathbb{R}$, there exists a real number $c$ such that$$\phi(y) \ge \phi(x) + c(y - x)$$for all $y \in \mathbb{R}$. But I am at a loss on how to continue, how to finish. Could aybody help?",,['real-analysis']
93,Function $\Bbb Q\rightarrow\Bbb Q$ with everywhere irrational derivative,Function  with everywhere irrational derivative,\Bbb Q\rightarrow\Bbb Q,"As in topic, my question is as follows: Is there a function $f:\Bbb Q\rightarrow\Bbb Q$ such that $f'(q)$ exists and is irrational for all $q\in\Bbb Q$? For the sake of completeness, I define $f'(q)$ as the limit of $\lim\limits_{h\rightarrow 0}\frac{f(q+h)-f(q)}{h}$ where $h$ ranges over rational numbers. I don't know of any different ""reasonable"" definition of derivative for function from $\Bbb Q$ to itself,, but if you can find an example of a function like in question, or prove that there is none, for some different notion of derivative, I would love to see it. I can't provide much background on this question, it's just something I've been wondering about for the past few days. Thanks in advance for all feedback.","As in topic, my question is as follows: Is there a function $f:\Bbb Q\rightarrow\Bbb Q$ such that $f'(q)$ exists and is irrational for all $q\in\Bbb Q$? For the sake of completeness, I define $f'(q)$ as the limit of $\lim\limits_{h\rightarrow 0}\frac{f(q+h)-f(q)}{h}$ where $h$ ranges over rational numbers. I don't know of any different ""reasonable"" definition of derivative for function from $\Bbb Q$ to itself,, but if you can find an example of a function like in question, or prove that there is none, for some different notion of derivative, I would love to see it. I can't provide much background on this question, it's just something I've been wondering about for the past few days. Thanks in advance for all feedback.",,"['real-analysis', 'derivatives']"
94,"Where is a ""fat Cantor staircase"" differentiable?","Where is a ""fat Cantor staircase"" differentiable?",,"The Cantor staircase is an example of a continuous function $f$ such that $f'=0$ almost everywhere, and yet $f$ is nonconstant. It is differentiable precisely at the points that aren't in the Cantor set. Now, repeat the construction with a fat Cantor set (such as the Smith–Volterra–Cantor set, shown here). Where is this fat Cantor staircase differentiable? Naively, I would've expected it to be differentiable on the complement of the Cantor set again, just like the original staircase. But this contradicts the theorem that says monotone functions are differentiable almost everywhere. So the fat Cantor staircase must be differentiable on some points in the Cantor set! Where, precisely, in the Cantor set does this happen? What is the derivative there? And what does the integral of this function's derivative look like? (Conjecture: it is differentiable on the ""pseudo-interior"" - the set of points in the fat Cantor set without successor or predecessor, not counting 0 and 1.)","The Cantor staircase is an example of a continuous function such that almost everywhere, and yet is nonconstant. It is differentiable precisely at the points that aren't in the Cantor set. Now, repeat the construction with a fat Cantor set (such as the Smith–Volterra–Cantor set, shown here). Where is this fat Cantor staircase differentiable? Naively, I would've expected it to be differentiable on the complement of the Cantor set again, just like the original staircase. But this contradicts the theorem that says monotone functions are differentiable almost everywhere. So the fat Cantor staircase must be differentiable on some points in the Cantor set! Where, precisely, in the Cantor set does this happen? What is the derivative there? And what does the integral of this function's derivative look like? (Conjecture: it is differentiable on the ""pseudo-interior"" - the set of points in the fat Cantor set without successor or predecessor, not counting 0 and 1.)",f f'=0 f,"['real-analysis', 'measure-theory', 'cantor-set']"
95,Show that $\sum\limits_pa_p$ converges iff $\sum\limits_{n}\frac{a_n}{\log n}$ converges,Show that  converges iff  converges,\sum\limits_pa_p \sum\limits_{n}\frac{a_n}{\log n},"I am going through A. J. Hildebrand's lecture notes on Introduction to Analytic Number Theory . I'm currently stuck at the exercises at the end of Chapter 3 (Distribution of Primes I - Elementary Results). The problem statement is: Let $(a_n)$ be a nonincreasing sequence of positive numbers. Show that $\sum\limits_p a_p$ converges if and only if $\sum\limits_{n=2}^{\infty}\frac{a_n}{\log n}$ converges. The way I was trying to go about the proof is using the integral convergence test and the Prime Number Theorem, by saying that $$\int_1^\infty a(p(x))dx = \int_2^\infty a(t)\pi'(t)dt$$ where $p(x)$ is an interpolated version of the n-th prime sequence, and $\pi(t)$ is the prime counting function. Then by the PNT, we know that $\pi(t) = \frac{t}{\log t} + O\left(\frac{t}{\log^2 t}\right)$ . By a leap of logic, I'd hope that $\pi'(t) = \frac{1}{\log t} + o\left(\frac{1}{\log t}\right)$ , which would make the last integral equal to $$\int_{2}^{\infty}\frac{a(t)}{\log t} dt + \text{terms of lower order}$$ This would then converge if and only if $\sum\limits_{n=2}^{\infty}\frac{a_n}{\log n}$ converges. The problem is that differentiating the Big-O estimate doesn't seem valid, and I am unable to come up with good enough estimates to prove this relationship (if it is even true).","I am going through A. J. Hildebrand's lecture notes on Introduction to Analytic Number Theory . I'm currently stuck at the exercises at the end of Chapter 3 (Distribution of Primes I - Elementary Results). The problem statement is: Let be a nonincreasing sequence of positive numbers. Show that converges if and only if converges. The way I was trying to go about the proof is using the integral convergence test and the Prime Number Theorem, by saying that where is an interpolated version of the n-th prime sequence, and is the prime counting function. Then by the PNT, we know that . By a leap of logic, I'd hope that , which would make the last integral equal to This would then converge if and only if converges. The problem is that differentiating the Big-O estimate doesn't seem valid, and I am unable to come up with good enough estimates to prove this relationship (if it is even true).",(a_n) \sum\limits_p a_p \sum\limits_{n=2}^{\infty}\frac{a_n}{\log n} \int_1^\infty a(p(x))dx = \int_2^\infty a(t)\pi'(t)dt p(x) \pi(t) \pi(t) = \frac{t}{\log t} + O\left(\frac{t}{\log^2 t}\right) \pi'(t) = \frac{1}{\log t} + o\left(\frac{1}{\log t}\right) \int_{2}^{\infty}\frac{a(t)}{\log t} dt + \text{terms of lower order} \sum\limits_{n=2}^{\infty}\frac{a_n}{\log n},"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'prime-numbers', 'analytic-number-theory']"
96,Could Euclid have proven that multiplication of real numbers distributes over addition?,Could Euclid have proven that multiplication of real numbers distributes over addition?,,"In Euclid's day, the modern notion of real number did not exist; Euclid did not believe that the length of a line segment was a quantity measurable by number.  But he did think it made sense to talk about the ratio of two lengths.  In fact, he devotes Book V of his Elements to the study of such ratios, using the so-called Eudoxian theory of proportions.  Here's how it works. Let $w$ and $x$ be two magnitudes of the same kind (for instance two length), and let $y$ and $z$ be two magnitudes of the same kind (for instance two areas). Then according to Euclid, the ratio of $w$ to $x$ is said to be equal to the ratio of $y$ to $z$ if for all positive integers $m$ and $n$, if $nw$ is greater, equal, or less than $mx$, then $ny$ is greater, equal, or less than $mz$, respectively.  Or to put it in modern language, $w/x = y/z$ if the same rational numbers $m/n$ are less than both, the same rational numbers are equal to both, and the same rational numbers are greater than both. In other words, a ratio is defined by the classes of rational numbers which are less than, equal to, and greater than it.  If you've studied real analysis; this should look familiar to you: it is how the real number system is constructed using Dedekind cuts!  In fact, Dedekind took the Eudoxian theory of proportions in Euclid's Book V as the inspiration for his Dedekind cut construction.  So to sum up, while Euclid wouldn't have thought of them as numbers, his notion of ""ratios"" basically corresponds to our notion of ""positive real numbers"". Now with that background, in this question I wanted to try to prove that real number multiplication is commutative, but it turned out that Euclid had beat me to the punch.  Now I'd like to prove that multiplication of real numbers distributes over addition.  First let me explain how the sum and product of two ratios is defined.  We say that the sum of $w/x$ and $y/z$ is equal to $u/v$ if there exist magnitudes $r,s,$ and $t$ such that $w/x = r/t$, $y/z = s/t$, and $(r+s)/t = u/v$. And we say that the product of $w/x$ and $y/z$ is equal to $u/v$ if there exist magnitudes $r,s,$ and $t$ such that $w/x = r/s$, $y/z = s/t$, and $r/t = u/v$. So in order to establish that multiplication distributes over addition, we need to prove the following: Suppose that the product of $a/b$ and $c/e$ is $f/h$, and the product of $a/b$ and $d/e$ is $g/h$.  Then the product of $a/b$ and $(c+d)/e$ is $(f+g)/h$ So how would I go about proving that?  Euclid's Book V contains a lot of theorems about ratios that are potentially relevant, but I'm not sure how to proceed. Note that there are other kinds of distributive properties proved in Euclid's Elements, including this one , this one , and this one , but they're not relevant here.","In Euclid's day, the modern notion of real number did not exist; Euclid did not believe that the length of a line segment was a quantity measurable by number.  But he did think it made sense to talk about the ratio of two lengths.  In fact, he devotes Book V of his Elements to the study of such ratios, using the so-called Eudoxian theory of proportions.  Here's how it works. Let $w$ and $x$ be two magnitudes of the same kind (for instance two length), and let $y$ and $z$ be two magnitudes of the same kind (for instance two areas). Then according to Euclid, the ratio of $w$ to $x$ is said to be equal to the ratio of $y$ to $z$ if for all positive integers $m$ and $n$, if $nw$ is greater, equal, or less than $mx$, then $ny$ is greater, equal, or less than $mz$, respectively.  Or to put it in modern language, $w/x = y/z$ if the same rational numbers $m/n$ are less than both, the same rational numbers are equal to both, and the same rational numbers are greater than both. In other words, a ratio is defined by the classes of rational numbers which are less than, equal to, and greater than it.  If you've studied real analysis; this should look familiar to you: it is how the real number system is constructed using Dedekind cuts!  In fact, Dedekind took the Eudoxian theory of proportions in Euclid's Book V as the inspiration for his Dedekind cut construction.  So to sum up, while Euclid wouldn't have thought of them as numbers, his notion of ""ratios"" basically corresponds to our notion of ""positive real numbers"". Now with that background, in this question I wanted to try to prove that real number multiplication is commutative, but it turned out that Euclid had beat me to the punch.  Now I'd like to prove that multiplication of real numbers distributes over addition.  First let me explain how the sum and product of two ratios is defined.  We say that the sum of $w/x$ and $y/z$ is equal to $u/v$ if there exist magnitudes $r,s,$ and $t$ such that $w/x = r/t$, $y/z = s/t$, and $(r+s)/t = u/v$. And we say that the product of $w/x$ and $y/z$ is equal to $u/v$ if there exist magnitudes $r,s,$ and $t$ such that $w/x = r/s$, $y/z = s/t$, and $r/t = u/v$. So in order to establish that multiplication distributes over addition, we need to prove the following: Suppose that the product of $a/b$ and $c/e$ is $f/h$, and the product of $a/b$ and $d/e$ is $g/h$.  Then the product of $a/b$ and $(c+d)/e$ is $(f+g)/h$ So how would I go about proving that?  Euclid's Book V contains a lot of theorems about ratios that are potentially relevant, but I'm not sure how to proceed. Note that there are other kinds of distributive properties proved in Euclid's Elements, including this one , this one , and this one , but they're not relevant here.",,"['real-analysis', 'geometry', 'euclidean-geometry', 'math-history', 'real-numbers']"
97,Does there exists a continuous surjection from $\mathbb{R}$ to $\mathbb{R}^2$?,Does there exists a continuous surjection from  to ?,\mathbb{R} \mathbb{R}^2,"I constructed a bijection by using decimal expansions of two real numbers and taking numbers 1 by 1 consecutively. (It took me hours to come up with this). I remember someone saying appealing to some sort of expansion is the only way to do this, and I think this type of method can never be continuous. Is there anyway to prove that no such function exists?","I constructed a bijection by using decimal expansions of two real numbers and taking numbers 1 by 1 consecutively. (It took me hours to come up with this). I remember someone saying appealing to some sort of expansion is the only way to do this, and I think this type of method can never be continuous. Is there anyway to prove that no such function exists?",,"['real-analysis', 'general-topology']"
98,Prove that a sequence converges to a finite limit iff lim inf equals lim sup,Prove that a sequence converges to a finite limit iff lim inf equals lim sup,,"This problem is purely for my own benefit, so I'd appreciate it if you offer help but don't spoil the proof for me. I've worked out the following solution, but I want to make sure that my reasoning doesn't have any holes in it: Suppose we have the sequence $\{a_n\}$, and $\lim_{n \rightarrow \infty}\{a_n\}=L$. Then given $\epsilon>0$, we can choose $N \in \mathbb{N}$ such that $$\left| a_n-L \right|<\frac{\epsilon}{2}$$ for every term $a_n \in \{a_n\}$ such that $n>N$. Now for $\inf_{n>N}\{a_n\}$, (that is, to my understanding, the infimum of the sequence with the first N terms truncated) we must have some $a_{low} \in \{a_n|n>N\}$ such that $$a_{low}-\inf_{n>N}\{a_n\}<\frac{\epsilon}{2}$$ because otherwise $\inf_{n>N}\{a_n\}-\frac{\epsilon}{2}$ would be a greater lower bound for $\{a_n|n>N\}$. Similar reasoning demonstrates that we must have some $a_{high}$ such that $$\sup_{n>N}\{a_n\}-a_{high}<\frac{\epsilon}{2}$$ Now since $a_{high} \in \{a_n|n>N\}$, we must have $$-\epsilon<a_{high}-L<\epsilon$$ and, adding this inequality to the above one gives $$-\epsilon<\sup_{n>N}\{a_n\}-L<\epsilon$$ for $n>N$, and therefore $\lim_{N \rightarrow \infty}\sup_{n>N}\{a_n\}=L$. Likewise, since $a_{low} \in \{a_n|n>N\}$ we must have $$\left|a_{low}-L\right|=\left|L-a_{low}\right|<\frac{\epsilon}{2}$$ So $$-\frac{\epsilon}{2}<L-a_{low}<\frac{\epsilon}{2}$$ and adding this gives to the second inequality gives $$-\epsilon<L-\inf_{n>N}\{a_n\}<\epsilon$$ for $n>N$. So we have $\lim_{N \rightarrow \infty}\inf_{n>N}\{a_n\}=\lim_{N \rightarrow \infty}\sup_{n>N}\{a_n\}=L$. Now conversely assume that $\lim \inf=\lim \sup$. We have $$\inf_{n>N}\{a_n\} \leq a_n \leq \sup_{n>N}\{a_n\}$$ for $a_n \in \{a_n|n>N\}$. Since the limits of the left and right sides of the inequality are equal, the limit of the middle exists and equals that of the other two by the squeeze theorem (I've only seen the squeeze theorem proven for functions, but I believe it applies here since sequences are just functions with domains on the naturals). Therefore the sequence $\{a_n\}$ converges iff its $\lim \inf$ equals its $\lim \sup$ (and, as a corollary, the limit of the sequence always equals that of the infima and suprema). Please tell me if you see any holes or things I could've done better, as I've made many stupid mistakes in proofs before. Thanks for your help!","This problem is purely for my own benefit, so I'd appreciate it if you offer help but don't spoil the proof for me. I've worked out the following solution, but I want to make sure that my reasoning doesn't have any holes in it: Suppose we have the sequence $\{a_n\}$, and $\lim_{n \rightarrow \infty}\{a_n\}=L$. Then given $\epsilon>0$, we can choose $N \in \mathbb{N}$ such that $$\left| a_n-L \right|<\frac{\epsilon}{2}$$ for every term $a_n \in \{a_n\}$ such that $n>N$. Now for $\inf_{n>N}\{a_n\}$, (that is, to my understanding, the infimum of the sequence with the first N terms truncated) we must have some $a_{low} \in \{a_n|n>N\}$ such that $$a_{low}-\inf_{n>N}\{a_n\}<\frac{\epsilon}{2}$$ because otherwise $\inf_{n>N}\{a_n\}-\frac{\epsilon}{2}$ would be a greater lower bound for $\{a_n|n>N\}$. Similar reasoning demonstrates that we must have some $a_{high}$ such that $$\sup_{n>N}\{a_n\}-a_{high}<\frac{\epsilon}{2}$$ Now since $a_{high} \in \{a_n|n>N\}$, we must have $$-\epsilon<a_{high}-L<\epsilon$$ and, adding this inequality to the above one gives $$-\epsilon<\sup_{n>N}\{a_n\}-L<\epsilon$$ for $n>N$, and therefore $\lim_{N \rightarrow \infty}\sup_{n>N}\{a_n\}=L$. Likewise, since $a_{low} \in \{a_n|n>N\}$ we must have $$\left|a_{low}-L\right|=\left|L-a_{low}\right|<\frac{\epsilon}{2}$$ So $$-\frac{\epsilon}{2}<L-a_{low}<\frac{\epsilon}{2}$$ and adding this gives to the second inequality gives $$-\epsilon<L-\inf_{n>N}\{a_n\}<\epsilon$$ for $n>N$. So we have $\lim_{N \rightarrow \infty}\inf_{n>N}\{a_n\}=\lim_{N \rightarrow \infty}\sup_{n>N}\{a_n\}=L$. Now conversely assume that $\lim \inf=\lim \sup$. We have $$\inf_{n>N}\{a_n\} \leq a_n \leq \sup_{n>N}\{a_n\}$$ for $a_n \in \{a_n|n>N\}$. Since the limits of the left and right sides of the inequality are equal, the limit of the middle exists and equals that of the other two by the squeeze theorem (I've only seen the squeeze theorem proven for functions, but I believe it applies here since sequences are just functions with domains on the naturals). Therefore the sequence $\{a_n\}$ converges iff its $\lim \inf$ equals its $\lim \sup$ (and, as a corollary, the limit of the sequence always equals that of the infima and suprema). Please tell me if you see any holes or things I could've done better, as I've made many stupid mistakes in proofs before. Thanks for your help!",,"['real-analysis', 'sequences-and-series', 'proof-verification', 'convergence-divergence', 'limsup-and-liminf']"
99,why is this sequence convergent,why is this sequence convergent,,"Suppose to have two sequence $(a_n)_{n\geq1}$ and $(b_n)_{n\geq1}$ such that $a_n=\frac{1}{2}(a_{n-1}+b_{n-1})$. I want to prove that if $b_n\rightarrow0$ then $a_n\rightarrow0$. The only thing I was able to prove is that $a_n$ is bounded, in fact: $b_n$ is convergent and so bounded $|b_n|\leq M$. And so $|a_n|\leq |\frac{a_2}{2^{n-1}}+\frac{b_2}{2^{n-2}}+\cdots+\frac{b_{n-1}}{2}|\leq|\frac{a_2}{2^{n-2}}|+M\sum^\infty_{k=1}\frac{1}{2^k}$ and for great $n$ we have $|\frac{a_2}{2^{n-2}}|\leq\varepsilon$ Could you help me to continue (if I'm on the right track), please? I see that if we prove that $a_n$ has a limit $L$ then necessarily $L=0$ because $L$ must satisfy $L=\frac{1}{2}L$, but I don't know how to prove that it has a limit.","Suppose to have two sequence $(a_n)_{n\geq1}$ and $(b_n)_{n\geq1}$ such that $a_n=\frac{1}{2}(a_{n-1}+b_{n-1})$. I want to prove that if $b_n\rightarrow0$ then $a_n\rightarrow0$. The only thing I was able to prove is that $a_n$ is bounded, in fact: $b_n$ is convergent and so bounded $|b_n|\leq M$. And so $|a_n|\leq |\frac{a_2}{2^{n-1}}+\frac{b_2}{2^{n-2}}+\cdots+\frac{b_{n-1}}{2}|\leq|\frac{a_2}{2^{n-2}}|+M\sum^\infty_{k=1}\frac{1}{2^k}$ and for great $n$ we have $|\frac{a_2}{2^{n-2}}|\leq\varepsilon$ Could you help me to continue (if I'm on the right track), please? I see that if we prove that $a_n$ has a limit $L$ then necessarily $L=0$ because $L$ must satisfy $L=\frac{1}{2}L$, but I don't know how to prove that it has a limit.",,"['real-analysis', 'sequences-and-series', 'analysis']"
