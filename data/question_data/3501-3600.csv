,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Hölder continuous but not differentiable function,Hölder continuous but not differentiable function,,"How can it be proved that the function $$f(x)=\sum_{n=1}^{\infty}2^{-n\alpha}\cos(2^nx)$$ for $\alpha \in ]0,1[$ is $\alpha$-Hölder  continuous but not differentiable at any point of $[0,1]$? I tried to write down $f(x+h)-f(x)$ and use addition formulas for the cosine, but I don't obtain anything..","How can it be proved that the function $$f(x)=\sum_{n=1}^{\infty}2^{-n\alpha}\cos(2^nx)$$ for $\alpha \in ]0,1[$ is $\alpha$-Hölder  continuous but not differentiable at any point of $[0,1]$? I tried to write down $f(x+h)-f(x)$ and use addition formulas for the cosine, but I don't obtain anything..",,"['real-analysis', 'analysis']"
1,Show that every upper semi-continuous real function is measurable [duplicate],Show that every upper semi-continuous real function is measurable [duplicate],,"This question already has an answer here : Closed 12 years ago . Possible Duplicate: Subset of the preimage of a semicontinuous real function is Borel A real function $f$ on the line is upper semi-continuous at $x$, if for each $\epsilon > 0$, there exists $\delta > 0$ such that $|x-y|<\delta$ implies that $f(y) < f(x) + \epsilon$. Check that if $f$ is everywhere upper semi-continuous, then it is measurable. I could not do this question. Thanks and regards.","This question already has an answer here : Closed 12 years ago . Possible Duplicate: Subset of the preimage of a semicontinuous real function is Borel A real function $f$ on the line is upper semi-continuous at $x$, if for each $\epsilon > 0$, there exists $\delta > 0$ such that $|x-y|<\delta$ implies that $f(y) < f(x) + \epsilon$. Check that if $f$ is everywhere upper semi-continuous, then it is measurable. I could not do this question. Thanks and regards.",,"['real-analysis', 'measure-theory', 'semicontinuous-functions']"
2,Differentiable+Not monotone,Differentiable+Not monotone,,Is there a real function that is differentiable at any point but nowhere monotone?,Is there a real function that is differentiable at any point but nowhere monotone?,,['real-analysis']
3,What is the minimum value of $a$ such that $x^a \geq \ln(x)$ for all $x > 0$?,What is the minimum value of  such that  for all ?,a x^a \geq \ln(x) x > 0,"This is probably just elementary, but I don't know how to do it. I would like to find the minimum value of $a$ such that $x^a \geq \ln(x)$ for all $x > 0$. Numerically, I have found that this minimum value lies between 0.365 and 0.37 (i.e., $x^{0.37} > \ln(x)$ for all $x > 0$, but $x^{0.365}$ is not). Is there any analytical way to find out exactly this minimum value? EDIT: Based on the received answers, I finally came up with my own one as follows. Consider the function $f(x) = x^a - \ln(x).$ This function is convex in $x$, and hence, achieves the unique minimum as $x^*$ such that $f'(x^*) = 0.$ Solving that equation yields $$f_{\mathrm{min}} = \min\limits_{x>0} f(x) = \frac{\ln(a)+1}{a}.$$ Now, by letting $f_{min} = 0$, we get the desired value $a^* = 1/e.$ Thank everyone for the answers!","This is probably just elementary, but I don't know how to do it. I would like to find the minimum value of $a$ such that $x^a \geq \ln(x)$ for all $x > 0$. Numerically, I have found that this minimum value lies between 0.365 and 0.37 (i.e., $x^{0.37} > \ln(x)$ for all $x > 0$, but $x^{0.365}$ is not). Is there any analytical way to find out exactly this minimum value? EDIT: Based on the received answers, I finally came up with my own one as follows. Consider the function $f(x) = x^a - \ln(x).$ This function is convex in $x$, and hence, achieves the unique minimum as $x^*$ such that $f'(x^*) = 0.$ Solving that equation yields $$f_{\mathrm{min}} = \min\limits_{x>0} f(x) = \frac{\ln(a)+1}{a}.$$ Now, by letting $f_{min} = 0$, we get the desired value $a^* = 1/e.$ Thank everyone for the answers!",,"['calculus', 'real-analysis']"
4,"A generalized ""Rare"" integral involving $\operatorname{Li}_3$","A generalized ""Rare"" integral involving",\operatorname{Li}_3,"In my previous post , it can be shown that $$\int_{0}^{1}  \frac{\operatorname{Li}_2(-x)- \operatorname{Li}_2(1-x)+\ln(x)\ln(1+x)+\pi x\ln(1+x) -\pi x\ln(x)}{1+x^2}\frac{\text{d}x}{\sqrt{1-x^2} }  =\frac{\pi^3}{48\sqrt{2} }.$$ But how we verify this? $$\int_{0}^{1}  \frac{\operatorname{Li}_3(1-z)+\operatorname{Li}_3 \left ( \frac{1}{1+z}  \right ) +\frac{\pi^2}{3}\ln(1+z)  -\frac{\pi z}{2}\ln(1+z)^2-\frac{1}{6}\ln(1+z)^3 - \frac{\pi z}{2}\ln(z)^2+\pi z\ln(z)\ln(1+z) }{1+z^2}  \frac{\text{d}z}{\sqrt{1-z^2} }  =\frac{35\pi\zeta(3)}{64\sqrt{2} }+\frac{\pi^3}{32\sqrt{2} }\ln(2).$$ Where $\operatorname{Li}_3$ is trilogarithm and $\zeta(3)=\operatorname{Li}_3(1)$ in the principal branch. The same method seems not quite powerful. Any suggestion will be appreciated.","In my previous post , it can be shown that But how we verify this? Where is trilogarithm and in the principal branch. The same method seems not quite powerful. Any suggestion will be appreciated.","\int_{0}^{1} 
\frac{\operatorname{Li}_2(-x)-
\operatorname{Li}_2(1-x)+\ln(x)\ln(1+x)+\pi x\ln(1+x)
-\pi x\ln(x)}{1+x^2}\frac{\text{d}x}{\sqrt{1-x^2} } 
=\frac{\pi^3}{48\sqrt{2} }. \int_{0}^{1} 
\frac{\operatorname{Li}_3(1-z)+\operatorname{Li}_3
\left ( \frac{1}{1+z}  \right ) +\frac{\pi^2}{3}\ln(1+z) 
-\frac{\pi z}{2}\ln(1+z)^2-\frac{1}{6}\ln(1+z)^3
- \frac{\pi z}{2}\ln(z)^2+\pi z\ln(z)\ln(1+z) }{1+z^2} 
\frac{\text{d}z}{\sqrt{1-z^2} } 
=\frac{35\pi\zeta(3)}{64\sqrt{2} }+\frac{\pi^3}{32\sqrt{2} }\ln(2). \operatorname{Li}_3 \zeta(3)=\operatorname{Li}_3(1)","['real-analysis', 'integration', 'definite-integrals', 'closed-form', 'polylogarithm']"
5,Find the sum: $\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n$ [duplicate],Find the sum:  [duplicate],\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n,"This question already has an answer here : How to find sum of this series $\sum_{0}^{\infty} \frac{(n!)^{2}x^{n}}{(2n)!}$ (1 answer) Closed 7 months ago . The community reviewed whether to reopen this question 7 months ago and left it closed: Original close reason(s) were not resolved Find the sum: $$\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n$$ My try: I played a bit with the coefficient to make it look easier/familiar: First attempt: $$\begin{align} \sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n &= \sum_{n=0}^\infty \frac{n!}{2^n(2n-1)!!}x^n \\ &= \sum_{n=0}^\infty \frac{n!}{(2n-1)!!}\left(\frac x2\right)^n \end{align}$$ Second attempt: $$\begin{align} \sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n &= \sum_{n=0}^\infty \frac{n!\cdot n!}{(n+n)!}x^n \\ &= \sum_{n=0}^\infty \frac{1}{{2n \choose n}}x^n \end{align}$$ However, I could not proceed with any of them. Also, I have figured out that the convergence radius is $4$ . My research: I have also found the same sum has been discussed at AoPS , which unfortunately uses Beta function that my course has not covered yet. Entering the sum to Wolphram Alpha , I got the following output for the partial sum : $$\sum_{n=0}^k\frac{(n!)^2x^n}{(2n)!}=\frac{4\sqrt{x}\left(\sin^{-1}\left(\frac{\sqrt{x}}{2}\right)-\frac{2^{2k}k!(k+1)!B_\frac{x}{4}\left(k+\frac{1}{2},\frac{3}{2}\right)}{(2k)!}\right)}{(4-x)^{3/2}}+\frac{4}{4-x}.$$ My background: As I have already mentioned, I cannot use Gamma, Beta or similar functions. I only know about the convergence theorems on functional series and operations on them. So, I'm looking for some method that uses quite elementary tricks. Thanks in advance.","This question already has an answer here : How to find sum of this series $\sum_{0}^{\infty} \frac{(n!)^{2}x^{n}}{(2n)!}$ (1 answer) Closed 7 months ago . The community reviewed whether to reopen this question 7 months ago and left it closed: Original close reason(s) were not resolved Find the sum: My try: I played a bit with the coefficient to make it look easier/familiar: First attempt: Second attempt: However, I could not proceed with any of them. Also, I have figured out that the convergence radius is . My research: I have also found the same sum has been discussed at AoPS , which unfortunately uses Beta function that my course has not covered yet. Entering the sum to Wolphram Alpha , I got the following output for the partial sum : My background: As I have already mentioned, I cannot use Gamma, Beta or similar functions. I only know about the convergence theorems on functional series and operations on them. So, I'm looking for some method that uses quite elementary tricks. Thanks in advance.","\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n \begin{align}
\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n
&= \sum_{n=0}^\infty \frac{n!}{2^n(2n-1)!!}x^n \\
&= \sum_{n=0}^\infty \frac{n!}{(2n-1)!!}\left(\frac x2\right)^n
\end{align} \begin{align}
\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n
&= \sum_{n=0}^\infty \frac{n!\cdot n!}{(n+n)!}x^n \\
&= \sum_{n=0}^\infty \frac{1}{{2n \choose n}}x^n
\end{align} 4 \sum_{n=0}^k\frac{(n!)^2x^n}{(2n)!}=\frac{4\sqrt{x}\left(\sin^{-1}\left(\frac{\sqrt{x}}{2}\right)-\frac{2^{2k}k!(k+1)!B_\frac{x}{4}\left(k+\frac{1}{2},\frac{3}{2}\right)}{(2k)!}\right)}{(4-x)^{3/2}}+\frac{4}{4-x}.","['real-analysis', 'calculus', 'sequences-and-series', 'summation', 'power-series']"
6,Computing $\sum_{n=1}^\infty\frac{2^{2n}H_{n+1}}{(n+1)^2{2n\choose n}}$,Computing,\sum_{n=1}^\infty\frac{2^{2n}H_{n+1}}{(n+1)^2{2n\choose n}},"An advanced sum proposed by Cornel Valean: $$S=\sum_{n=1}^\infty\frac{2^{2n}H_{n+1}}{(n+1)^2{2n\choose n}}$$ $$=4\text{Li}_4\left(\frac12\right)-\frac12\zeta(4)+\frac72\zeta(3)-4\ln^22\zeta(2)+6\ln2\zeta(2)+\frac16\ln^42-1$$ I managed to find the integral representation of $\ \displaystyle\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^2{2n\choose n}}\ $ but not $S$ : Since $$\frac{\arcsin x}{\sqrt{1-x^2}}=\sum_{n=1}^\infty\frac{(2x)^{2n-1}}{n{2n\choose n}}$$ we can write $$\frac{2\sqrt{x}\arcsin \sqrt{x}}{\sqrt{1-x}}=\sum_{n=1}^\infty\frac{2^{2n}x^{n}}{n{2n\choose n}}$$ now multiply both sides by $-\frac{\ln(1-x)}{x}$ then $\int_0^1$ and use that $-\int_0^1 x^{n-1}\ln(1-x)dx=\frac{H_n}{n}$ we have $$\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^2{2n\choose n}}=-2\int_0^1 \frac{\arcsin \sqrt{x}\ln(1-x)}{\sqrt{x}\sqrt{1-x}}dx\tag1$$ But I could not get the integral representation of $S$ . Any idea? In case you find the integral, I prefer solutions that do not use contour integration or you can leave it to me to give it a try. Thank you. In case the reader is curious about computing the integral in $(1)$ , set $x=\sin^2\theta$ then use the Fourier series of $\ln(\cos \theta)$ .","An advanced sum proposed by Cornel Valean: I managed to find the integral representation of but not : Since we can write now multiply both sides by then and use that we have But I could not get the integral representation of . Any idea? In case you find the integral, I prefer solutions that do not use contour integration or you can leave it to me to give it a try. Thank you. In case the reader is curious about computing the integral in , set then use the Fourier series of .",S=\sum_{n=1}^\infty\frac{2^{2n}H_{n+1}}{(n+1)^2{2n\choose n}} =4\text{Li}_4\left(\frac12\right)-\frac12\zeta(4)+\frac72\zeta(3)-4\ln^22\zeta(2)+6\ln2\zeta(2)+\frac16\ln^42-1 \ \displaystyle\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^2{2n\choose n}}\  S \frac{\arcsin x}{\sqrt{1-x^2}}=\sum_{n=1}^\infty\frac{(2x)^{2n-1}}{n{2n\choose n}} \frac{2\sqrt{x}\arcsin \sqrt{x}}{\sqrt{1-x}}=\sum_{n=1}^\infty\frac{2^{2n}x^{n}}{n{2n\choose n}} -\frac{\ln(1-x)}{x} \int_0^1 -\int_0^1 x^{n-1}\ln(1-x)dx=\frac{H_n}{n} \sum_{n=1}^\infty\frac{2^{2n}H_n}{n^2{2n\choose n}}=-2\int_0^1 \frac{\arcsin \sqrt{x}\ln(1-x)}{\sqrt{x}\sqrt{1-x}}dx\tag1 S (1) x=\sin^2\theta \ln(\cos \theta),"['real-analysis', 'integration', 'sequences-and-series', 'binomial-coefficients', 'harmonic-numbers']"
7,A challenging problem on continuity-MADHAVA-2020.,A challenging problem on continuity-MADHAVA-2020.,,"Suppose $f$ be a continuous function on $(0,\infty)$ such that $f(\frac{x}{x+1})=f(x)+2$ .Is the function monotonically decreasing?I have tried to prove it in different ways but still I could not find any way out.Also how to show $f(x) \to \infty$ as $x\to 0+$ .Actually it is a question from MADHAVA-2020 which took place on 12 Jan,2020.","Suppose be a continuous function on such that .Is the function monotonically decreasing?I have tried to prove it in different ways but still I could not find any way out.Also how to show as .Actually it is a question from MADHAVA-2020 which took place on 12 Jan,2020.","f (0,\infty) f(\frac{x}{x+1})=f(x)+2 f(x) \to \infty x\to 0+","['real-analysis', 'sequences-and-series', 'limits', 'continuity', 'problem-solving']"
8,How to prove that the functional equation $f(x)+f(y)=f(\frac{xf'(x)+yf'(y)}{f'(x)+f'(y)})+f(\frac{x+y}2)$ is verified only by some basic functions?,How to prove that the functional equation  is verified only by some basic functions?,f(x)+f(y)=f(\frac{xf'(x)+yf'(y)}{f'(x)+f'(y)})+f(\frac{x+y}2),"It's an question about functional analysis : Let $x,y>0$ and $f$ a continuous and convex/concave function on $(0,\infty)$ with $f'(x)\neq 0$ on the previous interval  and: $$f(x)+f(y)=f\left(\frac{xf'(x)+yf'(y)}{f'(x)+f'(y)}\right)+f\left(\frac{x+y}{2}\right)$$ My claim is : The only function verifying this equation are $$\pm (cx+b)$$ $$\pm (\frac{c}{x}+b)$$ $$\pm \ln(x)$$ It's not hard to check that the functions above verify the functional equation . Edit : It seems that the only function wich verify the functional above are reciprocally convex/concave . My question : How to prove it ? Is the sentence of my edit true ? Thanks a lot for your time .",It's an question about functional analysis : Let and a continuous and convex/concave function on with on the previous interval  and: My claim is : The only function verifying this equation are It's not hard to check that the functions above verify the functional equation . Edit : It seems that the only function wich verify the functional above are reciprocally convex/concave . My question : How to prove it ? Is the sentence of my edit true ? Thanks a lot for your time .,"x,y>0 f (0,\infty) f'(x)\neq 0 f(x)+f(y)=f\left(\frac{xf'(x)+yf'(y)}{f'(x)+f'(y)}\right)+f\left(\frac{x+y}{2}\right) \pm (cx+b) \pm (\frac{c}{x}+b) \pm \ln(x)","['real-analysis', 'functional-equations']"
9,$\int_0^1\int_0^1\binom{\text{something}}{\text{something}}\binom{\text{something}}{\text{something}}\cdot \text{something }dxdy$ with closed-form,with closed-form,\int_0^1\int_0^1\binom{\text{something}}{\text{something}}\binom{\text{something}}{\text{something}}\cdot \text{something }dxdy,"I've calculated an approximation of integrals like than $$\int_0^1\int_0^1\binom{f(x)}{f(y)}\binom{f(y)}{f(x)}dxdy\tag{1}$$ for simple functions $f(x)$. I don't know if some of these were in the literature or have a nice closed-form. Question . I would like to know how to create, if it is feasible, nice examples of double integrals of binomials similar than $(1)$. Do you know how to calculate a nice example using different functions   $$\int_0^1\int_0^1\binom{\text{something}}{\text{something}}\binom{\text{something}}{\text{something}}\cdot \text{something }dxdy\,?\tag{2}$$   If you know an example from the literature with a nice closed-form, please  answer this question as a reference request, then I am going to try search such literature and read the example. Many thanks. Your closed-form can be expressed as a series of special functions ( I'm especially interested in how to create such an example).","I've calculated an approximation of integrals like than $$\int_0^1\int_0^1\binom{f(x)}{f(y)}\binom{f(y)}{f(x)}dxdy\tag{1}$$ for simple functions $f(x)$. I don't know if some of these were in the literature or have a nice closed-form. Question . I would like to know how to create, if it is feasible, nice examples of double integrals of binomials similar than $(1)$. Do you know how to calculate a nice example using different functions   $$\int_0^1\int_0^1\binom{\text{something}}{\text{something}}\binom{\text{something}}{\text{something}}\cdot \text{something }dxdy\,?\tag{2}$$   If you know an example from the literature with a nice closed-form, please  answer this question as a reference request, then I am going to try search such literature and read the example. Many thanks. Your closed-form can be expressed as a series of special functions ( I'm especially interested in how to create such an example).",,"['real-analysis', 'integration']"
10,"Proving that, $|f'(x)-f'(y)|\le k|x-y| \implies (f'(x))^2< 2 kf(x) $","Proving that,",|f'(x)-f'(y)|\le k|x-y| \implies (f'(x))^2< 2 kf(x) ,"Let $f:\Bbb R\to (0,\infty)$ be a differentiable function such that for some constant $k$ we have, $$|f'(x)-f'(y)|\le k|x-y|$$   for all $x,y \in\Bbb R.$ Then prove that, $$(f'(x))^2<2 kf(x).$$ Due to lack of arguments I could not proof this inequality. But rather I proved that is true for the particular function, $$f(x) =\cos^2(x)+1\implies f'(x) = -\sin(2x).$$ And we readily have,$$|\sin(2x)-\sin(2y)| = \left|\int_{2x}^{2y}\cos t dt\right|\le 2|x-y|$$ as well $$(f'(x))^2 = 4 \sin^2 x\cos^2x < 4(\cos^2x +1) = 4f(x).$$ It is also true for the function $x\mapsto \sin^2x +1.$ From these example I don't see how to prove the general case. Any hint will be welcome.","Let $f:\Bbb R\to (0,\infty)$ be a differentiable function such that for some constant $k$ we have, $$|f'(x)-f'(y)|\le k|x-y|$$   for all $x,y \in\Bbb R.$ Then prove that, $$(f'(x))^2<2 kf(x).$$ Due to lack of arguments I could not proof this inequality. But rather I proved that is true for the particular function, $$f(x) =\cos^2(x)+1\implies f'(x) = -\sin(2x).$$ And we readily have,$$|\sin(2x)-\sin(2y)| = \left|\int_{2x}^{2y}\cos t dt\right|\le 2|x-y|$$ as well $$(f'(x))^2 = 4 \sin^2 x\cos^2x < 4(\cos^2x +1) = 4f(x).$$ It is also true for the function $x\mapsto \sin^2x +1.$ From these example I don't see how to prove the general case. Any hint will be welcome.",,"['calculus', 'real-analysis', 'analysis', 'functions', 'lipschitz-functions']"
11,Theorem 7.26 in Baby Rudin: The Stone Weierstrass Theorem,Theorem 7.26 in Baby Rudin: The Stone Weierstrass Theorem,,"Here is Theorem 7.26 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f$ is a continuous complex function on $[a, b]$ , there exists a sequence of polynomials $P_n$ such that $$ \lim_{n \to \infty} P_n(x) = f(x) $$ uniformly on $[a, b]$ . If $f$ is real, the $P_n$ may be taken real. Here is Rudin's proof: We may assume, without loss of generality that $[a, b] = [0, 1]$ . [I would like to have an explicit argument on how the truth of this result for the unit closed interval $[0, 1]$ would lead to the truth of this result for an arbitrary closed interval $[a, b]$ .] We may also assume that $f(0) = f(1) = 0$ . For if the theorem is proved for this case, consider $$ g(x) = f(x) - f(0) - x [ f(1) - f(0) ] \qquad (0 \leq x \leq 1). $$ Here $g(0) = g(1) = 0$ , and if $g$ can be obtained as the limit of a uniformly convergent sequence of polynomials, it is clear that the same is true for $f$ , since $f-g$ is a polynomial. Furthermore, we define $f(x)$ to be zero for $x$ outside $[0, 1]$ . Then $f$ is uniformly continuous on the whole line. [How is $f$ uniformly continuous? How to show this rigorously?] We put $$\tag{47}  Q_n(x) = c_n \left( 1- x^2 \right)^n \qquad (n = 1, 2, 3, \ldots), $$ where $c_n$ is chosen so that $$ \tag{48} \int_{-1}^1 Q_n(x) \ \mathrm{d} x = 1 \qquad (n = 1, 2, 3, \ldots). $$ We need some information about the order of magnitude of $c_n$ . Since $$  \begin{align}  \int_{-1}^1 \left( 1-x^2 \right)^n \ \mathrm{d} x &= 2 \int_0^1 \left( 1-x^2 \right)^n \ \mathrm{d} x \\  &\geq 2 \int_0^{1/\sqrt{n}} \left( 1-x^2 \right)^n \ \mathrm{d} x \\ &\geq 2 \int_0^{1/\sqrt{n}} \left( 1- n x^2 \right) \ \mathrm{d} x \\ &= \frac{4}{3 \sqrt{n} } \\ &> \frac{1}{ \sqrt{n} },  \end{align} $$ it follows from (48) that $$ \tag{49} c_n < \sqrt{n}. $$ The inequality $\left( 1-x^2 \right)^n \geq 1-nx^2$ which we used above is easily shown to be true by considering the function $$ \left( 1- x^2 \right)^n - 1+nx^2 $$ which is zero at $x= 0$ and whose derivative is positive in $(0, 1)$ . For any $\delta > 0$ , (49) implies $$ \tag{50} Q_n(x) \leq \sqrt{n} \left( 1- \delta^2 \right)^n \qquad ( \delta \leq \lvert x \rvert \leq 1), $$ so that $Q_n \to 0$ uniformly in $\delta \leq \lvert x \rvert \leq 1$ . [Is this fact really needed in this proof?] Now set $$ \tag{51}  P_n(x) = \int_{-1}^1 f(x+t) Q_n (t) \ \mathrm{d} t \qquad (0 \leq x \leq 1). $$ Our assumptions about $f$ show, by a simple change of variable, that $$ P_n(x) = \int_{-x}^{1-x} f(x+t) Q_n(t) \ \mathrm{d} t = \int_0^1 f(t) Q_n(t-x) \ \mathrm{d} t, $$ and the last integral is clearly a polynomial in $x$ . [How to demonstrate this explicitly?] Thus $\left\{ P_n \right\}$ is a sequence of polynomials, which are real if $f$ is real. Given $\varepsilon > 0$ , we choose $\delta > 0$ such that $\lvert y-x \rvert < \delta$ implies $$ \lvert f(y) - f(x) \rvert < \frac{\varepsilon}{2}. $$ Let $M = \sup \lvert f(x) \rvert$ . Using (48), (50), and the fact that $Q_n(x) \geq 0$ , we see that for $0 \leq x \leq 1$ , $$  \begin{align} & \ \ \  \left\lvert P_n(x) - f(x) \right\rvert \\  &= \left\lvert \int_{-1}^1 [ f(x+t) - f(x) ] Q_n(t) \ \mathrm{d} t \right\rvert \\ &\leq \int_{-1}^1 \lvert f(x+t) - f(x) \rvert Q_n(t) \ \mathrm{d} t \\ &\leq 2M \int_{-1}^{-\delta} Q_n(t) \ \mathrm{d} t + \frac{\varepsilon}{2} \int_{-\delta}^\delta Q_n(t) \ \mathrm{d} t + 2 M \int_\delta^1 Q_n(t) \ \mathrm{d} t \\ &\leq 4M \sqrt{n} \left( 1 - \delta^2 \right)^n + \frac{\varepsilon}{2} \\ &< \varepsilon \end{align} $$ for all large enough $n$ , which proves the theorem. [In the hindsight, I think in the last chain we are better served with a $\delta$ such that $0 < \delta < 1$ . Am I right?] I have here reproduced Rudin's proof and (through my questions and remarks enclosed within pairs of brackets) have asked for clarification of those points in the proof that cause me confusion. Hope the learned Math Stack Exchange community will come to my rescue!! Is (are)  there any easier (or alternative) proof(s) of this very theorem? If so, I would be highly appreciative of any references to such a proof (or proofs)!","Here is Theorem 7.26 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If is a continuous complex function on , there exists a sequence of polynomials such that uniformly on . If is real, the may be taken real. Here is Rudin's proof: We may assume, without loss of generality that . [I would like to have an explicit argument on how the truth of this result for the unit closed interval would lead to the truth of this result for an arbitrary closed interval .] We may also assume that . For if the theorem is proved for this case, consider Here , and if can be obtained as the limit of a uniformly convergent sequence of polynomials, it is clear that the same is true for , since is a polynomial. Furthermore, we define to be zero for outside . Then is uniformly continuous on the whole line. [How is uniformly continuous? How to show this rigorously?] We put where is chosen so that We need some information about the order of magnitude of . Since it follows from (48) that The inequality which we used above is easily shown to be true by considering the function which is zero at and whose derivative is positive in . For any , (49) implies so that uniformly in . [Is this fact really needed in this proof?] Now set Our assumptions about show, by a simple change of variable, that and the last integral is clearly a polynomial in . [How to demonstrate this explicitly?] Thus is a sequence of polynomials, which are real if is real. Given , we choose such that implies Let . Using (48), (50), and the fact that , we see that for , for all large enough , which proves the theorem. [In the hindsight, I think in the last chain we are better served with a such that . Am I right?] I have here reproduced Rudin's proof and (through my questions and remarks enclosed within pairs of brackets) have asked for clarification of those points in the proof that cause me confusion. Hope the learned Math Stack Exchange community will come to my rescue!! Is (are)  there any easier (or alternative) proof(s) of this very theorem? If so, I would be highly appreciative of any references to such a proof (or proofs)!","f [a, b] P_n  \lim_{n \to \infty} P_n(x) = f(x)  [a, b] f P_n [a, b] = [0, 1] [0, 1] [a, b] f(0) = f(1) = 0  g(x) = f(x) - f(0) - x [ f(1) - f(0) ] \qquad (0 \leq x \leq 1).  g(0) = g(1) = 0 g f f-g f(x) x [0, 1] f f \tag{47}  Q_n(x) = c_n \left( 1- x^2 \right)^n \qquad (n = 1, 2, 3, \ldots),  c_n  \tag{48} \int_{-1}^1 Q_n(x) \ \mathrm{d} x = 1 \qquad (n = 1, 2, 3, \ldots).  c_n  
\begin{align} 
\int_{-1}^1 \left( 1-x^2 \right)^n \ \mathrm{d} x &= 2 \int_0^1 \left( 1-x^2 \right)^n \ \mathrm{d} x \\
 &\geq 2 \int_0^{1/\sqrt{n}} \left( 1-x^2 \right)^n \ \mathrm{d} x \\
&\geq 2 \int_0^{1/\sqrt{n}} \left( 1- n x^2 \right) \ \mathrm{d} x \\
&= \frac{4}{3 \sqrt{n} } \\
&> \frac{1}{ \sqrt{n} }, 
\end{align}
  \tag{49} c_n < \sqrt{n}.  \left( 1-x^2 \right)^n \geq 1-nx^2  \left( 1- x^2 \right)^n - 1+nx^2  x= 0 (0, 1) \delta > 0  \tag{50} Q_n(x) \leq \sqrt{n} \left( 1- \delta^2 \right)^n \qquad ( \delta \leq \lvert x \rvert \leq 1),  Q_n \to 0 \delta \leq \lvert x \rvert \leq 1  \tag{51}  P_n(x) = \int_{-1}^1 f(x+t) Q_n (t) \ \mathrm{d} t \qquad (0 \leq x \leq 1).  f  P_n(x) = \int_{-x}^{1-x} f(x+t) Q_n(t) \ \mathrm{d} t = \int_0^1 f(t) Q_n(t-x) \ \mathrm{d} t,  x \left\{ P_n \right\} f \varepsilon > 0 \delta > 0 \lvert y-x \rvert < \delta  \lvert f(y) - f(x) \rvert < \frac{\varepsilon}{2}.  M = \sup \lvert f(x) \rvert Q_n(x) \geq 0 0 \leq x \leq 1  
\begin{align}
& \ \ \  \left\lvert P_n(x) - f(x) \right\rvert \\ 
&= \left\lvert \int_{-1}^1 [ f(x+t) - f(x) ] Q_n(t) \ \mathrm{d} t \right\rvert \\
&\leq \int_{-1}^1 \lvert f(x+t) - f(x) \rvert Q_n(t) \ \mathrm{d} t \\
&\leq 2M \int_{-1}^{-\delta} Q_n(t) \ \mathrm{d} t + \frac{\varepsilon}{2} \int_{-\delta}^\delta Q_n(t) \ \mathrm{d} t + 2 M \int_\delta^1 Q_n(t) \ \mathrm{d} t \\
&\leq 4M \sqrt{n} \left( 1 - \delta^2 \right)^n + \frac{\varepsilon}{2} \\
&< \varepsilon
\end{align}
 n \delta 0 < \delta < 1","['real-analysis', 'sequences-and-series', 'analysis', 'proof-explanation', 'uniform-convergence']"
12,How does one prove that $e$ exists?,How does one prove that  exists?,e,"In my calculus class, $e$ was defined to be the number such that $\frac{d}{dx}e^x = e^x$. From the definition of the derivative, we have \begin{align*} \frac{d}{dx}a^x &= \lim_{h \to 0} \frac{a^{x+h} - a^x}{h}\\ &= a^x \lim_{h \to 0} \frac{a^h - 1}{h} \end{align*} Thus $e$ is the number such that $$ \lim_{h \to 0} \frac{e^h - 1}{h} = 1 $$ But how is it proven that there exists such number?","In my calculus class, $e$ was defined to be the number such that $\frac{d}{dx}e^x = e^x$. From the definition of the derivative, we have \begin{align*} \frac{d}{dx}a^x &= \lim_{h \to 0} \frac{a^{x+h} - a^x}{h}\\ &= a^x \lim_{h \to 0} \frac{a^h - 1}{h} \end{align*} Thus $e$ is the number such that $$ \lim_{h \to 0} \frac{e^h - 1}{h} = 1 $$ But how is it proven that there exists such number?",,"['calculus', 'real-analysis']"
13,"If $f(x)=0 \implies f'(x)>0$, is the zero set of $f$ a single point?","If , is the zero set of  a single point?",f(x)=0 \implies f'(x)>0 f,"Let $f:\mathbb R \to \mathbb R$ be a real-valued differentiable function. Suppose that $f'(x)>0$ for every $x$ such that $f(x)=0$. Does it follow that the number of zeroes of $f$ is at most one? This sounds quite reasonable to me: it seems intuitive that if $x_1, x_2$ are two different zeroes of $f$, then a third zero with negative derivative should lie between them. I can't seem to adapt this argument to a solid proof though. Is there any way to prove (or disprove!) this fact in a quick fashion?","Let $f:\mathbb R \to \mathbb R$ be a real-valued differentiable function. Suppose that $f'(x)>0$ for every $x$ such that $f(x)=0$. Does it follow that the number of zeroes of $f$ is at most one? This sounds quite reasonable to me: it seems intuitive that if $x_1, x_2$ are two different zeroes of $f$, then a third zero with negative derivative should lie between them. I can't seem to adapt this argument to a solid proof though. Is there any way to prove (or disprove!) this fact in a quick fashion?",,"['real-analysis', 'derivatives', 'roots']"
14,"Derivative of the magnitude of a vector. Does it exist, or not?","Derivative of the magnitude of a vector. Does it exist, or not?",,"I have a puzzling situation involving derivatives. I want to derivate: $$ \frac{d}{dx}| \mathbf F(x)| $$ This was actually something involving physics. Lets be 2-dimensional for simplicity. Let a particle be at position $\mathbf r = (x, y)$. The distance $s$ of the particle from point $(0, 0)$ is simply $s = |\mathbf r|$. I want to calculate how that distance changes over time. $$ \frac{ds}{dt} =  \frac{d}{dt}|\mathbf r| =  \frac{d}{dt}\sqrt{x(t)^2 + y(t)^2} =  \frac{1}{\sqrt{x(t)^2 + y(t)^2}}\left(x\frac{dx}{dt} + y\frac{dy}{dt}\right) = \frac{1}{|\mathbf r|}\left(x\frac{dx}{dt} + y\frac{dy}{dt}\right)  $$ As you can see, $ds/dt$ is not defined when $|\mathbf r| = 0$. I can't see why. On physics point of view, the particle should always travel continuously in the plane (assuming the path it makes is continuous and fully differentiable). Why is the distance variation undefined? Assume for instance, I have a table, and $(x, y)$ is the position of my fingers. I can't see why it wouldn't exist. Hypothesis: Notice that, by description I told, the curve $(x, y)$ is continuous on all points, and smooth/differentiable on all points. Thus, $x(t), y(t), x'(t), y'(t)$ is well defined, for all points. If you want, consider them to be class $C^\infty$. My question : Does this derivative exist or not when $|\mathbf r| = 0$? What is the value/evaluation of such derivative in an arbitrary given period $t_0$ when $|\mathbf r| = 0$? Considering $x(t) = t^2$ and $y(t) = t^2$, we get $s$ proportional to $t^2$, and thus its derivative exists at $t=0$ with the derivative having a well defined value of zero.","I have a puzzling situation involving derivatives. I want to derivate: $$ \frac{d}{dx}| \mathbf F(x)| $$ This was actually something involving physics. Lets be 2-dimensional for simplicity. Let a particle be at position $\mathbf r = (x, y)$. The distance $s$ of the particle from point $(0, 0)$ is simply $s = |\mathbf r|$. I want to calculate how that distance changes over time. $$ \frac{ds}{dt} =  \frac{d}{dt}|\mathbf r| =  \frac{d}{dt}\sqrt{x(t)^2 + y(t)^2} =  \frac{1}{\sqrt{x(t)^2 + y(t)^2}}\left(x\frac{dx}{dt} + y\frac{dy}{dt}\right) = \frac{1}{|\mathbf r|}\left(x\frac{dx}{dt} + y\frac{dy}{dt}\right)  $$ As you can see, $ds/dt$ is not defined when $|\mathbf r| = 0$. I can't see why. On physics point of view, the particle should always travel continuously in the plane (assuming the path it makes is continuous and fully differentiable). Why is the distance variation undefined? Assume for instance, I have a table, and $(x, y)$ is the position of my fingers. I can't see why it wouldn't exist. Hypothesis: Notice that, by description I told, the curve $(x, y)$ is continuous on all points, and smooth/differentiable on all points. Thus, $x(t), y(t), x'(t), y'(t)$ is well defined, for all points. If you want, consider them to be class $C^\infty$. My question : Does this derivative exist or not when $|\mathbf r| = 0$? What is the value/evaluation of such derivative in an arbitrary given period $t_0$ when $|\mathbf r| = 0$? Considering $x(t) = t^2$ and $y(t) = t^2$, we get $s$ proportional to $t^2$, and thus its derivative exists at $t=0$ with the derivative having a well defined value of zero.",,"['calculus', 'real-analysis', 'derivatives']"
15,Asymptotic behaviour of sum over the inverse japanese symbol,Asymptotic behaviour of sum over the inverse japanese symbol,,"I am interested in the asymptotic behavior of the sum $$\sum_{m=1}^M\frac{1}{\sqrt{m^2+\omega}}$$ for $1>\omega>0$ in the Limit $M\to\infty$ up to order $\mathcal{O}(M^{-1})$. The first thing I did was splitting the sum as follows: $$\sum_{m=1}^M\frac{1}{\sqrt{m^2+\omega}}=\sum_{m=1}^M\frac{1}{m}+\sum_{m=1}^M\left(\frac{1}{\sqrt{m^2+\omega}}-\frac{1}{m}\right)$$ For the first sum I use the Euler approximation to get $$\sum_{m=1}^M\frac{1}{m}=\log(M)+\gamma_E+\mathcal{O}(M^{-1}).$$ Now I suspect that the remainder goes like $$\sum_{m=1}^M\left(\frac{1}{\sqrt{m^2+\omega}}-\frac{1}{m}\right)=c(\omega)+\mathcal{O}(M^{-1}).$$ The question is, how can I explicitly compute $c(\omega)$?","I am interested in the asymptotic behavior of the sum $$\sum_{m=1}^M\frac{1}{\sqrt{m^2+\omega}}$$ for $1>\omega>0$ in the Limit $M\to\infty$ up to order $\mathcal{O}(M^{-1})$. The first thing I did was splitting the sum as follows: $$\sum_{m=1}^M\frac{1}{\sqrt{m^2+\omega}}=\sum_{m=1}^M\frac{1}{m}+\sum_{m=1}^M\left(\frac{1}{\sqrt{m^2+\omega}}-\frac{1}{m}\right)$$ For the first sum I use the Euler approximation to get $$\sum_{m=1}^M\frac{1}{m}=\log(M)+\gamma_E+\mathcal{O}(M^{-1}).$$ Now I suspect that the remainder goes like $$\sum_{m=1}^M\left(\frac{1}{\sqrt{m^2+\omega}}-\frac{1}{m}\right)=c(\omega)+\mathcal{O}(M^{-1}).$$ The question is, how can I explicitly compute $c(\omega)$?",,"['real-analysis', 'summation', 'asymptotics']"
16,Examples of statements that are true for real analytic functions but false for smooth functions,Examples of statements that are true for real analytic functions but false for smooth functions,,"I'm writing because I don't know the usefulness of real analytic functions. I mean, I know that analyticity is something more respect differentiable ($C^\infty$ function), but I don't have in mind a result, which is true only for real analytic functions, and then become false for $C^\infty$ functions which are not analytic.","I'm writing because I don't know the usefulness of real analytic functions. I mean, I know that analyticity is something more respect differentiable ($C^\infty$ function), but I don't have in mind a result, which is true only for real analytic functions, and then become false for $C^\infty$ functions which are not analytic.",,"['real-analysis', 'analysis', 'functional-analysis']"
17,Don't understand proof that interior of a set is open,Don't understand proof that interior of a set is open,,"Say we want to show that the interior of a set $A$ is open. If $x \in Int(A)$ , then there exists an open ball $B_r(x) \subseteq A$ . Since $B_r(x)$ is open, $y \in B_r(x)$ also has an open ball $B_s(y) \subseteq B_r(x) \subseteq A$ , so $y \in Int(A)$ . Now, somehow we have to show that the ball $B_r(x) \subseteq Int(A)$ , and that would complete the proof. All of the proofs I read say this is obvious, but I don't see how $B_r(x) \subseteq Int(A)$ immediately follows here.","Say we want to show that the interior of a set is open. If , then there exists an open ball . Since is open, also has an open ball , so . Now, somehow we have to show that the ball , and that would complete the proof. All of the proofs I read say this is obvious, but I don't see how immediately follows here.",A x \in Int(A) B_r(x) \subseteq A B_r(x) y \in B_r(x) B_s(y) \subseteq B_r(x) \subseteq A y \in Int(A) B_r(x) \subseteq Int(A) B_r(x) \subseteq Int(A),"['real-analysis', 'general-topology', 'elementary-set-theory', 'solution-verification']"
18,Solve $x^5 + x - 1 = 0$,Solve,x^5 + x - 1 = 0,"Solve $x^5 +x - 1 = 0$ I am simply curious to see how the solution would go, since it is a quintic; it cannot be done by regular methods. I'm just curious to see what people would come up with, and I can not solve the equation. Thanks!","Solve I am simply curious to see how the solution would go, since it is a quintic; it cannot be done by regular methods. I'm just curious to see what people would come up with, and I can not solve the equation. Thanks!",x^5 +x - 1 = 0,"['real-analysis', 'algebra-precalculus', 'analysis', 'polynomials', 'roots']"
19,"Is $\cup_{k=1}^\infty (r_k-\frac{1}{k}, r_k+\frac{1}{k}) = \mathbb{R}$?",Is ?,"\cup_{k=1}^\infty (r_k-\frac{1}{k}, r_k+\frac{1}{k}) = \mathbb{R}","Let $r_k$ be the rational numbers in $\mathbb{R}$. (1).Is $\cup_{k=1}^\infty (r_k-\frac{1}{k^2}, r_k+\frac{1}{k^2}) = \mathbb{R}$? (2).Is $\cup_{k=1}^\infty (r_k-\frac{1}{k}, r_k+\frac{1}{k}) = \mathbb{R}$? (1).Because $m(\mathbb{R})=+\infty, \sum_{k=1}^\infty \frac{1}{k^2}<+\infty$, so $\mathbb{R} \setminus\cup_{k=1}^\infty (r_k-\frac{1}{k^2}, r_k+\frac{1}{k^2})\neq \Phi  $ (2) What about (2)?","Let $r_k$ be the rational numbers in $\mathbb{R}$. (1).Is $\cup_{k=1}^\infty (r_k-\frac{1}{k^2}, r_k+\frac{1}{k^2}) = \mathbb{R}$? (2).Is $\cup_{k=1}^\infty (r_k-\frac{1}{k}, r_k+\frac{1}{k}) = \mathbb{R}$? (1).Because $m(\mathbb{R})=+\infty, \sum_{k=1}^\infty \frac{1}{k^2}<+\infty$, so $\mathbb{R} \setminus\cup_{k=1}^\infty (r_k-\frac{1}{k^2}, r_k+\frac{1}{k^2})\neq \Phi  $ (2) What about (2)?",,['real-analysis']
20,Function such that zeros$=$order of the derivative,Function such that zerosorder of the derivative,=,"Does there exist a function $f\in C^n(\mathbb{R},\mathbb{R})$ for $n\ge2$ such that $f^{(n)}$ has exactly $n$ zeros, $f^{(n-1)}$ has exactly $n-1$ zeros and so on ? Where $f^{(n)}$ is the nth derivative of $f$ This question arose from lot of question asked here on MSE.","Does there exist a function $f\in C^n(\mathbb{R},\mathbb{R})$ for $n\ge2$ such that $f^{(n)}$ has exactly $n$ zeros, $f^{(n-1)}$ has exactly $n-1$ zeros and so on ? Where $f^{(n)}$ is the nth derivative of $f$ This question arose from lot of question asked here on MSE.",,['real-analysis']
21,Hausdorff metric and Vietoris topology,Hausdorff metric and Vietoris topology,,"I am supposed to show that on a compact metric space, the Hausdorff metric and the Vietoris topology induce the same topology. Does anybody know how this can be done? I wanted to start by showing that they contain the same basis elements, but this was not successful.","I am supposed to show that on a compact metric space, the Hausdorff metric and the Vietoris topology induce the same topology. Does anybody know how this can be done? I wanted to start by showing that they contain the same basis elements, but this was not successful.",,['real-analysis']
22,Continuity of absolute value of a function,Continuity of absolute value of a function,,"Let $f(x)$ be a continuous function. Prove that $\left|f(x)\right|$ is also continuous. Is it correct to say that, by the reverse triangle inequality, $\left|f(x)-f(c)\right| \geq \left|f(x)\right|-\left|f(c)\right|$ in all cases, so we will always have that for all $\left|x-c\right|<\delta$ implies $\left|f(x)-f(c)\right|\leq \epsilon$? I am not sure if my solution adequately uses the functional form, since I just used the equation and not my actual function. Please help with the proper solution!","Let $f(x)$ be a continuous function. Prove that $\left|f(x)\right|$ is also continuous. Is it correct to say that, by the reverse triangle inequality, $\left|f(x)-f(c)\right| \geq \left|f(x)\right|-\left|f(c)\right|$ in all cases, so we will always have that for all $\left|x-c\right|<\delta$ implies $\left|f(x)-f(c)\right|\leq \epsilon$? I am not sure if my solution adequately uses the functional form, since I just used the equation and not my actual function. Please help with the proper solution!",,"['real-analysis', 'general-topology', 'continuity', 'absolute-value']"
23,Find a solution for $f\left(\frac{1}{x}\right)+f(x+1)=x$,Find a solution for,f\left(\frac{1}{x}\right)+f(x+1)=x,"Title says all. If $f$ is an analytic function on the real line, and $f\left(\dfrac{1}{x}\right)+f(x+1)=x$, what, if any, is a possible solution for $f(x)$? Additionally, what are any solutions for $f\left(\dfrac{1}{x}\right)-f(x+1)=x$?","Title says all. If $f$ is an analytic function on the real line, and $f\left(\dfrac{1}{x}\right)+f(x+1)=x$, what, if any, is a possible solution for $f(x)$? Additionally, what are any solutions for $f\left(\dfrac{1}{x}\right)-f(x+1)=x$?",,"['calculus', 'real-analysis', 'functional-equations']"
24,Limit point of sequence vs limit point of the set containing all point of the sequence,Limit point of sequence vs limit point of the set containing all point of the sequence,,I need to show that there exist sequences s.t. for fix $\epsilon>0$ there exist $|z_n-\alpha|<\epsilon$ (1) holds for infinitely many $n\in N$ but s.t. $\alpha$ is not a limit point of the set containing all terms $z_n$. Thus far I've basically constructed a sequence with several limit points but that does not converge and I guess that's way to go. What confuses me that if I have infinitely many $n \in N$ that satisfy (1) how can there be an epsilon neighborhood around $\alpha$ which contains no points of $z_n$? Is this the archimedean principle at work? My hunch is that im first choosing $N(\epsilon)$ and then in then in then in the set part choosing $\epsilon(N)$. I'd love any answer that gets me any closer to understanding this and/or the archimedean principle at work here. Thanks /I,I need to show that there exist sequences s.t. for fix $\epsilon>0$ there exist $|z_n-\alpha|<\epsilon$ (1) holds for infinitely many $n\in N$ but s.t. $\alpha$ is not a limit point of the set containing all terms $z_n$. Thus far I've basically constructed a sequence with several limit points but that does not converge and I guess that's way to go. What confuses me that if I have infinitely many $n \in N$ that satisfy (1) how can there be an epsilon neighborhood around $\alpha$ which contains no points of $z_n$? Is this the archimedean principle at work? My hunch is that im first choosing $N(\epsilon)$ and then in then in then in the set part choosing $\epsilon(N)$. I'd love any answer that gets me any closer to understanding this and/or the archimedean principle at work here. Thanks /I,,"['real-analysis', 'complex-analysis']"
25,How to prove essential supremum is a norm,How to prove essential supremum is a norm,,"Let $f$ be a measure function on $X$. If there exists an $M>0$ such that: $\mu(\{t\in X: |f(t)|>M\})=0$, we say $f$ is essentially bounded. The infimum of all such $M$ is called the essential supremum of $|f|$. It is written as $ ||f||_{\infty}= {\rm ess \;sup}|f|$ How to prove essential supremum is a norm on $(X, \Gamma, \mu)$- a $\sigma$-finite measure space.","Let $f$ be a measure function on $X$. If there exists an $M>0$ such that: $\mu(\{t\in X: |f(t)|>M\})=0$, we say $f$ is essentially bounded. The infimum of all such $M$ is called the essential supremum of $|f|$. It is written as $ ||f||_{\infty}= {\rm ess \;sup}|f|$ How to prove essential supremum is a norm on $(X, \Gamma, \mu)$- a $\sigma$-finite measure space.",,"['real-analysis', 'functional-analysis', 'measure-theory']"
26,Example of a set $Y$ that has zero Lebesgue measure and a continuous function $f$ such that $f(Y)$ is not a set of zero Lebesgue measure.,Example of a set  that has zero Lebesgue measure and a continuous function  such that  is not a set of zero Lebesgue measure.,Y f f(Y),Could someone give me an example of a set $Y\subset \mathbb{R}$ that has zero Lebesgue measure and a continuous function $f:X\subset \mathbb{R}\to\mathbb{R}$ such that $Y\subset X$ and $f(Y)$ is not a set of zero Lebesgue measure? Thanks.,Could someone give me an example of a set $Y\subset \mathbb{R}$ that has zero Lebesgue measure and a continuous function $f:X\subset \mathbb{R}\to\mathbb{R}$ such that $Y\subset X$ and $f(Y)$ is not a set of zero Lebesgue measure? Thanks.,,"['real-analysis', 'measure-theory', 'continuity', 'examples-counterexamples']"
27,"If a fixed point is a limit of a subsequence of iterates, must the whole sequence converge to it?","If a fixed point is a limit of a subsequence of iterates, must the whole sequence converge to it?",,"Say $X$ is a compact metric space, with $f:X \to X$ continuous. Now, for any $x_0$, $f^n(x_0)$ must have a convergent subsequence, say $f^{n_i}(x_0) \to x_\infty$. If we know that any such limit is a fixed point of $f$, that is, $f(x_\infty) = x_\infty$ for any such convergent subsequence, does it force the whole sequence $f^n(x_0)$ to converge to $x_\infty$? If needed, we may  assume $f$ has only finitely many, and hence discrete fixed points. I see that each translate of the subsequence ($f^{n_i+r}(x_0)$) converges to $x_\infty$, and this would have solved the problem if the differences between consecutive $n_i$ were bounded. Also, I'm pretty sure that this, combined with the fixed points being isolated, forces any convergent subsequence to have the same limit, but that doesn't seem to prove the result either. Edited : I realized my wording was wrong on a crucial point, and since the sole existing answer was not perfectly correct, I decided to put in the change. Previously we only knew the limit to be a fixed point for one particular subsequence, now we now this for any convergent subsequence of iterates.","Say $X$ is a compact metric space, with $f:X \to X$ continuous. Now, for any $x_0$, $f^n(x_0)$ must have a convergent subsequence, say $f^{n_i}(x_0) \to x_\infty$. If we know that any such limit is a fixed point of $f$, that is, $f(x_\infty) = x_\infty$ for any such convergent subsequence, does it force the whole sequence $f^n(x_0)$ to converge to $x_\infty$? If needed, we may  assume $f$ has only finitely many, and hence discrete fixed points. I see that each translate of the subsequence ($f^{n_i+r}(x_0)$) converges to $x_\infty$, and this would have solved the problem if the differences between consecutive $n_i$ were bounded. Also, I'm pretty sure that this, combined with the fixed points being isolated, forces any convergent subsequence to have the same limit, but that doesn't seem to prove the result either. Edited : I realized my wording was wrong on a crucial point, and since the sole existing answer was not perfectly correct, I decided to put in the change. Previously we only knew the limit to be a fixed point for one particular subsequence, now we now this for any convergent subsequence of iterates.",,"['real-analysis', 'convergence-divergence', 'metric-spaces']"
28,"Is a real number the limit of a Cauchy sequence, the sequence itself, a shrinking closed interval of rational numbers, or what?","Is a real number the limit of a Cauchy sequence, the sequence itself, a shrinking closed interval of rational numbers, or what?",,"I've been studying a collection of analysis books (one of them Bishop's Constructive version) and contemplating the reals. Correct me if I'm wrong, but I feel that I have seen the Cauchy sequence itself in some places and its limit in other places described as the real number. I do understand that nested closed intervals have a point as the limit of their countably infinite intersection. I can visualize an arbitrarily small interval of rationals, any of which has an equal claim (its seem to me, at least until other considerations are brought in) to being an ""approximation"" of this point. Unless we know the limit point (via the geometric series, for instance), what are we approximating if not a yet better approximation of a yet better approximation? (Maybe the ""approximation"" terminology works better for cuts.) I suppose I'm asking not only for the clarification of the dominant convention but also for ""real talk"" about the mathematical imagining of real numbers.","I've been studying a collection of analysis books (one of them Bishop's Constructive version) and contemplating the reals. Correct me if I'm wrong, but I feel that I have seen the Cauchy sequence itself in some places and its limit in other places described as the real number. I do understand that nested closed intervals have a point as the limit of their countably infinite intersection. I can visualize an arbitrarily small interval of rationals, any of which has an equal claim (its seem to me, at least until other considerations are brought in) to being an ""approximation"" of this point. Unless we know the limit point (via the geometric series, for instance), what are we approximating if not a yet better approximation of a yet better approximation? (Maybe the ""approximation"" terminology works better for cuts.) I suppose I'm asking not only for the clarification of the dominant convention but also for ""real talk"" about the mathematical imagining of real numbers.",,['real-analysis']
29,$|f(x)-f(y)|\le(x-y)^2$ without gaplessness,without gaplessness,|f(x)-f(y)|\le(x-y)^2,"If $|f(x)-f(y)|\le(x-y)^2$ for all $x,y\in\mathbb R$, then it's easy to show that $f'=0$ everywhere, and the mean value theorem implies that that means $f$ is constant.  If there were a gap in the real line and $f=3$ on one side of the gap and $f=4$ on the other side of the gap, then $f'=0$ everywhere but $f$ is not constant.  Gaplessness enters via the mean value theorem. But I wonder if the proposition holds even in a line with gaps.  For example if $f:\mathbb Q\to\mathbb Q$ and for all $x,y\in\mathbb Q$ this inequality holds, does that imply $f$ is constant?  And what about other gap-filled lines than $\mathbb Q$?  Such as $\mathbb R\setminus A$ where $A$ is a finite set, or a countable set, or a set whose complement is dense, or whatever set it might be of interest to ask this question about?","If $|f(x)-f(y)|\le(x-y)^2$ for all $x,y\in\mathbb R$, then it's easy to show that $f'=0$ everywhere, and the mean value theorem implies that that means $f$ is constant.  If there were a gap in the real line and $f=3$ on one side of the gap and $f=4$ on the other side of the gap, then $f'=0$ everywhere but $f$ is not constant.  Gaplessness enters via the mean value theorem. But I wonder if the proposition holds even in a line with gaps.  For example if $f:\mathbb Q\to\mathbb Q$ and for all $x,y\in\mathbb Q$ this inequality holds, does that imply $f$ is constant?  And what about other gap-filled lines than $\mathbb Q$?  Such as $\mathbb R\setminus A$ where $A$ is a finite set, or a countable set, or a set whose complement is dense, or whatever set it might be of interest to ask this question about?",,"['real-analysis', 'general-topology', 'inequality', 'metric-spaces']"
30,$f:\mathbb R\rightarrow \mathbb R$ be a continuous function such that $\int_{0}^{\infty}f(x)dx$ exists.,be a continuous function such that  exists.,f:\mathbb R\rightarrow \mathbb R \int_{0}^{\infty}f(x)dx,"I was thinking about the following problem: Let $f:\mathbb R\rightarrow \mathbb R$ be a continuous function such that $\int_{0}^{\infty}f(x)dx$ exists. Then which of the following statements are correct? (a) If $\lim_{x\to\infty}f(x)$ exists, then $\lim_{x\to\infty}f(x)=0,$ (b) The limit $\lim_{x\to\infty}f(x)$ must exist and is zero, (c) In case $f$ is a nonnegative function, the limit $\lim_{x\to\infty}f(x)$ must exist and is zero, (d) In case $f$ is a differentiable function, the limit $\lim_{x\to\infty}f'(x)$ must exist and is zero. If I take $f(x)=e^{-x}$ , so that the given condition is satisfied then we see that options (a) and (c) are correct. But I am not sure about the choice given in (b) and (d). But If I have to prove it in general, then how can I prove it? I mean an alternative better approach. Please help. Thanks in advance for your time.","I was thinking about the following problem: Let be a continuous function such that exists. Then which of the following statements are correct? (a) If exists, then (b) The limit must exist and is zero, (c) In case is a nonnegative function, the limit must exist and is zero, (d) In case is a differentiable function, the limit must exist and is zero. If I take , so that the given condition is satisfied then we see that options (a) and (c) are correct. But I am not sure about the choice given in (b) and (d). But If I have to prove it in general, then how can I prove it? I mean an alternative better approach. Please help. Thanks in advance for your time.","f:\mathbb R\rightarrow \mathbb R \int_{0}^{\infty}f(x)dx \lim_{x\to\infty}f(x) \lim_{x\to\infty}f(x)=0, \lim_{x\to\infty}f(x) f \lim_{x\to\infty}f(x) f \lim_{x\to\infty}f'(x) f(x)=e^{-x}","['real-analysis', 'analysis']"
31,What are the limit points of $\tan(\mathbb{N})$ in $\mathbb{R}$?,What are the limit points of  in ?,\tan(\mathbb{N}) \mathbb{R},"I was working on a an old worksheet problem here . It asks Let $S=\{\tan(k):k=1,2,\dots\}$. Find the set of limit points of $S$ on the real line. The answer is $(-\infty,\infty)$. Intuitively I feel that if we keep evaluating tangent at positive integer points, they will be so scattered over the real line that we could always construct some subsequence converging to any real number. How can this be made rigorous to get this purported conclusion? Thanks.","I was working on a an old worksheet problem here . It asks Let $S=\{\tan(k):k=1,2,\dots\}$. Find the set of limit points of $S$ on the real line. The answer is $(-\infty,\infty)$. Intuitively I feel that if we keep evaluating tangent at positive integer points, they will be so scattered over the real line that we could always construct some subsequence converging to any real number. How can this be made rigorous to get this purported conclusion? Thanks.",,"['real-analysis', 'general-topology']"
32,Compactness of  Multiplication Operator on $L^2$,Compactness of  Multiplication Operator on,L^2,"Suppose we have an bounded linear operator A that operates from $L^2([a,b]) \mapsto L^2([a,b])$.  Now suppose that $A(f)(t) = tf(t)$. Is A compact? Edit: I know $A = A^*$ but I'm not really sure how to start on this.  It's not homework, just summer fun :D","Suppose we have an bounded linear operator A that operates from $L^2([a,b]) \mapsto L^2([a,b])$.  Now suppose that $A(f)(t) = tf(t)$. Is A compact? Edit: I know $A = A^*$ but I'm not really sure how to start on this.  It's not homework, just summer fun :D",,"['real-analysis', 'functional-analysis', 'hilbert-spaces']"
33,A way to justify interchanging a summation and integration,A way to justify interchanging a summation and integration,,"For all real $a>0$ consider $$\int_0^\infty x^a \left (\sum_{n = 1}^\infty (-1)^n n e^{-2nx} \right ) \, dx.$$ I would like to interchange the summation with the integration in order to find its value but I am having trouble justifying such a change. Fubini's theorem is not strong enough to justify the interchange for all $a > 0$ . If we put absolute values on the terms, we have $$\left | \sum_{n = 1}^\infty (-1)^n n e^{-2nx} \right | \leqslant \sum_{n = 1}^\infty n e^{-2nx} = \frac{e^{2x}}{(e^{2x} - 1)^2}$$ but $\displaystyle\int_0^\infty \frac{x^a e^{2x}}{(e^{2x} - 1)^2}~dx$ only converges for $a > 1$ and not for all $a > 0$ . So my question is, how can the interchange between the summation and integration be justified so that it holds for all $a > 0$ ?","For all real consider I would like to interchange the summation with the integration in order to find its value but I am having trouble justifying such a change. Fubini's theorem is not strong enough to justify the interchange for all . If we put absolute values on the terms, we have but only converges for and not for all . So my question is, how can the interchange between the summation and integration be justified so that it holds for all ?","a>0 \int_0^\infty x^a \left (\sum_{n = 1}^\infty (-1)^n n e^{-2nx} \right ) \, dx. a > 0 \left | \sum_{n = 1}^\infty (-1)^n n e^{-2nx} \right | \leqslant \sum_{n = 1}^\infty n e^{-2nx} = \frac{e^{2x}}{(e^{2x} - 1)^2} \displaystyle\int_0^\infty \frac{x^a e^{2x}}{(e^{2x} - 1)^2}~dx a > 1 a > 0 a > 0","['real-analysis', 'integration', 'sequences-and-series', 'analysis', 'improper-integrals']"
34,Solve this integral of degree $2043$,Solve this integral of degree,2043,Find the value of $$\frac{\displaystyle\int_0^1(x+1)^{1010}\:\:dx}{\displaystyle\int_0^1(x^{2043}+1)^{1010}\:\:dx}$$ I evaluated the value of the numerator as $$\frac{2^{1011}-1}{1011}$$ But can't do the same for the denominator. Any help is greatly appreciated.,Find the value of I evaluated the value of the numerator as But can't do the same for the denominator. Any help is greatly appreciated.,\frac{\displaystyle\int_0^1(x+1)^{1010}\:\:dx}{\displaystyle\int_0^1(x^{2043}+1)^{1010}\:\:dx} \frac{2^{1011}-1}{1011},"['real-analysis', 'calculus', 'integration']"
35,Number of zeros of $f(x)= \frac{1}{2} E\left[ \tanh \left( \frac{x+Z}{2} \right) \right]-\tanh(x)+\frac{x}{2}$ where $Z$ is standard normal,Number of zeros of  where  is standard normal,f(x)= \frac{1}{2} E\left[ \tanh \left( \frac{x+Z}{2} \right) \right]-\tanh(x)+\frac{x}{2} Z,"Consider the following function: \begin{align} f(x)= \frac{1}{2} E\left[ \tanh \left( \frac{x+Z}{2} \right) \right]-\tanh(x)+\frac{x}{2}, \end{align} where $Z$ is standard normal. Question : How to show that this function has only three zeros? Note, that we are not interested in the locations just the number of zeros. By using that $\tanh(x)$ is an odd function, it is not difficult to show that $f(0)=0$ . However, I am not sure how to show the existence of the other two zeros. I know that two more zeros exist from the numerical simulation (see the attached figure). Edit: The current answer shows that there are at least 3 zeros.  Now we need to show that there can be no more than 3 zeros. Edit 2 Idea for a proof.  Consider only positive $x$ .  I think  if we can show the following: $f(x)>0$ for  all $x>x_1$ , $f(x)$ is convex for $x \in (0,x_2)$ , and $x_2>x_1$ . Then this will imply that the function is convex in the regime while it changes a sign. Therefore, it can only have at most one sign change.","Consider the following function: where is standard normal. Question : How to show that this function has only three zeros? Note, that we are not interested in the locations just the number of zeros. By using that is an odd function, it is not difficult to show that . However, I am not sure how to show the existence of the other two zeros. I know that two more zeros exist from the numerical simulation (see the attached figure). Edit: The current answer shows that there are at least 3 zeros.  Now we need to show that there can be no more than 3 zeros. Edit 2 Idea for a proof.  Consider only positive .  I think  if we can show the following: for  all , is convex for , and . Then this will imply that the function is convex in the regime while it changes a sign. Therefore, it can only have at most one sign change.","\begin{align}
f(x)= \frac{1}{2} E\left[ \tanh \left( \frac{x+Z}{2} \right) \right]-\tanh(x)+\frac{x}{2},
\end{align} Z \tanh(x) f(0)=0 x f(x)>0 x>x_1 f(x) x \in (0,x_2) x_2>x_1","['real-analysis', 'probability', 'roots', 'hyperbolic-functions']"
36,Is there a formula for $\int_0^1 x\uparrow\uparrow n dx$,Is there a formula for,\int_0^1 x\uparrow\uparrow n dx,$$\int_0^1  x\uparrow\uparrow n dx$$ I was working on an answer to this question on integrating $x^{x^x}$ and I was thinking is there a particular formula/rule/pattern in integration if $x\uparrow\uparrow n$ ( $x$ to the power of $x \ n$ times) from $0$ to $1$ . Integrating $x^x$ we use the Taylor series to arrive a $$\int_0^1 x^xdx=\int_0^1 e^{x\ln(x)}dx =\int_0^1 \sum^\infty_{n=0}\frac{x^n\ln^n(x)}{n!}dx=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n^n}=0.78343\ldots $$ similarly integrating $x^{x^x}$ the taylor series would be $$x^{x^x}=\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}x^{x^n}=\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}\Bigg(\sum_{k=0}^\infty\frac{x^k\ln^k(x)}{k!}\Bigg)^n$$ and integrating $x\uparrow\uparrow 3$ the series would be $$x^{x^{x^x}}=\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}x^{x^{x^n}}=\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}\Bigg(\sum_{k=0}^\infty\frac{\ln^k(x)}{k!}x^{x^k}\Bigg)^n= \sum^\infty_{n=0}\frac{\ln^n(x)}{n!}\Bigg(\sum_{k=0}^\infty\frac{\ln^k(x)}{k!}\Big(\sum^\infty_{m=0}\frac{x^m\ln^m(x)}{m!}\Big)^k\Bigg)^n $$ So is there a solution to the integral? $$\int_0^1  x\uparrow\uparrow n dx$$ Can it be simplified? Can one find series solutions for all $n$ ? What if $n\not\in \mathbb{N}$ ? I have been interested in this function for quite a long time so thank you for your time,I was working on an answer to this question on integrating and I was thinking is there a particular formula/rule/pattern in integration if ( to the power of times) from to . Integrating we use the Taylor series to arrive a similarly integrating the taylor series would be and integrating the series would be So is there a solution to the integral? Can it be simplified? Can one find series solutions for all ? What if ? I have been interested in this function for quite a long time so thank you for your time,"\int_0^1  x\uparrow\uparrow n dx x^{x^x} x\uparrow\uparrow n x x \ n 0 1 x^x \int_0^1 x^xdx=\int_0^1 e^{x\ln(x)}dx =\int_0^1 \sum^\infty_{n=0}\frac{x^n\ln^n(x)}{n!}dx=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n^n}=0.78343\ldots
 x^{x^x} x^{x^x}=\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}x^{x^n}=\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}\Bigg(\sum_{k=0}^\infty\frac{x^k\ln^k(x)}{k!}\Bigg)^n x\uparrow\uparrow 3 x^{x^{x^x}}=\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}x^{x^{x^n}}=\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}\Bigg(\sum_{k=0}^\infty\frac{\ln^k(x)}{k!}x^{x^k}\Bigg)^n=
\sum^\infty_{n=0}\frac{\ln^n(x)}{n!}\Bigg(\sum_{k=0}^\infty\frac{\ln^k(x)}{k!}\Big(\sum^\infty_{m=0}\frac{x^m\ln^m(x)}{m!}\Big)^k\Bigg)^n
 \int_0^1  x\uparrow\uparrow n dx n n\not\in \mathbb{N}","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'taylor-expansion']"
37,"If $\sum_{n=1}^{\infty} a_{n}$ converges, then$\sum_{n=1}^{\infty} \sin(a_{n})$ also converges","If  converges, then also converges",\sum_{n=1}^{\infty} a_{n} \sum_{n=1}^{\infty} \sin(a_{n}),"If series $\sum_{n=1}^{\infty} a_{n}$ converges, prove series $\sum_{n=1}^{\infty} \sin(a_{n})$ converges  too. Is this a series function problem or something related to? I tried this: If $a_{n} \ge 0 $ for all $n$ , then $| \sin (a_{n})| \le a_{n}$ , on the other hand $\sin(x)$ is continuous function in $[0,x]$ and differentiable in $(0,x)$ then exist $c \in (0,x)$ and $$(x-0)\cos (c)= \sin(x)-\sin(0)$$ then $$x\cos(c)= \sin(x)$$ but, $|\sin(x)|=|x\cos(c)|= |x| |\cos(c)| \le |x|$ , thus $|\sin (x)| \le |x|$ . We know, $|\sin (a_{n})| \le |a_{n}| $ and $\sum_{n=1}^{\infty} a_{n}$ converges, then $\sum_{n=1}^{\infty} |\sin(a_{n})|$ converges. Therefore $\sum_{n=1}^{\infty} \sin(a_{n})$ converges. But this proof use ${a_{n}}$ positive and in the original problem I don't have this hypotesis.","If series converges, prove series converges  too. Is this a series function problem or something related to? I tried this: If for all , then , on the other hand is continuous function in and differentiable in then exist and then but, , thus . We know, and converges, then converges. Therefore converges. But this proof use positive and in the original problem I don't have this hypotesis.","\sum_{n=1}^{\infty} a_{n} \sum_{n=1}^{\infty} \sin(a_{n}) a_{n} \ge 0  n | \sin (a_{n})| \le a_{n} \sin(x) [0,x] (0,x) c \in (0,x) (x-0)\cos (c)= \sin(x)-\sin(0) x\cos(c)= \sin(x) |\sin(x)|=|x\cos(c)|= |x| |\cos(c)| \le |x| |\sin (x)| \le |x| |\sin (a_{n})| \le |a_{n}|  \sum_{n=1}^{\infty} a_{n} \sum_{n=1}^{\infty} |\sin(a_{n})| \sum_{n=1}^{\infty} \sin(a_{n}) {a_{n}}","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
38,Horrible limit envolving floor function,Horrible limit envolving floor function,,"Let $x\in [0,1]$ , $\ell\in\mathbb{Z}$ and $\tau>0$ . I want to calculate $$\lim_{L\to\infty}\sum_{k=0}^{\lfloor \tau L^2\rfloor}\frac{1}{2^{\lfloor \tau L^2\rfloor}}\binom{\lfloor \tau L^2\rfloor}{k}\cos\left(2\pi \ell\frac{\lfloor xL\rfloor-\lfloor \tau L^2\rfloor+2k}{L}\right).$$ I think the result is $\exp(-2\pi^2\ell^2\tau)\cos(2\pi\ell x)$ but I have no idea in how to prove it. I tried estimating the binomial with Stirling's aproximation but without success.","Let , and . I want to calculate I think the result is but I have no idea in how to prove it. I tried estimating the binomial with Stirling's aproximation but without success.","x\in [0,1] \ell\in\mathbb{Z} \tau>0 \lim_{L\to\infty}\sum_{k=0}^{\lfloor \tau L^2\rfloor}\frac{1}{2^{\lfloor \tau L^2\rfloor}}\binom{\lfloor \tau L^2\rfloor}{k}\cos\left(2\pi \ell\frac{\lfloor xL\rfloor-\lfloor \tau L^2\rfloor+2k}{L}\right). \exp(-2\pi^2\ell^2\tau)\cos(2\pi\ell x)","['real-analysis', 'limits']"
39,Proof explanation: convexity of the numerical range of an operator (Toeplitz-Hausdorff Theorem),Proof explanation: convexity of the numerical range of an operator (Toeplitz-Hausdorff Theorem),,"Let $\mathcal{H}$ be a complex Hilbert space and $\mathcal{B}(\mathcal{H})$ be the algebra of all bounded linear operators on $\mathcal{H}$ . Theorem: Let $T\in \mathcal{B}(\mathcal{H})$ , then $W(T)$ is a convex subset of $\mathbb{C}$ , where $$W(T):=\{\langle T x\mid x\rangle;\;x \in \mathcal{H}\;\;\text{with}\;\|x\|=1\}.$$ Proof: Given $\lambda,\mu\in W(T)$ such that $\lambda\neq\mu$ and we will prove that $\eta=t\lambda+(1-t)\mu \in W(T)$ whenever $t\in [0,1]$ . Clearly we have $$\eta\in W(T)\Leftrightarrow t\in W(\alpha I+\beta T),$$ with $\alpha=-\frac{\mu}{\lambda-\mu}$ and $\beta=\frac{1}{\lambda-\mu}$ . Let $S=\alpha I+\beta T$ . We observe that $\eta\in W(T)$ if and only if $t\in W(S)$ for all $t\in [0,1]$ . Since $\lambda,\mu\in W(T)$ , then there exist unit vectors $x,y\in \mathcal{H}$ such that $$\lambda=\langle Tx\mid x\rangle\;\text{and}\;\mu=\langle Ty\mid y\rangle.$$ A simple calculation shows that $$\langle Sx\mid x\rangle=1\;\text{and}\;\langle Sy\mid y\rangle=0.$$ Define $g:\mathbb{R}\to \mathbb{C}$ by $$g(\theta)=\langle Sy\mid x\rangle e^{-i\theta}+\langle Sx\mid y\rangle e^{i\theta},\;\theta\in \mathbb{R}.$$ Obviously $g(\theta+\pi) = -g(\theta)$ for all $\theta\in \mathbb{R}$ . Further, $\Im m g(0)=-\Im m g(\pi)$ . Since $g$ is a continuous function, then there exists $\theta_0\in [0,\pi]$ such that $\Im m g(\theta_0)=0$ . Now observe that the vectors $y$ and $\hat{x}=e^{i\theta_0}x$ are linearly independent. Otherwise, we write $y=\alpha \hat{x}$ for some $\alpha\in \mathbb{C}$ , then $|\alpha|=1$ and $0=\langle Sy\mid y\rangle=|\alpha|^2\langle S\hat{x}\mid \hat{x}\rangle=\langle Sx\mid x\rangle=1$ which is a contradiction. Hence, $\|(1-t)y+t\hat{x}\|\neq 0$ for all $t\in [0,1]$ . Put $$z_t=\frac{(1-t)y+t\hat{x}}{\|(1-t)y+t\hat{x}\|},\;t\in [0,1].$$ Clearly $\|z_t\|=1$ for all $t\in [0,1]$ . Moreover, one can see that $\langle Sz_0\mid z_0\rangle=0$ and $\langle Sz_1\mid z_1\rangle=1$ . This implies that $0,1\in W(S)$ . To finish the proof, define a continuous function $f$ on $[0,1]$ by $$f(t)=\langle Sz_t\mid z_t\rangle,\;t\in [0,1].$$ A straightforward calculation shows that $f$ is a real-valued function with $f(0)=0$ and $f(1) = 1$ . Hence $[0,1]\subset W(S)$ . In this proof I don't understant the following facts: $\eta\in W(T)\Leftrightarrow t\in W(\alpha I+\beta T).$ Why we use $\Im m g$ in order to find $\theta_0$ . Why we use $f$ in order to conclude that $[0,1]\subset W(S)$ ?","Let be a complex Hilbert space and be the algebra of all bounded linear operators on . Theorem: Let , then is a convex subset of , where Proof: Given such that and we will prove that whenever . Clearly we have with and . Let . We observe that if and only if for all . Since , then there exist unit vectors such that A simple calculation shows that Define by Obviously for all . Further, . Since is a continuous function, then there exists such that . Now observe that the vectors and are linearly independent. Otherwise, we write for some , then and which is a contradiction. Hence, for all . Put Clearly for all . Moreover, one can see that and . This implies that . To finish the proof, define a continuous function on by A straightforward calculation shows that is a real-valued function with and . Hence . In this proof I don't understant the following facts: Why we use in order to find . Why we use in order to conclude that ?","\mathcal{H} \mathcal{B}(\mathcal{H}) \mathcal{H} T\in \mathcal{B}(\mathcal{H}) W(T) \mathbb{C} W(T):=\{\langle T x\mid x\rangle;\;x \in \mathcal{H}\;\;\text{with}\;\|x\|=1\}. \lambda,\mu\in W(T) \lambda\neq\mu \eta=t\lambda+(1-t)\mu \in W(T) t\in [0,1] \eta\in W(T)\Leftrightarrow t\in W(\alpha I+\beta T), \alpha=-\frac{\mu}{\lambda-\mu} \beta=\frac{1}{\lambda-\mu} S=\alpha I+\beta T \eta\in W(T) t\in W(S) t\in [0,1] \lambda,\mu\in W(T) x,y\in \mathcal{H} \lambda=\langle Tx\mid x\rangle\;\text{and}\;\mu=\langle Ty\mid y\rangle. \langle Sx\mid x\rangle=1\;\text{and}\;\langle Sy\mid y\rangle=0. g:\mathbb{R}\to \mathbb{C} g(\theta)=\langle Sy\mid x\rangle e^{-i\theta}+\langle Sx\mid y\rangle e^{i\theta},\;\theta\in \mathbb{R}. g(\theta+\pi) = -g(\theta) \theta\in \mathbb{R} \Im m g(0)=-\Im m g(\pi) g \theta_0\in [0,\pi] \Im m g(\theta_0)=0 y \hat{x}=e^{i\theta_0}x y=\alpha \hat{x} \alpha\in \mathbb{C} |\alpha|=1 0=\langle Sy\mid y\rangle=|\alpha|^2\langle S\hat{x}\mid \hat{x}\rangle=\langle Sx\mid x\rangle=1 \|(1-t)y+t\hat{x}\|\neq 0 t\in [0,1] z_t=\frac{(1-t)y+t\hat{x}}{\|(1-t)y+t\hat{x}\|},\;t\in [0,1]. \|z_t\|=1 t\in [0,1] \langle Sz_0\mid z_0\rangle=0 \langle Sz_1\mid z_1\rangle=1 0,1\in W(S) f [0,1] f(t)=\langle Sz_t\mid z_t\rangle,\;t\in [0,1]. f f(0)=0 f(1) = 1 [0,1]\subset W(S) \eta\in W(T)\Leftrightarrow t\in W(\alpha I+\beta T). \Im m g \theta_0 f [0,1]\subset W(S)","['real-analysis', 'functional-analysis', 'operator-theory']"
40,All norms of $\mathbb R^n$ are equivalent,All norms of  are equivalent,\mathbb R^n,"1) First, all $\|x\|_p=\sqrt[p]{|x_1|^p+...+|x_n|^p}$ are equivalent. In my course, they used Hölder inequality, i.e. that $$\|x\cdot y\|_1\leq \|x\|_p\|x\|_q$$ for $p,q\geq 1$ s.t. $\frac{1}{p}+\frac{1}{q}=1$ . But can I do as follow : Let $1\leq p<\infty $ . I denote $\|x\|_\infty =\max\{|x_1|,...,|x_n|\}$ . Then, $$\|x\|_\infty ^p\leq |x_1|^p+...+|x_n|^p\leq n\|x\|_\infty^p ,$$ and thus $$\|x\|_\infty \leq \|x\|_p\leq n^{1/p}\|x\|_\infty .$$ Therefore, if $1\leq r,p <\infty $ , then $$\|x\|_p\leq C\|x\|_\infty \leq C\|x\|_r\leq Cn^{1/r}\|x\|_\infty \leq Cn^{1/r}\|x\|_p.$$ Therefore, $\|\cdot \|_p$ and $\|\cdot \|_r$ are equivalent. Question 1: Does it work ? (then no need Hölder?) 2) All norms of $\mathbb R^n$ are equivalent. Let $N$ a norm. Question 2: By what I did previously, I just need to prove that $N$ is equivalent to $\|\cdot \|_\infty $ to conclude that all norm are equivalent, true ? To do it I tried as follow : $$N(x)=N(x_1e_1+...+x_ne_n)\leq |x_1|N(e_1)+...+|x_n|N(e_n)\leq \|x\|_\infty (N(e_1)+...+N(e_n)),$$ where $e_i=(0,...,0,1,0,...,0)$ where $1$ is at the $i-$ th position. Question 3: How can I show that $ N(x)\geq C\|x\|_\infty$ ?","1) First, all are equivalent. In my course, they used Hölder inequality, i.e. that for s.t. . But can I do as follow : Let . I denote . Then, and thus Therefore, if , then Therefore, and are equivalent. Question 1: Does it work ? (then no need Hölder?) 2) All norms of are equivalent. Let a norm. Question 2: By what I did previously, I just need to prove that is equivalent to to conclude that all norm are equivalent, true ? To do it I tried as follow : where where is at the th position. Question 3: How can I show that ?","\|x\|_p=\sqrt[p]{|x_1|^p+...+|x_n|^p} \|x\cdot y\|_1\leq \|x\|_p\|x\|_q p,q\geq 1 \frac{1}{p}+\frac{1}{q}=1 1\leq p<\infty  \|x\|_\infty =\max\{|x_1|,...,|x_n|\} \|x\|_\infty ^p\leq |x_1|^p+...+|x_n|^p\leq n\|x\|_\infty^p , \|x\|_\infty \leq \|x\|_p\leq n^{1/p}\|x\|_\infty . 1\leq r,p <\infty  \|x\|_p\leq C\|x\|_\infty \leq C\|x\|_r\leq Cn^{1/r}\|x\|_\infty \leq Cn^{1/r}\|x\|_p. \|\cdot \|_p \|\cdot \|_r \mathbb R^n N N \|\cdot \|_\infty  N(x)=N(x_1e_1+...+x_ne_n)\leq |x_1|N(e_1)+...+|x_n|N(e_n)\leq \|x\|_\infty (N(e_1)+...+N(e_n)), e_i=(0,...,0,1,0,...,0) 1 i-  N(x)\geq C\|x\|_\infty","['real-analysis', 'normed-spaces']"
41,Pointwise Convergence of Lipschitz Functions is Uniform,Pointwise Convergence of Lipschitz Functions is Uniform,,"Say we have $f_n:[a,b]\to \mathbb{R}$ , such that for all $n$ we have $|f_n(x)-f_n(y)|\leq L|x-y|$ and $f_n \to f$ pointwise. Is the convergence uniform? I started with an attempt to prove it by showing Cauchy Criterion for uniform convergence : for any $c\in[a,b]$ $$|f_n(x)-f_m(x)|=|f_n(x)-f_n(c)+f_n(c)-f_m(c)+f_m(c)-f_m(x)|\\\leq|f_n(x)-f_n(c)|+|f_n(c)-f_m(c)|+|f_m(c)-f_m(x)|\leq 2L|x-c|+|f_m(c)-f_n(c)|.$$ Now let $\epsilon>0$, there exists $n,m$ such that $|f_m(c)-f_n(c)|<\frac \epsilon 2$ from pointwise convergence. Also, if we take $x\in(c-\frac \epsilon {4L},c+\frac \epsilon {4L})$, we get $|f_n(x)-f_m(x)|<2L\frac \epsilon {4L}+\frac \epsilon 2=\epsilon$. So we have uniform convergence in $(c-\frac \epsilon {4L},c+\frac \epsilon {4L})$ for all $c\in[a,b]$. We can get finite cover of these covers, where we have uniform convergence, and take the maximum $N$ from all of those intervals to get uniform convergence in $[a,b]$. However, My friend presented me with a possible counter-example : $f_n(x)=nxe^{-nx}$ in [0,1], where the derivative is bounded, and the convergence is only pointwise to 0, and not uniform. I couldn't find where my proof fails, and I`d be glad if someone can point it out for me.","Say we have $f_n:[a,b]\to \mathbb{R}$ , such that for all $n$ we have $|f_n(x)-f_n(y)|\leq L|x-y|$ and $f_n \to f$ pointwise. Is the convergence uniform? I started with an attempt to prove it by showing Cauchy Criterion for uniform convergence : for any $c\in[a,b]$ $$|f_n(x)-f_m(x)|=|f_n(x)-f_n(c)+f_n(c)-f_m(c)+f_m(c)-f_m(x)|\\\leq|f_n(x)-f_n(c)|+|f_n(c)-f_m(c)|+|f_m(c)-f_m(x)|\leq 2L|x-c|+|f_m(c)-f_n(c)|.$$ Now let $\epsilon>0$, there exists $n,m$ such that $|f_m(c)-f_n(c)|<\frac \epsilon 2$ from pointwise convergence. Also, if we take $x\in(c-\frac \epsilon {4L},c+\frac \epsilon {4L})$, we get $|f_n(x)-f_m(x)|<2L\frac \epsilon {4L}+\frac \epsilon 2=\epsilon$. So we have uniform convergence in $(c-\frac \epsilon {4L},c+\frac \epsilon {4L})$ for all $c\in[a,b]$. We can get finite cover of these covers, where we have uniform convergence, and take the maximum $N$ from all of those intervals to get uniform convergence in $[a,b]$. However, My friend presented me with a possible counter-example : $f_n(x)=nxe^{-nx}$ in [0,1], where the derivative is bounded, and the convergence is only pointwise to 0, and not uniform. I couldn't find where my proof fails, and I`d be glad if someone can point it out for me.",,"['calculus', 'real-analysis', 'sequences-and-series', 'uniform-convergence', 'lipschitz-functions']"
42,When ODE tells more than the explicit solution.,When ODE tells more than the explicit solution.,,"""ODE is not just about 'solving' an equation and spitting out a (probably nasty) formula"" -- this is what I want my (undergraduate) students to learn from my course this summer. One example I am looking for is a scenario where one extracts info about a function from the ODE it satisfies much more easily than from the solution's explicit formula. There are easy example to see this. For instance, the IVP $y'=y; y(0)=1$ tells us that the function will be increasing on zero to infinity, by taking another derivative that it will be concave up, etc. However, one may rightly argue that $e^x$ which is the solution easily gives these properties. So, I am looking for a less trivial, yet, interesting example where it is much easier to understand a function from its ODE than from its explicit formula . Do you have such examples? I will appreciate them. **The example may be important from computational/numerical point of view.""","""ODE is not just about 'solving' an equation and spitting out a (probably nasty) formula"" -- this is what I want my (undergraduate) students to learn from my course this summer. One example I am looking for is a scenario where one extracts info about a function from the ODE it satisfies much more easily than from the solution's explicit formula. There are easy example to see this. For instance, the IVP $y'=y; y(0)=1$ tells us that the function will be increasing on zero to infinity, by taking another derivative that it will be concave up, etc. However, one may rightly argue that $e^x$ which is the solution easily gives these properties. So, I am looking for a less trivial, yet, interesting example where it is much easier to understand a function from its ODE than from its explicit formula . Do you have such examples? I will appreciate them. **The example may be important from computational/numerical point of view.""",,"['real-analysis', 'ordinary-differential-equations', 'numerical-methods']"
43,Prove that $f$ is constant if $f(x)=f(x^2-x+1)$,Prove that  is constant if,f f(x)=f(x^2-x+1),"Let $f:[0,1] \to \mathbb{R}$ be a function which is continuous at $x_0=1$ and which satisfies$$f(x)=f(x^2-x+1), \: \forall \: x\in [0,1]$$   Prove that $f$ is constant. My idea is to get as much as possible from the initial equation, eventually getting a chain like $f(x)=...=f(\text{something})$, that $\text{something}$ getting to $1$ eventually, independently of $x$. This way we could apply the continuity at $1$, thus proving the claim. With these in mind, I made the substitution $x \to 1-x$ and got $$f(1-x)=f(x^2-x+1)=f(x), \: \forall \: x \in [0,1]$$ which means that it's enough to prove that $f$ is constant on $[0,\frac{1}{2}]$ and also gives $$f(x)=f(1-x)=f(x^2-x+1)=f(x-x^2), \: \forall \: x \in [0,1]$$ From here, everything eventually came back to one of those $4$ terms above and I got stuck...","Let $f:[0,1] \to \mathbb{R}$ be a function which is continuous at $x_0=1$ and which satisfies$$f(x)=f(x^2-x+1), \: \forall \: x\in [0,1]$$   Prove that $f$ is constant. My idea is to get as much as possible from the initial equation, eventually getting a chain like $f(x)=...=f(\text{something})$, that $\text{something}$ getting to $1$ eventually, independently of $x$. This way we could apply the continuity at $1$, thus proving the claim. With these in mind, I made the substitution $x \to 1-x$ and got $$f(1-x)=f(x^2-x+1)=f(x), \: \forall \: x \in [0,1]$$ which means that it's enough to prove that $f$ is constant on $[0,\frac{1}{2}]$ and also gives $$f(x)=f(1-x)=f(x^2-x+1)=f(x-x^2), \: \forall \: x \in [0,1]$$ From here, everything eventually came back to one of those $4$ terms above and I got stuck...",,"['calculus', 'real-analysis', 'continuity', 'functional-equations']"
44,$\sum\limits_{n=1}^\infty \log(1+a_n)$ converges absolutely $\iff\sum\limits_{n=1}^\infty a_n$ converges absolutely.,converges absolutely  converges absolutely.,\sum\limits_{n=1}^\infty \log(1+a_n) \iff\sum\limits_{n=1}^\infty a_n,"$$\sum\limits_{n=1}^\infty \log(1+a_n) \text{ converges absolutely}  \Leftrightarrow \sum_{n=1}^\infty a_n \text{ converges absolutely}.$$ How to prove this, Suppose $$\sum_{n=1}^\infty a_n \text{ converges absolutely}.$$ Let $u_{n}=a_{n}$ and $v_{n}=\log(1+a_n)$, then $$\lim_{n\to\infty} \frac{u_{n}}{v_{n}}=1>0 \implies\sum_{n=1}^\infty \log(1+ a_n) \text{ converges absolutely}.$$ How to prove the converse part?","$$\sum\limits_{n=1}^\infty \log(1+a_n) \text{ converges absolutely}  \Leftrightarrow \sum_{n=1}^\infty a_n \text{ converges absolutely}.$$ How to prove this, Suppose $$\sum_{n=1}^\infty a_n \text{ converges absolutely}.$$ Let $u_{n}=a_{n}$ and $v_{n}=\log(1+a_n)$, then $$\lim_{n\to\infty} \frac{u_{n}}{v_{n}}=1>0 \implies\sum_{n=1}^\infty \log(1+ a_n) \text{ converges absolutely}.$$ How to prove the converse part?",,['real-analysis']
45,evil derivative,evil derivative,,"I'm looking at the following statement in my textbook: let $u : (0, 1) → (0, 1)$ the devil's staircase function , aka Cantor-Lebesgue function. Then it's derivative is $u' = 0$ pointwise a.e. We obtain $$\int_0^1 u \, \varphi_k' \to -1 \, \, (k \to \infty)$$   if we choose $(\varphi_k)_k \subset C^\infty_c((0,1))$ suitably with $\varphi_k → \chi_{(0,1)}$ $(k → ∞)$ the distributional derivative does not vanish, hence it can't have a weak derivative in $L^1_{\text{loc}}((0,1))$ I get why the the derivative is zero a.e., and why the weak derivative should be zero, if it existed but I don't understand the reasoning with the $\varphi_k$! Why does this converge to $-1$? And what is ""suitably""? Help is much appreciated! Edit: I made a mistake in my first version; it should be $\varphi_k'$ in the integral with $\varphi_k \to \chi_{(0,1)}$ Now it's updated correctly","I'm looking at the following statement in my textbook: let $u : (0, 1) → (0, 1)$ the devil's staircase function , aka Cantor-Lebesgue function. Then it's derivative is $u' = 0$ pointwise a.e. We obtain $$\int_0^1 u \, \varphi_k' \to -1 \, \, (k \to \infty)$$   if we choose $(\varphi_k)_k \subset C^\infty_c((0,1))$ suitably with $\varphi_k → \chi_{(0,1)}$ $(k → ∞)$ the distributional derivative does not vanish, hence it can't have a weak derivative in $L^1_{\text{loc}}((0,1))$ I get why the the derivative is zero a.e., and why the weak derivative should be zero, if it existed but I don't understand the reasoning with the $\varphi_k$! Why does this converge to $-1$? And what is ""suitably""? Help is much appreciated! Edit: I made a mistake in my first version; it should be $\varphi_k'$ in the integral with $\varphi_k \to \chi_{(0,1)}$ Now it's updated correctly",,"['real-analysis', 'distribution-theory', 'weak-derivatives']"
46,Why was it necessary for the Riemann integral to consider all partitions and taggings?,Why was it necessary for the Riemann integral to consider all partitions and taggings?,,"Suppose we have a function $f:[a,b]\to\mathbb{R}$. The Riemann integral $\int$ of the function is defined somewhat like this: For any tagged partition $\dot{P}=(P,t_P)$, (where $P$ is the partition, and $t_P=\{t_i\}$ the corresponding tagging) of $[a,b]$, denote the Riemann sum of $f$ corresponding to $\dot{P}$ by $S(f,\dot{P})$. If $S(f,\dot{P})$ approaches a definite value $I$ as $||\dot{P}||\to0$, then $$\int f = I$$ By the definition, $\dot{P}$ runs over the set of all tagged partitions of [$a,b]$, taking into account any partitioning and any tagging for that particular partition. Is this much freedom of choice really necessary? From this question , I know that if we specify the choices for both $P$ and $t$, the integral is no longer equivalent to the Riemann integral (In the link, $P$ is always an equipartition, and $t$ is the set of the left endpoints of $P$). What if we fix exactly one of $P,t$? Is the resulting ""integral"" still equivalent to the Riemann integral? For example, if I define another integral $\int_1$ similarly (but specifying choice of $P$): For $n\in\mathbb{N}$ define $P_n$ to be the equipartition of $[a,b]$ into $n$ subintervals of equal length, and for any tagging $t_{P_n}$ of $P_n$ denote the corresponding tagged partition as $\dot{P}_{n,t}$. If $S(f,\dot{P}_{n,t})$ approaches a definite value $I_1$ as $n\to \infty$ then    $$\int_1f = I_1$$ I can similarly define some other $\int_2$ by considering all partitions, but  specifying $t$, for example, by taking the mid-points of each subinterval of a given partition. Will this $\int_1$ or $\int_2$ be equivalent to $\int$? More or less, what is the advantage of simultaneously varying $P$ as well as $t$, which couldn't be achieved by doing that one at a time? [Most probably my definitions are not extremely precise, I apologise for those]","Suppose we have a function $f:[a,b]\to\mathbb{R}$. The Riemann integral $\int$ of the function is defined somewhat like this: For any tagged partition $\dot{P}=(P,t_P)$, (where $P$ is the partition, and $t_P=\{t_i\}$ the corresponding tagging) of $[a,b]$, denote the Riemann sum of $f$ corresponding to $\dot{P}$ by $S(f,\dot{P})$. If $S(f,\dot{P})$ approaches a definite value $I$ as $||\dot{P}||\to0$, then $$\int f = I$$ By the definition, $\dot{P}$ runs over the set of all tagged partitions of [$a,b]$, taking into account any partitioning and any tagging for that particular partition. Is this much freedom of choice really necessary? From this question , I know that if we specify the choices for both $P$ and $t$, the integral is no longer equivalent to the Riemann integral (In the link, $P$ is always an equipartition, and $t$ is the set of the left endpoints of $P$). What if we fix exactly one of $P,t$? Is the resulting ""integral"" still equivalent to the Riemann integral? For example, if I define another integral $\int_1$ similarly (but specifying choice of $P$): For $n\in\mathbb{N}$ define $P_n$ to be the equipartition of $[a,b]$ into $n$ subintervals of equal length, and for any tagging $t_{P_n}$ of $P_n$ denote the corresponding tagged partition as $\dot{P}_{n,t}$. If $S(f,\dot{P}_{n,t})$ approaches a definite value $I_1$ as $n\to \infty$ then    $$\int_1f = I_1$$ I can similarly define some other $\int_2$ by considering all partitions, but  specifying $t$, for example, by taking the mid-points of each subinterval of a given partition. Will this $\int_1$ or $\int_2$ be equivalent to $\int$? More or less, what is the advantage of simultaneously varying $P$ as well as $t$, which couldn't be achieved by doing that one at a time? [Most probably my definitions are not extremely precise, I apologise for those]",,"['real-analysis', 'soft-question', 'riemann-integration']"
47,"If $x_{n+1}=f(x_n)$ and $x_{n+1}-x_n\to 0$, then $\{x_n\}$ converges","If  and , then  converges",x_{n+1}=f(x_n) x_{n+1}-x_n\to 0 \{x_n\},"Let $f:[0,1] \rightarrow [0,1]$be a continuous function. Choose any point $x_0 \in [0,1]$ and define a sequence recursively by $x_{n+1}=f(x_n)$. Suppose $\lim_{n \rightarrow \infty}x_{n+1}-x_n =0$, does this sequence converge?","Let $f:[0,1] \rightarrow [0,1]$be a continuous function. Choose any point $x_0 \in [0,1]$ and define a sequence recursively by $x_{n+1}=f(x_n)$. Suppose $\lim_{n \rightarrow \infty}x_{n+1}-x_n =0$, does this sequence converge?",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'fixed-point-theorems']"
48,Is the following set closed in $\ell_{p}$ for $1\le p$?,Is the following set closed in  for ?,\ell_{p} 1\le p,"Let $S=\left\{ \left\{a_n\right\}_{n=1}^\infty\in\ell_{p}\mid  \sum_{n=1}^\infty {a_n}=1, a_n\ge0 \ \forall \ n\in\Bbb N \right\}$ Is the set $S$ closed in the normed space   $\left(\ell_{p},\|\cdot\|_{p}\right)$ for $1\le p$ ? I was able to conclude that S is actually closed in $\left(\ell_{1},\|\cdot\|_{1}\right)$ by proceeding the following way: Let $\left\{y_{n}\right\}_{n=1}^\infty$ be a sequence (of sequences) such that $y_n=\left\{y_{k}^{n}\right\}_{k=1}^\infty\in S\ \forall \ n\in\Bbb N$ and $y_{n}\rightarrow x$ as $n\rightarrow\infty$, where $x=\left\{x_k\right\}_{k=1}^\infty\in \ell_{1}$ If we prove that $x\in S$, then S would be closed. Given any fixed $k\in\Bbb N$, we can deduce from the convergence of $y_{n}$ to $x$ as $n\rightarrow\infty$ that $y_{k}^{n}\rightarrow x_k$ as $n\rightarrow\infty$. Since $y_{k}^{n}\ge0\  \forall \ k\in\Bbb N$ then $x_k\ge0$. Therefore $x_k\ge0\ \forall \ k\in\Bbb N$. Now, since $y_n\rightarrow x$ as $n\rightarrow\infty$ and $\|y_n\|_{1}=1\ \forall \ n\in\Bbb N$, by continuity of the norm we obtain that $1=\|y_n\|_{1}\rightarrow \|x\|_{1}$ as $n\rightarrow\infty$. Then, $\|x\|_{1}=1$, which implies that $\|x\|_{1}=\sum_{k=1}^\infty {|x_k|}=\sum_{k=1}^\infty {x_k}=1$. Therefore $x\in S$. However, I haven't been able to prove or disprove the closedness of $S$ for the case in which $p\gt1$. I was trying to follow the same proof, but I can't conclude that $\sum_{k=1}^\infty {x_k}=1$ (at most I can conclude that $\sum_{k=1}^\infty {x_k^{p}}=1$ by continuity of the norm in $\ell_{p}$). Since the proof failed I started to think that the set might not be closed for the case in which $p\gt1$, but I haven't been to prove that it isn't closed either. Any hints or ideas would be highly appreciated.","Let $S=\left\{ \left\{a_n\right\}_{n=1}^\infty\in\ell_{p}\mid  \sum_{n=1}^\infty {a_n}=1, a_n\ge0 \ \forall \ n\in\Bbb N \right\}$ Is the set $S$ closed in the normed space   $\left(\ell_{p},\|\cdot\|_{p}\right)$ for $1\le p$ ? I was able to conclude that S is actually closed in $\left(\ell_{1},\|\cdot\|_{1}\right)$ by proceeding the following way: Let $\left\{y_{n}\right\}_{n=1}^\infty$ be a sequence (of sequences) such that $y_n=\left\{y_{k}^{n}\right\}_{k=1}^\infty\in S\ \forall \ n\in\Bbb N$ and $y_{n}\rightarrow x$ as $n\rightarrow\infty$, where $x=\left\{x_k\right\}_{k=1}^\infty\in \ell_{1}$ If we prove that $x\in S$, then S would be closed. Given any fixed $k\in\Bbb N$, we can deduce from the convergence of $y_{n}$ to $x$ as $n\rightarrow\infty$ that $y_{k}^{n}\rightarrow x_k$ as $n\rightarrow\infty$. Since $y_{k}^{n}\ge0\  \forall \ k\in\Bbb N$ then $x_k\ge0$. Therefore $x_k\ge0\ \forall \ k\in\Bbb N$. Now, since $y_n\rightarrow x$ as $n\rightarrow\infty$ and $\|y_n\|_{1}=1\ \forall \ n\in\Bbb N$, by continuity of the norm we obtain that $1=\|y_n\|_{1}\rightarrow \|x\|_{1}$ as $n\rightarrow\infty$. Then, $\|x\|_{1}=1$, which implies that $\|x\|_{1}=\sum_{k=1}^\infty {|x_k|}=\sum_{k=1}^\infty {x_k}=1$. Therefore $x\in S$. However, I haven't been able to prove or disprove the closedness of $S$ for the case in which $p\gt1$. I was trying to follow the same proof, but I can't conclude that $\sum_{k=1}^\infty {x_k}=1$ (at most I can conclude that $\sum_{k=1}^\infty {x_k^{p}}=1$ by continuity of the norm in $\ell_{p}$). Since the proof failed I started to think that the set might not be closed for the case in which $p\gt1$, but I haven't been to prove that it isn't closed either. Any hints or ideas would be highly appreciated.",,"['real-analysis', 'functional-analysis']"
49,Difficult Intermediate Value Theorem Problem- two roots,Difficult Intermediate Value Theorem Problem- two roots,,I have been stuck on this Real Analysis problem for hours and am just totally clueless- I am sure it is some application of the Intermediate Value Theorem- suppose $\ f: \mathbb{R}\rightarrow\mathbb{R} $ is continuous at every point. Prove that the equation $ \ f(x) = c $ cannot have exactly two solutions for every value of $\ c. $ Would appreciate some help Thanks,I have been stuck on this Real Analysis problem for hours and am just totally clueless- I am sure it is some application of the Intermediate Value Theorem- suppose $\ f: \mathbb{R}\rightarrow\mathbb{R} $ is continuous at every point. Prove that the equation $ \ f(x) = c $ cannot have exactly two solutions for every value of $\ c. $ Would appreciate some help Thanks,,['real-analysis']
50,$\lim_{n\to\infty} \frac{1}{\log(n)}\sum _{k=1}^n \frac{\cos (\sin (2 \pi \log (k)))}{k}$,,\lim_{n\to\infty} \frac{1}{\log(n)}\sum _{k=1}^n \frac{\cos (\sin (2 \pi \log (k)))}{k},What tools would you gladly recommend me for computing precisely the limit below? Maybe a starting point? $$\lim_{n\to\infty} \frac{1}{\log(n)}\sum _{k=1}^n \frac{\cos (\sin (2 \pi  \log (k)))}{k}$$,What tools would you gladly recommend me for computing precisely the limit below? Maybe a starting point? $$\lim_{n\to\infty} \frac{1}{\log(n)}\sum _{k=1}^n \frac{\cos (\sin (2 \pi  \log (k)))}{k}$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
51,Show the sequence converges to M,Show the sequence converges to M,,"Assume $f : [a,b] \to R$ is continuous and $f(x) \ge 0$ for all $x \in [a,b]$, and $M = \sup\{f(x) : x \in [a,b]\}$. Show that $$\lim_{n\to\infty}\left[\int_a^bf(x)^ndx\right]^{1/n}$$ converges to $M$. I know you have to get $|[\int_a^b[f(x)^ndx]^{1/n}-M| < \epsilon$ by the definition of a convergent sequence, but I'm not sure how to simplify the left side of this inequality. Are you suppose to look at the function inside the sequence as a composition of two functions maybe $f(x)$ and $g(x) = x^n$ or use a change of variable to better integrate this function?","Assume $f : [a,b] \to R$ is continuous and $f(x) \ge 0$ for all $x \in [a,b]$, and $M = \sup\{f(x) : x \in [a,b]\}$. Show that $$\lim_{n\to\infty}\left[\int_a^bf(x)^ndx\right]^{1/n}$$ converges to $M$. I know you have to get $|[\int_a^b[f(x)^ndx]^{1/n}-M| < \epsilon$ by the definition of a convergent sequence, but I'm not sure how to simplify the left side of this inequality. Are you suppose to look at the function inside the sequence as a composition of two functions maybe $f(x)$ and $g(x) = x^n$ or use a change of variable to better integrate this function?",,"['real-analysis', 'integration', 'sequences-and-series', 'analysis']"
52,Is it possible to extend a $C^1$-function smoothly from any Lipschitz domain?,Is it possible to extend a -function smoothly from any Lipschitz domain?,C^1,"If $\Omega$ is a cube in $\mathbb{R}^n$ and $f\in C^1(\overline\Omega)$. By reflection one can extend such a function to all of $\mathbb{R}^n$ and the extenstion is in $C^1(\mathbb{R}^n)$.  If $\Omega$ is a polygon, has piecewise $C^1$ boundary (so edges and corneres are not to wild) or is a convex set this still seems to be possible.  Can this be extended to arbitrary Lipschitz domains? Are there examples and or references for these cases (starting from polygons)?","If $\Omega$ is a cube in $\mathbb{R}^n$ and $f\in C^1(\overline\Omega)$. By reflection one can extend such a function to all of $\mathbb{R}^n$ and the extenstion is in $C^1(\mathbb{R}^n)$.  If $\Omega$ is a polygon, has piecewise $C^1$ boundary (so edges and corneres are not to wild) or is a convex set this still seems to be possible.  Can this be extended to arbitrary Lipschitz domains? Are there examples and or references for these cases (starting from polygons)?",,"['real-analysis', 'reference-request']"
53,Infinite Series $\sum\limits_{n=1}^{\infty}\frac{(m-1)^n-1}{m^n}\zeta(n+1)$,Infinite Series,\sum\limits_{n=1}^{\infty}\frac{(m-1)^n-1}{m^n}\zeta(n+1),How to prove the following identity? $$\sum_{n=1}^{\infty}\frac{(m-1)^n-1}{m^n}\zeta(n+1)=\pi\cot\left(\frac{\pi}{m}\right)$$,How to prove the following identity? $$\sum_{n=1}^{\infty}\frac{(m-1)^n-1}{m^n}\zeta(n+1)=\pi\cot\left(\frac{\pi}{m}\right)$$,,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'closed-form']"
54,Does $\sum_{n=3}^\infty \frac {1}{(\log n)^{\log(\log(n)}}$ converge?,Does  converge?,\sum_{n=3}^\infty \frac {1}{(\log n)^{\log(\log(n)}},Does the following series converge: $$\sum_{n=3}^\infty \frac {1}{(\log n)^{\log(\log(n)}}$$ I've tried all test I know... Any ideas ?,Does the following series converge: $$\sum_{n=3}^\infty \frac {1}{(\log n)^{\log(\log(n)}}$$ I've tried all test I know... Any ideas ?,,"['real-analysis', 'sequences-and-series']"
55,Does this double series converge?,Does this double series converge?,,"$$\sum\limits_{y=1}^{Y}\sum\limits_{z=1}^{y} a^{y-1} b^y \binom{y-1}{z-1} (c + 2z)^d $$ Does this series converge when $Y=∞$? If the series converges, what does it converge to? If the series does not converge, how can I evaluate it for a finite $Y$? where: $0 \lt a \lt 1$ $0 \lt b \lt 1$ $0 \lt d \lt 1$ $0 \lt c$","$$\sum\limits_{y=1}^{Y}\sum\limits_{z=1}^{y} a^{y-1} b^y \binom{y-1}{z-1} (c + 2z)^d $$ Does this series converge when $Y=∞$? If the series converges, what does it converge to? If the series does not converge, how can I evaluate it for a finite $Y$? where: $0 \lt a \lt 1$ $0 \lt b \lt 1$ $0 \lt d \lt 1$ $0 \lt c$",,"['real-analysis', 'combinatorics', 'sequences-and-series', 'summation']"
56,"Given a bounded set $E$, there is a $G_\delta$ set such that the outer measures are equal","Given a bounded set , there is a  set such that the outer measures are equal",E G_\delta,"Question: Show that for any bounded set $E \in \mathbb{R}$, there is a $G_\delta$ set $G$ for which $E \subseteq G$ and $m^*(E)=m^*(G)$. Let $\{I_n\}$ be a countable collection of open intervals such that $E \subset \bigcup\limits_{n=1}^{\infty} I_n$.  Observe the following: $$m^*(E) \leq m^*(\bigcup\limits_{n=1}^{\infty} I_n) \leq \sum\limits_{n=1}^\infty l(I_n) < m^*(E)+\frac{1}{2^n}.$$ Now let $G=\bigcap\bigcup I_n$.  $G$ is a $G_\delta$ set.  From this, we see that $E \subset G$.  Thus for each $n \in \mathbb{N}$: $$m^*(E) \leq m^*(G) \leq m^*(\bigcup\limits_{n=1}^{\infty} I_n) \leq \sum\limits_{n=1}^\infty l(I_n) < m^*(E)+\frac{1}{2^n}.$$ So we take $n \rightarrow \infty$. Is my proof correct?","Question: Show that for any bounded set $E \in \mathbb{R}$, there is a $G_\delta$ set $G$ for which $E \subseteq G$ and $m^*(E)=m^*(G)$. Let $\{I_n\}$ be a countable collection of open intervals such that $E \subset \bigcup\limits_{n=1}^{\infty} I_n$.  Observe the following: $$m^*(E) \leq m^*(\bigcup\limits_{n=1}^{\infty} I_n) \leq \sum\limits_{n=1}^\infty l(I_n) < m^*(E)+\frac{1}{2^n}.$$ Now let $G=\bigcap\bigcup I_n$.  $G$ is a $G_\delta$ set.  From this, we see that $E \subset G$.  Thus for each $n \in \mathbb{N}$: $$m^*(E) \leq m^*(G) \leq m^*(\bigcup\limits_{n=1}^{\infty} I_n) \leq \sum\limits_{n=1}^\infty l(I_n) < m^*(E)+\frac{1}{2^n}.$$ So we take $n \rightarrow \infty$. Is my proof correct?",,['real-analysis']
57,Evaluating integral using Riemann sums,Evaluating integral using Riemann sums,,"It is given that: $$\sin\frac{\pi }{n} \sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}=\frac{n}{2^{n-1}}$$ It is asked to use the above identity to evaluate the following improper integral: $$\int_0^\pi  \log(\sin x) \, dx$$ I used the definition of the integral in terms of Riemann sums: $$\begin{align*}\int_0^\pi \log(\sin x) \, dx &=\lim_{n\rightarrow \infty }\frac{\pi }{n}\left[\sum_{k=1}^{k=n-1}\log\left(\sin\left(\frac{k\pi }{n}\right)\right)\right]\\ &=\lim_{n\rightarrow \infty }\frac{\pi }{n}\left[\log\left(\sin\frac{\pi }{n}\sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}\right)\right]\\ &=\lim_{n\rightarrow \infty }\frac{\pi }{n}\Big(\log n-(n-1)\log 2\Big)\\ &=-\pi \log 2 \end{align*}$$ However, this integral is improper, so $\log(\sin(\pi ))=\log(0)=-\infty $. I am kind of cheating in my solution, because the Riemann sum above should be:  $$\sum_{k=1}^{k=n-1}\log\left(\sin\left(\frac{k\pi }{n}\right)\right)+\frac{\pi }{n}\log\left(\sin\left(\frac{n\pi }{n}\right)\right)\;,$$ but I have no idea how to deal with the last term of the sum since $\sin\left(\frac{n\pi }{n}\right)=\sin(\pi)=0 $. Can anyone show me how to deal with this? Also, if someone knows how to prove the first identity: $$\sin\frac{\pi }{n}\sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}=\frac{n}{2^{n-1}}$$  please write down your proof below? Thanks","It is given that: $$\sin\frac{\pi }{n} \sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}=\frac{n}{2^{n-1}}$$ It is asked to use the above identity to evaluate the following improper integral: $$\int_0^\pi  \log(\sin x) \, dx$$ I used the definition of the integral in terms of Riemann sums: $$\begin{align*}\int_0^\pi \log(\sin x) \, dx &=\lim_{n\rightarrow \infty }\frac{\pi }{n}\left[\sum_{k=1}^{k=n-1}\log\left(\sin\left(\frac{k\pi }{n}\right)\right)\right]\\ &=\lim_{n\rightarrow \infty }\frac{\pi }{n}\left[\log\left(\sin\frac{\pi }{n}\sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}\right)\right]\\ &=\lim_{n\rightarrow \infty }\frac{\pi }{n}\Big(\log n-(n-1)\log 2\Big)\\ &=-\pi \log 2 \end{align*}$$ However, this integral is improper, so $\log(\sin(\pi ))=\log(0)=-\infty $. I am kind of cheating in my solution, because the Riemann sum above should be:  $$\sum_{k=1}^{k=n-1}\log\left(\sin\left(\frac{k\pi }{n}\right)\right)+\frac{\pi }{n}\log\left(\sin\left(\frac{n\pi }{n}\right)\right)\;,$$ but I have no idea how to deal with the last term of the sum since $\sin\left(\frac{n\pi }{n}\right)=\sin(\pi)=0 $. Can anyone show me how to deal with this? Also, if someone knows how to prove the first identity: $$\sin\frac{\pi }{n}\sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}=\frac{n}{2^{n-1}}$$  please write down your proof below? Thanks",,"['calculus', 'real-analysis', 'analysis']"
58,Least upper bound property iff convergence of Cauchy sequences,Least upper bound property iff convergence of Cauchy sequences,,"From http://en.wikipedia.org/wiki/Non-Archimedean_ordered_field : The field of rational functions over $\mathbb{R}$ can be used to   construct an ordered field which is complete (in the sense of   convergence of Cauchy sequences) but is not the real numbers.   Sometimes the term complete is used to mean that the least upper bound   property holds. With this meaning of complete there are no complete   non-Archemedian ordered fields. The subtle distinction between these   two uses of the word complete is occasionally a source of confusion. I had the idea that, in an ordered field, completeness in the sense of convergence of Cauchy sequences is equivalent to convergence in the sense that the least upper bound property holds. Now that I think about it, I know how to prove that the least upper bound property implies the convergence of Cauchy sequences, but I'm not so sure about the converse. According to this Wikipedia page, the converse is not true. Can anyone confirm this? (I need to be sure this is not an error on Wikipedia) If possible, provide a counter-example (perhaps the field of rational functions suggested in the quote, but how is the structure, i.e., ordering, operations, etc., defined?). Thanks.","From http://en.wikipedia.org/wiki/Non-Archimedean_ordered_field : The field of rational functions over $\mathbb{R}$ can be used to   construct an ordered field which is complete (in the sense of   convergence of Cauchy sequences) but is not the real numbers.   Sometimes the term complete is used to mean that the least upper bound   property holds. With this meaning of complete there are no complete   non-Archemedian ordered fields. The subtle distinction between these   two uses of the word complete is occasionally a source of confusion. I had the idea that, in an ordered field, completeness in the sense of convergence of Cauchy sequences is equivalent to convergence in the sense that the least upper bound property holds. Now that I think about it, I know how to prove that the least upper bound property implies the convergence of Cauchy sequences, but I'm not so sure about the converse. According to this Wikipedia page, the converse is not true. Can anyone confirm this? (I need to be sure this is not an error on Wikipedia) If possible, provide a counter-example (perhaps the field of rational functions suggested in the quote, but how is the structure, i.e., ordering, operations, etc., defined?). Thanks.",,"['real-analysis', 'abstract-algebra', 'field-theory', 'order-theory']"
59,"Are Vitali sets dense in [0,1)?","Are Vitali sets dense in [0,1)?",,"I am trying to get a better handle on the nature of Vitali sets, generated by choosing representatives in $[0,1)$ from the equivalence classes $\mathbb{Q} + r$ where $r \in \mathbb{R}$. If $V$ is a Vitali set described above, I understand that $V$ contains only a single rational number and that it is uncountable.  Also, its complement $[0,1) \sim V$ is uncountable since we have excluded from $V$ a collection of elements in $[0,1)$ associated to each of the uncountable number of elements in $V$. However, I have been unable to find (or determine) if $V$ is dense in [0,1).  Is this known?","I am trying to get a better handle on the nature of Vitali sets, generated by choosing representatives in $[0,1)$ from the equivalence classes $\mathbb{Q} + r$ where $r \in \mathbb{R}$. If $V$ is a Vitali set described above, I understand that $V$ contains only a single rational number and that it is uncountable.  Also, its complement $[0,1) \sim V$ is uncountable since we have excluded from $V$ a collection of elements in $[0,1)$ associated to each of the uncountable number of elements in $V$. However, I have been unable to find (or determine) if $V$ is dense in [0,1).  Is this known?",,"['real-analysis', 'measure-theory']"
60,Transfinite series: Uncountable sums,Transfinite series: Uncountable sums,,"If you sum an expression over an uncountable set $\sum_{x\in \mathbb{R}}f(x)$, then do we need $f(x)=0$ on all but a countable subset in order for the sum to have a finite value? If not can you give an example of a function everywhere nonzero that has a transfinite sum with a finite value? Possible keywords: Transseries, Écalle–Borel Summation, analyzable function Transseries for beginners , GA Edgar, 2009","If you sum an expression over an uncountable set $\sum_{x\in \mathbb{R}}f(x)$, then do we need $f(x)=0$ on all but a countable subset in order for the sum to have a finite value? If not can you give an example of a function everywhere nonzero that has a transfinite sum with a finite value? Possible keywords: Transseries, Écalle–Borel Summation, analyzable function Transseries for beginners , GA Edgar, 2009",,"['analysis', 'real-analysis']"
61,Positive integers $k = p_{1}^{r_{1}} \cdots p_{n}^{r_{n}} > 1$ satisfying $\sum_{i = 1}^{n} p_{i}^{-r_{i}} < 1$,Positive integers  satisfying,k = p_{1}^{r_{1}} \cdots p_{n}^{r_{n}} > 1 \sum_{i = 1}^{n} p_{i}^{-r_{i}} < 1,"A divisor $d$ of $k = p_{1}^{r_{1}} \cdots p_{n}^{r_{n}}$ is unitary if and only if $d = p_{1}^{\varepsilon_{1}} \cdots p_{n}^{\varepsilon_{n}}$, where each exponent $\varepsilon_{i}$ is either $0$ or $r_{i}$. Let $D_{k} = ${ $d$ } be the subset of unitary divisors of an integer $k > 1$ satisfying $\omega(d) = \omega(k) - 1$. Definition . A positive integer $k = p_{1}^{r_{1}} \cdots p_{n}^{r_{n}} > 1$ is hyperbolic if and only if $\sum_{i = 1}^{n} p_{i}^{-r_{i}} < 1$ or, equivalently, $\sum_{d \in D_{k}} d < k$. See my OEIS entry. For example, $3$, $10$ and $20$ are hyperbolic, but $30$ and $510510 = 2 \cdot 3 \cdot 5 \cdot 7 \cdot 11 \cdot 13 \cdot 17$ are not. Assuming my calculations are correct, I'll state the following with some confidence: Indeed, many positive integers are hyperbolic. Of the first $10^{8}$ integers, $70334760$ are hyperbolic. Non-trivial prime powers, squares or higher, are also hyperbolic. If $k$ is a hyperbolic integer, then so are its proper, non-trivial unitary divisors; however, the same cannot be inferred for all divisors (consider the hyperbolic integer $900$ and its non-hyperbolic divisor $30$). In fact, an arbitrary product of any number of non-trivial unitary divisors of a hyperbolic integer is again hyperbolic. The set of hyperbolic integers is closed under exponentiation , but not under addition (e.g., $10 + 20 = 30$) or under multiplication (e.g., $3 \times 10 = 30$). Define the Prime zeta function , $\zeta_{P}(s) = \sum_{p \text{ prime}} p^{-s}$, which converges absolutely for $\mathsf{Re}(s) > 1$. Recall that the multiplicity of a prime divisor $p$ of $k$ is the largest exponent $r$ such that $p^{r}$ divides $k$ but $p^{r+1}$ does not. If the minimum multiplicity of an integer $k$ is $2$ or greater, then $k$ is hyperbolic as can be seen by the elementary bound \begin{eqnarray} \sum_{i = 1}^{n} p_{i}^{-r_{i}} < \zeta_{P}(2) \approx 0.452247 .... \end{eqnarray} Thus, the question of hyperbolicity is non-trivial only for integers with minimum multiplicity $1$. Numerical evidence suggests that the natural density of the hyperbolic integers is greater than $0.988284 \dots$, and I conjecture that almost all integers are indeed hyperbolic (i.e., the natural density is 1). Question: Is anything presently known about such integers? (References welcome!) Question: Is there a simple proof showing (or refuting) that almost all integers are hyperbolic? Thanks!","A divisor $d$ of $k = p_{1}^{r_{1}} \cdots p_{n}^{r_{n}}$ is unitary if and only if $d = p_{1}^{\varepsilon_{1}} \cdots p_{n}^{\varepsilon_{n}}$, where each exponent $\varepsilon_{i}$ is either $0$ or $r_{i}$. Let $D_{k} = ${ $d$ } be the subset of unitary divisors of an integer $k > 1$ satisfying $\omega(d) = \omega(k) - 1$. Definition . A positive integer $k = p_{1}^{r_{1}} \cdots p_{n}^{r_{n}} > 1$ is hyperbolic if and only if $\sum_{i = 1}^{n} p_{i}^{-r_{i}} < 1$ or, equivalently, $\sum_{d \in D_{k}} d < k$. See my OEIS entry. For example, $3$, $10$ and $20$ are hyperbolic, but $30$ and $510510 = 2 \cdot 3 \cdot 5 \cdot 7 \cdot 11 \cdot 13 \cdot 17$ are not. Assuming my calculations are correct, I'll state the following with some confidence: Indeed, many positive integers are hyperbolic. Of the first $10^{8}$ integers, $70334760$ are hyperbolic. Non-trivial prime powers, squares or higher, are also hyperbolic. If $k$ is a hyperbolic integer, then so are its proper, non-trivial unitary divisors; however, the same cannot be inferred for all divisors (consider the hyperbolic integer $900$ and its non-hyperbolic divisor $30$). In fact, an arbitrary product of any number of non-trivial unitary divisors of a hyperbolic integer is again hyperbolic. The set of hyperbolic integers is closed under exponentiation , but not under addition (e.g., $10 + 20 = 30$) or under multiplication (e.g., $3 \times 10 = 30$). Define the Prime zeta function , $\zeta_{P}(s) = \sum_{p \text{ prime}} p^{-s}$, which converges absolutely for $\mathsf{Re}(s) > 1$. Recall that the multiplicity of a prime divisor $p$ of $k$ is the largest exponent $r$ such that $p^{r}$ divides $k$ but $p^{r+1}$ does not. If the minimum multiplicity of an integer $k$ is $2$ or greater, then $k$ is hyperbolic as can be seen by the elementary bound \begin{eqnarray} \sum_{i = 1}^{n} p_{i}^{-r_{i}} < \zeta_{P}(2) \approx 0.452247 .... \end{eqnarray} Thus, the question of hyperbolicity is non-trivial only for integers with minimum multiplicity $1$. Numerical evidence suggests that the natural density of the hyperbolic integers is greater than $0.988284 \dots$, and I conjecture that almost all integers are indeed hyperbolic (i.e., the natural density is 1). Question: Is anything presently known about such integers? (References welcome!) Question: Is there a simple proof showing (or refuting) that almost all integers are hyperbolic? Thanks!",,"['number-theory', 'real-analysis', 'analysis', 'asymptotics']"
62,"If $F:\mathbb R^m\to \mathbb R^m$ is continuous with $|F(x)-F(y)|\geq \lambda|x-y|$, then $F$ is surjective.","If  is continuous with , then  is surjective.",F:\mathbb R^m\to \mathbb R^m |F(x)-F(y)|\geq \lambda|x-y| F,"I know that this question has been asked for several times (such as this post and the one-dimensional case ). However, I still couldn't find an answer without using invariance of domain. The original question is stated as: If $F:\mathbb R^m\to \mathbb R^m$ is continuous with $|F(x)-F(y)|\geq \lambda |x-y|$ for some $\lambda>0$ , then $F$ is a homeomorphism. Here is my attempt: It is clear that $F$ is injective and $F^{-1}:F(\mathbb R^m)\to \mathbb R^m$ is continuous. Hence,  it remains to show that $F$ is surjective. As $\mathbb R^m$ is connected, we can infer that $F(\mathbb R^m)=\mathbb R^m$ by proving that $F(\mathbb R^m)$ is a clopen subset. The closedness of $F(\mathbb R^m)$ is easy: Suppose $q_n$ is a sequence in $F(R^m)$ with $q_n\to q$ . Then there exists $p_n\in \mathbb R^m$ such that $F(p_n)=q_n$ . Since $|q_n-q_m|\geq \lambda |p_n-p_m|$ and $\{q_n\}$ is Cauchy, $p_n$ is Cauchy, which implies that $p_n\to p$ for some $p\in \mathbb R^m$ . Them by the continuity of $F$ , $q=F(p)\in F(\mathbb R^m)$ . However, I still need show that $F(\mathbb R^m)$ is open and that is where I get stumped. Since it  is a question given in our introductory real analysis courses, invariance of domain is completely out of scope of this curriculum. Thus, I seek for an elementary solution.","I know that this question has been asked for several times (such as this post and the one-dimensional case ). However, I still couldn't find an answer without using invariance of domain. The original question is stated as: If is continuous with for some , then is a homeomorphism. Here is my attempt: It is clear that is injective and is continuous. Hence,  it remains to show that is surjective. As is connected, we can infer that by proving that is a clopen subset. The closedness of is easy: Suppose is a sequence in with . Then there exists such that . Since and is Cauchy, is Cauchy, which implies that for some . Them by the continuity of , . However, I still need show that is open and that is where I get stumped. Since it  is a question given in our introductory real analysis courses, invariance of domain is completely out of scope of this curriculum. Thus, I seek for an elementary solution.",F:\mathbb R^m\to \mathbb R^m |F(x)-F(y)|\geq \lambda |x-y| \lambda>0 F F F^{-1}:F(\mathbb R^m)\to \mathbb R^m F \mathbb R^m F(\mathbb R^m)=\mathbb R^m F(\mathbb R^m) F(\mathbb R^m) q_n F(R^m) q_n\to q p_n\in \mathbb R^m F(p_n)=q_n |q_n-q_m|\geq \lambda |p_n-p_m| \{q_n\} p_n p_n\to p p\in \mathbb R^m F q=F(p)\in F(\mathbb R^m) F(\mathbb R^m),"['real-analysis', 'general-topology', 'open-map']"
63,Approximate Maclaurin series for $\sqrt x$,Approximate Maclaurin series for,\sqrt x,"One of my hobbies for the past while has been Taylor and Maclaurin series. I understand that since $ f(x)=\sqrt x $ isn't analytic at zero, we can't make a Maclaurin series expansion for it. However, after some playing around, I was able to construct a Maclaurin series which closely approximates square root of x. It isn't obvious to me why this function should exist (I tried searching for it), and I was hoping someone more knowledgeable could explain, or point me in the right direction at the very least. The function is: $$f(x)= \frac{1}{\sqrt \pi} \sum_{n=0}^{\infty}\frac{x^n (-1)^n}{n! (1-2n)}$$ Here's a plot for reference :) Approximate Maclaurin series for sqrt(x) Edits after some insight from Goncalo, I think I understand. I'll construct the approximation, but first some foreground The $\sqrt x \operatorname{erf}(\sqrt x) $ term seemed mysterious, and led me down a rabbit hole. I initally assumed that given an analytic function $ f(x) $ , $ f(\sqrt x)$ wouldn't have a Maclaurin series representation because $ \sqrt x $ isn't analytic at zero. However, as long as $ f(x) $ is an even function, we're in luck! Consider $$ f(x) = \cos(\sqrt x) $$ We could write this as $$ \cos(\sqrt x) = \sum_{n=0}^{\infty} \frac{x^\frac{n}{2} \operatorname{t}(n)}{n!} $$ Where $ \operatorname{t}(n) = \cos(\frac{\pi}{2}n)$ Now, since cosine is an even function, all the fractional power terms are cancelled (!!), meaning we can rearrange to $$ \cos(\sqrt x) =  \sum_{n=0}^{\infty} \frac{x^n \operatorname{t}(2n)}{n! \operatorname{P}(n,2n)} $$ Where $\operatorname{P}(k,x)$ is the falling factorial: $$ \operatorname{P}(k,x) = \prod_{m=0}^{k-1}x-m$$ Interestingly, this lets us find values for imaginary numbers, and smoothly extends the domain for $ \cos(\sqrt x) $ into the negative reals. We can use this trick on the integral of $ \operatorname{erf}(x) $ like so. First, set up this ugly but useful Maclaurin series, which represents the J $ _{th} $ integral of the Gaussian $ e^{-x^2} $ $$ \operatorname{t}(n,J) = \left\{n+\operatorname{mod}\left(J,2\right)=1:0,\frac{\cos\left(\frac{\pi}{2}\left(n-J\right)\right)P\left(\frac{n+1-J}{2},n+1-J\right)}{n+1-J}\right\} $$ Let $ \operatorname{even}(x) = \sum_{n=0}^{\infty}\frac{x^n t(n,2)}{n!} $ . This is the second integral of the Gaussian. Notice that if you normalize this function like so: $\frac{1}{\sqrt \pi}(2\operatorname{even}(x)-1)$ , it approximates $\operatorname{y}=x$ . But we'll leave it for now. Since $ \operatorname{even}(x) $ is an even function, we can use our trick to find $ \operatorname{even}(\sqrt x) $ We'll set up a new Maclaurin series based on our first one: $$ \operatorname{u}(n) = \frac{\operatorname{t}(2n, 2)}{P(n,2n)} $$ Now, we have $ \operatorname{even}(\sqrt x)=\sum_{n=0}^{\infty}\frac{x^n \operatorname{u}(n)}{n!}$ All that's left to do is normalize, and we get $$ \frac{1}{\sqrt \pi}(2\sum_{n=0}^\infty\frac{x^n \operatorname{u}(n)}{n!}-1) \approx \sqrt x $$ If anyone finds this interesting, here's a Desmos demo","One of my hobbies for the past while has been Taylor and Maclaurin series. I understand that since isn't analytic at zero, we can't make a Maclaurin series expansion for it. However, after some playing around, I was able to construct a Maclaurin series which closely approximates square root of x. It isn't obvious to me why this function should exist (I tried searching for it), and I was hoping someone more knowledgeable could explain, or point me in the right direction at the very least. The function is: Here's a plot for reference :) Approximate Maclaurin series for sqrt(x) Edits after some insight from Goncalo, I think I understand. I'll construct the approximation, but first some foreground The term seemed mysterious, and led me down a rabbit hole. I initally assumed that given an analytic function , wouldn't have a Maclaurin series representation because isn't analytic at zero. However, as long as is an even function, we're in luck! Consider We could write this as Where Now, since cosine is an even function, all the fractional power terms are cancelled (!!), meaning we can rearrange to Where is the falling factorial: Interestingly, this lets us find values for imaginary numbers, and smoothly extends the domain for into the negative reals. We can use this trick on the integral of like so. First, set up this ugly but useful Maclaurin series, which represents the J integral of the Gaussian Let . This is the second integral of the Gaussian. Notice that if you normalize this function like so: , it approximates . But we'll leave it for now. Since is an even function, we can use our trick to find We'll set up a new Maclaurin series based on our first one: Now, we have All that's left to do is normalize, and we get If anyone finds this interesting, here's a Desmos demo"," f(x)=\sqrt x  f(x)= \frac{1}{\sqrt \pi} \sum_{n=0}^{\infty}\frac{x^n (-1)^n}{n! (1-2n)} \sqrt x \operatorname{erf}(\sqrt x)   f(x)   f(\sqrt x)  \sqrt x   f(x)   f(x) = \cos(\sqrt x)   \cos(\sqrt x) = \sum_{n=0}^{\infty} \frac{x^\frac{n}{2} \operatorname{t}(n)}{n!}   \operatorname{t}(n) = \cos(\frac{\pi}{2}n)  \cos(\sqrt x) =  \sum_{n=0}^{\infty} \frac{x^n \operatorname{t}(2n)}{n! \operatorname{P}(n,2n)}  \operatorname{P}(k,x)  \operatorname{P}(k,x) = \prod_{m=0}^{k-1}x-m  \cos(\sqrt x)   \operatorname{erf}(x)   _{th}   e^{-x^2}   \operatorname{t}(n,J) = \left\{n+\operatorname{mod}\left(J,2\right)=1:0,\frac{\cos\left(\frac{\pi}{2}\left(n-J\right)\right)P\left(\frac{n+1-J}{2},n+1-J\right)}{n+1-J}\right\}   \operatorname{even}(x) = \sum_{n=0}^{\infty}\frac{x^n t(n,2)}{n!}  \frac{1}{\sqrt \pi}(2\operatorname{even}(x)-1) \operatorname{y}=x  \operatorname{even}(x)   \operatorname{even}(\sqrt x)   \operatorname{u}(n) = \frac{\operatorname{t}(2n, 2)}{P(n,2n)}   \operatorname{even}(\sqrt x)=\sum_{n=0}^{\infty}\frac{x^n \operatorname{u}(n)}{n!}  \frac{1}{\sqrt \pi}(2\sum_{n=0}^\infty\frac{x^n \operatorname{u}(n)}{n!}-1) \approx \sqrt x ","['real-analysis', 'sequences-and-series', 'taylor-expansion', 'power-series', 'radicals']"
64,An apparently simple but somehow unexpected inequality between integrals,An apparently simple but somehow unexpected inequality between integrals,,"Let $f:[0,1]\to[0,1]$ be any Lebesgue integrable function. Then $$I_1= \left(\int_0^1f(x)\sqrt x dx\right)^2 \leq \frac{4}{3}\int_0^1f(x)x^2dx=I_2\,.\tag{$\star$}$$ At first sight, I would have expected the reverse inequality, as $x^2\leq \sqrt x$ on $[0,1]$ and I didn't think that taking the square of the LHS would have been enough to compensate. However, it is easy to see that if $f=\chi_{[0, b]}$ , for some $b\in(0, 1]$ , where $\chi_A$ is the characteristic function of the set $A$ , then $I_1=I_2 = \frac{4}{9} b^3$ . On the other hand, if we let $[a, b]\subseteq[0,1]$ , then taking $f=\chi_{[a, b]}$ we have $I_1 = \frac{4}{9}(b^{3/2}-a^{3/2})^2$ and $I_2 = \frac{4}{9}(b^{3}-a^{3})$ . It is not hard to prove (for instance using the sub-additivity of $\sqrt\cdot$ ) that $I_1\leq I_2$ . Then for $\epsilon\in(0,1]$ and $f=\epsilon\chi_{[a,b]}$ it is clear that the same result holds true, and actually $I_1/I_2$ will scale as $\epsilon$ , so it is maximised for $\epsilon=1$ . (Note that allowing $\epsilon>1$ the inequality would not be true, but we would not be in the hypothesis $f\subseteq [0,1]$ .) So, for all $f$ which is a simple function (non-zero and constant on finitely many intervals) we get that ( $\star$ ) is satisfied. Taking the limit of simple functions we can obtain any measurable function, and so conclude. [This is wrong! See edit.] Now, as the inequality looks quite simple, I would expect that there is a much simpler proof for it, for instance something making use of Hölder's or Jensen's inequality. I might have missed something trivial, but so far I didn't manage to do it, and I have the feeling that I am not really grasping what is going on. Notice that we can restate ( $\star$ ) as $E_1^2\leq E_2$ , where $$E_1 = \frac{3}{2}\int_0^1 f(x)\sqrt x dx$$ and $$E_2 = 3 \int_0^1 f(x) x^2 dx\,.$$ The reason for this transform is that then you can see $\frac{3}{2}\sqrt x$ and $3 x^2$ as probability densities on $[0,1]$ , and so rewrite everything in terms of expectations of $f$ . However, so far I couldn't find any smart trick for a few-lines proof of ( $\star$ ), without having to pass via a sequence of simple functions converging to $f$ . Edit. I've actually just realised that the proof I had provided does not work, as if $f=\sum_i f_i =  \chi_{(a_i, b_i)}$ , then we just have $$I_1 = \left(\sum_i\int_0^1 f_i(x)\sqrt xdx\right)^2 \leq 2\sum_i \left(\int_0^1 f_i(x)\sqrt xdx\right)\leq 2 I_2\,.$$ However the proof from @Sangchul Lee shows that the inequality $I_1\leq I_2$ holds.","Let be any Lebesgue integrable function. Then At first sight, I would have expected the reverse inequality, as on and I didn't think that taking the square of the LHS would have been enough to compensate. However, it is easy to see that if , for some , where is the characteristic function of the set , then . On the other hand, if we let , then taking we have and . It is not hard to prove (for instance using the sub-additivity of ) that . Then for and it is clear that the same result holds true, and actually will scale as , so it is maximised for . (Note that allowing the inequality would not be true, but we would not be in the hypothesis .) So, for all which is a simple function (non-zero and constant on finitely many intervals) we get that ( ) is satisfied. Taking the limit of simple functions we can obtain any measurable function, and so conclude. [This is wrong! See edit.] Now, as the inequality looks quite simple, I would expect that there is a much simpler proof for it, for instance something making use of Hölder's or Jensen's inequality. I might have missed something trivial, but so far I didn't manage to do it, and I have the feeling that I am not really grasping what is going on. Notice that we can restate ( ) as , where and The reason for this transform is that then you can see and as probability densities on , and so rewrite everything in terms of expectations of . However, so far I couldn't find any smart trick for a few-lines proof of ( ), without having to pass via a sequence of simple functions converging to . Edit. I've actually just realised that the proof I had provided does not work, as if , then we just have However the proof from @Sangchul Lee shows that the inequality holds.","f:[0,1]\to[0,1] I_1= \left(\int_0^1f(x)\sqrt x dx\right)^2 \leq \frac{4}{3}\int_0^1f(x)x^2dx=I_2\,.\tag{\star} x^2\leq \sqrt x [0,1] f=\chi_{[0, b]} b\in(0, 1] \chi_A A I_1=I_2 = \frac{4}{9} b^3 [a, b]\subseteq[0,1] f=\chi_{[a, b]} I_1 = \frac{4}{9}(b^{3/2}-a^{3/2})^2 I_2 = \frac{4}{9}(b^{3}-a^{3}) \sqrt\cdot I_1\leq I_2 \epsilon\in(0,1] f=\epsilon\chi_{[a,b]} I_1/I_2 \epsilon \epsilon=1 \epsilon>1 f\subseteq [0,1] f \star \star E_1^2\leq E_2 E_1 = \frac{3}{2}\int_0^1 f(x)\sqrt x dx E_2 = 3 \int_0^1 f(x) x^2 dx\,. \frac{3}{2}\sqrt x 3 x^2 [0,1] f \star f f=\sum_i f_i =  \chi_{(a_i, b_i)} I_1 = \left(\sum_i\int_0^1 f_i(x)\sqrt xdx\right)^2 \leq 2\sum_i \left(\int_0^1 f_i(x)\sqrt xdx\right)\leq 2 I_2\,. I_1\leq I_2","['real-analysis', 'probability', 'inequality', 'definite-integrals', 'integral-inequality']"
65,Evaluating $\sum _{k=1}^{\infty }\frac{H_k}{4^k\left(2k+1\right)}\binom{2k}{k}$.,Evaluating .,\sum _{k=1}^{\infty }\frac{H_k}{4^k\left(2k+1\right)}\binom{2k}{k},"My attempt. $$\sum _{k=1}^{\infty }\frac{H_k}{4^k\left(2k+1\right)}\binom{2k}{k}$$ $$=\frac{1}{2}\sum _{k=1}^{\infty }\frac{H_k}{k\:4^k}\binom{2k}{k}-\frac{1}{2}\sum _{k=1}^{\infty }\frac{H_k}{k\:4^k\left(2k+1\right)}\binom{2k}{k}$$ The first sum can be evaluated easily if one uses the central binomial coefficient generating function , the closed form is $2\zeta \left(2\right)$ . For the remaining sum consider the $\arcsin$ series expansion. $$\sum _{k=0}^{\infty }\frac{x^{2k+1}}{4^k\left(2k+1\right)}\binom{2k}{k}=\arcsin \left(x\right)$$ $$\sum _{k=1}^{\infty }\frac{x^k}{4^k\left(2k+1\right)}\binom{2k}{k}=\frac{\arcsin \left(\sqrt{x}\right)}{\sqrt{x}}-1$$ $$-\sum _{k=1}^{\infty }\frac{1}{4^k\left(2k+1\right)}\binom{2k}{k}\int _0^1x^{k-1}\ln \left(1-x\right)\:dx=-\int _0^1\frac{\arcsin \left(\sqrt{x}\right)\ln \left(1-x\right)}{x\sqrt{x}}\:dx$$ $$+\int _0^1\frac{\ln \left(1-x\right)}{x}\:dx$$ $$\sum _{k=1}^{\infty }\frac{H_k}{k\:4^k\left(2k+1\right)}\binom{2k}{k}=-2\int _0^1\frac{\arcsin \left(x\right)\ln \left(1-x^2\right)}{x^2}\:dx-\zeta \left(2\right)$$ But I got stuck with: $$\int _0^1\frac{\arcsin \left(x\right)\ln \left(1-x^2\right)}{x^2}\:dx$$ Anything I try yields more complicated stuff, is there a way to calculate the main sum or the second one (or the integral) elegantly\in a simple manner?","My attempt. The first sum can be evaluated easily if one uses the central binomial coefficient generating function , the closed form is . For the remaining sum consider the series expansion. But I got stuck with: Anything I try yields more complicated stuff, is there a way to calculate the main sum or the second one (or the integral) elegantly\in a simple manner?",\sum _{k=1}^{\infty }\frac{H_k}{4^k\left(2k+1\right)}\binom{2k}{k} =\frac{1}{2}\sum _{k=1}^{\infty }\frac{H_k}{k\:4^k}\binom{2k}{k}-\frac{1}{2}\sum _{k=1}^{\infty }\frac{H_k}{k\:4^k\left(2k+1\right)}\binom{2k}{k} 2\zeta \left(2\right) \arcsin \sum _{k=0}^{\infty }\frac{x^{2k+1}}{4^k\left(2k+1\right)}\binom{2k}{k}=\arcsin \left(x\right) \sum _{k=1}^{\infty }\frac{x^k}{4^k\left(2k+1\right)}\binom{2k}{k}=\frac{\arcsin \left(\sqrt{x}\right)}{\sqrt{x}}-1 -\sum _{k=1}^{\infty }\frac{1}{4^k\left(2k+1\right)}\binom{2k}{k}\int _0^1x^{k-1}\ln \left(1-x\right)\:dx=-\int _0^1\frac{\arcsin \left(\sqrt{x}\right)\ln \left(1-x\right)}{x\sqrt{x}}\:dx +\int _0^1\frac{\ln \left(1-x\right)}{x}\:dx \sum _{k=1}^{\infty }\frac{H_k}{k\:4^k\left(2k+1\right)}\binom{2k}{k}=-2\int _0^1\frac{\arcsin \left(x\right)\ln \left(1-x^2\right)}{x^2}\:dx-\zeta \left(2\right) \int _0^1\frac{\arcsin \left(x\right)\ln \left(1-x^2\right)}{x^2}\:dx,"['real-analysis', 'integration', 'sequences-and-series', 'definite-integrals', 'harmonic-numbers']"
66,"If $f:\mathbb R^2 \to \mathbb R$ continuous on straight lines and $f(\text{compact})= \text{compact}$, then $f$ continuous?","If  continuous on straight lines and , then  continuous?",f:\mathbb R^2 \to \mathbb R f(\text{compact})= \text{compact} f,"Let $f:\mathbb{R}^2$ $\rightarrow$ $\mathbb{R}$ be a map such that $f$ is continuous over all segments (namely, for all $a,b$ $\in$ $\mathbb{R}^2$ , $t$ $\mapsto$ $f(at+b)$ is continuous), and If $K$ $\subset$ $\mathbb{R}^2$ is compact, then its image $f(K)$ is compact. Then $f$ is continuous. I tried to prove this, but I could not do it. Can you give me a hint?","Let be a map such that is continuous over all segments (namely, for all , is continuous), and If is compact, then its image is compact. Then is continuous. I tried to prove this, but I could not do it. Can you give me a hint?","f:\mathbb{R}^2 \rightarrow \mathbb{R} f a,b \in \mathbb{R}^2 t \mapsto f(at+b) K \subset \mathbb{R}^2 f(K) f","['real-analysis', 'general-topology']"
67,Value bound for integrable periodic function,Value bound for integrable periodic function,,"Let $f:\mathbb R\to\mathbb R_{\geqslant0}$ be an integrable function with period $1$ such that $\displaystyle\int_0^1 f(x)\,\mathrm dx = 1$ and define $$A:=\left\{y\in[0,1]:\int_y^{y+0.6}f(t)\,\mathrm dt\geqslant0.6\right\}.$$ What is the smallest possible Lebesgue measure of $A$ ? If $f(x)=2$ for $0\leqslant x\leqslant 0.5$ and $f(x)=0$ for $0.5\leqslant x\leqslant 1$ , then $A=[0,0.2]\cup[0.7,1]$ , which has size $0.5$ . So the answer is at most $0.5$ . If we change $0.6$ to $0.5$ in the problem (in both places), then a quick argument shows that $0.5$ is the right answer. I suspect $0.5$ is also the right answer here, but a different proof method is needed.","Let be an integrable function with period such that and define What is the smallest possible Lebesgue measure of ? If for and for , then , which has size . So the answer is at most . If we change to in the problem (in both places), then a quick argument shows that is the right answer. I suspect is also the right answer here, but a different proof method is needed.","f:\mathbb R\to\mathbb R_{\geqslant0} 1 \displaystyle\int_0^1 f(x)\,\mathrm dx = 1 A:=\left\{y\in[0,1]:\int_y^{y+0.6}f(t)\,\mathrm dt\geqslant0.6\right\}. A f(x)=2 0\leqslant x\leqslant 0.5 f(x)=0 0.5\leqslant x\leqslant 1 A=[0,0.2]\cup[0.7,1] 0.5 0.5 0.6 0.5 0.5 0.5","['real-analysis', 'inequality', 'optimization']"
68,Is there a closed form for $\int_a^b\frac{{\rm arccosh}x}{\sqrt{(x-a)(b-x)}}$?,Is there a closed form for ?,\int_a^b\frac{{\rm arccosh}x}{\sqrt{(x-a)(b-x)}},"I have solved $$\int^3_1\dfrac{{\rm arccosh}x}{\sqrt{(x-1)(3-x)}}{\rm d}x=4G$$ where $G$ is Catalan constant, by rewriting it into double integral $$\iint_{[0,\pi]\times[0,\pi]}\ln(2-\cos x-\cos y){\rm d}x{\rm d}y$$ and   computing it using Leibniz integral rule. Now my question here is: Is there a closed form for integral $$\int_a^b\dfrac{{\rm arccosh}x}{\sqrt{(x-a)(b-x)}}{\rm d}x$$ as $b>a>0$ ?   Or it is just a coincidence?","I have solved where is Catalan constant, by rewriting it into double integral and   computing it using Leibniz integral rule. Now my question here is: Is there a closed form for integral as ?   Or it is just a coincidence?","\int^3_1\dfrac{{\rm arccosh}x}{\sqrt{(x-1)(3-x)}}{\rm d}x=4G G \iint_{[0,\pi]\times[0,\pi]}\ln(2-\cos x-\cos y){\rm d}x{\rm d}y \int_a^b\dfrac{{\rm arccosh}x}{\sqrt{(x-a)(b-x)}}{\rm d}x b>a>0","['real-analysis', 'integration', 'sequences-and-series', 'definite-integrals', 'closed-form']"
69,Darboux integrability implies Riemann integrability,Darboux integrability implies Riemann integrability,,"I have searched the site for posts regarding Darboux integrability $\implies$ Riemann integrability, but haven't found any that specifically adress this question. My definition of Darboux integrability: Let $f$ be defined and bounded on $[a,b]$ , then $f$ is Darboux integrable if for all $\epsilon >0$ there exists a partition $P$ of $[a,b]$ such that $U(f,P)-L(f,P)<\epsilon$ (where $U$ and $L$ are the upper and lower Riemann sums respectively). My definition of Riemann integrability: Let $f$ be defined and bounded on $[a,b]$ , then $f$ is Riemann integrable if $$\lim_{N\to\infty} \sum\limits_{k=1}^{N} f(c_k)(x_{k}-x_{k-1})$$ has the same limit for all sequences of partitions $P_N$ and all choices of $c_k\in[x_{k-1},x_{k}]$ . If my definitions are correct, it seems that Darboux integrability only requires one partition to fulfil the epsilon-inequality, whereas Riemann integrability requires all sequences of partitions to be fulfilled. How can this lead to an implication nevertheless?","I have searched the site for posts regarding Darboux integrability Riemann integrability, but haven't found any that specifically adress this question. My definition of Darboux integrability: Let be defined and bounded on , then is Darboux integrable if for all there exists a partition of such that (where and are the upper and lower Riemann sums respectively). My definition of Riemann integrability: Let be defined and bounded on , then is Riemann integrable if has the same limit for all sequences of partitions and all choices of . If my definitions are correct, it seems that Darboux integrability only requires one partition to fulfil the epsilon-inequality, whereas Riemann integrability requires all sequences of partitions to be fulfilled. How can this lead to an implication nevertheless?","\implies f [a,b] f \epsilon >0 P [a,b] U(f,P)-L(f,P)<\epsilon U L f [a,b] f \lim_{N\to\infty} \sum\limits_{k=1}^{N} f(c_k)(x_{k}-x_{k-1}) P_N c_k\in[x_{k-1},x_{k}]","['real-analysis', 'calculus', 'riemann-integration']"
70,The limit points of an open interval (open set),The limit points of an open interval (open set),,"I am not sure I understand why for the open set $(a,b)$, the limit points are $[a,b]$. Why are $a,b$ now included as limit points? Is this because we can somehow find a sequence in the open interval converging to these points?","I am not sure I understand why for the open set $(a,b)$, the limit points are $[a,b]$. Why are $a,b$ now included as limit points? Is this because we can somehow find a sequence in the open interval converging to these points?",,"['real-analysis', 'general-topology']"
71,How to study the convergence of $ \int_{0}^{\infty}\frac{dx}{1+x^{2}|\sin(x)|} $?,How to study the convergence of ?, \int_{0}^{\infty}\frac{dx}{1+x^{2}|\sin(x)|} ,"I am rather stuck in studying the convergence of this integral: $$ \int_{0}^{\infty} {\mathrm{d}x \over 1 + x^{2}\left\vert\,\sin\left(\,x\,\right)\,\right\vert} $$ I can't really find an equivalent, and I don't really see what I can compare it too. Any help would be appreciated.","I am rather stuck in studying the convergence of this integral: $$ \int_{0}^{\infty} {\mathrm{d}x \over 1 + x^{2}\left\vert\,\sin\left(\,x\,\right)\,\right\vert} $$ I can't really find an equivalent, and I don't really see what I can compare it too. Any help would be appreciated.",,"['real-analysis', 'integration', 'sequences-and-series', 'improper-integrals']"
72,What are some useful problem solving strategies for real analysis?,What are some useful problem solving strategies for real analysis?,,"In this blog , Professor Tao exhibited some problem solving strategies that can help students in their study of (mostly) measure theory and some are intended for analysis in general. I'd love to see some other  good ""tricks"" that  students of real analysis would like to learn and master since they'd make their life easier in proving theorems and doing exercises. A more preferable answer would be one that includes a ""trick"" or a ""strategy"" with an example where this trick is already useful (for example, in proving such-and-such theorm)","In this blog , Professor Tao exhibited some problem solving strategies that can help students in their study of (mostly) measure theory and some are intended for analysis in general. I'd love to see some other  good ""tricks"" that  students of real analysis would like to learn and master since they'd make their life easier in proving theorems and doing exercises. A more preferable answer would be one that includes a ""trick"" or a ""strategy"" with an example where this trick is already useful (for example, in proving such-and-such theorm)",,"['real-analysis', 'problem-solving']"
73,Evaluating the limit of a certain definite integral,Evaluating the limit of a certain definite integral,,"Let $\displaystyle f(x)= \lim_{\epsilon \to 0} \frac{1}{\sqrt{\epsilon}}\int_0^x ze^{-(\epsilon)^{-1}\tan^2z}dz$ for $x\in[0,\infty)$ . Evaluate $f(x)$ in closed form for all $x\in[0,\infty)$ and sketch a graph of this function. Hints, as well as solutions are welcome for this question :-) Edit:  So far, I have, from substituting $\sqrt{\epsilon}u$ = z, $\displaystyle f(x)= \lim_{\epsilon \to 0} \int_0^{\sqrt{\epsilon}u}  \sqrt{\epsilon}ue^{-(\epsilon)^{-1}\tan^2\sqrt{\epsilon}u}du$ But we can split the integral into two terms, with the first integral equal to zero, by dominated convergence theorem.  I think we only need to look at: $\displaystyle f(x)= \lim_{\epsilon \to 0} \int_0^{a}  \sqrt{\epsilon}ue^{-(\epsilon)^{-1}\tan^2\sqrt{\epsilon}u}du$ + $\displaystyle \lim_{\epsilon \to 0} \int_a^{\sqrt{\epsilon}u}  \sqrt{\epsilon}ue^{-(\epsilon)^{-1}\tan^2\sqrt{\epsilon}u}du$ = $$0+\displaystyle \lim_{\epsilon \to 0} \int_a^{\sqrt{\epsilon}u}  \sqrt{\epsilon}ue^{-(\epsilon)^{-1}\tan^2\sqrt{\epsilon}u}du$$ (I'm not sure if integrating away from the origin helps much, to be honest.)","Let for . Evaluate in closed form for all and sketch a graph of this function. Hints, as well as solutions are welcome for this question :-) Edit:  So far, I have, from substituting = z, But we can split the integral into two terms, with the first integral equal to zero, by dominated convergence theorem.  I think we only need to look at: + = (I'm not sure if integrating away from the origin helps much, to be honest.)","\displaystyle f(x)= \lim_{\epsilon \to 0} \frac{1}{\sqrt{\epsilon}}\int_0^x ze^{-(\epsilon)^{-1}\tan^2z}dz x\in[0,\infty) f(x) x\in[0,\infty) \sqrt{\epsilon}u \displaystyle f(x)= \lim_{\epsilon \to 0} \int_0^{\sqrt{\epsilon}u}  \sqrt{\epsilon}ue^{-(\epsilon)^{-1}\tan^2\sqrt{\epsilon}u}du \displaystyle f(x)= \lim_{\epsilon \to 0} \int_0^{a}  \sqrt{\epsilon}ue^{-(\epsilon)^{-1}\tan^2\sqrt{\epsilon}u}du \displaystyle \lim_{\epsilon \to 0} \int_a^{\sqrt{\epsilon}u}  \sqrt{\epsilon}ue^{-(\epsilon)^{-1}\tan^2\sqrt{\epsilon}u}du 0+\displaystyle \lim_{\epsilon \to 0} \int_a^{\sqrt{\epsilon}u}  \sqrt{\epsilon}ue^{-(\epsilon)^{-1}\tan^2\sqrt{\epsilon}u}du","['calculus', 'real-analysis', 'integration', 'limits', 'closed-form']"
74,Using second derivative to find a bound for the first derivative,Using second derivative to find a bound for the first derivative,,"Let $f$ be a twice differentiable function on $\left[0,1\right]$ satisfying $f\left(0\right)=f\left(1\right)=0$. Additionally $\left|f''\left(x\right)\right|\leq1$ in $\left(0,1\right)$. Prove that $$\left|f'\left(x\right)\right|\le\frac{1}{2},\quad\forall x\in\left[0,1\right]$$ The hint we were given is to expand into a first order Taylor polynomial at the minimum of $f'$. So I tried doing that: As $f'$ is differentiable, it is continuous, and attains a minimum at $\left[0,1\right]$. Thus we can denote $x_{0}$ as the minimum, and expending into a Taylor polynomial of the first order around it gives us, for some $c$ between $x$ and $x_0$. $$T_{x_{0}}\left(x\right)=f\left(x_{0}\right)+f'\left(x_{0}\right)\left(x-x_{0}\right)+\frac{f''\left(c\right)}{2}\left(x-x_{0}\right)^{2}$$ Now at $x=0$ we have $$T_{x_{0}}\left(0\right)=f\left(x_{0}\right)-x_{0}f'\left(x_{0}\right)+x_{0}^{2}\frac{f''\left(c\right)}{2}=0$$ And at $x=1$ we have $$T_{x_{0}}\left(0\right)=f\left(x_{0}\right)+\left(1-x_{0}\right)f'\left(x_{0}\right)+\left(1-x_{0}\right)^{2}\frac{f''\left(c\right)}{2}=0$$ and I'm pretty much stuck here.. So I tried a different approach using Mean Value Theorem directly, by Rolle's I know the derivative is $0$ somewhere (as $f(0)=f(1)$), suppose at $x_0$, so by mean value theorem $\frac{f'(x)}{x-x_0} =f''(c)\leq1$ for some $c$ and $f'(x)<x-x_0$. But using this approach as well I'm not sure how to proceed, as this gives me the desired propety only in $1/2$ environment around $x_0$... Any help?","Let $f$ be a twice differentiable function on $\left[0,1\right]$ satisfying $f\left(0\right)=f\left(1\right)=0$. Additionally $\left|f''\left(x\right)\right|\leq1$ in $\left(0,1\right)$. Prove that $$\left|f'\left(x\right)\right|\le\frac{1}{2},\quad\forall x\in\left[0,1\right]$$ The hint we were given is to expand into a first order Taylor polynomial at the minimum of $f'$. So I tried doing that: As $f'$ is differentiable, it is continuous, and attains a minimum at $\left[0,1\right]$. Thus we can denote $x_{0}$ as the minimum, and expending into a Taylor polynomial of the first order around it gives us, for some $c$ between $x$ and $x_0$. $$T_{x_{0}}\left(x\right)=f\left(x_{0}\right)+f'\left(x_{0}\right)\left(x-x_{0}\right)+\frac{f''\left(c\right)}{2}\left(x-x_{0}\right)^{2}$$ Now at $x=0$ we have $$T_{x_{0}}\left(0\right)=f\left(x_{0}\right)-x_{0}f'\left(x_{0}\right)+x_{0}^{2}\frac{f''\left(c\right)}{2}=0$$ And at $x=1$ we have $$T_{x_{0}}\left(0\right)=f\left(x_{0}\right)+\left(1-x_{0}\right)f'\left(x_{0}\right)+\left(1-x_{0}\right)^{2}\frac{f''\left(c\right)}{2}=0$$ and I'm pretty much stuck here.. So I tried a different approach using Mean Value Theorem directly, by Rolle's I know the derivative is $0$ somewhere (as $f(0)=f(1)$), suppose at $x_0$, so by mean value theorem $\frac{f'(x)}{x-x_0} =f''(c)\leq1$ for some $c$ and $f'(x)<x-x_0$. But using this approach as well I'm not sure how to proceed, as this gives me the desired propety only in $1/2$ environment around $x_0$... Any help?",,"['real-analysis', 'derivatives']"
75,Proving $\lim_{x\to 1} x^3=1$ with $\epsilon$-$\delta$ definition,Proving  with - definition,\lim_{x\to 1} x^3=1 \epsilon \delta,"Problem: I need to formally prove that $$\lim_{x\to 1} x^3 = 1.$$ My work: This is what I have so far and I'm generally a bit stuck with these proofs from here onwards. Because $$-\epsilon < x^3-1 < \epsilon  =  | x^3-1 | < \epsilon,$$ then $$ -\epsilon < x^3-1 < \epsilon$$ $$-\epsilon+1 < x^3 < \epsilon +1$$ $$ \sqrt[3]{-\epsilon+1}< x < \sqrt[3]{\epsilon+1}.$$ Hoping that what I have done so far is correct. Am I right in thinking that $$ \sqrt[3]{-\epsilon+1}< x < \sqrt[3]{\epsilon+1}$$ is giving me an interval where $x$ is going to give me a $f(x)$ value that falls within the distance $\epsilon$ from the limit on the $y$ -axis ? Or is this interval smaller than the $\epsilon$ -distance on the $y$ -axis?",Problem: I need to formally prove that My work: This is what I have so far and I'm generally a bit stuck with these proofs from here onwards. Because then Hoping that what I have done so far is correct. Am I right in thinking that is giving me an interval where is going to give me a value that falls within the distance from the limit on the -axis ? Or is this interval smaller than the -distance on the -axis?,"\lim_{x\to 1} x^3 = 1. -\epsilon < x^3-1 < \epsilon  =  | x^3-1 | < \epsilon,  -\epsilon < x^3-1 < \epsilon -\epsilon+1 < x^3 < \epsilon +1  \sqrt[3]{-\epsilon+1}< x < \sqrt[3]{\epsilon+1}.  \sqrt[3]{-\epsilon+1}< x < \sqrt[3]{\epsilon+1} x f(x) \epsilon y \epsilon y","['calculus', 'real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
76,When does convergence of function imply convergence of its derivative?,When does convergence of function imply convergence of its derivative?,,"Let $F_n$ be a sequence of differentiable real valued functions. Suppose that $$\lim_{n \to \infty} F_n(x) = F(x)$$ and that $F(x)$ is differentiable. Under which conditions does that imply $$\lim_{n \to \infty} F'_n(x) = F'(x)$$ ? Do I need some regularity, or maybe that the $F_n$ converges uniformly?","Let be a sequence of differentiable real valued functions. Suppose that and that is differentiable. Under which conditions does that imply ? Do I need some regularity, or maybe that the converges uniformly?",F_n \lim_{n \to \infty} F_n(x) = F(x) F(x) \lim_{n \to \infty} F'_n(x) = F'(x) F_n,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'uniform-convergence']"
77,Is sum and product of a infinite number of continuous functions are also continuous functions?,Is sum and product of a infinite number of continuous functions are also continuous functions?,,"Whether in Real Analysis or by Open Set Def of Continuity in Topology, it is easy to show that the sum and product of a FINITE number of continuous functions are also continuous functions. That is, assuming that $f_1, ..., f_m:\Bbb R\rightarrow\Bbb R$ are continuous, then $S:\Bbb R\rightarrow\Bbb R$ and $P:\Bbb R\rightarrow\Bbb R$, defined by $S(x) = f_1(x) + ... + f_m(x)$ and $P(x) = f_1(x) \times  ... \times f_m(x)$, are continuous. But many analytic functions that are continuous can be written in their expanded form (by Taylor Series), which are the sum and product of INFINITE functions.  My question is, EVEN if there is another way to show that some/all analytic functions are continuous (which I don't know that way), still we should prove from ""the sum and product of infinite functions"" way. Would you please help me regarding the question? I think one of Topology or Analysis way of proof should be enough, because, as we can prove, the topological definition of continuity is equivalent to the $\epsilon - \delta$ definition for functions that map $\Bbb R$ to $\Bbb R$. EDIT: Let me rephrase it: limit of sum of two functions exists if limit of each of the two functions exists. If sum of in finite number of functions is a function that has limit in some point, is it mean that we are allowed to say that for this type of function, sum of limit infinite number of functions exists since limit of sum of those infinite number of functions exists?","Whether in Real Analysis or by Open Set Def of Continuity in Topology, it is easy to show that the sum and product of a FINITE number of continuous functions are also continuous functions. That is, assuming that $f_1, ..., f_m:\Bbb R\rightarrow\Bbb R$ are continuous, then $S:\Bbb R\rightarrow\Bbb R$ and $P:\Bbb R\rightarrow\Bbb R$, defined by $S(x) = f_1(x) + ... + f_m(x)$ and $P(x) = f_1(x) \times  ... \times f_m(x)$, are continuous. But many analytic functions that are continuous can be written in their expanded form (by Taylor Series), which are the sum and product of INFINITE functions.  My question is, EVEN if there is another way to show that some/all analytic functions are continuous (which I don't know that way), still we should prove from ""the sum and product of infinite functions"" way. Would you please help me regarding the question? I think one of Topology or Analysis way of proof should be enough, because, as we can prove, the topological definition of continuity is equivalent to the $\epsilon - \delta$ definition for functions that map $\Bbb R$ to $\Bbb R$. EDIT: Let me rephrase it: limit of sum of two functions exists if limit of each of the two functions exists. If sum of in finite number of functions is a function that has limit in some point, is it mean that we are allowed to say that for this type of function, sum of limit infinite number of functions exists since limit of sum of those infinite number of functions exists?",,['real-analysis']
78,Product of Absolutely Continuous Measures is Absolutely Continuous,Product of Absolutely Continuous Measures is Absolutely Continuous,,"I am stuck on this problem from Folland's Real Analysis, Second Edition: For $j = 1, 2$, let $\mu_j, \nu_j$ be $\sigma$-finite measures on $(X_j, \mathcal{M}_j)$ such that $\nu_j <\!\!< \mu_j$.  Then $\nu_1 \times \nu_2 <\!\!< \mu_1 \times \mu_2$. Here is about where I am at. (1)  It is immediate that if $\mu_1 \times \mu_2(A \times B) = 0$, then $\nu_1 \times \nu_2(A \times B) = 0$. (2)  Because $\nu_1 \times \nu_2$ is $\sigma$-finite, it is enough to prove the result for when $\nu_1 \times \nu_2$ is finite.  Since the result is quickly verified if either $\nu_1$ or $\nu_2$ is the zero measure, we may assume that both $\nu_1$ and $\nu_2$ are finite.  We then have the following equivalent formulation for $\nu_1 <\!\!< \mu_1$: For every $\epsilon > 0$, there exists $\delta > 0$ such that $\mu_1(E) < \delta$ implies $\nu_1(E) < \epsilon$. And similarly for $\nu_2 <\!\!< \mu_2$. I thought I might be able to prove the same condition for $\nu_1 \times \nu_2$ with respect to $\mu_1 \times \mu_2$, which would then imply $\nu_1 \times \nu_2 <\!\!< \mu_1 \times \mu_2$ since $\nu_1 \times \nu_2$ has been reduced to being finite.  As a suggestion, following Folland's technique, it might be easier to argue by contradiction, assuming first the $\epsilon-\delta$ condition is false. (3)  If $\mu_1 \times \mu_2(E) = 0$, then, by definition, $$0 = \inf \bigg\{ \sum_n \mu_1(A_n)\mu_2(B_n) \colon A_n \times B_n \text{ are rectangles such that } E \subset \bigcup_n A_n \times B_n\bigg\}.$$ To show $\nu_1 \times \nu_2(E) = 0$, we want to show that the analogous equation holds for $\nu_1 \times \nu_2$. I can't seem to put the pieces together, despite some effort. Any help would be greatly appreciated.  Thanks.","I am stuck on this problem from Folland's Real Analysis, Second Edition: For $j = 1, 2$, let $\mu_j, \nu_j$ be $\sigma$-finite measures on $(X_j, \mathcal{M}_j)$ such that $\nu_j <\!\!< \mu_j$.  Then $\nu_1 \times \nu_2 <\!\!< \mu_1 \times \mu_2$. Here is about where I am at. (1)  It is immediate that if $\mu_1 \times \mu_2(A \times B) = 0$, then $\nu_1 \times \nu_2(A \times B) = 0$. (2)  Because $\nu_1 \times \nu_2$ is $\sigma$-finite, it is enough to prove the result for when $\nu_1 \times \nu_2$ is finite.  Since the result is quickly verified if either $\nu_1$ or $\nu_2$ is the zero measure, we may assume that both $\nu_1$ and $\nu_2$ are finite.  We then have the following equivalent formulation for $\nu_1 <\!\!< \mu_1$: For every $\epsilon > 0$, there exists $\delta > 0$ such that $\mu_1(E) < \delta$ implies $\nu_1(E) < \epsilon$. And similarly for $\nu_2 <\!\!< \mu_2$. I thought I might be able to prove the same condition for $\nu_1 \times \nu_2$ with respect to $\mu_1 \times \mu_2$, which would then imply $\nu_1 \times \nu_2 <\!\!< \mu_1 \times \mu_2$ since $\nu_1 \times \nu_2$ has been reduced to being finite.  As a suggestion, following Folland's technique, it might be easier to argue by contradiction, assuming first the $\epsilon-\delta$ condition is false. (3)  If $\mu_1 \times \mu_2(E) = 0$, then, by definition, $$0 = \inf \bigg\{ \sum_n \mu_1(A_n)\mu_2(B_n) \colon A_n \times B_n \text{ are rectangles such that } E \subset \bigcup_n A_n \times B_n\bigg\}.$$ To show $\nu_1 \times \nu_2(E) = 0$, we want to show that the analogous equation holds for $\nu_1 \times \nu_2$. I can't seem to put the pieces together, despite some effort. Any help would be greatly appreciated.  Thanks.",,"['real-analysis', 'measure-theory']"
79,About composition of Holder functions.,About composition of Holder functions.,,"Let $f,g$ be Holder continuous functions with respective exponents $\alpha, \beta \in (0,1)$. More precisely $f \in C^{\alpha}(\mathbb{R}^n;\mathbb{R}^n)$, $g\in C^{\beta}(\mathbb{R}^n,\mathbb{R})$. I am wondering whether the composition $g \circ f:\mathbb{R}^n \to \mathbb{R}$ is Holder of exponent $\min \{\alpha,\beta \}$, i.e, if $g \circ f \in C^{\min\{\alpha,\beta \}}(\mathbb{R}^n;\mathbb{R})$. This is claimed on a paper I am reading but I don't know how to prove it. The naive approach would be: $$ |g(f(x+h))-g(f(x))| \le [g]_{\beta}|f(x+h)-f(x)|^{\beta} \le [g]_{\beta} |h|^{\beta \alpha} [f]^{\beta}_{\alpha} $$ so we have $g \circ f \in C^{\alpha \beta }(\mathbb{R}^n)$. But $\alpha \beta < \min \{\alpha,\beta\}$, so this is not very helpful. Any help would be welcome. Thak you.","Let $f,g$ be Holder continuous functions with respective exponents $\alpha, \beta \in (0,1)$. More precisely $f \in C^{\alpha}(\mathbb{R}^n;\mathbb{R}^n)$, $g\in C^{\beta}(\mathbb{R}^n,\mathbb{R})$. I am wondering whether the composition $g \circ f:\mathbb{R}^n \to \mathbb{R}$ is Holder of exponent $\min \{\alpha,\beta \}$, i.e, if $g \circ f \in C^{\min\{\alpha,\beta \}}(\mathbb{R}^n;\mathbb{R})$. This is claimed on a paper I am reading but I don't know how to prove it. The naive approach would be: $$ |g(f(x+h))-g(f(x))| \le [g]_{\beta}|f(x+h)-f(x)|^{\beta} \le [g]_{\beta} |h|^{\beta \alpha} [f]^{\beta}_{\alpha} $$ so we have $g \circ f \in C^{\alpha \beta }(\mathbb{R}^n)$. But $\alpha \beta < \min \{\alpha,\beta\}$, so this is not very helpful. Any help would be welcome. Thak you.",,"['real-analysis', 'partial-differential-equations', 'fourier-analysis', 'banach-spaces', 'holder-spaces']"
80,"Proof that a sequence of continuous functions $(f_n)$ cannot converge pointwise to $1_\mathbb{Q}$ on $[0,1]$",Proof that a sequence of continuous functions  cannot converge pointwise to  on,"(f_n) 1_\mathbb{Q} [0,1]","As a homework question, we got asked the following: Construct a function $f:[0,1] \rightarrow \mathbb{R}$ which is not the pointwise limit of any sequence of continuous functions Thinking about it, a function which is nowhere continuous should be sufficient; thus I decided to try to use the indicator function on the rationals, $1_\mathbb{Q}$. Basically, I was just wondering if my proof was alright or not: Suppose $(f_n)$ is a sequence of continuous functions converging pointwise to $1_\mathbb{Q}$; thus for any $\epsilon > 0$ we have that for all $x$ we can find a $N$ such that if $n \geq N$ then $| f_n(x) - 1_\mathbb{Q} (x) | < \epsilon$. In particular, we have for some $x_1 \in \mathbb{Q}, x_2 \notin \mathbb{Q}$ a $N_1$ and $ N_2$ respectively such that if $ n \geq N_1$ then $ | f_n(x_1) - 1 | < \epsilon$ if $ n \geq N_2$ then $ | f_n(x_2) | < \epsilon$ Now, as $f_k$ is continuous, for any $\epsilon > 0$ we have that for all $x$ there exists a $\delta > 0$ such that if $ | x- y | < \delta$ then $|f_k(x) - f_k(y) | < \epsilon $. However, if we pick an arbitrary $x \in \mathbb{Q}$, for $n \geq \max\{N_1, N_2\}$, for any $\delta > 0$ we can pick a $ y \notin \mathbb{Q}$ such that $| x- y| < \delta$ yet $ | f_n(x) - f_n(y) - 1|  < 2\epsilon$ (by the triangle inequality and using the pointwise convergence properties); so if we choose $\epsilon = 1/10$, say, then we have by continuity that $|f_n(x) - f_n(y)| < 1/10$, yet $| f_n(x) - f_n(y) - 1|  < 1/5$, which (I believe) is absurd, so we have a contradiction. I'm a bit iffy about whether this is alright or not (I feel I may be subtly abusing the definitions here), so any feedback would be much appreciated.","As a homework question, we got asked the following: Construct a function $f:[0,1] \rightarrow \mathbb{R}$ which is not the pointwise limit of any sequence of continuous functions Thinking about it, a function which is nowhere continuous should be sufficient; thus I decided to try to use the indicator function on the rationals, $1_\mathbb{Q}$. Basically, I was just wondering if my proof was alright or not: Suppose $(f_n)$ is a sequence of continuous functions converging pointwise to $1_\mathbb{Q}$; thus for any $\epsilon > 0$ we have that for all $x$ we can find a $N$ such that if $n \geq N$ then $| f_n(x) - 1_\mathbb{Q} (x) | < \epsilon$. In particular, we have for some $x_1 \in \mathbb{Q}, x_2 \notin \mathbb{Q}$ a $N_1$ and $ N_2$ respectively such that if $ n \geq N_1$ then $ | f_n(x_1) - 1 | < \epsilon$ if $ n \geq N_2$ then $ | f_n(x_2) | < \epsilon$ Now, as $f_k$ is continuous, for any $\epsilon > 0$ we have that for all $x$ there exists a $\delta > 0$ such that if $ | x- y | < \delta$ then $|f_k(x) - f_k(y) | < \epsilon $. However, if we pick an arbitrary $x \in \mathbb{Q}$, for $n \geq \max\{N_1, N_2\}$, for any $\delta > 0$ we can pick a $ y \notin \mathbb{Q}$ such that $| x- y| < \delta$ yet $ | f_n(x) - f_n(y) - 1|  < 2\epsilon$ (by the triangle inequality and using the pointwise convergence properties); so if we choose $\epsilon = 1/10$, say, then we have by continuity that $|f_n(x) - f_n(y)| < 1/10$, yet $| f_n(x) - f_n(y) - 1|  < 1/5$, which (I believe) is absurd, so we have a contradiction. I'm a bit iffy about whether this is alright or not (I feel I may be subtly abusing the definitions here), so any feedback would be much appreciated.",,"['real-analysis', 'proof-verification']"
81,"If the sum and the product of two sequences converges to zero, does that mean that each sequence converges to zero?","If the sum and the product of two sequences converges to zero, does that mean that each sequence converges to zero?",,"If the sum and the product of two sequences converges to zero,  does that mean that each sequence converges to zero ? Thanks","If the sum and the product of two sequences converges to zero,  does that mean that each sequence converges to zero ? Thanks",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
82,Proving a necessary and sufficient condition for compactness of a subset of $\ell^p$,Proving a necessary and sufficient condition for compactness of a subset of,\ell^p,"Let $A \subset \ell^p$, where $1 \le p \lt \infty$.  Suppose the following conditions are true: 1) $A$ is closed and bounded 2) $\forall \epsilon \gt 0, \: \exists \: N \in \mathbb{N}$ such that $\forall x \in A$, we have $\sum_{n \ge N}|x_{n}|^{p} \lt \epsilon$. Then show that $A$ is compact. Also, show that the converse is true.","Let $A \subset \ell^p$, where $1 \le p \lt \infty$.  Suppose the following conditions are true: 1) $A$ is closed and bounded 2) $\forall \epsilon \gt 0, \: \exists \: N \in \mathbb{N}$ such that $\forall x \in A$, we have $\sum_{n \ge N}|x_{n}|^{p} \lt \epsilon$. Then show that $A$ is compact. Also, show that the converse is true.",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'compactness']"
83,Inclusion of $L^p$ spaces,Inclusion of  spaces,L^p,Let $X \subset L^1(\mathbb{R})$ a closed linear subspace satisfying \begin{align} X\subset \bigcup_{p>1} L^p(\mathbb{R})\end{align}    Show that $X\subset L^{p_0}(\mathbb{R})$ for some $p_0>1.$ I guess the problem is that in infinite measure spaces the inclusion $L^p\subset L^q$ only holds for $p=q$. Is it maybe possbile to apply Baire's Theorem in some way?,Let $X \subset L^1(\mathbb{R})$ a closed linear subspace satisfying \begin{align} X\subset \bigcup_{p>1} L^p(\mathbb{R})\end{align}    Show that $X\subset L^{p_0}(\mathbb{R})$ for some $p_0>1.$ I guess the problem is that in infinite measure spaces the inclusion $L^p\subset L^q$ only holds for $p=q$. Is it maybe possbile to apply Baire's Theorem in some way?,,"['real-analysis', 'functional-analysis', 'measure-theory', 'banach-spaces']"
84,Lipschitz Continuity of Linear Map Between Finite Dimensional Vector Spaces,Lipschitz Continuity of Linear Map Between Finite Dimensional Vector Spaces,,"I am trying to show from first principles that a linear map $T:X \rightarrow Y$ between finite dimensional vector spaces $X$ and $Y$ is Lipschitz. I'm rather stuck and would appreciate a hint how I might continue the argument I have constructed so far: To show that $T$ is Lipschitz, it is necessary to show for $x,y \in U$ that $|T(x) - T(y)| \leq \lambda|x - y|$ for some $\lambda > 0$. If $u_1, \dots, u_n$ is a basis for $X$, there exists scalars $a^i, b^i$ such that $x = a^iu_i$ and $y=b^iu_i$. Therefore, $$ |T(x - y)| \leq |T(a^iu_i)| + |T(b^iu_i)| = |a^i|\cdot|T(u_i)| + |b^i|\cdot|T(u_i)| \leq k \sum_{i=1}^n|T(u_i)| $$ where $$k = \max_{j=1, \dots n}|a^j + b^j|$$ So, I've succeeded in finding a bounds of sorts for $T$ but I'm not sure how to translate the bound $|T(u_i)|$ to $u_i$ and other than showing something rather obvious so far, I'm not really sure that the above argument gets me a lot closer to my goal. How could I proceed with this?","I am trying to show from first principles that a linear map $T:X \rightarrow Y$ between finite dimensional vector spaces $X$ and $Y$ is Lipschitz. I'm rather stuck and would appreciate a hint how I might continue the argument I have constructed so far: To show that $T$ is Lipschitz, it is necessary to show for $x,y \in U$ that $|T(x) - T(y)| \leq \lambda|x - y|$ for some $\lambda > 0$. If $u_1, \dots, u_n$ is a basis for $X$, there exists scalars $a^i, b^i$ such that $x = a^iu_i$ and $y=b^iu_i$. Therefore, $$ |T(x - y)| \leq |T(a^iu_i)| + |T(b^iu_i)| = |a^i|\cdot|T(u_i)| + |b^i|\cdot|T(u_i)| \leq k \sum_{i=1}^n|T(u_i)| $$ where $$k = \max_{j=1, \dots n}|a^j + b^j|$$ So, I've succeeded in finding a bounds of sorts for $T$ but I'm not sure how to translate the bound $|T(u_i)|$ to $u_i$ and other than showing something rather obvious so far, I'm not really sure that the above argument gets me a lot closer to my goal. How could I proceed with this?",,"['linear-algebra', 'real-analysis']"
85,How many strict local minima a quartic polynomial in two variables might have?,How many strict local minima a quartic polynomial in two variables might have?,,"What is the maximum number $N$ of strict local minima that a degree 4 polynomial $p:\Bbb R^2\to \Bbb R$ can have? One dimension: For a single variable quartic polynomial $q:\Bbb R \to \Bbb R$ the answer to this question would be easy: The polynomial has at most two local minima. Indeed, the derivative $q'$ is a cubic polynomial, so by Fundamental Theorem of Analysis $q$ has at most 3 critical points. However, when $q$ has 3 critical points, then all of them are simple roots, and so two consecutive critical points of $q$ can not be both local minima (local minima/maxima are alternating). Along the lines: My first thought was to study $p$ along the lines $(x,y)=(a+bt,c+dt)$ as one would do to prove that a quadratic polynomial $p:\Bbb R^2\to \Bbb R$ has no more than one strict local minimum. However, this argument fails in the attempt to show that a quartic polynomial $p$ has at most two strict local minima. The problem is that unlike two points, three points in $\Bbb R^2$ can not in general be interpolated by a line. Three points in $\Bbb R^2$ can be interpolated by a quadratic curve like $(x(t),y(t))=(t,at^2+bt+c)$ , but then $t\mapsto p(x(t),y(t))$ could be a polynomial of degree $8$ . At lest 4: Since the polynomial $q(t)=(t^2-1)^2$ has minima at the two points $t=\pm 1$ , the polynomial $p(x,y) = q(x) + q(y)$ has minima at the four points points $(x,y)=(\pm 1,\pm 1)$ . Thus $N\geq 4$ . At most 9: As commented by @GerryMyerson, the partial derivatives $p_x,p_y$ of $p$ are cubic polynomials, and so by Bézout's Theorem the curves $p_x(x,y)=0$ and $p_y(x,y)=0$ intersect at no more than $3\times 3$ isolated points. Hence, $p$ has no more than 9 critical points. The question then is how many critical points $p$ needs to have in order to have $N$ strict local minima: For the case of $N=2$ and a general function $f$ this was answered in the post: If a two variable smooth function has two global minima, will it necessarily have a third critical point? I asked the related question for a degree 4 polynomial and $N\geq 2:$ Is there a quartic polynomial in two variables that have multiple local minima and no other critical points? If you could not determine the exact value of $N$ , could you at least give some lower or upper bound, like that $N>4$ or that $N$ is finite?","What is the maximum number of strict local minima that a degree 4 polynomial can have? One dimension: For a single variable quartic polynomial the answer to this question would be easy: The polynomial has at most two local minima. Indeed, the derivative is a cubic polynomial, so by Fundamental Theorem of Analysis has at most 3 critical points. However, when has 3 critical points, then all of them are simple roots, and so two consecutive critical points of can not be both local minima (local minima/maxima are alternating). Along the lines: My first thought was to study along the lines as one would do to prove that a quadratic polynomial has no more than one strict local minimum. However, this argument fails in the attempt to show that a quartic polynomial has at most two strict local minima. The problem is that unlike two points, three points in can not in general be interpolated by a line. Three points in can be interpolated by a quadratic curve like , but then could be a polynomial of degree . At lest 4: Since the polynomial has minima at the two points , the polynomial has minima at the four points points . Thus . At most 9: As commented by @GerryMyerson, the partial derivatives of are cubic polynomials, and so by Bézout's Theorem the curves and intersect at no more than isolated points. Hence, has no more than 9 critical points. The question then is how many critical points needs to have in order to have strict local minima: For the case of and a general function this was answered in the post: If a two variable smooth function has two global minima, will it necessarily have a third critical point? I asked the related question for a degree 4 polynomial and Is there a quartic polynomial in two variables that have multiple local minima and no other critical points? If you could not determine the exact value of , could you at least give some lower or upper bound, like that or that is finite?","N p:\Bbb R^2\to \Bbb R q:\Bbb R \to \Bbb R q' q q q p (x,y)=(a+bt,c+dt) p:\Bbb R^2\to \Bbb R p \Bbb R^2 \Bbb R^2 (x(t),y(t))=(t,at^2+bt+c) t\mapsto p(x(t),y(t)) 8 q(t)=(t^2-1)^2 t=\pm 1 p(x,y) = q(x) + q(y) (x,y)=(\pm 1,\pm 1) N\geq 4 p_x,p_y p p_x(x,y)=0 p_y(x,y)=0 3\times 3 p p N N=2 f N\geq 2: N N>4 N","['real-analysis', 'multivariable-calculus', 'polynomials', 'roots', 'maxima-minima']"
86,Result analogous to the Central Limit Theorem if the third moment is also finite,Result analogous to the Central Limit Theorem if the third moment is also finite,,"Motivation Let $\{X_n\}_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables that have finite first moment. Let $S_n =\sum_{i=1}^n X_i$ . We have the Law of Large Number $$ n^{-1}S_n \to \mathbb{E}[X_1] \quad \text{a.s.} $$ We can view $n^{-1}S_n$ as converging (in some sense) to $\mathbb{E}[X_1]$ , which is a (degenerate) random variable that has the same first moment as $X_n$ . In the meantime, if we assume finite second moment, we also have the Central Limit Theorem, $$ n^{-\frac{1}{2}}S_n \overset{d}{\longrightarrow} N(\mathbb{E}[X_1],\text{Var}[X_1]). $$ We can view $n^{-\frac{1}{2}}S_n$ as converging to a random variable that has the same first and second moments as $X_1$ . The takeaway is, for the above two cases, we can always find: 1) a notion of probability convergence; 2) a random variable that matches the corresponding moments of $X_1$ ; 3) a proper exponent $\alpha$ that is put in front of $S_n$ -- that make $$ n^{-\alpha}S_n \to Y $$ hold. Question For the same i.i.d. sequence, if we further assume that its third moment is finite, can we get an analogous result? The form of the result is likely to be $$ n^{-\alpha}S_n \to Y, $$ where $\alpha$ is some positive number, $Y$ is a random variable with the same first three moments as $X_1$ , and the concept of convergence is something that is well-defined. Moreover, for even higher orders, do we have a general result for this analogy?","Motivation Let be a sequence of i.i.d. random variables that have finite first moment. Let . We have the Law of Large Number We can view as converging (in some sense) to , which is a (degenerate) random variable that has the same first moment as . In the meantime, if we assume finite second moment, we also have the Central Limit Theorem, We can view as converging to a random variable that has the same first and second moments as . The takeaway is, for the above two cases, we can always find: 1) a notion of probability convergence; 2) a random variable that matches the corresponding moments of ; 3) a proper exponent that is put in front of -- that make hold. Question For the same i.i.d. sequence, if we further assume that its third moment is finite, can we get an analogous result? The form of the result is likely to be where is some positive number, is a random variable with the same first three moments as , and the concept of convergence is something that is well-defined. Moreover, for even higher orders, do we have a general result for this analogy?","\{X_n\}_{n \in \mathbb{N}} S_n =\sum_{i=1}^n X_i 
n^{-1}S_n \to \mathbb{E}[X_1] \quad \text{a.s.}
 n^{-1}S_n \mathbb{E}[X_1] X_n 
n^{-\frac{1}{2}}S_n \overset{d}{\longrightarrow} N(\mathbb{E}[X_1],\text{Var}[X_1]).
 n^{-\frac{1}{2}}S_n X_1 X_1 \alpha S_n 
n^{-\alpha}S_n \to Y
 
n^{-\alpha}S_n \to Y,
 \alpha Y X_1","['real-analysis', 'probability', 'probability-theory', 'central-limit-theorem', 'law-of-large-numbers']"
87,Prove $\sum \frac{1}{(f(n))^2}$ converges.,Prove  converges.,\sum \frac{1}{(f(n))^2},"Let $f$ be an unbounded, non-decreasing function, With the following property: For any positive sequence $a_n$ s.t. $\sum a_n$ converges, $\sum\frac{1}{f(\frac{1}{a_n})}$ also converges. Prove, Or disprove: $\displaystyle\sum \frac{1}{(f(n))^2}$ also converges. So far, I have tried to show convergence of $\sum \frac{1}{(f(n))^2}$ by showing the limit of $\frac{f(n)^2}{f(1/a_n)}$ exists but it got me no where and after a couple of attempts I realized that's not the way to go. Any attempts to find an upper bound failed as well. Any hints will be appreciated.","Let be an unbounded, non-decreasing function, With the following property: For any positive sequence s.t. converges, also converges. Prove, Or disprove: also converges. So far, I have tried to show convergence of by showing the limit of exists but it got me no where and after a couple of attempts I realized that's not the way to go. Any attempts to find an upper bound failed as well. Any hints will be appreciated.",f a_n \sum a_n \sum\frac{1}{f(\frac{1}{a_n})} \displaystyle\sum \frac{1}{(f(n))^2} \sum \frac{1}{(f(n))^2} \frac{f(n)^2}{f(1/a_n)},"['real-analysis', 'calculus', 'sequences-and-series']"
88,Do distance-preserving maps from $\mathbb R^2 \rightarrow \mathbb R$ exist?,Do distance-preserving maps from  exist?,\mathbb R^2 \rightarrow \mathbb R,"So, I know that it's 'impossible' to have a perfectly bijective map $F:\mathbb R^2 \rightarrow \mathbb R$ , but I was wondering nevertheless: what would the 'best' possible map be, that is closest to being bijective? Additionally, can you make $F$ 'preserve distance' in some sense - and if so, what would the best form of $F$ be to do that? To clarify, if you had $n$ points in 2-D space, $F$ would ensure that points clustered close together stay relatively close. Apologies if this is too vague a question! Edit: Maybe I should make it clearer what I mean by 'close' since I didn't define it very well. I was thinking about collision detection, which is how I initially stumbled on this question. In collision detection, if you have two (circular) objects $A$ and $B$ , that have co-ords $(x_1,y_1), (x_2,y_2)$ , they collide if the sum of the distances between them is less than the sum of their radii. Practically though, it's computationally really expensive to do this check if you have a very large number of objects. In that case, you'd want to have a rough idea of the neighbourhood of each object, and only check for collisions within that neighbourhood -  some defined circular region of radius $r$ . If you had a map from $\mathbb R^2 \rightarrow \mathbb R$ (albeit a slightly imperfect, discontinuous map), then it'd be really easy to figure out what the neighbourhood is. Hopefully that makes things clearer!","So, I know that it's 'impossible' to have a perfectly bijective map , but I was wondering nevertheless: what would the 'best' possible map be, that is closest to being bijective? Additionally, can you make 'preserve distance' in some sense - and if so, what would the best form of be to do that? To clarify, if you had points in 2-D space, would ensure that points clustered close together stay relatively close. Apologies if this is too vague a question! Edit: Maybe I should make it clearer what I mean by 'close' since I didn't define it very well. I was thinking about collision detection, which is how I initially stumbled on this question. In collision detection, if you have two (circular) objects and , that have co-ords , they collide if the sum of the distances between them is less than the sum of their radii. Practically though, it's computationally really expensive to do this check if you have a very large number of objects. In that case, you'd want to have a rough idea of the neighbourhood of each object, and only check for collisions within that neighbourhood -  some defined circular region of radius . If you had a map from (albeit a slightly imperfect, discontinuous map), then it'd be really easy to figure out what the neighbourhood is. Hopefully that makes things clearer!","F:\mathbb R^2 \rightarrow \mathbb R F F n F A B (x_1,y_1), (x_2,y_2) r \mathbb R^2 \rightarrow \mathbb R","['real-analysis', 'general-topology', 'geometry', 'elementary-number-theory']"
89,"What is the sign of the integral $\int_{0}^{2\pi}e^{\sin(x)}\cos(nx)\,dx$?",What is the sign of the integral ?,"\int_{0}^{2\pi}e^{\sin(x)}\cos(nx)\,dx","Let $I_n = \int_{0}^{2\pi}e^{\sin(x)}\cos(nx)\,dx$ for a natural $n$ . I would like to prove that for $n$ odd this integral vanishes, while for $n = 4k$ for some natural $k$ , the integral is always positive and for $n = 4k+2$ it is always negative. I was able to do a few integrations by parts, and provided I didn't make mistakes, the result was the following rather ugly recurrence relation, valid for $n > 1$ : $$I_{n+2} = -\frac1{n-1}\left[(4n^3-2n)I_n+(n+1)I_{n-2}\right].$$ Also we have $I_1 = I_3 = 0$ (the first is obvious from the integral, the second follows by integration by parts). And from here, clearly it is immediate to see that it vanishes for every $n$ odd, just by induction. However I'm not able to see why $I_{4k}$ is always positive or $I_{4k+2}$ is always negative from this relation. What I know is that $|I_n| \le I_0 \approxeq 7.95$ . I also tried to rewrite the integral using $e^x = 1 + x + \ldots$ , moving the integral inside the summation, but still I couldn't come up with a solution. What do you suggest? Note : this integral comes from the calculation of the Fourier coefficients of $e^{\sin(x)}$ , and analogous properties hold for the coefficients of the sine terms. Also by a well-known theorem (Riemann-Lebesgue), $I_n$ goes to zero as $n\to\infty$ , and hence it's not possible to find a fixed constant lower bound for $|I_n|$ , making any estimation in the recurrence relation more difficult.","Let for a natural . I would like to prove that for odd this integral vanishes, while for for some natural , the integral is always positive and for it is always negative. I was able to do a few integrations by parts, and provided I didn't make mistakes, the result was the following rather ugly recurrence relation, valid for : Also we have (the first is obvious from the integral, the second follows by integration by parts). And from here, clearly it is immediate to see that it vanishes for every odd, just by induction. However I'm not able to see why is always positive or is always negative from this relation. What I know is that . I also tried to rewrite the integral using , moving the integral inside the summation, but still I couldn't come up with a solution. What do you suggest? Note : this integral comes from the calculation of the Fourier coefficients of , and analogous properties hold for the coefficients of the sine terms. Also by a well-known theorem (Riemann-Lebesgue), goes to zero as , and hence it's not possible to find a fixed constant lower bound for , making any estimation in the recurrence relation more difficult.","I_n = \int_{0}^{2\pi}e^{\sin(x)}\cos(nx)\,dx n n n = 4k k n = 4k+2 n > 1 I_{n+2} = -\frac1{n-1}\left[(4n^3-2n)I_n+(n+1)I_{n-2}\right]. I_1 = I_3 = 0 n I_{4k} I_{4k+2} |I_n| \le I_0 \approxeq 7.95 e^x = 1 + x + \ldots e^{\sin(x)} I_n n\to\infty |I_n|","['real-analysis', 'integration', 'complex-analysis', 'definite-integrals', 'recurrence-relations']"
90,"If $\lim_{x \to \infty} f(x) - xf'(x)$ exists, does $\lim_{x \to\infty} f'(x)$ exist as well?","If  exists, does  exist as well?",\lim_{x \to \infty} f(x) - xf'(x) \lim_{x \to\infty} f'(x),"Let $f(x)$ be a differentiable function on $(0, \infty)$ with $\lim_{x\to \infty} f(x) - xf'(x) = L\in \mathbb{R}$ . I'm trying to prove or disprove that $\lim_{x\to\infty} f'(x)$ exists as well. Here's what I have so far: if the limit does exist, it must also equal $\lim_{x\to\infty} \frac{f(x)}{x}$ (divide the limit condition by $x$ ). Then I rewrote the condition as $x^2 \frac{d}{dx} \frac{f(x)}{x} \to L$ , and from this I expect (using some $1/x^2$ asymptotic argument) my statement to be true - but I'm not sure how to rigorously proceed with this argument.. Can someone provide some next steps, or a counter example?","Let be a differentiable function on with . I'm trying to prove or disprove that exists as well. Here's what I have so far: if the limit does exist, it must also equal (divide the limit condition by ). Then I rewrote the condition as , and from this I expect (using some asymptotic argument) my statement to be true - but I'm not sure how to rigorously proceed with this argument.. Can someone provide some next steps, or a counter example?","f(x) (0, \infty) \lim_{x\to \infty} f(x) - xf'(x) = L\in \mathbb{R} \lim_{x\to\infty} f'(x) \lim_{x\to\infty} \frac{f(x)}{x} x x^2 \frac{d}{dx} \frac{f(x)}{x} \to L 1/x^2","['real-analysis', 'limits', 'derivatives']"
91,Converse of Taylor's Theorem,Converse of Taylor's Theorem,,"Let $n$ be a nonnegative integer and $a,b\in\mathbb{R}$ such that $a<b$ .  From Taylor's Theorem, we know that any $n$ -time differentiable function $f:(a,b)\to \mathbb{R}$ satisfies the condition that $$f(x+h)=\sum_{k=0}^n\,\frac{f_k(x)}{k!}\,h^k+R_n(x,h)\text{ for all $x\in(a,b)$ and $h\in(a-x,b-x)$}\,,\tag{*}$$ where $f_k:(a,b)\to\mathbb{R}$ is the $k$ -th derivative of $f$ for each $k=0,1,2,\ldots,n$ (in particular, $f_0=f$ ), and the $n$ -th remainder term $R_n(x,h)$ satisfies $$R_n(x,h)\in o\left(h^n\right)\text{ for each $x\in(a,b)$ and for every small $h\in\mathbb{R}$}\,.\tag{**}$$ (In other words, $\lim\limits_{h\to 0}\,\dfrac{R_n(x,h)}{h^n}=0$ for all $x\in (a,b)$ .) I have a question whether the converse of Taylor's Theorem is true.  In other words, is the following conjecture correct? Conjecture. Suppose that functions $f,f_0,f_1,f_2,\ldots,f_n:(a,b)\to\mathbb{R}$ satisfy (*) and (**).  Then, $f$ is $n$ -time differentiable, with $k$ -th derivative $f_k$ for each $k=0,1,2,\ldots,n$ (in particular, $f_0=f$ ). From this link , some continuity or boundedness constraints on the $f_k$ 's or on the remainder term $R_n$ are assumed for the converse to hold.  If the converse does not hold in general (i.e., without these continuity or boundedness constraints), could anybody give a counterexample?  If it is true, then can you please give me a proof or a reference?  What I know is that the converse holds for $n=0$ (trivially) and $n=1$ (with a small amount of work).","Let be a nonnegative integer and such that .  From Taylor's Theorem, we know that any -time differentiable function satisfies the condition that where is the -th derivative of for each (in particular, ), and the -th remainder term satisfies (In other words, for all .) I have a question whether the converse of Taylor's Theorem is true.  In other words, is the following conjecture correct? Conjecture. Suppose that functions satisfy (*) and (**).  Then, is -time differentiable, with -th derivative for each (in particular, ). From this link , some continuity or boundedness constraints on the 's or on the remainder term are assumed for the converse to hold.  If the converse does not hold in general (i.e., without these continuity or boundedness constraints), could anybody give a counterexample?  If it is true, then can you please give me a proof or a reference?  What I know is that the converse holds for (trivially) and (with a small amount of work).","n a,b\in\mathbb{R} a<b n f:(a,b)\to \mathbb{R} f(x+h)=\sum_{k=0}^n\,\frac{f_k(x)}{k!}\,h^k+R_n(x,h)\text{ for all x\in(a,b) and h\in(a-x,b-x)}\,,\tag{*} f_k:(a,b)\to\mathbb{R} k f k=0,1,2,\ldots,n f_0=f n R_n(x,h) R_n(x,h)\in o\left(h^n\right)\text{ for each x\in(a,b) and for every small h\in\mathbb{R}}\,.\tag{**} \lim\limits_{h\to 0}\,\dfrac{R_n(x,h)}{h^n}=0 x\in (a,b) f,f_0,f_1,f_2,\ldots,f_n:(a,b)\to\mathbb{R} f n k f_k k=0,1,2,\ldots,n f_0=f f_k R_n n=0 n=1","['real-analysis', 'numerical-methods', 'taylor-expansion', 'functional-equations', 'approximation-theory']"
92,Prove/Show that Limit is equal to 0.,Prove/Show that Limit is equal to 0.,,"I'm really struggling on this question.  I've been thinking about it for awhile now but this is one of those where I don't really have much intuition on what to do. Problem 3. Prove that if the limit $\lim_{x\to +\infty}f(x) =: L$ exists (finite or infinity) and the improper integral. $$\int_a^{+\infty}f(x) dx$$ is convergent, then $L = 0$ . Here is some facts that I know that I think I should perhaps use. Facts/Knowledge I know: The above integral can be rewritten as: $$\int_a^{\infty}f(x)dx = \lim_{A\to \infty}\int_a^{A}f(x)dx$$ We know that limit as x goest to infinity of $f(x)=L$ .  I need to somehow show that limit L is equal to 0. If I take the integral I get $\lim_{A \to \infty} F(A)-f(a)$ and I know this does not equal infinity as the integral converges.  I'm not sure if this is on the right track or what to do next. I was thinking maybe perhaps instead I could use the Cauchy Criterion to come up with a proof.  Note: I'm not used to doing proofs with Improper integrals. Theorem 1 (Cauchy Criterion). The improper integral (1) converges if and only if for every $\epsilon > 0$ there is an $M\geqslant a$ so that for all $A, B \geqslant M$ we have $$\Bigg|\int_A^B f(x) dx\Bigg| < \epsilon$$","I'm really struggling on this question.  I've been thinking about it for awhile now but this is one of those where I don't really have much intuition on what to do. Problem 3. Prove that if the limit exists (finite or infinity) and the improper integral. is convergent, then . Here is some facts that I know that I think I should perhaps use. Facts/Knowledge I know: The above integral can be rewritten as: We know that limit as x goest to infinity of .  I need to somehow show that limit L is equal to 0. If I take the integral I get and I know this does not equal infinity as the integral converges.  I'm not sure if this is on the right track or what to do next. I was thinking maybe perhaps instead I could use the Cauchy Criterion to come up with a proof.  Note: I'm not used to doing proofs with Improper integrals. Theorem 1 (Cauchy Criterion). The improper integral (1) converges if and only if for every there is an so that for all we have","\lim_{x\to +\infty}f(x) =: L \int_a^{+\infty}f(x) dx L = 0 \int_a^{\infty}f(x)dx = \lim_{A\to \infty}\int_a^{A}f(x)dx f(x)=L \lim_{A \to \infty} F(A)-f(a) \epsilon > 0 M\geqslant a A, B \geqslant M \Bigg|\int_A^B f(x) dx\Bigg| < \epsilon","['real-analysis', 'integration', 'analysis', 'limits', 'improper-integrals']"
93,"If continuity preserves convergence, and Cauchy sequences are convergent sequences, why do we need uniform continuity to preserve Cauchy sequences?","If continuity preserves convergence, and Cauchy sequences are convergent sequences, why do we need uniform continuity to preserve Cauchy sequences?",,"In $\mathbb R$, all Cauchy sequences are convergent and all convergent sequences are Cauchy. So, why isn't continuity enough to preserve Cauchy sequences? A function is continuous iff it preserves convergent sequences. A sequence is convergent iff it is Cauchy. So, why doesn't it follow that continuous functions preserve Cauchy sequences?","In $\mathbb R$, all Cauchy sequences are convergent and all convergent sequences are Cauchy. So, why isn't continuity enough to preserve Cauchy sequences? A function is continuous iff it preserves convergent sequences. A sequence is convergent iff it is Cauchy. So, why doesn't it follow that continuous functions preserve Cauchy sequences?",,"['real-analysis', 'intuition']"
94,"Get a good approximation of $\int_0^1 \left(H_x\right)^2 dx$, where $H_x$ is the generalized harmonic number","Get a good approximation of , where  is the generalized harmonic number",\int_0^1 \left(H_x\right)^2 dx H_x,"The code integrate (H_x)^2 dx, from x=0 to x=1 in Wolfram alpha online calculator , where as you see $H_x$ is a generalized harmonic number, tell us that holds $$\int_0^1 \left(H_x\right)^2 dx\approx 0.413172.$$ I've curiosity about Question. How one can calculate with analysis or numerical analysis an approximation of $$\int_0^1 \left(H_x\right)^2 dx?$$ Thus you are able to use your knowledges about the harmonic numbers, or well if your approach is using numerical analysis tell us what's your numerical method and how works it. Many thanks.","The code integrate (H_x)^2 dx, from x=0 to x=1 in Wolfram alpha online calculator , where as you see is a generalized harmonic number, tell us that holds I've curiosity about Question. How one can calculate with analysis or numerical analysis an approximation of Thus you are able to use your knowledges about the harmonic numbers, or well if your approach is using numerical analysis tell us what's your numerical method and how works it. Many thanks.",H_x \int_0^1 \left(H_x\right)^2 dx\approx 0.413172. \int_0^1 \left(H_x\right)^2 dx?,"['real-analysis', 'integration']"
95,"Proving algebraically $a^2+b^2\ge a^{\alpha}b^{2-\alpha}$ for $0\le\alpha\le2$ and $a,b\ge0$",Proving algebraically  for  and,"a^2+b^2\ge a^{\alpha}b^{2-\alpha} 0\le\alpha\le2 a,b\ge0","My Analysis professor showed this inequality and elegantly proved it using polar coordinates, saying that it can't be done algebraically. Instead, here's how I think I have handled it: firstly we see it's true for $ab=0$; dividing by the RHS we get $$\left(\frac{a}{b}\right)^{2-\alpha}+\left(\frac{a}{b}\right)^{-\alpha}\ge1, $$ or equivalently, setting $t=a/b$, $$t^2+1\ge t^\alpha$$ which holds because the LHS is $\ge t^2\ge t^\alpha$ for $t\ge1$ and $\ge1\ge t^\alpha$ for $0\le t<1$. Am I missing something? Are there fancy algebraic (perhaps some linear algebra inequalities) ways to prove the inequality?","My Analysis professor showed this inequality and elegantly proved it using polar coordinates, saying that it can't be done algebraically. Instead, here's how I think I have handled it: firstly we see it's true for $ab=0$; dividing by the RHS we get $$\left(\frac{a}{b}\right)^{2-\alpha}+\left(\frac{a}{b}\right)^{-\alpha}\ge1, $$ or equivalently, setting $t=a/b$, $$t^2+1\ge t^\alpha$$ which holds because the LHS is $\ge t^2\ge t^\alpha$ for $t\ge1$ and $\ge1\ge t^\alpha$ for $0\le t<1$. Am I missing something? Are there fancy algebraic (perhaps some linear algebra inequalities) ways to prove the inequality?",,"['real-analysis', 'linear-algebra', 'inequality', 'alternative-proof']"
96,"Let $a_n=\cos(a_{n-1}), L=[a_1,a_2,...,a_n,...].$ Is there an $a_0$ such that $L$ is dense in$[-1,1]?$",Let  Is there an  such that  is dense in,"a_n=\cos(a_{n-1}), L=[a_1,a_2,...,a_n,...]. a_0 L [-1,1]?","I've been experimenting with recursive sequences lately and I've come up with this problem: Let  $a_n= \cos(a_{n-1})$ with $a_0 \in \Bbb{R}$ and $L=[a_1,a_2,...,a_n,...].$ Does there exist an $a_0$ such that $L$ is dense in $[-1,1]?$ I know of $3$ ways of examining whether a set is dense: $i)$The definition, that is, whether its closure is the set on which it is dense, in our case this means if: $\bar L=[-1,1]$. ii)$(\forall x \in [-1,1])(\forall \epsilon>0)(\exists b \in L):|x-b|<\epsilon$ $iii)$ $(\forall x \in [-1,1])(\exists b_n \subseteq L):b_n\rightarrow  x$ So far I haven't been able to use these to answer the question. I tried plugging in different values of $a_0$ and see where that leads but I have not found any corresponding promising ""pattern"" for $a_n$. Any ideas on how to approach this?","I've been experimenting with recursive sequences lately and I've come up with this problem: Let  $a_n= \cos(a_{n-1})$ with $a_0 \in \Bbb{R}$ and $L=[a_1,a_2,...,a_n,...].$ Does there exist an $a_0$ such that $L$ is dense in $[-1,1]?$ I know of $3$ ways of examining whether a set is dense: $i)$The definition, that is, whether its closure is the set on which it is dense, in our case this means if: $\bar L=[-1,1]$. ii)$(\forall x \in [-1,1])(\forall \epsilon>0)(\exists b \in L):|x-b|<\epsilon$ $iii)$ $(\forall x \in [-1,1])(\exists b_n \subseteq L):b_n\rightarrow  x$ So far I haven't been able to use these to answer the question. I tried plugging in different values of $a_0$ and see where that leads but I have not found any corresponding promising ""pattern"" for $a_n$. Any ideas on how to approach this?",,"['real-analysis', 'sequences-and-series']"
97,What is difference between open set and open interval?,What is difference between open set and open interval?,,"Let $\tau$ be the Euclidean topology defined on $\mathbb R$. If we define a set $S = (2,3) \cup (5,6)$. Then is the set $S$ an open set and open interval on $\tau$? As per definition of open set, A subset $A$ of $\tau$ is open set if  $\forall x \in A$, $\hspace{5pt} \exists  \hspace{3pt} a,b $ such that $x \in (a,b) \subseteq A$ As per this definition, we can find $(a,b) \in S$ such that $x \in (a,b)$. $x$ is any number in set $S$. So, set $S$ is open set. I am not sure whether I correctly proved why set $S$ is open set. But I do not know how to prove set $S$ is open interval.","Let $\tau$ be the Euclidean topology defined on $\mathbb R$. If we define a set $S = (2,3) \cup (5,6)$. Then is the set $S$ an open set and open interval on $\tau$? As per definition of open set, A subset $A$ of $\tau$ is open set if  $\forall x \in A$, $\hspace{5pt} \exists  \hspace{3pt} a,b $ such that $x \in (a,b) \subseteq A$ As per this definition, we can find $(a,b) \in S$ such that $x \in (a,b)$. $x$ is any number in set $S$. So, set $S$ is open set. I am not sure whether I correctly proved why set $S$ is open set. But I do not know how to prove set $S$ is open interval.",,"['real-analysis', 'general-topology', 'functional-analysis']"
98,"Does $\infty$ mean $+\infty$ in ""English mathematics""?","Does  mean  in ""English mathematics""?",\infty +\infty,My question background is the set $\mathbb R$ . I often see the symbol $\infty$ . Does it always mean $+\infty$ or can it have the meaning of $\pm \infty$ ? In particular what means that a real sequence $(a_n)$ has $\infty$ for limit? That it has $+\infty$ for limit? Or that the sequence $(\vert a_n \vert)$ has $+\infty$ for limit? I'm French and we usually don't use $\infty$ without a sign in the context of real numbers.,My question background is the set . I often see the symbol . Does it always mean or can it have the meaning of ? In particular what means that a real sequence has for limit? That it has for limit? Or that the sequence has for limit? I'm French and we usually don't use without a sign in the context of real numbers.,\mathbb R \infty +\infty \pm \infty (a_n) \infty +\infty (\vert a_n \vert) +\infty \infty,"['calculus', 'real-analysis', 'notation']"
99,Does the Fourier transform of a smooth $L^2$ function decay rapidly at infinity,Does the Fourier transform of a smooth  function decay rapidly at infinity,L^2,"For $L^1$ functions, there is a correspondence through the Fourier Transform between functions which are smooth and functions which decay rapidly at infinity. More precisely, if $f$ is a smooth $L^1$ function, $\hat{f}$ is an $L^\infty$ function which decays faster than any polynomial at infinity, and conversely, if $g$ is an $L^\infty$ function which decays faster than any polynomial at infinity, then the inverse fourier transform of $g$ is $L^1$ and smooth. For $L^2$ functions, there is a difficulty. The Fourier transform formula is no longer valid on individual functions. It only is defined on the $L^2$ classes of functions. I have two questions: Is there a standard way to extend ""decays rapidly at infinity"" to $L^2$ classes of functions? Assuming (1), is there a 1-1 correspondence between classes of $L^2$ functions with a smooth representative and classes of $L^2$ functions which decay rapidly at infinity?","For $L^1$ functions, there is a correspondence through the Fourier Transform between functions which are smooth and functions which decay rapidly at infinity. More precisely, if $f$ is a smooth $L^1$ function, $\hat{f}$ is an $L^\infty$ function which decays faster than any polynomial at infinity, and conversely, if $g$ is an $L^\infty$ function which decays faster than any polynomial at infinity, then the inverse fourier transform of $g$ is $L^1$ and smooth. For $L^2$ functions, there is a difficulty. The Fourier transform formula is no longer valid on individual functions. It only is defined on the $L^2$ classes of functions. I have two questions: Is there a standard way to extend ""decays rapidly at infinity"" to $L^2$ classes of functions? Assuming (1), is there a 1-1 correspondence between classes of $L^2$ functions with a smooth representative and classes of $L^2$ functions which decay rapidly at infinity?",,"['real-analysis', 'functional-analysis', 'fourier-analysis']"
