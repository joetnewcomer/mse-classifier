,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Will a Roomba always vacuum the whole room?,Will a Roomba always vacuum the whole room?,,"This question, as indicated in the title, is motivated by the Roomba vacuum, but I am going to formulate it in slightly more mathematical terms: Suppose that I am given a union $U$ of finitely many almost disjoint squares of area $A\in \mathbb{N}$. Moreover, a point $p$ were chosen from $U$ randomly (evenly distributed). Finally, assume that I know if any point $q$ is within distance $\frac{\sqrt{A}}{2}$ of the boundary of $U$. Can I come up with an algorithm to find a path that is piece-wise linear and will pass through each square once? My initial thoughts are to think about this like a graph theoretic problem. You can construct a graph out of $U$ by treating it like a graph (corners as nodes) and taking the dual graph, which we will call $U^*$. This graph tells you about the connectedness of the squares. Then you start on a random node $v_1$ of $U^*$ and the probability of moving to an adjacent node $v_2$ is as follows $$ P(v_2|v_1) = \begin{cases} 1/n \quad d(v_1,v_2) > \frac{\sqrt{A}}{2}\\ 0 \quad\quad d(v_1,v_2) \le \frac{\sqrt{A}}{2} \end{cases}$$ where $n$ is the number of adjacent nodes with $d(v_1,v_2) > \frac{\sqrt{A}}{2}$. Then you do the same thing for all the following nodes. The thing you would try to prove is that $\forall 1 \geq \epsilon > 0$, there is a $k \in \mathbb{N}$ such that, after $k$ steps, the probability that you have been to every node is $1 - \epsilon$. But I have no idea how to prove that sort of thing. Any thoughts on how to better approach this problem? (Or on how Roombas actually work?)","This question, as indicated in the title, is motivated by the Roomba vacuum, but I am going to formulate it in slightly more mathematical terms: Suppose that I am given a union $U$ of finitely many almost disjoint squares of area $A\in \mathbb{N}$. Moreover, a point $p$ were chosen from $U$ randomly (evenly distributed). Finally, assume that I know if any point $q$ is within distance $\frac{\sqrt{A}}{2}$ of the boundary of $U$. Can I come up with an algorithm to find a path that is piece-wise linear and will pass through each square once? My initial thoughts are to think about this like a graph theoretic problem. You can construct a graph out of $U$ by treating it like a graph (corners as nodes) and taking the dual graph, which we will call $U^*$. This graph tells you about the connectedness of the squares. Then you start on a random node $v_1$ of $U^*$ and the probability of moving to an adjacent node $v_2$ is as follows $$ P(v_2|v_1) = \begin{cases} 1/n \quad d(v_1,v_2) > \frac{\sqrt{A}}{2}\\ 0 \quad\quad d(v_1,v_2) \le \frac{\sqrt{A}}{2} \end{cases}$$ where $n$ is the number of adjacent nodes with $d(v_1,v_2) > \frac{\sqrt{A}}{2}$. Then you do the same thing for all the following nodes. The thing you would try to prove is that $\forall 1 \geq \epsilon > 0$, there is a $k \in \mathbb{N}$ such that, after $k$ steps, the probability that you have been to every node is $1 - \epsilon$. But I have no idea how to prove that sort of thing. Any thoughts on how to better approach this problem? (Or on how Roombas actually work?)",,"['probability', 'algorithms', 'random-walk']"
1,"Finding $\operatorname{E}(X)$ given that $\operatorname{E}(X) = \operatorname{var}(X)$, $\operatorname{E}(Y) = \operatorname{var}(Y)$, and $Y=3X+3$","Finding  given that , , and",\operatorname{E}(X) \operatorname{E}(X) = \operatorname{var}(X) \operatorname{E}(Y) = \operatorname{var}(Y) Y=3X+3,"$\newcommand{\E}{\operatorname{E}}\newcommand{\v}{\operatorname{var}}$ Suppose a random variable $X$ is such that its expected value is equal to its variance. If $Y= 3X+ 3$ is also a random variable having its expected value equal to its variance, what must the value of $\E(X)$ be? Attempted Solution: I'm making use of the following formulas: $\E(aX+b) = a\E(X) + b$ $\v(aX+b) = a^2\v(X)$ We're given $\E(X) = \v(X)$ and $\E(Y) = \v(Y)$ . $\Rightarrow$ $\E(Y) = \v(Y)$ $\Rightarrow$ $\E(3X+3) = \v(3X+3)$ $\Rightarrow$ $3\E(X)+3 = 3^2\v(X) = 3^2\E(X)$ $\Rightarrow$ $3 = 6\E(X)$ $\Rightarrow$ $\E(X) = {1 \over 2}$ I think I did this correctly but I just wanted to make sure.","Suppose a random variable is such that its expected value is equal to its variance. If is also a random variable having its expected value equal to its variance, what must the value of be? Attempted Solution: I'm making use of the following formulas: We're given and . I think I did this correctly but I just wanted to make sure.",\newcommand{\E}{\operatorname{E}}\newcommand{\v}{\operatorname{var}} X Y= 3X+ 3 \E(X) \E(aX+b) = a\E(X) + b \v(aX+b) = a^2\v(X) \E(X) = \v(X) \E(Y) = \v(Y) \Rightarrow \E(Y) = \v(Y) \Rightarrow \E(3X+3) = \v(3X+3) \Rightarrow 3\E(X)+3 = 3^2\v(X) = 3^2\E(X) \Rightarrow 3 = 6\E(X) \Rightarrow \E(X) = {1 \over 2},"['probability', 'variance', 'expected-value']"
2,The probability of hitting equally distanced mines on a straight line,The probability of hitting equally distanced mines on a straight line,,"Recently I was given the following problem. Anti-tank mines are placed on a straight line 15 meters apart from each other. The tank, 3 meters wide, runs perpendicular to this line. What is the probability that the tank will hit a mine. My problem with this question is that the exact length of the line is not given. I guess depending on the length of the line, the probability might be different. It seems to me that the problem is not well defined. What is your opinion?","Recently I was given the following problem. Anti-tank mines are placed on a straight line 15 meters apart from each other. The tank, 3 meters wide, runs perpendicular to this line. What is the probability that the tank will hit a mine. My problem with this question is that the exact length of the line is not given. I guess depending on the length of the line, the probability might be different. It seems to me that the problem is not well defined. What is your opinion?",,['probability']
3,Problem with two loaded die and the probability to get a sum greater than 10 knowing the outcome of one die,Problem with two loaded die and the probability to get a sum greater than 10 knowing the outcome of one die,,"I face the following problem but disagree with the solution that has been provided to me: You throw two times a loaded die such that $P(1)=P(3)=P(4)=P(5)=1/8$ and >$P(2)=P(6)=1/4$. One of the dice gave 6 (but we don't know which one). What is the probability that the sum that you obtain is strictly greater than 10? The solution that I received is the following: The results can be written as $(6, x)$ and $(x, 6)$, where $x$ must be equal to 5 or 6. So $P(> 10) = 2×(1/8+1/4)−1/4 = 1/2$. But I am skeptical about this answer, as the ""method"" does not work if we try to compute the complement $P(=10$ or lower than $10)$, as the result would give a probability greater than 1... I however cannot figure out where the problem comes from!","I face the following problem but disagree with the solution that has been provided to me: You throw two times a loaded die such that $P(1)=P(3)=P(4)=P(5)=1/8$ and >$P(2)=P(6)=1/4$. One of the dice gave 6 (but we don't know which one). What is the probability that the sum that you obtain is strictly greater than 10? The solution that I received is the following: The results can be written as $(6, x)$ and $(x, 6)$, where $x$ must be equal to 5 or 6. So $P(> 10) = 2×(1/8+1/4)−1/4 = 1/2$. But I am skeptical about this answer, as the ""method"" does not work if we try to compute the complement $P(=10$ or lower than $10)$, as the result would give a probability greater than 1... I however cannot figure out where the problem comes from!",,['probability']
4,Probability distribution of $\frac{1}{\sigma^2}Y'AY$,Probability distribution of,\frac{1}{\sigma^2}Y'AY,"If $A_{p\times p}$ is a non-random matrix, symmetric and idempotent   matrix with $\mu_{p\times 1}=0$ and $\Sigma=\sigma^2  I_{p\times p}$, then $$V=\frac{1}{\sigma^2}Y'AY\sim \chi_r^2$$ where   $Y_{p\times 1}\sim N(\mu,\Sigma)$ and $r=rank(A)$. First I used the matrix properties of $A$, so $$V=\frac{1}{\sigma^2}Y'AY=\frac{1}{\sigma^2}Y'AAY=\frac{1}{\sigma^2}(AY)'AY$$ Let $Z_{p\times 1}=AY$ then $$V=\frac{1}{\sigma^2}Z'Z=\frac{1}{\sigma^2}\sum_{i=1}^pZ_i^2 \quad(*)$$ To find the distribution of $Z$ I used moment generating function $$M_Z(t)=E[\exp(t'(AY)]=E[\exp(A't)'Y]=M_Y(t)(A't)$$ $$=\exp\Big(\frac{1}{2}t'(A\Sigma A')t\Big)$$ so $Z\sim N_p(0,A\Sigma A')$ and from marginalization propertie I know that each $Z_i$ have Normal distribution also. The problem is that I don't find a way to link it with $(*)$ I'm not understanding well the relationsheep between the rank of $A$ and the degree of freedom in the chi-squared distribution. Why when I have $A=I_{p\times p}$ (identity matrix) I get that $V\sim \chi_p^2$?","If $A_{p\times p}$ is a non-random matrix, symmetric and idempotent   matrix with $\mu_{p\times 1}=0$ and $\Sigma=\sigma^2  I_{p\times p}$, then $$V=\frac{1}{\sigma^2}Y'AY\sim \chi_r^2$$ where   $Y_{p\times 1}\sim N(\mu,\Sigma)$ and $r=rank(A)$. First I used the matrix properties of $A$, so $$V=\frac{1}{\sigma^2}Y'AY=\frac{1}{\sigma^2}Y'AAY=\frac{1}{\sigma^2}(AY)'AY$$ Let $Z_{p\times 1}=AY$ then $$V=\frac{1}{\sigma^2}Z'Z=\frac{1}{\sigma^2}\sum_{i=1}^pZ_i^2 \quad(*)$$ To find the distribution of $Z$ I used moment generating function $$M_Z(t)=E[\exp(t'(AY)]=E[\exp(A't)'Y]=M_Y(t)(A't)$$ $$=\exp\Big(\frac{1}{2}t'(A\Sigma A')t\Big)$$ so $Z\sim N_p(0,A\Sigma A')$ and from marginalization propertie I know that each $Z_i$ have Normal distribution also. The problem is that I don't find a way to link it with $(*)$ I'm not understanding well the relationsheep between the rank of $A$ and the degree of freedom in the chi-squared distribution. Why when I have $A=I_{p\times p}$ (identity matrix) I get that $V\sim \chi_p^2$?",,"['probability', 'statistics', 'normal-distribution']"
5,What's the average euclidian distance between two points on a unit n-sphere?,What's the average euclidian distance between two points on a unit n-sphere?,,"Suppose we randomly place 2 points the 100-dimensional unit sphere. So we have $$x_1, x_2\in \mathbb{R}^{100}\quad\text{ and }\quad|x_1|=|x_2|=1$$ What's the expected value of the euclidian distance between them? $$E[|x_1 - x_2|]=\ ?$$ From just eyeballing some data, the answer looks like $\approx1.2$ And what about in general? So for 2 points on an n-dimensional sphere?","Suppose we randomly place 2 points the 100-dimensional unit sphere. So we have $$x_1, x_2\in \mathbb{R}^{100}\quad\text{ and }\quad|x_1|=|x_2|=1$$ What's the expected value of the euclidian distance between them? $$E[|x_1 - x_2|]=\ ?$$ From just eyeballing some data, the answer looks like $\approx1.2$ And what about in general? So for 2 points on an n-dimensional sphere?",,['probability']
6,Why do distribution functions have Leibniz rule?,Why do distribution functions have Leibniz rule?,,"Suppose I have two distribution functions $f, g: \mathbb{R} \rightarrow [0,1]$, I.e. Non-decreasing, right continuous with $\underset{t\rightarrow \infty}{\lim} = 1, \underset{t\rightarrow - \infty}{\lim}= 0$.  Why does the measure given by the product of the distribution functions obey the Leibniz rule: $d(f\cdot g)=g df + f dg$?  By $df$ I mean the Borel measure given by assigning the measure $f(b)-f(a)$ to the interval $(a,b]$.","Suppose I have two distribution functions $f, g: \mathbb{R} \rightarrow [0,1]$, I.e. Non-decreasing, right continuous with $\underset{t\rightarrow \infty}{\lim} = 1, \underset{t\rightarrow - \infty}{\lim}= 0$.  Why does the measure given by the product of the distribution functions obey the Leibniz rule: $d(f\cdot g)=g df + f dg$?  By $df$ I mean the Borel measure given by assigning the measure $f(b)-f(a)$ to the interval $(a,b]$.",,"['probability', 'analysis']"
7,What are chance of two randomly generated 4 digit strings being the same?,What are chance of two randomly generated 4 digit strings being the same?,,"I am building a website where an alphanumeric ID will be generated. As of now, I have it randomly grabbing a number or letter from this data set: ABCDEFGHIJKLMNOPQRSTUVWXYZ 0123456789 So there are 36 possibilities. If my math is right, if I make the ID 4 digits/characters long, the odds of a duplicate would be: 1/36 * 1/36 * 1/36 * 1/36 which equals 1/1,679,616. In a nutshell, I want to eliminate (within reason) the possibility of a duplicate ID being created, but the shorter the ID the better. If I went with 4 digits and ended up creating 15,000 IDs (like 4T6H, GF29, etc.), what are the chances that two of them will be the exact same? If I generated ONE, it's obviously 1 in 1,679,616 (i think).  But wondering how the odds are affected if I generate 15,000 of them. Any insight/corrections on my math would be greatly appreciated.","I am building a website where an alphanumeric ID will be generated. As of now, I have it randomly grabbing a number or letter from this data set: ABCDEFGHIJKLMNOPQRSTUVWXYZ 0123456789 So there are 36 possibilities. If my math is right, if I make the ID 4 digits/characters long, the odds of a duplicate would be: 1/36 * 1/36 * 1/36 * 1/36 which equals 1/1,679,616. In a nutshell, I want to eliminate (within reason) the possibility of a duplicate ID being created, but the shorter the ID the better. If I went with 4 digits and ended up creating 15,000 IDs (like 4T6H, GF29, etc.), what are the chances that two of them will be the exact same? If I generated ONE, it's obviously 1 in 1,679,616 (i think).  But wondering how the odds are affected if I generate 15,000 of them. Any insight/corrections on my math would be greatly appreciated.",,['probability']
8,Types of convergence in probability confusion?,Types of convergence in probability confusion?,,"Okay so I recently learned about three types of convergence in probability. Convergence in distribution, convergence in probability and almost sure convergence. I have the definitions memorised but I am struggling to understand what they mean in essence, why we need all these different types of convergence and a few technical details. Here are my concerns. 1) In two of these convergences we require that a certain limit is satisfied. Take for example convergence in probability it says $X_1,X_2,...\implies X$ in probability if for all $\epsilon>0:\lim_{n \rightarrow \infty} P(|X_n-X|>\epsilon)=0$. Now in my mind intuitively this is saying that as we go further and further along the sequence of random variables ($n \rightarrow \infty$) the probability that $X_n$ differs from $X$ by any positive amount becomes arbitrarily small. But I don't understand what it means for $|X_n-X|$ to become small since these are random variables which are themselves functions. How exactly do we measure distance between these two functions? I have seen some metrics of spaces of functions but I don't think that is what is being used here. Would it just mean that $|X_n(w)-X(w)|$ gets small for any $w$ in the sample space? In which case why don't we specify the definition as $X_1,X_2,...\implies X$ in probability if for all $\epsilon>0$ and for all $w$ in the sample space :$\lim_{n \rightarrow \infty} P(|X_n(w)-X(w)|>\epsilon)=0$? 2) Almost sure convergence says $X_1,... \rightarrow X$ if we have $P(\lim_{n \rightarrow \infty} X_n=X)=1$ again what does this mean? Is it saying that $X_n(w)=X(w)$ in the limit as $n$ tends to infinity for all $w$ in the sample space. 3) Why do we need both almost sure and convergence in probability? I know that almost sure is stronger and thus I suppose I have answered my own question but I don't really see why. To me almost sure says that $X_n$ becomes the same as $X$ as we let $n$ become very large and convergence in probability to me seems to say $X_n$ can't be different from $X$ as $n$ becomes large this seems like two ways to say the same thing. I think that is all I have at the moment hopefully I have expressed my concerns in a way that makes it somewhat easy to see what I don't get. If anyone could help clarify that would be great. Thanks! By the way I have met these definitions in my lectures but nobody in our year has covered measure theory yet so I wonder if I am not truly meant to understand these definitions yet.","Okay so I recently learned about three types of convergence in probability. Convergence in distribution, convergence in probability and almost sure convergence. I have the definitions memorised but I am struggling to understand what they mean in essence, why we need all these different types of convergence and a few technical details. Here are my concerns. 1) In two of these convergences we require that a certain limit is satisfied. Take for example convergence in probability it says $X_1,X_2,...\implies X$ in probability if for all $\epsilon>0:\lim_{n \rightarrow \infty} P(|X_n-X|>\epsilon)=0$. Now in my mind intuitively this is saying that as we go further and further along the sequence of random variables ($n \rightarrow \infty$) the probability that $X_n$ differs from $X$ by any positive amount becomes arbitrarily small. But I don't understand what it means for $|X_n-X|$ to become small since these are random variables which are themselves functions. How exactly do we measure distance between these two functions? I have seen some metrics of spaces of functions but I don't think that is what is being used here. Would it just mean that $|X_n(w)-X(w)|$ gets small for any $w$ in the sample space? In which case why don't we specify the definition as $X_1,X_2,...\implies X$ in probability if for all $\epsilon>0$ and for all $w$ in the sample space :$\lim_{n \rightarrow \infty} P(|X_n(w)-X(w)|>\epsilon)=0$? 2) Almost sure convergence says $X_1,... \rightarrow X$ if we have $P(\lim_{n \rightarrow \infty} X_n=X)=1$ again what does this mean? Is it saying that $X_n(w)=X(w)$ in the limit as $n$ tends to infinity for all $w$ in the sample space. 3) Why do we need both almost sure and convergence in probability? I know that almost sure is stronger and thus I suppose I have answered my own question but I don't really see why. To me almost sure says that $X_n$ becomes the same as $X$ as we let $n$ become very large and convergence in probability to me seems to say $X_n$ can't be different from $X$ as $n$ becomes large this seems like two ways to say the same thing. I think that is all I have at the moment hopefully I have expressed my concerns in a way that makes it somewhat easy to see what I don't get. If anyone could help clarify that would be great. Thanks! By the way I have met these definitions in my lectures but nobody in our year has covered measure theory yet so I wonder if I am not truly meant to understand these definitions yet.",,"['probability', 'probability-theory']"
9,A combinatorial solution to the tool maker problem,A combinatorial solution to the tool maker problem,,"I'm trying to verify the claim presented in this question . Let's assume that we are interested in the following probability in both cases (i.e. sorted piles(SP) and messy heap(MH)): The probability of choosing $k$ stones from $N$ stones with which the tool maker can make $k$ different tools. For the (SP) case: The probability is actually one. Because we have distinct piles of each stone's type. So, one can pick one stone from each pile (Assume we have $k$ types of stone with the frequency of $n_k$ such that $\sum_{k} n_k = N$): $P_{SP}(X) = \dfrac{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}}   {{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}} = 1$ For the (MH) case: The selection space now is all of the stones, as there is no labelled type to any arbitrary stone. Hence: $P_{MH}(X) = \dfrac{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}}   {{N}\choose{k}}$ The claim is: $\lim_{N\to\infty} \dfrac{P_{SP}(X)}{P_{MH}(X)} = k$ Now, one can use Stirling's Approximation leading to: $\lim_{N\to\infty} \dfrac{P_{SP}(X)}{P_{MH}(X)} = \lim_{N\to\infty} \dfrac{{N}\choose{k}}{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}} = \lim_{N\to\infty} \dfrac{\dfrac{N!}{k!(N-k)!}}{n_{1}*n_{2}*...*n_{k}}$ How should I proceed now?! Update: I myself solved the question: Let's assume that all frequencies are the same (for the sake of simplicity): $n_{1} = n_{2} = ... n_{k} = \psi$, therefore, $k\psi = N \Rightarrow \psi = \dfrac{N}{k}$ $\lim_{N\to\infty} \dfrac{{N}\choose{k}}{n_{1}*n_{2}*...*n_{k}} = \lim_{N\to\infty} \dfrac{{N}\choose{k}}{(\dfrac{N}{k})^{k}}$ Now by the following approximation, we have: ${{N}\choose{k}} \approx \dfrac{N^k}{k!}$ if $N\gg k$ $\lim_{N\to\infty} \dfrac{{N}\choose{k}}{(\dfrac{N}{k})^{k}} = \lim_{N\to\infty} \dfrac{\dfrac{N^k}{k!}}{\dfrac{N^k}{k^k}} = \dfrac{k^k}{k!}$ Let $\phi = \dfrac{k^k}{k!}$ We can use the following approximation here: $\log(k!) \approx k\log k - k$, So, $\log \phi = \log k^{k} - \log k! = k\log k - [k\log k -k] = k$ Finally, $\phi = exp(k)$ I should have reached to $k$, not $exp(k)$. What's wrong with my argument?!","I'm trying to verify the claim presented in this question . Let's assume that we are interested in the following probability in both cases (i.e. sorted piles(SP) and messy heap(MH)): The probability of choosing $k$ stones from $N$ stones with which the tool maker can make $k$ different tools. For the (SP) case: The probability is actually one. Because we have distinct piles of each stone's type. So, one can pick one stone from each pile (Assume we have $k$ types of stone with the frequency of $n_k$ such that $\sum_{k} n_k = N$): $P_{SP}(X) = \dfrac{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}}   {{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}} = 1$ For the (MH) case: The selection space now is all of the stones, as there is no labelled type to any arbitrary stone. Hence: $P_{MH}(X) = \dfrac{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}}   {{N}\choose{k}}$ The claim is: $\lim_{N\to\infty} \dfrac{P_{SP}(X)}{P_{MH}(X)} = k$ Now, one can use Stirling's Approximation leading to: $\lim_{N\to\infty} \dfrac{P_{SP}(X)}{P_{MH}(X)} = \lim_{N\to\infty} \dfrac{{N}\choose{k}}{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}} = \lim_{N\to\infty} \dfrac{\dfrac{N!}{k!(N-k)!}}{n_{1}*n_{2}*...*n_{k}}$ How should I proceed now?! Update: I myself solved the question: Let's assume that all frequencies are the same (for the sake of simplicity): $n_{1} = n_{2} = ... n_{k} = \psi$, therefore, $k\psi = N \Rightarrow \psi = \dfrac{N}{k}$ $\lim_{N\to\infty} \dfrac{{N}\choose{k}}{n_{1}*n_{2}*...*n_{k}} = \lim_{N\to\infty} \dfrac{{N}\choose{k}}{(\dfrac{N}{k})^{k}}$ Now by the following approximation, we have: ${{N}\choose{k}} \approx \dfrac{N^k}{k!}$ if $N\gg k$ $\lim_{N\to\infty} \dfrac{{N}\choose{k}}{(\dfrac{N}{k})^{k}} = \lim_{N\to\infty} \dfrac{\dfrac{N^k}{k!}}{\dfrac{N^k}{k^k}} = \dfrac{k^k}{k!}$ Let $\phi = \dfrac{k^k}{k!}$ We can use the following approximation here: $\log(k!) \approx k\log k - k$, So, $\log \phi = \log k^{k} - \log k! = k\log k - [k\log k -k] = k$ Finally, $\phi = exp(k)$ I should have reached to $k$, not $exp(k)$. What's wrong with my argument?!",,['probability']
10,"Probability of drawing the same color marble twice in a bag of $10$ red, $10$ orange, $10$ green.","Probability of drawing the same color marble twice in a bag of  red,  orange,  green.",10 10 10,"The question is worded as follows. A bag of marbles contains $10$ red marbles, $10$ orange marbles, and $10$ green marbles. You randomly take one marble out of the bag for yourself, and without replacement you then randomly take another marble out of the bag to give to your roommate. What is the probability that your roommate has the same color of marble as you do?"" The answer says that it is $10/30 \times 9/29 = 0.103$. I understand that this question is testing your understanding of ""dependent events,"" where the occurrence of one event affects the likelihood of another event in the sequence. What I don't understand is why it isn't simply $9/29$. I'll explain my thinking, and hopefully someone can point to where I'm wrong. On the first draw, there are $30$ total possible marbles that can be matched by your roommate.  Draw $1$: P{drawing a matchable marble} = $30/30$ On the second draw, there are $9$ marbles of the same color remaining out of 29.  Draw $2$: P{drawing the same color} = $9/29$. $$30/30 \times 9/29 = 9/29. $$ It seems to me that the answer to this problem matches the question: ""what is the probability you both draw a green marble?"" However, my professor told me that wasn't the case, and that the answer is correct. Can someone help me? Thanks :) EDIT Turns out the problem was incorrect, the answer actually is 9/29! Thank you guys for your help :)","The question is worded as follows. A bag of marbles contains $10$ red marbles, $10$ orange marbles, and $10$ green marbles. You randomly take one marble out of the bag for yourself, and without replacement you then randomly take another marble out of the bag to give to your roommate. What is the probability that your roommate has the same color of marble as you do?"" The answer says that it is $10/30 \times 9/29 = 0.103$. I understand that this question is testing your understanding of ""dependent events,"" where the occurrence of one event affects the likelihood of another event in the sequence. What I don't understand is why it isn't simply $9/29$. I'll explain my thinking, and hopefully someone can point to where I'm wrong. On the first draw, there are $30$ total possible marbles that can be matched by your roommate.  Draw $1$: P{drawing a matchable marble} = $30/30$ On the second draw, there are $9$ marbles of the same color remaining out of 29.  Draw $2$: P{drawing the same color} = $9/29$. $$30/30 \times 9/29 = 9/29. $$ It seems to me that the answer to this problem matches the question: ""what is the probability you both draw a green marble?"" However, my professor told me that wasn't the case, and that the answer is correct. Can someone help me? Thanks :) EDIT Turns out the problem was incorrect, the answer actually is 9/29! Thank you guys for your help :)",,['probability']
11,Find CDF of random variable which depends on other variable,Find CDF of random variable which depends on other variable,,"Let $X$ be a random variable with uniform distribution on $[-1, 1]$. Find the CDF of random variable Y given by the following formula: $Y = \left\{\begin{matrix}  -\frac{1}{2},& X < - \frac{1}{2}\\   X,& -\frac{1}{2} \leq X \leq \frac{1}{4}\\   \frac{1}{4}, & X > \frac{1}{4} \end{matrix}\right.$ So I've found PDF and CDF of $X$: $f_X(x) = \begin{cases}  \frac{1}{2}, & x \in [-1, 1]\\   0, & \text{otherwise} \end{cases}$ $F_X(a) = \int_{-\infty}^{a} f_X(x) dx = \left\{\begin{matrix}  0, & a \leq -1\\   \frac{a+1}{2}, & a \in (-1, 1) \\   1, & a \geq 1  \end{matrix}\right.$ I tried to find Y's CDF by: $F_Y(a) = P(Y \leq a)$ $= P(-\frac{1}{2} \leq a, X < - \frac{1}{2}) + P(X \leq a, - \frac{1}{2} \leq X \leq \frac{1}{4}) + P(\frac{1}{4} \leq a, X > -\frac{1}{4})$ But what should I do next? I'm finding such CDF for the first time and my notes say I need to consider a few different cases, but I have no clue what they should look like and how to do it. Any tips would be helpful.","Let $X$ be a random variable with uniform distribution on $[-1, 1]$. Find the CDF of random variable Y given by the following formula: $Y = \left\{\begin{matrix}  -\frac{1}{2},& X < - \frac{1}{2}\\   X,& -\frac{1}{2} \leq X \leq \frac{1}{4}\\   \frac{1}{4}, & X > \frac{1}{4} \end{matrix}\right.$ So I've found PDF and CDF of $X$: $f_X(x) = \begin{cases}  \frac{1}{2}, & x \in [-1, 1]\\   0, & \text{otherwise} \end{cases}$ $F_X(a) = \int_{-\infty}^{a} f_X(x) dx = \left\{\begin{matrix}  0, & a \leq -1\\   \frac{a+1}{2}, & a \in (-1, 1) \\   1, & a \geq 1  \end{matrix}\right.$ I tried to find Y's CDF by: $F_Y(a) = P(Y \leq a)$ $= P(-\frac{1}{2} \leq a, X < - \frac{1}{2}) + P(X \leq a, - \frac{1}{2} \leq X \leq \frac{1}{4}) + P(\frac{1}{4} \leq a, X > -\frac{1}{4})$ But what should I do next? I'm finding such CDF for the first time and my notes say I need to consider a few different cases, but I have no clue what they should look like and how to do it. Any tips would be helpful.",,"['probability', 'probability-theory', 'probability-distributions', 'random-variables']"
12,Probability of getting off at the bus stop,Probability of getting off at the bus stop,,"6 stops on the line, and a car with 4 passengers. Assume they are equally likely to get off at any stop. What is the probability that A $2$ passengers get off stop 2, and $2$ get off stop 4? B $2$ get off stop 1, and $2$ get of at another stop (but they get off at the same stop as each other?) A I get $1/81$ which is wrong? B I get $5/6^4$ which is also wrong? Why are both of these wrong?","6 stops on the line, and a car with 4 passengers. Assume they are equally likely to get off at any stop. What is the probability that A $2$ passengers get off stop 2, and $2$ get off stop 4? B $2$ get off stop 1, and $2$ get of at another stop (but they get off at the same stop as each other?) A I get $1/81$ which is wrong? B I get $5/6^4$ which is also wrong? Why are both of these wrong?",,"['probability', 'combinatorics']"
13,Proof that conditional probabilities sum up to one,Proof that conditional probabilities sum up to one,,"I've read on Wikipedia that the sum (or integral, for continuous $Y$) of $P(Y=y|X=x)$ over $y$ is always equal to one. I've attempted a proof of this statement for the discrete (sum) case: Proof: By the Kolmogorov definition of conditional probability and the Law of Total Probability, $$\sum_k P(A_k |  B) = \sum_k \frac{P(A_k \cap B)}{P(B)} = \frac{1}{P(B)}\sum_kP(A_k \cap B) = \frac{1}{P(B)}P(B)=1.\ \square$$ Is this a correct proof? How would I proof the above statement for the continuous case, i.e. that $\int_y P(Y=y|X=x)$ always equals one?","I've read on Wikipedia that the sum (or integral, for continuous $Y$) of $P(Y=y|X=x)$ over $y$ is always equal to one. I've attempted a proof of this statement for the discrete (sum) case: Proof: By the Kolmogorov definition of conditional probability and the Law of Total Probability, $$\sum_k P(A_k |  B) = \sum_k \frac{P(A_k \cap B)}{P(B)} = \frac{1}{P(B)}\sum_kP(A_k \cap B) = \frac{1}{P(B)}P(B)=1.\ \square$$ Is this a correct proof? How would I proof the above statement for the continuous case, i.e. that $\int_y P(Y=y|X=x)$ always equals one?",,['probability']
14,Is a mixture of two uniform distributions more complex than a single distribution?,Is a mixture of two uniform distributions more complex than a single distribution?,,"I'm a psychologist studying perception of visual ensembles (e.g., lots of lines with different orientations drawn on a screen) that have different underlying probability distributions. One of the reviewer's for our paper has asked us to justify a statement that seems intuitively correct to me but I wasn't able to find a proper reference. The statement in question says that a mixture of two uniform distributions is more complex than a normal or a uniform one. The mixture distribution here consists of two non-intersecting uniform distributions with equal ranges but different means. Intuitively it seems to me that it should be more complex as its probability density function has more parameters than the functions of a single uniform or a normal distribution hence it could be said that it has lower ""description length"". Am I correct in saying that this mixture distribution is more complex? And if so, could you please provide any reference supporting this?","I'm a psychologist studying perception of visual ensembles (e.g., lots of lines with different orientations drawn on a screen) that have different underlying probability distributions. One of the reviewer's for our paper has asked us to justify a statement that seems intuitively correct to me but I wasn't able to find a proper reference. The statement in question says that a mixture of two uniform distributions is more complex than a normal or a uniform one. The mixture distribution here consists of two non-intersecting uniform distributions with equal ranges but different means. Intuitively it seems to me that it should be more complex as its probability density function has more parameters than the functions of a single uniform or a normal distribution hence it could be said that it has lower ""description length"". Am I correct in saying that this mixture distribution is more complex? And if so, could you please provide any reference supporting this?",,"['probability', 'probability-distributions']"
15,Maximum Likelihood and Density Function,Maximum Likelihood and Density Function,,"Likelihood function $$\mathcal{L}(\theta;\vec{x})=\prod_i f(x_i;\theta)=\prod_i\frac{dP_\theta}{dm}$$ where $\frac{dP_\theta}{dm}$ is the Radon-Nikodym Derivative w.r.t. Lebesgue measure in continuous case For a continuous random variable, the probability of it takes on any value is zero. But in a statistical setting, for example maximum likelihood or EM algorithm, we plug in the observed values in order to maximize the probability. Is there a mathematically rigours definition of likelihood function or maximum likelihood estimate? Is the likelihood function the same as the joint density function of independent samples? Do we see semicolon "";"" as a sign of conditional probability?","Likelihood function $$\mathcal{L}(\theta;\vec{x})=\prod_i f(x_i;\theta)=\prod_i\frac{dP_\theta}{dm}$$ where $\frac{dP_\theta}{dm}$ is the Radon-Nikodym Derivative w.r.t. Lebesgue measure in continuous case For a continuous random variable, the probability of it takes on any value is zero. But in a statistical setting, for example maximum likelihood or EM algorithm, we plug in the observed values in order to maximize the probability. Is there a mathematically rigours definition of likelihood function or maximum likelihood estimate? Is the likelihood function the same as the joint density function of independent samples? Do we see semicolon "";"" as a sign of conditional probability?",,"['probability', 'functional-analysis', 'measure-theory', 'statistics', 'maximum-likelihood']"
16,Infimum of probability and probability of infimum,Infimum of probability and probability of infimum,,I am studying the Borel Cantelli proof and there is the following step: $$\Pr\left( \bigcap \limits_{N=1}^{\infty} \bigcup\limits_{n=N}^{\infty}E_n\right) \le \inf_{N\ge1} \Pr\left( \bigcup\limits_{n=N}^{\infty} E_n\right)$$ What happened here? I guess that: $$\Pr\left(\bigcap \limits_{N=1}^{\infty}\bigcup\limits_{n=N}^{\infty} E_n\right) = \Pr\left(\inf_{N\ge1}\bigcup\limits_{n=N}^{\infty} E_n\right) \le \inf_{N\ge1} \Pr\left( \bigcup\limits_{n=N}^{\infty} E_n\right)$$ But why is this true?,I am studying the Borel Cantelli proof and there is the following step: $$\Pr\left( \bigcap \limits_{N=1}^{\infty} \bigcup\limits_{n=N}^{\infty}E_n\right) \le \inf_{N\ge1} \Pr\left( \bigcup\limits_{n=N}^{\infty} E_n\right)$$ What happened here? I guess that: $$\Pr\left(\bigcap \limits_{N=1}^{\infty}\bigcup\limits_{n=N}^{\infty} E_n\right) = \Pr\left(\inf_{N\ge1}\bigcup\limits_{n=N}^{\infty} E_n\right) \le \inf_{N\ge1} \Pr\left( \bigcup\limits_{n=N}^{\infty} E_n\right)$$ But why is this true?,,"['probability', 'elementary-set-theory', 'supremum-and-infimum', 'borel-cantelli-lemmas']"
17,Probability an 6 sided die will be higher than a 8 sided die?,Probability an 6 sided die will be higher than a 8 sided die?,,"Say one person rolls an 8 sided die and the other rolls a six, what is the probability that the six sided die is higher than the 8? I know that the expected value of the eight is 4.5 and the six is 3.5 but am having trouble figuring out how to find the probability. EDIT: Answer is 15/48 but still curious if there's a way of doing this without creating a grid.","Say one person rolls an 8 sided die and the other rolls a six, what is the probability that the six sided die is higher than the 8? I know that the expected value of the eight is 4.5 and the six is 3.5 but am having trouble figuring out how to find the probability. EDIT: Answer is 15/48 but still curious if there's a way of doing this without creating a grid.",,"['probability', 'dice']"
18,Conditional expectation when minimum is given.,Conditional expectation when minimum is given.,,"I try to solve this: Let $X,Y$ be two independent exponential r.v. with parameters $\mu,\lambda>0$. > Let $T:=\min(X,Y)$ Compute $\mathbb{E}(T\vert X)$ Now there is a hint to compute $\mathbb{E}(Tf(X))$ for some measurable function $f:\mathbb{R}\to \mathbb{R}$ but what confuses me, is that $\min(x,y)$ has two components and is not from $\mathbb{R}\to \mathbb{R}$. So I tried to rewrite $$\mathbb{E}(Tf(X))=\mathbb{E}(\mathbb{E}(Tf(X)\vert X))=\mathbb{E}(f(X)\mathbb{E}(T\vert X))$$ but I don't see whether this is useful or not. My second attempt was to work with the density. I computed the CDF for $T$ which is $F(t)=1-e^{-t(\lambda + \mu)}$ and the PDF $$f(t)=(\lambda + \mu)e^{-t(\lambda + \mu)}$$ But I'm not sure, if this is useful here.","I try to solve this: Let $X,Y$ be two independent exponential r.v. with parameters $\mu,\lambda>0$. > Let $T:=\min(X,Y)$ Compute $\mathbb{E}(T\vert X)$ Now there is a hint to compute $\mathbb{E}(Tf(X))$ for some measurable function $f:\mathbb{R}\to \mathbb{R}$ but what confuses me, is that $\min(x,y)$ has two components and is not from $\mathbb{R}\to \mathbb{R}$. So I tried to rewrite $$\mathbb{E}(Tf(X))=\mathbb{E}(\mathbb{E}(Tf(X)\vert X))=\mathbb{E}(f(X)\mathbb{E}(T\vert X))$$ but I don't see whether this is useful or not. My second attempt was to work with the density. I computed the CDF for $T$ which is $F(t)=1-e^{-t(\lambda + \mu)}$ and the PDF $$f(t)=(\lambda + \mu)e^{-t(\lambda + \mu)}$$ But I'm not sure, if this is useful here.",,['probability']
19,Partial marginalization of conditional probability,Partial marginalization of conditional probability,,"I was reading about marginalization on Wikipedia , specifically I read: $$p_X(x) = \int_y p_{X\mid Y}(x\mid y)p_Y(y)\,dy$$ I was wondering if the following is true $$\int_y p_{X\mid YZ}(x\mid y,z)p_Y(y) \, dy = p_{X\mid Z}(x\mid z)$$ $X, Y$ and $Z$ are random variables, with pdf-s $p_X$ , p $_Y$ and $p_Z$ respectively.","I was reading about marginalization on Wikipedia , specifically I read: I was wondering if the following is true and are random variables, with pdf-s , p and respectively.","p_X(x) = \int_y p_{X\mid Y}(x\mid y)p_Y(y)\,dy \int_y p_{X\mid YZ}(x\mid y,z)p_Y(y) \, dy = p_{X\mid Z}(x\mid z) X, Y Z p_X _Y p_Z",['probability']
20,"Randomly putting things in buckets, computing overflow probability (Basic probability question)","Randomly putting things in buckets, computing overflow probability (Basic probability question)",,"Setting: Assume I have $n$ buckets, where each can hold $w$ balls and initially there is one ball in each bucket. I do the following. I keep taking one ball from a random bucket and I put it into another random bucket. I do this $l$ times. I want to figure out with what probability one of the buckets will overflow, i.e. what the probability is that at some point I try to put more than $w$ balls into some bucket. Furthermore, I would like to know how large the buckets need to be s.t. the probability of such an overflow is below a threshold. Question: I have trouble deciding what the correct approach to this problem is and it's somehow not clear to me how, generally, to decide what approach to take. There are several methods that seem somewhat unrelated, yet useful for this problem. Approaches: For instance, I could look at an arbitrary, but fixed bucket and try to use the binomial formula to compute the probability that I will put $k$ out of $l$ balls into a specific bucket. However, I'm not sure how to consider the fact that I don't just put balls into buckets, but also remove them randomly. Also, I'm not sure whether looking at one bucket is really the same as looking at all of them. Another, to me seemingly totally independent, approach would be to compute the expected value for the number of balls in a bucket (which should be 1) and then use the Chernoff bound to bound the probability that a specific bucket will overflow. Here, I'm not sure which Chernoff bound to use. There seem to be several very similar formulas, but I don't really understand the differences. Would I get some probability bound that depends on the number of iterations of my experiment, i.e. how often I randomly relocate a ball into some bucket?","Setting: Assume I have $n$ buckets, where each can hold $w$ balls and initially there is one ball in each bucket. I do the following. I keep taking one ball from a random bucket and I put it into another random bucket. I do this $l$ times. I want to figure out with what probability one of the buckets will overflow, i.e. what the probability is that at some point I try to put more than $w$ balls into some bucket. Furthermore, I would like to know how large the buckets need to be s.t. the probability of such an overflow is below a threshold. Question: I have trouble deciding what the correct approach to this problem is and it's somehow not clear to me how, generally, to decide what approach to take. There are several methods that seem somewhat unrelated, yet useful for this problem. Approaches: For instance, I could look at an arbitrary, but fixed bucket and try to use the binomial formula to compute the probability that I will put $k$ out of $l$ balls into a specific bucket. However, I'm not sure how to consider the fact that I don't just put balls into buckets, but also remove them randomly. Also, I'm not sure whether looking at one bucket is really the same as looking at all of them. Another, to me seemingly totally independent, approach would be to compute the expected value for the number of balls in a bucket (which should be 1) and then use the Chernoff bound to bound the probability that a specific bucket will overflow. Here, I'm not sure which Chernoff bound to use. There seem to be several very similar formulas, but I don't really understand the differences. Would I get some probability bound that depends on the number of iterations of my experiment, i.e. how often I randomly relocate a ball into some bucket?",,"['probability', 'balls-in-bins']"
21,Does absolutely continuity of probability measures imply absolute continuity of conditional probability measures almost everywhere?,Does absolutely continuity of probability measures imply absolute continuity of conditional probability measures almost everywhere?,,"I have a Polish space $X$ equipped with a Borel probability measure $\mu$. I have another Polish space $Y$ and a continuous function $f : X \rightarrow Y$. I give $Y$ the pushforward probability measure $\nu(\cdot) = \mu\left(f^{-1}(\cdot)\right)$. Now suppose I have another Borel probability measure $\mu'$ on $X$ such that $\mu \ll \mu'$ and $\mu\left(f^{-1}(\cdot)\right) = \mu'\left(f^{-1}(\cdot)\right)$. Is it true that $\mu_y \ll \mu_y'$ $\nu$-almost everywhere? Where $\mu_y$ and $\mu_y'$ are the conditional probability measures of $\mu$ and $\mu'$. So far I have tried the following : We know $\mu(A) > 0  \Rightarrow \mu'(A) > 0$ for all measurable set $A \subset X$. Let $Y_A := \{ y \in Y \; | \; \mu_y'(A) > 0\} \cup \{ y \in Y \; | \; \mu_y(A) = 0 \}$. Pick $A \subset X$ such that $\mu(A) > 0$, then $\mu'(A) = \int_{Y} \mu'_y(A) d\nu > 0$ (by the disintegration theorem) so we know $\nu(Y_A) = 1$. I was then able to show \begin{align*} \nu\left(\bigcap_{A \subset X\text{ with non-emptry interior }} Y_A \right) = 1. \end{align*} but I am still short of showing \begin{align*} \nu\left(\bigcap_{A\subset X \text{ measurable}} Y_A \right) = 1. \end{align*} I have solved it for $Y$ countable, but it would be great to have the general case.","I have a Polish space $X$ equipped with a Borel probability measure $\mu$. I have another Polish space $Y$ and a continuous function $f : X \rightarrow Y$. I give $Y$ the pushforward probability measure $\nu(\cdot) = \mu\left(f^{-1}(\cdot)\right)$. Now suppose I have another Borel probability measure $\mu'$ on $X$ such that $\mu \ll \mu'$ and $\mu\left(f^{-1}(\cdot)\right) = \mu'\left(f^{-1}(\cdot)\right)$. Is it true that $\mu_y \ll \mu_y'$ $\nu$-almost everywhere? Where $\mu_y$ and $\mu_y'$ are the conditional probability measures of $\mu$ and $\mu'$. So far I have tried the following : We know $\mu(A) > 0  \Rightarrow \mu'(A) > 0$ for all measurable set $A \subset X$. Let $Y_A := \{ y \in Y \; | \; \mu_y'(A) > 0\} \cup \{ y \in Y \; | \; \mu_y(A) = 0 \}$. Pick $A \subset X$ such that $\mu(A) > 0$, then $\mu'(A) = \int_{Y} \mu'_y(A) d\nu > 0$ (by the disintegration theorem) so we know $\nu(Y_A) = 1$. I was then able to show \begin{align*} \nu\left(\bigcap_{A \subset X\text{ with non-emptry interior }} Y_A \right) = 1. \end{align*} but I am still short of showing \begin{align*} \nu\left(\bigcap_{A\subset X \text{ measurable}} Y_A \right) = 1. \end{align*} I have solved it for $Y$ countable, but it would be great to have the general case.",,"['probability', 'measure-theory', 'metric-spaces']"
22,Does finite variance imply on a finite mean?,Does finite variance imply on a finite mean?,,"Assume that a random variable has a finite variance. Does it mean it also has a finite mean? My approach: I think since it has a finite variance, it means it is in $\mathcal{L}_2$ space of the probability measure $\mu$. Therefore, it should be in $\mathcal{L}_1$ space of the probability measure $\mu$ and thus the mean is finite. IS that correct?","Assume that a random variable has a finite variance. Does it mean it also has a finite mean? My approach: I think since it has a finite variance, it means it is in $\mathcal{L}_2$ space of the probability measure $\mu$. Therefore, it should be in $\mathcal{L}_1$ space of the probability measure $\mu$ and thus the mean is finite. IS that correct?",,"['probability', 'probability-theory', 'probability-distributions', 'stochastic-processes', 'stochastic-calculus']"
23,"Three length 3 ""gapped"" straights vs. one length 6 straight. Which is more likely and by how much?","Three length 3 ""gapped"" straights vs. one length 6 straight. Which is more likely and by how much?",,"Using  a well shuffled standard $52$ card deck, $2$ players (call them A and B) decide to play a game.  They draw community (shared) cards (without replacement) until a winner for that hand is declared or they run out of cards to draw, whichever comes first.  For A to win, there has to be three length $3$ ascending straights drawn in order such as $6,7,8$ ... $2,3,4$... $10,J,Q$.  The $3$ straights have to all be ""gapped"" by at least $1$ rank so something like $7,8,9$ ... $10,J,Q$ ... $2,3,4$ is not a winner for A because of the $9$ and $10$ not being gapped.  The rank $2$ card is considered the lowest card and the $A$ card is considered the highest ranked card so $A,2,3$ is NOT considered a straight.  For B to win, there has to be a straight of length $6$ such as $7,8,9,10,J,Q$. So who has the higher probability to win and by how much? My initial simulation had an error in it so I fixed and am rerunning it. An example of a tie would be  $2,3,4,6,7,8,7,8,9,10,J,Q$  The Q gives A and B the last card needed at the same time so that is a tie (no win for either). I am now trying to do some mathematical analysis on paper starting with simple enumeration of the different ways for A to win.  I see $10$ patterns: $~1$) $2,3,4,6,7,8,10,J,Q$ $~2$) $2,3,4,6,7,8,J,Q,K$ $~3$) $2,3,4,6,7,8,Q,K,A$ $~4$) $2,3,4,7,8,9,J,Q,K$ $~5$) $2,3,4,7,8,9,Q,K,A$ $~6$) $2,3,4,8,9,10,Q,K,A$ $~7$) $3,4,5,7,8,9,J,Q,K$ $~8$) $3,4,5,7,8,9,Q,K,A$ $~9$) $3,4,5,8,9,10,Q,K,A$ $10$) $4,5,6,8,9,10,Q,K,A$ For player B to win, there are only $8$ ways: $1)~ 2,3,4,5,6,7$ $2)~ 3,4,5,6,7,8$ $3)~ 4,5,6,7,8,9$ $4)~ 5,6,7,8,9,10$ $5)~ 6,7,8,9,10,J$ $6)~ 7,8,9,10,J,Q$ $7)~ 8,9,10,J,Q,K$ $8)~ 9,10,J,Q,K,A$ What I will do is create a bucket for each possible  pattern (way) to win shown above and count them up to make sure the program is catching all of them.  I think they should all be equally likely. What if we started by just doing a ""straight"" analysis of the probability or A winning ignoring any cases where B wins first?  What if we also do the same thing and compute the chances of B winning ignoring cases where A wins first.  I wonder how far off those will be from the simulated results. Would this analysis be correct for computing the chance of B winning if there was no player A? Chance of choosing good 1st card is $32/52$, good 2nd card is $4/51$, good 3rd card is $4/50$, good 4th card is $4/49$, good 5th card is $4/48$, and good 6th card is $4/47$.  This is about $1$ in $447,000$ but that is only counting an immediate win for B with only $6$ cards drawn.  B can use the full deck if need be to win so how do I compute that using math?  In other words, if the only player is B how many hands on average will be required for a B win?","Using  a well shuffled standard $52$ card deck, $2$ players (call them A and B) decide to play a game.  They draw community (shared) cards (without replacement) until a winner for that hand is declared or they run out of cards to draw, whichever comes first.  For A to win, there has to be three length $3$ ascending straights drawn in order such as $6,7,8$ ... $2,3,4$... $10,J,Q$.  The $3$ straights have to all be ""gapped"" by at least $1$ rank so something like $7,8,9$ ... $10,J,Q$ ... $2,3,4$ is not a winner for A because of the $9$ and $10$ not being gapped.  The rank $2$ card is considered the lowest card and the $A$ card is considered the highest ranked card so $A,2,3$ is NOT considered a straight.  For B to win, there has to be a straight of length $6$ such as $7,8,9,10,J,Q$. So who has the higher probability to win and by how much? My initial simulation had an error in it so I fixed and am rerunning it. An example of a tie would be  $2,3,4,6,7,8,7,8,9,10,J,Q$  The Q gives A and B the last card needed at the same time so that is a tie (no win for either). I am now trying to do some mathematical analysis on paper starting with simple enumeration of the different ways for A to win.  I see $10$ patterns: $~1$) $2,3,4,6,7,8,10,J,Q$ $~2$) $2,3,4,6,7,8,J,Q,K$ $~3$) $2,3,4,6,7,8,Q,K,A$ $~4$) $2,3,4,7,8,9,J,Q,K$ $~5$) $2,3,4,7,8,9,Q,K,A$ $~6$) $2,3,4,8,9,10,Q,K,A$ $~7$) $3,4,5,7,8,9,J,Q,K$ $~8$) $3,4,5,7,8,9,Q,K,A$ $~9$) $3,4,5,8,9,10,Q,K,A$ $10$) $4,5,6,8,9,10,Q,K,A$ For player B to win, there are only $8$ ways: $1)~ 2,3,4,5,6,7$ $2)~ 3,4,5,6,7,8$ $3)~ 4,5,6,7,8,9$ $4)~ 5,6,7,8,9,10$ $5)~ 6,7,8,9,10,J$ $6)~ 7,8,9,10,J,Q$ $7)~ 8,9,10,J,Q,K$ $8)~ 9,10,J,Q,K,A$ What I will do is create a bucket for each possible  pattern (way) to win shown above and count them up to make sure the program is catching all of them.  I think they should all be equally likely. What if we started by just doing a ""straight"" analysis of the probability or A winning ignoring any cases where B wins first?  What if we also do the same thing and compute the chances of B winning ignoring cases where A wins first.  I wonder how far off those will be from the simulated results. Would this analysis be correct for computing the chance of B winning if there was no player A? Chance of choosing good 1st card is $32/52$, good 2nd card is $4/51$, good 3rd card is $4/50$, good 4th card is $4/49$, good 5th card is $4/48$, and good 6th card is $4/47$.  This is about $1$ in $447,000$ but that is only counting an immediate win for B with only $6$ cards drawn.  B can use the full deck if need be to win so how do I compute that using math?  In other words, if the only player is B how many hands on average will be required for a B win?",,"['probability', 'card-games']"
24,Proof that $\mathbb{E} X^k = 0$ for all odd $k$ implies $X$ symmetric for bounded $X$ without characteristic functions,Proof that  for all odd  implies  symmetric for bounded  without characteristic functions,\mathbb{E} X^k = 0 k X X,"I'm working through the exercises in Terry Tao's Topics in Random Matrix Theory , and came across: Let $X$ be a bounded real random variable.  Show that $X$ is symmetric if and only if $\mathbb{E}X^k = 0 $ for all positive odd integers $k$. Clearly, if $X$ is symmetric, then $\mathbb{E} X^k = 0$ for all positive odd integers $k$; for the reverse direction, I can show this using Fourier analysis/characteristic functions: Since $X$ is bounded, we know by DCT that $$ \mathbb{E} e^{it X} = \sum\limits_{k = 0}^\infty \frac{(it)^k \mathbb{E} X^k}{k!} = \sum\limits_{j = 0}^\infty \frac{(-1)^jt^{2j} \mathbb{E} X^{2j}}{(2j)!}$$ where we also used that the odd moments are zero.  This is real for real $t$, which then implies that $$\mathbb{E} e^{-itX} = \overline{\mathbb{E} e^{itX}} = \mathbb{E} e^{itX}.$$ Since $X$ and $-X$ have the same characteristic function, they must also have the same distribution. Tao states afterwards that ""it is also instructive to find a 'real-variable' proof that avoids the use of this function,"" but I'm unable to find it.  Intuitively, it seems like there must be some sort of exponential moment, because you need to capture the data for all of the moments at the same time.  I'm not sure how to do that without going directly to the characteristic function.  Any help is appreciated.","I'm working through the exercises in Terry Tao's Topics in Random Matrix Theory , and came across: Let $X$ be a bounded real random variable.  Show that $X$ is symmetric if and only if $\mathbb{E}X^k = 0 $ for all positive odd integers $k$. Clearly, if $X$ is symmetric, then $\mathbb{E} X^k = 0$ for all positive odd integers $k$; for the reverse direction, I can show this using Fourier analysis/characteristic functions: Since $X$ is bounded, we know by DCT that $$ \mathbb{E} e^{it X} = \sum\limits_{k = 0}^\infty \frac{(it)^k \mathbb{E} X^k}{k!} = \sum\limits_{j = 0}^\infty \frac{(-1)^jt^{2j} \mathbb{E} X^{2j}}{(2j)!}$$ where we also used that the odd moments are zero.  This is real for real $t$, which then implies that $$\mathbb{E} e^{-itX} = \overline{\mathbb{E} e^{itX}} = \mathbb{E} e^{itX}.$$ Since $X$ and $-X$ have the same characteristic function, they must also have the same distribution. Tao states afterwards that ""it is also instructive to find a 'real-variable' proof that avoids the use of this function,"" but I'm unable to find it.  Intuitively, it seems like there must be some sort of exponential moment, because you need to capture the data for all of the moments at the same time.  I'm not sure how to do that without going directly to the characteristic function.  Any help is appreciated.",,"['probability', 'probability-theory']"
25,Probabilities ant cube,Probabilities ant cube,,"I have attached a picture of the cube in the question. An ant moves along the edges of the cube always starting at $A$ and never repeating an edge. This defines a trail of edges. For example, $ABFE$ and $ABCDAE$ are trails, but $ABCB$ is not a trail. The number of edges in a trail is known as its length. At each vertex, the ant must proceed along one of the edges that has not yet been traced, if there is one. If there is a choice of untraced edges, the following probabilities for taking each of them apply. If only one edge at a vertex has been traced and that edge is vertical, then the probability of the ant taking each horizontal edge is $\frac12$. If only one edge at a vertex has been traced and that edge is horizontal, then the probability of the ant taking the vertical edge is $\frac23$ and the probability of the ant taking the horizontal edge is $\frac13$. If no edge at a vertex has been traced, then the probability of the ant taking the vertical edge is $\frac23$ and the probability of the ant taking each of the horizontal edges is $\frac16$. In your solutions to the following problems use exact fractions not decimals. a) If the ant moves from $A$ to $D$, what is the probability it will then move to $H$? If the ant moves from $A$ to $E$, what is the probability it will then move to $H$? My answer: $A$ to $D$ then to $H = \dfrac23$ $A$ to $E$ then to $H = \dfrac12$ b) What is the probability the ant takes the trail $ABCG$? My answer: Multiply the probabilities: $$\frac16\times\frac13\times\frac23 = \frac1{27}$$ c) Find two trails of length $3$ from $A$ to $G$ that have probabilities of being traced by the ant that are different to each other and to the probability for the trail $ABCG$. My answer: $$\begin{align} ABFG&=\frac16\times\frac23\times\frac12=\frac1{18}\\[5pt] AEHG&=\frac23\times\frac12\times\frac13=\frac19 \end{align}$$ d) What is the probability that the ant will trace a trail of length $3$ from $A$ to $G$? I don't know how to do d). Do I just multiply every single probability? Also, could you please check to see if I have done the a) to c) correctly? I am not completely sure if this is the correct application of the multiplicative principle.","I have attached a picture of the cube in the question. An ant moves along the edges of the cube always starting at $A$ and never repeating an edge. This defines a trail of edges. For example, $ABFE$ and $ABCDAE$ are trails, but $ABCB$ is not a trail. The number of edges in a trail is known as its length. At each vertex, the ant must proceed along one of the edges that has not yet been traced, if there is one. If there is a choice of untraced edges, the following probabilities for taking each of them apply. If only one edge at a vertex has been traced and that edge is vertical, then the probability of the ant taking each horizontal edge is $\frac12$. If only one edge at a vertex has been traced and that edge is horizontal, then the probability of the ant taking the vertical edge is $\frac23$ and the probability of the ant taking the horizontal edge is $\frac13$. If no edge at a vertex has been traced, then the probability of the ant taking the vertical edge is $\frac23$ and the probability of the ant taking each of the horizontal edges is $\frac16$. In your solutions to the following problems use exact fractions not decimals. a) If the ant moves from $A$ to $D$, what is the probability it will then move to $H$? If the ant moves from $A$ to $E$, what is the probability it will then move to $H$? My answer: $A$ to $D$ then to $H = \dfrac23$ $A$ to $E$ then to $H = \dfrac12$ b) What is the probability the ant takes the trail $ABCG$? My answer: Multiply the probabilities: $$\frac16\times\frac13\times\frac23 = \frac1{27}$$ c) Find two trails of length $3$ from $A$ to $G$ that have probabilities of being traced by the ant that are different to each other and to the probability for the trail $ABCG$. My answer: $$\begin{align} ABFG&=\frac16\times\frac23\times\frac12=\frac1{18}\\[5pt] AEHG&=\frac23\times\frac12\times\frac13=\frac19 \end{align}$$ d) What is the probability that the ant will trace a trail of length $3$ from $A$ to $G$? I don't know how to do d). Do I just multiply every single probability? Also, could you please check to see if I have done the a) to c) correctly? I am not completely sure if this is the correct application of the multiplicative principle.",,"['probability', 'word-problem']"
26,The probability of rolling 4 dice and getting a 6.,The probability of rolling 4 dice and getting a 6.,,"The probability of rolling 2 dice and getting a 6 on either one of the die or both is   : 11/36 or about  0.305. Also I calculate the probability of rolling 4 dice and getting a 6 on either one, two, three or all four dice is :  421/1296 or about 0.32. Is that correct?  I am just surprised to find both the probabilities so close together. Thank you.","The probability of rolling 2 dice and getting a 6 on either one of the die or both is   : 11/36 or about  0.305. Also I calculate the probability of rolling 4 dice and getting a 6 on either one, two, three or all four dice is :  421/1296 or about 0.32. Is that correct?  I am just surprised to find both the probabilities so close together. Thank you.",,"['probability', 'dice']"
27,"Choose 3 points A, B and C in a circle O","Choose 3 points A, B and C in a circle O",,"I have to get $p$,$q$ and $r$. $p$ = the probability of triangle $ABC$ is an acute-angled triangle $q$ = the probability of triangle $ABC$ is a right-angled triangle $r$ = the probability of triangle $ABC$ is an obtuse-angled triangle. I have an idea that the diameter of O is important in determining the angle. But I cannot get exact $p$,$q$ and $r$. Please help.","I have to get $p$,$q$ and $r$. $p$ = the probability of triangle $ABC$ is an acute-angled triangle $q$ = the probability of triangle $ABC$ is a right-angled triangle $r$ = the probability of triangle $ABC$ is an obtuse-angled triangle. I have an idea that the diameter of O is important in determining the angle. But I cannot get exact $p$,$q$ and $r$. Please help.",,"['probability', 'geometry', 'geometric-probability']"
28,Bridge probability problem,Bridge probability problem,,"From volume 1 of Feller: North and South have 10 trumps between them ( trumps being cards of a specific suit) (a) find the probability that all three remaining trumps are in the same hand (that is, either East or West has no trumps) (b) if it is known that the king of trumps is included among the three, what is the probability that he is ""unguarded"" ( that is, one player has to king, the other the remaining two trumps ) I am stuck with (a).  It seems there is probability $2 * \frac{\binom{52-26-3}{10}}{\binom{52-26}{13}}$ for this scenario but the solution decreases this by $\frac{11}{50}$ which I do not see the origin of.","From volume 1 of Feller: North and South have 10 trumps between them ( trumps being cards of a specific suit) (a) find the probability that all three remaining trumps are in the same hand (that is, either East or West has no trumps) (b) if it is known that the king of trumps is included among the three, what is the probability that he is ""unguarded"" ( that is, one player has to king, the other the remaining two trumps ) I am stuck with (a).  It seems there is probability $2 * \frac{\binom{52-26-3}{10}}{\binom{52-26}{13}}$ for this scenario but the solution decreases this by $\frac{11}{50}$ which I do not see the origin of.",,['probability']
29,Probability with n dice,Probability with n dice,,"I'm studying probability and am currently stuck on this question: Let's say we have n distinct dice, each of which is fair and 6-sided. If all of these dice are rolled, what is the probability that there is at least one pair that sums up to 7? I interpreted the above as being equivalent to the following: 1 - (Probability that there is no pair that sums up to 7) So if I were to consider just one pair of dice, then the probability that the pair adds up to 7 is 1/6, I think? So Pr(one pair doesn't add up to 7) = 5/6. But then I'm stuck on how to proceed. Because there are lots of possible pairs amongst the n die, and some of these pairs overlap...for example, (die1, die2) is a pair, (die1, die3) is a pair, and so on. So I don't know how to account for these overlaps. $$===============================================$$ EDIT: As per John's response below, here is my attempt: Case 1: Probability(all n die show a single number) = $1*(\frac{1}{6})^{n-1}$? Is this right? My thinking is that the first die can show any number (probability = 1), then the second thru last die must show the same number (probability = 1/6) Case 2: Probability(all n die show exactly two numbers that don't add up to 7) = $1*(\frac{4}{6})*(\frac{2}{6})^{n-2}$? My thinking here is that the first die can show any number, the second die must show any of the 4 other numbers such that the first two die won't add up to 7, and then all other die must show either the first or the second die's number. Case 3: Probability(all n die show exactly three numbers that don't add up to 7) = $1 * \frac{4}{6} * \frac{1}{6} * (\frac{3}{6})^{n-3}$? And then do we just add all of the 3 cases together, then subtract from 1? I might be way off here...","I'm studying probability and am currently stuck on this question: Let's say we have n distinct dice, each of which is fair and 6-sided. If all of these dice are rolled, what is the probability that there is at least one pair that sums up to 7? I interpreted the above as being equivalent to the following: 1 - (Probability that there is no pair that sums up to 7) So if I were to consider just one pair of dice, then the probability that the pair adds up to 7 is 1/6, I think? So Pr(one pair doesn't add up to 7) = 5/6. But then I'm stuck on how to proceed. Because there are lots of possible pairs amongst the n die, and some of these pairs overlap...for example, (die1, die2) is a pair, (die1, die3) is a pair, and so on. So I don't know how to account for these overlaps. $$===============================================$$ EDIT: As per John's response below, here is my attempt: Case 1: Probability(all n die show a single number) = $1*(\frac{1}{6})^{n-1}$? Is this right? My thinking is that the first die can show any number (probability = 1), then the second thru last die must show the same number (probability = 1/6) Case 2: Probability(all n die show exactly two numbers that don't add up to 7) = $1*(\frac{4}{6})*(\frac{2}{6})^{n-2}$? My thinking here is that the first die can show any number, the second die must show any of the 4 other numbers such that the first two die won't add up to 7, and then all other die must show either the first or the second die's number. Case 3: Probability(all n die show exactly three numbers that don't add up to 7) = $1 * \frac{4}{6} * \frac{1}{6} * (\frac{3}{6})^{n-3}$? And then do we just add all of the 3 cases together, then subtract from 1? I might be way off here...",,"['probability', 'combinatorics']"
30,Can the partial sums of independent random variables with no normalization converge in distribution to a constant?,Can the partial sums of independent random variables with no normalization converge in distribution to a constant?,,"If $\{X_n , n\ge 1 \}$ is a sequence of independent random variables and $X_n$ is nondegenerate for at least one $n\ge1$, can there exist a finite constant c such that $S_n = \sum_{j=1}^n X_j \stackrel{d}{\rightarrow}c$ ? (Here we are not dividing $S_n$ by any sequence of normalizing constants.) I am struggling to find an example where this can hold. Could someone please provide an appropriate example, or a proof that no such constant $c$ can exist?","If $\{X_n , n\ge 1 \}$ is a sequence of independent random variables and $X_n$ is nondegenerate for at least one $n\ge1$, can there exist a finite constant c such that $S_n = \sum_{j=1}^n X_j \stackrel{d}{\rightarrow}c$ ? (Here we are not dividing $S_n$ by any sequence of normalizing constants.) I am struggling to find an example where this can hold. Could someone please provide an appropriate example, or a proof that no such constant $c$ can exist?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'probability-limit-theorems']"
31,What is the probability that a five-card poker hand has four ACES?,What is the probability that a five-card poker hand has four ACES?,,"What is the probability that a five-card poker hand has four ACES?   When I was solving the above stated problem, I got confused while trying different methods : Assume a normal $52$ deck of cards. Method 1: Selecting the $4$ aces from total $4$ aces can be done in $\mathsf C(4,4)$ ways and selecting any non ace element from rest $48$ cards can be done by $\mathsf C(48,1)$ ways. Any $5$ cards can be drawn from $52$ deck of card in $\mathsf C(52,5)$ ways. So the probability is  $$\frac{\mathsf C(4,4)\times \mathsf C(48,1)}{\mathsf C(52,5)}$$ Method 2: We have $4$ aces in total. so probability of selecting an ace from $52$ cards is  $4/524$ , then we are left with $51$ cards and selecting again another ace gives probability $3/51$. Similarly for next two aces probability will be $2/50$ and $1/49$. Now we are left with total $48$ cards and we can obviously choose any of these $48$ card which gives probability of $48/48$. Multiplying the probabilities gives us $$\frac{(4\cdot 3\cdot 2\cdot 1\cdot 48)}{(52\cdot 51\cdot 50\cdot 49\cdot 48)}.$$ Method $1$ produces result which is $5$ times more than method $2$. What am I doing wrong ? Help appreciated :)","What is the probability that a five-card poker hand has four ACES?   When I was solving the above stated problem, I got confused while trying different methods : Assume a normal $52$ deck of cards. Method 1: Selecting the $4$ aces from total $4$ aces can be done in $\mathsf C(4,4)$ ways and selecting any non ace element from rest $48$ cards can be done by $\mathsf C(48,1)$ ways. Any $5$ cards can be drawn from $52$ deck of card in $\mathsf C(52,5)$ ways. So the probability is  $$\frac{\mathsf C(4,4)\times \mathsf C(48,1)}{\mathsf C(52,5)}$$ Method 2: We have $4$ aces in total. so probability of selecting an ace from $52$ cards is  $4/524$ , then we are left with $51$ cards and selecting again another ace gives probability $3/51$. Similarly for next two aces probability will be $2/50$ and $1/49$. Now we are left with total $48$ cards and we can obviously choose any of these $48$ card which gives probability of $48/48$. Multiplying the probabilities gives us $$\frac{(4\cdot 3\cdot 2\cdot 1\cdot 48)}{(52\cdot 51\cdot 50\cdot 49\cdot 48)}.$$ Method $1$ produces result which is $5$ times more than method $2$. What am I doing wrong ? Help appreciated :)",,"['probability', 'combinatorics', 'discrete-mathematics', 'permutations', 'combinations']"
32,Expected number of trials before I get one of each type,Expected number of trials before I get one of each type,,"Say I have a population that I sample from without replacement. In this population there are $a_1$ members of type 1, $a_2$ members of type two and so forth up to $a_k$ of type $k$. What is the expected number of trials until I collect at least one member out of each type? An example here would be the expected number of cards drawn from a deck until I get one of every valor.","Say I have a population that I sample from without replacement. In this population there are $a_1$ members of type 1, $a_2$ members of type two and so forth up to $a_k$ of type $k$. What is the expected number of trials until I collect at least one member out of each type? An example here would be the expected number of cards drawn from a deck until I get one of every valor.",,['probability']
33,How to find $z$-score,How to find -score,z,"I have some probabilities, but I have to find the $z$ -score. I am not sure how do to this when I am told I have to use slope-intercept. Where do I plug the numbers in exactly? Here is one of my problems: Find $d^{\prime}$ and locate $X_C$ approximately on drawing of distributions. $$\begin{array}{|l|l|l|} \hline \text{Response} & \text{Stimuli}\\ & N  & S+N  \\ \hline N & 39 & 21 \\ S+N & 30 & 57 \\ \hline \end{array}$$ First step is to convert raw numbers into probabilities $$ \begin{array}{lclcl} p(\text{HIT}) &=& p(Y|S+N) = 57/(57+21) = 57/78 &=& 0.7308 \\ p(\text{FA}) &=& p(Y|N) = 30/(39+30) = 30/69 &=& 0.4347 \end{array} $$ You then need to use the table to convert these values to $z$ -scores. Remember because the table does not have every value, you will need to use a slope intercept approach to calculate this value. Then use the formula to calculate $d'$ . EDIT : Here is the table it is referring to: \begin{array}{|} \hline \text{$\quad\quad$Tabled values of the normal curve}\\ \hline  \text{area}\ 0-t & t\\   0  & 0\\   0.39  & 0.1 \\   0.79 &0.2 \\   0.118  & 0.3\\   0.155  & 0.4\\   0.192 & 0.5\\   0.226  & 0.6\\   0.258  & 0.7\\   0.288  & 0.8\\   0.316 & 0.9\\   0.341  & 1.0\\   0.364  & 1.1\\   0.385  & 1.2\\   0.403  & 1.3\\   0.419  & 1.4\\   0.433  & 1.5\\   0.445 & 1.6\\   0.455  & 1.7\\   0.464 & 1.8\\   0.471  & 1.9\\  0.477  & 2.0\\  0.482  & 2.1\\   0.486  & 2.2\\   0.489  & 2.3\\   0.492  & 2.4\\   0.494  & 2.5\\ \hline \end{array}","I have some probabilities, but I have to find the -score. I am not sure how do to this when I am told I have to use slope-intercept. Where do I plug the numbers in exactly? Here is one of my problems: Find and locate approximately on drawing of distributions. First step is to convert raw numbers into probabilities You then need to use the table to convert these values to -scores. Remember because the table does not have every value, you will need to use a slope intercept approach to calculate this value. Then use the formula to calculate . EDIT : Here is the table it is referring to:","z d^{\prime} X_C \begin{array}{|l|l|l|} \hline \text{Response} & \text{Stimuli}\\ & N
 & S+N  \\ \hline N & 39 & 21 \\ S+N & 30 & 57 \\ \hline \end{array} 
\begin{array}{lclcl}
p(\text{HIT}) &=& p(Y|S+N) = 57/(57+21) = 57/78 &=& 0.7308 \\
p(\text{FA}) &=& p(Y|N) = 30/(39+30) = 30/69 &=& 0.4347
\end{array}
 z d' \begin{array}{|}
\hline
\text{\quad\quadTabled values of the normal curve}\\ \hline
 \text{area}\ 0-t & t\\ 
 0  & 0\\ 
 0.39  & 0.1 \\ 
 0.79 &0.2 \\ 
 0.118  & 0.3\\ 
 0.155  & 0.4\\ 
 0.192 & 0.5\\ 
 0.226  & 0.6\\ 
 0.258  & 0.7\\ 
 0.288  & 0.8\\ 
 0.316 & 0.9\\ 
 0.341  & 1.0\\ 
 0.364  & 1.1\\ 
 0.385  & 1.2\\ 
 0.403  & 1.3\\ 
 0.419  & 1.4\\ 
 0.433  & 1.5\\ 
 0.445 & 1.6\\ 
 0.455  & 1.7\\ 
 0.464 & 1.8\\ 
 0.471  & 1.9\\
 0.477  & 2.0\\
 0.482  & 2.1\\ 
 0.486  & 2.2\\ 
 0.489  & 2.3\\ 
 0.492  & 2.4\\ 
 0.494  & 2.5\\ \hline
\end{array}","['probability', 'statistics']"
34,How do you find the probability of A winning if the probability of getting a favourable outcome in the $r^{th}$ turn is a function of $r$?,How do you find the probability of A winning if the probability of getting a favourable outcome in the  turn is a function of ?,r^{th} r,"Problem: Two players A and B are playing snake and ladder. A is at 99 and he needs 1 to win in rolling of a dice. However, he is always allowed to re-throw the dice if 6 appears. What is the probability that A will win eventually before B gets a chance, if the probability of getting 1 in the $r^{th}$ throw is $\frac{1}{3^r}$ and that of getting 6 in the $r^{th}$ throw is $\frac{2r-1}{4r}$? My attempt: We know that A can win before B gets a chance only if he rolls {$1$},{$6$,$1$},{$6$,$6$,$1$} and so on. In the $r^{th}$ turn, we have the probability: $$\frac{1}{3^r}\cdot\frac{1}{4}\cdot\frac{3}{8}\cdot\cdot\cdot\frac{2r-3}{4(r-1)}$$ $$=\frac{1}{3^r}\cdot\frac{1}{4^{r-1}}\left(\frac{1\cdot3\cdot5\cdot7\cdot\cdot\cdot(2r-3)}{1\cdot2\cdot3\cdot4\cdot\cdot\cdot(r-1)}\right)$$ $$=\frac{1}{3^r}\cdot\frac{1}{4^{r-1}}\left(\frac{(2r-2)!}{(r-1)!\cdot(r-1)!\cdot2^{r-1}}\right)$$ $$=\frac{1}{3}\cdot\frac{1}{24^{r-1}}\left(\frac{(2r-2)!}{(r-1)!\cdot(r-1)!}\right)$$ Therefore, we have the probability as $$\sum_{r=1}^{\infty} \frac{1}{3}\cdot\binom{2r-2}{r-1} \frac{1}{24^{r-1}}$$ Taking $r-1$=$n$ $$\frac{1}{3}\sum_{n=0}^{\infty} \binom{2n}{n} \frac{1}{24^{n}}$$ I got stuck at the last step because I do not know how to evaluate that summation. Any help with the summation/providing an alternate way to solve this question will be appreciated.","Problem: Two players A and B are playing snake and ladder. A is at 99 and he needs 1 to win in rolling of a dice. However, he is always allowed to re-throw the dice if 6 appears. What is the probability that A will win eventually before B gets a chance, if the probability of getting 1 in the $r^{th}$ throw is $\frac{1}{3^r}$ and that of getting 6 in the $r^{th}$ throw is $\frac{2r-1}{4r}$? My attempt: We know that A can win before B gets a chance only if he rolls {$1$},{$6$,$1$},{$6$,$6$,$1$} and so on. In the $r^{th}$ turn, we have the probability: $$\frac{1}{3^r}\cdot\frac{1}{4}\cdot\frac{3}{8}\cdot\cdot\cdot\frac{2r-3}{4(r-1)}$$ $$=\frac{1}{3^r}\cdot\frac{1}{4^{r-1}}\left(\frac{1\cdot3\cdot5\cdot7\cdot\cdot\cdot(2r-3)}{1\cdot2\cdot3\cdot4\cdot\cdot\cdot(r-1)}\right)$$ $$=\frac{1}{3^r}\cdot\frac{1}{4^{r-1}}\left(\frac{(2r-2)!}{(r-1)!\cdot(r-1)!\cdot2^{r-1}}\right)$$ $$=\frac{1}{3}\cdot\frac{1}{24^{r-1}}\left(\frac{(2r-2)!}{(r-1)!\cdot(r-1)!}\right)$$ Therefore, we have the probability as $$\sum_{r=1}^{\infty} \frac{1}{3}\cdot\binom{2r-2}{r-1} \frac{1}{24^{r-1}}$$ Taking $r-1$=$n$ $$\frac{1}{3}\sum_{n=0}^{\infty} \binom{2n}{n} \frac{1}{24^{n}}$$ I got stuck at the last step because I do not know how to evaluate that summation. Any help with the summation/providing an alternate way to solve this question will be appreciated.",,"['probability', 'sequences-and-series', 'algebra-precalculus', 'binomial-coefficients', 'contest-math']"
35,"How to calculate the shortest interval, for $P ( X ≤ 1 . 645) = 0 . 95$?","How to calculate the shortest interval, for ?",P ( X ≤ 1 . 645) = 0 . 95,"The problem statement said: Based on the fact that $\Phi(1 . 645) = 0 . 95$ ﬁnd an interval in which $X$   will fall with $95\%$ probability. Therefore: Since $P ( X ≤ 1 . 645) = 0 . 95, ( -∞ , 1 . 645)$ is a $95\%$ conﬁdence interval for $X$ The question I have problem to understand is: Among all possible intervals into which $X$ falls with $95\%$ probability,   ﬁnd the shortest one. How can I compute or see which is the shortest interval? Thanks!","The problem statement said: Based on the fact that $\Phi(1 . 645) = 0 . 95$ ﬁnd an interval in which $X$   will fall with $95\%$ probability. Therefore: Since $P ( X ≤ 1 . 645) = 0 . 95, ( -∞ , 1 . 645)$ is a $95\%$ conﬁdence interval for $X$ The question I have problem to understand is: Among all possible intervals into which $X$ falls with $95\%$ probability,   ﬁnd the shortest one. How can I compute or see which is the shortest interval? Thanks!",,"['probability', 'probability-theory', 'statistics']"
36,Absolute Continuity of the sum of two Cantor random variables,Absolute Continuity of the sum of two Cantor random variables,,"If we have two independent random variables each having a Cantor distribution is there an easy way to see that the distribution of their sum is not absolutely continuous? I am pretty sure that if we let $S_n$ be the set of  positive integers having an $n$ digit ternary expansion (leading zeros included) with $n/2$ or more 1's, and let $$T_n = \left\{\frac{2s+1}{3^n}:s\in S_n\right\}$$  Then our random variable has more than a 50-50 chance of being within $3^{-n-1}$ of a member of $T_n$.  As the number of intervals grows as $2^n$, and their width shrinks as $3^{-n}$, the measure of the whole thing goes to 0 as $n$ goes to infinity.  (It took some handwaving and arithmetic to get here, so don't trust me.) In the best of all possible worlds, there would be an argument that works for the absolute continuity of the sum of three (or any number) of independent Cantor Random variables.","If we have two independent random variables each having a Cantor distribution is there an easy way to see that the distribution of their sum is not absolutely continuous? I am pretty sure that if we let $S_n$ be the set of  positive integers having an $n$ digit ternary expansion (leading zeros included) with $n/2$ or more 1's, and let $$T_n = \left\{\frac{2s+1}{3^n}:s\in S_n\right\}$$  Then our random variable has more than a 50-50 chance of being within $3^{-n-1}$ of a member of $T_n$.  As the number of intervals grows as $2^n$, and their width shrinks as $3^{-n}$, the measure of the whole thing goes to 0 as $n$ goes to infinity.  (It took some handwaving and arithmetic to get here, so don't trust me.) In the best of all possible worlds, there would be an argument that works for the absolute continuity of the sum of three (or any number) of independent Cantor Random variables.",,"['probability', 'probability-theory', 'measure-theory']"
37,Show that $\limsup_{n\rightarrow\infty} \frac{\sum_{k=1}^{n} X_k}{n}<\infty$ a.s.,Show that  a.s.,\limsup_{n\rightarrow\infty} \frac{\sum_{k=1}^{n} X_k}{n}<\infty,"Let $X_k, k \geq 1$, be i.i.d. random variables such that $\limsup_{n\rightarrow\infty} \frac{X_n}{n}<\infty$ a.s, then show that $\limsup_{n\rightarrow\infty} \frac{\sum_{k=1}^{n} X_k}{n}<\infty$ a.s. I'm thinking to use Borel-Cantelli lemma but don't know where to start. Any hints would be helpful. Thanks.","Let $X_k, k \geq 1$, be i.i.d. random variables such that $\limsup_{n\rightarrow\infty} \frac{X_n}{n}<\infty$ a.s, then show that $\limsup_{n\rightarrow\infty} \frac{\sum_{k=1}^{n} X_k}{n}<\infty$ a.s. I'm thinking to use Borel-Cantelli lemma but don't know where to start. Any hints would be helpful. Thanks.",,"['probability', 'probability-theory', 'borel-cantelli-lemmas']"
38,Confused about fair coins?,Confused about fair coins?,,"Bob and Alice play with a coin. Together they have thrown $30$ heads and $70$ tails. Bob says the coin is not fair. Alice disagrees and proposes to keep tossing. They toss another $100$ times and end Up with $100$ heads and $100$ tails. Bob admits he is surprised but argues about the fairness anyway. He says there are $2^{200}$ possible outcomes and $\binom {200}{100} $ of them are $100$ heads and $100$ tails. So the probability that you are correct in claiming it has a probability of heads over tails between $\frac {99}{200}$ and $\frac {101}{200}$ is very close to $A = 1 - \frac{\binom {200}{100}}{2^{200}}$. Cindy comes along , draws a Pascal triangle and points out that with a fair coin you have a probability of getting 100 heads equal to $A$. Danny joins and claims that - because of what Cindy Said - it works in reverse too , if your probability is $A$ then your coin must be fair. Emmet joins the discussion and says Danny is wrong , because he does not take into account the unfair coins. Who is right , who is wrong ? What is the truth ?  What are the probabilities ? How certain are we that the coin is fair ?","Bob and Alice play with a coin. Together they have thrown $30$ heads and $70$ tails. Bob says the coin is not fair. Alice disagrees and proposes to keep tossing. They toss another $100$ times and end Up with $100$ heads and $100$ tails. Bob admits he is surprised but argues about the fairness anyway. He says there are $2^{200}$ possible outcomes and $\binom {200}{100} $ of them are $100$ heads and $100$ tails. So the probability that you are correct in claiming it has a probability of heads over tails between $\frac {99}{200}$ and $\frac {101}{200}$ is very close to $A = 1 - \frac{\binom {200}{100}}{2^{200}}$. Cindy comes along , draws a Pascal triangle and points out that with a fair coin you have a probability of getting 100 heads equal to $A$. Danny joins and claims that - because of what Cindy Said - it works in reverse too , if your probability is $A$ then your coin must be fair. Emmet joins the discussion and says Danny is wrong , because he does not take into account the unfair coins. Who is right , who is wrong ? What is the truth ?  What are the probabilities ? How certain are we that the coin is fair ?",,"['probability', 'combinatorics']"
39,Find the PDF of X1 +X2 +X3.,Find the PDF of X1 +X2 +X3.,,"The problem said: If X1,X2,X3 are independent random variables that are uniformly   distributed on (0,1), find the PDF of X1 +X2 +X3. The theory I have said: Following the theory and the example for the sum of two random variables, I try to set up the integral, therofore: $f_{X_1}(a)=f_{X_2}(a)=f_{X_3}(a)= 1$ when $0 < a < 1$ and $0$ otherwise ; I belive the region of the integral will be a cube of $1 x 1 x 1$, then: $$P((X_1\leq x_1, X_2\leq x_2, X_3\leq x_3) = \iiint_{\Bbb [0;1]^3} f_{X_1,X_2,X_3}(x_1,x_2,x_3) \;\mathrm d\,x_1 \;\mathrm d\,x_2 \;\mathrm d\,x_3$$ because are independient I have and from the definition of convolution the pdf is: $$f_{X_1+X_2+X_3}(a)= \int_0^1 \int_0^1 f_{X_1}(a-x_2-x_3) f_{X_2}(x_2) f_{X_3}(a-x_2)\;\mathrm d\,x_2 \;\mathrm d\,x_3$$ Well I am stuck in this part, as I have no idea how to calculate the correct PDF of X1,X2,X3 fx1,x2,x3 from here or if all my procerdure is wrong. Thanks","The problem said: If X1,X2,X3 are independent random variables that are uniformly   distributed on (0,1), find the PDF of X1 +X2 +X3. The theory I have said: Following the theory and the example for the sum of two random variables, I try to set up the integral, therofore: $f_{X_1}(a)=f_{X_2}(a)=f_{X_3}(a)= 1$ when $0 < a < 1$ and $0$ otherwise ; I belive the region of the integral will be a cube of $1 x 1 x 1$, then: $$P((X_1\leq x_1, X_2\leq x_2, X_3\leq x_3) = \iiint_{\Bbb [0;1]^3} f_{X_1,X_2,X_3}(x_1,x_2,x_3) \;\mathrm d\,x_1 \;\mathrm d\,x_2 \;\mathrm d\,x_3$$ because are independient I have and from the definition of convolution the pdf is: $$f_{X_1+X_2+X_3}(a)= \int_0^1 \int_0^1 f_{X_1}(a-x_2-x_3) f_{X_2}(x_2) f_{X_3}(a-x_2)\;\mathrm d\,x_2 \;\mathrm d\,x_3$$ Well I am stuck in this part, as I have no idea how to calculate the correct PDF of X1,X2,X3 fx1,x2,x3 from here or if all my procerdure is wrong. Thanks",,"['probability', 'probability-distributions', 'convolution']"
40,Show the following pdf is memoryless,Show the following pdf is memoryless,,I've been thinking about this around 2 weeks for the midterm. but still can't prove it. I used this $$ P(X > r+s | X > s) = P(X > r) = \mathrm e^{−\lambda r}$$ $$P(X > r + s) / P(x >s ) = \mathrm e^{−\lambda(r+s)} / \mathrm e^{−\lambda s}$$ Could you please give me some hint? how can I prove ? Thanks.,I've been thinking about this around 2 weeks for the midterm. but still can't prove it. I used this $$ P(X > r+s | X > s) = P(X > r) = \mathrm e^{−\lambda r}$$ $$P(X > r + s) / P(x >s ) = \mathrm e^{−\lambda(r+s)} / \mathrm e^{−\lambda s}$$ Could you please give me some hint? how can I prove ? Thanks.,,"['probability', 'probability-distributions', 'geometric-probability']"
41,Examples of convergence almost surely,Examples of convergence almost surely,,"I know the definition of almost sure convergence, but I don't understand it in practice....for example, if I have independent random variables $$X_n \sim U (1, 1+ 1/n),$$ does it converge almost surely? Thanks to all!","I know the definition of almost sure convergence, but I don't understand it in practice....for example, if I have independent random variables $$X_n \sim U (1, 1+ 1/n),$$ does it converge almost surely? Thanks to all!",,"['probability', 'statistics']"
42,Markov chains and conditioning on impossible events,Markov chains and conditioning on impossible events,,"Consider a Markov chain $(X_0,X_1,\ldots)$ with a state space $S\equiv\{s_1,s_2\}$ and the following matrix of “transition probabilities” (I will explain the use of quotation marks below): \begin{align*} \begin{array}{c|cc} &s_1&s_2\\ \hline s_1&1&0\\ s_2&1&0 \end{array} \end{align*} That is, no matter what initial state the system starts in, it will always end up in state $s_1$ in one period and stay there forever. Rigorously speaking, these “transition probabilities” are to be interpreted as follows: \begin{align*} \mathbb P\,(X_{n}=s_1\,|\,X_{n-1}=s_1)=&\,1,\\ \mathbb P\,(X_{n}=s_2\,|\,X_{n-1}=s_1)=&\,0,\\ \mathbb P\,(X_{n}=s_1\,|\,X_{n-1}=s_2)=&\,1,\\ \mathbb P\,(X_{n}=s_2\,|\,X_{n-1}=s_2)=&\,0 \end{align*} for each $n\in\mathbb N$. My concern is that the last two probabilities are ill-defined (except possibly for $n=1$), because for any given initial probabilities, the condition events $\{X_{n-1}=s_2\}_{n=2}^{\infty}$ have zero probability! Strictly speaking, therefore, the above matrix cannot be interpreted as conditional probabilities because of the problem of conditioning on events that never occur. What is the standard resolution of this technical problem? Does one make the hand-waving assumption of defining conditional probabilities that depend on impossible events anyway, or is there a more sophisticated and rigorous way around this issue? Any input is appreciated.","Consider a Markov chain $(X_0,X_1,\ldots)$ with a state space $S\equiv\{s_1,s_2\}$ and the following matrix of “transition probabilities” (I will explain the use of quotation marks below): \begin{align*} \begin{array}{c|cc} &s_1&s_2\\ \hline s_1&1&0\\ s_2&1&0 \end{array} \end{align*} That is, no matter what initial state the system starts in, it will always end up in state $s_1$ in one period and stay there forever. Rigorously speaking, these “transition probabilities” are to be interpreted as follows: \begin{align*} \mathbb P\,(X_{n}=s_1\,|\,X_{n-1}=s_1)=&\,1,\\ \mathbb P\,(X_{n}=s_2\,|\,X_{n-1}=s_1)=&\,0,\\ \mathbb P\,(X_{n}=s_1\,|\,X_{n-1}=s_2)=&\,1,\\ \mathbb P\,(X_{n}=s_2\,|\,X_{n-1}=s_2)=&\,0 \end{align*} for each $n\in\mathbb N$. My concern is that the last two probabilities are ill-defined (except possibly for $n=1$), because for any given initial probabilities, the condition events $\{X_{n-1}=s_2\}_{n=2}^{\infty}$ have zero probability! Strictly speaking, therefore, the above matrix cannot be interpreted as conditional probabilities because of the problem of conditioning on events that never occur. What is the standard resolution of this technical problem? Does one make the hand-waving assumption of defining conditional probabilities that depend on impossible events anyway, or is there a more sophisticated and rigorous way around this issue? Any input is appreciated.",,"['probability', 'stochastic-processes', 'markov-chains']"
43,Counting the expected number of strings with a given contiguous substring.,Counting the expected number of strings with a given contiguous substring.,,"Question: Let a random bit string $x$ of length $n$ be given. What is the expected number of bit strings $w$ of length $2n$ that contain $x$ as a contiguous substring? What I know: For any bit string $x$ of length $n$, the total number of length $2n$ bit strings $w$ that contain $x$ as a contiguous substring is between $2^n$ and $(n+1) \cdot 2^n$.  The exact number may depend on the string $x$ that you pick. To the point: Is the expected number $\Theta(2^n)$, or $\Theta(n \cdot 2^n)$, or something in between?","Question: Let a random bit string $x$ of length $n$ be given. What is the expected number of bit strings $w$ of length $2n$ that contain $x$ as a contiguous substring? What I know: For any bit string $x$ of length $n$, the total number of length $2n$ bit strings $w$ that contain $x$ as a contiguous substring is between $2^n$ and $(n+1) \cdot 2^n$.  The exact number may depend on the string $x$ that you pick. To the point: Is the expected number $\Theta(2^n)$, or $\Theta(n \cdot 2^n)$, or something in between?",,"['probability', 'combinatorics', 'discrete-mathematics', 'computer-science', 'combinatorics-on-words']"
44,Probability of the card following first ace being ace of spades or two of clubs,Probability of the card following first ace being ace of spades or two of clubs,,"I am learning probability from Scheldon Ross' book . The question reads like this: A deck of 52 playing cards is shuffled, and the cards are turned up one at a time until the first ace appears. Is the next card – that is, the card following the first ace – more likely to be the ace of spades or the two of clubs? Solution given is: Each ordering of the 52 cards can be obtained by first ordering the 51 cards different from the ace of spades and then inserting the ace of spades into that ordering. Furthermore, for each of the 51! orderings of the other cards, there is only one place where the ace of spades can be placed so that it follows the first ace. Thus, $$P{(\text{the ace of spades follows the first ace})} = \frac{51!}{52!}=\frac{1}{52}$$ Similarly, the probability that the two of clubs (or any other card) follows the first ace is also $\frac{1}{52}$ . which sounds reasonable. However the author carefully notes the confusion usually made while solving this problem. I got his point, but somehow the explanation given to eliminate the confusion is still confusing me. The author notes below: Many people’s common reaction is to suppose initially that it is more likely that the two of clubs follows the first ace, since that first ace might itself be the ace of spades. This reaction is often followed by the realization that the two of clubs might itself appear before the first ace, thus negating its chance of immediately following the first ace. Upto this I got it, but did not get how the probability of first ace itself is ace of spade and probability that the two of clubs comes before the first ace are same, as author explains: However, as there is one chance in four that the ace of spades will be the first ace (because all 4 aces are equally likely to be first) and only one chance in five that the two of clubs will appear before the first ace (because each of the set of 5 cards consisting of the two of clubs and the 4 aces is equally likely to be the first of this set to appear), it again appears that the two of clubs is more likely. However, this is not the case as explained above. I did not get this: only one chance in five that the two of clubs will appear before the first ace (because each of the set of 5 cards consisting of the two of clubs and the 4 aces is equally likely to be the first of this set to appear) What I think is we have to prove: $$P(\text{first ace itself is an ace of spade}) = P(\text{two of clubs appear before first ace})$$ How authors' explanation proves this. And if it does not, then how it can be proven?","I am learning probability from Scheldon Ross' book . The question reads like this: A deck of 52 playing cards is shuffled, and the cards are turned up one at a time until the first ace appears. Is the next card – that is, the card following the first ace – more likely to be the ace of spades or the two of clubs? Solution given is: Each ordering of the 52 cards can be obtained by first ordering the 51 cards different from the ace of spades and then inserting the ace of spades into that ordering. Furthermore, for each of the 51! orderings of the other cards, there is only one place where the ace of spades can be placed so that it follows the first ace. Thus, Similarly, the probability that the two of clubs (or any other card) follows the first ace is also . which sounds reasonable. However the author carefully notes the confusion usually made while solving this problem. I got his point, but somehow the explanation given to eliminate the confusion is still confusing me. The author notes below: Many people’s common reaction is to suppose initially that it is more likely that the two of clubs follows the first ace, since that first ace might itself be the ace of spades. This reaction is often followed by the realization that the two of clubs might itself appear before the first ace, thus negating its chance of immediately following the first ace. Upto this I got it, but did not get how the probability of first ace itself is ace of spade and probability that the two of clubs comes before the first ace are same, as author explains: However, as there is one chance in four that the ace of spades will be the first ace (because all 4 aces are equally likely to be first) and only one chance in five that the two of clubs will appear before the first ace (because each of the set of 5 cards consisting of the two of clubs and the 4 aces is equally likely to be the first of this set to appear), it again appears that the two of clubs is more likely. However, this is not the case as explained above. I did not get this: only one chance in five that the two of clubs will appear before the first ace (because each of the set of 5 cards consisting of the two of clubs and the 4 aces is equally likely to be the first of this set to appear) What I think is we have to prove: How authors' explanation proves this. And if it does not, then how it can be proven?",P{(\text{the ace of spades follows the first ace})} = \frac{51!}{52!}=\frac{1}{52} \frac{1}{52} P(\text{first ace itself is an ace of spade}) = P(\text{two of clubs appear before first ace}),"['probability', 'combinatorics']"
45,Brownian motion: Strong Markov versus translation invariance,Brownian motion: Strong Markov versus translation invariance,,"In the proof of the reflection principle in Durrett's textbook ( Probability: Theory and Examples (4e) , Theorem 8.4.1, page 317), there's a step which I'm a little shaky on. Basically, this proof invokes the strong Markov property so to set this up in Durrett's notation, let $B_s$ denote Brownian motion and define (I think for fixed $t$ here, but correct me if I'm wrong): $$ Y_s(\omega) =  \begin{cases} 1, & \textrm{if } s<t,\, \omega(t-s)>a\\ 0, & \textrm{otherwise} \end{cases} $$ Then defining the stopping time $S = \inf\{s<t: B_s=a\}$ (and $\inf \emptyset = \infty$), we get: $$ Y_S(\theta_S \omega) =  \begin{cases} 1, & \textrm{if } S<t,\, B_t>a\\ 0, & \textrm{otherwise} \end{cases} $$ where $\theta_S$ is the usual random shift operator for elements of $\mathcal{C}[0,\infty)$. So far, so good. This took a while to wrap my head around but it makes sense. Applying the strong Markov property we get $$ E_0(Y_S \circ \theta_S | \mathcal{F}_S) = E_{B_S} Y_S \textrm{ on } \{S < \infty\}$$ Now taking expectations, Durrett gets: $$ P_0(T_a < t, B_t \geq a) = E_0(Y_S \circ \theta_S; S < \infty) $$ I'm not seeing the right hand side. I'm trying to see how $$ E_0 (E_{B_S} Y_S; S < \infty) = E_0(Y_S \circ \theta_S; S<\infty) $$ but I'm getting confused. Does this somehow involve translation invarance of $B_s$ or is it straight from the definition of $Y_S$ and I'm just missing something obvious? EDIT : Just putting @saz's solution here to clear up exactly where the strong Markov property is used. Essentially integrating $Y_S(\theta_S \omega)$ on $\{S < \infty\}$ and writing it as: $$E_0(Y_S \circ \theta_S; S < \infty) = E_0(E_0(Y_S \circ \theta_S | \mathcal{F}_S); S < \infty)$$ It becomes obvious where to apply the Markov property.","In the proof of the reflection principle in Durrett's textbook ( Probability: Theory and Examples (4e) , Theorem 8.4.1, page 317), there's a step which I'm a little shaky on. Basically, this proof invokes the strong Markov property so to set this up in Durrett's notation, let $B_s$ denote Brownian motion and define (I think for fixed $t$ here, but correct me if I'm wrong): $$ Y_s(\omega) =  \begin{cases} 1, & \textrm{if } s<t,\, \omega(t-s)>a\\ 0, & \textrm{otherwise} \end{cases} $$ Then defining the stopping time $S = \inf\{s<t: B_s=a\}$ (and $\inf \emptyset = \infty$), we get: $$ Y_S(\theta_S \omega) =  \begin{cases} 1, & \textrm{if } S<t,\, B_t>a\\ 0, & \textrm{otherwise} \end{cases} $$ where $\theta_S$ is the usual random shift operator for elements of $\mathcal{C}[0,\infty)$. So far, so good. This took a while to wrap my head around but it makes sense. Applying the strong Markov property we get $$ E_0(Y_S \circ \theta_S | \mathcal{F}_S) = E_{B_S} Y_S \textrm{ on } \{S < \infty\}$$ Now taking expectations, Durrett gets: $$ P_0(T_a < t, B_t \geq a) = E_0(Y_S \circ \theta_S; S < \infty) $$ I'm not seeing the right hand side. I'm trying to see how $$ E_0 (E_{B_S} Y_S; S < \infty) = E_0(Y_S \circ \theta_S; S<\infty) $$ but I'm getting confused. Does this somehow involve translation invarance of $B_s$ or is it straight from the definition of $Y_S$ and I'm just missing something obvious? EDIT : Just putting @saz's solution here to clear up exactly where the strong Markov property is used. Essentially integrating $Y_S(\theta_S \omega)$ on $\{S < \infty\}$ and writing it as: $$E_0(Y_S \circ \theta_S; S < \infty) = E_0(E_0(Y_S \circ \theta_S | \mathcal{F}_S); S < \infty)$$ It becomes obvious where to apply the Markov property.",,"['probability', 'stochastic-processes', 'brownian-motion']"
46,How to calculate entropy from a set of samples?,How to calculate entropy from a set of samples?,,"entropy (information content) is defined as: $$ H(X) = \sum_{i} {\mathrm{P}(x_i)\,\mathrm{I}(x_i)} = -\sum_{i} {\mathrm{P}(x_i) \log_b \mathrm{P}(x_i)} $$ This allows to calculate the entropy of a random variable given its probability distribution. But, what if I have a set of scalar samples and I want to calculate their entropy? In this case the probability density function is not available, but maybe there is a formula to get an approximation (as in the sample mean)? Does it have a name?","entropy (information content) is defined as: $$ H(X) = \sum_{i} {\mathrm{P}(x_i)\,\mathrm{I}(x_i)} = -\sum_{i} {\mathrm{P}(x_i) \log_b \mathrm{P}(x_i)} $$ This allows to calculate the entropy of a random variable given its probability distribution. But, what if I have a set of scalar samples and I want to calculate their entropy? In this case the probability density function is not available, but maybe there is a formula to get an approximation (as in the sample mean)? Does it have a name?",,"['probability', 'approximation', 'information-theory', 'estimation', 'entropy']"
47,Why binomial distribution doesn't count permutations?,Why binomial distribution doesn't count permutations?,,"Why in Binomial distribution the formula starts with $n\choose k$ and not with something like $k!\over n!$? Isn't the order important? Or, it is important but due to the independence of the event?","Why in Binomial distribution the formula starts with $n\choose k$ and not with something like $k!\over n!$? Isn't the order important? Or, it is important but due to the independence of the event?",,['probability']
48,How many words can be formed using letters such that first 2 letters are...,How many words can be formed using letters such that first 2 letters are...,,We make an assumption that any combination of letters is a word and we should take repetition into account because that would mean the same word. How many words can be formed using all the letters in the word EXAMINATION in such a way that the first 2 are different consonants and the last 2 are different vowels. Okay so here's my approach. There are 4 different consonants and 6 vowels (with 2 Identical I's and A's) So ${{4} \choose {2}}(2!)$ would be the consonants and ${6 \choose 2}(2!) $would be the vowels (still including repetition for now) Now I'd take these two and times by 7! All over 2!2!2! To account for repetition and get  226800. Is this correct?,We make an assumption that any combination of letters is a word and we should take repetition into account because that would mean the same word. How many words can be formed using all the letters in the word EXAMINATION in such a way that the first 2 are different consonants and the last 2 are different vowels. Okay so here's my approach. There are 4 different consonants and 6 vowels (with 2 Identical I's and A's) So ${{4} \choose {2}}(2!)$ would be the consonants and ${6 \choose 2}(2!) $would be the vowels (still including repetition for now) Now I'd take these two and times by 7! All over 2!2!2! To account for repetition and get  226800. Is this correct?,,"['probability', 'combinatorics', 'statistics', 'solution-verification']"
49,Average distance between 2 points on surface of sphere?,Average distance between 2 points on surface of sphere?,,How can I find an average distance between two points lying on surface of a sphere of a certain radius? More importantly : can knowing the average distance between two points on surface of a disk ( this question has already an answer on MSE) be useful to answer the question about average distance between two points on surface of sphere? Or there are no immediate obvious relationship/generalization between the two question?,How can I find an average distance between two points lying on surface of a sphere of a certain radius? More importantly : can knowing the average distance between two points on surface of a disk ( this question has already an answer on MSE) be useful to answer the question about average distance between two points on surface of sphere? Or there are no immediate obvious relationship/generalization between the two question?,,['probability']
50,Show that $E[X_t^2]<\infty$,Show that,E[X_t^2]<\infty,"Show that $E[X_t^2]<\infty$, where $$ X_t=e^{3W_t-\frac{3t}{2}}-3e^{W_t-\frac{t}{2}}\underbrace{\int_0^te^{2W_s-s}ds}_{A_t},\quad. t\geq0, $$ where $t$ is a fixed number and $W_t$ is Brownian motion. What I did was: By Itō's formula: $X_t=\int_0^t\left(3e^{3W_s-\frac{3s}{2}}-3e^{W_s-\frac{s}{2}}A_s\right)dW_s$ Itō isometry: \begin{align*} E[X_t^2]&=E\left[\left(\int_0^t\left(3e^{3W_s-\frac{3s}{2}}-3e^{W_s-\frac{s}{2}}A_s\right)dW_s\right)^2\right]\\[1.ex] &=E\left[\int_0^t\left(3e^{3W_s-\frac{3s}{2}}-3e^{W_s-\frac{s}{2}}A_s\right)^2ds\right]\\[1.ex] &= ...\\[1.ex] &= \int_0^t\left(9e^{-3s}\underbrace{E\left[e^{6W_s}\right]}_{=e^{18s}}-18e^{-2s}\underbrace{E\left[e^{4W_s}A_s\right]}_{(1)}+9e^{-s}\underbrace{E\left[e^{2W_s}A_s^2\right]}_{(2)}\right)ds \end{align*} My main problem is that I don't know how to make an estimation of $(1)$ and $(2)$. I've tried using Cauchy-Schwarz, but it does not help me. Any help would be appreciated. Thanks!","Show that $E[X_t^2]<\infty$, where $$ X_t=e^{3W_t-\frac{3t}{2}}-3e^{W_t-\frac{t}{2}}\underbrace{\int_0^te^{2W_s-s}ds}_{A_t},\quad. t\geq0, $$ where $t$ is a fixed number and $W_t$ is Brownian motion. What I did was: By Itō's formula: $X_t=\int_0^t\left(3e^{3W_s-\frac{3s}{2}}-3e^{W_s-\frac{s}{2}}A_s\right)dW_s$ Itō isometry: \begin{align*} E[X_t^2]&=E\left[\left(\int_0^t\left(3e^{3W_s-\frac{3s}{2}}-3e^{W_s-\frac{s}{2}}A_s\right)dW_s\right)^2\right]\\[1.ex] &=E\left[\int_0^t\left(3e^{3W_s-\frac{3s}{2}}-3e^{W_s-\frac{s}{2}}A_s\right)^2ds\right]\\[1.ex] &= ...\\[1.ex] &= \int_0^t\left(9e^{-3s}\underbrace{E\left[e^{6W_s}\right]}_{=e^{18s}}-18e^{-2s}\underbrace{E\left[e^{4W_s}A_s\right]}_{(1)}+9e^{-s}\underbrace{E\left[e^{2W_s}A_s^2\right]}_{(2)}\right)ds \end{align*} My main problem is that I don't know how to make an estimation of $(1)$ and $(2)$. I've tried using Cauchy-Schwarz, but it does not help me. Any help would be appreciated. Thanks!",,"['probability', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion', 'stochastic-integrals']"
51,What is the expected number of questions answered to complete a sequence in which wrong answers send you to the start?,What is the expected number of questions answered to complete a sequence in which wrong answers send you to the start?,,"Given a sequence of n questions that each contain x answer choices, what is the expected number of questions answered before answering all questions correctly if answering a question incorrectly sends you to the beginning of the sequence? The questions do not change, so once a question has been answered correctly, guessing does not have to occur again. All choices are made by guessing between whichever answer choices remain. In other words, once a wrong answer has been chosen, it cannot be picked again later in the sequence, and once a right answer has been found, it will always be chosen when visiting that question. For example, for n equals 2 and x equals 2 the expected value is $E_{2,2} = \frac{7}{2}$: Get both questions right on the first try: $(\frac{1}{2})^{2}(2)$ Get the first question wrong once, then the second question right: $(\frac{1}{2})^{2}(3)$ Get the first question right, then the second question wrong, then right: $(\frac{1}{2})^{2}(4)$ Get the first wrong, then right, then the second wrong, then right: $(\frac{1}{2})^{2}(5)$ I've been unable to derive a closed form expression for any single value of x (other than the trivial case of x = 1), much less a closed form expression in terms of both n and x. I've tried to start with a recurrence relation such as $E_{n+1,x} = E_{n,x} + (\frac{1}{x})^{n+1}S_{n,x} + E_{1,x}$ where $S_{n,x}$ is the number of times a reset occurs in the tree diagram that represents $E_{n,x}$. However, I haven't been able to take this any further. For the sake of clarity, $E_{3,1} = 2$: Get the first question right: $\frac{1}{3}(1)$ Get the first question wrong then right: $(\frac{2}{3})(\frac{1}{2})(2)$ Get the first question wrong twice then right: $(\frac{2}{3})(\frac{1}{2})(3)$ I thought it was a fairly interesting problem that I haven't been able to find anywhere else. Could anyone provide some insight into deriving a closed form expression for this problem? Or at least how I might develop a recursive solution?","Given a sequence of n questions that each contain x answer choices, what is the expected number of questions answered before answering all questions correctly if answering a question incorrectly sends you to the beginning of the sequence? The questions do not change, so once a question has been answered correctly, guessing does not have to occur again. All choices are made by guessing between whichever answer choices remain. In other words, once a wrong answer has been chosen, it cannot be picked again later in the sequence, and once a right answer has been found, it will always be chosen when visiting that question. For example, for n equals 2 and x equals 2 the expected value is $E_{2,2} = \frac{7}{2}$: Get both questions right on the first try: $(\frac{1}{2})^{2}(2)$ Get the first question wrong once, then the second question right: $(\frac{1}{2})^{2}(3)$ Get the first question right, then the second question wrong, then right: $(\frac{1}{2})^{2}(4)$ Get the first wrong, then right, then the second wrong, then right: $(\frac{1}{2})^{2}(5)$ I've been unable to derive a closed form expression for any single value of x (other than the trivial case of x = 1), much less a closed form expression in terms of both n and x. I've tried to start with a recurrence relation such as $E_{n+1,x} = E_{n,x} + (\frac{1}{x})^{n+1}S_{n,x} + E_{1,x}$ where $S_{n,x}$ is the number of times a reset occurs in the tree diagram that represents $E_{n,x}$. However, I haven't been able to take this any further. For the sake of clarity, $E_{3,1} = 2$: Get the first question right: $\frac{1}{3}(1)$ Get the first question wrong then right: $(\frac{2}{3})(\frac{1}{2})(2)$ Get the first question wrong twice then right: $(\frac{2}{3})(\frac{1}{2})(3)$ I thought it was a fairly interesting problem that I haven't been able to find anywhere else. Could anyone provide some insight into deriving a closed form expression for this problem? Or at least how I might develop a recursive solution?",,"['probability', 'recurrence-relations', 'closed-form']"
52,Probability of winning a rigged coin-flipping game,Probability of winning a rigged coin-flipping game,,Betsy and Katie are playing a game with an unfair coin. The coin is rigged to come up heads with probability $\frac35$ and tails with probability $\frac25$. Betsy goes first. The two take turns. The first player to flip a tail wins. What is Betsy's probability of winning? What confuses me is that the game can continue for an unlimited amount of times (at least until Betsy flips a tail). How would I solve this?,Betsy and Katie are playing a game with an unfair coin. The coin is rigged to come up heads with probability $\frac35$ and tails with probability $\frac25$. Betsy goes first. The two take turns. The first player to flip a tail wins. What is Betsy's probability of winning? What confuses me is that the game can continue for an unlimited amount of times (at least until Betsy flips a tail). How would I solve this?,,"['probability', 'combinatorics']"
53,chi-square test for uniform distribution,chi-square test for uniform distribution,,"So, i have a hash function which maps a set of possible inputs to a defined range of outputs. I want to test if the mapped outputs are uniformly distributed over the defined range. Wikipedia seems to suggest that the chi-square test can be used to measure the uniformity of the result - how would i go about doing this?","So, i have a hash function which maps a set of possible inputs to a defined range of outputs. I want to test if the mapped outputs are uniformly distributed over the defined range. Wikipedia seems to suggest that the chi-square test can be used to measure the uniformity of the result - how would i go about doing this?",,"['probability', 'statistics']"
54,"If the probability density on a random vector is symmetric, then each variable is identically distributed?","If the probability density on a random vector is symmetric, then each variable is identically distributed?",,"Let $X$ be a random vector with joint distribution $F$ and density $f$. If $f$ is symmetric, is this equivalent to each random variable being identically distributed? We say $f$ is symmetric if it is invariant to a permutation in its arguments. For $X$ a vector of length $n$ of discrete random variables (say integer valued just for the sake of it) this gives $\Pr(X_i=x)=\sum_{Y\in\mathbb{Z}^{n-1}} f(X_i=x,X_{-i}=Y)$. But with $f$ symmetric, whichever $i$ we choose should be arbitrary, so $\Pr(X_i=x)=\Pr(X_j=x)$ for any choice of $x,i,j$. Is this correct? Does this change at all for continuous random variables? I would just replace the sum with an integral?","Let $X$ be a random vector with joint distribution $F$ and density $f$. If $f$ is symmetric, is this equivalent to each random variable being identically distributed? We say $f$ is symmetric if it is invariant to a permutation in its arguments. For $X$ a vector of length $n$ of discrete random variables (say integer valued just for the sake of it) this gives $\Pr(X_i=x)=\sum_{Y\in\mathbb{Z}^{n-1}} f(X_i=x,X_{-i}=Y)$. But with $f$ symmetric, whichever $i$ we choose should be arbitrary, so $\Pr(X_i=x)=\Pr(X_j=x)$ for any choice of $x,i,j$. Is this correct? Does this change at all for continuous random variables? I would just replace the sum with an integral?",,['probability']
55,Las Vegas algorithm to satisfy most clauses in SAT,Las Vegas algorithm to satisfy most clauses in SAT,,"Consider an instance of SAT with $m$ clauses, where every clause has exactly $k$ literals. Give a Las Vegas algorithm (i.e., an algorithm that always gives the correct result) that finds an assignment satisfying at least $m(1-2^{-k})$ clauses, and analyze its expected running time. A possible Las Vegas algorithm is to randomly assign true/false to every variable, each with probability $1/2$. If the assignment satisfies less than $m(1-2^{-k})$ clauses, rerandomize all variables. Keep doing this until we get an assignment satisfying at least $m(1-2^{-k})$ clauses. This is a valid Las Vegas algorithm, but I'm not sure it's one that would be expected by the question. Also, to analyze the expected running time, we would need to compute the probability that a random assignment works. This means the random assignment satisfies at least $m(1-2^{-k})$ clauses, and it doesn't seem easy to compute. What would be a better Las Vegas algorithm?","Consider an instance of SAT with $m$ clauses, where every clause has exactly $k$ literals. Give a Las Vegas algorithm (i.e., an algorithm that always gives the correct result) that finds an assignment satisfying at least $m(1-2^{-k})$ clauses, and analyze its expected running time. A possible Las Vegas algorithm is to randomly assign true/false to every variable, each with probability $1/2$. If the assignment satisfies less than $m(1-2^{-k})$ clauses, rerandomize all variables. Keep doing this until we get an assignment satisfying at least $m(1-2^{-k})$ clauses. This is a valid Las Vegas algorithm, but I'm not sure it's one that would be expected by the question. Also, to analyze the expected running time, we would need to compute the probability that a random assignment works. This means the random assignment satisfies at least $m(1-2^{-k})$ clauses, and it doesn't seem easy to compute. What would be a better Las Vegas algorithm?",,"['probability', 'computer-science', 'satisfiability']"
56,Convergence in Total Variation Implies Convergence in Distribution,Convergence in Total Variation Implies Convergence in Distribution,,"Suppose $X,Y$ are random variables. We define the total variation distance of random variables to be $d(X,Y)= \inf \{P(|X′−Y′|>0): X′,Y′$ are couplings of $ X,Y$ respectively$\}$. Does convergence in total variation imply convergence in distribution? My feeling is that it should but I'm having trouble proving it. Thanks!","Suppose $X,Y$ are random variables. We define the total variation distance of random variables to be $d(X,Y)= \inf \{P(|X′−Y′|>0): X′,Y′$ are couplings of $ X,Y$ respectively$\}$. Does convergence in total variation imply convergence in distribution? My feeling is that it should but I'm having trouble proving it. Thanks!",,"['probability', 'measure-theory']"
57,What is the probability of being selected?,What is the probability of being selected?,,"I don't know if I have enough data to calculate (or even estimate) this because I don't understand statistics, distributions, etc. 550 people took a test to try to gain entry to a school.  There were 2 tests: one in English and one in math.  The outcome was as follows, in raw numbers (i.e., not percentages): Mean score in English = 100 Mean score in math = 94 The scores in both subjects were added together to give the candidate's overall score. Of the 550 people, 221 scored more than the pass mark of 196. The range of scores from all 550 candidates, for each test was 69 to 141. Although 221 passed the test, only 150 places are available in the school, so 71 people are going to be out of luck. Assuming a normal distribution of scores, and the top 150 people are offered a place, is it possible to determine whether a particular candidate is likely to be in the top 150 if his score is known?  For example, a score of 220. I don't have values for variance or SD, so I'm not expecting an answer unless some math genius out there can work it out from the limited data I have given.","I don't know if I have enough data to calculate (or even estimate) this because I don't understand statistics, distributions, etc. 550 people took a test to try to gain entry to a school.  There were 2 tests: one in English and one in math.  The outcome was as follows, in raw numbers (i.e., not percentages): Mean score in English = 100 Mean score in math = 94 The scores in both subjects were added together to give the candidate's overall score. Of the 550 people, 221 scored more than the pass mark of 196. The range of scores from all 550 candidates, for each test was 69 to 141. Although 221 passed the test, only 150 places are available in the school, so 71 people are going to be out of luck. Assuming a normal distribution of scores, and the top 150 people are offered a place, is it possible to determine whether a particular candidate is likely to be in the top 150 if his score is known?  For example, a score of 220. I don't have values for variance or SD, so I'm not expecting an answer unless some math genius out there can work it out from the limited data I have given.",,['probability']
58,"What does it mean for a theorem to be ""almost surely true"", in a probabilistic sense? (Note: Not referring to ""the probabilistic method"")","What does it mean for a theorem to be ""almost surely true"", in a probabilistic sense? (Note: Not referring to ""the probabilistic method"")",,"I recently came across this paper where the Goldbach conjecture is explored probabilistically. I have seen this done with other unsolved theorems as well (unfortunately, I cant find a link to them anymore). The author purports to bound the probability of the Goldbach Conjecture being false as $\approx 10^{–150,000,000,000}$ What is mathematics to make of such a statement? Since a theorem is either true or false, how can a theorem be true with $99.999..\%$ certainty? In the past, I have seen another prime theorems (forgot which one) where it was shown that it was ""almost surely true"". So, lets say I assume some (theoretically plausible) probability distribution over the instances $t\in T_n$ of a general theorem I am studying, where $|T_n|=n$. Lets call this measure $P_{T_n}(t)$, and lets say I conclude $\lim\limits_{n\to\infty}E_{T_n}[\mathbf{1}_{\perp}(T_n)]=0$. This seems to say that $P(\{\mathbf{1}_{\perp}(T)=1\;\; i.o\})=0$ Has anyone used a probabilistic heuristic like in the paper to demonstrate ""almost surely true"". Or have you also run across it. If so, how is it used to as part of mathematical research, apart from a ""gee whiz...that's interesting...I guess."" Is there a domain of ""Theorems we are ""almost sure"" about, but can't prove"" where this method takes the place of certain/non probabilistic deduction?","I recently came across this paper where the Goldbach conjecture is explored probabilistically. I have seen this done with other unsolved theorems as well (unfortunately, I cant find a link to them anymore). The author purports to bound the probability of the Goldbach Conjecture being false as $\approx 10^{–150,000,000,000}$ What is mathematics to make of such a statement? Since a theorem is either true or false, how can a theorem be true with $99.999..\%$ certainty? In the past, I have seen another prime theorems (forgot which one) where it was shown that it was ""almost surely true"". So, lets say I assume some (theoretically plausible) probability distribution over the instances $t\in T_n$ of a general theorem I am studying, where $|T_n|=n$. Lets call this measure $P_{T_n}(t)$, and lets say I conclude $\lim\limits_{n\to\infty}E_{T_n}[\mathbf{1}_{\perp}(T_n)]=0$. This seems to say that $P(\{\mathbf{1}_{\perp}(T)=1\;\; i.o\})=0$ Has anyone used a probabilistic heuristic like in the paper to demonstrate ""almost surely true"". Or have you also run across it. If so, how is it used to as part of mathematical research, apart from a ""gee whiz...that's interesting...I guess."" Is there a domain of ""Theorems we are ""almost sure"" about, but can't prove"" where this method takes the place of certain/non probabilistic deduction?",,"['probability', 'probability-theory']"
59,Maximum likelihood estimate of $N$ (trials) in Binomial,Maximum likelihood estimate of  (trials) in Binomial,N,"Suppose, we throw a biased coin $N$ times with $p(\text{head}) = \pi$, and we observe the number of heads as $k$ (could be any number, say $k=4$ for simplicity). We are interested in to find the most likely $N$ as a function of $\pi$. The likelihood can be written as (for $k=4$), $$p(x = 4 | N,\pi) = {N\choose 4} \pi^4 (1-\pi)^{N-4}$$ I aim to calculate,$$N^* = \text{argmax}_N p(x=4|N,\pi)$$which is, it turns out, pretty hard to solve analytically for $N$ (you can try it yourself). Although it is a discrete variable, I tried to differentiate the log-likelihood wrt $N$ (since log is monotone, the result stays same) and tried to solve for $N$ which resulted in insolvable equations for me. So far so good. What makes this interesting for me is that, solving the problem for $\pi$ and finding most likely values of $\pi$ as a function of $N$, and then leaving $N$ alone seems to give the correct result. If you differentiate the likelihood (not log-likelihood) with respect to $\pi$, then set it to zero, and solve for $\pi$, you will find $\pi = 4/N$. Now choosing $N = 4/\pi$ is consistent with empirical results, it seems true; although, I couldn't calculate it via maximizing $N$ directly. Now see the figure. Blue line is the computationally calculated for maximum $N$'s for corresponding $\pi$'s and red is the $4/\pi$. I wonder how it can be true via solving for $\pi$ instead of $N$. Is there a general property about this likelihood that I am missing?","Suppose, we throw a biased coin $N$ times with $p(\text{head}) = \pi$, and we observe the number of heads as $k$ (could be any number, say $k=4$ for simplicity). We are interested in to find the most likely $N$ as a function of $\pi$. The likelihood can be written as (for $k=4$), $$p(x = 4 | N,\pi) = {N\choose 4} \pi^4 (1-\pi)^{N-4}$$ I aim to calculate,$$N^* = \text{argmax}_N p(x=4|N,\pi)$$which is, it turns out, pretty hard to solve analytically for $N$ (you can try it yourself). Although it is a discrete variable, I tried to differentiate the log-likelihood wrt $N$ (since log is monotone, the result stays same) and tried to solve for $N$ which resulted in insolvable equations for me. So far so good. What makes this interesting for me is that, solving the problem for $\pi$ and finding most likely values of $\pi$ as a function of $N$, and then leaving $N$ alone seems to give the correct result. If you differentiate the likelihood (not log-likelihood) with respect to $\pi$, then set it to zero, and solve for $\pi$, you will find $\pi = 4/N$. Now choosing $N = 4/\pi$ is consistent with empirical results, it seems true; although, I couldn't calculate it via maximizing $N$ directly. Now see the figure. Blue line is the computationally calculated for maximum $N$'s for corresponding $\pi$'s and red is the $4/\pi$. I wonder how it can be true via solving for $\pi$ instead of $N$. Is there a general property about this likelihood that I am missing?",,['probability']
60,"an urn contains two green, three yellow and five red balls.","an urn contains two green, three yellow and five red balls.",,"An urn contains two green, three yellow and five red balls. We draw one ball at random and put it aside. then we draw another ball until there are no more balls. Find the probability of drawing at first the two green balls, then the three yellow balls and finally the red balls My answer: We have two green balls so the probability of pulling two greens is: $\large\frac{1}{10} \cdot \frac{1}{9}$ for the yellow balls then we have $\large\frac{3}{8} \cdot \frac{2}{7} \cdot \frac{1}{6},$ and for the red ones we have: $1$ (since there are only $5$ balls left and there are five red balls) if we multiply all of these we get: $0.00019$ correct answer is: $0.0004$","An urn contains two green, three yellow and five red balls. We draw one ball at random and put it aside. then we draw another ball until there are no more balls. Find the probability of drawing at first the two green balls, then the three yellow balls and finally the red balls My answer: We have two green balls so the probability of pulling two greens is: for the yellow balls then we have and for the red ones we have: (since there are only balls left and there are five red balls) if we multiply all of these we get: correct answer is:","\large\frac{1}{10} \cdot \frac{1}{9} \large\frac{3}{8} \cdot \frac{2}{7} \cdot \frac{1}{6}, 1 5 0.00019 0.0004",['probability']
61,Conditional probability plane problem,Conditional probability plane problem,,"I was presented with this problem and am not sure where to take it. A plane is missing and is presume to have equal probability of going down in any of 3 regions. If a plane is actually down in region $i$, let $1-\alpha$ denote the probability that the plane will be found upon a search of the $ith$ region, $i=1,2,3.$ What is the conditional probability that the plane is in: A) region 1, given the search of region 1 is unsuccessful B) region 2, given the search of region 1 is unsuccessful C) region 3, given the search of region 1 is unsuccessful My idea: So we know $P(1)=P(2)=P(3) = \frac{1}{3}$. (Probability it is that region) We also know that $P(F_1\mid 1)=1-\alpha_1$, and similarly for the other regions. Then for A) want $P(1\mid F_1^c)$ $$P(1\mid F_1^c) = \frac{P(F_1^c\mid 1)P(1)}{P(F_1^c\mid 1)P(1)+P(F_2^c\mid 1)P(2)+P(F_3^c\mid 1)P(3)} =  \frac{\alpha_1}{\alpha_1+\alpha_2+\alpha_3}$$ However I feel this is wrong but cannot find why. For B) and C) I am at a loss. I know we want $P(2|F_1^c)$ and $P(2\mid F_1^c)$ but I can't see how we could use the same formula to get that. Any help would be much appreciated.","I was presented with this problem and am not sure where to take it. A plane is missing and is presume to have equal probability of going down in any of 3 regions. If a plane is actually down in region $i$, let $1-\alpha$ denote the probability that the plane will be found upon a search of the $ith$ region, $i=1,2,3.$ What is the conditional probability that the plane is in: A) region 1, given the search of region 1 is unsuccessful B) region 2, given the search of region 1 is unsuccessful C) region 3, given the search of region 1 is unsuccessful My idea: So we know $P(1)=P(2)=P(3) = \frac{1}{3}$. (Probability it is that region) We also know that $P(F_1\mid 1)=1-\alpha_1$, and similarly for the other regions. Then for A) want $P(1\mid F_1^c)$ $$P(1\mid F_1^c) = \frac{P(F_1^c\mid 1)P(1)}{P(F_1^c\mid 1)P(1)+P(F_2^c\mid 1)P(2)+P(F_3^c\mid 1)P(3)} =  \frac{\alpha_1}{\alpha_1+\alpha_2+\alpha_3}$$ However I feel this is wrong but cannot find why. For B) and C) I am at a loss. I know we want $P(2|F_1^c)$ and $P(2\mid F_1^c)$ but I can't see how we could use the same formula to get that. Any help would be much appreciated.",,"['probability', 'conditional-probability', 'solution-verification']"
62,Convergence of expected values as random variables converge almost surely,Convergence of expected values as random variables converge almost surely,,Let I have a sequence of random variables $X_n$ that converges to random variable $X$ almost surely as $n\to\infty$. How can I proof that $\lim_{n\to\infty}\mathcal{E}[X_n]=\mathcal{E}[X]$ where $\mathcal{E}[\cdot]$ stands for expected value?,Let I have a sequence of random variables $X_n$ that converges to random variable $X$ almost surely as $n\to\infty$. How can I proof that $\lim_{n\to\infty}\mathcal{E}[X_n]=\mathcal{E}[X]$ where $\mathcal{E}[\cdot]$ stands for expected value?,,"['probability', 'probability-theory', 'probability-distributions']"
63,Unbiased asymptotic variance,Unbiased asymptotic variance,,"Problem: Let $X_1,...,X_n$ be indep. r.v.'s that satisfy, for $i = 1,...,n$, $E(X_i) = \mu_i(\theta)$ & $\mathrm{Var}(X_i)= \sigma_i^2(\theta)$. $\theta$ is the parameter of interest and the functional forms $\mu_i(.)$ and $\sigma_i^2(.)$ are known. Among all unbiased estimating equations of the form $\hspace{15mm}\sum_{i=1}^{n}g_i(\theta)(X_i-\mu_i(\theta))=0$ $(*)$, what is the form of the optimal $g_i(\theta)$ that minimizes the asymptotic variance of the solution $\hat{\theta}$ of $(*)$? It may be assumed that $\hat{\theta}$ is the unique solution of $(*)$ and it is $\sqrt{n}$ consistent for $\theta$ and asymptotically normal. This question is quite difficult. Any insight would be much appreciated.","Problem: Let $X_1,...,X_n$ be indep. r.v.'s that satisfy, for $i = 1,...,n$, $E(X_i) = \mu_i(\theta)$ & $\mathrm{Var}(X_i)= \sigma_i^2(\theta)$. $\theta$ is the parameter of interest and the functional forms $\mu_i(.)$ and $\sigma_i^2(.)$ are known. Among all unbiased estimating equations of the form $\hspace{15mm}\sum_{i=1}^{n}g_i(\theta)(X_i-\mu_i(\theta))=0$ $(*)$, what is the form of the optimal $g_i(\theta)$ that minimizes the asymptotic variance of the solution $\hat{\theta}$ of $(*)$? It may be assumed that $\hat{\theta}$ is the unique solution of $(*)$ and it is $\sqrt{n}$ consistent for $\theta$ and asymptotically normal. This question is quite difficult. Any insight would be much appreciated.",,"['probability', 'statistics', 'probability-theory', 'taylor-expansion', 'statistical-inference']"
64,Entropy of the product of two random variables,Entropy of the product of two random variables,,"Consider a random matrix $X$ and a random vector $Y$. Let the Shannon entropies $H(X) = H(Y) = n$.  Is there a simple upper bound for entropy $H(XY)$?  I believe $H(XY) \leq 2n$ as that is a simple upper bound for the joint entropy of $(X,Y)$, but is it also the case that $H(XY) \leq n$? The entries of $X$ need not be independent. $X$ is $m$ by $n$ for some $m \leq n$ and $Y$ has $n$ entries.","Consider a random matrix $X$ and a random vector $Y$. Let the Shannon entropies $H(X) = H(Y) = n$.  Is there a simple upper bound for entropy $H(XY)$?  I believe $H(XY) \leq 2n$ as that is a simple upper bound for the joint entropy of $(X,Y)$, but is it also the case that $H(XY) \leq n$? The entries of $X$ need not be independent. $X$ is $m$ by $n$ for some $m \leq n$ and $Y$ has $n$ entries.",,['probability']
65,Easy way to compute $Pr[\sum_{i=1}^t X_i \geq z]$,Easy way to compute,Pr[\sum_{i=1}^t X_i \geq z],"We have a set of $t$ independent random variables $X_i \sim \mathrm{Bin}(n_i, p_i)$. We know that $$\mathrm{Pr}[X_i \geq z] = \sum_{j=z}^{\infty} { n_i \choose j } p_i^j (1-p_i)^{n_i -j}.$$ But is there an easy way to compute: $$\mathrm{Pr}\left[\sum_{i=1}^t X_i \geq z\right]?$$ MY IDEAS: This should have to do something with convolution, but I am not sure. Is it easier to compute $$\mathrm{Pr}\left[\sum_{i=1}^t X_i \leq z \right]?$$ What I thought of is maybe: $$\mathrm{Pr}\left[\sum_{i=1}^t X_i \leq z \right]=\sum_{j=1}^{z} \mathrm{Pr}\left[\sum_{i=1}^t X_i = z \right]$$ but this seems to be quite hard with $t$ random variables. Would appreciate any hint and if you don't write an answer I am interested in whether it is too difficult or too easy?! thank you..","We have a set of $t$ independent random variables $X_i \sim \mathrm{Bin}(n_i, p_i)$. We know that $$\mathrm{Pr}[X_i \geq z] = \sum_{j=z}^{\infty} { n_i \choose j } p_i^j (1-p_i)^{n_i -j}.$$ But is there an easy way to compute: $$\mathrm{Pr}\left[\sum_{i=1}^t X_i \geq z\right]?$$ MY IDEAS: This should have to do something with convolution, but I am not sure. Is it easier to compute $$\mathrm{Pr}\left[\sum_{i=1}^t X_i \leq z \right]?$$ What I thought of is maybe: $$\mathrm{Pr}\left[\sum_{i=1}^t X_i \leq z \right]=\sum_{j=1}^{z} \mathrm{Pr}\left[\sum_{i=1}^t X_i = z \right]$$ but this seems to be quite hard with $t$ random variables. Would appreciate any hint and if you don't write an answer I am interested in whether it is too difficult or too easy?! thank you..",,"['probability', 'probability-theory', 'probability-distributions', 'summation', 'convolution']"
66,Non-uniform sampling of N-sphere,Non-uniform sampling of N-sphere,,"Suppose I have a unit $N$-sphere from which I want to draw points at random.  To obtain uniformly distributed points I do the usual technique of drawing $N$ random variables $x_i$ from a Gaussian $\mathcal N(0,1)$, then making vectors $\mathbf y$ by $$y_i = \frac{x_i}{\sqrt{ \sum_j x_j^2}}.$$ However, this isn't quite what I want to do.  I would like a sampling technique that has some set of parameters that I can tune (preferably continuously) that lets me smoothly transition between having a non-uniform sample that bunches at the ""poles"" and one that is uniformly distributed as above.  The exact nature of the ""bunching"" is not hugely important, as long as these two qualitative limiting behaviors exist. Obviously, I can get bunching at the poles by sampling $N$-dimensional polar angles uniformly on $\theta_i \in [0,2\pi]$, but I'm at a bit of a loss regarding how to continuously move the concentration away from the poles and become uniformly distributed.  Is there a sensible way to express the uniformly distributed equal-area sampling as a prior probability on the polar angles $\theta_i$?  I can hack it in 2 dimensions quite easily, but in $N$ it's a bit harder. Any hint at an algorithm would be extremely helpful!","Suppose I have a unit $N$-sphere from which I want to draw points at random.  To obtain uniformly distributed points I do the usual technique of drawing $N$ random variables $x_i$ from a Gaussian $\mathcal N(0,1)$, then making vectors $\mathbf y$ by $$y_i = \frac{x_i}{\sqrt{ \sum_j x_j^2}}.$$ However, this isn't quite what I want to do.  I would like a sampling technique that has some set of parameters that I can tune (preferably continuously) that lets me smoothly transition between having a non-uniform sample that bunches at the ""poles"" and one that is uniformly distributed as above.  The exact nature of the ""bunching"" is not hugely important, as long as these two qualitative limiting behaviors exist. Obviously, I can get bunching at the poles by sampling $N$-dimensional polar angles uniformly on $\theta_i \in [0,2\pi]$, but I'm at a bit of a loss regarding how to continuously move the concentration away from the poles and become uniformly distributed.  Is there a sensible way to express the uniformly distributed equal-area sampling as a prior probability on the polar angles $\theta_i$?  I can hack it in 2 dimensions quite easily, but in $N$ it's a bit harder. Any hint at an algorithm would be extremely helpful!",,"['probability', 'geometry', 'probability-distributions', 'sampling']"
67,What is the expected value of the number of randomly chosen real numbers between $0$ and $1$ needed to reach a sum of $1$? [duplicate],What is the expected value of the number of randomly chosen real numbers between  and  needed to reach a sum of ? [duplicate],0 1 1,"This question already has answers here : Choose a random number between $0$ and $1$ and record its value. Keep doing it until the sum of the numbers exceeds $1$. How many tries do we need? (8 answers) Closed 9 years ago . My friend told me that the answer to this question was $e$, which intrigued me, but he refused to tell me why. My initial intuition was completely wrong. I thought that since the expected value of any one of these numbers (uniformly distributed on $[0,1]$) was $1/2$, the expected number of them you'd need to get a sum of $1$ would be two. I realized this was rather stupid. This is my attempt at calculating the expected value term-by term. Let $N$ be the number of numbers between $0$ and $1$ necessary for their sum to exceed $1$, and let $P(N=k)$ be the probability that it takes $k$ such numbers. Then the expected value of $N$ is $$E(N) = P(N=2) \cdot 2 + P(N=3) \cdot 3 + P(N=4) \cdot 4 + \cdots$$ I then calculated the first term: $$P(N=2) \cdot 2 = \frac{1}{2} \cdot 2 = 1$$ Unfortunately, the second and higher terms aren't as easy ...","This question already has answers here : Choose a random number between $0$ and $1$ and record its value. Keep doing it until the sum of the numbers exceeds $1$. How many tries do we need? (8 answers) Closed 9 years ago . My friend told me that the answer to this question was $e$, which intrigued me, but he refused to tell me why. My initial intuition was completely wrong. I thought that since the expected value of any one of these numbers (uniformly distributed on $[0,1]$) was $1/2$, the expected number of them you'd need to get a sum of $1$ would be two. I realized this was rather stupid. This is my attempt at calculating the expected value term-by term. Let $N$ be the number of numbers between $0$ and $1$ necessary for their sum to exceed $1$, and let $P(N=k)$ be the probability that it takes $k$ such numbers. Then the expected value of $N$ is $$E(N) = P(N=2) \cdot 2 + P(N=3) \cdot 3 + P(N=4) \cdot 4 + \cdots$$ I then calculated the first term: $$P(N=2) \cdot 2 = \frac{1}{2} \cdot 2 = 1$$ Unfortunately, the second and higher terms aren't as easy ...",,"['calculus', 'probability', 'expectation']"
68,Conditional Probability - Derivation of Probability Unknown,Conditional Probability - Derivation of Probability Unknown,,"Here's a problem from Sheldon Ross's A First Course in Probability that I don't understand: At a certain stage of a criminal investigation, the inspector in charge is 60 percent convinced of the guilt of a certain suspect. Suppose, however, that a new piece of evidence which shows that the criminal has a certain characteristic (such as left-handedness, baldness, or brown hair) is uncovered. If 20 percent of the population possesses this characteristic, how certain of the guilt of the suspect should the inspector now be if it turns out that the suspect has the characteristic? The solution is: Letting $G$ denote the event that the suspect is guilty and $C$ the event that he possesses the characteristic of the criminal, we have $$ P(G\mid C) = \frac{P(GC)}{P(C)} = \frac{P(C\mid G)P(G)}{P(C\mid G)P(G) + P(C\mid G^c)P(G^c)} = \frac{1(0.6)}{1(0.6) + (0.2)(0.4)} = 0.882 $$ What I don't understand is why $P(C) = P(C\mid G)P(G) + P(C\mid G^c)P(G^c)$. I feel that it should be either 1, if the suspect has the characteristic, else 0.2 if it's unknown. If someone could explain how this is derived, I'd really appreciate it.","Here's a problem from Sheldon Ross's A First Course in Probability that I don't understand: At a certain stage of a criminal investigation, the inspector in charge is 60 percent convinced of the guilt of a certain suspect. Suppose, however, that a new piece of evidence which shows that the criminal has a certain characteristic (such as left-handedness, baldness, or brown hair) is uncovered. If 20 percent of the population possesses this characteristic, how certain of the guilt of the suspect should the inspector now be if it turns out that the suspect has the characteristic? The solution is: Letting $G$ denote the event that the suspect is guilty and $C$ the event that he possesses the characteristic of the criminal, we have $$ P(G\mid C) = \frac{P(GC)}{P(C)} = \frac{P(C\mid G)P(G)}{P(C\mid G)P(G) + P(C\mid G^c)P(G^c)} = \frac{1(0.6)}{1(0.6) + (0.2)(0.4)} = 0.882 $$ What I don't understand is why $P(C) = P(C\mid G)P(G) + P(C\mid G^c)P(G^c)$. I feel that it should be either 1, if the suspect has the characteristic, else 0.2 if it's unknown. If someone could explain how this is derived, I'd really appreciate it.",,"['probability', 'conditional-probability']"
69,Expected in-sample error of linear regression with respect to a dataset D,Expected in-sample error of linear regression with respect to a dataset D,,"In my textbook, there is a statement mentioned on the topic of linear regression/machine learning, and a question, which is simply quoted as, Consider a noisy target, $ y = (w^{*})^T \textbf{x} + \epsilon  $ , for generating the data, where $\epsilon$ is a noise term with zero mean and $\sigma^2$ variance, independently generated for every example $(\textbf{x},y)$ . The expected error of the best possible linear fit to this target is thus $\sigma^2$ . For the data $D =  \{ (\textbf{x}_1,y_1), ..., (\textbf{x}_N,y_N)  \}$ , denote the noise in $y_n$ as $\epsilon_n$ , and let $ \mathbf{\epsilon}   = [\epsilon_1, \epsilon_2, ...\epsilon_N]^T$ ; assume that $X^TX$ is invertible. By following the steps below, show that the expected in-sample error of linear regression with respect to $D$ is given by , $ \mathbb{E}_D[E_{in}( \textbf{w}_{lin} )] = \sigma^2 (1 - \frac{d+1}{N})$ Below is my methodology, Book says that, In-sample error vector, $\hat{\textbf{y}} - \textbf{y}$ , can be expressed as $(H-I)\epsilon$ , which is simply, hat matrix, $H= X(X^TX)^{-1}X^T$ , times, error vector, $\epsilon$ . So, I calculated in-sample error, $E_{in}( \textbf{w}_{lin} )$ , as, $E_{in}( \textbf{w}_{lin} ) = \frac{1}{N}(\hat{\textbf{y}} - \textbf{y})^T (\hat{\textbf{y}} - \textbf{y}) =  \frac{1}{N}  (\epsilon^T (H-I)^T (H-I) \epsilon)$ Since it is given by the book that, $(I-H)^K = (I-H)$ , and also $(I-H)$ is symetric, $trace(H) = d+1$ I got the following simplified expression, $E_{in}( \textbf{w}_{lin} ) =\frac{1}{N}  (\epsilon^T (H-I)^T (H-I) \epsilon) = \frac{1}{N} \epsilon^T (I-H) \epsilon = \frac{1}{N} \epsilon^T \epsilon - \frac{1}{N} \epsilon^T H \epsilon$ Here, I see that, $\mathbb{E}_D[\frac{1}{N} \epsilon^T \epsilon] = \frac {N \sigma^2}{N}$ And, also, the sum formed by $ - \frac{1}{N} \epsilon^T H \epsilon$ , gives the following sum, $ - \frac{1}{N} \epsilon^T H \epsilon = - \frac{1}{N} \{ \sum_{i=1}^{N} H_{ii} \epsilon_i^2 + \sum_{i,j \ \in \ \{1..N\} \ and \ i \neq j}^{} \ H_{ij} \ \epsilon_i \ \epsilon_j \}$ I undestand that, $ - \frac{1}{N} \mathbb{E}_D[\sum_{i=1}^{N} H_{ii} \epsilon_i^2] = - trace(H) \ \sigma^2 = - (d+1) \ \sigma^2$ However, I don't understand why, $ - \frac{1}{N} \mathbb{E}_D[\sum_{i,j \ \in \ \{1..N\} \ and \ i \neq j}^{} \ H_{ij} \ \epsilon_i \ \epsilon_j ] = 0$ $\ \ \ \ \ \ \ \ \ \ \ \ (eq \ 1)$ $(eq 1)$ should be equal to $0$ in order to satisfy the equation, $   \mathbb{E}_D[E_{in}( \textbf{w}_{lin} )] = \sigma^2 (1 - \frac{d+1}{N})$ Can any one mind to explain me why $(eq1)$ leads to a zero result ?","In my textbook, there is a statement mentioned on the topic of linear regression/machine learning, and a question, which is simply quoted as, Consider a noisy target, , for generating the data, where is a noise term with zero mean and variance, independently generated for every example . The expected error of the best possible linear fit to this target is thus . For the data , denote the noise in as , and let ; assume that is invertible. By following the steps below, show that the expected in-sample error of linear regression with respect to is given by , Below is my methodology, Book says that, In-sample error vector, , can be expressed as , which is simply, hat matrix, , times, error vector, . So, I calculated in-sample error, , as, Since it is given by the book that, , and also is symetric, I got the following simplified expression, Here, I see that, And, also, the sum formed by , gives the following sum, I undestand that, However, I don't understand why, should be equal to in order to satisfy the equation, Can any one mind to explain me why leads to a zero result ?"," y = (w^{*})^T \textbf{x} + \epsilon   \epsilon \sigma^2 (\textbf{x},y) \sigma^2 D =  \{ (\textbf{x}_1,y_1), ..., (\textbf{x}_N,y_N)  \} y_n \epsilon_n  \mathbf{\epsilon}   = [\epsilon_1, \epsilon_2, ...\epsilon_N]^T X^TX D  \mathbb{E}_D[E_{in}( \textbf{w}_{lin} )] = \sigma^2 (1 - \frac{d+1}{N}) \hat{\textbf{y}} - \textbf{y} (H-I)\epsilon H= X(X^TX)^{-1}X^T \epsilon E_{in}( \textbf{w}_{lin} ) E_{in}( \textbf{w}_{lin} ) = \frac{1}{N}(\hat{\textbf{y}} - \textbf{y})^T (\hat{\textbf{y}} - \textbf{y}) =  \frac{1}{N}  (\epsilon^T (H-I)^T (H-I) \epsilon) (I-H)^K = (I-H) (I-H) trace(H) = d+1 E_{in}( \textbf{w}_{lin} ) =\frac{1}{N}  (\epsilon^T (H-I)^T (H-I) \epsilon) = \frac{1}{N} \epsilon^T (I-H) \epsilon = \frac{1}{N} \epsilon^T \epsilon - \frac{1}{N} \epsilon^T H \epsilon \mathbb{E}_D[\frac{1}{N} \epsilon^T \epsilon] = \frac {N \sigma^2}{N}  - \frac{1}{N} \epsilon^T H \epsilon  - \frac{1}{N} \epsilon^T H \epsilon = - \frac{1}{N} \{ \sum_{i=1}^{N} H_{ii} \epsilon_i^2 + \sum_{i,j \ \in \ \{1..N\} \ and \ i \neq j}^{} \ H_{ij} \ \epsilon_i \ \epsilon_j \}  - \frac{1}{N} \mathbb{E}_D[\sum_{i=1}^{N} H_{ii} \epsilon_i^2] = - trace(H) \ \sigma^2 = - (d+1) \ \sigma^2  - \frac{1}{N} \mathbb{E}_D[\sum_{i,j \ \in \ \{1..N\} \ and \ i \neq j}^{} \ H_{ij} \ \epsilon_i \ \epsilon_j ] = 0 \ \ \ \ \ \ \ \ \ \ \ \ (eq \ 1) (eq 1) 0    \mathbb{E}_D[E_{in}( \textbf{w}_{lin} )] = \sigma^2 (1 - \frac{d+1}{N}) (eq1)","['probability', 'statistics', 'regression', 'machine-learning']"
70,How can probabilities be modeled in a universe where time travel is possible?,How can probabilities be modeled in a universe where time travel is possible?,,"Please don't take this as a joke, its actually a serious question. If it sounds silly its only because of my (lack of) understanding of probabilities, but my motivation is genuine. Lets take the following 2 events: $A$: Shooting once at a bullseye and hitting at an specific point $P_A$. $B$: Shooting twice at a bullseye and hitting both times in the same point $P_B$. Under classic asumptions (for some sensible definition of classic) both events have $0$ probability. Right? Now suppose we have backwards time travel (despite physics contradictions). I can watch where the projectile hits the first time, then go back in time, and make a prediction with probability $1$ for $P_A$. However, even with time travel, event $B$ still has $0$ probability. My questions are: Is there some real difference in how these two events are modeled, or interpreted, or is there (more probably) some logical flaw in my reasoning? Does this means that time travel is impossible from a logical point of view, without even looking at physics?","Please don't take this as a joke, its actually a serious question. If it sounds silly its only because of my (lack of) understanding of probabilities, but my motivation is genuine. Lets take the following 2 events: $A$: Shooting once at a bullseye and hitting at an specific point $P_A$. $B$: Shooting twice at a bullseye and hitting both times in the same point $P_B$. Under classic asumptions (for some sensible definition of classic) both events have $0$ probability. Right? Now suppose we have backwards time travel (despite physics contradictions). I can watch where the projectile hits the first time, then go back in time, and make a prediction with probability $1$ for $P_A$. However, even with time travel, event $B$ still has $0$ probability. My questions are: Is there some real difference in how these two events are modeled, or interpreted, or is there (more probably) some logical flaw in my reasoning? Does this means that time travel is impossible from a logical point of view, without even looking at physics?",,"['probability', 'soft-question']"
71,Variance of sums of independent random variables,Variance of sums of independent random variables,,I have the following formula - $Var(\overline{X}) = Var(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2}\sum_{i=1}^n Var(X_i)$ I know that the variance of the sum of independent random variables is equal to the sum of the variances of the random variables but I don't see where the $\frac{1}{n^2}$ is coming from? Why isn't it $\frac{1}{n}$?,I have the following formula - $Var(\overline{X}) = Var(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2}\sum_{i=1}^n Var(X_i)$ I know that the variance of the sum of independent random variables is equal to the sum of the variances of the random variables but I don't see where the $\frac{1}{n^2}$ is coming from? Why isn't it $\frac{1}{n}$?,,['probability']
72,Is a constant (deterministic random variable) Gaussian?,Is a constant (deterministic random variable) Gaussian?,,"Consider a constant $c$. Is this constant a Gaussian random variable (i.e. is $c\sim\mathcal{N}(c,0)$)? I realize a constant is easily described as a discrete random variable, but I wish to use normal random variable properties on constants when combining them with other continuous random variables. It makes sense to me that this should be, but it leads to a delta function for the pdf which I assume is bad form.","Consider a constant $c$. Is this constant a Gaussian random variable (i.e. is $c\sim\mathcal{N}(c,0)$)? I realize a constant is easily described as a discrete random variable, but I wish to use normal random variable properties on constants when combining them with other continuous random variables. It makes sense to me that this should be, but it leads to a delta function for the pdf which I assume is bad form.",,"['probability', 'probability-distributions', 'normal-distribution']"
73,Show that the nth order statistic is a consistent estimator of a uniform parameter,Show that the nth order statistic is a consistent estimator of a uniform parameter,,"We assume that our observations come from a uniform $(0,\theta)$ distribution. Can you please check my work on the following? We can derive the distribution function of the maximum of the sample, $Y_n$ for some $t$ as follows: $$ F_{Y_n} (t)= \begin{cases}  1 \quad t> \theta \\ \left( \frac{t}{\theta} \right)^n \quad 0<t \leq \theta \\ 0 \quad t\leq 0 \end{cases} $$ Now the definition of consistence states for our estimator $Y_n$ to be consistent we require that $\lim_{n\to \infty} P\left[|Y_n - \theta| <\epsilon \right]=1$ for all $\epsilon>0$ Using the above CDF, $P\left[ \theta< Y_n<\epsilon+\theta \right]=1-\left(\frac{\epsilon}{\theta} \right)^n$ The second term goes to zero in the limit so we get $1$. Is everything alright in the above? Do I need to do anything else? Thanks.","We assume that our observations come from a uniform $(0,\theta)$ distribution. Can you please check my work on the following? We can derive the distribution function of the maximum of the sample, $Y_n$ for some $t$ as follows: $$ F_{Y_n} (t)= \begin{cases}  1 \quad t> \theta \\ \left( \frac{t}{\theta} \right)^n \quad 0<t \leq \theta \\ 0 \quad t\leq 0 \end{cases} $$ Now the definition of consistence states for our estimator $Y_n$ to be consistent we require that $\lim_{n\to \infty} P\left[|Y_n - \theta| <\epsilon \right]=1$ for all $\epsilon>0$ Using the above CDF, $P\left[ \theta< Y_n<\epsilon+\theta \right]=1-\left(\frac{\epsilon}{\theta} \right)^n$ The second term goes to zero in the limit so we get $1$. Is everything alright in the above? Do I need to do anything else? Thanks.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'uniform-distribution']"
74,Unknown number of colours Bernoulli Urn,Unknown number of colours Bernoulli Urn,,"Okay, so, in the traditional Bernoulli Urn problem, we have an urn with a number N, possibly infinite, of coloured balls, and there are k possible colours. That one I grok. However, what if I don't actually know what k is? That is, what if I have an urn with N balls and an unknown but finite and strictly positive number of possible colours? The main question is, in fact, what my priors should be. What's the prior that there is exactly one colour? Exactly two? At least two? How do I update on the relative frequencies of each colour? Is this problem even solvable? My first lines of thinking are to have a vector of parameters $\vec \theta \in \mathbb R^\infty$ such that the first parameter is the number of colours in the urn (let's call it $\alpha$ ) and the remaining are the relative frequencies of each colour. If $P(A=n\mid\vec\theta)$ is the probability that the colour of the next draw will be $n$ given the knowledge contained by $\vec\theta$ , we'd have: $\vec\theta = (\alpha, p_1, p_2, p_3, \ldots)$ $\alpha \in \mathbb N^*$ $\left(\sum\limits_{n=1}^\infty P(\alpha = n) \right)= 1$ $\left(\sum\limits_{n=1}^\infty p_n\right) = 1$ $\forall n > \alpha : p_n = 0$ $\forall n \in \mathbb N^* : P(A=n\mid\vec\theta) = p_n$ However, this is just wild speculation on my part. I'm mostly curious about whether this is even in principle solvable. What I'd want to know is a way to compute both the prior and posterior distributions of $P(\vec\theta)$ or, in other words, the pdfs $P(\alpha)$ , $P(p_1)$ , $P(p_2)$ , etc. How to start with them and how to update on them.","Okay, so, in the traditional Bernoulli Urn problem, we have an urn with a number N, possibly infinite, of coloured balls, and there are k possible colours. That one I grok. However, what if I don't actually know what k is? That is, what if I have an urn with N balls and an unknown but finite and strictly positive number of possible colours? The main question is, in fact, what my priors should be. What's the prior that there is exactly one colour? Exactly two? At least two? How do I update on the relative frequencies of each colour? Is this problem even solvable? My first lines of thinking are to have a vector of parameters such that the first parameter is the number of colours in the urn (let's call it ) and the remaining are the relative frequencies of each colour. If is the probability that the colour of the next draw will be given the knowledge contained by , we'd have: However, this is just wild speculation on my part. I'm mostly curious about whether this is even in principle solvable. What I'd want to know is a way to compute both the prior and posterior distributions of or, in other words, the pdfs , , , etc. How to start with them and how to update on them.","\vec \theta \in \mathbb R^\infty \alpha P(A=n\mid\vec\theta) n \vec\theta \vec\theta = (\alpha, p_1, p_2, p_3, \ldots) \alpha \in \mathbb N^* \left(\sum\limits_{n=1}^\infty P(\alpha = n) \right)= 1 \left(\sum\limits_{n=1}^\infty p_n\right) = 1 \forall n > \alpha : p_n = 0 \forall n \in \mathbb N^* : P(A=n\mid\vec\theta) = p_n P(\vec\theta) P(\alpha) P(p_1) P(p_2)","['probability', 'probability-theory', 'probability-distributions']"
75,"Prove that Cov(X,Y)=Cov(X,E[Y|X])","Prove that Cov(X,Y)=Cov(X,E[Y|X])",,"I've been working on this problem for 3 hours now, and my complete lack of progress is getting disheartening. I've looked up definitions, proofs, and have even seen a solution for this particular problem. However, I did not understand the steps the solution used, which means I have no business reading it. I'm hoping that if I show my attempt, y'all can give me some pointers. Thanks in advance! Here is my work: $$\mathrm{Cov}(X,Y)=E[(X-E[X])(Y-E[Y])] =E[XY-X*E[Y]-Y*E[X]+E[X]*E[Y]]$$ By linearity of expected value: =E[XY]-E[X*E[Y]]-E[Y*E[X]]+E[E[X]*E[Y]], as E[Y] and E[X] are constants, E[X*E[Y]]=E[Y]*E[X] =E[XY]-E[X]E[Y]-E[X]E[Y]+E[X]E[Y] =E[XY]-E[X]E[Y] Cov(X,E[Y|X]) =E[(X-E[X])(E[Y|X]-E[E[Y|X]])] =E[(X-E[X])(E[Y|X]-E[Y])] =E[X*E[Y|X]-X*E[Y]-E[X]E[Y|X]+E[X]E[Y]] = E[X*E[Y|X]]-E[X*E[Y]]-E[E[X]E[Y|X]]+E[E[X]E[Y]] =E[X]E[Y|X]-E[X]E[Y]-E[X]E[E[Y|X]]+E[X]E[Y] =0 I really have no idea where to go with this problem. If E[X*E[Y]]=E[X]E[Y], I cannot figure out why E[XE[Y|X]]$\neq$E[X]E[Y|X]. I believe I am making a mistake when I go from the bold line to the next, but I can't figure out why.","I've been working on this problem for 3 hours now, and my complete lack of progress is getting disheartening. I've looked up definitions, proofs, and have even seen a solution for this particular problem. However, I did not understand the steps the solution used, which means I have no business reading it. I'm hoping that if I show my attempt, y'all can give me some pointers. Thanks in advance! Here is my work: $$\mathrm{Cov}(X,Y)=E[(X-E[X])(Y-E[Y])] =E[XY-X*E[Y]-Y*E[X]+E[X]*E[Y]]$$ By linearity of expected value: =E[XY]-E[X*E[Y]]-E[Y*E[X]]+E[E[X]*E[Y]], as E[Y] and E[X] are constants, E[X*E[Y]]=E[Y]*E[X] =E[XY]-E[X]E[Y]-E[X]E[Y]+E[X]E[Y] =E[XY]-E[X]E[Y] Cov(X,E[Y|X]) =E[(X-E[X])(E[Y|X]-E[E[Y|X]])] =E[(X-E[X])(E[Y|X]-E[Y])] =E[X*E[Y|X]-X*E[Y]-E[X]E[Y|X]+E[X]E[Y]] = E[X*E[Y|X]]-E[X*E[Y]]-E[E[X]E[Y|X]]+E[E[X]E[Y]] =E[X]E[Y|X]-E[X]E[Y]-E[X]E[E[Y|X]]+E[X]E[Y] =0 I really have no idea where to go with this problem. If E[X*E[Y]]=E[X]E[Y], I cannot figure out why E[XE[Y|X]]$\neq$E[X]E[Y|X]. I believe I am making a mistake when I go from the bold line to the next, but I can't figure out why.",,"['probability', 'statistics', 'probability-theory', 'covariance', 'conditional-expectation']"
76,Proof on Brownian Bridge,Proof on Brownian Bridge,,"PROBLEM Let $U_t$ be a Brownian bridge on $[0,1]$ and let $Z$ be a standard normal random variable independent of $U_t$. $(a)$ Prove that the process $W_t = U_t + tZ$ is a brownian motion. $(b)$ Prove that the process $W_t = (1+t)U_{\frac{t}{1+t}}$ on $[0,\infty)$ is a Brownian motion. I have no idea how to prove these statements, I know that a Brownian motion follows a normal distribution with $\mu = 0$ and  $\sigma^2 = t$. Can anybody please help me to prove this?","PROBLEM Let $U_t$ be a Brownian bridge on $[0,1]$ and let $Z$ be a standard normal random variable independent of $U_t$. $(a)$ Prove that the process $W_t = U_t + tZ$ is a brownian motion. $(b)$ Prove that the process $W_t = (1+t)U_{\frac{t}{1+t}}$ on $[0,\infty)$ is a Brownian motion. I have no idea how to prove these statements, I know that a Brownian motion follows a normal distribution with $\mu = 0$ and  $\sigma^2 = t$. Can anybody please help me to prove this?",,"['probability', 'brownian-motion']"
77,Expected number of points on circle to form an acute angled triangle,Expected number of points on circle to form an acute angled triangle,,This problem was asked to me in an interview. We keep on adding points on a circle uniformly until there exist three points on the circle which form an acute angled triangle. What is the expected number of points on the circle when the process stops?,This problem was asked to me in an interview. We keep on adding points on a circle uniformly until there exist three points on the circle which form an acute angled triangle. What is the expected number of points on the circle when the process stops?,,"['probability', 'recreational-mathematics', 'circles', 'problem-solving']"
78,Large deviations: showing $n^{-1}\log \mathbb P(|n^{-1} \sum_{i=1}^n X_i| \ge \delta) \to -\delta^2/2$ for $X_i$ i.i.d. Gaussian,Large deviations: showing  for  i.i.d. Gaussian,n^{-1}\log \mathbb P(|n^{-1} \sum_{i=1}^n X_i| \ge \delta) \to -\delta^2/2 X_i,"I am having a problem with the first example of Amir Dembo and Ofer Zeitouni book Large Deviations Techniques and Applications. Could someone please help me confirm the following statement if $X_i$ are i.i.d standard normal random variables, $n^{-1}\log \mathbb P(|n^{-1} \sum_{i=1}^n X_i| \ge \delta) \to -\delta^2/2$, as $n \to \infty$.","I am having a problem with the first example of Amir Dembo and Ofer Zeitouni book Large Deviations Techniques and Applications. Could someone please help me confirm the following statement if $X_i$ are i.i.d standard normal random variables, $n^{-1}\log \mathbb P(|n^{-1} \sum_{i=1}^n X_i| \ge \delta) \to -\delta^2/2$, as $n \to \infty$.",,"['probability', 'probability-theory', 'law-of-large-numbers', 'large-deviation-theory']"
79,Conditional Probability injection-molding,Conditional Probability injection-molding,,"I thought about this questions for a while now. I understand the fact that the foreman forgets  to shut up the injection is 0.48.  then the questions confuses me when they to tell us the probability of the defective molding will be produced 0.04 to 0.19 in early morning. this fact really threw me off. I am now confused. I am not even sure how to approach this problem. A foreman for an injection-molding firm admits that on 48% of his shifts, he forgets to shut off the injection machine on his line. This causes the machine to overheat, increasing the probability that a defective molding will be produced during the early morning run from 4% to 19%. If a molding is randomly selected from the early morning run of a random day, what is the probability that it is defective? Probability =","I thought about this questions for a while now. I understand the fact that the foreman forgets  to shut up the injection is 0.48.  then the questions confuses me when they to tell us the probability of the defective molding will be produced 0.04 to 0.19 in early morning. this fact really threw me off. I am now confused. I am not even sure how to approach this problem. A foreman for an injection-molding firm admits that on 48% of his shifts, he forgets to shut off the injection machine on his line. This causes the machine to overheat, increasing the probability that a defective molding will be produced during the early morning run from 4% to 19%. If a molding is randomly selected from the early morning run of a random day, what is the probability that it is defective? Probability =",,['probability']
80,What does $P(\overline{N})$ mean?,What does  mean?,P(\overline{N}),"Sorry, this must be laughably trivial, but I have to be sure. I am a grade 12 math tutor, and while running through some older exam questions I encountered this problem: $$\text{If } P\left(N\right) = \frac{1}{4}\text{, determine } P\left(\overline{N}\right)$$ The multiple choice answers are: $$\text{A. }\frac{-1}{4}\quad\text{B. }\frac{1}{4}\quad\text{C. }\frac{3}{4}\quad\text{D. }4$$ I eliminated A and D , because $0 \leq P(\text{Anything}) \leq 1$. My intuition tells me that $P\left(\overline{N}\right) = P\left(\lnot N\right)$ so the answer is C . Wolfram says that the bar could mean the arithmetic mean of a set of values or negation of a logical expression. Either way I would like some clarification on this notation.","Sorry, this must be laughably trivial, but I have to be sure. I am a grade 12 math tutor, and while running through some older exam questions I encountered this problem: $$\text{If } P\left(N\right) = \frac{1}{4}\text{, determine } P\left(\overline{N}\right)$$ The multiple choice answers are: $$\text{A. }\frac{-1}{4}\quad\text{B. }\frac{1}{4}\quad\text{C. }\frac{3}{4}\quad\text{D. }4$$ I eliminated A and D , because $0 \leq P(\text{Anything}) \leq 1$. My intuition tells me that $P\left(\overline{N}\right) = P\left(\lnot N\right)$ so the answer is C . Wolfram says that the bar could mean the arithmetic mean of a set of values or negation of a logical expression. Either way I would like some clarification on this notation.",,"['probability', 'notation']"
81,Is the MLE strongly consistent and asymptotically efficient for exponential families?,Is the MLE strongly consistent and asymptotically efficient for exponential families?,,"It is known that the Maximum Likelihood Estimator (MLE) is strongly consistent and asymptotically efficient under certain regularity conditions. By strongly consistent I mean that $\hat{\theta}_{MLE} \rightarrow \theta$ almost surely. By asymptotically efficient I mean that $\sqrt{n}(\hat{\theta}_{MLE}-\theta)\rightarrow N(0,I^{-1}(\theta))$ in distribution. These regularity conditions are cumbersome to check so I was wondering if there is a general and easy to check case for when the regularity conditions hold. For example, do these regularity conditions always hold for exponential families? I am not asking anyone to prove this, I am just wondering if someone knows the answer. Regularity Conditions for Asymptotic Efficiency: http://en.wikipedia.org/wiki/Maximum_likelihood#Asymptotic_normality Regularity Conditions for Strong Consistency: http://en.wikipedia.org/wiki/Maximum_likelihood#Consistency","It is known that the Maximum Likelihood Estimator (MLE) is strongly consistent and asymptotically efficient under certain regularity conditions. By strongly consistent I mean that $\hat{\theta}_{MLE} \rightarrow \theta$ almost surely. By asymptotically efficient I mean that $\sqrt{n}(\hat{\theta}_{MLE}-\theta)\rightarrow N(0,I^{-1}(\theta))$ in distribution. These regularity conditions are cumbersome to check so I was wondering if there is a general and easy to check case for when the regularity conditions hold. For example, do these regularity conditions always hold for exponential families? I am not asking anyone to prove this, I am just wondering if someone knows the answer. Regularity Conditions for Asymptotic Efficiency: http://en.wikipedia.org/wiki/Maximum_likelihood#Asymptotic_normality Regularity Conditions for Strong Consistency: http://en.wikipedia.org/wiki/Maximum_likelihood#Consistency",,"['probability', 'statistics', 'convergence-divergence']"
82,Probability for getting $n$ black balls,Probability for getting  black balls,n,"In a jar there are $ 5 $ white and $ 7 $ black balls. each time we choose a ball, it is returned with addition of two balls in the same color.Find the probability that the n first chosen balls are black. For 2 balls the probability is $\dfrac{7\cdot 9}{12 \cdot 14}=\dfrac 3 8$, For 3 balls the probability is $\dfrac{7 \cdot 9 \cdot 11}{12 \cdot 14 \cdot 16}=\dfrac {33} {128}$ $\vdots$ For $n$ balls the probability is defined to be $\frac{\displaystyle\prod_{k=1}^{n}(5+2k)}{\displaystyle\prod_{j=1}^{n}(10+2j)}$. Even though on first look it seems a final solution, when is n is divided by 4 some elements from the counter and the denominator are reducible. My question is if we can either determine which elements are reducible or simplify the result to a more ""friendly"" one.","In a jar there are $ 5 $ white and $ 7 $ black balls. each time we choose a ball, it is returned with addition of two balls in the same color.Find the probability that the n first chosen balls are black. For 2 balls the probability is $\dfrac{7\cdot 9}{12 \cdot 14}=\dfrac 3 8$, For 3 balls the probability is $\dfrac{7 \cdot 9 \cdot 11}{12 \cdot 14 \cdot 16}=\dfrac {33} {128}$ $\vdots$ For $n$ balls the probability is defined to be $\frac{\displaystyle\prod_{k=1}^{n}(5+2k)}{\displaystyle\prod_{j=1}^{n}(10+2j)}$. Even though on first look it seems a final solution, when is n is divided by 4 some elements from the counter and the denominator are reducible. My question is if we can either determine which elements are reducible or simplify the result to a more ""friendly"" one.",,[]
83,"Begin with one cell, which can die, do nothing, transform to 2 or 3 cells, with probability 1/4 respectively. How's the probability of extinction?","Begin with one cell, which can die, do nothing, transform to 2 or 3 cells, with probability 1/4 respectively. How's the probability of extinction?",,"A colony begins with a cell, which can die, do nothing, transform to two or three cells, with probability 1/4 for each case at next time point. Children cells share the same property described above. What's the probability of this colony's extinction? I got two solutions, $1$ and $\sqrt{2}-1$, from a simple recursive equation, $p=\frac{1}{4}(1+p+p^2+p^3)$. But I've no idea which one is correct. Thanks for your help!","A colony begins with a cell, which can die, do nothing, transform to two or three cells, with probability 1/4 for each case at next time point. Children cells share the same property described above. What's the probability of this colony's extinction? I got two solutions, $1$ and $\sqrt{2}-1$, from a simple recursive equation, $p=\frac{1}{4}(1+p+p^2+p^3)$. But I've no idea which one is correct. Thanks for your help!",,"['probability', 'probability-theory']"
84,Blackwell–Girshick equation?,Blackwell–Girshick equation?,,"We have the following theorem: Let $N$ be a random variable assuming positive integer values $1, 2, 3,\dots\,$. Let $(X_i)$ be a sequence of independent random variables which are also independent of $N$ with $V(X_i)$ the same for $i$, and $E(X_i) = E(X)$ the same for $i$. We denote the random walk as $S_N=\sum_{i=1}^{N}X_i$. Then $$V(S_N)=E(N)V(X)+E(X)^2V(N)$$ Is there a variant for the case where $N$ is not independent of $X_i$ ? This is not clear for me, what are the examples for which we have $N$ dependent of $X_i$ ? Thank you","We have the following theorem: Let $N$ be a random variable assuming positive integer values $1, 2, 3,\dots\,$. Let $(X_i)$ be a sequence of independent random variables which are also independent of $N$ with $V(X_i)$ the same for $i$, and $E(X_i) = E(X)$ the same for $i$. We denote the random walk as $S_N=\sum_{i=1}^{N}X_i$. Then $$V(S_N)=E(N)V(X)+E(X)^2V(N)$$ Is there a variant for the case where $N$ is not independent of $X_i$ ? This is not clear for me, what are the examples for which we have $N$ dependent of $X_i$ ? Thank you",,"['probability', 'statistics', 'probability-theory']"
85,A die is rolled until a 6 comes up. Should the sample space of this experiment contain the set of all infinite sequences which do not contain a 6?,A die is rolled until a 6 comes up. Should the sample space of this experiment contain the set of all infinite sequences which do not contain a 6?,,"Is there a standard way to view this? The problem is, In an experiment, die is rolled continually until a 6 appears, at which point the experiment stops. What is the sample space of this experiment? My first instinct was to say that it was the set of all finite sequences with exactly one 6, which is in the final position. But this neglects the (zero-probability) event that a 6 never comes up. Should this be in the sample space? Does it matter?","Is there a standard way to view this? The problem is, In an experiment, die is rolled continually until a 6 appears, at which point the experiment stops. What is the sample space of this experiment? My first instinct was to say that it was the set of all finite sequences with exactly one 6, which is in the final position. But this neglects the (zero-probability) event that a 6 never comes up. Should this be in the sample space? Does it matter?",,['probability']
86,When does convergence in distribution imply convergence in probability?,When does convergence in distribution imply convergence in probability?,,"I was looking at the proof for the Delta Method ( http://en.wikipedia.org/wiki/Delta_method#Proof_in_the_univariate_case ) and there is something I am quite confused about. It gives $\sqrt(n)[X_n - \theta]$ converges in distribution to $\text{N}(0,\sigma^2)$ where $\theta$ is some fixed value. It then says $X_n$ converges in probability to $\theta$. I can't figure out why this is true. Edit: (a solution using Slutzky's) I've thought of another way to see why this is true using Slutzky's theorem. $\sqrt(n)(X_n-\theta) \rightarrow N(0,\sigma^2)$ in distribution by assumption. $1/\sqrt(n) \rightarrow 0$ and thus also converges to 0 in probability. By Slutzky's, $(X_n-\theta) \rightarrow 0$*N(0,$\sigma^2$) = 0 in distribution. Thus, $X_n-\theta \rightarrow 0$ in probability.","I was looking at the proof for the Delta Method ( http://en.wikipedia.org/wiki/Delta_method#Proof_in_the_univariate_case ) and there is something I am quite confused about. It gives $\sqrt(n)[X_n - \theta]$ converges in distribution to $\text{N}(0,\sigma^2)$ where $\theta$ is some fixed value. It then says $X_n$ converges in probability to $\theta$. I can't figure out why this is true. Edit: (a solution using Slutzky's) I've thought of another way to see why this is true using Slutzky's theorem. $\sqrt(n)(X_n-\theta) \rightarrow N(0,\sigma^2)$ in distribution by assumption. $1/\sqrt(n) \rightarrow 0$ and thus also converges to 0 in probability. By Slutzky's, $(X_n-\theta) \rightarrow 0$*N(0,$\sigma^2$) = 0 in distribution. Thus, $X_n-\theta \rightarrow 0$ in probability.",,"['probability', 'convergence-divergence']"
87,Probability (usage of recursion),Probability (usage of recursion),,"In an hour, a bacterium dies with probability $p$ or else splits into two. What is the probability that a single bacterium produces a population that will never become extinct?","In an hour, a bacterium dies with probability $p$ or else splits into two. What is the probability that a single bacterium produces a population that will never become extinct?",,"['probability', 'limits', 'recursion']"
88,"""Square root"" of a normal RV?","""Square root"" of a normal RV?",,"Say $X_1,X_2$ are independently drawn from the same distribution (call it $X$) and that their product, $X_1X_2$ falls on a standard normal distribution. Is it possible to get a pdf or cdf for $X$? My progress: The $n$th moment of a standard normal is $0$ for odd $n$ and $n!!$ for even $n$. Then for even $n$: $\mathbb{E}[(X_1 X_2)^n] = \mathbb{E}[X_1^n] \mathbb{E}[X_2^n] = \mathbb{E}[X^n]^2 = n!! $ Thus the $n$th moment of $X$ is $\mathbb{E}[X^n] = \sqrt{n!!}$ for even $n$ and zero otherwise. Therefore...","Say $X_1,X_2$ are independently drawn from the same distribution (call it $X$) and that their product, $X_1X_2$ falls on a standard normal distribution. Is it possible to get a pdf or cdf for $X$? My progress: The $n$th moment of a standard normal is $0$ for odd $n$ and $n!!$ for even $n$. Then for even $n$: $\mathbb{E}[(X_1 X_2)^n] = \mathbb{E}[X_1^n] \mathbb{E}[X_2^n] = \mathbb{E}[X^n]^2 = n!! $ Thus the $n$th moment of $X$ is $\mathbb{E}[X^n] = \sqrt{n!!}$ for even $n$ and zero otherwise. Therefore...",,"['probability', 'statistics', 'probability-theory', 'random-variables']"
89,Prove that a random walk on $\mathbb{Z}_+\cup \{0\}$ is transient,Prove that a random walk on  is transient,\mathbb{Z}_+\cup \{0\},"Prove that a random walk on $\mathbb{Z}_+ \cup \{0\}$ is transient with $p_{i,i+1}=\frac{i^2+2i+1}{2i^2+2i+1}$ and $p_{i,i-1}=\frac{i^2}{2i^2+2i+1}$. So since this Markov chain has only a single communicating class we only need prove that $0$ is a transient state.  There really isn't much other theory to go off of.  I'm basically just trying to find a general formula for $p_{0,0}^{(n)}$ so that I can show that the infinite series $\sum p_{0,0}^{(n)} <\infty$.  But I cannot for the life of me come up with a combinatorial formula to find $p_{0,0}^{(n)}$.  Can anyone help me come up with this formula?  Thanks.","Prove that a random walk on $\mathbb{Z}_+ \cup \{0\}$ is transient with $p_{i,i+1}=\frac{i^2+2i+1}{2i^2+2i+1}$ and $p_{i,i-1}=\frac{i^2}{2i^2+2i+1}$. So since this Markov chain has only a single communicating class we only need prove that $0$ is a transient state.  There really isn't much other theory to go off of.  I'm basically just trying to find a general formula for $p_{0,0}^{(n)}$ so that I can show that the infinite series $\sum p_{0,0}^{(n)} <\infty$.  But I cannot for the life of me come up with a combinatorial formula to find $p_{0,0}^{(n)}$.  Can anyone help me come up with this formula?  Thanks.",,"['probability', 'stochastic-processes', 'markov-chains', 'random-walk']"
90,Proof of Khintchine's inequality,Proof of Khintchine's inequality,,"I'm trying to understand the proof of Khintchine's inequality in these lecture notes: http://www.math.ubc.ca/~ilaba/wolff/notes_march2002.pdf On page 27, second display-style equation after (51), the author claims $$\mathrm{Prob}\left(\sum_n a_n\omega_n\ge \lambda\right)\le e^{-t\lambda+\frac{t^2}{2}\sum_n a_n^2}$$ I don't understand this conclusion. Where does the exponential function come from? I would be glad if someone could shed light on this. Please don't assume any knowledge in probability, as I don't have any.","I'm trying to understand the proof of Khintchine's inequality in these lecture notes: http://www.math.ubc.ca/~ilaba/wolff/notes_march2002.pdf On page 27, second display-style equation after (51), the author claims $$\mathrm{Prob}\left(\sum_n a_n\omega_n\ge \lambda\right)\le e^{-t\lambda+\frac{t^2}{2}\sum_n a_n^2}$$ I don't understand this conclusion. Where does the exponential function come from? I would be glad if someone could shed light on this. Please don't assume any knowledge in probability, as I don't have any.",,"['probability', 'analysis', 'inequality']"
91,asymptotic order of the variance of the maximum of iid standard Gaussian,asymptotic order of the variance of the maximum of iid standard Gaussian,,"Suppose that $X_1,\cdots,X_n$ are iid standard Gaussian. $X_{(n)}$ is the maximum of $(X_1,\cdots,X_n)$, how can I find the asymptotic order of $VAR[X_{(n)}]$? The density function of $X_{(n)}$ can be obtained as follows: $ P(X_{(n)}<t)=\prod P(X_i< t)=[\Phi(t)]^n, $ So the density is $f(t)=n\phi(t)[\Phi(t)]^{n-1}$. But it's unclear to me about the approximation of the integral: $\int_{-\infty}^\infty t^2n\phi(t)[\Phi(t)]^{n-1} dt$ which is the second moment. I know that the first moment can be bounded by  $$ E(X_{(n)})\le \sqrt{2\log n}-\frac{\log\log n+\log 4\pi - 2\gamma}{2\sqrt{2\log n}}. $$","Suppose that $X_1,\cdots,X_n$ are iid standard Gaussian. $X_{(n)}$ is the maximum of $(X_1,\cdots,X_n)$, how can I find the asymptotic order of $VAR[X_{(n)}]$? The density function of $X_{(n)}$ can be obtained as follows: $ P(X_{(n)}<t)=\prod P(X_i< t)=[\Phi(t)]^n, $ So the density is $f(t)=n\phi(t)[\Phi(t)]^{n-1}$. But it's unclear to me about the approximation of the integral: $\int_{-\infty}^\infty t^2n\phi(t)[\Phi(t)]^{n-1} dt$ which is the second moment. I know that the first moment can be bounded by  $$ E(X_{(n)})\le \sqrt{2\log n}-\frac{\log\log n+\log 4\pi - 2\gamma}{2\sqrt{2\log n}}. $$",,"['probability', 'statistics', 'probability-theory', 'order-statistics']"
92,Simple probability problems which hide important concepts,Simple probability problems which hide important concepts,,"Together with a group of students we need to compose a course on probability theory having the form of a debate. In order to do that we need to decide on a probability concept simple enough so that it could be explained in 10-15 minutes to an audience with basic math knowledge. Still, the concept to be explained must be hidden in some tricky probability problems where intuition does not work. Until now we have two leads: the probability of the union is not necessarily the sum of the probabilities Bayes' law (for a rare disease the probability of testing positive when you are not sick is very large) The second one is clearly not intuitive, but it cannot be explained easily to a general audience. Do you know any other probability issues which are simple enough to explain, but create big difficulties in problems when not applied correctly?","Together with a group of students we need to compose a course on probability theory having the form of a debate. In order to do that we need to decide on a probability concept simple enough so that it could be explained in 10-15 minutes to an audience with basic math knowledge. Still, the concept to be explained must be hidden in some tricky probability problems where intuition does not work. Until now we have two leads: the probability of the union is not necessarily the sum of the probabilities Bayes' law (for a rare disease the probability of testing positive when you are not sick is very large) The second one is clearly not intuitive, but it cannot be explained easily to a general audience. Do you know any other probability issues which are simple enough to explain, but create big difficulties in problems when not applied correctly?",,['probability']
93,Variance of the number of balls between two specified balls,Variance of the number of balls between two specified balls,,"Question: Assume we have 100 different balls numbered from 1 to 100, distributed in 100 different bins, each bin has 1 ball in it. What is the variance of the number of balls in between ball #1 and ball #2? What I did: I defined $X_i$ as an indicator for ball $i$ - ""Is it in between balls 1 and 2?"" Also I thought of the question as this problem: ""We have actually just 3 places to put the 98 remaining balls: before, after and between balls #1,2, so for each ball there is a probability of 1/3 to be in between. So by this we have $E[X_i]= $$1 \over 3$ . Now $X=\sum _{i=1} ^{98} X_i$. Since $X_i$ is a Bernoulli RV then: $V(X_i)=p(1-p)=$$2 \over 9$. But I know that the correct answer is 549 $8 \over 9$. I know that I should somehow use the formula to the sum of variances, but somehow I don't get to the correct answer.","Question: Assume we have 100 different balls numbered from 1 to 100, distributed in 100 different bins, each bin has 1 ball in it. What is the variance of the number of balls in between ball #1 and ball #2? What I did: I defined $X_i$ as an indicator for ball $i$ - ""Is it in between balls 1 and 2?"" Also I thought of the question as this problem: ""We have actually just 3 places to put the 98 remaining balls: before, after and between balls #1,2, so for each ball there is a probability of 1/3 to be in between. So by this we have $E[X_i]= $$1 \over 3$ . Now $X=\sum _{i=1} ^{98} X_i$. Since $X_i$ is a Bernoulli RV then: $V(X_i)=p(1-p)=$$2 \over 9$. But I know that the correct answer is 549 $8 \over 9$. I know that I should somehow use the formula to the sum of variances, but somehow I don't get to the correct answer.",,['probability']
94,Variance of time to find first duplicate,Variance of time to find first duplicate,,"In repeated uniform sampling from $\{1,\dots,n\}$  the mean time $E(X)$ to find the first duplicate is asymptotically $\sqrt{n\pi/2}$. What about  the variance? The variance is $E(X^2) -E(X)^2$. $$E(X^2) = \sum_{x=1}^{\infty} P(X \geq \sqrt{x}) = \sum_{x=1}^{\infty} \prod_{\ell = 1}^{\lfloor \sqrt{x} \rfloor} \frac{n-\ell}{n}$$ $$ \sum_{x=1}^{\infty} \prod_{\ell = 1}^{\lfloor \sqrt{x} \rfloor} \frac{n-\ell}{n} \approx \int_{x=1}^{\infty} e^{\frac{-x}{2n}} \sim 2n$$ So $\mathrm{Var}(X) \sim \left(2-\frac{\pi}{2}\right)n \approx 0.43n$. Can we make this rigorous?","In repeated uniform sampling from $\{1,\dots,n\}$  the mean time $E(X)$ to find the first duplicate is asymptotically $\sqrt{n\pi/2}$. What about  the variance? The variance is $E(X^2) -E(X)^2$. $$E(X^2) = \sum_{x=1}^{\infty} P(X \geq \sqrt{x}) = \sum_{x=1}^{\infty} \prod_{\ell = 1}^{\lfloor \sqrt{x} \rfloor} \frac{n-\ell}{n}$$ $$ \sum_{x=1}^{\infty} \prod_{\ell = 1}^{\lfloor \sqrt{x} \rfloor} \frac{n-\ell}{n} \approx \int_{x=1}^{\infty} e^{\frac{-x}{2n}} \sim 2n$$ So $\mathrm{Var}(X) \sim \left(2-\frac{\pi}{2}\right)n \approx 0.43n$. Can we make this rigorous?",,['probability']
95,"Definition of an ""Experiment"" in Probability","Definition of an ""Experiment"" in Probability",,"One can define the fundamental concepts of probability theory (such as a probability measure, random variable, etc) in a purely axiomatic manner. However, when we teach probability, we start off with the notion of an ""experiment"", a concept it seems to me which is something akin to pornography: difficult to define, but you tend to know it when you see it. So I am curious if there is a general definition of an experiment (or if it something really best regarded more as an explanatory construct). To try to define an experiment as a type of function seems difficult to me b/c it would require the notion of a ""random function"" of some type. Thanks, Jack","One can define the fundamental concepts of probability theory (such as a probability measure, random variable, etc) in a purely axiomatic manner. However, when we teach probability, we start off with the notion of an ""experiment"", a concept it seems to me which is something akin to pornography: difficult to define, but you tend to know it when you see it. So I am curious if there is a general definition of an experiment (or if it something really best regarded more as an explanatory construct). To try to define an experiment as a type of function seems difficult to me b/c it would require the notion of a ""random function"" of some type. Thanks, Jack",,"['probability', 'statistics', 'definition']"
96,Moments and weak convergence of probability measures II,Moments and weak convergence of probability measures II,,"Suppose a sequence of probability measures $\mu_n \Rightarrow \mu$ converges weakly to a limit, and suppose moreover that $$\lim_{n \rightarrow \infty} \int x^k \mu_n (dx) = m_k \in \mathbb{R}$$ for some sequence of numbers $\{m_k\}_{k=1}^{\infty}$.  Is it true that $$\int x^k \mu (dx) = m_k?$$ I believe so, but I can't seem to prove it, since the functions $x^k$ are unbounded.  If anyone could offer any insight, I'd greatly appreciate it!","Suppose a sequence of probability measures $\mu_n \Rightarrow \mu$ converges weakly to a limit, and suppose moreover that $$\lim_{n \rightarrow \infty} \int x^k \mu_n (dx) = m_k \in \mathbb{R}$$ for some sequence of numbers $\{m_k\}_{k=1}^{\infty}$.  Is it true that $$\int x^k \mu (dx) = m_k?$$ I believe so, but I can't seem to prove it, since the functions $x^k$ are unbounded.  If anyone could offer any insight, I'd greatly appreciate it!",,"['real-analysis', 'probability', 'measure-theory']"
97,Expected intersection size of two random sets,Expected intersection size of two random sets,,"There are two sets, $A=\{a_1, \dots, a_n\}$ and $B=\{b_1, \dots, b_m\}$, for some $m$ and $n$. The elements $a_1, \dots, a_n$ and $b_1, \dots, b_m$ are all $x$-bit binary strings, and they are all uniformly sampled from $X$, which is defined as the set of all $x$-bit binary strings. Let $I_{n,m}$ be the value $|A\cap B|$. Obviously, $I_{n,m}$ is a random variable. So, how can I calculate $E[I_{n,m}]$? Note that $A$ and $B$ are multisets. Besides, given $x=3$, if $A$ and $B$ happen to have two $111$'s and three $111$'s, respectively ($\{111,111\}\subseteq A$ and $\{111,111,111\}\subseteq B$), then they only contribute $1$ to $I_{n,m}$.","There are two sets, $A=\{a_1, \dots, a_n\}$ and $B=\{b_1, \dots, b_m\}$, for some $m$ and $n$. The elements $a_1, \dots, a_n$ and $b_1, \dots, b_m$ are all $x$-bit binary strings, and they are all uniformly sampled from $X$, which is defined as the set of all $x$-bit binary strings. Let $I_{n,m}$ be the value $|A\cap B|$. Obviously, $I_{n,m}$ is a random variable. So, how can I calculate $E[I_{n,m}]$? Note that $A$ and $B$ are multisets. Besides, given $x=3$, if $A$ and $B$ happen to have two $111$'s and three $111$'s, respectively ($\{111,111\}\subseteq A$ and $\{111,111,111\}\subseteq B$), then they only contribute $1$ to $I_{n,m}$.",,['probability']
98,Is the multiplicative Chernoff bound stronger than additive one?,Is the multiplicative Chernoff bound stronger than additive one?,,"The multiplicative Chernoff Bound says for $X_i \in \{0,1\}$ that satisfies $\mathbb{E}[X_i] = p$, $$ \mathbb P\left(\sum\limits_i^n{X_i} \geq np(1+\delta)\right) \leq e^{-\frac{1}{3} np\delta^2} \>. $$ The additive version says that  $$ \mathbb P\left(\sum\limits_{i}^nX_i \geq np+n\epsilon \right) \leq e^{-2n\epsilon^2} \>. $$ I wonder if the multiplicative version could be stronger. Let $\epsilon = p \delta$, then the additive Chernoff bound is reduced to  $$ \mathbb P\left(\sum\limits_{i}^{n}{X_i} \geq np(1+\delta)\right) \leq e^{-2np^2\delta^2} \>. $$ This is a much weaker bound when $p \ll 1$. How can these two versions of Chernoff bounds have such a difference? I mean, which step in deriving these two bounds diverges, causing the fact I have illustrated?","The multiplicative Chernoff Bound says for $X_i \in \{0,1\}$ that satisfies $\mathbb{E}[X_i] = p$, $$ \mathbb P\left(\sum\limits_i^n{X_i} \geq np(1+\delta)\right) \leq e^{-\frac{1}{3} np\delta^2} \>. $$ The additive version says that  $$ \mathbb P\left(\sum\limits_{i}^nX_i \geq np+n\epsilon \right) \leq e^{-2n\epsilon^2} \>. $$ I wonder if the multiplicative version could be stronger. Let $\epsilon = p \delta$, then the additive Chernoff bound is reduced to  $$ \mathbb P\left(\sum\limits_{i}^{n}{X_i} \geq np(1+\delta)\right) \leq e^{-2np^2\delta^2} \>. $$ This is a much weaker bound when $p \ll 1$. How can these two versions of Chernoff bounds have such a difference? I mean, which step in deriving these two bounds diverges, causing the fact I have illustrated?",,"['probability', 'inequality']"
99,Disprove independence of vector of Gaussians by independence of marginals,Disprove independence of vector of Gaussians by independence of marginals,,"If we have three random variables $X,Y,Z$, then if $X$ and $Z$ are independent, and $Y$ and $Z$ are independent, it doesn't follow that $Z$ is independent of the vector $(X,Y)$. There is a simple counter example for this. But, I can't find a counter example in the case where all three are Normal, ie $X,Y,Z$ are Gaussian variables, and not multivariate Gaussian. Appreciate any help.","If we have three random variables $X,Y,Z$, then if $X$ and $Z$ are independent, and $Y$ and $Z$ are independent, it doesn't follow that $Z$ is independent of the vector $(X,Y)$. There is a simple counter example for this. But, I can't find a counter example in the case where all three are Normal, ie $X,Y,Z$ are Gaussian variables, and not multivariate Gaussian. Appreciate any help.",,"['probability', 'statistics']"
