,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the counter example?,What is the counter example?,,"The Liouville's theorem states that if $u$ is a non negative, subharmonic function, $L^\infty(\mathbf{R}^n)$, then $u$ is constant ($n\leq2$). Someone knows a counter example if $n>2$, or where can I find this Liouville's theorem?","The Liouville's theorem states that if $u$ is a non negative, subharmonic function, $L^\infty(\mathbf{R}^n)$, then $u$ is constant ($n\leq2$). Someone knows a counter example if $n>2$, or where can I find this Liouville's theorem?",,"['analysis', 'partial-differential-equations', 'harmonic-functions']"
1,Online graph theory analysis tool?,Online graph theory analysis tool?,,"I don't know much (any?) graph theory, but I'm interested in learning a bit, and doing some calculations for a game. Is there a tool online where I could construct a graph (this one has 30-40 vertices, maybe 100 edges), and play around to explore its properties? Useful things to do would be describing paths, finding related paths, and letting me write formulas to calculate the value of a path. (By contrast with Online tool for making graphs (vertices and edges)? , I'm not interested in the presentation, I'm interested in analysis, playing, exploring, manipulating, sharing...)","I don't know much (any?) graph theory, but I'm interested in learning a bit, and doing some calculations for a game. Is there a tool online where I could construct a graph (this one has 30-40 vertices, maybe 100 edges), and play around to explore its properties? Useful things to do would be describing paths, finding related paths, and letting me write formulas to calculate the value of a path. (By contrast with Online tool for making graphs (vertices and edges)? , I'm not interested in the presentation, I'm interested in analysis, playing, exploring, manipulating, sharing...)",,"['analysis', 'graph-theory']"
2,Asymptotic on $\max\{d(n): 1\leq n\leq N\}$.,Asymptotic on .,\max\{d(n): 1\leq n\leq N\},"Question: Let $d(n)=\sum_{d|n}1$ , be the divisor function. Estimate the asymptotic behaver $$D(N):=\max\{d(n):1\leq n\leq N\}$$ when $N$ is large. I know $$\sum_{n=1}^N d(n)=\sum_{n=1}^N [\frac{N}{n}]=N\sum_{n=1}^N\frac{1}{n} + O(N)\approx N{\rm log} N.$$ So $D(N)\geq \frac{1}{N}\sum_{n=1}^N d(n)\approx {\rm log}N$ . But I can't determine if this is optimal, i.e. $D(N)=O({\rm log}N)$ .","Question: Let , be the divisor function. Estimate the asymptotic behaver when is large. I know So . But I can't determine if this is optimal, i.e. .",d(n)=\sum_{d|n}1 D(N):=\max\{d(n):1\leq n\leq N\} N \sum_{n=1}^N d(n)=\sum_{n=1}^N [\frac{N}{n}]=N\sum_{n=1}^N\frac{1}{n} + O(N)\approx N{\rm log} N. D(N)\geq \frac{1}{N}\sum_{n=1}^N d(n)\approx {\rm log}N D(N)=O({\rm log}N),"['analysis', 'analytic-number-theory']"
3,Can we identify the limit of this arithmetic/geometric mean like iteration?,Can we identify the limit of this arithmetic/geometric mean like iteration?,,"Let $a_0 = 1$ and $b_0 = x \ge 1$. Let $$ a_{n+1} = (a_n+\sqrt{a_n b_n})/2, \qquad b_{n+1} = (b_n + \sqrt{a_{n+1} b_n})/2. $$ Numeric computation suggests that regardless of the choice of $x$, $a_n$ and $b_n$ always converge to the same value. Can we prove this? Moreover, if we assume this is true and define $f(x) = \lim_{n \to \infty} a_n$, then numeric computation shows that $$ f'(1) \approx 0.57142857142857 \approx 4/7 \\ f''(1) \approx -8/49 \\ f^{(3)}(1) \approx 1056/4459 \\ f^{(4)}(1) \approx -65664/111475. $$ There seems to be some patterns here. Can we actually find the limit with these clues? I was actually trying to check this recursion $$ a_{n+1} = (a_n+\sqrt{a_n b_n})/2, \qquad b_{n+1} = (b_n + \sqrt{a_{n} b_n})/2. $$ I made a mistaked in my code and computed the one at the top. See The Computer as Crucible , pp 130.","Let $a_0 = 1$ and $b_0 = x \ge 1$. Let $$ a_{n+1} = (a_n+\sqrt{a_n b_n})/2, \qquad b_{n+1} = (b_n + \sqrt{a_{n+1} b_n})/2. $$ Numeric computation suggests that regardless of the choice of $x$, $a_n$ and $b_n$ always converge to the same value. Can we prove this? Moreover, if we assume this is true and define $f(x) = \lim_{n \to \infty} a_n$, then numeric computation shows that $$ f'(1) \approx 0.57142857142857 \approx 4/7 \\ f''(1) \approx -8/49 \\ f^{(3)}(1) \approx 1056/4459 \\ f^{(4)}(1) \approx -65664/111475. $$ There seems to be some patterns here. Can we actually find the limit with these clues? I was actually trying to check this recursion $$ a_{n+1} = (a_n+\sqrt{a_n b_n})/2, \qquad b_{n+1} = (b_n + \sqrt{a_{n} b_n})/2. $$ I made a mistaked in my code and computed the one at the top. See The Computer as Crucible , pp 130.",,"['analysis', 'computer-algebra-systems', 'experimental-mathematics']"
4,a_i are positive integers and not equal to each other .prove : $\frac{a_1^2+a_2^2+\cdots+a_n^2}{a_1+a_2+\cdots+a_n}\geqslant \frac{2n+1}{3}$,a_i are positive integers and not equal to each other .prove :,\frac{a_1^2+a_2^2+\cdots+a_n^2}{a_1+a_2+\cdots+a_n}\geqslant \frac{2n+1}{3},I met this inequality : a_i (i from 1 to n)are positive integers and not equal to each other . Prove : $\displaystyle \frac{a_1^2+a_2^2+\cdots+a_n^2}{a_1+a_2+\cdots+a_n}\geqslant \frac{2n+1}{3}$ I tried : $\displaystyle \Leftrightarrow \frac{n(n+1)}{2}\sum_{i=1}^n a_i^2\geqslant\frac{n(n+1)(2n+1)}{6}\sum_{i=1}^n a_i$ $\displaystyle \Leftrightarrow\sum_{i=1}^n i\sum_{i=1}^na_i^2\geqslant\sum_{i=1}^ni^2\sum_{i=1}^na_i$ It seems right.But i don't know how to prove it.Who can help me. Thanks!,I met this inequality : a_i (i from 1 to n)are positive integers and not equal to each other . Prove : $\displaystyle \frac{a_1^2+a_2^2+\cdots+a_n^2}{a_1+a_2+\cdots+a_n}\geqslant \frac{2n+1}{3}$ I tried : $\displaystyle \Leftrightarrow \frac{n(n+1)}{2}\sum_{i=1}^n a_i^2\geqslant\frac{n(n+1)(2n+1)}{6}\sum_{i=1}^n a_i$ $\displaystyle \Leftrightarrow\sum_{i=1}^n i\sum_{i=1}^na_i^2\geqslant\sum_{i=1}^ni^2\sum_{i=1}^na_i$ It seems right.But i don't know how to prove it.Who can help me. Thanks!,,"['analysis', 'inequality']"
5,Uniqueness of solution for the functional equation $f(x)=\frac{x}{2}f\left(\frac{x}{2}\right)+h(x)$,Uniqueness of solution for the functional equation,f(x)=\frac{x}{2}f\left(\frac{x}{2}\right)+h(x),"Let $h \in \mathcal{C}:=C([-a,a])$, where $a>0$. Prove that there exists a unique function $f \in \mathcal{C}$ such that $$ f(x)=\frac{x}{2}f\Big(\frac{x}{2}\Big)+h(x)\quad \forall x \in [-a,a]. $$","Let $h \in \mathcal{C}:=C([-a,a])$, where $a>0$. Prove that there exists a unique function $f \in \mathcal{C}$ such that $$ f(x)=\frac{x}{2}f\Big(\frac{x}{2}\Big)+h(x)\quad \forall x \in [-a,a]. $$",,"['analysis', 'functional-equations']"
6,twice differentiable function (question from exam),twice differentiable function (question from exam),,"$f$ is twice differentiable, $f(0)=f(1)=0$ and $f''$ is continuous. Prove that there exists $c\in[0,1]$ such that $$\int_0^1f(x)dx=-\frac1{12}f''(c).$$ I haven't progressed much on this problem. A lot of ideas came up to my mind but none seems to work. Obviously, this has something to do with mean value theorem. In fact, we only need to show that there exist $a,b\in[0,1]$ such that $\int_0^1f(x)dx=\frac{f'(a)-f'(b)}{a-b}$. By mean value theorem, there exists $c\in[a,b]$ such that $f''(c)=\frac{f'(a)-f'(b)}{a-b}$. But this does not seem to be the correct path, because we haven't used that $f''$ is continuous. This leads to the second idea to show that $f''(x)$ attains some values below and above $\int_0^1f(x)dx$. So I think we need to work out some inequalities, which I don't have any idea. Anyway, I just started learning calculus for a few weeks. This question is from the previous exam paper, it's the only question I can't solve. Any help is appreciated, thanks.","$f$ is twice differentiable, $f(0)=f(1)=0$ and $f''$ is continuous. Prove that there exists $c\in[0,1]$ such that $$\int_0^1f(x)dx=-\frac1{12}f''(c).$$ I haven't progressed much on this problem. A lot of ideas came up to my mind but none seems to work. Obviously, this has something to do with mean value theorem. In fact, we only need to show that there exist $a,b\in[0,1]$ such that $\int_0^1f(x)dx=\frac{f'(a)-f'(b)}{a-b}$. By mean value theorem, there exists $c\in[a,b]$ such that $f''(c)=\frac{f'(a)-f'(b)}{a-b}$. But this does not seem to be the correct path, because we haven't used that $f''$ is continuous. This leads to the second idea to show that $f''(x)$ attains some values below and above $\int_0^1f(x)dx$. So I think we need to work out some inequalities, which I don't have any idea. Anyway, I just started learning calculus for a few weeks. This question is from the previous exam paper, it's the only question I can't solve. Any help is appreciated, thanks.",,['analysis']
7,Sobolev space on closed subset of the real line,Sobolev space on closed subset of the real line,,"Everywhere I look in the literature, Sobolev spaces are defined on an open subset of the real line. What are the technical issues with defining a Sobolev space on a closed subset, i.e. are there problems at the boundary, and does anyone know any good references that cover this? My main purpose is to prove $H^1([0,T];\mathbb{R}) =  \{ x \in L^2([0,T];\mathbb{R}) : ||x'||_{L^2} + \gamma^{2}||x||_{L^2} < \infty \}$ is a reproducing kernel Hilbert space. I can do this for $(0,T)$ and want to know if the proof is transferable to the case of the closed interval $[0,T]$. Many thanks, Matthew.","Everywhere I look in the literature, Sobolev spaces are defined on an open subset of the real line. What are the technical issues with defining a Sobolev space on a closed subset, i.e. are there problems at the boundary, and does anyone know any good references that cover this? My main purpose is to prove $H^1([0,T];\mathbb{R}) =  \{ x \in L^2([0,T];\mathbb{R}) : ||x'||_{L^2} + \gamma^{2}||x||_{L^2} < \infty \}$ is a reproducing kernel Hilbert space. I can do this for $(0,T)$ and want to know if the proof is transferable to the case of the closed interval $[0,T]$. Many thanks, Matthew.",,"['analysis', 'sobolev-spaces']"
8,Show there exists no diffeomorphism between positive real line and a spiral,Show there exists no diffeomorphism between positive real line and a spiral,,"I'm given the following two subset of $\mathbb{R}^2$ : $$ M = \{(t,0) \in \mathbb{R}^2 \mid t>0\},  \quad  N = \{(t \cos(1/t),\, t \sin(1/t)) \in \mathbb{R}^2 \mid t>0 \}  $$ The goal of the exercise is to show that there exists no diffeomorphism $\varphi: \mathbb{R}^2 \to \mathbb{R}^2 $ such that $\varphi (M) = N$ . In a previous exercise I showed that there exists a homeomorphism such that the same conditions are fullfiled. I'm also given the hint that one should start by showing that the partial derivatives with respect to x of such a diffeomorphism $\varphi: \mathbb{R}^2 \to \mathbb{R}^2$ (if he would exist) would vanish in $(0,0)$ . I know how to finish the exercise if I find a way of showing that the partial derivatives with respect to x vanish at $(0,0)$ . Then the Jacobian determinant is zero at $(0,0)$ so the inverse function is not differentiable at $(0,0)$ and I find a contradiction to the existence of a differeomorphic $\varphi$ . So far I tried to use the difference quotient $$ \lim_{h\to 0} \frac{\varphi_1(x+h,y) - \varphi_1(x,y)}{h}.  $$ For this Limit to be zero the numerator has to decrease faster than the linear denominator. I figure that I will have to use that the line segment around $(0,0)$ in $N$ is infintely long while the length of the line segment of the preimage is finite but so far I'm not sure how to combine this with the difference quotient. The concept of differentiating an unknown function is not something I encountered before so I would be thankful for any help.",I'm given the following two subset of : The goal of the exercise is to show that there exists no diffeomorphism such that . In a previous exercise I showed that there exists a homeomorphism such that the same conditions are fullfiled. I'm also given the hint that one should start by showing that the partial derivatives with respect to x of such a diffeomorphism (if he would exist) would vanish in . I know how to finish the exercise if I find a way of showing that the partial derivatives with respect to x vanish at . Then the Jacobian determinant is zero at so the inverse function is not differentiable at and I find a contradiction to the existence of a differeomorphic . So far I tried to use the difference quotient For this Limit to be zero the numerator has to decrease faster than the linear denominator. I figure that I will have to use that the line segment around in is infintely long while the length of the line segment of the preimage is finite but so far I'm not sure how to combine this with the difference quotient. The concept of differentiating an unknown function is not something I encountered before so I would be thankful for any help.,"\mathbb{R}^2 
M = \{(t,0) \in \mathbb{R}^2 \mid t>0\}, 
\quad 
N = \{(t \cos(1/t),\, t \sin(1/t)) \in \mathbb{R}^2 \mid t>0 \} 
 \varphi: \mathbb{R}^2 \to \mathbb{R}^2  \varphi (M) = N \varphi: \mathbb{R}^2 \to \mathbb{R}^2 (0,0) (0,0) (0,0) (0,0) \varphi 
\lim_{h\to 0} \frac{\varphi_1(x+h,y) - \varphi_1(x,y)}{h}. 
 (0,0) N","['analysis', 'derivatives', 'differential-geometry', 'differential-topology']"
9,Why is multiplication on the space of smooth functions with compact support continuous?,Why is multiplication on the space of smooth functions with compact support continuous?,,"I was reading Terence Tao post https://terrytao.wordpress.com/2009/04/19/245c-notes-3-distributions/ and i'm not able to prove the last item of exercise 4. I have a map $F:C_c^{\infty}(\mathbb R^d)\times C_c^{\infty}(\mathbb R^d)\to C_c^{\infty}(\mathbb R^d)$ given by $F(f,g) = fg$. The question is: Why is $F$ continuous? I proved that if a sequence $(f_n,g_n)$ converges to $(f,g)$ then $F(f_n,g_n) \to F(f,g)$, that is, $F$ is sequentially continuous. But, as far as i know, this does not implies that $F$ is continuous. The topology of $C_c^{\infty}(\mathbb R^d)$ is given by seminorms $p:C_c^{\infty}(\mathbb R^d) \to \mathbb R_{\geq 0}$ such that $p\big|_{C_c^{\infty}( K)}:{C_c^{\infty}( K)} \to \mathbb R_{\geq 0}$ is continuous for every $K\subset \mathbb R^d$ compact, the topology of ${C_c^{\infty}( K)}$ is given by the seminorms $ f\mapsto \sup_{x\in K} |\partial^{\alpha} f(x)|$, $\alpha \in \mathbb N^d,$ and $C_c^{\infty}( K)$ is a Fréchet space.","I was reading Terence Tao post https://terrytao.wordpress.com/2009/04/19/245c-notes-3-distributions/ and i'm not able to prove the last item of exercise 4. I have a map $F:C_c^{\infty}(\mathbb R^d)\times C_c^{\infty}(\mathbb R^d)\to C_c^{\infty}(\mathbb R^d)$ given by $F(f,g) = fg$. The question is: Why is $F$ continuous? I proved that if a sequence $(f_n,g_n)$ converges to $(f,g)$ then $F(f_n,g_n) \to F(f,g)$, that is, $F$ is sequentially continuous. But, as far as i know, this does not implies that $F$ is continuous. The topology of $C_c^{\infty}(\mathbb R^d)$ is given by seminorms $p:C_c^{\infty}(\mathbb R^d) \to \mathbb R_{\geq 0}$ such that $p\big|_{C_c^{\infty}( K)}:{C_c^{\infty}( K)} \to \mathbb R_{\geq 0}$ is continuous for every $K\subset \mathbb R^d$ compact, the topology of ${C_c^{\infty}( K)}$ is given by the seminorms $ f\mapsto \sup_{x\in K} |\partial^{\alpha} f(x)|$, $\alpha \in \mathbb N^d,$ and $C_c^{\infty}( K)$ is a Fréchet space.",,"['analysis', 'distribution-theory', 'topological-vector-spaces']"
10,counterexample for ill posedness of the laplace equation,counterexample for ill posedness of the laplace equation,,"Consider the wave equation with initial data: $$u_{tt}(t,x) + u_{xx}(t,x) = 0$$ $$u(0,x) = u_0(x)$$ $$u_t(0,x) = u_1(x)$$ Hadamard showed that this problem is ill-posed : there exist large solutions with arbitrarily small initial data. For instance, if we take $u(t,x) = a_{\omega} \sinh(\omega t) \sin(\omega x)$, then $u_0(x) = 0$ and $u_1(x) = a_{\omega} \omega \sin (\omega x)$, then we can make $u(t,x)$ grow arbitrarily fast while keeping $u_0$ and $u_1$ small. Tweaking this construction, it is not hard to see that for any given $k$ and $\epsilon > 0$, we can construct initial data such that $$ ||u_0||_{\infty} + ||u_0^{(1)}||_{\infty} + \ldots + ||u_0^{(k)}||_{\infty} + ||u_1||_{\infty} + ||u_1^{(1)}||_{\infty} + \ldots + ||u_1^{(k)}||_{\infty} < \epsilon $$ and $||u(\epsilon,x)||_{\infty}  > \frac{1}{\epsilon}$. This can be interpreted as saying that the problem is ill-posed even in a Holder sense. My question is: can one construct an example of a solution $u(t,x)$ with initial data $u_0(x)$ and $u_1(x)$ such that $$\sum_{i=0}^{\infty} ||u_0^{(i)}||_{\infty} + ||u_1^{(i)}||_{\infty} < \epsilon$$ while $||u(\epsilon,x)||_{\infty}  > \frac{1}{\epsilon}$? This is exercise 26 in http://www.math.mcgill.ca/gantumur/math580f11/downloads/notes2.pdf , which suggests that it should be possible. An explicit construction may be difficult, in which case I would be happy with an abstract argument for existence.","Consider the wave equation with initial data: $$u_{tt}(t,x) + u_{xx}(t,x) = 0$$ $$u(0,x) = u_0(x)$$ $$u_t(0,x) = u_1(x)$$ Hadamard showed that this problem is ill-posed : there exist large solutions with arbitrarily small initial data. For instance, if we take $u(t,x) = a_{\omega} \sinh(\omega t) \sin(\omega x)$, then $u_0(x) = 0$ and $u_1(x) = a_{\omega} \omega \sin (\omega x)$, then we can make $u(t,x)$ grow arbitrarily fast while keeping $u_0$ and $u_1$ small. Tweaking this construction, it is not hard to see that for any given $k$ and $\epsilon > 0$, we can construct initial data such that $$ ||u_0||_{\infty} + ||u_0^{(1)}||_{\infty} + \ldots + ||u_0^{(k)}||_{\infty} + ||u_1||_{\infty} + ||u_1^{(1)}||_{\infty} + \ldots + ||u_1^{(k)}||_{\infty} < \epsilon $$ and $||u(\epsilon,x)||_{\infty}  > \frac{1}{\epsilon}$. This can be interpreted as saying that the problem is ill-posed even in a Holder sense. My question is: can one construct an example of a solution $u(t,x)$ with initial data $u_0(x)$ and $u_1(x)$ such that $$\sum_{i=0}^{\infty} ||u_0^{(i)}||_{\infty} + ||u_1^{(i)}||_{\infty} < \epsilon$$ while $||u(\epsilon,x)||_{\infty}  > \frac{1}{\epsilon}$? This is exercise 26 in http://www.math.mcgill.ca/gantumur/math580f11/downloads/notes2.pdf , which suggests that it should be possible. An explicit construction may be difficult, in which case I would be happy with an abstract argument for existence.",,"['analysis', 'partial-differential-equations', 'examples-counterexamples', 'harmonic-functions']"
11,What is the radius of the largest $k$-dimensional ball that fits in an $n$ dimensional unit hypercube?,What is the radius of the largest -dimensional ball that fits in an  dimensional unit hypercube?,k n,"This question is adapted from another question on the 2008 Putnam test which asks specifically for the case when $n = 4$ and $k = 2$. The answer is $\dfrac{1}{2}\sqrt{\dfrac{n}{k}}$ but I am looking for a proof or other justification. In many of the attempts I have seen for similar problems, the hypercube is scaled by a factor of $2$ and centered at the origin. Here are some posts regarding the Putnam and a discussion for the case when $k = 2$ for all values of $n$. I would also be grateful for intuition regarding why the maximum radius increases and decreases with square roots when the dimension of the ball and cube are changed.","This question is adapted from another question on the 2008 Putnam test which asks specifically for the case when $n = 4$ and $k = 2$. The answer is $\dfrac{1}{2}\sqrt{\dfrac{n}{k}}$ but I am looking for a proof or other justification. In many of the attempts I have seen for similar problems, the hypercube is scaled by a factor of $2$ and centered at the origin. Here are some posts regarding the Putnam and a discussion for the case when $k = 2$ for all values of $n$. I would also be grateful for intuition regarding why the maximum radius increases and decreases with square roots when the dimension of the ball and cube are changed.",,"['analysis', 'volume']"
12,Comparing two close numbers,Comparing two close numbers,,"How to compare these two numbers without using a calculator ? $A=\left(\dfrac{11}{10}\right)^{\sqrt{5}}$ and $\;B=\left(\dfrac{12}{11}\right)^{\sqrt{6}}$ . Thanks for your help ! Here is what I tried for example : $$\left(\frac{A}{B}\right)^{\sqrt6-\sqrt5}=\frac{11}{10^{\sqrt{30}−5}12^{6−\sqrt{30}}}.$$ ln is concave, so $$10^{\sqrt{30}−5}12^{6−\sqrt{30}}\leq10(\sqrt{30}−5))+12(6−\sqrt{30})=22−2\sqrt{30}.$$ But $$22−2\sqrt{30}\approx11,05...$$","How to compare these two numbers without using a calculator ? and . Thanks for your help ! Here is what I tried for example : ln is concave, so But","A=\left(\dfrac{11}{10}\right)^{\sqrt{5}} \;B=\left(\dfrac{12}{11}\right)^{\sqrt{6}} \left(\frac{A}{B}\right)^{\sqrt6-\sqrt5}=\frac{11}{10^{\sqrt{30}−5}12^{6−\sqrt{30}}}. 10^{\sqrt{30}−5}12^{6−\sqrt{30}}\leq10(\sqrt{30}−5))+12(6−\sqrt{30})=22−2\sqrt{30}. 22−2\sqrt{30}\approx11,05...","['analysis', 'inequality']"
13,Divergent Series,Divergent Series,,"Thinking about divergent series and ways of ""summing"" them, they seem to fall into two categories (roughly): Series like $\sum_{k=1}^\infty \frac{1}{k}$, which defy all kinds of regularization or summing methods. Series which can be summed, in one way or another, like $\sum_{k=1}^\infty k^s$, which for at least $s \geq 1$ can be seen as $\zeta(-s)$. My question is: Can the groundfields $\mathbb{R}$ or $\mathbb{C}$ be extented in some ways (like the Hyperreals $\mathbb{R}^*$, or the surreals $\mathbb{S}_\mathbb{R}$ or the surcomplex numbers $\mathbb{S}_\mathbb{C}$), such that you can sum all the divergent series (at least the second type from above) in a more rigorous fashion than the bag of tricks that are usually used to explain these results. This sounds like a ""sort of"" completion of the reals with all the summable but divergent series in it. I would like to know if such a beast exist, what its properties are and if the $\sum_{k=1}^\infty k^s=\zeta(-s)$ results can be explained from this.","Thinking about divergent series and ways of ""summing"" them, they seem to fall into two categories (roughly): Series like $\sum_{k=1}^\infty \frac{1}{k}$, which defy all kinds of regularization or summing methods. Series which can be summed, in one way or another, like $\sum_{k=1}^\infty k^s$, which for at least $s \geq 1$ can be seen as $\zeta(-s)$. My question is: Can the groundfields $\mathbb{R}$ or $\mathbb{C}$ be extented in some ways (like the Hyperreals $\mathbb{R}^*$, or the surreals $\mathbb{S}_\mathbb{R}$ or the surcomplex numbers $\mathbb{S}_\mathbb{C}$), such that you can sum all the divergent series (at least the second type from above) in a more rigorous fashion than the bag of tricks that are usually used to explain these results. This sounds like a ""sort of"" completion of the reals with all the summable but divergent series in it. I would like to know if such a beast exist, what its properties are and if the $\sum_{k=1}^\infty k^s=\zeta(-s)$ results can be explained from this.",,"['analysis', 'divergent-series', 'regularization']"
14,Can any function be upper bounded by a separable function?,Can any function be upper bounded by a separable function?,,"Given a function $f(x,y)$, can we always find functions $h(x), g(y)$ such that $$f(x,y) \leq h(x) + g(y)$$ for all $x,y, \geq 0$? Note that I have placed no restrictions on the functions $f(x,y), g(x), h(y)$ above. Now perhaps this will fall out automatically from of the answer, but I would also be interested to know if it makes any difference whether $f(x,y)$ is continuous or smooth, and if the answer is yes in that case, whether $h(x)$ and $g(y)$ can then be taken to be continuous/smooth as well.","Given a function $f(x,y)$, can we always find functions $h(x), g(y)$ such that $$f(x,y) \leq h(x) + g(y)$$ for all $x,y, \geq 0$? Note that I have placed no restrictions on the functions $f(x,y), g(x), h(y)$ above. Now perhaps this will fall out automatically from of the answer, but I would also be interested to know if it makes any difference whether $f(x,y)$ is continuous or smooth, and if the answer is yes in that case, whether $h(x)$ and $g(y)$ can then be taken to be continuous/smooth as well.",,['analysis']
15,Hausdorff Dimension of Set of Measure Zero,Hausdorff Dimension of Set of Measure Zero,,It's clear that every $A \subset \mathbb R^n $ with $\dim_H(A) < n$ we have $\mathcal H^n(A) = 0$. Is there any $A \subset \mathbb R^n $ with $\mathcal H^n(A) = 0$ but $\dim_H(A) = n$? Thank you.,It's clear that every $A \subset \mathbb R^n $ with $\dim_H(A) < n$ we have $\mathcal H^n(A) = 0$. Is there any $A \subset \mathbb R^n $ with $\mathcal H^n(A) = 0$ but $\dim_H(A) = n$? Thank you.,,"['analysis', 'measure-theory', 'geometric-measure-theory']"
16,"In characteristic $2$, can a function have a non-zero second derivative?","In characteristic , can a function have a non-zero second derivative?",2,"Let $\mathbb F$ be a field with characteristic $2$ , and with a non-trivial absolute value. (The simplest example is $\mathbb F_2(T)$ , the field of formal rational expressions with coefficients in $\mathbb F_2$ . The absolute value of a polynomial is $|p(T)|=2^{-n}\in\mathbb R$ , where $p(T)$ has a factor of $T^n$ but not $T^{n+1}$ .) Let $f:\mathbb F\to\mathbb F$ be a function. Limits and derivatives are defined as usual: $\lim_{x\to a}f(x)=b$ means that for any real (or rational) $\varepsilon>0$ there exists $\delta>0$ such that, for all $x\in\mathbb F$ where $0<|x-a|<\delta$ , $|f(x)-b|<\varepsilon$ . And $$f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}.$$ Is it possible that $f''(a)$ exists in $\mathbb F\setminus\{0\}$ ? If $f=g\cdot h$ where $g$ and $h$ are twice differentiable, the product rule gives $$f''=g''h+2g'h'+gh''$$ $$=g''h+gh''.$$ If $f=g\circ h$ , the chain rule gives $$f''=(g''\circ h)\cdot h'^2+(g'\circ h)\cdot h''.$$ Any polynomial function $p(x)=\sum a_nx^n$ has second derivative $p''(x)=\sum a_nn(n-1)x^{n-2}=0$ because $n(n-1)$ is always even. The derivatives of the reciprocal $f(x)=1/x$ are $f'(x)=-1/x^2$ and $f''(x)=2/x^3=0$ . It follows from the above properties that any rational function $f(x)=p(x)/q(x)$ also has $f''(x)=0$ . The square root function satisfies $f(x)^2-x=0$ ; differentiating this gives $-1=0$ , so it must not be differentiable. I'm not sure about general algebraic functions. I thought of using the symmetric second derivative formula, but I don't think it's valid: $$f''(a)\overset?=\lim_{x\to0}\frac{f(a+x)-2f(a)+f(a-x)}{x^2}=\lim_{x\to0}\frac{0}{x^2}$$ Well, let's apply the definition directly and see what happens: $$f''(a)=\lim_{x\to0}\frac{f'(a+x)-f'(a)}{x}$$ $$=\lim_{x\to0}\frac{\lim_{y\to0}\frac{f(a+x+y)-f(a+x)}{y}-\lim_{y\to0}\frac{f(a+y)-f(a)}{y}}{x}$$ $$=\lim_{x\to0}\lim_{y\to0}\frac{f(a+x+y)-f(a+x)-f(a+y)+f(a)}{xy}$$ We can't take $y=x$ here to get $f''=0$ , because it's not a single limit $(x,y)\to(0,0)$ but two nested limits. An equivalent description of differentiability is that there's some continuous function $h$ with $h(0)=0$ such that $$f(a+x)=f(a)+xf'(a)+xh(x).$$ So the second derivative is $$f''(a)=\lim_{x\to0}\lim_{y\to0}\frac{(x+y)h(x+y)-xh(x)-yh(y)}{xy}$$ $$=\lim_{x\to0}\lim_{y\to0}\left(\frac{h(x+y)-h(x)}{y}+\frac{h(x+y)-h(y)}{x}\right)$$ $$=\lim_{x\to0}\left(\lim_{y\to0}\frac{h(x+y)-h(x)}{y}+\frac{h(x)}{x}\right)$$ $$=\lim_{x\to0}\left(h'(x)+\frac{h(x)}{x}\right).$$ Note that all of these limits must exist, in order for $f''(a)$ to exist. But we can't simplify this further, since $h'$ may be discontinuous or undefined at $0$ . We may even consider the special case where $f$ has a $2$ nd-order Peano derivative: there's some continuous function $g$ with $g(0)=0$ such that $$f(a+x)=f(a)+xf'(a)+x^2f^\ddagger(a)+x^2g(x).$$ Then the second derivative is $$f''(a)=2f^\ddagger(a)+\lim_{x\to0}xg'(x).$$ Is it possible that this limit exists in $\mathbb F\setminus\{0\}$ ?","Let be a field with characteristic , and with a non-trivial absolute value. (The simplest example is , the field of formal rational expressions with coefficients in . The absolute value of a polynomial is , where has a factor of but not .) Let be a function. Limits and derivatives are defined as usual: means that for any real (or rational) there exists such that, for all where , . And Is it possible that exists in ? If where and are twice differentiable, the product rule gives If , the chain rule gives Any polynomial function has second derivative because is always even. The derivatives of the reciprocal are and . It follows from the above properties that any rational function also has . The square root function satisfies ; differentiating this gives , so it must not be differentiable. I'm not sure about general algebraic functions. I thought of using the symmetric second derivative formula, but I don't think it's valid: Well, let's apply the definition directly and see what happens: We can't take here to get , because it's not a single limit but two nested limits. An equivalent description of differentiability is that there's some continuous function with such that So the second derivative is Note that all of these limits must exist, in order for to exist. But we can't simplify this further, since may be discontinuous or undefined at . We may even consider the special case where has a nd-order Peano derivative: there's some continuous function with such that Then the second derivative is Is it possible that this limit exists in ?","\mathbb F 2 \mathbb F_2(T) \mathbb F_2 |p(T)|=2^{-n}\in\mathbb R p(T) T^n T^{n+1} f:\mathbb F\to\mathbb F \lim_{x\to a}f(x)=b \varepsilon>0 \delta>0 x\in\mathbb F 0<|x-a|<\delta |f(x)-b|<\varepsilon f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}. f''(a) \mathbb F\setminus\{0\} f=g\cdot h g h f''=g''h+2g'h'+gh'' =g''h+gh''. f=g\circ h f''=(g''\circ h)\cdot h'^2+(g'\circ h)\cdot h''. p(x)=\sum a_nx^n p''(x)=\sum a_nn(n-1)x^{n-2}=0 n(n-1) f(x)=1/x f'(x)=-1/x^2 f''(x)=2/x^3=0 f(x)=p(x)/q(x) f''(x)=0 f(x)^2-x=0 -1=0 f''(a)\overset?=\lim_{x\to0}\frac{f(a+x)-2f(a)+f(a-x)}{x^2}=\lim_{x\to0}\frac{0}{x^2} f''(a)=\lim_{x\to0}\frac{f'(a+x)-f'(a)}{x} =\lim_{x\to0}\frac{\lim_{y\to0}\frac{f(a+x+y)-f(a+x)}{y}-\lim_{y\to0}\frac{f(a+y)-f(a)}{y}}{x} =\lim_{x\to0}\lim_{y\to0}\frac{f(a+x+y)-f(a+x)-f(a+y)+f(a)}{xy} y=x f''=0 (x,y)\to(0,0) h h(0)=0 f(a+x)=f(a)+xf'(a)+xh(x). f''(a)=\lim_{x\to0}\lim_{y\to0}\frac{(x+y)h(x+y)-xh(x)-yh(y)}{xy} =\lim_{x\to0}\lim_{y\to0}\left(\frac{h(x+y)-h(x)}{y}+\frac{h(x+y)-h(y)}{x}\right) =\lim_{x\to0}\left(\lim_{y\to0}\frac{h(x+y)-h(x)}{y}+\frac{h(x)}{x}\right) =\lim_{x\to0}\left(h'(x)+\frac{h(x)}{x}\right). f''(a) h' 0 f 2 g g(0)=0 f(a+x)=f(a)+xf'(a)+x^2f^\ddagger(a)+x^2g(x). f''(a)=2f^\ddagger(a)+\lim_{x\to0}xg'(x). \mathbb F\setminus\{0\}","['analysis', 'derivatives', 'field-theory', 'positive-characteristic']"
17,Covering number of the standard simplex,Covering number of the standard simplex,,"The covering number of the standard simplex is the minimal number of $k$-dimensional balls of radius $\epsilon$ that suffices for covering the probability simplex $\{x \in \mathbb{R}^{k} : x_1 + \dots + x_k = 1, x_i \ge 0, i=1, \dots, k \}$. A simple argument shows that this cover number is asymptotically $\Theta(\epsilon^{1-k})$. Is there a standard reference for this claim?","The covering number of the standard simplex is the minimal number of $k$-dimensional balls of radius $\epsilon$ that suffices for covering the probability simplex $\{x \in \mathbb{R}^{k} : x_1 + \dots + x_k = 1, x_i \ge 0, i=1, \dots, k \}$. A simple argument shows that this cover number is asymptotically $\Theta(\epsilon^{1-k})$. Is there a standard reference for this claim?",,['analysis']
18,Rational analysis,Rational analysis,,"I found myself thinking about how much of real analysis that can also be developed within the rational numbers. Of course, $\Bbb Q$ is lacking what is perhaps the most important property of the real numbers, the Cauchy completeness (or, equivalently, the existence of the least upper bound). But still, it appears you could get quite far in $\Bbb Q$. First of all, elementary notions like continuity, limits and differentiability are easily definable (though even among the real functions that can be restricted to operators on $\Bbb Q$, not all of them will satisfy these definitions). Riemann integrability can also be defined in a limited form: In real analysis, the definition of the integral involves the supremum and the infimum. But we can define rational Riemann integrability if the upper and lower bounds in question do exist in $\Bbb Q$ and satisfy the appropriate relations, which will of course often be the case. But obviously, a function like $\Bbb Q\setminus\{0\}\ni x\mapsto 1/x$ is not integrable, since $\int_a^b \tfrac 1 x d x = \log(\tfrac b a)$ is not rational for unequal $a,b\in\Bbb Q$. But still, it seems we can get quite far. Does someone have an impression of how far we can get? Also, I have been totally unable to find any mention of rational analysis, as poor as it may be, in literature. Seeing that how far I could get, I find that strange. Does someone have an opinion about why this is the case.","I found myself thinking about how much of real analysis that can also be developed within the rational numbers. Of course, $\Bbb Q$ is lacking what is perhaps the most important property of the real numbers, the Cauchy completeness (or, equivalently, the existence of the least upper bound). But still, it appears you could get quite far in $\Bbb Q$. First of all, elementary notions like continuity, limits and differentiability are easily definable (though even among the real functions that can be restricted to operators on $\Bbb Q$, not all of them will satisfy these definitions). Riemann integrability can also be defined in a limited form: In real analysis, the definition of the integral involves the supremum and the infimum. But we can define rational Riemann integrability if the upper and lower bounds in question do exist in $\Bbb Q$ and satisfy the appropriate relations, which will of course often be the case. But obviously, a function like $\Bbb Q\setminus\{0\}\ni x\mapsto 1/x$ is not integrable, since $\int_a^b \tfrac 1 x d x = \log(\tfrac b a)$ is not rational for unequal $a,b\in\Bbb Q$. But still, it seems we can get quite far. Does someone have an impression of how far we can get? Also, I have been totally unable to find any mention of rational analysis, as poor as it may be, in literature. Seeing that how far I could get, I find that strange. Does someone have an opinion about why this is the case.",,"['analysis', 'rational-numbers']"
19,"$W^{1,p} $ and $W^{2,p}$ Estimates.",and  Estimates.,"W^{1,p}  W^{2,p}","In the beginning of section 4 in here the author says that one can easily adapt the methods in the preceding section to obtain $W^{1,p}$ estimate. I'm trying to do this. I think the following: the lemma 7 is replaced by Lemma: There is a constant $N_1$ so that for any $\varepsilon >0, \exists \delta = \delta(\varepsilon)>0$  and if $u$ is a solution of     $$ \Delta u = f \quad \mbox{in}\quad \Omega,$$     in a domain $\Omega \supset B_4,$ with      $$\left \{\mathcal{M}(|f|^2) \le \delta^2\right \} \cap \left \{\mathcal{M}(|\nabla u|^2) \le 1 \right \} \cap B_1 \neq \emptyset$$     then     $$|\left \{ \mathcal{M}(|\nabla u|^2)>N_1^2 \right \} \cap B_1| < \varepsilon |B_1|.$$ Here $$\mathcal{M}v(x) = \sup_{r>0}\dfrac{1}{|B_r(x)|}\int_{B_r(x)}|v|$$ is the maximal function of $v.$ As in here there is a point $x_0 \in B_1$ so that $$ \dfrac{1}{|B_r(x_0)|}\int_{B_r(x_0)} |\nabla u |^2 \le 1  \quad \mbox{and } \quad \dfrac{1}{|B_r(x_0)|}\int_{B_r(x_0)}|f|^2 \le \delta^2$$ and consequently $$ \int_{B_4} |\nabla u|^2 \le 2^n \quad \mbox{and} \quad \int_{B_4}|f|^2 \le 2^n \delta^2  \tag{1}$$ by Poincaré and the first inequality we have \begin{equation} \int_{B_4}|  u - \overline{u}_{B_4}|^2 \le C_1 \end{equation} In lemma 7 in the same way we get \begin{equation} \int_{B_4}| \nabla u - \overline{\nabla u}_{B_4}|^2 \le C_1 \end{equation} and considering $v$ the solution of the following equation $$ \left \{ \begin{array}{rclcl} \Delta v & = & 0 & \mbox{in} & B_4 \\ v&=& u - (\overline{\nabla u})_{B_4}\cdot x - \overline{u}_{B_4} & \mbox{on}&  \partial B_4 \end{array} \right . $$ Then by minimality of harmonic functions with respect to energy in $B_4.$ \begin{equation} \int_{B_4} | \nabla v|^2 \le \int_{B_4}| \nabla u - \overline{\nabla u}_{B_4}|^2 \le C_1. \end{equation} So as $\nabla v$ is also harmonic by gradient estiamte (of the harmonic function $\nabla v$) we have \begin{equation} \|D^2 v\|_{L^\infty(B_3)}\le N_0^2. \end{equation} Then my intention is also to obtain universal control as below \begin{equation} \| \nabla v \|_{L^\infty(B_3)} \le N_0^2. \end{equation} Am I right? What should I do? Thank you. I will be very grateful.","In the beginning of section 4 in here the author says that one can easily adapt the methods in the preceding section to obtain $W^{1,p}$ estimate. I'm trying to do this. I think the following: the lemma 7 is replaced by Lemma: There is a constant $N_1$ so that for any $\varepsilon >0, \exists \delta = \delta(\varepsilon)>0$  and if $u$ is a solution of     $$ \Delta u = f \quad \mbox{in}\quad \Omega,$$     in a domain $\Omega \supset B_4,$ with      $$\left \{\mathcal{M}(|f|^2) \le \delta^2\right \} \cap \left \{\mathcal{M}(|\nabla u|^2) \le 1 \right \} \cap B_1 \neq \emptyset$$     then     $$|\left \{ \mathcal{M}(|\nabla u|^2)>N_1^2 \right \} \cap B_1| < \varepsilon |B_1|.$$ Here $$\mathcal{M}v(x) = \sup_{r>0}\dfrac{1}{|B_r(x)|}\int_{B_r(x)}|v|$$ is the maximal function of $v.$ As in here there is a point $x_0 \in B_1$ so that $$ \dfrac{1}{|B_r(x_0)|}\int_{B_r(x_0)} |\nabla u |^2 \le 1  \quad \mbox{and } \quad \dfrac{1}{|B_r(x_0)|}\int_{B_r(x_0)}|f|^2 \le \delta^2$$ and consequently $$ \int_{B_4} |\nabla u|^2 \le 2^n \quad \mbox{and} \quad \int_{B_4}|f|^2 \le 2^n \delta^2  \tag{1}$$ by Poincaré and the first inequality we have \begin{equation} \int_{B_4}|  u - \overline{u}_{B_4}|^2 \le C_1 \end{equation} In lemma 7 in the same way we get \begin{equation} \int_{B_4}| \nabla u - \overline{\nabla u}_{B_4}|^2 \le C_1 \end{equation} and considering $v$ the solution of the following equation $$ \left \{ \begin{array}{rclcl} \Delta v & = & 0 & \mbox{in} & B_4 \\ v&=& u - (\overline{\nabla u})_{B_4}\cdot x - \overline{u}_{B_4} & \mbox{on}&  \partial B_4 \end{array} \right . $$ Then by minimality of harmonic functions with respect to energy in $B_4.$ \begin{equation} \int_{B_4} | \nabla v|^2 \le \int_{B_4}| \nabla u - \overline{\nabla u}_{B_4}|^2 \le C_1. \end{equation} So as $\nabla v$ is also harmonic by gradient estiamte (of the harmonic function $\nabla v$) we have \begin{equation} \|D^2 v\|_{L^\infty(B_3)}\le N_0^2. \end{equation} Then my intention is also to obtain universal control as below \begin{equation} \| \nabla v \|_{L^\infty(B_3)} \le N_0^2. \end{equation} Am I right? What should I do? Thank you. I will be very grateful.",,"['analysis', 'partial-differential-equations']"
20,"Prime numbers, analysis of polylogarithms","Prime numbers, analysis of polylogarithms",,"Can any interesting results concering prime numbers be obtained using the analytic properties of the polylogarithm, similar to how analytic methods are used on the zeta function to obtain results about primes? $\text{Li}_s(x)$ is the polylogarithm, and $\zeta(s)$ is the zeta function: $$\text{Li}_s(x)=\sum_{n=1}^\infty\frac{x^n}{n^s} ,\text{    }\text{ } \text{ } \zeta(s)=\sum_{n=1}^\infty\frac{1}{n^s}$$ The zeta function satisfies $$-\frac{d}{ds}\ln(\zeta(s))=\sum_{p\text{ prime}}\frac{\ln(p)}{p^s-1}=\sum_{n=1}^\infty\frac{\Lambda(n)}{n^s}$$ and the polylogarithm satisfies a similar result:  $$-\frac{d}{ds}\text{Li}_s(x)=\sum_{p\text{ prime}}\ln(p)( \frac{ \text {Li}_s(x^p)}{p^s}+\frac{ \text {Li}_s(x^{p^2})}{p^{2s}}+\frac{ \text {Li}_s(x^{p^3})}{p^{3s}}+....)=\sum_{n=1}^\infty\frac{\Lambda(n)\text{Li}_s(x^n)}{n^s}$$ Also, I don't know if its worth noting, but by substituting roots of unity for the argument of the polylogarithm, using some orthogonality relations and Möbius inversion one can obtain, $$-\frac{1}{a}\sum_{n=1}^\infty\frac{\mu(n)}{n^s}\sum_{t=0}^{a-1}\frac{d}{ds}\text{Li}_s(e^{2\pi itn/a})e^{-2\pi i t b/a}=\sum_{n=0}^\infty \frac{\Lambda(an+b)}{(an+b)^s}=\sum_{p\equiv \text{ b mod a }}\frac{\ln(p)}{p^s}+o(1)$$ And thus, $$-\frac{1}{a}\int_{s}^\infty\sum_{n=1}^\infty\frac{\mu(n)}{n^s}\sum_{t=0}^{a-1}\frac{d}{ds}\text{Li}_s(e^{2\pi itn/a})e^{-2\pi i t b/a}\ ds=\sum_{p\equiv \text{ b mod a }}\frac{1}{p^s}+o(1)$$ which I used to obtain results like: $$\sum_{p\equiv 2 \text{ mod 3}}\frac{\ln(p)p^s}{p^{2s}-1}=\frac{\zeta(s,\frac{2}{3})\zeta'(s,\frac{1}{3})-\zeta(s,\frac{1}{3})\zeta'(s,\frac{2}{3})}{\zeta(s,\frac{1}{3})^2-\zeta(s,\frac{2}{3})^2}$$ where $\zeta(s,q)$ are Hurwitz zeta functions. I know Dirichlet $L$ functions can be used to give similar results. I just thought it was worth mentioning, and in this case it's much easier to recognize divergence of the left hand sum as $\lim_{s\to 1}$.","Can any interesting results concering prime numbers be obtained using the analytic properties of the polylogarithm, similar to how analytic methods are used on the zeta function to obtain results about primes? $\text{Li}_s(x)$ is the polylogarithm, and $\zeta(s)$ is the zeta function: $$\text{Li}_s(x)=\sum_{n=1}^\infty\frac{x^n}{n^s} ,\text{    }\text{ } \text{ } \zeta(s)=\sum_{n=1}^\infty\frac{1}{n^s}$$ The zeta function satisfies $$-\frac{d}{ds}\ln(\zeta(s))=\sum_{p\text{ prime}}\frac{\ln(p)}{p^s-1}=\sum_{n=1}^\infty\frac{\Lambda(n)}{n^s}$$ and the polylogarithm satisfies a similar result:  $$-\frac{d}{ds}\text{Li}_s(x)=\sum_{p\text{ prime}}\ln(p)( \frac{ \text {Li}_s(x^p)}{p^s}+\frac{ \text {Li}_s(x^{p^2})}{p^{2s}}+\frac{ \text {Li}_s(x^{p^3})}{p^{3s}}+....)=\sum_{n=1}^\infty\frac{\Lambda(n)\text{Li}_s(x^n)}{n^s}$$ Also, I don't know if its worth noting, but by substituting roots of unity for the argument of the polylogarithm, using some orthogonality relations and Möbius inversion one can obtain, $$-\frac{1}{a}\sum_{n=1}^\infty\frac{\mu(n)}{n^s}\sum_{t=0}^{a-1}\frac{d}{ds}\text{Li}_s(e^{2\pi itn/a})e^{-2\pi i t b/a}=\sum_{n=0}^\infty \frac{\Lambda(an+b)}{(an+b)^s}=\sum_{p\equiv \text{ b mod a }}\frac{\ln(p)}{p^s}+o(1)$$ And thus, $$-\frac{1}{a}\int_{s}^\infty\sum_{n=1}^\infty\frac{\mu(n)}{n^s}\sum_{t=0}^{a-1}\frac{d}{ds}\text{Li}_s(e^{2\pi itn/a})e^{-2\pi i t b/a}\ ds=\sum_{p\equiv \text{ b mod a }}\frac{1}{p^s}+o(1)$$ which I used to obtain results like: $$\sum_{p\equiv 2 \text{ mod 3}}\frac{\ln(p)p^s}{p^{2s}-1}=\frac{\zeta(s,\frac{2}{3})\zeta'(s,\frac{1}{3})-\zeta(s,\frac{1}{3})\zeta'(s,\frac{2}{3})}{\zeta(s,\frac{1}{3})^2-\zeta(s,\frac{2}{3})^2}$$ where $\zeta(s,q)$ are Hurwitz zeta functions. I know Dirichlet $L$ functions can be used to give similar results. I just thought it was worth mentioning, and in this case it's much easier to recognize divergence of the left hand sum as $\lim_{s\to 1}$.",,"['analysis', 'prime-numbers', 'mobius-inversion']"
21,Every Cauchy sequence in a metric space is bounded,Every Cauchy sequence in a metric space is bounded,,"Is the following correct or along the right lines? Thanks for any help Question A sequence $\{x_n\}$ in a metric space is said to be bounded if it is contained in some open ball $B(a,r)$. Prove that every Cauchy sequence in a metric space is bounded. Proof Put $\{x_n\}\in(X,d)$ s.t. $\forall\epsilon>0\ \exists k\in\mathbb{N}\ s.t. \ d(x_n,x_m)<\epsilon$ whenever $n,m\geq k$. Now for $x_n\geq k$ we have $x_n\in{B(x_n,\epsilon)}$ Defining the finite sequence $y=\{x_1,x_2,\ldots,x_n\}$. Take $r=\epsilon+\max\{|\max(y)|,|\min(y)|\}$, and so $x_n\in{B(0,r)}$","Is the following correct or along the right lines? Thanks for any help Question A sequence $\{x_n\}$ in a metric space is said to be bounded if it is contained in some open ball $B(a,r)$. Prove that every Cauchy sequence in a metric space is bounded. Proof Put $\{x_n\}\in(X,d)$ s.t. $\forall\epsilon>0\ \exists k\in\mathbb{N}\ s.t. \ d(x_n,x_m)<\epsilon$ whenever $n,m\geq k$. Now for $x_n\geq k$ we have $x_n\in{B(x_n,\epsilon)}$ Defining the finite sequence $y=\{x_1,x_2,\ldots,x_n\}$. Take $r=\epsilon+\max\{|\max(y)|,|\min(y)|\}$, and so $x_n\in{B(0,r)}$",,"['analysis', 'metric-spaces']"
22,"Is $\lim_{n \to \infty} \lim_{l \to \infty} \ a_{n,l} = \lim_{l \to \infty} \lim_{n \to \infty} \ a_{n,l}\;$?",Is ?,"\lim_{n \to \infty} \lim_{l \to \infty} \ a_{n,l} = \lim_{l \to \infty} \lim_{n \to \infty} \ a_{n,l}\;","Is $\lim_{n \to \infty} \lim_{l \to \infty} \ a_{n,l} = \lim_{l \to \infty} \lim_{n \to \infty} \ a_{n,l}\;$? What happens if I replace limits with lim sups? Thanks!","Is $\lim_{n \to \infty} \lim_{l \to \infty} \ a_{n,l} = \lim_{l \to \infty} \lim_{n \to \infty} \ a_{n,l}\;$? What happens if I replace limits with lim sups? Thanks!",,['analysis']
23,"""nice functions""","""nice functions""",,"I see the statement of ""nice functions"" in textbooks and the authors usually don't need to give the definition of ""nice functions"". For example in a book which I read now the authors write ""Morrey spaces is not separable. A version of Morrey space where it is possible to approximate by ""nice functions"" is vanishing Morrey space."" and don't give the definition of ""nice functions"" anywhere in the book. I wonder in here what is the meaning of ""nice functions"" ? and Is there a fixed definition of ""nice functions"" ?","I see the statement of ""nice functions"" in textbooks and the authors usually don't need to give the definition of ""nice functions"". For example in a book which I read now the authors write ""Morrey spaces is not separable. A version of Morrey space where it is possible to approximate by ""nice functions"" is vanishing Morrey space."" and don't give the definition of ""nice functions"" anywhere in the book. I wonder in here what is the meaning of ""nice functions"" ? and Is there a fixed definition of ""nice functions"" ?",,"['analysis', 'functions']"
24,Minimum of $x^2+\frac{a}{x}$ without Calculus,Minimum of  without Calculus,x^2+\frac{a}{x},How can I find the minimum of $x^2+\frac{a}{x}$ on $\mathbb{R}_+$ without calculus?,How can I find the minimum of $x^2+\frac{a}{x}$ on $\mathbb{R}_+$ without calculus?,,[]
25,Why do functions with compact support include those that vanish at infinity?,Why do functions with compact support include those that vanish at infinity?,,"The support of a function is defined in Wikipedia as ""the set of points where the function is not zero-valued, or the closure of that set"". Functions with compact support in $X$ are defined in Wikipedia as ""those with support that is a compact subset of $X$. For example, if $X$ is the real line, they are functions of bounded support and therefore vanish at infinity (and negative infinity)"". Why functions vanishing at infinity are considered as having compact support? An example of a function vanishing at infinity is $f(x) = \frac{1}{x^2+1}$, it's support is $\mathbb{R}$. The compactness of a subset $K$ is defined as ""every arbitrary collection of open subsets of $X$ such that covers $K$, there is a finite subset also covers $K$"". Now $\mathbb{R}$ is not compact, we can't say $f(x) = \frac{1}{x^2+1}$ has a compact support, am I right there?","The support of a function is defined in Wikipedia as ""the set of points where the function is not zero-valued, or the closure of that set"". Functions with compact support in $X$ are defined in Wikipedia as ""those with support that is a compact subset of $X$. For example, if $X$ is the real line, they are functions of bounded support and therefore vanish at infinity (and negative infinity)"". Why functions vanishing at infinity are considered as having compact support? An example of a function vanishing at infinity is $f(x) = \frac{1}{x^2+1}$, it's support is $\mathbb{R}$. The compactness of a subset $K$ is defined as ""every arbitrary collection of open subsets of $X$ such that covers $K$, there is a finite subset also covers $K$"". Now $\mathbb{R}$ is not compact, we can't say $f(x) = \frac{1}{x^2+1}$ has a compact support, am I right there?",,"['analysis', 'definition']"
26,Is there some intuition for Lagrange interpolation formula?,Is there some intuition for Lagrange interpolation formula?,,"How do I prove the Lagrange interpolation formula is true as stated in this link ? I ask this because the article isn't self contained on intuition of each step in the proof, please don't use things too advanced because I am just a high school student. Thanks in advance.","How do I prove the Lagrange interpolation formula is true as stated in this link ? I ask this because the article isn't self contained on intuition of each step in the proof, please don't use things too advanced because I am just a high school student. Thanks in advance.",,"['analysis', 'interpolation']"
27,To construct a set with a limit point.,To construct a set with a limit point.,,"I learned how to construct a Cantor Set, and I am asked to do the following. ""Construct a bounded set with exactly 3 limit points."" Since the Cantor set contains infinitely many points, I don't think something like it will not work. But this is the only thing that I have learned from the book that tells me anything about constructing a set that has a limit point. I am also considering the interval $[0,1]$ and constructing a set so that the limit points are $\{0,1/2,1\}$. If possible, I would like to see more than one simple examples because I am new to analysis and I have no teacher. It's very tough.","I learned how to construct a Cantor Set, and I am asked to do the following. ""Construct a bounded set with exactly 3 limit points."" Since the Cantor set contains infinitely many points, I don't think something like it will not work. But this is the only thing that I have learned from the book that tells me anything about constructing a set that has a limit point. I am also considering the interval $[0,1]$ and constructing a set so that the limit points are $\{0,1/2,1\}$. If possible, I would like to see more than one simple examples because I am new to analysis and I have no teacher. It's very tough.",,['analysis']
28,"Integral $\int_0^\pi \cot(x/2)\sin(nx)\,dx$",Integral,"\int_0^\pi \cot(x/2)\sin(nx)\,dx","It seems that $$\int_0^\pi \cot(x/2)\sin(nx)\,dx=\pi$$ for all positive integers $n$. But I have trouble proving it. Anyone?","It seems that $$\int_0^\pi \cot(x/2)\sin(nx)\,dx=\pi$$ for all positive integers $n$. But I have trouble proving it. Anyone?",,"['analysis', 'definite-integrals']"
29,"principal value as distribution, written as integral over singularity","principal value as distribution, written as integral over singularity",,"Let  $C_0^\infty(\mathbb{R})$ be the set of smooth functions with compact support on the real line $\mathbb{R}.$ Then, the map $$\operatorname{p.\!v.}\left(\frac{1}{x}\right)\,: C_0^\infty(\mathbb{R}) \to \mathbb{C}$$ defined via the Cauchy principal value as $$ \operatorname{p.\!v.}\left(\frac{1}{x}\right)(u)=\lim_{\varepsilon\to 0+} \int_{\mathbb{R}\setminus [-\varepsilon;\varepsilon]} \frac{u(x)}{x} \, \mathrm{d}x \quad\text{ for }u\in C_0^\infty(\mathbb{R})$$ Now why is $$ \lim_{\varepsilon\to 0+} \int_{\mathbb{R}\setminus [-\varepsilon;\varepsilon]} \frac{u(x)}{x} \, \mathrm{d}x = \int_0^{+\infty} \frac{u(x)-u(-x)}{x}\, \mathrm{d}x $$ why is the integral defined on the left.","Let  $C_0^\infty(\mathbb{R})$ be the set of smooth functions with compact support on the real line $\mathbb{R}.$ Then, the map $$\operatorname{p.\!v.}\left(\frac{1}{x}\right)\,: C_0^\infty(\mathbb{R}) \to \mathbb{C}$$ defined via the Cauchy principal value as $$ \operatorname{p.\!v.}\left(\frac{1}{x}\right)(u)=\lim_{\varepsilon\to 0+} \int_{\mathbb{R}\setminus [-\varepsilon;\varepsilon]} \frac{u(x)}{x} \, \mathrm{d}x \quad\text{ for }u\in C_0^\infty(\mathbb{R})$$ Now why is $$ \lim_{\varepsilon\to 0+} \int_{\mathbb{R}\setminus [-\varepsilon;\varepsilon]} \frac{u(x)}{x} \, \mathrm{d}x = \int_0^{+\infty} \frac{u(x)-u(-x)}{x}\, \mathrm{d}x $$ why is the integral defined on the left.",,"['analysis', 'distribution-theory']"
30,Difference between soft analysis and hard analysis,Difference between soft analysis and hard analysis,,I have sometimes overheard people using the terms hard analysis and soft analysis.I am not a particularly well-read person in mathematics but I have wondered what that is all about.I hope  there exists an explanation for someone with single-variable calculus background.,I have sometimes overheard people using the terms hard analysis and soft analysis.I am not a particularly well-read person in mathematics but I have wondered what that is all about.I hope  there exists an explanation for someone with single-variable calculus background.,,"['analysis', 'terminology']"
31,"How to transform a Laplacian operator from (x,y) coordinate system to polar system?","How to transform a Laplacian operator from (x,y) coordinate system to polar system?",,"for any functions in $C_2^2$, we have a $-D^2$ operator $-D^2u=-(u_{xx}+u_{yy})$ However now i need to transform this operator from $(x,y)$ to $(r,\theta)$ for some sphere boundary condition, how am I suppose to do? As we know that $x=r\cos\theta,y=r\sin\theta$. the form should be something near $-(u_{\theta\theta}+u_{rr})$","for any functions in $C_2^2$, we have a $-D^2$ operator $-D^2u=-(u_{xx}+u_{yy})$ However now i need to transform this operator from $(x,y)$ to $(r,\theta)$ for some sphere boundary condition, how am I suppose to do? As we know that $x=r\cos\theta,y=r\sin\theta$. the form should be something near $-(u_{\theta\theta}+u_{rr})$",,"['analysis', 'multivariable-calculus', 'coordinate-systems']"
32,How to prove $\max_{x \in I} |f(x)| \leq \max_{x \in I} |f'(x)|$?,How to prove ?,\max_{x \in I} |f(x)| \leq \max_{x \in I} |f'(x)|,"Today we had a probational exam in analysis. I wasn't able to solve one of the exercises and I have no idea what theorem to apply in order to solve it: Let $I=[0,1]$ and $f: I \rightarrow \mathbb{R}$ be continuously differentiable. Assuming that $f$ has a root. Show: $$\max_{x \in I} |f(x)| \leq \max_{x \in I} |f'(x)|$$ Does this theorem have a name? What other theorem will I need in order to prove it? I'm sure the fact that the function has a root is important, but I don't see why and how to make use of it... Thanks for your help!","Today we had a probational exam in analysis. I wasn't able to solve one of the exercises and I have no idea what theorem to apply in order to solve it: Let $I=[0,1]$ and $f: I \rightarrow \mathbb{R}$ be continuously differentiable. Assuming that $f$ has a root. Show: $$\max_{x \in I} |f(x)| \leq \max_{x \in I} |f'(x)|$$ Does this theorem have a name? What other theorem will I need in order to prove it? I'm sure the fact that the function has a root is important, but I don't see why and how to make use of it... Thanks for your help!",,['analysis']
33,The local lipschitz condition implies differentiability?,The local lipschitz condition implies differentiability?,,"I know differentiability implies the local lipschitz condition. however, I am not sure the converse. Actually, I think it might be. The definition of the local lipschitz condition is that for $$ f: A \subset \mathbb{R}^m \rightarrow \mathbb{R}^n $$ and $ \forall x_{0} \in A$ , there are a constant $M > 0$ and a $\delta_{0} > 0$ such that $\lVert x- x_{0}\rVert < \delta_{0}$ implies $$ \lVert f(x) - f(x_{0})\rVert \leq M\lVert x-x_{0}\rVert $$ And the definition of differentialbility is that $\lVert x-x_{0}\rVert < \delta $ implies $$ \lVert f(x) - f(x_{0}) - Df(x_{0})(x-x_{0})\rVert < \epsilon \lVert x-x_{0}\rVert $$","I know differentiability implies the local lipschitz condition. however, I am not sure the converse. Actually, I think it might be. The definition of the local lipschitz condition is that for and , there are a constant and a such that implies And the definition of differentialbility is that implies","
f: A \subset \mathbb{R}^m \rightarrow \mathbb{R}^n
  \forall x_{0} \in A M > 0 \delta_{0} > 0 \lVert x- x_{0}\rVert < \delta_{0} 
\lVert f(x) - f(x_{0})\rVert \leq M\lVert x-x_{0}\rVert
 \lVert x-x_{0}\rVert < \delta  
\lVert f(x) - f(x_{0}) - Df(x_{0})(x-x_{0})\rVert < \epsilon \lVert x-x_{0}\rVert
","['analysis', 'derivatives']"
34,Asymptotic formula for $\sum_{n\leq x}\mu(n)[x/n]^2$ and the Totient summatory function $\sum_{n\leq x} \phi(n)$,Asymptotic formula for  and the Totient summatory function,\sum_{n\leq x}\mu(n)[x/n]^2 \sum_{n\leq x} \phi(n),I would like to show (for $x \ge 2$) that $$\sum_{n \le x}\mu(n)\left[\frac{x}{n}\right]^2 = \frac{x^2}{\zeta(2)} + O(x \log(x)).$$ I already have the identity $$\sum_{n \le x}\mu(n)\left[\frac{x}{n}\right] = 1$$ but the method to show this does not extend to my case. I don't know how to approach this problem so any advice would be very welcome! Since $$\left[\frac{x}{n}\right]^2 = \left(\frac{x}{n}\right)^2 + O(\frac{x}{n})$$ we have $$\sum_{n \le x}\mu(n)\left[\frac{x}{n}\right]^2 = \sum_{n \le x} \mu(n) \left(\frac{x}{n}\right)^2 + O(\sum_{n \le x} \mu(n) \frac{x}{n})$$ and the value estimate is $\frac{1}{\zeta(2)}$ so it remains to show that the error estimate is $O(x \log(x))$.,I would like to show (for $x \ge 2$) that $$\sum_{n \le x}\mu(n)\left[\frac{x}{n}\right]^2 = \frac{x^2}{\zeta(2)} + O(x \log(x)).$$ I already have the identity $$\sum_{n \le x}\mu(n)\left[\frac{x}{n}\right] = 1$$ but the method to show this does not extend to my case. I don't know how to approach this problem so any advice would be very welcome! Since $$\left[\frac{x}{n}\right]^2 = \left(\frac{x}{n}\right)^2 + O(\frac{x}{n})$$ we have $$\sum_{n \le x}\mu(n)\left[\frac{x}{n}\right]^2 = \sum_{n \le x} \mu(n) \left(\frac{x}{n}\right)^2 + O(\sum_{n \le x} \mu(n) \frac{x}{n})$$ and the value estimate is $\frac{1}{\zeta(2)}$ so it remains to show that the error estimate is $O(x \log(x))$.,,"['analysis', 'number-theory', 'analytic-number-theory', 'multiplicative-function']"
35,show that: $f$ is injective $\iff$ there exists a $g: Y\rightarrow X$ such that $g \circ f = idX$,show that:  is injective  there exists a  such that,f \iff g: Y\rightarrow X g \circ f = idX,"** proof under construction - will post when done and more or less confident it's true. ** also please easy with the downgrades.. i don't understand why i'm getting them. what is meant by show that? am i supposed to give an example? sure g(y) can be y/2 if f is x*2.  am i supposed to give a proof ? (we are learning the axioms and the lemmas.) in which case sure, again, given the sets A, B, X, Y and g: Y -> X, and f an injective function defined f: A->B with A a subset of X and B a subset of Y.  we know that f will map only specific values of X to specific values of Y, i then define f(x) = x*2 and g(y) = y/2 thus g o f = idX is valid. (i am not sure if this counts as a correct proof, but i am trying) I can explain it verbally. I understand the concept. but i have NO idea what the question wants from me. ""show that"" is too vague.","** proof under construction - will post when done and more or less confident it's true. ** also please easy with the downgrades.. i don't understand why i'm getting them. what is meant by show that? am i supposed to give an example? sure g(y) can be y/2 if f is x*2.  am i supposed to give a proof ? (we are learning the axioms and the lemmas.) in which case sure, again, given the sets A, B, X, Y and g: Y -> X, and f an injective function defined f: A->B with A a subset of X and B a subset of Y.  we know that f will map only specific values of X to specific values of Y, i then define f(x) = x*2 and g(y) = y/2 thus g o f = idX is valid. (i am not sure if this counts as a correct proof, but i am trying) I can explain it verbally. I understand the concept. but i have NO idea what the question wants from me. ""show that"" is too vague.",,"['analysis', 'functions', 'elementary-set-theory', 'function-and-relation-composition']"
36,How to prove that $\exp(x)$ and $\log(x)$ are inverse?,How to prove that  and  are inverse?,\exp(x) \log(x),"How does one prove that the exponential and logarithmic functions are inverse using the definitions: $$e^x= \sum_{i=0}^{\infty} \frac{x^i}{i!}$$ and $$\log(x)=\int_{1}^{x}\frac{1}{t}dt$$ My naive approach (sort of ignoring issues of convergence) is to just apply the definitions straightforwardly, so in one direction I get: \begin{align}\log(e^x)&=\int_{1}^{e^x}\frac{1}{t}dt\\ &=\int_{0}^{e^x-1}\frac{1}{1+t}dt\\ &=\int_0^{e^x-1}\sum_{j=0}^\infty (-1)^jt^jdt \\ &=\sum_{j=0}^\infty (-1)^j \int_0^{e^x-1} t^jdt\\ &=\sum_{j=0}^\infty \frac{(-1)^j}{j+1}(e^x-1)^{j+1}\\ &=\sum_{j=0}^\infty \frac{(-1)^j}{j+1} \sum_{k=0}^{j+1} \frac{n!}{k!(n-k)!}e^{x(n-k)}(-1)^k\\ &=\sum_{j=0}^\infty \sum_{k=0}^{j+1}  \frac{(-1)^{j+k}n!}{(j+1)k!(n-k)!} e^{x(n-k)}\\ &=\sum_{j=0}^\infty \sum_{k=0}^{j+1}  \frac{(-1)^{j+k}n!}{(j+1)k!(n-k)!} \sum_{\ell=0}^\infty \frac{(-1)^\ell}{\ell !}(n-k)^\ell x^\ell\\ &=\sum_{j=0}^\infty \sum_{k=0}^{j+1} \sum_{\ell=0}^\infty \frac{(-1)^{j+k+\ell}n!(n-k)^\ell x^\ell}{(j+1)k!\ell!(n-k)!} \end{align} and I cant see at all that this is equal to $x$ . My guess is I'm going about this all wrong.","How does one prove that the exponential and logarithmic functions are inverse using the definitions: and My naive approach (sort of ignoring issues of convergence) is to just apply the definitions straightforwardly, so in one direction I get: and I cant see at all that this is equal to . My guess is I'm going about this all wrong.","e^x= \sum_{i=0}^{\infty} \frac{x^i}{i!} \log(x)=\int_{1}^{x}\frac{1}{t}dt \begin{align}\log(e^x)&=\int_{1}^{e^x}\frac{1}{t}dt\\
&=\int_{0}^{e^x-1}\frac{1}{1+t}dt\\
&=\int_0^{e^x-1}\sum_{j=0}^\infty (-1)^jt^jdt \\
&=\sum_{j=0}^\infty (-1)^j \int_0^{e^x-1} t^jdt\\
&=\sum_{j=0}^\infty \frac{(-1)^j}{j+1}(e^x-1)^{j+1}\\
&=\sum_{j=0}^\infty \frac{(-1)^j}{j+1} \sum_{k=0}^{j+1} \frac{n!}{k!(n-k)!}e^{x(n-k)}(-1)^k\\
&=\sum_{j=0}^\infty \sum_{k=0}^{j+1}  \frac{(-1)^{j+k}n!}{(j+1)k!(n-k)!} e^{x(n-k)}\\
&=\sum_{j=0}^\infty \sum_{k=0}^{j+1}  \frac{(-1)^{j+k}n!}{(j+1)k!(n-k)!} \sum_{\ell=0}^\infty \frac{(-1)^\ell}{\ell !}(n-k)^\ell x^\ell\\
&=\sum_{j=0}^\infty \sum_{k=0}^{j+1} \sum_{\ell=0}^\infty \frac{(-1)^{j+k+\ell}n!(n-k)^\ell x^\ell}{(j+1)k!\ell!(n-k)!} \end{align} x",['analysis']
37,Density of zeros of a power series over the reals,Density of zeros of a power series over the reals,,"Let $f(x) = \sum_{i =0}^\infty a_i x^i$ be a power series which converges for all real $x$. Assume that $f(x)$ is not identically zero. I'm interested in the density of the zeros of $f(x)$. Let $Z$ be the set of zeros of $f(x)$. Which of the following claims about density of $Z$ are true? Claim 1: $Z$ is nowhere dense. Claim 2: $Z$ is countable. Claim 3: For any $a,b \in \mathbb{R}$ , $Z \cap [a,b]$ is finite. I believe (correct me if I'm wrong) that claim 3 implies the other two. I suspect all three claims are true. I suspect that the answers to these questions are well-known, though I was not able to find an obvious reference. Can anyone suggest a reference with a nice treatment of these questions?","Let $f(x) = \sum_{i =0}^\infty a_i x^i$ be a power series which converges for all real $x$. Assume that $f(x)$ is not identically zero. I'm interested in the density of the zeros of $f(x)$. Let $Z$ be the set of zeros of $f(x)$. Which of the following claims about density of $Z$ are true? Claim 1: $Z$ is nowhere dense. Claim 2: $Z$ is countable. Claim 3: For any $a,b \in \mathbb{R}$ , $Z \cap [a,b]$ is finite. I believe (correct me if I'm wrong) that claim 3 implies the other two. I suspect all three claims are true. I suspect that the answers to these questions are well-known, though I was not able to find an obvious reference. Can anyone suggest a reference with a nice treatment of these questions?",,"['analysis', 'power-series']"
38,What does directional derivative zeros imply when directional vector is not zero?,What does directional derivative zeros imply when directional vector is not zero?,,"This question might sound stupid but I want to confirm an answer from it. I saw somewhere online that it means that when the directional derivative of function $f$ along the none zero vector $v$ at certain point is equal to $0$, it means that the function $f$ is constant in that direction. But what does ""constant in direction"" mean? can anyone give me an example of it such as $f(x,y)$ to explain this? Thanks!","This question might sound stupid but I want to confirm an answer from it. I saw somewhere online that it means that when the directional derivative of function $f$ along the none zero vector $v$ at certain point is equal to $0$, it means that the function $f$ is constant in that direction. But what does ""constant in direction"" mean? can anyone give me an example of it such as $f(x,y)$ to explain this? Thanks!",,"['analysis', 'derivatives']"
39,Uncountable set with exactly one limit point,Uncountable set with exactly one limit point,,Is there any uncountable subset of $\mathbb{C}$ with exactly one limit point?,Is there any uncountable subset of $\mathbb{C}$ with exactly one limit point?,,['analysis']
40,$f$ strictly increasing does not imply $f'>0$,strictly increasing does not imply,f f'>0,"We know that a function $f: [a,b] \to \mathbb{R}$ continuous on $[a,b]$ and differentiable on $(a,b)$, and if $f'>0 \mbox{ on} (a,b)$ , f is strictly increasing on $[a,b]$. Is there any counterexample that shows the converse fails? I have been trying to come up with simple examples but they all involve functions that are discontinuous or has derivative $f'=0$ which does not agree with the hypothesis hmmm","We know that a function $f: [a,b] \to \mathbb{R}$ continuous on $[a,b]$ and differentiable on $(a,b)$, and if $f'>0 \mbox{ on} (a,b)$ , f is strictly increasing on $[a,b]$. Is there any counterexample that shows the converse fails? I have been trying to come up with simple examples but they all involve functions that are discontinuous or has derivative $f'=0$ which does not agree with the hypothesis hmmm",,['analysis']
41,"If $ds$ is not a differential form, can I make sense of its intuitive notation somehow?","If  is not a differential form, can I make sense of its intuitive notation somehow?",ds,"I understand that a line element is not actually a differential form but a $1$-density. My question is: is the notation $ds^2 = dx^2 + dy^2$ formal in any way? Can it be interpreted as outer or tensor products? Is it just simply an informal useful way to describe an integrable object? I think that a straightforeward interpretation of $ds^2$ as the wedge product of two vector valued differential forms is not possible, but is there any other way I can look ar this? After looking into the other posts related to this same subject, such as Why is arc length not a differential form? and Is $ds$ a differential form? I still can't find a useful way of thinking about this particular problem.","I understand that a line element is not actually a differential form but a $1$-density. My question is: is the notation $ds^2 = dx^2 + dy^2$ formal in any way? Can it be interpreted as outer or tensor products? Is it just simply an informal useful way to describe an integrable object? I think that a straightforeward interpretation of $ds^2$ as the wedge product of two vector valued differential forms is not possible, but is there any other way I can look ar this? After looking into the other posts related to this same subject, such as Why is arc length not a differential form? and Is $ds$ a differential form? I still can't find a useful way of thinking about this particular problem.",,"['analysis', 'differential-geometry', 'differential']"
42,Why the members of $\mathbb R$ will be certain subsets of $\mathbb Q$?,Why the members of  will be certain subsets of ?,\mathbb R \mathbb Q,"$\mathbb{R}$ is real numbers set, $\mathbb{Q}$ denotes rational numbers set. This is quoted from Rudin's mathematical analysis book page 17 about Dedekind' s construction. Why the members of $\mathbb{R}$ will be certain subsets of $\mathbb{Q}$? There are two levels' confusing, one is that in Mathematics, another is this English sentence, or the expression of this fact. Maybe I'd like to comprehend like this: members of $\mathbb{R}$ are some thing decided by some certain subsets of of $\mathbb{Q}$. At first glance, it seems like members of $\mathbb{R}$ are are equal to some subsets of $\mathbb{Q}$. But which subset is $\sqrt{2}$ correspoinding to ? This maybe not so obvious. IMO As @Hagen von Eitzen's answer mentioned, it means that $\mathbb{Q}$ is obtained from Integers . And this is just one construction, I agree. That is obvious. And the same to $\mathbb{C}$, Complex number is a pair of real numbers, we do accept the fact quikly. But if you say, $\mathbb{C}$ is some certain subsets of  $\mathbb{R}$ $\mathbb{C}$ is some certain subsets of  $\mathbb{Q}$ $\mathbb{C}$ is some certain subsets of  $\mathbb{Z}$ There will also be some confusions at first glance in my point of view. @Robert Israel 's answer is more about the fact what Real Number is.","$\mathbb{R}$ is real numbers set, $\mathbb{Q}$ denotes rational numbers set. This is quoted from Rudin's mathematical analysis book page 17 about Dedekind' s construction. Why the members of $\mathbb{R}$ will be certain subsets of $\mathbb{Q}$? There are two levels' confusing, one is that in Mathematics, another is this English sentence, or the expression of this fact. Maybe I'd like to comprehend like this: members of $\mathbb{R}$ are some thing decided by some certain subsets of of $\mathbb{Q}$. At first glance, it seems like members of $\mathbb{R}$ are are equal to some subsets of $\mathbb{Q}$. But which subset is $\sqrt{2}$ correspoinding to ? This maybe not so obvious. IMO As @Hagen von Eitzen's answer mentioned, it means that $\mathbb{Q}$ is obtained from Integers . And this is just one construction, I agree. That is obvious. And the same to $\mathbb{C}$, Complex number is a pair of real numbers, we do accept the fact quikly. But if you say, $\mathbb{C}$ is some certain subsets of  $\mathbb{R}$ $\mathbb{C}$ is some certain subsets of  $\mathbb{Q}$ $\mathbb{C}$ is some certain subsets of  $\mathbb{Z}$ There will also be some confusions at first glance in my point of view. @Robert Israel 's answer is more about the fact what Real Number is.",,['analysis']
43,"How to show for $\alpha\in (0,1)$, any $f\in C^\alpha([0,1]/{\sim})$ has a Fourier series $S_nf$ uniformly converging to $f$","How to show for , any  has a Fourier series  uniformly converging to","\alpha\in (0,1) f\in C^\alpha([0,1]/{\sim}) S_nf f","Technically homework(a midterm) but its over and I'm itching to know the solution. I know how to show it for $\alpha>1/2$ (the Fourier series will converge absolutely), but apparently its true for any $\alpha$; the question guided me as follows: Show that if a equicontinuous sequence of functions ($f_n$) converges pointwise to $f$, then $f_n$ converges uniformly to $f$. Show for $f∈ C^\alpha([0,1]/{\sim})$ that $S_nf → f$ pointwise. Show that the sequence $(S_nf)$ is equicontinuous and conclude. 1 and 2 posed no problems to me but I could not do 3. Any help? In addition, I would not mind other ways to prove the result.","Technically homework(a midterm) but its over and I'm itching to know the solution. I know how to show it for $\alpha>1/2$ (the Fourier series will converge absolutely), but apparently its true for any $\alpha$; the question guided me as follows: Show that if a equicontinuous sequence of functions ($f_n$) converges pointwise to $f$, then $f_n$ converges uniformly to $f$. Show for $f∈ C^\alpha([0,1]/{\sim})$ that $S_nf → f$ pointwise. Show that the sequence $(S_nf)$ is equicontinuous and conclude. 1 and 2 posed no problems to me but I could not do 3. Any help? In addition, I would not mind other ways to prove the result.",,"['analysis', 'fourier-analysis']"
44,"Family of connected sets, proving union is connected","Family of connected sets, proving union is connected",,"I am having some trouble trying to prove the following statement:$$$$ Let $(X,d)$ be a metric space and $\mathcal A$ a family of connected sets in $X$ such that for every pair of subsets $A,B \in \mathcal A$ there exist $A_0$,...,$A_n \in \mathcal A$ that satisfy $A_0=A$, $A_n=B$ and $A_i \cap A_{i+1} \neq \emptyset$ for every i=0,...,n-1. Prove that $\bigcup_{A \in \mathcal A}A$ is connected. $$$$I've tried to prove it by the absurd: Suppose the union is disconnected, then there exist $U$ and $V$ nonempty disjoint open sets such that $\bigcup_{A \in \mathcal A}A=U \cup V$. Then, there is $A \in \bigcup_{A \in \mathcal A}A$ : $A \subset V$ and $A \cap V=\emptyset$. The same argument applies for $B \in \bigcup_{A \in \mathcal A}A$ with $B \subset U$ ($A$ and $B$ both nonempty). By hypothesis, $A=A_0$ and $B=A_n$. In this part I got stuck. I know I have to use the fact that the intersection of $A_i$ and $A_{i+1}$ is nonempty and that all the sets in $\mathcal A$ are connected, but I don't know where to use that.","I am having some trouble trying to prove the following statement:$$$$ Let $(X,d)$ be a metric space and $\mathcal A$ a family of connected sets in $X$ such that for every pair of subsets $A,B \in \mathcal A$ there exist $A_0$,...,$A_n \in \mathcal A$ that satisfy $A_0=A$, $A_n=B$ and $A_i \cap A_{i+1} \neq \emptyset$ for every i=0,...,n-1. Prove that $\bigcup_{A \in \mathcal A}A$ is connected. $$$$I've tried to prove it by the absurd: Suppose the union is disconnected, then there exist $U$ and $V$ nonempty disjoint open sets such that $\bigcup_{A \in \mathcal A}A=U \cup V$. Then, there is $A \in \bigcup_{A \in \mathcal A}A$ : $A \subset V$ and $A \cap V=\emptyset$. The same argument applies for $B \in \bigcup_{A \in \mathcal A}A$ with $B \subset U$ ($A$ and $B$ both nonempty). By hypothesis, $A=A_0$ and $B=A_n$. In this part I got stuck. I know I have to use the fact that the intersection of $A_i$ and $A_{i+1}$ is nonempty and that all the sets in $\mathcal A$ are connected, but I don't know where to use that.",,"['analysis', 'connectedness']"
45,"What is a ""linear set""","What is a ""linear set""",,"I'm reading "" L'hypothèse du continu "" by Sierpinski. He mentions many times ""ensembles linéaires"" or ""linear sets"" without defining this notion. Does anyone know what the definition of a such a set is ? Here is a translation into English of 2 propositions he gave : ""There exists a linear set whose cardinality is that of the continuum and whose image by any continuous function is of measure zero"". ""There exists a linear set whose cardinality is that of the continuum and whose homeomorphic images have the same measure"".","I'm reading "" L'hypothèse du continu "" by Sierpinski. He mentions many times ""ensembles linéaires"" or ""linear sets"" without defining this notion. Does anyone know what the definition of a such a set is ? Here is a translation into English of 2 propositions he gave : ""There exists a linear set whose cardinality is that of the continuum and whose image by any continuous function is of measure zero"". ""There exists a linear set whose cardinality is that of the continuum and whose homeomorphic images have the same measure"".",,"['analysis', 'measure-theory', 'set-theory']"
46,Borel Measure such that integrating a polynomial yields the derivative at a point,Borel Measure such that integrating a polynomial yields the derivative at a point,,"Does there exist a signed regular Borel measure such that $$ \int_0^1 p(x) d\mu(x) = p'(0) $$ for all polynomials of at most degree $N$ for some fixed $N$. This seems similar to a Dirac measure at a point. If it were instead asking for the integral to yield $p(0)$, I would suggest letting $\mu = \delta_0$. That is, $\mu(E) = 1$ iff $0 \in E$. However, this is slightly different and I'm a bit unsure of it. It's been a while since  I've done any real analysis, so I've forgotten quite a bit. I took a look back at my old textbook and didn't see anything too similar. If anyone could give me a pointer in the right direction, that would be great. I'm also kind of curious if changing the integration interval from [0,1] to all of $\mathbb{R}$ changes anything or if the validity of the statement is altered by allowing it to be for all polynomials, instead of just polynomials of at most some degree. Thanks!","Does there exist a signed regular Borel measure such that $$ \int_0^1 p(x) d\mu(x) = p'(0) $$ for all polynomials of at most degree $N$ for some fixed $N$. This seems similar to a Dirac measure at a point. If it were instead asking for the integral to yield $p(0)$, I would suggest letting $\mu = \delta_0$. That is, $\mu(E) = 1$ iff $0 \in E$. However, this is slightly different and I'm a bit unsure of it. It's been a while since  I've done any real analysis, so I've forgotten quite a bit. I took a look back at my old textbook and didn't see anything too similar. If anyone could give me a pointer in the right direction, that would be great. I'm also kind of curious if changing the integration interval from [0,1] to all of $\mathbb{R}$ changes anything or if the validity of the statement is altered by allowing it to be for all polynomials, instead of just polynomials of at most some degree. Thanks!",,"['analysis', 'measure-theory', 'distribution-theory']"
47,Showing that $\mathbb{Q}$ is not complete,Showing that  is not complete,\mathbb{Q},"Show that there is no least upper bound for $A=\{x: x^2<2\}$ in $\mathbb{Q}$. Suppose $\alpha \in \mathbb{Q}$ is the least upper bound of $A$. Then either $\alpha^2 < 2$ or $\alpha^2 > 2$. I know the proof of the former case and why we can't have equality. So here is my attempt at the latter case ($\alpha^2 > 2$): Since $\alpha \le x$ for all $x \in A$, for any $\epsilon > 0$ there is an $x' \in A$ such that $x' > \alpha - \epsilon$. Clearly $1\le\alpha\le 2$, so choose $\epsilon=\dfrac{\epsilon^2}{2\alpha}<1$. Now we have $x'^2>\alpha^2 - \epsilon^2 + \epsilon^2 = \alpha^2>2$. Contradiction. Is that acceptable?","Show that there is no least upper bound for $A=\{x: x^2<2\}$ in $\mathbb{Q}$. Suppose $\alpha \in \mathbb{Q}$ is the least upper bound of $A$. Then either $\alpha^2 < 2$ or $\alpha^2 > 2$. I know the proof of the former case and why we can't have equality. So here is my attempt at the latter case ($\alpha^2 > 2$): Since $\alpha \le x$ for all $x \in A$, for any $\epsilon > 0$ there is an $x' \in A$ such that $x' > \alpha - \epsilon$. Clearly $1\le\alpha\le 2$, so choose $\epsilon=\dfrac{\epsilon^2}{2\alpha}<1$. Now we have $x'^2>\alpha^2 - \epsilon^2 + \epsilon^2 = \alpha^2>2$. Contradiction. Is that acceptable?",,['analysis']
48,Elliptic operators on compact space are Fredholm,Elliptic operators on compact space are Fredholm,,"I have come across this fact in a reading of mine, but I cannot seem to prove it, and I cannot seem to find a proof of it. Mostly, I am confused why the range of an elliptic operator between appropriate Sobolev spaces has closed range. Thank you for any help.","I have come across this fact in a reading of mine, but I cannot seem to prove it, and I cannot seem to find a proof of it. Mostly, I am confused why the range of an elliptic operator between appropriate Sobolev spaces has closed range. Thank you for any help.",,"['analysis', 'partial-differential-equations']"
49,Differentiation under integral sign (Gamma function),Differentiation under integral sign (Gamma function),,"This might be a silly question, but I'm reading this article about differentiation under the integral sign, and I'm stumped by something that's written early on. The author is giving a derivation of the formula for $n!$ in terms of the gamma function. He shows how you can get $$\frac{n!}{t^{n+1}} = \int_0^{\infty}x^ne^{-tx}\,dx$$ by differentating under the integral sign of $\int_0^{\infty}e^{-tx}dx$. He then says that the above ""immediately implies"" the formula $$n! = \int_0^\infty x^ne^{-x}\,dx.$$ However, I can't for the life of me see how this follows. Multiplying the first equation by $t^{n+1}$ gives $n! = t^{n+1}\int_0^{\infty}x^ne^{-tx}$, so apparently $$t^{n+1}\int_0^\infty x^ne^{-tx} \, dx = \int_0^\infty x^ne^{-x} \, dx,$$ but I don't see how this is true. Can anyone explain this?","This might be a silly question, but I'm reading this article about differentiation under the integral sign, and I'm stumped by something that's written early on. The author is giving a derivation of the formula for $n!$ in terms of the gamma function. He shows how you can get $$\frac{n!}{t^{n+1}} = \int_0^{\infty}x^ne^{-tx}\,dx$$ by differentating under the integral sign of $\int_0^{\infty}e^{-tx}dx$. He then says that the above ""immediately implies"" the formula $$n! = \int_0^\infty x^ne^{-x}\,dx.$$ However, I can't for the life of me see how this follows. Multiplying the first equation by $t^{n+1}$ gives $n! = t^{n+1}\int_0^{\infty}x^ne^{-tx}$, so apparently $$t^{n+1}\int_0^\infty x^ne^{-tx} \, dx = \int_0^\infty x^ne^{-x} \, dx,$$ but I don't see how this is true. Can anyone explain this?",,"['analysis', 'gamma-function']"
50,A Van der Corput style inequality for highly oscillatory integrals,A Van der Corput style inequality for highly oscillatory integrals,,"Suppose $f$ has at least two continuous derivatives, $f'$ is monotonically increasing, and $f' \geq \lambda$ for some $\lambda > 0$. How might one find the upper bound $|\int_a^b \cos(f(x))| \leq 2/\lambda$? I've tried a number of basic approaches and none have worked. For instance, I've tried rewriting $\cos(f(x))$ as $(\sin(f(x)))' / f'(x)$, which seemed promising until I realized that I cannot justify the inequality $|\int_a^b (\sin(f(x)))' / f'(x)| \leq (1/\lambda) |\int_a^b (\sin(f(x)))'|$. Of course, were that true, then the result would follow quickly by evaluating the new integral, applying the triangle inequality, and finally noting that $|\sin(x)| \leq 1$. I've tried a similar approach using the mean value theorem for integrals but was only able to show that there is some $\theta$, $a < \theta < b$, so that $|\int_a^b \cos(f(x))| \leq \frac{1}{\lambda} (|f(\theta) - f(a)| + |f(b) - f(\theta)|)$, which is not strong enough either.","Suppose $f$ has at least two continuous derivatives, $f'$ is monotonically increasing, and $f' \geq \lambda$ for some $\lambda > 0$. How might one find the upper bound $|\int_a^b \cos(f(x))| \leq 2/\lambda$? I've tried a number of basic approaches and none have worked. For instance, I've tried rewriting $\cos(f(x))$ as $(\sin(f(x)))' / f'(x)$, which seemed promising until I realized that I cannot justify the inequality $|\int_a^b (\sin(f(x)))' / f'(x)| \leq (1/\lambda) |\int_a^b (\sin(f(x)))'|$. Of course, were that true, then the result would follow quickly by evaluating the new integral, applying the triangle inequality, and finally noting that $|\sin(x)| \leq 1$. I've tried a similar approach using the mean value theorem for integrals but was only able to show that there is some $\theta$, $a < \theta < b$, so that $|\int_a^b \cos(f(x))| \leq \frac{1}{\lambda} (|f(\theta) - f(a)| + |f(b) - f(\theta)|)$, which is not strong enough either.",,"['analysis', 'harmonic-analysis']"
51,will a nonempty countable and compact subset of a metric space always contain an isolated point?,will a nonempty countable and compact subset of a metric space always contain an isolated point?,,"Let $(X,d)$ be a metric space, $K \subseteq X$ is nonempty, countable, and compact. I could not come up with an example where such a K has no isolated point(s). so I want to prove that an isolated point will always exist. I tried by first assuming that no point is isolated, then I want to come up with an open cover which will not admit a finite subcover. I don't know how to proceed next.","Let $(X,d)$ be a metric space, $K \subseteq X$ is nonempty, countable, and compact. I could not come up with an example where such a K has no isolated point(s). so I want to prove that an isolated point will always exist. I tried by first assuming that no point is isolated, then I want to come up with an open cover which will not admit a finite subcover. I don't know how to proceed next.",,['analysis']
52,Is the convolution of a function $f(x)$ and a polynomial $p(x)$ always a polynomial?,Is the convolution of a function  and a polynomial  always a polynomial?,f(x) p(x),"After reading the following question: How do I prove a convolution is a polynomial? I want to ask if that is always the expected result, that is to say, does the following holds? A convolution of a function $f(x)$ and a polynomial $p(x)$ will always   result in a polynomial Is that true? If so, how to prove that? if not, can you give a counterexample? If the answer depends on the properties of $f(x)$, continuity, differentiability, or anything else, please describe the required properties. For example, I've easily proved that if $f(x)$ is like: $$f(x)=u(x)\,\mathrm{e}^{-x}\,q(x)$$ where $u(x)$ is the unit step function and $q(x)$ is any polynomial, then the convolution of $f(x)$ with $p(x)$ will be a polynomial with the same degree of $p(x)$.","After reading the following question: How do I prove a convolution is a polynomial? I want to ask if that is always the expected result, that is to say, does the following holds? A convolution of a function $f(x)$ and a polynomial $p(x)$ will always   result in a polynomial Is that true? If so, how to prove that? if not, can you give a counterexample? If the answer depends on the properties of $f(x)$, continuity, differentiability, or anything else, please describe the required properties. For example, I've easily proved that if $f(x)$ is like: $$f(x)=u(x)\,\mathrm{e}^{-x}\,q(x)$$ where $u(x)$ is the unit step function and $q(x)$ is any polynomial, then the convolution of $f(x)$ with $p(x)$ will be a polynomial with the same degree of $p(x)$.",,"['analysis', 'polynomials', 'definite-integrals', 'convolution']"
53,Even integer approximations to multiples of pi,Even integer approximations to multiples of pi,,"I admit that I'm probably out of my depth with this question, but I can't help but feel curious. I wanted to show that, in the sequence $\{\sin(n)\}$, there is never a largest term (the sequence never attains its limit superior). My reasoning was that, given $$ \left| \sin(x) - \sin(y) \right| \leq \left| x - y \right|,$$ if we can furnish an even $M$ with $M = N\pi \pm \epsilon$, then either $\frac{3}{2}M$ or $\frac{1}{2}M$ will nearly equal an odd multiple $K$ of $\frac{\pi}{2}$ such that $\sin(K\frac{\pi}{2}) = 1$. By the inequality, the difference between $\sin(\frac{3}{2}M)$ and $1$ -- or $\sin(\frac{1}{2}M)$ and $1$, whichever -- would be at most $\frac{3}{2} \epsilon$. Thus, the problem reduces to showing that we can get an arbitrarily small $\epsilon$. (I recognize that the inequality above is pretty watered down: the mean value theorem shows that the inequality is as stark as $\cos(\xi) \leq 1$ for $\xi \in (x,y)$, which, if both points $x$ and $y$ are close to $(2k+ \frac{1}{2}) \pi$, is really much stronger than what I've got. This seems like a hard way to prove the claim, so if anybody has a better one, I'd also like to hear about that.) But my main question , which I came to because of the above, is about approximating multiples of $\pi$ by integers. If $\pi = \frac{p}{q} + \epsilon$, then $q\pi - q\epsilon = p$; thus, the size of $q$ becomes important to the accuracy, since the $\epsilon$ we were considering above is $q\epsilon$ in these terms. Spivak's Calculus has a little discussion about this when he proves $e$ is transcendental. He notes that the proof of $e$'s irrationality shows that $\sum_{k=1}^n \frac{n!}{k!} = n!e - R_n,$ with $|R_n| < \frac{3}{n+1}$. The sum on the left can be controlled for parity, since choosing $n$ odd leaves $(\cdots + n + 1)$ at the tail of this sum, and all other terms multiplied by $(n-1)$. So there must exist, given $\epsilon > 0$, an $N$ and an even $M$ such that $M = Ne \pm \epsilon$. (In other words, if $e$ were $\pi$, I'd be home already!) Spivak mentioned that this property - good approximations existing with small denominators - is somehow characteristic of transcendental numbers. ""The number $e$ is by no means unique in this respect: generally speaking, the better a number can be approximated by rational numbers, the worse it is."" So I wonder: Can we furnish an approximation $\frac{p}{q}$ to $\pi$ with $q\epsilon$ arbitrarily small? (For my purposes, can we do better and furnish one with an even $p$?) More generally (and I am out of my depth here, but would enjoy references), what can we prove about the ""goodness"" of rational approximations to transcendental numbers, in the sense of small denominators?","I admit that I'm probably out of my depth with this question, but I can't help but feel curious. I wanted to show that, in the sequence $\{\sin(n)\}$, there is never a largest term (the sequence never attains its limit superior). My reasoning was that, given $$ \left| \sin(x) - \sin(y) \right| \leq \left| x - y \right|,$$ if we can furnish an even $M$ with $M = N\pi \pm \epsilon$, then either $\frac{3}{2}M$ or $\frac{1}{2}M$ will nearly equal an odd multiple $K$ of $\frac{\pi}{2}$ such that $\sin(K\frac{\pi}{2}) = 1$. By the inequality, the difference between $\sin(\frac{3}{2}M)$ and $1$ -- or $\sin(\frac{1}{2}M)$ and $1$, whichever -- would be at most $\frac{3}{2} \epsilon$. Thus, the problem reduces to showing that we can get an arbitrarily small $\epsilon$. (I recognize that the inequality above is pretty watered down: the mean value theorem shows that the inequality is as stark as $\cos(\xi) \leq 1$ for $\xi \in (x,y)$, which, if both points $x$ and $y$ are close to $(2k+ \frac{1}{2}) \pi$, is really much stronger than what I've got. This seems like a hard way to prove the claim, so if anybody has a better one, I'd also like to hear about that.) But my main question , which I came to because of the above, is about approximating multiples of $\pi$ by integers. If $\pi = \frac{p}{q} + \epsilon$, then $q\pi - q\epsilon = p$; thus, the size of $q$ becomes important to the accuracy, since the $\epsilon$ we were considering above is $q\epsilon$ in these terms. Spivak's Calculus has a little discussion about this when he proves $e$ is transcendental. He notes that the proof of $e$'s irrationality shows that $\sum_{k=1}^n \frac{n!}{k!} = n!e - R_n,$ with $|R_n| < \frac{3}{n+1}$. The sum on the left can be controlled for parity, since choosing $n$ odd leaves $(\cdots + n + 1)$ at the tail of this sum, and all other terms multiplied by $(n-1)$. So there must exist, given $\epsilon > 0$, an $N$ and an even $M$ such that $M = Ne \pm \epsilon$. (In other words, if $e$ were $\pi$, I'd be home already!) Spivak mentioned that this property - good approximations existing with small denominators - is somehow characteristic of transcendental numbers. ""The number $e$ is by no means unique in this respect: generally speaking, the better a number can be approximated by rational numbers, the worse it is."" So I wonder: Can we furnish an approximation $\frac{p}{q}$ to $\pi$ with $q\epsilon$ arbitrarily small? (For my purposes, can we do better and furnish one with an even $p$?) More generally (and I am out of my depth here, but would enjoy references), what can we prove about the ""goodness"" of rational approximations to transcendental numbers, in the sense of small denominators?",,"['number-theory', 'analysis', 'approximation']"
54,Showing that a function is the sine function,Showing that a function is the sine function,,"How can I prove the following: If $ f:  \mathbb{R} \rightarrow \mathbb{C} $ is a $2\pi$-periodic function of class $C^{\infty}$ such that $f'(0)=1$ and that for any $n\in \mathbb{N}, x\in\mathbb{R}$, $ \vert f^{(n)}(x) \vert \leq 1 $, then $f$ is the sine function?","How can I prove the following: If $ f:  \mathbb{R} \rightarrow \mathbb{C} $ is a $2\pi$-periodic function of class $C^{\infty}$ such that $f'(0)=1$ and that for any $n\in \mathbb{N}, x\in\mathbb{R}$, $ \vert f^{(n)}(x) \vert \leq 1 $, then $f$ is the sine function?",,"['analysis', 'fourier-analysis']"
55,Diameter and Hausdorff Distance,Diameter and Hausdorff Distance,,"Let $A,B \subset \mathbb R^n$ be non empty compact sets and $d_H$ be Hausdorff distance. I'm thinking that if we know the distance between two sets, the difference between their diameters is bounded. How to (dis)prove that if $d_H(A,B)=r$ then $diam(A)+2r \ge diam(B) \ge diam(A)-2r$?","Let $A,B \subset \mathbb R^n$ be non empty compact sets and $d_H$ be Hausdorff distance. I'm thinking that if we know the distance between two sets, the difference between their diameters is bounded. How to (dis)prove that if $d_H(A,B)=r$ then $diam(A)+2r \ge diam(B) \ge diam(A)-2r$?",,"['analysis', 'analytic-geometry']"
56,Generalization of Hölder's inequality,Generalization of Hölder's inequality,,"Assume $1<p_k< \infty$ for $k=1,\ldots,N$ , and $\displaystyle\sum^N_{k=1}\frac{1}{p_k} =1$. I want to prove that $$\left|\int_X f_1 f_2\cdots f_N\; d\mu \right| \le \lVert f_1\rVert_{p_1} \lVert f_2\rVert_{p_2} \cdots \lVert f_N\rVert_{p_N}.$$ How can I directly adjust Hölder's inequality for it?","Assume $1<p_k< \infty$ for $k=1,\ldots,N$ , and $\displaystyle\sum^N_{k=1}\frac{1}{p_k} =1$. I want to prove that $$\left|\int_X f_1 f_2\cdots f_N\; d\mu \right| \le \lVert f_1\rVert_{p_1} \lVert f_2\rVert_{p_2} \cdots \lVert f_N\rVert_{p_N}.$$ How can I directly adjust Hölder's inequality for it?",,"['analysis', 'measure-theory', 'inequality']"
57,prove $\sum\cos^3{A}+64\prod\cos^3{A}\ge\frac{1}{2}$,prove,\sum\cos^3{A}+64\prod\cos^3{A}\ge\frac{1}{2},"In every acute-angled triangle $ABC$ ,show that $$(\cos{A})^3+(\cos{B})^3+(\cos{C})^3+64(\cos{A})^3(\cos{B})^3(\cos{C})^3\ge\dfrac{1}{2}$$ I want use Schur inequality $$x^3+y^3+z^3+3xyz\ge xy(y+z)+yz(y+z)+zx(z+x)$$ then we have $$x^3+y^3+z^3+6xyz\ge (x+y+z)(xy+yz+zx)$$ But I can't use this to prove my question and I use this post methods links also can't solve my problem,use $AM-GM $ inequality $$\cos^3{A}+\dfrac{\cos{A}}{4}\ge\cos^2{A}$$ so $$LHS\ge \sum_{cyc}\cos^2{A}-\dfrac{1}{4}\sum_{cyc}\cos{A}+64\prod_{cyc}\cos^3{A}$$ use $$\cos^2{A}+\cos^2{B}+\cos^2{C}+2\cos{A}\cos{B}\cos{C}=1$$ it must to prove $$\frac{1}{2}+64\cos^3{A}\cos^3{B}\cos^3{C}\ge 2\cos{A}\cos{B}\cos{C}+\dfrac{1}{4}(\cos{A}+\cos{B}+\cos{C})$$","In every acute-angled triangle ,show that I want use Schur inequality then we have But I can't use this to prove my question and I use this post methods links also can't solve my problem,use inequality so use it must to prove",ABC (\cos{A})^3+(\cos{B})^3+(\cos{C})^3+64(\cos{A})^3(\cos{B})^3(\cos{C})^3\ge\dfrac{1}{2} x^3+y^3+z^3+3xyz\ge xy(y+z)+yz(y+z)+zx(z+x) x^3+y^3+z^3+6xyz\ge (x+y+z)(xy+yz+zx) AM-GM  \cos^3{A}+\dfrac{\cos{A}}{4}\ge\cos^2{A} LHS\ge \sum_{cyc}\cos^2{A}-\dfrac{1}{4}\sum_{cyc}\cos{A}+64\prod_{cyc}\cos^3{A} \cos^2{A}+\cos^2{B}+\cos^2{C}+2\cos{A}\cos{B}\cos{C}=1 \frac{1}{2}+64\cos^3{A}\cos^3{B}\cos^3{C}\ge 2\cos{A}\cos{B}\cos{C}+\dfrac{1}{4}(\cos{A}+\cos{B}+\cos{C}),"['analysis', 'multivariable-calculus', 'trigonometry', 'inequality', 'lagrange-multiplier']"
58,Estimate the bound of the sum of the roots of $1/x+\ln x=a$ where $a>1$,Estimate the bound of the sum of the roots of  where,1/x+\ln x=a a>1,"If $a>1$ then  $\frac{1}{x}+\ln x=a$ has two distinct roots($x_1$ and $x_2$, Assume $x_1<x_2$). Show that $$x_1+x_2+1<3\exp(a-1)$$ First I tried to estimate the place of the roots separately. I have got that $x_1\leq \frac{1}{a}$ and $\exp(a-1)<x_2<\exp(a)$. Then I tried to think about $x_1 + x_2$ as a whole. Because $1/x_1+\ln x_1=a$ and $1/x_2+\ln x_2=a$. I tried to express $x_1+x_2$ as a function of $a$, But I failed. I have no idea to solve this problem. Please help me  :)","If $a>1$ then  $\frac{1}{x}+\ln x=a$ has two distinct roots($x_1$ and $x_2$, Assume $x_1<x_2$). Show that $$x_1+x_2+1<3\exp(a-1)$$ First I tried to estimate the place of the roots separately. I have got that $x_1\leq \frac{1}{a}$ and $\exp(a-1)<x_2<\exp(a)$. Then I tried to think about $x_1 + x_2$ as a whole. Because $1/x_1+\ln x_1=a$ and $1/x_2+\ln x_2=a$. I tried to express $x_1+x_2$ as a function of $a$, But I failed. I have no idea to solve this problem. Please help me  :)",,"['analysis', 'inequality']"
59,Prove $\sin(1/x)$ is discontinuous at 0 using epsilon delta definition of continuity,Prove  is discontinuous at 0 using epsilon delta definition of continuity,\sin(1/x),"Let $$f(x) = \begin{cases} 0 &\text{ if $x=0$,}\\ \sin(1/x) &\text{ otherwise.} \end{cases} $$ Prove that $f$ is discontinuous at $0$ using the $\epsilon \delta$ definition of continuity. I know that the $\epsilon, \delta$ criterion is as follows: for all $\epsilon>0$ there exists $\delta \gt 0$ such that for all $x\in A$, if $|x-x_0|\lt \delta$ implies that  $|f(x)-f(x_0)|\lt \epsilon$. I am unsure how to go about writing the proof and any help would be appreciated! Thank you.","Let $$f(x) = \begin{cases} 0 &\text{ if $x=0$,}\\ \sin(1/x) &\text{ otherwise.} \end{cases} $$ Prove that $f$ is discontinuous at $0$ using the $\epsilon \delta$ definition of continuity. I know that the $\epsilon, \delta$ criterion is as follows: for all $\epsilon>0$ there exists $\delta \gt 0$ such that for all $x\in A$, if $|x-x_0|\lt \delta$ implies that  $|f(x)-f(x_0)|\lt \epsilon$. I am unsure how to go about writing the proof and any help would be appreciated! Thank you.",,"['analysis', 'continuity', 'epsilon-delta']"
60,"Give an example of two closed sets $A, B \subseteq \mathbb{R}$ such that the set $A + B $ is not closed [duplicate]",Give an example of two closed sets  such that the set  is not closed [duplicate],"A, B \subseteq \mathbb{R} A + B ","This question already has answers here : Closed 11 years ago . Possible Duplicate: Sum of two closed sets in $\mathbb R$ is closed? Give an example of two closed sets $A, B \subseteq \mathbb{R}$ such that the set $A + B = \{a + b : a \in A, b \in B\}$ is not closed. This question appears on an old analysis qual I am studying. I know that both $A, B$ must be unbounded sets, because in an earlier part of the problem I have proved that $A + B$ is closed if either of the two sets are compact. The simplest unbounded and closed subset of $\mathbb{R}$ that I know is $\mathbb{Z}$. So I was starting with $A = \mathbb{Z}$, but I'm not yet able to come up with an appropriate $B$. Hints or solutions are greatly appreciated.","This question already has answers here : Closed 11 years ago . Possible Duplicate: Sum of two closed sets in $\mathbb R$ is closed? Give an example of two closed sets $A, B \subseteq \mathbb{R}$ such that the set $A + B = \{a + b : a \in A, b \in B\}$ is not closed. This question appears on an old analysis qual I am studying. I know that both $A, B$ must be unbounded sets, because in an earlier part of the problem I have proved that $A + B$ is closed if either of the two sets are compact. The simplest unbounded and closed subset of $\mathbb{R}$ that I know is $\mathbb{Z}$. So I was starting with $A = \mathbb{Z}$, but I'm not yet able to come up with an appropriate $B$. Hints or solutions are greatly appreciated.",,['analysis']
61,Approximation of bounded measurable functions with continuous functions,Approximation of bounded measurable functions with continuous functions,,"This is not homework. I was reading a paper where the authors showed a result for all continuous functions and then just proceeded to write ""the usual limiting Argument gives the result for all bounded functions"" - so I am asking myself what this ""usual limiting argument"" might be. I do not know whether they mean uniform or pointwise convergence. As I see it pointwise convergence should suffice :D Thus I was am wondering whether there is a theorem having or leading to the following statement: Let $K\subset\mathbb{R}^2$ be compact. Any bounded measurable function $f:K\to\mathbb{R} $ can be approximated by a sequence of continuous functions $(g_m)$ on $K$. Nate Eldredge suggested that I post some excerpt from the original to provide more context for the problem. Here I go: The goal is to proof the existence of a weak limit for a tight sequence of probability measures on $\mathcal{C}^0([0,1]^2,\mathbb{R})$ associated with reflecting Brownian Motions on the compact the set $[0,1]^2$ which is a Lipshitz Domain. Thus we already, know that some weak limit must exist and it remains to show that two limit-Points agree. Weak-Convergence is generally defined via bounded measurable functions. Now let $P'$ and $P''$ be two subsequential limit points. The authors show that $f \in \mathcal{C}^0([0,1]^2,\mathbb{R})$ the following holds  (here $X_s$ the canonical process) $E'f(X_s)=E''f(X_s)$ And now comes the actual source of my question: ""The usual limiting argument gives the result for bounded $f$ and hence the one-dimensional distributions agree."" (the second part I understand only the ""standard limiting argument thing"" is somewhat confusing) Any Help is much appreciated and Thanks in Advance :D","This is not homework. I was reading a paper where the authors showed a result for all continuous functions and then just proceeded to write ""the usual limiting Argument gives the result for all bounded functions"" - so I am asking myself what this ""usual limiting argument"" might be. I do not know whether they mean uniform or pointwise convergence. As I see it pointwise convergence should suffice :D Thus I was am wondering whether there is a theorem having or leading to the following statement: Let $K\subset\mathbb{R}^2$ be compact. Any bounded measurable function $f:K\to\mathbb{R} $ can be approximated by a sequence of continuous functions $(g_m)$ on $K$. Nate Eldredge suggested that I post some excerpt from the original to provide more context for the problem. Here I go: The goal is to proof the existence of a weak limit for a tight sequence of probability measures on $\mathcal{C}^0([0,1]^2,\mathbb{R})$ associated with reflecting Brownian Motions on the compact the set $[0,1]^2$ which is a Lipshitz Domain. Thus we already, know that some weak limit must exist and it remains to show that two limit-Points agree. Weak-Convergence is generally defined via bounded measurable functions. Now let $P'$ and $P''$ be two subsequential limit points. The authors show that $f \in \mathcal{C}^0([0,1]^2,\mathbb{R})$ the following holds  (here $X_s$ the canonical process) $E'f(X_s)=E''f(X_s)$ And now comes the actual source of my question: ""The usual limiting argument gives the result for bounded $f$ and hence the one-dimensional distributions agree."" (the second part I understand only the ""standard limiting argument thing"" is somewhat confusing) Any Help is much appreciated and Thanks in Advance :D",,"['analysis', 'measure-theory']"
62,What is a formal definition of series?,What is a formal definition of series?,,"Is there a formal definition for series? For example, cardinal sum has a formal definition such that $\sum a_i$ = $\bigcup a_i$. Is there any clear definition for series of real or complex number? The definition on my book is the sum of $a_0 + ... + a_n$. This seems very intuitive to me so i don't like it.. I tried to define series such that $\gamma(0) = a_0$ and $\gamma(n+1) = f_n(\gamma(n))$ where $f_n(x) = x+ a_n$ Since $f_n ≠ f_{n+1}$, i can't apply finite recurssion theorem. However it seems obvious that $\gamma$ is a function and unique. How do i prove the existence and uniqueness of $\gamma$? I have proved that ""If A is a set, 'c' a fixed point in A and $f_n : A →A$ a function for every $n\in \mathbb{N}$, then there exists a unique function $\gamma : \mathbb{N} →A$ such that $\gamma(0) = c$ and $\gamma(n+1) = f_n(\gamma(n))$. This is a bit generalized form of original finite recursion theorem. Let $\alpha$ be a sequence. Let $f_n(x)=x+\alpha(n+1) : F→F$. (F denotes an arbitrary field here) Then by above theorem, we can construct $\gamma$. It can be easily checked that $\gamma(n)$ is a summation of $a_0,...,a_n$. By the way, i've never said series is a 'finite summation'. Of course, series is a sequence..","Is there a formal definition for series? For example, cardinal sum has a formal definition such that $\sum a_i$ = $\bigcup a_i$. Is there any clear definition for series of real or complex number? The definition on my book is the sum of $a_0 + ... + a_n$. This seems very intuitive to me so i don't like it.. I tried to define series such that $\gamma(0) = a_0$ and $\gamma(n+1) = f_n(\gamma(n))$ where $f_n(x) = x+ a_n$ Since $f_n ≠ f_{n+1}$, i can't apply finite recurssion theorem. However it seems obvious that $\gamma$ is a function and unique. How do i prove the existence and uniqueness of $\gamma$? I have proved that ""If A is a set, 'c' a fixed point in A and $f_n : A →A$ a function for every $n\in \mathbb{N}$, then there exists a unique function $\gamma : \mathbb{N} →A$ such that $\gamma(0) = c$ and $\gamma(n+1) = f_n(\gamma(n))$. This is a bit generalized form of original finite recursion theorem. Let $\alpha$ be a sequence. Let $f_n(x)=x+\alpha(n+1) : F→F$. (F denotes an arbitrary field here) Then by above theorem, we can construct $\gamma$. It can be easily checked that $\gamma(n)$ is a summation of $a_0,...,a_n$. By the way, i've never said series is a 'finite summation'. Of course, series is a sequence..",,['analysis']
63,Does for every continuous  function $f:R \rightarrow R$ there exist a sequence of analytic functions convergent uniformly to $f$?,Does for every continuous  function  there exist a sequence of analytic functions convergent uniformly to ?,f:R \rightarrow R f,"A function $f: \mathbb{R} \rightarrow \mathbb{R}$ is called $\mathbb{R}$-analytic iff for every $x_0 \in \mathbb{R} $ there exist $R>0$  and power series $\sum_{n=0}^\infty a_n (x-x_0)^n$ convergent for $|x-x_0|<R$ and such that $f(x)=\sum_{n=0}^\infty a_n (x-x_0)^n$ for $|x-x_0|<R$. For some strange continuous functions on $\mathbb{R}$, for example for Weierstrass continuous nondifferentiable function (i.e. $f(x)=\sum_{n=0}^\infty a^n \cos(b^n \pi x)$ for $x\in \mathbb{R}$, where $0<a<1$, $b$ is positive odd integer such that $ab> 1+\frac{3}{2}\pi$), there exist a sequence of $\mathbb{R}$-analytic functions (even entire functions) which converges to $f$ uniformly (in the case of Weierstrass function it is sufficient to take the sequence of partial sum of series defining this function). Is it maybe true that for every continuous function $f: \mathbb{R} \rightarrow \mathbb{R}$ there exists a sequence of $\mathbb{R}$ analytic functions which converges uniformly to $f$ ?","A function $f: \mathbb{R} \rightarrow \mathbb{R}$ is called $\mathbb{R}$-analytic iff for every $x_0 \in \mathbb{R} $ there exist $R>0$  and power series $\sum_{n=0}^\infty a_n (x-x_0)^n$ convergent for $|x-x_0|<R$ and such that $f(x)=\sum_{n=0}^\infty a_n (x-x_0)^n$ for $|x-x_0|<R$. For some strange continuous functions on $\mathbb{R}$, for example for Weierstrass continuous nondifferentiable function (i.e. $f(x)=\sum_{n=0}^\infty a^n \cos(b^n \pi x)$ for $x\in \mathbb{R}$, where $0<a<1$, $b$ is positive odd integer such that $ab> 1+\frac{3}{2}\pi$), there exist a sequence of $\mathbb{R}$-analytic functions (even entire functions) which converges to $f$ uniformly (in the case of Weierstrass function it is sufficient to take the sequence of partial sum of series defining this function). Is it maybe true that for every continuous function $f: \mathbb{R} \rightarrow \mathbb{R}$ there exists a sequence of $\mathbb{R}$ analytic functions which converges uniformly to $f$ ?",,['analysis']
64,Monotonic behavior of a function,Monotonic behavior of a function,,"I have the following problem related to a statistics question: Prove that the function defined for $x\ge 1, y\ge 1$, $$f(x,y)=\frac{\Gamma\left(\frac{x+y}{2}\right)(x/y)^{x/2}}{\Gamma(x/2)\Gamma(y/2)}\int_1^\infty w^{(x/2)-1}\left(1+\frac{xw}{y}\right)^{-(x+y)/2} dw$$ is increasing in $x$ for each $y\ge 1$ and decreasing in $y$ for each $x\ge 1$. (Here $\Gamma$ is the gamma function.) Trying to prove by using derivatives seems difficult.","I have the following problem related to a statistics question: Prove that the function defined for $x\ge 1, y\ge 1$, $$f(x,y)=\frac{\Gamma\left(\frac{x+y}{2}\right)(x/y)^{x/2}}{\Gamma(x/2)\Gamma(y/2)}\int_1^\infty w^{(x/2)-1}\left(1+\frac{xw}{y}\right)^{-(x+y)/2} dw$$ is increasing in $x$ for each $y\ge 1$ and decreasing in $y$ for each $x\ge 1$. (Here $\Gamma$ is the gamma function.) Trying to prove by using derivatives seems difficult.",,"['analysis', 'statistics', 'special-functions']"
65,A measurable function on an atom is almost everywhere constant,A measurable function on an atom is almost everywhere constant,,"Let $f \in m(\Omega,\mathcal{F})$, i.e. $f \mapsto [-\infty,\infty]$ and let $A \in \mathcal{F}$ be an atom. Prove that $f$ is almost everywhere constant on A: there exists $k \in [-\infty,\infty]$ such that $\mu (\{\omega \in A : f(\omega) \neq k \} )=0$. I was thinking let $k=\frac{1}{\mu(A)}\int \limits_{A} f\,d\mu$. Then let $B=\{\omega \in A :f(\omega) \neq k \}$.  Since A is an atom, and B is a subset of A then $\mu(B)=0$, in which case we're done, or $\mu(B)=\mu(A)$.  So I need to show that $\mu(B)<\mu(A)$.","Let $f \in m(\Omega,\mathcal{F})$, i.e. $f \mapsto [-\infty,\infty]$ and let $A \in \mathcal{F}$ be an atom. Prove that $f$ is almost everywhere constant on A: there exists $k \in [-\infty,\infty]$ such that $\mu (\{\omega \in A : f(\omega) \neq k \} )=0$. I was thinking let $k=\frac{1}{\mu(A)}\int \limits_{A} f\,d\mu$. Then let $B=\{\omega \in A :f(\omega) \neq k \}$.  Since A is an atom, and B is a subset of A then $\mu(B)=0$, in which case we're done, or $\mu(B)=\mu(A)$.  So I need to show that $\mu(B)<\mu(A)$.",,"['analysis', 'measure-theory']"
66,"$f$ is monotonically increasing, $0 \le f \le 1$ and $\int_0^1 (f(x) - x) dx = 0$ then $\int_0^1|f(x)-x|dx \le \frac{1}{2}$.","is monotonically increasing,  and  then .",f 0 \le f \le 1 \int_0^1 (f(x) - x) dx = 0 \int_0^1|f(x)-x|dx \le \frac{1}{2},"$f(x)$ is monotonically increasing in $[0,1]$ , $0 \le f \le 1$ and $\int_0^1 (f(x) - x) \mathrm{d}x = 0$ . Prove that $\int_0^1|f(x)-x|\mathrm{d}x \le \frac{1}{2}$ . It's easy if $f(x) \ge x$ in $[0,1]$ . And even in $[a,b]$ we have $\int_a^b |f(x)-x|\mathrm{d}x \le \frac{(b-a)^2}{2}$ . But the zero points of $f(x) - x$ may be infinitely many. This is where difficulty exists.","is monotonically increasing in , and . Prove that . It's easy if in . And even in we have . But the zero points of may be infinitely many. This is where difficulty exists.","f(x) [0,1] 0 \le f \le 1 \int_0^1 (f(x) - x) \mathrm{d}x = 0 \int_0^1|f(x)-x|\mathrm{d}x \le \frac{1}{2} f(x) \ge x [0,1] [a,b] \int_a^b |f(x)-x|\mathrm{d}x \le \frac{(b-a)^2}{2} f(x) - x",['analysis']
67,Frechet derivative of a composition of functions over matrices,Frechet derivative of a composition of functions over matrices,,"In control theory, the discrete Lyapunov equation is defined as \begin{align*} A^T X A + Q  = X, \end{align*}  where $A \in \mathcal{M}(n \times n; \mathbb R)$ and $Q \in \mathbb {S}_{++}$ ( positive definite matrices). There is a theorem stating if the spectral radius of $A$ satisfies $\rho(A) < 1$ and for fixed $Q > 0$, there exists a unique $X \in \mathbb {S}_{++}$ which solves above equation. Let $D = \{A \in \mathcal{M}(n \times n; \mathbb R): \rho(A) < 1\}$ and fix $Q$. Suppose we define some scalar valued function $f$ over $X$ which are solutions of Lyapunov equation over $D$. To make it more concrete, let us define this scalar valued function to be $f(X) = \text{tr}(X)$. This function can be also viewed as a function $g$ over $D$, i.e., it is a composition \begin{align*} g \colon A \xrightarrow{h} X \xrightarrow{f} \text{tr}(X). \end{align*}  Now I would like to differentiate $g$ with respect to $A$. Is it possible to find an explict formula for this Frechet derivative? The difficulty is the first function $h$ is not explicitly defined. Another question to ask is whether this $h : A \mapsto X$ is continuous.","In control theory, the discrete Lyapunov equation is defined as \begin{align*} A^T X A + Q  = X, \end{align*}  where $A \in \mathcal{M}(n \times n; \mathbb R)$ and $Q \in \mathbb {S}_{++}$ ( positive definite matrices). There is a theorem stating if the spectral radius of $A$ satisfies $\rho(A) < 1$ and for fixed $Q > 0$, there exists a unique $X \in \mathbb {S}_{++}$ which solves above equation. Let $D = \{A \in \mathcal{M}(n \times n; \mathbb R): \rho(A) < 1\}$ and fix $Q$. Suppose we define some scalar valued function $f$ over $X$ which are solutions of Lyapunov equation over $D$. To make it more concrete, let us define this scalar valued function to be $f(X) = \text{tr}(X)$. This function can be also viewed as a function $g$ over $D$, i.e., it is a composition \begin{align*} g \colon A \xrightarrow{h} X \xrightarrow{f} \text{tr}(X). \end{align*}  Now I would like to differentiate $g$ with respect to $A$. Is it possible to find an explict formula for this Frechet derivative? The difficulty is the first function $h$ is not explicitly defined. Another question to ask is whether this $h : A \mapsto X$ is continuous.",,"['analysis', 'derivatives', 'matrix-calculus', 'control-theory', 'frechet-derivative']"
68,How much of Stirling is in Stirling's formula?,How much of Stirling is in Stirling's formula?,,"This is a naive question about history. My understanding is that Stirling's formula or something trivially equivalent to it first appeared in an early edition of Abraham de Moivre's book The Doctrine of Chances (and maybe in a journal article he wrote before that?), provided that we understand ""Stirling's formula"" to mean that $$ \lim_{n\to\infty} \frac{\sqrt{n}\cdot  n^n e^{-n}}{n!} = \text{some finite non-zero number}. $$ If I'm not mistaken, de Moivre computed this number numerically and it was James Stirling who later showed just which number it is, and then de Moivre included it in a later edition of his book. It's been a while since I read about all this and I don't remember enough detail to be 100% sure I've got all of the above right. But today the term ""Stirling's formula"" seems to be used to refer to an asymptotic expansion. How much of that series called ""Stirling's formula"" can be attributed to Stirling?  And who should get credit for what he shouldn't get credit for?","This is a naive question about history. My understanding is that Stirling's formula or something trivially equivalent to it first appeared in an early edition of Abraham de Moivre's book The Doctrine of Chances (and maybe in a journal article he wrote before that?), provided that we understand ""Stirling's formula"" to mean that $$ \lim_{n\to\infty} \frac{\sqrt{n}\cdot  n^n e^{-n}}{n!} = \text{some finite non-zero number}. $$ If I'm not mistaken, de Moivre computed this number numerically and it was James Stirling who later showed just which number it is, and then de Moivre included it in a later edition of his book. It's been a while since I read about all this and I don't remember enough detail to be 100% sure I've got all of the above right. But today the term ""Stirling's formula"" seems to be used to refer to an asymptotic expansion. How much of that series called ""Stirling's formula"" can be attributed to Stirling?  And who should get credit for what he shouldn't get credit for?",,"['analysis', 'math-history', 'factorial']"
69,Fourier series of almost periodic functions and regularity,Fourier series of almost periodic functions and regularity,,"Let $f$ a $2\pi$-periodic function represented by its Fourier series $\displaystyle\sum_{k=-\infty}^{+\infty}c_ke^{ikx}$. We know that $f$ is smooth if we have $\displaystyle\lim_{|n|\to +\infty}|c_n|n^k =0$ for all $k\in\mathbb{N}$; if $f$ is $C^k$ we have $\displaystyle\lim_{|n|\to +\infty}n^k|c_n|=0$ and if $c_n =o\left(\dfrac 1{|n|^{k+2}}\right)$ then $f$ is $c_k$. The space of almost periodic functions is the closure for the uniform norm of $\mathrm{Span}\left\{e^{i\lambda x},\lambda\in\mathbb R\right\}$. We define  $$a(\lambda,f) := \lim_{T\to +\infty}\dfrac 1{2T}\int_{-T}^Tf(t)e^{-i\lambda t}dt$$ and if we put $C:=\left\{\lambda\in\mathbb{R},a(\lambda,f)\neq 0\right\}$, then $C$ is at most countable and we can associate a series $\displaystyle\sum_{\lambda\in C}a(\lambda,f)e^{i\lambda t}$. The numbers $a(\lambda,f)$ are the Fourier coefficients of $f$ and $\lambda\in C$ the Fourier exponents. The question is: are there some properties of the Fourier coefficients and exponents which allow us to ""read"" the regularity of an almost periodic function?","Let $f$ a $2\pi$-periodic function represented by its Fourier series $\displaystyle\sum_{k=-\infty}^{+\infty}c_ke^{ikx}$. We know that $f$ is smooth if we have $\displaystyle\lim_{|n|\to +\infty}|c_n|n^k =0$ for all $k\in\mathbb{N}$; if $f$ is $C^k$ we have $\displaystyle\lim_{|n|\to +\infty}n^k|c_n|=0$ and if $c_n =o\left(\dfrac 1{|n|^{k+2}}\right)$ then $f$ is $c_k$. The space of almost periodic functions is the closure for the uniform norm of $\mathrm{Span}\left\{e^{i\lambda x},\lambda\in\mathbb R\right\}$. We define  $$a(\lambda,f) := \lim_{T\to +\infty}\dfrac 1{2T}\int_{-T}^Tf(t)e^{-i\lambda t}dt$$ and if we put $C:=\left\{\lambda\in\mathbb{R},a(\lambda,f)\neq 0\right\}$, then $C$ is at most countable and we can associate a series $\displaystyle\sum_{\lambda\in C}a(\lambda,f)e^{i\lambda t}$. The numbers $a(\lambda,f)$ are the Fourier coefficients of $f$ and $\lambda\in C$ the Fourier exponents. The question is: are there some properties of the Fourier coefficients and exponents which allow us to ""read"" the regularity of an almost periodic function?",,"['analysis', 'fourier-series']"
70,Algorithmic Analysis Simplified under Big O,Algorithmic Analysis Simplified under Big O,,"Hi I am revising for my exams and I have the following inhomogeneous first order recurrence relation defined as follows: f(0) = 2 f(n) = 6f(n-1) - 5, n > 0 I have tried for ages using the methods I have been taught to solve this but I cannot get a proper solution. 1.  Integrate a new function g(n) 2.     f(n) = 6^n.g(n) 3.  => 6^n.g(n) = 6.6^(n-1) .g(n-1) -5    4.  => g(n) = g(n-1)-5/6^n 5.  => g(n) = sum(i=1, n)-5/6^i 6.  => f(n) = 6^n.sum(i=1, n)-5/6^i 7.  => *Evaluate the sum using geometric series forumla* 8.  => sum(i = 1, n)-5/6^i = [sum(i = 1, n)a^i] -------> (a = -5/6) 9.  => *sub a = -5/6 into geometric forumla [a(1-a^n)/(1-a)]* 10. => [(-5/6(1 - (-5/6)^n))/(1-(-5/6))] 11. => g(n) = [(-5/6(1 + (5/6)^n))/(1+5/6)] 12. => f(n) = 6^n . g(n) = 6^n[(-5/6(1 + (5/6)^n))/(1+5/6)] 13. => *sub in n = 0 to see if f(0) = 2* I cannot get this working, however. f(0) [base case] doesn't equal 2...Where have I gone wrong?? Just to let you know here is the example I am following: f(0) = 0 f(n) = 3f(n-1)+1, n>0  f(n) = 3^n.g(n) 3^n.g(n) = g(n-1)+(1/3)^n g(n) = sum(i=1, n)(1/3)^i f(n) = 3^n . sum(i=1, n)(1/3)^n sum(i=1, n) = sum(i=1, n)(a^i) ----> a = 1/3 sub into geometric series formula gives:  1/2(1-(1/3^n)  Hence: f(n) = 3^n/2(1-(1/3^n)) = 1/2(3^n - 1) = O(3^n) Now my maths isn't great, I know enough to get about but I have followed the exact steps as my lecturer did in the example, but I cannot get a solution to fit f(0). I have to follow the methods used in above example and I am absolutely stumped as to where the issue is..","Hi I am revising for my exams and I have the following inhomogeneous first order recurrence relation defined as follows: f(0) = 2 f(n) = 6f(n-1) - 5, n > 0 I have tried for ages using the methods I have been taught to solve this but I cannot get a proper solution. 1.  Integrate a new function g(n) 2.     f(n) = 6^n.g(n) 3.  => 6^n.g(n) = 6.6^(n-1) .g(n-1) -5    4.  => g(n) = g(n-1)-5/6^n 5.  => g(n) = sum(i=1, n)-5/6^i 6.  => f(n) = 6^n.sum(i=1, n)-5/6^i 7.  => *Evaluate the sum using geometric series forumla* 8.  => sum(i = 1, n)-5/6^i = [sum(i = 1, n)a^i] -------> (a = -5/6) 9.  => *sub a = -5/6 into geometric forumla [a(1-a^n)/(1-a)]* 10. => [(-5/6(1 - (-5/6)^n))/(1-(-5/6))] 11. => g(n) = [(-5/6(1 + (5/6)^n))/(1+5/6)] 12. => f(n) = 6^n . g(n) = 6^n[(-5/6(1 + (5/6)^n))/(1+5/6)] 13. => *sub in n = 0 to see if f(0) = 2* I cannot get this working, however. f(0) [base case] doesn't equal 2...Where have I gone wrong?? Just to let you know here is the example I am following: f(0) = 0 f(n) = 3f(n-1)+1, n>0  f(n) = 3^n.g(n) 3^n.g(n) = g(n-1)+(1/3)^n g(n) = sum(i=1, n)(1/3)^i f(n) = 3^n . sum(i=1, n)(1/3)^n sum(i=1, n) = sum(i=1, n)(a^i) ----> a = 1/3 sub into geometric series formula gives:  1/2(1-(1/3^n)  Hence: f(n) = 3^n/2(1-(1/3^n)) = 1/2(3^n - 1) = O(3^n) Now my maths isn't great, I know enough to get about but I have followed the exact steps as my lecturer did in the example, but I cannot get a solution to fit f(0). I have to follow the methods used in above example and I am absolutely stumped as to where the issue is..",,"['analysis', 'algorithms', 'asymptotics']"
71,Proof of Zorn's Lemma,Proof of Zorn's Lemma,,"I am looking at the Zorn's Lemma. The axiom of choice is the following:  Let $I$ be an arbitrary set of indices and let $A_i$ be a family of non-empty sets ( $A_i\neq \emptyset$ ), then there exists a map \begin{equation*}f: \ I\rightarrow \bigcup_{i\in I}A_i \ \text{ with } \ f(I)\in A_i\end{equation*} The Zorn's Lemma is the following:  Each nonempty partially ordered set in which each totally ordered subset has an upper bound contains at least one maximum element. $$$$ I am looking at the proof: A sketch of the proof of Zorn's lemma follows, assuming the axiom of choice. Suppose the lemma is false. Then there exists a partially ordered set $P$ such that every totally ordered subset has an upper bound, and every element has a bigger one. For every totally ordered subset $T$ we may then define a bigger element b(T), because T has an upper bound, and that upper bound has a bigger element. To actually define the function b, we need to employ the axiom of choice. Using the function $b$ , we are going to define elements $a_0 < a_1 < a_2 < a_3 < \ldots$ in $P$ . This sequence is really long: the indices are not just the natural numbers, but all ordinals. In fact, the sequence is too long for the set $P$ ; there are too many ordinals (a proper class), more than there are elements in any set, and the set $P$ will be exhausted before long and then we will run into the desired contradiction. The $a_i$ are defined by transfinite recursion: we pick $a_0$ in $P$ arbitrary (this is possible, since $P$ contains an upper bound for the empty set and is thus not empty) and for any other ordinal $w$ we set $a_w = b(\{a_v: v < w\})$ . Because the $a_v$ are totally ordered, this is a well-founded definition. This proof shows that actually a slightly stronger version of Zorn's lemma is true: If $P$ is a poset in which every well-ordered subset has an upper bound, and if $x$ is any element of $P$ , then $P$ has a maximal element greater than or equal to $x$ . That is, there is a maximal element which is comparable to $x$ . $$$$ I have some questions: Then there exists a partially ordered set $P$ such that every totally ordered subset has an upper bound, and every element has a bigger one. This is the negation of Zorn's lemma, isn't it? $$$$ Using the function $b$ , we are going to define elements $a_0 < a_1 < a_2 < a_3 < \ldots$ in $P$ . This sequence is really long: the indices are not just the natural numbers, but all ordinals. In fact, the sequence is too long for the set $P$ ; there are too many ordinals (a proper class), more than there are elements in any set, and the set P will be exhausted before long and then we will run into the desired contradiction. Could you explain me this part?   What exactly is an ordinal? And how do we get the contradiction? $$$$ The $a_i$ are defined by transfinite recursion: we pick $a_0$ in $P$ arbitrary (this is possible, since $P$ contains an upper bound for the empty set and is thus not empty) and for any other ordinal $w$ we set $a_w = b(\{a_v: v < w\})$ . Because the $a_v$ are totally ordered, this is a well-founded definition. Do we define these $a_i$ by taking a subset of $P$ and then we get a bigger element, then we take a bigger subset and so we get a bigger element, and so on?","I am looking at the Zorn's Lemma. The axiom of choice is the following:  Let be an arbitrary set of indices and let be a family of non-empty sets ( ), then there exists a map The Zorn's Lemma is the following:  Each nonempty partially ordered set in which each totally ordered subset has an upper bound contains at least one maximum element. I am looking at the proof: A sketch of the proof of Zorn's lemma follows, assuming the axiom of choice. Suppose the lemma is false. Then there exists a partially ordered set such that every totally ordered subset has an upper bound, and every element has a bigger one. For every totally ordered subset we may then define a bigger element b(T), because T has an upper bound, and that upper bound has a bigger element. To actually define the function b, we need to employ the axiom of choice. Using the function , we are going to define elements in . This sequence is really long: the indices are not just the natural numbers, but all ordinals. In fact, the sequence is too long for the set ; there are too many ordinals (a proper class), more than there are elements in any set, and the set will be exhausted before long and then we will run into the desired contradiction. The are defined by transfinite recursion: we pick in arbitrary (this is possible, since contains an upper bound for the empty set and is thus not empty) and for any other ordinal we set . Because the are totally ordered, this is a well-founded definition. This proof shows that actually a slightly stronger version of Zorn's lemma is true: If is a poset in which every well-ordered subset has an upper bound, and if is any element of , then has a maximal element greater than or equal to . That is, there is a maximal element which is comparable to . I have some questions: Then there exists a partially ordered set such that every totally ordered subset has an upper bound, and every element has a bigger one. This is the negation of Zorn's lemma, isn't it? Using the function , we are going to define elements in . This sequence is really long: the indices are not just the natural numbers, but all ordinals. In fact, the sequence is too long for the set ; there are too many ordinals (a proper class), more than there are elements in any set, and the set P will be exhausted before long and then we will run into the desired contradiction. Could you explain me this part?   What exactly is an ordinal? And how do we get the contradiction? The are defined by transfinite recursion: we pick in arbitrary (this is possible, since contains an upper bound for the empty set and is thus not empty) and for any other ordinal we set . Because the are totally ordered, this is a well-founded definition. Do we define these by taking a subset of and then we get a bigger element, then we take a bigger subset and so we get a bigger element, and so on?",I A_i A_i\neq \emptyset \begin{equation*}f: \ I\rightarrow \bigcup_{i\in I}A_i \ \text{ with } \ f(I)\in A_i\end{equation*}  P T b a_0 < a_1 < a_2 < a_3 < \ldots P P P a_i a_0 P P w a_w = b(\{a_v: v < w\}) a_v P x P P x x  P  b a_0 < a_1 < a_2 < a_3 < \ldots P P  a_i a_0 P P w a_w = b(\{a_v: v < w\}) a_v a_i P,"['analysis', 'proof-explanation', 'set-theory', 'axiom-of-choice']"
72,Is there any positive function $f$ such that $f(x)f(y)\leq |x-y|$ for every $x$ rational and $y$ irrational? [duplicate],Is there any positive function  such that  for every  rational and  irrational? [duplicate],f f(x)f(y)\leq |x-y| x y,"This question already has answers here : Is there a positive function $f$ on real line such that $f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q$? (2 answers) Closed 6 years ago . We are given a function $f$ from reals to positive reals (not including $0)$ satisfying    $$f(x)f(y)\leq |x-y|$$   for every rational number $x$ and irrational number $y$. Does this function exist? If $f$ is continuous, it is easy to show that $f$ must be a constant zero function (So it does not exist, in this case). Otherwise, for an irrational number $y$ it can be shown that for any sequence $x_n$ of rationals converging to $y$, $f(x_n)$ converges to zero and the same is true for rational number $x$ and the sequence of irrationals $y_n$. Does this argument give us any information about $f$, and does it say whether $f$ exists at all?","This question already has answers here : Is there a positive function $f$ on real line such that $f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q$? (2 answers) Closed 6 years ago . We are given a function $f$ from reals to positive reals (not including $0)$ satisfying    $$f(x)f(y)\leq |x-y|$$   for every rational number $x$ and irrational number $y$. Does this function exist? If $f$ is continuous, it is easy to show that $f$ must be a constant zero function (So it does not exist, in this case). Otherwise, for an irrational number $y$ it can be shown that for any sequence $x_n$ of rationals converging to $y$, $f(x_n)$ converges to zero and the same is true for rational number $x$ and the sequence of irrationals $y_n$. Does this argument give us any information about $f$, and does it say whether $f$ exists at all?",,"['analysis', 'functional-equations']"
73,Poisson summation formula and Schwartz functions,Poisson summation formula and Schwartz functions,,"I am reading a proof of the Poisson summation formula which states that (with my version of the Fourier transform - I think they sometimes vary by a constant factor) for $f$ a Schwartz function on $\mathbb{R}$ (that is, a smooth function with all derivatives of $f(x)$ of all orders decay faster than any polynomial function as $|x| \to \infty$), the following relationship holds: $$\sum \limits_{n \in \mathbb{Z}}f(n) = \sum \limits_{n \in \mathbb{Z}}\hat{f}(2\pi n)$$ where $\hat{f}$ denotes the Fourier transform. The proof goes as follows: define 2 functions on $\mathbb{T} = \{z: |z| = 1\}$ by $F, G: \mathbb{T} \to \mathbb{C}$ by $F(\theta) = \sum \limits_{n \in \mathbb{Z}}\hat{f}(2\pi n) e^{2 \pi i n \theta}$, and $G(\theta) = \sum \limits_{k \in \mathbb{Z}}f(\theta + k)$. Then take Fourier transforms and show they are equal. After showing also that $F$ and $G$ are both Schwartz functions on $\mathbb{T}$, we apply uniqueness of Fourier series to show that $F=G$. Now uniqueness here relies very much on the fact that both $F$ and $G$ are Schwartz functions on $\mathbb{T}$: but a Schwartz function on $\mathbb{T}$ is simply an element of $C^\infty (\mathbb{T})$, i.e. a smooth function on $\mathbb{T}$. So, my question is this: how do we know (or show) that $F$ and $G$ are smooth? It is clear both are periodic, so it will suffice to show smoothness within their respective periods I suppose. $f$ is Schwartz on $\mathbb{R}$ which means it is smooth, but we are taking an infinite sum of translations of $f$, so it is not manifestly clear that either $F$ or $G$ will remain smooth. How do we show this? I guess we need to make use of the fact that at large values $f$ is very fast-decaying to show that most terms are ""insignificant"" in the sums, but whenever I tried to prove the smoothness formally it became messy. Is there a nice trick to showing $F$ and $G$ are smooth on $\mathbb{T}$? I would be very grateful for a proof of the fact. Many thanks in advance.","I am reading a proof of the Poisson summation formula which states that (with my version of the Fourier transform - I think they sometimes vary by a constant factor) for $f$ a Schwartz function on $\mathbb{R}$ (that is, a smooth function with all derivatives of $f(x)$ of all orders decay faster than any polynomial function as $|x| \to \infty$), the following relationship holds: $$\sum \limits_{n \in \mathbb{Z}}f(n) = \sum \limits_{n \in \mathbb{Z}}\hat{f}(2\pi n)$$ where $\hat{f}$ denotes the Fourier transform. The proof goes as follows: define 2 functions on $\mathbb{T} = \{z: |z| = 1\}$ by $F, G: \mathbb{T} \to \mathbb{C}$ by $F(\theta) = \sum \limits_{n \in \mathbb{Z}}\hat{f}(2\pi n) e^{2 \pi i n \theta}$, and $G(\theta) = \sum \limits_{k \in \mathbb{Z}}f(\theta + k)$. Then take Fourier transforms and show they are equal. After showing also that $F$ and $G$ are both Schwartz functions on $\mathbb{T}$, we apply uniqueness of Fourier series to show that $F=G$. Now uniqueness here relies very much on the fact that both $F$ and $G$ are Schwartz functions on $\mathbb{T}$: but a Schwartz function on $\mathbb{T}$ is simply an element of $C^\infty (\mathbb{T})$, i.e. a smooth function on $\mathbb{T}$. So, my question is this: how do we know (or show) that $F$ and $G$ are smooth? It is clear both are periodic, so it will suffice to show smoothness within their respective periods I suppose. $f$ is Schwartz on $\mathbb{R}$ which means it is smooth, but we are taking an infinite sum of translations of $f$, so it is not manifestly clear that either $F$ or $G$ will remain smooth. How do we show this? I guess we need to make use of the fact that at large values $f$ is very fast-decaying to show that most terms are ""insignificant"" in the sums, but whenever I tried to prove the smoothness formally it became messy. Is there a nice trick to showing $F$ and $G$ are smooth on $\mathbb{T}$? I would be very grateful for a proof of the fact. Many thanks in advance.",,"['analysis', 'fourier-analysis', 'derivatives', 'fourier-series']"
74,"Continuous $f$, $\sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|<\infty$, then exist only one real matrix $A$, such that $f(x)-Ax$ is bounded.","Continuous , , then exist only one real matrix , such that  is bounded.","f \sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|<\infty A f(x)-Ax","Problem: f is a continuous function, $f: \mathbb{R}^m\to \mathbb{R}^n$ , $\sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|<\infty$ , there exist only one real matrix $A$ , such that $f(x)-Ax$ is bounded. My thoughts 1: $\sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|:=M<\infty$ , then $\|f(2x)-2f(x)\|<M$ , $\|f(3x)-f(2x)-f(x)\|<M$ , so $\|f(3x)-3f(x)\|\leq\|f(3x)-f(2x)-f(x)+f(2x)-2f(x)\|\leq2M$ , so by reduction, we have $\|f(nx)-nf(x)\|<(n-1)M$ , then $\|\frac{f(nx)}{n}-f(x)\|<(1-\frac{1}{n})M$ I want to fix $x$ and let $n \to \infty$ , but is hard to show $\lim_{n \to \infty}\frac{f(nx)}{n}$ exists. My thoughts 2: If we assert $\lim_{n \to \infty}\frac{f(nx)}{n}$ exists, define $G(x)=\lim_{n \to \infty}\frac{f(nx)}{n}$ , I want to show $G(x)$ is linear. If so, $G(x)=Ax$ , then the $A$ is what we want. My thoughts: The limit may not exist. Three steps: 1. $R^1$ case 2.each element in $R^n$ is actually the $R^1$ case 3.different elements are ""independent""","Problem: f is a continuous function, , , there exist only one real matrix , such that is bounded. My thoughts 1: , then , , so , so by reduction, we have , then I want to fix and let , but is hard to show exists. My thoughts 2: If we assert exists, define , I want to show is linear. If so, , then the is what we want. My thoughts: The limit may not exist. Three steps: 1. case 2.each element in is actually the case 3.different elements are ""independent""","f: \mathbb{R}^m\to \mathbb{R}^n \sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|<\infty A f(x)-Ax \sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|:=M<\infty \|f(2x)-2f(x)\|<M \|f(3x)-f(2x)-f(x)\|<M \|f(3x)-3f(x)\|\leq\|f(3x)-f(2x)-f(x)+f(2x)-2f(x)\|\leq2M \|f(nx)-nf(x)\|<(n-1)M \|\frac{f(nx)}{n}-f(x)\|<(1-\frac{1}{n})M x n \to \infty \lim_{n \to \infty}\frac{f(nx)}{n} \lim_{n \to \infty}\frac{f(nx)}{n} G(x)=\lim_{n \to \infty}\frac{f(nx)}{n} G(x) G(x)=Ax A R^1 R^n R^1",['analysis']
75,Prove that region under graph of function is measurable,Prove that region under graph of function is measurable,,"In the measure theory book that I am studying, we consider the 'area' under (i.e. the product measure of) the graph of a function as an example of an application of Fubini's Theorem for integrals (with respect to measures). The setting: $(X,\mathcal{A}, \mu)$ is a $\sigma$ -finite measure space, $\lambda$ is Lebesgue measure on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ (Borel $\sigma$ -algebra), $f:X \to [0,+\infty]$ is $\mathcal{A}$ -measurable, and we are considering the region under the graph of $f$ , $E=\{(x,y)\in X \times \mathbb{R}|0\leq y < f(x)\}$ . I need to prove $E \in \mathcal{A} \times \mathcal{B}(\mathbb{R})$ . I thought to write $E=g^{-1}((0,+\infty])\cap(X \times [0,+\infty])$ where $g(x,y)=f(x)-y$ but I can't see why $g$ must be $\mathcal{A} \times \mathcal{B}(\mathbb{R})$ -measurable. Any help would be appreciated.","In the measure theory book that I am studying, we consider the 'area' under (i.e. the product measure of) the graph of a function as an example of an application of Fubini's Theorem for integrals (with respect to measures). The setting: is a -finite measure space, is Lebesgue measure on (Borel -algebra), is -measurable, and we are considering the region under the graph of , . I need to prove . I thought to write where but I can't see why must be -measurable. Any help would be appreciated.","(X,\mathcal{A}, \mu) \sigma \lambda (\mathbb{R},\mathcal{B}(\mathbb{R})) \sigma f:X \to [0,+\infty] \mathcal{A} f E=\{(x,y)\in X \times \mathbb{R}|0\leq y < f(x)\} E \in \mathcal{A} \times \mathcal{B}(\mathbb{R}) E=g^{-1}((0,+\infty])\cap(X \times [0,+\infty]) g(x,y)=f(x)-y g \mathcal{A} \times \mathcal{B}(\mathbb{R})","['analysis', 'measure-theory', 'graphing-functions']"
76,Is the gradient operator surjective?,Is the gradient operator surjective?,,"Let $\Omega \subset \mathbb{R}^{n}$ be open and bounded with Lipschitz boundary. Is the gradient operator $\nabla :H^{1} ( \Omega ) \rightarrow L^{2} ( \Omega )$ surjective? Here $H^{1} ( \Omega ) =W^{1,2} ( \Omega )$ the Sobolev space of real valued functions in $L^{2}$ with weak derivative in $L^{2}$. For unbounded sets in $\mathbb{R}$ the answer is clearly no, since any compactly supported continuous function is in $L^{2} ( \mathbb{R} )$ but has non integrable primitive. So, if I try to disprove this for bounded sets, my usual simple test for these things, the set $( 0,1 ) \subset \mathbb{R}$ and the function $x^{\alpha}$, which is in $L^{p}$ iff $\alpha >-n/p$ doesn't really help much as a candidate. Integrating is always possible and yields a function in $H^{1} ( \Omega )$. Ideas? Solution anyone? What about vector valued functions?","Let $\Omega \subset \mathbb{R}^{n}$ be open and bounded with Lipschitz boundary. Is the gradient operator $\nabla :H^{1} ( \Omega ) \rightarrow L^{2} ( \Omega )$ surjective? Here $H^{1} ( \Omega ) =W^{1,2} ( \Omega )$ the Sobolev space of real valued functions in $L^{2}$ with weak derivative in $L^{2}$. For unbounded sets in $\mathbb{R}$ the answer is clearly no, since any compactly supported continuous function is in $L^{2} ( \mathbb{R} )$ but has non integrable primitive. So, if I try to disprove this for bounded sets, my usual simple test for these things, the set $( 0,1 ) \subset \mathbb{R}$ and the function $x^{\alpha}$, which is in $L^{p}$ iff $\alpha >-n/p$ doesn't really help much as a candidate. Integrating is always possible and yields a function in $H^{1} ( \Omega )$. Ideas? Solution anyone? What about vector valued functions?",,"['analysis', 'sobolev-spaces']"
77,How to prove $e^{\log(1+x)}= 1+x$ by series expansion?,How to prove  by series expansion?,e^{\log(1+x)}= 1+x,"as the title says, i want to prove $e^{\log(1+x)}= 1+x$, by substitute $\log(1+x) = \sum _{i=1} ^{\infty} \frac{(-1)^{i+1}x^i}{i}$ and $e^x=\sum _{i=0} ^{\infty} \frac{x^i}{i!} $. Can some one help me?","as the title says, i want to prove $e^{\log(1+x)}= 1+x$, by substitute $\log(1+x) = \sum _{i=1} ^{\infty} \frac{(-1)^{i+1}x^i}{i}$ and $e^x=\sum _{i=0} ^{\infty} \frac{x^i}{i!} $. Can some one help me?",,"['analysis', 'operator-theory']"
78,Show $e^{D}(f(x)) = f(x+1)$ where $D$ is the derivative operator,Show  where  is the derivative operator,e^{D}(f(x)) = f(x+1) D,"I would appreciate help showing $e^{D}(f(x)) = f(x+1)$ Where $D$ is the linear operator $D: \mathbb{C}[x] \rightarrow \mathbb{C}[x]$ where (in the context where this statement arose) $x \in \mathbb{N}$; $f(x) \mapsto \frac{d}{dx} f(x)$ By the Taylor series expansion $e^{D} = \sum_{n=0}^{\infty} \frac{D^n}{n!}$ $(1)$ Then $e^{D} (f(x)) = f(x) + f'(x) + \frac{f''(x)}{2!} +\dots$ I would appreciate help showing that the above line $(1)$ is equal to $f(x +1)$ This is what I have tried, but it feels forced: for a Taylor series representation of $f(x +1)$ near $x$ I could write $f(x+1) = f(x) + f'(x)(x+1 - x) + \frac{f''(x)}{2!}(x+1 -x)^2 \dots$ This then is equal to the RHS of line $(1)$. Could this line of thinking be correct? Thanks very much.","I would appreciate help showing $e^{D}(f(x)) = f(x+1)$ Where $D$ is the linear operator $D: \mathbb{C}[x] \rightarrow \mathbb{C}[x]$ where (in the context where this statement arose) $x \in \mathbb{N}$; $f(x) \mapsto \frac{d}{dx} f(x)$ By the Taylor series expansion $e^{D} = \sum_{n=0}^{\infty} \frac{D^n}{n!}$ $(1)$ Then $e^{D} (f(x)) = f(x) + f'(x) + \frac{f''(x)}{2!} +\dots$ I would appreciate help showing that the above line $(1)$ is equal to $f(x +1)$ This is what I have tried, but it feels forced: for a Taylor series representation of $f(x +1)$ near $x$ I could write $f(x+1) = f(x) + f'(x)(x+1 - x) + \frac{f''(x)}{2!}(x+1 -x)^2 \dots$ This then is equal to the RHS of line $(1)$. Could this line of thinking be correct? Thanks very much.",,[]
79,Why does $d(\varphi^*f)=\varphi^*df$?,Why does ?,d(\varphi^*f)=\varphi^*df,"I'm trying to learn a bit about differential forms to supplement my study in analysis, but I'm having a hard time with some of the basic manipulations. Anyway, suppose $\Omega$ is an open set in $\mathbb{C}$, and $\varphi:\Omega\to\mathbb{C}$ a smooth map. For a function $f$, I have the definition $\varphi^*f=f\circ\phi$, (when this makes sense for $f$ of course). I also have the definitions  $$ \varphi^*\,dx=\frac{\partial\varphi_1}{\partial x}\,dx+\frac{\partial\varphi_1}{\partial y}\,dy, \qquad \varphi^*dy=\frac{\partial\varphi_2}{\partial x}\,dx+\frac{\partial\varphi_2}{\partial y}\,dy, $$ where $\varphi_1$ is the $x$ component of $\varphi$ and $\varphi_2$ is the $y$ component. For a $1$-form $h=f\,dx+g\,dy$,  $$ \varphi^*h=(\varphi^*f)\varphi^*\,dx+(\varphi^*g)\varphi^*\,dy. $$ What I don't get is why how $d(\varphi^* f)=\varphi^*df$. I thought $d(\varphi^*f)=d(f\circ \phi)=f\,d\varphi+\varphi \,df$. I think my big problem here is I don't understand how to apply $d$ to a composition of functions. I also think $$ \varphi^*(df)=\varphi^*\left(\frac{\partial f}{\partial x}\,dx+\frac{\partial f}{\partial y}\,dy\right) $$ but I don't know how to take this further. They don't look equal to me. How does equality follow here? Thank you.","I'm trying to learn a bit about differential forms to supplement my study in analysis, but I'm having a hard time with some of the basic manipulations. Anyway, suppose $\Omega$ is an open set in $\mathbb{C}$, and $\varphi:\Omega\to\mathbb{C}$ a smooth map. For a function $f$, I have the definition $\varphi^*f=f\circ\phi$, (when this makes sense for $f$ of course). I also have the definitions  $$ \varphi^*\,dx=\frac{\partial\varphi_1}{\partial x}\,dx+\frac{\partial\varphi_1}{\partial y}\,dy, \qquad \varphi^*dy=\frac{\partial\varphi_2}{\partial x}\,dx+\frac{\partial\varphi_2}{\partial y}\,dy, $$ where $\varphi_1$ is the $x$ component of $\varphi$ and $\varphi_2$ is the $y$ component. For a $1$-form $h=f\,dx+g\,dy$,  $$ \varphi^*h=(\varphi^*f)\varphi^*\,dx+(\varphi^*g)\varphi^*\,dy. $$ What I don't get is why how $d(\varphi^* f)=\varphi^*df$. I thought $d(\varphi^*f)=d(f\circ \phi)=f\,d\varphi+\varphi \,df$. I think my big problem here is I don't understand how to apply $d$ to a composition of functions. I also think $$ \varphi^*(df)=\varphi^*\left(\frac{\partial f}{\partial x}\,dx+\frac{\partial f}{\partial y}\,dy\right) $$ but I don't know how to take this further. They don't look equal to me. How does equality follow here? Thank you.",,"['analysis', 'differential-forms']"
80,"Game theory, olympiad question","Game theory, olympiad question",,"I've seen the following question in the brazilian olympiad for university students, and I couldn't solve it. Thor and Loki play the game: Thor chooses an integer $n_1 \ge 1$ , Loki chooses $n_2 \gt n_1$, Thor chooses $n_3 \gt n_2$ and so on. Let $X$ be such that $$X = \bigcup_{j\in\mathbb N^*} \left(\left[n_{2j-1},n_{2j}\right) \cap \mathbb Z \right)$$ and $$s= \sum_{n\in X} \frac {1}{n^2}$$ Thor wins if s is rational, and Loki wins if s is irrational. Determine who has got the winning strategy.","I've seen the following question in the brazilian olympiad for university students, and I couldn't solve it. Thor and Loki play the game: Thor chooses an integer $n_1 \ge 1$ , Loki chooses $n_2 \gt n_1$, Thor chooses $n_3 \gt n_2$ and so on. Let $X$ be such that $$X = \bigcup_{j\in\mathbb N^*} \left(\left[n_{2j-1},n_{2j}\right) \cap \mathbb Z \right)$$ and $$s= \sum_{n\in X} \frac {1}{n^2}$$ Thor wins if s is rational, and Loki wins if s is irrational. Determine who has got the winning strategy.",,"['analysis', 'contest-math', 'game-theory']"
81,Equivalent measures if integral of $C_b$ functions is equal,Equivalent measures if integral of  functions is equal,C_b,"Is it true that if $X$ is a measure space and $\mu, \nu$ are Borel probability measures on $X$ if  $$ \int_X \phi \ d \mu = \int_X \phi \ d \nu \qquad \forall \phi \in C_b(X) \text{ (continuous and bounded functions)} $$ then $$ \mu = \nu  \text{ ?}$$ If $E$ is a measureable set s.t. $1_E \in C_b(X)$ then $\mu(E) = \nu(E)$, but if it's not?","Is it true that if $X$ is a measure space and $\mu, \nu$ are Borel probability measures on $X$ if  $$ \int_X \phi \ d \mu = \int_X \phi \ d \nu \qquad \forall \phi \in C_b(X) \text{ (continuous and bounded functions)} $$ then $$ \mu = \nu  \text{ ?}$$ If $E$ is a measureable set s.t. $1_E \in C_b(X)$ then $\mu(E) = \nu(E)$, but if it's not?",,['analysis']
82,Smooth map with surjective Jacobian is open,Smooth map with surjective Jacobian is open,,"I'd like to show that if $U\subset R^n$ is open, $f:U\to R^m$ is smooth, and $J_f(x)$ is surjective (full rank) for every $x\in U$, then $f(U)$ is open. My thoughts so far: For any $f(x)\in f(U)$, Taylor's theorem provides $\varepsilon>0$ so that whenever $\|z\|<\varepsilon$, $f(x+z)=f(x)+J_f(x)z+O(\|z\|^2)$. Choose $\delta>0$ so that whenever $r\in R^m$ and $\|r\|<\delta$ there is a $z\in R^n$ so that $r=J_f(x)z$, $\|z\|<\varepsilon$ and $x+z\in U$. Choose any $r\in R^m$ so that $\|r\|<\delta$. If we can find $z$ so that $f(x+z)-f(x)=r$, we will have shown that $f(x)$ is an interior point of $f(U)$. From the choice of $\delta$, we can choose $z\in R^n$ so that $\|z\|<\varepsilon$, and $x+z\in U$ and therefore $f(x+z)=f(x)+J_f(x)z+O(\|z\|^2)$ in other words $f(x+z)-f(x)=r+O(\|z\|^2)$. I don't know what to do about the $O(\|z\|^2)$ terms and I'd appreciate some more eyes checking my work so far. I suspect there's something much easier anyway. Internet searches have led me to discussions of manifolds, but I haven't studied them yet. This is my first question, so I apologize for all the expectations I'm breaking.","I'd like to show that if $U\subset R^n$ is open, $f:U\to R^m$ is smooth, and $J_f(x)$ is surjective (full rank) for every $x\in U$, then $f(U)$ is open. My thoughts so far: For any $f(x)\in f(U)$, Taylor's theorem provides $\varepsilon>0$ so that whenever $\|z\|<\varepsilon$, $f(x+z)=f(x)+J_f(x)z+O(\|z\|^2)$. Choose $\delta>0$ so that whenever $r\in R^m$ and $\|r\|<\delta$ there is a $z\in R^n$ so that $r=J_f(x)z$, $\|z\|<\varepsilon$ and $x+z\in U$. Choose any $r\in R^m$ so that $\|r\|<\delta$. If we can find $z$ so that $f(x+z)-f(x)=r$, we will have shown that $f(x)$ is an interior point of $f(U)$. From the choice of $\delta$, we can choose $z\in R^n$ so that $\|z\|<\varepsilon$, and $x+z\in U$ and therefore $f(x+z)=f(x)+J_f(x)z+O(\|z\|^2)$ in other words $f(x+z)-f(x)=r+O(\|z\|^2)$. I don't know what to do about the $O(\|z\|^2)$ terms and I'd appreciate some more eyes checking my work so far. I suspect there's something much easier anyway. Internet searches have led me to discussions of manifolds, but I haven't studied them yet. This is my first question, so I apologize for all the expectations I'm breaking.",,"['analysis', 'differential-geometry']"
83,How to argue this consequence?,How to argue this consequence?,,"Suppose that $\Omega=\mathbf{R}^n_+$ and consider a function $0<u<\sup\limits_\Omega u=M<\infty$ such that: $$\Delta u+u-1=0 \ \ \text{in} \ \ \Omega,$$ $$u=0 \ \ \text{on} \ \ \partial\Omega.$$ If $u$ exists, then $M>1$. I don't know to argue this. My idea is to try by contradiction. Suppose that $M\leq1$, so $$\Delta u=1-u\geq0,$$ that is, $u$ is a subharmonic function. If $u$ attains a maximum in the interior of $\Omega$, for the maximum principle, $u$ should be a constant function and this would be the contradiction. But I don't know how to prove that the maximum is attain in interior.","Suppose that $\Omega=\mathbf{R}^n_+$ and consider a function $0<u<\sup\limits_\Omega u=M<\infty$ such that: $$\Delta u+u-1=0 \ \ \text{in} \ \ \Omega,$$ $$u=0 \ \ \text{on} \ \ \partial\Omega.$$ If $u$ exists, then $M>1$. I don't know to argue this. My idea is to try by contradiction. Suppose that $M\leq1$, so $$\Delta u=1-u\geq0,$$ that is, $u$ is a subharmonic function. If $u$ attains a maximum in the interior of $\Omega$, for the maximum principle, $u$ should be a constant function and this would be the contradiction. But I don't know how to prove that the maximum is attain in interior.",,"['analysis', 'partial-differential-equations', 'harmonic-functions']"
84,Continuous root map of the coefficients of a polynomial,Continuous root map of the coefficients of a polynomial,,"I have a set of polynomials $P_t(z)= z^n+ a_{n-1}(t)z^{n-1}+\cdots+ a_0(t)$ which depends on a real parameter $t \in [a,b]$ and where $a_{n-1}(t),\ldots, a_0(t)$ are real continuous functions. May I say that there exists a continuous map $\theta(t)$ such that $\theta(t)$ is a root of $P_t$ (for all $t$)? I mean, I know that there exists a continuous dependence of the roots of a polynomial with respect to the coefficients and that the Viète map descends to a homeomorphism $w:C^n/S_n\to C^n$, but, can I 'choose' a root? Or I need the axiom of choice to affirm that there exists a map $C^n/S_n\to C^n$? In that case, may I get a such map to be continuous ? Any bibliography reference for all this?","I have a set of polynomials $P_t(z)= z^n+ a_{n-1}(t)z^{n-1}+\cdots+ a_0(t)$ which depends on a real parameter $t \in [a,b]$ and where $a_{n-1}(t),\ldots, a_0(t)$ are real continuous functions. May I say that there exists a continuous map $\theta(t)$ such that $\theta(t)$ is a root of $P_t$ (for all $t$)? I mean, I know that there exists a continuous dependence of the roots of a polynomial with respect to the coefficients and that the Viète map descends to a homeomorphism $w:C^n/S_n\to C^n$, but, can I 'choose' a root? Or I need the axiom of choice to affirm that there exists a map $C^n/S_n\to C^n$? In that case, may I get a such map to be continuous ? Any bibliography reference for all this?",,"['analysis', 'polynomials', 'roots', 'axiom-of-choice']"
85,Carathéodory's method gives a complete measure,Carathéodory's method gives a complete measure,,"I would really like to show that the following is true. ""Suppose that $X$ is a set and $\theta$ is an outer measure on $X$, and let $\mu$ be the measure on $X$ defined by Carathéodory's method. Then if $\theta E = 0$, then $\mu$ measures $E$."" I'm not exactly sure how $E$ is defined, which could be the problem, but the question goes onto ask me to deduce that if $E \subseteq X$ is $\mu$-negligible iff $\theta E = 0$, so I assume that this is the same $E$ as in the statement above. I have been using the following definitions and theorems. Definition (sigma algebra) Let $X$ be a set. A *$\sigma$-algebra of subsets of $X$ is a family $\Sigma$ of subsets of $X$ such that (i) $\emptyset \in \Sigma$; (ii) for every sequence $\left< E_n \right>_{n \in \mathbb{N}}$ in $\Sigma$, its union $\bigcup _{n \in \mathbb{N}} E_n$ belongs to $\Sigma$. Definition (measure space) A measure space is a triple $(X, \Sigma, \mu)$ where (i) $X$ is a set; (ii) $\Sigma$ is a $\sigma$-algebra of subsets of X; (iii) $\mu : \Sigma \rightarrow [0, \infty]$ is a function such that (a) $\mu \emptyset = 0 $; (b) if $\left<E_n \right>_{n \in \mathbb{N}}$ is a disjoint sequence in $\Sigma$, then $\mu \left( \bigcup _{n \in \mathbb{N} } E_n \right) = \sum_{n=1}^{\infty} \mu E_n$. In this context, members of $\Sigma$ are called measurable sets, and $\mu$ is called a measure on $X$ . Definition (outer measure) Let $X$ be a set. An outer measure on $X$ is a function $\theta : \mathcal{P}X \rightarrow \left[ 0 , \infty \right]$ such that (i) $\theta \emptyset = 0$, (ii) if $A \subseteq B \subseteq X$ then $\theta A \leq \theta B$, (iii) for every sequence $\left< A_n \right> _{n \in \mathbb{N}}$ of subsets of $X$, $\theta \left( \bigcup _{n \in \mathbb{N}} A_n \right) \leq \sum_{n=1}^{\infty} \theta A_n$. Carothéodory's Method: Theorem Let X be a set and $\theta$ an outer measure on $X$. Set $$ \Sigma := \left\{ E \subseteq X : \theta A = \theta \left(A \cap E \right) + \theta \left( A \setminus E \right), \forall  A \subseteq X \right\}. $$ Then $\Sigma$ is a $\sigma$ algebra of subsets of $X$. Define $\mu : \Sigma \rightarrow \left[0, \infty \right]$ by writing $\mu E = \theta E$ for $E \in \Sigma$; then $\left( X, \Sigma, \mu \right)$ is a measure space. I think I've followed the proof I have for the above Carothéodory's Method, and I suspect that the proof of the statement in question shall follow it, but proving instead that $\left(A, \Sigma, \mu \right)$ is a measure space. Perhaps the proof is trivial? I just can't see it.","I would really like to show that the following is true. ""Suppose that $X$ is a set and $\theta$ is an outer measure on $X$, and let $\mu$ be the measure on $X$ defined by Carathéodory's method. Then if $\theta E = 0$, then $\mu$ measures $E$."" I'm not exactly sure how $E$ is defined, which could be the problem, but the question goes onto ask me to deduce that if $E \subseteq X$ is $\mu$-negligible iff $\theta E = 0$, so I assume that this is the same $E$ as in the statement above. I have been using the following definitions and theorems. Definition (sigma algebra) Let $X$ be a set. A *$\sigma$-algebra of subsets of $X$ is a family $\Sigma$ of subsets of $X$ such that (i) $\emptyset \in \Sigma$; (ii) for every sequence $\left< E_n \right>_{n \in \mathbb{N}}$ in $\Sigma$, its union $\bigcup _{n \in \mathbb{N}} E_n$ belongs to $\Sigma$. Definition (measure space) A measure space is a triple $(X, \Sigma, \mu)$ where (i) $X$ is a set; (ii) $\Sigma$ is a $\sigma$-algebra of subsets of X; (iii) $\mu : \Sigma \rightarrow [0, \infty]$ is a function such that (a) $\mu \emptyset = 0 $; (b) if $\left<E_n \right>_{n \in \mathbb{N}}$ is a disjoint sequence in $\Sigma$, then $\mu \left( \bigcup _{n \in \mathbb{N} } E_n \right) = \sum_{n=1}^{\infty} \mu E_n$. In this context, members of $\Sigma$ are called measurable sets, and $\mu$ is called a measure on $X$ . Definition (outer measure) Let $X$ be a set. An outer measure on $X$ is a function $\theta : \mathcal{P}X \rightarrow \left[ 0 , \infty \right]$ such that (i) $\theta \emptyset = 0$, (ii) if $A \subseteq B \subseteq X$ then $\theta A \leq \theta B$, (iii) for every sequence $\left< A_n \right> _{n \in \mathbb{N}}$ of subsets of $X$, $\theta \left( \bigcup _{n \in \mathbb{N}} A_n \right) \leq \sum_{n=1}^{\infty} \theta A_n$. Carothéodory's Method: Theorem Let X be a set and $\theta$ an outer measure on $X$. Set $$ \Sigma := \left\{ E \subseteq X : \theta A = \theta \left(A \cap E \right) + \theta \left( A \setminus E \right), \forall  A \subseteq X \right\}. $$ Then $\Sigma$ is a $\sigma$ algebra of subsets of $X$. Define $\mu : \Sigma \rightarrow \left[0, \infty \right]$ by writing $\mu E = \theta E$ for $E \in \Sigma$; then $\left( X, \Sigma, \mu \right)$ is a measure space. I think I've followed the proof I have for the above Carothéodory's Method, and I suspect that the proof of the statement in question shall follow it, but proving instead that $\left(A, \Sigma, \mu \right)$ is a measure space. Perhaps the proof is trivial? I just can't see it.",,"['analysis', 'measure-theory']"
86,Alternate proof for weighted alternating shifted central binomial sum relation,Alternate proof for weighted alternating shifted central binomial sum relation,,"In two recent posts, MSE 2824529 and MSE 2825442 , both initiated by user196574, I answered two asymptotic questions for $n \to \infty$ with the following identity: $$ [1]\,\,\,\,\,\sum_{k=1}^n (-1)^{k+1} \binom{2n}{n+k} k^s = \binom{2n}{n} \sin(\pi s/2) \int_0^\infty \frac{dx \, \,x^s}{\sinh{\pi x}} \frac{n!^2}{(n+ix)!(n-ix)!}. $$ I'm asking for a simpler proof or a generalization in which this formula is a special case. For scoring, Generalization > Simplicity. An example of a generalization might be a Dirichlet character analog. So on the left there is squeezed in some sequence of (+1,-1,0) and on the RHS the hyperbolic trig function might get replaced with something more complicated, like a ratio of a linear combination of hyperbolic trig functions. My proof, as requested from two contributors, will follow. I was also asked for the motivation for such a formula.  It wasn't for asymptotics, but instead to cobble a proof of the functional equation for Riemann's zeta function (at least formally) by using a hypergeometric identity.  Since there are many rigorous proofs for the functional equation (and I don't publish anyway) I'd almost forgotten about it. If someone can generalize the formula as suggested above, it may be worth a revisit. $\textbf{Proof}$ Establish the formula, valid for $0 < Re\,s < 2n$ $$ [2]\,\,\,\,\,\sum_{k=1}^n (-1)^{k+1} \binom{2n}{n+k} k^s = 2^{2n-s}\frac{s!}{\pi}\, \sin(\pi s/2) \int_0^\infty \frac{dt}{t^{s+1}}\, \sin^{2n}t. $$ The next few lines reproduce a well-known trig ID. By the binomial theorem $$\sum_{k=0}^{2n}\binom{2n}{k}(-x)^k = (x-1)^{2n}=\sum_{k=-n}^n\binom{2n}{k+n}(-x)^{k+n} $$ By splitting the sum at $k=-1$ and $k=1$ we have $$\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\big(x^k + x^{-k} \big) = (-1)^n(x-1)^{2n}\,x^{-n} - \binom{2n}{n}.$$ Insert $x=\exp{(2it)}$ to get $$[3]\,\,\,\,\,(2\sin t)^{2n}= 2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\cos(2\, k\, t) + \binom{2n}{n}.$$ Insert Left Hand Side (LHS) of previous equation into integral on RHS of eq [2]. Also insert a regularizer $\exp{(-p\,t)}$ and we'll let $p \to 0$ : $$ \int_0^\infty \frac{(2\sin{t})^{2n}}{t^{s+1}}dt = \lim_{p\to 0}  \int_0^\infty dt \Big(2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\cos(2\, k\, t) + \binom{2n}{n} \Big)\frac{e^{-pt}}{t^{s+1}}.  $$ $$ = \lim_{p\to 0} 2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\Gamma(-s)\cos\big(s \arctan(2k/p)\big)(p^2+(2k)^2)^{s/2} + \binom{2n}{n}\Gamma(-s)p^{s} $$ where the Euler integral for the $\Gamma$ function has been used.  The requirement $s>0$ and the assumption $s$ not an integer will mean the last term $\to$ 0. The last assumption will be relaxed upon analytic continuation. Taking the limit, e.g. $lim_{p\to 0} \arctan(2k/p)=\pi/2$ and using the reflection formula for the $\Gamma$ function allows us to write the result as eq [2].  Analytic continuation permits the $s$ to be extended to the stated range.  The proof of [2] is complete. Now the $t^{-s-1}$ will be represented by the Euler integral to derive a double integral that will eventually give rise to the form in eq. [1]. $$s!\int_0^\infty \sin^{2n}\!t\frac{dt}{t^{s+1}}= \int_0^\infty dt \sin^{2n}t\int_0^\infty\exp{(-xt)}x^s dx = \int_0^\infty dx \, x^s \int_0^\infty dt\, e^{-xt}\sin^{2n}\!t$$ Now we'll show $$[4]\,\,\,\, J := \int_0^\infty dt\, e^{-xt}\big(2\,\sin{t}\big)^{2n} = (2n)! \frac{i}{2} \frac{\Gamma(ix/2)}{(n+ix/2)!}   \frac{\Gamma(1-ix/2)}{(n-ix/2)!}$$ Again insert [3] into the LHS of 4, and again use the Euler integral to find $$J=2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\frac{x}{x^2+(2k)^2} + \binom{2n}{n}\frac{1}{x} $$ $$=\binom{2n}{n}\Big( 2\sum_{k=1}^n\frac{(-n)_k}{(n+1)_k}\frac{x}{x^2+(2k)^2} + \frac{1}{x} \Big) $$ where the expression in the last line uses the Pochhammer symbol. Now the following identity is known, e.g., Table of Series and Products , Hansen 6.6.57: $$ \sum_{k=1}^n \frac{(-n)_k}{(n+1)_k}\frac{1}{k^2-a^2} = \frac{1}{2a^2} \Big(1-\frac{n!^2}{(1+a)_n\,(1-a)_n} \Big).$$ Use of it completes the proof of [4]. Use the $\Gamma$ reflection formula, rescale the integral and do some algebra and the proof of eq. [1] is complete.","In two recent posts, MSE 2824529 and MSE 2825442 , both initiated by user196574, I answered two asymptotic questions for with the following identity: I'm asking for a simpler proof or a generalization in which this formula is a special case. For scoring, Generalization > Simplicity. An example of a generalization might be a Dirichlet character analog. So on the left there is squeezed in some sequence of (+1,-1,0) and on the RHS the hyperbolic trig function might get replaced with something more complicated, like a ratio of a linear combination of hyperbolic trig functions. My proof, as requested from two contributors, will follow. I was also asked for the motivation for such a formula.  It wasn't for asymptotics, but instead to cobble a proof of the functional equation for Riemann's zeta function (at least formally) by using a hypergeometric identity.  Since there are many rigorous proofs for the functional equation (and I don't publish anyway) I'd almost forgotten about it. If someone can generalize the formula as suggested above, it may be worth a revisit. Establish the formula, valid for The next few lines reproduce a well-known trig ID. By the binomial theorem By splitting the sum at and we have Insert to get Insert Left Hand Side (LHS) of previous equation into integral on RHS of eq [2]. Also insert a regularizer and we'll let : where the Euler integral for the function has been used.  The requirement and the assumption not an integer will mean the last term 0. The last assumption will be relaxed upon analytic continuation. Taking the limit, e.g. and using the reflection formula for the function allows us to write the result as eq [2].  Analytic continuation permits the to be extended to the stated range.  The proof of [2] is complete. Now the will be represented by the Euler integral to derive a double integral that will eventually give rise to the form in eq. [1]. Now we'll show Again insert [3] into the LHS of 4, and again use the Euler integral to find where the expression in the last line uses the Pochhammer symbol. Now the following identity is known, e.g., Table of Series and Products , Hansen 6.6.57: Use of it completes the proof of [4]. Use the reflection formula, rescale the integral and do some algebra and the proof of eq. [1] is complete.","n \to \infty  [1]\,\,\,\,\,\sum_{k=1}^n (-1)^{k+1} \binom{2n}{n+k} k^s =
\binom{2n}{n} \sin(\pi s/2) \int_0^\infty \frac{dx \, \,x^s}{\sinh{\pi x}} \frac{n!^2}{(n+ix)!(n-ix)!}.
 \textbf{Proof} 0 < Re\,s < 2n  [2]\,\,\,\,\,\sum_{k=1}^n (-1)^{k+1} \binom{2n}{n+k} k^s =
2^{2n-s}\frac{s!}{\pi}\, \sin(\pi s/2) \int_0^\infty \frac{dt}{t^{s+1}}\, \sin^{2n}t.
 \sum_{k=0}^{2n}\binom{2n}{k}(-x)^k = (x-1)^{2n}=\sum_{k=-n}^n\binom{2n}{k+n}(-x)^{k+n}  k=-1 k=1 \sum_{k=1}^n (-1)^k\binom{2n}{n+k}\big(x^k + x^{-k} \big) = (-1)^n(x-1)^{2n}\,x^{-n} - \binom{2n}{n}. x=\exp{(2it)} [3]\,\,\,\,\,(2\sin t)^{2n}= 2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\cos(2\, k\, t)
+ \binom{2n}{n}. \exp{(-p\,t)} p \to 0  \int_0^\infty \frac{(2\sin{t})^{2n}}{t^{s+1}}dt = \lim_{p\to 0} 
\int_0^\infty dt \Big(2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\cos(2\, k\, t)
+ \binom{2n}{n} \Big)\frac{e^{-pt}}{t^{s+1}}.
  
= \lim_{p\to 0} 2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\Gamma(-s)\cos\big(s \arctan(2k/p)\big)(p^2+(2k)^2)^{s/2}
+ \binom{2n}{n}\Gamma(-s)p^{s}
 \Gamma s>0 s \to lim_{p\to 0} \arctan(2k/p)=\pi/2 \Gamma s t^{-s-1} s!\int_0^\infty \sin^{2n}\!t\frac{dt}{t^{s+1}}= \int_0^\infty dt \sin^{2n}t\int_0^\infty\exp{(-xt)}x^s dx = \int_0^\infty dx \, x^s \int_0^\infty dt\, e^{-xt}\sin^{2n}\!t [4]\,\,\,\, J := \int_0^\infty dt\, e^{-xt}\big(2\,\sin{t}\big)^{2n} = (2n)! \frac{i}{2} \frac{\Gamma(ix/2)}{(n+ix/2)!}
  \frac{\Gamma(1-ix/2)}{(n-ix/2)!} J=2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\frac{x}{x^2+(2k)^2} + \binom{2n}{n}\frac{1}{x}  =\binom{2n}{n}\Big( 2\sum_{k=1}^n\frac{(-n)_k}{(n+1)_k}\frac{x}{x^2+(2k)^2} + \frac{1}{x} \Big)   \sum_{k=1}^n \frac{(-n)_k}{(n+1)_k}\frac{1}{k^2-a^2} = \frac{1}{2a^2}
\Big(1-\frac{n!^2}{(1+a)_n\,(1-a)_n} \Big). \Gamma","['analysis', 'asymptotics', 'closed-form']"
87,What's the difference/connection between PCA and inverse Fourier transform?,What's the difference/connection between PCA and inverse Fourier transform?,,"Principle Component Analysis (PCA) finds the component with the highest contribution, which is very similar to the idea of inverse Fourier transform, which finds the frequency with the highest weight. Could someone help clarify their difference/connection. It seems that they are connected in some mathematical forms.","Principle Component Analysis (PCA) finds the component with the highest contribution, which is very similar to the idea of inverse Fourier transform, which finds the frequency with the highest weight. Could someone help clarify their difference/connection. It seems that they are connected in some mathematical forms.",,"['analysis', 'fourier-analysis', 'signal-processing', 'svd', 'wavelets']"
88,$\max\min$ problem,problem,\max\min,"Suppose there is a real valued function $f(x,y)$ where $x,y$ are vector variables $x\in\mathbb{R}^n$ $y\in\mathbb{R}^m$. Moreover $f$ is strictly/concave in $x$ and strictly/convex in $y$. $f$ is also continuous. The problem is $$\max_x\min_y f(x,y)=\min_y\max_x f(x,y).$$ Please explain to me how to analyse this problem. From When $\min \max = \max \min$? I see that it is not generally true. But I guess the convexity information given above may force equality. Thanks a lot","Suppose there is a real valued function $f(x,y)$ where $x,y$ are vector variables $x\in\mathbb{R}^n$ $y\in\mathbb{R}^m$. Moreover $f$ is strictly/concave in $x$ and strictly/convex in $y$. $f$ is also continuous. The problem is $$\max_x\min_y f(x,y)=\min_y\max_x f(x,y).$$ Please explain to me how to analyse this problem. From When $\min \max = \max \min$? I see that it is not generally true. But I guess the convexity information given above may force equality. Thanks a lot",,"['analysis', 'optimization', 'convex-analysis', 'nonlinear-optimization']"
89,Lebesgue measure in $\mathbb{R}^2$ of uncountable union,Lebesgue measure in  of uncountable union,\mathbb{R}^2,"Suppose we have a collection $(A_r)_{r\in\mathbb{R}}$ of Lebesgue subsets of $\mathbb{R}$, each with Lebesgue measure $0$. Consider the set $$E=\bigcup_{r\in\mathbb{R}}\{r\}\times A_r\subset\mathbb{R}^2.$$ Is $E$ necessarily a Lebesgue subset of $\mathbb{R}^2$ and, if so, does it have measure $0$? I was wondering this today, but didn't really get anywhere. Perhaps a counterexample could be constructed by considering an $\mathbb{R}^2$ analogue of the fat Cantor set? Thank you.","Suppose we have a collection $(A_r)_{r\in\mathbb{R}}$ of Lebesgue subsets of $\mathbb{R}$, each with Lebesgue measure $0$. Consider the set $$E=\bigcup_{r\in\mathbb{R}}\{r\}\times A_r\subset\mathbb{R}^2.$$ Is $E$ necessarily a Lebesgue subset of $\mathbb{R}^2$ and, if so, does it have measure $0$? I was wondering this today, but didn't really get anywhere. Perhaps a counterexample could be constructed by considering an $\mathbb{R}^2$ analogue of the fat Cantor set? Thank you.",,"['analysis', 'measure-theory']"
90,singular shock solutions for strictly hyperbolic system of conservation law,singular shock solutions for strictly hyperbolic system of conservation law,,"Shallow water equation \begin{eqnarray} \rho_t+q_x=0\\ q_t + \left(q^2 +\frac{\rho ^2}{2} \right)_x=0 \end{eqnarray} being a strictly hyperbolic equation does not admit delta shock solutions. Where as some strictly hyperbolic systems such as Chaplygin gas equation given by \begin{eqnarray} \rho_t +q_x=0\\ q_t+\left( \frac{q^2-1}{\rho} \right)_x=0 \end{eqnarray} admits delta shock solution. Why is this so? What is the difference between these two equations?. Why the procedure to get solution of the Riemann problem as a combination of shock and rarefaction for strictly hyperbolic equation as explained in  ""Numeric approximations of hyperbolic systems of conservation laws"" by Godlewsky and Raviart, does not work for this equation?","Shallow water equation being a strictly hyperbolic equation does not admit delta shock solutions. Where as some strictly hyperbolic systems such as Chaplygin gas equation given by admits delta shock solution. Why is this so? What is the difference between these two equations?. Why the procedure to get solution of the Riemann problem as a combination of shock and rarefaction for strictly hyperbolic equation as explained in  ""Numeric approximations of hyperbolic systems of conservation laws"" by Godlewsky and Raviart, does not work for this equation?","\begin{eqnarray}
\rho_t+q_x=0\\
q_t + \left(q^2 +\frac{\rho ^2}{2} \right)_x=0
\end{eqnarray} \begin{eqnarray}
\rho_t +q_x=0\\
q_t+\left( \frac{q^2-1}{\rho} \right)_x=0
\end{eqnarray}","['analysis', 'partial-differential-equations', 'hyperbolic-equations']"
91,"If $F:S^3 \rightarrow \mathbb{R}^3$ is a nonvanishing smooth function then exists a ""smooth basis""","If  is a nonvanishing smooth function then exists a ""smooth basis""",F:S^3 \rightarrow \mathbb{R}^3,"I think this is easy, but I'm stuck on this problem. I am trying (and failing miserably) to demonstrate the following theorem Theorem: Let $F: S^3 \rightarrow \mathbb{R}^3$ be a nonvanishing smooth   function. Then  exists smooth functions $G,  H: S^3 \rightarrow \mathbb{R}^3$ , such that $\{ F(p), G(p), H(p) \}$ is a basis of $\mathbb{R}^3$ for every $p$ $\in$ $S^3$ . If $F: S^3 \rightarrow \mathbb{R}^2$ $(F(p) = (F_1(p) , F_2(p) )$ the demonstration would be trivial, we would define $G(p) = (-F_2(p) , F_1(p))$ and and the proof would be ready. For any reason, I can't do this kind of trick in $\mathbb{R}^3$ (I'm just able to do this locally and not globally). Does anyone know some magic trick that demonstrates the existence of the functions $H$ and $G$ ? EDIT: Couple days ago I figured out the solution Solution Using that $S^3$ is a Lie group ( $S^3$ is diffeomorphic to $SU (2) $ ), there is a trivialization of $TS^3$ , i.e. there is a smooth function $$H: S^3 \times \mathbb {R}^3\rightarrow TS^3$$ $$(p,v) \mapsto (p,A_p v),$$ where $A_p$ is a isomorphism of $\mathbb {R}^3$ to $T_p S^3$ . So we can define the following vector field in $S^3$ , $\tilde{F}: S^3 \rightarrow TS^3$ , $\tilde {F}(p) = (p,A_p F (p))$ . Let $F_t (p)$ be the flow of $\tilde {F} $ , i. e; $F_0 (p) = p$ , for all $p$ $\in $ $S^3$ . $\frac {d}{dt} F_t (p) = A_{F_t (p)} F (F_t (p))$ , for all $(t,p)$ $\in$ $\mathbb {R} \times S^3$ . (Since $S^3$ is compact $\tilde {F} $ has complete flow). Define $T (p,t) = \frac {F (F_t (p))}{\|F (F_t (p)) \|}$ and $G (p) = \left.\frac {d}{dt}T (p,t)\right|_{t=0}$ Finally, $G $ is the required function because \begin{align*} \left\langle G (p), \frac {F (p)}{\|F (p)\|}\right\rangle &= \left\langle \left.\frac {d}{dt}T (p,t)\right|_{t=0} , T (p,0)\right\rangle\\ &=\frac {1}{2}\left.\frac {d}{dt} \left\langle T (p,t), T (p,t)\right\rangle\right|_{t=0}\\ &=\frac {1}{2}\frac {d}{dt} 1\\ &=0. \end{align*} $H $ can be taken as the cross product between $F $ and $G $ . Bonus Information: If $F$ is just a continuous function we can find a sequence of smooth functions $\{F_n\}_{n\in\mathbb{N}}$ ( $F_n: S^3 \rightarrow \mathbb{R}^3$ ), such that $F_n \rightarrow \frac{F}{\|F\|}$ uniformly. Note that if $F_n \rightarrow \frac{F}{\|F\|}$ uniformly, then $\frac{F_n}{\|F_n\|} \rightarrow \frac{F}{\|F\|}$ uniformly. For each $n$ $\in$ $\mathbb{N}$ , there is (by the demonstration of the theorem above) a smooth function $G_n:S^3 \rightarrow \mathbb{R}^3$ , satisfying $\left\langle G_n, F_n \right\rangle = 0 $ . Let $n_0$ be a natural number such that $\left\|\frac{F}{\|F\|} - \frac{F_{n_0}}{\|F_{n_0}\|} \right\|<\frac{1}{2}$ . Realize that \begin{align*} \left|\left\langle \frac{G_{n_0}(p)}{\|G_{n_0}(p)\|} , \frac{F(p)}{\|F(p)\|}\right\rangle\right|&=  \left|\left\langle \frac{G_{n_0}(p)}{\|G_{n_0}(p)\|} , \frac{F(p)}{\|F(p)\|} - \frac{F_{n_0}(p)}{\|F_{n_0}(p)\|}\right\rangle \right|\\ &\leq \left\|\frac{G_{n_0}(p)}{\|G_{n_0}(p)\|}\right\|\cdot \left\| \frac{F(p)}{\|F(p)\|} - \frac{F_{n_0}(p)}{\|F_{n_0}(p)\|}\right\|\\ &\leq \frac{1}{2} \end{align*} Then $G_{n_0}(p)$ is l.i with $F(p)$ for every $p$ $\in$ $S^{3}$ (otherwise we would have the existence of $p$ satisfying $\left|\left\langle \frac{G_{n_0}(p)}{\|G_{n_0}(p)\|} , \frac{F(p)}{\|F(p)\|}\right\rangle\right|=1$ ). One more time we can take $H$ as the cross product between $G_{n_0}$ and $F$ .","I think this is easy, but I'm stuck on this problem. I am trying (and failing miserably) to demonstrate the following theorem Theorem: Let be a nonvanishing smooth   function. Then  exists smooth functions , such that is a basis of for every . If the demonstration would be trivial, we would define and and the proof would be ready. For any reason, I can't do this kind of trick in (I'm just able to do this locally and not globally). Does anyone know some magic trick that demonstrates the existence of the functions and ? EDIT: Couple days ago I figured out the solution Solution Using that is a Lie group ( is diffeomorphic to ), there is a trivialization of , i.e. there is a smooth function where is a isomorphism of to . So we can define the following vector field in , , . Let be the flow of , i. e; , for all . , for all . (Since is compact has complete flow). Define and Finally, is the required function because can be taken as the cross product between and . Bonus Information: If is just a continuous function we can find a sequence of smooth functions ( ), such that uniformly. Note that if uniformly, then uniformly. For each , there is (by the demonstration of the theorem above) a smooth function , satisfying . Let be a natural number such that . Realize that Then is l.i with for every (otherwise we would have the existence of satisfying ). One more time we can take as the cross product between and .","F: S^3 \rightarrow \mathbb{R}^3 G,
 H: S^3 \rightarrow \mathbb{R}^3 \{ F(p), G(p), H(p) \} \mathbb{R}^3 p \in S^3 F: S^3 \rightarrow \mathbb{R}^2 (F(p) = (F_1(p) , F_2(p) ) G(p) = (-F_2(p) , F_1(p)) \mathbb{R}^3 H G S^3 S^3 SU (2)  TS^3 H: S^3 \times \mathbb {R}^3\rightarrow TS^3 (p,v) \mapsto (p,A_p v), A_p \mathbb {R}^3 T_p S^3 S^3 \tilde{F}: S^3 \rightarrow TS^3 \tilde {F}(p) = (p,A_p F (p)) F_t (p) \tilde {F}  F_0 (p) = p p \in  S^3 \frac {d}{dt} F_t (p) = A_{F_t (p)} F (F_t (p)) (t,p) \in \mathbb {R} \times S^3 S^3 \tilde {F}  T (p,t) = \frac {F (F_t (p))}{\|F (F_t (p)) \|} G (p) = \left.\frac {d}{dt}T (p,t)\right|_{t=0} G  \begin{align*}
\left\langle G (p), \frac {F (p)}{\|F (p)\|}\right\rangle &= \left\langle \left.\frac {d}{dt}T (p,t)\right|_{t=0} , T (p,0)\right\rangle\\
&=\frac {1}{2}\left.\frac {d}{dt} \left\langle T (p,t), T (p,t)\right\rangle\right|_{t=0}\\
&=\frac {1}{2}\frac {d}{dt} 1\\
&=0.
\end{align*} H  F  G  F \{F_n\}_{n\in\mathbb{N}} F_n: S^3 \rightarrow \mathbb{R}^3 F_n \rightarrow \frac{F}{\|F\|} F_n \rightarrow \frac{F}{\|F\|} \frac{F_n}{\|F_n\|} \rightarrow \frac{F}{\|F\|} n \in \mathbb{N} G_n:S^3 \rightarrow \mathbb{R}^3 \left\langle G_n, F_n \right\rangle = 0  n_0 \left\|\frac{F}{\|F\|} - \frac{F_{n_0}}{\|F_{n_0}\|} \right\|<\frac{1}{2} \begin{align*}
\left|\left\langle \frac{G_{n_0}(p)}{\|G_{n_0}(p)\|} , \frac{F(p)}{\|F(p)\|}\right\rangle\right|&=  \left|\left\langle \frac{G_{n_0}(p)}{\|G_{n_0}(p)\|} , \frac{F(p)}{\|F(p)\|} - \frac{F_{n_0}(p)}{\|F_{n_0}(p)\|}\right\rangle \right|\\
&\leq \left\|\frac{G_{n_0}(p)}{\|G_{n_0}(p)\|}\right\|\cdot \left\| \frac{F(p)}{\|F(p)\|} - \frac{F_{n_0}(p)}{\|F_{n_0}(p)\|}\right\|\\
&\leq \frac{1}{2}
\end{align*} G_{n_0}(p) F(p) p \in S^{3} p \left|\left\langle \frac{G_{n_0}(p)}{\|G_{n_0}(p)\|} , \frac{F(p)}{\|F(p)\|}\right\rangle\right|=1 H G_{n_0} F","['analysis', 'differential-topology']"
92,Exponential inequality(Proof) [duplicate],Exponential inequality(Proof) [duplicate],,"This question already has answers here : Proving the exponential inequality: $x^y+y^x\gt1$ (3 answers) Closed 7 years ago . Given $\forall x,y>0$ Prove that $x^y+y^x\ge1$ I have tried weighted inequalities and Jensen's but unfortunately ended up no where. Please help me. (I know this is a basic inequality).","This question already has answers here : Proving the exponential inequality: $x^y+y^x\gt1$ (3 answers) Closed 7 years ago . Given $\forall x,y>0$ Prove that $x^y+y^x\ge1$ I have tried weighted inequalities and Jensen's but unfortunately ended up no where. Please help me. (I know this is a basic inequality).",,"['analysis', 'inequality', 'exponential-function']"
93,Wick Rotation technique,Wick Rotation technique,,"I am trying to get my head around the Wick rotation technique. I have tried to play around with some elementary examples. Let us imagine I need to solve on the real line $$ y’ = \cos (x)$$ the prime denoting differentiation. I can view the function on the r.h.s as the real part of a complex function and re-formulate the differential equation, now on the imaginary line ( I suppose this is the ""rotation""), as $$ y’ = e^{ix} $$  Whose solution is $$y = \frac{1}{i} e^{ix}$$ Now to get my solution on the real line I have to re-rotate back, by multiplying by $$-i$$, and then take the imaginary part of the ensuing expression to get my solution $ y = - \sin (x)$. Is this a correct,formally rigorous, application of the Wick rotation technique? I would also be the most grateful if somebody could show me an elementary example of analytical continuation by the Wick rotation. Thanks","I am trying to get my head around the Wick rotation technique. I have tried to play around with some elementary examples. Let us imagine I need to solve on the real line $$ y’ = \cos (x)$$ the prime denoting differentiation. I can view the function on the r.h.s as the real part of a complex function and re-formulate the differential equation, now on the imaginary line ( I suppose this is the ""rotation""), as $$ y’ = e^{ix} $$  Whose solution is $$y = \frac{1}{i} e^{ix}$$ Now to get my solution on the real line I have to re-rotate back, by multiplying by $$-i$$, and then take the imaginary part of the ensuing expression to get my solution $ y = - \sin (x)$. Is this a correct,formally rigorous, application of the Wick rotation technique? I would also be the most grateful if somebody could show me an elementary example of analytical continuation by the Wick rotation. Thanks",,['analysis']
94,Composition of functions that are onto or one-to-one,Composition of functions that are onto or one-to-one,,"I found part of my answer here: If g(f(x)) is one-to-one (injective) show f(x) is also one-to-one (given that...) ; however I wanted to flesh out the last two statements I had in a proposition in my notes. Proposition: Let $f: A \rightarrow B$ and $g: B \rightarrow C$.  Then: (i) If $g \circ f$ is one-to-one, then $f$ is one-to-one. (ii) If $g \circ f$ is onto, then $g$ is onto. Proof: (i)  Suppose $f(x)=f(y)$ for some $x,y$. Since $g \circ f$ is one-to-one: $$g\circ f(x) = g\circ f(y) \Rightarrow x=y,\forall x,y \in A.$$ Therefore $f$ must be one-to-one. (ii)  Since $g \circ f (x)$ is onto, then for every $c \in C$ there exists an $a \in A$ such that $c=g(f(a))$.  Then there exists a $b \in B$ with $b=f(a)$ such that $g(b)=c$.  Thus g is onto. I wanted to confirm that these proofs are both correct for my peace of mind (as they weren't proven in class).","I found part of my answer here: If g(f(x)) is one-to-one (injective) show f(x) is also one-to-one (given that...) ; however I wanted to flesh out the last two statements I had in a proposition in my notes. Proposition: Let $f: A \rightarrow B$ and $g: B \rightarrow C$.  Then: (i) If $g \circ f$ is one-to-one, then $f$ is one-to-one. (ii) If $g \circ f$ is onto, then $g$ is onto. Proof: (i)  Suppose $f(x)=f(y)$ for some $x,y$. Since $g \circ f$ is one-to-one: $$g\circ f(x) = g\circ f(y) \Rightarrow x=y,\forall x,y \in A.$$ Therefore $f$ must be one-to-one. (ii)  Since $g \circ f (x)$ is onto, then for every $c \in C$ there exists an $a \in A$ such that $c=g(f(a))$.  Then there exists a $b \in B$ with $b=f(a)$ such that $g(b)=c$.  Thus g is onto. I wanted to confirm that these proofs are both correct for my peace of mind (as they weren't proven in class).",,['analysis']
95,How can I describe the vertical component of a juggling ball's path with a sine wave?,How can I describe the vertical component of a juggling ball's path with a sine wave?,,"I juggle, and then track the juggling balls. I want to describe this juggling trick using sine waves. A metronome was used to keep the throws periodic. The video is 120fps, so there are 120 observations per second. The Y-values correspond to the location of the ball in the image. The video is 800 pixles tall, so the Y-values range from about 200 to 600. This is a graph of ( the data ): Using this Python/OpenCV script , was able to manually fit a sine wave to the data. The thick blue line is the source data. The thick green line is the manually fitted sine wave, which is composed of the two thinnest sine curves: From manually fitting the sine wave, I know that this function is the sum of two sine waves. The period of the longer wave is 2x the period of the shorter wave. A FFT seems to confirm this: In conclusion, I can describe this data by manually (visually) fitting a sine curve. I would like to use a mathematical and statistical methods to fit a sine curve to this data. The data that I used in this example was pretty simple, but the juggling tricks can get more complicated:","I juggle, and then track the juggling balls. I want to describe this juggling trick using sine waves. A metronome was used to keep the throws periodic. The video is 120fps, so there are 120 observations per second. The Y-values correspond to the location of the ball in the image. The video is 800 pixles tall, so the Y-values range from about 200 to 600. This is a graph of ( the data ): Using this Python/OpenCV script , was able to manually fit a sine wave to the data. The thick blue line is the source data. The thick green line is the manually fitted sine wave, which is composed of the two thinnest sine curves: From manually fitting the sine wave, I know that this function is the sum of two sine waves. The period of the longer wave is 2x the period of the shorter wave. A FFT seems to confirm this: In conclusion, I can describe this data by manually (visually) fitting a sine curve. I would like to use a mathematical and statistical methods to fit a sine curve to this data. The data that I used in this example was pretty simple, but the juggling tricks can get more complicated:",,"['analysis', 'recreational-mathematics', 'signal-processing']"
96,Uniformly Lipschitz and bounded variation,Uniformly Lipschitz and bounded variation,,"I found an interesting question in a textbook (Apostol analysis chapter 6 problem… 2?) and was wondering if anyone could help me figure this out, or at least help adjust my thinking. A function $f$ is called uniformly Lipschitz of order $\alpha>0$ on $[0,1]$ if $|f(x)-f(y)|<M|x-y|^\alpha$ for all $0\le x,y\le1$ and for some fixed $M$ . (a) Prove that $\alpha>1$ implies $f$ is constant, where $\alpha=1$ implies $f$ is of bounded variation. (b) Find an $f$ that is uniformly Lipschitz on $[0,1]$ for some $\alpha<1$ but not of bounded variation. (c) Find an $f$ of bounded variation on $[0,1]$ that satisfies no uniformly Lipschitz condition. My progress: (a) I solved, if $\alpha>1$ then $\left|\frac{f(x+h)-f(x)}h\right|<M|h|^{\alpha-1}\to0$ as $h\to0$ , so $f'=0$ . For $\alpha=1$ , replace the RHS with $M$ , so $f'$ is bounded, so it is of bounded variation. (b) I am not sure. The only functions not of bounded variation I can think of are things involving $\sin(1/x)$ , which I have no conceivable way to prove are uniformly Lipschitz. In particular I guess that $x\sin(1/x)$ works but have no idea how to prove that it's Lipschitz (if it's even). Maybe $\sqrt{x}\sin(1/x)$ would be better because then there is a conceivable way to connect that to $\alpha=1/2<1$ , plus we proved that it's not Lipschitz in a previous exercise. (c) I haven't been able to find anything. If it satisfies no Lipschitz condition, then I am pretty sure that probably $f'$ needs to be unbounded at some point. I'm pretty sure the way to approach this problem is by finding some monotonic function that satisfies no Lipschitz condition since that would imply it is of bounded variation. It can't be something like $\sqrt{x}$ (chosen because of the vertical tangent at $x=0$ ) since that would probably satisfy like $\alpha=1/2$ . There are only a few classical continuous functions with vertical asymptotes, like $x^a$ ( $0<a<1$ ) or $\sqrt{1-x^2}$ , where the second is basically symmetric with the first one. I also tried to construct it with the following reasoning: translate so that $f(0)=0$ , then make $\left|\frac{f(x)}{x^\alpha}\right|$ unbounded for all $\alpha>0$ . The smallest function that comes to mind that dominates all polynomial growth is $e^x$ , which would give the construction of $f(x)=e^{1/x}$ . But if $f(0^+)=+\infty$ , then there will be no way to define $f(0)$ so that $f$ is strictly monotonic. Also there is a property that all functions of bounded variation need to be bounded too. So, this probably won't work. Anyways thank you for the help.","I found an interesting question in a textbook (Apostol analysis chapter 6 problem… 2?) and was wondering if anyone could help me figure this out, or at least help adjust my thinking. A function is called uniformly Lipschitz of order on if for all and for some fixed . (a) Prove that implies is constant, where implies is of bounded variation. (b) Find an that is uniformly Lipschitz on for some but not of bounded variation. (c) Find an of bounded variation on that satisfies no uniformly Lipschitz condition. My progress: (a) I solved, if then as , so . For , replace the RHS with , so is bounded, so it is of bounded variation. (b) I am not sure. The only functions not of bounded variation I can think of are things involving , which I have no conceivable way to prove are uniformly Lipschitz. In particular I guess that works but have no idea how to prove that it's Lipschitz (if it's even). Maybe would be better because then there is a conceivable way to connect that to , plus we proved that it's not Lipschitz in a previous exercise. (c) I haven't been able to find anything. If it satisfies no Lipschitz condition, then I am pretty sure that probably needs to be unbounded at some point. I'm pretty sure the way to approach this problem is by finding some monotonic function that satisfies no Lipschitz condition since that would imply it is of bounded variation. It can't be something like (chosen because of the vertical tangent at ) since that would probably satisfy like . There are only a few classical continuous functions with vertical asymptotes, like ( ) or , where the second is basically symmetric with the first one. I also tried to construct it with the following reasoning: translate so that , then make unbounded for all . The smallest function that comes to mind that dominates all polynomial growth is , which would give the construction of . But if , then there will be no way to define so that is strictly monotonic. Also there is a property that all functions of bounded variation need to be bounded too. So, this probably won't work. Anyways thank you for the help.","f \alpha>0 [0,1] |f(x)-f(y)|<M|x-y|^\alpha 0\le x,y\le1 M \alpha>1 f \alpha=1 f f [0,1] \alpha<1 f [0,1] \alpha>1 \left|\frac{f(x+h)-f(x)}h\right|<M|h|^{\alpha-1}\to0 h\to0 f'=0 \alpha=1 M f' \sin(1/x) x\sin(1/x) \sqrt{x}\sin(1/x) \alpha=1/2<1 f' \sqrt{x} x=0 \alpha=1/2 x^a 0<a<1 \sqrt{1-x^2} f(0)=0 \left|\frac{f(x)}{x^\alpha}\right| \alpha>0 e^x f(x)=e^{1/x} f(0^+)=+\infty f(0) f","['analysis', 'lipschitz-functions', 'bounded-variation']"
97,When does interchangibility of limit and Riemann integral imply uniform convergence?,When does interchangibility of limit and Riemann integral imply uniform convergence?,,"Let  $\{f_n\}$ be a sequence of real-valued functions defined on an interval $[a,b]$ such that each $f_n$ is Riemann integrable, $\{f_n\}$ converges point-wise to $f$, $f$ is Riemann integrable and $\lim_{n \to \infty} \int_a^b f_n$ exists and equals $\int_a^b f$; then under what additional condition(s) can we conclude that $\{f_n\} $ converges uniformly to $f$?","Let  $\{f_n\}$ be a sequence of real-valued functions defined on an interval $[a,b]$ such that each $f_n$ is Riemann integrable, $\{f_n\}$ converges point-wise to $f$, $f$ is Riemann integrable and $\lim_{n \to \infty} \int_a^b f_n$ exists and equals $\int_a^b f$; then under what additional condition(s) can we conclude that $\{f_n\} $ converges uniformly to $f$?",,"['analysis', 'convergence-divergence']"
98,Abstract definition of a differential operator,Abstract definition of a differential operator,,"In Natural Operations in Differential Geometry by Kolar, Michor, and Slovak, a differential operator is said to be a rule transforming sections of a fibred manifold $Y \to M$ into sections of another fibred manifold $Y' \to M'$. Is this is a precise definition, or just a hand-wavy characterization? I would have expected that a differential operator would be required to be a sheaf homomorphism between the sheaves of smooth sections of $Y \to M$ and $Y' \to M'$. Is there any reason why KMS do not require a differential operator to be a sheaf homomorphism, but (apparently) just a function between the sets of global sections?","In Natural Operations in Differential Geometry by Kolar, Michor, and Slovak, a differential operator is said to be a rule transforming sections of a fibred manifold $Y \to M$ into sections of another fibred manifold $Y' \to M'$. Is this is a precise definition, or just a hand-wavy characterization? I would have expected that a differential operator would be required to be a sheaf homomorphism between the sheaves of smooth sections of $Y \to M$ and $Y' \to M'$. Is there any reason why KMS do not require a differential operator to be a sheaf homomorphism, but (apparently) just a function between the sets of global sections?",,"['analysis', 'differential-geometry', 'manifolds', 'sheaf-theory', 'differential-operators']"
99,Image of a set of zero measure has zero measure,Image of a set of zero measure has zero measure,,"I am studying for my final and got stuck on the following problem from the previous year. I put my attempt below. Suppose that $I\subset \mathbb{R}$ is an open interval, $f:I\rightarrow \mathbb{R}$ is differentiable on $I$ and its derivative is continuous on $I$. If $a,b\in I$ and $E\subseteq [a,b]$ is of Lebesgue measure zero, show that $f(E)$ is a set of Lebesgue measure zero. I think that since $E$ is a set of measure zero for every $\varepsilon>0$, there is a countable covering of $E$ with disjoint open intervals $E\subset \cup_{i\geq 1} U_i$ where $|U_i|=d_i$, so that $\sum_{i=1}m^\ast(U_i)=\sum_{i=1} d_i<\varepsilon$, then:  $$ m^\ast(f(E))\leq \sum_{i=1} m^\ast(f(U_i))\leq \sum_{i=1}|(f(u_{i1}),f(u_{i2}))|=\sum_{i=1}|f(u_{i1})-f(u_{i2})|\leq \sup_{a\leq t\leq b}|f'(t)| \varepsilon, $$ so $m^\ast(f(E))$ has measure zero as $f'$ is bounded on $[a,b]$. Howerver, I am not sure why $\sum_{i=1} m^\ast(f(U_i))\leq \sum_{i=1}|(f(u_{i1}),f(u_{i2}))|$. I am also trying to see why this would imply that for a set $E\subseteq I$ with measure zero, $f(E)$ is a set of measure zero.","I am studying for my final and got stuck on the following problem from the previous year. I put my attempt below. Suppose that $I\subset \mathbb{R}$ is an open interval, $f:I\rightarrow \mathbb{R}$ is differentiable on $I$ and its derivative is continuous on $I$. If $a,b\in I$ and $E\subseteq [a,b]$ is of Lebesgue measure zero, show that $f(E)$ is a set of Lebesgue measure zero. I think that since $E$ is a set of measure zero for every $\varepsilon>0$, there is a countable covering of $E$ with disjoint open intervals $E\subset \cup_{i\geq 1} U_i$ where $|U_i|=d_i$, so that $\sum_{i=1}m^\ast(U_i)=\sum_{i=1} d_i<\varepsilon$, then:  $$ m^\ast(f(E))\leq \sum_{i=1} m^\ast(f(U_i))\leq \sum_{i=1}|(f(u_{i1}),f(u_{i2}))|=\sum_{i=1}|f(u_{i1})-f(u_{i2})|\leq \sup_{a\leq t\leq b}|f'(t)| \varepsilon, $$ so $m^\ast(f(E))$ has measure zero as $f'$ is bounded on $[a,b]$. Howerver, I am not sure why $\sum_{i=1} m^\ast(f(U_i))\leq \sum_{i=1}|(f(u_{i1}),f(u_{i2}))|$. I am also trying to see why this would imply that for a set $E\subseteq I$ with measure zero, $f(E)$ is a set of measure zero.",,"['analysis', 'measure-theory']"
