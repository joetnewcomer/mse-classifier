,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What changes for linear algebra over a finite field?,What changes for linear algebra over a finite field?,,"This question asks which standard results from linear algebra over a field no longer hold when we generalize the algebraic structure of the scalars to be an arbitrary division ring. My question is similar but considers a less drastic generalization. In elementary courses on linear algebra, the underlying field is virtually always assumed to be either the real or the complex numbers. (Maybe once in a blue moon, the rationals.) As such, all my intuition is for infinite fields. Moreover, I know that fields of characteristic 2 are especially problematic. Which theorems from linear algebra no longer hold when we go from an infinite field to a finite field of characteristic greater than 2? Which further theorems break down (nontrivially) when we go from characteristic greater than 2 to characteristic 2?","This question asks which standard results from linear algebra over a field no longer hold when we generalize the algebraic structure of the scalars to be an arbitrary division ring. My question is similar but considers a less drastic generalization. In elementary courses on linear algebra, the underlying field is virtually always assumed to be either the real or the complex numbers. (Maybe once in a blue moon, the rationals.) As such, all my intuition is for infinite fields. Moreover, I know that fields of characteristic 2 are especially problematic. Which theorems from linear algebra no longer hold when we go from an infinite field to a finite field of characteristic greater than 2? Which further theorems break down (nontrivially) when we go from characteristic greater than 2 to characteristic 2?",,"['linear-algebra', 'finite-fields', 'intuition']"
1,Construct matrix given eigenvalues and eigenvectors,Construct matrix given eigenvalues and eigenvectors,,"Given eigenvectors $v_1, v_2, \dots, v_n$ and eigenvalues $\lambda_1,\lambda_2,\dots,\lambda_n$, how do I construct a matrix whose eigenvectors and eigenvalues are $v$ and $\lambda$? The straightforward way of doing this is to encapsulate all $n^2$ constraints into a linear system and solve for each element of the matrix $M_i$. I.e., $$ \begin{bmatrix} v_{11} & v_{12} & \dots & v_{1n} & 0 & 0 &\dots & 0 & \dots & 0 & 0 &\dots & 0 \\ 0 & 0 &\dots & 0 & v_{11} & v_{12} & \dots & v_{1n} & \dots & 0 & 0 &\dots & 0\\  & & & & &  & \vdots \\ 0 & 0 &\dots & 0 & 0 & 0 &\dots & 0 & \dots & v_{n1} & v_{n2} & \dots & v_{nn}\\ \end{bmatrix}  \begin{bmatrix} M_1 \\ M_2 \\ \vdots \\ M_{n^2} \end{bmatrix}  =   \begin{bmatrix} \lambda_1 v_{11} \\ \lambda_1 v_{12} \\ \vdots \\\lambda_n v_{nn} \end{bmatrix} $$ Is there a more elegant way?","Given eigenvectors $v_1, v_2, \dots, v_n$ and eigenvalues $\lambda_1,\lambda_2,\dots,\lambda_n$, how do I construct a matrix whose eigenvectors and eigenvalues are $v$ and $\lambda$? The straightforward way of doing this is to encapsulate all $n^2$ constraints into a linear system and solve for each element of the matrix $M_i$. I.e., $$ \begin{bmatrix} v_{11} & v_{12} & \dots & v_{1n} & 0 & 0 &\dots & 0 & \dots & 0 & 0 &\dots & 0 \\ 0 & 0 &\dots & 0 & v_{11} & v_{12} & \dots & v_{1n} & \dots & 0 & 0 &\dots & 0\\  & & & & &  & \vdots \\ 0 & 0 &\dots & 0 & 0 & 0 &\dots & 0 & \dots & v_{n1} & v_{n2} & \dots & v_{nn}\\ \end{bmatrix}  \begin{bmatrix} M_1 \\ M_2 \\ \vdots \\ M_{n^2} \end{bmatrix}  =   \begin{bmatrix} \lambda_1 v_{11} \\ \lambda_1 v_{12} \\ \vdots \\\lambda_n v_{nn} \end{bmatrix} $$ Is there a more elegant way?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
2,Deriving the normal distance from the origin to the decision surface,Deriving the normal distance from the origin to the decision surface,,"While studying discriminant functions for linear classification, I encountered the following: .. if $\textbf{x}$ is a point on the decision surface, then $y(\textbf{x}) = 0$, and so the normal distance from the origin to the decision surface is given by: $$ \frac{\textbf{w}^T \textbf{x}}{\lvert\lvert \textbf{w} \lvert\lvert} = -\frac{w_0}{\lvert\lvert \textbf{w} \lvert\lvert} \tag 1 $$ Where $\textbf{w}$ is a weight vector, and $w_0$ is a bias. In an attempt to derive the above formula I tried the following: \begin{align*} & \textbf{w}^T \textbf{x} + w_0 = 0 \tag 2\\ & \textbf{w}^T \textbf{x} = -w_0 \tag 3 \end{align*} After which I am basically stuck. I think that the author gets about from equation $(3)$ to equation $(1)$ by normalising. But isn't calculating the normal (perpendicular) distance quite separate from normalising a vector? Secondly, how does equation $(1)$ translate into the normal distance being $ - \frac{w_0}{\lvert\lvert \textbf{w} \lvert\lvert}$ i.e. How is the quantity $\frac{\textbf{w}^T \textbf{x}}{\lvert\lvert \textbf{w} \lvert\lvert}$ the normal distance ?","While studying discriminant functions for linear classification, I encountered the following: .. if $\textbf{x}$ is a point on the decision surface, then $y(\textbf{x}) = 0$, and so the normal distance from the origin to the decision surface is given by: $$ \frac{\textbf{w}^T \textbf{x}}{\lvert\lvert \textbf{w} \lvert\lvert} = -\frac{w_0}{\lvert\lvert \textbf{w} \lvert\lvert} \tag 1 $$ Where $\textbf{w}$ is a weight vector, and $w_0$ is a bias. In an attempt to derive the above formula I tried the following: \begin{align*} & \textbf{w}^T \textbf{x} + w_0 = 0 \tag 2\\ & \textbf{w}^T \textbf{x} = -w_0 \tag 3 \end{align*} After which I am basically stuck. I think that the author gets about from equation $(3)$ to equation $(1)$ by normalising. But isn't calculating the normal (perpendicular) distance quite separate from normalising a vector? Secondly, how does equation $(1)$ translate into the normal distance being $ - \frac{w_0}{\lvert\lvert \textbf{w} \lvert\lvert}$ i.e. How is the quantity $\frac{\textbf{w}^T \textbf{x}}{\lvert\lvert \textbf{w} \lvert\lvert}$ the normal distance ?",,"['linear-algebra', 'self-learning', 'machine-learning']"
3,"Exponent of $GL(n,q)$.",Exponent of .,"GL(n,q)","Another exponent problem.  $GL(n,q)$ is the group of invertible $n\times n$ matrices over the finite field $GF(q)$, where $q$ is a prime power.  I am trying to figure out the exponent of this group. Ideas : We'd be doin real good if we could find the set of element orders $\{o(A):A\in GL(n,q)\}$, so I am looking for things to help me figure this out. The order of $GL(n,q)$ is $$|GL(n,q)|=\prod_{k=0}^{n-1}\left(q^n-q^k\right)$$ The set of prime factors of this number is a little unpredictable so I'm not sure whether I can use this. If $q$ is prime, the maximum order of an element in $GL(n,q)$ is at most $p^n-1$ (and this bound is strict I think).  Also I guess the maximal power of $p$ that occurs as an order of this group is $p^{\lceil \log_pn\rceil}$.  ( source ) I am aware of this , which I believe implies that $\text{Exp}(GL(2,p^n))=\text{lcm}\{p(p^{2n}-1),p-1\}$.  I am trying to think of how to extend it to the general case but I'm not sure. And I'm stuck; can't think of anything else that might help.","Another exponent problem.  $GL(n,q)$ is the group of invertible $n\times n$ matrices over the finite field $GF(q)$, where $q$ is a prime power.  I am trying to figure out the exponent of this group. Ideas : We'd be doin real good if we could find the set of element orders $\{o(A):A\in GL(n,q)\}$, so I am looking for things to help me figure this out. The order of $GL(n,q)$ is $$|GL(n,q)|=\prod_{k=0}^{n-1}\left(q^n-q^k\right)$$ The set of prime factors of this number is a little unpredictable so I'm not sure whether I can use this. If $q$ is prime, the maximum order of an element in $GL(n,q)$ is at most $p^n-1$ (and this bound is strict I think).  Also I guess the maximal power of $p$ that occurs as an order of this group is $p^{\lceil \log_pn\rceil}$.  ( source ) I am aware of this , which I believe implies that $\text{Exp}(GL(2,p^n))=\text{lcm}\{p(p^{2n}-1),p-1\}$.  I am trying to think of how to extend it to the general case but I'm not sure. And I'm stuck; can't think of anything else that might help.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'finite-groups', 'finite-fields']"
4,Relationship between cross product and outer product,Relationship between cross product and outer product,,"Since inner products $(V)$ are generalisations of dot products $(\mathbb{R}^n),$ then are outer products $(V)$ also related to cross products $(\mathbb{R}^3)$ in some way? A quick search reveals that they are—yet the outer product of two column vectors in $ \mathbb{R}^3$ is a $3\times3$ matrix, not another column vector. What is the connection?","Since inner products are generalisations of dot products then are outer products also related to cross products in some way? A quick search reveals that they are—yet the outer product of two column vectors in is a matrix, not another column vector. What is the connection?","(V) (\mathbb{R}^n), (V) (\mathbb{R}^3)  \mathbb{R}^3 3\times3","['linear-algebra', 'matrices', 'terminology', 'vector-spaces']"
5,Expected value of the smallest eigenvalue,Expected value of the smallest eigenvalue,,"Consider a random $m\times n$ matrix $M$ with elements from $\{-1,1\}$ and $m<n$. What is known about the expected value of the smallest eigenvalue of $MM^T$? The following picture shows numerical results for the expected value of the smallest eigenvalue of $MM^T$ for $n=120$ and $m=1 \dots 60$. How can you prove that the expected value is monotonically decreasing? Is it possible to get an estimate for the expected value?","Consider a random $m\times n$ matrix $M$ with elements from $\{-1,1\}$ and $m<n$. What is known about the expected value of the smallest eigenvalue of $MM^T$? The following picture shows numerical results for the expected value of the smallest eigenvalue of $MM^T$ for $n=120$ and $m=1 \dots 60$. How can you prove that the expected value is monotonically decreasing? Is it possible to get an estimate for the expected value?",,"['linear-algebra', 'probability']"
6,What are the advantages of studying the dual problem in linear programming?,What are the advantages of studying the dual problem in linear programming?,,"I am studying linear programming and I came across primal-dual algorithm in Linear Programming . I understood it but I am unable to understand why there is a need to calculate a dual, if problem can be solved in primal space. Are there some inherent advantages? I am fine with the proofs of weak duality and strong duality but I just wonder that what is the reason. Any help appreciated.","I am studying linear programming and I came across primal-dual algorithm in Linear Programming . I understood it but I am unable to understand why there is a need to calculate a dual, if problem can be solved in primal space. Are there some inherent advantages? I am fine with the proofs of weak duality and strong duality but I just wonder that what is the reason. Any help appreciated.",,"['linear-algebra', 'optimization', 'linear-programming']"
7,Sub-determinants of an orthogonal matrix,Sub-determinants of an orthogonal matrix,,"Let $A$ be a matrix in the special orthogonal group, $A \in \mathrm{SO}_n$ .  This means that $A$ is real, $n \times n$ , $A^t A = I$ and $\det(A)=1$ , that is, the column vectors of $A$ make a positively-oriented orthonormal basis for $\mathbb R^n$ . Decompose $A$ as a block matrix $$A = \begin{pmatrix} B & C \\ D & E\end{pmatrix}$$ where $B$ is $k \times k$ and $E$ is $(n-k)\times (n-k)$ . I'm looking for a basic linear-algebra argument that $\det(B) = \det(E)$ , ideally something that could be presented in a 2nd or 3rd year undergraduate course.  So I do not want people to invoke anything like tensor products or differential forms.","Let be a matrix in the special orthogonal group, .  This means that is real, , and , that is, the column vectors of make a positively-oriented orthonormal basis for . Decompose as a block matrix where is and is . I'm looking for a basic linear-algebra argument that , ideally something that could be presented in a 2nd or 3rd year undergraduate course.  So I do not want people to invoke anything like tensor products or differential forms.",A A \in \mathrm{SO}_n A n \times n A^t A = I \det(A)=1 A \mathbb R^n A A = \begin{pmatrix} B & C \\ D & E\end{pmatrix} B k \times k E (n-k)\times (n-k) \det(B) = \det(E),"['linear-algebra', 'euclidean-geometry', 'orthogonal-matrices']"
8,image of adjoint equals orthogonal complement of kernel [duplicate],image of adjoint equals orthogonal complement of kernel [duplicate],,"This question already has answers here : The range of $T^*$ is the orthogonal complement of $\ker(T)$ (2 answers) Closed 5 years ago . Let $T:V\to W$ be a linear map of finite-dimensional spaces.  Then $${\rm im}(T^{\textstyle*})=({\rm ker}\,T)^\perp\ .\tag{$*$}$$ I can prove this as follows: $${\rm ker}(T^{\textstyle*})=({\rm im}\,T)^\perp\tag{$**$}$$ is quite easy, and we know $T^{\textstyle*}{}^{\textstyle*}=T$ and $W^{\perp\perp}=W$, so $${\rm im}(T^{\textstyle*})=({\rm im}\,T^{\textstyle*})^{\perp\perp}=({\rm ker}\,T^{\textstyle*}{}^{\textstyle*})^\perp=({\rm ker}\,T)^\perp\ .$$ However, I would be interested in a ""direct"" proof of $(*)$.  It's fairly easy to show ${\rm LHS}\subseteq{\rm RHS}$.  For the converse I have tried obvious things but seem to be going round in circles. Also, any insights as to why $(**)$ is harder than $(*)$ - if in fact it is :) Edit .  To clarify, I am considering the adjoint defined in terms of an inner product, $$\langle\,T({\bf v})\mid{\bf w}\,\rangle   =\langle\,{\bf v}\mid T^{\textstyle*}({\bf w})\,\rangle\ .$$","This question already has answers here : The range of $T^*$ is the orthogonal complement of $\ker(T)$ (2 answers) Closed 5 years ago . Let $T:V\to W$ be a linear map of finite-dimensional spaces.  Then $${\rm im}(T^{\textstyle*})=({\rm ker}\,T)^\perp\ .\tag{$*$}$$ I can prove this as follows: $${\rm ker}(T^{\textstyle*})=({\rm im}\,T)^\perp\tag{$**$}$$ is quite easy, and we know $T^{\textstyle*}{}^{\textstyle*}=T$ and $W^{\perp\perp}=W$, so $${\rm im}(T^{\textstyle*})=({\rm im}\,T^{\textstyle*})^{\perp\perp}=({\rm ker}\,T^{\textstyle*}{}^{\textstyle*})^\perp=({\rm ker}\,T)^\perp\ .$$ However, I would be interested in a ""direct"" proof of $(*)$.  It's fairly easy to show ${\rm LHS}\subseteq{\rm RHS}$.  For the converse I have tried obvious things but seem to be going round in circles. Also, any insights as to why $(**)$ is harder than $(*)$ - if in fact it is :) Edit .  To clarify, I am considering the adjoint defined in terms of an inner product, $$\langle\,T({\bf v})\mid{\bf w}\,\rangle   =\langle\,{\bf v}\mid T^{\textstyle*}({\bf w})\,\rangle\ .$$",,"['linear-algebra', 'adjoint-operators']"
9,"Taking derivative of $L_0$-norm, $L_1$-norm, $L_2$-norm","Taking derivative of -norm, -norm, -norm",L_0 L_1 L_2,"I am a little confused about taking derivatives w.r.t. the norms. $L_0$-norm : $L_0$ means number of non-zero elements  in a vector.  Say, I am interested in an $x_i$. $$\displaystyle\min_{i}(y_i-x_i)^2+c\|x_i \|_{0}$$ The answer depends on $x_i=0$ or not? My work: take the norm of $x_i$, which is a constant, then, derivative, so it's 0. $L_1$-norm : Manhattan distance.  What should I do? $$\displaystyle\min_{i}(y_i-x_i)^2+c\|x_i \|_{1}$$ $L_2$-norm :Euclidean distance.  What should I do? $$\displaystyle\min_{i}(y_i-x_i)^2+c\|x_i \|_{2}$$","I am a little confused about taking derivatives w.r.t. the norms. $L_0$-norm : $L_0$ means number of non-zero elements  in a vector.  Say, I am interested in an $x_i$. $$\displaystyle\min_{i}(y_i-x_i)^2+c\|x_i \|_{0}$$ The answer depends on $x_i=0$ or not? My work: take the norm of $x_i$, which is a constant, then, derivative, so it's 0. $L_1$-norm : Manhattan distance.  What should I do? $$\displaystyle\min_{i}(y_i-x_i)^2+c\|x_i \|_{1}$$ $L_2$-norm :Euclidean distance.  What should I do? $$\displaystyle\min_{i}(y_i-x_i)^2+c\|x_i \|_{2}$$",,"['linear-algebra', 'optimization']"
10,Proof that Eigenvalues are the Diagonal Entries of the Upper-Triangular Matrix in Axler,Proof that Eigenvalues are the Diagonal Entries of the Upper-Triangular Matrix in Axler,,"This is 5.18 from Axler's Linear Algebra Done Right: Theorem : Suppose $T \in L(V)$ has an upper-triangular matrix with respect to some basis of $V$ . Then the eigenvalues of $T$ consist precisely of the entries on the diagonal of that upper-triangular matrix. Proof: Suppose $(v_1, \ldots , v_n)$ is a basis of $V$ with respect to which $T$ has an upper-triangular matrix where the diagonal entries are $\lambda_1, \ldots, \lambda_n$ . Let $\lambda \in F$ Then for matrix $M(T - \lambda I$ ) where the diagonal entries are $\lambda_1 - \lambda, \ldots \lambda_n - \lambda.$ We can suppose we are dealing with complex vector spaces. From 5.16, where it has proven that $T$ is not invertible iff one of the $\lambda_k$ 's equals $0$ . Hence $T - λI$ is not invertible if and only if $λ$ equals one of the $λ_j$ 's. In other words, $λ$ is an eigenvalue of $T$ if and only if $λ$ equals one of the $λ_j$ s, as desired. Question: This only showed that one of the diagonal entries is an eigenvalue but not all of them, as the theorem claimed.","This is 5.18 from Axler's Linear Algebra Done Right: Theorem : Suppose has an upper-triangular matrix with respect to some basis of . Then the eigenvalues of consist precisely of the entries on the diagonal of that upper-triangular matrix. Proof: Suppose is a basis of with respect to which has an upper-triangular matrix where the diagonal entries are . Let Then for matrix ) where the diagonal entries are We can suppose we are dealing with complex vector spaces. From 5.16, where it has proven that is not invertible iff one of the 's equals . Hence is not invertible if and only if equals one of the 's. In other words, is an eigenvalue of if and only if equals one of the s, as desired. Question: This only showed that one of the diagonal entries is an eigenvalue but not all of them, as the theorem claimed.","T \in L(V) V T (v_1, \ldots , v_n) V T \lambda_1, \ldots, \lambda_n \lambda \in F M(T - \lambda I \lambda_1 - \lambda, \ldots \lambda_n - \lambda. T \lambda_k 0 T - λI λ λ_j λ T λ λ_j","['linear-algebra', 'eigenvalues-eigenvectors']"
11,reference for linear algebra books that teach reverse Hermite method for symmetric matrices,reference for linear algebra books that teach reverse Hermite method for symmetric matrices,,"May 11, 2019. Evidently the original method should be attributed to Lagrange in 1759. I got confused, Hermite is much more recent. January 13, 2016: book that does this mentioned in a question today, Linear Algebra Done Wrong by Sergei Treil. He calls it non-orthogonal diagonalization of a quadratic form, calls his first method completion of squares, pages 201-202, section 2.2.1.  In section 2.2.2, pages 202-205, he describes this method, calling it Diagonalization using row/column operations. The method I mean is useful for symmetric matrices with integer, or at least rational entries. It diagonalizes but does NOT orthogonally diagonalize. The direction I do it, I usually call it Hermite reduction or Hermite's method. At the end, I need to find the inverse of my matrix (which usually has determinant one so it is not so bad). This other method produces an answer directly, a cookbook method not conceptually different from row reduction of matrices, especially using that to find its inverse. This method is very similar to Gauss reduction for positive binary quadratic forms, just allowing rational coefficients in the elementary matrices used; Gauss stuck with integers. The method is mostly Gauss reduction, intended for binary positive forms. We deal with two variables (row/column pairs) at a time. As long as one of the two diagonal entries is nonzero there is no trouble, no choices to be made. We start with a symmetric matrix $A_0.$ At each step, call it step $n,$ we are going to use some elementary matrix $E_n,$ same as in row reduction, such that $A_n =E_n^T A_{n-1} E_n$ has one fewer pair of off-diagonal nonzero entries. We also began with $P_0=I,$ then each step we take $P_n=P_{n-1}E_n.$ Eventually we get to some $n=N$ such that $A_N=D$ is diagonal and $P_N=P,$ with $P^T A P = D$ by construction. Oh, also by construction, $P$ has determinant $1.$ I JUST PUT AN EXAMPLE AT Find the transitional matrix that would transform this form to a diagonal form. not yet typeset, it is input and output from gp-pari and should not be too difficult to read, indeed one may copy the individual commands into pari and see how it progresses. I also put a 4 by 4 answer, the final answer typeset otherwise gp-pari output, at Given a $4\times 4$ symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it? Let me go through the two examples, the second involves a choice because we get a zero diagonal element at one point. First: Let $$A = \left(\begin{array}{cc} 2&3 \\ 3&4  \end{array}\right) \in M_n(\mathbb{C})$$ Find $P$ such that $P^TAP = D$ where $D$ is a diagonal matrix. So here's the solution: $$A = \left(\begin{array}{cc|cc} 2&3&1&0\\ 3&4&0&1    \end{array}\right) \sim \left(\begin{array}{cc|cc} 2&0&1&-3/2\\ 0&-1/2&0&1    \end{array}\right)$$ Therefore, $$P = \left(\begin{array}{cc} 1&-3/2\\ 0&1    \end{array}\right) \\ P^TAP = \left(\begin{array}{cc} 2&0\\ 0&-1/2    \end{array}\right) $$ So, this one was just Gauss reduction, allowing a rational off-diagonal entry in my $E_1$ in order to force the $1,2$ and $2,1$ pair of  positions to become zero. As long as the upper left of the two diagonal coefficients is nonzero, we may take our $E_n$ to be upper triangular. If we are faced with a zero diagonal entry in the first row/diagonal that possesses any nonzero (therefore off-diagonal) entries,  we need to do an extra step to force a nonzero diagonal element. So, let's do the ever popular form $2xy$ this way. $$ A = A_0 = \left( \begin{array}{cc} 0 & 1 \\ 1 & 0 \end{array} \right)  $$ As both diagonal entries are zero, switching row/columns 1 and 2 will still give $0$ in the 1,1 position. We do not like that. Instead, we take a lower triangular $E_n,$ here $$ E_1 =  \left( \begin{array}{cc} 1 & 0 \\ 1 & 1 \end{array} \right)  $$ The way i am numbering the matrices, this gives $$ A_1 = E_1^T A E_1 =  \left( \begin{array}{cc} 2 & 1 \\ 1 & 0 \end{array} \right),  $$ also $$ P_1 = E_1.  $$ Next, we go back to the more common upper triangular elementary matrices, with $$ E_2 =  \left( \begin{array}{cc} 1 & -\frac{1}{2} \\ 0 & 1 \end{array} \right).  $$ $$ D= A_2 = E_2^T A_1 E_2 =  \left( \begin{array}{cc} 2 & 0 \\ 0 & -\frac{1}{2} \end{array} \right),  $$ also $$  P = P_2 = P_1 E_2 = E_1 E_2 = \left( \begin{array}{cc} 1 & -\frac{1}{2} \\ 1 & \frac{1}{2} \end{array} \right),  $$ Note that, from $A_1 = E_1^T A E_1 $ and $D= A_2 = E_2^T A_1 E_2$ we indeed have $$\color{red}{ D= A_2 = E_2^T (E_1^T A E_1) E_2 = E_2^T E_1^T A E_1 E_2 =  (E_1 E_2)^T A (E_1 E_2)} $$ which is why $P = E_1 E_2.$ The solution manual that has this would use ""augmented"" matrices, 4 by 2, not record the individual $E_i,$ just the $A_i$ augmented by $P_i.$ At least, given how I am numbering things, this is how I  prefer to write such a summary, it may be slightly different for the examples in the other question: $$ (A_0|P_0) = \left(\begin{array}{cc|cc} 0&1&1&0\\ 1&0&0&1    \end{array}\right)$$ $$ \mapsto  (A_1|P_1) =  \left(\begin{array}{cc|cc} 2&1&1&0\\ 1&0&1&1    \end{array}\right)$$ $$ \mapsto  (A_2|P_2) = \left(\begin{array}{cc|cc} 2&0&1&-\frac{1}{2}\\ 0&-\frac{1}{2}&1&\frac{1}{2}    \end{array}\right)$$ I have been seeing this method lately, but do not know any book that teaches it (or in what language). It would appear to be a book about matrix theory or linear algebra, and may never mention quadratic forms, hard to predict. Or, it may do quadratic forms over the reals, as is pretty common, and ignore the case of integer coefficients. I suspect nobody on MSE has taught this method, perhaps it is a recent book. Here are recent occurrences, apparently two by the same guy, then two by another person. To find others, look for my answers that use the phrase Hermite reduction. One of the latter is answered my way, just called repeated completing the square, which is exactly right. Finding $P$ such that $P^TAP$ is a diagonal matrix Diagonalize a symmetric matrix Find the transitional matrix that would transform this form to a diagonal form. Diagonalizing a particular $3\times3$-matrix. Very similar to the method in a Schaum's outline as seen in this answer: Given a $4\times 4$ symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it? Indeed, here is the image uploaded by el.Salvador there:","May 11, 2019. Evidently the original method should be attributed to Lagrange in 1759. I got confused, Hermite is much more recent. January 13, 2016: book that does this mentioned in a question today, Linear Algebra Done Wrong by Sergei Treil. He calls it non-orthogonal diagonalization of a quadratic form, calls his first method completion of squares, pages 201-202, section 2.2.1.  In section 2.2.2, pages 202-205, he describes this method, calling it Diagonalization using row/column operations. The method I mean is useful for symmetric matrices with integer, or at least rational entries. It diagonalizes but does NOT orthogonally diagonalize. The direction I do it, I usually call it Hermite reduction or Hermite's method. At the end, I need to find the inverse of my matrix (which usually has determinant one so it is not so bad). This other method produces an answer directly, a cookbook method not conceptually different from row reduction of matrices, especially using that to find its inverse. This method is very similar to Gauss reduction for positive binary quadratic forms, just allowing rational coefficients in the elementary matrices used; Gauss stuck with integers. The method is mostly Gauss reduction, intended for binary positive forms. We deal with two variables (row/column pairs) at a time. As long as one of the two diagonal entries is nonzero there is no trouble, no choices to be made. We start with a symmetric matrix At each step, call it step we are going to use some elementary matrix same as in row reduction, such that has one fewer pair of off-diagonal nonzero entries. We also began with then each step we take Eventually we get to some such that is diagonal and with by construction. Oh, also by construction, has determinant I JUST PUT AN EXAMPLE AT Find the transitional matrix that would transform this form to a diagonal form. not yet typeset, it is input and output from gp-pari and should not be too difficult to read, indeed one may copy the individual commands into pari and see how it progresses. I also put a 4 by 4 answer, the final answer typeset otherwise gp-pari output, at Given a $4\times 4$ symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it? Let me go through the two examples, the second involves a choice because we get a zero diagonal element at one point. First: Let Find such that where is a diagonal matrix. So here's the solution: Therefore, So, this one was just Gauss reduction, allowing a rational off-diagonal entry in my in order to force the and pair of  positions to become zero. As long as the upper left of the two diagonal coefficients is nonzero, we may take our to be upper triangular. If we are faced with a zero diagonal entry in the first row/diagonal that possesses any nonzero (therefore off-diagonal) entries,  we need to do an extra step to force a nonzero diagonal element. So, let's do the ever popular form this way. As both diagonal entries are zero, switching row/columns 1 and 2 will still give in the 1,1 position. We do not like that. Instead, we take a lower triangular here The way i am numbering the matrices, this gives also Next, we go back to the more common upper triangular elementary matrices, with also Note that, from and we indeed have which is why The solution manual that has this would use ""augmented"" matrices, 4 by 2, not record the individual just the augmented by At least, given how I am numbering things, this is how I  prefer to write such a summary, it may be slightly different for the examples in the other question: I have been seeing this method lately, but do not know any book that teaches it (or in what language). It would appear to be a book about matrix theory or linear algebra, and may never mention quadratic forms, hard to predict. Or, it may do quadratic forms over the reals, as is pretty common, and ignore the case of integer coefficients. I suspect nobody on MSE has taught this method, perhaps it is a recent book. Here are recent occurrences, apparently two by the same guy, then two by another person. To find others, look for my answers that use the phrase Hermite reduction. One of the latter is answered my way, just called repeated completing the square, which is exactly right. Finding $P$ such that $P^TAP$ is a diagonal matrix Diagonalize a symmetric matrix Find the transitional matrix that would transform this form to a diagonal form. Diagonalizing a particular $3\times3$-matrix. Very similar to the method in a Schaum's outline as seen in this answer: Given a $4\times 4$ symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it? Indeed, here is the image uploaded by el.Salvador there:","A_0. n, E_n, A_n =E_n^T A_{n-1} E_n P_0=I, P_n=P_{n-1}E_n. n=N A_N=D P_N=P, P^T A P = D P 1. A = \left(\begin{array}{cc} 2&3 \\ 3&4  \end{array}\right) \in
M_n(\mathbb{C}) P P^TAP = D D A = \left(\begin{array}{cc|cc} 2&3&1&0\\ 3&4&0&1    \end{array}\right) \sim \left(\begin{array}{cc|cc} 2&0&1&-3/2\\ 0&-1/2&0&1    \end{array}\right) P = \left(\begin{array}{cc} 1&-3/2\\ 0&1    \end{array}\right) \\ P^TAP = \left(\begin{array}{cc} 2&0\\ 0&-1/2    \end{array}\right)  E_1 1,2 2,1 E_n 2xy  A = A_0 =
\left(
\begin{array}{cc}
0 & 1 \\
1 & 0
\end{array}
\right) 
 0 E_n,  E_1 = 
\left(
\begin{array}{cc}
1 & 0 \\
1 & 1
\end{array}
\right) 
  A_1 = E_1^T A E_1 = 
\left(
\begin{array}{cc}
2 & 1 \\
1 & 0
\end{array}
\right), 
  P_1 = E_1.    E_2 = 
\left(
\begin{array}{cc}
1 & -\frac{1}{2} \\
0 & 1
\end{array}
\right). 
  D= A_2 = E_2^T A_1 E_2 = 
\left(
\begin{array}{cc}
2 & 0 \\
0 & -\frac{1}{2}
\end{array}
\right), 
   P = P_2 = P_1 E_2 = E_1 E_2 =
\left(
\begin{array}{cc}
1 & -\frac{1}{2} \\
1 & \frac{1}{2}
\end{array}
\right), 
 A_1 = E_1^T A E_1  D= A_2 = E_2^T A_1 E_2 \color{red}{
D= A_2 = E_2^T (E_1^T A E_1) E_2 = E_2^T E_1^T A E_1 E_2 =  (E_1 E_2)^T A (E_1 E_2)}
 P = E_1 E_2. E_i, A_i P_i.  (A_0|P_0) = \left(\begin{array}{cc|cc} 0&1&1&0\\ 1&0&0&1    \end{array}\right)  \mapsto  (A_1|P_1) =  \left(\begin{array}{cc|cc} 2&1&1&0\\ 1&0&1&1    \end{array}\right)  \mapsto  (A_2|P_2) = \left(\begin{array}{cc|cc} 2&0&1&-\frac{1}{2}\\ 0&-\frac{1}{2}&1&\frac{1}{2}    \end{array}\right)","['linear-algebra', 'reference-request', 'quadratic-forms']"
12,Linear homomorphisms of square matrices are conjugations,Linear homomorphisms of square matrices are conjugations,,"I was doing some linear algebra exercises and came across the following tough problem : Let $M_{n\times n}(\mathbf{R})$ denote the set of all the matrices whose entries are real numbers. Suppose $\phi:M_{n\times n}(\mathbf{R})\to M_{n\times n}(\mathbf{R})$ is a nonzero linear transform (i.e. there is a matrix $A$ such that $\phi(A)\neq 0$) such that for all $A,B\in M_{n\times n}(\mathbf{R})$   $$\phi(AB)=\phi(A)\phi(B).$$   Prove that there exists a invertible matrix $T\in M_{n\times n}(\mathbf{R})$ such that    $$\phi(A)=TAT^{-1}$$   for all $A\in M_{n\times n}(\mathbf{R})$. This is an exercise from my textbook and I am all thumbs when I attempted to solve it . Can someone tell me as to how should I , at least , start the problem ?","I was doing some linear algebra exercises and came across the following tough problem : Let $M_{n\times n}(\mathbf{R})$ denote the set of all the matrices whose entries are real numbers. Suppose $\phi:M_{n\times n}(\mathbf{R})\to M_{n\times n}(\mathbf{R})$ is a nonzero linear transform (i.e. there is a matrix $A$ such that $\phi(A)\neq 0$) such that for all $A,B\in M_{n\times n}(\mathbf{R})$   $$\phi(AB)=\phi(A)\phi(B).$$   Prove that there exists a invertible matrix $T\in M_{n\times n}(\mathbf{R})$ such that    $$\phi(A)=TAT^{-1}$$   for all $A\in M_{n\times n}(\mathbf{R})$. This is an exercise from my textbook and I am all thumbs when I attempted to solve it . Can someone tell me as to how should I , at least , start the problem ?",,"['linear-algebra', 'matrices', 'lie-groups', 'lie-algebras']"
13,What is the intuitive meaning of the adjugate matrix?,What is the intuitive meaning of the adjugate matrix?,,"The definition of the adjugate matrix is easy to understand, but I have never seen it used for anything. What is the intuitive meaning of this matrix? Are there examples of applications which may shed light on its conceptual meaning?  I would be especially interested to hear examples of usage in representation theory or other abstract algebra disciplines.","The definition of the adjugate matrix is easy to understand, but I have never seen it used for anything. What is the intuitive meaning of this matrix? Are there examples of applications which may shed light on its conceptual meaning?  I would be especially interested to hear examples of usage in representation theory or other abstract algebra disciplines.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'intuition', 'definition']"
14,The arithmetic-geometric mean for symmetric positive definite matrices,The arithmetic-geometric mean for symmetric positive definite matrices,,"A while back, I wanted to see if the notion of the arithmetic-geometric mean could be extended to a pair of symmetric positive definite matrices. (I considered positive definite matrices only since the notion of the matrix square root is a bit intricate for other kinds of matrices.) I expected that some complications would arise since, unlike scalar multiplication, matrix multiplication is noncommutative. Another complication would be that the product of two symmetric matrices need not be symmetric (though the positive definiteness is retained, so one can still speak of a principal matrix square root). By analogy with the scalar AGM, I considered the iteration $$\mathbf A_0=\mathbf A \; ; \; \mathbf B_0=\mathbf B$$ $$\mathbf A_{i+1}=\frac12(\mathbf A_i+\mathbf B_i) \; ; \; \mathbf B_{i+1}=\sqrt{\mathbf A_i \mathbf B_i}$$ I cranked up a short Mathematica routine: matAGM[u_, v_] := First[FixedPoint[       {Apply[Plus, #]/2, MatrixPower[Apply[Dot, #], 1/2]} &, {u, v}]] /;       MatrixQ[u, InexactNumberQ] && MatrixQ[v, InexactNumberQ] and decided to try it out on randomly generated SPD matrices. (A numerical note: Mathematica uses the numerically stable Schur decomposition in computing matrix functions like the matrix square root.) I found that for all of the randomly generated pairs of SPD matrices I tried, the process was convergent (though the rate of convergence is apparently not as fast as the scalar AGM). As expected, the order matters: matAGM[A, B] and matAGM[B, A] are usually not equal (and both results are unsymmetric ) unless A and B commute (for the special case of diagonal A and B , the result is the diagonal matrix whose entries are the arithmetic-geometric means of the corresponding entries of the pair.) I now have three questions: How do I prove or disprove that this process converges for any pair of SPD matrices? If it is convergent, what is the rate of convergence? Is there any relationship between matAGM[A, B] and matAGM[B, A] if the two matrices A and B do not commute? Is there any relationship between this matrix arithmetic-geometric mean and the usual scalar arithmetic-geometric mean? Would, say, arithmetic-geometric means of the eigenvalues of the two matrices have anything to do with this? (added 8/12/2011) More digging around has me convinced that I should indeed be considering the formulation of the geometric mean by Pusz and Woronowicz : $$\mathbf A\#\mathbf B=\mathbf A^{1/2}(\mathbf A^{-1/2}\mathbf B\mathbf A^{-1/2})^{1/2}\mathbf A^{1/2}$$ as more natural; the proof of convergence is then simplified, as shown in the article Willie linked to. However, I'm still wondering why the original ""unnatural"" formulation seems to be convergent (or else, I'd like to see a pair of SPD matrices that cause trouble for the unnatural iteration). I am also interested in how elliptic integrals might crop up in here, just as they did for the scalar version of the AGM.","A while back, I wanted to see if the notion of the arithmetic-geometric mean could be extended to a pair of symmetric positive definite matrices. (I considered positive definite matrices only since the notion of the matrix square root is a bit intricate for other kinds of matrices.) I expected that some complications would arise since, unlike scalar multiplication, matrix multiplication is noncommutative. Another complication would be that the product of two symmetric matrices need not be symmetric (though the positive definiteness is retained, so one can still speak of a principal matrix square root). By analogy with the scalar AGM, I considered the iteration $$\mathbf A_0=\mathbf A \; ; \; \mathbf B_0=\mathbf B$$ $$\mathbf A_{i+1}=\frac12(\mathbf A_i+\mathbf B_i) \; ; \; \mathbf B_{i+1}=\sqrt{\mathbf A_i \mathbf B_i}$$ I cranked up a short Mathematica routine: matAGM[u_, v_] := First[FixedPoint[       {Apply[Plus, #]/2, MatrixPower[Apply[Dot, #], 1/2]} &, {u, v}]] /;       MatrixQ[u, InexactNumberQ] && MatrixQ[v, InexactNumberQ] and decided to try it out on randomly generated SPD matrices. (A numerical note: Mathematica uses the numerically stable Schur decomposition in computing matrix functions like the matrix square root.) I found that for all of the randomly generated pairs of SPD matrices I tried, the process was convergent (though the rate of convergence is apparently not as fast as the scalar AGM). As expected, the order matters: matAGM[A, B] and matAGM[B, A] are usually not equal (and both results are unsymmetric ) unless A and B commute (for the special case of diagonal A and B , the result is the diagonal matrix whose entries are the arithmetic-geometric means of the corresponding entries of the pair.) I now have three questions: How do I prove or disprove that this process converges for any pair of SPD matrices? If it is convergent, what is the rate of convergence? Is there any relationship between matAGM[A, B] and matAGM[B, A] if the two matrices A and B do not commute? Is there any relationship between this matrix arithmetic-geometric mean and the usual scalar arithmetic-geometric mean? Would, say, arithmetic-geometric means of the eigenvalues of the two matrices have anything to do with this? (added 8/12/2011) More digging around has me convinced that I should indeed be considering the formulation of the geometric mean by Pusz and Woronowicz : $$\mathbf A\#\mathbf B=\mathbf A^{1/2}(\mathbf A^{-1/2}\mathbf B\mathbf A^{-1/2})^{1/2}\mathbf A^{1/2}$$ as more natural; the proof of convergence is then simplified, as shown in the article Willie linked to. However, I'm still wondering why the original ""unnatural"" formulation seems to be convergent (or else, I'd like to see a pair of SPD matrices that cause trouble for the unnatural iteration). I am also interested in how elliptic integrals might crop up in here, just as they did for the scalar version of the AGM.",,"['linear-algebra', 'matrices']"
15,Are matrices best understood as linear maps?,Are matrices best understood as linear maps?,,"Any linear map between finite-dimensional vector spaces may be represented by a matrix, and conversely. Matrix-matrix multiplication corresponds to map composition, and matrix-vector multiplication corresponds to map application. This provides a nice way of understanding matrices and the origin of the rules for their calculation. Having attained this perspective is a huge boon to me, because I've always resented that matrix math is basically just taught as a collection of arbitrary manipulations of symbols (multiply the entries! now add them! don't ask why! ). But is this understanding always meaningful? For in general, just because you've found an interpretation for a mathematical concept doesn't mean it will always apply. For instance, the trig functions can be thought of as simply being the coordinates on a unit circle, but this fails to explain why they show up as solutions to differential equations having nothing to do with circles (or indeed geometry) - to explain this we require a more abstract understanding of the trig functions. Thus, in all non-trivial uses of matrices in pure and applied mathematics, can we meaningfully say that the matrices involved represent, at least implicitly, linear maps? For instance, we can think of solving a system of equations as simply trying to find a vector which, when a given map is applied to it, yields a specific image. By non-trivial, I mean that the particular use of matrices involves, at minimum, matrix multiplication and/or determinants or other elementary aspects of the calculus of matrices. For instance, you could call a spreadsheet a matrix, but unless you intend to do anything matrix-like with it apart from draw it as a grid, you may as well just call it a set of numbers. Note: this question may be soft, but I feel it can be given a clean answer. Either you can come up with a situation where the linear-map interpretation doesn't apply, or you can't.","Any linear map between finite-dimensional vector spaces may be represented by a matrix, and conversely. Matrix-matrix multiplication corresponds to map composition, and matrix-vector multiplication corresponds to map application. This provides a nice way of understanding matrices and the origin of the rules for their calculation. Having attained this perspective is a huge boon to me, because I've always resented that matrix math is basically just taught as a collection of arbitrary manipulations of symbols (multiply the entries! now add them! don't ask why! ). But is this understanding always meaningful? For in general, just because you've found an interpretation for a mathematical concept doesn't mean it will always apply. For instance, the trig functions can be thought of as simply being the coordinates on a unit circle, but this fails to explain why they show up as solutions to differential equations having nothing to do with circles (or indeed geometry) - to explain this we require a more abstract understanding of the trig functions. Thus, in all non-trivial uses of matrices in pure and applied mathematics, can we meaningfully say that the matrices involved represent, at least implicitly, linear maps? For instance, we can think of solving a system of equations as simply trying to find a vector which, when a given map is applied to it, yields a specific image. By non-trivial, I mean that the particular use of matrices involves, at minimum, matrix multiplication and/or determinants or other elementary aspects of the calculus of matrices. For instance, you could call a spreadsheet a matrix, but unless you intend to do anything matrix-like with it apart from draw it as a grid, you may as well just call it a set of numbers. Note: this question may be soft, but I feel it can be given a clean answer. Either you can come up with a situation where the linear-map interpretation doesn't apply, or you can't.",,"['linear-algebra', 'matrices', 'soft-question', 'intuition', 'quadratic-forms']"
16,What do characteristic polynomials characterize?,What do characteristic polynomials characterize?,,"Let $R$ be an integral domain and $F$ a finitely generated free module over $R$. For a linear transformation $\alpha\in\operatorname{End}_R(F)$, the characteristic polynomial is \begin{equation} p_\alpha(t)=\det(t-\alpha)\in R[t]. \end{equation} Similar transformations have same characteristic polynomial. However, these polynomials fail to characterize similarity, in the sense that non-similar transformations can have the same characteristic polynomial. So my question is: What can we say about two transformations $\alpha$ and $\beta$ that share the same polynomial? Obviously, $\alpha$ and $\beta$ have the same spectrum and same algebraic multiplicity for each eigenvalue. But unless $R$ is an algebraically closed field--in this case the polynomial is determined by the roots and their multiplicities--we should be able to say more about $\alpha$ and $\beta$. Also we should know more than traces and determinants since they are just two of the coefficients. Can someone give a hint? Thanks!","Let $R$ be an integral domain and $F$ a finitely generated free module over $R$. For a linear transformation $\alpha\in\operatorname{End}_R(F)$, the characteristic polynomial is \begin{equation} p_\alpha(t)=\det(t-\alpha)\in R[t]. \end{equation} Similar transformations have same characteristic polynomial. However, these polynomials fail to characterize similarity, in the sense that non-similar transformations can have the same characteristic polynomial. So my question is: What can we say about two transformations $\alpha$ and $\beta$ that share the same polynomial? Obviously, $\alpha$ and $\beta$ have the same spectrum and same algebraic multiplicity for each eigenvalue. But unless $R$ is an algebraically closed field--in this case the polynomial is determined by the roots and their multiplicities--we should be able to say more about $\alpha$ and $\beta$. Also we should know more than traces and determinants since they are just two of the coefficients. Can someone give a hint? Thanks!",,"['linear-algebra', 'abstract-algebra', 'matrices', 'characteristic-polynomial']"
17,Integrating a matrix function involving a determinant and exponential trace,Integrating a matrix function involving a determinant and exponential trace,,"I am trying to find the normalizing constant for a probability distribution and ran into a difficult integral. When $X$ is an $p \times k$ matrix, $a>0,$ and $g>0,$ how can I compute $$\int \left|I_k + \frac{g}{a}X^TX\right|^{-d/2} \text{exp}\left\{-\frac{1}{2}\text{tr}[aX^TX]\right\}dX$$ where the bars indicate the determinant? I've wanted to change variables. For example, to send $X \to \left|I_k + \frac{g}{a}X^TX\right|$ but this isn't injective. Similarly, I've considered other transformations, e.g. using a singular value decomposition, but haven't had much success. If $k=1$ then I can use a result like this one: Integration of radial functions? but I don't know of any similar formula that helps when $k>1.$ Edit: I claim the integrand is the kernel of a spherical matrix variate density in $X.$ From the Matrix Variate Distributions book referenced in my comment, a random matrix $X$ is said to have a spherical distribution if $X = \Gamma X \Lambda$ where the equality is in distribution and $\Gamma$ and $\Lambda$ are orthogonal matrices of appropriate dimension. The claim follows from two observations: That $\text{tr}[a(\Gamma X \Lambda)^T\Gamma X \Lambda] = \text{tr}[aX^TX],$ which follows easily from the othogonality of $\Gamma$ and $\Lambda$ and the cyclic property of the trace That $\left|I_k + \frac{g}{a}(\Gamma X \Lambda)^T\Gamma X \Lambda\right| = \left|I_k + \frac{g}{a} \Lambda^T X^T X \Lambda \right|  = \left|I_k + \frac{g}{a}X^TX\right|,$ which we get using orthogonality and the matrix determinant lemma. The Gupta and Nagar book discusses densities, including normalizing constants, of spherical matrix variate distributions. So it is likely possible to calculate the integral by recognizing this kernel. Alternatively, we might do some sort of matrix integration by parts in which we differentiate the determinant term with respect to $X$.","I am trying to find the normalizing constant for a probability distribution and ran into a difficult integral. When $X$ is an $p \times k$ matrix, $a>0,$ and $g>0,$ how can I compute $$\int \left|I_k + \frac{g}{a}X^TX\right|^{-d/2} \text{exp}\left\{-\frac{1}{2}\text{tr}[aX^TX]\right\}dX$$ where the bars indicate the determinant? I've wanted to change variables. For example, to send $X \to \left|I_k + \frac{g}{a}X^TX\right|$ but this isn't injective. Similarly, I've considered other transformations, e.g. using a singular value decomposition, but haven't had much success. If $k=1$ then I can use a result like this one: Integration of radial functions? but I don't know of any similar formula that helps when $k>1.$ Edit: I claim the integrand is the kernel of a spherical matrix variate density in $X.$ From the Matrix Variate Distributions book referenced in my comment, a random matrix $X$ is said to have a spherical distribution if $X = \Gamma X \Lambda$ where the equality is in distribution and $\Gamma$ and $\Lambda$ are orthogonal matrices of appropriate dimension. The claim follows from two observations: That $\text{tr}[a(\Gamma X \Lambda)^T\Gamma X \Lambda] = \text{tr}[aX^TX],$ which follows easily from the othogonality of $\Gamma$ and $\Lambda$ and the cyclic property of the trace That $\left|I_k + \frac{g}{a}(\Gamma X \Lambda)^T\Gamma X \Lambda\right| = \left|I_k + \frac{g}{a} \Lambda^T X^T X \Lambda \right|  = \left|I_k + \frac{g}{a}X^TX\right|,$ which we get using orthogonality and the matrix determinant lemma. The Gupta and Nagar book discusses densities, including normalizing constants, of spherical matrix variate distributions. So it is likely possible to calculate the integral by recognizing this kernel. Alternatively, we might do some sort of matrix integration by parts in which we differentiate the determinant term with respect to $X$.",,"['linear-algebra', 'integration', 'matrices', 'probability-theory', 'probability-distributions']"
18,Verify matrix identity $A^tD-C^tB=I$ on certain hypotheses,Verify matrix identity  on certain hypotheses,A^tD-C^tB=I,"Given $n\times n$ real matrices $A,B,C,D$ such that: $AB^T$ and $CD^T$ are symmetric $AD^T-BC^T=I$ Prove that $A^TD-C^TB=I$ The solution I have come up with after a very long time is to consider: $$\left( \begin{array}{cc} A & -B \\ -C & D \end{array} \right)\left( \begin{array}{cc} D^T & B^T \\ C^T & A^T \end{array} \right)=\left( \begin{array}{cc} I & 0 \\ 0 & I \end{array} \right)\rightarrow\left( \begin{array}{cc} D^T & B^T \\ C^T & A^T \end{array} \right)\left( \begin{array}{cc} A & -B \\ -C & D \end{array} \right)=\left( \begin{array}{cc} I & 0 \\ 0 & I \end{array} \right)$$ then $D^TA-B^TC=I$. However this solution is apparently tricky. I would love to have a more natural solution (which may be applied to other problems as well)","Given $n\times n$ real matrices $A,B,C,D$ such that: $AB^T$ and $CD^T$ are symmetric $AD^T-BC^T=I$ Prove that $A^TD-C^TB=I$ The solution I have come up with after a very long time is to consider: $$\left( \begin{array}{cc} A & -B \\ -C & D \end{array} \right)\left( \begin{array}{cc} D^T & B^T \\ C^T & A^T \end{array} \right)=\left( \begin{array}{cc} I & 0 \\ 0 & I \end{array} \right)\rightarrow\left( \begin{array}{cc} D^T & B^T \\ C^T & A^T \end{array} \right)\left( \begin{array}{cc} A & -B \\ -C & D \end{array} \right)=\left( \begin{array}{cc} I & 0 \\ 0 & I \end{array} \right)$$ then $D^TA-B^TC=I$. However this solution is apparently tricky. I would love to have a more natural solution (which may be applied to other problems as well)",,"['linear-algebra', 'alternative-proof']"
19,What are examples of vectors that are not usually called vectors?,What are examples of vectors that are not usually called vectors?,,"In algebra, a vector is an element of a vector space. An example of such an element is a matrix. In linear algebra, a vector is a shorthand name for a $1 \times m$ or a $ n \times 1 $ matrix. (Whereas a matrix itself is also a vector, by definition, but rarely referred to as such.) In (analytic) geometry, a (euclidean) vector is a geometric object with a magnitude and direction. These can be represented by tuples . Both matrices and tuples are clearly vector elements in some spaces, but are for some reason not called vectors by name, unlike euclidean vectors and row vectors . Are there any other examples of vectors that are not called vectors, like matrices and tuples?","In algebra, a vector is an element of a vector space. An example of such an element is a matrix. In linear algebra, a vector is a shorthand name for a $1 \times m$ or a $ n \times 1 $ matrix. (Whereas a matrix itself is also a vector, by definition, but rarely referred to as such.) In (analytic) geometry, a (euclidean) vector is a geometric object with a magnitude and direction. These can be represented by tuples . Both matrices and tuples are clearly vector elements in some spaces, but are for some reason not called vectors by name, unlike euclidean vectors and row vectors . Are there any other examples of vectors that are not called vectors, like matrices and tuples?",,"['linear-algebra', 'definition']"
20,Is $A + A^{-1}$ always invertible?,Is  always invertible?,A + A^{-1},"Let $A$ be an invertible matrix. Then is $A + A^{-1}$ invertible for any $A$? I have a hunch that it's false, but can't really find a way to prove it. If you give a counterexample, could you please explain how you arrived at the counterexample? Thanks. This isn't HW, and I don't really have any work to show.","Let $A$ be an invertible matrix. Then is $A + A^{-1}$ invertible for any $A$? I have a hunch that it's false, but can't really find a way to prove it. If you give a counterexample, could you please explain how you arrived at the counterexample? Thanks. This isn't HW, and I don't really have any work to show.",,"['linear-algebra', 'matrices', 'inverse']"
21,Why represent a complex number $a+ib$ as $[\begin{smallmatrix}a & -b\\ b & \hphantom{-}a\end{smallmatrix}]$? [duplicate],Why represent a complex number  as ? [duplicate],a+ib [\begin{smallmatrix}a & -b\\ b & \hphantom{-}a\end{smallmatrix}],"This question already has answers here : Why is the complex number $z=a+bi$ equivalent to the matrix form $\left(\begin{smallmatrix}a &-b\\b&a\end{smallmatrix}\right)$ [duplicate] (8 answers) Closed 9 years ago . I am reading through John Stillwell's Naive Lie Algebra and it is claimed that all complex numbers can be represented by a $2\times 2$ matrix $\begin{bmatrix}a & -b\\ b & \hphantom{-}a\end{bmatrix}$. But obviously $a+ib$ is quite different from $\begin{bmatrix}a & -b\\ b & \hphantom{-}a\end{bmatrix}$, as the latter being quite clumsy to use and seldom seen in any applications I am aware of. Furthermore, it complicates simple operations such as matrix multiplication whereby you have to go one extra step and extract the complex number after doing the multiplication. Can someone explain what exactly is the difference (if there is any) between the two different representations? In what instances is a matrix representation advantageous?","This question already has answers here : Why is the complex number $z=a+bi$ equivalent to the matrix form $\left(\begin{smallmatrix}a &-b\\b&a\end{smallmatrix}\right)$ [duplicate] (8 answers) Closed 9 years ago . I am reading through John Stillwell's Naive Lie Algebra and it is claimed that all complex numbers can be represented by a $2\times 2$ matrix $\begin{bmatrix}a & -b\\ b & \hphantom{-}a\end{bmatrix}$. But obviously $a+ib$ is quite different from $\begin{bmatrix}a & -b\\ b & \hphantom{-}a\end{bmatrix}$, as the latter being quite clumsy to use and seldom seen in any applications I am aware of. Furthermore, it complicates simple operations such as matrix multiplication whereby you have to go one extra step and extract the complex number after doing the multiplication. Can someone explain what exactly is the difference (if there is any) between the two different representations? In what instances is a matrix representation advantageous?",,"['linear-algebra', 'matrices', 'complex-numbers', 'lie-groups']"
22,Why is the tensor product constructed in this way?,Why is the tensor product constructed in this way?,,"I've already asked about the definition of tensor product here and now I understand the steps of the construction. I'm just in doubt about the motivation to construct it in that way. Well, if all that we want is to have tuples of vectors that behave linearly on addition and multiplication by scalar, couldn't we just take all vector spaces $L_1, L_2,\dots,L_p$, form their cartesian product $L_1\times L_2\times \cdots \times L_p$ and simply introduce operations analogous to that of $\mathbb{R}^n$ ? We would get a space of tuples of vectors on wich all those linear properties are obeyed. What's the reason/motivation to define the tensor product using the free vector space and that quotient to impose linearity ? Can someone point me out the motivation for that definition ? Thanks very much in advance.","I've already asked about the definition of tensor product here and now I understand the steps of the construction. I'm just in doubt about the motivation to construct it in that way. Well, if all that we want is to have tuples of vectors that behave linearly on addition and multiplication by scalar, couldn't we just take all vector spaces $L_1, L_2,\dots,L_p$, form their cartesian product $L_1\times L_2\times \cdots \times L_p$ and simply introduce operations analogous to that of $\mathbb{R}^n$ ? We would get a space of tuples of vectors on wich all those linear properties are obeyed. What's the reason/motivation to define the tensor product using the free vector space and that quotient to impose linearity ? Can someone point me out the motivation for that definition ? Thanks very much in advance.",,"['linear-algebra', 'abstract-algebra', 'tensor-products', 'multilinear-algebra', 'motivation']"
23,Why is the determinant zero iff the column vectors are linearly dependent?,Why is the determinant zero iff the column vectors are linearly dependent?,,"The determinant of a square matrix is zero if and only if the column vectors are linearly dependent. I see a lot of references to this all over the web, but I can't find an actual explanation for this anywhere.","The determinant of a square matrix is zero if and only if the column vectors are linearly dependent. I see a lot of references to this all over the web, but I can't find an actual explanation for this anywhere.",,"['linear-algebra', 'matrices', 'determinant']"
24,Do row operations change the column space of a matrix?,Do row operations change the column space of a matrix?,,"I know that (i) row operations do not change the row space (ii) column operations do not change the column space and (iii) row rank = column rank (but this is sort of unrelated, I think). But, is it true that row operations do not change both the row space and the column space of a matrix? Thanks, EDIT: I am guessing that it's most likely true, since in Guassian elimination, solving Ax=b involves only row operations -- there's something about column operations that makes the algorithm not work, I think (according to the book by Friedberg, Insel and Spence.)","I know that (i) row operations do not change the row space (ii) column operations do not change the column space and (iii) row rank = column rank (but this is sort of unrelated, I think). But, is it true that row operations do not change both the row space and the column space of a matrix? Thanks, EDIT: I am guessing that it's most likely true, since in Guassian elimination, solving Ax=b involves only row operations -- there's something about column operations that makes the algorithm not work, I think (according to the book by Friedberg, Insel and Spence.)",,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-equations']"
25,Expressing the trace of $A^2$ by trace of $A$,Expressing the trace of  by trace of,A^2 A,Let $A$ be a a square matrix. Is it possible  to express $\operatorname{trace}(A^2)$ by means of $\operatorname{trace}(A)$ ? or at least something close?,Let $A$ be a a square matrix. Is it possible  to express $\operatorname{trace}(A^2)$ by means of $\operatorname{trace}(A)$ ? or at least something close?,,"['linear-algebra', 'matrices', 'trace']"
26,How are eigenvectors/eigenvalues and differential equations connected?,How are eigenvectors/eigenvalues and differential equations connected?,,"In school and at university we never had eigenvalues nor differential equations, so these concepts were really giving me a hard time. Now I developed some intuition for both concepts. I learned that both are connected in some way, i.e. there is an eigenvector/eigenvalue approach  to solve differential equations. Unfortunately most of the text I find are way beyond my understanding and seem to require very extensive study. Is there a possibility to provide me with an intuition of the connection anyway and perhaps give an easy example of how to use this approach to solve a differential equation?","In school and at university we never had eigenvalues nor differential equations, so these concepts were really giving me a hard time. Now I developed some intuition for both concepts. I learned that both are connected in some way, i.e. there is an eigenvector/eigenvalue approach  to solve differential equations. Unfortunately most of the text I find are way beyond my understanding and seem to require very extensive study. Is there a possibility to provide me with an intuition of the connection anyway and perhaps give an easy example of how to use this approach to solve a differential equation?",,"['linear-algebra', 'intuition', 'ordinary-differential-equations', 'examples-counterexamples', 'eigenvalues-eigenvectors']"
27,Vector Spaces: Understanding the basics,Vector Spaces: Understanding the basics,,"I'm studying for the Math GRE subject test and I'm currently going over Linear Algebra. I'm carefully re-reading my course book ""Linear Algebra Done Right"" By Sheldon Axler (Third Edition). I have a few questions in regards to the fundamental concepts: Going over the definition of a Vector Space , it doesn't seem obvious to me why we need the scalars , adjoined to the vector set , to be members of a Field . If we loosen this definition of a Vector Space a little bit I think we can get algebraic structures that are analogous to that of a Vector Space and thus meaningful. Hence my first question: Is the Real Number Line a Vector Space? . If so, then this would imply that numbers themselves can be interpreted as vectors. Then, in trying to consider ""small"" Vector Spaces (other than the trivial case of the singleton zero vector set $\{0\}$) I wonder, in this sense, my second question: can there be Vector Spaces contained inside the Real Number Line? It is here where loosening the definition of a vector space can be of merit. If we allow our scalars to be Integers and our vector set to be all the multiples (positive and negative) of a Natural Number then that vector set adjoined with the operation of vector addition and scalar multiplication would manifest all the characteristics that define a Vector Space. Try using the set that contains all the multiples of three - $\{x | x = 3\alpha \text{ where } \alpha \in \mathbb{Z}\}$. Thus why would such a structure not be a vector space. Is it missing something? Or why do we demand that we use scalars from a field? I'm interested in knowing what you think.","I'm studying for the Math GRE subject test and I'm currently going over Linear Algebra. I'm carefully re-reading my course book ""Linear Algebra Done Right"" By Sheldon Axler (Third Edition). I have a few questions in regards to the fundamental concepts: Going over the definition of a Vector Space , it doesn't seem obvious to me why we need the scalars , adjoined to the vector set , to be members of a Field . If we loosen this definition of a Vector Space a little bit I think we can get algebraic structures that are analogous to that of a Vector Space and thus meaningful. Hence my first question: Is the Real Number Line a Vector Space? . If so, then this would imply that numbers themselves can be interpreted as vectors. Then, in trying to consider ""small"" Vector Spaces (other than the trivial case of the singleton zero vector set $\{0\}$) I wonder, in this sense, my second question: can there be Vector Spaces contained inside the Real Number Line? It is here where loosening the definition of a vector space can be of merit. If we allow our scalars to be Integers and our vector set to be all the multiples (positive and negative) of a Natural Number then that vector set adjoined with the operation of vector addition and scalar multiplication would manifest all the characteristics that define a Vector Space. Try using the set that contains all the multiples of three - $\{x | x = 3\alpha \text{ where } \alpha \in \mathbb{Z}\}$. Thus why would such a structure not be a vector space. Is it missing something? Or why do we demand that we use scalars from a field? I'm interested in knowing what you think.",,"['linear-algebra', 'abstract-algebra', 'ring-theory', 'vector-spaces', 'modules']"
28,Why isn't every linear subspace of an infinite-dimensional normed space closed?,Why isn't every linear subspace of an infinite-dimensional normed space closed?,,"Can somebody please point out the error in the following reasoning? I know that there exist non-closed subspaces of infinite-dimensional normed spaces. Let $V$ be a vector space of infinite dimension equipped with a norm and $U \subsetneq V$ a proper subspace. $U$ has a basis $\{\hat{u}_\alpha\}_{\alpha \in A}$ and we can extend this basis with additional vectors $\{\hat{w}_\beta\}_{\beta \in B}$ so that $\{\hat{u}_\alpha\} \cup \{\hat{w}_\beta\}$ is a basis of $V$ . Let's say that all these basis vectors are normed. Now I want to prove that $\partial U = U$ : Let $u$ be a vector in $U$ and $\beta \in B$ arbitrary. For any $\varepsilon > 0$ the point $u + \frac{\varepsilon}{2} \hat{w}_\beta$ lies outside of $U$ , so $u \in \partial U$ . Let $v \in V \setminus U$ , so there exist a $\beta \in B$ and an $a \in \mathbb{R}$ so that $a \,\hat{w}_\beta$ appears in the linear combination that describes $v$ . Now for every $\varepsilon < |a|$ the open ball of radius $\varepsilon$ around $v$ doesn't intersect with $U$ . As a consequence $v \not\in \partial U$ . Since the boundary of a set is always closed, $U$ has to be closed.","Can somebody please point out the error in the following reasoning? I know that there exist non-closed subspaces of infinite-dimensional normed spaces. Let be a vector space of infinite dimension equipped with a norm and a proper subspace. has a basis and we can extend this basis with additional vectors so that is a basis of . Let's say that all these basis vectors are normed. Now I want to prove that : Let be a vector in and arbitrary. For any the point lies outside of , so . Let , so there exist a and an so that appears in the linear combination that describes . Now for every the open ball of radius around doesn't intersect with . As a consequence . Since the boundary of a set is always closed, has to be closed.","V U \subsetneq V U \{\hat{u}_\alpha\}_{\alpha \in A} \{\hat{w}_\beta\}_{\beta \in B} \{\hat{u}_\alpha\} \cup \{\hat{w}_\beta\} V \partial U = U u U \beta \in B \varepsilon > 0 u + \frac{\varepsilon}{2} \hat{w}_\beta U u \in \partial U v \in V \setminus U \beta \in B a \in \mathbb{R} a \,\hat{w}_\beta v \varepsilon < |a| \varepsilon v U v \not\in \partial U U","['linear-algebra', 'functional-analysis', 'vector-spaces']"
29,Can all arbuzoids assume the same color?,Can all arbuzoids assume the same color?,,"This puzzle is from a Russian web-site http://www.arbuz.uz/ and there are many solutions to it, but mine uses linear algebra and is very naive. There’s a planet inhabited by arbuzoids (watermeloners, to translate from Russian). Those creatures are found in three colors: red, green and blue. There are 13 red arbuzoids, 15 blue ones, and 17 green. When two differently colored arbuzoids meet, they both change to the third color. The question is, can it ever happen that all of them assume the same color? Edited: Apr 10, 2021, I just wanted to add that this is quoted from Jim Hefferon, Linear Algebra . I still have no idea how this problem carved its way into my Linear Algebra book, but I gave it a go. Strangely, I didn't realize the relationship this had with Linear Algebra,  so I decided to formalize it. let $S$ be a set such that: $\langle 13, 17, 15\rangle \in S $ $\forall r,g,b,a \in \mathbb N ( \langle r,g,b\rangle\in S \to ($ $[(a \leq r \land a\leq g) \to \langle r-a,g-a,b+2a\rangle \in S] \land$ $\qquad\qquad\qquad\qquad\qquad\qquad\quad\space[(a \leq r \land a\leq b) \to \langle r-a,g+2a,b-a\rangle \in S] \land$ $\qquad\qquad\qquad\qquad\qquad\qquad\quad\space[(a \leq g \land a\leq b) \to \langle r+2a,g-a,b-a\rangle \in S] \space ))$ We have to show that: $\exists a \in \mathbb N(\langle a,0,0 \rangle\in S \lor \langle 0,a,0 \rangle\in S \lor \langle 0,0,a \rangle\in S) \tag{I}$ or.. $\not\exists a \in \mathbb N(\langle a,0,0 \rangle\in S \lor \langle 0,a,0 \rangle\in S \lor \langle 0,0,a \rangle\in S) \tag{II}$ I don't know how this is at all related to Linear Algebra, and I barely know anything about set theory, so bye. What I did I started from the first element in $S$ , as shown. First start with the element. remember, the order is red, green, blue: $\langle 13,17,15 \rangle \in S \tag{0}$ Eliminate the red arbuzoids: $\langle 0,43,2 \rangle \in S \tag{1}$ combine $1$ green with $1$ blue: $\langle 2,42,1 \rangle \in S \tag{2}$ Note that if the blue arbuzoids in $(1)$ were $3$ , the problem would have been solved.Also, If they had been any multiple of 3 less than or equal to green,  the problem would have been solved, since you'll combine 1 third of blue with green, and then red and blue would be equal, then you combine them, and get all green. Anyway,the  blue arbuzoids were not 3. I did many failed steps, noting a few things, and I couldn't get anything. I decided to write some code to test(i.e brute force the combinations and find) whether it's possible, starting from (2), but it failed to reach anything. Can you prove $(I)$ , or $(II)$ ?","This puzzle is from a Russian web-site http://www.arbuz.uz/ and there are many solutions to it, but mine uses linear algebra and is very naive. There’s a planet inhabited by arbuzoids (watermeloners, to translate from Russian). Those creatures are found in three colors: red, green and blue. There are 13 red arbuzoids, 15 blue ones, and 17 green. When two differently colored arbuzoids meet, they both change to the third color. The question is, can it ever happen that all of them assume the same color? Edited: Apr 10, 2021, I just wanted to add that this is quoted from Jim Hefferon, Linear Algebra . I still have no idea how this problem carved its way into my Linear Algebra book, but I gave it a go. Strangely, I didn't realize the relationship this had with Linear Algebra,  so I decided to formalize it. let be a set such that: We have to show that: or.. I don't know how this is at all related to Linear Algebra, and I barely know anything about set theory, so bye. What I did I started from the first element in , as shown. First start with the element. remember, the order is red, green, blue: Eliminate the red arbuzoids: combine green with blue: Note that if the blue arbuzoids in were , the problem would have been solved.Also, If they had been any multiple of 3 less than or equal to green,  the problem would have been solved, since you'll combine 1 third of blue with green, and then red and blue would be equal, then you combine them, and get all green. Anyway,the  blue arbuzoids were not 3. I did many failed steps, noting a few things, and I couldn't get anything. I decided to write some code to test(i.e brute force the combinations and find) whether it's possible, starting from (2), but it failed to reach anything. Can you prove , or ?","S \langle 13, 17, 15\rangle \in S  \forall r,g,b,a \in \mathbb N ( \langle r,g,b\rangle\in S \to ( [(a \leq r \land a\leq g) \to \langle r-a,g-a,b+2a\rangle \in S] \land \qquad\qquad\qquad\qquad\qquad\qquad\quad\space[(a \leq r \land a\leq b) \to \langle r-a,g+2a,b-a\rangle \in S] \land \qquad\qquad\qquad\qquad\qquad\qquad\quad\space[(a \leq g \land a\leq b) \to \langle r+2a,g-a,b-a\rangle \in S] \space )) \exists a \in \mathbb N(\langle a,0,0 \rangle\in S \lor \langle 0,a,0 \rangle\in S \lor \langle 0,0,a \rangle\in S) \tag{I} \not\exists a \in \mathbb N(\langle a,0,0 \rangle\in S \lor \langle 0,a,0 \rangle\in S \lor \langle 0,0,a \rangle\in S) \tag{II} S \langle 13,17,15 \rangle \in S \tag{0} \langle 0,43,2 \rangle \in S \tag{1} 1 1 \langle 2,42,1 \rangle \in S \tag{2} (1) 3 (I) (II)","['linear-algebra', 'elementary-set-theory', 'puzzle']"
30,Norm of a symmetric matrix?,Norm of a symmetric matrix?,,Say I have a symmetric matrix. I have the concept of 2-norm as defined on wikipedia. Now I want to prove (disprove?) that the norm of a symmetric matrix is maximum absolute value of its eigenvalue. I would really appreciate if this can be done only using simple concepts of linear algebra. I am quite new to mathematics.,Say I have a symmetric matrix. I have the concept of 2-norm as defined on wikipedia. Now I want to prove (disprove?) that the norm of a symmetric matrix is maximum absolute value of its eigenvalue. I would really appreciate if this can be done only using simple concepts of linear algebra. I am quite new to mathematics.,,"['linear-algebra', 'normed-spaces']"
31,What does this (double absolute value like) notation mean?,What does this (double absolute value like) notation mean?,,"Here, $$\left\lVert\frac{\partial\bf x}{\partial s}\times\frac{\partial\bf x}{\partial t}\right\rVert$$ the inside will at last be a vector. and two absolute value signs have covered it. what does it mean? Can someone explain it to me? $||\vec a||$","Here, $$\left\lVert\frac{\partial\bf x}{\partial s}\times\frac{\partial\bf x}{\partial t}\right\rVert$$ the inside will at last be a vector. and two absolute value signs have covered it. what does it mean? Can someone explain it to me? $||\vec a||$",,"['linear-algebra', 'notation', 'vectors', 'normed-spaces']"
32,What's the interpretation of a unitary matrix?,What's the interpretation of a unitary matrix?,,"I know that a unitary matrix is a matrix whose inverse equals its conjugate transpose (or that multiplying it by its conjugate transpose yields the identity), but I don't have a deep intuition about it (I just accept the definition). So for example, when I encounter the statement that the left and right singular decompositions $U$ and $V$ in the SVD are unitary, I don't get the significance. I would appreciate if somebody could enlighten me to connect the dots and how to feel when encountering unitary matrices. I have the feeling there is something unwritten that I'm missing.","I know that a unitary matrix is a matrix whose inverse equals its conjugate transpose (or that multiplying it by its conjugate transpose yields the identity), but I don't have a deep intuition about it (I just accept the definition). So for example, when I encounter the statement that the left and right singular decompositions and in the SVD are unitary, I don't get the significance. I would appreciate if somebody could enlighten me to connect the dots and how to feel when encountering unitary matrices. I have the feeling there is something unwritten that I'm missing.",U V,"['linear-algebra', 'matrices', 'unitary-matrices']"
33,Every matrix can be written as a sum of unitary matrices?,Every matrix can be written as a sum of unitary matrices?,,"Any matrix $A \in \mbox{GL}(n, \mathbb{C})$ can be written as a finite linear combination of elements $U_i\in U(n)$ : $$ A = \sum_{i} \lambda_i U_i$$ Is this true? How could I prove it?",Any matrix can be written as a finite linear combination of elements : Is this true? How could I prove it?,"A \in \mbox{GL}(n, \mathbb{C}) U_i\in U(n)  A = \sum_{i} \lambda_i U_i","['linear-algebra', 'matrices', 'unitary-matrices']"
34,Complexity of computing $ABx$ depends on the order of multiplication,Complexity of computing  depends on the order of multiplication,ABx,"To calculate $y = ABx$ where $A$ and $B$ are two $N\times N$ matrices and $x$ is an vector in $N$-dimensional space. there are two methods. One is to first calculate $z=Bx$, and then $y = Az$. This way takes roughly $2N^2$ multiplications. The other is to first calculate $C = AB$, and then $y=Cx$. This way, however, takes $N^3+N^2$ multiplications! Am I correct on this? I do not understand why change of the order of the calculation introduces so much difference in the computation complexity.","To calculate $y = ABx$ where $A$ and $B$ are two $N\times N$ matrices and $x$ is an vector in $N$-dimensional space. there are two methods. One is to first calculate $z=Bx$, and then $y = Az$. This way takes roughly $2N^2$ multiplications. The other is to first calculate $C = AB$, and then $y=Cx$. This way, however, takes $N^3+N^2$ multiplications! Am I correct on this? I do not understand why change of the order of the calculation introduces so much difference in the computation complexity.",,"['linear-algebra', 'matrices', 'computational-complexity']"
35,how to extend a basis,how to extend a basis,,"This is a very elementary question but I can't find the answer in my book at the moment. If I have, for example, two vectors $v_1$ and $v_2$ in $\mathbb R^5$ and knowing that they are linear independent , how can I extend this two vectors to a basis of $\mathbb R^5$. I  know that I just have to add $v_i$ for $3\leq i \leq 5$ such that they are linear independent but how can I do that? Is there an easy algorithm?","This is a very elementary question but I can't find the answer in my book at the moment. If I have, for example, two vectors $v_1$ and $v_2$ in $\mathbb R^5$ and knowing that they are linear independent , how can I extend this two vectors to a basis of $\mathbb R^5$. I  know that I just have to add $v_i$ for $3\leq i \leq 5$ such that they are linear independent but how can I do that? Is there an easy algorithm?",,['linear-algebra']
36,Showing a matrix is not diagonalizable,Showing a matrix is not diagonalizable,,"For eigenvalue 1, the eigenspace I got was $span[ 1,0,0]$, and for eigenvalue 4, the eigenspace I got was $span[ 1,-3,9]$. Do they look right? The reason the matrix is not diagonalizable is because we only have 2 linearly independent eigevectors so we can't span R3 with them, hence we can't create a matrix E with the eigenvectors as its basis. Is that correct, did I word it right?","For eigenvalue 1, the eigenspace I got was $span[ 1,0,0]$, and for eigenvalue 4, the eigenspace I got was $span[ 1,-3,9]$. Do they look right? The reason the matrix is not diagonalizable is because we only have 2 linearly independent eigevectors so we can't span R3 with them, hence we can't create a matrix E with the eigenvectors as its basis. Is that correct, did I word it right?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
37,Relation between trace and rank for projection matrices,Relation between trace and rank for projection matrices,,If $A $ is  an  $n \times n$  matrix  over $\mathbb C$ such that $A^2=A$ then is it true  that  $\operatorname{trace} A = \operatorname{rank} A$?,If $A $ is  an  $n \times n$  matrix  over $\mathbb C$ such that $A^2=A$ then is it true  that  $\operatorname{trace} A = \operatorname{rank} A$?,,"['linear-algebra', 'matrices', 'matrix-rank', 'trace', 'projection-matrices']"
38,Proving that a symmetric matrix is positive definite iff all eigenvalues are positive,Proving that a symmetric matrix is positive definite iff all eigenvalues are positive,,"This has essentially been asked before here but I guess I need 50 reputation to comment. Also, here I have some questions of my own. My Proof outline: (forward direction/Necessary direction): Call the symmetric matrix $A$. Write the quadratic form for $A$ as $x^{t}Ax$, where superscript $t$ denotes transpose. $A$ p.d. (positive definite) implies $x^{t}Ax >0 \ \forall x\neq 0$. if $v$ is an eigenvector of A, then $v^t Av \ =v^t \lambda v \ =\lambda \ >0$ where $\lambda$ is the eigenvalue associated with $v$. $\therefore$ all eigenvalues are positive. Any hints for the reverse direction? Perhaps I need to write $A$ as $PDP^{-1} $ where D is a diagonal matrix of the eigenvalues of A and the columns of P are eigenvectors? Also, a more general question but one that is probably important, is that, since the statement does not assume that A is real (in addition to symmetric), does the possibility of complex entries introduce any complications? Do I need to show that the eigenvalues are real? Thanks.","This has essentially been asked before here but I guess I need 50 reputation to comment. Also, here I have some questions of my own. My Proof outline: (forward direction/Necessary direction): Call the symmetric matrix $A$. Write the quadratic form for $A$ as $x^{t}Ax$, where superscript $t$ denotes transpose. $A$ p.d. (positive definite) implies $x^{t}Ax >0 \ \forall x\neq 0$. if $v$ is an eigenvector of A, then $v^t Av \ =v^t \lambda v \ =\lambda \ >0$ where $\lambda$ is the eigenvalue associated with $v$. $\therefore$ all eigenvalues are positive. Any hints for the reverse direction? Perhaps I need to write $A$ as $PDP^{-1} $ where D is a diagonal matrix of the eigenvalues of A and the columns of P are eigenvectors? Also, a more general question but one that is probably important, is that, since the statement does not assume that A is real (in addition to symmetric), does the possibility of complex entries introduce any complications? Do I need to show that the eigenvalues are real? Thanks.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
39,How to find the absolute value of a vector?,How to find the absolute value of a vector?,,"In my linear algebra course I keep seeing something like this: a = {1, 3, 5} Then in formulas I see this: |a| What does this mean, what is the absolute value of a vector? Wouldn't just be {1,3,5}?","In my linear algebra course I keep seeing something like this: a = {1, 3, 5} Then in formulas I see this: |a| What does this mean, what is the absolute value of a vector? Wouldn't just be {1,3,5}?",,"['linear-algebra', 'vectors']"
40,Additive function $T: \mathbb{R} \rightarrow \mathbb{R}$ that is not linear.,Additive function  that is not linear.,T: \mathbb{R} \rightarrow \mathbb{R},"A function $T:V \rightarrow W$ is additive if $T(x+y) = T(x) + T(y)$ for every $x, y \in V$. Prove that there exists an additive function $T: \mathbb{R} \rightarrow \mathbb{R}$ that is not linear. My attempt: Let $T$ be the function $T: \mathbb{R}$ (over the field $\mathbb{Q}$) $\rightarrow \mathbb{R}$ (over the field $\mathbb{R}$). The set $\{1, \sqrt{2}\} \subseteq \mathbb{R}$ is linearly independent for the vector space $\mathbb{R}$ over $\mathbb{Q}$. Then, there must exist a linearly independent set $W \subseteq \mathbb{R}$ (over $\mathbb{Q}$) such that $\{1, \sqrt{2}\} \subseteq W \subseteq \text{span}(W) = \mathbb{R}$ (over $\mathbb{Q}$). I have been told that the function defined as $T(1) = 1$ and $T(w) = 0$ for all $w \in \text{span}(W) \setminus \{1\}$ is additive but not linear, but I cannot see why this is? I can see why it is not linear, clearly $T(\sqrt{2} \cdot 1) = 0$ but $\sqrt{2} T(1) = \sqrt{2}$. But, why is $T$ additive? For example, $T(1+1) = T(2) =0$ but $T(1) + T(1) = 1+1 = 2$? Is there a mistake somewhere?","A function $T:V \rightarrow W$ is additive if $T(x+y) = T(x) + T(y)$ for every $x, y \in V$. Prove that there exists an additive function $T: \mathbb{R} \rightarrow \mathbb{R}$ that is not linear. My attempt: Let $T$ be the function $T: \mathbb{R}$ (over the field $\mathbb{Q}$) $\rightarrow \mathbb{R}$ (over the field $\mathbb{R}$). The set $\{1, \sqrt{2}\} \subseteq \mathbb{R}$ is linearly independent for the vector space $\mathbb{R}$ over $\mathbb{Q}$. Then, there must exist a linearly independent set $W \subseteq \mathbb{R}$ (over $\mathbb{Q}$) such that $\{1, \sqrt{2}\} \subseteq W \subseteq \text{span}(W) = \mathbb{R}$ (over $\mathbb{Q}$). I have been told that the function defined as $T(1) = 1$ and $T(w) = 0$ for all $w \in \text{span}(W) \setminus \{1\}$ is additive but not linear, but I cannot see why this is? I can see why it is not linear, clearly $T(\sqrt{2} \cdot 1) = 0$ but $\sqrt{2} T(1) = \sqrt{2}$. But, why is $T$ additive? For example, $T(1+1) = T(2) =0$ but $T(1) + T(1) = 1+1 = 2$? Is there a mistake somewhere?",,['linear-algebra']
41,Rotating one 3d-vector to another,Rotating one 3d-vector to another,,"I have written an algorithm for solving the following problem: Given two 3d-vectors, say: $a,b$, find rotation of $a$ so that its orientation matches $b$. However, I am not sure if the following algorithm works in all cases: 1) Find axis and angle using cross product and dot product: $$\mathbf{x}=\frac{a\times b}{||a\times b||}\\ \theta=\cos^{-1}(\frac{a\cdot b}{||a||\cdot ||b||})$$ 3) Find rotation matrix using exponential map: $$\mathbf{R}=e^{\mathbf{A}\theta} =\mathbf{I}+\sin(\theta)\cdot \mathbf{A}+\left(1-\cos(\theta)\right) \cdot \mathbf{A}^{2}$$ where $\mathbf{A}$ is a skew-symmetric matrix corresponding to $\mathbf{x}$: $$\mathbf{A}=[\mathbf{x}]_{\times}=\begin{bmatrix}0 & -\mathbf{x}_{3} & \mathbf{x}_{2} \\ \mathbf{x}_{3} & 0 & -\mathbf{x}_{1} \\ -\mathbf{x}_{2} & \mathbf{x}_{1} & 0\end{bmatrix}$$ Notes: The axis is computed using cross product as this gives vector perpendicular to both $a$ and $b$. Only direction of the axis is important, hence it is divided by its magnitude. However, I am not sure if $\mathbf{x}$ will always have the proper direction (the result can be $-\mathbf{x}$ instead of $\mathbf{x}$?). The rotation matrix is computed using Rodrigues' rotation formula. Finally, the vector $\mathbf{R}a$ should have same direction as $b$. I have tested this numerically and it seems working, but I would like to be sure the formulas work for any two $a,b$.","I have written an algorithm for solving the following problem: Given two 3d-vectors, say: $a,b$, find rotation of $a$ so that its orientation matches $b$. However, I am not sure if the following algorithm works in all cases: 1) Find axis and angle using cross product and dot product: $$\mathbf{x}=\frac{a\times b}{||a\times b||}\\ \theta=\cos^{-1}(\frac{a\cdot b}{||a||\cdot ||b||})$$ 3) Find rotation matrix using exponential map: $$\mathbf{R}=e^{\mathbf{A}\theta} =\mathbf{I}+\sin(\theta)\cdot \mathbf{A}+\left(1-\cos(\theta)\right) \cdot \mathbf{A}^{2}$$ where $\mathbf{A}$ is a skew-symmetric matrix corresponding to $\mathbf{x}$: $$\mathbf{A}=[\mathbf{x}]_{\times}=\begin{bmatrix}0 & -\mathbf{x}_{3} & \mathbf{x}_{2} \\ \mathbf{x}_{3} & 0 & -\mathbf{x}_{1} \\ -\mathbf{x}_{2} & \mathbf{x}_{1} & 0\end{bmatrix}$$ Notes: The axis is computed using cross product as this gives vector perpendicular to both $a$ and $b$. Only direction of the axis is important, hence it is divided by its magnitude. However, I am not sure if $\mathbf{x}$ will always have the proper direction (the result can be $-\mathbf{x}$ instead of $\mathbf{x}$?). The rotation matrix is computed using Rodrigues' rotation formula. Finally, the vector $\mathbf{R}a$ should have same direction as $b$. I have tested this numerically and it seems working, but I would like to be sure the formulas work for any two $a,b$.",,"['linear-algebra', '3d', 'rotations']"
42,Any good Graduate Level linear algebra textbook for practice/problem solving?,Any good Graduate Level linear algebra textbook for practice/problem solving?,,"I am looking for good graduate linear algebra books that contain practice problems with solutions (which is better) or hints to solve the problems. By the way, two graduate courses I am gonna take are a continuation of the undergrad course I have already taken based on the textbook: ""Linear Algebra: A modern Approach"" by ""D.Poole"". I did find some textbooks that cover the material, like: ""Linear Algebra done right"", ""Linear Algebra done wrong"", etc and some of these books suggest few problems without given solutions. So, I need books with many solved problems (or hints) that help me to practice what I will learn in the lecture and to ensure I well understand the material. Any suggestion is more than welcome. Thanks","I am looking for good graduate linear algebra books that contain practice problems with solutions (which is better) or hints to solve the problems. By the way, two graduate courses I am gonna take are a continuation of the undergrad course I have already taken based on the textbook: ""Linear Algebra: A modern Approach"" by ""D.Poole"". I did find some textbooks that cover the material, like: ""Linear Algebra done right"", ""Linear Algebra done wrong"", etc and some of these books suggest few problems without given solutions. So, I need books with many solved problems (or hints) that help me to practice what I will learn in the lecture and to ensure I well understand the material. Any suggestion is more than welcome. Thanks",,"['linear-algebra', 'reference-request', 'book-recommendation']"
43,Non-isomorphic exact sequences with isomorphic terms,Non-isomorphic exact sequences with isomorphic terms,,"I love it when an undergraduate catches me out. I'm lecturing a first course in (not necessarily commutative) rings (with 1) and I've spent the last few weeks doing basic module theory. I defined a short exact sequence of (left) $R$-modules and a homomorphism of short exact sequences (a homomorphism from $0\to A\to B\to C\to 0$ to $0\to A'\to B'\to C'\to 0$ is just $R$-module maps $A\to A'$, $B\to B'$ and $C\to C'$ such that the obvious two squares commute). There's hence an obvious notion of an isomorphism of short exact sequences. Today one of the students asked me if it was possible to have a ring $R$ and modules $A,A',B,B',C,C'$ sitting in two short exact sequences as above, and such that $A$ was isomorphic to $A'$, $B$ was isomorphic to $B'$ and $C$ was isomorphic to $C'$, but that the sequences weren't isomorphic. I said ""sure, I'll email you a counterexample later"" (the logic being that if this were a theorem, it would be one I knew about). I thought I'd knock up a counterexample on the tube home -- but I failed :-/ If $R$ is a field then short exact sequences split (yes we're assuming AC) so that's not going anywhere. So I thought that $R=k[X]$ would be a good place to start, $k$ a field. In this case an $R$-module is just a $k$-vector space equipped with an endomorphism and I figured this would give me enough flexibility. I wanted $A,A',C,C'$ to be $R/(x^2)$ and tried some messy matrix calculations to figure out an example, but I couldn't get it to work. I then went for $R=k[x,y]$ but now a 2-dimensional vector space is an $R$-module when we give it two commuting linear maps and somehow this set-up had too many endomorphisms for me to face doing the algebra. I then figured that I might want to try a polynomial ring in two noncommuting variables--but then it was my stop and it was time to start thinking about other things. I am almost certain that there will even be a counterexample with $R$ commutative (that's why I was thinking about the commutative case). Can anyone tell me the trick I'm missing?","I love it when an undergraduate catches me out. I'm lecturing a first course in (not necessarily commutative) rings (with 1) and I've spent the last few weeks doing basic module theory. I defined a short exact sequence of (left) $R$-modules and a homomorphism of short exact sequences (a homomorphism from $0\to A\to B\to C\to 0$ to $0\to A'\to B'\to C'\to 0$ is just $R$-module maps $A\to A'$, $B\to B'$ and $C\to C'$ such that the obvious two squares commute). There's hence an obvious notion of an isomorphism of short exact sequences. Today one of the students asked me if it was possible to have a ring $R$ and modules $A,A',B,B',C,C'$ sitting in two short exact sequences as above, and such that $A$ was isomorphic to $A'$, $B$ was isomorphic to $B'$ and $C$ was isomorphic to $C'$, but that the sequences weren't isomorphic. I said ""sure, I'll email you a counterexample later"" (the logic being that if this were a theorem, it would be one I knew about). I thought I'd knock up a counterexample on the tube home -- but I failed :-/ If $R$ is a field then short exact sequences split (yes we're assuming AC) so that's not going anywhere. So I thought that $R=k[X]$ would be a good place to start, $k$ a field. In this case an $R$-module is just a $k$-vector space equipped with an endomorphism and I figured this would give me enough flexibility. I wanted $A,A',C,C'$ to be $R/(x^2)$ and tried some messy matrix calculations to figure out an example, but I couldn't get it to work. I then went for $R=k[x,y]$ but now a 2-dimensional vector space is an $R$-module when we give it two commuting linear maps and somehow this set-up had too many endomorphisms for me to face doing the algebra. I then figured that I might want to try a polynomial ring in two noncommuting variables--but then it was my stop and it was time to start thinking about other things. I am almost certain that there will even be a counterexample with $R$ commutative (that's why I was thinking about the commutative case). Can anyone tell me the trick I'm missing?",,"['linear-algebra', 'modules', 'exact-sequence']"
44,How is the determinant related to the inverse of matrix?,How is the determinant related to the inverse of matrix?,,"Whenever I needed to find the inverse of a matrix, I was told to check if its determinant is not zero. However, once I directly applied the Gauss-Jordan's method for finding the inverse of matrix whose determinant was zero. The inverse matrix that I got looked pretty normal like any other (if there wasn't a mistake). I want to know how does the determinant of the matrix is related to inverse of matrix or why is that if determinant is zero then inverse doesn't exist? What exactly is inverse?","Whenever I needed to find the inverse of a matrix, I was told to check if its determinant is not zero. However, once I directly applied the Gauss-Jordan's method for finding the inverse of matrix whose determinant was zero. The inverse matrix that I got looked pretty normal like any other (if there wasn't a mistake). I want to know how does the determinant of the matrix is related to inverse of matrix or why is that if determinant is zero then inverse doesn't exist? What exactly is inverse?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'commutative-algebra', 'inverse']"
45,Integral around unit sphere of inner product,Integral around unit sphere of inner product,,"For arbitrary $n\times n$ matrices M, I am trying to solve the integral $$\int_{\|v\| = 1} v^T M v.$$ Solving this integral in a few low dimensions (by passing to spherical coordinates) suggests the answer in general to be  $$\frac{A\,\mathrm{tr}(M)}{n}$$ where $A$ is the surface area of the $(n-1)$-dimensional sphere. Is there a nice, coordinate-free approach to proving this formula?","For arbitrary $n\times n$ matrices M, I am trying to solve the integral $$\int_{\|v\| = 1} v^T M v.$$ Solving this integral in a few low dimensions (by passing to spherical coordinates) suggests the answer in general to be  $$\frac{A\,\mathrm{tr}(M)}{n}$$ where $A$ is the surface area of the $(n-1)$-dimensional sphere. Is there a nice, coordinate-free approach to proving this formula?",,['linear-algebra']
46,Proof that columns of an invertible matrix are linearly independent,Proof that columns of an invertible matrix are linearly independent,,"Explain why   the columns of  an $n \times n$ matrix $A$ are linearly   independent when $A$ is  invertible. The proof that I thought of was: If $A$ is invertible, then $A \sim I$ ($A$ is row equivalent to the identity matrix). Therefore, $A$ has $n$ pivots, one in each column, which means that the columns of $A$ are linearly independent. The proof that was provided was: Suppose $A$ is invertible. Therefore the equation $Ax = 0$ has only one solution, namely, the zero solution. This means that the columns of $A$ are linearly independent. I am not sure whether or not my proof is correct. If it is, would there be a reason to prefer one proof over the other? As seen in the Wikipedia article and in Linear Algebra and Its Applications , $\sim$ indicates row equivalence between matrices.","Explain why   the columns of  an $n \times n$ matrix $A$ are linearly   independent when $A$ is  invertible. The proof that I thought of was: If $A$ is invertible, then $A \sim I$ ($A$ is row equivalent to the identity matrix). Therefore, $A$ has $n$ pivots, one in each column, which means that the columns of $A$ are linearly independent. The proof that was provided was: Suppose $A$ is invertible. Therefore the equation $Ax = 0$ has only one solution, namely, the zero solution. This means that the columns of $A$ are linearly independent. I am not sure whether or not my proof is correct. If it is, would there be a reason to prefer one proof over the other? As seen in the Wikipedia article and in Linear Algebra and Its Applications , $\sim$ indicates row equivalence between matrices.",,"['linear-algebra', 'matrices', 'proof-verification']"
47,Why is the 'change-of-basis matrix' called such?,Why is the 'change-of-basis matrix' called such?,,"""Let $P$ be the change-of-basis matrix   from a basis $S$ to a basis $S'$ in a   vector space $V$. Then, for any vector   $v \in V$, we have $$P[v]_{S'}=[v]_{S}  \text{ and hence, }  P^{-1}[v]_{S} =  [v]_{S'}$$ Namely, if we multiply the coordinates   of $v$ in the original basis $S$ by   $P^{-1}$, we get the coordinates of   $v$ in the new basis $S'$."" - Schaum's   Outlines: Linear Algebra. 4th Ed. I am having a lot of difficulty keeping these matrices straight. Could someone please help me understand the reasoning behind (what appears to me as) the counter-intuitive naming of $P$ as the change of basis matrix from $S$ to $S'$? It seems like $P^{-1}$ is the matrix which actually changes a coordinate vector in terms of the 'old' basis $S$ to a coordinate vector in terms of the 'new' basis $S'$... Added: ""Consider a basis $S =  \{u_1,u_2,...,u_n\}$ of a vector space   $V$ over a field $K$. For any vector   $v\in V$, suppose $v = a_1u_1  +a_2u_2+...+a_nu_n$ Then the coordinate vector of $v$   relative to the basis $S$, which we   assume to be a column vector (unless   otherwise stated or implied), is   denoted and defined by $[v]_S =  [a_1,a_2,...,a_n]^{T}$. "" ""Let $S = \{ u_1,u_2,...,u_n\}$ be a   basis of a vector space $V$, and let   $S'=\{v_1,v_2,...,v_n\}$ be another   basis. (For reference, we will call   $S$ the 'old' basis and $S'$ the 'new'   basis.) Because $S$ is a basis, each   vector in the 'new' basis $S'$ can be   written uniquely as a linear   combination of the vectors in S; say, $\begin{array}{c} v_1 = a_{11}u_1 +  a_{12}u_2 + \cdots +a_{1n}u_n \\ v_2 =  a_{21}u_1 + a_{22}u_2 + \cdots  +a_{2n}u_n \\ \cdots \cdots \cdots \\ v_n = a_{n1}u_1 + a_{n2}u_2 + \cdots  +a_{nn}u_n \end{array}$ Let $P$ be the transpose of the above   matrix of coefficients; that is, let   $P = [p_{ij}]$, where $p_{ij} =  a_{ij}$. Then $P$ is called the   \textit{change-of-basis matrix} from   the 'old' basis $S$ to the 'new' basis   $S'$."" - Schaum's Outline: Linear Algebra 4th Ed. I am trying to understand the above definitions with this example: Basis vectors of $\mathbb{R}^{2}: S= \{u_1,u_2\}=\{(1,-2),(3,-4)\}$ and $S' = \{v_1,v_2\}= \{(1,3), (3,8)\}$ the change of basis matrix from $S$ to $S'$ is $P = \left( \begin{array}{cc} -\frac{13}{2} & -18 \\ \frac{5}{2} & 7 \end{array} \right)$. My current understanding is the following: normally vectors such as $u_1, u_2$ are written under the assumption of the usual basis that is $u_1 = (1,-2) = e_1 - 2e_2 = [u_1]_E$. So actually $[u_1]_S = (1,0)$ and I guess this would be true in general... But I am not really understanding what effect if any $P$ is supposed to have on the basis vectors themselves (I think I understand the effect on the coordinates relative to a basis). I guess I could calculate a matrix $P'$ which has the effect  $P'u_1, P'u_2,...,P'u_n = v_1, v_2,..., v_n$ but would this be anything?","""Let $P$ be the change-of-basis matrix   from a basis $S$ to a basis $S'$ in a   vector space $V$. Then, for any vector   $v \in V$, we have $$P[v]_{S'}=[v]_{S}  \text{ and hence, }  P^{-1}[v]_{S} =  [v]_{S'}$$ Namely, if we multiply the coordinates   of $v$ in the original basis $S$ by   $P^{-1}$, we get the coordinates of   $v$ in the new basis $S'$."" - Schaum's   Outlines: Linear Algebra. 4th Ed. I am having a lot of difficulty keeping these matrices straight. Could someone please help me understand the reasoning behind (what appears to me as) the counter-intuitive naming of $P$ as the change of basis matrix from $S$ to $S'$? It seems like $P^{-1}$ is the matrix which actually changes a coordinate vector in terms of the 'old' basis $S$ to a coordinate vector in terms of the 'new' basis $S'$... Added: ""Consider a basis $S =  \{u_1,u_2,...,u_n\}$ of a vector space   $V$ over a field $K$. For any vector   $v\in V$, suppose $v = a_1u_1  +a_2u_2+...+a_nu_n$ Then the coordinate vector of $v$   relative to the basis $S$, which we   assume to be a column vector (unless   otherwise stated or implied), is   denoted and defined by $[v]_S =  [a_1,a_2,...,a_n]^{T}$. "" ""Let $S = \{ u_1,u_2,...,u_n\}$ be a   basis of a vector space $V$, and let   $S'=\{v_1,v_2,...,v_n\}$ be another   basis. (For reference, we will call   $S$ the 'old' basis and $S'$ the 'new'   basis.) Because $S$ is a basis, each   vector in the 'new' basis $S'$ can be   written uniquely as a linear   combination of the vectors in S; say, $\begin{array}{c} v_1 = a_{11}u_1 +  a_{12}u_2 + \cdots +a_{1n}u_n \\ v_2 =  a_{21}u_1 + a_{22}u_2 + \cdots  +a_{2n}u_n \\ \cdots \cdots \cdots \\ v_n = a_{n1}u_1 + a_{n2}u_2 + \cdots  +a_{nn}u_n \end{array}$ Let $P$ be the transpose of the above   matrix of coefficients; that is, let   $P = [p_{ij}]$, where $p_{ij} =  a_{ij}$. Then $P$ is called the   \textit{change-of-basis matrix} from   the 'old' basis $S$ to the 'new' basis   $S'$."" - Schaum's Outline: Linear Algebra 4th Ed. I am trying to understand the above definitions with this example: Basis vectors of $\mathbb{R}^{2}: S= \{u_1,u_2\}=\{(1,-2),(3,-4)\}$ and $S' = \{v_1,v_2\}= \{(1,3), (3,8)\}$ the change of basis matrix from $S$ to $S'$ is $P = \left( \begin{array}{cc} -\frac{13}{2} & -18 \\ \frac{5}{2} & 7 \end{array} \right)$. My current understanding is the following: normally vectors such as $u_1, u_2$ are written under the assumption of the usual basis that is $u_1 = (1,-2) = e_1 - 2e_2 = [u_1]_E$. So actually $[u_1]_S = (1,0)$ and I guess this would be true in general... But I am not really understanding what effect if any $P$ is supposed to have on the basis vectors themselves (I think I understand the effect on the coordinates relative to a basis). I guess I could calculate a matrix $P'$ which has the effect  $P'u_1, P'u_2,...,P'u_n = v_1, v_2,..., v_n$ but would this be anything?",,"['linear-algebra', 'matrices', 'change-of-basis']"
48,Why is PageRank an eigenvector problem?,Why is PageRank an eigenvector problem?,,"You hear all the time that PageRank uses eigenvectors. But for those who don't really understand what eigenvectors are, it is unclear why Pagerank needed to invoke eigenvectors and eigenvalues in order to make Google work. Can someone explain why is PageRank an eigenvector problem?","You hear all the time that PageRank uses eigenvectors. But for those who don't really understand what eigenvectors are, it is unclear why Pagerank needed to invoke eigenvectors and eigenvalues in order to make Google work. Can someone explain why is PageRank an eigenvector problem?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'markov-chains']"
49,LU decomposition steps,LU decomposition steps,,"I've been looking at some LU Decomposition problems and I understand that making a matrix A reduced to the form A=LU , where L is a lower triangular matrix and U is a upper triangular matrix, however I am having trouble understanding the steps to get to these matrices. Could someone please explain the method for LU Decomposition in detail, preferably excluding the concept of permutation matrices? ( We haven't talked about permutation matrices in class yet so our professor forbids us to use them for the decomposition).","I've been looking at some LU Decomposition problems and I understand that making a matrix A reduced to the form A=LU , where L is a lower triangular matrix and U is a upper triangular matrix, however I am having trouble understanding the steps to get to these matrices. Could someone please explain the method for LU Decomposition in detail, preferably excluding the concept of permutation matrices? ( We haven't talked about permutation matrices in class yet so our professor forbids us to use them for the decomposition).",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'lu-decomposition']"
50,Dimensions of vector spaces in an exact sequence,Dimensions of vector spaces in an exact sequence,,"I've read the following formula in wikipedia : Given finite dimensional vector spaces $V_i$ and an exact sequence $\cdots\rightarrow V_i\rightarrow V_{i+1}\rightarrow\cdots$, we have $$ \sum_{n\in 2\mathbb{Z}}\dim V_n = \sum_{n\in 2\mathbb{Z}+1}\dim V_n $$ Is there a name for this theorem? Could anyone please tell me where to find a proof of this in the literature? Thank you!","I've read the following formula in wikipedia : Given finite dimensional vector spaces $V_i$ and an exact sequence $\cdots\rightarrow V_i\rightarrow V_{i+1}\rightarrow\cdots$, we have $$ \sum_{n\in 2\mathbb{Z}}\dim V_n = \sum_{n\in 2\mathbb{Z}+1}\dim V_n $$ Is there a name for this theorem? Could anyone please tell me where to find a proof of this in the literature? Thank you!",,"['linear-algebra', 'homological-algebra']"
51,How can LU factorization be used in non-square matrix?,How can LU factorization be used in non-square matrix?,,"In my textbook, there is some information about LU factorization of square matrix $A$, but not about non-square matrix. How can LU factorization be used to factorize non-square matrix?","In my textbook, there is some information about LU factorization of square matrix $A$, but not about non-square matrix. How can LU factorization be used to factorize non-square matrix?",,['linear-algebra']
52,"How many $n\times m$ binary matrices are there, up to row and column permutations?","How many  binary matrices are there, up to row and column permutations?",n\times m,"I'm interested in the number of binary matrices of a given size that are distinct with regard to row and column permutations. If $\sim$ is the equivalence relation on $n\times m$ binary matrices such that $A \sim B$ iff one can obtain B from applying a permutation matrix to A, I'm interested in the number of $\sim$-equivalence classes over all $n\times m$ binary matrices. I know there are $2^{nm}$ binary matrices of size $n\times m$, and $n!m!$ possible permutations, but somehow I fail to get an intuition on what this implies for the equivalence classes.","I'm interested in the number of binary matrices of a given size that are distinct with regard to row and column permutations. If $\sim$ is the equivalence relation on $n\times m$ binary matrices such that $A \sim B$ iff one can obtain B from applying a permutation matrix to A, I'm interested in the number of $\sim$-equivalence classes over all $n\times m$ binary matrices. I know there are $2^{nm}$ binary matrices of size $n\times m$, and $n!m!$ possible permutations, but somehow I fail to get an intuition on what this implies for the equivalence classes.",,"['linear-algebra', 'combinatorics', 'matrices', 'permutations']"
53,What does it mean if $\det(A)$ equals $1$?,What does it mean if  equals ?,\det(A) 1,What does it mean if $\det(A)$ equals $1$? Does it mean that the identity matrix can be obtained from $A$ by only adding multiples of rows onto others?,What does it mean if $\det(A)$ equals $1$? Does it mean that the identity matrix can be obtained from $A$ by only adding multiples of rows onto others?,,"['linear-algebra', 'matrices', 'determinant']"
54,Prove that T is diagonalizable if and only if the minimal polynomial of T has no repeated roots.,Prove that T is diagonalizable if and only if the minimal polynomial of T has no repeated roots.,,"Prove that T is diagonalizable if and only if the minimal polynomial of T has no repeated roots. EDIT: ( Over $\Bbb C $ ) though it is obvious i am working over $\Bbb C $ as one of my statements is not true over $ \Bbb R $ I would like a better proof of this result what i did is below, there is the same question on here somewhere but only has an answer to one direction im looking for both. I proved the result by using a different equivalent statement to diagonalizable looking for any complete proof that is shorter. We notice that it is equivalent to prove that V has a basis consisting of eigenvectors of T iff the minimal polynomial of T has no repeated roots by theorem. $(\Rightarrow ) $ First suppose that there is a basis $\beta = (v_1,\cdots , v_n ) $ consisting of eigenvectors of T. let $\lambda_1 , \cdots , \lambda_m $ be distinct eigenvalues of T. Then for each $ v_i $ there exists a $\lambda_k $ with $(T- \lambda_k I) v_i =0 $ it then follows that $(T- \lambda_1 I) \cdots (T- \lambda_mI) v_i =0 $ for each i as we can commute the operators. Since an operator that sends each vector in a basis to the $0$ vector is the $0$ operator we have that $(T- \lambda_1 I) \cdots (T- \lambda_mI) =0 $ Thus the polynomial $(z-\lambda_1) \cdots (z-\lambda_m ) $ when applied to T gives 0. but by theorem we know that that the minimal polynomial of T is a divisor of $(z-\lambda_1) \cdots (z-\lambda_m ) $ which has no repeated roots so the minimal polynomial cannot possibly have repeated roots the result follows. $(\Leftarrow ) $ Let us assume that the minimal polynomial has no repeated roots; if we let $ \lambda_1 \cdots \lambda_m $ denote the distinct eigenvalues of T, this means the minimal polynomial of T is $(z-\lambda_1) \cdots (z-\lambda_m ) $ It follows that $(T- \lambda_1 I) \cdots (T- \lambda_mI) =0 $ Let $U_m $ be the subspace of a generalized eigenvectors corresponding to the eigenvalue $\lambda_m $. Since $ U_m $ is invariant under T by theorem we consider $ v\in U_m $ let $u= (T- \lambda_m I) v $ it follows that $u\in U_m $  Hence $$ (T|_{U_m} - \lambda_1 I ) \cdots (T|_{U_m} - \lambda_{m-1}I) u =   (T- \lambda_1 I) \cdots (T- \lambda_mI) v =0 $$ by theorem we have that $( T- \lambda_m I )|_{U_m} $ is nilpotent by previous question we have that 0 is the only eigenvalue of $( T- \lambda_m I )|_{U_m} $. Thus $T|_{U_m} - \lambda_jI $ is an invertable operator on $U_m $ for $j= 1, \cdots , m-1 $ it then follows by $$ (T|_{U_m} - \lambda_1 I ) \cdots (T|_{U_m} - \lambda_{m-1}I) u =   (T- \lambda_1 I) \cdots (T- \lambda_mI) v =0 $$ that $u=0$ in other words, $v$ is an eigenvector of T! We have shown that every generalized eigenvector of T corresponding to the eigenvalue $\lambda_m $ is an eigenvector of T. However we choose $ \lambda_m $ arbitrarily we could of just of easily relabeled the eigenvalues so that any of them was called $ \lambda_m $. Therefore we have that every generalized eigenvector of T is actually an eigenvector of T. By theorem we have that there is a basis for V consisting of generalized eigenvectors of T but by above we have that there is a basis of V consisting of eigenvectors of T the desired result.","Prove that T is diagonalizable if and only if the minimal polynomial of T has no repeated roots. EDIT: ( Over $\Bbb C $ ) though it is obvious i am working over $\Bbb C $ as one of my statements is not true over $ \Bbb R $ I would like a better proof of this result what i did is below, there is the same question on here somewhere but only has an answer to one direction im looking for both. I proved the result by using a different equivalent statement to diagonalizable looking for any complete proof that is shorter. We notice that it is equivalent to prove that V has a basis consisting of eigenvectors of T iff the minimal polynomial of T has no repeated roots by theorem. $(\Rightarrow ) $ First suppose that there is a basis $\beta = (v_1,\cdots , v_n ) $ consisting of eigenvectors of T. let $\lambda_1 , \cdots , \lambda_m $ be distinct eigenvalues of T. Then for each $ v_i $ there exists a $\lambda_k $ with $(T- \lambda_k I) v_i =0 $ it then follows that $(T- \lambda_1 I) \cdots (T- \lambda_mI) v_i =0 $ for each i as we can commute the operators. Since an operator that sends each vector in a basis to the $0$ vector is the $0$ operator we have that $(T- \lambda_1 I) \cdots (T- \lambda_mI) =0 $ Thus the polynomial $(z-\lambda_1) \cdots (z-\lambda_m ) $ when applied to T gives 0. but by theorem we know that that the minimal polynomial of T is a divisor of $(z-\lambda_1) \cdots (z-\lambda_m ) $ which has no repeated roots so the minimal polynomial cannot possibly have repeated roots the result follows. $(\Leftarrow ) $ Let us assume that the minimal polynomial has no repeated roots; if we let $ \lambda_1 \cdots \lambda_m $ denote the distinct eigenvalues of T, this means the minimal polynomial of T is $(z-\lambda_1) \cdots (z-\lambda_m ) $ It follows that $(T- \lambda_1 I) \cdots (T- \lambda_mI) =0 $ Let $U_m $ be the subspace of a generalized eigenvectors corresponding to the eigenvalue $\lambda_m $. Since $ U_m $ is invariant under T by theorem we consider $ v\in U_m $ let $u= (T- \lambda_m I) v $ it follows that $u\in U_m $  Hence $$ (T|_{U_m} - \lambda_1 I ) \cdots (T|_{U_m} - \lambda_{m-1}I) u =   (T- \lambda_1 I) \cdots (T- \lambda_mI) v =0 $$ by theorem we have that $( T- \lambda_m I )|_{U_m} $ is nilpotent by previous question we have that 0 is the only eigenvalue of $( T- \lambda_m I )|_{U_m} $. Thus $T|_{U_m} - \lambda_jI $ is an invertable operator on $U_m $ for $j= 1, \cdots , m-1 $ it then follows by $$ (T|_{U_m} - \lambda_1 I ) \cdots (T|_{U_m} - \lambda_{m-1}I) u =   (T- \lambda_1 I) \cdots (T- \lambda_mI) v =0 $$ that $u=0$ in other words, $v$ is an eigenvector of T! We have shown that every generalized eigenvector of T corresponding to the eigenvalue $\lambda_m $ is an eigenvector of T. However we choose $ \lambda_m $ arbitrarily we could of just of easily relabeled the eigenvalues so that any of them was called $ \lambda_m $. Therefore we have that every generalized eigenvector of T is actually an eigenvector of T. By theorem we have that there is a basis for V consisting of generalized eigenvectors of T but by above we have that there is a basis of V consisting of eigenvectors of T the desired result.",,['linear-algebra']
55,Cross product and pseudovector confusion.,Cross product and pseudovector confusion.,,"So called pseudovectors pop up in physics when discussing quantities defined by cross products, such as angular momentum $\mathbf L=\mathbf r\times\mathbf p$. Under the active transformation $\mathbf x \mapsto \mathbf{-x}$, we claim that such a vector gets mapped to itself because $\mathbf{-r} \times \mathbf{-p} = \mathbf r\times\mathbf p$. (Or under the equivalent passive transformation, a pseudovector turns into to its negative.) But it seems like we're just pretending that a linear transformation $T$ preserve cross products, so that $T(\mathbf a \times \mathbf b) = T(\mathbf a) \times T(\mathbf b)$, and then when things don't go as expected we label the result as a pseudovector. Is there more to the story?","So called pseudovectors pop up in physics when discussing quantities defined by cross products, such as angular momentum $\mathbf L=\mathbf r\times\mathbf p$. Under the active transformation $\mathbf x \mapsto \mathbf{-x}$, we claim that such a vector gets mapped to itself because $\mathbf{-r} \times \mathbf{-p} = \mathbf r\times\mathbf p$. (Or under the equivalent passive transformation, a pseudovector turns into to its negative.) But it seems like we're just pretending that a linear transformation $T$ preserve cross products, so that $T(\mathbf a \times \mathbf b) = T(\mathbf a) \times T(\mathbf b)$, and then when things don't go as expected we label the result as a pseudovector. Is there more to the story?",,"['linear-algebra', 'physics']"
56,"Hamel basis for $\mathbb{R}$ over $\mathbb{Q}$ cannot be closed under scalar multiplication by $a \ne 0,1$",Hamel basis for  over  cannot be closed under scalar multiplication by,"\mathbb{R} \mathbb{Q} a \ne 0,1","The following is the problem 206 from Golan's book Linear Algebra a Beginning Graduate Student Ought to Know . I've been unable to make any progress. Definition: A Hamel basis is a (necessarily infinite dimensional) basis for $\mathbb{R}$ as a vector space over $\mathbb{Q}$. Problem: Let $B$ be a Hamel basis for $\mathbb{R}$ as a vector space over $\mathbb{Q}$ and fix some element $a\in\mathbb{R}$ with $a\neq 0,1$. Show there exists some $y\in B$ with $ay\notin B$.","The following is the problem 206 from Golan's book Linear Algebra a Beginning Graduate Student Ought to Know . I've been unable to make any progress. Definition: A Hamel basis is a (necessarily infinite dimensional) basis for $\mathbb{R}$ as a vector space over $\mathbb{Q}$. Problem: Let $B$ be a Hamel basis for $\mathbb{R}$ as a vector space over $\mathbb{Q}$ and fix some element $a\in\mathbb{R}$ with $a\neq 0,1$. Show there exists some $y\in B$ with $ay\notin B$.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'extension-field']"
57,Positive definite matrix must be Hermitian,Positive definite matrix must be Hermitian,,"Is there a simple way to show that a positive definite matrix must be Hermitian? I feel there is a long drawn out proof of this to be had by taking unit vectors and applying the positive definiteness property, and brute forcing it. But is there some simple clever proof why a positive definite matrix is necessarily Hermitian?","Is there a simple way to show that a positive definite matrix must be Hermitian? I feel there is a long drawn out proof of this to be had by taking unit vectors and applying the positive definiteness property, and brute forcing it. But is there some simple clever proof why a positive definite matrix is necessarily Hermitian?",,"['linear-algebra', 'matrices', 'positive-definite', 'hermitian-matrices']"
58,"The matrix logarithm is well-defined - but how can we *algebraically* see that it is inverse to the exponential, as a finite polynomial?","The matrix logarithm is well-defined - but how can we *algebraically* see that it is inverse to the exponential, as a finite polynomial?",,"This question is inspired by this which I saw earlier today. I started writing my answer, to share the insight that the matrix logarithm can be defined on matrices that do not have unit norm using an alternative technique. Now, Sangchul has posted a great answer explaining how it is that we know the map $X\mapsto\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n}X^n$ defines a logarithm of $1+X$ whenever the sum is convergent, whenever $\|X\|\lt1$ . By scaling, we can use this map to obtain arbitrary logarithms since there is an easy definition $\log(\lambda I)=(\log\lambda)I$ which satisfies $\exp(\log(\lambda I))=\lambda I$ trivially, and this matrix will also commute with all other matrices. I prefer an approach that sidesteps the functional calculus entirely, that I first learnt on Wikipedia over a year ago. The argument proceeds like this, paraphrased by me: For any invertible $n\times n$ complex matrix $X$ , there is a basis of the space $\Bbb C^n$ in which $X=\bigoplus_{m=1}^kJ_m$ a decomposition into Jordan blocks with some associated eigenvalues $\lambda_m$ . If we can find a matrix $Y=\bigoplus_{m=1}^nT_m$ , where $\exp(T_m)=J_m$ for all $m$ , then a simple inspection of the exponential series shows $\exp(Y)=\bigoplus_{m=1}^n\exp(T_m)=\bigoplus_{m=1}^nJ_m=X$ , so $Y$ is a logarithm of $X$ . Many will exist due to branching concerns. It remains to find a logarithm of any arbitrary Jordan block. For a block $J$ with eigenvalue $\lambda$ , we can write $J=\lambda(I+K)$ where $K$ is the matrix will all zero entries, except for entries $\lambda^{-1}$ on the first superdiagonal ( $\lambda\neq0$ by invertibility). If we suppose the formal power series argument is valid , we can say: $$\begin{align}\log(\lambda(I+K))&=\log(\lambda)I+\log(I+K)\\&=\log(\lambda)I+K-\frac{1}{2}K^2+\frac{1}{3}K^3\\&+\cdots+(-1)^j\frac{1}{j-1}K^{j-1}\end{align}$$ Since $K$ will be nilpotent of order $j$ if $j$ is the dimension of the Jordan block, the tail terms of the Mercator series vanish. Any branch of the complex logarithm is appropriate for $(\log(\lambda))I$ - due to commutativity, re-exponentiation gives $\lambda\exp(K-(1/2)K^2+\cdots)\overset{?}{=}\lambda(1+K)$ . Then to claim that this process produces a logarithm for all invertible $X$ , it suffices to demonstrate the following: For all $\lambda\in\Bbb C$ and all $n\times n$ square matrices $K$ of the form: $$K=\begin{pmatrix}0&\lambda&0&0&\cdots\\0&0&\lambda&0&\cdots\\0&0&0&\lambda&\cdots\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&0\end{pmatrix}$$ We have the identity (by commutativity, the two are equivalent): $$\exp\left(\sum_{m=1}^{n-1}\frac{(-1)^{m-1}}{m}K^m\right)=\prod_{m=1}^{n-1}\exp\left(\frac{(-1)^{m-1}}{m}K^m\right)=I_n+K$$ I am looking for an algebraic (or similar) proof of this result. Don't get me wrong - I appreciate indirect proofs, and find them rather magical whenever they arise, but I have never studied any rigorous development of the functional calculus. Analytic functions can be extended to matrix arguments through a variety of methods - matrix Taylor series, Cayley-Hamilton, bizarre Cauchy integral representations, power series convergent in Banach space... and I accept all of these as extensions , with perhaps some convenient properties such as derivative relations carrying over from the complex/real-analytic theory. However, it seems suspicious that ""higher-order"" properties should also be preserved in this extension process. Although we can give well-defined and well-motivated analogues of $\exp$ and $\log$ to matrices, I don't see any immediate reason, a priori , to suppose that the extension reflects relations between them such as $\exp\log\equiv\mathrm{Id}$ . The main reason for my suspicion is the following observation: analysis tends to work through limiting arguments, and finite sums just won't do - they yield polynomials only. It is then rather odd that an analytic series maintains its ""special"" properties despite collapsing into a finite polynomial series - $\Bbb C$ has no nilpotent nonzero elements, but the space of matrices certainly does, and is a key ingredient in the above construction of the logarithm. So what am I looking for? I'm looking for a strong explanation for why what I'm calling ""higher-order"" properties should carry over in this extension process, especially since there are many different ways to extend analytic functions to matrix arguments: of course, any properties that can be deduced from the power series will carry over, e.g. $\exp(A+B)=\exp(A)\exp(B)$ if $A,B$ commute. However, $\exp\circ\log$ 's power series is unclear to me here, since the $\log$ is not actually a power series in this context, really, but a finite polynomial. To reiterate, it is the use of nilpotent elements that concerns me - since this challenges the algebra of $\Bbb C$ , I feel it should also challenge the analytic series which hail from $\Bbb C$ : at least, it should need some more justification. An algebraic proof (direct matrix-arithmetic proof, or maybe some clever linear algebra argument) for why this particular nilpotent logarithm should hold, would be greatly appreciated. My thoughts on this matter so far: $$T:=\sum_{m=1}^{n-1}\frac{(-1)^{m-1}}{m}K^m\\=\begin{pmatrix}0&\lambda&-\frac{1}{2}\lambda^2&\cdots&(-1)^n\frac{1}{n-1}\lambda^{n-1}\\0&0&\lambda&\cdots&(-1)^{n-1}\frac{1}{n-2}\lambda^{n-2}\\0&0&0&\cdots&(-1)^n\frac{1}{n-3}\lambda^{n-3}\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&0\end{pmatrix}$$ An observation of the way that this matrix's elements ""shift"" up a diagonal every time the matrix is squared, cubed, etc. shows that the main superdiagonal has nonzero entries once and only once in $I+T+\frac{1}{2}T^2+\cdots$ , and we can easily partially compute the exponential: $$\exp(T)=\begin{pmatrix}1&\lambda&?&?&\cdots&?\\0&1&\lambda&?&\cdots&?\\0&0&1&\lambda&\cdots&?\\0&0&0&1&\cdots&?\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&0&\cdots&1\end{pmatrix}$$ So it remains to show that the sum over all the remaining superdiagonals vanishes. Other pertinent point is that $T$ is nilpotent of order $n$ , so the exponential series terminates at $\frac{1}{(n-1)!}T^{n-1}$ . This re-explains my suspicion, since now we are claiming these two polynomials are inverse, which is false in the complex world that we began in. How can we fill in the algebraic gaps here? The matrix powers seems quite intractable to symbolically compute. N.B. Sanchul's answer in the linked post never once needs the invertibility of $X$ , as far as I can see, whereas this Wikipedia-based construction does. How do we reconcile the two?","This question is inspired by this which I saw earlier today. I started writing my answer, to share the insight that the matrix logarithm can be defined on matrices that do not have unit norm using an alternative technique. Now, Sangchul has posted a great answer explaining how it is that we know the map defines a logarithm of whenever the sum is convergent, whenever . By scaling, we can use this map to obtain arbitrary logarithms since there is an easy definition which satisfies trivially, and this matrix will also commute with all other matrices. I prefer an approach that sidesteps the functional calculus entirely, that I first learnt on Wikipedia over a year ago. The argument proceeds like this, paraphrased by me: For any invertible complex matrix , there is a basis of the space in which a decomposition into Jordan blocks with some associated eigenvalues . If we can find a matrix , where for all , then a simple inspection of the exponential series shows , so is a logarithm of . Many will exist due to branching concerns. It remains to find a logarithm of any arbitrary Jordan block. For a block with eigenvalue , we can write where is the matrix will all zero entries, except for entries on the first superdiagonal ( by invertibility). If we suppose the formal power series argument is valid , we can say: Since will be nilpotent of order if is the dimension of the Jordan block, the tail terms of the Mercator series vanish. Any branch of the complex logarithm is appropriate for - due to commutativity, re-exponentiation gives . Then to claim that this process produces a logarithm for all invertible , it suffices to demonstrate the following: For all and all square matrices of the form: We have the identity (by commutativity, the two are equivalent): I am looking for an algebraic (or similar) proof of this result. Don't get me wrong - I appreciate indirect proofs, and find them rather magical whenever they arise, but I have never studied any rigorous development of the functional calculus. Analytic functions can be extended to matrix arguments through a variety of methods - matrix Taylor series, Cayley-Hamilton, bizarre Cauchy integral representations, power series convergent in Banach space... and I accept all of these as extensions , with perhaps some convenient properties such as derivative relations carrying over from the complex/real-analytic theory. However, it seems suspicious that ""higher-order"" properties should also be preserved in this extension process. Although we can give well-defined and well-motivated analogues of and to matrices, I don't see any immediate reason, a priori , to suppose that the extension reflects relations between them such as . The main reason for my suspicion is the following observation: analysis tends to work through limiting arguments, and finite sums just won't do - they yield polynomials only. It is then rather odd that an analytic series maintains its ""special"" properties despite collapsing into a finite polynomial series - has no nilpotent nonzero elements, but the space of matrices certainly does, and is a key ingredient in the above construction of the logarithm. So what am I looking for? I'm looking for a strong explanation for why what I'm calling ""higher-order"" properties should carry over in this extension process, especially since there are many different ways to extend analytic functions to matrix arguments: of course, any properties that can be deduced from the power series will carry over, e.g. if commute. However, 's power series is unclear to me here, since the is not actually a power series in this context, really, but a finite polynomial. To reiterate, it is the use of nilpotent elements that concerns me - since this challenges the algebra of , I feel it should also challenge the analytic series which hail from : at least, it should need some more justification. An algebraic proof (direct matrix-arithmetic proof, or maybe some clever linear algebra argument) for why this particular nilpotent logarithm should hold, would be greatly appreciated. My thoughts on this matter so far: An observation of the way that this matrix's elements ""shift"" up a diagonal every time the matrix is squared, cubed, etc. shows that the main superdiagonal has nonzero entries once and only once in , and we can easily partially compute the exponential: So it remains to show that the sum over all the remaining superdiagonals vanishes. Other pertinent point is that is nilpotent of order , so the exponential series terminates at . This re-explains my suspicion, since now we are claiming these two polynomials are inverse, which is false in the complex world that we began in. How can we fill in the algebraic gaps here? The matrix powers seems quite intractable to symbolically compute. N.B. Sanchul's answer in the linked post never once needs the invertibility of , as far as I can see, whereas this Wikipedia-based construction does. How do we reconcile the two?","X\mapsto\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n}X^n 1+X \|X\|\lt1 \log(\lambda I)=(\log\lambda)I \exp(\log(\lambda I))=\lambda I n\times n X \Bbb C^n X=\bigoplus_{m=1}^kJ_m \lambda_m Y=\bigoplus_{m=1}^nT_m \exp(T_m)=J_m m \exp(Y)=\bigoplus_{m=1}^n\exp(T_m)=\bigoplus_{m=1}^nJ_m=X Y X J \lambda J=\lambda(I+K) K \lambda^{-1} \lambda\neq0 \begin{align}\log(\lambda(I+K))&=\log(\lambda)I+\log(I+K)\\&=\log(\lambda)I+K-\frac{1}{2}K^2+\frac{1}{3}K^3\\&+\cdots+(-1)^j\frac{1}{j-1}K^{j-1}\end{align} K j j (\log(\lambda))I \lambda\exp(K-(1/2)K^2+\cdots)\overset{?}{=}\lambda(1+K) X \lambda\in\Bbb C n\times n K K=\begin{pmatrix}0&\lambda&0&0&\cdots\\0&0&\lambda&0&\cdots\\0&0&0&\lambda&\cdots\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&0\end{pmatrix} \exp\left(\sum_{m=1}^{n-1}\frac{(-1)^{m-1}}{m}K^m\right)=\prod_{m=1}^{n-1}\exp\left(\frac{(-1)^{m-1}}{m}K^m\right)=I_n+K \exp \log \exp\log\equiv\mathrm{Id} \Bbb C \exp(A+B)=\exp(A)\exp(B) A,B \exp\circ\log \log \Bbb C \Bbb C T:=\sum_{m=1}^{n-1}\frac{(-1)^{m-1}}{m}K^m\\=\begin{pmatrix}0&\lambda&-\frac{1}{2}\lambda^2&\cdots&(-1)^n\frac{1}{n-1}\lambda^{n-1}\\0&0&\lambda&\cdots&(-1)^{n-1}\frac{1}{n-2}\lambda^{n-2}\\0&0&0&\cdots&(-1)^n\frac{1}{n-3}\lambda^{n-3}\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&0\end{pmatrix} I+T+\frac{1}{2}T^2+\cdots \exp(T)=\begin{pmatrix}1&\lambda&?&?&\cdots&?\\0&1&\lambda&?&\cdots&?\\0&0&1&\lambda&\cdots&?\\0&0&0&1&\cdots&?\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&0&\cdots&1\end{pmatrix} T n \frac{1}{(n-1)!}T^{n-1} X","['linear-algebra', 'matrix-calculus', 'matrix-exponential', 'functional-calculus']"
59,Why is the trace of a matrix the sum along its diagonal?,Why is the trace of a matrix the sum along its diagonal?,,"Define the trace of a matrix with entries in $\mathbb C$ to be the sum of its eigenvalues, counted with multiplicity. It is a standard (but I think extremely surprising) fact that this is the sum of the elements along the diagonal. One proof of this is as follows: Define $Tr'(A)$ to be the sum of the entries along the diagonal of $A$. If $A$ is an $n\times m$ matrix and $B$ and $m\times n$ matrix, we have $$Tr'(AB)=\sum_{i=1}^n\sum_{j=1}^m a_{ij}b_{ji}=\sum_{j=1}^m\sum_{i=1}^n b_{ji}a_{ij}=Tr'(BA)$$ and thus for any invertible matrix $P$ we have $Tr'(PAP^{-1})=Tr'(P^{-1}PA)=Tr'(A)$, i.e. $Tr'$ is independent of basis. Thus it suffices to note that when $A$ is in Jordan Normal Form, $Tr'(A)$ is the trace of $A$. I find this proof pretty unsatisfying, mainly because I don't see any reason I would expect the sum along the diagonal to be basis-independent. Is there a more illuminating proof of this out there?","Define the trace of a matrix with entries in $\mathbb C$ to be the sum of its eigenvalues, counted with multiplicity. It is a standard (but I think extremely surprising) fact that this is the sum of the elements along the diagonal. One proof of this is as follows: Define $Tr'(A)$ to be the sum of the entries along the diagonal of $A$. If $A$ is an $n\times m$ matrix and $B$ and $m\times n$ matrix, we have $$Tr'(AB)=\sum_{i=1}^n\sum_{j=1}^m a_{ij}b_{ji}=\sum_{j=1}^m\sum_{i=1}^n b_{ji}a_{ij}=Tr'(BA)$$ and thus for any invertible matrix $P$ we have $Tr'(PAP^{-1})=Tr'(P^{-1}PA)=Tr'(A)$, i.e. $Tr'$ is independent of basis. Thus it suffices to note that when $A$ is in Jordan Normal Form, $Tr'(A)$ is the trace of $A$. I find this proof pretty unsatisfying, mainly because I don't see any reason I would expect the sum along the diagonal to be basis-independent. Is there a more illuminating proof of this out there?",,"['linear-algebra', 'matrices']"
60,"In the context of vectors, is there a difference between the terms ""magnitude"" and ""length""?","In the context of vectors, is there a difference between the terms ""magnitude"" and ""length""?",,"I noticed vectors are usually said to have ""length"" and ""direction"", but then it is said that people want to find the ""magnitude"". Is this just a difference in terminology or is there something more to it? For example, here they discuss ""magnitude"" and here they use the word ""length"".","I noticed vectors are usually said to have ""length"" and ""direction"", but then it is said that people want to find the ""magnitude"". Is this just a difference in terminology or is there something more to it? For example, here they discuss ""magnitude"" and here they use the word ""length"".",,"['linear-algebra', 'vectors', 'terminology']"
61,"If matrices $A$ and $B$ commute, $A$ with distinct eigenvalues, then $B$ is a polynomial in $A$","If matrices  and  commute,  with distinct eigenvalues, then  is a polynomial in",A B A B A,"If $A\in M_{n}$ has $n$ distinct eigenvalues and if $A$ commutes with a given matrix $B\in M_{n}$, how can I show that $B$ is a polynomial in $A$ of degree at most $n-1$? I think first I need to show that $A$ and $B$ are simultaneously diagonalizable, but I don't know how to begin. Any advice is much appreciated.","If $A\in M_{n}$ has $n$ distinct eigenvalues and if $A$ commutes with a given matrix $B\in M_{n}$, how can I show that $B$ is a polynomial in $A$ of degree at most $n-1$? I think first I need to show that $A$ and $B$ are simultaneously diagonalizable, but I don't know how to begin. Any advice is much appreciated.",,"['linear-algebra', 'control-theory']"
62,Is the $0\times 0$ matrix (zero-times-zero matrix) a well-defined concept?,Is the  matrix (zero-times-zero matrix) a well-defined concept?,0\times 0,"Is the $0\times 0$ matrix a well-defined concept, and if yes, what can be said about it? Intuitively it should be a well-defined concept, since we have the zero vector space, and every linear mapping between vector spaces (such as the zero mapping from the zero vector space to itself) can be represented by a matrix. The determinant of a $0\times 0$ matrix should be $1$ , intuitively, just as the zero-dimensional volume of a point is $1$ . If the $0\times 0$ matrix is a bad concept or ill-defined concept, can you explain where to find the catch?","Is the matrix a well-defined concept, and if yes, what can be said about it? Intuitively it should be a well-defined concept, since we have the zero vector space, and every linear mapping between vector spaces (such as the zero mapping from the zero vector space to itself) can be represented by a matrix. The determinant of a matrix should be , intuitively, just as the zero-dimensional volume of a point is . If the matrix is a bad concept or ill-defined concept, can you explain where to find the catch?",0\times 0 0\times 0 1 1 0\times 0,"['linear-algebra', 'matrices']"
63,Prove that a matrix is invertible [duplicate],Prove that a matrix is invertible [duplicate],,This question already has an answer here : Why does the inverse of the Hilbert matrix have integer entries? (1 answer) Closed 10 years ago . Show that the matrix $A = \begin{bmatrix} 1 & \frac{1}{2} & \ldots & \frac{1}{n}\\ \frac{1}{2} & \frac{1}{3} & \ldots & \frac{1}{n+1}\\ \vdots & \vdots & & \vdots \\ \frac{1}{n} &\frac{1}{n+1} &\ldots &\frac{1}{2n-1} \end{bmatrix}$ is invertible and $A^{-1}$ has integer entries. This problem appeared in Chapter one of my linear algebra textbook so I assume nothing more is required to prove it than elementary row operations. I've been staring at it for a while and considered small values of $n$ but there doesn't seem to be a clever way of easily reducing it to the identity matrix. Could someone give me a hint or a poke in the right direction rather than a full solution as I'm still hoping to work it out myself.,This question already has an answer here : Why does the inverse of the Hilbert matrix have integer entries? (1 answer) Closed 10 years ago . Show that the matrix $A = \begin{bmatrix} 1 & \frac{1}{2} & \ldots & \frac{1}{n}\\ \frac{1}{2} & \frac{1}{3} & \ldots & \frac{1}{n+1}\\ \vdots & \vdots & & \vdots \\ \frac{1}{n} &\frac{1}{n+1} &\ldots &\frac{1}{2n-1} \end{bmatrix}$ is invertible and $A^{-1}$ has integer entries. This problem appeared in Chapter one of my linear algebra textbook so I assume nothing more is required to prove it than elementary row operations. I've been staring at it for a while and considered small values of $n$ but there doesn't seem to be a clever way of easily reducing it to the identity matrix. Could someone give me a hint or a poke in the right direction rather than a full solution as I'm still hoping to work it out myself.,,"['linear-algebra', 'hilbert-matrices']"
64,Inverse of a $2 \times 2$ block matrix,Inverse of a  block matrix,2 \times 2,"Let $$S := \pmatrix{A&B\\C&D}$$ If $A^{-1}$ or $D^{-1}$ exist, we know that matrix $S$ can be inverted. $$S^{-1} = \pmatrix{A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}&-A^{-1}B(D-CA^{-1}B)^{-1}\\-(D-CA^{-1}B)^{-1}CA^{-1}&(D-CA^{-1}B)^{-1}}$$ But, what if $A^{-1}$ and $D^{-1}$ do not exist? Can we invert matrix $S$ ? For example, $$S = \pmatrix{0&1\\1&0}$$ or $$S = \pmatrix{2&3&1&1\\4&6&1&2\\1&1&3&1\\4&1&12&4}$$ both their $A^{-1}$ and $D^{-1}$ don't exist, but $S^{-1}$ exists.","Let If or exist, we know that matrix can be inverted. But, what if and do not exist? Can we invert matrix ? For example, or both their and don't exist, but exists.",S := \pmatrix{A&B\\C&D} A^{-1} D^{-1} S S^{-1} = \pmatrix{A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}&-A^{-1}B(D-CA^{-1}B)^{-1}\\-(D-CA^{-1}B)^{-1}CA^{-1}&(D-CA^{-1}B)^{-1}} A^{-1} D^{-1} S S = \pmatrix{0&1\\1&0} S = \pmatrix{2&3&1&1\\4&6&1&2\\1&1&3&1\\4&1&12&4} A^{-1} D^{-1} S^{-1},"['linear-algebra', 'matrices', 'inverse', 'block-matrices', 'schur-complement']"
65,vector spaces whose algebra of endomorphisms is generated by its idempotents,vector spaces whose algebra of endomorphisms is generated by its idempotents,,Let $V$ be a $K$-vector space whose algebra of endomorphisms is generated (as a $K$-algebra) by its idempotents. Is $V$ necessarily finite dimensional? EDIT (Jul 26 '14) A closely related question: Is there a field $K$ and a $K$-vector space whose algebra of endomorphisms of is not generated (as a $K$-algebra) by its idempotents?,Let $V$ be a $K$-vector space whose algebra of endomorphisms is generated (as a $K$-algebra) by its idempotents. Is $V$ necessarily finite dimensional? EDIT (Jul 26 '14) A closely related question: Is there a field $K$ and a $K$-vector space whose algebra of endomorphisms of is not generated (as a $K$-algebra) by its idempotents?,,['linear-algebra']
66,"Why do siamese magic squares have real eigenvalues, symmetric around zero?","Why do siamese magic squares have real eigenvalues, symmetric around zero?",,"There is a standard method to construct magic squares of odd size, known as the Siamese construction . I'll write $S_m$ for the $m \times m$ Siamese square. For example, here is $S_5$. 17    24     1     8    15 23     5     7    14    16  4     6    13    20    22 10    12    19    21     3 11    18    25     2     9 As you can see, this matrix certainly isn't symmetric! It has one obvious eigenvector, the all ones vector, whose corresponding eigenvalue is $\frac{m^3+m}{2}$, and there is no obvious reason its other eigenvalues should have nice structure. Nonetheless, experimentation suggests: (1) All eigenvalues of $S_m$ are real. (2) If $\lambda$ is an eigenvalue of $S_m$ other than $\frac{m^3+m}{2}$, then $-\lambda$ is as well. Can anyone explain why? Remark In case anyone is curious how I found this, I am teaching myself MATLAB and came up with the task of listing characteristic polynomials of magic squares as a more or less random task which would involve learning some basic control structures and some mathematical tools. This piece of MATLAB documentation describes some alternate ways of thinking about the Siamese construction, which may be relevant.","There is a standard method to construct magic squares of odd size, known as the Siamese construction . I'll write $S_m$ for the $m \times m$ Siamese square. For example, here is $S_5$. 17    24     1     8    15 23     5     7    14    16  4     6    13    20    22 10    12    19    21     3 11    18    25     2     9 As you can see, this matrix certainly isn't symmetric! It has one obvious eigenvector, the all ones vector, whose corresponding eigenvalue is $\frac{m^3+m}{2}$, and there is no obvious reason its other eigenvalues should have nice structure. Nonetheless, experimentation suggests: (1) All eigenvalues of $S_m$ are real. (2) If $\lambda$ is an eigenvalue of $S_m$ other than $\frac{m^3+m}{2}$, then $-\lambda$ is as well. Can anyone explain why? Remark In case anyone is curious how I found this, I am teaching myself MATLAB and came up with the task of listing characteristic polynomials of magic squares as a more or less random task which would involve learning some basic control structures and some mathematical tools. This piece of MATLAB documentation describes some alternate ways of thinking about the Siamese construction, which may be relevant.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'recreational-mathematics', 'magic-square']"
67,Show that $P_i$ and $\sum_i P_i$ being idempotent implies $P_i P_j=\delta_{ij}$,Show that  and  being idempotent implies,P_i \sum_i P_i P_i P_j=\delta_{ij},"Let $X$ be a finite dimensional real linear space, or more generally a finite dimensional vector space over a field of characteristic $0$ .  Let $(P_i)_{i=1}^n$ be a finite sequence of linear mappings $P_i :X\rightarrow X$ such that $P_i^2=P_i$ for $i=1,...,n$ , $(P_1+...+P_n)^2=P_1+...+P_n$ . I wish to show that $$P_i\circ P_j=0 \textrm{ for } i \neq j.$$ I know how to prove it only for $n=2$ : if $(P_1+P_2)^2=P_1+P_2$ then $$P_1P_2+P_2P_1=0. \tag{$\ast$}$$ By multiplying both sides this equality by $P_1$ from left and by $P_1$ by right and obtain two equalities: $P_1 P_2+P_1P_2P_1=0$ and $P_1P_2P_1+P_2P_1=0$ . By subtracting: $$P_1P_2-P_2P_1=0. \tag{$\ast\ast$}$$ From $(\ast)$ , $({\ast}\ast)$ , we get $P_1P_2=0$ , $P_2P_1=0$ .","Let be a finite dimensional real linear space, or more generally a finite dimensional vector space over a field of characteristic .  Let be a finite sequence of linear mappings such that for , . I wish to show that I know how to prove it only for : if then By multiplying both sides this equality by from left and by by right and obtain two equalities: and . By subtracting: From , , we get , .","X 0 (P_i)_{i=1}^n P_i :X\rightarrow X P_i^2=P_i i=1,...,n (P_1+...+P_n)^2=P_1+...+P_n P_i\circ P_j=0 \textrm{ for } i \neq j. n=2 (P_1+P_2)^2=P_1+P_2 P_1P_2+P_2P_1=0. \tag{\ast} P_1 P_1 P_1 P_2+P_1P_2P_1=0 P_1P_2P_1+P_2P_1=0 P_1P_2-P_2P_1=0. \tag{\ast\ast} (\ast) ({\ast}\ast) P_1P_2=0 P_2P_1=0","['linear-algebra', 'matrices', 'operator-theory', 'projection-matrices']"
68,Geometry of the set of coefficients such that monic polynomials have roots within unit disk,Geometry of the set of coefficients such that monic polynomials have roots within unit disk,,"We let $\pi$ be the bijection between coefficients of the real monic polynomials to the real monic polynomials. Let $a\in \mathbb R^n$ be fixed vector. Then \begin{align*} \pi(a) = t^n + a_{n-1} t^{n-1} + \dots + a_0. \\ \end{align*} Now denote the set $$ \Delta = \{ x\in \mathbb R^n: \pi(x) \text{ has roots in the open unit disk of } \mathbb C\}.$$ It can be shown $\Delta$ is a path-connected set by Vieta's formula (maybe slightly modified for the real case). Let us consider a line (1-dim subspace) in $\mathbb R^n$ , $L = \alpha b$ where $\alpha \in \mathbb R$ and $b \in \mathbb R^n$ is fixed. Clearly $L \cap \Delta$ is nonempty since $0 \in L \cap \Delta$ . I am trying to determine the number of connected components of $L \cap \Delta$ . We shall assume $n > 2$ . If $n=1$ , $L \cap \Delta$ is clearly connected. For $n=2$ , if I am not mistaken, the paper by Fell https://projecteuclid.org/euclid.pjm/1102779366#ui-tabs-1 asserts that convex combination of real monic polynomials of the same degree with roots in the unit disk remains in the unit disk (Theorem 4). This allows us to construct a path between any $x \in L \cap \Delta$ and $0 \in \mathbb R^n$ and so $L \cap \Delta$ is connected. Edit: The question was initially unclear in the sense: I didn't know whether $L \cap \Delta$ is connected.  @Jean-Claude Arbaut gave nice numerical examples and plot to show that the set is not connected. I rewarded a bounty and started a new one to see whether there is some bound of the number of connected components. I would reward the bounty to any bound or if the number of connected components could be unbounded.","We let be the bijection between coefficients of the real monic polynomials to the real monic polynomials. Let be fixed vector. Then Now denote the set It can be shown is a path-connected set by Vieta's formula (maybe slightly modified for the real case). Let us consider a line (1-dim subspace) in , where and is fixed. Clearly is nonempty since . I am trying to determine the number of connected components of . We shall assume . If , is clearly connected. For , if I am not mistaken, the paper by Fell https://projecteuclid.org/euclid.pjm/1102779366#ui-tabs-1 asserts that convex combination of real monic polynomials of the same degree with roots in the unit disk remains in the unit disk (Theorem 4). This allows us to construct a path between any and and so is connected. Edit: The question was initially unclear in the sense: I didn't know whether is connected.  @Jean-Claude Arbaut gave nice numerical examples and plot to show that the set is not connected. I rewarded a bounty and started a new one to see whether there is some bound of the number of connected components. I would reward the bounty to any bound or if the number of connected components could be unbounded.","\pi a\in \mathbb R^n \begin{align*}
\pi(a) = t^n + a_{n-1} t^{n-1} + \dots + a_0. \\
\end{align*}  \Delta = \{ x\in \mathbb R^n: \pi(x) \text{ has roots in the open unit disk of } \mathbb C\}. \Delta \mathbb R^n L = \alpha b \alpha \in \mathbb R b \in \mathbb R^n L \cap \Delta 0 \in L \cap \Delta L \cap \Delta n > 2 n=1 L \cap \Delta n=2 x \in L \cap \Delta 0 \in \mathbb R^n L \cap \Delta L \cap \Delta","['linear-algebra', 'general-topology', 'algebraic-geometry', 'polynomials', 'path-connected']"
69,How find this determinant $\det(\cos^4{(i-j)})_{n\times n}$,How find this determinant,\det(\cos^4{(i-j)})_{n\times n},"Question: Define the matrix $A_{k}=(a^k_{ij})_{n\times n}\quad$where $a_{ij}=\cos{(i-j)},\quad n\ge 6$ Find the value $$\det(A_{4})=\:?$$ My try: since $$\det(A_{4})=\begin{vmatrix} 1&\cos^4{1}&\cos^4{2}&\cdots&\cos^4{(n-1)}\\ \cos^4{1}&1&\cos^4{1}&\cdots&\cos^4{(n-2)}\\ \vdots&\vdots&\vdots&\ddots&\vdots&\\ \cos^4{(n-1)}&\cos^4{(n-2)}&\cos^4{(n-3)}&\cdots&1 \end{vmatrix}$$ and I know that $$\det(A_{1})=\det(A_{2})=0$$ I suspect the following result: $$\det(A_{4})=0,n\ge 6$$ But I can't prove it.","Question: Define the matrix $A_{k}=(a^k_{ij})_{n\times n}\quad$where $a_{ij}=\cos{(i-j)},\quad n\ge 6$ Find the value $$\det(A_{4})=\:?$$ My try: since $$\det(A_{4})=\begin{vmatrix} 1&\cos^4{1}&\cos^4{2}&\cdots&\cos^4{(n-1)}\\ \cos^4{1}&1&\cos^4{1}&\cdots&\cos^4{(n-2)}\\ \vdots&\vdots&\vdots&\ddots&\vdots&\\ \cos^4{(n-1)}&\cos^4{(n-2)}&\cos^4{(n-3)}&\cdots&1 \end{vmatrix}$$ and I know that $$\det(A_{1})=\det(A_{2})=0$$ I suspect the following result: $$\det(A_{4})=0,n\ge 6$$ But I can't prove it.",,"['linear-algebra', 'matrices', 'determinant']"
70,Eigenvalues of outer product matrix of two N-dimensional vectors,Eigenvalues of outer product matrix of two N-dimensional vectors,,"I have a vector $\textbf{a}=(a_1, a_2, ....)$, and the outer product $M_{ij}=a_i a_j$. What are the eigenvalues of this matrix? and what can you say about the co-ordinate system in which $M$ is diagonal? I have proved that the only eigenvalue of the matrix is the norm of the vector squared, and that one of the eigenvectors is $\textbf{a}$ itself. $$Mu=a a^{T}u=\lambda u\\\ \implies a^{T}(aa^{T})u=a^{T}\lambda u \implies a^{T}a (a^{T}u)=\lambda a^{T}u \implies ||a||^2=\lambda$$ Also it is obvious that $a$ is the eigenvector of $aa^{T}$, which implies $M= \begin{pmatrix} ||a||^2 &0 &0 &..... \\0 & ||a||^2\\.\\.\\\\. \end{pmatrix}$ Is this correct? What are all the eigenvectors of this the outer product? and what intuitively happens in the transformed basis when M is diagonal?","I have a vector $\textbf{a}=(a_1, a_2, ....)$, and the outer product $M_{ij}=a_i a_j$. What are the eigenvalues of this matrix? and what can you say about the co-ordinate system in which $M$ is diagonal? I have proved that the only eigenvalue of the matrix is the norm of the vector squared, and that one of the eigenvectors is $\textbf{a}$ itself. $$Mu=a a^{T}u=\lambda u\\\ \implies a^{T}(aa^{T})u=a^{T}\lambda u \implies a^{T}a (a^{T}u)=\lambda a^{T}u \implies ||a||^2=\lambda$$ Also it is obvious that $a$ is the eigenvector of $aa^{T}$, which implies $M= \begin{pmatrix} ||a||^2 &0 &0 &..... \\0 & ||a||^2\\.\\.\\\\. \end{pmatrix}$ Is this correct? What are all the eigenvectors of this the outer product? and what intuitively happens in the transformed basis when M is diagonal?",,['linear-algebra']
71,Why is the matrix-defined Cross Product of two 3D vectors always orthogonal?,Why is the matrix-defined Cross Product of two 3D vectors always orthogonal?,,"By matrix-defined, I mean $$\left<a,b,c\right>\times\left<d,e,f\right> = \left|   \begin{array}{ccc} i & j & k\\ a & b & c\\ d & e & f \end{array}  \right|$$ ...instead of the definition of the product of the magnitudes multiplied by the sign of their angle, in the direction orthogonal) If I try cross producting two vectors with no $k$ component, I get one with only $k$, which is expected. But why? As has been pointed out, I am asking why the algebraic definition lines up with the geometric definition.","By matrix-defined, I mean $$\left<a,b,c\right>\times\left<d,e,f\right> = \left|   \begin{array}{ccc} i & j & k\\ a & b & c\\ d & e & f \end{array}  \right|$$ ...instead of the definition of the product of the magnitudes multiplied by the sign of their angle, in the direction orthogonal) If I try cross producting two vectors with no $k$ component, I get one with only $k$, which is expected. But why? As has been pointed out, I am asking why the algebraic definition lines up with the geometric definition.",,"['linear-algebra', 'matrices', 'inner-products', 'orthogonality', 'cross-product']"
72,Bound on nilpotency index of endomorphisms,Bound on nilpotency index of endomorphisms,,"Let $A$ be a Noetherian ring (commutative with $1$) and $M$ a finitely generated $A$-module. I want to show that there exists a bound $n$ such that for every nilpotent endomorphism $T : M \to M$ we have $T^n = 0$. There are two examples in mind: $M$ a module over a field $K$, that is $M = K^n$ a finite-dimensional vector space. In that case, $n$ is the required upper bound. $A=K[x]/(x^d)$ as a module over itself. In that case the dimension is $1$ (a basis is $\{ 1 \}$) but the upper bound is $d$ (consider the endomorphism $y \mapsto x y$).","Let $A$ be a Noetherian ring (commutative with $1$) and $M$ a finitely generated $A$-module. I want to show that there exists a bound $n$ such that for every nilpotent endomorphism $T : M \to M$ we have $T^n = 0$. There are two examples in mind: $M$ a module over a field $K$, that is $M = K^n$ a finite-dimensional vector space. In that case, $n$ is the required upper bound. $A=K[x]/(x^d)$ as a module over itself. In that case the dimension is $1$ (a basis is $\{ 1 \}$) but the upper bound is $d$ (consider the endomorphism $y \mapsto x y$).",,"['linear-algebra', 'commutative-algebra', 'modules']"
73,What is the relation between dual spaces and inner product?,What is the relation between dual spaces and inner product?,,"What is the relation (if any) between dual spaces and inner product? As far as I understand the dual space of a vector space is the set of all linear mappings from the vector set to the field over which the space is defined. But the definition of the inner product is a bilinear mapping of two vectors to a scalar. It sounds to me like if we had defined the same thing twice, in two different ways, is that so? If the answer is yes, and given that every space has a dual space, does that mean that every vector space is automatically an inner product space? Moreover, if the polarization identity can be used to define a norm from an inner product, are all vector spaces inner normed spaces? I am sure I'm misunderstanding some definition, but I'm totally lost here. Any help?","What is the relation (if any) between dual spaces and inner product? As far as I understand the dual space of a vector space is the set of all linear mappings from the vector set to the field over which the space is defined. But the definition of the inner product is a bilinear mapping of two vectors to a scalar. It sounds to me like if we had defined the same thing twice, in two different ways, is that so? If the answer is yes, and given that every space has a dual space, does that mean that every vector space is automatically an inner product space? Moreover, if the polarization identity can be used to define a norm from an inner product, are all vector spaces inner normed spaces? I am sure I'm misunderstanding some definition, but I'm totally lost here. Any help?",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'inner-products', 'duality-theorems']"
74,Strange inequality involving *nested* binomial coefficients and combinatorial interpretation,Strange inequality involving *nested* binomial coefficients and combinatorial interpretation,,"Recently, I solved a question that asked for a geometric proof of the identity $\binom{\binom{n}{3}}{2} < \binom{\binom{n}{2}}{3}$ and I did it here using the combinatorial interpretation of lines and triangles in an $n$ -gon, and exploiting the symmetry of these line-triangle configurations to provide an injective (and non-surjective) map from a set with cardinality $\binom{\binom{n}{3}}{2}$ to a set of cardinality $\binom{\binom{n}{2}}{3}$ . This brought me to the more general question : Suppose that $b>c$ . Then, for all $n$ , is the following true? $$\binom{\binom{n}{b}}{c} < \binom{\binom{n}{c}}{b}$$ (Note : we define $\binom{k}{l} = 0$ if $l>k$ ). I want a proof of this, but I've just left some avenues open so that others can explore them below. To break the suspense, this identity is true, but I've actually never seen a proof of it, so here goes. Gamma function I looked here for some details, and apparently a ""lengthy,uninspiring computation involving Gamma functions"" is required.  Nevertheless, I couldn't quite find a reference to this computation : so I'd love one. Having said that , if someone can write up an (lengthy, uninspiring etc.) argument that involves Gamma functions, I'd be delighted. To provide some details on the link, we have : $$ \binom{n}{k} = \frac{n!}{(n-k)!k!} = \frac{\Gamma(n+1)}{\Gamma(n-k+1)\Gamma(k+1)} $$ and therefore, nested binomial coefficients will involve nested Gamma functions, at which point I wasn't quite able to work my way through. Combinatorial interpretation The same document as above provides a telling combinatorial interpretation : $\binom{\binom{n}{b}}{c}$ is the number of ways of choosing a subset of $c$ elements ,from the set of subsets of $b$ elements of $\{1,2,...,n\}$ . which it then twists into : $\binom{\binom{n}{b}}{c}$ is the number of $b \times c$ matrices with entries in $\{1,...,n\}$ , such that the elements are strictly increasing along rows, and the rows, interpreted as vectors , are strictly lexicographically increasing. So perhaps one can work with these combinatorial objects. On the other hand, one can see if something around what I did with the $n$ -gons can be generalized, I struggled to do so. Short note : binomial basis expansion Note also, that the linear-algebraic approach that I proposed in my answer (to give an algebraic proof of the nested binomial identity) can probably work here (i.e. writing the nested binomial coefficients in the polynomial basis $\binom{n}{k},0<k\leq bc$ ) : but the coefficients when one uses the binomial basis are quite complicated, see here . Note that both the nested binomial coefficients are polynomials of degree $bc$ in $n$ , so one can try to see if something works out via binomial basis expansions. Alternate definitions See if alternate definitions/generalizations of the binomial coefficient are helpful, like this one . Note that if we can make the binomial coefficient into a nice continuous-parameter function, then we can investigate these properties using real analytic methods. EDIT : This document of  Marko Riedel contains details of the Egorychev (Егорычев) method , which is basically a complex-analytic representation of the binomial coefficients. Anyone is free to use any of the identities available here, and I thank Marko for creating this useful list. The basic problem here is the nesting of the coefficients, so any fundamental operation that simplifies the nesting (converting the nesting into an operation that is easier to understand) will be appreciated, either in the comments or as a partial answer. Finally, further comments on inequalities involving further nestings like $\binom{\binom{\binom{n}{a}}{b}}{c}$ and so on will be appreciated as well.","Recently, I solved a question that asked for a geometric proof of the identity and I did it here using the combinatorial interpretation of lines and triangles in an -gon, and exploiting the symmetry of these line-triangle configurations to provide an injective (and non-surjective) map from a set with cardinality to a set of cardinality . This brought me to the more general question : Suppose that . Then, for all , is the following true? (Note : we define if ). I want a proof of this, but I've just left some avenues open so that others can explore them below. To break the suspense, this identity is true, but I've actually never seen a proof of it, so here goes. Gamma function I looked here for some details, and apparently a ""lengthy,uninspiring computation involving Gamma functions"" is required.  Nevertheless, I couldn't quite find a reference to this computation : so I'd love one. Having said that , if someone can write up an (lengthy, uninspiring etc.) argument that involves Gamma functions, I'd be delighted. To provide some details on the link, we have : and therefore, nested binomial coefficients will involve nested Gamma functions, at which point I wasn't quite able to work my way through. Combinatorial interpretation The same document as above provides a telling combinatorial interpretation : is the number of ways of choosing a subset of elements ,from the set of subsets of elements of . which it then twists into : is the number of matrices with entries in , such that the elements are strictly increasing along rows, and the rows, interpreted as vectors , are strictly lexicographically increasing. So perhaps one can work with these combinatorial objects. On the other hand, one can see if something around what I did with the -gons can be generalized, I struggled to do so. Short note : binomial basis expansion Note also, that the linear-algebraic approach that I proposed in my answer (to give an algebraic proof of the nested binomial identity) can probably work here (i.e. writing the nested binomial coefficients in the polynomial basis ) : but the coefficients when one uses the binomial basis are quite complicated, see here . Note that both the nested binomial coefficients are polynomials of degree in , so one can try to see if something works out via binomial basis expansions. Alternate definitions See if alternate definitions/generalizations of the binomial coefficient are helpful, like this one . Note that if we can make the binomial coefficient into a nice continuous-parameter function, then we can investigate these properties using real analytic methods. EDIT : This document of  Marko Riedel contains details of the Egorychev (Егорычев) method , which is basically a complex-analytic representation of the binomial coefficients. Anyone is free to use any of the identities available here, and I thank Marko for creating this useful list. The basic problem here is the nesting of the coefficients, so any fundamental operation that simplifies the nesting (converting the nesting into an operation that is easier to understand) will be appreciated, either in the comments or as a partial answer. Finally, further comments on inequalities involving further nestings like and so on will be appreciated as well.","\binom{\binom{n}{3}}{2} < \binom{\binom{n}{2}}{3} n \binom{\binom{n}{3}}{2} \binom{\binom{n}{2}}{3} b>c n \binom{\binom{n}{b}}{c} < \binom{\binom{n}{c}}{b} \binom{k}{l} = 0 l>k 
\binom{n}{k} = \frac{n!}{(n-k)!k!} = \frac{\Gamma(n+1)}{\Gamma(n-k+1)\Gamma(k+1)}
 \binom{\binom{n}{b}}{c} c b \{1,2,...,n\} \binom{\binom{n}{b}}{c} b \times c \{1,...,n\} n \binom{n}{k},0<k\leq bc bc n \binom{\binom{\binom{n}{a}}{b}}{c}","['linear-algebra', 'binomial-coefficients', 'gamma-function', 'combinatorial-proofs']"
75,Direct Sum of vector subspaces,Direct Sum of vector subspaces,,"How is direct sum of two vector subspaces different from the sum of two vector subspaces i.e. how is $X\oplus Y$ different from $X + Y$, where $X, Y$ are subspaces.","How is direct sum of two vector subspaces different from the sum of two vector subspaces i.e. how is $X\oplus Y$ different from $X + Y$, where $X, Y$ are subspaces.",,['linear-algebra']
76,Is there a fundamental theorem of algebra for matrices?,Is there a fundamental theorem of algebra for matrices?,,"The fundamental theorem of algebra says we can do this ($z\in\mathbb{C}$ of course) $$\sum_{k=0}^n a_kz^k= a_n\prod_{k=1}^n (z-\omega_k)=0$$ for some set $\{\omega_k \in\mathbb{C}\}_{k=1,2,\ldots , n}$. Is there a similar answer for square matrices with complex entries (i.e. for linear operators $A_k:\mathbb{C}^n \to\mathbb{C}^n,\,\,(k=0,1,2,\ldots, n))$? That is, can we do such a factorization $$\sum_{k=0}^n A_kz^k= A_n\prod_{k=1}^n (z\mathbb{1}-\Omega_k)=0$$ where $\mathbb{1}$ is the identity operator and $\Omega_k : \mathbb{C}^n\to\mathbb{C}^n$ are linear operators? Certainly we're restricted here by the non-commutativity of matrices in general, so the order of this decomposition would matter. But, maybe there are some types of matrices that admit commutative ""root"" matrices? Sorry if my notation is awkward, and also if the answer to this is considered common knowledge.","The fundamental theorem of algebra says we can do this ($z\in\mathbb{C}$ of course) $$\sum_{k=0}^n a_kz^k= a_n\prod_{k=1}^n (z-\omega_k)=0$$ for some set $\{\omega_k \in\mathbb{C}\}_{k=1,2,\ldots , n}$. Is there a similar answer for square matrices with complex entries (i.e. for linear operators $A_k:\mathbb{C}^n \to\mathbb{C}^n,\,\,(k=0,1,2,\ldots, n))$? That is, can we do such a factorization $$\sum_{k=0}^n A_kz^k= A_n\prod_{k=1}^n (z\mathbb{1}-\Omega_k)=0$$ where $\mathbb{1}$ is the identity operator and $\Omega_k : \mathbb{C}^n\to\mathbb{C}^n$ are linear operators? Certainly we're restricted here by the non-commutativity of matrices in general, so the order of this decomposition would matter. But, maybe there are some types of matrices that admit commutative ""root"" matrices? Sorry if my notation is awkward, and also if the answer to this is considered common knowledge.",,"['linear-algebra', 'abstract-algebra']"
77,Please explain definition of determinant using permutations?,Please explain definition of determinant using permutations?,,"Many people (in different texts) use the following famous definition of the determinant of a matrix $A$: \begin{align*} \det(A) = \sum_{\tau \in S_n}\operatorname{sgn}(\tau)\,a_{1,\tau(1)}a_{2,\tau(2)} \ldots a_{n,\tau(n)}, \end{align*} where the sum is over all permutations of $n$ elements over the symmetric group. None of them actually explains how one interprets this definition, so this makes me suspicious and think they don't know it either. This is what I understand so far: Definition: A permutation $\tau$ of $n$ elements is a bijective function having the set $ \left\{1, 2, ..., n\right\}$ both as its domain and codomain. The number of permutations of $n$ elements, and hence the cardinality of the set $S_n$ is $n!$ So for example, for every integer $i \in \left\{1, 2, ..., n\right\}$ there exists exactly one integer $j \in \left\{1, 2, ..., n\right\}$ for which $\tau(j) = i$. Permutations can also be represented in matrices, for example if $\tau(1) = 3, \tau(2) = 1, \tau(3) =4, \tau(4) =5, \tau(5) =2$, then \begin{align*} \tau = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 3 & 1 & 4 & 5 & 2 \end{pmatrix}. \end{align*} Definition: Let $\tau \in S_n$ be a permutation. Then an inversion pair $(i,j)$ of $\tau$ is a pair of positive integers $i, j \in \left\{1, 2, ..., n\right\}$ for which $i < j$ but $\tau(i) > \tau(j)$. This determines how many elements are 'out of order'. For example if $\tau = \begin{pmatrix} 1 & 2 & 3 \\ 1 & 3 & 2 \end{pmatrix}$, then $\tau$ has one single inversion pair $(2,3)$, since $\tau(2) = 3 > \tau(3) = 2$. Definition: A transposition , called $t_{ij}$, is the permutation that interchanges $i$ and $j$ while leaving all other integers fixed in place. The numbers of inversions in a transposition is always odd, because one can compute that the number of inversion pairs in $t_{ij}$ is exactly $2(j-1)-1$. Definition: Let $\tau \in S_n$ be a permutation. Then the sign of $\tau$, denoted by sign$(\tau)$ is defined by \begin{align*} sign(\tau) = (-1)^{\text{# of inversion pairs in}\ \tau} \end{align*} This is $+1$ if the number of inversions is even, and $-1$ if the number is odd. Every transposition is an odd permutation. This is all clear to me, but can someone explain to me, in an understandable fashion, how one interprets the definition of the determinant on the basis of all this information? That would be greatly appreciated (not only by me, but I think by many others aswell). For example: what do I make of the $a_{1,\tau(1)}$ etc. in the definition of the determinant, all the way up to $n$? What do they represent?","Many people (in different texts) use the following famous definition of the determinant of a matrix $A$: \begin{align*} \det(A) = \sum_{\tau \in S_n}\operatorname{sgn}(\tau)\,a_{1,\tau(1)}a_{2,\tau(2)} \ldots a_{n,\tau(n)}, \end{align*} where the sum is over all permutations of $n$ elements over the symmetric group. None of them actually explains how one interprets this definition, so this makes me suspicious and think they don't know it either. This is what I understand so far: Definition: A permutation $\tau$ of $n$ elements is a bijective function having the set $ \left\{1, 2, ..., n\right\}$ both as its domain and codomain. The number of permutations of $n$ elements, and hence the cardinality of the set $S_n$ is $n!$ So for example, for every integer $i \in \left\{1, 2, ..., n\right\}$ there exists exactly one integer $j \in \left\{1, 2, ..., n\right\}$ for which $\tau(j) = i$. Permutations can also be represented in matrices, for example if $\tau(1) = 3, \tau(2) = 1, \tau(3) =4, \tau(4) =5, \tau(5) =2$, then \begin{align*} \tau = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 3 & 1 & 4 & 5 & 2 \end{pmatrix}. \end{align*} Definition: Let $\tau \in S_n$ be a permutation. Then an inversion pair $(i,j)$ of $\tau$ is a pair of positive integers $i, j \in \left\{1, 2, ..., n\right\}$ for which $i < j$ but $\tau(i) > \tau(j)$. This determines how many elements are 'out of order'. For example if $\tau = \begin{pmatrix} 1 & 2 & 3 \\ 1 & 3 & 2 \end{pmatrix}$, then $\tau$ has one single inversion pair $(2,3)$, since $\tau(2) = 3 > \tau(3) = 2$. Definition: A transposition , called $t_{ij}$, is the permutation that interchanges $i$ and $j$ while leaving all other integers fixed in place. The numbers of inversions in a transposition is always odd, because one can compute that the number of inversion pairs in $t_{ij}$ is exactly $2(j-1)-1$. Definition: Let $\tau \in S_n$ be a permutation. Then the sign of $\tau$, denoted by sign$(\tau)$ is defined by \begin{align*} sign(\tau) = (-1)^{\text{# of inversion pairs in}\ \tau} \end{align*} This is $+1$ if the number of inversions is even, and $-1$ if the number is odd. Every transposition is an odd permutation. This is all clear to me, but can someone explain to me, in an understandable fashion, how one interprets the definition of the determinant on the basis of all this information? That would be greatly appreciated (not only by me, but I think by many others aswell). For example: what do I make of the $a_{1,\tau(1)}$ etc. in the definition of the determinant, all the way up to $n$? What do they represent?",,"['linear-algebra', 'permutations', 'determinant']"
78,Does $SL_2(K) \simeq SL_2(L)$ imply $K\simeq L$?,Does  imply ?,SL_2(K) \simeq SL_2(L) K\simeq L,"Let $K$ and $L$ be two fields. Assume characteristics are not 2. I can show in a quite elementary way that if the statement $SL_2(K) \simeq SL_2(L) \implies K \simeq L$ holds, then for $n \geq 2$ , the statement $SL_n(K) \simeq SL_n(L) \implies K \simeq L$ holds. But I do not know how to prove this for $n=2$ in its full generality. We can of course assume the groups i.e. the fields are infinite, otherwise counting the number of elements should be enough. On the other hand by using any non-central diagonal element as a parameter, one can define the field $K$ in the group $SL_2(K)$ as follows. Let $t_0$ be one such element. Let $T=C_{SL_2(K)}(t_0) \simeq K^*$ (torus). We may regard $T$ as the group of diagonal matrices with determinant 1. There are exactly two abelian subgroups $H$ of $SL_2(K)$ of the form $\langle h^T\cup\{1\} \rangle$ for any $1\neq h \in H$ and with the property that $H \cap Z(SL_2(K))=1$ , the strictly upper and lower triangular matrices, say $U$ and $V$ (unipotent) respectively. (Because $x = (1+x/2)^2 - 1^2 - (x/2)^2$ for any $x\in K$ , see below.) They are both isomorphic to the addive group of $K$ . Choose one of them, say $U$ . The choice does not matter as the automorphism ""transpose inverse"" interchanges them fixing $T$ . Denote the elements of $T$ by $t(x)$ where $x\in K^*$ and elements of $U$ by $u(y)$ where $y\in K$ . Then $T$ acts on $U$ as follows $u(y)^{t(x)} = u(x^2y)$ . Thus we get the subfield of $K$ generated by the squares. But since $x = (1+x/2)^2 - 1^2 - (x/2)^2$ for any $x\in K$ , the subfield generated by the squares is $K$ itself. Thus the field $K$ is definable with one parameter, namely $t_0$ . (Except that the group does not know the unit element 1 of the field, we only get an affine version of a field; to fix 1 of the field $K$ we need one more parameter,  but this is irrelevant to us). It follows that in the group $SL_2(L)$ both fields $K$ and $L$ are definable. In particular if the automorphism takes a non-central diagonalizable element of $SL_2(K)$ to a non-central diagonalizable element of $SL_2(L)$ , then we will necessarily have $K\simeq L$ . This will be so if we can distinguish diagonalizable elements of $SL_2(K)$ from its non-diagonalizable semisimple elements (i.e. diagonalizable in the algebraic closure) in a group theoretic way. If $K$ and $L$ are algebraically closed, all the semisimple elements will be diagonalizable, so there will be no problem, in this case $K$ will be isomorphic to $L$ .","Let and be two fields. Assume characteristics are not 2. I can show in a quite elementary way that if the statement holds, then for , the statement holds. But I do not know how to prove this for in its full generality. We can of course assume the groups i.e. the fields are infinite, otherwise counting the number of elements should be enough. On the other hand by using any non-central diagonal element as a parameter, one can define the field in the group as follows. Let be one such element. Let (torus). We may regard as the group of diagonal matrices with determinant 1. There are exactly two abelian subgroups of of the form for any and with the property that , the strictly upper and lower triangular matrices, say and (unipotent) respectively. (Because for any , see below.) They are both isomorphic to the addive group of . Choose one of them, say . The choice does not matter as the automorphism ""transpose inverse"" interchanges them fixing . Denote the elements of by where and elements of by where . Then acts on as follows . Thus we get the subfield of generated by the squares. But since for any , the subfield generated by the squares is itself. Thus the field is definable with one parameter, namely . (Except that the group does not know the unit element 1 of the field, we only get an affine version of a field; to fix 1 of the field we need one more parameter,  but this is irrelevant to us). It follows that in the group both fields and are definable. In particular if the automorphism takes a non-central diagonalizable element of to a non-central diagonalizable element of , then we will necessarily have . This will be so if we can distinguish diagonalizable elements of from its non-diagonalizable semisimple elements (i.e. diagonalizable in the algebraic closure) in a group theoretic way. If and are algebraically closed, all the semisimple elements will be diagonalizable, so there will be no problem, in this case will be isomorphic to .",K L SL_2(K) \simeq SL_2(L) \implies K \simeq L n \geq 2 SL_n(K) \simeq SL_n(L) \implies K \simeq L n=2 K SL_2(K) t_0 T=C_{SL_2(K)}(t_0) \simeq K^* T H SL_2(K) \langle h^T\cup\{1\} \rangle 1\neq h \in H H \cap Z(SL_2(K))=1 U V x = (1+x/2)^2 - 1^2 - (x/2)^2 x\in K K U T T t(x) x\in K^* U u(y) y\in K T U u(y)^{t(x)} = u(x^2y) K x = (1+x/2)^2 - 1^2 - (x/2)^2 x\in K K K t_0 K SL_2(L) K L SL_2(K) SL_2(L) K\simeq L SL_2(K) K L K L,"['linear-algebra', 'group-theory', 'classical-groups']"
79,Ratio of largest eigenvalue to sum of eigenvalues -- where to read about it?,Ratio of largest eigenvalue to sum of eigenvalues -- where to read about it?,,"Let $E_j$ be the $j$th largest-magnitude eigenvalue of a real symmetric $N \times N$ matrix $M$. I've found that the ratio $$\frac{|E_1|}{\sum_{j=1}^N{|E_j|}},$$ is a measure of the ""rank-one-ness"" of $M$. Qualitatively, the more similar the columns of $M$ are to each other, the higher the ratio. In my graduate research, this measure appears naturally for a specific class of matrices. I'm certain that there's been prior research on the properties and usefulness of this measure for deciding how well-aligned and similar the columns of a matrix are. For example, I've seen it used as a measure of ""compressibility"". Still, my searches haven't turned up much. Where can I find out more?","Let $E_j$ be the $j$th largest-magnitude eigenvalue of a real symmetric $N \times N$ matrix $M$. I've found that the ratio $$\frac{|E_1|}{\sum_{j=1}^N{|E_j|}},$$ is a measure of the ""rank-one-ness"" of $M$. Qualitatively, the more similar the columns of $M$ are to each other, the higher the ratio. In my graduate research, this measure appears naturally for a specific class of matrices. I'm certain that there's been prior research on the properties and usefulness of this measure for deciding how well-aligned and similar the columns of a matrix are. For example, I've seen it used as a measure of ""compressibility"". Still, my searches haven't turned up much. Where can I find out more?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'applications', 'spectral-theory']"
80,Extracting individual race results from Mario Kart final scores,Extracting individual race results from Mario Kart final scores,,"In Mario Kart, one ""cup"" involves 4 races, and after every race each racer gets points awarded based on what place they came in (better rank means more points). After playing it enough I grew curious about the mathematical problem of trying to reason backwards from the final scores to the individual race results - when is it possible and how might one do it? Formally, the problem goes like this: suppose we have a scoring vector $\mathbf{v}\in\mathbb{Z}^n$ with components that are strictly decreasing with index. There are $r$ permutation matrices $P_1,P_2,\dots,P_r$ that we don't have, but we do have the final score vector $\mathbf{f}=(P_1+P_2+\cdots+P_r)\mathbf{v}$. What conditions on our information - $\mathbf{v},r,\mathbf{f}$ - allow there to be a unique solution, and how might we go about computing them? I imagine if $r$ is small enough compared to $n$ and the scoring vector $\mathbf{v}$ is ""sparse"" or the components ""independent"" enough (a fuzzy intuition), there will probably be a unique solution, but it doesn't look at all amenable to linear algebra methods that I know of. More generally, we could understand the scoring vector to hail from a different type of vector space, or understand the score-distributing matrices $P_i$ to be from a different candidate set than permutation matrices (perhaps from a group representation of something other than the symmetric group?).","In Mario Kart, one ""cup"" involves 4 races, and after every race each racer gets points awarded based on what place they came in (better rank means more points). After playing it enough I grew curious about the mathematical problem of trying to reason backwards from the final scores to the individual race results - when is it possible and how might one do it? Formally, the problem goes like this: suppose we have a scoring vector $\mathbf{v}\in\mathbb{Z}^n$ with components that are strictly decreasing with index. There are $r$ permutation matrices $P_1,P_2,\dots,P_r$ that we don't have, but we do have the final score vector $\mathbf{f}=(P_1+P_2+\cdots+P_r)\mathbf{v}$. What conditions on our information - $\mathbf{v},r,\mathbf{f}$ - allow there to be a unique solution, and how might we go about computing them? I imagine if $r$ is small enough compared to $n$ and the scoring vector $\mathbf{v}$ is ""sparse"" or the components ""independent"" enough (a fuzzy intuition), there will probably be a unique solution, but it doesn't look at all amenable to linear algebra methods that I know of. More generally, we could understand the scoring vector to hail from a different type of vector space, or understand the score-distributing matrices $P_i$ to be from a different candidate set than permutation matrices (perhaps from a group representation of something other than the symmetric group?).",,"['linear-algebra', 'algorithms', 'recreational-mathematics']"
81,Proof of $\det(\textbf{ST})=\det(\textbf{S})\det(\textbf{T})$ in Penrose graphical notation,Proof of  in Penrose graphical notation,\det(\textbf{ST})=\det(\textbf{S})\det(\textbf{T}),"For two matrices $\textbf{S}$ and $\textbf{T}$ , a proof of $\det(\textbf{ST})=\det(\textbf{S})\det(\textbf{T})$ is given below in the diagrammatic tensor notation . Here $\det$ denotes the determinant. Why can the antisymmetrizing bar be inserted in the middle because ""there is already antisymmetry in the index lines""? For an introduction to the notation, you can refer to Figures 12.17 and 12.18 below.","For two matrices and , a proof of is given below in the diagrammatic tensor notation . Here denotes the determinant. Why can the antisymmetrizing bar be inserted in the middle because ""there is already antisymmetry in the index lines""? For an introduction to the notation, you can refer to Figures 12.17 and 12.18 below.",\textbf{S} \textbf{T} \det(\textbf{ST})=\det(\textbf{S})\det(\textbf{T}) \det,"['linear-algebra', 'matrices', 'tensors', 'penrose-graphical-notation']"
82,The number of non-singular $n\times n$ matrices over $\mathbb{F}_2$ with exactly $k$ non-zero entries,The number of non-singular  matrices over  with exactly  non-zero entries,n\times n \mathbb{F}_2 k,"Suppose $M_{n}^{k}$ is the number of regular matrices in $M_n(\mathbb{F}_2)$ , that have exactly $k$ non-zero entries.  Is there some sort of formula to calculate $M_n^k$ ? $$(k < n\;\lor\;k > n^2 - n + 1)\overset{\text{pigeonhole principle}}\implies M_n^k = 0$$ (in $1^{\text{st}}$ case we always have at least one zero row, in $2^{\text{nd}}$ case we always have at least two identical rows).  If $k = n$ , then all such regular matrices have to be permutation matrices. Thus $M_n^n = n!$ .  However, I do not know, how to deal with the situation, where $n < k < n^2 - n + 1$ . Any help will be appreciated.","Suppose is the number of regular matrices in , that have exactly non-zero entries.  Is there some sort of formula to calculate ? (in case we always have at least one zero row, in case we always have at least two identical rows).  If , then all such regular matrices have to be permutation matrices. Thus .  However, I do not know, how to deal with the situation, where . Any help will be appreciated.",M_{n}^{k} M_n(\mathbb{F}_2) k M_n^k (k < n\;\lor\;k > n^2 - n + 1)\overset{\text{pigeonhole principle}}\implies M_n^k = 0 1^{\text{st}} 2^{\text{nd}} k = n M_n^n = n! n < k < n^2 - n + 1,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'matrices', 'finite-fields']"
83,Simpson's Rule for Double Integrals,Simpson's Rule for Double Integrals,,"Simpson's Rule for double integrals: $$\int_a^b\int_c^df(x,y) \,dx \,dy$$  is given by $$S_{mn}=\frac{(b-a)(d-c)}{9mn} \sum_{i,j=0,0}^{m,n} W_{i+1,j+1} f(x_i,y_j) $$ where: $$W= \begin{pmatrix} 1&4&2&4& \ldots&4&1\\ 4&16&8&16&\ldots&16&4\\ 2&8&4&8&\ldots&8&2\\ \ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\\ 1&4&2&4&\ldots&4&1\\ \end{pmatrix}$$  and where $m$ and $n$ are the subdivisions of the domains along $x$ and $y$ respectively. They are obviously both even. The proof usually involves applying Simpson's Rule to the first integral $\int_a^bf(x,y)dx=g(x,y)$, then applying it to the second $\int_c^dg(x,y)dy$ thus obtaining the coefficients (or weights) $W$ of $f(x,y)$. I've thought of an alternative: first we find the area under $f(x_i,y_j)$ where $i$ is a fixed value but $y_j$ goes from $c$  to $d$ using Simpson's Composite Rule and fill the coefficients in a vertical matrix $$ \begin{pmatrix} 1\\ 4\\ 2\\ 4\\ 2\\ \vdots \\ 4\\ 1\\ \end{pmatrix} $$ Similarly for a fixed $j$: $$\begin{pmatrix} 1&4&2&4&2& \ldots &4&1\end{pmatrix}$$ By multiplying the vertical matrix by the horizontal one we get $W$. I thought of the integral (surface) as a linear combination of the $f(x_i,y_j)$'s, justifying the product of the two matrices as a composition of both iterations of Simpson's rule for each variable. Does any of this make sense? Is there a more formal proof of these statements? Also , can you recommend a textbook or some online resource where I can find more approximations like this one?","Simpson's Rule for double integrals: $$\int_a^b\int_c^df(x,y) \,dx \,dy$$  is given by $$S_{mn}=\frac{(b-a)(d-c)}{9mn} \sum_{i,j=0,0}^{m,n} W_{i+1,j+1} f(x_i,y_j) $$ where: $$W= \begin{pmatrix} 1&4&2&4& \ldots&4&1\\ 4&16&8&16&\ldots&16&4\\ 2&8&4&8&\ldots&8&2\\ \ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\\ 1&4&2&4&\ldots&4&1\\ \end{pmatrix}$$  and where $m$ and $n$ are the subdivisions of the domains along $x$ and $y$ respectively. They are obviously both even. The proof usually involves applying Simpson's Rule to the first integral $\int_a^bf(x,y)dx=g(x,y)$, then applying it to the second $\int_c^dg(x,y)dy$ thus obtaining the coefficients (or weights) $W$ of $f(x,y)$. I've thought of an alternative: first we find the area under $f(x_i,y_j)$ where $i$ is a fixed value but $y_j$ goes from $c$  to $d$ using Simpson's Composite Rule and fill the coefficients in a vertical matrix $$ \begin{pmatrix} 1\\ 4\\ 2\\ 4\\ 2\\ \vdots \\ 4\\ 1\\ \end{pmatrix} $$ Similarly for a fixed $j$: $$\begin{pmatrix} 1&4&2&4&2& \ldots &4&1\end{pmatrix}$$ By multiplying the vertical matrix by the horizontal one we get $W$. I thought of the integral (surface) as a linear combination of the $f(x_i,y_j)$'s, justifying the product of the two matrices as a composition of both iterations of Simpson's rule for each variable. Does any of this make sense? Is there a more formal proof of these statements? Also , can you recommend a textbook or some online resource where I can find more approximations like this one?",,"['linear-algebra', 'integration', 'matrices', 'numerical-methods', 'simpsons-rule']"
84,Vector spaces. When in the real world are we checking if it's a vector space or not?,Vector spaces. When in the real world are we checking if it's a vector space or not?,,I am reading this text: When in the real world are we checking to see if sets are vector spaces or not? The examples above seem like really specific sets... Are there any places where we redefined scalar multiplication like this?,I am reading this text: When in the real world are we checking to see if sets are vector spaces or not? The examples above seem like really specific sets... Are there any places where we redefined scalar multiplication like this?,,['linear-algebra']
85,"Is there a matrix $A$ such that for every other matrix $B$, we have $\mbox{tr}(AB) = 0$?","Is there a matrix  such that for every other matrix , we have ?",A B \mbox{tr}(AB) = 0,"I'm struggling to prove/disprove this notion. I've figured if such matrix exists, it has to be nilpotent and non-invertible, and the sum of its eigenvalues is 0. Can anyone chip in? Edit : Oh yeah, $A \neq 0$","I'm struggling to prove/disprove this notion. I've figured if such matrix exists, it has to be nilpotent and non-invertible, and the sum of its eigenvalues is 0. Can anyone chip in? Edit : Oh yeah, $A \neq 0$",,"['linear-algebra', 'matrices']"
86,Eigenvalues for $4\times 4$ matrix,Eigenvalues for  matrix,4\times 4,"Show that $0,2,4$ are the eigenvalues for the matrix $A$:  $$A=\pmatrix{ 2  &  -1 & -1 &  0  \\ -1  & 3 &  -1 &  -1  \\ -1  &  -1 &  3 & -1  \\ 0  &  -1 & -1 &  2  \\ }$$ and conclude that $0,2,4$ are the only eigenvalues for $A$. I know that you can find the eigenvalues by finding the $\det(A-\lambda \cdot I)$, but it seems to me that the computation will be rather difficult to compute as it is a $4 \times 4$ matrix. My question: is there an easier method to calculate the eigenvalues of $A$?  And if I have to conclude that these are the only eigenvalues, is there a theorem that argues how many eigenvalues a matrix can have?","Show that $0,2,4$ are the eigenvalues for the matrix $A$:  $$A=\pmatrix{ 2  &  -1 & -1 &  0  \\ -1  & 3 &  -1 &  -1  \\ -1  &  -1 &  3 & -1  \\ 0  &  -1 & -1 &  2  \\ }$$ and conclude that $0,2,4$ are the only eigenvalues for $A$. I know that you can find the eigenvalues by finding the $\det(A-\lambda \cdot I)$, but it seems to me that the computation will be rather difficult to compute as it is a $4 \times 4$ matrix. My question: is there an easier method to calculate the eigenvalues of $A$?  And if I have to conclude that these are the only eigenvalues, is there a theorem that argues how many eigenvalues a matrix can have?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
87,Vector Spaces: Redundant Axiom?,Vector Spaces: Redundant Axiom?,,Question Why are the axioms for vector space independent? More precisely $1x=x$ seems redundant... (I take the axioms from: Wikipedia ) Explanation One has for zero vector: $$\lambda0+\lambda0=\lambda(0+0)=\lambda0\implies\lambda0=0$$ And for zero scalar: $$0x+0x=(0+0)x=0x\implies0x=0$$ In familiar form: $$\lambda x=0\implies\lambda=0\lor x=0$$ Threrefore one calculates: $$1(1x+x^{-1})=1(1x)+1(x^{-1})=(11)x+1(x^{-1})=1x+1(x^{-1})=1(x+x^{-1})=10=0$$ Hence for nontrivial field: $$1\neq0\implies1x+x^{-1}=0\implies1x=x$$ But where is the flaw in that check??,Question Why are the axioms for vector space independent? More precisely $1x=x$ seems redundant... (I take the axioms from: Wikipedia ) Explanation One has for zero vector: $$\lambda0+\lambda0=\lambda(0+0)=\lambda0\implies\lambda0=0$$ And for zero scalar: $$0x+0x=(0+0)x=0x\implies0x=0$$ In familiar form: $$\lambda x=0\implies\lambda=0\lor x=0$$ Threrefore one calculates: $$1(1x+x^{-1})=1(1x)+1(x^{-1})=(11)x+1(x^{-1})=1x+1(x^{-1})=1(x+x^{-1})=10=0$$ Hence for nontrivial field: $$1\neq0\implies1x+x^{-1}=0\implies1x=x$$ But where is the flaw in that check??,,"['linear-algebra', 'vector-spaces', 'definition', 'axioms']"
88,If $A+B=AB$ then $AB=BA$,If  then,A+B=AB AB=BA,"I was doing the problem $$ A+B=AB\implies AB=BA. $$ $AB=BA$ means they're invertible, but I can't figure out how to show that $A+B=AB$ implies invertibility.","I was doing the problem $$ A+B=AB\implies AB=BA. $$ $AB=BA$ means they're invertible, but I can't figure out how to show that $A+B=AB$ implies invertibility.",,"['linear-algebra', 'matrices']"
89,An inequality on trace of product of two matrices,An inequality on trace of product of two matrices,,"Suppose we have two $n \times n$ positive semidefinite matrices, $A$ and $B$, such that $\mbox{tr}(A), \mbox{tr}(B) \le 1$. Can we say anything about $\mbox{tr}(AB)$? Is $\mbox{tr}(AB) \le 1 $ too?","Suppose we have two $n \times n$ positive semidefinite matrices, $A$ and $B$, such that $\mbox{tr}(A), \mbox{tr}(B) \le 1$. Can we say anything about $\mbox{tr}(AB)$? Is $\mbox{tr}(AB) \le 1 $ too?",,"['linear-algebra', 'matrices', 'inequality', 'operator-theory', 'trace']"
90,"If $A^2$ and $B^2$ are similar matrices, do $A$ and $B$ have to be similar?","If  and  are similar matrices, do  and  have to be similar?",A^2 B^2 A B,"I know that the converse is true; that is, if A and B are similar matrices, then $A^2$ and $B^2$ are similar . However, I'm not sure about the reverse.","I know that the converse is true; that is, if A and B are similar matrices, then $A^2$ and $B^2$ are similar . However, I'm not sure about the reverse.",,['linear-algebra']
91,Solving a system of equations with 3 variables in under a minute,Solving a system of equations with 3 variables in under a minute,,"2x + y = 2 2y + z = 8 2z + x = 7  Quantity I: The average value of x, y, and z. Quantity II: 2  Which of the following is true:  A) I is bigger than II B) II is bigger than I C) I is equal to II D) Insufficient information to determine This is a question on a (home-made) SAT-like test where you have a minute per question. I solve it in the most basic way possible (exchanging one variable for another, solving for that, exchanging for the variable in the next equation, etc) but that is already cumbersome and the numbers you get to work with here are very unforgiving ($x = -\frac19$ , $y=2+\frac29$, $z=7+\frac {1}{18}$) for a test where you have very little time and no calculator. Is there a faster way to solve it?","2x + y = 2 2y + z = 8 2z + x = 7  Quantity I: The average value of x, y, and z. Quantity II: 2  Which of the following is true:  A) I is bigger than II B) II is bigger than I C) I is equal to II D) Insufficient information to determine This is a question on a (home-made) SAT-like test where you have a minute per question. I solve it in the most basic way possible (exchanging one variable for another, solving for that, exchanging for the variable in the next equation, etc) but that is already cumbersome and the numbers you get to work with here are very unforgiving ($x = -\frac19$ , $y=2+\frac29$, $z=7+\frac {1}{18}$) for a test where you have very little time and no calculator. Is there a faster way to solve it?",,"['linear-algebra', 'systems-of-equations']"
92,Is $SO_n({\mathbb R})$ a divisible group?,Is  a divisible group?,SO_n({\mathbb R}),"The title says it all ... Formally, if  $SO_n(\mathbb R)=\lbrace A\in M_n({\mathbb R}) |AA^{T}=I_n, {\sf det}(A)=1 \rbrace$ and $W\in SO_n(\mathbb R)$, is it true that for every integer $p$,  there is a $V\in SO_n(\mathbb R)$ satisfying $V^p=W$  ? This is obvious when $n=2$, because rotations in the plane are defined by an angle which can be divided at will.","The title says it all ... Formally, if  $SO_n(\mathbb R)=\lbrace A\in M_n({\mathbb R}) |AA^{T}=I_n, {\sf det}(A)=1 \rbrace$ and $W\in SO_n(\mathbb R)$, is it true that for every integer $p$,  there is a $V\in SO_n(\mathbb R)$ satisfying $V^p=W$  ? This is obvious when $n=2$, because rotations in the plane are defined by an angle which can be divided at will.",,"['linear-algebra', 'group-theory', 'lie-groups', 'divisible-groups']"
93,"Are positive definite matrices robust to ""small changes""?","Are positive definite matrices robust to ""small changes""?",,Let $A$ be a positive-definite matrix and let $B$ be some other symmetric matrix. Consider the matrix $$ C=A+\varepsilon B. $$ for some $\varepsilon>0$. Is it true that for $\varepsilon$ small enough $C$ is also positive definite?,Let $A$ be a positive-definite matrix and let $B$ be some other symmetric matrix. Consider the matrix $$ C=A+\varepsilon B. $$ for some $\varepsilon>0$. Is it true that for $\varepsilon$ small enough $C$ is also positive definite?,,"['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices', 'positive-semidefinite']"
94,Prove that the union of three subspaces of $V$ is a subspace iff one of the subspaces contains the other two.,Prove that the union of three subspaces of  is a subspace iff one of the subspaces contains the other two.,V,"Prove that the union of three subspaces of V is a subspace iff one of the subspaces contains the other two. I can do this problem when I am working in only two subspaces of $V$ but I don't know how to do it with three. What I tried is: If one of the subspaces contains the other two, Then their union is obviously a subspace because the subspace that contains them is a subspace. (Is this sufficient??). If the union of three subspaces is a subspace..... How do I prove that one of the subspaces must contain the other two from here? *When proving this for two I said that there is an element in one of the subspaces that is not the other and proved by contradiction that one of the subspaces must be contained in the other. How would I do this for three?","Prove that the union of three subspaces of V is a subspace iff one of the subspaces contains the other two. I can do this problem when I am working in only two subspaces of $V$ but I don't know how to do it with three. What I tried is: If one of the subspaces contains the other two, Then their union is obviously a subspace because the subspace that contains them is a subspace. (Is this sufficient??). If the union of three subspaces is a subspace..... How do I prove that one of the subspaces must contain the other two from here? *When proving this for two I said that there is an element in one of the subspaces that is not the other and proved by contradiction that one of the subspaces must be contained in the other. How would I do this for three?",,"['linear-algebra', 'vector-spaces']"
95,Solve the system of equations for $\sqrt{xy}$,Solve the system of equations for,\sqrt{xy},"$$x + y\sqrt{x} = \frac{95}{8}$$ $$y + x\sqrt{y} = \frac{93}{8}$$ $$x, y \in \mathbb{R}$$ I can't solve this system of equations I got asked in a group. I added and substracted them to find the following equations but I don't really know what to do with them. $$\left(\sqrt{x}-\sqrt{y}\right)\left(\sqrt{x}+\sqrt{y}-\sqrt{xy}\right) = \frac{1}{4}$$ $$\left(\sqrt{x}+\sqrt{y}\right)\left(\sqrt{x}+\sqrt{y}+\sqrt{xy}\right) - 2\sqrt{xy} = \frac{47}{2}$$ This is all I have. I checked the answer using Wolframalpha and it was $\frac{15}{4}$ for $\sqrt{xy}$ . I have been looking for this question's solution but didn't find any related question/solution on Stack Exchange or Web. Sorry if it is duplicate, thank you in advance.","I can't solve this system of equations I got asked in a group. I added and substracted them to find the following equations but I don't really know what to do with them. This is all I have. I checked the answer using Wolframalpha and it was for . I have been looking for this question's solution but didn't find any related question/solution on Stack Exchange or Web. Sorry if it is duplicate, thank you in advance.","x + y\sqrt{x} = \frac{95}{8} y + x\sqrt{y} = \frac{93}{8} x, y \in \mathbb{R} \left(\sqrt{x}-\sqrt{y}\right)\left(\sqrt{x}+\sqrt{y}-\sqrt{xy}\right) = \frac{1}{4} \left(\sqrt{x}+\sqrt{y}\right)\left(\sqrt{x}+\sqrt{y}+\sqrt{xy}\right) - 2\sqrt{xy} = \frac{47}{2} \frac{15}{4} \sqrt{xy}","['linear-algebra', 'systems-of-equations']"
96,Find intersection of two 3D lines,Find intersection of two 3D lines,,"I have two lines $(5,5,4) (10,10,6)$ and $(5,5,5) (10,10,3)$ with same $x$, $y$ and difference in $z$ values. Please some body tell me how can I find the intersection of these lines. EDIT: By using the answer given by coffemath I would able to find the intersection point for the above given points. But I'm getting a problem for $(6,8,4) (12,15,4)$ and $(6,8,2) (12,15,6)$. I'm unable to calculate the common point for these points as it is resulting in Zero. Any ideas to resolve this? Thanks,  Kumar.","I have two lines $(5,5,4) (10,10,6)$ and $(5,5,5) (10,10,3)$ with same $x$, $y$ and difference in $z$ values. Please some body tell me how can I find the intersection of these lines. EDIT: By using the answer given by coffemath I would able to find the intersection point for the above given points. But I'm getting a problem for $(6,8,4) (12,15,4)$ and $(6,8,2) (12,15,6)$. I'm unable to calculate the common point for these points as it is resulting in Zero. Any ideas to resolve this? Thanks,  Kumar.",,"['linear-algebra', 'geometry', 'analytic-geometry']"
97,What is the fastest way to find the characteristic polynomial of a matrix?,What is the fastest way to find the characteristic polynomial of a matrix?,,"Finding the characteristic polynomial of a matrix of order $n$ is a tedious and boring task for $n > 2$. I know that: the coefficient of $\lambda^n$ is $(-1)^n$, the coefficient of $\lambda^{n-1}$ is $(-1)^{n-1}(a_{11} + a_{22} + \dots + a_{nn})$, the constant term is $\det{A}$. When finding the coefficient of the linear term $\lambda$ of the characteristic polynomial of a $3\times 3$ matrix, one has to calculate the determinant of the matrix $A - \lambda I_n$ anyway. (But you don't have to sum all the terms, only the linear terms.) Does anybody know a faster way?","Finding the characteristic polynomial of a matrix of order $n$ is a tedious and boring task for $n > 2$. I know that: the coefficient of $\lambda^n$ is $(-1)^n$, the coefficient of $\lambda^{n-1}$ is $(-1)^{n-1}(a_{11} + a_{22} + \dots + a_{nn})$, the constant term is $\det{A}$. When finding the coefficient of the linear term $\lambda$ of the characteristic polynomial of a $3\times 3$ matrix, one has to calculate the determinant of the matrix $A - \lambda I_n$ anyway. (But you don't have to sum all the terms, only the linear terms.) Does anybody know a faster way?",,"['linear-algebra', 'matrices', 'determinant', 'eigenvalues-eigenvectors']"
98,"Prove that the real vector space consisting of all continuous, real-valued functions on the interval $[0,1]$ is infinite-dimensional.","Prove that the real vector space consisting of all continuous, real-valued functions on the interval  is infinite-dimensional.","[0,1]","Prove that the real vector space consisting of all continuous, real-valued functions on the interval $[0,1]$ is infinite-dimensional. Clearly it's infinite dimensional, because if you consider say $P (\mathbb{F})$ on $[0,1]$, then there are an infinite amount of continuous real-valued functions on the interval, but how do I prove this?","Prove that the real vector space consisting of all continuous, real-valued functions on the interval $[0,1]$ is infinite-dimensional. Clearly it's infinite dimensional, because if you consider say $P (\mathbb{F})$ on $[0,1]$, then there are an infinite amount of continuous real-valued functions on the interval, but how do I prove this?",,['linear-algebra']
99,Is this matrix obviously positive definite?,Is this matrix obviously positive definite?,,"Consider the matrix $A$ whose elements are $A_{ij} = a^{|i-j|}$ for $-1<a<1$ and $i,j=1,\dots,n$ . E.g., for $n=4$ , the matrix is $$A = \left[ \begin{matrix} 1 & a & a^2 & a^3 \\ a & 1 & a & a^2 \\ a^2 & a & 1 & a \\ a^3 & a^2 & a & 1 \end{matrix} \right]$$ Is this matrix always positive definite? If so, what is the simplest way to see that? I strongly suspect that the matrix is positive definite for all $n$ and $a$ , but am having trouble coming up with a proof. Extra credit: The eigenvalues seem to lie within an interval $[\lambda_{\rm min}, \lambda_{\rm max}]$ which is a function of $a$ but not of $n$ . For example, for $a=1/2$ all eigenvalues lie in $[1/3, 3]$ . Is it true for for general $a$ , the eigenvalues lie in $[\lambda(a)^{-1}, \lambda(a)]$ ? If so, what is $\lambda(a)$ ? Also, the eigenvectors seem to have a particularly regular form. In particular, they look like that could be expressed as simple combinations of trigonometric functions. Is this the case?","Consider the matrix whose elements are for and . E.g., for , the matrix is Is this matrix always positive definite? If so, what is the simplest way to see that? I strongly suspect that the matrix is positive definite for all and , but am having trouble coming up with a proof. Extra credit: The eigenvalues seem to lie within an interval which is a function of but not of . For example, for all eigenvalues lie in . Is it true for for general , the eigenvalues lie in ? If so, what is ? Also, the eigenvectors seem to have a particularly regular form. In particular, they look like that could be expressed as simple combinations of trigonometric functions. Is this the case?","A A_{ij} = a^{|i-j|} -1<a<1 i,j=1,\dots,n n=4 A = \left[
\begin{matrix}
1 & a & a^2 & a^3 \\
a & 1 & a & a^2 \\
a^2 & a & 1 & a \\
a^3 & a^2 & a & 1
\end{matrix}
\right] n a [\lambda_{\rm min}, \lambda_{\rm max}] a n a=1/2 [1/3, 3] a [\lambda(a)^{-1}, \lambda(a)] \lambda(a)","['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices', 'toeplitz-matrices']"
