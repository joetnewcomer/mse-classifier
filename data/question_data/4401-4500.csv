,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Integral Asymptotics for inhomogenous phase,Integral Asymptotics for inhomogenous phase,,"I'm looking for asymptotics for an integral of the form: $$F(n):=\int_{1/2-i\infty}^{1/2+i\infty} e^{\phi(n,z)}dz$$ where $\phi(n,z)=(n-n^3)\log(1-z)+n^2\log(1+z)-n\log(z)$. One can solve for the saddle points of $\phi$: $$z_{1,2}=\frac{1-n-n^2\pm \sqrt{1-6n+3n^2+2n^3+n^4}}{2(n^2-n)},$$ with taylor series: $$z_1=-1-\frac{2}{n}+O(1/n^2)$$ $$z_2=\frac{1}{n^2}+O(1/n^3).$$ It looks like $z_2$ is the maximum saddle, and $z_1$ the minimum. So to get asymptotics of $F(n)$, we need to deform the vertical line contour to the steepest-descent contour, passing through $z_2$. The steepest descent contour looks like this: I have some questions. First, I'm having trouble solving for the contour of steepest-descent through $z_2$, in other words $Im(\phi(n,z))=0$, passing through $z_2$. The equation is rather nasty, and probably transcendental. Is it sufficient to somehow approximate this contour to some order in $n$? The issue is that I'm thinking I need to parametrize the steepest descent contour with some running variable $t$, so that $\Gamma_t=z_2+f_n(t)$, where $f_n(0)=0$. Also, $z_2$ depends on $n$, and so I'm thinking I need a parametrization of $\Gamma_t$ that's somehow uniform in $n,t$ for $t$ near zero and $n$ large. Ultimately, it looks like I need to perform some sort of rescaling in the integral in terms of $z$. I was thinking of trying $u=z\cdot z_1$, so that the saddle point gets moved to $u=1$. But I'm still stuck in getting some kind of a uniform estimate on $\phi(n,z)$ near $z_1$, along the contour of steepest descent. Note that the integral has singularities at $z=\pm 1$ and $z=0$. In particular, $z_2\rightarrow 0$, so this further complicates taylor expansions of $\phi(n,z)$ near $z_2$. I've also noticed that $\phi^{(k)}(n,z_1)$ looks like a growing polynomial in $n$, e.g. $n^k$, so I can't seem to truncate taylor series for $\phi(n,z)$ around $z_1$ so that it's uniform in $n,z$. I've never seen literature on such inhomogenous steepest descent problems. A reference would be sincerely appreciated!","I'm looking for asymptotics for an integral of the form: $$F(n):=\int_{1/2-i\infty}^{1/2+i\infty} e^{\phi(n,z)}dz$$ where $\phi(n,z)=(n-n^3)\log(1-z)+n^2\log(1+z)-n\log(z)$. One can solve for the saddle points of $\phi$: $$z_{1,2}=\frac{1-n-n^2\pm \sqrt{1-6n+3n^2+2n^3+n^4}}{2(n^2-n)},$$ with taylor series: $$z_1=-1-\frac{2}{n}+O(1/n^2)$$ $$z_2=\frac{1}{n^2}+O(1/n^3).$$ It looks like $z_2$ is the maximum saddle, and $z_1$ the minimum. So to get asymptotics of $F(n)$, we need to deform the vertical line contour to the steepest-descent contour, passing through $z_2$. The steepest descent contour looks like this: I have some questions. First, I'm having trouble solving for the contour of steepest-descent through $z_2$, in other words $Im(\phi(n,z))=0$, passing through $z_2$. The equation is rather nasty, and probably transcendental. Is it sufficient to somehow approximate this contour to some order in $n$? The issue is that I'm thinking I need to parametrize the steepest descent contour with some running variable $t$, so that $\Gamma_t=z_2+f_n(t)$, where $f_n(0)=0$. Also, $z_2$ depends on $n$, and so I'm thinking I need a parametrization of $\Gamma_t$ that's somehow uniform in $n,t$ for $t$ near zero and $n$ large. Ultimately, it looks like I need to perform some sort of rescaling in the integral in terms of $z$. I was thinking of trying $u=z\cdot z_1$, so that the saddle point gets moved to $u=1$. But I'm still stuck in getting some kind of a uniform estimate on $\phi(n,z)$ near $z_1$, along the contour of steepest descent. Note that the integral has singularities at $z=\pm 1$ and $z=0$. In particular, $z_2\rightarrow 0$, so this further complicates taylor expansions of $\phi(n,z)$ near $z_2$. I've also noticed that $\phi^{(k)}(n,z_1)$ looks like a growing polynomial in $n$, e.g. $n^k$, so I can't seem to truncate taylor series for $\phi(n,z)$ around $z_1$ so that it's uniform in $n,z$. I've never seen literature on such inhomogenous steepest descent problems. A reference would be sincerely appreciated!",,"['calculus', 'real-analysis', 'complex-analysis', 'asymptotics']"
1,How can using a different definition for the integral be useful?,How can using a different definition for the integral be useful?,,"It's often said that the Lebesgue integral is superior to the Riemann integral because it satisfies nicer properties, for instance things like $$\lim_{n\to\infty} \int f_n = \int \lim_{n\to\infty} f_n$$ But if in the course of a problem we're blocked by the fact that our (Riemann) integrals aren't satsifying some mathematical property, how could it possibly solve our problem to change the definition of the integral ? The original problem statement, after all, was formulated for Riemann integrals, so by changing the definition of the integral, we're surely addressing a different problem . I realize the Lebesgue and Riemann definitions agree when they're both defined, but that's just the thing - the cases in which the Lebesgue integral is useful are precisely those cases in which the Riemann integral isn't defined. Imagine I was doing algebra, investigating some particular ring, and I was bothered by the fact that the ring wasn't principal. I wouldn't be able to get out of that trouble by simply changing the definition of ""principal"". Otherwise I may as well just say ""by definition, all rings are Jack Principal, so my ring is principal"". What the question really boils down to is: Why does changing the definition of an integral not change the nature of the problem so much as to make any conclusions irrelevant to the original problem? Maybe the best kind of answer to this question would be an example of a problem which is satisfactorily solved by changing the definition of the integral, but I'm open to any explanations you feel address this confusion.","It's often said that the Lebesgue integral is superior to the Riemann integral because it satisfies nicer properties, for instance things like $$\lim_{n\to\infty} \int f_n = \int \lim_{n\to\infty} f_n$$ But if in the course of a problem we're blocked by the fact that our (Riemann) integrals aren't satsifying some mathematical property, how could it possibly solve our problem to change the definition of the integral ? The original problem statement, after all, was formulated for Riemann integrals, so by changing the definition of the integral, we're surely addressing a different problem . I realize the Lebesgue and Riemann definitions agree when they're both defined, but that's just the thing - the cases in which the Lebesgue integral is useful are precisely those cases in which the Riemann integral isn't defined. Imagine I was doing algebra, investigating some particular ring, and I was bothered by the fact that the ring wasn't principal. I wouldn't be able to get out of that trouble by simply changing the definition of ""principal"". Otherwise I may as well just say ""by definition, all rings are Jack Principal, so my ring is principal"". What the question really boils down to is: Why does changing the definition of an integral not change the nature of the problem so much as to make any conclusions irrelevant to the original problem? Maybe the best kind of answer to this question would be an example of a problem which is satisfactorily solved by changing the definition of the integral, but I'm open to any explanations you feel address this confusion.",,"['real-analysis', 'integration', 'soft-question', 'lebesgue-integral']"
2,"Question 2.25 on Folland, Real Analysis","Question 2.25 on Folland, Real Analysis",,"I'm trying to solve the exercises from Real Analysis - Moderns Techniques and Their applications by Folland and I have questios about the exercise 2.25 b). The statement: Let $f(x) = x^{-1/2}$ if $0<x<1$ and f(x)=0 otherwise. Let ${r_n}$ be and enumeration of the rationals, and set $g(x) = \sum_{n=1}^\infty 2^{-n} f(x-r_n)$. Then g is discontinuous at every point and unbounded on every interval, and it remains so after any modification on a Lebesgue null set. It's proved before (item a) that $g$ is finite a.e.. If $g(x)<+\infty$, it's not so difficult to prove that $g$ is discontinuous at $x$ (there is a solution here: http://www.math.mcgill.ca/spicard/folland2.pdf ) but I can't prove the same when $g(x)=+\infty$. I'm assuming that I have to see $g$ as a function $g:\mathbb R \longrightarrow [0,+\infty]$ and use the usual topology that Folland put in this set. So, if I'm interpreting the question correctly, how can I prove that $g$ is discontinuous in a point $x$ such that $g(x)=+\infty$? The prove made in previously case doesn't help, because it shows that $g(x)$ can assume high values.","I'm trying to solve the exercises from Real Analysis - Moderns Techniques and Their applications by Folland and I have questios about the exercise 2.25 b). The statement: Let $f(x) = x^{-1/2}$ if $0<x<1$ and f(x)=0 otherwise. Let ${r_n}$ be and enumeration of the rationals, and set $g(x) = \sum_{n=1}^\infty 2^{-n} f(x-r_n)$. Then g is discontinuous at every point and unbounded on every interval, and it remains so after any modification on a Lebesgue null set. It's proved before (item a) that $g$ is finite a.e.. If $g(x)<+\infty$, it's not so difficult to prove that $g$ is discontinuous at $x$ (there is a solution here: http://www.math.mcgill.ca/spicard/folland2.pdf ) but I can't prove the same when $g(x)=+\infty$. I'm assuming that I have to see $g$ as a function $g:\mathbb R \longrightarrow [0,+\infty]$ and use the usual topology that Folland put in this set. So, if I'm interpreting the question correctly, how can I prove that $g$ is discontinuous in a point $x$ such that $g(x)=+\infty$? The prove made in previously case doesn't help, because it shows that $g(x)$ can assume high values.",,['real-analysis']
3,Find all continuous functions $f:\mathbb R\to\mathbb R$ such that $f(x)^2-2\big\lfloor2|f(2x)|\big\rfloor f(x)+\big\lfloor{4f(2x)^2-1}\big\rfloor=0$.,Find all continuous functions  such that .,f:\mathbb R\to\mathbb R f(x)^2-2\big\lfloor2|f(2x)|\big\rfloor f(x)+\big\lfloor{4f(2x)^2-1}\big\rfloor=0,"Find all continuous functions $f:\mathbb{R} \to \mathbb{R}$ such that $$f(x)^2 - 2\Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor f(x) + \Bigl\lfloor{4f(2x)^2-1}\Bigr\rfloor = 0\,\text.$$ Here is my attempt: $f(x)^2 - 2\Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor f(x) + \Bigl\lfloor{4f(2x)^2-1}\Bigr\rfloor = 0$ $\implies \biggl(f(x) - \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor\biggr)^2 -1 = 0$ $\implies f(x) - \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor = \pm1\,$ . We have two cases: When: $ f(x) = 1+ \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor$ so we can conclude these: since $\ \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor\geq 0 \ \& \ 1 \geq 1 $ we have $\ \ \forall x \in \mathbb{R}\ \ :\ f(x) \geq 1\,$ . from above note and $\ \left \lfloor{2f(2x)}\right \rfloor \in \mathbb{N} \ \ \& \ 1 \in \mathbb{N} $ we have $\ \ \forall x \in \mathbb{R}\ \ :\ f(x) \in \mathbb{N}\,$ . Similary, When: $ f(x) = -1+ \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor$ so we can conclude these: since $\ \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor\geq 0 \ \& \ -1 \geq -1 $ we have $\ \ \forall x \in \mathbb{R}\ \ :\ f(x) \geq -1\,$ . from above note and $\ \left \lfloor{2f(2x)}\right \rfloor \in \mathbb{Z} \ \ \& \ 1 \in \mathbb{Z} $ we have $\ \ \forall x \in \mathbb{R}\ \ :\ f(x) \in \mathbb{Z}\,$ . Now if $f\ $ is continuous on $\ \mathbb{R}\ $ and $ \left \lfloor{f(x)}\right \rfloor$ is continuous on $(k, k+1)$ then $ \left \lfloor{2f(x)}\right \rfloor$ is continuous on $\left (\frac{k}{2}, \frac{k+1}{2} \right )$ . After that I don't know what to do. I hope I am on the right path.","Find all continuous functions such that Here is my attempt: . We have two cases: When: so we can conclude these: since we have . from above note and we have . Similary, When: so we can conclude these: since we have . from above note and we have . Now if is continuous on and is continuous on then is continuous on . After that I don't know what to do. I hope I am on the right path.","f:\mathbb{R} \to \mathbb{R} f(x)^2 - 2\Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor f(x) + \Bigl\lfloor{4f(2x)^2-1}\Bigr\rfloor = 0\,\text. f(x)^2 - 2\Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor f(x) + \Bigl\lfloor{4f(2x)^2-1}\Bigr\rfloor = 0 \implies \biggl(f(x) - \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor\biggr)^2 -1 = 0 \implies f(x) - \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor = \pm1\,  f(x) = 1+ \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor \ \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor\geq 0 \ \& \ 1 \geq 1  \ \ \forall x \in \mathbb{R}\ \ :\ f(x) \geq 1\, \ \left \lfloor{2f(2x)}\right \rfloor \in \mathbb{N} \ \ \& \ 1 \in \mathbb{N}  \ \ \forall x \in \mathbb{R}\ \ :\ f(x) \in \mathbb{N}\,  f(x) = -1+ \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor \ \Bigl\lfloor\,2\,\bigl|f(2x)\bigr|\,\Bigr\rfloor\geq 0 \ \& \ -1 \geq -1  \ \ \forall x \in \mathbb{R}\ \ :\ f(x) \geq -1\, \ \left \lfloor{2f(2x)}\right \rfloor \in \mathbb{Z} \ \ \& \ 1 \in \mathbb{Z}  \ \ \forall x \in \mathbb{R}\ \ :\ f(x) \in \mathbb{Z}\, f\  \ \mathbb{R}\   \left \lfloor{f(x)}\right \rfloor (k, k+1)  \left \lfloor{2f(x)}\right \rfloor \left (\frac{k}{2}, \frac{k+1}{2} \right )","['real-analysis', 'functional-equations']"
4,"Continuous bijections between $\mathbb{R}^2$ and $\mathbb{R}^2 \setminus \{(0,0)\}$",Continuous bijections between  and,"\mathbb{R}^2 \mathbb{R}^2 \setminus \{(0,0)\}","It is well-known that $\mathbb{R}^2$ is not homeomorphic to $\mathbb{R}^2 \setminus \{(0,0)\}$. I have two questions. a) Does there exist a continuous bijection $f: \mathbb{R}^2 \to \mathbb{R}^2 \setminus \{(0,0)\}$ ? b) Does there exist a continuous bijection $g: \mathbb{R}^2 \setminus \{(0,0)\} \to \mathbb{R}^2$ ? Thank you very much for your answers in advance!","It is well-known that $\mathbb{R}^2$ is not homeomorphic to $\mathbb{R}^2 \setminus \{(0,0)\}$. I have two questions. a) Does there exist a continuous bijection $f: \mathbb{R}^2 \to \mathbb{R}^2 \setminus \{(0,0)\}$ ? b) Does there exist a continuous bijection $g: \mathbb{R}^2 \setminus \{(0,0)\} \to \mathbb{R}^2$ ? Thank you very much for your answers in advance!",,"['real-analysis', 'general-topology']"
5,"supremum of $\int |\int f(x)-f(y)\,dy| \,dx $.",supremum of .,"\int |\int f(x)-f(y)\,dy| \,dx ","Let $A_k$, $k\in \mathbb{N}$, be the family of $C^\infty([0,1])$ functions defined by $$ A_k=\{||f^{(j)}||_{\infty}\le 1,\;\;0\le j \le k \} $$ where $||\cdot||_{\infty}$ denotes the supremum norm over $[0,1]$ and $f^{(k)}$ is the k-th derivative of $f$.  We define $$ \sigma_k=\sup \left\{\int_0^1\left|\int_0^1f(x)-f(y)\,dy\right|\,dx ,\;\; f\in A_k\right\} $$ What is the value of $\sigma_k$? Edit: I think $\sigma_0=1$. I believe $\sigma_k= \frac{1}{4}$ for all $k>0$ (with $f=x$ the supremum is reached). It is not homework and is not related to my work. Just curious. I hope you find interesting too. (((The problem turned out to be uninteresting.))) Thanks.","Let $A_k$, $k\in \mathbb{N}$, be the family of $C^\infty([0,1])$ functions defined by $$ A_k=\{||f^{(j)}||_{\infty}\le 1,\;\;0\le j \le k \} $$ where $||\cdot||_{\infty}$ denotes the supremum norm over $[0,1]$ and $f^{(k)}$ is the k-th derivative of $f$.  We define $$ \sigma_k=\sup \left\{\int_0^1\left|\int_0^1f(x)-f(y)\,dy\right|\,dx ,\;\; f\in A_k\right\} $$ What is the value of $\sigma_k$? Edit: I think $\sigma_0=1$. I believe $\sigma_k= \frac{1}{4}$ for all $k>0$ (with $f=x$ the supremum is reached). It is not homework and is not related to my work. Just curious. I hope you find interesting too. (((The problem turned out to be uninteresting.))) Thanks.",,"['real-analysis', 'functional-analysis']"
6,Does every continuous everywhere but differentiable nowhere curve have an infinite length?,Does every continuous everywhere but differentiable nowhere curve have an infinite length?,,"Given a curve $\!\,\gamma : [a, b] \rightarrow ℝ^2$ that is continuous everywhere but differentiable nowhere (or almost nowhere), is its length: $$\text{length} (\gamma)=\sup \left\{ \sum_{i=1}^n d(\gamma(t_i),\gamma(t_{i-1})) : n \in \mathbb{N} \text{ and } a = t_0 < t_1 < \cdots < t_n = b \right\}.$$ always infinite?","Given a curve $\!\,\gamma : [a, b] \rightarrow ℝ^2$ that is continuous everywhere but differentiable nowhere (or almost nowhere), is its length: $$\text{length} (\gamma)=\sup \left\{ \sum_{i=1}^n d(\gamma(t_i),\gamma(t_{i-1})) : n \in \mathbb{N} \text{ and } a = t_0 < t_1 < \cdots < t_n = b \right\}.$$ always infinite?",,"['real-analysis', 'plane-curves']"
7,"The number of limit points of the set $\left\{\frac1p+\frac1q:p,q \in \Bbb N\right\}$ is which of the following:",The number of limit points of the set  is which of the following:,"\left\{\frac1p+\frac1q:p,q \in \Bbb N\right\}","I am stuck on the following problem: The number of limit points of the set $\left\{\frac1p+\frac1q:p,q \in \Bbb N\right\}$ is which of the following: $1$ $2$ Infinitely many Finitely many If I take $p$ to be fixed (say= $k$ ) and let $q \to \infty$ , then the limit point is given by $\frac{1}{k}$ . Since $k$ is an arbitrary natural number, the number of limit points is infinite. The same case can be continued after taking $q$ to be fixed (say= $k_1$ ). I think option 3 is the right choice. Am I on the right track? Can someone give further explanation?","I am stuck on the following problem: The number of limit points of the set is which of the following: Infinitely many Finitely many If I take to be fixed (say= ) and let , then the limit point is given by . Since is an arbitrary natural number, the number of limit points is infinite. The same case can be continued after taking to be fixed (say= ). I think option 3 is the right choice. Am I on the right track? Can someone give further explanation?","\left\{\frac1p+\frac1q:p,q \in \Bbb N\right\} 1 2 p k q \to \infty \frac{1}{k} k q k_1",['real-analysis']
8,"Prove that if $x$ is a non-zero rational number, then $\tan(x)$ is not a rational number and use this to prove that $\pi$ is not a rational number.","Prove that if  is a non-zero rational number, then  is not a rational number and use this to prove that  is not a rational number.",x \tan(x) \pi,"Prove that if $x$ is a non-zero rational number, then $\tan(x)$ is not a rational number and use this to prove that $\pi$ is not a rational number. I heard that this was proved two hundred years ago. I need this proof because I want to know the proof of why $\pi$ is not rational. I need the simplest proof! thanx !","Prove that if $x$ is a non-zero rational number, then $\tan(x)$ is not a rational number and use this to prove that $\pi$ is not a rational number. I heard that this was proved two hundred years ago. I need this proof because I want to know the proof of why $\pi$ is not rational. I need the simplest proof! thanx !",,"['real-analysis', 'algebra-precalculus', 'trigonometry', 'pi']"
9,"Convergence in probability, not almost surely","Convergence in probability, not almost surely",,"This is a classic example of convergence in probability, but not almost surely, but I am trying to rigorously prove it as opposed to ""arguing against"" the almost sure convergence.  $\DeclareMathOperator{\Pb}{\mathbf{P}}$ $\DeclareMathOperator{\Unif}{\mathsf{Uniform}}$ Definition 1 A sequence of random variables, $X_n$, is said to converge in probability if for any real number $\epsilon > 0$ $$ \lim_{n \to \infty} \Pb(|X_n - X | > \epsilon ) \to 0$$ Definition 2 A sequence of random variables $X_n$, is said to converge almost surely (a.s.) to a limit if $$\Pb(\lim_{n \to \infty} X_n = X) = 1$$ Recall also that a sequence of real numbers $a_n$ converges to a limit $a$ if for any $\epsilon > 0$ there exists a large enough value $N$ so that $|a_n - a| < \epsilon $ for all $n \geq N$. In other words, the difference between the sequence and the limit is uniformly small after some point in the sequence. The Question Let $U \sim \Unif(0,1)$. For $n = 2^k + m$ where $k \geq 0$ and $0 \leq m \leq 2^k - 1$ define the functions $f_n : [0, 1) \to \{0,1\}$ as follows $$ f_n(x) = \begin{cases} 1 & \text{if } {m \over 2^k} \leq x < {m +1 \over 2^k} \\ 0 & \text{o.w.} \end{cases} $$ What is the distribution of $f_n(U)$ for $n = 2^k + m$ for $k \geq 0$ and $ 0 \leq m \leq 2^k - 1$? Using the previous part, show that $X_n := f_n(U)$ converges to $0$ in probability. Show that for any fixed $x \in (0,1)$, $f_n(x) = 1$ for infinitely many values of $n$. Use this fact to reason why $X_n = f_n(U)$ does not converge to 0 almost surely. My Work $f_n(U)$ is equal to $1$ with probability ${1 \over 2^k}$ and $0$ otherwise. This takes care of the distribution, then to show that $X_n$ does not converge in probability, $$\Pb(|X_n - 0| > \epsilon) = \Pb(X_n = 1) = {1 \over 2^k}$$ As $n \to \infty$, for $k = \lfloor \log_2 n\rfloor $ we have that $k \to \infty$, hence $${1 \over 2^k} = \Pb(|X_n - 0| > \epsilon) \to 0$$ This is the part that I have often seen slightly glossed over. It is also the part that I am unclear about. I want to fix an $x$, and show that there is an $N$ so that $X_n$ for all $n \geq N$ is uniformly close to $0$. However, I'm really not sure how to do this. I tried working with binary expansions of my fixed $x$. Thanks for any help.","This is a classic example of convergence in probability, but not almost surely, but I am trying to rigorously prove it as opposed to ""arguing against"" the almost sure convergence.  $\DeclareMathOperator{\Pb}{\mathbf{P}}$ $\DeclareMathOperator{\Unif}{\mathsf{Uniform}}$ Definition 1 A sequence of random variables, $X_n$, is said to converge in probability if for any real number $\epsilon > 0$ $$ \lim_{n \to \infty} \Pb(|X_n - X | > \epsilon ) \to 0$$ Definition 2 A sequence of random variables $X_n$, is said to converge almost surely (a.s.) to a limit if $$\Pb(\lim_{n \to \infty} X_n = X) = 1$$ Recall also that a sequence of real numbers $a_n$ converges to a limit $a$ if for any $\epsilon > 0$ there exists a large enough value $N$ so that $|a_n - a| < \epsilon $ for all $n \geq N$. In other words, the difference between the sequence and the limit is uniformly small after some point in the sequence. The Question Let $U \sim \Unif(0,1)$. For $n = 2^k + m$ where $k \geq 0$ and $0 \leq m \leq 2^k - 1$ define the functions $f_n : [0, 1) \to \{0,1\}$ as follows $$ f_n(x) = \begin{cases} 1 & \text{if } {m \over 2^k} \leq x < {m +1 \over 2^k} \\ 0 & \text{o.w.} \end{cases} $$ What is the distribution of $f_n(U)$ for $n = 2^k + m$ for $k \geq 0$ and $ 0 \leq m \leq 2^k - 1$? Using the previous part, show that $X_n := f_n(U)$ converges to $0$ in probability. Show that for any fixed $x \in (0,1)$, $f_n(x) = 1$ for infinitely many values of $n$. Use this fact to reason why $X_n = f_n(U)$ does not converge to 0 almost surely. My Work $f_n(U)$ is equal to $1$ with probability ${1 \over 2^k}$ and $0$ otherwise. This takes care of the distribution, then to show that $X_n$ does not converge in probability, $$\Pb(|X_n - 0| > \epsilon) = \Pb(X_n = 1) = {1 \over 2^k}$$ As $n \to \infty$, for $k = \lfloor \log_2 n\rfloor $ we have that $k \to \infty$, hence $${1 \over 2^k} = \Pb(|X_n - 0| > \epsilon) \to 0$$ This is the part that I have often seen slightly glossed over. It is also the part that I am unclear about. I want to fix an $x$, and show that there is an $N$ so that $X_n$ for all $n \geq N$ is uniformly close to $0$. However, I'm really not sure how to do this. I tried working with binary expansions of my fixed $x$. Thanks for any help.",,"['real-analysis', 'probability-theory']"
10,From injective map to continuous map,From injective map to continuous map,,"Let $X$ and $Y$ metric spaces, $f$ is an injective from $X$ to $Y$, and $f$ sets every compact set in $X$ to compact set in $Y$. How to prove $f$ is continuous map? Any comments and advice will be appreciated.","Let $X$ and $Y$ metric spaces, $f$ is an injective from $X$ to $Y$, and $f$ sets every compact set in $X$ to compact set in $Y$. How to prove $f$ is continuous map? Any comments and advice will be appreciated.",,"['real-analysis', 'general-topology', 'analysis', 'functional-analysis']"
11,Must a countable disjoint union of closed balls in $\mathbb{R}^n$ with positive radius be disconnected?,Must a countable disjoint union of closed balls in  with positive radius be disconnected?,\mathbb{R}^n,"A disjoint union of open balls is of course disconnected. Here it is proved that a locally compact, connected, Hausdorff space is not a countable disjoint union of compact subsets, so a countable disjoint union of closed balls in $\mathbb{R}^n$ can't be connected and locally compact (hence cannot be open or closed connected), which rules out the connected sets for $n=1$ . But what if we just ask this disjoint union to be connected? Please forgive me if this question turns out to be trivial. Thank you in advance for any help. Edit. Actually I should have asked about something more general, like Is there a connected subset $\mathbb{R}^n$ that can be written as a countable disjoint union of closed sets in $\mathbb{R}^n$ ? If not, is there a connected subset $\mathbb{R}^n$ that can be written as a countable disjoint union of closed sets in the subspace topology? And the answer to the second question above is still negative for $n=1$ , or for open subsets of $\mathbb{R}^n$ (the link requires also local connectedness which is satisfied for open connected sets). However, I will keep the question as it was given the discussion already presented. Edit 2. A connected subset of $\mathbb{R}^3$ as a countable disjoint union of closed sets has be constructed in the existing answer. I would still be interested in the closed ball case: Must a countable disjoint union of closed balls in $\mathbb{R}^n$ not be connected?","A disjoint union of open balls is of course disconnected. Here it is proved that a locally compact, connected, Hausdorff space is not a countable disjoint union of compact subsets, so a countable disjoint union of closed balls in can't be connected and locally compact (hence cannot be open or closed connected), which rules out the connected sets for . But what if we just ask this disjoint union to be connected? Please forgive me if this question turns out to be trivial. Thank you in advance for any help. Edit. Actually I should have asked about something more general, like Is there a connected subset that can be written as a countable disjoint union of closed sets in ? If not, is there a connected subset that can be written as a countable disjoint union of closed sets in the subspace topology? And the answer to the second question above is still negative for , or for open subsets of (the link requires also local connectedness which is satisfied for open connected sets). However, I will keep the question as it was given the discussion already presented. Edit 2. A connected subset of as a countable disjoint union of closed sets has be constructed in the existing answer. I would still be interested in the closed ball case: Must a countable disjoint union of closed balls in not be connected?",\mathbb{R}^n n=1 \mathbb{R}^n \mathbb{R}^n \mathbb{R}^n n=1 \mathbb{R}^n \mathbb{R}^3 \mathbb{R}^n,"['real-analysis', 'general-topology', 'analysis']"
12,Evaluate $\lim\limits_{n\to\infty}\big(\frac{n}{2}+\min\limits_{x\in\mathbb{R}}\sum_{k=0}^n{\cos(2^k x)}\big)$,Evaluate,\lim\limits_{n\to\infty}\big(\frac{n}{2}+\min\limits_{x\in\mathbb{R}}\sum_{k=0}^n{\cos(2^k x)}\big),"What is the exact value of the following limit? $$L=\lim_{n\to\infty}\left(\frac{n}{2}+\min_{x\in\mathbb{R}}\sum_{k=0}^n{\cos(2^k x)}\right)$$ Experimenting on desmos suggests the following claims: $\sum_{k=0}^n{\cos{(2^k x)}}$ is minimized when $x\approx \left(2m\pm\dfrac{2}{3}\right)\pi,m\in\mathbb{Z}$ , with the approximation approaching equality as $n\to\infty$ $L\approx -0.704$ I do not know how to prove these claims. (This question was inspired by another question .)","What is the exact value of the following limit? Experimenting on desmos suggests the following claims: is minimized when , with the approximation approaching equality as I do not know how to prove these claims. (This question was inspired by another question .)","L=\lim_{n\to\infty}\left(\frac{n}{2}+\min_{x\in\mathbb{R}}\sum_{k=0}^n{\cos(2^k x)}\right) \sum_{k=0}^n{\cos{(2^k x)}} x\approx \left(2m\pm\dfrac{2}{3}\right)\pi,m\in\mathbb{Z} n\to\infty L\approx -0.704","['real-analysis', 'limits', 'trigonometry', 'maxima-minima']"
13,Special version of Tonelli’s theorem,Special version of Tonelli’s theorem,,"I am trying to prove this theorem. I have not find anything similar to it in the internet Special version of Tonelli’s theorem Assume that the function $f(x,u): [a,b] \times \mathbb{R} \to \mathbb{R},\,\, g(x, \xi): [a,b] \times \mathbb{R} \to \mathbb{R}$ are continuous, $f$ is bounded below, $g$ is convex in $\xi$ and satisfies $$\exists r>1,\, \exists C>0\,\, \text{such that}\,\, g(x,\xi) \ge C| \xi|^r,\,\, \forall (x, \xi) \in [a,b] \times \mathbb{R}.$$ Then there exists a minimizer of the functional $J[u] = \displaystyle\int_a^b (f(x,u(x)) + g(x,u'(x))) dx$ in the space $X= \{ u \in AC([a,b]); u(a)=\alpha, u(b)= \beta \}.$ Proof Since $f$ is bounded then there is a real number $m \in \mathbb{R}$ such that $m (b-a)\le f(x,u(x)), \quad \forall (x,u(x)) \in [a,b] \times \mathbb{R}$ . From the properties of $g$ we get $$m+ C \int_a^b |u'(x)|^r dx \leq J[u] \Rightarrow m+ C \| u'\|_{L^r[a,b]}^r \leq J[u]\,\,\, \forall u \in X.$$ We can see that $J[u]$ is bounded below and from the definition of the infimum there is a minimizing sequence $\{u_n\}_{n\in \mathbb{N}} \subset X$ such that $$\underset{n \to \infty}{\lim} J[u_n] = \inf \{ J[u] | u \in X \}> -\infty \,\, \text{ in } \mathbb{R}.$$ and hence, $\{ u_n'\}_{n \in \mathbb{N}}$ is uniformly bounded, i.e. there is $N>0$ such that $\forall n >N$ we have $$\| u'_n\|_{L^r[a,b]} \leq \left(\frac{J[u_N] -m}{c} \right)^\frac{1}{r}.$$ Now, since $\{u_n\}$ is equicontinuous, and uniformly bounded in $L^r[a,b]$ then according to Arzela-Ascoli theorem there is a subsequence $\{ u_{n_k} \}_{k \in \mathbb{N}}$ and $\overline{u} \in AC[a,b]$ such that $u_{n_k} \to \overline{u}$ uniformly, and $u'_{n_k} \to \overline{u}'$ in the sense of $L^r[a,b]$ . I am not sure of my last argument is right? I want to make it more rigorous. Although I found the general idea of the proof on page (140) in the book of Hansjörg Kielhöfer named ( Calculus of Variations An Introduction to the One-Dimensional Theory with Examples and Exercises ) I have no idea about completing the proof of the theorem. Could you please help.","I am trying to prove this theorem. I have not find anything similar to it in the internet Special version of Tonelli’s theorem Assume that the function are continuous, is bounded below, is convex in and satisfies Then there exists a minimizer of the functional in the space Proof Since is bounded then there is a real number such that . From the properties of we get We can see that is bounded below and from the definition of the infimum there is a minimizing sequence such that and hence, is uniformly bounded, i.e. there is such that we have Now, since is equicontinuous, and uniformly bounded in then according to Arzela-Ascoli theorem there is a subsequence and such that uniformly, and in the sense of . I am not sure of my last argument is right? I want to make it more rigorous. Although I found the general idea of the proof on page (140) in the book of Hansjörg Kielhöfer named ( Calculus of Variations An Introduction to the One-Dimensional Theory with Examples and Exercises ) I have no idea about completing the proof of the theorem. Could you please help.","f(x,u): [a,b] \times \mathbb{R} \to \mathbb{R},\,\, g(x, \xi): [a,b] \times \mathbb{R} \to \mathbb{R} f g \xi \exists r>1,\, \exists C>0\,\, \text{such that}\,\, g(x,\xi) \ge C| \xi|^r,\,\, \forall (x, \xi) \in [a,b] \times \mathbb{R}. J[u] = \displaystyle\int_a^b (f(x,u(x)) + g(x,u'(x))) dx X= \{ u \in AC([a,b]); u(a)=\alpha, u(b)= \beta \}. f m \in \mathbb{R} m (b-a)\le f(x,u(x)), \quad \forall (x,u(x)) \in [a,b] \times \mathbb{R} g m+ C \int_a^b |u'(x)|^r dx \leq J[u] \Rightarrow m+ C \| u'\|_{L^r[a,b]}^r \leq J[u]\,\,\, \forall u \in X. J[u] \{u_n\}_{n\in \mathbb{N}} \subset X \underset{n \to \infty}{\lim} J[u_n] = \inf \{ J[u] | u \in X \}> -\infty \,\, \text{ in } \mathbb{R}. \{ u_n'\}_{n \in \mathbb{N}} N>0 \forall n >N \| u'_n\|_{L^r[a,b]} \leq \left(\frac{J[u_N] -m}{c} \right)^\frac{1}{r}. \{u_n\} L^r[a,b] \{ u_{n_k} \}_{k \in \mathbb{N}} \overline{u} \in AC[a,b] u_{n_k} \to \overline{u} u'_{n_k} \to \overline{u}' L^r[a,b]","['real-analysis', 'lp-spaces', 'nonlinear-optimization', 'calculus-of-variations']"
14,Isolating frequencies and phases of superposed cosine functions,Isolating frequencies and phases of superposed cosine functions,,"This problem is mathematical, but it is needed in physics. I have points on the graph of the function $y = f(x)$ . These points were obtained experimentally. I have a lot of such graphs. The pictures below show only two examples. It is known that $f(x) = A_{1}\cos(w_{1}x + \phi_{1}) + A_{2}\cos(w_{2}x + \phi_{2})$ , where $A_{1}, w_{1}, A_{2}, w_{2}, \phi_{1}, \phi_{2}$ are some constants (real parameters) and $w_{1} > 0, w_{2} > 0$ . I need to determine the approximate values of numbers $A_{1}, w_{1}, A_{2}, w_{2}$ knowing what the graph of a function looks like. I want to know the values of numbers $A_{1}, w_{1}, A_{2}, w_{2}$ even with a large error, for example, with an error of 50%. I need to know this so that I can use a computer program OriginLab to analyze this graph. My question. The graph of the function clearly shows where the extremum points are. You can see the values of the function at the extremum points. The points of intersection of the graph of the function with the abscissa axis are also clearly visible. Is it possible on the basis of this to find approximate (not exact) values of numbers $A_{1}, w_{1}, A_{2}, w_{2}$ ? I would like to know not only the method of how to do this, but I also want to know the mathematical proof of this method. It is about a method how to calculate these parameters on paper. This is not about numerical methods. My work. At the moment, I do not know this method. On the Internet, you can find special cases of how this function looks, but I'm interested in the general case for arbitrary values of $A_{1}, w_{1}, A_{2}, w_{2}, \phi_{1}, \phi_{2}$ . I tried to investigate the function myself using differentiation, but very complex transcendental equations are obtained.","This problem is mathematical, but it is needed in physics. I have points on the graph of the function . These points were obtained experimentally. I have a lot of such graphs. The pictures below show only two examples. It is known that , where are some constants (real parameters) and . I need to determine the approximate values of numbers knowing what the graph of a function looks like. I want to know the values of numbers even with a large error, for example, with an error of 50%. I need to know this so that I can use a computer program OriginLab to analyze this graph. My question. The graph of the function clearly shows where the extremum points are. You can see the values of the function at the extremum points. The points of intersection of the graph of the function with the abscissa axis are also clearly visible. Is it possible on the basis of this to find approximate (not exact) values of numbers ? I would like to know not only the method of how to do this, but I also want to know the mathematical proof of this method. It is about a method how to calculate these parameters on paper. This is not about numerical methods. My work. At the moment, I do not know this method. On the Internet, you can find special cases of how this function looks, but I'm interested in the general case for arbitrary values of . I tried to investigate the function myself using differentiation, but very complex transcendental equations are obtained.","y = f(x) f(x) = A_{1}\cos(w_{1}x + \phi_{1}) + A_{2}\cos(w_{2}x + \phi_{2}) A_{1}, w_{1}, A_{2}, w_{2}, \phi_{1}, \phi_{2} w_{1} > 0, w_{2} > 0 A_{1}, w_{1}, A_{2}, w_{2} A_{1}, w_{1}, A_{2}, w_{2} A_{1}, w_{1}, A_{2}, w_{2} A_{1}, w_{1}, A_{2}, w_{2}, \phi_{1}, \phi_{2}","['real-analysis', 'trigonometry']"
15,"Is $\left\{ a \sqrt{3} - b \sqrt{2} | a, b \in \mathbb{N} \right\}$ dense in $\mathbb{R}$?",Is  dense in ?,"\left\{ a \sqrt{3} - b \sqrt{2} | a, b \in \mathbb{N} \right\} \mathbb{R}","I come along this problem while messing around with some other problems. So the question is: Is $\left\{ a \sqrt{3} - b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\}$ dense in $\mathbb{R}$ ? I know for sure that: $A = \left\{ a \sqrt{3} + b \sqrt{2} \ | \ a, b \in \color{red}{\mathbb{Z}} \right\}$ is dense in $\mathbb{R}$ . $B = \left\{ a \sqrt{3} + b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\}$ is not dense in $\mathbb{R}$ . But what about $\left\{ a \sqrt{3} - b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\}$ ? I suspect that it may be dense; however, I cannot prove it. Can someone please give me a push? Thanks very much in advance, :*","I come along this problem while messing around with some other problems. So the question is: Is dense in ? I know for sure that: is dense in . is not dense in . But what about ? I suspect that it may be dense; however, I cannot prove it. Can someone please give me a push? Thanks very much in advance, :*","\left\{ a \sqrt{3} - b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\} \mathbb{R} A = \left\{ a \sqrt{3} + b \sqrt{2} \ | \ a, b \in \color{red}{\mathbb{Z}} \right\} \mathbb{R} B = \left\{ a \sqrt{3} + b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\} \mathbb{R} \left\{ a \sqrt{3} - b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\}","['real-analysis', 'general-topology', 'metric-spaces']"
16,"Family of Generalized Integrals ${I}(a,b,p)=\int_0^{ab} \left( \left\{\frac{x}{a}\right\}-p\right) \left( \left\{\frac{x}{b}\right\}-p\right) \; dx$",Family of Generalized Integrals,"{I}(a,b,p)=\int_0^{ab} \left( \left\{\frac{x}{a}\right\}-p\right) \left( \left\{\frac{x}{b}\right\}-p\right) \; dx","Background: I came across the following family of generalized Franel integrals, and found them quite interesting.  I don't think I've seen anything about these integrals before, at least not generalized, and I want to know if this is a known family of generalized integrals.  I am also interested behind the behavior of this family of integrals, specifically when $p=\frac{1}{2}$ .  I want to find out if there's a simple algebraic closed form expression for this family of integrals.  Wolfram does not do a good job factoring the integrands, but to be fair it didn't factor $I(a,b,p)$ as I have done in this post.  Also, how would you approach cases where $\gcd{(a,b,c,\dots)} \neq 1$ ? Consider the family of generalized integrals as the following: $${I}(a,b,p)=\int_0^{ab} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \; dx$$ $${I}(a,b,c,p)=\int_0^{abc} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \left( \bigg\{\frac{x}{c}\bigg\}-p\right)\; dx$$ $${I}(a,b,c,d,p)=\int_0^{abcd} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \left( \bigg\{\frac{x}{c}\bigg\}-p\right) \left( \bigg\{\frac{x}{d}\bigg\}-p\right)\; dx$$ $$\ldots$$ Where $a,b,c,\ldots \in \mathbb{N}$ , $p \in \mathbb{Q}^+$ , and $\gcd{(a,b,c,\ldots)}=1$ . Calculations: Express the integral as the following: $${I}(a,b,p)=\sum_{i=0}^{a-1} \sum_{k=0}^{b-1} \int_0^1 \left(\frac{t+i}{a}-p\right)\left(\frac{t+k}{b}-p\right) \; dt$$ Changing the order of the summations and integral and using some algebra: $${I}(a,b,p)=\int_0^1 \left(\frac{a-1}{2}+t-ap\right)\left(\frac{b-1}{2}+t-bp\right) \; dt$$ Expanding the integrand out and factoring yields: $${I}(a,b,p)=\int_0^1 \frac{ab}{4}{\left(2p-1\right)}^2+\frac{at}{2}\left(1-2p\right)+\frac{bt}{2}\left(1-2p\right)+\frac{(a+b)}{4}\left(2p-1\right)+{\left(t-\frac{1}{2}\right)}^2 \; dt$$ $${I}(a,b,p)=\int_0^1 \frac{ab}{4}{\left(2p-1\right)}^2+{\left(t-\frac{1}{2}\right)}^2 \; dt$$ And so: $$\boxed{{I}(a,b,p)= \frac{ab}{4}{\left(1-2p\right)}^2+\frac{1}{12}}$$ Calculated similarly,I got the following: $$I(a,b,c,p)=\frac{abc{\left(1-2p\right)}^3}{8}+\frac{c}{24}\left(1-2p\right)$$ $$I(a,b,c,d,p)= \frac{abcd}{16}{\left(1-2p\right)}^4+\frac{{(1-2p)}^2}{48}\left(ab+cd\right)+\frac{1}{80} $$ However, as @Varun Vejalla and @OliverDiaz pointed out in the comments, these results are illogical, and there actually is no closed form for $I(a,b,c,d,p)$ . Further Observations: Interestingly enough, $p=\frac{1}{2}$ is a special case for this entire family generalized integrals.  Why is this? Assuming the aforementioned conditions are met: $$I\left(a,b,\frac{1}{2}\right)=\frac{1}{12}$$ $$I\left(a,b,c,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^3 \; dt=0$$ $$I\left(a,b,c,d,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^4 \; dt=\frac{1}{80} $$ $$I\left(a,b,c,d,e,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^5 \; dt=0$$ And so it seems that the following statement is true: $$I\left(a_1,a_2,\ldots,a_n,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^n \; dt=\cases{        0       & $n \; \text{is odd}$ \cr                         \frac{1}{2^n\left(n+1\right)}       & $n \; \text{is even}$ }$$ However, Wolfram Alpha computed $I\left(a,b,c,d,\frac{1}{2}\right)=0$ for valid $a,b,c,d$ values. Final Remarks: I wonder what other interesting observations can be made about this family of generalized integrals.  Specifically, are there other interesting special cases, and if so why are they so special?","Background: I came across the following family of generalized Franel integrals, and found them quite interesting.  I don't think I've seen anything about these integrals before, at least not generalized, and I want to know if this is a known family of generalized integrals.  I am also interested behind the behavior of this family of integrals, specifically when .  I want to find out if there's a simple algebraic closed form expression for this family of integrals.  Wolfram does not do a good job factoring the integrands, but to be fair it didn't factor as I have done in this post.  Also, how would you approach cases where ? Consider the family of generalized integrals as the following: Where , , and . Calculations: Express the integral as the following: Changing the order of the summations and integral and using some algebra: Expanding the integrand out and factoring yields: And so: Calculated similarly,I got the following: However, as @Varun Vejalla and @OliverDiaz pointed out in the comments, these results are illogical, and there actually is no closed form for . Further Observations: Interestingly enough, is a special case for this entire family generalized integrals.  Why is this? Assuming the aforementioned conditions are met: And so it seems that the following statement is true: However, Wolfram Alpha computed for valid values. Final Remarks: I wonder what other interesting observations can be made about this family of generalized integrals.  Specifically, are there other interesting special cases, and if so why are they so special?","p=\frac{1}{2} I(a,b,p) \gcd{(a,b,c,\dots)} \neq 1 {I}(a,b,p)=\int_0^{ab} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \; dx {I}(a,b,c,p)=\int_0^{abc} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \left( \bigg\{\frac{x}{c}\bigg\}-p\right)\; dx {I}(a,b,c,d,p)=\int_0^{abcd} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \left( \bigg\{\frac{x}{c}\bigg\}-p\right) \left( \bigg\{\frac{x}{d}\bigg\}-p\right)\; dx \ldots a,b,c,\ldots \in \mathbb{N} p \in \mathbb{Q}^+ \gcd{(a,b,c,\ldots)}=1 {I}(a,b,p)=\sum_{i=0}^{a-1} \sum_{k=0}^{b-1} \int_0^1 \left(\frac{t+i}{a}-p\right)\left(\frac{t+k}{b}-p\right) \; dt {I}(a,b,p)=\int_0^1 \left(\frac{a-1}{2}+t-ap\right)\left(\frac{b-1}{2}+t-bp\right) \; dt {I}(a,b,p)=\int_0^1 \frac{ab}{4}{\left(2p-1\right)}^2+\frac{at}{2}\left(1-2p\right)+\frac{bt}{2}\left(1-2p\right)+\frac{(a+b)}{4}\left(2p-1\right)+{\left(t-\frac{1}{2}\right)}^2 \; dt {I}(a,b,p)=\int_0^1 \frac{ab}{4}{\left(2p-1\right)}^2+{\left(t-\frac{1}{2}\right)}^2 \; dt \boxed{{I}(a,b,p)= \frac{ab}{4}{\left(1-2p\right)}^2+\frac{1}{12}} I(a,b,c,p)=\frac{abc{\left(1-2p\right)}^3}{8}+\frac{c}{24}\left(1-2p\right) I(a,b,c,d,p)= \frac{abcd}{16}{\left(1-2p\right)}^4+\frac{{(1-2p)}^2}{48}\left(ab+cd\right)+\frac{1}{80}  I(a,b,c,d,p) p=\frac{1}{2} I\left(a,b,\frac{1}{2}\right)=\frac{1}{12} I\left(a,b,c,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^3 \; dt=0 I\left(a,b,c,d,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^4 \; dt=\frac{1}{80}  I\left(a,b,c,d,e,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^5 \; dt=0 I\left(a_1,a_2,\ldots,a_n,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^n \; dt=\cases{        0       & n \; \text{is odd} \cr
                        \frac{1}{2^n\left(n+1\right)}       & n \; \text{is even} } I\left(a,b,c,d,\frac{1}{2}\right)=0 a,b,c,d","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'solution-verification']"
17,Question about $f(x)=\sum_{k=1}^\infty (-1)^{k+1}\sin (\frac{x}{k}) $,Question about,f(x)=\sum_{k=1}^\infty (-1)^{k+1}\sin (\frac{x}{k}) ,"This function is rather peculiar. It is easy to establish the following: $$f(x) =\sum_{k=0}^\infty (-1)^k A_{2k+1} \cdot x^{2k+1}, \mbox{ with } A_k=\Big(1-\frac{1}{2^{k}} + \frac{1}{3^{k}}- \frac{1}{4^{k}}+\cdots\Big).$$ Note that $A(1)=\log 2$ , and for $k>1$ , we have $$A(k)= \Big(1-\frac{1}{2^{k-1}}\Big)\zeta(k)$$ where $\zeta$ is the Riemann Zeta function. Also, $f(-x) = - f(x)$ and we have the following approximation when $x$ is large, using a value of $K$ such that $x/K < 0.01$ : $$f(x) \approx \sum_{k=1}^K (-1)^{k+1}\sin \Big(\frac{x}{k}\Big) - x\cdot\sum_{k=K+1}^\infty \frac{(-1)^{k}}{k}$$ The function is smooth but exhibits infinitely many roots, maxima and minima. I am in particular interested in the following quantity: $$g(x) = \sup_{0\leq y\leq x}f(y).$$ What is the growth rate for $g(x)$ ? Is it linear, sub-linear, or super-linear? Another question of interest is the average spacing between two roots or two extrema. Below are two plots of $f(x)$ , the first one for $0\leq x\leq 200$ , the second one for $0\leq x\leq 2000$ . Addendum: Failed attempt to solve this I used the Euler-Maclaurin summation formula to get a good approximation for $f(x)$ when $x$ is large, and this leads to $$f(x) \approx \int_1^\infty \Big(\sin\frac{x}{2u} - \sin\frac{x}{2u+1}\Big) du.$$ A closed form for this integral exists, involving the cosine integral, see WolframAlpha here . Lots of asymptotic formulas are available (see here ) but when I apply them, I end up with $f(x)$ being bounded, which is very clearly not the case based on my observations. As an illustration, below is the computation of $f(x)$ for $x = 52,000,001$ . The first chart shows $f(x)$ based on the first $n=2000$ terms in the series. Here the X-axis represents $n$ , and the Y-axis represents $f(x)$ for the particular value of $x$ in question, when using a growing number of terms. In the second chart, $n$ goes to $200,000$ . Stability is reached after adding about $4,100$ terms, and oscillations are slowly dampening then. One promising approach is this. Let $$ f_k(x)=\sum_{i=1}^k (-1)^{i+1}\sin \Big(\frac{x}{i}\Big) .$$ Define $h_k(x) =\frac{1}{2}(f_k(x) + f_{k-1}(x))$ .Then $f(x) = \lim_{k\rightarrow\infty} h_k(x)$ . The iterates $h_k$ 's are much smoother than the $f_k$ 's, and convergence is much faster.","This function is rather peculiar. It is easy to establish the following: Note that , and for , we have where is the Riemann Zeta function. Also, and we have the following approximation when is large, using a value of such that : The function is smooth but exhibits infinitely many roots, maxima and minima. I am in particular interested in the following quantity: What is the growth rate for ? Is it linear, sub-linear, or super-linear? Another question of interest is the average spacing between two roots or two extrema. Below are two plots of , the first one for , the second one for . Addendum: Failed attempt to solve this I used the Euler-Maclaurin summation formula to get a good approximation for when is large, and this leads to A closed form for this integral exists, involving the cosine integral, see WolframAlpha here . Lots of asymptotic formulas are available (see here ) but when I apply them, I end up with being bounded, which is very clearly not the case based on my observations. As an illustration, below is the computation of for . The first chart shows based on the first terms in the series. Here the X-axis represents , and the Y-axis represents for the particular value of in question, when using a growing number of terms. In the second chart, goes to . Stability is reached after adding about terms, and oscillations are slowly dampening then. One promising approach is this. Let Define .Then . The iterates 's are much smoother than the 's, and convergence is much faster.","f(x) =\sum_{k=0}^\infty (-1)^k A_{2k+1} \cdot x^{2k+1}, \mbox{ with } A_k=\Big(1-\frac{1}{2^{k}} + \frac{1}{3^{k}}- \frac{1}{4^{k}}+\cdots\Big). A(1)=\log 2 k>1 A(k)= \Big(1-\frac{1}{2^{k-1}}\Big)\zeta(k) \zeta f(-x) = - f(x) x K x/K < 0.01 f(x) \approx \sum_{k=1}^K (-1)^{k+1}\sin \Big(\frac{x}{k}\Big) - x\cdot\sum_{k=K+1}^\infty \frac{(-1)^{k}}{k} g(x) = \sup_{0\leq y\leq x}f(y). g(x) f(x) 0\leq x\leq 200 0\leq x\leq 2000 f(x) x f(x) \approx \int_1^\infty \Big(\sin\frac{x}{2u} - \sin\frac{x}{2u+1}\Big) du. f(x) f(x) x = 52,000,001 f(x) n=2000 n f(x) x n 200,000 4,100  f_k(x)=\sum_{i=1}^k (-1)^{i+1}\sin \Big(\frac{x}{i}\Big) . h_k(x) =\frac{1}{2}(f_k(x) + f_{k-1}(x)) f(x) = \lim_{k\rightarrow\infty} h_k(x) h_k f_k","['real-analysis', 'sequences-and-series', 'summation', 'maxima-minima', 'riemann-zeta']"
18,"Suppose $a_{n+2}=\frac{2}{a_n+a_{n+1}}$,show the existence of $\lambda$,$|a_n-1|<C\lambda^n$","Suppose ,show the existence of ,",a_{n+2}=\frac{2}{a_n+a_{n+1}} \lambda |a_n-1|<C\lambda^n,"Suppose $a_1>0$ , $a_2>0$ , $a_{n+2}=\frac{2}{a_n+a_{n+1}}$ .Prove that there exists $\lambda\in (0,1)$ and a constant C,such that $|a_n-1|<C\lambda^n$ forall n $\in \mathbb{N} $ .Furthermore,find the minimal $\lambda$ which suit $|a_n-1|<C\lambda^n$ . I can see that $\lim_{n\to \infty}a_n=1$ .The proof below can be found in kaczor's book ""Problems in Mathematical Analysis I"".In fact,if $\frac1a\leqslant a_n,a_{n+1}\leqslant a$ ,then $\frac{1}{a}\leqslant a_{n+2}=\frac{2}{a_n+a_{n+1}}\leqslant a$ .Thus ,by the principle of induction ,the sequence $\{a_n\}$ is bounded.Put $$l=\varliminf_{n\to \infty}a_n,L=\varlimsup_{n\to \infty}a_n$$ then for an arbitrarily fixed $\varepsilon>0$ there exist $n_1,n_2\in\mathbb{N}$ ,such that \begin{align*} a_n<L+\varepsilon ,for \;n>n_1\tag{i}\\ a_n>l-\varepsilon ,for\; n>n_2\tag{ii} \end{align*} By (i), $a_{n+2}=\frac{2}{a_n+a_{n+1}}>\frac{1}{L+\varepsilon}$ , $n>n_1$ .Since the positive $\varepsilon$ can be arbitrarily small,we  get $l\geqslant \frac1L$ .In much the same way (ii) implies that $L\leqslant \frac1l$ .Thus $l=\frac1L$ .Let $\{n_k\}$ be a sequence of positive integers such that $\lim_{k\to \infty}a_{n_k+2}=L$ .We can assume that the sequences $\{a_{n_k+1}\}$ , $\{a_{n_k}\}$ and $\{a_{n_k-1}\}$ converge to $l_1,l_2$ and $l_3$ ,respectively.In fact ,if this is not the case,we can choose subsequences which do.By the definition of $\{a_n\}$ , $$l_1+l_2=\frac{2}{L}=2l,\;l_2+l_3=\frac{2}{l_1}$$ and since $l\leqslant l_1,l_2,l_3\leqslant L$ ,we get $l_1=l_2=l$ and $l_2=l_3=L$ .Hence $l=L$ .This and the equality $l=\frac1L$ imply that the sequence $\{a_n\}$ converges to $1$ . It's hard for me to prove $|a_n-1|<C\lambda^n$ ,I've been thinking about this for one day, but I don't have any clue.","Suppose , , .Prove that there exists and a constant C,such that forall n .Furthermore,find the minimal which suit . I can see that .The proof below can be found in kaczor's book ""Problems in Mathematical Analysis I"".In fact,if ,then .Thus ,by the principle of induction ,the sequence is bounded.Put then for an arbitrarily fixed there exist ,such that By (i), , .Since the positive can be arbitrarily small,we  get .In much the same way (ii) implies that .Thus .Let be a sequence of positive integers such that .We can assume that the sequences , and converge to and ,respectively.In fact ,if this is not the case,we can choose subsequences which do.By the definition of , and since ,we get and .Hence .This and the equality imply that the sequence converges to . It's hard for me to prove ,I've been thinking about this for one day, but I don't have any clue.","a_1>0 a_2>0 a_{n+2}=\frac{2}{a_n+a_{n+1}} \lambda\in (0,1) |a_n-1|<C\lambda^n \in \mathbb{N}  \lambda |a_n-1|<C\lambda^n \lim_{n\to \infty}a_n=1 \frac1a\leqslant a_n,a_{n+1}\leqslant a \frac{1}{a}\leqslant a_{n+2}=\frac{2}{a_n+a_{n+1}}\leqslant a \{a_n\} l=\varliminf_{n\to \infty}a_n,L=\varlimsup_{n\to \infty}a_n \varepsilon>0 n_1,n_2\in\mathbb{N} \begin{align*}
a_n<L+\varepsilon ,for \;n>n_1\tag{i}\\
a_n>l-\varepsilon ,for\; n>n_2\tag{ii}
\end{align*} a_{n+2}=\frac{2}{a_n+a_{n+1}}>\frac{1}{L+\varepsilon} n>n_1 \varepsilon l\geqslant \frac1L L\leqslant \frac1l l=\frac1L \{n_k\} \lim_{k\to \infty}a_{n_k+2}=L \{a_{n_k+1}\} \{a_{n_k}\} \{a_{n_k-1}\} l_1,l_2 l_3 \{a_n\} l_1+l_2=\frac{2}{L}=2l,\;l_2+l_3=\frac{2}{l_1} l\leqslant l_1,l_2,l_3\leqslant L l_1=l_2=l l_2=l_3=L l=L l=\frac1L \{a_n\} 1 |a_n-1|<C\lambda^n","['real-analysis', 'sequences-and-series']"
19,Prove that $f(n)\ge 2^{n-1}$,Prove that,f(n)\ge 2^{n-1},"This is from a Brazilian math contest for college students (OBMU): Let $f: (0,+\infty) \to (0,+\infty)$ be a infinitely differentiable function such that For all positive integer $k$ and positive real $x$ , $f^{(k)}(x)> 0$ (where $f^{(k)}$ is the kth derivative). For all positive integer $m$ , $f(m)$ is a positive integer. Prove that $f(n)\ge 2^{n-1}$ for all positive integer $n$ . Attempt By the mean value theorem, we have $$f(2)-f(1) = f'(c_1),\space c_1 \in (1,2)$$ $$f(3)-f(2) = f'(c_2),\space c_2 \in (2,3)$$ $$\vdots $$ $$f(n)-f(n-1) = f'(c_{n-1}),\space c_2 \in (n,n-1) $$ Then, $f'(c_k)$ is positive integer for all $k \in \{1,2,\cdots, n-1\}$ . Besides, $f'$ is strictly increasing. Thus, $f'(c_k) \ge k$ . Adding all the inequalities, we get $$f(n) \ge \sum_{k=1}^{n-1}k + f(1) = \frac{n(n-1)}{2} + f(1) $$","This is from a Brazilian math contest for college students (OBMU): Let be a infinitely differentiable function such that For all positive integer and positive real , (where is the kth derivative). For all positive integer , is a positive integer. Prove that for all positive integer . Attempt By the mean value theorem, we have Then, is positive integer for all . Besides, is strictly increasing. Thus, . Adding all the inequalities, we get","f: (0,+\infty) \to (0,+\infty) k x f^{(k)}(x)> 0 f^{(k)} m f(m) f(n)\ge 2^{n-1} n f(2)-f(1) = f'(c_1),\space c_1 \in (1,2) f(3)-f(2) = f'(c_2),\space c_2 \in (2,3) \vdots  f(n)-f(n-1) = f'(c_{n-1}),\space c_2 \in (n,n-1)  f'(c_k) k \in \{1,2,\cdots, n-1\} f' f'(c_k) \ge k f(n) \ge \sum_{k=1}^{n-1}k + f(1) = \frac{n(n-1)}{2} + f(1) ","['real-analysis', 'inequality', 'contest-math']"
20,"How does one evaluate $I(\alpha,\beta,\gamma)= \int_{0}^{\infty} \cos\left(\frac{x(x^2-\alpha^2)}{x^2-\beta^2}\right)\frac{1}{x^2+\gamma^2} \, dx.$",How does one evaluate,"I(\alpha,\beta,\gamma)= \int_{0}^{\infty} \cos\left(\frac{x(x^2-\alpha^2)}{x^2-\beta^2}\right)\frac{1}{x^2+\gamma^2} \, dx.","I have been trying to evaluate this integral for a while now, $$I(\alpha,\beta,\gamma)= \int_{0}^{\infty} \cos\left(\frac{x(x^2-\alpha^2)}{x^2-\beta^2}\right)\frac{1}{x^2+\gamma^2} \, dx.$$ I first came across this integral on Quora ( https://www.quora.com/What-improper-integrals-are-hard-to-solve ). Someone stated that this integral was submitted as a problem to the Gazette of the Royal Mathematics Society of Spain and was (at the time) still open, so the complete solution would not be published there. (have not been able to find the publication). Although no derivation, a closed form expression was given: $$I(\alpha,\beta,\gamma)=\frac{\pi}{2\gamma}e^{-\frac{\gamma(\alpha^2+\gamma^2)}{\beta^2+\gamma^2}}.$$ One might be wondering, why not just respond to the post on Quora? That I did. However, the thread seems to inactive. I have tried several methods including the one employed by Mark Viola in this post ( Computing $\int_{-\infty}^{\infty} \frac{\cos x}{x^{2} + a^{2}}dx$ using residue calculus ). Trying to work backwards from the closed form expression, by treating the term $ \frac{\gamma(\alpha^2+\gamma^2)}{\beta^2+\gamma^2}$ in exponent as one variable. However, I have not been able to succeed. Hopefully, you can help me solve this integral/ give suggestions/ different methods. edit: I found that when $\alpha=\beta$, the integral becomes $I= \int_{0}^{\infty} \cos(x)\frac{1}{x^2+\gamma^2}dx$ , which evaluates to $I=\frac{\pi}{2\gamma}e^{-\gamma}$. This is comparable to the desired result. The argument in the exponent is different however.","I have been trying to evaluate this integral for a while now, $$I(\alpha,\beta,\gamma)= \int_{0}^{\infty} \cos\left(\frac{x(x^2-\alpha^2)}{x^2-\beta^2}\right)\frac{1}{x^2+\gamma^2} \, dx.$$ I first came across this integral on Quora ( https://www.quora.com/What-improper-integrals-are-hard-to-solve ). Someone stated that this integral was submitted as a problem to the Gazette of the Royal Mathematics Society of Spain and was (at the time) still open, so the complete solution would not be published there. (have not been able to find the publication). Although no derivation, a closed form expression was given: $$I(\alpha,\beta,\gamma)=\frac{\pi}{2\gamma}e^{-\frac{\gamma(\alpha^2+\gamma^2)}{\beta^2+\gamma^2}}.$$ One might be wondering, why not just respond to the post on Quora? That I did. However, the thread seems to inactive. I have tried several methods including the one employed by Mark Viola in this post ( Computing $\int_{-\infty}^{\infty} \frac{\cos x}{x^{2} + a^{2}}dx$ using residue calculus ). Trying to work backwards from the closed form expression, by treating the term $ \frac{\gamma(\alpha^2+\gamma^2)}{\beta^2+\gamma^2}$ in exponent as one variable. However, I have not been able to succeed. Hopefully, you can help me solve this integral/ give suggestions/ different methods. edit: I found that when $\alpha=\beta$, the integral becomes $I= \int_{0}^{\infty} \cos(x)\frac{1}{x^2+\gamma^2}dx$ , which evaluates to $I=\frac{\pi}{2\gamma}e^{-\gamma}$. This is comparable to the desired result. The argument in the exponent is different however.",,"['real-analysis', 'integration', 'complex-analysis']"
21,"If $x_0=x$ and $x_{i+1}=x_i+\sqrt{x_i}$, in how many steps $x_r \ge 2x$?","If  and , in how many steps ?",x_0=x x_{i+1}=x_i+\sqrt{x_i} x_r \ge 2x,"In the question of the title, if I define $r(x)=\inf\{s\in \mathbb {N}:x_s\ge 2x\} $, i want to know how $r(x)$ behaves asymptotically, more precisely to know if $\lim\limits_{x\rightarrow \infty} \dfrac {r(x)}{\sqrt{x}}$ exists, and in that case,to find that limit.","In the question of the title, if I define $r(x)=\inf\{s\in \mathbb {N}:x_s\ge 2x\} $, i want to know how $r(x)$ behaves asymptotically, more precisely to know if $\lim\limits_{x\rightarrow \infty} \dfrac {r(x)}{\sqrt{x}}$ exists, and in that case,to find that limit.",,['real-analysis']
22,Completing the space of series so there is a slowest converging series,Completing the space of series so there is a slowest converging series,,"It is well known that there is no slowest converging infinite series (see e.g. here ). But there is also no largest rational number whose square <=2. Once we complete the rationals to the reals, such a number exists. Looking at the standard examples of series that diverge ever more slowly: ${1\over n}, {1 \over {n \ log(n)}}, {1\over{n \ log(n) \ log(log(n))}} ... $ you get the feeling that these are tending towards something , even if this something is not itself a series, or it is not a limit in the usual sense. Is there some notion of completion of the space of infinite sequences, so that some member of the larger space lies exactly at the border of convergence and and divergence? Edit : One possible way to do this would be via nonstandard analysis. Hyperreals are equivalence classes of sequences, and are totally ordered - i.e. they provide a way to say whether any sequence is larger than any another. So one might expect to find a ""boundary"" between sequences whose sum converges, and sequences whose sum does not converge. Specifically, you would totally order non-negative series by comparing the hyperreals defined by their partial sums $A_n = \sum_{m=1}^n a_m$. This order would respect convergence, in the sense that a divergent series could not be ""less than"" a convergent series, and would also respect convergence speeds, in the sense that if $a_n$ would be less than $b_n$ if $A_n/B_n \to 0$. However the hyperreals are not complete , so there need be no supremum to the set of sequences whose sum converges. It is possible to complete them , but the question then becomes what sort of objects are these completed hyperreals, and can we gain any intuition from them about our original question concerning convergent and divergent series. I found a related previous question . But no complete answer.","It is well known that there is no slowest converging infinite series (see e.g. here ). But there is also no largest rational number whose square <=2. Once we complete the rationals to the reals, such a number exists. Looking at the standard examples of series that diverge ever more slowly: ${1\over n}, {1 \over {n \ log(n)}}, {1\over{n \ log(n) \ log(log(n))}} ... $ you get the feeling that these are tending towards something , even if this something is not itself a series, or it is not a limit in the usual sense. Is there some notion of completion of the space of infinite sequences, so that some member of the larger space lies exactly at the border of convergence and and divergence? Edit : One possible way to do this would be via nonstandard analysis. Hyperreals are equivalence classes of sequences, and are totally ordered - i.e. they provide a way to say whether any sequence is larger than any another. So one might expect to find a ""boundary"" between sequences whose sum converges, and sequences whose sum does not converge. Specifically, you would totally order non-negative series by comparing the hyperreals defined by their partial sums $A_n = \sum_{m=1}^n a_m$. This order would respect convergence, in the sense that a divergent series could not be ""less than"" a convergent series, and would also respect convergence speeds, in the sense that if $a_n$ would be less than $b_n$ if $A_n/B_n \to 0$. However the hyperreals are not complete , so there need be no supremum to the set of sequences whose sum converges. It is possible to complete them , but the question then becomes what sort of objects are these completed hyperreals, and can we gain any intuition from them about our original question concerning convergent and divergent series. I found a related previous question . But no complete answer.",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'asymptotics', 'nonstandard-analysis']"
23,"If $f(x)$ is continuously decreasing and $\lim_{x \to \infty} f(x) = 0$, is $xf(x)$ uniformly continuous?","If  is continuously decreasing and , is  uniformly continuous?",f(x) \lim_{x \to \infty} f(x) = 0 xf(x),"Suppose $f:[0, \infty) \to [0, \infty)$ is decreasing and continuous with $\lim_{x \to \infty} f(x) = 0$. Let $g(x) = xf(x)$. Is $g(x)$ uniformly continuous on $[0, \infty)$? My work: I've been able to come up with a counterexample where $g(x)$ is not Lipschitz . Since Lipschitz is stronger than uniform continuity, this isn't a full solution, but nevertheless the ideas may be useful. That said, there might be a very simple counterexample that's eluding me . First, recall that there are continuous functions $h:[0, \infty) \to [0, \infty)$ such that $$\limsup_{x \to +\infty} \ h(x) = +\infty$$ but $\int_0^{+\infty} h(x) \ dx < +\infty$ Here is a brief construction: for integer $n \geq 1$, in each interval $[n, n+\frac{2^{1-n}}{n}]$ the graph of $h$ looks like an isosceles triangle of area $2^{-n}$ and height $n$. Elsewhere $h(x) = 0$. It's easy to see the integral of $h$ over $\mathbb{R}_+$ is $1$, but its $\lim \sup$ is $+\infty$. Now consider that $$f(x) = 1 - \int_0^{x} h(t) \ dt$$ is continuous (actually differentiable), decreasing and vanishing at $+\infty$. If we consider $g(x) = xf(x)$, note that $g'(x) = xf'(x) + f(x) = f(x) - xh(x)$. $f(x)$ is bounded and $|xh(x)|$ can be made arbitrarily large, and hence $|g'|$ can be made arbitrarily large, so there is no uniform bound on $\left|\frac{g(x) - g(y)}{x-y}\right|$. This implies $g$ is not Lipschitz. This example may or may not be uniformly continuous, but I can't prove it.","Suppose $f:[0, \infty) \to [0, \infty)$ is decreasing and continuous with $\lim_{x \to \infty} f(x) = 0$. Let $g(x) = xf(x)$. Is $g(x)$ uniformly continuous on $[0, \infty)$? My work: I've been able to come up with a counterexample where $g(x)$ is not Lipschitz . Since Lipschitz is stronger than uniform continuity, this isn't a full solution, but nevertheless the ideas may be useful. That said, there might be a very simple counterexample that's eluding me . First, recall that there are continuous functions $h:[0, \infty) \to [0, \infty)$ such that $$\limsup_{x \to +\infty} \ h(x) = +\infty$$ but $\int_0^{+\infty} h(x) \ dx < +\infty$ Here is a brief construction: for integer $n \geq 1$, in each interval $[n, n+\frac{2^{1-n}}{n}]$ the graph of $h$ looks like an isosceles triangle of area $2^{-n}$ and height $n$. Elsewhere $h(x) = 0$. It's easy to see the integral of $h$ over $\mathbb{R}_+$ is $1$, but its $\lim \sup$ is $+\infty$. Now consider that $$f(x) = 1 - \int_0^{x} h(t) \ dt$$ is continuous (actually differentiable), decreasing and vanishing at $+\infty$. If we consider $g(x) = xf(x)$, note that $g'(x) = xf'(x) + f(x) = f(x) - xh(x)$. $f(x)$ is bounded and $|xh(x)|$ can be made arbitrarily large, and hence $|g'|$ can be made arbitrarily large, so there is no uniform bound on $\left|\frac{g(x) - g(y)}{x-y}\right|$. This implies $g$ is not Lipschitz. This example may or may not be uniformly continuous, but I can't prove it.",,"['real-analysis', 'limits', 'continuity', 'uniform-continuity']"
24,Proving $\int_0^\infty\frac{\sin(x)}x\ dx=\frac{\pi}2$. Why is this step correct?,Proving . Why is this step correct?,\int_0^\infty\frac{\sin(x)}x\ dx=\frac{\pi}2,"I came across a different approach on the proof: $$\int_0^\infty \frac{\sin(x)}x\ dx=\frac{\pi}2$$ First, recall the identity: $$\sin(A)-\sin(B)=2\sin\left(\frac{A}2-\frac{B}2\right)\cos\left(\frac{A}2+\frac{B}2\right)$$  Applying the identity for: $$A=kx+\frac{x}2\ \land\  B=kx-\frac{x}2$$  We obtain:$$\sin\left(kx+\frac{x}2\right)-\sin\left(kx-\frac{x}2\right)=2\sin\left(\frac{x}2\right)\cos\left(kx\right)\Rightarrow \\\cos\left(kx\right)=\frac{\sin\left(kx+\frac{x}2\right)-\sin\left(kx-\frac{x}2\right)}{2\sin\left(\frac{x}2\right)}$$ Using the previous result, we can easily show that: $$\frac12+\cos(x)+\cos(2x)+\cdots+\cos(\lambda x)=\frac{\sin\left(\lambda x+\frac{x}2\right)}{2\sin\left(\frac{x}2\right)}  \quad \text{where $\lambda \in \mathbb{N}$}$$ Integrating the last expression: $$\int_0^\pi\frac{\sin\left(\lambda x+\frac{x}2\right)}{\sin\left(\frac{x}2\right)}\ dx=\int_0^\pi\left(1+2\cos(x)+2\cos(2x)+\cdots+2\cos(\lambda x)\right)\ dx=\pi$$ We can also prove (since $f(x)$ is continuous on $[0,\pi]$), using Riemann-Lebesgue Lemma, that: $$\lim_{\lambda\to\infty}\int_0^\pi\underbrace{\left(\frac2t-\frac1{\sin\left(\frac{t}2\right)}\right)}_{f(x)}\sin\left(\lambda t+\frac{t}2\right)dt=\lim_{\lambda\to\infty}\int_0^\pi\left(\frac{2\sin\left(\lambda t+\frac{t}2\right)}t-\frac{\sin\left(\lambda t+\frac{t}2\right)}{\sin\left(\frac{t}2\right)}\right)=0$$ Therefore: $$\left(1\right)\  \lim_{\lambda\to\infty}\int_0^\pi\frac{2\sin\left(\lambda t+\frac{t}2\right)}t=\lim_{\lambda\to\infty}\int_0^\pi\frac{\sin\left(\lambda t+\frac{t}2\right)}{\sin\left(\frac{t}2\right)}=\pi$$ $$$$Returning to the initial problem: $$\\$$ Let: $$x=\lambda t+\frac{t}2$$ Thus: $$\int_0^\infty \frac{\sin(x)}x\ dx \stackrel{\eqref{*}}=\frac12\lim_{\lambda\to\infty}\int_0^{\color{teal}{\pi}}\frac{2\sin\left(\lambda t+\frac{t}2\right)}{t}\ dt$$ Using the result obtained from $(1)$:$$\int_0^\infty \frac{\sin(x)}x\ dx=\boxed{\frac{\pi}2}$$ $$$$ My question comes from $\color{teal}{(???)}$, Why is it correct to have $\pi$ instead of $\infty$ when changing the limits of integration?","I came across a different approach on the proof: $$\int_0^\infty \frac{\sin(x)}x\ dx=\frac{\pi}2$$ First, recall the identity: $$\sin(A)-\sin(B)=2\sin\left(\frac{A}2-\frac{B}2\right)\cos\left(\frac{A}2+\frac{B}2\right)$$  Applying the identity for: $$A=kx+\frac{x}2\ \land\  B=kx-\frac{x}2$$  We obtain:$$\sin\left(kx+\frac{x}2\right)-\sin\left(kx-\frac{x}2\right)=2\sin\left(\frac{x}2\right)\cos\left(kx\right)\Rightarrow \\\cos\left(kx\right)=\frac{\sin\left(kx+\frac{x}2\right)-\sin\left(kx-\frac{x}2\right)}{2\sin\left(\frac{x}2\right)}$$ Using the previous result, we can easily show that: $$\frac12+\cos(x)+\cos(2x)+\cdots+\cos(\lambda x)=\frac{\sin\left(\lambda x+\frac{x}2\right)}{2\sin\left(\frac{x}2\right)}  \quad \text{where $\lambda \in \mathbb{N}$}$$ Integrating the last expression: $$\int_0^\pi\frac{\sin\left(\lambda x+\frac{x}2\right)}{\sin\left(\frac{x}2\right)}\ dx=\int_0^\pi\left(1+2\cos(x)+2\cos(2x)+\cdots+2\cos(\lambda x)\right)\ dx=\pi$$ We can also prove (since $f(x)$ is continuous on $[0,\pi]$), using Riemann-Lebesgue Lemma, that: $$\lim_{\lambda\to\infty}\int_0^\pi\underbrace{\left(\frac2t-\frac1{\sin\left(\frac{t}2\right)}\right)}_{f(x)}\sin\left(\lambda t+\frac{t}2\right)dt=\lim_{\lambda\to\infty}\int_0^\pi\left(\frac{2\sin\left(\lambda t+\frac{t}2\right)}t-\frac{\sin\left(\lambda t+\frac{t}2\right)}{\sin\left(\frac{t}2\right)}\right)=0$$ Therefore: $$\left(1\right)\  \lim_{\lambda\to\infty}\int_0^\pi\frac{2\sin\left(\lambda t+\frac{t}2\right)}t=\lim_{\lambda\to\infty}\int_0^\pi\frac{\sin\left(\lambda t+\frac{t}2\right)}{\sin\left(\frac{t}2\right)}=\pi$$ $$$$Returning to the initial problem: $$\\$$ Let: $$x=\lambda t+\frac{t}2$$ Thus: $$\int_0^\infty \frac{\sin(x)}x\ dx \stackrel{\eqref{*}}=\frac12\lim_{\lambda\to\infty}\int_0^{\color{teal}{\pi}}\frac{2\sin\left(\lambda t+\frac{t}2\right)}{t}\ dt$$ Using the result obtained from $(1)$:$$\int_0^\infty \frac{\sin(x)}x\ dx=\boxed{\frac{\pi}2}$$ $$$$ My question comes from $\color{teal}{(???)}$, Why is it correct to have $\pi$ instead of $\infty$ when changing the limits of integration?",,['real-analysis']
25,Linear independence of a set of solutions and the Wronskian,Linear independence of a set of solutions and the Wronskian,,"Consider a general $n$th order linear equation   $$x^{n}(t)+a_{n-1}x^{n-1}(t)+ \dots + a_{1}x'(t) + a_{0}x(t)=0\tag{$*$}.$$   Let $x_1, x_2 , \dots , x_n$ be a fundamental set of solutions of above and set $W(t)=W(x_1, x_2 , \dots , x_n ; t).$ Question. Show that a set of solutions $x_1 , x_2 , \dots , x_k$ of $(*)$ are linearly  independent over $(-\infty, \infty)$ if and only if their Wronskian $W(x_1 , x_2 , \dots , x_k; t_0) \neq 0$ for some $t_0 \in (-\infty, \infty).$ Also show that those solutions form a vector space of dimension $n$. My approach : Writing the equivalent first order system, $$y_1=x ,~y_2=x' ,~\dots~,y_n=x^{(n-1)},$$ from which we get $$y_1'=y_2,~~y_2'=y_3,~~\dots~~,y_{n-1}'=y_n,~~y_n'=-a_{n-1}(t) y_n- \cdots - a_{1}(t) y_2-a_{0}(t) y_1.$$ For the contrapositive statement: i.e., if $W(x_1 , x_2 , \dots , x_k; t_0) = 0,$ for some $t_0 \in (-\infty, \infty),$ doesn't that clearly implies that the set of vectors $\{ x_1 , x_2 , \dots , x_k \}$ is linearly dependent. I'm stuck in progressing any further. Any help in proving this is much appreciated.","Consider a general $n$th order linear equation   $$x^{n}(t)+a_{n-1}x^{n-1}(t)+ \dots + a_{1}x'(t) + a_{0}x(t)=0\tag{$*$}.$$   Let $x_1, x_2 , \dots , x_n$ be a fundamental set of solutions of above and set $W(t)=W(x_1, x_2 , \dots , x_n ; t).$ Question. Show that a set of solutions $x_1 , x_2 , \dots , x_k$ of $(*)$ are linearly  independent over $(-\infty, \infty)$ if and only if their Wronskian $W(x_1 , x_2 , \dots , x_k; t_0) \neq 0$ for some $t_0 \in (-\infty, \infty).$ Also show that those solutions form a vector space of dimension $n$. My approach : Writing the equivalent first order system, $$y_1=x ,~y_2=x' ,~\dots~,y_n=x^{(n-1)},$$ from which we get $$y_1'=y_2,~~y_2'=y_3,~~\dots~~,y_{n-1}'=y_n,~~y_n'=-a_{n-1}(t) y_n- \cdots - a_{1}(t) y_2-a_{0}(t) y_1.$$ For the contrapositive statement: i.e., if $W(x_1 , x_2 , \dots , x_k; t_0) = 0,$ for some $t_0 \in (-\infty, \infty),$ doesn't that clearly implies that the set of vectors $\{ x_1 , x_2 , \dots , x_k \}$ is linearly dependent. I'm stuck in progressing any further. Any help in proving this is much appreciated.",,['real-analysis']
26,Integration of Power Series. When is the Function in $L_1$?,Integration of Power Series. When is the Function in ?,L_1,"Suppose we are given a function $f$ in terms of its power series \begin{align} f(x)= \sum_{n+0}^\infty a_n x^n. \end{align} We assume that we know all $a_n$'s and $f$ has infinite radius of convergense. Can we say based on $a_n$ if the function is integrable or not? That is if  \begin{align} \int_{\mathbb{R}} |f(x)| dx<\infty. \end{align} For example, if $a_n \ge 0$ then we have that \begin{align} f(x) \ge a_0+a_1 x,\  x >0 , \end{align} and the function is not integrable. The case that I am interested is when $a_n$'s have alternating sign.    Specifically, can we determine if the following $f(x)$ is integrable   \begin{align} f(x)= \sum_{k=0}^\infty \frac{\cos( \frac{\pi}{2}k)\  (k+1)^{\frac{k+1}{6}}}{k!} x^k. \end{align} Here is the plot of $f(x)$ where the series was computed up to $N=600$. where blue curve is $f(x)$ and red curve is $sinc(\pi/2 x)= \frac{\sin(\pi/2 x)}{\pi/2 x}$. It seems that $f(x)$ has a tail that decays faster than $sinc(\pi/2 x)$. Moreover, next we give a plot of $f(x) \cdot x$  and  $f(x) \cdot x^2$ where blue curve is $f(x) \cdot x$ and red curve is  $f(x) \cdot x^2$. This seems to indicate that $f(x)$ decreases faster than $\frac{1}{x^2}$ which would imply that $f(x)$ is integrable. Therefore, it would also be interesting to show that  \begin{align} \lim_{x \to \infty} f(x)=0, \end{align} which at this moment I do not know how to do. Thanks you. **Edit: **  Please see a possible solution via Mellin transform.","Suppose we are given a function $f$ in terms of its power series \begin{align} f(x)= \sum_{n+0}^\infty a_n x^n. \end{align} We assume that we know all $a_n$'s and $f$ has infinite radius of convergense. Can we say based on $a_n$ if the function is integrable or not? That is if  \begin{align} \int_{\mathbb{R}} |f(x)| dx<\infty. \end{align} For example, if $a_n \ge 0$ then we have that \begin{align} f(x) \ge a_0+a_1 x,\  x >0 , \end{align} and the function is not integrable. The case that I am interested is when $a_n$'s have alternating sign.    Specifically, can we determine if the following $f(x)$ is integrable   \begin{align} f(x)= \sum_{k=0}^\infty \frac{\cos( \frac{\pi}{2}k)\  (k+1)^{\frac{k+1}{6}}}{k!} x^k. \end{align} Here is the plot of $f(x)$ where the series was computed up to $N=600$. where blue curve is $f(x)$ and red curve is $sinc(\pi/2 x)= \frac{\sin(\pi/2 x)}{\pi/2 x}$. It seems that $f(x)$ has a tail that decays faster than $sinc(\pi/2 x)$. Moreover, next we give a plot of $f(x) \cdot x$  and  $f(x) \cdot x^2$ where blue curve is $f(x) \cdot x$ and red curve is  $f(x) \cdot x^2$. This seems to indicate that $f(x)$ decreases faster than $\frac{1}{x^2}$ which would imply that $f(x)$ is integrable. Therefore, it would also be interesting to show that  \begin{align} \lim_{x \to \infty} f(x)=0, \end{align} which at this moment I do not know how to do. Thanks you. **Edit: **  Please see a possible solution via Mellin transform.",,"['real-analysis', 'integration', 'sequences-and-series', 'power-series', 'lp-spaces']"
27,Prove that function has only one maximum,Prove that function has only one maximum,,"I have a function $f_n(x)$ with an integer parameter $n \in \{3,4,5,\dots\}$ and $x\in ]0,1[$, and i want to show that $f_n(x)$ has only one critical point for every value of $n$. The function is $$ f_n(x)=(1 - x)^n + (1 + x)^n + \frac{  x [(1 - x)^n - (1 + x)^n]}{ \sqrt{1 - x^2}-1}.  $$ Setting the derivative to zero, gives after some simplification and case differentiations the equation $$ \left(\frac{1-x}{1+x}\right)^n= \frac{1+n \left(1-x -\sqrt{1-x^2}\right)}{1+n \left(1+x -\sqrt{1-x^2}\right)}. \quad \quad (1)$$ So if there is only one $x \in ]0,1[$ that fullfills equation (1), the problem is solved. However i struggle to prove that. (The fact is obvious from plotting the function) Maybe there is a better way to show that, instead of setting the derivative to zero? Just as an example consider the plot of the function for $n=5$, Update (with the help of MotylaNogaTomkaMazura) If you parametrize $x=\cos(2t)$ with $t \in [0,\pi/4]$, the original function can be written as $$ g_k(t)=\frac{\cos ^k(t)-\sin ^k(t)}{\cos (t)-\sin (t)},$$ with $k=2n+1$ and omitting a factor $2^{1 + n}$. The maximum of $g_k(t)$ is the maximum of $f_n(x)$. An equivalent form is $$ g_k(t)=\frac{1}{\sin(t)} \sum_{n=1}^k \cos(t)^{k - n} \sin(t)^n.$$ Setting the derivative to zero gives $$ k \left( \frac{\tan(t)}{\tan^k(t)-1}-\frac{\cot(t)}{\cot^k(t)-1} \right)=\frac{1}{\cot(t)-1}-\frac{1}{\tan(t)-1}.$$ Update 2 Another way to rewrite the function $f_n(x)$ is $$h_m(t)=\cos ^m(t) \cdot \sum _{k=0}^m \tan ^k(t),\quad \quad \quad (2)$$ with $x=\cos(2t)$, $t \in [0,\pi/4]$ and $m=2n$, again with omitting the factor $2^{1 + n}$. So proving that $h_m(t)$ has only one maximum is equivalent to the original problem. Now we have the product of two functions one is monotonically increasing and convex, and the other is monotonically decreasing and convex. Is this enough to prove that the function has one and only one maximum for all $n$?","I have a function $f_n(x)$ with an integer parameter $n \in \{3,4,5,\dots\}$ and $x\in ]0,1[$, and i want to show that $f_n(x)$ has only one critical point for every value of $n$. The function is $$ f_n(x)=(1 - x)^n + (1 + x)^n + \frac{  x [(1 - x)^n - (1 + x)^n]}{ \sqrt{1 - x^2}-1}.  $$ Setting the derivative to zero, gives after some simplification and case differentiations the equation $$ \left(\frac{1-x}{1+x}\right)^n= \frac{1+n \left(1-x -\sqrt{1-x^2}\right)}{1+n \left(1+x -\sqrt{1-x^2}\right)}. \quad \quad (1)$$ So if there is only one $x \in ]0,1[$ that fullfills equation (1), the problem is solved. However i struggle to prove that. (The fact is obvious from plotting the function) Maybe there is a better way to show that, instead of setting the derivative to zero? Just as an example consider the plot of the function for $n=5$, Update (with the help of MotylaNogaTomkaMazura) If you parametrize $x=\cos(2t)$ with $t \in [0,\pi/4]$, the original function can be written as $$ g_k(t)=\frac{\cos ^k(t)-\sin ^k(t)}{\cos (t)-\sin (t)},$$ with $k=2n+1$ and omitting a factor $2^{1 + n}$. The maximum of $g_k(t)$ is the maximum of $f_n(x)$. An equivalent form is $$ g_k(t)=\frac{1}{\sin(t)} \sum_{n=1}^k \cos(t)^{k - n} \sin(t)^n.$$ Setting the derivative to zero gives $$ k \left( \frac{\tan(t)}{\tan^k(t)-1}-\frac{\cot(t)}{\cot^k(t)-1} \right)=\frac{1}{\cot(t)-1}-\frac{1}{\tan(t)-1}.$$ Update 2 Another way to rewrite the function $f_n(x)$ is $$h_m(t)=\cos ^m(t) \cdot \sum _{k=0}^m \tan ^k(t),\quad \quad \quad (2)$$ with $x=\cos(2t)$, $t \in [0,\pi/4]$ and $m=2n$, again with omitting the factor $2^{1 + n}$. So proving that $h_m(t)$ has only one maximum is equivalent to the original problem. Now we have the product of two functions one is monotonically increasing and convex, and the other is monotonically decreasing and convex. Is this enough to prove that the function has one and only one maximum for all $n$?",,['real-analysis']
28,Why do we use open sets for outer measure?,Why do we use open sets for outer measure?,,"The standard definition for outer measure of a set of real numbers $A$ is: $$ m*(A) = inf {\Large \{} \sum_{k=1}^{\infty} \ell(I_k) \; {\Large |} \; A \subseteq \bigcup_{k=1}^{\infty} I_k {\Large \}} $$ (as, for example, in Royden, Real Analysis, 4th ed., p. 31) where the $I_k$ are required to be nonempty, open , bounded intervals. My question is: Would it amount to the same thing if we instead required that the $I_k$ are nonempty, closed , bounded intervals? It feels like there should be some reason why inner measure uses closed sets and outer measure uses open sets, but I can't think of an example set $A$ where $m*(A)$ would differ from $m**(A)$ defined exactly as above except that each $I_k$ is required to be closed.","The standard definition for outer measure of a set of real numbers $A$ is: $$ m*(A) = inf {\Large \{} \sum_{k=1}^{\infty} \ell(I_k) \; {\Large |} \; A \subseteq \bigcup_{k=1}^{\infty} I_k {\Large \}} $$ (as, for example, in Royden, Real Analysis, 4th ed., p. 31) where the $I_k$ are required to be nonempty, open , bounded intervals. My question is: Would it amount to the same thing if we instead required that the $I_k$ are nonempty, closed , bounded intervals? It feels like there should be some reason why inner measure uses closed sets and outer measure uses open sets, but I can't think of an example set $A$ where $m*(A)$ would differ from $m**(A)$ defined exactly as above except that each $I_k$ is required to be closed.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
29,Is $\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu$ a convergent integral?,Is  a convergent integral?,\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu,"Is the  following integral a convergent integral? Can we compute it, precisely? $$\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu $$ Here $\mu$ is the usual measure of $M_{n}(\mathbb{R})\simeq \mathbb{R}^{n^{2}}$? So $\mu$  can be  counted  as $\mu=\prod_{i,j} da_{ij}$ Note: If this integral would be convergent , either in Lebesgue or  in Riemann sense, then it would be equal to a scalar matrix. Because for every invertible matrix  $P$ we  have: $P^{-1}(\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu) P= \int_{M_{n}(\mathbb{R})} e^{-(P^{-1}AP)^{2}}d\mu=\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu$ since the  mapping $A\mapsto P^{-1}AP$  is  a  measure preserving and volum preserving linear map.Now we apply the  change of coordinate formula for integral.","Is the  following integral a convergent integral? Can we compute it, precisely? $$\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu $$ Here $\mu$ is the usual measure of $M_{n}(\mathbb{R})\simeq \mathbb{R}^{n^{2}}$? So $\mu$  can be  counted  as $\mu=\prod_{i,j} da_{ij}$ Note: If this integral would be convergent , either in Lebesgue or  in Riemann sense, then it would be equal to a scalar matrix. Because for every invertible matrix  $P$ we  have: $P^{-1}(\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu) P= \int_{M_{n}(\mathbb{R})} e^{-(P^{-1}AP)^{2}}d\mu=\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu$ since the  mapping $A\mapsto P^{-1}AP$  is  a  measure preserving and volum preserving linear map.Now we apply the  change of coordinate formula for integral.",,"['real-analysis', 'integration', 'matrices', 'improper-integrals', 'gaussian-integral']"
30,"If $u\in L^1(0,1)$ is nonnegative and $E_n = \int_0^1 x^n u(x) \, dx$, prove $E_{n-k} E_k \leq E_0 E_n$.","If  is nonnegative and , prove .","u\in L^1(0,1) E_n = \int_0^1 x^n u(x) \, dx E_{n-k} E_k \leq E_0 E_n","$\textbf{Question:}$ Let $ u \in L^1(0,1)$ be a nonnegative function. Define $$E_n := \int_0^1 x^n u(x) dx$$ Prove the following inequality, $\forall n \ge 0$, and $\forall k \in [0,n]$, we have $$ E_{n-k} E_k \le E_0 E_n$$ $\textbf{My Attempt:}$ We have, $$ E_0 := \int_0^1 u(x) dx$$ $$E_1= \int_0^1 x u(x) dx$$ $$E_2 = \int_0^1 x^2 u(x) dx$$ Since $x \in (0,1)$, we have $$E_0 \ge E_1 \ge E_2 \dots$$ Thus $$ E_n E_0 \ge E_n E_1 \ge E_n E_2 \ge \dots $$ To show that $E_0 E_n \ge E_k E_{n-k}$, we must show that $$\frac{E_0}{E_k} \ge \frac{E_{n-k}}{E_n}$$ This is equivalent to show that $x^{-k} \ge x^{n-2k}$ which is true as lons as $-k \le n-2k$, which is whenever $k \ge n$. Is the above proof correct?","$\textbf{Question:}$ Let $ u \in L^1(0,1)$ be a nonnegative function. Define $$E_n := \int_0^1 x^n u(x) dx$$ Prove the following inequality, $\forall n \ge 0$, and $\forall k \in [0,n]$, we have $$ E_{n-k} E_k \le E_0 E_n$$ $\textbf{My Attempt:}$ We have, $$ E_0 := \int_0^1 u(x) dx$$ $$E_1= \int_0^1 x u(x) dx$$ $$E_2 = \int_0^1 x^2 u(x) dx$$ Since $x \in (0,1)$, we have $$E_0 \ge E_1 \ge E_2 \dots$$ Thus $$ E_n E_0 \ge E_n E_1 \ge E_n E_2 \ge \dots $$ To show that $E_0 E_n \ge E_k E_{n-k}$, we must show that $$\frac{E_0}{E_k} \ge \frac{E_{n-k}}{E_n}$$ This is equivalent to show that $x^{-k} \ge x^{n-2k}$ which is true as lons as $-k \le n-2k$, which is whenever $k \ge n$. Is the above proof correct?",,"['real-analysis', 'inequality']"
31,Nonlinear heat equation $u_{t} = \Delta(u^{4})$,Nonlinear heat equation,u_{t} = \Delta(u^{4}),"Consider the nonlinear heat equation $u_{t} = \Delta(u^{4})$ in $\{x \in \mathbb{R}^{3}: |x| < 1\}$ with $u = 0$ on $\{x \in \mathbb{R}^{3}: |x| = 1\}$. The problem I am working on is to show that all solutions to this PDE tend to zero (pointwise) as $t \rightarrow \infty$. The equation looks like the porous medium equation so I thought about looking at how the Barenblatt solution is derived, but the issue is that this doesn't cover all solutions, so I'm thinking that there is a general argument to prove the above result.","Consider the nonlinear heat equation $u_{t} = \Delta(u^{4})$ in $\{x \in \mathbb{R}^{3}: |x| < 1\}$ with $u = 0$ on $\{x \in \mathbb{R}^{3}: |x| = 1\}$. The problem I am working on is to show that all solutions to this PDE tend to zero (pointwise) as $t \rightarrow \infty$. The equation looks like the porous medium equation so I thought about looking at how the Barenblatt solution is derived, but the issue is that this doesn't cover all solutions, so I'm thinking that there is a general argument to prove the above result.",,"['real-analysis', 'analysis', 'partial-differential-equations']"
32,one-sided differentiability,one-sided differentiability,,"Well known theorem: If $f\colon\mathbb{R}\to\mathbb{R}$ is differentiable and $f'(x)=0$ for all $x$, then $f$ is constant. The assumption of differentiability can be weakened to continuity and one-sided differentiability: If $f\colon\mathbb{R}\to\mathbb{R}$ is continuous and for every $x$ function $f$ is right differentiable at $x$ and the right derivative equals 0, then $f$ is constant. This is also a known fact, and the same holds when ""right"" is replaced with ""left"". I wonder if we could make it even stronger: If $f\colon\mathbb{R}\to\mathbb{R}$ is continuous and for every $x$ left or right derivative exists and equals 0, then $f$ is constant? (in my version the sides can be different for different points -- this is the difference between my conjecture and the theorem)","Well known theorem: If $f\colon\mathbb{R}\to\mathbb{R}$ is differentiable and $f'(x)=0$ for all $x$, then $f$ is constant. The assumption of differentiability can be weakened to continuity and one-sided differentiability: If $f\colon\mathbb{R}\to\mathbb{R}$ is continuous and for every $x$ function $f$ is right differentiable at $x$ and the right derivative equals 0, then $f$ is constant. This is also a known fact, and the same holds when ""right"" is replaced with ""left"". I wonder if we could make it even stronger: If $f\colon\mathbb{R}\to\mathbb{R}$ is continuous and for every $x$ left or right derivative exists and equals 0, then $f$ is constant? (in my version the sides can be different for different points -- this is the difference between my conjecture and the theorem)",,['real-analysis']
33,$f$ is continuous and open implies $f$ injective,is continuous and open implies  injective,f f,"Question: Let $f: \mathbb R \to \mathbb R$ be continuous and open, that is if $A \subset \mathbb R$ is open then $f(A) \subset \mathbb R$ is open. Prove that $f$ is injective. Attempt: Suppose $f$ is not injective then there exit $x, y \in \mathbb R$ such that $$x < y \implies f(x) = f(y) = c$$ Take the closed inteval $[x,y] \subset \mathbb R$, as $f$ is continuous then $\displaystyle {f|_{[x,y]}}$ is continuous, by Weierstrass Theorem we have that $f$ has a maximum or a minimum point. Let's assume $m = \max \{f(a) ; a \in [x,y]\}$. Now there is  $x' \in [x,y]$ such that $f(x') = m$. If we take the open $(x'-\delta, x'+\delta)$ centered at $x'$ and  we have (1)  If $m = c$ then $f$ is constant on the interval $[x,y]$ and $f((x'-\delta, x'+\delta)) = \{c\}$ which is closed, thus a contradiction; (2)  If $m \neq c$ then we would have $f((x'-\delta,  x'+\delta)) = (b, m]$, where $b$ can also be $b = \infty$. Again a contradiction. Well, this is my least embarassing attempt. I'm not sure how to show $(2)$ $100 \%$. I have also tried to show $f^{-1}f(A) = A$ for any $A \subset \mathbb R$, tried to work on the connected space  $\mathbb R$ by finding a contradiction using the intervals $E_{[f > c]}$ and  $E_{[f < c]}$ open when $A$ is open. Any thoughts? Note: I've already seen this to try something out, but the fact that $f$ is monotone on this exercise comes as a consequence.","Question: Let $f: \mathbb R \to \mathbb R$ be continuous and open, that is if $A \subset \mathbb R$ is open then $f(A) \subset \mathbb R$ is open. Prove that $f$ is injective. Attempt: Suppose $f$ is not injective then there exit $x, y \in \mathbb R$ such that $$x < y \implies f(x) = f(y) = c$$ Take the closed inteval $[x,y] \subset \mathbb R$, as $f$ is continuous then $\displaystyle {f|_{[x,y]}}$ is continuous, by Weierstrass Theorem we have that $f$ has a maximum or a minimum point. Let's assume $m = \max \{f(a) ; a \in [x,y]\}$. Now there is  $x' \in [x,y]$ such that $f(x') = m$. If we take the open $(x'-\delta, x'+\delta)$ centered at $x'$ and  we have (1)  If $m = c$ then $f$ is constant on the interval $[x,y]$ and $f((x'-\delta, x'+\delta)) = \{c\}$ which is closed, thus a contradiction; (2)  If $m \neq c$ then we would have $f((x'-\delta,  x'+\delta)) = (b, m]$, where $b$ can also be $b = \infty$. Again a contradiction. Well, this is my least embarassing attempt. I'm not sure how to show $(2)$ $100 \%$. I have also tried to show $f^{-1}f(A) = A$ for any $A \subset \mathbb R$, tried to work on the connected space  $\mathbb R$ by finding a contradiction using the intervals $E_{[f > c]}$ and  $E_{[f < c]}$ open when $A$ is open. Any thoughts? Note: I've already seen this to try something out, but the fact that $f$ is monotone on this exercise comes as a consequence.",,"['real-analysis', 'analysis', 'continuity']"
34,Asymptotics of $\prod_{x=1}^{\lceil\frac{n}{\log_2{n} }\rceil} \left(\frac{1}{\sqrt{n}} + x\left(\frac{1}{n}-\frac{2}{n^\frac{3}{2}} \right)\right) $,Asymptotics of,\prod_{x=1}^{\lceil\frac{n}{\log_2{n} }\rceil} \left(\frac{1}{\sqrt{n}} + x\left(\frac{1}{n}-\frac{2}{n^\frac{3}{2}} \right)\right) ,"I am trying to work out the large $n$ asymptotics of $$S_n = \prod_{x=1}^{\lceil\frac{n}{\log_2{n} }\rceil} \left(\frac{1}{\sqrt{n}} + x\left(\frac{1}{n}-\frac{2}{n^\frac{3}{2}} \right)\right) .$$ Here is my attempt so far $$\prod_{x=1}^{k} (A + Bx) = \frac{B^k \Gamma(k+1+A/B)}{\Gamma(1+A/B)}.$$ In our case $A/B \approx \sqrt{n}$ and $\left(\frac{1}{n}-\frac{2}{n^\frac{3}{2}} \right) \approx \frac{1}{n}$.  Therefore $$S_n \approx \frac{\frac{1}{n}^{\frac{n}{\log_2{n}}} \left(\frac{n}{\log_2{n}} + \sqrt{n}\right)!}{\sqrt{n}!}$$ I am not really sure where to go from here, if I haven't already taken an approximation too far. I tried taking logs and defining in maple f:=(x,n)-> log(1/sqrt(n)+x*(1/n-2/n^(3/2))) Now if you do plot(-(sum(f(x, n), x = 1 .. n/log(n))), n = 10 .. 100) you get what looks like a linear function of $n$. However if you do limit(-(sum(f(x, n), x = 1 .. n/log(n)))/n, n = infinity) you get $0$.","I am trying to work out the large $n$ asymptotics of $$S_n = \prod_{x=1}^{\lceil\frac{n}{\log_2{n} }\rceil} \left(\frac{1}{\sqrt{n}} + x\left(\frac{1}{n}-\frac{2}{n^\frac{3}{2}} \right)\right) .$$ Here is my attempt so far $$\prod_{x=1}^{k} (A + Bx) = \frac{B^k \Gamma(k+1+A/B)}{\Gamma(1+A/B)}.$$ In our case $A/B \approx \sqrt{n}$ and $\left(\frac{1}{n}-\frac{2}{n^\frac{3}{2}} \right) \approx \frac{1}{n}$.  Therefore $$S_n \approx \frac{\frac{1}{n}^{\frac{n}{\log_2{n}}} \left(\frac{n}{\log_2{n}} + \sqrt{n}\right)!}{\sqrt{n}!}$$ I am not really sure where to go from here, if I haven't already taken an approximation too far. I tried taking logs and defining in maple f:=(x,n)-> log(1/sqrt(n)+x*(1/n-2/n^(3/2))) Now if you do plot(-(sum(f(x, n), x = 1 .. n/log(n))), n = 10 .. 100) you get what looks like a linear function of $n$. However if you do limit(-(sum(f(x, n), x = 1 .. n/log(n)))/n, n = infinity) you get $0$.",,['calculus']
35,"Given $f(0)>0, f(1)<0$ then prove $\exists x_0$ st $f(x_0)=0$",Given  then prove  st,"f(0)>0, f(1)<0 \exists x_0 f(x_0)=0","Let $f$ be a function satisfying $f(0)>0, f(1)<0$ , prove that $\exists x_0$ st $f(x_0)=0$ under the assumption that there exists continuous function $g(x)$ such that $f+g$ is non decreasing. I noticed that if $0\le x\le1$ then $$g(0)<g(0)+f(0)\le g(x)+f(x)\le g(1)+f(1)< g(1)$$ so that $g$ assumes all values $g(x)+f(x)$ for $x\in(0,1)$ using intermediate value property.How shall I proceed further?","Let $f$ be a function satisfying $f(0)>0, f(1)<0$ , prove that $\exists x_0$ st $f(x_0)=0$ under the assumption that there exists continuous function $g(x)$ such that $f+g$ is non decreasing. I noticed that if $0\le x\le1$ then $$g(0)<g(0)+f(0)\le g(x)+f(x)\le g(1)+f(1)< g(1)$$ so that $g$ assumes all values $g(x)+f(x)$ for $x\in(0,1)$ using intermediate value property.How shall I proceed further?",,['real-analysis']
36,Show the following limit $\lim_{n\rightarrow\infty}\sqrt{n}\int_0^1\frac{f(x)}{1+nx^2}\text{d}x=\frac{\pi}{2}f(0)$ [closed],Show the following limit  [closed],\lim_{n\rightarrow\infty}\sqrt{n}\int_0^1\frac{f(x)}{1+nx^2}\text{d}x=\frac{\pi}{2}f(0),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Prove that $$\lim_{n\rightarrow\infty}\sqrt{n}\int_0^1\frac{f(x)}{1+nx^2}\text{d}x=\frac{\pi}{2}f(0),$$ where $f(x)$ is a continuous function on $[0,1].$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Prove that $$\lim_{n\rightarrow\infty}\sqrt{n}\int_0^1\frac{f(x)}{1+nx^2}\text{d}x=\frac{\pi}{2}f(0),$$ where $f(x)$ is a continuous function on $[0,1].$",,"['real-analysis', 'limits']"
37,Infinite product simplification,Infinite product simplification,,I found the following identity $\prod_{k=0}^{\infty}(1+\frac{1}{2^{2^k}-1})=\frac{1}{2}+\sum_{k=0}^{\infty}\frac{1}{\prod_{j=0}^{k-1}(2^{2^j}-1)}$ My first thought was to use eulers identity somehow $\prod_{k=1}^{\infty}(1+z^k)=\prod_{k=1}^{\infty}(1-z^{2k-1})^{-1}$ but it does not help me. If you have an idea or know a helpful identity to prove this result I would really appreciate it.,I found the following identity $\prod_{k=0}^{\infty}(1+\frac{1}{2^{2^k}-1})=\frac{1}{2}+\sum_{k=0}^{\infty}\frac{1}{\prod_{j=0}^{k-1}(2^{2^j}-1)}$ My first thought was to use eulers identity somehow $\prod_{k=1}^{\infty}(1+z^k)=\prod_{k=1}^{\infty}(1-z^{2k-1})^{-1}$ but it does not help me. If you have an idea or know a helpful identity to prove this result I would really appreciate it.,,"['real-analysis', 'infinite-product']"
38,Proving a limit involving multiple variables using epsilon-delta,Proving a limit involving multiple variables using epsilon-delta,,"I'm having some issues with $\varepsilon$-$\delta$ proofs of limits with more than one variable. I understand the $\varepsilon$-$\delta$ definition of a limit, but I don't know how to deal with multiple variables. Here's a simple example: prove the following limit, if it exists, using the epsilon-delta definition: $$ \lim_{(x,y)\to (1,2)}\frac{x^2}{x+y} = a \iff \forall(\varepsilon>0)\, \exists(\delta) \left[   |(x,y)-(1,2)|<\delta \implies   \left|\frac{x^2}{x+y}-a\right|<\varepsilon \right] $$ Obviously this limit is $1/3$, but how do I prove it?","I'm having some issues with $\varepsilon$-$\delta$ proofs of limits with more than one variable. I understand the $\varepsilon$-$\delta$ definition of a limit, but I don't know how to deal with multiple variables. Here's a simple example: prove the following limit, if it exists, using the epsilon-delta definition: $$ \lim_{(x,y)\to (1,2)}\frac{x^2}{x+y} = a \iff \forall(\varepsilon>0)\, \exists(\delta) \left[   |(x,y)-(1,2)|<\delta \implies   \left|\frac{x^2}{x+y}-a\right|<\varepsilon \right] $$ Obviously this limit is $1/3$, but how do I prove it?",,['calculus']
39,Prove that $\lim\limits_{n\rightarrow \infty}\int_1^3\frac{nx^{99}+5}{x^3+nx^{66}} d x$ exists and evaluate it.,Prove that  exists and evaluate it.,\lim\limits_{n\rightarrow \infty}\int_1^3\frac{nx^{99}+5}{x^3+nx^{66}} d x,"I am trying to show that $\lim_{n\rightarrow \infty}\int_1^3\dfrac{nx^{99}+5}{x^3+nx^{66}} d x$ exists and what its value is. I know that to do this I must show that $\dfrac{nx^{99}+5}{x^3+nx^{66}}\rightarrow x^{33}$ uniformly on $[1,3]$ and that each $\dfrac{nx^{99}+5}{x^3+nx^{66}}$ is integrable on $[1,3]$ and the rest will follow. I am having a difficult time showing that $\dfrac{nx^{99}+5}{x^3+nx^{66}}\rightarrow x^{33}$ uniformly on $[1,3]$. So far my proof for uniform convergence is as follows. Let $\epsilon > 0$, choose $N\in \mathbb{N}$ such that $\left|\frac{4}{N+1}\right|<\epsilon$. Then $n\geq N$ implies \begin{align*} \left|\frac{nx^{99}+5}{x^3+nx^{66}}-x^{33}\right|&=\left|\frac{nx^{99}+5-x^{33}(x^3+nx^{66})}{x^3+nx^{66}}\right|\\ &=\left|\frac{5-x^{36}}{x^3+nx^{66}}\right|\\ &\leq\left|\frac{4}{n+1}\right|\\ &\leq\left|\frac{4}{N+1}\right|<\epsilon \end{align*} and so we have uniform convergence on $[1,3]$.","I am trying to show that $\lim_{n\rightarrow \infty}\int_1^3\dfrac{nx^{99}+5}{x^3+nx^{66}} d x$ exists and what its value is. I know that to do this I must show that $\dfrac{nx^{99}+5}{x^3+nx^{66}}\rightarrow x^{33}$ uniformly on $[1,3]$ and that each $\dfrac{nx^{99}+5}{x^3+nx^{66}}$ is integrable on $[1,3]$ and the rest will follow. I am having a difficult time showing that $\dfrac{nx^{99}+5}{x^3+nx^{66}}\rightarrow x^{33}$ uniformly on $[1,3]$. So far my proof for uniform convergence is as follows. Let $\epsilon > 0$, choose $N\in \mathbb{N}$ such that $\left|\frac{4}{N+1}\right|<\epsilon$. Then $n\geq N$ implies \begin{align*} \left|\frac{nx^{99}+5}{x^3+nx^{66}}-x^{33}\right|&=\left|\frac{nx^{99}+5-x^{33}(x^3+nx^{66})}{x^3+nx^{66}}\right|\\ &=\left|\frac{5-x^{36}}{x^3+nx^{66}}\right|\\ &\leq\left|\frac{4}{n+1}\right|\\ &\leq\left|\frac{4}{N+1}\right|<\epsilon \end{align*} and so we have uniform convergence on $[1,3]$.",,"['real-analysis', 'limits']"
40,Mistake in Bartle's proof of Hake's Theorem?,Mistake in Bartle's proof of Hake's Theorem?,,"Here is Bartle's proof of Hake's Theorem found in ""A Modern Theory of Integration"". I think there is a mistake in the highlighted line: The Theorem: $f:[a,b]\to \mathbb{R}$ is gauge integrable if and only if it is gauge integrable in $[a,x]\ \forall x\in [a,b)$ and the limit $\lim_{x\to b}\int_a^xf$ is finite. In that case \begin{equation}\int_a^bf=\lim_{x\to b^-}\int_a^xf\end{equation} The problem is in the $(\impliedby)$ proof: Let $(c_n)$ be a strictly increasing sequence with $c_0=a$, $c_n\to b$. As $\lim_{n\to +\infty}\int_{c_0}^{c_n}f=\lim_{x\to b^-}\int_a^xf=L\in \mathbb{R}$ and $c_n\to b$, $\exists r\in \mathbb{N}$ so that \begin{equation}\forall x\in (c_r,b)\ \left|\int_a^xf-L\right|<\epsilon\text{ and }\left|f(b)\right|(b-c_r)<\epsilon\end{equation} By the integrability in $[c_{n-1},c_n]$, there exists a gauge $\delta_n$ on $[c_{n-1},c_n]$ so that if $\dot{\mathcal{P}}_n<<\delta_n$, \begin{equation}\left|S(f,\dot{\mathcal{P}}_n)-\int_{c_{n-1}}^{c_n}f\right|<\frac{\epsilon}{2^n}\end{equation} Without loss of generality, \begin{align*}\delta_1(c_0)&\le \frac12(c_1-c_0)\tag{i}\\ \delta_{n+1}(c_n)&\le \min\left\{\delta_n(c_n),\frac12(c_n-c_{n-1}),\frac12(c_{n+1}-c_n)\right\}\tag{ii}\\ \delta_n(x)&\le \min\left\{\frac12(x-c_{n-1}),\frac12(c_n-x)\right\}\text{ for }x\in (c_{n-1},c_n)\tag{iii} \end{align*} We define a gauge in $[a,b]$ by  \begin{equation}\delta(x)=\begin{cases}\delta_n(x)&\text{ if, }x\in [c_n,c_{n+1})\\ b-c_N&\text{ if, }x=b\end{cases}\end{equation} Let $\mathcal{P}=\left\{a=x_0<...<x_m=b\right\}$ partition $[a,b]$ with tags $t_i$ so that $\dot{\mathcal{P}}<<\delta$.  As $b\not\in\cup_{n=0}^{\infty}[c_n,c_{n+1})=[a,b)$, this forces $b$ to be a tag of $[x_{m-1},b]$. But then, as $t_m=b$, \begin{equation}c_r=b-\delta(b)<x_{m-1}\end{equation} Let $s\in \mathbb{N}$ be the smallest integer with $x_{m-1}\le c_s$ so that $r\le s$. If $k=1,...,s-1$ then condition (iii) implies that the point $c_k$ must be a tag of the sub-interval in $\mathcal{P}$ that contains it. ... My argument: Suppose $c_k\in [x_p,x_{p+1}]$. But, $[x_p,x_{p+1}]$ may also contain other points of the sequence (finitely many) . Let $c_{q},c_{q+1},...,c_k,c_{k+1},...,c_l$ be all these points. Suppose $t_p$ is the tag of $[x_p,x_{p+1}]$. If $t_p\neq c_q,c_{q+1},...,c_l$ and $t_p>c_k$ then $t_p\in (c_i,c_{i+1})$ with $c_i\ge c_k$. Condition (iii) and the fact that $\dot{\mathcal{P}}<<\delta$ imply that  $$c_k> t_p-\delta(t_p)=t_p-\delta_i(t_p)\ge t_p-\frac{t_p-c_i}2=\frac{t_p+c_i}2\ge \frac{c_k+c_k}2=c_k$$ which is a contradiction. Similarly if $t_p<c_k$. What I have shown is that the tag must be a point of the sequence. But why must it be $c_k$ like Bartle says? . This fact is crucial to the proof. Bartle then continues: Using the right-left procedure we may assume $c_0,c_1,...,c_{s-1}\in \mathcal{P}$. That can't be done (I think). Indeed consider the simple case where the only points of the sequence in  $[x_p,x_{p+1}]$ are $c_k$ and $c_{k+1}$. The tag $t_p$ must be one of $c_k,c_{k+1}$. Suppose it were $c_k$. Then $c_k\in \mathcal{P}$ and $c_{k+1}\in (c_k,x_{p+1})$. How is it then possible to prove $c_{k+1}$ is the tag of $[c_k,x_{p+1}]$?. $c_k$ seems like an equally strong candidate for this. EDIT: I also think there is a gap in ""As $b\not\in\cup_{n=0}^{\infty}[c_n,c_{n+1})=[a,b)$, this forces $b$ to be a tag of $[x_{m-1},b]$."" Why can't a point $c_n$ be the tag?","Here is Bartle's proof of Hake's Theorem found in ""A Modern Theory of Integration"". I think there is a mistake in the highlighted line: The Theorem: $f:[a,b]\to \mathbb{R}$ is gauge integrable if and only if it is gauge integrable in $[a,x]\ \forall x\in [a,b)$ and the limit $\lim_{x\to b}\int_a^xf$ is finite. In that case \begin{equation}\int_a^bf=\lim_{x\to b^-}\int_a^xf\end{equation} The problem is in the $(\impliedby)$ proof: Let $(c_n)$ be a strictly increasing sequence with $c_0=a$, $c_n\to b$. As $\lim_{n\to +\infty}\int_{c_0}^{c_n}f=\lim_{x\to b^-}\int_a^xf=L\in \mathbb{R}$ and $c_n\to b$, $\exists r\in \mathbb{N}$ so that \begin{equation}\forall x\in (c_r,b)\ \left|\int_a^xf-L\right|<\epsilon\text{ and }\left|f(b)\right|(b-c_r)<\epsilon\end{equation} By the integrability in $[c_{n-1},c_n]$, there exists a gauge $\delta_n$ on $[c_{n-1},c_n]$ so that if $\dot{\mathcal{P}}_n<<\delta_n$, \begin{equation}\left|S(f,\dot{\mathcal{P}}_n)-\int_{c_{n-1}}^{c_n}f\right|<\frac{\epsilon}{2^n}\end{equation} Without loss of generality, \begin{align*}\delta_1(c_0)&\le \frac12(c_1-c_0)\tag{i}\\ \delta_{n+1}(c_n)&\le \min\left\{\delta_n(c_n),\frac12(c_n-c_{n-1}),\frac12(c_{n+1}-c_n)\right\}\tag{ii}\\ \delta_n(x)&\le \min\left\{\frac12(x-c_{n-1}),\frac12(c_n-x)\right\}\text{ for }x\in (c_{n-1},c_n)\tag{iii} \end{align*} We define a gauge in $[a,b]$ by  \begin{equation}\delta(x)=\begin{cases}\delta_n(x)&\text{ if, }x\in [c_n,c_{n+1})\\ b-c_N&\text{ if, }x=b\end{cases}\end{equation} Let $\mathcal{P}=\left\{a=x_0<...<x_m=b\right\}$ partition $[a,b]$ with tags $t_i$ so that $\dot{\mathcal{P}}<<\delta$.  As $b\not\in\cup_{n=0}^{\infty}[c_n,c_{n+1})=[a,b)$, this forces $b$ to be a tag of $[x_{m-1},b]$. But then, as $t_m=b$, \begin{equation}c_r=b-\delta(b)<x_{m-1}\end{equation} Let $s\in \mathbb{N}$ be the smallest integer with $x_{m-1}\le c_s$ so that $r\le s$. If $k=1,...,s-1$ then condition (iii) implies that the point $c_k$ must be a tag of the sub-interval in $\mathcal{P}$ that contains it. ... My argument: Suppose $c_k\in [x_p,x_{p+1}]$. But, $[x_p,x_{p+1}]$ may also contain other points of the sequence (finitely many) . Let $c_{q},c_{q+1},...,c_k,c_{k+1},...,c_l$ be all these points. Suppose $t_p$ is the tag of $[x_p,x_{p+1}]$. If $t_p\neq c_q,c_{q+1},...,c_l$ and $t_p>c_k$ then $t_p\in (c_i,c_{i+1})$ with $c_i\ge c_k$. Condition (iii) and the fact that $\dot{\mathcal{P}}<<\delta$ imply that  $$c_k> t_p-\delta(t_p)=t_p-\delta_i(t_p)\ge t_p-\frac{t_p-c_i}2=\frac{t_p+c_i}2\ge \frac{c_k+c_k}2=c_k$$ which is a contradiction. Similarly if $t_p<c_k$. What I have shown is that the tag must be a point of the sequence. But why must it be $c_k$ like Bartle says? . This fact is crucial to the proof. Bartle then continues: Using the right-left procedure we may assume $c_0,c_1,...,c_{s-1}\in \mathcal{P}$. That can't be done (I think). Indeed consider the simple case where the only points of the sequence in  $[x_p,x_{p+1}]$ are $c_k$ and $c_{k+1}$. The tag $t_p$ must be one of $c_k,c_{k+1}$. Suppose it were $c_k$. Then $c_k\in \mathcal{P}$ and $c_{k+1}\in (c_k,x_{p+1})$. How is it then possible to prove $c_{k+1}$ is the tag of $[c_k,x_{p+1}]$?. $c_k$ seems like an equally strong candidate for this. EDIT: I also think there is a gap in ""As $b\not\in\cup_{n=0}^{\infty}[c_n,c_{n+1})=[a,b)$, this forces $b$ to be a tag of $[x_{m-1},b]$."" Why can't a point $c_n$ be the tag?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'gauge-integral']"
41,Limit power series at boundary [duplicate],Limit power series at boundary [duplicate],,This question already has an answer here : Closed 11 years ago . Possible Duplicate: Abel limit theorem Let the series $\sum_{k=0}^\infty a_k x^k$ have radius of convergence 1. Assume further that $\sum_{k=0}^\infty a_k = \infty$ . Is it the case that $\lim_{x\to 1^-} \sum_{k=0}^\infty a_k x^k = \infty$ ?,This question already has an answer here : Closed 11 years ago . Possible Duplicate: Abel limit theorem Let the series $\sum_{k=0}^\infty a_k x^k$ have radius of convergence 1. Assume further that $\sum_{k=0}^\infty a_k = \infty$ . Is it the case that $\lim_{x\to 1^-} \sum_{k=0}^\infty a_k x^k = \infty$ ?,,"['real-analysis', 'analysis']"
42,"$h\left(\frac{m}{2^n}\right)=0$ $\forall m\in\mathbb{Z},n\in\mathbb{N}$ implies $h(x)=0$ $\forall x\in\mathbb{R}$ if $h$ is continuous.",implies   if  is continuous.,"h\left(\frac{m}{2^n}\right)=0 \forall m\in\mathbb{Z},n\in\mathbb{N} h(x)=0 \forall x\in\mathbb{R} h","Let $h:\mathbb{R}\to\mathbb{R}$ be a function that is continuous on $\mathbb{R}$ and has the property that $$h\left(\frac{m}{2^n}\right)=0,\quad\forall m\in\mathbb{Z},n\in\mathbb{N}.$$ How can we show that this implies that $h(x)=0$, $\forall x\in\mathbb{R}$? The way I thought about this problem is to show that the set $$ S:=\left\{\frac{m}{2^n}:\forall m\in\mathbb{Z},n\in\mathbb{N}\right\}$$ is dense in $\mathbb{R}$. It then follows that for any $c\in\mathbb{R}$ there exists a sequence $(x_n)$ that converges to $c$ such that all the terms are of the form $p/2^q$. Thus, the sequence $(h(x_n))$ converges to $0$ and so by the sequential criterion for continuity we must have $h(c)=0$. Is there a simpler approach? Proving that $S$ is dense in $\mathbb{R}$ seems overly complicated for this problem. Or, can we deduce it from the density of $\mathbb{Q}$ in $\mathbb{R}$?","Let $h:\mathbb{R}\to\mathbb{R}$ be a function that is continuous on $\mathbb{R}$ and has the property that $$h\left(\frac{m}{2^n}\right)=0,\quad\forall m\in\mathbb{Z},n\in\mathbb{N}.$$ How can we show that this implies that $h(x)=0$, $\forall x\in\mathbb{R}$? The way I thought about this problem is to show that the set $$ S:=\left\{\frac{m}{2^n}:\forall m\in\mathbb{Z},n\in\mathbb{N}\right\}$$ is dense in $\mathbb{R}$. It then follows that for any $c\in\mathbb{R}$ there exists a sequence $(x_n)$ that converges to $c$ such that all the terms are of the form $p/2^q$. Thus, the sequence $(h(x_n))$ converges to $0$ and so by the sequential criterion for continuity we must have $h(c)=0$. Is there a simpler approach? Proving that $S$ is dense in $\mathbb{R}$ seems overly complicated for this problem. Or, can we deduce it from the density of $\mathbb{Q}$ in $\mathbb{R}$?",,"['real-analysis', 'continuity']"
43,Can cross partial derivatives exist everywhere but be equal nowhere?,Can cross partial derivatives exist everywhere but be equal nowhere?,,"In 1949 Tolstov proved that there exists $f:\mathbb{R^2} \to \mathbb{R}$ so that $f_x$ and $f_y$ exist and are continuous everywhere and $f_{xy}$ and $f_{yx}$ exist everywhere and differ on a set of positive Lebesgue measure.  I have also heard that he provided an example where the cross partials do not exist everywhere but differ on a set of full measure.  The only copies of the papers I have heard of are in Russian so I could not read them to find his examples.  In any case I would still like to play around with those questions a bit more on my own before looking up the answers. I am wondering if any further progress on this question is reasonably well known?  It seems like this question is natural enough that it should either be a known open research problem or solved, though I may be wrong on that.  My intuition says that if the cross partials can differ on a set of positive measure or even a.e. (but not necessarily exist everywhere) then it should probably be the case that they can exist and be different everywhere, though I would not be surprised to be proven completely wrong. More broadly, if anyone has a good way of thinking about functions with cross partial derivatives which exist everywhere and are different on some ""large"" set I would be interested in getting a better feel for what kind of pathological behavior I should be looking for.","In 1949 Tolstov proved that there exists $f:\mathbb{R^2} \to \mathbb{R}$ so that $f_x$ and $f_y$ exist and are continuous everywhere and $f_{xy}$ and $f_{yx}$ exist everywhere and differ on a set of positive Lebesgue measure.  I have also heard that he provided an example where the cross partials do not exist everywhere but differ on a set of full measure.  The only copies of the papers I have heard of are in Russian so I could not read them to find his examples.  In any case I would still like to play around with those questions a bit more on my own before looking up the answers. I am wondering if any further progress on this question is reasonably well known?  It seems like this question is natural enough that it should either be a known open research problem or solved, though I may be wrong on that.  My intuition says that if the cross partials can differ on a set of positive measure or even a.e. (but not necessarily exist everywhere) then it should probably be the case that they can exist and be different everywhere, though I would not be surprised to be proven completely wrong. More broadly, if anyone has a good way of thinking about functions with cross partial derivatives which exist everywhere and are different on some ""large"" set I would be interested in getting a better feel for what kind of pathological behavior I should be looking for.",,"['real-analysis', 'multivariable-calculus']"
44,How to prove that a function is class $C^{\infty}(\mathbb{R}).$,How to prove that a function is class,C^{\infty}(\mathbb{R}).,"I would like to prove that this function (with the $a, b\in \mathbb{R}$ satisfying  $a < b$), $$f(x)=\begin{cases} \exp\left(-\frac{1}{x-a}+\frac{1}{x-b}\right), & \text{if },x\in (a,b)\,, \\ 0, & \text{if }x\notin (a,b)\text{} \end{cases}$$ is class $C^\infty (\mathbb{R}).$ But how?","I would like to prove that this function (with the $a, b\in \mathbb{R}$ satisfying  $a < b$), $$f(x)=\begin{cases} \exp\left(-\frac{1}{x-a}+\frac{1}{x-b}\right), & \text{if },x\in (a,b)\,, \\ 0, & \text{if }x\notin (a,b)\text{} \end{cases}$$ is class $C^\infty (\mathbb{R}).$ But how?",,"['calculus', 'real-analysis']"
45,Uniform continuity and boundedness,Uniform continuity and boundedness,,"I have come across a proof which I understand almost completely, except for one part: THEOREM:  If $f$ is uniformly continuous on a bounded interval $I$, then $f$ is also bounded on $I$. PROOF: In this case we assume that $I$ is of the form $(a,b), (a,b], [a,b)$, or $[a,b]$, with $a,b \in \mathbb{R}$. Fix an $\epsilon > 0$, for instance $\epsilon = 1$.  Since $f$ is uniformly continuous, there is a $\delta > 0$ such that: $|f(x_1) - f(x_2)| < \epsilon = 1$ when $x_1, x_2 \in I$ and $|x_1 - x_2| < \delta$ Divide $I$ into $N$ intervals, $I_1, . . ., I_N$, where $N$ is chosen so that $\frac{b-a}{N} < \delta$. Let $z_i$ be the center point of $I_i$.  For each $i$ and $x \in I_i$, $|x - z_i| < \delta$, and then we have: $|f(x)| = |f(x) - f(z_i) + f(z_i)| \leq |f(x) - f(z_i)| + |f(z_i)| \leq 1 + |f(z_i)|$.  Then for $x \in I_i$, $|f(x)| \leq 1 + \max_{1 \leq i \leq N}\{|f(z_i)|\}$. Let $M = \max_{1 \leq i \leq N}\{|f(z_i)|\}$.  Then $|f(x)| \leq 1 + M$ QED OK, so the one thing I am a bit unsure of here, is when we write: Let $M = \max_{1 \leq i \leq N}\{|f(z_i)|\}$. How is it that we know for sure that each $|f(z_i)|$ is also bounded?  I see how the presence of a maximum value completes the proof, but why is it not possible that we have an $|f(z_i)|$ which is unbounded? If anyone could explain this to me I would greatly appreciate it!","I have come across a proof which I understand almost completely, except for one part: THEOREM:  If $f$ is uniformly continuous on a bounded interval $I$, then $f$ is also bounded on $I$. PROOF: In this case we assume that $I$ is of the form $(a,b), (a,b], [a,b)$, or $[a,b]$, with $a,b \in \mathbb{R}$. Fix an $\epsilon > 0$, for instance $\epsilon = 1$.  Since $f$ is uniformly continuous, there is a $\delta > 0$ such that: $|f(x_1) - f(x_2)| < \epsilon = 1$ when $x_1, x_2 \in I$ and $|x_1 - x_2| < \delta$ Divide $I$ into $N$ intervals, $I_1, . . ., I_N$, where $N$ is chosen so that $\frac{b-a}{N} < \delta$. Let $z_i$ be the center point of $I_i$.  For each $i$ and $x \in I_i$, $|x - z_i| < \delta$, and then we have: $|f(x)| = |f(x) - f(z_i) + f(z_i)| \leq |f(x) - f(z_i)| + |f(z_i)| \leq 1 + |f(z_i)|$.  Then for $x \in I_i$, $|f(x)| \leq 1 + \max_{1 \leq i \leq N}\{|f(z_i)|\}$. Let $M = \max_{1 \leq i \leq N}\{|f(z_i)|\}$.  Then $|f(x)| \leq 1 + M$ QED OK, so the one thing I am a bit unsure of here, is when we write: Let $M = \max_{1 \leq i \leq N}\{|f(z_i)|\}$. How is it that we know for sure that each $|f(z_i)|$ is also bounded?  I see how the presence of a maximum value completes the proof, but why is it not possible that we have an $|f(z_i)|$ which is unbounded? If anyone could explain this to me I would greatly appreciate it!",,['real-analysis']
46,"Is the space of uniformly left continuous functions on [0,1] complete?","Is the space of uniformly left continuous functions on [0,1] complete?",,"We'll say that a function on $[0,1]$ is uniformly left continuous if for every $\epsilon > 0$ there exists $\delta > 0$ such that $x \in (y - \delta, y)$ implies $|f(x) - f(y)| < \epsilon$ for every $x, y \in [0,1]$.  I want to know if the space of all such functions is complete with respect to the uniform norm. I'm interested in this space because I suspect that it is the completion of the space of piecewise constant functions which are continuous from the left. Thanks in advance for the help! EDIT: As was pointed out in the comments, what I wrote above is equivalent to uniform continuity.  I think the following revised definition captures what I want.  $f$ is uniformly left continuous if for every $\epsilon$ there is a partition $0 = t_0 < t_1 < \ldots < t_n = 1$ such that $|f(x) - f(y)| < \epsilon$ whenever $t_0 \leq x \leq y \leq t_1$ or $t_i < x \leq y \leq t_{i+1}$ for $i > 0$.  So with this definition the characteristic function of $(1/2,1]$ is uniformly left continuous while the characteristic function of $[1/2,1]$ is not. Here are the examples that I'm really trying to kill.  Take $f$ to be the function which is 0 on the interval $[0,1/2]$ and let $f(x) = 1/(x-1/2)$ on $(1/2,1]$.  This is left continuous, but it shouldn't be uniformly left continuous.  Even if you insist that $f$ be bounded, you could set $f(x) = \sin(1/(x-1/2))$ for $x$ in $(1/2,1]$ and get a left continuous function which is not uniformly left continuous.  If you can think of a better definition that captures this intuition, let me know.","We'll say that a function on $[0,1]$ is uniformly left continuous if for every $\epsilon > 0$ there exists $\delta > 0$ such that $x \in (y - \delta, y)$ implies $|f(x) - f(y)| < \epsilon$ for every $x, y \in [0,1]$.  I want to know if the space of all such functions is complete with respect to the uniform norm. I'm interested in this space because I suspect that it is the completion of the space of piecewise constant functions which are continuous from the left. Thanks in advance for the help! EDIT: As was pointed out in the comments, what I wrote above is equivalent to uniform continuity.  I think the following revised definition captures what I want.  $f$ is uniformly left continuous if for every $\epsilon$ there is a partition $0 = t_0 < t_1 < \ldots < t_n = 1$ such that $|f(x) - f(y)| < \epsilon$ whenever $t_0 \leq x \leq y \leq t_1$ or $t_i < x \leq y \leq t_{i+1}$ for $i > 0$.  So with this definition the characteristic function of $(1/2,1]$ is uniformly left continuous while the characteristic function of $[1/2,1]$ is not. Here are the examples that I'm really trying to kill.  Take $f$ to be the function which is 0 on the interval $[0,1/2]$ and let $f(x) = 1/(x-1/2)$ on $(1/2,1]$.  This is left continuous, but it shouldn't be uniformly left continuous.  Even if you insist that $f$ be bounded, you could set $f(x) = \sin(1/(x-1/2))$ for $x$ in $(1/2,1]$ and get a left continuous function which is not uniformly left continuous.  If you can think of a better definition that captures this intuition, let me know.",,"['calculus', 'real-analysis', 'analysis', 'banach-spaces']"
47,Interchange of integration and summation and the Taylor expansion,Interchange of integration and summation and the Taylor expansion,,"Suppose we have a bounded function $f:\mathbb{R} \to \mathbb{C}$. I want to compute $$ \int_\mathbb{R} e^{-x^2} f(x) dx $$ Of course this integral exists. I know that $f$ has a Taylor expansion which is valid for all of $\mathbb{R}$, say $$ f(x)=\sum_{r=0}^\infty a_r \frac{x^r}{r!} $$ Is it generally true that $$ \int_\mathbb{R} e^{-x^2} f(x) dx = \sum_{r=0}^\infty \frac{a_r}{r!} \int_\mathbb{R} e^{-x^2}  x^r dx$$ Of course all the integrals on the right hand side also exist. However, I cannot use the theorem of uniform convergence, since the Taylor expansion does not converge uniformly on $\mathbb{R}$ but of course on any compact subset. So my guess is that a this equality should hold anyways, in particular since the sum is not some wierdly constructed counterexample but a regular Taylor series. How could I prove such a result. In fact I do not necessarily need to know how to prove it but just know it. Is there a reference?","Suppose we have a bounded function $f:\mathbb{R} \to \mathbb{C}$. I want to compute $$ \int_\mathbb{R} e^{-x^2} f(x) dx $$ Of course this integral exists. I know that $f$ has a Taylor expansion which is valid for all of $\mathbb{R}$, say $$ f(x)=\sum_{r=0}^\infty a_r \frac{x^r}{r!} $$ Is it generally true that $$ \int_\mathbb{R} e^{-x^2} f(x) dx = \sum_{r=0}^\infty \frac{a_r}{r!} \int_\mathbb{R} e^{-x^2}  x^r dx$$ Of course all the integrals on the right hand side also exist. However, I cannot use the theorem of uniform convergence, since the Taylor expansion does not converge uniformly on $\mathbb{R}$ but of course on any compact subset. So my guess is that a this equality should hold anyways, in particular since the sum is not some wierdly constructed counterexample but a regular Taylor series. How could I prove such a result. In fact I do not necessarily need to know how to prove it but just know it. Is there a reference?",,"['real-analysis', 'improper-integrals']"
48,I want to understand uniform integrability in terms of Lebesgue integration,I want to understand uniform integrability in terms of Lebesgue integration,,"According to my Real Analysis textbook, a family $\scr{F}$ of measurable functions on $E$ is said to be uniformly integrable over $E$ provided for each $\epsilon$ $>$ $0$, there is a $\delta$ $>$ $0$ such that for each $f$ $\in$ $\scr{F}$, if $A$ $\subseteq$ $E$ is measurable and $m(A)$ $<$ $\delta$, then $\int_{A}$$|f|$ $<$ $\epsilon$. That is fine, but my textbook doesn't really give any good examples or counterexamples. I am studying for a final exam in Real Analysis, and would like some input in regards to any examples or counterexamples for uniform integrability of Lebesgue integrable functions. Thanks!","According to my Real Analysis textbook, a family $\scr{F}$ of measurable functions on $E$ is said to be uniformly integrable over $E$ provided for each $\epsilon$ $>$ $0$, there is a $\delta$ $>$ $0$ such that for each $f$ $\in$ $\scr{F}$, if $A$ $\subseteq$ $E$ is measurable and $m(A)$ $<$ $\delta$, then $\int_{A}$$|f|$ $<$ $\epsilon$. That is fine, but my textbook doesn't really give any good examples or counterexamples. I am studying for a final exam in Real Analysis, and would like some input in regards to any examples or counterexamples for uniform integrability of Lebesgue integrable functions. Thanks!",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'uniform-integrability']"
49,Sum of distance between integer numbers,Sum of distance between integer numbers,,"Let $p$ is a positive integer, and let $A_1,\ldots,A_p$ be a partition of $\{1,\ldots,n\}$ . My conjecture is that \begin{equation} \displaystyle\sum_{i=1}^p\left( \frac{\displaystyle\sum_{a_i,a_j \in A_p}|a_i-a_j|}{|A_p|} \right)\leq \frac{\displaystyle\sum_{1\leq i,j \leq n}|i-j|}{n}=\frac{(n-1)(n+1)}{3} \tag{1} \end{equation} I have checked that this is true for $n=4,5,6$ . I have tried to prove this conjecture by induction, but I did not succeed yet. I think also about the weighted network. The sum $\displaystyle\sum_{a_i,a_j \in A_p}|a_i-a_j|$ is the sum of the inverse of closeness of all points in network represented by $A_p$ . But once again, I did not find any good references. Another way I have thought about the proof is as follows. Let $A,B$ be two disjoint sets of positive integers. The conjecture above is true if we can prove that \begin{equation} \frac{\displaystyle\sum_{a_i,a_j \in A}|a_i-a_j|}{|A|}+\frac{\displaystyle\sum_{b_i,b_j \in B}|b_i-b_j|}{|B|}\leq \frac{\displaystyle\sum_{x_i,x_j \in A\cup B}|x_i-x_j|}{|A|+|B|} \tag{2} \end{equation} I am trying to prove inequality (2) by proving that if we move one element from set $B$ to set $A$ , then it will increase the sum in LHS of (2) provided that $|A|\geq |B|$ . I am still working on it. If you have any ideas, comments, or even counter examples, please do not hesitate to write them here. Thank you very much. Update: The inequality in (2) seems wrong. I have done a simulation for the case $n=10$ . The values of those partitions are not monotone; however, they're all smaller than $\frac{(n-1)(n+1)}{3}$ .","Let is a positive integer, and let be a partition of . My conjecture is that I have checked that this is true for . I have tried to prove this conjecture by induction, but I did not succeed yet. I think also about the weighted network. The sum is the sum of the inverse of closeness of all points in network represented by . But once again, I did not find any good references. Another way I have thought about the proof is as follows. Let be two disjoint sets of positive integers. The conjecture above is true if we can prove that I am trying to prove inequality (2) by proving that if we move one element from set to set , then it will increase the sum in LHS of (2) provided that . I am still working on it. If you have any ideas, comments, or even counter examples, please do not hesitate to write them here. Thank you very much. Update: The inequality in (2) seems wrong. I have done a simulation for the case . The values of those partitions are not monotone; however, they're all smaller than .","p A_1,\ldots,A_p \{1,\ldots,n\} \begin{equation}
\displaystyle\sum_{i=1}^p\left( \frac{\displaystyle\sum_{a_i,a_j \in A_p}|a_i-a_j|}{|A_p|} \right)\leq \frac{\displaystyle\sum_{1\leq i,j \leq n}|i-j|}{n}=\frac{(n-1)(n+1)}{3}
\tag{1}
\end{equation} n=4,5,6 \displaystyle\sum_{a_i,a_j \in A_p}|a_i-a_j| A_p A,B \begin{equation}
\frac{\displaystyle\sum_{a_i,a_j \in A}|a_i-a_j|}{|A|}+\frac{\displaystyle\sum_{b_i,b_j \in B}|b_i-b_j|}{|B|}\leq \frac{\displaystyle\sum_{x_i,x_j \in A\cup B}|x_i-x_j|}{|A|+|B|}
\tag{2}
\end{equation} B A |A|\geq |B| n=10 \frac{(n-1)(n+1)}{3}","['real-analysis', 'discrete-mathematics', 'inequality', 'network']"
50,Uniform convergence of a series $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{\sqrt{n}}\sin(\frac{x}{n}).$,Uniform convergence of a series,\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{\sqrt{n}}\sin(\frac{x}{n}).,"I know this series is uniformly convergent on any compact subset of $\mathbb{R}$ by the famous Weierstrass test. However, it is NOT uniformly convergent on $(-\infty, +\infty)$ , could someone give me some ideas to prove this claim?","I know this series is uniformly convergent on any compact subset of by the famous Weierstrass test. However, it is NOT uniformly convergent on , could someone give me some ideas to prove this claim?","\mathbb{R} (-\infty, +\infty)","['real-analysis', 'uniform-convergence']"
51,$u_n$ converges if and only if $\frac1{\sum_{k=0}^n a_k} \sum_{k=0}^n a_ku_k $ converges.,converges if and only if  converges.,u_n \frac1{\sum_{k=0}^n a_k} \sum_{k=0}^n a_ku_k ,"Let $(a_n)_{n \in \mathbb{N}} $ be a positive sequence with $a_0\neq0$ . Find a necessary and sufficient condition on $(a_n) $ in order that: for any real sequence $(u_n)_{n \in \mathbb{N}}$ , $$\boxed{(u_n)_{n \in \mathbb{N}}\quad \text{converges}\quad \iff\quad \left(\frac1{\sum_{k=0}^n a_k} \sum_{k=0}^n a_ku_k  \right)_{n \in \mathbb{N}}\quad  \text{converges }}$$ Add 1: Thank you very much @SeverinSchraven , I see now that the condition sought is $\liminf_{n\rightarrow \infty} \frac{a_n}{ \sum_{k=0}^n a_k} >0.$","Let be a positive sequence with . Find a necessary and sufficient condition on in order that: for any real sequence , Add 1: Thank you very much @SeverinSchraven , I see now that the condition sought is",(a_n)_{n \in \mathbb{N}}  a_0\neq0 (a_n)  (u_n)_{n \in \mathbb{N}} \boxed{(u_n)_{n \in \mathbb{N}}\quad \text{converges}\quad \iff\quad \left(\frac1{\sum_{k=0}^n a_k} \sum_{k=0}^n a_ku_k  \right)_{n \in \mathbb{N}}\quad  \text{converges }} \liminf_{n\rightarrow \infty} \frac{a_n}{ \sum_{k=0}^n a_k} >0.,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'cesaro-summable']"
52,"$x_n=f\left(\frac{1}{n^2}\right)+f\left(\frac{2}{n^2}\right)+\cdots+f\left(\frac{n}{n^2}\right)$, $\lim_{n\to\infty}x_n=\dfrac{f'(0)}{2}$",",",x_n=f\left(\frac{1}{n^2}\right)+f\left(\frac{2}{n^2}\right)+\cdots+f\left(\frac{n}{n^2}\right) \lim_{n\to\infty}x_n=\dfrac{f'(0)}{2},"Let $f(x)$ be a function and $f(0)=0$ , $f'(0)$ exists. Let $$x_n=f\left(\frac{1}{n^2}\right)+f\left(\frac{2}{n^2}\right)+\cdots+f\left(\frac{n}{n^2}\right).$$ Prove that $\lim_{n\to\infty}x_n=\dfrac{f'(0)}{2}$ . The solution is given by: Since $f'(0)=\lim_{x\to0}\frac{f(x)-f(0)}{x-0}$ , we have that $$f(x)=f(0)+f'(0)x+o(x)=f'(0)x+o(x).$$ When $n\to+\infty$ , we have that $$f\left(\frac{1}{n^2}\right)=\frac{f'(0)}{n^2}+o\left(\frac{1}{n^2}\right)= \frac{f'(0)}{n^2}+o\left(\frac{1}{n}\right),\quad f\left(\frac{2}{n^2}\right)=\frac{2f'(0)}{n^2}+o\left(\frac{2}{n^2}\right)=\frac{2f'(0)}{n^2}+o\left(\frac{1}{n}\right)$$ $$f\left(\frac{k}{n^2}\right)=\frac{kf'(0)}{n^2}+o\left(\frac{k}{n^2}\right)= \frac{kf'(0)}{n^2}+o\left(\frac{1}{n}\right),\dots$$ Hence $$x_n=f'(0)\cdot\frac{1+2+\cdots+n}{n^2}+n\cdot o\left(\frac{1}{n}\right)=f'(0)\cdot\frac{n+1}{2n}+o(1),$$ then $$\lim_{n\to\infty}x_n=\frac{f'(0)}{2}.$$ I have a question: in the above steps, $o\left(\frac{1}{n}\right)$ are different, can we do this $n\cdot o\left(\frac{1}{n}\right)=o(1)$ . A more accurate question is: suppose there is a two-dimensional sequence $\{x_{nm}\}_{n,m\in\mathbb{N}}$ and for a fixed $m$ , there will always be $\lim_{n\to\infty}x_{nm}=0$ , is it right that $$\lim_{n\to\infty}\frac{x_{n1}+x_{n2}+\cdots+x_{nn}}{n}=0 ?$$ The original intention of this question is that we used the conclusion $n\cdot o(1/n)=o(1)$ in the answers to the above questions. Since these $o(1/n)$ are different, set to $$\alpha_{n1},\alpha_{n2},\dots,\alpha_{nn},\quad \alpha_{nk}=o(1/n),\quad 1\leq k\leq n.$$ which is $$\lim_{n\to\infty}\dfrac{\alpha_{nk}}{\dfrac{1}{n}}=\lim_{n\to\infty}n\alpha_{nk}=0,\quad 1\leq k\leq n.$$ Is the limit of the sum of $\alpha_{n1},\alpha_{n2},\dots,\alpha_{nn}$ is also $0$ ? That is whether there is $$\lim_{n\to\infty}(\alpha_{n1}+\alpha_{n2}+\cdots+\alpha_{nn})= \lim_{n\to\infty}\frac{n\alpha_{n1}+n\alpha_{n2}+\cdots+n\alpha_{nn}}{n}\stackrel{?}=0.$$ This is the solution: This is the $\epsilon$ - $\delta$ solution given by a teacher, here we assume $f'(0)=1$","Let be a function and , exists. Let Prove that . The solution is given by: Since , we have that When , we have that Hence then I have a question: in the above steps, are different, can we do this . A more accurate question is: suppose there is a two-dimensional sequence and for a fixed , there will always be , is it right that The original intention of this question is that we used the conclusion in the answers to the above questions. Since these are different, set to which is Is the limit of the sum of is also ? That is whether there is This is the solution: This is the - solution given by a teacher, here we assume","f(x) f(0)=0 f'(0) x_n=f\left(\frac{1}{n^2}\right)+f\left(\frac{2}{n^2}\right)+\cdots+f\left(\frac{n}{n^2}\right). \lim_{n\to\infty}x_n=\dfrac{f'(0)}{2} f'(0)=\lim_{x\to0}\frac{f(x)-f(0)}{x-0} f(x)=f(0)+f'(0)x+o(x)=f'(0)x+o(x). n\to+\infty f\left(\frac{1}{n^2}\right)=\frac{f'(0)}{n^2}+o\left(\frac{1}{n^2}\right)=
\frac{f'(0)}{n^2}+o\left(\frac{1}{n}\right),\quad
f\left(\frac{2}{n^2}\right)=\frac{2f'(0)}{n^2}+o\left(\frac{2}{n^2}\right)=\frac{2f'(0)}{n^2}+o\left(\frac{1}{n}\right) f\left(\frac{k}{n^2}\right)=\frac{kf'(0)}{n^2}+o\left(\frac{k}{n^2}\right)=
\frac{kf'(0)}{n^2}+o\left(\frac{1}{n}\right),\dots x_n=f'(0)\cdot\frac{1+2+\cdots+n}{n^2}+n\cdot o\left(\frac{1}{n}\right)=f'(0)\cdot\frac{n+1}{2n}+o(1), \lim_{n\to\infty}x_n=\frac{f'(0)}{2}. o\left(\frac{1}{n}\right) n\cdot o\left(\frac{1}{n}\right)=o(1) \{x_{nm}\}_{n,m\in\mathbb{N}} m \lim_{n\to\infty}x_{nm}=0 \lim_{n\to\infty}\frac{x_{n1}+x_{n2}+\cdots+x_{nn}}{n}=0 ? n\cdot o(1/n)=o(1) o(1/n) \alpha_{n1},\alpha_{n2},\dots,\alpha_{nn},\quad \alpha_{nk}=o(1/n),\quad 1\leq k\leq n. \lim_{n\to\infty}\dfrac{\alpha_{nk}}{\dfrac{1}{n}}=\lim_{n\to\infty}n\alpha_{nk}=0,\quad 1\leq k\leq n. \alpha_{n1},\alpha_{n2},\dots,\alpha_{nn} 0 \lim_{n\to\infty}(\alpha_{n1}+\alpha_{n2}+\cdots+\alpha_{nn})=
\lim_{n\to\infty}\frac{n\alpha_{n1}+n\alpha_{n2}+\cdots+n\alpha_{nn}}{n}\stackrel{?}=0. \epsilon \delta f'(0)=1","['real-analysis', 'calculus']"
53,"If $\sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}x^{2s}(1-x^2)^{m}=0$ for all $x\in [-1,1]$ what can we say about coefficients $b_{s,m}$?",If  for all  what can we say about coefficients ?,"\sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}x^{2s}(1-x^2)^{m}=0 x\in [-1,1] b_{s,m}","Consider the following power series: \begin{align} f(x)=\sum_{s=0}^\infty \sum_{m=0}^{\infty} b_{s,m}x^{2s}(1-x^2)^{m}. \end{align} Suppose that  we know that $f(x)=0$ for all $x\in [-1,1]$ . My question is, under the above assumption, what can we conclude about the coefficients $b_{s,m}$ ? Some Thoughts $f$ can be re-written in terms of a single sum as \begin{align} \sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}x^{2s}(1-x^2)^{m}= \sum_{k=0}^\infty a_k x^{2k}. \end{align} The expression for $a_k$ in terms $b_{s,m}$ was found here and is given by \begin{align} a_k =\sum_{s=0}^k \sum_{m \geq (k-s)} b_{s,m} {m \choose (k-s)} (-1)^{k-s} . \end{align} Since $f(x)=0,x \in [-1,1]$ , this implies that $a_k=0$ for all $k$ . However, since the mapping between $b_{s,m}$ and $a_k$ is not simple I am not sure what we can conclude about the coefficients $b_{s,m}$ and what $a_k$ 's equal zero imply. Edit 1: As suggest in the comment section, it might be usefull to transform the problem by setting $x=\sin(t)$ in which case we get \begin{align} 0= \sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}\sin^{2s}(t)\cos^{2m}(t), \end{align} for all $t\in [-\pi,\pi]$ . Edit 2: If assumptions are needed, then we can assume that $\sum b_{s,m}^2 <\infty$ .","Consider the following power series: Suppose that  we know that for all . My question is, under the above assumption, what can we conclude about the coefficients ? Some Thoughts can be re-written in terms of a single sum as The expression for in terms was found here and is given by Since , this implies that for all . However, since the mapping between and is not simple I am not sure what we can conclude about the coefficients and what 's equal zero imply. Edit 1: As suggest in the comment section, it might be usefull to transform the problem by setting in which case we get for all . Edit 2: If assumptions are needed, then we can assume that .","\begin{align}
f(x)=\sum_{s=0}^\infty \sum_{m=0}^{\infty} b_{s,m}x^{2s}(1-x^2)^{m}.
\end{align} f(x)=0 x\in [-1,1] b_{s,m} f \begin{align}
\sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}x^{2s}(1-x^2)^{m}= \sum_{k=0}^\infty a_k x^{2k}.
\end{align} a_k b_{s,m} \begin{align}
a_k =\sum_{s=0}^k \sum_{m \geq (k-s)} b_{s,m} {m \choose (k-s)} (-1)^{k-s} .
\end{align} f(x)=0,x \in [-1,1] a_k=0 k b_{s,m} a_k b_{s,m} a_k x=\sin(t) \begin{align}
0= \sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}\sin^{2s}(t)\cos^{2m}(t),
\end{align} t\in [-\pi,\pi] \sum b_{s,m}^2 <\infty","['real-analysis', 'power-series', 'taylor-expansion']"
54,"$\{f_n\}$ be a sequence of continuous functions converging pointwise to $0$, show $\lim \int_0^1 f_n dx = 0$","be a sequence of continuous functions converging pointwise to , show",\{f_n\} 0 \lim \int_0^1 f_n dx = 0,"This problem comes from Rudin's Real and Complex Analysis and is Exercise 2.10. Let $\{f_n\}$ be a sequence of continuous functions such that $0\leq f_n\leq 1$ and $f_n(x)\to0$ for all $x\in [0,1]$ . We need to show that $$\lim_{n\to\infty}\int_0^1 f_n(x)dx = 0$$ In particular I am trying to show this without using Measure Theory or Lebesgue Integration. What I have tried so far is to consider the sequence of functions $\{g_n\}$ defined by $$g_n(x)= \min\{f_1(x),\ldots f_n(x)\}$$ Then $\{g_n\}$ is a continuous, decreasing sequence of functions which converge pointwise to $0$ . Since the sequence of continuous functions decreases to a continuous function, then we may apply Dini's Theorem to know that $g_n \to 0$ uniformly. Hence $$ \lim_{n\to 0}\int_0^1 g_n(x)dx = \int_0^1\lim_{n\to0}g_n(x)dx = 0$$ The only thing left to show is that $$ \lim_{n\to 0}\int_0^1 f_n(x)dx= \lim_{n\to 0}\int_0^1 g_n(x)dx $$ But this is where I am stuck. Again I am avoiding measure theory proofs.","This problem comes from Rudin's Real and Complex Analysis and is Exercise 2.10. Let be a sequence of continuous functions such that and for all . We need to show that In particular I am trying to show this without using Measure Theory or Lebesgue Integration. What I have tried so far is to consider the sequence of functions defined by Then is a continuous, decreasing sequence of functions which converge pointwise to . Since the sequence of continuous functions decreases to a continuous function, then we may apply Dini's Theorem to know that uniformly. Hence The only thing left to show is that But this is where I am stuck. Again I am avoiding measure theory proofs.","\{f_n\} 0\leq f_n\leq 1 f_n(x)\to0 x\in [0,1] \lim_{n\to\infty}\int_0^1 f_n(x)dx = 0 \{g_n\} g_n(x)= \min\{f_1(x),\ldots f_n(x)\} \{g_n\} 0 g_n \to 0  \lim_{n\to 0}\int_0^1 g_n(x)dx = \int_0^1\lim_{n\to0}g_n(x)dx = 0  \lim_{n\to 0}\int_0^1 f_n(x)dx= \lim_{n\to 0}\int_0^1 g_n(x)dx ","['real-analysis', 'uniform-convergence']"
55,A ball around of a generic point has a positive ergodic measure,A ball around of a generic point has a positive ergodic measure,,"Let $T:X \to X$ be a continuous function on the compact metric space $X$ . We say that $\mu$ is $T-$ invarint if $\mu(T^{-1}(A))=\mu(A) \hspace{0.2cm} \forall A \in \beta.$ We denote by $\mathcal{M}(X,T)$ the set of $T-$ invariant Borel probability measures. For $\mu \in \mathcal{M}(X, T),$ the set $G_{\mu}$ of $\mu-$ generic points is defined by $$ G_{\mu}:=\left\{x \in X: \frac{1}{n} \sum_{j=0}^{n-1} \delta_{T^{j} x} \rightarrow \mu \text { in the weak }^{*} \text { topology as } n \rightarrow \infty\right\} $$ where $\delta_{y}$ denotes the probability measure whose support is the single point $y$ . Let $\mu$ be an ergodic measure. By Birkhoff ergodic theorem, $\mu(G_{\mu})=1$ . $\textit{Question:}$ Let $\mu$ be an ergodic measure and $x$ be a $\mu-$ generic point.  Why is $\mu(B(x, \frac{1}{h}))>0$ for every $h \in \mathbb{N}$ , where $B(x, \frac{1}{h})$ is the ball of radius $\frac{1}{h}$ centered at $x.$ ? I think one should consider $\chi_{B(x, 1/h)}$ in Birkhoff ergodic theorem, but I don't know how to get the sum is positive.","Let be a continuous function on the compact metric space . We say that is invarint if We denote by the set of invariant Borel probability measures. For the set of generic points is defined by where denotes the probability measure whose support is the single point . Let be an ergodic measure. By Birkhoff ergodic theorem, . Let be an ergodic measure and be a generic point.  Why is for every , where is the ball of radius centered at ? I think one should consider in Birkhoff ergodic theorem, but I don't know how to get the sum is positive.","T:X \to X X \mu T- \mu(T^{-1}(A))=\mu(A) \hspace{0.2cm} \forall A \in \beta. \mathcal{M}(X,T) T- \mu \in \mathcal{M}(X, T), G_{\mu} \mu- 
G_{\mu}:=\left\{x \in X: \frac{1}{n} \sum_{j=0}^{n-1} \delta_{T^{j} x} \rightarrow \mu \text { in the weak }^{*} \text { topology as } n \rightarrow \infty\right\}
 \delta_{y} y \mu \mu(G_{\mu})=1 \textit{Question:} \mu x \mu- \mu(B(x, \frac{1}{h}))>0 h \in \mathbb{N} B(x, \frac{1}{h}) \frac{1}{h} x. \chi_{B(x, 1/h)}","['real-analysis', 'probability-theory', 'measure-theory', 'dynamical-systems', 'ergodic-theory']"
56,Determining if a function is differentiable,Determining if a function is differentiable,,"$$f(x)= \sum^{\infty}_{n=1}\frac{1}{(n+x)^2}$$ for $x \in [0, \infty)$ This above is a function which (with help) I have proved to be continuous on $[0, \infty)$ I now want to prove that $f$ is differentiable however I am struggling to see how. I know that since $f$ is continuous, if is integrable which also applies to $f'$ ? Furthermore the solution to this question posted ( Uniform convergence of $f'$ on an interval implies locally uniform convergence of $f$ ) is clearly a related answer as I know that proving differentiability involves the uniform convergence of a sequence of functions. Can anyone explain to me how the example I linked might be applied to my particular problem? Many thanks!","for This above is a function which (with help) I have proved to be continuous on I now want to prove that is differentiable however I am struggling to see how. I know that since is continuous, if is integrable which also applies to ? Furthermore the solution to this question posted ( Uniform convergence of $f'$ on an interval implies locally uniform convergence of $f$ ) is clearly a related answer as I know that proving differentiability involves the uniform convergence of a sequence of functions. Can anyone explain to me how the example I linked might be applied to my particular problem? Many thanks!","f(x)= \sum^{\infty}_{n=1}\frac{1}{(n+x)^2} x \in [0, \infty) [0, \infty) f f f'","['real-analysis', 'analysis', 'continuity', 'uniform-convergence']"
57,"Prove that $\int_a^\infty f(x)\sin(e^x) \, dx$ conditionally converges.",Prove that  conditionally converges.,"\int_a^\infty f(x)\sin(e^x) \, dx","Let $f$ be a bounded and with a continuous derivative at the interval $[a,\infty)$ . The integral: $\displaystyle \int_a^\infty f(x) \, dx$ diverges. Also: $$ \exists t> a, \forall x>t: f'(x) < f(x) $$ Prove that the $\displaystyle \int_a^\infty f(x) \sin(e^x) \, dx$ conditionally converges. What i tried: So i want to show that it diverges in its absolute value and converges in its ""normal"" value. For diverges in its absolute value We know that: $$ |\sin(e^x)| < 1 $$ Therefore we can write: $$ \int_a^\infty |f(x)| \leq \int_a^\infty |f(x)\sin(e^x)| \, dx $$ But we know that the left integral diverges from the question, therefore, by comparison test: $$ \int_a^\infty |f(x)\sin(e^x)| \, dx $$ diverges. Now the problem in proving converges for the ""normal"" function. I thought to use Dirichlet test, but i dont see how to say that $f(x)$ is decreasing monotonic or to talk about the limit. I must say it very sounds like drichlet test, but i cant see how it feets... Couldnt think of other functions for dirichle So i thought about the comparison test, yet couldn't think of a converging function that will fit. In the end, i am stuck. Those are my homework, so i prefer a hint than a solution. Thank you.","Let be a bounded and with a continuous derivative at the interval . The integral: diverges. Also: Prove that the conditionally converges. What i tried: So i want to show that it diverges in its absolute value and converges in its ""normal"" value. For diverges in its absolute value We know that: Therefore we can write: But we know that the left integral diverges from the question, therefore, by comparison test: diverges. Now the problem in proving converges for the ""normal"" function. I thought to use Dirichlet test, but i dont see how to say that is decreasing monotonic or to talk about the limit. I must say it very sounds like drichlet test, but i cant see how it feets... Couldnt think of other functions for dirichle So i thought about the comparison test, yet couldn't think of a converging function that will fit. In the end, i am stuck. Those are my homework, so i prefer a hint than a solution. Thank you.","f [a,\infty) \displaystyle \int_a^\infty f(x) \, dx 
\exists t> a, \forall x>t: f'(x) < f(x)
 \displaystyle \int_a^\infty f(x) \sin(e^x) \, dx 
|\sin(e^x)| < 1
 
\int_a^\infty |f(x)| \leq \int_a^\infty |f(x)\sin(e^x)| \, dx
 
\int_a^\infty |f(x)\sin(e^x)| \, dx
 f(x)","['real-analysis', 'calculus', 'integration', 'improper-integrals']"
58,How different can $f(g(x))$ and $g(f(x))$ be?,How different can  and  be?,f(g(x)) g(f(x)),"Given $f,g: \mathbb{R} \rightarrow \mathbb{R}$ , how ""different"" can $f(g(x))$ and $g(f(x))$ be? By ""how different"" I mean: Given two real-valued functions $a,b$ do there exist two real-valued functions $f,g$ such that $f(g(x))=a(x)$ and $g(f(x))= b(x)$ ? If not, is there some sens in which $f(g(x))$ and $g(f(x))$ can't be ""too different""?","Given , how ""different"" can and be? By ""how different"" I mean: Given two real-valued functions do there exist two real-valued functions such that and ? If not, is there some sens in which and can't be ""too different""?","f,g: \mathbb{R} \rightarrow \mathbb{R} f(g(x)) g(f(x)) a,b f,g f(g(x))=a(x) g(f(x))= b(x) f(g(x)) g(f(x))","['real-analysis', 'functions', 'real-numbers']"
59,Prove that $e$ is transcendental.,Prove that  is transcendental.,e,"Prove that $e$ is transcendental. I found a proof below, but I'm not sure if it's correct. We prove by contradiction. Suppose $e$ is algebraic. Then $\exists c_0, c_1,...,c_n\in\mathbb{Z}$ such that $c_ne^n + c_{n-1}e^{n-1} + \dots + c_1e+c_0 = 0$ . We may assume $c_n \neq 0$ and $c_0\neq 0$ , since there must be at least one nonzero constant. We attempt to arrive at a contradiction by constructing an integer in $(0,1)$ . Pick a large prime $p > \max \{n, |c_0|\}$ . Let $$f(x) = \dfrac{x^{p-1}}{(p-1)!}(1-x)^p (2-x)^p\dots(n-x)^p=\dfrac{n!}{(p-1)!}x^{p-1}+\dfrac{a_p}{(p-1)!}x^p+\dots+\dfrac{(-1)^n}{(p-1)!}x^{(n+1)p-1},$$ where $a_p$ is a constant. Let $r:=\deg f(x) = (n+1)p-1$ . We claim that if $k \geq p \wedge j\in\mathbb{Z}$ , $f^{(k)}(j) \equiv 0\pmod p$ . $f^{(k)}(x) = \dfrac{a_p}{(p-1)!}p(p-1)\dots(p+1-k)x^{p-k}+\dots+\dfrac{a_l}{(p-1)!}l(l-1)(l-2)\dots(l+1-k)x^{l-k}\equiv 0\pmod p$ as the product of at least $p$ consecutive integers contains at least one multiple of $p$ . $f^{(k)}(j) = \displaystyle\sum_{l=p}^ra_l{l\choose {p-1}}(l+1-p)(l-p)\dots(l+1-k)j^{l-k}\equiv 0\pmod p$ as ${l\choose {p-1}}(l+1-p)$ is a product of $p$ consecutive integers and $(p-1)!$ does not contain any multiples of $p$ . We next claim that if $0\leq k < p,$ then $f^{(k)}(j) = \begin{cases}0&&\text{ , if $j\in \{1,2,...,n\}$ or $j=0$ and $k<p-1$}\\n!&&\text { , if $j=0$ and $k=p-1$}\end{cases}$ . For $i \leq j \leq n$ , $f(x) = (x-j)^p (q_0(x)),$ where $q_0(x)$ is the required polynomial. $f'(x) = (x-j)^{p-1}q_1(x)$ , $f''(x) = (x-j)^{p-2}q_2(x),$ and $f^{(p-1)}(x) = (x-j)q_{p-1}(x)$ . When $j=0$ , $f(x) = \dfrac{x^{p-1}}{(p-1)!}n!+\dots$ and $f^{(p-1)}(x) = n!+\dots$ . Let $F(x) = \displaystyle\sum_{i=0}^r f^{(i)}(x)$ , where $f^{(0)}(x):=f(x)$ . Let $G(x) = e^{-x}F(x)$ . $G'(x) = e^{-x}(F'(x)-F(x)) = -e^{-x}f(x)$ as $f^{(r+1)}(x) =0 $ . For $k=1,2,...,n$ , apply the MVT to $G(x)$ on $[0,k]$ . $\dfrac{G(k)-G(0)}{k}=G'(x_k), x_k\in (0,k).$ Therefore, $e^{-k}F(k) - F(0) = -ke^{-x_k}f(x_k)\Rightarrow F(k)-e^{k}F(0) = -ke^{k-x_k}f(x_k).$ Hence $\displaystyle\sum_{k=j}^n c_k (F(k)-e^kF(0)) = \displaystyle\sum_{k=1}^n c_k F(k) - \displaystyle\sum_{k=1}^n (c_ke^{k})F(0) = \displaystyle\sum_{k=0}^nc_kF(k)\equiv c_0n!\pmod p$ . $F(k) = \displaystyle\sum_{j=0}^nf^{(j)}(k) \equiv 0\pmod p,1\leq k\leq n.$ $F(0) = \displaystyle\sum_{j=0}^{p-2}f^{(j)}(k) + f^{(p-1)}(k)+\displaystyle\sum_{j=p}^r f^{(j)}(k).$ Therefore, $\displaystyle\sum_{k=0}^n c_kF(k) = \displaystyle\sum_{k=1}^n -c_k(ke^{k-x_k}f(x_k))\leq (\displaystyle\sum_{k=1}^n|c_k|ke^k)\dfrac{(n+1)^{(n+1)p}}{(p-1)!}$ . Let $\dfrac{AB^p}{(p-1)!}$ represent the right hand side of this inequality. Hence we have that $\exists x\in\mathbb{Z}$ such that $0 < x < \dfrac{AB^p}{(p-1)!}\to 0 $ as $p \to \infty$ , which is a contradiction. Thus, $e$ is transcendental. Edit: so it turns out this proof is similar to some popular proofs. So I have a few questions: I don't understand why we can't just say $f^{(k)}(x) = pa_p$ or $0$ . Why does $f(x) = (x-j)^p(q_0(x))$ ? How is $\displaystyle\sum_{k=0}^n c_kF(k) = \displaystyle\sum_{k=1}^n -c_k(ke^{k-x_k}f(x_k))\leq (\displaystyle\sum_{k=1}^n|c_k|ke^k)\dfrac{(n+1)^{(n+1)p}}{(p-1)!}$ ?","Prove that is transcendental. I found a proof below, but I'm not sure if it's correct. We prove by contradiction. Suppose is algebraic. Then such that . We may assume and , since there must be at least one nonzero constant. We attempt to arrive at a contradiction by constructing an integer in . Pick a large prime . Let where is a constant. Let . We claim that if , . as the product of at least consecutive integers contains at least one multiple of . as is a product of consecutive integers and does not contain any multiples of . We next claim that if then . For , where is the required polynomial. , and . When , and . Let , where . Let . as . For , apply the MVT to on . Therefore, Hence . Therefore, . Let represent the right hand side of this inequality. Hence we have that such that as , which is a contradiction. Thus, is transcendental. Edit: so it turns out this proof is similar to some popular proofs. So I have a few questions: I don't understand why we can't just say or . Why does ? How is ?","e e \exists c_0, c_1,...,c_n\in\mathbb{Z} c_ne^n + c_{n-1}e^{n-1} + \dots + c_1e+c_0 = 0 c_n \neq 0 c_0\neq 0 (0,1) p > \max \{n, |c_0|\} f(x) = \dfrac{x^{p-1}}{(p-1)!}(1-x)^p (2-x)^p\dots(n-x)^p=\dfrac{n!}{(p-1)!}x^{p-1}+\dfrac{a_p}{(p-1)!}x^p+\dots+\dfrac{(-1)^n}{(p-1)!}x^{(n+1)p-1}, a_p r:=\deg f(x) = (n+1)p-1 k \geq p \wedge j\in\mathbb{Z} f^{(k)}(j) \equiv 0\pmod p f^{(k)}(x) = \dfrac{a_p}{(p-1)!}p(p-1)\dots(p+1-k)x^{p-k}+\dots+\dfrac{a_l}{(p-1)!}l(l-1)(l-2)\dots(l+1-k)x^{l-k}\equiv 0\pmod p p p f^{(k)}(j) = \displaystyle\sum_{l=p}^ra_l{l\choose {p-1}}(l+1-p)(l-p)\dots(l+1-k)j^{l-k}\equiv 0\pmod p {l\choose {p-1}}(l+1-p) p (p-1)! p 0\leq k < p, f^{(k)}(j) = \begin{cases}0&&\text{ , if j\in \{1,2,...,n\} or j=0 and k<p-1}\\n!&&\text { , if j=0 and k=p-1}\end{cases} i \leq j \leq n f(x) = (x-j)^p (q_0(x)), q_0(x) f'(x) = (x-j)^{p-1}q_1(x) f''(x) = (x-j)^{p-2}q_2(x), f^{(p-1)}(x) = (x-j)q_{p-1}(x) j=0 f(x) = \dfrac{x^{p-1}}{(p-1)!}n!+\dots f^{(p-1)}(x) = n!+\dots F(x) = \displaystyle\sum_{i=0}^r f^{(i)}(x) f^{(0)}(x):=f(x) G(x) = e^{-x}F(x) G'(x) = e^{-x}(F'(x)-F(x)) = -e^{-x}f(x) f^{(r+1)}(x) =0  k=1,2,...,n G(x) [0,k] \dfrac{G(k)-G(0)}{k}=G'(x_k), x_k\in (0,k). e^{-k}F(k) - F(0) = -ke^{-x_k}f(x_k)\Rightarrow F(k)-e^{k}F(0) = -ke^{k-x_k}f(x_k). \displaystyle\sum_{k=j}^n c_k (F(k)-e^kF(0)) = \displaystyle\sum_{k=1}^n c_k F(k) - \displaystyle\sum_{k=1}^n (c_ke^{k})F(0) = \displaystyle\sum_{k=0}^nc_kF(k)\equiv c_0n!\pmod p F(k) = \displaystyle\sum_{j=0}^nf^{(j)}(k) \equiv 0\pmod p,1\leq k\leq n. F(0) = \displaystyle\sum_{j=0}^{p-2}f^{(j)}(k) + f^{(p-1)}(k)+\displaystyle\sum_{j=p}^r f^{(j)}(k). \displaystyle\sum_{k=0}^n c_kF(k) = \displaystyle\sum_{k=1}^n -c_k(ke^{k-x_k}f(x_k))\leq
(\displaystyle\sum_{k=1}^n|c_k|ke^k)\dfrac{(n+1)^{(n+1)p}}{(p-1)!} \dfrac{AB^p}{(p-1)!} \exists x\in\mathbb{Z} 0 < x < \dfrac{AB^p}{(p-1)!}\to 0  p \to \infty e f^{(k)}(x) = pa_p 0 f(x) = (x-j)^p(q_0(x)) \displaystyle\sum_{k=0}^n c_kF(k) = \displaystyle\sum_{k=1}^n -c_k(ke^{k-x_k}f(x_k))\leq
(\displaystyle\sum_{k=1}^n|c_k|ke^k)\dfrac{(n+1)^{(n+1)p}}{(p-1)!}","['real-analysis', 'calculus']"
60,Equivalent of a recursively defined sequence,Equivalent of a recursively defined sequence,,Let $u_n$ the real sequence defined by $u_0=1$ and $$u_n=\frac{1}{n}\sum_{k=0}^{n-1} \frac{u_k}{n-k}.$$ The goal is to show that there exists $C>0$ such that $u_n \sim C/n^2$ . I am only able to prove that $u_n =O(\log(n)/n^2)$ . I tried to use an approached based on Cauchy product of power series but it failed.,Let the real sequence defined by and The goal is to show that there exists such that . I am only able to prove that . I tried to use an approached based on Cauchy product of power series but it failed.,u_n u_0=1 u_n=\frac{1}{n}\sum_{k=0}^{n-1} \frac{u_k}{n-k}. C>0 u_n \sim C/n^2 u_n =O(\log(n)/n^2),"['real-analysis', 'sequences-and-series', 'asymptotics']"
61,"If I know $E[\xi]=1$, what can I say about $E[\log (\xi)]$?","If I know , what can I say about ?",E[\xi]=1 E[\log (\xi)],"If I know $E[\xi_{i}]=1$ , what can I say about $E[\log (\xi_{i})]$ ? Background: Let $(\xi_{i})_{i}$ be IID random variables with $E[\xi_{i}]=1$ where $P(\xi_{i}=1)<1$ . I want to use the law of large numbers to show that $\frac{\sum\limits_{i=1}^{n}\log (\xi_{i})}{n}\to c < 0$ So I need to show that $E(\log(\xi_{i}))<0$ . But all we know about the distribution of $\xi_{i}$ is that $P(\xi_{i}=1)<1$ Note: I previously proved that $X_{n}:=\prod\limits_{i=1}^{n}\xi_{i}\xrightarrow{n \to \infty} 0-$ a.s, I do not know whether this helps. Intuitively it makes sense because comparing the $\xi_{i}>1$ weight with that of $\xi_{i}<1$ , moving along the $\log$ curve gives a steeper ""decline"" than an ""increase""","If I know , what can I say about ? Background: Let be IID random variables with where . I want to use the law of large numbers to show that So I need to show that . But all we know about the distribution of is that Note: I previously proved that a.s, I do not know whether this helps. Intuitively it makes sense because comparing the weight with that of , moving along the curve gives a steeper ""decline"" than an ""increase""",E[\xi_{i}]=1 E[\log (\xi_{i})] (\xi_{i})_{i} E[\xi_{i}]=1 P(\xi_{i}=1)<1 \frac{\sum\limits_{i=1}^{n}\log (\xi_{i})}{n}\to c < 0 E(\log(\xi_{i}))<0 \xi_{i} P(\xi_{i}=1)<1 X_{n}:=\prod\limits_{i=1}^{n}\xi_{i}\xrightarrow{n \to \infty} 0- \xi_{i}>1 \xi_{i}<1 \log,"['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'expected-value']"
62,Showing that Sobolev norms on manifolds are equivalent,Showing that Sobolev norms on manifolds are equivalent,,"Let me first define a ""Sobolev space on manifold"". Let $M$ be a closed $n$ -dimensional manifold, $E \rightarrow M$ a complex vector bundle. Let us pick: A finite cover of $M$ by sets $U_i$ . charts $h_i:U_i \cong \Bbb R^n$ . Trivilizations $\phi_i$ of $E|_{U_i}$ $\mu_i$ particion of unity of subordinate to $\{U_i\}$ . Define the Sobolev norm of a section $u \in \Gamma(M,E)$ by $$ ||u||_k^2 := \sum_i ||(\mu_i \circ h_i^{-1}) (\phi_i \circ u \circ h_i^{-1} ) ||_k^2$$ this is well defined, the RHS being a fintie sum of Sobolev $k$ -norm of compactly supported functions on $\Bbb R^n$ . So I want to show The equivalence class of $|| \cdot ||_k$ is independent of the choices made. What I know: Result 1: Let $a \in C^\infty_c$ . Then $f \mapsto af$ extends to a bounded operator $M_a:W^s \rightarrow W^s$ for each $s \in \Bbb Z$ . $$||au||_s \le C(a)||u||_s$$ Result 2: Let $\phi:U' \rightarrow V'$ be a diffeomoprhism of open subsets of $\Bbb R^n$ with $U \subseteq U'$ and $V= \phi(U) \subseteq V'$ be relatively compact. Then $u \mapsto u\circ \phi$ extends to a bounded map  for all $s \in \Bbb Z$ . $$W^s (V) \rightarrow W^s(U) $$ Thoughts so far: Edit: I believe we start by proving 4. Let us see first vary the partition of unity, with $\tau:= \{ \tau_i \}$ .  So that $$ \tau _j  = \sum_i \tau_j \mu_i $$ \begin{align*}  || (\tau_j \circ h_j^{-1}) (\phi_j \circ u \circ h_j^{-1}) ||_k^2 & \le \sum _i || (\tau_j \circ h_j^{-1}) (\mu_i \circ h_j^{-1})  (\phi_j \circ u \circ h_j^{-1}) ||_k^2  \\   & \le C(\tau) ||u||_{k}^2 \end{align*} constant $C(\tau)$ dependent on partition. This uses Result 1 .  Then if we take a another cover $\{V_j\}$ . From independence of 4, we may choose a partition wrt $U_i \cap V_j$ . Using Result 2 , we take care of 2. Now I am stuck at addressing 3. This post should be pretty much self contained, but for those who might find it helpful in consulting original source, I am concered with Lemmea 3.6.2, pg 47.","Let me first define a ""Sobolev space on manifold"". Let be a closed -dimensional manifold, a complex vector bundle. Let us pick: A finite cover of by sets . charts . Trivilizations of particion of unity of subordinate to . Define the Sobolev norm of a section by this is well defined, the RHS being a fintie sum of Sobolev -norm of compactly supported functions on . So I want to show The equivalence class of is independent of the choices made. What I know: Result 1: Let . Then extends to a bounded operator for each . Result 2: Let be a diffeomoprhism of open subsets of with and be relatively compact. Then extends to a bounded map  for all . Thoughts so far: Edit: I believe we start by proving 4. Let us see first vary the partition of unity, with .  So that constant dependent on partition. This uses Result 1 .  Then if we take a another cover . From independence of 4, we may choose a partition wrt . Using Result 2 , we take care of 2. Now I am stuck at addressing 3. This post should be pretty much self contained, but for those who might find it helpful in consulting original source, I am concered with Lemmea 3.6.2, pg 47.","M n E \rightarrow M M U_i h_i:U_i \cong \Bbb R^n \phi_i E|_{U_i} \mu_i \{U_i\} u \in \Gamma(M,E)  ||u||_k^2 := \sum_i ||(\mu_i \circ h_i^{-1}) (\phi_i \circ u \circ h_i^{-1} ) ||_k^2 k \Bbb R^n || \cdot ||_k a \in C^\infty_c f \mapsto af M_a:W^s \rightarrow W^s s \in \Bbb Z ||au||_s \le C(a)||u||_s \phi:U' \rightarrow V' \Bbb R^n U \subseteq U' V= \phi(U) \subseteq V' u \mapsto u\circ \phi s \in \Bbb Z W^s (V) \rightarrow W^s(U)  \tau:= \{ \tau_i \}  \tau _j  = \sum_i \tau_j \mu_i  \begin{align*}
 || (\tau_j \circ h_j^{-1}) (\phi_j \circ u \circ h_j^{-1}) ||_k^2 & \le \sum _i || (\tau_j \circ h_j^{-1}) (\mu_i \circ h_j^{-1})  (\phi_j \circ u \circ h_j^{-1}) ||_k^2  \\ 
 & \le C(\tau) ||u||_{k}^2
\end{align*} C(\tau) \{V_j\} U_i \cap V_j","['real-analysis', 'functional-analysis', 'differential-geometry', 'differential-topology', 'sobolev-spaces']"
63,Advice on thinking as an algebraist or topologist to prove theorems in analysis,Advice on thinking as an algebraist or topologist to prove theorems in analysis,,"Browsing through stackexchange, I'm often struck by the elegance of topological/algebraic proofs for claims in analysis. Time and again, I can come up with one proof but would not have thought of the other, with the ones I think of coming from an analyst's, or sometimes more sacrilegiously, an engineer's perspective. I'm good at juggling around epsilons, establishing inequalities, or proving one of many types of  convergence, but whenever the key step is to invent new spaces as simple as a quotient space, I get stuck. Infuriatingly, I tend to understand the proof once it is presented - I just cannot come up with it on my own. Let me illustrate with an example: The claim to be proven is the following: Let $T: X \rightarrow Y $ be a linear mapping between two normed   linear spaces. Assume you have proved that if $X$ is finite dimensional,   $T$ is continuous. Prove that if $Y$ is finite dimensional, $T$ is   continuous if and only if $\ker(T)$ is closed. If $T$ is continuous, then since $\ker(T) = T^{-1}(0)$ is the preimage of a closed set, it is closed. So far so good. For the second portion I have two different proofs: The first proof is gritty, and in a style of an analysis textbook: Without loss of generality, $T$ is surjective. Let $\{e_i\}_{i=1}^n$ be a basis for $Y$. Then there exists $\{u_i\}_{i=1}^n$ such that $Tu_i = e_i$. If $T$ were not continuous, there is a sequence $\{x_j\}_{j=1}^\infty \rightarrow 0$ such that $\|Tx_j\|=1$. The unit sphere in $X$ in $Y$ is compact, hence there is a subsequence (for simplicity just $\{x_j\}_{j=1}^\infty$) and a $y \in Y$ with $\|y\| =1$ such that $Tx_j \rightarrow y$. Since $Y$ is finite-dimensional, we have for some appropriate coefficients that  $$Tx_j = \sum_{i=1}^n \alpha_{i,j} e_i, \quad y = \sum_{i=1}^n \alpha_i e_i , \text{ where } \alpha_{i,j} \rightarrow \alpha_i \text{ as } j \rightarrow \infty .$$ Denote $w_j = \sum_{i=1}^n \alpha_{i,j} u_i, w = \sum_{i=1}^n \alpha_i u_i $. Note that $T(w_j - x_j) = 0$, so $w_j - x_j \in \ker(T)$. Since $\ker(T)$ is closed $w = \lim w_j - x_j \in \ker(T)$, but $Tw = y \neq 0$, a contradiction. Hence $T$ must be continuous. The second proof is somewhat topological, and much more clean and elegant: Since $\ker(T)$ is closed, $X/\ker(T)$ is a normed linear space. Define $\bar{T}:X/\ker(T) \rightarrow Y$ by $\bar{T}(x + \ker(T)) = T(x)$, which is linear, and continuous since $X/ker(T)$ is finite-dimensional. Define $\pi: X \rightarrow X/\ker(T)$ in the obvious way. Then note that $T = \bar{T} \circ \pi$ is a composition of continuous function, hence continuous. # This is definitely not a perfect example, but in my imperfect vocabulary, the first, gritty proof is very detailed, full of analysis concepts such as convergence, or simple linear algebra, whereas the second deals with properties of the space as a whole, without relying much on the objects within it. I realize that it is hard to compare these 'styles' in proof, and the first style can go a long way. Nevertheless, I feel that to become a better student of mathematics, and even of analysis, I should get a better handle on algebraic or topological techniques. I've taken courses in abstract algebra and topology, but my intuition for these has not grown sufficiently for me to think of questions in analysis in largely algebraic or topological ways, at least beyond standard linear algebra. In the end, I am sure that I want to study analysis, mostly for applied purposes, and I'm aware that there are practical constraints - I won't be in school forever. Would you still agree that developing greater topological intuition is worthwhile? Why or why not? If you feel that it is worth it, how should I go about cultivating said intuition (keeping in mind that eventually, I will enter applied rather than pure math)?","Browsing through stackexchange, I'm often struck by the elegance of topological/algebraic proofs for claims in analysis. Time and again, I can come up with one proof but would not have thought of the other, with the ones I think of coming from an analyst's, or sometimes more sacrilegiously, an engineer's perspective. I'm good at juggling around epsilons, establishing inequalities, or proving one of many types of  convergence, but whenever the key step is to invent new spaces as simple as a quotient space, I get stuck. Infuriatingly, I tend to understand the proof once it is presented - I just cannot come up with it on my own. Let me illustrate with an example: The claim to be proven is the following: Let $T: X \rightarrow Y $ be a linear mapping between two normed   linear spaces. Assume you have proved that if $X$ is finite dimensional,   $T$ is continuous. Prove that if $Y$ is finite dimensional, $T$ is   continuous if and only if $\ker(T)$ is closed. If $T$ is continuous, then since $\ker(T) = T^{-1}(0)$ is the preimage of a closed set, it is closed. So far so good. For the second portion I have two different proofs: The first proof is gritty, and in a style of an analysis textbook: Without loss of generality, $T$ is surjective. Let $\{e_i\}_{i=1}^n$ be a basis for $Y$. Then there exists $\{u_i\}_{i=1}^n$ such that $Tu_i = e_i$. If $T$ were not continuous, there is a sequence $\{x_j\}_{j=1}^\infty \rightarrow 0$ such that $\|Tx_j\|=1$. The unit sphere in $X$ in $Y$ is compact, hence there is a subsequence (for simplicity just $\{x_j\}_{j=1}^\infty$) and a $y \in Y$ with $\|y\| =1$ such that $Tx_j \rightarrow y$. Since $Y$ is finite-dimensional, we have for some appropriate coefficients that  $$Tx_j = \sum_{i=1}^n \alpha_{i,j} e_i, \quad y = \sum_{i=1}^n \alpha_i e_i , \text{ where } \alpha_{i,j} \rightarrow \alpha_i \text{ as } j \rightarrow \infty .$$ Denote $w_j = \sum_{i=1}^n \alpha_{i,j} u_i, w = \sum_{i=1}^n \alpha_i u_i $. Note that $T(w_j - x_j) = 0$, so $w_j - x_j \in \ker(T)$. Since $\ker(T)$ is closed $w = \lim w_j - x_j \in \ker(T)$, but $Tw = y \neq 0$, a contradiction. Hence $T$ must be continuous. The second proof is somewhat topological, and much more clean and elegant: Since $\ker(T)$ is closed, $X/\ker(T)$ is a normed linear space. Define $\bar{T}:X/\ker(T) \rightarrow Y$ by $\bar{T}(x + \ker(T)) = T(x)$, which is linear, and continuous since $X/ker(T)$ is finite-dimensional. Define $\pi: X \rightarrow X/\ker(T)$ in the obvious way. Then note that $T = \bar{T} \circ \pi$ is a composition of continuous function, hence continuous. # This is definitely not a perfect example, but in my imperfect vocabulary, the first, gritty proof is very detailed, full of analysis concepts such as convergence, or simple linear algebra, whereas the second deals with properties of the space as a whole, without relying much on the objects within it. I realize that it is hard to compare these 'styles' in proof, and the first style can go a long way. Nevertheless, I feel that to become a better student of mathematics, and even of analysis, I should get a better handle on algebraic or topological techniques. I've taken courses in abstract algebra and topology, but my intuition for these has not grown sufficiently for me to think of questions in analysis in largely algebraic or topological ways, at least beyond standard linear algebra. In the end, I am sure that I want to study analysis, mostly for applied purposes, and I'm aware that there are practical constraints - I won't be in school forever. Would you still agree that developing greater topological intuition is worthwhile? Why or why not? If you feel that it is worth it, how should I go about cultivating said intuition (keeping in mind that eventually, I will enter applied rather than pure math)?",,"['real-analysis', 'general-topology', 'functional-analysis', 'soft-question', 'normed-spaces']"
64,"If $\{j_n\}$ is increasing and $\lim\limits_{n→∞}\frac{j_n}n=+∞$, then $\sum\limits_{n=1}^∞\frac{(-1)^{[\sqrt n]}}{j_n}$ converges","If  is increasing and , then  converges",\{j_n\} \lim\limits_{n→∞}\frac{j_n}n=+∞ \sum\limits_{n=1}^∞\frac{(-1)^{[\sqrt n]}}{j_n},"My friend gave me an interesting analysis problem, and I was tackling it but it was very hard indeed. Here is the problem: Let $j_1,j_2,j_3,\cdots$ be a sequence of strictly increasing positive integers such that $\lim\limits_{n\rightarrow \infty} \dfrac{j_n}{n} = +\infty$. Prove that $\sum\limits_{n=1}^{\infty}\dfrac{(-1)^{[\sqrt{n}]}}{j_n}$ converges. Note: $[x]$ is the greatest integer less than or equal to $x.$ I think this is some hardcore analysis problem, I would really love to know how you solve this problem, and possibly a full out solution. I am trying my hardest to solve it too, and would appreciate any help offered. My attempt at proof of the above: We know that $$\lim_{n \rightarrow \infty}\frac{\frac{1}{j_n\sqrt{n}}}{\frac{1}{n\sqrt{n}}}=0$$ and clearly the summation: $$\sum_{n=1}^{\infty} \frac{1}{n\sqrt{n}}$$ converges, so we cann directly deduce that  $\sum_{n=1}^{\infty} \frac{1}{j_n\sqrt{n}}$ converges by the limit comparison test. Now, let $k_n =\frac{1}{j_n}$, and claim that $S_l=\sum\limits_{n=1}^{l}(-1)^{[\sqrt{n}]}k_n$ is Cauchy. Now we take the terms $S_l-S_{m-1}=\sum\limits_{n=m}^{l}(-1)^{[\sqrt{n}]}k_n$ in to consideration, and to bound the term from above, we delete all terms with $[\sqrt{n}]=[\sqrt{m}]$ if $[\sqrt{m}]$ is odd and insert all missing terms with $[\sqrt{m}] =  [\sqrt{n}]$ if  $[\sqrt{m}]$ is even. Hence for a fixed $l,$ taking $1< m < l$, there exists $n,s$ with $n\le s$ such that: $$S_l-S_{m-1}=\sum\limits_{t=m}^{l}(-1)^{[\sqrt{t}]}k_t\le \sum\limits_{t=(2n)^2}^{(2s+1)^2-1}(-1)^{[\sqrt{t}]}k_t\le $$ $$\sum\limits_{t=(2n)^2}^{(2s+1)^2-1}k_t + \sum^s_{t=n+1}\bigg[2k_{(2t+1)^2-2}+\sum_{u=0}^{(2t)^2-(2t-1)^2-1}(k_{(2t)^2+u}-k_{(2t-1)^2=u})\bigg]$$ $$\le (4n+1)k_{(2n)^2}+\sum^{s}_{t=n+1}2k_{(2t+1)^2-2}\le (4n+1)k_{(2n)^2}+2\sum^s_{t=n+1}\frac{1}{8t}\sum^{(2t+1)^2-2}_{u=(2t-1)^2-1}k_u$$ $$\le (4n+1)k_{(2n)^2}+\sum^s_{t=n+1}\sum^{(2t+1)^2-2}_{u=(2t-1)^2-1}\frac{k_u}{\sqrt{u}}\le  (4n+1)k_{(2n)^2}+\sum^{(2s+1)^2-2}_{u=(2n+1)^2-1}\frac{k_u}{\sqrt{u}} $$ It is not hard to see that as $m \rightarrow \infty,$ $n \rightarrow \infty$. We finally claim that the last expression goes to $0$ as $n \rightarrow \infty$. Note that the second term of the last expression tends to $0$ as as $n \rightarrow \infty$ as $\frac{k_u}{\sqrt{u}}$ converges. So it remains to show that the first term tends to $0$. We prove this by contradiction. Proof: For some $\epsilon > 0,$ assume that $(4n+1)k_{(2n)^2} >\epsilon$ infinitely often. If we recursively take $n_t>2n_{t-1}$ for $t \ge 1,$ with $(4n+1)k_{(2n)^2}>\epsilon$, then: $$\sum_{l=1}^{\infty} \frac{k_l}{\sqrt{l}}\ge \sum_{t=1}^{\infty}\sum_{u=n_t^2}^{(2n_t)^2}\frac{k_u}{\sqrt{u}}\le \sum_{t=1}^{\infty}3n_t^2 \frac{k_{(2n_t)^2}}{2n_t}=\frac{3}{2}\sum^{\infty}_{t=1}n_t k_{(2n_t)^2}$$ $$\ge \frac{3}{10}\sum^{\infty}_{t=1}(k_{(2n_t)^2} \ge \frac{3}{10}\sum^{\infty}_{u=1}\epsilon =+\infty$$ This is a clear contradiction. So $\lim_{l\rightarrow \infty}\sup\{S_l-S_{m-1}:m<l\}\le 0$. We can repeat the above with $S_l-S_{m-1}$ bounded from below and get $\lim_{l\rightarrow \infty}\inf\{S_l-S_{m-1}:m<l\}\ge 0$, hence clearly $ S_l$ is Cauchy and thus converges. The reason I tried to come with a different idea is that I do not know what Dedekind's Test is, and even though I looked it up just so I can understand the accepted answer, it did not seem to make me fully satisfied. Although I am more aware of Dirichlet's Criterion for series. But of course I now come to realize that they are infact similar.","My friend gave me an interesting analysis problem, and I was tackling it but it was very hard indeed. Here is the problem: Let $j_1,j_2,j_3,\cdots$ be a sequence of strictly increasing positive integers such that $\lim\limits_{n\rightarrow \infty} \dfrac{j_n}{n} = +\infty$. Prove that $\sum\limits_{n=1}^{\infty}\dfrac{(-1)^{[\sqrt{n}]}}{j_n}$ converges. Note: $[x]$ is the greatest integer less than or equal to $x.$ I think this is some hardcore analysis problem, I would really love to know how you solve this problem, and possibly a full out solution. I am trying my hardest to solve it too, and would appreciate any help offered. My attempt at proof of the above: We know that $$\lim_{n \rightarrow \infty}\frac{\frac{1}{j_n\sqrt{n}}}{\frac{1}{n\sqrt{n}}}=0$$ and clearly the summation: $$\sum_{n=1}^{\infty} \frac{1}{n\sqrt{n}}$$ converges, so we cann directly deduce that  $\sum_{n=1}^{\infty} \frac{1}{j_n\sqrt{n}}$ converges by the limit comparison test. Now, let $k_n =\frac{1}{j_n}$, and claim that $S_l=\sum\limits_{n=1}^{l}(-1)^{[\sqrt{n}]}k_n$ is Cauchy. Now we take the terms $S_l-S_{m-1}=\sum\limits_{n=m}^{l}(-1)^{[\sqrt{n}]}k_n$ in to consideration, and to bound the term from above, we delete all terms with $[\sqrt{n}]=[\sqrt{m}]$ if $[\sqrt{m}]$ is odd and insert all missing terms with $[\sqrt{m}] =  [\sqrt{n}]$ if  $[\sqrt{m}]$ is even. Hence for a fixed $l,$ taking $1< m < l$, there exists $n,s$ with $n\le s$ such that: $$S_l-S_{m-1}=\sum\limits_{t=m}^{l}(-1)^{[\sqrt{t}]}k_t\le \sum\limits_{t=(2n)^2}^{(2s+1)^2-1}(-1)^{[\sqrt{t}]}k_t\le $$ $$\sum\limits_{t=(2n)^2}^{(2s+1)^2-1}k_t + \sum^s_{t=n+1}\bigg[2k_{(2t+1)^2-2}+\sum_{u=0}^{(2t)^2-(2t-1)^2-1}(k_{(2t)^2+u}-k_{(2t-1)^2=u})\bigg]$$ $$\le (4n+1)k_{(2n)^2}+\sum^{s}_{t=n+1}2k_{(2t+1)^2-2}\le (4n+1)k_{(2n)^2}+2\sum^s_{t=n+1}\frac{1}{8t}\sum^{(2t+1)^2-2}_{u=(2t-1)^2-1}k_u$$ $$\le (4n+1)k_{(2n)^2}+\sum^s_{t=n+1}\sum^{(2t+1)^2-2}_{u=(2t-1)^2-1}\frac{k_u}{\sqrt{u}}\le  (4n+1)k_{(2n)^2}+\sum^{(2s+1)^2-2}_{u=(2n+1)^2-1}\frac{k_u}{\sqrt{u}} $$ It is not hard to see that as $m \rightarrow \infty,$ $n \rightarrow \infty$. We finally claim that the last expression goes to $0$ as $n \rightarrow \infty$. Note that the second term of the last expression tends to $0$ as as $n \rightarrow \infty$ as $\frac{k_u}{\sqrt{u}}$ converges. So it remains to show that the first term tends to $0$. We prove this by contradiction. Proof: For some $\epsilon > 0,$ assume that $(4n+1)k_{(2n)^2} >\epsilon$ infinitely often. If we recursively take $n_t>2n_{t-1}$ for $t \ge 1,$ with $(4n+1)k_{(2n)^2}>\epsilon$, then: $$\sum_{l=1}^{\infty} \frac{k_l}{\sqrt{l}}\ge \sum_{t=1}^{\infty}\sum_{u=n_t^2}^{(2n_t)^2}\frac{k_u}{\sqrt{u}}\le \sum_{t=1}^{\infty}3n_t^2 \frac{k_{(2n_t)^2}}{2n_t}=\frac{3}{2}\sum^{\infty}_{t=1}n_t k_{(2n_t)^2}$$ $$\ge \frac{3}{10}\sum^{\infty}_{t=1}(k_{(2n_t)^2} \ge \frac{3}{10}\sum^{\infty}_{u=1}\epsilon =+\infty$$ This is a clear contradiction. So $\lim_{l\rightarrow \infty}\sup\{S_l-S_{m-1}:m<l\}\le 0$. We can repeat the above with $S_l-S_{m-1}$ bounded from below and get $\lim_{l\rightarrow \infty}\inf\{S_l-S_{m-1}:m<l\}\ge 0$, hence clearly $ S_l$ is Cauchy and thus converges. The reason I tried to come with a different idea is that I do not know what Dedekind's Test is, and even though I looked it up just so I can understand the accepted answer, it did not seem to make me fully satisfied. Although I am more aware of Dirichlet's Criterion for series. But of course I now come to realize that they are infact similar.",,"['real-analysis', 'sequences-and-series']"
65,"Theorem 3.4.11 in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: Limit superior and limit inferior of a bounded sequence of real numbers","Theorem 3.4.11 in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: Limit superior and limit inferior of a bounded sequence of real numbers",,"Here is Theorem 3.4.11 in the book Introduction to Real Analysis by Robert G. Bartle and Donald R. Sherbert, 4th edition: If $\left( x_n \right)$ is a bounded sequence of real numbers, then the following statements for a real number $x^*$ are equivalent. (a) $x^* = \limsup \left( x_n \right)$. (b) If $\varepsilon > 0$, there are at most a finite number of $n \in \mathbb{N}$ such that $x^* + \varepsilon < x_n$, but an infinite number of $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$. (c) If $u_m = \sup \left\{ \ x_n \ \colon \ n \geq m \ \right\}$, then $x^* = \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} = \lim \left( u_m \right)$. (d) If $S$ is the set of subsequential limits of $\left( x_n \right)$, then $x^* = \sup S$. And, here is Definition 3.4.10: Let $X = \left( x_n \right)$ be a bounded sequence of real numbers. (a) The limit superior of $\left( x_n \right)$ is the infimum of the set $V$ of $v \in \mathbb{R}$ such that $v < x_n$ for at most a finite number of $n \in \mathbb{N}$. It is denoted by    $$ \limsup \left( x_n \right) \qquad \mbox{ or } \qquad \limsup X \qquad \mbox{ or } \qquad \overline{\lim} \left( x_n \right). $$ (b) The limit inferior of $\left( x_n \right)$ is the supremum of the set of $w \in \mathbb{R}$ such that $x_n < w$ for at most a finite number of $n \in \mathbb{N}$. It is denoted by    $$ \liminf  \left( x_n \right) \qquad \mbox{ or } \qquad \liminf X \qquad \mbox{ or } \qquad \underline{\lim} \left( x_n \right). $$ Finally, here is the proof of Theorem 3.4.11: [In what follows, I have included my questions and explanations within square brackets.] (a) implies (b). If $\varepsilon > 0$, then the fact that $x^*$ is an infimum implies that there exists a $v$ in $V$ such that $x^* \leq v < x^* + \varepsilon$. Therefore $x^*$ also belongs to $V$, [I think here it should be $x^*+\varepsilon$ instead of $x^*$. Am I right?] so there can be at most a finite number of $n \in \mathbb{N}$ such that $x^*+\varepsilon < x_n$. On the other hand, $x^*-\varepsilon$ is not in $V$ so there are an infinite number $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$. (b) implies (c). If (b) holds, given $\varepsilon > 0$, then for all sufficiently large $m$ we have $u_m < x+\varepsilon$. [I think this should be $u_m < x^* + \varepsilon$. Am I right?] Therefore, $\inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} \leq x^* + \varepsilon$. [I think we can even write this as $\inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} < x^* + \varepsilon $. Am I right?] Also, since there are an infinite number of $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$, then $x^* - \varepsilon < u_m$ for all $m \in \mathbb{N}$ and hence $x^* - \varepsilon \leq \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\}$. Since $\varepsilon > 0$ is arbitrary, we conclude that $x^* = \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\}$. Moreover, since the sequence $\left( u_m \right)$ is monotone decreasing, we have $\inf \left( u_m \right) = \lim \left( u_m \right)$. (c) implies (d). Suppose that $X^\prime = \left( x_{n_k} \right)$ is a convergent subsequence of $X = \left( x_n \right)$. Since $n_k \geq k$, we have $x_{n_k} \leq u_k$ and hence $\lim X^\prime \leq \lim \left( u_k \right) = x^*$. Conversely, there exists $n_1$ such that $u_1 - 1 \leq x_{n_1} \leq u_1$. [I think here we can even write $u_1 - 1 < x_{n_1} \leq u_1$. Am I right?] [Suppose that $n_k$ has been chosen.] Inductively choose $n_{k+1} > n_k$ such that    $$ u_k - \frac{1}{k+1} < x_{n_{k+1}} \leq u_k. $$   [But I think we should write this as $u_{k+1} - \frac{1}{k+1} < x_{n_{k+1}} \leq u_{k+1}$ or as $u_{k+1} - \frac{1}{k+1} \leq x_{n_{k+1}} \leq u_{k+1}$. Am I right? What is an explicit procedure and justification of how $x_{n_2}$ can be chosen such that $n_2 > n_1$ and such that $u_2 - \frac{1}{2} \leq x_{n_2} \leq u_2$? I would appreciate a detailed account of this. ] Since $\lim \left( u_k \right) = x^*$, it follows that $x^* = \lim \left( x_{n_k} \right)$, and hence $x^* \in S$. [In fact, here we have even shown that $x^*$ is the largest element of $S$. Am I right?] (d) implies (a). Let $w = \sup S$. If $\varepsilon > 0$ is given, then [because there can be no subsequential limit of $\left( x_n \right)$ exceeding $w$ and, as there is always a convergent subsequence of every bounded sequence of real numbers, so there can be no subsequence of $\left( x_n \right)$ having infinitely many terms in the interval $(w, +\infty)$] there are at most finitely many $n$ with $w + \varepsilon < x_n$. Therefore $w+\varepsilon$ belongs to $V$ and $\limsup \left( x_n \right) \leq w+\varepsilon$. On the other hand, there exists a subsequence of $\left( x_n \right)$ converging to some number larger than $w - \varepsilon$, [We call that number $x$.] so that $w - \varepsilon$ is not in $V$,  [Is this because there are infinitely many terms of that convergent subsequence that are in any neighborhood of $x$ and hence are greater than $w + \varepsilon$?] and hence $w - \varepsilon \leq \limsup \left( x_n \right)$. [How is this true? Is it because of the fact that if $\limsup \left(x_n \right)$ were less than $w - \varepsilon$, then there would be some element $v \in V$ such that  $\limsup \left(x_n \right) \leq v < w - \varepsilon < x$ and by definition of set $V$ in that case there would be only finitely many terms of the sequence exceeding $v$ and hence only finitely many terms of the sequence in the neighborhood $( w - \varepsilon , 2x + w-\varepsilon )$, for example, of $x$?] Since $\varepsilon > 0$ is arbitrary, we conclude that $w = \limsup \left( x_n \right)$. In the above proof, I have included my questions and points of confusion within square brackets. Is there any occasion where (in any of my explanations within square brackets) I've gone wrong with my reasoning? In (c) implies (d), once we have shown that $x^*$ is an upper bound for the set $S$ of subsequential limits of $\left( x_n \right)$, is there an alternative approach to showing that $x^*$ is in fact the supremum of $S$ (or even an element of $S$)?","Here is Theorem 3.4.11 in the book Introduction to Real Analysis by Robert G. Bartle and Donald R. Sherbert, 4th edition: If $\left( x_n \right)$ is a bounded sequence of real numbers, then the following statements for a real number $x^*$ are equivalent. (a) $x^* = \limsup \left( x_n \right)$. (b) If $\varepsilon > 0$, there are at most a finite number of $n \in \mathbb{N}$ such that $x^* + \varepsilon < x_n$, but an infinite number of $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$. (c) If $u_m = \sup \left\{ \ x_n \ \colon \ n \geq m \ \right\}$, then $x^* = \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} = \lim \left( u_m \right)$. (d) If $S$ is the set of subsequential limits of $\left( x_n \right)$, then $x^* = \sup S$. And, here is Definition 3.4.10: Let $X = \left( x_n \right)$ be a bounded sequence of real numbers. (a) The limit superior of $\left( x_n \right)$ is the infimum of the set $V$ of $v \in \mathbb{R}$ such that $v < x_n$ for at most a finite number of $n \in \mathbb{N}$. It is denoted by    $$ \limsup \left( x_n \right) \qquad \mbox{ or } \qquad \limsup X \qquad \mbox{ or } \qquad \overline{\lim} \left( x_n \right). $$ (b) The limit inferior of $\left( x_n \right)$ is the supremum of the set of $w \in \mathbb{R}$ such that $x_n < w$ for at most a finite number of $n \in \mathbb{N}$. It is denoted by    $$ \liminf  \left( x_n \right) \qquad \mbox{ or } \qquad \liminf X \qquad \mbox{ or } \qquad \underline{\lim} \left( x_n \right). $$ Finally, here is the proof of Theorem 3.4.11: [In what follows, I have included my questions and explanations within square brackets.] (a) implies (b). If $\varepsilon > 0$, then the fact that $x^*$ is an infimum implies that there exists a $v$ in $V$ such that $x^* \leq v < x^* + \varepsilon$. Therefore $x^*$ also belongs to $V$, [I think here it should be $x^*+\varepsilon$ instead of $x^*$. Am I right?] so there can be at most a finite number of $n \in \mathbb{N}$ such that $x^*+\varepsilon < x_n$. On the other hand, $x^*-\varepsilon$ is not in $V$ so there are an infinite number $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$. (b) implies (c). If (b) holds, given $\varepsilon > 0$, then for all sufficiently large $m$ we have $u_m < x+\varepsilon$. [I think this should be $u_m < x^* + \varepsilon$. Am I right?] Therefore, $\inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} \leq x^* + \varepsilon$. [I think we can even write this as $\inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} < x^* + \varepsilon $. Am I right?] Also, since there are an infinite number of $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$, then $x^* - \varepsilon < u_m$ for all $m \in \mathbb{N}$ and hence $x^* - \varepsilon \leq \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\}$. Since $\varepsilon > 0$ is arbitrary, we conclude that $x^* = \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\}$. Moreover, since the sequence $\left( u_m \right)$ is monotone decreasing, we have $\inf \left( u_m \right) = \lim \left( u_m \right)$. (c) implies (d). Suppose that $X^\prime = \left( x_{n_k} \right)$ is a convergent subsequence of $X = \left( x_n \right)$. Since $n_k \geq k$, we have $x_{n_k} \leq u_k$ and hence $\lim X^\prime \leq \lim \left( u_k \right) = x^*$. Conversely, there exists $n_1$ such that $u_1 - 1 \leq x_{n_1} \leq u_1$. [I think here we can even write $u_1 - 1 < x_{n_1} \leq u_1$. Am I right?] [Suppose that $n_k$ has been chosen.] Inductively choose $n_{k+1} > n_k$ such that    $$ u_k - \frac{1}{k+1} < x_{n_{k+1}} \leq u_k. $$   [But I think we should write this as $u_{k+1} - \frac{1}{k+1} < x_{n_{k+1}} \leq u_{k+1}$ or as $u_{k+1} - \frac{1}{k+1} \leq x_{n_{k+1}} \leq u_{k+1}$. Am I right? What is an explicit procedure and justification of how $x_{n_2}$ can be chosen such that $n_2 > n_1$ and such that $u_2 - \frac{1}{2} \leq x_{n_2} \leq u_2$? I would appreciate a detailed account of this. ] Since $\lim \left( u_k \right) = x^*$, it follows that $x^* = \lim \left( x_{n_k} \right)$, and hence $x^* \in S$. [In fact, here we have even shown that $x^*$ is the largest element of $S$. Am I right?] (d) implies (a). Let $w = \sup S$. If $\varepsilon > 0$ is given, then [because there can be no subsequential limit of $\left( x_n \right)$ exceeding $w$ and, as there is always a convergent subsequence of every bounded sequence of real numbers, so there can be no subsequence of $\left( x_n \right)$ having infinitely many terms in the interval $(w, +\infty)$] there are at most finitely many $n$ with $w + \varepsilon < x_n$. Therefore $w+\varepsilon$ belongs to $V$ and $\limsup \left( x_n \right) \leq w+\varepsilon$. On the other hand, there exists a subsequence of $\left( x_n \right)$ converging to some number larger than $w - \varepsilon$, [We call that number $x$.] so that $w - \varepsilon$ is not in $V$,  [Is this because there are infinitely many terms of that convergent subsequence that are in any neighborhood of $x$ and hence are greater than $w + \varepsilon$?] and hence $w - \varepsilon \leq \limsup \left( x_n \right)$. [How is this true? Is it because of the fact that if $\limsup \left(x_n \right)$ were less than $w - \varepsilon$, then there would be some element $v \in V$ such that  $\limsup \left(x_n \right) \leq v < w - \varepsilon < x$ and by definition of set $V$ in that case there would be only finitely many terms of the sequence exceeding $v$ and hence only finitely many terms of the sequence in the neighborhood $( w - \varepsilon , 2x + w-\varepsilon )$, for example, of $x$?] Since $\varepsilon > 0$ is arbitrary, we conclude that $w = \limsup \left( x_n \right)$. In the above proof, I have included my questions and points of confusion within square brackets. Is there any occasion where (in any of my explanations within square brackets) I've gone wrong with my reasoning? In (c) implies (d), once we have shown that $x^*$ is an upper bound for the set $S$ of subsequential limits of $\left( x_n \right)$, is there an alternative approach to showing that $x^*$ is in fact the supremum of $S$ (or even an element of $S$)?",,"['calculus', 'real-analysis', 'sequences-and-series', 'proof-explanation', 'limsup-and-liminf']"
66,Elegant but pugnacious inequality,Elegant but pugnacious inequality,,"I have a solid and difficult problem that I can't solve, this is the following : Let $p,q,r,s,t,u$ be real positive numbers then we have :   $$\frac{1}{s}+\frac{1}{t}+\frac{1}{u}+\frac{-3}{\frac{p+r+q}{2}-s-t-u}\geq \frac{1}{p}+\frac{1}{q}+\frac{1}{r} $$ With the condition :   $$\frac{s}{p}+\frac{t}{q}+\frac{u}{r}=1 $$ My geometric try : We know this : Let ABC be a triangle, and let P, Q, R be any points in the plane distinct from A; B; C; respectively and suppose the cevians AP; BQ; CR meet at T then we have : $$\frac{TQ}{AQ}+\frac{TP}{BP}+\frac{TR}{CR}=1 $$ So it's a geometric interpretation of our condition . Now put the following substitution : $\frac{1}{s}=a$$\quad$$\frac{1}{t}=b$ $\frac{1}{u}=c$$\quad$$\frac{1}{p}=x$ $\frac{1}{q}=y$$\quad$$\frac{1}{r}=z$ We get : $$a+b+c+\frac{-3}{\frac{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}{2}-\frac{1}{x}-\frac{1}{y}-\frac{1}{z}}\geq x+y+z $$ Furthermore we have for an interior point (in $ABC$) $P$, the Barrow's inequality .Finally we remark that the inequality above seems to have the behavior of the Barrow's inequality . Edit : As point out in the comment of Doyun Nam I add the implicit condition $p>q+r$$\quad$$q>p+r$$\quad$$r>p+r$.Thanks to him. After that I have no more idea...Thanks a lot.","I have a solid and difficult problem that I can't solve, this is the following : Let $p,q,r,s,t,u$ be real positive numbers then we have :   $$\frac{1}{s}+\frac{1}{t}+\frac{1}{u}+\frac{-3}{\frac{p+r+q}{2}-s-t-u}\geq \frac{1}{p}+\frac{1}{q}+\frac{1}{r} $$ With the condition :   $$\frac{s}{p}+\frac{t}{q}+\frac{u}{r}=1 $$ My geometric try : We know this : Let ABC be a triangle, and let P, Q, R be any points in the plane distinct from A; B; C; respectively and suppose the cevians AP; BQ; CR meet at T then we have : $$\frac{TQ}{AQ}+\frac{TP}{BP}+\frac{TR}{CR}=1 $$ So it's a geometric interpretation of our condition . Now put the following substitution : $\frac{1}{s}=a$$\quad$$\frac{1}{t}=b$ $\frac{1}{u}=c$$\quad$$\frac{1}{p}=x$ $\frac{1}{q}=y$$\quad$$\frac{1}{r}=z$ We get : $$a+b+c+\frac{-3}{\frac{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}{2}-\frac{1}{x}-\frac{1}{y}-\frac{1}{z}}\geq x+y+z $$ Furthermore we have for an interior point (in $ABC$) $P$, the Barrow's inequality .Finally we remark that the inequality above seems to have the behavior of the Barrow's inequality . Edit : As point out in the comment of Doyun Nam I add the implicit condition $p>q+r$$\quad$$q>p+r$$\quad$$r>p+r$.Thanks to him. After that I have no more idea...Thanks a lot.",,['real-analysis']
67,Brutal gaussian integral of death $\int_{\mathbb{R}} x \Phi(x) \phi(Bx-b)$,Brutal gaussian integral of death,\int_{\mathbb{R}} x \Phi(x) \phi(Bx-b),"Ciao, I was making some computation and I've been stucked in this one. Let $B$ and $b$ be positive contant. We call $\phi(x)$ standard gaussian distribution and $\Phi(x)$ its cumulative function, i.e. $$ \phi(x) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{x^2}{2}} $$ $$ \Phi(x) = \int_{-\infty}^x \phi(s) ds $$ then compute $$ \int_{-\infty}^{+\infty}x\Phi(x)\phi(Bx - b) dx $$ If it helps I can proove this result: $$ \int_{-\infty}^{+\infty}x\Phi(x)\phi(x) dx = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} e^{-x^2}= \frac{1}{\sqrt{2}} $$ Any suggestion or hint will be appreciated, thank you! Ciao AM","Ciao, I was making some computation and I've been stucked in this one. Let $B$ and $b$ be positive contant. We call $\phi(x)$ standard gaussian distribution and $\Phi(x)$ its cumulative function, i.e. $$ \phi(x) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{x^2}{2}} $$ $$ \Phi(x) = \int_{-\infty}^x \phi(s) ds $$ then compute $$ \int_{-\infty}^{+\infty}x\Phi(x)\phi(Bx - b) dx $$ If it helps I can proove this result: $$ \int_{-\infty}^{+\infty}x\Phi(x)\phi(x) dx = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} e^{-x^2}= \frac{1}{\sqrt{2}} $$ Any suggestion or hint will be appreciated, thank you! Ciao AM",,"['calculus', 'real-analysis', 'integration', 'provability']"
68,Sup of measure of intersection points,Sup of measure of intersection points,,"This is a problem I've thought up myself while travelling in a car, and I have no idea how to solve it. Let $f: [0,1] \to [0,1]$ be bijection. Consider unit square $[0,1] \times [0,1]$, and label one edge $x$ and the opposite edge $f(x)$. Connect all points $x$ in the $x$ edge to their corresponding points $f(x)$ in the $f(x)$ edge with straight lines. For any function $f(x)$ as described before, let $A(f)$ be the set of points where two or more lines intersect. The question is this. If $\mu$ is the Lebesgue measure, when taken for all bijections $f(x)$ for which $\mu(A(f))$ exists,  what is $\sup \mu(A(f))$? Clearly, considering $$f(x) = \begin{cases}        x + 1/2 & 0< x < 1/2 \\       x - 1/2 & x \geq 1/2\\       1 & x = 0    \end{cases} $$ The square is illustrated in the image below from which we get that $$ \mu(A(f)) = 1/4$$ and so  $$\frac{1}{4}\leq \sup \mu(A(f))\leq 1$$ The illustration below shows what the square with the lines looks like for $f(x) = 1-x$. In this case, $A(f)$ is clearly $\{(1/2,1/2)\}$ and so $\mu(A(f)) = 0$","This is a problem I've thought up myself while travelling in a car, and I have no idea how to solve it. Let $f: [0,1] \to [0,1]$ be bijection. Consider unit square $[0,1] \times [0,1]$, and label one edge $x$ and the opposite edge $f(x)$. Connect all points $x$ in the $x$ edge to their corresponding points $f(x)$ in the $f(x)$ edge with straight lines. For any function $f(x)$ as described before, let $A(f)$ be the set of points where two or more lines intersect. The question is this. If $\mu$ is the Lebesgue measure, when taken for all bijections $f(x)$ for which $\mu(A(f))$ exists,  what is $\sup \mu(A(f))$? Clearly, considering $$f(x) = \begin{cases}        x + 1/2 & 0< x < 1/2 \\       x - 1/2 & x \geq 1/2\\       1 & x = 0    \end{cases} $$ The square is illustrated in the image below from which we get that $$ \mu(A(f)) = 1/4$$ and so  $$\frac{1}{4}\leq \sup \mu(A(f))\leq 1$$ The illustration below shows what the square with the lines looks like for $f(x) = 1-x$. In this case, $A(f)$ is clearly $\{(1/2,1/2)\}$ and so $\mu(A(f)) = 0$",,"['real-analysis', 'general-topology', 'geometry', 'measure-theory', 'lebesgue-measure']"
69,Is the set of sum over a diagram is uncountable set?,Is the set of sum over a diagram is uncountable set?,,"Consider the following diagram: Let $0<x<\frac{1}{2}$ Note that $[.]$ is not box function It is easy to see that $1$ split into $x$ and $(1-x)$ . In next stage $x$ split into $x^2$ and $x(1-x)$ and $(1-x)$ split into $x(1-x)$ and $(1-x)^2$ . Note that I do not write $x(1-x)$ twice, instead of that in this stage we have three values $x^2,x(1-x),(1-x)^2$ . Continue this process in the next stage (see the diagram for more detail) Now comes the important part: Add take $1$ first. Then add either $x$ or $(1-x)$ . If you added $x$ then next add either $x^2$ or $x(1-x)$ and if you added $(1-x)$ then up next add either $x(1-x)$ or $x^2$ . Continue this process again. For example you will get a value $1+\sum_{n=1}^{\infty}x^n$ . Another one could be $1+\sum_{n=1}^{\infty}x(1-x)^{n-1}$ . It depends on which path you choose. Every zigzag path will give you a $\color{red}{\text{different sums}}$ . $\color{red}{\text{different sums}}$ means sum over different zigzag path. Note that $\color{red}{\text{different sums}}$ does not mean two different path always give different values. $\large{F}$ or example $1+(1-x)\sum_{n=0}^{\infty}x^n$ and $1+x\sum_{n=0}^{\infty}(1-x)^n$ are two $\color{red}{\text{different sums}}$ but they yeilds same value $2$ i.e. $1+(1-x)\sum_{n=0}^{\infty}x^n=1+x\sum_{n=0}^{\infty}(1-x)^n=2$ Let say $T_x=\{\text{set of all $\color{red}{\text{different sums}}$ for a given $x$ }\}$ It can be easily seen there is uncountable number of $\color{red}{\text{different sums}}$ . Question : is there uncountable number of distinct elements in $T_x$ for a given $x$ ? Observation: It can be easily seen that minimum element of $T_x$ is $\frac{1}{1-x}$ and maximum element of $T_x$ is $\frac{1}{x}$ . $2\in T_x$ for every $x\in \Big(0,\frac{1}{2}\Big)$ But I could not find a way to prove or disprove uncountability. Any help would be appreciable. $\blacksquare\space\space\blacksquare\space\space\blacksquare\space\space$ Thanks to MANMAID I am attaching a new diagram: Let $0<x<1$","Consider the following diagram: Let Note that is not box function It is easy to see that split into and . In next stage split into and and split into and . Note that I do not write twice, instead of that in this stage we have three values . Continue this process in the next stage (see the diagram for more detail) Now comes the important part: Add take first. Then add either or . If you added then next add either or and if you added then up next add either or . Continue this process again. For example you will get a value . Another one could be . It depends on which path you choose. Every zigzag path will give you a . means sum over different zigzag path. Note that does not mean two different path always give different values. or example and are two but they yeilds same value i.e. Let say It can be easily seen there is uncountable number of . Question : is there uncountable number of distinct elements in for a given ? Observation: It can be easily seen that minimum element of is and maximum element of is . for every But I could not find a way to prove or disprove uncountability. Any help would be appreciable. Thanks to MANMAID I am attaching a new diagram: Let","0<x<\frac{1}{2} [.] 1 x (1-x) x x^2 x(1-x) (1-x) x(1-x) (1-x)^2 x(1-x) x^2,x(1-x),(1-x)^2 1 x (1-x) x x^2 x(1-x) (1-x) x(1-x) x^2 1+\sum_{n=1}^{\infty}x^n 1+\sum_{n=1}^{\infty}x(1-x)^{n-1} \color{red}{\text{different sums}} \color{red}{\text{different sums}} \color{red}{\text{different sums}} \large{F} 1+(1-x)\sum_{n=0}^{\infty}x^n 1+x\sum_{n=0}^{\infty}(1-x)^n \color{red}{\text{different sums}} 2 1+(1-x)\sum_{n=0}^{\infty}x^n=1+x\sum_{n=0}^{\infty}(1-x)^n=2 T_x=\{\text{set of all \color{red}{\text{different sums}} for a given x }\} \color{red}{\text{different sums}} T_x x T_x \frac{1}{1-x} T_x \frac{1}{x} 2\in T_x x\in \Big(0,\frac{1}{2}\Big) \blacksquare\space\space\blacksquare\space\space\blacksquare\space\space 0<x<1","['real-analysis', 'sequences-and-series', 'problem-solving']"
70,How to show that these functions are equivalent for $x \to 0$?,How to show that these functions are equivalent for ?,x \to 0,"Let $\{p_n(x)\}_{n=1}^{+\infty}$ be a set of functions such that for $\forall x \in \mathbb{R} \forall n \in \mathbb{N}: 0 < p_n(x) < 1$ and $\sum\limits_{n=1}^{+\infty} p_n(x) = x$. I wonder if it is always true that $1 - \prod\limits_{n=1}^{+\infty} (1-p_n(x)) \sim x$ as $x$ goes to $0$? So can someone prove the equivalence in general or provide a counterexample? If a counterexample exists , I'm interested in finding $\{p_n(x)\}_{n=1}^{+\infty}$ that minimizes the following limit: $$\lim\limits_{x \to 0}\dfrac{1- \prod\limits_{n=1}^{+\infty} (1-p_n(x))}{x}$$ If the limit above can be equal to $0$ for some $\{p_n(x)\}_{n=1}^{+\infty}$, I would be interested to see any example of such sequence. Any ideas, suggestions, hints and references related to the problem would be greatly appreciated.","Let $\{p_n(x)\}_{n=1}^{+\infty}$ be a set of functions such that for $\forall x \in \mathbb{R} \forall n \in \mathbb{N}: 0 < p_n(x) < 1$ and $\sum\limits_{n=1}^{+\infty} p_n(x) = x$. I wonder if it is always true that $1 - \prod\limits_{n=1}^{+\infty} (1-p_n(x)) \sim x$ as $x$ goes to $0$? So can someone prove the equivalence in general or provide a counterexample? If a counterexample exists , I'm interested in finding $\{p_n(x)\}_{n=1}^{+\infty}$ that minimizes the following limit: $$\lim\limits_{x \to 0}\dfrac{1- \prod\limits_{n=1}^{+\infty} (1-p_n(x))}{x}$$ If the limit above can be equal to $0$ for some $\{p_n(x)\}_{n=1}^{+\infty}$, I would be interested to see any example of such sequence. Any ideas, suggestions, hints and references related to the problem would be greatly appreciated.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'products']"
71,Limit of an indicator function?,Limit of an indicator function?,,"I'm having some problem in the computation of limits of indicator functions.  Why the following limit takes values $0$? $$\lim_{n\to\infty}1_{[n,n+1]}$$ Which are the steps I should follow to compute every kind of limit of such functions? I started thinking about applying the definition of limit as the x-values go to infinity, but at some point I get really confused because of the set of the indicator function... Does it work like the domain for other functions? How does this work for limsup and liminf of an indicator function? Thank you!","I'm having some problem in the computation of limits of indicator functions.  Why the following limit takes values $0$? $$\lim_{n\to\infty}1_{[n,n+1]}$$ Which are the steps I should follow to compute every kind of limit of such functions? I started thinking about applying the definition of limit as the x-values go to infinity, but at some point I get really confused because of the set of the indicator function... Does it work like the domain for other functions? How does this work for limsup and liminf of an indicator function? Thank you!",,"['real-analysis', 'limits', 'characteristic-functions']"
72,Possible alternate proof of uniqueness of power series?,Possible alternate proof of uniqueness of power series?,,"I want to show that if $\sum_{k=0}^\infty a_kx^k = 0$ on $[0,1]$, then $a_k=0 \forall k\in\mathbb{N}$. I'm aware of the standard proof, but wanted to try another argument. We know that a polynomial of degree $k$ has at most k roots, so the polynomial $\sum_{k=0}^N a_kx^k$ has at most $N$ roots. Now if we let $N\rightarrow \infty$ we know that we can have at most ""$\aleph_0$ roots"", but for the function to be identically zero we must have uncountably many roots. I know that this argument is rubbish, but is there a way (by transfinite induction/Baire Category or something else) to make this idea rigorous?","I want to show that if $\sum_{k=0}^\infty a_kx^k = 0$ on $[0,1]$, then $a_k=0 \forall k\in\mathbb{N}$. I'm aware of the standard proof, but wanted to try another argument. We know that a polynomial of degree $k$ has at most k roots, so the polynomial $\sum_{k=0}^N a_kx^k$ has at most $N$ roots. Now if we let $N\rightarrow \infty$ we know that we can have at most ""$\aleph_0$ roots"", but for the function to be identically zero we must have uncountably many roots. I know that this argument is rubbish, but is there a way (by transfinite induction/Baire Category or something else) to make this idea rigorous?",,['real-analysis']
73,Is a function whose derivative vanishes at rationals constant? [duplicate],Is a function whose derivative vanishes at rationals constant? [duplicate],,"This question already has an answer here : Let $f:\mathbb{R}\longrightarrow \mathbb{R}$ a differentiable function such that $f'(x)=0$ for all $x\in\mathbb{Q}$ [closed] (1 answer) Closed 7 years ago . I'm trying to make a problem for my advanced calculus students.  I was thinking, if we have a differentiable function $f:\mathbb{R}\to\mathbb{R}$ such that $f'(q)=0$ for all $q\in\mathbb{Q}$, can we say that $f$ is constant?","This question already has an answer here : Let $f:\mathbb{R}\longrightarrow \mathbb{R}$ a differentiable function such that $f'(x)=0$ for all $x\in\mathbb{Q}$ [closed] (1 answer) Closed 7 years ago . I'm trying to make a problem for my advanced calculus students.  I was thinking, if we have a differentiable function $f:\mathbb{R}\to\mathbb{R}$ such that $f'(q)=0$ for all $q\in\mathbb{Q}$, can we say that $f$ is constant?",,"['calculus', 'real-analysis', 'derivatives']"
74,Intuition behind convolution,Intuition behind convolution,,"I always wondered about the idea behind convolution. I get what the definition of the convolution does (and I saw all the animations), but what I don't understand is how it relates to so many topics in physics. It seems to me that it is not really an intuitive concept. I guess my question is - what were the ideas, thoughts of the guy that discovered convolution? About two years ago I read a blog post about convolution that explains it pretty intuitively, but I forgot the name of the website.","I always wondered about the idea behind convolution. I get what the definition of the convolution does (and I saw all the animations), but what I don't understand is how it relates to so many topics in physics. It seems to me that it is not really an intuitive concept. I guess my question is - what were the ideas, thoughts of the guy that discovered convolution? About two years ago I read a blog post about convolution that explains it pretty intuitively, but I forgot the name of the website.",,"['real-analysis', 'intuition', 'convolution']"
75,An application $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ and $C^{1}$ such that $f(x)=0$ for $x>r$ implies the value of jacobian integral is zero,An application  and  such that  for  implies the value of jacobian integral is zero,f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n} C^{1} f(x)=0 x>r,"Let $f \colon \mathbb{R}^n \to \mathbb{R}^n$ of class $C^{1}$. Suppose that exists $r>0$ such that $f(x)=0$ if $|x|\geq r$ .Prove that exists $k>0$ such that: $\displaystyle \int_{B[0,k]}$ det$Jf(x)=0$ I have yet to see Stokes theorem which i think is related to this question and i'm trying to adapt the proof of ""change of variable"" theorem without much success,any hint is appreciated.","Let $f \colon \mathbb{R}^n \to \mathbb{R}^n$ of class $C^{1}$. Suppose that exists $r>0$ such that $f(x)=0$ if $|x|\geq r$ .Prove that exists $k>0$ such that: $\displaystyle \int_{B[0,k]}$ det$Jf(x)=0$ I have yet to see Stokes theorem which i think is related to this question and i'm trying to adapt the proof of ""change of variable"" theorem without much success,any hint is appreciated.",,"['real-analysis', 'integration', 'analysis', 'multivariable-calculus']"
76,An intuitive way to understand the Jacobi's formula.,An intuitive way to understand the Jacobi's formula.,,"Suppose that $\mathbf A=\mathbf A(t)$ is a matrix whose entries are parametrized by a variable $t$. The Jacobi's formular states that $$ \frac d{dt}\left( \det \mathbf A\right)= \text{Tr}\left( \text{adj} (\mathbf A ) \frac{d\mathbf A}{dt} \right)\ , $$ where $\text{adj}(\mathbf A)$ is the adjugate (or adjoint) of $\mathbf A$. There is a proof of that in this wikipedia article . Another closely related identity, which appears to go by the same name, is $$ \det\left( e^{\mathbf A} \right) = e^{\text{Tr}(\mathbf A)}\ . $$ I want to understand the reason why these formulas are true, especially the second one. In particularly, what is the interpretations of $\det\left( e^{\mathbf A} \right)$ and $\text{Tr}(\mathbf A)$, when $\mathbf A$ is considered as a linear operator? These are some related links that I have found about the question: Derivative of a determinant of a matrix field Proof for the derivative of the determinant of a matrix Exponential of a matrix and related derivative","Suppose that $\mathbf A=\mathbf A(t)$ is a matrix whose entries are parametrized by a variable $t$. The Jacobi's formular states that $$ \frac d{dt}\left( \det \mathbf A\right)= \text{Tr}\left( \text{adj} (\mathbf A ) \frac{d\mathbf A}{dt} \right)\ , $$ where $\text{adj}(\mathbf A)$ is the adjugate (or adjoint) of $\mathbf A$. There is a proof of that in this wikipedia article . Another closely related identity, which appears to go by the same name, is $$ \det\left( e^{\mathbf A} \right) = e^{\text{Tr}(\mathbf A)}\ . $$ I want to understand the reason why these formulas are true, especially the second one. In particularly, what is the interpretations of $\det\left( e^{\mathbf A} \right)$ and $\text{Tr}(\mathbf A)$, when $\mathbf A$ is considered as a linear operator? These are some related links that I have found about the question: Derivative of a determinant of a matrix field Proof for the derivative of the determinant of a matrix Exponential of a matrix and related derivative",,"['calculus', 'real-analysis', 'linear-algebra', 'matrices', 'lie-groups']"
77,Find a Continuous Function with Cantor Set Level Sets,Find a Continuous Function with Cantor Set Level Sets,,"This was a problem from a class that I thought was really interesting. It asked to find function $f\in C[0,1]$ such that the sets $\{x:f(x)=c\}$ form a Cantor Set for all $0\leq c\leq 1$. I found a non-constructive proof of the existence of such functions, but would be curious if anyone could give a constructive example of such a function. Definition: A set is a Cantor Set if it is uncountable, perfect, compact, nowhere-dense set. This makes it homeomorphic to ""the Cantor set.""","This was a problem from a class that I thought was really interesting. It asked to find function $f\in C[0,1]$ such that the sets $\{x:f(x)=c\}$ form a Cantor Set for all $0\leq c\leq 1$. I found a non-constructive proof of the existence of such functions, but would be curious if anyone could give a constructive example of such a function. Definition: A set is a Cantor Set if it is uncountable, perfect, compact, nowhere-dense set. This makes it homeomorphic to ""the Cantor set.""",,"['real-analysis', 'general-topology', 'analysis', 'cantor-set']"
78,Does pointwise convergence implies uniform convergence when the limit is continous?,Does pointwise convergence implies uniform convergence when the limit is continous?,,"Suppose we have a series of functions $f_n: \mathbb R \rightarrow [0,1]$ and continous function $f: \mathbb R \rightarrow [0,1]$. Suppose that $f_n \rightarrow f$ pointwise as $n \rightarrow \infty$. Is it true, that $f_n$ converges uniformly also? What is the case if all $f_n$ and $f$ are monotone functions? Edit1 : Consider the case when $f_n$ and $f$ are distribution functions. All are monoton, continous functions with $$\lim_{x\to -\infty}f(x)=0$$ and $$\lim_{x\to\infty}f(x)=1.$$ There are numerous question on the site already, the most relevant is this: Does pointwise convergence against a continuous function imply uniform convergence? In the marked answer, there are two counterexamples, but I think both example series of functions converge to a $g(x)=\delta(x)$, which is not continous . Am I right? If yes, how the original statement could be proved? I am also aware of Dini's Theorem, but that applies only to function on closed intervals.","Suppose we have a series of functions $f_n: \mathbb R \rightarrow [0,1]$ and continous function $f: \mathbb R \rightarrow [0,1]$. Suppose that $f_n \rightarrow f$ pointwise as $n \rightarrow \infty$. Is it true, that $f_n$ converges uniformly also? What is the case if all $f_n$ and $f$ are monotone functions? Edit1 : Consider the case when $f_n$ and $f$ are distribution functions. All are monoton, continous functions with $$\lim_{x\to -\infty}f(x)=0$$ and $$\lim_{x\to\infty}f(x)=1.$$ There are numerous question on the site already, the most relevant is this: Does pointwise convergence against a continuous function imply uniform convergence? In the marked answer, there are two counterexamples, but I think both example series of functions converge to a $g(x)=\delta(x)$, which is not continous . Am I right? If yes, how the original statement could be proved? I am also aware of Dini's Theorem, but that applies only to function on closed intervals.",,"['real-analysis', 'probability-theory', 'probability-distributions', 'uniform-convergence']"
79,Let $f: \Bbb R \to \Bbb R$ be a differentiable function such that $\sup_{x \in \Bbb R}|f'(x)| \lt \infty$. Then,Let  be a differentiable function such that . Then,f: \Bbb R \to \Bbb R \sup_{x \in \Bbb R}|f'(x)| \lt \infty,"(UGC CSIR-2015, DECEMEMBER, MATHEMATICAL SCIENCES) $f$ maps a bounded sequence to a bounded sequence. $f$ maps a Cauchy sequence to a Cauchy sequence. $f$ maps a convergent sequence to a convergent sequence. $f$ is uniformly continuous. I choose all of the options as possible answers because the condition $\sup_{x\in \Bbb R}|f'(x)| \lt \infty$ forces $f$ to be uniformly continuous.(Because $f$ becomes Lipschitz and Lipschitz condition implies uniform continuity) i.e. $\frac {|f(x)-f(y)|}{|x-y|} \le \sup_{x\in \Bbb R}|f'(x)|$ $ \forall x,y$. Hence all other options are bound to be true. Am I correct?","(UGC CSIR-2015, DECEMEMBER, MATHEMATICAL SCIENCES) $f$ maps a bounded sequence to a bounded sequence. $f$ maps a Cauchy sequence to a Cauchy sequence. $f$ maps a convergent sequence to a convergent sequence. $f$ is uniformly continuous. I choose all of the options as possible answers because the condition $\sup_{x\in \Bbb R}|f'(x)| \lt \infty$ forces $f$ to be uniformly continuous.(Because $f$ becomes Lipschitz and Lipschitz condition implies uniform continuity) i.e. $\frac {|f(x)-f(y)|}{|x-y|} \le \sup_{x\in \Bbb R}|f'(x)|$ $ \forall x,y$. Hence all other options are bound to be true. Am I correct?",,"['real-analysis', 'functions', 'convergence-divergence', 'uniform-continuity', 'cauchy-sequences']"
80,"A discontinuous function $f: X \rightarrow Y$ satisfying: for each closed ball $B$ of $Y, f^{-1}(B)$ is closed in $X$",A discontinuous function  satisfying: for each closed ball  of  is closed in,"f: X \rightarrow Y B Y, f^{-1}(B) X","Find a function $f: X \rightarrow Y$ between metric spaces $X$ and $Y$ that is not continuous but has the property that for each closed ball $B$ of $Y, f^{-1}(B)$ is closed in $X$ Solution Attempt: A continuous function $f : X \rightarrow Y$ is defined as : For every open set $V$ in $Y$, there's an open set $U \in X$ such that $f(U) \subseteq V$. Strategy: The issue with closed sets while defining continuity can be that a closed set $U_o \in X $ can also contain limit points which may not get mapped to the closed set $V_o$. EDIT: Let $f: X \rightarrow Y$ be a an identity function where $X$ is a non-discrete metric space and $Y$ is a metric space with the discrete metric. (As far as it's set elements were concerned, $X$=$Y$..)So, $f(x) = x ~\forall x \in X$. $X$ is non discrete metric space $\implies$ there is atleast one subset of $X$ which is closed but not open in $X$. Could you please explain how these arguments tell us that $f^{-1}(x_{|Y}) = x_{|X}$  is closed in $X$? . Thank you!","Find a function $f: X \rightarrow Y$ between metric spaces $X$ and $Y$ that is not continuous but has the property that for each closed ball $B$ of $Y, f^{-1}(B)$ is closed in $X$ Solution Attempt: A continuous function $f : X \rightarrow Y$ is defined as : For every open set $V$ in $Y$, there's an open set $U \in X$ such that $f(U) \subseteq V$. Strategy: The issue with closed sets while defining continuity can be that a closed set $U_o \in X $ can also contain limit points which may not get mapped to the closed set $V_o$. EDIT: Let $f: X \rightarrow Y$ be a an identity function where $X$ is a non-discrete metric space and $Y$ is a metric space with the discrete metric. (As far as it's set elements were concerned, $X$=$Y$..)So, $f(x) = x ~\forall x \in X$. $X$ is non discrete metric space $\implies$ there is atleast one subset of $X$ which is closed but not open in $X$. Could you please explain how these arguments tell us that $f^{-1}(x_{|Y}) = x_{|X}$  is closed in $X$? . Thank you!",,"['real-analysis', 'metric-spaces', 'continuity', 'advice']"
81,Definite integral with logarithm and arctangent inside of arctangent,Definite integral with logarithm and arctangent inside of arctangent,,"How to prove $$\int_0^1 \left[ \frac{2}{\pi }\arctan \left(\frac 2 \pi \arctan \frac{1}{x} + \frac{1}{\pi }\ln \frac{1 + x}{1 - x}\right) - \frac{1}{2} \right]\frac{\mathrm{d}x} x = \frac{1}{2} \ln \left( \frac \pi {2\sqrt 2 } \right).$$ I have tried let $t=\frac1x$, but it seems no use! Could you help me to solve it?","How to prove $$\int_0^1 \left[ \frac{2}{\pi }\arctan \left(\frac 2 \pi \arctan \frac{1}{x} + \frac{1}{\pi }\ln \frac{1 + x}{1 - x}\right) - \frac{1}{2} \right]\frac{\mathrm{d}x} x = \frac{1}{2} \ln \left( \frac \pi {2\sqrt 2 } \right).$$ I have tried let $t=\frac1x$, but it seems no use! Could you help me to solve it?",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
82,Is this proof sufficient to show that a concave function of a metric is also a metric?,Is this proof sufficient to show that a concave function of a metric is also a metric?,,"Given a function $f: [0, \infty) \rightarrow [0, \infty)$ that's concave with $f(0) = 0$ and $f(x) > 0, \forall x \in [0, \infty)$, so $f(tx + (1-t)y) \geq tf(x) + (1-t)f(y)$, how do I show that if $d(x, y)$ is a metric on the metric space $M$, then $f(d(x, y))$ is also a metric for $M$? I know that since $d$ is a metric, $d(x, y) \leq d(x, z) + d(z, y)$ by the triangle inequality. Is it enough to use the triangle inequality like this: \begin{align*} d(x, y) &\leq d(x, z) + d(z, y) \\ \Rightarrow f(d(x, y)) & \leq f(d(x, z) + d(z, y)) \\ &\leq f(d(x, z)) + f(d(z, y)) \end{align*} because concave functions are subadditive (which I can prove). Is that enough? The implication in the second step assumes that the function is increasing, or at least not decreasing (I'm not sure how to prove that), but is that really all I have to do? This question discusses how this relates to subadditivity, but also makes the assumption of the function being increasing. From the given information, I don't think I have that assumption.","Given a function $f: [0, \infty) \rightarrow [0, \infty)$ that's concave with $f(0) = 0$ and $f(x) > 0, \forall x \in [0, \infty)$, so $f(tx + (1-t)y) \geq tf(x) + (1-t)f(y)$, how do I show that if $d(x, y)$ is a metric on the metric space $M$, then $f(d(x, y))$ is also a metric for $M$? I know that since $d$ is a metric, $d(x, y) \leq d(x, z) + d(z, y)$ by the triangle inequality. Is it enough to use the triangle inequality like this: \begin{align*} d(x, y) &\leq d(x, z) + d(z, y) \\ \Rightarrow f(d(x, y)) & \leq f(d(x, z) + d(z, y)) \\ &\leq f(d(x, z)) + f(d(z, y)) \end{align*} because concave functions are subadditive (which I can prove). Is that enough? The implication in the second step assumes that the function is increasing, or at least not decreasing (I'm not sure how to prove that), but is that really all I have to do? This question discusses how this relates to subadditivity, but also makes the assumption of the function being increasing. From the given information, I don't think I have that assumption.",,"['real-analysis', 'functions', 'metric-spaces']"
83,Example of continuous function whose Fourier series doesn't converge on an uncountable dense set.,Example of continuous function whose Fourier series doesn't converge on an uncountable dense set.,,"According to a well-known theorem (Theorem 5.12 in Rudin's Real and Complex Analysis ), there is a dense $G_\delta$ set of continuous periodic functions $f:\mathbb{R}\to\mathbb{C}$ such that the Fourier series of $f$ does not converge for all $x$ in a dense $G_\delta$ subset of $\mathbb{R}$ (hence uncountable). This is completely mind blowing, especially the fact there is a whole dense $G_\delta$ set of such pathological functions. But is there at least one known explicit example of such a function?","According to a well-known theorem (Theorem 5.12 in Rudin's Real and Complex Analysis ), there is a dense $G_\delta$ set of continuous periodic functions $f:\mathbb{R}\to\mathbb{C}$ such that the Fourier series of $f$ does not converge for all $x$ in a dense $G_\delta$ subset of $\mathbb{R}$ (hence uncountable). This is completely mind blowing, especially the fact there is a whole dense $G_\delta$ set of such pathological functions. But is there at least one known explicit example of such a function?",,"['real-analysis', 'fourier-series', 'examples-counterexamples']"
84,Understanding the Proof of the Arzela-Ascoli Theorem from Carothers,Understanding the Proof of the Arzela-Ascoli Theorem from Carothers,,"The below is the proof for the Arzela-Ascoli Theorem from Carothers' Real Analysis . I had a few questions regarding some steps in his proof which I have put in blue. If anyone could explain the blue lines, it would be appreciated. Some definitions that I recently learned: Uniformly bounded means $\sup_{f\in\mathcal{F}}||f||_{\infty}<\infty$ Equicontinuous means $\forall \epsilon>0 \exists \delta:d(x,y)<\delta\implies |f(x)-f(y)|<\epsilon \text{ for all } f\in\mathcal{F}$ Theorem (Arzela-Ascoli) Let $X$ be a compact metric space, and let $\mathcal{F}\subset C(X)$. Then $\mathcal{F}$ compact if and only if  $\mathcal{F}$ is closed, uniformly bounded, and equicontinuous. (Note that $\mathcal{F}$ is a collection of continuous real-valued functions on $X$, and  $C(X)$ is the space of all continuous functions $f:X\to\mathbb{R}$) Proof. (I understand the $\implies$ direction, so I omit that part of the proof.) For $\impliedby$: Suppose $\mathcal{F}$ is closed, uniformly bounded, and equicontinuous and let $(f_n)\in\mathcal{F}.$ $\color{blue}{\text{We need to show that $(f_n)$ has a uniformly convergent subsequence.}}$ $\color{blue}{\text{First note that $(f_n)$ is equicontinuous.}}$ Thus given $\epsilon>0, \exists \delta >0: d(x,y)<\delta\implies |f_n(x)-f_n(y)|<\epsilon/3, \forall n$. Next since $X$ is totally bounded, $X$ has a finite $\delta$-net, i.e, there exists $x_1,\ldots, x_k\in X$ such that each $x\in X$ satisfies $d(x,x_i)<\delta$. $\color{blue}{\text{Now since $(f_n)$ is also uniformly bounded,}}$ each of the sequences $(f_n(x_i))_{n=1}^{\infty}$ is bounded in $\mathbb{R}$ for $i=1,2,\ldots, k$. $\color{blue}{\text{Thus by passing  to a subsequence of the $f_n$ (and relabeling), we may suppose that $(f_n(x_i))_{n=1}^{\infty}$ converges for each $i=1,2,\ldots, k$}}$. In particular, we can find some $N$ such that $|f_m(x_i)-f_n(x_i)|<\epsilon/3$ for any $m,n\geq N$ and any $i=1,2,\ldots, k$. And now we are done! Given $x\in X$, first find $i$ such that $d(x,x_i)<\delta$ and then whenever $m,n\geq N$, we will have  \begin{align*} &|f_m(x)-f_n(x)|\\ &\leq|f_m(x)-f_m(x_i)|+|f_m(x_i)-f_n(x_i)|+|f_n(x_i)-f_n(x)|\\ &<\epsilon/3+\epsilon/3+\epsilon/3\\ &=\epsilon \end{align*} That is, $f_n$ is uniformy Cauchy, since our choice of $N$ does not depend on $x$. Since $\mathcal{F}$ is closed in $C(X)$ by assumption, it follows that $f_n(x)$ converges to some $f\in\mathcal{F}$, uniformly. $\blacksquare$ More explictly, my questions are: For the first blue line, why are we trying to show that $(f_n)$ has a uniformly convergent subsequence? For the second blue line, how do we know $(f_n)$ is equicontinuous? For the third blue line, how can we tell that $(f_n)$ is uniformly bounded? For the fourth blue line, can someone elaborate on what Carothers is trying to do here? I'm having trouble understanding what is occurring at this step. Thanks.","The below is the proof for the Arzela-Ascoli Theorem from Carothers' Real Analysis . I had a few questions regarding some steps in his proof which I have put in blue. If anyone could explain the blue lines, it would be appreciated. Some definitions that I recently learned: Uniformly bounded means $\sup_{f\in\mathcal{F}}||f||_{\infty}<\infty$ Equicontinuous means $\forall \epsilon>0 \exists \delta:d(x,y)<\delta\implies |f(x)-f(y)|<\epsilon \text{ for all } f\in\mathcal{F}$ Theorem (Arzela-Ascoli) Let $X$ be a compact metric space, and let $\mathcal{F}\subset C(X)$. Then $\mathcal{F}$ compact if and only if  $\mathcal{F}$ is closed, uniformly bounded, and equicontinuous. (Note that $\mathcal{F}$ is a collection of continuous real-valued functions on $X$, and  $C(X)$ is the space of all continuous functions $f:X\to\mathbb{R}$) Proof. (I understand the $\implies$ direction, so I omit that part of the proof.) For $\impliedby$: Suppose $\mathcal{F}$ is closed, uniformly bounded, and equicontinuous and let $(f_n)\in\mathcal{F}.$ $\color{blue}{\text{We need to show that $(f_n)$ has a uniformly convergent subsequence.}}$ $\color{blue}{\text{First note that $(f_n)$ is equicontinuous.}}$ Thus given $\epsilon>0, \exists \delta >0: d(x,y)<\delta\implies |f_n(x)-f_n(y)|<\epsilon/3, \forall n$. Next since $X$ is totally bounded, $X$ has a finite $\delta$-net, i.e, there exists $x_1,\ldots, x_k\in X$ such that each $x\in X$ satisfies $d(x,x_i)<\delta$. $\color{blue}{\text{Now since $(f_n)$ is also uniformly bounded,}}$ each of the sequences $(f_n(x_i))_{n=1}^{\infty}$ is bounded in $\mathbb{R}$ for $i=1,2,\ldots, k$. $\color{blue}{\text{Thus by passing  to a subsequence of the $f_n$ (and relabeling), we may suppose that $(f_n(x_i))_{n=1}^{\infty}$ converges for each $i=1,2,\ldots, k$}}$. In particular, we can find some $N$ such that $|f_m(x_i)-f_n(x_i)|<\epsilon/3$ for any $m,n\geq N$ and any $i=1,2,\ldots, k$. And now we are done! Given $x\in X$, first find $i$ such that $d(x,x_i)<\delta$ and then whenever $m,n\geq N$, we will have  \begin{align*} &|f_m(x)-f_n(x)|\\ &\leq|f_m(x)-f_m(x_i)|+|f_m(x_i)-f_n(x_i)|+|f_n(x_i)-f_n(x)|\\ &<\epsilon/3+\epsilon/3+\epsilon/3\\ &=\epsilon \end{align*} That is, $f_n$ is uniformy Cauchy, since our choice of $N$ does not depend on $x$. Since $\mathcal{F}$ is closed in $C(X)$ by assumption, it follows that $f_n(x)$ converges to some $f\in\mathcal{F}$, uniformly. $\blacksquare$ More explictly, my questions are: For the first blue line, why are we trying to show that $(f_n)$ has a uniformly convergent subsequence? For the second blue line, how do we know $(f_n)$ is equicontinuous? For the third blue line, how can we tell that $(f_n)$ is uniformly bounded? For the fourth blue line, can someone elaborate on what Carothers is trying to do here? I'm having trouble understanding what is occurring at this step. Thanks.",,"['real-analysis', 'functional-analysis', 'proof-explanation']"
85,First-term approximation for singular perturbation of ODE (with two turning points),First-term approximation for singular perturbation of ODE (with two turning points),,"I'm reading ""Introduction to Perturbation Methods"" by Mark Holmes, and I came across an exercise that I don't know how to approach. As I decided to independently read this book, I have no friends/classmates/teachers with whom I can discuss this subject. The exercise reads as follows (ex. 2.47): Consider the problem $$\epsilon y'' -(x-a)(x-b)y' - x(y-1) = 0, \text{ for } 0<x<1$$ with the boundary conditions $y(0)=-2$ and $y(1)=2$. The numerical solution in the case $a=1/4$, $b=3/4$ and $\epsilon = 10^{-4}$ is shown below. Based on this information, derive a first-term approximation of the solution for arbitrary $0<a<b<1$. I'll be more than happy if you just tell me/direct me on how to solve the problem for the particular case $a=1/4$, $b=3/4$. -EDIT- Looking at the graph and after skimming through a lot of books, I think there's an interior boundary layer at $x=1/4$ and another boundary layer at $x=1$. I found the outer solution, valid in $0 \leq x<1/4$, to be: $$\tag{1} y_{\text{out}}=1 - 3 \left( \frac{3}{3-4x} \right)^{\frac{3}{2}} \sqrt{1-4x}$$ I still have to find/guess the boundary layer thickness at $x=1/4$ and compute an inner solution. Then, it somehow seems that this boundary layer will ""connect"" the outer solution (1) to the solution $y(x)=1$, which will then be connected to the boundary layer at $x=1$. I'd appreciate any help/insight on how to proceed.","I'm reading ""Introduction to Perturbation Methods"" by Mark Holmes, and I came across an exercise that I don't know how to approach. As I decided to independently read this book, I have no friends/classmates/teachers with whom I can discuss this subject. The exercise reads as follows (ex. 2.47): Consider the problem $$\epsilon y'' -(x-a)(x-b)y' - x(y-1) = 0, \text{ for } 0<x<1$$ with the boundary conditions $y(0)=-2$ and $y(1)=2$. The numerical solution in the case $a=1/4$, $b=3/4$ and $\epsilon = 10^{-4}$ is shown below. Based on this information, derive a first-term approximation of the solution for arbitrary $0<a<b<1$. I'll be more than happy if you just tell me/direct me on how to solve the problem for the particular case $a=1/4$, $b=3/4$. -EDIT- Looking at the graph and after skimming through a lot of books, I think there's an interior boundary layer at $x=1/4$ and another boundary layer at $x=1$. I found the outer solution, valid in $0 \leq x<1/4$, to be: $$\tag{1} y_{\text{out}}=1 - 3 \left( \frac{3}{3-4x} \right)^{\frac{3}{2}} \sqrt{1-4x}$$ I still have to find/guess the boundary layer thickness at $x=1/4$ and compute an inner solution. Then, it somehow seems that this boundary layer will ""connect"" the outer solution (1) to the solution $y(x)=1$, which will then be connected to the boundary layer at $x=1$. I'd appreciate any help/insight on how to proceed.",,"['real-analysis', 'ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
86,Translate a vector field,Translate a vector field,,"Imagine that you have a vector field $A = \frac{A_0}{r} e_{\theta}$ in cylindrical coordinates, where $A_0 \in \mathbb{R}$. Now you translate your coordinate system in $e_x$ direction by $x \mapsto x + d$ for some constant $d \in \mathbb{R}$. What happens to the field then? I would say that $\frac{A_0}{r} \mapsto \frac{A_0}{\sqrt{(x-d)^2+y^2}}$, but what happens to the unit vector $e_{\theta}$? So the question is: How can we express this vector field in the shifted coordinate system? If anything is unclear, please let me know.","Imagine that you have a vector field $A = \frac{A_0}{r} e_{\theta}$ in cylindrical coordinates, where $A_0 \in \mathbb{R}$. Now you translate your coordinate system in $e_x$ direction by $x \mapsto x + d$ for some constant $d \in \mathbb{R}$. What happens to the field then? I would say that $\frac{A_0}{r} \mapsto \frac{A_0}{\sqrt{(x-d)^2+y^2}}$, but what happens to the unit vector $e_{\theta}$? So the question is: How can we express this vector field in the shifted coordinate system? If anything is unclear, please let me know.",,"['calculus', 'real-analysis']"
87,Separability of functions with compact support,Separability of functions with compact support,,"Let $X$ be a locally compact metric space which is also $\sigma$-compact. Let $C_{c}(X)$ be the continuous functions on $f$ from $X$ to $\mathbb{R}$ with compact support. Is $C_{c}(X)$ separable? My work so far: If $X$ is a compact metric space, then by Urysohn's Lemma and Stone-Weierstrass, the continuous functions $C(X)$ on $X$ are separable and hence the result follows as $C_{c}(X) = C(X)$. Suppose $X = \mathbb{R}$. Write $\mathbb{R} = \bigcup_{N  = 1}^{\infty}[-N, N]$. Let $f \in C_{c}(\mathbb{R})$. Then $f$ is supported on a compact set $K \subset [-N, N]$ for some $N$. Thus $C_{c}(\mathbb{R}) =\bigcup_{N = 1}^{\infty}C([-N, N])$. Each $C([-N, N])$ has a countable dense subset $\{\psi_{N, n}\}_{n = 1}^{\infty}$ and so $\bigcup_{N, n = 1}^{\infty}\{\psi_{N, n}\}$ is a countable dense subset of $C_{c}(\mathbb{R})$. In the general case, $X = \bigcup_{i = 1}^{\infty}X_{i}$ where each $X_{i}$ is compact and $X_{1} \subset X_{2} \subset \cdots$. Let $f \in C_{c}(X)$. Then $f$ is supported on a compact set $K = \bigcup_{i = 1}^{\infty}K \cap X_{i}$. Is it still true that $C_{c}(X) = \bigcup_{i = 1}^{\infty}C(X_{i})$?","Let $X$ be a locally compact metric space which is also $\sigma$-compact. Let $C_{c}(X)$ be the continuous functions on $f$ from $X$ to $\mathbb{R}$ with compact support. Is $C_{c}(X)$ separable? My work so far: If $X$ is a compact metric space, then by Urysohn's Lemma and Stone-Weierstrass, the continuous functions $C(X)$ on $X$ are separable and hence the result follows as $C_{c}(X) = C(X)$. Suppose $X = \mathbb{R}$. Write $\mathbb{R} = \bigcup_{N  = 1}^{\infty}[-N, N]$. Let $f \in C_{c}(\mathbb{R})$. Then $f$ is supported on a compact set $K \subset [-N, N]$ for some $N$. Thus $C_{c}(\mathbb{R}) =\bigcup_{N = 1}^{\infty}C([-N, N])$. Each $C([-N, N])$ has a countable dense subset $\{\psi_{N, n}\}_{n = 1}^{\infty}$ and so $\bigcup_{N, n = 1}^{\infty}\{\psi_{N, n}\}$ is a countable dense subset of $C_{c}(\mathbb{R})$. In the general case, $X = \bigcup_{i = 1}^{\infty}X_{i}$ where each $X_{i}$ is compact and $X_{1} \subset X_{2} \subset \cdots$. Let $f \in C_{c}(X)$. Then $f$ is supported on a compact set $K = \bigcup_{i = 1}^{\infty}K \cap X_{i}$. Is it still true that $C_{c}(X) = \bigcup_{i = 1}^{\infty}C(X_{i})$?",,"['real-analysis', 'analysis', 'functional-analysis', 'measure-theory']"
88,Is my statistician friend right/wrong on metric spaces and norms?,Is my statistician friend right/wrong on metric spaces and norms?,,"I was talking to a statistician friend of mine who said that instead of minimizing this function  $\sum_{i,j}W_{ij}d_{ij}^2(X)$ over $X$ it would be better to solve an analogous  minimization problem $\sum_{i,j}W_{ij}||f(Y_{i.})-f(Y_{j.})||_\mathcal{H}$ over $f$ instead with the norm being a RKHS(Reproducing Kernel Hilbert Space) norm where the functions $f(.)$ come from a Hilbert space of functions. Both minimizations are under a constraint that $\sum_{i,j}d^2_{i,j}(X)$ or $\sum_{i,j}||f(Y_{i.})-f(Y_{j.})||_\mathcal{H}$ is constrained to a fixed real positive value $\nu$. Here, $d_{ij}^2(X)$ is the squared Euclidean distance between the rows $i,j$ of the real unknown matrix $X$. $Y$ is a fixed real matrix. Question: Why would the latter problem under the function space be better!? What does the RKHS norm provide, which is different than the euclidean distance/norm? From this description, why do you think my academic friend has said this-when thought from different mathematical directions? What are we gaining or losing between these two formulations? After all, once the $f(.)$ is solved for, the norms seem to be preserving some sort of notion of distance/dissimilarity or nearness. What is special about the second problem!? I understand that the question is a little open ended. So please feel free to be verbose in expressing your thoughts! Perspective:  I showed him the initial Euclidean problem. He thought about it and posed the other problem as being interesting. Please shed your thoughts.","I was talking to a statistician friend of mine who said that instead of minimizing this function  $\sum_{i,j}W_{ij}d_{ij}^2(X)$ over $X$ it would be better to solve an analogous  minimization problem $\sum_{i,j}W_{ij}||f(Y_{i.})-f(Y_{j.})||_\mathcal{H}$ over $f$ instead with the norm being a RKHS(Reproducing Kernel Hilbert Space) norm where the functions $f(.)$ come from a Hilbert space of functions. Both minimizations are under a constraint that $\sum_{i,j}d^2_{i,j}(X)$ or $\sum_{i,j}||f(Y_{i.})-f(Y_{j.})||_\mathcal{H}$ is constrained to a fixed real positive value $\nu$. Here, $d_{ij}^2(X)$ is the squared Euclidean distance between the rows $i,j$ of the real unknown matrix $X$. $Y$ is a fixed real matrix. Question: Why would the latter problem under the function space be better!? What does the RKHS norm provide, which is different than the euclidean distance/norm? From this description, why do you think my academic friend has said this-when thought from different mathematical directions? What are we gaining or losing between these two formulations? After all, once the $f(.)$ is solved for, the norms seem to be preserving some sort of notion of distance/dissimilarity or nearness. What is special about the second problem!? I understand that the question is a little open ended. So please feel free to be verbose in expressing your thoughts! Perspective:  I showed him the initial Euclidean problem. He thought about it and posed the other problem as being interesting. Please shed your thoughts.",,"['real-analysis', 'linear-algebra', 'general-topology', 'functional-analysis', 'hilbert-spaces']"
89,Is this (classical?) exercice missing a hypothesis?,Is this (classical?) exercice missing a hypothesis?,,"A friend just told me about an exercice he was given quite a few years ago, but he wasn't sure wether he remembered all the hypothesis correctly. Does anybody recognize this? Let $f$ be a smooth real-valued function on $\Bbb R$ such that for all $n\in\Bbb N,~\sup_{\Bbb R}|f^{(n)}|=1$ and $f(0)=1$, then $f=\cos$. He wasn't sure wether theose were the exact hypothesis needed for this to work. He noticed that $f$ is automatically a power series with infinite radius of convergence, but wondered wether one ought to impose $f$ to be even, and wether the condition on the suprermums of the derivatives is correct. NOTE. If you know the hypothesis, please post them as an answer, and if you want to post a solution, please hide the text so that one needs to scroll over it to reveal it, thanks!","A friend just told me about an exercice he was given quite a few years ago, but he wasn't sure wether he remembered all the hypothesis correctly. Does anybody recognize this? Let $f$ be a smooth real-valued function on $\Bbb R$ such that for all $n\in\Bbb N,~\sup_{\Bbb R}|f^{(n)}|=1$ and $f(0)=1$, then $f=\cos$. He wasn't sure wether theose were the exact hypothesis needed for this to work. He noticed that $f$ is automatically a power series with infinite radius of convergence, but wondered wether one ought to impose $f$ to be even, and wether the condition on the suprermums of the derivatives is correct. NOTE. If you know the hypothesis, please post them as an answer, and if you want to post a solution, please hide the text so that one needs to scroll over it to reveal it, thanks!",,"['real-analysis', 'reference-request', 'power-series']"
90,Convergence in $L^1$ space,Convergence in  space,L^1,"Suppose that $f_{n}$ is a sequence of measurable functions, in a finite measure space, $f_{n}\to f $ in $m$-measure and that there exists $g$ in $L^1$ such that $\vert f_n\vert \le g$.   Prove that    $$ \lim_{n\to +\infty}\Vert f_n-f\Vert_{L^1}=0. $$ What I obviously thought of doing was splitting the difference $|f_n-f|$  to the less than and greater than $\epsilon$ and bound the greater part by $2g$. I am stuck right there, I can show it is finite but can not show it is less than epsilon. Next I thought of using the R. Fisher's argument of getting the subsequence of $f_n$ which converges a.e, and finiteness of space give you a. uniform by Egoroff). But that way I can only show result will be good for the case of subsequence. I am not sure if I can conclude from there though( by arguing that original sequence and its subsequence goes to the same limit). I am sure I am missing something here. I would love to get out of this confusion. Help please.","Suppose that $f_{n}$ is a sequence of measurable functions, in a finite measure space, $f_{n}\to f $ in $m$-measure and that there exists $g$ in $L^1$ such that $\vert f_n\vert \le g$.   Prove that    $$ \lim_{n\to +\infty}\Vert f_n-f\Vert_{L^1}=0. $$ What I obviously thought of doing was splitting the difference $|f_n-f|$  to the less than and greater than $\epsilon$ and bound the greater part by $2g$. I am stuck right there, I can show it is finite but can not show it is less than epsilon. Next I thought of using the R. Fisher's argument of getting the subsequence of $f_n$ which converges a.e, and finiteness of space give you a. uniform by Egoroff). But that way I can only show result will be good for the case of subsequence. I am not sure if I can conclude from there though( by arguing that original sequence and its subsequence goes to the same limit). I am sure I am missing something here. I would love to get out of this confusion. Help please.",,"['real-analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
91,Is $C_c^{\infty}$ dense in $X_0^{\alpha}$?,Is  dense in ?,C_c^{\infty} X_0^{\alpha},"While reading papers on fractional Laplacian, I always meet space $X_0^{\alpha}(\mathcal{C}_{\Omega})$ which is defined as following: $$X_0^{\alpha}(\mathcal{C}_{\Omega})=\{z\in L^2(\mathcal{C}_{\Omega}): z=0 \text{ on }\partial_L\mathcal{C}_{\Omega}, \int_{\mathcal{C}_{\Omega}}y^{1-\alpha}|\nabla z(x,y)|^2\,dxdy<\infty\},$$ Where $\Omega$ is bounded domain in $\mathbb{R}^n$, $\mathcal{C}_{\Omega}=\{(x,y):x\in \Omega,y\in \mathbb{R}_+\}\subset \mathbb{R}^{n+1}_+$, and $\partial_L\mathcal{C}_{\Omega}$ is  the lateral boundary of $\mathcal{C}_{\Omega}$. And we equip $X_0^{\alpha}$ with norm  $$\Vert z\Vert_{X_0^{\alpha}}^2=\int_{\mathcal{C}_{\Omega}}y^{1-\alpha}|\nabla z(x,y)|^2\,dxdy.$$ Then my question is: Is $C_c^{\infty}(\overline{\mathbb{R}^{n+1}_+})$ dense in $X_0^{\alpha}(\mathcal{C}_{\Omega})$, and how to prove it? I don't even know whether we have $\Vert \eta_{\varepsilon}*u-u\Vert_{X_0^{\alpha}}\to 0$, where $\eta_{\varepsilon}$ is the standard mollifier.","While reading papers on fractional Laplacian, I always meet space $X_0^{\alpha}(\mathcal{C}_{\Omega})$ which is defined as following: $$X_0^{\alpha}(\mathcal{C}_{\Omega})=\{z\in L^2(\mathcal{C}_{\Omega}): z=0 \text{ on }\partial_L\mathcal{C}_{\Omega}, \int_{\mathcal{C}_{\Omega}}y^{1-\alpha}|\nabla z(x,y)|^2\,dxdy<\infty\},$$ Where $\Omega$ is bounded domain in $\mathbb{R}^n$, $\mathcal{C}_{\Omega}=\{(x,y):x\in \Omega,y\in \mathbb{R}_+\}\subset \mathbb{R}^{n+1}_+$, and $\partial_L\mathcal{C}_{\Omega}$ is  the lateral boundary of $\mathcal{C}_{\Omega}$. And we equip $X_0^{\alpha}$ with norm  $$\Vert z\Vert_{X_0^{\alpha}}^2=\int_{\mathcal{C}_{\Omega}}y^{1-\alpha}|\nabla z(x,y)|^2\,dxdy.$$ Then my question is: Is $C_c^{\infty}(\overline{\mathbb{R}^{n+1}_+})$ dense in $X_0^{\alpha}(\mathcal{C}_{\Omega})$, and how to prove it? I don't even know whether we have $\Vert \eta_{\varepsilon}*u-u\Vert_{X_0^{\alpha}}\to 0$, where $\eta_{\varepsilon}$ is the standard mollifier.",,"['real-analysis', 'partial-differential-equations']"
92,Proving $\{P(a)\}$ (where $a$ is transcendental) is dense,Proving  (where  is transcendental) is dense,\{P(a)\} a,"So, I have read this problem, and it's bugged me since: Let $a \in (1,2)$ be a transcendental number 1) Let $Y = \{ P(a) : P \in \mathbb{Z}[X] \}$, show that Y is dense in $\mathbb{R}$. 2) Let $X = \{ P(a) : P \in \mathbb{Z}[X], P \text{ has coefficients in } \{0,1,-1\}\}$, show that X is dense in $\mathbb{R}$. I have been able to prove 1), by showing that $Y$ is a non-cyclic subgroup of $\mathbb{R}$ and therefore is dense (a well-known result which I can prove too). 2) had a hint attached: Hint : Start by proving that $0$ is in the closure of $X \backslash \{0\}$ I was able to prove this too: I took $Z = \{ \sum_{k=0}^{n-1} b_k a^k / \forall k, b_k \in \{0, 1\} \}$. We have $\forall z \in Z, 0 \leq z \leq \frac{a^n - 1}{a-1} < \frac{a^n}{a-1}$. $Z$ has $2^n$ elements, so if we cut $[ 0, \frac{a^n}{a-1} )$ in the $2^n - 1$ intervals $I_k = [ \frac{(a/2)^n}{a-1} \times k, \frac{(a/2)^n}{a-1} \times (k+1) )$, then there is a $k$ such that $I_k \cap Z = \{z_1, z_2\}$ with $z_1 \neq z_2$. Therefore, $| z_1 - z_2 | \in X \cap [0, \frac{(a/2)^n}{a-1})$. Since $0 < a < 2$, $(\frac{a}{2})^n \rightarrow 0$ and we have that X is dense around 0. But now I'm stuck. I've thought of trying to approximate any real by an element of the form $a^n \varepsilon$ with $\varepsilon \in X$ close to 0, but it doesn't work. Do you have any hints? Edit: It's not homework, it's in preparation of an upcoming oral exam.","So, I have read this problem, and it's bugged me since: Let $a \in (1,2)$ be a transcendental number 1) Let $Y = \{ P(a) : P \in \mathbb{Z}[X] \}$, show that Y is dense in $\mathbb{R}$. 2) Let $X = \{ P(a) : P \in \mathbb{Z}[X], P \text{ has coefficients in } \{0,1,-1\}\}$, show that X is dense in $\mathbb{R}$. I have been able to prove 1), by showing that $Y$ is a non-cyclic subgroup of $\mathbb{R}$ and therefore is dense (a well-known result which I can prove too). 2) had a hint attached: Hint : Start by proving that $0$ is in the closure of $X \backslash \{0\}$ I was able to prove this too: I took $Z = \{ \sum_{k=0}^{n-1} b_k a^k / \forall k, b_k \in \{0, 1\} \}$. We have $\forall z \in Z, 0 \leq z \leq \frac{a^n - 1}{a-1} < \frac{a^n}{a-1}$. $Z$ has $2^n$ elements, so if we cut $[ 0, \frac{a^n}{a-1} )$ in the $2^n - 1$ intervals $I_k = [ \frac{(a/2)^n}{a-1} \times k, \frac{(a/2)^n}{a-1} \times (k+1) )$, then there is a $k$ such that $I_k \cap Z = \{z_1, z_2\}$ with $z_1 \neq z_2$. Therefore, $| z_1 - z_2 | \in X \cap [0, \frac{(a/2)^n}{a-1})$. Since $0 < a < 2$, $(\frac{a}{2})^n \rightarrow 0$ and we have that X is dense around 0. But now I'm stuck. I've thought of trying to approximate any real by an element of the form $a^n \varepsilon$ with $\varepsilon \in X$ close to 0, but it doesn't work. Do you have any hints? Edit: It's not homework, it's in preparation of an upcoming oral exam.",,"['real-analysis', 'general-topology', 'polynomials']"
93,When is the square root differentiable?,When is the square root differentiable?,,"Let $f$ be a non negative differentiable function (defined on $\mathbb R$ ) and $g(x)=\sqrt{f(x)}$ . Can you characterise the points $x_0$ where $g$ is differentiable at $x_0$ ? It is clear that when $f(x_0)\neq0$ then $g$ is differentiable at $x_0$ (by chain rule). So the real question is, what happens when $f(x_0)=0$ ? For such a point it is easy to see that if $g$ is differentiable at $x_0$ then necessarily $f'(x_0)=0$ . But the converse is not true as the example $f(x)=x^2$ shows (at $x_0=0$ ). So for the points $x_0$ such that $f(x_0)=0$ under what conditions on $f$ is $g$ differentiable at $x_0$ ? I suspect that the answer is: $g$ is differentiable at $x_0$ iff $f''(x_0)=0$ , but I cannot give a proof. Do you have any ideas?","Let be a non negative differentiable function (defined on ) and . Can you characterise the points where is differentiable at ? It is clear that when then is differentiable at (by chain rule). So the real question is, what happens when ? For such a point it is easy to see that if is differentiable at then necessarily . But the converse is not true as the example shows (at ). So for the points such that under what conditions on is differentiable at ? I suspect that the answer is: is differentiable at iff , but I cannot give a proof. Do you have any ideas?",f \mathbb R g(x)=\sqrt{f(x)} x_0 g x_0 f(x_0)\neq0 g x_0 f(x_0)=0 g x_0 f'(x_0)=0 f(x)=x^2 x_0=0 x_0 f(x_0)=0 f g x_0 g x_0 f''(x_0)=0,"['real-analysis', 'analysis', 'derivatives', 'roots']"
94,Does the SVD work on an incomplete field?,Does the SVD work on an incomplete field?,,"Does the singular value decomposition (SVD) require a complete field? SVD clearly can't work on $\mathbb Q$ , since we need square roots.  But can it work on $\mathbb E$ , the smallest Euclidean field containing $\mathbb Q$ ? I ask because, to my surprise, my synthetic geometry proof of polar decomposition required completeness, and would not, as far as I can tell, work for $\mathbb E$ .  (This is startling, because, it is generally accepted, as Dedekind himself wrote when introducing his cuts, that Euclidean geometry does not require, and would not be changed, by completeness, as long as it includes all constructible numbers.) Additionally, when proving SVD, Trefethen & Bau make use of compactness to show the existence of a maximum, which of course requires a complete field. And, from Harvard's fabled 55a , another use of completeness: Compactness... gives us another way to prove the spectral theorem: we can find an eigenvector for $T$ by seeing where the function $w \mapsto \langle Tw,w \rangle$ achieves its maximum on the unit sphere. Thus, can the SVD be done in a field that admits square roots but is not complete?  And, if yes: What aspect of complete fields finds its way into all three of these proofs? Is SVD somehow different or ""stronger"" in complete fields?","Does the singular value decomposition (SVD) require a complete field? SVD clearly can't work on , since we need square roots.  But can it work on , the smallest Euclidean field containing ? I ask because, to my surprise, my synthetic geometry proof of polar decomposition required completeness, and would not, as far as I can tell, work for .  (This is startling, because, it is generally accepted, as Dedekind himself wrote when introducing his cuts, that Euclidean geometry does not require, and would not be changed, by completeness, as long as it includes all constructible numbers.) Additionally, when proving SVD, Trefethen & Bau make use of compactness to show the existence of a maximum, which of course requires a complete field. And, from Harvard's fabled 55a , another use of completeness: Compactness... gives us another way to prove the spectral theorem: we can find an eigenvector for by seeing where the function achieves its maximum on the unit sphere. Thus, can the SVD be done in a field that admits square roots but is not complete?  And, if yes: What aspect of complete fields finds its way into all three of these proofs? Is SVD somehow different or ""stronger"" in complete fields?","\mathbb Q \mathbb E \mathbb Q \mathbb E T w \mapsto \langle Tw,w \rangle","['real-analysis', 'linear-algebra', 'field-theory', 'svd', 'complete-spaces']"
95,"Can we find $f\in \Bbb{R}^{[0, 1]}$ with the property $\mathcal{M}$ which doesn't satisfy the property $\mathcal{B}$?",Can we find  with the property  which doesn't satisfy the property ?,"f\in \Bbb{R}^{[0, 1]} \mathcal{M} \mathcal{B}","$f:[0, 1]\to \Bbb{R}$ be a function. $f$ satisfy the property $\mathcal{M}$ of $f(A) $ is meagre for every $A\subset [0, 1]$ meagre. $f$ satisfy the property $\mathcal{B}$ if $f(A) $ is a set with the property of Baire for every $A\subset [0, 1]$ having the property of Baire. I don't know whether the property $\mathcal{M}, \mathcal{B}$ has any name or not. $f\in \Bbb{R}^{[0, 1]}$ with property $\mathcal{B}$ and  we know every meager set satisfy the property of Baire this implies image of every meager set is a set with the baire property and this means it is a symmetric difference of an open set and meager set. I believe we can find $f\in \Bbb{R}^{[0, 1]}$ with the property $\mathcal{B}$ which doesn't satisfy the property $\mathcal{M}$ . Question  : Can you give me an explicit example of $f\in \Bbb{R}^{[0,  1]}$ which map every set with Baire property to a set with Baire property but doesn't map a meager set to a meager set? Question : Can we find $f\in \Bbb{R}^{[0, 1]}$ with the property $\mathcal{M}$ which doesn't satisfy the property $\mathcal{B}$ ? The sets with the Baire property forms a $\sigma$ -algebra generated by open sets and meagre sets. Suppose $F\in \Bbb{R}^{[0, 1]}$ satisfy the property $\mathcal{M}$ . $F(M) \subset \Bbb{R}$ is meagre for $\forall M\subset [0, 1]$ meagre. Hence $F$ maps the Cantor set $\mathcal{C}$ and all subsets of $\mathcal{C}$ to meagre set. So $F(\mathcal{C}) =\mathcal{C}$ . And suppose $F(U) $ is not open for some $U\subset [0, 1]\setminus\mathcal{C}$ open. Now again we have to map every meagre subset of $U$ to a meagre set. This is difficult and how to map rest of points. Does this type of function exists? If yes how to construct?",be a function. satisfy the property of is meagre for every meagre. satisfy the property if is a set with the property of Baire for every having the property of Baire. I don't know whether the property has any name or not. with property and  we know every meager set satisfy the property of Baire this implies image of every meager set is a set with the baire property and this means it is a symmetric difference of an open set and meager set. I believe we can find with the property which doesn't satisfy the property . Question  : Can you give me an explicit example of which map every set with Baire property to a set with Baire property but doesn't map a meager set to a meager set? Question : Can we find with the property which doesn't satisfy the property ? The sets with the Baire property forms a -algebra generated by open sets and meagre sets. Suppose satisfy the property . is meagre for meagre. Hence maps the Cantor set and all subsets of to meagre set. So . And suppose is not open for some open. Now again we have to map every meagre subset of to a meagre set. This is difficult and how to map rest of points. Does this type of function exists? If yes how to construct?,"f:[0, 1]\to \Bbb{R} f \mathcal{M} f(A)  A\subset [0, 1] f \mathcal{B} f(A)  A\subset [0, 1] \mathcal{M}, \mathcal{B} f\in \Bbb{R}^{[0, 1]} \mathcal{B} f\in \Bbb{R}^{[0, 1]} \mathcal{B} \mathcal{M} f\in \Bbb{R}^{[0,
 1]} f\in \Bbb{R}^{[0, 1]} \mathcal{M} \mathcal{B} \sigma F\in \Bbb{R}^{[0, 1]} \mathcal{M} F(M) \subset \Bbb{R} \forall M\subset [0, 1] F \mathcal{C} \mathcal{C} F(\mathcal{C}) =\mathcal{C} F(U)  U\subset [0, 1]\setminus\mathcal{C} U","['real-analysis', 'general-topology', 'functional-analysis', 'measure-theory', 'descriptive-set-theory']"
96,"For any conditionally convergent series $\sum _{n=1}^\infty a_n,\ \exists\ k\geq 2\ $ such that the subseries $\sum _{n=1}^\infty a_{nk}$ converges.",For any conditionally convergent series  such that the subseries  converges.,"\sum _{n=1}^\infty a_n,\ \exists\ k\geq 2\  \sum _{n=1}^\infty a_{nk}","A subseries of the series $\displaystyle\sum _{n=1}^\infty a_n$ is defined to be a series of the form $\displaystyle\sum _{k=1}^\infty a_{n_k}$ , for $n_k \subseteq \Bbb N$ . Prove or disprove: For any conditionally convergent series $\displaystyle\sum _{n=1}^\infty a_n,\ \exists\ k\geq 2\ $ such that the subseries $\displaystyle\sum _{n=1}^\infty a_{kn}$ converges. Certainly, every conditionally convergent series has some subseries that converge (decreasing alternating terms), and some subseries that diverge (the subseries of all of the positive terms; also the subseries of all of the negative terms). $$$$ My original ""answer"" which I now doubt is a counter-example to the proposition: Hint: Find a subset $\ A \subset \mathbb{N}\ $ with the property that, for each $\ n \in \mathbb{N},\ $ the set $ \{kn\}_{k \in \mathbb{N} }$ has finitely many elements in common with $A.$ Answer: The prime numbers is the standard example of such a subset of $\mathbb{N}$ with the property in the hint. This point is to make the 2nd, 3rd, 5th, 7th, ... members of our sequence have one sign, and every other member of our sequence have another sign. For example, $$ a_1 =  \frac{1}{1},\ a_2 =  \frac{-1}{4},\ a_3 =  \frac{-1}{4},\ a_4 =  \frac{1}{3},\ a_5 =  \frac{-1}{4},\ a_6 =  \frac{1}{5},\ a_7 =  \frac{-1}{6},\ a_8 = a_9 = a_{10} =  \frac{1}{3} \times \frac{1}{7},\ a_{11} = \frac{-1}{8},\ ... $$ This is simply the alternating harmonic series split up in a way that (I thought) answers the question. That was my answer, but now I'm not sure the series above is a valid counter-example. I constructed it so that every subseries of the form $\displaystyle\sum _{n=1}^\infty a_{kn}$ contains finitely many negative numbers. And then I'm not sure what my logic was. It was either that "" every subseries of a monotonic divergent series diverges "", but now I realise that this is not what the link says, and is also not true. Or my reasoning was that, for each $k \in \mathbb{N},\ $ every subseries $\displaystyle\sum _{n=1}^\infty \frac{1}{kn}$ of the harmonic series, diverges, which is true, but the subseries of my series defined above do not match these subseries of the harmonic series. So the problem remains open...","A subseries of the series is defined to be a series of the form , for . Prove or disprove: For any conditionally convergent series such that the subseries converges. Certainly, every conditionally convergent series has some subseries that converge (decreasing alternating terms), and some subseries that diverge (the subseries of all of the positive terms; also the subseries of all of the negative terms). My original ""answer"" which I now doubt is a counter-example to the proposition: Hint: Find a subset with the property that, for each the set has finitely many elements in common with Answer: The prime numbers is the standard example of such a subset of with the property in the hint. This point is to make the 2nd, 3rd, 5th, 7th, ... members of our sequence have one sign, and every other member of our sequence have another sign. For example, This is simply the alternating harmonic series split up in a way that (I thought) answers the question. That was my answer, but now I'm not sure the series above is a valid counter-example. I constructed it so that every subseries of the form contains finitely many negative numbers. And then I'm not sure what my logic was. It was either that "" every subseries of a monotonic divergent series diverges "", but now I realise that this is not what the link says, and is also not true. Or my reasoning was that, for each every subseries of the harmonic series, diverges, which is true, but the subseries of my series defined above do not match these subseries of the harmonic series. So the problem remains open...","\displaystyle\sum _{n=1}^\infty a_n \displaystyle\sum _{k=1}^\infty a_{n_k} n_k \subseteq \Bbb N \displaystyle\sum _{n=1}^\infty a_n,\ \exists\ k\geq 2\  \displaystyle\sum _{n=1}^\infty a_{kn}  \ A \subset \mathbb{N}\  \ n \in \mathbb{N},\   \{kn\}_{k \in \mathbb{N} } A. \mathbb{N}  a_1 =  \frac{1}{1},\ a_2 =  \frac{-1}{4},\ a_3 =  \frac{-1}{4},\ a_4 =  \frac{1}{3},\ a_5 =  \frac{-1}{4},\ a_6 =  \frac{1}{5},\ a_7 =  \frac{-1}{6},\ a_8 = a_9 = a_{10} =  \frac{1}{3} \times \frac{1}{7},\ a_{11} = \frac{-1}{8},\ ...  \displaystyle\sum _{n=1}^\infty a_{kn} k \in \mathbb{N},\  \displaystyle\sum _{n=1}^\infty \frac{1}{kn}","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'absolute-convergence', 'conditional-convergence']"
97,Prove $\lim\limits_{n \to \infty} na_n=0.$,Prove,\lim\limits_{n \to \infty} na_n=0.,"Suppose $\sum_{n=1}^{\infty} a_n$ be a convergent positive series, and satisfy $\lim\limits_{n \to \infty}\frac{a_{n+1}}{a_n}=1$ . Prove $\lim\limits_{n \to \infty} na_n=0.$ First, we may consider applying Cauchy's condensation test. Since $\sum_{n=1}^{\infty}a_n$ is convergent, $\sum_{n=1}^{\infty} 2^n a_{2^n}$ is convergent as well, which implies $\lim\limits_{n \to \infty}2^n a_{2^n}=0$ . Hence, there already exists a subsequence of $\{na_n\}$ convergent to zero. But how to go on ?","Suppose be a convergent positive series, and satisfy . Prove First, we may consider applying Cauchy's condensation test. Since is convergent, is convergent as well, which implies . Hence, there already exists a subsequence of convergent to zero. But how to go on ?",\sum_{n=1}^{\infty} a_n \lim\limits_{n \to \infty}\frac{a_{n+1}}{a_n}=1 \lim\limits_{n \to \infty} na_n=0. \sum_{n=1}^{\infty}a_n \sum_{n=1}^{\infty} 2^n a_{2^n} \lim\limits_{n \to \infty}2^n a_{2^n}=0 \{na_n\},"['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
98,A functional analogue to a finite geometric series,A functional analogue to a finite geometric series,,"The well-known geometric series identity $$1+x+x^2+\cdots+x^k=\frac{x^{k+1}-1}{x-1}$$ holds for all $x\ne1$ and positive integers $k$ . Now consider a real function $f(x)\ne1$ that satisfies $$1+f(x)+f(f(x))+\cdots+f^{k}(x)=\frac{f^{k+1}(x)-1}{f(x)-1}$$ for all $x\in[a,b]$ where $f^{k+1}(x)=f(f^k(x))$ and $a,b$ are real numbers such that $a<b$ . Does such an $f\not\equiv0$ exist for each $k$ ? For example, the case $k=1$ is easy as we have $$1+f(x)=\frac{f(f(x))-1}{f(x)-1}\implies f(x)^2=f(f(x))\implies f(x)=x^2$$ for all $x\in[-(1-\epsilon),1-\epsilon]$ with $0<\epsilon<1$ . It is also straightforward to prove for even $k$ since $f(x)=-x$ for all $x\ge0$ yields $$1+(-x+x)+\cdots+(-x+x)=\frac{-x-1}{-x-1}$$ which is true, so the cases where $k\ge3$ is odd remain.","The well-known geometric series identity holds for all and positive integers . Now consider a real function that satisfies for all where and are real numbers such that . Does such an exist for each ? For example, the case is easy as we have for all with . It is also straightforward to prove for even since for all yields which is true, so the cases where is odd remain.","1+x+x^2+\cdots+x^k=\frac{x^{k+1}-1}{x-1} x\ne1 k f(x)\ne1 1+f(x)+f(f(x))+\cdots+f^{k}(x)=\frac{f^{k+1}(x)-1}{f(x)-1} x\in[a,b] f^{k+1}(x)=f(f^k(x)) a,b a<b f\not\equiv0 k k=1 1+f(x)=\frac{f(f(x))-1}{f(x)-1}\implies f(x)^2=f(f(x))\implies f(x)=x^2 x\in[-(1-\epsilon),1-\epsilon] 0<\epsilon<1 k f(x)=-x x\ge0 1+(-x+x)+\cdots+(-x+x)=\frac{-x-1}{-x-1} k\ge3","['real-analysis', 'functional-equations']"
99,"Functions of the form $\int_{a} ^{x} f(t) \, dt$ with regard to Riemann and Lebesgue integral",Functions of the form  with regard to Riemann and Lebesgue integral,"\int_{a} ^{x} f(t) \, dt","Consider the class $\mathcal{R}[a, b] $ of functions $F:[a, b] \to\mathbb {R} $ which can be expressed as $$F(x) =\int_{a} ^{x} f(t) \, dt$$ for some Riemann integrable function $f$ and consider similar class $\mathcal{L}[a, b] $ of functions where the integral is Lebesgue instead of Riemann. Since Riemann integrable functions are also Lebesgue integrable $\mathcal{R} [a, b] \subseteq \mathcal{L} [a, b] $ . I suspect these sets are not equal. Further it is known that the functions in $\mathcal{L}[a, b] $ are characterized by the following properties: They are continuous on $[a, b] $ They are of bounded variation on $[a, b] $ They satisfy Luzin N property on $[a, b] $ ie if $A\subseteq[a, b] $ is a set of measure zero then $F(A) $ is also a set of measure zero. Clearly these properties are also possessed by the functions $F\in\mathcal {R} [a, b] $ but since this is supposed to be a smaller set compared to $\mathcal{L} [a, b] $ its members must have some other unique properties not possessed by members of $\mathcal{L} [a, b] $ . How can we characterize the members of $\mathcal{R} [a, b] $ ? Any specific examples will help to illustrate the properties involved. The motivation for this question comes from the fact the Lebesgue integrable functions can be much weirder (eg discontinuous everywhere) than Riemann integrable functions (necessarily continuous almost everywhere) and yet their integrals are far more well behaved (continuous and of bounded variation). A weird function like Dirichlet characteristic function of rationals upon integrating gives the constant function $0$ . I think that finding functions which lie in $\mathcal{L} $ and not in $\mathcal{R} $ is not trivial. I also tried to apply the Fundamental Theorem of Calculus for Riemann integrals and figured that the functions in $\mathcal{R} $ are differentiable almost everywhere, but due to bounded variation the same holds for functions in $\mathcal{L} $ also. The difference between these two classes is a bit deep and not easy to figure out at least for me.","Consider the class of functions which can be expressed as for some Riemann integrable function and consider similar class of functions where the integral is Lebesgue instead of Riemann. Since Riemann integrable functions are also Lebesgue integrable . I suspect these sets are not equal. Further it is known that the functions in are characterized by the following properties: They are continuous on They are of bounded variation on They satisfy Luzin N property on ie if is a set of measure zero then is also a set of measure zero. Clearly these properties are also possessed by the functions but since this is supposed to be a smaller set compared to its members must have some other unique properties not possessed by members of . How can we characterize the members of ? Any specific examples will help to illustrate the properties involved. The motivation for this question comes from the fact the Lebesgue integrable functions can be much weirder (eg discontinuous everywhere) than Riemann integrable functions (necessarily continuous almost everywhere) and yet their integrals are far more well behaved (continuous and of bounded variation). A weird function like Dirichlet characteristic function of rationals upon integrating gives the constant function . I think that finding functions which lie in and not in is not trivial. I also tried to apply the Fundamental Theorem of Calculus for Riemann integrals and figured that the functions in are differentiable almost everywhere, but due to bounded variation the same holds for functions in also. The difference between these two classes is a bit deep and not easy to figure out at least for me.","\mathcal{R}[a, b]  F:[a, b] \to\mathbb {R}  F(x) =\int_{a} ^{x} f(t) \, dt f \mathcal{L}[a, b]  \mathcal{R} [a, b] \subseteq \mathcal{L} [a, b]  \mathcal{L}[a, b]  [a, b]  [a, b]  [a, b]  A\subseteq[a, b]  F(A)  F\in\mathcal {R} [a, b]  \mathcal{L} [a, b]  \mathcal{L} [a, b]  \mathcal{R} [a, b]  0 \mathcal{L}  \mathcal{R}  \mathcal{R}  \mathcal{L} ","['real-analysis', 'lebesgue-integral', 'riemann-integration']"
