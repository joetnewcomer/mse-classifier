,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving wedge product is associative,Proving wedge product is associative,,"Fix a real vector space $V$ of finite dimension. Let's denote by $\Lambda^p(V)$ the vector space of $p$-forms on $V$ (i.e. alternating $p$-tensors). Then we have the product $\wedge : \Lambda^p(V) \times \Lambda^q(V) \to \Lambda^{p + q}(V)$ given by $(\omega \wedge \eta)(X_1, \ldots, X_{p + q}) = \frac{1}{p! q!} \sum_{\sigma \in S_{p + q}} sgn\ \sigma \cdot \omega(X_{\sigma(1)}, \ldots, X_{\sigma(p)}) \eta(X_{\sigma(p + 1)}, \ldots, X_{\sigma(p + q)})$. How can I prove that $\wedge$ is associative? I've tried developing it from the definition but in the end I don't get things that look nice or are obviously equal. Specifically, I get: $(\omega \wedge \eta) \wedge \theta(X_1, \ldots, X_{p + q + r}) = \frac{1}{(p + q)! r! p! q!} \sum_{\sigma \in S_{p + q + r}} \sum_{\tau \in S_{p + q}} sgn\ \sigma\ sgn \tau \cdot \omega(X_{\sigma \tau(1)}, \ldots, X_{\sigma \tau(p)}) \eta(X_{\sigma \tau(p + 1)}, \ldots, X_{\sigma \tau(p + q)}) \theta(X_{\sigma(p + q + 1)}, \ldots, X_{\sigma(p + q + r)})$ $\omega \wedge (\eta \wedge \theta)(X_1, \ldots, X_{p + q + r}) = \frac{1}{p! (q + r)! q! r!} \sum_{\sigma \in S_{p + q + r}} \sum_{\tau \in S_{q + r}} sgn\ \sigma\ sgn\ \tau \cdot \omega(X_{\sigma(1)}, \ldots, X_{\sigma(p)}) \eta(X_{\sigma(p + \tau(1))}, \ldots, X_{\sigma(p + \tau(q))}) \theta(X_{\sigma(p + \tau(q + 1))}, \ldots, X_{\sigma(p + \tau(q + r))})$","Fix a real vector space $V$ of finite dimension. Let's denote by $\Lambda^p(V)$ the vector space of $p$-forms on $V$ (i.e. alternating $p$-tensors). Then we have the product $\wedge : \Lambda^p(V) \times \Lambda^q(V) \to \Lambda^{p + q}(V)$ given by $(\omega \wedge \eta)(X_1, \ldots, X_{p + q}) = \frac{1}{p! q!} \sum_{\sigma \in S_{p + q}} sgn\ \sigma \cdot \omega(X_{\sigma(1)}, \ldots, X_{\sigma(p)}) \eta(X_{\sigma(p + 1)}, \ldots, X_{\sigma(p + q)})$. How can I prove that $\wedge$ is associative? I've tried developing it from the definition but in the end I don't get things that look nice or are obviously equal. Specifically, I get: $(\omega \wedge \eta) \wedge \theta(X_1, \ldots, X_{p + q + r}) = \frac{1}{(p + q)! r! p! q!} \sum_{\sigma \in S_{p + q + r}} \sum_{\tau \in S_{p + q}} sgn\ \sigma\ sgn \tau \cdot \omega(X_{\sigma \tau(1)}, \ldots, X_{\sigma \tau(p)}) \eta(X_{\sigma \tau(p + 1)}, \ldots, X_{\sigma \tau(p + q)}) \theta(X_{\sigma(p + q + 1)}, \ldots, X_{\sigma(p + q + r)})$ $\omega \wedge (\eta \wedge \theta)(X_1, \ldots, X_{p + q + r}) = \frac{1}{p! (q + r)! q! r!} \sum_{\sigma \in S_{p + q + r}} \sum_{\tau \in S_{q + r}} sgn\ \sigma\ sgn\ \tau \cdot \omega(X_{\sigma(1)}, \ldots, X_{\sigma(p)}) \eta(X_{\sigma(p + \tau(1))}, \ldots, X_{\sigma(p + \tau(q))}) \theta(X_{\sigma(p + \tau(q + 1))}, \ldots, X_{\sigma(p + \tau(q + r))})$",,"['linear-algebra', 'determinant', 'differential-forms', 'exterior-algebra']"
1,$A$ is a symmetric postivie definite matrix. Prove that $A^k$ is also a positive deinite,is a symmetric postivie definite matrix. Prove that  is also a positive deinite,A A^k,"Let $A\in M_n(\mathbb{R})$, a symmetric positive-definite matrix. Prove that for every $k\in\mathbb{N}$, $A^k$ is also positive definite. So since $A\in M_n(\mathbb{R})$ is symmetric and positive definite, we know there's an invertible $P$ such that $P^tAP = \text{Diag}(c_1,\ldots,c_n)$ such that $c_1,\ldots ,c_n > 0$. I think it's sufficent to prove it for $k=2$ and the rest is given by induction. I also know that Matrix congruence is an equivalence relation so I tried to ""play"" with the definitions to infer something about $A^2$ but havn't got something useful.","Let $A\in M_n(\mathbb{R})$, a symmetric positive-definite matrix. Prove that for every $k\in\mathbb{N}$, $A^k$ is also positive definite. So since $A\in M_n(\mathbb{R})$ is symmetric and positive definite, we know there's an invertible $P$ such that $P^tAP = \text{Diag}(c_1,\ldots,c_n)$ such that $c_1,\ldots ,c_n > 0$. I think it's sufficent to prove it for $k=2$ and the rest is given by induction. I also know that Matrix congruence is an equivalence relation so I tried to ""play"" with the definitions to infer something about $A^2$ but havn't got something useful.",,"['linear-algebra', 'matrices']"
2,Find a plane with distance $3$ from $3x-y-z = 0$,Find a plane with distance  from,3 3x-y-z = 0,"I need to find a plane such that its distance from the plane $3x-y-z = 0$ is $3$. Since distance is defined only for parallel planes, I already know that they have to be parallel, and then, the equation of the new plane will have the same normal vector $(3,-1,-1)$. Also, this plane intersects the origin, because $(0,0,0)$ satisfies its equation. So my idea was to normalize the normal vector, and then multiply it by $3$. I could then put it in the origin and see it as a point of distance $3$ from the origin (and also from the plane), and then this point should be in the new plane, so it must satisfy is equation, which is: $$3x-y-z + d = 0$$ I think this migth work, but I don't think it's the best way to solve this exercise. A friend of mine sent me a solution like this: $$3(x-x_0) -1(y-y_0) -1(z-z_0) = 0 \\3x -y -z + (-3x_0 +y_0+z_0) = 0$$ then we should find $P = (x_0,y_0,z_0)$ such that $-3x_0 +y_0+z_0 = 0$. Then, $3P$ should be a point of the new plane, and therefore satisfy its new equation. What's the best way to solve this, and could you explain my what my friend did in his solution?","I need to find a plane such that its distance from the plane $3x-y-z = 0$ is $3$. Since distance is defined only for parallel planes, I already know that they have to be parallel, and then, the equation of the new plane will have the same normal vector $(3,-1,-1)$. Also, this plane intersects the origin, because $(0,0,0)$ satisfies its equation. So my idea was to normalize the normal vector, and then multiply it by $3$. I could then put it in the origin and see it as a point of distance $3$ from the origin (and also from the plane), and then this point should be in the new plane, so it must satisfy is equation, which is: $$3x-y-z + d = 0$$ I think this migth work, but I don't think it's the best way to solve this exercise. A friend of mine sent me a solution like this: $$3(x-x_0) -1(y-y_0) -1(z-z_0) = 0 \\3x -y -z + (-3x_0 +y_0+z_0) = 0$$ then we should find $P = (x_0,y_0,z_0)$ such that $-3x_0 +y_0+z_0 = 0$. Then, $3P$ should be a point of the new plane, and therefore satisfy its new equation. What's the best way to solve this, and could you explain my what my friend did in his solution?",,"['linear-algebra', 'geometry', 'analytic-geometry']"
3,A question on Green's functions & integral operators,A question on Green's functions & integral operators,,"I'm fairly new to the concept of Green's functions, but from what I understand so far, they are a powerful tool for solving PDEs with boundary conditions. Given a differential equation (in operator form) $$\mathcal{L}u(x)=f(x)$$ (where $\mathcal{L}$ is a differential operator) it is possible to find a solution for $u(x)$ of the form $$u(x)=\int G(x,\xi)f(\xi)d\xi$$ I'm struggling to understand the motivations given for the integral operator form of the solution given above. Why is it possible to represent it like this? I understand that the motivation partially comes from the fact that $\mathcal{L}$ will have an inverse (assuming that $\mathcal{L}$ is non-singular ), and as it is a differential operator this implies that the inverse $\mathcal{L}^{-1}$ will be an integral operator . From the differential equation above we have $$u(x)=\mathcal{L}^{-1}f(x)$$ What is the motivation for introducing a so-called Green's function $G(x,\xi)$ such that the integral operator is of the form given above? Is it just that $G(x,\xi)$ is a generator of the operator? Also, is the reason why we end up with the defining equation for a Green's function $$\mathcal{L}G(x,\xi)=\delta (x-\xi)$$ simply because $\mathcal{L}\cdot\mathcal{L}^{-1}=\mathbf{1}$ and the fact that we can formally write (as $\mathcal{L}$ is linear ) $$\mathcal{L}u(x)=\mathcal{L}\left(\int G(x,\xi)f(\xi)d\xi\right)=\int \left(\mathcal{L}G(x,\xi)\right)f(\xi)d\xi=f(x)\quad\Rightarrow\quad\mathcal{L}G(x,\xi)=\delta (x-\xi)$$","I'm fairly new to the concept of Green's functions, but from what I understand so far, they are a powerful tool for solving PDEs with boundary conditions. Given a differential equation (in operator form) $$\mathcal{L}u(x)=f(x)$$ (where $\mathcal{L}$ is a differential operator) it is possible to find a solution for $u(x)$ of the form $$u(x)=\int G(x,\xi)f(\xi)d\xi$$ I'm struggling to understand the motivations given for the integral operator form of the solution given above. Why is it possible to represent it like this? I understand that the motivation partially comes from the fact that $\mathcal{L}$ will have an inverse (assuming that $\mathcal{L}$ is non-singular ), and as it is a differential operator this implies that the inverse $\mathcal{L}^{-1}$ will be an integral operator . From the differential equation above we have $$u(x)=\mathcal{L}^{-1}f(x)$$ What is the motivation for introducing a so-called Green's function $G(x,\xi)$ such that the integral operator is of the form given above? Is it just that $G(x,\xi)$ is a generator of the operator? Also, is the reason why we end up with the defining equation for a Green's function $$\mathcal{L}G(x,\xi)=\delta (x-\xi)$$ simply because $\mathcal{L}\cdot\mathcal{L}^{-1}=\mathbf{1}$ and the fact that we can formally write (as $\mathcal{L}$ is linear ) $$\mathcal{L}u(x)=\mathcal{L}\left(\int G(x,\xi)f(\xi)d\xi\right)=\int \left(\mathcal{L}G(x,\xi)\right)f(\xi)d\xi=f(x)\quad\Rightarrow\quad\mathcal{L}G(x,\xi)=\delta (x-\xi)$$",,"['linear-algebra', 'ordinary-differential-equations', 'greens-function']"
4,Taylor series approximation of function under norm,Taylor series approximation of function under norm,,"I am reading this paper . At page number $4$, term $||Au - f||^{2}$  is  approximated by taylor series approximation around $u^{k}$. The resulting approximations are $$\|Au - f\|^{2} \approx \|Au^{k}- f\|^{2}  +  2 \langle u, A^{T}(Au^{k}- f)\rangle  +  \frac{1}{\delta} \| u - u^{k}\|^{2} $$ I am finding it difficult to figure out this step. Background: This paper discusses the convergence of the linearized Bregman iteration for solving the following minimization problem: $$\min \{ \|u\|_1: Au = f\}$$ where matrix $ A \in \mathbb{R}^{m\times n}$, with $n > m$ and $f \in \mathbb{R}^{m}$. I am aware of the Taylor series of a real valued function $f(x)$ about a point. However, I haven't read about finding the Taylor series of normed function. I need help to understand this. Any reference regarding this theory will also be helpful.","I am reading this paper . At page number $4$, term $||Au - f||^{2}$  is  approximated by taylor series approximation around $u^{k}$. The resulting approximations are $$\|Au - f\|^{2} \approx \|Au^{k}- f\|^{2}  +  2 \langle u, A^{T}(Au^{k}- f)\rangle  +  \frac{1}{\delta} \| u - u^{k}\|^{2} $$ I am finding it difficult to figure out this step. Background: This paper discusses the convergence of the linearized Bregman iteration for solving the following minimization problem: $$\min \{ \|u\|_1: Au = f\}$$ where matrix $ A \in \mathbb{R}^{m\times n}$, with $n > m$ and $f \in \mathbb{R}^{m}$. I am aware of the Taylor series of a real valued function $f(x)$ about a point. However, I haven't read about finding the Taylor series of normed function. I need help to understand this. Any reference regarding this theory will also be helpful.",,"['linear-algebra', 'optimization', 'taylor-expansion', 'normed-spaces']"
5,What does it mean if the standard Hermitian form of complex two vectors is purely imaginary?,What does it mean if the standard Hermitian form of complex two vectors is purely imaginary?,,"If $v,w \in \mathbb{C}^n$, what does it mean geometrically for $\langle v , w \rangle$ to be purely imaginary?","If $v,w \in \mathbb{C}^n$, what does it mean geometrically for $\langle v , w \rangle$ to be purely imaginary?",,"['linear-algebra', 'complex-numbers']"
6,Prove that finite dimensional $V$ is the direct sum of its generalized eigenspaces $V_\lambda$,Prove that finite dimensional  is the direct sum of its generalized eigenspaces,V V_\lambda,"Let $T$ be a linear operator on a finite dimensional complex vector space $V$ . Prove that $V$ is the direct sum of its generalized eigenspaces. I already proved that every eigenspace $V_\lambda$ is a $T$ invariant subspace of $V$ . I can find a proof that the generalized eigenspaces are linearly independent. Can anyone help direct me towards a reasonably straightforward method of proving the rest of this? Can I prove that the dimensions of the eigenspaces sum to the dimension of $V$ ? That would complete this, but I can't think of how to do that.","Let be a linear operator on a finite dimensional complex vector space . Prove that is the direct sum of its generalized eigenspaces. I already proved that every eigenspace is a invariant subspace of . I can find a proof that the generalized eigenspaces are linearly independent. Can anyone help direct me towards a reasonably straightforward method of proving the rest of this? Can I prove that the dimensions of the eigenspaces sum to the dimension of ? That would complete this, but I can't think of how to do that.",T V V V_\lambda T V V,"['linear-algebra', 'eigenvalues-eigenvectors']"
7,"Linear algebra terminology: unique, trivial, non-trivial, inconsistent and consistent","Linear algebra terminology: unique, trivial, non-trivial, inconsistent and consistent",,Let me get this straight: Unique solution- has an exact solution (such as a POI of 2 intersecting lines) Trivial solution- when 0 needs to equal the zero vector in $Ax=0 vector$ Non-Trivial-? Perhaps when 0 does not equal the zero vector in $Ax=0$. This is due to the presence of free variables in Matrix A. Consistent- Where there is a unique solution? Inconsistent? When there is an inequality such as 4 can't equal 3 meaning the solution can not be solved. This is due to parallel lines. Am I correct on these  technical terms? I put inequality in italics because I'm not sure if I used it correctly.,Let me get this straight: Unique solution- has an exact solution (such as a POI of 2 intersecting lines) Trivial solution- when 0 needs to equal the zero vector in $Ax=0 vector$ Non-Trivial-? Perhaps when 0 does not equal the zero vector in $Ax=0$. This is due to the presence of free variables in Matrix A. Consistent- Where there is a unique solution? Inconsistent? When there is an inequality such as 4 can't equal 3 meaning the solution can not be solved. This is due to parallel lines. Am I correct on these  technical terms? I put inequality in italics because I'm not sure if I used it correctly.,,"['linear-algebra', 'terminology']"
8,How to represent matrix as the sum of rank-one matrices,How to represent matrix as the sum of rank-one matrices,,"If we're given $B$ to be a $4 \times 7$ matrix: $$\begin{bmatrix}1 & 2 & -3 & 7 & 0 & -2 & 5\\1 & 2 & -3 & 7 & 1 & 3 & -2\\0 & 0 & 0 & 0 & 1 & 5 & -7\\0 & 0 & 0 & 0 & 0 & 0& 0\end{bmatrix}$$ I know that the rank is equal to 2. However I'm trying to find a way to represent $B$ as $B = u_1 v_1^T + u_2 v_2^T$ such that $u_1$ and $u_2$ are in $\mathbb R^4$ and $v_1$ and $v_2$ are in $R^7$. I'm having trouble formulating these vectors, can anyone help me figure out what they are?","If we're given $B$ to be a $4 \times 7$ matrix: $$\begin{bmatrix}1 & 2 & -3 & 7 & 0 & -2 & 5\\1 & 2 & -3 & 7 & 1 & 3 & -2\\0 & 0 & 0 & 0 & 1 & 5 & -7\\0 & 0 & 0 & 0 & 0 & 0& 0\end{bmatrix}$$ I know that the rank is equal to 2. However I'm trying to find a way to represent $B$ as $B = u_1 v_1^T + u_2 v_2^T$ such that $u_1$ and $u_2$ are in $\mathbb R^4$ and $v_1$ and $v_2$ are in $R^7$. I'm having trouble formulating these vectors, can anyone help me figure out what they are?",,"['linear-algebra', 'matrices', 'vector-spaces', 'matrix-equations']"
9,operation to get a diagonal matrix from a vector,operation to get a diagonal matrix from a vector,,"In many programs you can create diagonal matrix from a vector, like diag function in Matlab and DiagonalMatrix function in Mathematica. I'm wondering whether we can use matrix product (or hadamard product, kronecker product, etc) of a vector and identity matrices to create a diagonal matrix. Thank you!","In many programs you can create diagonal matrix from a vector, like diag function in Matlab and DiagonalMatrix function in Mathematica. I'm wondering whether we can use matrix product (or hadamard product, kronecker product, etc) of a vector and identity matrices to create a diagonal matrix. Thank you!",,['linear-algebra']
10,"A ""wrong"" counterexample to splitting lemma in linear algebra","A ""wrong"" counterexample to splitting lemma in linear algebra",,"I have a ""wrong"" counterexample to the following statement in linear algebra but I don't see why it's wrong: let $T:V\to W$ be a linear map between vector spaces. Then, $V$ is the direct sum of $\textrm{im}(T)$ and $\ker(T)$. Let $V$ be the space of polynomials over a field with degree less than or equal to n, and let $T \colon V \to V$ be the differential operator. Then $\textrm{im}(T)$ is the space of polynomials with degree less than or equal to $n-1$, and $\ker(T)$ is the base field. Their direct sum is a proper subspace of $V$.","I have a ""wrong"" counterexample to the following statement in linear algebra but I don't see why it's wrong: let $T:V\to W$ be a linear map between vector spaces. Then, $V$ is the direct sum of $\textrm{im}(T)$ and $\ker(T)$. Let $V$ be the space of polynomials over a field with degree less than or equal to n, and let $T \colon V \to V$ be the differential operator. Then $\textrm{im}(T)$ is the space of polynomials with degree less than or equal to $n-1$, and $\ker(T)$ is the base field. Their direct sum is a proper subspace of $V$.",,['linear-algebra']
11,"Determinant evaluation for matrix with $-1, 2, -1$ below/on/above diagonal [duplicate]",Determinant evaluation for matrix with  below/on/above diagonal [duplicate],"-1, 2, -1",This question already has answers here : Determinant of a tridiagonal Toeplitz matrix (2 answers) Closed 9 years ago . What is the trick for evaluating the determinant of this matrix? $$\begin{bmatrix} 2 & -1 \\ -1 & 2 & -1 \\ & -1 & 2 & -1 \\ && -1 & 2 & -1 \\ &&& -1 & 2 & -1 \\ &&&& -1 & 2 \end{bmatrix}$$,This question already has answers here : Determinant of a tridiagonal Toeplitz matrix (2 answers) Closed 9 years ago . What is the trick for evaluating the determinant of this matrix?,"\begin{bmatrix}
2 & -1 \\
-1 & 2 & -1 \\
& -1 & 2 & -1 \\
&& -1 & 2 & -1 \\
&&& -1 & 2 & -1 \\
&&&& -1 & 2 \end{bmatrix}","['linear-algebra', 'matrices', 'determinant', 'tridiagonal-matrices', 'toeplitz-matrices']"
12,Kernel and Image of a polynomial linear transformation,Kernel and Image of a polynomial linear transformation,,"I have here a linear transformation $T : P_3(\mathbb{R})\rightarrow P_3(\mathbb{R}) $ defined by: $ T(at^3 + bt^2 + ct + d) = (a-b)t^3 + (c-d)t $ I'm very very new in this subject and I'm not going well with polynomials. I need find the $ Kernel $ and the $ Image $ of the transformation. Look what I've been thinking: $Ker(T) =  \{ T(p) = 0 / p \in P_3\} $ $ T(at^3 + bt^2 + ct + d) = (a-b)t^3 + (c-d)t = 0 $ $(a-b) = 0 \ ;\ \ (c-d) = 0 \ ;\ \ a = b \ ; \ \ c = d $ $ Ker(T) = \{ at^3 + at^2 + ct +c\ /\ a,c \in \mathbb{R} \} $ And what about the $ Image $ ? I know that $Im(T) = \{ T(p) / p \in P_3 \}$ , but how can I show it? And how can I test if a polynomial such as $ p(t) = t^3 + t^2 + t -1 \in Im(t)$ ?","I have here a linear transformation defined by: I'm very very new in this subject and I'm not going well with polynomials. I need find the and the of the transformation. Look what I've been thinking: And what about the ? I know that , but how can I show it? And how can I test if a polynomial such as ?","T : P_3(\mathbb{R})\rightarrow P_3(\mathbb{R})   T(at^3 + bt^2 + ct + d) = (a-b)t^3 + (c-d)t   Kernel   Image  Ker(T) =  \{ T(p) = 0 / p \in P_3\}   T(at^3 + bt^2 + ct + d) = (a-b)t^3 + (c-d)t = 0  (a-b) = 0 \ ;\ \ (c-d) = 0 \ ;\ \ a = b \ ; \ \ c = d   Ker(T) = \{ at^3 + at^2 + ct +c\ /\ a,c \in \mathbb{R} \}   Image  Im(T) = \{ T(p) / p \in P_3 \}  p(t) = t^3 + t^2 + t -1 \in Im(t)","['linear-algebra', 'linear-transformations']"
13,A method of finding the eigenvector that I don't fully understand,A method of finding the eigenvector that I don't fully understand,,"Let $$A=\begin{pmatrix} a & b & c  \\ d & e & f  \\ g & h & t \\ \end{pmatrix}$$   Which has a known eigenvalue : $\lambda$ Find the corresponding eigenvector Over the last few months, I have come across  several ways to do this: (1) $$Ae=\lambda e$$ $$\begin{pmatrix} a & b & c  \\ d & e & f  \\ g & h & t \\ \end{pmatrix} \begin{pmatrix} x_1  \\ x_2  \\ x_3 \\ \end{pmatrix}= \lambda \begin{pmatrix} x_1  \\ x_2  \\ x_3 \\ \end{pmatrix}$$ Which is  the longer (more time consuming method. (2) Some  do it in this way: $$(A-\lambda I)e=0$$ $$\begin{pmatrix} a-\lambda & b & c  \\ d & e-\lambda & f  \\ g & h &  t-\lambda \\ \end{pmatrix} \begin{pmatrix} x_1  \\ x_2  \\ x_3 \\ \end{pmatrix}=0$$ And mostly  they  perform reduce row echleon form to speed the process up. (3) Most recently I came across a method which is the fastest so far, but I don't understand how it works. And sometimes it doesn't work for me, I get correct R answer and sometimes wrong. Here is how it is, (similar to taking cross product of two vectors): It is EVALUATING $$\begin{bmatrix} i & j & k  \\ d & e-\lambda & f  \\ g & h & t-\lambda \\ \end{bmatrix}$$ Or $$\begin{bmatrix} i & j & k  \\ a-\lambda & b & c  \\ d & e-\lambda & f \\ \end{bmatrix}$$ Or $$\begin{bmatrix} i & j & k  \\ a-\lambda & b & c  \\ g & h & t-\lambda \\ \end{bmatrix}$$ Etc any two rows How does this work? And why doesn't it work sometime? Why  doesn't it work with some matrices ? (Sometimes) Although this is much time saving, it is unreliable to me which I believe is because I don't understand well in order to apply it correctly Example : Let $$A=\begin{pmatrix} 4 & 1 & -1  \\ -4 & -1 & 4  \\ 0 & -1 & 5 \\ \end{pmatrix}$$ $\lambda=3$ $$\begin{bmatrix} i & j & k  \\ 4-3 & 1 & -1  \\ 0 & -1 & 5-3 \\ \end{bmatrix}$$ Eigenvector: $$\begin{pmatrix} 1   \\  -2  \\  -1 \\ \end{pmatrix}$$ But yes the evaluation goes like this.","Let $$A=\begin{pmatrix} a & b & c  \\ d & e & f  \\ g & h & t \\ \end{pmatrix}$$   Which has a known eigenvalue : $\lambda$ Find the corresponding eigenvector Over the last few months, I have come across  several ways to do this: (1) $$Ae=\lambda e$$ $$\begin{pmatrix} a & b & c  \\ d & e & f  \\ g & h & t \\ \end{pmatrix} \begin{pmatrix} x_1  \\ x_2  \\ x_3 \\ \end{pmatrix}= \lambda \begin{pmatrix} x_1  \\ x_2  \\ x_3 \\ \end{pmatrix}$$ Which is  the longer (more time consuming method. (2) Some  do it in this way: $$(A-\lambda I)e=0$$ $$\begin{pmatrix} a-\lambda & b & c  \\ d & e-\lambda & f  \\ g & h &  t-\lambda \\ \end{pmatrix} \begin{pmatrix} x_1  \\ x_2  \\ x_3 \\ \end{pmatrix}=0$$ And mostly  they  perform reduce row echleon form to speed the process up. (3) Most recently I came across a method which is the fastest so far, but I don't understand how it works. And sometimes it doesn't work for me, I get correct R answer and sometimes wrong. Here is how it is, (similar to taking cross product of two vectors): It is EVALUATING $$\begin{bmatrix} i & j & k  \\ d & e-\lambda & f  \\ g & h & t-\lambda \\ \end{bmatrix}$$ Or $$\begin{bmatrix} i & j & k  \\ a-\lambda & b & c  \\ d & e-\lambda & f \\ \end{bmatrix}$$ Or $$\begin{bmatrix} i & j & k  \\ a-\lambda & b & c  \\ g & h & t-\lambda \\ \end{bmatrix}$$ Etc any two rows How does this work? And why doesn't it work sometime? Why  doesn't it work with some matrices ? (Sometimes) Although this is much time saving, it is unreliable to me which I believe is because I don't understand well in order to apply it correctly Example : Let $$A=\begin{pmatrix} 4 & 1 & -1  \\ -4 & -1 & 4  \\ 0 & -1 & 5 \\ \end{pmatrix}$$ $\lambda=3$ $$\begin{bmatrix} i & j & k  \\ 4-3 & 1 & -1  \\ 0 & -1 & 5-3 \\ \end{bmatrix}$$ Eigenvector: $$\begin{pmatrix} 1   \\  -2  \\  -1 \\ \end{pmatrix}$$ But yes the evaluation goes like this.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
14,"$\operatorname{span}\{AB-BA : A, B \in M_n(\Bbb R)\}$ is the set of all matrices with trace $0$ [duplicate]",is the set of all matrices with trace  [duplicate],"\operatorname{span}\{AB-BA : A, B \in M_n(\Bbb R)\} 0","This question already has an answer here : Dimensionality of null space when Trace is Zero (1 answer) Closed 6 years ago . Let $W$ be the space $$\operatorname{span}\{AB-BA\} ,$$ where A and B are square matrices, and let $H$ be the space of all square matrices of trace $0$. Then prove that $W=H$. The fact that $W$ is a subset of $H$ is fairly easy to prove. I could prove the converse.","This question already has an answer here : Dimensionality of null space when Trace is Zero (1 answer) Closed 6 years ago . Let $W$ be the space $$\operatorname{span}\{AB-BA\} ,$$ where A and B are square matrices, and let $H$ be the space of all square matrices of trace $0$. Then prove that $W=H$. The fact that $W$ is a subset of $H$ is fairly easy to prove. I could prove the converse.",,"['linear-algebra', 'matrices', 'lie-algebras']"
15,Bounds on the singular values of a matrix with unitary columns,Bounds on the singular values of a matrix with unitary columns,,"if $X$ is a matrix with unitary columns ( each column has unit norm ), are there lower and upper bounds on the minimum and maximum singular values of $X$? I could prove a lower bound for $\Sigma_{min}$ and $\Sigma_{max}$ i.e. $\Sigma_{min} \geq 0$ and $\Sigma_{max} \geq 1$, respectively. Do upper bounds exist for the same? Thanks in advance.","if $X$ is a matrix with unitary columns ( each column has unit norm ), are there lower and upper bounds on the minimum and maximum singular values of $X$? I could prove a lower bound for $\Sigma_{min}$ and $\Sigma_{max}$ i.e. $\Sigma_{min} \geq 0$ and $\Sigma_{max} \geq 1$, respectively. Do upper bounds exist for the same? Thanks in advance.",,['linear-algebra']
16,What is the Singular Value Decomposition for the Zero Matrix?,What is the Singular Value Decomposition for the Zero Matrix?,,"I am interested in the singular value decomposition of a matrix: $\mathbf{M} = \mathbf{U} \mathbf{S} \mathbf{V}^T$. Suppose $\mathbf{M} = \mathbf{0}$ (zero matrix) and square.  Clearly, $\mathbf{S} = \mathbf{0}$, but what about $\mathbf{U}$ and $\mathbf{V}^T$ -- Do they have defined values? When use MATLAB, $\mathbf{U} = \mathbf{V} = \mathbf{I}$, but is this a definition or pure luck?  If so, why is it defined as such, since any matrix for $\mathbf{U}$ or $\mathbf{V}$ will satisfy the decomposition - it does not even need to be orthogonal.  Is there some sort of proof that it must indeed fall to a specific value? >> [U, S, V] = svd(zeros(3))  U =       1     0     0      0     1     0      0     0     1   S =       0     0     0      0     0     0      0     0     0   V =       1     0     0      0     1     0      0     0     1","I am interested in the singular value decomposition of a matrix: $\mathbf{M} = \mathbf{U} \mathbf{S} \mathbf{V}^T$. Suppose $\mathbf{M} = \mathbf{0}$ (zero matrix) and square.  Clearly, $\mathbf{S} = \mathbf{0}$, but what about $\mathbf{U}$ and $\mathbf{V}^T$ -- Do they have defined values? When use MATLAB, $\mathbf{U} = \mathbf{V} = \mathbf{I}$, but is this a definition or pure luck?  If so, why is it defined as such, since any matrix for $\mathbf{U}$ or $\mathbf{V}$ will satisfy the decomposition - it does not even need to be orthogonal.  Is there some sort of proof that it must indeed fall to a specific value? >> [U, S, V] = svd(zeros(3))  U =       1     0     0      0     1     0      0     0     1   S =       0     0     0      0     0     0      0     0     0   V =       1     0     0      0     1     0      0     0     1",,"['linear-algebra', 'svd']"
17,When a system of rational linear equations have complex solutions does it have rational solutions?,When a system of rational linear equations have complex solutions does it have rational solutions?,,"Problem: The fact that a system of rational linear equations    $Ax=0$ has a nontrivial complex solution (not necessarily rational), does it imply that it also admit a nontrivial rational one? One solution is as follows: Write $Ax = 0$, where $x = (x_1,\cdots,x_n)^T$ is the complex solution. Let $V$ be the $\mathbb{Q}$-span of $\{x_1,\cdots,x_n\}$, and let $\{y_1,\cdots,y_m\}$ be a $\mathbb{Q}$-basis of $V$. Then we have $By =x$, where $B$ is a matrix of rational entries and $y = (y_1,\cdots,y_m)^T$. Then from $ABy = 0$ we deduce that $AB= 0$ since $y$ is a $\mathbb{Q}$-basis and $AB$ is a rational matrix. Thus any column of $B$ is a rational solution of the original equation. Question: Is it true for ""inhomogeneous"" linear systems? That is: if $A$ is a rational matrix and $b$ is a rational vector such that $Ax = b$ has a complex solution, does it imply that it also has a rational solution?","Problem: The fact that a system of rational linear equations    $Ax=0$ has a nontrivial complex solution (not necessarily rational), does it imply that it also admit a nontrivial rational one? One solution is as follows: Write $Ax = 0$, where $x = (x_1,\cdots,x_n)^T$ is the complex solution. Let $V$ be the $\mathbb{Q}$-span of $\{x_1,\cdots,x_n\}$, and let $\{y_1,\cdots,y_m\}$ be a $\mathbb{Q}$-basis of $V$. Then we have $By =x$, where $B$ is a matrix of rational entries and $y = (y_1,\cdots,y_m)^T$. Then from $ABy = 0$ we deduce that $AB= 0$ since $y$ is a $\mathbb{Q}$-basis and $AB$ is a rational matrix. Thus any column of $B$ is a rational solution of the original equation. Question: Is it true for ""inhomogeneous"" linear systems? That is: if $A$ is a rational matrix and $b$ is a rational vector such that $Ax = b$ has a complex solution, does it imply that it also has a rational solution?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
18,What is $\frac{dXX^T}{dX}$?,What is ?,\frac{dXX^T}{dX},"Given $X \in \mathbb{R}^{n \times r}$, what is $$\dfrac{dXX^T}{dX}?$$ I'm aware it is a order 4 tensor.","Given $X \in \mathbb{R}^{n \times r}$, what is $$\dfrac{dXX^T}{dX}?$$ I'm aware it is a order 4 tensor.",,"['linear-algebra', 'matrix-calculus']"
19,"A question about invertible matrices, $A,B$ are invertible matrices, $AB+BA=0$, show that n is even","A question about invertible matrices,  are invertible matrices, , show that n is even","A,B AB+BA=0","Let $A,B\in M_n(\mathbb R)$ be invertible matrices, and let $AB+BA=0$, show that n is even. I know what the solution is: $AB=-BA\Rightarrow |1|=(-1)^n|1|\Rightarrow \text{n is even}$. So we know that every invertible matrix say $C$ has only one matrix say $D$ such that, $CD=DC=I$, but in this question it doesn't say if $A$ and $B$ are such matrices that they are  unique one to each other, so why can we assume that $AB=BA=I$ ?","Let $A,B\in M_n(\mathbb R)$ be invertible matrices, and let $AB+BA=0$, show that n is even. I know what the solution is: $AB=-BA\Rightarrow |1|=(-1)^n|1|\Rightarrow \text{n is even}$. So we know that every invertible matrix say $C$ has only one matrix say $D$ such that, $CD=DC=I$, but in this question it doesn't say if $A$ and $B$ are such matrices that they are  unique one to each other, so why can we assume that $AB=BA=I$ ?",,"['linear-algebra', 'matrices']"
20,How to find linearly independent columns in a matrix,How to find linearly independent columns in a matrix,,"For a general square matrix $A$, how do I find which columns are linearly dependent? When I say linear independent I mean not linearly dependent with any other column or any combination of other columns in the matrix. For example: \begin{matrix}   0 & -2 & 1 \\   0 & -4 & 2 \\   1 & -2 & 1  \end{matrix} In this matrix we know that column 1 is linear independent and columns 2 and 3 are dependent. I want something that always works, and I already have the SVD and QR decomposition implemented in Java and I hope one or both of them can help me solving this. Thanks in advance!","For a general square matrix $A$, how do I find which columns are linearly dependent? When I say linear independent I mean not linearly dependent with any other column or any combination of other columns in the matrix. For example: \begin{matrix}   0 & -2 & 1 \\   0 & -4 & 2 \\   1 & -2 & 1  \end{matrix} In this matrix we know that column 1 is linear independent and columns 2 and 3 are dependent. I want something that always works, and I already have the SVD and QR decomposition implemented in Java and I hope one or both of them can help me solving this. Thanks in advance!",,"['linear-algebra', 'matrices']"
21,Dual space and linear functional,Dual space and linear functional,,"The problem is this: Let $V$ be a vector space over $\mathbb{K}$ and $v \in V$. Show that if $f(v) = 0, \forall$ $f \in L(V, \mathbb{K})$, then $v=0$. It's a problem of a book I'm using to study Dual Spaces. It seems easy to solve, but I'm out of ideas... I tried write $v$ as a linear combination of the basis of $V$ and $f$ as a combination of the dual basis, but then I realized dimension of $V$ can be infinity.","The problem is this: Let $V$ be a vector space over $\mathbb{K}$ and $v \in V$. Show that if $f(v) = 0, \forall$ $f \in L(V, \mathbb{K})$, then $v=0$. It's a problem of a book I'm using to study Dual Spaces. It seems easy to solve, but I'm out of ideas... I tried write $v$ as a linear combination of the basis of $V$ and $f$ as a combination of the dual basis, but then I realized dimension of $V$ can be infinity.",,"['linear-algebra', 'vector-spaces']"
22,"If $A$ is similar to $-A^T$, is $A$ similar to an antisymmetric matrix?","If  is similar to , is  similar to an antisymmetric matrix?",A -A^T A,"Suppose that $A$ is an $n \times n$-matrix with entries from some field $F$. Assume also that $A$ is similar to $-A^T$, ie. $PAP^{-1} = -A^T$ for some invertible matrix $P$. (Here $A^T$ denotes the transpose of $A$.) Does it follow that $A$ is similar to an antisymmetric matrix?","Suppose that $A$ is an $n \times n$-matrix with entries from some field $F$. Assume also that $A$ is similar to $-A^T$, ie. $PAP^{-1} = -A^T$ for some invertible matrix $P$. (Here $A^T$ denotes the transpose of $A$.) Does it follow that $A$ is similar to an antisymmetric matrix?",,"['linear-algebra', 'matrices']"
23,Calculate the determinant of given matrix,Calculate the determinant of given matrix,,"The matrix $A_n\in\mathbb{R}^{n\times n}$ is given by $$\left[a_{i,j}\right] = \left\lbrace\begin{array}{cc} 1 & i=j \\ -j & i = j+1\\ i & i = j-1 \\ 0 & \text{other cases} \end{array} \right.$$ I already showed that it holds $$\det{A_n}= \det{A_{n-1}}+\left(n-1\right)^2\cdot\det{A_{n-2}}$$ However, can we find an explicit expression for the determinant of $A_n$?","The matrix $A_n\in\mathbb{R}^{n\times n}$ is given by $$\left[a_{i,j}\right] = \left\lbrace\begin{array}{cc} 1 & i=j \\ -j & i = j+1\\ i & i = j-1 \\ 0 & \text{other cases} \end{array} \right.$$ I already showed that it holds $$\det{A_n}= \det{A_{n-1}}+\left(n-1\right)^2\cdot\det{A_{n-2}}$$ However, can we find an explicit expression for the determinant of $A_n$?",,"['linear-algebra', 'matrices', 'determinant']"
24,Tensor product of a field with itself.,Tensor product of a field with itself.,,"I am proving the fact that if $A$ and $B$ are two central $k$-algebras where $k$ is a field (so then $Z(A) = Z(B) = k$), then $A \otimes B$ is also central. I made almost everything except this: I came up with the fact that $Z(A \otimes B) = k \otimes k$. But is it really true that $k \otimes k = k$?","I am proving the fact that if $A$ and $B$ are two central $k$-algebras where $k$ is a field (so then $Z(A) = Z(B) = k$), then $A \otimes B$ is also central. I made almost everything except this: I came up with the fact that $Z(A \otimes B) = k \otimes k$. But is it really true that $k \otimes k = k$?",,"['linear-algebra', 'field-theory', 'tensor-products']"
25,"if the matrix such $B-A,A$ is Positive-semidefinite,then $\sqrt{B}-\sqrt{A}$ is Positive-semidefinite","if the matrix such  is Positive-semidefinite,then  is Positive-semidefinite","B-A,A \sqrt{B}-\sqrt{A}","Question: let the matrix $A,B$ such  $B-A,A$ is Positive-semidefinite show that: $\sqrt{B}-\sqrt{A}$ is Positive-semidefinite maybe The general is true? question 2: (2)$\sqrt[k]{B}-\sqrt[k]{A}$ is Positive-semidefinite This problem is very nice,because we are all know this if $$x\ge y\ge 0$$,then we have $$\sqrt{x}\ge \sqrt{y}$$ But in matrix,then this is also true,But I can't prove it.Thank you","Question: let the matrix $A,B$ such  $B-A,A$ is Positive-semidefinite show that: $\sqrt{B}-\sqrt{A}$ is Positive-semidefinite maybe The general is true? question 2: (2)$\sqrt[k]{B}-\sqrt[k]{A}$ is Positive-semidefinite This problem is very nice,because we are all know this if $$x\ge y\ge 0$$,then we have $$\sqrt{x}\ge \sqrt{y}$$ But in matrix,then this is also true,But I can't prove it.Thank you",,"['linear-algebra', 'matrices']"
26,"How prove this $det\left(\frac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}\right)_{n\times n}>0,-2<t<2$",How prove this,"det\left(\frac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}\right)_{n\times n}>0,-2<t<2","Question: Show that for $t\in (-2,2)$ and $0<\lambda_1<\lambda_2<\ldots<\lambda_n$ we have $$det(A)=det\left(\dfrac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}\right)_{n\times n}>0$$ My try: few days ago,I have ask this problem How prove this matrix $\det (A)=\left(\frac{1}{\ln{(a_{i}+a_{j})}}\right)_{n\times n}\neq 0$ But I want try use achille hui methods,so  $$\dfrac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}=\int_{0}^{\infty}f(\lambda_{i},\lambda_{j},x)dx$$ But I can't find this $f$. Thank you very much! Now Sanchez has prove for $t\le 0$ then $\det(A)>0$,so other case is true? Thank you","Question: Show that for $t\in (-2,2)$ and $0<\lambda_1<\lambda_2<\ldots<\lambda_n$ we have $$det(A)=det\left(\dfrac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}\right)_{n\times n}>0$$ My try: few days ago,I have ask this problem How prove this matrix $\det (A)=\left(\frac{1}{\ln{(a_{i}+a_{j})}}\right)_{n\times n}\neq 0$ But I want try use achille hui methods,so  $$\dfrac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}=\int_{0}^{\infty}f(\lambda_{i},\lambda_{j},x)dx$$ But I can't find this $f$. Thank you very much! Now Sanchez has prove for $t\le 0$ then $\det(A)>0$,so other case is true? Thank you",,"['linear-algebra', 'matrices', 'determinant']"
27,Determining if a vector is in the column space of a matrix,Determining if a vector is in the column space of a matrix,,"Hi there I'm having some trouble with the following problem: I have a $3\times3$ symmetric matrix $$ A=\pmatrix{1+t&1&1\\ 1&1+t&1\\ 1&1&1+t}. $$ I am trying to determine the values of $t$ for which the vector $b = (1,t,t^2)^\top$ (this is a column vector) is in the column space of $A$. I think I'm fairly aware of how to go about it, forming the augmented matrix $[A|b]$ and basically using row ops to find a solution with which I could solve for the value(s) of $t$.  But I've been trying this and have no luck.  May I be missing something? Thank you","Hi there I'm having some trouble with the following problem: I have a $3\times3$ symmetric matrix $$ A=\pmatrix{1+t&1&1\\ 1&1+t&1\\ 1&1&1+t}. $$ I am trying to determine the values of $t$ for which the vector $b = (1,t,t^2)^\top$ (this is a column vector) is in the column space of $A$. I think I'm fairly aware of how to go about it, forming the augmented matrix $[A|b]$ and basically using row ops to find a solution with which I could solve for the value(s) of $t$.  But I've been trying this and have no luck.  May I be missing something? Thank you",,"['linear-algebra', 'matrices']"
28,Change of basis matrix for orthogonal bases,Change of basis matrix for orthogonal bases,,"I am trying to show that if $B_1$ and $B_2$ are orthonormal bases for $\mathbb{R}^n$, then the change of basis matrix $P$ from $B_1$ to $B_2$ is an orthogonal matrix. I'm a bit stuck.  I started with this: Let $x,y \in \mathbb{R}^n$.  Then $[x]_{B_2}^t[y]_{B_2} = (P[x]_{B_1})^tP[y]_{B_1} = [x]_{B_1}^tP^tP[y]_{B_1}$.  I want to show that $P^tP = I_n$.  I know I need to apply the fact that $B_1$ and $B_2$ are orthonormal, but I don't see how to apply that to this expression.  Am I approaching this properly or is there a better way to think about it?","I am trying to show that if $B_1$ and $B_2$ are orthonormal bases for $\mathbb{R}^n$, then the change of basis matrix $P$ from $B_1$ to $B_2$ is an orthogonal matrix. I'm a bit stuck.  I started with this: Let $x,y \in \mathbb{R}^n$.  Then $[x]_{B_2}^t[y]_{B_2} = (P[x]_{B_1})^tP[y]_{B_1} = [x]_{B_1}^tP^tP[y]_{B_1}$.  I want to show that $P^tP = I_n$.  I know I need to apply the fact that $B_1$ and $B_2$ are orthonormal, but I don't see how to apply that to this expression.  Am I approaching this properly or is there a better way to think about it?",,['linear-algebra']
29,For each given subspace W there is one and only one row-reduced echelon matrix that has W as its row space.,For each given subspace W there is one and only one row-reduced echelon matrix that has W as its row space.,,Let m and n be positive integers and let F be a field. Suppose W is a subspace of $ {F ^{n}}$ and $ dim W \le m$. Prove that there is precisely one m x n row-reduced echelon matrix over F which has W as its row space.,Let m and n be positive integers and let F be a field. Suppose W is a subspace of $ {F ^{n}}$ and $ dim W \le m$. Prove that there is precisely one m x n row-reduced echelon matrix over F which has W as its row space.,,['linear-algebra']
30,What is the proof that SVM can be used to solve the least squares problem with norm equality constraint?,What is the proof that SVM can be used to solve the least squares problem with norm equality constraint?,,"I've seen it claimed that the solution to the minimization problem: $$\begin{align*} \arg \min_{b} \quad & {\left\| A b \right\|}_{2}^{2} \\ \text{subject to} \quad & {\left\| b \right\|}_{2} = 1 \end{align*}$$ is given by first finding the singular value decomposition of A, $$\textbf{A} = \bf{U \Sigma V}$$ And then taking the column of $\bf{V}$ corresponding to the smallest singular value. Can someone present a proof that this is so?","I've seen it claimed that the solution to the minimization problem: is given by first finding the singular value decomposition of A, And then taking the column of corresponding to the smallest singular value. Can someone present a proof that this is so?","\begin{align*}
\arg \min_{b} \quad & {\left\| A b \right\|}_{2}^{2} \\
\text{subject to} \quad & {\left\| b \right\|}_{2} = 1
\end{align*} \textbf{A} = \bf{U \Sigma V} \bf{V}","['linear-algebra', 'optimization', 'least-squares', 'svd']"
31,How to prove that $B$ is nilpotent.,How to prove that  is nilpotent.,B,"Let $A$ and $B$ be complex matrices with $AB^2-B^2A=B$ . Prove that $B$ is nilpotent. By the way: This problem is from American Mathematical Monthly, Problem 10339,and this solution post 1996 American Mathematical Monthly, page 907. My question: This problem has other nice solutions? Thank you.","Let and be complex matrices with . Prove that is nilpotent. By the way: This problem is from American Mathematical Monthly, Problem 10339,and this solution post 1996 American Mathematical Monthly, page 907. My question: This problem has other nice solutions? Thank you.",A B AB^2-B^2A=B B,"['linear-algebra', 'matrices']"
32,"$f,g:\mathbb R^3\to\mathbb R$ are nonzero linear maps. Then which are true?",are nonzero linear maps. Then which are true?,"f,g:\mathbb R^3\to\mathbb R","(A) is wrong: Consider $f:(x,y,z)\mapsto(x,0,0),~g:(x,y,z)\mapsto(x,y,0)$ (B) is wrong: Consider $f:(x,y,z)\mapsto(x,0,0),~g:(x,y,z)\mapsto(x,y,0)$ (C) is wrong: $\ker g$ is a subspace of $\mathbb R^3$ and let $f:\ker g\to\mathbb R$ be the restriction of $f.$ Then since $\ker f\subset\ker g$ by isomorphism theorem $\ker g /\ker f\simeq\Im(f)\ne\mathbb R.$ (D) is correct. Please tell me if I'm right. I'm skeptical especially about (C).",(A) is wrong: Consider (B) is wrong: Consider (C) is wrong: is a subspace of and let be the restriction of Then since by isomorphism theorem (D) is correct. Please tell me if I'm right. I'm skeptical especially about (C).,"f:(x,y,z)\mapsto(x,0,0),~g:(x,y,z)\mapsto(x,y,0) f:(x,y,z)\mapsto(x,0,0),~g:(x,y,z)\mapsto(x,y,0) \ker g \mathbb R^3 f:\ker g\to\mathbb R f. \ker f\subset\ker g \ker g /\ker f\simeq\Im(f)\ne\mathbb R.","['linear-algebra', 'proof-verification']"
33,Minimal polynomial,Minimal polynomial,,Let $V$ be the vector space of square matrices of order $n$ over the field $F$. Let $A$ be  a fixed square matrix of $n$ and let $T$ be a linear operator on $V$ such that $T(B) = AB$. Show that the minimal polynomial for $T$ is the minimal polynomial for $A$. Thank you for your time.,Let $V$ be the vector space of square matrices of order $n$ over the field $F$. Let $A$ be  a fixed square matrix of $n$ and let $T$ be a linear operator on $V$ such that $T(B) = AB$. Show that the minimal polynomial for $T$ is the minimal polynomial for $A$. Thank you for your time.,,['linear-algebra']
34,Is there another method?,Is there another method?,,"If $x$ and $y$ are positive numbers such that $x + y = 1$, find the maximum value of $x^4y + xy^4$. I could do this problem my simplifying the expression to $xy(1-3xy)$ and taking $k=xy$, forming a quadratic equation and got the answer as $1/12.$ But is there another method using calculus to do this problem?","If $x$ and $y$ are positive numbers such that $x + y = 1$, find the maximum value of $x^4y + xy^4$. I could do this problem my simplifying the expression to $xy(1-3xy)$ and taking $k=xy$, forming a quadratic equation and got the answer as $1/12.$ But is there another method using calculus to do this problem?",,"['linear-algebra', 'multivariable-calculus', 'optimization']"
35,Orthogonal matrix confusion,Orthogonal matrix confusion,,"I have a confusion about orthogonal matrix. If columns of a square matrix are orthonormal to each other, is the matrix orthogonal? If yes, then are the rows of the matrix also orthonormal? Why? Why is it that QQ'=I? I get Q'Q=I but why QQ' is also I? Thanks, Tom","I have a confusion about orthogonal matrix. If columns of a square matrix are orthonormal to each other, is the matrix orthogonal? If yes, then are the rows of the matrix also orthonormal? Why? Why is it that QQ'=I? I get Q'Q=I but why QQ' is also I? Thanks, Tom",,['linear-algebra']
36,Simple looking log problem,Simple looking log problem,,"How would I solve this for $x$? The original problem is $$x+x^{\log_{2}3}=x^{\log_{2}5}$$ I have tried to reduce it down to this, $$x^{\log_{10}3}+x^{\log_{10}2}=x^{\log_{10}5}$$ I have been stuck on this for a while now, can't seem to figure out the trick to this. Sorry if I tagged this wrong, not sure what tags it should be.","How would I solve this for $x$? The original problem is $$x+x^{\log_{2}3}=x^{\log_{2}5}$$ I have tried to reduce it down to this, $$x^{\log_{10}3}+x^{\log_{10}2}=x^{\log_{10}5}$$ I have been stuck on this for a while now, can't seem to figure out the trick to this. Sorry if I tagged this wrong, not sure what tags it should be.",,"['linear-algebra', 'algebra-precalculus', 'logarithms']"
37,Find a basis for the range and kernel of $T$.,Find a basis for the range and kernel of .,T,"Find a basis for the range and kernel of $T$. $$A =\begin{bmatrix} 2 & 0 & -1\\ 4 & 0 &  -2\\ 0 & 0 & 0 \end{bmatrix}  $$ Attempt at Solving for Basis of Range: On finding the basis for the range, I know that the range is the same thing as the column space. So, finding the basis of the column space should be equivalent to finding the basis of the range. I got the following after reducing: $$\mathrm{Rref}(A) =\begin{bmatrix} 1 & 0 & -\frac 12\\ 0 & 0 &  0\\ 0 & 0 & 0 \end{bmatrix} $$ Because I only have a pivot in column $1$, my corresponding column in the original $A$ (thus my basis for the range), would be: $$\mathrm{Rref}(A) =\begin{bmatrix} 2\\ 4\\ 0 \end{bmatrix} $$ The solution given in the text says that this should actually be: $$\mathrm{Rref}(A) =\begin{bmatrix} 1\\ 2\\ 0 \end{bmatrix}$$ I can see that the book solution only differs from mine in that their solution seems to have been divided by $2$. However, my question is - why are they dividing by 2? I don't understand why the original column is being altered. Attempt at Solving for Basis of Kernel: In solving for the kernel, I know that the basis of the kernel should be the same as the basis for the nullspace.  From the $\mathrm{Rref}(A)$ above, I got the following equation: $(X_1) = (\frac12X_3)$. Letting $X_3$ equal one, I got the following matrix for the basis of the kernel: $$\text{Basis of Kernel} =\begin{bmatrix} \frac12\\ 0\\ 1 \end{bmatrix}$$ This answer checks out with my solution in the text, but the text also provides the following solution:  =\begin{bmatrix} 0\\ 1\\ 0 \end{bmatrix} I'm guessing this comes from the zero row in my $\mathrm{Rref}(A)$. But, why is this done? I thought the number of bases came from the number of independent variables in the $\mathrm{Rref}(A)$...","Find a basis for the range and kernel of $T$. $$A =\begin{bmatrix} 2 & 0 & -1\\ 4 & 0 &  -2\\ 0 & 0 & 0 \end{bmatrix}  $$ Attempt at Solving for Basis of Range: On finding the basis for the range, I know that the range is the same thing as the column space. So, finding the basis of the column space should be equivalent to finding the basis of the range. I got the following after reducing: $$\mathrm{Rref}(A) =\begin{bmatrix} 1 & 0 & -\frac 12\\ 0 & 0 &  0\\ 0 & 0 & 0 \end{bmatrix} $$ Because I only have a pivot in column $1$, my corresponding column in the original $A$ (thus my basis for the range), would be: $$\mathrm{Rref}(A) =\begin{bmatrix} 2\\ 4\\ 0 \end{bmatrix} $$ The solution given in the text says that this should actually be: $$\mathrm{Rref}(A) =\begin{bmatrix} 1\\ 2\\ 0 \end{bmatrix}$$ I can see that the book solution only differs from mine in that their solution seems to have been divided by $2$. However, my question is - why are they dividing by 2? I don't understand why the original column is being altered. Attempt at Solving for Basis of Kernel: In solving for the kernel, I know that the basis of the kernel should be the same as the basis for the nullspace.  From the $\mathrm{Rref}(A)$ above, I got the following equation: $(X_1) = (\frac12X_3)$. Letting $X_3$ equal one, I got the following matrix for the basis of the kernel: $$\text{Basis of Kernel} =\begin{bmatrix} \frac12\\ 0\\ 1 \end{bmatrix}$$ This answer checks out with my solution in the text, but the text also provides the following solution:  =\begin{bmatrix} 0\\ 1\\ 0 \end{bmatrix} I'm guessing this comes from the zero row in my $\mathrm{Rref}(A)$. But, why is this done? I thought the number of bases came from the number of independent variables in the $\mathrm{Rref}(A)$...",,['linear-algebra']
38,Are two matrices of the same rank similar?,Are two matrices of the same rank similar?,,"I know that if two matrices $A$ and $B$ are similar implies that they have the same rank. However, if they have the same rank are they similar?","I know that if two matrices $A$ and $B$ are similar implies that they have the same rank. However, if they have the same rank are they similar?",,"['linear-algebra', 'matrices']"
39,diagonalize quadratic form,diagonalize quadratic form,,"I have this quadratic form $Q= x^2 + 4y^2 + 9z^2 + 4xy + 6xz+ 12yz$ And they ask me: for which values of $x,y$ and $z$ is $Q=0$? and I have to diagonalize also the quadratic form. I calculated the eigenvalues: $k_{1}=0=k_{2}, k_{3}=14$, and the eigenvector $v_{1}=(-2,1,0), v_{2}=(1,2,3), v_{3}=(3,6,-5)$ I don't know if this is usefull in order to diagonalize or to see when is $Q=0$","I have this quadratic form $Q= x^2 + 4y^2 + 9z^2 + 4xy + 6xz+ 12yz$ And they ask me: for which values of $x,y$ and $z$ is $Q=0$? and I have to diagonalize also the quadratic form. I calculated the eigenvalues: $k_{1}=0=k_{2}, k_{3}=14$, and the eigenvector $v_{1}=(-2,1,0), v_{2}=(1,2,3), v_{3}=(3,6,-5)$ I don't know if this is usefull in order to diagonalize or to see when is $Q=0$",,"['linear-algebra', 'matrices', 'quadratic-forms', 'diagonalization']"
40,"Given $B \in M_{n\times n}(\mathbb R)$ is invertible and $B^2+B^4+B^7 = I$, find an expression for $B^{-1}$ in terms of only $B$.","Given  is invertible and , find an expression for  in terms of only .",B \in M_{n\times n}(\mathbb R) B^2+B^4+B^7 = I B^{-1} B,"Given $B \in M_{n\times n}(\mathbb R)$ is invertible and $B^2+B^4+B^7=I$, find an expression for $B^{-1}$ in terms of only $B$. I don't know where to start. Thanks in advance.","Given $B \in M_{n\times n}(\mathbb R)$ is invertible and $B^2+B^4+B^7=I$, find an expression for $B^{-1}$ in terms of only $B$. I don't know where to start. Thanks in advance.",,"['linear-algebra', 'matrices']"
41,Proving that a set over a field is a vector space,Proving that a set over a field is a vector space,,"Given: $S$ is a nonempty set, $K$ is a field. Let $C(S, K)$ denote the set of all functions ${f}\in\ C(S,K)$ such that ${f}(s) = 0 $ for all but a finite number of elements of $S$ . Prove that $C(S, K)$ is a vector space. OK. I was thinking about using the simple additive axioms that define vector spaces. One of those is that there exist two elements such that $x$ (which is some vector) added to zero equals $x$ , or $x + 0 = x$ . Let $g(s)$ be an arbitrary function. $f + g = g$ when $f(s) = 0$ . In addition, if we assume $g(s)$ to be in the space $C(S,K)$ and $f + g = g$ then both vectors are in the space $C(S,K)$ and are closed under addition. Am I on the right track here? I feel like there's another step I need to have.","Given: is a nonempty set, is a field. Let denote the set of all functions such that for all but a finite number of elements of . Prove that is a vector space. OK. I was thinking about using the simple additive axioms that define vector spaces. One of those is that there exist two elements such that (which is some vector) added to zero equals , or . Let be an arbitrary function. when . In addition, if we assume to be in the space and then both vectors are in the space and are closed under addition. Am I on the right track here? I feel like there's another step I need to have.","S K C(S, K) {f}\in\ C(S,K) {f}(s) = 0  S C(S, K) x x x + 0 = x g(s) f + g = g f(s) = 0 g(s) C(S,K) f + g = g C(S,K)","['linear-algebra', 'vector-spaces']"
42,Prove that $T$ is an orthogonal projection,Prove that  is an orthogonal projection,T,Let $T$ be a linear operator on a finite-dimensional inner product space $V$. Suppose that $T$ is a projection such that $\|T(x)\| \le \|x\|$ for $x \in V$. Prove that $T$ is an orthogonal projection. I can't understand well. The definition of orthogonal operator is $\|T(x)\| = \|x\|$. But why that $\|T(x)\| \le \|x\|$ for $x \in V$ means orthogonal projection?,Let $T$ be a linear operator on a finite-dimensional inner product space $V$. Suppose that $T$ is a projection such that $\|T(x)\| \le \|x\|$ for $x \in V$. Prove that $T$ is an orthogonal projection. I can't understand well. The definition of orthogonal operator is $\|T(x)\| = \|x\|$. But why that $\|T(x)\| \le \|x\|$ for $x \in V$ means orthogonal projection?,,"['linear-algebra', 'operator-theory', 'inner-products']"
43,Rank of idempotent matrices,Rank of idempotent matrices,,"Let $B_1, B_2, \dots, B_k$ be idempotent matrices, i.e., $B_i^2=B_i$. Can we prove that $$\mbox{rank}(I-B_1\cdots B_k)\leq \sum\limits_{i=1}^k \mathrm{rank}(I-B_i)$$ where $I$ is the identity matrix?","Let $B_1, B_2, \dots, B_k$ be idempotent matrices, i.e., $B_i^2=B_i$. Can we prove that $$\mbox{rank}(I-B_1\cdots B_k)\leq \sum\limits_{i=1}^k \mathrm{rank}(I-B_i)$$ where $I$ is the identity matrix?",,"['linear-algebra', 'inequality', 'matrix-rank', 'idempotents', 'projection-matrices']"
44,Orthogonal complement of the diagonal matrices in the inner product space of matrices,Orthogonal complement of the diagonal matrices in the inner product space of matrices,,"$V$ is the matrices space (scalar over the complex). definition of inner product space is: $(A,B)=tr(AB^*)$. $A$,$B$ matrices. assuming $D$ is the subspace of all Diagonal matrices. I need to find the subspace that each matrix $B$ in it $(A,B)=0$. $A$ belongs to $D$. $[A]_{i,j}=a_{i,j},\quad   [B]_{i,j}=b_{i,j}$ so I figure it out that the $tr(AB^*)= \sigma(a_{i,i}c_{i,i}),\quad  1\leq i\leq n$ while $c$ is the conjugation of $b$. but compare it to zero still do not give any information what $c_{i,i}$ is.","$V$ is the matrices space (scalar over the complex). definition of inner product space is: $(A,B)=tr(AB^*)$. $A$,$B$ matrices. assuming $D$ is the subspace of all Diagonal matrices. I need to find the subspace that each matrix $B$ in it $(A,B)=0$. $A$ belongs to $D$. $[A]_{i,j}=a_{i,j},\quad   [B]_{i,j}=b_{i,j}$ so I figure it out that the $tr(AB^*)= \sigma(a_{i,i}c_{i,i}),\quad  1\leq i\leq n$ while $c$ is the conjugation of $b$. but compare it to zero still do not give any information what $c_{i,i}$ is.",,['linear-algebra']
45,"Problem related with a linear transformation and its rank,nullity...","Problem related with a linear transformation and its rank,nullity...",,I am stuck on the following problem: Suppose $W$ be  a vector space of dimension $m \geq 2.$ Let $F \colon W \to W$ be a linear transformation such that $F^{n+1}=0$ and $F^n \ne 0$ for some $n \ge 1.$ Then I have to check which of  the following statement(s) is/are true? 1. $\text{trace}(F) \ne 0 $ 2. $\text{Rank}(F^n) \le \text{Nullity}(F^n)$ 3. $n=m$ 4. $F$ is diagonalizable. I do not know how to progress with the problem. Any kind of hints with some explanation will be helpful.,I am stuck on the following problem: Suppose be  a vector space of dimension Let be a linear transformation such that and for some Then I have to check which of  the following statement(s) is/are true? 1. 2. 3. 4. is diagonalizable. I do not know how to progress with the problem. Any kind of hints with some explanation will be helpful.,W m \geq 2. F \colon W \to W F^{n+1}=0 F^n \ne 0 n \ge 1. \text{trace}(F) \ne 0  \text{Rank}(F^n) \le \text{Nullity}(F^n) n=m F,[]
46,Primary Decomposition Theorem: What are the characteristic polynomials of the maps of the decomposition?,Primary Decomposition Theorem: What are the characteristic polynomials of the maps of the decomposition?,,"Let $T$ be an operator on a finite dimensional vector space $V$. Suppose that the characteristic polynomial of $T$ is $$\chi(t)=f_1^{n_1}(t)\cdots f_k^{n_k}(t)$$ where $f_1,\ldots,f_k$ are distinct irreducible polynomials, and suppose the minimal polynomial of $T$ is  $$m(t)=f_1^{m_1}(t)\cdots f_k^{m_k}(t)$$ Then, by the Primary Decomposition Theorem, we get that $$V=W_1\oplus\cdots\oplus W_k$$ where $W_i:=\ker(f_i^{m_i}(T))$. Moreover, we have  $$T=T_1\oplus\cdots\oplus T_k$$ where $T_i:=T|_{W_i}$, and the minimal polynomial of $T_i$ is $f_i^{m_i}(t)$. Now, here is my Question: How can we show that the characteristic polynomial of $T_i$ is $f_i^{n_i}(t)$?","Let $T$ be an operator on a finite dimensional vector space $V$. Suppose that the characteristic polynomial of $T$ is $$\chi(t)=f_1^{n_1}(t)\cdots f_k^{n_k}(t)$$ where $f_1,\ldots,f_k$ are distinct irreducible polynomials, and suppose the minimal polynomial of $T$ is  $$m(t)=f_1^{m_1}(t)\cdots f_k^{m_k}(t)$$ Then, by the Primary Decomposition Theorem, we get that $$V=W_1\oplus\cdots\oplus W_k$$ where $W_i:=\ker(f_i^{m_i}(T))$. Moreover, we have  $$T=T_1\oplus\cdots\oplus T_k$$ where $T_i:=T|_{W_i}$, and the minimal polynomial of $T_i$ is $f_i^{m_i}(t)$. Now, here is my Question: How can we show that the characteristic polynomial of $T_i$ is $f_i^{n_i}(t)$?",,['linear-algebra']
47,"Prove T is normal if and only if T = T1 + iT2, where T1 and T2 are selfadjoint operators which commute.","Prove T is normal if and only if T = T1 + iT2, where T1 and T2 are selfadjoint operators which commute.",,"Got this question for homework, im having troubles to prove one side of it The question: Prove T is normal if and only if  T = T1 + iT2, where T1 and T2 are selfadjoint operators which commute. $<=$ lets assume we have T1, and T2 such as mentioned, so $\ TT$*$=(T1 + iT2)(T1 + iT2)$* = ... = $(T1+iT2)$*$(T1+iT2)$$=T$*$T$ $=>$ having troubles with that part... any clue will help thanks!","Got this question for homework, im having troubles to prove one side of it The question: Prove T is normal if and only if  T = T1 + iT2, where T1 and T2 are selfadjoint operators which commute. $<=$ lets assume we have T1, and T2 such as mentioned, so $\ TT$*$=(T1 + iT2)(T1 + iT2)$* = ... = $(T1+iT2)$*$(T1+iT2)$$=T$*$T$ $=>$ having troubles with that part... any clue will help thanks!",,['linear-algebra']
48,Show that a matrix is nilpotent.,Show that a matrix is nilpotent.,,"Let $A,B \in \mathbb{M}_n (\mathbb{C})$. If $A^2B + BA^2 = 2ABA$ then exist $k \in \mathbb{N}$ where $(AB-BA)^k = 0$. I tried solve with minimal polynomial, but I did not have much effect.","Let $A,B \in \mathbb{M}_n (\mathbb{C})$. If $A^2B + BA^2 = 2ABA$ then exist $k \in \mathbb{N}$ where $(AB-BA)^k = 0$. I tried solve with minimal polynomial, but I did not have much effect.",,"['linear-algebra', 'matrices']"
49,Steinitz exchange lemma,Steinitz exchange lemma,,"How can I show this? if $b_{1}, ..., b_{n+1}$ are linears combinations of $a_{1}, ..., a_{n}$ then $b_{1}, ..., b_{n+1}$ are linearly dependents. In my textbook they call it Steinitz lemma. I wonder if is it equivalent this? http://en.wikipedia.org/wiki/Steinitz_exchange_lemma","How can I show this? if $b_{1}, ..., b_{n+1}$ are linears combinations of $a_{1}, ..., a_{n}$ then $b_{1}, ..., b_{n+1}$ are linearly dependents. In my textbook they call it Steinitz lemma. I wonder if is it equivalent this? http://en.wikipedia.org/wiki/Steinitz_exchange_lemma",,['linear-algebra']
50,Two different definitions of vector space,Two different definitions of vector space,,"I have two different linear algebra books and realized that the definitions of vector space on them are slightly different. One of the definition has the following statement for the condition of scalar multiplication and the other does not: ""For all $a,b \in \mathbb F, u \in V$ implies $(ab)u=a(bu)$."" I cannot derive it from the other conditions in the definition of vector space nor I can give the example that satisfies other definitions of vector spaces but not this particular one. http://en.wikipedia.org/wiki/Vector_space#Definition One of my textbooks has the same definition as Wikipedia. and the definition on the other book is the same as this except that it does not have ""For all a,bF,uV implies (ab)u=a(bu)."" Would you help me figure out if the two definitions are the same or not? Thanks in advance.","I have two different linear algebra books and realized that the definitions of vector space on them are slightly different. One of the definition has the following statement for the condition of scalar multiplication and the other does not: ""For all $a,b \in \mathbb F, u \in V$ implies $(ab)u=a(bu)$."" I cannot derive it from the other conditions in the definition of vector space nor I can give the example that satisfies other definitions of vector spaces but not this particular one. http://en.wikipedia.org/wiki/Vector_space#Definition One of my textbooks has the same definition as Wikipedia. and the definition on the other book is the same as this except that it does not have ""For all a,bF,uV implies (ab)u=a(bu)."" Would you help me figure out if the two definitions are the same or not? Thanks in advance.",,['linear-algebra']
51,"Showing that if $fg=gf$ and $fh=hf$, then $gh=hg$, where $f$, $g$, and $h$ are affine functions","Showing that if  and , then , where , , and  are affine functions",fg=gf fh=hf gh=hg f g h,"Given real numbers $a$ and $b$ ($a \ne 0$), let $f_{a,b}$ be the function $\mathbb{R} \to \mathbb{R}$ defined by $x \mapsto ax+b$.  The set of such functions is a permutation group on $\mathbb{R}$, under function composition. Let $f,g,h, \in G$, where $f$ is not the identity.  If $f$ commutes with both $g$ and $h$, show that $g$ and $h$ commute with each other. (Problem from I.M. Isaacs) I think I have proved this, but I'd like to be sure. Let $f(x)=ax+b$, $g(x)=cx+d$, and $h(x)=mx+n$.  We have the following: $$f(g(x))=acx+ad+b,\quad g(f(x))=acx+bc+d$$ For these two functions to be equal, therefore, we must have that $ad+b=bc+d$.  Am I correct in thinking that this further implies that $a,c=1$? If so, then by essentially the same argument we also obtain $a,m=1$, and so $c,m=1$, which would imply that $gh=hg$ (since $g(h(x))=cmx+cn+d$ and $h(g(x))=cmx+md+n$). Is this an effective proof?  It's clear that, for example in the first case, $a,c=1$ implies $ad+b=bc+d$, but it's not completely clear to me that $ad+b=bc+d$ necessarily implies $a,c=1$. Thanks.","Given real numbers $a$ and $b$ ($a \ne 0$), let $f_{a,b}$ be the function $\mathbb{R} \to \mathbb{R}$ defined by $x \mapsto ax+b$.  The set of such functions is a permutation group on $\mathbb{R}$, under function composition. Let $f,g,h, \in G$, where $f$ is not the identity.  If $f$ commutes with both $g$ and $h$, show that $g$ and $h$ commute with each other. (Problem from I.M. Isaacs) I think I have proved this, but I'd like to be sure. Let $f(x)=ax+b$, $g(x)=cx+d$, and $h(x)=mx+n$.  We have the following: $$f(g(x))=acx+ad+b,\quad g(f(x))=acx+bc+d$$ For these two functions to be equal, therefore, we must have that $ad+b=bc+d$.  Am I correct in thinking that this further implies that $a,c=1$? If so, then by essentially the same argument we also obtain $a,m=1$, and so $c,m=1$, which would imply that $gh=hg$ (since $g(h(x))=cmx+cn+d$ and $h(g(x))=cmx+md+n$). Is this an effective proof?  It's clear that, for example in the first case, $a,c=1$ implies $ad+b=bc+d$, but it's not completely clear to me that $ad+b=bc+d$ necessarily implies $a,c=1$. Thanks.",,"['linear-algebra', 'group-theory', 'affine-geometry']"
52,Are there $0$-$1$-matrices that are not unimodular?,Are there --matrices that are not unimodular?,0 1,"I am just wondering if there are matrices that only consists of $0$ s and a few $1$ s that are not totally unimodular (TU)? I cannot come up with an example but I am not very experienced with this stuff. In a specific case, if I have a very sparse $0$ - $1$ matrix where every row consist of at most two $1$ s and every column consists of at most three $1$ s, how can I find out whether this matrix is TU? I cannot use the four sufficient conditions for $A$ to be totally unimodular (where $B$ , $C$ is a disjoint partition of the rows of $A$ ), because condition $3$ might be violated: Every column of contains at most two non-zero entries; Every entry in is $0, +1$ , or $1$ ; If two non-zero entries in a column of have the same sign, then the row of one is in $B$ , and the other in $C$ ; If two non-zero entries in a column of have opposite signs, then the rows of both are in $B$ , or both in $C$ . Thank you!","I am just wondering if there are matrices that only consists of s and a few s that are not totally unimodular (TU)? I cannot come up with an example but I am not very experienced with this stuff. In a specific case, if I have a very sparse - matrix where every row consist of at most two s and every column consists of at most three s, how can I find out whether this matrix is TU? I cannot use the four sufficient conditions for to be totally unimodular (where , is a disjoint partition of the rows of ), because condition might be violated: Every column of contains at most two non-zero entries; Every entry in is , or ; If two non-zero entries in a column of have the same sign, then the row of one is in , and the other in ; If two non-zero entries in a column of have opposite signs, then the rows of both are in , or both in . Thank you!","0 1 0 1 1 1 A B C A 3 0, +1 1 B C B C","['linear-algebra', 'discrete-mathematics', 'linear-programming', 'integer-programming']"
53,Can a generalized eigenvectors be generated by a scalar which is not an eigenvalue?,Can a generalized eigenvectors be generated by a scalar which is not an eigenvalue?,,"I have gone through the definition of generalized eigenvectors. It mentions a scalar value only (not an eigenvalue). Can a scalar other than an eigenvalue generate generalized eigenvectors? In other words, for the equation $(A - \lambda I)^kx = 0$, for a solution $x$, is it possible that $\lambda$ is not an eigenvalue?","I have gone through the definition of generalized eigenvectors. It mentions a scalar value only (not an eigenvalue). Can a scalar other than an eigenvalue generate generalized eigenvectors? In other words, for the equation $(A - \lambda I)^kx = 0$, for a solution $x$, is it possible that $\lambda$ is not an eigenvalue?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
54,Determine a formula for a dual basis.,Determine a formula for a dual basis.,,"Let $\beta= \{ (2,1),(3,1) \} $ be an ordered basis for $\Bbb R^2$. Suppose that the dual basis of $\beta$ is given by $\beta^*= \{f_1,f_2 \} $ To explicitly determine a formula for $f_1$ we need to consider the equations $$1=f_1(2,1)=f_1(2e_1+e_2)=2f_1(e_1)+f_1(e_2)$$ $$0=f_1(3,1)=f_1(3e_1+e_2)=3f_1(e_1)+f_1(e_2)$$ Solving this equations, we obtain $f_1(e_1)=-1$ and $f_1(e_2)=3$, that is $f_1(x,y)=-x+3y$. My question is why do we need to solve the above equations for 1 and 0 respectively?","Let $\beta= \{ (2,1),(3,1) \} $ be an ordered basis for $\Bbb R^2$. Suppose that the dual basis of $\beta$ is given by $\beta^*= \{f_1,f_2 \} $ To explicitly determine a formula for $f_1$ we need to consider the equations $$1=f_1(2,1)=f_1(2e_1+e_2)=2f_1(e_1)+f_1(e_2)$$ $$0=f_1(3,1)=f_1(3e_1+e_2)=3f_1(e_1)+f_1(e_2)$$ Solving this equations, we obtain $f_1(e_1)=-1$ and $f_1(e_2)=3$, that is $f_1(x,y)=-x+3y$. My question is why do we need to solve the above equations for 1 and 0 respectively?",,['linear-algebra']
55,Find the necessary and sufficient conditions on $A$ such that $\|T(\vec{x})\|=|\det A|\cdot\|\vec{x}\|$ for all $\vec{x}$.,Find the necessary and sufficient conditions on  such that  for all .,A \|T(\vec{x})\|=|\det A|\cdot\|\vec{x}\| \vec{x},Consider the mapping $T:\mathbb{R}^n\mapsto\mathbb{R}^n$ defined by $T(\vec{x})=A\vec{x}$ where $A$ is a $n\times n$ matrix. Find the necessary and sufficient conditions on $A$ such that $\|T(\vec{x})\|=|\det A|\cdot\|\vec{x}\|$ for all $\vec{x}$. Here $\|\cdot\|$ denotes the norm.,Consider the mapping $T:\mathbb{R}^n\mapsto\mathbb{R}^n$ defined by $T(\vec{x})=A\vec{x}$ where $A$ is a $n\times n$ matrix. Find the necessary and sufficient conditions on $A$ such that $\|T(\vec{x})\|=|\det A|\cdot\|\vec{x}\|$ for all $\vec{x}$. Here $\|\cdot\|$ denotes the norm.,,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'transformation', 'vector-analysis']"
56,Properties of $\det$ and $\operatorname{trace}$ given a $4\times 4$ real valued matrix,Properties of  and  given a  real valued matrix,\det \operatorname{trace} 4\times 4,"Let $A$, be a real $4 \times 4$ matrix such that $-1,1,2,-2$ are its eigenvalues. If $B=A^4-5A^2+5I$, then which of the following are true? $\det(A+B)=0$ $\det (B)=1$ $\operatorname{trace}(A-B)=0 $ $\operatorname{trace}(A+B)=4$ Using Cayley-Hamilton I get $B=I$, and I know that $\operatorname{trace}(A+B)=\operatorname{trace}(A)+\operatorname{trace}(B)$. From these facts we can obtain easily about 2,3,4 but I am confused in 1. How can I verify (1)? Thanks for your help.","Let $A$, be a real $4 \times 4$ matrix such that $-1,1,2,-2$ are its eigenvalues. If $B=A^4-5A^2+5I$, then which of the following are true? $\det(A+B)=0$ $\det (B)=1$ $\operatorname{trace}(A-B)=0 $ $\operatorname{trace}(A+B)=4$ Using Cayley-Hamilton I get $B=I$, and I know that $\operatorname{trace}(A+B)=\operatorname{trace}(A)+\operatorname{trace}(B)$. From these facts we can obtain easily about 2,3,4 but I am confused in 1. How can I verify (1)? Thanks for your help.",,['linear-algebra']
57,Let T be a linear transformation on the real vector space $\mathbb{R^n}$ over $\mathbb R$ such that $T ^2 =T$ for same $\mathbb R$ [duplicate],Let T be a linear transformation on the real vector space  over  such that  for same  [duplicate],\mathbb{R^n} \mathbb R T ^2 =T \mathbb R,"This question already has answers here : Linear transformation $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ such that $T^{2}=\lambda T.$ [closed] If $T\colon \mathbb R^n \to \mathbb R^n $ linear and $T^2 = kT$   [closed] (3 answers) Closed 6 years ago . Let $T$ be a linear transformation on the real vector space $\mathbb R^n$ over $\mathbb R$ such that $T^2 =\mu T$ for some $\mu\in\mathbb R$ . Then which of the following is/are true? $\|Tx\| = |\mu| \|x\|$ for all $x \in\mathbb {R^n}$ If $\|Tx\| =  \| x\| $for some non zero vector $x \in\mathbb R^n$, then $\mu=\pm1$ $T= \mu I$ where $I$ is the identity transformation on $\mathbb R^n$ If $\|Tx \|>\|x\|$ for a non zero vector $x \in \mathbb R^n$, then $T$ is necessarily singular. I am completely stuck on it. Can anybody help me please?","This question already has answers here : Linear transformation $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ such that $T^{2}=\lambda T.$ [closed] If $T\colon \mathbb R^n \to \mathbb R^n $ linear and $T^2 = kT$   [closed] (3 answers) Closed 6 years ago . Let $T$ be a linear transformation on the real vector space $\mathbb R^n$ over $\mathbb R$ such that $T^2 =\mu T$ for some $\mu\in\mathbb R$ . Then which of the following is/are true? $\|Tx\| = |\mu| \|x\|$ for all $x \in\mathbb {R^n}$ If $\|Tx\| =  \| x\| $for some non zero vector $x \in\mathbb R^n$, then $\mu=\pm1$ $T= \mu I$ where $I$ is the identity transformation on $\mathbb R^n$ If $\|Tx \|>\|x\|$ for a non zero vector $x \in \mathbb R^n$, then $T$ is necessarily singular. I am completely stuck on it. Can anybody help me please?",,"['linear-algebra', 'linear-transformations']"
58,Conditions for matrix similarity,Conditions for matrix similarity,,"Two things that are not clear to me from the Wikipedia page on ""Matrix similarity"": If the geometric multiplicity of an eigenvalue is different in two matrices $A$ and $B$ then $A$ and $B$ are not similar? If all eigenvalues of $A$ and $B$ coincide, together with their algebraic and geometric multiplicities, then $A$ and $B$ are similar? Thanks!","Two things that are not clear to me from the Wikipedia page on ""Matrix similarity"": If the geometric multiplicity of an eigenvalue is different in two matrices $A$ and $B$ then $A$ and $B$ are not similar? If all eigenvalues of $A$ and $B$ coincide, together with their algebraic and geometric multiplicities, then $A$ and $B$ are similar? Thanks!",,['linear-algebra']
59,Relation between Rayleigh quotient and eigenvalues for non-Hermitian matrices?,Relation between Rayleigh quotient and eigenvalues for non-Hermitian matrices?,,"For a Hermitian matrix, its eigenvalues can be determined from its Rayleigh quotient via the min-max theorem . Are there generalizations of such relation to a non-Hermitian matrix? Note that the Rayleigh quotient itself does not require the matrix to be Hermitian, but the eigenvalues of a square matrix and of its Hermitian part may not be the same. Thanks and regards!","For a Hermitian matrix, its eigenvalues can be determined from its Rayleigh quotient via the min-max theorem . Are there generalizations of such relation to a non-Hermitian matrix? Note that the Rayleigh quotient itself does not require the matrix to be Hermitian, but the eigenvalues of a square matrix and of its Hermitian part may not be the same. Thanks and regards!",,"['linear-algebra', 'matrices']"
60,"Show that $DF(x,y)$ is invertible in a dense and open subset of $\mathbb {R^2}$",Show that  is invertible in a dense and open subset of,"DF(x,y) \mathbb {R^2}","My problem is the following: Let $p$ be a non constant polynomial over $\mathbb {R}$ and define $F(x,y)=(p(x+y),p(x-y))$.Show that $DF(x,y)$  is invertible in a dense and open subset of $\mathbb {R^2}$. I've been thinking a lot on this one, but couldn't get far... I'm really stuck... any help is much appreciated!","My problem is the following: Let $p$ be a non constant polynomial over $\mathbb {R}$ and define $F(x,y)=(p(x+y),p(x-y))$.Show that $DF(x,y)$  is invertible in a dense and open subset of $\mathbb {R^2}$. I've been thinking a lot on this one, but couldn't get far... I'm really stuck... any help is much appreciated!",,"['linear-algebra', 'analysis', 'multivariable-calculus']"
61,Triangle equality implies vector dependence.,Triangle equality implies vector dependence.,,"I am trying to prove this statement: Show that if $x$ and $y$ are two vectors in an inner product space such that $||x+y||=||x||+||y||$, then $x$ and $y$ are linearly dependent. Squaring the equality I get $$\langle x+y,x+y\rangle=\langle x,x\rangle +2||x||\cdot||y||+\langle y,y\rangle $$ then, using linearity of the inner product I get $$ \langle x,x\rangle +\langle y,y\rangle+\langle x,y\rangle+\langle y,x\rangle=\langle x,x\rangle +2||x||\cdot||y||+\langle y,y\rangle $$ After all the cancellation I finally arrive at $$ \mathrm{Re}\langle x,y\rangle=||x||\cdot||y|| $$ This looks like Cauchy-Schwarz inequality, so the only thing left to show is that $\mathrm{Re}\langle x,y\rangle=|\langle x,y\rangle|$, how can I do that?","I am trying to prove this statement: Show that if $x$ and $y$ are two vectors in an inner product space such that $||x+y||=||x||+||y||$, then $x$ and $y$ are linearly dependent. Squaring the equality I get $$\langle x+y,x+y\rangle=\langle x,x\rangle +2||x||\cdot||y||+\langle y,y\rangle $$ then, using linearity of the inner product I get $$ \langle x,x\rangle +\langle y,y\rangle+\langle x,y\rangle+\langle y,x\rangle=\langle x,x\rangle +2||x||\cdot||y||+\langle y,y\rangle $$ After all the cancellation I finally arrive at $$ \mathrm{Re}\langle x,y\rangle=||x||\cdot||y|| $$ This looks like Cauchy-Schwarz inequality, so the only thing left to show is that $\mathrm{Re}\langle x,y\rangle=|\langle x,y\rangle|$, how can I do that?",,['linear-algebra']
62,Value of Logarithm of negative number,Value of Logarithm of negative number,,Why the logarithmic value of negative number can't be define? Is there any special reason?,Why the logarithmic value of negative number can't be define? Is there any special reason?,,"['linear-algebra', 'logarithms']"
63,Affine Spaces and Affine transformations,Affine Spaces and Affine transformations,,Can anyone please recommend a book that describes Affine Spaces and Affine Transformations? Many books i saw described it very briefly. Can anyone please suggest a book that deals with it in detail? Thanks!,Can anyone please recommend a book that describes Affine Spaces and Affine Transformations? Many books i saw described it very briefly. Can anyone please suggest a book that deals with it in detail? Thanks!,,"['linear-algebra', 'reference-request', 'affine-geometry']"
64,How to motivate inner product spaces,How to motivate inner product spaces,,"What is the most motivating way to introduce general inner product spaces? I am looking for examples which have a real impact. For Euclidean spaces we relate the dot product to the angle between the vectors which most people find tangible. How can we extend this idea to the inner product of general vectors spaces such as the set of matrices, polynomials, functions?","What is the most motivating way to introduce general inner product spaces? I am looking for examples which have a real impact. For Euclidean spaces we relate the dot product to the angle between the vectors which most people find tangible. How can we extend this idea to the inner product of general vectors spaces such as the set of matrices, polynomials, functions?",,"['linear-algebra', 'soft-question']"
65,Question regarding technicalities in the paper Iterated Prisoners Dilemma contains strategies that dominate any evolutionary opponent,Question regarding technicalities in the paper Iterated Prisoners Dilemma contains strategies that dominate any evolutionary opponent,,"For people on this board I have a probably pretty modest question, but since I'm not a mathematician (just an economist), I'm having trouble. The full pdf can be found here: http://www.pnas.org/content/early/2012/05/16/1206569109.full.pdf+html The question is regarding the following passage and has to do with linear algebra. They write: ""where Adj(M) is the adjugate matrix (also known as the classical adjoint or, as in high-school algebra, the matrix of minors). Eq. 2 implies that every row of Adj(M) is proportional to v. Choosing the fourth row, we see that the components of v are (up to a sign) the determinants of the 3  3 matrices formed from the first three columns of M, leaving out each one of the four rows in turn. These determinants are unchanged if we add the first column of M into the second and third columns. The result of these manipulations is a formula for the dot product of an arbitrary four-vector f with the stationary vector v of the Markov matrix, v  f  D(p; q; f), where D is the 4  4 determinant shown explicitly in Fig. 2B. This result follows from expanding the determinant by minors on its fourth column and noting that the 3  3 determinants multiplying each fi are just the ones described above."" To understand the full context you will probably have to read the beginning of the passage, which is also very short. Yet my question is specifically regarding the formulated relationship between the stationary vector v of the Markov transition-matrix M and the Adj(M'), which is Adj.(M-I). As they say: Every row of Adj.(M') is proportional to v, which is sort of intuitive looking at Eq. 2, but I simply do not understand how they got that. Also the immediately following conclusion that the elements of v are the 3x3 column determinants of M' if you were to eliminate from the bottom of the fourth column. Also to point out a petty mistake but  v  f  D(p; q; f) can't be correct as the dimensions do not link up correctly. v'  f  D(p; q; f) is correct. But yet again I grasp that this formulation makes sense, but fail to understand how this can be arrived at. If you can point me in the direction of a book or can flat-out explain this to me, I would be very obliged. Thanks in advance o_s","For people on this board I have a probably pretty modest question, but since I'm not a mathematician (just an economist), I'm having trouble. The full pdf can be found here: http://www.pnas.org/content/early/2012/05/16/1206569109.full.pdf+html The question is regarding the following passage and has to do with linear algebra. They write: ""where Adj(M) is the adjugate matrix (also known as the classical adjoint or, as in high-school algebra, the matrix of minors). Eq. 2 implies that every row of Adj(M) is proportional to v. Choosing the fourth row, we see that the components of v are (up to a sign) the determinants of the 3  3 matrices formed from the first three columns of M, leaving out each one of the four rows in turn. These determinants are unchanged if we add the first column of M into the second and third columns. The result of these manipulations is a formula for the dot product of an arbitrary four-vector f with the stationary vector v of the Markov matrix, v  f  D(p; q; f), where D is the 4  4 determinant shown explicitly in Fig. 2B. This result follows from expanding the determinant by minors on its fourth column and noting that the 3  3 determinants multiplying each fi are just the ones described above."" To understand the full context you will probably have to read the beginning of the passage, which is also very short. Yet my question is specifically regarding the formulated relationship between the stationary vector v of the Markov transition-matrix M and the Adj(M'), which is Adj.(M-I). As they say: Every row of Adj.(M') is proportional to v, which is sort of intuitive looking at Eq. 2, but I simply do not understand how they got that. Also the immediately following conclusion that the elements of v are the 3x3 column determinants of M' if you were to eliminate from the bottom of the fourth column. Also to point out a petty mistake but  v  f  D(p; q; f) can't be correct as the dimensions do not link up correctly. v'  f  D(p; q; f) is correct. But yet again I grasp that this formulation makes sense, but fail to understand how this can be arrived at. If you can point me in the direction of a book or can flat-out explain this to me, I would be very obliged. Thanks in advance o_s",,"['linear-algebra', 'matrices', 'game-theory']"
66,"Eigenvalues, singular values, and the angles between eigenvectors","Eigenvalues, singular values, and the angles between eigenvectors",,"Suppose the $n \times n$ matrix $A$ has eigenvalues $\lambda_1, \ldots, \lambda_n$ and singular values $\sigma_1, \ldots, \sigma_n$. It seems plausible that by comparing the singular values and eigenvalues we gets some sort of information about eigenvectors. Consider: a. The singular values are equal to the absolute values of eigenvalues if and only if the matrix is normal, i.e., the eigenvectors are orthogonal (see http://en.wikipedia.org/wiki/Normal_matrix , item 11 of the ""Equivalent definitions"" section ). b. Suppose we have two distinct eigenvalues $\lambda_1, \lambda_2$ with eigenvectors $v_1, v_2$. Suppose, hypothetically, we let $v_1$ approach $v_2$, while keeping all the other eigenvalues and eigenvectors the same. Then the largest singular value approaches infinity. This follows since $\sigma_{\rm max} = ||A||_2$ and $A$ maps the vector $v_1 - v_2$, which approaches $0$, to $\lambda_1 v_1 - \lambda_2 v_2$, which does not approach $0$. It seems reasonable to guess that the ``more equal'' $|\lambda_1|, \ldots, |\lambda_n|$ and $\sigma_1, \ldots, \sigma_n$ are, the more the eigenvectors look like an orthogonal collection.  So naturally my question is whether there is a formal statement to this effect.","Suppose the $n \times n$ matrix $A$ has eigenvalues $\lambda_1, \ldots, \lambda_n$ and singular values $\sigma_1, \ldots, \sigma_n$. It seems plausible that by comparing the singular values and eigenvalues we gets some sort of information about eigenvectors. Consider: a. The singular values are equal to the absolute values of eigenvalues if and only if the matrix is normal, i.e., the eigenvectors are orthogonal (see http://en.wikipedia.org/wiki/Normal_matrix , item 11 of the ""Equivalent definitions"" section ). b. Suppose we have two distinct eigenvalues $\lambda_1, \lambda_2$ with eigenvectors $v_1, v_2$. Suppose, hypothetically, we let $v_1$ approach $v_2$, while keeping all the other eigenvalues and eigenvectors the same. Then the largest singular value approaches infinity. This follows since $\sigma_{\rm max} = ||A||_2$ and $A$ maps the vector $v_1 - v_2$, which approaches $0$, to $\lambda_1 v_1 - \lambda_2 v_2$, which does not approach $0$. It seems reasonable to guess that the ``more equal'' $|\lambda_1|, \ldots, |\lambda_n|$ and $\sigma_1, \ldots, \sigma_n$ are, the more the eigenvectors look like an orthogonal collection.  So naturally my question is whether there is a formal statement to this effect.",,['linear-algebra']
67,Does the identity $\det(I+g^{-1})\det(I+g)=|\det(g-I)|^2$ hold for $g \in U(n)$?,Does the identity  hold for ?,\det(I+g^{-1})\det(I+g)=|\det(g-I)|^2 g \in U(n),"In a paper (corollary 1, p.14) the following identity is used: Let g be a unitary matrix. Then: $$\det(I+g^{-1})\det(I+g)=|\det(g-I)|^2 \text{ for }g \in U(n)$$ Now my question is why this holds I calculated: $$\det(I+g^{-1})\det(I+g)=\overline{\det(I+g^t)}\det(I+g)=\overline{\det(I+g)}\det(I+g)=|\det(I+g)|^2$$ Where the second equality holds as $I$ has only entries in the diagonal ( $I$ is of course the unit matrix). But this is not the same as on the right side. (I also thought that maybe there was a typo on the left side where should be minus-signs. However in the paper itself it is needed that there are plus-signs.) Thanks for any hints. Edit : This equality was in the scope of an integral: $$\int_{U(n)}\prod_{l=1}^{k}det(I+g^{-1})\prod_{l=1}^{k}\det(I+g)dg=\int_{U(n)}|\det(g-I)|^{2k}dg$$ With a change of variable it was solved with my calculation done above. See Giuseppe's answer.","In a paper (corollary 1, p.14) the following identity is used: Let g be a unitary matrix. Then: Now my question is why this holds I calculated: Where the second equality holds as has only entries in the diagonal ( is of course the unit matrix). But this is not the same as on the right side. (I also thought that maybe there was a typo on the left side where should be minus-signs. However in the paper itself it is needed that there are plus-signs.) Thanks for any hints. Edit : This equality was in the scope of an integral: With a change of variable it was solved with my calculation done above. See Giuseppe's answer.",\det(I+g^{-1})\det(I+g)=|\det(g-I)|^2 \text{ for }g \in U(n) \det(I+g^{-1})\det(I+g)=\overline{\det(I+g^t)}\det(I+g)=\overline{\det(I+g)}\det(I+g)=|\det(I+g)|^2 I I \int_{U(n)}\prod_{l=1}^{k}det(I+g^{-1})\prod_{l=1}^{k}\det(I+g)dg=\int_{U(n)}|\det(g-I)|^{2k}dg,"['linear-algebra', 'matrices', 'determinant']"
68,"How to divide by $(a_1,a_2,a_3)$",How to divide by,"(a_1,a_2,a_3)","I have been searching for an explanation in Howard's Linear Algebra and couldn't find an identical example to the one below. The example tells me that vectors $\boldsymbol{a}_1$, $\boldsymbol{a}_2$ and $\boldsymbol{a}_3$ are: $$\boldsymbol a_1 = (a,0,0)$$ $$\boldsymbol a_2 = (0,a,0)$$ $$\boldsymbol a_3 = (0,0,a)$$ And I have to calculate $\boldsymbol b_1$ using equation: $$\boldsymbol{b}_1 = \frac{2 \pi \, (\boldsymbol a_2 \times \boldsymbol a_3)}{(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)}$$ So far I've only managed to calculate the cross product $(\boldsymbol a_2 \times \boldsymbol a_3)$ using Sarrus' rule and what I get is: $$\boldsymbol{b}_1 = \frac{2 \pi \, \hat{\boldsymbol{i}} \, a^2}{(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)}$$ But now I am stuck as I don't know how to calculate with a $(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)$, as this is first time I've come across something like this. Could you just point me to what to do next, or point me to a good html site as I still want to calculate this myself. Best regards.","I have been searching for an explanation in Howard's Linear Algebra and couldn't find an identical example to the one below. The example tells me that vectors $\boldsymbol{a}_1$, $\boldsymbol{a}_2$ and $\boldsymbol{a}_3$ are: $$\boldsymbol a_1 = (a,0,0)$$ $$\boldsymbol a_2 = (0,a,0)$$ $$\boldsymbol a_3 = (0,0,a)$$ And I have to calculate $\boldsymbol b_1$ using equation: $$\boldsymbol{b}_1 = \frac{2 \pi \, (\boldsymbol a_2 \times \boldsymbol a_3)}{(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)}$$ So far I've only managed to calculate the cross product $(\boldsymbol a_2 \times \boldsymbol a_3)$ using Sarrus' rule and what I get is: $$\boldsymbol{b}_1 = \frac{2 \pi \, \hat{\boldsymbol{i}} \, a^2}{(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)}$$ But now I am stuck as I don't know how to calculate with a $(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)$, as this is first time I've come across something like this. Could you just point me to what to do next, or point me to a good html site as I still want to calculate this myself. Best regards.",,"['linear-algebra', 'cross-product']"
69,Special orthogonal matrix uniquely determined by $n-1 \times n-1$ entries?,Special orthogonal matrix uniquely determined by  entries?,n-1 \times n-1,"For example, consider the specific question: Given $a_{11},a_{12},a_{21},a_{22}$ does that uniquely determine $A=\begin{bmatrix} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23} \\ a_{31}&a_{32}&a_{33} \end{bmatrix}$ where $A\in SO(3)$.","For example, consider the specific question: Given $a_{11},a_{12},a_{21},a_{22}$ does that uniquely determine $A=\begin{bmatrix} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23} \\ a_{31}&a_{32}&a_{33} \end{bmatrix}$ where $A\in SO(3)$.",,"['linear-algebra', 'matrices']"
70,Algorithm for a conjugating matrix?,Algorithm for a conjugating matrix?,,"Suppose that $A_{1},\ldots, A_{k}$ are rational $n\times n$ matrices that generate a finite group.  It is a well-known fact that there is a matrix $T$ such that $T^{-1}A_{1}T,\ldots, T^{-1}A_{k}T$ are integer matrices (and so generate an isomorphic subgroup of $\operatorname{GL}_{n}(\mathbb{Z})$.  Is there an algorithm which takes the given matrices $A_{i}$ and outputs a suitable matrix $T$?","Suppose that $A_{1},\ldots, A_{k}$ are rational $n\times n$ matrices that generate a finite group.  It is a well-known fact that there is a matrix $T$ such that $T^{-1}A_{1}T,\ldots, T^{-1}A_{k}T$ are integer matrices (and so generate an isomorphic subgroup of $\operatorname{GL}_{n}(\mathbb{Z})$.  Is there an algorithm which takes the given matrices $A_{i}$ and outputs a suitable matrix $T$?",,"['linear-algebra', 'group-theory']"
71,Eigenvalues of a matrix and its square,Eigenvalues of a matrix and its square,,"Ok so I messed up my last question, I'll rephrase it: Is there a matrix $A$ of real elements, for which this holds true: $A^2$ has more unique eigenvalues than $A$. If not, then what about if the elements of $A$ were complex numbers? I didn't manage to find such a matrix yet, so I tried proving that it's impossible. I know that the eigenvalues can be calculated by constructing the characteristic polynomial for both $A$ and $A^2$ and finding the roots. $$|A - tI| = 0$$ $$|A^2 - tI| = 0$$ But in the general case with a $n*n$ matrix the resulting polynomials are way too complicated to say anything about, so there must be another way to do it. Thanks","Ok so I messed up my last question, I'll rephrase it: Is there a matrix $A$ of real elements, for which this holds true: $A^2$ has more unique eigenvalues than $A$. If not, then what about if the elements of $A$ were complex numbers? I didn't manage to find such a matrix yet, so I tried proving that it's impossible. I know that the eigenvalues can be calculated by constructing the characteristic polynomial for both $A$ and $A^2$ and finding the roots. $$|A - tI| = 0$$ $$|A^2 - tI| = 0$$ But in the general case with a $n*n$ matrix the resulting polynomials are way too complicated to say anything about, so there must be another way to do it. Thanks",,"['linear-algebra', 'polynomials']"
72,Finding Null Space Basis over a Finite Field,Finding Null Space Basis over a Finite Field,,"I have more a systems background, but I have a math-y type question so I figured I'd give it a shot here...This is more of an implementation question, I don't need to prove anything at the moment. I'm trying to find the basis of the null space over $\mathbb{F}_2^N$ for a given basis quickly and I was hoping someone here would know how. For example if I have the following basis in $\mathbb{F}_2^{16}$: $$  \left(  \begin{array}{cccccccccccccccc} 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\ 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\ \end{array}  \right) $$ How would I find the null space basis for this matrix? If I put my basis into reduced row echelon form, I could find it easily, but for my particular problem I cannot do that. I know there are exhaustive search methods, but the matrices I'm dealing with can be quite large, which make those impractical. BEGIN EDIT @Neil de Beaudrap, It has to do with the fact that I am actually splitting up the vector space and using part of it for another purpose. If I change this matrix with elementary row operations and put it into reduced-row-echelon form it messes things up.... I am unfamiliar with column operations, could you explain in a bit more detail what you are talking about? Thanks! END EDIT","I have more a systems background, but I have a math-y type question so I figured I'd give it a shot here...This is more of an implementation question, I don't need to prove anything at the moment. I'm trying to find the basis of the null space over $\mathbb{F}_2^N$ for a given basis quickly and I was hoping someone here would know how. For example if I have the following basis in $\mathbb{F}_2^{16}$: $$  \left(  \begin{array}{cccccccccccccccc} 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\ 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\ \end{array}  \right) $$ How would I find the null space basis for this matrix? If I put my basis into reduced row echelon form, I could find it easily, but for my particular problem I cannot do that. I know there are exhaustive search methods, but the matrices I'm dealing with can be quite large, which make those impractical. BEGIN EDIT @Neil de Beaudrap, It has to do with the fact that I am actually splitting up the vector space and using part of it for another purpose. If I change this matrix with elementary row operations and put it into reduced-row-echelon form it messes things up.... I am unfamiliar with column operations, could you explain in a bit more detail what you are talking about? Thanks! END EDIT",,"['linear-algebra', 'matrices', 'coding-theory']"
73,Eigenvalues of a matrix,Eigenvalues of a matrix,,"Let $A$ be a square matrix of order, say, $4$. Consider the matrix $$B=\left( \begin{array}{ccc}A &I \\ I & A\end{array} \right)$$ where $I$ is the identity matrix of order $4$. Let $\lambda$ be an eigenvalue of $A$. Then there exists a nonzero vector $\bf{x}=$ $\left( \begin{array}{c} x_1 &x_2 &x_3 &x_4\end{array} \right)^T$ such that $A\bf{x}=\lambda \bf{x}$ Now, $$\begin{eqnarray*}\left( \begin{array}{ccc}A &I \\ I & A\end{array} \right) \left( \begin{array}{rrrrrrrr}x_1 \\x_2 \\x_3 \\x_4 \\x_1 \\x_2 \\x_3 \\x_4\end{array} \right)&=&\left( \begin{array}{rr}A\bf{x}+I\bf{x} \\ I\bf{x}+A\bf{x}\end{array} \right)\\&=&\left( \begin{array}{rr}\lambda \bf{x}+I\bf{x} \\ I\bf{x}+\lambda \bf{x}\end{array} \right)\\&=& (\lambda+1) \left( \begin{array}{rrrrrrrr}x_1 \\x_2 \\x_3 \\x_4 \\x_1 \\x_2 \\x_3 \\x_4 \end{array} \right)\end{eqnarray*}$$ So $\lambda+1$ is an eigenvalue of $B$. So, in this way we are able to say $4$ eigenvalues of $B$. Can we say anything about the other $4$ eigenvalues of $B$? Is there anything special about this kind of matrix $B$?","Let $A$ be a square matrix of order, say, $4$. Consider the matrix $$B=\left( \begin{array}{ccc}A &I \\ I & A\end{array} \right)$$ where $I$ is the identity matrix of order $4$. Let $\lambda$ be an eigenvalue of $A$. Then there exists a nonzero vector $\bf{x}=$ $\left( \begin{array}{c} x_1 &x_2 &x_3 &x_4\end{array} \right)^T$ such that $A\bf{x}=\lambda \bf{x}$ Now, $$\begin{eqnarray*}\left( \begin{array}{ccc}A &I \\ I & A\end{array} \right) \left( \begin{array}{rrrrrrrr}x_1 \\x_2 \\x_3 \\x_4 \\x_1 \\x_2 \\x_3 \\x_4\end{array} \right)&=&\left( \begin{array}{rr}A\bf{x}+I\bf{x} \\ I\bf{x}+A\bf{x}\end{array} \right)\\&=&\left( \begin{array}{rr}\lambda \bf{x}+I\bf{x} \\ I\bf{x}+\lambda \bf{x}\end{array} \right)\\&=& (\lambda+1) \left( \begin{array}{rrrrrrrr}x_1 \\x_2 \\x_3 \\x_4 \\x_1 \\x_2 \\x_3 \\x_4 \end{array} \right)\end{eqnarray*}$$ So $\lambda+1$ is an eigenvalue of $B$. So, in this way we are able to say $4$ eigenvalues of $B$. Can we say anything about the other $4$ eigenvalues of $B$? Is there anything special about this kind of matrix $B$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
74,Vector space over finite field,Vector space over finite field,,"Let's $\mathbb{F}_{p}^{5}$ is $5-$dimension space over $\mathbb{F}_{p}$, where $p$ is prime. How many ways can be decomposed the space $\mathbb{F}_{p}^{5}$ into a direct sum of two subspaces of dimension $2$ and $3$, i.e. present as $$\mathbb{F}_{p}^{5}=V_1\oplus V_2,$$ $$\dim V_1 = 2,~~~\dim V_2 = 3.$$ Thanks.","Let's $\mathbb{F}_{p}^{5}$ is $5-$dimension space over $\mathbb{F}_{p}$, where $p$ is prime. How many ways can be decomposed the space $\mathbb{F}_{p}^{5}$ into a direct sum of two subspaces of dimension $2$ and $3$, i.e. present as $$\mathbb{F}_{p}^{5}=V_1\oplus V_2,$$ $$\dim V_1 = 2,~~~\dim V_2 = 3.$$ Thanks.",,"['linear-algebra', 'abstract-algebra']"
75,On rank of a matrix whose entries are polynomials,On rank of a matrix whose entries are polynomials,,"(I took courses on linear algebra, but I don't know anything about $R$-modules or such things.) How do you define the rank of a matrix whose entries are polynomials in $K[X]$? If you assign some element of $K$ in the entries of such a matrix, what is the rank of the produced matrix (in $M_{mn}(K)$)?  Is it larger, equal, or smaller than that of the original matrix? EDIT: Here $K$ denotes an arbitrary field, but mostly I'm interested in $\mathbb{R}$ and $\mathbb{F}_p$.","(I took courses on linear algebra, but I don't know anything about $R$-modules or such things.) How do you define the rank of a matrix whose entries are polynomials in $K[X]$? If you assign some element of $K$ in the entries of such a matrix, what is the rank of the produced matrix (in $M_{mn}(K)$)?  Is it larger, equal, or smaller than that of the original matrix? EDIT: Here $K$ denotes an arbitrary field, but mostly I'm interested in $\mathbb{R}$ and $\mathbb{F}_p$.",,"['linear-algebra', 'matrices', 'polynomials', 'modules']"
76,vectors in graph theory,vectors in graph theory,,"I am reading from a book (Combinatorial optimization by Schrijver) and at one point I am not clear as to how his arguments follow. Consider the following: Let $G(V,E)$ be an undirected graph and let $w:E\rightarrow R_+$. For any subset $F$ of $E$, denote $w(F)=\sum_{e\in F}w(e)$. Denote the incidence vector of $F$ in $\mathbb{R}^E$ by $\chi^F$, i.e. for any edge $e$ in $E$, $\chi^F(e)=1$ if $e\in F$ and $\chi^F(e)=0$ otherwise.  Considering $w$ as a vector in in $\mathbb{R}^E$ we have $w(F)=w^T\chi^F$. My doubts are as follows: Is $E$ a vector space and $w,\chi^F$ linear transformations from $E$ to $\mathbb{R}$? Why are they referred to as vectors? Aren't they functions? What does the term incidence vector mean? How does it mathematically follow that $w(F)=w^T\chi^F$ ?","I am reading from a book (Combinatorial optimization by Schrijver) and at one point I am not clear as to how his arguments follow. Consider the following: Let $G(V,E)$ be an undirected graph and let $w:E\rightarrow R_+$. For any subset $F$ of $E$, denote $w(F)=\sum_{e\in F}w(e)$. Denote the incidence vector of $F$ in $\mathbb{R}^E$ by $\chi^F$, i.e. for any edge $e$ in $E$, $\chi^F(e)=1$ if $e\in F$ and $\chi^F(e)=0$ otherwise.  Considering $w$ as a vector in in $\mathbb{R}^E$ we have $w(F)=w^T\chi^F$. My doubts are as follows: Is $E$ a vector space and $w,\chi^F$ linear transformations from $E$ to $\mathbb{R}$? Why are they referred to as vectors? Aren't they functions? What does the term incidence vector mean? How does it mathematically follow that $w(F)=w^T\chi^F$ ?",,['linear-algebra']
77,Using the notation of wedge product to solve a linear system of equations,Using the notation of wedge product to solve a linear system of equations,,I am trying to solve a problem that seems like a standard idea from linear algebra but with a the notion of wedge product and exterior algebra added it gets more complicated  for someone who isn't very comfortable using the wedge product notation.  Any explanation of the following problem would be greatly appreciated. Let $\sum_{j=1}^{n} a_{ij} v_j  = b_i$ for $1 \leq i \leq m$ where the system of linear equations is given over an arbitrary commutative ring $R$.  Let $c_j$ be columns of the matrix $(a_{ij}) $. Suppose that the all minors of the matrix $(a_{ij})$ of order greater than $p$ are zero but that $c_1 \wedge c_2 \wedge \ldots \wedge c_p \neq 0$. If we let $b = ( b_i)$ then why is it true that for $\sum_{j=1}^{n} a_{ij} v_j  = b_i$ to have a solution for $1\leq i \leq m$ then it is necessary that $c_1 \wedge c_2 \wedge \ldots \wedge c_p \wedge b = 0$?,I am trying to solve a problem that seems like a standard idea from linear algebra but with a the notion of wedge product and exterior algebra added it gets more complicated  for someone who isn't very comfortable using the wedge product notation.  Any explanation of the following problem would be greatly appreciated. Let $\sum_{j=1}^{n} a_{ij} v_j  = b_i$ for $1 \leq i \leq m$ where the system of linear equations is given over an arbitrary commutative ring $R$.  Let $c_j$ be columns of the matrix $(a_{ij}) $. Suppose that the all minors of the matrix $(a_{ij})$ of order greater than $p$ are zero but that $c_1 \wedge c_2 \wedge \ldots \wedge c_p \neq 0$. If we let $b = ( b_i)$ then why is it true that for $\sum_{j=1}^{n} a_{ij} v_j  = b_i$ to have a solution for $1\leq i \leq m$ then it is necessary that $c_1 \wedge c_2 \wedge \ldots \wedge c_p \wedge b = 0$?,,"['linear-algebra', 'abstract-algebra', 'tensor-products', 'exterior-algebra']"
78,Do a matrix and its transpose have the same invariant factors over a PID?,Do a matrix and its transpose have the same invariant factors over a PID?,,"I suspect this is true since it holds in the case over a field. But suppose $A\in M_{m\times n}(R)$ where $R$ is a PID. Does it still hold that $A$ and $A^{T}$ have the same invariant factors? Regards,","I suspect this is true since it holds in the case over a field. But suppose $A\in M_{m\times n}(R)$ where $R$ is a PID. Does it still hold that $A$ and $A^{T}$ have the same invariant factors? Regards,",,"['linear-algebra', 'abstract-algebra', 'matrices', 'modules', 'principal-ideal-domains']"
79,Surjectivity of a map between a module and its double dual,Surjectivity of a map between a module and its double dual,,"Let $\mathbb{Z} = R$ be our base ring.  I am trying to show for a countable direct product of $\mathbb{Z}$ modules there is an isomorphism between it and its dual. I am stuck on the part about surjectivity and I am a little confused because according to Dummit and Foote you can only get surjectivity of the map if the modules are projective and finitely generated. Let me explain the problem in detail: Let $P = \oplus_{i \in \mathbb{N}} A_i$ where each $A_i = \mathbb{Z}$ .  How do we  to show the map $c_P : P \rightarrow P^{**}$ given by $x \mapsto (y^{*} \mapsto \left< x, y^{*} \right>$ is surjective? I know how to compute the dual of $\mathbb{Z}^{*} = Hom_{\mathbb{Z}}(\mathbb{Z},\mathbb{Z})$ by showing the mapping of each $y* \in \mathbb{Z}^{*}$ given by $y^{*} \mapsto y^{*}(1)$ is an isomorphism so $\mathbb{Z}^{*} \cong \mathbb{Z}$ .  Now since the dual of a direct sum is the direct product of corresponding duals we have $P^{*} \cong \prod_{i \in \mathbb{N}} \mathbb{Z} \cong \mathbb{Z} \times \mathbb{Z} \times \ldots $ From here I don't know what to do to prove the map $c_p$ is surjective.  I am confused about the statements I have read saying we need the module to be projective and finitely generated.  Is it just the fact that the dual of a direct product should be the direct sum of the duals?",Let be our base ring.  I am trying to show for a countable direct product of modules there is an isomorphism between it and its dual. I am stuck on the part about surjectivity and I am a little confused because according to Dummit and Foote you can only get surjectivity of the map if the modules are projective and finitely generated. Let me explain the problem in detail: Let where each .  How do we  to show the map given by is surjective? I know how to compute the dual of by showing the mapping of each given by is an isomorphism so .  Now since the dual of a direct sum is the direct product of corresponding duals we have From here I don't know what to do to prove the map is surjective.  I am confused about the statements I have read saying we need the module to be projective and finitely generated.  Is it just the fact that the dual of a direct product should be the direct sum of the duals?,"\mathbb{Z} = R \mathbb{Z} P = \oplus_{i \in \mathbb{N}} A_i A_i = \mathbb{Z} c_P : P \rightarrow P^{**} x \mapsto (y^{*} \mapsto \left< x, y^{*} \right> \mathbb{Z}^{*} = Hom_{\mathbb{Z}}(\mathbb{Z},\mathbb{Z}) y* \in \mathbb{Z}^{*} y^{*} \mapsto y^{*}(1) \mathbb{Z}^{*} \cong \mathbb{Z} P^{*} \cong \prod_{i \in \mathbb{N}} \mathbb{Z} \cong \mathbb{Z} \times \mathbb{Z} \times \ldots  c_p","['linear-algebra', 'abstract-algebra', 'modules']"
80,Proof of $X\cup Y\neq V$,Proof of,X\cup Y\neq V,"Suppose $X,Y$ are subspaces of dimension $n-k$ of the vector space $V$ of dimension $n$. Why is it always true that $X\cup Y\neq V$? I can show this by arguing that if $X=Y$ then clearly by the difference in dimension, that $X\cup Y\neq V$. If $X\neq Y$ then there is a member, $x$, in the basis of $X$ that in not in $Y$ and vice versa. Then $x+y\not\in X\cup Y$ but is in $V$. Is this argument valid? Is there a more elegant/simpler argument? Thanks!","Suppose $X,Y$ are subspaces of dimension $n-k$ of the vector space $V$ of dimension $n$. Why is it always true that $X\cup Y\neq V$? I can show this by arguing that if $X=Y$ then clearly by the difference in dimension, that $X\cup Y\neq V$. If $X\neq Y$ then there is a member, $x$, in the basis of $X$ that in not in $Y$ and vice versa. Then $x+y\not\in X\cup Y$ but is in $V$. Is this argument valid? Is there a more elegant/simpler argument? Thanks!",,"['linear-algebra', 'vector-spaces']"
81,Writing an invertible $2\times2$ matrix as a conjugate of an upper triangular matrix,Writing an invertible  matrix as a conjugate of an upper triangular matrix,2\times2,"It's been a while since I've studied linear algebra, and I wanted to follow up on something I read on MathOverflow. In this answer , KConrad mentions you can write any invertible $2\times2$ matrix as a conjugate of an upper triangular matrix. How does this work again? If I wanted to more formally pose my question: Suppose $A=\begin{bmatrix} a & b \\ c & d\end{bmatrix}\in GL_2(\mathbb{C})$. How could I determine matrices $B$ and $C$ such that $A=CBC^{-1}$ with $B$ upper triangular?","It's been a while since I've studied linear algebra, and I wanted to follow up on something I read on MathOverflow. In this answer , KConrad mentions you can write any invertible $2\times2$ matrix as a conjugate of an upper triangular matrix. How does this work again? If I wanted to more formally pose my question: Suppose $A=\begin{bmatrix} a & b \\ c & d\end{bmatrix}\in GL_2(\mathbb{C})$. How could I determine matrices $B$ and $C$ such that $A=CBC^{-1}$ with $B$ upper triangular?",,['linear-algebra']
82,"What if the only eigenvectors of $A$ are multiples of $x=(1,0,0)^T$?",What if the only eigenvectors of  are multiples of ?,"A x=(1,0,0)^T","The question is from an exercise in Gilbert Strang's Linear Algebra and its Applications : Suppose the only eigenvectors of $A$ are multiples of $x=(1,0,0)$. True or false: (a) $A$ is not invertible. (b) $A$ has a repeated eigenvalue. (c) $A$ is not diagonalizable. (b) has to be true. Since if $A$ has different eigenvalues, then there will be linear independent eigenvectors. One example that I can come up with for which (a) and (c) are true is  $$A=\left(   \begin{array}{ccc}      0&  1& 0 \\      0& 0 & 1 \\      0& 0 & 0 \\   \end{array} \right)$$ Here are my questions: Are there any counterexamples for (a) and (c)? What's the underlying picture of this problem?","The question is from an exercise in Gilbert Strang's Linear Algebra and its Applications : Suppose the only eigenvectors of $A$ are multiples of $x=(1,0,0)$. True or false: (a) $A$ is not invertible. (b) $A$ has a repeated eigenvalue. (c) $A$ is not diagonalizable. (b) has to be true. Since if $A$ has different eigenvalues, then there will be linear independent eigenvectors. One example that I can come up with for which (a) and (c) are true is  $$A=\left(   \begin{array}{ccc}      0&  1& 0 \\      0& 0 & 1 \\      0& 0 & 0 \\   \end{array} \right)$$ Here are my questions: Are there any counterexamples for (a) and (c)? What's the underlying picture of this problem?",,['linear-algebra']
83,Inner product and dot product,Inner product and dot product,,"If I am correct dot product is an example of inner product on coordinate space. I wonder if for an arbitrary inner product space with base field being $\mathbb{R}$ or $\mathbb{C}$, there always exists a coordinate system so that the inner product becomes the dot product in coordinate? What is the name of the topic regarding this question? Thanks and regards!","If I am correct dot product is an example of inner product on coordinate space. I wonder if for an arbitrary inner product space with base field being $\mathbb{R}$ or $\mathbb{C}$, there always exists a coordinate system so that the inner product becomes the dot product in coordinate? What is the name of the topic regarding this question? Thanks and regards!",,"['linear-algebra', 'inner-products']"
84,An exercise for eigenvalues and eigenvectors,An exercise for eigenvalues and eigenvectors,,"The following is from an exercise in Gilbert Strang's Linear Algebra and its Applications : Suppose $A$ has eigenvalues $0,3,5$ with independent eigenvectors $u,v,w$. Find a particular solution to $Ax = v+w$. Find all solutions. It is not difficult to find that the particular solution can be $\frac{1}{3}v+\frac{1}{5}w$.  Here is my question : How should I find all solutions for the equation? If the equation is $Ax = 0$, one needs to find a basis for the null space of $A$. However in this case, the right hand side is $v+w$.","The following is from an exercise in Gilbert Strang's Linear Algebra and its Applications : Suppose $A$ has eigenvalues $0,3,5$ with independent eigenvectors $u,v,w$. Find a particular solution to $Ax = v+w$. Find all solutions. It is not difficult to find that the particular solution can be $\frac{1}{3}v+\frac{1}{5}w$.  Here is my question : How should I find all solutions for the equation? If the equation is $Ax = 0$, one needs to find a basis for the null space of $A$. However in this case, the right hand side is $v+w$.",,[]
85,A problem in Linear algebra,A problem in Linear algebra,,"Suppose $A$ is a $n$ by $n$ matrix with entries $a_{ij}$ such that  $$|a_{ii}|>\sum_{k\neq i}|a_{ki}|$$ for $i=1,2,...,n$,  prove $A$ is invertible.","Suppose $A$ is a $n$ by $n$ matrix with entries $a_{ij}$ such that  $$|a_{ii}|>\sum_{k\neq i}|a_{ki}|$$ for $i=1,2,...,n$,  prove $A$ is invertible.",,[]
86,Projecting a Non Negative Vector onto the Simplex,Projecting a Non Negative Vector onto the Simplex,,"Given an elementwise nonnegative vector $y$, I'd like to find the projection of $y$ onto the simplex $S: \{ (x_1, \ldots, x_n) ~|~ \sum_{i=1}^n x_i=1, x_i \geq 0 \mbox{ for all } i \}$. Is there a closed form expression for this? If not, I need to write a computer program which will compute this projection; is there something simple I could do to compute this? Simplicity is more important to me than running time; I don't want to spend a long time coding this. I do realize this is a convex optimization problem and could be solved by using various optimization solvers, but that seems like overkill.","Given an elementwise nonnegative vector $y$, I'd like to find the projection of $y$ onto the simplex $S: \{ (x_1, \ldots, x_n) ~|~ \sum_{i=1}^n x_i=1, x_i \geq 0 \mbox{ for all } i \}$. Is there a closed form expression for this? If not, I need to write a computer program which will compute this projection; is there something simple I could do to compute this? Simplicity is more important to me than running time; I don't want to spend a long time coding this. I do realize this is a convex optimization problem and could be solved by using various optimization solvers, but that seems like overkill.",,"['linear-algebra', 'geometry']"
87,Are the inverses of these matrices always tridiagonal?,Are the inverses of these matrices always tridiagonal?,,"While putzing around with the linear algebra capabilities of my computing environment, I noticed that inverses of $n\times n$ matrices $\mathbf M$ associated with a sequence $a_i$, $i=1\dots n$ with $m_{ij}=a_{\max(i,j)}$, which take the form $$\mathbf M=\begin{pmatrix}a_1&a_2&\cdots&a_n\\a_2&a_2&\cdots&a_n\\\vdots&\vdots&\ddots&a_n\\a_n&a_n&a_n&a_n\end{pmatrix}$$ (i.e., constant along ""backwards L"" sections of the matrix) are tridiagonal. (I have no idea if there's a special name for these matrices, so if they've already been studied in the literature, I'd love to hear about references.) How can I prove that the inverses of these special matrices are indeed tridiagonal?","While putzing around with the linear algebra capabilities of my computing environment, I noticed that inverses of $n\times n$ matrices $\mathbf M$ associated with a sequence $a_i$, $i=1\dots n$ with $m_{ij}=a_{\max(i,j)}$, which take the form $$\mathbf M=\begin{pmatrix}a_1&a_2&\cdots&a_n\\a_2&a_2&\cdots&a_n\\\vdots&\vdots&\ddots&a_n\\a_n&a_n&a_n&a_n\end{pmatrix}$$ (i.e., constant along ""backwards L"" sections of the matrix) are tridiagonal. (I have no idea if there's a special name for these matrices, so if they've already been studied in the literature, I'd love to hear about references.) How can I prove that the inverses of these special matrices are indeed tridiagonal?",,"['linear-algebra', 'matrices', 'inverse', 'tridiagonal-matrices']"
88,How to prove that $f(A)$ is invertible iff $f$ is relatively prime with the minimal polynomial of $A$?,How to prove that  is invertible iff  is relatively prime with the minimal polynomial of ?,f(A) f A,"Let $A$ be a matrix from $\mathbb{M}_{n \times n}(F)$ and $f(x) \in F[x]$. How does one prove the following: $f(A)$ is invertible iff $\gcd(Ma,f)=1$ where $Ma$ is the minimal polynomial of $A$. Thanks.","Let $A$ be a matrix from $\mathbb{M}_{n \times n}(F)$ and $f(x) \in F[x]$. How does one prove the following: $f(A)$ is invertible iff $\gcd(Ma,f)=1$ where $Ma$ is the minimal polynomial of $A$. Thanks.",,[]
89,A question on $\operatorname{GL}_2(\mathbb R)$,A question on,\operatorname{GL}_2(\mathbb R),I know that all finite subgroups of $\operatorname{SL}_2(\mathbb R)$ are cyclic by standard averaging argument. They are all conjugate to some finite subgroup of $\operatorname{SO}_2(\mathbb R)$ and therefore cyclic. My question is how to classify all finite subgroups of $\operatorname{GL}_2(\mathbb R)$. Thanking you.,I know that all finite subgroups of $\operatorname{SL}_2(\mathbb R)$ are cyclic by standard averaging argument. They are all conjugate to some finite subgroup of $\operatorname{SO}_2(\mathbb R)$ and therefore cyclic. My question is how to classify all finite subgroups of $\operatorname{GL}_2(\mathbb R)$. Thanking you.,,"['linear-algebra', 'abstract-algebra', 'group-theory']"
90,Two questions with respect to the determinants,Two questions with respect to the determinants,,"I have got a proof of $det(AB)$=$(detA)(detB)$ in my book. It goes as follows (for invertible A): we know that rref[A|AB]=[$I_{n}$|B] We also know that det(A)=$(-1)^{s}k_{1}k_{2}...k_{r}$ where s is the number of row swaps needed to get to the rrefA, and $k_{i}$ are coefficients by which we divide rows of A to get to rrefA. Hence, the book concludes, det(AB)=$(-1)^{s}k_{1}k_{2}...k_{r}$(detB)=(detA)(detB), but I don't see how we go from det(AB) to $(-1)^{s}k_{1}k_{2}...k_{r}$(detB). Could you please explain the logic behind this step? (I see how (detA)(detB)=$(-1)^{s}k_{1}k_{2}...k_{r}$(detB), obviously). The book then also uses the fact that if A is not invertible, neither will be AB (because image of AB is contained in A), so (detA)(detB)=0(detB)=0=det(AB). My question is, how would we prove that this equation holds if B is non-invertible, and hence detB=0? I could think of saying that since B is not invertible then it can't be represented as a product of elementary matrices, while A can, so AB can't be represented as such either, but that sounds hand-wavy to me. Thanks a lot!","I have got a proof of $det(AB)$=$(detA)(detB)$ in my book. It goes as follows (for invertible A): we know that rref[A|AB]=[$I_{n}$|B] We also know that det(A)=$(-1)^{s}k_{1}k_{2}...k_{r}$ where s is the number of row swaps needed to get to the rrefA, and $k_{i}$ are coefficients by which we divide rows of A to get to rrefA. Hence, the book concludes, det(AB)=$(-1)^{s}k_{1}k_{2}...k_{r}$(detB)=(detA)(detB), but I don't see how we go from det(AB) to $(-1)^{s}k_{1}k_{2}...k_{r}$(detB). Could you please explain the logic behind this step? (I see how (detA)(detB)=$(-1)^{s}k_{1}k_{2}...k_{r}$(detB), obviously). The book then also uses the fact that if A is not invertible, neither will be AB (because image of AB is contained in A), so (detA)(detB)=0(detB)=0=det(AB). My question is, how would we prove that this equation holds if B is non-invertible, and hence detB=0? I could think of saying that since B is not invertible then it can't be represented as a product of elementary matrices, while A can, so AB can't be represented as such either, but that sounds hand-wavy to me. Thanks a lot!",,['linear-algebra']
91,Orthogonal Decomposition of A Matrix,Orthogonal Decomposition of A Matrix,,"I'm trying to follow/understand a research paper that I have, and well, it's been a while since I've done this kind of math. At this point I have an nxn matrix H and from that construct an (n-1)xn matrix H' = $[\textbf{h}_1-\textbf{h}_n$, ..., $\textbf{h}_{1-n}-\textbf{h}_n]^T$. Now ""using orthogonal decomposition"" I'm to obtain H' =$\textbf{QU}$, where $\textbf{Q}$ is an (n-1)x(n-1) orthogonal matrix and $\textbf{U}$, which is an (n-1)xn upper diagonal matrix. I guess I'm hoping someone can better explain orthogonal decomposition and help me write the elements of $\textbf{Q}$ and $\textbf{U}$ in terms of the elements of H' .","I'm trying to follow/understand a research paper that I have, and well, it's been a while since I've done this kind of math. At this point I have an nxn matrix H and from that construct an (n-1)xn matrix H' = $[\textbf{h}_1-\textbf{h}_n$, ..., $\textbf{h}_{1-n}-\textbf{h}_n]^T$. Now ""using orthogonal decomposition"" I'm to obtain H' =$\textbf{QU}$, where $\textbf{Q}$ is an (n-1)x(n-1) orthogonal matrix and $\textbf{U}$, which is an (n-1)xn upper diagonal matrix. I guess I'm hoping someone can better explain orthogonal decomposition and help me write the elements of $\textbf{Q}$ and $\textbf{U}$ in terms of the elements of H' .",,"['linear-algebra', 'matrices']"
92,"Classify, up to similarity, the $3$ by $3$ matrices with coefficients in $\mathbb{Q}$ that satisfy $A^6=I$.","Classify, up to similarity, the  by  matrices with coefficients in  that satisfy .",3 3 \mathbb{Q} A^6=I,"I am working on the following question in review for my algebra final: Classify, up to similarity, the $3$ by $3$ matrices with coefficients in $\mathbb{Q}$ that satisfy $A^6=I$ . My work: As $A^6 - I = 0$ , we know that the minimal polynomial of $A$ , $m_A(x)$ divides $x^6 - 1$ . We factor $$x^6 - 1 = (x^3 - 1)(x^3 + 1) = (x - 1)(x^2 + x + 1)(x + 1)(x^2 - x + 1).$$ Thus, the possible minimal polynomial must have degree of the matrix (which is $3$ ), so $m_A(x)=$ $1$ . $(x-1)(x^2 + x + 1)$ $2$ . $(x-1)(x^2 - x + 1)$ $3$ . $(x+1)(x^2 - x + 1)$ $4$ . $(x+1)(x^2 + x + 1)$ These have rational canonical forms $$\begin{pmatrix}     1 & 0 & 0 \\     0 & 0 & -1 \\     0 & 1 & -1  \end{pmatrix} \quad \begin{pmatrix}     1 & 0 & 0 \\     0 & 0 & -1 \\     0 & 1 & 1   \end{pmatrix} \quad \begin{pmatrix}     -1 & 0 & 0 \\     0 & 0 & -1 \\     0 & 1 & 1   \end{pmatrix} \quad \begin{pmatrix}     -1 & 0 & 0 \\     0 & 0 & -1 \\     0 & 1 & -1   \end{pmatrix}$$ Is this correct? Edit: I forgot $I$ . I am also trying to find the number of classes if the matrices are over $\mathbb{C}$ . My idea is that is is $6$ choose $3$ because over $\mathbb{C}$ there are 6 total roots, but the correct answer is apparently $56$ ??","I am working on the following question in review for my algebra final: Classify, up to similarity, the by matrices with coefficients in that satisfy . My work: As , we know that the minimal polynomial of , divides . We factor Thus, the possible minimal polynomial must have degree of the matrix (which is ), so . . . . These have rational canonical forms Is this correct? Edit: I forgot . I am also trying to find the number of classes if the matrices are over . My idea is that is is choose because over there are 6 total roots, but the correct answer is apparently ??","3 3 \mathbb{Q} A^6=I A^6 - I = 0 A m_A(x) x^6 - 1 x^6 - 1 = (x^3 - 1)(x^3 + 1) = (x - 1)(x^2 + x + 1)(x + 1)(x^2 - x + 1). 3 m_A(x)= 1 (x-1)(x^2 + x + 1) 2 (x-1)(x^2 - x + 1) 3 (x+1)(x^2 - x + 1) 4 (x+1)(x^2 + x + 1) \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 0 & -1 \\
    0 & 1 & -1 
\end{pmatrix} \quad \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 0 & -1 \\
    0 & 1 & 1  
\end{pmatrix} \quad \begin{pmatrix}
    -1 & 0 & 0 \\
    0 & 0 & -1 \\
    0 & 1 & 1  
\end{pmatrix} \quad \begin{pmatrix}
    -1 & 0 & 0 \\
    0 & 0 & -1 \\
    0 & 1 & -1  
\end{pmatrix} I \mathbb{C} 6 3 \mathbb{C} 56","['linear-algebra', 'abstract-algebra', 'jordan-normal-form', 'minimal-polynomials', 'similar-matrices']"
93,Suppose $A$ is a $7\times 7$ matrix with all entries less than $1$ in magnitude. Prove that $\det (100I+A) > 0$,Suppose  is a  matrix with all entries less than  in magnitude. Prove that,A 7\times 7 1 \det (100I+A) > 0,"Suppose $A$ is a $7\times 7$ matrix with all entries less than $1$ in magnitude. Prove that $\det (100I+A) > 0$ . This is my first post on the mathematics stack exchange, so forgive me if its not in the right format or if its been asked before (I couldnt find anything like it, though). My elementary linear algebra professor posted this question in the beginning of class this morning and left it up to us to discuss independently. I can gather that the determinant of $100 I_7$ is $100^7$ and every element in $A$ is in between $-1$ and $1$ (given) but other than that, Ive got no clue where to start. Thank you!","Suppose is a matrix with all entries less than in magnitude. Prove that . This is my first post on the mathematics stack exchange, so forgive me if its not in the right format or if its been asked before (I couldnt find anything like it, though). My elementary linear algebra professor posted this question in the beginning of class this morning and left it up to us to discuss independently. I can gather that the determinant of is and every element in is in between and (given) but other than that, Ive got no clue where to start. Thank you!",A 7\times 7 1 \det (100I+A) > 0 100 I_7 100^7 A -1 1,"['linear-algebra', 'proof-writing', 'determinant']"
94,Geometry of linear equations,Geometry of linear equations,,"its my first question here. I'm re-self-studying linear algebra from different sources and one of them is Linear Algebra and Its applications by g.strang 4th ed. . While i have studied a bunch of material i still don't grasp some basics and i really struggle. Page 4-5 gives two approaches for the geometry of linear equations: my problem is the second graph. I get it, the operations, how we use them as a transformation to create the vectors and that we have to guess a solution to get to the (1,5). I also get that he is trying to make a point for later on, on Gauss elimination. But how/why can he take the coefficients of each equation for example for the y part \begin{bmatrix}-1\\1\end{bmatrix} and use it as x,y coordinates to map it to the graph? How can -1y and 1y coefficients turn into a (x,y) / ( -1,1) vector? It looks like to me that we got two coeffients and turned them from y,y to x,y.I don't get how that concept works. Do i make sense? I've searched a lot but i may lack knowledge of the correct terms, so even if the question is novice level/not worth answering please provide me with some keywords.And of course correct me for any mistakes. Much appreciated","its my first question here. I'm re-self-studying linear algebra from different sources and one of them is Linear Algebra and Its applications by g.strang 4th ed. . While i have studied a bunch of material i still don't grasp some basics and i really struggle. Page 4-5 gives two approaches for the geometry of linear equations: my problem is the second graph. I get it, the operations, how we use them as a transformation to create the vectors and that we have to guess a solution to get to the (1,5). I also get that he is trying to make a point for later on, on Gauss elimination. But how/why can he take the coefficients of each equation for example for the y part and use it as x,y coordinates to map it to the graph? How can -1y and 1y coefficients turn into a (x,y) / ( -1,1) vector? It looks like to me that we got two coeffients and turned them from y,y to x,y.I don't get how that concept works. Do i make sense? I've searched a lot but i may lack knowledge of the correct terms, so even if the question is novice level/not worth answering please provide me with some keywords.And of course correct me for any mistakes. Much appreciated",\begin{bmatrix}-1\\1\end{bmatrix},['linear-algebra']
95,Find the lengths of the principal axes of the ellipsoid $\sum_{i \leq j} x_ix_j = 1$ (Arnold 85),Find the lengths of the principal axes of the ellipsoid  (Arnold 85),\sum_{i \leq j} x_ix_j = 1,"Find the lengths of the principal axes of the ellipsoid $$\sum_{i \leq j} x_ix_j = 1.$$ -- Arnold, Trivium 85 My solution is below.  I request verification, feedback, or alternate approaches (especially ways to simplify). Solution: In $\mathbb R^n$ , the ellipsoid $\sum_{i \leq j} x_ix_j = 1$ has a single axis of length $\sqrt {\frac 8 {n+1}}$ and all other axes length $2\sqrt 2$ . Proof: Recall that if $D$ is a diagonal matrix, then $$\mathbf x^\top D \mathbf x = 1$$ is an ellipsoid in standard position, with axis $i$ of length $\frac 2 {\sqrt {D_{ii}}}$ . Simple multiplication shows that the ellipsoid $\sum_{i \leq j} x_ix_j = 1$ has equation $$\mathbf x^\top S \mathbf x = 1$$ where $$S_{ij} = \begin{cases}1 &\text{ if } i = j\\ \frac 1 2 &\text{ otherwise}.\end{cases}$$ Since $S$ is symmetric, the spectral theorem shows that $S$ has $n$ orthogonal eigenvectors with real eigenvalues, and that $S$ decomposes into $S = DQ$ , with $Q$ orthogonal and $D$ diagonal.  The diagonal entries of $D$ are the eigenvalues of $S$ . Since $Q$ is orthogonal, it preserves lengths.  Consequently, if the eigenvalues of $S$ are $\lambda_i$ , then the ellipsoid's axes will have length $\frac 2 {\sqrt {\lambda_i}}$ . Inspection shows that the vector $\mathbf \ell$ with all components $1$ is an eigenvector of $S$ with eigenvalue $\frac {n+1} 2$ .  Inspection likewise shows that for $1 \leq i < n$ , the vectors $\mathbf m_i$ with components $$m_{i_j} = \begin{cases} 1 &\text{ if } j = i \\ 1 - \sqrt n - n &\text{ if } j = n\\ 1 + \sqrt n&\text{ otherwise}\end{cases}$$ are other eigenvectors, each with eigenvalue $\frac 1 2$ , which completes the proof. Remark: The components of $\mathbf m_i$ were determined by solving for $$a + (n-2)b + c = 0 \text{ since } \mathbf m_i \cdot \mathbf \ell = 0 \\ 2ab + (n-3)b^2 + c^2 = 0 \text{ since } \mathbf m_i \cdot \mathbf m_j = 0 \text { when } i \neq j.$$ Is there a simpler way to determine them?  The fact that the rows of $S$ are rotations of each other suggests some type of symmetry of its eigenvalues, and we know their sum from $\operatorname{trace} S = n$ , but I could not develop this further.","Find the lengths of the principal axes of the ellipsoid -- Arnold, Trivium 85 My solution is below.  I request verification, feedback, or alternate approaches (especially ways to simplify). Solution: In , the ellipsoid has a single axis of length and all other axes length . Proof: Recall that if is a diagonal matrix, then is an ellipsoid in standard position, with axis of length . Simple multiplication shows that the ellipsoid has equation where Since is symmetric, the spectral theorem shows that has orthogonal eigenvectors with real eigenvalues, and that decomposes into , with orthogonal and diagonal.  The diagonal entries of are the eigenvalues of . Since is orthogonal, it preserves lengths.  Consequently, if the eigenvalues of are , then the ellipsoid's axes will have length . Inspection shows that the vector with all components is an eigenvector of with eigenvalue .  Inspection likewise shows that for , the vectors with components are other eigenvectors, each with eigenvalue , which completes the proof. Remark: The components of were determined by solving for Is there a simpler way to determine them?  The fact that the rows of are rotations of each other suggests some type of symmetry of its eigenvalues, and we know their sum from , but I could not develop this further.","\sum_{i \leq j} x_ix_j = 1. \mathbb R^n \sum_{i \leq j} x_ix_j = 1 \sqrt {\frac 8 {n+1}} 2\sqrt 2 D \mathbf x^\top D \mathbf x = 1 i \frac 2 {\sqrt {D_{ii}}} \sum_{i \leq j} x_ix_j = 1 \mathbf x^\top S \mathbf x = 1 S_{ij} = \begin{cases}1 &\text{ if } i = j\\ \frac 1 2 &\text{ otherwise}.\end{cases} S S n S S = DQ Q D D S Q S \lambda_i \frac 2 {\sqrt {\lambda_i}} \mathbf \ell 1 S \frac {n+1} 2 1 \leq i < n \mathbf m_i m_{i_j} = \begin{cases}
1 &\text{ if } j = i \\
1 - \sqrt n - n &\text{ if } j = n\\
1 + \sqrt n&\text{ otherwise}\end{cases} \frac 1 2 \mathbf m_i a + (n-2)b + c = 0 \text{ since } \mathbf m_i \cdot \mathbf \ell = 0 \\
2ab + (n-3)b^2 + c^2 = 0 \text{ since } \mathbf m_i \cdot \mathbf m_j = 0 \text { when } i \neq j. S \operatorname{trace} S = n","['linear-algebra', 'geometry', 'spectral-theory', 'quadratic-forms']"
96,Detemining the eigenvalues $F: \ M_{nxn}(\mathbb{R}) \to M_{nxn}(\mathbb{R}) : X \mapsto AX-XB^T$ if we know the eigenvalues of $A$ and $B$.,Detemining the eigenvalues  if we know the eigenvalues of  and .,F: \ M_{nxn}(\mathbb{R}) \to M_{nxn}(\mathbb{R}) : X \mapsto AX-XB^T A B,"I'm starting to look at some linear algebra again, and it's been a long time since I have worked with eigenvalues. That being said, I found this exercise: Take $A,B \in M_{nxn}(\mathbb{R})$ and $\lambda_1,....,\lambda_n \in \mathbb{R}$ , $v_1,...,v_n \in \mathbb{R}^n$ , $\mu_1,....,\mu_n \in \mathbb{R}$ , $w_1,...,w_n \in \mathbb{R}^n$ be respectively the eigenvalues and associated eigenvectors of $A$ and $B$ . Now consider the linear map $F:  M_{nxn}(\mathbb{R})\rightarrow  M_{nxn}(\mathbb{R})$ defined by $F: X \rightarrow AX-XB^T$ . Show that the eigenvalues of $F$ are the $\lambda_i-\mu_j$ for $i,j=1,...n$ . Now it's easy to show that the $\lambda_i-\mu_j$ are indeed eigenvalues but in the correction they say that since $F$ cannot have more than $n^2$ eigenvalues we have shown that they are no others eigenvalues. But I must be missing some properties of eigenvalues since nothing tells us that all $\lambda_i-\mu_j$ are different and if two are the same we might be missing one since we haven't compute the characteristic polynomials, and we don't know the multiplicity of the eigenvalues. So I'm wondering if the correction is indeed missing a part and if yes, is there an easier way to solve the exercise than computing the characteristic polynomial of $F$ ? Edit: If eigenvalues are all different, it works, but what about if they are not ?","I'm starting to look at some linear algebra again, and it's been a long time since I have worked with eigenvalues. That being said, I found this exercise: Take and , , , be respectively the eigenvalues and associated eigenvectors of and . Now consider the linear map defined by . Show that the eigenvalues of are the for . Now it's easy to show that the are indeed eigenvalues but in the correction they say that since cannot have more than eigenvalues we have shown that they are no others eigenvalues. But I must be missing some properties of eigenvalues since nothing tells us that all are different and if two are the same we might be missing one since we haven't compute the characteristic polynomials, and we don't know the multiplicity of the eigenvalues. So I'm wondering if the correction is indeed missing a part and if yes, is there an easier way to solve the exercise than computing the characteristic polynomial of ? Edit: If eigenvalues are all different, it works, but what about if they are not ?","A,B \in M_{nxn}(\mathbb{R}) \lambda_1,....,\lambda_n \in \mathbb{R} v_1,...,v_n \in \mathbb{R}^n \mu_1,....,\mu_n \in \mathbb{R} w_1,...,w_n \in \mathbb{R}^n A B F:  M_{nxn}(\mathbb{R})\rightarrow  M_{nxn}(\mathbb{R}) F: X \rightarrow AX-XB^T F \lambda_i-\mu_j i,j=1,...n \lambda_i-\mu_j F n^2 \lambda_i-\mu_j F","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
97,Diagonalization and Homomorphism,Diagonalization and Homomorphism,,"Maybe this question could sound silly, but after carefully re-reading my linear algebra notes one particular detail catch my sight. Let $V$ a $\Bbb K$ -vector space, $\dim(V) = n$ , $f$ an $\textbf{endomorphism}$ is said to be diagonalizable if exist a base $\mathcal{B}$ of $V$ s.t. $f$ representative matrix with respect to $\mathcal{B}$ is a diagonal matrix. Till there nothing strange is the definition of diagonalization, but as far as I'm concerned there are multiple ways to define some application $g: V \to W$ with $\dim(V) = \dim(W)$ , and the representative matrix of $g$ is a square matrix. The question is: we want $f$ to be an endomorphism because otherwise there isn't nothing relevant to say even tough algorithmically speaking we could make the same eigenvalue/eigenvector calculation over any square matrix, or since $V,W$ have the same dimension are isomorphic as vector spaces and we are basically working with endomorphism? My guess is that the second reasoning could makes sense since (using the little bit of understanding I have of category theory) let $F$ be a $W,V$ isomorphism and $g:V\to W$ $$\require{AMScd} \begin{CD} V @>{g}>> W \\ @V{id_{V}}VV @V{F}VV \\ V @>{Fg}>> V  \end{CD}$$ This is a commutative diagram ad $Fg$ is an endomorphism. Hope for some clarification, thank you. Edit After some responses, the possibility to choose different basis for $V,W$ lead to every homomorphism to have a diagonal form (this isn't the case considering only endomorphism). In the situation descibed above can we deduce some information (eigenvalues/eigenvectors,...) from $g$ that can be transported to $Fg$ ?","Maybe this question could sound silly, but after carefully re-reading my linear algebra notes one particular detail catch my sight. Let a -vector space, , an is said to be diagonalizable if exist a base of s.t. representative matrix with respect to is a diagonal matrix. Till there nothing strange is the definition of diagonalization, but as far as I'm concerned there are multiple ways to define some application with , and the representative matrix of is a square matrix. The question is: we want to be an endomorphism because otherwise there isn't nothing relevant to say even tough algorithmically speaking we could make the same eigenvalue/eigenvector calculation over any square matrix, or since have the same dimension are isomorphic as vector spaces and we are basically working with endomorphism? My guess is that the second reasoning could makes sense since (using the little bit of understanding I have of category theory) let be a isomorphism and This is a commutative diagram ad is an endomorphism. Hope for some clarification, thank you. Edit After some responses, the possibility to choose different basis for lead to every homomorphism to have a diagonal form (this isn't the case considering only endomorphism). In the situation descibed above can we deduce some information (eigenvalues/eigenvectors,...) from that can be transported to ?","V \Bbb K \dim(V) = n f \textbf{endomorphism} \mathcal{B} V f \mathcal{B} g: V \to W \dim(V) = \dim(W) g f V,W F W,V g:V\to W \require{AMScd}
\begin{CD}
V @>{g}>> W \\
@V{id_{V}}VV @V{F}VV \\
V @>{Fg}>> V 
\end{CD} Fg V,W g Fg","['linear-algebra', 'linear-transformations', 'diagonalization']"
98,Gradients for partially symmetric CP decomposition of 3rd order tensor,Gradients for partially symmetric CP decomposition of 3rd order tensor,,"I am interested in computing a rank- $R$ CP decomposition of a 3rd order tensor that is partially symmetric about the first 2 modes. The factorization of a vanilla CP decomposition is given below as a least squares minimization problem: $$ \min_{A,B,C} || \mathcal{X} - [[A,B,C]]||^{2} $$ where $\mathcal{X} \in \mathbb{R}^{L \times M \times N} $ is the tensor to approximate, $A \in \mathbb{R}^{L \times R}$ , $B \in \mathbb{R}^{M \times R}$ , and $C \in \mathbb{R}^{N \times R}$ are factor matrices to estimate. [[]] denotes the sum of outer products of each of the corresponding factor matrix columns: $$ [[A,B,C]] = \sum_{r=1}^{R} a_{r} \otimes b_{r} \otimes c_{r} $$ Where $a_r$ is the $r$ -th column of $A$ , etc. This objective can be rewritten as sub-problems corresponding to each mode: $$ \min_{A} ||\mathcal{X}_{(1)} - A(C \odot B)^{T}||^{2} \\ \min_{B} ||\mathcal{X}_{(2)} - B(C \odot A)^{T}||^{2} \\ \min_{C} ||\mathcal{X}_{(3)} - C(B \odot A)^{T}||^{2}  $$ where the subscript $\mathcal{X}_{(i)}$ indicates the $i$ -th mode unfolding of a tensor (matricization where the mode- $i$ fibers become the columns of the resulting matrix) and $\odot$ is the Khatri-Rao product. The partial derivatives, with respect to each factor matrix, are as follows: $$ \frac{\partial f}{\partial A} = (\mathcal{T}_{(1)} - \mathcal{X}_{(1)})(C \odot B) \\ \frac{\partial f}{\partial B} = (\mathcal{T}_{(2)} - \mathcal{X}_{(2)})(C \odot A) \\ \frac{\partial f}{\partial C} = (\mathcal{T}_{(3)} - \mathcal{X}_{(3)})(B \odot A) $$ where $\mathcal{T}=[[A,B,C]]$ . See here and here for details regarding all of the above derivation/implementation. In the case of partial symmetry about the first two modes, my problem becomes: $$ \min_{A,A,C} || \mathcal{X} - [[A,A,C]]||^{2} $$ where $\mathcal{X} \in \mathbb{R}^{M \times M \times N} $ is a the tensor to approximate, $A \in \mathbb{R}^{M \times R}$ , and $C \in \mathbb{R}^{N \times R}$ . So, my question is, what would the partial derivatives be for the above case with partial symmetry with respect to $A$ and $C$ ? My actual optimization problem is more complex than this, so I want the gradients for non-linear optimization and this is the particular aspect of the objective that I have been unable to compute them. Any advise is greatly appreciated, I've been banging my head trying to figure this out - thank you!","I am interested in computing a rank- CP decomposition of a 3rd order tensor that is partially symmetric about the first 2 modes. The factorization of a vanilla CP decomposition is given below as a least squares minimization problem: where is the tensor to approximate, , , and are factor matrices to estimate. [[]] denotes the sum of outer products of each of the corresponding factor matrix columns: Where is the -th column of , etc. This objective can be rewritten as sub-problems corresponding to each mode: where the subscript indicates the -th mode unfolding of a tensor (matricization where the mode- fibers become the columns of the resulting matrix) and is the Khatri-Rao product. The partial derivatives, with respect to each factor matrix, are as follows: where . See here and here for details regarding all of the above derivation/implementation. In the case of partial symmetry about the first two modes, my problem becomes: where is a the tensor to approximate, , and . So, my question is, what would the partial derivatives be for the above case with partial symmetry with respect to and ? My actual optimization problem is more complex than this, so I want the gradients for non-linear optimization and this is the particular aspect of the objective that I have been unable to compute them. Any advise is greatly appreciated, I've been banging my head trying to figure this out - thank you!","R  \min_{A,B,C} || \mathcal{X} - [[A,B,C]]||^{2}
 \mathcal{X} \in \mathbb{R}^{L \times M \times N}  A \in \mathbb{R}^{L \times R} B \in \mathbb{R}^{M \times R} C \in \mathbb{R}^{N \times R}  [[A,B,C]] = \sum_{r=1}^{R} a_{r} \otimes b_{r} \otimes c_{r}
 a_r r A  \min_{A} ||\mathcal{X}_{(1)} - A(C \odot B)^{T}||^{2} \\
\min_{B} ||\mathcal{X}_{(2)} - B(C \odot A)^{T}||^{2} \\
\min_{C} ||\mathcal{X}_{(3)} - C(B \odot A)^{T}||^{2} 
 \mathcal{X}_{(i)} i i \odot  \frac{\partial f}{\partial A} = (\mathcal{T}_{(1)} - \mathcal{X}_{(1)})(C \odot B) \\
\frac{\partial f}{\partial B} = (\mathcal{T}_{(2)} - \mathcal{X}_{(2)})(C \odot A) \\
\frac{\partial f}{\partial C} = (\mathcal{T}_{(3)} - \mathcal{X}_{(3)})(B \odot A)
 \mathcal{T}=[[A,B,C]]  \min_{A,A,C} || \mathcal{X} - [[A,A,C]]||^{2}
 \mathcal{X} \in \mathbb{R}^{M \times M \times N}  A \in \mathbb{R}^{M \times R} C \in \mathbb{R}^{N \times R} A C","['linear-algebra', 'optimization', 'partial-derivative', 'matrix-calculus', 'tensor-decomposition']"
99,Unitary operators and a product of reflections,Unitary operators and a product of reflections,,"The book Advanced Linear Algebra by Steven Roman states that Reflections or Housholder transformations are self-adjoint and unitary. Moreover, Theorem 10.17 of this book states every unitary $\tau \in \mathcal{L}(V)$ , where $V$ is an inner product space, is a product of reflections. Now, consider $\mathbb{C}$ as a unitary space. Self-adjoint operators are real numbers and $i$ is a unitary operator. But, $i$ is not a product of reflections since an imaginary number cannot be a product of real numbers. What is the problem?","The book Advanced Linear Algebra by Steven Roman states that Reflections or Housholder transformations are self-adjoint and unitary. Moreover, Theorem 10.17 of this book states every unitary , where is an inner product space, is a product of reflections. Now, consider as a unitary space. Self-adjoint operators are real numbers and is a unitary operator. But, is not a product of reflections since an imaginary number cannot be a product of real numbers. What is the problem?",\tau \in \mathcal{L}(V) V \mathbb{C} i i,"['linear-algebra', 'inner-products', 'reflection', 'unitary-matrices']"
