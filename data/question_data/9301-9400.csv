,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Nature of $\sum 1/f(n)$ with $f(n) := n\ln(n)\ln(\ln(n))...*\ln^{(k_n)}(n)$ ; $k_n$ being the largest natural integer $k$ such that $\ln^{(k)}(n)≥1$ [duplicate],Nature of  with  ;  being the largest natural integer  such that  [duplicate],\sum 1/f(n) f(n) := n\ln(n)\ln(\ln(n))...*\ln^{(k_n)}(n) k_n k \ln^{(k)}(n)≥1,"This question already has an answer here : Does the series converge (1 answer) Closed last year . For all $n$ in $\mathbb N^*$, let $f(n) := n*\ln(n)*\ln(\ln(n))*...*\ln^{(k_n)}(n)$, with $\ln^{(k)}$ being the   logarithm iterated $k$ times, and $k_n$ being the largest natural   integer $k$ such that $\ln^{(k)}(n)≥1$. Study the nature of the series $\sum 1/f(n)$. One can show that when $k_n$ is a constant, the series diverges (by comparison with integral) but here  it is not the case. I think the series also diverges. How to prove it ?","This question already has an answer here : Does the series converge (1 answer) Closed last year . For all $n$ in $\mathbb N^*$, let $f(n) := n*\ln(n)*\ln(\ln(n))*...*\ln^{(k_n)}(n)$, with $\ln^{(k)}$ being the   logarithm iterated $k$ times, and $k_n$ being the largest natural   integer $k$ such that $\ln^{(k)}(n)≥1$. Study the nature of the series $\sum 1/f(n)$. One can show that when $k_n$ is a constant, the series diverges (by comparison with integral) but here  it is not the case. I think the series also diverges. How to prove it ?",,"['calculus', 'real-analysis']"
1,"If $x,y\in E$ then $\frac{x+y}{2}\in E$. Prove that $E$ has an interior point",If  then . Prove that  has an interior point,"x,y\in E \frac{x+y}{2}\in E E","Let $E\subset \mathbb{R}$ be a set of positive Lebesgue measure. Assume that if $x,y\in E$ then $\frac{x+y}{2}\in E$. Prove that $E$ has at least one interior point. Here is what I have done: (1). By regularity, for any $\epsilon>0$ we can find an open set $O_\epsilon$ such that $E\subseteq O_\epsilon$ and $m(O_\epsilon)-m(E)<\epsilon.$ Write $O_\epsilon$ as a disjoint union of open intervals $\{I_j\}$ $$O_\epsilon=\bigsqcup_{j=1}^\infty I_j$$ (2). WLOG we can do the indexing in such a way that $I_{j+1}$ is the next interval to $I_j$ (in the sense that $I_{j+1}$ is on the right of $I_j$ and there is no $I_k$ which is in between $I_j$ and $I_{j+1}$.) (3). If at least one  $I_j\subseteq E$ then we are done. So assume that $I_j\subsetneq E$ for all $j$. Chose an $I_j$ and pick a point $x\in I_j\cap E$. Chose $y\in I_{j+1}\cap E$. Now $z=\frac{x+y}{2}\in E$ and thanks to the indexing, $z\in I_j$ or $z\in I_{j+1}.$  WLOG we can assume that $z\in I_j$. (4) Now we have two point $x,z\in I_j$. We can recursively pick the midpoints on the line joining $x$ and $z$ and all these points will be in $E$. (First pick $\frac{x+z}{2}$, then pick $\frac{x+\frac{x+z}{2}}{2}$ and $\frac{z+\frac{x+z}{2}}{2}$ and so on) (5). My guess is that one of the midpoints (constructed in the previous step) on the line joining $x$ and $z$ will be an interior point. But I don't know if my guess is correct. Am I moving in the right direction? Is there a different way to solve this problem?","Let $E\subset \mathbb{R}$ be a set of positive Lebesgue measure. Assume that if $x,y\in E$ then $\frac{x+y}{2}\in E$. Prove that $E$ has at least one interior point. Here is what I have done: (1). By regularity, for any $\epsilon>0$ we can find an open set $O_\epsilon$ such that $E\subseteq O_\epsilon$ and $m(O_\epsilon)-m(E)<\epsilon.$ Write $O_\epsilon$ as a disjoint union of open intervals $\{I_j\}$ $$O_\epsilon=\bigsqcup_{j=1}^\infty I_j$$ (2). WLOG we can do the indexing in such a way that $I_{j+1}$ is the next interval to $I_j$ (in the sense that $I_{j+1}$ is on the right of $I_j$ and there is no $I_k$ which is in between $I_j$ and $I_{j+1}$.) (3). If at least one  $I_j\subseteq E$ then we are done. So assume that $I_j\subsetneq E$ for all $j$. Chose an $I_j$ and pick a point $x\in I_j\cap E$. Chose $y\in I_{j+1}\cap E$. Now $z=\frac{x+y}{2}\in E$ and thanks to the indexing, $z\in I_j$ or $z\in I_{j+1}.$  WLOG we can assume that $z\in I_j$. (4) Now we have two point $x,z\in I_j$. We can recursively pick the midpoints on the line joining $x$ and $z$ and all these points will be in $E$. (First pick $\frac{x+z}{2}$, then pick $\frac{x+\frac{x+z}{2}}{2}$ and $\frac{z+\frac{x+z}{2}}{2}$ and so on) (5). My guess is that one of the midpoints (constructed in the previous step) on the line joining $x$ and $z$ will be an interior point. But I don't know if my guess is correct. Am I moving in the right direction? Is there a different way to solve this problem?",,"['real-analysis', 'measure-theory']"
2,derivatives of $x^x$ positive on a halfline,derivatives of  positive on a halfline,x^x,"I'm being curious how it is: Is there a number $a$ such that all derivatives of $x^x$ are positive on $(a,\infty)$ ?","I'm being curious how it is: Is there a number $a$ such that all derivatives of $x^x$ are positive on $(a,\infty)$ ?",,"['real-analysis', 'derivatives']"
3,Riesz Representation Theorem on noncompact space,Riesz Representation Theorem on noncompact space,,"I meet this problem: $\Lambda(f)$ is a nonnegative bounded linear functional on $C[0,\infty)$. Assume $\Lambda(1) = 1$. Then $\Lambda$ has a representation $\Lambda(f) = \int_{R^+} f \mathrm{d}\mu$ if and only if $\Lambda$ satisfies $f_n \downarrow 0 \Rightarrow \Lambda(f_n) \rightarrow 0$, i.e. the monotone convergence theorem holds. Why don't we need this condition in the compact case? One direction is just the monotone convergence theorem. I have difficulty dealing with the other side. When learning measure theory, I learned that a set function $\mu$ over a ring $R \subset \mathcal{P}(X)$ which is nonnegative, finitely additive, takes $0$ at $\varnothing$ and $\mu(X)<\infty$ is a measure iff $\mu$ is continuous at $\varnothing$. This seems similar to this problem, but I don't know how to proceed. Thank you for any help!","I meet this problem: $\Lambda(f)$ is a nonnegative bounded linear functional on $C[0,\infty)$. Assume $\Lambda(1) = 1$. Then $\Lambda$ has a representation $\Lambda(f) = \int_{R^+} f \mathrm{d}\mu$ if and only if $\Lambda$ satisfies $f_n \downarrow 0 \Rightarrow \Lambda(f_n) \rightarrow 0$, i.e. the monotone convergence theorem holds. Why don't we need this condition in the compact case? One direction is just the monotone convergence theorem. I have difficulty dealing with the other side. When learning measure theory, I learned that a set function $\mu$ over a ring $R \subset \mathcal{P}(X)$ which is nonnegative, finitely additive, takes $0$ at $\varnothing$ and $\mu(X)<\infty$ is a measure iff $\mu$ is continuous at $\varnothing$. This seems similar to this problem, but I don't know how to proceed. Thank you for any help!",,"['real-analysis', 'functional-analysis', 'measure-theory', 'riesz-representation-theorem']"
4,How to prove this strange limit? [duplicate],How to prove this strange limit? [duplicate],,"This question already has answers here : If $f(x) + f'(x) + f''(x) \to A$ as $x \to \infty$, then show that $f(x) \to A$ as $x \to \infty$ (2 answers) Closed 6 years ago . Let $f:[0,\infty)\to\mathbb R$ be a function in $C^2$ such that  $\lim_{x\to\infty} (f(x)+f'(x)+f''(x)) = a.$ Prove that $\lim_{x\to\infty} f(x)=a$","This question already has answers here : If $f(x) + f'(x) + f''(x) \to A$ as $x \to \infty$, then show that $f(x) \to A$ as $x \to \infty$ (2 answers) Closed 6 years ago . Let $f:[0,\infty)\to\mathbb R$ be a function in $C^2$ such that  $\lim_{x\to\infty} (f(x)+f'(x)+f''(x)) = a.$ Prove that $\lim_{x\to\infty} f(x)=a$",,"['calculus', 'real-analysis']"
5,"How to show that $\lim_{n\to\infty}[a_1,\cdots,a_n]$ exists if $a_k\geq 2$ for all $k$?",How to show that  exists if  for all ?,"\lim_{n\to\infty}[a_1,\cdots,a_n] a_k\geq 2 k","Consider a sequence of positive real numbers $(a_n)$. Define $[a_1]=\frac{1}{a_1}$ and recursively inductively $[a_1,\cdots,a_n]=\frac{1}{a_1+[a_2,\cdots,a_n]}$. Suppose $a_k\geq 2$ for all $k$. How to show that  $\lim_{n\to\infty}[a_1,\cdots,a_n]$ exists? I was trying to show that the sequence is monotone, which is not true. A special related case is done here , which is not very helpful to have a generalization. [Added to answer the confusion in comments.] The definition above should be understood properly as follows. For any positive real number $a$, define $[a]:=\frac{1}{a}$. Now, given any two positive real numbers $a_1,a_2$, one can define $[a_1,a_2]:=\frac{1}{1+[a_2]}$. One can thus keep going on in this fashion to define $[a_1,a_2,\cdots,a_n]$. To write down a few terms explicitly,  $$ [a_1,a_2]=\frac{1}{a_1+\frac{1}{a_2}},\  [a_1,a_2,a_3]=\frac{1}{a_1+\frac{1}{a_2+\frac{1}{a_3}}},\ [a_1,a_2,a_3,a_4]=\frac{1}{a_1+\frac{1}{a_2+\frac{1}{a_3+\frac{1}{a_4}}}},\cdots $$","Consider a sequence of positive real numbers $(a_n)$. Define $[a_1]=\frac{1}{a_1}$ and recursively inductively $[a_1,\cdots,a_n]=\frac{1}{a_1+[a_2,\cdots,a_n]}$. Suppose $a_k\geq 2$ for all $k$. How to show that  $\lim_{n\to\infty}[a_1,\cdots,a_n]$ exists? I was trying to show that the sequence is monotone, which is not true. A special related case is done here , which is not very helpful to have a generalization. [Added to answer the confusion in comments.] The definition above should be understood properly as follows. For any positive real number $a$, define $[a]:=\frac{1}{a}$. Now, given any two positive real numbers $a_1,a_2$, one can define $[a_1,a_2]:=\frac{1}{1+[a_2]}$. One can thus keep going on in this fashion to define $[a_1,a_2,\cdots,a_n]$. To write down a few terms explicitly,  $$ [a_1,a_2]=\frac{1}{a_1+\frac{1}{a_2}},\  [a_1,a_2,a_3]=\frac{1}{a_1+\frac{1}{a_2+\frac{1}{a_3}}},\ [a_1,a_2,a_3,a_4]=\frac{1}{a_1+\frac{1}{a_2+\frac{1}{a_3+\frac{1}{a_4}}}},\cdots $$",,"['real-analysis', 'sequences-and-series']"
6,Convergence of the taylor polynomial $P_{2n}$ to $f$ implies convergence of $P_n$?,Convergence of the taylor polynomial  to  implies convergence of ?,P_{2n} f P_n,"Let $f:(a,b) \to \mathbb{R}$ be infinitely differentiable at $x_0 \in (a,b)$, and let $P_n$ be its taylor polynomial of order $n$ around $x_0$. Fix a point $x \in (a,b)$, and suppose that $\lim_{n \to \infty }P_{2n}(x)=f(x)$. Is it true that $\lim_{n \to \infty }P_n(x)=f(x)$? More generally, is it true that if $P_{2n}(x)$ converges (not necessarily to $f(x)$) then $P_n(x)$ also converges?","Let $f:(a,b) \to \mathbb{R}$ be infinitely differentiable at $x_0 \in (a,b)$, and let $P_n$ be its taylor polynomial of order $n$ around $x_0$. Fix a point $x \in (a,b)$, and suppose that $\lim_{n \to \infty }P_{2n}(x)=f(x)$. Is it true that $\lim_{n \to \infty }P_n(x)=f(x)$? More generally, is it true that if $P_{2n}(x)$ converges (not necessarily to $f(x)$) then $P_n(x)$ also converges?",,"['calculus', 'real-analysis', 'taylor-expansion']"
7,Given $|f(x) - f(y)| \leq 7(x-y)^2$ show $f$ is a constant function [duplicate],Given  show  is a constant function [duplicate],|f(x) - f(y)| \leq 7(x-y)^2 f,"This question already has answers here : Function on $[a,b]$ that satisfies a Hölder condition of order $\alpha > 1 $ is constant (2 answers) Closed 6 years ago . I think that I've managed to figure this one out, I was just wondering if someone could look it over because there are a few areas that I'm not sure about my steps. Here is the full question: Suppose $f$ is a real valued function defined on all $x$ and $|f(x) - f(y)| \leq 7(x-y)^2$ for all $x$ and $y$. Prove $f$ is a constant function. First, define $y = x+h$ and take $h \neq 0$ , then $|f(y) - f(x)| \leq 7(x-y)^2$ becomes $|f(x+h) - f(x)| \leq 7h^2$. Dividing by $|h|$ we get: $\left|\frac{f(x+h) - f(x)}{h}\right| \leq 7|h|$. Clearly, $\lim_{h\rightarrow 0} 7|h| = 0$. Then by the sandwich theorem (squeeze theorem): $\lim_{h\rightarrow 0} \left|\frac{f(x+h) - f(x)}{h}\right| = 0.$ And thus $f'(x) = 0$ for arbitrary $x$. So $f(x)$ is a constant function. Added crucial condition on $h$ thanks to help from comments","This question already has answers here : Function on $[a,b]$ that satisfies a Hölder condition of order $\alpha > 1 $ is constant (2 answers) Closed 6 years ago . I think that I've managed to figure this one out, I was just wondering if someone could look it over because there are a few areas that I'm not sure about my steps. Here is the full question: Suppose $f$ is a real valued function defined on all $x$ and $|f(x) - f(y)| \leq 7(x-y)^2$ for all $x$ and $y$. Prove $f$ is a constant function. First, define $y = x+h$ and take $h \neq 0$ , then $|f(y) - f(x)| \leq 7(x-y)^2$ becomes $|f(x+h) - f(x)| \leq 7h^2$. Dividing by $|h|$ we get: $\left|\frac{f(x+h) - f(x)}{h}\right| \leq 7|h|$. Clearly, $\lim_{h\rightarrow 0} 7|h| = 0$. Then by the sandwich theorem (squeeze theorem): $\lim_{h\rightarrow 0} \left|\frac{f(x+h) - f(x)}{h}\right| = 0.$ And thus $f'(x) = 0$ for arbitrary $x$. So $f(x)$ is a constant function. Added crucial condition on $h$ thanks to help from comments",,"['real-analysis', 'derivatives', 'proof-verification']"
8,$L^2$ decay of solutions of heat equation,decay of solutions of heat equation,L^2,"Consider heat equation: in $\Omega \subset \mathbb R^n:$ \begin{equation*} \begin{cases} \begin{aligned} u_t - \Delta u &=0 \\ u(0,x) &= u_0 \\ u &= 0\quad\text{on }\partial \Omega \end{aligned} \end{cases} \end{equation*} I want to show $\|u(t)\|_{L^2}\rightarrow 0$ as $t \rightarrow \infty$ . So far, we know $$\frac{d}{dt} \int_\Omega u^2 = 2\int uu_t = 2\int u\Delta u = -2\int|\nabla u|^2 \leq 0.$$ I doubt we can conclude from here that $\|u(t)\|_{L^2}\rightarrow 0$ as $t \rightarrow \infty$ . The derivative of the function is negative and the function $\int_\Omega u^2$ is non-negative.","Consider heat equation: in I want to show as . So far, we know I doubt we can conclude from here that as . The derivative of the function is negative and the function is non-negative.","\Omega \subset \mathbb R^n: \begin{equation*}
\begin{cases}
\begin{aligned}
u_t - \Delta u &=0 \\
u(0,x) &= u_0 \\
u &= 0\quad\text{on }\partial \Omega
\end{aligned}
\end{cases}
\end{equation*} \|u(t)\|_{L^2}\rightarrow 0 t \rightarrow \infty \frac{d}{dt} \int_\Omega u^2 = 2\int uu_t = 2\int u\Delta u = -2\int|\nabla u|^2 \leq 0. \|u(t)\|_{L^2}\rightarrow 0 t \rightarrow \infty \int_\Omega u^2","['real-analysis', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
9,If $a+b=2$ so $a^a+b^b+3\sqrt[3]{a^2b^2}\geq5$,If  so,a+b=2 a^a+b^b+3\sqrt[3]{a^2b^2}\geq5,"Let $a$ and $b$ be positive numbers such that $a+b=2$. Prove that:   $$a^a+b^b+3\sqrt[3]{a^2b^2}\geq5$$ My trying. Easy to show that $x^x\geq\frac{x^3-x^2+x+1}{2}$ for all $x>0$, but $$\frac{a^3-a^2+a+1}{2}+\frac{b^3-b^2+b+1}{2}+3\sqrt[3]{a^2b^2}\geq5$$ is wrong for $a\rightarrow0^+$","Let $a$ and $b$ be positive numbers such that $a+b=2$. Prove that:   $$a^a+b^b+3\sqrt[3]{a^2b^2}\geq5$$ My trying. Easy to show that $x^x\geq\frac{x^3-x^2+x+1}{2}$ for all $x>0$, but $$\frac{a^3-a^2+a+1}{2}+\frac{b^3-b^2+b+1}{2}+3\sqrt[3]{a^2b^2}\geq5$$ is wrong for $a\rightarrow0^+$",,"['calculus', 'real-analysis', 'inequality', 'contest-math', 'exponential-function']"
10,Finding a limit - is my argument correct?,Finding a limit - is my argument correct?,,The limit is: $$ \lim_{\lambda\to 0} \frac{\int_{\lambda}^{a}{\frac{\cos(x)}{x}dx}}{\ln\lambda}. $$ My argument is: First rewrite the integral: $$ \lim_{\lambda\to 0} \frac{\int_{0}^{a}{\frac{\cos(x)}{x}dx} - \int_{0}^{\lambda}{\frac{\cos(x)}{x}dx}}{\ln\lambda}. $$ Then use l'Hopital's rule. The first term on the top vanishes as it has not $\lambda$ dependence. The second term is found by applying fundamental theorem of calculus. So I get: $$ \lim_{\lambda\to 0} \frac{-{\frac{\cos(\lambda)}{\lambda}dx}}{\frac{1}{\lambda}} = \lim_{\lambda\to 0}{-\cos(\lambda)} = -1. $$ Are there any problems with my argument?,The limit is: $$ \lim_{\lambda\to 0} \frac{\int_{\lambda}^{a}{\frac{\cos(x)}{x}dx}}{\ln\lambda}. $$ My argument is: First rewrite the integral: $$ \lim_{\lambda\to 0} \frac{\int_{0}^{a}{\frac{\cos(x)}{x}dx} - \int_{0}^{\lambda}{\frac{\cos(x)}{x}dx}}{\ln\lambda}. $$ Then use l'Hopital's rule. The first term on the top vanishes as it has not $\lambda$ dependence. The second term is found by applying fundamental theorem of calculus. So I get: $$ \lim_{\lambda\to 0} \frac{-{\frac{\cos(\lambda)}{\lambda}dx}}{\frac{1}{\lambda}} = \lim_{\lambda\to 0}{-\cos(\lambda)} = -1. $$ Are there any problems with my argument?,,"['calculus', 'real-analysis', 'limits']"
11,$\lim_{r\to 0}\frac{\operatorname{vol}f(B(a;r))}{\operatorname{vol}B(a;r)}=|\det f'(a)|$,,\lim_{r\to 0}\frac{\operatorname{vol}f(B(a;r))}{\operatorname{vol}B(a;r)}=|\det f'(a)|,"I'm trying to solve this question: Let $U\subset \mathbb R^m$ be an open set and $f:U\to \mathbb R^m$ a function of class $C^1$. Suppose there is  $a\in U$ such that $f'(a):\mathbb R^m\to \mathbb R^m$ is an isomorphism. Show $$\lim_{r\to 0}\frac{\operatorname{vol}f(B(a;r))}{\operatorname{vol}B(a;r)} = |\det f'(a)|$$ My attempt Using the inverse function theorem, there is $\delta>0$ such that $f_{|B(a,r)}$ is a diffeomorphism (henceforth for simplicity let's still call this restriction $f$). Suppose from now on $|r|\lt \delta$. Since $f$ is a diffeomorphism and $B(a,r)$ is compact we can use change of variables: \begin{align} & \operatorname{vol}f(B(a,r)) \\[10pt] = {} & \int_{f(B(a;r))}1\cdot dy \\[10pt] = {} & \int_{B(a;r)}1\cdot (f(a))|\det f'(a)|dx \\[10pt] = {} & \int1\cdot|\det f'(a)| \, dx \\[10pt] = {} & |\det f'(a)|\int_{B(a;r)}1 \, dx \\[10pt] = {} & |\det f'(a)|\operatorname{vol}B(a;r) \end{align} So I think I have proved that $\dfrac{\operatorname{vol}f(B(a;r))}{\operatorname{vol}B(a;r)}=|\det f'(a)|$ which is stronger than what the question asks. What is wrong with my answer and how can I correct that?","I'm trying to solve this question: Let $U\subset \mathbb R^m$ be an open set and $f:U\to \mathbb R^m$ a function of class $C^1$. Suppose there is  $a\in U$ such that $f'(a):\mathbb R^m\to \mathbb R^m$ is an isomorphism. Show $$\lim_{r\to 0}\frac{\operatorname{vol}f(B(a;r))}{\operatorname{vol}B(a;r)} = |\det f'(a)|$$ My attempt Using the inverse function theorem, there is $\delta>0$ such that $f_{|B(a,r)}$ is a diffeomorphism (henceforth for simplicity let's still call this restriction $f$). Suppose from now on $|r|\lt \delta$. Since $f$ is a diffeomorphism and $B(a,r)$ is compact we can use change of variables: \begin{align} & \operatorname{vol}f(B(a,r)) \\[10pt] = {} & \int_{f(B(a;r))}1\cdot dy \\[10pt] = {} & \int_{B(a;r)}1\cdot (f(a))|\det f'(a)|dx \\[10pt] = {} & \int1\cdot|\det f'(a)| \, dx \\[10pt] = {} & |\det f'(a)|\int_{B(a;r)}1 \, dx \\[10pt] = {} & |\det f'(a)|\operatorname{vol}B(a;r) \end{align} So I think I have proved that $\dfrac{\operatorname{vol}f(B(a;r))}{\operatorname{vol}B(a;r)}=|\det f'(a)|$ which is stronger than what the question asks. What is wrong with my answer and how can I correct that?",,['real-analysis']
12,Radially unbounded functions and 1D characterization,Radially unbounded functions and 1D characterization,,"Definition $f : \mathbb{R}^n \to \mathbb{R}$ is radially unbounded if $\|x\| \to \infty$ implies $f(x) \to \infty$ Question Which conditions on $f$ imply that if the one-dimensional function $t \to f(t d)$ is radially unbounded for any $d \in \mathbb{R}^n$ such that $\|d\|_2 = 1$ , then $f$ is radially unbounded? I have a counter-example, which is a discontinous $f$ . I was not able to find any counterexample for the case when $f$ is continuously differentiable, but I was also unable to prove the statement for this case. Are there any known theorems on the subject?","Definition is radially unbounded if implies Question Which conditions on imply that if the one-dimensional function is radially unbounded for any such that , then is radially unbounded? I have a counter-example, which is a discontinous . I was not able to find any counterexample for the case when is continuously differentiable, but I was also unable to prove the statement for this case. Are there any known theorems on the subject?",f : \mathbb{R}^n \to \mathbb{R} \|x\| \to \infty f(x) \to \infty f t \to f(t d) d \in \mathbb{R}^n \|d\|_2 = 1 f f f,"['real-analysis', 'analysis']"
13,Limit of derivative function,Limit of derivative function,,"$f:\Bbb{R}\to\Bbb{R},$ $f$ continuous, defined by $xf(x) = e^x-1$ then $$\lim _{n\to \infty }nf^{\left(n\right)}\left(x\right)= ?$$ I've tried to calculate $f'(x), f''(x)$ and $f'''(x)$ but I didn't find any pattern.","$f:\Bbb{R}\to\Bbb{R},$ $f$ continuous, defined by $xf(x) = e^x-1$ then $$\lim _{n\to \infty }nf^{\left(n\right)}\left(x\right)= ?$$ I've tried to calculate $f'(x), f''(x)$ and $f'''(x)$ but I didn't find any pattern.",,"['real-analysis', 'limits', 'functions', 'derivatives', 'continuity']"
14,Does every non null lebesgue measurable set contain a non-measurable subset?,Does every non null lebesgue measurable set contain a non-measurable subset?,,"We know that the Lebesgue measure obtained via the usual Caratheodory extension is complete. As such, the subset of every null set is null. Is it possible to prove that every non-null measurable subset $A\subseteq \mathbb R$ contains a non-measurable subset $B\subseteq A$? I suppose it must be true, if it has non-empty interior then it must be true (just use an isometry to take a ""copy"" of a vitali set inside the interval).","We know that the Lebesgue measure obtained via the usual Caratheodory extension is complete. As such, the subset of every null set is null. Is it possible to prove that every non-null measurable subset $A\subseteq \mathbb R$ contains a non-measurable subset $B\subseteq A$? I suppose it must be true, if it has non-empty interior then it must be true (just use an isometry to take a ""copy"" of a vitali set inside the interval).",,"['real-analysis', 'measure-theory']"
15,How to show that the Dini derivatives of a continuous function is measurable?,How to show that the Dini derivatives of a continuous function is measurable?,,"Suppose $F:[a,b]\to\mathbb{R}$ is continuous. Show that   $$ D^+(F)(x)=\limsup_{h\to 0+}\frac{F(x+h)-F(x)}{h} $$   is measurable. This question is related to this one . But specifically I would like to follow the hint in Stein-Shakarchi's Real Analysis : the continuity of $F$ allows one to restrict to countably many $h$ in taking the $\limsup$. I don't quite understand the hint. I guess one might aim at getting $$ D^+(F)(x)=\lim_{m\to\infty}\sup_{n\geq m}\biggr[F\bigr(x+\dfrac1n\bigr)-F(x)\biggr]\cdot n\tag{1} $$ But I don't see how to use the continuity of $F$ to get (1).","Suppose $F:[a,b]\to\mathbb{R}$ is continuous. Show that   $$ D^+(F)(x)=\limsup_{h\to 0+}\frac{F(x+h)-F(x)}{h} $$   is measurable. This question is related to this one . But specifically I would like to follow the hint in Stein-Shakarchi's Real Analysis : the continuity of $F$ allows one to restrict to countably many $h$ in taking the $\limsup$. I don't quite understand the hint. I guess one might aim at getting $$ D^+(F)(x)=\lim_{m\to\infty}\sup_{n\geq m}\biggr[F\bigr(x+\dfrac1n\bigr)-F(x)\biggr]\cdot n\tag{1} $$ But I don't see how to use the continuity of $F$ to get (1).",,['real-analysis']
16,"Exists $a < c < b$ where $\int_a^b (b - x)^n f^{(n + 1)}(x)\,dx = {{f^{(n + 1)}(c)}\over{n + 1}}(b - a)^{n + 1}$?",Exists  where ?,"a < c < b \int_a^b (b - x)^n f^{(n + 1)}(x)\,dx = {{f^{(n + 1)}(c)}\over{n + 1}}(b - a)^{n + 1}","Let $\tilde{a} < a < b < \tilde{b}$ and let $n$ be a nonnegative integer. Suppose $f(x)$ is a real-valued function on $(\tilde{a}, \tilde{b})$ which is $(n + 1)$-times continuously differentiable on $(\tilde{a}, \tilde{b})$. My question is this. Does there necessarily exist some $a < c < b$ such that$$\int_a^b (b - x)^n f^{(n + 1)}(x)\,dx = {{f^{(n + 1)}(c)}\over{n + 1}}(b - a)^{n + 1}?$$","Let $\tilde{a} < a < b < \tilde{b}$ and let $n$ be a nonnegative integer. Suppose $f(x)$ is a real-valued function on $(\tilde{a}, \tilde{b})$ which is $(n + 1)$-times continuously differentiable on $(\tilde{a}, \tilde{b})$. My question is this. Does there necessarily exist some $a < c < b$ such that$$\int_a^b (b - x)^n f^{(n + 1)}(x)\,dx = {{f^{(n + 1)}(c)}\over{n + 1}}(b - a)^{n + 1}?$$",,"['calculus', 'real-analysis']"
17,Inequality of continuous functions with integral,Inequality of continuous functions with integral,,"Let M be a positive real number.Let $f:[0,\infty)\to[0,M] $ be a continuous function satisfying $$ \int\limits_{0}^\infty (1+x)f(x)dx<\infty.$$ Prove the following inequlity. $$\left( \int\limits_{0}^\infty f(x)dx  \right)^2\le 4M\int\limits_{0}^\infty xf(x)dx.$$","Let M be a positive real number.Let $f:[0,\infty)\to[0,M] $ be a continuous function satisfying $$ \int\limits_{0}^\infty (1+x)f(x)dx<\infty.$$ Prove the following inequlity. $$\left( \int\limits_{0}^\infty f(x)dx  \right)^2\le 4M\int\limits_{0}^\infty xf(x)dx.$$",,"['calculus', 'real-analysis', 'analysis', 'inequality', 'integral-inequality']"
18,Abel/Cesaro summable implies Borel summable?,Abel/Cesaro summable implies Borel summable?,,"Does Abel or Cesaro summable imply Borel summable for a series? In other words, for a sequence $(a_n)$ and its partial sums $(s_n)$ , is it true that: $$ \begin{split} \lim_{n \to \infty}\frac{1}{n}\sum_{k=0}^{n-1} s_k &= A \implies \lim_{t \to \infty}e^{-t}\sum_{n=0}^{\infty}s_n\frac{t^n}{n!} = A\\ \lim_{x \to 1^-}\sum_{n=0}^{\infty}a_nx^n &= A \implies \lim_{t \to \infty}e^{-t}\sum_{n=0}^{\infty}s_n\frac{t^n}{n!} = A \end{split}\;? $$ Is there a proof of this if it is true? If it isn't, then is there a sequence which is Abel/Cesaro summable but not Borel summable, and is Borel summability consistent with Abel/Cesaro summability?","Does Abel or Cesaro summable imply Borel summable for a series? In other words, for a sequence and its partial sums , is it true that: Is there a proof of this if it is true? If it isn't, then is there a sequence which is Abel/Cesaro summable but not Borel summable, and is Borel summability consistent with Abel/Cesaro summability?","(a_n) (s_n) 
\begin{split}
\lim_{n \to \infty}\frac{1}{n}\sum_{k=0}^{n-1} s_k &= A \implies \lim_{t \to \infty}e^{-t}\sum_{n=0}^{\infty}s_n\frac{t^n}{n!} = A\\
\lim_{x \to 1^-}\sum_{n=0}^{\infty}a_nx^n &= A \implies \lim_{t \to \infty}e^{-t}\sum_{n=0}^{\infty}s_n\frac{t^n}{n!} = A
\end{split}\;?
","['real-analysis', 'summation', 'divergent-series', 'summation-method', 'cesaro-summable']"
19,Sum of $\sum_{n \geq 1} \frac{(\ln x +1)^n}{n^n}$,Sum of,\sum_{n \geq 1} \frac{(\ln x +1)^n}{n^n},"I want to find the sum of the following series $$\sum_{n \geq 1} \frac{(\ln x +1)^n}{n^n}$$ Using theorems on integration and differentiation of series. I can set $t=\ln x+1$ so that I get  $$\sum_{n \geq 1} \frac{t^n}{n^n}$$ But then I don't see how to proceed, since that $n^n$ is difficult to see as the result of a differentiation or integration. How can I see it?","I want to find the sum of the following series $$\sum_{n \geq 1} \frac{(\ln x +1)^n}{n^n}$$ Using theorems on integration and differentiation of series. I can set $t=\ln x+1$ so that I get  $$\sum_{n \geq 1} \frac{t^n}{n^n}$$ But then I don't see how to proceed, since that $n^n$ is difficult to see as the result of a differentiation or integration. How can I see it?",,"['calculus', 'real-analysis', 'sequences-and-series', 'summation', 'power-series']"
20,"Any homeomorphism from $[0,1)\to [0,1)$ has a fixed point.",Any homeomorphism from  has a fixed point.,"[0,1)\to [0,1)","Show that Any homeomorphism from $[0,1)\to [0,1)$ has a fixed point. My try : Suppose that  $f(x)\neq x$ for all $x$,then either $f(x)>x$ or $f(x)<x$ for all $x$ otherwise if $f(a)>a$ and  $f(b)<b$ for some  $a,b\in [0,1)$ then by IVP $f(p)=p;p\in [a,b]\subset [0,1)$ which is false. Hence take WLOG ; $f(x)>x$ for all $x$ ; Also if $f^{-1}(x)>x$ then by above we have $f(f^{-1}(x))>f^{-1}(x)>x\implies x>x $ false .Hence $f^{-1}(x)\le x$ for some $x$ . But I can't complete the proof from here.Please give some hints so that I can take it forward.","Show that Any homeomorphism from $[0,1)\to [0,1)$ has a fixed point. My try : Suppose that  $f(x)\neq x$ for all $x$,then either $f(x)>x$ or $f(x)<x$ for all $x$ otherwise if $f(a)>a$ and  $f(b)<b$ for some  $a,b\in [0,1)$ then by IVP $f(p)=p;p\in [a,b]\subset [0,1)$ which is false. Hence take WLOG ; $f(x)>x$ for all $x$ ; Also if $f^{-1}(x)>x$ then by above we have $f(f^{-1}(x))>f^{-1}(x)>x\implies x>x $ false .Hence $f^{-1}(x)\le x$ for some $x$ . But I can't complete the proof from here.Please give some hints so that I can take it forward.",,"['real-analysis', 'general-topology']"
21,"""Analytic Continuation"" of the Convolution Operator?","""Analytic Continuation"" of the Convolution Operator?",,"The convolution of two functions $f, g: \mathbb{R} \rightarrow \mathbb{R}$ is defined as: $$(f \ast g)(t) := \displaystyle \int_{\mathbb{R}}f(\tau)g(t - \tau)d\tau,$$ and can be generalised to higher dimensions. A result of significant importance is the well-known convolution theorem, a version of which states that: $$\displaystyle \hat{f} \ast \hat{g} = \widehat{f \cdot g},$$ where $\hat{f}$ denotes the Fourier transform of $f$. Since it can also be shown that $\ast$ is associative, then we also have: $$\hat{f}^{\otimes_{k}} = \underbrace{\hat{f} \ast ... \ast \hat{f}}_{k \text{ terms}} = \widehat{f^k},$$ for any positive integer $k$. Now, as silly as this sounds, I would like to investigate this for $k = 1/2$. That is, given that we know the Fourier coefficients of $f$, can we derive, in terms of convolutions, the Fourier coefficients of $\sqrt{f}$ ? There is such an extension of the convolution operator, known as fractional convolution, for which several papers have been published, the majority of which fall in the category of signal processing. However, the ""fractional convolution theorems"" seem to depend on being able to write $f$ as a product of functions whose Fourier coefficients are known (see, for example, David Mustard's 1995 paper entitled Fractional Convolution). But this is not possible here. Does anyone know of an analytic continuation of $\otimes_{k}$ to rational values of $k$ that would allow this to be possible? Otherwise, are there any other ways to find the Fourier coefficients of $\sqrt{f}$ given that the Fourier coefficients of $f$ are known?","The convolution of two functions $f, g: \mathbb{R} \rightarrow \mathbb{R}$ is defined as: $$(f \ast g)(t) := \displaystyle \int_{\mathbb{R}}f(\tau)g(t - \tau)d\tau,$$ and can be generalised to higher dimensions. A result of significant importance is the well-known convolution theorem, a version of which states that: $$\displaystyle \hat{f} \ast \hat{g} = \widehat{f \cdot g},$$ where $\hat{f}$ denotes the Fourier transform of $f$. Since it can also be shown that $\ast$ is associative, then we also have: $$\hat{f}^{\otimes_{k}} = \underbrace{\hat{f} \ast ... \ast \hat{f}}_{k \text{ terms}} = \widehat{f^k},$$ for any positive integer $k$. Now, as silly as this sounds, I would like to investigate this for $k = 1/2$. That is, given that we know the Fourier coefficients of $f$, can we derive, in terms of convolutions, the Fourier coefficients of $\sqrt{f}$ ? There is such an extension of the convolution operator, known as fractional convolution, for which several papers have been published, the majority of which fall in the category of signal processing. However, the ""fractional convolution theorems"" seem to depend on being able to write $f$ as a product of functions whose Fourier coefficients are known (see, for example, David Mustard's 1995 paper entitled Fractional Convolution). But this is not possible here. Does anyone know of an analytic continuation of $\otimes_{k}$ to rational values of $k$ that would allow this to be possible? Otherwise, are there any other ways to find the Fourier coefficients of $\sqrt{f}$ given that the Fourier coefficients of $f$ are known?",,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'signal-processing']"
22,"Compute $\lim_{t\rightarrow0}\int_B \frac{l(t,x,y)}{t^2}d(x,y)$.",Compute .,"\lim_{t\rightarrow0}\int_B \frac{l(t,x,y)}{t^2}d(x,y)","This is a problem that I saw on the Internet. I'd like some hints to solve it. Let $B$ be a ball in $\mathbb{R}^2$. Given $(x,y)\in B$, consider the circle of radius $t>0$ and centre $(x,y)$. Let $l(t,x,y)$ be the length of the arc of that circle that is outside of $B$. Find $$\lim_{t\rightarrow0}\int_B \frac{l(t,x,y)}{t^2}d(x,y).$$ Here is a picture: in black the boundary of the Ball $B$. In $\color{red}{\text{red}}$ (+ blue) the circle considered (centered at some $x,y$ with radius $t$). In $\color{blue}{\text{blue}}$ the arc whose length we define to be $l(t,x,y)$","This is a problem that I saw on the Internet. I'd like some hints to solve it. Let $B$ be a ball in $\mathbb{R}^2$. Given $(x,y)\in B$, consider the circle of radius $t>0$ and centre $(x,y)$. Let $l(t,x,y)$ be the length of the arc of that circle that is outside of $B$. Find $$\lim_{t\rightarrow0}\int_B \frac{l(t,x,y)}{t^2}d(x,y).$$ Here is a picture: in black the boundary of the Ball $B$. In $\color{red}{\text{red}}$ (+ blue) the circle considered (centered at some $x,y$ with radius $t$). In $\color{blue}{\text{blue}}$ the arc whose length we define to be $l(t,x,y)$",,"['real-analysis', 'integration', 'contest-math']"
23,Textbooks for real analysis that will help me solve actual problems??,Textbooks for real analysis that will help me solve actual problems??,,"Are there any analysis textbooks with concrete examples and problem sets? I've studied mathematical analysis and real analysis with Rudin. There was not much trouble for me in understanding what was written inside those textbooks. But recently I've found I have a problem while preparing for my GRE maths test. I know all the definitions and theorems in the textbooks but I'm not really able to apply them to real questions. So for instance I know what a limit is and what traits it has, but if you give me a complicated function and ask me to find the limit I'm not really able to do so. I think maybe this is because abstract theory and application are slightly different and even if I know the theory I need some practice in application to solve problems for my GRE test. And I don't think Rudin will be the one who can help. So do any of you know good textbooks that will help me prepare in learning the 'practical' maths?","Are there any analysis textbooks with concrete examples and problem sets? I've studied mathematical analysis and real analysis with Rudin. There was not much trouble for me in understanding what was written inside those textbooks. But recently I've found I have a problem while preparing for my GRE maths test. I know all the definitions and theorems in the textbooks but I'm not really able to apply them to real questions. So for instance I know what a limit is and what traits it has, but if you give me a complicated function and ask me to find the limit I'm not really able to do so. I think maybe this is because abstract theory and application are slightly different and even if I know the theory I need some practice in application to solve problems for my GRE test. And I don't think Rudin will be the one who can help. So do any of you know good textbooks that will help me prepare in learning the 'practical' maths?",,"['real-analysis', 'analysis']"
24,Analytic Number Theory - Prerequisites [closed],Analytic Number Theory - Prerequisites [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question I'm interested in studying Analytic Number Theory using Apostol's Introduction to Analytic Number Theory. Having looked briefly at Apostol's Introduction to Analytic Number Theory and having read the reviews, I can see that complex analysis is assumed in this book. In my degree I studied some complex analysis (up to and including the residue theorem). However this was things like using Laurent Series, contour integration methods and stuff - I'm not sure if that would count as complex 'analysis' (as this was not a hugely rigorous course (in the sense that a Real Analysis course would cover deriving the derivative and stuff by use of limits, convergence etc)). Also I did not study any real analysis. I also studied a lot of pure maths (group theory, ring and field theory, number theory). Would it be sufficient (or necessary) to go over: Basic real analysis (up to convergence of functions). based on the book: http://www.springer.com/us/book/9781493927111 up to the chapter 'Functional Limits and Continuity' and Complex Variable up to and including the residue theorem (based on this book: https://www.pearsonhighered.com/program/Osborne-Complex-Variables-and-their-Applications/PGM271956.html Chapters 1-5 (in particular, is this rigorous enough for a prerequisite to Analytic Number Theory? This was the book used when I was an undergraduate (in actual fact taught by the author) Thank you for any guidance.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question I'm interested in studying Analytic Number Theory using Apostol's Introduction to Analytic Number Theory. Having looked briefly at Apostol's Introduction to Analytic Number Theory and having read the reviews, I can see that complex analysis is assumed in this book. In my degree I studied some complex analysis (up to and including the residue theorem). However this was things like using Laurent Series, contour integration methods and stuff - I'm not sure if that would count as complex 'analysis' (as this was not a hugely rigorous course (in the sense that a Real Analysis course would cover deriving the derivative and stuff by use of limits, convergence etc)). Also I did not study any real analysis. I also studied a lot of pure maths (group theory, ring and field theory, number theory). Would it be sufficient (or necessary) to go over: Basic real analysis (up to convergence of functions). based on the book: http://www.springer.com/us/book/9781493927111 up to the chapter 'Functional Limits and Continuity' and Complex Variable up to and including the residue theorem (based on this book: https://www.pearsonhighered.com/program/Osborne-Complex-Variables-and-their-Applications/PGM271956.html Chapters 1-5 (in particular, is this rigorous enough for a prerequisite to Analytic Number Theory? This was the book used when I was an undergraduate (in actual fact taught by the author) Thank you for any guidance.",,"['real-analysis', 'complex-analysis', 'analytic-number-theory']"
25,Is the limit of a convergent sequence always a limit point of the sequence or the range of the sequence?,Is the limit of a convergent sequence always a limit point of the sequence or the range of the sequence?,,"In this video lecture ( Real analysis, HMC,2010, by Prof. Su ) Professor Francis Su says (around 54:30) that ""If a sequence $\{p_n\}$ converges to a point $p$ it does not necessarily mean $p$ is a limit point of the range of $\{p_n\}$."" I'm not sure how that can hold (except in case of a constant sequence). I'm not able to understand the difference between the set $\{p_n\}$ and the range of $\{p_n\}$ (which I understand is the set of all values attained by $p_n$). As per my understanding they're the same (except $\{p_n\}$ might contain some repeated values which the range of $p_n$ won't, e.g. the range of the sequence {1/2,1/2,1/2,1/2,1/3,1/3,1/3,1/3,1/4,1/4,...} would be {1/2,1/3,1/4,...}, or that of the constant sequence {1,1,1,...} would be {1}. Based on this understanding, my reasoning is as follows: By the definition of a convergent sequence $\{p_n\}$ converging to $p$, for every $\epsilon \gt 0$ we can find an infinite number of terms of $\{p_n\}$ which lie at a distance less than $\epsilon $ from p, i.e. within an $\epsilon $-neighborhood of $p$. Hence every $\epsilon $-neighborhood of p contains an infinite number of points of the set $\{p_n\}$ other than itself ($p$). Hence $p$ is a limit point of $\{p_n\}$. Now this would not hold only in case of a constant sequence, which converges, but any neighborhood of of its limit cannot contain any points in common with the sequence other than itself . Hence the limit won't be a limit point of the sequence. Other than this special case, I cannot think of any situation where the limit of a convergent sequence is not also a limit point of the sequence. Can anyone help? Thanks in advance.","In this video lecture ( Real analysis, HMC,2010, by Prof. Su ) Professor Francis Su says (around 54:30) that ""If a sequence $\{p_n\}$ converges to a point $p$ it does not necessarily mean $p$ is a limit point of the range of $\{p_n\}$."" I'm not sure how that can hold (except in case of a constant sequence). I'm not able to understand the difference between the set $\{p_n\}$ and the range of $\{p_n\}$ (which I understand is the set of all values attained by $p_n$). As per my understanding they're the same (except $\{p_n\}$ might contain some repeated values which the range of $p_n$ won't, e.g. the range of the sequence {1/2,1/2,1/2,1/2,1/3,1/3,1/3,1/3,1/4,1/4,...} would be {1/2,1/3,1/4,...}, or that of the constant sequence {1,1,1,...} would be {1}. Based on this understanding, my reasoning is as follows: By the definition of a convergent sequence $\{p_n\}$ converging to $p$, for every $\epsilon \gt 0$ we can find an infinite number of terms of $\{p_n\}$ which lie at a distance less than $\epsilon $ from p, i.e. within an $\epsilon $-neighborhood of $p$. Hence every $\epsilon $-neighborhood of p contains an infinite number of points of the set $\{p_n\}$ other than itself ($p$). Hence $p$ is a limit point of $\{p_n\}$. Now this would not hold only in case of a constant sequence, which converges, but any neighborhood of of its limit cannot contain any points in common with the sequence other than itself . Hence the limit won't be a limit point of the sequence. Other than this special case, I cannot think of any situation where the limit of a convergent sequence is not also a limit point of the sequence. Can anyone help? Thanks in advance.",,"['real-analysis', 'sequences-and-series']"
26,Derivative of Convolution with Respect to One of the Arguments of the Convolution,Derivative of Convolution with Respect to One of the Arguments of the Convolution,,"Continuous Case Let $ z \left( t \right) = \left( h \ast x \right) \left( t \right) $. What is the derivative of $ z \left( t \right) $ with respect to $ x \left( t \right) $? Discrete Case Given $2$ vectors $ x \in \mathbb{R}^{n} $ and $ h \in \mathbb{R}^{m} $, their convolution given by $$ z = h \ast x $$ What would be the gradient of $ z $ with respect to $ x $? And what would be the gradient w ith respect to $ x $ of the following quadratic cost function? $$ \frac{1}{2} {\left\| h \ast x - y \right\|}_{2}^{2} $$","Continuous Case Let $ z \left( t \right) = \left( h \ast x \right) \left( t \right) $. What is the derivative of $ z \left( t \right) $ with respect to $ x \left( t \right) $? Discrete Case Given $2$ vectors $ x \in \mathbb{R}^{n} $ and $ h \in \mathbb{R}^{m} $, their convolution given by $$ z = h \ast x $$ What would be the gradient of $ z $ with respect to $ x $? And what would be the gradient w ith respect to $ x $ of the following quadratic cost function? $$ \frac{1}{2} {\left\| h \ast x - y \right\|}_{2}^{2} $$",,"['real-analysis', 'derivatives', 'operator-theory', 'matrix-calculus', 'convolution']"
27,An application of Egoroff' theorem,An application of Egoroff' theorem,,"Let $\left\{f_{n}\right\}$ be a sequence of measurable functions on the real line $\mathbb{R}$, and $f_n\rightarrow f$ almost everywhere. Prove that there exists a sequence of measurable sets $\left\{E_{k}\right\}$ such that the Lebesgue measure of $\mathbb{R}\setminus\bigcup^{\infty}_{k=1}E_{k}$ is zero, and $f_{n}\rightarrow f$ uniformly on each $E_{k}$. The first thing occurs to me is the Egoroff's theorem. So my thought was to construct some bounded sets and select subsets of it using Egoroff's theorem to finish the proof. But I don't know how to choose these sets.","Let $\left\{f_{n}\right\}$ be a sequence of measurable functions on the real line $\mathbb{R}$, and $f_n\rightarrow f$ almost everywhere. Prove that there exists a sequence of measurable sets $\left\{E_{k}\right\}$ such that the Lebesgue measure of $\mathbb{R}\setminus\bigcup^{\infty}_{k=1}E_{k}$ is zero, and $f_{n}\rightarrow f$ uniformly on each $E_{k}$. The first thing occurs to me is the Egoroff's theorem. So my thought was to construct some bounded sets and select subsets of it using Egoroff's theorem to finish the proof. But I don't know how to choose these sets.",,"['real-analysis', 'measure-theory']"
28,Finding a special subsequence of any Cauchy sequence,Finding a special subsequence of any Cauchy sequence,,"Let $(X,d)$ be a metric space and let $(x_n)$ be a Cauchy sequence in $X$. Let $(\epsilon_n)$ be a sequence of real numbers and decrease to $0$. Show that there is a subsequence $(x_{n_k})$ of $(x_n)$ such that: $$d(x_{n_j},x_{n_k})<\epsilon_{\min\{j,k\}}\:\:\: j,k=1,2,\dots$$ I'm not sure about my solution. Each time I try to find a subsequence, it just makes me more confused. Here is my solution. Since $(x_n)$ is Cauchy, so for $\epsilon_1$ there is an integer $N_1\in\mathbb{N}$, such that for any $j,m\geq N_1$, we have $d(x_m,x_j)<\epsilon_1.$ Now define a subsequence $(x_{n_k})$ of $(x_n)$ such that: $x_{n_1}=x_{N_1},\: x_{n_2}=x_{N_1+1}$ and so on. Obviously, the subsequence $(x_{n_k})$ is Cauchy. Similarly, for $\epsilon_2$, there is an integer $N_2$ such that for any $m,j\geq N_2$, we have $d(x_{n_m},x_{n_j})<\epsilon_2.$ Without loss of generality, let $(x_{n})=(x_{n_k})$. Define the subsequence $(x_{n_k})$ of $(x_{n})$ such that $x_{n_1}=x_{N_2}, \: x_{n_2}=x_{N_2+1}$. Hence, $d(x_{n_1},x_{n_2})<\epsilon_2<\epsilon_1$ and also, $d(x_{n_2},x_{n_3})<\epsilon_2$. Continuing this method $n'$ times, for $\epsilon_{n'}$, there is an integer $N_{n'}$ such that for any $m,j\geq N_{n'}$, we have $d(x_{m},x_{j})<\epsilon_{n'}$. Now define seubsequence $(x_{n_k})$ of $(x_n)$ such that: $x_{n_1}=x_{N_{n'}},\: x_{n_2}=x_{N_{n'}+1}$ and so on.  Now for the subsequence $(x_{n_k})$ we have: $$d(x_{n_j},x_{n_k})<\epsilon_{n'}<\epsilon_{\min\{j,k\}}\:\:\: j,k=1,2,\dots,n' $$","Let $(X,d)$ be a metric space and let $(x_n)$ be a Cauchy sequence in $X$. Let $(\epsilon_n)$ be a sequence of real numbers and decrease to $0$. Show that there is a subsequence $(x_{n_k})$ of $(x_n)$ such that: $$d(x_{n_j},x_{n_k})<\epsilon_{\min\{j,k\}}\:\:\: j,k=1,2,\dots$$ I'm not sure about my solution. Each time I try to find a subsequence, it just makes me more confused. Here is my solution. Since $(x_n)$ is Cauchy, so for $\epsilon_1$ there is an integer $N_1\in\mathbb{N}$, such that for any $j,m\geq N_1$, we have $d(x_m,x_j)<\epsilon_1.$ Now define a subsequence $(x_{n_k})$ of $(x_n)$ such that: $x_{n_1}=x_{N_1},\: x_{n_2}=x_{N_1+1}$ and so on. Obviously, the subsequence $(x_{n_k})$ is Cauchy. Similarly, for $\epsilon_2$, there is an integer $N_2$ such that for any $m,j\geq N_2$, we have $d(x_{n_m},x_{n_j})<\epsilon_2.$ Without loss of generality, let $(x_{n})=(x_{n_k})$. Define the subsequence $(x_{n_k})$ of $(x_{n})$ such that $x_{n_1}=x_{N_2}, \: x_{n_2}=x_{N_2+1}$. Hence, $d(x_{n_1},x_{n_2})<\epsilon_2<\epsilon_1$ and also, $d(x_{n_2},x_{n_3})<\epsilon_2$. Continuing this method $n'$ times, for $\epsilon_{n'}$, there is an integer $N_{n'}$ such that for any $m,j\geq N_{n'}$, we have $d(x_{m},x_{j})<\epsilon_{n'}$. Now define seubsequence $(x_{n_k})$ of $(x_n)$ such that: $x_{n_1}=x_{N_{n'}},\: x_{n_2}=x_{N_{n'}+1}$ and so on.  Now for the subsequence $(x_{n_k})$ we have: $$d(x_{n_j},x_{n_k})<\epsilon_{n'}<\epsilon_{\min\{j,k\}}\:\:\: j,k=1,2,\dots,n' $$",,"['real-analysis', 'analysis', 'metric-spaces', 'cauchy-sequences']"
29,Is g with $\cosh(xg(x))=x\cosh(g(x))$ decreasing?,Is g with  decreasing?,\cosh(xg(x))=x\cosh(g(x)),"Let $g:\mathbb{R}_{>0} \to \mathbb{R}_{>0}$ be defined implicitly by $\cosh(xg(x))=x\cosh(g(x))$ and $g(1)\sinh(g(1))=\cosh(g(1))$. How to show that $g$ is differentiable? Furthermore, is it true that $g$ is monotone decreasing? Assuming that $g$ is differentiable, I showed that $g'\mid_{]0,1[}<0$ is equivalent to $g\mid_{]1,\infty[}<g(1)$ but I wasn't able to show much more.","Let $g:\mathbb{R}_{>0} \to \mathbb{R}_{>0}$ be defined implicitly by $\cosh(xg(x))=x\cosh(g(x))$ and $g(1)\sinh(g(1))=\cosh(g(1))$. How to show that $g$ is differentiable? Furthermore, is it true that $g$ is monotone decreasing? Assuming that $g$ is differentiable, I showed that $g'\mid_{]0,1[}<0$ is equivalent to $g\mid_{]1,\infty[}<g(1)$ but I wasn't able to show much more.",,"['calculus', 'real-analysis', 'implicit-differentiation', 'implicit-function-theorem']"
30,Prove that $\operatorname{p.v.}(k\;*f)$ does not exist if $k(x)=|x|^{-n+i\gamma}$ and $f\in\mathcal{C}_c^1$,Prove that  does not exist if  and,\operatorname{p.v.}(k\;*f) k(x)=|x|^{-n+i\gamma} f\in\mathcal{C}_c^1,"I put a bounty only because I need quickly a solution, NOT because I know it's difficult - maybe it is, maybe not. I'm trying to do it, but without results. If I get some ""intermediate result"" (something I think it could be useful) I will write it here immediately. Consider the function $k(x)=|x|^{-n+i\gamma}$ defined on $\Bbb R^n$, where $\gamma$ is any nonzero real number and $i$ is the imaginary unit. $k$ is homogeneous of degree $-n+i\gamma$. I have to show that $$ \operatorname{p.v.}(k\;*f)(x):=\lim_{\epsilon\to0^+} \int_{\{|t|>\epsilon\}}|t|^{-n+i\gamma}f(x-t)\;dt %(k\chi_{\{|x|>\epsilon\}})*f $$ does NOT exists for $f\in\mathcal C_c^1(\Bbb R^n)$ (i.e. $\mathcal{C}^1$ functions with compact support), but we can find a sequence $\epsilon_j\to0$ such that $$ \lim_{j\to+\infty}\int_{\{|t|>\epsilon_j\}}|t|^{-n+i\gamma}f(x-t)\;dt %(k\chi_{\{|x|>\epsilon_j\}})*f $$ exists for such $f\;$. We exclude the trivial case $f\equiv 0$. I don't know where to start, can someone help me please?","I put a bounty only because I need quickly a solution, NOT because I know it's difficult - maybe it is, maybe not. I'm trying to do it, but without results. If I get some ""intermediate result"" (something I think it could be useful) I will write it here immediately. Consider the function $k(x)=|x|^{-n+i\gamma}$ defined on $\Bbb R^n$, where $\gamma$ is any nonzero real number and $i$ is the imaginary unit. $k$ is homogeneous of degree $-n+i\gamma$. I have to show that $$ \operatorname{p.v.}(k\;*f)(x):=\lim_{\epsilon\to0^+} \int_{\{|t|>\epsilon\}}|t|^{-n+i\gamma}f(x-t)\;dt %(k\chi_{\{|x|>\epsilon\}})*f $$ does NOT exists for $f\in\mathcal C_c^1(\Bbb R^n)$ (i.e. $\mathcal{C}^1$ functions with compact support), but we can find a sequence $\epsilon_j\to0$ such that $$ \lim_{j\to+\infty}\int_{\{|t|>\epsilon_j\}}|t|^{-n+i\gamma}f(x-t)\;dt %(k\chi_{\{|x|>\epsilon_j\}})*f $$ exists for such $f\;$. We exclude the trivial case $f\equiv 0$. I don't know where to start, can someone help me please?",,"['real-analysis', 'functional-analysis', 'harmonic-analysis']"
31,"Show $x_n := \frac{1-p^{n+1}}{1-p^n} \frac{n}{n+1}$ is increasing in n for $p \in (0,1)$",Show  is increasing in n for,"x_n := \frac{1-p^{n+1}}{1-p^n} \frac{n}{n+1} p \in (0,1)","I need to show that $x_n := \frac{1-p^{n+1}}{1-p^n} \frac{n}{n+1}$ is increasing in $n$ for $p \in (0,1)$. My attempts have involved trying to show $\begin{eqnarray*} \frac{1-p^{n+2}}{1-p^{n+1}} \frac{n+1}{n+2} &>& \frac{1-p^{n+1}}{1-p^n} \frac{n}{n+1} \\ \left(1 + \frac{p^{n+1}}{\sum_{k=0}^n p^k} \right) \frac{n+1}{n+2} &>& \left(1 + \frac{p^{n}}{\sum_{k=0}^{n-1} p^k} \right) \frac{n}{n+1} \\  \frac{n+1}{n+2} - \frac{n}{n+1} &>& \left(\frac{p^{n}}{\sum_{k=0}^{n-1} p^k} \right)\frac{n}{n+1}-\left(\frac{p^{n+1}}{\sum_{k=0}^n p^k} \right) \frac{n+1}{n+2}\\  \frac{n+1}{n+2} - \frac{n}{n+1} &>& p^n \left[\left(\frac{1}{\sum_{k=0}^{n-1} p^k} \right)\frac{n}{n+1}-\left(\frac{p}{\sum_{k=0}^n p^k} \right) \frac{n+1}{n+2} \right]\end{eqnarray*}$ but this does not seem to simplify. I have also attmpted to treat $x_n$ as a continuous function in $n$ and differentiated (this leaves a mess so I won't put it up here). Sadly, I seem to be unable to get my required result from either. EDIT: It might be helpful to note that $x_n = \frac{\sum_{k=0}^n p^k}{\sum_{k=0}^{n-1} p^k}\frac{n}{n+1}$ EDIT 2: I can show that $x_n < 1$ for all $n \in \mathbb{N}$ and that $x_n \to 1$. But I don't think this suffices... EDIT 3: If we can show that $\frac{\sum_{k=0}^{n+1}p^k}{n+2} \cdot \frac{\sum_{k=0}^{n-1}p^k}{n} \geq  \left(\frac{\sum_{k=0}^{n+1}p^k}{n+2} \right)^2$, we are done. But I don't know how to show this.","I need to show that $x_n := \frac{1-p^{n+1}}{1-p^n} \frac{n}{n+1}$ is increasing in $n$ for $p \in (0,1)$. My attempts have involved trying to show $\begin{eqnarray*} \frac{1-p^{n+2}}{1-p^{n+1}} \frac{n+1}{n+2} &>& \frac{1-p^{n+1}}{1-p^n} \frac{n}{n+1} \\ \left(1 + \frac{p^{n+1}}{\sum_{k=0}^n p^k} \right) \frac{n+1}{n+2} &>& \left(1 + \frac{p^{n}}{\sum_{k=0}^{n-1} p^k} \right) \frac{n}{n+1} \\  \frac{n+1}{n+2} - \frac{n}{n+1} &>& \left(\frac{p^{n}}{\sum_{k=0}^{n-1} p^k} \right)\frac{n}{n+1}-\left(\frac{p^{n+1}}{\sum_{k=0}^n p^k} \right) \frac{n+1}{n+2}\\  \frac{n+1}{n+2} - \frac{n}{n+1} &>& p^n \left[\left(\frac{1}{\sum_{k=0}^{n-1} p^k} \right)\frac{n}{n+1}-\left(\frac{p}{\sum_{k=0}^n p^k} \right) \frac{n+1}{n+2} \right]\end{eqnarray*}$ but this does not seem to simplify. I have also attmpted to treat $x_n$ as a continuous function in $n$ and differentiated (this leaves a mess so I won't put it up here). Sadly, I seem to be unable to get my required result from either. EDIT: It might be helpful to note that $x_n = \frac{\sum_{k=0}^n p^k}{\sum_{k=0}^{n-1} p^k}\frac{n}{n+1}$ EDIT 2: I can show that $x_n < 1$ for all $n \in \mathbb{N}$ and that $x_n \to 1$. But I don't think this suffices... EDIT 3: If we can show that $\frac{\sum_{k=0}^{n+1}p^k}{n+2} \cdot \frac{\sum_{k=0}^{n-1}p^k}{n} \geq  \left(\frac{\sum_{k=0}^{n+1}p^k}{n+2} \right)^2$, we are done. But I don't know how to show this.",,"['real-analysis', 'sequences-and-series']"
32,Which real numbers have $2$ possible decimal representations?,Which real numbers have  possible decimal representations?,2,"I know that all positive and negative whole numbers have $2$ possible decimal representations. For example, $1+1+1+1$ could be represented as $4$ or as $3.99999...$ (I believe $4.000..1$ isn't a thing, right?). All terminating decimal numbers have $2$ representations as well, for example $1.5$ is the same as $1.4999..$ However, I can't really see how you would make a second representation for non-terminating decimals (especially irrational numbers) and zero.","I know that all positive and negative whole numbers have $2$ possible decimal representations. For example, $1+1+1+1$ could be represented as $4$ or as $3.99999...$ (I believe $4.000..1$ isn't a thing, right?). All terminating decimal numbers have $2$ representations as well, for example $1.5$ is the same as $1.4999..$ However, I can't really see how you would make a second representation for non-terminating decimals (especially irrational numbers) and zero.",,"['calculus', 'real-analysis', 'algebra-precalculus']"
33,Lebesgue measure 0 set which is not Borel,Lebesgue measure 0 set which is not Borel,,"Let $A$ be an uncountable Borel subset of $\mathbb{R}^n$, and consider the Lebesgue measure on $\mathbb{R}^n$. Assume the axiom of choice (if you need it). Does there exist a Lebesgue measurable set $B \subset A$, which has zero Lebesgue measure and is not Borel? I have no idea of the answer. I know Lebsegue measure theory, and more generally, abstract measure theory, but not the the theory of Polish spaces and analytic sets (the so called descriptive set theory, which could be the key to answer the question). Any help is welcome.","Let $A$ be an uncountable Borel subset of $\mathbb{R}^n$, and consider the Lebesgue measure on $\mathbb{R}^n$. Assume the axiom of choice (if you need it). Does there exist a Lebesgue measurable set $B \subset A$, which has zero Lebesgue measure and is not Borel? I have no idea of the answer. I know Lebsegue measure theory, and more generally, abstract measure theory, but not the the theory of Polish spaces and analytic sets (the so called descriptive set theory, which could be the key to answer the question). Any help is welcome.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
34,Proof of this inequality,Proof of this inequality,,"I have a finite sequence of positive numbers $(a_i)_1^n$ for which: $a_1>a_n$, $a_j\geq a_{j+1}\geq\cdots\geq a_n$ for some $j\in\{2,\ldots,n-1\}$, $a_1>a_2>\cdots>a_{j-1}$, $a_j\geq a_i$ for all $1\leq i\leq n$. I conjecture that: $$(a_n+a_2+\cdots+a_j)\left(\sum_{i=j+1}^{n-1}{\frac{a_i^2}{(a_1+\cdots+a_i)(a_n+a_2+\cdots + a_i)}}+\frac{a_1+a_n}{a_1+\cdots+a_n}\right) \geq  (a_n+a_1+\cdots+a_j)\left(\sum_{i=j+1}^{n-1}{\frac{a_i^2}{(a_1+\cdots+a_i)(a_n+a_1+\cdots + a_i)}}+\frac{a_n}{a_1+\cdots+a_n}\right).$$ I have an unappealing brute-force proof when $n\in\{3,4,5\}$  but I can't prove it in general. I have tried calculus to no avail, and it doesn't seem like a good fit for any of the standard inequalities.Does this look even remotely similar to anything already done? I appreciate that it's a rather ugly inequality but some suggestions would be greatly appreciated! The proof when $n=3$ is outlined below. By condition 2. we know that $j=2$ so that \begin{align*} (a_3+a_2)\left(\frac{a_1+a_3}{a_1+a_2+a_3}\right)-(a_3+a_1+a_2)\left(\frac{a_3}{a_1+a_2+a_3}\right)=\frac{a_2(a_1-a_3)}{a_1+a_2+a_3}>0 \end{align*} where we have used condition 1, which states that $a_1>a_3$.The proofs when $n=4$ and $n=5$ are likewise, only uglier. When $n=5$ the trick is to find common denominators then simply pair each negative term with some larger positive term. It's horrible, but it works. Perhaps a general proof would involve a similar argument but more formalised? If a full proof can't be found then I'd be happy for a proof in the special case where $j=2$ and $j=3$.","I have a finite sequence of positive numbers $(a_i)_1^n$ for which: $a_1>a_n$, $a_j\geq a_{j+1}\geq\cdots\geq a_n$ for some $j\in\{2,\ldots,n-1\}$, $a_1>a_2>\cdots>a_{j-1}$, $a_j\geq a_i$ for all $1\leq i\leq n$. I conjecture that: $$(a_n+a_2+\cdots+a_j)\left(\sum_{i=j+1}^{n-1}{\frac{a_i^2}{(a_1+\cdots+a_i)(a_n+a_2+\cdots + a_i)}}+\frac{a_1+a_n}{a_1+\cdots+a_n}\right) \geq  (a_n+a_1+\cdots+a_j)\left(\sum_{i=j+1}^{n-1}{\frac{a_i^2}{(a_1+\cdots+a_i)(a_n+a_1+\cdots + a_i)}}+\frac{a_n}{a_1+\cdots+a_n}\right).$$ I have an unappealing brute-force proof when $n\in\{3,4,5\}$  but I can't prove it in general. I have tried calculus to no avail, and it doesn't seem like a good fit for any of the standard inequalities.Does this look even remotely similar to anything already done? I appreciate that it's a rather ugly inequality but some suggestions would be greatly appreciated! The proof when $n=3$ is outlined below. By condition 2. we know that $j=2$ so that \begin{align*} (a_3+a_2)\left(\frac{a_1+a_3}{a_1+a_2+a_3}\right)-(a_3+a_1+a_2)\left(\frac{a_3}{a_1+a_2+a_3}\right)=\frac{a_2(a_1-a_3)}{a_1+a_2+a_3}>0 \end{align*} where we have used condition 1, which states that $a_1>a_3$.The proofs when $n=4$ and $n=5$ are likewise, only uglier. When $n=5$ the trick is to find common denominators then simply pair each negative term with some larger positive term. It's horrible, but it works. Perhaps a general proof would involve a similar argument but more formalised? If a full proof can't be found then I'd be happy for a proof in the special case where $j=2$ and $j=3$.",,"['real-analysis', 'sequences-and-series', 'inequality']"
35,Positive distribution $\Lambda$ as positive Radon measure,Positive distribution  as positive Radon measure,\Lambda,"Exercise 4 of Chapter 6 in Rudin's Functional Analysis states that every ""positive"" distribution $\Lambda\in D^{'}(\Omega)$, i.e, $\Lambda\psi\geq 0$ whenever $\psi\in D(\Omega)$, is a positive measure in $\Omega$, where $\Omega\subseteq\mathbb{R}^d$ open and $ D^{'}(\Omega)$ is the space of test functions on it. My question is that does problem directly follow from the Riesz Representation theorem that says every positive linear functional on the space of compactly supported smooth functions on a locally compact Hausdorff space is a positve Radon measure?","Exercise 4 of Chapter 6 in Rudin's Functional Analysis states that every ""positive"" distribution $\Lambda\in D^{'}(\Omega)$, i.e, $\Lambda\psi\geq 0$ whenever $\psi\in D(\Omega)$, is a positive measure in $\Omega$, where $\Omega\subseteq\mathbb{R}^d$ open and $ D^{'}(\Omega)$ is the space of test functions on it. My question is that does problem directly follow from the Riesz Representation theorem that says every positive linear functional on the space of compactly supported smooth functions on a locally compact Hausdorff space is a positve Radon measure?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'distribution-theory']"
36,"Prove $f$ is not differentiable at $(0,0)$",Prove  is not differentiable at,"f (0,0)","For  $$f(x,y)=\begin{cases}                \frac{x|y|}{\sqrt{x^2+y^2}} & \text{ for }(x,y)\neq (0,0)\\                0 &  \text{ for } (x,y)=(0,0)             \end{cases}$$ I'm trying to prove $f$ is not differentiable at $(0,0)$. I showed if $f$ is differentiable at $(0,0),$ then $A=Df_{(0,0)}=0.$ But I don't know how this lead to a contradiction. Anyone has ideas?","For  $$f(x,y)=\begin{cases}                \frac{x|y|}{\sqrt{x^2+y^2}} & \text{ for }(x,y)\neq (0,0)\\                0 &  \text{ for } (x,y)=(0,0)             \end{cases}$$ I'm trying to prove $f$ is not differentiable at $(0,0)$. I showed if $f$ is differentiable at $(0,0),$ then $A=Df_{(0,0)}=0.$ But I don't know how this lead to a contradiction. Anyone has ideas?",,['real-analysis']
37,"Bijection between $[0,1)$ and the space of binary sequences",Bijection between  and the space of binary sequences,"[0,1)","My question deals with the problem of showing that the set $$ \Omega = \{ \omega \colon \omega =(a_1,a_2, \ldots ), a_i =0,1\} $$ has the same cardinality as the interval $[0,1)$. In a textbook I read the following explanation: It is well known that every number $a \in [0,1)$ has a unique binary expansion (containing an infinite number of zeros)   $$ a = \frac{a_1}{2} + \frac{a_2}{2^2} + \ldots  \qquad (a_i=0,1). $$   Hence it is clear that there is a one-to-one correspondence between the points $\omega \in \Omega$ and the points $a$ of the set $[0,1)$, and therefore $\Omega$ has the cardinality of the continuum. Now I have two questions: What is meant with ""containing an infinite number of zeros""? The discussions in Set of points of $[0,1)$ that have a unique binary expansion and Cardinality: Set of all binary sequence equal c show that binary expansion is usually not unique. So I am a bit surprised, as I did not expect such a mistake in this text. Did I understand that incorrectly or is there a natural way to make the binary expansion unique for all numbers in $[0,1)$?","My question deals with the problem of showing that the set $$ \Omega = \{ \omega \colon \omega =(a_1,a_2, \ldots ), a_i =0,1\} $$ has the same cardinality as the interval $[0,1)$. In a textbook I read the following explanation: It is well known that every number $a \in [0,1)$ has a unique binary expansion (containing an infinite number of zeros)   $$ a = \frac{a_1}{2} + \frac{a_2}{2^2} + \ldots  \qquad (a_i=0,1). $$   Hence it is clear that there is a one-to-one correspondence between the points $\omega \in \Omega$ and the points $a$ of the set $[0,1)$, and therefore $\Omega$ has the cardinality of the continuum. Now I have two questions: What is meant with ""containing an infinite number of zeros""? The discussions in Set of points of $[0,1)$ that have a unique binary expansion and Cardinality: Set of all binary sequence equal c show that binary expansion is usually not unique. So I am a bit surprised, as I did not expect such a mistake in this text. Did I understand that incorrectly or is there a natural way to make the binary expansion unique for all numbers in $[0,1)$?",,"['real-analysis', 'irrational-numbers', 'binary']"
38,"Verification : Prove the Fubini-Tonelli theorem when $(X,\mathcal{M},\mu)$ is any measure space and $Y$ is a countable set with the counting measure.",Verification : Prove the Fubini-Tonelli theorem when  is any measure space and  is a countable set with the counting measure.,"(X,\mathcal{M},\mu) Y","The Fubini-Tonelli theorem is valid when $(X,\mathcal{M},\mu)$ is an arbitrary measure space and $Y$ is a countable set $\mathcal{N}=\mathcal{P}(Y)$, and $\nu$ is counting measure on $Y$. This is an exercise from Folland. I tried to prove this as in the proof of the original theorem, but as you can see below, the proof of the Tonelli theorem uses Theorem 2.36, which in turn requires the $\sigma-$finiteness of both measures. So how can I prove this? I would greatly appreciate any help. Edit : I came up with a solution below but need it verified. My Attempt For convenience, let $Y=\mathbb{N}$, and let $Y$ be the union of $Y_n=\{1,\dots, n\}$. Let $f\in L^{+}(X \times Y)$. Let $f_n=f\chi_{X\times Y_n}$. Then clearly $f_n$ is an increasing sequence converging to $f$, so we can use the Monotone Convergence Theorem later. Now we have: $\int f_n d(\mu \times \nu)=\sum_{y=1}^n\int_{X\times y} f d(\mu \times \nu)=\sum_{y=1}^n\int_X f_n (x,y) d\mu(x)$. $g_n(x)=\int f{_n}(x,y)d\nu(y)=\sum_{y=1}^n f_n(x,y)$. $h_n(y)=\int f_n(x,y)d\mu(x)$. $\int g_n(x) d\mu(x)=\int \sum_{y=1}^n f_n(x,y)d\mu=\sum_{y=1}^n \int_X f_n(x,y) d\mu(x)$. $\int h_n(y) d\nu(y)=\sum_{y=1}^n \int_X f_n(x,y) d\mu(x)$. Also, $\lim_n g_n(x)=\lim_n \sum_{y=1}^n f_n(x,y)=\lim_n \sum_{y=1}^n f(x,y)\cdot  \chi_{X\times Y_n}=\sum_{y=1}^\infty f(x,y)$. and $\lim_n h_n(y)=\lim \int_X f_n(x,y) d\mu(x)=\int_X \lim f_n(x,y) d\mu(x)=\int_X f(x,y) d\mu(x)$ by the Monotone Convergence Theorem. So the three integrals are equal, and now we can apply the Monotone Convergence Theorem and the rest of the proof is identical to the one given below.","The Fubini-Tonelli theorem is valid when $(X,\mathcal{M},\mu)$ is an arbitrary measure space and $Y$ is a countable set $\mathcal{N}=\mathcal{P}(Y)$, and $\nu$ is counting measure on $Y$. This is an exercise from Folland. I tried to prove this as in the proof of the original theorem, but as you can see below, the proof of the Tonelli theorem uses Theorem 2.36, which in turn requires the $\sigma-$finiteness of both measures. So how can I prove this? I would greatly appreciate any help. Edit : I came up with a solution below but need it verified. My Attempt For convenience, let $Y=\mathbb{N}$, and let $Y$ be the union of $Y_n=\{1,\dots, n\}$. Let $f\in L^{+}(X \times Y)$. Let $f_n=f\chi_{X\times Y_n}$. Then clearly $f_n$ is an increasing sequence converging to $f$, so we can use the Monotone Convergence Theorem later. Now we have: $\int f_n d(\mu \times \nu)=\sum_{y=1}^n\int_{X\times y} f d(\mu \times \nu)=\sum_{y=1}^n\int_X f_n (x,y) d\mu(x)$. $g_n(x)=\int f{_n}(x,y)d\nu(y)=\sum_{y=1}^n f_n(x,y)$. $h_n(y)=\int f_n(x,y)d\mu(x)$. $\int g_n(x) d\mu(x)=\int \sum_{y=1}^n f_n(x,y)d\mu=\sum_{y=1}^n \int_X f_n(x,y) d\mu(x)$. $\int h_n(y) d\nu(y)=\sum_{y=1}^n \int_X f_n(x,y) d\mu(x)$. Also, $\lim_n g_n(x)=\lim_n \sum_{y=1}^n f_n(x,y)=\lim_n \sum_{y=1}^n f(x,y)\cdot  \chi_{X\times Y_n}=\sum_{y=1}^\infty f(x,y)$. and $\lim_n h_n(y)=\lim \int_X f_n(x,y) d\mu(x)=\int_X \lim f_n(x,y) d\mu(x)=\int_X f(x,y) d\mu(x)$ by the Monotone Convergence Theorem. So the three integrals are equal, and now we can apply the Monotone Convergence Theorem and the rest of the proof is identical to the one given below.",,"['real-analysis', 'analysis', 'measure-theory', 'proof-verification']"
39,Showing continuity of an operator from $L^p$ to $L^q$,Showing continuity of an operator from  to,L^p L^q,Question: Let $1 \leq q \leq p < \infty$ and let $a(x)$ be a measurable function. Assume that $au \in L^q$ for all $u \in L^p$. Show that the map $u \to au$ is continuous. My Approach: I have tried to use closed graph theorem to show continuity of $a$. Let $\{u_n\} \subset L^p$ s.t. $u_n \rightarrow u$ in $L^p$ and $au_n \rightarrow v$ in $L^q$. Then we need to show $v = au$ a.e. $$ |v-au|_q \leq |v-au_n|_q + |a(u_n - u)|_q $$  The first term converges to zero but I got stuck in showing that the second term also converges to zero. Can anyone help?,Question: Let $1 \leq q \leq p < \infty$ and let $a(x)$ be a measurable function. Assume that $au \in L^q$ for all $u \in L^p$. Show that the map $u \to au$ is continuous. My Approach: I have tried to use closed graph theorem to show continuity of $a$. Let $\{u_n\} \subset L^p$ s.t. $u_n \rightarrow u$ in $L^p$ and $au_n \rightarrow v$ in $L^q$. Then we need to show $v = au$ a.e. $$ |v-au|_q \leq |v-au_n|_q + |a(u_n - u)|_q $$  The first term converges to zero but I got stuck in showing that the second term also converges to zero. Can anyone help?,,"['real-analysis', 'functional-analysis']"
40,Can someone help me obtain a contradiction for this statement in 1-D real analysis,Can someone help me obtain a contradiction for this statement in 1-D real analysis,,I am trying to prove the following is not possible: Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a strictly increasing function such that $f(0)=0$. Fix $\alpha >1$ and suppose that for $h>0$ $\lim_{h \to 0} \frac{f(h)}{h} = \infty$ and $\lim_{h \to 0} \frac{f(h)}{f(\alpha h)} =0$ I am quite sure it is not possible for these two limits to be both true and I've been trying to prove it from first definitions but I can't seem to see it.,I am trying to prove the following is not possible: Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a strictly increasing function such that $f(0)=0$. Fix $\alpha >1$ and suppose that for $h>0$ $\lim_{h \to 0} \frac{f(h)}{h} = \infty$ and $\lim_{h \to 0} \frac{f(h)}{f(\alpha h)} =0$ I am quite sure it is not possible for these two limits to be both true and I've been trying to prove it from first definitions but I can't seem to see it.,,"['real-analysis', 'limits']"
41,To evaluate integral using Beta function - Which substitution should i use?,To evaluate integral using Beta function - Which substitution should i use?,,"$$\int_{0}^{1} \frac{x^{m-1}(1-x)^{n-1}}{(a+bx)^{m+n}}dx = \frac{B(m,n)}{(a+b)^ma^n}$$ I have to use some kind of substitution but i do not understand what i use and why ? Thanks","$$\int_{0}^{1} \frac{x^{m-1}(1-x)^{n-1}}{(a+bx)^{m+n}}dx = \frac{B(m,n)}{(a+b)^ma^n}$$ I have to use some kind of substitution but i do not understand what i use and why ? Thanks",,"['real-analysis', 'beta-function']"
42,Definition of Sigma Algebra,Definition of Sigma Algebra,,"I was wondering, why are we not allowed to take arbitrary unions (likewise intersections) in the definition of a sigma algebra?; I am looking for a more or less intuitive reason. It seems to me that most of the motivation in defining sigma algebras lies in Measure Theory. So, does allowing arbitrary unions/intersections somehow screw-up the theory? Also, is there a mathematical structure which is similar to a sigma algebra, for which arbitrary unions/intersections is specified? EDIT: By arbitrary, I mean to include families of subsets which are not necessarily index-able by a countable set.","I was wondering, why are we not allowed to take arbitrary unions (likewise intersections) in the definition of a sigma algebra?; I am looking for a more or less intuitive reason. It seems to me that most of the motivation in defining sigma algebras lies in Measure Theory. So, does allowing arbitrary unions/intersections somehow screw-up the theory? Also, is there a mathematical structure which is similar to a sigma algebra, for which arbitrary unions/intersections is specified? EDIT: By arbitrary, I mean to include families of subsets which are not necessarily index-able by a countable set.",,"['real-analysis', 'general-topology', 'measure-theory']"
43,"Show that $f$ does not change sign on some interval $(\beta,+\infty)$.",Show that  does not change sign on some interval .,"f (\beta,+\infty)","Let $a,b,f$ are continuous functions on some interval $(\alpha,+\infty)$ such that $a,b$ have constant sign on $(\alpha,+\infty)$ and $f$ is differentiable on $(\alpha,+\infty)$. Suppose $f'=af+b$. Show that $f$ does not change sign on some interval $(\beta,+\infty)$. So I consider this as cases: Case I: $a>0,b>0$ So I can rewrite $f=(f'-b)/a$. To the contrary assume that given $\beta \in \mathbb{R}$ there is $c_1,c_2>\beta $ such that $f(c_1)>0$ and $f(c_2)<0$. Then by intermediate value theorem there is $c\in(c_1,c_2)$ such that $f(c)=0$. So,$f'(c)=b(c)>0$. But after that I was stuck. Even a periodic function like sin function does have the property I got. So how do I prove the result?","Let $a,b,f$ are continuous functions on some interval $(\alpha,+\infty)$ such that $a,b$ have constant sign on $(\alpha,+\infty)$ and $f$ is differentiable on $(\alpha,+\infty)$. Suppose $f'=af+b$. Show that $f$ does not change sign on some interval $(\beta,+\infty)$. So I consider this as cases: Case I: $a>0,b>0$ So I can rewrite $f=(f'-b)/a$. To the contrary assume that given $\beta \in \mathbb{R}$ there is $c_1,c_2>\beta $ such that $f(c_1)>0$ and $f(c_2)<0$. Then by intermediate value theorem there is $c\in(c_1,c_2)$ such that $f(c)=0$. So,$f'(c)=b(c)>0$. But after that I was stuck. Even a periodic function like sin function does have the property I got. So how do I prove the result?",,"['calculus', 'real-analysis']"
44,Derivative of intersection volume,Derivative of intersection volume,,"Let $K$ be a convex body in $\mathbb{R}^n$ and set $f:\textrm{SL}(n)\rightarrow \mathbb{R}$ as $f(T)=\textrm{Vol}_n (TB\cap K)$ where $B$ is the Euclidean unit ball. How can we find extreme points of $f$? What I'm looking for is some Taylor expansion of $f$, so I may write for matrices such as $Q=I_n + \epsilon F$ something in the line of $$f(Q)=f(I_n)+\epsilon f'(Q)$$ where $f'$ is a directional derivative of some sort of $f$. I believe this should amount to something like $f'(T)=\textrm{Vol}_{n-1} (\partial TB\cap K)$, but this is pure intuition, I'm not sure how this can be proven.","Let $K$ be a convex body in $\mathbb{R}^n$ and set $f:\textrm{SL}(n)\rightarrow \mathbb{R}$ as $f(T)=\textrm{Vol}_n (TB\cap K)$ where $B$ is the Euclidean unit ball. How can we find extreme points of $f$? What I'm looking for is some Taylor expansion of $f$, so I may write for matrices such as $Q=I_n + \epsilon F$ something in the line of $$f(Q)=f(I_n)+\epsilon f'(Q)$$ where $f'$ is a directional derivative of some sort of $f$. I believe this should amount to something like $f'(T)=\textrm{Vol}_{n-1} (\partial TB\cap K)$, but this is pure intuition, I'm not sure how this can be proven.",,"['real-analysis', 'geometry', 'convex-analysis', 'euclidean-geometry', 'analytic-geometry']"
45,Proving that the Lebesgue integral over a measurable function $f$ is equal to the area/volume below the graph of $f$,Proving that the Lebesgue integral over a measurable function  is equal to the area/volume below the graph of,f f,"Given a Borel set $A \subseteq \mathbb{R}^d, d ≥ 1$ and a measurable function $f: A \to [0, \infty)$, I want to consider the set: $$E = \{(x, y) \in \mathbb{R}^{d+1}: x \in A, 0 ≤ y ≤ f(x)\} \subseteq \mathbb{R}^{d+1}$$ I first want to show that $E$ is a Borel set. Then, I want to prove that $$\lambda_{d+1}(E) = \int_A f(x) d \lambda_d(x)$$ where $\lambda_d$ is the $d$-dimensional Lebesgue measure. I unfortunately wasn't even successful showing that $E$ is a Borel set so far. I first thought that one could write $E$ as the product of two Borel sets ($E = A \times \text{another Borel set}$), but I then realized that it isn't that simple, seeing as the $y$ in a vector $(x, y) \in E$ is dependent on $x$. Maybe one could construct a clever measurable function that sends $E$ onto a measurable set in $\mathbb{R}$ or something like that? I'm not really all that sure though. Once established that $E$ is measurable, wouldn't the second part follow more or less right from Fubini's theorem ? Also, I think the intuition behind this excercice is to acknowledge that, in case $d = 1$, the Lebesgue integral of $f$ over $A$ is nothing but the area inbetween the graph of $f$ and the $x$-axis; for $d = 2$, it's the volume, and so on. I'm not really sure how that helps me (formally) showing it.","Given a Borel set $A \subseteq \mathbb{R}^d, d ≥ 1$ and a measurable function $f: A \to [0, \infty)$, I want to consider the set: $$E = \{(x, y) \in \mathbb{R}^{d+1}: x \in A, 0 ≤ y ≤ f(x)\} \subseteq \mathbb{R}^{d+1}$$ I first want to show that $E$ is a Borel set. Then, I want to prove that $$\lambda_{d+1}(E) = \int_A f(x) d \lambda_d(x)$$ where $\lambda_d$ is the $d$-dimensional Lebesgue measure. I unfortunately wasn't even successful showing that $E$ is a Borel set so far. I first thought that one could write $E$ as the product of two Borel sets ($E = A \times \text{another Borel set}$), but I then realized that it isn't that simple, seeing as the $y$ in a vector $(x, y) \in E$ is dependent on $x$. Maybe one could construct a clever measurable function that sends $E$ onto a measurable set in $\mathbb{R}$ or something like that? I'm not really all that sure though. Once established that $E$ is measurable, wouldn't the second part follow more or less right from Fubini's theorem ? Also, I think the intuition behind this excercice is to acknowledge that, in case $d = 1$, the Lebesgue integral of $f$ over $A$ is nothing but the area inbetween the graph of $f$ and the $x$-axis; for $d = 2$, it's the volume, and so on. I'm not really sure how that helps me (formally) showing it.",,"['real-analysis', 'integration', 'analysis', 'lebesgue-integral']"
46,A locally finite Borel measure on a locally and $\sigma$ compact metric space is a Radon measure,A locally finite Borel measure on a locally and  compact metric space is a Radon measure,\sigma,"Let $X$ be a locally compact metric space which is $\sigma$-compact, and let $\mu$ is an unsigned Borel measure which is finite on every compact set. Show that $\mu$ is a Radon measure. I know that every unsigned Borel measure on a compact metric space which is $\sigma$-compact is a Radon measure. From the assumption, we only need to verify outer regularity and inner regularity. Inner regularity is easier, since we can write $X$ as a countable union and compacts sets $K_n$, and a closed set in each $K_n$ is also a closed set in $X$. I have difficulty verifying outer regularity, open set in $K_n$ may not be open in $X$.","Let $X$ be a locally compact metric space which is $\sigma$-compact, and let $\mu$ is an unsigned Borel measure which is finite on every compact set. Show that $\mu$ is a Radon measure. I know that every unsigned Borel measure on a compact metric space which is $\sigma$-compact is a Radon measure. From the assumption, we only need to verify outer regularity and inner regularity. Inner regularity is easier, since we can write $X$ as a countable union and compacts sets $K_n$, and a closed set in each $K_n$ is also a closed set in $X$. I have difficulty verifying outer regularity, open set in $K_n$ may not be open in $X$.",,['real-analysis']
47,"""Increasingify"" a function / Total variation of a function","""Increasingify"" a function / Total variation of a function",,"Let $f : [a,b] \rightarrow \mathbb{R}$ be a $C^1$ function such that $f$ is monotonic on each $[t_k, t_{k+1}]$, with $a = t_0 < t_1 < ... < t_N = b$. Let g be the increasing-ified version of $f$, i.e. on each interval where $f$ is decreasing we define $g(x) = -f(x) + constant$, such that the function $g$ is continuous. More precisely : if $f$ is increasing or constant on $[t_0, t_{1}]$,  then $g = f$ on this interval if $f$ is decreasing on $[t_0, t_{1}]$,  then $g = -f$ on this interval and thus $g$ is increasing on this interval we do the same on each following interval $[t_k, t_{k+1}]$ : if $f$ is decreasing, we set $g(x) = -f(x) + \alpha_k$, where $\alpha_k$ is chosen such that $g$ is continuous. Example : $f(x) = \sin(x)$ in red, the function $g$ in green: Questions: 1) This concept surely exists somewhere. How is it called? 2) Without loss of generaly, let's assume $a=0$ and $f(0)=0$. It seems that $g$ is : $$ g(x) = \int_0^x | f'(t)| d t.$$ Is that true? 3) It seems that $R(x) = g(x) / x$ looks like a good measure of how much $f(t)$ ""moves"" vertically when $t$ goes from $0$ to $x$, i.e. : if $g(x) / x$ is close to zero, $f$ has very little variation (nearly constant) on $[0, x]$ if $g(x) / x$ is big, $f$ has much variation on $[0, x]$ Does this ratio have a name? Example: with the previous example, $R(10) \simeq 6.54 / 10 = 0.654$ Example: with $f(x) = \sin(x^2)$, we have $R(10) \simeq 63.49 / 10 = 6.349$ Note: now having written this whole thing, I thing this is related to length of arc length / rectification. But still, I'd like to know more about these things.","Let $f : [a,b] \rightarrow \mathbb{R}$ be a $C^1$ function such that $f$ is monotonic on each $[t_k, t_{k+1}]$, with $a = t_0 < t_1 < ... < t_N = b$. Let g be the increasing-ified version of $f$, i.e. on each interval where $f$ is decreasing we define $g(x) = -f(x) + constant$, such that the function $g$ is continuous. More precisely : if $f$ is increasing or constant on $[t_0, t_{1}]$,  then $g = f$ on this interval if $f$ is decreasing on $[t_0, t_{1}]$,  then $g = -f$ on this interval and thus $g$ is increasing on this interval we do the same on each following interval $[t_k, t_{k+1}]$ : if $f$ is decreasing, we set $g(x) = -f(x) + \alpha_k$, where $\alpha_k$ is chosen such that $g$ is continuous. Example : $f(x) = \sin(x)$ in red, the function $g$ in green: Questions: 1) This concept surely exists somewhere. How is it called? 2) Without loss of generaly, let's assume $a=0$ and $f(0)=0$. It seems that $g$ is : $$ g(x) = \int_0^x | f'(t)| d t.$$ Is that true? 3) It seems that $R(x) = g(x) / x$ looks like a good measure of how much $f(t)$ ""moves"" vertically when $t$ goes from $0$ to $x$, i.e. : if $g(x) / x$ is close to zero, $f$ has very little variation (nearly constant) on $[0, x]$ if $g(x) / x$ is big, $f$ has much variation on $[0, x]$ Does this ratio have a name? Example: with the previous example, $R(10) \simeq 6.54 / 10 = 0.654$ Example: with $f(x) = \sin(x^2)$, we have $R(10) \simeq 63.49 / 10 = 6.349$ Note: now having written this whole thing, I thing this is related to length of arc length / rectification. But still, I'd like to know more about these things.",,"['real-analysis', 'integration', 'monotone-functions', 'arc-length']"
48,"How to show that the function $x^\alpha \sin\left(\frac{1}{x}\right)$ ($\alpha > 1$) is of bounded variation on $(0,1]$?",How to show that the function  () is of bounded variation on ?,"x^\alpha \sin\left(\frac{1}{x}\right) \alpha > 1 (0,1]","I am given the function  $$f(x) = \begin{cases} x^\alpha \sin\left(\frac{1}{x}\right) &\text{on }(0,1], \\ 0 & \text{if }x = 0. \end{cases}$$  how do I show that this function is of bounded variation on $[0,1]$ if $\alpha >1$? The variation is given by  $$Vf=\sup\{\Sigma_{n=1}^N|f(x_n)-f(x_{n-1})|: 0=x_0 <x_1,..<x_N=b\}.$$ What I managed to do was to show that it was of bounded variation when $\alpha \ge 2$, because then the derivative is bounded, so the result follow from the mean value theorem. But what about the case $1<\alpha<2$? Any tips?","I am given the function  $$f(x) = \begin{cases} x^\alpha \sin\left(\frac{1}{x}\right) &\text{on }(0,1], \\ 0 & \text{if }x = 0. \end{cases}$$  how do I show that this function is of bounded variation on $[0,1]$ if $\alpha >1$? The variation is given by  $$Vf=\sup\{\Sigma_{n=1}^N|f(x_n)-f(x_{n-1})|: 0=x_0 <x_1,..<x_N=b\}.$$ What I managed to do was to show that it was of bounded variation when $\alpha \ge 2$, because then the derivative is bounded, so the result follow from the mean value theorem. But what about the case $1<\alpha<2$? Any tips?",,['real-analysis']
49,Prove that a sequence is square summable,Prove that a sequence is square summable,,Let $a_n$ be a sequence of real numbers such that $\sum_{n=1}^{\infty}a_nb_n < \infty$ whenever $\sum_{n=1}^{\infty}b_n^{2}< \infty.$ Prove that $\sum_n a_n^2 < \infty.$ Can anyone provide a hint to prove this ? I don't know where to start. I am thinking along the lines of Schwarz inequality.,Let $a_n$ be a sequence of real numbers such that $\sum_{n=1}^{\infty}a_nb_n < \infty$ whenever $\sum_{n=1}^{\infty}b_n^{2}< \infty.$ Prove that $\sum_n a_n^2 < \infty.$ Can anyone provide a hint to prove this ? I don't know where to start. I am thinking along the lines of Schwarz inequality.,,['real-analysis']
50,"Certain set is dense in $l^p$ if and only if $\{x_n : n \in \mathbb{N}\} \notin l^q$, where $1/p + 1/q = 1$","Certain set is dense in  if and only if , where",l^p \{x_n : n \in \mathbb{N}\} \notin l^q 1/p + 1/q = 1,"Assume that $\{x_n : n \in \mathbb{N}\} \subset \mathbb{R}$ is such that $x_n \neq 0$ for some $n$. Let $p \in (1, \infty)$ and$$G := \left\{\{y_n : n \in \mathbb{N}\} \in l^p : \lim_{N \to \infty} \sum_{n=1}^N y_n x_n = 0\right\}.$$Do we have that $G$ is dense in $l^p$ if and only if $\{x_n : n \in \mathbb{N}\} \notin l^q$ where $1/p + 1/q = 1$?","Assume that $\{x_n : n \in \mathbb{N}\} \subset \mathbb{R}$ is such that $x_n \neq 0$ for some $n$. Let $p \in (1, \infty)$ and$$G := \left\{\{y_n : n \in \mathbb{N}\} \in l^p : \lim_{N \to \infty} \sum_{n=1}^N y_n x_n = 0\right\}.$$Do we have that $G$ is dense in $l^p$ if and only if $\{x_n : n \in \mathbb{N}\} \notin l^q$ where $1/p + 1/q = 1$?",,"['calculus', 'real-analysis']"
51,"Does there exist $f \in L^1(\mathbb{R})$ where $\lim_{r \to 0} {1\over{r}} \int_{x-r}^{x+r} f(y)\,dy = \infty$?",Does there exist  where ?,"f \in L^1(\mathbb{R}) \lim_{r \to 0} {1\over{r}} \int_{x-r}^{x+r} f(y)\,dy = \infty","If $E \subset \mathbb{R}$ has measure $0$, does there exist $f \in L^1(\mathbb{R})$ such that, for every $x \in E$, $$\lim_{r \to 0} {1\over{r}} \int_{x-r}^{x+r} f(y)\,dy = \infty?$$ What if $E$ has positive measure?","If $E \subset \mathbb{R}$ has measure $0$, does there exist $f \in L^1(\mathbb{R})$ such that, for every $x \in E$, $$\lim_{r \to 0} {1\over{r}} \int_{x-r}^{x+r} f(y)\,dy = \infty?$$ What if $E$ has positive measure?",,"['real-analysis', 'integration', 'functional-analysis', 'measure-theory', 'lp-spaces']"
52,Prove $|a|=\sqrt{a^2}$,Prove,|a|=\sqrt{a^2},"I am having difficulty showing that the last case holds and I also just wanted to make sure I am proving this correctly. Case (i): If $a=0$ then we have $|a|=0=\sqrt{0^2}$ so trivially this is true. Case (ii): If $a>0$ then we have $|a|=a=\sqrt{a^2}$ Case (iii): This is where I get stuck since if $a<0$ then we have that $|a|=-a$, so if I start from $\sqrt{a^2}=\sqrt{(-a)^2}$ I'm not really sure what to do since I need this being equal to $|a|=-a$, I would like to say $\sqrt{(-a)^2}=-a$ but this seems to be a problem since it could also be written as $\sqrt{(-a)^2}=\sqrt{a^2}=a$","I am having difficulty showing that the last case holds and I also just wanted to make sure I am proving this correctly. Case (i): If $a=0$ then we have $|a|=0=\sqrt{0^2}$ so trivially this is true. Case (ii): If $a>0$ then we have $|a|=a=\sqrt{a^2}$ Case (iii): This is where I get stuck since if $a<0$ then we have that $|a|=-a$, so if I start from $\sqrt{a^2}=\sqrt{(-a)^2}$ I'm not really sure what to do since I need this being equal to $|a|=-a$, I would like to say $\sqrt{(-a)^2}=-a$ but this seems to be a problem since it could also be written as $\sqrt{(-a)^2}=\sqrt{a^2}=a$",,['real-analysis']
53,Determine if $\sum_{n=2}^{\infty} \frac{(-1)^n}{n+(-1)^n}$ converges or diverges,Determine if  converges or diverges,\sum_{n=2}^{\infty} \frac{(-1)^n}{n+(-1)^n},I'm having a lot of trouble figuring this one out. Determine if  $\sum_{i=2}^{\infty} \frac{(-1)^n}{n+(-1)^n}$ converges or diverges Both ratio and root test are inconclusive and I'm at a loss. Can anyone help me?,I'm having a lot of trouble figuring this one out. Determine if  $\sum_{i=2}^{\infty} \frac{(-1)^n}{n+(-1)^n}$ converges or diverges Both ratio and root test are inconclusive and I'm at a loss. Can anyone help me?,,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
54,Suppose $\lim \sup_{n \to \infty}a_n \le \rho$. Show $\lim \sup_{n \to \infty} a_n^{{(n-m)}/{n}} \le \rho$.,Suppose . Show .,\lim \sup_{n \to \infty}a_n \le \rho \lim \sup_{n \to \infty} a_n^{{(n-m)}/{n}} \le \rho,"Suppose that $\{a_n\}$ is a sequence of positive numbers with $\lim \sup_{n \to \infty} a_n\le \rho$. Show $\lim \sup_{n \to \infty} a_n^{{(n-m)}/{n}} \le \rho$, where $m \in \Bbb N$. I was thinking about doing something along the lines of $\lim \sup_{n \to \infty} a_n^{{(n-m)}/{n}}=\lim \sup_{n \to \infty}a_n (a_n^{-m})^{\frac {1}n}$ Then proving that $(a_n^{-m})^{\frac {1}n}$ goes to $1$ as you take the limit supremum. Please let me know if I am completely off base with this logic. EDIT: $$(a_n^{-m})^{\frac {1}n} = \frac 1 {\underbrace{a_n^{1/n} \cdots a_n^{1/n}}_{m \text{ times}}}$$ Because $a_n^{1/n}$ converges to 1, $\lim_{n \to \infty} a_n^{1/n} =  \lim \sup_{n \to \infty} a_n^{1/n}=1$. Thus, $\lim \sup_{n \to \infty} a_n^{{(n-m)}/{n}}=\lim \sup_{n \to \infty} a_n\le \rho $. Is this correct? If not, where did I mess up? If so, this is not the most elegant solution, is there a better one?","Suppose that $\{a_n\}$ is a sequence of positive numbers with $\lim \sup_{n \to \infty} a_n\le \rho$. Show $\lim \sup_{n \to \infty} a_n^{{(n-m)}/{n}} \le \rho$, where $m \in \Bbb N$. I was thinking about doing something along the lines of $\lim \sup_{n \to \infty} a_n^{{(n-m)}/{n}}=\lim \sup_{n \to \infty}a_n (a_n^{-m})^{\frac {1}n}$ Then proving that $(a_n^{-m})^{\frac {1}n}$ goes to $1$ as you take the limit supremum. Please let me know if I am completely off base with this logic. EDIT: $$(a_n^{-m})^{\frac {1}n} = \frac 1 {\underbrace{a_n^{1/n} \cdots a_n^{1/n}}_{m \text{ times}}}$$ Because $a_n^{1/n}$ converges to 1, $\lim_{n \to \infty} a_n^{1/n} =  \lim \sup_{n \to \infty} a_n^{1/n}=1$. Thus, $\lim \sup_{n \to \infty} a_n^{{(n-m)}/{n}}=\lim \sup_{n \to \infty} a_n\le \rho $. Is this correct? If not, where did I mess up? If so, this is not the most elegant solution, is there a better one?",,['real-analysis']
55,"Given $v_i∈B^n$, bounding $\sum b_iv_i$ for $b_i= \pm 1$","Given , bounding  for",v_i∈B^n \sum b_iv_i b_i= \pm 1,"Let $(v_i)_{i∈ℕ}$ be a vector sequence. Say $(v_i)$ is boundable (under $M$) if there exists a sequence $(b_i)_{i∈ℕ}$ taking values in $\{-1,1\}$ such that $(|\sum^N_i b_iv_i|)_{N∈ℕ}$ is bounded (under $M$). If $(v_i)$ takes values in the unit ball $B^n⊆ℝ^n$, does it follow $(v_i)$ is boundable? With the definition formulated analogously for finite sequences, I see I can utilize the (weak) Konig's lemma to show that a vector sequence is boundable under $M$ if and only if all its finite starting sequences are boundable under $M$. As such, it follows that if every such vector sequence is boundable, then there is a minimal such $α_n$ such that every vector sequence is boundable under $α_n$ (for example, $α_1=1$). Based on some randomized computations of mine it appears the hypothesis holds for $n=2$ and $1.6≤α_2≤1.7$, however I'm not sure how to continue. Edit: It appears actually that should the hypothesis be true, then $α_2 ≥ \sqrt{3}$. This diagram indicates how to construct a finite sequence unboundable under $r$ when $r < \sqrt{3}$: Start with the resultant vector $w_1$ (initial $v_1=(1,0)$) of magnitude $1$ or greater; choose the next vector $w_2$ such that $|w_1+w_2| > r$ yet the angle between $w_1,w_2$ is less than $2π/3$. Then $|w_1-w_2| > |w_1|$ and we can repeat the process with resultant vector $w_1-w_2$ to at least constantly greater effect each time. (Note the mirror situation of $b_1=-1$ is taken care of by symmetry)","Let $(v_i)_{i∈ℕ}$ be a vector sequence. Say $(v_i)$ is boundable (under $M$) if there exists a sequence $(b_i)_{i∈ℕ}$ taking values in $\{-1,1\}$ such that $(|\sum^N_i b_iv_i|)_{N∈ℕ}$ is bounded (under $M$). If $(v_i)$ takes values in the unit ball $B^n⊆ℝ^n$, does it follow $(v_i)$ is boundable? With the definition formulated analogously for finite sequences, I see I can utilize the (weak) Konig's lemma to show that a vector sequence is boundable under $M$ if and only if all its finite starting sequences are boundable under $M$. As such, it follows that if every such vector sequence is boundable, then there is a minimal such $α_n$ such that every vector sequence is boundable under $α_n$ (for example, $α_1=1$). Based on some randomized computations of mine it appears the hypothesis holds for $n=2$ and $1.6≤α_2≤1.7$, however I'm not sure how to continue. Edit: It appears actually that should the hypothesis be true, then $α_2 ≥ \sqrt{3}$. This diagram indicates how to construct a finite sequence unboundable under $r$ when $r < \sqrt{3}$: Start with the resultant vector $w_1$ (initial $v_1=(1,0)$) of magnitude $1$ or greater; choose the next vector $w_2$ such that $|w_1+w_2| > r$ yet the angle between $w_1,w_2$ is less than $2π/3$. Then $|w_1-w_2| > |w_1|$ and we can repeat the process with resultant vector $w_1-w_2$ to at least constantly greater effect each time. (Note the mirror situation of $b_1=-1$ is taken care of by symmetry)",,"['real-analysis', 'inequality', 'vectors']"
56,A question about Measurable function,A question about Measurable function,,Let $f$ be a real-valued Lebesgue measurable function on $\mathbb{R}$. Prove that there exist Borel measurable functions $g$ and $h$ such that $g(x)=h(x)$ almost everywhere and $g(x)\le f(x) \le h(x)$ for every $x \in \mathbb R$. I know that $f$ is measurable since there exists a sequence of simple function that converges to $f$. I have no further idea how to tackle this problem.,Let $f$ be a real-valued Lebesgue measurable function on $\mathbb{R}$. Prove that there exist Borel measurable functions $g$ and $h$ such that $g(x)=h(x)$ almost everywhere and $g(x)\le f(x) \le h(x)$ for every $x \in \mathbb R$. I know that $f$ is measurable since there exists a sequence of simple function that converges to $f$. I have no further idea how to tackle this problem.,,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
57,Too simple proof for convergence of $\sum_n a_n b_n$?,Too simple proof for convergence of ?,\sum_n a_n b_n,"My question relates to Chapter 3, Exercise 8 in ""Baby Rudin"". It states: If $\sum_n a_n$ converges , and if $\{b_n\}$ is monotonic and bounded , prove that $\sum_n a_n b_n$ converges . My attempt would have been: Since $\{b_n\}$ is monotonic and bounded, $\{b_n\}$ converges and it exists $\inf \{b_n\}$ as well as $\sup \{b_n\}$ . But then we have $$ \left| \sum_n a_n \inf \{b_n\} \right| \leq \left| \sum_n a_n b_n \right| \leq  \left| \sum_n a_n \sup \{b_n\} \right| \leq \max \left( \left| \sup \{b_n\} \right|, \left| \inf \{b_n \} \right| \right)\varepsilon \leq \tilde{\varepsilon} $$ since $\sum_n a_n$ converges and $\max \left( \left| \sup \{b_n\} \right|, \left| \inf \{b_n \} \right| \right)$ is a finite number. This would imply, by the comparison test, that $\sum_n a_n b_n$ converges as well. $\quad \Box$ But as there was a way longer, more rigorous proof chosen in this solution manual , I'm a bit suspicious that my proof is not complete. Am I missing something? EDIT: Thanks everyone! @hermes: The last part of your proof gave me the following idea. As $\lim_{n \to \infty} b_n = c$ and $\{b_n\}$ is monotonic, couldn't we just set $b_n = c - c_n$ with a monotonically decreasing sequence $\{c_n\}$ which has $\lim_{n \to \infty} c_n = 0$ . Then we have $$\sum_n a_n b_n = \underbrace{c \sum_n a_n}_{\text{converges by assumption}} - \underbrace{\sum_n a_n c_n}_{\text{converges by Theorem 3.42}} \leq \varepsilon_1 - \varepsilon_2 = \varepsilon $$ since Theorem 3.42 Suppose the partial sums of $\sum_n a_n$ form a bounded space $\quad \checkmark$ $c_0 \geq c_1 \geq c_2 \geq \dots \quad \checkmark$ $\lim_{n \to \infty} c_n = 0 \quad \checkmark$ Then $\sum_n a_n c_n$ converges. Thus $\sum_n a_n b_n$ converges as well. $\quad \Box$ Now that should hold, I think. So I wouldn't need to go through all the estimates.","My question relates to Chapter 3, Exercise 8 in ""Baby Rudin"". It states: If converges , and if is monotonic and bounded , prove that converges . My attempt would have been: Since is monotonic and bounded, converges and it exists as well as . But then we have since converges and is a finite number. This would imply, by the comparison test, that converges as well. But as there was a way longer, more rigorous proof chosen in this solution manual , I'm a bit suspicious that my proof is not complete. Am I missing something? EDIT: Thanks everyone! @hermes: The last part of your proof gave me the following idea. As and is monotonic, couldn't we just set with a monotonically decreasing sequence which has . Then we have since Theorem 3.42 Suppose the partial sums of form a bounded space Then converges. Thus converges as well. Now that should hold, I think. So I wouldn't need to go through all the estimates.","\sum_n a_n \{b_n\} \sum_n a_n b_n \{b_n\} \{b_n\} \inf \{b_n\} \sup \{b_n\}  \left| \sum_n a_n \inf \{b_n\} \right| \leq \left| \sum_n a_n b_n \right| \leq  \left| \sum_n a_n \sup \{b_n\} \right| \leq \max \left( \left| \sup \{b_n\} \right|, \left| \inf \{b_n \} \right| \right)\varepsilon \leq \tilde{\varepsilon}  \sum_n a_n \max \left( \left| \sup \{b_n\} \right|, \left| \inf \{b_n \} \right| \right) \sum_n a_n b_n \quad \Box \lim_{n \to \infty} b_n = c \{b_n\} b_n = c - c_n \{c_n\} \lim_{n \to \infty} c_n = 0 \sum_n a_n b_n = \underbrace{c \sum_n a_n}_{\text{converges by assumption}} - \underbrace{\sum_n a_n c_n}_{\text{converges by Theorem 3.42}} \leq \varepsilon_1 - \varepsilon_2 = \varepsilon  \sum_n a_n \quad \checkmark c_0 \geq c_1 \geq c_2 \geq \dots \quad \checkmark \lim_{n \to \infty} c_n = 0 \quad \checkmark \sum_n a_n c_n \sum_n a_n b_n \quad \Box","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
58,Non-constant $L^1$ function has a non-zero integral over some interval,Non-constant  function has a non-zero integral over some interval,L^1,"Prove that if $f\in L^1([0,1],\lambda)$ is not constant almost   everywhere then there exists an interval so that   $\int_I\!f\,\mathrm{d}\lambda\neq 0$. Here $\lambda$ is the Lebesgue   measure. Since this is obviously true for continuous functions, I've been trying to use the fact that continuous functions with compact support are dense in $L^1$, but I'm not sure how to set it up.","Prove that if $f\in L^1([0,1],\lambda)$ is not constant almost   everywhere then there exists an interval so that   $\int_I\!f\,\mathrm{d}\lambda\neq 0$. Here $\lambda$ is the Lebesgue   measure. Since this is obviously true for continuous functions, I've been trying to use the fact that continuous functions with compact support are dense in $L^1$, but I'm not sure how to set it up.",,['real-analysis']
59,"Let $m$ be Lebesgue measure and $a \in R$. Suppose that $f : R \to R$ is integrable, and $\int_a^xf(y) \, dy = 0$ for all $x$. Then $f = 0$ a.e.","Let  be Lebesgue measure and . Suppose that  is integrable, and  for all . Then  a.e.","m a \in R f : R \to R \int_a^xf(y) \, dy = 0 x f = 0","This is a corollary to a proof in Bass, but I don't understand why it follows from the proof he gives. I follow everything up until the last statement. Why is it that proving that the integral is $0$ for all Borel measurable sets guarantees it is true for all Lebesgue measurable sets? Am I missing something vital about the relationship between Lebesgue measurable sets and Borel measurable sets? Statement: Let $m$ be Lebesgue measure and $a \in\mathbb R$ . Suppose that $f : \mathbb R \to\mathbb R$ is integrable, and $\int_a^xf(y) \, dy = 0$ for all $x$ . Then $f = 0$ a.e. Proof: We prove the conclusion first for intervals, then for unions of intervals, then for open sets, then for Borel measurable sets. For any interval $(c,d)$ , we have $$\int_c^df = \int_a^d f - \int^c_a f = 0.$$ By linearity, if $G$ is the finite union of disjoint open intervals, then $\int_G f = 0$ . Now, if $G$ is open, then $G$ is the countable union of disjoint, open intervals, $G = \bigcup^n_{i=1}I_i$ . We have $$\int_G f = \int \lim_{n\to\infty} f \cdot \chi_{\bigcup^n_{i=1} I_i}$$ and since $f_n$ is integrable for all $n$ , and $|f_n| \le f$ for all $n$ , by the dominated convergence theorem, $$\int_G f = \lim_{n\to\infty} \int_{\bigcup^n_{i=1}I_i} f =0.$$ Now, if $G_n$ is a decreasing sequence of open sets converging to $H$ , since $|f \cdot \chi_{G_n}| \le f \cdot \chi_{G_1}$ for all $n$ , we again have by the DCT that $\int_H f = 0 = \lim_{n\to\infty} \int_{G_n}f = 0$ . Finally, if $G$ is a Borel measurable set, then we know that there exists a sequence $G_n$ of decreasing, open sets, that converge to some set $H$ , where $H \setminus G$ is a null set. Thus, $$\int_G f = \int_H f = 0.$$ We have found that for every Borel measurable set $G$ , $\int_G f =0$ . Since $f$ is real valued and measurable, $f = 0$ a.e.","This is a corollary to a proof in Bass, but I don't understand why it follows from the proof he gives. I follow everything up until the last statement. Why is it that proving that the integral is for all Borel measurable sets guarantees it is true for all Lebesgue measurable sets? Am I missing something vital about the relationship between Lebesgue measurable sets and Borel measurable sets? Statement: Let be Lebesgue measure and . Suppose that is integrable, and for all . Then a.e. Proof: We prove the conclusion first for intervals, then for unions of intervals, then for open sets, then for Borel measurable sets. For any interval , we have By linearity, if is the finite union of disjoint open intervals, then . Now, if is open, then is the countable union of disjoint, open intervals, . We have and since is integrable for all , and for all , by the dominated convergence theorem, Now, if is a decreasing sequence of open sets converging to , since for all , we again have by the DCT that . Finally, if is a Borel measurable set, then we know that there exists a sequence of decreasing, open sets, that converge to some set , where is a null set. Thus, We have found that for every Borel measurable set , . Since is real valued and measurable, a.e.","0 m a \in\mathbb R f : \mathbb R \to\mathbb R \int_a^xf(y) \, dy = 0 x f = 0 (c,d) \int_c^df = \int_a^d f - \int^c_a f = 0. G \int_G f = 0 G G G = \bigcup^n_{i=1}I_i \int_G f = \int \lim_{n\to\infty} f \cdot \chi_{\bigcup^n_{i=1} I_i} f_n n |f_n| \le f n \int_G f = \lim_{n\to\infty} \int_{\bigcup^n_{i=1}I_i} f =0. G_n H |f \cdot \chi_{G_n}| \le f \cdot \chi_{G_1} n \int_H f = 0 = \lim_{n\to\infty} \int_{G_n}f = 0 G G_n H H \setminus G \int_G f = \int_H f = 0. G \int_G f =0 f f = 0","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
60,Proving $\pi \coth \pi a= \frac{1}{a}+ \sum_{n=1}^{\infty}\frac{2a}{n^2+a^2}$ using the Fourier series for $\cosh ax$,Proving  using the Fourier series for,\pi \coth \pi a= \frac{1}{a}+ \sum_{n=1}^{\infty}\frac{2a}{n^2+a^2} \cosh ax,"The exercise wants me to prove the identity $$\pi \coth \pi a= \frac{1}{a}+ \sum_{n=1}^{\infty}\frac{2a}{n^2+a^2}$$ using the Fourier series of $\cosh ax, \; x \in [-\pi, \pi], \; a \neq 0$. Evaluating the coefficients ($b_n$ is actually zero as $\cosh ax$ is even) we have that: $\displaystyle {\color{gray} \blacksquare} \;\; a_0 = \frac{1}{\pi}\int_{-\pi}^{\pi}\cosh ax \, {\rm d}x= \frac{2\sinh \pi a}{\pi a}$ $\require{cancel}\displaystyle  {\color{gray} \blacksquare} \;\; a_n = \frac{1}{\pi}\int_{-\pi}^{\pi}\cosh ax \cos n x\, {\rm d}x= \frac{2[a \sinh \pi a \cos \pi n+ \cancelto{0}{n \cosh \pi a \sin \pi n}]}{\pi (a^2+n^2)}$ So far so good except one little problem. I get that $\cos \pi n$ in the nominator which is $(-1)^n$ so I get the alternating series not the wanted one. That is , this way I evaluated the series: $$\cosh ax = \frac{\sinh \pi a}{\pi a}+ \frac{2 a\sinh \pi a}{\pi}\sum_{n=1}^{\infty}\frac{ (-1)^n}{n^2 +a^2} \implies \\ \implies \pi\coth \pi a = \frac{1}{a}+ \sum_{n=1}^{\infty}\frac{2(-1)^n}{n^2+a^2}$$ and not what I want. Calculations of the coefficients were done by Wolfram because they were too tedious to be done by hand. However, I know that using contour integration using the kernel $\pi \cot \pi z$ that this series indeed evaluates to the LHS. What is wrong here?","The exercise wants me to prove the identity $$\pi \coth \pi a= \frac{1}{a}+ \sum_{n=1}^{\infty}\frac{2a}{n^2+a^2}$$ using the Fourier series of $\cosh ax, \; x \in [-\pi, \pi], \; a \neq 0$. Evaluating the coefficients ($b_n$ is actually zero as $\cosh ax$ is even) we have that: $\displaystyle {\color{gray} \blacksquare} \;\; a_0 = \frac{1}{\pi}\int_{-\pi}^{\pi}\cosh ax \, {\rm d}x= \frac{2\sinh \pi a}{\pi a}$ $\require{cancel}\displaystyle  {\color{gray} \blacksquare} \;\; a_n = \frac{1}{\pi}\int_{-\pi}^{\pi}\cosh ax \cos n x\, {\rm d}x= \frac{2[a \sinh \pi a \cos \pi n+ \cancelto{0}{n \cosh \pi a \sin \pi n}]}{\pi (a^2+n^2)}$ So far so good except one little problem. I get that $\cos \pi n$ in the nominator which is $(-1)^n$ so I get the alternating series not the wanted one. That is , this way I evaluated the series: $$\cosh ax = \frac{\sinh \pi a}{\pi a}+ \frac{2 a\sinh \pi a}{\pi}\sum_{n=1}^{\infty}\frac{ (-1)^n}{n^2 +a^2} \implies \\ \implies \pi\coth \pi a = \frac{1}{a}+ \sum_{n=1}^{\infty}\frac{2(-1)^n}{n^2+a^2}$$ and not what I want. Calculations of the coefficients were done by Wolfram because they were too tedious to be done by hand. However, I know that using contour integration using the kernel $\pi \cot \pi z$ that this series indeed evaluates to the LHS. What is wrong here?",,"['real-analysis', 'fourier-series']"
61,"Attempt to prove: $f:[0,1]\to\mathbb R$, for every $c \in [0,1]$ the limit $ \lim_{x\to c} f(x)$ exists, so $f$ is bounded.","Attempt to prove: , for every  the limit  exists, so  is bounded.","f:[0,1]\to\mathbb R c \in [0,1]  \lim_{x\to c} f(x) f","Let $f:[0,1]\to\mathbb R$ such that for every $c \in [0,1]$ the limit $ \lim_{x\to c} f(x)$ exists (and finite). Prove that $f$ is bounded. My attempt: For every $c \in [0,1]$ there exists $\delta_c >0$ such that for every $x\in I_c=(c-\delta_c, c+\delta_c)$, $L_c-1<f(x)<L_c+1$. Mark $K_c=\max \{|L_c+1|, |L_c-1|\}$. The open intervals $(c-\delta_c, c+\delta_c)$ cover $[0,1]$, and by Heine—Borel, finitely many of them, $n$, are enough to cover it. Take $M=\max \{K_1, K_2, \dots, K_n\}$, so for every $x \in [0,1]$, $|f(x)|<M.$ Is this prove is correct? Is ther something I missed, or should add?","Let $f:[0,1]\to\mathbb R$ such that for every $c \in [0,1]$ the limit $ \lim_{x\to c} f(x)$ exists (and finite). Prove that $f$ is bounded. My attempt: For every $c \in [0,1]$ there exists $\delta_c >0$ such that for every $x\in I_c=(c-\delta_c, c+\delta_c)$, $L_c-1<f(x)<L_c+1$. Mark $K_c=\max \{|L_c+1|, |L_c-1|\}$. The open intervals $(c-\delta_c, c+\delta_c)$ cover $[0,1]$, and by Heine—Borel, finitely many of them, $n$, are enough to cover it. Take $M=\max \{K_1, K_2, \dots, K_n\}$, so for every $x \in [0,1]$, $|f(x)|<M.$ Is this prove is correct? Is ther something I missed, or should add?",,"['real-analysis', 'limits', 'functions']"
62,A product version of Riemann integral,A product version of Riemann integral,,"Motivated by Riemann sum in Riemann integral and motivated by relations between infinite series and infinite products we ask: Assume that $f:[0, 1]\to \mathbb{R}$ is  a positive function. Assume that there is  a real number $A$ with the following property: For every $\epsilon>0$  there is  a  partition $P=\{0=x_{0},x_{1},\ldots,x_{n-1},x_{n}=1\}$  of $[0,1]$  such that for every $t_{i}\in [x_{i-1}, \;\;x_{i}]$ we have $$| \prod_{i=1}^{n} (1+f(t_{i})\Delta x_{i})-A| < \epsilon $$ We put $A=\prod_{[0,\;1]} f $. Is there any relation between this concept  and Riemann integrability? Is this  an appropriate generalization of Riemann integral?. Is the collection of all function $f$ for which this quantity exist both for $f^{+}$  and  $f^{-}$, an algebra of  functions? If yes, is it ismorphic to the algebra of Riemann integrable functions? Finally can one express $\prod (f+g)$ and $\prod fg$  in term of $\prod f$   and  $\prod g$ ?","Motivated by Riemann sum in Riemann integral and motivated by relations between infinite series and infinite products we ask: Assume that $f:[0, 1]\to \mathbb{R}$ is  a positive function. Assume that there is  a real number $A$ with the following property: For every $\epsilon>0$  there is  a  partition $P=\{0=x_{0},x_{1},\ldots,x_{n-1},x_{n}=1\}$  of $[0,1]$  such that for every $t_{i}\in [x_{i-1}, \;\;x_{i}]$ we have $$| \prod_{i=1}^{n} (1+f(t_{i})\Delta x_{i})-A| < \epsilon $$ We put $A=\prod_{[0,\;1]} f $. Is there any relation between this concept  and Riemann integrability? Is this  an appropriate generalization of Riemann integral?. Is the collection of all function $f$ for which this quantity exist both for $f^{+}$  and  $f^{-}$, an algebra of  functions? If yes, is it ismorphic to the algebra of Riemann integrable functions? Finally can one express $\prod (f+g)$ and $\prod fg$  in term of $\prod f$   and  $\prod g$ ?",,"['real-analysis', 'integration', 'reference-request', 'infinite-product']"
63,How to modify a $H^1$ weak convergence sequence so that I have the $L^2$ equi-integrability of gradient?,How to modify a  weak convergence sequence so that I have the  equi-integrability of gradient?,H^1 L^2,"Assume $u_n\to u$ weakly in $H^1(\Omega)$ where $\Omega\subset \mathbb R^N$ is open bounded Lipschitz boundary. My goal is to find a new sequence $\bar u_n$ and a new function $\bar u$ such that $\int_\Omega|\nabla \bar u_n|^2dx\leq \int_\Omega|\nabla u_n|^2dx$ and $\int_\Omega|\nabla \bar u|^2dx\leq \int_\Omega|\nabla u|^2dx$ $\bar{u}_n\to \bar{u}$ weakly in $H^1$. $\nabla \bar u_n$ is $L^2$-equi-integrable, i.e., for any $\epsilon>0$ we have there exists $\delta>0$ such that for all set $T\subset \Omega$ with $\mathcal L^N(T)<\delta$ we have $$ \sup_{n\in\mathbb N}\int_{T} |\nabla \bar u_n|^2dx<\epsilon. \tag 1 $$ My idea is to define $$ \bar u_n:=\min_{v\in\mathcal A(u_n)}\left\{\int_\Omega|\nabla v^2|\,dx\right\},\text{ and }\bar u:=\min_{v\in\mathcal A(u)}\left\{\int_\Omega|\nabla v^2|\,dx\right\}, $$ where  $$   \mathcal A(u_n):=\left\{v\in H^1(\Omega), T[v]=T[u_n]\right\}, $$ and $T[\cdot]$ denotes the usual trace operator. The property $1$ is obviously true. The prove of property $2$ I put it at the end of this post. Please help me to check whether it is correct. However, I can not prove property $3$. The best I can do is assuming $(1)$ does not hold, i.e., there exists a sequence of set $T_n\subset \Omega$ such that $\lim_{n\to 0}\mathcal L^N(T_n)=0$ and  $$ \lim_{n\to\infty} \int_{T_n}|\nabla \bar u_n|^2dx\geq \epsilon>0 $$ and hope to have a contradiction. We can compute $$ \liminf_{n\to\infty}\int_\Omega|\nabla \bar u_n|^2dx\geq \liminf_{n\to\infty}\int_{\Omega\setminus T_n}|\nabla \bar u_n|^2dx+\liminf_{n\to\infty}\int_{T_n}|\nabla \bar u_n|^2dx\geq \int_\Omega|\nabla \bar u\,|^2dx+\epsilon  $$ but I can not get any contradiction from here. I feel I need to use the minimality of $\nabla\bar u_n$ but I don't see how... Any help of new idea of how to construct $\bar u_n$ is really welcome! Below is how to proof property $2$. Now let me prove property $2$. Clearly $\bar u_n$ is bounded in $H^1$ and hence, up to a subsequence, $\bar u_n\to u_0$ weakly in $H^1$. I only need to prove that $u_0=\bar u$. To do so, I only need to prove that $u_0$ is the weak solution of PDE $$ \begin{cases} -\Delta v=0, & x\in\Omega\\ v=u, & x\in\partial\Omega \end{cases} $$ By weak convergence in $H^1$, we have  $$ \int_\Omega \nabla u_0\nabla \phi=0 $$ for all $\phi\in H_0^1(\Omega)$. I only need to prove that $u_0\in \mathcal A(u)$ then I would be done. To do so, I need to prove $u_0-u\in H_0^1(\Omega)$. I will claim $$ \left|\int_\Omega (u_0-u)(x) \partial_i\varphi(x)dx\right|\leq C\|\varphi\|_{L^2(\Omega)} $$ for all $\phi\in C_c^\infty(\mathbb R^N)$. We observe that  \begin{multline} \left|\int_\Omega (u_0-u)(x) \partial_i\varphi(x)dx\right|=\\ \lim_{n\to\infty}\left|\int_\Omega (\bar u_n-u_n)(x) \partial_i\varphi(x)dx\right|=\lim_{n\to\infty}\left|\int_\Omega \partial x_i(\bar u_n-u_n)(x) \varphi(x)dx\right|\\ \leq \lim_{n\to\infty}\|\nabla (\bar u_n-u_n)\|_{L^2}\|v\|_{L^2}\leq C\|v\|_{L^2} \end{multline} as desired, where the 3rd inequality used the fact that $T[\bar u_n-u_n]\equiv 0$. Hence, by the uniqueness of solution, we have $u_0=\bar u$, and hence property $2$ is true. PS: I also post this problem in Mathoverflow here because this post is just an update of my yesterday's post which exist on both set... Sorry for double posting here! I will avoid this situation for my next post.","Assume $u_n\to u$ weakly in $H^1(\Omega)$ where $\Omega\subset \mathbb R^N$ is open bounded Lipschitz boundary. My goal is to find a new sequence $\bar u_n$ and a new function $\bar u$ such that $\int_\Omega|\nabla \bar u_n|^2dx\leq \int_\Omega|\nabla u_n|^2dx$ and $\int_\Omega|\nabla \bar u|^2dx\leq \int_\Omega|\nabla u|^2dx$ $\bar{u}_n\to \bar{u}$ weakly in $H^1$. $\nabla \bar u_n$ is $L^2$-equi-integrable, i.e., for any $\epsilon>0$ we have there exists $\delta>0$ such that for all set $T\subset \Omega$ with $\mathcal L^N(T)<\delta$ we have $$ \sup_{n\in\mathbb N}\int_{T} |\nabla \bar u_n|^2dx<\epsilon. \tag 1 $$ My idea is to define $$ \bar u_n:=\min_{v\in\mathcal A(u_n)}\left\{\int_\Omega|\nabla v^2|\,dx\right\},\text{ and }\bar u:=\min_{v\in\mathcal A(u)}\left\{\int_\Omega|\nabla v^2|\,dx\right\}, $$ where  $$   \mathcal A(u_n):=\left\{v\in H^1(\Omega), T[v]=T[u_n]\right\}, $$ and $T[\cdot]$ denotes the usual trace operator. The property $1$ is obviously true. The prove of property $2$ I put it at the end of this post. Please help me to check whether it is correct. However, I can not prove property $3$. The best I can do is assuming $(1)$ does not hold, i.e., there exists a sequence of set $T_n\subset \Omega$ such that $\lim_{n\to 0}\mathcal L^N(T_n)=0$ and  $$ \lim_{n\to\infty} \int_{T_n}|\nabla \bar u_n|^2dx\geq \epsilon>0 $$ and hope to have a contradiction. We can compute $$ \liminf_{n\to\infty}\int_\Omega|\nabla \bar u_n|^2dx\geq \liminf_{n\to\infty}\int_{\Omega\setminus T_n}|\nabla \bar u_n|^2dx+\liminf_{n\to\infty}\int_{T_n}|\nabla \bar u_n|^2dx\geq \int_\Omega|\nabla \bar u\,|^2dx+\epsilon  $$ but I can not get any contradiction from here. I feel I need to use the minimality of $\nabla\bar u_n$ but I don't see how... Any help of new idea of how to construct $\bar u_n$ is really welcome! Below is how to proof property $2$. Now let me prove property $2$. Clearly $\bar u_n$ is bounded in $H^1$ and hence, up to a subsequence, $\bar u_n\to u_0$ weakly in $H^1$. I only need to prove that $u_0=\bar u$. To do so, I only need to prove that $u_0$ is the weak solution of PDE $$ \begin{cases} -\Delta v=0, & x\in\Omega\\ v=u, & x\in\partial\Omega \end{cases} $$ By weak convergence in $H^1$, we have  $$ \int_\Omega \nabla u_0\nabla \phi=0 $$ for all $\phi\in H_0^1(\Omega)$. I only need to prove that $u_0\in \mathcal A(u)$ then I would be done. To do so, I need to prove $u_0-u\in H_0^1(\Omega)$. I will claim $$ \left|\int_\Omega (u_0-u)(x) \partial_i\varphi(x)dx\right|\leq C\|\varphi\|_{L^2(\Omega)} $$ for all $\phi\in C_c^\infty(\mathbb R^N)$. We observe that  \begin{multline} \left|\int_\Omega (u_0-u)(x) \partial_i\varphi(x)dx\right|=\\ \lim_{n\to\infty}\left|\int_\Omega (\bar u_n-u_n)(x) \partial_i\varphi(x)dx\right|=\lim_{n\to\infty}\left|\int_\Omega \partial x_i(\bar u_n-u_n)(x) \varphi(x)dx\right|\\ \leq \lim_{n\to\infty}\|\nabla (\bar u_n-u_n)\|_{L^2}\|v\|_{L^2}\leq C\|v\|_{L^2} \end{multline} as desired, where the 3rd inequality used the fact that $T[\bar u_n-u_n]\equiv 0$. Hence, by the uniqueness of solution, we have $u_0=\bar u$, and hence property $2$ is true. PS: I also post this problem in Mathoverflow here because this post is just an update of my yesterday's post which exist on both set... Sorry for double posting here! I will avoid this situation for my next post.",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
64,What is this series? $\cos\frac{\pi z^2}{2}\sum_{n=1}^{\infty} \frac{\left(-1\right)^n \pi^{2n +1} z^{4n +3} }{1 \cdot 3 \cdots\left(4n + 3\right)}$,What is this series?,\cos\frac{\pi z^2}{2}\sum_{n=1}^{\infty} \frac{\left(-1\right)^n \pi^{2n +1} z^{4n +3} }{1 \cdot 3 \cdots\left(4n + 3\right)},"Before I began to study mathematics, a friend of mine bought me a shirt with the imprint of a formula. I did not know what these characters were and had no desire to think about it. Yesterday, I cleaned out my closet and found this shirt (After three years study of mathematics). I looked at this formula again and realized that this formula has been a series, i.e. $$\cos\left(\frac{\pi}{2} z^2\right)\sum\limits_{n=1}^{\infty} \frac{\left(-1\right)^n \pi^{2n +1} z^{4n +3} }{1 \cdot 3 \cdots\left(4n + 3\right)} = ?$$ I don't know if $z$ should be a complex or a real number. Also I can't figure out if this series converge or not. My question: Is this a known series? Does this series converge? Thanks in advance.","Before I began to study mathematics, a friend of mine bought me a shirt with the imprint of a formula. I did not know what these characters were and had no desire to think about it. Yesterday, I cleaned out my closet and found this shirt (After three years study of mathematics). I looked at this formula again and realized that this formula has been a series, i.e. I don't know if should be a complex or a real number. Also I can't figure out if this series converge or not. My question: Is this a known series? Does this series converge? Thanks in advance.",\cos\left(\frac{\pi}{2} z^2\right)\sum\limits_{n=1}^{\infty} \frac{\left(-1\right)^n \pi^{2n +1} z^{4n +3} }{1 \cdot 3 \cdots\left(4n + 3\right)} = ? z,"['calculus', 'real-analysis', 'sequences-and-series', 'soft-question']"
65,Define $S\equiv\{ x\in \mathbb{Q}\mid x^2<2\}$. Show that $\sup S=\sqrt{2} $.,Define . Show that .,S\equiv\{ x\in \mathbb{Q}\mid x^2<2\} \sup S=\sqrt{2} ,"Define $S\equiv\{ x\in \mathbb{Q}\mid x^2<2\}$. Show that $\sup S=\sqrt{2} $. For this question, I think that I would use the completeness axiom. As $3$ is greater than $2$, so $S$ has a upper bound. To yield a contradiction, I think I need to find there exists an constant $c$ which is a least upper bound such that $b^2>2$, then there exists a number $r$ such that $b-r$ also in upper bound of $S$ which gives a contradiction since $b$ is the least upper bound of $S$. Then we can end the proof. Does the idea right? If not, can anyone give a suggestion or a hit to write a better proof? Thanks.","Define $S\equiv\{ x\in \mathbb{Q}\mid x^2<2\}$. Show that $\sup S=\sqrt{2} $. For this question, I think that I would use the completeness axiom. As $3$ is greater than $2$, so $S$ has a upper bound. To yield a contradiction, I think I need to find there exists an constant $c$ which is a least upper bound such that $b^2>2$, then there exists a number $r$ such that $b-r$ also in upper bound of $S$ which gives a contradiction since $b$ is the least upper bound of $S$. Then we can end the proof. Does the idea right? If not, can anyone give a suggestion or a hit to write a better proof? Thanks.",,"['real-analysis', 'proof-writing']"
66,Cauchy's Mean Value Theorem. What can we say about $c$ with more information.,Cauchy's Mean Value Theorem. What can we say about  with more information.,c,"My questions is about Cauchy's Mean Value theorem which states: If functions f and g are both continuous on the closed interval [a,b],   and differentiable on the open interval (a, b), then there exists some   c ∈ (a,b), such that   \begin{align*} (f(b)-f(a))g'(c)=(g(b)-g(a))f'(c) \end{align*} From my understanding $c$ is not that trivial to find and it might not even be unique.  Here are is my questions? For $f \neq g$ 1) Under what conditions on $g$ and $ f$ is $c$ unique? For example, is monotonicity or concavity enough? 2) If we give more information on $g$ and $f$.  Can we say more about the location of $c$ in the range $(a,b)$? For example, say that $g$ and $f$ are non-negative and $g'',f''> 0$, what can we say then? **  For example, is it true if $f$ and $g$ are concave then $ c \in( \frac{b+a}{2},b)$ I skimmed through several reference but wasn't able to find anything concrete about c based on properties of $f$ and $g$? Thank you for any help in advance. Also, any references would be greatly appreciated. Edit: As pointed out by @LeonAragones monotonicity of $f,g$ is not enuogh to guarantee uniquens of $c$. ** Edit:** Please the answer to the first part of the question by @san.","My questions is about Cauchy's Mean Value theorem which states: If functions f and g are both continuous on the closed interval [a,b],   and differentiable on the open interval (a, b), then there exists some   c ∈ (a,b), such that   \begin{align*} (f(b)-f(a))g'(c)=(g(b)-g(a))f'(c) \end{align*} From my understanding $c$ is not that trivial to find and it might not even be unique.  Here are is my questions? For $f \neq g$ 1) Under what conditions on $g$ and $ f$ is $c$ unique? For example, is monotonicity or concavity enough? 2) If we give more information on $g$ and $f$.  Can we say more about the location of $c$ in the range $(a,b)$? For example, say that $g$ and $f$ are non-negative and $g'',f''> 0$, what can we say then? **  For example, is it true if $f$ and $g$ are concave then $ c \in( \frac{b+a}{2},b)$ I skimmed through several reference but wasn't able to find anything concrete about c based on properties of $f$ and $g$? Thank you for any help in advance. Also, any references would be greatly appreciated. Edit: As pointed out by @LeonAragones monotonicity of $f,g$ is not enuogh to guarantee uniquens of $c$. ** Edit:** Please the answer to the first part of the question by @san.",,"['calculus', 'real-analysis', 'derivatives']"
67,"find all continuous functions $f:\mathbb{R}^n \rightarrow \mathbb{R}$ satisfying $f(x+y)+f(x-y)=2f(x)+2f(y)$ for all $x,y \in \mathbb{R}^n$",find all continuous functions  satisfying  for all,"f:\mathbb{R}^n \rightarrow \mathbb{R} f(x+y)+f(x-y)=2f(x)+2f(y) x,y \in \mathbb{R}^n","find all continuous functions $f:\mathbb{R}^n \rightarrow \mathbb{R}$ satisfying  \begin{equation*} f(x+y)+f(x-y)=2f(x)+2f(y)~\forall x,y \in \mathbb{R}^n. \end{equation*} My attempt: I manage to show that for any $q \in \mathbb{Q}$, $f(qx)=q^2f(x)$ for all $x \in \mathbb{R}^n$. I have a feeling that the answer is $f(x)=A\| x \|^2$, but I'm unable to prove it. Can anyone give some hint?","find all continuous functions $f:\mathbb{R}^n \rightarrow \mathbb{R}$ satisfying  \begin{equation*} f(x+y)+f(x-y)=2f(x)+2f(y)~\forall x,y \in \mathbb{R}^n. \end{equation*} My attempt: I manage to show that for any $q \in \mathbb{Q}$, $f(qx)=q^2f(x)$ for all $x \in \mathbb{R}^n$. I have a feeling that the answer is $f(x)=A\| x \|^2$, but I'm unable to prove it. Can anyone give some hint?",,"['real-analysis', 'contest-math', 'functional-equations']"
68,uniform limit of step function,uniform limit of step function,,"Define a step function to be a function that is piecewise constant, $$ f(x)=\sum_{i=1}^{n}c_i\chi_{[a_i,b_i)},$$ where $[a_i,b_i)$ are disjoint intervals. Prove that every continuous function on a compact interval is a uniform limit of step functions. Prove that a uniform limit of step functions (on a compact  interval) is Riemann integrable. For second one, if 1. holds, uniform convergence can be used to establish integrability and dif­ferentiability of limits of functions and the interchange of the operation and the limit. So, if $f_n$ are Riemann integrable on $[a,b]$ and $f_n$ converges uniformly to $f:[a,b]\rightarrow\mathbb{R}$, then $f$ is Riemann integrable and $$ \int_a^bf_ndx\longrightarrow\int_a^bfdx. $$ But, how can I prove the first lemma?","Define a step function to be a function that is piecewise constant, $$ f(x)=\sum_{i=1}^{n}c_i\chi_{[a_i,b_i)},$$ where $[a_i,b_i)$ are disjoint intervals. Prove that every continuous function on a compact interval is a uniform limit of step functions. Prove that a uniform limit of step functions (on a compact  interval) is Riemann integrable. For second one, if 1. holds, uniform convergence can be used to establish integrability and dif­ferentiability of limits of functions and the interchange of the operation and the limit. So, if $f_n$ are Riemann integrable on $[a,b]$ and $f_n$ converges uniformly to $f:[a,b]\rightarrow\mathbb{R}$, then $f$ is Riemann integrable and $$ \int_a^bf_ndx\longrightarrow\int_a^bfdx. $$ But, how can I prove the first lemma?",,['real-analysis']
69,"Does $(f_n(x))= (\frac{nx}{1+nx^2})$ converge pointwise/uniformly on $I= [0,1]$?",Does  converge pointwise/uniformly on ?,"(f_n(x))= (\frac{nx}{1+nx^2}) I= [0,1]","Does $\displaystyle(f_n(x))= \bigg(\frac{nx}{1+nx^2}\bigg)$ converge pointwise/uniformly on $I= [0,1]$? My attempt: Pointwise: $\displaystyle \lim_{n \to \infty}f_n(x) = \lim_{n \to \infty} \frac{x}{\frac{1}{n}+ x^2} = \frac{1}{x}$. However, notice that $f(x) = \frac{1}{x}$ is discontinuous at the point $x=0$, hence $(f_n(x))$ does not converge pointwise on $I = [0,1]$. Consequently it does not converge uniformly on $I = [0,1]$. Is my reasoning correct? Or am I missing something?","Does $\displaystyle(f_n(x))= \bigg(\frac{nx}{1+nx^2}\bigg)$ converge pointwise/uniformly on $I= [0,1]$? My attempt: Pointwise: $\displaystyle \lim_{n \to \infty}f_n(x) = \lim_{n \to \infty} \frac{x}{\frac{1}{n}+ x^2} = \frac{1}{x}$. However, notice that $f(x) = \frac{1}{x}$ is discontinuous at the point $x=0$, hence $(f_n(x))$ does not converge pointwise on $I = [0,1]$. Consequently it does not converge uniformly on $I = [0,1]$. Is my reasoning correct? Or am I missing something?",,"['real-analysis', 'sequences-and-series', 'uniform-convergence']"
70,Properties of the space of $ T $-invariant probability measures over a compact topological space.,Properties of the space of -invariant probability measures over a compact topological space., T ,"Let $T: X \to X$ be a continuos map defined on the compact space (Maybe Haussdorff) $X$.  Denote by $M_T$ the space of $T$-invariant probability measures defined over the Sigma Algebra of Borel sets. Prove the following: Show that $M_T$ is non-empty Show that $M_T$ is compact and Haussdorff. Important remark: I know the result is true for spaces which are Compact and Metric. I think that the result is false for general Compact and Haussdorff Topological Spaces . If the result is false, please give me a counterexample. I tried to find a counterexample but I couldn't. At the moment I'm more interested to discover if $M_T$ is nonempty rather than the second proposition.","Let $T: X \to X$ be a continuos map defined on the compact space (Maybe Haussdorff) $X$.  Denote by $M_T$ the space of $T$-invariant probability measures defined over the Sigma Algebra of Borel sets. Prove the following: Show that $M_T$ is non-empty Show that $M_T$ is compact and Haussdorff. Important remark: I know the result is true for spaces which are Compact and Metric. I think that the result is false for general Compact and Haussdorff Topological Spaces . If the result is false, please give me a counterexample. I tried to find a counterexample but I couldn't. At the moment I'm more interested to discover if $M_T$ is nonempty rather than the second proposition.",,"['real-analysis', 'dynamical-systems', 'ergodic-theory']"
71,Limit points and subsequences,Limit points and subsequences,,"I'm having trouble with proving this exercise: Let $(a_{n})_{n = 0}^\infty$ be a sequence of real numbers, and $L \in \mathbb{R}$ . Prove: $L$ is a limit point of $(a_{n})_{n = 0}^\infty \Leftrightarrow$ There is a subsequence of $(a_{n})_{n = 0}^\infty$ that converges to L. So far, I've done this: $\Leftarrow:$ There is a subsequence of $(a_{n})_{n = 0}^\infty$ that converges to L. Assume $(b_{n})_{n = 0}^\infty$ is that subsequence, and $\epsilon \in \mathbb{R}$ $\Rightarrow \exists$ $n \in \mathbb{N}$ such that $|b_{n} - L| < \epsilon$ $\forall$ $n \geqslant N, \epsilon > 0$ $\Rightarrow$ $(b_{n})_{n = 0}^\infty$ is a subsequence of $(a_{n})_{n = 0}^\infty$, so $b_{n} = a_{m}$ for a certain $m \geqslant 0$ $\Rightarrow$ $\exists$ $m \in \mathbb{N}$ such that $|a_{m} - L| < \epsilon$ $\forall$ $m \geqslant N, \epsilon > 0$ $\Rightarrow L$ is a limit point of $(a_{n})_{n = 0}^\infty$ Is my left implication correct? And if not, how does it have to be? Could you explain me the right implication? Thanks in advance!","I'm having trouble with proving this exercise: Let $(a_{n})_{n = 0}^\infty$ be a sequence of real numbers, and $L \in \mathbb{R}$ . Prove: $L$ is a limit point of $(a_{n})_{n = 0}^\infty \Leftrightarrow$ There is a subsequence of $(a_{n})_{n = 0}^\infty$ that converges to L. So far, I've done this: $\Leftarrow:$ There is a subsequence of $(a_{n})_{n = 0}^\infty$ that converges to L. Assume $(b_{n})_{n = 0}^\infty$ is that subsequence, and $\epsilon \in \mathbb{R}$ $\Rightarrow \exists$ $n \in \mathbb{N}$ such that $|b_{n} - L| < \epsilon$ $\forall$ $n \geqslant N, \epsilon > 0$ $\Rightarrow$ $(b_{n})_{n = 0}^\infty$ is a subsequence of $(a_{n})_{n = 0}^\infty$, so $b_{n} = a_{m}$ for a certain $m \geqslant 0$ $\Rightarrow$ $\exists$ $m \in \mathbb{N}$ such that $|a_{m} - L| < \epsilon$ $\forall$ $m \geqslant N, \epsilon > 0$ $\Rightarrow L$ is a limit point of $(a_{n})_{n = 0}^\infty$ Is my left implication correct? And if not, how does it have to be? Could you explain me the right implication? Thanks in advance!",,"['real-analysis', 'sequences-and-series']"
72,Convergence in probability iff convergence for every bounded continuous function,Convergence in probability iff convergence for every bounded continuous function,,"I'm trying to show the following: $X_n \overset{p}{\to}X \iff f \circ X_n \overset{p}{\to} f \circ X$ for every continuous, bounded function $f$. I can show ($\Rightarrow$) already using the usual almost-everywhere converging subsequences approach.  The other direction is giving me difficulty. I believe I can show it if $f$ is strictly increasing: then $f$ is a homeomorphism of $\mathbb{R}$ onto some interval $(a,b)$, thus $f^{-1}$ is continuous, so it suffices to apply the continuous function $f^{-1}$ to $f\circ X_n$.  (Though if this is incorrect please do let me know.) For the general case, however, I'm not quite sure how to proceed.  My two questions are: 1) How to show $(\Leftarrow)$ when I can't rely on $f$ being strictly monotone? 2) What is the intuition behind needing $f$ to be bounded?  I feel like this should be obvious but I can't seem to intuit why. Many thanks. Disclaimer: I'm pursuing self-study in Cinlar's 'Probability and Stochastics' in my spare time, this is problem 3.14.","I'm trying to show the following: $X_n \overset{p}{\to}X \iff f \circ X_n \overset{p}{\to} f \circ X$ for every continuous, bounded function $f$. I can show ($\Rightarrow$) already using the usual almost-everywhere converging subsequences approach.  The other direction is giving me difficulty. I believe I can show it if $f$ is strictly increasing: then $f$ is a homeomorphism of $\mathbb{R}$ onto some interval $(a,b)$, thus $f^{-1}$ is continuous, so it suffices to apply the continuous function $f^{-1}$ to $f\circ X_n$.  (Though if this is incorrect please do let me know.) For the general case, however, I'm not quite sure how to proceed.  My two questions are: 1) How to show $(\Leftarrow)$ when I can't rely on $f$ being strictly monotone? 2) What is the intuition behind needing $f$ to be bounded?  I feel like this should be obvious but I can't seem to intuit why. Many thanks. Disclaimer: I'm pursuing self-study in Cinlar's 'Probability and Stochastics' in my spare time, this is problem 3.14.",,"['real-analysis', 'probability', 'measure-theory', 'convergence-divergence', 'recreational-mathematics']"
73,Axiom of Completeness to prove intermediate value theorem,Axiom of Completeness to prove intermediate value theorem,,"I am having a little trouble understanding one of the steps in this proof. From Stephen Abbott's Analysis : Using AoC to prove the IVT: TO simplify matters, consider $f$ as a continuous function which satisfies $f(a)<0<f(b)$ and show that $f(c) =0 $ for some $c \in (a,b)$. First let Clearly we can see $K$ satisfies the axiom of completeness, and thus we can let $c = \sup K$ There are three cases to consider: $f(c) > 0, f(c)<0 \space \text{and} \space f(c) = 0$ Part where I have difficulty understanding the proof . The author states: ""Since $c$ is the least upper bound of $K$, we can rule out the first two cases. "" I don't understand how we can conclude this immediately? Thinking about it, if $f(c)>0$, then $c \notin K$ so this would contradict $c$ being a least upper bound.(Right?) However, how can we rule out $f(c)<0$? Here $c \in K$, which is a valid least upper bound?","I am having a little trouble understanding one of the steps in this proof. From Stephen Abbott's Analysis : Using AoC to prove the IVT: TO simplify matters, consider $f$ as a continuous function which satisfies $f(a)<0<f(b)$ and show that $f(c) =0 $ for some $c \in (a,b)$. First let Clearly we can see $K$ satisfies the axiom of completeness, and thus we can let $c = \sup K$ There are three cases to consider: $f(c) > 0, f(c)<0 \space \text{and} \space f(c) = 0$ Part where I have difficulty understanding the proof . The author states: ""Since $c$ is the least upper bound of $K$, we can rule out the first two cases. "" I don't understand how we can conclude this immediately? Thinking about it, if $f(c)>0$, then $c \notin K$ so this would contradict $c$ being a least upper bound.(Right?) However, how can we rule out $f(c)<0$? Here $c \in K$, which is a valid least upper bound?",,['real-analysis']
74,"Proof that the function $f(x)=d(x,y)$ is uniformly continuous?",Proof that the function  is uniformly continuous?,"f(x)=d(x,y)","Consider a metric space $(M, {\rm d})$  and $y$  fixed in $M$. I want to prove that the function $f$ defined by $f(x)\colon={\rm d}(x,y)$ is uniformly continuous. So I know that if this function is uniformly continuous, then for all $\epsilon>0$, there exists a $\delta>0$ such that if the ${\rm d}(x_1,x_2)<\delta$, then this implies the ${\rm d}(f(x_1),f(x_2))<\epsilon$. So for all epsilon greater than zero, there exists a delta greater than zero such that ${\rm d}(x_1,x_2)<\delta \implies |{\rm d}(x_1,y)-{\rm d}(x_2,y)|<\epsilon$. I can't figure out how to manipulate this equation to solve for delta!!  I tried using the triangle inequality but it gets me nowhere. Please help!","Consider a metric space $(M, {\rm d})$  and $y$  fixed in $M$. I want to prove that the function $f$ defined by $f(x)\colon={\rm d}(x,y)$ is uniformly continuous. So I know that if this function is uniformly continuous, then for all $\epsilon>0$, there exists a $\delta>0$ such that if the ${\rm d}(x_1,x_2)<\delta$, then this implies the ${\rm d}(f(x_1),f(x_2))<\epsilon$. So for all epsilon greater than zero, there exists a delta greater than zero such that ${\rm d}(x_1,x_2)<\delta \implies |{\rm d}(x_1,y)-{\rm d}(x_2,y)|<\epsilon$. I can't figure out how to manipulate this equation to solve for delta!!  I tried using the triangle inequality but it gets me nowhere. Please help!",,"['real-analysis', 'metric-spaces', 'continuity', 'uniform-continuity', 'epsilon-delta']"
75,"Show that sets of real numbers $A, B$ are adjacent iff $\sup A = \inf B$",Show that sets of real numbers  are adjacent iff,"A, B \sup A = \inf B","If $A,B \subset \mathbb{R}$ satisfy :   $$\begin{cases}\forall\ a \in A,\ \forall\ b \in B,\ a \le b \cr   \forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b-a \le \epsilon\end{cases}$$   then we say that  $A$ and $B$ are adjacent . Show that $A$ and $B$ are adjacent if and only if : $\sup(A) = \inf(B)$. My thoughts : note that : $$\sup A =: \begin{cases}\forall\ a \in A,\  ,\ a \le \sup A \cr   \forall\ \epsilon > 0,\ \exists\ a \in A,\ \text{     such that }\quad \epsilon-\sup A  <  a\le \epsilon\end{cases}$$ $$\inf B =: \begin{cases}\forall\ b \in B,\  ,\ \inf B \le b \cr   \forall\ \epsilon > 0,\ \exists\ b \in B,\ \text{     such that }\quad \inf B \le b  <  \inf B+\epsilon\end{cases}$$ To show the first implication : Assume that $A$ and $B$ are adjacent and let's show that $\sup(A)$, and $\inf(B)$ exists such that $\sup(A) = \inf(B)$. Show first that $\sup(A)$, and $\inf(B)$ exists : Let $b\in B$, we have $$\forall a\in A,\quad a \le b$$ then b is  upper bound, $A \neq  \emptyset, A  \subseteq   \mathbb{R}$ then $\sup(A)$ exist. Let $a\in A$, we have $$\forall b\in B,\quad a \le b$$ then a is  Lower bound, $B \neq  \emptyset, B  \subseteq   \mathbb{R}$ then $\inf(B)$ exist. Show first that $\sup(A)=\inf(B)$: we have :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b-a \le \epsilon$$ or $$\forall\ \epsilon > 0,\ \exists\ a,b \in A\times B,\ \text{     such that }$$ $$ \begin{cases}\epsilon-\sup A  <  a\le \epsilon \cr    \inf B \le b  <  \inf B+\epsilon\end{cases}$$ $$\iff \begin{cases}-\sup A  <  a\le \sup A-\epsilon\cr    \inf B \le b  <  \inf B+\epsilon\end{cases}$$ $$\iff \inf B-\sup A  < b-a < \sup A+\inf B $$ $$\iff \inf B-\sup A  < \epsilon \quad \forall \epsilon > 0 $$ i'm stuk here or we can say : since $\forall a,b \in A\times B \quad  a\leq  b $ then $\forall b\in B,\quad  \sup A \leq b$ then $$\sup A \leq \inf B **(1)** $$ we have :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b-a \le \epsilon$$ then  :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b<a+\epsilon $$ then  :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  \inf B \le \sup A + \epsilon $$ Then  $$\inf B \le \sup A **(2)**$$ From  (1) and (2) we have $$  \inf B=\sup A $$ To show the second implication : Assume that  $\sup(A)$, and $\inf(B)$ exists such that $\sup(A) = \inf(B)$ and let's show that $A$ and $B$ are adjacent To show : $$\forall\ a \in A,\ \forall\ b \in B,\ a \le b $$ from the defintion of $\sup A$ and $\inf B$ we have: $$ \forall\ a \in A,\  ,\ a \le \sup A \text{ and } \forall\ b \in B,\  ,\ \inf B \le b$$ or we know that $\sup(A) = \inf(B)$ then $$\forall\ a \in A,\ \forall\ b \in B,\ a \le \sup A =\inf B\le b $$ To show : $$  \forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b-a \le \epsilon $$ from the defintion of $\sup A$ and $\inf B$ we have: $$ \ \forall\ \epsilon > 0,\ \exists\ a \in A,\ \text{ such that }\quad \epsilon-\sup A  <  a\le \epsilon \text{ and } \forall\ \epsilon > 0,\ \exists\ b \in B,\ \text{ such that }\quad \inf B \le b  <  \inf B+\epsilon$$ we can also say that : $$\forall\ \epsilon > 0,\ \exists\ a,b \in A\times B,\ \text{     tell que } \begin{cases}\frac{ \epsilon }{2}-\sup A  <  a\le \frac{ \epsilon }{2}\cr    \inf B \le b  <  \inf B+\frac{ \epsilon }{2}\end{cases}$$ any help would be appreciated !!","If $A,B \subset \mathbb{R}$ satisfy :   $$\begin{cases}\forall\ a \in A,\ \forall\ b \in B,\ a \le b \cr   \forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b-a \le \epsilon\end{cases}$$   then we say that  $A$ and $B$ are adjacent . Show that $A$ and $B$ are adjacent if and only if : $\sup(A) = \inf(B)$. My thoughts : note that : $$\sup A =: \begin{cases}\forall\ a \in A,\  ,\ a \le \sup A \cr   \forall\ \epsilon > 0,\ \exists\ a \in A,\ \text{     such that }\quad \epsilon-\sup A  <  a\le \epsilon\end{cases}$$ $$\inf B =: \begin{cases}\forall\ b \in B,\  ,\ \inf B \le b \cr   \forall\ \epsilon > 0,\ \exists\ b \in B,\ \text{     such that }\quad \inf B \le b  <  \inf B+\epsilon\end{cases}$$ To show the first implication : Assume that $A$ and $B$ are adjacent and let's show that $\sup(A)$, and $\inf(B)$ exists such that $\sup(A) = \inf(B)$. Show first that $\sup(A)$, and $\inf(B)$ exists : Let $b\in B$, we have $$\forall a\in A,\quad a \le b$$ then b is  upper bound, $A \neq  \emptyset, A  \subseteq   \mathbb{R}$ then $\sup(A)$ exist. Let $a\in A$, we have $$\forall b\in B,\quad a \le b$$ then a is  Lower bound, $B \neq  \emptyset, B  \subseteq   \mathbb{R}$ then $\inf(B)$ exist. Show first that $\sup(A)=\inf(B)$: we have :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b-a \le \epsilon$$ or $$\forall\ \epsilon > 0,\ \exists\ a,b \in A\times B,\ \text{     such that }$$ $$ \begin{cases}\epsilon-\sup A  <  a\le \epsilon \cr    \inf B \le b  <  \inf B+\epsilon\end{cases}$$ $$\iff \begin{cases}-\sup A  <  a\le \sup A-\epsilon\cr    \inf B \le b  <  \inf B+\epsilon\end{cases}$$ $$\iff \inf B-\sup A  < b-a < \sup A+\inf B $$ $$\iff \inf B-\sup A  < \epsilon \quad \forall \epsilon > 0 $$ i'm stuk here or we can say : since $\forall a,b \in A\times B \quad  a\leq  b $ then $\forall b\in B,\quad  \sup A \leq b$ then $$\sup A \leq \inf B **(1)** $$ we have :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b-a \le \epsilon$$ then  :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b<a+\epsilon $$ then  :$$\forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  \inf B \le \sup A + \epsilon $$ Then  $$\inf B \le \sup A **(2)**$$ From  (1) and (2) we have $$  \inf B=\sup A $$ To show the second implication : Assume that  $\sup(A)$, and $\inf(B)$ exists such that $\sup(A) = \inf(B)$ and let's show that $A$ and $B$ are adjacent To show : $$\forall\ a \in A,\ \forall\ b \in B,\ a \le b $$ from the defintion of $\sup A$ and $\inf B$ we have: $$ \forall\ a \in A,\  ,\ a \le \sup A \text{ and } \forall\ b \in B,\  ,\ \inf B \le b$$ or we know that $\sup(A) = \inf(B)$ then $$\forall\ a \in A,\ \forall\ b \in B,\ a \le \sup A =\inf B\le b $$ To show : $$  \forall\ \epsilon > 0,\ \exists\ a \in A,\ \exists\ b \in B \text{     such that }\quad  b-a \le \epsilon $$ from the defintion of $\sup A$ and $\inf B$ we have: $$ \ \forall\ \epsilon > 0,\ \exists\ a \in A,\ \text{ such that }\quad \epsilon-\sup A  <  a\le \epsilon \text{ and } \forall\ \epsilon > 0,\ \exists\ b \in B,\ \text{ such that }\quad \inf B \le b  <  \inf B+\epsilon$$ we can also say that : $$\forall\ \epsilon > 0,\ \exists\ a,b \in A\times B,\ \text{     tell que } \begin{cases}\frac{ \epsilon }{2}-\sup A  <  a\le \frac{ \epsilon }{2}\cr    \inf B \le b  <  \inf B+\frac{ \epsilon }{2}\end{cases}$$ any help would be appreciated !!",,"['calculus', 'real-analysis', 'supremum-and-infimum']"
76,Approximating a piecewise continuous function with a function in $\mathcal{C}^{\infty}_{0}(\mathbb{R})$,Approximating a piecewise continuous function with a function in,\mathcal{C}^{\infty}_{0}(\mathbb{R}),"Let $\eta \in \mathcal{C}^{\infty}_{0}(\mathbb{R})$, where $\mathcal{C}^{\infty}_{0}(\mathbb{R})$ is the set of compactly supported infinitely differentiable function, be a function which is non-negative, non-vanishing and constant in a neighborhood of $x=0$, symmetric about $x=0$, supported in $[-1,1]$, bounded between $0$ and $\eta(0)$, and normalized so that $\int \eta d\mu =1$. Let $\varphi$ be a compactly supported absolutely continuous function with a piecewise continuous derivative $\varphi'$. I have been told (by a user of MSE whom I thank very much again) the following, which I think to be interesting enough to be the object of a specifical question: Then one defines$$\eta_{n}(x) = n\eta(x/n)$$so that   $\int_{\mathbb{R}}\eta_{n}\,d\mu =1$ for all $n$. For any compactly   supported absolutely continuous function $\varphi$,   define$$\varphi_{n}=\int_{\mathbb{R}}\eta_{n}(x-y)\varphi(y)\,d\mu(y).$$The function   $\varphi_{n}$ is in $\mathcal{C}^{\infty}_{0}(\mathbb{R})$. Because   $\eta_{n}$ is supported in $[-1/n,1/n]$, and $\varphi$ is continuous,   then $\varphi_{n}$ converges uniformly to $\varphi$ as   $n\rightarrow\infty$. And, because $\varphi$ is absolutely continuous,   then$$\varphi_{n}'=\int_{\mathbb{R}}\eta_{n}'(x-y)\varphi(y)\,d\mu(y)=-\int_{\mathbb{R}}\eta_{n}(x-y)\varphi'(y)\,d\mu(y).$$   For my case $\varphi'$ is piecewise continuous, and the right   side then converges pointwise everywhere to mean of the left- and   right-hand limits of $\varphi'$, and it remains uniformly bounded by   any bound for $\varphi'$. I must say that I have no knowledge of the theory of mollification until now. I would like to understand why the fact that $\eta_{n}$ is supported in $[-1/n,1/n]$, and $\varphi$ is continuous, implies that $\varphi_{n}$ converges uniformly to $\varphi$ as $n\rightarrow\infty$; the absolute continuity of $\varphi$ implies $\varphi_{n}'=\int_{\mathbb{R}}\eta_{n}'(x-y)\varphi(y)\,d\mu(y)=-\int_{\mathbb{R}}\eta_{n}(x-y)\varphi'(y)\,d\mu(y)$; the piecewise continuity of $\varphi'$ implies the facts that $-\int_{\mathbb{R}}\eta_{n}(x-y)\varphi'(y)\,d\mu(y)$ converges to $(\lim_{t\to x^+}\varphi'(t)+\lim_{t\to x^-}\varphi'(t))/2$ and $|\int_{\mathbb{R}}\eta_{n}(x-y)\varphi'(y)\,d\mu(y)|\le |\varphi'(x)|$. I heartily thank both who told me these interesting facts and whoever will help me to understand the reason of the quoted facts.","Let $\eta \in \mathcal{C}^{\infty}_{0}(\mathbb{R})$, where $\mathcal{C}^{\infty}_{0}(\mathbb{R})$ is the set of compactly supported infinitely differentiable function, be a function which is non-negative, non-vanishing and constant in a neighborhood of $x=0$, symmetric about $x=0$, supported in $[-1,1]$, bounded between $0$ and $\eta(0)$, and normalized so that $\int \eta d\mu =1$. Let $\varphi$ be a compactly supported absolutely continuous function with a piecewise continuous derivative $\varphi'$. I have been told (by a user of MSE whom I thank very much again) the following, which I think to be interesting enough to be the object of a specifical question: Then one defines$$\eta_{n}(x) = n\eta(x/n)$$so that   $\int_{\mathbb{R}}\eta_{n}\,d\mu =1$ for all $n$. For any compactly   supported absolutely continuous function $\varphi$,   define$$\varphi_{n}=\int_{\mathbb{R}}\eta_{n}(x-y)\varphi(y)\,d\mu(y).$$The function   $\varphi_{n}$ is in $\mathcal{C}^{\infty}_{0}(\mathbb{R})$. Because   $\eta_{n}$ is supported in $[-1/n,1/n]$, and $\varphi$ is continuous,   then $\varphi_{n}$ converges uniformly to $\varphi$ as   $n\rightarrow\infty$. And, because $\varphi$ is absolutely continuous,   then$$\varphi_{n}'=\int_{\mathbb{R}}\eta_{n}'(x-y)\varphi(y)\,d\mu(y)=-\int_{\mathbb{R}}\eta_{n}(x-y)\varphi'(y)\,d\mu(y).$$   For my case $\varphi'$ is piecewise continuous, and the right   side then converges pointwise everywhere to mean of the left- and   right-hand limits of $\varphi'$, and it remains uniformly bounded by   any bound for $\varphi'$. I must say that I have no knowledge of the theory of mollification until now. I would like to understand why the fact that $\eta_{n}$ is supported in $[-1/n,1/n]$, and $\varphi$ is continuous, implies that $\varphi_{n}$ converges uniformly to $\varphi$ as $n\rightarrow\infty$; the absolute continuity of $\varphi$ implies $\varphi_{n}'=\int_{\mathbb{R}}\eta_{n}'(x-y)\varphi(y)\,d\mu(y)=-\int_{\mathbb{R}}\eta_{n}(x-y)\varphi'(y)\,d\mu(y)$; the piecewise continuity of $\varphi'$ implies the facts that $-\int_{\mathbb{R}}\eta_{n}(x-y)\varphi'(y)\,d\mu(y)$ converges to $(\lim_{t\to x^+}\varphi'(t)+\lim_{t\to x^-}\varphi'(t))/2$ and $|\int_{\mathbb{R}}\eta_{n}(x-y)\varphi'(y)\,d\mu(y)|\le |\varphi'(x)|$. I heartily thank both who told me these interesting facts and whoever will help me to understand the reason of the quoted facts.",,"['real-analysis', 'integration', 'functional-analysis']"
77,"Limit of the Derivative of an Increasing, Bounded-Above Function","Limit of the Derivative of an Increasing, Bounded-Above Function",,"Let $f:(0,\infty) \to \mathbb{R}$ be a differentiable function, which is increasing and bounded above.  Then does $\lim_{x \to \infty} f'(x)=0$? If we assume that $\lim_{x \to \infty} f'(x)$ exists, then this is true by an argument using the mean value theorem: By assumption $L=\lim_{x \to \infty} f(x)$ exists and is finite, and then $0=L-L=\lim_{n \to \infty} f(n+1)-f(n)=\lim_{n \to \infty} f'(x_n)$ for some $x_n \in (n,n+1)$ by the mean value theorem.  But this doesn't work if we don't assume $\lim_{x \to \infty} f'(x)$ exists because $x_n$ isn't an arbitrary sequence with $x_n \to \infty$. Intuitively it seems that it should be true without this assumption, but of course that doesn't mean that it's true.","Let $f:(0,\infty) \to \mathbb{R}$ be a differentiable function, which is increasing and bounded above.  Then does $\lim_{x \to \infty} f'(x)=0$? If we assume that $\lim_{x \to \infty} f'(x)$ exists, then this is true by an argument using the mean value theorem: By assumption $L=\lim_{x \to \infty} f(x)$ exists and is finite, and then $0=L-L=\lim_{n \to \infty} f(n+1)-f(n)=\lim_{n \to \infty} f'(x_n)$ for some $x_n \in (n,n+1)$ by the mean value theorem.  But this doesn't work if we don't assume $\lim_{x \to \infty} f'(x)$ exists because $x_n$ isn't an arbitrary sequence with $x_n \to \infty$. Intuitively it seems that it should be true without this assumption, but of course that doesn't mean that it's true.",,['real-analysis']
78,Show that f is periodic if $f(x+a)+f(x+b)=\frac{f(2x)}{2}$?,Show that f is periodic if ?,f(x+a)+f(x+b)=\frac{f(2x)}{2},Suppose $a$ and $b$ are distinct real numbers and $f$ is a continuous real function such that $\frac{f(x)}{x^2}$ goes to 0 when $x$ goes to infinity or minus infinity. Suppose that$ f(x+a)+f(x+b)=\frac{f(2x)}{2}$. How show that $f$ is periodic?,Suppose $a$ and $b$ are distinct real numbers and $f$ is a continuous real function such that $\frac{f(x)}{x^2}$ goes to 0 when $x$ goes to infinity or minus infinity. Suppose that$ f(x+a)+f(x+b)=\frac{f(2x)}{2}$. How show that $f$ is periodic?,,"['real-analysis', 'functions', 'functional-equations']"
79,Proving limit through definition,Proving limit through definition,,"Prove  $$\lim_{x\to 2}\frac{x^2+4}{x+2}=2$$ through definition. My solution: Fix $\epsilon >0$ and find $\delta$ \begin{align} 0<|x-2|<\delta &\Rightarrow \left| \frac{x^2+4}{x+2}-2 \right| < \epsilon\\ &\Rightarrow\left|\frac{x(x-2)}{x+2}\right| < \epsilon \end{align} Let $\delta <1$, then $0<|x-2|<1$ then $x\in (1,3)$ and $x>0$ and $x+2>0$ $$0<|x-2|<1 \Rightarrow \frac{x}{x+2}|x-2|<\epsilon$$ In conclusion $\delta :=\min \left\{ 1,\frac{5\epsilon}3\right\}$ Unfortunately the answer is not correct (according to my book). It says $\delta :=\min \left\{ 1,\epsilon \right\}$. Where did I make a mistake?","Prove  $$\lim_{x\to 2}\frac{x^2+4}{x+2}=2$$ through definition. My solution: Fix $\epsilon >0$ and find $\delta$ \begin{align} 0<|x-2|<\delta &\Rightarrow \left| \frac{x^2+4}{x+2}-2 \right| < \epsilon\\ &\Rightarrow\left|\frac{x(x-2)}{x+2}\right| < \epsilon \end{align} Let $\delta <1$, then $0<|x-2|<1$ then $x\in (1,3)$ and $x>0$ and $x+2>0$ $$0<|x-2|<1 \Rightarrow \frac{x}{x+2}|x-2|<\epsilon$$ In conclusion $\delta :=\min \left\{ 1,\frac{5\epsilon}3\right\}$ Unfortunately the answer is not correct (according to my book). It says $\delta :=\min \left\{ 1,\epsilon \right\}$. Where did I make a mistake?",,"['calculus', 'real-analysis']"
80,Confusing Analysis proof,Confusing Analysis proof,,"I have a question about a proof of the Beltrami-Enneper theorem: In the following $\nu$ is the surface-normal and $e_1,e_2,e_3$ the Frenet 3-frame. It states: Every asymptotic curve $c: I \rightarrow S \subset \mathbb{R}^3$  ( $II(c',c')=0$, where $II$ is the second fundamental form) with curvature $\kappa \neq 0$ and torsion $\tau$ satisfies the equation $\tau^2=-K$, where $\tau$ is the torsion of the curve and $K$ the Gauß-curvature of the surface $S$. Proof: Let $c(s)$ be an aymptotic curve with $II(c',c')=0$. Then the normal curvature of $c$ vanishes. Hence, $e_2$ is tangential to the surface (so far is everything alright), but now it goes on with, then $e_3 = \nu$.  I mean, I see that $e_3$ is then either $+ \nu$ or $- \nu$, but I don't see why the $+$ sign is already clear here. Then it goes on like $\tau= \langle e_2',e_3 \rangle = \langle e_2', \nu \rangle $ which is just definition and the fact we just proved, but now it is claimed that this is equal to $II(e_1,e_2).$ Then the author claims that  $K = Det(II)/Det(I) = II(e_1,e_1)II(e_2,e_2) - (II(e_1,e_2))^2 = 0 - \tau^2.$ Apparently, the determinant of the metric tensor is supposed to be equal to one here(why?). You find this proof in this book: It is theorem 3.19 on page 85: see here If anything is unclear, please let me know.","I have a question about a proof of the Beltrami-Enneper theorem: In the following $\nu$ is the surface-normal and $e_1,e_2,e_3$ the Frenet 3-frame. It states: Every asymptotic curve $c: I \rightarrow S \subset \mathbb{R}^3$  ( $II(c',c')=0$, where $II$ is the second fundamental form) with curvature $\kappa \neq 0$ and torsion $\tau$ satisfies the equation $\tau^2=-K$, where $\tau$ is the torsion of the curve and $K$ the Gauß-curvature of the surface $S$. Proof: Let $c(s)$ be an aymptotic curve with $II(c',c')=0$. Then the normal curvature of $c$ vanishes. Hence, $e_2$ is tangential to the surface (so far is everything alright), but now it goes on with, then $e_3 = \nu$.  I mean, I see that $e_3$ is then either $+ \nu$ or $- \nu$, but I don't see why the $+$ sign is already clear here. Then it goes on like $\tau= \langle e_2',e_3 \rangle = \langle e_2', \nu \rangle $ which is just definition and the fact we just proved, but now it is claimed that this is equal to $II(e_1,e_2).$ Then the author claims that  $K = Det(II)/Det(I) = II(e_1,e_1)II(e_2,e_2) - (II(e_1,e_2))^2 = 0 - \tau^2.$ Apparently, the determinant of the metric tensor is supposed to be equal to one here(why?). You find this proof in this book: It is theorem 3.19 on page 85: see here If anything is unclear, please let me know.",,"['real-analysis', 'analysis']"
81,Does a sequence of moments determine the function?,Does a sequence of moments determine the function?,,"Related questions and answers: Find a smooth function with prescribed moments When do equations represent the same curve? Consider a real valued integrable function $f(x)$ at the interval $a \le x \le b$ ; $x,a,b \in \mathbb{R}$ ; $ b > a$ . The moments of such a function are defined by: $$ M_0 = \int_a^b f(x)\,dx \\ M_1 = \int_a^b x \, f(x)\,dx \\ M_2 = \int_a^b x^2 \, f(x)\,dx \\ \cdots \\ M_k = \int_a^b x^k \, f(x)\,dx \\ \cdots $$ Note that the function $f$ is not said to be positive. Also note that the moments $M$ form an infinite sequence. Now the gist of the question is: can this problem be reversed? That is, given the infinite sequence $M_k$ , can $f(x)$ be solved - and how? - from: $$ \int_a^b f(x)\,dx = M_0\\ \int_a^b x \, f(x)\,dx = M_1\\ \int_a^b x^2 \, f(x)\,dx = M_2\\ \cdots \\ \int_a^b x^k \, f(x)\,dx = M_k\\ \cdots $$ Theorem. Without loss of generality, the domain of the function $f(x)$ may be restricted to $0 \le x \le 1$ and the range of the moments may be restricted to $0 \le M_k \le 1$ . The primary purpose of this Theorem is to make numerical experiments easier to accomplish. Proof. Let $t=(x-a)/(b-a)$ or $x=(b-a)t+a$ with $0 \le t \le 1$ and $g(t) = f((b-a)t+a)$ or $f(x) = g((x-a)/(b-a))$ . The moments $m_k$ of $g(t)$ are calculated by: $$ m_k = \int_0^1 t^k \, g(t) \, dt $$ It follows (with Newton's binomial) that: $$ M_k = \int_a^b x^k \, f(x)\,dx = \int_0^1 \left[(b-a)t+a\right]^k g(t) (b-a) \, dt = \sum_{i=0}^k \binom{k}{i} (b-a)^{i+1} a^{k-i} m_i $$ Or, the other way around: $$ m_k = \int_0^1 t^k \, g(t) \, dt = \int_a^b \left[\frac{x-a}{b-a}\right]^k f(x)/(b-a) \, dx = \frac{1}{(b-a)^{k+1}}\sum_{i=0}^k \binom{k}{i}(-a)^{k-i} M_i $$ Herewith the $f(x)$-moments $M_k$ can be converted into $g(t)$-moments $m_k$ and vice versa. Then maybe we can solve for a function $g(t)$ . But anyway we can do the backward transformation $f(x) = g((x-a)/(b-a))$ . This proves the first part of the Theorem. The moments $\mu_k$ of a function $h(t) = \lambda\, g(t) + C$ are calculated by: $$ \mu_k = \int_0^1 t^k \, h(t) \, dt = \lambda\, m_k + C/(k+1) $$  Suppose the smallest moment $\mu_k$ is found for $k=p$ and the largest moment is found for $k=q$ , then we can adapt the constants $\lambda$ and $C$ in such a way that all moments $\mu_k$ are between $0$ and $1$ : $$ \mu_p = \lambda\, m_p + C/(p+1) = 0 \quad ; \quad \mu_q = \lambda\, m_q + C/(q+1) = 1 \quad \Longleftrightarrow \\ \lambda = \frac{q+1}{m_q(q+1) - m_p(p+1)} \quad ; \quad C = - \frac{m_p(p+1)(q+1)}{m_q(q+1) - m_p(p+1)} $$ This proves the second part of the Theorem. But how to proceed with the (re)main(ing) part of the question: given $0 \le M_k \le 1$ , then - if possible - solve $f$ from $$ \int_0^1 f(x)\,dx = M_0\\ \int_0^1 x \, f(x)\,dx = M_1\\ \int_0^1 x^2 \, f(x)\,dx = M_2\\ \cdots \\ \int_0^1 x^k \, f(x)\,dx = M_k\\ \cdots $$ Note again that nothing has been said about the range of $f$ . Simplification (?) with help of the above Theorem is optional.","Related questions and answers: Find a smooth function with prescribed moments When do equations represent the same curve? Consider a real valued integrable function $f(x)$ at the interval $a \le x \le b$ ; $x,a,b \in \mathbb{R}$ ; $ b > a$ . The moments of such a function are defined by: $$ M_0 = \int_a^b f(x)\,dx \\ M_1 = \int_a^b x \, f(x)\,dx \\ M_2 = \int_a^b x^2 \, f(x)\,dx \\ \cdots \\ M_k = \int_a^b x^k \, f(x)\,dx \\ \cdots $$ Note that the function $f$ is not said to be positive. Also note that the moments $M$ form an infinite sequence. Now the gist of the question is: can this problem be reversed? That is, given the infinite sequence $M_k$ , can $f(x)$ be solved - and how? - from: $$ \int_a^b f(x)\,dx = M_0\\ \int_a^b x \, f(x)\,dx = M_1\\ \int_a^b x^2 \, f(x)\,dx = M_2\\ \cdots \\ \int_a^b x^k \, f(x)\,dx = M_k\\ \cdots $$ Theorem. Without loss of generality, the domain of the function $f(x)$ may be restricted to $0 \le x \le 1$ and the range of the moments may be restricted to $0 \le M_k \le 1$ . The primary purpose of this Theorem is to make numerical experiments easier to accomplish. Proof. Let $t=(x-a)/(b-a)$ or $x=(b-a)t+a$ with $0 \le t \le 1$ and $g(t) = f((b-a)t+a)$ or $f(x) = g((x-a)/(b-a))$ . The moments $m_k$ of $g(t)$ are calculated by: $$ m_k = \int_0^1 t^k \, g(t) \, dt $$ It follows (with Newton's binomial) that: $$ M_k = \int_a^b x^k \, f(x)\,dx = \int_0^1 \left[(b-a)t+a\right]^k g(t) (b-a) \, dt = \sum_{i=0}^k \binom{k}{i} (b-a)^{i+1} a^{k-i} m_i $$ Or, the other way around: $$ m_k = \int_0^1 t^k \, g(t) \, dt = \int_a^b \left[\frac{x-a}{b-a}\right]^k f(x)/(b-a) \, dx = \frac{1}{(b-a)^{k+1}}\sum_{i=0}^k \binom{k}{i}(-a)^{k-i} M_i $$ Herewith the $f(x)$-moments $M_k$ can be converted into $g(t)$-moments $m_k$ and vice versa. Then maybe we can solve for a function $g(t)$ . But anyway we can do the backward transformation $f(x) = g((x-a)/(b-a))$ . This proves the first part of the Theorem. The moments $\mu_k$ of a function $h(t) = \lambda\, g(t) + C$ are calculated by: $$ \mu_k = \int_0^1 t^k \, h(t) \, dt = \lambda\, m_k + C/(k+1) $$  Suppose the smallest moment $\mu_k$ is found for $k=p$ and the largest moment is found for $k=q$ , then we can adapt the constants $\lambda$ and $C$ in such a way that all moments $\mu_k$ are between $0$ and $1$ : $$ \mu_p = \lambda\, m_p + C/(p+1) = 0 \quad ; \quad \mu_q = \lambda\, m_q + C/(q+1) = 1 \quad \Longleftrightarrow \\ \lambda = \frac{q+1}{m_q(q+1) - m_p(p+1)} \quad ; \quad C = - \frac{m_p(p+1)(q+1)}{m_q(q+1) - m_p(p+1)} $$ This proves the second part of the Theorem. But how to proceed with the (re)main(ing) part of the question: given $0 \le M_k \le 1$ , then - if possible - solve $f$ from $$ \int_0^1 f(x)\,dx = M_0\\ \int_0^1 x \, f(x)\,dx = M_1\\ \int_0^1 x^2 \, f(x)\,dx = M_2\\ \cdots \\ \int_0^1 x^k \, f(x)\,dx = M_k\\ \cdots $$ Note again that nothing has been said about the range of $f$ . Simplification (?) with help of the above Theorem is optional.",,"['real-analysis', 'integration', 'sequences-and-series', 'functions', 'interpolation']"
82,A Base of a metric space intuition,A Base of a metric space intuition,,"From what I have read online and from what I have read in Rudin, a collection of open sets $\lbrace$$V_{n}$$\rbrace$ is said to be a base for a metric space $X$ if every open set in $X$ can be expressed as a union of a subcollection of $\lbrace$$V_{n}$$\rbrace$. In other words, I am taking it to mean that if you give me an open set in X, I can express it as the union of some of those sets. What will be some examples of bases for some Metric spaces? For instance, what's a basis for the set $0\cup$$\lbrace$$\frac{1}{n}$ : $n \in$ $\mathbb{N}$$\rbrace$? Can I write All neighborhoods with radius bigger than 0 as a basis? How about a basis for $\mathbb{R}$? Can I write all positive intervals or the same basis I just named above? I just need a few concrete examples of some bases to get the understanding I think.","From what I have read online and from what I have read in Rudin, a collection of open sets $\lbrace$$V_{n}$$\rbrace$ is said to be a base for a metric space $X$ if every open set in $X$ can be expressed as a union of a subcollection of $\lbrace$$V_{n}$$\rbrace$. In other words, I am taking it to mean that if you give me an open set in X, I can express it as the union of some of those sets. What will be some examples of bases for some Metric spaces? For instance, what's a basis for the set $0\cup$$\lbrace$$\frac{1}{n}$ : $n \in$ $\mathbb{N}$$\rbrace$? Can I write All neighborhoods with radius bigger than 0 as a basis? How about a basis for $\mathbb{R}$? Can I write all positive intervals or the same basis I just named above? I just need a few concrete examples of some bases to get the understanding I think.",,"['real-analysis', 'metric-spaces', 'intuition']"
83,Convergence of Integral near 0,Convergence of Integral near 0,,"I am trying to determine the convergence of the integral \begin{equation} \int_0^1 \frac{f(x)}{x}\, dx \end{equation} given that $f(x)$ is bounded and continuous on $[0,1]$, and that $f(x)=0$.  The boundedness is just so that the question of convergence is only at the point $x=0$.  I specifically want $f$ to be only continuous on $[0,1]$ and not differentiable in a neighborhood of the origin as I could just use a Taylor expansion of $f$ to solve the problem then. I believe that the integral should converge but I can't figure out exactly how to write it down.  Since $1/x$ is the critical exponent of convergence near $0$ it seems that multiplying $1/x$ by any function which vanishes at the origin should be enough to make the integral converge.  More concretely, if $f(x)=x^{1/n} log(x)^m$ then $\lim_{x \to 0} f(x)=0$ for all positive values of $n$ and $m$, and $\int_0^1 f(x)/x \, dx < \infty$.  The derivative of these $f$ become infinite as $x\to 0$, and at faster rates for larger $m$ and $n$, so they are good candidates for $\int f(x)/x$ to not converge, yet the integral still converges. Any suggestions for a proof, or a counterexample to show the integral does not always converge would be much appreciated.","I am trying to determine the convergence of the integral \begin{equation} \int_0^1 \frac{f(x)}{x}\, dx \end{equation} given that $f(x)$ is bounded and continuous on $[0,1]$, and that $f(x)=0$.  The boundedness is just so that the question of convergence is only at the point $x=0$.  I specifically want $f$ to be only continuous on $[0,1]$ and not differentiable in a neighborhood of the origin as I could just use a Taylor expansion of $f$ to solve the problem then. I believe that the integral should converge but I can't figure out exactly how to write it down.  Since $1/x$ is the critical exponent of convergence near $0$ it seems that multiplying $1/x$ by any function which vanishes at the origin should be enough to make the integral converge.  More concretely, if $f(x)=x^{1/n} log(x)^m$ then $\lim_{x \to 0} f(x)=0$ for all positive values of $n$ and $m$, and $\int_0^1 f(x)/x \, dx < \infty$.  The derivative of these $f$ become infinite as $x\to 0$, and at faster rates for larger $m$ and $n$, so they are good candidates for $\int f(x)/x$ to not converge, yet the integral still converges. Any suggestions for a proof, or a counterexample to show the integral does not always converge would be much appreciated.",,"['real-analysis', 'integration', 'improper-integrals']"
84,Is every such function convex or concave?,Is every such function convex or concave?,,"I was pondering convex functions today, and the following questions naturally posed themselves. Call a function $f : \mathbb{R} \rightarrow \mathbb{R}$ $2$-limited iff for all $m,c \in \mathbb{R},$ the cardinality of $\{x \in \mathbb{R} \mid f(x)=mx+c\}$ is at most $2.$ Question 0. Is every continuous 2-limited function $f : \mathbb{R} \rightarrow \mathbb{R}$ convex or concave? Question 1. Assuming the answer to the above question is ""yes,"" does there exist a (necessarily discontinuous) $2$-limited function $f : \mathbb{R} \rightarrow \mathbb{R}$ that is neither convex nor concave?","I was pondering convex functions today, and the following questions naturally posed themselves. Call a function $f : \mathbb{R} \rightarrow \mathbb{R}$ $2$-limited iff for all $m,c \in \mathbb{R},$ the cardinality of $\{x \in \mathbb{R} \mid f(x)=mx+c\}$ is at most $2.$ Question 0. Is every continuous 2-limited function $f : \mathbb{R} \rightarrow \mathbb{R}$ convex or concave? Question 1. Assuming the answer to the above question is ""yes,"" does there exist a (necessarily discontinuous) $2$-limited function $f : \mathbb{R} \rightarrow \mathbb{R}$ that is neither convex nor concave?",,"['real-analysis', 'convex-analysis', 'examples-counterexamples']"
85,Product of Lebesgue and counting measures,Product of Lebesgue and counting measures,,"Let $\mathbb R$ be endowed with the standard Euclidean topology and let $\widetilde {\mathbb R}$ denote the line endowed with the discrete topology. Let $\mu$ and $\nu$ denote the Lebesgue and counting measures on these two spaces, respectively. Suppose that $E\in\mathscr B_{\mathbb R\times\widetilde{\mathbb R}}$ is a Borel-measurable set on the product space. For each $y\in\widetilde{\mathbb R}$, define $$E^y\equiv\{x\in\mathbb R\,|\,(x,y)\in E\}.$$ Assume also that $$G\equiv\{y\in\widetilde{\mathbb R}\,|\,\mu(E^y)>0\}$$ is a countable set and also that $$\sum_{y\in G}\mu(E^y)<\infty.$$ I am trying to prove the following: $$\boxed{(\mu\times\nu)(E)=\sum_{y\in G}\mu(E^y),}\tag{*}$$ where $$(\mu\times\nu)(E)\equiv\inf\left\{\sum_{k=1}^{\infty}\mu(A_k)\cdot \nu(B_k)\,\Bigg|\,E\subseteq\bigcup_{k=1}^{\infty}A_k\times B_k,\,A_k\in\mathscr B_{\mathbb R},\,B_k\subseteq\widetilde{\mathbb R}\,\forall k\right\}\tag{**}$$ is the product measure on $\mathscr B_{\mathbb R}\otimes\mathscr B_{\widetilde{\mathbb R}}=\mathscr B_{\mathbb R\times\widetilde{\mathbb R}}$. (Remark: I have already proved that these latter two $\sigma$-algebras coincide and also that $E^y\in\mathscr B_{\mathbb R}$ for any $y\in\widetilde{\mathbb R}$). Attempts: It is not difficult to show that $(\mu\times\nu)(E)\geq\sum_{y\in G}\mu(E^y)$, given that $$E=\bigcup_{y\in\widetilde{\mathbb R}}E^y\times\{y\}\tag{***}$$ and this union is disjoint. As for the other direction, note that $\{E^y\times\{y\}\}_{y\in\widetilde{\mathbb R}}$ cover $E$ according to $(***)$, so one could use the infimum property of the product measure $\mu\times\nu$ in $(**)$ to establish the desired inequality. The problem is that this union is uncountable (even though only countably many $y\in\widetilde{\mathbb R}$ have $m(E^y)>0$), so it may not be a suitable cover based on which $(**)$ could be made use of. Any hints/thoughts are appreciated. UPDATE #1: Possible counterexample: Let $E\equiv\{(x,x)\,|\,x\in[0,1]\}$. Clearly, $E^y=\{y\}$ if $y\in[0,1]$ and $\varnothing$ otherwise. It is clear also that $E\in\mathscr B_{\mathbb R}\otimes\mathscr B_{\widetilde{\mathbb R}}$. It follows that $G=\varnothing$ and the right-hand side of $(*)$ vanishes. However, it is well-known that $(\mu\times\nu)(E)=\infty$ whenever $\widetilde{\mathbb R}$ is endowed with the Borel $\sigma$-algebra and the counting measure . In the present setting, though, $\widetilde{\mathbb R}$ is endowed with the discrete topology and the corresponding $\sigma$-algebra $2^{\widetilde{\mathbb R}}$! Hence, there may still be some hope that the left-hand side of $(*)$ vanishes, too; to do this, one must cover $E$ with a countable union of rectangles of the form $(A_k,B_k)\in\mathscr B_{\mathbb R}\times 2^{\widetilde{\mathbb R}}$ the measures of whose products are small. To achieve the desired result, at least some of $B_k$'s must be non-Borel-measurable (say, cleverly chosen Vitali sets), in order to avoid the conclusion that $(\mu\times\nu)(E)=\infty$. Is such a choice of non-Borel sets possible, or is it the case that $(\mu\times\nu)(E)=\infty$ with the discrete topology, too? UPDATE #2: The result should be true. According to Bogachev (2007) (Example 7.14.65, pp. 154–155), the measure $$E\mapsto\sum_{y\in\widetilde{\mathbb R}}\mu\left(E^y\right)$$ coincides with $(\mu\times\nu)$, as defined above in $(*)$, but this result is mentioned only; no proof is provided. If this is true, then the possible counterexample is obviated, too, and the diagonal of $[0,1]\times[0,1]$ should have a vanishing product measure.","Let $\mathbb R$ be endowed with the standard Euclidean topology and let $\widetilde {\mathbb R}$ denote the line endowed with the discrete topology. Let $\mu$ and $\nu$ denote the Lebesgue and counting measures on these two spaces, respectively. Suppose that $E\in\mathscr B_{\mathbb R\times\widetilde{\mathbb R}}$ is a Borel-measurable set on the product space. For each $y\in\widetilde{\mathbb R}$, define $$E^y\equiv\{x\in\mathbb R\,|\,(x,y)\in E\}.$$ Assume also that $$G\equiv\{y\in\widetilde{\mathbb R}\,|\,\mu(E^y)>0\}$$ is a countable set and also that $$\sum_{y\in G}\mu(E^y)<\infty.$$ I am trying to prove the following: $$\boxed{(\mu\times\nu)(E)=\sum_{y\in G}\mu(E^y),}\tag{*}$$ where $$(\mu\times\nu)(E)\equiv\inf\left\{\sum_{k=1}^{\infty}\mu(A_k)\cdot \nu(B_k)\,\Bigg|\,E\subseteq\bigcup_{k=1}^{\infty}A_k\times B_k,\,A_k\in\mathscr B_{\mathbb R},\,B_k\subseteq\widetilde{\mathbb R}\,\forall k\right\}\tag{**}$$ is the product measure on $\mathscr B_{\mathbb R}\otimes\mathscr B_{\widetilde{\mathbb R}}=\mathscr B_{\mathbb R\times\widetilde{\mathbb R}}$. (Remark: I have already proved that these latter two $\sigma$-algebras coincide and also that $E^y\in\mathscr B_{\mathbb R}$ for any $y\in\widetilde{\mathbb R}$). Attempts: It is not difficult to show that $(\mu\times\nu)(E)\geq\sum_{y\in G}\mu(E^y)$, given that $$E=\bigcup_{y\in\widetilde{\mathbb R}}E^y\times\{y\}\tag{***}$$ and this union is disjoint. As for the other direction, note that $\{E^y\times\{y\}\}_{y\in\widetilde{\mathbb R}}$ cover $E$ according to $(***)$, so one could use the infimum property of the product measure $\mu\times\nu$ in $(**)$ to establish the desired inequality. The problem is that this union is uncountable (even though only countably many $y\in\widetilde{\mathbb R}$ have $m(E^y)>0$), so it may not be a suitable cover based on which $(**)$ could be made use of. Any hints/thoughts are appreciated. UPDATE #1: Possible counterexample: Let $E\equiv\{(x,x)\,|\,x\in[0,1]\}$. Clearly, $E^y=\{y\}$ if $y\in[0,1]$ and $\varnothing$ otherwise. It is clear also that $E\in\mathscr B_{\mathbb R}\otimes\mathscr B_{\widetilde{\mathbb R}}$. It follows that $G=\varnothing$ and the right-hand side of $(*)$ vanishes. However, it is well-known that $(\mu\times\nu)(E)=\infty$ whenever $\widetilde{\mathbb R}$ is endowed with the Borel $\sigma$-algebra and the counting measure . In the present setting, though, $\widetilde{\mathbb R}$ is endowed with the discrete topology and the corresponding $\sigma$-algebra $2^{\widetilde{\mathbb R}}$! Hence, there may still be some hope that the left-hand side of $(*)$ vanishes, too; to do this, one must cover $E$ with a countable union of rectangles of the form $(A_k,B_k)\in\mathscr B_{\mathbb R}\times 2^{\widetilde{\mathbb R}}$ the measures of whose products are small. To achieve the desired result, at least some of $B_k$'s must be non-Borel-measurable (say, cleverly chosen Vitali sets), in order to avoid the conclusion that $(\mu\times\nu)(E)=\infty$. Is such a choice of non-Borel sets possible, or is it the case that $(\mu\times\nu)(E)=\infty$ with the discrete topology, too? UPDATE #2: The result should be true. According to Bogachev (2007) (Example 7.14.65, pp. 154–155), the measure $$E\mapsto\sum_{y\in\widetilde{\mathbb R}}\mu\left(E^y\right)$$ coincides with $(\mu\times\nu)$, as defined above in $(*)$, but this result is mentioned only; no proof is provided. If this is true, then the possible counterexample is obviated, too, and the diagonal of $[0,1]\times[0,1]$ should have a vanishing product measure.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
86,"$\int_0^1 (f(x))^n =$ constant, $f\geq 0$, then $f$ is a characteristic function of a measurable set.","constant, , then  is a characteristic function of a measurable set.",\int_0^1 (f(x))^n = f\geq 0 f,"$\int_0^1 (f(x))^n =$ constant, $f\geq 0$, then $f$ is a characteristic function of a measurable set. This is the result from question part (a). Now for part (b), will it also hold when the assumption $f\geq0$ is removed? Here are my thoughts. Since the result from part (a) holds for $f^2 \geq 0$, then $f^2$ is a characteristic function of a measurable set. Now if $f$ takes the value $-1$ on a set of positive measure, $\int_0^1 (f(x))^n$ would not be constant for even and odd $n$. Is this correct? Thanks!","$\int_0^1 (f(x))^n =$ constant, $f\geq 0$, then $f$ is a characteristic function of a measurable set. This is the result from question part (a). Now for part (b), will it also hold when the assumption $f\geq0$ is removed? Here are my thoughts. Since the result from part (a) holds for $f^2 \geq 0$, then $f^2$ is a characteristic function of a measurable set. Now if $f$ takes the value $-1$ on a set of positive measure, $\int_0^1 (f(x))^n$ would not be constant for even and odd $n$. Is this correct? Thanks!",,"['real-analysis', 'proof-verification']"
87,Does this arithmetic operation have a name,Does this arithmetic operation have a name,,"I've came across the following product-like operation on reals $$   a\times b:=1 - (1-a)(1-b). $$ This operation is commutative, associative and has $1$ as a zero element and $0$ as a unit element: $$   a\times 1 = 1, \qquad a\times 0  =a. $$ Perhaps there is even a version of addition such that $a\times b$ becomes distributive. Has this operation been studied somewhere?","I've came across the following product-like operation on reals $$   a\times b:=1 - (1-a)(1-b). $$ This operation is commutative, associative and has $1$ as a zero element and $0$ as a unit element: $$   a\times 1 = 1, \qquad a\times 0  =a. $$ Perhaps there is even a version of addition such that $a\times b$ becomes distributive. Has this operation been studied somewhere?",,"['real-analysis', 'abstract-algebra']"
88,"There exist $x_1, x_2, x_3$ such that $\frac{1}{f'(x_1)} + \frac{1}{f'(x_2)} + \frac{1}{f'(x_3)} = 3$",There exist  such that,"x_1, x_2, x_3 \frac{1}{f'(x_1)} + \frac{1}{f'(x_2)} + \frac{1}{f'(x_3)} = 3","Let $f$ be a real-valued function defined in $[a, b] \subset \mathbb{R}$, with $f(a) = a, f(b) = b$. Suppose that $f$ is continuous on $[a, b]$ and differentiable on $(a, b)$. Show that there exist three distinct points $x_1, x_2, x_3$ such that $$\frac{1}{f'(x_1)} + \frac{1}{f'(x_2)} + \frac{1}{f'(x_3)} = 3$$ My hunch is to use the mean-value theorem or Rolle's theorem somehow. But these theorems only guarantee the existence of a certain point, and not a triple of points, so I am stuck.","Let $f$ be a real-valued function defined in $[a, b] \subset \mathbb{R}$, with $f(a) = a, f(b) = b$. Suppose that $f$ is continuous on $[a, b]$ and differentiable on $(a, b)$. Show that there exist three distinct points $x_1, x_2, x_3$ such that $$\frac{1}{f'(x_1)} + \frac{1}{f'(x_2)} + \frac{1}{f'(x_3)} = 3$$ My hunch is to use the mean-value theorem or Rolle's theorem somehow. But these theorems only guarantee the existence of a certain point, and not a triple of points, so I am stuck.",,['real-analysis']
89,Set of subsequential limits of a bounded sequence,Set of subsequential limits of a bounded sequence,,"Let $\{x_n\}_{n=1}^{\infty}$ be a bounded sequence, and let $E$ be the set of subsequential limits of that sequence. Prove that $E$ is bounded and contains both sup$E$ and inf$E$. Here is my sad attempt: Let $\{x_n\}_{n=1}^{\infty}$ be a bounded sequence, and let $E$ be a set of subsequential limits of that sequence. $E$ is not empty since every bounded sequence of real numbers has at least one convergent subsequence. Suppose $\{x_n\}_{n=1}^{\infty}$ converges to a real number $L$. Then $E=\{L\}$, since every subsequence of $\{x_n\}_{n=1}^{\infty}$ must also converge to $L$. Therefore, $E$ is bounded and sup$E$=inf$E$=$L\in E$. Suppose $\{x_n\}_{n=1}^{\infty}$ diverges . As stated above, there is at least one convergent subsequence, so $E$ is not empty. Let $S$ be the range of $\{x_n\}_{n=1}^{\infty}$. Then $S$ is bounded since the sequence is bounded Suppose $S$ is finite; that is, $S=\{x_1,...,x_k\}$ for $k\in\mathbb{N}$. Then there exists a term $x_i$ with $1\leq i\leq k$, of $\{x_n\}_{n=1}^{\infty}$ that repeats infinitely many times, and thus the subsequence $\{x_i\}_{n=1}^{\infty}$ converges to $x_i$. This implies that, $E\subset S$, which also implies that $E$ is bounded, since $S$ is bounded. Furthermore, since $S$ is finite, then $S$ has a smallest and largest element and so does $E$. Hence, inf$E=$ min$E$ and sup$E=$ max$E$, which are both contained in $E$. Suppose $S$ is infinite. Then, by the Bolzano-Weierstrass Theorem, there is at least one accumulation point, $a_1$, and thus, at least one subsequence that converges to $a_1$. Let $A$ be the set of all accumulation points of $S$. Then $E\subset A$, since for each member $a_i$ of $A$ there exists a subsequence of $\{x_n\}_{n=1}^{\infty}$ that converges to $a_i$. Also, notice that since $S$ is bounded, then there exists $M_1,M_2\in\mathbb{R}$, with $M_1<M_2$, such that for all $x\in S$, $x\in(M_1,M_2)$. If $A$ is finite, then $A$ must have a smallest and a largest element, which implies that $E$ has a smallest and largest element. So, inf$E=$ min$E$ and sup$E=$ max$E$, which are both contained in $E$, which also implies that $E$ is bounded. Suppose $A$ is infinite. Every element of $S$ being contained in $(M_1,M_2)$ implies that every element of $A$ is contained in $[M_1,M_2]$. Thus, since $E\subset A$, $E$ is bounded. We also see that $A$ has a smallest and largest element. Ok, that's as far as I can go. All I need to show now is that inf$E$ and sup$E$ are contained in $E$, but I just can't seem to figure that out. I'm sure that there is probably a better way to approach this whole proof. If you have any advice on this, please share. Thank you in advance for any and all assistance.","Let $\{x_n\}_{n=1}^{\infty}$ be a bounded sequence, and let $E$ be the set of subsequential limits of that sequence. Prove that $E$ is bounded and contains both sup$E$ and inf$E$. Here is my sad attempt: Let $\{x_n\}_{n=1}^{\infty}$ be a bounded sequence, and let $E$ be a set of subsequential limits of that sequence. $E$ is not empty since every bounded sequence of real numbers has at least one convergent subsequence. Suppose $\{x_n\}_{n=1}^{\infty}$ converges to a real number $L$. Then $E=\{L\}$, since every subsequence of $\{x_n\}_{n=1}^{\infty}$ must also converge to $L$. Therefore, $E$ is bounded and sup$E$=inf$E$=$L\in E$. Suppose $\{x_n\}_{n=1}^{\infty}$ diverges . As stated above, there is at least one convergent subsequence, so $E$ is not empty. Let $S$ be the range of $\{x_n\}_{n=1}^{\infty}$. Then $S$ is bounded since the sequence is bounded Suppose $S$ is finite; that is, $S=\{x_1,...,x_k\}$ for $k\in\mathbb{N}$. Then there exists a term $x_i$ with $1\leq i\leq k$, of $\{x_n\}_{n=1}^{\infty}$ that repeats infinitely many times, and thus the subsequence $\{x_i\}_{n=1}^{\infty}$ converges to $x_i$. This implies that, $E\subset S$, which also implies that $E$ is bounded, since $S$ is bounded. Furthermore, since $S$ is finite, then $S$ has a smallest and largest element and so does $E$. Hence, inf$E=$ min$E$ and sup$E=$ max$E$, which are both contained in $E$. Suppose $S$ is infinite. Then, by the Bolzano-Weierstrass Theorem, there is at least one accumulation point, $a_1$, and thus, at least one subsequence that converges to $a_1$. Let $A$ be the set of all accumulation points of $S$. Then $E\subset A$, since for each member $a_i$ of $A$ there exists a subsequence of $\{x_n\}_{n=1}^{\infty}$ that converges to $a_i$. Also, notice that since $S$ is bounded, then there exists $M_1,M_2\in\mathbb{R}$, with $M_1<M_2$, such that for all $x\in S$, $x\in(M_1,M_2)$. If $A$ is finite, then $A$ must have a smallest and a largest element, which implies that $E$ has a smallest and largest element. So, inf$E=$ min$E$ and sup$E=$ max$E$, which are both contained in $E$, which also implies that $E$ is bounded. Suppose $A$ is infinite. Every element of $S$ being contained in $(M_1,M_2)$ implies that every element of $A$ is contained in $[M_1,M_2]$. Thus, since $E\subset A$, $E$ is bounded. We also see that $A$ has a smallest and largest element. Ok, that's as far as I can go. All I need to show now is that inf$E$ and sup$E$ are contained in $E$, but I just can't seem to figure that out. I'm sure that there is probably a better way to approach this whole proof. If you have any advice on this, please share. Thank you in advance for any and all assistance.",,"['real-analysis', 'sequences-and-series']"
90,About $\sum \sin(\kappa \sin (\kappa \sin(\cdots \sin \kappa x)\cdots)$,About,\sum \sin(\kappa \sin (\kappa \sin(\cdots \sin \kappa x)\cdots),"I have proved that, given $|\kappa|<1$, the function: $$f(x)=\sum_{n\geqslant 1} \underbrace{\sin(\kappa \sin (\kappa \sin (\cdots \sin}_n\, \kappa x)\cdots)$$ Is defined for all real $x$ (the series converges everywhere). My question is, what does the function look like? I don't have the software to plot it, and I can't figure it out on paper. Sorry if the question is a little vague.","I have proved that, given $|\kappa|<1$, the function: $$f(x)=\sum_{n\geqslant 1} \underbrace{\sin(\kappa \sin (\kappa \sin (\cdots \sin}_n\, \kappa x)\cdots)$$ Is defined for all real $x$ (the series converges everywhere). My question is, what does the function look like? I don't have the software to plot it, and I can't figure it out on paper. Sorry if the question is a little vague.",,"['calculus', 'real-analysis']"
91,"I would like to show that a dense set in $[0,1]$ is not equal to $[0,1]$ without measure theory",I would like to show that a dense set in  is not equal to  without measure theory,"[0,1] [0,1]","Consider the following set $$ I_{n,j}=[\frac{n}{j}-\frac{1}{4^{n+j}},\frac{n}{j}+\frac{1}{4^{n+j}}]$$ for some integers $n$ and $j$. Now let  $$A:=\bigcup_{n\geq 1}\bigcup_{j\geq1}I_{n,j}$$ The goal of some exercise I was working on was to show that $A$ is dense in $[0,1]$ and in next step to show that $[0,1]\setminus A\neq \emptyset$. The first part follows directly as rationals are dense in $\mathbb R$ and  I managed to show the second part by using that the Lebesgue-measure of $A$ is strictly smaller then 1. And so the question appeared  if it is possible to show $[0,1]\setminus A\neq \emptyset$ without measure theory and in particular if it is possible to find an explicit element in $[0,1]$ which is not in $A$. I would appreciate any help. Thanks in advance!","Consider the following set $$ I_{n,j}=[\frac{n}{j}-\frac{1}{4^{n+j}},\frac{n}{j}+\frac{1}{4^{n+j}}]$$ for some integers $n$ and $j$. Now let  $$A:=\bigcup_{n\geq 1}\bigcup_{j\geq1}I_{n,j}$$ The goal of some exercise I was working on was to show that $A$ is dense in $[0,1]$ and in next step to show that $[0,1]\setminus A\neq \emptyset$. The first part follows directly as rationals are dense in $\mathbb R$ and  I managed to show the second part by using that the Lebesgue-measure of $A$ is strictly smaller then 1. And so the question appeared  if it is possible to show $[0,1]\setminus A\neq \emptyset$ without measure theory and in particular if it is possible to find an explicit element in $[0,1]$ which is not in $A$. I would appreciate any help. Thanks in advance!",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'real-numbers']"
92,Square of a series,Square of a series,,"Suppose that $\sum_{j=0}^\infty a_j$ converges and $\sum_{j=0}^\infty a_j^2<\infty$, where $a_j\in\mathbb R$, $j\ge0$. I would like to prove that   $$ \biggl[\sum_{j=0}^\infty a_j\biggr]^2 =\sum_{j=0}^\infty a_j^2+2\sum_{k=1}^\infty\sum_{j=0}^\infty a_ja_{j+k}. $$ Using continuity and squaring the sum, we obtain $$ \biggl[\sum_{j=0}^\infty a_j\biggr]^2=\lim_{n\to\infty}\biggl[\sum_{j=0}^na_j\biggr]^2=\sum_{j=0}^\infty a_j^2+2\lim_{n\to\infty}\sum_{j=0}^{n-1}\sum_{k=1}^{n-j}a_ja_{j+k}. $$ The proof would be complete if I could justify the following two equalities: $\lim_{n\to\infty}\sum_{j=0}^{n-1}\sum_{k=1}^{n-j}a_ja_{j+k}=\sum_{j=0}^\infty\sum_{k=1}^\infty a_ja_{j+k}$; $\sum_{j=0}^\infty\sum_{k=1}^\infty a_ja_{j+k}=\sum_{k=1}^\infty\sum_{j=0}^\infty a_ja_{j+k}$. Are these equalities true? How can I justify them if they're true? Or should the proof be completely different? Any help is much appreciated!","Suppose that $\sum_{j=0}^\infty a_j$ converges and $\sum_{j=0}^\infty a_j^2<\infty$, where $a_j\in\mathbb R$, $j\ge0$. I would like to prove that   $$ \biggl[\sum_{j=0}^\infty a_j\biggr]^2 =\sum_{j=0}^\infty a_j^2+2\sum_{k=1}^\infty\sum_{j=0}^\infty a_ja_{j+k}. $$ Using continuity and squaring the sum, we obtain $$ \biggl[\sum_{j=0}^\infty a_j\biggr]^2=\lim_{n\to\infty}\biggl[\sum_{j=0}^na_j\biggr]^2=\sum_{j=0}^\infty a_j^2+2\lim_{n\to\infty}\sum_{j=0}^{n-1}\sum_{k=1}^{n-j}a_ja_{j+k}. $$ The proof would be complete if I could justify the following two equalities: $\lim_{n\to\infty}\sum_{j=0}^{n-1}\sum_{k=1}^{n-j}a_ja_{j+k}=\sum_{j=0}^\infty\sum_{k=1}^\infty a_ja_{j+k}$; $\sum_{j=0}^\infty\sum_{k=1}^\infty a_ja_{j+k}=\sum_{k=1}^\infty\sum_{j=0}^\infty a_ja_{j+k}$. Are these equalities true? How can I justify them if they're true? Or should the proof be completely different? Any help is much appreciated!",,"['real-analysis', 'sequences-and-series']"
93,Monotonic function satisfying darboux property $\Rightarrow$ continuous,Monotonic function satisfying darboux property  continuous,\Rightarrow,"Assume $f : I \rightarrow \mathbb{R}$ is a non-decreasing on an open interval $I$ and that $f$ satisfies the Intermediate value property or Darboux's property on $I$ (that is, for any $a < b$ in $I$ and any $L$ between $f(a)$ and $f(b)$ , there exists $c \in [a, b]$ such that $ f (c) = L)$ . Then, prove that $f$ is continuous. However, I know that a function can be discontinuous and also satisfy the IVT at the same time. Could someone point me in the right direction?","Assume is a non-decreasing on an open interval and that satisfies the Intermediate value property or Darboux's property on (that is, for any in and any between and , there exists such that . Then, prove that is continuous. However, I know that a function can be discontinuous and also satisfy the IVT at the same time. Could someone point me in the right direction?","f : I \rightarrow \mathbb{R} I f I a < b I L f(a) f(b) c \in [a, b]  f (c) = L) f","['real-analysis', 'functions', 'continuity']"
94,"$f(x,y)$ is such that partial derivative w.r.t $x$ is zero, but$ f$ still depends on $x$?","is such that partial derivative w.r.t  is zero, but still depends on ?","f(x,y) x  f x","I have a problem where it seems like I should be able to visualize an answer, but I can't.  Perhaps I need to take a more formal approach. ""Let $A$ be a non-empty open convex subset of $\mathbb{R}^2$, and suppose that $f: A \rightarrow \mathbb{R}$ satisfies that $\frac{\partial f}{\partial x} = 0$ at all points in $A$.  Prove that there is a function $g$ of one variable such that $f(x,y) = g(y)$.  Show that this conclusion may fail if convexity is replaced by connectedness."" For the first part, maybe I can do something like the following: When $A$ is convex, I can take any points $\boldsymbol{a_0}, \boldsymbol{a_1} \in A$, where $\boldsymbol{a_0} = (x_0, y_0), \boldsymbol{a_1} = (x_0 + h, y_0)$, and have the line segment connecting them be in $A$. Since $\frac{\partial f}{\partial x} = 0$ everywhere on this line segment, some mean value business shows that $f(\boldsymbol{a_0}) = f(\boldsymbol{a_1})$, regardless of the value of $h$.  Therefore, at any given $y=y_0$, $f$ does not depend on $x$, so $f(x,y) = g(y)$ only. Obviously I used convexity of $A$ to show this, but I can't seem to find a counterexample that makes it fail when $A$ is connected but not convex.  If $A$ were not even not connected, so it was separable into disjoint open sets $A_1$ and $A_2$, it would be easy to see that $f$ could have different constant values w.r.t. $x$ in each separate region.  This separation could allow $f$ to depend on $x$ overall even though $\frac{\partial f}{\partial x} = 0$ where ever $f$ is defined. It's the connected but not convex case that's troublesome. The more I think about it, the less sensible it sounds. If we want to show that $f(x,y)$ is not just a function of $y$, we should be able to find some $\boldsymbol{a_0}, \boldsymbol{a_1} \in A$, as above (having the same $y$ values but different $x$ values) such that $f(\boldsymbol{a_0}) \neq f(\boldsymbol{a_1})$.  We suppose $A$ is connected.  Since $\frac{\partial f}{\partial x} = 0$ everywhere in $A$, it seems like any curve in $f(A)$ that connects those points has to lie in a plane parallel to the $xy$ plane, else $f$ would change with changing $x$.  But if that were so, how could we have $f(\boldsymbol{a_0}) \neq f(\boldsymbol{a_1})$? Clearly my intuition is failing me here.  Could somebody set me straight? This is my first post on Math Exchange, please let me know if I'm doing something wrong.  Thanks!","I have a problem where it seems like I should be able to visualize an answer, but I can't.  Perhaps I need to take a more formal approach. ""Let $A$ be a non-empty open convex subset of $\mathbb{R}^2$, and suppose that $f: A \rightarrow \mathbb{R}$ satisfies that $\frac{\partial f}{\partial x} = 0$ at all points in $A$.  Prove that there is a function $g$ of one variable such that $f(x,y) = g(y)$.  Show that this conclusion may fail if convexity is replaced by connectedness."" For the first part, maybe I can do something like the following: When $A$ is convex, I can take any points $\boldsymbol{a_0}, \boldsymbol{a_1} \in A$, where $\boldsymbol{a_0} = (x_0, y_0), \boldsymbol{a_1} = (x_0 + h, y_0)$, and have the line segment connecting them be in $A$. Since $\frac{\partial f}{\partial x} = 0$ everywhere on this line segment, some mean value business shows that $f(\boldsymbol{a_0}) = f(\boldsymbol{a_1})$, regardless of the value of $h$.  Therefore, at any given $y=y_0$, $f$ does not depend on $x$, so $f(x,y) = g(y)$ only. Obviously I used convexity of $A$ to show this, but I can't seem to find a counterexample that makes it fail when $A$ is connected but not convex.  If $A$ were not even not connected, so it was separable into disjoint open sets $A_1$ and $A_2$, it would be easy to see that $f$ could have different constant values w.r.t. $x$ in each separate region.  This separation could allow $f$ to depend on $x$ overall even though $\frac{\partial f}{\partial x} = 0$ where ever $f$ is defined. It's the connected but not convex case that's troublesome. The more I think about it, the less sensible it sounds. If we want to show that $f(x,y)$ is not just a function of $y$, we should be able to find some $\boldsymbol{a_0}, \boldsymbol{a_1} \in A$, as above (having the same $y$ values but different $x$ values) such that $f(\boldsymbol{a_0}) \neq f(\boldsymbol{a_1})$.  We suppose $A$ is connected.  Since $\frac{\partial f}{\partial x} = 0$ everywhere in $A$, it seems like any curve in $f(A)$ that connects those points has to lie in a plane parallel to the $xy$ plane, else $f$ would change with changing $x$.  But if that were so, how could we have $f(\boldsymbol{a_0}) \neq f(\boldsymbol{a_1})$? Clearly my intuition is failing me here.  Could somebody set me straight? This is my first post on Math Exchange, please let me know if I'm doing something wrong.  Thanks!",,['real-analysis']
95,Integral formula for polar coordinates,Integral formula for polar coordinates,,"The polar coordinates of point $x \in \mathbb{R} \setminus \{0\}$ are pairs $(r,\gamma)$, where $0 < r < \infty$ and $\gamma \in S^{d-1} = \{x \in \mathbb{R}^{d}\mid |x| = 1\}$. These are determined by $$r = |x|, \quad\gamma = x/|x|,$$ and reciprocally by $x = r\gamma$. Then we have: $$\int_{\mathbb{R}^{d}}f(x)dx = \int_{S^{d-1}} \left( \int_{0}^{\infty}f(r\gamma)r^{d-1}dr \right) d\sigma(\gamma).$$ The proof of this formula using the Fubini's theorem. However, I can't understand that the relationship between those measure spaces. And I was so confused by this equation: $$\mu_{1}(E) = \int_{E}r^{d-1}dr.$$ Besides, I also want to know about the integral formula for the general case. Is there a universal steps to construct the integral formula with respect to the Jacobi in the Reimann integral.","The polar coordinates of point $x \in \mathbb{R} \setminus \{0\}$ are pairs $(r,\gamma)$, where $0 < r < \infty$ and $\gamma \in S^{d-1} = \{x \in \mathbb{R}^{d}\mid |x| = 1\}$. These are determined by $$r = |x|, \quad\gamma = x/|x|,$$ and reciprocally by $x = r\gamma$. Then we have: $$\int_{\mathbb{R}^{d}}f(x)dx = \int_{S^{d-1}} \left( \int_{0}^{\infty}f(r\gamma)r^{d-1}dr \right) d\sigma(\gamma).$$ The proof of this formula using the Fubini's theorem. However, I can't understand that the relationship between those measure spaces. And I was so confused by this equation: $$\mu_{1}(E) = \int_{E}r^{d-1}dr.$$ Besides, I also want to know about the integral formula for the general case. Is there a universal steps to construct the integral formula with respect to the Jacobi in the Reimann integral.",,"['real-analysis', 'polar-coordinates']"
96,Showing that $\|f\|_p\to\|f\|_{\infty}$,Showing that,\|f\|_p\to\|f\|_{\infty},"I know this question has been asked a lot in this site, I've been checking those questions myself, however according to the theory we use in class there are things that I can't use and/or I don't know how to use (for example, we don't work with probability spaces, and other things that are very measure theory). So this is another version of this same old problem. If $|\Omega|<\infty$, $f\in L^{\infty}(\Omega)$, then $$\|f\|_{\infty}=\lim_{p\to{\infty}}|\Omega|^{-1/p}\|f\|_p$$ I see, that this problem really is like this one, since is easy to see that $\lim_{p\to\infty}|\Omega|^{-1/p}=1$. Now, we are given two hints: Prove that: (a) $|\Omega|^{-1/p}\|f\|_p\le\|f\|_{\infty},\forall p$. (b) If $\varepsilon>0$ then $|\Omega|^{-1/p}\|f\|_p\ge\|f\|_{\infty}-\varepsilon$ for $p$ sufficiently big. For the first one, it comes easily from a proposition that says, if $|\Omega|<\infty$ and $p<\infty$ then $\|f\|_p\le|\Omega|^{1/p}\|f\|_{\infty}$, if $|\Omega|\not=0$ then we have that $|\Omega|^{-1/p}\|f\|_p\le\|f\|_{\infty}$. Now onto proving the second one. I'm having a lot of trouble, I used an idea from here , but instead of getting the measure of $\Omega$, I get the measure of a subset, and I'm not using that $p$ has to be big or not, that part of the hint confuses me really, I can't see how do you use it for the problem.","I know this question has been asked a lot in this site, I've been checking those questions myself, however according to the theory we use in class there are things that I can't use and/or I don't know how to use (for example, we don't work with probability spaces, and other things that are very measure theory). So this is another version of this same old problem. If $|\Omega|<\infty$, $f\in L^{\infty}(\Omega)$, then $$\|f\|_{\infty}=\lim_{p\to{\infty}}|\Omega|^{-1/p}\|f\|_p$$ I see, that this problem really is like this one, since is easy to see that $\lim_{p\to\infty}|\Omega|^{-1/p}=1$. Now, we are given two hints: Prove that: (a) $|\Omega|^{-1/p}\|f\|_p\le\|f\|_{\infty},\forall p$. (b) If $\varepsilon>0$ then $|\Omega|^{-1/p}\|f\|_p\ge\|f\|_{\infty}-\varepsilon$ for $p$ sufficiently big. For the first one, it comes easily from a proposition that says, if $|\Omega|<\infty$ and $p<\infty$ then $\|f\|_p\le|\Omega|^{1/p}\|f\|_{\infty}$, if $|\Omega|\not=0$ then we have that $|\Omega|^{-1/p}\|f\|_p\le\|f\|_{\infty}$. Now onto proving the second one. I'm having a lot of trouble, I used an idea from here , but instead of getting the measure of $\Omega$, I get the measure of a subset, and I'm not using that $p$ has to be big or not, that part of the hint confuses me really, I can't see how do you use it for the problem.",,"['real-analysis', 'functional-analysis', 'lp-spaces']"
97,Countable stability of Hausdorff dimension,Countable stability of Hausdorff dimension,,"Can some one explain this property & the proof for me ?  It goes like this: If $F_1,F_2,....$ is a countable sequence of sets then $\dim_H \cup_{i=1}^{\infty}F_i=\sup_{1 \leq i < \infty} \{\dim_HF_i\}$. Since $\dim_H \cup_{i=1}^{\infty}F_i \geq \sup_{1 \leq i < \infty} \{\dim_HF_i\}$ from the monotonicity. For the other direction, if $s>\dim_HF_i$ for all $i$, then $H^s(F_i)=0$, so we have $H^s(\cup_{i=1}^{\infty}F_i)=0$. I know the first direction, but what about the second direction ?","Can some one explain this property & the proof for me ?  It goes like this: If $F_1,F_2,....$ is a countable sequence of sets then $\dim_H \cup_{i=1}^{\infty}F_i=\sup_{1 \leq i < \infty} \{\dim_HF_i\}$. Since $\dim_H \cup_{i=1}^{\infty}F_i \geq \sup_{1 \leq i < \infty} \{\dim_HF_i\}$ from the monotonicity. For the other direction, if $s>\dim_HF_i$ for all $i$, then $H^s(F_i)=0$, so we have $H^s(\cup_{i=1}^{\infty}F_i)=0$. I know the first direction, but what about the second direction ?",,"['real-analysis', 'measure-theory']"
98,Prove that there can be at most countably many disjoint letter T's in the plane,Prove that there can be at most countably many disjoint letter T's in the plane,,"A letter T in the plane is defined as a non-zero length segment with an orthogonal non-zero length segment that has an end-point in the strict interior of the first segment. Prove that there can be at most countably many disjoint letter T's in the plane. I've tried clumsily to prove that I can find balls around each of the endpoints of a 2 segments defining a T, such that if any other letter T has segment endpoints contained in the balls then the two T's must intersect. This would be enough because for each ball we can choose a pure rational ball that has rational center and rational radius that is contained inside the ball. But I've had a hard time making this proof rigorous. Any help?","A letter T in the plane is defined as a non-zero length segment with an orthogonal non-zero length segment that has an end-point in the strict interior of the first segment. Prove that there can be at most countably many disjoint letter T's in the plane. I've tried clumsily to prove that I can find balls around each of the endpoints of a 2 segments defining a T, such that if any other letter T has segment endpoints contained in the balls then the two T's must intersect. This would be enough because for each ball we can choose a pure rational ball that has rational center and rational radius that is contained inside the ball. But I've had a hard time making this proof rigorous. Any help?",,['real-analysis']
99,Hyperbolic Systems ODE,Hyperbolic Systems ODE,,Let $M_n$ the set of matrices of order $n \times n$ identified with $\mathbb{R^{n^2}}$ e $S=\{A \in M_n ; x'=Ax$ is hyperbolic$\}$. Show that $S$ is open and dense $M_n$.,Let $M_n$ the set of matrices of order $n \times n$ identified with $\mathbb{R^{n^2}}$ e $S=\{A \in M_n ; x'=Ax$ is hyperbolic$\}$. Show that $S$ is open and dense $M_n$.,,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
