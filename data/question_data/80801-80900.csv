,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove that the product of two invertible matrices also invertible,Prove that the product of two invertible matrices also invertible,,"I am working on a homework problem, but I am lacking some understanding.  Here is the problem: Let $A$ and $B$ be invertible $n \times n$ matrices with $\det(A) = 3$ and $\det(B) = 4$. I know that the product matrix of two invertible matrices must be invertible as well, but I am not sure how to prove that. I am trying to show it through the product of determinants if possible.","I am working on a homework problem, but I am lacking some understanding.  Here is the problem: Let $A$ and $B$ be invertible $n \times n$ matrices with $\det(A) = 3$ and $\det(B) = 4$. I know that the product matrix of two invertible matrices must be invertible as well, but I am not sure how to prove that. I am trying to show it through the product of determinants if possible.",,"['linear-algebra', 'matrices', 'determinant']"
1,Wouldn't each addition take time $O(n)$?,Wouldn't each addition take time ?,O(n),"I am going over the asymptotic runtime of regular matrix multiplication. Here  is a lecture slide I am referencing(too much to type out, shown below), from Algorithms Everything makes sense up until the point that the author states that ""each addition takes $O(n^2)$ time"". Can anyone explain that? I have a counter example here. Say I have a 4 by 4 matrix. The product of $A$ and $E$ would result in something like (2 by 2 matrix) $$         \begin{bmatrix}         1 & 2 \\         3 & 4  \\         \end{bmatrix} $$ and the product of $B$ and $G$ would result in something like $$         \begin{bmatrix}         5 & 6 \\         7 & 8  \\         \end{bmatrix} $$ The author argues that one addition runs in $O(n^2)$ but I argue that one addition runs in $O(n)$. Here to add $AE$ and $BG$, you perform additions 1 + 5, 2 + 6, 3 + 7, and 4 + 8, or 4 additions. 4 was the original n, so the runtime of one addition would be $O(n)$ Do you guys agree with my counterexample/argument or did I miss something and runtime of one addition would be $O(n^2)$?","I am going over the asymptotic runtime of regular matrix multiplication. Here  is a lecture slide I am referencing(too much to type out, shown below), from Algorithms Everything makes sense up until the point that the author states that ""each addition takes $O(n^2)$ time"". Can anyone explain that? I have a counter example here. Say I have a 4 by 4 matrix. The product of $A$ and $E$ would result in something like (2 by 2 matrix) $$         \begin{bmatrix}         1 & 2 \\         3 & 4  \\         \end{bmatrix} $$ and the product of $B$ and $G$ would result in something like $$         \begin{bmatrix}         5 & 6 \\         7 & 8  \\         \end{bmatrix} $$ The author argues that one addition runs in $O(n^2)$ but I argue that one addition runs in $O(n)$. Here to add $AE$ and $BG$, you perform additions 1 + 5, 2 + 6, 3 + 7, and 4 + 8, or 4 additions. 4 was the original n, so the runtime of one addition would be $O(n)$ Do you guys agree with my counterexample/argument or did I miss something and runtime of one addition would be $O(n^2)$?",,"['matrices', 'discrete-mathematics', 'asymptotics', 'computer-science', 'computational-complexity']"
2,"If $X$ is a singular matrix, then is $X'X$ singular?","If  is a singular matrix, then is  singular?",X X'X,"If $X$ is a singular matrix, then is $X'X$ singular? And why? ( ' means transpose) Thanks in advance!","If $X$ is a singular matrix, then is $X'X$ singular? And why? ( ' means transpose) Thanks in advance!",,['matrices']
3,How do I show that $T$ is invertible?,How do I show that  is invertible?,T,"I'm really stuck on these linear transformations, so I have $T(x_1,x_2)=(-5x_1+9x_2,4x_1-7x_2)$, and I need to show that $T$ is invertible. So would I pretty much just say that this is the matrix: $$\left[\begin{matrix}-5&9\\4&-7\end{matrix}\right]$$ Then it's inverse must be $\frac{1}{(-5)(-7)-(9)(4)}\left[\begin{matrix}-7&-9\\-4&-5\end{matrix}\right]=\left[\begin{matrix}7&9\\4&5\end{matrix}\right]$. But is that ""showing"" that $T$ is invertible? I'm also supposed to find a formula for $T^{-1}$. But that's the matrix I just found right?","I'm really stuck on these linear transformations, so I have $T(x_1,x_2)=(-5x_1+9x_2,4x_1-7x_2)$, and I need to show that $T$ is invertible. So would I pretty much just say that this is the matrix: $$\left[\begin{matrix}-5&9\\4&-7\end{matrix}\right]$$ Then it's inverse must be $\frac{1}{(-5)(-7)-(9)(4)}\left[\begin{matrix}-7&-9\\-4&-5\end{matrix}\right]=\left[\begin{matrix}7&9\\4&5\end{matrix}\right]$. But is that ""showing"" that $T$ is invertible? I'm also supposed to find a formula for $T^{-1}$. But that's the matrix I just found right?",,"['linear-algebra', 'matrices']"
4,"Given an $n \times n$ matrix $A$, if $Ax = x$ for all $x \in \Bbb R^n$, prove that $A$ is the identity matrix.","Given an  matrix , if  for all , prove that  is the identity matrix.",n \times n A Ax = x x \in \Bbb R^n A,How can I prove that this statement is true? I found this in an old textbook I was flipping through and was wondering how I could construct a proof for it.,How can I prove that this statement is true? I found this in an old textbook I was flipping through and was wondering how I could construct a proof for it.,,"['linear-algebra', 'matrices']"
5,Matrices for which $\mathbf{A}^{-1}=-\mathbf{A}$,Matrices for which,\mathbf{A}^{-1}=-\mathbf{A},"Consider matrices $\mathbf{A}\in\mathbb{C}^{n\times n}$ (or maybe $A\in\mathbb{R}^{n\times n}$) for which $\mathbf{A}^{-1}=-\mathbf{A}$. A conical example (and the only one I can come up with) would be $\mathbf{A} = \boldsymbol{i}\mathbf{I},\quad \boldsymbol i^2=-1$. Now I have a few questions about this class of matrices: Are there more matrices than this example matrix (I guess yes) or can they even be generally constructed somehow? Are there also real matrices for which this holds? Now each matrix that is both skew-Hermitian and unitary fulfills this property. But does it also hold in the other direction, meaning is each matrix for which $\mathbf{A}^{-1}=-\mathbf{A}$ both skew-Hermitian and unitary (maybe this is simple to prove, but I don't know where to start at the moment, but of course I know if one holds the other has to hold, too)? Do such matrices have any practical meaning? For example I know that Hermitian and unitary matrices are reflections (in a general sense), but what about skew-Hermitian and unitary (if 3 holds)? This is just for personal interrest without any practical application. I just stumbled accross this property by accident and want to know more about its implications and applications.","Consider matrices $\mathbf{A}\in\mathbb{C}^{n\times n}$ (or maybe $A\in\mathbb{R}^{n\times n}$) for which $\mathbf{A}^{-1}=-\mathbf{A}$. A conical example (and the only one I can come up with) would be $\mathbf{A} = \boldsymbol{i}\mathbf{I},\quad \boldsymbol i^2=-1$. Now I have a few questions about this class of matrices: Are there more matrices than this example matrix (I guess yes) or can they even be generally constructed somehow? Are there also real matrices for which this holds? Now each matrix that is both skew-Hermitian and unitary fulfills this property. But does it also hold in the other direction, meaning is each matrix for which $\mathbf{A}^{-1}=-\mathbf{A}$ both skew-Hermitian and unitary (maybe this is simple to prove, but I don't know where to start at the moment, but of course I know if one holds the other has to hold, too)? Do such matrices have any practical meaning? For example I know that Hermitian and unitary matrices are reflections (in a general sense), but what about skew-Hermitian and unitary (if 3 holds)? This is just for personal interrest without any practical application. I just stumbled accross this property by accident and want to know more about its implications and applications.",,"['linear-algebra', 'matrices']"
6,How to find a value for a variable that makes a matrix (with said variable) equal to its own inverse,How to find a value for a variable that makes a matrix (with said variable) equal to its own inverse,,I'm given $$\begin{bmatrix}3&x\\-2&-3\\\end{bmatrix}$$ and am asked to find x such that it's inverse would equal itself. To attempt this I first tried to put the question into an augmented matrix and got this: $$\begin{bmatrix}1&x/3&1/3&0\\0&(x/3)-(3/2)&1/3&1/2\\ \end{bmatrix}$$ I found that my answer was wrong so I tried: $$\begin{bmatrix}3&x\\-2&-3\\\end{bmatrix}$$ times $$\begin{bmatrix}x_1&x_2\\x_3&x_4\\\end{bmatrix}$$ to try and solve for x but found similar dissatisfactory results. The answer is listed as x = 4; how might I go about solving this? Did I just make a mistake with my methods or is this the entirely wrong way about?,I'm given $$\begin{bmatrix}3&x\\-2&-3\\\end{bmatrix}$$ and am asked to find x such that it's inverse would equal itself. To attempt this I first tried to put the question into an augmented matrix and got this: $$\begin{bmatrix}1&x/3&1/3&0\\0&(x/3)-(3/2)&1/3&1/2\\ \end{bmatrix}$$ I found that my answer was wrong so I tried: $$\begin{bmatrix}3&x\\-2&-3\\\end{bmatrix}$$ times $$\begin{bmatrix}x_1&x_2\\x_3&x_4\\\end{bmatrix}$$ to try and solve for x but found similar dissatisfactory results. The answer is listed as x = 4; how might I go about solving this? Did I just make a mistake with my methods or is this the entirely wrong way about?,,"['linear-algebra', 'matrices']"
7,How form a linear mapping $f:\mathbb{R}^{4} \rightarrow \mathbb{R}^{3}$ to a matrix correctly?(solved),How form a linear mapping  to a matrix correctly?(solved),f:\mathbb{R}^{4} \rightarrow \mathbb{R}^{3},"Given is $f: \mathbb{R}^4 \rightarrow \mathbb{R}^3$ $f(x)= \begin{pmatrix} x_1-2x_2+x_4\\  -2x_1+5x_2+x_3-4x_4\\  x_1+2x_3-3x_4 \end{pmatrix}$ How can I form this to a matrix correctly? We have $f: \mathbb{R}^4 \rightarrow \mathbb{R}^3$, and we have $x_1,x_2,x_3,x_4$ I think because we go to $\mathbb{R}^3$, we will only have $x_1,x_2,x_3$ So when I form a matrix, I will ignore $x_4$: \begin{pmatrix}  1 & -2 &  0\\  -2 &  5 &  1\\   1 &  0 &  2 \end{pmatrix} Is it fine like that?","Given is $f: \mathbb{R}^4 \rightarrow \mathbb{R}^3$ $f(x)= \begin{pmatrix} x_1-2x_2+x_4\\  -2x_1+5x_2+x_3-4x_4\\  x_1+2x_3-3x_4 \end{pmatrix}$ How can I form this to a matrix correctly? We have $f: \mathbb{R}^4 \rightarrow \mathbb{R}^3$, and we have $x_1,x_2,x_3,x_4$ I think because we go to $\mathbb{R}^3$, we will only have $x_1,x_2,x_3$ So when I form a matrix, I will ignore $x_4$: \begin{pmatrix}  1 & -2 &  0\\  -2 &  5 &  1\\   1 &  0 &  2 \end{pmatrix} Is it fine like that?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations']"
8,How do I find a matrix $B\neq 0$ so $AB = BA = 0$,How do I find a matrix  so,B\neq 0 AB = BA = 0,"Sorry if it's relatively easy, I just have no idea what to do here: $A = \begin{pmatrix} 1 & 2 &3 \\  4 & 5 & 6\\  7 & 8 & 9 \end{pmatrix}$ how to find  $B\neq 0$ so $AB = BA = 0$ I only know if I need to find it, then A is linear dependant if that is useful for it somehow. I probably can build a $3x3$ matrix with variables and solve a huge equation system but I don't think it's what I'm supposed to do here and there must be a smarter solution.","Sorry if it's relatively easy, I just have no idea what to do here: $A = \begin{pmatrix} 1 & 2 &3 \\  4 & 5 & 6\\  7 & 8 & 9 \end{pmatrix}$ how to find  $B\neq 0$ so $AB = BA = 0$ I only know if I need to find it, then A is linear dependant if that is useful for it somehow. I probably can build a $3x3$ matrix with variables and solve a huge equation system but I don't think it's what I'm supposed to do here and there must be a smarter solution.",,"['linear-algebra', 'matrices']"
9,"Finding $B,C$ such that $B\left[\begin{smallmatrix}1&2\\4&8\end{smallmatrix}\right]C=\left[\begin{smallmatrix}1&0\\0&0\end{smallmatrix}\right]$",Finding  such that,"B,C B\left[\begin{smallmatrix}1&2\\4&8\end{smallmatrix}\right]C=\left[\begin{smallmatrix}1&0\\0&0\end{smallmatrix}\right]","State $B,C$, such that $B\begin{bmatrix} 1 & 2  \\ 4 & 8    \end{bmatrix}C=\begin{bmatrix} 1 & 0  \\ 0 & 0   \end{bmatrix}$ I tried some things, but I ended up with non invertible matrices, so I stopped for the moment. Is there a quick way to guesswork here? (it's an assignment so, maybe some ""nice"" values for the entries will do) I don't even know how I multiply a matrix to get 0-Entries with invertible matrices. So maybe let's start at that part?","State $B,C$, such that $B\begin{bmatrix} 1 & 2  \\ 4 & 8    \end{bmatrix}C=\begin{bmatrix} 1 & 0  \\ 0 & 0   \end{bmatrix}$ I tried some things, but I ended up with non invertible matrices, so I stopped for the moment. Is there a quick way to guesswork here? (it's an assignment so, maybe some ""nice"" values for the entries will do) I don't even know how I multiply a matrix to get 0-Entries with invertible matrices. So maybe let's start at that part?",,"['matrices', 'systems-of-equations']"
10,How to prove $I-BA$ is invertible [duplicate],How to prove  is invertible [duplicate],I-BA,"This question already has answers here : $I-AB$ be invertible $\Leftrightarrow$ $I-BA$ is invertible [duplicate] (3 answers) Closed 7 years ago . Show that $I-BA$ is invertible if $I-AB$ is invertible. And also, we have to prove that eigenvalues are same for $AB$ and $BA$ Till now, I used the equation $(I-AB)(I-AB)^{-1}=I$ which gives $(I-AB)AB(I-AB)^{-1}=AB$ but I don't know how to proceed further. Any hint would be sufficient !","This question already has answers here : $I-AB$ be invertible $\Leftrightarrow$ $I-BA$ is invertible [duplicate] (3 answers) Closed 7 years ago . Show that $I-BA$ is invertible if $I-AB$ is invertible. And also, we have to prove that eigenvalues are same for $AB$ and $BA$ Till now, I used the equation $(I-AB)(I-AB)^{-1}=I$ which gives $(I-AB)AB(I-AB)^{-1}=AB$ but I don't know how to proceed further. Any hint would be sufficient !",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
11,Every subspace is the kernel of a linear map,Every subspace is the kernel of a linear map,,"I know that every kernel of a linear map from $\mathbb R^n$ to $\mathbb R^m$ is a subspace of $\mathbb R^n$. I am wondering if the converse is true, i.e. every subspace of $\mathbb R^n$ is the kernel of a certain linear map fromm $\mathbb R^n$ to $\mathbb R^m$. It was for me easy to see that this is true for  $n=m=2$ and I am pretty sure that this is true in general. But I dont know how to prove this.","I know that every kernel of a linear map from $\mathbb R^n$ to $\mathbb R^m$ is a subspace of $\mathbb R^n$. I am wondering if the converse is true, i.e. every subspace of $\mathbb R^n$ is the kernel of a certain linear map fromm $\mathbb R^n$ to $\mathbb R^m$. It was for me easy to see that this is true for  $n=m=2$ and I am pretty sure that this is true in general. But I dont know how to prove this.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces']"
12,Questions about the subgroups of $SU(2)$ and relevant problems?,Questions about the subgroups of  and relevant problems?,SU(2),"This question is based on the invariant gauge groups in condensed matter physics( https://physics.stackexchange.com/questions/76644/different-invariant-gauge-groups-igg-on-different-lattices-with-the-same-form ). As we know, the group $SU(2)$ can be viewed as the set of all the $2\times2$ unitary matrices with determinant $1$. And let the groups $U(1)=\left \{\begin{pmatrix}  e^{i\theta}&0 \\  0 & e^{-i\theta} \end{pmatrix} \mid 0\leqslant\theta<2\pi  \right \}$, $\mathbb{Z}_2=\left \{I,-I  \right \}$, where $I$ is the $2\times2$ identity matrix. Then $U(1)$ and $\mathbb{Z}_2$ are both the subgroups of $SU(2)$. My questions are as follows: (1) In addition to $U(1)$ and $\mathbb{Z}_2$(of course $SU(2)$ itself and its subgroup { $I$ }), are there any other subgroup in $SU(2)$? (2) Does $SU(2)$ have two subgroups called $A$ and $B$ such that: $U(1)$ is a subgroup of $A$ , $B$ is a subgroup of $U(1)$, and $\mathbb{Z}_2$ is a subgroup of $B$ ? (3) Also as we know, $SO(3)\cong SU(2)/\mathbb{Z}_2$, and does $SU(2)$ have a subgroup called $C$ such that $C\cong SO(3)$? More generally, if $K$ is a normal subgroup of group $G$, then does $G$ have a subgroup called $H$ such that $H\cong G/K$? Thanks in advance.","This question is based on the invariant gauge groups in condensed matter physics( https://physics.stackexchange.com/questions/76644/different-invariant-gauge-groups-igg-on-different-lattices-with-the-same-form ). As we know, the group $SU(2)$ can be viewed as the set of all the $2\times2$ unitary matrices with determinant $1$. And let the groups $U(1)=\left \{\begin{pmatrix}  e^{i\theta}&0 \\  0 & e^{-i\theta} \end{pmatrix} \mid 0\leqslant\theta<2\pi  \right \}$, $\mathbb{Z}_2=\left \{I,-I  \right \}$, where $I$ is the $2\times2$ identity matrix. Then $U(1)$ and $\mathbb{Z}_2$ are both the subgroups of $SU(2)$. My questions are as follows: (1) In addition to $U(1)$ and $\mathbb{Z}_2$(of course $SU(2)$ itself and its subgroup { $I$ }), are there any other subgroup in $SU(2)$? (2) Does $SU(2)$ have two subgroups called $A$ and $B$ such that: $U(1)$ is a subgroup of $A$ , $B$ is a subgroup of $U(1)$, and $\mathbb{Z}_2$ is a subgroup of $B$ ? (3) Also as we know, $SO(3)\cong SU(2)/\mathbb{Z}_2$, and does $SU(2)$ have a subgroup called $C$ such that $C\cong SO(3)$? More generally, if $K$ is a normal subgroup of group $G$, then does $G$ have a subgroup called $H$ such that $H\cong G/K$? Thanks in advance.",,"['group-theory', 'matrices', 'representation-theory', 'lie-groups', 'equivalence-relations']"
13,$4^\text{th}$ power of a $2\times 2$ matrix,power of a  matrix,4^\text{th} 2\times 2,$$A = \left(\begin{array}{cc}\cos x & -\sin x \\ \sin x & \cos x\end{array}\right)$$ is given as a matrix. What is the result of $$ad + bc \text{ if } A^4=\left(\begin{array}{cc}a&b\\c&d\end{array}\right)$$ Note that $A^4$ is the $4^\text{th}$ power of the matrix $A$. I tried to use some trigonometric expressions but it gets very complicated and couldn't solve it.,$$A = \left(\begin{array}{cc}\cos x & -\sin x \\ \sin x & \cos x\end{array}\right)$$ is given as a matrix. What is the result of $$ad + bc \text{ if } A^4=\left(\begin{array}{cc}a&b\\c&d\end{array}\right)$$ Note that $A^4$ is the $4^\text{th}$ power of the matrix $A$. I tried to use some trigonometric expressions but it gets very complicated and couldn't solve it.,,"['linear-algebra', 'matrices', 'trigonometry']"
14,Why is $\det⁡(-A)=(-1)^n\det(A)$? [closed],Why is ? [closed],\det⁡(-A)=(-1)^n\det(A),"This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time, or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making this question more broadly applicable, visit the help center . Closed 11 years ago . Why is $\det⁡(-A)=(-1)^n\det(A)$?","This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time, or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making this question more broadly applicable, visit the help center . Closed 11 years ago . Why is $\det⁡(-A)=(-1)^n\det(A)$?",,"['linear-algebra', 'matrices']"
15,Minimize $\| A x - b \|_{2}^{2}$ Subject To $\| x \|_2 = 1$ and $x \succeq 0$ (Least Squares with Inequality and Non Linear Equality of $ L_2 $ Norm),Minimize  Subject To  and  (Least Squares with Inequality and Non Linear Equality of  Norm),\| A x - b \|_{2}^{2} \| x \|_2 = 1 x \succeq 0  L_2 ,"Given $y \in \mathbb R^n$ and $A \in \mathbb R^{n \times n}$, whis is some way for $$\min_x \| y- Ax\|$$ subject to $\|x\|=1$, and $x \geq 0$ (which means every components of $x$ is nonnegative)? Is there any book discussing such a problem? Thanks! Remark: The objective functions $\left\| A x - y \right\|$ and $\frac{1}{2} {\left\| A x - y \right\|}$ are equivalent, while the latter is differentiable and easier to handle.","Given $y \in \mathbb R^n$ and $A \in \mathbb R^{n \times n}$, whis is some way for $$\min_x \| y- Ax\|$$ subject to $\|x\|=1$, and $x \geq 0$ (which means every components of $x$ is nonnegative)? Is there any book discussing such a problem? Thanks! Remark: The objective functions $\left\| A x - y \right\|$ and $\frac{1}{2} {\left\| A x - y \right\|}$ are equivalent, while the latter is differentiable and easier to handle.",,"['linear-algebra', 'matrices', 'optimization', 'least-squares', 'non-convex-optimization']"
16,How to invert sum of matrices?,How to invert sum of matrices?,,"Given are two matrices: $\bf A, \bf B$ We know that matrices $\bf A \neq \bf B$ are invertable, symmetric, positive-definite and of full rank. Is it possible to give the formula for following sum of these matrices: $[\bf A + \lambda\bf B]^{-1} = ?$ where $\lambda$ is a scalar such as $0 < \lambda < 1$.","Given are two matrices: $\bf A, \bf B$ We know that matrices $\bf A \neq \bf B$ are invertable, symmetric, positive-definite and of full rank. Is it possible to give the formula for following sum of these matrices: $[\bf A + \lambda\bf B]^{-1} = ?$ where $\lambda$ is a scalar such as $0 < \lambda < 1$.",,"['linear-algebra', 'matrices', 'inverse']"
17,Matrix of a quadratic form?,Matrix of a quadratic form?,,"What exactly is the matrix of a quadratic form? I have seen this notation occuring in a few papers (e.g. Siegel's unreadable German papers), with particular reference to the trace of a quadratic form. I'm at a loss as to what this means, and as a bonus question in passing I'd be interested if the trace of a quadratic form was interesting for an ""obvious"" reason (echoes of character theory, maybe?) For example what is the matrix of the quadratic form $x^2+y^2+z^2$? Or $x^2+xy+y^2$?","What exactly is the matrix of a quadratic form? I have seen this notation occuring in a few papers (e.g. Siegel's unreadable German papers), with particular reference to the trace of a quadratic form. I'm at a loss as to what this means, and as a bonus question in passing I'd be interested if the trace of a quadratic form was interesting for an ""obvious"" reason (echoes of character theory, maybe?) For example what is the matrix of the quadratic form $x^2+y^2+z^2$? Or $x^2+xy+y^2$?",,['matrices']
18,the transformation which rotates a matrix by a half turn,the transformation which rotates a matrix by a half turn,,"Consider $$T_{2}:  \left[ \begin{array}{cc} a & b  \\ c & d \\ \end{array} \right] \rightarrow  \left[ \begin{array}{cc} d & c  \\ b & a \\ \end{array} \right] $$ $$T_{3}:  \left[ \begin{array}{ccc} a & b &c   \\ d & e & f \\ g & h & i \end{array} \right] \rightarrow  \left[ \begin{array}{cc} i & h & g  \\ f & e & d\\ c & b & a \\ \end{array} \right] $$ What is the appropriate name for this sort of transformation? (googling for combinations of 'matrix' and 'rotation"" hasn't been fruitful for obvious reasons). I know that $T_{n}$ is an involution -- $T_{n}^{2}=I_{n}$ --  but I don't know what effect it has in general, that is, what it does to $GL_{n}(\mathbb{R})$ or anything else representable by $n\times n$ matrices. (the motivation for this question is the effect of $T_{2}$ on the modular group $SL(2,\mathbb{Z})$)","Consider $$T_{2}:  \left[ \begin{array}{cc} a & b  \\ c & d \\ \end{array} \right] \rightarrow  \left[ \begin{array}{cc} d & c  \\ b & a \\ \end{array} \right] $$ $$T_{3}:  \left[ \begin{array}{ccc} a & b &c   \\ d & e & f \\ g & h & i \end{array} \right] \rightarrow  \left[ \begin{array}{cc} i & h & g  \\ f & e & d\\ c & b & a \\ \end{array} \right] $$ What is the appropriate name for this sort of transformation? (googling for combinations of 'matrix' and 'rotation"" hasn't been fruitful for obvious reasons). I know that $T_{n}$ is an involution -- $T_{n}^{2}=I_{n}$ --  but I don't know what effect it has in general, that is, what it does to $GL_{n}(\mathbb{R})$ or anything else representable by $n\times n$ matrices. (the motivation for this question is the effect of $T_{2}$ on the modular group $SL(2,\mathbb{Z})$)",,"['group-theory', 'matrices', 'reference-request', 'lie-groups']"
19,What does an inverse matrix abstracts?,What does an inverse matrix abstracts?,,"I am trying to understand inverse matrixes more in depth. I took the simplest example: 2 points in a 2d space and put it into a matrix. $$\begin{pmatrix}5&7\\-2&3\end{pmatrix}$$ Calculating the inverse, we would get another matrix and another 2 points. Where could this inverse be used / and for what purposes? Can someone provide me with a trivial example (preferably in 2d)?","I am trying to understand inverse matrixes more in depth. I took the simplest example: 2 points in a 2d space and put it into a matrix. $$\begin{pmatrix}5&7\\-2&3\end{pmatrix}$$ Calculating the inverse, we would get another matrix and another 2 points. Where could this inverse be used / and for what purposes? Can someone provide me with a trivial example (preferably in 2d)?",,"['matrices', 'vector-spaces']"
20,Simplest proof to calculate the center of M_n(F),Simplest proof to calculate the center of M_n(F),,"If we consider the set of all n by n matrices M, and denote by Z(M) the set of matrices which commute with every matrix I personally dislike the only proof I know. So I was wondering if anybody here knows of a more elegant proof, or rather just a different proof, given that ""elegant"" is subjective. Forgot to mention, the proof I know is where we consider $A\in Z(M)$ and calculate the effect on the standard basis of the set of n by n matrices, to notice that A must be scalar in light of the implied conditions on the rows and columns of A. Thank you for all the proofs, I am most definitely satisfied in elegance and variety.","If we consider the set of all n by n matrices M, and denote by Z(M) the set of matrices which commute with every matrix I personally dislike the only proof I know. So I was wondering if anybody here knows of a more elegant proof, or rather just a different proof, given that ""elegant"" is subjective. Forgot to mention, the proof I know is where we consider and calculate the effect on the standard basis of the set of n by n matrices, to notice that A must be scalar in light of the implied conditions on the rows and columns of A. Thank you for all the proofs, I am most definitely satisfied in elegance and variety.",A\in Z(M),"['linear-algebra', 'matrices']"
21,Finding the smallest power of $A$ such that $A^n = I$,Finding the smallest power of  such that,A A^n = I,"Let $A=\begin{bmatrix}0 & 1\\-1 & 1\end{bmatrix}$ then the smallest positive positive integer $n$ such that $A^n = I$ is : (a) $1$ (b) $2$ (c) $4$ (d) $6$ proof: option (d) 6. The characteristic polynomial of $A$ is $\lambda^2 - \lambda + 1$ . So the eigenvalues of $A$ are $\omega, \omega^2$ where $\omega$ is a cube root of $-1$ and not equal to $-1$ . So the eigenvalues of $A^n$ are $\omega^n$ and $\omega^{2n}$ and the eigenvalue of $I$ is $1$ , so $\omega^{2n} = 1$ and $\omega^n = 1$ which implies $n$ as a multiple of $3$ and $2$ which means $6$ is the only option we have. Is my reasoning correct?? And can we solve this without using eigenvalues?","Let then the smallest positive positive integer such that is : (a) (b) (c) (d) proof: option (d) 6. The characteristic polynomial of is . So the eigenvalues of are where is a cube root of and not equal to . So the eigenvalues of are and and the eigenvalue of is , so and which implies as a multiple of and which means is the only option we have. Is my reasoning correct?? And can we solve this without using eigenvalues?","A=\begin{bmatrix}0 & 1\\-1 & 1\end{bmatrix} n A^n = I 1 2 4 6 A \lambda^2 - \lambda + 1 A \omega, \omega^2 \omega -1 -1 A^n \omega^n \omega^{2n} I 1 \omega^{2n} = 1 \omega^n = 1 n 3 2 6","['linear-algebra', 'matrices', 'solution-verification']"
22,Maximize $\mathrm{tr}(Q^TCQ)$ subject to $Q^TQ=I$,Maximize  subject to,\mathrm{tr}(Q^TCQ) Q^TQ=I,"Let $C \in \mathbb{R}^{d \times d}$ be symmetric, and $$Q = \begin{bmatrix}       \vert & \vert &    & \vert \\       q_1   & q_2   & \dots & q_K  \\       \vert & \vert &    & \vert       \end{bmatrix} \in \mathbb{R}^{d\times K}$$ where $d \geq K$ . Using Lagrange multipliers, $$\begin{array}{ll} \text{maximize} & \mbox{tr} \left( Q^T C Q \right)\\ \text{subject to} & Q^T Q = I\end{array}$$ I am unfamiliar with these kind of constraints with this method, and after reading another post I believe the same specific and simple result given is also applicable, and therefore the lagrangian would be: $$\mathcal{L}(Q,\lambda)=\mathrm{tr}(Q^TCQ)-\left<\lambda,Q^TQ-I\right>$$ where $\lambda\in\mathbb{R}^{K\times K}$ , and $\left<\cdot,\cdot\right>$ is the element wise inner product (what kind of makes sense to me since we're actually adding as many constraints as there are elements in these matrices. In attempting to do that I start taking $\frac{\partial \mathcal{L}}{\partial Q}=O\in\mathbb{R}^{d\times K}$ , and compute that LHS element by element; for the $(l,m)$ one: \begin{equation} 0=\frac{\partial \mathcal{L}}{\partial Q_{lm}}=(CQ+C^TQ)_{lm}-\underbrace{\frac{\partial}{\partial Q_{lm}}\sum_{i,j}\lambda_{i,j}(Q^TQ-I)_{ij}}_{=\lambda_{lm}\frac{\partial (Q^TQ)_{lm}}{\partial Q_{lm}}}=2(CQ)_{lm}-\lambda_{lm}\frac{\partial (q_l^Tq_m)}{\partial q_m(l)} \tag{1}\end{equation} where in the last step I've used the definition I made at the beginning for $Q$ , and $q_m(l)$ denotes the $l$ -th component of the column vector $q_m$ . In trying to compute the very last term: $$\frac{\partial (q_l^Tq_m)}{\partial q_m(l)}=\frac{\partial \left[q_l(1)q_m(1)+ \ldots + q_l(d)q_m(d)\right]}{\partial q_m(l)}= \begin{cases}     q_l(l)\equiv Q_{ll}    & \text{if } l\neq m\\     2q_l(l)\equiv 2Q_{ll}  & \text{if} l=m \end{cases}$$ The whole equality (1) then can be written: $$0=2(CQ)_{lm}-\lambda_{lm}Q_{ll}(1+\delta_{lm})$$ where $\delta_{lm}$ is the Kronecker delta. The equation for the other stationary point of the lagrangian, $\frac{\partial \mathcal{L}}{\partial \lambda}=O\in\mathbb{R}^{K\times K}$ , for the $(l,m)$ element as well: $$ 0=\frac{\partial \mathcal L}{\partial \lambda_{lm}}= \frac{\partial }{\partial \lambda_{lm}}\sum_{i,j}\lambda_{i,j}(Q^TQ-I)_{ij}=(Q^TQ-I)_{lm}\tag{2}$$ what obviously leads to $(Q^TQ)_{lm}=\delta_{lm}$ . All this should tell that the columns of $Q$ are eventually the $K$ first eigenvectors of $C$ , but I don't know how to continue from here to prove that, supposing I didn't make a mistake. Please I would sincerely appreciate any help. Edit: I have rewritten the inner product as a trace of a product of matrices (after seeing this question ): $$\left<\lambda,Q^TQ-I\right>=\sum_{i,j}\lambda_{i,j}(Q^TQ-I)_{ij}=\mathrm{tr}(\lambda^TQ^TQ) $$ and have thus managed to do the derivative without losing the matrix format (using formulas from the Matrix Cookbook ): \begin{align} O=&\frac{\partial \mathcal{L}}{\partial Q}=\frac{\partial}{\partial Q}\mathrm{tr}(Q^TCQ)-\frac{\partial}{\partial Q}\underbrace{\mathrm{tr}(\lambda^T(Q^TQ-I))}_{\mathrm{tr}(\lambda^TQ^TQ)-\mathrm{tr}(\lambda^T)}\\=&(CQ+C^TQ)-(Q(\lambda^T)^T+Q\lambda^T)=2CQ+Q(\lambda+\lambda^T) \end{align} And this leads to: $$CQ=Q\underbrace{\left(-\frac{\lambda+\lambda^T}{2}\right)}_{:=\widetilde{\lambda}};\quad CQ=Q$$ If the defined matrix $\widetilde{\lambda}=Q^TCQ$ were diagonal we would already have the result.","Let be symmetric, and where . Using Lagrange multipliers, I am unfamiliar with these kind of constraints with this method, and after reading another post I believe the same specific and simple result given is also applicable, and therefore the lagrangian would be: where , and is the element wise inner product (what kind of makes sense to me since we're actually adding as many constraints as there are elements in these matrices. In attempting to do that I start taking , and compute that LHS element by element; for the one: where in the last step I've used the definition I made at the beginning for , and denotes the -th component of the column vector . In trying to compute the very last term: The whole equality (1) then can be written: where is the Kronecker delta. The equation for the other stationary point of the lagrangian, , for the element as well: what obviously leads to . All this should tell that the columns of are eventually the first eigenvectors of , but I don't know how to continue from here to prove that, supposing I didn't make a mistake. Please I would sincerely appreciate any help. Edit: I have rewritten the inner product as a trace of a product of matrices (after seeing this question ): and have thus managed to do the derivative without losing the matrix format (using formulas from the Matrix Cookbook ): And this leads to: If the defined matrix were diagonal we would already have the result.","C \in \mathbb{R}^{d \times d} Q = \begin{bmatrix}
      \vert & \vert &    & \vert \\
      q_1   & q_2   & \dots & q_K  \\
      \vert & \vert &    & \vert
      \end{bmatrix} \in \mathbb{R}^{d\times K} d \geq K \begin{array}{ll} \text{maximize} & \mbox{tr} \left( Q^T C Q \right)\\ \text{subject to} & Q^T Q = I\end{array} \mathcal{L}(Q,\lambda)=\mathrm{tr}(Q^TCQ)-\left<\lambda,Q^TQ-I\right> \lambda\in\mathbb{R}^{K\times K} \left<\cdot,\cdot\right> \frac{\partial \mathcal{L}}{\partial Q}=O\in\mathbb{R}^{d\times K} (l,m) \begin{equation}
0=\frac{\partial \mathcal{L}}{\partial Q_{lm}}=(CQ+C^TQ)_{lm}-\underbrace{\frac{\partial}{\partial Q_{lm}}\sum_{i,j}\lambda_{i,j}(Q^TQ-I)_{ij}}_{=\lambda_{lm}\frac{\partial (Q^TQ)_{lm}}{\partial Q_{lm}}}=2(CQ)_{lm}-\lambda_{lm}\frac{\partial (q_l^Tq_m)}{\partial q_m(l)}
\tag{1}\end{equation} Q q_m(l) l q_m \frac{\partial (q_l^Tq_m)}{\partial q_m(l)}=\frac{\partial \left[q_l(1)q_m(1)+ \ldots + q_l(d)q_m(d)\right]}{\partial q_m(l)}=
\begin{cases}
    q_l(l)\equiv Q_{ll}    & \text{if } l\neq m\\
    2q_l(l)\equiv 2Q_{ll}  & \text{if} l=m
\end{cases} 0=2(CQ)_{lm}-\lambda_{lm}Q_{ll}(1+\delta_{lm}) \delta_{lm} \frac{\partial \mathcal{L}}{\partial \lambda}=O\in\mathbb{R}^{K\times K} (l,m)  0=\frac{\partial \mathcal L}{\partial \lambda_{lm}}= \frac{\partial }{\partial \lambda_{lm}}\sum_{i,j}\lambda_{i,j}(Q^TQ-I)_{ij}=(Q^TQ-I)_{lm}\tag{2} (Q^TQ)_{lm}=\delta_{lm} Q K C \left<\lambda,Q^TQ-I\right>=\sum_{i,j}\lambda_{i,j}(Q^TQ-I)_{ij}=\mathrm{tr}(\lambda^TQ^TQ)  \begin{align}
O=&\frac{\partial \mathcal{L}}{\partial Q}=\frac{\partial}{\partial Q}\mathrm{tr}(Q^TCQ)-\frac{\partial}{\partial Q}\underbrace{\mathrm{tr}(\lambda^T(Q^TQ-I))}_{\mathrm{tr}(\lambda^TQ^TQ)-\mathrm{tr}(\lambda^T)}\\=&(CQ+C^TQ)-(Q(\lambda^T)^T+Q\lambda^T)=2CQ+Q(\lambda+\lambda^T)
\end{align} CQ=Q\underbrace{\left(-\frac{\lambda+\lambda^T}{2}\right)}_{:=\widetilde{\lambda}};\quad CQ=Q \widetilde{\lambda}=Q^TCQ","['matrices', 'optimization', 'lagrange-multiplier', 'qcqp', 'stiefel-manifolds']"
23,"If A is a matrix such that $A^{2}+A+2I=O$ ,then $A$ can't be skew symmetric.","If A is a matrix such that  ,then  can't be skew symmetric.",A^{2}+A+2I=O A,"If $A$ is a matrix such that $A^{2}+A+2I=O$ , then $A$ can't be skew symmetric. (True/false) When $A$ is odd order matrix then the statement is true, since $A$ is non singular. $( |A||A+I| = (-2)^{n})$ and skew symmetric matrix of odd order is singular. How to check for even case $?$","If is a matrix such that , then can't be skew symmetric. (True/false) When is odd order matrix then the statement is true, since is non singular. and skew symmetric matrix of odd order is singular. How to check for even case",A A^{2}+A+2I=O A A A ( |A||A+I| = (-2)^{n}) ?,"['linear-algebra', 'abstract-algebra', 'matrices', 'matrix-equations', 'skew-symmetric-matrices']"
24,Prove that $\operatorname{rank}A=\operatorname{rank}B$ if $A$ and $B$ are idempotent matrices and $I - (A+B)$ is invertible,Prove that  if  and  are idempotent matrices and  is invertible,\operatorname{rank}A=\operatorname{rank}B A B I - (A+B),"I've got a very interesting problem and wondering if my idea for the solution is right and complete. Please, take a look. Problem Given two matrices $A$ and $B$ such that $A^2=A$ and $B^2=B$ . Prove that $\operatorname{rank}A=\operatorname{rank}B$ if the matrix $I - (A+B)$ is invertible. Solution $A^2=A$ $B^2=B$ Let's subtract the second equation from the first one. $A^2 - B^2 = A - B$ $(A - B)(A+B) = A - B$ $(A - B)(A+B) - (A-B)= 0$ $(A - B)((A+B) - I))= 0$ $(A - B)(I - (A+B))= 0$ Since $(I - (A+B))$ is invertible, let's multiply the above equation by $(I - (A+B))^{-1}$ from the right. $(A - B)(I - (A+B))(I - (A+B))^{-1}= 0$ $A - B = 0$ Hence, $A = B$ which means that $rankA = rankB$ .","I've got a very interesting problem and wondering if my idea for the solution is right and complete. Please, take a look. Problem Given two matrices and such that and . Prove that if the matrix is invertible. Solution Let's subtract the second equation from the first one. Since is invertible, let's multiply the above equation by from the right. Hence, which means that .",A B A^2=A B^2=B \operatorname{rank}A=\operatorname{rank}B I - (A+B) A^2=A B^2=B A^2 - B^2 = A - B (A - B)(A+B) = A - B (A - B)(A+B) - (A-B)= 0 (A - B)((A+B) - I))= 0 (A - B)(I - (A+B))= 0 (I - (A+B)) (I - (A+B))^{-1} (A - B)(I - (A+B))(I - (A+B))^{-1}= 0 A - B = 0 A = B rankA = rankB,"['linear-algebra', 'matrices', 'proof-verification']"
25,Solving for unknown 3x3 matrix,Solving for unknown 3x3 matrix,,"Given the following I am trying to solve for $A$ : $$ v_1 = A v_2$$ where $v_1$ is a known $3\times 2$ matrix, $A$ is an unknown $3\times 3$ matrix and $v_2$ is a known $3\times 2$ matrix Additionally: $A^{-1} v_1 = v_2$ I am not quite sure how to proceed—is there a way to get a solution for A since all of the given vectors are non square? Any leads would be appreciated!","Given the following I am trying to solve for : where is a known matrix, is an unknown matrix and is a known matrix Additionally: I am not quite sure how to proceed—is there a way to get a solution for A since all of the given vectors are non square? Any leads would be appreciated!",A  v_1 = A v_2 v_1 3\times 2 A 3\times 3 v_2 3\times 2 A^{-1} v_1 = v_2,"['linear-algebra', 'matrices', 'matrix-equations']"
26,Show that determinant of a symmetric matrix of order $4$ is $0$,Show that determinant of a symmetric matrix of order  is,4 0,"Without expanding prove that the determinant of the following matrix is $0$. $$ \begin{bmatrix}1^2 & 2^2 & 3^2 & 4^2\\2^2 & 3^2 & 4^2 & 5^2\\3^2 & 4^2 & 5^2 & 6^2\\4^2 & 5^2 & 6^2 & 7^2\end{bmatrix}$$ It is a symmetric matrix. I'm trying to show by operation that any two row or any two column are identical, but I'm unable to do that. Any hint.?","Without expanding prove that the determinant of the following matrix is $0$. $$ \begin{bmatrix}1^2 & 2^2 & 3^2 & 4^2\\2^2 & 3^2 & 4^2 & 5^2\\3^2 & 4^2 & 5^2 & 6^2\\4^2 & 5^2 & 6^2 & 7^2\end{bmatrix}$$ It is a symmetric matrix. I'm trying to show by operation that any two row or any two column are identical, but I'm unable to do that. Any hint.?",,"['linear-algebra', 'matrices', 'determinant']"
27,Why does $(A-I)^2=0$ implies all eigenvalues of $A$ are $1$?,Why does  implies all eigenvalues of  are ?,(A-I)^2=0 A 1,"Why does $(A-I)^2=0$ implies all eigenvalues of $A$ are $1$? Here $A$ is a $n \times n$ matrix. Write down the characteristic polynomial of $A$, which is $p(A)=(t-\lambda_1)(t-\lambda_2)\dots(t-\lambda_n)$. $1$ is one of its eigenvalues, but why all $\lambda$ are $1$?","Why does $(A-I)^2=0$ implies all eigenvalues of $A$ are $1$? Here $A$ is a $n \times n$ matrix. Write down the characteristic polynomial of $A$, which is $p(A)=(t-\lambda_1)(t-\lambda_2)\dots(t-\lambda_n)$. $1$ is one of its eigenvalues, but why all $\lambda$ are $1$?",,"['linear-algebra', 'matrices']"
28,Origin of Jacobian determinant [duplicate],Origin of Jacobian determinant [duplicate],,"This question already has answers here : Intuitive proof of multivariable changing of variables formula (Jacobian) without using mapping and/or measure theory? (5 answers) Closed 4 years ago . What is the origin of the Jacobian determinant for changing variables in multiple integrals? I mean, how to derive the formula for the Jacobian determinant? I have seen the use of Jacobian in some books but could not find how it arises.","This question already has answers here : Intuitive proof of multivariable changing of variables formula (Jacobian) without using mapping and/or measure theory? (5 answers) Closed 4 years ago . What is the origin of the Jacobian determinant for changing variables in multiple integrals? I mean, how to derive the formula for the Jacobian determinant? I have seen the use of Jacobian in some books but could not find how it arises.",,"['integration', 'matrices', 'multivariable-calculus', 'jacobian']"
29,proof of matrix singularity,proof of matrix singularity,,"If anyone can help me with the next question I would appreciate it a lot. Let $A$ and $B$ be $n*n$ matrices and let $C=A-B$. Show that if $Ax_0=Bx_0$ and  $x_0$ is not zero, then $C$ must be singular. The first thing I don't get is the notation, what do $Ax_0$ and $Bx_0$ mean? Thanks in advance :)","If anyone can help me with the next question I would appreciate it a lot. Let $A$ and $B$ be $n*n$ matrices and let $C=A-B$. Show that if $Ax_0=Bx_0$ and  $x_0$ is not zero, then $C$ must be singular. The first thing I don't get is the notation, what do $Ax_0$ and $Bx_0$ mean? Thanks in advance :)",,"['linear-algebra', 'matrices']"
30,Why is rank $uv^{T}$ always equal to 1? [duplicate],Why is rank  always equal to 1? [duplicate],uv^{T},"This question already has answers here : How to prove whether a matrix has rank $1$ (3 answers) Closed 6 years ago . Let $u \in \mathbb{R}^m$ and $v \in \mathbb{R}^n$ and $u,v \neq 0$. Since $u$ is an $m\times1$ matrix and $v^T$ is a $1\times n$ matrix, the product $uv^{T}$  is a $m\times n$ matrix. Then: $$\text{rank} (uv^{T}) =1$$ Just to illustrate: pick a random vector $u$ in $\mathbb{R}^4$, and $v$ in $\mathbb{R}^5$, for example: $u= (1, 2, 3, 4),$ and $v= (1, 2, 3, 4, 5)$. The rank of the matrix $m\times n$ is $1$ which can be checked by row-reducing. Chosing arbitrary vectors $u,v \neq 0$, the rank of the product matrix mxn of $uv^{T}$ is always $1$. Why is this? Any help would be greatly appreciated.","This question already has answers here : How to prove whether a matrix has rank $1$ (3 answers) Closed 6 years ago . Let $u \in \mathbb{R}^m$ and $v \in \mathbb{R}^n$ and $u,v \neq 0$. Since $u$ is an $m\times1$ matrix and $v^T$ is a $1\times n$ matrix, the product $uv^{T}$  is a $m\times n$ matrix. Then: $$\text{rank} (uv^{T}) =1$$ Just to illustrate: pick a random vector $u$ in $\mathbb{R}^4$, and $v$ in $\mathbb{R}^5$, for example: $u= (1, 2, 3, 4),$ and $v= (1, 2, 3, 4, 5)$. The rank of the matrix $m\times n$ is $1$ which can be checked by row-reducing. Chosing arbitrary vectors $u,v \neq 0$, the rank of the product matrix mxn of $uv^{T}$ is always $1$. Why is this? Any help would be greatly appreciated.",,"['linear-algebra', 'matrices', 'matrix-rank']"
31,Do similar matrices have equal singular values?,Do similar matrices have equal singular values?,,"Is it true that if $A$ and $B$ are similar matrices, $B=S^{-1}AS$, then $A$ and $B$ have the same singular values?","Is it true that if $A$ and $B$ are similar matrices, $B=S^{-1}AS$, then $A$ and $B$ have the same singular values?",,"['linear-algebra', 'matrices', 'svd']"
32,Isomorphic graphs,Isomorphic graphs,,"I was wondering if this solution for finding wheter or not two graphs are isomorphic would work: I claim that two graphs are isomorphic if  their degree list coincide. For example let's say that I have graphs A and B given by their adjacence matrix like so: $$ A = \begin{pmatrix} 0 &  1& 1 & 1 &0 \\  1 &  0& 0 & 0 &1 \\   1& 0 & 0 & 0 &0 \\   1& 0 & 0 & 0 &1 \\   0& 1 & 0 & 1 &0  \end{pmatrix} $$ $$B= \begin{pmatrix} 0 & 1 & 1 & 0 & 0\\  1 & 0 & 0 & 1 & 1\\  1 & 0 & 0 & 1 & 0\\  0 & 1 & 1 & 0 & 0\\  0 & 1 & 0 & 0 & 0 \end{pmatrix}$$ The degree list for A is 3,2,1,2,2 and for B is 2,3,2,2,1. This sets are equal. Therefore I say that A and B are isomorphic. If I am wrong, can you please explain me why is that with a counterexample","I was wondering if this solution for finding wheter or not two graphs are isomorphic would work: I claim that two graphs are isomorphic if  their degree list coincide. For example let's say that I have graphs A and B given by their adjacence matrix like so: $$ A = \begin{pmatrix} 0 &  1& 1 & 1 &0 \\  1 &  0& 0 & 0 &1 \\   1& 0 & 0 & 0 &0 \\   1& 0 & 0 & 0 &1 \\   0& 1 & 0 & 1 &0  \end{pmatrix} $$ $$B= \begin{pmatrix} 0 & 1 & 1 & 0 & 0\\  1 & 0 & 0 & 1 & 1\\  1 & 0 & 0 & 1 & 0\\  0 & 1 & 1 & 0 & 0\\  0 & 1 & 0 & 0 & 0 \end{pmatrix}$$ The degree list for A is 3,2,1,2,2 and for B is 2,3,2,2,1. This sets are equal. Therefore I say that A and B are isomorphic. If I am wrong, can you please explain me why is that with a counterexample",,"['matrices', 'graph-theory', 'graph-isomorphism']"
33,Does the equation $(AB-BA)^2=I_n$ have a solution for each $n\geq 3$?,Does the equation  have a solution for each ?,(AB-BA)^2=I_n n\geq 3,"Does the equation $(AB-BA)^2=I_n$ have a solution for each $n\geq 3$ ? $A$ and $B$ are matrices? I found out that the equation $(AB-BA)^m=I_n$ does have solution when $n=km$ where $k$ is an arbitrary integer number. It should be noted that $m>1$ is an integer number. To prove, we just need to consider $C=\operatorname{diag}(r_1,..., r_m)$ where $r_1,..., r_m$ are the roots of $x^m-1=0$ , i.e., the eigenvalues of matrix $C$ . In this case we know $\operatorname{trace}(C)=0$ , and so there are two matrices $A$ and $B$ such that $C=AB-BA$ , and also it is vivid that $C^m=I_m$ . For each $n=km$ , we can duplicate the matrix $C$ , $k$ times to get an $mk\times mk$ matrix to consider as new matrix $C$ .","Does the equation have a solution for each ? and are matrices? I found out that the equation does have solution when where is an arbitrary integer number. It should be noted that is an integer number. To prove, we just need to consider where are the roots of , i.e., the eigenvalues of matrix . In this case we know , and so there are two matrices and such that , and also it is vivid that . For each , we can duplicate the matrix , times to get an matrix to consider as new matrix .","(AB-BA)^2=I_n n\geq 3 A B (AB-BA)^m=I_n n=km k m>1 C=\operatorname{diag}(r_1,..., r_m) r_1,..., r_m x^m-1=0 C \operatorname{trace}(C)=0 A B C=AB-BA C^m=I_m n=km C k mk\times mk C","['linear-algebra', 'matrices', 'matrix-equations']"
34,How to obtain the sum of squared eigenvalues without finding the eigenvalues?,How to obtain the sum of squared eigenvalues without finding the eigenvalues?,,How to obtain sum of square of eigenvalues without finding eigenvalues of a matrix?,How to obtain sum of square of eigenvalues without finding eigenvalues of a matrix?,,"['matrices', 'eigenvalues-eigenvectors']"
35,How come least square can have many solutions?,How come least square can have many solutions?,,"I know there always exists a least-square solution $\hat{x}$, regardless of the properties of the matrix $A$. However, I keep finding online that least-square can have infinitely many solutions, if $A$ is not full column rank. Shouldn't $\hat{x}$ always be unique, as the minimization of a quadratic function (the error) always yields a global minima/maxima? Therefore, regardless of what the matrix $A$ is (even if it is a badly constructed matrix with dependent columns), least-square should find a single 'best' solution $\hat{x}$? Is there an easy (or intuitive) proof showing why would the least-square method produce infinitely many solutions if there are dependent columns?","I know there always exists a least-square solution $\hat{x}$, regardless of the properties of the matrix $A$. However, I keep finding online that least-square can have infinitely many solutions, if $A$ is not full column rank. Shouldn't $\hat{x}$ always be unique, as the minimization of a quadratic function (the error) always yields a global minima/maxima? Therefore, regardless of what the matrix $A$ is (even if it is a badly constructed matrix with dependent columns), least-square should find a single 'best' solution $\hat{x}$? Is there an easy (or intuitive) proof showing why would the least-square method produce infinitely many solutions if there are dependent columns?",,"['linear-algebra', 'matrices', 'regression', 'least-squares']"
36,Do positive-definite matrices always have real eigen values?,Do positive-definite matrices always have real eigen values?,,"Do positive-definite matrices always have real eigenvalues? I tried looking for examples of matrices without real eigenvalues (they would have even dimensions). But the examples I tend to see all have zero diagonal entries. So they are not positive definite. Would anyone have an example of positive-definite matrix without any real eigenvalue? Or it is a property of positive-definite matrices that they always have some real eigenvalues? EDIT: A matrix $A$ is positive definite iff $\forall x, x^TAx>0$. No symmetry is implied here.","Do positive-definite matrices always have real eigenvalues? I tried looking for examples of matrices without real eigenvalues (they would have even dimensions). But the examples I tend to see all have zero diagonal entries. So they are not positive definite. Would anyone have an example of positive-definite matrix without any real eigenvalue? Or it is a property of positive-definite matrices that they always have some real eigenvalues? EDIT: A matrix $A$ is positive definite iff $\forall x, x^TAx>0$. No symmetry is implied here.",,"['linear-algebra', 'matrices']"
37,How do I restrict k to ensure my matrix has exactly 3 distinct eigenvalues?,How do I restrict k to ensure my matrix has exactly 3 distinct eigenvalues?,,"$$A=\begin{bmatrix}-1&-1&0\\-12&3&-1\\k&0&0\end{bmatrix}$$ How do I restrict $k$ to ensure that my matrix has 3 distinct real eigenvalues? I tried going about it the long way by using the characteristic polynomial and factorizing, but I'm sure there is a faster way.","$$A=\begin{bmatrix}-1&-1&0\\-12&3&-1\\k&0&0\end{bmatrix}$$ How do I restrict $k$ to ensure that my matrix has 3 distinct real eigenvalues? I tried going about it the long way by using the characteristic polynomial and factorizing, but I'm sure there is a faster way.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'vectors', 'linear-transformations']"
38,Can a product of symmetric matrices give a non-zero skew symmetric matrix?,Can a product of symmetric matrices give a non-zero skew symmetric matrix?,,"I've been trying to find symmetric $\mathbf{A},\mathbf{B}$ such that $\mathbf{AB}$ is skew-symmetric, but it seems that no matter what I try, I end up forcing $\mathbf{AB}=\mathbf{0}$. Is it possible for this product to be non-zero? I've also tried proving that $\mathbf{AB}$ must be $\mathbf{0}$ but haven't got much further than $\mathbf{AB}=-\mathbf{BA}$.","I've been trying to find symmetric $\mathbf{A},\mathbf{B}$ such that $\mathbf{AB}$ is skew-symmetric, but it seems that no matter what I try, I end up forcing $\mathbf{AB}=\mathbf{0}$. Is it possible for this product to be non-zero? I've also tried proving that $\mathbf{AB}$ must be $\mathbf{0}$ but haven't got much further than $\mathbf{AB}=-\mathbf{BA}$.",,"['linear-algebra', 'matrices']"
39,Gauss Elimination - Diagonal dominant matrices don't need row changes,Gauss Elimination - Diagonal dominant matrices don't need row changes,,"I was asked to prove the following statement: let $A$ be an $n$ by $n$ matrix with real entries such that $\forall k \in \mathbb N, k\leq n$: $$\sum_{i \neq k} |A_{i,k}| < |A_{kk}|$$ Show that if we were to do gauss elimination (or LU factorization) of $A$, then there will be no need for row changes, no need for partial pivoting. I don't see why this is true, I'd appreciate a hint in the right direction. Maybe I should take a general $n$ by $n$ matrix that is diagonly dominant, try to $LU$ factor it and see that I don't need row changes? is this the way?","I was asked to prove the following statement: let $A$ be an $n$ by $n$ matrix with real entries such that $\forall k \in \mathbb N, k\leq n$: $$\sum_{i \neq k} |A_{i,k}| < |A_{kk}|$$ Show that if we were to do gauss elimination (or LU factorization) of $A$, then there will be no need for row changes, no need for partial pivoting. I don't see why this is true, I'd appreciate a hint in the right direction. Maybe I should take a general $n$ by $n$ matrix that is diagonly dominant, try to $LU$ factor it and see that I don't need row changes? is this the way?",,"['matrices', 'gaussian-elimination']"
40,Is matrix multiplication really a group operator?,Is matrix multiplication really a group operator?,,"A group has an operation that can be performed over ANY two elements in a set. Given that an $n \times m$ matrix can only be multiplied by an $m \times o$ matrix, doesn't that mean that matrix multiplication can't be a group operator except for sets of commonly sized square matrices? I ask this because it's called linear algebra yet this aspect seems inconsistent with groups.","A group has an operation that can be performed over ANY two elements in a set. Given that an $n \times m$ matrix can only be multiplied by an $m \times o$ matrix, doesn't that mean that matrix multiplication can't be a group operator except for sets of commonly sized square matrices? I ask this because it's called linear algebra yet this aspect seems inconsistent with groups.",,"['abstract-algebra', 'group-theory', 'matrices']"
41,Solutions to $\mathbf{AX}=\mathbf{B}$,Solutions to,\mathbf{AX}=\mathbf{B},"Let $\mathbf{A},\mathbf{B}$ be $n\times n$ matrices over a field $\mathbb{F}$. How can we find if there exist a $n\times n$ matrix $\mathbf X$ s.t. $\mathbf{AX}=\mathbf{B}$? (and how can we find $\mathbf X$ if it exists?) Note: if $|\mathbf A|\neq 0$ then it's easy since $\mathbf{X}=\mathbf{A}^{-1}\mathbf{B}$, but I stumbled on a problem where my $\mathbf A$ is not invertible.","Let $\mathbf{A},\mathbf{B}$ be $n\times n$ matrices over a field $\mathbb{F}$. How can we find if there exist a $n\times n$ matrix $\mathbf X$ s.t. $\mathbf{AX}=\mathbf{B}$? (and how can we find $\mathbf X$ if it exists?) Note: if $|\mathbf A|\neq 0$ then it's easy since $\mathbf{X}=\mathbf{A}^{-1}\mathbf{B}$, but I stumbled on a problem where my $\mathbf A$ is not invertible.",,"['linear-algebra', 'matrices']"
42,Find matrices $A$ and $B$ given $AB$ and $BA$,Find matrices  and  given  and,A B AB BA,"Given that: $$AB= \left[ {\matrix{    3 & 1 \cr     2 & 1 \cr    } } \right]$$ and $$BA= \left[ {\matrix{    5 & 3 \cr     -2 & -1 \cr    } } \right]$$ find $A$ and $B$.","Given that: $$AB= \left[ {\matrix{    3 & 1 \cr     2 & 1 \cr    } } \right]$$ and $$BA= \left[ {\matrix{    5 & 3 \cr     -2 & -1 \cr    } } \right]$$ find $A$ and $B$.",,['matrices']
43,The longest chain of subgroups for real matrices,The longest chain of subgroups for real matrices,,"I would like to know what is the longest possible chain of subgroups starting from the group of real invertible matrices. Of course, I suppose there many possible chains, but the most valuable, let's assume, would be that  with the greatest number of subgroups. My initial proposition is as follows: Group of invertible real matrices $GL_n(R)$ Subgroup of invertible real matrices with rational determinant. Subgroup of invertible real matrices with determinant equal to $1$ or $-1$ . Subgroup of orthogonal matrices. Subgroup of rotation matrices (determinant equal to $1$ ) Subgroup of rotation matrices with rational elements. Subgroup of rotation matrices with elements only $1,0,-1$ . I hope all the listed subgroups are satisfying definition of subgroup. So we have group $GL_n(R)$ and 6 subgroups where every listed group is a subgroup of it's predecessor . Such list I name a chain. Can we present a longer chain of subgroups? It is allowable to increase resolution of the given chain (if it is possible) or to give other longer chain, starting from $GL_n(R)$ . Parallel branches are not allowable, only single simple chain.","I would like to know what is the longest possible chain of subgroups starting from the group of real invertible matrices. Of course, I suppose there many possible chains, but the most valuable, let's assume, would be that  with the greatest number of subgroups. My initial proposition is as follows: Group of invertible real matrices Subgroup of invertible real matrices with rational determinant. Subgroup of invertible real matrices with determinant equal to or . Subgroup of orthogonal matrices. Subgroup of rotation matrices (determinant equal to ) Subgroup of rotation matrices with rational elements. Subgroup of rotation matrices with elements only . I hope all the listed subgroups are satisfying definition of subgroup. So we have group and 6 subgroups where every listed group is a subgroup of it's predecessor . Such list I name a chain. Can we present a longer chain of subgroups? It is allowable to increase resolution of the given chain (if it is possible) or to give other longer chain, starting from . Parallel branches are not allowable, only single simple chain.","GL_n(R) 1 -1 1 1,0,-1 GL_n(R) GL_n(R)","['linear-algebra', 'matrices', 'group-theory']"
44,Calculating a matrix-exponential [duplicate],Calculating a matrix-exponential [duplicate],,This question already has answers here : Finding the matrix exponential (3 answers) The matrix $e^A$ is defined by $e^A=\Sigma_{k=0}^{\infty}\frac {A^k}{k!}$ Suppose M=$\begin{bmatrix}1 & 1\\0 & 1\end{bmatrix}$. Calculate $e^M$ (3 answers) Closed 2 years ago . Let A be the following matrix. $$\begin{pmatrix} 1 & 1 \\ 0 & 1 \\ \end{pmatrix} $$ I have to calculate $e^A$ . My idea was to diagonalize A because then $e^A = Pe^DP^-1$ if $A = PDP^-1$ . But A cannot be diagonalized since 1 is a double eigenvalue and therefore A does not have 2 linearly independent eigenvectors. How else can I calculate $e^A$ ? Thank you!,This question already has answers here : Finding the matrix exponential (3 answers) The matrix $e^A$ is defined by $e^A=\Sigma_{k=0}^{\infty}\frac {A^k}{k!}$ Suppose M=$\begin{bmatrix}1 & 1\\0 & 1\end{bmatrix}$. Calculate $e^M$ (3 answers) Closed 2 years ago . Let A be the following matrix. I have to calculate . My idea was to diagonalize A because then if . But A cannot be diagonalized since 1 is a double eigenvalue and therefore A does not have 2 linearly independent eigenvectors. How else can I calculate ? Thank you!,"\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}
 e^A e^A = Pe^DP^-1 A = PDP^-1 e^A","['linear-algebra', 'matrices', 'matrix-exponential']"
45,How many matrices satisfy this equality?,How many matrices satisfy this equality?,,How many matrices $A\in\mathcal{M}_{3\times 3} (\mathbb{N})$ satisfy this equality? $$\begin{pmatrix} 1 \ \ 2 \ \ 4 \end{pmatrix}\cdot A=\begin{pmatrix} 3 \ \ 2 \ \ 1 \end{pmatrix}$$ I tried with examples and I found just one but I want to know how to approach this exercise.The right answer is $3$,How many matrices satisfy this equality? I tried with examples and I found just one but I want to know how to approach this exercise.The right answer is,"A\in\mathcal{M}_{3\times 3} (\mathbb{N}) \begin{pmatrix}
1 \ \ 2 \ \ 4
\end{pmatrix}\cdot A=\begin{pmatrix}
3 \ \ 2 \ \ 1
\end{pmatrix} 3","['linear-algebra', 'matrices']"
46,"Prove or disprove: If $A$ is $n\times n$ and $\exists\;m\in \Bbb{N}:\;A^m=I_n$, then $A$ is invertible.","Prove or disprove: If  is  and , then  is invertible.",A n\times n \exists\;m\in \Bbb{N}:\;A^m=I_n A,"Is this statement true? If $A$ is an $n\times n$ matrix and $A^m=I_n$ for some $m\in \Bbb{N}$ , then $A$ is invertible. My trial Let $n\in \Bbb{N}$ be fixed. Then, $$[\det(A)]^m=\det(A^m)=I_n=1.$$ Hence, $$\det(A)=1\neq 0.$$ Thus, $A$ is invertible since $\det(A)\neq 0.$ . I'm I right or is there a counter-example?","Is this statement true? If is an matrix and for some , then is invertible. My trial Let be fixed. Then, Hence, Thus, is invertible since . I'm I right or is there a counter-example?",A n\times n A^m=I_n m\in \Bbb{N} A n\in \Bbb{N} [\det(A)]^m=\det(A^m)=I_n=1. \det(A)=1\neq 0. A \det(A)\neq 0.,"['matrices', 'algebra-precalculus', 'matrix-equations']"
47,"Show that, for square matrices $A$ and $B$, $A+B=AB$ implies $AB=BA$. [duplicate]","Show that, for square matrices  and ,  implies . [duplicate]",A B A+B=AB AB=BA,This question already has answers here : Matrices A+B=AB implies A commutes with B (2 answers) Closed 5 years ago . Let $A$ and $B$ be two $n$ -by- $n$ real matrices such that $A+B = AB$ .   How do I prove that $AB= BA$ ? I have tried using the trace function on $A+B-AB$ . But I could not get any Ideas. Kindly provide me with hints.,This question already has answers here : Matrices A+B=AB implies A commutes with B (2 answers) Closed 5 years ago . Let and be two -by- real matrices such that .   How do I prove that ? I have tried using the trace function on . But I could not get any Ideas. Kindly provide me with hints.,A B n n A+B = AB AB= BA A+B-AB,"['linear-algebra', 'matrices']"
48,When does trace and determinant of a 2 x 2 matrix equal each other? (Linear Algebra),When does trace and determinant of a 2 x 2 matrix equal each other? (Linear Algebra),,"Background Information: I am new to linear algebra, and I recently came across this homework question that I am confused about. I appreciate any explanation that can help me improve my solution. Question: What condition on the entries of a 2x2 matrix A means Tr(A) = det(A)? Provide two distinct examples of 2x2 matrices which satisfy this. My approach (Not Complete): Considering the following 2 x 2 matrix, the det(A) = 4, and Tr(A) = 4 \begin{bmatrix}     2       & 0\\     0    &   2 \end{bmatrix} However, considering this 2 x 2 matrix, the det(A) = 9, and Tr(A) = 6 \begin{bmatrix}     3       & 0\\     0    &   3 \end{bmatrix} I think the condition would be having 2 x 2 matrix such that the matrix is  (symmetric) and (n = 2). \begin{bmatrix}     n       & 0\\     0    &   n \end{bmatrix} My solution makes sense, but I feel it is incomplete. Am I missing a key point or a concept that I can add to my answer? Edited: I have tried this solution with so many numbers and it seems to work. Would this be an acceptable solution? \begin{bmatrix}     a       & b\\     c    &   d \end{bmatrix} such that a = c = d and b = c - 2, so here is an example \begin{bmatrix}     5       & 3\\     5    &   5 \end{bmatrix} det(A) = 25 - 15 = 10 , and Tr(A) = 5 + 5 = 10","Background Information: I am new to linear algebra, and I recently came across this homework question that I am confused about. I appreciate any explanation that can help me improve my solution. Question: What condition on the entries of a 2x2 matrix A means Tr(A) = det(A)? Provide two distinct examples of 2x2 matrices which satisfy this. My approach (Not Complete): Considering the following 2 x 2 matrix, the det(A) = 4, and Tr(A) = 4 \begin{bmatrix}     2       & 0\\     0    &   2 \end{bmatrix} However, considering this 2 x 2 matrix, the det(A) = 9, and Tr(A) = 6 \begin{bmatrix}     3       & 0\\     0    &   3 \end{bmatrix} I think the condition would be having 2 x 2 matrix such that the matrix is  (symmetric) and (n = 2). \begin{bmatrix}     n       & 0\\     0    &   n \end{bmatrix} My solution makes sense, but I feel it is incomplete. Am I missing a key point or a concept that I can add to my answer? Edited: I have tried this solution with so many numbers and it seems to work. Would this be an acceptable solution? \begin{bmatrix}     a       & b\\     c    &   d \end{bmatrix} such that a = c = d and b = c - 2, so here is an example \begin{bmatrix}     5       & 3\\     5    &   5 \end{bmatrix} det(A) = 25 - 15 = 10 , and Tr(A) = 5 + 5 = 10",,"['linear-algebra', 'matrices', 'determinant', 'trace']"
49,"If $AB = -BA$, then tr$(A) = $ tr$(B) = 0 $","If , then tr tr",AB = -BA (A) =  (B) = 0 ,"If $A, B$ are invertible matrices such that $AB = -BA$ ( which implies tr$(AB) =$ tr$(-BA)$), then show that tr$(A) = $ tr$(B) = 0$. My attempt: I could only show that tr$(AB) = 0$.","If $A, B$ are invertible matrices such that $AB = -BA$ ( which implies tr$(AB) =$ tr$(-BA)$), then show that tr$(A) = $ tr$(B) = 0$. My attempt: I could only show that tr$(AB) = 0$.",,"['linear-algebra', 'matrices', 'trace']"
50,Matrix Exponentiation $A^{15}$,Matrix Exponentiation,A^{15},"A is a 2x2 matrix. Let it satisfy $A^2 = A - I$, where $I = \begin{bmatrix} 1 & 0\\ 0 & 1\end{bmatrix}$. I want to find $A^{15}$ : My Approach: I isolated $A$: $A^2-A=-I$ $A(A-I)=-I$ From that, I just tried to solve the system that would generate: $\begin{bmatrix}a & b \\ c & d\end{bmatrix}\begin{bmatrix}a-1 & b \\ c & d-1\end{bmatrix} = \begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}$ Turns out this doesn't help me get closer to the answer as it leads to: $\begin{bmatrix}a(a-1)+bc & b(d-1)+ab\\c\left(a-1\right)+dc &d(d-1)+bc\end{bmatrix} = \begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}$ Is that the correct approach, or else, which property am I missing in order to solve this problem?","A is a 2x2 matrix. Let it satisfy $A^2 = A - I$, where $I = \begin{bmatrix} 1 & 0\\ 0 & 1\end{bmatrix}$. I want to find $A^{15}$ : My Approach: I isolated $A$: $A^2-A=-I$ $A(A-I)=-I$ From that, I just tried to solve the system that would generate: $\begin{bmatrix}a & b \\ c & d\end{bmatrix}\begin{bmatrix}a-1 & b \\ c & d-1\end{bmatrix} = \begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}$ Turns out this doesn't help me get closer to the answer as it leads to: $\begin{bmatrix}a(a-1)+bc & b(d-1)+ab\\c\left(a-1\right)+dc &d(d-1)+bc\end{bmatrix} = \begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}$ Is that the correct approach, or else, which property am I missing in order to solve this problem?",,"['matrices', 'exponentiation']"
51,Is there a name for $\max \| A x \|$ for all $\|x\|=1?$,Is there a name for  for all,\max \| A x \| \|x\|=1?,"Is there a name for $\max \| A x \|$ for all $\|x\|=1?$ ($A$ is a matrix, and $x$ is a vector) One might be inclined to guess that it's the spectral radius of $A$, but that's not true. I'm wondering if there is a special term for this property of $A$.","Is there a name for $\max \| A x \|$ for all $\|x\|=1?$ ($A$ is a matrix, and $x$ is a vector) One might be inclined to guess that it's the spectral radius of $A$, but that's not true. I'm wondering if there is a special term for this property of $A$.",,"['linear-algebra', 'matrices']"
52,Express a linear map in matrix form,Express a linear map in matrix form,,"Is there anyway to express the linear map $x\mapsto (a^Tx)b, \, a,b,x\in\Bbb R^n$ in matrix form $x\mapsto Ax$? Context: I'm trying to solve an equation in $x\in\Bbb R^n$: $$ab^Tx-(a^Tx)b=c$$ If I'm able to express $(a^Tx)b$ in martrix form $Ax$ I'm done (we don't need to care about existence or uniqueness of solutions since from other aspects it's guaranteed $(ab^T - A)$ will be invetible. Best regards.","Is there anyway to express the linear map $x\mapsto (a^Tx)b, \, a,b,x\in\Bbb R^n$ in matrix form $x\mapsto Ax$? Context: I'm trying to solve an equation in $x\in\Bbb R^n$: $$ab^Tx-(a^Tx)b=c$$ If I'm able to express $(a^Tx)b$ in martrix form $Ax$ I'm done (we don't need to care about existence or uniqueness of solutions since from other aspects it's guaranteed $(ab^T - A)$ will be invetible. Best regards.",,"['linear-algebra', 'matrices']"
53,Inverse of a specific lower triangular matrix,Inverse of a specific lower triangular matrix,,"How can I find the inverse of a n-by-n lower triangular matrix with diagonal elements all equal to 2, and lower triangular elements all equal to one? \begin{bmatrix}   2&&&& \\   1&2&&&\\   1&1&2&&\\   \vdots& \ddots & \ddots & \ddots &\\   1&\dots&1&1&2   \end{bmatrix}","How can I find the inverse of a n-by-n lower triangular matrix with diagonal elements all equal to 2, and lower triangular elements all equal to one? \begin{bmatrix}   2&&&& \\   1&2&&&\\   1&1&2&&\\   \vdots& \ddots & \ddots & \ddots &\\   1&\dots&1&1&2   \end{bmatrix}",,"['matrices', 'inverse']"
54,Dimension of the kernel of a square matrix.,Dimension of the kernel of a square matrix.,,If I have an $n$ by $n$ matrix. Is the dimension of the kernel always equal to the number of zero rows when a matrix is in rref form? I believe it is but if so why is this the case and what would be a proof? Thanks.,If I have an $n$ by $n$ matrix. Is the dimension of the kernel always equal to the number of zero rows when a matrix is in rref form? I believe it is but if so why is this the case and what would be a proof? Thanks.,,"['linear-algebra', 'matrices', 'linear-transformations', 'gaussian-elimination']"
55,Does the linear transformation that a matrix encodes depend on a choice of basis?,Does the linear transformation that a matrix encodes depend on a choice of basis?,,"Context Let $M$ be an $m \times n$ matrix of real numbers. Let $\mathbf{x}$ be column vector of length $n$ with elements $x_1, \ldots , x_n \in \mathbb{R}$. Let $\vec{x} = (x_1, \ldots , x_n) \in \mathbb{R}^n$ be its analogue in $\mathbb{R}^n$. Let $\vec{\mathbf{e}}^t$ denote the standard basis in $\mathbb{R}^n$ so that $$ \vec{\mathbf{e}} =  \left[ \begin{array} .\vec{e_1} \\ \vec{e_2} \\ \vdots \\ \vec{e_n} \end{array} \right] $$ and let $\vec{\mathbf{b}}^t$ denote a non-standard basis in $\mathbb{R}^n$ with $$ \vec{\mathbf{b}} =  \left[ \begin{array} .\vec{b_1} \\ \vec{b_2} \\ \vdots \\ \vec{b_n} \end{array} \right] $$ Fact from Linear Algebra: $M$ encodes a linear transformation $T:  \mathbb{R}^n \rightarrow \mathbb{R}^m$ such that $$ M \mathbf{x} = \mathbf{y} $$ where $\mathbf{\vec{e}}^t M \mathbf{x} = T(\vec{x}) = \vec{y}$ is some vector in $\mathbf{R}^m$. Question Does the linear transformation $T$ that $M$ corresponds to depend on our choice of basis? That is, if instead of working with  $\mathbf{\vec{e}}^t$  we instead worked with $\mathbf{\vec{b}}^t$, would $M$ encode a different linear transformation? For example, if $$ \mathbf{\vec{e}}^t \mathbf{x} = \vec{x} = \mathbf{\vec{b}}^t \mathbf{x'} $$ then do we have that $$ \mathbf{\vec{e}}^t M \mathbf{x} = T(\vec{x}) = \mathbf{\vec{b}}^t M \mathbf{x'}? $$ EDIT Ok. Upon reflection, it is obvious that our choice of basis matters significantly. For example, let us work in $\mathbb{R}^2$ and let our non-standard basis be $(1,1)$ and $(1, -1)$. Then the identity matrix $$ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} $$ will now send $(1,0)$ to $(1,1)$ since $$ \begin{bmatrix} (1, 1) & (1, -1) \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \left[ \begin{array} & 1 \\ 0 \end{array} \right] = (1,1) $$ which is clearly a different mapping than the identity mapping that would result were we to use the standard basis (where $(1, 0) \mapsto (1,0)$).","Context Let $M$ be an $m \times n$ matrix of real numbers. Let $\mathbf{x}$ be column vector of length $n$ with elements $x_1, \ldots , x_n \in \mathbb{R}$. Let $\vec{x} = (x_1, \ldots , x_n) \in \mathbb{R}^n$ be its analogue in $\mathbb{R}^n$. Let $\vec{\mathbf{e}}^t$ denote the standard basis in $\mathbb{R}^n$ so that $$ \vec{\mathbf{e}} =  \left[ \begin{array} .\vec{e_1} \\ \vec{e_2} \\ \vdots \\ \vec{e_n} \end{array} \right] $$ and let $\vec{\mathbf{b}}^t$ denote a non-standard basis in $\mathbb{R}^n$ with $$ \vec{\mathbf{b}} =  \left[ \begin{array} .\vec{b_1} \\ \vec{b_2} \\ \vdots \\ \vec{b_n} \end{array} \right] $$ Fact from Linear Algebra: $M$ encodes a linear transformation $T:  \mathbb{R}^n \rightarrow \mathbb{R}^m$ such that $$ M \mathbf{x} = \mathbf{y} $$ where $\mathbf{\vec{e}}^t M \mathbf{x} = T(\vec{x}) = \vec{y}$ is some vector in $\mathbf{R}^m$. Question Does the linear transformation $T$ that $M$ corresponds to depend on our choice of basis? That is, if instead of working with  $\mathbf{\vec{e}}^t$  we instead worked with $\mathbf{\vec{b}}^t$, would $M$ encode a different linear transformation? For example, if $$ \mathbf{\vec{e}}^t \mathbf{x} = \vec{x} = \mathbf{\vec{b}}^t \mathbf{x'} $$ then do we have that $$ \mathbf{\vec{e}}^t M \mathbf{x} = T(\vec{x}) = \mathbf{\vec{b}}^t M \mathbf{x'}? $$ EDIT Ok. Upon reflection, it is obvious that our choice of basis matters significantly. For example, let us work in $\mathbb{R}^2$ and let our non-standard basis be $(1,1)$ and $(1, -1)$. Then the identity matrix $$ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} $$ will now send $(1,0)$ to $(1,1)$ since $$ \begin{bmatrix} (1, 1) & (1, -1) \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \left[ \begin{array} & 1 \\ 0 \end{array} \right] = (1,1) $$ which is clearly a different mapping than the identity mapping that would result were we to use the standard basis (where $(1, 0) \mapsto (1,0)$).",,"['linear-algebra', 'matrices']"
56,What is the derivative of $f (\mathrm X) = \mathrm X^3$?,What is the derivative of ?,f (\mathrm X) = \mathrm X^3,Let $E = M_{3}(\mathbb{R})$ and $f: E \rightarrow E$ which $f(X) = X^3$ is $f^{'}(X)H = 3X^2 H$ the derivative for this function? I tried to prove that $r(H) \rightarrow 0$ where $r(H) = -X^3 + (X+H)^3 - 3X^2 H$ but i dont get in anywhere.,Let $E = M_{3}(\mathbb{R})$ and $f: E \rightarrow E$ which $f(X) = X^3$ is $f^{'}(X)H = 3X^2 H$ the derivative for this function? I tried to prove that $r(H) \rightarrow 0$ where $r(H) = -X^3 + (X+H)^3 - 3X^2 H$ but i dont get in anywhere.,,"['matrices', 'analysis', 'derivatives', 'matrix-calculus']"
57,Find a linear operator s.t. $A^{2}=A^{3}$ but $A^{2}\neq A$?,Find a linear operator s.t.  but ?,A^{2}=A^{3} A^{2}\neq A,"From Halmos's Finite-Dimensional Vector Spaces, question 6a section 43, the section after projections. Find a linear transformation A such that $A^{2}(1-A)=0$ but A is not idempotent (I remember A is idempotent iff it is a projection). I had no luck.","From Halmos's Finite-Dimensional Vector Spaces, question 6a section 43, the section after projections. Find a linear transformation A such that $A^{2}(1-A)=0$ but A is not idempotent (I remember A is idempotent iff it is a projection). I had no luck.",,['linear-algebra']
58,"Find $\det(A^{2}+A^{T})$ when eigenvalues are $1,2,3$",Find  when eigenvalues are,"\det(A^{2}+A^{T}) 1,2,3","We have to find $\det(A^{2}+A^{T})$. It is given that eigenvalues of $A$ are $1,2,3$. My attempt: Since the question implicitly states that the answer would be same for all $A$ with eigenvalues $1,2,3$ we find that the answer is $(1+1)*(2+4)*(3+9)$ by taking $A$ as a diagonal matrix :P I am unable to prove it for general $A$. Any help will be appreciated.","We have to find $\det(A^{2}+A^{T})$. It is given that eigenvalues of $A$ are $1,2,3$. My attempt: Since the question implicitly states that the answer would be same for all $A$ with eigenvalues $1,2,3$ we find that the answer is $(1+1)*(2+4)*(3+9)$ by taking $A$ as a diagonal matrix :P I am unable to prove it for general $A$. Any help will be appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
59,Why is the left inverse of a matrix equal to the right inverse? [duplicate],Why is the left inverse of a matrix equal to the right inverse? [duplicate],,"This question already has answers here : Looking for insightful explanation as to why right inverse equals left inverse for square invertible  matrices (2 answers) If $AB = I$ then $BA = I$ (34 answers) Closed 9 years ago . Given a square matrix $A$ that has full row rank we know that the matrix is invertible. So there is a matrix $B$ such that $$ AB=1 $$ writing this in component notation, $$ A_{ij}B_{jk}=\delta_{ik} $$ Now, we tend to write $A^{-1}$ instead of $B$ but let's leave it like that for now. My question is how can we show that $BA=1$? We mechanically jump to the conclusion that if the inverse exists, $AA^{-1}=A^{-1}A=1$ but how to show that? Equivalently why is the left inverse equal to the right inverse? It seems intuitively obvious! Thanks a bunch, I appreciate.","This question already has answers here : Looking for insightful explanation as to why right inverse equals left inverse for square invertible  matrices (2 answers) If $AB = I$ then $BA = I$ (34 answers) Closed 9 years ago . Given a square matrix $A$ that has full row rank we know that the matrix is invertible. So there is a matrix $B$ such that $$ AB=1 $$ writing this in component notation, $$ A_{ij}B_{jk}=\delta_{ik} $$ Now, we tend to write $A^{-1}$ instead of $B$ but let's leave it like that for now. My question is how can we show that $BA=1$? We mechanically jump to the conclusion that if the inverse exists, $AA^{-1}=A^{-1}A=1$ but how to show that? Equivalently why is the left inverse equal to the right inverse? It seems intuitively obvious! Thanks a bunch, I appreciate.",,"['linear-algebra', 'matrices', 'inverse']"
60,Prove that $AB-BA = I$ has no solution in $M_{n\times n}(\mathbb R)$ without using matrix trace,Prove that  has no solution in  without using matrix trace,AB-BA = I M_{n\times n}(\mathbb R),"The title is self-explanatory. Prove that $AB-BA = I$ has no solution in $M_{n\times n}(\mathbb R)$ without using matrix trace. A,B are both from $M_{n\times n}(\mathbb R)$ and $AB$ is matrix multiplication.  I am aware that these matrices' traces coincide and hence $tr(AB-BA) = 0$, while $tr(I) = n$ but are there other ways to solve this?","The title is self-explanatory. Prove that $AB-BA = I$ has no solution in $M_{n\times n}(\mathbb R)$ without using matrix trace. A,B are both from $M_{n\times n}(\mathbb R)$ and $AB$ is matrix multiplication.  I am aware that these matrices' traces coincide and hence $tr(AB-BA) = 0$, while $tr(I) = n$ but are there other ways to solve this?",,"['linear-algebra', 'matrices', 'matrix-equations']"
61,"Find two $2 \times 2$ matrices $A$ and $B$ with the same rank, determinant,...but they are not similar.","Find two  matrices  and  with the same rank, determinant,...but they are not similar.",2 \times 2 A B,"Find two $2 \times 2$ matrices $A$ and $B$ with the same rank, determinant, trace and characteristic polynomial, but that are not similar to each other. I come up with two matrices: $A=\begin{pmatrix} 0 &1 \\  0 &0  \end{pmatrix}$ and $B=\begin{pmatrix} 0 &0 \\  1 &0  \end{pmatrix}.$ It is easy to check that they have same rank, determinant, trace and characteristic polynomial. However, my question is I do not know how to prove two matrices are similar or not. I have learnt the converse in my textbook, i.e. If two matrices are similar, they have the same determinant, characteristic polynomial,etc. I have also known that (but I do now know the proof), we can check by using Jordan form of two matrices. I do not know if this claim is correct: ""If two matrices have the same Jordan form, they are similar to each other."" Yet, go back to the question, is it a quick way to prove? Thank you in advance.","Find two matrices and with the same rank, determinant, trace and characteristic polynomial, but that are not similar to each other. I come up with two matrices: and It is easy to check that they have same rank, determinant, trace and characteristic polynomial. However, my question is I do not know how to prove two matrices are similar or not. I have learnt the converse in my textbook, i.e. If two matrices are similar, they have the same determinant, characteristic polynomial,etc. I have also known that (but I do now know the proof), we can check by using Jordan form of two matrices. I do not know if this claim is correct: ""If two matrices have the same Jordan form, they are similar to each other."" Yet, go back to the question, is it a quick way to prove? Thank you in advance.","2 \times 2 A B A=\begin{pmatrix}
0 &1 \\ 
0 &0 
\end{pmatrix} B=\begin{pmatrix}
0 &0 \\ 
1 &0 
\end{pmatrix}.","['linear-algebra', 'matrices']"
62,"If a matrix is not invertible, is it still possible to find a left and/or right inverse?","If a matrix is not invertible, is it still possible to find a left and/or right inverse?",,"I was recently asked to find the right inverse of some matrixes. I found that all three of them were invertible, so it was just a matter of finding their inverses, which would be exactly the same as the right inverses. What if a matrix is not invertible? Basically, what I want to know is, if a matrix is not invertible, does it mean that there are no left and/or right inverses at all ? That is, $$|A| = 0 \iff \not \exists B(AB = I \ \lor \ BA = I)$$","I was recently asked to find the right inverse of some matrixes. I found that all three of them were invertible, so it was just a matter of finding their inverses, which would be exactly the same as the right inverses. What if a matrix is not invertible? Basically, what I want to know is, if a matrix is not invertible, does it mean that there are no left and/or right inverses at all ? That is, $$|A| = 0 \iff \not \exists B(AB = I \ \lor \ BA = I)$$",,"['linear-algebra', 'matrices', 'inverse']"
63,"If $A^2+A=0$,then $\lambda=1$ cannot be an eigenvalue of A.","If ,then  cannot be an eigenvalue of A.",A^2+A=0 \lambda=1,"Prove the following statement: If $A^2+A=0$ ,then $\lambda=1$ cannot be an eigenvalue of A. I've been struggling on this question for a couple of hours and don't know how to approach it.","Prove the following statement: If ,then cannot be an eigenvalue of A. I've been struggling on this question for a couple of hours and don't know how to approach it.",A^2+A=0 \lambda=1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'vectors']"
64,Determine the determinant and the inverse to the matrix A.,Determine the determinant and the inverse to the matrix A.,,"I am currently working on determining the determinant and the inverse to the matrix A. $A =\begin{bmatrix} 1 & 0 & 0 & 0 & 0\\0 & 1 & 0 & 0 & 0\\0 & 0 & 1 & 0 & 0\\0 & 0 & 0 & a & b\\0 & 0 & 0 & c & d\\\end{bmatrix}$ The determinant is $ad-bc$. But how do I find out the inverse? I guess there should be some kind of way to see what the inverse would be... Any help would be much appreciated, as I am a beginner in this area a good explanation is always helpful :)!","I am currently working on determining the determinant and the inverse to the matrix A. $A =\begin{bmatrix} 1 & 0 & 0 & 0 & 0\\0 & 1 & 0 & 0 & 0\\0 & 0 & 1 & 0 & 0\\0 & 0 & 0 & a & b\\0 & 0 & 0 & c & d\\\end{bmatrix}$ The determinant is $ad-bc$. But how do I find out the inverse? I guess there should be some kind of way to see what the inverse would be... Any help would be much appreciated, as I am a beginner in this area a good explanation is always helpful :)!",,"['linear-algebra', 'matrices']"
65,Find a matrix $B$ such that $B^2=A$,Find a matrix  such that,B B^2=A,"$A= \begin{pmatrix} 1 & 2+3i \\ 2-3i & -1 \end{pmatrix}$ What is matrix $B$ such that $B^2=A$? Its eigenvalues are $\sqrt{14}, -\sqrt{14}$ and I tried to use formula $B=U\sqrt{\lambda}U^{H}$ where $U$ is unitary matrix. But then $\sqrt{　}$ of $-\sqrt{14}$ is not possible. How can I solve this?","$A= \begin{pmatrix} 1 & 2+3i \\ 2-3i & -1 \end{pmatrix}$ What is matrix $B$ such that $B^2=A$? Its eigenvalues are $\sqrt{14}, -\sqrt{14}$ and I tried to use formula $B=U\sqrt{\lambda}U^{H}$ where $U$ is unitary matrix. But then $\sqrt{　}$ of $-\sqrt{14}$ is not possible. How can I solve this?",,"['linear-algebra', 'matrices', 'radicals']"
66,How to show that $A^3+A=0 \Rightarrow \text{Trace}(A)=0$ when $A$ is a real matrix,How to show that  when  is a real matrix,A^3+A=0 \Rightarrow \text{Trace}(A)=0 A,"If $A\in M_n(\mathbb{R})$, and satisfies $$A^3+A=0$$how to show that$$\text{Trace}(A)=0$$ thanks","If $A\in M_n(\mathbb{R})$, and satisfies $$A^3+A=0$$how to show that$$\text{Trace}(A)=0$$ thanks",,['matrices']
67,By using properties of determinants show that determinant is equal to $(1+a^2+b^2)^3$,By using properties of determinants show that determinant is equal to,(1+a^2+b^2)^3,$$\begin{vmatrix}1+a^2-b^2&2ab&-2b\\ 2ab&1-a^2+b^2&2a\\ 2b&-2a&1-a^2-b^2\end{vmatrix}=(1+a^2+b^2)^3$$ I have been trying to solve the above determinant. But unfortunately my answer is always coming as: $$1+3a^2+3a^4+a^6+3a^2b^4+3b^2+4a^2b^2+a^4b^2+b^6+3b^4$$  Please help me to solve this problem.,$$\begin{vmatrix}1+a^2-b^2&2ab&-2b\\ 2ab&1-a^2+b^2&2a\\ 2b&-2a&1-a^2-b^2\end{vmatrix}=(1+a^2+b^2)^3$$ I have been trying to solve the above determinant. But unfortunately my answer is always coming as: $$1+3a^2+3a^4+a^6+3a^2b^4+3b^2+4a^2b^2+a^4b^2+b^6+3b^4$$  Please help me to solve this problem.,,"['matrices', 'determinant']"
68,What is zero power of a non-square matrix?,What is zero power of a non-square matrix?,,I'm told that zero power of square matrix is an identity matrix of appropriate size. How it is with a non-square matrix?,I'm told that zero power of square matrix is an identity matrix of appropriate size. How it is with a non-square matrix?,,"['linear-algebra', 'matrices']"
69,"If $\mathbf A$ and $\mathbf B$ are Hermitian, when does $\det(\mathbf A-\lambda\mathbf B)=0$ have only real roots?","If  and  are Hermitian, when does  have only real roots?",\mathbf A \mathbf B \det(\mathbf A-\lambda\mathbf B)=0,"Let $\mathbf A, \mathbf B$ be Hermitian matrices of the same size. What is the characterization of $\mathbf A, \mathbf B$ such that $p(\lambda)=\det(\mathbf A-\lambda\mathbf B)=0$ has only real roots? If $\mathbf B$ is positive definite (I corrected this), it is easy to see $p(\lambda)$ has only real roots.","Let $\mathbf A, \mathbf B$ be Hermitian matrices of the same size. What is the characterization of $\mathbf A, \mathbf B$ such that $p(\lambda)=\det(\mathbf A-\lambda\mathbf B)=0$ has only real roots? If $\mathbf B$ is positive definite (I corrected this), it is easy to see $p(\lambda)$ has only real roots.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
70,How to find Determinant of a matrix,How to find Determinant of a matrix,,"I could not understand the concept while googling. can anybody provide help? what will be the determinant of the following matrix? $$ \left[\begin{array}{cccc} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \\ 13 & 14 & 15 & 16 \end{array} \right]$$ Thanks.... :)","I could not understand the concept while googling. can anybody provide help? what will be the determinant of the following matrix? $$ \left[\begin{array}{cccc} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \\ 13 & 14 & 15 & 16 \end{array} \right]$$ Thanks.... :)",,"['matrices', 'determinant']"
71,Is the trace preserved (up to scaling) by Lie algebra homomorphisms (between matrix Lie algebras)?,Is the trace preserved (up to scaling) by Lie algebra homomorphisms (between matrix Lie algebras)?,,"Suppose $\phi:V\rightarrow W$ is a Lie algebra isomorphism between matrix Lie algebras $V$ and $W$ . Since $V$ and $W$ are real vector spaces of matrices, we can consider the traces of their elements. My question is, do we have $\operatorname{tr} \phi(X) = \lambda\cdot\operatorname{tr} X$ for some $\lambda\ne 0$ ? If not, is there a good counterexample? I am thinking we can choose a basis $\{E_{1}, \ldots, E_{n}\}$ for $V$ , take $F_{j} := \phi(E_{j})$ for all $j$ , and then consider structure constants $$ [E_{i}, E_{j}] = c_{ijk}E_{k} $$ where we sum over repeated indices (Einstein summation notation). Since $\phi$ is a Lie algebra isomorphism, we have $$ [F_{i}, F_{j}] = c_{ijk}F_{k}. $$ By taking the traces, and noting that the trace of commutators is zero, we find $$ c_{ijk}e_{k} = 0 \qquad\text{ and }\qquad c_{ijk}f_{k} = 0 $$ where $e_{k} := \operatorname{tr} E_{k}$ and $f_{k} := \operatorname{tr} F_{k}$ . Now my idea is to note that the $c_{ijk}$ 's are determined, and our question is simply what is the null space / kernel of the $n^{2}\times n$ matrix $C = (c_{ij, k})$ ( $ij$ determine row; $k$ determines column), but it's not clear how to proceed from here, or if this approach is any good to begin with. Unfortunately, I don't know of many non-trivial matrix Lie algebra examples at the moment.","Suppose is a Lie algebra isomorphism between matrix Lie algebras and . Since and are real vector spaces of matrices, we can consider the traces of their elements. My question is, do we have for some ? If not, is there a good counterexample? I am thinking we can choose a basis for , take for all , and then consider structure constants where we sum over repeated indices (Einstein summation notation). Since is a Lie algebra isomorphism, we have By taking the traces, and noting that the trace of commutators is zero, we find where and . Now my idea is to note that the 's are determined, and our question is simply what is the null space / kernel of the matrix ( determine row; determines column), but it's not clear how to proceed from here, or if this approach is any good to begin with. Unfortunately, I don't know of many non-trivial matrix Lie algebra examples at the moment.","\phi:V\rightarrow W V W V W \operatorname{tr} \phi(X) = \lambda\cdot\operatorname{tr} X \lambda\ne 0 \{E_{1}, \ldots, E_{n}\} V F_{j} := \phi(E_{j}) j  [E_{i}, E_{j}] = c_{ijk}E_{k}  \phi  [F_{i}, F_{j}] = c_{ijk}F_{k}.   c_{ijk}e_{k} = 0 \qquad\text{ and }\qquad c_{ijk}f_{k} = 0  e_{k} := \operatorname{tr} E_{k} f_{k} := \operatorname{tr} F_{k} c_{ijk} n^{2}\times n C = (c_{ij, k}) ij k","['matrices', 'lie-algebras', 'trace']"
72,"If A is a $2×2 $ matrix, does $A^2=3A$ imply $ A=0$ or $A=3I$?","If A is a  matrix, does  imply  or ?",2×2  A^2=3A  A=0 A=3I,"Problem : Let $A$ be a $2\times 2$ matrix, $O$ be the null matrix, and $I$ be the identity matrix. Is the following statement true? $$\text{if } A^2=3A \implies A=O\text{ or }3I$$ I tried proving this just by factorising $A^2-3A=O \implies A(A-3I)=O \implies A-3I=O, A=O$ . However, I cannot tell if this is correct. Do polynomials work the same way with matricies as they do with real/complex numbers?","Problem : Let be a matrix, be the null matrix, and be the identity matrix. Is the following statement true? I tried proving this just by factorising . However, I cannot tell if this is correct. Do polynomials work the same way with matricies as they do with real/complex numbers?","A 2\times 2 O I \text{if } A^2=3A \implies A=O\text{ or }3I A^2-3A=O \implies A(A-3I)=O \implies A-3I=O, A=O","['linear-algebra', 'matrices']"
73,Existence of a negative eigenvalues for a certain symmetric matrix,Existence of a negative eigenvalues for a certain symmetric matrix,,"Given $x>0$ and \begin{bmatrix} 1 & 1+x & 1\\ 1+x & 1 & 1+x\\ 1 & 1+x & 1 \end{bmatrix} Show that there is exactly one positive and one negative eigenvalue. I solved this problem. This is easy to do by simply computing the characteristic polynomial, $0$ is an eigenvalue and two others of different signs. Now, I want to generalize this to $n \times n$ case ( $x>0$ ). Note that the $n \times n$ analogue will have rank $2$ and hence exactly two non-zero eigenvalues both of which are real because the matrix is symmetric. Now also the trace is $n$ so there is a positive eigenvalue. Now I want to show that there is a negative eigenvalue. Computing the characteristic polynomial will be clumsy now, even though it would finally look like $t^{n-2}(t^2 +at +b)$ . If both the other roots are positive then we have that the spectral radius is strictly less than $4$ . Can we use this?","Given and Show that there is exactly one positive and one negative eigenvalue. I solved this problem. This is easy to do by simply computing the characteristic polynomial, is an eigenvalue and two others of different signs. Now, I want to generalize this to case ( ). Note that the analogue will have rank and hence exactly two non-zero eigenvalues both of which are real because the matrix is symmetric. Now also the trace is so there is a positive eigenvalue. Now I want to show that there is a negative eigenvalue. Computing the characteristic polynomial will be clumsy now, even though it would finally look like . If both the other roots are positive then we have that the spectral radius is strictly less than . Can we use this?","x>0 \begin{bmatrix}
1 & 1+x & 1\\
1+x & 1 & 1+x\\
1 & 1+x & 1
\end{bmatrix} 0 n \times n x>0 n \times n 2 n t^{n-2}(t^2 +at +b) 4","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
74,"""Absolute"" version of determinant","""Absolute"" version of determinant",,"It is well known that for a matrix $A = (A_{i,j})$ , the determinant of $A$ is $$\det(A) = \sum_{\sigma \in S_n} \mbox{sgn} (\sigma)\prod_{i \in [n]} A_{i,\sigma(i)}.$$ Is there any way to manipulate the matrix to obtain, or any existing interpretation for, the ""absolute"" version of the determinant, which is $$\det^+(A) = \sum_{\sigma \in S_n} \prod_{i \in [n]} A_{i,\sigma(i)}?$$","It is well known that for a matrix , the determinant of is Is there any way to manipulate the matrix to obtain, or any existing interpretation for, the ""absolute"" version of the determinant, which is","A = (A_{i,j}) A \det(A) = \sum_{\sigma \in S_n} \mbox{sgn} (\sigma)\prod_{i \in [n]} A_{i,\sigma(i)}. \det^+(A) = \sum_{\sigma \in S_n} \prod_{i \in [n]} A_{i,\sigma(i)}?","['linear-algebra', 'matrices', 'permutations', 'determinant', 'permanent']"
75,Calculating $B^{10}$,Calculating,B^{10},"Calculate $B^{10}$ when $$B = \begin{pmatrix} 1 & -1\\ 1 & 1 \end{pmatrix}$$ The way I did it was $$ B = I + A $$ where $$A = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$$ and $A^2=-I$ . Since $A$ and $I$ are commutative, $$\begin{aligned} B^2 &= (I+A)^2 = 2A \\ B^3 &= (I+A)2A = 2A-2I\\ B^4 &= (I+A)(2A-2I) = -4I\\ \vdots \\ B^{10} &= 32A \end{aligned}$$ Is there a simpler method or a smarter approach if you want to do this for, e.g., $B^{100}$ ?","Calculate when The way I did it was where and . Since and are commutative, Is there a simpler method or a smarter approach if you want to do this for, e.g., ?",B^{10} B = \begin{pmatrix} 1 & -1\\ 1 & 1 \end{pmatrix}  B = I + A  A = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix} A^2=-I A I \begin{aligned} B^2 &= (I+A)^2 = 2A \\ B^3 &= (I+A)2A = 2A-2I\\ B^4 &= (I+A)(2A-2I) = -4I\\ \vdots \\ B^{10} &= 32A \end{aligned} B^{100},"['linear-algebra', 'matrices', 'exponentiation']"
76,How to show that this matrix is symmetric definite positive,How to show that this matrix is symmetric definite positive,,"I need help. We want to show that the matrix $A$ is symmetric definite positive for all $n$ where $n$ is the size of the matrix $A$ . Here is $A$ : $$ A = \begin{bmatrix}  1&  -1&  0& ...& 0\\   -1&  2&  -1& \ddots & \vdots\\   0&  -1&  2& \ddots &0\\   \vdots &  \ddots &  \ddots&  \ddots &-1 \\ 0 & ...  &0 & -1& 2 \end{bmatrix}  $$ I tried brute forcing by calculating for $x$ a vector of size $n$ , $x^TAx$ , but it seems tricky. Thanks for your help","I need help. We want to show that the matrix is symmetric definite positive for all where is the size of the matrix . Here is : I tried brute forcing by calculating for a vector of size , , but it seems tricky. Thanks for your help","A n n A A 
A = \begin{bmatrix}
 1&  -1&  0& ...& 0\\ 
 -1&  2&  -1& \ddots & \vdots\\ 
 0&  -1&  2& \ddots &0\\ 
 \vdots &  \ddots &  \ddots&  \ddots &-1 \\
0 & ...  &0 & -1& 2
\end{bmatrix} 
 x n x^TAx","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
77,Why $X^TX$ not always positive semidefinite using numpy.linalg.eig? [duplicate],Why  not always positive semidefinite using numpy.linalg.eig? [duplicate],X^TX,"This question already has answers here : Is $AA^T$ positive semidefinite? (2 answers) Closed 3 years ago . We can use $z^T (X^TX)z = (Xz)^T(Xz)=||Xz||^2_2 \geq 0$ to prove $X^TX$ is positive semidefinite. However, when I use numpy.linalg.eig to compute the eigenvalues of dot product matrix, I cannot get all positive eigenvalues. How to explain for it? import numpy as np import math np.random.seed(33) X = np.random.randint(10, size=(2,6)) print(X) Kernel_Matrix = np.dot(X.T,X) eigenvalue, eigenvector = np.linalg.eig(Kernel_Matrix) print(eigenvalue > 0) print(eigenvalue) Output: [[4 7 8 2 2 9]  [9 3 6 3 3 1]] [[ 97  55  86  35  35  45]  [ 55  58  74  23  23  66]  [ 86  74 100  34  34  78]  [ 35  23  34  13  13  21]  [ 35  23  34  13  13  21]  [ 45  66  78  21  21  82]] [ True  True False False  True False] [ 3.12680220e+02  5.03197805e+01 -9.39575362e-15 -1.44635182e-14   1.43755791e-16 -7.87904824e-32]","This question already has answers here : Is $AA^T$ positive semidefinite? (2 answers) Closed 3 years ago . We can use to prove is positive semidefinite. However, when I use numpy.linalg.eig to compute the eigenvalues of dot product matrix, I cannot get all positive eigenvalues. How to explain for it? import numpy as np import math np.random.seed(33) X = np.random.randint(10, size=(2,6)) print(X) Kernel_Matrix = np.dot(X.T,X) eigenvalue, eigenvector = np.linalg.eig(Kernel_Matrix) print(eigenvalue > 0) print(eigenvalue) Output: [[4 7 8 2 2 9]  [9 3 6 3 3 1]] [[ 97  55  86  35  35  45]  [ 55  58  74  23  23  66]  [ 86  74 100  34  34  78]  [ 35  23  34  13  13  21]  [ 35  23  34  13  13  21]  [ 45  66  78  21  21  82]] [ True  True False False  True False] [ 3.12680220e+02  5.03197805e+01 -9.39575362e-15 -1.44635182e-14   1.43755791e-16 -7.87904824e-32]",z^T (X^TX)z = (Xz)^T(Xz)=||Xz||^2_2 \geq 0 X^TX,"['linear-algebra', 'matrices', 'python']"
78,"For $n \times n$ matrices $A$ and $B$, if $(A^i - B^i)x = 0$ for $i = 1, \dots,\ n$, does $(A^{n+1} - B^{n+1}) x = 0$?","For  matrices  and , if  for , does ?","n \times n A B (A^i - B^i)x = 0 i = 1, \dots,\ n (A^{n+1} - B^{n+1}) x = 0","Conjecture: Let $A,B \in \mathbb{R}^{n \times n}$ and $x \in \mathbb{R}^n \setminus \{0\}$ . If $$\left( A^{i} - B^i \right) \, x = 0, \quad \forall i \in \{ 1, \dots, n \}$$ then $$\left( A^{j} - B^{j} \right) \, x = 0, \quad \forall j \in \{ n+1, n+2, \dots \}$$ My initial goal is to prove this for $j = n+1$ , i.e., $$\left(A^{n+1} - B^{n+1}\right)\,x = 0.$$ Since $$\left(A - B\right)\,x = 0,$$ we have $$A\,x = B\,x,$$ and similarly, $$A^{i}\,x = B^{i}\,x,\quad i = 1,\ \dots,\ n.$$ So $$\left(A^{n+1} - B^{n+1}\right)\,x = A^{n+1}\,x - B^{n+1}\,x = A^{n}\,A\,x - B^{n}\,B\,x = \left(A^{n} - B^{n}\right)\,A\,x = \left(A^{n} - B^{n}\right)\,B\,x,$$ and similarly, $$\left(A^{n+1} - B^{n+1}\right)\,x = \left(A^{n+1-i} - B^{n+1-i}\right)\,A^{i}\,x = \left(A^{n+1-i} - B^{n+1-i}\right)\,B^{i}\,x,\quad i = 1,\ \dots,\ n.$$ I'm not entirely sure where to go from here.","Conjecture: Let and . If then My initial goal is to prove this for , i.e., Since we have and similarly, So and similarly, I'm not entirely sure where to go from here.","A,B \in \mathbb{R}^{n \times n} x \in \mathbb{R}^n \setminus \{0\} \left( A^{i} - B^i \right) \, x = 0, \quad \forall i \in \{ 1, \dots, n \} \left( A^{j} - B^{j} \right) \, x = 0, \quad \forall j \in \{ n+1, n+2, \dots \} j = n+1 \left(A^{n+1} - B^{n+1}\right)\,x = 0. \left(A - B\right)\,x = 0, A\,x = B\,x, A^{i}\,x = B^{i}\,x,\quad i = 1,\ \dots,\ n. \left(A^{n+1} - B^{n+1}\right)\,x = A^{n+1}\,x - B^{n+1}\,x = A^{n}\,A\,x - B^{n}\,B\,x = \left(A^{n} - B^{n}\right)\,A\,x = \left(A^{n} - B^{n}\right)\,B\,x, \left(A^{n+1} - B^{n+1}\right)\,x = \left(A^{n+1-i} - B^{n+1-i}\right)\,A^{i}\,x = \left(A^{n+1-i} - B^{n+1-i}\right)\,B^{i}\,x,\quad i = 1,\ \dots,\ n.","['linear-algebra', 'matrices']"
79,Is $\mbox{Rank}(A + A^2) \leq \mbox{Rank} (A)$? [closed],Is ? [closed],\mbox{Rank}(A + A^2) \leq \mbox{Rank} (A),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Here, $A$ is an $n \times n$ matrix. I am not able to find any counterexample but not able to prove this as well. The examples I have tried so far shows me that $\mbox{Rank} (A + A^2) = \mbox{Rank} (A)$ . I don't know how to show the inequality.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Here, is an matrix. I am not able to find any counterexample but not able to prove this as well. The examples I have tried so far shows me that . I don't know how to show the inequality.",A n \times n \mbox{Rank} (A + A^2) = \mbox{Rank} (A),"['linear-algebra', 'matrices', 'matrix-rank']"
80,How do you rewrite a determinant of a matrix into a polynomial by induction?,How do you rewrite a determinant of a matrix into a polynomial by induction?,,$$\det\begin{bmatrix} {x} & {0} & {\cdots} & {\cdots} & {0} & {a_{1}} \\  {-1} & {x} & {0} & {\cdots} & {0} & {a_{2}} \\ {\ddots} & {\ddots} & {\ddots} & {\ddots} & {\vdots} & {\vdots} \\ {\cdots} & {0} & {-1} & {x} & {0} & {a_{n-3}} \\{\cdots} & {\cdots} & {0} & {-1} & {x} & {a_{n-2}} \\{\cdots} & {\cdots} & {\cdots} & {0} & {-1} & {a_{n-1}+x} \end{bmatrix}=a_1+a_2x+\cdots+a_{n-1}x^{n-2}+x^{n-1}$$ How can I replace the determinant on the left by induction to get $a_1+a_2x+\cdots+a_{n-1}x^{n-2}+x^{n-1}$ ? I need this to determine the determinant of the companion matrix and I don't understand how this step is done. Thanks in advance!,How can I replace the determinant on the left by induction to get ? I need this to determine the determinant of the companion matrix and I don't understand how this step is done. Thanks in advance!,\det\begin{bmatrix} {x} & {0} & {\cdots} & {\cdots} & {0} & {a_{1}} \\  {-1} & {x} & {0} & {\cdots} & {0} & {a_{2}} \\ {\ddots} & {\ddots} & {\ddots} & {\ddots} & {\vdots} & {\vdots} \\ {\cdots} & {0} & {-1} & {x} & {0} & {a_{n-3}} \\{\cdots} & {\cdots} & {0} & {-1} & {x} & {a_{n-2}} \\{\cdots} & {\cdots} & {\cdots} & {0} & {-1} & {a_{n-1}+x} \end{bmatrix}=a_1+a_2x+\cdots+a_{n-1}x^{n-2}+x^{n-1} a_1+a_2x+\cdots+a_{n-1}x^{n-2}+x^{n-1},"['linear-algebra', 'matrices', 'induction', 'determinant', 'companion-matrices']"
81,"""Square-normal"" matrices are normal","""Square-normal"" matrices are normal",,"This is from a practice exam for my quals. Let $A$ be an $n \times n$ complex matrix. Suppose $A$ satisfies the following property: $(AA^\dagger)^2 = (A^\dagger A)^2$ Prove that $A$ is normal, that is, that $AA^\dagger = A^\dagger A$ . My attempt: Recall that a matrix is normal if and only if it can be diagonalized by a unitary matrix. I will attempt to show this is the case. $A$ has a singular value decomposition $A = V \Sigma U^\dagger$ , where $V$ and $U$ are unitary and $\Sigma$ is diagonal with non-negative real diagonal entries. This factorization is unique up to permutation of diagonal elements of $\Sigma$ , since $A$ is square. $A =  V \Sigma U^\dagger, A^\dagger =  U \Sigma^\dagger V^\dagger$ and since $A A^\dagger$ and its conjugate transpose are normal, $AA^\dagger = V \Sigma U^\dagger U \Sigma^\dagger V^\dagger = V \Sigma \Sigma^\dagger V^\dagger$ and similarly for $A^\dagger A$ . I tried to use all of these assumptions to show that $U = V$ , but wasn't able to reach a contradiction by assuming otherwise.","This is from a practice exam for my quals. Let be an complex matrix. Suppose satisfies the following property: Prove that is normal, that is, that . My attempt: Recall that a matrix is normal if and only if it can be diagonalized by a unitary matrix. I will attempt to show this is the case. has a singular value decomposition , where and are unitary and is diagonal with non-negative real diagonal entries. This factorization is unique up to permutation of diagonal elements of , since is square. and since and its conjugate transpose are normal, and similarly for . I tried to use all of these assumptions to show that , but wasn't able to reach a contradiction by assuming otherwise.","A n \times n A (AA^\dagger)^2 = (A^\dagger A)^2 A AA^\dagger = A^\dagger A A A = V \Sigma U^\dagger V U \Sigma \Sigma A A =  V \Sigma U^\dagger, A^\dagger =  U \Sigma^\dagger V^\dagger A A^\dagger AA^\dagger = V \Sigma U^\dagger U \Sigma^\dagger V^\dagger = V \Sigma \Sigma^\dagger V^\dagger A^\dagger A U = V","['matrices', 'svd', 'self-adjoint-operators']"
82,"If $A^3 =0$, what is the determinant of $(A-3I)$? [closed]","If , what is the determinant of ? [closed]",A^3 =0 (A-3I),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question From $A^3 = 0$ , I can conclude that A must be singular. But how to proceed from there?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question From , I can conclude that A must be singular. But how to proceed from there?",A^3 = 0,"['linear-algebra', 'matrices', 'determinant']"
83,How can one show that a matrix is nilpotent or not?,How can one show that a matrix is nilpotent or not?,,"Context : In linear algebra, a nilpotent matrix is a square matrix $N$ such that $$ N^{k}=0\, $$ for some positive integer $k$ . (The smallest such $k$ is sometimes called the index of $N$ .) Question : How can one show that a matrix is nilpotent or not? Direct matrix multiplication for checking the definition is an obvious choice but seems not efficient. Is there any other method to do it?","Context : In linear algebra, a nilpotent matrix is a square matrix such that for some positive integer . (The smallest such is sometimes called the index of .) Question : How can one show that a matrix is nilpotent or not? Direct matrix multiplication for checking the definition is an obvious choice but seems not efficient. Is there any other method to do it?","N 
N^{k}=0\,
 k k N","['linear-algebra', 'matrices', 'nilpotence']"
84,Inverse of Identity minus a stricty upper tringular matrix,Inverse of Identity minus a stricty upper tringular matrix,,"Let $S$ be an $n\times n$ strictly upper triangular matrix. Show that $(I-S)^{-1} = I+S+S^2+ \dots + S^{n-1}$ . This seems like it should be an easy problem to do by induction, but I am having trouble justifying the last step. We can define a sequence of matrices $\{S_i\}$ such that $S_n$ and $S_{n+1}$ agree in the $n\times n$ upper left corner of $S_{n+1}$ , so that the only new entries being added as we increase $n$ appear in the furthest right column. By induction: \begin{align*} \begin{bmatrix} 1 & -s_{12} & \dots & -s_{1,n+1}\\ 0 & \ddots & \dots & -s_{2,n+1}\\ \vdots & & \ddots & \vdots\\ 0 & \dots & \dots & 1 \end{bmatrix} (I + S_{n+1} + S_{n+1}^2 + \dots + S_{n+1}^n) =  \begin{bmatrix} 1 & 0 & \dots & \text{stuff}\\ 0 & \ddots & \dots & \text{stuff}\\ \vdots & & \ddots & \vdots\\ 0 & \dots & \dots & 1 \end{bmatrix} \end{align*} Put into words, by induction we have that the upper left corner is the $n \times n$ identity matrix, but I am not sure how to justify that the ""stuff"" rightmost column becomes $0$ .","Let be an strictly upper triangular matrix. Show that . This seems like it should be an easy problem to do by induction, but I am having trouble justifying the last step. We can define a sequence of matrices such that and agree in the upper left corner of , so that the only new entries being added as we increase appear in the furthest right column. By induction: Put into words, by induction we have that the upper left corner is the identity matrix, but I am not sure how to justify that the ""stuff"" rightmost column becomes .","S n\times n (I-S)^{-1} = I+S+S^2+ \dots + S^{n-1} \{S_i\} S_n S_{n+1} n\times n S_{n+1} n \begin{align*}
\begin{bmatrix}
1 & -s_{12} & \dots & -s_{1,n+1}\\
0 & \ddots & \dots & -s_{2,n+1}\\
\vdots & & \ddots & \vdots\\
0 & \dots & \dots & 1
\end{bmatrix} (I + S_{n+1} + S_{n+1}^2 + \dots + S_{n+1}^n) = 
\begin{bmatrix}
1 & 0 & \dots & \text{stuff}\\
0 & \ddots & \dots & \text{stuff}\\
\vdots & & \ddots & \vdots\\
0 & \dots & \dots & 1
\end{bmatrix}
\end{align*} n \times n 0","['linear-algebra', 'matrices']"
85,Eigenvalue and similar matrices,Eigenvalue and similar matrices,,"if $A$ and $B$ are two $n\times n$ matrices with same eigenvalues such that each eigenvalue has same algebraic and geometric multiplicity. Does $A$ and $B$ are similar? If $A$ is diagnalizable  then the claim is true. But does it true even when sum of geometric multiplicity is not $n$ . Please give me a hint to start with. If given a counter example, please help me to show they are not similar. Thanks","if and are two matrices with same eigenvalues such that each eigenvalue has same algebraic and geometric multiplicity. Does and are similar? If is diagnalizable  then the claim is true. But does it true even when sum of geometric multiplicity is not . Please give me a hint to start with. If given a counter example, please help me to show they are not similar. Thanks",A B n\times n A B A n,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'examples-counterexamples']"
86,"Let $A,B \in M_{2x2}(\mathbb{K})$, prove that $(AB-BA)^2=-\det(AB-BA)I$","Let , prove that","A,B \in M_{2x2}(\mathbb{K}) (AB-BA)^2=-\det(AB-BA)I","Problem: Let $A,B \in M_{2x2}(\mathbb{K})$ , prove that $(AB-BA)^2=-\det(AB-BA)I$ . There is no more information Question: Is there some other way to prove this besides brute force?, and if is, could you give me some hint/solution? Thanks so much :)","Problem: Let , prove that . There is no more information Question: Is there some other way to prove this besides brute force?, and if is, could you give me some hint/solution? Thanks so much :)","A,B \in M_{2x2}(\mathbb{K}) (AB-BA)^2=-\det(AB-BA)I","['linear-algebra', 'matrices', 'matrix-calculus']"
87,Intersection point of 2 lines defined by 2 points each,Intersection point of 2 lines defined by 2 points each,,"I'm implementing this in code, but I'll rewrite it so that it is easier understood (like pseudocode): # a = pt 1 on line 1 # b = pt 2 on line 1 # c = pt 1 on line 2 # d = pt 2 on line 2 def intersect(a,b,c,d):      # stuff for line 1     a1 = b.y-a.y     b1 = a.x-b.x     c1 = a1*a.x + b1*a.y      # stuff for line 2     a2 = d.y-c.y     b2 = c.x-d.x     c2 = a2*c.x + b2*c.y      determinant = a1*b2 - a2*b1      if (determinant == 0):         # Return (infinity, infinity) if they never intersect         # By ""never intersect"", I mean that the lines are parallel to each other         return math.inf, math,inf     else:         x = (b2*c1 - b1*c2)/determinant         y = (a1*c2 - a2*c1)/determinant         return x,y All the above works, ... but only does by assuming that the lines extend infinitely in each direction, like a linear equation. I'll show what I mean here . There are the 2 lines, red and green, and the gold dot is what is returned when I test this code ... but the lines don't actually intersect. What can be used to test whether the lines truly intersect? Heres the actual Python code if needed.","I'm implementing this in code, but I'll rewrite it so that it is easier understood (like pseudocode): # a = pt 1 on line 1 # b = pt 2 on line 1 # c = pt 1 on line 2 # d = pt 2 on line 2 def intersect(a,b,c,d):      # stuff for line 1     a1 = b.y-a.y     b1 = a.x-b.x     c1 = a1*a.x + b1*a.y      # stuff for line 2     a2 = d.y-c.y     b2 = c.x-d.x     c2 = a2*c.x + b2*c.y      determinant = a1*b2 - a2*b1      if (determinant == 0):         # Return (infinity, infinity) if they never intersect         # By ""never intersect"", I mean that the lines are parallel to each other         return math.inf, math,inf     else:         x = (b2*c1 - b1*c2)/determinant         y = (a1*c2 - a2*c1)/determinant         return x,y All the above works, ... but only does by assuming that the lines extend infinitely in each direction, like a linear equation. I'll show what I mean here . There are the 2 lines, red and green, and the gold dot is what is returned when I test this code ... but the lines don't actually intersect. What can be used to test whether the lines truly intersect? Heres the actual Python code if needed.",,"['linear-algebra', 'matrices', 'python']"
88,Is not full rank matrix invertible?,Is not full rank matrix invertible?,,Problem $A$ is a $4 \times 4$ matrix. It is known that $\text{rank}(A)=3$ . Is matrix A invertible ? Attempt to solve $\text{rank(A)}=3 \implies \det(A)=0$ which implies matrix is $\textbf{not}$ invertible. One dimension is lost during linear transformation if matrix is not full rank by definition. This implies determinant will be $0$ and that some information is lost in this linear transformation. Is my intuition behind this correct ?,Problem is a matrix. It is known that . Is matrix A invertible ? Attempt to solve which implies matrix is invertible. One dimension is lost during linear transformation if matrix is not full rank by definition. This implies determinant will be and that some information is lost in this linear transformation. Is my intuition behind this correct ?,A 4 \times 4 \text{rank}(A)=3 \text{rank(A)}=3 \implies \det(A)=0 \textbf{not} 0,"['linear-algebra', 'matrices', 'linear-transformations', 'determinant', 'matrix-rank']"
89,Elementary Matrix and Row Operations,Elementary Matrix and Row Operations,,"Find an elementary matrix E such that EA = B $$A = \begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}$$ $$B = \begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ $$\begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}*E=\begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ So the Elementary Matrix is an Identity Matrix that has one elementary row operation performed on it. Multiplying an Elementary Matrix by A should result in a matrix that is equivalent to having that elementary row operation performed onto A . I can see that only the first row of A is modified to obtain B and I can tell that the first row of A is scaled by a value of -2. Therefore the Elementary Matrix should be the Identity Matrix with the first row scaled by -2.$$\begin{bmatrix} -2 & 0 \\ 0 & 1 \end{bmatrix}$$ However, $$\begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}*\begin{bmatrix} -2 & 0 \\ 0 & 1 \end{bmatrix}≠\begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ What am I doing incorrectly?","Find an elementary matrix E such that EA = B $$A = \begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}$$ $$B = \begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ $$\begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}*E=\begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ So the Elementary Matrix is an Identity Matrix that has one elementary row operation performed on it. Multiplying an Elementary Matrix by A should result in a matrix that is equivalent to having that elementary row operation performed onto A . I can see that only the first row of A is modified to obtain B and I can tell that the first row of A is scaled by a value of -2. Therefore the Elementary Matrix should be the Identity Matrix with the first row scaled by -2.$$\begin{bmatrix} -2 & 0 \\ 0 & 1 \end{bmatrix}$$ However, $$\begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}*\begin{bmatrix} -2 & 0 \\ 0 & 1 \end{bmatrix}≠\begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ What am I doing incorrectly?",,"['linear-algebra', 'matrices']"
90,"If A is such that $(A+2I)^2=0$, prove that $A+I$ is invertible.","If A is such that , prove that  is invertible.",(A+2I)^2=0 A+I,"I'm stuck on the second question of this matrix exercise: Consider a matrix $A$ $\in$ $\mathcal{M}_n(K)$ such that $(A+2I)^2=0$ , where $I$ is the identity matrix. Prove that $A$ is invertible and describe the $A^{-1}$ matrix as a function of $A$ . Prove that $(A+I)$ is invertible. Solution for 1.: $(A+2I)^2=0$ $\Rightarrow$ $\Rightarrow A^2 + 4AI +4I^2 = 0 \Rightarrow$ $\Rightarrow A(A+4I) = -4I \Rightarrow$ $A(-\frac {A+4I}{4}) = I$ , therefore A is invertible, and $A^{-1} = -\frac 14A-I.$ I've tried pretty much every replacement I could think of, but no matter what, I couldn't come to the form $(A+I)B=I$ . Many thanks in advance.","I'm stuck on the second question of this matrix exercise: Consider a matrix such that , where is the identity matrix. Prove that is invertible and describe the matrix as a function of . Prove that is invertible. Solution for 1.: , therefore A is invertible, and I've tried pretty much every replacement I could think of, but no matter what, I couldn't come to the form . Many thanks in advance.",A \in \mathcal{M}_n(K) (A+2I)^2=0 I A A^{-1} A (A+I) (A+2I)^2=0 \Rightarrow \Rightarrow A^2 + 4AI +4I^2 = 0 \Rightarrow \Rightarrow A(A+4I) = -4I \Rightarrow A(-\frac {A+4I}{4}) = I A^{-1} = -\frac 14A-I. (A+I)B=I,"['matrices', 'inverse']"
91,"Eigenvalues of $A^n$, $A=\begin{bmatrix}1&1\\1&0\end{bmatrix}$","Eigenvalues of ,",A^n A=\begin{bmatrix}1&1\\1&0\end{bmatrix},"Let $A=\begin{bmatrix}1&1\\1&0\end{bmatrix}$ and $\alpha_n$ and $\beta_n$ denote the two eigenvalues of $A^n$ such that $|\alpha_n|\geq |\beta_n|$ . Then $\alpha_n\rightarrow \infty$ as $n\to\infty$ $\beta_n\to 0$ as $n\to\infty$ $\beta_n$ is positive if $n$ is even. $\beta_n$ is negative if $n$ is odd. $F_n$ is $n$ -th Fibonacci sequence, with $F_{-1}=0,F_0=1,F_1=1$ I found $A^n=\begin{bmatrix}F_n&F_{n-1}\\F_{n-1}&F_{n-2}\end{bmatrix}$ Eigenvalues are $\dfrac{F_n+F_{n-2}\pm\sqrt{(F_n-F_{n-2})^2+4F^2_{n-1}}}{2}$ Is there any result I need to know, because it is a MSQ (Multiple selection question) and meant to solve within 4-5 minutes. One other thing, I found (which may not be important here): $\begin{bmatrix}F_n&F_{n-1}\\F_{n-1}&F_{n-2}\end{bmatrix}\begin{bmatrix}1&1\\1&1\end{bmatrix}=\begin{bmatrix}F_{n+1}&0\\0&F_n\end{bmatrix}\begin{bmatrix}1&1\\1&1\end{bmatrix}$","Let and and denote the two eigenvalues of such that . Then as as is positive if is even. is negative if is odd. is -th Fibonacci sequence, with I found Eigenvalues are Is there any result I need to know, because it is a MSQ (Multiple selection question) and meant to solve within 4-5 minutes. One other thing, I found (which may not be important here):","A=\begin{bmatrix}1&1\\1&0\end{bmatrix} \alpha_n \beta_n A^n |\alpha_n|\geq |\beta_n| \alpha_n\rightarrow \infty n\to\infty \beta_n\to 0 n\to\infty \beta_n n \beta_n n F_n n F_{-1}=0,F_0=1,F_1=1 A^n=\begin{bmatrix}F_n&F_{n-1}\\F_{n-1}&F_{n-2}\end{bmatrix} \dfrac{F_n+F_{n-2}\pm\sqrt{(F_n-F_{n-2})^2+4F^2_{n-1}}}{2} \begin{bmatrix}F_n&F_{n-1}\\F_{n-1}&F_{n-2}\end{bmatrix}\begin{bmatrix}1&1\\1&1\end{bmatrix}=\begin{bmatrix}F_{n+1}&0\\0&F_n\end{bmatrix}\begin{bmatrix}1&1\\1&1\end{bmatrix}","['linear-algebra', 'sequences-and-series', 'matrices', 'eigenvalues-eigenvectors', 'fibonacci-numbers']"
92,For which value of k is a matrix diagonalizable?,For which value of k is a matrix diagonalizable?,,"I have attempted to solve this problem and I reach a certain point (finding eigenvalues) before getting stuck. Let  $$     A=   \left[ {\begin{array}{cc}    2 & 0 & k \\    0 & 3 & 0\\ 0 & 0 & k \\   \end{array} } \right]  $$ To find the values for which this matrix is diagonalizable, I first found the characteristic polynomial, which I calculated to be: $(2- \lambda)(3- \lambda)(k - \lambda)$ I calculated this by solving for the determinant of $A - \lambda I$. The answer is apparently for all $k \neq 2$, but I am unsure of how to prove this.  Could someone provide any insight on this?  Thanks! For self-study.","I have attempted to solve this problem and I reach a certain point (finding eigenvalues) before getting stuck. Let  $$     A=   \left[ {\begin{array}{cc}    2 & 0 & k \\    0 & 3 & 0\\ 0 & 0 & k \\   \end{array} } \right]  $$ To find the values for which this matrix is diagonalizable, I first found the characteristic polynomial, which I calculated to be: $(2- \lambda)(3- \lambda)(k - \lambda)$ I calculated this by solving for the determinant of $A - \lambda I$. The answer is apparently for all $k \neq 2$, but I am unsure of how to prove this.  Could someone provide any insight on this?  Thanks! For self-study.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
93,"Given a $2 \times 2$ matrix $A$ with $A^3=O$, find the sum of all the entries of matrix $A^2$","Given a  matrix  with , find the sum of all the entries of matrix",2 \times 2 A A^3=O A^2,Given a matrix $$A=\begin{bmatrix} a & b\\c &d \end{bmatrix}$$ satisfying $$A^3=O$$ find the sum of all the elements of matrix $A^2$ . My try: Since $A$ is $2 \times 2$ matrix it can be expressed as $$A^2=pA+qI$$ where $p$ and $q$ are some scalars. Then we have $$A^3=pA^2+qA=O$$ So $$p(pA+qI)+qA=O$$ that is $$A=\frac{-pq}{p^2+q} I$$ So $$A^2=\frac{p^2q^2}{(p^2+2q)^2} I$$ so sum  of elements of $A^2$ is $\frac{2p^2q^2}{(p^2+2q)^2}$ . But the answer is zero. Can I know my mistake?,Given a matrix satisfying find the sum of all the elements of matrix . My try: Since is matrix it can be expressed as where and are some scalars. Then we have So that is So so sum  of elements of is . But the answer is zero. Can I know my mistake?,A=\begin{bmatrix} a & b\\c &d \end{bmatrix} A^3=O A^2 A 2 \times 2 A^2=pA+qI p q A^3=pA^2+qA=O p(pA+qI)+qA=O A=\frac{-pq}{p^2+q} I A^2=\frac{p^2q^2}{(p^2+2q)^2} I A^2 \frac{2p^2q^2}{(p^2+2q)^2},"['linear-algebra', 'matrices', 'nilpotence']"
94,Quick and easy way to show whether a matrix is injective / surjective?,Quick and easy way to show whether a matrix is injective / surjective?,,"I'm afraid there could be a task like that in my exam. What way would you recommend me if there was a quadratic matrix given, such as $A= \begin{pmatrix} 2 & 0 & 4\\  0 & 3 & 0\\  1 & 7 & 2 \end{pmatrix}$? There are several (for me confusing) ways doing it I think. The one we had in our readings is to check if the column vectors are linearly independent (or something like that :S). But I think this would only tell us whether the linear mapping is injective. But I think there is another, faster way with rank? I hope you can explain with this example?","I'm afraid there could be a task like that in my exam. What way would you recommend me if there was a quadratic matrix given, such as $A= \begin{pmatrix} 2 & 0 & 4\\  0 & 3 & 0\\  1 & 7 & 2 \end{pmatrix}$? There are several (for me confusing) ways doing it I think. The one we had in our readings is to check if the column vectors are linearly independent (or something like that :S). But I think this would only tell us whether the linear mapping is injective. But I think there is another, faster way with rank? I hope you can explain with this example?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
95,"If $A$ and $B$ are similar matrices, then the matrices $p(A)$ and $p(B)$ are similar?","If  and  are similar matrices, then the matrices  and  are similar?",A B p(A) p(B),"$p$ can be any polynomial and the matrices are square matrices Now I know that by the Cayley–Hamilton theorem any matrice A satisfies it's characteristic polynomial and I feel that the key to the proof lies there, but not sure how to capitalize on it. My guess is since $A$ and $B$ are similar they have the same $p(\lambda)$ so doesn't that imply (by what was stated earlier) that the matrices $p(A)$ and $p(B)$ are similar?","$p$ can be any polynomial and the matrices are square matrices Now I know that by the Cayley–Hamilton theorem any matrice A satisfies it's characteristic polynomial and I feel that the key to the proof lies there, but not sure how to capitalize on it. My guess is since $A$ and $B$ are similar they have the same $p(\lambda)$ so doesn't that imply (by what was stated earlier) that the matrices $p(A)$ and $p(B)$ are similar?",,['linear-algebra']
96,Why can't my graphing calculator find the RREF of the transpose of a matrix?,Why can't my graphing calculator find the RREF of the transpose of a matrix?,,"I know this is somewhat of an odd question, but I am having trouble with my TI-84 calculator and I don't know why. I'm trying to find the RREF of the transpose of a $4\times6$ matrix; for some reason my graphing calculator gives me an error. Something to do with the dimensions? Here is a photo of matrix $A$ . I want to find RREF $(A$ transposed $)$ .","I know this is somewhat of an odd question, but I am having trouble with my TI-84 calculator and I don't know why. I'm trying to find the RREF of the transpose of a matrix; for some reason my graphing calculator gives me an error. Something to do with the dimensions? Here is a photo of matrix . I want to find RREF transposed .",4\times6 A (A ),"['linear-algebra', 'matrices', 'calculator']"
97,Is $\det(I+AA^*) $ always be non-zero?,Is  always be non-zero?,\det(I+AA^*) ,"Is it always true for $\det(I+AA^*)\neq 0$ ? Here, $A^*$ is the adjoint of $A$. I have found that $\det(I+BC)=\det(I+CB)$, but it seems no use here.","Is it always true for $\det(I+AA^*)\neq 0$ ? Here, $A^*$ is the adjoint of $A$. I have found that $\det(I+BC)=\det(I+CB)$, but it seems no use here.",,"['linear-algebra', 'matrices']"
98,Why is a linear transformation expressed using its transpose?,Why is a linear transformation expressed using its transpose?,,"If $A$ is an invertible matrix with entries from $\mathbb{R}$, what is the reasoning behind defining an invertible linear transformation $f_A:\mathbb{R}^n \rightarrow \mathbb{R}^n$ as $f_A=xA^t$, where $A^t$ represents the transpose of $A$? A more ""natural"" definition to me would be$f_A=Ax$.","If $A$ is an invertible matrix with entries from $\mathbb{R}$, what is the reasoning behind defining an invertible linear transformation $f_A:\mathbb{R}^n \rightarrow \mathbb{R}^n$ as $f_A=xA^t$, where $A^t$ represents the transpose of $A$? A more ""natural"" definition to me would be$f_A=Ax$.",,"['matrices', 'group-theory']"
99,Is there an alternative way to represent the $\operatorname{diag}$ function?,Is there an alternative way to represent the  function?,\operatorname{diag},"In optimization, it is common to see the so called $\operatorname{diag}$ function Given a vector $x \in \mathbb{R}^n$, $\operatorname{diag}(x)$ =  $n \times n$ diagonal matrix with components of $x$ on the diagonal For example: Optimization that involves inverse operation. Reformulation of BQP to SDP The reason of using  $\operatorname{diag}$ is because it is used in several platforms such as MATLAB, and people generally understands what the function is supposed to do Is there a more linear algebra, step by step way of converting a   vector $x \in \mathbb{R}^n$ into a diagonal matrix with components on   the diagonal without having a define a function that directly performs   the task ? i.e. given $x$, we find a series of functions/steps $f_2 \circ f_1 (x)$ which give us the same matrix as  $\operatorname{diag}(x)$","In optimization, it is common to see the so called $\operatorname{diag}$ function Given a vector $x \in \mathbb{R}^n$, $\operatorname{diag}(x)$ =  $n \times n$ diagonal matrix with components of $x$ on the diagonal For example: Optimization that involves inverse operation. Reformulation of BQP to SDP The reason of using  $\operatorname{diag}$ is because it is used in several platforms such as MATLAB, and people generally understands what the function is supposed to do Is there a more linear algebra, step by step way of converting a   vector $x \in \mathbb{R}^n$ into a diagonal matrix with components on   the diagonal without having a define a function that directly performs   the task ? i.e. given $x$, we find a series of functions/steps $f_2 \circ f_1 (x)$ which give us the same matrix as  $\operatorname{diag}(x)$",,"['linear-algebra', 'matrices', 'soft-question', 'computational-mathematics', 'article-writing']"
