,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A group in which every proper subgroup is contained in a maximal subgroup,A group in which every proper subgroup is contained in a maximal subgroup,,Let $G$ be a group in which every proper subgroup is contained in a maximal subgroup of $G$. Can we conclude that $G$ is finitely generated? (@Max commented that converse of this statement is true.),Let $G$ be a group in which every proper subgroup is contained in a maximal subgroup of $G$. Can we conclude that $G$ is finitely generated? (@Max commented that converse of this statement is true.),,"['abstract-algebra', 'group-theory']"
1,Usage of triangulated categories,Usage of triangulated categories,,"As the title indicates the question has to do with the usage of triangulated categories. What's the aim behind their study and where do we need them? Is there some algebraic insight behind their study, or some geometric intuition? Because there is a huge theory behind them I guess that there should be a well-defined reason for their existence. Applications/Examples are more than welcomed! References instead are welcomed too.","As the title indicates the question has to do with the usage of triangulated categories. What's the aim behind their study and where do we need them? Is there some algebraic insight behind their study, or some geometric intuition? Because there is a huge theory behind them I guess that there should be a well-defined reason for their existence. Applications/Examples are more than welcomed! References instead are welcomed too.",,['abstract-algebra']
2,A few questions on the Gaussian integers,A few questions on the Gaussian integers,,"I have a few questions surrounding the Gaussian integers, which I hope can be answered together in one fell swoop. The Gaussian integers are defined as $\mathbb{Z}[i] = \{x + iy : x, y \in \mathbb{Z}\}$. What is the intuition for working with them, and why should we care about them? What is arithmetic like in $\mathbb{Z}[i]$? Are there ""prime numbers"" in $\mathbb{Z}[i]$? Do Gaussian integers factor into primes? If so, do they factor uniquely?","I have a few questions surrounding the Gaussian integers, which I hope can be answered together in one fell swoop. The Gaussian integers are defined as $\mathbb{Z}[i] = \{x + iy : x, y \in \mathbb{Z}\}$. What is the intuition for working with them, and why should we care about them? What is arithmetic like in $\mathbb{Z}[i]$? Are there ""prime numbers"" in $\mathbb{Z}[i]$? Do Gaussian integers factor into primes? If so, do they factor uniquely?",,"['abstract-algebra', 'number-theory']"
3,polynomial with all roots on the unit circle,polynomial with all roots on the unit circle,,"I'm wondering if the following statement is true: if all roots of a polynomial with integer coefficients are on the unit circle, then these roots are in fact roots of unity and the polynomial is a product of cyclotomic polynomial.","I'm wondering if the following statement is true: if all roots of a polynomial with integer coefficients are on the unit circle, then these roots are in fact roots of unity and the polynomial is a product of cyclotomic polynomial.",,['abstract-algebra']
4,Cancellation laws in Rings,Cancellation laws in Rings,,In rings left and right cancellation laws generally don't hold. can anyone generalize some cases so that we are ensured  when the cancellation laws hold in rings?( the case I found was in Integral domains they hold. ),In rings left and right cancellation laws generally don't hold. can anyone generalize some cases so that we are ensured  when the cancellation laws hold in rings?( the case I found was in Integral domains they hold. ),,"['abstract-algebra', 'ring-theory']"
5,"Show that $(\mathbb Z[x],+)$ and $(\mathbb Q_{>0},\cdot)$ are isomorphic groups [closed]",Show that  and  are isomorphic groups [closed],"(\mathbb Z[x],+) (\mathbb Q_{>0},\cdot)","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $(\mathbb{Z}[x],+)$ be the additive group of polynomials with integer coefficients and $ (\mathbb{Q}_{>0},\cdot)$ the multiplicative group of positive rationals. Show these groups are isomorphic. Thanks","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $(\mathbb{Z}[x],+)$ be the additive group of polynomials with integer coefficients and $ (\mathbb{Q}_{>0},\cdot)$ the multiplicative group of positive rationals. Show these groups are isomorphic. Thanks",,"['abstract-algebra', 'group-theory']"
6,What characterizes topological spaces where every open set is closed?,What characterizes topological spaces where every open set is closed?,,"Motivated by the valuation topology on a discrete valuation ring, which has the above property, I want to know if there's some (subjectively, probably) nicer criterion for when a space has every open set closed. A preliminary guess (justified again by discrete valuation rings) is that a totally disconnected space has the above property; but this seems qualitatively too simple to actively characterize such a space, and certainly there's no apparent proof that totally disconnected implies this clopenness property. (Considering I'm mostly interested in this in the setting of topological rings, I'd be interested in the class of rings that satisfy this property, too, and thus the abstract algebra tag.)","Motivated by the valuation topology on a discrete valuation ring, which has the above property, I want to know if there's some (subjectively, probably) nicer criterion for when a space has every open set closed. A preliminary guess (justified again by discrete valuation rings) is that a totally disconnected space has the above property; but this seems qualitatively too simple to actively characterize such a space, and certainly there's no apparent proof that totally disconnected implies this clopenness property. (Considering I'm mostly interested in this in the setting of topological rings, I'd be interested in the class of rings that satisfy this property, too, and thus the abstract algebra tag.)",,['abstract-algebra']
7,Can every ideal have a minimal generating set?,Can every ideal have a minimal generating set?,,"Let $I$ be an ideal of commutative ring $A$ with unity. Does $I$ have a minimal generating set? At times, I am able to compute what they are for specific example, but it seems like it is true in general with some existence proof (I guess it is true for non-commutative rings with some adjectives such as ""left"" or ""right""). I have thought about Zorn's lemma but generating sets are very likely to be incomparable (nor the existence of lower bound of each chain was not clear) and the intersection of all generating sets may be too small to generate an ideal. Sounds not too difficult, but it seems not clear what to do.","Let $I$ be an ideal of commutative ring $A$ with unity. Does $I$ have a minimal generating set? At times, I am able to compute what they are for specific example, but it seems like it is true in general with some existence proof (I guess it is true for non-commutative rings with some adjectives such as ""left"" or ""right""). I have thought about Zorn's lemma but generating sets are very likely to be incomparable (nor the existence of lower bound of each chain was not clear) and the intersection of all generating sets may be too small to generate an ideal. Sounds not too difficult, but it seems not clear what to do.",,"['abstract-algebra', 'commutative-algebra', 'ideals']"
8,Is the general linear group generated by elementary matrices?,Is the general linear group generated by elementary matrices?,,"(Cfr. Wikipedia for the definition of Elementary matrix). Have a look at the following excerpt of Jacobson's Basic algebra vol.I, 2nd edition, pag.186. There exist PID in which not every invertible matrix is a product of elementary ones. An example of this type is given in a paper by P.M.Cohn, On the structure of the $\text{GL}_2$ of a ring , Institut des Hautes Etudes Scientifiques, #30 (1966), pp 5 - 54. This leaves me puzzled. Take an invertible matrix $A$ over a PID. Then $A$ has a Smith normal form , that is, up to elementary row and columns operations it is equivalent to something like this $$\begin{bmatrix} d_1 & && \\ & d_2 &&\\ &&\ddots&\\ &&&d_n\end{bmatrix}.$$ In particular $\det A= d_1\ldots d_n u$ for some unit element $u$ . But $\det A$ needs be unit, so all of $d_i$ 's are units, which means that up to some other elementary row operation $A$ is equivalent to the identity matrix. It seems to me that we have just proven that $A$ is the product of elementary matrices, which is false as of Jacobson's claim. There must be an error somewhere, but where? Thank you.","(Cfr. Wikipedia for the definition of Elementary matrix). Have a look at the following excerpt of Jacobson's Basic algebra vol.I, 2nd edition, pag.186. There exist PID in which not every invertible matrix is a product of elementary ones. An example of this type is given in a paper by P.M.Cohn, On the structure of the of a ring , Institut des Hautes Etudes Scientifiques, #30 (1966), pp 5 - 54. This leaves me puzzled. Take an invertible matrix over a PID. Then has a Smith normal form , that is, up to elementary row and columns operations it is equivalent to something like this In particular for some unit element . But needs be unit, so all of 's are units, which means that up to some other elementary row operation is equivalent to the identity matrix. It seems to me that we have just proven that is the product of elementary matrices, which is false as of Jacobson's claim. There must be an error somewhere, but where? Thank you.",\text{GL}_2 A A \begin{bmatrix} d_1 & && \\ & d_2 &&\\ &&\ddots&\\ &&&d_n\end{bmatrix}. \det A= d_1\ldots d_n u u \det A d_i A A,"['abstract-algebra', 'group-theory', 'matrices']"
9,the ring of dual numbers over a field $k$,the ring of dual numbers over a field,k,"Suppose $k$ is a field,then the quotient ring $k[\epsilon]/\epsilon^2$ is called the ring of dual numbers over $k$. I learn this from Hartshorne. I wonder why it has this name(maybe this question is a bit soft,or senseless). Are there any interesting things about this ring?Would someone be kind enough to say something about it?Thank you very much!","Suppose $k$ is a field,then the quotient ring $k[\epsilon]/\epsilon^2$ is called the ring of dual numbers over $k$. I learn this from Hartshorne. I wonder why it has this name(maybe this question is a bit soft,or senseless). Are there any interesting things about this ring?Would someone be kind enough to say something about it?Thank you very much!",,"['abstract-algebra', 'ring-theory', 'rngs']"
10,"Space with subtraction, but no addition","Space with subtraction, but no addition",,"Is there a name for a mathematical space with a distance and ordering defined, and a subtraction operation defined, but no addition operation or scalar multiplication?  Essentially this is like the real number line with no notion of an origin. The concrete example I'm thinking of is datetimes; given two datetimes, we can subtract them to find an interval.  We can add/subtract an interval to/from a datetime, or multiply an interval by a scalar, but we can't add two datetimes or multiply a datetime by a scalar. So if d1 and d2 are datetimes, then d1 + (d1 - d2) has a well-defined meaning, but (d1 + d1) - d2 does not. I'm also just realizing - the result of subtraction is not an element in the same space, so I guess this is a family of two sets $A$ and $B$ , where $a_i - a_j \in B$ $a_i \pm b_j \in A$ $b_i \pm b_j \in B$ $k \cdot b_i \in B$","Is there a name for a mathematical space with a distance and ordering defined, and a subtraction operation defined, but no addition operation or scalar multiplication?  Essentially this is like the real number line with no notion of an origin. The concrete example I'm thinking of is datetimes; given two datetimes, we can subtract them to find an interval.  We can add/subtract an interval to/from a datetime, or multiply an interval by a scalar, but we can't add two datetimes or multiply a datetime by a scalar. So if d1 and d2 are datetimes, then d1 + (d1 - d2) has a well-defined meaning, but (d1 + d1) - d2 does not. I'm also just realizing - the result of subtraction is not an element in the same space, so I guess this is a family of two sets and , where",A B a_i - a_j \in B a_i \pm b_j \in A b_i \pm b_j \in B k \cdot b_i \in B,['abstract-algebra']
11,Is this quotient ring finite?,Is this quotient ring finite?,,"Let $R$ be a commutative ring with unity satisfying $r^{10}=r^2$  $\forall$$r$$\in$$R$, and $P$ a prime ideal of $R$. The original question is to find the possible orders of the quotient ring $R/P$. My thoughts: If $R/P$ is finite it is a finite domain which is a field and since $r^8=1$ iff $r\neq0$, isomorphic to the Galois field $GF(9), GF(5), GF(3),$ or $GF(2)$. It remains to show that $R/P$ is finite. Can I use the fact that $P$ is a prime ideal? Or should I somehow make use of the given equation? Thank you in advance.","Let $R$ be a commutative ring with unity satisfying $r^{10}=r^2$  $\forall$$r$$\in$$R$, and $P$ a prime ideal of $R$. The original question is to find the possible orders of the quotient ring $R/P$. My thoughts: If $R/P$ is finite it is a finite domain which is a field and since $r^8=1$ iff $r\neq0$, isomorphic to the Galois field $GF(9), GF(5), GF(3),$ or $GF(2)$. It remains to show that $R/P$ is finite. Can I use the fact that $P$ is a prime ideal? Or should I somehow make use of the given equation? Thank you in advance.",,"['abstract-algebra', 'ring-theory']"
12,Is $X$ irreducible in $R[X]$?,Is  irreducible in ?,X R[X],"Suppose we are working with a commutative ring $R$. I am reading a proof where the author writes ""If we are in $R[X]$ and we know that $f\mid X$. Thus either $f$ is a unit multiple of $X$ or it is a unit, so $(f)=(X)$ or $(f)=(1)$."" This seems to assume that $X$ is irreducible in $R[X]$. However I don't know how we would prove this unless we assume $R$ is an integral domain. Solving by hand$$(a_0+a_1X+\cdots+a_nX^n)(b_0+b_1X+\cdots+b_mX^m)=X$$ does not seem to require that one of these polynomials is a unit. Am I missing something here?","Suppose we are working with a commutative ring $R$. I am reading a proof where the author writes ""If we are in $R[X]$ and we know that $f\mid X$. Thus either $f$ is a unit multiple of $X$ or it is a unit, so $(f)=(X)$ or $(f)=(1)$."" This seems to assume that $X$ is irreducible in $R[X]$. However I don't know how we would prove this unless we assume $R$ is an integral domain. Solving by hand$$(a_0+a_1X+\cdots+a_nX^n)(b_0+b_1X+\cdots+b_mX^m)=X$$ does not seem to require that one of these polynomials is a unit. Am I missing something here?",,"['abstract-algebra', 'ring-theory', 'irreducible-polynomials']"
13,"Proving that if $(ab)^{p}=a^{p}\,b^{p}$, then the p-sylow subgroup is normal","Proving that if , then the p-sylow subgroup is normal","(ab)^{p}=a^{p}\,b^{p}","So, while studying Abstract Algebra, i ran into this problem (i.n. Herstein, second edition, chapter 2.12) and have been stuck since: Given a group of finite order, and a prime $p$ that divides $o(G)$, and suppose that $\forall\,a,b\in G$, $(ab)^{p}=a^{p}\,b^{p}$. Prove that the $p$-sylow subgroup is normal in $G$. What I've tried: I defined a mapping $\varphi:G\to H=\lbrace x^{p}:\,x\in G\rbrace;\,\,\varphi(x)=x^{p}$, wich would be a surjective homomorphism. Then, I proved that $\ker(\varphi)\subseteq P$, where $P$ is a $p$-sylow subgroup. If I could prove either that $P\subseteq\ker(\varphi)$ or that $o(\ker(\varphi))=o(P)$, that would end it, because that would imply that $P=\ker(\varphi)$, and I know that $\ker(\varphi)\unlhd G$. One ideia to follow up on those would be to use the firs isomorphism theorem ($G/\ker(\varphi)\simeq Im(\varphi)$) to get $o(G)=o(\ker(\varphi))o(Im(\varphi))$ and from there work something out about the orders, but I cannot think of how to do that. I've also tried proving that $G$ only has one $p$-sylow subgroup, using Sylow's third theorem, but I believe it's a dead end. Any ideas?","So, while studying Abstract Algebra, i ran into this problem (i.n. Herstein, second edition, chapter 2.12) and have been stuck since: Given a group of finite order, and a prime $p$ that divides $o(G)$, and suppose that $\forall\,a,b\in G$, $(ab)^{p}=a^{p}\,b^{p}$. Prove that the $p$-sylow subgroup is normal in $G$. What I've tried: I defined a mapping $\varphi:G\to H=\lbrace x^{p}:\,x\in G\rbrace;\,\,\varphi(x)=x^{p}$, wich would be a surjective homomorphism. Then, I proved that $\ker(\varphi)\subseteq P$, where $P$ is a $p$-sylow subgroup. If I could prove either that $P\subseteq\ker(\varphi)$ or that $o(\ker(\varphi))=o(P)$, that would end it, because that would imply that $P=\ker(\varphi)$, and I know that $\ker(\varphi)\unlhd G$. One ideia to follow up on those would be to use the firs isomorphism theorem ($G/\ker(\varphi)\simeq Im(\varphi)$) to get $o(G)=o(\ker(\varphi))o(Im(\varphi))$ and from there work something out about the orders, but I cannot think of how to do that. I've also tried proving that $G$ only has one $p$-sylow subgroup, using Sylow's third theorem, but I believe it's a dead end. Any ideas?",,"['abstract-algebra', 'finite-groups', 'sylow-theory']"
14,Show that a radical ideal has no embedded prime ideals. [closed],Show that a radical ideal has no embedded prime ideals. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $A$ be a commutative ring and $I$ a decomposable ideal. Let $I=\bigcap_{k=1}^{n} I_k$ be a minimal primary decomposition. Show that if $I=\sqrt{I}$ then $I$ has no embedded prime ideals. (I noticed that $I=\bigcap_{k=1}^{n} P_k$ where $P_k=\sqrt{I_k}$, $\forall k\in\lbrace1,...,n\rbrace$. I have to show that $P_i \nsubseteq P_j, \forall  i\neq j $.)","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $A$ be a commutative ring and $I$ a decomposable ideal. Let $I=\bigcap_{k=1}^{n} I_k$ be a minimal primary decomposition. Show that if $I=\sqrt{I}$ then $I$ has no embedded prime ideals. (I noticed that $I=\bigcap_{k=1}^{n} P_k$ where $P_k=\sqrt{I_k}$, $\forall k\in\lbrace1,...,n\rbrace$. I have to show that $P_i \nsubseteq P_j, \forall  i\neq j $.)",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'ideals', 'maximal-and-prime-ideals']"
15,Proving $\mathbb{Z}[\sqrt {10}]$ is not a UFD,Proving  is not a UFD,\mathbb{Z}[\sqrt {10}],"I am wondering how to show that $\mathbb{Z}[\sqrt {10}]$ is not a UFD. My only idea is to show that there are two factorizations of $10$, say, $ab, uv$ such that $a$ is not a unit times $u$ or $v$. In this ring $10=2\cdot5=\sqrt {10}\cdot \sqrt {10}$, so it suffices to show $2$ is not a unit times $\sqrt {10}$. Suppose $2=\sqrt {10}(a+b\sqrt{10})=a\sqrt{10}+10b$. Then $a=0$ since $\sqrt{10}$ is not rational. So $10b=2$, which has no integer solutions. So $\mathbb{Z}[\sqrt {10}]$ is not a UFD. Is my reasoning correct? What are the flaws?","I am wondering how to show that $\mathbb{Z}[\sqrt {10}]$ is not a UFD. My only idea is to show that there are two factorizations of $10$, say, $ab, uv$ such that $a$ is not a unit times $u$ or $v$. In this ring $10=2\cdot5=\sqrt {10}\cdot \sqrt {10}$, so it suffices to show $2$ is not a unit times $\sqrt {10}$. Suppose $2=\sqrt {10}(a+b\sqrt{10})=a\sqrt{10}+10b$. Then $a=0$ since $\sqrt{10}$ is not rational. So $10b=2$, which has no integer solutions. So $\mathbb{Z}[\sqrt {10}]$ is not a UFD. Is my reasoning correct? What are the flaws?",,"['abstract-algebra', 'solution-verification', 'unique-factorization-domains']"
16,Profinite topology of a Group,Profinite topology of a Group,,"Let $G$ be a group. Consider now the set of all (left for instance) cosets in $G$ of subgroups of finite index. This set is a base for a topology in $G$. I found somewhere that if $G$ is residually finite then $G$ is a compact space in this topology but I cannot see why. First of all, is it true? Second, any (maybe more group theoretical) hints?","Let $G$ be a group. Consider now the set of all (left for instance) cosets in $G$ of subgroups of finite index. This set is a base for a topology in $G$. I found somewhere that if $G$ is residually finite then $G$ is a compact space in this topology but I cannot see why. First of all, is it true? Second, any (maybe more group theoretical) hints?",,"['abstract-algebra', 'general-topology', 'group-theory', 'topological-groups', 'profinite-groups']"
17,"Modern research into Grassman's ""theory of forms""?","Modern research into Grassman's ""theory of forms""?",,"I quote from Petsche's Hermann Graßmann: Biography (emphasis mine): The mathematical part of the book begins with the conception of the “General Theory of Forms”. Starting with a perspective on mathematics as a theory of forms, Graßmann analyses in the most abstract way possible the general structures of concrete “conjunctions of forms”. Here, he places special emphasis on “elementary conjunctions”, demanding they have module properties, that is, associativity, commutativity and an inverse and neutral element. The so-defined conjunction of the first order, or “formal addition”, is then followed up by an investigation of a conjunction of the second order (“formal multiplication”), for which    he only requires distributivity with respect to formal addition. Graßmann directly posits    the validity of the module properties for formal addition and distributivity for formal    multiplication as the principles for constructing these conjunctions: “This generally is the    way”, he wrote, “that initially, that is when no species of conjunction is yet given, such a    conjunction of next higher order is defined.” Since Graßmann does not require the forms generated by conjunctions of the second order to be embedded in the fundamental domain, he can use this form of conjunction for the formal generation of new mathematical objects in the further course of the    text. ... After Graßmann has laid down the foundation for all mathematical disciplines by    presenting these uniquely generalized group-theoretical and structural abstractions he starts with the actual presentation of his new mathematical discipline. What is the modern terminology for Grassmann's ""General Theory of Forms""? What research work has been done in order to continue this line of thinking? Which resources could I acquaint myself with in order to answer these questions? I think the answer is simply (and very generally, thus unhelpfully) ""universal algebra"": http://en.wikipedia.org/wiki/Algebraic_structure Am I on the right track? I am not sure. See also this question of mine, which is looking for something similar in spirit. See also (again, unsure of the relevancy): http://arxiv.org/pdf/0904.3349v1.pdf Can someone with familiarity weave together a proper answer from these three resources and others as appropriate?","I quote from Petsche's Hermann Graßmann: Biography (emphasis mine): The mathematical part of the book begins with the conception of the “General Theory of Forms”. Starting with a perspective on mathematics as a theory of forms, Graßmann analyses in the most abstract way possible the general structures of concrete “conjunctions of forms”. Here, he places special emphasis on “elementary conjunctions”, demanding they have module properties, that is, associativity, commutativity and an inverse and neutral element. The so-defined conjunction of the first order, or “formal addition”, is then followed up by an investigation of a conjunction of the second order (“formal multiplication”), for which    he only requires distributivity with respect to formal addition. Graßmann directly posits    the validity of the module properties for formal addition and distributivity for formal    multiplication as the principles for constructing these conjunctions: “This generally is the    way”, he wrote, “that initially, that is when no species of conjunction is yet given, such a    conjunction of next higher order is defined.” Since Graßmann does not require the forms generated by conjunctions of the second order to be embedded in the fundamental domain, he can use this form of conjunction for the formal generation of new mathematical objects in the further course of the    text. ... After Graßmann has laid down the foundation for all mathematical disciplines by    presenting these uniquely generalized group-theoretical and structural abstractions he starts with the actual presentation of his new mathematical discipline. What is the modern terminology for Grassmann's ""General Theory of Forms""? What research work has been done in order to continue this line of thinking? Which resources could I acquaint myself with in order to answer these questions? I think the answer is simply (and very generally, thus unhelpfully) ""universal algebra"": http://en.wikipedia.org/wiki/Algebraic_structure Am I on the right track? I am not sure. See also this question of mine, which is looking for something similar in spirit. See also (again, unsure of the relevancy): http://arxiv.org/pdf/0904.3349v1.pdf Can someone with familiarity weave together a proper answer from these three resources and others as appropriate?",,"['abstract-algebra', 'reference-request', 'self-learning']"
18,The set consisting of all zero divisors in a commutative ring with unity contains at least one prime ideal [duplicate],The set consisting of all zero divisors in a commutative ring with unity contains at least one prime ideal [duplicate],,"This question already has answers here : Showing the set of zero-divisors is a union of prime ideals (5 answers) Closed 23 days ago . I'm asked to prove that the set consisting of all zero divisors in a commutative ring with unity contains at least one prime ideal. I can't even start in the proof, I've just defined my set but cant move on construction the ideal !","This question already has answers here : Showing the set of zero-divisors is a union of prime ideals (5 answers) Closed 23 days ago . I'm asked to prove that the set consisting of all zero divisors in a commutative ring with unity contains at least one prime ideal. I can't even start in the proof, I've just defined my set but cant move on construction the ideal !",,"['abstract-algebra', 'ring-theory', 'ideals']"
19,The existence of a group automorphism with some properties implies commutativity.,The existence of a group automorphism with some properties implies commutativity.,,"Let $G $ be a finite group, $T$ be an automorphisom of $ G $ st $ Tx = x \iff x=e $. Suppose further that $ T^2 =I $. Prove that $ G $ is abelian. I was thinking  if  I show $ T aba^{-1} b^ {-1}=aba^ {-1}b^{-1} \forall a, b \in G$. But I was unable to show it. Please  give me any hints about it.","Let $G $ be a finite group, $T$ be an automorphisom of $ G $ st $ Tx = x \iff x=e $. Suppose further that $ T^2 =I $. Prove that $ G $ is abelian. I was thinking  if  I show $ T aba^{-1} b^ {-1}=aba^ {-1}b^{-1} \forall a, b \in G$. But I was unable to show it. Please  give me any hints about it.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups']"
20,How can I know how many real roots this polynomial has?,How can I know how many real roots this polynomial has?,,"Let $x^7-10x^5+15x+5$ a polynomial over $\mathbb Q$ . I would like to know how many real roots this polynomial has? I know we have to use the intermediate value theorem, but I don't know how to use in this case in particular. I need help please. Thanks","Let a polynomial over . I would like to know how many real roots this polynomial has? I know we have to use the intermediate value theorem, but I don't know how to use in this case in particular. I need help please. Thanks",x^7-10x^5+15x+5 \mathbb Q,"['abstract-algebra', 'polynomials', 'field-theory']"
21,Universal property of tensor product [duplicate],Universal property of tensor product [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Equality of two notions of tensor products over a commutative ring Let $A$ be a commutative ring. There are two common definitions of tensor product of two $A$-modules $M$ and $N$ in terms of universal property. One definition defines it as a universal object of a map $$\tau:M\times N\rightarrow G,$$ where $G$ is an abelian group, and $\tau$ is $A$-balanced, i.e., $\tau(m+m',n)=\tau(m,n)+\tau(m',n)$, $\tau(m,n+n')=\tau(m,n)+\tau(m,n')$, $\tau(a\cdot m,n)=\tau(m,a\cdot n)$. Another definition defines it as a universal object of a map $$\sigma:M\times N\rightarrow L,$$ where $L$ is an $A$-module, and $\sigma$ is $A$-bilinear, i.e., $\sigma(m+m',n)=\sigma(m,n)+\sigma(m',n)$, $\sigma(m,n+n')=\sigma(m,n)+\sigma(m,n')$, $\sigma(a\cdot m,n)=a\cdot\sigma(m,n)$, $\sigma(m,a\cdot n)=a\cdot\sigma(m,n)$. I always thought they were equivalent definitions, but now I made a couple of observations: Let $\tau:M\times N\rightarrow G $ be a universal object in the first sense. If $G$ happens to be an $A$-module and $\tau$ happens to be $A$-bilinear, then $\tau$ is a universal object in the second sense. Let $\sigma:M\times N\rightarrow L$ be a universal object in the second sense. It doesn't seem to follow that $\sigma$ is a universal object in the first sense. So intuitively, it seems that ""the set of the first universal objects contains the set of the second universal objects"". Am I correct? If so, then it seems necessary to distinguish the two definitions of tensor product.","This question already has answers here : Closed 11 years ago . Possible Duplicate: Equality of two notions of tensor products over a commutative ring Let $A$ be a commutative ring. There are two common definitions of tensor product of two $A$-modules $M$ and $N$ in terms of universal property. One definition defines it as a universal object of a map $$\tau:M\times N\rightarrow G,$$ where $G$ is an abelian group, and $\tau$ is $A$-balanced, i.e., $\tau(m+m',n)=\tau(m,n)+\tau(m',n)$, $\tau(m,n+n')=\tau(m,n)+\tau(m,n')$, $\tau(a\cdot m,n)=\tau(m,a\cdot n)$. Another definition defines it as a universal object of a map $$\sigma:M\times N\rightarrow L,$$ where $L$ is an $A$-module, and $\sigma$ is $A$-bilinear, i.e., $\sigma(m+m',n)=\sigma(m,n)+\sigma(m',n)$, $\sigma(m,n+n')=\sigma(m,n)+\sigma(m,n')$, $\sigma(a\cdot m,n)=a\cdot\sigma(m,n)$, $\sigma(m,a\cdot n)=a\cdot\sigma(m,n)$. I always thought they were equivalent definitions, but now I made a couple of observations: Let $\tau:M\times N\rightarrow G $ be a universal object in the first sense. If $G$ happens to be an $A$-module and $\tau$ happens to be $A$-bilinear, then $\tau$ is a universal object in the second sense. Let $\sigma:M\times N\rightarrow L$ be a universal object in the second sense. It doesn't seem to follow that $\sigma$ is a universal object in the first sense. So intuitively, it seems that ""the set of the first universal objects contains the set of the second universal objects"". Am I correct? If so, then it seems necessary to distinguish the two definitions of tensor product.",,"['abstract-algebra', 'modules', 'tensor-products']"
22,"Are there any examples of rings $R$ such that $\mathrm{End}(R,+,0)\not\cong R$?",Are there any examples of rings  such that ?,"R \mathrm{End}(R,+,0)\not\cong R","In a handful of examples, I've noticed that the endomorphism ring $\mathrm{End}(R,+,0)$ is isomorphic to the ring $R$ itself. For instance, $\mathrm{End}(\mathbb{Z},+,0)\cong\mathbb{Z}$ and $\mathrm{End}(\mathbb{Z}/n\mathbb{Z},+,0)\cong\mathbb{Z}/n\mathbb{Z}$. Is this true in general, or are there examples of rings which are not isomorphic to the endomorphism ring as above? If not, is it at least always true for $R$ a field? Thanks.","In a handful of examples, I've noticed that the endomorphism ring $\mathrm{End}(R,+,0)$ is isomorphic to the ring $R$ itself. For instance, $\mathrm{End}(\mathbb{Z},+,0)\cong\mathbb{Z}$ and $\mathrm{End}(\mathbb{Z}/n\mathbb{Z},+,0)\cong\mathbb{Z}/n\mathbb{Z}$. Is this true in general, or are there examples of rings which are not isomorphic to the endomorphism ring as above? If not, is it at least always true for $R$ a field? Thanks.",,"['abstract-algebra', 'ring-theory']"
23,"What is the abstract algebraic structure of ""Elementary Algebra""?","What is the abstract algebraic structure of ""Elementary Algebra""?",,"From what I understand the Set of Complex Numbers is Closed under Addition, Subtraction, Multiplication, Division, Exponentiation, Radicals and Logarithms. I understand this to mean that these binary operations applied to any two elements of this Set will always result in a third element also contained within this set (the complex numbers). (However upon further reflection perhaps division by zero creates an exception to this) My question is: what sort of algebraic structure does this create? I've been learning about Rings, Groups, Fields etc. But have always been left wondering what structure arises from all the standard tools of Elementary Algebra taken as a whole. That is the set of Complex Numbers and the standard operations I listed at the start.","From what I understand the Set of Complex Numbers is Closed under Addition, Subtraction, Multiplication, Division, Exponentiation, Radicals and Logarithms. I understand this to mean that these binary operations applied to any two elements of this Set will always result in a third element also contained within this set (the complex numbers). (However upon further reflection perhaps division by zero creates an exception to this) My question is: what sort of algebraic structure does this create? I've been learning about Rings, Groups, Fields etc. But have always been left wondering what structure arises from all the standard tools of Elementary Algebra taken as a whole. That is the set of Complex Numbers and the standard operations I listed at the start.",,['abstract-algebra']
24,"Is the quotient of a complete ring, complete?","Is the quotient of a complete ring, complete?",,"If $(R,\mathfrak m)$ is a complete local ring (with respect to the $\mathfrak m$ -adic topology) and $I$ a prime ideal in $R$ , is $R/I$ complete (with respect to the $\mathfrak m/I$ -adic topology)? It seems too strong, but I am unable to give a counterexample.","If is a complete local ring (with respect to the -adic topology) and a prime ideal in , is complete (with respect to the -adic topology)? It seems too strong, but I am unable to give a counterexample.","(R,\mathfrak m) \mathfrak m I R R/I \mathfrak m/I","['abstract-algebra', 'commutative-algebra', 'ring-theory']"
25,Element of a group G raise to the index of a subgroup H is in H,Element of a group G raise to the index of a subgroup H is in H,,"Let $H$ be normal in $G$ and $[G:H]=m$. Prove that $x^m$ is in $H$ for   all $x$ in $G$. Since $H$ is normal its left cosets form a group. Also since $m$ is the order of that group $(xH)^m=x^mH=H$, which implies that $x^m \in H$. I am actually not sure what my question is, it just hard to believe that if I pick any element of a group $G$ and I raise it to $m$ the resulting element will be in $H$. Anyways, is there another way one prove this? Thanks.","Let $H$ be normal in $G$ and $[G:H]=m$. Prove that $x^m$ is in $H$ for   all $x$ in $G$. Since $H$ is normal its left cosets form a group. Also since $m$ is the order of that group $(xH)^m=x^mH=H$, which implies that $x^m \in H$. I am actually not sure what my question is, it just hard to believe that if I pick any element of a group $G$ and I raise it to $m$ the resulting element will be in $H$. Anyways, is there another way one prove this? Thanks.",,['abstract-algebra']
26,Elementary number theory results that are not generalized by ring or group theory,Elementary number theory results that are not generalized by ring or group theory,,"I've taken an undergraduate course in ring and group theory, but haven't studied number theory formally. I've noticed that many important results in number theory have been generalized in group/ring theory (e.g. Lagrange's Theorem generalizing Fermat's Little Theorem). My question is, are there any results in elementary number theory that have not been generalized using elementary group and ring theory?","I've taken an undergraduate course in ring and group theory, but haven't studied number theory formally. I've noticed that many important results in number theory have been generalized in group/ring theory (e.g. Lagrange's Theorem generalizing Fermat's Little Theorem). My question is, are there any results in elementary number theory that have not been generalized using elementary group and ring theory?",,"['abstract-algebra', 'elementary-number-theory', 'soft-question']"
27,When is $M \otimes_A -$ representable?,When is  representable?,M \otimes_A -,"Let $A$ be a commutative ring, $M$ be a $A$-module. When is $M \otimes_A - : A\text{-mod} \rightarrow A\text{-mod}$ representable? In other words, when will there exist a $A$-module $P$ s.t $M \otimes_A -=Hom(P,-)$ ? A neccessary condition is that $M$ is flat, a sufficient condition is that $M$ is free of finite rank. I wonder whether flatness or projectivity is sufficient.","Let $A$ be a commutative ring, $M$ be a $A$-module. When is $M \otimes_A - : A\text{-mod} \rightarrow A\text{-mod}$ representable? In other words, when will there exist a $A$-module $P$ s.t $M \otimes_A -=Hom(P,-)$ ? A neccessary condition is that $M$ is flat, a sufficient condition is that $M$ is free of finite rank. I wonder whether flatness or projectivity is sufficient.",,['abstract-algebra']
28,Is there a term for a magma with identity (only)?,Is there a term for a magma with identity (only)?,,"If we start from magmas and consider: associativity, identity, invertibility (divisibility). We will theoretically get $2^3=8$ structures by regarding whether such structure possess these properties. As is shown in the picture: https://en.wikipedia.org/wiki/Magma_(algebra)#/media/File:Magma_to_group2.svg There miss two structures - associative, divisible magma and magma with identity. Well, we know associative quasigroup is a group. So, there is only one left. Is there a term for it? And it is necessary to differ between invertibility and divisibility?","If we start from magmas and consider: associativity, identity, invertibility (divisibility). We will theoretically get $2^3=8$ structures by regarding whether such structure possess these properties. As is shown in the picture: https://en.wikipedia.org/wiki/Magma_(algebra)#/media/File:Magma_to_group2.svg There miss two structures - associative, divisible magma and magma with identity. Well, we know associative quasigroup is a group. So, there is only one left. Is there a term for it? And it is necessary to differ between invertibility and divisibility?",,"['abstract-algebra', 'group-theory', 'magma']"
29,Commutativity of a ring from idempotents. [closed],Commutativity of a ring from idempotents. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question In a ring $R$ with unity, every element can be written as product of finitely many idempotents. Can one show that the ring is commutative?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question In a ring $R$ with unity, every element can be written as product of finitely many idempotents. Can one show that the ring is commutative?",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'idempotents']"
30,What is necessary and/or sufficient for polynomials to provide isomorphic quotient rings?,What is necessary and/or sufficient for polynomials to provide isomorphic quotient rings?,,"Let $R$ be a commutative ring (with identity). Let $f,g\in R\left[x\right]$ both be monic polynomials of degree $d$ . Then the underlying abelian groups of the rings $R\left[x\right]/\left(f\left(x\right)\right)$ and $R\left[x\right]/\left(g\left(x\right)\right)$ are isomorphic. Actually both are isomorphic to the direct sum $R^{\oplus d}$ . Questions: What sort of relation between $f$ and $g$ will ensure that $R\left[x\right]/\left(f\left(x\right)\right)$ and $R\left[x\right]/\left(g\left(x\right)\right)$ are also isomorphic as rings (with identities)? What sort of relation between $f$ and $g$ will ensure that $R\left[x\right]/\left(f\left(x\right)\right)$ and $R\left[x\right]/\left(g\left(x\right)\right)$ are also isomorphic as $R$ -algebras? Maybe my questions are too broad. Partial answers (e.g. with extra conditions on $R$ ) or references are also very welcome.",Let be a commutative ring (with identity). Let both be monic polynomials of degree . Then the underlying abelian groups of the rings and are isomorphic. Actually both are isomorphic to the direct sum . Questions: What sort of relation between and will ensure that and are also isomorphic as rings (with identities)? What sort of relation between and will ensure that and are also isomorphic as -algebras? Maybe my questions are too broad. Partial answers (e.g. with extra conditions on ) or references are also very welcome.,"R f,g\in R\left[x\right] d R\left[x\right]/\left(f\left(x\right)\right) R\left[x\right]/\left(g\left(x\right)\right) R^{\oplus d} f g R\left[x\right]/\left(f\left(x\right)\right) R\left[x\right]/\left(g\left(x\right)\right) f g R\left[x\right]/\left(f\left(x\right)\right) R\left[x\right]/\left(g\left(x\right)\right) R R","['abstract-algebra', 'polynomials', 'ring-theory', 'commutative-algebra']"
31,Showing that the intersection of all subgroups of order $n$ is normal,Showing that the intersection of all subgroups of order  is normal,n,"I have been thinking about this problem: ""Suppose that a group $G$ has a subgroup of order $n$.  Prove that the intersection of all subgroups of $G$ of order $n$ is a normal subgroup of $G$."" Unfortunately, I don't have much of a start.  I know that the intersection will be a subgroup of each of the subgroups of order $n$, and thus will have order dividing $n$.  I also know that conjugation preserves the order of elements, which suggests that $xNx^{-1}$ could be a subset of $N$ ($N$ being the intersection of all subgroups of order $n$), but I don't see why conjugation necessarily takes elements of $N$ to other elements of $N$. I'd really appreciate a hint on how to go about showing this.  Thanks.","I have been thinking about this problem: ""Suppose that a group $G$ has a subgroup of order $n$.  Prove that the intersection of all subgroups of $G$ of order $n$ is a normal subgroup of $G$."" Unfortunately, I don't have much of a start.  I know that the intersection will be a subgroup of each of the subgroups of order $n$, and thus will have order dividing $n$.  I also know that conjugation preserves the order of elements, which suggests that $xNx^{-1}$ could be a subset of $N$ ($N$ being the intersection of all subgroups of order $n$), but I don't see why conjugation necessarily takes elements of $N$ to other elements of $N$. I'd really appreciate a hint on how to go about showing this.  Thanks.",,"['abstract-algebra', 'group-theory']"
32,"How to show that the ring $S/A$ has no zero divisors? (Hungerford, Algebra, Problem 12, Chapter III, Section 2)","How to show that the ring  has no zero divisors? (Hungerford, Algebra, Problem 12, Chapter III, Section 2)",S/A,"I am having trouble with another homework problem (Hungerford, Algebra , Problem 12, Chapter III, Section 2). Let $R$ be a ring without identity and with no zero divisors. Let $S$ be the ring whose additive group is $R \times \mathbb{Z}$ with multiplication given by $$(r_1,n_1)(r_2,n_2)=(r_1r_2+n_2r_1+n_1r_2,n_1n_2).$$ Let $A=\{(r,n) \in S \mid rx+nx=0 \text{ for all } x \in R\}$ . (a) $A$ is an ideal in $S$ . (b) $S/A$ has an identity and contains a subring isomorphic to $R$ . (c) $S/A$ has no zero divisors. I've done (a) and (b), but haven't been able to crack (c). Here's what I have so far: (c) Suppose $((r, n) + A)((s, m) + A) = (0, 0) + A$ . Then $(r, n)(s, m) \in A$ , so $(rs + mr + ns)x + (nm)x = 0$ for all $x \in R$ . But $(rs + mr + ns)x + (nm)x = rsx + mrx + nsx + nmx = r(sx + mx) + n(sx + mx)$ . Now here we have an expression involving $r(something) + n(something)$ and $s(something) + m(something)$ , and I want to say that if it's zero for all $x$ then $rx + nx = 0$ for all $x$ or $sx + mx = 0$ for all $x$ , but all I seem to be able to say is that $rx + nx = 0$ for all $x = sx' + mx'$ , which is not (or not obviously) what I want. I appreciate any help!","I am having trouble with another homework problem (Hungerford, Algebra , Problem 12, Chapter III, Section 2). Let be a ring without identity and with no zero divisors. Let be the ring whose additive group is with multiplication given by Let . (a) is an ideal in . (b) has an identity and contains a subring isomorphic to . (c) has no zero divisors. I've done (a) and (b), but haven't been able to crack (c). Here's what I have so far: (c) Suppose . Then , so for all . But . Now here we have an expression involving and , and I want to say that if it's zero for all then for all or for all , but all I seem to be able to say is that for all , which is not (or not obviously) what I want. I appreciate any help!","R S R \times \mathbb{Z} (r_1,n_1)(r_2,n_2)=(r_1r_2+n_2r_1+n_1r_2,n_1n_2). A=\{(r,n) \in S \mid rx+nx=0 \text{ for all } x \in R\} A S S/A R S/A ((r, n) + A)((s, m) + A) = (0, 0) + A (r, n)(s, m) \in A (rs + mr + ns)x + (nm)x = 0 x \in R (rs + mr + ns)x + (nm)x = rsx + mrx + nsx + nmx = r(sx + mx) + n(sx + mx) r(something) + n(something) s(something) + m(something) x rx + nx = 0 x sx + mx = 0 x rx + nx = 0 x = sx' + mx'","['abstract-algebra', 'ring-theory', 'ideals']"
33,Subgroups of $S_4$ isomorphic to $S_3$ and $S_2$?,Subgroups of  isomorphic to  and ?,S_4 S_3 S_2,"It's a home work problem I got: Find $4$ different subgroups of $S_4$ isomorphic to $S_3$ and $9$ isomorphic to $S_2$. My approach is: since $S_3=\{1, (123),(132),(12),(23),(13)\}$, just take the groups of permutations on $\{1,2,3\},\{1,2,4\},\{1,3,4\},\{2,3,4\}$, obviously they are all subgroups of $S_4$. To find the isomorphism, for each subgroup, just assign 1 to the first element, 2 to the second, 3 to the third. This is going to be an isomorphism. For example, in the group of permutations on $\{1,2,4\}$, assign $(124)\rightarrow(123) (24)\rightarrow(23),(142)\rightarrow(132), (14)\rightarrow(13)$, etc. I think this method is fine, but I have trouble with the second part of the problem. That is, to find the $9$ different subgroups of $S_4$ isomorphic to $S_2$. When I pick $2$ elements out of $\{1,2,3,4\}$, there can only be $\frac{4!}{2!2!}=6$ ways, which means this method only gives $6$ different subgroups isomorphic to $S_2$, but the problem says there are $9$. Is my method wrong? Or are there some other subgroups that I've missed? Thanks!!!","It's a home work problem I got: Find $4$ different subgroups of $S_4$ isomorphic to $S_3$ and $9$ isomorphic to $S_2$. My approach is: since $S_3=\{1, (123),(132),(12),(23),(13)\}$, just take the groups of permutations on $\{1,2,3\},\{1,2,4\},\{1,3,4\},\{2,3,4\}$, obviously they are all subgroups of $S_4$. To find the isomorphism, for each subgroup, just assign 1 to the first element, 2 to the second, 3 to the third. This is going to be an isomorphism. For example, in the group of permutations on $\{1,2,4\}$, assign $(124)\rightarrow(123) (24)\rightarrow(23),(142)\rightarrow(132), (14)\rightarrow(13)$, etc. I think this method is fine, but I have trouble with the second part of the problem. That is, to find the $9$ different subgroups of $S_4$ isomorphic to $S_2$. When I pick $2$ elements out of $\{1,2,3,4\}$, there can only be $\frac{4!}{2!2!}=6$ ways, which means this method only gives $6$ different subgroups isomorphic to $S_2$, but the problem says there are $9$. Is my method wrong? Or are there some other subgroups that I've missed? Thanks!!!",,"['abstract-algebra', 'group-theory', 'permutations']"
34,(Ir)reducibility criteria for homogeneous polynomials,(Ir)reducibility criteria for homogeneous polynomials,,"Suppose I have a homogeneous polynomial in at least 3 variables over some algebraically closed field (of characteristic 0, if need be). Question : How may I test — by hand — whether it is irreducible? I understand that there aren't even yet algorithms which can decide whether a univariate polynomial is irreducible, but nonetheless there are tools like the Eisenstein criterion, reduction modulo $p$, and Gauss's lemma which come in useful. Example : How can I show that $X^n + Y^n + Z^n$ is irreducible over a field of characteristic 0? The following argument works for $n$ a power of 2 and base field $\mathbb{Q}$: we know that $Y^n + Z^n$ is irreducible, since otherwise we could produce a $n$-th root of $-1$ in $\mathbb{Q}$. Thus $\mathfrak{p} = (Y^n + Z^n)$ is a prime ideal, and we can apply the generalised Eisenstein criterion. But this argument breaks down when $n$ is divisible by an odd number (because then we could factor out ($X^m + Y^m$, where $m$ is the highest power of 2 dividing $n$) or when there are $n$-th roots of $-1$. Alternatively, for $n = 2$, I can use an ad hoc approach: if $G(X, Y, Z) H(X, Y, Z) = X^2 + Y^2 + Z^2$, treating $Y$ and $Z$ as constants, we see that we must have a difference-of-squares factorisation, so we have $- (Y^2 + Z^2)$ a square, and iterating the argument, we see this cannot possibly be the case, even if the field is algebraically closed. That kind of trick works well when there aren't any cross terms, but what about, say, the polynomial $X^2 Y^3 + Y^2 Z^3 + Z^2 X^3$? Here I'm quite lost. If the base field is algebraically closed, then substituting constants for $Z$ don't yield anything useful, since the resulting polynomial is always reducible. While it remains true that the polynomial is irreducible if if I can find an ideal $\mathfrak{I}$ such that the polynomial modulo $\mathfrak{I}$ is irreducible, it's not obvious to me whether any such ideals exist or not. Context : Some of my exercises / exam problems ask me to show that a given hypersurface in projective space is irreducible. I'm not sure how much weight is placed on it, but it would be reassuring if I could do it properly.","Suppose I have a homogeneous polynomial in at least 3 variables over some algebraically closed field (of characteristic 0, if need be). Question : How may I test — by hand — whether it is irreducible? I understand that there aren't even yet algorithms which can decide whether a univariate polynomial is irreducible, but nonetheless there are tools like the Eisenstein criterion, reduction modulo $p$, and Gauss's lemma which come in useful. Example : How can I show that $X^n + Y^n + Z^n$ is irreducible over a field of characteristic 0? The following argument works for $n$ a power of 2 and base field $\mathbb{Q}$: we know that $Y^n + Z^n$ is irreducible, since otherwise we could produce a $n$-th root of $-1$ in $\mathbb{Q}$. Thus $\mathfrak{p} = (Y^n + Z^n)$ is a prime ideal, and we can apply the generalised Eisenstein criterion. But this argument breaks down when $n$ is divisible by an odd number (because then we could factor out ($X^m + Y^m$, where $m$ is the highest power of 2 dividing $n$) or when there are $n$-th roots of $-1$. Alternatively, for $n = 2$, I can use an ad hoc approach: if $G(X, Y, Z) H(X, Y, Z) = X^2 + Y^2 + Z^2$, treating $Y$ and $Z$ as constants, we see that we must have a difference-of-squares factorisation, so we have $- (Y^2 + Z^2)$ a square, and iterating the argument, we see this cannot possibly be the case, even if the field is algebraically closed. That kind of trick works well when there aren't any cross terms, but what about, say, the polynomial $X^2 Y^3 + Y^2 Z^3 + Z^2 X^3$? Here I'm quite lost. If the base field is algebraically closed, then substituting constants for $Z$ don't yield anything useful, since the resulting polynomial is always reducible. While it remains true that the polynomial is irreducible if if I can find an ideal $\mathfrak{I}$ such that the polynomial modulo $\mathfrak{I}$ is irreducible, it's not obvious to me whether any such ideals exist or not. Context : Some of my exercises / exam problems ask me to show that a given hypersurface in projective space is irreducible. I'm not sure how much weight is placed on it, but it would be reassuring if I could do it properly.",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'polynomials']"
35,What's stopping us from defining isomorphism in general?,What's stopping us from defining isomorphism in general?,,"I'm told that an isomorphism is a kind of underdetermined term, unlike say, group isomorphism or ring isomorphism.  Why couldn't we just say $\phi$ is an isomorphism on object $A$ if for all operations (or primitive relations) $R$ on $A$ , and all $a, b \in A$ , $\phi(a) R \phi(b) = \phi(a R b)$ and it is a bijection.  I realize it requires some second-order quantification of relations, but that's not exactly unheard of. If I've been careful, then this definition of isomorphism when applied to groups gives us group isomorphisms, and when applied to rings gives us ring isomorphisms.  If we wanted to discuss things that preserved some but not all relations on an object, say for example we wanted to talk about preserving the abelian group operation on a ring, we could just refer to that as a group isomorphism on the ring or something like that.","I'm told that an isomorphism is a kind of underdetermined term, unlike say, group isomorphism or ring isomorphism.  Why couldn't we just say is an isomorphism on object if for all operations (or primitive relations) on , and all , and it is a bijection.  I realize it requires some second-order quantification of relations, but that's not exactly unheard of. If I've been careful, then this definition of isomorphism when applied to groups gives us group isomorphisms, and when applied to rings gives us ring isomorphisms.  If we wanted to discuss things that preserved some but not all relations on an object, say for example we wanted to talk about preserving the abelian group operation on a ring, we could just refer to that as a group isomorphism on the ring or something like that.","\phi A R A a, b \in A \phi(a) R \phi(b) = \phi(a R b)",['abstract-algebra']
36,Are $\mathbb Q\times\mathbb Q$ and $\mathbb Q\times\mathbb Q\times\mathbb Q$ isomorphic?,Are  and  isomorphic?,\mathbb Q\times\mathbb Q \mathbb Q\times\mathbb Q\times\mathbb Q,"I know $\mathbb Q$ is not isomorphic to $\mathbb Q\times \mathbb Q$ as groups. But is it true for $\mathbb Q\times\mathbb Q$ and $\mathbb Q\times\mathbb Q\times\mathbb Q$ ? That is, are $\mathbb Q\times\mathbb Q$ and $\mathbb Q\times\mathbb Q\times\mathbb Q$ isomorphic as groups?  Where $\mathbb Q$ is set of rational numbers.","I know is not isomorphic to as groups. But is it true for and ? That is, are and isomorphic as groups?  Where is set of rational numbers.",\mathbb Q \mathbb Q\times \mathbb Q \mathbb Q\times\mathbb Q \mathbb Q\times\mathbb Q\times\mathbb Q \mathbb Q\times\mathbb Q \mathbb Q\times\mathbb Q\times\mathbb Q \mathbb Q,"['abstract-algebra', 'group-theory']"
37,What's the intuition behind a split exact sequence?,What's the intuition behind a split exact sequence?,,"I've learned the following definition for a split exact sequence: A short exact sequence of $R$-module homomorphisms $0\to A \stackrel{f}{\to} B \stackrel{g}{\to} C \to 0$ is split if there exists a homomorphism $\alpha:B\to A$ such that $\alpha\circ f = 1$ or a homomorphism $\beta:C\to B$ such that $g\circ \beta = 1$. I can never remember whether the definition requires $\alpha\circ f = 1$ or $f \circ \alpha = 1$, and similarly for the other mapping, $\beta$. I suspect that my inability to remember this stems from an underlying lack of understanding about what split exact sequences are for, what a sequence being split really tells us about the modules and morphisms involved, and what concept split exact sequences are meant to capture or generalize. Is there a good way to remember the directions of composition for these definitions? What is the intuition, utility, and underlying meaning behind a split exact sequence?","I've learned the following definition for a split exact sequence: A short exact sequence of $R$-module homomorphisms $0\to A \stackrel{f}{\to} B \stackrel{g}{\to} C \to 0$ is split if there exists a homomorphism $\alpha:B\to A$ such that $\alpha\circ f = 1$ or a homomorphism $\beta:C\to B$ such that $g\circ \beta = 1$. I can never remember whether the definition requires $\alpha\circ f = 1$ or $f \circ \alpha = 1$, and similarly for the other mapping, $\beta$. I suspect that my inability to remember this stems from an underlying lack of understanding about what split exact sequences are for, what a sequence being split really tells us about the modules and morphisms involved, and what concept split exact sequences are meant to capture or generalize. Is there a good way to remember the directions of composition for these definitions? What is the intuition, utility, and underlying meaning behind a split exact sequence?",,"['abstract-algebra', 'modules']"
38,$A_5$ is the only subgroup of $S_5$ of order 60 [duplicate],is the only subgroup of  of order 60 [duplicate],A_5 S_5,"This question already has answers here : $A_n$ is the only subgroup of $S_n$ of index $2$. (7 answers) Closed 3 years ago . My attempt : Suppose $H$ is another subgroup of $S_5$ of order $60$ by Lagrange's Theorem $\vert H\cap A_5\vert ||H|$, then $|H \cap A_5| \in\{1,2,3,5,6,10,20,30,60\}$ $$\Rightarrow |HA_5| =\frac{|H||A_5|}{|H \cap A_5|}=3600,1800,1200,900,720,600,360,180,120,60$$ So, the only candidates for the order of $|H \cap A_5|$ are $30$ and $60$. Then I want to claim that the order must be $60$. thus $|H \cap A_5|=|A_5|$ $$\Rightarrow H=A_5$$ But I didn't know how to show the former case doesn't hold.  Please give me a hint please! PS. I haven't learned the concepts of normal group and simple group. So, I would like you to explain without using these concepts! Thank you.","This question already has answers here : $A_n$ is the only subgroup of $S_n$ of index $2$. (7 answers) Closed 3 years ago . My attempt : Suppose $H$ is another subgroup of $S_5$ of order $60$ by Lagrange's Theorem $\vert H\cap A_5\vert ||H|$, then $|H \cap A_5| \in\{1,2,3,5,6,10,20,30,60\}$ $$\Rightarrow |HA_5| =\frac{|H||A_5|}{|H \cap A_5|}=3600,1800,1200,900,720,600,360,180,120,60$$ So, the only candidates for the order of $|H \cap A_5|$ are $30$ and $60$. Then I want to claim that the order must be $60$. thus $|H \cap A_5|=|A_5|$ $$\Rightarrow H=A_5$$ But I didn't know how to show the former case doesn't hold.  Please give me a hint please! PS. I haven't learned the concepts of normal group and simple group. So, I would like you to explain without using these concepts! Thank you.",,"['abstract-algebra', 'group-theory', 'finite-groups']"
39,Intuitive meaning of transitive action,Intuitive meaning of transitive action,,"In the context of abstract algebra, following from Dummit's textbook.  The action of $G$ on $A$ is called transitive if there is only one orbit. $i.e$,  given any two elements $a,b \in A$, there is some $g \in G$ such that $a=g.b$. I want to know why we called this as transitive. From some basic set theory knowledge, In equivalence relation, transitivity,  $a \sim b$, $b\sim c$ then $a\sim c$. Does the same thing happens in the group action?","In the context of abstract algebra, following from Dummit's textbook.  The action of $G$ on $A$ is called transitive if there is only one orbit. $i.e$,  given any two elements $a,b \in A$, there is some $g \in G$ such that $a=g.b$. I want to know why we called this as transitive. From some basic set theory knowledge, In equivalence relation, transitivity,  $a \sim b$, $b\sim c$ then $a\sim c$. Does the same thing happens in the group action?",,['abstract-algebra']
40,Cyclotomic polynomials and Galois group,Cyclotomic polynomials and Galois group,,"Let $\zeta\in \mathbb C$ be a primitive $7^{th}$ root of unity. Show that there exists a $\sigma\in \operatorname{Gal}(\mathbb Q(\zeta)/\mathbb Q)$ such that $\sigma(\zeta)=\zeta^3$. I already know that $\zeta$ is a root of $f(x)=x^6+x^5+x^4+x^3+x^2+x+1$ and that $f$ is irreducible (By applying Eisenstein's criterion on $f(x+1)$). Also the powers of $\zeta$ are also roots of $f$. So $\mathbb Q(\zeta)$ is a splitting field of $f$. Now its clear that this extension is a Galois extension, since all the roots are different. But how to show the desired statement?","Let $\zeta\in \mathbb C$ be a primitive $7^{th}$ root of unity. Show that there exists a $\sigma\in \operatorname{Gal}(\mathbb Q(\zeta)/\mathbb Q)$ such that $\sigma(\zeta)=\zeta^3$. I already know that $\zeta$ is a root of $f(x)=x^6+x^5+x^4+x^3+x^2+x+1$ and that $f$ is irreducible (By applying Eisenstein's criterion on $f(x+1)$). Also the powers of $\zeta$ are also roots of $f$. So $\mathbb Q(\zeta)$ is a splitting field of $f$. Now its clear that this extension is a Galois extension, since all the roots are different. But how to show the desired statement?",,"['abstract-algebra', 'field-theory', 'galois-theory', 'cyclotomic-polynomials']"
41,Equivalent definitions of an algebra over a ring,Equivalent definitions of an algebra over a ring,,"I have seen two different definitions from several sources of an algebra over a ring. From Wikipedia: Let R be a commutative ring.   An R-algebra is an R-Module A together with a binary operation [·,·] [$\cdot$,$\cdot$]: A $\times$ A $\to$ A called A-multiplication, which satisfies the following axiom: Bilinearity: [$\alpha x + \beta y, z] = \alpha [x, z] + \beta [y, z]$, $\quad  [z, \alpha x + \beta y] = \alpha [z, x] + \beta [z, y]$ for all scalars $\alpha, \beta$ in R and all elements x, y, z in A * I have also frequently seen (this one's from Virginia): Let R be a commutative ring. An R-algebra is a ring A which is also an R-module such that the multiplication map A $\times$  A $\to$ A is R-bilinear, that is, r $\ast$ (ab) = (r $\ast$ a)  b = a $\cdot$ (r $\ast$ b) for any $a, b \in A$   $r \in R$ where $\ast$ denotes the R-action on A. I'm trying to prove they are equivalent. I am fine with everything apart from proving that, if definition 1 is satisfied, then multiplication in A is associative. Unless this property is what determines whether the algebra is associative or not?","I have seen two different definitions from several sources of an algebra over a ring. From Wikipedia: Let R be a commutative ring.   An R-algebra is an R-Module A together with a binary operation [·,·] [$\cdot$,$\cdot$]: A $\times$ A $\to$ A called A-multiplication, which satisfies the following axiom: Bilinearity: [$\alpha x + \beta y, z] = \alpha [x, z] + \beta [y, z]$, $\quad  [z, \alpha x + \beta y] = \alpha [z, x] + \beta [z, y]$ for all scalars $\alpha, \beta$ in R and all elements x, y, z in A * I have also frequently seen (this one's from Virginia): Let R be a commutative ring. An R-algebra is a ring A which is also an R-module such that the multiplication map A $\times$  A $\to$ A is R-bilinear, that is, r $\ast$ (ab) = (r $\ast$ a)  b = a $\cdot$ (r $\ast$ b) for any $a, b \in A$   $r \in R$ where $\ast$ denotes the R-action on A. I'm trying to prove they are equivalent. I am fine with everything apart from proving that, if definition 1 is satisfied, then multiplication in A is associative. Unless this property is what determines whether the algebra is associative or not?",,"['abstract-algebra', 'ring-theory', 'modules']"
42,"Aluffi, Exercise 2.12, regarding the cokernel in $\sf{Ring}$ of $\mathbb{Z} \hookrightarrow \mathbb{Q}$","Aluffi, Exercise 2.12, regarding the cokernel in  of",\sf{Ring} \mathbb{Z} \hookrightarrow \mathbb{Q},"I am working in Aluffi's Algebra: Chapter $0$ textbook, and Chapter 3, Exercise 2.12 asks one to determine the cokernel in $\sf{Ring}$ of the imbedding $i \colon \mathbb{Z} \hookrightarrow \mathbb{Q}$.  At the beginning of Chapter 3, Section 5, he hints that the answer is perhaps weird, when he says ""Also, cokernels do not behave as one would hope [in $\sf{Ring}$]"" and then references this exercise. Note that Aluffi assumes that all rings are unital and that ring homomorphisms send $1$ to $1$. Here's my question:  Is the statement ill-defined (or perhaps vacuous -- not exactly sure which description is more accurate)?  It appears to me that the setup of the cokernel in this situation requires that one start with a ring homomorphism $\varphi \colon \mathbb{Q} \to R$, where $R$ is some ring, satisfying $\varphi(i(n)) = 0$ for all $n \in \mathbb{Z}$, but then $\varphi(i(1)) = \varphi(1) = 0$ contradicting the existence of such a ring homomorphism $\varphi$ (assuming $R$ is not trivial). Am I missing something?  If I am right so far, then one quickly realizes that the same problem occurs for every ring homomorphism $\varphi \colon R \to S$.  If this is true, is it better to say that ""cokernels do not exist"" or that ""cokernels are not well-defined"" in $\sf{Ring}$?","I am working in Aluffi's Algebra: Chapter $0$ textbook, and Chapter 3, Exercise 2.12 asks one to determine the cokernel in $\sf{Ring}$ of the imbedding $i \colon \mathbb{Z} \hookrightarrow \mathbb{Q}$.  At the beginning of Chapter 3, Section 5, he hints that the answer is perhaps weird, when he says ""Also, cokernels do not behave as one would hope [in $\sf{Ring}$]"" and then references this exercise. Note that Aluffi assumes that all rings are unital and that ring homomorphisms send $1$ to $1$. Here's my question:  Is the statement ill-defined (or perhaps vacuous -- not exactly sure which description is more accurate)?  It appears to me that the setup of the cokernel in this situation requires that one start with a ring homomorphism $\varphi \colon \mathbb{Q} \to R$, where $R$ is some ring, satisfying $\varphi(i(n)) = 0$ for all $n \in \mathbb{Z}$, but then $\varphi(i(1)) = \varphi(1) = 0$ contradicting the existence of such a ring homomorphism $\varphi$ (assuming $R$ is not trivial). Am I missing something?  If I am right so far, then one quickly realizes that the same problem occurs for every ring homomorphism $\varphi \colon R \to S$.  If this is true, is it better to say that ""cokernels do not exist"" or that ""cokernels are not well-defined"" in $\sf{Ring}$?",,"['abstract-algebra', 'category-theory']"
43,Novel approaches to elementary number theory and abstract algebra,Novel approaches to elementary number theory and abstract algebra,,"As a part of a university course, I'll have to study Herstein's Topics in algebra and Hardy&Wright's Introduction to the theory of numbers . Can you suggest some books (to be used as companions) that offer a unique approach to abstract algebra and number theory (for example, some geometric approaches to otherwise number-theoretical topics and problems, physical intuitions, or some intuitive but rigorous explanations of algebraic topics)?","As a part of a university course, I'll have to study Herstein's Topics in algebra and Hardy&Wright's Introduction to the theory of numbers . Can you suggest some books (to be used as companions) that offer a unique approach to abstract algebra and number theory (for example, some geometric approaches to otherwise number-theoretical topics and problems, physical intuitions, or some intuitive but rigorous explanations of algebraic topics)?",,"['abstract-algebra', 'elementary-number-theory', 'physics', 'intuition', 'book-recommendation']"
44,Subgroups of Symmetric groups isomorphic to dihedral group,Subgroups of Symmetric groups isomorphic to dihedral group,,"Is $D_n$, the dihedral group of order $2n$, isomorphic to a subgroup of $S_n$ ( symmetric group of $n$ letters) for all $n>2$?","Is $D_n$, the dihedral group of order $2n$, isomorphic to a subgroup of $S_n$ ( symmetric group of $n$ letters) for all $n>2$?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
45,Nilpotent elements in $\mathbb{Z}_n$,Nilpotent elements in,\mathbb{Z}_n,"I'm trying to show that $\mathbb{Z}_n$ has a nonzero nilpotent element if and only if $n$ is divisible by the square of some prime. I have figured out the proof of showing that if $n$ is divisible by the square of a prime, then there is a nonzero nilpotent element. I'm having trouble with the other direction. Start: If there exists a nonzero nilpotent element $a$, then $n$ does not divide $a$, but $n$ divides $a^k$ for some positive integer $k$....Hint?","I'm trying to show that $\mathbb{Z}_n$ has a nonzero nilpotent element if and only if $n$ is divisible by the square of some prime. I have figured out the proof of showing that if $n$ is divisible by the square of a prime, then there is a nonzero nilpotent element. I'm having trouble with the other direction. Start: If there exists a nonzero nilpotent element $a$, then $n$ does not divide $a$, but $n$ divides $a^k$ for some positive integer $k$....Hint?",,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
46,Quaternions as an Algebra,Quaternions as an Algebra,,"I'm lacking some vital understanding about quaternions and algebras in general. If we first define $V=\{a+bi+cj+dk|a,b,c,d\in\mathbb{R}\}$. Then we define scalar multiplication, vector multiplication, addition e.t.c. But as I understand it, aren't $1,i,j,k$ meant to be vectors, meaning that $bi$ for example has to already be defined even for the set to be defined. Very confused! As usual thanks for any replies!","I'm lacking some vital understanding about quaternions and algebras in general. If we first define $V=\{a+bi+cj+dk|a,b,c,d\in\mathbb{R}\}$. Then we define scalar multiplication, vector multiplication, addition e.t.c. But as I understand it, aren't $1,i,j,k$ meant to be vectors, meaning that $bi$ for example has to already be defined even for the set to be defined. Very confused! As usual thanks for any replies!",,"['abstract-algebra', 'quaternions']"
47,"Prove that the kernel of a homomorphism is a principal ideal. (Artin, Exercise 9.13)","Prove that the kernel of a homomorphism is a principal ideal. (Artin, Exercise 9.13)",,"I have been having trouble with an exercise in my abstract algebra course. It is as follows: Let $f: \mathbb{C}[x,y] \rightarrow \mathbb{C}[t]$ be a homomorphism that is the identity on $\mathbb{C}$ and sends $x$ to $x(t)$ and $y$ to $y(t)$ such that $x(t)$ and $y(t)$ are not both constant. Prove that the kernel of $f$ is a principal ideal.","I have been having trouble with an exercise in my abstract algebra course. It is as follows: Let $f: \mathbb{C}[x,y] \rightarrow \mathbb{C}[t]$ be a homomorphism that is the identity on $\mathbb{C}$ and sends $x$ to $x(t)$ and $y$ to $y(t)$ such that $x(t)$ and $y(t)$ are not both constant. Prove that the kernel of $f$ is a principal ideal.",,"['abstract-algebra', 'commutative-algebra', 'ring-theory']"
48,What is the center of a semidirect product?,What is the center of a semidirect product?,,Let $G_1$ and $G_2$ be groups. Let $\varphi:G_2\rightarrow \operatorname{Aut}(G_1) $ be a group homomorphism defining the semidirect product $G_1 \rtimes G_2$. Determine the center $\operatorname{Z}(G_1 \rtimes G_2)$.,Let $G_1$ and $G_2$ be groups. Let $\varphi:G_2\rightarrow \operatorname{Aut}(G_1) $ be a group homomorphism defining the semidirect product $G_1 \rtimes G_2$. Determine the center $\operatorname{Z}(G_1 \rtimes G_2)$.,,['abstract-algebra']
49,Distinguishable painted prisms with six colors (repetition allowed),Distinguishable painted prisms with six colors (repetition allowed),,"Fraleigh(7th) Ex17.9: A rectangular prism 2 ft long with 1-ft square ends is to have each of its six faces painted with one of six possible colors. How many distinguishable painted prisms are possible if each color may be used on any number of faces? I solved it but the answer is different from mine. I think my answer is right, but I haven't seen any error in the solution. Here is the solution. We use Burnside's formula: (number of orbits in $X$ under $G$)=$\frac 1 {|G|} \cdot \sum_{g \in G}|X_g|$. In this problem, the group $G$ of the prism has order $8$, four positions leaving the end faces in the same position and four positions with the end faces swapped. The set $X$ of possible ways of painting the prism has $6^6$ elements. We have $|X\text{_id}|=6^6$, $|X\text{_(same ends, rotate 90° or 270°)}|=6^3$, $|X\text{_(same ends, rotate 180°)}|=6^4$, $|X\text{_(swap ends, keeping top face on top)}|=6^4$, $|X\text{_(swap ends, as above, rotate 90° or 270°)}|=6^2$, $|X\text{_(swap ends, as above, rotate 180°)}|=6^3$: But I think this should be $6^4$. If you swap ends(keeping top face on top) and then rotate $180°$, the left and right faces are fixed and top-bottom, front-back is interchanged. So there are $6^4$ fixed elements as same to swapping ends keeping top face on top case. Who is right? And is there a proof by elementary(or combinatoric??) method without using group-theory? I haven't studied combinatorics in college so I don't know how this problem will be solved by combinatorics. Does it also use group-theory and Burnside's formula, etc.?","Fraleigh(7th) Ex17.9: A rectangular prism 2 ft long with 1-ft square ends is to have each of its six faces painted with one of six possible colors. How many distinguishable painted prisms are possible if each color may be used on any number of faces? I solved it but the answer is different from mine. I think my answer is right, but I haven't seen any error in the solution. Here is the solution. We use Burnside's formula: (number of orbits in $X$ under $G$)=$\frac 1 {|G|} \cdot \sum_{g \in G}|X_g|$. In this problem, the group $G$ of the prism has order $8$, four positions leaving the end faces in the same position and four positions with the end faces swapped. The set $X$ of possible ways of painting the prism has $6^6$ elements. We have $|X\text{_id}|=6^6$, $|X\text{_(same ends, rotate 90° or 270°)}|=6^3$, $|X\text{_(same ends, rotate 180°)}|=6^4$, $|X\text{_(swap ends, keeping top face on top)}|=6^4$, $|X\text{_(swap ends, as above, rotate 90° or 270°)}|=6^2$, $|X\text{_(swap ends, as above, rotate 180°)}|=6^3$: But I think this should be $6^4$. If you swap ends(keeping top face on top) and then rotate $180°$, the left and right faces are fixed and top-bottom, front-back is interchanged. So there are $6^4$ fixed elements as same to swapping ends keeping top face on top case. Who is right? And is there a proof by elementary(or combinatoric??) method without using group-theory? I haven't studied combinatorics in college so I don't know how this problem will be solved by combinatorics. Does it also use group-theory and Burnside's formula, etc.?",,"['abstract-algebra', 'combinatorics', 'group-theory']"
50,Field reductions,Field reductions,,"If there is a field $F$ that is a field reduction of the real numbers, that is $F(a)=\mathbb{R}$ for some $a$, let's also denote this $F=\mathbb{R}(\setminus a)$, then given $x \in \mathbb{R}$ is there a general method to determine whether $x$ is in $F$ or $x$ is in $\mathbb{R}\setminus F$ ?","If there is a field $F$ that is a field reduction of the real numbers, that is $F(a)=\mathbb{R}$ for some $a$, let's also denote this $F=\mathbb{R}(\setminus a)$, then given $x \in \mathbb{R}$ is there a general method to determine whether $x$ is in $F$ or $x$ is in $\mathbb{R}\setminus F$ ?",,"['abstract-algebra', 'field-theory']"
51,How to find the fixed field for Galois group?,How to find the fixed field for Galois group?,,"Let be $K$ the finite splitting field of $f(x) (\in \Bbb Q[x])$ over the field, $\Bbb Q$ (rational number set) And say $E_H$ is a fixed field of $H\subset \operatorname{Gal}(K/\mathbb{Q}) $ . Main Question) Find the fixed field $E_H$ (1) $f(x) = x^4 -2$ , $H= \langle \sigma \rangle$ with $\sigma(\alpha) =  -\alpha i $ , $ \sigma(i)  = -i $ , and $\alpha = 2^{1 \over 4}$ (2) $f(x) = x^8 +1$ , $H= \{ \sigma_1,   \sigma_7 ,\sigma_9,  \sigma_{15 }  \}$ with $\sigma_n (\omega) =  \omega  \to \omega^n $ for $\omega = e^{{2\pi i} \over 16} $ and $gcd(n,16)=1$ P.s.) I've solved the (1) by inefficient way that writing the element form like a method in attached image. So I use this method for solving (2) to find the fixed field for $H$ . But the process really complicated, so I can't  find the fixed field.(C.f. the below of this post's image is my attempt) Are there any simple method for finding the fixed field? Thanks.","Let be the finite splitting field of over the field, (rational number set) And say is a fixed field of . Main Question) Find the fixed field (1) , with , , and (2) , with for and P.s.) I've solved the (1) by inefficient way that writing the element form like a method in attached image. So I use this method for solving (2) to find the fixed field for . But the process really complicated, so I can't  find the fixed field.(C.f. the below of this post's image is my attempt) Are there any simple method for finding the fixed field? Thanks.","K f(x) (\in \Bbb Q[x]) \Bbb Q E_H H\subset \operatorname{Gal}(K/\mathbb{Q})  E_H f(x) = x^4 -2 H= \langle \sigma \rangle \sigma(\alpha) =  -\alpha i   \sigma(i)  = -i  \alpha = 2^{1 \over 4} f(x) = x^8 +1 H= \{ \sigma_1,   \sigma_7 ,\sigma_9,  \sigma_{15 }  \} \sigma_n (\omega) =  \omega  \to \omega^n  \omega = e^{{2\pi i} \over 16}  gcd(n,16)=1 H","['abstract-algebra', 'field-theory', 'galois-theory']"
52,An efficient way to check the coefficient of $x_1 x_2...x_n$ in the product of $m$ polynomials,An efficient way to check the coefficient of  in the product of  polynomials,x_1 x_2...x_n m,"We have $n$ variables $x_1, ..., x_n$ and $m$ polynomials $f_1, ..., f_m$. The maximum degree of each variable $x_i$ in each polynomial $f_j$ is $1$. Moreover, each variable is present in at most two of the polynomials and each polynomial contains at most 3 of the variables. We define $F$ as the product of all the polynomials: $F=f_1*f_2*...*f_m$ . We are interested to know whether a particular term ($x_1 x_2...x_n$) appears in $F$ or not. In other words, whether or not the coefficient of $x_1 x_2...x_n$ would be nonzero if we fully expand $F$. Is there an efficient way to check that? Here is an example: $f_1 = (5 x + 3 x y - z)$ $f_2 = (- x y + 2 y )$ $f_3 = (z -3)$ $F = f_1 f_2 f_3 = (5 x + 3 x y - z)(- x y + 2 y)(z -3)$ $F = -30 x y+15 x^2 y-18 x y^2+9 x^2 y^2+6 y z+7 x y z-5 x^2 y z+6 x y^2 z-3 x^2 y^2 z-2 y z^2+x y z^2$ We are interested to know whether or not the coefficient of $xyz$ is zero. In the above example, it is not zero and is equal to $7$. Is there an efficient way to do that (at least for some non-trivial special cases) which is applicable to cases with for example $n=300$ variables and $m=200$ polynomials. Edit :  this is equivalent to check whether $\alpha = \frac{\partial^n }{\partial x_1 \partial x_2 ... \partial x_n}   F \bigg|_{x_1=x_2=...=x_n=0}$ is zero or not. Is there a reasonable numerical approximation method to check the derivative at the origin? Background : Here is some background information about the origins of this problem for the interested readers: Given any boolean function in 3 variables, we could represent it with a binary vector in an 8 dimensional space with entities equal to 1 iff the the function returns TRUE for that combination of inputs. (i.e. standard sum of products form) We could use the following matrix to linearly map the space of the boolean functions to a new space which can be represented with polynomials in 3 variable: The above matrix defines a reversible transformation (from boolean domain to polynomial domain) with several interesting properties, some listed below: i)  Row $X$ only depends on $x$, and is $1$ in columns with $x$ and $-1$ in columns with $x'$ (complement of $x$). ii) Row $XY$ is the element by element product of row $X$ and row $Y$. (and only depends on $X$ and $Y$, or more specifically $x \mathbin{\oplus} y$). iii) The inner product of each pair of rows (or columns) is zero (orthogonal). iv) All the elements are 1 and -1 (for normalized $w$, it is $\frac{1}{\sqrt{2^n}}$ and $\frac{-1}{\sqrt{2^n}}$  where $n$ is # of variables) v)  Matrix is symmetric, so $w’=w$ and for normalized $w$, we have $w^{-1}$=$w$. (The inverse transform and forward transform only differ in a constant factor $\frac{1}{2^n}$). vi) Any boolean function on three (or in general $n$) variables, could be represented by binary vector $u$, where $u_i$ is 1 if and only if $f$ is TRUE for the corresponding combination of inputs. vii) Moreover, if we have two functions $f_1$ and $f_2$, $f_{12}=f_1 \land f_2$ is represented by $u_{12}=u_1 * u_2$ where, $*$ is element by element product. Then $v_{12}=w.u_{12}=w(u_1 *u_2)$. (not equal to $w u_1 * w u_2$, however you can perform regular multiplication in polynomial domain you just need to replace any $x^2$ with $1$ at the end and it will be equivalent to $\land$ in boolean domain. Note that given (iv), square of any row will be equal to row $1$) viii)   Similarly with three functions $f_{123}=f_1 \land f_2 \land f_3$, we have $u_{123}=u_1*u_2*u_3$, $v_{123}=w(u_1*u_2*u_3)$ ix) If we multiply $F$ by $X$ in polynomial domain, in boolean domain all the terms with $x$ will be multiplied by $1$ and all the terms with $x'$ will be multiplied by $-1$. x) $F(X=0)$ in polynomial domain gives the number of solutions ($1$ elements) in boolean domain. Similarly, the coefficient for $X$ in polynomial domain is the number of solutions with $x=TRUE$ minus number of solutions with $x=FALSE$. Similarly the coefficient for $XY$ is the number of solutions with $xy$ weighted by parity (i.e. $xy$ and $x'y'$ solutions are counted with positive sign and $xy'$ and $x'y$ solutions are counted with negative sign). This is also direct consequence of i and ii. Is there any good references for this transformation? Is it known by any names so I can look it up? Any ideas for possible applications? My motivation was that using these orthogonal bases, one could simplify some computations in the transformed domain (e.g. develop faster algorithms for solving SAT or simplify logical circuits using algebraic and non-algebraic methods). To test if a set of boolean equations are simultaneously satisfiable, one could transform them to polynomials, expand them, replace all $x_i^2$ with $1$ and check if the remainder is absolute zero or not. To do it more efficiently, one could set $x_i$ to zero to eliminate some intermediate terms as expanding. For example suppose $x_i$ is only present in $f_1=a x_i +b$ and $f_2 = c x_i +d$, expanding for $x_i$ and eliminating $x_i$ using the above method would yeild: $F = f_1 f_2 f_3 ... f_m = (a x_i +b)(c x_i +d)f_3 ... f_m =(ac x_i^2  +(bc+ad) x_i +bd)f_3 ... f_m \\ \equiv (ac+bd) f_3 ... f_m$ We can then continue with eliminating other variables. After eliminating all the variables, the remainder would give the total number of solutions that would satisfy all the equations simultaneously. Is there an efficient way to get the solution for some subset of cases without the need for exponential calculations ?","We have $n$ variables $x_1, ..., x_n$ and $m$ polynomials $f_1, ..., f_m$. The maximum degree of each variable $x_i$ in each polynomial $f_j$ is $1$. Moreover, each variable is present in at most two of the polynomials and each polynomial contains at most 3 of the variables. We define $F$ as the product of all the polynomials: $F=f_1*f_2*...*f_m$ . We are interested to know whether a particular term ($x_1 x_2...x_n$) appears in $F$ or not. In other words, whether or not the coefficient of $x_1 x_2...x_n$ would be nonzero if we fully expand $F$. Is there an efficient way to check that? Here is an example: $f_1 = (5 x + 3 x y - z)$ $f_2 = (- x y + 2 y )$ $f_3 = (z -3)$ $F = f_1 f_2 f_3 = (5 x + 3 x y - z)(- x y + 2 y)(z -3)$ $F = -30 x y+15 x^2 y-18 x y^2+9 x^2 y^2+6 y z+7 x y z-5 x^2 y z+6 x y^2 z-3 x^2 y^2 z-2 y z^2+x y z^2$ We are interested to know whether or not the coefficient of $xyz$ is zero. In the above example, it is not zero and is equal to $7$. Is there an efficient way to do that (at least for some non-trivial special cases) which is applicable to cases with for example $n=300$ variables and $m=200$ polynomials. Edit :  this is equivalent to check whether $\alpha = \frac{\partial^n }{\partial x_1 \partial x_2 ... \partial x_n}   F \bigg|_{x_1=x_2=...=x_n=0}$ is zero or not. Is there a reasonable numerical approximation method to check the derivative at the origin? Background : Here is some background information about the origins of this problem for the interested readers: Given any boolean function in 3 variables, we could represent it with a binary vector in an 8 dimensional space with entities equal to 1 iff the the function returns TRUE for that combination of inputs. (i.e. standard sum of products form) We could use the following matrix to linearly map the space of the boolean functions to a new space which can be represented with polynomials in 3 variable: The above matrix defines a reversible transformation (from boolean domain to polynomial domain) with several interesting properties, some listed below: i)  Row $X$ only depends on $x$, and is $1$ in columns with $x$ and $-1$ in columns with $x'$ (complement of $x$). ii) Row $XY$ is the element by element product of row $X$ and row $Y$. (and only depends on $X$ and $Y$, or more specifically $x \mathbin{\oplus} y$). iii) The inner product of each pair of rows (or columns) is zero (orthogonal). iv) All the elements are 1 and -1 (for normalized $w$, it is $\frac{1}{\sqrt{2^n}}$ and $\frac{-1}{\sqrt{2^n}}$  where $n$ is # of variables) v)  Matrix is symmetric, so $w’=w$ and for normalized $w$, we have $w^{-1}$=$w$. (The inverse transform and forward transform only differ in a constant factor $\frac{1}{2^n}$). vi) Any boolean function on three (or in general $n$) variables, could be represented by binary vector $u$, where $u_i$ is 1 if and only if $f$ is TRUE for the corresponding combination of inputs. vii) Moreover, if we have two functions $f_1$ and $f_2$, $f_{12}=f_1 \land f_2$ is represented by $u_{12}=u_1 * u_2$ where, $*$ is element by element product. Then $v_{12}=w.u_{12}=w(u_1 *u_2)$. (not equal to $w u_1 * w u_2$, however you can perform regular multiplication in polynomial domain you just need to replace any $x^2$ with $1$ at the end and it will be equivalent to $\land$ in boolean domain. Note that given (iv), square of any row will be equal to row $1$) viii)   Similarly with three functions $f_{123}=f_1 \land f_2 \land f_3$, we have $u_{123}=u_1*u_2*u_3$, $v_{123}=w(u_1*u_2*u_3)$ ix) If we multiply $F$ by $X$ in polynomial domain, in boolean domain all the terms with $x$ will be multiplied by $1$ and all the terms with $x'$ will be multiplied by $-1$. x) $F(X=0)$ in polynomial domain gives the number of solutions ($1$ elements) in boolean domain. Similarly, the coefficient for $X$ in polynomial domain is the number of solutions with $x=TRUE$ minus number of solutions with $x=FALSE$. Similarly the coefficient for $XY$ is the number of solutions with $xy$ weighted by parity (i.e. $xy$ and $x'y'$ solutions are counted with positive sign and $xy'$ and $x'y$ solutions are counted with negative sign). This is also direct consequence of i and ii. Is there any good references for this transformation? Is it known by any names so I can look it up? Any ideas for possible applications? My motivation was that using these orthogonal bases, one could simplify some computations in the transformed domain (e.g. develop faster algorithms for solving SAT or simplify logical circuits using algebraic and non-algebraic methods). To test if a set of boolean equations are simultaneously satisfiable, one could transform them to polynomials, expand them, replace all $x_i^2$ with $1$ and check if the remainder is absolute zero or not. To do it more efficiently, one could set $x_i$ to zero to eliminate some intermediate terms as expanding. For example suppose $x_i$ is only present in $f_1=a x_i +b$ and $f_2 = c x_i +d$, expanding for $x_i$ and eliminating $x_i$ using the above method would yeild: $F = f_1 f_2 f_3 ... f_m = (a x_i +b)(c x_i +d)f_3 ... f_m =(ac x_i^2  +(bc+ad) x_i +bd)f_3 ... f_m \\ \equiv (ac+bd) f_3 ... f_m$ We can then continue with eliminating other variables. After eliminating all the variables, the remainder would give the total number of solutions that would satisfy all the equations simultaneously. Is there an efficient way to get the solution for some subset of cases without the need for exponential calculations ?",,"['abstract-algebra', 'combinatorics', 'algebraic-geometry', 'polynomials', 'numerical-methods']"
53,$F(u) = F(u^2)$ if $u$ is algebraic of odd degree,if  is algebraic of odd degree,F(u) = F(u^2) u,"This has been asked before, but I have a different solution and I would like to check it. Let $F$ be a field and $u$ be algebraic over $F$ of odd degree. Prove that $F(u) = F(u^2)$. Of course $F(u^2) \subset F(u)$. If we can show that $u \in F(u^2)$, we are through. Let $$f(x) = x^{2n+1} + a_{2n}x^{2n} + \cdots + a_1 x + a_0$$ be the minimum polynomial of $u$ over $F$. Then: $$u g(u) + h(u) = 0$$ where $$g(x) = x^{2n} + a_{2n-1}bx^{2n-2} + a_{2n-3} x^{n-2} + \cdots + a_3 x^2 + a_1$$ and $$h(x) = a_{2n}x^{2n} + a_{2n-2} x^{2n-2} + \cdots + a_2 x^2 + a_0$$ We can't have $g(u) = 0$ because the degree of $u$ is $2n+1$. So $g(u)$ is a unit in $F(u^2)$. Hence $u = - h(u)g(u)^{-1} \in F(u^2)$. Does this look good?","This has been asked before, but I have a different solution and I would like to check it. Let $F$ be a field and $u$ be algebraic over $F$ of odd degree. Prove that $F(u) = F(u^2)$. Of course $F(u^2) \subset F(u)$. If we can show that $u \in F(u^2)$, we are through. Let $$f(x) = x^{2n+1} + a_{2n}x^{2n} + \cdots + a_1 x + a_0$$ be the minimum polynomial of $u$ over $F$. Then: $$u g(u) + h(u) = 0$$ where $$g(x) = x^{2n} + a_{2n-1}bx^{2n-2} + a_{2n-3} x^{n-2} + \cdots + a_3 x^2 + a_1$$ and $$h(x) = a_{2n}x^{2n} + a_{2n-2} x^{2n-2} + \cdots + a_2 x^2 + a_0$$ We can't have $g(u) = 0$ because the degree of $u$ is $2n+1$. So $g(u)$ is a unit in $F(u^2)$. Hence $u = - h(u)g(u)^{-1} \in F(u^2)$. Does this look good?",,"['abstract-algebra', 'polynomials', 'proof-verification', 'field-theory']"
54,Geometric reason as to why $H^2$ of the Klein bottle is $\mathbb{Z}/2\mathbb{Z}$?,Geometric reason as to why  of the Klein bottle is ?,H^2 \mathbb{Z}/2\mathbb{Z},I was reading this document when I came across the following: Recall that $H^2(K; \mathbb{Z}) \cong \mathbb{Z}/2\mathbb{Z}$. Here $K$ denotes the Klein bottle. Is there a good geometric intuition/reason as to why this is true?,I was reading this document when I came across the following: Recall that $H^2(K; \mathbb{Z}) \cong \mathbb{Z}/2\mathbb{Z}$. Here $K$ denotes the Klein bottle. Is there a good geometric intuition/reason as to why this is true?,,['abstract-algebra']
55,Finite Abelian groups with the same number of elements for all orders are isomorphic,Finite Abelian groups with the same number of elements for all orders are isomorphic,,"Let $A$ and $B$ be finite abelian groups. Suppose that for every natural number $m$ , the number of elements of order $m$ in $A$ is equal to the number of elements of order $m$ in $B$ . Prove that $A$ and $B$ are isomorphic. Idea Given that these groups are finite, I think you have to use the primary decomposition theorem somehow.","Let and be finite abelian groups. Suppose that for every natural number , the number of elements of order in is equal to the number of elements of order in . Prove that and are isomorphic. Idea Given that these groups are finite, I think you have to use the primary decomposition theorem somehow.",A B m m A m B A B,"['abstract-algebra', 'group-theory', 'abelian-groups']"
56,Reverse Polish notation in (abstract) algebra,Reverse Polish notation in (abstract) algebra,,"If I have something like $\phi\circ \psi(x)$ this means first apply $\psi$ and then $\phi$. Going right to left is pretty contrary to my intuition. In computer science some programming languages (and many compilers) use reverse Polish notation, where this would be written $x\psi\phi$. I occasionally see algebraic texts using this type of notation (Glass' Partially Ordered Groups is one example); I suspect because this notation fits better with the idea of groups. Is there a name for this notation in algebra or is it just called Reverse Polish Notation there too? And why do we have the bizarre right to left function notation? Wikipedia cites Euler on several pages , but doesn't explain how he came up with it or why people stuck with it. Even if he had just decided to write it like $(x)f=x^2$ that would've been better. EDIT: to explain why this is confusing, suppose I was explaining an algorithm to someone like ""first double a number, then square it, then take the sine of that."" If I wrote this in the usual math notation I would write it in the opposite order of my instructions: $$(x\mapsto \sin x)\circ (x\mapsto x^2)\circ (x\mapsto 2x)(x)$$ instead, it would be easier if I did it in the same order as my instructions: $$(x)(x\mapsto 2x)\circ (x\mapsto x^2)\circ (x\mapsto \sin x)$$","If I have something like $\phi\circ \psi(x)$ this means first apply $\psi$ and then $\phi$. Going right to left is pretty contrary to my intuition. In computer science some programming languages (and many compilers) use reverse Polish notation, where this would be written $x\psi\phi$. I occasionally see algebraic texts using this type of notation (Glass' Partially Ordered Groups is one example); I suspect because this notation fits better with the idea of groups. Is there a name for this notation in algebra or is it just called Reverse Polish Notation there too? And why do we have the bizarre right to left function notation? Wikipedia cites Euler on several pages , but doesn't explain how he came up with it or why people stuck with it. Even if he had just decided to write it like $(x)f=x^2$ that would've been better. EDIT: to explain why this is confusing, suppose I was explaining an algorithm to someone like ""first double a number, then square it, then take the sine of that."" If I wrote this in the usual math notation I would write it in the opposite order of my instructions: $$(x\mapsto \sin x)\circ (x\mapsto x^2)\circ (x\mapsto 2x)(x)$$ instead, it would be easier if I did it in the same order as my instructions: $$(x)(x\mapsto 2x)\circ (x\mapsto x^2)\circ (x\mapsto \sin x)$$",,"['abstract-algebra', 'soft-question', 'notation', 'polish-notation']"
57,Cardinality of $\text{Aut}(G\times G) $,Cardinality of,\text{Aut}(G\times G) ,"Let $G$ be a finite group. If $|\text{Aut}(G)|$ is known, what can we say about $|\text{Aut}(G\times G)|$ ?","Let $G$ be a finite group. If $|\text{Aut}(G)|$ is known, what can we say about $|\text{Aut}(G\times G)|$ ?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
58,Homogeneous groups,Homogeneous groups,,"Let's call a group $G$ homogeneous if for every two distinct, non-identity elements $a$ and $b$ there is an automorphism $\phi$ of $G$ such that $\phi(a)=b$. Examining this definition, we can see that the underlying additive group of any field (indeed, any division ring) is homogeneous by examining the automorphism $\phi(x)=ba^{-1}x$. Thus for any $n$ and any prime $p$, we have that $Z_p^n$ is homogeneous. Conversely, if we also know that $G$ is finite, we can show that $G$ is of the form $Z_p^n$ for some $n$ and $p$. We can clearly see that all elements of a homogeneous group have the same order. And by Cauchy's theorem, we get that $G$ must have prime-power order. Say $|G|=p^n$. The Conjugacy Class Equation then implies that $G$ has non-trivial center. And since $G$ is homogeneous, all elements are in the center and thus $G$ is abelian. Since $G$ is finite, abelian, and all elements have the same order, we can conclude that $G$ is isomorphic to $Z_p^n$. If it weren't the fundamental theorem of abelian groups would imply there were an element of order $p^i$ for $i>1$ contradicting the fact that all elements have the same order (Cauchy's theorem still implies there's an element of order $p$). Thus finite, homogeneous groups are kind of boring. Infinite examples are more interesting as we can garner a couple examples just from fields, and they are more difficult to investigate. We can still surmise a couple things when $G$ is infinite. For example, if $G$ has a single non-identity element of finite order then all (non-identity) elements of $G$ have prime order. Let $x\neq e$ have finite order $n$. Let $n=p^em$ for some prime $p$ with $e\geq 1$ and $p\nmid m$. Then the element $x^{p^{e-1}m}$ has order $p$, and thus all elements have order $p$. Also, in general, If $G$ has a non-trivial center then it is abelian. Any quotient by a characteristic subgroup is also homogeneous. I am having difficulty figuring out other, even basic, things though. My question is kind of general: What else can we determine about homogeneous groups? I am specifically interested in these questions: Does $G$ have to be abelian? If not, can $G$ be perfect? Can $G$ be free? It clearly can't have rank 1, but what about other ranks? If all elements have prime order, is $G$ a direct sum of multiple copies of $Z_p$? But any addition is welcome and appreciated. -UPDATE Qiaochu Yuan has deftly answered three of my original questions, but one still remains though it may be difficult. However his answer has left me intrigued for a proof of the following claim: Every abelian, homogenous group is the underlying additive group of some field. I don't see how you could construct the multiplication operation, but it seems the choice for which element would become unity is a free choice.","Let's call a group $G$ homogeneous if for every two distinct, non-identity elements $a$ and $b$ there is an automorphism $\phi$ of $G$ such that $\phi(a)=b$. Examining this definition, we can see that the underlying additive group of any field (indeed, any division ring) is homogeneous by examining the automorphism $\phi(x)=ba^{-1}x$. Thus for any $n$ and any prime $p$, we have that $Z_p^n$ is homogeneous. Conversely, if we also know that $G$ is finite, we can show that $G$ is of the form $Z_p^n$ for some $n$ and $p$. We can clearly see that all elements of a homogeneous group have the same order. And by Cauchy's theorem, we get that $G$ must have prime-power order. Say $|G|=p^n$. The Conjugacy Class Equation then implies that $G$ has non-trivial center. And since $G$ is homogeneous, all elements are in the center and thus $G$ is abelian. Since $G$ is finite, abelian, and all elements have the same order, we can conclude that $G$ is isomorphic to $Z_p^n$. If it weren't the fundamental theorem of abelian groups would imply there were an element of order $p^i$ for $i>1$ contradicting the fact that all elements have the same order (Cauchy's theorem still implies there's an element of order $p$). Thus finite, homogeneous groups are kind of boring. Infinite examples are more interesting as we can garner a couple examples just from fields, and they are more difficult to investigate. We can still surmise a couple things when $G$ is infinite. For example, if $G$ has a single non-identity element of finite order then all (non-identity) elements of $G$ have prime order. Let $x\neq e$ have finite order $n$. Let $n=p^em$ for some prime $p$ with $e\geq 1$ and $p\nmid m$. Then the element $x^{p^{e-1}m}$ has order $p$, and thus all elements have order $p$. Also, in general, If $G$ has a non-trivial center then it is abelian. Any quotient by a characteristic subgroup is also homogeneous. I am having difficulty figuring out other, even basic, things though. My question is kind of general: What else can we determine about homogeneous groups? I am specifically interested in these questions: Does $G$ have to be abelian? If not, can $G$ be perfect? Can $G$ be free? It clearly can't have rank 1, but what about other ranks? If all elements have prime order, is $G$ a direct sum of multiple copies of $Z_p$? But any addition is welcome and appreciated. -UPDATE Qiaochu Yuan has deftly answered three of my original questions, but one still remains though it may be difficult. However his answer has left me intrigued for a proof of the following claim: Every abelian, homogenous group is the underlying additive group of some field. I don't see how you could construct the multiplication operation, but it seems the choice for which element would become unity is a free choice.",,['abstract-algebra']
59,Eisenstein Criterion Shift Conditions,Eisenstein Criterion Shift Conditions,,"I have been working through my Abstract Algebra book and have come across Eisenstein's criterion.  Many of the problems have been proving polynomials are irreducible using a shift in Eisenstein's criterion.  That is, give some $f(x)$, let $f(x+b)=g(x)$ for which we can use Eisenstein's criterion on $g(x)$ to prove that $f(x)$ is irreducible. I understand the conditions and the proof. However, I was wondering if any irreducible polynomial $f(x)$ can be shifted to $g(x)=f(ax+b)$ then use Eisenstein's criterion on $g(x)$? I was researching some of this and came across the following: Let $f(x)$ be monic with integer coefficients and let $g(x)=f(ax+b)$ for integers $a$ and $b$. Suppose there exists a prime $p$ for which Eisenstein applies to $g(x)$. Then $f(x)$ is irreducible. Now show that such an $a$ and $b$ exists if and only if $g(x)=c(x-b)^n\pmod p$ for some $c$. The hint was to use Taylor expansions. I have been working on this problem for a while and just cannot seem to figure out how to go about proving the if and only if statement. Please provide hints. Not answers. Thanks!","I have been working through my Abstract Algebra book and have come across Eisenstein's criterion.  Many of the problems have been proving polynomials are irreducible using a shift in Eisenstein's criterion.  That is, give some $f(x)$, let $f(x+b)=g(x)$ for which we can use Eisenstein's criterion on $g(x)$ to prove that $f(x)$ is irreducible. I understand the conditions and the proof. However, I was wondering if any irreducible polynomial $f(x)$ can be shifted to $g(x)=f(ax+b)$ then use Eisenstein's criterion on $g(x)$? I was researching some of this and came across the following: Let $f(x)$ be monic with integer coefficients and let $g(x)=f(ax+b)$ for integers $a$ and $b$. Suppose there exists a prime $p$ for which Eisenstein applies to $g(x)$. Then $f(x)$ is irreducible. Now show that such an $a$ and $b$ exists if and only if $g(x)=c(x-b)^n\pmod p$ for some $c$. The hint was to use Taylor expansions. I have been working on this problem for a while and just cannot seem to figure out how to go about proving the if and only if statement. Please provide hints. Not answers. Thanks!",,"['abstract-algebra', 'irreducible-polynomials']"
60,Nonsingular projective variety of degree $d$,Nonsingular projective variety of degree,d,For each $d>0$ and $p=0$ or $p$ prime find a nonsingular curve in $\mathbb{P}^{2}$ of degree $d$. I'm very close just stuck on one small case. If $p\nmid d$ then $x^{d}+y^{d}+z^{d}$ works. If $p\mid d$ then I have chosen the curve $zx^{d-1}+xy^{d-1}+yz^{d-1}$. After some work using the Jacobian criterion for nonsingularity I arrive at $3z=0$. As long as $p\neq 3$ this curve is nonsingular. But I haven't been able to deal with the $p=3$ case. Any ideas? Thanks in advance.,For each $d>0$ and $p=0$ or $p$ prime find a nonsingular curve in $\mathbb{P}^{2}$ of degree $d$. I'm very close just stuck on one small case. If $p\nmid d$ then $x^{d}+y^{d}+z^{d}$ works. If $p\mid d$ then I have chosen the curve $zx^{d-1}+xy^{d-1}+yz^{d-1}$. After some work using the Jacobian criterion for nonsingularity I arrive at $3z=0$. As long as $p\neq 3$ this curve is nonsingular. But I haven't been able to deal with the $p=3$ case. Any ideas? Thanks in advance.,,"['abstract-algebra', 'algebraic-geometry', 'algebraic-curves']"
61,Describe all ring homomorphisms from $\mathbb Z\times\mathbb Z$ to $\mathbb Z\times\mathbb Z$,Describe all ring homomorphisms from  to,\mathbb Z\times\mathbb Z \mathbb Z\times\mathbb Z,"Note: In this class, a ring homomorphism must map multiplicative and additive identities to multiplicative and additive identities. This is different from our textbook's requirement, and often means there are fewer situations to consider. I always have a pretty hard time answering these types of questions: Let $\phi: \mathbb{Z~ \times ~Z} \rightarrow \mathbb{Z~ \times ~Z}$  be a ring homomorphism. We know, then, by definition of a ring homomorphism, that $\phi(1,1) = (1,1)$ (because $(1,1)$ is the multiplicative identity of $\mathbb{Z~ \times ~Z}$). Any ring homomorphism must then have the form $\phi(a,b) = (a,b)$ or $\phi(a,b) = (b,a)$. Any addition/multiplication to elements would cease to send $(1,1)$ to $(1,1)$. Is... this correct? It seems too simple, but I'm pretty sure it covers the possibilities.","Note: In this class, a ring homomorphism must map multiplicative and additive identities to multiplicative and additive identities. This is different from our textbook's requirement, and often means there are fewer situations to consider. I always have a pretty hard time answering these types of questions: Let $\phi: \mathbb{Z~ \times ~Z} \rightarrow \mathbb{Z~ \times ~Z}$  be a ring homomorphism. We know, then, by definition of a ring homomorphism, that $\phi(1,1) = (1,1)$ (because $(1,1)$ is the multiplicative identity of $\mathbb{Z~ \times ~Z}$). Any ring homomorphism must then have the form $\phi(a,b) = (a,b)$ or $\phi(a,b) = (b,a)$. Any addition/multiplication to elements would cease to send $(1,1)$ to $(1,1)$. Is... this correct? It seems too simple, but I'm pretty sure it covers the possibilities.",,"['abstract-algebra', 'ring-theory']"
62,What is the intersection of all Sylow $p$-subgroup's normalizer?,What is the intersection of all Sylow -subgroup's normalizer?,p,"Intersection of all Sylow $p$-subgroups is generally denoted by $O_p(G)$ and it is one of the well studied topics in group theory as there are many theorems related to this. Let $R$ be intersection of all Sylow $p$-subgroup's normalizer in $G$. It is easy to observe that $R$ is a characteristic subgroup of $G$ containing $O_p(G)$. I wonder the properties of $R$ and its relation with $O_p(G)$. If anyone can find or observe something about $R$, I would be thankful.","Intersection of all Sylow $p$-subgroups is generally denoted by $O_p(G)$ and it is one of the well studied topics in group theory as there are many theorems related to this. Let $R$ be intersection of all Sylow $p$-subgroup's normalizer in $G$. It is easy to observe that $R$ is a characteristic subgroup of $G$ containing $O_p(G)$. I wonder the properties of $R$ and its relation with $O_p(G)$. If anyone can find or observe something about $R$, I would be thankful.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory']"
63,A group of order $30$ has a normal $5$-Sylow subgroup.,A group of order  has a normal -Sylow subgroup.,30 5,"There are several things that confuse me about this proof, so I was wondering if anybody could clarify them for me. Lemma Let $G$ be a group of order $30$ . Then the $5$ -Sylow subgroup of $G$ is normal. Proof We argue by contradiction. Let $P_5$ be a $5$ -Sylow subgroup of $G$ . Then the number of conjugates of $P_5$ is congruent to $1 \bmod 5$ and divides $6$ . Thus, there must be six conjugates of $P_5$ . Since the number of conjugates is the index of the normalizer, we see that $N_G(P_5) = P_5$ . Why does the fact that the order of $N_G(P_5)$ is 5 mean that it is equal to $P_5$ ? Since the $5$ -Sylow subgroups of $G$ have order $5$ , any two of them intersect in the identity element only. Thus, there are $6\cdot4 = 24$ elements in $G$ of order $5$ . This leaves $6$ elements whose order does not equal $5$ . We claim now that the $3$ -Sylow subgroup, $P_3$ , must be normal in $G$ . The number of conjugates of $P_3$ is congruent to $1 \bmod 3$ and divides $10$ . Thus, if $P_3$ is not normal, it must have $10$ conjugates. But this would give $20$ elements of order $3$ when there cannot be more than $6$ elements of order unequal to $5$ so that $P_3$ must indeed be normal. But then $P_5$ normalizes $P_3$ , and hence $P_5P_3$ is a subgroup of $G$ . Moreover, the Second Noether Theorem gives $(P_5P_3)/P_3 \cong P_5/(P_5 \cap P_3)$ . But since $|P_5|$ and $|P_3|$ are relatively prime, $P_5 \cap P_3 = 1$ , and hence $P_5P_3$ must have order $15$ . Why do we need to use the second Noether theorem? Why can't we just use the formula $\frac{|P_5||P_3|}{|P_3 \cap P_5|}$ to compute the order? Thus, $P_5P_3 \cong Z_{15}$ , by Corollary $5.3.17$ . But then $P_5P_3$ normalizes $P_5$ , which contradicts the earlier statement that $N_G(P_5)$ has order $5$ . Why do we have to realize that $P_5P_3$ is isomorphic to $Z_{15}$ ? Also, how can we conclude that $P_5P_3$ normalizes $P_5$ ? Thanks in advance.","There are several things that confuse me about this proof, so I was wondering if anybody could clarify them for me. Lemma Let be a group of order . Then the -Sylow subgroup of is normal. Proof We argue by contradiction. Let be a -Sylow subgroup of . Then the number of conjugates of is congruent to and divides . Thus, there must be six conjugates of . Since the number of conjugates is the index of the normalizer, we see that . Why does the fact that the order of is 5 mean that it is equal to ? Since the -Sylow subgroups of have order , any two of them intersect in the identity element only. Thus, there are elements in of order . This leaves elements whose order does not equal . We claim now that the -Sylow subgroup, , must be normal in . The number of conjugates of is congruent to and divides . Thus, if is not normal, it must have conjugates. But this would give elements of order when there cannot be more than elements of order unequal to so that must indeed be normal. But then normalizes , and hence is a subgroup of . Moreover, the Second Noether Theorem gives . But since and are relatively prime, , and hence must have order . Why do we need to use the second Noether theorem? Why can't we just use the formula to compute the order? Thus, , by Corollary . But then normalizes , which contradicts the earlier statement that has order . Why do we have to realize that is isomorphic to ? Also, how can we conclude that normalizes ? Thanks in advance.",G 30 5 G P_5 5 G P_5 1 \bmod 5 6 P_5 N_G(P_5) = P_5 N_G(P_5) P_5 5 G 5 6\cdot4 = 24 G 5 6 5 3 P_3 G P_3 1 \bmod 3 10 P_3 10 20 3 6 5 P_3 P_5 P_3 P_5P_3 G (P_5P_3)/P_3 \cong P_5/(P_5 \cap P_3) |P_5| |P_3| P_5 \cap P_3 = 1 P_5P_3 15 \frac{|P_5||P_3|}{|P_3 \cap P_5|} P_5P_3 \cong Z_{15} 5.3.17 P_5P_3 P_5 N_G(P_5) 5 P_5P_3 Z_{15} P_5P_3 P_5,"['abstract-algebra', 'group-theory']"
64,Maximal ideals in $R[x]$,Maximal ideals in,R[x],"I want to prove the following result: Let $R$ be a ring and $M$ a maximal ideal in $R$. If $P$ is a prime ideal in $R[x]$ that (strictly) contains $M[x]$, then $P$ is a maximal ideal in $R[x]$. I have an idea how to prove that, but I'm not quite sure if the argument is totally valid; maybe someone has a cleaner proof. My argument is like that: $R[x]/M[x]$ is isomorphic to $(R/M)[x]$, that is a PID (since $R/M$ is a field). So, any prime ideal in this ring is maximal; If $P$ contains $M[x]$, then $P$ corresponds to a (prime??) ideal in $R[x]/M[x]$ (this needs more clarification); The prime ideal (that corresponds to $P$) in $R[x]/M[x]$ is then maximal, and this guarantees that $P$ is maximal in $R[x]$. I'll appreciate any comment.","I want to prove the following result: Let $R$ be a ring and $M$ a maximal ideal in $R$. If $P$ is a prime ideal in $R[x]$ that (strictly) contains $M[x]$, then $P$ is a maximal ideal in $R[x]$. I have an idea how to prove that, but I'm not quite sure if the argument is totally valid; maybe someone has a cleaner proof. My argument is like that: $R[x]/M[x]$ is isomorphic to $(R/M)[x]$, that is a PID (since $R/M$ is a field). So, any prime ideal in this ring is maximal; If $P$ contains $M[x]$, then $P$ corresponds to a (prime??) ideal in $R[x]/M[x]$ (this needs more clarification); The prime ideal (that corresponds to $P$) in $R[x]/M[x]$ is then maximal, and this guarantees that $P$ is maximal in $R[x]$. I'll appreciate any comment.",,"['abstract-algebra', 'ideals']"
65,Every Ring is Isomorphic to a Subring of an Endomorphism Ring of an Abelian Group,Every Ring is Isomorphic to a Subring of an Endomorphism Ring of an Abelian Group,,"Show that for every ring $(R,+,\cdot)$, there is an abelian group, $(A,+)$, such that $R$ is isomorphic to a subring of $(\operatorname{End}(A),+,\circ)$. $(\operatorname{End}(A),+,\circ)$ is the set of homomorphisms of $A$ that form a ring under function addition and composition. I am thinking to let $\operatorname{End}(A)$ be the group such that $A$ is the abelian group $(R,+)$ and create a ring homomorphism from $(R,+,\cdot)$ into $\operatorname{End}((R,+))$. Thoughts?","Show that for every ring $(R,+,\cdot)$, there is an abelian group, $(A,+)$, such that $R$ is isomorphic to a subring of $(\operatorname{End}(A),+,\circ)$. $(\operatorname{End}(A),+,\circ)$ is the set of homomorphisms of $A$ that form a ring under function addition and composition. I am thinking to let $\operatorname{End}(A)$ be the group such that $A$ is the abelian group $(R,+)$ and create a ring homomorphism from $(R,+,\cdot)$ into $\operatorname{End}((R,+))$. Thoughts?",,"['abstract-algebra', 'group-theory', 'ring-theory']"
66,Annihilator of quotient module M/IM,Annihilator of quotient module M/IM,,"Let $A$ be a commutative ring, $I$ an ideal of $A$ and $M$ an module over $A$. Is it true that $\operatorname{Ann}(M/IM) = \operatorname{Ann}(M) + I$? One inclusion is certainly true, but I don't know about the other one. If the above statement does not hold in general, does it perhaps for finitely generated modules?","Let $A$ be a commutative ring, $I$ an ideal of $A$ and $M$ an module over $A$. Is it true that $\operatorname{Ann}(M/IM) = \operatorname{Ann}(M) + I$? One inclusion is certainly true, but I don't know about the other one. If the above statement does not hold in general, does it perhaps for finitely generated modules?",,"['abstract-algebra', 'commutative-algebra']"
67,Invertibility of elements in a left Noetherian ring,Invertibility of elements in a left Noetherian ring,,Let $A$ be a left Noetherian ring.  How do I show that every element $a\in A$ which is left invertible is actually two-sided invertible?,Let $A$ be a left Noetherian ring.  How do I show that every element $a\in A$ which is left invertible is actually two-sided invertible?,,"['abstract-algebra', 'ring-theory', 'noncommutative-algebra']"
68,Bijection = bijection + bijection on symmetric integer intervals,Bijection = bijection + bijection on symmetric integer intervals,,"Given a bijection $f:\mathbb Z \to \mathbb Z$ where $\mathbb Z$ is the set of all integers, does there always exist two bijections $g:\mathbb Z \to \mathbb Z$ and $h:\mathbb Z \to \mathbb Z$ which satisfy $f(n) = g(n) + h(n), ~ \forall n \in \mathbb Z$ ? Evidently it does while $\mathbb Z$ is replaced with the set of all rational numbers $\mathbb Q$, since $f(q) = 2f(q) - f(q), ~ \forall q \in \mathbb Q$. Nevertheless, $2f$ doesn't maps $\mathbb Z$ onto $\mathbb Z$, thus it's not a bijection on $\mathbb Z$; therefor we need a new construction for the $\mathbb Z$ case. Firstly we note that   $0 = 0 + 0$   and \begin{align*} 1 &= (-2) + 3, &\qquad -1 &= 2 + (-3),\\ 2 &= 3 + (- 1), &-2 &= (-3) + 1,\\ 3 &= 1 + 2, &-3 &= (-1) + (-2). \end{align*} It's easy to show that any bijections on the symmetric integer interval $[-3,3]$ can be represented as a sum of two bijections; or equivalently, all integers $a \in [-3,3]$ can be represented as a sum of two others, i.e., $a = b + c$ where $b, c \in [-3, 3]$, while $a,b,c$ exhausted the integer interval $[-3,3]$ respectively, and keeps $0 = 0 + 0$. For convenience, let's denote this fact as $[-3,3] = [-3,3] + [-3,3]$. Suppose now we have $[-m, m] = [-m, m] + [-m, m]$, then we may obtain $$[-m + aL, ~ m + aL] = [-m + bL, ~ m + bL] + [-m + cL, ~ m + cL],\qquad L = 2m+1,$$ where $a,b,c \in [-n,n]$ for some positive integers $n$, provided $a = b + c$. Well, if $a,b,c$ form a representation, say, $[-n,n] = [-n,n] + [-n,n]$, then we may get a ""larger"" representation $$[-m - nL, ~ m + nL] = [-m - nL, ~ m + nL] + [-m - nL, ~ m + nL], \qquad L = 2m+1,$$ through the repeatedly use of ""small"" representations; and obviously, the larger representation ""contains"" the small representation as a ""subrepresentation"", since $0 = 0 + 0$. From now on, we may construct a representation $\mathbb Z = \mathbb Z + \mathbb Z$ by induction, start from $[-3,3] = [-3,3] + [-3,3]$. Thus we've proved that any bijections on $\mathbb Z$ can be represented as a sum of two bijections on $\mathbb Z$. Now a problem rises: for any integers $n > 2$, is there always a representation $[-n, n] = [-n, n] + [-n, n]$ ? Note that by our means of ""representation"", $0 = 0 + 0$ is required; otherwise it's trivial; note also the above approach we've used for resolving the ""$\mathbb{Z}$-representation problem"" can't reach all cases. Anyone can answer this question? Please help me.","Given a bijection $f:\mathbb Z \to \mathbb Z$ where $\mathbb Z$ is the set of all integers, does there always exist two bijections $g:\mathbb Z \to \mathbb Z$ and $h:\mathbb Z \to \mathbb Z$ which satisfy $f(n) = g(n) + h(n), ~ \forall n \in \mathbb Z$ ? Evidently it does while $\mathbb Z$ is replaced with the set of all rational numbers $\mathbb Q$, since $f(q) = 2f(q) - f(q), ~ \forall q \in \mathbb Q$. Nevertheless, $2f$ doesn't maps $\mathbb Z$ onto $\mathbb Z$, thus it's not a bijection on $\mathbb Z$; therefor we need a new construction for the $\mathbb Z$ case. Firstly we note that   $0 = 0 + 0$   and \begin{align*} 1 &= (-2) + 3, &\qquad -1 &= 2 + (-3),\\ 2 &= 3 + (- 1), &-2 &= (-3) + 1,\\ 3 &= 1 + 2, &-3 &= (-1) + (-2). \end{align*} It's easy to show that any bijections on the symmetric integer interval $[-3,3]$ can be represented as a sum of two bijections; or equivalently, all integers $a \in [-3,3]$ can be represented as a sum of two others, i.e., $a = b + c$ where $b, c \in [-3, 3]$, while $a,b,c$ exhausted the integer interval $[-3,3]$ respectively, and keeps $0 = 0 + 0$. For convenience, let's denote this fact as $[-3,3] = [-3,3] + [-3,3]$. Suppose now we have $[-m, m] = [-m, m] + [-m, m]$, then we may obtain $$[-m + aL, ~ m + aL] = [-m + bL, ~ m + bL] + [-m + cL, ~ m + cL],\qquad L = 2m+1,$$ where $a,b,c \in [-n,n]$ for some positive integers $n$, provided $a = b + c$. Well, if $a,b,c$ form a representation, say, $[-n,n] = [-n,n] + [-n,n]$, then we may get a ""larger"" representation $$[-m - nL, ~ m + nL] = [-m - nL, ~ m + nL] + [-m - nL, ~ m + nL], \qquad L = 2m+1,$$ through the repeatedly use of ""small"" representations; and obviously, the larger representation ""contains"" the small representation as a ""subrepresentation"", since $0 = 0 + 0$. From now on, we may construct a representation $\mathbb Z = \mathbb Z + \mathbb Z$ by induction, start from $[-3,3] = [-3,3] + [-3,3]$. Thus we've proved that any bijections on $\mathbb Z$ can be represented as a sum of two bijections on $\mathbb Z$. Now a problem rises: for any integers $n > 2$, is there always a representation $[-n, n] = [-n, n] + [-n, n]$ ? Note that by our means of ""representation"", $0 = 0 + 0$ is required; otherwise it's trivial; note also the above approach we've used for resolving the ""$\mathbb{Z}$-representation problem"" can't reach all cases. Anyone can answer this question? Please help me.",,"['abstract-algebra', 'number-theory', 'combinatorics', 'discrete-mathematics']"
69,"Given a field $\mathbb F$, is there a smallest field $\mathbb G\supseteq\mathbb F$ where every element in $\mathbb G$ has an $n$th root for all $n$?","Given a field , is there a smallest field  where every element in  has an th root for all ?",\mathbb F \mathbb G\supseteq\mathbb F \mathbb G n n,"The base field $\mathbb F$ is probably not important, but I'm using the rational expressions over the binary field, $\mathbb F=\mathbb F_2(x)$ . One subfield of $\mathbb G$ consists of ratios of polynomials in roots of $x$ , with coefficients in $\mathbb F_2$ , such as $$\frac{x^{2/5}+x^{-1/3}+x^2}{x^{-2/3}+1}=\frac{x^{16/15}+x^{1/3}+x^{8/3}}{1+x^{2/3}}=\frac{\sqrt[15]x^{40}+\sqrt[15]x^{16}+\sqrt[15]x^5}{\sqrt[15]x^{10}+1},$$ and such expressions are added and multiplied using the usual rules for rational functions, and $x^ax^b=x^{a+b}$ for $a,b\in\mathbb Q$ . Squaring and square roots are linear over $\mathbb F_2$ , so we can simply halve the exponents on $x$ to get a square root of the expression. Thus, any $2^n$ th root always exists. For example, $$x+1=(x^{1/2}+1)^2=(x^{1/4}+1)^4=(x^{1/8}+1)^8.$$ But this doesn't work for other $n$ th roots; it's not possible to write $$x+1=\frac{p(x^{1/m})^3}{q(x^{1/m})^3}$$ where $p,q$ are polynomials and $m\in\mathbb N$ . So it's necessary to formally adjoin $\sqrt[3]{x+1}$ to the field, or instead $\sqrt[3]{x^{1/2}+1}$ , etc. Is there a unique, well-defined field $\mathbb G$ of such algebraic expressions over $\mathbb F$ ? Note that I don't want $n$ different $n$ th roots of each element, just a single root (unless $\mathbb F$ already has roots of unity; but I chose $\mathbb F_2$ to avoid this). Given the algebraic closure $\mathbb A\supseteq\mathbb F$ , we might just take the intersection $\mathbb G\overset?=\bigcap\{\mathbb B\}$ of all intermediate fields $\mathbb A\supseteq\mathbb B\supseteq\mathbb F$ with the property $\forall n\in\mathbb N,\,\forall a\in\mathbb B,\,\exists b\in\mathbb B,\,a=b^n$ . But this doesn't work because different fields have different roots of $a$ , so their intersection contains no root of $a$ . Presumably there's some way to use the axiom of choice to construct $\mathbb G$ , either through $\mathbb A$ , or directly from $\mathbb F$ . Is this the case? Can the Wiki proof of existence (I haven't followed it in detail) be modified to give $n$ th roots of everything without introducing new roots of unity? And what about uniqueness? Is there a simpler construction of $\mathbb G$ for the special case of $\mathbb F_2(x)$ , that doesn't use the axiom of choice? Here I don't require uniqueness. See for example this answer ; we would be using polynomials of the form $x^p-a$ which are irreducible over the field defined by the previous polynomials. Having several $n$ th roots of $a\neq0$ is equivalent to having an $n$ th root of unity: If $x_1^n=x_2^n=a$ and $x_1\neq x_2$ , then $(x_1/x_2)^n=1$ and $(x_1/x_2)\neq1$ . Conversely, if $\omega^n=1$ and $\omega\neq1$ , and $x_1^n=a$ , then $(\omega x_1)^n=a$ and $x_1\neq\omega x_1$ . If $\mathbb F$ has a primitive $mn$ th root of unity, then it also has a primitive $n$ th root of unity; so we need only consider prime numbers. Fix two primes $p\neq q$ . If $\mathbb F$ has a primitive $p$ th root of unity $\omega_1$ , then $\mathbb G$ should have a primitive $p^n$ th root of unity $\omega_n$ for all $n$ , since non-primitive $p^n$ th roots of unity can never reach $\omega_{n-1}$ as a $p$ th power. If $\mathbb F$ doesn't have a primitive $q$ th root of unity, then $\mathbb G$ shouldn't either, since we already have $1^q=1$ and $(\omega_n^r)^q=\omega_n$ where $r=q^{-1}\bmod p^n$ .","The base field is probably not important, but I'm using the rational expressions over the binary field, . One subfield of consists of ratios of polynomials in roots of , with coefficients in , such as and such expressions are added and multiplied using the usual rules for rational functions, and for . Squaring and square roots are linear over , so we can simply halve the exponents on to get a square root of the expression. Thus, any th root always exists. For example, But this doesn't work for other th roots; it's not possible to write where are polynomials and . So it's necessary to formally adjoin to the field, or instead , etc. Is there a unique, well-defined field of such algebraic expressions over ? Note that I don't want different th roots of each element, just a single root (unless already has roots of unity; but I chose to avoid this). Given the algebraic closure , we might just take the intersection of all intermediate fields with the property . But this doesn't work because different fields have different roots of , so their intersection contains no root of . Presumably there's some way to use the axiom of choice to construct , either through , or directly from . Is this the case? Can the Wiki proof of existence (I haven't followed it in detail) be modified to give th roots of everything without introducing new roots of unity? And what about uniqueness? Is there a simpler construction of for the special case of , that doesn't use the axiom of choice? Here I don't require uniqueness. See for example this answer ; we would be using polynomials of the form which are irreducible over the field defined by the previous polynomials. Having several th roots of is equivalent to having an th root of unity: If and , then and . Conversely, if and , and , then and . If has a primitive th root of unity, then it also has a primitive th root of unity; so we need only consider prime numbers. Fix two primes . If has a primitive th root of unity , then should have a primitive th root of unity for all , since non-primitive th roots of unity can never reach as a th power. If doesn't have a primitive th root of unity, then shouldn't either, since we already have and where .","\mathbb F \mathbb F=\mathbb F_2(x) \mathbb G x \mathbb F_2 \frac{x^{2/5}+x^{-1/3}+x^2}{x^{-2/3}+1}=\frac{x^{16/15}+x^{1/3}+x^{8/3}}{1+x^{2/3}}=\frac{\sqrt[15]x^{40}+\sqrt[15]x^{16}+\sqrt[15]x^5}{\sqrt[15]x^{10}+1}, x^ax^b=x^{a+b} a,b\in\mathbb Q \mathbb F_2 x 2^n x+1=(x^{1/2}+1)^2=(x^{1/4}+1)^4=(x^{1/8}+1)^8. n x+1=\frac{p(x^{1/m})^3}{q(x^{1/m})^3} p,q m\in\mathbb N \sqrt[3]{x+1} \sqrt[3]{x^{1/2}+1} \mathbb G \mathbb F n n \mathbb F \mathbb F_2 \mathbb A\supseteq\mathbb F \mathbb G\overset?=\bigcap\{\mathbb B\} \mathbb A\supseteq\mathbb B\supseteq\mathbb F \forall n\in\mathbb N,\,\forall a\in\mathbb B,\,\exists b\in\mathbb B,\,a=b^n a a \mathbb G \mathbb A \mathbb F n \mathbb G \mathbb F_2(x) x^p-a n a\neq0 n x_1^n=x_2^n=a x_1\neq x_2 (x_1/x_2)^n=1 (x_1/x_2)\neq1 \omega^n=1 \omega\neq1 x_1^n=a (\omega x_1)^n=a x_1\neq\omega x_1 \mathbb F mn n p\neq q \mathbb F p \omega_1 \mathbb G p^n \omega_n n p^n \omega_{n-1} p \mathbb F q \mathbb G 1^q=1 (\omega_n^r)^q=\omega_n r=q^{-1}\bmod p^n","['abstract-algebra', 'field-theory', 'extension-field', 'radicals', 'axiom-of-choice']"
70,Permutation Representations and Group Actions,Permutation Representations and Group Actions,,"Suppose we have a finite group $G$ acting on a finite set $X=\{ x_1, ..., x_n \}$. Then we can take the free Abelian group generated by elements of $X$ (which is of course isomorphic to $\mathbb{Z}^n$) and we get an induced action of $G$. My question is if it is possible to have two non-isomorphic $G$ actions on $X$ that induce isomorphic actions on $\mathbb{Z}^n$. Thanks!","Suppose we have a finite group $G$ acting on a finite set $X=\{ x_1, ..., x_n \}$. Then we can take the free Abelian group generated by elements of $X$ (which is of course isomorphic to $\mathbb{Z}^n$) and we get an induced action of $G$. My question is if it is possible to have two non-isomorphic $G$ actions on $X$ that induce isomorphic actions on $\mathbb{Z}^n$. Thanks!",,"['abstract-algebra', 'group-theory', 'permutations']"
71,"About the Center of the Special Linear Group $SL(n,F)$",About the Center of the Special Linear Group,"SL(n,F)","I want to classify the center of the Special Linear Group. I already determined the center for $SL(n,F)$ : $$Z(SL(n,F))=\left\{ \lambda I_n:\lambda^n=1 \right\}.$$ I showed that $Z(SL(n,F))$ is itself a group and now I want to show that $Z(SL(n,F))$ is cyclic and has a order dividing $n$ . I thought this is possible by regarding the map $SL(n,F)\rightarrow P(F)$ , $P(F)$ being the projective space. The kernel of this map is $Z(SL(n,F))$ . How do I have to argue now?","I want to classify the center of the Special Linear Group. I already determined the center for : I showed that is itself a group and now I want to show that is cyclic and has a order dividing . I thought this is possible by regarding the map , being the projective space. The kernel of this map is . How do I have to argue now?","SL(n,F) Z(SL(n,F))=\left\{ \lambda I_n:\lambda^n=1 \right\}. Z(SL(n,F)) Z(SL(n,F)) n SL(n,F)\rightarrow P(F) P(F) Z(SL(n,F))","['abstract-algebra', 'group-theory', 'cyclic-groups', 'roots-of-unity']"
72,Is the ring of formal power series in infinitely many variables a unique factorization domain?,Is the ring of formal power series in infinitely many variables a unique factorization domain?,,"Is $R[[x_1,x_2\dots]]$ a unique factorization domain where the notation means infinite sums where each term is a finite product over the $x_i's$ with coefficients in $R$. I am most interested in the case where $R = \Bbb R$ or $\Bbb C$.","Is $R[[x_1,x_2\dots]]$ a unique factorization domain where the notation means infinite sums where each term is a finite product over the $x_i's$ with coefficients in $R$. I am most interested in the case where $R = \Bbb R$ or $\Bbb C$.",,"['abstract-algebra', 'ring-theory', 'unique-factorization-domains', 'formal-power-series']"
73,Classifying groups of order 18,Classifying groups of order 18,,"I am trying to classify groups of order 18. So far, I have shown that a group $G$ of order 18 is given by $G\cong C_9 \rtimes_{\varphi} C_2$ or $G\cong (C_3 \times C_3)\rtimes_{\varphi} C_2$. If $G\cong C_9 \rtimes_{\varphi} C_2$, then $\mid \varphi(1) \mid$ divides $2$, so that $\varphi(1)$ is trivial or inverts a generator of $C_9$. I concluded that $G \cong C_{18}$ in the former case and $G \cong D_{18}$ in the latter case. I am now considering the case $G\cong (C_3 \times C_3)\rtimes_{\varphi} C_2$, but I am stuck. I have found this article , but they do not do it the same way as me (finding the image of $\varphi(1)$ in $Aut(C_3 \times C_3)$). Is there a way to do this using my method? My goal is to be able to do questions like this on my algebra qual without having to fiddle around. I was messing around with the fact that $Aut(C_3 \times C_3)\cong GL_2(C_3)$.","I am trying to classify groups of order 18. So far, I have shown that a group $G$ of order 18 is given by $G\cong C_9 \rtimes_{\varphi} C_2$ or $G\cong (C_3 \times C_3)\rtimes_{\varphi} C_2$. If $G\cong C_9 \rtimes_{\varphi} C_2$, then $\mid \varphi(1) \mid$ divides $2$, so that $\varphi(1)$ is trivial or inverts a generator of $C_9$. I concluded that $G \cong C_{18}$ in the former case and $G \cong D_{18}$ in the latter case. I am now considering the case $G\cong (C_3 \times C_3)\rtimes_{\varphi} C_2$, but I am stuck. I have found this article , but they do not do it the same way as me (finding the image of $\varphi(1)$ in $Aut(C_3 \times C_3)$). Is there a way to do this using my method? My goal is to be able to do questions like this on my algebra qual without having to fiddle around. I was messing around with the fact that $Aut(C_3 \times C_3)\cong GL_2(C_3)$.",,"['abstract-algebra', 'group-theory']"
74,Difference between Stabilizer and Centralizer?,Difference between Stabilizer and Centralizer?,,"I know that the Centralizer of an element $a$ in a group $G$ is defined as follows $$C_G(a) = \{ g \in G \space  | \space ga = ag \}.$$ It can also be defined as follows $$C_G(a) = \{ g \in G \space  | \space  gag^{-1} = a \}$$ Informally, It contains all the elements in $G$ that commutes with $a$ And also I know that $C_G(a)$ is a subgroup of $G$ Now the center of a group is defined as follows $$Z(G) = \{ g \in G \space | \space gx =xg \space\forall x \in G \}$$ I also know that the center $Z(G)$ of the group $G$ is a subgroup of $G$ and $Z(G) \subseteq C_G(a)$ and hence $Z(G)$ is a subgroup of $C_G(a)$ And so $Z(G) \leq C_G(a) \leq G $ where $\leq$ denotes a subgroup Now I am very confused about the nature of the Stabilizer. All I know is that if $G$ is a group acting on a set $S$ and $s \in S$ then the stabilizer of $s$ in $G$ is the set $$G_s = \{ g\in G \space | \space gs = s \}.$$ Here are my questions (1) Is the stabilizer a subgroup of $G$ I figured this one out. First of all it's not empty since by the definition of a group action $es = s$ and so $e \in G_s$ Now let $g,h \in G_s$ then $(gh)s = g(hs)= g(s) =s$ hence $gh \in G_s$ Now let $g \in G_s$ then $g^{-1}s = g^{-1} (gs) = (g^{-1}g)s = es = s$ and so $g^{-1} \in G_s$ and hence $G_s \leq G$ (2) What is the difference between the Stabilizer and the centralizer because most of the time I see them used interchangeably especially when dealing with the number of element in a conjugacy class. (3) Is it true that $[G:C_G(a)] = [G:G_a]$? (4) Where does that stabilizer fit in  here  $Z(G) \leq C_G(a) \leq G $ where $\leq$  denotes subgroup?","I know that the Centralizer of an element $a$ in a group $G$ is defined as follows $$C_G(a) = \{ g \in G \space  | \space ga = ag \}.$$ It can also be defined as follows $$C_G(a) = \{ g \in G \space  | \space  gag^{-1} = a \}$$ Informally, It contains all the elements in $G$ that commutes with $a$ And also I know that $C_G(a)$ is a subgroup of $G$ Now the center of a group is defined as follows $$Z(G) = \{ g \in G \space | \space gx =xg \space\forall x \in G \}$$ I also know that the center $Z(G)$ of the group $G$ is a subgroup of $G$ and $Z(G) \subseteq C_G(a)$ and hence $Z(G)$ is a subgroup of $C_G(a)$ And so $Z(G) \leq C_G(a) \leq G $ where $\leq$ denotes a subgroup Now I am very confused about the nature of the Stabilizer. All I know is that if $G$ is a group acting on a set $S$ and $s \in S$ then the stabilizer of $s$ in $G$ is the set $$G_s = \{ g\in G \space | \space gs = s \}.$$ Here are my questions (1) Is the stabilizer a subgroup of $G$ I figured this one out. First of all it's not empty since by the definition of a group action $es = s$ and so $e \in G_s$ Now let $g,h \in G_s$ then $(gh)s = g(hs)= g(s) =s$ hence $gh \in G_s$ Now let $g \in G_s$ then $g^{-1}s = g^{-1} (gs) = (g^{-1}g)s = es = s$ and so $g^{-1} \in G_s$ and hence $G_s \leq G$ (2) What is the difference between the Stabilizer and the centralizer because most of the time I see them used interchangeably especially when dealing with the number of element in a conjugacy class. (3) Is it true that $[G:C_G(a)] = [G:G_a]$? (4) Where does that stabilizer fit in  here  $Z(G) \leq C_G(a) \leq G $ where $\leq$  denotes subgroup?",,"['abstract-algebra', 'group-theory', 'group-actions']"
75,Degree of splitting field less than n! [duplicate],Degree of splitting field less than n! [duplicate],,"This question already has an answer here : Degree of splitting field divides n! [duplicate] (1 answer) Closed 7 years ago . I've been asked to prove that if a polynomial $f\in \mathbb{Q}[x]$ has degree $n$, then the splitting field of $f$ has degree less than or equal to $n!$. That is, $[\mathbb{Q}(\alpha_1,...,\alpha_n):\mathbb Q]\leq n!$, where $\alpha_i$ are the roots of $f$. My approach to this question is to use the tower law, to arrive at the equation: $$[\mathbb{Q}(\alpha_1,...,\alpha_n):\mathbb{Q}]=[\mathbb{Q}(\alpha_1,...,\alpha_n):\mathbb{Q}(\alpha_1,...,\alpha_{n-1})][\mathbb{Q}(\alpha_1,...,\alpha_{n-1}):\mathbb{Q}(\alpha_1,...,\alpha_{n-2})]\cdots[\mathbb{Q}(\alpha_1):\mathbb{Q}]$$ Clearly $[\mathbb{Q}(\alpha_1):\mathbb{Q}]\leq n$, since the minimum polynomial of $\alpha_1$ divides $f$. I claim that in general: $[\mathbb{Q}(\alpha_1,...,\alpha_i):\mathbb{Q}(\alpha_1,...,\alpha_{i-1})]\leq n-i, \forall i= 1,2,...,n$. My question is, whether this claim is indeed true, and if so how would I prove it. I've tried using induction but I can't seem to make it work. Also, if anyone has a proof for the original question without using my claim, then can you please post that too.","This question already has an answer here : Degree of splitting field divides n! [duplicate] (1 answer) Closed 7 years ago . I've been asked to prove that if a polynomial $f\in \mathbb{Q}[x]$ has degree $n$, then the splitting field of $f$ has degree less than or equal to $n!$. That is, $[\mathbb{Q}(\alpha_1,...,\alpha_n):\mathbb Q]\leq n!$, where $\alpha_i$ are the roots of $f$. My approach to this question is to use the tower law, to arrive at the equation: $$[\mathbb{Q}(\alpha_1,...,\alpha_n):\mathbb{Q}]=[\mathbb{Q}(\alpha_1,...,\alpha_n):\mathbb{Q}(\alpha_1,...,\alpha_{n-1})][\mathbb{Q}(\alpha_1,...,\alpha_{n-1}):\mathbb{Q}(\alpha_1,...,\alpha_{n-2})]\cdots[\mathbb{Q}(\alpha_1):\mathbb{Q}]$$ Clearly $[\mathbb{Q}(\alpha_1):\mathbb{Q}]\leq n$, since the minimum polynomial of $\alpha_1$ divides $f$. I claim that in general: $[\mathbb{Q}(\alpha_1,...,\alpha_i):\mathbb{Q}(\alpha_1,...,\alpha_{i-1})]\leq n-i, \forall i= 1,2,...,n$. My question is, whether this claim is indeed true, and if so how would I prove it. I've tried using induction but I can't seem to make it work. Also, if anyone has a proof for the original question without using my claim, then can you please post that too.",,"['abstract-algebra', 'field-theory', 'splitting-field']"
76,why algebraic structures?,why algebraic structures?,,"According to wikipedia, an algebraic structure is an arbitrary set with one or more finitary operations defined on it. From a model theory perspective, I understand this definition as:  structure with no relations, only functions and constants (I added the constants to my definition, because I know that group, rings, have them). So for example an ordered field is not an algebraic structure (has relation), and a set isn't too (no function). Regardless of the exact definition, a massive part of mathematics is concerned with the study of these structures, and the relations between them. Geometry benefits from algebraic geometry, number theory from algebraic number theory, and so on. My question is, what do this sort of structures have in them, that makes us study and use them so much, and not other structures?","According to wikipedia, an algebraic structure is an arbitrary set with one or more finitary operations defined on it. From a model theory perspective, I understand this definition as:  structure with no relations, only functions and constants (I added the constants to my definition, because I know that group, rings, have them). So for example an ordered field is not an algebraic structure (has relation), and a set isn't too (no function). Regardless of the exact definition, a massive part of mathematics is concerned with the study of these structures, and the relations between them. Geometry benefits from algebraic geometry, number theory from algebraic number theory, and so on. My question is, what do this sort of structures have in them, that makes us study and use them so much, and not other structures?",,"['abstract-algebra', 'soft-question', 'model-theory']"
77,Splitting field implies Galois extension,Splitting field implies Galois extension,,"I hope this isn't too elementary of a question, but I'm not sure I understand Artin's proof that if $K/F$ is a finite extension, then $K/F$ Galois is equivalent to $K$ being a splitting field over $F$ (this is Theorem 16.6.4 in the second edition). We're working in characteristic zero here. I understand one direction, that Galois implies splitting field: if we let $\gamma_{1}$ generate $K/F$ and $f$ be its minimal polynomial of degree $n$ , each automorphism of $K/F$ comes from sending $\gamma_{1}$ to another root of $f$ , and in order for $\operatorname{Gal}(K/F)$ to have order $n$ we need all of the roots of $f$ to be in $K$ . However, I still am not seeing the other direction - after stating the above Artin seems to finish by saying that if we define $\gamma_{1},f$ as before, "" $K$ is a splitting field over $F$ iff $f$ splits completely in $K$ ."" But isn't it possible that $f$ doesn't split, but $K$ is the splitting field of some other polynomial?","I hope this isn't too elementary of a question, but I'm not sure I understand Artin's proof that if is a finite extension, then Galois is equivalent to being a splitting field over (this is Theorem 16.6.4 in the second edition). We're working in characteristic zero here. I understand one direction, that Galois implies splitting field: if we let generate and be its minimal polynomial of degree , each automorphism of comes from sending to another root of , and in order for to have order we need all of the roots of to be in . However, I still am not seeing the other direction - after stating the above Artin seems to finish by saying that if we define as before, "" is a splitting field over iff splits completely in ."" But isn't it possible that doesn't split, but is the splitting field of some other polynomial?","K/F K/F K F \gamma_{1} K/F f n K/F \gamma_{1} f \operatorname{Gal}(K/F) n f K \gamma_{1},f K F f K f K","['abstract-algebra', 'field-theory', 'galois-theory']"
78,"In a field extension, if each element's degree is bounded uniformly by $n$, is the extension finite?","In a field extension, if each element's degree is bounded uniformly by , is the extension finite?",n,"Given a field extension $F\subset E$, if there exists a $n$ such that for each $\alpha\in E$ we have $|F[\alpha]: F| \leq n$, can we conclude that $|E:F|<\infty$? This is the question here , but I would like to drop the char $0$ assumption on $F$.","Given a field extension $F\subset E$, if there exists a $n$ such that for each $\alpha\in E$ we have $|F[\alpha]: F| \leq n$, can we conclude that $|E:F|<\infty$? This is the question here , but I would like to drop the char $0$ assumption on $F$.",,"['abstract-algebra', 'field-theory', 'extension-field']"
79,"When does $f(X) = g(X+1)-g(X)$ where $f, g \in \mathbb{C}(X)$?",When does  where ?,"f(X) = g(X+1)-g(X) f, g \in \mathbb{C}(X)","Is there an interesting or useful necessary and sufficient condition for a rational function $f \in \mathbb{C}(X)$ to be expressible as the difference $g(X+1)-g(X)$ of another rational function $g \in \mathbb{C}(X)$? As an example, every polynomial $f$ is expressible in such a way since we have the Faulhaber's formula . Alternately, we can observe that $(X+1)^n - X^n = \sum\limits_{k=0}^{n-1}{n\choose k}X^k$ and so given a polynomial $f(X) = a_nX^n + \ldots + a_1X + a_0$ we may solve the system of equations obtained by equating the $k$-th terms of $f$ with those of $\sum\limits_{k=0}^{n}\left(\sum\limits_{i=k+1}^{n+1}b_i{i \choose k}\right)X^k$. The associated matrix consists of the upper triangular matrix with entries $A_{ij} = {j \choose i-1}, 1 \leq i \leq j \leq n+1$, and its determinant is clearly nonzero since it is the product of the diagonal entries ${i \choose i-1} = i$, so the determinant is $(n+1)!$. The solution for the $b_i$, $1 \leq i \leq n+1$ will give a polynomial $g(X) = b_{n+1}X^{n+1} + \ldots b_1X + b_0$ such that $f(X) = g(X+1)-g(X)$ ($b_0$ is arbitrary). Another example is the rational function $f(X) = \frac{1}{X(X+1)}$ where $g(X) = -\frac{1}{X}$. As a non-example, consider the function $f(X) = \frac{1}{X}$. Suppose there exist polynomials $p(X), q(X) \in \mathbb{C}[X]$ such that $\frac{p(X+1)}{q(X+1)}-\frac{p(X)}{q(X)} = \frac{1}{X}$. Assume moreover that $p$ and $q$ are coprime after dividing by any common factors. Then we have $Xp(X+1)q(X)-Xp(X)q(X+1)=q(X+1)q(X)$ so that $q(X)$ divides $Xq(X+1)$ and $q(X+1)$ divides $Xq(X)$. This means we have polynomials $\alpha(X), \beta(X)$ such that $$Xq(X+1) = \alpha(X)q(X)$$ $$Xq(X) = \beta(X)q(X+1)$$ By degree considerations, we have $\deg \alpha = \deg \beta = 1$. Then $X^2q(X) = \alpha(X)\beta(X)q(X)$ which means $X^2 = \alpha(X)\beta(X)$ and finally this forces $\alpha(X) = cX$ and $\beta(X) = \frac{1}{c}X$ for some $c \neq 0$. Substituting in the equations above we obtain $q(X+1) = cq(X)$ which is only possible if $q$ is constant because any zero would lead to infinitely many zeros. But this is impossible because $\frac{p(X)}{q(X)}$ would then be a polynomial, and its forward difference cannot equal $\frac{1}{X}$. I believe this proof can generalize to other cases where $f(X) = \frac{1}{g(X)}$ where $g$ is irreducible (that would just be the linear functions since $\mathbb{C}$ is algebraically closed). But I have not found a ""nice"" condition on $f$ which could be necessary and sufficient. The continuous analogue of the problem is easy, we know that $f$ has a rational antiderivative if and only if its partial fraction decomposition does not contain denominators of degree $1$.","Is there an interesting or useful necessary and sufficient condition for a rational function $f \in \mathbb{C}(X)$ to be expressible as the difference $g(X+1)-g(X)$ of another rational function $g \in \mathbb{C}(X)$? As an example, every polynomial $f$ is expressible in such a way since we have the Faulhaber's formula . Alternately, we can observe that $(X+1)^n - X^n = \sum\limits_{k=0}^{n-1}{n\choose k}X^k$ and so given a polynomial $f(X) = a_nX^n + \ldots + a_1X + a_0$ we may solve the system of equations obtained by equating the $k$-th terms of $f$ with those of $\sum\limits_{k=0}^{n}\left(\sum\limits_{i=k+1}^{n+1}b_i{i \choose k}\right)X^k$. The associated matrix consists of the upper triangular matrix with entries $A_{ij} = {j \choose i-1}, 1 \leq i \leq j \leq n+1$, and its determinant is clearly nonzero since it is the product of the diagonal entries ${i \choose i-1} = i$, so the determinant is $(n+1)!$. The solution for the $b_i$, $1 \leq i \leq n+1$ will give a polynomial $g(X) = b_{n+1}X^{n+1} + \ldots b_1X + b_0$ such that $f(X) = g(X+1)-g(X)$ ($b_0$ is arbitrary). Another example is the rational function $f(X) = \frac{1}{X(X+1)}$ where $g(X) = -\frac{1}{X}$. As a non-example, consider the function $f(X) = \frac{1}{X}$. Suppose there exist polynomials $p(X), q(X) \in \mathbb{C}[X]$ such that $\frac{p(X+1)}{q(X+1)}-\frac{p(X)}{q(X)} = \frac{1}{X}$. Assume moreover that $p$ and $q$ are coprime after dividing by any common factors. Then we have $Xp(X+1)q(X)-Xp(X)q(X+1)=q(X+1)q(X)$ so that $q(X)$ divides $Xq(X+1)$ and $q(X+1)$ divides $Xq(X)$. This means we have polynomials $\alpha(X), \beta(X)$ such that $$Xq(X+1) = \alpha(X)q(X)$$ $$Xq(X) = \beta(X)q(X+1)$$ By degree considerations, we have $\deg \alpha = \deg \beta = 1$. Then $X^2q(X) = \alpha(X)\beta(X)q(X)$ which means $X^2 = \alpha(X)\beta(X)$ and finally this forces $\alpha(X) = cX$ and $\beta(X) = \frac{1}{c}X$ for some $c \neq 0$. Substituting in the equations above we obtain $q(X+1) = cq(X)$ which is only possible if $q$ is constant because any zero would lead to infinitely many zeros. But this is impossible because $\frac{p(X)}{q(X)}$ would then be a polynomial, and its forward difference cannot equal $\frac{1}{X}$. I believe this proof can generalize to other cases where $f(X) = \frac{1}{g(X)}$ where $g$ is irreducible (that would just be the linear functions since $\mathbb{C}$ is algebraically closed). But I have not found a ""nice"" condition on $f$ which could be necessary and sufficient. The continuous analogue of the problem is easy, we know that $f$ has a rational antiderivative if and only if its partial fraction decomposition does not contain denominators of degree $1$.",,"['abstract-algebra', 'complex-analysis', 'functional-equations', 'rational-functions']"
80,"Ideal of $\mathbb{C}[x,y]$ not generated by two elements",Ideal of  not generated by two elements,"\mathbb{C}[x,y]","Consider the ring of polynomials in two variables $\mathbb{C}[x,y]$. Show that the ideal $\langle xy^3, x^2y^2, x^3y\rangle$ cannot be generated by two elements. Until now, I assumed by contradiction that $\langle xy^3, x^2y^2, x^3y\rangle=\langle g, h \rangle$ for some $g,h \in \mathbb{C}[x,y]$. Then, we have that: $g=xy^3a + x^2y^2b + x^3yc$ $h=xy^3d + x^2y^2e + x^3yf$ for $a, b, c, d, e, f$ in the polynomial ring. However, I'm not sure if this is the correct approach as I'm not sure on how to proceed. I was thinking that maybe one can write an element in $\langle xy^3, x^2y^2, x^3y\rangle$ as the square of a sum. What would be the best approach to do this? Thanks for the help.","Consider the ring of polynomials in two variables $\mathbb{C}[x,y]$. Show that the ideal $\langle xy^3, x^2y^2, x^3y\rangle$ cannot be generated by two elements. Until now, I assumed by contradiction that $\langle xy^3, x^2y^2, x^3y\rangle=\langle g, h \rangle$ for some $g,h \in \mathbb{C}[x,y]$. Then, we have that: $g=xy^3a + x^2y^2b + x^3yc$ $h=xy^3d + x^2y^2e + x^3yf$ for $a, b, c, d, e, f$ in the polynomial ring. However, I'm not sure if this is the correct approach as I'm not sure on how to proceed. I was thinking that maybe one can write an element in $\langle xy^3, x^2y^2, x^3y\rangle$ as the square of a sum. What would be the best approach to do this? Thanks for the help.",,"['abstract-algebra', 'polynomials', 'ring-theory', 'commutative-algebra', 'ideals']"
81,"Making a proof precise of ""Aut$(Q_8)\cong S_4$""","Making a proof precise of ""Aut""",(Q_8)\cong S_4,"I know, with some machinery, how to prove that Aut$(Q_8)\cong S_4$. My question here is about not how to prove, but is about an incomplete proof (I feel) given by a student to me (it should be possibly somewhere on-line, since it is not so easy to get the idea of his proof, I think.) Consider a cube, and label $i,-i$ on a pair of opposite faces; similarly put $j,-j$ and $k,-k$ on other faces. Since the group of rotations of cube is $S_4$, Aut$(Q_8)$ is $S_4$. Can anyone make this argument precise?","I know, with some machinery, how to prove that Aut$(Q_8)\cong S_4$. My question here is about not how to prove, but is about an incomplete proof (I feel) given by a student to me (it should be possibly somewhere on-line, since it is not so easy to get the idea of his proof, I think.) Consider a cube, and label $i,-i$ on a pair of opposite faces; similarly put $j,-j$ and $k,-k$ on other faces. Since the group of rotations of cube is $S_4$, Aut$(Q_8)$ is $S_4$. Can anyone make this argument precise?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
82,Module which is not direct sum of indecomposable submodules,Module which is not direct sum of indecomposable submodules,,"I would like to find an example of a ring $R$ and a $R$-module $M$, which can't be written as a direct sum of indecomposable submodules, i.e.   $$ M \not \cong \bigoplus\limits_{i \in I} M_i$$   for all set $\{M_i \;\vert\; i \in I \}$ of indecomposable submodules. In that case, I know that $M$ should be neither noetherian nor artinian, but I wasn't able to find such an example. Any help would be appreciated!","I would like to find an example of a ring $R$ and a $R$-module $M$, which can't be written as a direct sum of indecomposable submodules, i.e.   $$ M \not \cong \bigoplus\limits_{i \in I} M_i$$   for all set $\{M_i \;\vert\; i \in I \}$ of indecomposable submodules. In that case, I know that $M$ should be neither noetherian nor artinian, but I wasn't able to find such an example. Any help would be appreciated!",,"['abstract-algebra', 'ring-theory', 'modules', 'direct-sum']"
83,"Show that $k[x,y]/(xy-1)$ is not isomorphic to a polynomial ring in one variable.",Show that  is not isomorphic to a polynomial ring in one variable.,"k[x,y]/(xy-1)","Let $R=k[x,y]$ be a polynomial ring ($k$, of course, is a field). Show that   $R/(xy-1)$ is not isomorphic to a polynomial ring in one variable.","Let $R=k[x,y]$ be a polynomial ring ($k$, of course, is a field). Show that   $R/(xy-1)$ is not isomorphic to a polynomial ring in one variable.",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'ideals', 'maximal-and-prime-ideals']"
84,Eulers Doubly Infinite Geometric Series,Eulers Doubly Infinite Geometric Series,,"Recently I came across and ""Identity"" discovered by Euler. $$ E = \cdots + X^3 + X^2 + X + 1 + \frac{1}{X} + \frac{1}{X^2} + \frac{1}{X^3} + \cdots = 0 $$ You can formally obtain this expression by applying the formula for the ordinary geometric series. $$ S = 1 + X + X^2 + \cdots = \frac{1}{1-X}$$ $$\Rightarrow E = \frac{1}{1-X} + \frac{1}{1-\frac{1}{X}} - 1 = 0$$ Disclaimer Before the community jumps all over me and starts talking about, The radius of convergence of the geometric series Validity (or rather lack thereof) of manipulating divergent series. Some snarky statement about using this to prove 1=0. Calm down, I know that the above is not a proof in $\mathbb{R}$ with the usual notion of convergence. And I am not claiming that it is. I know all about infinite series and partial sums and epsilons and deltas and all that great fantastic stuff. This is just a neat mathematical exploration and I'm hoping some of you come along with me on this. Motivation Now the above ""doubly infinite geometric series"" actually has some uses. One thing is that it provides and insight in how to determine the representation of $-1$ as a p-adic number. For instance in the context of so called $10-adic$ numbers we can easily see from the arithmetic that, $$ \dots \overline{999}. + 1 = 0 $$ Now in the reals, $$1 = \sum_{k=1}^\infty \frac{9}{10^k} $$ In the 10-adic numbers, $$ \dots \overline{999} = \sum_{k=0}^\infty 9 \cdot 10^k $$ Which means that we could write, $$ 0 = \dots \overline{999}. + 1 = \left(  \sum_{k=0}^\infty 9 \cdot 10^k  \right)_{\mathbb{Q}_{10}} + \left(\sum_{k=1}^\infty \frac{9}{10^k} \right)_{\mathbb{R}}$$ So it seems as if somehow this identity allows us to to relate the expressions for rationals in $\mathbb{Q}_p$ to the expressions for rationals in $\mathbb{R}$. Observation Something which has always struck me about the usual proof of the finite/infinite geometric series is that the formula depends only on the algebraic properties of the number system. In that sense perhaps we can think of this identity not as a statement about the real numbers (for which it is obviously not true) but somehow a statement about the properties of fields. The Question I want to know other contexts in which this ""identity"" has proven useful. Possible answers include but are not limited to, Any useful application of the identity distinct from the one above. Rigorous notions of convergence for which the identity makes sense. Rigorous settings in which the formal manipulations makes sense (e.g. similar to the way Algebraists think of polynomials, not as functions but algebraic entities).","Recently I came across and ""Identity"" discovered by Euler. $$ E = \cdots + X^3 + X^2 + X + 1 + \frac{1}{X} + \frac{1}{X^2} + \frac{1}{X^3} + \cdots = 0 $$ You can formally obtain this expression by applying the formula for the ordinary geometric series. $$ S = 1 + X + X^2 + \cdots = \frac{1}{1-X}$$ $$\Rightarrow E = \frac{1}{1-X} + \frac{1}{1-\frac{1}{X}} - 1 = 0$$ Disclaimer Before the community jumps all over me and starts talking about, The radius of convergence of the geometric series Validity (or rather lack thereof) of manipulating divergent series. Some snarky statement about using this to prove 1=0. Calm down, I know that the above is not a proof in $\mathbb{R}$ with the usual notion of convergence. And I am not claiming that it is. I know all about infinite series and partial sums and epsilons and deltas and all that great fantastic stuff. This is just a neat mathematical exploration and I'm hoping some of you come along with me on this. Motivation Now the above ""doubly infinite geometric series"" actually has some uses. One thing is that it provides and insight in how to determine the representation of $-1$ as a p-adic number. For instance in the context of so called $10-adic$ numbers we can easily see from the arithmetic that, $$ \dots \overline{999}. + 1 = 0 $$ Now in the reals, $$1 = \sum_{k=1}^\infty \frac{9}{10^k} $$ In the 10-adic numbers, $$ \dots \overline{999} = \sum_{k=0}^\infty 9 \cdot 10^k $$ Which means that we could write, $$ 0 = \dots \overline{999}. + 1 = \left(  \sum_{k=0}^\infty 9 \cdot 10^k  \right)_{\mathbb{Q}_{10}} + \left(\sum_{k=1}^\infty \frac{9}{10^k} \right)_{\mathbb{R}}$$ So it seems as if somehow this identity allows us to to relate the expressions for rationals in $\mathbb{Q}_p$ to the expressions for rationals in $\mathbb{R}$. Observation Something which has always struck me about the usual proof of the finite/infinite geometric series is that the formula depends only on the algebraic properties of the number system. In that sense perhaps we can think of this identity not as a statement about the real numbers (for which it is obviously not true) but somehow a statement about the properties of fields. The Question I want to know other contexts in which this ""identity"" has proven useful. Possible answers include but are not limited to, Any useful application of the identity distinct from the one above. Rigorous notions of convergence for which the identity makes sense. Rigorous settings in which the formal manipulations makes sense (e.g. similar to the way Algebraists think of polynomials, not as functions but algebraic entities).",,"['abstract-algebra', 'sequences-and-series', 'divergent-series']"
85,A finite field extension $K\supset\mathbb Q$ contains finitely many roots of unity,A finite field extension  contains finitely many roots of unity,K\supset\mathbb Q,"I must show that any finite field extension $K\supset \mathbb Q$ can only contain finitely many roots of unity. I reasoned in the following manner: Let $n<\infty$ be the degree of $K$ over $\mathbb Q$. Now, for any $\alpha\in K$ we must clearly have $\mathbb Q(\alpha)\subseteq K$. So the degree of $\alpha$ over $\mathbb Q$ is at most $n$. Hence, if we can show that there are only finitely many roots of unity of degree less than or equal to $n$, we are done. Now, since any root of unity of degree less than or equal to $n$ is a primitive $k$'th root of unity for some $k\leq n$ it follows that $K$ can at most contain all roots of the finite list of cyclotomic polynomials $\Phi_1,\Phi_2,...,\Phi_n$ which each has finitely many roots. All together this will be finitely many, which proves the claim. So my questions are: Is this correct/enough? When this exercise was on the blackboard in class a rather lengthy argument came up. So am I missing some important point here? My final version Having understood the point I missed before, I now see that it will suffice to argue that $\operatorname{deg}(\Phi_k)=\varphi(k)$ eventually exceeds $n$. We know that $$ \varphi(k)=\prod_ j\varphi(p_j^{a_j}) $$ where $p_j$ are the prime factors of $k$ (similarly to what Ewan Delanoy gave in his answer). Now first note that $\varphi(p^k)$ is stricly increasing both with the prime $p$ and the multiplicity $k$. So we may find a multiplicity $M$ such that $\varphi(2^M)>n$ thus implying $\varphi(p^M)>n$ for all primes. Also any prime $p>n+1$ must have $\varphi(p^k)\geq p-1>n$ for all $k$. This shows that $\varphi(k)>n$ if one of the prime factors of $k$ is larger than $n+1$ or one of the multiplicities is at least $M$. This will certainly happen for $k$ large enough.","I must show that any finite field extension $K\supset \mathbb Q$ can only contain finitely many roots of unity. I reasoned in the following manner: Let $n<\infty$ be the degree of $K$ over $\mathbb Q$. Now, for any $\alpha\in K$ we must clearly have $\mathbb Q(\alpha)\subseteq K$. So the degree of $\alpha$ over $\mathbb Q$ is at most $n$. Hence, if we can show that there are only finitely many roots of unity of degree less than or equal to $n$, we are done. Now, since any root of unity of degree less than or equal to $n$ is a primitive $k$'th root of unity for some $k\leq n$ it follows that $K$ can at most contain all roots of the finite list of cyclotomic polynomials $\Phi_1,\Phi_2,...,\Phi_n$ which each has finitely many roots. All together this will be finitely many, which proves the claim. So my questions are: Is this correct/enough? When this exercise was on the blackboard in class a rather lengthy argument came up. So am I missing some important point here? My final version Having understood the point I missed before, I now see that it will suffice to argue that $\operatorname{deg}(\Phi_k)=\varphi(k)$ eventually exceeds $n$. We know that $$ \varphi(k)=\prod_ j\varphi(p_j^{a_j}) $$ where $p_j$ are the prime factors of $k$ (similarly to what Ewan Delanoy gave in his answer). Now first note that $\varphi(p^k)$ is stricly increasing both with the prime $p$ and the multiplicity $k$. So we may find a multiplicity $M$ such that $\varphi(2^M)>n$ thus implying $\varphi(p^M)>n$ for all primes. Also any prime $p>n+1$ must have $\varphi(p^k)\geq p-1>n$ for all $k$. This shows that $\varphi(k)>n$ if one of the prime factors of $k$ is larger than $n+1$ or one of the multiplicities is at least $M$. This will certainly happen for $k$ large enough.",,"['abstract-algebra', 'field-theory', 'roots-of-unity']"
86,Quaternion group associativity,Quaternion group associativity,,"Consider the eight objects $\pm 1, \pm i, \pm j, \pm k$ with multiplication rules: $ij=k,jk=i,ki=j,ji=-k,kj=-i,ik=-j,i^2=j^2=k^2=-1$, where the minus signs behave as expected and $1$ and $-1$ multiply as expected. Show that these objects form a group containing exactly one involution. Well, it is easy to determine closure from the definition. $1$ is clearly the identity, and the inverses can also be determined $(i,-i),(j,-j),(k,-k)$, $-1$ with itself, and $1$ with itself. The only involution is $-1$. Is there an easy way to check associativity of this group? There are too many possible combinations $(ab)c=a(bc)$ to check directly. ($8^3$ possible combinations)","Consider the eight objects $\pm 1, \pm i, \pm j, \pm k$ with multiplication rules: $ij=k,jk=i,ki=j,ji=-k,kj=-i,ik=-j,i^2=j^2=k^2=-1$, where the minus signs behave as expected and $1$ and $-1$ multiply as expected. Show that these objects form a group containing exactly one involution. Well, it is easy to determine closure from the definition. $1$ is clearly the identity, and the inverses can also be determined $(i,-i),(j,-j),(k,-k)$, $-1$ with itself, and $1$ with itself. The only involution is $-1$. Is there an easy way to check associativity of this group? There are too many possible combinations $(ab)c=a(bc)$ to check directly. ($8^3$ possible combinations)",,"['abstract-algebra', 'group-theory', 'quaternions']"
87,Can infinitely many primes lie over a prime?,Can infinitely many primes lie over a prime?,,"Let $R \subset S$ be an integral extension of domains and $\mathfrak p \subset R$ a prime ideal. Can it be the case that there are infinitely many distinct primes ${\cal P} \subset S$ such that ${\cal P} \cap R=\mathfrak p$? Certainly this is impossible if $S$ is a Dedekind domain, because the primes lying over $\mathfrak p$ are the primes of $S$ occurring in the factorization of $\mathfrak p$ over $S$.  I don't have much of an intuition for integral ring extensions that aren't number fields, so past this I'm not particularly sure.","Let $R \subset S$ be an integral extension of domains and $\mathfrak p \subset R$ a prime ideal. Can it be the case that there are infinitely many distinct primes ${\cal P} \subset S$ such that ${\cal P} \cap R=\mathfrak p$? Certainly this is impossible if $S$ is a Dedekind domain, because the primes lying over $\mathfrak p$ are the primes of $S$ occurring in the factorization of $\mathfrak p$ over $S$.  I don't have much of an intuition for integral ring extensions that aren't number fields, so past this I'm not particularly sure.",,"['abstract-algebra', 'commutative-algebra', 'integral-domain']"
88,"For finite abelian groups, show that $G \times G \cong H \times H$ implies $G \cong H$","For finite abelian groups, show that  implies",G \times G \cong H \times H G \cong H,"Let $G$ and $H$ be finite abelian groups such that $G \times G \cong H \times H$. Then $G \cong H$. I was going to just write the hypothesis as $G^2 \cong H^2$ and take square roots on both sides, but I don't think that would suffice (and neither would saying ""true"" a la Myself !)... The hypothesis tells us that there is an isomorphism, say $f: G \times G\to H \times H$. I would like to use this to come to the conclusion that there is a bijective homomorphism $g: G \to H$.  (I will be using additive notation...) Since $f((a, b) + (c, d)) = f(a,b) + f(c, d)$ for all $a,b \in G$ and $c, d \in H$, I was thinking about choosing an arbitrary $a, b \in G$ and calculating: $$ f((a, 0) + (b, 0)) = f(a, 0) + f(b, 0) \\ \Rightarrow f(a + b, 0) = f(a, 0) + f(b, 0). $$ I basically need to define g in such a way that it extracts the first dimension from the equation. Clearly (I think!), g will invoke f in some way. Can I have a tip on this? It's probably very simple, but I'm not sure how to express it symbolically. I might not need to show that g is bijective if I can say something to the effect of ""this routine verification is straightforward and left to the reader"", but I'm afraid I might not be able to perform such a verification if put on the spot! Here's a stab: f injective $\Leftrightarrow f(a,b) = f(c,d) \Rightarrow (a, b) = (c, d) \Rightarrow a = c \wedge b = d$ So by definition of g (forthcoming...), g(a) = g(c) implies that a = c. Surjective : For all $x, y \in H$, there exists $a, b \in G : f(a,b) = (x, y)$ Man, this really seems trivial, but without my definition of g, I feel like I'm handwaving... Thanks again, guys!","Let $G$ and $H$ be finite abelian groups such that $G \times G \cong H \times H$. Then $G \cong H$. I was going to just write the hypothesis as $G^2 \cong H^2$ and take square roots on both sides, but I don't think that would suffice (and neither would saying ""true"" a la Myself !)... The hypothesis tells us that there is an isomorphism, say $f: G \times G\to H \times H$. I would like to use this to come to the conclusion that there is a bijective homomorphism $g: G \to H$.  (I will be using additive notation...) Since $f((a, b) + (c, d)) = f(a,b) + f(c, d)$ for all $a,b \in G$ and $c, d \in H$, I was thinking about choosing an arbitrary $a, b \in G$ and calculating: $$ f((a, 0) + (b, 0)) = f(a, 0) + f(b, 0) \\ \Rightarrow f(a + b, 0) = f(a, 0) + f(b, 0). $$ I basically need to define g in such a way that it extracts the first dimension from the equation. Clearly (I think!), g will invoke f in some way. Can I have a tip on this? It's probably very simple, but I'm not sure how to express it symbolically. I might not need to show that g is bijective if I can say something to the effect of ""this routine verification is straightforward and left to the reader"", but I'm afraid I might not be able to perform such a verification if put on the spot! Here's a stab: f injective $\Leftrightarrow f(a,b) = f(c,d) \Rightarrow (a, b) = (c, d) \Rightarrow a = c \wedge b = d$ So by definition of g (forthcoming...), g(a) = g(c) implies that a = c. Surjective : For all $x, y \in H$, there exists $a, b \in G : f(a,b) = (x, y)$ Man, this really seems trivial, but without my definition of g, I feel like I'm handwaving... Thanks again, guys!",,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups']"
89,Roots of a polynomial in an integral domain,Roots of a polynomial in an integral domain,,"Let $R$ be a ring and $f(X) \in R[x]$ be a non-constant polynomial. We know that the number of roots, of $f(X)$ in $R$ has no relation, to its degree if $R$ is not commutative , or commutative but not a domain . But, The number of roots, of a non-zero polynomial over commutative integral domain, is at most its degree. How does one prove above result?","Let $R$ be a ring and $f(X) \in R[x]$ be a non-constant polynomial. We know that the number of roots, of $f(X)$ in $R$ has no relation, to its degree if $R$ is not commutative , or commutative but not a domain . But, The number of roots, of a non-zero polynomial over commutative integral domain, is at most its degree. How does one prove above result?",,"['abstract-algebra', 'commutative-algebra']"
90,Continued Fraction using all Perfect Squares,Continued Fraction using all Perfect Squares,,"What is known about the infinite continued fraction $$1 + \cfrac{1}{4 + \cfrac{1}{9 + \cfrac{1}{16 + \cdots}}} $$ whose terms include all perfect squares in order? Do we have a closed form expression for the value of this number? Is it known to be transcendental, or satisfy any other interesting properties?","What is known about the infinite continued fraction whose terms include all perfect squares in order? Do we have a closed form expression for the value of this number? Is it known to be transcendental, or satisfy any other interesting properties?",1 + \cfrac{1}{4 + \cfrac{1}{9 + \cfrac{1}{16 + \cdots}}} ,"['abstract-algebra', 'sequences-and-series', 'number-theory', 'arithmetic', 'continued-fractions']"
91,Galois Theoretic Proof of Fundamental Theorem of Algebra,Galois Theoretic Proof of Fundamental Theorem of Algebra,,"The Galois Theory proof involves proving that $F(i)$ is algebraically closed for any real closed field $F$ (where $i$ is a square root of $-1$), where we may define a real closed field as an ordered field in which every odd degree polynomial has a root and every positive number has a square root. The last step in the proof seems a bit ad-hoc though; it involves showing that $F(i)$ has no quadratic extension, which in turn involves showing that every element of $F(i)$ has a square root, which involves solving $(a+bi)^2=c+di$ for $a, b \in F$. Is there a more Galois-theoretic approach to the last step that I'm not aware of? Or, for that matter, some other deeper reason why the computation works out? Edit Here's the proof that I have that $F(i)$ is closed under square roots, to illustrate how it's ad-hoc. I'd prefer a solution that doesn't involve explicitly solving for a square root - is there a more abstract reason why $F(i)$ should be closed under taking square roots? We can explicitly solve $(a+bi)^2=c+di$ for $a$ and $b$ in terms of $c$ and $d$. We have $(a+bi)^2 = a^2-b^2+2abi$, yielding the system of equations $$a^2-b^2=c$$ and $$2ab=d.$$ Since every element of $F$ already has a square root in $F(i)$, we may assume $d \neq 0$, and substitute $b = d/(2a)$ into the first equation, which gives $a^2-d^2/(4a^2) = c$, or $4a^4-4ca^2-d^2=0$. Observing that the discriminant of this quadratic is positive, we can then see that $$a^2 := \frac{4c + \sqrt{16c^2+16d^2}}{8} \in F$$ is positive, because $16c^2+16d^2>(4c)^2$ (and by $\sqrt{\bullet}$ I mean the positive square root), and therefore has two square roots in $F$. Letting $a$ be such a square root, we set $b = d/2a$ and call it a day.","The Galois Theory proof involves proving that $F(i)$ is algebraically closed for any real closed field $F$ (where $i$ is a square root of $-1$), where we may define a real closed field as an ordered field in which every odd degree polynomial has a root and every positive number has a square root. The last step in the proof seems a bit ad-hoc though; it involves showing that $F(i)$ has no quadratic extension, which in turn involves showing that every element of $F(i)$ has a square root, which involves solving $(a+bi)^2=c+di$ for $a, b \in F$. Is there a more Galois-theoretic approach to the last step that I'm not aware of? Or, for that matter, some other deeper reason why the computation works out? Edit Here's the proof that I have that $F(i)$ is closed under square roots, to illustrate how it's ad-hoc. I'd prefer a solution that doesn't involve explicitly solving for a square root - is there a more abstract reason why $F(i)$ should be closed under taking square roots? We can explicitly solve $(a+bi)^2=c+di$ for $a$ and $b$ in terms of $c$ and $d$. We have $(a+bi)^2 = a^2-b^2+2abi$, yielding the system of equations $$a^2-b^2=c$$ and $$2ab=d.$$ Since every element of $F$ already has a square root in $F(i)$, we may assume $d \neq 0$, and substitute $b = d/(2a)$ into the first equation, which gives $a^2-d^2/(4a^2) = c$, or $4a^4-4ca^2-d^2=0$. Observing that the discriminant of this quadratic is positive, we can then see that $$a^2 := \frac{4c + \sqrt{16c^2+16d^2}}{8} \in F$$ is positive, because $16c^2+16d^2>(4c)^2$ (and by $\sqrt{\bullet}$ I mean the positive square root), and therefore has two square roots in $F$. Letting $a$ be such a square root, we set $b = d/2a$ and call it a day.",,"['abstract-algebra', 'galois-theory']"
92,Isomorphism from $\mathbb{C}[G]$ to $\prod_{i=1}^h M_{n_i}(\mathbb{C})$.,Isomorphism from  to .,\mathbb{C}[G] \prod_{i=1}^h M_{n_i}(\mathbb{C}),"What I want to ask is the proof of the Proposition 10. in ""Linear Representations of Finite Groups"" by Jean-Pierre Serre. Let $\rho_i : G \rightarrow GL(W_i)$ be the distinct irreducible representations of $G$.$(1 \le i \le h)$ We can extend $\rho_i$ to $\widetilde{\rho_i} : \mathbb{C}[G] \rightarrow \text{End}(W_i)$ by $\widetilde{\rho_i}(\sum_{g \in G} a_g g) = a_g \sum_{g \in G} \rho_i(g)$. Then $\widetilde{\rho} = (\widetilde{\rho_i}) : \mathbb{C}[G] \rightarrow \prod_{i=1}^{h} \text{End}(W_i) \cong \prod_{i=1}^h M_{n_i}(\mathbb{C})$ is a homomorphism. Then Proposition 10 states that it is an isomorphism.  Since dimensions of domain and codomain are same, it suffices to show that $\widetilde{\rho}$ is surjective. In the proof of the book, If $\widetilde{\rho}$ is not surjective, then there exists a nonzero linear form on $\prod M_{n_i}(\mathbb{C})$ vanishing on the image of $\widetilde{\rho}$. This gives a nontrivial relation on the coefficients of the representations $\rho_i$ which is impossible because of the orthogonality formulas of 2.2. But I can not follow the above three lines in the proof. I will appreciate it if you give an explanation for the proof. Thank you.","What I want to ask is the proof of the Proposition 10. in ""Linear Representations of Finite Groups"" by Jean-Pierre Serre. Let $\rho_i : G \rightarrow GL(W_i)$ be the distinct irreducible representations of $G$.$(1 \le i \le h)$ We can extend $\rho_i$ to $\widetilde{\rho_i} : \mathbb{C}[G] \rightarrow \text{End}(W_i)$ by $\widetilde{\rho_i}(\sum_{g \in G} a_g g) = a_g \sum_{g \in G} \rho_i(g)$. Then $\widetilde{\rho} = (\widetilde{\rho_i}) : \mathbb{C}[G] \rightarrow \prod_{i=1}^{h} \text{End}(W_i) \cong \prod_{i=1}^h M_{n_i}(\mathbb{C})$ is a homomorphism. Then Proposition 10 states that it is an isomorphism.  Since dimensions of domain and codomain are same, it suffices to show that $\widetilde{\rho}$ is surjective. In the proof of the book, If $\widetilde{\rho}$ is not surjective, then there exists a nonzero linear form on $\prod M_{n_i}(\mathbb{C})$ vanishing on the image of $\widetilde{\rho}$. This gives a nontrivial relation on the coefficients of the representations $\rho_i$ which is impossible because of the orthogonality formulas of 2.2. But I can not follow the above three lines in the proof. I will appreciate it if you give an explanation for the proof. Thank you.",,"['abstract-algebra', 'group-theory', 'representation-theory']"
93,The unique loop (quasigroup with unit) $L$ of order $5$ satisfying $x^2 = 1$ for all $x \in L$,The unique loop (quasigroup with unit)  of order  satisfying  for all,L 5 x^2 = 1 x \in L,"Recall that a quasigroup is a pair $(Q, \ast)$ , where $Q$ is a set and $\ast$ is a binary product $$\ast: Q \times Q \to Q$$ satisfying the Latin square property , namely that for all $x, y \in Q$ there is a unique $a \in Q$ such that $y = ax$ and a unique $b \in Q$ such that $y = x b$ , or equivalently, that the multiplication table of $\ast$ is a Latin square . A quasigroup $(Q, \ast)$ is a loop iff it has an identity element, that is an element $1$ such that $1\ast x = x = x\ast 1$ for all $x \in Q$ . This question asks about constructing a loop $(L, \ast)$ on a set $L$ of five elements (denote $L = \{1, a, b, c, d\}$ ) that satisfies the involution condition $x^2 = 1$ for all $x \in L$ . My answer there shows that there is only one such loop up to isomorphism; its multiplication table is: $$ \begin{array}{c|ccccc} \ast & 1 & a & b & c & d \\ \hline    1 & 1 & a & b & c & d \\    a & a & 1 & c & d & b \\    b & b & d & 1 & a & c \\    c & c & b & d & 1 & a \\    d & d & c & a & b & 1 \end{array}.$$ This operation is nonassociative, as $(ab)d = a \neq ac = a(bd)$ , but the fact that every element squares to $1$ implies that it is power-associative; in fact it turns out to be flexible. Even without the involution condition, this example is minimal in the sense that any loop of order $< 5$ is in fact a group. Up to isomorphism there are $6$ loops of order $5$ : This loop, the group $C_5$ , and $4$ other non-groups (the other $4$ are not even power-associative). So, given that this example is both minimal and unique, it's natural to ask: Is there a more informative/interesting way to view the loop structure $\ast$ on $L$ than via its multiplication table? That is, does it arise naturally in some other setting?","Recall that a quasigroup is a pair , where is a set and is a binary product satisfying the Latin square property , namely that for all there is a unique such that and a unique such that , or equivalently, that the multiplication table of is a Latin square . A quasigroup is a loop iff it has an identity element, that is an element such that for all . This question asks about constructing a loop on a set of five elements (denote ) that satisfies the involution condition for all . My answer there shows that there is only one such loop up to isomorphism; its multiplication table is: This operation is nonassociative, as , but the fact that every element squares to implies that it is power-associative; in fact it turns out to be flexible. Even without the involution condition, this example is minimal in the sense that any loop of order is in fact a group. Up to isomorphism there are loops of order : This loop, the group , and other non-groups (the other are not even power-associative). So, given that this example is both minimal and unique, it's natural to ask: Is there a more informative/interesting way to view the loop structure on than via its multiplication table? That is, does it arise naturally in some other setting?","(Q, \ast) Q \ast \ast: Q \times Q \to Q x, y \in Q a \in Q y = ax b \in Q y = x b \ast (Q, \ast) 1 1\ast x = x = x\ast 1 x \in Q (L, \ast) L L = \{1, a, b, c, d\} x^2 = 1 x \in L 
\begin{array}{c|ccccc}
\ast & 1 & a & b & c & d \\
\hline
   1 & 1 & a & b & c & d \\
   a & a & 1 & c & d & b \\
   b & b & d & 1 & a & c \\
   c & c & b & d & 1 & a \\
   d & d & c & a & b & 1
\end{array}. (ab)d = a \neq ac = a(bd) 1 < 5 6 5 C_5 4 4 \ast L","['abstract-algebra', 'quasigroups']"
94,"Number of elements in the quotient ring $\mathbb{Z}[X]/(X^2-3, 2X+4)$",Number of elements in the quotient ring,"\mathbb{Z}[X]/(X^2-3, 2X+4)","I had to calculate the number of elements of this quotient ring:   $$R = \mathbb{Z}[X]/(X^2-3, 2X+4).$$ This is what I've got by myself and by using an internet source: Writing the ring $R = \mathbb{Z}[X]/(X^2−3, 2X+4)$ as $\mathbb{Z}[X]/I$, we note that $I $ contains $2(X^2−3)−(X−2)(2X+4) = 2$. We then note that the generator $2X + 4$ is actually superfluous since $2X + 4 = 2(X + 2)$. Now we can write $R = \mathbb{Z}[X]/(X^2 − 3, 2) = \mathbb{Z}[x]/((X+1)^2, 2)$, because $(X+1)^2=X^2+2X+1=X^2-3+2X+4$. The internet source states now the following: $$ R \cong (\mathbb{Z}/2\mathbb{Z})[\alpha] \quad \text{with} \quad \alpha = X+1$$ I guess that $\mathbb{Z}/2\mathbb{Z}[\alpha]$ represents the set of dual numbers of the field $\mathbb{Z}/2\mathbb{Z}$. I see that $\alpha^2=0$, but what exactly implies the isomorphism? And does this mean that the quotient ring $R$ contains four elements? If necessary, you can take at the site I used. (It's about page 5, exercise 4.3a.) http://www.math.umn.edu/~musiker/5286H/Sol1.pdf I thank you in advance for your answers.","I had to calculate the number of elements of this quotient ring:   $$R = \mathbb{Z}[X]/(X^2-3, 2X+4).$$ This is what I've got by myself and by using an internet source: Writing the ring $R = \mathbb{Z}[X]/(X^2−3, 2X+4)$ as $\mathbb{Z}[X]/I$, we note that $I $ contains $2(X^2−3)−(X−2)(2X+4) = 2$. We then note that the generator $2X + 4$ is actually superfluous since $2X + 4 = 2(X + 2)$. Now we can write $R = \mathbb{Z}[X]/(X^2 − 3, 2) = \mathbb{Z}[x]/((X+1)^2, 2)$, because $(X+1)^2=X^2+2X+1=X^2-3+2X+4$. The internet source states now the following: $$ R \cong (\mathbb{Z}/2\mathbb{Z})[\alpha] \quad \text{with} \quad \alpha = X+1$$ I guess that $\mathbb{Z}/2\mathbb{Z}[\alpha]$ represents the set of dual numbers of the field $\mathbb{Z}/2\mathbb{Z}$. I see that $\alpha^2=0$, but what exactly implies the isomorphism? And does this mean that the quotient ring $R$ contains four elements? If necessary, you can take at the site I used. (It's about page 5, exercise 4.3a.) http://www.math.umn.edu/~musiker/5286H/Sol1.pdf I thank you in advance for your answers.",,"['abstract-algebra', 'ring-theory']"
95,Is $\mathbb{Q}\otimes_\mathbb{Z}\mathbb{Z}/n=0$?,Is ?,\mathbb{Q}\otimes_\mathbb{Z}\mathbb{Z}/n=0,Is $\mathbb{Q}\otimes_\mathbb{Z}\mathbb{Z}/n=0$? Because  \begin{equation}\frac{a}{b}\otimes_\mathbb{Z}1=\frac{na}{nb}\otimes_\mathbb{Z}1=\frac{a}{nb}\otimes_{\mathbb{Z}}n=\frac{a}{nb}\otimes_\mathbb{Z}0=0?\end{equation},Is $\mathbb{Q}\otimes_\mathbb{Z}\mathbb{Z}/n=0$? Because  \begin{equation}\frac{a}{b}\otimes_\mathbb{Z}1=\frac{na}{nb}\otimes_\mathbb{Z}1=\frac{a}{nb}\otimes_{\mathbb{Z}}n=\frac{a}{nb}\otimes_\mathbb{Z}0=0?\end{equation},,['abstract-algebra']
96,Isomorphic Hilbert spaces,Isomorphic Hilbert spaces,,"As part of a broader proof , I need to show that every two separable Hilbert spaces (that contains a dense countable set) are isomorphic (the linear mapping from one space to the other is injective and isometric if I say right). I'd be happy to get any help on this.","As part of a broader proof , I need to show that every two separable Hilbert spaces (that contains a dense countable set) are isomorphic (the linear mapping from one space to the other is injective and isometric if I say right). I'd be happy to get any help on this.",,"['functional-analysis', 'hilbert-spaces', 'abstract-algebra']"
97,"Nonempty subset H of group G is subgroup iff $ab^{-1} \in H $ for any $a,b \in H$",Nonempty subset H of group G is subgroup iff  for any,"ab^{-1} \in H  a,b \in H","Let $G$ be a group. Show that a nonempty subset $H$ is a subgroup of $G$ if any only if $ab^{-1} \in H $ for any $a,b \in H$. The forward direction is quite easy. Suppose $H$ is a subgroup. Then by closure, $ab \in H$ for any $a,b \in H$. Every element has an inverse. Hence, if $b \in H $, then $b^{-1} \in H$. Hence, by closure again, $ab^{-1} \in H$. Backward direction, suppose $ab^{-1} \in H $ for any $a,b \in H$. Let $a=b$. Then we have $bb^{-1}=1_H\in H$. Let $a=1_H$ , we have $1_Hb^{-1}=b^{-1} \in H$. I don't know how to show closure. Reference: Fraleigh p. 58  Question 5.45 in A First Course in Abstract Algebra","Let $G$ be a group. Show that a nonempty subset $H$ is a subgroup of $G$ if any only if $ab^{-1} \in H $ for any $a,b \in H$. The forward direction is quite easy. Suppose $H$ is a subgroup. Then by closure, $ab \in H$ for any $a,b \in H$. Every element has an inverse. Hence, if $b \in H $, then $b^{-1} \in H$. Hence, by closure again, $ab^{-1} \in H$. Backward direction, suppose $ab^{-1} \in H $ for any $a,b \in H$. Let $a=b$. Then we have $bb^{-1}=1_H\in H$. Let $a=1_H$ , we have $1_Hb^{-1}=b^{-1} \in H$. I don't know how to show closure. Reference: Fraleigh p. 58  Question 5.45 in A First Course in Abstract Algebra",,"['abstract-algebra', 'group-theory']"
98,Center of Weyl algebra over a field of characterstic $0$?,Center of Weyl algebra over a field of characterstic ?,0,"I was doing some reading on ring theory, and there's something mentioned in a footnote I'd like to clarify. Suppose you have some vector space $V$ of even finite dimension, and some nondegenerate, alternating bilinear form $g$ . I'll use $W_g(V)$ to denote the corresponding Weyl algebra. Now if the underlying field $F$ is such that $\mathrm{char}(F)=0$ , then why does the center of the Weyl algebra coincide with $F$ ? I'd appreciate any proof or reference to a proof I could read, as I could not find or come up with one. Cheers!","I was doing some reading on ring theory, and there's something mentioned in a footnote I'd like to clarify. Suppose you have some vector space of even finite dimension, and some nondegenerate, alternating bilinear form . I'll use to denote the corresponding Weyl algebra. Now if the underlying field is such that , then why does the center of the Weyl algebra coincide with ? I'd appreciate any proof or reference to a proof I could read, as I could not find or come up with one. Cheers!",V g W_g(V) F \mathrm{char}(F)=0 F,"['abstract-algebra', 'reference-request', 'noncommutative-algebra']"
99,Galois over Galois,Galois over Galois,,"I am working on this exercise: If $E$ is an intermediate ﬁeld of an extension $F/K$ of ﬁelds. Suppose $F/E$ and $E/K$ are Galois extensions, and every $\sigma\in Gal(E/K)$ is extendible to an automorphism of $F$, then show that $F/K$ is Galois. I can see that any $\sigma$ extended over $F$ fixes elements in $K$ but not in $E-K$. But how to show it doesn't fix elements in $F-E$? Hints only please, this is homework. p.s. we use Kaplansky, he doesn't require Galois extensions to be finite dimensional.","I am working on this exercise: If $E$ is an intermediate ﬁeld of an extension $F/K$ of ﬁelds. Suppose $F/E$ and $E/K$ are Galois extensions, and every $\sigma\in Gal(E/K)$ is extendible to an automorphism of $F$, then show that $F/K$ is Galois. I can see that any $\sigma$ extended over $F$ fixes elements in $K$ but not in $E-K$. But how to show it doesn't fix elements in $F-E$? Hints only please, this is homework. p.s. we use Kaplansky, he doesn't require Galois extensions to be finite dimensional.",,"['abstract-algebra', 'galois-theory']"
