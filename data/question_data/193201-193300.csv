,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Bohr Radius of an atom,Bohr Radius of an atom,,"This is the question I am working on: The Bohr radius, $a_0$ , of the hydrogen atom is the value of $r$ that minimizes the energy $E(r)=\frac{h^2}{2mr^2} - \frac{e^2}{4\pi\epsilon_0r}$ where $h, m,e$ , and $\epsilon_0$ are physical constants. Show that $a_0=\frac{4\pi\epsilon_0h^2}{me^2}$ I have tried to plug in $a_0$ for the $r$ value and take the derivative of $E(r)$ but that didn't seem to be getting me any closer to the answer I needed. Any help with this question would be greatly appreciated. Thank you!","This is the question I am working on: The Bohr radius, , of the hydrogen atom is the value of that minimizes the energy where , and are physical constants. Show that I have tried to plug in for the value and take the derivative of but that didn't seem to be getting me any closer to the answer I needed. Any help with this question would be greatly appreciated. Thank you!","a_0 r E(r)=\frac{h^2}{2mr^2} - \frac{e^2}{4\pi\epsilon_0r} h, m,e \epsilon_0 a_0=\frac{4\pi\epsilon_0h^2}{me^2} a_0 r E(r)","['calculus', 'derivatives', 'physics']"
1,Is the reverse of the first derivative test statement true?,Is the reverse of the first derivative test statement true?,,"The first derivative test says that if the derivative of a function is positive/negative in the open interval $(x-a,x)$ and negative/positive in the open interval $(x,x+b)$ where both $a$ and $b$ are greater than zero, then there is a local maxima/minima at $x$ . Is the reverse of this statement true ? For example, if $x$ is a local maxima of the function $f(x)$ and if the function $f(x)$ is continuous and differentiable in $(a,b)$ where $a < x < b$ , then there exists an interval to the left of $x$ , howsoever small, in which the function has a positive slope and an interval to the right of $x$ , howsoever small, in which the function has a negative slope.","The first derivative test says that if the derivative of a function is positive/negative in the open interval and negative/positive in the open interval where both and are greater than zero, then there is a local maxima/minima at . Is the reverse of this statement true ? For example, if is a local maxima of the function and if the function is continuous and differentiable in where , then there exists an interval to the left of , howsoever small, in which the function has a positive slope and an interval to the right of , howsoever small, in which the function has a negative slope.","(x-a,x) (x,x+b) a b x x f(x) f(x) (a,b) a < x < b x x","['calculus', 'derivatives', 'maxima-minima']"
2,Prove an inequality by using the integral test or another way,Prove an inequality by using the integral test or another way,,"‎Let the function ‎‎ $‎g:‎\mathbb{R^+}‎‎‎\rightarrow‎‎\mathbb{R^+}‎$ ‎have ‎the ‎properties ‎that‎‎ for each ‎ $‎w>0‎$ ‎, ‎ $$ \lim_{x\to\infty}‎\frac{g(x+w)}{g(x)} = 1‎ $$ and $\log g(x)‎$ ‎is ‎concave‎. ‎Now, since ‎ $‎\log g‎$ ‎is ‎concave, ‎so ‎it ‎has ‎derivative‎ $$ \big(\log g(x)\big)^\prime=\frac{g^\prime_-(x)+g^\prime_+(x)}{2g(x)} $$ except, possibly, on a countable set,  where ‎ $‎g^\prime_{+}(x‏)‎$ ‎and ‎‎ $‎g^\prime_{-}(x)‎$ ‎are ‎right ‎and ‎left ‎derivatives, ‎respectively. Also, since $$ ‎‎‎‎\displaystyle{\lim_{x\to\infty}}‎\frac{g(x+w)}{g(x)} = 1‎ $$ for each ‎ $‎w>0‎$ and $\log g(x)‎$ ‎is ‎concave, ‎then ‎‎ $‎g‎$ ‎is ‎increasing. ‎Now, ‎‎‎‎‎‎‎‎‎my ‎questions ‎is:‎‎ ‎‏‎ How should I prove the following inequality?‎ $$ 0\leq\gamma_g+\log g(1)\le \frac{g^\prime_-(1)+g^\prime_+(1)}{2g(1)}, ‎$$ where $$ \sum_{i=1}^n \frac{g^\prime_-(i)+g^\prime_+(i)}{2g(i)}\to \gamma_g\text{ as }n\to\infty. $$ Hint . Note that we can using the proof of the integral test for the convergence of infinite series, for the proof inequality (but I can't prove it).","‎Let the function ‎‎ ‎have ‎the ‎properties ‎that‎‎ for each ‎ ‎, ‎ and ‎is ‎concave‎. ‎Now, since ‎ ‎is ‎concave, ‎so ‎it ‎has ‎derivative‎ except, possibly, on a countable set,  where ‎ ‎and ‎‎ ‎are ‎right ‎and ‎left ‎derivatives, ‎respectively. Also, since for each ‎ and ‎is ‎concave, ‎then ‎‎ ‎is ‎increasing. ‎Now, ‎‎‎‎‎‎‎‎‎my ‎questions ‎is:‎‎ ‎‏‎ How should I prove the following inequality?‎ where Hint . Note that we can using the proof of the integral test for the convergence of infinite series, for the proof inequality (but I can't prove it).","‎g:‎\mathbb{R^+}‎‎‎\rightarrow‎‎\mathbb{R^+}‎ ‎w>0‎ 
\lim_{x\to\infty}‎\frac{g(x+w)}{g(x)} = 1‎
 \log g(x)‎ ‎\log g‎ 
\big(\log g(x)\big)^\prime=\frac{g^\prime_-(x)+g^\prime_+(x)}{2g(x)}
 ‎g^\prime_{+}(x‏)‎ ‎g^\prime_{-}(x)‎ 
‎‎‎‎\displaystyle{\lim_{x\to\infty}}‎\frac{g(x+w)}{g(x)} = 1‎
 ‎w>0‎ \log g(x)‎ ‎g‎ 
0\leq\gamma_g+\log g(1)\le \frac{g^\prime_-(1)+g^\prime_+(1)}{2g(1)},
‎ 
\sum_{i=1}^n \frac{g^\prime_-(i)+g^\prime_+(i)}{2g(i)}\to \gamma_g\text{ as }n\to\infty.
","['real-analysis', 'sequences-and-series', 'derivatives', 'inequality', 'convex-analysis']"
3,Is it possible to find a continous but nowhere differentiable function $f:E \to E$ such that $|f-h| < \epsilon$ on $E$?,Is it possible to find a continous but nowhere differentiable function  such that  on ?,f:E \to E |f-h| < \epsilon E,"Let $h:E \to E$ be a continous function (where $E:=[0,1]$ ) and $\epsilon >0$ . Is it possible to find a continous but nowhere differentiable function $f:E \to E$ such that $|f-h| < \epsilon$ on $E$ ? Note: For the purposes of this question, it only makes sense to talk about differentiability on interior points of the domain. I know some people define one-sided ""derivatives"" at endpoints, but I do not. WLOG, we can take $\epsilon< \frac 1 {100}$ . I already know that $v:E \to E$ by $v(x) = \sum_{i=1} ^ \infty 4^{-i} \phi(4^ix)$ is a continous but nowhere differentiable function. (Where $\phi$ is the zig-zag function with period $4$ which agrees with $|x|$ on $[-2,2]$ .) One of my first thoughts was to take $f(x)= \frac \epsilon 2 v(x)+h(x)$ . This will work if $h$ is differentiable on $(a,b)$ and if the range stays small enough, but I don't think it works in general. [Look what happens when $h(x)=1- \frac \epsilon 2v(x).$ ] Can we construct a continous but nowhere differentiable function which stays close enough to $h$ ? (Ideally, constructed from $v$ ?)","Let be a continous function (where ) and . Is it possible to find a continous but nowhere differentiable function such that on ? Note: For the purposes of this question, it only makes sense to talk about differentiability on interior points of the domain. I know some people define one-sided ""derivatives"" at endpoints, but I do not. WLOG, we can take . I already know that by is a continous but nowhere differentiable function. (Where is the zig-zag function with period which agrees with on .) One of my first thoughts was to take . This will work if is differentiable on and if the range stays small enough, but I don't think it works in general. [Look what happens when ] Can we construct a continous but nowhere differentiable function which stays close enough to ? (Ideally, constructed from ?)","h:E \to E E:=[0,1] \epsilon >0 f:E \to E |f-h| < \epsilon E \epsilon< \frac 1 {100} v:E \to E v(x) = \sum_{i=1} ^ \infty 4^{-i} \phi(4^ix) \phi 4 |x| [-2,2] f(x)= \frac \epsilon 2 v(x)+h(x) h (a,b) h(x)=1- \frac \epsilon 2v(x). h v","['real-analysis', 'derivatives', 'continuity']"
4,How to finish the derivative calculation?,How to finish the derivative calculation?,,"I know that this can be calculated as: $x^n=n \cdot x^{n-1}$ but I need to find a solution as a limit: $f(x)=\sqrt[3]{x} ;f^{'}(x)=\lim _{\Delta x\to  0}\frac{\sqrt[3]{x_0+\Delta x} -\sqrt[3]{x_0}}{\Delta x}=\lim  _{\Delta x\to 0}\frac{(\sqrt[3]{x_0+\Delta x} -\sqrt[3]{x_0}  )(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}{\Delta  x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=\lim _{\Delta x\to  0}\frac{(\sqrt[3]{x_0+\Delta x})^{2}-(\sqrt[3]{x_0})^{2}}{\Delta  x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=\lim _{\Delta x\to  0}\frac{(\sqrt[3]{x_0+\Delta x})^{2}-(\sqrt[3]{x_0})^{2}}{\Delta  x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=...$ I don't know how to calculate further due to: $(\sqrt[3]{x_{0}+\Delta x})^{2}-(\sqrt[3]{x_{0}})^{2}$ . If instead it would be: $(x_{0}+\Delta x)^{2}-(x_{0})^{2}$ , then I would write $(x_{0}+\Delta x)^{2}$ as: $(x_{0})^{2}+2 \cdot x_{0} \cdot \Delta x +(\Delta x)^{2}$ , but i don't know how to be with cube root ?","I know that this can be calculated as: but I need to find a solution as a limit: I don't know how to calculate further due to: . If instead it would be: , then I would write as: , but i don't know how to be with cube root ?","x^n=n \cdot x^{n-1} f(x)=\sqrt[3]{x} ;f^{'}(x)=\lim _{\Delta x\to 
0}\frac{\sqrt[3]{x_0+\Delta x} -\sqrt[3]{x_0}}{\Delta x}=\lim 
_{\Delta x\to 0}\frac{(\sqrt[3]{x_0+\Delta x} -\sqrt[3]{x_0} 
)(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}{\Delta 
x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=\lim _{\Delta x\to 
0}\frac{(\sqrt[3]{x_0+\Delta x})^{2}-(\sqrt[3]{x_0})^{2}}{\Delta 
x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=\lim _{\Delta x\to 
0}\frac{(\sqrt[3]{x_0+\Delta x})^{2}-(\sqrt[3]{x_0})^{2}}{\Delta 
x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=... (\sqrt[3]{x_{0}+\Delta x})^{2}-(\sqrt[3]{x_{0}})^{2} (x_{0}+\Delta x)^{2}-(x_{0})^{2} (x_{0}+\Delta x)^{2} (x_{0})^{2}+2 \cdot x_{0} \cdot \Delta x +(\Delta x)^{2}",['derivatives']
5,Why is $x^2\sin(1/x)$ not strictly differentiable?,Why is  not strictly differentiable?,x^2\sin(1/x),"I am on Wikipedia reading on strict differentiability and I don't particularly understand the example proving a function that is differentiable does not have to be strictly differentiable, where $f(x)=x^2 \sin(1/x)$ , $f(0)=0$ . How can we show this is differentiable directly using difference quotients. Also then how can we show it's not strictly differentiable using our difference quotients? The simplest setting in which strict differentiability can be considered, is that of a real-valued function defined on an interval $I$ of the real line. The function $f: I \rightarrow \mathbf{R}$ is said strictly differentiable in a point $a \in I$ if $$ \lim _{(x, y) \rightarrow(a, a)} \frac{f(x)-f(y)}{x-y} $$ exists, where $(x, y) \rightarrow(a, a)$ is to be considered as limit in $\mathbf{R}^{2}$ , and of course requiring $x \neq y$ . A strictly differentiable function is obviously differentiable, but the converse is wrong, as can be seen from the counter-example $f(x)=x^{2}$ sin $\frac{1}{x}, f(0)=0, x_{n}=\frac{1}{\left(n+\frac{1}{2}\right) \pi}, y_{n}=x_{n+1}$ . One has however the equivalence of strict differentiability on an interval $I$ , and being of differentiability class $C^{1}(I)$ . (Transcribed from Wikipedia screenshot) so far I have; To show $f(x)$ is differentiable, I would assume I'd use the statement that if $x^2$ is differentiable and $\sin(1/x)$ is differentiable (which can be shown by the difference quotients, then $x^2 \sin(1/x)$ is also differentiable?","I am on Wikipedia reading on strict differentiability and I don't particularly understand the example proving a function that is differentiable does not have to be strictly differentiable, where , . How can we show this is differentiable directly using difference quotients. Also then how can we show it's not strictly differentiable using our difference quotients? The simplest setting in which strict differentiability can be considered, is that of a real-valued function defined on an interval of the real line. The function is said strictly differentiable in a point if exists, where is to be considered as limit in , and of course requiring . A strictly differentiable function is obviously differentiable, but the converse is wrong, as can be seen from the counter-example sin . One has however the equivalence of strict differentiability on an interval , and being of differentiability class . (Transcribed from Wikipedia screenshot) so far I have; To show is differentiable, I would assume I'd use the statement that if is differentiable and is differentiable (which can be shown by the difference quotients, then is also differentiable?","f(x)=x^2 \sin(1/x) f(0)=0 I f: I \rightarrow \mathbf{R} a \in I 
\lim _{(x, y) \rightarrow(a, a)} \frac{f(x)-f(y)}{x-y}
 (x, y) \rightarrow(a, a) \mathbf{R}^{2} x \neq y f(x)=x^{2} \frac{1}{x}, f(0)=0, x_{n}=\frac{1}{\left(n+\frac{1}{2}\right) \pi}, y_{n}=x_{n+1} I C^{1}(I) f(x) x^2 \sin(1/x) x^2 \sin(1/x)","['real-analysis', 'limits', 'derivatives']"
6,"If $f(x)=f(1/x)$, is it possible for $f'(x)=f'(1/x)$?","If , is it possible for ?",f(x)=f(1/x) f'(x)=f'(1/x),"If $f:\mathbb{R} \to \mathbb{R}$ such that $f(x) = f(\frac{1}{x})$ for all $x \neq0$ and $$\lim_{x \to \infty}f(x) = \lim_{x \to -\infty}f(x) = f(0),$$ then we say $f$ is an inverse reflective function. All inverse reflective functions are completely described on $[-1,1]$ , which is the most intriguing feature I can think of. One example is $$g(x) = \frac{x}{x^2+1}.$$ Let $$D := \{f \mid f\text{ is inverse reflective and $f$ is differetiable everywhere}\}.$$ Question: Does there exists a non-constant $f \in D$ such that $f'$ is inverse reflective? I personally believe the answer is no, but my belief is not based on any mathematical deductions.","If such that for all and then we say is an inverse reflective function. All inverse reflective functions are completely described on , which is the most intriguing feature I can think of. One example is Let Question: Does there exists a non-constant such that is inverse reflective? I personally believe the answer is no, but my belief is not based on any mathematical deductions.","f:\mathbb{R} \to \mathbb{R} f(x) = f(\frac{1}{x}) x \neq0 \lim_{x \to \infty}f(x) = \lim_{x \to -\infty}f(x) = f(0), f [-1,1] g(x) = \frac{x}{x^2+1}. D := \{f \mid f\text{ is inverse reflective and f is differetiable everywhere}\}. f \in D f'","['real-analysis', 'derivatives']"
7,Find $f'(x)$ given that $f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)d\theta$,Find  given that,f'(x) f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)d\theta,"Find $f'(x)$ given that $f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)\,d\theta$ , where $|x| < 1$ Recently I asked a question that involved the theory in dealing with a question like this: Find $f'$ for the function $f(x,y) = \int_{a}^{x + y}g$ Using what I got from that question I went about solving this expression: $$\text{Define} \ F(u) = \int_{0}^{\frac{\pi}{2}}g(\theta)\,d\theta,\ \text{where}\ g(\theta) = \log(1 - x^{2}\cos^{2}\theta)\ \text{where}\ F:\mathbb{R} \to \mathbb{R} \\ \text{Define}\  u(x) = \frac{\pi}{2}, \ \text{where}\ u:\mathbb{R} \to \mathbb{R}$$ Using the ideas covered previously: $$(F \circ u)(x) = F(u(x)) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)\,d\theta $$ THerefore: $$D(F \circ u)(x) =  DF(u(x)) \cdot Du(x) \\ \text{where}\ DF(u(x)) = \log(1-x^{2}\cos^{2}\theta) \\ \text{and}\ Du(x) = 0 \\ \therefore DF(u(x)) \cdot Du(x) = 0 $$ And I know/feel this is not right. I feel I'm supposed to treat the things within the log function in some way, but according to how I was looking at them in my previous question they are all dummy variables with no importance. Where am I going wrong in my examination of the expression ?","Find given that , where Recently I asked a question that involved the theory in dealing with a question like this: Find $f'$ for the function $f(x,y) = \int_{a}^{x + y}g$ Using what I got from that question I went about solving this expression: Using the ideas covered previously: THerefore: And I know/feel this is not right. I feel I'm supposed to treat the things within the log function in some way, but according to how I was looking at them in my previous question they are all dummy variables with no importance. Where am I going wrong in my examination of the expression ?","f'(x) f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)\,d\theta |x| < 1 \text{Define} \ F(u) = \int_{0}^{\frac{\pi}{2}}g(\theta)\,d\theta,\ \text{where}\ g(\theta) = \log(1 - x^{2}\cos^{2}\theta)\ \text{where}\ F:\mathbb{R} \to \mathbb{R} \\ \text{Define}\  u(x) = \frac{\pi}{2}, \ \text{where}\ u:\mathbb{R} \to \mathbb{R} (F \circ u)(x) = F(u(x)) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)\,d\theta  D(F \circ u)(x) =  DF(u(x)) \cdot Du(x) \\
\text{where}\ DF(u(x)) = \log(1-x^{2}\cos^{2}\theta) \\ \text{and}\ Du(x) = 0 \\ \therefore DF(u(x)) \cdot Du(x) = 0 ","['real-analysis', 'calculus', 'integration', 'derivatives']"
8,Uniform bound on derivatives and uniform convergence,Uniform bound on derivatives and uniform convergence,,"If $f_n:[0,1]\to \mathbb{R}$ are differentiable, $|f_n'(x)|\leq C$ for all $n\in\mathbb{N}$ and $x\in [0,1]$ , $f_n\to f$ uniformly, $f_n'(x)\to g(x)$ pointwise and $f$ is differentiable, can we conclude that $f'=g$ ? Equivalently, can we conclude that $$ \lim_{n\to \infty}\lim_{h\to 0}\frac{f_n(x+h)-f(x)}{h}=\lim_{h\to 0}\lim_{n\to \infty}\frac{f_n(x+h)-f(x)}{h} $$ given the assumptions above? My guess is no, but I am unsure of a counter-example.","If are differentiable, for all and , uniformly, pointwise and is differentiable, can we conclude that ? Equivalently, can we conclude that given the assumptions above? My guess is no, but I am unsure of a counter-example.","f_n:[0,1]\to \mathbb{R} |f_n'(x)|\leq C n\in\mathbb{N} x\in [0,1] f_n\to f f_n'(x)\to g(x) f f'=g 
\lim_{n\to \infty}\lim_{h\to 0}\frac{f_n(x+h)-f(x)}{h}=\lim_{h\to 0}\lim_{n\to \infty}\frac{f_n(x+h)-f(x)}{h}
","['real-analysis', 'limits', 'derivatives', 'convergence-divergence', 'uniform-convergence']"
9,Derivatives Problem: Why is second term's coefficient less than zero?,Derivatives Problem: Why is second term's coefficient less than zero?,,"Problem: If $f(x)=cx^2+dx+e$ for the function shown in the graph, then what values can $c$ , $d$ , and $e$ take on? The answer is $c<0$ , $d<0$ , $e>0$ . I just don't understand why $d<0$ . Since the slope of $f(x)$ is always negative, $f'(x)<0$ and since $f'(x)=(2c)x+d$ , $(2c)x+d<0$ . Since $c<0$ , let's choose $c=-1$ , and let's choose $x=3$ . $(2)(-1)(3)+d<0$ which leads to $-6+d<0$ which leads to $d<6$ . And the maximum value (in this case 6) of this open inequality can change based on what $c$ and $x$ values are chosen. So where am I wrong?","Problem: If for the function shown in the graph, then what values can , , and take on? The answer is , , . I just don't understand why . Since the slope of is always negative, and since , . Since , let's choose , and let's choose . which leads to which leads to . And the maximum value (in this case 6) of this open inequality can change based on what and values are chosen. So where am I wrong?",f(x)=cx^2+dx+e c d e c<0 d<0 e>0 d<0 f(x) f'(x)<0 f'(x)=(2c)x+d (2c)x+d<0 c<0 c=-1 x=3 (2)(-1)(3)+d<0 -6+d<0 d<6 c x,['calculus']
10,What is the use or importance of continuity and differentiability? [closed],What is the use or importance of continuity and differentiability? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question In a lot of mathematical proofs I often see things like ""Assume $f$ is continuous"" or ""Assume $f$ is differentiable"" and sometimes I've even seen both ""Assume $f$ is continuous and differentiable"", though I believe (differentiability implies/requires continuity). What is the use or utility of this? Under what kind of circumstances, going into a problem or framework, would we want something to be continuous or differentiable? Continuous I can understand as a kind of ""useful for things that don't have sudden jumps out of nowhere"" but when would we want something to be differentiable as well? For instance when reading about lots of probability curves I often see that these curves are defined up front as both continuous and differentiable. Why? What pushes us to start off with these definitions? What's the motivation? What do we ""lose"" if we do away with these assumptions?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question In a lot of mathematical proofs I often see things like ""Assume is continuous"" or ""Assume is differentiable"" and sometimes I've even seen both ""Assume is continuous and differentiable"", though I believe (differentiability implies/requires continuity). What is the use or utility of this? Under what kind of circumstances, going into a problem or framework, would we want something to be continuous or differentiable? Continuous I can understand as a kind of ""useful for things that don't have sudden jumps out of nowhere"" but when would we want something to be differentiable as well? For instance when reading about lots of probability curves I often see that these curves are defined up front as both continuous and differentiable. Why? What pushes us to start off with these definitions? What's the motivation? What do we ""lose"" if we do away with these assumptions?",f f f,"['real-analysis', 'derivatives', 'continuity', 'soft-question', 'definition']"
11,A Lipschitz function is $C^1$?,A Lipschitz function is ?,C^1,"I am wondering if a Lipschitz function $f:[a,b]\to\mathbb{R}$ is $C^1$ , that is its derivative is also continuous? I have seen that in a text however I could not prove it and does not seem so obvious for me! Any suggestion?","I am wondering if a Lipschitz function is , that is its derivative is also continuous? I have seen that in a text however I could not prove it and does not seem so obvious for me! Any suggestion?","f:[a,b]\to\mathbb{R} C^1","['real-analysis', 'calculus', 'derivatives', 'continuity', 'lipschitz-functions']"
12,Why do the derivations of $\ln(\frac{a}{b})$ and $\ln(a) - \ln(b)$ yield different results,Why do the derivations of  and  yield different results,\ln(\frac{a}{b}) \ln(a) - \ln(b),"So I was doing some exercises and I noticed that for one example which was of the form $$\ln( \frac{a}{b} )$$ (with a and b being some term with x), that I was getting a different result for taking the derivation when using the logarithmic rule $$\ln( \frac{a}{b} ) = \ln(a) - \ln(b)$$ before deriving  versus applying chain and quotient rules right away. I tried this with some other examples of that form too and I always ended up getting different results, but I have no idea what would cause this to happen. One of the examples I tried would be $$\ln( \frac{4+x}{4-x} )$$ which yields $$\frac{-8+8x}{(4+x)^3}$$ when applying chain and quotient rules right away and $$\frac{8}{16-x^2}$$ when using $$\ln( \frac{a}{b} ) = \ln(a) - \ln(b)$$ before deriving. Would really appreciate some help, thanks in advance.","So I was doing some exercises and I noticed that for one example which was of the form (with a and b being some term with x), that I was getting a different result for taking the derivation when using the logarithmic rule before deriving  versus applying chain and quotient rules right away. I tried this with some other examples of that form too and I always ended up getting different results, but I have no idea what would cause this to happen. One of the examples I tried would be which yields when applying chain and quotient rules right away and when using before deriving. Would really appreciate some help, thanks in advance.",\ln( \frac{a}{b} ) \ln( \frac{a}{b} ) = \ln(a) - \ln(b) \ln( \frac{4+x}{4-x} ) \frac{-8+8x}{(4+x)^3} \frac{8}{16-x^2} \ln( \frac{a}{b} ) = \ln(a) - \ln(b),"['analysis', 'derivatives', 'logarithms']"
13,"Are $C^\infty$ Functions with all derivatives positive on [a,$\infty$),a$\gt$0 always made of exponential?","Are  Functions with all derivatives positive on [a,),a0 always made of exponential?",C^\infty \infty \gt,"Are there any $C^\infty$ real functions except the exponential family and gamma function family which has all the derivatives of same sign on an interval [a, $\infty$ ) with a $\gt$ 0 ? I speculate the function is always uses exponential as building blocks and it is unique defining property of exponential functions. Please provide some instances otherwise. I have not been able to find any so far.","Are there any real functions except the exponential family and gamma function family which has all the derivatives of same sign on an interval [a, ) with a 0 ? I speculate the function is always uses exponential as building blocks and it is unique defining property of exponential functions. Please provide some instances otherwise. I have not been able to find any so far.",C^\infty \infty \gt,"['derivatives', 'taylor-expansion', 'exponential-function', 'analytic-functions']"
14,Find number of zeros $ x = \ln |x-a| $,Find number of zeros, x = \ln |x-a| ,"Find number of zeros $$ x = \ln |x-a| $$ depending on the value of $a$ . My try Usually I solve task like that in use of derivatives: Let $$f(x) = x - \ln |x-a| $$ Ok now it is time for derivatives and check what is happening Assume that $f'(x)$ exists... $$f'(x) = 1 - \frac{(|x-a|)'}{|x-a|} $$ Okay, I see that for $$|x-a| = 0 \rightarrow x=a$$ $f'(x)$ doesn't exists but it is not weird because $f(x)$ doesn't exists too. Now I can check different cases for $x<a$ , $x>a+1$ and $x \le a+1$ - then I can see behavior of $f$ depending on the value of $a$ . But there is some cases to check. I wonder if there is a simpler solution, maybe ""tricky"" one?","Find number of zeros depending on the value of . My try Usually I solve task like that in use of derivatives: Let Ok now it is time for derivatives and check what is happening Assume that exists... Okay, I see that for doesn't exists but it is not weird because doesn't exists too. Now I can check different cases for , and - then I can see behavior of depending on the value of . But there is some cases to check. I wonder if there is a simpler solution, maybe ""tricky"" one?", x = \ln |x-a|  a f(x) = x - \ln |x-a|  f'(x) f'(x) = 1 - \frac{(|x-a|)'}{|x-a|}  |x-a| = 0 \rightarrow x=a f'(x) f(x) x<a x>a+1 x \le a+1 f a,"['real-analysis', 'derivatives']"
15,"Can one modify $1 - e^{-\frac{1}{x^2}}$ so that all derivatives at $x=0$ are $1$ and support is $[-1,1]?$",Can one modify  so that all derivatives at  are  and support is,"1 - e^{-\frac{1}{x^2}} x=0 1 [-1,1]?","It is well-known that $f(x) = 1 - e^{-\frac{1}{x^2}}$ satisfies $f^{(i)}(0) = 1$ for all natural numbers $i.$ However, its support $\{x\in \mathbb{R}:f(x)\neq 0\} = \mathbb{R}.$ Question: Can one modify $1 - e^{-\frac{1}{x^2}}$ so that all derivatives at $x=0$ are $1$ and support is $[-1,1]?$ I tried $1 - e^{1-\frac{1}{x^2}}$ because it has value $0$ at both $x=-1$ and $x=1.$ However, it is not differentiable at those points.","It is well-known that satisfies for all natural numbers However, its support Question: Can one modify so that all derivatives at are and support is I tried because it has value at both and However, it is not differentiable at those points.","f(x) = 1 - e^{-\frac{1}{x^2}} f^{(i)}(0) = 1 i. \{x\in \mathbb{R}:f(x)\neq 0\} = \mathbb{R}. 1 - e^{-\frac{1}{x^2}} x=0 1 [-1,1]? 1 - e^{1-\frac{1}{x^2}} 0 x=-1 x=1.","['real-analysis', 'derivatives', 'differential-geometry']"
16,Prove that $g(z)=\overline{f(z)}$ is complex diﬀerentiable at $a$ if and only if $f'(a) = 0$.,Prove that  is complex diﬀerentiable at  if and only if .,g(z)=\overline{f(z)} a f'(a) = 0,"Suppose that $f$ is holomorphic in $\Omega$ , and let $g : \Omega → > \mathbb{C}$ be given by $g(z) = \overline{f(z)}$ . Prove that $g$ is   complex diﬀerentiable at $a$ if and only if $f'(a) = 0$ . I have tried the basic limit definition for derivatives but couldn't come up with a proper solution.  Also I know that for a complex valued function $f=u+iv$ , If $u$ and $v$ have continuous first derivatives, then $f$ should be analytic on $\Omega$ . But I'm not exactly sure whether it will be useful here. Hep would be appreciated","Suppose that is holomorphic in , and let be given by . Prove that is   complex diﬀerentiable at if and only if . I have tried the basic limit definition for derivatives but couldn't come up with a proper solution.  Also I know that for a complex valued function , If and have continuous first derivatives, then should be analytic on . But I'm not exactly sure whether it will be useful here. Hep would be appreciated","f \Omega g : \Omega →
> \mathbb{C} g(z) = \overline{f(z)} g a f'(a) = 0 f=u+iv u v f \Omega","['complex-analysis', 'derivatives']"
17,Proving $\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=f''(a)$,Proving,\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=f''(a),"The following question can be found in Bartle, Introduction to real analysis as well as Walter Rudin, Principle of Mathematical analysis: Let $f$ be differentiable function defined on an open interval $I$ Suppose $f''(a)$ exist at $a \in I$ Then $\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=f''(a)$ The first approach suggested by the Rudin is applying L'hopital's rule to $\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}$ This yields $\lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=\lim_{h\to0}\frac{f'(a+h)-f'(a-h)}{2h}=\lim_{h\to0}\frac{f'(a+h)-f'(a)+f'(a)-f'(a-h)}{2h}=f''(a)$ And thus proving the statement. However, I encounter a difficulties when I tried to prove the statement by mean value theorem. Here is my attempt. By mean value theorem, $\exists c_2 \in (a,a+h)$ such that $f(a+h)-f(a)=hf'(c_2)$ . Similarly, $\exists c_1 \in (a-h,a)$ such that $f(a)-f(a-h)=hf'(c_1)$ Hence $f(a-h)-f(a)=-hf'(c_1)$ Therefore, we have $\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=\frac{f'(c_2)-f'(c_1)}{h}=\frac{f'(c_2)-f'(a)+f'(a)-f'(c_1)}{h}$ As $f''(a)$ exists, $\forall \epsilon>0,\exists\delta>0$ such that $|\frac{f'(x)-f'(a)}{h}-f''(a)|<\epsilon$ when $ |x-a|<\delta$ Hence if $|h|<\delta$ then $|c_2-a|<\delta,|c_1-a|<\delta$ and we get $lim_{h \to 0}\frac{f'(c_2)-f'(a)+f'(a)-f'(c_1)}{h}=2f''(a)$ which is inconsistent with the statement. Can anyone tell me about what is wrong in my attempt. Thank you.","The following question can be found in Bartle, Introduction to real analysis as well as Walter Rudin, Principle of Mathematical analysis: Let be differentiable function defined on an open interval Suppose exist at Then The first approach suggested by the Rudin is applying L'hopital's rule to This yields And thus proving the statement. However, I encounter a difficulties when I tried to prove the statement by mean value theorem. Here is my attempt. By mean value theorem, such that . Similarly, such that Hence Therefore, we have As exists, such that when Hence if then and we get which is inconsistent with the statement. Can anyone tell me about what is wrong in my attempt. Thank you.","f I f''(a) a \in I \lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=f''(a) \lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2} \lim_{h\to0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}=\lim_{h\to0}\frac{f'(a+h)-f'(a-h)}{2h}=\lim_{h\to0}\frac{f'(a+h)-f'(a)+f'(a)-f'(a-h)}{2h}=f''(a) \exists c_2 \in (a,a+h) f(a+h)-f(a)=hf'(c_2) \exists c_1 \in (a-h,a) f(a)-f(a-h)=hf'(c_1) f(a-h)-f(a)=-hf'(c_1) \frac{f(a+h)-2f(a)+f(a-h)}{h^2}=\frac{f'(c_2)-f'(c_1)}{h}=\frac{f'(c_2)-f'(a)+f'(a)-f'(c_1)}{h} f''(a) \forall \epsilon>0,\exists\delta>0 |\frac{f'(x)-f'(a)}{h}-f''(a)|<\epsilon  |x-a|<\delta |h|<\delta |c_2-a|<\delta,|c_1-a|<\delta lim_{h \to 0}\frac{f'(c_2)-f'(a)+f'(a)-f'(c_1)}{h}=2f''(a)","['real-analysis', 'calculus', 'limits', 'derivatives']"
18,What are the solutions of $x=\cot x$?,What are the solutions of ?,x=\cot x,"Need to find intervals in which the function $y=\frac{x}{2}\cdot \cos x$ is increasing and decreasing. I tried to solve it on the way below but don't know how to continue. $ \\  y=\frac{x}{2}\cdot \cos x,\ x\in (0,2\pi)\\ y=\frac{1}{2}\cdot x\cdot \cos x \\ {y}'=({\frac{1}{2}\cdot x\cdot \cos x})' \\ {y}'=\frac{1}{2}\cdot ({x\cdot \cos x})' \\ {y}'=\frac{1}{2}\cdot ({x}'\cdot \cos x+x\cdot (\cos x)') \\ {y}'=\frac{1}{2}\cdot(\cos x-x\cdot \sin x) \\ {y}'=0 \\ \frac{1}{2}\cdot(\cos x-x\cdot \sin x)=0 \\ $ $\hspace{1cm}$ $\\ \cos x-x\cdot \sin x=0\ /(\cos x) \\ 1-x\cdot \tan x=0 \\ -x\cdot \tan x=-1 \\ x\cdot \tan x=1 \\ x=\frac{1}{\tan x} \\ x=\cot x $",Need to find intervals in which the function is increasing and decreasing. I tried to solve it on the way below but don't know how to continue.,"y=\frac{x}{2}\cdot \cos x  \\ 
y=\frac{x}{2}\cdot \cos x,\ x\in (0,2\pi)\\
y=\frac{1}{2}\cdot x\cdot \cos x \\
{y}'=({\frac{1}{2}\cdot x\cdot \cos x})' \\
{y}'=\frac{1}{2}\cdot ({x\cdot \cos x})' \\
{y}'=\frac{1}{2}\cdot ({x}'\cdot \cos x+x\cdot (\cos x)') \\
{y}'=\frac{1}{2}\cdot(\cos x-x\cdot \sin x) \\
{y}'=0 \\
\frac{1}{2}\cdot(\cos x-x\cdot \sin x)=0 \\  \hspace{1cm} \\
\cos x-x\cdot \sin x=0\ /(\cos x) \\
1-x\cdot \tan x=0 \\
-x\cdot \tan x=-1 \\
x\cdot \tan x=1 \\
x=\frac{1}{\tan x} \\
x=\cot x ","['derivatives', 'trigonometry']"
19,Finding slope m of tangent to curve,Finding slope m of tangent to curve,,"been years since I took calculus and am currently struggling with how to properly work out the following: Using the tanget line slope formula: My understanding is that I would need to find the slope with the following approach: Now, at this point, don't I want to rationalize the numerator? Or the denominator? If I go with rationalizing the numerator (multiplying by the conjugate), I do the following, but end up with a denominator that looks really overly complicated: What am I doing wrong here?","been years since I took calculus and am currently struggling with how to properly work out the following: Using the tanget line slope formula: My understanding is that I would need to find the slope with the following approach: Now, at this point, don't I want to rationalize the numerator? Or the denominator? If I go with rationalizing the numerator (multiplying by the conjugate), I do the following, but end up with a denominator that looks really overly complicated: What am I doing wrong here?",,"['calculus', 'derivatives', 'tangent-line', 'slope']"
20,Relation between roots of function and roots of its derivative,Relation between roots of function and roots of its derivative,,"I'm reading a section my Calculus book that is about the relation between the roots of a polynomial function and the roots of its derivative. So: Notice that if $x_1$ and $x_2$ are roots of $f$ , so that $f(x_1) = f(x_2) = 0$ , then by Rolle's Theorem there is a number $x$ between $x_1$ and $x_2$ such that $f'(x) = 0$ . Ok that makes sense. Then: This means that if $f$ has $k$ different roots $x_1 < x_2 < ... < x_k$ , then $f'$ has at least least $k-1$ roots: one between $x_1$ and $x_2$ , one between $x_2$ and $x_3$ , etc. That also makes sense, but what confuses me is ""at least least $k-1$ roots"". Why ""at least""? Didn't we just show that there are exactly $k-1$ roots for the derivative, or so to say, if we have a polynomial of degree $n$ , then its derivative has $n-1$ roots?","I'm reading a section my Calculus book that is about the relation between the roots of a polynomial function and the roots of its derivative. So: Notice that if and are roots of , so that , then by Rolle's Theorem there is a number between and such that . Ok that makes sense. Then: This means that if has different roots , then has at least least roots: one between and , one between and , etc. That also makes sense, but what confuses me is ""at least least roots"". Why ""at least""? Didn't we just show that there are exactly roots for the derivative, or so to say, if we have a polynomial of degree , then its derivative has roots?",x_1 x_2 f f(x_1) = f(x_2) = 0 x x_1 x_2 f'(x) = 0 f k x_1 < x_2 < ... < x_k f' k-1 x_1 x_2 x_2 x_3 k-1 k-1 n n-1,"['real-analysis', 'derivatives', 'polynomials', 'proof-explanation', 'roots']"
21,Evaluating Derivative of $\sqrt{1+\sqrt{1+\sqrt{1-2x}}}$,Evaluating Derivative of,\sqrt{1+\sqrt{1+\sqrt{1-2x}}},"Did I evaluate the following derivative correctly? I know I have not simplified to the utmost extent, but I want to know if this method is correct. Consider, $\sqrt{1+\sqrt{1+\sqrt{1-2x}}}$ Let A = $1+\sqrt{1+\sqrt{1-2x}}$ Let B = $1+\sqrt{1-2x}$ Let C = $1-2x$ $$\left(\sqrt{1+\sqrt{1+\sqrt{1-2x}}}\right)' = \frac{1}{2}A^{-\frac{1}{2}} \cdot \frac{1}{2}B^{-\frac{1}{2}} \cdot \frac{1}{2}C^{-\frac{1}{2}} \cdot -2 = \frac{-1}{4\sqrt{ABC}}$$","Did I evaluate the following derivative correctly? I know I have not simplified to the utmost extent, but I want to know if this method is correct. Consider, Let A = Let B = Let C =",\sqrt{1+\sqrt{1+\sqrt{1-2x}}} 1+\sqrt{1+\sqrt{1-2x}} 1+\sqrt{1-2x} 1-2x \left(\sqrt{1+\sqrt{1+\sqrt{1-2x}}}\right)' = \frac{1}{2}A^{-\frac{1}{2}} \cdot \frac{1}{2}B^{-\frac{1}{2}} \cdot \frac{1}{2}C^{-\frac{1}{2}} \cdot -2 = \frac{-1}{4\sqrt{ABC}},"['calculus', 'derivatives']"
22,Approximation of $\log(x)$ for very small $x$,Approximation of  for very small,\log(x) x,"To avoid the $\log()$ function, I am looking for a good approximation of $\log(x)$ for very small $x$ (e.g. order $10^{-5}$ ). I think Taylor series expansion is useless because around these small $x$ , the first order derivative approachs $+\infty$ . I did try this approximation $\log_{10}(x) \approx 1 - \frac{1}{\sqrt{x}}$ but still don't have satisfactory results. Could anyone suggest some better approximations?","To avoid the function, I am looking for a good approximation of for very small (e.g. order ). I think Taylor series expansion is useless because around these small , the first order derivative approachs . I did try this approximation but still don't have satisfactory results. Could anyone suggest some better approximations?",\log() \log(x) x 10^{-5} x +\infty \log_{10}(x) \approx 1 - \frac{1}{\sqrt{x}},"['derivatives', 'logarithms', 'approximation']"
23,How to reduce this to have only one x ...,How to reduce this to have only one x ...,,"Let $f(x)=\frac{5}{x+4} $ Reduce the difference quotient in the alternate definition of the derivative below so that you only have one x: $$\frac{f(x) - f(2)}{x-2}$$ I've gotten down to $\frac{5(10-x)}{x-2}$ . but I can't figure out how to reduce it to one $x$. Am I doing something wrong? I have tried long division, which ended up being incorrect, so I have no idea.","Let $f(x)=\frac{5}{x+4} $ Reduce the difference quotient in the alternate definition of the derivative below so that you only have one x: $$\frac{f(x) - f(2)}{x-2}$$ I've gotten down to $\frac{5(10-x)}{x-2}$ . but I can't figure out how to reduce it to one $x$. Am I doing something wrong? I have tried long division, which ended up being incorrect, so I have no idea.",,"['calculus', 'derivatives']"
24,Differentiate $\tan^3(x^2)$,Differentiate,\tan^3(x^2),"Differentiate $\tan^3(x^2)$ I first applied the chain rule and made $u=x^2$ and $g=\tan^3u$. I then calculated the derivative of $u$, which is $$u'=2x$$ and the derivative of $g$, which is  $$g'=3\tan^2u$$ I then applied the chain rule and multiplied them together, which gave me $$f'(x)=2x3\tan^2(x^2)$$ Is this correct? If not, any hints as to how to get the correct answer?","Differentiate $\tan^3(x^2)$ I first applied the chain rule and made $u=x^2$ and $g=\tan^3u$. I then calculated the derivative of $u$, which is $$u'=2x$$ and the derivative of $g$, which is  $$g'=3\tan^2u$$ I then applied the chain rule and multiplied them together, which gave me $$f'(x)=2x3\tan^2(x^2)$$ Is this correct? If not, any hints as to how to get the correct answer?",,"['calculus', 'derivatives']"
25,"Prove that if $f$ is differentiable at $a$, then $|f|$ is also differentiable at $a$, provided that $f(a) \ne 0$","Prove that if  is differentiable at , then  is also differentiable at , provided that",f a |f| a f(a) \ne 0,"Prove that if $f$ is differentiable at $a$, then $|f|$ is also differentiable at $a$, provided that $f(a) \ne 0$ Solution Attempt: $f$ is differentiable at $a \implies \lim_{x \rightarrow a} \dfrac {f(x)-f(a)}{x-a} = M$ $\implies \forall ~\epsilon>0,~\exists \delta \in \mathbb R^+$ such that $~~|\dfrac{f(x)-f(a)}{x-a} - M| < \epsilon,$ whenever $|x-a| < \delta$ Now, if we prove that $|~\dfrac{|f(x)|-|f(a)|}{x-a}-|M|~| ~~\le \epsilon$ whenever $|x-a| < \delta$, we are done. In general, we have the following inequalities which can come handy, for example: $|X \pm Y| \le |X| + |Y|$ and $|X-Y| \ge ||X|-|Y||$ I tried a bit of approaches but haven't got close to proving the desired result. Could someone please give me a direction. Thanks a lot!","Prove that if $f$ is differentiable at $a$, then $|f|$ is also differentiable at $a$, provided that $f(a) \ne 0$ Solution Attempt: $f$ is differentiable at $a \implies \lim_{x \rightarrow a} \dfrac {f(x)-f(a)}{x-a} = M$ $\implies \forall ~\epsilon>0,~\exists \delta \in \mathbb R^+$ such that $~~|\dfrac{f(x)-f(a)}{x-a} - M| < \epsilon,$ whenever $|x-a| < \delta$ Now, if we prove that $|~\dfrac{|f(x)|-|f(a)|}{x-a}-|M|~| ~~\le \epsilon$ whenever $|x-a| < \delta$, we are done. In general, we have the following inequalities which can come handy, for example: $|X \pm Y| \le |X| + |Y|$ and $|X-Y| \ge ||X|-|Y||$ I tried a bit of approaches but haven't got close to proving the desired result. Could someone please give me a direction. Thanks a lot!",,"['calculus', 'derivatives', 'absolute-value']"
26,Prove/disprove: $f+g$ and $f$ are differentiable at $x_0$ $\implies$ $g$ is differentiable at $x_0$,Prove/disprove:  and  are differentiable at    is differentiable at,f+g f x_0 \implies g x_0,"Prove/disprove: $f+g$ and $f$ are differentiable at $x_0$ $\implies$ $g$ is differentiable at $x_0$ attempt Suppose $g$ is not differentiable at $x_0$. There are three cases: Case I: The two following one-sided limits exist, but not equal.$$\lim_{x\to {x_0}^+}\frac{g(x)-g(x_0)}{x-x_0}\neq \lim_{x\to {x_0}^-}\frac{g(x)-g(x_0)}{x-x_0}$$ Case II: $g$ is not continuous at $x_0$ Case III: $g$ is undefined at $x_0$ Case I: $$\lim_{x\to {x_0}^-}\frac{(f+g)(x)-(f+g)(x_0)}{x-x_0}=\lim_{x\to {x_0}^-}\Big(\frac{f(x)-f(x_0)}{x-x_0}+\frac{g(x)-g(x_0)}{x-x_0}\Big)=\lim_{x\to {x_0}^-}\frac{f(x)-f(x_0)}{x-x_0}+\lim_{x\to {x_0}^-}\frac{g(x)-g(x_0)}{x-x_0}=L_f+L_{g^-}$$Similarly, with the other one-sided limit:$$\lim_{x\to {x_0}^+}\Big(...\Big)=...=L_f+L_{g^+}$$ Case II: For every type of discontinuity of $g$ we use one-sided limit rules and continuity of $f$ to show that $f+g$ is not continuous and therefore not differentiable. Case III: That leads to $f+g$ being undefined at $x_0$. comment I haven't yet found a counterexample. If this idea of a proof works, is there a shorter one?","Prove/disprove: $f+g$ and $f$ are differentiable at $x_0$ $\implies$ $g$ is differentiable at $x_0$ attempt Suppose $g$ is not differentiable at $x_0$. There are three cases: Case I: The two following one-sided limits exist, but not equal.$$\lim_{x\to {x_0}^+}\frac{g(x)-g(x_0)}{x-x_0}\neq \lim_{x\to {x_0}^-}\frac{g(x)-g(x_0)}{x-x_0}$$ Case II: $g$ is not continuous at $x_0$ Case III: $g$ is undefined at $x_0$ Case I: $$\lim_{x\to {x_0}^-}\frac{(f+g)(x)-(f+g)(x_0)}{x-x_0}=\lim_{x\to {x_0}^-}\Big(\frac{f(x)-f(x_0)}{x-x_0}+\frac{g(x)-g(x_0)}{x-x_0}\Big)=\lim_{x\to {x_0}^-}\frac{f(x)-f(x_0)}{x-x_0}+\lim_{x\to {x_0}^-}\frac{g(x)-g(x_0)}{x-x_0}=L_f+L_{g^-}$$Similarly, with the other one-sided limit:$$\lim_{x\to {x_0}^+}\Big(...\Big)=...=L_f+L_{g^+}$$ Case II: For every type of discontinuity of $g$ we use one-sided limit rules and continuity of $f$ to show that $f+g$ is not continuous and therefore not differentiable. Case III: That leads to $f+g$ being undefined at $x_0$. comment I haven't yet found a counterexample. If this idea of a proof works, is there a shorter one?",,"['real-analysis', 'derivatives']"
27,Evaluate the Limit $\lim_{x \to 0}\frac{x}{\sqrt{1-e^{-x^2}}}$,Evaluate the Limit,\lim_{x \to 0}\frac{x}{\sqrt{1-e^{-x^2}}},Evaluate the Limit $$L=\lim_{x \to 0}\frac{x}{\sqrt{1-e^{-x^2}}}$$ Now it is in Indeterminate form $\frac{0}{0}$ I Tried using L'Hopital's Rule as below: $$L=\lim_{x \to 0}\frac{1}{\frac{1}{2\sqrt{1-e^{-x^2}}}{\left(-e^{-x^2}\right)}{(-2x)}}$$ $\implies$ $$L=\lim_{x \to 0}\frac{\sqrt{1-e^{-x^2}}}{x}=\frac{1}{L}$$ hence $$L=1$$ is this right approach?,Evaluate the Limit $$L=\lim_{x \to 0}\frac{x}{\sqrt{1-e^{-x^2}}}$$ Now it is in Indeterminate form $\frac{0}{0}$ I Tried using L'Hopital's Rule as below: $$L=\lim_{x \to 0}\frac{1}{\frac{1}{2\sqrt{1-e^{-x^2}}}{\left(-e^{-x^2}\right)}{(-2x)}}$$ $\implies$ $$L=\lim_{x \to 0}\frac{\sqrt{1-e^{-x^2}}}{x}=\frac{1}{L}$$ hence $$L=1$$ is this right approach?,,"['algebra-precalculus', 'limits', 'derivatives']"
28,Can we say $\lim_{x \to 0} \frac{\sin x}{x} $ exactly $1$,Can we say  exactly,\lim_{x \to 0} \frac{\sin x}{x}  1,"Can we say $$\lim_{x \to 0} \frac{\sin x}{x} $$ is exactly equal to $1$ I got this doubt while solving the limit $$\lfloor\lim_{x \to 0}\frac{\sin x}{x}\rfloor$$ because as per my knowledge $\lim_{x \to 0} \frac{\sin x}{x}$ approaches $1$ from below hence $$\lfloor\lim_{x \to 0}\frac{\sin x}{x}\rfloor=0$$ But some books give it as $1$, so what exactly is the concept here?","Can we say $$\lim_{x \to 0} \frac{\sin x}{x} $$ is exactly equal to $1$ I got this doubt while solving the limit $$\lfloor\lim_{x \to 0}\frac{\sin x}{x}\rfloor$$ because as per my knowledge $\lim_{x \to 0} \frac{\sin x}{x}$ approaches $1$ from below hence $$\lfloor\lim_{x \to 0}\frac{\sin x}{x}\rfloor=0$$ But some books give it as $1$, so what exactly is the concept here?",,"['algebra-precalculus', 'limits', 'derivatives']"
29,To Evaluate the Limit $\lim_{n \to \infty}\left(1+\sum_{k=1}^{n} \frac{1}{\binom{n}{k}}\right)^n$,To Evaluate the Limit,\lim_{n \to \infty}\left(1+\sum_{k=1}^{n} \frac{1}{\binom{n}{k}}\right)^n,To Evaluate the Limit $$L=\lim_{n \to \infty}\left(1+\sum_{k=1}^{n} \frac{1}{\binom{n}{k}}\right)^n \tag{1}$$ My try: I tried to use $$\frac{1}{\binom{n}{k}}+\frac{1}{\binom{n}{k+1}}=\frac{n+1}{n} \frac{1}{\binom{n-1}{k}} $$ taking summation both sides from $k=1$ to $k=n$ we get $$\sum_{k=1}^{n} \frac{1}{\binom{n}{k}}+\sum_{k=1}^{n} \frac{1}{\binom{n}{k+1}}=\frac{n+1}{n} \sum_{k=1}^{n} \frac{1}{\binom{n-1}{k}} \tag{2}$$ Now let $$S=\lim_{n \to \infty} \sum_{k=1}^{n} \frac{1}{\binom{n}{k}}$$ we have from $(2)$ $$S+S=S$$ hence $$S=0$$ Now $(1)$ is in form of $1^{\infty}$ Indeterminate form whose limit is given by $$L=e^\left({\lim_{n \to \infty}}n \times \sum_{k=1}^{n} \frac{1}{\binom{n}{k}}\right)$$ How to proceed now?,To Evaluate the Limit $$L=\lim_{n \to \infty}\left(1+\sum_{k=1}^{n} \frac{1}{\binom{n}{k}}\right)^n \tag{1}$$ My try: I tried to use $$\frac{1}{\binom{n}{k}}+\frac{1}{\binom{n}{k+1}}=\frac{n+1}{n} \frac{1}{\binom{n-1}{k}} $$ taking summation both sides from $k=1$ to $k=n$ we get $$\sum_{k=1}^{n} \frac{1}{\binom{n}{k}}+\sum_{k=1}^{n} \frac{1}{\binom{n}{k+1}}=\frac{n+1}{n} \sum_{k=1}^{n} \frac{1}{\binom{n-1}{k}} \tag{2}$$ Now let $$S=\lim_{n \to \infty} \sum_{k=1}^{n} \frac{1}{\binom{n}{k}}$$ we have from $(2)$ $$S+S=S$$ hence $$S=0$$ Now $(1)$ is in form of $1^{\infty}$ Indeterminate form whose limit is given by $$L=e^\left({\lim_{n \to \infty}}n \times \sum_{k=1}^{n} \frac{1}{\binom{n}{k}}\right)$$ How to proceed now?,,"['sequences-and-series', 'algebra-precalculus', 'limits', 'derivatives', 'convergence-divergence']"
30,If $\lim_{h\to 0} \frac{ f_1(a+ h) - 2f_1(a) + f_1(a-h) }{h^2}$ is constant then is $f_1$ a quadratic function? [closed],If  is constant then is  a quadratic function? [closed],\lim_{h\to 0} \frac{ f_1(a+ h) - 2f_1(a) + f_1(a-h) }{h^2} f_1,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $f_2(x)$ be a contstant function and $f_1(x)$ be a continous funtion $f:\mathbb{R}\to\mathbb{R}$ . Let $$\lim_{h\to 0} \frac{ f_1(a+ h) - 2f_1(a) + f_1(a-h) }{h^2}=f_2(a).$$ Is it true that if the limit above exist for all $a$ s, then $f_1(a)$ is polynomial function with degree at most 2? Here is the reverse of this problem: reverse Please help, I am thankful for every solution! EDIT: In my original problem, “continous” was missing","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let be a contstant function and be a continous funtion . Let Is it true that if the limit above exist for all s, then is polynomial function with degree at most 2? Here is the reverse of this problem: reverse Please help, I am thankful for every solution! EDIT: In my original problem, “continous” was missing",f_2(x) f_1(x) f:\mathbb{R}\to\mathbb{R} \lim_{h\to 0} \frac{ f_1(a+ h) - 2f_1(a) + f_1(a-h) }{h^2}=f_2(a). a f_1(a),"['real-analysis', 'functional-analysis', 'derivatives', 'polynomials']"
31,"If $\frac{\cos^4 \alpha}{x}+\frac{\sin^4 \alpha}{y}=\frac{1}{x+y}$,prove that $\frac{dy}{dx}=\tan^2\alpha$","If ,prove that",\frac{\cos^4 \alpha}{x}+\frac{\sin^4 \alpha}{y}=\frac{1}{x+y} \frac{dy}{dx}=\tan^2\alpha,"If $\frac{\cos^4 \alpha}{x}+\frac{\sin^4 \alpha}{y}=\frac{1}{x+y}$,prove that $\frac{dy}{dx}=\tan^2\alpha$ It is very long to direct differentiate it.Can someone help me?","If $\frac{\cos^4 \alpha}{x}+\frac{\sin^4 \alpha}{y}=\frac{1}{x+y}$,prove that $\frac{dy}{dx}=\tan^2\alpha$ It is very long to direct differentiate it.Can someone help me?",,"['trigonometry', 'derivatives']"
32,"$f^{(n)}(x)$ exist $\forall x \in I$, an interval in $\mathbb{R}$ but $f^{(n+1)}(x)$ does not exist for any $x \in I$","exist , an interval in  but  does not exist for any",f^{(n)}(x) \forall x \in I \mathbb{R} f^{(n+1)}(x) x \in I,"$f'(x)$ , $f''(x)$ , $f^{(3)}(x)$ , ..., $f^{(n)}(x)$ all exist $\forall x \in I$ , an interval in $\mathbb{R}$ but $f^{(n+1)}(x)$ does not exist for any $x \in I$ Can such a function exist? In words: Does there exist a function $f$ that is $n$ -times differentiable on an interval $I$ but is not $(n+1)$ -times differentiable anywhere on $I$ (i.e. $(n+1)$ -differentiable nowhere on $I$ )? I think it can't but can't think of the reason why. I can construct a function with countably infinite points in I where the function is only differentiable n times, but, it seems impossible to have every point in the interval, I, have this property.",", , , ..., all exist , an interval in but does not exist for any Can such a function exist? In words: Does there exist a function that is -times differentiable on an interval but is not -times differentiable anywhere on (i.e. -differentiable nowhere on )? I think it can't but can't think of the reason why. I can construct a function with countably infinite points in I where the function is only differentiable n times, but, it seems impossible to have every point in the interval, I, have this property.",f'(x) f''(x) f^{(3)}(x) f^{(n)}(x) \forall x \in I \mathbb{R} f^{(n+1)}(x) x \in I f n I (n+1) I (n+1) I,"['real-analysis', 'calculus', 'derivatives']"
33,Find the inflection points of $f(x)={1 +\ln^2 x \over x}$,Find the inflection points of,f(x)={1 +\ln^2 x \over x},"Given $$f(x)={1+\ln^2x\over x}, x>0$$ • Find the inflection points of $C_f.$ Personal work: In order to find the inflection points of $C_f$ we first need to find the second derivative and then at which "" $x$ "", $f''(x)=0.$ $${d\over dx}({1+\ln^2 x \over x})={2lnx-1-\ln^2x\over x^2}$$ $${d\over dx}({2lnx-1-\ln^2x\over x^2})=\cdots={2\ln^2-6\ln x+4 \over x^3}$$ or $${d\over dx}({d\over dx}({1+\ln^2 x \over x}))={2\ln^2-6\ln x+4 \over x^3}$$ I'm struggling on finding the sign of $f'(x)$ and the points that $f''(x)=0.$","Given • Find the inflection points of Personal work: In order to find the inflection points of we first need to find the second derivative and then at which "" "", or I'm struggling on finding the sign of and the points that","f(x)={1+\ln^2x\over x}, x>0 C_f. C_f x f''(x)=0. {d\over dx}({1+\ln^2 x \over x})={2lnx-1-\ln^2x\over x^2} {d\over dx}({2lnx-1-\ln^2x\over x^2})=\cdots={2\ln^2-6\ln x+4 \over x^3} {d\over dx}({d\over dx}({1+\ln^2 x \over x}))={2\ln^2-6\ln x+4 \over x^3} f'(x) f''(x)=0.","['calculus', 'derivatives']"
34,Question about Product Rule? (basic calculus),Question about Product Rule? (basic calculus),,"so as you all know the product rule states $\frac{d}{dx}[f(x)g(x)]=f'(x)g(x)+f(x)g'(x)$ Lets say we have the following: $\frac{d}{dx}[x^2\sin(x)]$ In this case we can use the product rule as such that we take both functions separately: $f(x)=x^2$ and $g(x)=\sin(x)$ $f'(x)=2x$ and $g'(x)=\cos(x)$ Now this is exactly what I don't get. Like at all. How is it that at the beginning we had this one function ""$\frac{d}{dx}[x^2\sin(x)]$"" that we had to take the derivative of, and we just split it in half and turned this into two separate functions? It just doesn't make any sense to me unless I'm missing something. Thanks in advance!","so as you all know the product rule states $\frac{d}{dx}[f(x)g(x)]=f'(x)g(x)+f(x)g'(x)$ Lets say we have the following: $\frac{d}{dx}[x^2\sin(x)]$ In this case we can use the product rule as such that we take both functions separately: $f(x)=x^2$ and $g(x)=\sin(x)$ $f'(x)=2x$ and $g'(x)=\cos(x)$ Now this is exactly what I don't get. Like at all. How is it that at the beginning we had this one function ""$\frac{d}{dx}[x^2\sin(x)]$"" that we had to take the derivative of, and we just split it in half and turned this into two separate functions? It just doesn't make any sense to me unless I'm missing something. Thanks in advance!",,"['calculus', 'derivatives']"
35,Find $f'(a)$ if $f(x) = \frac {x^n - a^n}{x-a}$,Find  if,f'(a) f(x) = \frac {x^n - a^n}{x-a},"I'll state the multiple choice question from my textbook below: If $f(x) = \frac {x^n - a^n}{x - a}$ for some constant '$a$', then $f'(a)$ is (A) $1$ (B) $0$ (C) does not exist (D) $\frac 12$ Here's what I tried: By using quotient rule we get: $f'(x) = \frac {(n-1)x^n - nax^{n-1} + a^n}{(x-a)^2}$ Clearly, $f'(a) = \frac 00$ And the correct choice according to my book is (C). Does getting $\frac 00$ for a particular value of the variable (in this case, for $x=a$) mean that the derivative of the function doesn't exist at that point? Not satisfied by the answer I factorised the numerator of $f(x)$ and cancelled the factor $(x-a)$ as below: $f(x) = \frac {(x-a)(x^{n-1} + x^{n-2}a + x^{n-3}a^2 +..........+ xa^{n-2} + a^{n-1})}{x-a}$ $\implies f(x) = x^{n-1} + x^{n-2}a + x^{n-3}a^2 +..........+ xa^{n-2} + a^{n-1}$ Differentiating with respect to $x$ we get, $f'(x) = (n-1)x^{n-2} + (n-2)x^{n-3}a + (n-3)x^{n-4}a^2 +..........+ a^{n-2}$ $\implies f'(a) = \Big[(n-1) + (n-2) + (n-3) + .......... + 1\Big]a^{n-2}$ $\implies f'(a) = \frac {n(n-1)}2 a^{n-2}$ Now this is a completely different answer. So what have I done wrong? Is there some problem with cancelling the factor $(x-a)$? Does it have something to with the continuity and differentiabilty of $f(x)$ at $a$? What if the function was $f(x) = \begin{cases} \frac {x^n - a^n}{x - a},  & \text{if $x \ne a$} \\ na^{n-1}, & \text{if $x = a$} \end{cases}$? How do I find the derivative at $x = a$ in this case?","I'll state the multiple choice question from my textbook below: If $f(x) = \frac {x^n - a^n}{x - a}$ for some constant '$a$', then $f'(a)$ is (A) $1$ (B) $0$ (C) does not exist (D) $\frac 12$ Here's what I tried: By using quotient rule we get: $f'(x) = \frac {(n-1)x^n - nax^{n-1} + a^n}{(x-a)^2}$ Clearly, $f'(a) = \frac 00$ And the correct choice according to my book is (C). Does getting $\frac 00$ for a particular value of the variable (in this case, for $x=a$) mean that the derivative of the function doesn't exist at that point? Not satisfied by the answer I factorised the numerator of $f(x)$ and cancelled the factor $(x-a)$ as below: $f(x) = \frac {(x-a)(x^{n-1} + x^{n-2}a + x^{n-3}a^2 +..........+ xa^{n-2} + a^{n-1})}{x-a}$ $\implies f(x) = x^{n-1} + x^{n-2}a + x^{n-3}a^2 +..........+ xa^{n-2} + a^{n-1}$ Differentiating with respect to $x$ we get, $f'(x) = (n-1)x^{n-2} + (n-2)x^{n-3}a + (n-3)x^{n-4}a^2 +..........+ a^{n-2}$ $\implies f'(a) = \Big[(n-1) + (n-2) + (n-3) + .......... + 1\Big]a^{n-2}$ $\implies f'(a) = \frac {n(n-1)}2 a^{n-2}$ Now this is a completely different answer. So what have I done wrong? Is there some problem with cancelling the factor $(x-a)$? Does it have something to with the continuity and differentiabilty of $f(x)$ at $a$? What if the function was $f(x) = \begin{cases} \frac {x^n - a^n}{x - a},  & \text{if $x \ne a$} \\ na^{n-1}, & \text{if $x = a$} \end{cases}$? How do I find the derivative at $x = a$ in this case?",,"['calculus', 'derivatives']"
36,derivative of square of dot product,derivative of square of dot product,,"I've got: $\frac{\partial}{\partial \textbf{w}} \bigg( \textbf{x}^T\textbf{w}\bigg)^2$ when taking the derivative, does this result in $2 (\underbrace{\textbf{x} ~\textbf{x}^T}_{\Sigma_x}) \textbf{w}^T$ or in the dot product of the $\textbf{x}$s $2 (\underbrace{\textbf{x}^T ~\textbf{x}}_{scalar}) \textbf{w}^T$","I've got: $\frac{\partial}{\partial \textbf{w}} \bigg( \textbf{x}^T\textbf{w}\bigg)^2$ when taking the derivative, does this result in $2 (\underbrace{\textbf{x} ~\textbf{x}^T}_{\Sigma_x}) \textbf{w}^T$ or in the dot product of the $\textbf{x}$s $2 (\underbrace{\textbf{x}^T ~\textbf{x}}_{scalar}) \textbf{w}^T$",,"['derivatives', 'vectors']"
37,A weird differentiation question.,A weird differentiation question.,,I was looking at questions on differentiation and came across a weird form that both me and my friend had two different approaches to. This was the question. Calculate the derivative: $$ \frac{d(x-\sin(x))}{d(1-\cos(x))} $$ My approach was to factorize out an $x$ in the denominator of the fraction and differentiate as normal like so: $$ \frac{d(x-\sin(x))}{dx(\frac{1}{x} - \frac{\cos(x)}{x})} = \frac{d}{dx}(\frac{x-\sin(x)}{\frac{1}{x} - \frac{\cos(x)}{x}}) $$ My friend's approach was to let $ u = 1-\cos(x) $ and to then take the approach that way leading to an answer. Which way would be considered correct and why would this work? Thank you!,I was looking at questions on differentiation and came across a weird form that both me and my friend had two different approaches to. This was the question. Calculate the derivative: $$ \frac{d(x-\sin(x))}{d(1-\cos(x))} $$ My approach was to factorize out an $x$ in the denominator of the fraction and differentiate as normal like so: $$ \frac{d(x-\sin(x))}{dx(\frac{1}{x} - \frac{\cos(x)}{x})} = \frac{d}{dx}(\frac{x-\sin(x)}{\frac{1}{x} - \frac{\cos(x)}{x}}) $$ My friend's approach was to let $ u = 1-\cos(x) $ and to then take the approach that way leading to an answer. Which way would be considered correct and why would this work? Thank you!,,"['calculus', 'derivatives']"
38,Calculating $\lim_{x\rightarrow 0}\frac{1}{x^n}e^{\frac{-1}{x^2}}$ (by hand),Calculating  (by hand),\lim_{x\rightarrow 0}\frac{1}{x^n}e^{\frac{-1}{x^2}},$$\lim_{x\rightarrow 0}\frac{1}{x^n}e^{\frac{-1}{x^2}}$$ I tried using de l'Hospital but can't get any further Thanks!,$$\lim_{x\rightarrow 0}\frac{1}{x^n}e^{\frac{-1}{x^2}}$$ I tried using de l'Hospital but can't get any further Thanks!,,"['calculus', 'analysis', 'derivatives']"
39,Why is the inverse of the derivative of f not the actual derivative of the inverse of f?,Why is the inverse of the derivative of f not the actual derivative of the inverse of f?,,"So, I've explored this a little, but it is still confusing. When you calculate the inverse of a function, f, that is one-to-one, the points switch: a point (2,8) on f would be (8,2) on the inverse. So, one would assume that the derivatives of the functions would also constitute the reversal of points. However, that is not the case. For example, you have: $f (x) = 5x^2 \phantom{=}\text{ for  $x\geq0$}$ $f '(x) = 10x$ and $(f^{-1}) (x) = \sqrt{\frac{x}{5}}$ Here is my question: Why is finding the inverse of the derivative of $f$, $f '(x)$, and taking its inverse not the real derivative of the inverse? I would think $(f^{-1}) '(x) = \frac{x}{10}$, but that is not the case. The real inverse would be taking the derivative of $(f^{-1}) (x)$ and finding $(f^{-1}) '(x) = (\frac{1}{10\sqrt{x/5}})$. In my mind, both of these seem like they could be the derivatives of the inverse, yet only the latter is true. Why is this? Also, maybe I missed out in class, but is there some sort of quick relationship between (besides the formula) $f '(x)$ and $(f^{-1}) '(x)$ similar to how points switch between $f (x)$ and $(f^{-1}) (x)$. Thanks.","So, I've explored this a little, but it is still confusing. When you calculate the inverse of a function, f, that is one-to-one, the points switch: a point (2,8) on f would be (8,2) on the inverse. So, one would assume that the derivatives of the functions would also constitute the reversal of points. However, that is not the case. For example, you have: $f (x) = 5x^2 \phantom{=}\text{ for  $x\geq0$}$ $f '(x) = 10x$ and $(f^{-1}) (x) = \sqrt{\frac{x}{5}}$ Here is my question: Why is finding the inverse of the derivative of $f$, $f '(x)$, and taking its inverse not the real derivative of the inverse? I would think $(f^{-1}) '(x) = \frac{x}{10}$, but that is not the case. The real inverse would be taking the derivative of $(f^{-1}) (x)$ and finding $(f^{-1}) '(x) = (\frac{1}{10\sqrt{x/5}})$. In my mind, both of these seem like they could be the derivatives of the inverse, yet only the latter is true. Why is this? Also, maybe I missed out in class, but is there some sort of quick relationship between (besides the formula) $f '(x)$ and $(f^{-1}) '(x)$ similar to how points switch between $f (x)$ and $(f^{-1}) (x)$. Thanks.",,"['calculus', 'derivatives', 'inverse', 'inverse-function']"
40,Number of real roots of the following polynomial,Number of real roots of the following polynomial,,"I have a cubic polynomial: $f(x)=x^3+3x^2+3x+7$ I wanted to obtain number of real roots of the provided polynomial. What i did: I took the derivative of the function which turned out to be $3x^2+6x+3$ which is  a perfect square after taking $3$ common. Hence, the function is strictly increasing. Also, it has two negative roots. Now, it's function can have at the most three roots using this monotonicity concept. Since, the polynomial is of order three, it can have at the most three roots. Using the rule of sign, we can also say that this polynomial will have either three negative roots or one negative roots. The three negative root situation can be ignored using the monotonicity. Only plausible situation is one negative roots. Hence, this polynomial will have one real root. I think the number of steps that I took can be reduced. I am also using some redundant things please guide me. Are my steps correct to obtain the number of roots. If not, please add some point that I can think of while obtaining number of roots of a polynomial or of any other function whatsoever. Thanks in advance.","I have a cubic polynomial: $f(x)=x^3+3x^2+3x+7$ I wanted to obtain number of real roots of the provided polynomial. What i did: I took the derivative of the function which turned out to be $3x^2+6x+3$ which is  a perfect square after taking $3$ common. Hence, the function is strictly increasing. Also, it has two negative roots. Now, it's function can have at the most three roots using this monotonicity concept. Since, the polynomial is of order three, it can have at the most three roots. Using the rule of sign, we can also say that this polynomial will have either three negative roots or one negative roots. The three negative root situation can be ignored using the monotonicity. Only plausible situation is one negative roots. Hence, this polynomial will have one real root. I think the number of steps that I took can be reduced. I am also using some redundant things please guide me. Are my steps correct to obtain the number of roots. If not, please add some point that I can think of while obtaining number of roots of a polynomial or of any other function whatsoever. Thanks in advance.",,"['calculus', 'derivatives', 'monotone-functions']"
41,"Where does the limit go in the ""little o"" definition of $f'(x)$?","Where does the limit go in the ""little o"" definition of ?",f'(x),"\begin{equation} f'(x)=\lim_{a\to x} \frac{f(a)-f(x)}{a-x} \end{equation} is the definition of a limit that I've known forever, but my professor was saying that we can rearrange this ""ignoring the limit"" to get \begin{equation} f'(x)(a-x)=f(a)-f(x)\tag{1} \end{equation} \begin{equation} f'(x)(a-x)+f(x)=f(a)\tag{2} \end{equation} But then we have to add a ""little o"" term so  \begin{equation} f(a)=f(x)+f'(x)(a-x)+o(a-x) \end{equation} where $\lim_{a\to x} o(a-x)/(a-x)=0$ I just don't understand how the ""little o"" term makes up for the original limit that we ignored until the last step. If we rearranged this formula again, it wouldn't equal the original limit because only one term has a limit on it...","\begin{equation} f'(x)=\lim_{a\to x} \frac{f(a)-f(x)}{a-x} \end{equation} is the definition of a limit that I've known forever, but my professor was saying that we can rearrange this ""ignoring the limit"" to get \begin{equation} f'(x)(a-x)=f(a)-f(x)\tag{1} \end{equation} \begin{equation} f'(x)(a-x)+f(x)=f(a)\tag{2} \end{equation} But then we have to add a ""little o"" term so  \begin{equation} f(a)=f(x)+f'(x)(a-x)+o(a-x) \end{equation} where $\lim_{a\to x} o(a-x)/(a-x)=0$ I just don't understand how the ""little o"" term makes up for the original limit that we ignored until the last step. If we rearranged this formula again, it wouldn't equal the original limit because only one term has a limit on it...",,"['limits', 'derivatives']"
42,Is $\frac{\partial}{\partial x} f(x-y) = - \frac{\partial}{\partial y} f(x-y)$?,Is ?,\frac{\partial}{\partial x} f(x-y) = - \frac{\partial}{\partial y} f(x-y),This seems intuitively plausible to me. But the notation sort of gets in the way when trying to prove this exactly. In particular when using the chain rule to write $\frac{\partial}{\partial y} f(x-y) = - f'(x-y)$ the $'$ looses the information that the chain rule has already been applied.,This seems intuitively plausible to me. But the notation sort of gets in the way when trying to prove this exactly. In particular when using the chain rule to write $\frac{\partial}{\partial y} f(x-y) = - f'(x-y)$ the $'$ looses the information that the chain rule has already been applied.,,['derivatives']
43,Derivative of $\sin^{-1}(x)$,Derivative of,\sin^{-1}(x),"I can find this using the fact that $\sin(\sin^{-1}(x)) = x$, for all $x\in[-1,1].$ Now, differentiate. $$\frac{d}{d\sin^{-1}(x)}\sin(\sin^{-1}(x))\cdot \frac{d}{dx} \sin^{-1}(x)= \frac{d}{dx} x= 1$$ $$\cos(\sin^{-1}(x))\cdot \frac{d}{dx} \sin^{-1}(x) = 1$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\cos(\sin^{-1}(x))}$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\sqrt{1-\sin^2(\sin^{-1}(x))}}$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\sqrt{1-x^2}}$$ However, what if I wanted to differentiate this like $\ \sin^{-1}(\sin(x))$ without knowing the fact that $\ \frac{d}{dx}\sin^{-1}(x) = \frac{1}{\sqrt{1-x^2}}$ ? Is there a solution for it? I keep getting stuck at a certain step when I try this...","I can find this using the fact that $\sin(\sin^{-1}(x)) = x$, for all $x\in[-1,1].$ Now, differentiate. $$\frac{d}{d\sin^{-1}(x)}\sin(\sin^{-1}(x))\cdot \frac{d}{dx} \sin^{-1}(x)= \frac{d}{dx} x= 1$$ $$\cos(\sin^{-1}(x))\cdot \frac{d}{dx} \sin^{-1}(x) = 1$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\cos(\sin^{-1}(x))}$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\sqrt{1-\sin^2(\sin^{-1}(x))}}$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\sqrt{1-x^2}}$$ However, what if I wanted to differentiate this like $\ \sin^{-1}(\sin(x))$ without knowing the fact that $\ \frac{d}{dx}\sin^{-1}(x) = \frac{1}{\sqrt{1-x^2}}$ ? Is there a solution for it? I keep getting stuck at a certain step when I try this...",,"['trigonometry', 'derivatives']"
44,Solving Algebraic Riccati Equation by using Newton-Raphson's method?,Solving Algebraic Riccati Equation by using Newton-Raphson's method?,,Assume that we have a function called $$F(X) = 0$$ That function can be written as the Algebraic Riccati Equation: $$F(X) = A^T X + XA - XBR^{-1}B^TX + Q = 0$$ Where the solution to the equation is matrix $X$. To do a Newton-Raphson solveing method. I need to use this equation: $$X_{i+1} = X_{i} - F'(X_{i})^{-1}F(X_{i})$$ Where $F'(X)$ is the jacobian function of $F(X)$ and $X_{i+1}$ is going the be the solution to the Algebraic Riccati Equation. My question is: How can I find the derivative of $F(X)$ ? Can I just assume that the derivative is: $$F'(X) = A^T + A - XBR^{-1}B^T$$ ?,Assume that we have a function called $$F(X) = 0$$ That function can be written as the Algebraic Riccati Equation: $$F(X) = A^T X + XA - XBR^{-1}B^TX + Q = 0$$ Where the solution to the equation is matrix $X$. To do a Newton-Raphson solveing method. I need to use this equation: $$X_{i+1} = X_{i} - F'(X_{i})^{-1}F(X_{i})$$ Where $F'(X)$ is the jacobian function of $F(X)$ and $X_{i+1}$ is going the be the solution to the Algebraic Riccati Equation. My question is: How can I find the derivative of $F(X)$ ? Can I just assume that the derivative is: $$F'(X) = A^T + A - XBR^{-1}B^T$$ ?,,"['matrices', 'derivatives', 'numerical-methods', 'optimal-control', 'newton-raphson']"
45,"Are there any functions that are differentiable on on $(a,b)$ and such that $f(a)=f(b)$ but do not follow Rolle's theorem?",Are there any functions that are differentiable on on  and such that  but do not follow Rolle's theorem?,"(a,b) f(a)=f(b)","So, Rolle's Theorem requires three conditions: $(i) f$ is continuous on $[a,b]$ $(ii)$ $f$ is differentiable on $(a,b)$ $(iii)$ $f(a)=f(b)$ So I am trying to show that all three conditions are necessary for Rolle's theorem to apply, but I have been unable to come up with a function that meets $(ii)$ and $(iii)$ but there does not exist a $c \in (a,b)$ such that $f'(c)=0$ Any thoughts? For the other two condition combinations I came up with for $(i$) and $(ii)$ $f(x)=x$, and $(i)$ and $(iii)$ $f(x)=|x|$","So, Rolle's Theorem requires three conditions: $(i) f$ is continuous on $[a,b]$ $(ii)$ $f$ is differentiable on $(a,b)$ $(iii)$ $f(a)=f(b)$ So I am trying to show that all three conditions are necessary for Rolle's theorem to apply, but I have been unable to come up with a function that meets $(ii)$ and $(iii)$ but there does not exist a $c \in (a,b)$ such that $f'(c)=0$ Any thoughts? For the other two condition combinations I came up with for $(i$) and $(ii)$ $f(x)=x$, and $(i)$ and $(iii)$ $f(x)=|x|$",,"['calculus', 'derivatives']"
46,Is $\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x-h\right)}{2h}=f'(x)$?,Is ?,\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x-h\right)}{2h}=f'(x),"I was working on problem trying to find if some point on the function for some shape was a vertex of not by testing if $\lim_{h\to 0}f'(x+h)\not=f'(x)$ to see if the was a sudden change at the point as seen in vertices, but I kept running into the problem that the derivatives at those points where undefined or nonsensical either due to the function being piecewise or the derivative being $\frac{0}{0}$. So I considered taking instead the derivative between the two points adjacent to $x$ as represent in the function in the title above of $\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x-h\right)}{2h}$. But the question that I have is a valid redefinition? Some of my thoughts: If $$\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x-h\right)}{2h}=f'(x)$$ then $$\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x-h\right)}{2h}=\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x\right)}{h}$$ further implying that $$f(x)=\lim_{h\to0}\frac{f(x+h)+f(x-h)}{2}$$ When I input this function in WolframAlpha I get an output telling me that this is true for all analytical functions and the series expansion approaches $f(x)$. If I remove the limit on $h$ WolframAlpha tells me that $f(x)$ must be even function, although I am skeptical of this.","I was working on problem trying to find if some point on the function for some shape was a vertex of not by testing if $\lim_{h\to 0}f'(x+h)\not=f'(x)$ to see if the was a sudden change at the point as seen in vertices, but I kept running into the problem that the derivatives at those points where undefined or nonsensical either due to the function being piecewise or the derivative being $\frac{0}{0}$. So I considered taking instead the derivative between the two points adjacent to $x$ as represent in the function in the title above of $\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x-h\right)}{2h}$. But the question that I have is a valid redefinition? Some of my thoughts: If $$\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x-h\right)}{2h}=f'(x)$$ then $$\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x-h\right)}{2h}=\lim_{h\to0}\frac{f\left(x+h\right)-f\left(x\right)}{h}$$ further implying that $$f(x)=\lim_{h\to0}\frac{f(x+h)+f(x-h)}{2}$$ When I input this function in WolframAlpha I get an output telling me that this is true for all analytical functions and the series expansion approaches $f(x)$. If I remove the limit on $h$ WolframAlpha tells me that $f(x)$ must be even function, although I am skeptical of this.",,"['calculus', 'geometry', 'limits', 'derivatives']"
47,Existence of one-sided derivatives for a Lipschitz function,Existence of one-sided derivatives for a Lipschitz function,,"Let $f(x):\mathbb{R}\to[0,1]$ be Lipschitz continuous, differentiable everywhere except for $x=0$ and such that $f(0)=0$. My question is whether or not the ""left"" derivative  $\lim_{x\uparrow 0} \frac{f(x)}{x}$ and the ""right"" derivative  $\lim_{x\downarrow 0} \frac{f(x)}{x}$ always exist.","Let $f(x):\mathbb{R}\to[0,1]$ be Lipschitz continuous, differentiable everywhere except for $x=0$ and such that $f(0)=0$. My question is whether or not the ""left"" derivative  $\lim_{x\uparrow 0} \frac{f(x)}{x}$ and the ""right"" derivative  $\lim_{x\downarrow 0} \frac{f(x)}{x}$ always exist.",,"['real-analysis', 'derivatives', 'continuity', 'lipschitz-functions']"
48,Help proving an inequality with arcsin function using Mean value theorem,Help proving an inequality with arcsin function using Mean value theorem,,"So i have an inequality: $$ \frac{b-a}{\sqrt{1-a^2}} < \arcsin(b) - \arcsin(a) < \frac{b-a}{\sqrt{1-b^2}}$$  $$0\leq a < b < 1$$ So I kinda see that both sides look similar to derivative, but how can use it in here? This is probably going to use Lagrange mean value theorem, or am I mistaken? I wonder how can I use the fact: $$\frac{d}{dx}(arcsin x)= \frac{1}{\sqrt{1-x^2}}$$ So I kinda see that $$(b-1)(\arcsin(a))' ?<\arcsin(b)-\arcsin(a)<(b-1)(\arcsin(b))'$$ The right inequality, should work since it's Lagrange theorem usage on it? right? I wonder that to to the left side, since I put a question mark there. Is it just the same just because of the fact: $a<b$ that it works the other way around as the right side one? Any help regarding my solution would be helpful, if it's right or wrong.","So i have an inequality: $$ \frac{b-a}{\sqrt{1-a^2}} < \arcsin(b) - \arcsin(a) < \frac{b-a}{\sqrt{1-b^2}}$$  $$0\leq a < b < 1$$ So I kinda see that both sides look similar to derivative, but how can use it in here? This is probably going to use Lagrange mean value theorem, or am I mistaken? I wonder how can I use the fact: $$\frac{d}{dx}(arcsin x)= \frac{1}{\sqrt{1-x^2}}$$ So I kinda see that $$(b-1)(\arcsin(a))' ?<\arcsin(b)-\arcsin(a)<(b-1)(\arcsin(b))'$$ The right inequality, should work since it's Lagrange theorem usage on it? right? I wonder that to to the left side, since I put a question mark there. Is it just the same just because of the fact: $a<b$ that it works the other way around as the right side one? Any help regarding my solution would be helpful, if it's right or wrong.",,"['calculus', 'derivatives', 'inequality']"
49,What does the third derivative imply here?,What does the third derivative imply here?,,"Let $f:I \subseteq \mathbb{R} \rightarrow \mathbb{R}$ be a function that is (at least) three times differentiable on the interval $I$. Suppose that $f'''$ is continuous and that $f'(a)=f''(a)=0$ for an $a \in I$ but that $f'''(a) \neq 0$. Is it possible for $f$ to reach a local extremum in $a$? I reckon that it is not possible, seen as $f$ has an inflection point in $a$. But I'm not sure how to put this in a proof.","Let $f:I \subseteq \mathbb{R} \rightarrow \mathbb{R}$ be a function that is (at least) three times differentiable on the interval $I$. Suppose that $f'''$ is continuous and that $f'(a)=f''(a)=0$ for an $a \in I$ but that $f'''(a) \neq 0$. Is it possible for $f$ to reach a local extremum in $a$? I reckon that it is not possible, seen as $f$ has an inflection point in $a$. But I'm not sure how to put this in a proof.",,"['real-analysis', 'derivatives']"
50,Let $\{ x_n \}_{n=1}^{\infty}$ such that $x_{n+1}=x_n-x_n^3$ and $0<x_1<1$,Let  such that  and,\{ x_n \}_{n=1}^{\infty} x_{n+1}=x_n-x_n^3 0<x_1<1,"Let $\{ x_n \}_{n=1}^{\infty}$ such that $x_{n+1}=x_n-x_n^3$ and $0<x_1<1, \ \forall n\in \mathbb{N}.$ Prove: $\lim_{n \rightarrow \infty} x_n=0$ Calculate$\ \lim_{n \rightarrow \infty} nx_n^2$ Let $f$ be a differentiable in $\mathbb{R}$ such that $f(n)=x_n,\forall n\in \mathbb{N}.$ Prove that if $\lim_{x \rightarrow \infty} f'(x)$ exists, then it equals to $0$. I proved the first, but struggling with the next two. My intuition tells me that $\lim_{n \rightarrow \infty} nx_n^2=0$, and I tried to squeeze it but got stuck. I tried to prove by contradiction the third, and I managed to contradict the limit is greater than $0$, but couldn't get further. Any help appreciated.","Let $\{ x_n \}_{n=1}^{\infty}$ such that $x_{n+1}=x_n-x_n^3$ and $0<x_1<1, \ \forall n\in \mathbb{N}.$ Prove: $\lim_{n \rightarrow \infty} x_n=0$ Calculate$\ \lim_{n \rightarrow \infty} nx_n^2$ Let $f$ be a differentiable in $\mathbb{R}$ such that $f(n)=x_n,\forall n\in \mathbb{N}.$ Prove that if $\lim_{x \rightarrow \infty} f'(x)$ exists, then it equals to $0$. I proved the first, but struggling with the next two. My intuition tells me that $\lim_{n \rightarrow \infty} nx_n^2=0$, and I tried to squeeze it but got stuck. I tried to prove by contradiction the third, and I managed to contradict the limit is greater than $0$, but couldn't get further. Any help appreciated.",,"['calculus', 'sequences-and-series', 'limits', 'derivatives']"
51,Find the Derivative of $f(x)=\frac{7}{\sqrt {x}}$ using the definition.,Find the Derivative of  using the definition.,f(x)=\frac{7}{\sqrt {x}},"I get that $$\frac{d}{dx}\left(7\times\dfrac{1}{\sqrt{x}}\right)=\frac{d}{dx}(7x^{.5})=\dfrac{7}{2}x^{-.5}$$ is the derivative, but I can't ever use $\lim_{h \to 0} \dfrac{f(x+h)-f(x)}{h}$. If someone or anyone could go step by step and do the problem, I would be eternally grateful.","I get that $$\frac{d}{dx}\left(7\times\dfrac{1}{\sqrt{x}}\right)=\frac{d}{dx}(7x^{.5})=\dfrac{7}{2}x^{-.5}$$ is the derivative, but I can't ever use $\lim_{h \to 0} \dfrac{f(x+h)-f(x)}{h}$. If someone or anyone could go step by step and do the problem, I would be eternally grateful.",,"['calculus', 'derivatives']"
52,"Find $y'$ given $y\,\sin\,x^3=x\,\sin\,y^3$?",Find  given ?,"y' y\,\sin\,x^3=x\,\sin\,y^3","The problem is $$y\,\sin\,x^3=x\,\sin\,y^3$$ Find the $y'$ The answer is Can some explain how to do this, please help.","The problem is Find the The answer is Can some explain how to do this, please help.","y\,\sin\,x^3=x\,\sin\,y^3 y'","['calculus', 'derivatives', 'implicit-differentiation']"
53,"Given that $\cos(x/2)\cos(x/4)\cos(x/8)\ldots=(\sin x)/x,$ prove that $(1/2^2)\sec^2(x/2)+(1/2^4)\sec^2(x/4)\ldots=\csc^2(x) - (1/x^2).$",Given that  prove that,"\cos(x/2)\cos(x/4)\cos(x/8)\ldots=(\sin x)/x, (1/2^2)\sec^2(x/2)+(1/2^4)\sec^2(x/4)\ldots=\csc^2(x) - (1/x^2).",How to solve this one I know this is related to  differentiation but how to proceed with this??? Please give all steps so that it is easily understood.,How to solve this one I know this is related to  differentiation but how to proceed with this??? Please give all steps so that it is easily understood.,,['derivatives']
54,Gradient of a softmax applied on a linear function,Gradient of a softmax applied on a linear function,,"I am trying to calculate the softmax gradient: $$p_j=[f(\vec{x})]_j = \frac{e^{W_jx+b_j}}{\sum_k e^{W_kx+b_k}}$$ With the cross-entropy error: $$L = -\sum_j y_j \log p_j$$ Using this question I get that $$\frac{\partial L}{\partial o_i} = p_i - y_i$$ Where $o_i=W_ix+b_i$ So, by applying the chain rule I get to: $$\frac{\partial L}{\partial b_i}=\frac{\partial L}{\partial o_i}\frac{\partial o_i}{\partial b_i} = (p_i - y_i)1=p_i - y_i$$ Which makes sense (dimensionality wise) $$\frac{\partial L}{\partial W_i}=\frac{\partial L}{\partial o_i}\frac{\partial o_i}{\partial W_i} = (p_i - y_i)\vec{x}$$ Which has a dimensionality mismatch (for example if dimensions are $W_{3\times 4},\vec{b}_4,\vec{x}_3$) What am I doing wrong ? and what is the correct gradient ?","I am trying to calculate the softmax gradient: $$p_j=[f(\vec{x})]_j = \frac{e^{W_jx+b_j}}{\sum_k e^{W_kx+b_k}}$$ With the cross-entropy error: $$L = -\sum_j y_j \log p_j$$ Using this question I get that $$\frac{\partial L}{\partial o_i} = p_i - y_i$$ Where $o_i=W_ix+b_i$ So, by applying the chain rule I get to: $$\frac{\partial L}{\partial b_i}=\frac{\partial L}{\partial o_i}\frac{\partial o_i}{\partial b_i} = (p_i - y_i)1=p_i - y_i$$ Which makes sense (dimensionality wise) $$\frac{\partial L}{\partial W_i}=\frac{\partial L}{\partial o_i}\frac{\partial o_i}{\partial W_i} = (p_i - y_i)\vec{x}$$ Which has a dimensionality mismatch (for example if dimensions are $W_{3\times 4},\vec{b}_4,\vec{x}_3$) What am I doing wrong ? and what is the correct gradient ?",,"['linear-algebra', 'derivatives', 'partial-derivative', 'gradient-descent']"
55,How did the author take this derivative?,How did the author take this derivative?,,"From Taylor's Classical Mechanics, pg. 220: Given: $f(y+\eta (x)\alpha,y'+\eta (x) '\alpha,x)$ Derivative: $\frac{\partial f(y+\eta (x)\alpha,y'+\eta (x) '\alpha,x)}{\partial \alpha}=\eta \frac{\partial f}{\partial y}+\eta' \frac{\partial f}{\partial y'}$ I guess I can see how the derivative is a summation of 2 terms - it's because there are 2 terms in the argument for the function that depend on $\alpha$ . However, I don't see where the $\frac{\partial f}{\partial y}$ and the $\frac{\partial f}{\partial y'}$ come from. I would have just said that the derivative with respect to $\alpha$ is $\eta + \eta '$ . Update: My main question is just this now: why does the author take the derivative of $f$ w.r.t $y$ ? He wanted to find the derivative with respect to a parameter that introduced, which is $\alpha$ .","From Taylor's Classical Mechanics, pg. 220: Given: Derivative: I guess I can see how the derivative is a summation of 2 terms - it's because there are 2 terms in the argument for the function that depend on . However, I don't see where the and the come from. I would have just said that the derivative with respect to is . Update: My main question is just this now: why does the author take the derivative of w.r.t ? He wanted to find the derivative with respect to a parameter that introduced, which is .","f(y+\eta (x)\alpha,y'+\eta (x) '\alpha,x) \frac{\partial f(y+\eta (x)\alpha,y'+\eta (x) '\alpha,x)}{\partial \alpha}=\eta \frac{\partial f}{\partial y}+\eta' \frac{\partial f}{\partial y'} \alpha \frac{\partial f}{\partial y} \frac{\partial f}{\partial y'} \alpha \eta + \eta ' f y \alpha","['calculus', 'derivatives', 'partial-derivative', 'calculus-of-variations']"
56,Let $f(x)=\sum_{n=1}^\infty \frac{(-1)^n}{\sqrt{n}}e^{\frac{x}{n}}$.,Let .,f(x)=\sum_{n=1}^\infty \frac{(-1)^n}{\sqrt{n}}e^{\frac{x}{n}},Let $f(x)=\sum_{n=1}^\infty \frac{(-1)^n}{\sqrt{n}}e^{\frac{x}{n}}$. Is f differentiable on $\mathbb{R}$? I can prove  the series does not uniformly convergent on $\mathbb{R}$. But this does not imply f is not differentiable.,Let $f(x)=\sum_{n=1}^\infty \frac{(-1)^n}{\sqrt{n}}e^{\frac{x}{n}}$. Is f differentiable on $\mathbb{R}$? I can prove  the series does not uniformly convergent on $\mathbb{R}$. But this does not imply f is not differentiable.,,"['real-analysis', 'sequences-and-series', 'derivatives', 'convergence-divergence']"
57,What justifies writing the chain rule as $\frac{d}{dx}=\frac{dy}{dx}\frac{d}{dy}$ when there is no function for it to operate on?,What justifies writing the chain rule as  when there is no function for it to operate on?,\frac{d}{dx}=\frac{dy}{dx}\frac{d}{dy},"This previous question of mine has lead me to ask the following question: It was my understanding that the chain rule $$\dfrac{du}{dx}=\dfrac{dy}{dx}\dfrac{du}{dy}$$ only makes sense when there is some function $u$ for it to operate on. So how can we possibly justify writing $$\dfrac{d}{dx}=\dfrac{dy}{dx}\dfrac{d}{dy}?$$ One of the answers to the previous question mentioned that if $$\frac{du}{dx}=\frac{du}{dy}\sqrt{\frac{m\,\omega_0}{\hbar}}$$ then just looking at  the operator $\frac{d}{dx}$ without explicitly addressing the function $u$ the operator is to apply:   \begin{align*} \frac{d}{dx}&=\sqrt{\frac{m\,\omega_0}{\hbar}}\frac{d}{dy} \end{align*} In one of the comments given to the other answer to the previous question states that I find it easier to look at what is $\dfrac{du}{dx}$ is in terms of $y$ this means working out the derivative of $u(y(x))$ to do this you use the chain rule - so we have   $$\frac{du}{dx} =\frac{dy}{dx}\frac{du}{dy}$$   so looking at the operator part I.e ""ignore"" the $u$. This is how my physicist brain computes changes of variables. But I am finding it hard to accept that we simply ""ignore"" the $u$. I realize that we cannot simply cancel out the $u$ since we are not guaranteed that $u\ne 0$. Is there a more plausible explanation as to why we may write $$\dfrac{d}{dx}=\dfrac{dy}{dx}\dfrac{d}{dy}$$ in the absence of a function to operate upon? Regards.","This previous question of mine has lead me to ask the following question: It was my understanding that the chain rule $$\dfrac{du}{dx}=\dfrac{dy}{dx}\dfrac{du}{dy}$$ only makes sense when there is some function $u$ for it to operate on. So how can we possibly justify writing $$\dfrac{d}{dx}=\dfrac{dy}{dx}\dfrac{d}{dy}?$$ One of the answers to the previous question mentioned that if $$\frac{du}{dx}=\frac{du}{dy}\sqrt{\frac{m\,\omega_0}{\hbar}}$$ then just looking at  the operator $\frac{d}{dx}$ without explicitly addressing the function $u$ the operator is to apply:   \begin{align*} \frac{d}{dx}&=\sqrt{\frac{m\,\omega_0}{\hbar}}\frac{d}{dy} \end{align*} In one of the comments given to the other answer to the previous question states that I find it easier to look at what is $\dfrac{du}{dx}$ is in terms of $y$ this means working out the derivative of $u(y(x))$ to do this you use the chain rule - so we have   $$\frac{du}{dx} =\frac{dy}{dx}\frac{du}{dy}$$   so looking at the operator part I.e ""ignore"" the $u$. This is how my physicist brain computes changes of variables. But I am finding it hard to accept that we simply ""ignore"" the $u$. I realize that we cannot simply cancel out the $u$ since we are not guaranteed that $u\ne 0$. Is there a more plausible explanation as to why we may write $$\dfrac{d}{dx}=\dfrac{dy}{dx}\dfrac{d}{dy}$$ in the absence of a function to operate upon? Regards.",,"['calculus', 'derivatives', 'chain-rule']"
58,Find n-th derivative of $f(x)=\sqrt{1+\cosh{x}}$,Find n-th derivative of,f(x)=\sqrt{1+\cosh{x}},"$f(x)=\sqrt{1+\cosh{x}}$ $f'(x)=\frac{\sinh{x}}{2\sqrt{\cosh{x}+1}}$ $f''(x)=\frac{(\cosh{\frac{x}{2}})^4}{(\cosh{x}+1)^\frac{3}{2}}$ I wanted to use Leibnitz rule, but i can't seem to find any similarities between derivatives. I don't know how to expand $\cosh{x}$ as Taylor series. These are the only two general methods i know for finding upper derivatives. I think there must be some trick. Anyone has any ideas? This is what i got so far. $$f(x)=\frac{\sum_{n=0}^\infty ( \frac{(\frac{x}{2})^n}{n!}+\frac{(\frac{-x}{2})^n}{n!})}{\sqrt{2}}=\frac{\sum_{k=0}^\infty \frac{2(\frac{x}{2})^{2k}}{(2k)!}}{\sqrt{2}}$$ Dunno what to do now New approach. $f(x)=\sqrt{2}\cosh{\frac{x}{2}}$ $f'(x)=\frac{\sqrt{2}}{2}\sinh{\frac{x}{2}}$ $f''(x)=\frac{\sqrt{2}}{4}\cosh{\frac{x}{2}}=\frac{1}{4}f(x)$ For $n$ even $f^{(n)}=\frac{1}{2^n}f(x)=\frac{\sqrt{2}}{2^n}\cosh{\frac{x}{2}}$ For $n$ odd $f^{(n)}=\frac{\sqrt{2}}{2^n}\sinh{\frac{x}{2}}$","$f(x)=\sqrt{1+\cosh{x}}$ $f'(x)=\frac{\sinh{x}}{2\sqrt{\cosh{x}+1}}$ $f''(x)=\frac{(\cosh{\frac{x}{2}})^4}{(\cosh{x}+1)^\frac{3}{2}}$ I wanted to use Leibnitz rule, but i can't seem to find any similarities between derivatives. I don't know how to expand $\cosh{x}$ as Taylor series. These are the only two general methods i know for finding upper derivatives. I think there must be some trick. Anyone has any ideas? This is what i got so far. $$f(x)=\frac{\sum_{n=0}^\infty ( \frac{(\frac{x}{2})^n}{n!}+\frac{(\frac{-x}{2})^n}{n!})}{\sqrt{2}}=\frac{\sum_{k=0}^\infty \frac{2(\frac{x}{2})^{2k}}{(2k)!}}{\sqrt{2}}$$ Dunno what to do now New approach. $f(x)=\sqrt{2}\cosh{\frac{x}{2}}$ $f'(x)=\frac{\sqrt{2}}{2}\sinh{\frac{x}{2}}$ $f''(x)=\frac{\sqrt{2}}{4}\cosh{\frac{x}{2}}=\frac{1}{4}f(x)$ For $n$ even $f^{(n)}=\frac{1}{2^n}f(x)=\frac{\sqrt{2}}{2^n}\cosh{\frac{x}{2}}$ For $n$ odd $f^{(n)}=\frac{\sqrt{2}}{2^n}\sinh{\frac{x}{2}}$",,"['real-analysis', 'derivatives']"
59,Why does the error term tend to zero faster than the difference?,Why does the error term tend to zero faster than the difference?,,"In the differential approximation for f we have an error term ""E"" which tends to zero faster than the difference on the x axis ""h"" thus we can use the little o notation My question is how do we know that the error term approaches zero faster than the difference when x (or ""a"" int the first picture) is approaching x0? And why do we bother to use the little o notation? Is it easier to use it in multi-variable functions?","In the differential approximation for f we have an error term ""E"" which tends to zero faster than the difference on the x axis ""h"" thus we can use the little o notation My question is how do we know that the error term approaches zero faster than the difference when x (or ""a"" int the first picture) is approaching x0? And why do we bother to use the little o notation? Is it easier to use it in multi-variable functions?",,"['analysis', 'derivatives']"
60,Differentiate and simplify. $m(x) = \frac{x}{\sqrt{4x-3}}$,Differentiate and simplify.,m(x) = \frac{x}{\sqrt{4x-3}},My work so far is: \begin{align} m'(x) &= \frac{(1)(\sqrt{4x-3})-(x)(1/2)(4x-3)^{-1/2}(4)}{(\sqrt{4x-3})^2} \\ &= \frac{\sqrt{4x-3} - 2x(4x-3)^{1/2}}{4x-3} \end{align} and now I'm stuck on how to simplify further,My work so far is: \begin{align} m'(x) &= \frac{(1)(\sqrt{4x-3})-(x)(1/2)(4x-3)^{-1/2}(4)}{(\sqrt{4x-3})^2} \\ &= \frac{\sqrt{4x-3} - 2x(4x-3)^{1/2}}{4x-3} \end{align} and now I'm stuck on how to simplify further,,"['calculus', 'derivatives']"
61,Critical points of a cubic function,Critical points of a cubic function,,"There is a function $x^3 - 6x^2 + 9x + 1$. Its critical points are $1$ and $3$. I am very confused, if these points are maximum and minimum points respectively or are both inflection points. Can someone please help me with these ? These points can not be maximum and minimum points since function attains higher and lower values as compared to what the function attains at these $2$ points. Please correct me if I am wrong.","There is a function $x^3 - 6x^2 + 9x + 1$. Its critical points are $1$ and $3$. I am very confused, if these points are maximum and minimum points respectively or are both inflection points. Can someone please help me with these ? These points can not be maximum and minimum points since function attains higher and lower values as compared to what the function attains at these $2$ points. Please correct me if I am wrong.",,"['calculus', 'derivatives']"
62,How to derive through a convolution?,How to derive through a convolution?,,"Let $f(t) = \alpha e^{-\beta t}$, where $\alpha, \beta$ are constants Let $g(t) = y(t)$ Then the resulting convolution $f\ast g$ is: $$f \ast g = \int_0^t \alpha  e^{-\beta (t-\tau)} y(\tau) d\tau$$ Does anyone know how one would take the derivative of this expression? In general, are there rules for taking derivative of $f \ast g$, for some given $f,g$? Idea: using fundamental theorem of calculus, treat the integrand as a   single function $h(t)$ $$\int_0^t \alpha  e^{-\beta (t-\tau)} y(\tau) d\tau = \int_0^t  h(\tau) d\tau$$","Let $f(t) = \alpha e^{-\beta t}$, where $\alpha, \beta$ are constants Let $g(t) = y(t)$ Then the resulting convolution $f\ast g$ is: $$f \ast g = \int_0^t \alpha  e^{-\beta (t-\tau)} y(\tau) d\tau$$ Does anyone know how one would take the derivative of this expression? In general, are there rules for taking derivative of $f \ast g$, for some given $f,g$? Idea: using fundamental theorem of calculus, treat the integrand as a   single function $h(t)$ $$\int_0^t \alpha  e^{-\beta (t-\tau)} y(\tau) d\tau = \int_0^t  h(\tau) d\tau$$",,"['calculus', 'integration', 'derivatives', 'laplace-transform', 'convolution']"
63,"Given a rocket with constant acceleration after t = 4, when will it hit the ground?","Given a rocket with constant acceleration after t = 4, when will it hit the ground?",,"A rocket is launched straight upward. During the first four seconds of powered flight, its height is given by: $h(t) = 16.1t^2 − 1.75t^3$ The function is valid when $0 ≤ t ≤ 4$ $t$ in seconds and $h$ in feet. At the instant when $t = 4$ seconds, the fuel cuts off. From that point in time onward, the rocket has constant acceleration of $-32.2 ft/s^2$. When does it hit the ground? Given this information I have hypothesized: $h'(t) = -32.2t$ $h''(t) = -32.2$ At the instant fuel cuts off $h(4)=145.6$ Taking two anti-derivatives, the $h(t)$ for $t > 4$ is now: $h(t)= -16.1t^2+403.2$ Solving for $h(t)=0$ at $t ≥ 4$ gives $t=5.004$ but this is not the correct answer. Can someone help me understand what I'm missing? Graph of $h(t)$ and $y=0$ intersection provided: https://www.desmos.com/calculator/hddvv1de1z","A rocket is launched straight upward. During the first four seconds of powered flight, its height is given by: $h(t) = 16.1t^2 − 1.75t^3$ The function is valid when $0 ≤ t ≤ 4$ $t$ in seconds and $h$ in feet. At the instant when $t = 4$ seconds, the fuel cuts off. From that point in time onward, the rocket has constant acceleration of $-32.2 ft/s^2$. When does it hit the ground? Given this information I have hypothesized: $h'(t) = -32.2t$ $h''(t) = -32.2$ At the instant fuel cuts off $h(4)=145.6$ Taking two anti-derivatives, the $h(t)$ for $t > 4$ is now: $h(t)= -16.1t^2+403.2$ Solving for $h(t)=0$ at $t ≥ 4$ gives $t=5.004$ but this is not the correct answer. Can someone help me understand what I'm missing? Graph of $h(t)$ and $y=0$ intersection provided: https://www.desmos.com/calculator/hddvv1de1z",,"['calculus', 'derivatives', 'physics']"
64,The set where a derivative vanishes is G-delta,The set where a derivative vanishes is G-delta,,"If $f:I\to R$ ($I$ - interval) is differentiable, then $\{x\colon f'(x)=0\}$ is a $G_{\delta}$ set. The lecturer didn't prove this fact and I found no proof in my books. How it can be proven?","If $f:I\to R$ ($I$ - interval) is differentiable, then $\{x\colon f'(x)=0\}$ is a $G_{\delta}$ set. The lecturer didn't prove this fact and I found no proof in my books. How it can be proven?",,"['real-analysis', 'general-topology', 'derivatives']"
65,Suppose $f(0) = f(1) = 0$ and $f(x_0) = 1$. Show that there is $\rho$ with $\lvert f'(\rho) \rvert > 2$.,Suppose  and . Show that there is  with .,f(0) = f(1) = 0 f(x_0) = 1 \rho \lvert f'(\rho) \rvert > 2,"Suppose that $f : [0; 1] \rightarrow \mathbb{R}$ is continous and differentiable on $(0,1)$, that $f(0) = f(1) = 0$, and that $\exists_{x_0 \in (0; 1)} f(x_0) = 1$. Prove that $\exists_{\rho \in (0;1)}|f'(\rho)| > 2$ Specifically, I cannot prove that $\exists_{\rho \in (0;1)}|f'(\rho)| > 2$ and not just $\exists_{\rho \in (0;1)}|f'(\rho)| \ge 2$ - it gets problematic when $\rho = \frac{1}{2}$. There's been two similar questions asked about this problem, but none of the answers seem to be complete: Suppose $f(0) = f(1) = 0$ and $f(x_0) = 1$. Show that there is $\rho$ with $\lvert f'(\rho) \rvert \geq 2$. https://math.stackexchange.com/questions/1747287/f0-f1-0-f-frac12-1-find-such-c-that-fc-2 Unfortunately, I cannot post comments to these answers as my rank is too low.","Suppose that $f : [0; 1] \rightarrow \mathbb{R}$ is continous and differentiable on $(0,1)$, that $f(0) = f(1) = 0$, and that $\exists_{x_0 \in (0; 1)} f(x_0) = 1$. Prove that $\exists_{\rho \in (0;1)}|f'(\rho)| > 2$ Specifically, I cannot prove that $\exists_{\rho \in (0;1)}|f'(\rho)| > 2$ and not just $\exists_{\rho \in (0;1)}|f'(\rho)| \ge 2$ - it gets problematic when $\rho = \frac{1}{2}$. There's been two similar questions asked about this problem, but none of the answers seem to be complete: Suppose $f(0) = f(1) = 0$ and $f(x_0) = 1$. Show that there is $\rho$ with $\lvert f'(\rho) \rvert \geq 2$. https://math.stackexchange.com/questions/1747287/f0-f1-0-f-frac12-1-find-such-c-that-fc-2 Unfortunately, I cannot post comments to these answers as my rank is too low.",,"['calculus', 'derivatives']"
66,"Find the derivative of I(x) = $\int _{\sin\left(x\right)}^{\cos\left(x\right)}\arctan\left(t^2\right)\,dt$",Find the derivative of I(x) =,"\int _{\sin\left(x\right)}^{\cos\left(x\right)}\arctan\left(t^2\right)\,dt","$$I'(x)= \frac{d}{dx}\left(\int_{\sin\left(x\right)}^{\cos\left(x\right)}\arctan\left(t^2\right)\,dt\right)$$ I'm not sure how to approach this problem, initially I thought to use the Fundamental theorem with the substitution $u = \cos(x)$ but then I still can't use the Fundamental theorem because of the $\sin(x)$. A point in the right direction would be appreciated.","$$I'(x)= \frac{d}{dx}\left(\int_{\sin\left(x\right)}^{\cos\left(x\right)}\arctan\left(t^2\right)\,dt\right)$$ I'm not sure how to approach this problem, initially I thought to use the Fundamental theorem with the substitution $u = \cos(x)$ but then I still can't use the Fundamental theorem because of the $\sin(x)$. A point in the right direction would be appreciated.",,"['calculus', 'derivatives']"
67,How to show $f$ is non-negative.,How to show  is non-negative.,f,I am stuck in the following problem. Can someone show me how shall I finish the track ? the problem is : if $f:\mathbb{R}\rightarrow \mathbb{R}$ twice differentiable function and $f(0)=f'(0)=1$ and $f''\geqslant f$. Then show that $f$ is non-negative. Thank you in advance.,I am stuck in the following problem. Can someone show me how shall I finish the track ? the problem is : if $f:\mathbb{R}\rightarrow \mathbb{R}$ twice differentiable function and $f(0)=f'(0)=1$ and $f''\geqslant f$. Then show that $f$ is non-negative. Thank you in advance.,,"['real-analysis', 'derivatives']"
68,"Are there any pairs of functions where $g(n,x)=f^{(n)}(x)$?",Are there any pairs of functions where ?,"g(n,x)=f^{(n)}(x)","Are there any non-piecewise pairs of functions that satisfy this quality? $g(n,x)=f^{(n)}(x)$ Where $n\in \Bbb{Z}$ and is the $n^{th}$ derivitive of $f(x)$ This is a long shot but I'm just curious",Are there any non-piecewise pairs of functions that satisfy this quality? Where and is the derivitive of This is a long shot but I'm just curious,"g(n,x)=f^{(n)}(x) n\in \Bbb{Z} n^{th} f(x)",['calculus']
69,A little advice on using the chain rule for differentiation,A little advice on using the chain rule for differentiation,,"I must be a little rusty, but how would I evaluate the following: $$ \frac{d}{dr}\left(1-\frac{b(r)}{r}\right)^{-1} $$ My stickler is that $b$ is a function of $r$...","I must be a little rusty, but how would I evaluate the following: $$ \frac{d}{dr}\left(1-\frac{b(r)}{r}\right)^{-1} $$ My stickler is that $b$ is a function of $r$...",,"['calculus', 'derivatives', 'chain-rule']"
70,Polar form of Laplacian operator. [closed],Polar form of Laplacian operator. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove   $$ \frac{\partial^2 u}{\partial x^2}+\frac{\partial^2 u}{\partial y^2}=\frac{1}{r}\frac{\partial}{\partial r}\left(r\frac{\partial u}{\partial r}\right)+\frac{1}{r^2}\left(\frac{\partial^2 u}{\partial \theta^2}\right) $$   for $u=f(x,y)$ Where should I start from? $u$ is a general function of $x$ and $y$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove   $$ \frac{\partial^2 u}{\partial x^2}+\frac{\partial^2 u}{\partial y^2}=\frac{1}{r}\frac{\partial}{\partial r}\left(r\frac{\partial u}{\partial r}\right)+\frac{1}{r^2}\left(\frac{\partial^2 u}{\partial \theta^2}\right) $$   for $u=f(x,y)$ Where should I start from? $u$ is a general function of $x$ and $y$?",,"['derivatives', 'partial-derivative', 'polar-coordinates', 'laplacian']"
71,"Show that $f$ is not differentiable at $(0,0)$ - $\frac{x_1^2x_2}{x_1^2+x_2^2}$",Show that  is not differentiable at  -,"f (0,0) \frac{x_1^2x_2}{x_1^2+x_2^2}","Let the function $f: \mathbb{R^2} \to \mathbb{R}$ defined as $$f(x) =    \begin{cases}    \frac{x_1^2x_2}{x_1^2+x_2^2},       & \quad \text{if } (x_1,x_2) \not= 0  \\  0,  & \quad \text{if } (x_1,x_2) = 0  \\    \end{cases} $$ Show that $f$ is not differentiable at $(0,0)$. I think I can use the directionnal derivative or the polar coordinate, but I don't know how. Is anyone is able to explain to me how I can use these properties? If I can't, could you propose to me an alternative? P.S. Please, don't try to use a very specific analysis theory; I am only an undergraduate student (bachelor).","Let the function $f: \mathbb{R^2} \to \mathbb{R}$ defined as $$f(x) =    \begin{cases}    \frac{x_1^2x_2}{x_1^2+x_2^2},       & \quad \text{if } (x_1,x_2) \not= 0  \\  0,  & \quad \text{if } (x_1,x_2) = 0  \\    \end{cases} $$ Show that $f$ is not differentiable at $(0,0)$. I think I can use the directionnal derivative or the polar coordinate, but I don't know how. Is anyone is able to explain to me how I can use these properties? If I can't, could you propose to me an alternative? P.S. Please, don't try to use a very specific analysis theory; I am only an undergraduate student (bachelor).",,"['real-analysis', 'derivatives']"
72,Proving that the exponential inequality $e^x \ge x^e$ holds for all $x \ge 0$ [duplicate],Proving that the exponential inequality  holds for all  [duplicate],e^x \ge x^e x \ge 0,"This question already has answers here : Why $e^x$ is always greater than $x^e$? (8 answers) Closed 8 years ago . How does one prove that $$e^x \ge x^e$$ for all $x \ge 0$? I tried to do this by setting $f(x)=e^x-x^e$ Plotting this function shows this easily, as seen here . However, when I tried to prove this, it proved quite difficult. It seems to be increasing for $0 \le x \le e$, and seems to be increasing for all $x \ge e$. I tried to use that $f'(x)=e^x-ex^{e-1}$ but was not able to. Any help would be appreciated.","This question already has answers here : Why $e^x$ is always greater than $x^e$? (8 answers) Closed 8 years ago . How does one prove that $$e^x \ge x^e$$ for all $x \ge 0$? I tried to do this by setting $f(x)=e^x-x^e$ Plotting this function shows this easily, as seen here . However, when I tried to prove this, it proved quite difficult. It seems to be increasing for $0 \le x \le e$, and seems to be increasing for all $x \ge e$. I tried to use that $f'(x)=e^x-ex^{e-1}$ but was not able to. Any help would be appreciated.",,"['calculus', 'derivatives', 'inequality', 'exponential-function']"
73,Second derivative of $x^3+y^3=1$ using implicit differentiation,Second derivative of  using implicit differentiation,x^3+y^3=1,"I need to find the $D_x^2y$ of     $x^3+y^3=1$ using implicit differentiation So, $$ x^3 + y^3 =1 \\ 3x^2+3y^2 \cdot D_xy = 0 \\ 3y^2 \cdot D_xy= -3x^2 \\ D_xy = - {x^2 \over y^2} $$ Now I need to find the $D_x^2y$. I am pretty sure that means the second derivative. How would I do it to find the second derivative? apparently, it is supposed to be$$-{2x \over y^5}$$","I need to find the $D_x^2y$ of     $x^3+y^3=1$ using implicit differentiation So, $$ x^3 + y^3 =1 \\ 3x^2+3y^2 \cdot D_xy = 0 \\ 3y^2 \cdot D_xy= -3x^2 \\ D_xy = - {x^2 \over y^2} $$ Now I need to find the $D_x^2y$. I am pretty sure that means the second derivative. How would I do it to find the second derivative? apparently, it is supposed to be$$-{2x \over y^5}$$",,"['calculus', 'algebra-precalculus', 'derivatives', 'implicit-differentiation']"
74,Related Rates - Distance between two ships,Related Rates - Distance between two ships,,"At noon, a vessel is sailing due north at the uniform rate of $15$ kilometers per hour. Another vessel, $30$ km due north of the first vessel, is sailing due east at the uniform rate of $20$ kilometers per hour. At what rate is the distance between the vessels changing at the end of an hour? Here's what I thought. $$dy/dt = 15 \;\text{km/hr}$$ $$dx/dt = 20 \;\text{km/hr}$$ $$x = 30 \;\text{km}$$ $y$ is unknown $z$ is unknown The first ship purely sailed from south to north since it's stated ""due north"" on the problem. The second one, was a tough nut to crack for me. I was hoping that the second ship sailed as well from south to north but changed its direction from north to east. My problem is depicting the diagram. How would it look like? By the way, I let z be the distance that would serve as the main subject for the problem. Am I also wrong on citing my analysis on the problem?","At noon, a vessel is sailing due north at the uniform rate of $15$ kilometers per hour. Another vessel, $30$ km due north of the first vessel, is sailing due east at the uniform rate of $20$ kilometers per hour. At what rate is the distance between the vessels changing at the end of an hour? Here's what I thought. $$dy/dt = 15 \;\text{km/hr}$$ $$dx/dt = 20 \;\text{km/hr}$$ $$x = 30 \;\text{km}$$ $y$ is unknown $z$ is unknown The first ship purely sailed from south to north since it's stated ""due north"" on the problem. The second one, was a tough nut to crack for me. I was hoping that the second ship sailed as well from south to north but changed its direction from north to east. My problem is depicting the diagram. How would it look like? By the way, I let z be the distance that would serve as the main subject for the problem. Am I also wrong on citing my analysis on the problem?",,"['calculus', 'derivatives']"
75,Growth rates slower than logarithmic? [closed],Growth rates slower than logarithmic? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question So far, I've been able to determine growth rates using the following limit:$$\lim_{x\to\infty}\frac{f(x)}{g(x)}$$Which, if need be, can be solved with calculus. From this, I deduced that it is very difficult to have a function $f(x)$ that grows slower than a function $g(x)=\ln(x)$. I noted a few, the Lambert W function, or something simple like $\sqrt{\ln(x)}$.  This means we are excluding composite functions. However, I wondered if there are any ""parent"" functions that have slower growth rates than a logarithm. This is excluding non-elementary things like the Lambert W function or the inverse factorial. And by ""parent"" function, I mean you cannot combine multiple $x$'s or apply multiple operations to $x$. That is, we can't try to do something silly like $\ln[\ln(x)]$ or $x-\ln(x)$. One function, one operation, elementary functions only.  By elementary functions, I define an elementary function as being analytic and algebraic, unlike the inverse factorial or Lambert W function or similar things. Can we have growth slower than logarithmic under these restrictions? EDIT One last restriction.  The proposed function $g(x)$ must have the following:$$\lim_{x\to\infty}g(x)=\infty$$ Because it has come to my attention that horizontal asymptotes may apply.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question So far, I've been able to determine growth rates using the following limit:$$\lim_{x\to\infty}\frac{f(x)}{g(x)}$$Which, if need be, can be solved with calculus. From this, I deduced that it is very difficult to have a function $f(x)$ that grows slower than a function $g(x)=\ln(x)$. I noted a few, the Lambert W function, or something simple like $\sqrt{\ln(x)}$.  This means we are excluding composite functions. However, I wondered if there are any ""parent"" functions that have slower growth rates than a logarithm. This is excluding non-elementary things like the Lambert W function or the inverse factorial. And by ""parent"" function, I mean you cannot combine multiple $x$'s or apply multiple operations to $x$. That is, we can't try to do something silly like $\ln[\ln(x)]$ or $x-\ln(x)$. One function, one operation, elementary functions only.  By elementary functions, I define an elementary function as being analytic and algebraic, unlike the inverse factorial or Lambert W function or similar things. Can we have growth slower than logarithmic under these restrictions? EDIT One last restriction.  The proposed function $g(x)$ must have the following:$$\lim_{x\to\infty}g(x)=\infty$$ Because it has come to my attention that horizontal asymptotes may apply.",,"['calculus', 'derivatives', 'logarithms', 'recreational-mathematics']"
76,Geometrical interpretation of $\lim_{h \to 0} \frac{f(x_0+h) - f(x_0+h) }{2h}$,Geometrical interpretation of,\lim_{h \to 0} \frac{f(x_0+h) - f(x_0+h) }{2h},"Let $f:D \subset \Bbb R\to \Bbb R$ is $f$ differentiable at $x_0$ Is it true that: $$f'(x_0)=\lim_{h \to 0} \frac{f(x_0+h) - f(x_0+h) }{2h}$$ $$f'(x_0)=\lim_{h \to 0} \frac{f(x_0+2h) - f(x_0)}{h} \text{ ?}$$ Write the geometrical interpretation of each case. For the first one I have already proved that are equals and the reciprocal is not true, but I do not know what is the geometrical interpretation. Any ideas?","Let $f:D \subset \Bbb R\to \Bbb R$ is $f$ differentiable at $x_0$ Is it true that: $$f'(x_0)=\lim_{h \to 0} \frac{f(x_0+h) - f(x_0+h) }{2h}$$ $$f'(x_0)=\lim_{h \to 0} \frac{f(x_0+2h) - f(x_0)}{h} \text{ ?}$$ Write the geometrical interpretation of each case. For the first one I have already proved that are equals and the reciprocal is not true, but I do not know what is the geometrical interpretation. Any ideas?",,"['calculus', 'real-analysis', 'derivatives']"
77,f(a) = a using the mean value theorem,f(a) = a using the mean value theorem,,"Say that $f$ is differentiable and that the derivative of $f$ does not equal $1$ on $(-\infty, \infty)$. Show that there is at most one real number a such that $f(a)=a$. In order to solve this I am required to use the mean value theorem. I understand that this will be true when $f(x)=x$, or when $f(x)-x = 0$. Thus if I could show that $f(x) -x !=0$ at any point this would be proven. Let $g(x) = f(x)-x$ $\dfrac{\mathrm dg}{\mathrm dx} = \dfrac{\mathrm df(x)}{\mathrm dx}$, which does not equal one $\dfrac{\mathrm df(x)}{\mathrm dx} \neq \dfrac{f(b)-f(a)}{b-a}$, by the mean value theorem thus: $b-a\neq f(b)-f(a)$ This is as far as I can get. I know I'm really close, and any help would be greatly appreciated.","Say that $f$ is differentiable and that the derivative of $f$ does not equal $1$ on $(-\infty, \infty)$. Show that there is at most one real number a such that $f(a)=a$. In order to solve this I am required to use the mean value theorem. I understand that this will be true when $f(x)=x$, or when $f(x)-x = 0$. Thus if I could show that $f(x) -x !=0$ at any point this would be proven. Let $g(x) = f(x)-x$ $\dfrac{\mathrm dg}{\mathrm dx} = \dfrac{\mathrm df(x)}{\mathrm dx}$, which does not equal one $\dfrac{\mathrm df(x)}{\mathrm dx} \neq \dfrac{f(b)-f(a)}{b-a}$, by the mean value theorem thus: $b-a\neq f(b)-f(a)$ This is as far as I can get. I know I'm really close, and any help would be greatly appreciated.",,['derivatives']
78,What is a quicker method to differentiate $h(x)=2x-|3x-3|+|x^2-1|$?,What is a quicker method to differentiate ?,h(x)=2x-|3x-3|+|x^2-1|,$$h(x)=2x-|3x-3|+|x^2-1|$$ My Approach: $$u=|3x-3|$$ $$u^2=(3x-3)^2$$ $$2 u'=2(3x-3)\cdot3$$ $$u'=3(3x-3)$$ $$m=|x^2-1|$$ $$m^2=(x^2-1)^2$$ $$2m'=2(x^2-1)\cdot 2x$$ $$m'=2x(x^2-1)$$ $$\therefore h'(x)=2-3(3x-3)+2x(x^2-1)$$ Quicker and shorter method?,$$h(x)=2x-|3x-3|+|x^2-1|$$ My Approach: $$u=|3x-3|$$ $$u^2=(3x-3)^2$$ $$2 u'=2(3x-3)\cdot3$$ $$u'=3(3x-3)$$ $$m=|x^2-1|$$ $$m^2=(x^2-1)^2$$ $$2m'=2(x^2-1)\cdot 2x$$ $$m'=2x(x^2-1)$$ $$\therefore h'(x)=2-3(3x-3)+2x(x^2-1)$$ Quicker and shorter method?,,"['calculus', 'derivatives']"
79,How to show that $\lim_{n\to\infty}\left\{f(c+\frac{1}{n^2})+f(c+\frac{2}{n^2})+\cdots+f(c+\frac{n}{n^2})-nf(c)\right\}=\frac{1}{2} f'(c)$,How to show that,\lim_{n\to\infty}\left\{f(c+\frac{1}{n^2})+f(c+\frac{2}{n^2})+\cdots+f(c+\frac{n}{n^2})-nf(c)\right\}=\frac{1}{2} f'(c),If $f:\mathbb{R}\to\mathbb{R}$ be a differentiable at $x=c$ then how can I show that $$\lim_{n\to\infty}\left\{f(c+\frac{1}{n^2})+f(c+\frac{2}{n^2})+\cdots+f(c+\frac{n}{n^2})-nf(c)\right\}=\frac{1}{2} f'(c)$$ I had no clue which result should I use to prove this. Please give some hints. Thanks!,If $f:\mathbb{R}\to\mathbb{R}$ be a differentiable at $x=c$ then how can I show that $$\lim_{n\to\infty}\left\{f(c+\frac{1}{n^2})+f(c+\frac{2}{n^2})+\cdots+f(c+\frac{n}{n^2})-nf(c)\right\}=\frac{1}{2} f'(c)$$ I had no clue which result should I use to prove this. Please give some hints. Thanks!,,"['real-analysis', 'limits', 'derivatives']"
80,Derivation for the derivative of $a^{t}$ from The Equation,Derivation for the derivative of  from The Equation,a^{t},"In Calculus, the Equation is known as: $$f'(x)=\lim\limits_{h \to 0} \frac{f(x+h)-f(x)}{h}$$ This equation allow us to find the derivatives of functions. Let's try this with the exponential function:$f(x)=a^x$ where $a \gt 1$. $$f'(x)=\lim\limits_{h \to 0} \frac{a^{x+h}-a^x}{h}$$ $$f'(x)=\lim\limits_{h \to 0} \frac{a^x a^h-a^x}{h}$$ $$f'(x)= a^x \lim\limits_{h \to 0} \frac{a^h-1}{h}$$ As you can see, we need to determine $\lim\limits_{h \to 0} (\frac{a^h-1}{h})$ to find the derivative. At this point, we ask the question: what would $a$ be such that the limit would be 1? The answer is $e$ and that's the end of everything. What I want to know is can we actually evaluate that limit? How can we actually find $e$ other than guessing with trial and error?","In Calculus, the Equation is known as: $$f'(x)=\lim\limits_{h \to 0} \frac{f(x+h)-f(x)}{h}$$ This equation allow us to find the derivatives of functions. Let's try this with the exponential function:$f(x)=a^x$ where $a \gt 1$. $$f'(x)=\lim\limits_{h \to 0} \frac{a^{x+h}-a^x}{h}$$ $$f'(x)=\lim\limits_{h \to 0} \frac{a^x a^h-a^x}{h}$$ $$f'(x)= a^x \lim\limits_{h \to 0} \frac{a^h-1}{h}$$ As you can see, we need to determine $\lim\limits_{h \to 0} (\frac{a^h-1}{h})$ to find the derivative. At this point, we ask the question: what would $a$ be such that the limit would be 1? The answer is $e$ and that's the end of everything. What I want to know is can we actually evaluate that limit? How can we actually find $e$ other than guessing with trial and error?",,"['derivatives', 'exponential-function']"
81,Derivative of $(-1)^x$,Derivative of,(-1)^x,"I'm taking a summer calc 2 class and we're getting into alternating patterns. I was interested in seeing the graph of $(-1)^x$ so I typed it into my TI-84 for $y = (-1)^x$. Surprisingly, the graph is blank and dots appear randomly throughout. Then I typed it into wolfram alpha and got a pattern I kind of expected. Since I'm learning this calculus stuff I thought hey... I can describe the function with the derivative... So I write down $f(x) = (-1)^x$ and then $f'(x)=\;?$ and realize I have no idea how to get this derivative. Wolfram alpha tells me it is $i\pi(-1)^x$. The ""step by step"" on wolfram alpha looks like this infamous bundle of joy: Step 1: derivation of $-1^x$ Step 2: derivation of $-1^x$ is $i\pi(-1)^x$ Yey! I'm no more smarter having read that. So... how do I (or why can't I) take the derivative of $(-1)^x$? Also why can't my calculator graph this?","I'm taking a summer calc 2 class and we're getting into alternating patterns. I was interested in seeing the graph of $(-1)^x$ so I typed it into my TI-84 for $y = (-1)^x$. Surprisingly, the graph is blank and dots appear randomly throughout. Then I typed it into wolfram alpha and got a pattern I kind of expected. Since I'm learning this calculus stuff I thought hey... I can describe the function with the derivative... So I write down $f(x) = (-1)^x$ and then $f'(x)=\;?$ and realize I have no idea how to get this derivative. Wolfram alpha tells me it is $i\pi(-1)^x$. The ""step by step"" on wolfram alpha looks like this infamous bundle of joy: Step 1: derivation of $-1^x$ Step 2: derivation of $-1^x$ is $i\pi(-1)^x$ Yey! I'm no more smarter having read that. So... how do I (or why can't I) take the derivative of $(-1)^x$? Also why can't my calculator graph this?",,"['calculus', 'derivatives']"
82,Derivative of the function $y = 2^{\sqrt{\tan x}}$,Derivative of the function,y = 2^{\sqrt{\tan x}},"How to find derivative of the following function: $y = 2^{\sqrt{\tan x}}$ , $y' = ?$ I did the following $$\frac{d}{dx}2^{\sqrt{\tan x}} = 2^{\sqrt{\tan x}}\ln{2}(\sqrt{\tan}x)'$$ and stopped here. Can you guide me? The solution is as follows in the book: $$2^{\sqrt{\tan x}}\frac{\ln{2}}{2\cos^{2}{x}\sqrt{\tan{x}}}$$","How to find derivative of the following function: $y = 2^{\sqrt{\tan x}}$ , $y' = ?$ I did the following $$\frac{d}{dx}2^{\sqrt{\tan x}} = 2^{\sqrt{\tan x}}\ln{2}(\sqrt{\tan}x)'$$ and stopped here. Can you guide me? The solution is as follows in the book: $$2^{\sqrt{\tan x}}\frac{\ln{2}}{2\cos^{2}{x}\sqrt{\tan{x}}}$$",,['derivatives']
83,When does $f\sim g$ implies $f'\sim g'$?,When does  implies ?,f\sim g f'\sim g',"Given two $C^1$ functions $f,g:[0,+\infty)\to [0,+\infty)$ such that $f(x)\sim g(x)$ as $x\to\infty$, which good conditions guarantee that $f'(x)\sim g'(x)$? I thought that monotonicity of the derivatives could be such condition, but I wasn't able to prove or disprove. I think that one such condition can be $f$, $f'$, $g$, $g'$, $f-g$ and $f'-g'$ to be monotonic increasing, but that's so restrictive that hurts. Also, if anyone could suggest a book that treats problems like that (I don't know, ""asymptotic analysis""?) I would appreciate. Thanks in advance.","Given two $C^1$ functions $f,g:[0,+\infty)\to [0,+\infty)$ such that $f(x)\sim g(x)$ as $x\to\infty$, which good conditions guarantee that $f'(x)\sim g'(x)$? I thought that monotonicity of the derivatives could be such condition, but I wasn't able to prove or disprove. I think that one such condition can be $f$, $f'$, $g$, $g'$, $f-g$ and $f'-g'$ to be monotonic increasing, but that's so restrictive that hurts. Also, if anyone could suggest a book that treats problems like that (I don't know, ""asymptotic analysis""?) I would appreciate. Thanks in advance.",,"['real-analysis', 'derivatives', 'asymptotics']"
84,The derivative of $x!$ and its continuity,The derivative of  and its continuity,x!,"is the factorial of fractions and negative numbers defined? If yes, then what is its graph? Also please find its domain. Our teacher said the factorial of a fraction is the fraction itself. He also said the graph is continuous but could not determine the derivative of $x!$.","is the factorial of fractions and negative numbers defined? If yes, then what is its graph? Also please find its domain. Our teacher said the factorial of a fraction is the fraction itself. He also said the graph is continuous but could not determine the derivative of $x!$.",,"['derivatives', 'continuity', 'factorial', 'gamma-function']"
85,When is a continuous function differentiable? [duplicate],When is a continuous function differentiable? [duplicate],,"This question already has answers here : Why differentiability implies continuity, but continuity does not imply differentiability? (3 answers) Closed 9 years ago . I have been doing a lot of problems regarding calculus. An utmost basic question I stumble upon is ""when is a continuous function differentiable?"" (irrespective of whether its in an open or closed set).","This question already has answers here : Why differentiability implies continuity, but continuity does not imply differentiability? (3 answers) Closed 9 years ago . I have been doing a lot of problems regarding calculus. An utmost basic question I stumble upon is ""when is a continuous function differentiable?"" (irrespective of whether its in an open or closed set).",,"['derivatives', 'continuity']"
86,The sum of two coordinates at which the first two derivatives of $f(x) = e^{2x}(x^2 + 2x)$ are equal,The sum of two coordinates at which the first two derivatives of  are equal,f(x) = e^{2x}(x^2 + 2x),"I came across the problem on Khan Academy while studying differential calculus: Consider the function $f(x) = e^{2x}(x^2 + 2x)$ . There are two x-coordinates at which $f'(x) = f''(x)$ . What is the sum of these two coordinates? While finding the derivative of $f(x)$ , I got everything reduced down to $$f'(x) = e^{2x}(2x+2) + 2e^{2x}(x^2 + 2x)$$ Khan Academy says this can be further reduced to $e^{2x}(2x^2 + 6x + 2)$ , obviously so I can apply the product rule again to find the second derivative, but I have no idea how they made that happen. Can anyone help me understand their algebra?","I came across the problem on Khan Academy while studying differential calculus: Consider the function . There are two x-coordinates at which . What is the sum of these two coordinates? While finding the derivative of , I got everything reduced down to Khan Academy says this can be further reduced to , obviously so I can apply the product rule again to find the second derivative, but I have no idea how they made that happen. Can anyone help me understand their algebra?",f(x) = e^{2x}(x^2 + 2x) f'(x) = f''(x) f(x) f'(x) = e^{2x}(2x+2) + 2e^{2x}(x^2 + 2x) e^{2x}(2x^2 + 6x + 2),"['calculus', 'derivatives']"
87,What does $d \theta$ mean?,What does  mean?,d \theta,"i'm doing elementary differential geometry homework and i came across a notation that i cannot understand. I need to show that if $\gamma(t) = (x(t), y(t))$ then if we put $\gamma$ into polar coordinates $x= r \cos \theta$;  $y=r \sin \theta$ we get: If $$x^2 + y^2  = x \frac{dy}{d \alpha} - y \frac{dx}{d \alpha}$$ then $$r^2(1- \frac{ d \theta}{d \alpha})=0.$$ What puzzles me is the meaning of $d \theta$. What does that quantity represent? Is it the deriivative $$\frac{\partial \arctan (\frac{y(t)}{x(t)})}{\partial t}?$$","i'm doing elementary differential geometry homework and i came across a notation that i cannot understand. I need to show that if $\gamma(t) = (x(t), y(t))$ then if we put $\gamma$ into polar coordinates $x= r \cos \theta$;  $y=r \sin \theta$ we get: If $$x^2 + y^2  = x \frac{dy}{d \alpha} - y \frac{dx}{d \alpha}$$ then $$r^2(1- \frac{ d \theta}{d \alpha})=0.$$ What puzzles me is the meaning of $d \theta$. What does that quantity represent? Is it the deriivative $$\frac{\partial \arctan (\frac{y(t)}{x(t)})}{\partial t}?$$",,"['calculus', 'differential-geometry', 'derivatives']"
88,Cube Root function not differentiable,Cube Root function not differentiable,,"Why is the cube root function not differentiable at $x=0?$ I graphed it and the curve looks a bit vertical at that point, is that why? Can someone give a good explanation please.","Why is the cube root function not differentiable at I graphed it and the curve looks a bit vertical at that point, is that why? Can someone give a good explanation please.",x=0?,"['calculus', 'derivatives', 'differential']"
89,sketching derivative of a graph,sketching derivative of a graph,,I am wondering whether or not this is the right sketch of the derivative in the picture below: UPDATE: Here is the function as it appears in the question (please disregard the pencil marks):,I am wondering whether or not this is the right sketch of the derivative in the picture below: UPDATE: Here is the function as it appears in the question (please disregard the pencil marks):,,"['derivatives', 'graphing-functions']"
90,Evaluate $\sum_{n=1}^\infty nx^{n-1}$,Evaluate,\sum_{n=1}^\infty nx^{n-1},How can you evaluate $\sum_{n=1}^\infty nx^{n-1} = \frac{1}{(1-x)^2}$ without relying on the fact that it's the derivative of $\sum_{n=1}^\infty x^n = \frac{1}{1-x} $?,How can you evaluate $\sum_{n=1}^\infty nx^{n-1} = \frac{1}{(1-x)^2}$ without relying on the fact that it's the derivative of $\sum_{n=1}^\infty x^n = \frac{1}{1-x} $?,,"['calculus', 'derivatives', 'summation']"
91,Why is $|x|$ not differentiable at $x=0$?,Why is  not differentiable at ?,|x| x=0,"The definition of a derivative is the slope of a function tangent to a point. It is also defined as $$\lim_{h\to0} \frac{f(x+h)-f(x)}{h}$$ If we apply this to $f(x)= |x|$, we get that it is $\lim\limits_{h\to 0} \dfrac{|h|}{h}$, which is undefined. However, if we look at the graph of $|x|$, we see that there can exist a tangent line at x=0, with slope 0. So why is the derivative undefined instead of 0?","The definition of a derivative is the slope of a function tangent to a point. It is also defined as $$\lim_{h\to0} \frac{f(x+h)-f(x)}{h}$$ If we apply this to $f(x)= |x|$, we get that it is $\lim\limits_{h\to 0} \dfrac{|h|}{h}$, which is undefined. However, if we look at the graph of $|x|$, we see that there can exist a tangent line at x=0, with slope 0. So why is the derivative undefined instead of 0?",,"['limits', 'derivatives']"
92,Prove that a function is differentiable at a point,Prove that a function is differentiable at a point,,"At which values of $x$ is $f(x)$ differentiable? $f(x) = \begin{cases} 1-e^{-x},  & \text{$x \gt 0$} \\ \ln(1-x), & \text{$x\le 0$} \end{cases}$ I first proved that $f(x)$ is continuous for every $x$: $$ \lim_{x \to 0^+}f(x) = 1-e^{-0} = 0 $$ $$ \lim_{x \to 0^-}f(x) = \ln(1-x) = 0 $$ and $f(0) = 0$, is this OK? was that necessary when proving that a function is differentiable at a point? Next, I wanna prove the limit using the definition of derivative, so: $$ \lim_{x\to0^+} \frac{f(x)-f(0)}{x-0} = \frac{1-e^{-x}-(1-e^0)}{x} = \frac{-e^{-x}+e^0}{x} $$ $$ \lim_{x\to0^-} \frac{f(x)-f(0)}{x-0} = \frac{\ln(1-x)-0}{x} = \frac{\ln(1-x)}{x} $$ How do I continue from here? I'm stuck at both sides, some help please?:) Thanks!","At which values of $x$ is $f(x)$ differentiable? $f(x) = \begin{cases} 1-e^{-x},  & \text{$x \gt 0$} \\ \ln(1-x), & \text{$x\le 0$} \end{cases}$ I first proved that $f(x)$ is continuous for every $x$: $$ \lim_{x \to 0^+}f(x) = 1-e^{-0} = 0 $$ $$ \lim_{x \to 0^-}f(x) = \ln(1-x) = 0 $$ and $f(0) = 0$, is this OK? was that necessary when proving that a function is differentiable at a point? Next, I wanna prove the limit using the definition of derivative, so: $$ \lim_{x\to0^+} \frac{f(x)-f(0)}{x-0} = \frac{1-e^{-x}-(1-e^0)}{x} = \frac{-e^{-x}+e^0}{x} $$ $$ \lim_{x\to0^-} \frac{f(x)-f(0)}{x-0} = \frac{\ln(1-x)-0}{x} = \frac{\ln(1-x)}{x} $$ How do I continue from here? I'm stuck at both sides, some help please?:) Thanks!",,"['calculus', 'limits', 'derivatives']"
93,Approximate $f''(3)$ from Table of Values of $f(x)$,Approximate  from Table of Values of,f''(3) f(x),"Considering the table above, what is the best approximation for $f''(3)$? How would I solve?","Considering the table above, what is the best approximation for $f''(3)$? How would I solve?",,"['calculus', 'derivatives', 'approximation']"
94,Minimum value of a differentiable function at some point,Minimum value of a differentiable function at some point,,Let $f(x)$ be differentiable for all $x\in \mathbb{R}$ and let $f(0)=2$ and $f^\prime(x)\leq -2$. How could i find the minimum value of $f(-1).$,Let $f(x)$ be differentiable for all $x\in \mathbb{R}$ and let $f(0)=2$ and $f^\prime(x)\leq -2$. How could i find the minimum value of $f(-1).$,,"['calculus', 'real-analysis', 'inequality', 'derivatives']"
95,Economically computing $d\beta$,Economically computing,d\beta,"$\displaystyle \beta = z\frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{(x^2+y^2+z^2)^{2}}$ Show that $d\beta=0$. So, let $r=x^2+y^2+z^2$, $\begin{align} \displaystyle d\beta &= d(z\frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{(x^2+y^2+z^2)^{2}}) \\ &= d(\frac{zx dy \wedge dz + zy dz \wedge dx + z^2 dx \wedge dy}{r^{2}}) \\ &= d(\frac{zx dy \wedge dz + zy dz \wedge dx + z^2 dx \wedge dy}{r^{2}}) \\ &= d(\frac{zx}{r^2}) \wedge dy \wedge dz + d(\frac{zy}{r^2}) \wedge dz \wedge dx + d(\frac{z^2}{r^2}) \wedge dx \wedge dy \\ \end{align} $ Is there a better of doing this as I dont really want to have to crunch $d(\frac{zx}{r^2})$ with the product rule?","$\displaystyle \beta = z\frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{(x^2+y^2+z^2)^{2}}$ Show that $d\beta=0$. So, let $r=x^2+y^2+z^2$, $\begin{align} \displaystyle d\beta &= d(z\frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{(x^2+y^2+z^2)^{2}}) \\ &= d(\frac{zx dy \wedge dz + zy dz \wedge dx + z^2 dx \wedge dy}{r^{2}}) \\ &= d(\frac{zx dy \wedge dz + zy dz \wedge dx + z^2 dx \wedge dy}{r^{2}}) \\ &= d(\frac{zx}{r^2}) \wedge dy \wedge dz + d(\frac{zy}{r^2}) \wedge dz \wedge dx + d(\frac{z^2}{r^2}) \wedge dx \wedge dy \\ \end{align} $ Is there a better of doing this as I dont really want to have to crunch $d(\frac{zx}{r^2})$ with the product rule?",,"['calculus', 'differential-geometry', 'derivatives', 'exterior-algebra']"
96,How can I differentiate this equation?,How can I differentiate this equation?,,"I need to differentiate this: $$ y = b(e^{ax}-e^{-ax}) $$ I've got the solution from a book, but I don't found the process to differentiate it. The solution is: $$ y = ab(e^{ax}+e^{-ax}) $$ Here is my own process: $$ y = b(e^{ax}*(1-\frac{e^{-ax}}{e^{ax}}) $$ $$ ln(y) = ln(b)+ax+ln(1-\frac{e^{-ax}}{e^{ax}}) $$ $$ \frac{1}{y}\frac{dy}{dx} = a+\frac{1}{1-\frac{e^{-ax}}{e^{ax}}} = a+1-e^{2ax} $$ $$ \frac{dy}{dx} = (a+1-e^{2ax})*(b(e^{ax}-e^{-ax})) $$ Can someone help me, please?","I need to differentiate this: $$ y = b(e^{ax}-e^{-ax}) $$ I've got the solution from a book, but I don't found the process to differentiate it. The solution is: $$ y = ab(e^{ax}+e^{-ax}) $$ Here is my own process: $$ y = b(e^{ax}*(1-\frac{e^{-ax}}{e^{ax}}) $$ $$ ln(y) = ln(b)+ax+ln(1-\frac{e^{-ax}}{e^{ax}}) $$ $$ \frac{1}{y}\frac{dy}{dx} = a+\frac{1}{1-\frac{e^{-ax}}{e^{ax}}} = a+1-e^{2ax} $$ $$ \frac{dy}{dx} = (a+1-e^{2ax})*(b(e^{ax}-e^{-ax})) $$ Can someone help me, please?",,"['calculus', 'derivatives', 'logarithms']"
97,Evaluate $\lim_{x \to 0} \frac{\sqrt{1 + \tan x} - \sqrt{1 + \sin x}}{x^3}$,Evaluate,\lim_{x \to 0} \frac{\sqrt{1 + \tan x} - \sqrt{1 + \sin x}}{x^3},"What I attempted thus far: Multiplying by conjugate $$\lim_{x \to 0} \frac{\sqrt{1 + \tan x} - \sqrt{1 + \sin x}}{x^3} \cdot \frac{\sqrt{1 + \tan x} + \sqrt{1 + \sin x}}{\sqrt{1 + \tan x} + \sqrt{1 + \sin x}} = \lim_{x \to 0} \frac{\tan x - \sin x}{x^3 \cdot (\sqrt{1 + \tan x} + \sqrt{1 + \sin x})}$$ factor out $\sin x$ in the numerator $$\lim_{x \to 0} \frac{\sin x \cdot (\sec x - 1)}{x^3 \cdot (\sqrt{1 + \tan x} + \sqrt{1 + \sin x})}$$ simplify using $\lim_{x \to 0} \frac{\sin x}{x} = 1 $ $$\lim_{x \to 0} \frac{\sec x - 1}{x^2 \cdot (\sqrt{1 + \tan x} + \sqrt{1 + \sin x})}$$ From here I don't see any useful direction to go in, if I even went in an useful direction in the first place, I don't know. I suspect that this could be evaluated using the definition of derivatives, if so, or not, any suggestions?","What I attempted thus far: Multiplying by conjugate $$\lim_{x \to 0} \frac{\sqrt{1 + \tan x} - \sqrt{1 + \sin x}}{x^3} \cdot \frac{\sqrt{1 + \tan x} + \sqrt{1 + \sin x}}{\sqrt{1 + \tan x} + \sqrt{1 + \sin x}} = \lim_{x \to 0} \frac{\tan x - \sin x}{x^3 \cdot (\sqrt{1 + \tan x} + \sqrt{1 + \sin x})}$$ factor out $\sin x$ in the numerator $$\lim_{x \to 0} \frac{\sin x \cdot (\sec x - 1)}{x^3 \cdot (\sqrt{1 + \tan x} + \sqrt{1 + \sin x})}$$ simplify using $\lim_{x \to 0} \frac{\sin x}{x} = 1 $ $$\lim_{x \to 0} \frac{\sec x - 1}{x^2 \cdot (\sqrt{1 + \tan x} + \sqrt{1 + \sin x})}$$ From here I don't see any useful direction to go in, if I even went in an useful direction in the first place, I don't know. I suspect that this could be evaluated using the definition of derivatives, if so, or not, any suggestions?",,"['limits', 'derivatives']"
98,notation for first and second derivatives of a power series,notation for first and second derivatives of a power series,,"I have a power series $$\sum_{k=0}^\infty\frac{c_k}{k!}x^k$$ where $c_k$ is an arbitrary $k$-th term of some sequence.  Then $$\frac{d^2}{dx^2}\left[\sum_{k=0}^\infty\frac{c_k}{k!}x^k\right]=\frac{d}{dx}\left[\sum_{k=0}^\infty\frac{k\cdot c_k}{k!}x^{k-1}\right]=\sum_{k=0}^\infty\frac{k(k-1)\cdot c_k}{k!}x^{k-2}$$ Now, is it okay to leave this like this?  I know that the first two terms are  $0$, which removes the $1/x^2 $ term when $k=0$ and $1/x$ term when $k=1$.  I figure this is no problem, but perhaps I am wrong.  Should I re-index or can I leave it like this? EDIT:  One of the reasons I'm posting this is I have an expression with a polynomial in front of the power series.  For example,  $$(x-x^2)\sum_{k=0}^\infty\frac{k\cdot c_k}{k!}x^{k-1}$$  In this example, I have not re-indexed as has been mentioned in the previous posts and comments.  Now, since I haven't re-indexed, and if I distribute I get $$\sum_{k=0}^\infty\frac{k\cdot c_k}{k!}x^{k}+\sum_{k=0}^\infty\frac{k\cdot c_k}{k!}x^{k+1}$$ Now, given that there is a polynomial coefficient in front of the derivative of a power series, can I NOT re-index and distribute first?","I have a power series $$\sum_{k=0}^\infty\frac{c_k}{k!}x^k$$ where $c_k$ is an arbitrary $k$-th term of some sequence.  Then $$\frac{d^2}{dx^2}\left[\sum_{k=0}^\infty\frac{c_k}{k!}x^k\right]=\frac{d}{dx}\left[\sum_{k=0}^\infty\frac{k\cdot c_k}{k!}x^{k-1}\right]=\sum_{k=0}^\infty\frac{k(k-1)\cdot c_k}{k!}x^{k-2}$$ Now, is it okay to leave this like this?  I know that the first two terms are  $0$, which removes the $1/x^2 $ term when $k=0$ and $1/x$ term when $k=1$.  I figure this is no problem, but perhaps I am wrong.  Should I re-index or can I leave it like this? EDIT:  One of the reasons I'm posting this is I have an expression with a polynomial in front of the power series.  For example,  $$(x-x^2)\sum_{k=0}^\infty\frac{k\cdot c_k}{k!}x^{k-1}$$  In this example, I have not re-indexed as has been mentioned in the previous posts and comments.  Now, since I haven't re-indexed, and if I distribute I get $$\sum_{k=0}^\infty\frac{k\cdot c_k}{k!}x^{k}+\sum_{k=0}^\infty\frac{k\cdot c_k}{k!}x^{k+1}$$ Now, given that there is a polynomial coefficient in front of the derivative of a power series, can I NOT re-index and distribute first?",,"['derivatives', 'notation', 'power-series']"
99,What is an intuitive way to see $\frac{d}{dx}\sin^{-1}x+\frac{d}{dx}\cos^{-1}x=0$?,What is an intuitive way to see ?,\frac{d}{dx}\sin^{-1}x+\frac{d}{dx}\cos^{-1}x=0,"Without calculation, explain why $\frac{d}{dx}\sin^{-1}x+\frac{d}{dx}\cos^{-1}x=0$?","Without calculation, explain why $\frac{d}{dx}\sin^{-1}x+\frac{d}{dx}\cos^{-1}x=0$?",,"['calculus', 'trigonometry', 'derivatives']"
