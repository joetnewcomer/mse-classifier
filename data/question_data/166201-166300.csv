,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Category theory and statistics,Category theory and statistics,,"I've been juggling with some concepts from statistics revolving around properties of estimators and sufficient statistics, and I can't help but notice that they have a strong categorical flavor, e.g. I'm pretty sure minimal sufficient statistics are terminal objects in an appropriate category. I know someone must have worked these things out but haven't been able to find it - I'd be grateful if someone could give me a pointer to some illuminating discussion of applications of category theory to statistics.","I've been juggling with some concepts from statistics revolving around properties of estimators and sufficient statistics, and I can't help but notice that they have a strong categorical flavor, e.g. I'm pretty sure minimal sufficient statistics are terminal objects in an appropriate category. I know someone must have worked these things out but haven't been able to find it - I'd be grateful if someone could give me a pointer to some illuminating discussion of applications of category theory to statistics.",,"['statistics', 'reference-request', 'category-theory']"
1,On the probability of obtaining 27 sets in the card game Set,On the probability of obtaining 27 sets in the card game Set,,"For people not familiar with the card game Set, see its entry on Wikipedia and/or one of the related questions here on Math SE . It might be faster to just play the game a couple of times though, see e.g. this web-based version . As there are $3^4 = 81$ cards, the maximum number of sets that can be obtained is $3^3 = 27$. I have played the game many times, but never used up all cards. This makes me wonder what the odds are of obtaining $27$ sets. The game usually ends with either $23$ or $24$ sets, but $25$ is not uncommon. It is not possible to end with $26$ sets though, as the remaining $3$ cards will also form a set. Of course, certain types of sets are easier to spot than others. In addition, this seems to vary from person to person. In practice this might lead to a reduced number of obtained sets — the easier sets tend to have multiple features (number, shape, colour, shading) in common, thus upsetting the balance/distribution of features in the remaining cards. Therefore, it seems best to give priority to sets that have no features in common. This is just my intuition though. [Edit] Based on one of the comments below I'd like to update the question. Are there any tactics that improve the probability of obtaining $27$ sets?","For people not familiar with the card game Set, see its entry on Wikipedia and/or one of the related questions here on Math SE . It might be faster to just play the game a couple of times though, see e.g. this web-based version . As there are $3^4 = 81$ cards, the maximum number of sets that can be obtained is $3^3 = 27$. I have played the game many times, but never used up all cards. This makes me wonder what the odds are of obtaining $27$ sets. The game usually ends with either $23$ or $24$ sets, but $25$ is not uncommon. It is not possible to end with $26$ sets though, as the remaining $3$ cards will also form a set. Of course, certain types of sets are easier to spot than others. In addition, this seems to vary from person to person. In practice this might lead to a reduced number of obtained sets — the easier sets tend to have multiple features (number, shape, colour, shading) in common, thus upsetting the balance/distribution of features in the remaining cards. Therefore, it seems best to give priority to sets that have no features in common. This is just my intuition though. [Edit] Based on one of the comments below I'd like to update the question. Are there any tactics that improve the probability of obtaining $27$ sets?",,"['statistics', 'card-games']"
2,Maximum likelihood and sufficient statistics,Maximum likelihood and sufficient statistics,,"$$f_T(t;B,C) = \frac{\exp(-t/C)-\exp(-t/B)}{C-B}$$ where our mean is $C+B$ and $t>0$. so far i have found my log likelihood functions and differentiated them as follows: $$dl/dB = \sum[t\exp(t/C) / (B^2(\exp(t/c)-\exp(t/B)))] +n/(C-B) = 0$$ i have also found a similar $dl/dC$. I have now been asked to comment what you can find in the way of sufficient statistics for estimating these parameters and why there is no simple way of using Maximum Likelihood for estimation in the problem. I am simply unsure as to what to comment upon. Any help would be appreciated. Thanks, Rachel Editor's Note: Given here is the probability density function $$ f_T (t;B,C) = \frac{{e^{ - t/C}  - e^{ - t/B} }}{{C - B}}, \;\; t > 0, $$ where $B$ and $C$ are positive constants such that $C > B$. The mean is $C+B$. For the log likelihood function, see the last equation in my answer to this related question , and differentiate accordingly (with respect to $B$ and $C$).","$$f_T(t;B,C) = \frac{\exp(-t/C)-\exp(-t/B)}{C-B}$$ where our mean is $C+B$ and $t>0$. so far i have found my log likelihood functions and differentiated them as follows: $$dl/dB = \sum[t\exp(t/C) / (B^2(\exp(t/c)-\exp(t/B)))] +n/(C-B) = 0$$ i have also found a similar $dl/dC$. I have now been asked to comment what you can find in the way of sufficient statistics for estimating these parameters and why there is no simple way of using Maximum Likelihood for estimation in the problem. I am simply unsure as to what to comment upon. Any help would be appreciated. Thanks, Rachel Editor's Note: Given here is the probability density function $$ f_T (t;B,C) = \frac{{e^{ - t/C}  - e^{ - t/B} }}{{C - B}}, \;\; t > 0, $$ where $B$ and $C$ are positive constants such that $C > B$. The mean is $C+B$. For the log likelihood function, see the last equation in my answer to this related question , and differentiate accordingly (with respect to $B$ and $C$).",,['statistics']
3,Estimating Parameter - What is the qualitative difference between MLE fitting and Least Squares CDF fitting?,Estimating Parameter - What is the qualitative difference between MLE fitting and Least Squares CDF fitting?,,"Given a parametric pdf $f(x;\lambda)$ and a set of data $\{ x_k \}_{k=1}^n$, here are two ways of formulating a problem of selecting an optimal parameter vector $\lambda^*$ to fit to the data. The first is maximum likelihood estimation (MLE): $$\lambda^* = \arg \max_\lambda \prod_{k=1}^n f(x_k;\lambda)$$ where this product is called the likelihood function. The second is least squares CDF fitting: $$\lambda^*=\arg \min_\lambda \| E(x)-F(x;\lambda) \|_{L^2(dx)}$$ where $F(x;\lambda)$ is the CDF corresponding to $f(x;\lambda)$ and $E(x)$ is the empirical CDF: $E(x)=\frac{1}{n} \sum_{k=1}^n 1_{x_k \leq x}$. (One could also consider more general $L^p$ CDF fitting, but let's not go there for now.) In the experiments I have done, these two methods give similar but still significantly different results. For example, in a bimodal normal mixture fit, one gave one of the standard deviations as about $12.6$ while the other gave it as about $11.6$. This isn't a huge difference but it is large enough to easily see it in a graph. What is the intuition for the difference in these two ""goodness of fit"" metrics? An example answer would be something along the lines of ""MLE cares more about data points in the tail of the distribution than least squares CDF fit"" (I make no claims on the validity of this statement). An answer discussing other metrics of fitting parametric distributions to data would also be of some use.","Given a parametric pdf $f(x;\lambda)$ and a set of data $\{ x_k \}_{k=1}^n$, here are two ways of formulating a problem of selecting an optimal parameter vector $\lambda^*$ to fit to the data. The first is maximum likelihood estimation (MLE): $$\lambda^* = \arg \max_\lambda \prod_{k=1}^n f(x_k;\lambda)$$ where this product is called the likelihood function. The second is least squares CDF fitting: $$\lambda^*=\arg \min_\lambda \| E(x)-F(x;\lambda) \|_{L^2(dx)}$$ where $F(x;\lambda)$ is the CDF corresponding to $f(x;\lambda)$ and $E(x)$ is the empirical CDF: $E(x)=\frac{1}{n} \sum_{k=1}^n 1_{x_k \leq x}$. (One could also consider more general $L^p$ CDF fitting, but let's not go there for now.) In the experiments I have done, these two methods give similar but still significantly different results. For example, in a bimodal normal mixture fit, one gave one of the standard deviations as about $12.6$ while the other gave it as about $11.6$. This isn't a huge difference but it is large enough to easily see it in a graph. What is the intuition for the difference in these two ""goodness of fit"" metrics? An example answer would be something along the lines of ""MLE cares more about data points in the tail of the distribution than least squares CDF fit"" (I make no claims on the validity of this statement). An answer discussing other metrics of fitting parametric distributions to data would also be of some use.",,"['statistics', 'numerical-methods', 'least-squares', 'parameter-estimation', 'maximum-likelihood']"
4,What is the variance of self-information (or surprisal)?,What is the variance of self-information (or surprisal)?,,"The self-information of an outcome $x_i$, or surprisal, is defined as: $$ I(x_i)=-\log P(x_i), $$ where $P$ means probability. This way, the Shannon entropy can be seen as the ""average"" or ""expected"" surprisal: $$ H=-\sum_i P(x_i)\,\log P(x_i). $$ This is quite intuitive, and it helps understanding what Shannon entropy really measures. But what is instead the variance of the surprisal? Does this quantity: $$  \sum_i P(x_i)\,\big(\log P(x_i) \big)^2-H^2 $$ have any statistical meaning? Is it equivalent to anything known, or at least can it be written in a clearer way? Thanks.","The self-information of an outcome $x_i$, or surprisal, is defined as: $$ I(x_i)=-\log P(x_i), $$ where $P$ means probability. This way, the Shannon entropy can be seen as the ""average"" or ""expected"" surprisal: $$ H=-\sum_i P(x_i)\,\log P(x_i). $$ This is quite intuitive, and it helps understanding what Shannon entropy really measures. But what is instead the variance of the surprisal? Does this quantity: $$  \sum_i P(x_i)\,\big(\log P(x_i) \big)^2-H^2 $$ have any statistical meaning? Is it equivalent to anything known, or at least can it be written in a clearer way? Thanks.",,"['statistics', 'intuition', 'information-theory', 'entropy']"
5,Exponential distribution unbiased estimator,Exponential distribution unbiased estimator,,"Let $$X_1, \ldots, X_n \overset{iid}{\sim} Exp(\lambda), \quad \lambda > 0$$ The Maximum-Likelihood-Estimator is given by $$\widehat{\lambda} = \frac{1}{\frac{1}{n}\sum_{i=1}^{n}{X_i}} = \frac{n}{\sum_{i=1}^{n}{X_i}}$$ and it's not unbiased: Using $$X_1, \ldots, X_n \overset{iid}{\sim} Exp(\lambda) \quad \Rightarrow \quad \sum_{i=1}^{n}{X_i} \sim \Gamma(\lambda, n)$$ and $$\Gamma(n+1) = n\cdot\Gamma(n) \quad \text{ for } \quad n \in \mathbb{N}$$ we get: \begin{align*} 	E\bigg(\frac{n}{\sum_{i=1}^{n}{X_i}}\bigg) &= \int_{0}^{\infty}{\frac{n}{x}\cdot \frac{\lambda^n}{\Gamma(n)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\ 		&= \int_{0}^{\infty}{\frac{n}{x}\cdot \frac{\lambda^{n-1}\cdot\lambda}{(n-1)\cdot\Gamma(n-1)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda\cdot\int_{0}^{\infty}{\frac{n}{n-1}\cdot\frac{\lambda^{n-1}}{\Gamma(n-1)}\cdot x^{(n-1)-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda\cdot\frac{n}{n-1}\cdot\int_{0}^{\infty}{\frac{\lambda^{n-1}}{\Gamma(n-1)}\cdot x^{(n-1)-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda\cdot\frac{n}{n-1} \end{align*} So $\widehat{\lambda}$ is not unbiased. \begin{align*} 	E\bigg(\frac{n-1}{\sum_{i=1}^{n}{X_i}}\bigg) &= \int_{0}^{\infty}{\frac{n-1}{x}\cdot \frac{\lambda^n}{\Gamma(n)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\ 		&= \int_{0}^{\infty}{\frac{n-1}{x}\cdot \frac{\lambda^n}{(n-1)\cdot\Gamma(n-1)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\ 		&= \int_{0}^{\infty}{\frac{n-1}{x}\cdot \frac{\lambda^n}{\Gamma(n-1)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\  		&= \int_{0}^{\infty}{\frac{\lambda^{n-1}\cdot\lambda}{\Gamma(n-1)}\cdot x^{(n-1)-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda\cdot\int_{0}^{\infty}{\frac{\lambda^{n-1}}{\Gamma(n-1)}\cdot x^{(n-1)-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda \end{align*} So $\frac{n-1}{\sum_{i=1}^{n}{X_i}}$ is unbiased. Is that correct?","Let $$X_1, \ldots, X_n \overset{iid}{\sim} Exp(\lambda), \quad \lambda > 0$$ The Maximum-Likelihood-Estimator is given by $$\widehat{\lambda} = \frac{1}{\frac{1}{n}\sum_{i=1}^{n}{X_i}} = \frac{n}{\sum_{i=1}^{n}{X_i}}$$ and it's not unbiased: Using $$X_1, \ldots, X_n \overset{iid}{\sim} Exp(\lambda) \quad \Rightarrow \quad \sum_{i=1}^{n}{X_i} \sim \Gamma(\lambda, n)$$ and $$\Gamma(n+1) = n\cdot\Gamma(n) \quad \text{ for } \quad n \in \mathbb{N}$$ we get: \begin{align*} 	E\bigg(\frac{n}{\sum_{i=1}^{n}{X_i}}\bigg) &= \int_{0}^{\infty}{\frac{n}{x}\cdot \frac{\lambda^n}{\Gamma(n)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\ 		&= \int_{0}^{\infty}{\frac{n}{x}\cdot \frac{\lambda^{n-1}\cdot\lambda}{(n-1)\cdot\Gamma(n-1)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda\cdot\int_{0}^{\infty}{\frac{n}{n-1}\cdot\frac{\lambda^{n-1}}{\Gamma(n-1)}\cdot x^{(n-1)-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda\cdot\frac{n}{n-1}\cdot\int_{0}^{\infty}{\frac{\lambda^{n-1}}{\Gamma(n-1)}\cdot x^{(n-1)-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda\cdot\frac{n}{n-1} \end{align*} So $\widehat{\lambda}$ is not unbiased. \begin{align*} 	E\bigg(\frac{n-1}{\sum_{i=1}^{n}{X_i}}\bigg) &= \int_{0}^{\infty}{\frac{n-1}{x}\cdot \frac{\lambda^n}{\Gamma(n)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\ 		&= \int_{0}^{\infty}{\frac{n-1}{x}\cdot \frac{\lambda^n}{(n-1)\cdot\Gamma(n-1)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\ 		&= \int_{0}^{\infty}{\frac{n-1}{x}\cdot \frac{\lambda^n}{\Gamma(n-1)}\cdot x^{n-1}\cdot e^{-\lambda x}dx}\\  		&= \int_{0}^{\infty}{\frac{\lambda^{n-1}\cdot\lambda}{\Gamma(n-1)}\cdot x^{(n-1)-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda\cdot\int_{0}^{\infty}{\frac{\lambda^{n-1}}{\Gamma(n-1)}\cdot x^{(n-1)-1}\cdot e^{-\lambda x}dx}\\ 		&= \lambda \end{align*} So $\frac{n-1}{\sum_{i=1}^{n}{X_i}}$ is unbiased. Is that correct?",,['statistics']
6,"In a MOBA, is it possible for every character to have a <50% winrate?","In a MOBA, is it possible for every character to have a <50% winrate?",,"In a recent discussion with friends we were discussing win-rates in a video game. One person suggested it made sense that every champion could have a less than 50% win rate, while another suggested this was impossible because every loss suffered for a champion would result in a win for another. In practice it seems highly unlikely to ever have a <50% win rate for every character in a team based game like a MOBA, however we weren't able to come to the an answer and none of us are good enough at statistics for a definitive answer. Some constraints and details to consider given the ruleset of a MOBA like League of Legends: Win rate is defined as the percentage of games won for a specific character. Every game has an equal number of winning and losing players (5 on each side). I would prefer to disregard scenarios in which a player leaves in which case there would be 4 on one side and 5 on the other. Each player will select the character they play before the game begins from a list of characters. This list is larger than 10 so their will be characters whose win rates are not affected if they do not appear in a game. Players are unable to switch characters mid game. Each character may appear once on each team, which means their are a minimum of 5 characters and a maximum of 10 appearing in each game. If a character is on both teams in a game the result would count as both a loss and a win for that character, otherwise if the character is ""unmirrored"" it would only be a win or a loss depending on the result. Draws are not allowed, or at least do not affect win rates.","In a recent discussion with friends we were discussing win-rates in a video game. One person suggested it made sense that every champion could have a less than 50% win rate, while another suggested this was impossible because every loss suffered for a champion would result in a win for another. In practice it seems highly unlikely to ever have a <50% win rate for every character in a team based game like a MOBA, however we weren't able to come to the an answer and none of us are good enough at statistics for a definitive answer. Some constraints and details to consider given the ruleset of a MOBA like League of Legends: Win rate is defined as the percentage of games won for a specific character. Every game has an equal number of winning and losing players (5 on each side). I would prefer to disregard scenarios in which a player leaves in which case there would be 4 on one side and 5 on the other. Each player will select the character they play before the game begins from a list of characters. This list is larger than 10 so their will be characters whose win rates are not affected if they do not appear in a game. Players are unable to switch characters mid game. Each character may appear once on each team, which means their are a minimum of 5 characters and a maximum of 10 appearing in each game. If a character is on both teams in a game the result would count as both a loss and a win for that character, otherwise if the character is ""unmirrored"" it would only be a win or a loss depending on the result. Draws are not allowed, or at least do not affect win rates.",,['statistics']
7,Is a data set really a set?,Is a data set really a set?,,"Originally I thought that in statistics, a data set is just a set of real numbers, and that was it. But in the case of a set, there can only be one instance of any given entry, e.g. in  set theory $$\left \{ 1,2,2 \right \}=\left \{ 1,2 \right \}$$ A set of this form in set theory may also be called an unordered pair . But from the point of view of statistics the objects on both sides of the equation are distinct, e.g. the left one has a mode, while the right one doesn't; the left one's arithmetic mean is $\frac{5}{3}$, while the arithmetic mean of the right one is $\frac{3}{2}$. Question: are the concepts of data set in statistics and set in set theory really different things? Are data sets of real numbers in statistics really $n$-tuples of set theory in disguise, or taken to be ones most of the time implicitly?","Originally I thought that in statistics, a data set is just a set of real numbers, and that was it. But in the case of a set, there can only be one instance of any given entry, e.g. in  set theory $$\left \{ 1,2,2 \right \}=\left \{ 1,2 \right \}$$ A set of this form in set theory may also be called an unordered pair . But from the point of view of statistics the objects on both sides of the equation are distinct, e.g. the left one has a mode, while the right one doesn't; the left one's arithmetic mean is $\frac{5}{3}$, while the arithmetic mean of the right one is $\frac{3}{2}$. Question: are the concepts of data set in statistics and set in set theory really different things? Are data sets of real numbers in statistics really $n$-tuples of set theory in disguise, or taken to be ones most of the time implicitly?",,"['statistics', 'elementary-set-theory', 'terminology']"
8,Why are IQ test results normally distributed?,Why are IQ test results normally distributed?,,(I'm a nooby in probability) So why are IQ test results normally distributed?  Or more precisely what are the hypothesizes and theorems that imply this distribution? Has it to do with the central limit theorem? (But this theorem is about the arithmetic mean of iid variables. I dont see iid variables here: I suppose it's not one person repeating the test. Is it the skills given at a person that is considered as a random variable?),(I'm a nooby in probability) So why are IQ test results normally distributed?  Or more precisely what are the hypothesizes and theorems that imply this distribution? Has it to do with the central limit theorem? (But this theorem is about the arithmetic mean of iid variables. I dont see iid variables here: I suppose it's not one person repeating the test. Is it the skills given at a person that is considered as a random variable?),,"['statistics', 'normal-distribution']"
9,How do I calculate a weighted average from two averages?,How do I calculate a weighted average from two averages?,,"I have two sets of averaged data I am working with, which account for a score and the average amount of users that achieved this. For example: Average Score $4$,  Total Number of participants (which the average is derived from): $835$ Average Score $3.5$, Total Number of participants: $4,579$ Can I calculate a weighted mean from these two averages and participant counts, or would that be inaccurate?","I have two sets of averaged data I am working with, which account for a score and the average amount of users that achieved this. For example: Average Score $4$,  Total Number of participants (which the average is derived from): $835$ Average Score $3.5$, Total Number of participants: $4,579$ Can I calculate a weighted mean from these two averages and participant counts, or would that be inaccurate?",,['statistics']
10,How do I find the bias of an estimator?,How do I find the bias of an estimator?,,"My notes say $$B(\hat\theta) = E(\hat\theta) - \theta $$ And I understand that the bias is the difference between a parameter and the expectation of its estimator. What I don't understand is how to calulate the bias given only an estimator? My notes lack ANY examples of calculating the bias, so even if anyone could please give me an example I could understand it better!","My notes say $$B(\hat\theta) = E(\hat\theta) - \theta $$ And I understand that the bias is the difference between a parameter and the expectation of its estimator. What I don't understand is how to calulate the bias given only an estimator? My notes lack ANY examples of calculating the bias, so even if anyone could please give me an example I could understand it better!",,['statistics']
11,"Is pi lying on the ground, and on TV? - and on the sun?","Is pi lying on the ground, and on TV? - and on the sun?",,"Consider the leaves from a bunch of trees in a terraced plaza in the Autumn. It may well happen that the tiles of the terrace are squares whose length easily exceeds the length of the stem of the leaves (assuming leaves all of the same kind). Cannot this be regarded as a Buffon process? That is, if you took a photograph of it and counted the line crossing of the leaf-stems with the lines determined by the tiles, you would get Buffon's calculation for pi. Furthermore, you could do this even in the absence of the tiles, simply by imposing an appropriate tiling over the photograph later, right? And if we can impose a tiling later, what about imposing it on a TV program, with respect to some recurring object? Would that, properly done, not constitute a Buffon process as well? edit: It might apply to sunspots as well, because sunspots, during solar min, typically have low latitudes, i.e., they  occur near the equator of the sun, which can be considered as approximately a flat surface, and each sunspot approximates a highly non-smooth Jordan curve, which therefore almost always is going to have a unique greatest diameter, which can be considered as the needle. Sunspots vary greatly in size, so we could restrict our attention to ones of a given size (most likely the smaller ones, so as to maximize the number of sunspots considered), and then consider the circles of latitude, starting at the equator, separated by a distance a little bit greater than the needle length (perhaps, such that the needle length is 5/6 that of the circle separations, in imitation of what Mario Lazzarini did in 1901.) Of course, since we’re considering solar min, we might want to consider all sunspots over, say, a 22-year period, so as to get a lot of data points (trial tosses of the needle). So, how good an approximation to pi would be get? – and is there any correlation between how good an approximation we get and certain solar events such as flares?","Consider the leaves from a bunch of trees in a terraced plaza in the Autumn. It may well happen that the tiles of the terrace are squares whose length easily exceeds the length of the stem of the leaves (assuming leaves all of the same kind). Cannot this be regarded as a Buffon process? That is, if you took a photograph of it and counted the line crossing of the leaf-stems with the lines determined by the tiles, you would get Buffon's calculation for pi. Furthermore, you could do this even in the absence of the tiles, simply by imposing an appropriate tiling over the photograph later, right? And if we can impose a tiling later, what about imposing it on a TV program, with respect to some recurring object? Would that, properly done, not constitute a Buffon process as well? edit: It might apply to sunspots as well, because sunspots, during solar min, typically have low latitudes, i.e., they  occur near the equator of the sun, which can be considered as approximately a flat surface, and each sunspot approximates a highly non-smooth Jordan curve, which therefore almost always is going to have a unique greatest diameter, which can be considered as the needle. Sunspots vary greatly in size, so we could restrict our attention to ones of a given size (most likely the smaller ones, so as to maximize the number of sunspots considered), and then consider the circles of latitude, starting at the equator, separated by a distance a little bit greater than the needle length (perhaps, such that the needle length is 5/6 that of the circle separations, in imitation of what Mario Lazzarini did in 1901.) Of course, since we’re considering solar min, we might want to consider all sunspots over, say, a 22-year period, so as to get a lot of data points (trial tosses of the needle). So, how good an approximation to pi would be get? – and is there any correlation between how good an approximation we get and certain solar events such as flares?",,['statistics']
12,99th Percentile: top 1% or top 2%? [closed],99th Percentile: top 1% or top 2%? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question If one achieves a score in the 99th percentile on an exam, is that score considered in the top 1% or 2%? How is percentile defined in statistics? I read this somewhere: It’s top 2% - being in the x percentile doesn’t imply top (100-x) percent because the percentage getting exactly x is counted twice. Is this correct?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question If one achieves a score in the 99th percentile on an exam, is that score considered in the top 1% or 2%? How is percentile defined in statistics? I read this somewhere: It’s top 2% - being in the x percentile doesn’t imply top (100-x) percent because the percentage getting exactly x is counted twice. Is this correct?",,['statistics']
13,Mean concentration implies median concentration,Mean concentration implies median concentration,,"Exercise 2.14 in Wainwright, ""High-Dimensional Statistics"", states that if $X$ is such that $$P[|X-\mathbb{E}[X]|\geq t] \leq c_1 e^{-c_2t^2},$$ for $c_1, c_2$ positive constants, $t\geq 0$ , then for any median $m_X$ it holds that $$P[|X-m_X|\geq t] \leq c_3 e^{-c_4t^2},$$ with $c_3=4c_1$ and $c_4=c_2/8$ . I can get some loose concentration around the median using $|\mathbb{E}[X]-m_X|\leq \sqrt{\mathbb{V}[X]}$ , but this does not achieve the constants proposed. Any ideas for how to get the suggested bound, or any other bound resembling it?","Exercise 2.14 in Wainwright, ""High-Dimensional Statistics"", states that if is such that for positive constants, , then for any median it holds that with and . I can get some loose concentration around the median using , but this does not achieve the constants proposed. Any ideas for how to get the suggested bound, or any other bound resembling it?","X P[|X-\mathbb{E}[X]|\geq t] \leq c_1 e^{-c_2t^2}, c_1, c_2 t\geq 0 m_X P[|X-m_X|\geq t] \leq c_3 e^{-c_4t^2}, c_3=4c_1 c_4=c_2/8 |\mathbb{E}[X]-m_X|\leq \sqrt{\mathbb{V}[X]}","['statistics', 'median', 'concentration-of-measure']"
14,Finding the mean and median of a probability density function,Finding the mean and median of a probability density function,,"I suspect this is super-easy, but I haven't done any math in about ten years and I'm working with concepts that have been woefully explained... I need to find the mean and median of a continuous random variable that has a probability density function of: $f(x) = 2x^{-3}$ for $x > 1$ I know that this involves working out integrals and whatnot but, again, this is one of those concepts that wasn't actually explained to me.","I suspect this is super-easy, but I haven't done any math in about ten years and I'm working with concepts that have been woefully explained... I need to find the mean and median of a continuous random variable that has a probability density function of: $f(x) = 2x^{-3}$ for $x > 1$ I know that this involves working out integrals and whatnot but, again, this is one of those concepts that wasn't actually explained to me.",,"['statistics', 'random-variables']"
15,Equations For Quadratic Regression,Equations For Quadratic Regression,,"Does anyone know the specific equations for the three parameters in a least-squares quadratic regression? I'm looking for something like $\beta_1=,\beta_2=,\beta_3=$ for each of $y=\beta_1+\beta_2x+\beta_3x^2$. To be clear, the right side of each of these equations should be evaluateable, using the data, to find the parameter. I was able to find the equations for linear regression on line, but google hasn't turned anything up for this. Thanks in advance","Does anyone know the specific equations for the three parameters in a least-squares quadratic regression? I'm looking for something like $\beta_1=,\beta_2=,\beta_3=$ for each of $y=\beta_1+\beta_2x+\beta_3x^2$. To be clear, the right side of each of these equations should be evaluateable, using the data, to find the parameter. I was able to find the equations for linear regression on line, but google hasn't turned anything up for this. Thanks in advance",,"['statistics', 'regression']"
16,Finding UMVUE of $\theta$ when the underlying distribution is exponential distribution,Finding UMVUE of  when the underlying distribution is exponential distribution,\theta,"Hi I'm solving some exercise problems in my text : ""A Course in Mathematical Statistics"". I'm in the chapter ""Point estimation"" now, and I want to find a UMVUE of $\theta$ where $X_1 ,...,X_n$ are i.i.d random variables with the p.d.f $f(x; \theta)=\theta e^{-\theta x}, x\gt0$ . I know that $E(X_i)=1/\theta,$ for each $i$ , and also have that $\bar{X}$ (or equivalently $\sum_1^n X_i$ ) is a complete sufficient statistic for $\theta$ . But I cannot go any further here. Somebody can help me?","Hi I'm solving some exercise problems in my text : ""A Course in Mathematical Statistics"". I'm in the chapter ""Point estimation"" now, and I want to find a UMVUE of where are i.i.d random variables with the p.d.f . I know that for each , and also have that (or equivalently ) is a complete sufficient statistic for . But I cannot go any further here. Somebody can help me?","\theta X_1 ,...,X_n f(x; \theta)=\theta e^{-\theta x}, x\gt0 E(X_i)=1/\theta, i \bar{X} \sum_1^n X_i \theta","['statistics', 'probability-distributions', 'statistical-inference', 'exponential-distribution', 'parameter-estimation']"
17,Expectation of square of random variable and their mean.,Expectation of square of random variable and their mean.,,The following easy proof is well known to all students of statistics. I studied it three years ago and at present I can not remember two steps of it. If you can help me a little. $$E({X_i}^2) = \mu^2 + \sigma^2$$ $$E(\bar{X}^2) = \mu^2 + \frac{\sigma^2}{n}$$ How to prove the above two results. You may give me some hints or the complete proof. I prefer the last one. The main result with it proofs is mentioned below. Thank you for your answer.,The following easy proof is well known to all students of statistics. I studied it three years ago and at present I can not remember two steps of it. If you can help me a little. $$E({X_i}^2) = \mu^2 + \sigma^2$$ $$E(\bar{X}^2) = \mu^2 + \frac{\sigma^2}{n}$$ How to prove the above two results. You may give me some hints or the complete proof. I prefer the last one. The main result with it proofs is mentioned below. Thank you for your answer.,,['statistics']
18,Normalization of data in decision tree,Normalization of data in decision tree,,"After reading through a few references, I have come to know that for machine learning in general, it is necessary to normalize features so that no features are arbitrarily large ($centering$) and all features are on the same scale ($scaling$). However, I'm having a bit of difficulty in visualizing the impact of this for a decision tree. Does data normalization impact the decision tree structure? If yes, how?","After reading through a few references, I have come to know that for machine learning in general, it is necessary to normalize features so that no features are arbitrarily large ($centering$) and all features are on the same scale ($scaling$). However, I'm having a bit of difficulty in visualizing the impact of this for a decision tree. Does data normalization impact the decision tree structure? If yes, how?",,"['statistics', 'machine-learning', 'trees', 'entropy', 'data-mining']"
19,"fair die or not, from 3D printer","fair die or not, from 3D printer",,"I made a 3D printed die today, but depending on the heat applied, it may or may not be a ""fair"" die (i.e. have an equal chance of landing on each face). I have just tried rolling it 150 times.  The frequency results came out to: $$ \begin{array}{c|c} \hline 1's &  21 \\ \hline  2's & 30 \\  \hline 3's & 23 \\ \hline 4's & 31 \\ \hline 5's & 21 \\  \hline 6's & 24 \\  \hline \end{array} $$ How would I calculate the chance that this die is fair?","I made a 3D printed die today, but depending on the heat applied, it may or may not be a ""fair"" die (i.e. have an equal chance of landing on each face). I have just tried rolling it 150 times.  The frequency results came out to: $$ \begin{array}{c|c} \hline 1's &  21 \\ \hline  2's & 30 \\  \hline 3's & 23 \\ \hline 4's & 31 \\ \hline 5's & 21 \\  \hline 6's & 24 \\  \hline \end{array} $$ How would I calculate the chance that this die is fair?",,['statistics']
20,Probability that a sample comes from one of two distributions,Probability that a sample comes from one of two distributions,,"Let's say I have two normal distributions with means $\mu_1$, $\mu_2$ and standard deviations $\sigma_1$, $\sigma_2$ (which I know). I am handed a random variate from one of the distributions (I don't know which). What is the likelihood that my variate belongs to distribution 1 and not distribution 2? UPDATE: a concrete example. Machine one generates normally-distributed variates with mean 1053 and standard deviation 59. Machine two generates normally-distributed variates with mean 1187 and standard deviation 73. One of them is picked at random, the handle is turned (unseen by me) and the number 1162.4 comes out. What is the likelihood that number was generated by machine 1 as opposed to machine 2?","Let's say I have two normal distributions with means $\mu_1$, $\mu_2$ and standard deviations $\sigma_1$, $\sigma_2$ (which I know). I am handed a random variate from one of the distributions (I don't know which). What is the likelihood that my variate belongs to distribution 1 and not distribution 2? UPDATE: a concrete example. Machine one generates normally-distributed variates with mean 1053 and standard deviation 59. Machine two generates normally-distributed variates with mean 1187 and standard deviation 73. One of them is picked at random, the handle is turned (unseen by me) and the number 1162.4 comes out. What is the likelihood that number was generated by machine 1 as opposed to machine 2?",,"['statistics', 'estimation']"
21,"Why do statisticians like ""$n-1$"" instead of ""$n$""?","Why do statisticians like """" instead of """"?",n-1 n,"Does anyone have an intuitive explanation (no formulas, just words! :D) about the ""$n-1$"" instead of ""$n$"" in the unbiased variance estimator $$S_n^2 = \dfrac{\sum\limits_{i = 1}^n \left(X_i-\bar{X}\right)^2}{n-1}?$$","Does anyone have an intuitive explanation (no formulas, just words! :D) about the ""$n-1$"" instead of ""$n$"" in the unbiased variance estimator $$S_n^2 = \dfrac{\sum\limits_{i = 1}^n \left(X_i-\bar{X}\right)^2}{n-1}?$$",,"['statistics', 'notation', 'intuition', 'standard-deviation']"
22,Limiting distribution of sum of normals,Limiting distribution of sum of normals,,"How would I go about solving this problem below? I am not exactly sure where to start. I know that I need to make use of the Lebesgue Dominated Convergence theorem as well. Thanks for the help. Let $X_1, X_2, \ldots, X_n$ be a random sample of size $n$ from a distribution that is $N(\mu, \sigma^2)$ where $\sigma^2 > 0$. Show that $Z_n = \sum\limits_{i=1}^n X_i$ does not have a limiting distribution.","How would I go about solving this problem below? I am not exactly sure where to start. I know that I need to make use of the Lebesgue Dominated Convergence theorem as well. Thanks for the help. Let $X_1, X_2, \ldots, X_n$ be a random sample of size $n$ from a distribution that is $N(\mu, \sigma^2)$ where $\sigma^2 > 0$. Show that $Z_n = \sum\limits_{i=1}^n X_i$ does not have a limiting distribution.",,"['statistics', 'probability-distributions']"
23,What is the expected value of cosine of a multivariate Gaussian?,What is the expected value of cosine of a multivariate Gaussian?,,"Suppose $X \sim \mathcal{N}\left(\mu, \Sigma\right)$. How do I evaluate $\operatorname{E}\left[\cos \left(t^{T}X \right) \right] $ and $\operatorname{E}\left[\sin \left(t^{T}X\right) \right] $? Does this have to do with the characteristic function $\operatorname{E}\left[e^{it^{T}X} \right] =\exp \left\{i\mu^{T}t -\frac{1}{2}t^{T}\Sigma t\right\}$?","Suppose $X \sim \mathcal{N}\left(\mu, \Sigma\right)$. How do I evaluate $\operatorname{E}\left[\cos \left(t^{T}X \right) \right] $ and $\operatorname{E}\left[\sin \left(t^{T}X\right) \right] $? Does this have to do with the characteristic function $\operatorname{E}\left[e^{it^{T}X} \right] =\exp \left\{i\mu^{T}t -\frac{1}{2}t^{T}\Sigma t\right\}$?",,"['statistics', 'discrete-mathematics', 'expectation', 'gaussian-integral']"
24,Two-tailed hypothesis test; Why do we multiply p-value by two?,Two-tailed hypothesis test; Why do we multiply p-value by two?,,"I understand that in a two-tailed hypothesis test, we must multiply the p-value by two. i.e. if z=1.95 and it's a one-tailed hypothesis test, our p-value is 0.0256. But, if it's a two-tailed hypothesis test and z=1.95, we must multiply the p-value of 0.0256 by two. Hence, the correct p-value is 0.0512 for the two-tailed hypothesis test. I can draw it out on the standard normal curve and I understand that we must multiply the p-value by two. But, my question is why we have to multiply by two. What is the conceptual idea behind it?","I understand that in a two-tailed hypothesis test, we must multiply the p-value by two. i.e. if z=1.95 and it's a one-tailed hypothesis test, our p-value is 0.0256. But, if it's a two-tailed hypothesis test and z=1.95, we must multiply the p-value of 0.0256 by two. Hence, the correct p-value is 0.0512 for the two-tailed hypothesis test. I can draw it out on the standard normal curve and I understand that we must multiply the p-value by two. But, my question is why we have to multiply by two. What is the conceptual idea behind it?",,"['statistics', 'hypothesis-testing']"
25,How to explain tie-correction for Spearman's Rank Correlation?,How to explain tie-correction for Spearman's Rank Correlation?,,In Mathematics at my college we are being taught correlation in which when there are ties in ranks we take average rank for all of the ties and then total correction factor is added summation of square of difference in ranks. The formula for correction factor is $$\frac{m(m^2 - 1)}{12}.$$ Where did this correction factor formula came from ? How is it derived? I can't get my mind around it.,In Mathematics at my college we are being taught correlation in which when there are ties in ranks we take average rank for all of the ties and then total correction factor is added summation of square of difference in ranks. The formula for correction factor is $$\frac{m(m^2 - 1)}{12}.$$ Where did this correction factor formula came from ? How is it derived? I can't get my mind around it.,,"['statistics', 'correlation']"
26,Normalisation using Softmax- What advantage does exponential provide,Normalisation using Softmax- What advantage does exponential provide,,"I am trying to apply some bench marking across different organizations. I have 3 organizations with 3 scores using which I would like to relatively rank them. For e.g. Org 1 = 115, Org 2 = 105, Org 3= 50, then $\mathbf{x} = (115, 105, 50)$ I was told to try Softmax function \begin{equation} \mathrm{softmax}(\mathbf{x})=\frac{e^{x_{i}}}{\sum_{j=1}^{3}e^{x_{j}}} \end{equation} as it normalizes the values. I could also normalize using \begin{equation} \mathrm{standard~normalisation}(\mathbf{x})=\frac{x_{i}}{\sum_{j=1}^{3}x_{j}} \end{equation} Can anyone tell me what advantage does the Softmax function provide above the standard normalization discussed above? Does the exponential in softmax help in any specific way to increase/reduce the margin between the compared entities?","I am trying to apply some bench marking across different organizations. I have 3 organizations with 3 scores using which I would like to relatively rank them. For e.g. Org 1 = 115, Org 2 = 105, Org 3= 50, then I was told to try Softmax function as it normalizes the values. I could also normalize using Can anyone tell me what advantage does the Softmax function provide above the standard normalization discussed above? Does the exponential in softmax help in any specific way to increase/reduce the margin between the compared entities?","\mathbf{x} = (115, 105, 50) \begin{equation}
\mathrm{softmax}(\mathbf{x})=\frac{e^{x_{i}}}{\sum_{j=1}^{3}e^{x_{j}}}
\end{equation} \begin{equation}
\mathrm{standard~normalisation}(\mathbf{x})=\frac{x_{i}}{\sum_{j=1}^{3}x_{j}}
\end{equation}","['statistics', 'functions']"
27,Hoeffding's inequality and learning.,Hoeffding's inequality and learning.,,"I am studying the feasibility of learning  from the book Learning from Data . The author uses a bin analogy to discuss the feasibility of learning in a probabilistic sense. I have certain questions to ask. First, I will try to summarize what the author is trying to do: Consider a bin containing red and green marbles, possibly infinitely many. The proportion of the red and green marbles is such that if we pick a marble at random, the probability that it will be red is $\mu$. We assume that $\mu$ is unknown to us. We pick a random sample of $N$ independent marbles (with replacement). Let $X_i$ be the indicator for the $i$th marble in the sample to be red. i.e., $X_i=1$ if $i$th marble in the sample is red, $X_i=0$ otherwise. Define $\nu:=\frac{1}{n}\sum_{i=1}^N X_i$. By Hodeffing's inequality, $$\mathbb{P}(|\nu-\mu|>\varepsilon)\le 2\mathrm{e}^{-2\varepsilon ^2N}$$ In a learning problem, there is an unknown target function $f:\mathcal{X}\to\mathcal{Y}$ to be learned. The learning algorithm picks a hypothesis $g:\mathcal{X}\to\mathcal{Y}$ from a hypothesis set $\mathcal{H}$. We can connect the bin problem to the learning problem as follows. Take a single hypotheis $h\in\mathcal{H}$ and compare it to $f$  on each point $\mathbf{x}\in\mathcal{X}$. If $h(\mathbf{x})=f(\mathbf{x})$, color the point $\mathbf{x}$ green, else color it red. The color each point gets is unknown to us, since $f$ is unknown.. However, if we pick $\mathbf{x}$ at random according to some probability distribution $P$ over $\mathcal{X}$, we know that $\mathbf{x}$ wil be red with some probability $\mu$. Regardless of the value of $\mu$, the space $\mathcal{X}$ bow behaves like a bin. The training examples play the role of a sample from a bin. If the inputs $\mathbf{x_1},\cdots,\mathbf{x_N}$ in the data set $\mathcal{D}$ are picked independently according to $P$, we will get a random sample of red and green points. Each point will be red with probability $\mu$.The color of points$\mathbf{x_1},\cdots,\mathbf{x_N}$ will be known to us since we know $f$ on the data set $\mathcal{D}$. We define $E_{in}(h)$ to be the fraction of $D$ where $f$ and $h$ disagree, also called in-sample error, and similarly out-of-sample error $E_{out}$. Thus $$E_{in}(h)=\dfrac{1}{N}\sum_{i=1}^N[[h(\mathbf{x_i})\neq f(\mathbf{x_i})]], E_{out}=\mathbb{P}(h(\mathbf{x})\neq f(\mathbf{x})) $$ Using the Hoeffding's inequality, we have $$\mathbb{P}(|E_{in}(h)-E_{out}(h)|>\varepsilon)\le 2\mathrm{e}^{-2\varepsilon ^2N}\cdots\cdots(1)$$. Let us consider an entire hypothesis set $\mathcal{H}$ instead of just one hypothesis $h$, and assume that $\mathcal{H}$ has finite number of hypothesis $\mathcal{H}=\{h_1,\cdots,h_m\}$. We can construct a bin equivalent in this case by having $M$ bins. Each bin still represents the input space $\mathcal{X}$ with the red marbles in the $m$th bin corresponding to the points $\mathbf{x}\in\mathcal{X}$ where $h(\mathbf{x})\neq f(\mathbf{x})$. The probability of red marbles in the $m$th bin is $E_{out}(h_m)$ and the fraction of red marbles in the $m$th sample is $E_{in}(h_m)$ for $m=1,2,\cdots,M$. Hoeffding's inequality applies to each bin seperately. The Hoeffding's inequality $(1)$ assumes that the hypothesis $h$ is fixed before you generate the data set, and the probability is with respect to random data sets $\mathcal{D}$. The learning algorithm picks a final hypothesis $g$ based on $\mathcal{D}$. i.e., after generating the data set. Thus we cannot plug in $g$ for $h$ in the Hoeffding's inequality. A way to get around this is to try to bound $\mathbb{P}(|E_{in}(h)-E_{out}(h)|>\varepsilon)$ in a way that does not depend on which $g$ the learning algorithm picks. $$\left(|E_{in}(g)-E_{out}(g)|>\varepsilon\right)\subseteq\left(\bigcup_{i=1}^M |E_{in}(h_i)-E_{out}(h_)|>\varepsilon\right)$$ and hence,  $$\mathbb{P}\left(|E_{in}(g)-E_{out}(g)|>\varepsilon\right)\le\sum_{i=1}^M\mathbb{P}\left(|E_{in}(h_i)-E_{out}(h_i)|>\varepsilon\right)$$. Applying Hoeffding's inequality to $M$ terms one at a time, we get $$\mathbb{P}\left(|E_{in}(g)-E_{out}(g)|>\varepsilon\right)\le 2M\mathrm{e}^{-2\varepsilon ^2N} $$ I have the following questions. Why does Hoeffding's inequality requires that $h$ is fixed before generating the data set $\mathcal{D}$ ? Is it because $X_i$s, the indicator of $i$th point in $\mathcal{D}$ being red, are no longer independent after generating the data set, which is an assumption required for Hoeffding's inequality? For instance, after generating $\mathcal{D}$, knowing $X_i$ for some $1\le i\le N$ would give information for other $X_j$. Is this correct ? In the last step,for each term in the summation $\sum_{i=1}^M\mathbb{P}\left(|E_{in}(h_i)-E_{out}(h_i)|>\varepsilon\right)$, author states that Hoeffding's inequality applies to each bin separately and $\mathbb{P}\left(|E_{in}(h_i)-E_{out}(h_i)|>\varepsilon\right)\le 2\mathrm{e}^{-2\varepsilon ^2N}$ for all $1\le i\le M$. How is this possible? According to the previous question, the hypothesis $h$ must be fixed before generating the data set, but in this case we have a single data set $\mathcal{D}$ and we are later considering the hypothesis $h_1,h_2,\cdots,h_m$. How is the application of Hoeffding's inequality to each term in summation justified since the data set  is generated before hand i.e., before choosing a hypothesis? If we could apply Hoeffding's to each term in the summation separately, why don't we say that $g$ is one of the hypothesis $h_1,h_2,\cdots,h_m$ and hence $\mathbb{P}\left(|E_{in}(g)-E_{out}(g)|>\varepsilon\right)\le 2\mathrm{e}^{-2\varepsilon ^2N}$? Is the probability distribution $P$ on $\mathcal{X}$ independent of the hypothesis $h$ i.e., is $P$ chosen without bothering about the color of points in $\mathcal{X}$, which in turn is dictated by $h$?","I am studying the feasibility of learning  from the book Learning from Data . The author uses a bin analogy to discuss the feasibility of learning in a probabilistic sense. I have certain questions to ask. First, I will try to summarize what the author is trying to do: Consider a bin containing red and green marbles, possibly infinitely many. The proportion of the red and green marbles is such that if we pick a marble at random, the probability that it will be red is $\mu$. We assume that $\mu$ is unknown to us. We pick a random sample of $N$ independent marbles (with replacement). Let $X_i$ be the indicator for the $i$th marble in the sample to be red. i.e., $X_i=1$ if $i$th marble in the sample is red, $X_i=0$ otherwise. Define $\nu:=\frac{1}{n}\sum_{i=1}^N X_i$. By Hodeffing's inequality, $$\mathbb{P}(|\nu-\mu|>\varepsilon)\le 2\mathrm{e}^{-2\varepsilon ^2N}$$ In a learning problem, there is an unknown target function $f:\mathcal{X}\to\mathcal{Y}$ to be learned. The learning algorithm picks a hypothesis $g:\mathcal{X}\to\mathcal{Y}$ from a hypothesis set $\mathcal{H}$. We can connect the bin problem to the learning problem as follows. Take a single hypotheis $h\in\mathcal{H}$ and compare it to $f$  on each point $\mathbf{x}\in\mathcal{X}$. If $h(\mathbf{x})=f(\mathbf{x})$, color the point $\mathbf{x}$ green, else color it red. The color each point gets is unknown to us, since $f$ is unknown.. However, if we pick $\mathbf{x}$ at random according to some probability distribution $P$ over $\mathcal{X}$, we know that $\mathbf{x}$ wil be red with some probability $\mu$. Regardless of the value of $\mu$, the space $\mathcal{X}$ bow behaves like a bin. The training examples play the role of a sample from a bin. If the inputs $\mathbf{x_1},\cdots,\mathbf{x_N}$ in the data set $\mathcal{D}$ are picked independently according to $P$, we will get a random sample of red and green points. Each point will be red with probability $\mu$.The color of points$\mathbf{x_1},\cdots,\mathbf{x_N}$ will be known to us since we know $f$ on the data set $\mathcal{D}$. We define $E_{in}(h)$ to be the fraction of $D$ where $f$ and $h$ disagree, also called in-sample error, and similarly out-of-sample error $E_{out}$. Thus $$E_{in}(h)=\dfrac{1}{N}\sum_{i=1}^N[[h(\mathbf{x_i})\neq f(\mathbf{x_i})]], E_{out}=\mathbb{P}(h(\mathbf{x})\neq f(\mathbf{x})) $$ Using the Hoeffding's inequality, we have $$\mathbb{P}(|E_{in}(h)-E_{out}(h)|>\varepsilon)\le 2\mathrm{e}^{-2\varepsilon ^2N}\cdots\cdots(1)$$. Let us consider an entire hypothesis set $\mathcal{H}$ instead of just one hypothesis $h$, and assume that $\mathcal{H}$ has finite number of hypothesis $\mathcal{H}=\{h_1,\cdots,h_m\}$. We can construct a bin equivalent in this case by having $M$ bins. Each bin still represents the input space $\mathcal{X}$ with the red marbles in the $m$th bin corresponding to the points $\mathbf{x}\in\mathcal{X}$ where $h(\mathbf{x})\neq f(\mathbf{x})$. The probability of red marbles in the $m$th bin is $E_{out}(h_m)$ and the fraction of red marbles in the $m$th sample is $E_{in}(h_m)$ for $m=1,2,\cdots,M$. Hoeffding's inequality applies to each bin seperately. The Hoeffding's inequality $(1)$ assumes that the hypothesis $h$ is fixed before you generate the data set, and the probability is with respect to random data sets $\mathcal{D}$. The learning algorithm picks a final hypothesis $g$ based on $\mathcal{D}$. i.e., after generating the data set. Thus we cannot plug in $g$ for $h$ in the Hoeffding's inequality. A way to get around this is to try to bound $\mathbb{P}(|E_{in}(h)-E_{out}(h)|>\varepsilon)$ in a way that does not depend on which $g$ the learning algorithm picks. $$\left(|E_{in}(g)-E_{out}(g)|>\varepsilon\right)\subseteq\left(\bigcup_{i=1}^M |E_{in}(h_i)-E_{out}(h_)|>\varepsilon\right)$$ and hence,  $$\mathbb{P}\left(|E_{in}(g)-E_{out}(g)|>\varepsilon\right)\le\sum_{i=1}^M\mathbb{P}\left(|E_{in}(h_i)-E_{out}(h_i)|>\varepsilon\right)$$. Applying Hoeffding's inequality to $M$ terms one at a time, we get $$\mathbb{P}\left(|E_{in}(g)-E_{out}(g)|>\varepsilon\right)\le 2M\mathrm{e}^{-2\varepsilon ^2N} $$ I have the following questions. Why does Hoeffding's inequality requires that $h$ is fixed before generating the data set $\mathcal{D}$ ? Is it because $X_i$s, the indicator of $i$th point in $\mathcal{D}$ being red, are no longer independent after generating the data set, which is an assumption required for Hoeffding's inequality? For instance, after generating $\mathcal{D}$, knowing $X_i$ for some $1\le i\le N$ would give information for other $X_j$. Is this correct ? In the last step,for each term in the summation $\sum_{i=1}^M\mathbb{P}\left(|E_{in}(h_i)-E_{out}(h_i)|>\varepsilon\right)$, author states that Hoeffding's inequality applies to each bin separately and $\mathbb{P}\left(|E_{in}(h_i)-E_{out}(h_i)|>\varepsilon\right)\le 2\mathrm{e}^{-2\varepsilon ^2N}$ for all $1\le i\le M$. How is this possible? According to the previous question, the hypothesis $h$ must be fixed before generating the data set, but in this case we have a single data set $\mathcal{D}$ and we are later considering the hypothesis $h_1,h_2,\cdots,h_m$. How is the application of Hoeffding's inequality to each term in summation justified since the data set  is generated before hand i.e., before choosing a hypothesis? If we could apply Hoeffding's to each term in the summation separately, why don't we say that $g$ is one of the hypothesis $h_1,h_2,\cdots,h_m$ and hence $\mathbb{P}\left(|E_{in}(g)-E_{out}(g)|>\varepsilon\right)\le 2\mathrm{e}^{-2\varepsilon ^2N}$? Is the probability distribution $P$ on $\mathcal{X}$ independent of the hypothesis $h$ i.e., is $P$ chosen without bothering about the color of points in $\mathcal{X}$, which in turn is dictated by $h$?",,"['statistics', 'machine-learning']"
28,Why the chi-squared statistic follows chi-squared distribution?,Why the chi-squared statistic follows chi-squared distribution?,,"The formula for the Chi-Square test statistic is the following: $$\chi^2=\sum_{i=1}^n\frac{({O_i-E_i})^2}{E_i}$$ where $O_i$ is observed data, and $E_i$ is expected. I am just curious why this follows the $\chi^2$ distribution?","The formula for the Chi-Square test statistic is the following: $$\chi^2=\sum_{i=1}^n\frac{({O_i-E_i})^2}{E_i}$$ where $O_i$ is observed data, and $E_i$ is expected. I am just curious why this follows the $\chi^2$ distribution?",,"['statistics', 'probability-distributions', 'chi-squared']"
29,Estimate the size of a set from which a sample has been equiprobably drawn?,Estimate the size of a set from which a sample has been equiprobably drawn?,,"Here is the problem I'm trying to solve: In order to send spam, a spammer generates fake nicknames, by picking random girl names (and appending a random number to it). I suppose it randomly and fairly picks names from a fixed list, and I get to observe the outcome. Over time, names eventually start repeating. From the distribution of the repetition count of every distinct name, is there a way to estimate the size of the fixed list ? Intuitively, if there was no repetition among the names I have observed, I would have no information about the size of the list, but the minimum bound given by the actual distinct names observed. On the other hand, if the distribution of the repeat counts is far away from zero, I can be reasonably confident that the sample size is much larger than the word list, and that I probably have observed the full list already. The problem lies in the middle zone. So far the distribution of repeat counts looks like this: $$ \begin{matrix}       n & k \\    114 & 1  \\     66 & 2 \\     30 & 3 \\      4 & 4 \\      2 & 5 \\      1 & 6 \end{matrix} $$ Here $n$ is the count of distinct names appearing $k$ times in the sample. Is there a simple estimator for the size of the list?","Here is the problem I'm trying to solve: In order to send spam, a spammer generates fake nicknames, by picking random girl names (and appending a random number to it). I suppose it randomly and fairly picks names from a fixed list, and I get to observe the outcome. Over time, names eventually start repeating. From the distribution of the repetition count of every distinct name, is there a way to estimate the size of the fixed list ? Intuitively, if there was no repetition among the names I have observed, I would have no information about the size of the list, but the minimum bound given by the actual distinct names observed. On the other hand, if the distribution of the repeat counts is far away from zero, I can be reasonably confident that the sample size is much larger than the word list, and that I probably have observed the full list already. The problem lies in the middle zone. So far the distribution of repeat counts looks like this: $$ \begin{matrix}       n & k \\    114 & 1  \\     66 & 2 \\     30 & 3 \\      4 & 4 \\      2 & 5 \\      1 & 6 \end{matrix} $$ Here $n$ is the count of distinct names appearing $k$ times in the sample. Is there a simple estimator for the size of the list?",,['statistics']
30,"Sufficient statistics function for $N(\theta, c\theta^2)$ and symmetrical confidence interval using $\bar{X}$",Sufficient statistics function for  and symmetrical confidence interval using,"N(\theta, c\theta^2) \bar{X}","Exercise: Let $X_1, \dots, X_n$ be a random sample from the Normal Distribution $N(\theta,c\theta^2)$ where $c > 0$ is a known constant and $\theta \in \mathbb R$ an unknown parameter. i) Find a sufficient statistics function for $\theta$ . ii) Using only the statistics function $\bar{X}$ , construct a $100(1 - a)\%$ confidence interval for $\theta$ . Attempt: i) \begin{align*}p(x \mid c,\theta) &= \prod_{i=1}^n(2\pi c\theta^2)^{-1/2}\exp\big\{-(x_i-\theta)^2/(2c\theta^2)\big\}\\ &=(2\pi c\theta^2)^{-n/2}\exp\bigg\{-\frac{n}{2c\theta^2}\sum_{i=1}^n(x_i-\theta)^2\bigg\}\\ &=(2\pi c\theta^2)^{-n/2}\exp\bigg\{-\frac{n}{2c\theta^2}\bigg(\sum_{i=1}^nx_i^2 -2\theta\sum_{i=1}^nx_i+n\theta^2\bigg)\bigg\}. \end{align*} Thus, we can continue and figure out a sufficient statistics function by Fisher's factorization theorem. (ii) How would one proceed by finding a confidence interval for $\theta$ as asked though?","Exercise: Let be a random sample from the Normal Distribution where is a known constant and an unknown parameter. i) Find a sufficient statistics function for . ii) Using only the statistics function , construct a confidence interval for . Attempt: i) Thus, we can continue and figure out a sufficient statistics function by Fisher's factorization theorem. (ii) How would one proceed by finding a confidence interval for as asked though?","X_1, \dots, X_n N(\theta,c\theta^2) c > 0 \theta \in \mathbb R \theta \bar{X} 100(1 - a)\% \theta \begin{align*}p(x \mid c,\theta) &= \prod_{i=1}^n(2\pi c\theta^2)^{-1/2}\exp\big\{-(x_i-\theta)^2/(2c\theta^2)\big\}\\
&=(2\pi c\theta^2)^{-n/2}\exp\bigg\{-\frac{n}{2c\theta^2}\sum_{i=1}^n(x_i-\theta)^2\bigg\}\\
&=(2\pi c\theta^2)^{-n/2}\exp\bigg\{-\frac{n}{2c\theta^2}\bigg(\sum_{i=1}^nx_i^2 -2\theta\sum_{i=1}^nx_i+n\theta^2\bigg)\bigg\}.
\end{align*} \theta","['statistics', 'confidence-interval']"
31,Kullback-Leibler divergence of binomial distributions,Kullback-Leibler divergence of binomial distributions,,"Suppose $P \sim \mathrm{Bin}(n,p)$ and $Q \sim \mathrm{Bin}(n,q)$. Their Kullback-Leibler divergence is defined by $$D_{KL}(P||Q)=\mathbb{E}_{P}\left[\log\left(\frac{p(x)}{q(x)}\right)\right],$$ with $p(x)$ and $q(x)$ the pdf of $P$ and $Q$ resp. and the expected valued is for $P$. For the case I give above, I can write out the expected values and can also find an answer given by: $$\log\left(\left(\frac{p}{q}\right)^{np}\right)+\log\left(\left(\frac{1-p}{1-q}\right)^{n-np}\right).$$ But now i have to compute it for $P \sim \mathrm{Bin}(n,p)$ and $Q \sim \mathrm{Bin}(n+1,q)$ en $P \sim \mathrm{Bin}(n,p)$ and $Q \sim \mathrm{Bin}(n-1,q)$. But then I get problems with the formulas and the expected values. Can someone help we to get the good answer? Thank you","Suppose $P \sim \mathrm{Bin}(n,p)$ and $Q \sim \mathrm{Bin}(n,q)$. Their Kullback-Leibler divergence is defined by $$D_{KL}(P||Q)=\mathbb{E}_{P}\left[\log\left(\frac{p(x)}{q(x)}\right)\right],$$ with $p(x)$ and $q(x)$ the pdf of $P$ and $Q$ resp. and the expected valued is for $P$. For the case I give above, I can write out the expected values and can also find an answer given by: $$\log\left(\left(\frac{p}{q}\right)^{np}\right)+\log\left(\left(\frac{1-p}{1-q}\right)^{n-np}\right).$$ But now i have to compute it for $P \sim \mathrm{Bin}(n,p)$ and $Q \sim \mathrm{Bin}(n+1,q)$ en $P \sim \mathrm{Bin}(n,p)$ and $Q \sim \mathrm{Bin}(n-1,q)$. But then I get problems with the formulas and the expected values. Can someone help we to get the good answer? Thank you",,"['statistics', 'binomial-distribution']"
32,Hottest Days of The Year,Hottest Days of The Year,,"Recently, there has been much talk in the media of it being the hottest day of the year so far. It has always seemed to me that there are likely many more of these in the northern hemisphere than the southern. In the northern hemisphere ""the year"" starts close to the local minimum and so for a good half of the year there is a fairly reasonable chance that a given day will be the hottest so far. However, in the southern hemisphere after an initial cluster of hot days it is very unlikely there will be another so hot until the end of the year, limiting the number ""hottest so far"". (And vice versa for cold). I'm interested in quantitative models of this kind of phenomenon (not just for temperature, but similar patterns). But I don't know how to start developing a model which is tractable. Maybe the year could be modeled as a sinusoid, with a superimposed zero-mean normally distributed random element. It seems that the likely number of ""highest""/""lowest"" so-fars should be expressible in terms as a function of starting phase and variance. Is this something that has been studied, or is maybe trivial to those with the relevant skills?","Recently, there has been much talk in the media of it being the hottest day of the year so far. It has always seemed to me that there are likely many more of these in the northern hemisphere than the southern. In the northern hemisphere ""the year"" starts close to the local minimum and so for a good half of the year there is a fairly reasonable chance that a given day will be the hottest so far. However, in the southern hemisphere after an initial cluster of hot days it is very unlikely there will be another so hot until the end of the year, limiting the number ""hottest so far"". (And vice versa for cold). I'm interested in quantitative models of this kind of phenomenon (not just for temperature, but similar patterns). But I don't know how to start developing a model which is tractable. Maybe the year could be modeled as a sinusoid, with a superimposed zero-mean normally distributed random element. It seems that the likely number of ""highest""/""lowest"" so-fars should be expressible in terms as a function of starting phase and variance. Is this something that has been studied, or is maybe trivial to those with the relevant skills?",,['statistics']
33,What is the relationship betweeen a pdf and cdf?,What is the relationship betweeen a pdf and cdf?,,"I am learning stats. On page 20, my book, All of Statistics 1e,  defines a CDF as function that maps x to the probability that a random variable, X, is less than x. $F_{x}(x) = P(X\leq x)$ On page 23 it gives a function $P(a < X < b ) = \int_{a}^{b}f_{X}dx$ and then says that ""the function $f_{X}$ is called the probability density function. We have that..."" $F_{x}(x) = \int_{-\infty}^{x}f_{X}dt$ I am a little confused about how to characterize the most important difference between them. The equation above says that the cdf is the integral of the pdf from negative infinity to x. Is it fair to say that the cdf is the integral of the pdf from negative infinity to x?","I am learning stats. On page 20, my book, All of Statistics 1e,  defines a CDF as function that maps x to the probability that a random variable, X, is less than x. $F_{x}(x) = P(X\leq x)$ On page 23 it gives a function $P(a < X < b ) = \int_{a}^{b}f_{X}dx$ and then says that ""the function $f_{X}$ is called the probability density function. We have that..."" $F_{x}(x) = \int_{-\infty}^{x}f_{X}dt$ I am a little confused about how to characterize the most important difference between them. The equation above says that the cdf is the integral of the pdf from negative infinity to x. Is it fair to say that the cdf is the integral of the pdf from negative infinity to x?",,['statistics']
34,Name this paradox about most common first digits in numbers,Name this paradox about most common first digits in numbers,,"I remember hearing about a paradox (not a real paradox, more of a surprising oddity) about frequency of the first digit in a random number being most likely 1, second most likely 2, etc.  This was for measurements of seemingly random things, and it didn't work for uniformly generated pseudorandom numbers.  I also seem to recall there was a case in history of some sort of banking fraud being detected because the data, which was fudged, was not adhering to this law. It was also generalisable so that it didn't matter what base number system you used, the measurements would be distributed in an analagous way. I've googled for various things trying to identify it but I just can't find it because I don't know the right name to search for. I would like to read some more about this subject, so if anyone can tell me the magic terms to search for I'd be grateful, thanks!","I remember hearing about a paradox (not a real paradox, more of a surprising oddity) about frequency of the first digit in a random number being most likely 1, second most likely 2, etc.  This was for measurements of seemingly random things, and it didn't work for uniformly generated pseudorandom numbers.  I also seem to recall there was a case in history of some sort of banking fraud being detected because the data, which was fudged, was not adhering to this law. It was also generalisable so that it didn't matter what base number system you used, the measurements would be distributed in an analagous way. I've googled for various things trying to identify it but I just can't find it because I don't know the right name to search for. I would like to read some more about this subject, so if anyone can tell me the magic terms to search for I'd be grateful, thanks!",,"['statistics', 'random', 'paradoxes']"
35,What is the deepest / most interesting known connection between Trigonometry and Statistics?,What is the deepest / most interesting known connection between Trigonometry and Statistics?,,"I'm teaching both at the same time to different classes in high school, so I just wondered about this. Added by OP on 16.May.2011 (Beijing time) I mean Statistics only, without Probability. In other words, Descriptive Statistics only. This rules out Buffon’s Needle Problem. The occurrence of π is counted only as a connection to geometry. By Trigonometry is meant explicit, non-gratuitous, occurrence of the sine, cosine, tangent, or their reciprocals. Yes, the Law of Cosines fits the bill, but it is on the surface: everyone knows about it. It would be a hugely interesting meta theorem that this were the “deepest” connection between Trigonometry and Descriptive Statistics. My suspicion/hope is that there are deeper connections, somewhat along the line of the surprising use of trig in solving the cubic in closed form, or the use of trigonometric substitutions in evaluating certain integrals. The comment below about the arcsine transformation at first blush seems to be something along this line, but when you follow the link you see that someone is bringing it up only to say how bad it is. So, I hope the intent of my question is now much clearer.","I'm teaching both at the same time to different classes in high school, so I just wondered about this. Added by OP on 16.May.2011 (Beijing time) I mean Statistics only, without Probability. In other words, Descriptive Statistics only. This rules out Buffon’s Needle Problem. The occurrence of π is counted only as a connection to geometry. By Trigonometry is meant explicit, non-gratuitous, occurrence of the sine, cosine, tangent, or their reciprocals. Yes, the Law of Cosines fits the bill, but it is on the surface: everyone knows about it. It would be a hugely interesting meta theorem that this were the “deepest” connection between Trigonometry and Descriptive Statistics. My suspicion/hope is that there are deeper connections, somewhat along the line of the surprising use of trig in solving the cubic in closed form, or the use of trigonometric substitutions in evaluating certain integrals. The comment below about the arcsine transformation at first blush seems to be something along this line, but when you follow the link you see that someone is bringing it up only to say how bad it is. So, I hope the intent of my question is now much clearer.",,"['statistics', 'soft-question', 'trigonometry', 'big-list']"
36,Precise mathematical translation of the 68–95–99.7 rule?(Not a proof!),Precise mathematical translation of the 68–95–99.7 rule?(Not a proof!),,"The rule: In statistics, the 68–95–99.7 rule, also known as the three-sigma rule or empirical rule, states that nearly all values lie within 3 standard deviations of the mean in a normal distribution. About 68.27% of the values lie within 1 standard deviation of the mean. Similarly, about 95.45% of the values lie within 2 standard deviations of the mean. Nearly all (99.73%) of the values lie within 3 standard deviations of the mean. So suppose that I have a set of values (measurements) which has the normal distribution property. Let's call it S. When they say ""about 68.27% of the values"" what values do they mean? Do they mean that the standard deviation of any 68.27 % of the elements of S is smaller than 1? Do they mean something more? Could someone give me a precise mathematical statement that is equivalent to this ""68–95–99.7 rule"". I've posted this on math.stackexchange because I would like a mathematical answer.","The rule: In statistics, the 68–95–99.7 rule, also known as the three-sigma rule or empirical rule, states that nearly all values lie within 3 standard deviations of the mean in a normal distribution. About 68.27% of the values lie within 1 standard deviation of the mean. Similarly, about 95.45% of the values lie within 2 standard deviations of the mean. Nearly all (99.73%) of the values lie within 3 standard deviations of the mean. So suppose that I have a set of values (measurements) which has the normal distribution property. Let's call it S. When they say ""about 68.27% of the values"" what values do they mean? Do they mean that the standard deviation of any 68.27 % of the elements of S is smaller than 1? Do they mean something more? Could someone give me a precise mathematical statement that is equivalent to this ""68–95–99.7 rule"". I've posted this on math.stackexchange because I would like a mathematical answer.",,['statistics']
37,How come in statistics there is very little justification for the formulas used and proofs are almost nonexistent [closed],How come in statistics there is very little justification for the formulas used and proofs are almost nonexistent [closed],,"As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance. Closed 12 years ago . I don't understand why people accept certain formulas in statistics without a mathematical proof style argument. You see this a lot in statistics textbooks and unfortunately this spills over with the instructors who are themselves ignorant of where the formulas come from yet teach them anyway.","As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance. Closed 12 years ago . I don't understand why people accept certain formulas in statistics without a mathematical proof style argument. You see this a lot in statistics textbooks and unfortunately this spills over with the instructors who are themselves ignorant of where the formulas come from yet teach them anyway.",,"['statistics', 'soft-question']"
38,"How to visualize norm 3, norm 4,.. distance between two points in X-Y Plane","How to visualize norm 3, norm 4,.. distance between two points in X-Y Plane",,"Distance between Two points can be expressed as $(|x_1 - x_2|^m + |y_1 - y_2|^m)^{\frac{1}{m}}$ $m = 1$ 1 norm distance (Manhattan Distance) $m = 2$ 2 norm distance (Euclidean Distance) How can I visualize 3 norm , 4 norm ,... distance. I searched internet and all I could find is the comparison of 1 norm and 2 norm . I couldn't be able to find the visualization of 3 norm , 4 norm , ... distance. Can anybody help me to visualizes these distances.","Distance between Two points can be expressed as 1 norm distance (Manhattan Distance) 2 norm distance (Euclidean Distance) How can I visualize 3 norm , 4 norm ,... distance. I searched internet and all I could find is the comparison of 1 norm and 2 norm . I couldn't be able to find the visualization of 3 norm , 4 norm , ... distance. Can anybody help me to visualizes these distances.",(|x_1 - x_2|^m + |y_1 - y_2|^m)^{\frac{1}{m}} m = 1 m = 2,"['statistics', 'plane-geometry']"
39,Covariance of two chi-square random variables,Covariance of two chi-square random variables,,"Let $(X,Y)$ follow bivariate normal distribution where $X,Y$ both follow $N(0,1)$ distribution and $\operatorname{cov}(X,Y)=c$. Determine the $\operatorname{cov}(X^2,Y^2)$. I think that the answer will be $c^2$. But I can't make it out. Any help is appreciated.","Let $(X,Y)$ follow bivariate normal distribution where $X,Y$ both follow $N(0,1)$ distribution and $\operatorname{cov}(X,Y)=c$. Determine the $\operatorname{cov}(X^2,Y^2)$. I think that the answer will be $c^2$. But I can't make it out. Any help is appreciated.",,"['statistics', 'probability-distributions']"
40,Calculate p value,Calculate p value,,"taking stat 101, I was wondering how I could figure out the p-value, with the hypothesis mean being equal to -4 given the data below. Could someone explain the p-value?","taking stat 101, I was wondering how I could figure out the p-value, with the hypothesis mean being equal to -4 given the data below. Could someone explain the p-value?",,['statistics']
41,Better than Casella and Berger's *Statistical Inference*?,Better than Casella and Berger's *Statistical Inference*?,,"I have just finished an undergraduate degree in statistics and am looking into a graduate degree in statistics. One textbook that I've found in my searching is Casella and Berger's Statistical Inference (which is supposed to cover the theory of probability and statistics); however, I've found that many have stated that it lacks clarity. Is there a ""better"" reference for this topic? Note: I used Wackerly et al.'s Mathematical Statistics with Applications when I was an undergraduate.","I have just finished an undergraduate degree in statistics and am looking into a graduate degree in statistics. One textbook that I've found in my searching is Casella and Berger's Statistical Inference (which is supposed to cover the theory of probability and statistics); however, I've found that many have stated that it lacks clarity. Is there a ""better"" reference for this topic? Note: I used Wackerly et al.'s Mathematical Statistics with Applications when I was an undergraduate.",,"['statistics', 'reference-request']"
42,Get a Fisher information matrix for linear model with the normal distribution for measurement error?,Get a Fisher information matrix for linear model with the normal distribution for measurement error?,,"For given linear model $y = x \beta + \epsilon$, where $\beta$ is a $p$-dimentional column vector, and $\epsilon$ is a measurement error that follows a normal distribution, a FIM is a $p \times p$ positive definite matrix. How to find elements of the matrix?","For given linear model $y = x \beta + \epsilon$, where $\beta$ is a $p$-dimentional column vector, and $\epsilon$ is a measurement error that follows a normal distribution, a FIM is a $p \times p$ positive definite matrix. How to find elements of the matrix?",,"['statistics', 'information-geometry']"
43,Problem with unbiased but not consistent estimator,Problem with unbiased but not consistent estimator,,"I have some troubles with understanding of this explanation taken from wikipedia: ""An estimator can be unbiased but not consistent. For example, for an iid sample $\{x _1,..., x_n\}$ one can use $T(X) = x_1$ as the estimator of the mean $E[x]$. This estimator is obviously unbiased, and obviously inconsistent."" Why unbiased, and why inconsistent ... Can someone explain it in more details?","I have some troubles with understanding of this explanation taken from wikipedia: ""An estimator can be unbiased but not consistent. For example, for an iid sample $\{x _1,..., x_n\}$ one can use $T(X) = x_1$ as the estimator of the mean $E[x]$. This estimator is obviously unbiased, and obviously inconsistent."" Why unbiased, and why inconsistent ... Can someone explain it in more details?",,['statistics']
44,Probability density function of a product of uniform random variables,Probability density function of a product of uniform random variables,,"Let $z = xy$ be a product of two uniform random variables, with $x$ having the range $[a, b)$ and $y$ the range $[c, d)$. What is the probability density function of $z$, and how is it calculated?","Let $z = xy$ be a product of two uniform random variables, with $x$ having the range $[a, b)$ and $y$ the range $[c, d)$. What is the probability density function of $z$, and how is it calculated?",,"['statistics', 'probability-distributions', 'uniform-distribution']"
45,Can kurtosis measure peakedness?,Can kurtosis measure peakedness?,,"Wikipedia says kurtosis only measures tailedness but not peakedness. But I remember my teacher said several times that high excess kurtosis usually corresponds to fat tails AND thin peak. High excess kurtosis accompanied by fat tails can be easily seen by the usual definition of kurtosis(fourth central moment). But what about peakedness? If kurtosis doesn't measure it, is there any statistic that can do the job? My Statistics textbook isn't clear about this part.","Wikipedia says kurtosis only measures tailedness but not peakedness. But I remember my teacher said several times that high excess kurtosis usually corresponds to fat tails AND thin peak. High excess kurtosis accompanied by fat tails can be easily seen by the usual definition of kurtosis(fourth central moment). But what about peakedness? If kurtosis doesn't measure it, is there any statistic that can do the job? My Statistics textbook isn't clear about this part.",,['statistics']
46,What does rotational invariance mean in statistics?,What does rotational invariance mean in statistics?,,"What does rotational invariance mean in statistics? The property that the normal distribution satisfies for independent normal distributed $X_i$,  $\Sigma_i X_i$ is also normal with variance $\Sigma_i Var(X_i)$ is referred to as rotational invariance and I want to know why.","What does rotational invariance mean in statistics? The property that the normal distribution satisfies for independent normal distributed $X_i$,  $\Sigma_i X_i$ is also normal with variance $\Sigma_i Var(X_i)$ is referred to as rotational invariance and I want to know why.",,"['statistics', 'normal-distribution', 'covariance', 'invariance']"
47,"In statistics, why do you reject the null hypothesis when the p-value is less than the alpha value (the level of significance)","In statistics, why do you reject the null hypothesis when the p-value is less than the alpha value (the level of significance)",,"This is a question that I've always wondered in statistics, but never had the guts to ask the professor. The professor would say that if the p-value is less than or equal to the level of significance (denoted by alpha) we reject the null hypothesis because the test statistic falls in the rejection region. When I first learned this, I did not understand why were comparing the p values to the alpha values. After all, the alpha values were brought in arbitrarily. What is the reason for comparing them to the alpha values and where do the alpha values of 0.05 and 0.10 come from? Why does the statement $ p_\text{value} \leq \alpha$ allow you to reject $H_0$?","This is a question that I've always wondered in statistics, but never had the guts to ask the professor. The professor would say that if the p-value is less than or equal to the level of significance (denoted by alpha) we reject the null hypothesis because the test statistic falls in the rejection region. When I first learned this, I did not understand why were comparing the p values to the alpha values. After all, the alpha values were brought in arbitrarily. What is the reason for comparing them to the alpha values and where do the alpha values of 0.05 and 0.10 come from? Why does the statement $ p_\text{value} \leq \alpha$ allow you to reject $H_0$?",,"['statistics', 'statistical-inference', 'hypothesis-testing']"
48,"How to find the covariance of sample mean and sample variance $Cov(M,S^2)$ for Poisson distribution?",How to find the covariance of sample mean and sample variance  for Poisson distribution?,"Cov(M,S^2)","Suppose that I have a Poisson distribution $P(\lambda)$ . Let $X_1,X_2,\ldots,X_n$ be independent random variables from the distribution mentioned above. Let us define sample variance $S^2 = \frac{1}{n-1} \sum (X_i - M)^2 $ and sample mean as $M = \frac{1}{n}\sum X_i $ . I want to find the covariance, $Cov(M,S^2)$ . I've seen from this answer that when the distribution is symmetric, they are uncorrelated, which makes it zero. I also know that for large $\lambda$ values Poisson distribution is very close to Gaussian distribution, thus becoming symmetric, and probably the covariance is close to zero. However, I want to find the exact value of $Cov(M,S^2)$ , as I am working with small values of $\lambda$ Currently I tried the following: $Cov(M,S^2) =E( (M-\lambda) (S^2-\lambda) ) = E(MS^2)-\lambda E(M) - \lambda E(S^2) + \lambda^2$ Thus, $Cov(M,S^2) = E(MS^2) - \lambda^2 - \lambda^2 + \lambda^2  = E(MS^2) -\lambda^2$ I am having trouble with calculating $E(MS^2)$ . Any idea?","Suppose that I have a Poisson distribution . Let be independent random variables from the distribution mentioned above. Let us define sample variance and sample mean as . I want to find the covariance, . I've seen from this answer that when the distribution is symmetric, they are uncorrelated, which makes it zero. I also know that for large values Poisson distribution is very close to Gaussian distribution, thus becoming symmetric, and probably the covariance is close to zero. However, I want to find the exact value of , as I am working with small values of Currently I tried the following: Thus, I am having trouble with calculating . Any idea?","P(\lambda) X_1,X_2,\ldots,X_n S^2 = \frac{1}{n-1} \sum (X_i - M)^2  M = \frac{1}{n}\sum X_i  Cov(M,S^2) \lambda Cov(M,S^2) \lambda Cov(M,S^2) =E( (M-\lambda) (S^2-\lambda) ) = E(MS^2)-\lambda E(M) - \lambda E(S^2) + \lambda^2 Cov(M,S^2) = E(MS^2) - \lambda^2 - \lambda^2 + \lambda^2  = E(MS^2) -\lambda^2 E(MS^2)",['statistics']
49,What does it mean when a statistician says I’m 90% confident that the mean of the population is between 1 and 9?,What does it mean when a statistician says I’m 90% confident that the mean of the population is between 1 and 9?,,Does that mean if I draw samples from the population that 90% of the time I'll get a number between 1 and 9? Added : assume normal distribution for the population.,Does that mean if I draw samples from the population that 90% of the time I'll get a number between 1 and 9? Added : assume normal distribution for the population.,,['statistics']
50,Minimum variance unbiased estimator for scale parameter of a certain gamma distribution,Minimum variance unbiased estimator for scale parameter of a certain gamma distribution,,"Let $X_1, X_2, ..., X_n$ be a random sample from a distribution with p.d.f., $$f(x;\theta)=\theta^2xe^{-x\theta} ; 0<x<\infty, \theta>0$$ Obtain minimum variance unbiased estimator of $\theta$ and examine whether it is attained? MY WORK: Using MLE i have found the estimator for $\theta=\frac{2}{\bar{x}}$ Or as $$X\sim \operatorname{Gamma}(2, \theta)$$ So $E(X)=2\theta$ , $E(\frac{X}{2})=\theta$ so can I take $\frac {X}{2}$ as unbiased estimator of $\theta$ . I'm stuck and confused need some help. Thank u.","Let be a random sample from a distribution with p.d.f., Obtain minimum variance unbiased estimator of and examine whether it is attained? MY WORK: Using MLE i have found the estimator for Or as So , so can I take as unbiased estimator of . I'm stuck and confused need some help. Thank u.","X_1, X_2, ..., X_n f(x;\theta)=\theta^2xe^{-x\theta} ; 0<x<\infty, \theta>0 \theta \theta=\frac{2}{\bar{x}} X\sim \operatorname{Gamma}(2, \theta) E(X)=2\theta E(\frac{X}{2})=\theta \frac {X}{2} \theta","['statistics', 'probability-distributions', 'statistical-inference', 'expected-value', 'parameter-estimation']"
51,What is the most scientific way to assign weights to historical data?,What is the most scientific way to assign weights to historical data?,,"This is a common question I usually face while processing historical data. I have year on year data of an event for the past N years.I would like to assign weights to the data of these N years so that the data corresponding to the most recent year as the highest weight and the data corresponding to the oldest year has the least. This is to give more importance to recent trend as compared to very old trends. Questions Is there any scientific way to assign weights to the years i.e. what should be the weight for the most recent year, what should be the weight for the previous year an so on? Is there any scientific justification in assigning equal weights to the preceding historical data after a certain point? Is there any scientific justification to ignore the preceding historical data after a certain point i.e. weight = 0? I think the answers will depend on the exact nature of the data under study and we might not have a general answer that will work with all data. Even then can we at least have a rule of thumb which is independent of the actual data. Any reference or pointers to literature would also be helpful.","This is a common question I usually face while processing historical data. I have year on year data of an event for the past N years.I would like to assign weights to the data of these N years so that the data corresponding to the most recent year as the highest weight and the data corresponding to the oldest year has the least. This is to give more importance to recent trend as compared to very old trends. Questions Is there any scientific way to assign weights to the years i.e. what should be the weight for the most recent year, what should be the weight for the previous year an so on? Is there any scientific justification in assigning equal weights to the preceding historical data after a certain point? Is there any scientific justification to ignore the preceding historical data after a certain point i.e. weight = 0? I think the answers will depend on the exact nature of the data under study and we might not have a general answer that will work with all data. Even then can we at least have a rule of thumb which is independent of the actual data. Any reference or pointers to literature would also be helpful.",,"['statistics', 'average', 'data-analysis']"
52,What does -1.13 times faster mean?,What does -1.13 times faster mean?,,"I'm reading High Performance JavaScript , and I think the graphs in one chapter are just plain wrong. Here is one on Google Books . The y axis is ""Times faster"", and it runs from -1.5 to +4.0. Now, I would have thought that ""1 times faster"" means ""no faster"", ""2 times faster"" means ""twice as fast"", and ""0.5 times faster"" means ""half as fast""/""twice as slow"". Have they just got completely confused in that graph, or is it me?","I'm reading High Performance JavaScript , and I think the graphs in one chapter are just plain wrong. Here is one on Google Books . The y axis is ""Times faster"", and it runs from -1.5 to +4.0. Now, I would have thought that ""1 times faster"" means ""no faster"", ""2 times faster"" means ""twice as fast"", and ""0.5 times faster"" means ""half as fast""/""twice as slow"". Have they just got completely confused in that graph, or is it me?",,['statistics']
53,"If $X \sim N(0,1)$, why is $E(X^2)=1$?","If , why is ?","X \sim N(0,1) E(X^2)=1","If $X$ is a normally distributed with mean $0$ and variance $1$, expectation of $X$ equals $0$ but why is $E(X^2)=1$?","If $X$ is a normally distributed with mean $0$ and variance $1$, expectation of $X$ equals $0$ but why is $E(X^2)=1$?",,"['statistics', 'normal-distribution']"
54,Where did this statistics formula come from: $E[X^2] = \mu^2 + \sigma^2$,Where did this statistics formula come from:,E[X^2] = \mu^2 + \sigma^2,I am studying statistics and I need some guidance as to where this formula came from. All I know is that $\displaystyle E[X^2] = x^2 \sum_{i=0}^n p_{i}(x)$,I am studying statistics and I need some guidance as to where this formula came from. All I know is that $\displaystyle E[X^2] = x^2 \sum_{i=0}^n p_{i}(x)$,,['statistics']
55,Why do you use n-1 in Standard error of the mean but n in hypothesis testing,Why do you use n-1 in Standard error of the mean but n in hypothesis testing,,When you divide in hypothesis testing you use the formula: $$ \frac{\bar X-\mu}{s/\sqrt n} $$ but the standard error of the mean is: $$ \frac s{\sqrt {n-1}} $$ Why don't you use $n-1$ when calculating the standard error using the sample population?,When you divide in hypothesis testing you use the formula: $$ \frac{\bar X-\mu}{s/\sqrt n} $$ but the standard error of the mean is: $$ \frac s{\sqrt {n-1}} $$ Why don't you use $n-1$ when calculating the standard error using the sample population?,,"['statistics', 'standard-error']"
56,Intuitive explanation for dividing by n-1 when calculating sample variance? [duplicate],Intuitive explanation for dividing by n-1 when calculating sample variance? [duplicate],,"This question already has answers here : Intuitive Explanation of Bessel's Correction (7 answers) Closed 8 years ago . I understand how to mathematically show that the sample variance (that involves dividing by n-1) is an unbiased estimator of the population variance (which divides by n), and the mathematics has been shown many times here on Math.SE. I am wondering however if there is an intuitive way to understand this result that I can use to easily explain why this is done to layman.  So far I have seen many derivations but I haven't seen an elegant intuitve explanation for the result.","This question already has answers here : Intuitive Explanation of Bessel's Correction (7 answers) Closed 8 years ago . I understand how to mathematically show that the sample variance (that involves dividing by n-1) is an unbiased estimator of the population variance (which divides by n), and the mathematics has been shown many times here on Math.SE. I am wondering however if there is an intuitive way to understand this result that I can use to easily explain why this is done to layman.  So far I have seen many derivations but I haven't seen an elegant intuitve explanation for the result.",,['statistics']
57,Normalization for Chi square test,Normalization for Chi square test,,"The formula for the Chi-Square test statistic is the following: $\chi^2 = \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i}$ where O - is observed data, and E - is expected. I'm curious why it depends on the absolute values? For example, if we change the units we're measuring we'll get a different statistics. Suppose we're performing a test on apple weights. One of the samples weights 165 gram, and we expect it to be 182 gram, then the part of the formula will be: $\frac{(165 - 182)^2}{182} \sim 1.58791$ http://en.wikipedia.org/wiki/Pearson's_chi-squared_test Now suppose we're living in a country where the precision is on the top. We use milligrams  for everything and we get the same results in different units: 165000 milligrams and 182000, respectively. The statistic: $\frac{(165000 - 182000)^2}{182000} \sim 1587.91$ So our conclusion will be different based on the units we used. Why? What am I missing and why the values are not normalized in the Chi-squared test?","The formula for the Chi-Square test statistic is the following: $\chi^2 = \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i}$ where O - is observed data, and E - is expected. I'm curious why it depends on the absolute values? For example, if we change the units we're measuring we'll get a different statistics. Suppose we're performing a test on apple weights. One of the samples weights 165 gram, and we expect it to be 182 gram, then the part of the formula will be: $\frac{(165 - 182)^2}{182} \sim 1.58791$ http://en.wikipedia.org/wiki/Pearson's_chi-squared_test Now suppose we're living in a country where the precision is on the top. We use milligrams  for everything and we get the same results in different units: 165000 milligrams and 182000, respectively. The statistic: $\frac{(165000 - 182000)^2}{182000} \sim 1587.91$ So our conclusion will be different based on the units we used. Why? What am I missing and why the values are not normalized in the Chi-squared test?",,"['statistics', 'probability-distributions', 'statistical-inference']"
58,"What is the T-distribution, and what is it used for?","What is the T-distribution, and what is it used for?",,"(I'll post my own answer to this, but don't hesitate to post your own!) Student's t-distribution, or T-distribution, was introduced in 1908 by William Sealey Gossett writing under the pseudonym ""Student"". What is it, and what is it for?","(I'll post my own answer to this, but don't hesitate to post your own!) Student's t-distribution, or T-distribution, was introduced in 1908 by William Sealey Gossett writing under the pseudonym ""Student"". What is it, and what is it for?",,['statistics']
59,Confidence interval for uniform,Confidence interval for uniform,,"A random variable is uniformly distributed over $(0,\theta)$. The maximum of a random sample of $n$, call $y_n$ is sufficient for $\theta$ and it is also the maximum likelihood estimator. Show also that a $100\gamma\%$ conﬁdence interval for $\theta$ is $(y_n, y_n /(1 − \gamma )^{1/n})$. Could anyone tell me how to deal with this problem? Do I have to use the central limit theorem?","A random variable is uniformly distributed over $(0,\theta)$. The maximum of a random sample of $n$, call $y_n$ is sufficient for $\theta$ and it is also the maximum likelihood estimator. Show also that a $100\gamma\%$ conﬁdence interval for $\theta$ is $(y_n, y_n /(1 − \gamma )^{1/n})$. Could anyone tell me how to deal with this problem? Do I have to use the central limit theorem?",,['statistics']
60,"What do the two things such that ""Data is fixed"" and ""Parameters vary"" in Bayesian statistics mean?","What do the two things such that ""Data is fixed"" and ""Parameters vary"" in Bayesian statistics mean?",,"While following the bayesian statistics online, the lecturer said that ""Data is fixed"" and ""Parameters vary"" in Bayesian statistics. But the explanations I got doesn't really make me understand what those things mean. The two things sound important to begin with the basic idea of Bayesian statistics. Hope to hear explanations.","While following the bayesian statistics online, the lecturer said that ""Data is fixed"" and ""Parameters vary"" in Bayesian statistics. But the explanations I got doesn't really make me understand what those things mean. The two things sound important to begin with the basic idea of Bayesian statistics. Hope to hear explanations.",,"['statistics', 'mathematical-modeling', 'bayesian', 'bayes-theorem', 'bayesian-network']"
61,"What is a good measure of ""controversy"", given a support score and opposition score?","What is a good measure of ""controversy"", given a support score and opposition score?",,"Suppose I have a topic or discussion, and a number of ""support"" and ""opposition"" points on each side (You can also think of them as ""upvotes"" and ""downvotes"") and I want to calculate a score of how ""controversial"" a topic is. (Let $p$ be the support score, $c$ be the opposition score, and $f(p, c)$ be the function that determines the controversy score.) It should have the following properties: Controversy is maximized when equal support is given to both sides. Given that some property $g(p, c)$ is held constant (such that the slope of the tangent line of the level curve of $g(p, c)$ at any point is never positive), $f(p, c)$ should be maximized when $p = c$. More support on both sides means that more people care and therefore there is more controversy. Given that $p/c$ is held constant, a higher value of $p$ or $c$ should result in a higher value of $f(p, c)$. The amount of controversy is the same for the same imbalance of support no matter which side the imbalance favours. $f(p, c)$ should equal $f(c, p)$. All the support being on one side means there is no controversy. Given that either $p$ or $c$ is equal to zero, $f(p, c)$ should be equal to zero. Is there any function like this that is already in use? If not, could one be devised?","Suppose I have a topic or discussion, and a number of ""support"" and ""opposition"" points on each side (You can also think of them as ""upvotes"" and ""downvotes"") and I want to calculate a score of how ""controversial"" a topic is. (Let $p$ be the support score, $c$ be the opposition score, and $f(p, c)$ be the function that determines the controversy score.) It should have the following properties: Controversy is maximized when equal support is given to both sides. Given that some property $g(p, c)$ is held constant (such that the slope of the tangent line of the level curve of $g(p, c)$ at any point is never positive), $f(p, c)$ should be maximized when $p = c$. More support on both sides means that more people care and therefore there is more controversy. Given that $p/c$ is held constant, a higher value of $p$ or $c$ should result in a higher value of $f(p, c)$. The amount of controversy is the same for the same imbalance of support no matter which side the imbalance favours. $f(p, c)$ should equal $f(c, p)$. All the support being on one side means there is no controversy. Given that either $p$ or $c$ is equal to zero, $f(p, c)$ should be equal to zero. Is there any function like this that is already in use? If not, could one be devised?",,"['statistics', 'scoring-algorithm']"
62,How to interpret standard deviation of samples with different means?,How to interpret standard deviation of samples with different means?,,Let be two samples collections A and B. Mean of the first is 5 and the second is 15. Standard deviation of the first is 2 and the second is 5. Can we conclude something even though the two datasets have a different mean ? Can we consider the ratio (standard deviation / mean) and conclude that A (0.4) is more widespread than B (0.33) ?,Let be two samples collections A and B. Mean of the first is 5 and the second is 15. Standard deviation of the first is 2 and the second is 5. Can we conclude something even though the two datasets have a different mean ? Can we consider the ratio (standard deviation / mean) and conclude that A (0.4) is more widespread than B (0.33) ?,,"['statistics', 'standard-deviation']"
63,Method of moments with a Gamma distribution,Method of moments with a Gamma distribution,,"I'm more so confused on a specific step in obtaining the MOM than completely obtaining the MOM: Given a random sample of $ Y_1 , Y_2,..., Y_i$ ~ $ Gamma (\alpha , \beta)$ find the MOM So I found the population and sample moments $u_1^{'}= \alpha \beta  $ $ u_2^{'} = \sigma^2 + \mu^2 = \alpha ^2 \beta^2 +  \alpha \beta^2$ $  m_1^{'} = \overline  Y $ $   m_2^{'} = \frac{1}{n} \sum_{i=1}^{n} Y_i^2 $ solving for $ \hat \alpha_{MOM}$ I get $\hat \alpha_{MOM} = \frac{\overline Y}{\beta}$ The solutions say this ends up being equal to: $ \hat \alpha_{MOM} = \frac{\frac{1}{n} (\sum_{i=1}^{n} (Y_i - \overline Y) ^ 2)}{n \overline Y} $ I think I'm forgetting some property because I have no idea how they transformed the initial $ \hat \alpha_{MOM}$ equation into the 2nd $ \hat \alpha_{MOM}$ equation.",I'm more so confused on a specific step in obtaining the MOM than completely obtaining the MOM: Given a random sample of ~ find the MOM So I found the population and sample moments solving for I get The solutions say this ends up being equal to: I think I'm forgetting some property because I have no idea how they transformed the initial equation into the 2nd equation.," Y_1 , Y_2,..., Y_i  Gamma (\alpha , \beta) u_1^{'}= \alpha \beta    u_2^{'} = \sigma^2 + \mu^2 = \alpha ^2 \beta^2 +  \alpha \beta^2   m_1^{'} = \overline  Y     m_2^{'} = \frac{1}{n} \sum_{i=1}^{n} Y_i^2   \hat \alpha_{MOM} \hat \alpha_{MOM} = \frac{\overline Y}{\beta}  \hat \alpha_{MOM} = \frac{\frac{1}{n} (\sum_{i=1}^{n} (Y_i - \overline Y) ^ 2)}{n \overline Y}   \hat \alpha_{MOM}  \hat \alpha_{MOM}","['statistics', 'parameter-estimation', 'gamma-distribution']"
64,Chi-square goodness of fit test proof,Chi-square goodness of fit test proof,,"I understand the classcial $\chi^2$ ""goodness of fit"" test used in Statistics, in which we compute $\sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i}$ and, by comparing this quantity to a value found in a table of $\chi^2$ law (with a given risk $\alpha = 5\%$ for example), we decide if we should or not accept the hypothesis that the sample is likely to be an observation or not of a given distribution. But I haven't found a good precise proof online yet, that shows that it's not only a good ""recipe"", but also has a strict proof, using probability theory (I know it exists, but I haven't found one yet). Do you know a good detailed proof?","I understand the classcial $\chi^2$ ""goodness of fit"" test used in Statistics, in which we compute $\sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i}$ and, by comparing this quantity to a value found in a table of $\chi^2$ law (with a given risk $\alpha = 5\%$ for example), we decide if we should or not accept the hypothesis that the sample is likely to be an observation or not of a given distribution. But I haven't found a good precise proof online yet, that shows that it's not only a good ""recipe"", but also has a strict proof, using probability theory (I know it exists, but I haven't found one yet). Do you know a good detailed proof?",,"['statistics', 'chi-squared']"
65,Consecutive Coin Toss with static tosses,Consecutive Coin Toss with static tosses,,"I'm writing an algorithm for a coin toss problem. But I have a problem understanding the calculation given. Here is the question: You have an unbiased coin which you want to keep tossing until you get   N consecutive heads. You've tossed the coin M times and surprisingly,   all tosses resulted in heads. What is the expected number of   additional tosses needed until you get N consecutive heads? If N = 2 and M = 0, you need to keep tossing the coin until you get 2   consecutive heads. It is not hard to show that on average, 6 coin   tosses are needed. If N = 2 and M = 1, you need 2 consecutive heads and have already have   1. You need to toss once more no matter what. In that first toss, if you get heads, you are done. Otherwise, you need to start over, as the   consecutive counter resets, and you need to keep tossing the coin   until you get N=2 consecutive heads. The expected number of coin   tosses is thus 1 + (0.5 * 0 + 0.5 * 6) = 4.0 If N = 3 and M = 3, you already have got 3 heads, so you do not need   any more tosses. Now my problem is understanding the calculation: 1 + (0.5 * 0 + 0.5 * 6) = 4.0 when N = 2 and M = 1. I understood how they got the 6 (which is basically calculating it when M = 0, formula here ). Now what if I'm going to calculate N = 3, M = 1 or N = 3, M = 2 ? Could someone write this calculation in a formula for me please? What is the 1 ? What is (0.5 * 0 + 0.5 * 6) ?","I'm writing an algorithm for a coin toss problem. But I have a problem understanding the calculation given. Here is the question: You have an unbiased coin which you want to keep tossing until you get   N consecutive heads. You've tossed the coin M times and surprisingly,   all tosses resulted in heads. What is the expected number of   additional tosses needed until you get N consecutive heads? If N = 2 and M = 0, you need to keep tossing the coin until you get 2   consecutive heads. It is not hard to show that on average, 6 coin   tosses are needed. If N = 2 and M = 1, you need 2 consecutive heads and have already have   1. You need to toss once more no matter what. In that first toss, if you get heads, you are done. Otherwise, you need to start over, as the   consecutive counter resets, and you need to keep tossing the coin   until you get N=2 consecutive heads. The expected number of coin   tosses is thus 1 + (0.5 * 0 + 0.5 * 6) = 4.0 If N = 3 and M = 3, you already have got 3 heads, so you do not need   any more tosses. Now my problem is understanding the calculation: 1 + (0.5 * 0 + 0.5 * 6) = 4.0 when N = 2 and M = 1. I understood how they got the 6 (which is basically calculating it when M = 0, formula here ). Now what if I'm going to calculate N = 3, M = 1 or N = 3, M = 2 ? Could someone write this calculation in a formula for me please? What is the 1 ? What is (0.5 * 0 + 0.5 * 6) ?",,['statistics']
66,What are the measurement units of Fisher information? (Dimensional Analysis),What are the measurement units of Fisher information? (Dimensional Analysis),,"I know that there is a strong relationship between Shannon entropy and thermodynamic entropy -- they even have the same units and differ only by a constant factor. This suggests that they both intrinsically describe the same fundamental concept. Wikipedia says that there is a strong relationship between Fisher information and relative entropy (also called Kullback-Leibler divergence), as does an answer to a previous question on Math.SE. However, looking at the relevant formulas, it does not look like Fisher information would be measured with the same units that relative entropy would. This suggests that they are measuring fundamentally distinct, albeit related, physical concepts. The formula for the Shannon entropy can be written as follows: $$\int  [ - \log p(x) ]\ p(x) \, dx $$ This is usually measured in bits. What are the units of Fisher information (given that Shannon entropy can be measured in bits)? Fisher information can be written as: $$\int \left(\frac{\partial}{\partial \theta} \log p(x; \theta) \right)^2 p(x;\theta) \, dx $$ My guess, based on comparing the definitions of Shannon entropy and Fisher information, is that the latter would be measured in units something like $$\frac{\text{bit}^2}{\Theta^2} $$ where $\Theta$ is the unit of measurement of the parameter $\theta$ that is to be estimated. I am not quite sure how to account for the effects of the extra partial differentiation compared to the definition of Shannon entropy. Perhaps the expectation operation $\int ( \cdot) p(y) \, dy$ should leave the units unchanged, although I don't know how to give a non-intuitive explanation of this suspicion. Since the Fisher information is the variance of the score, this question might be answered by first deriving the units of the score. This question might be related, although it was unanswered.","I know that there is a strong relationship between Shannon entropy and thermodynamic entropy -- they even have the same units and differ only by a constant factor. This suggests that they both intrinsically describe the same fundamental concept. Wikipedia says that there is a strong relationship between Fisher information and relative entropy (also called Kullback-Leibler divergence), as does an answer to a previous question on Math.SE. However, looking at the relevant formulas, it does not look like Fisher information would be measured with the same units that relative entropy would. This suggests that they are measuring fundamentally distinct, albeit related, physical concepts. The formula for the Shannon entropy can be written as follows: $$\int  [ - \log p(x) ]\ p(x) \, dx $$ This is usually measured in bits. What are the units of Fisher information (given that Shannon entropy can be measured in bits)? Fisher information can be written as: $$\int \left(\frac{\partial}{\partial \theta} \log p(x; \theta) \right)^2 p(x;\theta) \, dx $$ My guess, based on comparing the definitions of Shannon entropy and Fisher information, is that the latter would be measured in units something like $$\frac{\text{bit}^2}{\Theta^2} $$ where $\Theta$ is the unit of measurement of the parameter $\theta$ that is to be estimated. I am not quite sure how to account for the effects of the extra partial differentiation compared to the definition of Shannon entropy. Perhaps the expectation operation $\int ( \cdot) p(y) \, dy$ should leave the units unchanged, although I don't know how to give a non-intuitive explanation of this suspicion. Since the Fisher information is the variance of the score, this question might be answered by first deriving the units of the score. This question might be related, although it was unanswered.",,"['statistics', 'information-theory', 'dimensional-analysis', 'fisher-information']"
67,Does Variance depend on change of scale and origin?,Does Variance depend on change of scale and origin?,,I know that SD depends on change of scale and not on change of origin. But what about Variance? And why?,I know that SD depends on change of scale and not on change of origin. But what about Variance? And why?,,['statistics']
68,Confidence interval multiplication,Confidence interval multiplication,,"The question looks pretty simple but I can't get my hands on it: Say I have a probability which is the product of two other independent probabilities $p = p_1p_2$. I have estimated each probability $p_1$ and $p_2$ and found some $95\%$ confidence interval for each. How do I obtain a $95\%$ confidence interval for $p$? Taking the product of the bounds of the interval won't work as I would be taking $95\%$ of a $95\%$ confidence interval resulting in an approximately $90\%$ which is not what I want. So conversely I would be taking $97.5\%$ confidence interval for each and by multiplying the bounds I will obtain a $95\%$ confidence interval, is that right? I feel like something is going wrong. In my situation I deal with probabilities but it could be anything so this question can be generalised to any type of confidence intervals. If my reasoning is correct, could someone convince me that's the correct way of doing so?","The question looks pretty simple but I can't get my hands on it: Say I have a probability which is the product of two other independent probabilities $p = p_1p_2$. I have estimated each probability $p_1$ and $p_2$ and found some $95\%$ confidence interval for each. How do I obtain a $95\%$ confidence interval for $p$? Taking the product of the bounds of the interval won't work as I would be taking $95\%$ of a $95\%$ confidence interval resulting in an approximately $90\%$ which is not what I want. So conversely I would be taking $97.5\%$ confidence interval for each and by multiplying the bounds I will obtain a $95\%$ confidence interval, is that right? I feel like something is going wrong. In my situation I deal with probabilities but it could be anything so this question can be generalised to any type of confidence intervals. If my reasoning is correct, could someone convince me that's the correct way of doing so?",,['statistics']
69,"Differential Entropy and ""Limiting density of discrete points""","Differential Entropy and ""Limiting density of discrete points""",,"I stumbled across the concept of differential entropy and was left puzzled by the Wikipedia page Limiting density of discrete points . The related ""talk page"" Talk: Limiting density of discrete points added to my confusion. The first points where I struggled is based on the statement that Shannon's differential entropy is not dimensionally correct, which is objected to in the Talk page. The differential entropy is given by $$ h(x) = - \int p(x) \log p(x) \mathrm{d}x $$ The Wiki page argues that as $h$ is to be dimensionless, the probability density must have dimension "" $1 / \mathrm{d}x$ "" , which would result in the logarithm argument not being dimensionless. This made sense to me actually, as a probability density will not be dimensionless. Yet, on the talk page (second link above), the following objection is presented. The differential entropy is the limit as $\Delta \to 0$ of a Riemann sum $$ -\sum p(x)\Delta \log \big( p(x) \Delta  \big) $$ which, so it is argued, is dimensionally consistent as $p(x) * \Delta$ is dimensionless. The latter can be written as $$-\sum p(x)\Delta \log \big( p(x)   \big) -\sum p(x)\Delta \log ( \Delta ) $$ and, while the second term could be proven to vanish,  the first term yields the differential entropy formula. The first question is, who is right in this argument? I understand that a term as $\log (A *B) $ where $A$ and $B$ are quantities with dimensions say of length and inverse of length, can be written as $\log(A) + \log(B)$ . The argument of the logarithm is not dimensionless, but the whole expression is ultimately at least invariant to  a change of units. Yet, in the manipulation presented in the second link, a term vanishes, and the expression is not independent of the units chosen. The second question I have concerns the ""invariant measure"" $m(x)$ used by Jaynes to correct the differential entropy formula, leading to the expression $$ H(x) = - \int p(x) \log \frac {p(x)}{m(x)} \mathrm{d}x $$ I understand how this expression is now dimensionally consistent. Yet its consequences seem quite strange to me. For example, if a uniform distribution with support over $[a,b]$ is considered, the differential entropy equals $\log[b-a]$ : it depends on the support length, as seems meaningful. IT does however also depend on the system of units chosen to measure length). To use Jaynes's equation, I believe a legitimate choice is $$m(x)  = \frac{1}{b-a}$$ Then, Jaynes's entropy turns out to equal $0$ , regardless of the support length. The second question then, is this conclusion of mine correct? Would not any constant value do the job, as far as dimensionality is concerned, in lieu of $m(x)$ ? I did read some original papers from Jaynes, but cannot work it out. Adapting, to the best of my understanding, his reasoning to the uniform distribution case I mentioned before, he starts from the discrete entropy expression $$ H_{d} = -\sum_{i} p_i \log(p_i) $$ over discrete points $x_1, x_2, \dots, x_n$ Further by noting $$\lim_{n \to \infty} n (x_{i+1}-x_i) = b-a$$ he writes $$p_i = p(x) \frac{1}{n m(x_i)} $$ which again for the uniform distribution I translate as $$ p_i = \frac{1}{n} = \frac{1}{b-a} \frac{b-a}{n}$$ which does make sense, yet it yields once the sum is turned to an integral, to the term $\log(\frac{p(x)}{n m(x)}) $ which as I said, seems equal to zero for any uniform distribution. I would be most grateful for a clarification. Sorry for the verbosity but I hope that by adding all the passages it will be easier to pinpoint my mistake.","I stumbled across the concept of differential entropy and was left puzzled by the Wikipedia page Limiting density of discrete points . The related ""talk page"" Talk: Limiting density of discrete points added to my confusion. The first points where I struggled is based on the statement that Shannon's differential entropy is not dimensionally correct, which is objected to in the Talk page. The differential entropy is given by The Wiki page argues that as is to be dimensionless, the probability density must have dimension "" "" , which would result in the logarithm argument not being dimensionless. This made sense to me actually, as a probability density will not be dimensionless. Yet, on the talk page (second link above), the following objection is presented. The differential entropy is the limit as of a Riemann sum which, so it is argued, is dimensionally consistent as is dimensionless. The latter can be written as and, while the second term could be proven to vanish,  the first term yields the differential entropy formula. The first question is, who is right in this argument? I understand that a term as where and are quantities with dimensions say of length and inverse of length, can be written as . The argument of the logarithm is not dimensionless, but the whole expression is ultimately at least invariant to  a change of units. Yet, in the manipulation presented in the second link, a term vanishes, and the expression is not independent of the units chosen. The second question I have concerns the ""invariant measure"" used by Jaynes to correct the differential entropy formula, leading to the expression I understand how this expression is now dimensionally consistent. Yet its consequences seem quite strange to me. For example, if a uniform distribution with support over is considered, the differential entropy equals : it depends on the support length, as seems meaningful. IT does however also depend on the system of units chosen to measure length). To use Jaynes's equation, I believe a legitimate choice is Then, Jaynes's entropy turns out to equal , regardless of the support length. The second question then, is this conclusion of mine correct? Would not any constant value do the job, as far as dimensionality is concerned, in lieu of ? I did read some original papers from Jaynes, but cannot work it out. Adapting, to the best of my understanding, his reasoning to the uniform distribution case I mentioned before, he starts from the discrete entropy expression over discrete points Further by noting he writes which again for the uniform distribution I translate as which does make sense, yet it yields once the sum is turned to an integral, to the term which as I said, seems equal to zero for any uniform distribution. I would be most grateful for a clarification. Sorry for the verbosity but I hope that by adding all the passages it will be easier to pinpoint my mistake."," h(x) = - \int p(x) \log p(x) \mathrm{d}x  h 1 / \mathrm{d}x \Delta \to 0  -\sum p(x)\Delta \log \big( p(x) \Delta  \big)  p(x) * \Delta -\sum p(x)\Delta \log \big( p(x)   \big) -\sum p(x)\Delta \log ( \Delta )  \log (A *B)  A B \log(A) + \log(B) m(x)  H(x) = - \int p(x) \log \frac {p(x)}{m(x)} \mathrm{d}x  [a,b] \log[b-a] m(x)  = \frac{1}{b-a} 0 m(x)  H_{d} = -\sum_{i} p_i \log(p_i)  x_1, x_2, \dots, x_n \lim_{n \to \infty} n (x_{i+1}-x_i) = b-a p_i = p(x) \frac{1}{n m(x_i)}   p_i = \frac{1}{n} = \frac{1}{b-a} \frac{b-a}{n} \log(\frac{p(x)}{n m(x)}) ","['statistics', 'entropy']"
70,"What is the expectation of norm of $[X_1,\ldots, X_n]$ where $X_i$ are indpendent complex Gaussian random variables",What is the expectation of norm of  where  are indpendent complex Gaussian random variables,"[X_1,\ldots, X_n] X_i","Consider a random vector $X=[X_1, X_2, \ldots , X_n]$ where $X_i$ ($i \in 1, 2,\ldots, n$) are independent complex Gaussian random variables with zero mean and variance $\sigma_i^2$, i.e.,  $X_i \sim CN(0, \sigma_i^2)$. How can I find expectation of norm of $X$, where the norm of $X$ is given by    \begin{equation} \|X\|=\sqrt{\sum_{i=1}^n |X_i|^2} \end{equation} Any help regarding this problem is really appreciated. Thanks.","Consider a random vector $X=[X_1, X_2, \ldots , X_n]$ where $X_i$ ($i \in 1, 2,\ldots, n$) are independent complex Gaussian random variables with zero mean and variance $\sigma_i^2$, i.e.,  $X_i \sim CN(0, \sigma_i^2)$. How can I find expectation of norm of $X$, where the norm of $X$ is given by    \begin{equation} \|X\|=\sqrt{\sum_{i=1}^n |X_i|^2} \end{equation} Any help regarding this problem is really appreciated. Thanks.",,"['statistics', 'random-variables', 'normal-distribution', 'random', 'random-functions']"
71,Recurrence relation in the total progeny of a branching process,Recurrence relation in the total progeny of a branching process,,"Let $Y_{n}= Z_0+Z_1+\cdots+Z_n$ model a branching process as the total number of individuals up through generation $n$.  The total progeny can be described as $Y = \lim_{n \to \infty} Y_n$. In order to analyze expected generation size, we can follow $$\lim_{n \to \infty} E(Z_n) = \lim_{n \to \infty}\mu^n= \begin{cases} 0, & \mu<1,\\ 1, & \mu=1,\\ \infty, & \mu>1. \end{cases}$$ It can be shown that for the critical case where $\mu =1$, $$E(Y) = \sum_{i=0}^n E(Z_i)=\infty.$$ Now let $\psi_n(s)= E(s^{Y_n})$ be the probability generating function of $Y_n$. How would one show that $\psi_n$ satisfies the recurrence relation $\psi_n(s)=sG(\psi_{n-1}(s))$ for $n=1,2,\ldots,$ where $G(s)$ is the probability generating function of the offspring distribution?","Let $Y_{n}= Z_0+Z_1+\cdots+Z_n$ model a branching process as the total number of individuals up through generation $n$.  The total progeny can be described as $Y = \lim_{n \to \infty} Y_n$. In order to analyze expected generation size, we can follow $$\lim_{n \to \infty} E(Z_n) = \lim_{n \to \infty}\mu^n= \begin{cases} 0, & \mu<1,\\ 1, & \mu=1,\\ \infty, & \mu>1. \end{cases}$$ It can be shown that for the critical case where $\mu =1$, $$E(Y) = \sum_{i=0}^n E(Z_i)=\infty.$$ Now let $\psi_n(s)= E(s^{Y_n})$ be the probability generating function of $Y_n$. How would one show that $\psi_n$ satisfies the recurrence relation $\psi_n(s)=sG(\psi_{n-1}(s))$ for $n=1,2,\ldots,$ where $G(s)$ is the probability generating function of the offspring distribution?",,['statistics']
72,MLE (Maximum Likelihood Estimator) of Beta Distribution,MLE (Maximum Likelihood Estimator) of Beta Distribution,,"Let $X_1,\ldots,X_n$ be i.i.d. random variables with a common density function given by: $f(x\mid\theta)=\theta x^{\theta-1}$ for $x\in[0,1]$ and $\theta>0$. Clearly this is a $\operatorname{BETA}(\theta,1)$ distribution.  Calculate the maximum likelihood estimator of $\theta$. After going through all the steps with the log likelihood, I end up calculating that the maximum likelihood estimator is $\hat\theta$ below: $$L:=\prod_{i=1}^N\theta x_i^{\theta-1}$$ $$l:=\ln(L)=\ln\left(\prod_{i=1}^N\theta x_i^{\theta-1}\right)=n\ln(\theta)+\sum_{i=1}^n(\theta-1)\ln(x_i)$$ $$\frac{dl}{d\theta}=\frac{n}{\theta}+\sum_{i=1}^n\ln(x_i)$$ $$\hat \theta=\frac{-n}{\sum_{i=1}^n\ln(x_i)}$$ But something about this doesn't look quite right to me.  Did I go wrong somewhere?","Let $X_1,\ldots,X_n$ be i.i.d. random variables with a common density function given by: $f(x\mid\theta)=\theta x^{\theta-1}$ for $x\in[0,1]$ and $\theta>0$. Clearly this is a $\operatorname{BETA}(\theta,1)$ distribution.  Calculate the maximum likelihood estimator of $\theta$. After going through all the steps with the log likelihood, I end up calculating that the maximum likelihood estimator is $\hat\theta$ below: $$L:=\prod_{i=1}^N\theta x_i^{\theta-1}$$ $$l:=\ln(L)=\ln\left(\prod_{i=1}^N\theta x_i^{\theta-1}\right)=n\ln(\theta)+\sum_{i=1}^n(\theta-1)\ln(x_i)$$ $$\frac{dl}{d\theta}=\frac{n}{\theta}+\sum_{i=1}^n\ln(x_i)$$ $$\hat \theta=\frac{-n}{\sum_{i=1}^n\ln(x_i)}$$ But something about this doesn't look quite right to me.  Did I go wrong somewhere?",,"['statistics', 'probability-distributions', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
73,"100 people with 100 dollars each, give 1 dollar to a random other person. What's the distribution?","100 people with 100 dollars each, give 1 dollar to a random other person. What's the distribution?",,"Source: http://www.decisionsciencenews.com/2017/06/19/counterintuitive-problem-everyone-room-keeps-giving-dollars-random-others-youll-never-guess-happens-next/ ""Imagine a room full of 100 people with 100 dollars each. With every tick of the clock, every person with money gives a dollar to one randomly chosen other person. After some time progresses, how will the money be distributed?"" I've seen various blog posts on this that use code to simulate the distribution. My question is: what is the reason that the distribution becomes skewed rather than stay uniform, what math do I need to understand this problem? I initially tried reasoning with expected value, that each person has at each moment an expected value of $1. So I thought it would be uniform. Clearly this can change as soon as one person goes broke, then the expected value drops. Thanks","Source: http://www.decisionsciencenews.com/2017/06/19/counterintuitive-problem-everyone-room-keeps-giving-dollars-random-others-youll-never-guess-happens-next/ ""Imagine a room full of 100 people with 100 dollars each. With every tick of the clock, every person with money gives a dollar to one randomly chosen other person. After some time progresses, how will the money be distributed?"" I've seen various blog posts on this that use code to simulate the distribution. My question is: what is the reason that the distribution becomes skewed rather than stay uniform, what math do I need to understand this problem? I initially tried reasoning with expected value, that each person has at each moment an expected value of $1. So I thought it would be uniform. Clearly this can change as soon as one person goes broke, then the expected value drops. Thanks",,['statistics']
74,Density of sum of two uniform random variables,Density of sum of two uniform random variables,,"I have two uniform random varibles. $X$ is uniform over $[\frac{1}{2},1]$ and $Y$ is uniform over $[0,1]$. I want to find the density funciton for $Z=X+Y$. There are many solutions to this on this site where the two variables have the same range ( density of sum of two uniform random variables $[0,1]$ ) but I don't understand how to translate them to the case where one variable have a different range. Here's what I tried: $$f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\,dx.$$ We will have $f_Z(z)=0$ for $z\lt \frac{1}{2}$, and also for $z\ge 2$. Two cases (i) $\frac{1}{2}\lt z\le 1$ and (ii) $1\lt z\lt 2$. (i) In order to have $f_Y(z-x)=1$, we need $z-x\ge 0$, that is, $x\le z$. So for (i), we will be integrating from $x=\frac{1}{2}$ to $x=z$. $$f_Z(z)=\int_\frac{1}{2}^z 2 * 1\,dx=2z-1.$$ for $\frac{1}{2}\lt z\le 1$. (ii) Suppose that $1\lt z\lt 2$. In order to have $f_Y(z-x)$ to be $1$, we need $z-x\le 1$, that is, we need $x\ge z-1$. $$f_Z(z)=\int_{z-1}^1 2*1\,dx=4-2z.$$ for $1\lt z\lt 2$. For $z=1$ these two cases give different results, which doesn't make sense to me. Is there something wrong? How do I know which integration limits to use?","I have two uniform random varibles. $X$ is uniform over $[\frac{1}{2},1]$ and $Y$ is uniform over $[0,1]$. I want to find the density funciton for $Z=X+Y$. There are many solutions to this on this site where the two variables have the same range ( density of sum of two uniform random variables $[0,1]$ ) but I don't understand how to translate them to the case where one variable have a different range. Here's what I tried: $$f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\,dx.$$ We will have $f_Z(z)=0$ for $z\lt \frac{1}{2}$, and also for $z\ge 2$. Two cases (i) $\frac{1}{2}\lt z\le 1$ and (ii) $1\lt z\lt 2$. (i) In order to have $f_Y(z-x)=1$, we need $z-x\ge 0$, that is, $x\le z$. So for (i), we will be integrating from $x=\frac{1}{2}$ to $x=z$. $$f_Z(z)=\int_\frac{1}{2}^z 2 * 1\,dx=2z-1.$$ for $\frac{1}{2}\lt z\le 1$. (ii) Suppose that $1\lt z\lt 2$. In order to have $f_Y(z-x)$ to be $1$, we need $z-x\le 1$, that is, we need $x\ge z-1$. $$f_Z(z)=\int_{z-1}^1 2*1\,dx=4-2z.$$ for $1\lt z\lt 2$. For $z=1$ these two cases give different results, which doesn't make sense to me. Is there something wrong? How do I know which integration limits to use?",,"['statistics', 'convolution', 'uniform-distribution']"
75,How do I sum two Poisson processes?,How do I sum two Poisson processes?,,"If we have a Poisson Process $Y$ with intensity $\lambda$ and a Poisson Process $X$ with intensity $\mu$, where $X$ and $Y$ are two independent Poisson processes. How can I find the process $Z=Y+X$? I know it should involve convolutions in some way but I dont know how to construct it!","If we have a Poisson Process $Y$ with intensity $\lambda$ and a Poisson Process $X$ with intensity $\mu$, where $X$ and $Y$ are two independent Poisson processes. How can I find the process $Z=Y+X$? I know it should involve convolutions in some way but I dont know how to construct it!",,"['statistics', 'probability-distributions']"
76,Median of the F-distribution,Median of the F-distribution,,"Is the median of the F-distribution with m and n degrees of freedom decreasing in n, for any m? From experiments it looks like it might be, but I have been unable to prove it.","Is the median of the F-distribution with m and n degrees of freedom decreasing in n, for any m? From experiments it looks like it might be, but I have been unable to prove it.",,"['statistics', 'probability-distributions', 'special-functions', 'median']"
77,"Do primes, expressed in binary, have more ""random"" bits on average than natural numbers?","Do primes, expressed in binary, have more ""random"" bits on average than natural numbers?",,"In my programming projects I sometimes pick large primes when I want somewhat ""random"" bits, e.g. for hashing or trivial obfuscation via XOR or modular mutiplication. My intuitive sense is that primes in binary are a little more ""random"", but I'm not sure if reality bears that out. So a couple of questions I was curious about: After stripping the leading and trailing $1$'s, is the probability of a $1$ in the remaining binary digits of a prime much different than the $0.5$ that one would assume for $\Bbb N$ at large? Is the ""diffusion"" or distribution of bits after the leading $1$ any more ""random"" in the primes than in $\Bbb N$? I'm not sure the statistical metric to ask about here, so apologies if the question is vague; intuitively I'm wondering if ""randomly distributed"" bits like $10010111$, as opposed to $11110000$, are more common in the primes. If it helps, the question could be confined to common integer representations on computers, e.g. unsigned 64-bit: $0 \le x \lt 2^{64}$.","In my programming projects I sometimes pick large primes when I want somewhat ""random"" bits, e.g. for hashing or trivial obfuscation via XOR or modular mutiplication. My intuitive sense is that primes in binary are a little more ""random"", but I'm not sure if reality bears that out. So a couple of questions I was curious about: After stripping the leading and trailing $1$'s, is the probability of a $1$ in the remaining binary digits of a prime much different than the $0.5$ that one would assume for $\Bbb N$ at large? Is the ""diffusion"" or distribution of bits after the leading $1$ any more ""random"" in the primes than in $\Bbb N$? I'm not sure the statistical metric to ask about here, so apologies if the question is vague; intuitively I'm wondering if ""randomly distributed"" bits like $10010111$, as opposed to $11110000$, are more common in the primes. If it helps, the question could be confined to common integer representations on computers, e.g. unsigned 64-bit: $0 \le x \lt 2^{64}$.",,"['statistics', 'prime-numbers', 'integers', 'binary']"
78,Bartlett's paradox in Bayesian evidence,Bartlett's paradox in Bayesian evidence,,"I've come across Bartlett's ""paradox"" (not to be confused with Lindley's paradox , also known as the Lindley-Bartlett paradox) in Bayesian statistics. The paradox originates from Bartlett's 1957 paper, A Comment on D. V. Lindley's Statistical Paradox . Bartlett's paradox is quite trivial. Suppose that with a Bayes-factor, one compares two models, $a$ and $b$, each with adjustable parameters. Model $b$ has an adjustable parameter $p$ that a priori could take on broad range of values $N$, for which a uniform distribution is an appropriate prior (suppose that it's a location parameter). The Bayes-factor is $$ \frac{p(D\mid M_a)}{p(D\mid M_b)} \propto N $$ If a priori we know very little about the parameter $p$, it might be that we want to consider an improper prior $N\to\infty$ or at least very large $N$. In this case, the we ought to favor model $a$, almost regardless of the data (though here we must take care with the limit - if each $p(D\mid M)$ is normalized, $p(D\mid M_b) \ge p(D\mid M_a)$ for some $D$). What are we to make of this? Is it paradox? How should we deal with this situation in which we have insufficient prior information to constrain a model parameter but want to make a model comparison? Maybe nothing has gone wrong, and we really should favour model $a$ in this situation?","I've come across Bartlett's ""paradox"" (not to be confused with Lindley's paradox , also known as the Lindley-Bartlett paradox) in Bayesian statistics. The paradox originates from Bartlett's 1957 paper, A Comment on D. V. Lindley's Statistical Paradox . Bartlett's paradox is quite trivial. Suppose that with a Bayes-factor, one compares two models, $a$ and $b$, each with adjustable parameters. Model $b$ has an adjustable parameter $p$ that a priori could take on broad range of values $N$, for which a uniform distribution is an appropriate prior (suppose that it's a location parameter). The Bayes-factor is $$ \frac{p(D\mid M_a)}{p(D\mid M_b)} \propto N $$ If a priori we know very little about the parameter $p$, it might be that we want to consider an improper prior $N\to\infty$ or at least very large $N$. In this case, the we ought to favor model $a$, almost regardless of the data (though here we must take care with the limit - if each $p(D\mid M)$ is normalized, $p(D\mid M_b) \ge p(D\mid M_a)$ for some $D$). What are we to make of this? Is it paradox? How should we deal with this situation in which we have insufficient prior information to constrain a model parameter but want to make a model comparison? Maybe nothing has gone wrong, and we really should favour model $a$ in this situation?",,"['statistics', 'bayesian', 'paradoxes']"
79,Heavy-tailed distributions,Heavy-tailed distributions,,"I have encountered the following two definitions of heavy-tailedness (right tail) for a $[0,\infty)$-valued random variable $X$ satisfying $\mathbb{E}[X]<\infty$: (i) $\limsup_{x\to\infty}\frac{\mathbb{P}(X>x)}{e^{-\lambda x}}>0$ for all $\lambda>0$, (ii) $\mathbb{E}[X-u|X>u]\to\infty$ as $u\to\infty$. Are these two notions equivalent? If yes, how to prove equivalence? Thank you in advance!","I have encountered the following two definitions of heavy-tailedness (right tail) for a $[0,\infty)$-valued random variable $X$ satisfying $\mathbb{E}[X]<\infty$: (i) $\limsup_{x\to\infty}\frac{\mathbb{P}(X>x)}{e^{-\lambda x}}>0$ for all $\lambda>0$, (ii) $\mathbb{E}[X-u|X>u]\to\infty$ as $u\to\infty$. Are these two notions equivalent? If yes, how to prove equivalence? Thank you in advance!",,"['statistics', 'probability-distributions', 'finance', 'risk-assessment']"
80,Frequency of Math Symbols,Frequency of Math Symbols,,"Does anyone know of a study that has calculated the frequency of math symbols based on some popular mathematics journals or math corpus? For example in English you have letter frequencies of the most common english documents. Obviously the list is much larger in Mathematics, but it would be interesting to see what characters are used the most in mathematics.  (This question might be better fit for meta.math.stackexchange.com, stackoverflow.com or even mathematica.stackexchange.com) Question: Does anyone know of a frequency table that plots out the frequency of math symbols based on mathematic journals or a math corpus?","Does anyone know of a study that has calculated the frequency of math symbols based on some popular mathematics journals or math corpus? For example in English you have letter frequencies of the most common english documents. Obviously the list is much larger in Mathematics, but it would be interesting to see what characters are used the most in mathematics.  (This question might be better fit for meta.math.stackexchange.com, stackoverflow.com or even mathematica.stackexchange.com) Question: Does anyone know of a frequency table that plots out the frequency of math symbols based on mathematic journals or a math corpus?",,"['statistics', 'notation']"
81,Variance of a max function,Variance of a max function,,"Say $x_1$ and $x_2$ are normal random variables with known means and standard deviations and $C$ is a constant. If $y = \max(x_1,x_2,C)$, what is $\mathrm{Var}(y)$? Well, I forgot to tell that $x_1$ and $x_2$ are independent.","Say $x_1$ and $x_2$ are normal random variables with known means and standard deviations and $C$ is a constant. If $y = \max(x_1,x_2,C)$, what is $\mathrm{Var}(y)$? Well, I forgot to tell that $x_1$ and $x_2$ are independent.",,"['statistics', 'standard-deviation']"
82,Justify an unbiased estimator is UMVUE,Justify an unbiased estimator is UMVUE,,"Suppose $X_1,\ldots,X_n$ are iid $N(\theta,\theta)$, with $\theta\in(0,\infty)$. Is $\bar{X}$ the UMVUE (beta unbiased estimator) of $\theta$? I find the complete sufficient statistic is $T=\sum_{i=1}^{n}X_i^2$. So $\bar{X}$ is not a function $T$. Then we cannot justify it is UMVUE or not. Can someone help me here? How to get complete sufficient statistis? $\frac{f(x\mid\theta)}{f(y\mid\theta)}=\exp(\frac{1}{2\theta}\sum_{i=1}^n (y_i^2-x_i^2)+\sum_{i=1}^n (x_i-y_i))$. Let $\sum_{i=1}^n y_i^2=\sum_{i=1}^n x_i^2$. My work I got $\log L(x\mid\theta) = -\frac{n}{2}\frac{1}{\theta} + \frac{\sum_{i=1}^n x_i^2}{2} \frac{1}{\theta^2}-\frac{n}{2}$. Then, I let $\frac{\partial \log (x\mid\theta)}{\partial \theta}=0$. Then, I have $-\frac{n}{2}\theta^2-\frac{n}{2}\theta+\frac{1}{2}\sum_{i=1}^n x_i^2=0$. Then, I find the solution is weird. Am I wrong?","Suppose $X_1,\ldots,X_n$ are iid $N(\theta,\theta)$, with $\theta\in(0,\infty)$. Is $\bar{X}$ the UMVUE (beta unbiased estimator) of $\theta$? I find the complete sufficient statistic is $T=\sum_{i=1}^{n}X_i^2$. So $\bar{X}$ is not a function $T$. Then we cannot justify it is UMVUE or not. Can someone help me here? How to get complete sufficient statistis? $\frac{f(x\mid\theta)}{f(y\mid\theta)}=\exp(\frac{1}{2\theta}\sum_{i=1}^n (y_i^2-x_i^2)+\sum_{i=1}^n (x_i-y_i))$. Let $\sum_{i=1}^n y_i^2=\sum_{i=1}^n x_i^2$. My work I got $\log L(x\mid\theta) = -\frac{n}{2}\frac{1}{\theta} + \frac{\sum_{i=1}^n x_i^2}{2} \frac{1}{\theta^2}-\frac{n}{2}$. Then, I let $\frac{\partial \log (x\mid\theta)}{\partial \theta}=0$. Then, I have $-\frac{n}{2}\theta^2-\frac{n}{2}\theta+\frac{1}{2}\sum_{i=1}^n x_i^2=0$. Then, I find the solution is weird. Am I wrong?",,"['statistics', 'normal-distribution', 'statistical-inference']"
83,How to estimate variances for Kalman filter from real sensor measurements without underestimating process noise.,How to estimate variances for Kalman filter from real sensor measurements without underestimating process noise.,,"As the title says, I want to estimate the variances needed for a Kalman filter from real sensor measurements only. For example we can take a temperature sensor, but the solution shall be as generalized as possible. Assume, we cannot accurately influence the sensor input (i.e. we cannot too accurately generate a fixed temperature) or do not want to do this. I need to estimate two variances for the Kalman filter: The measurement noise variance, $\sigma_m$ , which is the inaccuracy introduced by the sensor itself (e.g. fluctuations in power supply or limited measurement resolution). It models the difference between sensor's measurement value and real temperature that is to be measured. The process noise variance, $\sigma_p$ , which estimates the error in our system model. It is dependent on the model, i.e. how exactly it estimates future values from the current state of the Kalman filter. For the temperature sensor, we could model the temperature to be static and we predict the next temperature to be the same as our latest temperature estimation. However, as the real temperature slowly changes over time the filter's prediction is incorrect. Basically, the process noise means: If we have an estimation $e_t\sim N(\mu_e,\sigma_e^2)$ of the real value at time $t$ and we apply our prediction model $f(x)$ which introduces a Gaussian distributed process noise with a variance $\sigma_p^2$, the new estimation will be $e_{t+1}\sim N( f(\mu_e), \sigma_e^2+\sigma_p^2))$. How you can help... I already did some work on this and came to some practical solution. However, there is still an issue that could lead to underestimation of the process noise. But, it would be advisable to only use overestimated values for the Kalman filter. In that case one can use the variance from the filter state to give reasonable information on the accuracy of the current estimation. If you have an idea how to resolve the underestimation problem (see explanations below), please let me know. I didn't study math. So please check my math below for any stupid mistakes ;-) Here is what I came up with so far... Estimation of the measurement noise I model my sensor reading as: $m_t = x_t + e_t$ with $e_t \sim N(0,\sigma_m^2)$, where $m_t$ is the measured value, $x_t$ is the real value in absence of noise and $e_t$ is the measurement noise; whereas $t$ refers to the time. The noise is assumed to be Gaussian distributed without bias (zero mean). I assume that my signal is continuous such that: $\lim_{\delta\to0}(x_{t+\delta} - x_t) = 0$. This means that two real values that are very close in time are equal. Therefore, $\lim_{\delta\to0} m_{t+\delta}-m_t$ $= \lim_{\delta\to0} ((x_{t+\delta} + e_{t+\delta}) - (x_t + e_t))$ $= \lim_{\delta\to0} ((x_{t} + e_{t+\delta}) - (x_t + e_t))$ $= \lim_{\delta\to0} ((x_{t+\delta} + e_t) - (x_t + e_t))$ $= \lim_{\delta\to0} (e_{t+\delta} - e_t)$ $= e_{t+\delta} - e_t$. Thus, $\lim_{\delta\to0} Var(m_{t+\delta}-m_t)$ $= Var(e_{t+\delta}-e_t)$ $= Var(e_{t+\delta}) + Var(e_t)$. We know that $e_t\sim N(0,\sigma_m^2)$ and $e_{t+\delta}\sim N(0,\sigma_m^2)$ therefore $Var(e_{t+\delta}) + Var(e_t) = \sigma_m^2 + \sigma_m^2 = 2\sigma_m^2$. Resulting in $\hat{\sigma_m^2} = \frac{1}{2}\lim_{\delta\to0} Var(m_{t+\delta}-m_t)$. Hence, assuming we can take measurements very quickly (i.e. a magnitude quicker than the real signal values are usually changing as much as the the measurement noise) we could estimate the measurement noise by $\hat{\sigma_m^2} = \frac{1}{2}\lim_{\delta\to0} Var(m_{t+\delta}-m_t)$. Note, as $\delta$ will never be zero in a real scenario the estimator tends to overestimate the measurement noise, which will be important in the next step. Estimation of the process noise While the estimation of the measurement seems to be straightforward, I've some trouble with the process noise. As already mentioned, it is dependent on the estimation model. The approach that I came up with is to use the following formula that I used to describe what the process noise is: $e_{t+1}\sim N( f(\mu_e), \sigma_e^2+\sigma_p^2))$ We just take one measurement $\mu_m = x_t$ and $\sigma_m^2$ as the estimations for time step $t$, whereas we use the measurement noise $\sigma_m^2$ estimated as described above. Then we should get an estimation like this: $e_{t+1}\sim N( f(x_t), \sigma_m^2+\sigma_p^2))$ We could then take a measurement $x_{t+1}$ at time $t+1$ which should fulfill this equations: $x_{t+1} = f(x_t) + e_{t+1} \iff$ $e_{t+1} = x_{t+1} - f(x_t) \iff$ $Var(x_{t+1} - f(x_t)) = \sigma_m^2 + sigma_p^2 \iff$ $\hat{\sigma_p^2} = Var(x_{t+1} - f(x_t)) - \sigma_m^2$ But, if we use the overestimating estimator $\hat{\sigma_m^2}$ from the last step for $\sigma_m^2$ we will have an underestimating estimator for $\sigma_p^2$: $\hat{\sigma_p^2} = Var(x_{t+1} - f(x_t)) - \hat{\sigma_m^2}$ The Problem: underestimated process noise Meaning, whenever the estimation for the measurement noise $\hat{\sigma_m^2}$ is too high, the estimation $\hat{\sigma_p^2}$ for the process noise will be too low. In other words: While the estimated mean value of the filter might be very accurate, the estimation of its accuracy will be too optimistic. So if the result would say ""I know the temperature is 23.122... °C with a variance of 0.03232 K²"" you could not rely on this, as the variance given would probably be too low (and in fact should be higher than this). Is there any way to easily resolve this issue? How? PS: Let me know if you noticed any incorrect formulas and/or assumptions.","As the title says, I want to estimate the variances needed for a Kalman filter from real sensor measurements only. For example we can take a temperature sensor, but the solution shall be as generalized as possible. Assume, we cannot accurately influence the sensor input (i.e. we cannot too accurately generate a fixed temperature) or do not want to do this. I need to estimate two variances for the Kalman filter: The measurement noise variance, $\sigma_m$ , which is the inaccuracy introduced by the sensor itself (e.g. fluctuations in power supply or limited measurement resolution). It models the difference between sensor's measurement value and real temperature that is to be measured. The process noise variance, $\sigma_p$ , which estimates the error in our system model. It is dependent on the model, i.e. how exactly it estimates future values from the current state of the Kalman filter. For the temperature sensor, we could model the temperature to be static and we predict the next temperature to be the same as our latest temperature estimation. However, as the real temperature slowly changes over time the filter's prediction is incorrect. Basically, the process noise means: If we have an estimation $e_t\sim N(\mu_e,\sigma_e^2)$ of the real value at time $t$ and we apply our prediction model $f(x)$ which introduces a Gaussian distributed process noise with a variance $\sigma_p^2$, the new estimation will be $e_{t+1}\sim N( f(\mu_e), \sigma_e^2+\sigma_p^2))$. How you can help... I already did some work on this and came to some practical solution. However, there is still an issue that could lead to underestimation of the process noise. But, it would be advisable to only use overestimated values for the Kalman filter. In that case one can use the variance from the filter state to give reasonable information on the accuracy of the current estimation. If you have an idea how to resolve the underestimation problem (see explanations below), please let me know. I didn't study math. So please check my math below for any stupid mistakes ;-) Here is what I came up with so far... Estimation of the measurement noise I model my sensor reading as: $m_t = x_t + e_t$ with $e_t \sim N(0,\sigma_m^2)$, where $m_t$ is the measured value, $x_t$ is the real value in absence of noise and $e_t$ is the measurement noise; whereas $t$ refers to the time. The noise is assumed to be Gaussian distributed without bias (zero mean). I assume that my signal is continuous such that: $\lim_{\delta\to0}(x_{t+\delta} - x_t) = 0$. This means that two real values that are very close in time are equal. Therefore, $\lim_{\delta\to0} m_{t+\delta}-m_t$ $= \lim_{\delta\to0} ((x_{t+\delta} + e_{t+\delta}) - (x_t + e_t))$ $= \lim_{\delta\to0} ((x_{t} + e_{t+\delta}) - (x_t + e_t))$ $= \lim_{\delta\to0} ((x_{t+\delta} + e_t) - (x_t + e_t))$ $= \lim_{\delta\to0} (e_{t+\delta} - e_t)$ $= e_{t+\delta} - e_t$. Thus, $\lim_{\delta\to0} Var(m_{t+\delta}-m_t)$ $= Var(e_{t+\delta}-e_t)$ $= Var(e_{t+\delta}) + Var(e_t)$. We know that $e_t\sim N(0,\sigma_m^2)$ and $e_{t+\delta}\sim N(0,\sigma_m^2)$ therefore $Var(e_{t+\delta}) + Var(e_t) = \sigma_m^2 + \sigma_m^2 = 2\sigma_m^2$. Resulting in $\hat{\sigma_m^2} = \frac{1}{2}\lim_{\delta\to0} Var(m_{t+\delta}-m_t)$. Hence, assuming we can take measurements very quickly (i.e. a magnitude quicker than the real signal values are usually changing as much as the the measurement noise) we could estimate the measurement noise by $\hat{\sigma_m^2} = \frac{1}{2}\lim_{\delta\to0} Var(m_{t+\delta}-m_t)$. Note, as $\delta$ will never be zero in a real scenario the estimator tends to overestimate the measurement noise, which will be important in the next step. Estimation of the process noise While the estimation of the measurement seems to be straightforward, I've some trouble with the process noise. As already mentioned, it is dependent on the estimation model. The approach that I came up with is to use the following formula that I used to describe what the process noise is: $e_{t+1}\sim N( f(\mu_e), \sigma_e^2+\sigma_p^2))$ We just take one measurement $\mu_m = x_t$ and $\sigma_m^2$ as the estimations for time step $t$, whereas we use the measurement noise $\sigma_m^2$ estimated as described above. Then we should get an estimation like this: $e_{t+1}\sim N( f(x_t), \sigma_m^2+\sigma_p^2))$ We could then take a measurement $x_{t+1}$ at time $t+1$ which should fulfill this equations: $x_{t+1} = f(x_t) + e_{t+1} \iff$ $e_{t+1} = x_{t+1} - f(x_t) \iff$ $Var(x_{t+1} - f(x_t)) = \sigma_m^2 + sigma_p^2 \iff$ $\hat{\sigma_p^2} = Var(x_{t+1} - f(x_t)) - \sigma_m^2$ But, if we use the overestimating estimator $\hat{\sigma_m^2}$ from the last step for $\sigma_m^2$ we will have an underestimating estimator for $\sigma_p^2$: $\hat{\sigma_p^2} = Var(x_{t+1} - f(x_t)) - \hat{\sigma_m^2}$ The Problem: underestimated process noise Meaning, whenever the estimation for the measurement noise $\hat{\sigma_m^2}$ is too high, the estimation $\hat{\sigma_p^2}$ for the process noise will be too low. In other words: While the estimated mean value of the filter might be very accurate, the estimation of its accuracy will be too optimistic. So if the result would say ""I know the temperature is 23.122... °C with a variance of 0.03232 K²"" you could not rely on this, as the variance given would probably be too low (and in fact should be higher than this). Is there any way to easily resolve this issue? How? PS: Let me know if you noticed any incorrect formulas and/or assumptions.",,"['statistics', 'parameter-estimation', 'error-propagation', 'bayesian-network', 'kalman-filter']"
84,Is there a statistical hypothesis test that uses the mode?,Is there a statistical hypothesis test that uses the mode?,,Is there a statistical hypothesis test that considers the mode rather than the mean or median?,Is there a statistical hypothesis test that considers the mode rather than the mean or median?,,"['statistics', 'statistical-inference']"
85,"Find a one-dimensional sufficient statistic for $\theta$ given that $f(x;\theta)=\frac{1}{\theta^2}x e^{-\frac{x}{\theta}} I_{(0,\infty)}(x)$",Find a one-dimensional sufficient statistic for  given that,"\theta f(x;\theta)=\frac{1}{\theta^2}x e^{-\frac{x}{\theta}} I_{(0,\infty)}(x)","Assume that $(X_1, X_2, X_3, \dots, X_n)$ is a random sample of the distribution having the following probability distribution function (PDF): $$f(x;\theta)=\frac{1}{\theta^2}x e^{-\frac{x}{\theta}} I_{(0,\infty)}(x), \quad \theta >0$$ where $I_{(0,\infty)}(x)$ is an indicator function on the set $(0,\infty)$ (meaning that if $x$ belongs to that set, the output of the function is $1$ , otherwise it's $0$ ). Question : Find a one-dimensional sufficient statistic for $\theta$ . I'm stuck at many things: (1) What is a ""one-dimensional"" sufficient statistic? I know that a sufficient statistic somehow summarizes the data such that if we only see that statistic, we will do the same as if the real data was shown to us. (2) Assuming that we know what a one-dimensional sufficient statistic for $\theta$ is, how should I proceed further? I mean, is there a systematic way to find a sufficient statistic? or we just guess it and try to prove that it is actually a sufficient statistic? Note: I also saw this post which seems relevant. However, I am not sure how to use the factorization theorem (if it can be useful in my case!). What are the factors here? And how should I get rid of the indicator function?","Assume that is a random sample of the distribution having the following probability distribution function (PDF): where is an indicator function on the set (meaning that if belongs to that set, the output of the function is , otherwise it's ). Question : Find a one-dimensional sufficient statistic for . I'm stuck at many things: (1) What is a ""one-dimensional"" sufficient statistic? I know that a sufficient statistic somehow summarizes the data such that if we only see that statistic, we will do the same as if the real data was shown to us. (2) Assuming that we know what a one-dimensional sufficient statistic for is, how should I proceed further? I mean, is there a systematic way to find a sufficient statistic? or we just guess it and try to prove that it is actually a sufficient statistic? Note: I also saw this post which seems relevant. However, I am not sure how to use the factorization theorem (if it can be useful in my case!). What are the factors here? And how should I get rid of the indicator function?","(X_1, X_2, X_3, \dots, X_n) f(x;\theta)=\frac{1}{\theta^2}x e^{-\frac{x}{\theta}} I_{(0,\infty)}(x), \quad \theta >0 I_{(0,\infty)}(x) (0,\infty) x 1 0 \theta \theta","['statistics', 'probability-distributions']"
86,Expected gap between two consecutive order statistics,Expected gap between two consecutive order statistics,,"Consider a random variable $X\sim_X(x)$ with $\operatorname{supp}{p_X}=[a,b]\subset{\mathbb R}$, and let $X_1, X_2, \ldots, X_n$ be $n$ $i.i.d.$ samples from $p_X(x)$, then we have their associated order statistics $X_{(1:n)}, X_{(2:n)}, \ldots, X_{(n:n)}$ such that $X_{(1:n)}\leqslant X_{(2:n)}\leqslant \cdots\leqslant  X_{(n:n)}$. Define the spacing between any two consecutive order statistics as $W_{(r:n)}=X_{(r+1:n)}-X_{(r:n)} (1\leqslant r\leqslant n-1)$, then in ""Note on Francis Galton's problem"" by K. Pearson (1902), it has been shown that  $$ {\mathbb E}\left[W_{(r:n)}\right]=\frac{n!}{(n-r)!r!}\int_{-\infty}^{\infty}F(x)^{n-r}[1-F(x)]^r \, dx, \hspace{2mm} 1\leqslant r\leqslant n-1, $$ where $F(x)=\int_{-\infty}^xp_X(y) \, dy$ is the distribution function of $X$. So, $\forall r, n$ such that $1\leqslant r\leqslant n-1$, $2\leqslant n$, do we have $$ {\mathbb E}\left[W_{(r:n)}\right] \geqslant {\mathbb E}\left[W_{(r:n+1)}\right], $$  i.e., the expected gap between any two consecutive order statistics will decrease or remain the same as $n$ increases? Intuitively, this is correct since the more samples one draws, the denser the order statistics will be filled in the support. I also checked several specific distributions (e.g., uniform, Gaussian, etc), and the statement holds, but I have not figured out a good way to prove this for the general case. Hope someone can provide some inspiration.","Consider a random variable $X\sim_X(x)$ with $\operatorname{supp}{p_X}=[a,b]\subset{\mathbb R}$, and let $X_1, X_2, \ldots, X_n$ be $n$ $i.i.d.$ samples from $p_X(x)$, then we have their associated order statistics $X_{(1:n)}, X_{(2:n)}, \ldots, X_{(n:n)}$ such that $X_{(1:n)}\leqslant X_{(2:n)}\leqslant \cdots\leqslant  X_{(n:n)}$. Define the spacing between any two consecutive order statistics as $W_{(r:n)}=X_{(r+1:n)}-X_{(r:n)} (1\leqslant r\leqslant n-1)$, then in ""Note on Francis Galton's problem"" by K. Pearson (1902), it has been shown that  $$ {\mathbb E}\left[W_{(r:n)}\right]=\frac{n!}{(n-r)!r!}\int_{-\infty}^{\infty}F(x)^{n-r}[1-F(x)]^r \, dx, \hspace{2mm} 1\leqslant r\leqslant n-1, $$ where $F(x)=\int_{-\infty}^xp_X(y) \, dy$ is the distribution function of $X$. So, $\forall r, n$ such that $1\leqslant r\leqslant n-1$, $2\leqslant n$, do we have $$ {\mathbb E}\left[W_{(r:n)}\right] \geqslant {\mathbb E}\left[W_{(r:n+1)}\right], $$  i.e., the expected gap between any two consecutive order statistics will decrease or remain the same as $n$ increases? Intuitively, this is correct since the more samples one draws, the denser the order statistics will be filled in the support. I also checked several specific distributions (e.g., uniform, Gaussian, etc), and the statement holds, but I have not figured out a good way to prove this for the general case. Hope someone can provide some inspiration.",,"['statistics', 'order-statistics']"
87,What do angle brackets ($\langle\rangle$ ) mean in mathematics/statistics (autocorrelations)?,What do angle brackets ( ) mean in mathematics/statistics (autocorrelations)?,\langle\rangle,"Okay, so the logarithmic return on a stock is given by: $$r_τ (t) = \ln P(t+τ) - \ln P(t),$$ where τ is the interval of time. I have no problem calculating that. My question comes to the following formula: $$ρ(T) \sim 〈r_τ (t+T) \cdot r_τ (t)〉$$ This is supposedly the autocorrelation function of log-returns. What's the deal with the brackets?","Okay, so the logarithmic return on a stock is given by: $$r_τ (t) = \ln P(t+τ) - \ln P(t),$$ where τ is the interval of time. I have no problem calculating that. My question comes to the following formula: $$ρ(T) \sim 〈r_τ (t+T) \cdot r_τ (t)〉$$ This is supposedly the autocorrelation function of log-returns. What's the deal with the brackets?",,"['statistics', 'notation', 'correlation']"
88,Only three types of limit of distributions truncated to a finite interval in the upper tail?,Only three types of limit of distributions truncated to a finite interval in the upper tail?,,"Suppose random variable $X$ has a continuous probability distribution with an unbounded upper tail; that is, the CDF of $X$ (call it $F$) is absolutely continuous and $F(x)<1$ for all $x\in\mathbb{R}.$ Now consider the conditional CDF of ${X-a\over w}$ given that $a\le X\le a+w$, for $w>0$:$$G(y,a,w):=\mathbb{P}\left({X-a\over w}\le y\ {\LARGE \mid}\ a\le X\le a+w\right) ={F(a+w\,y)-F(a)\over F(a+w)-F(a)}\,1_{0\le y\le 1}+1_{y>1}.$$ WolframCloud computations lead to the following ... Observation : Apparently, for all well-known parametric families of such distributions (e.g., Normal, Lognormal, Student's t, Cauchy, Maxwell, Gamma, Gumbel, Weibull, etc.), there are only three types of limit of the CDF of ${X-a\over w}\mid a\le X\le a+w$ as $a\to\infty$ :   $$\lim_{a\to\infty}G(y,a,w)=\lim_{a\to\infty}\mathbb{P}\left({X-a\over w}\le y\ {\LARGE \mid}\ a\le X\le a+w\right) \\[3ex] = \begin{cases}  1_{y>0} & \text{(Type 1: Degenerate at $0$)}\\[2ex] y\,1_{0\le y\le 1}+1_{y>1} & \text{(Type 2: Uniform on $[0,1]$)}\\[2ex] {1- e^{-y\,w/\beta}\over 1-e^{-w/\beta}}1_{0\le y\le 1}+1_{y>1} & \text{(Type 3: Exponential$(\beta/w)$ on $[0,1]$)} \end{cases}$$    where $\beta$ is a parameter depending on the $X$ distribution. NB : The Type 3 limit depends on both $w$ and the $X$ distribution, but the Type 1 and Type 2 limits are free of w and free of all parameters of the $X$ distribution , which seems quite remarkable. NB : Letting $Y_a={X-a\over w}$, for all three types (and any other type, if there are any) we have $$\begin{align}y\le 0&\implies \{Y_a\le y\}\cap\{0\le Y_a\le 1\}=\{Y_a=0\}\implies \lim_{a\to\infty}G(y,a,w)=0\\ y\ge 1&\implies \{Y_a\le y\}\cap\{0\le Y_a\le 1\}=\{0\le Y_a\le 1\}\implies \lim_{a\to\infty}G(y,a,w)=1\end{align}$$ since $\mathbb{P}(Y_a=0)=0$ due to the distribution of $X$ (hence $Y_a$) being continuous;  thus, the types of limit differ only for $y$ in the open interval $(0,1)$ . Here are pictures of these three types of limit: Questions : How generally does this observation hold, and how can it be proved analytically? (Is this something well-known?) What $X$ distributions (if any) do in fact yield a limit not among these three types? In the Degenerate case, the limit is not right-continuous at $y=0$, so it fails to be a CDF. Nevertheless, as $a\to\infty$ the probability mass clearly concentrates in an arbitrarily small neighborhood of $y=0$ (as indicated by the plots below); so, is there a valid interpretation of this as a limiting distribution? (My original motivating question concerned only the asymptotic variance for just Normal vs. Lognormal. These variances of course follow from the above limiting distributions.) Examples Degenerate Limit Here are the CDFs (left) and corresponding PDFs (right) for Normal($0,1$), showing the CDFs, PDFs labeled as $G(y,a,w),\ g(y,a,w)$ for various $a$ with $w=1$: Computations show that the limit of the distribution is degenerate at $0$ for apparently any Normal, Gumbel, Rayleigh or Maxwell distribution, or any Weibull distribution with shape parameter $> 1.$ Uniform Limit Here are the CDFs (left) and corresponding PDFs (right) for LogNormal(0,1), showing the CDFs, PDFs labeled as $G(y,a,w),\ g(y,a,w)$ for various $a$ with $w=1$: Computations show that the limit distribution is Uniform on $[0,1]$ for apparently any LogNormal, Student-t, Cauchy, Levi, Pareto, or Inverse Gamma distribution, or any Weibull distribution with shape parameter $< 1.$ Exponential Limit Here are the CDFs (left) and corresponding PDFs (right) for Gamma($\alpha=2,\beta=3$), showing the CDFs, PDFs labeled as $G(y,a,w),\ g(y,a,w)$ for various $a$ with $w=1$: Computations show that the limit distribution is Exponential on $[0,1]$ for apparently any Gamma distribution (which includes the Weibull distribution with shape parameter $= 1$), or any Logistic, Laplace, or Extreme Value distribution. For reference, here's the Wolfram code I used: (* Find limit(a->infinity) Pr[(X-a)/w <= y | a<=X<=a+w] for 0<y<1 *) FindLimit[distr_,asms_]:=Module[{assms=asms~Join~{0<y<1,a>0,w>0}},  G[y_,a_,w_]:=Probability[(X-a)/w<=y\[Conditioned]a<=X<=a+w,X\[Distributed]distr]; Glim[y_,w_]:=Simplify[Cancel[Limit[G[y,a,w],a->Infinity]]]; Print[""0<y<1: "", Assuming[assms,Glim[y,w]]];]   (* Examples: *)  (* In[]= *) FindLimit[ NormalDistribution[\[Mu],\[Sigma]],{\[Mu]\[Element]Reals ,\[Sigma]>0} ]  (* Out[]= *) 0<y<1: 1  (* In[]= *) FindLimit[ LogNormalDistribution[\[Mu],\[Sigma]],{\[Mu]\[Element]Reals ,\[Sigma]>0} ]  (* Out[]= *) 0<y<1: y  (* In[]= *) FindLimit[ GammaDistribution[\[Alpha],\[Beta]],{\[Alpha]>0,\[Beta]>0} ] (* Out[]= *) 0<y<1: (E^(w/\[Beta]) (1-E^(-((w y)/\[Beta]))))/(-1+E^(w/\[Beta]))","Suppose random variable $X$ has a continuous probability distribution with an unbounded upper tail; that is, the CDF of $X$ (call it $F$) is absolutely continuous and $F(x)<1$ for all $x\in\mathbb{R}.$ Now consider the conditional CDF of ${X-a\over w}$ given that $a\le X\le a+w$, for $w>0$:$$G(y,a,w):=\mathbb{P}\left({X-a\over w}\le y\ {\LARGE \mid}\ a\le X\le a+w\right) ={F(a+w\,y)-F(a)\over F(a+w)-F(a)}\,1_{0\le y\le 1}+1_{y>1}.$$ WolframCloud computations lead to the following ... Observation : Apparently, for all well-known parametric families of such distributions (e.g., Normal, Lognormal, Student's t, Cauchy, Maxwell, Gamma, Gumbel, Weibull, etc.), there are only three types of limit of the CDF of ${X-a\over w}\mid a\le X\le a+w$ as $a\to\infty$ :   $$\lim_{a\to\infty}G(y,a,w)=\lim_{a\to\infty}\mathbb{P}\left({X-a\over w}\le y\ {\LARGE \mid}\ a\le X\le a+w\right) \\[3ex] = \begin{cases}  1_{y>0} & \text{(Type 1: Degenerate at $0$)}\\[2ex] y\,1_{0\le y\le 1}+1_{y>1} & \text{(Type 2: Uniform on $[0,1]$)}\\[2ex] {1- e^{-y\,w/\beta}\over 1-e^{-w/\beta}}1_{0\le y\le 1}+1_{y>1} & \text{(Type 3: Exponential$(\beta/w)$ on $[0,1]$)} \end{cases}$$    where $\beta$ is a parameter depending on the $X$ distribution. NB : The Type 3 limit depends on both $w$ and the $X$ distribution, but the Type 1 and Type 2 limits are free of w and free of all parameters of the $X$ distribution , which seems quite remarkable. NB : Letting $Y_a={X-a\over w}$, for all three types (and any other type, if there are any) we have $$\begin{align}y\le 0&\implies \{Y_a\le y\}\cap\{0\le Y_a\le 1\}=\{Y_a=0\}\implies \lim_{a\to\infty}G(y,a,w)=0\\ y\ge 1&\implies \{Y_a\le y\}\cap\{0\le Y_a\le 1\}=\{0\le Y_a\le 1\}\implies \lim_{a\to\infty}G(y,a,w)=1\end{align}$$ since $\mathbb{P}(Y_a=0)=0$ due to the distribution of $X$ (hence $Y_a$) being continuous;  thus, the types of limit differ only for $y$ in the open interval $(0,1)$ . Here are pictures of these three types of limit: Questions : How generally does this observation hold, and how can it be proved analytically? (Is this something well-known?) What $X$ distributions (if any) do in fact yield a limit not among these three types? In the Degenerate case, the limit is not right-continuous at $y=0$, so it fails to be a CDF. Nevertheless, as $a\to\infty$ the probability mass clearly concentrates in an arbitrarily small neighborhood of $y=0$ (as indicated by the plots below); so, is there a valid interpretation of this as a limiting distribution? (My original motivating question concerned only the asymptotic variance for just Normal vs. Lognormal. These variances of course follow from the above limiting distributions.) Examples Degenerate Limit Here are the CDFs (left) and corresponding PDFs (right) for Normal($0,1$), showing the CDFs, PDFs labeled as $G(y,a,w),\ g(y,a,w)$ for various $a$ with $w=1$: Computations show that the limit of the distribution is degenerate at $0$ for apparently any Normal, Gumbel, Rayleigh or Maxwell distribution, or any Weibull distribution with shape parameter $> 1.$ Uniform Limit Here are the CDFs (left) and corresponding PDFs (right) for LogNormal(0,1), showing the CDFs, PDFs labeled as $G(y,a,w),\ g(y,a,w)$ for various $a$ with $w=1$: Computations show that the limit distribution is Uniform on $[0,1]$ for apparently any LogNormal, Student-t, Cauchy, Levi, Pareto, or Inverse Gamma distribution, or any Weibull distribution with shape parameter $< 1.$ Exponential Limit Here are the CDFs (left) and corresponding PDFs (right) for Gamma($\alpha=2,\beta=3$), showing the CDFs, PDFs labeled as $G(y,a,w),\ g(y,a,w)$ for various $a$ with $w=1$: Computations show that the limit distribution is Exponential on $[0,1]$ for apparently any Gamma distribution (which includes the Weibull distribution with shape parameter $= 1$), or any Logistic, Laplace, or Extreme Value distribution. For reference, here's the Wolfram code I used: (* Find limit(a->infinity) Pr[(X-a)/w <= y | a<=X<=a+w] for 0<y<1 *) FindLimit[distr_,asms_]:=Module[{assms=asms~Join~{0<y<1,a>0,w>0}},  G[y_,a_,w_]:=Probability[(X-a)/w<=y\[Conditioned]a<=X<=a+w,X\[Distributed]distr]; Glim[y_,w_]:=Simplify[Cancel[Limit[G[y,a,w],a->Infinity]]]; Print[""0<y<1: "", Assuming[assms,Glim[y,w]]];]   (* Examples: *)  (* In[]= *) FindLimit[ NormalDistribution[\[Mu],\[Sigma]],{\[Mu]\[Element]Reals ,\[Sigma]>0} ]  (* Out[]= *) 0<y<1: 1  (* In[]= *) FindLimit[ LogNormalDistribution[\[Mu],\[Sigma]],{\[Mu]\[Element]Reals ,\[Sigma]>0} ]  (* Out[]= *) 0<y<1: y  (* In[]= *) FindLimit[ GammaDistribution[\[Alpha],\[Beta]],{\[Alpha]>0,\[Beta]>0} ] (* Out[]= *) 0<y<1: (E^(w/\[Beta]) (1-E^(-((w y)/\[Beta]))))/(-1+E^(w/\[Beta]))",,"['statistics', 'probability-distributions', 'asymptotics']"
89,Uniform distribution with probability density function. Find the value of $k$.,Uniform distribution with probability density function. Find the value of .,k,"For a random sample $X_1,X_2,...X_n$ from a uniform $[0,\Theta]$ distribution, with probability density function $$f(x;\Theta) = \left\{ \begin{array} \ \frac{1}{\Theta} & 0\le x \le\Theta,\\ 0 & \text{otherwise}.\end{array}\right.$$ What is the value of $k$ such that $\hat{\Theta}=k\bar{X}$ is an unbiased estimator of $\Theta$? I've done some questions similar to this but I'm not sure how to go about this one. I have a test in 3 hours so help is really appreciated!","For a random sample $X_1,X_2,...X_n$ from a uniform $[0,\Theta]$ distribution, with probability density function $$f(x;\Theta) = \left\{ \begin{array} \ \frac{1}{\Theta} & 0\le x \le\Theta,\\ 0 & \text{otherwise}.\end{array}\right.$$ What is the value of $k$ such that $\hat{\Theta}=k\bar{X}$ is an unbiased estimator of $\Theta$? I've done some questions similar to this but I'm not sure how to go about this one. I have a test in 3 hours so help is really appreciated!",,"['statistics', 'probability-distributions']"
90,Unbiased estimator of a uniform distribution,Unbiased estimator of a uniform distribution,,"For a random sample $X_1,X_2,\ldots,X_n$ from a $\operatorname{Uniform}[0,\theta]$ distribution, with probability density function $$ f(x;\theta) = \begin{cases} 1/\theta,  & 0 \le x \le \theta \\ 0, & \text{otherwise} \end{cases} $$ Let $X_{\max} = \max(X_1,X_2,\ldots,X_n).$ What is the value of k such that $\hat \theta  = kX_{\max}$ is an unbiased estimator of $\theta$ ? I'm not sure if there is more to this question, because my intuitive answer answer is just $k=1$ . This is because if you order the sample like $$x_{(1)} \le x_{(2)} \le \cdots \le x_{(n)}$$ such that $x_{(n)} = E[X_{\max}]$ .  and the fact that the distribution is uniform, the estimator of $\theta$ should just be $X_{\max}$ . Unbiased estimator -> $E\left[\widehat{\theta\,}\right] = kE[X_{\max}] = \theta$ Is my logic wrong here?","For a random sample from a distribution, with probability density function Let What is the value of k such that is an unbiased estimator of ? I'm not sure if there is more to this question, because my intuitive answer answer is just . This is because if you order the sample like such that .  and the fact that the distribution is uniform, the estimator of should just be . Unbiased estimator -> Is my logic wrong here?","X_1,X_2,\ldots,X_n \operatorname{Uniform}[0,\theta]  f(x;\theta) =
\begin{cases}
1/\theta,  & 0 \le x \le \theta \\
0, & \text{otherwise}
\end{cases}  X_{\max} = \max(X_1,X_2,\ldots,X_n). \hat \theta  = kX_{\max} \theta k=1 x_{(1)} \le x_{(2)} \le \cdots \le x_{(n)} x_{(n)} = E[X_{\max}] \theta X_{\max} E\left[\widehat{\theta\,}\right] = kE[X_{\max}] = \theta",['statistics']
91,Math vs Probability vs Statistics,Math vs Probability vs Statistics,,"For a certain job interview, I gave myself a 6 in SQL and 8 in Statistics .  I love math and probability but I always found significance testing and confidence intervals rather dry. What is the difference between math, probability and statistics again?  This is a soft question, but it should have a clear answer.","For a certain job interview, I gave myself a 6 in SQL and 8 in Statistics .  I love math and probability but I always found significance testing and confidence intervals rather dry. What is the difference between math, probability and statistics again?  This is a soft question, but it should have a clear answer.",,"['statistics', 'soft-question']"
92,What is the most fair way to scale grades?,What is the most fair way to scale grades?,,"In many universities, professors scale or ""curve"" grades at the end to ensure (among other things) that there is no grade inflation. I'm interested in studying ""fair"" ways of doing this from a mathematical standpoint. Let $S = \{X_1, X_2 \cdots X_k\}$ where $X_i \in [0,100]$ be the multiset of grades for a given class. A $\textit{scale}$ $S'$ of $S$ is some other multiset $S'=\{\phi(X_1), \phi(X_2), \cdots \phi(X_k)\}$ where $\phi:[0,100] \to [0,100]$ is some function. We say a scale is fair if $\phi$ is monotone increasing. Given two fair scales $S'$ and $S''$ with respective scale-functions $\phi, \psi$ , we say $S'$ is fairer than $S''$ if $\sum_i |\phi(X_i) - X_i| \leq \sum_i |\psi(X_i) - X_i|$ Let us suppose that the professor wants to scale the grades such that the mean grade is $70 \pm 5 \%$ . Given the above definitions, which scale function $\phi$ should he choose to ensure the scale is as fair as possible?  If there's not a simple function that always works, is there an algorithm or a strategy that might be helpful? This is, of course, but one model. There's also issues of subjectivity associated with the word ""fairness"". Perhaps there's some notion of ""fairness"" that this model doesn't quite capture. If so, please mention it.  My opinion is that the ""fairest"" way of scaling is ensuring that the scaling preserves the original order, and disturbs the original dataset as little as possible. One other possible notion (which you may consider if you are interested in, but not specifically the one I've chosen to ask about)  is considering the double sum $$\sum_{i,k} \left||\phi(X_i) - X_i| - |\phi(X_k) - X_k|\right|$$ and trying to minimize this among all possible (fair/monotone) scale functions $\phi$ . With my original model above, a scale is ""fair"" if it doesn't disturb the original dataset much. With this above model, a scale may disturb the original dataset a lot, but it still might be quite fair so long as students' grade are all altered a similar amount (for instance, a fixed scale of $20$ %). Feel free to discuss other mathematically rigorous notions of ""fair"" scaling which you believe are pertinent, or possibly cite relevant literature.","In many universities, professors scale or ""curve"" grades at the end to ensure (among other things) that there is no grade inflation. I'm interested in studying ""fair"" ways of doing this from a mathematical standpoint. Let where be the multiset of grades for a given class. A of is some other multiset where is some function. We say a scale is fair if is monotone increasing. Given two fair scales and with respective scale-functions , we say is fairer than if Let us suppose that the professor wants to scale the grades such that the mean grade is . Given the above definitions, which scale function should he choose to ensure the scale is as fair as possible?  If there's not a simple function that always works, is there an algorithm or a strategy that might be helpful? This is, of course, but one model. There's also issues of subjectivity associated with the word ""fairness"". Perhaps there's some notion of ""fairness"" that this model doesn't quite capture. If so, please mention it.  My opinion is that the ""fairest"" way of scaling is ensuring that the scaling preserves the original order, and disturbs the original dataset as little as possible. One other possible notion (which you may consider if you are interested in, but not specifically the one I've chosen to ask about)  is considering the double sum and trying to minimize this among all possible (fair/monotone) scale functions . With my original model above, a scale is ""fair"" if it doesn't disturb the original dataset much. With this above model, a scale may disturb the original dataset a lot, but it still might be quite fair so long as students' grade are all altered a similar amount (for instance, a fixed scale of %). Feel free to discuss other mathematically rigorous notions of ""fair"" scaling which you believe are pertinent, or possibly cite relevant literature.","S = \{X_1, X_2 \cdots X_k\} X_i \in [0,100] \textit{scale} S' S S'=\{\phi(X_1), \phi(X_2), \cdots \phi(X_k)\} \phi:[0,100] \to [0,100] \phi S' S'' \phi, \psi S' S'' \sum_i |\phi(X_i) - X_i| \leq \sum_i |\psi(X_i) - X_i| 70 \pm 5 \% \phi \sum_{i,k} \left||\phi(X_i) - X_i| - |\phi(X_k) - X_k|\right| \phi 20","['statistics', 'optimization']"
93,Understanding the P-value,Understanding the P-value,,"I'm having difficulty understanding the p-value. It is said to reject the null hypothesis when the p-value is small. Smaller than the significance level. So does that mean in a hypothesis test, the p-value represents the area of the null hypothesis? Therefore because the p-value is small, it would imply the probability of the null hypothesis being unlikely?","I'm having difficulty understanding the p-value. It is said to reject the null hypothesis when the p-value is small. Smaller than the significance level. So does that mean in a hypothesis test, the p-value represents the area of the null hypothesis? Therefore because the p-value is small, it would imply the probability of the null hypothesis being unlikely?",,"['statistics', 'hypothesis-testing']"
94,"Why ""hinge"" loss is equivalent to 0-1 loss in SVM?","Why ""hinge"" loss is equivalent to 0-1 loss in SVM?",,"I'm reading a book named The Elements of Statistical Learning by Hastie et al. In $\S 12.3.2$ it introduced the SVM as a penalization method: With $f(x)=h(x)^T \beta+\beta_0 $, the solution of the optimization problem $$\min_{\beta_0,\beta} \sum_{i=1}^N [1-y_i f(x_i)]_+ +\frac{\lambda}{2}\|\beta\|^2 $$ with $\lambda=\frac{1}{C}$, is the same as that for $$\min_{\beta_0,\beta} \frac{1}{2}\|\beta\|^2 +C \sum_{i=1}^N \xi_i$$ $$\text{subject to } \xi_i \geq 0,y_i f(x_i)\geq 1-\xi_i ~ \forall i, $$ Could anyone kindly give me some hint why they are equivalent, and what's the benefit of introducing the ""hinge"" loss? Thanks a lot!","I'm reading a book named The Elements of Statistical Learning by Hastie et al. In $\S 12.3.2$ it introduced the SVM as a penalization method: With $f(x)=h(x)^T \beta+\beta_0 $, the solution of the optimization problem $$\min_{\beta_0,\beta} \sum_{i=1}^N [1-y_i f(x_i)]_+ +\frac{\lambda}{2}\|\beta\|^2 $$ with $\lambda=\frac{1}{C}$, is the same as that for $$\min_{\beta_0,\beta} \frac{1}{2}\|\beta\|^2 +C \sum_{i=1}^N \xi_i$$ $$\text{subject to } \xi_i \geq 0,y_i f(x_i)\geq 1-\xi_i ~ \forall i, $$ Could anyone kindly give me some hint why they are equivalent, and what's the benefit of introducing the ""hinge"" loss? Thanks a lot!",,"['statistics', 'self-learning', 'machine-learning']"
95,Less than or equal on random variables,Less than or equal on random variables,,"Casella and Berger exercise 2.15 asks to prove ( $Y$ and $X$ are any random variables): $E[\max(X,Y)] = E[X] + E[Y] - E[\min(X,Y)]$ , in the solution it gives: Assume without loss of generality that $X \le Y$ . Then $\max(X, Y) = Y$ and $\min(X, Y) = X$ . Thus $X + Y = \max(X, Y) + \min(X, Y)$ . Taking expectations $E[X + Y] = E[\max(X,Y) + \min(X,Y)] = E[\max(X,Y)] + E[\min(X,Y)]$ . Therefore $E[\max(X,Y)] = E[X] + E[Y] − E[\min(X, Y)]$ . How shall I think of $X \le Y$ ""without loss of generality"". Is this stochastic dominance (but then I would not think we can assume it without loss of generality)? Or does it mean something else?","Casella and Berger exercise 2.15 asks to prove ( and are any random variables): , in the solution it gives: Assume without loss of generality that . Then and . Thus . Taking expectations . Therefore . How shall I think of ""without loss of generality"". Is this stochastic dominance (but then I would not think we can assume it without loss of generality)? Or does it mean something else?","Y X E[\max(X,Y)] = E[X] + E[Y] - E[\min(X,Y)] X \le Y \max(X, Y) = Y \min(X, Y) = X X + Y = \max(X, Y) + \min(X, Y) E[X + Y] = E[\max(X,Y) + \min(X,Y)] = E[\max(X,Y)] + E[\min(X,Y)] E[\max(X,Y)] = E[X] + E[Y] − E[\min(X, Y)] X \le Y","['statistics', 'expected-value']"
96,"Proof that the sample mean is the ""best estimator"" for the population mean.","Proof that the sample mean is the ""best estimator"" for the population mean.",,"I've always heard that the sample mean $\overline{X}$ is ""the best estimator"" for the population mean $\mu$ . But is that always true regardless of the population distribution? is there any proof for that? For example let's suppose for an unknown population, we have three samples, say $X_1$ , $X_2$ , $X_3$ . Based on what I've heard (Not necessarily true) the estimator defined as the following: $$\frac{1}{3}(X_1+X_2+X_3)$$ is always preferable to, for instance: $$\frac{1}{6}(X_1+X_3)+\frac{2}{3}X_2$$ or $$\max(X_1, X_2, X_3)$$ But in what sense is it better? and why?","I've always heard that the sample mean is ""the best estimator"" for the population mean . But is that always true regardless of the population distribution? is there any proof for that? For example let's suppose for an unknown population, we have three samples, say , , . Based on what I've heard (Not necessarily true) the estimator defined as the following: is always preferable to, for instance: or But in what sense is it better? and why?","\overline{X} \mu X_1 X_2 X_3 \frac{1}{3}(X_1+X_2+X_3) \frac{1}{6}(X_1+X_3)+\frac{2}{3}X_2 \max(X_1, X_2, X_3)","['statistics', 'statistical-inference', 'parameter-estimation']"
97,Maximum likelihood estimator of categorical distribution,Maximum likelihood estimator of categorical distribution,,"The task is: Population of the students has been divided into the following three   groups: Students with the mean of grades below 3.5 Students with the mean of grades between 3.5 and 4.5 Students with the mean of grades above 4.5 Each  student  in  the  population  is  described  by  a  vector  of    random  variables $x=  (x^1\ x^2\ x^3)^T$, taking one of three   possible states:   $(1\ 0\ 0)^T$ if the student belongs to the first   group,   $(0\ 1\ 0)^T$ if the student belongs to the second group, and $(0\ 0\ 1)^T$ if the student belongs to the third group.   The   distribution of $x$ is categorical distribution (also known as   generalized Bernoulli distribution or Multinoulli distribution) with   parameters $\theta= (\theta_1\ \theta_2\ \theta_3)^T$. From the   population of the students N examples were drawn. Calculate the   maximum likelihood estimator of $\theta$. I tried to do it similarly to Bernoulli case, but I'm stuck. The idea was to find $\theta^*$ by finding the maximum of probability distribution function. So my try was $$ M(x\mid\theta)=\prod_{d=0}^D \theta_d^{x_d}=\theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3}\\ \theta^* = \operatorname*{argmax}_\theta M(x\mid\theta) = \operatorname*{argmax}_\theta \ln(M(x\mid\theta))\\ \ln(M(x\mid\theta))= \ln(\theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3}) = x_1\ln\theta_1 + x_2\ln\theta_2 + x_3\ln\theta_3 = x^T (\ln\theta_1\ \ln\theta_2\ \ln\theta_3)^T $$ Next step would be calculating derivative with respect to $\theta$ and finding it's zero, but we don't have $\theta$ in the function. I'm not sure where is my mistake. Or perhaps there is no mistake and it is possible to convert $(\ln\theta_1\ \ln\theta_2\ \ln\theta_3)^T$ to some form with $\theta$?","The task is: Population of the students has been divided into the following three   groups: Students with the mean of grades below 3.5 Students with the mean of grades between 3.5 and 4.5 Students with the mean of grades above 4.5 Each  student  in  the  population  is  described  by  a  vector  of    random  variables $x=  (x^1\ x^2\ x^3)^T$, taking one of three   possible states:   $(1\ 0\ 0)^T$ if the student belongs to the first   group,   $(0\ 1\ 0)^T$ if the student belongs to the second group, and $(0\ 0\ 1)^T$ if the student belongs to the third group.   The   distribution of $x$ is categorical distribution (also known as   generalized Bernoulli distribution or Multinoulli distribution) with   parameters $\theta= (\theta_1\ \theta_2\ \theta_3)^T$. From the   population of the students N examples were drawn. Calculate the   maximum likelihood estimator of $\theta$. I tried to do it similarly to Bernoulli case, but I'm stuck. The idea was to find $\theta^*$ by finding the maximum of probability distribution function. So my try was $$ M(x\mid\theta)=\prod_{d=0}^D \theta_d^{x_d}=\theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3}\\ \theta^* = \operatorname*{argmax}_\theta M(x\mid\theta) = \operatorname*{argmax}_\theta \ln(M(x\mid\theta))\\ \ln(M(x\mid\theta))= \ln(\theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3}) = x_1\ln\theta_1 + x_2\ln\theta_2 + x_3\ln\theta_3 = x^T (\ln\theta_1\ \ln\theta_2\ \ln\theta_3)^T $$ Next step would be calculating derivative with respect to $\theta$ and finding it's zero, but we don't have $\theta$ in the function. I'm not sure where is my mistake. Or perhaps there is no mistake and it is possible to convert $(\ln\theta_1\ \ln\theta_2\ \ln\theta_3)^T$ to some form with $\theta$?",,['statistics']
98,Math formula with $\mathcal N$ symbol. What is it?,Math formula with  symbol. What is it?,\mathcal N,"Occasionally I stumble across formulas that use exotic letters, that I do not know how to read or pronounce or what they mean, even. Also it is hard to google for them, since there is no way of typing the letter. I searched through some Unicode tables, and the one letter that comes closest is 019D, but it is not exactly it. It is a large N but with curvy endings. I just now see it in a textbook (Mixed Effects Models in S and S-Plus, p. 8) and the formula looks about like this: $$b \sim \mathcal N(0, \sigma^2)$$ where $\mathcal N$ is the letter in question. Based on what is written in the book, I understand that the formula specifies some distribution of random variables with zero mean, and $\sigma^2$ variance. My questions are: How would I pronounce the letter? What's its Unicode value? What does it mean?","Occasionally I stumble across formulas that use exotic letters, that I do not know how to read or pronounce or what they mean, even. Also it is hard to google for them, since there is no way of typing the letter. I searched through some Unicode tables, and the one letter that comes closest is 019D, but it is not exactly it. It is a large N but with curvy endings. I just now see it in a textbook (Mixed Effects Models in S and S-Plus, p. 8) and the formula looks about like this: $$b \sim \mathcal N(0, \sigma^2)$$ where $\mathcal N$ is the letter in question. Based on what is written in the book, I understand that the formula specifies some distribution of random variables with zero mean, and $\sigma^2$ variance. My questions are: How would I pronounce the letter? What's its Unicode value? What does it mean?",,"['statistics', 'notation']"
99,Expectation of cumulative distribution function of a standard normal distributed random variable,Expectation of cumulative distribution function of a standard normal distributed random variable,,"Let $X$ be a normally distributed random variable with mean $0$ and variance $1$. Let $\Phi$ be the cumulative distribution function of the variable $X$. The find the expectation of $\Phi(X)$. I have  $$   E(\Phi(X))=   \int\limits_{-\infty}^{\infty}\Phi(x)\frac{e^{-x^2/2}}{\sqrt{2\pi}}\;\mathrm{d}x,   \quad\text{where}\quad   \Phi(x)=\int\limits_{-\infty}^{x}\frac{e^{-t^2/2}}{\sqrt{2\pi}}\;\mathrm{d}t. $$  I am stuck here. How do I proceed?","Let $X$ be a normally distributed random variable with mean $0$ and variance $1$. Let $\Phi$ be the cumulative distribution function of the variable $X$. The find the expectation of $\Phi(X)$. I have  $$   E(\Phi(X))=   \int\limits_{-\infty}^{\infty}\Phi(x)\frac{e^{-x^2/2}}{\sqrt{2\pi}}\;\mathrm{d}x,   \quad\text{where}\quad   \Phi(x)=\int\limits_{-\infty}^{x}\frac{e^{-t^2/2}}{\sqrt{2\pi}}\;\mathrm{d}t. $$  I am stuck here. How do I proceed?",,"['statistics', 'random-variables', 'normal-distribution']"
