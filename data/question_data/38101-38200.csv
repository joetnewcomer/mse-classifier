,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Problem from Herstein on group theory,Problem from Herstein on group theory,,"The problem is: If $G$ is a finite group with order not divisible by 3, and $(ab)^3=a^3b^3$ for all $a,b\in G$, then show that $G$ is abelian. I have been trying this for a long time but not been able to make any progress. The only thing that I can think of is: $$ab\cdot ab\cdot ab=aaa\cdot bbb\implies(ba)^2=a^2b^2=aabb=(\text{TPT})abba.$$ Now, how can I prove the last equality? If I write $aabb=abb^{-1}abb$, then in order for the hypothesis to be correct, $b^{-1}abb=ba\implies ab^2=b^2a$. Where am I going wrong? What should I do?","The problem is: If $G$ is a finite group with order not divisible by 3, and $(ab)^3=a^3b^3$ for all $a,b\in G$, then show that $G$ is abelian. I have been trying this for a long time but not been able to make any progress. The only thing that I can think of is: $$ab\cdot ab\cdot ab=aaa\cdot bbb\implies(ba)^2=a^2b^2=aabb=(\text{TPT})abba.$$ Now, how can I prove the last equality? If I write $aabb=abb^{-1}abb$, then in order for the hypothesis to be correct, $b^{-1}abb=ba\implies ab^2=b^2a$. Where am I going wrong? What should I do?",,['abstract-algebra']
1,The ring $\Bbb Z\left [\frac{-1+\sqrt{-19}}{2}\right ]$ is not a Euclidean domain,The ring  is not a Euclidean domain,\Bbb Z\left [\frac{-1+\sqrt{-19}}{2}\right ],"Let $\alpha = \frac{1+\sqrt{-19}}{2}$. Let $A = \mathbb Z[\alpha]$. Let's assume that we know that its invertibles are $\{1,-1\}$. During an exercise we proved that: Lemma: If $(D,g)$ is a Euclidean domain such that its invertibles are $\{1,-1\}$, and $x$ is an element of minimal degree among the elements that are not invertible, then $D/(x)$ is isomorphic to $\mathbb Z/2\mathbb Z$ or $\mathbb Z/3\mathbb Z$. Now the exercise asks: Prove that $A$ is not a Euclidean Domain. Everything hints to an argument by contradiction: let $(A, d)$ be a ED and $x$ an element of minimal degree among the non invertibles we'd like to show that $A/(x)$ is not isomorphic to $\mathbb Z/2\mathbb Z$ or $\mathbb Z/3\mathbb Z$. How do we do that? My problem is that, since I don't know what this degree function looks like, I don't know how to choose this $x$! I know that the elements of $A/(x)$ are of the form $a+(x)$, with $a$ of degree less than $x$ or zero. By minimality of $x$ this means that $a\in \{0, 1, -1\}$. Now I'm lost: how do we derive a contradiction from this?","Let $\alpha = \frac{1+\sqrt{-19}}{2}$. Let $A = \mathbb Z[\alpha]$. Let's assume that we know that its invertibles are $\{1,-1\}$. During an exercise we proved that: Lemma: If $(D,g)$ is a Euclidean domain such that its invertibles are $\{1,-1\}$, and $x$ is an element of minimal degree among the elements that are not invertible, then $D/(x)$ is isomorphic to $\mathbb Z/2\mathbb Z$ or $\mathbb Z/3\mathbb Z$. Now the exercise asks: Prove that $A$ is not a Euclidean Domain. Everything hints to an argument by contradiction: let $(A, d)$ be a ED and $x$ an element of minimal degree among the non invertibles we'd like to show that $A/(x)$ is not isomorphic to $\mathbb Z/2\mathbb Z$ or $\mathbb Z/3\mathbb Z$. How do we do that? My problem is that, since I don't know what this degree function looks like, I don't know how to choose this $x$! I know that the elements of $A/(x)$ are of the form $a+(x)$, with $a$ of degree less than $x$ or zero. By minimality of $x$ this means that $a\in \{0, 1, -1\}$. Now I'm lost: how do we derive a contradiction from this?",,"['abstract-algebra', 'ring-theory', 'principal-ideal-domains', 'euclidean-domain']"
2,Guides/tutorials to learn abstract algebra?,Guides/tutorials to learn abstract algebra?,,"I recently read up a bit on symmetry groups and was interested by how they apply to even the Rubik's cube . I'm also intrigued by how group theory helps prove that ""polynomials of degree $\gt4$ are not generally solvable"". I love set theory and stuff, but I'd like to learn something else of a similar type. Learning about groups, rings, fields and what-have-you seems like an obvious choice. Could anyone recommend any informal guides to abstract algebra that are written in (at least moderately) comprehensible language? (PDFs etc. would also be nice)","I recently read up a bit on symmetry groups and was interested by how they apply to even the Rubik's cube . I'm also intrigued by how group theory helps prove that ""polynomials of degree $\gt4$ are not generally solvable"". I love set theory and stuff, but I'd like to learn something else of a similar type. Learning about groups, rings, fields and what-have-you seems like an obvious choice. Could anyone recommend any informal guides to abstract algebra that are written in (at least moderately) comprehensible language? (PDFs etc. would also be nice)",,"['abstract-algebra', 'reference-request', 'soft-question', 'big-list']"
3,Example of a finite non-commutative ring without a unity,Example of a finite non-commutative ring without a unity,,"Give an example of a finite, non-commutative ring, which does not have a unity. I can't think of any thing which fits this question. I was thinking $M_2(\mathbb{R})$ but it has the identity. Any help is appreciated.","Give an example of a finite, non-commutative ring, which does not have a unity. I can't think of any thing which fits this question. I was thinking $M_2(\mathbb{R})$ but it has the identity. Any help is appreciated.",,"['abstract-algebra', 'examples-counterexamples']"
4,Definition of General Associativity for binary operations,Definition of General Associativity for binary operations,,"Let's say we are talking about addition defined in the real numbers. Then, by induction we define $\sum_{i=0}^{0}a_i=a_0$ and $\sum_{i=0}^{n}a_i=\sum_{i=0}^{n-1}a_i+a_n$ for $n> 1.\:$ Now, how do you define general associativity? I know that this has something to do with the fact that $\sum_{i=0}^{n}=\sum_{i=0}^{k}+\sum_{i=k+1}^{n}$ for $0\leq k<n$, being $\sum_{i=k}^{n}a_i=\sum_{i=0}^{n-k}a_{i+k}$ by definition. But the thing is that this doesn't quite define the notion of different ways of arranging brackets, like for example $(a_0+(a_1+a_2))+((a_3+a_4)+(a_5+a_6))$. So my question is how they formally define this process of bracketing. Think of the case when someone just tell you to prove the general associativity for the real numbers. How do they actually define this property in order to prove it? is it necessary to have one? Look at for example this proof , specifically at the point where professor M. Zuker says: ""Let us now assume that any bracketing of $a_1, a_2,...,a_k$ equals the standard form for $1\leq k\leq n-1$, where $n>3$"". But, then again my question: what is the definition of bracketing? is it actually necessary to have a definition of bracketing or this proof works whatever the definition of bracketing is? Also I have found this paper by William P. Wardlow - A generalized general associative law-  that contains different proofs of this general associativity law. The first one, the one that he suggests as his favorite, is done by Nathan Jacobson in his book ""Lectures of Abstract Algebra"" Vol. 1 page 20. Looking at this proof there is one point where he says ""Consider now any product associated with $(a_1, a_2,..., a_n)$..."", which means ""any bracketing associated with..."". Then again the same question. I hope you understand my point. If not, please fell free of asking anything related to my question. Edit: For clarification let's say we are talking about sum in the real numbers. Then, 1.- $(...(((a_0+a_1)+a_2)+a_3)...+a_n)$ is a representation of the formal definition by recursion, meaning $\sum_{i=0}^{n}a_i$ just as defined above:  $\sum_{i=0}^{0}a_i=a_0$ and $\sum_{i=0}^{n}a_i=\sum_{i=0}^{n-1}a_i+a_n$ for $n> 1$. 2.- What is the formal definition of $a_0+(a_1+(a_2+...+(a_{n-1}+a_n)...)$ ? 3.- What is the formal definition of something like $(a_0+((a_1+a_2)+a_3))+(((a_4+a_5)+a_6)+....+(a_{n-1}+a_n))$?","Let's say we are talking about addition defined in the real numbers. Then, by induction we define $\sum_{i=0}^{0}a_i=a_0$ and $\sum_{i=0}^{n}a_i=\sum_{i=0}^{n-1}a_i+a_n$ for $n> 1.\:$ Now, how do you define general associativity? I know that this has something to do with the fact that $\sum_{i=0}^{n}=\sum_{i=0}^{k}+\sum_{i=k+1}^{n}$ for $0\leq k<n$, being $\sum_{i=k}^{n}a_i=\sum_{i=0}^{n-k}a_{i+k}$ by definition. But the thing is that this doesn't quite define the notion of different ways of arranging brackets, like for example $(a_0+(a_1+a_2))+((a_3+a_4)+(a_5+a_6))$. So my question is how they formally define this process of bracketing. Think of the case when someone just tell you to prove the general associativity for the real numbers. How do they actually define this property in order to prove it? is it necessary to have one? Look at for example this proof , specifically at the point where professor M. Zuker says: ""Let us now assume that any bracketing of $a_1, a_2,...,a_k$ equals the standard form for $1\leq k\leq n-1$, where $n>3$"". But, then again my question: what is the definition of bracketing? is it actually necessary to have a definition of bracketing or this proof works whatever the definition of bracketing is? Also I have found this paper by William P. Wardlow - A generalized general associative law-  that contains different proofs of this general associativity law. The first one, the one that he suggests as his favorite, is done by Nathan Jacobson in his book ""Lectures of Abstract Algebra"" Vol. 1 page 20. Looking at this proof there is one point where he says ""Consider now any product associated with $(a_1, a_2,..., a_n)$..."", which means ""any bracketing associated with..."". Then again the same question. I hope you understand my point. If not, please fell free of asking anything related to my question. Edit: For clarification let's say we are talking about sum in the real numbers. Then, 1.- $(...(((a_0+a_1)+a_2)+a_3)...+a_n)$ is a representation of the formal definition by recursion, meaning $\sum_{i=0}^{n}a_i$ just as defined above:  $\sum_{i=0}^{0}a_i=a_0$ and $\sum_{i=0}^{n}a_i=\sum_{i=0}^{n-1}a_i+a_n$ for $n> 1$. 2.- What is the formal definition of $a_0+(a_1+(a_2+...+(a_{n-1}+a_n)...)$ ? 3.- What is the formal definition of something like $(a_0+((a_1+a_2)+a_3))+(((a_4+a_5)+a_6)+....+(a_{n-1}+a_n))$?",,"['abstract-algebra', 'definition', 'recursion']"
5,"If $|\lbrace g \in G: \pi (g)=g^{-1} \rbrace|>\frac{3|G|}{4}$, then $G$ is an abelian group.","If , then  is an abelian group.",|\lbrace g \in G: \pi (g)=g^{-1} \rbrace|>\frac{3|G|}{4} G,"Assume that $\pi$ is an automorphism of a finite group $G$. Let $S$ denote the set $\lbrace g \in G: \pi (g)=g^{-1} \rbrace$. Show that if $|S|>\frac{3|G|}{4}$, then $G$ is an abelian group. Anyone has any idea on how to solve this ? I have no idea how to start.","Assume that $\pi$ is an automorphism of a finite group $G$. Let $S$ denote the set $\lbrace g \in G: \pi (g)=g^{-1} \rbrace$. Show that if $|S|>\frac{3|G|}{4}$, then $G$ is an abelian group. Anyone has any idea on how to solve this ? I have no idea how to start.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups']"
6,Is Orzech's generalization of the surjective-endomorphism-is-injective theorem correct?,Is Orzech's generalization of the surjective-endomorphism-is-injective theorem correct?,,"In math.stackexchange answer #239445 , Makoto Kato quoted a statement from the paper Morris Orzech, Onto Endomorphisms are Isomorphisms , Amer. Math. Monthly 78 (1971), 357--362. The statement (Theorem 1 in said paper) claims that if $A$ is a commutative ring, if $M$ is a finitely-generated $A$-module, if $N$ is an $A$-submodule of $M$, and if $f : N \to M$ is a surjective $A$-linear map, then $f$ is also injective. This claim generalizes the well-known fact that a surjective endomorphism of a finitely-generated $A$-module is injective (which appears, e.g., as Lemma 10.15.4 in the Stacks project, Version 5b422bc, compiled on Nov 05, 2014 -- the numbering will probably change but it is Lemma {lemma-fun} in algebra.tex -- and I am citing this because the Stacks project has an interesting alternative proof of this fact). Orzech's proof first deals with the case of $A$ noetherian and then reduces the general case to it. Unfortunately, the reduction step is wrong: the map $f$ is applied to elements of $M$ which are not known to belong to $N$. (The proof in Orzech's paper is the same as the one in Kato's post.) Is the claim itself true for non-Noetherian $A$ ? I cannot even prove the $M = A$ case, nor can I obtain a counterexample from the usual suspects (polynomials in infinitely many variables, or idempotent variables, or nilpotent variables).","In math.stackexchange answer #239445 , Makoto Kato quoted a statement from the paper Morris Orzech, Onto Endomorphisms are Isomorphisms , Amer. Math. Monthly 78 (1971), 357--362. The statement (Theorem 1 in said paper) claims that if $A$ is a commutative ring, if $M$ is a finitely-generated $A$-module, if $N$ is an $A$-submodule of $M$, and if $f : N \to M$ is a surjective $A$-linear map, then $f$ is also injective. This claim generalizes the well-known fact that a surjective endomorphism of a finitely-generated $A$-module is injective (which appears, e.g., as Lemma 10.15.4 in the Stacks project, Version 5b422bc, compiled on Nov 05, 2014 -- the numbering will probably change but it is Lemma {lemma-fun} in algebra.tex -- and I am citing this because the Stacks project has an interesting alternative proof of this fact). Orzech's proof first deals with the case of $A$ noetherian and then reduces the general case to it. Unfortunately, the reduction step is wrong: the map $f$ is applied to elements of $M$ which are not known to belong to $N$. (The proof in Orzech's paper is the same as the one in Kato's post.) Is the claim itself true for non-Noetherian $A$ ? I cannot even prove the $M = A$ case, nor can I obtain a counterexample from the usual suspects (polynomials in infinitely many variables, or idempotent variables, or nilpotent variables).",,"['abstract-algebra', 'commutative-algebra', 'modules']"
7,Prove that prime ideals of a finite ring are maximal,Prove that prime ideals of a finite ring are maximal,,Let $R$ be a finite commutative unitary ring. How to prove that each prime ideal of $R$ is maximal?,Let $R$ be a finite commutative unitary ring. How to prove that each prime ideal of $R$ is maximal?,,"['abstract-algebra', 'ring-theory', 'maximal-and-prime-ideals', 'finite-rings']"
8,What exactly is an $R$-module?,What exactly is an -module?,R,"From Wikipedia : ""If $R$ is commutative, then left $R$-modules are the same as right $R$-modules and are simply called $R$-modules."" The definition of left $R$-module: $M$ is a left $R$-module if $M$ is an abelian group and $R$ a ring acting on $M$ such that (i) $r(m_1 + m_2) = rm_1 + rm_2$ (ii) $(r_1 + r_2 ) m = r_1 m + r_2 m$ (iii) $1m = m$ (iv) $r_1 (r_2m) = (r_1 r_2) m$ I don't understand what commutativity of $R$ has to do with the module being left and right. If $R$ is commutative it means that $r_1 r_2 = r_2 r_1$. Now how does it follow from that that $rm = mr$? $M$ is not a subset of $R$, it could be anything so how does commutativity of $R$ make elements of $M$ and $R$ commute, too?","From Wikipedia : ""If $R$ is commutative, then left $R$-modules are the same as right $R$-modules and are simply called $R$-modules."" The definition of left $R$-module: $M$ is a left $R$-module if $M$ is an abelian group and $R$ a ring acting on $M$ such that (i) $r(m_1 + m_2) = rm_1 + rm_2$ (ii) $(r_1 + r_2 ) m = r_1 m + r_2 m$ (iii) $1m = m$ (iv) $r_1 (r_2m) = (r_1 r_2) m$ I don't understand what commutativity of $R$ has to do with the module being left and right. If $R$ is commutative it means that $r_1 r_2 = r_2 r_1$. Now how does it follow from that that $rm = mr$? $M$ is not a subset of $R$, it could be anything so how does commutativity of $R$ make elements of $M$ and $R$ commute, too?",,"['abstract-algebra', 'modules']"
9,Every algebraic extension of a perfect field is separable and perfect,Every algebraic extension of a perfect field is separable and perfect,,"I am trying to prove this statement in the characteristic $p>0$ case. Every algebraic extension of a perfect field is separable and perfect. This is stated as a corollary of Proposition V.6.11 in Lang. Let $k$ be a field and let $K$ be a normal extension of $k$. Let $G$ be the group of automorphisms of $K$ that fix $k$. Let $K^G$ be the fixed field of $G$ acting on $K$, and $K_0$ the maximal separable subextension of $K$. Then $K^G$ is purely inseparable over $k$, $K$ is separable over $K^G$, $K_0\cap K^G =k$  and $K_0K^G=K$. Lang doesn't write many details in the proof except that every finite extension is contained in a normal extension. This is what I have so far. Suppose, $k=k^p$. Let $E$ be an algebraic extension of $k$. Let $\alpha \in E$. Now, $\alpha$ is contained in the field, say $K$ generated by the roots of the minimal polynomial of $\alpha$ over $k$ (and is hence normal as it's the splitting field of the minimal polynomial of $\alpha$). At this point, I don't know how to proceed. I am tempted to use the Frobenius map, but it does not fix every element of $k$ although it does stabilize it in this perfect case. Any hints will be appreciated. Thanks.","I am trying to prove this statement in the characteristic $p>0$ case. Every algebraic extension of a perfect field is separable and perfect. This is stated as a corollary of Proposition V.6.11 in Lang. Let $k$ be a field and let $K$ be a normal extension of $k$. Let $G$ be the group of automorphisms of $K$ that fix $k$. Let $K^G$ be the fixed field of $G$ acting on $K$, and $K_0$ the maximal separable subextension of $K$. Then $K^G$ is purely inseparable over $k$, $K$ is separable over $K^G$, $K_0\cap K^G =k$  and $K_0K^G=K$. Lang doesn't write many details in the proof except that every finite extension is contained in a normal extension. This is what I have so far. Suppose, $k=k^p$. Let $E$ be an algebraic extension of $k$. Let $\alpha \in E$. Now, $\alpha$ is contained in the field, say $K$ generated by the roots of the minimal polynomial of $\alpha$ over $k$ (and is hence normal as it's the splitting field of the minimal polynomial of $\alpha$). At this point, I don't know how to proceed. I am tempted to use the Frobenius map, but it does not fix every element of $k$ although it does stabilize it in this perfect case. Any hints will be appreciated. Thanks.",,"['abstract-algebra', 'field-theory']"
10,"Without the Axiom of Choice, does every infinite field contain a countably infinite subfield?","Without the Axiom of Choice, does every infinite field contain a countably infinite subfield?",,"Earlier today I asked whether every infinite field contains a countably infinite subfield. That question quickly received several positive answers, but the question of whether those answers use the Axiom of Choice has spawned off an interesting discussion in its own right. Thus I'd like to pose that question separately: Without using the Axiom of Choice, can it be shown that every infinite field contains a countably infinite subfield? Note that the corresponding question for sets has a negative answer, but since fields have so much more structure than sets, it is not a priori unreasonable to me that there might exist an AC-less resolution for fields. EDIT: For the purposes of this question, an infinite set is one that is not bijective with $\{1, \dots, n\}$ for any $n \in \Bbb N$. Thanks to @MartinSleziak for bringing this issue up in chat.","Earlier today I asked whether every infinite field contains a countably infinite subfield. That question quickly received several positive answers, but the question of whether those answers use the Axiom of Choice has spawned off an interesting discussion in its own right. Thus I'd like to pose that question separately: Without using the Axiom of Choice, can it be shown that every infinite field contains a countably infinite subfield? Note that the corresponding question for sets has a negative answer, but since fields have so much more structure than sets, it is not a priori unreasonable to me that there might exist an AC-less resolution for fields. EDIT: For the purposes of this question, an infinite set is one that is not bijective with $\{1, \dots, n\}$ for any $n \in \Bbb N$. Thanks to @MartinSleziak for bringing this issue up in chat.",,"['abstract-algebra', 'field-theory', 'set-theory', 'extension-field', 'axiom-of-choice']"
11,"Does the order, lattice of subgroups, and lattice of factor groups, uniquely determine a group up to isomorphism?","Does the order, lattice of subgroups, and lattice of factor groups, uniquely determine a group up to isomorphism?",,"If we have a two lattices (partially ordered) - one for subgroups, one for factor groups, and we know order of the group we want to have these subgroup and factor group lattices, is such a group unique up to isomorphism (if exists)? Or is there a counterexample? If that's true, are sufficient conditions on the order and subgroup lattices to guarantee uniqueness? Another way - what if we now lattice for subgroup and group of automorphism of group; is that group uniquely determined by that information? Thanks for help. (sorry for English)","If we have a two lattices (partially ordered) - one for subgroups, one for factor groups, and we know order of the group we want to have these subgroup and factor group lattices, is such a group unique up to isomorphism (if exists)? Or is there a counterexample? If that's true, are sufficient conditions on the order and subgroup lattices to guarantee uniqueness? Another way - what if we now lattice for subgroup and group of automorphism of group; is that group uniquely determined by that information? Thanks for help. (sorry for English)",,"['abstract-algebra', 'group-theory', 'lattice-orders']"
12,Good problem book on Abstract Algebra,Good problem book on Abstract Algebra,,"I am currently self-studying abstract algebra from Artin . In that background, I am looking for a problem book in a spirit somewhat similar to Problems in Mathematical Analysis by AMS so that I have a lot of problems to solve.","I am currently self-studying abstract algebra from Artin . In that background, I am looking for a problem book in a spirit somewhat similar to Problems in Mathematical Analysis by AMS so that I have a lot of problems to solve.",,['abstract-algebra']
13,Galois group of $X^4 + 4X^2 + 2$ over $\mathbb Q$.,Galois group of  over .,X^4 + 4X^2 + 2 \mathbb Q,"I'd like to calculate the Galois group of the polynomial $f = X^4 + 4X^2 + 2$ over $\mathbb Q$. My thoughts so far: By Eisenstein, $f$ is irreducible over $\mathbb Q$. So $\mathrm{Gal}(f)$ must be a transitive subgroup of $S_4$, i.e. $\mathrm{Gal}(f) = S_4, \ A_4, \ D_8, \ V_4$ or $C_4$. $X^4 + 4X^2 + 2 = 0 \Leftrightarrow X^2 = -2 \pm \sqrt{2}$. Write $\alpha_1 = \sqrt{-2 + \sqrt{2}}, \ \alpha_2 = -\sqrt{-2+\sqrt{2}}, \ \alpha_3 = \sqrt{-2-\sqrt{2}}, \ \alpha_4 = - \sqrt{-2-\sqrt{2}}$ for the roots of $f$. Then $\mathbb Q(\sqrt{2}, \alpha_1, \alpha_2) = \mathbb Q(\sqrt{2}, \alpha_1) $ is a degree $2$ extension of $\mathbb Q(\sqrt{2})$, and likewise for $\mathbb Q(\sqrt{2}, \alpha_3, \alpha_4) = \mathbb Q(\sqrt{2}, \alpha_3)$. So, by the tower law, the splitting field of $f$ is at most a degree $8$ extension of $\mathbb Q$, so $\mathrm{Gal}(f) = D_8, \ V_4$ or $C_4$. If I could show that $\mathbb Q(\sqrt{2}, \alpha_1) \neq \mathbb Q(\sqrt{2}, \alpha_3)$, then I'd have that $\mathrm{Gal}(f) = D_8$. At a glance this looks to be true, but I don't know how to prove it. $\mathrm{Gal}(\mathbb Q (\sqrt{2}) / \mathbb Q) = C_2 \lhd D_8, \ V_4$ and $C_4$, so this doesn't rule anything out. Any comments on my thoughts 1-4, or hints / explanations would be greatly appreciated.","I'd like to calculate the Galois group of the polynomial $f = X^4 + 4X^2 + 2$ over $\mathbb Q$. My thoughts so far: By Eisenstein, $f$ is irreducible over $\mathbb Q$. So $\mathrm{Gal}(f)$ must be a transitive subgroup of $S_4$, i.e. $\mathrm{Gal}(f) = S_4, \ A_4, \ D_8, \ V_4$ or $C_4$. $X^4 + 4X^2 + 2 = 0 \Leftrightarrow X^2 = -2 \pm \sqrt{2}$. Write $\alpha_1 = \sqrt{-2 + \sqrt{2}}, \ \alpha_2 = -\sqrt{-2+\sqrt{2}}, \ \alpha_3 = \sqrt{-2-\sqrt{2}}, \ \alpha_4 = - \sqrt{-2-\sqrt{2}}$ for the roots of $f$. Then $\mathbb Q(\sqrt{2}, \alpha_1, \alpha_2) = \mathbb Q(\sqrt{2}, \alpha_1) $ is a degree $2$ extension of $\mathbb Q(\sqrt{2})$, and likewise for $\mathbb Q(\sqrt{2}, \alpha_3, \alpha_4) = \mathbb Q(\sqrt{2}, \alpha_3)$. So, by the tower law, the splitting field of $f$ is at most a degree $8$ extension of $\mathbb Q$, so $\mathrm{Gal}(f) = D_8, \ V_4$ or $C_4$. If I could show that $\mathbb Q(\sqrt{2}, \alpha_1) \neq \mathbb Q(\sqrt{2}, \alpha_3)$, then I'd have that $\mathrm{Gal}(f) = D_8$. At a glance this looks to be true, but I don't know how to prove it. $\mathrm{Gal}(\mathbb Q (\sqrt{2}) / \mathbb Q) = C_2 \lhd D_8, \ V_4$ and $C_4$, so this doesn't rule anything out. Any comments on my thoughts 1-4, or hints / explanations would be greatly appreciated.",,"['abstract-algebra', 'galois-theory']"
14,"If $[G:H]=n$, then $g^{n!}\in H$ for all $g\in G$.","If , then  for all .",[G:H]=n g^{n!}\in H g\in G,"I have the following question: Let $G$ be a group and let $H$ be a subgroup of finite index of $G$ . Let $|G:H|=n$ Then this holds: $g^{n!}\in H$ for all $g\in G$ . Why is this true? I think, that's not very difficult, but I have no idea at the moment. Thanks!","I have the following question: Let be a group and let be a subgroup of finite index of . Let Then this holds: for all . Why is this true? I think, that's not very difficult, but I have no idea at the moment. Thanks!",G H G |G:H|=n g^{n!}\in H g\in G,"['abstract-algebra', 'group-theory']"
15,Is an integer uniquely determined by its multiplicative order mod every prime,Is an integer uniquely determined by its multiplicative order mod every prime,,"Let $x$ and $y$ be nonzero integers and $\mathrm{ord}_p(w)$ be the multiplicative order of $w$ in $ \mathbb{Z} /p \mathbb{Z} $. If $\mathrm{ord}_p(x) = \mathrm{ord}_p(y)$ for all primes ( Edit: not dividing $x$ or $y$), does this imply $x=y$?","Let $x$ and $y$ be nonzero integers and $\mathrm{ord}_p(w)$ be the multiplicative order of $w$ in $ \mathbb{Z} /p \mathbb{Z} $. If $\mathrm{ord}_p(x) = \mathrm{ord}_p(y)$ for all primes ( Edit: not dividing $x$ or $y$), does this imply $x=y$?",,"['abstract-algebra', 'number-theory']"
16,Is Geometric Algebra isomorphic to Tensor Algebra?,Is Geometric Algebra isomorphic to Tensor Algebra?,,"Is geometric algebra (Clifford algebra) isomorphic to tensor algebra?  If so, how then would one relate a unique 2-vector (this is what I'm going to call a multivector that is the sum of a scalar, vector, and bivector) to every 2nd rank tensor? Edit by the OP, converted from ""answer"" Okay.  Well I'm still curious if there's a way to represent any 2nd-rank tensor by a bivector, vector, and scalar.  Or in particular, can any $3 \times 3$ matrix be represented by a 2-vector in 3D. It seems to me that they can't because I would guess the matrix representation of a bivector (grade 2 element of a 2-vector) would be exactly the same as the $3 \times 3$ matrix representation of the cross product (i.e. $[a \times b]_{ij} = a_j b_i - a_i b_j$) which only uniquely identifies 3 components. I would also assume that the scalar part of the 2-vector would be represented by a scalar times the $3 \times 3$ identity matrix.  This would fill in 3 numbers, but really only uniquely gives 1 component. I don't know how to represent the vector component of the 2-vector as a $3 \times 3$ matrix but I don't see how it could identity the remaining 5 components by itself. Am I right then in assuming that there is a canonical matrix representation of a general 2-vector, but that there are matrices that cannot be represented by any 2-vector?","Is geometric algebra (Clifford algebra) isomorphic to tensor algebra?  If so, how then would one relate a unique 2-vector (this is what I'm going to call a multivector that is the sum of a scalar, vector, and bivector) to every 2nd rank tensor? Edit by the OP, converted from ""answer"" Okay.  Well I'm still curious if there's a way to represent any 2nd-rank tensor by a bivector, vector, and scalar.  Or in particular, can any $3 \times 3$ matrix be represented by a 2-vector in 3D. It seems to me that they can't because I would guess the matrix representation of a bivector (grade 2 element of a 2-vector) would be exactly the same as the $3 \times 3$ matrix representation of the cross product (i.e. $[a \times b]_{ij} = a_j b_i - a_i b_j$) which only uniquely identifies 3 components. I would also assume that the scalar part of the 2-vector would be represented by a scalar times the $3 \times 3$ identity matrix.  This would fill in 3 numbers, but really only uniquely gives 1 component. I don't know how to represent the vector component of the 2-vector as a $3 \times 3$ matrix but I don't see how it could identity the remaining 5 components by itself. Am I right then in assuming that there is a canonical matrix representation of a general 2-vector, but that there are matrices that cannot be represented by any 2-vector?",,['abstract-algebra']
17,Can the product of two non invertible elements in a ring be invertible?,Can the product of two non invertible elements in a ring be invertible?,,"Let $A$ be a unitary ring. The question is simply: can the product of two non invertible elements in $A$ be invertible? I proved that the answer is negative if $A$ does not have zero divisors, because if you have $a,b$ non invertible elements in $A$ and $abx=xab=1$, then $a(bx)=1$ so $a$ is right invertible and if we put $d=bxa  \implies db=b(xab)=b1=b  \implies d=1$ (because obviously $b$ is not $0$), so $a$ is left invertible, as desired. If $A$ does have zero divisors, I don't see how to adapt the proof... Thank you!","Let $A$ be a unitary ring. The question is simply: can the product of two non invertible elements in $A$ be invertible? I proved that the answer is negative if $A$ does not have zero divisors, because if you have $a,b$ non invertible elements in $A$ and $abx=xab=1$, then $a(bx)=1$ so $a$ is right invertible and if we put $d=bxa  \implies db=b(xab)=b1=b  \implies d=1$ (because obviously $b$ is not $0$), so $a$ is left invertible, as desired. If $A$ does have zero divisors, I don't see how to adapt the proof... Thank you!",,"['abstract-algebra', 'ring-theory']"
18,Centralizer of a given element in $S_n$?,Centralizer of a given element in ?,S_n,"It is known that any two disjoint cycles in $S_n$ commutes. Therefore, any $\pi\in S_n$ which is disjoint with $\sigma$ is in the centralizer of $\sigma$: $C_{S_n}(\sigma)$. Also  $$ \sigma^i\pi\in C_{S_n}(\sigma), 0\leq i\leq|\sigma|. \tag{*} $$ Here are my questions: Are these all the elements in $C_{S_n}(\sigma)$? If the answer is no , is there an method to find all the elements in $C_{S_n}(\sigma)$ other than check the elements one by one? A quick search returns several relevant questions: Order of the centralizer of a permutation Centralizer of a specific permutation Is there a systematic way of finding the conjugacy class and/or centralizer of an element? The method ($*$) is what I learned so far from these questions and their answers. An immediate example in which ($*$) does not work that I can come up with is $\sigma=(12)(34)(56)$ in $S_6$. No disjoint $\pi$ can be found in this case and  $$\langle(12),(34),(56)\rangle\subset C_{S_6}(\sigma).$$","It is known that any two disjoint cycles in $S_n$ commutes. Therefore, any $\pi\in S_n$ which is disjoint with $\sigma$ is in the centralizer of $\sigma$: $C_{S_n}(\sigma)$. Also  $$ \sigma^i\pi\in C_{S_n}(\sigma), 0\leq i\leq|\sigma|. \tag{*} $$ Here are my questions: Are these all the elements in $C_{S_n}(\sigma)$? If the answer is no , is there an method to find all the elements in $C_{S_n}(\sigma)$ other than check the elements one by one? A quick search returns several relevant questions: Order of the centralizer of a permutation Centralizer of a specific permutation Is there a systematic way of finding the conjugacy class and/or centralizer of an element? The method ($*$) is what I learned so far from these questions and their answers. An immediate example in which ($*$) does not work that I can come up with is $\sigma=(12)(34)(56)$ in $S_6$. No disjoint $\pi$ can be found in this case and  $$\langle(12),(34),(56)\rangle\subset C_{S_6}(\sigma).$$",,['abstract-algebra']
19,What is A Set Raised to the 0 Power? (In Relation to the Definition of a Nullary Operation),What is A Set Raised to the 0 Power? (In Relation to the Definition of a Nullary Operation),,"(I'm going to link to this Stackexchange post concerning Nullary Operations: why is a nullary operation a special element, usually 0 or 1? ) In general an operation is a function $f:S^n \to S$, where $n$ denotes the arity of the operation, and a nullary operation is a function $f:S^0 \to S$. It is clear that $S^n=S \times S \times \cdots \times S$ denotes the Cartesian product of a set with itself ($n$ times), however $S^0$ makes little sense to me. One might define $S^0=\emptyset$. At first glance, that seems like a good choice. However then a nullary operation makes no sense, since it must be a function $f: A \to B$ where $A$ and $B$ are two sets representing the domain and codomain of the function. And note a function is defined in terms of relation , where a relation is a subset of the Cartesian product $A \times B$. So when we think of $A=S^0=\emptyset$ and $B=S$, so that we have a nullary operation $f:\emptyset \to S$, what is a a subset of $\emptyset \times S$? What even is $\emptyset \times S$? By definition of the Cartesian product, \begin{equation} \emptyset \times S = \{(a,b):a \in \emptyset\textrm{ and }b \in S\}. \end{equation} But there exists no $a \in \emptyset$! Therefore there exist no ordered pairs in our Cartesian product! Therefore $\emptyset \times S = \emptyset !$. Remember a function is a relation which is a subset of the Cartesian product. Therefore our relation is the empty set, and therefore also $f=\emptyset$. Edit : We got the definition sorted out in the comments. Thanks especially to Hayden and Christoff for their great help. In summary, we figured out that $S^0=\{()\}$, the set containing only one element, the ""empty $n$-tuple"". Then if taking $S=\{1,2,...,k\}$, then a nullary operation $f:S^0 \to S$ is a function, i.e. some subset of $S^0 \times S = \{()\} \times S$. Looking at $\{()\} \times S$, this equals $\{((),1),((),2),...,((),k)\}$. Then a function is a special subset of this, so that each element of the domain (and there is only one element, namely $()$) is paired with exactly one element of the codomain. Therefore a function would have the form $f=\{((),a)\}$ where $a$ is one of $1,2,...,k$. Thank you for your help.","(I'm going to link to this Stackexchange post concerning Nullary Operations: why is a nullary operation a special element, usually 0 or 1? ) In general an operation is a function $f:S^n \to S$, where $n$ denotes the arity of the operation, and a nullary operation is a function $f:S^0 \to S$. It is clear that $S^n=S \times S \times \cdots \times S$ denotes the Cartesian product of a set with itself ($n$ times), however $S^0$ makes little sense to me. One might define $S^0=\emptyset$. At first glance, that seems like a good choice. However then a nullary operation makes no sense, since it must be a function $f: A \to B$ where $A$ and $B$ are two sets representing the domain and codomain of the function. And note a function is defined in terms of relation , where a relation is a subset of the Cartesian product $A \times B$. So when we think of $A=S^0=\emptyset$ and $B=S$, so that we have a nullary operation $f:\emptyset \to S$, what is a a subset of $\emptyset \times S$? What even is $\emptyset \times S$? By definition of the Cartesian product, \begin{equation} \emptyset \times S = \{(a,b):a \in \emptyset\textrm{ and }b \in S\}. \end{equation} But there exists no $a \in \emptyset$! Therefore there exist no ordered pairs in our Cartesian product! Therefore $\emptyset \times S = \emptyset !$. Remember a function is a relation which is a subset of the Cartesian product. Therefore our relation is the empty set, and therefore also $f=\emptyset$. Edit : We got the definition sorted out in the comments. Thanks especially to Hayden and Christoff for their great help. In summary, we figured out that $S^0=\{()\}$, the set containing only one element, the ""empty $n$-tuple"". Then if taking $S=\{1,2,...,k\}$, then a nullary operation $f:S^0 \to S$ is a function, i.e. some subset of $S^0 \times S = \{()\} \times S$. Looking at $\{()\} \times S$, this equals $\{((),1),((),2),...,((),k)\}$. Then a function is a special subset of this, so that each element of the domain (and there is only one element, namely $()$) is paired with exactly one element of the codomain. Therefore a function would have the form $f=\{((),a)\}$ where $a$ is one of $1,2,...,k$. Thank you for your help.",,"['abstract-algebra', 'elementary-set-theory']"
20,How is the general solution for algebraic equations of degree five formulated?,How is the general solution for algebraic equations of degree five formulated?,,"In a book on neural networks I found the statement: The general solution for algebraic equations of degree five, for example, cannot be formulated using only algebraic functions, yet this can be done if a more general class of functions is allowed as computational primitives. What are the ""more general class of functions""?","In a book on neural networks I found the statement: The general solution for algebraic equations of degree five, for example, cannot be formulated using only algebraic functions, yet this can be done if a more general class of functions is allowed as computational primitives. What are the ""more general class of functions""?",,"['abstract-algebra', 'polynomials']"
21,Which resources are available to self-study GAP?,Which resources are available to self-study GAP?,,"Background: This year I'll do another Group Theory course ( Open University M336 ). In the past I have used Mathematica's AbstractAlgebra package but (although visually appealing ) this is no longer sufficient (i.e. listing subgroups of $S_4$ takes ages). So, I want to learn more about GAP. I worked through beginner tutorials that I found via the GAP website . Currently, I am not making much progress with GAP. The reference manual does not help me much at this stage. Question: Which resources are available to self-study GAP? How does one become proficient in GAP? What ( books, tutorials ) should you study?","Background: This year I'll do another Group Theory course ( Open University M336 ). In the past I have used Mathematica's AbstractAlgebra package but (although visually appealing ) this is no longer sufficient (i.e. listing subgroups of takes ages). So, I want to learn more about GAP. I worked through beginner tutorials that I found via the GAP website . Currently, I am not making much progress with GAP. The reference manual does not help me much at this stage. Question: Which resources are available to self-study GAP? How does one become proficient in GAP? What ( books, tutorials ) should you study?",S_4,"['abstract-algebra', 'group-theory', 'computer-science', 'gap']"
22,What is computational group theory?,What is computational group theory?,,What is computational group theory? What is the difference between computational group theory and group theory? Is it an active area of the mathematical research currently? What are some of the most interesting results? What is the needed background to study it?,What is computational group theory? What is the difference between computational group theory and group theory? Is it an active area of the mathematical research currently? What are some of the most interesting results? What is the needed background to study it?,,"['abstract-algebra', 'group-theory', 'reference-request', 'computational-algebra']"
23,Algorithm for calculating $A^n$ with as few multiplications as possible,Algorithm for calculating  with as few multiplications as possible,A^n,"Is there an algorithm for working out the best way (i.e. fewest multiplications) of calculating $A^n$ in a structure where multiplication is associative? For example, suppose $A$ is a square matrix. Matrix multiplication is associative, and I can compute $A^9$ with $4$ multiplications: $$A^2 = A \cdot A$$ $$A^3 = A^2 \cdot A$$ $$ A^6 = A^3  \cdot A^3 $$ $$ A^9 = A^6 \cdot A^3 $$ One method which works is to compute $A^{2^i}$ and use the binary representation of $n$, but this is not always optimal, e.g. with $n=23$, we can do it in $6$ multiplications: $$ A^2 = A \cdot A $$ $$ A^3 = A^2 \cdot A $$ $$ A^5 = A^3 \cdot A^2 $$ $$ A^{10} = A^5 \cdot A^5 $$ $$ A^{20} = A^{10} \cdot A^{10} $$ $$ A^{23} = A^{20} \cdot A^3 $$ rather than $7$: $$ A^2 = A\cdot A $$ $$ A^4 = A^2 \cdot A^2 $$ $$ A^8 = A^4 \cdot A^4 $$ $$ A^{16} = A^8 \cdot A^8 $$ $$ A^{20} = A^{16} \cdot A^4 $$ $$ A^{22} = A^{20} \cdot A^2 $$ $$ A^{23} = A^{22} \cdot A $$ Is there an algorithm which gives the quickest way?","Is there an algorithm for working out the best way (i.e. fewest multiplications) of calculating $A^n$ in a structure where multiplication is associative? For example, suppose $A$ is a square matrix. Matrix multiplication is associative, and I can compute $A^9$ with $4$ multiplications: $$A^2 = A \cdot A$$ $$A^3 = A^2 \cdot A$$ $$ A^6 = A^3  \cdot A^3 $$ $$ A^9 = A^6 \cdot A^3 $$ One method which works is to compute $A^{2^i}$ and use the binary representation of $n$, but this is not always optimal, e.g. with $n=23$, we can do it in $6$ multiplications: $$ A^2 = A \cdot A $$ $$ A^3 = A^2 \cdot A $$ $$ A^5 = A^3 \cdot A^2 $$ $$ A^{10} = A^5 \cdot A^5 $$ $$ A^{20} = A^{10} \cdot A^{10} $$ $$ A^{23} = A^{20} \cdot A^3 $$ rather than $7$: $$ A^2 = A\cdot A $$ $$ A^4 = A^2 \cdot A^2 $$ $$ A^8 = A^4 \cdot A^4 $$ $$ A^{16} = A^8 \cdot A^8 $$ $$ A^{20} = A^{16} \cdot A^4 $$ $$ A^{22} = A^{20} \cdot A^2 $$ $$ A^{23} = A^{22} \cdot A $$ Is there an algorithm which gives the quickest way?",,"['algorithms', 'abstract-algebra']"
24,Normal subgroups of $S_n$ for $n\geq 5$.,Normal subgroups of  for .,S_n n\geq 5,"Question is to : Find all normal subgroups of $S_n$ for $n\geq 5$. What I have done so far is : We know that $A_n$ is one normal subgroup of $S_n$. Suppose $H\neq (1)$ is another normal subgroup of $S_n$ then, $H\cap A_n$ would also be a normal subgroup. As $H\cap A_n \subseteq A_n$ we see that $H\cap A_n \unlhd A_n$. But for $n\geq 5$, $A_n$ is a simple group so does not have proper normal subgroups. Thus, $H\cap A_n =(1)$. So, we should have $A_n\leq H \leq S_n$ with  $|A_n|=\frac{n!}{2}$ and $|S_n|=n!$ As there is no number in between $\frac{n!}{2}$ and $n!$ which is divisible by $\frac{n!}{2}$ and divides $n!$ we should end up with the case when $H=A_n$ or $H=S_n$. I feel thankful and it would be appreciated if someone can proof read this solution P.S : I have checked for this problem in this form and got two questions (one is tagged as duplicate) the original question do not have any proof for the Question ""Find all normal subgroups of $S_n$ for $n\geq 5$"". So, please do not tag it as duplicate as the question is not answered in any of the other OP.","Question is to : Find all normal subgroups of $S_n$ for $n\geq 5$. What I have done so far is : We know that $A_n$ is one normal subgroup of $S_n$. Suppose $H\neq (1)$ is another normal subgroup of $S_n$ then, $H\cap A_n$ would also be a normal subgroup. As $H\cap A_n \subseteq A_n$ we see that $H\cap A_n \unlhd A_n$. But for $n\geq 5$, $A_n$ is a simple group so does not have proper normal subgroups. Thus, $H\cap A_n =(1)$. So, we should have $A_n\leq H \leq S_n$ with  $|A_n|=\frac{n!}{2}$ and $|S_n|=n!$ As there is no number in between $\frac{n!}{2}$ and $n!$ which is divisible by $\frac{n!}{2}$ and divides $n!$ we should end up with the case when $H=A_n$ or $H=S_n$. I feel thankful and it would be appreciated if someone can proof read this solution P.S : I have checked for this problem in this form and got two questions (one is tagged as duplicate) the original question do not have any proof for the Question ""Find all normal subgroups of $S_n$ for $n\geq 5$"". So, please do not tag it as duplicate as the question is not answered in any of the other OP.",,"['abstract-algebra', 'group-theory']"
25,Why are superalgebras so important?,Why are superalgebras so important?,,"I know that a superalgebra is a $\mathbb Z/2\mathbb Z$-graded algebra and that it behaves nicely. I know very little physics though, so even though I know that the super- prefix is related to supersymmetry, I don't know what that means; is there a compelling mathematical reason to consider superalgebras?","I know that a superalgebra is a $\mathbb Z/2\mathbb Z$-graded algebra and that it behaves nicely. I know very little physics though, so even though I know that the super- prefix is related to supersymmetry, I don't know what that means; is there a compelling mathematical reason to consider superalgebras?",,['abstract-algebra']
26,$A_4$ extension of $\mathbb{Q}$ ramified at one prime,extension of  ramified at one prime,A_4 \mathbb{Q},How can one show that an $A_4$-extension of $\mathbb{Q}$ ramified at only one prime must be totally real?,How can one show that an $A_4$-extension of $\mathbb{Q}$ ramified at only one prime must be totally real?,,"['abstract-algebra', 'number-theory', 'algebraic-number-theory', 'galois-theory']"
27,Generic elementary group theory problems.,Generic elementary group theory problems.,,"This question is about generic group theory problems. here are examples for what I’m referring to: Prove that any group of order $p^2$ , where $p$ is a prime, is   abelian. Let $G$ be a group of order $2n$ . Suppose that half of the elements   of $G$ are order $2$ , and the other half form a subgroup $H$ of order $n$ . Prove that $H$ is of odd order ( $n$ is odd) and that it's   abelian. ""Let $G$ be a group with $|G|=pq$ for some $p$ , $q$ primes such that $p>q,q∤p−1$ . Prove $G$ is cyclic."" Most of the time I find myself attacking these kind of problems with no coherent strategy, just throwing all my ammunition (Cauchy theorem, Lagrange theorem, index 2 theorem, intersection unions and multiplication of (normal-) subgroups etc.). Best case scenario is I manage to prove the statement yet I don't quite get why the statement is true because the proof is so long and involves a lot of cases and assumptions by contradiction that i can't see the forest for the trees. A lot of the times there are more than one way to prove the statement which are not so similar. For an excellent example to what I’m referring to, look here . Since I feel that most of the time I’m just juggling variables I'd like to know what should i do to understand what's really going on? Another different thing that might ease my mind is an exhaustive list of the theorems that can be used to solve these kind of problems . That way i will at least know what are all the possible techniques that might work for these problems. ADDED: Although neat proofs for the specific problems i posted are welcome they are not the reason i asked this question. I solved these problems and others too yet my proofs were long and lacked an identifiable idea . What I'm looking for is a general principle that could guide me in constructing proofs for these problems.","This question is about generic group theory problems. here are examples for what I’m referring to: Prove that any group of order , where is a prime, is   abelian. Let be a group of order . Suppose that half of the elements   of are order , and the other half form a subgroup of order . Prove that is of odd order ( is odd) and that it's   abelian. ""Let be a group with for some , primes such that . Prove is cyclic."" Most of the time I find myself attacking these kind of problems with no coherent strategy, just throwing all my ammunition (Cauchy theorem, Lagrange theorem, index 2 theorem, intersection unions and multiplication of (normal-) subgroups etc.). Best case scenario is I manage to prove the statement yet I don't quite get why the statement is true because the proof is so long and involves a lot of cases and assumptions by contradiction that i can't see the forest for the trees. A lot of the times there are more than one way to prove the statement which are not so similar. For an excellent example to what I’m referring to, look here . Since I feel that most of the time I’m just juggling variables I'd like to know what should i do to understand what's really going on? Another different thing that might ease my mind is an exhaustive list of the theorems that can be used to solve these kind of problems . That way i will at least know what are all the possible techniques that might work for these problems. ADDED: Although neat proofs for the specific problems i posted are welcome they are not the reason i asked this question. I solved these problems and others too yet my proofs were long and lacked an identifiable idea . What I'm looking for is a general principle that could guide me in constructing proofs for these problems.","p^2 p G 2n G 2 H n H n G |G|=pq p q p>q,q∤p−1 G","['abstract-algebra', 'group-theory']"
28,Intuition about the first isomorphism theorem,Intuition about the first isomorphism theorem,,"I'm currently studying group theory and recently I've read about the first isomorphism theorem which can be stated as follows: Let $G$ and $H$ be groups and $\varphi :G\to H$ a homomorphism, then $\ker \varphi$ is a normal subgroup of $G$, $\varphi(G)$ is a subgroup of $H$ and  $G/\ker \varphi \simeq \varphi(G)$. The proof is quite easy, but I've been thinking about what's the best way to understand this result. In that setting I've came up with the following intuition: It's easy to see that a homomorphism $\varphi : G\to H$ is injective if and only if $\ker \varphi = \{e\}$ where $e$ is the identity of $G$. Now, my intuition about the first isomorphism theorem is: if $\varphi : G\to H$ is a homomorphism which is not injective, we can then construct a new group on which the equivalent homomorphism is indeed injective. We do this by quotienting out what is in the way of making $\varphi$ injective, that is, everything that is in the kernel. In that way taking the quotient $G/\ker \varphi$ we construct a group on which we ""kill"" everything that is in the kernel of $\varphi$. The natual projection of $\varphi$ to this quotient will then be an injective function. So is this the best way to understand the first isomorphism theorem? It's a way to ""get out of the way"" everything which is stoping a homomorphism from being an injective map? If not, what is the correct intuition about this theorem and its importance?","I'm currently studying group theory and recently I've read about the first isomorphism theorem which can be stated as follows: Let $G$ and $H$ be groups and $\varphi :G\to H$ a homomorphism, then $\ker \varphi$ is a normal subgroup of $G$, $\varphi(G)$ is a subgroup of $H$ and  $G/\ker \varphi \simeq \varphi(G)$. The proof is quite easy, but I've been thinking about what's the best way to understand this result. In that setting I've came up with the following intuition: It's easy to see that a homomorphism $\varphi : G\to H$ is injective if and only if $\ker \varphi = \{e\}$ where $e$ is the identity of $G$. Now, my intuition about the first isomorphism theorem is: if $\varphi : G\to H$ is a homomorphism which is not injective, we can then construct a new group on which the equivalent homomorphism is indeed injective. We do this by quotienting out what is in the way of making $\varphi$ injective, that is, everything that is in the kernel. In that way taking the quotient $G/\ker \varphi$ we construct a group on which we ""kill"" everything that is in the kernel of $\varphi$. The natual projection of $\varphi$ to this quotient will then be an injective function. So is this the best way to understand the first isomorphism theorem? It's a way to ""get out of the way"" everything which is stoping a homomorphism from being an injective map? If not, what is the correct intuition about this theorem and its importance?",,"['abstract-algebra', 'group-theory', 'intuition', 'group-homomorphism']"
29,"Pathologies in ""rng""","Pathologies in ""rng""",,"There is no general consensus regarding whether a ring should have a unity element or not. Many authors work with unital rings , and other does not essentially require unity.  If we do not assume unity to be a necessary part of ring, lets call that structure , a "" rng "" (which may or may not have unity i.e. $1$), then many pathologies do occur. I am hoping to get such pathologies (with at least one example) listed together on a page which I can keep as a record for future references. I will post my list as an answer here, to get it started, and hope others will contribute to it, and make this post valuable.","There is no general consensus regarding whether a ring should have a unity element or not. Many authors work with unital rings , and other does not essentially require unity.  If we do not assume unity to be a necessary part of ring, lets call that structure , a "" rng "" (which may or may not have unity i.e. $1$), then many pathologies do occur. I am hoping to get such pathologies (with at least one example) listed together on a page which I can keep as a record for future references. I will post my list as an answer here, to get it started, and hope others will contribute to it, and make this post valuable.",,"['abstract-algebra', 'ring-theory', 'ideals', 'rngs']"
30,Is the ring of holomorphic functions on $S^1$ Noetherian?,Is the ring of holomorphic functions on  Noetherian?,S^1,"Let $S^1={\{ z \in \Bbb{C} : |z|=1 \}}$ be the unit circle. Let $R= \mathcal{H}(S^1)$ be the ring of holomorphic functions on $S^1$, i.e. the ring of functions $f: S^1 \longrightarrow \Bbb{C}$ which can be extended to an holomorphic function on an open neighbourhood of $S^1$. My question is: Is $R$ is a Noetherian ring? What I tried: We can think $R$ as a direct limit $$R= \lim_{\longrightarrow} \mathcal{H}(\Omega_n)$$ where $\Omega_n = \{ z \in \Bbb{C} : 1-\frac{1}{n} < |z| < 1+\frac{1}{n}\}$ denotes the open annulus of amplitude $2/n$ for $n\ge3$, and  $$\mathcal{H}(\Omega_3) \longrightarrow \mathcal{H}(\Omega_4) \longrightarrow \mathcal{H}(\Omega_5) \longrightarrow \dots$$ is a direct system of rings whose maps are restrictions. Since restrictions are injective ring morphisms, we can think $R= \bigcup_n \mathcal{H}(\Omega_n)$ In particular $R$ is a Bezout domain (every finitely generated ideal of $R$ is principal). Now, if $R$ were Noetherian, then $R$ would be a PID. And here I got stuck: it could be possible that this is non-Noetherian since it is a union of non-Noetherian rings on the other hand $S^1$ is compact, so every function $f \in R$ can only have a finite number of zeroes: this gives me intuition that maybe $R$ is a UFD.","Let $S^1={\{ z \in \Bbb{C} : |z|=1 \}}$ be the unit circle. Let $R= \mathcal{H}(S^1)$ be the ring of holomorphic functions on $S^1$, i.e. the ring of functions $f: S^1 \longrightarrow \Bbb{C}$ which can be extended to an holomorphic function on an open neighbourhood of $S^1$. My question is: Is $R$ is a Noetherian ring? What I tried: We can think $R$ as a direct limit $$R= \lim_{\longrightarrow} \mathcal{H}(\Omega_n)$$ where $\Omega_n = \{ z \in \Bbb{C} : 1-\frac{1}{n} < |z| < 1+\frac{1}{n}\}$ denotes the open annulus of amplitude $2/n$ for $n\ge3$, and  $$\mathcal{H}(\Omega_3) \longrightarrow \mathcal{H}(\Omega_4) \longrightarrow \mathcal{H}(\Omega_5) \longrightarrow \dots$$ is a direct system of rings whose maps are restrictions. Since restrictions are injective ring morphisms, we can think $R= \bigcup_n \mathcal{H}(\Omega_n)$ In particular $R$ is a Bezout domain (every finitely generated ideal of $R$ is principal). Now, if $R$ were Noetherian, then $R$ would be a PID. And here I got stuck: it could be possible that this is non-Noetherian since it is a union of non-Noetherian rings on the other hand $S^1$ is compact, so every function $f \in R$ can only have a finite number of zeroes: this gives me intuition that maybe $R$ is a UFD.",,"['abstract-algebra', 'complex-analysis', 'ring-theory', 'noetherian']"
31,Construct ideals in $\mathbb Z[x]$ with a given least number of generators,Construct ideals in  with a given least number of generators,\mathbb Z[x],"How do you construct, for each $n\geq 1$, an ideal in $\mathbb Z[x]$ of the form $(a_1,a_2,\dots,a_n)$ with $a_i\in \mathbb Z[x]$ such that it is impossible to have $(b_1,b_2,\dots,b_m)=(a_1,a_2,\dots,a_n)$ with $m<n$ and $b_j\in\mathbb Z[x]$? Thank you!","How do you construct, for each $n\geq 1$, an ideal in $\mathbb Z[x]$ of the form $(a_1,a_2,\dots,a_n)$ with $a_i\in \mathbb Z[x]$ such that it is impossible to have $(b_1,b_2,\dots,b_m)=(a_1,a_2,\dots,a_n)$ with $m<n$ and $b_j\in\mathbb Z[x]$? Thank you!",,"['abstract-algebra', 'polynomials', 'commutative-algebra', 'ideals']"
32,"If $f(x)\in\mathbb Q[x]$ and $f(f(x)),f(f(f(x)))\in\mathbb Z[x]$ prove that $f(x)\in\mathbb Z[x]$",If  and  prove that,"f(x)\in\mathbb Q[x] f(f(x)),f(f(f(x)))\in\mathbb Z[x] f(x)\in\mathbb Z[x]","If a polynomial $f(x)\in\mathbb Q[x]$ and $$f(f(x)),f(f(f(x)))\in\mathbb Z[x]$$ prove that $f(x)\in\mathbb Z[x]$ . This problem is from 2019 Japan Mathematical Olympiad: see problem 9 (the third day last problem) https://www.imojp.org/archive/mo2019/selection_camp/problems/2019.pdf Here is what I tried:: Let $f(x)=\dfrac{P(x)}{d}$ where $P(x)\in\mathbb Z[x],d$ is the common denominator of the coefficients of $f(x)$ . If let $P(x)=\sum_{i=0}^{n}a_{i}x^i,a_{i}\in\mathbb Z$ , then we have $$f(f(x))=\dfrac{1}{d}\sum_{i=0}^{n}\dfrac{a_{i}}{d^i}\left(\sum_{i=0}^{n}a_{i}x^i\right)^i\in\mathbb Z[x]$$ and $f(f(f(x)))$ is very ugly, so how to solve this problem?","If a polynomial and prove that . This problem is from 2019 Japan Mathematical Olympiad: see problem 9 (the third day last problem) https://www.imojp.org/archive/mo2019/selection_camp/problems/2019.pdf Here is what I tried:: Let where is the common denominator of the coefficients of . If let , then we have and is very ugly, so how to solve this problem?","f(x)\in\mathbb Q[x] f(f(x)),f(f(f(x)))\in\mathbb Z[x] f(x)\in\mathbb Z[x] f(x)=\dfrac{P(x)}{d} P(x)\in\mathbb Z[x],d f(x) P(x)=\sum_{i=0}^{n}a_{i}x^i,a_{i}\in\mathbb Z f(f(x))=\dfrac{1}{d}\sum_{i=0}^{n}\dfrac{a_{i}}{d^i}\left(\sum_{i=0}^{n}a_{i}x^i\right)^i\in\mathbb Z[x] f(f(f(x)))","['abstract-algebra', 'polynomials', 'contest-math']"
33,Simple proof of Cauchy's theorem in Group theory,Simple proof of Cauchy's theorem in Group theory,,"I will refer to the following simple proof of Cauchy's theorem that appears in chapter 33 of Pinter's A Book of Abstract Algebra . I have copied it below so my question can be properly understood. This proof is crystal clear, however what I cannot understand is why $p$ has to be a prime number? It seems to me that the proof works for any divisor of $|G|$. Could somebody clarify this? I would appreciate it. Cauchy's theorem: Let $G$ be a finite group of order $n$ and let $p$ be a prime divisor of $n$, then $G$ has an element of order $p$. Pinter proves Cauchy's theorem specifically for $p=5$; however, he says, the same argument works for any value of $p$. Consider all possible 5-tuples $(a, b, c, d, k)$ of elements of $G$ whose product $abcdk =e$. How many distinct 5-tuples of this kind are there? Well, if we select $a, b, c$ and $d$ at random, there is a unique $k=d^{-1} c^{-1} b^{-1} a^{-1}$ in $G$ making $abcdk = e$. Thus, there are $n^4$ such 5-tuples. Call two 5-tuples equivalent if one is merely a cyclic permutation of the  other. Thus, $(a, b, c, d, k)$ is equivalent to exactly five distinct 5-tuples, namely $(a, b, c, d, k)$, $(b, c, d, k, a)$, $(c, d, k, a, b)$, $(d, k, a, b, c)$ and $(k, a, b, c, d)$. The only exception occurs when a 5-tuple is of the form $(a, a, a, a, a)$ with all its components equal; it is equivalent only to itself. Thus, the equivalence class of any 5-tuple of the form $(a, a, a, a, a)$ has a single member, while all the other equivalence classes have five members. Are there any equivalence classes, other than ${(e, e, e, e, e)}$, with a single member? If not then $5$ divides $(n^4-1)$ (for there are $n^4$ 5-tuples under consideration, less $(e, e, e, e, e)$), hence $n^4\equiv 1\pmod 5$. But we are assuming that 5 divides $n$, hence $n^4\equiv 0\pmod 5$, which is a contradiction. This contradiction shows that there must be a 5-tuple $(a,a,a,a,a)\neq(e, e, e, e, e)$ such that $aaaaa=a^5=e$. Thus, there is an element $a\in G$ of order 5. ■","I will refer to the following simple proof of Cauchy's theorem that appears in chapter 33 of Pinter's A Book of Abstract Algebra . I have copied it below so my question can be properly understood. This proof is crystal clear, however what I cannot understand is why $p$ has to be a prime number? It seems to me that the proof works for any divisor of $|G|$. Could somebody clarify this? I would appreciate it. Cauchy's theorem: Let $G$ be a finite group of order $n$ and let $p$ be a prime divisor of $n$, then $G$ has an element of order $p$. Pinter proves Cauchy's theorem specifically for $p=5$; however, he says, the same argument works for any value of $p$. Consider all possible 5-tuples $(a, b, c, d, k)$ of elements of $G$ whose product $abcdk =e$. How many distinct 5-tuples of this kind are there? Well, if we select $a, b, c$ and $d$ at random, there is a unique $k=d^{-1} c^{-1} b^{-1} a^{-1}$ in $G$ making $abcdk = e$. Thus, there are $n^4$ such 5-tuples. Call two 5-tuples equivalent if one is merely a cyclic permutation of the  other. Thus, $(a, b, c, d, k)$ is equivalent to exactly five distinct 5-tuples, namely $(a, b, c, d, k)$, $(b, c, d, k, a)$, $(c, d, k, a, b)$, $(d, k, a, b, c)$ and $(k, a, b, c, d)$. The only exception occurs when a 5-tuple is of the form $(a, a, a, a, a)$ with all its components equal; it is equivalent only to itself. Thus, the equivalence class of any 5-tuple of the form $(a, a, a, a, a)$ has a single member, while all the other equivalence classes have five members. Are there any equivalence classes, other than ${(e, e, e, e, e)}$, with a single member? If not then $5$ divides $(n^4-1)$ (for there are $n^4$ 5-tuples under consideration, less $(e, e, e, e, e)$), hence $n^4\equiv 1\pmod 5$. But we are assuming that 5 divides $n$, hence $n^4\equiv 0\pmod 5$, which is a contradiction. This contradiction shows that there must be a 5-tuple $(a,a,a,a,a)\neq(e, e, e, e, e)$ such that $aaaaa=a^5=e$. Thus, there is an element $a\in G$ of order 5. ■",,"['abstract-algebra', 'group-theory']"
34,Does $A^2 \cong B^2$ imply $A \cong B$ for rings?,Does  imply  for rings?,A^2 \cong B^2 A \cong B,"If $A$ and $B$ are two unital rings such that $A \times A \cong B \times B$, as rings, does it follows that $A$ and $B$ are isomorphic (as rings)? I believe that the answer is no, but I can't come up with a counterexample. A similar question for groups has already been asked - the answer is not straightforward. Here is a possibly related question , but there are $R$-modules isomorphisms. [If $A$ and $B$ are fields, then we can see $B^2$ as a $2$-dimensional $A$-vector space, so that $A \cong B$ as $A$-vector spaces, because they have the same dimension. I may be wrong about this, but anyway this is not sufficient to get a field isomorphism.] Thank you for your comments!","If $A$ and $B$ are two unital rings such that $A \times A \cong B \times B$, as rings, does it follows that $A$ and $B$ are isomorphic (as rings)? I believe that the answer is no, but I can't come up with a counterexample. A similar question for groups has already been asked - the answer is not straightforward. Here is a possibly related question , but there are $R$-modules isomorphisms. [If $A$ and $B$ are fields, then we can see $B^2$ as a $2$-dimensional $A$-vector space, so that $A \cong B$ as $A$-vector spaces, because they have the same dimension. I may be wrong about this, but anyway this is not sufficient to get a field isomorphism.] Thank you for your comments!",,"['abstract-algebra', 'ring-theory', 'examples-counterexamples']"
35,Subring of $\mathcal O(\mathbb C)$,Subring of,\mathcal O(\mathbb C),"Let $\mathfrak A \subset \mathcal O(\mathbb C)$ be the subring generated by the nowhere zero analytic functions $f: \mathbb C \to \mathbb C$. Do we have a precise description of $\mathfrak A$? Is $\mathfrak A = \mathcal O (\mathbb C)$, or equivalently is there a holomorphic function $f$ such that $f \neq g_1 + \dots + g_r $ for all $g_1, \dots, g_r \in \mathcal O^*(\mathbb C)$ ? For the moment, I only know that $\mathfrak A$ contains constants functions, and the functions on the form $z \mapsto e^f$ where $f$ is some holomorphic map. But I have really no idea how to solve this problem. Any hints?","Let $\mathfrak A \subset \mathcal O(\mathbb C)$ be the subring generated by the nowhere zero analytic functions $f: \mathbb C \to \mathbb C$. Do we have a precise description of $\mathfrak A$? Is $\mathfrak A = \mathcal O (\mathbb C)$, or equivalently is there a holomorphic function $f$ such that $f \neq g_1 + \dots + g_r $ for all $g_1, \dots, g_r \in \mathcal O^*(\mathbb C)$ ? For the moment, I only know that $\mathfrak A$ contains constants functions, and the functions on the form $z \mapsto e^f$ where $f$ is some holomorphic map. But I have really no idea how to solve this problem. Any hints?",,"['abstract-algebra', 'complex-analysis', 'ring-theory', 'commutative-algebra']"
36,Was my abstract algebra textbook trying to kill me?,Was my abstract algebra textbook trying to kill me?,,"I'm doing an independent study (self-taught course) in abstract algebra and I'm using the abstract algebra textbook here: http://abstract.ups.edu/ . In Chapter 9: Isomorphisms, problem 20 asks: ""Prove or disprove: Every abelian group of order divisible by $3$ contains a subgroup of order $3$."" I spent a lot of time on this question and eventually came up with the answer below, but this question seemed a lot harder than all of the other questions. Keeping in mind that the book hasn't yet given me all of the usual tools to prove this (see below or the textbook for the ones I do have), did I miss something obvious? Or is this actually that hard? At one point, the book says, ""In Chapter 13, we will prove that all finite abelian groups are isomorphic to direct products of the form $\mathbb{Z}_{p_1^{e_1}}\times\cdots\times\mathbb{Z}_{p_k^{e_k}}$, where $p_1,\dots,p_k$ are (not necessarily distinct) primes."" Do you think I was supposed to use this, even though the book hasn't proven it yet? So far, I've covered some basic material about cyclic groups, the groups $S_n$, $A_n$, and $D_n$, Lagrange's Theorem, basic properties of isomorphisms, and some basic direct product stuff. In particular, I have not covered quotients or group actions. For more complete information, please see the textbook. My solution (abbreviated) Call a group $3$-free if its order is not divisible by 3. Let $G$ be a group with order divisible by $3$, and find a maximal $3$-free subgroup $H$ (one that is not contained in any other $3$-free subgroup). Pick a $g\in G\setminus H$ and let $d$ be the least positive integer such that $g^d\in H$. Then $g^iH=g^jH$ iff $i\equiv j\text{ mod }d$, so the subgroup (of $G$) $H'=\langle g\rangle H$ has $|H'|=d\cdot |H|$. $|H'|$ must be divisible by $3$, so $d$ is divisible by $3$, and therefore the order $k$ of $g$ is divisible by $3$, so $g^{k/3}$ has order $3$. Thanks!","I'm doing an independent study (self-taught course) in abstract algebra and I'm using the abstract algebra textbook here: http://abstract.ups.edu/ . In Chapter 9: Isomorphisms, problem 20 asks: ""Prove or disprove: Every abelian group of order divisible by $3$ contains a subgroup of order $3$."" I spent a lot of time on this question and eventually came up with the answer below, but this question seemed a lot harder than all of the other questions. Keeping in mind that the book hasn't yet given me all of the usual tools to prove this (see below or the textbook for the ones I do have), did I miss something obvious? Or is this actually that hard? At one point, the book says, ""In Chapter 13, we will prove that all finite abelian groups are isomorphic to direct products of the form $\mathbb{Z}_{p_1^{e_1}}\times\cdots\times\mathbb{Z}_{p_k^{e_k}}$, where $p_1,\dots,p_k$ are (not necessarily distinct) primes."" Do you think I was supposed to use this, even though the book hasn't proven it yet? So far, I've covered some basic material about cyclic groups, the groups $S_n$, $A_n$, and $D_n$, Lagrange's Theorem, basic properties of isomorphisms, and some basic direct product stuff. In particular, I have not covered quotients or group actions. For more complete information, please see the textbook. My solution (abbreviated) Call a group $3$-free if its order is not divisible by 3. Let $G$ be a group with order divisible by $3$, and find a maximal $3$-free subgroup $H$ (one that is not contained in any other $3$-free subgroup). Pick a $g\in G\setminus H$ and let $d$ be the least positive integer such that $g^d\in H$. Then $g^iH=g^jH$ iff $i\equiv j\text{ mod }d$, so the subgroup (of $G$) $H'=\langle g\rangle H$ has $|H'|=d\cdot |H|$. $|H'|$ must be divisible by $3$, so $d$ is divisible by $3$, and therefore the order $k$ of $g$ is divisible by $3$, so $g^{k/3}$ has order $3$. Thanks!",,"['abstract-algebra', 'group-theory']"
37,The tensor product of two Artinian modules is Artinian,The tensor product of two Artinian modules is Artinian,,user$xxxxx$ posted (and then deleted) the following question which I think deserves to be here: Prove that the tensor product of two Artinian modules is Artinian.,user$xxxxx$ posted (and then deleted) the following question which I think deserves to be here: Prove that the tensor product of two Artinian modules is Artinian.,,['abstract-algebra']
38,"For $G$ group and $H$ subgroup of finite index, prove that $N \subset H$ normal subgroup of $G$ of finite index exists","For  group and  subgroup of finite index, prove that  normal subgroup of  of finite index exists",G H N \subset H G,"Let $G$ be a group and $H$ be a subgroup of $G$ with finite index. I want to show that there exists a normal subgroup $N$ of $G$ with finite index and $N \subset H$. The hint for this exercise is to find a homomorphism $G \to S_n$ for $n := [G:H]$ with kernel contained in $H$. The standard solution suggests to choose $\varphi$ as the homomorphism induced by left-multiplication $\varphi: G \to S(G/H) \cong S_n$. I'm not 100% sure if I understand this correctly. What exactly does $\varphi$ do? We take $g \in G$ and send it to a bijection $\varphi_g: G/H \to G/H, xH \mapsto gxH$? If so, how can I see that its kernel is contained in $H$? Also, the standard solution claims its image is isomorphic to $G/N$ and thus $N$ has a finite index in $G$, how can I see that the image is isomorphic to $G/N$? Thanks in advance for any help.","Let $G$ be a group and $H$ be a subgroup of $G$ with finite index. I want to show that there exists a normal subgroup $N$ of $G$ with finite index and $N \subset H$. The hint for this exercise is to find a homomorphism $G \to S_n$ for $n := [G:H]$ with kernel contained in $H$. The standard solution suggests to choose $\varphi$ as the homomorphism induced by left-multiplication $\varphi: G \to S(G/H) \cong S_n$. I'm not 100% sure if I understand this correctly. What exactly does $\varphi$ do? We take $g \in G$ and send it to a bijection $\varphi_g: G/H \to G/H, xH \mapsto gxH$? If so, how can I see that its kernel is contained in $H$? Also, the standard solution claims its image is isomorphic to $G/N$ and thus $N$ has a finite index in $G$, how can I see that the image is isomorphic to $G/N$? Thanks in advance for any help.",,['abstract-algebra']
39,Alternative construction of the tensor product (or: pass this secret),Alternative construction of the tensor product (or: pass this secret),,"The paper Tensor products and bimorphisms by B. Banachewski and E. Nelson studies tensor products (defined by classifying bimorphisms) in concrete categories. It is quite interesting that their main existence theorem gives an alternative, quite explicit construction of the tensor product of two modules (or any other algebraic structures). If $M,N$ are $R$-modules with underlying sets $|M|,|N|$, consider $$P=\bigoplus_{m \in |M|} N \oplus \bigoplus_{n \in |N|} M$$ with the natural inclusions $i_m : N \to P$  for $m \in |M|$ and $j_n : M \to P$ for $n \in |N|$. Let $U=\langle i_m(n)-j_n(m) : (m,n) \in |M| \times |N| \rangle$. Then $P/U$ is a model for $M \otimes_R N$. Question 1 . Is there any other paper or book at all which mentions this construction? Or is it well-known? Question 2 . Is there a textbook introducing tensor products and gives this construction as a proof that it exists? Question 3 (subjective): Isn't this construction more explicit than the usual one (which starts with the free module on $|M| \times |N|$ and mods out bilinear relations)? It only uses direct sums, generated submodules, and quotients, no free modules are needed. What do you think, do you favor it? Is it suited for the use in textbooks and classes? If you are a teacher or professor, would you consider using this construction in your class? What are your reasons? I have found a smiliar ""free module""-free construction of the module of differentials $\Omega^1_{A/R}$ for an $R$-algebra $A$: The $R$-linear map $A \otimes_R A \to A \otimes_R A$, $a \otimes b \mapsto  ab \otimes 1 - b \otimes a - a \otimes b$ extends to an $A$-linear map $(A \otimes_R A) \otimes_R A \to A \otimes_R A$, when $A$ acts on the right. Let $\Omega^1_{A/R}$ be its cokernel, and $d(a)$ the image of $a \otimes 1$. The universal property is immediate. I would like to ask the same questions as above.","The paper Tensor products and bimorphisms by B. Banachewski and E. Nelson studies tensor products (defined by classifying bimorphisms) in concrete categories. It is quite interesting that their main existence theorem gives an alternative, quite explicit construction of the tensor product of two modules (or any other algebraic structures). If $M,N$ are $R$-modules with underlying sets $|M|,|N|$, consider $$P=\bigoplus_{m \in |M|} N \oplus \bigoplus_{n \in |N|} M$$ with the natural inclusions $i_m : N \to P$  for $m \in |M|$ and $j_n : M \to P$ for $n \in |N|$. Let $U=\langle i_m(n)-j_n(m) : (m,n) \in |M| \times |N| \rangle$. Then $P/U$ is a model for $M \otimes_R N$. Question 1 . Is there any other paper or book at all which mentions this construction? Or is it well-known? Question 2 . Is there a textbook introducing tensor products and gives this construction as a proof that it exists? Question 3 (subjective): Isn't this construction more explicit than the usual one (which starts with the free module on $|M| \times |N|$ and mods out bilinear relations)? It only uses direct sums, generated submodules, and quotients, no free modules are needed. What do you think, do you favor it? Is it suited for the use in textbooks and classes? If you are a teacher or professor, would you consider using this construction in your class? What are your reasons? I have found a smiliar ""free module""-free construction of the module of differentials $\Omega^1_{A/R}$ for an $R$-algebra $A$: The $R$-linear map $A \otimes_R A \to A \otimes_R A$, $a \otimes b \mapsto  ab \otimes 1 - b \otimes a - a \otimes b$ extends to an $A$-linear map $(A \otimes_R A) \otimes_R A \to A \otimes_R A$, when $A$ acts on the right. Let $\Omega^1_{A/R}$ be its cokernel, and $d(a)$ the image of $a \otimes 1$. The universal property is immediate. I would like to ask the same questions as above.",,"['abstract-algebra', 'reference-request', 'modules', 'tensor-products', 'education']"
40,"**UNSOLVED** Find an integer $\geqslant2$ that is build up out of only $1$'s and $0$'s in base $1,\;\ldots,\;10$.",**UNSOLVED** Find an integer  that is build up out of only 's and 's in base .,"\geqslant2 1 0 1,\;\ldots,\;10","This riddle bothers me for a few weeks now and I'm starting to worry that I need some $p$ -adic Number theory to solve this. I solve most of the riddles in a day, but this one is just annoying to me. I was thinking of taking the number $10!$ . Any help is appreciated! Edit I'm looking for a number that is written as only a series of $1$ 's and $0$ 's which represent the number $\sum_{n=0}^{\infty}a_n*p^n$ with $a_n\in\{0,\ldots,p-1\}$ where $a_n$ is either $1$ or $0$ . Edit 2 Probably this is a not so well known problem, but people are working on it with no solution yet found for this case. They let a program search for such numbers and there were no matches so far. There is a solution in base $1,\;\ldots,\;5$ , namely $$82000 = 10100000001010000\ (2) = 11011111001\ (3) = 110001100\ (4) = 10111000\ (5).$$ This number fails in the original problem for $82000=1431344\ (6)$ . Originally checked to $2^{65520}$ (or about $3*10^{19723}$ ) on Nov 07 2008. Conjectured to be complete. a(4), if it exists, it is greater than $10^{15}$ . Apr 06 2012. In 2016, a Mathematician stated that it is plausible that there are no more such terms, but it is not proven. See for yourself: https://oeis.org/A146025 .","This riddle bothers me for a few weeks now and I'm starting to worry that I need some -adic Number theory to solve this. I solve most of the riddles in a day, but this one is just annoying to me. I was thinking of taking the number . Any help is appreciated! Edit I'm looking for a number that is written as only a series of 's and 's which represent the number with where is either or . Edit 2 Probably this is a not so well known problem, but people are working on it with no solution yet found for this case. They let a program search for such numbers and there were no matches so far. There is a solution in base , namely This number fails in the original problem for . Originally checked to (or about ) on Nov 07 2008. Conjectured to be complete. a(4), if it exists, it is greater than . Apr 06 2012. In 2016, a Mathematician stated that it is plausible that there are no more such terms, but it is not proven. See for yourself: https://oeis.org/A146025 .","p 10! 1 0 \sum_{n=0}^{\infty}a_n*p^n a_n\in\{0,\ldots,p-1\} a_n 1 0 1,\;\ldots,\;5 82000 = 10100000001010000\ (2) = 11011111001\ (3) = 110001100\ (4) = 10111000\ (5). 82000=1431344\ (6) 2^{65520} 3*10^{19723} 10^{15}","['abstract-algebra', 'number-theory', 'contest-math']"
41,Why is associativity required for groups?,Why is associativity required for groups?,,"Why is associativity required for groups? I'm doing a linear algebra paper and we're focusing on groups at the moment, specifically proving whether something is or is not a group. There are four axioms: The set is closed under the operation. The operation is associative. The exists and identity in the group. Each element in the group has an inverse which is also in the group. Why does the operation need to be associative? Thanks","Why is associativity required for groups? I'm doing a linear algebra paper and we're focusing on groups at the moment, specifically proving whether something is or is not a group. There are four axioms: The set is closed under the operation. The operation is associative. The exists and identity in the group. Each element in the group has an inverse which is also in the group. Why does the operation need to be associative? Thanks",,"['abstract-algebra', 'group-theory', 'definition']"
42,Idempotents in a local ring,Idempotents in a local ring,,"Is it true that a local ring, i.e., a commutative ring with a unique maximal ideal, doesn't contain idempotent elements $\neq 0, 1$ ?  Why ? Any hint ?","Is it true that a local ring, i.e., a commutative ring with a unique maximal ideal, doesn't contain idempotent elements $\neq 0, 1$ ?  Why ? Any hint ?",,"['abstract-algebra', 'commutative-algebra']"
43,Does a finite commutative ring necessarily have a unity?,Does a finite commutative ring necessarily have a unity?,,"Does a finite commutative ring necessarily have a unity? I ask because of the following theorem given in my lecture notes: Theorem. In a finite commutative ring every non-zero-divisor is a unit. If it had said ""finite commutative ring with unity ..."" there would be no question to ask, I understand that part. What I'm asking about is whether or not we can omit explicitly stating it because it follows from the finiteness of our commutative ring. [ Clarification ] The way I'm learning ring theory now, a ""ring"" is defined as an additive Abelian group further equipped (I hope I'm using the right terminology) with an associative multiplication operation which distributes over addition. In this definition we do not require the existence of 1. In other words, when I say ""ring"" I mean a rng.","Does a finite commutative ring necessarily have a unity? I ask because of the following theorem given in my lecture notes: Theorem. In a finite commutative ring every non-zero-divisor is a unit. If it had said ""finite commutative ring with unity ..."" there would be no question to ask, I understand that part. What I'm asking about is whether or not we can omit explicitly stating it because it follows from the finiteness of our commutative ring. [ Clarification ] The way I'm learning ring theory now, a ""ring"" is defined as an additive Abelian group further equipped (I hope I'm using the right terminology) with an associative multiplication operation which distributes over addition. In this definition we do not require the existence of 1. In other words, when I say ""ring"" I mean a rng.",,"['abstract-algebra', 'ring-theory', 'rngs', 'finite-rings']"
44,"Prove that $\mathbb{R^*}$, the set of all real numbers except $0$, is not a cyclic group","Prove that , the set of all real numbers except , is not a cyclic group",\mathbb{R^*} 0,Prove that $\mathbb{R^*}$ is not a cyclic group. (Here $\mathbb{R^*}$ means all the elements of $\mathbb{R}$ except $0$.) I know from the definition of a cyclic group that a group is cyclic if it is generated by a single element. I was thinking of doing a proof by contradiction but then that ended up nowhere.,Prove that $\mathbb{R^*}$ is not a cyclic group. (Here $\mathbb{R^*}$ means all the elements of $\mathbb{R}$ except $0$.) I know from the definition of a cyclic group that a group is cyclic if it is generated by a single element. I was thinking of doing a proof by contradiction but then that ended up nowhere.,,"['abstract-algebra', 'group-theory', 'cyclic-groups']"
45,Proving that all integers are even or odd [duplicate],Proving that all integers are even or odd [duplicate],,"This question already has answers here : Proving that an integer is even if and only if it is not odd (2 answers) Closed 8 years ago . I know that $\mathbb{Z}$ is a group under addition with a multiplication defined. I have just the definition of even and odd integers: $n$ is even if $n = 2k$ for some integer $k$ and $n$ is odd if $n = 2k+1$ for some integer $k$. Using just this I am wondering how to prove that all integers are either even or odd. That is, how can I prove that given an integer $n$, $n$ must be even or odd? My problem with this is that it seems so simple. I know that one can divide an integer by $2$ and the remainder will be $0$ or $1$. Using this, it is clear that the even and odd integers make up everything. But how can I prove it without using this fact about remainders and such? I guess one could also use facts about prime numbers, but I am looking for a proof that just uses the definition of odd and even.","This question already has answers here : Proving that an integer is even if and only if it is not odd (2 answers) Closed 8 years ago . I know that $\mathbb{Z}$ is a group under addition with a multiplication defined. I have just the definition of even and odd integers: $n$ is even if $n = 2k$ for some integer $k$ and $n$ is odd if $n = 2k+1$ for some integer $k$. Using just this I am wondering how to prove that all integers are either even or odd. That is, how can I prove that given an integer $n$, $n$ must be even or odd? My problem with this is that it seems so simple. I know that one can divide an integer by $2$ and the remainder will be $0$ or $1$. Using this, it is clear that the even and odd integers make up everything. But how can I prove it without using this fact about remainders and such? I guess one could also use facts about prime numbers, but I am looking for a proof that just uses the definition of odd and even.",,"['abstract-algebra', 'elementary-number-theory', 'parity']"
46,In a PID every nonzero prime ideal is maximal,In a PID every nonzero prime ideal is maximal,,"In a principal ideal domain, prove that every non trivial prime ideal is a maximal ideal Attempt: Let $R$ be the principal ideal domain. A principal ideal domain $R$ is an integral domain in which every ideal $A$ is of the form $\langle a \rangle = \{ar~~ | ~~r \in R\}$ Let $ \langle a \rangle $ be a prime ideal $\implies R/A $ is an integral domain. A finite integral domain is a field . Hence, if we prove that $R/A$ is finite, then $R/A$ is a field $\implies A$ is a maximal ideal. Now, $\langle a \rangle = \{ar~~ | ~~r \in R\}$ Since, R is an integral domain, there are no zero divisors and cancellation is allowed $\implies ar_1 = ar_2 \implies r_1=r_2 \implies ar_i$ maps to a different member of $R$ for each different $r_i \implies \langle a \rangle$ represents the elements of $R$ in some random order. $\implies \langle a \rangle = R$ and hence, $R/A \approx {0}$ is finite and hence $A$ is maximal. Is my attempt correct?","In a principal ideal domain, prove that every non trivial prime ideal is a maximal ideal Attempt: Let $R$ be the principal ideal domain. A principal ideal domain $R$ is an integral domain in which every ideal $A$ is of the form $\langle a \rangle = \{ar~~ | ~~r \in R\}$ Let $ \langle a \rangle $ be a prime ideal $\implies R/A $ is an integral domain. A finite integral domain is a field . Hence, if we prove that $R/A$ is finite, then $R/A$ is a field $\implies A$ is a maximal ideal. Now, $\langle a \rangle = \{ar~~ | ~~r \in R\}$ Since, R is an integral domain, there are no zero divisors and cancellation is allowed $\implies ar_1 = ar_2 \implies r_1=r_2 \implies ar_i$ maps to a different member of $R$ for each different $r_i \implies \langle a \rangle$ represents the elements of $R$ in some random order. $\implies \langle a \rangle = R$ and hence, $R/A \approx {0}$ is finite and hence $A$ is maximal. Is my attempt correct?",,"['abstract-algebra', 'ring-theory', 'ideals', 'maximal-and-prime-ideals', 'principal-ideal-domains']"
47,Are all extensions of finite fields cyclic?,Are all extensions of finite fields cyclic?,,"My book says all extensions of finite fields are cyclic, but I could not find a proof (maybe I haven't looked hard enough).  If it's straightforward, can you tell me why it's true?  Thanks :)","My book says all extensions of finite fields are cyclic, but I could not find a proof (maybe I haven't looked hard enough).  If it's straightforward, can you tell me why it's true?  Thanks :)",,['abstract-algebra']
48,Is $\sqrt 7$ a sum of roots of unity?,Is  a sum of roots of unity?,\sqrt 7,"Let $a_1,\dots,a_n$ and $b_1,\dots,b_n$ be two sequences of rational numbers. Is it possible that $\sqrt 7 = \sum_{m=1}^{n} a_m (-1)^{b_m}$ ? Is it possible that $\sqrt{17}$ = $\sum_{m=1}^{n} a_m (-1)^{b_m}$ ? How to prove or disprove these ?",Let and be two sequences of rational numbers. Is it possible that ? Is it possible that = ? How to prove or disprove these ?,"a_1,\dots,a_n b_1,\dots,b_n \sqrt 7 = \sum_{m=1}^{n} a_m (-1)^{b_m} \sqrt{17} \sum_{m=1}^{n} a_m (-1)^{b_m}","['abstract-algebra', 'field-theory', 'roots-of-unity', 'cyclotomic-fields']"
49,"Show that $({\mathbb{Q}},+)$ is not finitely generated using the Fundamental Theorem of Finitely Generated Abelian Groups.",Show that  is not finitely generated using the Fundamental Theorem of Finitely Generated Abelian Groups.,"({\mathbb{Q}},+)","Can anyone please help me out on how to use the fundamental theorem of finitely generated abelian groups to prove that $({\mathbb{Q}},+)$ is not finitely generated?","Can anyone please help me out on how to use the fundamental theorem of finitely generated abelian groups to prove that $({\mathbb{Q}},+)$ is not finitely generated?",,"['abstract-algebra', 'group-theory', 'abelian-groups', 'finitely-generated']"
50,The correspondence theorem for groups,The correspondence theorem for groups,,"I had studied group theory a year ago, but still could not understand the proof involving The Correspondence theorem. let $G$ be a group and let $N⊴G$ , where $N⊴G$ indicates that $N$ is a normal subgroup of $G$ . Then there is a bijection from the set of subgroups $A$ of $G$ that contain $N$ onto the set of subgroups $\overline A $ = $A/N$ of $G/N.$ Can anyone explain the steps to me in: $(1.)$ a very easy-going manner so that I understand importance of each step. $(2.)$ I don't require the proof but only how the step follow each other . I'll be thankful if there is anyone kind enough to do this....","I had studied group theory a year ago, but still could not understand the proof involving The Correspondence theorem. let be a group and let , where indicates that is a normal subgroup of . Then there is a bijection from the set of subgroups of that contain onto the set of subgroups = of Can anyone explain the steps to me in: a very easy-going manner so that I understand importance of each step. I don't require the proof but only how the step follow each other . I'll be thankful if there is anyone kind enough to do this....",G N⊴G N⊴G N G A G N \overline A  A/N G/N. (1.) (2.),"['abstract-algebra', 'group-theory']"
51,Exercises in category theory for a non-working mathematican (undergrad),Exercises in category theory for a non-working mathematican (undergrad),,"I'm trying to learn category theory pretty much on my own (with some help from a professor). My main information source is the good old Categories for the working mathematician by Mac Lane. I find the book very good and and I don't have very much trouble understanding the theory, proofs and motivations and so on. But many examples fly over my head as do the exercises. The thing is that I'm far from a working mathematician or a grad student, which the book seems to be aimed towards. I know that one have to do learn math by doing math and I find it almost impossible when exercises involve things I'm not yet familiar with (modules, algebras etc). I simply don't have time to learn these things just so I can solve my exercises but on the other hand I don't want to miss out on learning things just because the exercises are on a too high level for me. So I want to ask you for references on exercises in category theory aimed at someone with limited knowledge of abstract algebra, suitable for concepts in Categories for the working mathematician but with more basic objects but still meaningful and challenging. (I consider myself to have fairly good basic knowledge of ""elementary""-style abstract algebra (monoids, groups, rings, fields, linear algebra, some galois theory, related number theory, some algebraic graph theory etc))","I'm trying to learn category theory pretty much on my own (with some help from a professor). My main information source is the good old Categories for the working mathematician by Mac Lane. I find the book very good and and I don't have very much trouble understanding the theory, proofs and motivations and so on. But many examples fly over my head as do the exercises. The thing is that I'm far from a working mathematician or a grad student, which the book seems to be aimed towards. I know that one have to do learn math by doing math and I find it almost impossible when exercises involve things I'm not yet familiar with (modules, algebras etc). I simply don't have time to learn these things just so I can solve my exercises but on the other hand I don't want to miss out on learning things just because the exercises are on a too high level for me. So I want to ask you for references on exercises in category theory aimed at someone with limited knowledge of abstract algebra, suitable for concepts in Categories for the working mathematician but with more basic objects but still meaningful and challenging. (I consider myself to have fairly good basic knowledge of ""elementary""-style abstract algebra (monoids, groups, rings, fields, linear algebra, some galois theory, related number theory, some algebraic graph theory etc))",,"['abstract-algebra', 'reference-request', 'category-theory']"
52,Analogy between the fundamental theorems of arithmetic and algebra,Analogy between the fundamental theorems of arithmetic and algebra,,"For the learned mathematician it may be obvious and not worth mentioning: that the fundamental theorems of arithmetic and algebra look very similar and have to do with each other, in abbreviated form: $$ n = p_1\cdot p_2 \cdots p_k$$ $$ P(z) = z_0\cdot(z_1 -z)\cdot (z_2 -z) \cdots (z_k -z)$$ which makes obvious that the irreducible polynoms of first degree play the same role in $\mathbb{C}[X]$ as do the prime numbers in $\mathbb{Z}$ (which both are unitary rings). It also gives — in this special case — the wording ""fundamental theorem"" a specific meaning: It is stated that and how some irreducible elements build the fundaments of a structure. Is this analogy helpful, or is it superficial and maybe misleading? If the former, can it be formalised? If the latter, what are the differences that make it merely superficial?","For the learned mathematician it may be obvious and not worth mentioning: that the fundamental theorems of arithmetic and algebra look very similar and have to do with each other, in abbreviated form: which makes obvious that the irreducible polynoms of first degree play the same role in as do the prime numbers in (which both are unitary rings). It also gives — in this special case — the wording ""fundamental theorem"" a specific meaning: It is stated that and how some irreducible elements build the fundaments of a structure. Is this analogy helpful, or is it superficial and maybe misleading? If the former, can it be formalised? If the latter, what are the differences that make it merely superficial?", n = p_1\cdot p_2 \cdots p_k  P(z) = z_0\cdot(z_1 -z)\cdot (z_2 -z) \cdots (z_k -z) \mathbb{C}[X] \mathbb{Z},"['abstract-algebra', 'arithmetic']"
53,Expressing associativity with only two variables,Expressing associativity with only two variables,,"I'm wondering if it is possible to axiomatize associativity using a set of equations in only two variables. Suppose we have a signature consisting of one binary operation $\cdot$ . Is it possible to find a set $\Sigma$ of equations containing only variables $x$ and $y$ , such that the equational theory generated by axioms $\Sigma$ is equal to the equational theory generated by axiom $(x\cdot y)\cdot z = x\cdot (y\cdot z)$ ? Or in other words, such that the variety defined by equations $\Sigma$ is equal to the variety defined by equation $(x\cdot y)\cdot z = x\cdot (y\cdot z)$ ? EDIT: We can take as $\Sigma$ the set of all equations in variables $x$ and $y$ that are entailed by equation $(x\cdot y)\cdot z = x\cdot (y\cdot z)$ . For example $(x\cdot y)\cdot x = x\cdot (y\cdot x)$ is one of the many equations contained in $\Sigma$ . The question is whether $\Sigma$ in turn entails $(x\cdot y)\cdot z = x\cdot (y\cdot z)$ . EDIT2: As per Milo Brandt's comment, for any three terms $p(x,y)$ , $q(x,y)$ , $r(x,y)$ containing at most variables $x, y$ , equation $p\cdot (q\cdot r)=(p\cdot q)\cdot r$ is in $\Sigma$ . Thus, for any algebra $A$ in the variety defined by equations $\Sigma$ , every subalgebra of $A$ generated by two elements is associative. So, in a sense, $A$ is ""locally associative"".","I'm wondering if it is possible to axiomatize associativity using a set of equations in only two variables. Suppose we have a signature consisting of one binary operation . Is it possible to find a set of equations containing only variables and , such that the equational theory generated by axioms is equal to the equational theory generated by axiom ? Or in other words, such that the variety defined by equations is equal to the variety defined by equation ? EDIT: We can take as the set of all equations in variables and that are entailed by equation . For example is one of the many equations contained in . The question is whether in turn entails . EDIT2: As per Milo Brandt's comment, for any three terms , , containing at most variables , equation is in . Thus, for any algebra in the variety defined by equations , every subalgebra of generated by two elements is associative. So, in a sense, is ""locally associative"".","\cdot \Sigma x y \Sigma (x\cdot y)\cdot z = x\cdot (y\cdot z) \Sigma (x\cdot y)\cdot z = x\cdot (y\cdot z) \Sigma x y (x\cdot y)\cdot z = x\cdot (y\cdot z) (x\cdot y)\cdot x = x\cdot (y\cdot x) \Sigma \Sigma (x\cdot y)\cdot z = x\cdot (y\cdot z) p(x,y) q(x,y) r(x,y) x, y p\cdot (q\cdot r)=(p\cdot q)\cdot r \Sigma A \Sigma A A","['abstract-algebra', 'logic', 'model-theory', 'universal-algebra', 'associativity']"
54,Applications of Abstract Algebra to elementary mathematics,Applications of Abstract Algebra to elementary mathematics,,"I'm currently an undergraduate student in mathematics. I am currently taking Algebra. The course is interesting, but I have grown very curious about the usefulness of algebra. I am NOT asking about ""applications of algebra to real-life"". I am asking about how algebra can be used to solve math problems. Unfortunately, Googling ""applications of algebra"" is not all that helpful. Right now I can only recall seeing two instances of ""useful"" applications of algebra -- a proof of Fermat's Little Theorem, and determining whether a polynomial is solvable in radicals by looking at its Galois group. What interests me about both problems is that they are of interest to someone who has not necessarily encountered abstract algebra yet (e.g. what is the remainder when you divide k^p by p? can you write explicitly the roots of some polynomial using only the integers and the specified functions?). At least from the way my course is currently progressing, it feels as though such applications are few and far in between. We are currently making observations about permutations (e.g. if p and q are permutations, then pq and qp have ""similar forms""), which is interesting, but I fail to see how algebra has helped make any interesting deduction -- all interesting results so far about permutations (e.g. the one mentioned above) were all done without any algebraic result. Only when we ask a question using algebraic terminology was algebra required (e.g. show An is a normal subgroup of Sn). If algebra were only used to answer questions about algebra, there would be no real need to study algebra, right? What are some other ""elementary"" applications of algebra? What are some other interesting results I would be able to understand after an introductory course? I have a suspicion that finding answers to these questions would better my understanding of algebra, but I have had difficulty in finding many good answers.","I'm currently an undergraduate student in mathematics. I am currently taking Algebra. The course is interesting, but I have grown very curious about the usefulness of algebra. I am NOT asking about ""applications of algebra to real-life"". I am asking about how algebra can be used to solve math problems. Unfortunately, Googling ""applications of algebra"" is not all that helpful. Right now I can only recall seeing two instances of ""useful"" applications of algebra -- a proof of Fermat's Little Theorem, and determining whether a polynomial is solvable in radicals by looking at its Galois group. What interests me about both problems is that they are of interest to someone who has not necessarily encountered abstract algebra yet (e.g. what is the remainder when you divide k^p by p? can you write explicitly the roots of some polynomial using only the integers and the specified functions?). At least from the way my course is currently progressing, it feels as though such applications are few and far in between. We are currently making observations about permutations (e.g. if p and q are permutations, then pq and qp have ""similar forms""), which is interesting, but I fail to see how algebra has helped make any interesting deduction -- all interesting results so far about permutations (e.g. the one mentioned above) were all done without any algebraic result. Only when we ask a question using algebraic terminology was algebra required (e.g. show An is a normal subgroup of Sn). If algebra were only used to answer questions about algebra, there would be no real need to study algebra, right? What are some other ""elementary"" applications of algebra? What are some other interesting results I would be able to understand after an introductory course? I have a suspicion that finding answers to these questions would better my understanding of algebra, but I have had difficulty in finding many good answers.",,"['abstract-algebra', 'big-list']"
55,Every normal subgroup is the kernel of some homomorphism,Every normal subgroup is the kernel of some homomorphism,,"Clearly the kernel of a group homomorphism is normal ( proof ), but I often hear my professor mention that any normal subgroup is the kernel of some homomorphism. This feels correct but isn't entirely obvious to me. One thought I had is that for any normal subgroup $N$ of $G$ , we could define the quotient homomorphism $\pi:G\to G/N$ since $G/N$ is a group. I was imagining that we could consider $\pi^{-1}:G/N\to G$ , whose kernel would then be $N$ . However, $\pi^{-1}$ doesn't exist since $\pi$ is not a bijection in general. So my question is this: is there an obvious way to define a homomorphism whose kernel is an arbitrary normal subgroup of $G$ ? Or does it depend on the particular group whether you can define such a homomorphism?","Clearly the kernel of a group homomorphism is normal ( proof ), but I often hear my professor mention that any normal subgroup is the kernel of some homomorphism. This feels correct but isn't entirely obvious to me. One thought I had is that for any normal subgroup of , we could define the quotient homomorphism since is a group. I was imagining that we could consider , whose kernel would then be . However, doesn't exist since is not a bijection in general. So my question is this: is there an obvious way to define a homomorphism whose kernel is an arbitrary normal subgroup of ? Or does it depend on the particular group whether you can define such a homomorphism?",N G \pi:G\to G/N G/N \pi^{-1}:G/N\to G N \pi^{-1} \pi G,"['abstract-algebra', 'group-theory', 'normal-subgroups', 'group-homomorphism']"
56,What's wrong with this proof that commutativity is implied by the other field axioms?,What's wrong with this proof that commutativity is implied by the other field axioms?,,"I seem to have found a proof that the commutativity of $+$ follows from the other field axioms. It is as follows: Let $(k,+,\cdot)$ be a structure satisfying all field axioms except commutativity of addition, with $a,b\in k$. Then 1) $(a + b)\in k$ and $(b+a)\in k$ (closure of addition) 2) $-(a+b)\in k$ and $-(b+a)\in k$ (invertible addition) 3) $(a+b) + [-(b+a)] =(a+b) + (-b) + (-a)$ (distributivity of $\cdot$ over $+$) 4) $(a+b) + (-b) + (-a) = a + (b + (-b)) + (-a)$ (associativity of $+$) 5) $a + (b + (-b)) + (-a) = a + 0 + (-a) = a + (-a)$ (invertibility and identity of $+$) 6) $(a + b) + [-(b+a)] = 0$ (identity of $+$) 7) $(a+b) = (b+a)$ (invertibility of $+$) $\,\square$ But commutativity is a field axiom, so it must be necessary. Given that, what is wrong with this proof? Can the final conclusion (7) not be drawn without commutativity?","I seem to have found a proof that the commutativity of $+$ follows from the other field axioms. It is as follows: Let $(k,+,\cdot)$ be a structure satisfying all field axioms except commutativity of addition, with $a,b\in k$. Then 1) $(a + b)\in k$ and $(b+a)\in k$ (closure of addition) 2) $-(a+b)\in k$ and $-(b+a)\in k$ (invertible addition) 3) $(a+b) + [-(b+a)] =(a+b) + (-b) + (-a)$ (distributivity of $\cdot$ over $+$) 4) $(a+b) + (-b) + (-a) = a + (b + (-b)) + (-a)$ (associativity of $+$) 5) $a + (b + (-b)) + (-a) = a + 0 + (-a) = a + (-a)$ (invertibility and identity of $+$) 6) $(a + b) + [-(b+a)] = 0$ (identity of $+$) 7) $(a+b) = (b+a)$ (invertibility of $+$) $\,\square$ But commutativity is a field axiom, so it must be necessary. Given that, what is wrong with this proof? Can the final conclusion (7) not be drawn without commutativity?",,"['abstract-algebra', 'arithmetic', 'fake-proofs']"
57,What exactly is a coset?,What exactly is a coset?,,"If $G$ is a group and $H$ a subgroup of $G$, then the set $gH = \{gh \mid h \in H\} = \{g, gh_1, gh_2,\ldots\}$ is a ""left coset"" of $G$ wrt $H.$ So does this basically mean, that when I multiply some element of group $H$ on the left by some element from the group $G$, this is a left coset? Doing this multiple times with multiple elements from $G$ and $H$ will give me the set of left cosets? The same for right cosets. Why is this important then? If the cosets have a one to one relation (like in the Orbit Stabiliser theorem), does that imply a bijection? As one element from the group $G$ is mapped to only another element in $H$ and each of these ""outcomes"" are unique?","If $G$ is a group and $H$ a subgroup of $G$, then the set $gH = \{gh \mid h \in H\} = \{g, gh_1, gh_2,\ldots\}$ is a ""left coset"" of $G$ wrt $H.$ So does this basically mean, that when I multiply some element of group $H$ on the left by some element from the group $G$, this is a left coset? Doing this multiple times with multiple elements from $G$ and $H$ will give me the set of left cosets? The same for right cosets. Why is this important then? If the cosets have a one to one relation (like in the Orbit Stabiliser theorem), does that imply a bijection? As one element from the group $G$ is mapped to only another element in $H$ and each of these ""outcomes"" are unique?",,"['abstract-algebra', 'group-theory']"
58,Applications of Gröbner bases,Applications of Gröbner bases,,I would like to present an application of Gröbner bases.  The audience is a class of first year graduate students who are taking first year algebra. Does anyone have suggestions on a specific application that the audience would appreciate?,I would like to present an application of Gröbner bases.  The audience is a class of first year graduate students who are taking first year algebra. Does anyone have suggestions on a specific application that the audience would appreciate?,,"['abstract-algebra', 'applications', 'groebner-basis']"
59,Can we permute the coefficients of a polynomial so that it has NO real roots?,Can we permute the coefficients of a polynomial so that it has NO real roots?,,Let $P(x)=a_{2n}x^{2n}+a_{2n-1}x^{2n-1}+\ldots+a_{0}$ be an even degree polynomial with positive coefficients. Is it possible to permute the coefficients of $P(x)$ so that the resulting polynomial will have NO real roots.,Let $P(x)=a_{2n}x^{2n}+a_{2n-1}x^{2n-1}+\ldots+a_{0}$ be an even degree polynomial with positive coefficients. Is it possible to permute the coefficients of $P(x)$ so that the resulting polynomial will have NO real roots.,,"['abstract-algebra', 'polynomials', 'roots']"
60,Computing the Galois group of polynomials $x^n-a \in \mathbb{Q}[x]$,Computing the Galois group of polynomials,x^n-a \in \mathbb{Q}[x],"I have some problems with this exercise. I don't know if it can be done. Consider the polynomial $ x^n - a \in \mathbb{Q}[x]$ . Can I compute the Galois group of this over $\mathbb{Q}$ ? Maybe having a nice basis. The splitting field is given by $\mathbb{Q}(\zeta_n,\alpha)$ , where $\zeta_n$ is a primitive root of unity, and $\alpha$ is some number such that $\alpha^n = a $ . First of all, I want a good basis for the splitting field. In the sense that the minimal polynomials of the adjoined elements are different (in this case the computation of the Galois group is very simple). For example, an easy case it's when $a>0$ , then $a^{\frac{1}{n}} \in \mathbb{R}$ , so clearly the minimal polynomials of $a^{\frac{1}{n}}$ , respectively $\zeta_n$ are distinct, and I'm done. If $n$ is odd, then it's also easy, since one root it's also real, for example $x^3+3,a=-3$ , the real root is $ \root 3 \of { - 3}  =  - \root 3 \of 3  $ , so I can consider the splitting field as $\mathbb{Q}(-\root 3 \of 3 , \zeta_3 )=\mathbb{Q}(\root 3 \of 3 , \zeta_3 )$ . The difficult case is when $n$ is even and $a<0$ , for example $x^8+20$ or $x^4+20$ . In some cases, as in the second, there are particular cases since there exist algorithms for the Galois group of quartics, but in general it can be done? Remark. I'm searching a good basis for the splitting field. In the sense that the minimal polynomials of the adjoined elements, are different, since in this case the computation of the Galois group is very simple.","I have some problems with this exercise. I don't know if it can be done. Consider the polynomial . Can I compute the Galois group of this over ? Maybe having a nice basis. The splitting field is given by , where is a primitive root of unity, and is some number such that . First of all, I want a good basis for the splitting field. In the sense that the minimal polynomials of the adjoined elements are different (in this case the computation of the Galois group is very simple). For example, an easy case it's when , then , so clearly the minimal polynomials of , respectively are distinct, and I'm done. If is odd, then it's also easy, since one root it's also real, for example , the real root is , so I can consider the splitting field as . The difficult case is when is even and , for example or . In some cases, as in the second, there are particular cases since there exist algorithms for the Galois group of quartics, but in general it can be done? Remark. I'm searching a good basis for the splitting field. In the sense that the minimal polynomials of the adjoined elements, are different, since in this case the computation of the Galois group is very simple."," x^n - a \in \mathbb{Q}[x] \mathbb{Q} \mathbb{Q}(\zeta_n,\alpha) \zeta_n \alpha \alpha^n = a  a>0 a^{\frac{1}{n}} \in \mathbb{R} a^{\frac{1}{n}} \zeta_n n x^3+3,a=-3 
\root 3 \of { - 3}  =  - \root 3 \of 3 
 \mathbb{Q}(-\root 3 \of 3 , \zeta_3 )=\mathbb{Q}(\root 3 \of 3 , \zeta_3 ) n a<0 x^8+20 x^4+20","['abstract-algebra', 'field-theory', 'galois-theory']"
61,Commutator subgroup of rank-2 free group is not finitely generated.,Commutator subgroup of rank-2 free group is not finitely generated.,,"I'm having trouble with this exercise: Let $G$ be the free group generated by $a$ and $b$. Prove that the commutator subgroup $G'$ is not finitely generated. I found a suggestion that says to prove $G'$ is generated by the collection $\{[x^m,y^n]\mid m,n\in\mathbb{Z}\}$. I don't know how to prove this, and how it helps. If you could please provide me with some hints... Thanks!","I'm having trouble with this exercise: Let $G$ be the free group generated by $a$ and $b$. Prove that the commutator subgroup $G'$ is not finitely generated. I found a suggestion that says to prove $G'$ is generated by the collection $\{[x^m,y^n]\mid m,n\in\mathbb{Z}\}$. I don't know how to prove this, and how it helps. If you could please provide me with some hints... Thanks!",,"['abstract-algebra', 'group-theory', 'free-groups']"
62,"What is the relation between semidirect products, extensions, and split extensions?","What is the relation between semidirect products, extensions, and split extensions?",,"When I read the textbook about semidirect products and split extensions, I feel like I'm lacking the intuition behind them and their relations. I was wondering if someone could briefly explain such relations to me. Specifically, this is what I'm struggling with: Let α : K → Aut(H) be a homomorphism. By the semidirect product of H and K with respect to α, written $H \rtimes_α K$ , we mean the set H×K with the binary operation given by setting $(h_1, k_1) · (h_2, k_2) = (h_1 · α(k_1)(h_2), k_1k_2)$ . For a splitting, it says: A section, or splitting, for f : G → K is a homomorphism s : K → G, such that f ◦ s is the identity map of K. A homomorphism f : G → K that admits a section is said to be a split surjection. An extension of H by K is called a split extension if f : G → K admits a section. The section s is said to split f. For an extension, it says: Let H and K be groups. An extension of H by K consists of a group G containing H as a normal subgroup, together with a surjective homomorphism f : G → K with kernel H. Also, in a proposition, it says that split extensions are semidirect products. But I'm not sure how an extension can ""fit"" the definition of a semidirect product. Also, I think I have trouble understanding the relationship between these two when it comes to classifying groups. For semidirect products, I understand that semidirect products are not unique, so classifying them means looking at the possibilities of what groups a semidirect product might represent up to isomorphism. Is that correct? But what do extensions have to do with this?","When I read the textbook about semidirect products and split extensions, I feel like I'm lacking the intuition behind them and their relations. I was wondering if someone could briefly explain such relations to me. Specifically, this is what I'm struggling with: Let α : K → Aut(H) be a homomorphism. By the semidirect product of H and K with respect to α, written , we mean the set H×K with the binary operation given by setting . For a splitting, it says: A section, or splitting, for f : G → K is a homomorphism s : K → G, such that f ◦ s is the identity map of K. A homomorphism f : G → K that admits a section is said to be a split surjection. An extension of H by K is called a split extension if f : G → K admits a section. The section s is said to split f. For an extension, it says: Let H and K be groups. An extension of H by K consists of a group G containing H as a normal subgroup, together with a surjective homomorphism f : G → K with kernel H. Also, in a proposition, it says that split extensions are semidirect products. But I'm not sure how an extension can ""fit"" the definition of a semidirect product. Also, I think I have trouble understanding the relationship between these two when it comes to classifying groups. For semidirect products, I understand that semidirect products are not unique, so classifying them means looking at the possibilities of what groups a semidirect product might represent up to isomorphism. Is that correct? But what do extensions have to do with this?","H \rtimes_α K (h_1, k_1) · (h_2, k_2) = (h_1 · α(k_1)(h_2), k_1k_2)","['abstract-algebra', 'group-theory']"
63,"If a group is the union of two subgroups, is one subgroup the group itself?","If a group is the union of two subgroups, is one subgroup the group itself?",,"""Let $G$ be a group, and suppose $G=H \cup K$, where $H$ and $K$ are subgroups.  Show that either $H=G$ or $K=G$."" Let $h \in H$ and $k \in K$.  Then $hk \in H$ or $hk \in K$ (since every element of $G$ is in either $H$ or $K$).  If $hk=h'$ for some $h' \in H$, then $k=h^{-1}h'$, so $k \in H$.  If $hk=k'$ for some $k' \in K$, then $h=k'k^{-1}$ so that $h \in K$. If for all $h \in H$ we have $h \in K$, or if for all $k \in K$ we have $k \in H$, then $H \subseteq K$ or $K \subseteq H$.  Then since $G=H \cup K$, we must have either $H=G$ or $K=G$. I'm not sure if the first paragraph of my 'proof' implies the second.  I've shown that for arbitrary $h \in H$, $h \in H$ and possibly $h \in K$, and similar for $k \in K$.  I don't know how to wrap it up (or perhaps this route won't lead anywhere at all). If this way won't work, I'd just like a hint on a new direction to take. Thanks.","""Let $G$ be a group, and suppose $G=H \cup K$, where $H$ and $K$ are subgroups.  Show that either $H=G$ or $K=G$."" Let $h \in H$ and $k \in K$.  Then $hk \in H$ or $hk \in K$ (since every element of $G$ is in either $H$ or $K$).  If $hk=h'$ for some $h' \in H$, then $k=h^{-1}h'$, so $k \in H$.  If $hk=k'$ for some $k' \in K$, then $h=k'k^{-1}$ so that $h \in K$. If for all $h \in H$ we have $h \in K$, or if for all $k \in K$ we have $k \in H$, then $H \subseteq K$ or $K \subseteq H$.  Then since $G=H \cup K$, we must have either $H=G$ or $K=G$. I'm not sure if the first paragraph of my 'proof' implies the second.  I've shown that for arbitrary $h \in H$, $h \in H$ and possibly $h \in K$, and similar for $k \in K$.  I don't know how to wrap it up (or perhaps this route won't lead anywhere at all). If this way won't work, I'd just like a hint on a new direction to take. Thanks.",,"['abstract-algebra', 'group-theory']"
64,Proving that a polynomial is not solvable by radicals.,Proving that a polynomial is not solvable by radicals.,,"I'm trying to prove that the following polynomial is not solvable by radicals: $$p(x) = x^5 - 4x + 2 $$ First, by Eisenstein is irreducible. (It is not difficult to see that this polynomial has exactly 3 real roots) How can I proceed? Thank you!","I'm trying to prove that the following polynomial is not solvable by radicals: $$p(x) = x^5 - 4x + 2 $$ First, by Eisenstein is irreducible. (It is not difficult to see that this polynomial has exactly 3 real roots) How can I proceed? Thank you!",,"['abstract-algebra', 'polynomials', 'field-theory', 'galois-theory']"
65,Problems from the Kourovka Notebook that undergraduate students can fully appreciate,Problems from the Kourovka Notebook that undergraduate students can fully appreciate,,"The Kourovka Notebook is a collection of open problems in Group   Theory. My question is: could you point out some (a ""big-list"" of) problems [by referencing them] presented in this book that are, in principle, accessible to undegraduate students: i.e. , problems that refer to (and possibly might be solved by applying) definitions, concepts, and theorems that are presented in a book like Herstein's Topics in Algebra (and then, by extension, in an abstract algebra course for undergraduates). The aim of this question is to allow undergraduate students to have a better understanding of current research in algebra by letting them see concretely open problems that can be easily related to known concepts.","The Kourovka Notebook is a collection of open problems in Group   Theory. My question is: could you point out some (a ""big-list"" of) problems [by referencing them] presented in this book that are, in principle, accessible to undegraduate students: i.e. , problems that refer to (and possibly might be solved by applying) definitions, concepts, and theorems that are presented in a book like Herstein's Topics in Algebra (and then, by extension, in an abstract algebra course for undergraduates). The aim of this question is to allow undergraduate students to have a better understanding of current research in algebra by letting them see concretely open problems that can be easily related to known concepts.",,"['abstract-algebra', 'group-theory', 'soft-question', 'big-list']"
66,Show that every ideal of the matrix ring $M_n(R)$ is of the form $M_n(I)$ where $I$ is an ideal of $R$ [duplicate],Show that every ideal of the matrix ring  is of the form  where  is an ideal of  [duplicate],M_n(R) M_n(I) I R,This question already has answers here : What are the left and right ideals of matrix ring? How about the two sided ideals? (2 answers) Closed 11 years ago . Suppose $R$ is a commutative ring. Show that every ideal of $M_n(R)$ is of the form $M_n(I)$  where $I$ is an ideal of $R$. I have spent 30 minutes on this question and I still got nowhere. Can anyone give some hints ?,This question already has answers here : What are the left and right ideals of matrix ring? How about the two sided ideals? (2 answers) Closed 11 years ago . Suppose $R$ is a commutative ring. Show that every ideal of $M_n(R)$ is of the form $M_n(I)$  where $I$ is an ideal of $R$. I have spent 30 minutes on this question and I still got nowhere. Can anyone give some hints ?,,"['abstract-algebra', 'matrices', 'ring-theory']"
67,Structure of Finite Commutative Rings,Structure of Finite Commutative Rings,,Is every finite commutative ring $A$ a direct product of finite algebras over $\mathbb Z/p^n$?,Is every finite commutative ring $A$ a direct product of finite algebras over $\mathbb Z/p^n$?,,"['abstract-algebra', 'commutative-algebra', 'ring-theory', 'finite-rings']"
68,Why is the set of commutators not a subgroup?,Why is the set of commutators not a subgroup?,,"I was surprised to see that one talks about the subgroup generated by the commutators, because I thought the commutators would form a subgroup. Some research told me that it's because commutators are not necessarily closed under product (books by Rotman and Mac Lane popped up in a google search telling me). However, I couldn't find an actual example of this. What is one? The books on google books made it seem like an actual example is hard to explain. Wikipedia did mention that the product $[a,b][c,d]$ on the free group on $a,b,c,d$ is an example. But why? I know this product is $aba^{-1}b^{-1}cdc^{-1}d^{-1}$, but why is that not a commutator in this group?","I was surprised to see that one talks about the subgroup generated by the commutators, because I thought the commutators would form a subgroup. Some research told me that it's because commutators are not necessarily closed under product (books by Rotman and Mac Lane popped up in a google search telling me). However, I couldn't find an actual example of this. What is one? The books on google books made it seem like an actual example is hard to explain. Wikipedia did mention that the product $[a,b][c,d]$ on the free group on $a,b,c,d$ is an example. But why? I know this product is $aba^{-1}b^{-1}cdc^{-1}d^{-1}$, but why is that not a commutator in this group?",,"['abstract-algebra', 'group-theory']"
69,What is the purpose of K-Theory?,What is the purpose of K-Theory?,,"I have recognized that there is a theory called K-Theory in mathematics is used also for applications in mathematical physics. There is existing algebraic   K-Theory and topological K-Theory. Are these theories very similar? For algebraic K-Theory by Milnor I have seen that the K-Groups are given by $K_n = T^n / a \otimes (1-a)$ (Wikipedia). Here, $T^n$ is the $n$-fold Tensor product. For n=2 one obtains abelian matrices. I don't understand this theory in depht. What is the reason that K- theory was introduced? (Is the theoretical physics application topological or algebraic?) And is there material (lecture Video or good pdf script) where the algebraic K-theory is explained? I would greatly appreciate an answer.","I have recognized that there is a theory called K-Theory in mathematics is used also for applications in mathematical physics. There is existing algebraic   K-Theory and topological K-Theory. Are these theories very similar? For algebraic K-Theory by Milnor I have seen that the K-Groups are given by $K_n = T^n / a \otimes (1-a)$ (Wikipedia). Here, $T^n$ is the $n$-fold Tensor product. For n=2 one obtains abelian matrices. I don't understand this theory in depht. What is the reason that K- theory was introduced? (Is the theoretical physics application topological or algebraic?) And is there material (lecture Video or good pdf script) where the algebraic K-theory is explained? I would greatly appreciate an answer.",,"['abstract-algebra', 'applications', 'k-theory', 'algebraic-k-theory']"
70,"A group such that $a^m b^m = b^m a^m$ and $a^n b^n = b^n a^n$ ($m$, $n$ coprime) is abelian?","A group such that  and  (,  coprime) is abelian?",a^m b^m = b^m a^m a^n b^n = b^n a^n m n,"Let $(G,.)$ be a group and $m,n \in\mathbb Z$ such that $\gcd(m,n)=1$. Assume that $$ \forall a,b \in G, \,a^mb^m=b^ma^m,$$ $$\forall a,b \in G, \, a^nb^n=b^na^n.$$ Then how prove $G$ is an abelian group? Some context : Some of these commutation relations often imply that $G$ is abelian, for example if $(ab)^i = a^i b^i$ for three consecutive integers $i$ then $G$ is abelian , or if $g^2 = e$ for all $g$ then $G$ is abelian . This looks like another example of this phenomenon, but the same techniques do not apply.","Let $(G,.)$ be a group and $m,n \in\mathbb Z$ such that $\gcd(m,n)=1$. Assume that $$ \forall a,b \in G, \,a^mb^m=b^ma^m,$$ $$\forall a,b \in G, \, a^nb^n=b^na^n.$$ Then how prove $G$ is an abelian group? Some context : Some of these commutation relations often imply that $G$ is abelian, for example if $(ab)^i = a^i b^i$ for three consecutive integers $i$ then $G$ is abelian , or if $g^2 = e$ for all $g$ then $G$ is abelian . This looks like another example of this phenomenon, but the same techniques do not apply.",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
71,A subgroup such that every left coset is contained in a right coset.,A subgroup such that every left coset is contained in a right coset.,,"Let $G$ be any group, and $H \leq G$ a subgroup. Suppose that for each $x \in G$, there exists a $y \in G$ such that $xH \subseteq Hy$. In other words, every left coset of $H$ is contained inside some right coset of $H$. Question : what can we say about $H$? In particular, does this imply that $H$ is normal? I know that if $G$ is finite, then $H$ must be normal, because then $xH \subseteq Hy$ implies $xH = Hy$, since $|xH| = |Hy|$.","Let $G$ be any group, and $H \leq G$ a subgroup. Suppose that for each $x \in G$, there exists a $y \in G$ such that $xH \subseteq Hy$. In other words, every left coset of $H$ is contained inside some right coset of $H$. Question : what can we say about $H$? In particular, does this imply that $H$ is normal? I know that if $G$ is finite, then $H$ must be normal, because then $xH \subseteq Hy$ implies $xH = Hy$, since $|xH| = |Hy|$.",,"['abstract-algebra', 'group-theory']"
72,Is the ring and semi-ring definition of algebra and set linked?,Is the ring and semi-ring definition of algebra and set linked?,,"""Ring"" and ""semiring"" are concepts defined both in algebra and set theory. In Algebra A ring in algebra is a set R equipped with two binary operations + and · called addition and multiplication, that Addition (+) is abelian, Multiplication (⋅) is associative, Multiplication distributes over addition, and Multiplicative identity (1) exists. A semiring in abstract algebra ,  is an algebraic structure similar to a ring, but without the requirement that each element must have an additive inverse. In Set theory A ring of sets in measure theory is a family of sets closed under unions and set-theoretic differences.  That is, it obeys the two properties $$A \setminus B \in \mathcal{R} $$ $$A \cup B \in \mathcal{R}$$ This implies that it is also closed under intersections, $$A \cap B \in \mathcal{R}$$ A semiring of sets is a non-empty collection S of sets such that $\emptyset \in S$ If $E \in S$ and $F \in S$ then $E \cap F \in S$. If $E \in S$ and $F \in S$ then there exists a finite number of mutually disjoint sets $C_i \in S$ for $i=1,\ldots,n$ such that $E    \setminus F = \bigcup_{i=1}^n C_i$. I wonder, are such definitions somehow linked or equivalent? or they are just coincidence that both fields of mathematics used the same terms?","""Ring"" and ""semiring"" are concepts defined both in algebra and set theory. In Algebra A ring in algebra is a set R equipped with two binary operations + and · called addition and multiplication, that Addition (+) is abelian, Multiplication (⋅) is associative, Multiplication distributes over addition, and Multiplicative identity (1) exists. A semiring in abstract algebra ,  is an algebraic structure similar to a ring, but without the requirement that each element must have an additive inverse. In Set theory A ring of sets in measure theory is a family of sets closed under unions and set-theoretic differences.  That is, it obeys the two properties $$A \setminus B \in \mathcal{R} $$ $$A \cup B \in \mathcal{R}$$ This implies that it is also closed under intersections, $$A \cap B \in \mathcal{R}$$ A semiring of sets is a non-empty collection S of sets such that $\emptyset \in S$ If $E \in S$ and $F \in S$ then $E \cap F \in S$. If $E \in S$ and $F \in S$ then there exists a finite number of mutually disjoint sets $C_i \in S$ for $i=1,\ldots,n$ such that $E    \setminus F = \bigcup_{i=1}^n C_i$. I wonder, are such definitions somehow linked or equivalent? or they are just coincidence that both fields of mathematics used the same terms?",,['abstract-algebra']
73,Why are modules called modules?,Why are modules called modules?,,"I know that a module is a generalization of a vector space, but I would like to know why are modules called modules? Thanks for your kindly help.","I know that a module is a generalization of a vector space, but I would like to know why are modules called modules? Thanks for your kindly help.",,['abstract-algebra']
74,"Let G be a nonabelian group of order $p^3$, where $p$ is a prime number. Prove that the center of $G$ is of order $p$.","Let G be a nonabelian group of order , where  is a prime number. Prove that the center of  is of order .",p^3 p G p,"Let G be a nonabelian group of order $p^3$, where $p$ is a prime number. Prove that the center of $G$ is of order $p$. Proof Since $G$ is not abelian, the order of its center cannot be $p^3$. Since it is a $p$-group, the center cannot be trivial. So the order of $Z(G)$ is either $p^2$ or $p$. Suppose, for contradiction, that $Z(G) = p^2$. Since $p$ is prime, we can assume that a subgroup $H= \langle p \rangle$ of order $p$ exists in $G$. We can also assume that $H$ and $Z(G)$ are disjoint. Otherwise, if there didn't exist a disjoint subgroup of order $p$, then the order of $G$ would be $p^2$. Since $Z(G)$ is the center, they commute with p. Since they commute with $p$, they must also commute with all powers of $p$. So $G = Z(G) \times H \implies$ G is abelian since $H$ and $Z(G)$ are abelian. So $|Z(G)|=p$. Do you think my answer is correct? Thanks in advance","Let G be a nonabelian group of order $p^3$, where $p$ is a prime number. Prove that the center of $G$ is of order $p$. Proof Since $G$ is not abelian, the order of its center cannot be $p^3$. Since it is a $p$-group, the center cannot be trivial. So the order of $Z(G)$ is either $p^2$ or $p$. Suppose, for contradiction, that $Z(G) = p^2$. Since $p$ is prime, we can assume that a subgroup $H= \langle p \rangle$ of order $p$ exists in $G$. We can also assume that $H$ and $Z(G)$ are disjoint. Otherwise, if there didn't exist a disjoint subgroup of order $p$, then the order of $G$ would be $p^2$. Since $Z(G)$ is the center, they commute with p. Since they commute with $p$, they must also commute with all powers of $p$. So $G = Z(G) \times H \implies$ G is abelian since $H$ and $Z(G)$ are abelian. So $|Z(G)|=p$. Do you think my answer is correct? Thanks in advance",,"['abstract-algebra', 'group-theory']"
75,Ideal contained in a finite union of prime ideals,Ideal contained in a finite union of prime ideals,,"Let $I \subset R$ be an ideal and $P_i$  $(i=\{1,...,n\})$ prime ideals with $I\subseteq\bigcup_{i=1}^nP_i$. Prove that then $I$ is contained in one $P_i$. I don't know how to show this because I don't have any approach. So I am looking for something to start with or something like a sequence of tips (Since I know this is a large proof) Thanks in advance!","Let $I \subset R$ be an ideal and $P_i$  $(i=\{1,...,n\})$ prime ideals with $I\subseteq\bigcup_{i=1}^nP_i$. Prove that then $I$ is contained in one $P_i$. I don't know how to show this because I don't have any approach. So I am looking for something to start with or something like a sequence of tips (Since I know this is a large proof) Thanks in advance!",,"['abstract-algebra', 'ideals']"
76,If $F[x]$ is a principal domain does $F$ have to be necessarily a field? [duplicate],If  is a principal domain does  have to be necessarily a field? [duplicate],F[x] F,"This question already has answers here : Let $R$ be a commutative ring. If $R[X]$ is a principal ideal domain, then $R$ is a field. [duplicate] (5 answers) Closed 3 years ago . If $F$ is a field, then $F[x]$ is a principal ideal domain. Conversely, if $F[x]$ is a principal domain does $F$ have to be necessarily a field? My Thoughts: Suppose instead of $F$ , we take the set of polynomials $R[x]$ over a commutative ring $R$ with unity. Then, suppose $I$ is an ideal of $R[x]$ . Let $g(x) \in I$ such that $g(x)$ is the polynomial of the lowest degree in $I$ . Then: $ \langle g(x) \rangle \subseteq I .......... (1)$ Let $f(x) \in I$ . Then $f(x) = p(x)g(x) + r(x)~~|~~p(x),r(x) \in R[x], \deg r(x) < \deg g(x)$ Since, $I$ is an ideal $\implies f(x) - p(x)g(x) = r(x) \in I$ But, $g(x)$ is of the lowest degree in $I \implies r(x) = 0 \implies f(x) \in \langle g(x)  \rangle \implies I \subseteq \langle g(x)  \rangle ......(2)$ Then from $(1),(2) : I = \langle g(x) \rangle$ Does the Presence of zero divisors in $R[x]$ really make a difference? The only advantage I see is that if there are no zero divisors in $R[x]$ then $I=\{0\} \implies I= \langle 0  \rangle$ . But, every ideal contains the zero element. Why is there the condition of a field specifically given in textbooks for $F[x]$ to be a principal ideal domain? Thank you for your help.","This question already has answers here : Let $R$ be a commutative ring. If $R[X]$ is a principal ideal domain, then $R$ is a field. [duplicate] (5 answers) Closed 3 years ago . If is a field, then is a principal ideal domain. Conversely, if is a principal domain does have to be necessarily a field? My Thoughts: Suppose instead of , we take the set of polynomials over a commutative ring with unity. Then, suppose is an ideal of . Let such that is the polynomial of the lowest degree in . Then: Let . Then Since, is an ideal But, is of the lowest degree in Then from Does the Presence of zero divisors in really make a difference? The only advantage I see is that if there are no zero divisors in then . But, every ideal contains the zero element. Why is there the condition of a field specifically given in textbooks for to be a principal ideal domain? Thank you for your help.","F F[x] F[x] F F R[x] R I R[x] g(x) \in I g(x) I  \langle g(x) \rangle \subseteq I .......... (1) f(x) \in I f(x) = p(x)g(x) + r(x)~~|~~p(x),r(x) \in R[x], \deg r(x) < \deg g(x) I \implies f(x) - p(x)g(x) = r(x) \in I g(x) I \implies r(x) = 0 \implies f(x) \in \langle g(x)  \rangle \implies I \subseteq \langle g(x)  \rangle ......(2) (1),(2) : I = \langle g(x) \rangle R[x] R[x] I=\{0\} \implies I= \langle 0  \rangle F[x]","['abstract-algebra', 'polynomials', 'ring-theory']"
77,Is the quotient ring of a PID a PID?,Is the quotient ring of a PID a PID?,,"Let $A$ be a commutative ring and $S$ a multiplicative closed subset of $A$. If $A$ is a PID, show that $S^{-1}A$ is a PID. I've taken an ideal $I$ of $S^{-1}A$ and I've tried to see that is generated by one element; the ideal $I$ has the form $S^{-1}J$ with $J$ an ideal of $A$. $J$ is generated by one element but I can't see why $I$ has to be generated by one element, maybe I'm wrong.","Let $A$ be a commutative ring and $S$ a multiplicative closed subset of $A$. If $A$ is a PID, show that $S^{-1}A$ is a PID. I've taken an ideal $I$ of $S^{-1}A$ and I've tried to see that is generated by one element; the ideal $I$ has the form $S^{-1}J$ with $J$ an ideal of $A$. $J$ is generated by one element but I can't see why $I$ has to be generated by one element, maybe I'm wrong.",,"['abstract-algebra', 'commutative-algebra']"
78,Tensor products of p-adic integers,Tensor products of p-adic integers,,"These are relatively simple questions, but I can't seem to find anything on tensor products and p-adic integers/numbers anywhere, so I thought I'd ask. My first question is: given some $\mathbb{Z}_p$, $\mathbb{Q}_p$ can be constructed as its field of fractions. Is the tensor product $\mathbb{Z}_p \otimes \mathbb{Q} $ also equal to $\mathbb{Q}_p$? My second question is: given some composite p, you can still construct a ring (now with zero divisors) that can be constructed as the inverse limit of $\mathbb{Z}/p^n\mathbb{Z}$. Any such p has a finite prime factorization, so is there a way to construct this non-integral-domain ring of composite p-adics by taking some sort of product of the rings of p-adics of its prime factors? For instance, can you construct the 10-adics by taking the direct product of the 2-adics and the 5-adics, or perhaps is it the tensor product, or...? My last question is: I haven't seen much about taking tensor products about p-adic integers in general; I've only seen stuff about taking direct products, as in the case of the profinite completion $\hat{\mathbb{Z}}$ of the integers. However, I find the tensor product to be of particular interest, since it's a coproduct in the category of commutative rings. So what, in general, do you get if you take the tensor product of two rings of p-adic integers? And I'm especially curious to know, what do you get if you take the tensor product of all of the rings of p-adic integers, rather than the direct product in the case of $\hat{\mathbb{Z}}$?","These are relatively simple questions, but I can't seem to find anything on tensor products and p-adic integers/numbers anywhere, so I thought I'd ask. My first question is: given some $\mathbb{Z}_p$, $\mathbb{Q}_p$ can be constructed as its field of fractions. Is the tensor product $\mathbb{Z}_p \otimes \mathbb{Q} $ also equal to $\mathbb{Q}_p$? My second question is: given some composite p, you can still construct a ring (now with zero divisors) that can be constructed as the inverse limit of $\mathbb{Z}/p^n\mathbb{Z}$. Any such p has a finite prime factorization, so is there a way to construct this non-integral-domain ring of composite p-adics by taking some sort of product of the rings of p-adics of its prime factors? For instance, can you construct the 10-adics by taking the direct product of the 2-adics and the 5-adics, or perhaps is it the tensor product, or...? My last question is: I haven't seen much about taking tensor products about p-adic integers in general; I've only seen stuff about taking direct products, as in the case of the profinite completion $\hat{\mathbb{Z}}$ of the integers. However, I find the tensor product to be of particular interest, since it's a coproduct in the category of commutative rings. So what, in general, do you get if you take the tensor product of two rings of p-adic integers? And I'm especially curious to know, what do you get if you take the tensor product of all of the rings of p-adic integers, rather than the direct product in the case of $\hat{\mathbb{Z}}$?",,"['abstract-algebra', 'ring-theory', 'tensor-products', 'p-adic-number-theory']"
79,Examples of fields which are not perfect,Examples of fields which are not perfect,,We know that all finite fields are perfect (fields with char $p$). Also fields with char 0 (infinite fields) are perfect. Then what are the fields that are not perfect?,We know that all finite fields are perfect (fields with char $p$). Also fields with char 0 (infinite fields) are perfect. Then what are the fields that are not perfect?,,"['abstract-algebra', 'field-theory']"
80,Is there a proper subfield $K\subset \mathbb R$ such that $[\mathbb R:K]$ is finite?,Is there a proper subfield  such that  is finite?,K\subset \mathbb R [\mathbb R:K],"Is there a proper subfield $K\subset \mathbb R$ such that $[\mathbb R:K]$ is finite? Here $[\mathbb R:K]$ means the dimension of $\mathbb R$ as a $K$-vector space. What I have tried: If we can find a finite subgroup $G\subset Gal (\mathbb C/\mathbb Q)$ such that $G$ contains the complex conjugation, it will be done by letting $K$ be the fixed field of $G$. But I don't know whether such a group exists. Maybe we can start with finding a suitable subgroup of $Gal(\bar{\mathbb Q}/\mathbb Q)$ and then lift it to $Gal(\mathbb C/\mathbb Q)$, where $\bar{\mathbb Q}$ denotes the algebraic closure of $\mathbb Q$. By isomorphism extension theorem, we can find many automorphisms of $\mathbb C$, none of them carries $\mathbb R$ to itself except for the identity. This is because $Gal(\mathbb R/\mathbb Q)$ is the trivial group. For example, now suppose $\{x_\alpha\}\subset \mathbb R$ is a transcendence basis over $\mathbb Q$. Let $\sigma$ be a permutation of $\{x_\alpha\}$, then by the isomorphism extension theorem, $\sigma$ extends to an automorphism of $\mathbb C$, which we still denote by $\sigma$. Then $L=\sigma(\mathbb R)$ is a copy of $\mathbb R$ and $\mathbb R$ is algebraic over $K=\mathbb R\cap L$. How large can $K$ be? Is it possible that $[\mathbb R:K]$ is finite? Does anyone has some ideas? Thanks!","Is there a proper subfield $K\subset \mathbb R$ such that $[\mathbb R:K]$ is finite? Here $[\mathbb R:K]$ means the dimension of $\mathbb R$ as a $K$-vector space. What I have tried: If we can find a finite subgroup $G\subset Gal (\mathbb C/\mathbb Q)$ such that $G$ contains the complex conjugation, it will be done by letting $K$ be the fixed field of $G$. But I don't know whether such a group exists. Maybe we can start with finding a suitable subgroup of $Gal(\bar{\mathbb Q}/\mathbb Q)$ and then lift it to $Gal(\mathbb C/\mathbb Q)$, where $\bar{\mathbb Q}$ denotes the algebraic closure of $\mathbb Q$. By isomorphism extension theorem, we can find many automorphisms of $\mathbb C$, none of them carries $\mathbb R$ to itself except for the identity. This is because $Gal(\mathbb R/\mathbb Q)$ is the trivial group. For example, now suppose $\{x_\alpha\}\subset \mathbb R$ is a transcendence basis over $\mathbb Q$. Let $\sigma$ be a permutation of $\{x_\alpha\}$, then by the isomorphism extension theorem, $\sigma$ extends to an automorphism of $\mathbb C$, which we still denote by $\sigma$. Then $L=\sigma(\mathbb R)$ is a copy of $\mathbb R$ and $\mathbb R$ is algebraic over $K=\mathbb R\cap L$. How large can $K$ be? Is it possible that $[\mathbb R:K]$ is finite? Does anyone has some ideas? Thanks!",,"['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
81,Quotient ring of Gaussian integers $\mathbb{Z}[i]/(a+bi)$ when $a$ and $b$ are NOT coprime,Quotient ring of Gaussian integers  when  and  are NOT coprime,\mathbb{Z}[i]/(a+bi) a b,"The isomorphism $\mathbb{Z}[i]/(a+bi) \cong \Bbb Z/(a^2+b^2)\Bbb Z$ is well-known , when the integers $a$ and $b$ are coprime. But what happens when they are not coprime, say $(a,b)=d>1$? — For instance if $p$ is prime (which is not coprime with $0$) then $$\mathbb{Z}[i]/(p) \cong \mathbb{F}_p[X]/(X^2+1) \cong \begin{cases} \mathbb{F}_{p^2} &\text{if } p \equiv 3 \pmod 4\\ \mathbb{F}_{p} \times \mathbb{F}_{p} &\text{if } p \equiv 1 \pmod 4 \end{cases}$$ (because $-1$ is a square mod $p$ iff $(-1)^{(p-1)/2}=1$). — More generally, if $n=p_1^{r_1} \cdots p_m^{r_m} \in \Bbb N$, then  each pair of integers $p_j^{r_j}$ are coprime, so that by CRT we get $$\mathbb{Z}[i]/(n) \cong \mathbb{Z}[i]/(p_1^{r_1}) \times \cdots \times \mathbb{Z}[i]/(p_m^{r_m})$$ I was not sure how to find the structure of $\mathbb{Z}[i]/(p^{r}) \cong (\Bbb Z/p^r \Bbb Z)[X] \,/\, (X^2+1)$ when $p$ is prime and $r>1$. — Even more generally, in order to determine the structure of $\mathbb{Z}[i]/(a+bi)$ with $a+bi=d(x+iy)$ and $(x,y)=1$, we could try to use the CRT, provided that $d$ is coprime with $x+iy$ in $\Bbb Z[i]$. But this is not always true: for $d=13$ and $x+iy=2+3i$, we can't find Gauss integers $u$ and $v$ such that $du + (x+iy)v=1$, because this would mean that $(2+3i)[(2-3i)u+v]=1$, i.e. $2+3i$ is a unit in $\Bbb Z[i]$ which is not because its norm is $13 \neq ±1$. — I was not able to go further. I recall that my general question is to known what $\mathbb{Z}[i]/(a+bi)$ is isomorphic to , when $a$ and $b$ are integers which are not coprime (for instance $a=p^r,b=0$ or $d=(a,b) = a^2+b^2>1$). Thank you for your help!","The isomorphism $\mathbb{Z}[i]/(a+bi) \cong \Bbb Z/(a^2+b^2)\Bbb Z$ is well-known , when the integers $a$ and $b$ are coprime. But what happens when they are not coprime, say $(a,b)=d>1$? — For instance if $p$ is prime (which is not coprime with $0$) then $$\mathbb{Z}[i]/(p) \cong \mathbb{F}_p[X]/(X^2+1) \cong \begin{cases} \mathbb{F}_{p^2} &\text{if } p \equiv 3 \pmod 4\\ \mathbb{F}_{p} \times \mathbb{F}_{p} &\text{if } p \equiv 1 \pmod 4 \end{cases}$$ (because $-1$ is a square mod $p$ iff $(-1)^{(p-1)/2}=1$). — More generally, if $n=p_1^{r_1} \cdots p_m^{r_m} \in \Bbb N$, then  each pair of integers $p_j^{r_j}$ are coprime, so that by CRT we get $$\mathbb{Z}[i]/(n) \cong \mathbb{Z}[i]/(p_1^{r_1}) \times \cdots \times \mathbb{Z}[i]/(p_m^{r_m})$$ I was not sure how to find the structure of $\mathbb{Z}[i]/(p^{r}) \cong (\Bbb Z/p^r \Bbb Z)[X] \,/\, (X^2+1)$ when $p$ is prime and $r>1$. — Even more generally, in order to determine the structure of $\mathbb{Z}[i]/(a+bi)$ with $a+bi=d(x+iy)$ and $(x,y)=1$, we could try to use the CRT, provided that $d$ is coprime with $x+iy$ in $\Bbb Z[i]$. But this is not always true: for $d=13$ and $x+iy=2+3i$, we can't find Gauss integers $u$ and $v$ such that $du + (x+iy)v=1$, because this would mean that $(2+3i)[(2-3i)u+v]=1$, i.e. $2+3i$ is a unit in $\Bbb Z[i]$ which is not because its norm is $13 \neq ±1$. — I was not able to go further. I recall that my general question is to known what $\mathbb{Z}[i]/(a+bi)$ is isomorphic to , when $a$ and $b$ are integers which are not coprime (for instance $a=p^r,b=0$ or $d=(a,b) = a^2+b^2>1$). Thank you for your help!",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'algebraic-number-theory', 'gaussian-integers']"
82,Proving that there exists a saturated set with given highest weight,Proving that there exists a saturated set with given highest weight,,"This is an question about an exercise in Humphreys book on Lie algebras. First of all a bunch of definitions and notation, see §13 in Humphreys for details. Let $\Phi$ be a root system, $\Delta$ a base for $\Phi$. The set of weights of $\Phi$ is the set  $\Lambda = \{\lambda : \langle \lambda, \alpha \rangle \in \mathbb{Z} \text{ for all $\alpha \in \Phi$} \}$ where $\langle x, y \rangle$ is the notation for Cartan integers. The set of dominant weights (wrt  the base $\Delta$) is the set $$\Lambda^+ = \{\lambda \in \Lambda: \langle \lambda, \alpha \rangle \geq 0 \text{ for all $\alpha \in \Delta$}\}$$ For weights we have the partial order $\prec$ wrt $\Delta$ ($\mu \prec \lambda$ if and only if $\lambda - \mu$ is a finite sum of positive roots). Now we call a subset $\Pi$ of $\Lambda$ saturated if for all $\lambda \in \Pi$, $\alpha \in \Phi$ and integer $i$ between $0$ and $\langle \lambda, \alpha \rangle$, we have $\lambda - i\alpha \in \Pi$. We say that $\Pi$ has highest weight $\lambda$ if $\lambda \in \Lambda^+$ and $\mu \prec \lambda$ for all $\mu \in \Pi$. It is immediate that $\Pi$ is closed under the action of the Weyl group. What I want to prove is that for any $\lambda \in \Lambda^+$, there is a unique saturated set $\Pi$ with highest weight $\lambda$. Some ideas: Everything follows once we can prove that the following set is saturated: $$\Pi = \{\sigma \mu: \mu \in \Lambda^+, \mu \prec \lambda, \sigma \in W\}$$ where $W$ is the Weyl group. It is enough to prove that for all $\mu \in \Lambda^+$, $\mu \prec \lambda$ the following holds: For all $\alpha \in \Phi$ and $i$ between $0$ and $\langle \mu, \alpha \rangle$, the element $\mu - i\alpha$ is conjugate under $W$ to some $\mu' \in \Lambda^+$, $\mu' \prec \lambda$. I can see how this holds in the case where $\alpha \in \Delta$. Now $\langle \lambda, \alpha \rangle \geq 0$. Let $0 \leq i \leq \langle \lambda, \alpha \rangle$ and $\mu = \lambda - i \alpha$. Consider two cases: If $0 \leq i \leq \langle \lambda, \alpha \rangle / 2$: then $\mu$ is dominant, because $\langle \mu, \alpha \rangle \geq 0$ by the condition on $i$ and because $\langle \alpha, \beta \rangle \leq 0$ for $\beta \in \Delta$, $\beta \neq \alpha$. If $\langle \lambda, \alpha \rangle / 2 \leq i \leq \langle \lambda, \alpha \rangle$: Now using the reflection $\sigma_\alpha$ with respect to $\alpha$, we have $\sigma_\alpha(\mu) = \lambda - j \alpha$, where $0 \leq j \leq \langle \lambda, \alpha \rangle / 2$. So we are done by the first case. How to prove this in the general case? I think it should be enough to do this for positive roots, but I have no idea how to generalize from the case $\alpha \in \Delta$.","This is an question about an exercise in Humphreys book on Lie algebras. First of all a bunch of definitions and notation, see §13 in Humphreys for details. Let $\Phi$ be a root system, $\Delta$ a base for $\Phi$. The set of weights of $\Phi$ is the set  $\Lambda = \{\lambda : \langle \lambda, \alpha \rangle \in \mathbb{Z} \text{ for all $\alpha \in \Phi$} \}$ where $\langle x, y \rangle$ is the notation for Cartan integers. The set of dominant weights (wrt  the base $\Delta$) is the set $$\Lambda^+ = \{\lambda \in \Lambda: \langle \lambda, \alpha \rangle \geq 0 \text{ for all $\alpha \in \Delta$}\}$$ For weights we have the partial order $\prec$ wrt $\Delta$ ($\mu \prec \lambda$ if and only if $\lambda - \mu$ is a finite sum of positive roots). Now we call a subset $\Pi$ of $\Lambda$ saturated if for all $\lambda \in \Pi$, $\alpha \in \Phi$ and integer $i$ between $0$ and $\langle \lambda, \alpha \rangle$, we have $\lambda - i\alpha \in \Pi$. We say that $\Pi$ has highest weight $\lambda$ if $\lambda \in \Lambda^+$ and $\mu \prec \lambda$ for all $\mu \in \Pi$. It is immediate that $\Pi$ is closed under the action of the Weyl group. What I want to prove is that for any $\lambda \in \Lambda^+$, there is a unique saturated set $\Pi$ with highest weight $\lambda$. Some ideas: Everything follows once we can prove that the following set is saturated: $$\Pi = \{\sigma \mu: \mu \in \Lambda^+, \mu \prec \lambda, \sigma \in W\}$$ where $W$ is the Weyl group. It is enough to prove that for all $\mu \in \Lambda^+$, $\mu \prec \lambda$ the following holds: For all $\alpha \in \Phi$ and $i$ between $0$ and $\langle \mu, \alpha \rangle$, the element $\mu - i\alpha$ is conjugate under $W$ to some $\mu' \in \Lambda^+$, $\mu' \prec \lambda$. I can see how this holds in the case where $\alpha \in \Delta$. Now $\langle \lambda, \alpha \rangle \geq 0$. Let $0 \leq i \leq \langle \lambda, \alpha \rangle$ and $\mu = \lambda - i \alpha$. Consider two cases: If $0 \leq i \leq \langle \lambda, \alpha \rangle / 2$: then $\mu$ is dominant, because $\langle \mu, \alpha \rangle \geq 0$ by the condition on $i$ and because $\langle \alpha, \beta \rangle \leq 0$ for $\beta \in \Delta$, $\beta \neq \alpha$. If $\langle \lambda, \alpha \rangle / 2 \leq i \leq \langle \lambda, \alpha \rangle$: Now using the reflection $\sigma_\alpha$ with respect to $\alpha$, we have $\sigma_\alpha(\mu) = \lambda - j \alpha$, where $0 \leq j \leq \langle \lambda, \alpha \rangle / 2$. So we are done by the first case. How to prove this in the general case? I think it should be enough to do this for positive roots, but I have no idea how to generalize from the case $\alpha \in \Delta$.",,"['abstract-algebra', 'lie-algebras', 'root-systems']"
83,Must an ideal contain the kernel for its image to be an ideal?,Must an ideal contain the kernel for its image to be an ideal?,,"I'm trying to learn some basic abstract algebra from Pinter's A Book of Abstract Algebra and I find myself puzzled by the following simple question about ring homomorphisms: Let $A$ and $B$ be rings.  If $f : A \to B$ is a homomorphism from $A$ onto $B$ with kernel $K$, and $J$ is an ideal of $A$ such that $K \subseteq J$, then $f(J)$ is an ideal of $B$. I'm clearly missing the obvious, but I don't see where the requirement $K \subseteq J$ comes into the proof.  Since $f$ is also a homomorphism of additive groups, the image $f(J)$ must be closed under addition and negatives (correct?). Then for $f(J)$ to be an ideal we have to show that it is closed under multiplication by an arbitrary element $b \in B$. Since $f$ is onto, there is some $a \in A$ such that $b = f(a)$. Let $j'$ be any element of $f(J)$, so $j' = f(j)$ for some $j \in J$. Then $bj' = f(a)f(j) = f(aj)$, and $aj \in J$ since $J$ is an ideal.  Then it seems that $f(J)$ is closed under multiplication by $B$. What mistaken assumption am I making?","I'm trying to learn some basic abstract algebra from Pinter's A Book of Abstract Algebra and I find myself puzzled by the following simple question about ring homomorphisms: Let $A$ and $B$ be rings.  If $f : A \to B$ is a homomorphism from $A$ onto $B$ with kernel $K$, and $J$ is an ideal of $A$ such that $K \subseteq J$, then $f(J)$ is an ideal of $B$. I'm clearly missing the obvious, but I don't see where the requirement $K \subseteq J$ comes into the proof.  Since $f$ is also a homomorphism of additive groups, the image $f(J)$ must be closed under addition and negatives (correct?). Then for $f(J)$ to be an ideal we have to show that it is closed under multiplication by an arbitrary element $b \in B$. Since $f$ is onto, there is some $a \in A$ such that $b = f(a)$. Let $j'$ be any element of $f(J)$, so $j' = f(j)$ for some $j \in J$. Then $bj' = f(a)f(j) = f(aj)$, and $aj \in J$ since $J$ is an ideal.  Then it seems that $f(J)$ is closed under multiplication by $B$. What mistaken assumption am I making?",,['abstract-algebra']
84,Why isn't the perfect closure separable?,Why isn't the perfect closure separable?,,"Let $F\subset K$ be an algebraic extension of fields. By taking the separable closure $K_s$, we obtain a tower $F\subset K_s \subset K$ such that $F\subset K_s$ is separable and $K_s\subset K$ is purely inseparable. Wikipedia , following Isaacs, Algebra, a graduate course p.301, says: On the other hand, an arbitrary algebraic extension $F\subset K$ may not possess an intermediate extension $E$ that is purely inseparable over $F$ and over which $K$ is separable. The question is: why? And more explicitly, had I not seen this this soon, I would surely have conjectured that the perfect closure, $K_p$, which satisfies $F\subset K_p$ purely inseparable, also satisfied $K_p\subset K$ separable... But why doesn't it?","Let $F\subset K$ be an algebraic extension of fields. By taking the separable closure $K_s$, we obtain a tower $F\subset K_s \subset K$ such that $F\subset K_s$ is separable and $K_s\subset K$ is purely inseparable. Wikipedia , following Isaacs, Algebra, a graduate course p.301, says: On the other hand, an arbitrary algebraic extension $F\subset K$ may not possess an intermediate extension $E$ that is purely inseparable over $F$ and over which $K$ is separable. The question is: why? And more explicitly, had I not seen this this soon, I would surely have conjectured that the perfect closure, $K_p$, which satisfies $F\subset K_p$ purely inseparable, also satisfied $K_p\subset K$ separable... But why doesn't it?",,"['abstract-algebra', 'field-theory']"
85,Show that $\cos\big(\frac{2\pi}{n}\big)$ is an algebraic number,Show that  is an algebraic number,\cos\big(\frac{2\pi}{n}\big),"$\bullet~$ Problem: Show that $\cos\bigg(\dfrac{2\pi}{n}\bigg)$ is an algebraic number [where $n$ $\in$ $\mathbb{Z} \setminus \{0\}$ ]. $\bullet~$ My approach: Let's consider the following polynomial in $\mathbb{Z}[x]$ in recursive terms. \begin{align*}     &T_{0}(x) = 1\\     &T_{1}(x) = x\\     &T_{n + 1}(x) = 2x T_{n}(x) - T_{n-1}(x) \end{align*} $\bullet~$ $\textbf{Claim:}$ The polynomial $T_{n}(x)$ for any $n$ $\in$ $\mathbb{N}$ satisfies the following \begin{align*}     T_{n}(\cos(\theta)) = \cos(n\theta)   \end{align*} $\bullet~$ Proof: We'll use induction on $n$ for this proof. At first, we easily obtain that for $n = 0$ the given is true. Now for some $n = k$ , we assume that \begin{align*}     T_{k}(\cos(\theta)) = \cos(k\theta)   \end{align*} Therefore we need to prove for $n = (k + 1)$ . Now from the recursion relation of $T_{n}(x)$ we have \begin{align*}     T_{k + 1}(\cos(\theta)) & = 2 \cos(\theta)T_{k}(\cos(\theta)) - T_{k -1}(\cos(\theta))\\        & = 2 \cos(\theta) \cos(k\theta) - \cos((k -1)\theta)\\     & = 2 \cos(\theta) \cos(k\theta) - \cos(k\theta) \cos(\theta) - \sin(k\theta)\sin(\theta)\\     & = \cos((k + 1)\theta) \end{align*} Hence by induction hypothesis, we obtain that our claim is true. Therefore we have \begin{align*}     T_{n}\Bigg(\cos\bigg(\frac{2\pi}{n}\bigg)\Bigg) = \cos(2\pi) = 1 \end{align*} Therefore we just need to consider a polynomial $P(x) = T_{n}(x) - 1.~$ As $T_{n}(x) \in \mathbb{Z}[x]$ it implies $P(x) \in \mathbb{Z}[x]$ Therefore we have $\cos\big(\frac{2\pi}{n}\big)$ is an algebraic number. Please check the solution and point out the glitches. Can you prove this in a different (like a pretty elementary one (by not using the idea of cyclotomic polynomials or Chebyshev's Polynomials)) way? $\bullet~$ $\large{\textbf{Edit:}}$ $\blacksquare~$ Alternate Approach: I have used the expansion of $\cos\bigg( \dfrac{2\pi}{n} \bigg)$ . And obviously which comes from de-Moivre's (simple for $n \in \mathbb{Z}$ ). Can you please try to give a solution not using these arguments? (de-Moivre's, Cyclotomic Polynomial, $\color{blue}{\text{Chebychev Polynomials}}$ , etc etc).","Problem: Show that is an algebraic number [where ]. My approach: Let's consider the following polynomial in in recursive terms. The polynomial for any satisfies the following Proof: We'll use induction on for this proof. At first, we easily obtain that for the given is true. Now for some , we assume that Therefore we need to prove for . Now from the recursion relation of we have Hence by induction hypothesis, we obtain that our claim is true. Therefore we have Therefore we just need to consider a polynomial As it implies Therefore we have is an algebraic number. Please check the solution and point out the glitches. Can you prove this in a different (like a pretty elementary one (by not using the idea of cyclotomic polynomials or Chebyshev's Polynomials)) way? Alternate Approach: I have used the expansion of . And obviously which comes from de-Moivre's (simple for ). Can you please try to give a solution not using these arguments? (de-Moivre's, Cyclotomic Polynomial, , etc etc).","\bullet~ \cos\bigg(\dfrac{2\pi}{n}\bigg) n \in \mathbb{Z} \setminus \{0\} \bullet~ \mathbb{Z}[x] \begin{align*}
    &T_{0}(x) = 1\\
    &T_{1}(x) = x\\
    &T_{n + 1}(x) = 2x T_{n}(x) - T_{n-1}(x)
\end{align*} \bullet~ \textbf{Claim:} T_{n}(x) n \in \mathbb{N} \begin{align*}
    T_{n}(\cos(\theta)) = \cos(n\theta)  
\end{align*} \bullet~ n n = 0 n = k \begin{align*}
    T_{k}(\cos(\theta)) = \cos(k\theta)  
\end{align*} n = (k + 1) T_{n}(x) \begin{align*}
    T_{k + 1}(\cos(\theta)) & = 2 \cos(\theta)T_{k}(\cos(\theta)) - T_{k -1}(\cos(\theta))\\   
    & = 2 \cos(\theta) \cos(k\theta) - \cos((k -1)\theta)\\
    & = 2 \cos(\theta) \cos(k\theta) - \cos(k\theta) \cos(\theta) - \sin(k\theta)\sin(\theta)\\
    & = \cos((k + 1)\theta)
\end{align*} \begin{align*}
    T_{n}\Bigg(\cos\bigg(\frac{2\pi}{n}\bigg)\Bigg) = \cos(2\pi) = 1
\end{align*} P(x) = T_{n}(x) - 1.~ T_{n}(x) \in \mathbb{Z}[x] P(x) \in \mathbb{Z}[x] \cos\big(\frac{2\pi}{n}\big) \bullet~ \large{\textbf{Edit:}} \blacksquare~ \cos\bigg( \dfrac{2\pi}{n} \bigg) n \in \mathbb{Z} \color{blue}{\text{Chebychev Polynomials}}","['abstract-algebra', 'ring-theory', 'solution-verification']"
86,What is a clever proof of Hilbert's basis theorem?,What is a clever proof of Hilbert's basis theorem?,,"So I am studying commutative algebra at the moment and I have come across the proof of the Hilbert Basis Theorem (the proof I have is the same as the one in Reid's Undergraduate Commutative Algebra ). I can't see how I would ever have thought of such a proof and I can't find anywhere which gives a good motivation for it. I wondered if anyone here could help me out? EDIT : Following Martin Brandenburg's advice I will outline the proof and explain where I get the feeling: ""why would you do that?"" Theorem. If $A$ is a Noetherian ring (commutative with 1), then $A[X]$ is also Noetherian. Proof/Discussion. First, we pick any ideal $I$ in $A[X]$. We aim to find a finite set of generators for it. We only have data about ideals in $A$, so we need to pass from the ideal $I$ in $A[X]$ to ideals in $A$. Given any polynomial $f \in I$, a natural way to obtain elements of $A$ is to look at its coefficients. The most ""obvious"" coefficients to look at are the constant term and the leading coefficient. Just looking at the constant terms discards a lot of information about $I$ and so we don't go this way. Let $\lambda(f)$ denote the leading coefficient of $f \in A[X]$. We can define $J=\{a \in A : \exists f \in I \text{ such that } a=\lambda(f)\}$ but this is not necessarily an ideal of $A$. For example, if $a,b \in J$, then $\exists f,g \in I$ with $a=\lambda(f)$ and $b=\lambda(g)$. It is then natural to say, ""well $f+g \in I$ (since $I$ is an ideal) and $\lambda(f+g)=a+b$ so we have $a+b \in J$"" but this only works if $\deg f = \deg g$. So we are led to defining, for each $m\in\mathbb{N},$ $$J_m=\{a \in A : \exists f \in I \text{ with } \deg f = m \text{ such that } a = \lambda(f)\}\cup \{0\}.$$ Then, indeed, each $J_m$ is indeed an ideal of $A$. Now we can use that $A$ is Noetherian to obtain that each $J_m$ is finitely generated: $$J_m=(a_{m1},\ldots,a_{mr_m})$$ for some $a_{ij} \in A$. In particular, given $m \in \mathbb{N}$, we have, for each $1\leq j \leq r_m$, some $f_{mj}\in I$ with $\deg f_{mj}=m$; $\lambda(f_{mj})=a_{mj}$. Define, for each $m\in \mathbb{N}$, $$S_m = \{f_{mj} : 1\leq j \leq r_m\}.$$ We claim that the (infinite) set $S=\bigcup_{m\in\mathbb{N}}S_m$ generates $I$. Suppose $f \in I$ with $\deg f = n$ and $\lambda(f)=a$. Then $a \in J_n$ and so there exists $s_1,\ldots,s_{r_n} \in A$ such that $$a=s_1a_{n1}+\ldots+s_{r_n}a_{nr_n}.$$ Consequently, if we define $g=\sum_{k=1}^{r_n}s_kf_{nk} \in (S_n)$, we have that $\lambda(g)=a$. In particular, $f-g \in I$ with $\deg(f-g)<n$. Proceeding inductively, we obtain $h \in (S_0,S_1,\ldots,S_n)$ such that $f=h$. This shows that $S$ is an infinite set of generators for $I$. However, we now observe that $J_m\subseteq J_{m+1}$ for each $m \in \mathbb{N}$. Indeed, if $a \in J_m$, then we take $f \in I$ with $\deg f=m$ such that $\lambda(f)=a$ and see that $Xf \in I$ (since $I$ is an ideal), $\deg Xf = m+1$, $\lambda(Xf)=a$, whence it follows that $a \in J_{m+1}$. As $A$ is Noetherian it satisfies the ACC on ideals and so there exists $N \in \mathbb{N}$ such that $$J_{N}=J_{N+k} \text{ for all } k \in \mathbb{N}$$ and so the set $S$, which generates $I$, is in fact finite! This concludes the proof. // Having typed it out in this way and really had a good think about it, it doesn't seem too unnatural I suppose (although, of course, I'm still not at all convinced I could have come up with it). I suppose the bit where we notice that the $J_m$'s form an ascending chain seems like a ""bit of good luck"" rather than having any reason to expect that to be the case... What are your thoughts about my interpretation? Many thanks!","So I am studying commutative algebra at the moment and I have come across the proof of the Hilbert Basis Theorem (the proof I have is the same as the one in Reid's Undergraduate Commutative Algebra ). I can't see how I would ever have thought of such a proof and I can't find anywhere which gives a good motivation for it. I wondered if anyone here could help me out? EDIT : Following Martin Brandenburg's advice I will outline the proof and explain where I get the feeling: ""why would you do that?"" Theorem. If $A$ is a Noetherian ring (commutative with 1), then $A[X]$ is also Noetherian. Proof/Discussion. First, we pick any ideal $I$ in $A[X]$. We aim to find a finite set of generators for it. We only have data about ideals in $A$, so we need to pass from the ideal $I$ in $A[X]$ to ideals in $A$. Given any polynomial $f \in I$, a natural way to obtain elements of $A$ is to look at its coefficients. The most ""obvious"" coefficients to look at are the constant term and the leading coefficient. Just looking at the constant terms discards a lot of information about $I$ and so we don't go this way. Let $\lambda(f)$ denote the leading coefficient of $f \in A[X]$. We can define $J=\{a \in A : \exists f \in I \text{ such that } a=\lambda(f)\}$ but this is not necessarily an ideal of $A$. For example, if $a,b \in J$, then $\exists f,g \in I$ with $a=\lambda(f)$ and $b=\lambda(g)$. It is then natural to say, ""well $f+g \in I$ (since $I$ is an ideal) and $\lambda(f+g)=a+b$ so we have $a+b \in J$"" but this only works if $\deg f = \deg g$. So we are led to defining, for each $m\in\mathbb{N},$ $$J_m=\{a \in A : \exists f \in I \text{ with } \deg f = m \text{ such that } a = \lambda(f)\}\cup \{0\}.$$ Then, indeed, each $J_m$ is indeed an ideal of $A$. Now we can use that $A$ is Noetherian to obtain that each $J_m$ is finitely generated: $$J_m=(a_{m1},\ldots,a_{mr_m})$$ for some $a_{ij} \in A$. In particular, given $m \in \mathbb{N}$, we have, for each $1\leq j \leq r_m$, some $f_{mj}\in I$ with $\deg f_{mj}=m$; $\lambda(f_{mj})=a_{mj}$. Define, for each $m\in \mathbb{N}$, $$S_m = \{f_{mj} : 1\leq j \leq r_m\}.$$ We claim that the (infinite) set $S=\bigcup_{m\in\mathbb{N}}S_m$ generates $I$. Suppose $f \in I$ with $\deg f = n$ and $\lambda(f)=a$. Then $a \in J_n$ and so there exists $s_1,\ldots,s_{r_n} \in A$ such that $$a=s_1a_{n1}+\ldots+s_{r_n}a_{nr_n}.$$ Consequently, if we define $g=\sum_{k=1}^{r_n}s_kf_{nk} \in (S_n)$, we have that $\lambda(g)=a$. In particular, $f-g \in I$ with $\deg(f-g)<n$. Proceeding inductively, we obtain $h \in (S_0,S_1,\ldots,S_n)$ such that $f=h$. This shows that $S$ is an infinite set of generators for $I$. However, we now observe that $J_m\subseteq J_{m+1}$ for each $m \in \mathbb{N}$. Indeed, if $a \in J_m$, then we take $f \in I$ with $\deg f=m$ such that $\lambda(f)=a$ and see that $Xf \in I$ (since $I$ is an ideal), $\deg Xf = m+1$, $\lambda(Xf)=a$, whence it follows that $a \in J_{m+1}$. As $A$ is Noetherian it satisfies the ACC on ideals and so there exists $N \in \mathbb{N}$ such that $$J_{N}=J_{N+k} \text{ for all } k \in \mathbb{N}$$ and so the set $S$, which generates $I$, is in fact finite! This concludes the proof. // Having typed it out in this way and really had a good think about it, it doesn't seem too unnatural I suppose (although, of course, I'm still not at all convinced I could have come up with it). I suppose the bit where we notice that the $J_m$'s form an ascending chain seems like a ""bit of good luck"" rather than having any reason to expect that to be the case... What are your thoughts about my interpretation? Many thanks!",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'noetherian']"
87,The smallest nontrivial conjugacy class in $S_n$,The smallest nontrivial conjugacy class in,S_n,"Find the smallest nontrivial conjugacy class in $S_n$. For small $n$, the answer can be found by counting the permutations of each possible cycle type. The result is: $$\begin{array}{ccc} n & \text{smallest nontrivial class(es)} & \text{size} \\ \hline 1 & \text{none} & \\ 2 & \text{transpositions} & 1 \\ 3 & 3\text{-cycles} & 2 \\ 4 & \text{double transpositions} & 3 \\ 5 & \text{transpositions} & 10 \\ 6 & \text{transpositions, triple transpositions} & 15 \end{array}$$ For $n\geq 7$, I think the unique smallest nontrivial conjugacy class in $S_n$ is given by the transpositions. The conjugacy classes in $S_n$ are given by the sets of permutations of the same cycle type. For the cycle type $\alpha = 1^{a_1} 2^{a_2} \ldots$, the size of the respective conjugacy class is known to be $$N_\alpha = \frac{n!}{\prod_i (i^{a_i} a_i!)}.$$ In particular, the number of transpositions in $S_n$ is $\binom{n}{2}$. In this way, it remains to show the following: Let $n \geq 7$ and $\alpha$ be a partition of $n$ distinct from $1^n$ and $2^1 1^{n-2}$. Show that $N_\alpha > \binom{n}{2}$. So far I can only come up with a lengthy and technical case-by-case study. I'm looking for a more elegant proof of the statement.","Find the smallest nontrivial conjugacy class in $S_n$. For small $n$, the answer can be found by counting the permutations of each possible cycle type. The result is: $$\begin{array}{ccc} n & \text{smallest nontrivial class(es)} & \text{size} \\ \hline 1 & \text{none} & \\ 2 & \text{transpositions} & 1 \\ 3 & 3\text{-cycles} & 2 \\ 4 & \text{double transpositions} & 3 \\ 5 & \text{transpositions} & 10 \\ 6 & \text{transpositions, triple transpositions} & 15 \end{array}$$ For $n\geq 7$, I think the unique smallest nontrivial conjugacy class in $S_n$ is given by the transpositions. The conjugacy classes in $S_n$ are given by the sets of permutations of the same cycle type. For the cycle type $\alpha = 1^{a_1} 2^{a_2} \ldots$, the size of the respective conjugacy class is known to be $$N_\alpha = \frac{n!}{\prod_i (i^{a_i} a_i!)}.$$ In particular, the number of transpositions in $S_n$ is $\binom{n}{2}$. In this way, it remains to show the following: Let $n \geq 7$ and $\alpha$ be a partition of $n$ distinct from $1^n$ and $2^1 1^{n-2}$. Show that $N_\alpha > \binom{n}{2}$. So far I can only come up with a lengthy and technical case-by-case study. I'm looking for a more elegant proof of the statement.",,"['abstract-algebra', 'combinatorics', 'group-theory', 'permutations', 'symmetric-groups']"
88,"Sum of irrational numbers, a basic algebra problem [duplicate]","Sum of irrational numbers, a basic algebra problem [duplicate]",,"This question already has answers here : The square roots of different primes are linearly independent over the field of rationals (3 answers) Closed 1 year ago . Let $x_1,\dots,x_n$ be positive  rational numbers. If $\sqrt[l_1]{x_1},\dots,\sqrt[l_n]{x_n}$ are all irrational numbers (where $l_1,l_2,\dotsc,l_n\in\Bbb N^*$), does it follow that $$\sqrt[l_1]{x_1}+ \dotsb + \sqrt[l_n]{x_n}$$ is an irrational number, too?","This question already has answers here : The square roots of different primes are linearly independent over the field of rationals (3 answers) Closed 1 year ago . Let $x_1,\dots,x_n$ be positive  rational numbers. If $\sqrt[l_1]{x_1},\dots,\sqrt[l_n]{x_n}$ are all irrational numbers (where $l_1,l_2,\dotsc,l_n\in\Bbb N^*$), does it follow that $$\sqrt[l_1]{x_1}+ \dotsb + \sqrt[l_n]{x_n}$$ is an irrational number, too?",,"['abstract-algebra', 'number-theory', 'field-theory']"
89,Are finitely generated projective modules free over the total ring of fractions?,Are finitely generated projective modules free over the total ring of fractions?,,Let $Q(A)$ be the total ring of fractions of a commutative reduced non-noetherian ring $A$. Let $P$ be a finitely generated projective module over $Q(A)$ which is of constant rank (i.e. locally free of constant rank for each localization). Is $P$ free? (Note that $Q(A)$ need not be zero dimensional.),Let $Q(A)$ be the total ring of fractions of a commutative reduced non-noetherian ring $A$. Let $P$ be a finitely generated projective module over $Q(A)$ which is of constant rank (i.e. locally free of constant rank for each localization). Is $P$ free? (Note that $Q(A)$ need not be zero dimensional.),,"['abstract-algebra', 'commutative-algebra']"
90,Show that $(R/I)[x]\cong R[x]/I[x]$.,Show that .,(R/I)[x]\cong R[x]/I[x],If $R$ is a commutative ring with unity and $I$ is an ideal of $R$ then show that $(R/I)[x]\cong R[x]/I[x]$. My effort : Define $\phi :R[x]\to (R/I)[x]$ $\phi(a_0+a_1x+a_2x^2+\cdots +a_nx^n)=(a_0+I)+(a_1+I)x+(a_2+I)x^2+\cdots +(a_n+I)x^n$ Obviously $\phi $ is a ring homomorphism and surjective. $\ker \phi =\{a_0+a_1x+a_2x^2+\cdots +a_nx^n:\phi(a_0+a_1x+a_2x^2+\cdots +a_nx^n)=I\}$ So $(a_0+I)+(a_1+I)x+(a_2+I)x^2+\cdots +(a_n+I)x^n=I\implies a_i\in I\forall i$ So $\ker \phi=I[x]$ Is the proof correct? Please help.,If $R$ is a commutative ring with unity and $I$ is an ideal of $R$ then show that $(R/I)[x]\cong R[x]/I[x]$. My effort : Define $\phi :R[x]\to (R/I)[x]$ $\phi(a_0+a_1x+a_2x^2+\cdots +a_nx^n)=(a_0+I)+(a_1+I)x+(a_2+I)x^2+\cdots +(a_n+I)x^n$ Obviously $\phi $ is a ring homomorphism and surjective. $\ker \phi =\{a_0+a_1x+a_2x^2+\cdots +a_nx^n:\phi(a_0+a_1x+a_2x^2+\cdots +a_nx^n)=I\}$ So $(a_0+I)+(a_1+I)x+(a_2+I)x^2+\cdots +(a_n+I)x^n=I\implies a_i\in I\forall i$ So $\ker \phi=I[x]$ Is the proof correct? Please help.,,"['abstract-algebra', 'proof-verification', 'ring-theory']"
91,"Does there exist a pair of infinite fields, the additive group of one isomorphic to the multiplicative group of the other?","Does there exist a pair of infinite fields, the additive group of one isomorphic to the multiplicative group of the other?",,"It is a common exercise in algebra to show that there does not exist a field $F$ such that its additive group $F^+$ and multiplicative group $F^*$ are isomorphic. See e.g. this question . One of the snappiest proofs I know is that, if we suppose for a contradiction they are, then any isomorphism sends solutions of the equation $2x = 0$ in the additive group to solutions of the equation $y^2 = 1$ in the multiplicative group. Depending on whether the characteristic of $F$ is or isn't 2, the former has either $|F|$ or $1$ solution(s), while the latter has $1$ or $2$ solutions, respectively. There is no field for which these numbers agree, so $F^+ \not\cong F^*$ ever. One might now ask whether there is a pair of fields, $E$ and $F$, for which $E^+ \cong F^*$ as groups. Clearly $\def\GF#1{\mathrm{GF}(#1)}\GF2^+ \cong \GF3^*$, $\GF3^+ \cong \GF4^*$, and in general if $p$ is a prime and $p+1$ is a prime power, then $\GF p^+ \cong \GF{p+1}^*$. You can see that this characterizes the situation in the positive characteristic case, from the same equation trick above: if $\def\c{\operatorname{char}}\c E = 2$ and $\c F \ne 2$, we can make $|E| = 2$ and get a solution. Else we must have $\c E \ne 2 = \c F$. If $\c E = c \ne 0$, then elements of $E$ must get mapped to $c$-th roots of unity in $F$, and there can be at most $c$ of those. This leaves the case where $\c E = 0$, for which there are no finite fields. In fact, none of the cases above permit any infinite fields, either. This brings me to my question: Do there exist infinite fields $E$ and $F$ such that $E^+ \cong F^*$? I believe the answer is no, and it seems unlikely that such an isomorphism would exist, but I can't make heads or tails of it, really. Here's what I have. As above, I can show that if $E$ and $F$ are infinite, and $\phi: E^+ \to F^*$ is an isomorphism, then we may assume $\c E = 0$—so WLOG it is an extension of $\mathbb Q$—and $\c F = 2$. Every element of $E$ has infinite additive order, so every element of $F^*$ has infinite multiplicative order, and there are no roots of unity except $1 = \phi(0)$. However, if $a \ne 1$ in $F$, then $a$ has a $k$-th root $\phi(\frac1k \phi^{-1}(a))$ for all $k$, since $E \supseteq \mathbb Q$.","It is a common exercise in algebra to show that there does not exist a field $F$ such that its additive group $F^+$ and multiplicative group $F^*$ are isomorphic. See e.g. this question . One of the snappiest proofs I know is that, if we suppose for a contradiction they are, then any isomorphism sends solutions of the equation $2x = 0$ in the additive group to solutions of the equation $y^2 = 1$ in the multiplicative group. Depending on whether the characteristic of $F$ is or isn't 2, the former has either $|F|$ or $1$ solution(s), while the latter has $1$ or $2$ solutions, respectively. There is no field for which these numbers agree, so $F^+ \not\cong F^*$ ever. One might now ask whether there is a pair of fields, $E$ and $F$, for which $E^+ \cong F^*$ as groups. Clearly $\def\GF#1{\mathrm{GF}(#1)}\GF2^+ \cong \GF3^*$, $\GF3^+ \cong \GF4^*$, and in general if $p$ is a prime and $p+1$ is a prime power, then $\GF p^+ \cong \GF{p+1}^*$. You can see that this characterizes the situation in the positive characteristic case, from the same equation trick above: if $\def\c{\operatorname{char}}\c E = 2$ and $\c F \ne 2$, we can make $|E| = 2$ and get a solution. Else we must have $\c E \ne 2 = \c F$. If $\c E = c \ne 0$, then elements of $E$ must get mapped to $c$-th roots of unity in $F$, and there can be at most $c$ of those. This leaves the case where $\c E = 0$, for which there are no finite fields. In fact, none of the cases above permit any infinite fields, either. This brings me to my question: Do there exist infinite fields $E$ and $F$ such that $E^+ \cong F^*$? I believe the answer is no, and it seems unlikely that such an isomorphism would exist, but I can't make heads or tails of it, really. Here's what I have. As above, I can show that if $E$ and $F$ are infinite, and $\phi: E^+ \to F^*$ is an isomorphism, then we may assume $\c E = 0$—so WLOG it is an extension of $\mathbb Q$—and $\c F = 2$. Every element of $E$ has infinite additive order, so every element of $F^*$ has infinite multiplicative order, and there are no roots of unity except $1 = \phi(0)$. However, if $a \ne 1$ in $F$, then $a$ has a $k$-th root $\phi(\frac1k \phi^{-1}(a))$ for all $k$, since $E \supseteq \mathbb Q$.",,"['field-theory', 'group-isomorphism', 'positive-characteristic']"
92,What is an easy example of non-Noetherian domain?,What is an easy example of non-Noetherian domain?,,"Keep in mind, I'm strictly an amateur, though a very old one. I learned about imaginary numbers barely two years ago and ideals a year ago, and I'm still decidedly a novice in both topics. In the university library, I was looking at Modules over Non-Noetherian Domains by Fuchs and Salce and I couldn't really understand anything. I'm also looking at the ""Questions that may already have your answer,"" but if they do, it's not in a way that I can understand. Then I thought what about a finite ring, like maybe $\mathbb{Z}_{10}$, but that's only created more questions, like: can a finite ring be non-Noetherian? Although $5 = 5^n$ for any $n \in \mathbb{Z}_{10}$ besides $0$, we're still dealing with only the ideal $\langle 5 \rangle$, right? There's no ascending chain of ideals even though some numbers in this domain have infinitely many factorizations, right? It is a Noetherian ring after all, right? My question, it seems, has then become if it's possible for a non-Noetherian ring to be within the comprehension of a dilettante such as myself, or must it necessarily be esoteric and exotic?","Keep in mind, I'm strictly an amateur, though a very old one. I learned about imaginary numbers barely two years ago and ideals a year ago, and I'm still decidedly a novice in both topics. In the university library, I was looking at Modules over Non-Noetherian Domains by Fuchs and Salce and I couldn't really understand anything. I'm also looking at the ""Questions that may already have your answer,"" but if they do, it's not in a way that I can understand. Then I thought what about a finite ring, like maybe $\mathbb{Z}_{10}$, but that's only created more questions, like: can a finite ring be non-Noetherian? Although $5 = 5^n$ for any $n \in \mathbb{Z}_{10}$ besides $0$, we're still dealing with only the ideal $\langle 5 \rangle$, right? There's no ascending chain of ideals even though some numbers in this domain have infinitely many factorizations, right? It is a Noetherian ring after all, right? My question, it seems, has then become if it's possible for a non-Noetherian ring to be within the comprehension of a dilettante such as myself, or must it necessarily be esoteric and exotic?",,"['abstract-algebra', 'ring-theory', 'noetherian']"
93,Irreducible polynomial means no roots?,Irreducible polynomial means no roots?,,"If a polynomial is irreducible in $R[x]$, where $R$ is a ring, it means that it does not have a root in $R$, right? For example, to say that a polynomial $f(x)\in\mathbb Z[x]$ is irreducible in $\mathbb Q[x]$ is equivalent to say that $f(x)$ does not have any rational root. I just want to make sure.","If a polynomial is irreducible in $R[x]$, where $R$ is a ring, it means that it does not have a root in $R$, right? For example, to say that a polynomial $f(x)\in\mathbb Z[x]$ is irreducible in $\mathbb Q[x]$ is equivalent to say that $f(x)$ does not have any rational root. I just want to make sure.",,"['abstract-algebra', 'polynomials', 'irreducible-polynomials']"
94,Are the reals genuinely a subset of the complex numbers? [duplicate],Are the reals genuinely a subset of the complex numbers? [duplicate],,"This question already has answers here : Why are integers subset of reals? (11 answers) Is it formally right to say that $\Bbb N \subset \Bbb Z$ etc? [duplicate] (1 answer) Why $\mathbb{R}$ is a subset of $\mathbb{C}$? [duplicate] (4 answers) Is ""$a + 0i$"" in every way equal to just ""$a$""? (10 answers) Closed 3 years ago . In Michael Spivak's Calculus , he defines a complex number as an ordered pair of real numbers: $z=(a,b)$ with $a,b \in \mathbb{R}$ . The imaginary unit $i$ is then just a shorthand for the ordered pair $(0,1)$ . Spivak goes on to say When complex numbers were first introduced, it was understood that real numbers were, in particular, complex numbers; if our definition is to be taken seriously then this is not true—a real number is not a pair of real numbers, after all. Although the complex number $(a,0)$ behaves in pretty much the same way as the real number $a$ , they are still not identical. There seem to be some non-trivial differences, as well: we can't write $(5,0)>(3,0)$ in the way we can write $5>3$ . With this in mind, I ask the following questions: Can the real numbers be said to be a subset of the complex numbers if complex numbers are defined as ordered pairs? If the complex numbers are constructed in some other way, then is it meaningful to write $\mathbb R \subset \mathbb C$ ?","This question already has answers here : Why are integers subset of reals? (11 answers) Is it formally right to say that $\Bbb N \subset \Bbb Z$ etc? [duplicate] (1 answer) Why $\mathbb{R}$ is a subset of $\mathbb{C}$? [duplicate] (4 answers) Is ""$a + 0i$"" in every way equal to just ""$a$""? (10 answers) Closed 3 years ago . In Michael Spivak's Calculus , he defines a complex number as an ordered pair of real numbers: with . The imaginary unit is then just a shorthand for the ordered pair . Spivak goes on to say When complex numbers were first introduced, it was understood that real numbers were, in particular, complex numbers; if our definition is to be taken seriously then this is not true—a real number is not a pair of real numbers, after all. Although the complex number behaves in pretty much the same way as the real number , they are still not identical. There seem to be some non-trivial differences, as well: we can't write in the way we can write . With this in mind, I ask the following questions: Can the real numbers be said to be a subset of the complex numbers if complex numbers are defined as ordered pairs? If the complex numbers are constructed in some other way, then is it meaningful to write ?","z=(a,b) a,b \in \mathbb{R} i (0,1) (a,0) a (5,0)>(3,0) 5>3 \mathbb R \subset \mathbb C","['abstract-algebra', 'complex-numbers']"
95,Has Abstract Algebra ever been of service to Analysis?,Has Abstract Algebra ever been of service to Analysis?,,"I’m not saying that it ought to be. I was just wondering whether it has. What I have in mind is that it would have been of material help in proving, say, the Hahn-Banach Theorem, or some such. If it has, what is the most important/impressive instance of this?","I’m not saying that it ought to be. I was just wondering whether it has. What I have in mind is that it would have been of material help in proving, say, the Hahn-Banach Theorem, or some such. If it has, what is the most important/impressive instance of this?",,"['abstract-algebra', 'analysis']"
96,Why the term and the concept of quotient group?,Why the term and the concept of quotient group?,,"The basic concept of Quotient Group is often a confusing thing for me,I mean can any one tell the intuitive concept and the necessity of the Quotient group, I thought that it would be nice to ask as any basic undergraduate can learn the intuition seeing the question. My Question is : Why is the name Quotient Group kept,normally in the case of division, let us take the example of $\large \frac{16}{4}$ the Quotient of the Division is '$4$' which means that there are four '$4$'s in $16$, I mean we can find only $4$ elements with value $4$ So how can we apply the same logic in the case of Quotient Groups,like consider the Group $A$ and normal subgroup $B$ of $A$, So if $A/B$ refers to ""Quotient group"", then does it mean: Are we finding how many copies of $B$ are present in $A$??, like in the case of normal division, or is it something different ?? I understood the Notion of Cosets and Quotient Groups,b ut I want a different Perspective to add Color to the concept. Can anyone tell me the necessity and background for the invention of Quotient Groups? Note: I tried my level best in formatting and typing with proper protocol,if in case, any errors still persist, I beg everyone to explain the reason of their downvote (if any), so that I can rectify myself, Thank you.","The basic concept of Quotient Group is often a confusing thing for me,I mean can any one tell the intuitive concept and the necessity of the Quotient group, I thought that it would be nice to ask as any basic undergraduate can learn the intuition seeing the question. My Question is : Why is the name Quotient Group kept,normally in the case of division, let us take the example of $\large \frac{16}{4}$ the Quotient of the Division is '$4$' which means that there are four '$4$'s in $16$, I mean we can find only $4$ elements with value $4$ So how can we apply the same logic in the case of Quotient Groups,like consider the Group $A$ and normal subgroup $B$ of $A$, So if $A/B$ refers to ""Quotient group"", then does it mean: Are we finding how many copies of $B$ are present in $A$??, like in the case of normal division, or is it something different ?? I understood the Notion of Cosets and Quotient Groups,b ut I want a different Perspective to add Color to the concept. Can anyone tell me the necessity and background for the invention of Quotient Groups? Note: I tried my level best in formatting and typing with proper protocol,if in case, any errors still persist, I beg everyone to explain the reason of their downvote (if any), so that I can rectify myself, Thank you.",,"['abstract-algebra', 'group-theory', 'quotient-group']"
97,Why did I never learn about magmas?,Why did I never learn about magmas?,,"While I’ve never taken an actual abstract algebra course , there are some things I know about the typical curriculum structure: First, define an algebraic structure. Explain groups. Everything else. But we seem to skip the most fundamental algebraic structure: The magma A magma is perhaps the simplest thing you could explain, way simpler than groups: “A magma is a set equipped with one binary operation which is closed by definition. That’s all there is to the definition of a magma! Some well-known magmas are: Integers over addition, subtraction, multiplication Real numbers over addition, subtraction multiplication, division Complex numbers over every arithmetic operation Why did I never hear about a magma ever before while still being well into groups? This diagram (source: Wikipedia: Magma (algebra) ) can show how they are relevant in the structure of the algebraic structures, magmas to groups: Isn’t this a nice visual to explain how all the algebraic structures between magmas and groups are related? PS: I find the name “magma” kind of interesting; why does it have the same name as molten natural material from which igneous rocks are formed? That makes them even more mysterious.","While I’ve never taken an actual abstract algebra course , there are some things I know about the typical curriculum structure: First, define an algebraic structure. Explain groups. Everything else. But we seem to skip the most fundamental algebraic structure: The magma A magma is perhaps the simplest thing you could explain, way simpler than groups: “A magma is a set equipped with one binary operation which is closed by definition. That’s all there is to the definition of a magma! Some well-known magmas are: Integers over addition, subtraction, multiplication Real numbers over addition, subtraction multiplication, division Complex numbers over every arithmetic operation Why did I never hear about a magma ever before while still being well into groups? This diagram (source: Wikipedia: Magma (algebra) ) can show how they are relevant in the structure of the algebraic structures, magmas to groups: Isn’t this a nice visual to explain how all the algebraic structures between magmas and groups are related? PS: I find the name “magma” kind of interesting; why does it have the same name as molten natural material from which igneous rocks are formed? That makes them even more mysterious.",,['abstract-algebra']
98,Nonzero rationals under multiplication are not a cyclic group,Nonzero rationals under multiplication are not a cyclic group,,"Are nonzero rationals under multiplication cyclic? Here's my thinking: They are not. The generator must be a rational $q = a/b$, $a$, $b$ integers with no common factors. Assume $a/b$ generates $\mathbb Q\setminus \{0\}$, then $q^n = a^n/b^n$ or $(1/q)^n = b^n/a^n$. This is impossible since we can find a prime (there're infinitely many of them) that doesn't divide either $a$ or $b$, which have a finite number of prime factors. Not sure if it proves it though.","Are nonzero rationals under multiplication cyclic? Here's my thinking: They are not. The generator must be a rational $q = a/b$, $a$, $b$ integers with no common factors. Assume $a/b$ generates $\mathbb Q\setminus \{0\}$, then $q^n = a^n/b^n$ or $(1/q)^n = b^n/a^n$. This is impossible since we can find a prime (there're infinitely many of them) that doesn't divide either $a$ or $b$, which have a finite number of prime factors. Not sure if it proves it though.",,"['abstract-algebra', 'group-theory', 'cyclic-groups']"
99,How powerful is Cayley's theorem?,How powerful is Cayley's theorem?,,So the Cayley's theorem gives a subgroup $H$ of $S_n$ for a $G$ such that $G$ is isomorphic to $H$ . So $S_n$ behaves like a Universal set to $G.$ Is there  a smaller universal object for all groups of size $n$ ?,So the Cayley's theorem gives a subgroup of for a such that is isomorphic to . So behaves like a Universal set to Is there  a smaller universal object for all groups of size ?,H S_n G G H S_n G. n,"['abstract-algebra', 'group-theory', 'finite-groups', 'symmetric-groups']"
