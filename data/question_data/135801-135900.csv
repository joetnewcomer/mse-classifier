,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Limit sets of a gradient field,Limit sets of a gradient field,,"I am trying to solve this question on J. Sotomayor's book on ODEs. Define $X=\nabla f$ , $f$ being defined in an open subset $\Delta \subset \mathbb R^n$ . Prove that $X$ has no periodic orbits. And, if $X$ have only isolated singular points, show that is $p\in \Delta$ then the limit set of $p$ is empty or is a singular point. About the first statement: if $\gamma$ is a (non-constant) periodic orbit, then, for some $T>0$ , $\gamma(0)=\gamma(T)$ . Therefore: $$0=f(\gamma(T))-f(\gamma(0))=\int_0^T\nabla f(\gamma(t)) \cdot\gamma'(t)dt=\int_0^T\nabla f(\gamma(t))  \cdot \nabla f(\gamma(t))  dt =$$ $$=\int_0^T\vert{\nabla f(\gamma(t))\vert^2dt>0 }$$ and this is an absurd. But I am having some troubles in the second part. I have some ideas. If the orbit $\gamma_p$ passing through $p$ is not periodic then it is constant or it is injective. If $y_p$ is constant, $p$ is a singular point and $\omega(p)=p$ . The trouble is when $\gamma_p$ is one-to-one. What I've been trying to do is to prove that in this case $q \in \omega(p)$ only if $$\lim_{t \to \infty} \gamma_p(t)=q$$ and then using the fact that $$f(q)-f(\gamma(0))=\int_0^\infty \vert\nabla f(\gamma(t))\vert^2dt$$ But the integral on the right side converges only if $$\lim_{t \to \infty} \nabla f(\gamma_p(t))=\nabla f(q)=0$$ and therefore $q$ is a singular point. Is this correct? If it is, any hints of how to complete the missing step? It seems pretty intuitive to me, but I can't formalize it.","I am trying to solve this question on J. Sotomayor's book on ODEs. Define , being defined in an open subset . Prove that has no periodic orbits. And, if have only isolated singular points, show that is then the limit set of is empty or is a singular point. About the first statement: if is a (non-constant) periodic orbit, then, for some , . Therefore: and this is an absurd. But I am having some troubles in the second part. I have some ideas. If the orbit passing through is not periodic then it is constant or it is injective. If is constant, is a singular point and . The trouble is when is one-to-one. What I've been trying to do is to prove that in this case only if and then using the fact that But the integral on the right side converges only if and therefore is a singular point. Is this correct? If it is, any hints of how to complete the missing step? It seems pretty intuitive to me, but I can't formalize it.",X=\nabla f f \Delta \subset \mathbb R^n X X p\in \Delta p \gamma T>0 \gamma(0)=\gamma(T) 0=f(\gamma(T))-f(\gamma(0))=\int_0^T\nabla f(\gamma(t)) \cdot\gamma'(t)dt=\int_0^T\nabla f(\gamma(t))  \cdot \nabla f(\gamma(t))  dt = =\int_0^T\vert{\nabla f(\gamma(t))\vert^2dt>0 } \gamma_p p y_p p \omega(p)=p \gamma_p q \in \omega(p) \lim_{t \to \infty} \gamma_p(t)=q f(q)-f(\gamma(0))=\int_0^\infty \vert\nabla f(\gamma(t))\vert^2dt \lim_{t \to \infty} \nabla f(\gamma_p(t))=\nabla f(q)=0 q,"['ordinary-differential-equations', 'vector-analysis']"
1,Describing mappings using dynamics of time-dependent ODE-flows,Describing mappings using dynamics of time-dependent ODE-flows,,"Let $f\colon\mathbb{R}^n\to\mathbb{R}^n$ . When is it possible to find some $g\in C^1([0,1]\times\mathbb{R}^n, \mathbb{R}^n)$ , uniformly Lipschitz continuous w.r.t the second argument, such that if $u_{x_0}$ is the solution of the IVP $$ \dot{x}(t) = g(t,x(t)),\quad x(0) = x_0,$$ we have $f = x_0\mapsto u_{x_0}(1)$ ? I.e., what do we need from $f$ for it to be describable by the (time-dependent) flow given by an ODE? How can find that ODE? I figured these might be questions answered in the analysis of dynamical systems (or not because of the time-dependency?); can somebody give a brief answer or direction on where to go looking? Many thanks in advance. Edit: The question is on mathoverflow now.","Let . When is it possible to find some , uniformly Lipschitz continuous w.r.t the second argument, such that if is the solution of the IVP we have ? I.e., what do we need from for it to be describable by the (time-dependent) flow given by an ODE? How can find that ODE? I figured these might be questions answered in the analysis of dynamical systems (or not because of the time-dependency?); can somebody give a brief answer or direction on where to go looking? Many thanks in advance. Edit: The question is on mathoverflow now.","f\colon\mathbb{R}^n\to\mathbb{R}^n g\in C^1([0,1]\times\mathbb{R}^n, \mathbb{R}^n) u_{x_0}  \dot{x}(t) = g(t,x(t)),\quad x(0) = x_0, f = x_0\mapsto u_{x_0}(1) f","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'initial-value-problems']"
2,"Stability of Mathieu equation: $x''(t)+\cos t \,x(t)=0$",Stability of Mathieu equation:,"x''(t)+\cos t \,x(t)=0","The equation $$ x''(t)+\cos t \,x(t)=0 \quad (1) $$ can be transformed to the system: $$\vec{x}'= \begin{pmatrix} 0 & 1\\ -\cos t & 0  \end{pmatrix} \vec{x}=A(t) \cdot x(t) $$ with minimum period $T=2\pi$ . Let $\mu_1,\mu_2$ be its characteristic values . A theorem gives: $$\mu_1\mu_2=\exp\Bigg\{\int_0^{2\pi} tr(A(t))dt\Bigg\}=1 \quad (2) $$ Therefore, the Wronskian of any two linearly independent solutions satisfies: $$ W(t+2\pi)=W(t) \quad (3) $$ Does $(3)$ imply that all solutions are bounded and thus we have asymptotic stability ? If not, in what way could we use $(2)$ to determine $(1)$ 's stability?","The equation can be transformed to the system: with minimum period . Let be its characteristic values . A theorem gives: Therefore, the Wronskian of any two linearly independent solutions satisfies: Does imply that all solutions are bounded and thus we have asymptotic stability ? If not, in what way could we use to determine 's stability?","
x''(t)+\cos t \,x(t)=0 \quad (1)
 \vec{x}'=
\begin{pmatrix}
0 & 1\\
-\cos t & 0 
\end{pmatrix} \vec{x}=A(t) \cdot x(t)
 T=2\pi \mu_1,\mu_2 \mu_1\mu_2=\exp\Bigg\{\int_0^{2\pi} tr(A(t))dt\Bigg\}=1 \quad (2)
 
W(t+2\pi)=W(t) \quad (3)
 (3) (2) (1)","['ordinary-differential-equations', 'dynamical-systems', 'periodic-functions', 'stability-in-odes']"
3,Analizing the stability of the equilibrium points of the system $\ddot{x}=(x-a)(x^2-a)$,Analizing the stability of the equilibrium points of the system,\ddot{x}=(x-a)(x^2-a),"$\require{amsmath}$ $\DeclareMathOperator{\Tr}{Tr}$ $\DeclareMathOperator{\Det}{Det}$ Investigate the stability of the equilibrium points of the system $\ddot{x}=(x-a)(x^2-a)$ for all real values of the parameter $a$ . (Hints: It might help to   graph the right-hand side. An alternative is to rewrite the equation as $\ddot{x}=−V′(x)$ for a suitable potential energy function $V$ and then use your intuition about particles   moving in potentials.) I am not really sure on how to approach the problem with the given hints, since it would require plotting the graph for different critical values for $a$ , which I am not really sure how to find. Thus, I am wondering if the following is correct. The system $\ddot{x}=(x-a)(x^2-a)$ can be re-written as $$\begin{cases} \dot{x}=y\\ \dot{y}=(x-a)(x^2-a) \end{cases}$$ with fixed points $P_1(a,0),\,P_2(\sqrt{a},0)$ and $P_3(\sqrt{a},0)$ . The Jacobian is $$J(x,y)=\begin{bmatrix} 0 &1\\ 3x^2-2ax-a &0 \end{bmatrix}$$ and thus $$J(a,0)=\begin{bmatrix} 0 &1\\ a^2-a &0 \end{bmatrix};\quad J(\sqrt{a},0)=\begin{bmatrix} 0 &1\\ 2a-2a^{3/2} &0 \end{bmatrix}; \quad J(-\sqrt{a},0)=\begin{bmatrix} 0 &1\\ 2a+2a^{3/2} &0 \end{bmatrix}.$$ It can be noticed that the $\Tr\left[J(x,y)\right]=0$ and that $$\begin{aligned} &1.\,\Det\left[J(a,0)\right]=a(1-a)\implies \text{Saddle for }a<0 \wedge a>1, \text{Center for }0<a<1.\\ &2.\,\Det\left[J(\sqrt{a},0)\right]=2a(\sqrt{a}-1)\implies \text{Saddle for }0<a<1, \text{Center for }a>1.\\ &3.\,\Det\left[J(-\sqrt{a},0)\right]=-2a(\sqrt{a}+1)\implies \text{Saddle for }a>0. \end{aligned}$$ If $a=0$ the system reduces to $\ddot{x}=x^3$ where the only fixed point is at $(0,0)$ and thus it is unstable. On the other hand, if $a=1$ then the system reduces to $\ddot{x}=x^3-x^2-x+1$ with fixed points at $(1,0),(-1,0)$ , both being unstable. Is my work correct?","Investigate the stability of the equilibrium points of the system for all real values of the parameter . (Hints: It might help to   graph the right-hand side. An alternative is to rewrite the equation as for a suitable potential energy function and then use your intuition about particles   moving in potentials.) I am not really sure on how to approach the problem with the given hints, since it would require plotting the graph for different critical values for , which I am not really sure how to find. Thus, I am wondering if the following is correct. The system can be re-written as with fixed points and . The Jacobian is and thus It can be noticed that the and that If the system reduces to where the only fixed point is at and thus it is unstable. On the other hand, if then the system reduces to with fixed points at , both being unstable. Is my work correct?","\require{amsmath} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\Det}{Det} \ddot{x}=(x-a)(x^2-a) a \ddot{x}=−V′(x) V a \ddot{x}=(x-a)(x^2-a) \begin{cases}
\dot{x}=y\\
\dot{y}=(x-a)(x^2-a)
\end{cases} P_1(a,0),\,P_2(\sqrt{a},0) P_3(\sqrt{a},0) J(x,y)=\begin{bmatrix}
0 &1\\
3x^2-2ax-a &0
\end{bmatrix} J(a,0)=\begin{bmatrix}
0 &1\\
a^2-a &0
\end{bmatrix};\quad J(\sqrt{a},0)=\begin{bmatrix}
0 &1\\
2a-2a^{3/2} &0
\end{bmatrix}; \quad J(-\sqrt{a},0)=\begin{bmatrix}
0 &1\\
2a+2a^{3/2} &0
\end{bmatrix}. \Tr\left[J(x,y)\right]=0 \begin{aligned}
&1.\,\Det\left[J(a,0)\right]=a(1-a)\implies \text{Saddle for }a<0 \wedge a>1, \text{Center for }0<a<1.\\
&2.\,\Det\left[J(\sqrt{a},0)\right]=2a(\sqrt{a}-1)\implies \text{Saddle for }0<a<1, \text{Center for }a>1.\\
&3.\,\Det\left[J(-\sqrt{a},0)\right]=-2a(\sqrt{a}+1)\implies \text{Saddle for }a>0.
\end{aligned} a=0 \ddot{x}=x^3 (0,0) a=1 \ddot{x}=x^3-x^2-x+1 (1,0),(-1,0)","['ordinary-differential-equations', 'analysis', 'dynamical-systems']"
4,Solving differential equation with linearization and Lyapunov method,Solving differential equation with linearization and Lyapunov method,,"For homework, I have to say something about the stability of the zero solution of the differential equation $v''+v+f(v')=0$ ,  where $f$ is a differentiable function satisfying $f(0)=0$ and $f'\geq0$ . I am asked to use the linearization method and if it leads nowhere, then try the Lyapunov method. The second one seems easier, I think that a function of the type $\frac{1}{2}\left ( (v')^{2}+v^2 \right )$ , or something like that including $f$ somehow, will offer a solution. But as far as the first method is concerned, I am stuck. How am I supposed to turn this system in the familiar form $\dot{y}=g(y)$ and linearize it? Any help would be very much appreciated.","For homework, I have to say something about the stability of the zero solution of the differential equation ,  where is a differentiable function satisfying and . I am asked to use the linearization method and if it leads nowhere, then try the Lyapunov method. The second one seems easier, I think that a function of the type , or something like that including somehow, will offer a solution. But as far as the first method is concerned, I am stuck. How am I supposed to turn this system in the familiar form and linearize it? Any help would be very much appreciated.",v''+v+f(v')=0 f f(0)=0 f'\geq0 \frac{1}{2}\left ( (v')^{2}+v^2 \right ) f \dot{y}=g(y),"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
5,Implicit functions and related differential equations,Implicit functions and related differential equations,,"I'm seeking guidance in derivation of implicit equation solutions to second degree differential equations.In the example below, differentiating twice just produced a tangle of terms which did not obviously lead to the required result. Example: If $$y^3 +3yx +2x^3 = 0, $$ prove that $$x^2(1+x^3)y'' - (3/2)xy' +y =0$$","I'm seeking guidance in derivation of implicit equation solutions to second degree differential equations.In the example below, differentiating twice just produced a tangle of terms which did not obviously lead to the required result. Example: If prove that","y^3 +3yx +2x^3 = 0,  x^2(1+x^3)y'' - (3/2)xy' +y =0","['ordinary-differential-equations', 'implicit-function']"
6,Using termwise (term-by-term) differentiation on an infinite series to satisfy a differential equation.,Using termwise (term-by-term) differentiation on an infinite series to satisfy a differential equation.,,I have a question which asks me to use termwise differentation on the series $$\sum_{n=0}^\infty\frac{(-1)^nx^{2n}}{(n!)^22^{2n}}$$ to show that it satisfies the differential equation $$x^2y''+xy'+x^2y=0$$ I dont understand what this question is asking me to do. I have found the interval and radius of convergence and the first 3 terms of this in previous questions if that is relevant at all? Can someone explain this to me or the method etc so that I know how to do complete it?,I have a question which asks me to use termwise differentation on the series to show that it satisfies the differential equation I dont understand what this question is asking me to do. I have found the interval and radius of convergence and the first 3 terms of this in previous questions if that is relevant at all? Can someone explain this to me or the method etc so that I know how to do complete it?,\sum_{n=0}^\infty\frac{(-1)^nx^{2n}}{(n!)^22^{2n}} x^2y''+xy'+x^2y=0,"['sequences-and-series', 'ordinary-differential-equations', 'derivatives', 'power-series', 'taylor-expansion']"
7,Is the solution of every differential equation CONTINUOUS in it's given domain?,Is the solution of every differential equation CONTINUOUS in it's given domain?,,I am a first reader of Differential Equation. I was solving a differential equation whose solution is $|f(x)|= c$ . Now my question is can I write that the solution is $f(x)= k$ . If $f(x)$ is continuous then I can  remove the mod. But I am not sure whether the function $f(x)$ is continuous or not. (Here $c$ and $k$ are constants) So that's why my question is the following.  Is the solution of every differential equation CONTINUOUS  in it's given domain? Can anyone please help me to understand this?,I am a first reader of Differential Equation. I was solving a differential equation whose solution is . Now my question is can I write that the solution is . If is continuous then I can  remove the mod. But I am not sure whether the function is continuous or not. (Here and are constants) So that's why my question is the following.  Is the solution of every differential equation CONTINUOUS  in it's given domain? Can anyone please help me to understand this?,|f(x)|= c f(x)= k f(x) f(x) c k,"['real-analysis', 'ordinary-differential-equations', 'continuity']"
8,Solving a system of ordinary differential equations,Solving a system of ordinary differential equations,,"Consider the simultaneous system of differential equations: $$ \begin{equation} x'(t)=y(t) -x(t)/2\\ y'(t)=x(t)/4-y(t)/2 \end{equation} $$ If $ x(0)=2 $ and $ y(0)=3 $ , then what is $ \lim_{t\to\infty}((x(t)+y(t)) $ ? Here is what I do: $$ \frac{dy}{dx}=\frac{\frac{1}{4}x-\frac{1}{2}y}{-\frac{1}{2}x+y}=-\frac{1}{2} $$ So $$ y=-\frac{1}{2}x+4 $$ and $$ x(t)+y(t)=\frac{1}{2}x(t)+4 .$$ Now solve for $ x(t) $ , we have $$ x(t)=4-\frac{2}{e^t} .$$ Hence $ \lim_{t\to\infty}((x(t)+y(t))=2+4=6 $ . However, there should be another method involving using matrices in the standard way. How to do it via matrices? The question is from:(14) of https://math.uchicago.edu/~min/GRE/files/week2.pdf","Consider the simultaneous system of differential equations: If and , then what is ? Here is what I do: So and Now solve for , we have Hence . However, there should be another method involving using matrices in the standard way. How to do it via matrices? The question is from:(14) of https://math.uchicago.edu/~min/GRE/files/week2.pdf"," \begin{equation}
x'(t)=y(t) -x(t)/2\\
y'(t)=x(t)/4-y(t)/2
\end{equation}   x(0)=2   y(0)=3   \lim_{t\to\infty}((x(t)+y(t))   \frac{dy}{dx}=\frac{\frac{1}{4}x-\frac{1}{2}y}{-\frac{1}{2}x+y}=-\frac{1}{2}   y=-\frac{1}{2}x+4   x(t)+y(t)=\frac{1}{2}x(t)+4 .  x(t)   x(t)=4-\frac{2}{e^t} .  \lim_{t\to\infty}((x(t)+y(t))=2+4=6 ",['ordinary-differential-equations']
9,Solve $f'(t)=0$ and $f'(t)=1$ using Fourier transform,Solve  and  using Fourier transform,f'(t)=0 f'(t)=1,"I'm trying to solve $f'(t)=0$ and $f'(t)=1$ using Fourier transform, but no luck: a) $f'(t)=0$ $$ f'(t)=0 \Rightarrow jwF(w)=0  \Rightarrow \begin{cases}F(w)=0 ~ \text{if} ~ w \ne 0\\ F(0) = \text{undefined ?} \end{cases} $$ Addendum from comments of @Winther (if I understood correctly): In case we choice $F(0) \ne \infty$ then $f(t)=0$ that is one of the valid solutions; if we choice $F(0) = \infty$ then $f(t)=1$ , another valid solution; no idea how to obtain all other possible solutions. b) $f'(t)=1$ $$ f'(t)=1 \Rightarrow jwF(w)=\delta(w)  \Rightarrow \begin{cases}F(w)=0 ~ \text{if} ~ w \ne 0\\ F(0) = \text{impossible! because left part is} =0 \text{ but right is }=\infty \end{cases} $$ After this disaster, I've tried to proof the results (calc the derivative using Fourier transform) that I expect as the correct solutions: c) $f(t)=c$ (being $c$ in $\mathbb{R}$ ) $$f(t)=c \Rightarrow\\ F(w)=c\delta(w) \Rightarrow\\ \mathscr{F}\{f'\}(w) = jwc\delta(w) = 0 \Rightarrow \\ f'(t) = 0  $$ success d) $f(t)=t+c$ $$f(t)=t+c\Rightarrow\\ F(w)=\delta^{(1)}(w)+c\delta(w)\Rightarrow\\ \mathscr{F}\{f'\}(w) = jw\delta^{(1)}(w)+jwc\delta(w) = jw\delta^{(1)}(w) = ~ ?  $$ ( where $\delta^{(1)}(x)=\frac{d}{dx}\delta(x)$ ) blocked","I'm trying to solve and using Fourier transform, but no luck: a) Addendum from comments of @Winther (if I understood correctly): In case we choice then that is one of the valid solutions; if we choice then , another valid solution; no idea how to obtain all other possible solutions. b) After this disaster, I've tried to proof the results (calc the derivative using Fourier transform) that I expect as the correct solutions: c) (being in ) success d) ( where ) blocked","f'(t)=0 f'(t)=1 f'(t)=0  f'(t)=0 \Rightarrow jwF(w)=0 
\Rightarrow \begin{cases}F(w)=0 ~ \text{if} ~ w \ne 0\\
F(0) = \text{undefined ?}
\end{cases}  F(0) \ne \infty f(t)=0 F(0) = \infty f(t)=1 f'(t)=1  f'(t)=1 \Rightarrow jwF(w)=\delta(w) 
\Rightarrow \begin{cases}F(w)=0 ~ \text{if} ~ w \ne 0\\
F(0) = \text{impossible! because left part is} =0 \text{ but right is }=\infty
\end{cases}  f(t)=c c \mathbb{R} f(t)=c \Rightarrow\\
F(w)=c\delta(w) \Rightarrow\\
\mathscr{F}\{f'\}(w) = jwc\delta(w) = 0 \Rightarrow \\
f'(t) = 0 
 f(t)=t+c f(t)=t+c\Rightarrow\\
F(w)=\delta^{(1)}(w)+c\delta(w)\Rightarrow\\
\mathscr{F}\{f'\}(w) = jw\delta^{(1)}(w)+jwc\delta(w) = jw\delta^{(1)}(w) = ~ ? 
 \delta^{(1)}(x)=\frac{d}{dx}\delta(x)","['ordinary-differential-equations', 'fourier-transform', 'dirac-delta']"
10,Fredholm Alternative for Singular ODE,Fredholm Alternative for Singular ODE,,"Consider the following inhomogeneous boundary value problem, $$t^2 u'' + tpu' +qu = f(t), \ t \in [-1,1], \ \ u(1) = \alpha, \ u(-1) = \beta,$$ where $p$ and $q$ are constants. I would like to determine a condition for the existence of a solution to this problem using the Fredholm alternative. To use it, I need to express the ODE above in self-adjoint form, $$-(a(t)u'(t))' + b(t)u = \tilde{f}.$$ However, doing so involves steps which are irreversible, i.e multiplying both sides of the differential equation by a power of $t$ , which may take the value $t = 0$ . This means that the self-adjoint equation is not equivalent to the original. Does this render the Fredholm alternative unusable? How could I determine a condition for the existence of a solution to this BVP?","Consider the following inhomogeneous boundary value problem, where and are constants. I would like to determine a condition for the existence of a solution to this problem using the Fredholm alternative. To use it, I need to express the ODE above in self-adjoint form, However, doing so involves steps which are irreversible, i.e multiplying both sides of the differential equation by a power of , which may take the value . This means that the self-adjoint equation is not equivalent to the original. Does this render the Fredholm alternative unusable? How could I determine a condition for the existence of a solution to this BVP?","t^2 u'' + tpu' +qu = f(t), \ t \in [-1,1], \ \ u(1) = \alpha, \ u(-1) = \beta, p q -(a(t)u'(t))' + b(t)u = \tilde{f}. t t = 0",['ordinary-differential-equations']
11,Existence of Hamiltonian for Planar Ordinary Differential Equations,Existence of Hamiltonian for Planar Ordinary Differential Equations,,"Consider the following planar ODE $$ \begin{cases} \dot x = f(x,y) \\ \dot y = g(x,y) \end{cases} $$ and suppose $ \frac{\partial f }{\partial x} +  \frac{\partial g }{\partial y} = 0$ . Is this a sufficient and necessary condition for the existence of a Hamiltonian (or any first integral) for the system?",Consider the following planar ODE and suppose . Is this a sufficient and necessary condition for the existence of a Hamiltonian (or any first integral) for the system?," \begin{cases} \dot x = f(x,y) \\ \dot y = g(x,y) \end{cases}   \frac{\partial f }{\partial x} +  \frac{\partial g }{\partial y} = 0","['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
12,Solve the differential equation $(4+t^2) \frac{dy}{dt} + 2ty = 4t$,Solve the differential equation,(4+t^2) \frac{dy}{dt} + 2ty = 4t,Solve the differential equation $$(4+t^2) \frac{dy}{dt} + 2ty = 4t$$ Solution: $$(4+t^2) \frac{dy}{dt} + 2ty = \frac{d}{dt}[(4+t^2)y]$$ How? Here's what I did: $$\frac{d}{dt}[(4+t^2)y] = 4t$$ Then we integrate both side: $$(4+t^2)y = 2t^2 + c$$ $$y = \frac{2t^2+c}{4+t^2}$$ I don't get the first step,Solve the differential equation Solution: How? Here's what I did: Then we integrate both side: I don't get the first step,(4+t^2) \frac{dy}{dt} + 2ty = 4t (4+t^2) \frac{dy}{dt} + 2ty = \frac{d}{dt}[(4+t^2)y] \frac{d}{dt}[(4+t^2)y] = 4t (4+t^2)y = 2t^2 + c y = \frac{2t^2+c}{4+t^2},['ordinary-differential-equations']
13,Theorema Egregium and coeficcients of the second fundamental form,Theorema Egregium and coeficcients of the second fundamental form,,"The Theorema Egregium says that Gaussian curvature $K$ of a regular surface $S$ is invariant under local isometries. We have a local description of the Gaussian curvature as follows $$K = \dfrac{eg-f^2}{EG-F^2}$$ where $E$ , $F$ , and $G$ are the coefficients of the first fundamental form of $S$ and $e$ , $f$ and $g$ the coefficients of the second fundamental form of $S$ . This implies that $eg-f^2$ is invariant under local isometries as well. I am following the proof of Manfredo's Differential Geometry of Curves and Surfaces for the theorema Egregium, but the end of the proof seems less natural to me that the proof of the fact that Christoffel's symbols are invariant under local isometries. My question: Is there another way of proving the quantity $eg-f^2$ depends only on Christoffel's symbols and the coefficients of the fundamental form, thus it is also invariant under local isometries? Thank you in advance. EDIT : Maybe I should add the following, it is not hard to see that, if $\chi: U \to S$ is a local parametrization compatible with some orientation $N$ on $S$ then $$eg-f^2 = \langle \chi_{u,u},\chi_{v,v} \rangle - \langle \chi_{u,v},\chi_{u,v} \rangle  + \mathrm{Christoffel's ~symbols}$$ Therefore my question reduces to showing that $\langle \chi_{u,u},\chi_{v,v} \rangle - \langle \chi_{u,v},\chi_{u,v} \rangle$ can be expressed in terms of Christoffel's symbols and the coefficients of the first fundamental form.","The Theorema Egregium says that Gaussian curvature of a regular surface is invariant under local isometries. We have a local description of the Gaussian curvature as follows where , , and are the coefficients of the first fundamental form of and , and the coefficients of the second fundamental form of . This implies that is invariant under local isometries as well. I am following the proof of Manfredo's Differential Geometry of Curves and Surfaces for the theorema Egregium, but the end of the proof seems less natural to me that the proof of the fact that Christoffel's symbols are invariant under local isometries. My question: Is there another way of proving the quantity depends only on Christoffel's symbols and the coefficients of the fundamental form, thus it is also invariant under local isometries? Thank you in advance. EDIT : Maybe I should add the following, it is not hard to see that, if is a local parametrization compatible with some orientation on then Therefore my question reduces to showing that can be expressed in terms of Christoffel's symbols and the coefficients of the first fundamental form.","K S K = \dfrac{eg-f^2}{EG-F^2} E F G S e f g S eg-f^2 eg-f^2 \chi: U \to S N S eg-f^2 = \langle \chi_{u,u},\chi_{v,v} \rangle - \langle \chi_{u,v},\chi_{u,v} \rangle  + \mathrm{Christoffel's ~symbols} \langle \chi_{u,u},\chi_{v,v} \rangle - \langle \chi_{u,v},\chi_{u,v} \rangle","['ordinary-differential-equations', 'riemannian-geometry', 'surfaces']"
14,while solving LDE why do we need to get the equation in the resolved form,while solving LDE why do we need to get the equation in the resolved form,,"It's most of the time quite easy to solve linear differential equations (LDE) thanks to all the result we have in linear algebra. Yet there is something I don't understand, let's say we have the following LDE : $$a_n(t)y^{(n)} +...+ a_0(t)y = b(t)$$ where $a_i$ are continuous functions. Then in my book they always put this equation in the following ""resolved form"" : $$y^{(n)} + \frac{a_{n-1}(t)}{a_n(t)} y^{(n-1)} +...+ \frac{a_0(t)}{a_n(t)} y = \frac{b(t)}{a_n(t)}$$ I am wondering why this is useful to get the equation in this form? I mean are there theorems in linear algebra that doesn't apply or techniques that don't work if we let the equation in the form : $$a_n(t)y^{(n)} +...+ a_0(t)y = b(t)~?$$ Thank you!","It's most of the time quite easy to solve linear differential equations (LDE) thanks to all the result we have in linear algebra. Yet there is something I don't understand, let's say we have the following LDE : where are continuous functions. Then in my book they always put this equation in the following ""resolved form"" : I am wondering why this is useful to get the equation in this form? I mean are there theorems in linear algebra that doesn't apply or techniques that don't work if we let the equation in the form : Thank you!",a_n(t)y^{(n)} +...+ a_0(t)y = b(t) a_i y^{(n)} + \frac{a_{n-1}(t)}{a_n(t)} y^{(n-1)} +...+ \frac{a_0(t)}{a_n(t)} y = \frac{b(t)}{a_n(t)} a_n(t)y^{(n)} +...+ a_0(t)y = b(t)~?,"['linear-algebra', 'ordinary-differential-equations']"
15,A specific homogeneous polar differential equation,A specific homogeneous polar differential equation,,"In an assignment of our school, we are asked to solve $$\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{x + y - 3}{x - y - 1}$$ by turning it into a homogeneous polar differential equation (equation of the form $\displaystyle \frac{\mathrm{d}y}{\mathrm{d}x} = F\left(\frac{y}{x}\right)$ ) using substitutions $x = X + a$ , $y = Y + b$ . My solution was: Firstly, I determined substitutions $x = X + 2$ , $y = Y + 1$ , such that $$\frac{\mathrm{d}Y}{\mathrm{d}X} = \frac{X + Y}{X - Y}$$ Then, let $Y = Xv$ , thus $$\begin{aligned} v + X\frac{\mathrm{d}v}{\mathrm{d}X} &= \frac{X + Xv}{X - Xv} \\ \int \frac{1 - v}{1 + v^2}\ \mathrm{d}v &= \int \frac{\mathrm{d}X}{X} \end{aligned}$$ The left-hand side, specifically, gives $$\int \frac{\mathrm{d}v}{1 + v^2} - \int \frac{v}{1 + v^2}\ \mathrm{d}v = \tan^{-1} v - \frac{\ln \left(1 + v^2\right)}{2} + \mathrm{constant}$$ Therefore, $$\tan^{-1} v - \frac{\ln \left(1 + v^2\right)}{2} = \ln \left|X\right| + \mathrm{constant}$$ i.e. $$2\tan^{-1} \frac{y - 1}{x - 2} = \ln \left[1 + \frac{\left(y - 1\right)^2}{\left(x - 2\right)^2}\right] + \ln \left(x - 2\right)^2 + \mathrm{constant}$$ However, our assignment didn't come with a standard solution, so I verified my answer with Wolfram Alpha, which gives $$ 2 \tan^{-1}\left(\frac{y(x) + x - 3}{-y(x) + x - 1}\right) = c_1 + \ln\left(\frac{x^2 + y(x)^2 - 2 y(x) - 4 x + 5}{2 \left(x - 2\right)^2}\right) + 2 \ln\left(x - 2\right)$$ which is different from my solution in the fraction inside function $\tan^{-1}$ is vastly different the denominator given by Wolfram Alpha inside the first $\ln$ is twice the denominator I gave the $x - 2$ in the last $\ln$ has no absolute value sign around it, but this seems a common problem of Wolfram Alpha solutions, so we can overlook it for the second May I know whether I'm wrong, or that this is a problem of the Wolfram Alpha solution? (or that the two solutions are actually equivalent, though seemingly very unlikely?)","In an assignment of our school, we are asked to solve by turning it into a homogeneous polar differential equation (equation of the form ) using substitutions , . My solution was: Firstly, I determined substitutions , , such that Then, let , thus The left-hand side, specifically, gives Therefore, i.e. However, our assignment didn't come with a standard solution, so I verified my answer with Wolfram Alpha, which gives which is different from my solution in the fraction inside function is vastly different the denominator given by Wolfram Alpha inside the first is twice the denominator I gave the in the last has no absolute value sign around it, but this seems a common problem of Wolfram Alpha solutions, so we can overlook it for the second May I know whether I'm wrong, or that this is a problem of the Wolfram Alpha solution? (or that the two solutions are actually equivalent, though seemingly very unlikely?)","\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{x + y - 3}{x - y - 1} \displaystyle \frac{\mathrm{d}y}{\mathrm{d}x} = F\left(\frac{y}{x}\right) x = X + a y = Y + b x = X + 2 y = Y + 1 \frac{\mathrm{d}Y}{\mathrm{d}X} = \frac{X + Y}{X - Y} Y = Xv \begin{aligned}
v + X\frac{\mathrm{d}v}{\mathrm{d}X} &= \frac{X + Xv}{X - Xv} \\
\int \frac{1 - v}{1 + v^2}\ \mathrm{d}v &= \int \frac{\mathrm{d}X}{X}
\end{aligned} \int \frac{\mathrm{d}v}{1 + v^2} - \int \frac{v}{1 + v^2}\ \mathrm{d}v = \tan^{-1} v - \frac{\ln \left(1 + v^2\right)}{2} + \mathrm{constant} \tan^{-1} v - \frac{\ln \left(1 + v^2\right)}{2} = \ln \left|X\right| + \mathrm{constant} 2\tan^{-1} \frac{y - 1}{x - 2} = \ln \left[1 + \frac{\left(y - 1\right)^2}{\left(x - 2\right)^2}\right] + \ln \left(x - 2\right)^2 + \mathrm{constant} 
2 \tan^{-1}\left(\frac{y(x) + x - 3}{-y(x) + x - 1}\right) = c_1 + \ln\left(\frac{x^2 + y(x)^2 - 2 y(x) - 4 x + 5}{2 \left(x - 2\right)^2}\right) + 2 \ln\left(x - 2\right) \tan^{-1} \ln x - 2 \ln","['ordinary-differential-equations', 'wolfram-alpha']"
16,Non-constant coefficient matrix in first order linear differential equations,Non-constant coefficient matrix in first order linear differential equations,,"I want to solve a differential equation of the following form  $$ \frac{d}{dt}x=A(t)x\, , $$ where $A(t)$ does not commute at different times. This equation holds on the interval $(a,b)$. Hence, the solution cannot be written as $e^{\int_a^td\tau A(\tau)}$. I have been searching now for several hours, and I only found (source: http://www.macs.hw.ac.uk/~simonm/linalg.pdf pg. 92) the so called ""Neumann series""  $$ x(t)=\left(I+\int_0^{t}A(\tau)d\tau + \int_0^{t}A(\tau_1)\int_0^{\tau_1}A(\tau_2)d\tau_2 d\tau_1+...\right) x_0\, . $$ (The series is very reminiscent of the Dyson series with the evolution operator in quantum mechanics, here $A(t)$ is not hermitian.) Here are my doubts: In the formula the lower bound is $0$. Is this part of the formula or can it be any number? in my case $a$. Is this an infinite sum? if yes how can I stop at a certain summand to have an approximation? How does this series continue? is it similar to the time ordering in quantum mechanics, i.e., one takes every possible commutation or is the third term just:  $$  \int_0^{t}A(\tau_1)\int_0^{\tau_1}A(\tau_2)\int_0^{\tau_2}A(\tau_3)d\tau_2d\tau_3 d\tau_1\, . $$ In the link above, it says, one can check that this is the solution by taking the derivative with respect to $t$. How can one take the derivative, when it depends on the upper bound of the integral and how is the product rule for non commutative matrices? Thank you very much.","I want to solve a differential equation of the following form  $$ \frac{d}{dt}x=A(t)x\, , $$ where $A(t)$ does not commute at different times. This equation holds on the interval $(a,b)$. Hence, the solution cannot be written as $e^{\int_a^td\tau A(\tau)}$. I have been searching now for several hours, and I only found (source: http://www.macs.hw.ac.uk/~simonm/linalg.pdf pg. 92) the so called ""Neumann series""  $$ x(t)=\left(I+\int_0^{t}A(\tau)d\tau + \int_0^{t}A(\tau_1)\int_0^{\tau_1}A(\tau_2)d\tau_2 d\tau_1+...\right) x_0\, . $$ (The series is very reminiscent of the Dyson series with the evolution operator in quantum mechanics, here $A(t)$ is not hermitian.) Here are my doubts: In the formula the lower bound is $0$. Is this part of the formula or can it be any number? in my case $a$. Is this an infinite sum? if yes how can I stop at a certain summand to have an approximation? How does this series continue? is it similar to the time ordering in quantum mechanics, i.e., one takes every possible commutation or is the third term just:  $$  \int_0^{t}A(\tau_1)\int_0^{\tau_1}A(\tau_2)\int_0^{\tau_2}A(\tau_3)d\tau_2d\tau_3 d\tau_1\, . $$ In the link above, it says, one can check that this is the solution by taking the derivative with respect to $t$. How can one take the derivative, when it depends on the upper bound of the integral and how is the product rule for non commutative matrices? Thank you very much.",,"['ordinary-differential-equations', 'matrix-calculus', 'noncommutative-algebra']"
17,ODE solution not unique,ODE solution not unique,,"Show that the solution of the initial value problem $$y'=-2\sin(x)\sqrt{y}, \quad y(0)=1, \quad y\in[0,2]$$ that are defined for all $x\in\mathbb{R}$ are not unique. I have found one such solution: $$\dfrac{dy}{dx}=-2\sin(x)\sqrt{y}$$ $$y=(\cos(x)+C)^2$$ combining with the initial value yields: $$y=(\cos(x))^2$$ Is it possible for me to find more? Is there a certain pattern here?","Show that the solution of the initial value problem $$y'=-2\sin(x)\sqrt{y}, \quad y(0)=1, \quad y\in[0,2]$$ that are defined for all $x\in\mathbb{R}$ are not unique. I have found one such solution: $$\dfrac{dy}{dx}=-2\sin(x)\sqrt{y}$$ $$y=(\cos(x)+C)^2$$ combining with the initial value yields: $$y=(\cos(x))^2$$ Is it possible for me to find more? Is there a certain pattern here?",,['ordinary-differential-equations']
18,How can I show that the system of non-linear differential equations does not have periodic orbits?,How can I show that the system of non-linear differential equations does not have periodic orbits?,,"The system is the following: $\left\{\begin{matrix} x'=&2x-x^5-xy^4 \\  y'=&y-y^3-x^2y  \end{matrix}\right.$. What I did was to find singular points in the system: from which I got the following singular points: $(0,0); (0,1);(0, -1);(\sqrt[4]{2},0);(\sqrt[4]{2},0)$. Then, I have classified them if they are attractors, chair, etc. But I do not know how to prove that the system does not have periodic orbits. Can someone guide me? Please. Thanks in advance. Best regards.","The system is the following: $\left\{\begin{matrix} x'=&2x-x^5-xy^4 \\  y'=&y-y^3-x^2y  \end{matrix}\right.$. What I did was to find singular points in the system: from which I got the following singular points: $(0,0); (0,1);(0, -1);(\sqrt[4]{2},0);(\sqrt[4]{2},0)$. Then, I have classified them if they are attractors, chair, etc. But I do not know how to prove that the system does not have periodic orbits. Can someone guide me? Please. Thanks in advance. Best regards.",,"['ordinary-differential-equations', 'nonlinear-system']"
19,Is Backward-Euler method considered the same as Runge Kutta $2^{\text{nd}}$ order method?,Is Backward-Euler method considered the same as Runge Kutta  order method?,2^{\text{nd}},"I have a book that quotes: Euler's method, Modified Euler's method and Runge's method are   Runge-Kutta methods of first, second and third order respectively. The   fourth-order Runge-Kutta method is method is most commonly used and is   often referred to as 'Runge-Kutta method' or 'classical Runge-Kutta   method' Similary Wikipedia categorizes Backward-Euler's method as ' Implicit methods' under the list of Runge-Kutta methods and also mentions: The backward Euler method is first order. Now the problem is that the same book (from which I have taken the above quote) solves the below problem using a method that seems quite different (at least to me) from the Backward-Euler's method. Consider the first order initial value problem   $y'=y+2x-x^{2}$,$y(0)=1$,$(0\le x\le\infty)$ with exact solution   $y(x)=x^2+e^x$. For $x=0.1$, what is solution obtained using a single   iteration of the second-order Runge-Kutta method with step size   $h=0.1$ The book then shows the solution using: $$k_1=hf(x_0,y_0)$$ $$k_2=hf(x_0+h,y_0+k_1)$$   $$y_1=y_0+\frac{1}{2}(k_1+k_2)$$ Here $f$ denotes the differential equation i.e. $y'=f(x,y)=y+2x-x^{2}$. Using the above equations and initial value, it gets the result as $y_1=1.1145$. I tried to calculate the vaule using Backward-Euler's method using: $$y_{1}=y_{0}+hf(x_{1},y_{1})$$ and I get the result as $y_1=1.1322$, which is different from the solution given in the book. So I have the following questions: Is Backward-Euler method considered the same as Runge-Kutta $2^{\text{nd}}$ order (RK2) method? If yes, is my book incorrect with the solution? Is the method used in the book the actual Runge-Kutta     $2^{\text{nd}}$ order method which is completely different from Backward-Euler's method? In case my first question's answer is yes , how can a method be a Runge-Kutta $2^{\text{nd}}$ order (RK2) while also being a  $1^{\text{st}}$ order in itself? (no need to answer if first question's answer is no ) I am really confused with the way the book used the name Backward Euler as RK2 but then used a different method to solve a question that wanted RK2. Please help me understand this. Note : My book states Backward Euler as Modified Euler's method (In case it's not so obvious).","I have a book that quotes: Euler's method, Modified Euler's method and Runge's method are   Runge-Kutta methods of first, second and third order respectively. The   fourth-order Runge-Kutta method is method is most commonly used and is   often referred to as 'Runge-Kutta method' or 'classical Runge-Kutta   method' Similary Wikipedia categorizes Backward-Euler's method as ' Implicit methods' under the list of Runge-Kutta methods and also mentions: The backward Euler method is first order. Now the problem is that the same book (from which I have taken the above quote) solves the below problem using a method that seems quite different (at least to me) from the Backward-Euler's method. Consider the first order initial value problem   $y'=y+2x-x^{2}$,$y(0)=1$,$(0\le x\le\infty)$ with exact solution   $y(x)=x^2+e^x$. For $x=0.1$, what is solution obtained using a single   iteration of the second-order Runge-Kutta method with step size   $h=0.1$ The book then shows the solution using: $$k_1=hf(x_0,y_0)$$ $$k_2=hf(x_0+h,y_0+k_1)$$   $$y_1=y_0+\frac{1}{2}(k_1+k_2)$$ Here $f$ denotes the differential equation i.e. $y'=f(x,y)=y+2x-x^{2}$. Using the above equations and initial value, it gets the result as $y_1=1.1145$. I tried to calculate the vaule using Backward-Euler's method using: $$y_{1}=y_{0}+hf(x_{1},y_{1})$$ and I get the result as $y_1=1.1322$, which is different from the solution given in the book. So I have the following questions: Is Backward-Euler method considered the same as Runge-Kutta $2^{\text{nd}}$ order (RK2) method? If yes, is my book incorrect with the solution? Is the method used in the book the actual Runge-Kutta     $2^{\text{nd}}$ order method which is completely different from Backward-Euler's method? In case my first question's answer is yes , how can a method be a Runge-Kutta $2^{\text{nd}}$ order (RK2) while also being a  $1^{\text{st}}$ order in itself? (no need to answer if first question's answer is no ) I am really confused with the way the book used the name Backward Euler as RK2 but then used a different method to solve a question that wanted RK2. Please help me understand this. Note : My book states Backward Euler as Modified Euler's method (In case it's not so obvious).",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods', 'eulers-method']"
20,Are Eigenvalues Invariant?,Are Eigenvalues Invariant?,,"Question: are the eigenvalues of a dynamical system invariant under a change of variables? More specifically, consider a dynamical system A defined on a manifold $M_A$ by the evolution function $\phi_A(t,x)$ for all $x \in M_A$. Let $g$ be a diffeomorphism from $M_A$ to a different manifold $M_B$ and define the dynamical system $B$ on the manifold $M_B$ through the evolution function $\phi_B(t,y) = g \circ \phi_A \circ g^{-1}$ for $y \in M_B$. I can see that the fixed points of system A map one-to-one to the fixed points of system B. For instance if a fixed point is stable in A, the corresponding fixed point in B will also be stable. Are the eigenvalues of the fixed points also the same in the two systems?","Question: are the eigenvalues of a dynamical system invariant under a change of variables? More specifically, consider a dynamical system A defined on a manifold $M_A$ by the evolution function $\phi_A(t,x)$ for all $x \in M_A$. Let $g$ be a diffeomorphism from $M_A$ to a different manifold $M_B$ and define the dynamical system $B$ on the manifold $M_B$ through the evolution function $\phi_B(t,y) = g \circ \phi_A \circ g^{-1}$ for $y \in M_B$. I can see that the fixed points of system A map one-to-one to the fixed points of system B. For instance if a fixed point is stable in A, the corresponding fixed point in B will also be stable. Are the eigenvalues of the fixed points also the same in the two systems?",,"['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems', 'fixed-points', 'lyapunov-functions']"
21,Does $x'' = x^3$ have an analytical solution?,Does  have an analytical solution?,x'' = x^3,"I encountered this differential equation $x'' = x^3$ during one of my work and couldn't find an analytical solution to the above. I've used numerical methods to solve the equation in the end. I was just wondering if there is any way to find an analytical solution or show that an elementary solution doesn't exist. WolframAlpha expresses the answer with Jacobi theta functions, and I was wondering if that's the only way to express the answer.","I encountered this differential equation $x'' = x^3$ during one of my work and couldn't find an analytical solution to the above. I've used numerical methods to solve the equation in the end. I was just wondering if there is any way to find an analytical solution or show that an elementary solution doesn't exist. WolframAlpha expresses the answer with Jacobi theta functions, and I was wondering if that's the only way to express the answer.",,['ordinary-differential-equations']
22,Finding a First Integral of the Lotka-Volterra System,Finding a First Integral of the Lotka-Volterra System,,"For the Lotka-Volterra system below   $$\frac{dF}{dt}=-aF+\alpha FR$$   $$\frac{dR}{dt}=bR-\beta FR$$   show that $V=R^aF^b e^{-\alpha R-\beta F}$ is a first integral, that is, $V(t)$ is constant along any trajectory. What can you conclude about the behaviour of the solutions? My attempt: $$\frac{dF}{dt}=-aF+\alpha FR \iff \frac{dF}{-aF+\alpha FR}=dt \ \ \ \ \ (1)$$ $$\frac{dR}{dt}=bR-\beta FR\iff \frac{dR}{bR-\beta FR}=dt \ \ \ \ \ \  \ \ \ \ \ \ (2)$$ Equating $(1)$ and $(2)$ \begin{align}  \frac{dF}{-aF+\alpha FR}&=\frac{dR}{bR-\beta FR} \\ \int \frac{b}{F}-\beta \ dF&=\int -\frac{a}{R}+\alpha \ dR \\ \ln|F^b|+\ln|R^a|&=\alpha R+\beta F+C \\ F^bR^a&=e^{\alpha R+\beta F}e^C \\ V&=R^aF^be^{-\alpha R-\beta F} \ \ \ \ \ (V=e^C\in\mathbb{R}) \end{align} Is this a correct method? I do not know what this tells us about the behaiour of the solutions.","For the Lotka-Volterra system below   $$\frac{dF}{dt}=-aF+\alpha FR$$   $$\frac{dR}{dt}=bR-\beta FR$$   show that $V=R^aF^b e^{-\alpha R-\beta F}$ is a first integral, that is, $V(t)$ is constant along any trajectory. What can you conclude about the behaviour of the solutions? My attempt: $$\frac{dF}{dt}=-aF+\alpha FR \iff \frac{dF}{-aF+\alpha FR}=dt \ \ \ \ \ (1)$$ $$\frac{dR}{dt}=bR-\beta FR\iff \frac{dR}{bR-\beta FR}=dt \ \ \ \ \ \  \ \ \ \ \ \ (2)$$ Equating $(1)$ and $(2)$ \begin{align}  \frac{dF}{-aF+\alpha FR}&=\frac{dR}{bR-\beta FR} \\ \int \frac{b}{F}-\beta \ dF&=\int -\frac{a}{R}+\alpha \ dR \\ \ln|F^b|+\ln|R^a|&=\alpha R+\beta F+C \\ F^bR^a&=e^{\alpha R+\beta F}e^C \\ V&=R^aF^be^{-\alpha R-\beta F} \ \ \ \ \ (V=e^C\in\mathbb{R}) \end{align} Is this a correct method? I do not know what this tells us about the behaiour of the solutions.",,['ordinary-differential-equations']
23,On a differential inequality,On a differential inequality,,"Let $A>0$. If $f$ satisfies the differential inequality $f^{\prime\prime}(t)+f(t)≥A$, and $y$ is the solution to the ODE  $y^{\prime\prime}(t)+y(t)=A$ with $y(0)=f(0)$ and $y^{\prime}(0)=f^{\prime}(0)$, then \begin{equation}\label{GP}  \begin{cases}  f(t)\leq y(t) \quad \text{for all $t<0$}\\ f(t)\geq y(t) \quad \text{for all $t>0$}. \end{cases}  \end{equation} Question: How can I prove this result?","Let $A>0$. If $f$ satisfies the differential inequality $f^{\prime\prime}(t)+f(t)≥A$, and $y$ is the solution to the ODE  $y^{\prime\prime}(t)+y(t)=A$ with $y(0)=f(0)$ and $y^{\prime}(0)=f^{\prime}(0)$, then \begin{equation}\label{GP}  \begin{cases}  f(t)\leq y(t) \quad \text{for all $t<0$}\\ f(t)\geq y(t) \quad \text{for all $t>0$}. \end{cases}  \end{equation} Question: How can I prove this result?",,"['ordinary-differential-equations', 'integral-inequality']"
24,Differential equation: $x''=\frac{2x}{x^2-1}$,Differential equation:,x''=\frac{2x}{x^2-1},"I want to solve the differential equation$$\begin{cases}x''=\frac{2x}{x^2-1}\\x'(0)=0\\x(0)=x_0\end{cases}$$ This is what I have done so far. I have not studied differential equation much, and introducing the function $s$ below is just a trick that I learned, but I'm not sure why/if it works. Let $s:=x'$. Then  $$\frac{d^2x}{dt^2}=\frac{ds}{dt}=\frac{ds}{dx}\frac{dx}{dt}=\frac{ds}{dx}s$$ so the above equation becomes $$s\frac{ds}{dx}=\frac{2x}{x^2-1}$$ $$s\,ds=\frac{2x}{x^2-1}\,dx$$$$\int s\,ds=\int\frac{2x}{x^2-1}\,dx+C$$ $$s^2=\log\lvert x^2-1\rvert+C$$$$x'=\omega\sqrt{\log\lvert x^2-1\rvert+C},\,\omega:\mathbb R_+\mapsto\{-1,1\}$$ and with $x'(0)=0$, $x(0)=x_0$ we have $$x'=\omega\sqrt{\log\bigg\lvert\frac{x^2-1}{x_0^2-1}\bigg\rvert}$$ How can I continue to solve for $x(t)$? And am I justified in introducing $s$ and treat the notation like I did to obtain $x'$?","I want to solve the differential equation$$\begin{cases}x''=\frac{2x}{x^2-1}\\x'(0)=0\\x(0)=x_0\end{cases}$$ This is what I have done so far. I have not studied differential equation much, and introducing the function $s$ below is just a trick that I learned, but I'm not sure why/if it works. Let $s:=x'$. Then  $$\frac{d^2x}{dt^2}=\frac{ds}{dt}=\frac{ds}{dx}\frac{dx}{dt}=\frac{ds}{dx}s$$ so the above equation becomes $$s\frac{ds}{dx}=\frac{2x}{x^2-1}$$ $$s\,ds=\frac{2x}{x^2-1}\,dx$$$$\int s\,ds=\int\frac{2x}{x^2-1}\,dx+C$$ $$s^2=\log\lvert x^2-1\rvert+C$$$$x'=\omega\sqrt{\log\lvert x^2-1\rvert+C},\,\omega:\mathbb R_+\mapsto\{-1,1\}$$ and with $x'(0)=0$, $x(0)=x_0$ we have $$x'=\omega\sqrt{\log\bigg\lvert\frac{x^2-1}{x_0^2-1}\bigg\rvert}$$ How can I continue to solve for $x(t)$? And am I justified in introducing $s$ and treat the notation like I did to obtain $x'$?",,['ordinary-differential-equations']
25,"Non homogeneous Sturm-Liouville problem and solution: Is the solution given in terms of $\lambda$, not particular eigenvalues?","Non homogeneous Sturm-Liouville problem and solution: Is the solution given in terms of , not particular eigenvalues?",\lambda,"Standard form of non homogeneous S.L. $$\dfrac{d}{dx}(p(x)y'(x))+(q(x)+\lambda r(x))y(x)=f(x)\tag1$$ with Boundary Conditions: let's say $(0<x<1)$+ B.C. The derivation of the solution is given as following, if you know this you can skip. For example we found the eigenvalues and corresponding eigenfunctions solving homogeneous S.L. : $\lambda_n\to \phi_n(x)$ and we assume that solution can be in form of these eigenfunctions. Such that $$y(x)=\displaystyle\sum_n^\infty a_n\phi_n(x)\tag2$$ Playing with eq (1)   $$\dfrac{d}{dx}(p(x)y'(x))+(q(x)+\lambda_n r(x)+\lambda r(x)-\lambda_n r(x))y(x)=f(x)$$ And plugging into the assuming solution (2) $$\displaystyle\sum_n^\infty a_n \left\{\underbrace{\dfrac{d}{dx}(p(x)\phi_n'(x))+(q(x)+\lambda_n r(x))\phi_n(x)}_{0}+(\lambda r(x)-\lambda_n r(x))\phi_n(x)\right\}=f(x)$$ Then: $$\displaystyle\sum_n^\infty a_n(\lambda-\lambda_n)r(x)\phi_n(x)=f(x) \tag3$$ and using orthogonality property: $$\displaystyle\int_0^1r(x)\phi_n(x)\phi_m(x)dx=\delta_{mn}$$ Then from eq (3) we get $$a_n=\dfrac1{(\lambda-\lambda_n)}\displaystyle\int_0^1 f(x)\phi_n(x)dx\tag4$$ So we have the solution: $$y(x)=\displaystyle\sum_n^\infty \left\{\dfrac{\phi_n(x)}{(\lambda-\lambda_n)}\displaystyle\int_0^1 f(x)\phi_n(x)dx\right\}\tag5$$ Question: What is work of $\lambda$ in the eq (5)? What is the difference between $\lambda$ and $\lambda_n$, do we just left the solution like this?","Standard form of non homogeneous S.L. $$\dfrac{d}{dx}(p(x)y'(x))+(q(x)+\lambda r(x))y(x)=f(x)\tag1$$ with Boundary Conditions: let's say $(0<x<1)$+ B.C. The derivation of the solution is given as following, if you know this you can skip. For example we found the eigenvalues and corresponding eigenfunctions solving homogeneous S.L. : $\lambda_n\to \phi_n(x)$ and we assume that solution can be in form of these eigenfunctions. Such that $$y(x)=\displaystyle\sum_n^\infty a_n\phi_n(x)\tag2$$ Playing with eq (1)   $$\dfrac{d}{dx}(p(x)y'(x))+(q(x)+\lambda_n r(x)+\lambda r(x)-\lambda_n r(x))y(x)=f(x)$$ And plugging into the assuming solution (2) $$\displaystyle\sum_n^\infty a_n \left\{\underbrace{\dfrac{d}{dx}(p(x)\phi_n'(x))+(q(x)+\lambda_n r(x))\phi_n(x)}_{0}+(\lambda r(x)-\lambda_n r(x))\phi_n(x)\right\}=f(x)$$ Then: $$\displaystyle\sum_n^\infty a_n(\lambda-\lambda_n)r(x)\phi_n(x)=f(x) \tag3$$ and using orthogonality property: $$\displaystyle\int_0^1r(x)\phi_n(x)\phi_m(x)dx=\delta_{mn}$$ Then from eq (3) we get $$a_n=\dfrac1{(\lambda-\lambda_n)}\displaystyle\int_0^1 f(x)\phi_n(x)dx\tag4$$ So we have the solution: $$y(x)=\displaystyle\sum_n^\infty \left\{\dfrac{\phi_n(x)}{(\lambda-\lambda_n)}\displaystyle\int_0^1 f(x)\phi_n(x)dx\right\}\tag5$$ Question: What is work of $\lambda$ in the eq (5)? What is the difference between $\lambda$ and $\lambda_n$, do we just left the solution like this?",,"['ordinary-differential-equations', 'sturm-liouville']"
26,How do I linearise the rational function to analyze the critical points?,How do I linearise the rational function to analyze the critical points?,,"For the system $$\frac{dx}{dt}=\frac{3xy}{1+x^2+y^2}-\frac{1+x^2}{1+y^2}\\\frac{dy}{dt}=x^2-y^2,$$ the point $\begin{pmatrix}1\\1\end{pmatrix}$ is A. an unstable node B. a stable node C. a saddle point D. a stable spiral point E. an unstable spiral point. I understand I need to shift the point to the origin and just eliminate $x^2$ and $y^2$ , but how do I deal with denominators? Thank you","For the system the point is A. an unstable node B. a stable node C. a saddle point D. a stable spiral point E. an unstable spiral point. I understand I need to shift the point to the origin and just eliminate and , but how do I deal with denominators? Thank you","\frac{dx}{dt}=\frac{3xy}{1+x^2+y^2}-\frac{1+x^2}{1+y^2}\\\frac{dy}{dt}=x^2-y^2, \begin{pmatrix}1\\1\end{pmatrix} x^2 y^2","['calculus', 'ordinary-differential-equations', 'linearization']"
27,Generalized eigenvector in a differential equation system,Generalized eigenvector in a differential equation system,,"This is the system: $$\begin{cases} \dot{x}=x+2y+e^{-t}\\ \dot{y}=2x+y+1 \end{cases}$$ Now I first solve the homogeneous one, without the vector $(e^{-t},1)$, so I have to find the eigenvalues of the matrix $$\begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix}$$ which are $(1-\lambda)^2-4=\lambda^2-2\lambda-3$ and $\lambda_1=3$, $\lambda_2=-1$. By the first one I obtain the eigenvector $u=(1,1)$ for example, for the second one I obtain the matrix $$\begin{pmatrix} 0 & 2\\ 2 & 0 \end{pmatrix}$$ that has null eigenvector. Now, I find the generalized eigenvector: $(A-\lambda I_d)u_1=u$ and obtain $$\begin{cases} 2u_1^1=1\\ 2u_2^2=1 \end{cases}$$ I chose the eigenvector $u_1=(\frac{1}{2},0)$ is it correct? I think no, because going on in the resolution of the equation I have to write the $A$ matrix of the system as a sum $A=P+N$ where $P=Sdiag[\lambda_i]S^{-1}$, $S=\begin{pmatrix} 1 & \frac{1}{2}\\ 1 & 0\end{pmatrix}$ and $N$ is nihilipotent of order 2 and then: $$e^{At}=e^{Pt}e^{Nt}=Sdiag[e^{\lambda_i}]S^{-1}(I_d+Nt)$$ I usually do like this when the eigenvalues of the matrix have multiplicity, is it correct to solve like this in this case? And if not, what is the correct method? Thanks and sorry for bad anglisk","This is the system: $$\begin{cases} \dot{x}=x+2y+e^{-t}\\ \dot{y}=2x+y+1 \end{cases}$$ Now I first solve the homogeneous one, without the vector $(e^{-t},1)$, so I have to find the eigenvalues of the matrix $$\begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix}$$ which are $(1-\lambda)^2-4=\lambda^2-2\lambda-3$ and $\lambda_1=3$, $\lambda_2=-1$. By the first one I obtain the eigenvector $u=(1,1)$ for example, for the second one I obtain the matrix $$\begin{pmatrix} 0 & 2\\ 2 & 0 \end{pmatrix}$$ that has null eigenvector. Now, I find the generalized eigenvector: $(A-\lambda I_d)u_1=u$ and obtain $$\begin{cases} 2u_1^1=1\\ 2u_2^2=1 \end{cases}$$ I chose the eigenvector $u_1=(\frac{1}{2},0)$ is it correct? I think no, because going on in the resolution of the equation I have to write the $A$ matrix of the system as a sum $A=P+N$ where $P=Sdiag[\lambda_i]S^{-1}$, $S=\begin{pmatrix} 1 & \frac{1}{2}\\ 1 & 0\end{pmatrix}$ and $N$ is nihilipotent of order 2 and then: $$e^{At}=e^{Pt}e^{Nt}=Sdiag[e^{\lambda_i}]S^{-1}(I_d+Nt)$$ I usually do like this when the eigenvalues of the matrix have multiplicity, is it correct to solve like this in this case? And if not, what is the correct method? Thanks and sorry for bad anglisk",,"['ordinary-differential-equations', 'systems-of-equations', 'generalized-eigenvector']"
28,Solution of the ode,Solution of the ode,,"Consider the IVP $$y'=h(t)y(t)$$ with $y(0)=1$ initial condition and $h(t)=1$ for $ t\geq 0 $ and $0$ elsewhere. Prove that it does not have a solution. Sollution: Let $y$ be a sollution then i get $y(t)=e^t$, $t \geq 0$ and 1 else. but that $y$ is not differentiable zero. hence no sollution. But isn't the $$y=1+ \int_{0}^{t}h(s)y(s)ds $$ a solution, or equivalent to the problem? And  now  the function $y(t)=e^t$ $t \geq 0$ and 1 else is a solution to the $y=1+ \int_{0}^{t}h(s)y(s)ds $ I think im confused . Can someone explain me the difference between the integral form and the ODE??","Consider the IVP $$y'=h(t)y(t)$$ with $y(0)=1$ initial condition and $h(t)=1$ for $ t\geq 0 $ and $0$ elsewhere. Prove that it does not have a solution. Sollution: Let $y$ be a sollution then i get $y(t)=e^t$, $t \geq 0$ and 1 else. but that $y$ is not differentiable zero. hence no sollution. But isn't the $$y=1+ \int_{0}^{t}h(s)y(s)ds $$ a solution, or equivalent to the problem? And  now  the function $y(t)=e^t$ $t \geq 0$ and 1 else is a solution to the $y=1+ \int_{0}^{t}h(s)y(s)ds $ I think im confused . Can someone explain me the difference between the integral form and the ODE??",,"['calculus', 'ordinary-differential-equations']"
29,Topological transitivity implies existence of dense forward orbit (understanding proof),Topological transitivity implies existence of dense forward orbit (understanding proof),,I'm trying to understand the following proof(see picture 1). We need to show that there exists a $y\in X$ such that $\overline{O^+(y)}=X$. But in the whole proof there is no $y$ mentioned . Can anyone explain why the forward orbit of $y$ is dense ? many thanks in advance The proof uses the following proposition,I'm trying to understand the following proof(see picture 1). We need to show that there exists a $y\in X$ such that $\overline{O^+(y)}=X$. But in the whole proof there is no $y$ mentioned . Can anyone explain why the forward orbit of $y$ is dense ? many thanks in advance The proof uses the following proposition,,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
30,"Find 2nd order homogenous ODE from solutions $x^2, e^{-x}$",Find 2nd order homogenous ODE from solutions,"x^2, e^{-x}","I'm trying to find the 2nd order ODE given those two solutions. I usually achieve this by finding the characteristic polynomial by multiplying the root factors, however the solution $x^2$ is giving me some trouble. As far as I understand, $x^2$ implies that $0$ is a triple root, but by finding the characteristic polynomial $x^3(x+1)$ I only get a 4th degree non quadratic equation, which I do not know how to ""convert"" to a 2nd degree one. Am I doing it wrong or should another method be used, ie. differentiating the solutions up to the 2nd derivative and proceeding by trying different polinomials to multiply each derivative? Thanks in advance.","I'm trying to find the 2nd order ODE given those two solutions. I usually achieve this by finding the characteristic polynomial by multiplying the root factors, however the solution $x^2$ is giving me some trouble. As far as I understand, $x^2$ implies that $0$ is a triple root, but by finding the characteristic polynomial $x^3(x+1)$ I only get a 4th degree non quadratic equation, which I do not know how to ""convert"" to a 2nd degree one. Am I doing it wrong or should another method be used, ie. differentiating the solutions up to the 2nd derivative and proceeding by trying different polinomials to multiply each derivative? Thanks in advance.",,"['ordinary-differential-equations', 'problem-solving']"
31,Lecture to solve 2nd order differential equation in matrix form.,Lecture to solve 2nd order differential equation in matrix form.,,I have an system of three differential equation coupled. I put this system to matrix form. I think it should be more easy to solve. $$ \begin{bmatrix}     \ddot x_1 \\      \ddot x_2 \\     \ddot x_3 \\     \end{bmatrix} =     \begin{bmatrix}     0 & a_{12} & -a_{31} \\      -a_{12} & 0 & a_{23} \\     a_{31} & -a_{23} & 0 \\     \end{bmatrix} \begin{bmatrix}     \dot x_1 \\      \dot x_2 \\     \dot x_3 \\     \end{bmatrix}+\begin{bmatrix}     c_1 \\      c_2 \\     c_3 \\     \end{bmatrix} $$ All constants are positive or zero. Could someone tell me what I should read to get the knowledge to solve this differential equation ? Thanks for your help. Tof Edit: $a_{23} -> -a_{23} $,I have an system of three differential equation coupled. I put this system to matrix form. I think it should be more easy to solve. $$ \begin{bmatrix}     \ddot x_1 \\      \ddot x_2 \\     \ddot x_3 \\     \end{bmatrix} =     \begin{bmatrix}     0 & a_{12} & -a_{31} \\      -a_{12} & 0 & a_{23} \\     a_{31} & -a_{23} & 0 \\     \end{bmatrix} \begin{bmatrix}     \dot x_1 \\      \dot x_2 \\     \dot x_3 \\     \end{bmatrix}+\begin{bmatrix}     c_1 \\      c_2 \\     c_3 \\     \end{bmatrix} $$ All constants are positive or zero. Could someone tell me what I should read to get the knowledge to solve this differential equation ? Thanks for your help. Tof Edit: $a_{23} -> -a_{23} $,,"['ordinary-differential-equations', 'matrix-equations']"
32,Difference equation solution,Difference equation solution,,My question : $y_{n+1}= ay_n+b$ ;  $y_0 = \alpha$ We solved this difference equation in a class and got that $y_n = a^nc + \beta$ Can someone please explain the way how to solve it? And what is the was to solve differential equations in general? Is there some good literature with difference equations theory and examples?,My question : $y_{n+1}= ay_n+b$ ;  $y_0 = \alpha$ We solved this difference equation in a class and got that $y_n = a^nc + \beta$ Can someone please explain the way how to solve it? And what is the was to solve differential equations in general? Is there some good literature with difference equations theory and examples?,,['ordinary-differential-equations']
33,Uniqueness of solutions of Cauchy problem with nonzero function,Uniqueness of solutions of Cauchy problem with nonzero function,,"Consider the initial value problem $$y'(x)=f(x,y(x)), \\ y(x_0)=y_0,$$ where the function $f \colon D \to \mathbb R$ is defined and continuous on some open set $D \subseteq \mathbb R \times \mathbb R$ and $(x_0, y_0) \in D$. Is the following statement true? This problem cannot have two distinct solutions on some interval $[x_0, x_1]$ if $$\forall (x,y) \in D \colon f(x, y) \ne 0.$$ If $f$ does not depend on $x$, the answer seems to be positive, i.e. we have some kind of uniqueness theorem here, but my intuition tells me that it is, generally, wrong. Can you provide a counterexample?","Consider the initial value problem $$y'(x)=f(x,y(x)), \\ y(x_0)=y_0,$$ where the function $f \colon D \to \mathbb R$ is defined and continuous on some open set $D \subseteq \mathbb R \times \mathbb R$ and $(x_0, y_0) \in D$. Is the following statement true? This problem cannot have two distinct solutions on some interval $[x_0, x_1]$ if $$\forall (x,y) \in D \colon f(x, y) \ne 0.$$ If $f$ does not depend on $x$, the answer seems to be positive, i.e. we have some kind of uniqueness theorem here, but my intuition tells me that it is, generally, wrong. Can you provide a counterexample?",,"['ordinary-differential-equations', 'cauchy-problem']"
34,Solution of $\nabla^2u=u_{xx}+u_{yy}=0$?,Solution of ?,\nabla^2u=u_{xx}+u_{yy}=0,"Calculate the Fourier expansion of $u$ for $$\nabla^2u=u_{xx}+u_{yy}=0,\\ y\ge0,0\le x\le L\\ u(0,y)=0=u_x(L,y), u(x,0)=g(x)=x/L$$. Solution: By separation of variables, we propose the solution $u(x,y)=X(x)Y(y)$. After calculations and after applying the conditions I get that  the general solution is $u(x,y)=\sum_{n=0}^\infty A_ne^{\sqrt{(\lambda_n)}y}D_n\sin(\sqrt \lambda_nx),$ where $A_n=\frac{(g(x),X_n(x))}{\Vert X_n\Vert^2}, X_n(x)=\sin(\frac{(n+1/2)\pi}{L}x),\lambda_n=(\frac{n\pi}{L})^2$ I found $A_n$ when I applied the boundary condition $u(x,0)$. Is there a way to find $D_n$ so the solution would be complete? Or the solution it's fine as it is? Thanks in advance for your time and help.","Calculate the Fourier expansion of $u$ for $$\nabla^2u=u_{xx}+u_{yy}=0,\\ y\ge0,0\le x\le L\\ u(0,y)=0=u_x(L,y), u(x,0)=g(x)=x/L$$. Solution: By separation of variables, we propose the solution $u(x,y)=X(x)Y(y)$. After calculations and after applying the conditions I get that  the general solution is $u(x,y)=\sum_{n=0}^\infty A_ne^{\sqrt{(\lambda_n)}y}D_n\sin(\sqrt \lambda_nx),$ where $A_n=\frac{(g(x),X_n(x))}{\Vert X_n\Vert^2}, X_n(x)=\sin(\frac{(n+1/2)\pi}{L}x),\lambda_n=(\frac{n\pi}{L})^2$ I found $A_n$ when I applied the boundary condition $u(x,0)$. Is there a way to find $D_n$ so the solution would be complete? Or the solution it's fine as it is? Thanks in advance for your time and help.",,"['ordinary-differential-equations', 'partial-differential-equations']"
35,Is the fixed point at the origin of this dynamical system asymptotically stable?,Is the fixed point at the origin of this dynamical system asymptotically stable?,,"I am given a dynamical system $$\dot x = f(x,y)= x - (1+\theta(x))x^3-y \\ \dot y =g(x,y)= y - 3x^2y + x$$ where $\theta(x)$ is a step function which is equal to $1$ when $x \geq 0$ and $0$ when $x<0$. Now I am asked to prove whether or not the fixed point at the origin is asymptotically stable. My thinking so far is as follows. 1) I know that when $x<|\sqrt{\frac{2}{3}}|$ that there can not exist a periodic orbit due to Bendixson's criterion 2) I have found that $\nabla \cdot \pmatrix{f(x,y) \\ g(x,y)} = 2$ when $x<0$ and $\nabla \cdot \pmatrix{f(x,y) \\ g(x,y)} = 2-3x^2$ when $x \geq 0$ 3) I know that if there exists a strict Liapounov function around the fixed point then the fixed point is asymptotically stable 4) Not sure if this is relevant but Poincare bendixson states that if there exists a non empty closed and bounded omega limit set then there is either a fixed point or a periodic orbit. Now I know that there can't be a periodic orbit so there must be a fixed point What I think I need to do I think I need to find a strict Liapounov function which will then allow me to state the fixed point is asymptotically stable. How I am going to find this function is still up in the air, but maybe it's something to do with the fact that the orbital derviative $$\frac{dV}{dt} = \int_{\phi(t,D)} d^nx(\nabla \cdot \pmatrix{f(x,y) \\ g(x,y)})$$ where $\phi(t,D)$ is the region obtained by evolving all the points in a set $D$.","I am given a dynamical system $$\dot x = f(x,y)= x - (1+\theta(x))x^3-y \\ \dot y =g(x,y)= y - 3x^2y + x$$ where $\theta(x)$ is a step function which is equal to $1$ when $x \geq 0$ and $0$ when $x<0$. Now I am asked to prove whether or not the fixed point at the origin is asymptotically stable. My thinking so far is as follows. 1) I know that when $x<|\sqrt{\frac{2}{3}}|$ that there can not exist a periodic orbit due to Bendixson's criterion 2) I have found that $\nabla \cdot \pmatrix{f(x,y) \\ g(x,y)} = 2$ when $x<0$ and $\nabla \cdot \pmatrix{f(x,y) \\ g(x,y)} = 2-3x^2$ when $x \geq 0$ 3) I know that if there exists a strict Liapounov function around the fixed point then the fixed point is asymptotically stable 4) Not sure if this is relevant but Poincare bendixson states that if there exists a non empty closed and bounded omega limit set then there is either a fixed point or a periodic orbit. Now I know that there can't be a periodic orbit so there must be a fixed point What I think I need to do I think I need to find a strict Liapounov function which will then allow me to state the fixed point is asymptotically stable. How I am going to find this function is still up in the air, but maybe it's something to do with the fact that the orbital derviative $$\frac{dV}{dt} = \int_{\phi(t,D)} d^nx(\nabla \cdot \pmatrix{f(x,y) \\ g(x,y)})$$ where $\phi(t,D)$ is the region obtained by evolving all the points in a set $D$.",,"['general-topology', 'ordinary-differential-equations', 'dynamical-systems', 'self-learning', 'lyapunov-functions']"
36,How to solve $2c x + x\dot x + (c^2-1)t = cd$,How to solve,2c x + x\dot x + (c^2-1)t = cd,"After computation on a certain problem, for a specific case I came across the differential equation $$2c x + x \dot x + (c^2-1) t = cd$$ with initial conditions $0 < x(0) = d $ and  $1 < - \dot x(0) = c $. Does this have a closed form? This differential equation doesn't appear to conform to solution by factors, integrating factors, Clairaut's equation, homogeneous methods, differentiating methods, Riccati's equation, or any other special form or method I know of.","After computation on a certain problem, for a specific case I came across the differential equation $$2c x + x \dot x + (c^2-1) t = cd$$ with initial conditions $0 < x(0) = d $ and  $1 < - \dot x(0) = c $. Does this have a closed form? This differential equation doesn't appear to conform to solution by factors, integrating factors, Clairaut's equation, homogeneous methods, differentiating methods, Riccati's equation, or any other special form or method I know of.",,['ordinary-differential-equations']
37,What is wrong in my solution of the following ODE : $y' = \tan(x)y + \sin(x)$,What is wrong in my solution of the following ODE :,y' = \tan(x)y + \sin(x),"I tried to solve the following simple equation using another approach which seems correct to me but I'm getting a different result than the standard approach : The ODE : $$ y' = \tan(x) y + \sin(x) $$ on $I = ] -\pi / 2 ; \pi/2[$ By standard approach I mean just solving the homogeneous equation then using variation of constant to find a particular solution but I tried something else : I turned the equation to $y' - \tan(x)y = \sin(x)$ I divided both sides on $\tan(x)$ for all $x$ different from $0$, I got $y' - y = \cos(x)$ I solved $y' - y = 0$ , intuitively, $ke^x$ is a solution I looked for a particular solution by supposing that $y = a\cos(x) + b\sin(x)$ and replaced in the equation to find $a,b$ by comparison to second member : $cos(x)$ I joined both solutions : $y = ke^x - 1/2 \cos(x) - 1/2 \sin(x) $ for all $x$s different from $0$. I then tried to solve it for $0$, I directly replaced $0$ in the initial equation, I got $y' = 0y + 0$ thus $y = \lambda$. So for $x$ in $I - \{0\} $ we have $y = ke^x - 1/2 cos(x) - 1/2 sin(x) $ otherwise it's just $y = \lambda $. I can't find the error in what I've done so please tell me what's wrong .","I tried to solve the following simple equation using another approach which seems correct to me but I'm getting a different result than the standard approach : The ODE : $$ y' = \tan(x) y + \sin(x) $$ on $I = ] -\pi / 2 ; \pi/2[$ By standard approach I mean just solving the homogeneous equation then using variation of constant to find a particular solution but I tried something else : I turned the equation to $y' - \tan(x)y = \sin(x)$ I divided both sides on $\tan(x)$ for all $x$ different from $0$, I got $y' - y = \cos(x)$ I solved $y' - y = 0$ , intuitively, $ke^x$ is a solution I looked for a particular solution by supposing that $y = a\cos(x) + b\sin(x)$ and replaced in the equation to find $a,b$ by comparison to second member : $cos(x)$ I joined both solutions : $y = ke^x - 1/2 \cos(x) - 1/2 \sin(x) $ for all $x$s different from $0$. I then tried to solve it for $0$, I directly replaced $0$ in the initial equation, I got $y' = 0y + 0$ thus $y = \lambda$. So for $x$ in $I - \{0\} $ we have $y = ke^x - 1/2 cos(x) - 1/2 sin(x) $ otherwise it's just $y = \lambda $. I can't find the error in what I've done so please tell me what's wrong .",,"['ordinary-differential-equations', 'proof-verification']"
38,Non-existence of limit cycle of a polynomial system,Non-existence of limit cycle of a polynomial system,,"I have been assigned a project in which I need to study the following system: $$\begin{cases}\dot{x} = x(ax^n + by^n + c)\\\dot{y} = y(dx^n + ey^n + f)\end{cases}$$ where $(a,b,c,d,e,f) \in \mathbb{R}^6$ and $n \in \mathbb{N}$. I'm being asked to find a relationship between the parameters of the system, $\phi(a,b,d,c,e,f,n)$ such that if $\phi(a,b,c,d,e,f,n) = 0$ the system may have periodic orbits but cannot contain limit cycles. I've tried using Bendixson-Dulac 's Theorem in order to find a function $B$ and an open set $U$ homeomorphic to a crown (disc with a hole) around every critic point and, perhaps, move on from there knowing there might exist periodic orbits. However, I don't believe this is even close to a possible solution and don't really know how to move on. Any hints on how I could be facing this problem are more than welcome. Thanks!","I have been assigned a project in which I need to study the following system: $$\begin{cases}\dot{x} = x(ax^n + by^n + c)\\\dot{y} = y(dx^n + ey^n + f)\end{cases}$$ where $(a,b,c,d,e,f) \in \mathbb{R}^6$ and $n \in \mathbb{N}$. I'm being asked to find a relationship between the parameters of the system, $\phi(a,b,d,c,e,f,n)$ such that if $\phi(a,b,c,d,e,f,n) = 0$ the system may have periodic orbits but cannot contain limit cycles. I've tried using Bendixson-Dulac 's Theorem in order to find a function $B$ and an open set $U$ homeomorphic to a crown (disc with a hole) around every critic point and, perhaps, move on from there knowing there might exist periodic orbits. However, I don't believe this is even close to a possible solution and don't really know how to move on. Any hints on how I could be facing this problem are more than welcome. Thanks!",,"['ordinary-differential-equations', 'systems-of-equations', 'limit-cycles']"
39,Geodesics of Fisher-Rao metric on the open interior of the finite-dimensional simplex.,Geodesics of Fisher-Rao metric on the open interior of the finite-dimensional simplex.,,"I am curious about the explicit form of the geodesics of the Fisher-Rao metric tensor on the open interior of the n-dimensional simplex. In the 2-dimensional case (only 1 parameter on the 2-simplex), it is easy to explicitely compute them, however, starting from the 3-dimensional case, the situation becomes very complicated. Since I am not an expert in this field, I am not able to see if the complexity of the situation depends on my ignorance, or it is intrinsic of the problem. Hence, I ask you if there are some general results that are known, for instance, some particular explicit form, the qualitative behaviour, the completeness, and so on. Thank You","I am curious about the explicit form of the geodesics of the Fisher-Rao metric tensor on the open interior of the n-dimensional simplex. In the 2-dimensional case (only 1 parameter on the 2-simplex), it is easy to explicitely compute them, however, starting from the 3-dimensional case, the situation becomes very complicated. Since I am not an expert in this field, I am not able to see if the complexity of the situation depends on my ignorance, or it is intrinsic of the problem. Hence, I ask you if there are some general results that are known, for instance, some particular explicit form, the qualitative behaviour, the completeness, and so on. Thank You",,"['probability', 'ordinary-differential-equations', 'statistics', 'differential-geometry', 'geodesic']"
40,Is it possible solve equation?`,Is it possible solve equation?`,,Is it possible and how to solve equation: $$p\cdot\dfrac{-32}{1+64(1-z)}+\sqrt{1+64\cdot(1-z)}\dfrac{dp}{dz}=\dfrac{-32\cdot\pi\cdot8}{\sqrt{1+64(1-z)}}$$ I need $p$ as results and $p=p(z)$. I am confused because I cannot extract variables on one side of equations? How else I can solve it?,Is it possible and how to solve equation: $$p\cdot\dfrac{-32}{1+64(1-z)}+\sqrt{1+64\cdot(1-z)}\dfrac{dp}{dz}=\dfrac{-32\cdot\pi\cdot8}{\sqrt{1+64(1-z)}}$$ I need $p$ as results and $p=p(z)$. I am confused because I cannot extract variables on one side of equations? How else I can solve it?,,"['integration', 'ordinary-differential-equations', 'derivatives']"
41,Minimization of functional using Euler-Lagrange,Minimization of functional using Euler-Lagrange,,"We've recently started doing Calculus of Variations in my analysis class and we're applying it to minimizing/maximizing functions. So the way we generally were taught to tackle the problem is to first find the Euler-Lagrange equation, solve the differential equation, then check concavity/convexity to ensure uniqueness. I'm having some trouble on the following question: (note: y with the circle thing on top means y') Problem 4. Solve the minimization problem   $$ \min \int_1^2 \left(y^2 + 2t\dot y y + 4 t^2 {\dot y}^2\right) dt , \; y(1) = 3, \; y(2)=2 $$ My attempt: I can't find where I'm going wrong because I'm ending up with a differential equation whose solutions (when I solve the characteristic equations) don't involve t at all, which is problematic. Any help at all would be great! :)","We've recently started doing Calculus of Variations in my analysis class and we're applying it to minimizing/maximizing functions. So the way we generally were taught to tackle the problem is to first find the Euler-Lagrange equation, solve the differential equation, then check concavity/convexity to ensure uniqueness. I'm having some trouble on the following question: (note: y with the circle thing on top means y') Problem 4. Solve the minimization problem   $$ \min \int_1^2 \left(y^2 + 2t\dot y y + 4 t^2 {\dot y}^2\right) dt , \; y(1) = 3, \; y(2)=2 $$ My attempt: I can't find where I'm going wrong because I'm ending up with a differential equation whose solutions (when I solve the characteristic equations) don't involve t at all, which is problematic. Any help at all would be great! :)",,"['ordinary-differential-equations', 'optimization', 'calculus-of-variations', 'euler-lagrange-equation']"
42,"Solving $u_t=ku_{xx}$ for $\ t\ge 0,-\infty<x<\infty$",Solving  for,"u_t=ku_{xx} \ t\ge 0,-\infty<x<\infty","Solve $$u_t=ku_{xx}\\u(x,0)=g(x)$$ for $t\ge 0, -\infty<x<\infty$, where $$g(x) = \begin{cases} 1, \quad \lvert x \rvert < 1 \\ 0, \quad \lvert x \rvert > 1 \end{cases}$$ Solution. We have that the solution is given by  \begin{align} u(x,t) &= \frac{1}{\sqrt{4\pi kt}}\int_{-1}^1e^{-(x-y)^2/4kt} \cdot 1 dy \\ &= \frac{-1}{\sqrt\pi}\int_{\frac{x-1}{\sqrt{4kt}}}^{\frac{x+1}{\sqrt{4kt}}} e^{-p^2}dp \end{align} if we consider  $p=\frac{(x-y)}{\sqrt{4kt}}$ in the solution formula. Is my solution correct so far? If yes, how do I integrate the integral with the $p$ variable? I was thinking to separate the integral like $$\int_{-a}^a=\int_{-a}^0+\int_0^a$$ but I'm not sure that $$\frac{x-1}{\sqrt{4kt}}<0$$","Solve $$u_t=ku_{xx}\\u(x,0)=g(x)$$ for $t\ge 0, -\infty<x<\infty$, where $$g(x) = \begin{cases} 1, \quad \lvert x \rvert < 1 \\ 0, \quad \lvert x \rvert > 1 \end{cases}$$ Solution. We have that the solution is given by  \begin{align} u(x,t) &= \frac{1}{\sqrt{4\pi kt}}\int_{-1}^1e^{-(x-y)^2/4kt} \cdot 1 dy \\ &= \frac{-1}{\sqrt\pi}\int_{\frac{x-1}{\sqrt{4kt}}}^{\frac{x+1}{\sqrt{4kt}}} e^{-p^2}dp \end{align} if we consider  $p=\frac{(x-y)}{\sqrt{4kt}}$ in the solution formula. Is my solution correct so far? If yes, how do I integrate the integral with the $p$ variable? I was thinking to separate the integral like $$\int_{-a}^a=\int_{-a}^0+\int_0^a$$ but I'm not sure that $$\frac{x-1}{\sqrt{4kt}}<0$$",,"['calculus', 'integration']"
43,"Nonlinear ODE $y''+y'+\frac {1}{y}=0$, $\int yy''dy$ and $\int yy''dt$","Nonlinear ODE ,  and",y''+y'+\frac {1}{y}=0 \int yy''dy \int yy''dt,"$$y''(t)+y'(t)+\frac {1}{y(t)}=0$$ (by y' and y'' I mean $\frac{dy}{dt}$ and $\frac{d^2y}{dt^2}$ ) First, I only know basic-ish Calculus but I'm willing to do some reading. Second, I'd appreciate some help with my (extra?) doubts (marked with ""‡""). Third, this and this are kinda similar and I understand how they work but I want the function that multiplies $y''$ to be the same (or a multiple but unless that makes it easier, let's stick with this) as the one that multiplies $y'$ . A square in the first derivative $y'$ actually makes sense for what I was trying to do but I'm not sure if it'll simplify things or if these are helpful here: $$(yy')'=yy''+y'y'$$ $$(y'/y)=\frac{y''}y-\frac{y'^2}{y^2}$$ My failed attempt(s)--- I know that not all nonlinear DEs are [nicely?] solvable but I just wanna make sure. My issue with this one is that if we integrate both sides in function of $t$ we get $$\int \frac{d}{dt}\left(\frac{dy}{dt}\right) dt+\int\frac{dy}{dt}dt+\int\frac {1}{y}dt=0\Rightarrow$$ $$y'+y+\int\frac {1}{y}dt=0$$ and I can't get rid of the last bit ‡[I've heard that you should be cautious with simplifying differentials like dt and dy algebraically but I don't understand why. Can you give me some examples?]. In function of y: $$\int \frac{d}{dt}\left(\frac{dy}{dt}\right) dy+\int\frac{dy}{dt}dy+\int\frac {1}{y}dy=0\Rightarrow$$ $$\int \frac{d}{dt}\left(\frac{dy}{dt}\right) dy+\int\frac{dy}{dt}dy+\ln|y|=0$$ and I can't get rid of the first two bits. If we multiply the equation by $y$ : $$yy''+yy'+1=0$$ Now, to successfully integrate in function of something we need to be able to solve either $$\int yy''+yy' dt$$ or $$\int yy''+yy' dy$$ . Let's try in function of $t$ first. $$\int yy' dt=\int y\frac{dy}{dt}dt=\frac {y^2}{2}+C$$ , so there's that. (integrating by parts with the integral I want on the left side and yes, it might be weird but I think it's so much simpler) $$\int yy'' dt=yy'-\int y'y' dt$$ or $$\int yy'' dt=y''\int y dt-\int (y'''\int y dt)dt$$ $\int y'y'dt=y'y-\int y''y dt$ Substituting this into the nicer one above: $$\int yy'' dt=yy'-\bigg(y'y-\int y''y dt\bigg)$$ , which cancels out and makes me sad Trying to integrate by parts like this also doesn't work: $\int yy''*1 dt$ . Let's try in function of $y$ . ‡[Now, for the stuff above, Wolfram Alpha didn't spit out anything useful but it gave me some results for the integrations by parts below I don't think are right. Is my boi WA wrong?] $$\int yy' dy=y\int \frac{dy}{dt}dy-\int 1(\int \frac{dy}{dt}dy)dy$$ or $$\int yy' dy=y'\frac{y^2}{2}-\int \frac{d}{dy}\frac{dy}{dt}\frac{y^2}{2}dy=y'\frac{y^2}{2}-\int \frac{y^2}{2}d\frac{dy}{dt}$$ $$\int \frac{y^2}{2}*1d\frac{dy}{dt}=\frac{y^2}{2}y'-\int \frac{d(y^2/2)}{dy'}y'dy'=\frac{y^2}{2}y'-\int yy' dy$$ Substituting above: $$\int yy' dy=y'\frac{y^2}{2}-(\frac{y^2}{2}y'-\int yy' dy)$$ , which leads us to the stunning conclusion that $0=0$ . Nevertheless, Wolfram Alpha says that $\int yy' dy=\frac{y^2}{2}y'+C$ but if $y=t^3$ , $$\int yy' dy=\int t^3(3t^2)d(t^3)=\int t^3(3t^2)(3t^2)dt=\frac{9t^8}{8}+C\neq\frac{3t^8}{2}$$ . Kinda close, though? It also says that $$\int yy'' dy=\frac{y^2}{2}y''+C$$ . Using that info to try to solve our equation that's multiplied by $y$ , we can only further complicate the problem. I've tried doing $u=\frac{1}{y}$ and $u=x'$ but those didn't seem to lead anywhere. [PS: Also, this doesn't come from a textbook or something, I just thought about a Physics thing (which I'm not even sure is reasonable) and got stuck on the math part. I've become more interested in the mathematics behind it than on whether or not it's useful, so the original situation doesn't matter anymore. But do see the comments if you're curious. PPS: Formatting math expressions is a thing that rhymes with ""itch"". But thanks for reading!]","(by y' and y'' I mean and ) First, I only know basic-ish Calculus but I'm willing to do some reading. Second, I'd appreciate some help with my (extra?) doubts (marked with ""‡""). Third, this and this are kinda similar and I understand how they work but I want the function that multiplies to be the same (or a multiple but unless that makes it easier, let's stick with this) as the one that multiplies . A square in the first derivative actually makes sense for what I was trying to do but I'm not sure if it'll simplify things or if these are helpful here: My failed attempt(s)--- I know that not all nonlinear DEs are [nicely?] solvable but I just wanna make sure. My issue with this one is that if we integrate both sides in function of we get and I can't get rid of the last bit ‡[I've heard that you should be cautious with simplifying differentials like dt and dy algebraically but I don't understand why. Can you give me some examples?]. In function of y: and I can't get rid of the first two bits. If we multiply the equation by : Now, to successfully integrate in function of something we need to be able to solve either or . Let's try in function of first. , so there's that. (integrating by parts with the integral I want on the left side and yes, it might be weird but I think it's so much simpler) or Substituting this into the nicer one above: , which cancels out and makes me sad Trying to integrate by parts like this also doesn't work: . Let's try in function of . ‡[Now, for the stuff above, Wolfram Alpha didn't spit out anything useful but it gave me some results for the integrations by parts below I don't think are right. Is my boi WA wrong?] or Substituting above: , which leads us to the stunning conclusion that . Nevertheless, Wolfram Alpha says that but if , . Kinda close, though? It also says that . Using that info to try to solve our equation that's multiplied by , we can only further complicate the problem. I've tried doing and but those didn't seem to lead anywhere. [PS: Also, this doesn't come from a textbook or something, I just thought about a Physics thing (which I'm not even sure is reasonable) and got stuck on the math part. I've become more interested in the mathematics behind it than on whether or not it's useful, so the original situation doesn't matter anymore. But do see the comments if you're curious. PPS: Formatting math expressions is a thing that rhymes with ""itch"". But thanks for reading!]",y''(t)+y'(t)+\frac {1}{y(t)}=0 \frac{dy}{dt} \frac{d^2y}{dt^2} y'' y' y' (yy')'=yy''+y'y' (y'/y)=\frac{y''}y-\frac{y'^2}{y^2} t \int \frac{d}{dt}\left(\frac{dy}{dt}\right) dt+\int\frac{dy}{dt}dt+\int\frac {1}{y}dt=0\Rightarrow y'+y+\int\frac {1}{y}dt=0 \int \frac{d}{dt}\left(\frac{dy}{dt}\right) dy+\int\frac{dy}{dt}dy+\int\frac {1}{y}dy=0\Rightarrow \int \frac{d}{dt}\left(\frac{dy}{dt}\right) dy+\int\frac{dy}{dt}dy+\ln|y|=0 y yy''+yy'+1=0 \int yy''+yy' dt \int yy''+yy' dy t \int yy' dt=\int y\frac{dy}{dt}dt=\frac {y^2}{2}+C \int yy'' dt=yy'-\int y'y' dt \int yy'' dt=y''\int y dt-\int (y'''\int y dt)dt \int y'y'dt=y'y-\int y''y dt \int yy'' dt=yy'-\bigg(y'y-\int y''y dt\bigg) \int yy''*1 dt y \int yy' dy=y\int \frac{dy}{dt}dy-\int 1(\int \frac{dy}{dt}dy)dy \int yy' dy=y'\frac{y^2}{2}-\int \frac{d}{dy}\frac{dy}{dt}\frac{y^2}{2}dy=y'\frac{y^2}{2}-\int \frac{y^2}{2}d\frac{dy}{dt} \int \frac{y^2}{2}*1d\frac{dy}{dt}=\frac{y^2}{2}y'-\int \frac{d(y^2/2)}{dy'}y'dy'=\frac{y^2}{2}y'-\int yy' dy \int yy' dy=y'\frac{y^2}{2}-(\frac{y^2}{2}y'-\int yy' dy) 0=0 \int yy' dy=\frac{y^2}{2}y'+C y=t^3 \int yy' dy=\int t^3(3t^2)d(t^3)=\int t^3(3t^2)(3t^2)dt=\frac{9t^8}{8}+C\neq\frac{3t^8}{2} \int yy'' dy=\frac{y^2}{2}y''+C y u=\frac{1}{y} u=x',"['calculus', 'integration', 'ordinary-differential-equations', 'solution-verification', 'nonlinear-system']"
44,Does local stability of a unique equilibrium in a bounded region implies global stability?,Does local stability of a unique equilibrium in a bounded region implies global stability?,,"If I have a non-linear system of ODE's, $\dot{\mathbf{x}} = f(\mathbf{x})$, where $f(\mathbf{\cdot})$ is a smooth non-linear function. Additionally, I know that $\mathbf{x}$ is bounded and the system has only a unique steady state that is locally asymptotically stable, $\mathbf{x}^*$. Is it wrong to conclude that $\mathbf{x}^*$ is globally asymptotically stable? I feel like it is too naive.","If I have a non-linear system of ODE's, $\dot{\mathbf{x}} = f(\mathbf{x})$, where $f(\mathbf{\cdot})$ is a smooth non-linear function. Additionally, I know that $\mathbf{x}$ is bounded and the system has only a unique steady state that is locally asymptotically stable, $\mathbf{x}^*$. Is it wrong to conclude that $\mathbf{x}^*$ is globally asymptotically stable? I feel like it is too naive.",,"['ordinary-differential-equations', 'dynamical-systems']"
45,"Solution for $\frac{dy}{dx}=\frac{1}{y^2}-\frac{1}{x^2}$, change of variables, integrating factor","Solution for , change of variables, integrating factor",\frac{dy}{dx}=\frac{1}{y^2}-\frac{1}{x^2},"I encountered the following nonlinear 1st order ODE as a simplified case of this question . $$\frac{dy}{dx}=\frac{1}{y^2}-\frac{1}{x^2}$$ I didn't find it in the literature and Wolfram Alpha couldn't solve it either. However, I believe it might have a solution,. Replace: $$y= r \sin t, \qquad x=r \cos t$$ After simplifications, this leads to the following ODE: $$(r^2 \sin^3 t \cos^2 t+\sin^2 t \cos t- \cos^3 t) dr+\left(r^3 \sin^2 t \cos^3 t+r( \sin t \cos^2 t-\sin^3 t) \right)dt=0$$ $$P(r,t) dr+Q(r,t)dt=0$$ Checking if the new ODE is exact or not, we find that it's not, but the partial derivatives look promising: $$P_t =r^2 (3 \sin^2 t \cos^3 t-2 \sin^4 t \cos t) +5 \sin t \cos^2 t-\sin^3 t$$ $$Q_r =r^2 (3 \sin^2 t \cos^3 t) +\sin t \cos^2 t-\sin^3 t$$ I feel like there's a simple integrating factor we can use here, but I have no idea what it is. Can we find an integrating factor which makes the last ODE exact? If we can, what is it? Is there another way to obtain an explicit or implicit solution for the titular ODE in terms of known functions? What about the more general case ( $a,b$ - constants)? $$\frac{dy}{dx}=\frac{a}{y^2}-\frac{b}{x^2}$$ Edit Multiplying by $\frac{1}{\cos^2 t}$ we obtain: $$\left(r^2 \sin^3 t +\frac{\sin^2 t }{\cos t}- \cos t\right) dr+\left(r^3 \sin^2 t \cos t+r\left( \sin t -\frac{\sin^3 t}{\cos^2 t}\right) \right)dt=0$$ Now we have: $$P_t=3 r^2 \sin^2 t \cos t+ \sin t +\frac{\sin^3 t }{\cos^2 t}$$ $$Q_r=3 r^2 \sin^2 t \cos t+ \sin t -\frac{\sin^3 t }{\cos^2 t}$$ It's almost exact. Did I make a mistake somewhere? Or do we need a more elaborate integrating factor? Edit 2 Hyperbolic substitution: $$y= r \sinh t, \qquad x=r \cosh t$$ leads to a similar equation, which is still not exact: $$\left(r^2 \sinh^3 t-\frac{1}{\cosh t} \right) dr+\left(r^3 \sinh^2 t \cosh t-r \frac{\sinh t}{\cosh^2 t} \right) dt=0$$ $$P_t=3 r^2 \sinh^2 t \cosh t +\frac{\sinh t }{\cosh^2 t}$$ $$Q_r=3 r^2 \sinh^2 t \cosh t -\frac{\sinh t }{\cosh^2 t}$$","I encountered the following nonlinear 1st order ODE as a simplified case of this question . I didn't find it in the literature and Wolfram Alpha couldn't solve it either. However, I believe it might have a solution,. Replace: After simplifications, this leads to the following ODE: Checking if the new ODE is exact or not, we find that it's not, but the partial derivatives look promising: I feel like there's a simple integrating factor we can use here, but I have no idea what it is. Can we find an integrating factor which makes the last ODE exact? If we can, what is it? Is there another way to obtain an explicit or implicit solution for the titular ODE in terms of known functions? What about the more general case ( - constants)? Edit Multiplying by we obtain: Now we have: It's almost exact. Did I make a mistake somewhere? Or do we need a more elaborate integrating factor? Edit 2 Hyperbolic substitution: leads to a similar equation, which is still not exact:","\frac{dy}{dx}=\frac{1}{y^2}-\frac{1}{x^2} y= r \sin t, \qquad x=r \cos t (r^2 \sin^3 t \cos^2 t+\sin^2 t \cos t- \cos^3 t) dr+\left(r^3 \sin^2 t \cos^3 t+r( \sin t \cos^2 t-\sin^3 t) \right)dt=0 P(r,t) dr+Q(r,t)dt=0 P_t =r^2 (3 \sin^2 t \cos^3 t-2 \sin^4 t \cos t) +5 \sin t \cos^2 t-\sin^3 t Q_r =r^2 (3 \sin^2 t \cos^3 t) +\sin t \cos^2 t-\sin^3 t a,b \frac{dy}{dx}=\frac{a}{y^2}-\frac{b}{x^2} \frac{1}{\cos^2 t} \left(r^2 \sin^3 t +\frac{\sin^2 t }{\cos t}- \cos t\right) dr+\left(r^3 \sin^2 t \cos t+r\left( \sin t -\frac{\sin^3 t}{\cos^2 t}\right) \right)dt=0 P_t=3 r^2 \sin^2 t \cos t+ \sin t +\frac{\sin^3 t }{\cos^2 t} Q_r=3 r^2 \sin^2 t \cos t+ \sin t -\frac{\sin^3 t }{\cos^2 t} y= r \sinh t, \qquad x=r \cosh t \left(r^2 \sinh^3 t-\frac{1}{\cosh t} \right) dr+\left(r^3 \sinh^2 t \cosh t-r \frac{\sinh t}{\cosh^2 t} \right) dt=0 P_t=3 r^2 \sinh^2 t \cosh t +\frac{\sinh t }{\cosh^2 t} Q_r=3 r^2 \sinh^2 t \cosh t -\frac{\sinh t }{\cosh^2 t}","['ordinary-differential-equations', 'integrating-factor']"
46,"Differential equation problem, how to solve such types?","Differential equation problem, how to solve such types?",,"$$y'=\frac {y(x-y\log(y))}{x(x\log(x)-y)}$$ Its a high school mathematics question. I tried solving by many methods but failed. It is not a homogeneous equation, so not in parametric form. I think it can be solved by inspection.....but when i tried I was stuck in a loop and couldn't progress forward in the question. How to do ? Options(4): $(log(x)÷x) + or -(log(y)÷y)=c$ $(x.logx +or - ylogy)÷xy=c$","$$y'=\frac {y(x-y\log(y))}{x(x\log(x)-y)}$$ Its a high school mathematics question. I tried solving by many methods but failed. It is not a homogeneous equation, so not in parametric form. I think it can be solved by inspection.....but when i tried I was stuck in a loop and couldn't progress forward in the question. How to do ? Options(4): $(log(x)÷x) + or -(log(y)÷y)=c$ $(x.logx +or - ylogy)÷xy=c$",,"['ordinary-differential-equations', 'differential-forms']"
47,How can I determine the transfer function of this servomechanism system?,How can I determine the transfer function of this servomechanism system?,,"How can I find transfer function of the given servomechanism system with input $V$(voltage) and output $θ_L$(angle of the load). Schematic of the system is given below. Schematic of the servomechanism system There are two differential equations of the given system. $$\dot{\omega}_L=-\dfrac{k_T}{J_L}\left[θ_L-\dfrac{1}{\rho}θ_M\right]-\dfrac{β_L}{J_L}\omega_L$$ $$\dot{\omega}_M=\dfrac{k_M}{RJ_M}\left[V-k_M\omega_M\right]-\dfrac{β_M}{J_M}\omega_M+\dfrac{k_T}{ρJ_M}\left[θ_L-\dfrac{1}{\rho}θ_M\right] $$ $k_T, J_L, ρ, k_M, J_M, β_L, β_M $ and $R$ are positive constants. In this case, $$F(s)=\dfrac{θ_L(s)}{V(s)}$$ has to be found.","How can I find transfer function of the given servomechanism system with input $V$(voltage) and output $θ_L$(angle of the load). Schematic of the system is given below. Schematic of the servomechanism system There are two differential equations of the given system. $$\dot{\omega}_L=-\dfrac{k_T}{J_L}\left[θ_L-\dfrac{1}{\rho}θ_M\right]-\dfrac{β_L}{J_L}\omega_L$$ $$\dot{\omega}_M=\dfrac{k_M}{RJ_M}\left[V-k_M\omega_M\right]-\dfrac{β_M}{J_M}\omega_M+\dfrac{k_T}{ρJ_M}\left[θ_L-\dfrac{1}{\rho}θ_M\right] $$ $k_T, J_L, ρ, k_M, J_M, β_L, β_M $ and $R$ are positive constants. In this case, $$F(s)=\dfrac{θ_L(s)}{V(s)}$$ has to be found.",,"['ordinary-differential-equations', 'control-theory', 'transfer-theory']"
48,Reduction to separable ODE,Reduction to separable ODE,,"The problem is find the particular solution of $\frac{dy}{dx}$=$\frac{y-x}{y+x}$ when f(7)=7 For $\frac{dy}{dx}$=$\frac{y-x}{y+x}$, first i substitute y and $\frac{dy}{dx}$ to $ux$ and $x\frac{du}{dx}+u$ so the equation change to $x\frac{du}{dx}+u$=$\frac{u-1}{u+1}$ now i think this equation is separable. $(u+1)du$=$\frac{-2}{x}dx$ Then I integration the equation and solve the problem but I wrong I think i solve the problem right way and I can't find my mistake what is the wrong part of my solution?","The problem is find the particular solution of $\frac{dy}{dx}$=$\frac{y-x}{y+x}$ when f(7)=7 For $\frac{dy}{dx}$=$\frac{y-x}{y+x}$, first i substitute y and $\frac{dy}{dx}$ to $ux$ and $x\frac{du}{dx}+u$ so the equation change to $x\frac{du}{dx}+u$=$\frac{u-1}{u+1}$ now i think this equation is separable. $(u+1)du$=$\frac{-2}{x}dx$ Then I integration the equation and solve the problem but I wrong I think i solve the problem right way and I can't find my mistake what is the wrong part of my solution?",,['ordinary-differential-equations']
49,Diffusion equation if f and φ are periodic the solution is also periodic,Diffusion equation if f and φ are periodic the solution is also periodic,,"If I have the diffusion equation $$ \frac{\partial u}{\partial t}-D \frac{\partial^{2}u}{\partial x^{2}}=f(x,t) $$ with the initial condition $$u(x, 0) = φ(x).$$ How would I prove that if $f$ and $φ$ are p-periodic in $x$, that is for some $p > 0$ the identities f(x+p, t) = f(x, t) and $φ(x + p)$ = $φ(x)$ hold for all x and t, then the solution is also p-periodic in x, i.e. $$u(x + p, t) = u(x, t)$$","If I have the diffusion equation $$ \frac{\partial u}{\partial t}-D \frac{\partial^{2}u}{\partial x^{2}}=f(x,t) $$ with the initial condition $$u(x, 0) = φ(x).$$ How would I prove that if $f$ and $φ$ are p-periodic in $x$, that is for some $p > 0$ the identities f(x+p, t) = f(x, t) and $φ(x + p)$ = $φ(x)$ hold for all x and t, then the solution is also p-periodic in x, i.e. $$u(x + p, t) = u(x, t)$$",,"['ordinary-differential-equations', 'partial-differential-equations', 'periodic-functions']"
50,Wronskian: How to prove that $W'(t)=\text{tr}(A) W(t)$,Wronskian: How to prove that,W'(t)=\text{tr}(A) W(t),"I have this system of differential equations: $$ X'(t)= A(t) X(t), $$ where $A$ is an $n\times n$ matrix of continuous functions. We know that if $\{X_1,\ldots, X_n\}$ is a system of solutions, then the Wronskian is  $$W(t)=\det( X_1(t),\ldots,X_n(t)).$$  How do we prove that $$W'(t)=\text{tr}(A)\, W(t)?$$ Thank you.","I have this system of differential equations: $$ X'(t)= A(t) X(t), $$ where $A$ is an $n\times n$ matrix of continuous functions. We know that if $\{X_1,\ldots, X_n\}$ is a system of solutions, then the Wronskian is  $$W(t)=\det( X_1(t),\ldots,X_n(t)).$$  How do we prove that $$W'(t)=\text{tr}(A)\, W(t)?$$ Thank you.",,"['real-analysis', 'ordinary-differential-equations']"
51,Uniqueness of a solution of $\ x' = Ax + b(t) \ $ that tends to $0$ in $+\infty$,Uniqueness of a solution of  that tends to  in,\ x' = Ax + b(t) \  0 +\infty,"Let $b : [0,+\infty) \to \mathbb R^n$ continuous and integrable. Prove there exists an unique solution of $x' = Ax + b(t)$ that tends to $0$ in $+\infty$ with $A \in M_n{(\mathbb R)}$. I don't really see how to deal with this problem if not to integrate the equation. Is not a condition on $A$ needed? If a condition is needed, then it would be $A$ is antisymmetric.","Let $b : [0,+\infty) \to \mathbb R^n$ continuous and integrable. Prove there exists an unique solution of $x' = Ax + b(t)$ that tends to $0$ in $+\infty$ with $A \in M_n{(\mathbb R)}$. I don't really see how to deal with this problem if not to integrate the equation. Is not a condition on $A$ needed? If a condition is needed, then it would be $A$ is antisymmetric.",,['real-analysis']
52,Nearly Bessel's Equation,Nearly Bessel's Equation,,"In my research I have come across a linear, second-order ordinary differential equation that looks much like Bessel's equation except for one small difference: \begin{equation} z^2\frac{\partial^2}{\partial z^2}R_m(z) + z\frac{\partial}{\partial z}R_m(z) + \left(z^2+Wz-m^2\right)R_m(z) = 0, \end{equation} where $m$ and $W$ are constants.  If $W\rightarrow0$, then Bessel's equation is recovered.  I believe this equation may be solved as a series, but its similarity to Bessel's equation makes me wonder if there is another way (maybe through some clever substitution) that makes use of well known functions.  I don't especially want to re-invent the wheel and I have tried to look up this equation, but I have not been able to find anything.  Does anyone know if this equation has previously established solutions?","In my research I have come across a linear, second-order ordinary differential equation that looks much like Bessel's equation except for one small difference: \begin{equation} z^2\frac{\partial^2}{\partial z^2}R_m(z) + z\frac{\partial}{\partial z}R_m(z) + \left(z^2+Wz-m^2\right)R_m(z) = 0, \end{equation} where $m$ and $W$ are constants.  If $W\rightarrow0$, then Bessel's equation is recovered.  I believe this equation may be solved as a series, but its similarity to Bessel's equation makes me wonder if there is another way (maybe through some clever substitution) that makes use of well known functions.  I don't especially want to re-invent the wheel and I have tried to look up this equation, but I have not been able to find anything.  Does anyone know if this equation has previously established solutions?",,"['ordinary-differential-equations', 'special-functions']"
53,Reduction of order where $y_1=(\beta \tan^2x+1)$ with $\beta \in R$,Reduction of order where  with,y_1=(\beta \tan^2x+1) \beta \in R,"I need to apply the reduction of order into differential equation $$\cos^2xy''-6y=0$$where the first solution is of the form $$y_1=(\beta \tan^2x+1)$$ with $\beta \in R$. I started solving equation with reduction of order, however I don't know how should I use the knowledge of ""$\beta \in R$."" $$y_1=(\beta \tan^2x+1)$$ $$y_2=u\cdot y_1=u\cdot(\beta \tan^2x+1)$$ $$y_2'=(u\cdot y_1)'= \dot u(\beta \tan^2x+1)+u(2\beta \cdot \tan x\cdot \sec^2x)$$ $$y_2''= (\dot u(\beta \tan^2x+1)+u(2\beta \cdot \tan x\cdot \sec^2x))'= \ddot u (\beta \tan^2x+1)+ \dot u(4\beta \tan x \cdot \sec^2x) +u(2\beta \sec^4x+4\beta \tan^2x \sec^2x) $$ After inserting $y_2, y_2',y_2''$ into   $\cos^2xy''-6y=0$ I got the form where any of the element of the equation want to reduce. Therefore, I would like to ask should I assume do with $\beta$, can I assume that $\beta$ equals for instance 1?","I need to apply the reduction of order into differential equation $$\cos^2xy''-6y=0$$where the first solution is of the form $$y_1=(\beta \tan^2x+1)$$ with $\beta \in R$. I started solving equation with reduction of order, however I don't know how should I use the knowledge of ""$\beta \in R$."" $$y_1=(\beta \tan^2x+1)$$ $$y_2=u\cdot y_1=u\cdot(\beta \tan^2x+1)$$ $$y_2'=(u\cdot y_1)'= \dot u(\beta \tan^2x+1)+u(2\beta \cdot \tan x\cdot \sec^2x)$$ $$y_2''= (\dot u(\beta \tan^2x+1)+u(2\beta \cdot \tan x\cdot \sec^2x))'= \ddot u (\beta \tan^2x+1)+ \dot u(4\beta \tan x \cdot \sec^2x) +u(2\beta \sec^4x+4\beta \tan^2x \sec^2x) $$ After inserting $y_2, y_2',y_2''$ into   $\cos^2xy''-6y=0$ I got the form where any of the element of the equation want to reduce. Therefore, I would like to ask should I assume do with $\beta$, can I assume that $\beta$ equals for instance 1?",,"['ordinary-differential-equations', 'reduction-of-order-ode']"
54,"Given $f'(x)\leq 1-f(x)^2$, prove $|f(x)|\leq 1$","Given , prove",f'(x)\leq 1-f(x)^2 |f(x)|\leq 1,"Let $f$ be $C^1$ function on $\mathbb{R}$. Suppose that $$f'(x)\leq  1-f(x)^2$$ Prove that $|f(x)|\leq 1 \: \forall x$. I suppose that such a function can be bounded by a solution to the ODE: $y'=1-y^2$ with the same initial value, but I don't know by which theorem, if at all. Trying to solve this ODE gives: $$\frac{dy}{1-y^2}=dx \Rightarrow \int\frac{dy}{1-y^2}=\int dx=x+c$$ but now another problem arises: $\frac{1}{y^2-1}=\frac{1}{2}\left ( \frac{1}{y-1}-\frac{1}{y+1} \right )$  and so taking anti-derviative depends whether $|y|\leq 1$ or not, which makes me even more confused. Can someone please make me some sense of all this mess?","Let $f$ be $C^1$ function on $\mathbb{R}$. Suppose that $$f'(x)\leq  1-f(x)^2$$ Prove that $|f(x)|\leq 1 \: \forall x$. I suppose that such a function can be bounded by a solution to the ODE: $y'=1-y^2$ with the same initial value, but I don't know by which theorem, if at all. Trying to solve this ODE gives: $$\frac{dy}{1-y^2}=dx \Rightarrow \int\frac{dy}{1-y^2}=\int dx=x+c$$ but now another problem arises: $\frac{1}{y^2-1}=\frac{1}{2}\left ( \frac{1}{y-1}-\frac{1}{y+1} \right )$  and so taking anti-derviative depends whether $|y|\leq 1$ or not, which makes me even more confused. Can someone please make me some sense of all this mess?",,"['ordinary-differential-equations', 'upper-lower-bounds']"
55,Finding the interval of definition of an ODE solution: $x'(t)=\frac{x^2-1}{2}$.,Finding the interval of definition of an ODE solution: .,x'(t)=\frac{x^2-1}{2},"This is the first exercise of my ODE notes, so I'd like you to check if my approach is correct or not. Find the maximum interval of definition of the solutions: $\dot{x}=\frac{x^2-1}{2}, \ \ x(t_0)=x_0.$ My approach: I found the solution of this ODE. They're of the form: $$x(t)=\frac{-e^{c_1+t}-1}{e^{c_1+t}-1}.$$ Now we take the denominator and equal it to $0$ to see what points are not in the domain. We see that $${e^{c_1+t}-1}=0 \implies t=-c_1.$$ So we have a vertical asymptote on $x(c_1)$ in every solution considered. Therefore, the interval is $$(c_1,\infty)\ \ if \ \ c_1>0$$ $$(-\infty,c_1)\ \ if \ \ c_1<0$$ $$(c_1,\infty)\ \ or \ \ (-\infty,c_1) \ \ if \ \ c_1=0.$$ Is my approach correct? Thanks for your time.","This is the first exercise of my ODE notes, so I'd like you to check if my approach is correct or not. Find the maximum interval of definition of the solutions: My approach: I found the solution of this ODE. They're of the form: Now we take the denominator and equal it to to see what points are not in the domain. We see that So we have a vertical asymptote on in every solution considered. Therefore, the interval is Is my approach correct? Thanks for your time.","\dot{x}=\frac{x^2-1}{2}, \ \ x(t_0)=x_0. x(t)=\frac{-e^{c_1+t}-1}{e^{c_1+t}-1}. 0 {e^{c_1+t}-1}=0 \implies t=-c_1. x(c_1) (c_1,\infty)\ \ if \ \ c_1>0 (-\infty,c_1)\ \ if \ \ c_1<0 (c_1,\infty)\ \ or \ \ (-\infty,c_1) \ \ if \ \ c_1=0.","['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
56,Finding $f$ from $f'$,Finding  from,f f',"I have a function $f:(1,+\infty)\rightarrow(0,+\infty)$ that derives and for which is true that: $x\cdot\ln(x)\cdot f'(x)=f(x)+x\cdot\ln(x)\cdot f(x)$ and $f(e)=e^e.$ I want to find $f$ so from the given equality I reach at this point: $$x\cdot\ln(x)\cdot f'(x)=f(x)+x\cdot\ln(x)\cdot f(x)\iff\frac{f'(x)}{f(x)}=\frac{(\ln(x))'}{\ln(x)}\iff(\ln(f(x)))'=(\ln(\ln(x)))'$$ but how can I continue?","I have a function $f:(1,+\infty)\rightarrow(0,+\infty)$ that derives and for which is true that: $x\cdot\ln(x)\cdot f'(x)=f(x)+x\cdot\ln(x)\cdot f(x)$ and $f(e)=e^e.$ I want to find $f$ so from the given equality I reach at this point: $$x\cdot\ln(x)\cdot f'(x)=f(x)+x\cdot\ln(x)\cdot f(x)\iff\frac{f'(x)}{f(x)}=\frac{(\ln(x))'}{\ln(x)}\iff(\ln(f(x)))'=(\ln(\ln(x)))'$$ but how can I continue?",,"['integration', 'ordinary-differential-equations', 'functions']"
57,Simple ODE Analytical Solution Question: $\frac{dx}{dt} = ax + bu$,Simple ODE Analytical Solution Question:,\frac{dx}{dt} = ax + bu,my question is in regards to the following formula: \begin{align} \frac{dx}{dt}=ax+bu \end{align} where x(0)=0 & u=constant and a & b are scalar The answer to this or at least what I was given as the answer is the following: \begin{align} x=-\frac{bu}{a}(1-e^{at)} \end{align} Now I understand that I need to manipulate the formula and integrate both sides. Utilizing the integration table I perform the following: Since u=constant we can then say bu=b therefore: \begin{align} \frac {dx}{ax+b}= {dt} \end{align} \begin{align} \int \frac {dx}{ax+b}= \int{dt} \end{align} \begin{align} \frac{1}{a}ln|ax+b| = t + C \end{align} \begin{align} e^{ln|ax+b|} = e^{(t + C)a} \end{align} \begin{align} ax+b = e^{at+Ct} \end{align} \begin{align} x = -\frac{b}{a} \frac{e^{at}e^{Ct}}{a} \end{align} I do not see where I am going wrong in the calcs above. I would appreciate any help in identifying where I went wrong. Thank you,my question is in regards to the following formula: \begin{align} \frac{dx}{dt}=ax+bu \end{align} where x(0)=0 & u=constant and a & b are scalar The answer to this or at least what I was given as the answer is the following: \begin{align} x=-\frac{bu}{a}(1-e^{at)} \end{align} Now I understand that I need to manipulate the formula and integrate both sides. Utilizing the integration table I perform the following: Since u=constant we can then say bu=b therefore: \begin{align} \frac {dx}{ax+b}= {dt} \end{align} \begin{align} \int \frac {dx}{ax+b}= \int{dt} \end{align} \begin{align} \frac{1}{a}ln|ax+b| = t + C \end{align} \begin{align} e^{ln|ax+b|} = e^{(t + C)a} \end{align} \begin{align} ax+b = e^{at+Ct} \end{align} \begin{align} x = -\frac{b}{a} \frac{e^{at}e^{Ct}}{a} \end{align} I do not see where I am going wrong in the calcs above. I would appreciate any help in identifying where I went wrong. Thank you,,['ordinary-differential-equations']
58,The Runge-Kutta method for a system of equations,The Runge-Kutta method for a system of equations,,"In many textbooks, the Runge-Kutta method is introduced for a single 1st order equation $$ \frac{d y }{dt } = f(t, y ) . $$ It is stated that the method generalizes directly to the multi-component case too, i.e., $$ \frac{d \vec{y }}{dt} =\vec{ f} (t, \vec{y }) .  $$ Can anyone explain that why this is obvious ?","In many textbooks, the Runge-Kutta method is introduced for a single 1st order equation $$ \frac{d y }{dt } = f(t, y ) . $$ It is stated that the method generalizes directly to the multi-component case too, i.e., $$ \frac{d \vec{y }}{dt} =\vec{ f} (t, \vec{y }) .  $$ Can anyone explain that why this is obvious ?",,"['ordinary-differential-equations', 'numerical-methods']"
59,Every smooth curve is solution of a differential equation,Every smooth curve is solution of a differential equation,,"Let $\gamma:\Bbb{R}\rightarrow \Bbb{R}^n$ be a $C^1$-function with $\gamma(t)\neq 0\ \forall t\in \Bbb{R}$. Then I want to show that there exists a continous function $f:\Bbb{R}\rightarrow \text{End}(\Bbb{R}^n)$ such that $\dot{\gamma}(t)=f(t)\gamma(t).$ For $n=1$ one can simply choose $f(t):=\frac{\dot{\gamma}(t)}{\gamma(t)}$. But what can I do in higher dimensions? I tried to apply the implicit function theorem but to apply it one needs that $A(x,t):=\dot{\gamma}(t)-x\gamma(t)$ is $C^1$, but our hypothesis only yields that this function is continuous. Any help will be greatful appreciated.","Let $\gamma:\Bbb{R}\rightarrow \Bbb{R}^n$ be a $C^1$-function with $\gamma(t)\neq 0\ \forall t\in \Bbb{R}$. Then I want to show that there exists a continous function $f:\Bbb{R}\rightarrow \text{End}(\Bbb{R}^n)$ such that $\dot{\gamma}(t)=f(t)\gamma(t).$ For $n=1$ one can simply choose $f(t):=\frac{\dot{\gamma}(t)}{\gamma(t)}$. But what can I do in higher dimensions? I tried to apply the implicit function theorem but to apply it one needs that $A(x,t):=\dot{\gamma}(t)-x\gamma(t)$ is $C^1$, but our hypothesis only yields that this function is continuous. Any help will be greatful appreciated.",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
60,Solving a system of third order homogenous ODEs,Solving a system of third order homogenous ODEs,,"I am trying to solve this third order system of homogenous ODEs. $x'''=2x+y$ $y'''=x+2y$ Initial conditions are given as well. Higher order systems weren't covered in the lectures hence I am a bit lost. Nor can I find any similar questions asked on this site. I have tried to solve the equations in two ways so far. Firstly express $y$  in terms of $x'''$ and $x$ from the first equation differentiate the equation three times giving: $x^{(6)}-2x=y'''$ and plugging this into the second equation finally obtaining: $x^{(6)}-4x'''+3x=0$ A higher order homogeneous ODE with constant coefficients that I can solve with the substitution $x = e^{\lambda x}$. The solution can then be differentiated three times and plugged into $x'''=2x+y$ to obtain $y$. Using the given initial conditions we can then find the specific solution. However, this proves very tedious, especially solving the 6x6 system of equations. Is this approach correct and applicable to other similar systems? Or is there a completely different approach to this type of problem I am not aware of? Could you try to solve the system $\vec{u}''' = A \vec{u}$ by diagonalizing A and look for a solution that way, I've tried this as well but I cannot seem to find a solution. Any help is much appreciated.","I am trying to solve this third order system of homogenous ODEs. $x'''=2x+y$ $y'''=x+2y$ Initial conditions are given as well. Higher order systems weren't covered in the lectures hence I am a bit lost. Nor can I find any similar questions asked on this site. I have tried to solve the equations in two ways so far. Firstly express $y$  in terms of $x'''$ and $x$ from the first equation differentiate the equation three times giving: $x^{(6)}-2x=y'''$ and plugging this into the second equation finally obtaining: $x^{(6)}-4x'''+3x=0$ A higher order homogeneous ODE with constant coefficients that I can solve with the substitution $x = e^{\lambda x}$. The solution can then be differentiated three times and plugged into $x'''=2x+y$ to obtain $y$. Using the given initial conditions we can then find the specific solution. However, this proves very tedious, especially solving the 6x6 system of equations. Is this approach correct and applicable to other similar systems? Or is there a completely different approach to this type of problem I am not aware of? Could you try to solve the system $\vec{u}''' = A \vec{u}$ by diagonalizing A and look for a solution that way, I've tried this as well but I cannot seem to find a solution. Any help is much appreciated.",,"['ordinary-differential-equations', 'systems-of-equations']"
61,"If $y' + p(x) y = q(x)$, $q \to 0$, and $p(x) \geq a > 0$, then $y \to 0$","If , , and , then",y' + p(x) y = q(x) q \to 0 p(x) \geq a > 0 y \to 0,"Given the first order equation $$ y' + p(x)y=q(x) $$ where $p,q: \mathbb{R} \longrightarrow \mathbb{R} $ are continuous functions such that $p(x) \geq \alpha > 0, \forall x \in \mathbb{R}$ and $q(x) \longrightarrow 0$ if $x \longrightarrow +\infty$. Show that any solution of equation fulfills that $$ \lim_{x \longrightarrow +\infty} y(x) = 0$$","Given the first order equation $$ y' + p(x)y=q(x) $$ where $p,q: \mathbb{R} \longrightarrow \mathbb{R} $ are continuous functions such that $p(x) \geq \alpha > 0, \forall x \in \mathbb{R}$ and $q(x) \longrightarrow 0$ if $x \longrightarrow +\infty$. Show that any solution of equation fulfills that $$ \lim_{x \longrightarrow +\infty} y(x) = 0$$",,"['calculus', 'ordinary-differential-equations', 'derivatives', 'integrating-factor']"
62,How do you solve the following PDE $\nabla_x y(x) \cdot y(x) = f(x)$?,How do you solve the following PDE ?,\nabla_x y(x) \cdot y(x) = f(x),"How do you solve PDEs of the form  $\nabla_x y(x) \cdot y(x) = f(x)$? Here, $y:\mathbb{R}^m \rightarrow \mathbb{R}$, $\nabla_x y(x)$ is the gradient of $y(x)$ w.r.t $x$ and $f:\mathbb{R}^m \rightarrow \mathbb{R}^m$ is a continuous function.","How do you solve PDEs of the form  $\nabla_x y(x) \cdot y(x) = f(x)$? Here, $y:\mathbb{R}^m \rightarrow \mathbb{R}$, $\nabla_x y(x)$ is the gradient of $y(x)$ w.r.t $x$ and $f:\mathbb{R}^m \rightarrow \mathbb{R}^m$ is a continuous function.",,"['ordinary-differential-equations', 'partial-differential-equations']"
63,"Solving a particular integro-differential equation: $g(x) = \int_1^x \frac{F(u)}{\sqrt{x^2-u^2}} \, du$",Solving a particular integro-differential equation:,"g(x) = \int_1^x \frac{F(u)}{\sqrt{x^2-u^2}} \, du","As the title says: I'm interested in the following integro-differential equation. Let $g:(1,\infty) \to [0,1]$ be given, and assume $g$ is smooth. I want to find functions $F:[1,\infty) \to [0,1]$ that satisfy: i) For every $x \in (1, \infty)$, $\displaystyle\int_1^x \frac{F(u)}{\sqrt{x^2-u^2}} \, du = g(x)$ ii) $F$ is non-decreasing, right continuous, $F(1) = 0$, and $\lim_{y \to \infty} F(y) = 1$, i.e. $F$ is a CDF supported on $(1,\infty)$. The first thing to try is differentiating, but this doesn't seem to go anywhere: since that the integrand $\frac{F(u)}{\sqrt{x^2-u^2}}$ blows up at $u = x$, we can't (immediately) pass the derivative through the integral. The next thing is integrating by parts, which yields $\displaystyle g(x) = \frac{\pi}{2} F(x) - \int_1^x F'(u) \arctan\Big(\frac{u}{\sqrt{x^2-u^2}}\Big) \,du.$ (Some additional assumptions are needed here: for example, that $F$ is differentiable.) Now using the Leibniz integral rule , $\displaystyle g'(x) = \int_1^x \frac{F'(u)}{x\sqrt{x^2-u^2}} \, du.$ But this doesn't seem to help much. Are there standard methods for this kind of thing? Is there some way to make sense of passing the derivative inside the integral? I would be happy to see an example of some $g$ and $F$ that satisfy this: I have no such examples right now.","As the title says: I'm interested in the following integro-differential equation. Let $g:(1,\infty) \to [0,1]$ be given, and assume $g$ is smooth. I want to find functions $F:[1,\infty) \to [0,1]$ that satisfy: i) For every $x \in (1, \infty)$, $\displaystyle\int_1^x \frac{F(u)}{\sqrt{x^2-u^2}} \, du = g(x)$ ii) $F$ is non-decreasing, right continuous, $F(1) = 0$, and $\lim_{y \to \infty} F(y) = 1$, i.e. $F$ is a CDF supported on $(1,\infty)$. The first thing to try is differentiating, but this doesn't seem to go anywhere: since that the integrand $\frac{F(u)}{\sqrt{x^2-u^2}}$ blows up at $u = x$, we can't (immediately) pass the derivative through the integral. The next thing is integrating by parts, which yields $\displaystyle g(x) = \frac{\pi}{2} F(x) - \int_1^x F'(u) \arctan\Big(\frac{u}{\sqrt{x^2-u^2}}\Big) \,du.$ (Some additional assumptions are needed here: for example, that $F$ is differentiable.) Now using the Leibniz integral rule , $\displaystyle g'(x) = \int_1^x \frac{F'(u)}{x\sqrt{x^2-u^2}} \, du.$ But this doesn't seem to help much. Are there standard methods for this kind of thing? Is there some way to make sense of passing the derivative inside the integral? I would be happy to see an example of some $g$ and $F$ that satisfy this: I have no such examples right now.",,"['calculus', 'probability', 'ordinary-differential-equations', 'analysis', 'integro-differential-equations']"
64,How can the following PDE be solved?,How can the following PDE be solved?,,"I am working on this problem on PDE's : $f(x, y)\frac{{\partial}f}{{\partial}x} + \frac{{\partial}f}{{\partial}y} = 1$ with $f(u,u) = \frac{u}{2} , 0< u < 1$. I tried to solve this PDE with the method of characteristics but since $f(x, y)$ is not given, I did not know how to continue. Any ideas?","I am working on this problem on PDE's : $f(x, y)\frac{{\partial}f}{{\partial}x} + \frac{{\partial}f}{{\partial}y} = 1$ with $f(u,u) = \frac{u}{2} , 0< u < 1$. I tried to solve this PDE with the method of characteristics but since $f(x, y)$ is not given, I did not know how to continue. Any ideas?",,['ordinary-differential-equations']
65,$ y''\sin^2 x + y' \tan x + y \cos^2x = 0 $,, y''\sin^2 x + y' \tan x + y \cos^2x = 0 ,"I have been going insane over the following differential equation over the past few days. $y''\sin^2(x) + y'\tan(x)  + y\cos^2(x) = 0 $ The assignment is: $a)$ Show that $x=0$ is a regular singular point of the differential equation. $b)$ Find the indicial roots. $c)$ Show that $\sin(\ln(\sin x))$ and $\cos(\ln(\sin x))$ are solutions of the differential equation for some $0<x<R$ and $R>0$ I have tried many times to show $a)$ : $p(x)=\tan x/\sin^2x$ and $q(x)=\cos^2x/\sin^2x$ are both not analytic at $x=0$ . $$xp(x)=\frac{x\tan x}{\sin^2x} = \frac{x}{\sin x \cos x}$$ Trying to compute this series directly will always yield a sine in the denumerator, making this fail at x=0. So I tried some tricks, the one I was most convinced of was determining the series of $\sin x\cos x$ and then dividing through by $x$ : $$\sin x\cos x=\frac{1}{2}\sum_0^k(-1)^k\frac{(2x)^{2k+1}}{(2k+1)!}$$ So then $$\frac{x}{\sin x \cos x}= \frac{x}{\frac{1}{2}\sum_0^k(-1)^k\frac{(2x)^{2k+1}}{(2k+1)!}}=\frac{2}{\sum_0^k(-1)^k\frac{2^{2k+1}(x)^{2k}}{(2k+1)!}}$$ Setting $x=0$ here yields $\frac22=1$ . So I thought this could work and that $xp(x)$ could be considered analytic at $0$ like this. A similar argument I tried for $x^2q(x)$ was: $$\frac{x^2\cos^2x}{\sin^2x}=\frac{x^2}{\tan^2x}$$ And then taking the series of $\tan x = x+\frac{x^3}{3}+\frac{2x^5}{15}+...$ So that $x^2q(x)$ becomes: $$\frac{x^2}{( x+\frac{x^3}{3}+\frac{2x^5}{15}+...)( x+\frac{x^3}{3}+\frac{2x^5}{15}+...)} = \frac{1}{1 + 2\frac{x^2}3+\frac{17x^4}{45}+...}$$ The series check out as far as I can tell, but I'm not convinced I can really say that $xp(x)$ and $x^2q(x)$ are analytic at the origin because of this. Any insight as to how this power series expansion conclusively shows that they are analytic is much appreciated. If the expansions are correct, it follows that $p_0=q_0=1$ , which will give indicial roots $+/-i$ . I have never seen complex indicial roots and they aren't covered by the course I'm taking. It feels like these are wrong and this is the main reason I started doubting my result for a). Showing the solutions in c) are true is simple (derive and plug in the equation). However I can't find the number R they want me to find. I thought they meant the interval of convergence of the series found in a), as this is directly related to the solution. I attempted to compute those: I used the denumerator only, because I didn't have a clue how to calculate a radius of convergence of the reciprocal of a power series. The expansion for $xp(x)$ gives: $$a_k= (-1)^k\frac{2^{2k+1}x^{2k}}{(2k+1)!}$$ and $$a_{k+1}=(-1)^{k+1}\frac{2^{2k+3}x^{2k+2}}{(2k+3)!}$$ $$\lim_{k->\infty}\frac{a_{k+1}}{a_{k}}=\lim_{k->\infty}\frac{(-1)^{k+1}\frac{2^{2k+3}x^{2k+2}}{(2k+3)!}}{(-1)^k\frac{2^{2k+1}x^{2k}}{(2k+1)!}}=\lim_{k->\infty}\frac{-4}{(2k+3)(2k+2)}x^2=0$$ So the whole thing doesn't even converge to begin with, which means my solution obtained in a) must be wrong. I don't know what or where exactly I am going into the wrong and any advice is welcome.","I have been going insane over the following differential equation over the past few days. The assignment is: Show that is a regular singular point of the differential equation. Find the indicial roots. Show that and are solutions of the differential equation for some and I have tried many times to show : and are both not analytic at . Trying to compute this series directly will always yield a sine in the denumerator, making this fail at x=0. So I tried some tricks, the one I was most convinced of was determining the series of and then dividing through by : So then Setting here yields . So I thought this could work and that could be considered analytic at like this. A similar argument I tried for was: And then taking the series of So that becomes: The series check out as far as I can tell, but I'm not convinced I can really say that and are analytic at the origin because of this. Any insight as to how this power series expansion conclusively shows that they are analytic is much appreciated. If the expansions are correct, it follows that , which will give indicial roots . I have never seen complex indicial roots and they aren't covered by the course I'm taking. It feels like these are wrong and this is the main reason I started doubting my result for a). Showing the solutions in c) are true is simple (derive and plug in the equation). However I can't find the number R they want me to find. I thought they meant the interval of convergence of the series found in a), as this is directly related to the solution. I attempted to compute those: I used the denumerator only, because I didn't have a clue how to calculate a radius of convergence of the reciprocal of a power series. The expansion for gives: and So the whole thing doesn't even converge to begin with, which means my solution obtained in a) must be wrong. I don't know what or where exactly I am going into the wrong and any advice is welcome.",y''\sin^2(x) + y'\tan(x)  + y\cos^2(x) = 0  a) x=0 b) c) \sin(\ln(\sin x)) \cos(\ln(\sin x)) 0<x<R R>0 a) p(x)=\tan x/\sin^2x q(x)=\cos^2x/\sin^2x x=0 xp(x)=\frac{x\tan x}{\sin^2x} = \frac{x}{\sin x \cos x} \sin x\cos x x \sin x\cos x=\frac{1}{2}\sum_0^k(-1)^k\frac{(2x)^{2k+1}}{(2k+1)!} \frac{x}{\sin x \cos x}= \frac{x}{\frac{1}{2}\sum_0^k(-1)^k\frac{(2x)^{2k+1}}{(2k+1)!}}=\frac{2}{\sum_0^k(-1)^k\frac{2^{2k+1}(x)^{2k}}{(2k+1)!}} x=0 \frac22=1 xp(x) 0 x^2q(x) \frac{x^2\cos^2x}{\sin^2x}=\frac{x^2}{\tan^2x} \tan x = x+\frac{x^3}{3}+\frac{2x^5}{15}+... x^2q(x) \frac{x^2}{( x+\frac{x^3}{3}+\frac{2x^5}{15}+...)( x+\frac{x^3}{3}+\frac{2x^5}{15}+...)} = \frac{1}{1 + 2\frac{x^2}3+\frac{17x^4}{45}+...} xp(x) x^2q(x) p_0=q_0=1 +/-i xp(x) a_k= (-1)^k\frac{2^{2k+1}x^{2k}}{(2k+1)!} a_{k+1}=(-1)^{k+1}\frac{2^{2k+3}x^{2k+2}}{(2k+3)!} \lim_{k->\infty}\frac{a_{k+1}}{a_{k}}=\lim_{k->\infty}\frac{(-1)^{k+1}\frac{2^{2k+3}x^{2k+2}}{(2k+3)!}}{(-1)^k\frac{2^{2k+1}x^{2k}}{(2k+1)!}}=\lim_{k->\infty}\frac{-4}{(2k+3)(2k+2)}x^2=0,"['ordinary-differential-equations', 'frobenius-method']"
66,Computing the state transition matrix,Computing the state transition matrix,,"Let $$\pmatrix{\dot x_1\\ \dot x_2\\ \dot x_3} = \underbrace{\pmatrix{0 & 3 & 2\cos(7t)\\-3 & 0 & -2\sin(7t)\\-2\cos(7t) & 2\sin(7t) & 0}}_{A(t)}\underbrace{\pmatrix{x_1(t)\\x_2(t)\\x_3(t)}}_{X(t)}.$$ Find the state transition matrix $\phi(t,0)$ for the system. I know how to work with this if the matrix $A$ is actually time-invariant. It is as simple as computing $\phi(t,0) = e^{At}$, which can be decided by the eigenvalues of $A$ or even putting the matrix $A$ into real-Jordan form. However, we have a time-variant matrix $A(t)$. Initially I thought that maybe I could try and use the $e^{At}$ formula again, but my first hint that this probably won't work was the fact that MATLAB and Mathematica spit out a gigantic mess. Can anyone provide a hint on what might need to be done in order to compute the state transition matrix for a time-varying system such as this? More specifically, is there a more intuitive method to finding the state transition matrix for a system like this than the Peano-Baker series?","Let $$\pmatrix{\dot x_1\\ \dot x_2\\ \dot x_3} = \underbrace{\pmatrix{0 & 3 & 2\cos(7t)\\-3 & 0 & -2\sin(7t)\\-2\cos(7t) & 2\sin(7t) & 0}}_{A(t)}\underbrace{\pmatrix{x_1(t)\\x_2(t)\\x_3(t)}}_{X(t)}.$$ Find the state transition matrix $\phi(t,0)$ for the system. I know how to work with this if the matrix $A$ is actually time-invariant. It is as simple as computing $\phi(t,0) = e^{At}$, which can be decided by the eigenvalues of $A$ or even putting the matrix $A$ into real-Jordan form. However, we have a time-variant matrix $A(t)$. Initially I thought that maybe I could try and use the $e^{At}$ formula again, but my first hint that this probably won't work was the fact that MATLAB and Mathematica spit out a gigantic mess. Can anyone provide a hint on what might need to be done in order to compute the state transition matrix for a time-varying system such as this? More specifically, is there a more intuitive method to finding the state transition matrix for a system like this than the Peano-Baker series?",,"['linear-algebra', 'ordinary-differential-equations', 'matrix-calculus', 'control-theory', 'linear-control']"
67,"Show that the trajectory of the solution $ x(t),y(t))$ is contained in the curve $E_{0}$. (Non-Linear Systems, ODE)","Show that the trajectory of the solution  is contained in the curve . (Non-Linear Systems, ODE)"," x(t),y(t)) E_{0}","I've been visiting this site for years to solve doubts, and this is my first question it's about something that's almost totally new to me. We have the autonomous system $x'=y$ and $y'=x^3-x$ (pendulum). And the following question: Let $(x(t),y(t))$ be a $(C^1)$ class) solution of the nonlinear system, with $t=0$ given by $(xo,yo)$. Show that the trajectory $(x(t),y(t))$ must be contained on the curve: $E_{0}=\dfrac{1}{2}y^{2}-\dfrac{1}{4}(x^{2}-1)^{2}, E_{0}$ its constant. I'm very confused about this exercise, because i don't know what to do with the curve $E_0$ if it's constant!.","I've been visiting this site for years to solve doubts, and this is my first question it's about something that's almost totally new to me. We have the autonomous system $x'=y$ and $y'=x^3-x$ (pendulum). And the following question: Let $(x(t),y(t))$ be a $(C^1)$ class) solution of the nonlinear system, with $t=0$ given by $(xo,yo)$. Show that the trajectory $(x(t),y(t))$ must be contained on the curve: $E_{0}=\dfrac{1}{2}y^{2}-\dfrac{1}{4}(x^{2}-1)^{2}, E_{0}$ its constant. I'm very confused about this exercise, because i don't know what to do with the curve $E_0$ if it's constant!.",,"['ordinary-differential-equations', 'systems-of-equations']"
68,Composition of Flow Maps for ODEs,Composition of Flow Maps for ODEs,,"The definition of a flow map given here: http://www.math.sjsu.edu/~simic/Fall05/Math134/flows.pdf states that if $$\phi_t(X_0) = \phi(t, X_0)$$ is a flow of some ODE $X' = F(X)$ then ""because of uniqueness of solutions"" $\phi_{s+t} = \phi(s) \circ \phi(t)$. I don't see how that result follows from uniqueness of solutions. Can someone make this more explicit?","The definition of a flow map given here: http://www.math.sjsu.edu/~simic/Fall05/Math134/flows.pdf states that if $$\phi_t(X_0) = \phi(t, X_0)$$ is a flow of some ODE $X' = F(X)$ then ""because of uniqueness of solutions"" $\phi_{s+t} = \phi(s) \circ \phi(t)$. I don't see how that result follows from uniqueness of solutions. Can someone make this more explicit?",,"['ordinary-differential-equations', 'analysis']"
69,"Solutions of : $y' = (y^2 + z^2 +1)^{-a} , z' = y(1+z^2)^a$",Solutions of :,"y' = (y^2 + z^2 +1)^{-a} , z' = y(1+z^2)^a","Study the existence of solutions that are set entirely on $\mathbb R$ for the functions : $$  y' = (y^2 + z^2 +1)^{-a}, z' = y(1+z^2)^a  $$ I came upon this problem while studying for my Dynamical Systems course, but I'm not sure on how to proceed. One thing I saw was that we could bound the second equation, such as : $$z' = (1 +z^2)^a \leq  (1+y^2 + z^2)^a = y/y'$$ since $y^2 \geq \forall y\in C(\mathbb R).$ So, we have that the second equation is bound between the solution and the derivative of the first equation, since : $$z \leq \frac{y}{y'}$$ but I cannot see how to use this in order to study the existence of the equation (it can maybe help on proving Lipschitz conditions for the uniqueness but that's not what I need here).","Study the existence of solutions that are set entirely on for the functions : I came upon this problem while studying for my Dynamical Systems course, but I'm not sure on how to proceed. One thing I saw was that we could bound the second equation, such as : since So, we have that the second equation is bound between the solution and the derivative of the first equation, since : but I cannot see how to use this in order to study the existence of the equation (it can maybe help on proving Lipschitz conditions for the uniqueness but that's not what I need here).","\mathbb R   y' = (y^2 + z^2 +1)^{-a}, z' = y(1+z^2)^a   z' = (1 +z^2)^a \leq  (1+y^2 + z^2)^a = y/y' y^2 \geq \forall y\in C(\mathbb R). z \leq \frac{y}{y'}","['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
70,General Solution of $y''y + (y')^2 - y = 0$,General Solution of,y''y + (y')^2 - y = 0,"I have the following non linear ODE and I'm struggling to find the solution. Naturally, I thought to let $w=y'$ so that $y''=w'=w*dw/dy$ but i keep getting lost in the calculation. Any help would be much appreciated. Thanks!!","I have the following non linear ODE and I'm struggling to find the solution. Naturally, I thought to let $w=y'$ so that $y''=w'=w*dw/dy$ but i keep getting lost in the calculation. Any help would be much appreciated. Thanks!!",,['ordinary-differential-equations']
71,Solving the ODE $y(1+\sqrt{x^2 y^4+1})dx+2xdy=0$,Solving the ODE,y(1+\sqrt{x^2 y^4+1})dx+2xdy=0,"Question: Solve the ODE given below: $y(1+\sqrt{x^2 y^4+1})dx+2xdy=0$ My try: The equation is not separable because a function of $x$ is added to a function of $y$. ($y+y\sqrt{x^2y^4+1}$) Also, it is not linear with respect to $x$ or $y$, because it has the term$\sqrt{x^2y^4+1}$. On the other hand, it's not a complete ODE because $\frac{d}{dy}(y(1+\sqrt{x^2 y^4+1})) \neq \frac{d}{dx}(2x)$ I also tried homogenous ODE's. But this ODE is not homogenous.  It doesn't seem to be a Clero DE either. Any idea? Thanks in advance.","Question: Solve the ODE given below: $y(1+\sqrt{x^2 y^4+1})dx+2xdy=0$ My try: The equation is not separable because a function of $x$ is added to a function of $y$. ($y+y\sqrt{x^2y^4+1}$) Also, it is not linear with respect to $x$ or $y$, because it has the term$\sqrt{x^2y^4+1}$. On the other hand, it's not a complete ODE because $\frac{d}{dy}(y(1+\sqrt{x^2 y^4+1})) \neq \frac{d}{dx}(2x)$ I also tried homogenous ODE's. But this ODE is not homogenous.  It doesn't seem to be a Clero DE either. Any idea? Thanks in advance.",,['ordinary-differential-equations']
72,Solution of an inhomogeneous modified Bessel equation,Solution of an inhomogeneous modified Bessel equation,,"I'm solving the equation $$x^2y''+xy'-(x^{2}\lambda^{2}+1)y=-C\frac{I_{1}(\lambda x)}{I_{1}(\lambda)}$$ where $I_{\alpha}$ is the modified Bessel function of the first kind. The complementary function for this problem is $$y_{cf}=AI_{1}(\lambda x)+BK_{1}(\lambda x),$$ where $K_{\alpha}$ is the modified Bessel function of the second kind. Using the method of variation of parameters I find that $$y_{p}=u_{1}I_{1}(\lambda x)+u_{2}K_{1}(\lambda x),$$ where \begin{align*} u_{1}&=\frac{C}{I_{1}(\lambda)}\int\frac{I_{1}(\lambda x)K_{1}(\lambda x)} {W[I_{1}(\lambda x),K_{1}(\lambda x)]}\,\textrm{d}x =-\frac{C}{I_{1}(\lambda)}\int xI_{1}(\lambda x)K_{1}(\lambda x)\,\textrm{d}x, \\ u_{2}&=-\frac{C}{I_{1}(\lambda)}\int\frac{I_{1}(\lambda x)I_{1}(\lambda x)} {W[I_{1}(\lambda x),K_{1}(\lambda x)]}\,\textrm{d}x =\frac{C}{I_{1}(\lambda)}\int xI_{1}(\lambda x)I_{1}(\lambda x)\,\textrm{d}x. \end{align*} Here I have used the fact that $$W[I_{1}(\lambda x),K_{1}(\lambda x)]=I_{1}(\lambda x)[K_{1}(\lambda x)]'-K_{1}(\lambda x)[I_{1}(\lambda x)]'=-\frac{1}{x}.$$ Can anyone suggest the best way to go about computing these integrals? I've tried 'by parts' (seemed the natural choice) but haven't managed to get anywhere thus far - thanks!","I'm solving the equation $$x^2y''+xy'-(x^{2}\lambda^{2}+1)y=-C\frac{I_{1}(\lambda x)}{I_{1}(\lambda)}$$ where $I_{\alpha}$ is the modified Bessel function of the first kind. The complementary function for this problem is $$y_{cf}=AI_{1}(\lambda x)+BK_{1}(\lambda x),$$ where $K_{\alpha}$ is the modified Bessel function of the second kind. Using the method of variation of parameters I find that $$y_{p}=u_{1}I_{1}(\lambda x)+u_{2}K_{1}(\lambda x),$$ where \begin{align*} u_{1}&=\frac{C}{I_{1}(\lambda)}\int\frac{I_{1}(\lambda x)K_{1}(\lambda x)} {W[I_{1}(\lambda x),K_{1}(\lambda x)]}\,\textrm{d}x =-\frac{C}{I_{1}(\lambda)}\int xI_{1}(\lambda x)K_{1}(\lambda x)\,\textrm{d}x, \\ u_{2}&=-\frac{C}{I_{1}(\lambda)}\int\frac{I_{1}(\lambda x)I_{1}(\lambda x)} {W[I_{1}(\lambda x),K_{1}(\lambda x)]}\,\textrm{d}x =\frac{C}{I_{1}(\lambda)}\int xI_{1}(\lambda x)I_{1}(\lambda x)\,\textrm{d}x. \end{align*} Here I have used the fact that $$W[I_{1}(\lambda x),K_{1}(\lambda x)]=I_{1}(\lambda x)[K_{1}(\lambda x)]'-K_{1}(\lambda x)[I_{1}(\lambda x)]'=-\frac{1}{x}.$$ Can anyone suggest the best way to go about computing these integrals? I've tried 'by parts' (seemed the natural choice) but haven't managed to get anywhere thus far - thanks!",,"['integration', 'ordinary-differential-equations', 'special-functions', 'indefinite-integrals', 'bessel-functions']"
73,Stability of critical point,Stability of critical point,,"Let $y''+f(t)y=0$ where $\lim\limits_{t\rightarrow\infty}f(t)=1$. When is the critical point $(0,0)$ stable? I want to show that this is the case when $\int_0^\infty|f(t')-1|dt'$ is finite. For this we write $y(t)=A(t)\cos(t)+B(t)\sin(t)$ s.t. $A'\cos(t)+B'\sin(t)=0.$ I have shown that for $C(t)=A^2(t)+B^2(t)$, we have $$C'\leq4|f(t)-1|C.$$ And I want to use this to prove that $(0,0)$ is stable when $\int_0^\infty|f(t')-1|dt'$ is finite. Plus, I know that stability means that for every neigbourhod $N$ of $(0,0)$ there is a $M\subset N$ s.t. if $y\in M$ then the flow $\phi(t,y)\in N$for all $t$. What can I do next?","Let $y''+f(t)y=0$ where $\lim\limits_{t\rightarrow\infty}f(t)=1$. When is the critical point $(0,0)$ stable? I want to show that this is the case when $\int_0^\infty|f(t')-1|dt'$ is finite. For this we write $y(t)=A(t)\cos(t)+B(t)\sin(t)$ s.t. $A'\cos(t)+B'\sin(t)=0.$ I have shown that for $C(t)=A^2(t)+B^2(t)$, we have $$C'\leq4|f(t)-1|C.$$ And I want to use this to prove that $(0,0)$ is stable when $\int_0^\infty|f(t')-1|dt'$ is finite. Plus, I know that stability means that for every neigbourhod $N$ of $(0,0)$ there is a $M\subset N$ s.t. if $y\in M$ then the flow $\phi(t,y)\in N$for all $t$. What can I do next?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
74,Finding extremal of the functional,Finding extremal of the functional,,"Find the stationary point of the functional $$ J[y]=\int \left( x^2y'^2+2y^2 \right) dx $$ where $y(0)=0, y(1)=2.$ My Solution: E-L equation: $x^2y''+2xy'-2y=0.$ This is also Cauchy-Euler equation. Let $y(x)=x^m$. Substituting to eqn. , we get $m_1=-2, m_2=1$ and I found the general solution $y(x)=c_1x^{-2}+c_2x$. Now, we will find $c_1,c_2.$ Since $y(1)=2$, we have $c_1+c_2=2$. But when I write $x=0$ to general solution, $0=y(x)=c_1.0^{-2}+c_20$, there is a uncertainty. Please help me.","Find the stationary point of the functional $$ J[y]=\int \left( x^2y'^2+2y^2 \right) dx $$ where $y(0)=0, y(1)=2.$ My Solution: E-L equation: $x^2y''+2xy'-2y=0.$ This is also Cauchy-Euler equation. Let $y(x)=x^m$. Substituting to eqn. , we get $m_1=-2, m_2=1$ and I found the general solution $y(x)=c_1x^{-2}+c_2x$. Now, we will find $c_1,c_2.$ Since $y(1)=2$, we have $c_1+c_2=2$. But when I write $x=0$ to general solution, $0=y(x)=c_1.0^{-2}+c_20$, there is a uncertainty. Please help me.",,"['ordinary-differential-equations', 'calculus-of-variations', 'optimal-control', 'euler-lagrange-equation', 'variational-analysis']"
75,Differential equations for chemical reaction $\mathrm{A + 2B \to 3C}$,Differential equations for chemical reaction,\mathrm{A + 2B \to 3C},"In a chemical reaction $\mathrm{A + 2B \to 3C}$, the concentrations $a(t)$, $b(t)$ and $c(t)$ of the three substances A, B and C measure up to the differential equations   $$ \begin{align} \frac{da}{dt} &= -rab^2\tag{1}\\ \frac{db}{dt} &= -2rab^2\tag{2}\\ \frac{dc}{dt} &= -3rab^2\tag{3} \end{align} $$   with $r > 0$ and begin condition $a(0) = 1$ and $b(0) = 2$. Show that $b(t) - 2a(t) = 0$ . Here is my solution, but it is not right. Any help would be great. First equation $$ \begin{align} \int\frac{da}{a} &= -rb^2\int dt\\ \ln(a) &= -rb^2t + C\\ a(t) &= e^{-rb^2t+ C}\\ a(0) &= 1 &&\to &1 &= e^{0 + C}\\ \ln(1) &= C &&\to &C &= 0\\ a(t) &= e^{-rb^2t} \end{align} $$ Second equation $$ \begin{align} \frac{db}{dt} &= -2rab^2\\   \int \frac{db}{b^2} &= -2ra\int dt\\ b &= \frac{1}{2rat + C}\\ b(0) &= 2 \quad \to \quad 2 = \frac{1}{C} \quad \to \quad C = \frac{1}{2}\\ b(t) &= \frac{2}{ 2rat + 1} \end{align} $$ But now $b(t) - 2a(t) \ne  0$. Where I am making mistake? Any tip will be enough.","In a chemical reaction $\mathrm{A + 2B \to 3C}$, the concentrations $a(t)$, $b(t)$ and $c(t)$ of the three substances A, B and C measure up to the differential equations   $$ \begin{align} \frac{da}{dt} &= -rab^2\tag{1}\\ \frac{db}{dt} &= -2rab^2\tag{2}\\ \frac{dc}{dt} &= -3rab^2\tag{3} \end{align} $$   with $r > 0$ and begin condition $a(0) = 1$ and $b(0) = 2$. Show that $b(t) - 2a(t) = 0$ . Here is my solution, but it is not right. Any help would be great. First equation $$ \begin{align} \int\frac{da}{a} &= -rb^2\int dt\\ \ln(a) &= -rb^2t + C\\ a(t) &= e^{-rb^2t+ C}\\ a(0) &= 1 &&\to &1 &= e^{0 + C}\\ \ln(1) &= C &&\to &C &= 0\\ a(t) &= e^{-rb^2t} \end{align} $$ Second equation $$ \begin{align} \frac{db}{dt} &= -2rab^2\\   \int \frac{db}{b^2} &= -2ra\int dt\\ b &= \frac{1}{2rat + C}\\ b(0) &= 2 \quad \to \quad 2 = \frac{1}{C} \quad \to \quad C = \frac{1}{2}\\ b(t) &= \frac{2}{ 2rat + 1} \end{align} $$ But now $b(t) - 2a(t) \ne  0$. Where I am making mistake? Any tip will be enough.",,"['ordinary-differential-equations', 'chemistry']"
76,inverse function of ODE solution,inverse function of ODE solution,,"I'm trying to determine the inverse of an ODE solution. If this is possible. $$\alpha(t)=\alpha_0(I(t)/I_0)^p$$ , where $I(t)$ is to be determined (although $lim_{t\to\infty}I(t)=I_{max}$ is fixed) Next: $$\frac{dn}{dt}=T(\alpha(t)(1-n)-\beta n)$$ And $$B(t) = G\alpha(t)(1-n(t))$$ Now, (I think) I've solved $n$ and $B$ to be $$n(t)=\frac{\alpha(t)}{\alpha(t)+\beta}+Ce^{-T(\alpha(t)+\beta)t}$$ $$B(t) = G\alpha(t)(\frac{\beta}{\alpha(t)+\beta}-Ce^{-T(\alpha(t)+\beta)t})$$ For $\lim_{t\to\infty}$: $$n_\infty=\frac{\alpha_\infty}{\alpha_\infty+\beta}$$ $$B_\infty=G\frac{\alpha_\infty\beta}{\alpha_\infty+\beta}$$ Let's evaluate this with for instance $I(t)=0.0158I_0$, $\alpha_0=0.1$, $p=0.5$, $T=60$, $\beta=0.007$, $G=37$, $n_0=0$ and evaluate over $0\leq t<24$. I want to determine $I(t)$ to realize $B(t)=B_\infty$. Effectively ramping-up $I(t)$ to keep $B(t)$ constant. I expect $I(t)$ to look somewhat like: $$I(t) = p+q(1-e^{-T(\alpha(t)+\beta)t})$$ , where $p+q=0.0158I_0$. But since $\alpha(t)$ is a function of $I(t)$, I run into problems there. So now I'm trying to invert $B(t)$ and $n(t)$. But especially the inverse of $n(t)$ is giving me headaches. Is my approach wrong? Should I instead use control theory?","I'm trying to determine the inverse of an ODE solution. If this is possible. $$\alpha(t)=\alpha_0(I(t)/I_0)^p$$ , where $I(t)$ is to be determined (although $lim_{t\to\infty}I(t)=I_{max}$ is fixed) Next: $$\frac{dn}{dt}=T(\alpha(t)(1-n)-\beta n)$$ And $$B(t) = G\alpha(t)(1-n(t))$$ Now, (I think) I've solved $n$ and $B$ to be $$n(t)=\frac{\alpha(t)}{\alpha(t)+\beta}+Ce^{-T(\alpha(t)+\beta)t}$$ $$B(t) = G\alpha(t)(\frac{\beta}{\alpha(t)+\beta}-Ce^{-T(\alpha(t)+\beta)t})$$ For $\lim_{t\to\infty}$: $$n_\infty=\frac{\alpha_\infty}{\alpha_\infty+\beta}$$ $$B_\infty=G\frac{\alpha_\infty\beta}{\alpha_\infty+\beta}$$ Let's evaluate this with for instance $I(t)=0.0158I_0$, $\alpha_0=0.1$, $p=0.5$, $T=60$, $\beta=0.007$, $G=37$, $n_0=0$ and evaluate over $0\leq t<24$. I want to determine $I(t)$ to realize $B(t)=B_\infty$. Effectively ramping-up $I(t)$ to keep $B(t)$ constant. I expect $I(t)$ to look somewhat like: $$I(t) = p+q(1-e^{-T(\alpha(t)+\beta)t})$$ , where $p+q=0.0158I_0$. But since $\alpha(t)$ is a function of $I(t)$, I run into problems there. So now I'm trying to invert $B(t)$ and $n(t)$. But especially the inverse of $n(t)$ is giving me headaches. Is my approach wrong? Should I instead use control theory?",,"['ordinary-differential-equations', 'control-theory', 'inverse-function']"
77,Using seperation of variables in Partial Differential Equations,Using seperation of variables in Partial Differential Equations,,"I am trying to use separation of variables method to solve $$ u_t = 9u_{xx}, \quad 0<x<1,\quad t>0$$ $$ u_x(0,t) = 0$$ $$u(1,t)=u(0,t) $$ $$u(x,0) = \sin\pi x, \quad 0 \leq x \le 1$$ and find an approximation (actual number) for large $t$. Here are my workings so far but I am running into some problems. Assume $u(x,t) = F(x)G(t)$ so that $$u_x = F'G, u_{xx} = F''G, u_t = FG'$$ Thus our equation PDE becomes, $$9F''G = FG' $$ Rearranging this equation yields, $$\frac{F''(x)}{F(x)} = \frac{G'(t)}{9G(t)} = \lambda$$ Thus we form two ODE'S, $$F''-\lambda F = 0$$ $$G'-9\lambda G = 0 $$ Using the B.C $u_x(0,t) = 0$ gives $F'(0) = 0 $ with $A\ne 0$ Thus we solve $F'' - \lambda F = 0$ using the above B Then if $\lambda = 0 $ then $F(x) = Ax + B$ and $F'(0) = 0 $ gives $A = 0$. But how do I use the other BC to determine $B?$ Then if $\lambda > 0 $ then the characteristic equation is $r^2 - \lambda = 0$ which has real, unequal roots $r = \pm \sqrt{\lambda }$. Then, $$F(x) = Ae^{\sqrt \lambda x } + B e^{-\sqrt \lambda x }$$ See that $F'(0) = 0 = \sqrt \lambda [Ae^0-Be^0]$ so $A=B$, but how do I use the other BC to determine $B$ Finally if $\lambda = -p^2<0$ then the characteristic eqn $r^2 - \lambda = r^2 - p^2 = 0 $ which has complex roots $r=\pm pi$. Thus $F(x) = A\cos px + B\sin px$. Then, $$F'(0) = 0 = -Ap\sin 0 + Bp\cos 0 = Bp$$ Then $B = 0$ But how do I use the other BC to determine $A$ After this I know I must do the same procedure for $G'-9 \lambda G = 0 $","I am trying to use separation of variables method to solve $$ u_t = 9u_{xx}, \quad 0<x<1,\quad t>0$$ $$ u_x(0,t) = 0$$ $$u(1,t)=u(0,t) $$ $$u(x,0) = \sin\pi x, \quad 0 \leq x \le 1$$ and find an approximation (actual number) for large $t$. Here are my workings so far but I am running into some problems. Assume $u(x,t) = F(x)G(t)$ so that $$u_x = F'G, u_{xx} = F''G, u_t = FG'$$ Thus our equation PDE becomes, $$9F''G = FG' $$ Rearranging this equation yields, $$\frac{F''(x)}{F(x)} = \frac{G'(t)}{9G(t)} = \lambda$$ Thus we form two ODE'S, $$F''-\lambda F = 0$$ $$G'-9\lambda G = 0 $$ Using the B.C $u_x(0,t) = 0$ gives $F'(0) = 0 $ with $A\ne 0$ Thus we solve $F'' - \lambda F = 0$ using the above B Then if $\lambda = 0 $ then $F(x) = Ax + B$ and $F'(0) = 0 $ gives $A = 0$. But how do I use the other BC to determine $B?$ Then if $\lambda > 0 $ then the characteristic equation is $r^2 - \lambda = 0$ which has real, unequal roots $r = \pm \sqrt{\lambda }$. Then, $$F(x) = Ae^{\sqrt \lambda x } + B e^{-\sqrt \lambda x }$$ See that $F'(0) = 0 = \sqrt \lambda [Ae^0-Be^0]$ so $A=B$, but how do I use the other BC to determine $B$ Finally if $\lambda = -p^2<0$ then the characteristic eqn $r^2 - \lambda = r^2 - p^2 = 0 $ which has complex roots $r=\pm pi$. Thus $F(x) = A\cos px + B\sin px$. Then, $$F'(0) = 0 = -Ap\sin 0 + Bp\cos 0 = Bp$$ Then $B = 0$ But how do I use the other BC to determine $A$ After this I know I must do the same procedure for $G'-9 \lambda G = 0 $",,"['ordinary-differential-equations', 'partial-differential-equations']"
78,How to obtain the solution of a differential equation using a convolution integral?,How to obtain the solution of a differential equation using a convolution integral?,,"I need to express the solution of this initial value problem about vibration below using Convolution Integral; $$my''+cy'+ky=f(t) \quad y(0)=0,\quad y'(0)=0$$ But don't have any idea where do i use the Convolution Integral. So how do I do it? I tried to take laplace transform of both sides. $$ (ms^2+cs+k)Y(s)=L(f(t))\quad (assuming \quad L(y(t))=Y(s)) $$ I presumed; $$ f(t)=\int_0^t g(t-T)h(T) \,dT \quad L(g(t))=G(s) \quad L(h(t))=H(s) $$ So the Convolution theorem gives me the laplace transform of the right side; $$ L(f(t))=G(s)H(s) $$ and putting it into the equation; $$ (ms^2+cs+k)Y(s)=G(s)H(s) $$ $$ Y(s)=\frac{G(s)H(s)}{ms^2+cs+k} $$ $$ y(t)=L^{-1}(\frac{G(s)H(s)}{ms^2+cs+k}) $$ I don't know if this solution is enough or correct.","I need to express the solution of this initial value problem about vibration below using Convolution Integral; $$my''+cy'+ky=f(t) \quad y(0)=0,\quad y'(0)=0$$ But don't have any idea where do i use the Convolution Integral. So how do I do it? I tried to take laplace transform of both sides. $$ (ms^2+cs+k)Y(s)=L(f(t))\quad (assuming \quad L(y(t))=Y(s)) $$ I presumed; $$ f(t)=\int_0^t g(t-T)h(T) \,dT \quad L(g(t))=G(s) \quad L(h(t))=H(s) $$ So the Convolution theorem gives me the laplace transform of the right side; $$ L(f(t))=G(s)H(s) $$ and putting it into the equation; $$ (ms^2+cs+k)Y(s)=G(s)H(s) $$ $$ Y(s)=\frac{G(s)H(s)}{ms^2+cs+k} $$ $$ y(t)=L^{-1}(\frac{G(s)H(s)}{ms^2+cs+k}) $$ I don't know if this solution is enough or correct.",,"['ordinary-differential-equations', 'laplace-transform', 'convolution']"
79,"Localized center modes with exponential decay tails, solved from non-linear differential equations","Localized center modes with exponential decay tails, solved from non-linear differential equations",,"Two coupled non-linear differential equations in a radial $r$-direction in the region $r \in [0, \infty)$: $$-a\big(\partial_r^2+\frac{\partial_r}{r}\big) U(r)+  B(r) \partial_r V(r)=0, $$   $$ -B(r) \partial_r  U(r) + a\big(\partial_r^2+\frac{\partial_r}{r}\big) V(r) =0, $$   We like to solve $U(r)$ and $V(r)$. The B(r) is given such that $B(r)$ is a nice smooth differentiable function, with $$B(0)=0$$ $$\lim_{r \to 0} B(r)=0$$ $$\lim_{r \to \infty} B(r)=b=constant >0,$$ and $B(r)>0$ is monotonically increasing along $r \in [0, \infty)$, also $$a=constant >0.$$ Both $a$ and $b$ are finite values. I have done some analysis myself. My expected analysis find that $U(r)$ and $V(r)$ have exponential decay tails that look like  $$\exp[-\int_0^r B(r')^{\#} dr']$$ The ${\#}$ means some tentative power. And both $U(r)$ and $V(r)$ likely contain Bessel functions $J_0(r),J_1(r), ...,etc$. What are the exact solutions of $U(r)$ and $V(r)$? I suppose that they have localized center modes at $r=0$ (namely, $U(0)$ and $V(0)$ are maximum and positive) with exponential decay tails $\lim_{r \to 0} U(r)=\lim_{r \to 0} V(r)=0.$ If exact analytic solutions are NOT possible, please give arguments, and please feel free to take approximations. Personally I believe that it can be solved analytically exactly by some Bessel type functions. (p.s. This is not a homework problem. Just do some trial analysis done by myself.)","Two coupled non-linear differential equations in a radial $r$-direction in the region $r \in [0, \infty)$: $$-a\big(\partial_r^2+\frac{\partial_r}{r}\big) U(r)+  B(r) \partial_r V(r)=0, $$   $$ -B(r) \partial_r  U(r) + a\big(\partial_r^2+\frac{\partial_r}{r}\big) V(r) =0, $$   We like to solve $U(r)$ and $V(r)$. The B(r) is given such that $B(r)$ is a nice smooth differentiable function, with $$B(0)=0$$ $$\lim_{r \to 0} B(r)=0$$ $$\lim_{r \to \infty} B(r)=b=constant >0,$$ and $B(r)>0$ is monotonically increasing along $r \in [0, \infty)$, also $$a=constant >0.$$ Both $a$ and $b$ are finite values. I have done some analysis myself. My expected analysis find that $U(r)$ and $V(r)$ have exponential decay tails that look like  $$\exp[-\int_0^r B(r')^{\#} dr']$$ The ${\#}$ means some tentative power. And both $U(r)$ and $V(r)$ likely contain Bessel functions $J_0(r),J_1(r), ...,etc$. What are the exact solutions of $U(r)$ and $V(r)$? I suppose that they have localized center modes at $r=0$ (namely, $U(0)$ and $V(0)$ are maximum and positive) with exponential decay tails $\lim_{r \to 0} U(r)=\lim_{r \to 0} V(r)=0.$ If exact analytic solutions are NOT possible, please give arguments, and please feel free to take approximations. Personally I believe that it can be solved analytically exactly by some Bessel type functions. (p.s. This is not a homework problem. Just do some trial analysis done by myself.)",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem']"
80,Why these tensor fields do not depend on the hypersurfaces chosen to define them?,Why these tensor fields do not depend on the hypersurfaces chosen to define them?,,"In the paper Dynamics of Extended Bodies in General Relativity. I. Momentum and Angular Momentum , W.G. Dixon proposes definitions for momentum and angular momentum of a certain distribution of matter in GR described by an energy momentum tensor $T$ and a current one-form $J$ on spacetime $(M,g)$. Essentialy, he defines $W = \operatorname{supp}T\cup \operatorname{supp}J$ as the so-called ""world tube"" of the matter and supposes that: if $\Sigma \subset M$ is one spacelike hypersurface such that $W\cap \Sigma\neq \emptyset$ then there is one open set $N_\Sigma\subset M$ such that $W\cap \Sigma\subset N_\Sigma$ and $N_\Sigma$ is a normal neighborhood of all its points. Under these conditions he defines: $$p^{\kappa}(z,\Sigma)=\int_\Sigma (K^\kappa_\alpha(x,z)\mathfrak{T}^{\alpha\beta}(x)+\Psi^{\kappa}(x,z)\mathfrak{J}^{\beta}(x))n_\beta(x)d^{n-1}x$$ $$S^{\kappa\lambda}(z,\Sigma)=2\int_\Sigma \sigma^{[\lambda}(H^{\kappa]}_\alpha(z,x)\mathfrak{T}^{\alpha\beta}(x)+\Phi^{\kappa]}(x)\mathfrak{J}^\beta(x))n_\beta(x)d^{n-1}x$$ where $\Sigma\subset M$ is a spacelike hypersurface, $n$ is its normal vector, with $z\in \Sigma\cap W \neq \emptyset$. Also $K^{\kappa}_\alpha$, $H^\kappa_\alpha$, $\Psi^\kappa$, $\Phi^\kappa$ are two-point tensors whose definitions don't really seem to matter here. The issue is that he derives that on maximally symmetric spacetimes (those with constant curvature), given a one-paramter family of hypersurfaces $\Sigma(s)$, $s\in (a,b)$ and a path $\gamma : (a,b)\to M$ with $\gamma(s)\in \Sigma(s)$ the tensor fields over $\gamma$ defined by $$p^{\kappa}(s)=p^{\kappa}(\gamma(s),\Sigma(s)),\quad S^{\kappa \lambda}(s)=S^{\kappa\lambda}(\gamma(s),\Sigma(s))$$ satisfy the differential equations (with $k$ the constant curvature) $$\dfrac{D}{ds}p^{\kappa}=k S^{\kappa\lambda}\dot{\gamma}_{\lambda},\quad \dfrac{D}{ds}S^{\kappa\lambda}=2p^{[\kappa}\dot{\gamma}^{\lambda]}.$$ Now the author of the paper says in his words: The pair of equations (5.7) and (5.9) [those above] can now be integrated along $L$ given the values of $p^\kappa$ and $S^{\kappa\lambda}$ at one point of it. This shows that $p^{\kappa}$ and $S^{\kappa\lambda}$ must be independent of the particular choice of $\Sigma$, depending only on the point $z$ at which they are evaluated. They are thus well defined tensor fields on $M$. Now I can't understand. He says that those equations implies that the definitions he gave are actually independent of $\Sigma$, i.e., $p^{\kappa}(z,\Sigma)=p^{\kappa}(z,\Sigma')$ and $S^{\kappa\lambda}(z,\Sigma)=S^{\kappa\lambda}(z,\Sigma')$ for distinct $\Sigma,\Sigma'$ with $z\in \Sigma\cap \Sigma'\cap W$. How to understand what the auhtor says? Why these equations implies independence of $\Sigma$? I thought that it is because if I pick the same curve with two choices of $\Sigma,\Sigma'$, the differential equation is the same, but if I pick another curve connecting two points, why would it be the same, for example? What really is the point here that solving this equation on the curve implies indepdence of $\Sigma$?","In the paper Dynamics of Extended Bodies in General Relativity. I. Momentum and Angular Momentum , W.G. Dixon proposes definitions for momentum and angular momentum of a certain distribution of matter in GR described by an energy momentum tensor $T$ and a current one-form $J$ on spacetime $(M,g)$. Essentialy, he defines $W = \operatorname{supp}T\cup \operatorname{supp}J$ as the so-called ""world tube"" of the matter and supposes that: if $\Sigma \subset M$ is one spacelike hypersurface such that $W\cap \Sigma\neq \emptyset$ then there is one open set $N_\Sigma\subset M$ such that $W\cap \Sigma\subset N_\Sigma$ and $N_\Sigma$ is a normal neighborhood of all its points. Under these conditions he defines: $$p^{\kappa}(z,\Sigma)=\int_\Sigma (K^\kappa_\alpha(x,z)\mathfrak{T}^{\alpha\beta}(x)+\Psi^{\kappa}(x,z)\mathfrak{J}^{\beta}(x))n_\beta(x)d^{n-1}x$$ $$S^{\kappa\lambda}(z,\Sigma)=2\int_\Sigma \sigma^{[\lambda}(H^{\kappa]}_\alpha(z,x)\mathfrak{T}^{\alpha\beta}(x)+\Phi^{\kappa]}(x)\mathfrak{J}^\beta(x))n_\beta(x)d^{n-1}x$$ where $\Sigma\subset M$ is a spacelike hypersurface, $n$ is its normal vector, with $z\in \Sigma\cap W \neq \emptyset$. Also $K^{\kappa}_\alpha$, $H^\kappa_\alpha$, $\Psi^\kappa$, $\Phi^\kappa$ are two-point tensors whose definitions don't really seem to matter here. The issue is that he derives that on maximally symmetric spacetimes (those with constant curvature), given a one-paramter family of hypersurfaces $\Sigma(s)$, $s\in (a,b)$ and a path $\gamma : (a,b)\to M$ with $\gamma(s)\in \Sigma(s)$ the tensor fields over $\gamma$ defined by $$p^{\kappa}(s)=p^{\kappa}(\gamma(s),\Sigma(s)),\quad S^{\kappa \lambda}(s)=S^{\kappa\lambda}(\gamma(s),\Sigma(s))$$ satisfy the differential equations (with $k$ the constant curvature) $$\dfrac{D}{ds}p^{\kappa}=k S^{\kappa\lambda}\dot{\gamma}_{\lambda},\quad \dfrac{D}{ds}S^{\kappa\lambda}=2p^{[\kappa}\dot{\gamma}^{\lambda]}.$$ Now the author of the paper says in his words: The pair of equations (5.7) and (5.9) [those above] can now be integrated along $L$ given the values of $p^\kappa$ and $S^{\kappa\lambda}$ at one point of it. This shows that $p^{\kappa}$ and $S^{\kappa\lambda}$ must be independent of the particular choice of $\Sigma$, depending only on the point $z$ at which they are evaluated. They are thus well defined tensor fields on $M$. Now I can't understand. He says that those equations implies that the definitions he gave are actually independent of $\Sigma$, i.e., $p^{\kappa}(z,\Sigma)=p^{\kappa}(z,\Sigma')$ and $S^{\kappa\lambda}(z,\Sigma)=S^{\kappa\lambda}(z,\Sigma')$ for distinct $\Sigma,\Sigma'$ with $z\in \Sigma\cap \Sigma'\cap W$. How to understand what the auhtor says? Why these equations implies independence of $\Sigma$? I thought that it is because if I pick the same curve with two choices of $\Sigma,\Sigma'$, the differential equation is the same, but if I pick another curve connecting two points, why would it be the same, for example? What really is the point here that solving this equation on the curve implies indepdence of $\Sigma$?",,"['ordinary-differential-equations', 'differential-geometry', 'mathematical-physics', 'general-relativity', 'semi-riemannian-geometry']"
81,Finding integrating factor for non-exact differential equation $(4y-10x)dx+(4x-6x^2y^{-1})dy=0$.,Finding integrating factor for non-exact differential equation .,(4y-10x)dx+(4x-6x^2y^{-1})dy=0,"I am given this equation $$(4y-10x)dx+(4x-6x^2y^{-1})dy=0$$ where I must find an integrating factor to turn this into an exact differential. The integrating factor is supposed to be in the form $\mu=x^ny^m$. I have found $M_{y}=4$ and $N_{x}=(4-12xy^{-1})$. It is here where I get stuck. How do I go about finding the integrating factor in the form $\mu=x^ny^m$, and what would the $n$ and $m$ end up being? Any help would be greatly appreciated!","I am given this equation $$(4y-10x)dx+(4x-6x^2y^{-1})dy=0$$ where I must find an integrating factor to turn this into an exact differential. The integrating factor is supposed to be in the form $\mu=x^ny^m$. I have found $M_{y}=4$ and $N_{x}=(4-12xy^{-1})$. It is here where I get stuck. How do I go about finding the integrating factor in the form $\mu=x^ny^m$, and what would the $n$ and $m$ end up being? Any help would be greatly appreciated!",,"['ordinary-differential-equations', 'integrating-factor']"
82,Diffusion/Heat equation with spatially variable source,Diffusion/Heat equation with spatially variable source,,"Many resources including but not limited to 1 , 2 , 3 , 4 , 5 , 6 , 7 ** discuss various forms of diffusion and heat equations under different conditions. However, the following specific formulation with a spatially variable source and inhomogeneous flux BCs was not studied in them: $$\frac{\partial c}{\partial t}=\frac{\partial}{\partial x}J=\frac{\partial}{\partial x}(k(\frac{\partial c}{\partial x} +S)) \quad \text{or} \quad c_t=k c_{xx}+k S_x$$ $$J(-L/2,t)=J_{-} \\ J(+L/2,t)=J_{+} \\ c(x,0)=c_0$$ where $-L/2\leq x \leq L/2$ and $S(x)$ is spatially variable. I'm looking for a nice closed form or an (infinite series-based) analytic expression -- if possible. I have done some work by using the technique of Laplace transformations but could not get it done fully. Any help on the rest of my work is highly appreciated. Here is my attempt: Transform inhomogeneous BCs to homogeneous BCs Introducing $u$ and $v$ such that $c = u + v$ and $u$ satisfies the non-homogeneous BCs, yields: \begin{align*} x = -L/2: & \quad u_x = (J_-/k) - S_- \\ x = +L/2: & \quad u_x = (J_+/k) - S_+ \end{align*} where the subscripts for $J$ and $S$ indicate evaluation at the corresponding endpoint. An easy function $u$ is $u(x) = A x^2 + B x$, where $A=\frac{(J_+-J_-)-(S_+-S_-) k}{2kL}$ and $B=\frac{(J_++J_-)-(S_++S_-) k}{2k}$. Now, the problem for $v$ is homogeneous and given by ($-L/2 < x < L/2$): $$ v_t = kv_{xx} + Q(x) $$ $$v_x(-L/2,t) = v_x(+L/2,t) = 0 \\ v(x,0) = c_0 - u(x)$$ where the source term is $Q(x) = k (u_x + S)_x$ which is also known. Thanks to superposition, the problem has now homogeneous BCs but contains a nonzero, in general, source term. I found the solution for the problem with $Q = 0$ and $0<x<L$ ( rather that $-L/2<x<L/2$) which is as follows (I verified it with 5 ): $$v=2\sum_{n=1}^{n=\infty}e^{-k\alpha_n^2t}\frac{\beta_n \cos(\alpha_n x)}{\alpha_n^2L/2}\int_{0}^{L}f(x)[\beta_n \cos(\alpha_n x)]dx+\frac{1}{L}\int_{0}^{L}f(x)dx$$ where $\beta_n=\alpha_n \cos(\alpha_n L/2)$ and $\alpha_n=\frac{(2n+1)\pi}{L}$. Also $f(x)=c_0-u(x)$. Extending the solution to where there is $Q(x)$ I was not able to convert the intervl from $0<x<L$ to $-L/2<x<L/2$ properly and expand the obtained solution to the original problem with a spatially variable source (i.e. $Q(x)$). A sanity check is also greatly appreciate as there might be some errors in my work. ** Essentially the question asked in 7 is even a more general form of the current question. While I learned a lot from the discussion done in 7 , the questions remained unanswered (in a complete way). The main and intersting complication in 7 was the varying diffusion coefficient rather than what is focused in current question. Special thanks to dmoreno for providing fruitful discussions.","Many resources including but not limited to 1 , 2 , 3 , 4 , 5 , 6 , 7 ** discuss various forms of diffusion and heat equations under different conditions. However, the following specific formulation with a spatially variable source and inhomogeneous flux BCs was not studied in them: $$\frac{\partial c}{\partial t}=\frac{\partial}{\partial x}J=\frac{\partial}{\partial x}(k(\frac{\partial c}{\partial x} +S)) \quad \text{or} \quad c_t=k c_{xx}+k S_x$$ $$J(-L/2,t)=J_{-} \\ J(+L/2,t)=J_{+} \\ c(x,0)=c_0$$ where $-L/2\leq x \leq L/2$ and $S(x)$ is spatially variable. I'm looking for a nice closed form or an (infinite series-based) analytic expression -- if possible. I have done some work by using the technique of Laplace transformations but could not get it done fully. Any help on the rest of my work is highly appreciated. Here is my attempt: Transform inhomogeneous BCs to homogeneous BCs Introducing $u$ and $v$ such that $c = u + v$ and $u$ satisfies the non-homogeneous BCs, yields: \begin{align*} x = -L/2: & \quad u_x = (J_-/k) - S_- \\ x = +L/2: & \quad u_x = (J_+/k) - S_+ \end{align*} where the subscripts for $J$ and $S$ indicate evaluation at the corresponding endpoint. An easy function $u$ is $u(x) = A x^2 + B x$, where $A=\frac{(J_+-J_-)-(S_+-S_-) k}{2kL}$ and $B=\frac{(J_++J_-)-(S_++S_-) k}{2k}$. Now, the problem for $v$ is homogeneous and given by ($-L/2 < x < L/2$): $$ v_t = kv_{xx} + Q(x) $$ $$v_x(-L/2,t) = v_x(+L/2,t) = 0 \\ v(x,0) = c_0 - u(x)$$ where the source term is $Q(x) = k (u_x + S)_x$ which is also known. Thanks to superposition, the problem has now homogeneous BCs but contains a nonzero, in general, source term. I found the solution for the problem with $Q = 0$ and $0<x<L$ ( rather that $-L/2<x<L/2$) which is as follows (I verified it with 5 ): $$v=2\sum_{n=1}^{n=\infty}e^{-k\alpha_n^2t}\frac{\beta_n \cos(\alpha_n x)}{\alpha_n^2L/2}\int_{0}^{L}f(x)[\beta_n \cos(\alpha_n x)]dx+\frac{1}{L}\int_{0}^{L}f(x)dx$$ where $\beta_n=\alpha_n \cos(\alpha_n L/2)$ and $\alpha_n=\frac{(2n+1)\pi}{L}$. Also $f(x)=c_0-u(x)$. Extending the solution to where there is $Q(x)$ I was not able to convert the intervl from $0<x<L$ to $-L/2<x<L/2$ properly and expand the obtained solution to the original problem with a spatially variable source (i.e. $Q(x)$). A sanity check is also greatly appreciate as there might be some errors in my work. ** Essentially the question asked in 7 is even a more general form of the current question. While I learned a lot from the discussion done in 7 , the questions remained unanswered (in a complete way). The main and intersting complication in 7 was the varying diffusion coefficient rather than what is focused in current question. Special thanks to dmoreno for providing fruitful discussions.",,"['ordinary-differential-equations', 'partial-differential-equations', 'taylor-expansion', 'laplace-transform', 'heat-equation']"
83,When to use forward difference over central difference/three point difference?,When to use forward difference over central difference/three point difference?,,"I understand that central difference and three point difference namely $\frac{−3 f(x) +4 f(x+h)− f(x+2h)}{2h}$ provide approximations of the first derivative up to a term of order $h^2$. Forward difference only approximates up to a term of order $h$. So for most situations central difference would be preferred over both three point difference (denominator contains 3! rather than 3) and forward difference. In what situations would forward difference be better than both central or three point difference? In what situations would three point difference be better? I can really only think of one situation where three point would be better where we want to approximate an x for which we know nothing about $f(x_0), x_0<x$. I cannot think of any situations that forward difference would be better assuming f(x) is smooth","I understand that central difference and three point difference namely $\frac{−3 f(x) +4 f(x+h)− f(x+2h)}{2h}$ provide approximations of the first derivative up to a term of order $h^2$. Forward difference only approximates up to a term of order $h$. So for most situations central difference would be preferred over both three point difference (denominator contains 3! rather than 3) and forward difference. In what situations would forward difference be better than both central or three point difference? In what situations would three point difference be better? I can really only think of one situation where three point would be better where we want to approximate an x for which we know nothing about $f(x_0), x_0<x$. I cannot think of any situations that forward difference would be better assuming f(x) is smooth",,"['ordinary-differential-equations', 'numerical-methods', 'approximation', 'finite-differences']"
84,Forward and Backward Euler,Forward and Backward Euler,,"I want to analytically solve $$ \frac{\partial y}{\partial t} = −αy,$$ where initial conditions are $y=1$ at $t=0$ and show the forward Euler method gets a smaller answer than the backward Euler method  $\forall$ $t>0$, provided that $0$ <$\alpha^2$ $\Delta t^2$ < $1$ Any help whatsoever will be appreciated!","I want to analytically solve $$ \frac{\partial y}{\partial t} = −αy,$$ where initial conditions are $y=1$ at $t=0$ and show the forward Euler method gets a smaller answer than the backward Euler method  $\forall$ $t>0$, provided that $0$ <$\alpha^2$ $\Delta t^2$ < $1$ Any help whatsoever will be appreciated!",,"['ordinary-differential-equations', 'numerical-methods']"
85,Frechet differentiable implies continuity in $\mathbb{R} ^n$,Frechet differentiable implies continuity in,\mathbb{R} ^n,"Need help completing a proof. The statement is as follows: Let $E \subset \mathbb{R} ^m$ be an open set. If $f:E \longrightarrow \mathbb{R}^n $ is Frechet differentiable at $a \in E$ , then it is continuous at a. Proof : Let $f:E \longrightarrow \mathbb{R}^n $ be Frechet differentiable. Then  for $a \in E$ , $\exists$ a linear map $A:\mathbb{R}^m \longrightarrow \mathbb{R}^n $ s.t. $\forall \epsilon>0$ , $\exists\delta >0$ , if $h \in \mathbb{R}^m$ and $0<||h||<\delta$ , then $$||f(a+h)-f(a)-Ah||<\epsilon||h||$$ We then have for $$||f(a+h)-f(a)||\leq||h||\frac{||f(a+h)-f(a)-Ah||}{||h||}+||Ah||$$ by the triangle inequality. But I get stuck here. I feel that I'm very close to finishing it but just cant see it. Any hints or help will be appreciated!","Need help completing a proof. The statement is as follows: Let be an open set. If is Frechet differentiable at , then it is continuous at a. Proof : Let be Frechet differentiable. Then  for , a linear map s.t. , , if and , then We then have for by the triangle inequality. But I get stuck here. I feel that I'm very close to finishing it but just cant see it. Any hints or help will be appreciated!",E \subset \mathbb{R} ^m f:E \longrightarrow \mathbb{R}^n  a \in E f:E \longrightarrow \mathbb{R}^n  a \in E \exists A:\mathbb{R}^m \longrightarrow \mathbb{R}^n  \forall \epsilon>0 \exists\delta >0 h \in \mathbb{R}^m 0<||h||<\delta ||f(a+h)-f(a)-Ah||<\epsilon||h|| ||f(a+h)-f(a)||\leq||h||\frac{||f(a+h)-f(a)-Ah||}{||h||}+||Ah||,['ordinary-differential-equations']
86,Closed-form formula for the differential equation $D^{(n)}(f)=f$,Closed-form formula for the differential equation,D^{(n)}(f)=f,"Let $n$ be a positive integer and $S_n$ be the set of all functions from $\mathbb{R}$ to $\mathbb{R}$ whose $n$th derivative is defined everywhere. In terms of the parameter $n$ and arbitrary real constants, is there a closed-form formula for the differential equation $D^{(n)}(f)=f$, where $D$ is the differentiation operator? Also, what if we consider functions from $\mathbb{C}$ to $\mathbb{C}$?","Let $n$ be a positive integer and $S_n$ be the set of all functions from $\mathbb{R}$ to $\mathbb{R}$ whose $n$th derivative is defined everywhere. In terms of the parameter $n$ and arbitrary real constants, is there a closed-form formula for the differential equation $D^{(n)}(f)=f$, where $D$ is the differentiation operator? Also, what if we consider functions from $\mathbb{C}$ to $\mathbb{C}$?",,['ordinary-differential-equations']
87,Solve differential equation $\left((1-x^2)\frac{d^2}{dx^2}-2x\frac{d}{dx}-\frac{n^2}{1-x^2}\right)y(x)=0$,Solve differential equation,\left((1-x^2)\frac{d^2}{dx^2}-2x\frac{d}{dx}-\frac{n^2}{1-x^2}\right)y(x)=0,"I have to solve the differential equation $$\left((1-x^2)\frac{d^2}{dx^2}-2x\frac{d}{dx}-\frac{n^2}{1-x^2}\right)y(x)=0$$ In order to solve above differential equation I have substitution i.e. if we let $$z=\ln\frac{1+x}{1-x}$$ This substitution can solve the above differential equation,but I don't know how we suppose this substitution, and what is basic concept behind this substitution. Is there any best method to solve this differential equation (any other substitution), if I face any other such type of differential equation?","I have to solve the differential equation $$\left((1-x^2)\frac{d^2}{dx^2}-2x\frac{d}{dx}-\frac{n^2}{1-x^2}\right)y(x)=0$$ In order to solve above differential equation I have substitution i.e. if we let $$z=\ln\frac{1+x}{1-x}$$ This substitution can solve the above differential equation,but I don't know how we suppose this substitution, and what is basic concept behind this substitution. Is there any best method to solve this differential equation (any other substitution), if I face any other such type of differential equation?",,"['integration', 'ordinary-differential-equations', 'greens-function']"
88,Calculus of Variation: Trouble finding the Euler-Lagrange equation,Calculus of Variation: Trouble finding the Euler-Lagrange equation,,"Let $\;f:\mathbb R^n\rightarrow \mathbb R^m\;$ and $\;G:\mathbb R^m  \rightarrow \mathbb R_{+}\;$ (NOTE: $\;n\;$ is not necessary equal to   $\;m\;$). Assume the functional: $\;I_{\mathbb R^n} (f) = \int_{\mathbb R^n} \frac{1}{2} {\vert \nabla  f \vert}^2 + G(f) \;dx\;$ where $\;\nabla f=(\frac{\partial  f_i}{\partial x_j})_{1\le i \le m,1\le j \le n}\;$ and $\;\vert \cdot  \vert\;$is the Euclidean norm of the matrix. Prove the Euler-Lagrange equation of the above functional is given by   the system : $\;\Delta f -G_f(f)=0\;$ where $\;G_f(f)=(\frac{\partial  G}{\partial f_1}, \dots, \frac{\partial G}{\partial f_m})^{T}\;$ My attempt: I searched on my Evans PDE's book , where I found this: The formula $\;(16)\;$, I believe is the solution to my problem. However I have trouble applying it here because I don't know what exactly is the $\;\frac{1}{2} {\vert \nabla f \vert}^2\;$. I understand that $\;p_i^{k}\;$ from the book stands for $\;\frac{\partial  f_k}{\partial x_i}\;$ but I'm a bit unsure how this appears in $\;\frac{1}{2} {\vert \nabla f \vert}^2\;$. How can I compute $\;L_{p_i^{k}}\;$ if I don't know what $\;\frac{1}{2} {\vert \nabla f \vert}^2\;$ looks like? I would really appreciate any help because I've been stuck here! Thanks in advance","Let $\;f:\mathbb R^n\rightarrow \mathbb R^m\;$ and $\;G:\mathbb R^m  \rightarrow \mathbb R_{+}\;$ (NOTE: $\;n\;$ is not necessary equal to   $\;m\;$). Assume the functional: $\;I_{\mathbb R^n} (f) = \int_{\mathbb R^n} \frac{1}{2} {\vert \nabla  f \vert}^2 + G(f) \;dx\;$ where $\;\nabla f=(\frac{\partial  f_i}{\partial x_j})_{1\le i \le m,1\le j \le n}\;$ and $\;\vert \cdot  \vert\;$is the Euclidean norm of the matrix. Prove the Euler-Lagrange equation of the above functional is given by   the system : $\;\Delta f -G_f(f)=0\;$ where $\;G_f(f)=(\frac{\partial  G}{\partial f_1}, \dots, \frac{\partial G}{\partial f_m})^{T}\;$ My attempt: I searched on my Evans PDE's book , where I found this: The formula $\;(16)\;$, I believe is the solution to my problem. However I have trouble applying it here because I don't know what exactly is the $\;\frac{1}{2} {\vert \nabla f \vert}^2\;$. I understand that $\;p_i^{k}\;$ from the book stands for $\;\frac{\partial  f_k}{\partial x_i}\;$ but I'm a bit unsure how this appears in $\;\frac{1}{2} {\vert \nabla f \vert}^2\;$. How can I compute $\;L_{p_i^{k}}\;$ if I don't know what $\;\frac{1}{2} {\vert \nabla f \vert}^2\;$ looks like? I would really appreciate any help because I've been stuck here! Thanks in advance",,"['matrices', 'ordinary-differential-equations', 'normed-spaces', 'calculus-of-variations', 'euler-lagrange-equation']"
89,Find solutions of a dynamical system,Find solutions of a dynamical system,,"Let $t\in\mathbb{R}$. Consider the following autonomous dynamical system: \begin{equation}%\label{eqn: u'(phi)} x'(t)=-\frac{a_{12}\, r _2-r _1\left(\frac{d_2}{y(t)}+a_{22}\right)}{a_{12}\,a_{21}-\left(\frac{d_1}{x(t)}+a_{11}\right)\left(\frac{d_2}{y(t)}+a_{22}\right)}, \end{equation} \begin{equation}%\label{eqn: v'(phi)} y'(t)=-\frac{a_{21}\, r _1-r _2\left(\frac{d_1}{x(t)}+a_{11}\right)}{a_{12}\,a_{21}-\left(\frac{d_1}{x(t)}+a_{11}\right)\left(\frac{d_2}{y(t)}+a_{22}\right)}, \end{equation} where $a_{ij} (i,j=1,2)$, $d_i (i=1,2)$ and $r_1$ are positive constants; $r_2$ is a negative constant. I have found numerical solutions for some parameters. These numerical solutions show the monotonicity property, i.e. $x'(t)<0$ and $y'(t)>0$. However, I have no idea to give a rigorous proof and determine under which parameters the solutions are monotone. Moreover, I also want to try some standard methods in dynamical systems to find other types of solution which are not monotone. I am not so familiar with dynamical systems. Any suggestion, idea, or comment is welcome, thanks!","Let $t\in\mathbb{R}$. Consider the following autonomous dynamical system: \begin{equation}%\label{eqn: u'(phi)} x'(t)=-\frac{a_{12}\, r _2-r _1\left(\frac{d_2}{y(t)}+a_{22}\right)}{a_{12}\,a_{21}-\left(\frac{d_1}{x(t)}+a_{11}\right)\left(\frac{d_2}{y(t)}+a_{22}\right)}, \end{equation} \begin{equation}%\label{eqn: v'(phi)} y'(t)=-\frac{a_{21}\, r _1-r _2\left(\frac{d_1}{x(t)}+a_{11}\right)}{a_{12}\,a_{21}-\left(\frac{d_1}{x(t)}+a_{11}\right)\left(\frac{d_2}{y(t)}+a_{22}\right)}, \end{equation} where $a_{ij} (i,j=1,2)$, $d_i (i=1,2)$ and $r_1$ are positive constants; $r_2$ is a negative constant. I have found numerical solutions for some parameters. These numerical solutions show the monotonicity property, i.e. $x'(t)<0$ and $y'(t)>0$. However, I have no idea to give a rigorous proof and determine under which parameters the solutions are monotone. Moreover, I also want to try some standard methods in dynamical systems to find other types of solution which are not monotone. I am not so familiar with dynamical systems. Any suggestion, idea, or comment is welcome, thanks!",,"['ordinary-differential-equations', 'dynamical-systems']"
90,Lindstedt-Poincare method to find a valid first approximation,Lindstedt-Poincare method to find a valid first approximation,,"I want to use Lindstedt-Poincare method or multiple scale method to find a uniformly valid first approximation to the equation $\frac{d^2u}{dt^2}-\epsilon (1-u^2)\frac{du}{dt}+u=0$, and show that for a large class of initial conditions, the solution approaches a limit cycle as $t\rightarrow \infty$. I would appreciate any assistance.","I want to use Lindstedt-Poincare method or multiple scale method to find a uniformly valid first approximation to the equation $\frac{d^2u}{dt^2}-\epsilon (1-u^2)\frac{du}{dt}+u=0$, and show that for a large class of initial conditions, the solution approaches a limit cycle as $t\rightarrow \infty$. I would appreciate any assistance.",,"['ordinary-differential-equations', 'perturbation-theory']"
91,Solve the differential equation $x^2\frac{d^2y}{dx^2}+(x^3-x)\frac{dy}{dx}-3y=0$,Solve the differential equation,x^2\frac{d^2y}{dx^2}+(x^3-x)\frac{dy}{dx}-3y=0,"$$x^2\frac{d^2y}{dx^2}+(x^3-x)\frac{dy}{dx}-3y=0$$ $$\sum_{n=0}^{\infty}(n+r)(n+r-2)c_nx^{n+r}+\sum_{n=2}^{\infty}(n+r-2)c_{n-2}x^{n+r}-3\sum_{n=0}^{\infty}c_nx^{n+r}=0$$ $$r(r-2)c_0x^r+(r^2-1)c_1x^{1+r}-3c_0x^r-3c_1x^{r+1}+\sum_{n=2}^{\infty}[(n+r)(n+r-2)c_n+(n+r-2)c_{n-2}-3c_n]x^{n+r}=0$$ The indicial equation is $$(r-3)(r+1)=0$$ We have two possibility Deal with the larger value $3$ first, $$c_n=-\frac{(n+1)c_{n-2}}{n^2+4n}$$ Taking values for $n\geq2$ $$c_2=-\frac{c_0}{4}$$ $$c_3=0$$ $$c_4=-\frac{5c_2}{32}$$ $$c_5=0$$ $$y_1(x)=C_1x^3(1-\frac{x^2}{4}+\frac{5x^4}{128}+...)$$ The second possibility $r=-1$ $$c_n=-\frac{(n-3)c_{n-2}}{n^2-4n},n\neq4$$ we know that $c_4 is an arbitrary constant We know that $c_4$ will make $c_2=0$ from the above equation $$c_2=-\frac{c_0}{4}$$ We see that it contradicts our initial assumption that $c_0\neq0$ $$c_3=0$$ $$c_5=-\frac{2c_3}{5}=0$$ $$c_6=-\frac{c_4}{4}$$ $$c_7=0$$ $$c_8=-\frac{5c_6}{32}$$ $$y_2=(x^{-1})c_4(x^4-\frac{x^6}{4}+\frac{5x^8}{128}+...)$$ Obviously they are two sides of the same coin. How to find another linearly independent solution $y_2$. Is there a closed form for the above series? The thing seems very messy.","$$x^2\frac{d^2y}{dx^2}+(x^3-x)\frac{dy}{dx}-3y=0$$ $$\sum_{n=0}^{\infty}(n+r)(n+r-2)c_nx^{n+r}+\sum_{n=2}^{\infty}(n+r-2)c_{n-2}x^{n+r}-3\sum_{n=0}^{\infty}c_nx^{n+r}=0$$ $$r(r-2)c_0x^r+(r^2-1)c_1x^{1+r}-3c_0x^r-3c_1x^{r+1}+\sum_{n=2}^{\infty}[(n+r)(n+r-2)c_n+(n+r-2)c_{n-2}-3c_n]x^{n+r}=0$$ The indicial equation is $$(r-3)(r+1)=0$$ We have two possibility Deal with the larger value $3$ first, $$c_n=-\frac{(n+1)c_{n-2}}{n^2+4n}$$ Taking values for $n\geq2$ $$c_2=-\frac{c_0}{4}$$ $$c_3=0$$ $$c_4=-\frac{5c_2}{32}$$ $$c_5=0$$ $$y_1(x)=C_1x^3(1-\frac{x^2}{4}+\frac{5x^4}{128}+...)$$ The second possibility $r=-1$ $$c_n=-\frac{(n-3)c_{n-2}}{n^2-4n},n\neq4$$ we know that $c_4 is an arbitrary constant We know that $c_4$ will make $c_2=0$ from the above equation $$c_2=-\frac{c_0}{4}$$ We see that it contradicts our initial assumption that $c_0\neq0$ $$c_3=0$$ $$c_5=-\frac{2c_3}{5}=0$$ $$c_6=-\frac{c_4}{4}$$ $$c_7=0$$ $$c_8=-\frac{5c_6}{32}$$ $$y_2=(x^{-1})c_4(x^4-\frac{x^6}{4}+\frac{5x^8}{128}+...)$$ Obviously they are two sides of the same coin. How to find another linearly independent solution $y_2$. Is there a closed form for the above series? The thing seems very messy.",,['ordinary-differential-equations']
92,What's the correct analog of the (damped) spring for SO(3)/quaternions?,What's the correct analog of the (damped) spring for SO(3)/quaternions?,,"The traditional (linearly) damped spring is modeled via a standard second-order differential equation: $\ddot{\bf x}(t)+a\dot{\bf x}(t)+b{\bf x}(t)=0$; this can be 'recentered' by letting ${\bf x}(t)={\bf y}(t)-{\bf y_0}$.  With appropriate coefficients (particularly for critical damping, where the equation takes on a doubled eigenvalue), this can be used as an 'ease-in' to a specific target position $\bf y_0$. The same notion — easing-in to a target value — obviously makes sense on $SO(3)$, but the equation itself doesn't: if one takes e.g. the usual quaternionic embedding in $\mathbb{R}^4$ then this equation doesn't guarantee uniticity of $\mathbb{x}(t)$, and if instead one tries to work on the sphere proper, it's not clear to me how one can take what amounts to a 'second-order tangent bundle' here. What would the appropriate mathematical representation of a second-order differential equation like this on the sphere (or presumably on any other manifold, though $SO(3)$ and particularly its quaternion representation is the specific case I'm interested in from a computational standpoint) be?","The traditional (linearly) damped spring is modeled via a standard second-order differential equation: $\ddot{\bf x}(t)+a\dot{\bf x}(t)+b{\bf x}(t)=0$; this can be 'recentered' by letting ${\bf x}(t)={\bf y}(t)-{\bf y_0}$.  With appropriate coefficients (particularly for critical damping, where the equation takes on a doubled eigenvalue), this can be used as an 'ease-in' to a specific target position $\bf y_0$. The same notion — easing-in to a target value — obviously makes sense on $SO(3)$, but the equation itself doesn't: if one takes e.g. the usual quaternionic embedding in $\mathbb{R}^4$ then this equation doesn't guarantee uniticity of $\mathbb{x}(t)$, and if instead one tries to work on the sphere proper, it's not clear to me how one can take what amounts to a 'second-order tangent bundle' here. What would the appropriate mathematical representation of a second-order differential equation like this on the sphere (or presumably on any other manifold, though $SO(3)$ and particularly its quaternion representation is the specific case I'm interested in from a computational standpoint) be?",,"['ordinary-differential-equations', 'differential-geometry', 'mathematical-physics', 'mathematical-modeling', 'quaternions']"
93,Show differentiability of $e^{\int_0^t A(\tau)\operatorname{d}\tau}$,Show differentiability of,e^{\int_0^t A(\tau)\operatorname{d}\tau},"Consider $A\in C(\mathbb{R},\mathcal{L}(E))$ for some Banach space $E$. For any $t\in\mathbb{R}$ $$B(t):=e^{\int_0^t A(\tau)\operatorname{d}\tau}:= \sum_k \left(\int_0^t A(\tau)\operatorname{d}\tau\right)^k/(k!)$$ is well-defined. How can I show that $B\in C^1(\mathbb{R},\mathcal{L}(E))$? Is a general result about the differentiability of $e^{f(\cdot)}$ for $f\in C^1(\mathbb{R},\mathcal{L}(E))$ possible?","Consider $A\in C(\mathbb{R},\mathcal{L}(E))$ for some Banach space $E$. For any $t\in\mathbb{R}$ $$B(t):=e^{\int_0^t A(\tau)\operatorname{d}\tau}:= \sum_k \left(\int_0^t A(\tau)\operatorname{d}\tau\right)^k/(k!)$$ is well-defined. How can I show that $B\in C^1(\mathbb{R},\mathcal{L}(E))$? Is a general result about the differentiability of $e^{f(\cdot)}$ for $f\in C^1(\mathbb{R},\mathcal{L}(E))$ possible?",,"['real-analysis', 'ordinary-differential-equations', 'banach-spaces', 'matrix-exponential']"
94,When to multiply the assumed particular solution by $t$ (Method of undetermined coefficients),When to multiply the assumed particular solution by  (Method of undetermined coefficients),t,"I'm given the DE : $$y''-y = 8te^{t}$$ First I solve the corresponding homogenous equation : $$r^2-1 = 0 \implies r_1= 1 \ r_2= -1$$ So The general solution of the corresponding homogenous equation looks like : $$y_c = c_1e^t + c_2e^{-t}$$ Now I want to find a particular solution so I assume $Y_1(t)$ solves the DE $$Y_1(t) = (At + B) e^t$$ Calculating we end up with : $$Y_1''(t) = (2A+B)e^t + Ate^t$$ However then the DE takes a form : $$(2A+B)e^t = 8te^t$$ Which does not make sense. So I believe I should have multiplied the assumed particular solution by $t$ beforehand. However I know that I do this multiplication when the particular solution is in the homogenous equation. However it is not. So I am confused. In General, I have a problem about when to multiply the assumed particular equation by t. For instance consider : $$f(x,y,y',y'') = xe^xsinx$$ Where $f$ is linear in all its input variables. Now let us further assume the corresponding homogenous equation is in some form of this : $$c_1e^x+c_2e^{-x}+ xc_3$$ Now How should I contruct the particular solution? Should I multiply the particular solution by $x?$ I don't think so since $sinx$ is not in the homogenous equation. However, If someone could give me an explanation on these two questions and make my mind clear about this issue I would be really glad. Thanks,","I'm given the DE : $$y''-y = 8te^{t}$$ First I solve the corresponding homogenous equation : $$r^2-1 = 0 \implies r_1= 1 \ r_2= -1$$ So The general solution of the corresponding homogenous equation looks like : $$y_c = c_1e^t + c_2e^{-t}$$ Now I want to find a particular solution so I assume $Y_1(t)$ solves the DE $$Y_1(t) = (At + B) e^t$$ Calculating we end up with : $$Y_1''(t) = (2A+B)e^t + Ate^t$$ However then the DE takes a form : $$(2A+B)e^t = 8te^t$$ Which does not make sense. So I believe I should have multiplied the assumed particular solution by $t$ beforehand. However I know that I do this multiplication when the particular solution is in the homogenous equation. However it is not. So I am confused. In General, I have a problem about when to multiply the assumed particular equation by t. For instance consider : $$f(x,y,y',y'') = xe^xsinx$$ Where $f$ is linear in all its input variables. Now let us further assume the corresponding homogenous equation is in some form of this : $$c_1e^x+c_2e^{-x}+ xc_3$$ Now How should I contruct the particular solution? Should I multiply the particular solution by $x?$ I don't think so since $sinx$ is not in the homogenous equation. However, If someone could give me an explanation on these two questions and make my mind clear about this issue I would be really glad. Thanks,",,[]
95,Solving an ordinary differential equation with Laplace Transform,Solving an ordinary differential equation with Laplace Transform,,"I am trying to solve the following ordinary differential equation, where $y$ is a function of $t$: $$ty''+(2+4t)y'+(4+4t)y=0.$$ My attempt was to use the Laplace Transform. The following formulae were assumed, where $\mathcal{L}(y)$ is the Laplace transform of a function $y$ defined by $$\mathcal{L}(y(t))=\int_0^{\infty}e^{-st}y(t)dt=Y(s):$$ $\mathcal{L}(ty'')=-s^2Y'(s)-2sY(s)+y(0)$; $\mathcal{L}(ty')=-sY'(s)-Y(s)$; $\mathcal{L}(ty)=-Y'(s)$; $\mathcal{L}(y')=sY(s)-y(0)$. After applying $\mathcal{L}$ to both sides of $ty''+(2+4t)y'+(4+4t)y=0$ and simplifying, I obtain  $$Y(s)=\frac{y(0)}{s+2}+C$$ and here $C=0$ because $\displaystyle\lim_{s\to\infty}Y(s)=0$. Then applying the inverse Laplace Transform to $Y(s)=\dfrac{y(0)}{s+2},$ we obtain the solution  $$y(t)=y(0)e^{-2t}.$$ But the general solution to the original differential equation should be $$y(t)=y(0)e^{-2t}+C_1e^{-2t}t^{-1}.$$ My Question : Why is the term $C_1e^{-2t}t^{-1}$ missing in my solution? Somehow I failed to find any mistake in my process, but the general solution is incomplete (although applying variation of parameters may make it complete, but my question is about the Laplace Transform method).","I am trying to solve the following ordinary differential equation, where $y$ is a function of $t$: $$ty''+(2+4t)y'+(4+4t)y=0.$$ My attempt was to use the Laplace Transform. The following formulae were assumed, where $\mathcal{L}(y)$ is the Laplace transform of a function $y$ defined by $$\mathcal{L}(y(t))=\int_0^{\infty}e^{-st}y(t)dt=Y(s):$$ $\mathcal{L}(ty'')=-s^2Y'(s)-2sY(s)+y(0)$; $\mathcal{L}(ty')=-sY'(s)-Y(s)$; $\mathcal{L}(ty)=-Y'(s)$; $\mathcal{L}(y')=sY(s)-y(0)$. After applying $\mathcal{L}$ to both sides of $ty''+(2+4t)y'+(4+4t)y=0$ and simplifying, I obtain  $$Y(s)=\frac{y(0)}{s+2}+C$$ and here $C=0$ because $\displaystyle\lim_{s\to\infty}Y(s)=0$. Then applying the inverse Laplace Transform to $Y(s)=\dfrac{y(0)}{s+2},$ we obtain the solution  $$y(t)=y(0)e^{-2t}.$$ But the general solution to the original differential equation should be $$y(t)=y(0)e^{-2t}+C_1e^{-2t}t^{-1}.$$ My Question : Why is the term $C_1e^{-2t}t^{-1}$ missing in my solution? Somehow I failed to find any mistake in my process, but the general solution is incomplete (although applying variation of parameters may make it complete, but my question is about the Laplace Transform method).",,"['ordinary-differential-equations', 'proof-verification', 'laplace-transform']"
96,Why I can not solve this differential equation?,Why I can not solve this differential equation?,,"I am asked to use the variation of parameters method to find the particular solution : $$t^2y'' - t(t+2)y' + (t+2)y = 6t^3 \\ t>0 \\ y_1(t) =t \\ y_2(t) = te^t$$ Where $y_i$ are the solutions of the homogenous one. I am using method of variation of parameters so I have the following formulas in my mind : $$u_1' = \frac{-y_2g}{W(y_1,y_2)} \ u_2' = \frac{y_1 g}{W(y_1,y_2)}$$ where $W(y_1,y_2)$ is the wronskian. Then clearly $u_1 = -6t + c_1 \ u_2 = -6e^{-t} + c_2$  So the general solution is : $$y= (-6t+c_1)(t) + (-6te^{-t}+c_2)(te^t) \\ = tc_1+te^{t}c_2-6t^2-6t$$ So I can take $-6t^2-6t$ as the particular solution. However answer sheet says that the answer is $-6t^2$ I kept checking my calculation mistakes on and on and on but couldn't find any? What am I doing wrong?","I am asked to use the variation of parameters method to find the particular solution : $$t^2y'' - t(t+2)y' + (t+2)y = 6t^3 \\ t>0 \\ y_1(t) =t \\ y_2(t) = te^t$$ Where $y_i$ are the solutions of the homogenous one. I am using method of variation of parameters so I have the following formulas in my mind : $$u_1' = \frac{-y_2g}{W(y_1,y_2)} \ u_2' = \frac{y_1 g}{W(y_1,y_2)}$$ where $W(y_1,y_2)$ is the wronskian. Then clearly $u_1 = -6t + c_1 \ u_2 = -6e^{-t} + c_2$  So the general solution is : $$y= (-6t+c_1)(t) + (-6te^{-t}+c_2)(te^t) \\ = tc_1+te^{t}c_2-6t^2-6t$$ So I can take $-6t^2-6t$ as the particular solution. However answer sheet says that the answer is $-6t^2$ I kept checking my calculation mistakes on and on and on but couldn't find any? What am I doing wrong?",,[]
97,Finding a weight function for Bessell Sturm-Liouville problem with some boundary conditions,Finding a weight function for Bessell Sturm-Liouville problem with some boundary conditions,,"I have a Bessel differential equation  $$r^2R'' + rR' + k^2r^2R=0$$ on $r = (R_1, R_2)$ with boundary conditions $$R'(r=R_1) = A$$ $$R'(r=R_2) = 0$$ where $A$ is some constant I can determine through a physical background of the problem. The solution is $$R(r) = J_0(kr) + B Y_0(kr)$$where $B=-\frac{J_1(kR_2)}{Y_1(kR2)}$. How would you go about finding a weight function, that would give orthogonality to the solutions of the problem? Also is there a way to find a weight function for any given boundary conditions?","I have a Bessel differential equation  $$r^2R'' + rR' + k^2r^2R=0$$ on $r = (R_1, R_2)$ with boundary conditions $$R'(r=R_1) = A$$ $$R'(r=R_2) = 0$$ where $A$ is some constant I can determine through a physical background of the problem. The solution is $$R(r) = J_0(kr) + B Y_0(kr)$$where $B=-\frac{J_1(kR_2)}{Y_1(kR2)}$. How would you go about finding a weight function, that would give orthogonality to the solutions of the problem? Also is there a way to find a weight function for any given boundary conditions?",,"['ordinary-differential-equations', 'mathematical-physics', 'bessel-functions', 'sturm-liouville']"
98,How does one find bifurcating solutions to the differential equation $\theta '' + \lambda \sin ( \theta ) = 0 $?,How does one find bifurcating solutions to the differential equation ?,\theta '' + \lambda \sin ( \theta ) = 0 ,"Consider the differential equation $$\theta '' + \lambda \sin ( \theta ) = 0, \quad \text{ for } 0 < x < 1 ,$$ in which $ \theta ' (0) = \theta ' (1) = 0$. For a homework question, I am asked to find the solutions to this equations that bifurcate from the equilibrium solution $\theta_{s} = 0$. This problem is related to a physical situation in which an initially straight rod is subjected to an axial load $\lambda$. The variable $\theta(x)$ is the angle the tangent to the rod makes with the horizontal at the spatial position $x$. I find it hard to get started with this exercise. I tried reasoning that, if $\theta_{s} = 0$, then surely $\theta_{ss} = 0$. So we are left with the equation $\lambda \sin( \theta ) = 0$. I could also expand the sine into a Taylor series, but that doesn't seem to be of much help. According to the solution manual, there should be a supercritical pitchfork when $\lambda_{n} = (n \pi )^{2} $ and $ \theta_{n} = 2 (2 \epsilon)^{1/2} \cos(n \pi x) / n \pi $. I don't have the faintest idea how they arrived at this solution. Do you know how I should approach this problem?","Consider the differential equation $$\theta '' + \lambda \sin ( \theta ) = 0, \quad \text{ for } 0 < x < 1 ,$$ in which $ \theta ' (0) = \theta ' (1) = 0$. For a homework question, I am asked to find the solutions to this equations that bifurcate from the equilibrium solution $\theta_{s} = 0$. This problem is related to a physical situation in which an initially straight rod is subjected to an axial load $\lambda$. The variable $\theta(x)$ is the angle the tangent to the rod makes with the horizontal at the spatial position $x$. I find it hard to get started with this exercise. I tried reasoning that, if $\theta_{s} = 0$, then surely $\theta_{ss} = 0$. So we are left with the equation $\lambda \sin( \theta ) = 0$. I could also expand the sine into a Taylor series, but that doesn't seem to be of much help. According to the solution manual, there should be a supercritical pitchfork when $\lambda_{n} = (n \pi )^{2} $ and $ \theta_{n} = 2 (2 \epsilon)^{1/2} \cos(n \pi x) / n \pi $. I don't have the faintest idea how they arrived at this solution. Do you know how I should approach this problem?",,"['ordinary-differential-equations', 'bifurcation']"
99,Differential equation - fundamental matrix,Differential equation - fundamental matrix,,"Prove using differentiation that $\phi(t)= \int^{t}_{t_0} X(t,\lambda) f(\lambda) \,d \lambda $, is the solution of the system below,  where $X(t,\lambda) =X(t)X^{-1}(\lambda)$ and $X$ is fundamental matrix of this system: $$ \begin{cases} x'=A(t)x + f(t)\\ x(t_0)=0 \end{cases}.$$ So I have that $$x'= \frac{d}{dt}\phi(t)= \frac{d}{dt}\int^{t}_{t_0} X(t,\lambda) f(\lambda) \,d \lambda =X(t,t) f(t)=f(t)$$ but then  from the first equation $f(t)=A(t)x+ f(t)$ thus $A(t)x =0$ which is not necessarily true? Where am I wrong?","Prove using differentiation that $\phi(t)= \int^{t}_{t_0} X(t,\lambda) f(\lambda) \,d \lambda $, is the solution of the system below,  where $X(t,\lambda) =X(t)X^{-1}(\lambda)$ and $X$ is fundamental matrix of this system: $$ \begin{cases} x'=A(t)x + f(t)\\ x(t_0)=0 \end{cases}.$$ So I have that $$x'= \frac{d}{dt}\phi(t)= \frac{d}{dt}\int^{t}_{t_0} X(t,\lambda) f(\lambda) \,d \lambda =X(t,t) f(t)=f(t)$$ but then  from the first equation $f(t)=A(t)x+ f(t)$ thus $A(t)x =0$ which is not necessarily true? Where am I wrong?",,"['matrices', 'ordinary-differential-equations']"
