,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Let $f: \mathbb{R} \to \mathbb{R}$ be continuous periodic function with period $T>0$,Let  be continuous periodic function with period,f: \mathbb{R} \to \mathbb{R} T>0,"Let $f: \mathbb{R} \to \mathbb{R}$ be a continuous periodic function with period $T>0$. Show that there exists an element $x \in \mathbb{R}$ for which $f(x)=f(x+T/2)$. This is what I came up with to prove the statement: For $f(x)=b$, with $b$ a constant, the proof is trivial (because in such a case $f(x)=f(x+T/2)$ for all $x \in \mathbb{R}$) To prove the proposition for all other $f(x)$, I have found that the following statements hold: $f(0)=f(T)$ (or actually, $f(k)=f(k+T)$ for all $k \in \mathbb{R}$) There exists at least one value $f(y)$, with $y \in [0,T]$ for which $f(y)\neq f(0)$ and $f(y) \neq f(T)$ All values of $f$ between $x=0$ and $x=T$ appear at least twice, except for the maximum and minimum, which only appear once (and they must exist). I'm guessing I have to make use of the intermediate value theorem to prove the result, but I'm unable to figure out how. Any help?","Let $f: \mathbb{R} \to \mathbb{R}$ be a continuous periodic function with period $T>0$. Show that there exists an element $x \in \mathbb{R}$ for which $f(x)=f(x+T/2)$. This is what I came up with to prove the statement: For $f(x)=b$, with $b$ a constant, the proof is trivial (because in such a case $f(x)=f(x+T/2)$ for all $x \in \mathbb{R}$) To prove the proposition for all other $f(x)$, I have found that the following statements hold: $f(0)=f(T)$ (or actually, $f(k)=f(k+T)$ for all $k \in \mathbb{R}$) There exists at least one value $f(y)$, with $y \in [0,T]$ for which $f(y)\neq f(0)$ and $f(y) \neq f(T)$ All values of $f$ between $x=0$ and $x=T$ appear at least twice, except for the maximum and minimum, which only appear once (and they must exist). I'm guessing I have to make use of the intermediate value theorem to prove the result, but I'm unable to figure out how. Any help?",,"['real-analysis', 'continuity', 'periodic-functions']"
1,Find all real polynomials $p(x)$ that satisfy $\sin( p(x) ) = p( \sin(x) )$,Find all real polynomials  that satisfy,p(x) \sin( p(x) ) = p( \sin(x) ),Find all real polynomials $p(x)$ that satisfy $\sin( p(x) ) = p( \sin(x) )$ . Is there an easy way to prove this?,Find all real polynomials that satisfy . Is there an easy way to prove this?,p(x) \sin( p(x) ) = p( \sin(x) ),"['real-analysis', 'polynomials', 'functional-equations']"
2,Show $\sum_{n=1}^{\infty}\frac{\sinh\pi}{\cosh(2n\pi)-\cosh\pi}=\frac1{\text{e}^{\pi}-1}$ and another,Show  and another,\sum_{n=1}^{\infty}\frac{\sinh\pi}{\cosh(2n\pi)-\cosh\pi}=\frac1{\text{e}^{\pi}-1},Show that : $$\sum_{n=1}^{\infty}\frac{\cosh(2nx)}{\cosh(4nx)-\cosh(2x)}=\frac1{4\sinh^2(x)}$$ $$\sum_{n=1}^{\infty}\frac{\sinh\pi}{\cosh(2n\pi)-\cosh\pi}=\frac1{\text{e}^{\pi}-1}$$,Show that : $$\sum_{n=1}^{\infty}\frac{\cosh(2nx)}{\cosh(4nx)-\cosh(2x)}=\frac1{4\sinh^2(x)}$$ $$\sum_{n=1}^{\infty}\frac{\sinh\pi}{\cosh(2n\pi)-\cosh\pi}=\frac1{\text{e}^{\pi}-1}$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'hyperbolic-functions']"
3,Existence of a continuous function with pre-image of each point  uncountable,Existence of a continuous function with pre-image of each point  uncountable,,"Does there exist a continuous function $f : [0, 1] → [0, 1]$ such that the pre-image $f^{−1}(y)$ of any point $y \in [0, 1]$ is uncountable?","Does there exist a continuous function $f : [0, 1] → [0, 1]$ such that the pre-image $f^{−1}(y)$ of any point $y \in [0, 1]$ is uncountable?",,['real-analysis']
4,Question about series rearrangement in Baby Rudin (theorem 3.54).,Question about series rearrangement in Baby Rudin (theorem 3.54).,,"I have trouble following the following theorem in Rudin: Let $\sum a_n$ be a series of real numbers which converges, but not absolutely. Suppose $$-\infty \leq \alpha \leq \beta \leq +\infty.$$ Then there exists a rearrangement $\sum a_n^\prime$ with partial sums $s_n^\prime$ such that $$\lim_{n\to\infty}\inf s_n^\prime = \alpha, \ \ \ \mbox{ and } \ \ \ \lim_{n\to\infty}\sup s_n^\prime = \beta. \tag{24}$$ Here's the proof: Let $$p_n = \frac{|a_n| + a_n}{2}, \ q_n = \frac{|a_n| - a_n}{2} \ (n = 1, 2, 3, \ldots). $$ Then $p_n - q_n = a_n$ , $p_n + q_n = |a_n|$ , $p_n \geq 0$ , $q_n \geq 0$ . The series $\sum p_n$ , $\sum q_n$ must both diverge. For if both were convergent, then $$\sum \left( p_n + q_n \right) = \sum |a_n|$$ would converge, contrary to hypothesis. Since $$ \sum_{n=1}^N a_n = \sum_{n=1}^N \left( p_n - q_n \right) = \sum_{n=1}^N p_n - \sum_{n=1}^N q_n,$$ divergence of $\sum p_n$ and convergence of $\sum q_n$ (or vice versa) implies divergence of $\sum a_n$ , again contrary to hypothesis. Now let $P_1, P_2, P_3, \ldots$ denote the non-negative terms of $\sum a_n$ , in the order in which they occur, and let $Q_1, Q_2, Q_3, \ldots$ be the absolute values of the negative terms of $\sum a_n$ , also in their original order. The series $\sum P_n$ , $\sum Q_n$ differ from $\sum p_n$ , $\sum q_n$ only by zero terms, and are therefore divergent. We shall construct sequences $\{m_n \}$ , $\{k_n\}$ , such that the series $$ P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} + \cdots \tag{25}, $$ which clearly is a rearrangement of $\sum a_n$ , satisfies (24). Choose real-valued sequences $\{ \alpha_n \}$ , $\{ \beta_n \}$ such that $\alpha_n \rightarrow \alpha$ , $\beta_n \rightarrow \beta$ , $\alpha_n < \beta_n$ , $\beta_1 > 0$ . Let $m_1$ , $k_1$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} > \beta_1,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} < \alpha_1;$$ let $m_2$ , $k_2$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} > \beta_2,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} < \alpha_2;$$ and continue in this way. This is possible since $\sum P_n$ , $\sum Q_n$ diverge. If $x_n$ , $y_n$ denote the partial sums of (25) whose last terms are $P_{m_n}$ , $-Q_{k_n}$ , then $$ | x_n - \beta_n | \leq P_{m_n}, \ \ \ |y_n - \alpha_n | \leq Q_{k_n}. $$ Since $P_n \rightarrow 0$ , $Q_n \rightarrow 0$ as $n \rightarrow \infty$ , we see that $x_n \rightarrow \beta$ , $y_n \rightarrow \alpha$ . Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25) . I don't understand the last two lines of the proof (put in bold). I'm aware that this question was already asked on this forum, but I didn't understand the answers provided in this question, so that's why I'm writing my own question. Thanks in advance.","I have trouble following the following theorem in Rudin: Let be a series of real numbers which converges, but not absolutely. Suppose Then there exists a rearrangement with partial sums such that Here's the proof: Let Then , , , . The series , must both diverge. For if both were convergent, then would converge, contrary to hypothesis. Since divergence of and convergence of (or vice versa) implies divergence of , again contrary to hypothesis. Now let denote the non-negative terms of , in the order in which they occur, and let be the absolute values of the negative terms of , also in their original order. The series , differ from , only by zero terms, and are therefore divergent. We shall construct sequences , , such that the series which clearly is a rearrangement of , satisfies (24). Choose real-valued sequences , such that , , , . Let , be the smallest integers such that let , be the smallest integers such that and continue in this way. This is possible since , diverge. If , denote the partial sums of (25) whose last terms are , , then Since , as , we see that , . Finally, it is clear that no number less than or greater than can be a subsequential limit of the partial sums of (25) . I don't understand the last two lines of the proof (put in bold). I'm aware that this question was already asked on this forum, but I didn't understand the answers provided in this question, so that's why I'm writing my own question. Thanks in advance.","\sum a_n -\infty \leq \alpha \leq \beta \leq +\infty. \sum a_n^\prime s_n^\prime \lim_{n\to\infty}\inf s_n^\prime = \alpha, \ \ \ \mbox{ and } \ \ \ \lim_{n\to\infty}\sup s_n^\prime = \beta. \tag{24} p_n = \frac{|a_n| + a_n}{2}, \ q_n = \frac{|a_n| - a_n}{2} \ (n = 1, 2, 3, \ldots).  p_n - q_n = a_n p_n + q_n = |a_n| p_n \geq 0 q_n \geq 0 \sum p_n \sum q_n \sum \left( p_n + q_n \right) = \sum |a_n|  \sum_{n=1}^N a_n = \sum_{n=1}^N \left( p_n - q_n \right) = \sum_{n=1}^N p_n - \sum_{n=1}^N q_n, \sum p_n \sum q_n \sum a_n P_1, P_2, P_3, \ldots \sum a_n Q_1, Q_2, Q_3, \ldots \sum a_n \sum P_n \sum Q_n \sum p_n \sum q_n \{m_n \} \{k_n\}  P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} + \cdots \tag{25},  \sum a_n \{ \alpha_n \} \{ \beta_n \} \alpha_n \rightarrow \alpha \beta_n \rightarrow \beta \alpha_n < \beta_n \beta_1 > 0 m_1 k_1 P_1 + \cdots + P_{m_1} > \beta_1, P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} < \alpha_1; m_2 k_2 P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} > \beta_2, P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} < \alpha_2; \sum P_n \sum Q_n x_n y_n P_{m_n} -Q_{k_n}  | x_n - \beta_n | \leq P_{m_n}, \ \ \ |y_n - \alpha_n | \leq Q_{k_n}.  P_n \rightarrow 0 Q_n \rightarrow 0 n \rightarrow \infty x_n \rightarrow \beta y_n \rightarrow \alpha \alpha \beta",['real-analysis']
5,Is every norm increasing?,Is every norm increasing?,,"Let $N$ be any norm on $\Bbb R^n$. Is it true that if $0 \leq a_i \leq b_i$ for $1 \leq i \leq n$, then $N(a_1, \ldots, a_n) \leq N(b_1, \ldots, b_n)$ ? Clearly this is true for the norms $\| \cdot \|_p$ where $1 \leq p \leq \infty$, being defined as a composition of increasing functions. Any two norms are equivalent, but I don't see how my property is preserved under equivalence.","Let $N$ be any norm on $\Bbb R^n$. Is it true that if $0 \leq a_i \leq b_i$ for $1 \leq i \leq n$, then $N(a_1, \ldots, a_n) \leq N(b_1, \ldots, b_n)$ ? Clearly this is true for the norms $\| \cdot \|_p$ where $1 \leq p \leq \infty$, being defined as a composition of increasing functions. Any two norms are equivalent, but I don't see how my property is preserved under equivalence.",,"['real-analysis', 'normed-spaces']"
6,Why $\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{iw(t-x)}dw$ is the Dirac delta function?,Why  is the Dirac delta function?,\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{iw(t-x)}dw,"I'm reading a book that says: $$f(x) = \int_{-\infty}^{\infty}f(t)\left\{\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{iw(t-x)}dw\right\}dt$$ and then says that the term in curly brackets can be seen as the Dirac Delta Function. As I understand, the Dirac Delta Function should be $0$ when $t\neq x$, right? $$\delta(t-x) = \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{iw(t-x)}dw$$ why this thing is $0$ when $t\neq x$?","I'm reading a book that says: $$f(x) = \int_{-\infty}^{\infty}f(t)\left\{\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{iw(t-x)}dw\right\}dt$$ and then says that the term in curly brackets can be seen as the Dirac Delta Function. As I understand, the Dirac Delta Function should be $0$ when $t\neq x$, right? $$\delta(t-x) = \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{iw(t-x)}dw$$ why this thing is $0$ when $t\neq x$?",,"['real-analysis', 'integration', 'fourier-analysis', 'laplace-transform']"
7,Show that $e^{f(x)}$ is convex.,Show that  is convex.,e^{f(x)},"Let $f : (0,\infty) \to \mathbb{R}$ be a convex function. Prove that $e^{f(x)}$ is a convex function on $(0,\infty)$ . My original idea was to try and show that the second derivative is positive, but this will not work since $f(x)$ need not be differentiable. Here's my second attempt: By definition, since $f$ is convex, we have that $f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)$ for any $\lambda \in (0,1)$ and $x,y \in (0,\infty)$ . Then, by applying the exponential to both sides we have that $e^{f(\lambda x+(1-\lambda)y))}\leq e^{\lambda f(x)+(1-\lambda)f(y)}$ . Applying rules of exponents, $e^{f(\lambda x+(1-\lambda)y))} \leq e^{\lambda f(x)}e^{(1-\lambda)f(y)}$ . From here, I want to bring the $\lambda$ and $1-\lambda$ terms down in front of the exponential, but I am stuck as to how to do this.","Let be a convex function. Prove that is a convex function on . My original idea was to try and show that the second derivative is positive, but this will not work since need not be differentiable. Here's my second attempt: By definition, since is convex, we have that for any and . Then, by applying the exponential to both sides we have that . Applying rules of exponents, . From here, I want to bring the and terms down in front of the exponential, but I am stuck as to how to do this.","f : (0,\infty) \to \mathbb{R} e^{f(x)} (0,\infty) f(x) f f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y) \lambda \in (0,1) x,y \in (0,\infty) e^{f(\lambda x+(1-\lambda)y))}\leq e^{\lambda f(x)+(1-\lambda)f(y)} e^{f(\lambda x+(1-\lambda)y))} \leq e^{\lambda f(x)}e^{(1-\lambda)f(y)} \lambda 1-\lambda","['real-analysis', 'convex-analysis']"
8,Elementary Proof of Ramanujan Master Theorem,Elementary Proof of Ramanujan Master Theorem,,"I was searching for an elementary proof of the Ramanujan Master Theorem and I found a page from Ramanujan's Notebook on wikipedia which contained the proof. I think that it has some gaps, so can anyone please explain the proof. Any help will be appreciated. Thanks.","I was searching for an elementary proof of the Ramanujan Master Theorem and I found a page from Ramanujan's Notebook on wikipedia which contained the proof. I think that it has some gaps, so can anyone please explain the proof. Any help will be appreciated. Thanks.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'taylor-expansion']"
9,$f(x)=x^2$ is not Lipschitz?,is not Lipschitz?,f(x)=x^2,"Consider the function $\ f:\mathbb{R}\to\mathbb{R}$ defined by $\ f(x)=x^2$ for all $x\in \mathbb{R}$. I think that $f$ is NOT Lipschitz (meaning, there exists no constant $c\in[0,\infty)$ such that $|f(x)-f(y)|\leq c |x-y|$ for all $x,y\in\mathbb{R}$). Here is my proof: Suppose that it IS Lipschitz. There there exists some constant $c\in[0,\infty)$ such that $|f(x)-f(y)|\leq c |x-y|$ for all $x,y\in\mathbb{R}$. Then: $\implies\ \ \ \ |x^2-y^2|\leq c |x-y|$ for all $x,y\in\mathbb{R}$ $\implies\ \ \ \ |x-y| \cdot |x+y|\leq c |x-y|$ for all $x,y\in\mathbb{R}$ $\implies\ \ \ \ |x+y|\leq c$ for all $x,y\in\mathbb{R}$ But there is no real number $c$ with the property that $|x+y|\leq c$ for all $x,y\in\mathbb{R}$. This is a contradiction. $f$ is not Lipschitz. Is this proof correct?","Consider the function $\ f:\mathbb{R}\to\mathbb{R}$ defined by $\ f(x)=x^2$ for all $x\in \mathbb{R}$. I think that $f$ is NOT Lipschitz (meaning, there exists no constant $c\in[0,\infty)$ such that $|f(x)-f(y)|\leq c |x-y|$ for all $x,y\in\mathbb{R}$). Here is my proof: Suppose that it IS Lipschitz. There there exists some constant $c\in[0,\infty)$ such that $|f(x)-f(y)|\leq c |x-y|$ for all $x,y\in\mathbb{R}$. Then: $\implies\ \ \ \ |x^2-y^2|\leq c |x-y|$ for all $x,y\in\mathbb{R}$ $\implies\ \ \ \ |x-y| \cdot |x+y|\leq c |x-y|$ for all $x,y\in\mathbb{R}$ $\implies\ \ \ \ |x+y|\leq c$ for all $x,y\in\mathbb{R}$ But there is no real number $c$ with the property that $|x+y|\leq c$ for all $x,y\in\mathbb{R}$. This is a contradiction. $f$ is not Lipschitz. Is this proof correct?",,"['calculus', 'real-analysis', 'uniform-continuity', 'lipschitz-functions']"
10,Proving a sequence converges when combinations of consecutive terms converge,Proving a sequence converges when combinations of consecutive terms converge,,"Problem: Let $\{x_n\}$ be a sequence of real numbers such that $$\lim_{n\to\infty} 2x_{n+1}-x_n=L \in \mathbf{R}.$$ Prove that $x_n \to L$ as $n\to\infty$. I can see that if $\{x_n\}$ converges to a limit, then that limit must be $L$. I am having trouble proving the convergence of the sequence. I first tried seeing if I could make $|x_n-L|$ small by playing around with the triangle inequalities, but that didn't pan out. I also tried to prove that $\{x_n\}$ is Cauchy or monotone and bounded, but I couldn't prove the sequence was Cauchy, and I found a counter-example for monotone and bounded: $x_n=(-1)^n/n$. Any help is appreciated.","Problem: Let $\{x_n\}$ be a sequence of real numbers such that $$\lim_{n\to\infty} 2x_{n+1}-x_n=L \in \mathbf{R}.$$ Prove that $x_n \to L$ as $n\to\infty$. I can see that if $\{x_n\}$ converges to a limit, then that limit must be $L$. I am having trouble proving the convergence of the sequence. I first tried seeing if I could make $|x_n-L|$ small by playing around with the triangle inequalities, but that didn't pan out. I also tried to prove that $\{x_n\}$ is Cauchy or monotone and bounded, but I couldn't prove the sequence was Cauchy, and I found a counter-example for monotone and bounded: $x_n=(-1)^n/n$. Any help is appreciated.",,"['real-analysis', 'sequences-and-series']"
11,What are some rigorous definitions for sine and cosine?,What are some rigorous definitions for sine and cosine?,,"Here are some of my ideas: 1. Addition Formula: $\sin{x}$ and $\cos{x}$ are the unique functions satisfying: $\sin(x + y) = \sin x \cos y + \cos x \sin y $ $\cos(x + y) = \cos x \cos y - \sin x \sin y$ $\sin 0 = 0\quad$ and $\quad\displaystyle{\lim_{x \rightarrow 0} \frac{\sin x }{x} = 1}$ $\cos 0 = 1\quad$ and $\quad\displaystyle{\lim_{x \rightarrow 0} \frac{1-\cos x}{x} = 0}$ 2. Taylor Series: $\displaystyle{\sin x = \sum_{n = 0}^{\infty} \frac{(-1)^n}{(2n+1)!}\;x^{2n+1}}$ $\displaystyle{\cos x = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!}\;x^{2n}}$ 3. Differential Equations: $\sin(x)$ and $\cos(x)$ are the unique solutions to $y'' = -y$ , where $\sin(0) = \cos^\prime(0) = 0$ and $\sin^\prime(0) = \cos(0) = 1$ . 4. Inverse Formula: We have: $$\begin{align} \arcsin x &= \phantom{\frac{\pi}{2} + } \int_0^x \frac{1}{\sqrt{1 - t^2}}\, dt \\[6pt] \arccos x &= \frac{\pi}{2} - \int_0^x \frac{1}{\sqrt{1 - t^2}}\, dt \end{align}$$ Then $\sin x$ is the inverse of $\arcsin x$ , extended appropriately to the real line, and $\cos x$ is similar. Question: Are there any others that you like? In particular, are there any good rigorous ones coming from the original geometric definition?","Here are some of my ideas: 1. Addition Formula: and are the unique functions satisfying: and and 2. Taylor Series: 3. Differential Equations: and are the unique solutions to , where and . 4. Inverse Formula: We have: Then is the inverse of , extended appropriately to the real line, and is similar. Question: Are there any others that you like? In particular, are there any good rigorous ones coming from the original geometric definition?","\sin{x} \cos{x} \sin(x + y) = \sin x \cos y + \cos x \sin y  \cos(x + y) = \cos x \cos y - \sin x \sin y \sin 0 = 0\quad \quad\displaystyle{\lim_{x \rightarrow 0} \frac{\sin x }{x} = 1} \cos 0 = 1\quad \quad\displaystyle{\lim_{x \rightarrow 0} \frac{1-\cos x}{x} = 0} \displaystyle{\sin x = \sum_{n = 0}^{\infty} \frac{(-1)^n}{(2n+1)!}\;x^{2n+1}} \displaystyle{\cos x = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!}\;x^{2n}} \sin(x) \cos(x) y'' = -y \sin(0) = \cos^\prime(0) = 0 \sin^\prime(0) = \cos(0) = 1 \begin{align}
\arcsin x &= \phantom{\frac{\pi}{2} + } \int_0^x \frac{1}{\sqrt{1 - t^2}}\, dt \\[6pt]
\arccos x &= \frac{\pi}{2} - \int_0^x \frac{1}{\sqrt{1 - t^2}}\, dt
\end{align} \sin x \arcsin x \cos x","['calculus', 'real-analysis', 'trigonometry']"
12,Evaluate $\int_{0}^{\large\frac{\pi}{4}} \ln {(\sin x)}\cdot\ln {(\cos x)} \left(\frac{\ln{(\sin x)}}{\cot x}+\frac{\ln {(\cos x)}}{\tan x}\right)dx$,Evaluate,\int_{0}^{\large\frac{\pi}{4}} \ln {(\sin x)}\cdot\ln {(\cos x)} \left(\frac{\ln{(\sin x)}}{\cot x}+\frac{\ln {(\cos x)}}{\tan x}\right)dx,"How do I find the value of this integral? $$I=\int_{0}^{\Large\frac{\pi}{4}} \ln {(\sin x)}\cdot\ln {(\cos x)} \left(\dfrac{\ln{(\sin x)}}{\cot x}+\dfrac{\ln {(\cos x)}}{\tan x}\right)dx$$ I  tried substituting $t=\ln {(\sin x)}\cdot \ln {(\cos x)}$ and $t=\dfrac{\ln {(\sin x)}}{\ln {(\cos x)}}$, but it isn't working.","How do I find the value of this integral? $$I=\int_{0}^{\Large\frac{\pi}{4}} \ln {(\sin x)}\cdot\ln {(\cos x)} \left(\dfrac{\ln{(\sin x)}}{\cot x}+\dfrac{\ln {(\cos x)}}{\tan x}\right)dx$$ I  tried substituting $t=\ln {(\sin x)}\cdot \ln {(\cos x)}$ and $t=\dfrac{\ln {(\sin x)}}{\ln {(\cos x)}}$, but it isn't working.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
13,"How to arrange $e^3,3^e,e^{\pi},\pi^e,3^{\pi},\pi^3$ in the increasing order?",How to arrange  in the increasing order?,"e^3,3^e,e^{\pi},\pi^e,3^{\pi},\pi^3","For these six numbers, $e^3,3^e,e^{\pi},\pi^e,3^{\pi},\pi^3$, how to arrange them in the increasing order? This problem is taken from the today test: National Higher Education Entrance Examination. I think we can consider $$f(x)=\dfrac{\ln{x}}{x}$$ or perhaps other methods, so I am looking forward to seeing other methods to solve this problem. Thank you.","For these six numbers, $e^3,3^e,e^{\pi},\pi^e,3^{\pi},\pi^3$, how to arrange them in the increasing order? This problem is taken from the today test: National Higher Education Entrance Examination. I think we can consider $$f(x)=\dfrac{\ln{x}}{x}$$ or perhaps other methods, so I am looking forward to seeing other methods to solve this problem. Thank you.",,"['real-analysis', 'algebra-precalculus', 'inequality']"
14,"How prove that there exists $\xi\in(a,b)$ with $f'(\xi)=\frac{f(\xi)-f(a)}{b-a}$",How prove that there exists  with,"\xi\in(a,b) f'(\xi)=\frac{f(\xi)-f(a)}{b-a}","Let $f(x)$ be continuous on $[a,b]$, differentiable on $(a,b)$, and with some $c\in(a,b)$ such that $f'(c)=0$. Show: There exists $\xi\in(a,b)$ such that   $$ f'(\xi)=\dfrac{f(\xi)-f(a)}{b-a} $$ My idea: I know this: Let $$F(x)=e^{-\dfrac{x}{b-a}}[f(x)-f(a)]$$ $$\Longrightarrow F'(x)=-\dfrac{1}{b-a}e^{-\dfrac{x}{b-a}}[f(x)-f(a)]+e^{-\dfrac{x}{b-a}}f'(x)$$ $$\Longrightarrow F'(c)=e^{-\dfrac{c}{b-a}}[f'(c)-\dfrac{f(c)-f(a)}{b-a}]=-\dfrac{F(c)}{b-a}$$ If $f'(x)$ is continuous, I can prove this problem, But this problem condition can't have $f'(x)$ have continuous, so I can't continue","Let $f(x)$ be continuous on $[a,b]$, differentiable on $(a,b)$, and with some $c\in(a,b)$ such that $f'(c)=0$. Show: There exists $\xi\in(a,b)$ such that   $$ f'(\xi)=\dfrac{f(\xi)-f(a)}{b-a} $$ My idea: I know this: Let $$F(x)=e^{-\dfrac{x}{b-a}}[f(x)-f(a)]$$ $$\Longrightarrow F'(x)=-\dfrac{1}{b-a}e^{-\dfrac{x}{b-a}}[f(x)-f(a)]+e^{-\dfrac{x}{b-a}}f'(x)$$ $$\Longrightarrow F'(c)=e^{-\dfrac{c}{b-a}}[f'(c)-\dfrac{f(c)-f(a)}{b-a}]=-\dfrac{F(c)}{b-a}$$ If $f'(x)$ is continuous, I can prove this problem, But this problem condition can't have $f'(x)$ have continuous, so I can't continue",,"['real-analysis', 'analysis', 'derivatives']"
15,"Prove that if f is a continuous strictly monotone function defined on an interval, then its inverse is also a continuous function.","Prove that if f is a continuous strictly monotone function defined on an interval, then its inverse is also a continuous function.",,"There is a theorem on continuous function that goes as follow: If f is a continuous strictly monotone function defined on an interval, then its inverse is also a continuous function. I have quite an ugly proof on this theorem. My textbook proof doesn't look good either. So I am just wondering if someone can provide me with a more elegant proof. Thanks. On top of that, I am just wondering why must the function be strictly monotone? Is it to ensure that it is one-to-one so that the inverse exists? Thanks.","There is a theorem on continuous function that goes as follow: If f is a continuous strictly monotone function defined on an interval, then its inverse is also a continuous function. I have quite an ugly proof on this theorem. My textbook proof doesn't look good either. So I am just wondering if someone can provide me with a more elegant proof. Thanks. On top of that, I am just wondering why must the function be strictly monotone? Is it to ensure that it is one-to-one so that the inverse exists? Thanks.",,"['real-analysis', 'continuity']"
16,Prove $f $ is identically zero if $f(0)=0$ and $|f'(x)|\le|f(x)|$,Prove  is identically zero if  and,f  f(0)=0 |f'(x)|\le|f(x)|,"$f:\Bbb R \to\Bbb R $ is differentiable, $f(0)=0$ and $|f'(x)|\le|f(x)|$ for all $x$ then prove $f$ is identically zero. I tried to use mean value theorem and end up in $|f(x)|\le |x||f(c)|$ for some constant $c$ which depends on $x$ . Can you help me little bit...","is differentiable, and for all then prove is identically zero. I tried to use mean value theorem and end up in for some constant which depends on . Can you help me little bit...",f:\Bbb R \to\Bbb R  f(0)=0 |f'(x)|\le|f(x)| x f |f(x)|\le |x||f(c)| c x,"['real-analysis', 'derivatives']"
17,Define second derivative ($f''$) without using first derivative ($f'$),Define second derivative () without using first derivative (),f'' f',"The question I'd like to ask is this: If $f''(0)$ exists, does $f'$ exist in a neighborhood of $0$? Of course, under the standard definition of $f''(0)$, we have already assumed that $f'$ exists in a neighborhood of $0$.  So instead: Question: Is there a standard way to define $f''(0)$ as a limit expression that does not include $f'$ in it, and if so, can we deduce from the fact that $f''(0)$ exists that $f'$ exists in a neighborhood of $0$? Details If I know what $f'(0)$ is, I can make $f''(0)$ be the constant (if it exists) such that $$ \lim\limits_{h \to 0} \frac{f(h) - \left[ f(0) + f'(0) h + \frac{1}{2} f''(0) h^2\right]}{h^2} = 0. $$ i.e., the Taylor polynomial approximates $f$ to second order. Then, I could just plug in for $f'(0)$ the expression $\frac{f(h) - f(0)}{h}$.  But this doesn't work; everything cancels out.  Is there a different standard way to define $f''$ without using $f'$? I should probably put some more work into answering this myself, but first I wanted to see if this is a standard or well-known question.","The question I'd like to ask is this: If $f''(0)$ exists, does $f'$ exist in a neighborhood of $0$? Of course, under the standard definition of $f''(0)$, we have already assumed that $f'$ exists in a neighborhood of $0$.  So instead: Question: Is there a standard way to define $f''(0)$ as a limit expression that does not include $f'$ in it, and if so, can we deduce from the fact that $f''(0)$ exists that $f'$ exists in a neighborhood of $0$? Details If I know what $f'(0)$ is, I can make $f''(0)$ be the constant (if it exists) such that $$ \lim\limits_{h \to 0} \frac{f(h) - \left[ f(0) + f'(0) h + \frac{1}{2} f''(0) h^2\right]}{h^2} = 0. $$ i.e., the Taylor polynomial approximates $f$ to second order. Then, I could just plug in for $f'(0)$ the expression $\frac{f(h) - f(0)}{h}$.  But this doesn't work; everything cancels out.  Is there a different standard way to define $f''$ without using $f'$? I should probably put some more work into answering this myself, but first I wanted to see if this is a standard or well-known question.",,"['real-analysis', 'derivatives']"
18,Sequence of simple functions nonnegative that converge to measurable function $f$,Sequence of simple functions nonnegative that converge to measurable function,f,"Suppose $f\geq 0$ is measurable. We want to find a sequence of $s_n$ of nonnegative simple functions such that $s_n \to_{pointwise} f$. My book says the we should consided the sequence: $$ s_n = \sum_{k=0}^{2^{2n}} \frac{k}{2^n} \chi_{f^{-1}( [ \frac{k}{2^n}, \frac{k +1}{2^n} ] ) }$$ But I don't know how to show this sequence converges pointwise to $f$. I mean it is obviously non-negative. Do I have to show that $s_n$ is monotone and bounded ? and hence convergent to $f$? maybe the notation of the sequence is confusing. Any help would be greatly appreciated.","Suppose $f\geq 0$ is measurable. We want to find a sequence of $s_n$ of nonnegative simple functions such that $s_n \to_{pointwise} f$. My book says the we should consided the sequence: $$ s_n = \sum_{k=0}^{2^{2n}} \frac{k}{2^n} \chi_{f^{-1}( [ \frac{k}{2^n}, \frac{k +1}{2^n} ] ) }$$ But I don't know how to show this sequence converges pointwise to $f$. I mean it is obviously non-negative. Do I have to show that $s_n$ is monotone and bounded ? and hence convergent to $f$? maybe the notation of the sequence is confusing. Any help would be greatly appreciated.",,"['real-analysis', 'measure-theory']"
19,The limit of the product $\prod_{i=1}^n\frac{1 - (2i + 1)a/(2n)}{1 - ia/n}$ as $n\to\infty$,The limit of the product  as,\prod_{i=1}^n\frac{1 - (2i + 1)a/(2n)}{1 - ia/n} n\to\infty,"Given that $0 < a < 1$ , what is the value of $$ P = \lim_{n\to \infty} \prod_{i=1}^n \frac{1 - (2i + 1)a/(2n)}{1 - ia/n} $$ Thus, $P_n$ , the $n$ th term, is $$ P_n = \frac{1-3a/2n}{1-a/n}\cdot \frac{1-5a/2n}{1-2a/n}\cdots \frac{1-(2n+1)a/2n}{1-a} $$ Motivation The product describes a real-live physical problem. Put simply, if you have a river or stream in which you're sampling at an upstream site A and a downstream site B and you analyze the chemistry of the stream at A and B, if the concentration has changed, it is because of ground water which has seeped in between A and B. By making some simplifying assumptions, the concentration and amount of ground water which has entered are calculated using the evaluation of this infinite product.","Given that , what is the value of Thus, , the th term, is Motivation The product describes a real-live physical problem. Put simply, if you have a river or stream in which you're sampling at an upstream site A and a downstream site B and you analyze the chemistry of the stream at A and B, if the concentration has changed, it is because of ground water which has seeped in between A and B. By making some simplifying assumptions, the concentration and amount of ground water which has entered are calculated using the evaluation of this infinite product.",0 < a < 1  P = \lim_{n\to \infty} \prod_{i=1}^n \frac{1 - (2i + 1)a/(2n)}{1 - ia/n}  P_n n  P_n = \frac{1-3a/2n}{1-a/n}\cdot \frac{1-5a/2n}{1-2a/n}\cdots \frac{1-(2n+1)a/2n}{1-a} ,"['real-analysis', 'sequences-and-series', 'limits', 'infinite-product']"
20,Baby Rudin theorem 2.41,Baby Rudin theorem 2.41,,"I am reading Baby Rudin chapter 2 and came across some question on Theorem 2.41 . When Rudin tries to prove that every infinite subset of $E$ has a limit points in $E$ implies $E$ is closed, he first supposes $E$ is not closed. Then there exists $x_0$ in $\mathbb{R}^k$ which is a limit point of $E$ but not a point in $E$. Then he constructs the set $S$. But later, he introduces $y$ and applies triangular inequality. I do not understand what $y$ is, and also, how he gets the last inequality of greater than or equal to $\frac12|x_0-y|$? For reference, here is the proof given by Rudin: 2.41 $\ \ $ Theorem $\ \ $ If a set $E$ in ${\bf R}^k$ has one of the following three properties, then it has the other two: $\quad(a)\ \ $ $E$ is closed and bounded. $\quad(b)\ \ $ $E$ is compact . $\quad(c)\ \ $ Every infinite subset of $E$ has a limit point in $E$. Proof $\ \ $ If $(a)$ holds, then $E\subset I$ for some $k$-cell $I$, and $(b)$ follows from Theorems $2.40$ and $2.35$. Theorem $2.37$ shows that $(b)$ implies $(c)$. It remains to be shown that $(c)$ implies $(a)$. $\qquad$ If $E$ is not bounded, then $E$ contains points ${\bf x}_n$ with $$|{\bf x}_n|>n\qquad(n=1,2,3,...).$$ The set $S$ consisting of three points ${\bf x}_n$ is infinite and clearly has no limit point in ${\bf R}^k$, hence has none in $E$. Thus $(c)$ implies that $E$ is bounded. $\qquad$ If $E$ is not closed, then there is a point ${\bf x}_0\in {\bf R}^k$ which is a limit point of $E$ but not a point of $E$. For $n=1,2,3,...,$ there are points ${\bf x}_n\in E$ such that $|{\bf x}_n-{\bf x}_0|<1/n$. Let $S$ be the set of these points ${\bf x}_n$ Then $S$ is infinite (otherwise $|{\bf x}_n-{\bf x}_0|$ would have a constant positive value, for infinitely many $n$), $S$ has ${\bf x}_0$ as a limit point, and $S$ has no other limit point in ${\bf R}^k$. For if $\mathbf y\in{\bf R}^k$, ${\bf y}\neq{\bf x}_0$, then $$\begin{align}|{\bf x}_n-{\bf y}| & \geq |{\bf x}_0-{\bf y}| - |{\bf x}_n-{\bf x}_0| \\ & \geq |{\bf x}_0-{\bf y}|-\dfrac1n\geq \dfrac12|{\bf x}_0-{\bf y}|\end{align}$$ for all but finitely many $n$; this shows that $\bf y$ is not a limit point of $S$ (Theorem $2.20$). $\qquad$ Thus $S$ has no limit point in $E$; hence $E$ must be closed if $(c)$ holds. $\qquad$ We should remark, at this point, that $(b)$ and $(c)$ are equivalent in any metric space (Exercise $26$) but that $(a)$ does not, in general, imply $(b)$ and $(c)$. Examples are furnished by Exercise $16$ and by the space ${\scr L}^2$, which is discussed in Chap. 11.","I am reading Baby Rudin chapter 2 and came across some question on Theorem 2.41 . When Rudin tries to prove that every infinite subset of $E$ has a limit points in $E$ implies $E$ is closed, he first supposes $E$ is not closed. Then there exists $x_0$ in $\mathbb{R}^k$ which is a limit point of $E$ but not a point in $E$. Then he constructs the set $S$. But later, he introduces $y$ and applies triangular inequality. I do not understand what $y$ is, and also, how he gets the last inequality of greater than or equal to $\frac12|x_0-y|$? For reference, here is the proof given by Rudin: 2.41 $\ \ $ Theorem $\ \ $ If a set $E$ in ${\bf R}^k$ has one of the following three properties, then it has the other two: $\quad(a)\ \ $ $E$ is closed and bounded. $\quad(b)\ \ $ $E$ is compact . $\quad(c)\ \ $ Every infinite subset of $E$ has a limit point in $E$. Proof $\ \ $ If $(a)$ holds, then $E\subset I$ for some $k$-cell $I$, and $(b)$ follows from Theorems $2.40$ and $2.35$. Theorem $2.37$ shows that $(b)$ implies $(c)$. It remains to be shown that $(c)$ implies $(a)$. $\qquad$ If $E$ is not bounded, then $E$ contains points ${\bf x}_n$ with $$|{\bf x}_n|>n\qquad(n=1,2,3,...).$$ The set $S$ consisting of three points ${\bf x}_n$ is infinite and clearly has no limit point in ${\bf R}^k$, hence has none in $E$. Thus $(c)$ implies that $E$ is bounded. $\qquad$ If $E$ is not closed, then there is a point ${\bf x}_0\in {\bf R}^k$ which is a limit point of $E$ but not a point of $E$. For $n=1,2,3,...,$ there are points ${\bf x}_n\in E$ such that $|{\bf x}_n-{\bf x}_0|<1/n$. Let $S$ be the set of these points ${\bf x}_n$ Then $S$ is infinite (otherwise $|{\bf x}_n-{\bf x}_0|$ would have a constant positive value, for infinitely many $n$), $S$ has ${\bf x}_0$ as a limit point, and $S$ has no other limit point in ${\bf R}^k$. For if $\mathbf y\in{\bf R}^k$, ${\bf y}\neq{\bf x}_0$, then $$\begin{align}|{\bf x}_n-{\bf y}| & \geq |{\bf x}_0-{\bf y}| - |{\bf x}_n-{\bf x}_0| \\ & \geq |{\bf x}_0-{\bf y}|-\dfrac1n\geq \dfrac12|{\bf x}_0-{\bf y}|\end{align}$$ for all but finitely many $n$; this shows that $\bf y$ is not a limit point of $S$ (Theorem $2.20$). $\qquad$ Thus $S$ has no limit point in $E$; hence $E$ must be closed if $(c)$ holds. $\qquad$ We should remark, at this point, that $(b)$ and $(c)$ are equivalent in any metric space (Exercise $26$) but that $(a)$ does not, in general, imply $(b)$ and $(c)$. Examples are furnished by Exercise $16$ and by the space ${\scr L}^2$, which is discussed in Chap. 11.",,"['real-analysis', 'proof-explanation']"
21,Can we make $\mathbb{R}^{2}$ an ordered field?,Can we make  an ordered field?,\mathbb{R}^{2},"We know that if we give $\mathbb{R}^{2}$ the complex field structure, we cannot make it an ordered field. Is there any field structure that we can put on $\mathbb{R}^{2}$ that makes this ordered field? I don't think there is, but I don't know how to start my argument.","We know that if we give $\mathbb{R}^{2}$ the complex field structure, we cannot make it an ordered field. Is there any field structure that we can put on $\mathbb{R}^{2}$ that makes this ordered field? I don't think there is, but I don't know how to start my argument.",,"['real-analysis', 'ordered-fields']"
22,Product of two Lebesgue integrable functions,Product of two Lebesgue integrable functions,,"In general, I know that it is not necessarily the case that the product of two Lebesgue integrable functions $f,g$ will be Lebesgue integrable. But I was reading in a textbook that if at least one of these functions is bounded, then their product will be Lebesgue integrable. How can we prove this statement? I'd appreciate some input on this, thanks.","In general, I know that it is not necessarily the case that the product of two Lebesgue integrable functions $f,g$ will be Lebesgue integrable. But I was reading in a textbook that if at least one of these functions is bounded, then their product will be Lebesgue integrable. How can we prove this statement? I'd appreciate some input on this, thanks.",,"['real-analysis', 'measure-theory']"
23,Interesting relation derived from the identity $\sin^2 x + \cos^2 x \equiv 1$,Interesting relation derived from the identity,\sin^2 x + \cos^2 x \equiv 1,"Every one knows that $$\sin^2 x + \cos^2 x \equiv  1.$$ It is also well known that $$\sin x \equiv  \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n + 1}}{(2n+1)!},\quad\cos x \equiv  \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}.$$ One can rewrite the above mentioned identity as $$\sum_{n, k = 0}^{\infty} \frac{(-1)^{n+k} x^{2(n+k+1)}}{(2n+1)! (2k+1)!} + \sum_{m, l = 0}^{\infty} \frac{(-1)^{m+l} x^{2(m+l)}}{(2m)! (2l)!} \equiv  1.$$ Terms of the $\sin^2 x$ series cancel out all but one (which is equal to 1)  terms of the $\cos^2 x$ series. This fact can be expressed as $$(-1)^{c-1} x^{2c} \sigma(c) + (-1)^c x^{2c} \gamma(c) = 0,$$ where $n+k+1 = m+l= c \geq 1$ and $$\sigma(c) = \sum_{n=0}^{c-1} \frac{1}{(2n+1)! (2(c-n)-1)!},\quad\gamma(c) = \sum_{m=0}^{c} \frac{1}{(2m)! (2(c-m))!}.$$ At the moment it is not obvious for me that $$\sigma(c) \equiv \gamma(c).$$ Does any one have an idea how last identity can be proved?",Every one knows that It is also well known that One can rewrite the above mentioned identity as Terms of the series cancel out all but one (which is equal to 1)  terms of the series. This fact can be expressed as where and At the moment it is not obvious for me that Does any one have an idea how last identity can be proved?,"\sin^2 x + \cos^2 x \equiv  1. \sin x \equiv  \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n + 1}}{(2n+1)!},\quad\cos x \equiv  \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}. \sum_{n, k = 0}^{\infty} \frac{(-1)^{n+k} x^{2(n+k+1)}}{(2n+1)! (2k+1)!} + \sum_{m, l = 0}^{\infty} \frac{(-1)^{m+l} x^{2(m+l)}}{(2m)! (2l)!} \equiv  1. \sin^2 x \cos^2 x (-1)^{c-1} x^{2c} \sigma(c) + (-1)^c x^{2c} \gamma(c) = 0, n+k+1 = m+l= c \geq 1 \sigma(c) = \sum_{n=0}^{c-1} \frac{1}{(2n+1)! (2(c-n)-1)!},\quad\gamma(c) = \sum_{m=0}^{c} \frac{1}{(2m)! (2(c-m))!}. \sigma(c) \equiv \gamma(c).","['real-analysis', 'algebra-precalculus']"
24,Building intuition for Riemann-Stieltjes integral,Building intuition for Riemann-Stieltjes integral,,"Soft question: how can I build geometric intuition for and/or visualize the Riemann–Stieltjes integral? This is pretty easy with Riemann-integral, but in case of Riemann-Stieltjes integral I always have to rely on symbolics.","Soft question: how can I build geometric intuition for and/or visualize the Riemann–Stieltjes integral? This is pretty easy with Riemann-integral, but in case of Riemann-Stieltjes integral I always have to rely on symbolics.",,"['real-analysis', 'measure-theory', 'riemann-integration']"
25,"Continuous $f:[0,1]\to\mathbb{R}$ such that $f(0)=f(1)$ and $\forall\alpha\in(0,1)\exists c\in[0,1-\alpha]|f(c)=f(c+\alpha)$?",Continuous  such that  and ?,"f:[0,1]\to\mathbb{R} f(0)=f(1) \forall\alpha\in(0,1)\exists c\in[0,1-\alpha]|f(c)=f(c+\alpha)","Let $f:[0,1]\to\mathbb{R}$ continuous such that $f(0)=f(1)$. Is it true that $\forall\alpha\in(0,1)\exists c\in[0,1-\alpha]|f(c)=f(c+\alpha)$? At first I tried to find a counterexample but my intuition says it's true. Then I've got the idea of applying Bolzano's Theorem to $g(x)=f(x)-f(x+\alpha)$ defined on $[0,1-\alpha]$  but I didn't get anything. What can I do?","Let $f:[0,1]\to\mathbb{R}$ continuous such that $f(0)=f(1)$. Is it true that $\forall\alpha\in(0,1)\exists c\in[0,1-\alpha]|f(c)=f(c+\alpha)$? At first I tried to find a counterexample but my intuition says it's true. Then I've got the idea of applying Bolzano's Theorem to $g(x)=f(x)-f(x+\alpha)$ defined on $[0,1-\alpha]$  but I didn't get anything. What can I do?",,['real-analysis']
26,Counter example for limit,Counter example for limit,,"I have a question,we know that limit of $3x$ when $x$ approaches $2$ is $6.$ We can prove that with epsilon and delta definition. Suppose I wrongly assume the limit is $7$ then by definition for some epsilon I must be unable to find a delta. What is that epsilon? Just trying understand the notion of limit using counter example. That is for what value of epsilon this fails? $|3x-7|< \epsilon$ whenever $|x-2|< \delta.$","I have a question,we know that limit of $3x$ when $x$ approaches $2$ is $6.$ We can prove that with epsilon and delta definition. Suppose I wrongly assume the limit is $7$ then by definition for some epsilon I must be unable to find a delta. What is that epsilon? Just trying understand the notion of limit using counter example. That is for what value of epsilon this fails? $|3x-7|< \epsilon$ whenever $|x-2|< \delta.$",,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
27,Show that $(1+\frac{x}{n})^n \rightarrow e^x$ uniformly on any bounded interval of the real line.,Show that  uniformly on any bounded interval of the real line.,(1+\frac{x}{n})^n \rightarrow e^x,"Show that $(1+\frac{x}{n})^n \rightarrow e^x$ uniformly on any bounded interval of the real line. I am trying to argue from the definition of uniform convergence for a sequence of real-valued functions, but am struggling quite a lot. My efforts so far have concentrated on trying to find a sequences, ${a_n}$ which tends to zero, such that $$|(1+\frac{x}{n})^n -e^x |\leq a_n$$ for all $n$. But I have been unsuccessful thus far. All help is greatly appreciated.","Show that $(1+\frac{x}{n})^n \rightarrow e^x$ uniformly on any bounded interval of the real line. I am trying to argue from the definition of uniform convergence for a sequence of real-valued functions, but am struggling quite a lot. My efforts so far have concentrated on trying to find a sequences, ${a_n}$ which tends to zero, such that $$|(1+\frac{x}{n})^n -e^x |\leq a_n$$ for all $n$. But I have been unsuccessful thus far. All help is greatly appreciated.",,"['real-analysis', 'sequences-and-series', 'uniform-convergence']"
28,Compute $\lim_{n \to \infty} n(\frac{{\pi}^2}{6} -( \sum_{k=1}^{n} \frac{1}{k^2} ) ) $,Compute,\lim_{n \to \infty} n(\frac{{\pi}^2}{6} -( \sum_{k=1}^{n} \frac{1}{k^2} ) ) ,"Basel series $$ \lim_{n\to \infty} \sum_{k=1}^{n} \frac{1}{k^2} = \frac{{\pi}^2}{6} $$ is well known. I'm interested in computing the limit value  $$ \lim_{n \to \infty} n\left(\frac{{\pi}^2}{6} - \sum_{k=1}^{n} \frac{1}{k^2} \right) $$ although I am not sure even whether this limit exists or not.. Does this limit exists? If exists, how can we compute this? Thanks in advance.","Basel series $$ \lim_{n\to \infty} \sum_{k=1}^{n} \frac{1}{k^2} = \frac{{\pi}^2}{6} $$ is well known. I'm interested in computing the limit value  $$ \lim_{n \to \infty} n\left(\frac{{\pi}^2}{6} - \sum_{k=1}^{n} \frac{1}{k^2} \right) $$ although I am not sure even whether this limit exists or not.. Does this limit exists? If exists, how can we compute this? Thanks in advance.",,"['calculus', 'real-analysis', 'limits']"
29,Tietze extension theorem in LCH spaces,Tietze extension theorem in LCH spaces,,"Let $X$ be a locally compact Hausdorff space and $K$ be a compact subset of $X$. Then any function $f\in C(K)$ can be extended to a function in $C(X)$ which vanishes outside a compact set. I have searched several books for the proof. It seems that the authors believe it is just an exercise-level proposition. Since $X$ is an LCH space, we can always find open set $V$ containing $K$ with compact closure $\overline{V}$ and we can extend $f$ to $\overline{V}$ by classical Tietze extension theorem. I also know the fact that $f\in C(K)$ implies that the the range of $f$ is contained in a closed interval $[a,b]$, but how do we define a $F \in C(X)$ satisfying the requirements?","Let $X$ be a locally compact Hausdorff space and $K$ be a compact subset of $X$. Then any function $f\in C(K)$ can be extended to a function in $C(X)$ which vanishes outside a compact set. I have searched several books for the proof. It seems that the authors believe it is just an exercise-level proposition. Since $X$ is an LCH space, we can always find open set $V$ containing $K$ with compact closure $\overline{V}$ and we can extend $f$ to $\overline{V}$ by classical Tietze extension theorem. I also know the fact that $f\in C(K)$ implies that the the range of $f$ is contained in a closed interval $[a,b]$, but how do we define a $F \in C(X)$ satisfying the requirements?",,"['real-analysis', 'general-topology']"
30,A double series $\frac13 \sum_{j=1}^{\infty}\sum_{i=1}^{\infty}\frac{(i-1)! (j-1)!}{(i+j)!}H_{i+j}$ giving $\zeta(3)$,A double series  giving,\frac13 \sum_{j=1}^{\infty}\sum_{i=1}^{\infty}\frac{(i-1)! (j-1)!}{(i+j)!}H_{i+j} \zeta(3),"Here is a symmetric rational double series giving Apery's constant : $$ \frac13 \sum_{j=1}^{\infty}\sum_{i=1}^{\infty} \displaystyle \frac{(i-1)! (j-1)!}{(i+j)!}  H_{i+j} = \zeta(3) $$ where $\displaystyle H_{n}:=\sum_{1}^{n} \frac{1}{k}$, $n=1,2,\cdots,$ are the harmonic numbers . How would you prove it? Edit. In 2005, I sent this result to Wolfram MathWorld (see equation 25) , I built it as an echo of $$  \sum_{j=1}^{\infty}\sum_{i=1}^{\infty} \displaystyle \frac{(i-1)! (j-1)!}{(i+j)!}  = \zeta(2). $$ See Jack's pretty answer and see my answer below.","Here is a symmetric rational double series giving Apery's constant : $$ \frac13 \sum_{j=1}^{\infty}\sum_{i=1}^{\infty} \displaystyle \frac{(i-1)! (j-1)!}{(i+j)!}  H_{i+j} = \zeta(3) $$ where $\displaystyle H_{n}:=\sum_{1}^{n} \frac{1}{k}$, $n=1,2,\cdots,$ are the harmonic numbers . How would you prove it? Edit. In 2005, I sent this result to Wolfram MathWorld (see equation 25) , I built it as an echo of $$  \sum_{j=1}^{\infty}\sum_{i=1}^{\infty} \displaystyle \frac{(i-1)! (j-1)!}{(i+j)!}  = \zeta(2). $$ See Jack's pretty answer and see my answer below.",,"['calculus', 'real-analysis', 'sequences-and-series', 'riemann-zeta', 'harmonic-numbers']"
31,Does $L^p$-convergence imply pointwise convergence for $C_0^\infty$ functions?,Does -convergence imply pointwise convergence for  functions?,L^p C_0^\infty,"It is stated in my professor's notes that, given a sequence $\{f_j\}$ of  $C_0^\infty(\Omega)$ functions (infinitely differentiable with compact support), and a function $g\in C_0^\infty(\Omega)$, all defined in an open set $\Omega\in\mathbb{R}^n$: $$\|f_j-g\|_{L_p(\Omega)}\to0\implies|f_j(x)-g(x)|\to0\,\forall x\in\Omega$$ I was not able to prove it, however. Searching on the Internet, I found many counterexamples to the statement that $L^p$-convergence implies pointwise convergence, but they do not deal with $C_0^\infty(\Omega)$ functions, so it may also be true. Can anyone point to a proof or a counterexample?","It is stated in my professor's notes that, given a sequence $\{f_j\}$ of  $C_0^\infty(\Omega)$ functions (infinitely differentiable with compact support), and a function $g\in C_0^\infty(\Omega)$, all defined in an open set $\Omega\in\mathbb{R}^n$: $$\|f_j-g\|_{L_p(\Omega)}\to0\implies|f_j(x)-g(x)|\to0\,\forall x\in\Omega$$ I was not able to prove it, however. Searching on the Internet, I found many counterexamples to the statement that $L^p$-convergence implies pointwise convergence, but they do not deal with $C_0^\infty(\Omega)$ functions, so it may also be true. Can anyone point to a proof or a counterexample?",,"['real-analysis', 'analysis', 'convergence-divergence', 'lp-spaces']"
32,Limit of $x\left(\left(1 + \frac{1}{x}\right)^x - e\right)$ when $x\to\infty$,Limit of  when,x\left(\left(1 + \frac{1}{x}\right)^x - e\right) x\to\infty,"I am stuck on how to calculate the following limit: $$\lim_{x\to\infty}x\left(\left(1 + \frac{1}{x}\right)^x - e\right).$$ Definitely, it has to be through l'Hôpital's rule. We know that $\lim_{x\to\infty} (1+(1/x))^x = e$. So, I wrote the above expression as $$\lim_{x\to\infty}\frac{\left(1 + \frac{1}{x}\right)^x - e}{1/x}.$$Both numerator and denominator tend to zero as x tends to infinity. I applied l'Hôpital's rule twice, but I got the limit equal to infinity which is clearly wrong. In the book, it says that the limit should be $-e/2$. Any help please? Also, in case someone managed to solve it, please tell me which numerator and denominator did you apply your l'Hôpital's rule to in both cases? Thanks a lot Also, guys can you tell me how to write equations in this forum? Thanks.","I am stuck on how to calculate the following limit: $$\lim_{x\to\infty}x\left(\left(1 + \frac{1}{x}\right)^x - e\right).$$ Definitely, it has to be through l'Hôpital's rule. We know that $\lim_{x\to\infty} (1+(1/x))^x = e$. So, I wrote the above expression as $$\lim_{x\to\infty}\frac{\left(1 + \frac{1}{x}\right)^x - e}{1/x}.$$Both numerator and denominator tend to zero as x tends to infinity. I applied l'Hôpital's rule twice, but I got the limit equal to infinity which is clearly wrong. In the book, it says that the limit should be $-e/2$. Any help please? Also, in case someone managed to solve it, please tell me which numerator and denominator did you apply your l'Hôpital's rule to in both cases? Thanks a lot Also, guys can you tell me how to write equations in this forum? Thanks.",,"['real-analysis', 'limits', 'analysis', 'exponential-function']"
33,"For which $n$ is $ \int \limits_0^{2\pi} \prod \limits_{k=1}^n \cos(k x)\,dx $ non-zero?",For which  is  non-zero?,"n  \int \limits_0^{2\pi} \prod \limits_{k=1}^n \cos(k x)\,dx ","I can verify easily that for $n=1$ and $2$ it's $0$, $3$ and $4$ nonzero, $4$ and $5$ $0$, etc. but it seems like there must be something deeper here (or at least a trick).","I can verify easily that for $n=1$ and $2$ it's $0$, $3$ and $4$ nonzero, $4$ and $5$ $0$, etc. but it seems like there must be something deeper here (or at least a trick).",,"['calculus', 'real-analysis', 'integration']"
34,A question about using the squeeze theorem?,A question about using the squeeze theorem?,,"If you have the inequalities not equal to, but the function is just strictly between two other functions, can you use the squeeze theorem?","If you have the inequalities not equal to, but the function is just strictly between two other functions, can you use the squeeze theorem?",,"['calculus', 'real-analysis']"
35,Differentiability of the Cantor Function,Differentiability of the Cantor Function,,"I know that the Cantor function is differentiable a.e. but I want to prove it without using the theorem about monotonic functions. I have already proved that $f'(x) = 0$ for all $x \in [0,1] \backslash \mathbb{C}$ where $\mathbb{C}$ is the Cantor set. But I'm not sure how to go about proving that if $x \in \mathbb{C}$ then $f$ is not differentiable at $x$. Actually, upon reflection, I think I have already proved differentiability a.e. but I would still like to know how to finish this part. Also, the definition I am using for the function:   $$f:[0,1] \to [0,1]$$ Let $x \in [0,1]$ with ternary expansion $0.a_1a_2...$ Let $N$ be the first $n \in \mathbb{N}$ such that $a_n = 1$. If for all $n \in \mathbb{N}$, $a_n \in \{0,2\}$, let $N = \infty$. Now define $b_n = \frac{a_n}{2}$ for all $n < N$ and $b_N = 1$. Then $$f(x) = \sum_{i=1}^{N} \frac{b_n}{2^n}.$$","I know that the Cantor function is differentiable a.e. but I want to prove it without using the theorem about monotonic functions. I have already proved that $f'(x) = 0$ for all $x \in [0,1] \backslash \mathbb{C}$ where $\mathbb{C}$ is the Cantor set. But I'm not sure how to go about proving that if $x \in \mathbb{C}$ then $f$ is not differentiable at $x$. Actually, upon reflection, I think I have already proved differentiability a.e. but I would still like to know how to finish this part. Also, the definition I am using for the function:   $$f:[0,1] \to [0,1]$$ Let $x \in [0,1]$ with ternary expansion $0.a_1a_2...$ Let $N$ be the first $n \in \mathbb{N}$ such that $a_n = 1$. If for all $n \in \mathbb{N}$, $a_n \in \{0,2\}$, let $N = \infty$. Now define $b_n = \frac{a_n}{2}$ for all $n < N$ and $b_N = 1$. Then $$f(x) = \sum_{i=1}^{N} \frac{b_n}{2^n}.$$",,"['real-analysis', 'derivatives']"
36,"If $f(3x)=f(x)$ and $f$ is continuous, show that $f(x)$ is a constant function.","If  and  is continuous, show that  is a constant function.",f(3x)=f(x) f f(x),"If $f(x)$ is a continuous function such that $f(3x)=f(x)$ and the domain of $f$ is all non-negative real numbers. Prove that $f$ is a constant function. What I did: $$f(3x)=f(x)=f\left(\frac{x}{3}\right)=\cdots= f\left(\frac{x}{3^n}\right)$$ Now as $n$ tends to infinity, $f(\frac{x}{3^n})$ tends to $f(0)$ and hence $f(x)=f(0)$ for all $x$. However, I think the second step is a bit dodgy. I can't quite tell how, since I lack sufficient mathematical maturity, but it just doesn't seem right. I'd be glad if someone could provide a more rigorous proof of the problem. Thanks in advance.","If $f(x)$ is a continuous function such that $f(3x)=f(x)$ and the domain of $f$ is all non-negative real numbers. Prove that $f$ is a constant function. What I did: $$f(3x)=f(x)=f\left(\frac{x}{3}\right)=\cdots= f\left(\frac{x}{3^n}\right)$$ Now as $n$ tends to infinity, $f(\frac{x}{3^n})$ tends to $f(0)$ and hence $f(x)=f(0)$ for all $x$. However, I think the second step is a bit dodgy. I can't quite tell how, since I lack sufficient mathematical maturity, but it just doesn't seem right. I'd be glad if someone could provide a more rigorous proof of the problem. Thanks in advance.",,"['real-analysis', 'functions', 'functional-equations']"
37,limit of absolute value of something equals absolute value of limit of something?,limit of absolute value of something equals absolute value of limit of something?,,"Does limit of  absolute value of something always equal absolute value of limit of something? Specifically, can I just say the below equality is true or do I need to prove it? I'm not sure how to prove it. Could you help me?","Does limit of  absolute value of something always equal absolute value of limit of something? Specifically, can I just say the below equality is true or do I need to prove it? I'm not sure how to prove it. Could you help me?",,"['calculus', 'real-analysis']"
38,Proving Abel's identity for the Dilogarithm.,Proving Abel's identity for the Dilogarithm.,,"Abel's Identity for the dilogarithm is stated as follows Theorem (Abels Identity) Given that $x,y \,{\not\in}\,[1,\infty)$ then   \begin{align*} 	\log(1-x) \log(1-y)                  =  \operatorname{Li}_2(u) + \operatorname{Li}_2(v) - \operatorname{Li}_2(u v) - \operatorname{Li}_2(x) - \operatorname{Li}_2(y)\,  \end{align*}   holds for all $x,y$. Where $u = \frac x{1-y}$ and $v = \frac y{1-x}$. I was able to find the original paper where the statement is proven. It can be read here Abels Identity . I was very pleased finding the paper, and being from Norway I thought I should be able to read it. (No pun inteded.) Alas the paper is written in french, a language foreign to me. Can anyone help me outline the proof for this identity? I tried differentiating the expression with regard to both $x$ and $y$ but it all got very messy, very fast. All help is appreciated =)","Abel's Identity for the dilogarithm is stated as follows Theorem (Abels Identity) Given that $x,y \,{\not\in}\,[1,\infty)$ then   \begin{align*} 	\log(1-x) \log(1-y)                  =  \operatorname{Li}_2(u) + \operatorname{Li}_2(v) - \operatorname{Li}_2(u v) - \operatorname{Li}_2(x) - \operatorname{Li}_2(y)\,  \end{align*}   holds for all $x,y$. Where $u = \frac x{1-y}$ and $v = \frac y{1-x}$. I was able to find the original paper where the statement is proven. It can be read here Abels Identity . I was very pleased finding the paper, and being from Norway I thought I should be able to read it. (No pun inteded.) Alas the paper is written in french, a language foreign to me. Can anyone help me outline the proof for this identity? I tried differentiating the expression with regard to both $x$ and $y$ but it all got very messy, very fast. All help is appreciated =)",,"['calculus', 'real-analysis', 'special-functions']"
39,Helly's selection theorem,Helly's selection theorem,,"Can someone guide me to a reference (preferably open access online) stating and proving Helly's selection theorem for sequences monotone  uniformly bounded  functions on $[0,1]$. Something that can actually be taught without introducing the ideas of bounded total variation or probability theory. Just one straight proof and coherent statement for students in an introductory real analysis course.","Can someone guide me to a reference (preferably open access online) stating and proving Helly's selection theorem for sequences monotone  uniformly bounded  functions on $[0,1]$. Something that can actually be taught without introducing the ideas of bounded total variation or probability theory. Just one straight proof and coherent statement for students in an introductory real analysis course.",,"['real-analysis', 'reference-request']"
40,"If $\alpha$ is an irrational real number, why is $\alpha\mathbb{Z}+\mathbb{Z}$ dense in $\mathbb{R}$?","If  is an irrational real number, why is  dense in ?",\alpha \alpha\mathbb{Z}+\mathbb{Z} \mathbb{R},"This is chapter $4$ exercise $25.b$ of Walter Rudin's Principles of Mathematical Analysis , this problem has occupied my mind for a long time, and I haven't been able to solve it, I would like to see an answer to this question. Thanks.","This is chapter $4$ exercise $25.b$ of Walter Rudin's Principles of Mathematical Analysis , this problem has occupied my mind for a long time, and I haven't been able to solve it, I would like to see an answer to this question. Thanks.",,['real-analysis']
41,Showing $f(x)=x^4$ is not uniformly continuous,Showing  is not uniformly continuous,f(x)=x^4,"I am looking at uniform continuity  (for my exam) at the moment and I'm fine with showing that a function is uniformly continuous but I'm having a bit more trouble showing that it is not uniformly continuous, for example: show that $x^4$ is not uniformly continuous on $\mathbb{R}$, so my solution would be something like: Assume that it is uniformly continuous then: $$\forall\epsilon\geq0\exists\delta>0:\forall{x,y}\in\mathbb{R}\ \mbox{if}\ |x-y|<\delta \mbox{then} |x^4-y^4|<\epsilon$$ Take $x=\frac{\delta}{2}+\frac{1}{\delta}$ and $y=\frac{1}{\delta}$ then we have that $|x-y|=|\frac{\delta}{2}+\frac{1}{\delta}-\frac{1}{\delta}|=|\frac{\delta}{2}|<\delta$ however  $$|f(x)-f(y)|=|\frac{\delta^3}{8}+3\frac{\delta}{4}+\frac{3}{2\delta}|$$ Now if $\delta\leq 1$ then $|f(x)-f(y)|>\frac{3}{4}$ and if $\delta\geq 1$ then $|f(x)-f(y)|>\frac{3}{4}$ so there exists not $\delta$ for $\epsilon < \frac{3}{4}$ and we have a contradiction. So I was wondering if this was ok (I think it's fine) but also if this was the general way to go about showing that some function is not uniformly continuous? Or if there was any other ways of doing this that are not from the definition? Thanks very much for any help","I am looking at uniform continuity  (for my exam) at the moment and I'm fine with showing that a function is uniformly continuous but I'm having a bit more trouble showing that it is not uniformly continuous, for example: show that $x^4$ is not uniformly continuous on $\mathbb{R}$, so my solution would be something like: Assume that it is uniformly continuous then: $$\forall\epsilon\geq0\exists\delta>0:\forall{x,y}\in\mathbb{R}\ \mbox{if}\ |x-y|<\delta \mbox{then} |x^4-y^4|<\epsilon$$ Take $x=\frac{\delta}{2}+\frac{1}{\delta}$ and $y=\frac{1}{\delta}$ then we have that $|x-y|=|\frac{\delta}{2}+\frac{1}{\delta}-\frac{1}{\delta}|=|\frac{\delta}{2}|<\delta$ however  $$|f(x)-f(y)|=|\frac{\delta^3}{8}+3\frac{\delta}{4}+\frac{3}{2\delta}|$$ Now if $\delta\leq 1$ then $|f(x)-f(y)|>\frac{3}{4}$ and if $\delta\geq 1$ then $|f(x)-f(y)|>\frac{3}{4}$ so there exists not $\delta$ for $\epsilon < \frac{3}{4}$ and we have a contradiction. So I was wondering if this was ok (I think it's fine) but also if this was the general way to go about showing that some function is not uniformly continuous? Or if there was any other ways of doing this that are not from the definition? Thanks very much for any help",,"['real-analysis', 'analysis', 'uniform-continuity']"
42,Is there a compact subset of the irrationals with positive Lebesgue measure?,Is there a compact subset of the irrationals with positive Lebesgue measure?,,"Does there exist $K \subseteq \mathbb{R} \backslash \mathbb{Q}$ such that $K$ is compact, and has Lebesgue measure greater than $0$? As I have been trying to think of examples, I suspect that any subset of $\mathbb{R} \backslash \mathbb{Q}$ that is closed can be at most countable, since the closure of an uncountable subset of irrationals should contain some rationals. And, the Lebesgue measure of a countable set is $0$. If there are any examples of such a set, I would be very interested to know how it is constructed.","Does there exist $K \subseteq \mathbb{R} \backslash \mathbb{Q}$ such that $K$ is compact, and has Lebesgue measure greater than $0$? As I have been trying to think of examples, I suspect that any subset of $\mathbb{R} \backslash \mathbb{Q}$ that is closed can be at most countable, since the closure of an uncountable subset of irrationals should contain some rationals. And, the Lebesgue measure of a countable set is $0$. If there are any examples of such a set, I would be very interested to know how it is constructed.",,"['real-analysis', 'measure-theory']"
43,Find $\lim_{n\to\infty}\left(\int _{0}^{n}\frac{\mathrm dx}{x^{x^n}}\right)^n$,Find,\lim_{n\to\infty}\left(\int _{0}^{n}\frac{\mathrm dx}{x^{x^n}}\right)^n,"I had to solve the following limit : $$\lim_{n\to\infty}\left(\int _{0}^{n} \frac{\mathrm dx}{x^{x^n}}\right)^n$$ So far I've made no progress and the integral looks unsolvable. With online calculators, it seems like the inner integral approaches to $1$ , yielding the indeterminate form of $1^{\infty}$ . How can I approach to solve it?","I had to solve the following limit : So far I've made no progress and the integral looks unsolvable. With online calculators, it seems like the inner integral approaches to , yielding the indeterminate form of . How can I approach to solve it?",\lim_{n\to\infty}\left(\int _{0}^{n} \frac{\mathrm dx}{x^{x^n}}\right)^n 1 1^{\infty},"['real-analysis', 'calculus', 'integration', 'limits']"
44,Calculate $\lim_{n \rightarrow \infty } \frac{n (1- na_n)}{\log n} $,Calculate,\lim_{n \rightarrow \infty } \frac{n (1- na_n)}{\log n} ,"Given the recursive sequence $\{a_n\}$ defined by setting $0 < a_1 < 1, \; a_{n+1} = a_n(1-a_n) , \; n \ge 1 $ Calculate :  $$\lim_{n \rightarrow \infty } \frac{n (1- na_n)}{\log n} $$ My attempts :  $$\lim_{n \rightarrow \infty } \frac{n (1- n a_n)}{\log n} =\lim_{n \rightarrow \infty }\frac {n \left (\frac{1}{n a_n} -1 \right) n a_n} {\log n}= \lim_{n \rightarrow \infty }  \frac {\frac{1}{a_n} - n}{ \log n}$$ Now I am not able to proceed further. Please help me. Thank You.","Given the recursive sequence $\{a_n\}$ defined by setting $0 < a_1 < 1, \; a_{n+1} = a_n(1-a_n) , \; n \ge 1 $ Calculate :  $$\lim_{n \rightarrow \infty } \frac{n (1- na_n)}{\log n} $$ My attempts :  $$\lim_{n \rightarrow \infty } \frac{n (1- n a_n)}{\log n} =\lim_{n \rightarrow \infty }\frac {n \left (\frac{1}{n a_n} -1 \right) n a_n} {\log n}= \lim_{n \rightarrow \infty }  \frac {\frac{1}{a_n} - n}{ \log n}$$ Now I am not able to proceed further. Please help me. Thank You.",,"['real-analysis', 'limits']"
45,A series about $n!$ and Riemann zeta function,A series about  and Riemann zeta function,n!,"Compute  $$ \sum_{n=1}^{\infty}{\left( \frac{n^n}{n!e^n}-\frac{1}{\sqrt{2\pi n}} \right)}. $$ By the software Mathematica, I find $$ \sum_{n=1}^{\infty}{\left( \frac{n^n}{n!e^n}-\frac{1}{\sqrt{2\pi n}} \right)}=-\frac{2}{3}-\frac{\zeta \left( 1/2 \right)}{\sqrt{2\pi}}. $$","Compute  $$ \sum_{n=1}^{\infty}{\left( \frac{n^n}{n!e^n}-\frac{1}{\sqrt{2\pi n}} \right)}. $$ By the software Mathematica, I find $$ \sum_{n=1}^{\infty}{\left( \frac{n^n}{n!e^n}-\frac{1}{\sqrt{2\pi n}} \right)}=-\frac{2}{3}-\frac{\zeta \left( 1/2 \right)}{\sqrt{2\pi}}. $$",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
46,Baby Rudin Theorem 2.34.,Baby Rudin Theorem 2.34.,,"Theorem 2.34 Compact subsets of a metric space are closed. Proof. Suppose $K\subseteq X$ , $K$ compact. Let $p\in K^c$ , $q\in K$ . Let $V_q,W_q$ be neighborhoods of $p$ and $q$ with radius less than $\frac 1 2 d(p,q)$ . Since $K$ is compact, we have $K\subseteq W_{q_1}\cup \cdots\cup W_{q_n}=W$ for some $q_1,...,q_n\in K$ . If V= $V_{q_1}\cap \cdots V_{q_n}$ , $V$ is a neighborhood of p which does not intersect $W$ , then $V\subseteq K^c$ so $p$ is an interior point of $K^c$ . QED. My understading of this proof is that $V$ is actually $$V= \text {'$\min$'}\left\{V_{q_i}\right\}$$ Where the minimum is to be taken in terms of inclusion. Also, we have that $V$ doesn't intersect $W$ because $V_{q_i}$ doesn't intersect $W_{q_i}$ for every $i$ , right (How do I prove that)? I'm uploading a picture of my understanding of the situation, am I understanding this correctly?","Theorem 2.34 Compact subsets of a metric space are closed. Proof. Suppose , compact. Let , . Let be neighborhoods of and with radius less than . Since is compact, we have for some . If V= , is a neighborhood of p which does not intersect , then so is an interior point of . QED. My understading of this proof is that is actually Where the minimum is to be taken in terms of inclusion. Also, we have that doesn't intersect because doesn't intersect for every , right (How do I prove that)? I'm uploading a picture of my understanding of the situation, am I understanding this correctly?","K\subseteq X K p\in K^c q\in K V_q,W_q p q \frac 1 2 d(p,q) K K\subseteq W_{q_1}\cup \cdots\cup W_{q_n}=W q_1,...,q_n\in K V_{q_1}\cap \cdots V_{q_n} V W V\subseteq K^c p K^c V V= \text {'\min'}\left\{V_{q_i}\right\} V W V_{q_i} W_{q_i} i","['real-analysis', 'analysis', 'proof-explanation']"
47,"If $f\in C[0,1)$, $\int_{0}^{1}f^{2}(t)dt =\infty$, can one construct $g\in C[0,1)$ so $\int_{0}^{1}g^{2}dt < \infty$, $\int fgdt = \infty$?","If , , can one construct  so , ?","f\in C[0,1) \int_{0}^{1}f^{2}(t)dt =\infty g\in C[0,1) \int_{0}^{1}g^{2}dt < \infty \int fgdt = \infty","If a real $f\in C[0,1)$ satisfies $\int_{0}^{1}f^{2}(t)dt =\infty$, can one explicitly construct $g\in C[0,1)$ such that $\int_{0}^{1}g^{2}dt < \infty$ and $\int_0^1 fgdt = \infty$? There must must exist such a function $g\in L^{2}(0,1)$ because of the Uniform boundedness principle of Functional Analysis. This comes from assuming there is no such $g \in L^{2}[0,1]$, and considering the family of bounded linear functionals $\Phi_{x}(g)=\int_{0}^{x}fgdt$ on $L^{2}[0,1]$ which then satisfy $\sup_{x \in (0,1)}|\Phi_{x}(g)| < \infty$ for every fixed $g \in L^{2}[0,1]$. The conclusion is that $\|\Phi_{x}\|=\int_{0}^{x}|f|^{2}dt$ is bounded in $x$, contrary to assumption. However, in the case that $f$ is continous on $[0,1)$, it seems to me that there should be a way to construct a continuous $g$ on $[0,1)$, but I don't see it. Feel free to assume $f$ is continuously differentiable on $[0,1)$ if it helps. This is not a homework problem ... I'm curious if there is a construction under reasonable circumstances.","If a real $f\in C[0,1)$ satisfies $\int_{0}^{1}f^{2}(t)dt =\infty$, can one explicitly construct $g\in C[0,1)$ such that $\int_{0}^{1}g^{2}dt < \infty$ and $\int_0^1 fgdt = \infty$? There must must exist such a function $g\in L^{2}(0,1)$ because of the Uniform boundedness principle of Functional Analysis. This comes from assuming there is no such $g \in L^{2}[0,1]$, and considering the family of bounded linear functionals $\Phi_{x}(g)=\int_{0}^{x}fgdt$ on $L^{2}[0,1]$ which then satisfy $\sup_{x \in (0,1)}|\Phi_{x}(g)| < \infty$ for every fixed $g \in L^{2}[0,1]$. The conclusion is that $\|\Phi_{x}\|=\int_{0}^{x}|f|^{2}dt$ is bounded in $x$, contrary to assumption. However, in the case that $f$ is continous on $[0,1)$, it seems to me that there should be a way to construct a continuous $g$ on $[0,1)$, but I don't see it. Feel free to assume $f$ is continuously differentiable on $[0,1)$ if it helps. This is not a homework problem ... I'm curious if there is a construction under reasonable circumstances.",,"['real-analysis', 'functional-analysis']"
48,vercongent sequences,vercongent sequences,,"Definition- We say a sequence $(x_n)$ verconges to $x$ if there exist an $\epsilon>0$ such that for all $N\in \Bbb{N}$ , $n\ge N \implies |x_n-x|<\epsilon$ . Loosely speaking, by convergent sequence we mean that a sequence is convergent to some point $x$ if we can confine its infinite ""tail"" in some neighbourhood of $x$ . In similar wordings, vercongent sequence should mean, that a sequence $(x_n)$ is vercongent to $x$ if  we are given a point in its infinite ""tail"", say $\mathfrak{n}\in \Bbb{N}$ , then we can put a wall around $x$ , in which whole tail after $\frak{n}$ must lie. Question- Give an example of convergent sequence and one that is not vercongent or verdigent According to me $(x_n)=\{1,\frac12,\frac13, \frac14, \dots \}$ verconges to $0$ , also to $1$ as for any given $n\in \Bbb{N}$ , all terms after $n$ lies in $\epsilon=\frac{1}{n}$ neighbourhood of $0$ , clearly, and for $1$ we $\epsilon=\frac{n-1}{n}$ works. I guess  a convergent sequence is also vercongent , with the same limit, any many more limits, almost any number works as a limit, am I correct? But sequence $\{1,-1,1,-1,1,-1,\dots\}$ is not convergent as it is oscillatory but it is vercongent as for any natural number $\frak{n}$ , we can always choose $\epsilon=10$ around $0$ or $1$ or $-1$ , so it verconges to many numbers, a big enough epsilon will work. Thus vercongent does not imply convergent . But on the other hand, sequence $\{1,2,3,\dots \}$ neither converges nor verconges . Please correct me if I am wrong in this understanding of this exercise from Abbott's book "" Understanding Analysis .""","Definition- We say a sequence verconges to if there exist an such that for all , . Loosely speaking, by convergent sequence we mean that a sequence is convergent to some point if we can confine its infinite ""tail"" in some neighbourhood of . In similar wordings, vercongent sequence should mean, that a sequence is vercongent to if  we are given a point in its infinite ""tail"", say , then we can put a wall around , in which whole tail after must lie. Question- Give an example of convergent sequence and one that is not vercongent or verdigent According to me verconges to , also to as for any given , all terms after lies in neighbourhood of , clearly, and for we works. I guess  a convergent sequence is also vercongent , with the same limit, any many more limits, almost any number works as a limit, am I correct? But sequence is not convergent as it is oscillatory but it is vercongent as for any natural number , we can always choose around or or , so it verconges to many numbers, a big enough epsilon will work. Thus vercongent does not imply convergent . But on the other hand, sequence neither converges nor verconges . Please correct me if I am wrong in this understanding of this exercise from Abbott's book "" Understanding Analysis .""","(x_n) x \epsilon>0 N\in \Bbb{N} n\ge N \implies |x_n-x|<\epsilon x x (x_n) x \mathfrak{n}\in \Bbb{N} x \frak{n} (x_n)=\{1,\frac12,\frac13, \frac14, \dots \} 0 1 n\in \Bbb{N} n \epsilon=\frac{1}{n} 0 1 \epsilon=\frac{n-1}{n} \{1,-1,1,-1,1,-1,\dots\} \frak{n} \epsilon=10 0 1 -1 \{1,2,3,\dots \}","['real-analysis', 'convergence-divergence']"
49,A series with only rational terms for $\ln \ln 2$,A series with only rational terms for,\ln \ln 2,We all know that $$ \ln 2 = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n}. $$ Do you know a series with only rational terms for $$\ln \ln 2 = ?$$ Let's exclude base expansions with non explicit coefficients. Thanks!,We all know that $$ \ln 2 = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n}. $$ Do you know a series with only rational terms for $$\ln \ln 2 = ?$$ Let's exclude base expansions with non explicit coefficients. Thanks!,,"['real-analysis', 'sequences-and-series', 'number-theory', 'logarithms', 'real-numbers']"
50,"Integral$\int_0^\infty \ln x\,\exp(-\frac{1+x^4}{2\alpha x^2}) \frac{x^4+\alpha x^2- 1}{x^4}dx$?",Integral?,"\int_0^\infty \ln x\,\exp(-\frac{1+x^4}{2\alpha x^2}) \frac{x^4+\alpha x^2- 1}{x^4}dx","I am trying to prove $$ I:=\int_0^\infty \ln x\,\exp\left(-\frac{1+x^4}{2\alpha x^2}\right) \frac{x^4+\alpha x^2- 1}{x^4}dx=\frac{\sqrt{2\alpha^3 \pi}}{2\sqrt[\alpha]e},\qquad \alpha>0. $$ Note: The proof below shows how this is just a Gaussian integral! I am not sure how to start this one.  It seems very difficult to me However the answer is very nice. I thought maybe trying to write $I(\alpha)$ and $I'(\alpha)$ to try and simplify things but it didn't help much.  at $x=0$ there seems to be a problem with the integrand also however I am not sure how to go about using this.  Perhaps we could try and use a series expansion for $e^x=\sum_{n=0}^\infty x^n / n!$ however the function $e^{-1/x^2}$ is well known that its taylor series is zero despite the function not being. The factor of $x^4+\alpha x^2-1$ has been giving me trouble with simplifying the integrand.  Thanks. To those who just made an edit:  If you are looking for a +2, please edit something worthwhile.  I edited it back to what I had considering you didn't fix anything as is shown in the Edit History.","I am trying to prove $$ I:=\int_0^\infty \ln x\,\exp\left(-\frac{1+x^4}{2\alpha x^2}\right) \frac{x^4+\alpha x^2- 1}{x^4}dx=\frac{\sqrt{2\alpha^3 \pi}}{2\sqrt[\alpha]e},\qquad \alpha>0. $$ Note: The proof below shows how this is just a Gaussian integral! I am not sure how to start this one.  It seems very difficult to me However the answer is very nice. I thought maybe trying to write $I(\alpha)$ and $I'(\alpha)$ to try and simplify things but it didn't help much.  at $x=0$ there seems to be a problem with the integrand also however I am not sure how to go about using this.  Perhaps we could try and use a series expansion for $e^x=\sum_{n=0}^\infty x^n / n!$ however the function $e^{-1/x^2}$ is well known that its taylor series is zero despite the function not being. The factor of $x^4+\alpha x^2-1$ has been giving me trouble with simplifying the integrand.  Thanks. To those who just made an edit:  If you are looking for a +2, please edit something worthwhile.  I edited it back to what I had considering you didn't fix anything as is shown in the Edit History.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
51,"Compute $\sum_{i,j=1}^{\infty} \frac{(-1)^{i+j}}{i^2+j^2}$",Compute,"\sum_{i,j=1}^{\infty} \frac{(-1)^{i+j}}{i^2+j^2}","Compute $$\sum_{i,j=1}^{\infty} \frac{(-1)^{i+j}}{i^2+j^2}$$","Compute $$\sum_{i,j=1}^{\infty} \frac{(-1)^{i+j}}{i^2+j^2}$$",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
52,Is $p\mapsto \|f\|_p$ continuous?,Is  continuous?,p\mapsto \|f\|_p,"Suppose that $\|f\|_p < \infty$ for all $1\leq p < p'$, I want to know if the the following is true and in that case how to show it $p \mapsto \|f\|_p$ is continuous on $[1,p')$ Or maybe we need to impose some more constraints such as finite measure space. In case of finite measure space, I tried to use Egoroff's theorem .","Suppose that $\|f\|_p < \infty$ for all $1\leq p < p'$, I want to know if the the following is true and in that case how to show it $p \mapsto \|f\|_p$ is continuous on $[1,p')$ Or maybe we need to impose some more constraints such as finite measure space. In case of finite measure space, I tried to use Egoroff's theorem .",,['real-analysis']
53,Multiplying convergent and divergent sequences,Multiplying convergent and divergent sequences,,"When testing for null sequences I've had to say whether they were convergent or divergent, but say you've got a convergent sequence (a) and divergent sequence (b) and you multiplied them (so {ab}) would it make a divergent sequence or would it just cancel?","When testing for null sequences I've had to say whether they were convergent or divergent, but say you've got a convergent sequence (a) and divergent sequence (b) and you multiplied them (so {ab}) would it make a divergent sequence or would it just cancel?",,"['real-analysis', 'sequences-and-series', 'examples-counterexamples']"
54,"Given a cubic and quadratic share a root, prove $(ac-b^{2})(bd-c^{2})\geq 0$","Given a cubic and quadratic share a root, prove",(ac-b^{2})(bd-c^{2})\geq 0,"Here is an interesting problem.  Perhaps someone would be so kind as to give me a shove in the right direction?. If $ax^{3}+3bx^{2}+3cx+d$ and $ax^{2}+2bx+c$ share a common root, then prove that $$(ac-b^{2})(bd-c^{2})\geq 0$$ I thought about equating coefficients somehow, but that got messy. I used the quadratic formula on the given quadratic to find $\displaystyle x=\frac{-b\pm\sqrt{b^{2}-ac}}{a}$ are two roots. So, if the cubic shares on of these, then I should be able to sub this in for x in the cubic. Upon doing so, I got: $$\frac{2(ac-b^{2})\sqrt{b^{2}-4ac}}{a^{2}}-\frac{3bc}{a}+\frac{2b^{3}}{a^{2}}+d$$ This is where I got hung up.  This may not even by a good way to go about it. I see a part of what is to be proven in the above expression, $ac-b^{2}$ Setting it to 0 does not help much. I also thought about dividing them. If they share a root, then it should reduce to a quadratic in the numerator and a linear denominator. But, then what?. Can anyone give a hint as to the best way to proceed?.","Here is an interesting problem.  Perhaps someone would be so kind as to give me a shove in the right direction?. If $ax^{3}+3bx^{2}+3cx+d$ and $ax^{2}+2bx+c$ share a common root, then prove that $$(ac-b^{2})(bd-c^{2})\geq 0$$ I thought about equating coefficients somehow, but that got messy. I used the quadratic formula on the given quadratic to find $\displaystyle x=\frac{-b\pm\sqrt{b^{2}-ac}}{a}$ are two roots. So, if the cubic shares on of these, then I should be able to sub this in for x in the cubic. Upon doing so, I got: $$\frac{2(ac-b^{2})\sqrt{b^{2}-4ac}}{a^{2}}-\frac{3bc}{a}+\frac{2b^{3}}{a^{2}}+d$$ This is where I got hung up.  This may not even by a good way to go about it. I see a part of what is to be proven in the above expression, $ac-b^{2}$ Setting it to 0 does not help much. I also thought about dividing them. If they share a root, then it should reduce to a quadratic in the numerator and a linear denominator. But, then what?. Can anyone give a hint as to the best way to proceed?.",,"['real-analysis', 'algebra-precalculus', 'polynomials', 'roots']"
55,$x^{x^{x^{x^{x^{...}}}}} = 2$. Why is $-\sqrt{2}$ not a solution?,. Why is  not a solution?,x^{x^{x^{x^{x^{...}}}}} = 2 -\sqrt{2},"I just watched a video by blackpenredpen where he solved this equation. Here are the steps he took (The process transitions from one line to another. He didn't explicitly use an implies or iff sign, so I'll leave it as such): $x^{x^{x^{x^{x^{...}}}}} = 2$ $x^2 = 2$ $\left[  \begin{array}{l} x = \sqrt{2} \\ x = -\sqrt{2}  \end{array}  \right. $ At this point, he just crossed out the $-\sqrt{2}$ option. Could someone please explain why $-\sqrt{2}$ is not a solution? Thank you!","I just watched a video by blackpenredpen where he solved this equation. Here are the steps he took (The process transitions from one line to another. He didn't explicitly use an implies or iff sign, so I'll leave it as such): At this point, he just crossed out the option. Could someone please explain why is not a solution? Thank you!","x^{x^{x^{x^{x^{...}}}}} = 2 x^2 = 2 \left[ 
\begin{array}{l}
x = \sqrt{2} \\
x = -\sqrt{2} 
\end{array} 
\right.
 -\sqrt{2} -\sqrt{2}","['real-analysis', 'calculus', 'limits', 'radicals', 'power-towers']"
56,"If $\sum_{k=0}^{n-1}\frac{x^k}{k!}=\sum_{k=n}^{\infty}\frac{x^k}{k!}$ for large $n$, approximate $x$ in terms of $n$.","If  for large , approximate  in terms of .",\sum_{k=0}^{n-1}\frac{x^k}{k!}=\sum_{k=n}^{\infty}\frac{x^k}{k!} n x n,"Split the Maclaurin series for $e^x$ into two sub-series - the first $n$ terms, and the remainder - then equate the two sub-series. What is a good approximation for $x$ , for large $n$ ? Since $e^x=\sum_{k=0}^{\infty}\frac{x^k}{k!}$ , we have $\sum_{k=0}^{n-1}\frac{x^k}{k!}=\frac{1}{2}e^x$ . Experimenting with desmos, it seems that $x\approx n-\frac{1}{3}$ for even or odd $n$ , and $x\approx -0.28n-0.4$ for even $n$ . Other than that, I do not know how to approach this question. (Context: I was trying to answer this question , but I think the question in this post is interesting by itself.)","Split the Maclaurin series for into two sub-series - the first terms, and the remainder - then equate the two sub-series. What is a good approximation for , for large ? Since , we have . Experimenting with desmos, it seems that for even or odd , and for even . Other than that, I do not know how to approach this question. (Context: I was trying to answer this question , but I think the question in this post is interesting by itself.)",e^x n x n e^x=\sum_{k=0}^{\infty}\frac{x^k}{k!} \sum_{k=0}^{n-1}\frac{x^k}{k!}=\frac{1}{2}e^x x\approx n-\frac{1}{3} n x\approx -0.28n-0.4 n,"['real-analysis', 'limits', 'taylor-expansion', 'exponential-function', 'approximation']"
57,Calculate the sum $S_n = \sum\limits_{k=1}^{\infty}\left\lfloor \frac{n}{2^k} + \frac{1}{2}\right\rfloor $,Calculate the sum,S_n = \sum\limits_{k=1}^{\infty}\left\lfloor \frac{n}{2^k} + \frac{1}{2}\right\rfloor ,"I am doing tasks from Concrete Mathematics by Knuth, Graham, Patashnik for trainning, but there are a lot of really tricky sums like that: Calculate sum $$S_n = \sum_{k=1}^{\infty} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor  $$ My idea I had the idea to check when $$\frac{n}{2^k} < \frac{1}{2}$$ because then $$ \forall_{k_0 \le k} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor=0$$ It should be $$ k_0 = \log_2(2n) $$ but I don't know how it helps me with this task (because I need not only ""stop moment"" but also sum of considered elements Book idea Let $$S_n = \sum_{k=1}^{\infty} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor $$ then $$ S_n-S_{n-1} = 1$$ and then solve this recursion. But I write $S_n - S_{n-1}$ and  I don't see how it can be $1$ , especially that is an infinite sum.","I am doing tasks from Concrete Mathematics by Knuth, Graham, Patashnik for trainning, but there are a lot of really tricky sums like that: Calculate sum My idea I had the idea to check when because then It should be but I don't know how it helps me with this task (because I need not only ""stop moment"" but also sum of considered elements Book idea Let then and then solve this recursion. But I write and  I don't see how it can be , especially that is an infinite sum.",S_n = \sum_{k=1}^{\infty} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor   \frac{n}{2^k} < \frac{1}{2}  \forall_{k_0 \le k} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor=0  k_0 = \log_2(2n)  S_n = \sum_{k=1}^{\infty} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor   S_n-S_{n-1} = 1 S_n - S_{n-1} 1,"['real-analysis', 'discrete-mathematics']"
58,What is meant by $\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-1$ ? How to interpret it?,What is meant by  ? How to interpret it?,\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-1,"Let $F(x,y,z)=0$. So $x,y,z$ are defined implicitly in function of the other variable, i.e. $x=x(y,z)$, $y=y(x,z)$ and $z=z(x,y)$. Now $$dx=\frac{\partial x}{\partial y}dy+\frac{\partial x}{\partial z}dz=\frac{\partial x}{\partial y}\left(\frac{\partial y}{\partial x}dx+\frac{\partial y}{\partial z}dz\right)+\frac{\partial x}{\partial z}dz$$ and thus $$\left(\frac{\partial x}{\partial y}\frac{\partial y}{\partial x}-1\right)dx+\left(\frac{\partial x}{\partial z}+\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\right)dz=0.$$ Since $dx$ and $dy$ are linearly independent, we finally get \begin{align*} \frac{\partial x}{\partial y}\frac{\partial y}{\partial x}&=1 \tag{1}\\ \frac{\partial x}{\partial y}\frac{\partial y}{\partial z}&=-\frac{\partial x}{\partial z}\tag{2} \end{align*} Equation $(1)$ is natural, but equation $(2)$ should be $\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}=\frac{\partial x}{\partial z}\frac{\partial y}{\partial y}=\frac{\partial x}{\partial z},$ no ? So what's the matter here ? I know it's correct, but what does it mean exactly such a contradiction ? Because at the end I get $$\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-\frac{\partial x}{\partial z}\frac{\partial z}{\partial x}=-1$$ instead of $1$ (what should be expected). What is the mystery behind ? :)","Let $F(x,y,z)=0$. So $x,y,z$ are defined implicitly in function of the other variable, i.e. $x=x(y,z)$, $y=y(x,z)$ and $z=z(x,y)$. Now $$dx=\frac{\partial x}{\partial y}dy+\frac{\partial x}{\partial z}dz=\frac{\partial x}{\partial y}\left(\frac{\partial y}{\partial x}dx+\frac{\partial y}{\partial z}dz\right)+\frac{\partial x}{\partial z}dz$$ and thus $$\left(\frac{\partial x}{\partial y}\frac{\partial y}{\partial x}-1\right)dx+\left(\frac{\partial x}{\partial z}+\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\right)dz=0.$$ Since $dx$ and $dy$ are linearly independent, we finally get \begin{align*} \frac{\partial x}{\partial y}\frac{\partial y}{\partial x}&=1 \tag{1}\\ \frac{\partial x}{\partial y}\frac{\partial y}{\partial z}&=-\frac{\partial x}{\partial z}\tag{2} \end{align*} Equation $(1)$ is natural, but equation $(2)$ should be $\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}=\frac{\partial x}{\partial z}\frac{\partial y}{\partial y}=\frac{\partial x}{\partial z},$ no ? So what's the matter here ? I know it's correct, but what does it mean exactly such a contradiction ? Because at the end I get $$\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-\frac{\partial x}{\partial z}\frac{\partial z}{\partial x}=-1$$ instead of $1$ (what should be expected). What is the mystery behind ? :)",,"['real-analysis', 'derivatives', 'partial-derivative']"
59,How to prove sin(nx) has no pointwise convergent subsequence without prior knowledge of Lebesgue's Theory?,How to prove sin(nx) has no pointwise convergent subsequence without prior knowledge of Lebesgue's Theory?,,"In Baby Rudin 7.20 example, the author mentions to prove that the function sequence $$f_n(x):=\sin(nx) \qquad0(\leq x\leq 2\pi)$$has no pointwise convergent subsequence would be troublesome without Lebesgue's Theorem. Is there a proof that doesn't refer to Lebesgue's Theorem , and only requires the knowledge introduced in the first 7 chapter in Rudin?","In Baby Rudin 7.20 example, the author mentions to prove that the function sequence $$f_n(x):=\sin(nx) \qquad0(\leq x\leq 2\pi)$$has no pointwise convergent subsequence would be troublesome without Lebesgue's Theorem. Is there a proof that doesn't refer to Lebesgue's Theorem , and only requires the knowledge introduced in the first 7 chapter in Rudin?",,['real-analysis']
60,Find $\lim\limits_{n \to \infty} \frac{\sqrt 1 + \sqrt 2 + \dots + \sqrt{n}}{n\sqrt{n}}$.,Find .,\lim\limits_{n \to \infty} \frac{\sqrt 1 + \sqrt 2 + \dots + \sqrt{n}}{n\sqrt{n}},"$$\lim_{n \to \infty} \dfrac{\sqrt 1 + \sqrt 2 + \dots + \sqrt{n}}{n\sqrt{n}}$$ $$\lim_{n \to \infty} \dfrac{\sqrt 1 + \sqrt 2 + \dots + \sqrt{n}}{n\sqrt{n}} =\lim_{n\to \infty} \dfrac1{n}\sum^{n}_{k = 1} \sqrt{\dfrac k n} $$ While searching this question I found, Turning infinite sum into integral . Like in the accepted answer I first compared my series to LRAM, $$\int_a^b f(x)\,dx=\lim_{n\to\infty}\frac 1n\sum_{i=1}^n f\left(a+\frac{b-a}n i \right)$$ I got $a = 0$, $b = 1$ and $f(x) =\sqrt{x}$ so, $$\int_0^1 \sqrt{x}\ dx = \dfrac2 3$$ should be the answer. Is there any simpler method to do this sum ? I have not learnt this method to do infinite sums so I can't use it.","$$\lim_{n \to \infty} \dfrac{\sqrt 1 + \sqrt 2 + \dots + \sqrt{n}}{n\sqrt{n}}$$ $$\lim_{n \to \infty} \dfrac{\sqrt 1 + \sqrt 2 + \dots + \sqrt{n}}{n\sqrt{n}} =\lim_{n\to \infty} \dfrac1{n}\sum^{n}_{k = 1} \sqrt{\dfrac k n} $$ While searching this question I found, Turning infinite sum into integral . Like in the accepted answer I first compared my series to LRAM, $$\int_a^b f(x)\,dx=\lim_{n\to\infty}\frac 1n\sum_{i=1}^n f\left(a+\frac{b-a}n i \right)$$ I got $a = 0$, $b = 1$ and $f(x) =\sqrt{x}$ so, $$\int_0^1 \sqrt{x}\ dx = \dfrac2 3$$ should be the answer. Is there any simpler method to do this sum ? I have not learnt this method to do infinite sums so I can't use it.",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
61,How to solve the matrix minimization for BFGS update in Quasi-Newton optimization,How to solve the matrix minimization for BFGS update in Quasi-Newton optimization,,"I am interested in deriving the unique solution to the following Quasi-Newton minimization problem $$  \min_{H}||  H-H_k||\\    H=H^\top,\quad Hy_k =s_k $$ The norm is the weighted Frobenius norm  $$ ||A||_W \equiv ||W^{1/2}A W^{1/2}||_F $$where the weight matrix $W$ is any positive definite matrix that satisfies $Ws_k=y_k$, and $||\cdot||_F$ is defined by $||C||^2_F= \sum_{i,j}^n c^2_{ij}$. The quantity $H$ is the inverse hessian which is symmetric, positive definite and satisfies the secant equation above, $Hy_k=s_k$.  We can assume that $W=G^{-1}_k $ where $G_k$ the average hessian is given by $$ G_k= \int_0^1 \nabla^2 f(x_k+\tau \alpha_k p_k)d\tau $$ The unique solution is given by $$ H_{k+1} = (1-\rho_k s_k y^\top_k)H_k (1-\rho_k y_k s^\top_k)+ \rho_k s_k s^\top_k $$ where $\rho_k = (y^\top_k s_k)^{-1}$.  Note, this is an iterative scheme where $k$ represents the current iteration and $H_{k+1}$ is an approximation to the inverse hessian. The notation I am using is directly from the book Nocedal & Wright - Numerical Optimization.  I am not able to find a full derivation of this anywhere, everything written above is all that Nocedal/Wright has in regards to this topic. For a reference, this is in chapter 6 - Quasi Newton Methods, of their newest/2nd edition. All links I have tried to google and other books also have no full derivation.  I am not able to find anything more thorough than Nocedal & Wright however they still don't have a derivation.  Thanks","I am interested in deriving the unique solution to the following Quasi-Newton minimization problem $$  \min_{H}||  H-H_k||\\    H=H^\top,\quad Hy_k =s_k $$ The norm is the weighted Frobenius norm  $$ ||A||_W \equiv ||W^{1/2}A W^{1/2}||_F $$where the weight matrix $W$ is any positive definite matrix that satisfies $Ws_k=y_k$, and $||\cdot||_F$ is defined by $||C||^2_F= \sum_{i,j}^n c^2_{ij}$. The quantity $H$ is the inverse hessian which is symmetric, positive definite and satisfies the secant equation above, $Hy_k=s_k$.  We can assume that $W=G^{-1}_k $ where $G_k$ the average hessian is given by $$ G_k= \int_0^1 \nabla^2 f(x_k+\tau \alpha_k p_k)d\tau $$ The unique solution is given by $$ H_{k+1} = (1-\rho_k s_k y^\top_k)H_k (1-\rho_k y_k s^\top_k)+ \rho_k s_k s^\top_k $$ where $\rho_k = (y^\top_k s_k)^{-1}$.  Note, this is an iterative scheme where $k$ represents the current iteration and $H_{k+1}$ is an approximation to the inverse hessian. The notation I am using is directly from the book Nocedal & Wright - Numerical Optimization.  I am not able to find a full derivation of this anywhere, everything written above is all that Nocedal/Wright has in regards to this topic. For a reference, this is in chapter 6 - Quasi Newton Methods, of their newest/2nd edition. All links I have tried to google and other books also have no full derivation.  I am not able to find anything more thorough than Nocedal & Wright however they still don't have a derivation.  Thanks",,"['calculus', 'real-analysis', 'optimization', 'numerical-methods', 'algorithms']"
62,$x_{n+m}\le \frac{x_n+x_{n+1}+\cdots+x_{n+m-1}}{m}$. Prove that this sequence has a limit.,. Prove that this sequence has a limit.,x_{n+m}\le \frac{x_n+x_{n+1}+\cdots+x_{n+m-1}}{m},Let $m \ge 2 -$ fixed positive integer. The sequence of non-negative real numbers $\{x_n\}_{n=1}^{\infty}$ is that for all $n\in \mathbb N$   $$x_{n+m}\le \frac{x_n+x_{n+1}+\cdots+x_{n+m-1}}{m}$$   Prove that this sequence has a limit. I know I need to prove 1) the sequence is monotone; 2) the sequence is limited. But I can not do any of these items.,Let $m \ge 2 -$ fixed positive integer. The sequence of non-negative real numbers $\{x_n\}_{n=1}^{\infty}$ is that for all $n\in \mathbb N$   $$x_{n+m}\le \frac{x_n+x_{n+1}+\cdots+x_{n+m-1}}{m}$$   Prove that this sequence has a limit. I know I need to prove 1) the sequence is monotone; 2) the sequence is limited. But I can not do any of these items.,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
63,Is every complete metric space closed?,Is every complete metric space closed?,,"I know that if $A\subset X$ where $X$ is a complede metric space, and $A$ is closed $\iff$ it's complete. However is every metric space closed? E.g., can I take $X\subset X$ and since $X$ is complete, can I conclude it's closed?","I know that if $A\subset X$ where $X$ is a complede metric space, and $A$ is closed $\iff$ it's complete. However is every metric space closed? E.g., can I take $X\subset X$ and since $X$ is complete, can I conclude it's closed?",,"['real-analysis', 'metric-spaces']"
64,Convergence of $\sum\limits_{n=1}^{\infty}\left(\frac{\sqrt n-1}{\sqrt n}\right)^n$,Convergence of,\sum\limits_{n=1}^{\infty}\left(\frac{\sqrt n-1}{\sqrt n}\right)^n,"$$\sum_{n=1}^{\infty}\left(\frac{\sqrt n-1}{\sqrt n}\right)^n$$ It kind of looks like the Euler's number limit, but I didn't succeed in proving that it converges. Anyone? This was an exercise at the end of a chapter on sequences and series, much before Taylor series and integrals, so I don't think it would be ""fair"" to use those.","$$\sum_{n=1}^{\infty}\left(\frac{\sqrt n-1}{\sqrt n}\right)^n$$ It kind of looks like the Euler's number limit, but I didn't succeed in proving that it converges. Anyone? This was an exercise at the end of a chapter on sequences and series, much before Taylor series and integrals, so I don't think it would be ""fair"" to use those.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
65,integral inequality $\int_0^a \left(\frac{f(x)}{2x}\right)^2 dx \le \int_0^a (f'(x))^2 dx$,integral inequality,\int_0^a \left(\frac{f(x)}{2x}\right)^2 dx \le \int_0^a (f'(x))^2 dx,"Let $f:[0,a]\rightarrow\mathbb{R}$ be continuous differentiable function satisfying $f(0)=0$. Prove the following inequality $$\int_0^a \left(\frac{f(x)}{2x}\right)^2 dx \le \int_0^a (f'(x))^2 dx$$","Let $f:[0,a]\rightarrow\mathbb{R}$ be continuous differentiable function satisfying $f(0)=0$. Prove the following inequality $$\int_0^a \left(\frac{f(x)}{2x}\right)^2 dx \le \int_0^a (f'(x))^2 dx$$",,"['real-analysis', 'inequality', 'contest-math']"
66,Find the value $\sum_{n=1}^{\infty}(e-(1+\frac{1}{n})^n)$,Find the value,\sum_{n=1}^{\infty}(e-(1+\frac{1}{n})^n),How to find the following series' value? $$\sum_{n=1}^{\infty}\bigg(e-\Big(1+\frac{1}{n}\Big)^n\bigg)$$,How to find the following series' value? $$\sum_{n=1}^{\infty}\bigg(e-\Big(1+\frac{1}{n}\Big)^n\bigg)$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
67,Two equivalent definitions of almost sure convergence of random variables.,Two equivalent definitions of almost sure convergence of random variables.,,"Let $\{X_n\}_{n=1,2,\cdots}$ is a sequence of random variables. There are equivalent definitions of almost sure convergence of random variables. How can one prove the equivalence? $\mathbb{P}[\omega:\lim_{n\to\infty}X_n(\omega) = X(\omega)] = 1 \Leftrightarrow \lim_{n\to\infty}\mathbb{P}[\omega:\sup_{k>n}|X_k(\omega) - X(\omega)|>\epsilon] = 0$",Let is a sequence of random variables. There are equivalent definitions of almost sure convergence of random variables. How can one prove the equivalence?,"\{X_n\}_{n=1,2,\cdots} \mathbb{P}[\omega:\lim_{n\to\infty}X_n(\omega) = X(\omega)] = 1 \Leftrightarrow \lim_{n\to\infty}\mathbb{P}[\omega:\sup_{k>n}|X_k(\omega) - X(\omega)|>\epsilon] = 0","['real-analysis', 'probability', 'convergence-divergence']"
68,"Computing $\lim\limits_{n\to\infty} \int_{0}^{\infty}\frac{\sin(x/n)}{(1+x/n)^{n}}\, dx$",Computing,"\lim\limits_{n\to\infty} \int_{0}^{\infty}\frac{\sin(x/n)}{(1+x/n)^{n}}\, dx","$$\lim_{n\to\infty} \int_{0}^{\infty}\frac{\sin(x/n)}{(1+x/n)^{n}}\, dx$$ I've been able to show that the integral is bounded above by 1 (several ways). One of the simplest is just letting $u=x/n$ and doing some bounding above. It must converge to something (if it is monotone increasing.. don't think so), but all my efforts to use Lebesgue Dominated Convergence have failed. I can't seem to dominate this function... However, if I could dominate it the integrand converges pointwise to 0 and so then the integral would be 0, which I think is incorrect. I have found it sufficient to deal with  $$\lim_{n\to\infty}\int_{0}^{n}\frac{\sin(x/n)}{(1+x/n)^{n}}\, dx$$ I tried to turn it into a series and use the converse of the integral test, but that hasn't went well. I've used Egoroff's theorem to try to use uniform convergence but that failed. Some of the inequalities and ideas I've tried to use in a solution include: $sin(x/n)\leq x/n$ after a $u$ sub I've tried to turn it into a riemann sum... failed $(1+x/n)^{n}\geq 1+x$ and the binomial theorem $(1+x/n)^{n}$ converges to $e^x$ and it is increasing in $n$ so $e^{-x}\leq \frac{1}{(1+x/n)^{n}}$ I've tried to use that the integrand is measurable and subtract out different measurable sets from the integral to make it more convenient.. I've been playing with this on and off for a month or so and no luck. I appreciate any help here. Update: From mixedmath's suggestion splitting it into $[0,10]$ and $[10,\infty]$ it is clear that the integrand is dominated on $[0,10]$ (say by 1) and so lebesgue sends that portion to 0. On the larger interval I am still running into a problem making it bounded nicely. If I try a $u$ sub the bounds get messy. I would like to choose an $n$ so that $2e^{-x} \leq (1+x/n)^n$ for all $x\in [10,\infty]$ .. maybe: there exists an $N$ such that for all $n\geq N$ $e^{10}/2 < (1+10/n)^{n}$ but for $x>10$.. $e^{10}/2<(1+10/n)^{n}<(1+x/n)^{n}$ but that's not quite right..","$$\lim_{n\to\infty} \int_{0}^{\infty}\frac{\sin(x/n)}{(1+x/n)^{n}}\, dx$$ I've been able to show that the integral is bounded above by 1 (several ways). One of the simplest is just letting $u=x/n$ and doing some bounding above. It must converge to something (if it is monotone increasing.. don't think so), but all my efforts to use Lebesgue Dominated Convergence have failed. I can't seem to dominate this function... However, if I could dominate it the integrand converges pointwise to 0 and so then the integral would be 0, which I think is incorrect. I have found it sufficient to deal with  $$\lim_{n\to\infty}\int_{0}^{n}\frac{\sin(x/n)}{(1+x/n)^{n}}\, dx$$ I tried to turn it into a series and use the converse of the integral test, but that hasn't went well. I've used Egoroff's theorem to try to use uniform convergence but that failed. Some of the inequalities and ideas I've tried to use in a solution include: $sin(x/n)\leq x/n$ after a $u$ sub I've tried to turn it into a riemann sum... failed $(1+x/n)^{n}\geq 1+x$ and the binomial theorem $(1+x/n)^{n}$ converges to $e^x$ and it is increasing in $n$ so $e^{-x}\leq \frac{1}{(1+x/n)^{n}}$ I've tried to use that the integrand is measurable and subtract out different measurable sets from the integral to make it more convenient.. I've been playing with this on and off for a month or so and no luck. I appreciate any help here. Update: From mixedmath's suggestion splitting it into $[0,10]$ and $[10,\infty]$ it is clear that the integrand is dominated on $[0,10]$ (say by 1) and so lebesgue sends that portion to 0. On the larger interval I am still running into a problem making it bounded nicely. If I try a $u$ sub the bounds get messy. I would like to choose an $n$ so that $2e^{-x} \leq (1+x/n)^n$ for all $x\in [10,\infty]$ .. maybe: there exists an $N$ such that for all $n\geq N$ $e^{10}/2 < (1+10/n)^{n}$ but for $x>10$.. $e^{10}/2<(1+10/n)^{n}<(1+x/n)^{n}$ but that's not quite right..",,"['real-analysis', 'limits', 'integration']"
69,Limit of positive sequence $(f_n)$ defined by $f_n(x)^2=\int^x_0 f_{n-1}(t)\mathrm{d}t$,Limit of positive sequence  defined by,(f_n) f_n(x)^2=\int^x_0 f_{n-1}(t)\mathrm{d}t,"Assume $f_{0}(x)$ is integrable in $[0,1]$, $f_{0}(x)>0$. $$f_n(x)=\sqrt{\int^x_0 f_{n-1}(t)\mathrm{d}t}, \quad n=1,2,...$$ How can one calculate $\lim\limits_{n \to {\infty}}f_n(x)$?","Assume $f_{0}(x)$ is integrable in $[0,1]$, $f_{0}(x)>0$. $$f_n(x)=\sqrt{\int^x_0 f_{n-1}(t)\mathrm{d}t}, \quad n=1,2,...$$ How can one calculate $\lim\limits_{n \to {\infty}}f_n(x)$?",,"['calculus', 'real-analysis']"
70,Increasing function on R that is discontinuous on the rationals,Increasing function on R that is discontinuous on the rationals,,"The question (Folland's Real Analysis, 3.5.30) asks to produce an increasing function on $\mathbb{R}$ whose set of discontinuities is the rationals. My train of thought is as follows: Let $f_0$ be the identity. We want to create a jump at every rational number, while keeping the function increasing. Create $f_1$ by cutting $f_0$ at each integer, and halving the slope of each resulting line segment, fixing the left endpoints. This results in a sort of slanted stair that is still increasing and discontinuous exactly at the integers. Given $f_{n-1}$ , we create $f_n$ by cutting each segment of $f_{n-1}$ into $n$ parts and halving the slope of each resulting segment, again fixing the left endpoints. Note: this is equivalent to making cuts at all rational points $\frac{m}{n!}$ at the $n^\text{th}$ step, with $m$ and $n$ not necessarily coprime. The limit of such sequence of functions exists. It's easy to prove that any rational point is eventually a left endpoint, and is thus kept fixed by all successive functions in the sequence. For an irrational point $x$ we can make two sequences $a_n$ and $b_n$ of rationals converging from the left and right respectively, such that $a_n=\frac{m}{n!}<x$ and $b_n=\frac{m+1}{n!}>x$ . Given $\epsilon$ , we can then choose $N$ large enough with $a_N$ and $b_N$ on the same segment of $f_{N-1}$ (so the jump from $f_{N-1}(a_N)$ to $f_{N-1}(b_N)$ is small), with this jump being less than $\epsilon$ . Since we can pinpoint the value of $f_n(x)$ between arbitrarily close values, the limit $f(x)=\lim_n f_n(x)$ exists. It's clear that the function is discontinuous at all rationals since it is constructed to be, and it is increasing since each $f_n$ is increasing. Can I say that $f$ is continuous at each irrational? My gut feeling is that, since any $\delta$ -ball around an irrational contains a rational to the left and one to the right, we cannot necessarily say $f$ is continuous there. However we know by theorem 3.23 in the book that the set of discontinuities of an increasing function is countable. Is there a contradiction somewhere? If so, can the construction be tweaked or is a different construction necessary? Thank you.","The question (Folland's Real Analysis, 3.5.30) asks to produce an increasing function on whose set of discontinuities is the rationals. My train of thought is as follows: Let be the identity. We want to create a jump at every rational number, while keeping the function increasing. Create by cutting at each integer, and halving the slope of each resulting line segment, fixing the left endpoints. This results in a sort of slanted stair that is still increasing and discontinuous exactly at the integers. Given , we create by cutting each segment of into parts and halving the slope of each resulting segment, again fixing the left endpoints. Note: this is equivalent to making cuts at all rational points at the step, with and not necessarily coprime. The limit of such sequence of functions exists. It's easy to prove that any rational point is eventually a left endpoint, and is thus kept fixed by all successive functions in the sequence. For an irrational point we can make two sequences and of rationals converging from the left and right respectively, such that and . Given , we can then choose large enough with and on the same segment of (so the jump from to is small), with this jump being less than . Since we can pinpoint the value of between arbitrarily close values, the limit exists. It's clear that the function is discontinuous at all rationals since it is constructed to be, and it is increasing since each is increasing. Can I say that is continuous at each irrational? My gut feeling is that, since any -ball around an irrational contains a rational to the left and one to the right, we cannot necessarily say is continuous there. However we know by theorem 3.23 in the book that the set of discontinuities of an increasing function is countable. Is there a contradiction somewhere? If so, can the construction be tweaked or is a different construction necessary? Thank you.",\mathbb{R} f_0 f_1 f_0 f_{n-1} f_n f_{n-1} n \frac{m}{n!} n^\text{th} m n x a_n b_n a_n=\frac{m}{n!}<x b_n=\frac{m+1}{n!}>x \epsilon N a_N b_N f_{N-1} f_{N-1}(a_N) f_{N-1}(b_N) \epsilon f_n(x) f(x)=\lim_n f_n(x) f_n f \delta f,['real-analysis']
71,How to prove that: $19.999<e^\pi-\pi<20$?,How to prove that: ?,19.999<e^\pi-\pi<20,"I would like to know how to prove    $$e^\pi-\pi\sim 20.$$   More precisely, I want to show by using only mathematical tools that,    $$19.999<e^\pi-\pi<20$$ I have checked with online  calculator and I got  $$e^\pi-\pi\approx19.9990999792\sim 20.$$ I tried to use the Tyalor expansion for exponential $$ e^\pi =\sum_{n=0}^{\infty} \frac{\pi^n}{n!} = \pi +1+\sum_{n=2}^{\infty} \frac{\pi^n}{n!}$$ then, $$e^\pi -\pi =1+\sum_{n=2}^{\infty} \frac{\pi^n}{n!}$$ which is not easy to continue from here, since the factor $\pi^n$ is involved. Any idea?","I would like to know how to prove    $$e^\pi-\pi\sim 20.$$   More precisely, I want to show by using only mathematical tools that,    $$19.999<e^\pi-\pi<20$$ I have checked with online  calculator and I got  $$e^\pi-\pi\approx19.9990999792\sim 20.$$ I tried to use the Tyalor expansion for exponential $$ e^\pi =\sum_{n=0}^{\infty} \frac{\pi^n}{n!} = \pi +1+\sum_{n=2}^{\infty} \frac{\pi^n}{n!}$$ then, $$e^\pi -\pi =1+\sum_{n=2}^{\infty} \frac{\pi^n}{n!}$$ which is not easy to continue from here, since the factor $\pi^n$ is involved. Any idea?",,"['real-analysis', 'taylor-expansion', 'exponential-function', 'approximation', 'real-numbers']"
72,Can any continuous function be represented as an infinite polynomial?,Can any continuous function be represented as an infinite polynomial?,,Can any continuous function be represented as an infinite polynomial? Motivation: the antiderivative $ \int^\ e^{-x^2}dx\ $ can be expressed as an infinite polynomial(write Taylor series for integrand function and integrate) but this antiderivative has no closed/elementary form expression according to Liouville's theorem but is clearly continuous. So are the rest of the non-elementary functions expressible as infinite polynomials? Fascinating.Any insights on how to proceed????,Can any continuous function be represented as an infinite polynomial? Motivation: the antiderivative $ \int^\ e^{-x^2}dx\ $ can be expressed as an infinite polynomial(write Taylor series for integrand function and integrate) but this antiderivative has no closed/elementary form expression according to Liouville's theorem but is clearly continuous. So are the rest of the non-elementary functions expressible as infinite polynomials? Fascinating.Any insights on how to proceed????,,"['real-analysis', 'integration', 'sequences-and-series', 'special-functions', 'elementary-functions']"
73,A function that is measurable but not Lebesgue integrable.,A function that is measurable but not Lebesgue integrable.,,"There is a theorem in our textbook that says, ""Let $f$ be a bounded function on a set of finite measure $E$. Then $f$ is Lebesgue integrable over $E$ if and only if it is measurable."" So I was wondering about an example of a function that was Lebesgue integrable but not measurable. I tried to search for some examples online but couldn't really find anything useful...","There is a theorem in our textbook that says, ""Let $f$ be a bounded function on a set of finite measure $E$. Then $f$ is Lebesgue integrable over $E$ if and only if it is measurable."" So I was wondering about an example of a function that was Lebesgue integrable but not measurable. I tried to search for some examples online but couldn't really find anything useful...",,['real-analysis']
74,Methods to evaluate $ \int _{a }^{b }\!{\frac {\ln \left( tx + u \right) }{m{x}^{2}+nx +p}}{dx} $,Methods to evaluate, \int _{a }^{b }\!{\frac {\ln \left( tx + u \right) }{m{x}^{2}+nx +p}}{dx} ,"Today I saw a question with an answer that made me rethink of the following question, since it's not the first time I try to find an answer to it. If you look at the answer of Mhenni Benghorbal here you'll see $2$ interesting integrals, namely: $$  \int _{0 }^{\infty }\!{\frac {\ln  \left( u \right) }{2+{u}^{2}- 2\,u}}{du} ; \int _{0}^{\infty }\!{\frac {\ln  \left( z \right) }{2+{z}^{2}+2\,z}}dz $$ I try to find out if  there is a well defined strategy to tackle such integrals.   In a more general sense, we have to deal with: $$  \int _{a }^{b }\!{\frac {\ln  \left( tx + u \right) }{m{x}^{2}+nx +p}}{dx}   $$ Could you help here? Thanks.","Today I saw a question with an answer that made me rethink of the following question, since it's not the first time I try to find an answer to it. If you look at the answer of Mhenni Benghorbal here you'll see $2$ interesting integrals, namely: $$  \int _{0 }^{\infty }\!{\frac {\ln  \left( u \right) }{2+{u}^{2}- 2\,u}}{du} ; \int _{0}^{\infty }\!{\frac {\ln  \left( z \right) }{2+{z}^{2}+2\,z}}dz $$ I try to find out if  there is a well defined strategy to tackle such integrals.   In a more general sense, we have to deal with: $$  \int _{a }^{b }\!{\frac {\ln  \left( tx + u \right) }{m{x}^{2}+nx +p}}{dx}   $$ Could you help here? Thanks.",,"['calculus', 'real-analysis', 'integration']"
75,Hausdorff dimension of Cantor set,Hausdorff dimension of Cantor set,,"I know this is probably a easy question, but some steps in the proofs I found almost everywhere contained some parts or assumptions which I think may not be that trivial, so I would like to make it rigorous and clear enough. Here is the question: Let $C$ be the Cantor set with delete the middle third of the interval and go on. The general Cantor can be considered similarly. We want to proof the Hausdorff dimension of $C$ is $\alpha:=\log2/\log3$. So we calculate the $d$-dimensional Hausdorff measure $H^d(C)$ for all $d$ to determine the Hausdorff dimension. Let $C(k)$ be the collection of $2^k$ intervals with length $1/3^k$ in the $k^{th}$-step of construction of Cantor set. It is rather easy to show that $H^{\alpha}(C)<\infty$ by showing that for any covering $\{E_j\}_{j=1}^{\infty}$of $C$ the set $C(k)$ also covers $C$ for $k$ large enough, so we can bound $H^{\alpha}(C)$ from above. Which implies that the Hausdorff dimension of $C$ is less than $\alpha$. To show the dimension is actually equal to $\alpha$, it suffices to show $H^{\alpha}(C)>0$. Now let $\{E_j\}_{j=1}^{\infty}$ be any covering of $C$ with diameter $diam(E_j)\le \delta$ for all $j$. How do we show that  $$\sum_j diam(E_j)^{\alpha}>constant$$ One author (see this link ) made the following assumption: $E_j$ be open, so one can find the Lebesgue number of this covering, and when $k$ large enough, any interval in $C(k)$ will be contained in $E_j$ for some $j$. Hence one can bound the $\sum_j diam(E_j)^{\alpha}$ from below by the ones of $C(k)$. I got confused here: First why we can assume $E_j$ to be open?","I know this is probably a easy question, but some steps in the proofs I found almost everywhere contained some parts or assumptions which I think may not be that trivial, so I would like to make it rigorous and clear enough. Here is the question: Let $C$ be the Cantor set with delete the middle third of the interval and go on. The general Cantor can be considered similarly. We want to proof the Hausdorff dimension of $C$ is $\alpha:=\log2/\log3$. So we calculate the $d$-dimensional Hausdorff measure $H^d(C)$ for all $d$ to determine the Hausdorff dimension. Let $C(k)$ be the collection of $2^k$ intervals with length $1/3^k$ in the $k^{th}$-step of construction of Cantor set. It is rather easy to show that $H^{\alpha}(C)<\infty$ by showing that for any covering $\{E_j\}_{j=1}^{\infty}$of $C$ the set $C(k)$ also covers $C$ for $k$ large enough, so we can bound $H^{\alpha}(C)$ from above. Which implies that the Hausdorff dimension of $C$ is less than $\alpha$. To show the dimension is actually equal to $\alpha$, it suffices to show $H^{\alpha}(C)>0$. Now let $\{E_j\}_{j=1}^{\infty}$ be any covering of $C$ with diameter $diam(E_j)\le \delta$ for all $j$. How do we show that  $$\sum_j diam(E_j)^{\alpha}>constant$$ One author (see this link ) made the following assumption: $E_j$ be open, so one can find the Lebesgue number of this covering, and when $k$ large enough, any interval in $C(k)$ will be contained in $E_j$ for some $j$. Hence one can bound the $\sum_j diam(E_j)^{\alpha}$ from below by the ones of $C(k)$. I got confused here: First why we can assume $E_j$ to be open?",,['real-analysis']
76,"If $g^{-1} \circ f \circ g$ is $C^\infty$ whenever $f$ is $C^\infty$, must $g$ be $C^\infty$?","If  is  whenever  is , must  be ?",g^{-1} \circ f \circ g C^\infty f C^\infty g C^\infty,"Suppose that $g$ is a bijection on the real line, and $g^{-1} \circ f \circ g$ is a $C^\infty$ function whenever $f$ is $C^\infty$.  It seems howlingly obvious that this can only happen if $g$ is itself $C^\infty$.  But I can't figure out how to prove it. Can you help me, Internets? (Context: I want to show that in a manifold of dimension at least 2, the facts about which unparameterized curves are smooth suffice to determine the differential structure.  I think I can sort of see how to prove this provided I can rely on the claim above, but that's where I'm stuck.  However, if anyone here knows of a proof of the geometric claim, pointers would be most welcome.)","Suppose that $g$ is a bijection on the real line, and $g^{-1} \circ f \circ g$ is a $C^\infty$ function whenever $f$ is $C^\infty$.  It seems howlingly obvious that this can only happen if $g$ is itself $C^\infty$.  But I can't figure out how to prove it. Can you help me, Internets? (Context: I want to show that in a manifold of dimension at least 2, the facts about which unparameterized curves are smooth suffice to determine the differential structure.  I think I can sort of see how to prove this provided I can rely on the claim above, but that's where I'm stuck.  However, if anyone here knows of a proof of the geometric claim, pointers would be most welcome.)",,"['real-analysis', 'differential-geometry']"
77,Convergence of $\sum\limits_{n=1}^{\infty} \frac{1}{nf(n)}$,Convergence of,\sum\limits_{n=1}^{\infty} \frac{1}{nf(n)},"This problem is taken from Vojtěch Jarník International Mathematical Competition 2010, Category I, Problem 1 . — edit by KennyTM On going through this post Does there exist a bijective $f:\mathbb{N} \to \mathbb{N}$ such that $\sum f(n)/n^2$ converges? i happened to get the following 2 problems into my mind: Let $f: \mathbb{N} \to \mathbb{N}$ be a bijection. Then does the series $$\sum\limits_{n=1}^{\infty} \frac{1}{nf(n)}$$ converge? Next, consider the series $$\sum\limits_{n=1}^{\infty} \frac{1}{n+f(n)}$$ where $f: \mathbb{N} \to \mathbb{N}$ is a bijection. Clearly by taking $f(n)=n$ we see that the series is divergent. Does there exist a bijection such that the sum above is convergent?","This problem is taken from Vojtěch Jarník International Mathematical Competition 2010, Category I, Problem 1 . — edit by KennyTM On going through this post Does there exist a bijective $f:\mathbb{N} \to \mathbb{N}$ such that $\sum f(n)/n^2$ converges? i happened to get the following 2 problems into my mind: Let $f: \mathbb{N} \to \mathbb{N}$ be a bijection. Then does the series $$\sum\limits_{n=1}^{\infty} \frac{1}{nf(n)}$$ converge? Next, consider the series $$\sum\limits_{n=1}^{\infty} \frac{1}{n+f(n)}$$ where $f: \mathbb{N} \to \mathbb{N}$ is a bijection. Clearly by taking $f(n)=n$ we see that the series is divergent. Does there exist a bijection such that the sum above is convergent?",,['real-analysis']
78,How can you approach $\int_0^{\pi/2} x\frac{\ln(\cos x)}{\sin x}dx$,How can you approach,\int_0^{\pi/2} x\frac{\ln(\cos x)}{\sin x}dx,"Here is a new challenging problem : Show that $$I=\int_0^{\pi/2} x\frac{\ln(\cos x)}{\sin x}dx=2\ln(2)G-\frac{\pi}{8}\ln^2(2)-\frac{5\pi^3}{32}+4\Im\left\{\text{Li}_3\left(\frac{1+i}{2}\right)\right\}$$ My attempt : With Weierstrass substitution we have $$I=2\int_0^1\frac{\arctan x}{x}\ln\left(\frac{1-x^2}{1+x^2}\right)dx\overset{x\to \frac{1-x}{1+x}}{=}4\int_0^1\frac{\frac{\pi}{4}-\arctan x}{1-x^2}\ln\left(\frac{2x}{1+x^2}\right)dx$$ $$=\pi\underbrace{\int_0^1\frac{1}{1-x^2}\ln\left(\frac{2x}{1+x^2}\right)dx}_{I_1}-4\underbrace{\int_0^1\frac{\arctan x}{1-x^2}\ln\left(\frac{2x}{1+x^2}\right)dx}_{I_2}$$ By setting $x\to \frac{1-x}{1+x}$ in the first integral we have $$I_1=\frac12\int_0^1\frac{1}{x}\ln\left(\frac{1-x^2}{1+x^2}\right)dx$$ $$=\frac14\int_0^1\frac{1}{x}\ln\left(\frac{1-x}{1+x}\right)dx=\frac14\left[-\text{Li}_2(x)+\text{Li}_2(-x)\right]_0^1=-\frac38\zeta(2)$$ For the second integral, write $\frac{1}{1-x^2}=\frac{1}{2(1-x)}+\frac{1}{2(1+x)}$ $$I_2=\frac12\int_0^1\frac{\arctan x}{1-x}\ln\left(\frac{2x}{1+x^2}\right)dx+\frac12\int_0^1\frac{\arctan x}{1+x}\ln\left(\frac{2x}{1+x^2}\right)dx$$ The first integral is very similar to this one $$\int_0^1\frac{\arctan\left(x\right)}{1-x}\, \ln\left(\frac{2x^2}{1+x^2}\right)\,\mathrm{d}x = -\frac{\pi}{16}\ln^{2}\left(2\right) - \frac{11}{192}\,\pi^{3} + 2\Im\left\{% \text{Li}_{3}\left(\frac{1 + \mathrm{i}}{2}\right)\right\}$$ So we are left with only $\int_0^1\frac{\arctan x\ln(1+x^2)}{1+x}dx$ as $\int_0^1\frac{\arctan x\ln x}{1+x}dx$ is already nicely calculated by FDP here . Any idea? I noticed that if we use $x\to\frac{1-x}{1+x}$ in $\int_0^1\frac{\arctan x\ln(1+x^2)}{1+x}dx$ we will have a nice symmerty but still some annoying integrals appear. In $I$ , I also tried the Fourier series of $\ln(\cos x)$ but I stopped at $\int_0^{\pi/2} \frac{x\cos(2nx)}{\sin x}dx$ . I would like to see different approaches if possible. Thank you.","Here is a new challenging problem : Show that My attempt : With Weierstrass substitution we have By setting in the first integral we have For the second integral, write The first integral is very similar to this one So we are left with only as is already nicely calculated by FDP here . Any idea? I noticed that if we use in we will have a nice symmerty but still some annoying integrals appear. In , I also tried the Fourier series of but I stopped at . I would like to see different approaches if possible. Thank you.","I=\int_0^{\pi/2} x\frac{\ln(\cos x)}{\sin x}dx=2\ln(2)G-\frac{\pi}{8}\ln^2(2)-\frac{5\pi^3}{32}+4\Im\left\{\text{Li}_3\left(\frac{1+i}{2}\right)\right\} I=2\int_0^1\frac{\arctan x}{x}\ln\left(\frac{1-x^2}{1+x^2}\right)dx\overset{x\to \frac{1-x}{1+x}}{=}4\int_0^1\frac{\frac{\pi}{4}-\arctan x}{1-x^2}\ln\left(\frac{2x}{1+x^2}\right)dx =\pi\underbrace{\int_0^1\frac{1}{1-x^2}\ln\left(\frac{2x}{1+x^2}\right)dx}_{I_1}-4\underbrace{\int_0^1\frac{\arctan x}{1-x^2}\ln\left(\frac{2x}{1+x^2}\right)dx}_{I_2} x\to \frac{1-x}{1+x} I_1=\frac12\int_0^1\frac{1}{x}\ln\left(\frac{1-x^2}{1+x^2}\right)dx =\frac14\int_0^1\frac{1}{x}\ln\left(\frac{1-x}{1+x}\right)dx=\frac14\left[-\text{Li}_2(x)+\text{Li}_2(-x)\right]_0^1=-\frac38\zeta(2) \frac{1}{1-x^2}=\frac{1}{2(1-x)}+\frac{1}{2(1+x)} I_2=\frac12\int_0^1\frac{\arctan x}{1-x}\ln\left(\frac{2x}{1+x^2}\right)dx+\frac12\int_0^1\frac{\arctan x}{1+x}\ln\left(\frac{2x}{1+x^2}\right)dx \int_0^1\frac{\arctan\left(x\right)}{1-x}\,
\ln\left(\frac{2x^2}{1+x^2}\right)\,\mathrm{d}x =
-\frac{\pi}{16}\ln^{2}\left(2\right) -
\frac{11}{192}\,\pi^{3} +
2\Im\left\{%
\text{Li}_{3}\left(\frac{1 + \mathrm{i}}{2}\right)\right\} \int_0^1\frac{\arctan x\ln(1+x^2)}{1+x}dx \int_0^1\frac{\arctan x\ln x}{1+x}dx x\to\frac{1-x}{1+x} \int_0^1\frac{\arctan x\ln(1+x^2)}{1+x}dx I \ln(\cos x) \int_0^{\pi/2} \frac{x\cos(2nx)}{\sin x}dx","['real-analysis', 'integration', 'fourier-series', 'polylogarithm']"
79,"If $f∈C^1$ and $\{∇f=0\}$ has Lebesgue measure $0$, then $\{f∈B\}$ has Lebesgue measure $0$ for all Borel measurable $B⊆ℝ$ with Lebesgue measure $0$","If  and  has Lebesgue measure , then  has Lebesgue measure  for all Borel measurable  with Lebesgue measure",f∈C^1 \{∇f=0\} 0 \{f∈B\} 0 B⊆ℝ 0,"Let $d\in\mathbb N$ and $f\in C^1(\mathbb R^d)$ . Assume $\left\{\nabla f=0\right\}$ has Lebesgue measure $0$ . How can we conclude that $\left\{f\in B\right\}$ has Lebesgue measure $0$ for all Borel measurable $B\subseteq\mathbb R$ with Lebesgue measure $0$ ? The claim can be found in an answer on mathoverflow . The author writes that the claim ""is true locally, in a neighborhood of each point where $\nabla f\ne0$ , due to the implicit function theorem"". Honestly, I don't even understand what exactly he's meaning. Let $a\in\mathbb R^d$ with $\nabla f(a)\ne0$ . Then surely, by continuity of $\nabla f$ at $a$ , there is an open neighborhood $N$ of $a$ with $$\nabla f(x)\ne0\;\;\;\text{for all }x\in N\tag1.$$ But how do we need to apply the implicit function theorem and what's the resulting ""local"" conclusion? Maybe that $N\cap\left\{f\in B\right\}$ has Lebesgue measure $0$ ?","Let and . Assume has Lebesgue measure . How can we conclude that has Lebesgue measure for all Borel measurable with Lebesgue measure ? The claim can be found in an answer on mathoverflow . The author writes that the claim ""is true locally, in a neighborhood of each point where , due to the implicit function theorem"". Honestly, I don't even understand what exactly he's meaning. Let with . Then surely, by continuity of at , there is an open neighborhood of with But how do we need to apply the implicit function theorem and what's the resulting ""local"" conclusion? Maybe that has Lebesgue measure ?",d\in\mathbb N f\in C^1(\mathbb R^d) \left\{\nabla f=0\right\} 0 \left\{f\in B\right\} 0 B\subseteq\mathbb R 0 \nabla f\ne0 a\in\mathbb R^d \nabla f(a)\ne0 \nabla f a N a \nabla f(x)\ne0\;\;\;\text{for all }x\in N\tag1. N\cap\left\{f\in B\right\} 0,"['real-analysis', 'measure-theory', 'differential-geometry', 'lebesgue-measure', 'implicit-function-theorem']"
80,Is a random variable constant iff it is trivial sigma-algebra-measurable?,Is a random variable constant iff it is trivial sigma-algebra-measurable?,,"I found a proof here for a measurable function (instead of probability theory's random variable) being constant if and only if the sigma-algebra generated by it is the trivia sigma-algebra, I think (If so, I believe it is the same in the probabilistic version since the poster actually says ""probability space""). I copied the proof below. Here are my questions: Is capital $X$ supposed to be $A$? Is $A$ supposed to be a sample space rather than a probability space? So we say $f$ is a random variable/measurable function on probability space/measure space ($X, F, P$) for some probability measure $P$? Is $C$ supposed to be a Borel set? What is the relevance of $c_1$ being a closed set? Here is the proof: In reply to ""probability"", posted by alex on May 10, 2004: Suppose that $A$ is probability space and $f$ is a any real-valued function on $A$. Prove that   If $F=\{\emptyset, A\}$, then $f$ is $F$-measurable $\iff f$ is a constant If $f==c$ is constant it is ALWAYS measurable (for any sigma-algebra). This holds as $f^{-1}[C]$ is $X$ if $c \in C$ and empty if $c \notin C$. And both sets are in any sigma-algebra. On the other hand, if $f$ is $F$-measurable and non-constant, then it assumes at least two values $c_1$ and $c_2$. The set $f^{-1}[{c_1}]$ must be in $F$ (by being $F$-measurable, as ${c_1}$ is a closed set) but this set is non-empty (as $c_1$ IS a value of $f$) and not $X$ (as the points $x$ where $f$ assumes the value $c_2$ are not in it). So this set cannot be in $F$, and so $f$ must be constant.","I found a proof here for a measurable function (instead of probability theory's random variable) being constant if and only if the sigma-algebra generated by it is the trivia sigma-algebra, I think (If so, I believe it is the same in the probabilistic version since the poster actually says ""probability space""). I copied the proof below. Here are my questions: Is capital $X$ supposed to be $A$? Is $A$ supposed to be a sample space rather than a probability space? So we say $f$ is a random variable/measurable function on probability space/measure space ($X, F, P$) for some probability measure $P$? Is $C$ supposed to be a Borel set? What is the relevance of $c_1$ being a closed set? Here is the proof: In reply to ""probability"", posted by alex on May 10, 2004: Suppose that $A$ is probability space and $f$ is a any real-valued function on $A$. Prove that   If $F=\{\emptyset, A\}$, then $f$ is $F$-measurable $\iff f$ is a constant If $f==c$ is constant it is ALWAYS measurable (for any sigma-algebra). This holds as $f^{-1}[C]$ is $X$ if $c \in C$ and empty if $c \notin C$. And both sets are in any sigma-algebra. On the other hand, if $f$ is $F$-measurable and non-constant, then it assumes at least two values $c_1$ and $c_2$. The set $f^{-1}[{c_1}]$ must be in $F$ (by being $F$-measurable, as ${c_1}$ is a closed set) but this set is non-empty (as $c_1$ IS a value of $f$) and not $X$ (as the points $x$ where $f$ assumes the value $c_2$ are not in it). So this set cannot be in $F$, and so $f$ must be constant.",,"['real-analysis', 'probability-theory', 'measure-theory', 'constants']"
81,How to prove that $\sqrt{2}+\sqrt{3}>\pi$,How to prove that,\sqrt{2}+\sqrt{3}>\pi,Does someone know a other proofs (using properties of $\pi$)  of following inequality: $$\sqrt{2}+\sqrt{3}>\pi$$ First proof : the area of ​​regular 48-gon circumscribed to the unit circle is greater than $\pi$ and less than $\sqrt{2}+\sqrt{3}$. Second proof : $\pi<\frac{355}{113}<\sqrt{2}+\sqrt{3}$ Any hints would be appreciated.,Does someone know a other proofs (using properties of $\pi$)  of following inequality: $$\sqrt{2}+\sqrt{3}>\pi$$ First proof : the area of ​​regular 48-gon circumscribed to the unit circle is greater than $\pi$ and less than $\sqrt{2}+\sqrt{3}$. Second proof : $\pi<\frac{355}{113}<\sqrt{2}+\sqrt{3}$ Any hints would be appreciated.,,"['real-analysis', 'number-theory', 'inequality']"
82,Evaluate $\int\sin(\sin x)~dx$,Evaluate,\int\sin(\sin x)~dx,"I was skimming the virtual pages here and noticed a limit that made me wonder the following question: is there any nice way to evaluate the indefinite integral below? $$\int\sin(\sin x)~dx$$ Perhaps one way might use Taylor expansion. Thanks for any hint, suggestion.","I was skimming the virtual pages here and noticed a limit that made me wonder the following question: is there any nice way to evaluate the indefinite integral below? $$\int\sin(\sin x)~dx$$ Perhaps one way might use Taylor expansion. Thanks for any hint, suggestion.",,"['calculus', 'real-analysis', 'integration', 'indefinite-integrals']"
83,Inequalities in $l_p$ norm,Inequalities in  norm,l_p,"I'm having difficulty with the following problem. Any help would be appreciated. Problem: Consider the sequence spaces $l_p$ with the usual norm. If $1\le p\le q\le \infty$, I want to show the following inequality for any sequence $a$. $$\|a\|_q\le \|a\|_p$$ If we restrict to $\mathbb{R}^n$ but still use the $l_p$ norms, I also want to show this: $$\|a\|_q\le \|a\|_p\le n^{\frac{1}{p}-\frac{1}{q}}\|a\|_q$$ Work so far: I strongly suspect that a clever application of Hölder is needed here, but I tried the following for the first inequality: First, we consider the case where a finite number of elements in the sequence are nonzero. We want to prove $$||x||_q\le ||x||_p \Leftrightarrow \left(\sum_1^n |x_j|^q\right)^{\frac{1}{q}} \le \left(\sum_1^n |x_j|^p\right)^{\frac{1}{p}}.$$ We induct on $n$. The base case is clear. Because we can multiply all of the variables by a constant without affecting the inequality, we assume $x_n=1$. Assume we have proven the inequality for $n-1$. Then $$\left(\sum_1^{n-1} |x_j|^q\right) \le \left(\sum_1^{n-1} |x_j|^p\right)^{\frac{q}{p}}$$ It suffices to show that $$\left(\sum_1^{n-1} |x_j|^q\right) + 1 \le \left(\sum_1^{n-1} |x_j|^p+1\right)^{\frac{q}{p}}$$ This is equivalent to $$\left(\sum_1^{n-1} |x_j|^q\right)\le \left(\sum_1^{n-1} |x_j|^p+1\right)^{\frac{q}{p}}-1$$ So we need to show that if $f(x)=x^{q/p}$, then $f(x+1)\ge f(x)+1$. But this is clear, as $q\ge p$. Now I think it should be an easy matter to pass to the $l_p$ spaces by taking limits. I'm not sure what to do about the second inequality yet.","I'm having difficulty with the following problem. Any help would be appreciated. Problem: Consider the sequence spaces $l_p$ with the usual norm. If $1\le p\le q\le \infty$, I want to show the following inequality for any sequence $a$. $$\|a\|_q\le \|a\|_p$$ If we restrict to $\mathbb{R}^n$ but still use the $l_p$ norms, I also want to show this: $$\|a\|_q\le \|a\|_p\le n^{\frac{1}{p}-\frac{1}{q}}\|a\|_q$$ Work so far: I strongly suspect that a clever application of Hölder is needed here, but I tried the following for the first inequality: First, we consider the case where a finite number of elements in the sequence are nonzero. We want to prove $$||x||_q\le ||x||_p \Leftrightarrow \left(\sum_1^n |x_j|^q\right)^{\frac{1}{q}} \le \left(\sum_1^n |x_j|^p\right)^{\frac{1}{p}}.$$ We induct on $n$. The base case is clear. Because we can multiply all of the variables by a constant without affecting the inequality, we assume $x_n=1$. Assume we have proven the inequality for $n-1$. Then $$\left(\sum_1^{n-1} |x_j|^q\right) \le \left(\sum_1^{n-1} |x_j|^p\right)^{\frac{q}{p}}$$ It suffices to show that $$\left(\sum_1^{n-1} |x_j|^q\right) + 1 \le \left(\sum_1^{n-1} |x_j|^p+1\right)^{\frac{q}{p}}$$ This is equivalent to $$\left(\sum_1^{n-1} |x_j|^q\right)\le \left(\sum_1^{n-1} |x_j|^p+1\right)^{\frac{q}{p}}-1$$ So we need to show that if $f(x)=x^{q/p}$, then $f(x+1)\ge f(x)+1$. But this is clear, as $q\ge p$. Now I think it should be an easy matter to pass to the $l_p$ spaces by taking limits. I'm not sure what to do about the second inequality yet.",,"['real-analysis', 'analysis', 'inequality', 'normed-spaces', 'lp-spaces']"
84,"When is $F(x)=x^a\sin(x^{-b})$ with $F(0)=0$ of bounded variation on $[0,1]$?",When is  with  of bounded variation on ?,"F(x)=x^a\sin(x^{-b}) F(0)=0 [0,1]",I'm trying to show that $F(x)=x^a\sin\left(x^{-b}\right)$ for $0<x \leq 1$ and $F(0)=0$ has bounded variation only if $a>b$. I know I have to show there exist an $M< \infty$ such that for any partition $0=t_0<t_1<...<t_n=1$ we have $$\sum_{j=1}^N |F(t_j)-F(t_{j-1})|<M \iff |F(t_1)| + \sum_{j=2}^N |F(t_j)-F(t_{j-1})|<M .$$ I'm stuck here.,I'm trying to show that $F(x)=x^a\sin\left(x^{-b}\right)$ for $0<x \leq 1$ and $F(0)=0$ has bounded variation only if $a>b$. I know I have to show there exist an $M< \infty$ such that for any partition $0=t_0<t_1<...<t_n=1$ we have $$\sum_{j=1}^N |F(t_j)-F(t_{j-1})|<M \iff |F(t_1)| + \sum_{j=2}^N |F(t_j)-F(t_{j-1})|<M .$$ I'm stuck here.,,"['real-analysis', 'measure-theory', 'bounded-variation']"
85,"Calculating $\int_0^1 \frac{\log (x) \log \left(\frac{1}{2} \left(1+\sqrt{1-x^2}\right)\right)}{x} \, dx$",Calculating,"\int_0^1 \frac{\log (x) \log \left(\frac{1}{2} \left(1+\sqrt{1-x^2}\right)\right)}{x} \, dx","How would you like to calculate this one? Do you see a fast, neat way here? Ideas? $$\int_0^1 \frac{\log (x) \log \left(\frac{1}{2} \left(1+\sqrt{1-x^2}\right)\right)}{x} \, dx$$ Sharing solutions is only optional . The closed form revealed is $$\frac{1}{4} \left(\frac{2 }{3}\log ^3(2)-\zeta(2) \log (2)+\zeta (3)\right).$$","How would you like to calculate this one? Do you see a fast, neat way here? Ideas? $$\int_0^1 \frac{\log (x) \log \left(\frac{1}{2} \left(1+\sqrt{1-x^2}\right)\right)}{x} \, dx$$ Sharing solutions is only optional . The closed form revealed is $$\frac{1}{4} \left(\frac{2 }{3}\log ^3(2)-\zeta(2) \log (2)+\zeta (3)\right).$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
86,Closed form $\int_{-1}^{1} \frac{\ln (\sqrt{3} x +2)}{\sqrt{1-x^{2}} (\sqrt{3} x + 2)^{n}}\ dx$,Closed form,\int_{-1}^{1} \frac{\ln (\sqrt{3} x +2)}{\sqrt{1-x^{2}} (\sqrt{3} x + 2)^{n}}\ dx,"Does the following integral  $$\int_{-1}^{1} \frac{\ln (\sqrt{3} x +2)}{\sqrt{1-x^{2}} (\sqrt{3} x + 2)^{n}}\ dx, \; \; n \in \mathbb{N}$$ have a nice closed form? Basically I cannot tackle it in any direction. Symmetry is useless . Applyig parts , well , gets things worse than they actually are. Could complex analysis help us here? That is, integrating around a dog bone contour ... I highly doubt it but it is just a thought. Any help on this one?","Does the following integral  $$\int_{-1}^{1} \frac{\ln (\sqrt{3} x +2)}{\sqrt{1-x^{2}} (\sqrt{3} x + 2)^{n}}\ dx, \; \; n \in \mathbb{N}$$ have a nice closed form? Basically I cannot tackle it in any direction. Symmetry is useless . Applyig parts , well , gets things worse than they actually are. Could complex analysis help us here? That is, integrating around a dog bone contour ... I highly doubt it but it is just a thought. Any help on this one?",,"['real-analysis', 'complex-analysis', 'improper-integrals', 'closed-form']"
87,What is the difference between the limit of a sequence and a limit point of a set?,What is the difference between the limit of a sequence and a limit point of a set?,,I always thought they were the same thing. The limit of a sequence is a point such that every neighborhood around it contains infinitely many terms of the sequence. The limit point of a set is a point such that every neighborhood around it contains infinitely many points of the set. So is the limit of a sequence also a limit point of a set that contains it? what's the difference?,I always thought they were the same thing. The limit of a sequence is a point such that every neighborhood around it contains infinitely many terms of the sequence. The limit point of a set is a point such that every neighborhood around it contains infinitely many points of the set. So is the limit of a sequence also a limit point of a set that contains it? what's the difference?,,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'epsilon-delta']"
88,"An infinite $\sigma$-algebra contains an infinite sequence of nonempty, disjoint sets.","An infinite -algebra contains an infinite sequence of nonempty, disjoint sets.",\sigma,"I am trying to solve Exercise 3 a) given here . The problem states: Let $\mathcal{M}$ be an infinite $\sigma$-algebra. Prove that   $\mathcal{M}$ contains an infinite sequence of nonempty, disjoint   sets. (Hint: if $\mathcal{M}$ contains an infinite sequence of   strictly nested sets, then we’re done, so assume that no such sequence   exists. Next, use this assumption to find a nonempty set in   $\mathcal{M}$ with no nonempty proper subsets in $\mathcal{M}$.   Finally, show that this can be done infinitely many times.) Here's my idea: Let's say $\mathcal{M}$ is $\sigma$-algebra on the set $X$. If $\mathcal{M}$ does not contain an infinite sequence of strictly nested sets, then given any $A\in\mathcal{M}$, every strict chain starting with $A$ must terminate after finite time: i.e. $A\subsetneq E_{1}\subsetneq E_{2}\subsetneq … \subsetneq E_{k}$ and there is no set $X\neq B\in\mathcal{M}$ such that $E_{k}\subsetneq B$. But then the complement $E_k^{c}$ has no nonempty proper subsets in $\mathcal{M}$. But I am having trouble showing that the process can repeated infinitely many times. I have been stuck with this for the whole day, and it is driving me crazy! Thanks for the help!","I am trying to solve Exercise 3 a) given here . The problem states: Let $\mathcal{M}$ be an infinite $\sigma$-algebra. Prove that   $\mathcal{M}$ contains an infinite sequence of nonempty, disjoint   sets. (Hint: if $\mathcal{M}$ contains an infinite sequence of   strictly nested sets, then we’re done, so assume that no such sequence   exists. Next, use this assumption to find a nonempty set in   $\mathcal{M}$ with no nonempty proper subsets in $\mathcal{M}$.   Finally, show that this can be done infinitely many times.) Here's my idea: Let's say $\mathcal{M}$ is $\sigma$-algebra on the set $X$. If $\mathcal{M}$ does not contain an infinite sequence of strictly nested sets, then given any $A\in\mathcal{M}$, every strict chain starting with $A$ must terminate after finite time: i.e. $A\subsetneq E_{1}\subsetneq E_{2}\subsetneq … \subsetneq E_{k}$ and there is no set $X\neq B\in\mathcal{M}$ such that $E_{k}\subsetneq B$. But then the complement $E_k^{c}$ has no nonempty proper subsets in $\mathcal{M}$. But I am having trouble showing that the process can repeated infinitely many times. I have been stuck with this for the whole day, and it is driving me crazy! Thanks for the help!",,"['real-analysis', 'measure-theory']"
89,A question about Baby Rudin Theorem 2.27 (a),A question about Baby Rudin Theorem 2.27 (a),,"Theorem 2.27: If $X$ is a metric space and $E \subset X$, then $\bar E$ (the closure of $E$)  is closed. The proof says: If $p \in X$ and $p \not \in \bar E$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. The complement of $\bar E$ is therefore open. Hence $\bar E$ is closed. I'm particularly questioning about ''Hence $p$ has a neighborhood which does not intersect $E$. The complement of $\bar E$ is therefore open.'' Should we also prove that the neighborhood of $p$ also does not intersect $E'$ (the set of all limit points of $E$)? Here's what I tried to prove, by contrapositive: ''For any $p \in {\bar E}^c$, if $N_r(p) \cap E' \ne \emptyset$ then $N_r(p) \cap E \ne \emptyset$''. Proof: For any $p \in {\bar E}^c$, if $N_r(p) \cap E' \ne \emptyset$, then take $q \in N_r(p) \cap E'$, $\exists N_h(q)$ s.t. $N_h(q) \subset N_r(p)$. Since $q \in E'$ is a limit point, $N_h(q) \cap E \ne \emptyset$, and hence $N_r(p) \cap E \ne \emptyset$. I'm not quite sure whether this is necessary. Or is there anything I missed from Rudin's proof?","Theorem 2.27: If $X$ is a metric space and $E \subset X$, then $\bar E$ (the closure of $E$)  is closed. The proof says: If $p \in X$ and $p \not \in \bar E$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. The complement of $\bar E$ is therefore open. Hence $\bar E$ is closed. I'm particularly questioning about ''Hence $p$ has a neighborhood which does not intersect $E$. The complement of $\bar E$ is therefore open.'' Should we also prove that the neighborhood of $p$ also does not intersect $E'$ (the set of all limit points of $E$)? Here's what I tried to prove, by contrapositive: ''For any $p \in {\bar E}^c$, if $N_r(p) \cap E' \ne \emptyset$ then $N_r(p) \cap E \ne \emptyset$''. Proof: For any $p \in {\bar E}^c$, if $N_r(p) \cap E' \ne \emptyset$, then take $q \in N_r(p) \cap E'$, $\exists N_h(q)$ s.t. $N_h(q) \subset N_r(p)$. Since $q \in E'$ is a limit point, $N_h(q) \cap E \ne \emptyset$, and hence $N_r(p) \cap E \ne \emptyset$. I'm not quite sure whether this is necessary. Or is there anything I missed from Rudin's proof?",,['real-analysis']
90,Does differentiable function of bounded variation have bounded derivative?,Does differentiable function of bounded variation have bounded derivative?,,"I learned that $f$ is a function of bounded variation, when function $f$ is differentiable on $[a,b]$ and has bounded derivative $f'$. What I want to know is converse part. If $f$ is differentiable on $[a,b]$ and $f$ is a function of bounded variation, Is derivative of $f$ bounded?  I guess it's false, but i cannot find a counterexample. If it's true, please show me proof.","I learned that $f$ is a function of bounded variation, when function $f$ is differentiable on $[a,b]$ and has bounded derivative $f'$. What I want to know is converse part. If $f$ is differentiable on $[a,b]$ and $f$ is a function of bounded variation, Is derivative of $f$ bounded?  I guess it's false, but i cannot find a counterexample. If it's true, please show me proof.",,"['real-analysis', 'bounded-variation']"
91,The extension of smooth function,The extension of smooth function,,"If $U$ is a bounded domain in $\mathbb R^n$ whose boundary is smooth, and $f$ is a smooth function on $U$ whose partial derivatives of all orders have a continuous extensions to $\bar U$. For an arbitrary domain $V \supseteq \bar U$, is there a smooth function $\tilde f$ on $V$ extending $f$?","If $U$ is a bounded domain in $\mathbb R^n$ whose boundary is smooth, and $f$ is a smooth function on $U$ whose partial derivatives of all orders have a continuous extensions to $\bar U$. For an arbitrary domain $V \supseteq \bar U$, is there a smooth function $\tilde f$ on $V$ extending $f$?",,"['real-analysis', 'partial-differential-equations']"
92,Find the supremum of the set $A=\{\cos(10^n)\mid n\in\mathbb{N} \}$,Find the supremum of the set,A=\{\cos(10^n)\mid n\in\mathbb{N} \},"I have just finished learning in class that every non-empty bounded above subset of $\mathbb{R}$ has a least upper bound, but my professor then showed us the following set: $$A=\{\cos(10^n)\mid n\in\mathbb{N} \} $$ and asked us to compute the first $5$ decimal places of the supremum of $A$ . At first glance, I want the supremum of this set to be $1$ , but that is not possible for integer values of $n$ , as $10^n$ is never an integer multiple of $2\pi$ . There seem to be no clear patterns concerning the periodicity of the function. My hypothesis is that there is no method to find the supremum of this set, but all we know is that it exists. I started out by noticing that $10^n$ must equal some integer multiple of $2\pi$ , so I got the equation $10^n=2\pi m$ where $m\in\mathbb{Z}$ . This leads to $n=\log(2\pi m)$ which has no integer solutions, but the question is what value of $m$ gets $\log(2\pi m)$ the closest to an integer, which I have no idea how to begin showing. So my question is, is there a way to find the supremum of this set, and if so, how can I compute the first $5$ decimal places?","I have just finished learning in class that every non-empty bounded above subset of has a least upper bound, but my professor then showed us the following set: and asked us to compute the first decimal places of the supremum of . At first glance, I want the supremum of this set to be , but that is not possible for integer values of , as is never an integer multiple of . There seem to be no clear patterns concerning the periodicity of the function. My hypothesis is that there is no method to find the supremum of this set, but all we know is that it exists. I started out by noticing that must equal some integer multiple of , so I got the equation where . This leads to which has no integer solutions, but the question is what value of gets the closest to an integer, which I have no idea how to begin showing. So my question is, is there a way to find the supremum of this set, and if so, how can I compute the first decimal places?",\mathbb{R} A=\{\cos(10^n)\mid n\in\mathbb{N} \}  5 A 1 n 10^n 2\pi 10^n 2\pi 10^n=2\pi m m\in\mathbb{Z} n=\log(2\pi m) m \log(2\pi m) 5,"['real-analysis', 'trigonometry', 'supremum-and-infimum']"
93,The sum of two continuous periodic functions is periodic if and only if the ratio of their periods is rational?,The sum of two continuous periodic functions is periodic if and only if the ratio of their periods is rational?,,"There are several questions here on MSE about the periodicity of the sum of two continuous, periodic functions. Here's what I know so far: It's obvious that if the ratio of the periods is rational, the sum of the functions will be periodic with a period equal to the LCM of the individual periods. It's relatively easy to give a counterexample to show that the sum of two continuous periodic functions is not always periodic, for instance by considering $\sin(x) + \sin(\pi x)$ . My hunch is that in fact, the sum of two continuous periodic functions is periodic if and only if the ratio of their periods is rational. First of all, is this true? Secondly, if it's true, the sufficient part  is obvious, but is there a simple proof to show the necessary part? By a ""simple"" proof I mean one that is accessible to someone with no extensive formal knowledge of analysis or algebra. This is the closest proof I could find , but I can't quite follow it.","There are several questions here on MSE about the periodicity of the sum of two continuous, periodic functions. Here's what I know so far: It's obvious that if the ratio of the periods is rational, the sum of the functions will be periodic with a period equal to the LCM of the individual periods. It's relatively easy to give a counterexample to show that the sum of two continuous periodic functions is not always periodic, for instance by considering $\sin(x) + \sin(\pi x)$ . My hunch is that in fact, the sum of two continuous periodic functions is periodic if and only if the ratio of their periods is rational. First of all, is this true? Secondly, if it's true, the sufficient part  is obvious, but is there a simple proof to show the necessary part? By a ""simple"" proof I mean one that is accessible to someone with no extensive formal knowledge of analysis or algebra. This is the closest proof I could find , but I can't quite follow it.",,"['real-analysis', 'periodic-functions']"
94,what is the difference between a set's closure and completion?,what is the difference between a set's closure and completion?,,"I know that when we talk about completion of a space, we infer that it is a metric space. So can I say that: Given a complete metric space $M$ and a subspace of $M$, $S$, the closure of $S$ is a completion of $S$?","I know that when we talk about completion of a space, we infer that it is a metric space. So can I say that: Given a complete metric space $M$ and a subspace of $M$, $S$, the closure of $S$ is a completion of $S$?",,"['real-analysis', 'general-topology']"
95,Problems with the proof that $\ell^p$ is complete,Problems with the proof that  is complete,\ell^p,"By struggling with the proof that $\ell^p$ is complete, I looked up different proofs by different authors, and I ended up focusing on the one given by Kreyszig in his classic book on functional analysis, because I found it the most perspicuous for my level. Still, there are some point that are not completely clear, thus I will write down here the whole proof and I will add my remarks and doubts. [I] Theorem: The space $\ell^p$ is complete; here $p$ is fixed and $1 \leq p < \infty$ . Proof: Let $(x_n)$ be any Cauchy sequence in $\ell^p$ , where $x_m = ( \xi^{(m)}_1, \xi^{(m)}_2, \dots  )$ . Then, for every $\varepsilon > 0 $ there is an $N$ such that for all $m, n >N$ , \begin{equation} d ( x_m, x_n ) = \Bigg( \sum_{j=1}^\infty |\xi^{(m)}_j - \xi^{(n)}_j |^p  \Bigg)^{\frac{1}{p}} < \varepsilon \hspace{2cm} \text{(1)} .  \end{equation} It follows that for every $j = 1,2, \dots$ we have for $m,n >N$ $$ |\xi^{(m)}_j - \xi^{(n)}_j | < \varepsilon \hspace{2cm}  \text{(2)}. $$ We choose a fixed $j$ . From (2) we see that $(  \xi^{(1)}_j, \xi^{(2)}_j, \dots )$ is a Cauchy sequence of numbers. It converges since $\Re$ is complete, say $\xi^{(m)}_j \to \xi_j$ as $m  \to \infty$ . Using these limits, we define $x = (\xi_1, \xi_2, \dots)$ and show that $x \in \ell^p$ and $x_m \to x$ . This looks fine, [II] From (1) we have for all $m,n >N$ $$ \sum_{j=1}^k |\xi^{(m)}_j -  \xi^{(n)}_j |^p  < \varepsilon^p \hspace{2cm} (k=1,2,\dots). $$ First problems! I see where the $\varepsilon^p$ comes from, but what puzzles me is the $k$ on the top of $\sum$ . I see it comes from (1) (indeed, if it works for $j \to \infty$ , then it has to work necessarily for a finite $k$ . Still, I don't see why we actually need to do it . Why don't we simply stick to (1)? [III] Letting $n \to \infty$ , we obtain for $m>N$ $$ \sum_{j=1}^k  |\xi^{(m)}_j - \xi_j |^p  \leq \varepsilon^p \hspace{2cm}  (k=1,2,\dots). $$ Same problem as before, plus the mysterious $\leq$ between LHS and RHS. Indeed, I think the idea should be that the sequence converge and it is Cauchy, thus the $\varepsilon > 0$ is the same that we use in the definition of Cauchy sequence, and in the standard limit definition. But then it should be still $<$ and not $\leq$ . [IV] We may now let $k \to \infty$ ; then for $m >N$ $$ \sum_{j=1}^\infty |\xi^{(m)}_j - \xi_j |^p  \leq \varepsilon^p \hspace{2cm} \text{(3)}. $$ Bypassing the previous related problems, this is fine. [V] This shows that $x_m - x = ( \xi^{(m)}_j - \xi_j ) \in \ell^p$ . Not completely sure I see why it is actually the case. [VI] Since $x_m \in \ell^p$ , it follows by means of the Minkowski inequality, that $$ x = x_m + (x - x_m) \in \ell^p. $$ The reference to the Minkowski inequality is really mysterious. [VII] Furthermore, the series in (3) represents $[d(x_m,x_)]^p$ , so that (3) implies that $x_m \to x$ . Since $(x_m)$ was an arbitrary Cauchy sequence in $\ell^p$ , this proves the completeness of $\ell^p$ , where $1 \leq p < \infty$ . QED Sorry for the vivisection of this rather straightforward proof, but I have the feeling that by properly catching each step here, I could improve dramatically my overall understanding of real analysis tools and procedures. Thank you for your time and patience. As always, any feedback is most welcome! Edit for BOUNTY: I am editing this question because I am putting a bounty. True enough, I received very helpful comments, but still they were only comments, and I would really  love to have an answer, because I do feel a lot of the things that look here mysterious have to be fairly important. I also have the feeling that some of the problems I showed in these questions, e.g. the one that is below under (1), can give the feeling that it is impossible I can actually try to read something sort of advanced, without having a solid grasps of other things, and maybe this could put a potential answerer in the akward position of feeling “Come on, man, are you kidding me? Don’t let me waste my time. I cannot go back to teach you 2+2!”. However, that’s how things are, and this is mostly due to the fact that I am self-taught, which put me in the position to choose my own topics. But, actually, exactly those very naive questions are the ones that would set me on the right path to keep on studying properly. Hence, I renumbered the parts in which I divided the proof of the theorem in order to easily refer the questions to each part. 1) Is the change from $\infty$ to $k$ in [II] related to the fact that we are dealing with a series, and thus we standardly see how a series behave with its finite terms, before letting the limit goes to $\infty$ (which is what happen in [III] ? 2) Is in step [V] implicitly assumed that it is the case due to the fact that it is for all $j$ ? 3) How does step [VI] come from the Minkowski inequality? Of course, any other addition or explanation is most welcome. Thanks for your time!","By struggling with the proof that is complete, I looked up different proofs by different authors, and I ended up focusing on the one given by Kreyszig in his classic book on functional analysis, because I found it the most perspicuous for my level. Still, there are some point that are not completely clear, thus I will write down here the whole proof and I will add my remarks and doubts. [I] Theorem: The space is complete; here is fixed and . Proof: Let be any Cauchy sequence in , where . Then, for every there is an such that for all , It follows that for every we have for We choose a fixed . From (2) we see that is a Cauchy sequence of numbers. It converges since is complete, say as . Using these limits, we define and show that and . This looks fine, [II] From (1) we have for all First problems! I see where the comes from, but what puzzles me is the on the top of . I see it comes from (1) (indeed, if it works for , then it has to work necessarily for a finite . Still, I don't see why we actually need to do it . Why don't we simply stick to (1)? [III] Letting , we obtain for Same problem as before, plus the mysterious between LHS and RHS. Indeed, I think the idea should be that the sequence converge and it is Cauchy, thus the is the same that we use in the definition of Cauchy sequence, and in the standard limit definition. But then it should be still and not . [IV] We may now let ; then for Bypassing the previous related problems, this is fine. [V] This shows that . Not completely sure I see why it is actually the case. [VI] Since , it follows by means of the Minkowski inequality, that The reference to the Minkowski inequality is really mysterious. [VII] Furthermore, the series in (3) represents , so that (3) implies that . Since was an arbitrary Cauchy sequence in , this proves the completeness of , where . QED Sorry for the vivisection of this rather straightforward proof, but I have the feeling that by properly catching each step here, I could improve dramatically my overall understanding of real analysis tools and procedures. Thank you for your time and patience. As always, any feedback is most welcome! Edit for BOUNTY: I am editing this question because I am putting a bounty. True enough, I received very helpful comments, but still they were only comments, and I would really  love to have an answer, because I do feel a lot of the things that look here mysterious have to be fairly important. I also have the feeling that some of the problems I showed in these questions, e.g. the one that is below under (1), can give the feeling that it is impossible I can actually try to read something sort of advanced, without having a solid grasps of other things, and maybe this could put a potential answerer in the akward position of feeling “Come on, man, are you kidding me? Don’t let me waste my time. I cannot go back to teach you 2+2!”. However, that’s how things are, and this is mostly due to the fact that I am self-taught, which put me in the position to choose my own topics. But, actually, exactly those very naive questions are the ones that would set me on the right path to keep on studying properly. Hence, I renumbered the parts in which I divided the proof of the theorem in order to easily refer the questions to each part. 1) Is the change from to in [II] related to the fact that we are dealing with a series, and thus we standardly see how a series behave with its finite terms, before letting the limit goes to (which is what happen in [III] ? 2) Is in step [V] implicitly assumed that it is the case due to the fact that it is for all ? 3) How does step [VI] come from the Minkowski inequality? Of course, any other addition or explanation is most welcome. Thanks for your time!","\ell^p \ell^p p 1 \leq p < \infty (x_n) \ell^p x_m = ( \xi^{(m)}_1, \xi^{(m)}_2, \dots  ) \varepsilon > 0  N m, n >N \begin{equation} d ( x_m, x_n ) = \Bigg( \sum_{j=1}^\infty |\xi^{(m)}_j - \xi^{(n)}_j |^p
 \Bigg)^{\frac{1}{p}} < \varepsilon \hspace{2cm} \text{(1)} .
 \end{equation} j = 1,2, \dots m,n >N  |\xi^{(m)}_j - \xi^{(n)}_j | < \varepsilon \hspace{2cm}
 \text{(2)}.  j (
 \xi^{(1)}_j, \xi^{(2)}_j, \dots ) \Re \xi^{(m)}_j \to \xi_j m
 \to \infty x = (\xi_1, \xi_2, \dots) x \in \ell^p x_m \to x m,n >N  \sum_{j=1}^k |\xi^{(m)}_j -
 \xi^{(n)}_j |^p  < \varepsilon^p \hspace{2cm} (k=1,2,\dots).  \varepsilon^p k \sum j \to \infty k n \to \infty m>N  \sum_{j=1}^k
 |\xi^{(m)}_j - \xi_j |^p  \leq \varepsilon^p \hspace{2cm}
 (k=1,2,\dots).  \leq \varepsilon > 0 < \leq k \to \infty m >N 
\sum_{j=1}^\infty |\xi^{(m)}_j - \xi_j |^p  \leq \varepsilon^p \hspace{2cm} \text{(3)}.
 x_m - x = ( \xi^{(m)}_j - \xi_j ) \in \ell^p x_m \in \ell^p 
x = x_m + (x - x_m) \in \ell^p.
 [d(x_m,x_)]^p x_m \to x (x_m) \ell^p \ell^p 1 \leq p < \infty \infty k \infty j","['real-analysis', 'functional-analysis', 'self-learning', 'lp-spaces']"
96,Calculate $\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx$,Calculate,\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx,"I am trying to calculate: $$\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx$$ I am not looking for an answer but simply a nudge in the right direction. A strategy, just something that would get me started. So, after doing the Taylor Expansion on the $\ln(1-x+x^2)$ ig to the following: Let $x=x-x^2$ then $\ln(1-x)$ then, \begin{align*} =&-x-\frac{x^2}{2}-\frac{x^3}{3}-\frac{x^4}{4}-...\\ =&-(x-x^2)-\frac{(x-x^2)^2}{2}-...\\ =&-x(1-x)+\frac{x^2}{2}(1-x)^2-\frac{x^3}{3}(1-x)^3\\ \text{thus the pattern is:}\\ =&\frac{x^n(1-x)^n}{n} \end{align*}  Am I right? Then our Integral would be: $$\sum_{n=0}^{\infty} \frac{1}{n+1} \int_0^1 x^n(1-x)^n$$ Am I on the right track? Suggestions, tips, comments? $\underline{NEW EDIT:}$ SO after integrating the function I got the following after a couple of iterations: \begin{align*} \frac{n(n-1)...1}{(n+1)(n+2)...(2n)}\int_0^1 x^{2n} dx \end{align*} This shows a pattern: \begin{align*} =&\frac{(n!)^2}{(2n)!} (\frac{1}{2n+1})\\ =& \frac{(n!)^2}{(2n+1)!} \end{align*} So my question is, what to do from here. I have done all this but still have no clue how to actually solve the integral. Can somebody shed some light on this! Thanks","I am trying to calculate: $$\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx$$ I am not looking for an answer but simply a nudge in the right direction. A strategy, just something that would get me started. So, after doing the Taylor Expansion on the $\ln(1-x+x^2)$ ig to the following: Let $x=x-x^2$ then $\ln(1-x)$ then, \begin{align*} =&-x-\frac{x^2}{2}-\frac{x^3}{3}-\frac{x^4}{4}-...\\ =&-(x-x^2)-\frac{(x-x^2)^2}{2}-...\\ =&-x(1-x)+\frac{x^2}{2}(1-x)^2-\frac{x^3}{3}(1-x)^3\\ \text{thus the pattern is:}\\ =&\frac{x^n(1-x)^n}{n} \end{align*}  Am I right? Then our Integral would be: $$\sum_{n=0}^{\infty} \frac{1}{n+1} \int_0^1 x^n(1-x)^n$$ Am I on the right track? Suggestions, tips, comments? $\underline{NEW EDIT:}$ SO after integrating the function I got the following after a couple of iterations: \begin{align*} \frac{n(n-1)...1}{(n+1)(n+2)...(2n)}\int_0^1 x^{2n} dx \end{align*} This shows a pattern: \begin{align*} =&\frac{(n!)^2}{(2n)!} (\frac{1}{2n+1})\\ =& \frac{(n!)^2}{(2n+1)!} \end{align*} So my question is, what to do from here. I have done all this but still have no clue how to actually solve the integral. Can somebody shed some light on this! Thanks",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
97,Can this intuition give a proof that an isometry $f:X \to X$ is surjective for compact metric space $X$?,Can this intuition give a proof that an isometry  is surjective for compact metric space ?,f:X \to X X,"A prelim problem asked to prove that if $X$ is a compact metric space, and $f:X \to X$ is an isometry (distance-preserving map) then $f$ is surjective. The official proof given used sequences/convergent subsequences and didn't appeal to my intuition. When I saw the problem, my immediate instinct was that an isometry should be ""volume-preserving"" as well, so the volume of $f(X)$ should be equal to the volume of $X$, which should mean surjectivity if $X$ is compact. The notion of ""volume"" I came up with was the minimum number of $\epsilon$-balls needed to cover $X$ for given $\epsilon > 0$. If $f$ were not surjective, then because $f$ is continuous this means there must be a point $y \in X$ and $\delta > 0$ so that the ball $B_\delta(y)$ is disjoint from $f(X)$. I wanted to choose $\epsilon$ in terms of $\delta$ and use the fact that an isometry carries $\epsilon$-balls to $\epsilon$-balls, and show that given a minimum-size cover of $X$ with $\epsilon$ balls, that a cover of $X$ with $\epsilon$-balls could be found with one fewer ball if $f(X) \cap B_\delta(y) = \emptyset$, giving a contradiction. Can someone see a way to make this intuition work?","A prelim problem asked to prove that if $X$ is a compact metric space, and $f:X \to X$ is an isometry (distance-preserving map) then $f$ is surjective. The official proof given used sequences/convergent subsequences and didn't appeal to my intuition. When I saw the problem, my immediate instinct was that an isometry should be ""volume-preserving"" as well, so the volume of $f(X)$ should be equal to the volume of $X$, which should mean surjectivity if $X$ is compact. The notion of ""volume"" I came up with was the minimum number of $\epsilon$-balls needed to cover $X$ for given $\epsilon > 0$. If $f$ were not surjective, then because $f$ is continuous this means there must be a point $y \in X$ and $\delta > 0$ so that the ball $B_\delta(y)$ is disjoint from $f(X)$. I wanted to choose $\epsilon$ in terms of $\delta$ and use the fact that an isometry carries $\epsilon$-balls to $\epsilon$-balls, and show that given a minimum-size cover of $X$ with $\epsilon$ balls, that a cover of $X$ with $\epsilon$-balls could be found with one fewer ball if $f(X) \cap B_\delta(y) = \emptyset$, giving a contradiction. Can someone see a way to make this intuition work?",,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness']"
98,Sum of $\Gamma(n+a) / \Gamma(n+b)$,Sum of,\Gamma(n+a) / \Gamma(n+b),"If $a$ and $b$ are positive real numbers, such that $b > a + 1$, can we find the sum $$\sum_{n=0}^{\infty} \frac{\Gamma(n+a)}{\Gamma(n+b)}?$$  For example I have found that $$\sum_{n=0}^{\infty} \frac{\Gamma(n+3/2)}{\Gamma(n+3)} = \sqrt{\pi} = \Gamma(1/2)$$ and $$\sum_{n=0}^{\infty} \frac{\Gamma(n+4/3)}{\Gamma(n+4)} = \frac{3}{10}\Gamma(4/3)$$ but no general rule.","If $a$ and $b$ are positive real numbers, such that $b > a + 1$, can we find the sum $$\sum_{n=0}^{\infty} \frac{\Gamma(n+a)}{\Gamma(n+b)}?$$  For example I have found that $$\sum_{n=0}^{\infty} \frac{\Gamma(n+3/2)}{\Gamma(n+3)} = \sqrt{\pi} = \Gamma(1/2)$$ and $$\sum_{n=0}^{\infty} \frac{\Gamma(n+4/3)}{\Gamma(n+4)} = \frac{3}{10}\Gamma(4/3)$$ but no general rule.",,"['calculus', 'real-analysis', 'sequences-and-series', 'gamma-function']"
99,Proving that product of two Cauchy sequences is Cauchy,Proving that product of two Cauchy sequences is Cauchy,,"Given that $x_n$ and $y_n$ are Cauchy sequences in $\mathbb{R} $, prove that $x_n y_n$ is Cauchy without the use of the Cauchy theorem stating that Cauchy $\Rightarrow$ convergence. Attempt: Without that condition on not been able to use the theorem, the question becomes trivial.  Instead: For all $\epsilon > 0$ there exists an $N \in \mathbb{N}$ such that for $n,m \geq N, -\frac{\epsilon}{2} \leq x_n - x_m \leq \frac{\epsilon}{2}$ and similar statment for $y_n$. Multiply the above by $y_n$ and the equivalent statement for $y_n$ by $x_m$.  Then add these together.  The result is: $$|x_ny_n - x_my_m| < \frac{\epsilon}{2}(x_m + y_n)$$  I have proved in a previous question that $x_n + y_n$ is Cauchy so could I apply that here and say for $n,m \geq N$, $x_n + y_n$ is Cauchy and hence convergent so tends to a finite limit for $n,m \geq N$.  This would mean my upper bound is a multiple of $\epsilon$ and since $\epsilon$ is arbritarily small, so is this upper bound. Hence Cauchy. I don't think this would warrant a full proof in any case since by multiplying by $x_m$ and $y_n$, I am assuming they are positive so as to not reverse the inequality signs.  Nonetheless, I would appreciate some feedback on what I have done. Many thanks","Given that $x_n$ and $y_n$ are Cauchy sequences in $\mathbb{R} $, prove that $x_n y_n$ is Cauchy without the use of the Cauchy theorem stating that Cauchy $\Rightarrow$ convergence. Attempt: Without that condition on not been able to use the theorem, the question becomes trivial.  Instead: For all $\epsilon > 0$ there exists an $N \in \mathbb{N}$ such that for $n,m \geq N, -\frac{\epsilon}{2} \leq x_n - x_m \leq \frac{\epsilon}{2}$ and similar statment for $y_n$. Multiply the above by $y_n$ and the equivalent statement for $y_n$ by $x_m$.  Then add these together.  The result is: $$|x_ny_n - x_my_m| < \frac{\epsilon}{2}(x_m + y_n)$$  I have proved in a previous question that $x_n + y_n$ is Cauchy so could I apply that here and say for $n,m \geq N$, $x_n + y_n$ is Cauchy and hence convergent so tends to a finite limit for $n,m \geq N$.  This would mean my upper bound is a multiple of $\epsilon$ and since $\epsilon$ is arbritarily small, so is this upper bound. Hence Cauchy. I don't think this would warrant a full proof in any case since by multiplying by $x_m$ and $y_n$, I am assuming they are positive so as to not reverse the inequality signs.  Nonetheless, I would appreciate some feedback on what I have done. Many thanks",,"['real-analysis', 'sequences-and-series', 'cauchy-sequences']"
