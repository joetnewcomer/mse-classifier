,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can anyone help to perform this heinous integral? (Peskin & Schroeder's Quantum field theory (4.76)-(4.78)) (Including my own trial ),Can anyone help to perform this heinous integral? (Peskin & Schroeder's Quantum field theory (4.76)-(4.78)) (Including my own trial ),,"I am reading the Peskin & Schroeder's An introduction to Quantum field theory, p.105~p.106 (Construction of a cross-section from the invariant matrix element (p.104) ) and stuck at understanding some integration. I think that I never seen a pattern like this. EDIT : I edited this post to try to answer this question on my own. I wish this edited post will be helpful to readers. (And if there is anything that needs to be corrected, then it will be appreciate to inform me.) First, we are given (their book, p.105, (4.76):) $$ d\sigma =(\prod_f\frac{d^3p_f}{(2\pi)^3}\frac{1}{2E_f}) \int d^2b(\prod_{i=A,B}\int \frac{d^3k_i}{(2\pi)^3}\frac{\phi_i(\vec{k}_i)}{\sqrt{2E_i}}\int \frac{d^3 \bar{k}_i}{(2\pi)^3}\frac{\phi_i^{*}(\bar{\vec{k}}_i)}{\sqrt{2\bar{E}_i}})$$ $$\times e^{\vec{b} \cdot (\bar{\vec{k}_B} - \vec{k}_B)} ( _{out}\langle \{\vec{p}_f\}|\{\vec{k}_i|\}\rangle_{in})( _{out}\langle \{\vec{p}_f\}|\{\bar{\vec{k}}_i|\rangle_{in})^*  \tag{4.76}.  $$ And the goal is an expression ( p. 106, (4.78) ) : $$ d\sigma = (\prod_f\frac{d^3p_f}{(2\pi)^3}\frac{1}{2E_f})\frac{|\mathcal{M}(p_A, p_B \to \{p_f\})|^2}{2E_A 2E_B|v_A-v_B|}  \tag{4.78} \int \frac{d^3k_A}{(2 \pi)^3}\int\frac{d^3k_B}{(2\pi)^3} $$ $$\times |\phi_A(\vec{k}_A)|^2|\phi_B(\vec{k}_B)|^2 (2\pi)^4 \delta^{(4)}(k_A + k_B - \Sigma p_f).$$ Through p.105~p.106 he derives this formula but I don't quite understand his argument in part. For example, ""..We can use the scond of these delta functions, together with the $\delta^{(2)}(k_B^{\perp}-\bar{k}_B^{\perp})$ , to perform all six of the $\bar{k}$ integrals in $(4.76)$ . Of the six (?) integrals, only those over $\bar{k}_A^z$ and $\bar{k}_B^z$ require some work. ( his book p.105 ).."" and ""Now recall that the initial wavepackets are localized in momentum space, centered on $\vec{p}_A$ and $\vec{p}_B$ . This means that we can evaluate all factors that are smooth functions of $\vec{k}_A$ and $\vec{k}_B$ at $\vec{p}_A$ and $\vec{p}_B$ , pulling them outside except the remaining delta function ( his book p.106, first paragraph ).. "" Second, meanwhile, I found next note (Introduction to quantum field theory by Nastase): https://professores.ift.unesp.br/ricardo.matheus/files/courses/2014tqc1/QFT1notes.pdf In p.176 ~ p.177 of his note, he notes that ( 19.35 , 19.36 ) $$ \int d^2\vec{b} e^{i\vec{b} \cdot (\vec{\bar{k}}_B-\vec{k}_B)} = (2\pi)^2 \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) $$ $$ ( _{out}\langle \{\vec{p}_f\}|\{\vec{k}_i|\}\rangle_{in}) = i \mathcal{M} (2 \pi)^4 \delta^{(4)}(\Sigma k_i - \Sigma p_f)$$ $$( _{out}\langle \{\vec{p}_f\}|\{\bar{\vec{k}}_i|\rangle_{in})^* = -i \mathcal{M}^* (2 \pi)^4 \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \tag{19.35} $$ Q. What's the definition of $k_B^{\perp}$ ( $\bar{k}_B^{\perp}$ )? EDIT : I think that $k_B^{\perp} := (k_B^{x},k_B^{y})$ , $\bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y}))$ . Accepting this, we can show the first equality in (19.35) ( ; $d^2 b = d b^x d b^y$ ? ). And ( using $\bar{k}_A = ( \bar{E}_A , \bar{k}_A^{x} , \bar{k}_A^{y} , \bar{k}_A^{z} )$ , $\bar{k}_B = ( \bar{E}_B , \bar{k}_B^{x} , \bar{k}_B^{y} , \bar{k}_A^{z} )$ and $p_f = (E_f, p_f^{x}, p_f^{y}, p_f^{z})$ ) $$\int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) $$ $$\stackrel{?}{=}  (k_i^{\perp} = \bar{k}_i^{\perp}) \times \int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f) $$ $$ = \int d \bar{k}_A^z \delta ( \sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A}$$ $$\stackrel{?}{=} \frac{1}{|\frac{\bar{k}^z_A}{\bar{E}_A} -\frac{\bar{k}_B^z}{\bar{E}_B} |} = \frac{1}{|v_A -v_B|}  \tag{19.36}$$ Putting everything together, we find the above (4.78). Q. My question is, why the equalities in (19.36) marked by question symbol  are true? Can we prove these more concretely? I think that this integral is one of the hardest one I've ever seen, because partially I don't know what is exact definition for $k_B^{\perp}$ ( or $\bar{k}_B^{\perp}$ ), $\bar{k}_A^z$ ( or $\bar{k}_B^z$ , $p_f^z$ ). EDIT : So far, I think that $k_B^{\perp} := (k_B^{x},k_B^{y})$ , $\bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y}))$ . Correct? For the first equality in (19.36) , perhaps, does next formula holds true? $$ \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) = 1 \operatorname{or} ( k_i^{\perp}= \bar{k}_i^{\perp})$$ ? If so, how? If this is true, then we maybe drive the first equiality in (19.36) (?) So far, I don't understand what the strange notation $(k_i^{\perp} = \bar{k}_i^{\perp})$ exactly means. Here, it seems that the definition of $k_B^{\perp}$ and $\bar{k}_B^{\perp}$ plays key role. EDIT : I think that the strange notation $(k_i^{\perp} = \bar{k}_i^{\perp})$ in (19.36) is negligible. For the first equality in (19.36), I think that we may use $$\int_{-\infty}^{\infty} \delta(x_i-x)dx_i = 1 $$ Note that $$\int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =\int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f) $$ $$\times \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) $$ So, if $$ \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =1 $$ , we obtain $$\int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =\int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f) $$ , which is the first equality in the (19.36) above. But I think that is true since, for example, $\delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y})$ ( C.f. Here we use $k_B^{\perp} := (k_B^{x},k_B^{y})$ , $\bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y})) $ is independent from variable $\bar{k}_A^{x}$ , this can be pulled outside the $\bar{k}_A^{x}$ -integral ; i.e., $$ \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp})$$ $$=  \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) $$ $$ = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) $$ $$ = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) \times 1$$ And so on.. This arguemnt really works? And for the final equality in (19.36) , my frist attempt is, I want to use that $$ \delta[f(x)] = \Sigma_{i} \frac{\delta(x-x_i)}{|f'(x_i)|}$$ if $f(x_i)=0, f'(x_i) \neq 0$ ; (C.f. Boas, mathematical methods in the physical sciences, p456) From this, we can show that $$ \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|}$$ (True?) And, let $$f(\bar{k}_A^{z}) := (\sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A} $$ Note that (by simply taking derivative) $$ \frac{d}{d \bar{k}_A^{z}} f(\bar{k}_A^z) = \frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}  $$ Then from this, how can we deduce the above final equality in (19.36) using $$ \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|}$$ ? What will be roots $x_i$ of $f( \bar{k}_A^{z})$ such that $f'(x_i) \neq 0$ ? EDIT : For this issue, let's look (19.36) more closely. First, $\int d \bar{k}_A^z \delta ( \sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A}$ is a 'number', which can be calculated by $ \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|}$ . Second, on the other side, $\frac{1}{|\frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}|} = \frac{1}{|\frac{d}{d \bar{k}_A^{z}} f(\bar{k}_A^z)|}$ is a 'function', which depends on variable $\bar{k}_A^{z}$ . Where does this discrepancy occurs? I guess that, the notation $\frac{1}{|\frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}|} $ means an 'implicit' notation indicating for $\Sigma_{i=1} \frac{1}{|f'(x_i)|}$ . True?","I am reading the Peskin & Schroeder's An introduction to Quantum field theory, p.105~p.106 (Construction of a cross-section from the invariant matrix element (p.104) ) and stuck at understanding some integration. I think that I never seen a pattern like this. EDIT : I edited this post to try to answer this question on my own. I wish this edited post will be helpful to readers. (And if there is anything that needs to be corrected, then it will be appreciate to inform me.) First, we are given (their book, p.105, (4.76):) And the goal is an expression ( p. 106, (4.78) ) : Through p.105~p.106 he derives this formula but I don't quite understand his argument in part. For example, ""..We can use the scond of these delta functions, together with the , to perform all six of the integrals in . Of the six (?) integrals, only those over and require some work. ( his book p.105 ).."" and ""Now recall that the initial wavepackets are localized in momentum space, centered on and . This means that we can evaluate all factors that are smooth functions of and at and , pulling them outside except the remaining delta function ( his book p.106, first paragraph ).. "" Second, meanwhile, I found next note (Introduction to quantum field theory by Nastase): https://professores.ift.unesp.br/ricardo.matheus/files/courses/2014tqc1/QFT1notes.pdf In p.176 ~ p.177 of his note, he notes that ( 19.35 , 19.36 ) Q. What's the definition of ( )? EDIT : I think that , . Accepting this, we can show the first equality in (19.35) ( ; ? ). And ( using , and ) Putting everything together, we find the above (4.78). Q. My question is, why the equalities in (19.36) marked by question symbol  are true? Can we prove these more concretely? I think that this integral is one of the hardest one I've ever seen, because partially I don't know what is exact definition for ( or ), ( or , ). EDIT : So far, I think that , . Correct? For the first equality in (19.36) , perhaps, does next formula holds true? ? If so, how? If this is true, then we maybe drive the first equiality in (19.36) (?) So far, I don't understand what the strange notation exactly means. Here, it seems that the definition of and plays key role. EDIT : I think that the strange notation in (19.36) is negligible. For the first equality in (19.36), I think that we may use Note that So, if , we obtain , which is the first equality in the (19.36) above. But I think that is true since, for example, ( C.f. Here we use , is independent from variable , this can be pulled outside the -integral ; i.e., And so on.. This arguemnt really works? And for the final equality in (19.36) , my frist attempt is, I want to use that if ; (C.f. Boas, mathematical methods in the physical sciences, p456) From this, we can show that (True?) And, let Note that (by simply taking derivative) Then from this, how can we deduce the above final equality in (19.36) using ? What will be roots of such that ? EDIT : For this issue, let's look (19.36) more closely. First, is a 'number', which can be calculated by . Second, on the other side, is a 'function', which depends on variable . Where does this discrepancy occurs? I guess that, the notation means an 'implicit' notation indicating for . True?"," d\sigma =(\prod_f\frac{d^3p_f}{(2\pi)^3}\frac{1}{2E_f}) \int d^2b(\prod_{i=A,B}\int \frac{d^3k_i}{(2\pi)^3}\frac{\phi_i(\vec{k}_i)}{\sqrt{2E_i}}\int \frac{d^3 \bar{k}_i}{(2\pi)^3}\frac{\phi_i^{*}(\bar{\vec{k}}_i)}{\sqrt{2\bar{E}_i}}) \times e^{\vec{b} \cdot (\bar{\vec{k}_B} - \vec{k}_B)} ( _{out}\langle \{\vec{p}_f\}|\{\vec{k}_i|\}\rangle_{in})( _{out}\langle \{\vec{p}_f\}|\{\bar{\vec{k}}_i|\rangle_{in})^*  \tag{4.76}.    d\sigma = (\prod_f\frac{d^3p_f}{(2\pi)^3}\frac{1}{2E_f})\frac{|\mathcal{M}(p_A, p_B \to \{p_f\})|^2}{2E_A 2E_B|v_A-v_B|}  \tag{4.78} \int \frac{d^3k_A}{(2 \pi)^3}\int\frac{d^3k_B}{(2\pi)^3}  \times |\phi_A(\vec{k}_A)|^2|\phi_B(\vec{k}_B)|^2 (2\pi)^4 \delta^{(4)}(k_A + k_B - \Sigma p_f). \delta^{(2)}(k_B^{\perp}-\bar{k}_B^{\perp}) \bar{k} (4.76) \bar{k}_A^z \bar{k}_B^z \vec{p}_A \vec{p}_B \vec{k}_A \vec{k}_B \vec{p}_A \vec{p}_B  \int d^2\vec{b} e^{i\vec{b} \cdot (\vec{\bar{k}}_B-\vec{k}_B)} = (2\pi)^2 \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp})   ( _{out}\langle \{\vec{p}_f\}|\{\vec{k}_i|\}\rangle_{in}) = i \mathcal{M} (2 \pi)^4 \delta^{(4)}(\Sigma k_i - \Sigma p_f) ( _{out}\langle \{\vec{p}_f\}|\{\bar{\vec{k}}_i|\rangle_{in})^* = -i \mathcal{M}^* (2 \pi)^4 \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \tag{19.35}  k_B^{\perp} \bar{k}_B^{\perp} k_B^{\perp} := (k_B^{x},k_B^{y}) \bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y})) d^2 b = d b^x d b^y \bar{k}_A = ( \bar{E}_A , \bar{k}_A^{x} , \bar{k}_A^{y} , \bar{k}_A^{z} ) \bar{k}_B = ( \bar{E}_B , \bar{k}_B^{x} , \bar{k}_B^{y} , \bar{k}_A^{z} ) p_f = (E_f, p_f^{x}, p_f^{y}, p_f^{z}) \int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp})  \stackrel{?}{=}  (k_i^{\perp} = \bar{k}_i^{\perp}) \times \int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f)   = \int d \bar{k}_A^z \delta ( \sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A} \stackrel{?}{=} \frac{1}{|\frac{\bar{k}^z_A}{\bar{E}_A} -\frac{\bar{k}_B^z}{\bar{E}_B} |} = \frac{1}{|v_A -v_B|}  \tag{19.36} k_B^{\perp} \bar{k}_B^{\perp} \bar{k}_A^z \bar{k}_B^z p_f^z k_B^{\perp} := (k_B^{x},k_B^{y}) \bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y}))  \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) = 1 \operatorname{or} ( k_i^{\perp}= \bar{k}_i^{\perp}) (k_i^{\perp} = \bar{k}_i^{\perp}) k_B^{\perp} \bar{k}_B^{\perp} (k_i^{\perp} = \bar{k}_i^{\perp}) \int_{-\infty}^{\infty} \delta(x_i-x)dx_i = 1  \int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =\int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f)  \times \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp})   \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =1  \int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =\int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f)  \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) k_B^{\perp} := (k_B^{x},k_B^{y}) \bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y}))  \bar{k}_A^{x} \bar{k}_A^{x}  \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =  \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y})   = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x})   = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) \times 1  \delta[f(x)] = \Sigma_{i} \frac{\delta(x-x_i)}{|f'(x_i)|} f(x_i)=0, f'(x_i) \neq 0  \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|} f(\bar{k}_A^{z}) := (\sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A}   \frac{d}{d \bar{k}_A^{z}} f(\bar{k}_A^z) = \frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}    \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|} x_i f( \bar{k}_A^{z}) f'(x_i) \neq 0 \int d \bar{k}_A^z \delta ( \sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A}  \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|} \frac{1}{|\frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}|} = \frac{1}{|\frac{d}{d \bar{k}_A^{z}} f(\bar{k}_A^z)|} \bar{k}_A^{z} \frac{1}{|\frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}|}  \Sigma_{i=1} \frac{1}{|f'(x_i)|}","['quantum-field-theory', 'integration']"
1,$\int_{0}^{1}\ln^2\Gamma(x)\ln\Gamma(1-x)\ dx$,,\int_{0}^{1}\ln^2\Gamma(x)\ln\Gamma(1-x)\ dx,"I came across the following integral $$\int_{0}^{1}\ln^2\Gamma(x)\ln\Gamma(1-x)\ dx$$ and I have no idea where to even start. I know you can interchange the $x$ and $1-x$ by king's property, and that $\int_{0}^{1}\ln^2\Gamma(x)dx$ has a nice closed form but otherwise I don't know what to do.","I came across the following integral and I have no idea where to even start. I know you can interchange the and by king's property, and that has a nice closed form but otherwise I don't know what to do.",\int_{0}^{1}\ln^2\Gamma(x)\ln\Gamma(1-x)\ dx x 1-x \int_{0}^{1}\ln^2\Gamma(x)dx,"['integration', 'definite-integrals', 'logarithms', 'gamma-function']"
2,Limit of $2^{n^2/2}\sum_{j=1}^{n/2} \sum_{k=1}^{n/2}\left(\cos^2(\frac{j \pi}{n+1}) + \cos^2(\frac{k \pi}{n+1})\right)$ as a double integral,Limit of  as a double integral,2^{n^2/2}\sum_{j=1}^{n/2} \sum_{k=1}^{n/2}\left(\cos^2(\frac{j \pi}{n+1}) + \cos^2(\frac{k \pi}{n+1})\right),"I am currently looking into Dimer coverings and my next step is to find how the following limit is calculated: $$\begin{align*} L &= \lim_{n \to \infty}\frac{1}{n^2}\ln\left(2^{n^2/2}\prod_{j=1}^{n/2} \prod_{k=1}^{n/2}\left(\cos^2(\frac{j \pi}{n+1}) + \cos^2(\frac{k \pi}{n+1})\right)\right) \\[1ex] &= \frac{1}{16 \pi^2}\int_{-\pi}^{\pi}\int_{-\pi}^{\pi}\ln[4+2\cos(x) + 2\cos(y)] \, dx \, dy \end{align*}$$ I think I got quite far, but there is some error in my calculation, which gives the required result, but the lower bound of my integrals is 0 instead of $- \pi$ . I will write my calculations down here. If anyone finds what I did wrong, please tell me. It would help a lot. $$\begin{align*} L &= \lim_{n \to \infty} \frac{1}{n^2} \ln\left(2^{n^2/2}\prod_{j=1}^{n/2} \prod_{k=1}^{n/2} \left(\cos^2\left(\frac{j \pi}{n+1}\right) + \cos^2\left(\frac{k \pi}{n+1}\right)\right)\right) \\[1ex] &= \lim_{n \to \infty} \frac{1}{n^2} \sum_{j=1}^{n/2} \sum_{k=1}^{n/2}\ln\left(4\left(\cos^2\left(\frac{j \pi}{n+1}\right) + \cos^2\left(\frac{k \pi}{n+1}\right)\right)\right) \\[1ex] &= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln(4(\cos^2(x)+\cos^2(y)) \, dx \, dy \\[1ex] &= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln\left(4\left(\frac12\cos(2x)+\frac12+\frac12\cos(2y)+\frac12\right)\right) \, dx \, dy\\ &= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln(2\cos(2x)+2\cos(2y)+4) \, dx \, dy\\ &= \frac{1}{16 \pi^2} \int_0^{\pi} \int_0^{\pi} \ln(2\cos(x)+2\cos(y)+4) \, dx \, dy \end{align*}$$","I am currently looking into Dimer coverings and my next step is to find how the following limit is calculated: I think I got quite far, but there is some error in my calculation, which gives the required result, but the lower bound of my integrals is 0 instead of . I will write my calculations down here. If anyone finds what I did wrong, please tell me. It would help a lot.","\begin{align*}
L &= \lim_{n \to \infty}\frac{1}{n^2}\ln\left(2^{n^2/2}\prod_{j=1}^{n/2} \prod_{k=1}^{n/2}\left(\cos^2(\frac{j \pi}{n+1}) + \cos^2(\frac{k \pi}{n+1})\right)\right) \\[1ex]
&= \frac{1}{16 \pi^2}\int_{-\pi}^{\pi}\int_{-\pi}^{\pi}\ln[4+2\cos(x) + 2\cos(y)] \, dx \, dy
\end{align*} - \pi \begin{align*}
L &= \lim_{n \to \infty} \frac{1}{n^2} \ln\left(2^{n^2/2}\prod_{j=1}^{n/2} \prod_{k=1}^{n/2} \left(\cos^2\left(\frac{j \pi}{n+1}\right) + \cos^2\left(\frac{k \pi}{n+1}\right)\right)\right) \\[1ex]
&= \lim_{n \to \infty} \frac{1}{n^2} \sum_{j=1}^{n/2} \sum_{k=1}^{n/2}\ln\left(4\left(\cos^2\left(\frac{j \pi}{n+1}\right) + \cos^2\left(\frac{k \pi}{n+1}\right)\right)\right) \\[1ex]
&= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln(4(\cos^2(x)+\cos^2(y)) \, dx \, dy \\[1ex]
&= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln\left(4\left(\frac12\cos(2x)+\frac12+\frac12\cos(2y)+\frac12\right)\right) \, dx \, dy\\
&= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln(2\cos(2x)+2\cos(2y)+4) \, dx \, dy\\
&= \frac{1}{16 \pi^2} \int_0^{\pi} \int_0^{\pi} \ln(2\cos(x)+2\cos(y)+4) \, dx \, dy
\end{align*}","['integration', 'limits', 'summation', 'riemann-sum']"
3,"If $f$ is real-valued bounded measurable and $\mu$ a complex measure, then $\left | \int_X f \mathrm d \mu \right | \le \int_X |f| \mathrm d |\mu|$","If  is real-valued bounded measurable and  a complex measure, then",f \mu \left | \int_X f \mathrm d \mu \right | \le \int_X |f| \mathrm d |\mu|,"Let $(X, \mathcal X)$ be a measurable space. Let $\mu$ be a complex measure on $X$ and $|\mu|$ its variation . Then $|\mu|$ is a non-negative finite measure. By definition, $|\mu(B)| \le |\mu| (B)$ for all $B \in \mathcal X$ . Let $\mu_1, \mu_2$ be the real and imaginary parts of $\mu$ . Then $\mu_1, \mu_2$ are finite signed measures such that $\mu = \mu_1 + i \mu_2$ . Let $(\mu_1^+, \mu_1^-)$ and $(\mu_2^+, \mu_2^-)$ be the Jordan decompositions of $\mu_1, \mu_2$ respectively. Then $\mu_1^+, \mu_1^-, \mu_2^+, \mu_2^-$ are non-negative finite measures such that $\mu_1 = \mu_1^+ - \mu_1^-$ and $\mu_2 = \mu_2^+ - \mu_2^-$ . Integration w.r.t. $\mu$ is defined as follows. If $f:X \to \mathbb R$ is measurable then $$ \begin{align} \int_X f \mathrm d \mu &:= \int_X f \mathrm d \mu_1 + i \int_X f \mathrm d \mu_2 \\ &:= \left [ \int_X f \mathrm d \mu_1^+ - \int_X f \mathrm d \mu_1^-  \right ] + \left [ \int_X f \mathrm d \mu_2^+ - \int_X f \mathrm d \mu_2^- \right ], \quad (\star) \end{align} $$ provided that each integral in $(\star)$ is well-defined. If $f:X \to \mathbb C$ is measurable then $$ \int_X f \mathrm d \mu := \int_X (\operatorname{Re} f) \mathrm d \mu  + i \int_X (\operatorname{Im} f) \mathrm d \mu. $$ In a proof of this result , I appealed to below inequality many times. Theorem: If $f:X \to \mathbb R$ measurable bounded, then $$ \left | \int_X f \mathrm d  \mu \right | \le \int_X |f| \mathrm d |\mu| $$ As such, I would like to prove it. Could you have a check on my attempt? Proof: For convenience, let $\alpha$ be the value of the LHS and $\beta$ that of the RHS. Let $f = 1_B$ with $B \in \mathcal X$ . So $f$ is a characteristic function. We have $\alpha = |\mu(B)|$ and $\beta = |\mu| (B)$ . The claim then holds. Let $f = \sum_{i=1}^m b_i 1_{B_1}$ with $b_i \in \mathbb R$ and $B_i \in \mathcal X$ such that $B_i \cap B_j \neq \emptyset \iff i=j$ . So $f$ is a simple function. We have $\alpha = |\sum_{i=1}^m b_i \mu(B_i)|$ and $\beta = \sum_{i=1}^m |b_i| \cdot |\mu| (B_i)$ . The claim then holds thanks to triangle inequality and (1.) Let $f:X \to \mathbb R$ be measurable bounded. There is a sequence $(f_n)$ of simple functions such that $(f_n)$ is uniformly bounded and that $f_n \to f$ pointwise everywhere . By applying DCT for each term in $(\star)$ , we have $$ \alpha = \left | \lim_n \int_X f_n \mathrm d  \mu \right | = \lim_n \left |  \int_X f_n \mathrm d  \mu \right | . $$ By (2.), we get $$ \alpha \le \lim_n \int_X |f_n| \mathrm d  |\mu|. $$ By DCT again, we have $$ \lim_n \int_X |f_n| \mathrm d  |\mu| = \int_X |f| \mathrm d  |\mu| = \beta. $$ This completes the proof.","Let be a measurable space. Let be a complex measure on and its variation . Then is a non-negative finite measure. By definition, for all . Let be the real and imaginary parts of . Then are finite signed measures such that . Let and be the Jordan decompositions of respectively. Then are non-negative finite measures such that and . Integration w.r.t. is defined as follows. If is measurable then provided that each integral in is well-defined. If is measurable then In a proof of this result , I appealed to below inequality many times. Theorem: If measurable bounded, then As such, I would like to prove it. Could you have a check on my attempt? Proof: For convenience, let be the value of the LHS and that of the RHS. Let with . So is a characteristic function. We have and . The claim then holds. Let with and such that . So is a simple function. We have and . The claim then holds thanks to triangle inequality and (1.) Let be measurable bounded. There is a sequence of simple functions such that is uniformly bounded and that pointwise everywhere . By applying DCT for each term in , we have By (2.), we get By DCT again, we have This completes the proof.","(X, \mathcal X) \mu X |\mu| |\mu| |\mu(B)| \le |\mu| (B) B \in \mathcal X \mu_1, \mu_2 \mu \mu_1, \mu_2 \mu = \mu_1 + i \mu_2 (\mu_1^+, \mu_1^-) (\mu_2^+, \mu_2^-) \mu_1, \mu_2 \mu_1^+, \mu_1^-, \mu_2^+, \mu_2^- \mu_1 = \mu_1^+ - \mu_1^- \mu_2 = \mu_2^+ - \mu_2^- \mu f:X \to \mathbb R 
\begin{align}
\int_X f \mathrm d \mu &:= \int_X f \mathrm d \mu_1 + i \int_X f \mathrm d \mu_2 \\
&:= \left [ \int_X f \mathrm d \mu_1^+ - \int_X f \mathrm d \mu_1^-  \right ] + \left [ \int_X f \mathrm d \mu_2^+ - \int_X f \mathrm d \mu_2^- \right ], \quad (\star)
\end{align}
 (\star) f:X \to \mathbb C 
\int_X f \mathrm d \mu := \int_X (\operatorname{Re} f) \mathrm d \mu  + i \int_X (\operatorname{Im} f) \mathrm d \mu.
 f:X \to \mathbb R 
\left | \int_X f \mathrm d  \mu \right | \le \int_X |f| \mathrm d |\mu|
 \alpha \beta f = 1_B B \in \mathcal X f \alpha = |\mu(B)| \beta = |\mu| (B) f = \sum_{i=1}^m b_i 1_{B_1} b_i \in \mathbb R B_i \in \mathcal X B_i \cap B_j \neq \emptyset \iff i=j f \alpha = |\sum_{i=1}^m b_i \mu(B_i)| \beta = \sum_{i=1}^m |b_i| \cdot |\mu| (B_i) f:X \to \mathbb R (f_n) (f_n) f_n \to f (\star) 
\alpha = \left | \lim_n \int_X f_n \mathrm d  \mu \right | = \lim_n \left |  \int_X f_n \mathrm d  \mu \right | .
 
\alpha \le \lim_n \int_X |f_n| \mathrm d  |\mu|.
 
\lim_n \int_X |f_n| \mathrm d  |\mu| = \int_X |f| \mathrm d  |\mu| = \beta.
","['integration', 'measure-theory', 'inequality']"
4,Does there exist a finite set of solutions to integrals such that any function composed of elementary functions is integrable?,Does there exist a finite set of solutions to integrals such that any function composed of elementary functions is integrable?,,"For indefinite integrals whose solutions cannot express with elementary functions, special functions are often defined, such as those shown below. $$ \mathrm{Si}(x) = \int_0^x\!\frac{\sin t}{t}\,\mathrm{d}t \qquad \mathrm{Li}(x) = \int_0^x\!\frac{1}{\ln t}\,\mathrm{d}t \qquad \mathrm{S}(x) = \int_0^x\!\sin(t^2)\,\mathrm{d}t $$ Through definining special functions as non-elementary integrals there exist cases where other functions also become integrable in terms of these newly defined special functions. An example is shown for the hyperbolic sine integral, whose solution is often given as a special function; however, this special function can be given in terms of the sine integral. $$ \mathrm{Shi}(x) = \int_0^x\!\frac{\sinh t}{t}\,\mathrm{d}t = \frac{\mathrm{Si}(ix)}{i} $$ Let $E$ be the set of all possible functions which can be created through combinations of the elementary functions. Examples are shown below for members of the set $E$ . $$ \frac{\sin x}{e^x} \qquad e^{x^2}\ln x \qquad \cosh\left(\frac{x^2 + 1}{\log_{10}(x)}\right)$$ Does there exist a finite set $S$ of defined solutions to non-elementary integrals, such that any function in the set $E$ can have its indefinite integral expressed as a combination of the functions in $S$ and the elementary functions?","For indefinite integrals whose solutions cannot express with elementary functions, special functions are often defined, such as those shown below. Through definining special functions as non-elementary integrals there exist cases where other functions also become integrable in terms of these newly defined special functions. An example is shown for the hyperbolic sine integral, whose solution is often given as a special function; however, this special function can be given in terms of the sine integral. Let be the set of all possible functions which can be created through combinations of the elementary functions. Examples are shown below for members of the set . Does there exist a finite set of defined solutions to non-elementary integrals, such that any function in the set can have its indefinite integral expressed as a combination of the functions in and the elementary functions?"," \mathrm{Si}(x) = \int_0^x\!\frac{\sin t}{t}\,\mathrm{d}t \qquad \mathrm{Li}(x) = \int_0^x\!\frac{1}{\ln t}\,\mathrm{d}t \qquad \mathrm{S}(x) = \int_0^x\!\sin(t^2)\,\mathrm{d}t   \mathrm{Shi}(x) = \int_0^x\!\frac{\sinh t}{t}\,\mathrm{d}t = \frac{\mathrm{Si}(ix)}{i}  E E  \frac{\sin x}{e^x} \qquad e^{x^2}\ln x \qquad \cosh\left(\frac{x^2 + 1}{\log_{10}(x)}\right) S E S","['integration', 'galois-theory', 'indefinite-integrals', 'elementary-functions', 'differential-field']"
5,Multipole-like integral on the unit disc,Multipole-like integral on the unit disc,,"I am interested in computing the following integrals \begin{align} I (\mathbf{x}_{1}) {} & = \!\! \int_{\mathbf{D}} \!\! \mathrm{d} \mathbf{x} \, \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}|}{\sqrt{1 \!-\! |\mathbf{x}|^{2}}} , \\ J (\mathbf{x}_{1} , \mathbf{x}_{2}) {} & = \!\! \int_{\mathbf{D}} \mathrm{d} \mathbf{x} \, \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}| \, |\mathbf{x}_{2} \!-\! \mathbf{x}|}{\sqrt{1 \!-\! |\mathbf{x}|^{2}}} , \end{align} where the integrals are to be performed on the unit disc, ${ \mathbf{D} \!=\! \{ \mathbf{x} \!\in\! \mathbb{R}^{2} \, | \, |\mathbf{x}| \!\leq\! 1 \} }$ , and the argument ${ \mathbf{x}_{1} , \mathbf{x}_{2} \!\in\! \mathbb{R^{2}} }$ can be both inside or outside the unit disc. I have tried numerous approaches to perform this integral. For example, to compute ${ I(\mathbf{x}_{1}) }$ I tried: (i) Writing a Fourier decomposition of the form \begin{align} |\mathbf{x}_{1} \!-\! \mathbf{x}| {} & = \mathrm{Max}[|\mathbf{x}_{1}|,|\mathbf{x}|] \, \sqrt{1 \!+\! \eta^{2} \!-\! 2 \eta \cos (\phi)} \nonumber \\ {} & = \sum_{\ell} \mathrm{e}^{\ell \phi} \, [\cdots] , \end{align} with $\phi$ the polar angle between $\mathbf{x}_{1}$ and $\mathbf{x}$ , ${ 0 \!\leq\! \eta \!\leq\! 1 }$ given by ${ \eta \!=\! \mathrm{Min}[|\mathbf{x}_{1}|,|\mathbf{x}|]/\mathrm{Max}[|\mathbf{x}_{1}|,|\mathbf{x}|] }$ , and the Fourier coefficients, ${ [\cdots] }$ , involving elliptic integrals of the second kind. (ii) Using the usual Legendre expansion of ${1/|\mathbf{x}_{1}\!-\!\mathbf{x}|}$ , to write \begin{align} |\mathbf{x}_{1} \!-\! \mathbf{x}| {} & = \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}|^{2}}{|\mathbf{x}_{1} \!-\! \mathbf{x}|} \nonumber \\ {} & = |\mathbf{x}_{1} \!-\! \mathbf{x}|^{2} \, \sum_{\ell} \eta^{\ell} \, P_{\ell} (\cos (\phi)) . \end{align} Unfortunately, these approaches did not prove much useful in evaluating explicitely the required integrals. From another argument, I know that for ${ |\mathbf{x}_{1}| \!\leq\! 1 }$ , one has \begin{equation} I (\mathbf{x}_{1}) = \tfrac{1}{2} \pi^{2} + \tfrac{1}{4} \pi^{2} |\mathbf{x}_{1}|^{2} , \end{equation} which I can reproduce by computing ${ I(\mathbf{x}_{1}) }$ numerically, but cannot recover analytically. My question are therefore as follows: How can one compute explicitly the integrals ${ I(\mathbf{x}_{1}) }$ and ${ J (\mathbf{x}_{1} , \mathbf{x}_{2}) }$ for arguments both inside and outside of the unit disc? What is the appropriate ""multipole expansion"" that one should use to represent ${ |\mathbf{x}_{1} \!-\! \mathbf{x}| }$ , so as to easily compute such integrals?","I am interested in computing the following integrals where the integrals are to be performed on the unit disc, , and the argument can be both inside or outside the unit disc. I have tried numerous approaches to perform this integral. For example, to compute I tried: (i) Writing a Fourier decomposition of the form with the polar angle between and , given by , and the Fourier coefficients, , involving elliptic integrals of the second kind. (ii) Using the usual Legendre expansion of , to write Unfortunately, these approaches did not prove much useful in evaluating explicitely the required integrals. From another argument, I know that for , one has which I can reproduce by computing numerically, but cannot recover analytically. My question are therefore as follows: How can one compute explicitly the integrals and for arguments both inside and outside of the unit disc? What is the appropriate ""multipole expansion"" that one should use to represent , so as to easily compute such integrals?","\begin{align}
I (\mathbf{x}_{1}) {} & = \!\! \int_{\mathbf{D}} \!\! \mathrm{d} \mathbf{x} \, \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}|}{\sqrt{1 \!-\! |\mathbf{x}|^{2}}} ,
\\
J (\mathbf{x}_{1} , \mathbf{x}_{2}) {} & = \!\! \int_{\mathbf{D}} \mathrm{d} \mathbf{x} \, \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}| \, |\mathbf{x}_{2} \!-\! \mathbf{x}|}{\sqrt{1 \!-\! |\mathbf{x}|^{2}}} ,
\end{align} { \mathbf{D} \!=\! \{ \mathbf{x} \!\in\! \mathbb{R}^{2} \, | \, |\mathbf{x}| \!\leq\! 1 \} } { \mathbf{x}_{1} , \mathbf{x}_{2} \!\in\! \mathbb{R^{2}} } { I(\mathbf{x}_{1}) } \begin{align}
|\mathbf{x}_{1} \!-\! \mathbf{x}| {} & = \mathrm{Max}[|\mathbf{x}_{1}|,|\mathbf{x}|] \, \sqrt{1 \!+\! \eta^{2} \!-\! 2 \eta \cos (\phi)}
\nonumber
\\
{} & = \sum_{\ell} \mathrm{e}^{\ell \phi} \, [\cdots] ,
\end{align} \phi \mathbf{x}_{1} \mathbf{x} { 0 \!\leq\! \eta \!\leq\! 1 } { \eta \!=\! \mathrm{Min}[|\mathbf{x}_{1}|,|\mathbf{x}|]/\mathrm{Max}[|\mathbf{x}_{1}|,|\mathbf{x}|] } { [\cdots] } {1/|\mathbf{x}_{1}\!-\!\mathbf{x}|} \begin{align}
|\mathbf{x}_{1} \!-\! \mathbf{x}| {} & = \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}|^{2}}{|\mathbf{x}_{1} \!-\! \mathbf{x}|}
\nonumber
\\
{} & = |\mathbf{x}_{1} \!-\! \mathbf{x}|^{2} \, \sum_{\ell} \eta^{\ell} \, P_{\ell} (\cos (\phi)) .
\end{align} { |\mathbf{x}_{1}| \!\leq\! 1 } \begin{equation}
I (\mathbf{x}_{1}) = \tfrac{1}{2} \pi^{2} + \tfrac{1}{4} \pi^{2} |\mathbf{x}_{1}|^{2} ,
\end{equation} { I(\mathbf{x}_{1}) } { I(\mathbf{x}_{1}) } { J (\mathbf{x}_{1} , \mathbf{x}_{2}) } { |\mathbf{x}_{1} \!-\! \mathbf{x}| }","['integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
6,Proof of a weighted norm inequality.,Proof of a weighted norm inequality.,,"I am reading the book Fourier Analysis by Javier Duandikoetxea and I am stuck in the proof of a lemma that it is the key part to prove the Hörmander's multiplier theorem. This is the lemma: Let $m\in L_a^2(\mathbb{R}^n),a>\frac{n}{2}$ and $\lambda>0$ . We define the operator: \begin{align*} 		T_{\lambda m}:L^2(\mathbb{R}^n)&\rightarrow L^2(\mathbb{R}^n) \\ 		f&\mapsto T_{\lambda m}(f)		 	\end{align*} where $\widehat{T_{\lambda m}(f)}(\xi)=m(\lambda\xi)\hat{f}(\xi)$ . Then $\forall~u:\mathbb{R}^n\to\mathbb{C}$ : $$\int_{\mathbb{R}^n}|T_{\lambda m}(f)(\xi)|^2u(\xi)d\xi\leq C\int_{\mathbb{R}^n}|f(\xi)|^2\mathcal{M}u(\xi)d\xi$$ where $C>0$ is a constant which is independent from $u$ and $\lambda$ and $\mathcal{M}$ is the Hardy-Littlewood maximal operator which is defined as: $$\mathcal{M}(f)(x)=\sup_{x\in Q}\frac{1}{|Q|}\int_{Q}|f(\xi)|d\xi$$ where $Q$ is any cube containing $x$ , and: $$L_a^2(\mathbb{R}^n)=\lbrace g:L^2(\mathbb{R}^n)\to L^2(\mathbb{R}^n)\mid (1+|\xi|^2)^{a/2}\hat{g}(\xi)\in L^2(\mathbb{R}^n)\rbrace$$ Proof: If $K =\hat{m}$ in then by our hypothesis, $(1 + |x|^2 )^{a/2}K(x) = R(x)\in L^2(\mathbb{R}^n)$ , and the kernel of $T_{\lambda m}$ is $\lambda^{-n}K(\lambda^{-1}x)$ . Hence: $$\int_{\mathbb{R}^n}|T_{\lambda m}(f)(\xi)|^2u(\xi)d\xi=\int_{\mathbb{R}^n}\left|\int_{\mathbb{R}^n}\frac{\lambda^{-n} R(\lambda^{-1}(\xi-x))}{(1+\left|\lambda^{-1}(\xi-x)\right|)^{a/2}}f(x)dx\right|^2u(\xi)d\xi\leq$$ $$\leq||m||_{L_a^2}^2\int_{\mathbb{R}^n}\int_{\mathbb{R}^n}\frac{\lambda^{-n}|f(x)|^2}{(1+\left|\lambda^{-1}(\xi-x)\right|)^{a/2}}dx u(\xi)d\xi\leq C_a ||m||_{L_a^2}^2\int_{\mathbb{R}^n}|f(x)|^2Mu(x)dx $$ Honestly I am quite lost with the proof, since it skips many  steps and it is hard for me to read it. It seems obvious that the first inequality should involve Cauchy-Scwarz and that the second inequality should involve Fubinni, but I am not able to write all the steps, especially the first equality has zero sense for me, since I can't get any of these symbols in my calculations. Sorry if I can't give many thoughts, but I don't have anything better. One of the things that annoys me is that I can't see what is the point of knowing what the kernel of $T_\lambda$ is. Any hints or help will be thanked.","I am reading the book Fourier Analysis by Javier Duandikoetxea and I am stuck in the proof of a lemma that it is the key part to prove the Hörmander's multiplier theorem. This is the lemma: Let and . We define the operator: where . Then : where is a constant which is independent from and and is the Hardy-Littlewood maximal operator which is defined as: where is any cube containing , and: Proof: If in then by our hypothesis, , and the kernel of is . Hence: Honestly I am quite lost with the proof, since it skips many  steps and it is hard for me to read it. It seems obvious that the first inequality should involve Cauchy-Scwarz and that the second inequality should involve Fubinni, but I am not able to write all the steps, especially the first equality has zero sense for me, since I can't get any of these symbols in my calculations. Sorry if I can't give many thoughts, but I don't have anything better. One of the things that annoys me is that I can't see what is the point of knowing what the kernel of is. Any hints or help will be thanked.","m\in L_a^2(\mathbb{R}^n),a>\frac{n}{2} \lambda>0 \begin{align*}
		T_{\lambda m}:L^2(\mathbb{R}^n)&\rightarrow L^2(\mathbb{R}^n) \\
		f&\mapsto T_{\lambda m}(f)		
	\end{align*} \widehat{T_{\lambda m}(f)}(\xi)=m(\lambda\xi)\hat{f}(\xi) \forall~u:\mathbb{R}^n\to\mathbb{C} \int_{\mathbb{R}^n}|T_{\lambda m}(f)(\xi)|^2u(\xi)d\xi\leq C\int_{\mathbb{R}^n}|f(\xi)|^2\mathcal{M}u(\xi)d\xi C>0 u \lambda \mathcal{M} \mathcal{M}(f)(x)=\sup_{x\in Q}\frac{1}{|Q|}\int_{Q}|f(\xi)|d\xi Q x L_a^2(\mathbb{R}^n)=\lbrace g:L^2(\mathbb{R}^n)\to L^2(\mathbb{R}^n)\mid (1+|\xi|^2)^{a/2}\hat{g}(\xi)\in L^2(\mathbb{R}^n)\rbrace K =\hat{m} (1 + |x|^2 )^{a/2}K(x) = R(x)\in L^2(\mathbb{R}^n) T_{\lambda m} \lambda^{-n}K(\lambda^{-1}x) \int_{\mathbb{R}^n}|T_{\lambda m}(f)(\xi)|^2u(\xi)d\xi=\int_{\mathbb{R}^n}\left|\int_{\mathbb{R}^n}\frac{\lambda^{-n} R(\lambda^{-1}(\xi-x))}{(1+\left|\lambda^{-1}(\xi-x)\right|)^{a/2}}f(x)dx\right|^2u(\xi)d\xi\leq \leq||m||_{L_a^2}^2\int_{\mathbb{R}^n}\int_{\mathbb{R}^n}\frac{\lambda^{-n}|f(x)|^2}{(1+\left|\lambda^{-1}(\xi-x)\right|)^{a/2}}dx u(\xi)d\xi\leq C_a ||m||_{L_a^2}^2\int_{\mathbb{R}^n}|f(x)|^2Mu(x)dx  T_\lambda","['integration', 'inequality', 'fourier-analysis', 'lp-spaces', 'fourier-transform']"
7,Proof of formula involving the Haar measure of SU(2).,Proof of formula involving the Haar measure of SU(2).,,"I would like to verify that $$\int_{\mathrm{SU}(2)}\mathrm{d}g\,\delta(g)=1$$ where the ""delta-function"" is defined via $$\delta(g):=\sum_{j\in\mathbb{N}_{0}/2}(1+2j)\chi^{j}(g)$$ where $\chi^{j}$ are the characters of the irreducible unitary spin-j representation of $\mathrm{SU}(2)$ with dimension $2j+1$ . My attempt: The normalized Haar measure of SU(2) can be written in terms of Euler angles $\alpha,\beta,\gamma$ as $$\int_{\mathrm{SU}(2)}\,\mathrm{d}g\,f(g)=\frac{1}{8\pi^{2}}\int_{0}^{2\pi}\,\mathrm{d}\alpha\,\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\int_{0}^{2\pi}\,\mathrm{d}\gamma\,f(\alpha,\beta,\gamma).$$ Furthermore, the characters $\chi_{j}$ can be written as $$\chi^{j}(g(\alpha,\beta,\gamma))=\frac{\mathrm{sin}((2j+1)\beta/2)}{\mathrm{sin}(\beta/2)}$$ according to this wikipedia article . Hence, we have to show that $$\int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\bigg (\sum_{j=0}^{\infty}\frac{j+1}{2}\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}\bigg )\overset{!}{=}1$$ (Here, I also rescaled the sum 2j $\to$ j). Now, since I am a physicist, I am allowed to change the infinite sum with the integral (maybe the error lies there; maybe I should check with dominated convergence theorem). Let us look at the integrals appearing in the expression above: $$\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}=\begin{cases}2 &\text{if j=0}\\ 4\frac{1+j-\mathrm{cos}(j\pi/2)}{j(2+j)} &\text{else}\end{cases}$$ Hence, I get something like $$\int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=1+\sum_{j=1}^{\infty}\frac{j+1}{2}\frac{4(1+j-\mathrm{cos}(j\pi/2))}{j(2+j)}$$ However, the series on the right-hand side is clearly not convergent: A quick mathematica calculation shows $$\sum_{j=1}^{N}\frac{j+1}{2}\frac{4(1+j-\mathrm{cos}(j\pi/2))}{j(2+j)}=\begin{cases}\frac{241}{11}\cong 22 &\text{N=10}\\\frac{1040350}{5151}\cong 202 &\text{N=100}\\\frac{1004003500}{501501}\cong 2002 &\text{N=1000}\\\vdots\end{cases}$$ I think the error lies indeed in exchanging the series with the integral. If I look at the series in $$\int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\bigg (\sum_{j=0}^{\infty}\frac{j+1}{2}\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}\bigg )$$ we have that $$\sum_{j=0}^{\infty}(j+1)\mathrm{sin}((j+1)\beta/2)=\sum_{j=1}^{\infty}j\cdot \mathrm{sin}\bigg(j\cdot\frac{\beta}{2}\bigg )$$ The series on the right-hand side seems to be zero, when evaluated with Mathematica, but I can't see why this is the case...","I would like to verify that where the ""delta-function"" is defined via where are the characters of the irreducible unitary spin-j representation of with dimension . My attempt: The normalized Haar measure of SU(2) can be written in terms of Euler angles as Furthermore, the characters can be written as according to this wikipedia article . Hence, we have to show that (Here, I also rescaled the sum 2j j). Now, since I am a physicist, I am allowed to change the infinite sum with the integral (maybe the error lies there; maybe I should check with dominated convergence theorem). Let us look at the integrals appearing in the expression above: Hence, I get something like However, the series on the right-hand side is clearly not convergent: A quick mathematica calculation shows I think the error lies indeed in exchanging the series with the integral. If I look at the series in we have that The series on the right-hand side seems to be zero, when evaluated with Mathematica, but I can't see why this is the case...","\int_{\mathrm{SU}(2)}\mathrm{d}g\,\delta(g)=1 \delta(g):=\sum_{j\in\mathbb{N}_{0}/2}(1+2j)\chi^{j}(g) \chi^{j} \mathrm{SU}(2) 2j+1 \alpha,\beta,\gamma \int_{\mathrm{SU}(2)}\,\mathrm{d}g\,f(g)=\frac{1}{8\pi^{2}}\int_{0}^{2\pi}\,\mathrm{d}\alpha\,\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\int_{0}^{2\pi}\,\mathrm{d}\gamma\,f(\alpha,\beta,\gamma). \chi_{j} \chi^{j}(g(\alpha,\beta,\gamma))=\frac{\mathrm{sin}((2j+1)\beta/2)}{\mathrm{sin}(\beta/2)} \int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\bigg (\sum_{j=0}^{\infty}\frac{j+1}{2}\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}\bigg )\overset{!}{=}1 \to \int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}=\begin{cases}2 &\text{if j=0}\\ 4\frac{1+j-\mathrm{cos}(j\pi/2)}{j(2+j)} &\text{else}\end{cases} \int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=1+\sum_{j=1}^{\infty}\frac{j+1}{2}\frac{4(1+j-\mathrm{cos}(j\pi/2))}{j(2+j)} \sum_{j=1}^{N}\frac{j+1}{2}\frac{4(1+j-\mathrm{cos}(j\pi/2))}{j(2+j)}=\begin{cases}\frac{241}{11}\cong 22 &\text{N=10}\\\frac{1040350}{5151}\cong 202 &\text{N=100}\\\frac{1004003500}{501501}\cong 2002 &\text{N=1000}\\\vdots\end{cases} \int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\bigg (\sum_{j=0}^{\infty}\frac{j+1}{2}\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}\bigg ) \sum_{j=0}^{\infty}(j+1)\mathrm{sin}((j+1)\beta/2)=\sum_{j=1}^{\infty}j\cdot \mathrm{sin}\bigg(j\cdot\frac{\beta}{2}\bigg )","['integration', 'definite-integrals', 'representation-theory', 'lie-groups', 'haar-measure']"
8,"""Improper"" Stokes theorem","""Improper"" Stokes theorem",,"In this question I will use Stokes' theorem with manifolds (well, domains in manifolds), rather than with chains. Let $M$ be a smooth manifold of dimension $m$ . A subset $D\subseteq M$ is called a regular domain if 1) $D$ is compact, 2) $D$ is the closure of its interior, 3) the topological boundary $\partial D$ is a smooth hypersurface in $M$ (everything should also be valid when $\partial M$ is a ""manifold with corners"", i.e. it is piecewise smooth instead of smooth, but I now ignore that case for simplicity). Stokes' theorem then states that $$ \int_{\partial D}\omega=\int_D d\omega, $$ where $\omega$ is a $C^1$ $m-1$ -form and the pullback to $\partial D$ on the left hand side is implicit. The compactness of $D$ as well as the regularity of $\omega$ ensures that both integrals are convergent. In the textbook G. de Rham: Differentiable Manifolds , there is a counterexample that shows that Stokes' theorem need not be valid when $D$ is noncompact (and $\omega$ has no compact support), even if everything converges. As an example, let $D=(-\infty,0]$ and $f$ is a function such that $f(x)= -1$ for $x\le -1$ and $f(x)=0$ for $x\ge 0$ . The boundary of $D$ is $\partial D=+\{0\}$ , where the $+$ sign means that this point is assigned a positive oriantation, then $$ \int_D df=\int_{-\infty}^0 df=1, $$ but $$ \int_{\partial D}f=f(0)=0, $$ which shows that Stokes' theorem does not apply to this case. The problem is that the integral $\int_D df$ is calculated as $\int_D df=\int_{-\infty}^0f^\prime(x)dx=f(0)-\lim_{r\rightarrow\infty}f(-r)$ , i.e. it is an improper Riemann integral, and a version of Stokes' theorem is obeyed with $D$ having a ""fictious"" boundary piece $-\{-\infty\}$ (the outer sign denotes orientation). But of course as a manifold with boundary, $\partial D=+\{0\}$ , and the ""boundary piece at infinity"" is missing here and this is why Stokes' theorem fails. A similar situation is often encountered in theoretical physics, where one often integrates over the entirety of $\mathbb R^3$ . As a manifold, $\partial\mathbb R^3=\varnothing$ and Stokes' theorem says that $\int_{\mathbb R^3}d\omega=0$ if $\omega$ has compact support and does not say anything otherwise. However in physics, such and integral would be considered with $\mathbb R^3$ having a fictious ""boundary at infinity"". In fact, what happens (often implicitly) is that instead of integrating over $\mathbb R^3$ , one integrates over the closed ball $\bar B_r\equiv \bar B_r(0)$ of radius $r$ , which is a regular domain whose boundary is $S_r\equiv S_r(0)$ the $2$ -sphere of radius $r$ . One may then take apply Stokes' theorem to $\mathbb R^3$ itself by $$ \lim_{r\rightarrow\infty}\int_{\bar B_r}d\omega=\lim_{r\rightarrow\infty}\int_{S_r}\omega, $$ with the compact closed balls $\bar B_r$ providing an exhaustion of $\mathbb R^3$ with compact sets. Now, unlike calculus texts where improper Riemann integrals are frequently treated, I have never ever seen a textbook on differential geometry, where integration and Stokes' theorem are treated in a way that improper integrals are incorporated. Ideally there would be a formalism where the Stokes' theorem $\int_M d\omega=\int_{\partial M}\omega$ could be interpreted for any $m$ -manifold (with or without boundary) $M$ and any $m-1$ -form $\omega$ such that the left hand side converges and where $\partial M$ is allowed to have ""fictious"" pieces at infinity. Probably such a version of Stokes' theorem would be defined in terms of exhaustions by regular domains, but of course there are questions I don't know the answer to such as Does every $M$ admit such an exhaustion? or Is the value of the integral independent of the exhaustion? I am looking for resources (textbooks, papers) where a coherent theory of ""improper integrals"" on manifolds (including Stokes' theorem!) is elaborated.","In this question I will use Stokes' theorem with manifolds (well, domains in manifolds), rather than with chains. Let be a smooth manifold of dimension . A subset is called a regular domain if 1) is compact, 2) is the closure of its interior, 3) the topological boundary is a smooth hypersurface in (everything should also be valid when is a ""manifold with corners"", i.e. it is piecewise smooth instead of smooth, but I now ignore that case for simplicity). Stokes' theorem then states that where is a -form and the pullback to on the left hand side is implicit. The compactness of as well as the regularity of ensures that both integrals are convergent. In the textbook G. de Rham: Differentiable Manifolds , there is a counterexample that shows that Stokes' theorem need not be valid when is noncompact (and has no compact support), even if everything converges. As an example, let and is a function such that for and for . The boundary of is , where the sign means that this point is assigned a positive oriantation, then but which shows that Stokes' theorem does not apply to this case. The problem is that the integral is calculated as , i.e. it is an improper Riemann integral, and a version of Stokes' theorem is obeyed with having a ""fictious"" boundary piece (the outer sign denotes orientation). But of course as a manifold with boundary, , and the ""boundary piece at infinity"" is missing here and this is why Stokes' theorem fails. A similar situation is often encountered in theoretical physics, where one often integrates over the entirety of . As a manifold, and Stokes' theorem says that if has compact support and does not say anything otherwise. However in physics, such and integral would be considered with having a fictious ""boundary at infinity"". In fact, what happens (often implicitly) is that instead of integrating over , one integrates over the closed ball of radius , which is a regular domain whose boundary is the -sphere of radius . One may then take apply Stokes' theorem to itself by with the compact closed balls providing an exhaustion of with compact sets. Now, unlike calculus texts where improper Riemann integrals are frequently treated, I have never ever seen a textbook on differential geometry, where integration and Stokes' theorem are treated in a way that improper integrals are incorporated. Ideally there would be a formalism where the Stokes' theorem could be interpreted for any -manifold (with or without boundary) and any -form such that the left hand side converges and where is allowed to have ""fictious"" pieces at infinity. Probably such a version of Stokes' theorem would be defined in terms of exhaustions by regular domains, but of course there are questions I don't know the answer to such as Does every admit such an exhaustion? or Is the value of the integral independent of the exhaustion? I am looking for resources (textbooks, papers) where a coherent theory of ""improper integrals"" on manifolds (including Stokes' theorem!) is elaborated.","M m D\subseteq M D D \partial D M \partial M  \int_{\partial D}\omega=\int_D d\omega,  \omega C^1 m-1 \partial D D \omega D \omega D=(-\infty,0] f f(x)= -1 x\le -1 f(x)=0 x\ge 0 D \partial D=+\{0\} +  \int_D df=\int_{-\infty}^0 df=1,   \int_{\partial D}f=f(0)=0,  \int_D df \int_D df=\int_{-\infty}^0f^\prime(x)dx=f(0)-\lim_{r\rightarrow\infty}f(-r) D -\{-\infty\} \partial D=+\{0\} \mathbb R^3 \partial\mathbb R^3=\varnothing \int_{\mathbb R^3}d\omega=0 \omega \mathbb R^3 \mathbb R^3 \bar B_r\equiv \bar B_r(0) r S_r\equiv S_r(0) 2 r \mathbb R^3  \lim_{r\rightarrow\infty}\int_{\bar B_r}d\omega=\lim_{r\rightarrow\infty}\int_{S_r}\omega,  \bar B_r \mathbb R^3 \int_M d\omega=\int_{\partial M}\omega m M m-1 \omega \partial M M","['integration', 'differential-geometry', 'improper-integrals', 'differential-topology', 'stokes-theorem']"
9,Can the LCT and MCT for Lebesgue integrable functions be viewed as a lattice completeness result?,Can the LCT and MCT for Lebesgue integrable functions be viewed as a lattice completeness result?,,"The set of Lebesgue integrable functions form a lattice under pointwise min and max (also more generally for R, Henstock-Kurzweil integrable functions with an upper or lower bound form a lattice as well). The convergence theorems look a lot like countable completeness for that lattice. Is there an alternative formulation or generalization of them that is exactly that or further in that direction?","The set of Lebesgue integrable functions form a lattice under pointwise min and max (also more generally for R, Henstock-Kurzweil integrable functions with an upper or lower bound form a lattice as well). The convergence theorems look a lot like countable completeness for that lattice. Is there an alternative formulation or generalization of them that is exactly that or further in that direction?",,"['integration', 'lebesgue-integral', 'lebesgue-measure', 'lattice-orders', 'gauge-integral']"
10,"Solution integral $\;\displaystyle \iint \sqrt{\cos^2(x \pi)+\sin^2(y \pi)} \ dx\,dy$",Solution integral,"\;\displaystyle \iint \sqrt{\cos^2(x \pi)+\sin^2(y \pi)} \ dx\,dy","Working on a hobby project: ""Circle from (2D) random walk"" [SE] and came across this integral: $$\bar{R}=\int_0^1 \int_0^1 \sqrt{\cos^2(X  \pi)+\sin^2(Y  \pi)} \ dX\,dY$$ My intention is to have the mean vector length of every vector (starting in origin) in a square: $x \in [0,1]$ and $y \in [0,1]$ where: $x=\cos(X  \pi)$ and $y=\sin(Y  \pi)$ . Initial I solved numerical with Python (taking sample of vectors): import numpy as np  x=np.linspace(-np.pi/2,0,1001) y=np.linspace(0,np.pi/2,1001)  X,Y =np.meshgrid(x,y)  def radius(x,y):     return np.sqrt((np.cos(x))**2+(np.sin(y))**2)  z=np.array([radius(x,y) for (x,y) in zip(np.ravel(X), np.ravel(Y))])  print(np.mean(z)) Giving: $$\bar{R}=0.95802...$$ Solving integral with Wolfram Alpha (online) gives: integral \sqrt(cos^2(x*pi)+sin^2(y*pi)) dxdy from x=0 to 1 and y=0 to 1 $$\bar{R}=0.958091\ldots$$ Values seems to match and looks like I am taking the mean vector length within square. $X$ and $Y$ are random values between $[0,2]$ in original problem. Is this integral known? And how to solve for it? I noticed that I can replace $sin^{2}$ for $cos^{2}$ giving: $$\bar{R}=\int_0^1 \int_0^1 \sqrt{\cos^2(X\pi) + \cos^2(Y\pi)} \ dX\,dY$$ or: $$\bar{R}=\int_0^1 \int_0^1 \sqrt{\sin^2(X\pi) + \sin^2(Y\pi)} \ dX\,dY$$ Does not help me gain more feeling. I would like to learn more about this integral where to start? And how do solutions (without intervals) look like? EDIT: original formula without $\cos$ and $\sin$ looks like: $\;\displaystyle \bar{R}=\frac{1}{a^2} \int_0^a \int_0^a \sqrt{x^2+y^2} \ dx\,dy$ . Here Wolfram Alpha (online) gives complicated overwhelming formula. Not sure if nice compact solution exists.","Working on a hobby project: ""Circle from (2D) random walk"" [SE] and came across this integral: My intention is to have the mean vector length of every vector (starting in origin) in a square: and where: and . Initial I solved numerical with Python (taking sample of vectors): import numpy as np  x=np.linspace(-np.pi/2,0,1001) y=np.linspace(0,np.pi/2,1001)  X,Y =np.meshgrid(x,y)  def radius(x,y):     return np.sqrt((np.cos(x))**2+(np.sin(y))**2)  z=np.array([radius(x,y) for (x,y) in zip(np.ravel(X), np.ravel(Y))])  print(np.mean(z)) Giving: Solving integral with Wolfram Alpha (online) gives: integral \sqrt(cos^2(x*pi)+sin^2(y*pi)) dxdy from x=0 to 1 and y=0 to 1 Values seems to match and looks like I am taking the mean vector length within square. and are random values between in original problem. Is this integral known? And how to solve for it? I noticed that I can replace for giving: or: Does not help me gain more feeling. I would like to learn more about this integral where to start? And how do solutions (without intervals) look like? EDIT: original formula without and looks like: . Here Wolfram Alpha (online) gives complicated overwhelming formula. Not sure if nice compact solution exists.","\bar{R}=\int_0^1 \int_0^1 \sqrt{\cos^2(X  \pi)+\sin^2(Y  \pi)} \ dX\,dY x \in [0,1] y \in [0,1] x=\cos(X  \pi) y=\sin(Y  \pi) \bar{R}=0.95802... \bar{R}=0.958091\ldots X Y [0,2] sin^{2} cos^{2} \bar{R}=\int_0^1 \int_0^1 \sqrt{\cos^2(X\pi) + \cos^2(Y\pi)} \ dX\,dY \bar{R}=\int_0^1 \int_0^1 \sqrt{\sin^2(X\pi) + \sin^2(Y\pi)} \ dX\,dY \cos \sin \;\displaystyle \bar{R}=\frac{1}{a^2} \int_0^a \int_0^a \sqrt{x^2+y^2} \ dx\,dy",['integration']
11,Solving a pair of dual integral equations,Solving a pair of dual integral equations,,"I have the equations $$ \int\limits_0^\infty dk \ A(k)k \sinh(ka)\cos(kx)=0 \ \ ; \ \ 1<|x|<\infty  \tag{1} $$ $$ \int\limits_0^\infty dk \  A(k) \cosh(ka) \cos(kx)=1  \ \ ; \ \ |x|<1 \tag{2} $$ Where $A(k)$ is unknown, $a$ is a real positive constant, and we expect $A=A(k,a)$ . I think the general idea is to write $$ \left[ A(k) \times \operatorname{convenient stuff}(k)\right]=\int\limits_0^1dx' \ g(x') \cos(kx') \tag{*} $$ Where $g$ is the new unknown, and (*) is chosen such that (1) is satisfied, then substitute this into (2) so that the integral over $k$ may be performed. Question in brief: What substitution in (*) will allow me to make progress? Or is there a different 'trick' here altogether? Maybe an expansion on the right in something other than cosines? The rest of this post is background, and my current attempt. In a similar problem $$ \int\limits_0^\infty dk \ A(k) k \sin(kx) =0 \ \ ; \ \ 1<|x|<\infty \tag{3} $$ $$ \int\limits_0^\infty dk \ A(k) \tanh(ka)\sin(kx)=1 \ \ ; \ \ |x|<1 \tag{4} $$ Setting $$ A(k) k = \int\limits_0^1 dx' \ g(x') \sin(kx') $$ Allows one to make progress by substituting into (4), and even (eventually) obtain $g$ analytically. I have tried the similar substitution $$ A(k) k \sinh(ak)=\int\limits_0^1 dx' g(x') \cos(kx') $$ So that (1) is satisfied, but substituting this into (2) is not fruitful (divergent integral over $k$ ). It is possible that the Fourier expansion in $\cos$ on the RHS of (*) is not the way to go here. Maybe expanding in Bessel functions or something else; but I don't have the intuition to 'see' what is a good expansion. For example, in a (different) similar problem, a useful expansion is $$ \left[ A(k) \times \operatorname{convenient stuff}(k)\right]=\int\limits_0^1dx' \ g(x') J_0(kx') $$ Where $J_0$ is a Bessel function of the first kind. The similar problems mentioned above are from: Mixed boundary value problems by Dean G. Duffy. Current progress By integrating (1) and (2) w.r.t. $x$ we obtain $$ \int\limits_0^\infty dk \ A(k) \sinh(ka)\sin(kx)=0 \ \ ; \ \ 1<|x|<\infty  \tag{1a} $$ $$ \int\limits_0^\infty dk \  A(k) k^{-1} \cosh(ka) \sin(kx)=x  \ \ ; \ \ |x|<1 \tag{2a} $$ With the ansatz $$ A(k) \cosh(ka) = \int\limits_0^1 dx' \ g(x') \sin(kx') \tag{**} $$ After substituting (**) into (2a), and some algebra, we find $$ A(k) \cosh(ka) = J_1(k) $$ However, to justify this we must show it satisfies (1a) $$ \int\limits_0^\infty dk \ \int\limits_0^1 dx' \ g(x') \sin(kx') \sin(kx) \tanh(ka) \stackrel{?}{=}0 \ \ ; \ \ 1<|x|<\infty  \tag{1b} $$ Within the integral, we always have $x \neq x'$ , so it would be nice to say (distributionally) this is true, but I'm not sure that it is. Alternative question: is (1b) true? Update: Numerically integrating (1b) shows that it's not true.","I have the equations Where is unknown, is a real positive constant, and we expect . I think the general idea is to write Where is the new unknown, and (*) is chosen such that (1) is satisfied, then substitute this into (2) so that the integral over may be performed. Question in brief: What substitution in (*) will allow me to make progress? Or is there a different 'trick' here altogether? Maybe an expansion on the right in something other than cosines? The rest of this post is background, and my current attempt. In a similar problem Setting Allows one to make progress by substituting into (4), and even (eventually) obtain analytically. I have tried the similar substitution So that (1) is satisfied, but substituting this into (2) is not fruitful (divergent integral over ). It is possible that the Fourier expansion in on the RHS of (*) is not the way to go here. Maybe expanding in Bessel functions or something else; but I don't have the intuition to 'see' what is a good expansion. For example, in a (different) similar problem, a useful expansion is Where is a Bessel function of the first kind. The similar problems mentioned above are from: Mixed boundary value problems by Dean G. Duffy. Current progress By integrating (1) and (2) w.r.t. we obtain With the ansatz After substituting (**) into (2a), and some algebra, we find However, to justify this we must show it satisfies (1a) Within the integral, we always have , so it would be nice to say (distributionally) this is true, but I'm not sure that it is. Alternative question: is (1b) true? Update: Numerically integrating (1b) shows that it's not true.","
\int\limits_0^\infty dk \ A(k)k \sinh(ka)\cos(kx)=0 \ \ ; \ \ 1<|x|<\infty  \tag{1}
 
\int\limits_0^\infty dk \  A(k) \cosh(ka) \cos(kx)=1  \ \ ; \ \ |x|<1 \tag{2}
 A(k) a A=A(k,a) 
\left[ A(k) \times \operatorname{convenient stuff}(k)\right]=\int\limits_0^1dx' \ g(x') \cos(kx') \tag{*}
 g k 
\int\limits_0^\infty dk \ A(k) k \sin(kx) =0 \ \ ; \ \ 1<|x|<\infty \tag{3}
 
\int\limits_0^\infty dk \ A(k) \tanh(ka)\sin(kx)=1 \ \ ; \ \ |x|<1 \tag{4}
 
A(k) k = \int\limits_0^1 dx' \ g(x') \sin(kx')
 g 
A(k) k \sinh(ak)=\int\limits_0^1 dx' g(x') \cos(kx')
 k \cos 
\left[ A(k) \times \operatorname{convenient stuff}(k)\right]=\int\limits_0^1dx' \ g(x') J_0(kx')
 J_0 x 
\int\limits_0^\infty dk \ A(k) \sinh(ka)\sin(kx)=0 \ \ ; \ \ 1<|x|<\infty  \tag{1a}
 
\int\limits_0^\infty dk \  A(k) k^{-1} \cosh(ka) \sin(kx)=x  \ \ ; \ \ |x|<1 \tag{2a}
 
A(k) \cosh(ka) = \int\limits_0^1 dx' \ g(x') \sin(kx') \tag{**}
 
A(k) \cosh(ka) = J_1(k)
 
\int\limits_0^\infty dk \ \int\limits_0^1 dx' \ g(x') \sin(kx') \sin(kx) \tanh(ka) \stackrel{?}{=}0 \ \ ; \ \ 1<|x|<\infty  \tag{1b}
 x \neq x'","['integration', 'special-functions', 'mathematical-physics', 'bessel-functions', 'integral-equations']"
12,Generalizing Contour Integration to Quaternions,Generalizing Contour Integration to Quaternions,,"I have recently entertained the possibility of defining complex contour integration for the quaternions. I am somewhat aware that the Frobenius theorem dictates that no division algebra can exist in $\mathbb{R}^3$ ; however, Cayley-Dickinson constructions can generalize these division algebras to spaces of the form $\mathbb{R}^{2^n}$ at the expense of commutativity and later associativity. Even if the holomorphicity and analyticity are not equivalent (although they are for $\mathbb{C}$ )  generalizations of the Cauchy-Riemann equations have been made to higher dimensions with some success. [ 1 ] Additionally, a field analyzing differential quaternions exists in computer graphics, though I believe it is more concerned with aptly representing rotations in $\mathbb{R}^3$ . [ 2 ] Would it be reasonable to believe that contour integration may be generalized to one of these division algebras by designing a hypercomplex function on a 1-manifold map (i.e., one that is diffeomorphic to a subset of $\mathbb{R}$ .) Although Liouville's theorem requires that the amount of holomorphic maps significantly decreases as the dimensionality increases and those that do exist must be representable as a composition of Mobius transforms, would a reasonable definition of quaternion complex integration be the sum of standard contour integration along each component of the one manifold for a given basis? (I also assume that the result would have to be invariant of the choice of basis in order to be well-defined.) In particular, I'm wondering if a generalization of contour integration could be used to define a biholomorphic map (if not a holomorphic map) in the quaternions or complex-like numbers spaces above? I'm very new to the topic and haven't the experience of many members on this website. As a semi-related question, why does the existence of quaternions not provide for the existence of division algebras in $\mathbb{R}^3$ ? I assume that taking the subset of all quaternions where only the same three components are nonzero would not suffice as the existence of a multiplicative inverse or closure under division would not hold. As a final soft question, does anyone suggest textbooks for hypercomplex numbers? I had considered purchasing Hypercomplex Numbers by Isaiah Kantor. Are there any papers that expound on the notion of integration on Hypercomplex Manifolds described on Wikipedia? [ 3 ] Thank you all.","I have recently entertained the possibility of defining complex contour integration for the quaternions. I am somewhat aware that the Frobenius theorem dictates that no division algebra can exist in ; however, Cayley-Dickinson constructions can generalize these division algebras to spaces of the form at the expense of commutativity and later associativity. Even if the holomorphicity and analyticity are not equivalent (although they are for )  generalizations of the Cauchy-Riemann equations have been made to higher dimensions with some success. [ 1 ] Additionally, a field analyzing differential quaternions exists in computer graphics, though I believe it is more concerned with aptly representing rotations in . [ 2 ] Would it be reasonable to believe that contour integration may be generalized to one of these division algebras by designing a hypercomplex function on a 1-manifold map (i.e., one that is diffeomorphic to a subset of .) Although Liouville's theorem requires that the amount of holomorphic maps significantly decreases as the dimensionality increases and those that do exist must be representable as a composition of Mobius transforms, would a reasonable definition of quaternion complex integration be the sum of standard contour integration along each component of the one manifold for a given basis? (I also assume that the result would have to be invariant of the choice of basis in order to be well-defined.) In particular, I'm wondering if a generalization of contour integration could be used to define a biholomorphic map (if not a holomorphic map) in the quaternions or complex-like numbers spaces above? I'm very new to the topic and haven't the experience of many members on this website. As a semi-related question, why does the existence of quaternions not provide for the existence of division algebras in ? I assume that taking the subset of all quaternions where only the same three components are nonzero would not suffice as the existence of a multiplicative inverse or closure under division would not hold. As a final soft question, does anyone suggest textbooks for hypercomplex numbers? I had considered purchasing Hypercomplex Numbers by Isaiah Kantor. Are there any papers that expound on the notion of integration on Hypercomplex Manifolds described on Wikipedia? [ 3 ] Thank you all.",\mathbb{R}^3 \mathbb{R}^{2^n} \mathbb{C} \mathbb{R}^3 \mathbb{R} \mathbb{R}^3,"['integration', 'quaternions', 'conformal-geometry']"
13,Closed forms for functions involved in calculating $\int_{x}^{\infty}\frac{e^{-at}}{t^{n}\left(e^{-t}\pm1\right)}dt$,Closed forms for functions involved in calculating,\int_{x}^{\infty}\frac{e^{-at}}{t^{n}\left(e^{-t}\pm1\right)}dt,"Start with the generating functions for the Bernoulli/Euler Polynomials . $$ \begin{align} &\frac{e^{-ax}}{x^{n}\left(e^{-x}-1\right)}=-\sum_{k=0}^{\infty}\left(-1\right)^{k}B_k\left(a\right)\frac{x^{k-n-1}}{k!},\qquad |x|<2\pi\\ &\frac{2e^{-ax}}{x^{n}\left(e^{-x}+1\right)}=\space\space\space\sum_{k=0}^{\infty}\left(-1\right)^{k}E_k\left(a\right)\frac{x^{k-n}}{k!},\quad\qquad |x|\le\pi \end{align} $$ Integrating from x to $\infty$ , we have $$ \begin{align} &\int_{x}^{\infty}\frac{e^{-at}}{t^{n}\left(e^{-t}-1\right)}dt\\\tag{1} &=C^-_{n}(a)\\ &+\sum_{k=0}^{n-1}\left(-1\right)^{k}B_k\left(a\right)\frac{x^{k-n}}{k!\left(k-n\right)}+\left(-1\right)^{n}B_n\left(a\right)\frac{1}{n!}\ln x+\sum_{k=n+1}^{\infty}\left(-1\right)^{k}B_k\left(a\right)\frac{x^{k-n}}{k!\left(k-n\right)},\qquad |x|<2\pi \\\\\\ &\int_x^\infty\frac{2e^{-at}}{t^n(e^{-t}+1)}dt\\\tag{2} &=C^+_{n}(a)\\ &-\left(\sum_{k=0}^{n-2}\left(-1\right)^{k}E_k\left(a\right)\frac{x^{k-n+1}}{k!\left(k-n+1\right)}+\left(-1\right)^{n-1}E_{n-1}\left(a\right)\frac{1}{\left(n-1\right)!}\ln{x}+\sum_{k=n}^{\infty}\left(-1\right)^{k}E_k\left(a\right)\frac{x^{k-n+1}}{k!\left(k-n+1\right)}\right),\\ &\qquad\qquad |x|\le\pi \end{align} $$ I'm interested in the constants of integration $C^-$ and $C^+$ . Some values for $C^+$ are listed below: $$ \begin{align} C^+_{1}(1)&=\ln\pi-\ln2-\gamma\\ C^+_{1}(2)&=-\ln\pi+\ln2-\gamma\\ C^+_{1}(3)&=\ln\pi-3\ln2-\gamma\\ \\ C^+_{2}(1)&=-6\ln A+\frac{2}{3}\ln2+\frac{1}{2}\gamma\\ C^+_{2}(2)&=6\ln A-\frac{2}{3}\ln2+\frac{3}{2}\gamma-2\\ C^+_{2}(3)&=-6\ln A+\frac{14}{3}\ln2+\frac{5}{2}\gamma-2\\ \\ C^+_{3}(1)&=\frac{7}{4\pi^{2}}\zeta(3)\\ C^+_{3}(2)&=-\frac{7}{4\pi^{2}}\zeta(3)-\gamma+\frac{3}{2}\\ C^+_{3}(3)&=\frac{7}{4\pi^{2}}\zeta(3)-4\ln2-3\gamma+\frac{9}{2}\\ \\ C^+_{4}(1)&=5\zeta'(-3)-\frac{2}{45}\ln2-\frac{1}{24}\gamma+\frac{11}{144}\\ C^+_{4}(2)&=-5\zeta'(-3)+\frac{2}{45}\ln2+\frac{3}{8}\gamma-\frac{11}{16}\\ C^+_{4}(3)&=5\zeta'(-3)+\frac{118}{45}\ln2+\frac{55}{24}\gamma-\frac{605}{144} \end{align} $$ Is there a nice closed form for $C^-$ and $C^+$ ? Here's how I calculated the values for $C^+$ . It's easy to see that $$\frac{d}{da}C^+_{n+1}(a)=-C^+_{n}(a)$$ Additionally $E_n(0)=-E_n(1)\implies C^+_{n}(0)=-C^+_{n}(1)$ . We can use this to get $$C^+_{n+1}(a)=-\frac12\left(\int_0^aC^+_{n}(v)dv+\int_1^aC^+_{n}(v)dv\right)$$ For the initial value, I had previously found the following for $n=1$ . $$ C^+_{1}(a)=-\ln2-\gamma-2\ln\Gamma\left(\frac{a+1}{2}\right)+2\ln\Gamma\left(\frac{a}{2}\right) $$ For $C^-$ we also have $$\frac{d}{da}C^-_{n+1}(a)=-C^-_{n}(a)$$ and an equation for $n=1$ , $$C^-_{1}(a)=\frac{1}{2}\ln2\pi-\left(a-\frac{1}{2}\right)\gamma-\ln\Gamma(a)$$ However since $B_n(0)=B_n(1)\implies C^-_{n}(0)=C^-_{n}(1)$ , we can't use the same trick for $C^-$ . So while I have a method for solving $C^+$ I'm not sure how to get anywhere with $C^-$ for $n>1$ . EDIT 5/13/21 I have a working conjecture that $$C^+_n(a)=2\left(-1\right)^{n+a+1}\frac{2^{n}-1}{\left(n-1\right)!}\zeta'\left(1-n\right)-\left(-1\right)^{a}\frac{2^n}{2^n-1}\frac{E_{n-1}\left(1\right)}{\left(n-1\right)!}\ln2+\left(-1\right)^{n}\frac{\gamma-H_{n-1}}{\left(n-1\right)!}E_{n-1}\left(a\right)+\left(-1\right)^{n+a+1}\frac{2}{\left(n-1\right)!}\sum_{k=1}^{a-1}\left(-1\right)^{k}k^{n-1}\ln k$$ EDIT 9/30/23 I realized I can use $B_n(1/2)=\left(\frac{1}{2^{n-1}}-1\right)B_n(0)$ implying $$C^-_{n}(1/2)=\left(\frac{1}{2^{n-1}}-1\right) C^-_{n}(0)-\frac{B_n}{2^{n-1}n!}\ln 2$$ to get $$C^-_{n+1}(a)=\left(\frac{1}{\frac{1}{2^{n-1}}-2}\right)\left(\int_{1/2}^a C^-_{n}(v)dv-\left(\frac{1}{2^{n-1}}-1\right)\int_0^a C^-_{n}(v)dv+\frac{B_n}{2^{n-1} n!}\ln{2}\right)$$ This means we get the next few solutions. $$ \begin{align} C^-_{2}(a)&=\left(\frac{a^{2}}{2}-\frac{a}{2}+\frac{1}{12}\right)\gamma-\frac{a}{2}\ln\left(2\pi\right)-\ln A+\psi^{(-2)}\left(a\right) \\ C^-_{3}(a)&=-\left(\frac{a^{3}}{6}-\frac{a^{2}}{4}+\frac{a}{12}\right)\gamma+\frac{a^{2}}{4}\ln\left(2\pi\right)+a\ln A+\frac{1}{8\pi^{2}}\zeta(3)-\psi^{(-3)}\left(a\right) \end{align} $$","Start with the generating functions for the Bernoulli/Euler Polynomials . Integrating from x to , we have I'm interested in the constants of integration and . Some values for are listed below: Is there a nice closed form for and ? Here's how I calculated the values for . It's easy to see that Additionally . We can use this to get For the initial value, I had previously found the following for . For we also have and an equation for , However since , we can't use the same trick for . So while I have a method for solving I'm not sure how to get anywhere with for . EDIT 5/13/21 I have a working conjecture that EDIT 9/30/23 I realized I can use implying to get This means we get the next few solutions.","
\begin{align}
&\frac{e^{-ax}}{x^{n}\left(e^{-x}-1\right)}=-\sum_{k=0}^{\infty}\left(-1\right)^{k}B_k\left(a\right)\frac{x^{k-n-1}}{k!},\qquad |x|<2\pi\\
&\frac{2e^{-ax}}{x^{n}\left(e^{-x}+1\right)}=\space\space\space\sum_{k=0}^{\infty}\left(-1\right)^{k}E_k\left(a\right)\frac{x^{k-n}}{k!},\quad\qquad |x|\le\pi
\end{align}
 \infty 
\begin{align}
&\int_{x}^{\infty}\frac{e^{-at}}{t^{n}\left(e^{-t}-1\right)}dt\\\tag{1}
&=C^-_{n}(a)\\
&+\sum_{k=0}^{n-1}\left(-1\right)^{k}B_k\left(a\right)\frac{x^{k-n}}{k!\left(k-n\right)}+\left(-1\right)^{n}B_n\left(a\right)\frac{1}{n!}\ln x+\sum_{k=n+1}^{\infty}\left(-1\right)^{k}B_k\left(a\right)\frac{x^{k-n}}{k!\left(k-n\right)},\qquad |x|<2\pi
\\\\\\
&\int_x^\infty\frac{2e^{-at}}{t^n(e^{-t}+1)}dt\\\tag{2}
&=C^+_{n}(a)\\
&-\left(\sum_{k=0}^{n-2}\left(-1\right)^{k}E_k\left(a\right)\frac{x^{k-n+1}}{k!\left(k-n+1\right)}+\left(-1\right)^{n-1}E_{n-1}\left(a\right)\frac{1}{\left(n-1\right)!}\ln{x}+\sum_{k=n}^{\infty}\left(-1\right)^{k}E_k\left(a\right)\frac{x^{k-n+1}}{k!\left(k-n+1\right)}\right),\\
&\qquad\qquad |x|\le\pi
\end{align}
 C^- C^+ C^+ 
\begin{align}
C^+_{1}(1)&=\ln\pi-\ln2-\gamma\\
C^+_{1}(2)&=-\ln\pi+\ln2-\gamma\\
C^+_{1}(3)&=\ln\pi-3\ln2-\gamma\\
\\
C^+_{2}(1)&=-6\ln A+\frac{2}{3}\ln2+\frac{1}{2}\gamma\\
C^+_{2}(2)&=6\ln A-\frac{2}{3}\ln2+\frac{3}{2}\gamma-2\\
C^+_{2}(3)&=-6\ln A+\frac{14}{3}\ln2+\frac{5}{2}\gamma-2\\
\\
C^+_{3}(1)&=\frac{7}{4\pi^{2}}\zeta(3)\\
C^+_{3}(2)&=-\frac{7}{4\pi^{2}}\zeta(3)-\gamma+\frac{3}{2}\\
C^+_{3}(3)&=\frac{7}{4\pi^{2}}\zeta(3)-4\ln2-3\gamma+\frac{9}{2}\\
\\
C^+_{4}(1)&=5\zeta'(-3)-\frac{2}{45}\ln2-\frac{1}{24}\gamma+\frac{11}{144}\\
C^+_{4}(2)&=-5\zeta'(-3)+\frac{2}{45}\ln2+\frac{3}{8}\gamma-\frac{11}{16}\\
C^+_{4}(3)&=5\zeta'(-3)+\frac{118}{45}\ln2+\frac{55}{24}\gamma-\frac{605}{144}
\end{align}
 C^- C^+ C^+ \frac{d}{da}C^+_{n+1}(a)=-C^+_{n}(a) E_n(0)=-E_n(1)\implies C^+_{n}(0)=-C^+_{n}(1) C^+_{n+1}(a)=-\frac12\left(\int_0^aC^+_{n}(v)dv+\int_1^aC^+_{n}(v)dv\right) n=1 
C^+_{1}(a)=-\ln2-\gamma-2\ln\Gamma\left(\frac{a+1}{2}\right)+2\ln\Gamma\left(\frac{a}{2}\right)
 C^- \frac{d}{da}C^-_{n+1}(a)=-C^-_{n}(a) n=1 C^-_{1}(a)=\frac{1}{2}\ln2\pi-\left(a-\frac{1}{2}\right)\gamma-\ln\Gamma(a) B_n(0)=B_n(1)\implies C^-_{n}(0)=C^-_{n}(1) C^- C^+ C^- n>1 C^+_n(a)=2\left(-1\right)^{n+a+1}\frac{2^{n}-1}{\left(n-1\right)!}\zeta'\left(1-n\right)-\left(-1\right)^{a}\frac{2^n}{2^n-1}\frac{E_{n-1}\left(1\right)}{\left(n-1\right)!}\ln2+\left(-1\right)^{n}\frac{\gamma-H_{n-1}}{\left(n-1\right)!}E_{n-1}\left(a\right)+\left(-1\right)^{n+a+1}\frac{2}{\left(n-1\right)!}\sum_{k=1}^{a-1}\left(-1\right)^{k}k^{n-1}\ln k B_n(1/2)=\left(\frac{1}{2^{n-1}}-1\right)B_n(0) C^-_{n}(1/2)=\left(\frac{1}{2^{n-1}}-1\right) C^-_{n}(0)-\frac{B_n}{2^{n-1}n!}\ln 2 C^-_{n+1}(a)=\left(\frac{1}{\frac{1}{2^{n-1}}-2}\right)\left(\int_{1/2}^a C^-_{n}(v)dv-\left(\frac{1}{2^{n-1}}-1\right)\int_0^a C^-_{n}(v)dv+\frac{B_n}{2^{n-1} n!}\ln{2}\right) 
\begin{align}
C^-_{2}(a)&=\left(\frac{a^{2}}{2}-\frac{a}{2}+\frac{1}{12}\right)\gamma-\frac{a}{2}\ln\left(2\pi\right)-\ln A+\psi^{(-2)}\left(a\right)
\\
C^-_{3}(a)&=-\left(\frac{a^{3}}{6}-\frac{a^{2}}{4}+\frac{a}{12}\right)\gamma+\frac{a^{2}}{4}\ln\left(2\pi\right)+a\ln A+\frac{1}{8\pi^{2}}\zeta(3)-\psi^{(-3)}\left(a\right)
\end{align}
","['integration', 'closed-form']"
14,Compute exact integrals with quaternions,Compute exact integrals with quaternions,,"It's common knowledge that complex analysis is helpful in computing a bunch of exact real integrals. Is there any occurence of quaternions/quaternion formalism helping in the same way? If not, what could be some plausible reasons? Motivation To address comments asking for motivation, I will try to outline some correspondences in quaternion analysis to complex analysis, especially those that in complex analysis are useful for evaluating definite real integrals. Describing quaternions as $q= t+ix+jy+kz$ , A. Sudberry's 1977 Quaternionic Analysis notes that if the Cauchy-Riemann analogue of $$\frac{\partial f}{\partial t}+i\frac{\partial f}{\partial x}+j\frac{\partial f}{\partial y}+k\frac{\partial f}{\partial z}=0$$ holds, then an analogue of Cauchy's theorem holds for $C$ a smooth, closed 3-manifold in $\mathbb{H}$ : $$\int_{C} f(q) \; Dq=0, \\ \text{for }Dq = (dx \,dy\, dz-i\,dt\, dy \, dz - j\,dt\, dx \, dz - k\,dt \, dx \, dy).$$ He also notes that for such regular functions, an analogue of Cauchy's integral formula holds: $$f(q_0) = \frac{1}{2 \pi^2} \int_{\partial D}\frac{1}{|q-q_0|^2} (q-q_0)^{-1} Dq \,f(q) $$ for $D$ a domain in which $f$ is regular. These results for integrals at least partially mirror those in complex analysis. In complex analysis, Cauchy's theorem and integral formula are very useful for deriving the results of definite real integrals. The higher dimensionality of the quaternionic integrals maybe suggests that real integrals of functions of three variables would be useful to consider, but I do not know. Can these results for quaternionic integrals of regular functions be used to evaluate definite real integrals?","It's common knowledge that complex analysis is helpful in computing a bunch of exact real integrals. Is there any occurence of quaternions/quaternion formalism helping in the same way? If not, what could be some plausible reasons? Motivation To address comments asking for motivation, I will try to outline some correspondences in quaternion analysis to complex analysis, especially those that in complex analysis are useful for evaluating definite real integrals. Describing quaternions as , A. Sudberry's 1977 Quaternionic Analysis notes that if the Cauchy-Riemann analogue of holds, then an analogue of Cauchy's theorem holds for a smooth, closed 3-manifold in : He also notes that for such regular functions, an analogue of Cauchy's integral formula holds: for a domain in which is regular. These results for integrals at least partially mirror those in complex analysis. In complex analysis, Cauchy's theorem and integral formula are very useful for deriving the results of definite real integrals. The higher dimensionality of the quaternionic integrals maybe suggests that real integrals of functions of three variables would be useful to consider, but I do not know. Can these results for quaternionic integrals of regular functions be used to evaluate definite real integrals?","q= t+ix+jy+kz \frac{\partial f}{\partial t}+i\frac{\partial f}{\partial x}+j\frac{\partial f}{\partial y}+k\frac{\partial f}{\partial z}=0 C \mathbb{H} \int_{C} f(q) \; Dq=0, \\ \text{for }Dq = (dx \,dy\, dz-i\,dt\, dy \, dz - j\,dt\, dx \, dz - k\,dt \, dx \, dy). f(q_0) = \frac{1}{2 \pi^2} \int_{\partial D}\frac{1}{|q-q_0|^2} (q-q_0)^{-1} Dq \,f(q)  D f","['integration', 'improper-integrals', 'contour-integration', 'quaternions']"
15,Approximation of the sum of a series $S(t)=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2}$ as $t\to +\infty$,Approximation of the sum of a series  as,S(t)=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2} t\to +\infty,"The function S(t) has the following infinite series form: \begin{align} S(t) &=\frac 2\pi \int_0^{\pi/2} dx \sin(tx){\sum_{m\ odd}^{\infty} \alpha_m \cos[m(\frac \pi2-x)]}\\ &=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2} \\ \end{align} Where $m=1,3,5,...$ (odd numbers), $\alpha_m$ is given by this messy expression: $$\alpha_m=-\frac{1}{2m}\left[(\frac ab+1)J_{\frac{m-1}{2}}(\frac{m\epsilon}{2})+(\frac ab-1)J_{\frac{m+1}{2}}(\frac{m\epsilon}{2})\right]e^{mU_b} + \frac{1}{2m}\left[(\frac ab-1)J_{\frac{m-1}{2}}(\frac{m\epsilon}{2})+(\frac ab+1)J_{\frac{m+1}{2}}(\frac{m\epsilon}{2})\right]e^{-mU_b}$$ where $a$ and $b$ are some positive constants $(a>b>0)$ , $J$ is the Bessel function of first kind, $\epsilon=\displaystyle \frac{a^2-b^2}{a^2+b^2}$ , $U_b=\displaystyle\frac 12[\tanh(2\mu_b)-2\mu_b]$ and $\mu_b=\tanh^{-1}(\displaystyle\frac ba)$ . I don't think this explicit form of $\alpha_m$ is going to help much in this problem, however, it has a very neat asymptotic behavior as: $$\alpha_m \sim \frac{p}{m^{3/2}} \qquad as\quad m \to +\infty$$ where $p$ is a positive constant. My question is: How can I get the asymptotic approximation of $S(t)$ as $t \to +\infty$ ? My observation and attempts: If $t \to m'$ , $m'$ is some large odd integer, we have: $$S(t) \to \frac{p}{2m'^{3/2}} \sin(\frac{\pi m'}{2})$$ \begin{align} S(t) &=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2}\\ &=-\frac{2}{\pi t^3} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{1-(\frac{m}{t})^2} \end{align} Since $\displaystyle\sum_{m\ odd}^{\infty} m^2 \alpha_m $ diverges due to the asymptotic behavior of $\alpha_m$ , if we look at the sum $\displaystyle\sum_{m\ odd}^{\infty}\frac{m^2 \alpha_m}{1-\frac{m^2}{t^2}}$ , when $t \to +\infty$ the major contribution should come from $\alpha_m$ of large $m$ . This, in some sense, justifies (not rigorously though) the move for replacing $\alpha_m$ by its asymptotic form $\displaystyle\frac{p}{m^{3/2}}$ in the sum. Thus, $$S(t)=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2} \sim -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2} \quad as \quad t \to +\infty$$ Now, the goal is to find the asymptotics to $$I \equiv -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2} $$ as $t \to +\infty$ . My attempts to solve this problem were all to approximate the summation with an integral by writing summation in the form of Reimann sum as $t \to +\infty$ . For instance, my first try is as following: \begin{align} I &\equiv -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\\ &=-\frac{p}{\pi t^{\frac32}} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{(\frac mt)^{1/2}}{1-(\frac mt)^2} \frac 2t\\ &\sim -\frac{p}{\pi t^{\frac32}} \cos(\frac{\pi t}{2}) \int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx \quad as \quad t \to +\infty \end{align} Since the integral $\int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx$ diverges due to the pole at $x=1$ , but the principle value exists and $V.P. \int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx=-\frac \pi 2$ , it follows that $$I \sim \frac{p}{2 t^{\frac32}} \cos(\frac{\pi t}{2}) \quad as \quad t \to +\infty$$ However, approximating the well-behaved summation by a divergent integral seems a bit problematic, because in this way I did not include the effect of $\cos(\pi t/2)$ term which resolves the singularity problem and also it is inconsistent with my observation $1$ . Therefore my second attempt is to include $\cos(\pi t/2)$ in my summation. Using triogeometric property $\cos(\displaystyle\frac{\pi t}{2}) = \cos[\displaystyle\frac{\pi}{2}(t-m)+\displaystyle\frac \pi2 m]=-\sin[\displaystyle\frac{\pi t}{2}(1-\displaystyle\frac mt)]\sin(\displaystyle\frac{\pi m}{2})$ , I thus deduce: \begin{align} I &= -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\\ &=-\frac{2p}{\pi t}\sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\cos(\frac{\pi t}{2})\\ &=\frac{p}{\pi t^{\frac32}}  \sum_{m\ odd}^{\infty}\frac{(\frac mt)^{1/2}}{1-(\frac mt)^2} \sin(\frac{\pi m}{2})sin[\frac{\pi t}{2}(1-\frac mt)]\frac 2t\\ &\sim \frac{p}{\pi t^{\frac32}} \int_0^{\infty} \frac{x^{1/2}}{1-x^2}\sin(\frac{\pi tx}{2})\sin[\frac{\pi t}{2}(1-x)]dx \quad as \quad t \to +\infty \end{align} The above integral can be solved using methods suggested in the comments and answers of this question . Hence, $$I \sim \frac{p}{4t^{\frac 32}} \sin(\frac{\pi t}{2}) + \frac{p}{4t^{\frac 32}} \cos(\frac{\pi t}{2}) \quad as \quad t \to +\infty$$ However, this result is still not consistent with my observation $1$ (I am short by a factor of $2$ ), also I think there is a problem in the step transforming summation into integral due to the term $\sin(\displaystyle\frac{\pi m}{2})$ . At this point, I don't know what should I do to proceed! Any tips, comments, and suggestions are very much welcomed and also I am willing to acknowledge anyone who provides any sorts of help to this problem in my work. Thank you all in advance! Edit I have figured out how to deal with this problem now, the key is indeed the Euler-Maclaurin formula. I am going to post the answer in case anyone is curious about it.","The function S(t) has the following infinite series form: Where (odd numbers), is given by this messy expression: where and are some positive constants , is the Bessel function of first kind, , and . I don't think this explicit form of is going to help much in this problem, however, it has a very neat asymptotic behavior as: where is a positive constant. My question is: How can I get the asymptotic approximation of as ? My observation and attempts: If , is some large odd integer, we have: Since diverges due to the asymptotic behavior of , if we look at the sum , when the major contribution should come from of large . This, in some sense, justifies (not rigorously though) the move for replacing by its asymptotic form in the sum. Thus, Now, the goal is to find the asymptotics to as . My attempts to solve this problem were all to approximate the summation with an integral by writing summation in the form of Reimann sum as . For instance, my first try is as following: Since the integral diverges due to the pole at , but the principle value exists and , it follows that However, approximating the well-behaved summation by a divergent integral seems a bit problematic, because in this way I did not include the effect of term which resolves the singularity problem and also it is inconsistent with my observation . Therefore my second attempt is to include in my summation. Using triogeometric property , I thus deduce: The above integral can be solved using methods suggested in the comments and answers of this question . Hence, However, this result is still not consistent with my observation (I am short by a factor of ), also I think there is a problem in the step transforming summation into integral due to the term . At this point, I don't know what should I do to proceed! Any tips, comments, and suggestions are very much welcomed and also I am willing to acknowledge anyone who provides any sorts of help to this problem in my work. Thank you all in advance! Edit I have figured out how to deal with this problem now, the key is indeed the Euler-Maclaurin formula. I am going to post the answer in case anyone is curious about it.","\begin{align}
S(t) &=\frac 2\pi \int_0^{\pi/2} dx \sin(tx){\sum_{m\ odd}^{\infty} \alpha_m \cos[m(\frac \pi2-x)]}\\
&=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2} \\
\end{align} m=1,3,5,... \alpha_m \alpha_m=-\frac{1}{2m}\left[(\frac ab+1)J_{\frac{m-1}{2}}(\frac{m\epsilon}{2})+(\frac ab-1)J_{\frac{m+1}{2}}(\frac{m\epsilon}{2})\right]e^{mU_b} + \frac{1}{2m}\left[(\frac ab-1)J_{\frac{m-1}{2}}(\frac{m\epsilon}{2})+(\frac ab+1)J_{\frac{m+1}{2}}(\frac{m\epsilon}{2})\right]e^{-mU_b} a b (a>b>0) J \epsilon=\displaystyle \frac{a^2-b^2}{a^2+b^2} U_b=\displaystyle\frac 12[\tanh(2\mu_b)-2\mu_b] \mu_b=\tanh^{-1}(\displaystyle\frac ba) \alpha_m \alpha_m \sim \frac{p}{m^{3/2}} \qquad as\quad m \to +\infty p S(t) t \to +\infty t \to m' m' S(t) \to \frac{p}{2m'^{3/2}} \sin(\frac{\pi m'}{2}) \begin{align}
S(t) &=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2}\\
&=-\frac{2}{\pi t^3} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{1-(\frac{m}{t})^2}
\end{align} \displaystyle\sum_{m\ odd}^{\infty} m^2 \alpha_m  \alpha_m \displaystyle\sum_{m\ odd}^{\infty}\frac{m^2 \alpha_m}{1-\frac{m^2}{t^2}} t \to +\infty \alpha_m m \alpha_m \displaystyle\frac{p}{m^{3/2}} S(t)=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2} \sim -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2} \quad as \quad t \to +\infty I \equiv -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}  t \to +\infty t \to +\infty \begin{align}
I &\equiv -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\\
&=-\frac{p}{\pi t^{\frac32}} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{(\frac mt)^{1/2}}{1-(\frac mt)^2} \frac 2t\\
&\sim -\frac{p}{\pi t^{\frac32}} \cos(\frac{\pi t}{2}) \int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx
\quad as \quad t \to +\infty
\end{align} \int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx x=1 V.P. \int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx=-\frac \pi 2 I \sim \frac{p}{2 t^{\frac32}} \cos(\frac{\pi t}{2}) \quad as \quad t \to +\infty \cos(\pi t/2) 1 \cos(\pi t/2) \cos(\displaystyle\frac{\pi t}{2}) = \cos[\displaystyle\frac{\pi}{2}(t-m)+\displaystyle\frac \pi2 m]=-\sin[\displaystyle\frac{\pi t}{2}(1-\displaystyle\frac mt)]\sin(\displaystyle\frac{\pi m}{2}) \begin{align}
I &= -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\\
&=-\frac{2p}{\pi t}\sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\cos(\frac{\pi t}{2})\\
&=\frac{p}{\pi t^{\frac32}}  \sum_{m\ odd}^{\infty}\frac{(\frac mt)^{1/2}}{1-(\frac mt)^2} \sin(\frac{\pi m}{2})sin[\frac{\pi t}{2}(1-\frac mt)]\frac 2t\\
&\sim \frac{p}{\pi t^{\frac32}} \int_0^{\infty} \frac{x^{1/2}}{1-x^2}\sin(\frac{\pi tx}{2})\sin[\frac{\pi t}{2}(1-x)]dx \quad as \quad t \to +\infty
\end{align} I \sim \frac{p}{4t^{\frac 32}} \sin(\frac{\pi t}{2}) + \frac{p}{4t^{\frac 32}} \cos(\frac{\pi t}{2}) \quad as \quad t \to +\infty 1 2 \sin(\displaystyle\frac{\pi m}{2})","['integration', 'sequences-and-series', 'asymptotics', 'approximation', 'riemann-sum']"
16,Does there exist a closed form for $\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx$?,Does there exist a closed form for ?,\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx,"I am not sure if there exists a closed form for $$I=\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx$$ which seems non-trivial. I used the reflection and landen's identity, didn't help much. In case you are curious how I came up with this integral: From here we have $$\arcsin^3(x)=6\sum_{k=1}^\infty\left[\sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}\right]\frac{{2k\choose k}}{4^k}\frac{x^{2k+1}}{2k+1},\quad |x|<1$$ differentiate both sides with respect to $x$ we get $$\frac{\arcsin^2(x)}{\sqrt{1-x^2}}=2\sum_{k=1}^\infty\left[\sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}\right]\frac{{2k\choose k}}{4^k}x^{2k}$$ use $\sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}=H_{2k}^{(2)}-\frac14H_k^{(2)}$ and replace $x$ by $\sqrt{x}$ we get the form $$\frac{\arcsin^2(\sqrt{x})}{\sqrt{1-x}}=2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}x^{k}$$ Divide both sides by $x$ then $\int_0^y$ we have $$\int_0^y\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}dx=2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{y^{k}}{k}$$ Next, multiply both sides by $-\frac{\ln(1-y)}{y}$ then $\int_0^1$ and use $-\int_0^1 y^{k-1}\ln(1-y)dy=\frac{H_k}{k}$ $$2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{H_k}{k^2}=-\int_0^1\int_0^y\frac{\arcsin^2(\sqrt{x})\ln(1-y)}{xy\sqrt{1-x}}dxdy$$ $$=\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}\left(-\int_x^1\frac{\ln(1-y)}{y}dy\right)dx$$ $$=\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}\left(\zeta(2)-\text{Li}_2(x)\right)dx$$ $$\overset{\sqrt{x}=\sin\theta}{=}2\int_0^{\pi/2}\frac{x^2}{\sin x}(\zeta(2)-\text{Li}_2(\sin^2x))dx$$ So we have $$\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{H_k}{k^2}=\zeta(2)\int_0^{\pi/2}\frac{x^2}{\sin x}dx-\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx$$ The first integral can be calculated by applying integration by parts then using the fourier series of $\ln(\tan\frac x2)$ . Another question is, clearly the two sums on the LHS are convergent as the denominator blows to infinity much faster than the numerator. But does there exist a closed form for each? All methods are welcome. Thank you","I am not sure if there exists a closed form for which seems non-trivial. I used the reflection and landen's identity, didn't help much. In case you are curious how I came up with this integral: From here we have differentiate both sides with respect to we get use and replace by we get the form Divide both sides by then we have Next, multiply both sides by then and use So we have The first integral can be calculated by applying integration by parts then using the fourier series of . Another question is, clearly the two sums on the LHS are convergent as the denominator blows to infinity much faster than the numerator. But does there exist a closed form for each? All methods are welcome. Thank you","I=\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx \arcsin^3(x)=6\sum_{k=1}^\infty\left[\sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}\right]\frac{{2k\choose k}}{4^k}\frac{x^{2k+1}}{2k+1},\quad |x|<1 x \frac{\arcsin^2(x)}{\sqrt{1-x^2}}=2\sum_{k=1}^\infty\left[\sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}\right]\frac{{2k\choose k}}{4^k}x^{2k} \sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}=H_{2k}^{(2)}-\frac14H_k^{(2)} x \sqrt{x} \frac{\arcsin^2(\sqrt{x})}{\sqrt{1-x}}=2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}x^{k} x \int_0^y \int_0^y\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}dx=2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{y^{k}}{k} -\frac{\ln(1-y)}{y} \int_0^1 -\int_0^1 y^{k-1}\ln(1-y)dy=\frac{H_k}{k} 2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{H_k}{k^2}=-\int_0^1\int_0^y\frac{\arcsin^2(\sqrt{x})\ln(1-y)}{xy\sqrt{1-x}}dxdy =\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}\left(-\int_x^1\frac{\ln(1-y)}{y}dy\right)dx =\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}\left(\zeta(2)-\text{Li}_2(x)\right)dx \overset{\sqrt{x}=\sin\theta}{=}2\int_0^{\pi/2}\frac{x^2}{\sin x}(\zeta(2)-\text{Li}_2(\sin^2x))dx \sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{H_k}{k^2}=\zeta(2)\int_0^{\pi/2}\frac{x^2}{\sin x}dx-\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx \ln(\tan\frac x2)","['integration', 'sequences-and-series', 'closed-form', 'harmonic-numbers', 'polylogarithm']"
17,Special functions for understanding elemetary integrals,Special functions for understanding elemetary integrals,,"In this earlier question I wrote: Sir Harold Jeffreys wrote: $\dagger$ Consider the integrals $$ I_n = \int_{-1}^1 (1-x^2)^n \cos\alpha x \,dx. $$ Two integrations by parts give the recurrence relation $$ \alpha^2 I_n = 2n(2n-1) I_{n-1} - 4n(n-1) I_{n-2}, \qquad n\ge 2. $$ So I did the two integrations by parts and got $$ \frac{2n}{\alpha^2} \int_{-1}^1 \cos(\alpha x) (1-x^2)^{n-2}(1 - (2n-1) x^2) \, dx, $$ so I had to ascertain whether that was equal to the right side of the recurrence relation. That comes down to this: \begin{align} & (1-x^2)^{n-2}(1 - (2n-1) x^2) \\[6pt] = {} & (2n-1)\,\underbrace{(1-x^2)^{n-1}} {} - 2(n-1) \underbrace{(1-x^2)^{n-2}} \end{align} As I said in that earlier question, here we use the set $\{ (1-x^2)^n : n=0,1,2,\ldots \}$ as a basis of the space of even polynomials. $$ \left\{\begin{array}{l} \textbf{Note inspired by a comment below:} \\ \textbf{The QUESTION posed here is NOT} \\ \textbf{how to deal with this integral, and} \\ \textbf{appears BELOW, and can be understood} \\ \textbf{ONLY by reading what appears below.} \\ {} \\ \textbf{AND this question is NOT about THIS integral.} \end{array}\right. $$ As I did not say in that earlier question, I attempted to ascertain whether the equality holds before it occurred to me to express the function I got as a linear combination of the functions in that particular sequence. Despite the triviality of it, seemingly being a matter for routine secondary-school algebra, it was hairy and icky until I realized that expressing everything as a linear combination of these functions is what was how it should be done. So now my somewhat vague question is whether, in the context of elementary sorts of integrals like this, there are OTHER instances of easily defined special functions whose employment instantly reveals a pattern without which the problem is hairy and with which it is straightforward and has a nice pattern? Just as there are tables of integrals, would it be worth making a table of such things? $\dagger$ Scientific Inference , third edition, Cambridge University Press, 1973. Appendix III. If I'm not mistaken, this appendix does not appear in other editions.","In this earlier question I wrote: Sir Harold Jeffreys wrote: Consider the integrals Two integrations by parts give the recurrence relation So I did the two integrations by parts and got so I had to ascertain whether that was equal to the right side of the recurrence relation. That comes down to this: As I said in that earlier question, here we use the set as a basis of the space of even polynomials. As I did not say in that earlier question, I attempted to ascertain whether the equality holds before it occurred to me to express the function I got as a linear combination of the functions in that particular sequence. Despite the triviality of it, seemingly being a matter for routine secondary-school algebra, it was hairy and icky until I realized that expressing everything as a linear combination of these functions is what was how it should be done. So now my somewhat vague question is whether, in the context of elementary sorts of integrals like this, there are OTHER instances of easily defined special functions whose employment instantly reveals a pattern without which the problem is hairy and with which it is straightforward and has a nice pattern? Just as there are tables of integrals, would it be worth making a table of such things? Scientific Inference , third edition, Cambridge University Press, 1973. Appendix III. If I'm not mistaken, this appendix does not appear in other editions.","\dagger  I_n = \int_{-1}^1 (1-x^2)^n \cos\alpha x \,dx.   \alpha^2 I_n = 2n(2n-1) I_{n-1} - 4n(n-1) I_{n-2}, \qquad n\ge 2.  
\frac{2n}{\alpha^2} \int_{-1}^1 \cos(\alpha x) (1-x^2)^{n-2}(1 - (2n-1) x^2) \, dx,
 \begin{align}
& (1-x^2)^{n-2}(1 - (2n-1) x^2) \\[6pt]
= {} & (2n-1)\,\underbrace{(1-x^2)^{n-1}} {} - 2(n-1) \underbrace{(1-x^2)^{n-2}}
\end{align} \{ (1-x^2)^n : n=0,1,2,\ldots \} 
\left\{\begin{array}{l}
\textbf{Note inspired by a comment below:} \\
\textbf{The QUESTION posed here is NOT} \\
\textbf{how to deal with this integral, and} \\
\textbf{appears BELOW, and can be understood} \\
\textbf{ONLY by reading what appears below.}
\\ {} \\
\textbf{AND this question is NOT about THIS integral.}
\end{array}\right.
 \dagger","['integration', 'definite-integrals', 'special-functions']"
18,Proving that the Kernel of an Integral Equation is Weakly Singular,Proving that the Kernel of an Integral Equation is Weakly Singular,,"I have a simple problem of deducing whether the kernel $$k(x,t) := \log |x-t|$$ is weakly singular or not. I have seen many basic examples of how to do this but I can't make the link to this. I know that the kernel is weakly singular if $$|k(x,t)| \leq C|x-t|^{- \alpha}$$ Does it have something to do with $\log 0$ being undefined and $|x-t| \rightarrow 0$ ? Any help would be appreicated.",I have a simple problem of deducing whether the kernel is weakly singular or not. I have seen many basic examples of how to do this but I can't make the link to this. I know that the kernel is weakly singular if Does it have something to do with being undefined and ? Any help would be appreicated.,"k(x,t) := \log |x-t| |k(x,t)| \leq C|x-t|^{- \alpha} \log 0 |x-t| \rightarrow 0","['integration', 'continuity', 'improper-integrals']"
19,Question about the sequence of moments of a continuous function,Question about the sequence of moments of a continuous function,,"Let $f:[0,1]\to\mathbb{R}$ be continuous. Consider for every $n\in\mathbb{N}$ : $$M_n(f)=\int_0^1t^n\,f(t)\,dt$$ It is easy to see that the sequence $\left(M_n(f)\right)_{n\in\mathbb{N}}$ converges to $0$ . But is it possible to choose $f$ in such a way that : $$\forall n\in\mathbb{N},\,M_n(f)=e^{-\lambda n^2}$$ where $\lambda$ is some positive constant ? It can be seen that such a function could not be positive and would necessarily verify $f(1)=0$ , but I wasn't able to get much more than that ... Any hint would be appreciated :)","Let be continuous. Consider for every : It is easy to see that the sequence converges to . But is it possible to choose in such a way that : where is some positive constant ? It can be seen that such a function could not be positive and would necessarily verify , but I wasn't able to get much more than that ... Any hint would be appreciated :)","f:[0,1]\to\mathbb{R} n\in\mathbb{N} M_n(f)=\int_0^1t^n\,f(t)\,dt \left(M_n(f)\right)_{n\in\mathbb{N}} 0 f \forall n\in\mathbb{N},\,M_n(f)=e^{-\lambda n^2} \lambda f(1)=0","['integration', 'asymptotics', 'laplace-transform']"
20,Why can't we integrate functions on a manifold?,Why can't we integrate functions on a manifold?,,"Most books about manifolds say that ""there is no way to integrate real-valued functions in a coordinate-independent way on a manifold"". I've read the usual reasons but they don't seem to differ from the usual theory of integration in $\mathbb{R}^n$ . I explain below what I mean. Let $M$ be an orientable smooth $n$ -manifold. Since it is orientable, we have a nonvanishing $n$ -form $\omega$ . This allows us to define the following linear functional: \begin{align*} \Lambda:C_c^{\infty}(M)&\to \mathbb{R}\\ f &\mapsto \int_M f\omega. \end{align*} By continuity, we can extend its domain to obtain a positive linear functional $C_c(M)\to \mathbb{R}$ . Then, the Riesz-Markov-Kakutani representation theorem implies the existence of a regular Borel measure $\mu$ such that $$\Lambda(f)=\int_M f \:\mathrm{d}\mu,$$ for all $f\in C_c^\infty(M)$ . Why isn't this a good notion of integral of functions on a manifold? It doesn't seem to depend on a choice of chart and while it depends of a choice of $\omega$ , I would argue that the same thing happens in $\mathbb{R}^n$ since the Lebesgue measure depends of a normalization factor.","Most books about manifolds say that ""there is no way to integrate real-valued functions in a coordinate-independent way on a manifold"". I've read the usual reasons but they don't seem to differ from the usual theory of integration in . I explain below what I mean. Let be an orientable smooth -manifold. Since it is orientable, we have a nonvanishing -form . This allows us to define the following linear functional: By continuity, we can extend its domain to obtain a positive linear functional . Then, the Riesz-Markov-Kakutani representation theorem implies the existence of a regular Borel measure such that for all . Why isn't this a good notion of integral of functions on a manifold? It doesn't seem to depend on a choice of chart and while it depends of a choice of , I would argue that the same thing happens in since the Lebesgue measure depends of a normalization factor.","\mathbb{R}^n M n n \omega \begin{align*}
\Lambda:C_c^{\infty}(M)&\to \mathbb{R}\\
f &\mapsto \int_M f\omega.
\end{align*} C_c(M)\to \mathbb{R} \mu \Lambda(f)=\int_M f \:\mathrm{d}\mu, f\in C_c^\infty(M) \omega \mathbb{R}^n","['integration', 'measure-theory', 'manifolds']"
21,"Finding $\int_0^1\int_0^1\ldots\int_0^1\frac{1}{1-\prod_{i=1}^{n}\ln(x_i)}\,dx_1dx_2\ldots dx_n$",Finding,"\int_0^1\int_0^1\ldots\int_0^1\frac{1}{1-\prod_{i=1}^{n}\ln(x_i)}\,dx_1dx_2\ldots dx_n","I am interested in finding $$f(n) = \int_0^1\int_0^1\ldots\int_0^1\frac{1}{1-\prod_{i=1}^{n}\ln(x_i)}\,dx_1dx_2\ldots dx_n$$ for positive integer $n$ . For example, $$f(2)=\int_0^1\int_0^1\frac{1}{1-\ln(x_1)\ln(x_2)}\,dx_1dx_2$$ I found $f(1) = e \cdot E_1(1) \approx 0.596$ , $f(2)$ diverges, $f(3) \approx 0.724$ . $$$$ My questions How can I find $f(n)$ for positive integer $n$ ? If that is not possible, what is an approximation? I know for sure that if $n$ is odd, $f(n)$ converges. This is because $\frac{1}{1-\prod_{i=1}^{n}\ln(x_i)}$ will converge for $0 \le x_i \le 1$ . I also suspect that if $n$ is even, $f(n)$ diverges.","I am interested in finding for positive integer . For example, I found , diverges, . My questions How can I find for positive integer ? If that is not possible, what is an approximation? I know for sure that if is odd, converges. This is because will converge for . I also suspect that if is even, diverges.","f(n) = \int_0^1\int_0^1\ldots\int_0^1\frac{1}{1-\prod_{i=1}^{n}\ln(x_i)}\,dx_1dx_2\ldots dx_n n f(2)=\int_0^1\int_0^1\frac{1}{1-\ln(x_1)\ln(x_2)}\,dx_1dx_2 f(1) = e \cdot E_1(1) \approx 0.596 f(2) f(3) \approx 0.724  f(n) n n f(n) \frac{1}{1-\prod_{i=1}^{n}\ln(x_i)} 0 \le x_i \le 1 n f(n)","['integration', 'definite-integrals', 'logarithms', 'multiple-integral']"
22,Sum involving incomplete beta functions,Sum involving incomplete beta functions,,"I am interested in the evaluation (A) , or at least an asymptotic expansion for large $N$ (B) , of the following finite sum \begin{equation}\begin{split} S^{(p)}(N)&\equiv2N\sum_{k=1}^{\left\lfloor\frac{N-1}{2}\right\rfloor}\left[\left(1-\frac{k}{N}+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-\frac{2k}{N}}{1-\frac{k}{N}+\frac{1}{2N}};N,p+1\right)\right.\\[8pt] &\left.\quad-\frac{(1+\frac{1}{N})^{N+p}}{2^p}B\left(\frac{1-\frac{2k}{N}}{1+\frac{1}{N}};N,p+1\right)\right], \end{split}\end{equation} where $N\in\mathbb{N}$ , $p>0$ and $B(x;a,b)$ is the incomplete beta function $$B(x;a,b):=\int_0^xx^{a-1}(1-x)^{b-1}\,\text{d}x.$$ (A) So far I have elaborated the above expression in the following way. Inspired by the fact that both terms in the square brakets are of the form $$x^qB\left(\frac{y}{x};N,p+1\right)$$ I used one of the possible hypergeometric representations of the incomplete beta function, namely $$B(x;a,b)=\frac{x^a(1-x)^{b-1}}{a}{}_2F_1\left(1,1-b;a+1;\frac{x}{x-1}\right),$$ where ${}_2F_1(\alpha,\beta;\gamma;z)$ is the hypergeometric function . Incidentally, this simplified a little bit the starting expression, which became after some simple algebra \begin{equation}\begin{split} S^{(p)}(N)&=\frac{2^{1-p}}{N^{N+p}}\sum_{k=1}^{\left\lfloor\frac{N-1}{2}\right\rfloor}(N-2k)^N(1+2k)^p\left[{}_2F_1\left(1,-p;N+1;-\frac{2(N-2k)}{1+2k}\right)\right.\\[6pt] &\left.\quad-{}_2F_1\left(1,-p;N+1;-\frac{N-2k}{1+2k}\right)\right]. \end{split}\end{equation} I wonder whether further simplifications can be performed, especially considering that the two hypergeometric functions differ only by a factor of $2$ in the last argument. I looked for a possible use of this observation, e.g. here , but I did not find anything readily applicable to the problem. (B) Setting $M=\left\lfloor\frac{N-1}{2}\right\rfloor$ for simplicity I approximated the sum with an integral in the large $N$ limit \begin{equation}\begin{split} S^{(p)}(N)&\sim 2N^2\int_{\frac{1}{N}}^{\frac{M}{N}}\left[\left(1-x+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-2x}{1-x+\frac{1}{2N}};N,p+1\right)\right.\\[6pt] &\left.\quad-\frac{\left(1+\frac{1}{N}\right)^{N+p}}{2^p}B\left(\frac{1-2x}{1+\frac{1}{N}};N,p+1\right)\right]\text{d}x. \end{split}\end{equation} At this point I discovered that Mathematica directly evaluates the integral of the second term in the square brakets, which results in (I am still trying to obtain this result analytically) \begin{equation}\begin{split} &\int_{\frac{1}{N}}^{\frac{M}{N}}B\left(\frac{1-2x}{1+\frac{1}{N}};N,p+1\right)\text{d}x\\[6pt] &=\frac{1}{2N}\left\{(2M-\frac{Np}{N+p+1})B\left(\frac{N-2M}{N+1};N,p+1\right)\right.\\[6pt] &\quad\left.-\left(2-\frac{Np}{N+p+1}\right)B\left(\frac{N-2}{N+1};N,p+1\right)\right.\\[6pt] &\quad\left.+\frac{1}{(N+p+1)(N+1)^{N+p}}\left[3^{p+1}(N-2)^N-(2M+1)^{p+1}(N-2M)^N\right]\right\}. \end{split}\end{equation} I wonder if something similar can be obtained for the first term, which can be rewritten as \begin{equation}\begin{split} &\int_{\frac{1}{N}}^{\frac{M}{N}}\left(1-x+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-2x}{1-x+\frac{1}{2N}};N,p+1\right)\text{d}x\\[8pt] &\quad=-\left(1+\frac{1}{N}\right)^{N+p+1}\int_{\frac{2(N-2)}{2N-1}}^{\frac{2(N-2M)}{2N-2M+1}}(2-y)^{-N-p-2}B(y;N,p+1)\text{d}y, \end{split}\end{equation} or equivalently as \begin{equation} =\frac{1}{N}\int_{\frac{1}{N}}^{\frac{M}{N}}\left(1-2x\right)^N\left(x+\frac{1}{2N}\right)^p{}_2F_1\left(1,-p;N+1;-\frac{2x-1}{x+\frac{1}{2N}}\right)\text{d}x. \end{equation}","I am interested in the evaluation (A) , or at least an asymptotic expansion for large (B) , of the following finite sum where , and is the incomplete beta function (A) So far I have elaborated the above expression in the following way. Inspired by the fact that both terms in the square brakets are of the form I used one of the possible hypergeometric representations of the incomplete beta function, namely where is the hypergeometric function . Incidentally, this simplified a little bit the starting expression, which became after some simple algebra I wonder whether further simplifications can be performed, especially considering that the two hypergeometric functions differ only by a factor of in the last argument. I looked for a possible use of this observation, e.g. here , but I did not find anything readily applicable to the problem. (B) Setting for simplicity I approximated the sum with an integral in the large limit At this point I discovered that Mathematica directly evaluates the integral of the second term in the square brakets, which results in (I am still trying to obtain this result analytically) I wonder if something similar can be obtained for the first term, which can be rewritten as or equivalently as","N \begin{equation}\begin{split}
S^{(p)}(N)&\equiv2N\sum_{k=1}^{\left\lfloor\frac{N-1}{2}\right\rfloor}\left[\left(1-\frac{k}{N}+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-\frac{2k}{N}}{1-\frac{k}{N}+\frac{1}{2N}};N,p+1\right)\right.\\[8pt]
&\left.\quad-\frac{(1+\frac{1}{N})^{N+p}}{2^p}B\left(\frac{1-\frac{2k}{N}}{1+\frac{1}{N}};N,p+1\right)\right],
\end{split}\end{equation} N\in\mathbb{N} p>0 B(x;a,b) B(x;a,b):=\int_0^xx^{a-1}(1-x)^{b-1}\,\text{d}x. x^qB\left(\frac{y}{x};N,p+1\right) B(x;a,b)=\frac{x^a(1-x)^{b-1}}{a}{}_2F_1\left(1,1-b;a+1;\frac{x}{x-1}\right), {}_2F_1(\alpha,\beta;\gamma;z) \begin{equation}\begin{split}
S^{(p)}(N)&=\frac{2^{1-p}}{N^{N+p}}\sum_{k=1}^{\left\lfloor\frac{N-1}{2}\right\rfloor}(N-2k)^N(1+2k)^p\left[{}_2F_1\left(1,-p;N+1;-\frac{2(N-2k)}{1+2k}\right)\right.\\[6pt]
&\left.\quad-{}_2F_1\left(1,-p;N+1;-\frac{N-2k}{1+2k}\right)\right].
\end{split}\end{equation} 2 M=\left\lfloor\frac{N-1}{2}\right\rfloor N \begin{equation}\begin{split}
S^{(p)}(N)&\sim 2N^2\int_{\frac{1}{N}}^{\frac{M}{N}}\left[\left(1-x+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-2x}{1-x+\frac{1}{2N}};N,p+1\right)\right.\\[6pt]
&\left.\quad-\frac{\left(1+\frac{1}{N}\right)^{N+p}}{2^p}B\left(\frac{1-2x}{1+\frac{1}{N}};N,p+1\right)\right]\text{d}x.
\end{split}\end{equation} \begin{equation}\begin{split}
&\int_{\frac{1}{N}}^{\frac{M}{N}}B\left(\frac{1-2x}{1+\frac{1}{N}};N,p+1\right)\text{d}x\\[6pt]
&=\frac{1}{2N}\left\{(2M-\frac{Np}{N+p+1})B\left(\frac{N-2M}{N+1};N,p+1\right)\right.\\[6pt]
&\quad\left.-\left(2-\frac{Np}{N+p+1}\right)B\left(\frac{N-2}{N+1};N,p+1\right)\right.\\[6pt]
&\quad\left.+\frac{1}{(N+p+1)(N+1)^{N+p}}\left[3^{p+1}(N-2)^N-(2M+1)^{p+1}(N-2M)^N\right]\right\}.
\end{split}\end{equation} \begin{equation}\begin{split}
&\int_{\frac{1}{N}}^{\frac{M}{N}}\left(1-x+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-2x}{1-x+\frac{1}{2N}};N,p+1\right)\text{d}x\\[8pt]
&\quad=-\left(1+\frac{1}{N}\right)^{N+p+1}\int_{\frac{2(N-2)}{2N-1}}^{\frac{2(N-2M)}{2N-2M+1}}(2-y)^{-N-p-2}B(y;N,p+1)\text{d}y,
\end{split}\end{equation} \begin{equation}
=\frac{1}{N}\int_{\frac{1}{N}}^{\frac{M}{N}}\left(1-2x\right)^N\left(x+\frac{1}{2N}\right)^p{}_2F_1\left(1,-p;N+1;-\frac{2x-1}{x+\frac{1}{2N}}\right)\text{d}x.
\end{equation}","['integration', 'asymptotics', 'special-functions', 'hypergeometric-function', 'beta-function']"
23,A Homogeneous Fredholm Equation of Second Kind,A Homogeneous Fredholm Equation of Second Kind,,"in my probability research I encounter the following integral equation for continuous non-negative $f: (0,\pi/4] \to \mathbb R$ : $$ f(\varphi) = \int_0^{\pi/4} \frac {4} {\pi} \sin \varphi_0 \cos \varphi_0 \sin \varphi \cos \varphi \, (\cos \varphi_0 \sqrt{1+\cos^2 \varphi_0} ( \frac {1} {4\cos^4\varphi_0 \sin^2 \varphi + \cos^4 \varphi} + \frac {1} {4\cos^4\varphi_0 \cos^2 \varphi + \sin^4 \varphi} + \frac {1} {1-4\cos^4\varphi_0 \sin^2 \varphi \cos^2 \varphi}) + \sin \varphi_0 \sqrt{1+\sin^2 \varphi_0} ( \frac {1} {4\sin^4\varphi_0 \cos^2 \varphi + \sin^4 \varphi} + \frac {1} {4\sin^4\varphi_0 \sin^2 \varphi + \cos^4 \varphi} + \frac {1} {1-4\sin^4\varphi_0 \sin^2 \varphi \cos^2 \varphi}) \, f(\varphi_0) \, d\varphi_0.$$ The integral kernel is a density for fixed $\varphi_0$ and $f$ integrates to $1$ . I have shown that such a density function $f$ exists uniquely and a numerically computed picture but I'm really stuck on finding a suitable analytic approach to solve for $f$ . I highly appreciate any suggestions. EDIT: Plot of solution $f$ (which is the Lebesgue density function of some stationary distribution): To me, it looks like $f$ is point-symmetric at $(\pi/8,4/\pi)$ , but I can not prove it. Perhaps it is to hard to find any closed form expression, series representation or anything exactly matching $f$ . Are there chances to have an approximation $g \in C((0,\pi/4])$ with $\lVert f-g\rVert_{\infty}$ small measured in the supremum norm?","in my probability research I encounter the following integral equation for continuous non-negative : The integral kernel is a density for fixed and integrates to . I have shown that such a density function exists uniquely and a numerically computed picture but I'm really stuck on finding a suitable analytic approach to solve for . I highly appreciate any suggestions. EDIT: Plot of solution (which is the Lebesgue density function of some stationary distribution): To me, it looks like is point-symmetric at , but I can not prove it. Perhaps it is to hard to find any closed form expression, series representation or anything exactly matching . Are there chances to have an approximation with small measured in the supremum norm?","f: (0,\pi/4] \to \mathbb R  f(\varphi) = \int_0^{\pi/4} \frac {4} {\pi} \sin \varphi_0 \cos \varphi_0 \sin \varphi \cos \varphi \, (\cos \varphi_0 \sqrt{1+\cos^2 \varphi_0} ( \frac {1} {4\cos^4\varphi_0 \sin^2 \varphi + \cos^4 \varphi} + \frac {1} {4\cos^4\varphi_0 \cos^2 \varphi + \sin^4 \varphi} + \frac {1} {1-4\cos^4\varphi_0 \sin^2 \varphi \cos^2 \varphi}) + \sin \varphi_0 \sqrt{1+\sin^2 \varphi_0} ( \frac {1} {4\sin^4\varphi_0 \cos^2 \varphi + \sin^4 \varphi} + \frac {1} {4\sin^4\varphi_0 \sin^2 \varphi + \cos^4 \varphi} + \frac {1} {1-4\sin^4\varphi_0 \sin^2 \varphi \cos^2 \varphi}) \, f(\varphi_0) \, d\varphi_0. \varphi_0 f 1 f f f f (\pi/8,4/\pi) f g \in C((0,\pi/4]) \lVert f-g\rVert_{\infty}","['integration', 'functional-analysis', 'spectral-theory', 'integral-equations', 'eigenfunctions']"
24,Is there a general theory of when certain polynomials are integrable due to symmetry tricks?,Is there a general theory of when certain polynomials are integrable due to symmetry tricks?,,"Consider the functions $x^2$ and $x^4 + 2x^2y^2$ on the unit sphere $S^2$ . The surface integral of these functions over the sphere can easily be calculated by symmetry via $$3 \iint_{S^2} x^2 \mathrm{d}A = \iint_{S^2} (x^2 + y^2 + z^2) \, \mathrm{d}A = \iint_{S^2} \mathrm{d}A = 4\pi$$ and $$3 \iint_{S^2} (x^4+2x^2y^2)\, \mathrm{d}A = \iint_{S^2} (x^2 + y^2 + z^2)^2 \, \mathrm{d}A = \iint_{S^2} \mathrm{d}A = 4\pi.$$ However, I suspect (although I cannot prove) that the function $x^4$ cannot be integrated without direct parameterization of the sphere and evaluation of the surface integral. My question is: in general, given any symmetries and polynomial relations on a manifold (in this case $(x, y, z) \mapsto (y, z, x)$ and $x^2 + y^2 + z^2 = 1$ ), is there a general theory to determine what functions are integrable over the manifold by symmetry and relations alone? A reference (or definitive statement of lack thereof) would be greatly appreciated.","Consider the functions and on the unit sphere . The surface integral of these functions over the sphere can easily be calculated by symmetry via and However, I suspect (although I cannot prove) that the function cannot be integrated without direct parameterization of the sphere and evaluation of the surface integral. My question is: in general, given any symmetries and polynomial relations on a manifold (in this case and ), is there a general theory to determine what functions are integrable over the manifold by symmetry and relations alone? A reference (or definitive statement of lack thereof) would be greatly appreciated.","x^2 x^4 + 2x^2y^2 S^2 3 \iint_{S^2} x^2 \mathrm{d}A = \iint_{S^2} (x^2 + y^2 + z^2) \, \mathrm{d}A = \iint_{S^2} \mathrm{d}A = 4\pi 3 \iint_{S^2} (x^4+2x^2y^2)\, \mathrm{d}A = \iint_{S^2} (x^2 + y^2 + z^2)^2 \, \mathrm{d}A = \iint_{S^2} \mathrm{d}A = 4\pi. x^4 (x, y, z) \mapsto (y, z, x) x^2 + y^2 + z^2 = 1","['integration', 'symmetry']"
25,Analytic or perturbative solution in any limits?,Analytic or perturbative solution in any limits?,,"Consider the system of 3 ordinary differential equations $$\dot{x}=v$$ $$\dot{v}=a$$ $$\dot{a}=-Aa+v^{2}-x$$ which can also be written as a single 3rd order ODE $$\dddot{x}=-A\ddot{x}+\dot{x}^{2}-x$$ $A$ is an arbitrary constant and the dot means derivative with respect to time, i.e. $\dot{x}=dx/dt,\ddot{x}=d^{2}x/dt^{2}$ , etc. This system can be thought as describing the time evolution of the position $x$ , velocity $v$ and acceleration $a$ of a particle. Are there any limits where we can solve analytically this system, i.e. find $x(t),v(t),a(t)$ ? For example when $A=0$ ? A perturbative solution would also be good. Or maybe there is a way of reparametrizing time to make the system a known integrable one? I know that the simpler system $$\dddot{x}=-A\ddot{x}\iff \dot{a}=-Aa$$ has the solution $$a(t)=c_{1}e^{-At}$$ which means that $$x(t)=\frac{c_{1}}{A^{2}}e^{-At}+c_{2}t+c_{3}$$","Consider the system of 3 ordinary differential equations which can also be written as a single 3rd order ODE is an arbitrary constant and the dot means derivative with respect to time, i.e. , etc. This system can be thought as describing the time evolution of the position , velocity and acceleration of a particle. Are there any limits where we can solve analytically this system, i.e. find ? For example when ? A perturbative solution would also be good. Or maybe there is a way of reparametrizing time to make the system a known integrable one? I know that the simpler system has the solution which means that","\dot{x}=v \dot{v}=a \dot{a}=-Aa+v^{2}-x \dddot{x}=-A\ddot{x}+\dot{x}^{2}-x A \dot{x}=dx/dt,\ddot{x}=d^{2}x/dt^{2} x v a x(t),v(t),a(t) A=0 \dddot{x}=-A\ddot{x}\iff \dot{a}=-Aa a(t)=c_{1}e^{-At} x(t)=\frac{c_{1}}{A^{2}}e^{-At}+c_{2}t+c_{3}","['integration', 'ordinary-differential-equations', 'power-series']"
26,"What is $\int_{-\infty}^{\infty}\exp(\mathrm{i} n \cosh{x}) \, \mathrm{d}x$?",What is ?,"\int_{-\infty}^{\infty}\exp(\mathrm{i} n \cosh{x}) \, \mathrm{d}x","I'm hoping to determine the value of the following integral: $$\int_{-\infty}^{\infty}\exp(\mathrm{i} n \cosh{x}) \, \mathrm{d}x$$ Here is a plot of the integrand as a function of $x$ with parameter $n$ varying from 0 to 10. The integral appears to not converge. However, it is known that $$\int_{-\infty}^{\infty}\exp(\mathrm{i} n x) \, \mathrm{d}x = 2\pi \delta(n)$$ where $\delta(x)$ is the Dirac delta function. Is it possible for the first integral to be expressed similarly using Dirac delta notation?","I'm hoping to determine the value of the following integral: Here is a plot of the integrand as a function of with parameter varying from 0 to 10. The integral appears to not converge. However, it is known that where is the Dirac delta function. Is it possible for the first integral to be expressed similarly using Dirac delta notation?","\int_{-\infty}^{\infty}\exp(\mathrm{i} n \cosh{x}) \, \mathrm{d}x x n \int_{-\infty}^{\infty}\exp(\mathrm{i} n x) \, \mathrm{d}x = 2\pi \delta(n) \delta(x)","['integration', 'definite-integrals', 'dirac-delta']"
27,Where is my mistake? Calculating surface integral/Stoke's theorem,Where is my mistake? Calculating surface integral/Stoke's theorem,,"Let $F(x,y,z)= \begin{pmatrix} -y \\ 2x\\z \end{pmatrix}$ be a vector field and $A$ a hemisphere with $x^2+y^2+z^2=9 $ , $ z>0 $ with a circular edge at the $x,y $ - level with the unit normal vector $n$ showing outwards. I want to determine $ \int_A ( \nabla \times F) n\; do $ 1) as a surface integral 2) with Stoke's theorem. 1) I used the parametrization $ \Phi(\phi, \theta) =\begin{pmatrix} R\sin \theta \cos\phi \\ R\sin\theta \sin \phi\\R\cos \theta \end{pmatrix}$ with $ 0 \leq \phi \leq 2 \pi $ and $ 0\leq \theta\leq \frac{\pi}{2}$ for the unit normal: $ \frac{ \delta \Phi }{ \phi} \times \frac{ \delta \Phi}{ \theta} = \begin{pmatrix} R^2\sin^2 \theta \cos \phi \\ R^2\sin^2 \theta \sin \phi \\R^2\sin \theta \cos\theta \end{pmatrix}$ so integrate $\int_0^{ \frac{ \pi}{2}} \int_0^{2 \pi} \begin{pmatrix} -Rsin\theta \cos \phi \\ 2R\sin \theta \sin \phi \\R \cos\theta \end{pmatrix}\begin{pmatrix} R^2\sin^2 \theta \cos \phi \\ R^2\sin^2 \theta \sin \phi \\R^2\sin \theta \cos\theta \end{pmatrix} d\phi d\theta = 2 \pi R^3 $ 2) for Stoke's theorem I use as parametrization $ \Phi ( \phi) = \begin{pmatrix} -\sin \phi \\ \cos \phi \\ 0 \end{pmatrix} $ because $z=0 .$ Then I get to calculate following: $\int_0^{2 \pi} \begin{pmatrix} -r \sin \phi \\ 2r\cos \phi \\ -r^2 \end{pmatrix}\begin{pmatrix} -\sin \phi \\ \cos \phi \\ 0 \end{pmatrix} d \phi =\int_0^{2 \pi} r \sin^2 \phi+ 2r \cos^2 \phi d \phi = 3\pi r, $ and with $r=3$ follows $ 9 \pi .$ So, they are not equal.  I dont see my mistake. Could not find one in the calculations, so there must be one in the process? I have lost perspective, I appreciate any help a loot !!","Let be a vector field and a hemisphere with , with a circular edge at the - level with the unit normal vector showing outwards. I want to determine 1) as a surface integral 2) with Stoke's theorem. 1) I used the parametrization with and for the unit normal: so integrate 2) for Stoke's theorem I use as parametrization because Then I get to calculate following: and with follows So, they are not equal.  I dont see my mistake. Could not find one in the calculations, so there must be one in the process? I have lost perspective, I appreciate any help a loot !!","F(x,y,z)= \begin{pmatrix} -y \\ 2x\\z \end{pmatrix} A x^2+y^2+z^2=9   z>0  x,y  n  \int_A ( \nabla \times F) n\; do   \Phi(\phi, \theta) =\begin{pmatrix} R\sin \theta \cos\phi \\ R\sin\theta \sin \phi\\R\cos \theta \end{pmatrix}  0 \leq \phi \leq 2 \pi   0\leq \theta\leq \frac{\pi}{2}  \frac{ \delta \Phi }{ \phi} \times \frac{ \delta \Phi}{ \theta} = \begin{pmatrix} R^2\sin^2 \theta \cos \phi \\ R^2\sin^2 \theta \sin \phi \\R^2\sin \theta \cos\theta \end{pmatrix} \int_0^{ \frac{ \pi}{2}} \int_0^{2 \pi} \begin{pmatrix} -Rsin\theta \cos \phi \\ 2R\sin \theta \sin \phi \\R \cos\theta \end{pmatrix}\begin{pmatrix} R^2\sin^2 \theta \cos \phi \\ R^2\sin^2 \theta \sin \phi \\R^2\sin \theta \cos\theta \end{pmatrix} d\phi d\theta = 2 \pi R^3   \Phi ( \phi) = \begin{pmatrix} -\sin \phi \\ \cos \phi \\ 0 \end{pmatrix}  z=0 . \int_0^{2 \pi} \begin{pmatrix} -r \sin \phi \\ 2r\cos \phi \\ -r^2 \end{pmatrix}\begin{pmatrix} -\sin \phi \\ \cos \phi \\ 0 \end{pmatrix} d \phi =\int_0^{2 \pi} r \sin^2 \phi+ 2r \cos^2 \phi d \phi = 3\pi r,  r=3  9 \pi .","['integration', 'multivariable-calculus', 'surface-integrals', 'multiple-integral', 'stokes-theorem']"
28,Obtaining a positive definite covariance matrix of order statistics,Obtaining a positive definite covariance matrix of order statistics,,"Suppose $X_1,\dots,X_n$ are independent samples from some distribution with known absolutely continuous CDF $F:\mathbb{R}\rightarrow[0,1]$. Let $X_{(1)},\dots,X_{(n)}$ denote the order statistics, i.e. the ordered sample. Defining the column vector $X_{(\cdot)}=[X_{(1)},\dots,X_{(n)}]'$, we want to numerically calculate $\mathbb{E}[(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})']$. The most obvious approach uses the fact that for $j,k\in\{1,\dots,n\}$, $j<k$: $$\mathbb{E}X_{(k)}=\int{x\frac{n! [F(x)]^{k-1}[1-F(x)]^{n-k}}{(k-1)!(n-k)!} dF(x)},$$ $$\mathbb{E}X_{(k)}^2=\int{x^2\frac{n! [F(x)]^{k-1}[1-F(x)]^{n-k}}{(k-1)!(n-k)!} dF(x)},$$ $$\mathbb{E}X_{(j)}X_{(k)}=\int{\int{xy\frac{n! [F(x)]^{j-1}[F(y)-F(x)]^{k-1-j}[1-F(y)]^{n-k}}{(j-1)!(k-j-1)!(n-k)!} 1[x\le y] dF(x) } dF(y)},$$ where $1[\cdot]$ is the indicator function. See e.g. Wikipedia here for an informal proof. Using these formulae with standard numerical integration methods works well for small $n$. However, for large $n$ the resulting covariance matrix often ends up non-positive definite unless implausibly many integration nodes are used. This is despite the individual elements of the covariance matrix usually being (loosely) close to a covariance matrix generated via a naïve Monte Carlo approach that guarantees positive definiteness (i.e. draw such a sample of length $n$, then sort it, repeat this lots of times, take the covariance). Is it possible to express these integrals in such a way that the resulting covariance matrix is guaranteed to be positive definite? E.g. is it the case that: $$\mathbb{E}[(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})']=\int{\int{g(u,v) dF(u)}dF(v)},$$ for some function $g:\mathbb{R}^2\rightarrow \mathbb{R}^{n\times n}$ where $g(u,v)$ is positive semi-definite for all $u,v\in\mathbb{R}$.","Suppose $X_1,\dots,X_n$ are independent samples from some distribution with known absolutely continuous CDF $F:\mathbb{R}\rightarrow[0,1]$. Let $X_{(1)},\dots,X_{(n)}$ denote the order statistics, i.e. the ordered sample. Defining the column vector $X_{(\cdot)}=[X_{(1)},\dots,X_{(n)}]'$, we want to numerically calculate $\mathbb{E}[(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})']$. The most obvious approach uses the fact that for $j,k\in\{1,\dots,n\}$, $j<k$: $$\mathbb{E}X_{(k)}=\int{x\frac{n! [F(x)]^{k-1}[1-F(x)]^{n-k}}{(k-1)!(n-k)!} dF(x)},$$ $$\mathbb{E}X_{(k)}^2=\int{x^2\frac{n! [F(x)]^{k-1}[1-F(x)]^{n-k}}{(k-1)!(n-k)!} dF(x)},$$ $$\mathbb{E}X_{(j)}X_{(k)}=\int{\int{xy\frac{n! [F(x)]^{j-1}[F(y)-F(x)]^{k-1-j}[1-F(y)]^{n-k}}{(j-1)!(k-j-1)!(n-k)!} 1[x\le y] dF(x) } dF(y)},$$ where $1[\cdot]$ is the indicator function. See e.g. Wikipedia here for an informal proof. Using these formulae with standard numerical integration methods works well for small $n$. However, for large $n$ the resulting covariance matrix often ends up non-positive definite unless implausibly many integration nodes are used. This is despite the individual elements of the covariance matrix usually being (loosely) close to a covariance matrix generated via a naïve Monte Carlo approach that guarantees positive definiteness (i.e. draw such a sample of length $n$, then sort it, repeat this lots of times, take the covariance). Is it possible to express these integrals in such a way that the resulting covariance matrix is guaranteed to be positive definite? E.g. is it the case that: $$\mathbb{E}[(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})']=\int{\int{g(u,v) dF(u)}dF(v)},$$ for some function $g:\mathbb{R}^2\rightarrow \mathbb{R}^{n\times n}$ where $g(u,v)$ is positive semi-definite for all $u,v\in\mathbb{R}$.",,"['integration', 'numerical-methods', 'random-variables', 'positive-definite', 'order-statistics']"
29,Weird use of Glasser's Master Theorem,Weird use of Glasser's Master Theorem,,"Consider the following enumeration of the rational numbers in $[\,0,1)$: $$0, \frac{1}{2},\frac{1}{3}, \frac{2}{3}, \frac{1}{4}, \frac{3}{4}, \frac{1}{5}, \frac{2}{5}, \frac{3}{5}, \frac{4}{5}, \frac{1}{6}, \frac{5}{6}, \cdots$$ Where the list is first group sorted by ascending denominator, then each group in ascending numerator, and deleting all reducible elements. Obviously $0$ is an exception to this sorting. This is an arbitrary choice, but the simplest to read (in my opinion) Also, the list begins at $0$ i.e. $r_0 = 0, r_1 = \frac{1}{2}, \cdots$ Define the following sequence of functions $\phi_n(x)$ $$\phi_0(x) = x- \pi \cot{\pi x}, \phi_1(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} \right) \\\\ \phi_2(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} + \cot{\pi\left(x-\frac{1}{3}\right)}\right) \\\\ \phi_3(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} + \cot{\pi\left(x-\frac{1}{3}\right)} + \cot{\left(x-\frac{2}{3}\right)}\right) \\\\ \phi_n(x) = x - \pi \left( \sum_{k=0}^n \cot{\pi \left(x-r_k\right)}\right)$$ where $r_k$ is the $k^{\text{th}}$ element in the enumeration described above. Assuming $\displaystyle A = \text{PV} \left( \int_{-\infty}^\infty f(x) \, \text{d}x \right)$ is finite and is Lesbegue measurable, consider the following sequence of integrals $$I_n = \text{PV} \left( \int_{\infty}^\infty f(\phi_n(x)) \, \text{d}x \right)$$ It can be shown with Glasser's Master Theorem, that for any finite $n$, $I_n = A$ What happens in the limit, for $n \to \infty$ ? Is the limit of $I_n$ still equal to $A$? If so, how can I rigorously prove it? Edit: The motivation of this question was out of simple curiosity. I wanted to see what would happen to the integral if I densely filled the function's argument with singularities, and whether any infinite weirdness happens, as is often the case. An internet person argued that if one chooses to accept the Axiom of Countable Choice, then the limit exists and is equal to $A$. Is this the case?","Consider the following enumeration of the rational numbers in $[\,0,1)$: $$0, \frac{1}{2},\frac{1}{3}, \frac{2}{3}, \frac{1}{4}, \frac{3}{4}, \frac{1}{5}, \frac{2}{5}, \frac{3}{5}, \frac{4}{5}, \frac{1}{6}, \frac{5}{6}, \cdots$$ Where the list is first group sorted by ascending denominator, then each group in ascending numerator, and deleting all reducible elements. Obviously $0$ is an exception to this sorting. This is an arbitrary choice, but the simplest to read (in my opinion) Also, the list begins at $0$ i.e. $r_0 = 0, r_1 = \frac{1}{2}, \cdots$ Define the following sequence of functions $\phi_n(x)$ $$\phi_0(x) = x- \pi \cot{\pi x}, \phi_1(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} \right) \\\\ \phi_2(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} + \cot{\pi\left(x-\frac{1}{3}\right)}\right) \\\\ \phi_3(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} + \cot{\pi\left(x-\frac{1}{3}\right)} + \cot{\left(x-\frac{2}{3}\right)}\right) \\\\ \phi_n(x) = x - \pi \left( \sum_{k=0}^n \cot{\pi \left(x-r_k\right)}\right)$$ where $r_k$ is the $k^{\text{th}}$ element in the enumeration described above. Assuming $\displaystyle A = \text{PV} \left( \int_{-\infty}^\infty f(x) \, \text{d}x \right)$ is finite and is Lesbegue measurable, consider the following sequence of integrals $$I_n = \text{PV} \left( \int_{\infty}^\infty f(\phi_n(x)) \, \text{d}x \right)$$ It can be shown with Glasser's Master Theorem, that for any finite $n$, $I_n = A$ What happens in the limit, for $n \to \infty$ ? Is the limit of $I_n$ still equal to $A$? If so, how can I rigorously prove it? Edit: The motivation of this question was out of simple curiosity. I wanted to see what would happen to the integral if I densely filled the function's argument with singularities, and whether any infinite weirdness happens, as is often the case. An internet person argued that if one chooses to accept the Axiom of Countable Choice, then the limit exists and is equal to $A$. Is this the case?",,"['integration', 'limits', 'improper-integrals', 'lebesgue-integral', 'lebesgue-measure']"
30,"Pointwise limit of continuous functions, but not Riemann integrable.","Pointwise limit of continuous functions, but not Riemann integrable.",,"I am trying to find a simple example of a function $f:[0,1]\rightarrow\mathbb{R}$ which is a pointwise limit of continuous functions, but is not Riemann integrable. I know the classical example where we build some functions $F_1,F_2,\dots$ on a cantor-like set, and then define $f_n = F_1.F_2.\dots . F_n$, and so on. But I was thinking whether there is a simpler example, one that you could present to students with no experience in Measure Theory. Any help is welcome.","I am trying to find a simple example of a function $f:[0,1]\rightarrow\mathbb{R}$ which is a pointwise limit of continuous functions, but is not Riemann integrable. I know the classical example where we build some functions $F_1,F_2,\dots$ on a cantor-like set, and then define $f_n = F_1.F_2.\dots . F_n$, and so on. But I was thinking whether there is a simpler example, one that you could present to students with no experience in Measure Theory. Any help is welcome.",,"['integration', 'sequences-and-series', 'pointwise-convergence']"
31,Integral over a curve in the sphere,Integral over a curve in the sphere,,"Fix $r\in(0,2)$, and consider arbitrary points $\mathbf{x},\mathbf{q}\in\mathbf{S}^2\subseteq\mathbf{R}^{3}$ such that $q_z > 1-r$ and $x_z > 1-r$. Consider the path $\gamma:[0,2\pi)\to\mathbf{S}^2$ given by $$\gamma(t)=(R\cos(t),R\sin(t),1-r),$$ where $R=\sqrt{r(2-r)}$. I want to compute $$\int_{\gamma([0,2\pi])}\frac{\ln(1-\mathbf{y}\cdot\mathbf{q})}{1-\mathbf{x}\cdot\mathbf{y}}~d\ell(\mathbf{y}).$$ I've been having a lot of trouble actually working this out by hand, and I'm not sure if there are any clever techniques that I could use to evaluate the above expression. I came across this integral when working out a representation formula for a solution to some sort of Dirichlet problem, and it would be very nice to have a closed form expression of the above. If there isn't any nice closed form solution for such an expression, any ideas on numerical techniques for computing the above integral quickly would be also appreciated.","Fix $r\in(0,2)$, and consider arbitrary points $\mathbf{x},\mathbf{q}\in\mathbf{S}^2\subseteq\mathbf{R}^{3}$ such that $q_z > 1-r$ and $x_z > 1-r$. Consider the path $\gamma:[0,2\pi)\to\mathbf{S}^2$ given by $$\gamma(t)=(R\cos(t),R\sin(t),1-r),$$ where $R=\sqrt{r(2-r)}$. I want to compute $$\int_{\gamma([0,2\pi])}\frac{\ln(1-\mathbf{y}\cdot\mathbf{q})}{1-\mathbf{x}\cdot\mathbf{y}}~d\ell(\mathbf{y}).$$ I've been having a lot of trouble actually working this out by hand, and I'm not sure if there are any clever techniques that I could use to evaluate the above expression. I came across this integral when working out a representation formula for a solution to some sort of Dirichlet problem, and it would be very nice to have a closed form expression of the above. If there isn't any nice closed form solution for such an expression, any ideas on numerical techniques for computing the above integral quickly would be also appreciated.",,"['integration', 'differential-geometry']"
32,"Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . .","Theorem 6.15 in Baby Rudin: If ,  is bounded on ,  is continuous at , and , then . . .","a<s<b f [a,b] f s \alpha(x)=I(x-s)","Here is Theorem 6.15 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then    $$ \int_a^b f d \alpha = f(s). $$ Here is Definition 6.14: The unit step function $I$ is defined by    $$ I(x) = \begin{cases} 0 \qquad & (x \leq 0), \\ 1 \qquad & (x > 0). \end{cases} $$ And, here is Rudin's proof: Consider partitions $P = \left\{ \ x_0, x_1, x_2, x_3 \  \right\}$, where $x_0 = a$, and $ x_1 = s < x_2 < x_3 = b$. Then    $$ U(P, f, \alpha) = M_2, \qquad L(P, f, \alpha) = m_2. $$    Since $f$ is continuous at $s$, we see that $M_2$ and $m_2$ converge to $f(s)$ as $x_2 \to s$. Now here is my reading of Rudin's proof: First of all, here are Definitions 6.1 and 6.2 in Baby Rudin, 3rd edition: Definition 6.1: Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we mean a finite set of points $x_0, x_1, \ldots, x_n$, where    $$ a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b.$$   We write    $$ \Delta x_i = x_i - x_{i-1} \qquad (i = 1, \ldots, n). $$    Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put   $$ \begin{align}  M_i &= \sup f(x) \qquad (x_{i-1} \leq x \leq x_i), \\ m_i &= \inf f(x) \qquad (x_{i-1} \leq x \leq x_i), \\ U(P, f) &= \sum_{i=1}^n M_i \Delta x_i, \\ L(P, f) &= \sum_{i=1}^n m_i \Delta x_i, \end{align}  $$   and finally    $$  \begin{align} \tag{1} \overline{\int}_a^b f dx &= \inf U(P, f), \\ \tag{2} \underline{\int}_a^b f dx &= \sup L(P, f), \end{align} $$   where the $\inf$ and the $\sup$ are taken over all partitions $P$ of $[a, b]$. The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively. If the upper and lower integrals are equal, we say that $f$ is Riemann-integrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemann-integrable functions), and we denote the common value of (1) and (2) by    $$ \tag{3} \int_a^b f dx, $$   or by    $$ \tag{4} \int_a^b f(x) dx. $$   This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that    $$ m \leq f(x) \leq M \qquad (a \leq x \leq b). $$   Hence, for every $P$,    $$ m(b-a) \leq L(P, f) \leq U(P, f) \leq M (b-a), $$   so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. . . . Definition 6.2: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write    $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$   It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put    $$  \begin{align} U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\ L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i,  \end{align} $$   where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define    $$ \begin{align} \tag{5} \overline{\int}_a^b f d \alpha = \inf U(P, f, \alpha), \\ \tag{6} \underline{\int}_a^b f d \alpha = \sup L(P, f, \alpha),  \end{align} $$   the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by    $$ \tag{7} \int_a^b f d \alpha $$   or sometimes by    $$ \tag{8} \int_a^b f(x) d \alpha(x). $$   This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. Now for the proof of Theorem 6.15: For any partition $P = \left\{ \ x_0, x_1, x_2 , x_3  \ \right\}$ of the closed interval $[a, b]$, where    $$a = x_0  < x_1 = s < x_2 < x_3 = b,$$    we note that    $$ \begin{align}   U(P, f, \alpha)  &= M_1 \left[ \alpha \left( x_1 \right) - \alpha \left( x_0 \right) \right] + M_2 \left[ \alpha \left( x_2 \right) - \alpha \left( x_1 \right) \right] + M_3 \left[ \alpha \left( x_3 \right) - \alpha \left( x_2 \right) \right] \\ &= M_1 ( 0 - 0) + M_2 ( 1 - 0) + M_3 ( 1 - 1 )  \\ &= M_2,  \end{align} $$   and similarly, $L(P, f, \alpha) = m_2$; thus    $$ U(P, f, \alpha) = M_2, \qquad L(P, f, \alpha) = m_2, \tag{A} $$   and so $$U(P, f, \alpha) - L(P, f, \alpha) = M_2 - m_2, \tag{B} $$ Let $\varepsilon > 0$ be given. As $f$ is continuous at the point $s \in (a, b)$, so  we can find a real number $\delta > 0$ such that $$ a < s-\delta < s < s + \delta < b,$$   and    $$ \lvert f(x) - f(s) \rvert < { \varepsilon \over 4 } $$   for all $x$ which satisfy $\lvert x-s \rvert < \delta$. Thus, if $s < x_2 < s+\delta$, then we must have    $$ \lvert f(x) - f(s) \rvert <  { \varepsilon \over 4 } $$   for all $x \in \left[ s, x_2 \right] =  \left[ x_1 , x_2 \right]$.    That is,    $$ f(s) - { \varepsilon \over 4 } < f(x) < f(s) + { \varepsilon \over 4 }$$   for all $x \in \left[ x_1, x_2 \right]$. Therefore, we must have    $$ f(s) - { \varepsilon \over 4 } \leq m_2 \leq M_2 \leq f(s) + { \varepsilon \over 4 }, \tag{C} $$    and so, by (B) above,    $$  U(P, f, \alpha) - L(P, f, \alpha) = M_2 - m_2 \leq   { \varepsilon \over 2 } < \varepsilon,$$   from which it follows (by Theorem 6.6 in Baby Rudin, 3rd edition) that $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Now as    $$ m_2 =  L(P, f, \alpha) \leq \int_a^b f d \alpha \leq U(P, f, \alpha) = M_2, \qquad \mbox{ [ using (A) ]  } $$    so (C) implies that    $$ f(s) - { \varepsilon \over 4 } \leq m_2  \leq \int_a^b f d \alpha  \leq M_2 \leq f(s) + { \varepsilon \over 4 },  $$     and so    $$ f(s) - { \varepsilon \over 4 }  \leq \int_a^b f d \alpha   \leq f(s) + { \varepsilon \over 4 },  $$    which in turn implies that    $$ \left\lvert \int_a^b f d \alpha - f(s) \right\rvert \leq  { \varepsilon \over 4 } < \varepsilon$$   for every real number $\varepsilon > 0$, showing that    $$ \int_a^b f d \alpha = f(s), $$   as required. Is my rendering of Rudin's proof correct and to the point? If not, then at which point have I gone astray?","Here is Theorem 6.15 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then    $$ \int_a^b f d \alpha = f(s). $$ Here is Definition 6.14: The unit step function $I$ is defined by    $$ I(x) = \begin{cases} 0 \qquad & (x \leq 0), \\ 1 \qquad & (x > 0). \end{cases} $$ And, here is Rudin's proof: Consider partitions $P = \left\{ \ x_0, x_1, x_2, x_3 \  \right\}$, where $x_0 = a$, and $ x_1 = s < x_2 < x_3 = b$. Then    $$ U(P, f, \alpha) = M_2, \qquad L(P, f, \alpha) = m_2. $$    Since $f$ is continuous at $s$, we see that $M_2$ and $m_2$ converge to $f(s)$ as $x_2 \to s$. Now here is my reading of Rudin's proof: First of all, here are Definitions 6.1 and 6.2 in Baby Rudin, 3rd edition: Definition 6.1: Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we mean a finite set of points $x_0, x_1, \ldots, x_n$, where    $$ a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b.$$   We write    $$ \Delta x_i = x_i - x_{i-1} \qquad (i = 1, \ldots, n). $$    Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put   $$ \begin{align}  M_i &= \sup f(x) \qquad (x_{i-1} \leq x \leq x_i), \\ m_i &= \inf f(x) \qquad (x_{i-1} \leq x \leq x_i), \\ U(P, f) &= \sum_{i=1}^n M_i \Delta x_i, \\ L(P, f) &= \sum_{i=1}^n m_i \Delta x_i, \end{align}  $$   and finally    $$  \begin{align} \tag{1} \overline{\int}_a^b f dx &= \inf U(P, f), \\ \tag{2} \underline{\int}_a^b f dx &= \sup L(P, f), \end{align} $$   where the $\inf$ and the $\sup$ are taken over all partitions $P$ of $[a, b]$. The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively. If the upper and lower integrals are equal, we say that $f$ is Riemann-integrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemann-integrable functions), and we denote the common value of (1) and (2) by    $$ \tag{3} \int_a^b f dx, $$   or by    $$ \tag{4} \int_a^b f(x) dx. $$   This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that    $$ m \leq f(x) \leq M \qquad (a \leq x \leq b). $$   Hence, for every $P$,    $$ m(b-a) \leq L(P, f) \leq U(P, f) \leq M (b-a), $$   so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. . . . Definition 6.2: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write    $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$   It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put    $$  \begin{align} U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\ L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i,  \end{align} $$   where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define    $$ \begin{align} \tag{5} \overline{\int}_a^b f d \alpha = \inf U(P, f, \alpha), \\ \tag{6} \underline{\int}_a^b f d \alpha = \sup L(P, f, \alpha),  \end{align} $$   the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by    $$ \tag{7} \int_a^b f d \alpha $$   or sometimes by    $$ \tag{8} \int_a^b f(x) d \alpha(x). $$   This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. Now for the proof of Theorem 6.15: For any partition $P = \left\{ \ x_0, x_1, x_2 , x_3  \ \right\}$ of the closed interval $[a, b]$, where    $$a = x_0  < x_1 = s < x_2 < x_3 = b,$$    we note that    $$ \begin{align}   U(P, f, \alpha)  &= M_1 \left[ \alpha \left( x_1 \right) - \alpha \left( x_0 \right) \right] + M_2 \left[ \alpha \left( x_2 \right) - \alpha \left( x_1 \right) \right] + M_3 \left[ \alpha \left( x_3 \right) - \alpha \left( x_2 \right) \right] \\ &= M_1 ( 0 - 0) + M_2 ( 1 - 0) + M_3 ( 1 - 1 )  \\ &= M_2,  \end{align} $$   and similarly, $L(P, f, \alpha) = m_2$; thus    $$ U(P, f, \alpha) = M_2, \qquad L(P, f, \alpha) = m_2, \tag{A} $$   and so $$U(P, f, \alpha) - L(P, f, \alpha) = M_2 - m_2, \tag{B} $$ Let $\varepsilon > 0$ be given. As $f$ is continuous at the point $s \in (a, b)$, so  we can find a real number $\delta > 0$ such that $$ a < s-\delta < s < s + \delta < b,$$   and    $$ \lvert f(x) - f(s) \rvert < { \varepsilon \over 4 } $$   for all $x$ which satisfy $\lvert x-s \rvert < \delta$. Thus, if $s < x_2 < s+\delta$, then we must have    $$ \lvert f(x) - f(s) \rvert <  { \varepsilon \over 4 } $$   for all $x \in \left[ s, x_2 \right] =  \left[ x_1 , x_2 \right]$.    That is,    $$ f(s) - { \varepsilon \over 4 } < f(x) < f(s) + { \varepsilon \over 4 }$$   for all $x \in \left[ x_1, x_2 \right]$. Therefore, we must have    $$ f(s) - { \varepsilon \over 4 } \leq m_2 \leq M_2 \leq f(s) + { \varepsilon \over 4 }, \tag{C} $$    and so, by (B) above,    $$  U(P, f, \alpha) - L(P, f, \alpha) = M_2 - m_2 \leq   { \varepsilon \over 2 } < \varepsilon,$$   from which it follows (by Theorem 6.6 in Baby Rudin, 3rd edition) that $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Now as    $$ m_2 =  L(P, f, \alpha) \leq \int_a^b f d \alpha \leq U(P, f, \alpha) = M_2, \qquad \mbox{ [ using (A) ]  } $$    so (C) implies that    $$ f(s) - { \varepsilon \over 4 } \leq m_2  \leq \int_a^b f d \alpha  \leq M_2 \leq f(s) + { \varepsilon \over 4 },  $$     and so    $$ f(s) - { \varepsilon \over 4 }  \leq \int_a^b f d \alpha   \leq f(s) + { \varepsilon \over 4 },  $$    which in turn implies that    $$ \left\lvert \int_a^b f d \alpha - f(s) \right\rvert \leq  { \varepsilon \over 4 } < \varepsilon$$   for every real number $\varepsilon > 0$, showing that    $$ \int_a^b f d \alpha = f(s), $$   as required. Is my rendering of Rudin's proof correct and to the point? If not, then at which point have I gone astray?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'definite-integrals']"
33,"Theorem 6.12 (d) in Baby Rudin: If $\lvert f(x) \rvert \leq M$ on $[a, b]$, then $\lvert \int_a^b f d\alpha \rvert \leq \ldots$","Theorem 6.12 (d) in Baby Rudin: If  on , then","\lvert f(x) \rvert \leq M [a, b] \lvert \int_a^b f d\alpha \rvert \leq \ldots","Here is Theorem 6.12 (d) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $\lvert f(x) \rvert \leq M$ on $[a, b]$, then    $$ \left\lvert \int_a^b f d\alpha \right\rvert \leq M \left[ \alpha(b) - \alpha(a) \right]. $$ Here is my proof of this assertion. As $\lvert f(x) \rvert \leq M$ on $[a, b]$, so $-M \leq f(x) \leq M$ on $[a, b]$ and for every partition $P$ of $[a, b]$, we have    $$ -M \left[ \alpha(b) - \alpha(a) \right] = - M \sum_{i=1}^n \left[ \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right)  \right] \leq L(P, f, \alpha ) \leq U(P, f, \alpha) \leq M \sum_{i=1}^n \left[ \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right)  \right] = M \left[ \alpha(b) - \alpha(a) \right]. \tag{1} $$   And, as $f \in \mathscr{R}(\alpha)$ on $[a, b]$, so for every partition $P$ of $[a, b]$, we also have    $$ L(P, f, \alpha ) \leq \int_a^b f d \alpha \leq U(P, f, \alpha). \tag{2} $$   From (1) and (2) we obtain    $$-M \left[ \alpha(b) - \alpha(a) \right] \leq   L(P, f, \alpha ) \leq \int_a^b f d \alpha \leq U(P, f, \alpha) \leq M \left[ \alpha(b) - \alpha(a) \right], $$   and so    $$-M \left[ \alpha(b) - \alpha(a) \right] \leq  \int_a^b f d \alpha \leq M \left[ \alpha(b) - \alpha(a) \right], $$   which implies that    $$ \left\lvert \int_a^b f d \alpha \right\rvert \leq M \left[ \alpha(b) - \alpha(a) \right], $$   as required. Is this proof lacking in logic, rigor, or presentation?","Here is Theorem 6.12 (d) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $\lvert f(x) \rvert \leq M$ on $[a, b]$, then    $$ \left\lvert \int_a^b f d\alpha \right\rvert \leq M \left[ \alpha(b) - \alpha(a) \right]. $$ Here is my proof of this assertion. As $\lvert f(x) \rvert \leq M$ on $[a, b]$, so $-M \leq f(x) \leq M$ on $[a, b]$ and for every partition $P$ of $[a, b]$, we have    $$ -M \left[ \alpha(b) - \alpha(a) \right] = - M \sum_{i=1}^n \left[ \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right)  \right] \leq L(P, f, \alpha ) \leq U(P, f, \alpha) \leq M \sum_{i=1}^n \left[ \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right)  \right] = M \left[ \alpha(b) - \alpha(a) \right]. \tag{1} $$   And, as $f \in \mathscr{R}(\alpha)$ on $[a, b]$, so for every partition $P$ of $[a, b]$, we also have    $$ L(P, f, \alpha ) \leq \int_a^b f d \alpha \leq U(P, f, \alpha). \tag{2} $$   From (1) and (2) we obtain    $$-M \left[ \alpha(b) - \alpha(a) \right] \leq   L(P, f, \alpha ) \leq \int_a^b f d \alpha \leq U(P, f, \alpha) \leq M \left[ \alpha(b) - \alpha(a) \right], $$   and so    $$-M \left[ \alpha(b) - \alpha(a) \right] \leq  \int_a^b f d \alpha \leq M \left[ \alpha(b) - \alpha(a) \right], $$   which implies that    $$ \left\lvert \int_a^b f d \alpha \right\rvert \leq M \left[ \alpha(b) - \alpha(a) \right], $$   as required. Is this proof lacking in logic, rigor, or presentation?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'definite-integrals']"
34,Integral depending measurable on a parameter,Integral depending measurable on a parameter,,"Let $(\Omega, \Sigma, \mu)$ be a measurable space and let $f\colon [0,1] \times \Omega \to \mathbb R$ be such that $x\mapsto f(t,x)$ is integrable for all $t\in [0,1]$. Then $$F\colon [0,1] \to \mathbb R,\quad t\mapsto F(t) = \int_\Omega f(t,x) d\mu(x)$$ is well-defined. Question : Are there mild conditions on $t\mapsto f(t,x)$ such that $F$ is measurable (perhaps even if $[0,1]$ is replaced by some measurable space)? I have found the following well-known statements: If $t\mapsto f(t,x)$ is continuous for a.e. $x\in \Omega$ and if $|f(t,x)| \leq g(x)$ for all $(t,x)$ and some integrable $g$, then $F$ is known to be continuous. If $\Omega$ is $\sigma$-finite and $(x,t)\mapsto f(t,x)$ is integrable on the product-space, then $F$ is integrable and one can interchange the order of integration by Fubini's theorem. Now I wonder, if one can drop continuity in $t$ without asking $f$ to be product-measurable.","Let $(\Omega, \Sigma, \mu)$ be a measurable space and let $f\colon [0,1] \times \Omega \to \mathbb R$ be such that $x\mapsto f(t,x)$ is integrable for all $t\in [0,1]$. Then $$F\colon [0,1] \to \mathbb R,\quad t\mapsto F(t) = \int_\Omega f(t,x) d\mu(x)$$ is well-defined. Question : Are there mild conditions on $t\mapsto f(t,x)$ such that $F$ is measurable (perhaps even if $[0,1]$ is replaced by some measurable space)? I have found the following well-known statements: If $t\mapsto f(t,x)$ is continuous for a.e. $x\in \Omega$ and if $|f(t,x)| \leq g(x)$ for all $(t,x)$ and some integrable $g$, then $F$ is known to be continuous. If $\Omega$ is $\sigma$-finite and $(x,t)\mapsto f(t,x)$ is integrable on the product-space, then $F$ is integrable and one can interchange the order of integration by Fubini's theorem. Now I wonder, if one can drop continuity in $t$ without asking $f$ to be product-measurable.",,"['integration', 'measure-theory']"
35,"Real methods for the evaluating $\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt$",Real methods for the evaluating,"\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt","One can show by integrating the following function $$f(z) = z^{n-m-1}(1+z^2)^m$$ around the following contour By equating the circular part of $|z|=1$ and the line on the imaginary part. $$\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt=2^{-m}\sin\left(\frac{n\pi -m\pi}{2} \right)\int^1_{0}t^{n-m-1}(1-t^2)^m\,dt \tag{1}$$ The left side looks close to the Beta representation $$2\int^{\pi/2}_0 \sin^{2n-1}(t) \cos^{2m-1}(t)\,dt = \frac{\Gamma(n)\Gamma(m)}{\Gamma(n+m)} \tag{2}$$ Questions Can we show (1) using elementary transformations ? What real methods can we use to prove the integral $$\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt=\frac{\pi \Gamma(m+1)}{2^{m+1}\Gamma\left(\frac{n+m+2}{2}\right)\Gamma\left(\frac{2-n+m}{2}\right)}\tag{3}$$","One can show by integrating the following function $$f(z) = z^{n-m-1}(1+z^2)^m$$ around the following contour By equating the circular part of $|z|=1$ and the line on the imaginary part. $$\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt=2^{-m}\sin\left(\frac{n\pi -m\pi}{2} \right)\int^1_{0}t^{n-m-1}(1-t^2)^m\,dt \tag{1}$$ The left side looks close to the Beta representation $$2\int^{\pi/2}_0 \sin^{2n-1}(t) \cos^{2m-1}(t)\,dt = \frac{\Gamma(n)\Gamma(m)}{\Gamma(n+m)} \tag{2}$$ Questions Can we show (1) using elementary transformations ? What real methods can we use to prove the integral $$\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt=\frac{\pi \Gamma(m+1)}{2^{m+1}\Gamma\left(\frac{n+m+2}{2}\right)\Gamma\left(\frac{2-n+m}{2}\right)}\tag{3}$$",,"['integration', 'definite-integrals', 'special-functions', 'beta-function']"
36,"""Non-homotopic"" change of variables in integral","""Non-homotopic"" change of variables in integral",,"Let $X$ be a smooth manifold endowed with a closed $2$-form $\omega$ and let $L$ be a submanifold of $X$ such that $\omega|_L = 0$. Consider a smooth map $u \colon S^1 \times [0,1] \to X$ such that $u(t,0) \in L$, $u(t,1) \in L$ for any $t$ (so that $u$ is a ""cylinder glued to $L$""): $\hspace{11em}$ I consider the integral $$ \int_{S^1 \times [0,1]} u^* \omega. $$ Suppose that the path $u(0,\cdot)$ can be homotopied to a point in $L$. Then $u$ can be homotopied to a map $\tilde u$ such that $\tilde u(0,\cdot)$ is constant: $\hspace{5em}$ Then it follows from Stokes formula and from $\omega|_L = 0$ that  $$   \int_{S^1 \times [0,1]} u^* \omega = \int_{S^1 \times [0,1]} \tilde u^* \omega. $$ Now I draw a schematic picture of $\tilde u(S^1 \times [0,1])$ and it looks the image of a map $v \colon D^2 \to X$, $v|_{\partial D^2} \in L$, where $v$ maps two points on $\partial D$ to the same point in $L$ (here $D^2 = \{ z \in \mathbb C, \; |z| \leq 1\}$) so that I expect $$    \int_{S^1 \times [0,1]} \tilde u^*\omega = \int_{D^2} v^* \omega. $$ Formally, $\tilde u$ and $v$ are hot homotopy equivalent since they are defined on different spaces. Could you help me to justify this ""change of variables"" formula?","Let $X$ be a smooth manifold endowed with a closed $2$-form $\omega$ and let $L$ be a submanifold of $X$ such that $\omega|_L = 0$. Consider a smooth map $u \colon S^1 \times [0,1] \to X$ such that $u(t,0) \in L$, $u(t,1) \in L$ for any $t$ (so that $u$ is a ""cylinder glued to $L$""): $\hspace{11em}$ I consider the integral $$ \int_{S^1 \times [0,1]} u^* \omega. $$ Suppose that the path $u(0,\cdot)$ can be homotopied to a point in $L$. Then $u$ can be homotopied to a map $\tilde u$ such that $\tilde u(0,\cdot)$ is constant: $\hspace{5em}$ Then it follows from Stokes formula and from $\omega|_L = 0$ that  $$   \int_{S^1 \times [0,1]} u^* \omega = \int_{S^1 \times [0,1]} \tilde u^* \omega. $$ Now I draw a schematic picture of $\tilde u(S^1 \times [0,1])$ and it looks the image of a map $v \colon D^2 \to X$, $v|_{\partial D^2} \in L$, where $v$ maps two points on $\partial D$ to the same point in $L$ (here $D^2 = \{ z \in \mathbb C, \; |z| \leq 1\}$) so that I expect $$    \int_{S^1 \times [0,1]} \tilde u^*\omega = \int_{D^2} v^* \omega. $$ Formally, $\tilde u$ and $v$ are hot homotopy equivalent since they are defined on different spaces. Could you help me to justify this ""change of variables"" formula?",,"['integration', 'differential-geometry']"
37,Finding volume using integration,Finding volume using integration,,"Find the volume generated when the region bounded by $y=x^3$ and the $x$-axis between $x=2$ and $x=7$ is rotated through $360^{\circ}$ about the $x$-axis. Here is my attempt is this correct? \begin{align*}   y &= x^3 \, , \; 2\le x\le7 \\   V &= \int_a^b \pi y^2 \, dx \\   &= \int_2^7\pi(x^3)^2 \, dx \\   &= \pi\int_2^7x^6 \, dx \\   &= \pi \left[ \frac{x^7}7 \right]_2^7 \\   &= \pi \left( 117649-\frac{128}{7} \right) \\   &= \frac{823415\pi}{7} \end{align*}","Find the volume generated when the region bounded by $y=x^3$ and the $x$-axis between $x=2$ and $x=7$ is rotated through $360^{\circ}$ about the $x$-axis. Here is my attempt is this correct? \begin{align*}   y &= x^3 \, , \; 2\le x\le7 \\   V &= \int_a^b \pi y^2 \, dx \\   &= \int_2^7\pi(x^3)^2 \, dx \\   &= \pi\int_2^7x^6 \, dx \\   &= \pi \left[ \frac{x^7}7 \right]_2^7 \\   &= \pi \left( 117649-\frac{128}{7} \right) \\   &= \frac{823415\pi}{7} \end{align*}",,['integration']
38,"Can conditionally convergent series be interpreted as a ""generalized Henstock-Kurzweil integral""?","Can conditionally convergent series be interpreted as a ""generalized Henstock-Kurzweil integral""?",,"One amazing thing about the Lebesgue integral is that is defined w.r.t. to a given measure and that there a lot of different measures making the Lebesgue integration a very general tool (consider Harmonic Analysis). The Henstock-Kurzweil integral doesn't seem anywhere nearly as general (at least not in this sense), so it looks less appealing. Although for real functions it's generally better because in that sense it is more general. On Wikipedia it says that the Henstock–Kurzweil integral can be thought of as a ""non-absolutely convergent version of Lebesgue integral"" (w.r.t. to Lebesgue measure I assume). Having just a tiny bit of hope for a more ""general Henstock-Kurzweil integral"" an obvious question arises: Is there a generalization of the Henstock-Kurzweil integral that you can take w.r.t. to different ""things"" (similar to measures) that also includes conditionally convergent series as ""integrals of sequences""? (one should compare this idea to Lebesgue integration w.r.t. counting measure)","One amazing thing about the Lebesgue integral is that is defined w.r.t. to a given measure and that there a lot of different measures making the Lebesgue integration a very general tool (consider Harmonic Analysis). The Henstock-Kurzweil integral doesn't seem anywhere nearly as general (at least not in this sense), so it looks less appealing. Although for real functions it's generally better because in that sense it is more general. On Wikipedia it says that the Henstock–Kurzweil integral can be thought of as a ""non-absolutely convergent version of Lebesgue integral"" (w.r.t. to Lebesgue measure I assume). Having just a tiny bit of hope for a more ""general Henstock-Kurzweil integral"" an obvious question arises: Is there a generalization of the Henstock-Kurzweil integral that you can take w.r.t. to different ""things"" (similar to measures) that also includes conditionally convergent series as ""integrals of sequences""? (one should compare this idea to Lebesgue integration w.r.t. counting measure)",,"['real-analysis', 'integration', 'sequences-and-series', 'gauge-integral']"
39,"On the integral $\int_0^\infty J_0(x)\,dx$",On the integral,"\int_0^\infty J_0(x)\,dx","I'm not sure if the following result is correct, but I also have no clue where I could have possibly made a mistake. \begin{align} \int_0^\infty J_0(x)\,dx &= \frac{1}{2}\int_{-\infty}^\infty J_0(x)\,dx \tag1\\ &=\frac{1}{4\pi}\int_{-\infty}^\infty \int_{-\pi}^\pi e^{ix\cos\phi} \, d\phi \, dx \tag2\\ &=\frac{1}{4\pi}\int_{-\pi}^\pi \int_{-\infty}^\infty e^{ix\cos\phi} \, dx \, d\phi \tag3\\ &=\frac{1}{2}\int_{-\pi}^\pi \delta(\cos\phi)\,d\phi \tag4\\ &=\frac{1}{2}\int_{-\pi}^\pi \left[\sum_{n=-\infty}^\infty \delta\left(\phi-\left(n+\frac{1}{2}\right)\pi\right)\right] \, d\phi \tag5\\ &=\frac{1}{2}\cdot 2 = 1 \tag6 \end{align} In $(1)$, I used the fact that $J_0(x)$ is even. In $(2)$, I used the integral representation of the Bessel Function of the First Kind. In $(3)$, I used Fubini's Theorem to interchange orders of integration. In $(4)$, I used the complex definition of the Delta Function. In $(5)$, I used the fact that $\delta(f(x)) = \sum\frac{\delta(x-x_i)}{|f'(x_i)|}$, where the $x_i$ are the roots of the function $f(x)$. In $(6)$, I used that only two delta functions contribute to the integral - the ones with singularities at $\pm\frac{\pi}{2}$, and the integral over each of these singularities is $1$. However, the reason that I am doubting my result is because the Bessel Function $J_0(x)$ looks somewhat like $\frac{\sin(x)}{\sqrt{x}}$ for large $x$ and actually encloses a larger area with the $x$-axis close to $0$ (as $J_0(0)= 1$, whereas $\lim\limits_{x\rightarrow 0}\frac{\sin(x)}{\sqrt{x}}=0$). Therefore, I would expect  that $$\int_0^\infty J_0(x) \, dx \geq \int_0^\infty \frac{\sin(x)}{\sqrt{x}} = \sqrt{\frac{\pi}{2}}\approx 1.25$$ would hold. Is my intuition wrong here, and if so, which part of my intuition is off? Or have I made a computational mistake somewhere?","I'm not sure if the following result is correct, but I also have no clue where I could have possibly made a mistake. \begin{align} \int_0^\infty J_0(x)\,dx &= \frac{1}{2}\int_{-\infty}^\infty J_0(x)\,dx \tag1\\ &=\frac{1}{4\pi}\int_{-\infty}^\infty \int_{-\pi}^\pi e^{ix\cos\phi} \, d\phi \, dx \tag2\\ &=\frac{1}{4\pi}\int_{-\pi}^\pi \int_{-\infty}^\infty e^{ix\cos\phi} \, dx \, d\phi \tag3\\ &=\frac{1}{2}\int_{-\pi}^\pi \delta(\cos\phi)\,d\phi \tag4\\ &=\frac{1}{2}\int_{-\pi}^\pi \left[\sum_{n=-\infty}^\infty \delta\left(\phi-\left(n+\frac{1}{2}\right)\pi\right)\right] \, d\phi \tag5\\ &=\frac{1}{2}\cdot 2 = 1 \tag6 \end{align} In $(1)$, I used the fact that $J_0(x)$ is even. In $(2)$, I used the integral representation of the Bessel Function of the First Kind. In $(3)$, I used Fubini's Theorem to interchange orders of integration. In $(4)$, I used the complex definition of the Delta Function. In $(5)$, I used the fact that $\delta(f(x)) = \sum\frac{\delta(x-x_i)}{|f'(x_i)|}$, where the $x_i$ are the roots of the function $f(x)$. In $(6)$, I used that only two delta functions contribute to the integral - the ones with singularities at $\pm\frac{\pi}{2}$, and the integral over each of these singularities is $1$. However, the reason that I am doubting my result is because the Bessel Function $J_0(x)$ looks somewhat like $\frac{\sin(x)}{\sqrt{x}}$ for large $x$ and actually encloses a larger area with the $x$-axis close to $0$ (as $J_0(0)= 1$, whereas $\lim\limits_{x\rightarrow 0}\frac{\sin(x)}{\sqrt{x}}=0$). Therefore, I would expect  that $$\int_0^\infty J_0(x) \, dx \geq \int_0^\infty \frac{\sin(x)}{\sqrt{x}} = \sqrt{\frac{\pi}{2}}\approx 1.25$$ would hold. Is my intuition wrong here, and if so, which part of my intuition is off? Or have I made a computational mistake somewhere?",,"['integration', 'definite-integrals', 'bessel-functions']"
40,"Cavalieri's, Mamikon's and Peano's methods for areas/volumes: how are they related?","Cavalieri's, Mamikon's and Peano's methods for areas/volumes: how are they related?",,"Three results from three mathematicians from three different eras provide interesting and surprisingly easy instruments to compute areas and volumes of plane figures or solids. The first two - Cavalieri's Principle (1635) and Mamikon's Theorem (1959) - directly supply stunningly calculus-free methods for calculating ""integrals"" (possibly by hiding calculus under the carpet). At first glance, they seem incredibly similar in fashion; yet, I don't believe that they are quickly (and calculus-free ly ) deducible one from the other. The third one is a not-so-well-known proposition by Italian mathematician Giuseppe Peano, who discussed it in his ""Applicazioni geometriche del calcolo infinitesimale"" (1887). It is much more general and it implies both Cavalieri's result for areas and Mamikon's Theorem as corollaries. Unluckily, it's calcululus-heavy both in its statement and its proof. I will summarise the three statements (for the sake of simplicity, just very stripped-down 2D-versions) in a short while. First of all, though, I want to make my question explicit: is there a way to formulate a proposition which is general enough to include both Cavalieri's Principle and Mamikon's Theorem, and at the same time ""narrow"" enough not to require calculus at least in its statement , as Peano's result does instead? Cavalieri's Principle ( Wikipedia ): Suppose two regions in a plane are included between two parallel lines in that plane. If every line parallel to these two lines intersects both regions in line segments of equal length, then the two regions have equal areas. Mamikon's Theorem ( Wikipedia: Visual Calculus ): The area of a tangent sweep to a curve is equal to the area of its tangent cluster, regardless of the shape of the original curve.   ( Tangent sweep : the surface ""swept"" by a family of segments tangent to a curve when one of their endpoints P continuously moves along the curve; tangent cluster : the surface obtained by translating the aforementioned segments so that P is no longer moving - see figure) Peano's result ( Archive ): Let $A(t), B(t)$ be plane $C^1$ curves, parametrised by $t$, such that the segment $AB(t)$ never passes through the same point as $t$ ranges from $t_0$ to $t_1$. The area swept by the moving segment $AB(t)$ is given by: $\int_{t_0}^{t_1}(B(t)-A(t))\cdot(\frac{d}{dt} A(t)+\frac{d}{dt} B(t))dt$. Special cases of Peano's result give Cavalieri's and Mamikon's ones, as discussed here by Gabriele Greco et al.: Particular instances of the formula considered by Peano are the following: a) The point A moves along a straight line and the angle of the segment AB   with that line is constant; b) The point A is fixed; c) The segment AB is tangent at the point A to the curve described by A; d) The segment AB is of constant length and normal to the curve described by   its midpoint. Case a) brings Cavalieri's Principle; case c) (perhaps in combination with b)) gives Mamikon's Theorem. So, question is: is it possible to ""merge"" Cavalieri and Mamikon (+ possibly other corollaries of Peano) without any direct reference to the integral? Something more in the style of Cavalieri and Mamikon, such as: ""if line segments constructed in {some general way encompassing both Cavalieri's and Mamikon constructions} in both figures are the same, then the two figures are area-equivalent""? I don't actually care about avoiding integrals in the proof : it's satisfying enough for me if the theorem statement is integral-free. And what about volumes?","Three results from three mathematicians from three different eras provide interesting and surprisingly easy instruments to compute areas and volumes of plane figures or solids. The first two - Cavalieri's Principle (1635) and Mamikon's Theorem (1959) - directly supply stunningly calculus-free methods for calculating ""integrals"" (possibly by hiding calculus under the carpet). At first glance, they seem incredibly similar in fashion; yet, I don't believe that they are quickly (and calculus-free ly ) deducible one from the other. The third one is a not-so-well-known proposition by Italian mathematician Giuseppe Peano, who discussed it in his ""Applicazioni geometriche del calcolo infinitesimale"" (1887). It is much more general and it implies both Cavalieri's result for areas and Mamikon's Theorem as corollaries. Unluckily, it's calcululus-heavy both in its statement and its proof. I will summarise the three statements (for the sake of simplicity, just very stripped-down 2D-versions) in a short while. First of all, though, I want to make my question explicit: is there a way to formulate a proposition which is general enough to include both Cavalieri's Principle and Mamikon's Theorem, and at the same time ""narrow"" enough not to require calculus at least in its statement , as Peano's result does instead? Cavalieri's Principle ( Wikipedia ): Suppose two regions in a plane are included between two parallel lines in that plane. If every line parallel to these two lines intersects both regions in line segments of equal length, then the two regions have equal areas. Mamikon's Theorem ( Wikipedia: Visual Calculus ): The area of a tangent sweep to a curve is equal to the area of its tangent cluster, regardless of the shape of the original curve.   ( Tangent sweep : the surface ""swept"" by a family of segments tangent to a curve when one of their endpoints P continuously moves along the curve; tangent cluster : the surface obtained by translating the aforementioned segments so that P is no longer moving - see figure) Peano's result ( Archive ): Let $A(t), B(t)$ be plane $C^1$ curves, parametrised by $t$, such that the segment $AB(t)$ never passes through the same point as $t$ ranges from $t_0$ to $t_1$. The area swept by the moving segment $AB(t)$ is given by: $\int_{t_0}^{t_1}(B(t)-A(t))\cdot(\frac{d}{dt} A(t)+\frac{d}{dt} B(t))dt$. Special cases of Peano's result give Cavalieri's and Mamikon's ones, as discussed here by Gabriele Greco et al.: Particular instances of the formula considered by Peano are the following: a) The point A moves along a straight line and the angle of the segment AB   with that line is constant; b) The point A is fixed; c) The segment AB is tangent at the point A to the curve described by A; d) The segment AB is of constant length and normal to the curve described by   its midpoint. Case a) brings Cavalieri's Principle; case c) (perhaps in combination with b)) gives Mamikon's Theorem. So, question is: is it possible to ""merge"" Cavalieri and Mamikon (+ possibly other corollaries of Peano) without any direct reference to the integral? Something more in the style of Cavalieri and Mamikon, such as: ""if line segments constructed in {some general way encompassing both Cavalieri's and Mamikon constructions} in both figures are the same, then the two figures are area-equivalent""? I don't actually care about avoiding integrals in the proof : it's satisfying enough for me if the theorem statement is integral-free. And what about volumes?",,"['integration', 'geometry', 'math-history', 'area', 'tangent-line']"
41,Integral $\int_0^2 \frac{\arctan x}{x^2+2x+2}dx$,Integral,\int_0^2 \frac{\arctan x}{x^2+2x+2}dx,I am tring to evaluate $$I=\int_0^2 \frac{\arctan x}{x^2+2x+2}dx$$ The first thing I did was to notice that $$\frac{1}{x^2+2x+2}=\frac{1}{(x+1)^2+1}=\frac{d}{dx}\arctan(x+1)$$ So I integrated by parts in order to get $$I=\arctan 2\arctan 3-\int_0^2\frac{\arctan(x+1)}{1+x^2}dx$$ I let $x=u+1$ but when I do that I get $$I=\arctan 2\arctan 3+\int_{-1}^1\frac{\arctan(u)}{1+(1+u)^2}du    =\arctan 2\arctan 3$$ Now this is not close to the approximation given by wolfram. What have I done wrong and how to solve this?,I am tring to evaluate The first thing I did was to notice that So I integrated by parts in order to get I let but when I do that I get Now this is not close to the approximation given by wolfram. What have I done wrong and how to solve this?,"I=\int_0^2 \frac{\arctan x}{x^2+2x+2}dx \frac{1}{x^2+2x+2}=\frac{1}{(x+1)^2+1}=\frac{d}{dx}\arctan(x+1) I=\arctan 2\arctan 3-\int_0^2\frac{\arctan(x+1)}{1+x^2}dx x=u+1 I=\arctan 2\arctan 3+\int_{-1}^1\frac{\arctan(u)}{1+(1+u)^2}du
   =\arctan 2\arctan 3",['integration']
42,Almost-identity: $[\int_0^\infty{\rm d}x-\sum_{x=1}^\infty] \prod_{k=0}^N\text{sinc}\left(\frac{x}{2k+1}\right) = \frac{1}{2}$,Almost-identity:,[\int_0^\infty{\rm d}x-\sum_{x=1}^\infty] \prod_{k=0}^N\text{sinc}\left(\frac{x}{2k+1}\right) = \frac{1}{2},"Show that the identity   $$\int_0^\infty \prod_{k=0}^N \text{sinc}\left(\frac{x}{2k+1}\right)\,{\rm d}x - \sum_{n=1}^\infty \prod_{k=0}^N \text{sinc}\left(\frac{n}{2k+1}\right) = \frac{1}{2}$$ where $\text{sinc}(x) = \frac{\sin(x)}{x}$ holds for $N=0,1,2,\ldots,40000$ but fails for all larger $N$. I remember seeing this strange identity a few years ago and it stuck to my mind, but unfortunately I can't find the source of this right now and it's reconstructed from memory (and checked with a computer for small $N$ although $40000$ might not be accurate). This is why I'm asking it here. If I remember correctly it's closely linked to Fourier transforms and for $N$ larger than $\sim 40000$ the difference between the left and right hand side should be smaller than $\sim 10^{-10000}$ so the agreement is extremely good. Do anyone know the source of this problem or otherwise how to solve it? What is the theory behind it (i.e. why does it break down at some finite value)?","Show that the identity   $$\int_0^\infty \prod_{k=0}^N \text{sinc}\left(\frac{x}{2k+1}\right)\,{\rm d}x - \sum_{n=1}^\infty \prod_{k=0}^N \text{sinc}\left(\frac{n}{2k+1}\right) = \frac{1}{2}$$ where $\text{sinc}(x) = \frac{\sin(x)}{x}$ holds for $N=0,1,2,\ldots,40000$ but fails for all larger $N$. I remember seeing this strange identity a few years ago and it stuck to my mind, but unfortunately I can't find the source of this right now and it's reconstructed from memory (and checked with a computer for small $N$ although $40000$ might not be accurate). This is why I'm asking it here. If I remember correctly it's closely linked to Fourier transforms and for $N$ larger than $\sim 40000$ the difference between the left and right hand side should be smaller than $\sim 10^{-10000}$ so the agreement is extremely good. Do anyone know the source of this problem or otherwise how to solve it? What is the theory behind it (i.e. why does it break down at some finite value)?",,"['integration', 'sequences-and-series', 'fourier-analysis']"
43,Addition is to Integration as Multiplication is to ______,Addition is to Integration as Multiplication is to ______,,"Addition is to Integration as Multiplication is to ______ ? Everyone knows that definite integration  is ""a way to sum continuum-many terms"" in a rough sense. Can we ""multiply continuum-many factors""  in a similar sense?","Addition is to Integration as Multiplication is to ______ ? Everyone knows that definite integration  is ""a way to sum continuum-many terms"" in a rough sense. Can we ""multiply continuum-many factors""  in a similar sense?",,"['calculus', 'analysis']"
44,Solve $\int_0^1 \int_0^{2\pi}\frac{ax-x^2\sin(\theta)}{\sqrt{a^2-2ax\sin(\theta)+x^2}}d\theta dx$,Solve,\int_0^1 \int_0^{2\pi}\frac{ax-x^2\sin(\theta)}{\sqrt{a^2-2ax\sin(\theta)+x^2}}d\theta dx,"Solve $$\int_0^1 \int_0^{2\pi}\frac{ax-x^2\sin(\theta)}{\sqrt{a^2-2ax\sin(\theta)+x^2}}d\theta dx$$ This integral is from the following paper : Frictional coupling between sliding and spinning motion and can be understood as an integration of unit vector field parallel to the velocity of a spinning disk which is sliding with the speed of $a$ over a unit disk. The evaluation of the integral is claimed to be $$\frac{4}{3}\frac{(a^2+1)E(a)+(a^2-1)K(a)}{a}$$ for $a<1$, and $$\frac{4}{3}((a^2+1)E(1/a)+(a^2-1)K(1/a))$$ for $a\geq1$ in the paper, where $K$ and $E$ are the first and second kind of complete elliptic integrals respectively. But for me this integration is not clear since if I do the integration for $x$ first then it results in a term with natural log which is irrelevant to elliptic integral, and doing it for $\theta$ first results in the terms including $E(\frac{4ax}{(a-x)^2})$ and $K(\frac{4ax}{(a-x)^2})$ which seems hard to be reduced to $E(a)$ and $K(a)$. What approach would give the most clarified evaluation? Any advice or help will be appreciated.","Solve $$\int_0^1 \int_0^{2\pi}\frac{ax-x^2\sin(\theta)}{\sqrt{a^2-2ax\sin(\theta)+x^2}}d\theta dx$$ This integral is from the following paper : Frictional coupling between sliding and spinning motion and can be understood as an integration of unit vector field parallel to the velocity of a spinning disk which is sliding with the speed of $a$ over a unit disk. The evaluation of the integral is claimed to be $$\frac{4}{3}\frac{(a^2+1)E(a)+(a^2-1)K(a)}{a}$$ for $a<1$, and $$\frac{4}{3}((a^2+1)E(1/a)+(a^2-1)K(1/a))$$ for $a\geq1$ in the paper, where $K$ and $E$ are the first and second kind of complete elliptic integrals respectively. But for me this integration is not clear since if I do the integration for $x$ first then it results in a term with natural log which is irrelevant to elliptic integral, and doing it for $\theta$ first results in the terms including $E(\frac{4ax}{(a-x)^2})$ and $K(\frac{4ax}{(a-x)^2})$ which seems hard to be reduced to $E(a)$ and $K(a)$. What approach would give the most clarified evaluation? Any advice or help will be appreciated.",,"['integration', 'definite-integrals']"
45,Riesz potential of a set and its complement,Riesz potential of a set and its complement,,"Let $F\subset [0,1]$ be a closed set, $G = [0,1]\setminus F$, $\alpha \in(1,2)$. Is there a simple condition on $F$ under which the integral $$ \int_F\int_G \frac{dx\,dy}{|x-y|^{\alpha}} $$ is finite? I suspect that if the fractal dimension of $\partial F$ is small, then this is the case.","Let $F\subset [0,1]$ be a closed set, $G = [0,1]\setminus F$, $\alpha \in(1,2)$. Is there a simple condition on $F$ under which the integral $$ \int_F\int_G \frac{dx\,dy}{|x-y|^{\alpha}} $$ is finite? I suspect that if the fractal dimension of $\partial F$ is small, then this is the case.",,"['integration', 'fractional-calculus', 'potential-theory']"
46,How do I symbolically compute $\int_{a}^{b} e^{x^2}(\textrm{erf}(x) - \textrm{erf(a)})\;\textrm{d}x$?,How do I symbolically compute ?,\int_{a}^{b} e^{x^2}(\textrm{erf}(x) - \textrm{erf(a)})\;\textrm{d}x,"I want to symbolically write (in the form of a series), the integral of: $$ \int_{a}^{b} e^{x^2}(\textrm{erf}(x) - \textrm{erf(a)})\;\textrm{d}x, \text{where }\{x, a, b\} \subset \mathbb{R} $$ The $\textrm{erf}$ function is: $$ \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}\;\textrm{d}t$$ The first idea I had is to write the integrand in terms of its power series. However, the product of two infinite series is very messy, and unlikely to be easy to write term-by-term, and thus we can't integrate some nice series term-by-term. Let us say that I could write the integrand prettily, then there is still the issue of which point I should write the Taylor series expansion of the integrand about, given that it needs to be an accurate representation of the function about all points in the range of integration. Perhaps the infinite number of terms suffices here? Well, that about exhausts my meagre symbolic integration capacities. Note that I do not want to compute this numerically. Wolfram Alpha runs out of computation time when given the input: integrate exp(x^2)*(erf(x) - erf(a)) dx from a to b However, when Wolfram Alpha is given the indefinite calculation: integrate exp(x^2)*(erf(x) - erf(a)) dx , it provides us with the result that uses hypergeometric functions, and the imaginary $\textrm{erf}$ function. It is easy to use the indefinite integral to compute the definite integral (Fundamental Theorem of Calculus), but I'd like to ask: if I didn't have Wolfram Alpha, and only pen and paper, how would I go about symbolically computing the indefinite integral ?","I want to symbolically write (in the form of a series), the integral of: $$ \int_{a}^{b} e^{x^2}(\textrm{erf}(x) - \textrm{erf(a)})\;\textrm{d}x, \text{where }\{x, a, b\} \subset \mathbb{R} $$ The $\textrm{erf}$ function is: $$ \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}\;\textrm{d}t$$ The first idea I had is to write the integrand in terms of its power series. However, the product of two infinite series is very messy, and unlikely to be easy to write term-by-term, and thus we can't integrate some nice series term-by-term. Let us say that I could write the integrand prettily, then there is still the issue of which point I should write the Taylor series expansion of the integrand about, given that it needs to be an accurate representation of the function about all points in the range of integration. Perhaps the infinite number of terms suffices here? Well, that about exhausts my meagre symbolic integration capacities. Note that I do not want to compute this numerically. Wolfram Alpha runs out of computation time when given the input: integrate exp(x^2)*(erf(x) - erf(a)) dx from a to b However, when Wolfram Alpha is given the indefinite calculation: integrate exp(x^2)*(erf(x) - erf(a)) dx , it provides us with the result that uses hypergeometric functions, and the imaginary $\textrm{erf}$ function. It is easy to use the indefinite integral to compute the definite integral (Fundamental Theorem of Calculus), but I'd like to ask: if I didn't have Wolfram Alpha, and only pen and paper, how would I go about symbolically computing the indefinite integral ?",,"['integration', 'hypergeometric-function', 'error-function', 'symbolic-computation', 'gaussian-integral']"
47,Understanding Stieltjes-Riemann,Understanding Stieltjes-Riemann,,"From my understanding from lectures, the Stieltjes-Riemann integral is a generalization of the Riemann integral. When using the identity function as integrator, the Riemann sum and Stieltjes-Riemann sum are identical. For any other (monotonically increasing) weighting function, the Riemann sum and Stieltjes-Riemann sum may converge to different values for a given interval. How would I interpret the result of $\int_a^b{f \text{d}\alpha}$ as it does not seem to relate to the area of the graph of $f$ anymore? How would I choose a weighting function $\alpha$ for meaningful results?","From my understanding from lectures, the Stieltjes-Riemann integral is a generalization of the Riemann integral. When using the identity function as integrator, the Riemann sum and Stieltjes-Riemann sum are identical. For any other (monotonically increasing) weighting function, the Riemann sum and Stieltjes-Riemann sum may converge to different values for a given interval. How would I interpret the result of $\int_a^b{f \text{d}\alpha}$ as it does not seem to relate to the area of the graph of $f$ anymore? How would I choose a weighting function $\alpha$ for meaningful results?",,"['integration', 'riemann-sum']"
48,Integration by parts on manifold with a boundary,Integration by parts on manifold with a boundary,,"Suppose $C$ is a 3-form, and $G$ is a 4-form defined by $G = dC$. Also, $M_{11}$ is an 11-dimensional manifold (without a boundary), $W_{6}$ is a 6-dimensional submanifold of $M_{11}$ and $D_{\epsilon}W_6 = -S_{\epsilon}W_{6}$ is the 4-sphere bundle over $W_6$. Further, suppose $\rho$ is a 0-form and $e_{2}^{1}$ is a 2-form. Under a variation, $$\delta C = -d(\rho e_{2}^{1})$$ I want to compute the variation $\delta S_{CS}$ in the Chern-Simons integral $$S_{CS} = -\lim_{\epsilon\rightarrow 0}\int\limits_{M_{11}\backslash D_{\epsilon}W_6} C \wedge G \wedge G$$ Apparently, the correct answer is $$\delta S_{CS} = -\lim_{\epsilon \rightarrow}\int\limits_{S_{\epsilon}W_6}\rho e_{2}^{1}\wedge G \wedge G$$ But what I get is something else. Here is my detailed derivation. $$\delta S_{CS} = -\lim_{\epsilon\rightarrow 0}\left[\int\limits_{M_{11}\backslash D_\epsilon W_6}\delta C \wedge G \wedge G + 2 \int\limits_{M_{11}\backslash D_\epsilon W_6}\delta G \wedge C \wedge G\right]$$ From integration by parts $$\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)}\delta C \wedge C \wedge G = \int\limits_{M_{11}\backslash D_\epsilon W_6}d(\delta C \wedge C \wedge G) = \int\limits_{M_{11}\backslash D_\epsilon W_6} \delta dC \wedge C \wedge G - \int\limits_{M_{11}\backslash D_\epsilon W_6} \delta C \wedge G \wedge G$$ So it should follow that $$\delta S_{CS} = -\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)}\delta C \wedge C \wedge G + 3\int\limits_{M_{11}\backslash D_\epsilon W_6}\delta C \wedge G \wedge G\right]$$ As $M_{11}\backslash D_{\epsilon}W_6$, for finite $\epsilon$ cannot support an 11-form, the second integral vanishes inside the limit, and we are left with $$\delta S_{CS} = -\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)}\delta C \wedge C \wedge G\right]$$ Substituting $\delta C = -d(\rho e_2^1)$ we get $$\delta S_{CS} = +\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)} d(\rho e_2^1) \wedge C \wedge G\right]$$ Now, $$d(\rho e_2^1 \wedge (C\wedge G)) = d(\rho e_2^1) \wedge C\wedge G + \rho e_2^1 \wedge d(C\wedge G)$$ and $\partial(\partial(M_{11}\backslash D_\epsilon W_6)) \equiv 0$, so $$\delta S_{CS} = -\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)}\rho e_2^1 \wedge G \wedge G\right]$$ As a last step, using $\partial(M_{11}\backslash D_\epsilon W_6) = -S_{\epsilon} W_6$, one gets the final expression $$\delta S_{CS} = +\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{S_\epsilon W_6}\rho e_2^1 \wedge G \wedge G\right]$$ This is off by a sign and a factor of 2. What seems to be wrong in the derivation here?","Suppose $C$ is a 3-form, and $G$ is a 4-form defined by $G = dC$. Also, $M_{11}$ is an 11-dimensional manifold (without a boundary), $W_{6}$ is a 6-dimensional submanifold of $M_{11}$ and $D_{\epsilon}W_6 = -S_{\epsilon}W_{6}$ is the 4-sphere bundle over $W_6$. Further, suppose $\rho$ is a 0-form and $e_{2}^{1}$ is a 2-form. Under a variation, $$\delta C = -d(\rho e_{2}^{1})$$ I want to compute the variation $\delta S_{CS}$ in the Chern-Simons integral $$S_{CS} = -\lim_{\epsilon\rightarrow 0}\int\limits_{M_{11}\backslash D_{\epsilon}W_6} C \wedge G \wedge G$$ Apparently, the correct answer is $$\delta S_{CS} = -\lim_{\epsilon \rightarrow}\int\limits_{S_{\epsilon}W_6}\rho e_{2}^{1}\wedge G \wedge G$$ But what I get is something else. Here is my detailed derivation. $$\delta S_{CS} = -\lim_{\epsilon\rightarrow 0}\left[\int\limits_{M_{11}\backslash D_\epsilon W_6}\delta C \wedge G \wedge G + 2 \int\limits_{M_{11}\backslash D_\epsilon W_6}\delta G \wedge C \wedge G\right]$$ From integration by parts $$\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)}\delta C \wedge C \wedge G = \int\limits_{M_{11}\backslash D_\epsilon W_6}d(\delta C \wedge C \wedge G) = \int\limits_{M_{11}\backslash D_\epsilon W_6} \delta dC \wedge C \wedge G - \int\limits_{M_{11}\backslash D_\epsilon W_6} \delta C \wedge G \wedge G$$ So it should follow that $$\delta S_{CS} = -\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)}\delta C \wedge C \wedge G + 3\int\limits_{M_{11}\backslash D_\epsilon W_6}\delta C \wedge G \wedge G\right]$$ As $M_{11}\backslash D_{\epsilon}W_6$, for finite $\epsilon$ cannot support an 11-form, the second integral vanishes inside the limit, and we are left with $$\delta S_{CS} = -\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)}\delta C \wedge C \wedge G\right]$$ Substituting $\delta C = -d(\rho e_2^1)$ we get $$\delta S_{CS} = +\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)} d(\rho e_2^1) \wedge C \wedge G\right]$$ Now, $$d(\rho e_2^1 \wedge (C\wedge G)) = d(\rho e_2^1) \wedge C\wedge G + \rho e_2^1 \wedge d(C\wedge G)$$ and $\partial(\partial(M_{11}\backslash D_\epsilon W_6)) \equiv 0$, so $$\delta S_{CS} = -\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{\partial(M_{11}\backslash D_\epsilon W_6)}\rho e_2^1 \wedge G \wedge G\right]$$ As a last step, using $\partial(M_{11}\backslash D_\epsilon W_6) = -S_{\epsilon} W_6$, one gets the final expression $$\delta S_{CS} = +\lim_{\epsilon\rightarrow 0}\left[2\int\limits_{S_\epsilon W_6}\rho e_2^1 \wedge G \wedge G\right]$$ This is off by a sign and a factor of 2. What seems to be wrong in the derivation here?",,"['integration', 'differential-geometry', 'manifolds', 'mathematical-physics', 'manifolds-with-boundary']"
49,Help solving integration: $I=\int_{-\infty}^{\infty}\phi\left(x\right)\Phi\left(a/\sqrt{b+c\mathrm{e}^{\frac{x-\mu}{\sigma}}}\right)dx$,Help solving integration:,I=\int_{-\infty}^{\infty}\phi\left(x\right)\Phi\left(a/\sqrt{b+c\mathrm{e}^{\frac{x-\mu}{\sigma}}}\right)dx,"My work has arrived at needing to solve the integral below for $a,b,c,\sigma>0$ $$I=\int_{-\infty}^{\infty}\phi\left(x\right)\Phi\left(\frac{a}{\sqrt{b+c\mathrm{e}^{(x-\mu)/\sigma}}}\right)dx$$ I have tried substitution: $u=\frac{a}{\sqrt{b+c\mathrm{e}^{\frac{x-\mu}{\sigma}}}}$ and then a couple of rounds of integration by parts. However it does not seem to be getting closer to finding an integration that can be done directly (i.e. without needing a further by parts integration). Is there another route to solving this, or does it not have a closed-form solution? Thanks","My work has arrived at needing to solve the integral below for $a,b,c,\sigma>0$ $$I=\int_{-\infty}^{\infty}\phi\left(x\right)\Phi\left(\frac{a}{\sqrt{b+c\mathrm{e}^{(x-\mu)/\sigma}}}\right)dx$$ I have tried substitution: $u=\frac{a}{\sqrt{b+c\mathrm{e}^{\frac{x-\mu}{\sigma}}}}$ and then a couple of rounds of integration by parts. However it does not seem to be getting closer to finding an integration that can be done directly (i.e. without needing a further by parts integration). Is there another route to solving this, or does it not have a closed-form solution? Thanks",,"['integration', 'definite-integrals', 'normal-distribution', 'gaussian-integral']"
50,"Evaluate $\int_0^2 \sqrt[3]{x^2 + 2x - 1} \, dx$",Evaluate,"\int_0^2 \sqrt[3]{x^2 + 2x - 1} \, dx","Calculate the value of the integral   $$ \int_0^2 \sqrt[3]{x^2 + 2x - 1} \,dx $$   with measurement uncertainty not larger than $10^{-3}$. I know we can evaluate integration using the ""trapezoidal rule"" or ""Simpson's rule"". But if we want to calculate the uncertainty, using the first rule, we have to calculate $\max_{x \in [0, 2]} |f''(x)|$, which does not exist ($|f''(x)| = {2 \over 9}|{x^2 + 2x + 7 \over (x^2 + 2x - 1)^{5/3}}| $, which has limit $+\infty$ when $x \rightarrow \sqrt{2} - 1$). Using the second rule, we need to calculate $\max_{x \in [0, 2]} |f''''(x)|$, which doesn't exist either. So, how can we evaluate this integral? Can someone give me a suggestion? Thanks.","Calculate the value of the integral   $$ \int_0^2 \sqrt[3]{x^2 + 2x - 1} \,dx $$   with measurement uncertainty not larger than $10^{-3}$. I know we can evaluate integration using the ""trapezoidal rule"" or ""Simpson's rule"". But if we want to calculate the uncertainty, using the first rule, we have to calculate $\max_{x \in [0, 2]} |f''(x)|$, which does not exist ($|f''(x)| = {2 \over 9}|{x^2 + 2x + 7 \over (x^2 + 2x - 1)^{5/3}}| $, which has limit $+\infty$ when $x \rightarrow \sqrt{2} - 1$). Using the second rule, we need to calculate $\max_{x \in [0, 2]} |f''''(x)|$, which doesn't exist either. So, how can we evaluate this integral? Can someone give me a suggestion? Thanks.",,"['integration', 'numerical-methods']"
51,Implementing the Risch algorithm to integrate $\dfrac{\log(x)+2}{x^{2}\log^{3}(x)}$,Implementing the Risch algorithm to integrate,\dfrac{\log(x)+2}{x^{2}\log^{3}(x)},"Following the work of Andreas Wurfl i am trying to implement the Risch algorithm on $\int{\dfrac{\log(x)+2}{x^{2}\log^{3}(x)}dx}$ following his method for extensions that are purely logarithmic, we let $\theta=\log(x)$ then the integrand becomes $\dfrac{\theta+2}{x^{2}\theta^{3}} \in \mathbb{Q}(x,\theta)$, let $p$ be the numerator and $q$ be the denominator of our integrant, first to show that this has an elementary integral we need to show that the roots (solutions for $z$) of the resultant (determinant of the sylvester matrix) of the polynomials $p-zq'$ and $q$ after some calculation this boils down to finding the resultant of $\theta+2-2xz\theta^{3}-3xz\theta^{2}$ with $x^{2}\theta^{3}$  i.e. the determinant of the matrix $ S =\begin{pmatrix} -2xz & -3xz& 1 &2 &0& 0  \\ 0&-2xz&-3xz&1&2&0 \\ 0&0&-2xz&-3xz&1&2 \\ x^{2}&0&0&0&0&0 \\ 0&x^{2}&0&0&0&0 \\ 0&0&x^{2}&0&0&0 \end{pmatrix}$ (i think although my resultant knowledge is a bit rusty so don't quote me on this) Which i get as 0 which causes problems as the next bit involves writing it in the form where we need to multiply a quotient by the roots of our resultant (roots for z) but this will just be 0 so we get a problem (see theorem 4 of the article.) Any help would be apprecaited","Following the work of Andreas Wurfl i am trying to implement the Risch algorithm on $\int{\dfrac{\log(x)+2}{x^{2}\log^{3}(x)}dx}$ following his method for extensions that are purely logarithmic, we let $\theta=\log(x)$ then the integrand becomes $\dfrac{\theta+2}{x^{2}\theta^{3}} \in \mathbb{Q}(x,\theta)$, let $p$ be the numerator and $q$ be the denominator of our integrant, first to show that this has an elementary integral we need to show that the roots (solutions for $z$) of the resultant (determinant of the sylvester matrix) of the polynomials $p-zq'$ and $q$ after some calculation this boils down to finding the resultant of $\theta+2-2xz\theta^{3}-3xz\theta^{2}$ with $x^{2}\theta^{3}$  i.e. the determinant of the matrix $ S =\begin{pmatrix} -2xz & -3xz& 1 &2 &0& 0  \\ 0&-2xz&-3xz&1&2&0 \\ 0&0&-2xz&-3xz&1&2 \\ x^{2}&0&0&0&0&0 \\ 0&x^{2}&0&0&0&0 \\ 0&0&x^{2}&0&0&0 \end{pmatrix}$ (i think although my resultant knowledge is a bit rusty so don't quote me on this) Which i get as 0 which causes problems as the next bit involves writing it in the form where we need to multiply a quotient by the roots of our resultant (roots for z) but this will just be 0 so we get a problem (see theorem 4 of the article.) Any help would be apprecaited",,"['integration', 'algorithms', 'elementary-functions']"
52,Difference between Riemann-Stieltjes and Darboux-Stieltjes integral,Difference between Riemann-Stieltjes and Darboux-Stieltjes integral,,"Usually, analysis textbooks do not classify Riemann-Stieltjes and Darboux-Stieltjes integral. But I know that a function is $\alpha$-DS integrable does not implies it is $\alpha$-RS integrable. Can anybody give me an example of that? Definition of $\alpha$-DS integrable: Let $P$ be a partition of $[a,b]$. $M_i=\sup(f(x):x\in[x_i,x_{i-1}])$, $m_i=\inf(f(x):x\in[x_i,x_{i-1}])$ $\Delta\alpha=\alpha(x_i)-\alpha(x_{i-1})$ $U(P,f,\alpha)=\sum M_i\Delta \alpha_i$, $L(P,f,\alpha)=\sum m_i\Delta \alpha_i$ A function $f:[a,b]\to R$ is $\alpha$-DS integrable iff $\sup(P,f,\alpha)=\inf(P,f,\alpha)$ where the sup and inf are taken over all partition of $[a,b]$. Definition of $\alpha$-RS integrable: A function $f:[a,b]\to R$ is $\alpha$-RS integrable iff for any $\epsilon>0$, there exists $\delta>0$ such that for any partition $P$ with $mesh(P)<\delta$, $$\left|\sum f(\xi_i)\Delta \alpha-\int^b_afd\alpha\right|<\epsilon,\quad \forall\xi_i\in[x_i,x_i-1]$$","Usually, analysis textbooks do not classify Riemann-Stieltjes and Darboux-Stieltjes integral. But I know that a function is $\alpha$-DS integrable does not implies it is $\alpha$-RS integrable. Can anybody give me an example of that? Definition of $\alpha$-DS integrable: Let $P$ be a partition of $[a,b]$. $M_i=\sup(f(x):x\in[x_i,x_{i-1}])$, $m_i=\inf(f(x):x\in[x_i,x_{i-1}])$ $\Delta\alpha=\alpha(x_i)-\alpha(x_{i-1})$ $U(P,f,\alpha)=\sum M_i\Delta \alpha_i$, $L(P,f,\alpha)=\sum m_i\Delta \alpha_i$ A function $f:[a,b]\to R$ is $\alpha$-DS integrable iff $\sup(P,f,\alpha)=\inf(P,f,\alpha)$ where the sup and inf are taken over all partition of $[a,b]$. Definition of $\alpha$-RS integrable: A function $f:[a,b]\to R$ is $\alpha$-RS integrable iff for any $\epsilon>0$, there exists $\delta>0$ such that for any partition $P$ with $mesh(P)<\delta$, $$\left|\sum f(\xi_i)\Delta \alpha-\int^b_afd\alpha\right|<\epsilon,\quad \forall\xi_i\in[x_i,x_i-1]$$",,"['integration', 'analysis']"
53,Banach Spaces: Uniform Integral vs. Riemann Integral,Banach Spaces: Uniform Integral vs. Riemann Integral,,"Problem Given a finite measure space $\Omega$ and a Banach space $E$. One has strict inclusion:   $$\mathcal{L}_\mathfrak{U}(\mu)\subsetneq\mathcal{L}_\mathfrak{R}(\mu):\quad\int_\mathfrak{U}F\mathrm{d}\mu=\int_\mathfrak{R}F\mathrm{d}\mu$$   How to prove this from scratch? Uniform Integral Predefine the simple integral: $$S=\sum_kb_k\chi(A_k):\quad\int_\mathfrak{S}S\mathrm{d}\mu:=\sum_k b_k\mu(A_k)$$ It is uniformly bounded: $$\|\int_\mathfrak{S}S\mathrm{d}\mu\|\leq\|S\|_\infty\mu(\Omega)$$ So define the uniform integral by: $$F=\lim_nS_n:\quad\int_\mathfrak{U}F\mathrm{d}\mu:=\lim_n\int_\mathfrak{S}S_n\mathrm{d}\mu$$ (More precisely, by the a.e. uniform closure!) Riemann Integral Define the Riemann integral by: $$\int_\mathfrak{R}F\mathrm{d}\mu:=\lim_\mathcal{P}\{\sum_{a\in A\in\mathcal{P}}F(a)\mu(A)\}_\mathcal{P}$$ Finite measurable partitions: $$\mathcal{P}\subseteq\Sigma:\quad\Omega=\bigsqcup_{A\in\mathcal{P}}A\quad(\#\mathcal{P}<\infty)$$ Order them by refinement: $$\mathcal{P}\leq\mathcal{P}':\iff\forall A'\in\mathcal{P}'\exists A\in\mathcal{P}:\quad A\supseteq A'$$ (That is the usual ordering.)","Problem Given a finite measure space $\Omega$ and a Banach space $E$. One has strict inclusion:   $$\mathcal{L}_\mathfrak{U}(\mu)\subsetneq\mathcal{L}_\mathfrak{R}(\mu):\quad\int_\mathfrak{U}F\mathrm{d}\mu=\int_\mathfrak{R}F\mathrm{d}\mu$$   How to prove this from scratch? Uniform Integral Predefine the simple integral: $$S=\sum_kb_k\chi(A_k):\quad\int_\mathfrak{S}S\mathrm{d}\mu:=\sum_k b_k\mu(A_k)$$ It is uniformly bounded: $$\|\int_\mathfrak{S}S\mathrm{d}\mu\|\leq\|S\|_\infty\mu(\Omega)$$ So define the uniform integral by: $$F=\lim_nS_n:\quad\int_\mathfrak{U}F\mathrm{d}\mu:=\lim_n\int_\mathfrak{S}S_n\mathrm{d}\mu$$ (More precisely, by the a.e. uniform closure!) Riemann Integral Define the Riemann integral by: $$\int_\mathfrak{R}F\mathrm{d}\mu:=\lim_\mathcal{P}\{\sum_{a\in A\in\mathcal{P}}F(a)\mu(A)\}_\mathcal{P}$$ Finite measurable partitions: $$\mathcal{P}\subseteq\Sigma:\quad\Omega=\bigsqcup_{A\in\mathcal{P}}A\quad(\#\mathcal{P}<\infty)$$ Order them by refinement: $$\mathcal{P}\leq\mathcal{P}':\iff\forall A'\in\mathcal{P}'\exists A\in\mathcal{P}:\quad A\supseteq A'$$ (That is the usual ordering.)",,"['integration', 'functional-analysis', 'banach-spaces']"
54,Can you prove a definite integral has no closed form?,Can you prove a definite integral has no closed form?,,"It is a well known fact that some functions posses no closed form antiderivative yet still they have definite integrals that have a closed form. A classic example is the Gaussian integral $$\int_{-\infty}^\infty e^{-x^2}\mathrm dx=\sqrt\pi$$ Now look at the functions that do not possess anitderivatives in a closed form. We do this because finding the antiderivative is the most trivial way of evaluating a definite integral. Is it possible to prove that a functions definite integral simply doesn't exist in a closed form? I imagine to do such a thing one would need to look at the nature of the integrand with respect to many different techniques for evaluating definite integrals. It looks to me that this is an extremely tedious and difficult task however is it possible? This is similar in a sense to proving that some polynomial equations cannot have a closed form by radicals however radicals and functions considered to be ""closed form"" are radically different. This means that the mechanics of proof may not be very well understood. I am specifically talking about Wikipedia's definition for closed form . And if possible the extra work that would be needed if we want to allow special functions. Beta, Gamma, Erf, PolyGamma etc.","It is a well known fact that some functions posses no closed form antiderivative yet still they have definite integrals that have a closed form. A classic example is the Gaussian integral $$\int_{-\infty}^\infty e^{-x^2}\mathrm dx=\sqrt\pi$$ Now look at the functions that do not possess anitderivatives in a closed form. We do this because finding the antiderivative is the most trivial way of evaluating a definite integral. Is it possible to prove that a functions definite integral simply doesn't exist in a closed form? I imagine to do such a thing one would need to look at the nature of the integrand with respect to many different techniques for evaluating definite integrals. It looks to me that this is an extremely tedious and difficult task however is it possible? This is similar in a sense to proving that some polynomial equations cannot have a closed form by radicals however radicals and functions considered to be ""closed form"" are radically different. This means that the mechanics of proof may not be very well understood. I am specifically talking about Wikipedia's definition for closed form . And if possible the extra work that would be needed if we want to allow special functions. Beta, Gamma, Erf, PolyGamma etc.",,"['integration', 'definite-integrals', 'closed-form']"
55,Integration of bundle-valued differential forms,Integration of bundle-valued differential forms,,"The literature, at least textbooks, seems to be very scarce on the topic of integrating bundle-valued differential forms. So I wonder where can I read on the topic? I want to see usual theorems, like change of variables formula, Stokes' theorem, ideally fiber integration. I understand that such integration requires addition of elements in different fibers, so as I understand some additional structure is required, like connection. I'm mostly interested right now in integrating vector-valued $\Omega^n(M,TM)$ and k-form valued $\Omega^n(M,\Lambda^k T^*M)$ differential forms.","The literature, at least textbooks, seems to be very scarce on the topic of integrating bundle-valued differential forms. So I wonder where can I read on the topic? I want to see usual theorems, like change of variables formula, Stokes' theorem, ideally fiber integration. I understand that such integration requires addition of elements in different fibers, so as I understand some additional structure is required, like connection. I'm mostly interested right now in integrating vector-valued $\Omega^n(M,TM)$ and k-form valued $\Omega^n(M,\Lambda^k T^*M)$ differential forms.",,"['integration', 'reference-request', 'differential-geometry']"
56,"How does the integral $\int_{D_C} e^{ia z}P(z)/Q(z)\,\mathrm{d}z$ blow up.",How does the integral  blow up.,"\int_{D_C} e^{ia z}P(z)/Q(z)\,\mathrm{d}z","In my book I have a theorem that goes something like the following Let $P(x)$ be $Q(x)$ polynomials such that $\deg(Q) \geq \deg(P) + 2$.   Then \begin{align*} 	\int_{-\infty}^{\infty} \frac{P(x)}{Q(x)}  e^{iax} \,\mathrm{d}x 	= 2 \pi i \frac{a}{|a|} \sum_{k=1}^{m} \mathrm{Res}\left[  \frac{P(x)}{Q(x)} , z_k \right] \end{align*} where $a$ is a real   constant and $z_1,\,\ldots\,,z_m$ are the singularities to $P(x)/Q(x)$ in   the upper halfplane if $a>0$ and the lower halfplane if $a<0$. I am having a bit of problems understanding this. I know the reason why we have to switch contour is that $e^{iz}$ blows up. But I really can not see why or how it blows up. Take the canonical example for why this theorem is useful  $$   J = \int_{-\infty}^\infty \frac{e^{iz}}{z^2+1} $$ Now the clue here is to show that the integral along the curve tends to zero, and then use the residue theorem. To show that the integral tends to zero I did this \begin{align*} 	\left| \int_{C_1} \frac{e^{iz}}{1+z^2} \,\mathrm{d}z\right| 	\leq \int_{C_1} \left| \frac{e^{iz}}{1+z^2} \right| \,\mathrm{d}z  	\leq \sup_{z = R e^{i \theta}} \left( \frac{1}{\left|1+z^2\right|} \right) 	     \int_{C_1} |e^{iz}| \,\mathrm{d}z \end{align*} where the $ML$-inequality was used in the last inequality  On the circle with radius $R$ we have $|e^{iz}|=R$, and we can use the inequality $|a+b|\leq|a|-|b|$ to simplify further \begin{align*} 	\left|\int_{D_R} \frac{e^{iz}}{1+z^2}\,\mathrm{d}z \right| 	\leq \sup_{z = R e^{i \theta}} \left( \frac{1}{|z|^2-1}\right) 	\int_{D_R} R \,\mathrm{d}z 	\leq \pi \frac{R}{R^2-1} \end{align*} which tends to zero as $R \to \infty$ as wanted. but if one instead had $e^{-iz}$ then the theorem states that one has to use the lower half plane. But I do not see where my calculations err if one persists in using the contour in the upper half plane? What goes wrong, and why does it go wrong? $|e^{-iz}|$ should still be $R$ on the semi-circle. How can one formally show that the integral diverges when picking the contour as a semi circle in the lower half plane?","In my book I have a theorem that goes something like the following Let $P(x)$ be $Q(x)$ polynomials such that $\deg(Q) \geq \deg(P) + 2$.   Then \begin{align*} 	\int_{-\infty}^{\infty} \frac{P(x)}{Q(x)}  e^{iax} \,\mathrm{d}x 	= 2 \pi i \frac{a}{|a|} \sum_{k=1}^{m} \mathrm{Res}\left[  \frac{P(x)}{Q(x)} , z_k \right] \end{align*} where $a$ is a real   constant and $z_1,\,\ldots\,,z_m$ are the singularities to $P(x)/Q(x)$ in   the upper halfplane if $a>0$ and the lower halfplane if $a<0$. I am having a bit of problems understanding this. I know the reason why we have to switch contour is that $e^{iz}$ blows up. But I really can not see why or how it blows up. Take the canonical example for why this theorem is useful  $$   J = \int_{-\infty}^\infty \frac{e^{iz}}{z^2+1} $$ Now the clue here is to show that the integral along the curve tends to zero, and then use the residue theorem. To show that the integral tends to zero I did this \begin{align*} 	\left| \int_{C_1} \frac{e^{iz}}{1+z^2} \,\mathrm{d}z\right| 	\leq \int_{C_1} \left| \frac{e^{iz}}{1+z^2} \right| \,\mathrm{d}z  	\leq \sup_{z = R e^{i \theta}} \left( \frac{1}{\left|1+z^2\right|} \right) 	     \int_{C_1} |e^{iz}| \,\mathrm{d}z \end{align*} where the $ML$-inequality was used in the last inequality  On the circle with radius $R$ we have $|e^{iz}|=R$, and we can use the inequality $|a+b|\leq|a|-|b|$ to simplify further \begin{align*} 	\left|\int_{D_R} \frac{e^{iz}}{1+z^2}\,\mathrm{d}z \right| 	\leq \sup_{z = R e^{i \theta}} \left( \frac{1}{|z|^2-1}\right) 	\int_{D_R} R \,\mathrm{d}z 	\leq \pi \frac{R}{R^2-1} \end{align*} which tends to zero as $R \to \infty$ as wanted. but if one instead had $e^{-iz}$ then the theorem states that one has to use the lower half plane. But I do not see where my calculations err if one persists in using the contour in the upper half plane? What goes wrong, and why does it go wrong? $|e^{-iz}|$ should still be $R$ on the semi-circle. How can one formally show that the integral diverges when picking the contour as a semi circle in the lower half plane?",,"['integration', 'complex-analysis', 'contour-integration']"
57,Is $\sigma$-finiteness really a necessary condition for this problem?,Is -finiteness really a necessary condition for this problem?,\sigma,"Question: Let $(X, \mathcal A, \mu)$ be a measure space and suppose $\mu$ is $\sigma$-finite. Suppose $f$ is integrable. Prove that given any $\varepsilon$, there exists a $\delta >0$ such that $$\int_A |f| d\mu < \varepsilon$$ whenever $\mu(A) < \delta$. My Solution: We use the fact that if $f \geq 0$ is measurable, then $\lim_{n\to\infty}\int (f\wedge n) \to \int f$, proved in Exercise 6.3. Since $|f|  = \max(f, 0) + \max(-f,0)\geq 0$ is a sum of measurable functions, it is itself measurable, and we can apply the result of Exercise 6.3. For simplicity, let $f_n = |f| \wedge n$. ( $f \wedge n = \min(f,n)$ ) Let $\varepsilon> 0$ be given and $A\in \mathcal A$. Then there exists an $N \in \mathbb N$ such that for all $n > N$, we have  $$ \int_A |f|- \int_A f_N d\mu < \varepsilon/2.$$ Since $f_n \leq n$ for all $n$, we know that $$\int_A f_N d\mu \leq \int_A N d\mu = N\cdot \mu(A).$$ Choose $\delta = \varepsilon / (2N)$ and $A \in \mathcal A$ such that $\mu(A) < \delta$. Then, \begin{align*} \int_A |f| d\mu &= \int_A f_N d\mu  + \int_A |f|d\mu - \int_A f_Nd\mu\\ &< N\cdot \mu(A) + \varepsilon/2\\ &< N\cdot \delta + \varepsilon/2\\ &= \varepsilon. \end{align*} Is $\sigma$-finiteness really a necessary condition on $\mu$?","Question: Let $(X, \mathcal A, \mu)$ be a measure space and suppose $\mu$ is $\sigma$-finite. Suppose $f$ is integrable. Prove that given any $\varepsilon$, there exists a $\delta >0$ such that $$\int_A |f| d\mu < \varepsilon$$ whenever $\mu(A) < \delta$. My Solution: We use the fact that if $f \geq 0$ is measurable, then $\lim_{n\to\infty}\int (f\wedge n) \to \int f$, proved in Exercise 6.3. Since $|f|  = \max(f, 0) + \max(-f,0)\geq 0$ is a sum of measurable functions, it is itself measurable, and we can apply the result of Exercise 6.3. For simplicity, let $f_n = |f| \wedge n$. ( $f \wedge n = \min(f,n)$ ) Let $\varepsilon> 0$ be given and $A\in \mathcal A$. Then there exists an $N \in \mathbb N$ such that for all $n > N$, we have  $$ \int_A |f|- \int_A f_N d\mu < \varepsilon/2.$$ Since $f_n \leq n$ for all $n$, we know that $$\int_A f_N d\mu \leq \int_A N d\mu = N\cdot \mu(A).$$ Choose $\delta = \varepsilon / (2N)$ and $A \in \mathcal A$ such that $\mu(A) < \delta$. Then, \begin{align*} \int_A |f| d\mu &= \int_A f_N d\mu  + \int_A |f|d\mu - \int_A f_Nd\mu\\ &< N\cdot \mu(A) + \varepsilon/2\\ &< N\cdot \delta + \varepsilon/2\\ &= \varepsilon. \end{align*} Is $\sigma$-finiteness really a necessary condition on $\mu$?",,"['real-analysis', 'integration', 'measure-theory']"
58,Evaluate $\int \frac {x^2}{\sqrt{\arctan x}} dx$,Evaluate,\int \frac {x^2}{\sqrt{\arctan x}} dx,Is there any closed form expression of $$\int \dfrac {x^2}{\sqrt{\arctan x}} dx?$$,Is there any closed form expression of $$\int \dfrac {x^2}{\sqrt{\arctan x}} dx?$$,,['integration']
59,"Hard integration problems book, special functions","Hard integration problems book, special functions",,I want hard integration problems which level is college competition or harder. I want problems book about hard integration. Would you recommend some problems books? And can you recommend books about special function (used in graduate level)?,I want hard integration problems which level is college competition or harder. I want problems book about hard integration. Would you recommend some problems books? And can you recommend books about special function (used in graduate level)?,,"['integration', 'reference-request']"
60,"Integral, Definite Integral $ \int_{-\infty}^\infty \exp{\big(\alpha x^4+\beta x^3+\gamma x^2 +\delta x+\epsilon}\big)dx, \ \alpha <0. $","Integral, Definite Integral"," \int_{-\infty}^\infty \exp{\big(\alpha x^4+\beta x^3+\gamma x^2 +\delta x+\epsilon}\big)dx, \ \alpha <0. ","Calculate  the integral $$ I=\int_{-\infty}^\infty \exp{\big(\alpha x^4+\beta x^3+\gamma x^2 +\delta x+\epsilon}\big)dx, \ \alpha <0. $$ The answer can be expressed analytically in terms of a series which will have a $\Gamma$ function in it.  I am not sure how to approach it since I do not know how to work with the argument of the exponential.  The other constants we can assume to be real.  The answer can be expressed as  $$ I=e^{\epsilon} \sum_{n,m,p=0} \frac{\beta^{4n}}{(4n)!}\frac{\gamma^{2m}}{(2m)!}\frac{\delta^{4p}}{(4p)!}\frac{\Gamma(3n+m+p+\frac{1}{4})}{\alpha^{3n+m+p+\frac{1}{4}}} $$","Calculate  the integral $$ I=\int_{-\infty}^\infty \exp{\big(\alpha x^4+\beta x^3+\gamma x^2 +\delta x+\epsilon}\big)dx, \ \alpha <0. $$ The answer can be expressed analytically in terms of a series which will have a $\Gamma$ function in it.  I am not sure how to approach it since I do not know how to work with the argument of the exponential.  The other constants we can assume to be real.  The answer can be expressed as  $$ I=e^{\epsilon} \sum_{n,m,p=0} \frac{\beta^{4n}}{(4n)!}\frac{\gamma^{2m}}{(2m)!}\frac{\delta^{4p}}{(4p)!}\frac{\Gamma(3n+m+p+\frac{1}{4})}{\alpha^{3n+m+p+\frac{1}{4}}} $$",,"['real-analysis', 'integration', 'definite-integrals', 'contest-math']"
61,How can I integrate this zeta function expression?,How can I integrate this zeta function expression?,,"Can you integrate this function: $$f(k)=\exp\left(-\Re\left(\sum\limits_{n=1}^{n=scale} \frac{1}{n} \zeta(1/2+i \cdot k)\sum\limits_{d|n} \frac{\mu(d)}{d^{(1/2+i \cdot k-1)}}\right)\right)$$ with respect to $k$? The result I would like to achieve is the plot from the accumulated function as in this Mathematica program: (*program start*) scale = 300; Print[""Counting to 60""] Monitor[g1 =     ListLinePlot[     0.69*Accumulate[       Table[Exp[-Re[           Zeta[1/2 - I*k]*            Total[Table[              Total[MoebiusMu[Divisors[n]]/                 Divisors[n]^(1/2 - I*k - 1)]/n, {n, 1, scale}]]]], {k,          0 + 1/1000, 60, N[1/6]}]], DataRange -> {0, 60},      PlotRange -> {-0.15, 15}];, Floor[k]] Show[g1, ListPlot[Table[{N[Im[ZetaZero[n]]], n}, {n, 1, 13}],    PlotStyle -> Black, Filling -> Axis]] (*program end*) The function jumps about one unit at $k$ values equal to zeta zeros.","Can you integrate this function: $$f(k)=\exp\left(-\Re\left(\sum\limits_{n=1}^{n=scale} \frac{1}{n} \zeta(1/2+i \cdot k)\sum\limits_{d|n} \frac{\mu(d)}{d^{(1/2+i \cdot k-1)}}\right)\right)$$ with respect to $k$? The result I would like to achieve is the plot from the accumulated function as in this Mathematica program: (*program start*) scale = 300; Print[""Counting to 60""] Monitor[g1 =     ListLinePlot[     0.69*Accumulate[       Table[Exp[-Re[           Zeta[1/2 - I*k]*            Total[Table[              Total[MoebiusMu[Divisors[n]]/                 Divisors[n]^(1/2 - I*k - 1)]/n, {n, 1, scale}]]]], {k,          0 + 1/1000, 60, N[1/6]}]], DataRange -> {0, 60},      PlotRange -> {-0.15, 15}];, Floor[k]] Show[g1, ListPlot[Table[{N[Im[ZetaZero[n]]], n}, {n, 1, 13}],    PlotStyle -> Black, Filling -> Axis]] (*program end*) The function jumps about one unit at $k$ values equal to zeta zeros.",,"['integration', 'elementary-number-theory', 'complex-numbers', 'riemann-zeta']"
62,Asymptotic property of integral involving Bessel function.,Asymptotic property of integral involving Bessel function.,,"Consider the following integral $$ I(s)=\int_{0}^{\infty}{J_{\frac{n-2}{2}}}(sr)r^{A+1}(e^{-r^{2\alpha}}-1)dr, $$ where $J_{\frac{n-2}{2}}$ is the Bessel function of order ${\frac{n-2}{2}}$, $s, A, \alpha>0$. I want to know the asymptotic property of $I(s)$ when $s\to+\infty$. Especially, I expect that we have $I(s)=o(s^{-\frac{n+2}{2}-A})$. I think the way to estimate such kinds of integrals is stationary phase. Can someone show me how to deal with it? Thanks very much","Consider the following integral $$ I(s)=\int_{0}^{\infty}{J_{\frac{n-2}{2}}}(sr)r^{A+1}(e^{-r^{2\alpha}}-1)dr, $$ where $J_{\frac{n-2}{2}}$ is the Bessel function of order ${\frac{n-2}{2}}$, $s, A, \alpha>0$. I want to know the asymptotic property of $I(s)$ when $s\to+\infty$. Especially, I expect that we have $I(s)=o(s^{-\frac{n+2}{2}-A})$. I think the way to estimate such kinds of integrals is stationary phase. Can someone show me how to deal with it? Thanks very much",,"['real-analysis', 'integration', 'asymptotics']"
63,Lebesgue's criterion for Riemann integrability of functions of two variables,Lebesgue's criterion for Riemann integrability of functions of two variables,,"Lebesgue's criterion for Riemann integrability for functions of one variable states that: $f:[a,b]\rightarrow \mathbb{R}$, with $a,b\in\mathbb{R},a\leq b$, is Riemann integrable if and only if $f$ is bounded and continuous $m$-a.e.,where $m$ is the Lebesgue measure on $\left(\mathbb{R},{\cal M}(\mathbb{R})\right)$. Is there such a statement (or similar) for Riemann integrability on $[a,b]\times[c,d]$, with $a,b,c,d\in\mathbb{R},a\leq b,c\leq d$,  of a function $f:[a,b]\times[c,d]\rightarrow \mathbb{R}$, in terms of boundedness and $m^2$-a.e. continuity, where $m^2$ is standard Lebesgue measure on $\left(\mathbb{R}^2,{\cal M}(\mathbb{R}^2)\right)$? Thank you very much. Comment : Thank you, Prahlad. Following Integration and Modern Analysis (Benedetto and Czaja), the proof of ""$\Leftarrow$"" in one variable needs basically the primitive $F(x) = \overline{\int_a^x}f(u)du$ (upper Riemann integral) whose derivative is equal to $f$ $m$-a.e. One then uses it to build a uniformly bounded sequence of functions $F_n(x) = n(F(x+1/n)-F(x))$ convergent to $f$ $m$-a.e. (LDC and some calculations then imply that the upper Riemann integral and Lebesgue integral are equal.) For the moment, I'm having trouble imagining correctly both $F$ and $F_n$ in two variables.","Lebesgue's criterion for Riemann integrability for functions of one variable states that: $f:[a,b]\rightarrow \mathbb{R}$, with $a,b\in\mathbb{R},a\leq b$, is Riemann integrable if and only if $f$ is bounded and continuous $m$-a.e.,where $m$ is the Lebesgue measure on $\left(\mathbb{R},{\cal M}(\mathbb{R})\right)$. Is there such a statement (or similar) for Riemann integrability on $[a,b]\times[c,d]$, with $a,b,c,d\in\mathbb{R},a\leq b,c\leq d$,  of a function $f:[a,b]\times[c,d]\rightarrow \mathbb{R}$, in terms of boundedness and $m^2$-a.e. continuity, where $m^2$ is standard Lebesgue measure on $\left(\mathbb{R}^2,{\cal M}(\mathbb{R}^2)\right)$? Thank you very much. Comment : Thank you, Prahlad. Following Integration and Modern Analysis (Benedetto and Czaja), the proof of ""$\Leftarrow$"" in one variable needs basically the primitive $F(x) = \overline{\int_a^x}f(u)du$ (upper Riemann integral) whose derivative is equal to $f$ $m$-a.e. One then uses it to build a uniformly bounded sequence of functions $F_n(x) = n(F(x+1/n)-F(x))$ convergent to $f$ $m$-a.e. (LDC and some calculations then imply that the upper Riemann integral and Lebesgue integral are equal.) For the moment, I'm having trouble imagining correctly both $F$ and $F_n$ in two variables.",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-measure']"
64,Asymptotic behaviour of sum of decreasing definite integrals,Asymptotic behaviour of sum of decreasing definite integrals,,"I would like to calculate: \begin{equation*}g(K, T) = \displaystyle \sum_{k=1}^{K} \sum_{t = 1}^{T} \int_{0}^{1} \left(1 - z^k\right)^t \, dz. \end{equation*} If no closed form solution exists, I would like to find a tight upper bound. Trivially we have \begin{equation*} g(K,T) = O(KT), \end{equation*} but we can do better than this. One can show that \begin{equation*}\displaystyle \int_{0}^{1} \left(1 - z^k\right)^t \, dz < \left(1 - \left(\frac{1}{2}\right)^\frac{1}{t}\right)^\frac{1}{k}, \end{equation*} and further that \begin{equation*} 1 - \left(\frac{1}{2}\right)^{\frac{1}{t}}  \le \frac{1}{t}, \end{equation*} which can be combined and fiddled with to provide the improved bound, \begin{equation*} g(K,T) = O(KT^{1-\frac{1}{K}}). \end{equation*} Can we do better than this?","I would like to calculate: \begin{equation*}g(K, T) = \displaystyle \sum_{k=1}^{K} \sum_{t = 1}^{T} \int_{0}^{1} \left(1 - z^k\right)^t \, dz. \end{equation*} If no closed form solution exists, I would like to find a tight upper bound. Trivially we have \begin{equation*} g(K,T) = O(KT), \end{equation*} but we can do better than this. One can show that \begin{equation*}\displaystyle \int_{0}^{1} \left(1 - z^k\right)^t \, dz < \left(1 - \left(\frac{1}{2}\right)^\frac{1}{t}\right)^\frac{1}{k}, \end{equation*} and further that \begin{equation*} 1 - \left(\frac{1}{2}\right)^{\frac{1}{t}}  \le \frac{1}{t}, \end{equation*} which can be combined and fiddled with to provide the improved bound, \begin{equation*} g(K,T) = O(KT^{1-\frac{1}{K}}). \end{equation*} Can we do better than this?",,"['integration', 'asymptotics']"
65,How to calculate this complex integral $\int_0^\infty \frac{1}{q+i}e^{-(q+b)^2}\text{d}q$? (Please Help),How to calculate this complex integral ? (Please Help),\int_0^\infty \frac{1}{q+i}e^{-(q+b)^2}\text{d}q,"I want to carry out the following integration $$\int_0^\infty  \frac{1}{q+i}e^{-(q+b)^2}\text{d}q$$ which is trivial if calculated numerically with any value for b. But I really need to get an analytic expression for this integral. I would really appreciate it if you can help with this integral. Or if you can tell it's not possible to carry it out analytically, that is also helpful. Thanks in advance Huijie","I want to carry out the following integration $$\int_0^\infty  \frac{1}{q+i}e^{-(q+b)^2}\text{d}q$$ which is trivial if calculated numerically with any value for b. But I really need to get an analytic expression for this integral. I would really appreciate it if you can help with this integral. Or if you can tell it's not possible to carry it out analytically, that is also helpful. Thanks in advance Huijie",,"['complex-analysis', 'integration', 'complex-integration']"
66,How to prove following integral equality?,How to prove following integral equality?,,"Let's have the equality $$ \int \limits_{-\infty}^{\infty} \left[ [\nabla_{\mathbf r'} \times \mathbf A (\mathbf r' )] \times \frac{\mathbf r' - \mathbf r}{|\mathbf r' - \mathbf r |^{3}}\right]d^{3}\mathbf r' = 4 \pi\mathbf A (\mathbf r), \qquad (.0) $$ where curl operator acts only on $\mathbf A$ . How to prove it? Addition . The equality is proven by me. [:))]. \begin{align*}& \int \limits_{-\infty}^{\infty} \left[ [\nabla_{\mathbf r'} \times \mathbf A (\mathbf r' )] \times \frac{\mathbf r' - \mathbf r}{|\mathbf r' - \mathbf r |^{3}}\right]d^{3}\mathbf r' = -\int \left[ \left[ \nabla ' \times \mathbf A ' \right] \times \nabla \left( \frac{1}{|\mathbf r' - \mathbf r|}\right)\right]d^{3}\mathbf r' \\& =\left[ \nabla \times \int \frac{[\nabla ' \times \mathbf A' ]d^{3}\mathbf r' }{|\mathbf r' - \mathbf r |} \right] \\&  = \left[ \nabla \times \int \nabla' \times \left( \frac{\mathbf A'}{|\mathbf r' - \mathbf r|}\right)d^{3}\mathbf r'\right] - \left[ \nabla \times \left[ \int \nabla' \left(\frac{1}{|\mathbf r' - \mathbf r|}\right) \times \mathbf A ' \right]d^{3}\mathbf r '\right] \\&=  \left[ \nabla \times \left[ \int \nabla \left(\frac{1}{|\mathbf r' - \mathbf r|}\right) \times \mathbf A ' \right]d^{3}\mathbf r '\right] \\&=  \left( \nabla \cdot \left( \nabla \int \frac{\mathbf A' d^{3}\mathbf r'}{|\mathbf r' - \mathbf r |} \right)\right) - \Delta \int \frac{\mathbf A' d^{3}\mathbf r'}{|\mathbf r' - \mathbf r|} \\&= 4 \pi \mathbf A - \left( \nabla \cdot \left( \nabla \int \frac{\mathbf A' d^{3}\mathbf r'}{|\mathbf r' - \mathbf r |} \right)\right) =  \\& = 4\pi\mathbf A - \left(\nabla \cdot \left( \int \nabla'\left( \frac{\mathbf A'd^{3}\mathbf r'}{|\mathbf r' - \mathbf r|}\right) - \int \frac{(\nabla ' \cdot \mathbf A')d^{3}\mathbf r' }{|\mathbf x' - \mathbf x |}\right) \right) = 4 \pi\mathbf A \end{align*} only if $(\nabla ' \cdot \mathbf A') = 0$ .",Let's have the equality where curl operator acts only on . How to prove it? Addition . The equality is proven by me. [:))]. only if .,"
\int \limits_{-\infty}^{\infty} \left[ [\nabla_{\mathbf r'} \times \mathbf A (\mathbf r' )] \times \frac{\mathbf r' - \mathbf r}{|\mathbf r' - \mathbf r |^{3}}\right]d^{3}\mathbf r' = 4 \pi\mathbf A (\mathbf r), \qquad (.0)
 \mathbf A \begin{align*}&
\int \limits_{-\infty}^{\infty} \left[ [\nabla_{\mathbf r'} \times \mathbf A (\mathbf r' )] \times \frac{\mathbf r' - \mathbf r}{|\mathbf r' - \mathbf r |^{3}}\right]d^{3}\mathbf r' = -\int \left[ \left[ \nabla ' \times \mathbf A ' \right] \times \nabla \left( \frac{1}{|\mathbf r' - \mathbf r|}\right)\right]d^{3}\mathbf r' \\&
=\left[ \nabla \times \int \frac{[\nabla ' \times \mathbf A' ]d^{3}\mathbf r' }{|\mathbf r' - \mathbf r |} \right] \\& 
= \left[ \nabla \times \int \nabla' \times \left( \frac{\mathbf A'}{|\mathbf r' - \mathbf r|}\right)d^{3}\mathbf r'\right] - \left[ \nabla \times \left[ \int \nabla' \left(\frac{1}{|\mathbf r' - \mathbf r|}\right) \times \mathbf A ' \right]d^{3}\mathbf r '\right] \\&= 
\left[ \nabla \times \left[ \int \nabla \left(\frac{1}{|\mathbf r' - \mathbf r|}\right) \times \mathbf A ' \right]d^{3}\mathbf r '\right] \\&= 
\left( \nabla \cdot \left( \nabla \int \frac{\mathbf A' d^{3}\mathbf r'}{|\mathbf r' - \mathbf r |} \right)\right) - \Delta \int \frac{\mathbf A' d^{3}\mathbf r'}{|\mathbf r' - \mathbf r|} \\&= 4 \pi \mathbf A - \left( \nabla \cdot \left( \nabla \int \frac{\mathbf A' d^{3}\mathbf r'}{|\mathbf r' - \mathbf r |} \right)\right) = 
\\&
= 4\pi\mathbf A - \left(\nabla \cdot \left( \int \nabla'\left( \frac{\mathbf A'd^{3}\mathbf r'}{|\mathbf r' - \mathbf r|}\right) - \int \frac{(\nabla ' \cdot \mathbf A')d^{3}\mathbf r' }{|\mathbf x' - \mathbf x |}\right) \right) = 4 \pi\mathbf A
\end{align*} (\nabla ' \cdot \mathbf A') = 0","['multivariable-calculus', 'integration']"
67,Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$,Evaluating,\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!},"I'm supposed to calculate: $$\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!}$$ By using WolframAlpha, I might guess that the limit is $\frac{1}{2}$ , which is a pretty interesting and nice result. I wonder in which ways we may approach it.","I'm supposed to calculate: By using WolframAlpha, I might guess that the limit is , which is a pretty interesting and nice result. I wonder in which ways we may approach it.",\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!} \frac{1}{2},"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
68,Algorithm to calculate multiple integral.,Algorithm to calculate multiple integral.,,"One of the major difficulties of student in advanced calculus  (including myself when student) is to obtain the extremes of repeated integrals to calculate the volume integral in $R^n$ i.e. transform  the integral  $$ \int_{D}f(x) \,\mathrm d\,V_n, \quad  f\in C(\mathbb{R}^n) $$ in the multiple integral  $$ \int_{c_1^1}^{d_1^1}\cdots\int_{c_n^1}^{d_n^1} f(x_1,\ldots,x_n) \,\mathrm d\,x_1\ldots \,\mathrm d\,x_n +\ldots + \int_{c_1^m}^{d_1^m}\cdots\int_{c_n^m}^{d_n^m} f(x_1,\ldots,x_n) \,\mathrm d\,x_1\ldots \,\mathrm d\,x_n.  $$ The integer $m$ arises from the need to partition the set $D$. More precisely I would get for a well-defined algorithm (that could be manually computable) whose output was the number $m\in\mathbb{N}$ and the extremes of integration in terms of vectors $a\in\mathbb{R}^p$, $b\in\mathbb{R}^p$ ​​and the matrices $M^1,\ldots,M^p\in\mathbb{R}^{n\times n}$ that define the set $D\subset\mathbb{R}^n$ as below   $$ D= \left\{ x= \left[ \begin{array}{c} x_1 \\ \vdots \\ x_n \end{array} \right] \in\mathbb{R}^n\quad \left|\quad \begin{array}{c} a_1 \leq x^TM^1x \leq  b_1 \\ \vdots\\ a_p \leq x^TM^px \leq b_p \\ \end{array} \right. \right\} \subset [-r,r]^n,\quad r>0. $$ In terms of explicit coordinates we have: $M^1=(M_{uv}^1)_{n\times n},\ldots ,M^p=(M_{uv}^p)_{n\times n}$ are the matrices  in $\mathbb{R}^{n\times n}$  and  $a=(a_i)_p$ and $b=(b_j)_p$ are the vectors  in $\mathbb{R}^{p}$. My question: Is there any algorithm that can be implemented but it is computationally simple for calculate the extrems of repeated  integrals above? Some reference? Thanks in advance.","One of the major difficulties of student in advanced calculus  (including myself when student) is to obtain the extremes of repeated integrals to calculate the volume integral in $R^n$ i.e. transform  the integral  $$ \int_{D}f(x) \,\mathrm d\,V_n, \quad  f\in C(\mathbb{R}^n) $$ in the multiple integral  $$ \int_{c_1^1}^{d_1^1}\cdots\int_{c_n^1}^{d_n^1} f(x_1,\ldots,x_n) \,\mathrm d\,x_1\ldots \,\mathrm d\,x_n +\ldots + \int_{c_1^m}^{d_1^m}\cdots\int_{c_n^m}^{d_n^m} f(x_1,\ldots,x_n) \,\mathrm d\,x_1\ldots \,\mathrm d\,x_n.  $$ The integer $m$ arises from the need to partition the set $D$. More precisely I would get for a well-defined algorithm (that could be manually computable) whose output was the number $m\in\mathbb{N}$ and the extremes of integration in terms of vectors $a\in\mathbb{R}^p$, $b\in\mathbb{R}^p$ ​​and the matrices $M^1,\ldots,M^p\in\mathbb{R}^{n\times n}$ that define the set $D\subset\mathbb{R}^n$ as below   $$ D= \left\{ x= \left[ \begin{array}{c} x_1 \\ \vdots \\ x_n \end{array} \right] \in\mathbb{R}^n\quad \left|\quad \begin{array}{c} a_1 \leq x^TM^1x \leq  b_1 \\ \vdots\\ a_p \leq x^TM^px \leq b_p \\ \end{array} \right. \right\} \subset [-r,r]^n,\quad r>0. $$ In terms of explicit coordinates we have: $M^1=(M_{uv}^1)_{n\times n},\ldots ,M^p=(M_{uv}^p)_{n\times n}$ are the matrices  in $\mathbb{R}^{n\times n}$  and  $a=(a_i)_p$ and $b=(b_j)_p$ are the vectors  in $\mathbb{R}^{p}$. My question: Is there any algorithm that can be implemented but it is computationally simple for calculate the extrems of repeated  integrals above? Some reference? Thanks in advance.",,"['real-analysis', 'integration', 'reference-request', 'multivariable-calculus']"
69,Difficult integral involving $\arcsin(x)$,Difficult integral involving,\arcsin(x),"I have a difficult integral to compute.  I know the result by guessing the answer, but need to know the method of calculation.  The integral is $$ \int_{a}^{b}{\rm d}p\,{p \over p^{2} - 2\mu}\, \left[{2\arcsin\left({1 \over p}\,{p^{2} - ba \over  b-a}\right) - \pi}\right] =\pi\ln\left({2\mu - a^{2} \over  \mu - {ba \over 2} + \frac{1}{2} \sqrt{\,\left(2\mu - a^{2}\right)\left(2\mu - b^{2}\,\right)\,}}\right) $$ I am considering the case where $a<b$, $a^2<2\mu$, and $b^2<2\mu$, so that there is no divergence from the denominator.  I suspect it can be solved from some contour integral trick, especially since, by using the properties of arcsin, I can rewrite it in the following form: $ {\rm Re} \int_{a}^{b} dp\, \frac{p}{p^2-2\mu} \Big[-2i \ln \big( \frac{1}{2} i (p^2 -ab) +\frac{1}{2} \sqrt{(p^2 - a^2) (b^2 - p^2) }\big)-\pi\Big]  =  \pi \ln \frac{ (2\mu - a^2)} {\mu-\frac{1}{2}ba + \frac{1}{2}\sqrt{(2\mu - a^2)(2\mu-b^2)}}$, where the Re takes the real part.  With this form, I am pretty sure there is a contour integral trick since the log function on the left is so similar to the log function on the right.  Unfortunately I cannot seem to figure it out, although I can verify it numerically.   Any help would be greatly appreciated!  Thanks, Dan","I have a difficult integral to compute.  I know the result by guessing the answer, but need to know the method of calculation.  The integral is $$ \int_{a}^{b}{\rm d}p\,{p \over p^{2} - 2\mu}\, \left[{2\arcsin\left({1 \over p}\,{p^{2} - ba \over  b-a}\right) - \pi}\right] =\pi\ln\left({2\mu - a^{2} \over  \mu - {ba \over 2} + \frac{1}{2} \sqrt{\,\left(2\mu - a^{2}\right)\left(2\mu - b^{2}\,\right)\,}}\right) $$ I am considering the case where $a<b$, $a^2<2\mu$, and $b^2<2\mu$, so that there is no divergence from the denominator.  I suspect it can be solved from some contour integral trick, especially since, by using the properties of arcsin, I can rewrite it in the following form: $ {\rm Re} \int_{a}^{b} dp\, \frac{p}{p^2-2\mu} \Big[-2i \ln \big( \frac{1}{2} i (p^2 -ab) +\frac{1}{2} \sqrt{(p^2 - a^2) (b^2 - p^2) }\big)-\pi\Big]  =  \pi \ln \frac{ (2\mu - a^2)} {\mu-\frac{1}{2}ba + \frac{1}{2}\sqrt{(2\mu - a^2)(2\mu-b^2)}}$, where the Re takes the real part.  With this form, I am pretty sure there is a contour integral trick since the log function on the left is so similar to the log function on the right.  Unfortunately I cannot seem to figure it out, although I can verify it numerically.   Any help would be greatly appreciated!  Thanks, Dan",,"['integration', 'contour-integration']"
70,"""Simple"" proof of Lebesgue's Differentiation Theorem for dimension 1?","""Simple"" proof of Lebesgue's Differentiation Theorem for dimension 1?",,"Lebesgue Differentiation Theorem for $\mathbb{R}$: Let $f:[a,b]\to \mathbb{R}$ be intergable and $F(x)=\int_a^xf$. Then $F$ is differentiable almost everywhere in $[a,b]$ and $F'=f$ a.e. Is there a (simple) proof of this result that uses basic measure theory (the Lebesgue measure on $\mathbb{R}$) and the convergence theorems for the Lebesgue integral? All the proofs I have found use the Vitalli Covering Theorem and discuss higher dimensions. That's why I think there is room for simplification","Lebesgue Differentiation Theorem for $\mathbb{R}$: Let $f:[a,b]\to \mathbb{R}$ be intergable and $F(x)=\int_a^xf$. Then $F$ is differentiable almost everywhere in $[a,b]$ and $F'=f$ a.e. Is there a (simple) proof of this result that uses basic measure theory (the Lebesgue measure on $\mathbb{R}$) and the convergence theorems for the Lebesgue integral? All the proofs I have found use the Vitalli Covering Theorem and discuss higher dimensions. That's why I think there is room for simplification",,"['real-analysis', 'measure-theory', 'integration', 'derivatives']"
71,Is there a way to Fourier transform $\cos\left(\sin (x)\right)$?,Is there a way to Fourier transform ?,\cos\left(\sin (x)\right),"In my physics problem, I encountered a solution has the form like $\cos\left(\sin (t)\right)$ , and I need to do the Fourier transform to this solution. Is there a way to do the Fourier transform analytically to $\cos\left(\sin (t)\right)$ on a certain range, say from $t=0$ to $t=2\pi$ ? Or is there some way to manipulate $\cos\left(\sin (x)\right)$ and get an approximate expression which can be transformed analytically? The problem may be rephrased as to do the integral analytically or approximately: $$ \int_0^{2\pi}\cos{(\sin{(t)}})~e^{i\omega t}dt $$ Thanks.","In my physics problem, I encountered a solution has the form like , and I need to do the Fourier transform to this solution. Is there a way to do the Fourier transform analytically to on a certain range, say from to ? Or is there some way to manipulate and get an approximate expression which can be transformed analytically? The problem may be rephrased as to do the integral analytically or approximately: Thanks.","\cos\left(\sin (t)\right) \cos\left(\sin (t)\right) t=0 t=2\pi \cos\left(\sin (x)\right) 
\int_0^{2\pi}\cos{(\sin{(t)}})~e^{i\omega t}dt
","['integration', 'fourier-analysis']"
72,definit integral of Airy function [closed],definit integral of Airy function [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How can I evaluate this definite integral $$ \int_0^\infty \frac{\operatorname{Ai}^2(z+a_n)}{z^2}dz $$ where $a_n$ are the zeroes of the Airy function.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How can I evaluate this definite integral $$ \int_0^\infty \frac{\operatorname{Ai}^2(z+a_n)}{z^2}dz $$ where $a_n$ are the zeroes of the Airy function.",,"['integration', 'definite-integrals', 'airy-functions']"
73,Various integration theories,Various integration theories,,"Could anyone briefly explain, or point me towards a resource explaining, the main differences between the main integration theories, namely: Riemann Integration Riemann-Stieltjes Integration Lebesgue Integration Lebesgue-Stieltjes Integration I understand that this a question with a potentially very long answer, since integration is no small theory. I can find information on these integrals separately, but I would really be interested in a comparison of these integrals with one another.","Could anyone briefly explain, or point me towards a resource explaining, the main differences between the main integration theories, namely: Riemann Integration Riemann-Stieltjes Integration Lebesgue Integration Lebesgue-Stieltjes Integration I understand that this a question with a potentially very long answer, since integration is no small theory. I can find information on these integrals separately, but I would really be interested in a comparison of these integrals with one another.",,"['integration', 'lebesgue-integral']"
74,Change of variables in line integral with abs. value,Change of variables in line integral with abs. value,,"Let $\gamma : I \rightarrow \mathbb C$ be a path. Let $g: \mathbb C \rightarrow \mathbb C$ be a biholomorphic map. Let $f$ be a holomorphic function. Consider the integral $$ \int_{g\circ \gamma} f(g(z)) \mathrm d|g(z)| $$. What is a suitable change of variable formula in this case? My difficulty with the normal is with the absolute value sign with $|dz|$, which I interpret in the following way. Set $z = x+ iy$ with $x,y$ real, $$|dz| = \left( \left(\frac{dx}{dt}\right)^2+ \left(\frac{dy}{dt}\right)^2\right)^{1/2} dt$$ where $t \in I$ parametrizes the path $\gamma$. I do not know how to derive a suitable change of variables formula with this setup and would be grateful for a reference with derivation.","Let $\gamma : I \rightarrow \mathbb C$ be a path. Let $g: \mathbb C \rightarrow \mathbb C$ be a biholomorphic map. Let $f$ be a holomorphic function. Consider the integral $$ \int_{g\circ \gamma} f(g(z)) \mathrm d|g(z)| $$. What is a suitable change of variable formula in this case? My difficulty with the normal is with the absolute value sign with $|dz|$, which I interpret in the following way. Set $z = x+ iy$ with $x,y$ real, $$|dz| = \left( \left(\frac{dx}{dt}\right)^2+ \left(\frac{dy}{dt}\right)^2\right)^{1/2} dt$$ where $t \in I$ parametrizes the path $\gamma$. I do not know how to derive a suitable change of variables formula with this setup and would be grateful for a reference with derivation.",,"['reference-request', 'complex-analysis', 'integration']"
75,Solving definite integral $\int_{0}^{1} \tan^{-1}(1-1/x)dx$,Solving definite integral,\int_{0}^{1} \tan^{-1}(1-1/x)dx,"$$\int_{0}^{1} \tan^{-1}\left(1-\frac1x\right)dx$$ Here's what I have done so far. (the answer is given as $-\pi/4$ ) Let $$ I = \int_{0}^{1}\tan^{-1}\left(1-\frac1x\right)dx = \int_{0}^{1}\tan^{-1}\left(\frac{x-1}x\right)dx. $$ Since, $\int_{0}^{1} f(x)dx = \int_{0}^{1} f(1-x)dx$ one has \begin{align} I &= \int_{0}^{1}\tan^{-1}\left(1-\frac1{1-x}\right)dx\\ & = \int_{0}^{1}\tan^{-1}\left(\frac x{x-1}\right)dx\\ & = \int_{0}^{1}\frac\pi2-\cot^{-1}\left(\frac x{x-1}\right)dx\\ & = \int_{0}^{1}\frac\pi2-\tan^{-1}\left(\frac{x-1}x\right)dx\\ & = \frac\pi2 - I, \end{align} Hence, $I = \dfrac\pi4$ . The given answer is $-\dfrac\pi4$ . Where have I gone wrong?","Here's what I have done so far. (the answer is given as ) Let Since, one has Hence, . The given answer is . Where have I gone wrong?","\int_{0}^{1} \tan^{-1}\left(1-\frac1x\right)dx -\pi/4 
I = \int_{0}^{1}\tan^{-1}\left(1-\frac1x\right)dx = \int_{0}^{1}\tan^{-1}\left(\frac{x-1}x\right)dx.
 \int_{0}^{1} f(x)dx = \int_{0}^{1} f(1-x)dx \begin{align}
I &= \int_{0}^{1}\tan^{-1}\left(1-\frac1{1-x}\right)dx\\
& = \int_{0}^{1}\tan^{-1}\left(\frac x{x-1}\right)dx\\
& = \int_{0}^{1}\frac\pi2-\cot^{-1}\left(\frac x{x-1}\right)dx\\
& = \int_{0}^{1}\frac\pi2-\tan^{-1}\left(\frac{x-1}x\right)dx\\
& = \frac\pi2 - I,
\end{align} I = \dfrac\pi4 -\dfrac\pi4","['integration', 'definite-integrals', 'improper-integrals', 'trigonometric-integrals']"
76,Upper bound on integral: $\int_1^\infty \frac{dx}{\sqrt{x^3-1}} < 4$,Upper bound on integral:,\int_1^\infty \frac{dx}{\sqrt{x^3-1}} < 4,"I'm going through Nahin's book Inside Interesting Integrals, and I'm stuck at an early problem, Challenge Problem 1.2: to show that $$\int_1^\infty \frac{dx}{\sqrt{x^3-1}}$$ exists because there is a finite upper-bound on its value. In particular, show that the integral is less than 4. I've tried various substitutions and also comparisons with similar integrals, but the problem is that any other integral that I can easily compare the above integral to is just as hard to integrate, which doesn't help solve the problem. I also tried just looking at the graph and hoping for insight, but that didn't work either. So how doesone one place an upper bound on the integral?","I'm going through Nahin's book Inside Interesting Integrals, and I'm stuck at an early problem, Challenge Problem 1.2: to show that $$\int_1^\infty \frac{dx}{\sqrt{x^3-1}}$$ exists because there is a finite upper-bound on its value. In particular, show that the integral is less than 4. I've tried various substitutions and also comparisons with similar integrals, but the problem is that any other integral that I can easily compare the above integral to is just as hard to integrate, which doesn't help solve the problem. I also tried just looking at the graph and hoping for insight, but that didn't work either. So how doesone one place an upper bound on the integral?",,"['integration', 'definite-integrals']"
77,"Calculate $\int 5^{x+1}e^{2x-1}\,dx$ [closed]",Calculate  [closed],"\int 5^{x+1}e^{2x-1}\,dx","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question How to calculate following integration? $$\int 5^{x+1}e^{2x-1}dx$$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question How to calculate following integration?",\int 5^{x+1}e^{2x-1}dx,['integration']
78,"Integration by parts: ""math is broken"" [duplicate]","Integration by parts: ""math is broken"" [duplicate]",,"This question already has answers here : Integration by parts of $\cot x$ (2 answers) Closed 8 years ago . just trying to solve a small example on integration by parts, and a weird thing happens: I come to an original expression increased by one. Please help me find out where the flaw is! The task is to calculate the following indefinite integral: $$ \int\tan^{-1}x\text{d}x $$ Integration by parts formula (just in case): $$ \int f(x)g'(x)dx = f(x)g(x) - \int f'(x)g(x)\text{d}x $$ Let's expand our original integral: $$ \int\tan^{-1}x\text{d}x = \int\cos x \sin^{-1}x\text{d}x $$ If $$ f(x) = \sin^{-1}x $$ $$ g'(x) = \cos x $$ then $$ f'(x) = -\sin^{-2}x\cos x $$ $$ g(x) = \sin x $$ Applying integration by parts formula: $$ \int\cos x \sin^{-1}x\text{d}x = \sin^{-1}x\sin x - \int-\sin^{-2}x\cos x\sin x\text{d}x = 1 + \int\tan^{-1}x\text{d}x  $$ So, where have I made a mistake?","This question already has answers here : Integration by parts of $\cot x$ (2 answers) Closed 8 years ago . just trying to solve a small example on integration by parts, and a weird thing happens: I come to an original expression increased by one. Please help me find out where the flaw is! The task is to calculate the following indefinite integral: $$ \int\tan^{-1}x\text{d}x $$ Integration by parts formula (just in case): $$ \int f(x)g'(x)dx = f(x)g(x) - \int f'(x)g(x)\text{d}x $$ Let's expand our original integral: $$ \int\tan^{-1}x\text{d}x = \int\cos x \sin^{-1}x\text{d}x $$ If $$ f(x) = \sin^{-1}x $$ $$ g'(x) = \cos x $$ then $$ f'(x) = -\sin^{-2}x\cos x $$ $$ g(x) = \sin x $$ Applying integration by parts formula: $$ \int\cos x \sin^{-1}x\text{d}x = \sin^{-1}x\sin x - \int-\sin^{-2}x\cos x\sin x\text{d}x = 1 + \int\tan^{-1}x\text{d}x  $$ So, where have I made a mistake?",,"['integration', 'integration-by-parts']"
79,The value of $ \int _{0}^{1}x^{99}(1-x)^{100}dx $ is,The value of  is, \int _{0}^{1}x^{99}(1-x)^{100}dx ,The value of $\int _{0}^{1}x^{99}(1-x)^{100}dx $ is Not able to do. I'm trying substituton. But clear failure. Please help.,The value of $\int _{0}^{1}x^{99}(1-x)^{100}dx $ is Not able to do. I'm trying substituton. But clear failure. Please help.,,"['integration', 'definite-integrals', 'factorial']"
80,"Different ways to tackle the integral $\int_0^1\sqrt\frac x{1-x}\,dx$",Different ways to tackle the integral,"\int_0^1\sqrt\frac x{1-x}\,dx","$$\int_0^1\sqrt\frac x{1-x}\,dx$$ I saw in my book that the solution is $x=\cos^2u$ and $dx=-2\cos u\sin u\ du$. I would like to see different approaches, can you provide them?","$$\int_0^1\sqrt\frac x{1-x}\,dx$$ I saw in my book that the solution is $x=\cos^2u$ and $dx=-2\cos u\sin u\ du$. I would like to see different approaches, can you provide them?",,"['integration', 'definite-integrals', 'alternative-proof']"
81,"Evaluate $\int\frac{\cos x}{\sin x + \cos x}\,\text{d}x$. [duplicate]",Evaluate . [duplicate],"\int\frac{\cos x}{\sin x + \cos x}\,\text{d}x","This question already has answers here : Compute $\int \frac{\sin(x)}{\sin(x)+\cos(x)}\mathrm dx$ (8 answers) Closed 11 years ago . I'm trying to figure out how to take this indefinite integral: $$ \int\frac{\cos x}{\sin x + \cos x}\,\text{d}x.$$ I tried simplifying and rearranging it, and this is the best I got: $$\int\frac{1}{\tan x + 1 }\,\text{d}x.$$ But I still can't figure out how to integrate from there. I know that it's integrable, as Wolfram Alpha indicates that the integral is $ \frac{1}{2}\big(x+\ln{(\sin x + \cos x)}\big)+C$, but I can't figure out the steps to deriving it. Does anyone know how to evaluate this integral?","This question already has answers here : Compute $\int \frac{\sin(x)}{\sin(x)+\cos(x)}\mathrm dx$ (8 answers) Closed 11 years ago . I'm trying to figure out how to take this indefinite integral: $$ \int\frac{\cos x}{\sin x + \cos x}\,\text{d}x.$$ I tried simplifying and rearranging it, and this is the best I got: $$\int\frac{1}{\tan x + 1 }\,\text{d}x.$$ But I still can't figure out how to integrate from there. I know that it's integrable, as Wolfram Alpha indicates that the integral is $ \frac{1}{2}\big(x+\ln{(\sin x + \cos x)}\big)+C$, but I can't figure out the steps to deriving it. Does anyone know how to evaluate this integral?",,"['integration', 'trigonometry']"
82,Integrating $\frac{\log(1+x)}{1+x^2}$ [duplicate],Integrating  [duplicate],\frac{\log(1+x)}{1+x^2},"This question already has answers here : Closed 11 years ago . Possible Duplicate: Evaluate the integral: $\int_{0}^{1} \frac{\ln(x+1)}{x^2+1} dx$ I am a bit stuck here in evaluating the following integral:$$\int_{0}^{1}\frac{\log(1+x)}{1+x^2}\,\mathrm dx$$.Your help is appreciated.","This question already has answers here : Closed 11 years ago . Possible Duplicate: Evaluate the integral: $\int_{0}^{1} \frac{\ln(x+1)}{x^2+1} dx$ I am a bit stuck here in evaluating the following integral:$$\int_{0}^{1}\frac{\log(1+x)}{1+x^2}\,\mathrm dx$$.Your help is appreciated.",,[]
83,How can I evaluate $\lim_{n \rightarrow \infty} \int_n^\infty \frac{n^2 \arctan {\frac{1}{x}}}{x^2+n^2}\ dx$?,How can I evaluate ?,\lim_{n \rightarrow \infty} \int_n^\infty \frac{n^2 \arctan {\frac{1}{x}}}{x^2+n^2}\ dx,"I'm here wondering if this integral that our math teacher gave us (students) is even possible to evaluate? I just started to study real analysis so I find this very disturbing. Here you go, and if anyone has any idea I will be very grateful. $$\lim_{n \rightarrow \infty} \int_n^\infty \frac{n^2 \arctan {\frac{1}{x}}}{x^2+n^2}\ dx$$","I'm here wondering if this integral that our math teacher gave us (students) is even possible to evaluate? I just started to study real analysis so I find this very disturbing. Here you go, and if anyone has any idea I will be very grateful. $$\lim_{n \rightarrow \infty} \int_n^\infty \frac{n^2 \arctan {\frac{1}{x}}}{x^2+n^2}\ dx$$",,"['real-analysis', 'integration', 'definite-integrals', 'riemann-integration']"
84,Evaluation of $\int x^8\sqrt{x^2+1}\ dx$,Evaluation of,\int x^8\sqrt{x^2+1}\ dx,"Evaluation of $\displaystyle \int x^8\sqrt{x^2+1}\ dx$ $\bf{My\: Try::}$ Put $x=\tan \theta\;,$ Then $dx = \sec^2 \theta d \theta$ So $$I = \int \sec ^{3}\theta \cdot \tan^{8}\theta d \theta=\int \frac{\sin^{8}\theta}{\cos^{5}\theta }d\theta$$ Now How can I solve it after that, Help Required, Thanks","Evaluation of Put Then So Now How can I solve it after that, Help Required, Thanks","\displaystyle \int x^8\sqrt{x^2+1}\ dx \bf{My\: Try::} x=\tan \theta\;, dx = \sec^2 \theta d \theta I = \int \sec ^{3}\theta \cdot \tan^{8}\theta d \theta=\int \frac{\sin^{8}\theta}{\cos^{5}\theta }d\theta","['integration', 'indefinite-integrals']"
85,Prove that $\lim_{n\to\infty} H_n/n = 0$ ($H_n$ is the $n$-th harmonic number) using certain techniques,Prove that  ( is the -th harmonic number) using certain techniques,\lim_{n\to\infty} H_n/n = 0 H_n n,"I can't seem to use certain methods such as $\varepsilon$-N, L'Hôspital's Rule, Riemann Sums, Integral Test and Divergence Test Contrapositive or Euler's Integral Representation to prove that $\lim_{n-> \infty} \frac{H_n}{n} = 0$ where $H_n$ is the nth Harmonic number $= \sum_{i=1}^{n} \frac{1}{i} = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + ... + \frac{1}{n}$? I was able to prove it using the Monotone Convergence Theorem, and I think polylogarithms make this easy . I would like to know if any of these are correct/can be modified to be correct/even possible to use. Here are my attempts: 1 $\varepsilon-\delta$ $\forall \varepsilon > 0, \exists N > 0$ s.t. $|\frac{H_n}{n} - 0| < \varepsilon$ whenever $n > N$ $|\frac{H_n}{n} - 0|$ $=|\frac{H_n}{n}|$ $ \leq |\frac{H_n}{n}|$ $ \leq \frac{|H_n|}{|n|}$ $ \leq \frac{|H_n|}{|n|} < \varepsilon$ if $n > N=\frac{H_n}{\epsilon}$ But I haven't really isolated n, so I don't think that's allowed? 2 L'Hôspital's Rule $\lim_{n-> \infty} \frac{H_n}{n}$ $= \lim_{n-> \infty} \frac{\frac{\partial}{\partial n} H_n}{1}$ Is there a discrete version of the Fundamental Theorem of Calculus that allows us to evaluate $\frac{\partial}{\partial n} H_n$? Maybe it's this one , but I don't understand it. Does $\frac{\partial}{\partial n} H_n$ even have meaning? Or is there some way to show that $\lim_{n-> \infty} \frac{H_n}{n} = \lim_{n-> \infty} \frac{1}{n}\int_{1}^{n} \frac{1}{x} dx$? 3 Riemann Sums $\lim_{n-> \infty} \frac{H_n}{n}$ $= \lim_{n-> \infty} \frac{\sum_{i=1}^{n}\frac{1}{i}}{n}$ $= \lim_{n-> \infty} \frac{1}{n}\sum_{i=1}^{n}\frac{1}{i}$ $= \lim_{n-> \infty} \frac{1}{n}\sum_{i=1}^{n}\frac{1}{i/n}\frac{1}{n}$ $= \lim_{n-> \infty} \frac{1}{n} \lim_{n-> \infty} \sum_{i=1}^{n}\frac{1}{i/n}\frac{1}{n}$ $= \lim_{n-> \infty} \frac{1}{n} \int_{0}^{1} \frac{1}{x} dx$ $= \lim_{n-> \infty} \frac{1}{n} ln|x||_{0}^{1}$ $= \lim_{n-> \infty} \frac{1}{n} (ln|1| - ln|0|)$ $= \lim_{n-> \infty} \frac{1}{n} (- ln|0|)$ $= \lim_{n-> \infty} \frac{1}{n} (- ln|1/n|)$ if that's even allowed $= \lim_{n-> \infty} \frac{1}{1} (- 1/(1/n)) \lim_{n-> \infty} \frac{-1}{n^{2}}$ by L'Hôspital's Rule $= \lim_{n-> \infty} \frac{1}{n^{2}} (1/(1/n))$ $= \lim_{n-> \infty} \frac{1}{n^{2}} (n)$ $= \lim_{n-> \infty} \frac{1}{n} = 0$ 4 Integral Test and Divergence Test Contrapositive If the series $\sum_{n=1}^{\infty} \frac{H_n}{n}$ is convergent then $\lim_{n-> \infty} \frac{H_n}{n} = 0$ If the integral $\int_{1}^{\infty} \frac{H_x}{x} dx$ is convergent then the series $\sum_{n=1}^{\infty} \frac{H_n}{n}$ is convergent. I don't know how to integrate $\int_{1}^{\infty} \frac{H_x}{x} dx$. How about $\int_{1}^{\infty} \frac{\int_{1}^{x} \frac{1}{y} dy}{x} dx$ ? If that is convergent does that mean the series $\sum_{n=1}^{\infty} \frac{H_n}{n}$ is convergent? It doesn't seem to be convergent in the first place though: $\int_{1}^{\infty} \frac{\int_{1}^{x} \frac{1}{y} dy}{x} dx$ $= \int_{1}^{\infty} \int_{1}^{x} \frac{1}{xy} dy dx$ $= \int_{1}^{\infty} \frac{ln|y|}{x}|_{y=1}^{y=x} dx$ $= \int_{1}^{\infty} \frac{ln|x|}{x} dx$ $= \int_{1}^{\infty} \frac{lnx}{x} dx$ $= (lnx)^{2}|_{1}^{\infty} = \infty$ 5 Euler's Integral Representation Euler proved that $H_n = \int_{0}^{1} \frac{1-x^{n}}{1-x} dx$ So, $\lim_{n-> \infty} \frac{H_n}{n}$ $= \lim_{n-> \infty} \frac{\int_{0}^{1} \frac{1-x^{n}}{1-x} dx}{n}$ $= \lim_{n-> \infty} \frac{\partial}{\partial n}\int_{0}^{1} \frac{1-x^{n}}{1-x} dx$ by L'Hôspital's Rule $= \lim_{n-> \infty} \int_{0}^{1} \frac{\partial}{\partial n} \frac{1-x^{n}}{1-x} dx$ $= \lim_{n-> \infty} \int_{0}^{1} \frac{\partial}{\partial n} \frac{1-x^{n}}{1-x} dx$ $= \lim_{n-> \infty} \int_{0}^{1} \frac{\partial}{\partial n} \frac{-x^{n}}{1-x} dx$ $= \lim_{n-> \infty} \int_{0}^{1} \frac{-nx^{n-1}}{1-x} dx$ $= \lim_{n-> \infty} \int_{0}^{1} \frac{nx^{n-1}}{x-1} dx$ $= \lim_{n-> \infty} n \int_{0}^{1} \frac{x^{n-1}}{x-1} dx$ $= \lim_{n-> \infty} n \int_{0}^{1} \frac{x^{n-1} - 1}{x-1} + \frac{1}{x-1}dx$ $= \lim_{n-> \infty} n \int_{0}^{1} \sum_{i=0}^{n-2} x^{i}+ \frac{1}{x-1}dx$ It looks like I will get an ln(0) again...","I can't seem to use certain methods such as $\varepsilon$-N, L'Hôspital's Rule, Riemann Sums, Integral Test and Divergence Test Contrapositive or Euler's Integral Representation to prove that $\lim_{n-> \infty} \frac{H_n}{n} = 0$ where $H_n$ is the nth Harmonic number $= \sum_{i=1}^{n} \frac{1}{i} = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + ... + \frac{1}{n}$? I was able to prove it using the Monotone Convergence Theorem, and I think polylogarithms make this easy . I would like to know if any of these are correct/can be modified to be correct/even possible to use. Here are my attempts: 1 $\varepsilon-\delta$ $\forall \varepsilon > 0, \exists N > 0$ s.t. $|\frac{H_n}{n} - 0| < \varepsilon$ whenever $n > N$ $|\frac{H_n}{n} - 0|$ $=|\frac{H_n}{n}|$ $ \leq |\frac{H_n}{n}|$ $ \leq \frac{|H_n|}{|n|}$ $ \leq \frac{|H_n|}{|n|} < \varepsilon$ if $n > N=\frac{H_n}{\epsilon}$ But I haven't really isolated n, so I don't think that's allowed? 2 L'Hôspital's Rule $\lim_{n-> \infty} \frac{H_n}{n}$ $= \lim_{n-> \infty} \frac{\frac{\partial}{\partial n} H_n}{1}$ Is there a discrete version of the Fundamental Theorem of Calculus that allows us to evaluate $\frac{\partial}{\partial n} H_n$? Maybe it's this one , but I don't understand it. Does $\frac{\partial}{\partial n} H_n$ even have meaning? Or is there some way to show that $\lim_{n-> \infty} \frac{H_n}{n} = \lim_{n-> \infty} \frac{1}{n}\int_{1}^{n} \frac{1}{x} dx$? 3 Riemann Sums $\lim_{n-> \infty} \frac{H_n}{n}$ $= \lim_{n-> \infty} \frac{\sum_{i=1}^{n}\frac{1}{i}}{n}$ $= \lim_{n-> \infty} \frac{1}{n}\sum_{i=1}^{n}\frac{1}{i}$ $= \lim_{n-> \infty} \frac{1}{n}\sum_{i=1}^{n}\frac{1}{i/n}\frac{1}{n}$ $= \lim_{n-> \infty} \frac{1}{n} \lim_{n-> \infty} \sum_{i=1}^{n}\frac{1}{i/n}\frac{1}{n}$ $= \lim_{n-> \infty} \frac{1}{n} \int_{0}^{1} \frac{1}{x} dx$ $= \lim_{n-> \infty} \frac{1}{n} ln|x||_{0}^{1}$ $= \lim_{n-> \infty} \frac{1}{n} (ln|1| - ln|0|)$ $= \lim_{n-> \infty} \frac{1}{n} (- ln|0|)$ $= \lim_{n-> \infty} \frac{1}{n} (- ln|1/n|)$ if that's even allowed $= \lim_{n-> \infty} \frac{1}{1} (- 1/(1/n)) \lim_{n-> \infty} \frac{-1}{n^{2}}$ by L'Hôspital's Rule $= \lim_{n-> \infty} \frac{1}{n^{2}} (1/(1/n))$ $= \lim_{n-> \infty} \frac{1}{n^{2}} (n)$ $= \lim_{n-> \infty} \frac{1}{n} = 0$ 4 Integral Test and Divergence Test Contrapositive If the series $\sum_{n=1}^{\infty} \frac{H_n}{n}$ is convergent then $\lim_{n-> \infty} \frac{H_n}{n} = 0$ If the integral $\int_{1}^{\infty} \frac{H_x}{x} dx$ is convergent then the series $\sum_{n=1}^{\infty} \frac{H_n}{n}$ is convergent. I don't know how to integrate $\int_{1}^{\infty} \frac{H_x}{x} dx$. How about $\int_{1}^{\infty} \frac{\int_{1}^{x} \frac{1}{y} dy}{x} dx$ ? If that is convergent does that mean the series $\sum_{n=1}^{\infty} \frac{H_n}{n}$ is convergent? It doesn't seem to be convergent in the first place though: $\int_{1}^{\infty} \frac{\int_{1}^{x} \frac{1}{y} dy}{x} dx$ $= \int_{1}^{\infty} \int_{1}^{x} \frac{1}{xy} dy dx$ $= \int_{1}^{\infty} \frac{ln|y|}{x}|_{y=1}^{y=x} dx$ $= \int_{1}^{\infty} \frac{ln|x|}{x} dx$ $= \int_{1}^{\infty} \frac{lnx}{x} dx$ $= (lnx)^{2}|_{1}^{\infty} = \infty$ 5 Euler's Integral Representation Euler proved that $H_n = \int_{0}^{1} \frac{1-x^{n}}{1-x} dx$ So, $\lim_{n-> \infty} \frac{H_n}{n}$ $= \lim_{n-> \infty} \frac{\int_{0}^{1} \frac{1-x^{n}}{1-x} dx}{n}$ $= \lim_{n-> \infty} \frac{\partial}{\partial n}\int_{0}^{1} \frac{1-x^{n}}{1-x} dx$ by L'Hôspital's Rule $= \lim_{n-> \infty} \int_{0}^{1} \frac{\partial}{\partial n} \frac{1-x^{n}}{1-x} dx$ $= \lim_{n-> \infty} \int_{0}^{1} \frac{\partial}{\partial n} \frac{1-x^{n}}{1-x} dx$ $= \lim_{n-> \infty} \int_{0}^{1} \frac{\partial}{\partial n} \frac{-x^{n}}{1-x} dx$ $= \lim_{n-> \infty} \int_{0}^{1} \frac{-nx^{n-1}}{1-x} dx$ $= \lim_{n-> \infty} \int_{0}^{1} \frac{nx^{n-1}}{x-1} dx$ $= \lim_{n-> \infty} n \int_{0}^{1} \frac{x^{n-1}}{x-1} dx$ $= \lim_{n-> \infty} n \int_{0}^{1} \frac{x^{n-1} - 1}{x-1} + \frac{1}{x-1}dx$ $= \lim_{n-> \infty} n \int_{0}^{1} \sum_{i=0}^{n-2} x^{i}+ \frac{1}{x-1}dx$ It looks like I will get an ln(0) again...",,"['integration', 'sequences-and-series', 'limits', 'convergence-divergence', 'summation']"
86,Find integral when $dx$ is in the numerator,Find integral when  is in the numerator,dx,Can someone please walk me through the steps to find the following integral? I'm not sure what to do when $dx$ is at the top. $$ \int  \frac{ x^{2}dx }{ (x^{3} + 5)^{2}} $$,Can someone please walk me through the steps to find the following integral? I'm not sure what to do when $dx$ is at the top. $$ \int  \frac{ x^{2}dx }{ (x^{3} + 5)^{2}} $$,,"['integration', 'indefinite-integrals']"
87,Show that $\int \limits_{0}^{\infty}\frac{x}{\sinh ax}dx=\left(\frac{\pi}{2a}\right)^2$,Show that,\int \limits_{0}^{\infty}\frac{x}{\sinh ax}dx=\left(\frac{\pi}{2a}\right)^2,Show that $$\int \limits_{0}^{\infty}\frac{x}{\sinh ax}dx=\left(\frac{\pi}{2a}\right)^2$$ using 2 ways: the first using contour integration and the second using real analysis.,Show that $$\int \limits_{0}^{\infty}\frac{x}{\sinh ax}dx=\left(\frac{\pi}{2a}\right)^2$$ using 2 ways: the first using contour integration and the second using real analysis.,,"['integration', 'contour-integration']"
88,"value of $\int_{-\infty}^{\infty}\arcsin\frac1{\cosh x}\,dx$",value of,"\int_{-\infty}^{\infty}\arcsin\frac1{\cosh x}\,dx","I want to know the value of $$I=\int_{-\infty}^{\infty}\arcsin\frac1{\cosh x}\,dx$$ The Symbolab integral calculator says that the integral diverges, but when one graphs it obvious that it converges. So what is the value? I was thinking that I might try Feynman integration, but I can't think of the right substitution. Alert! Alert! I've found an antiderivative! From the answer provided by @user10354138, we can reach $$\int\arcsin\frac1{\cosh x}dx=i\operatorname{Li}_2(i\phi)-i\operatorname{Li}_2(-i\phi)+C$$ Where $$\phi=\tan\bigg(\frac12\arcsin\frac1{\cosh x}\bigg)$$ And $$\operatorname{Li}_2(z)=\sum_{n\geq1}\frac{z^n}{n^2}$$ is the Di-logarithm .","I want to know the value of The Symbolab integral calculator says that the integral diverges, but when one graphs it obvious that it converges. So what is the value? I was thinking that I might try Feynman integration, but I can't think of the right substitution. Alert! Alert! I've found an antiderivative! From the answer provided by @user10354138, we can reach Where And is the Di-logarithm .","I=\int_{-\infty}^{\infty}\arcsin\frac1{\cosh x}\,dx \int\arcsin\frac1{\cosh x}dx=i\operatorname{Li}_2(i\phi)-i\operatorname{Li}_2(-i\phi)+C \phi=\tan\bigg(\frac12\arcsin\frac1{\cosh x}\bigg) \operatorname{Li}_2(z)=\sum_{n\geq1}\frac{z^n}{n^2}","['integration', 'analysis']"
89,Evaluating the following integral: $ \int \frac{x^2}{\sqrt{x^2 - 1}} \text{ d}x$,Evaluating the following integral:, \int \frac{x^2}{\sqrt{x^2 - 1}} \text{ d}x,"For this indefinite integral, I decided to use the substitution $x = \cosh u$ and I've ended up with a $| \sinh u |$ term in the denominator which I'm unsure about dealing with: $$\int \dfrac{x^2}{\sqrt{x^2 - 1}} \text{ d}x \ \overset{x = \cosh u}= \int \dfrac{\cosh^2 u \cdot \sinh u}{\left| \sinh u \right|} \text{ d}u$$ How would I deal with the denominator?","For this indefinite integral, I decided to use the substitution $x = \cosh u$ and I've ended up with a $| \sinh u |$ term in the denominator which I'm unsure about dealing with: $$\int \dfrac{x^2}{\sqrt{x^2 - 1}} \text{ d}x \ \overset{x = \cosh u}= \int \dfrac{\cosh^2 u \cdot \sinh u}{\left| \sinh u \right|} \text{ d}u$$ How would I deal with the denominator?",,"['integration', 'absolute-value', 'hyperbolic-functions']"
90,How to solve $\int \frac{x^4 + 1 }{x^6 + 1}$?,How to solve ?,\int \frac{x^4 + 1 }{x^6 + 1},"How to solve $\int \frac{x^4 + 1 }{x^6 + 1}$ ? The numerator is a irreducible polynomial so I can't use partial fractions. I tried the substitutions $t = x^2, t=x^4$ and for the formula $\int u\,dv = uv - \int v\,du$ I tried using: $u=\frac{x^4 + 1 }{x^6 + 1} , \,dv=\,dx \\ u=\frac{1}{x^6 + 1} , \,dv= (x^4 + 1) \,dx \\u=x^4 + 1 , \,dv=\frac{\,dx}{x^6 + 1}$ But I always get more complicated integrals. Any hints are appreciated!","How to solve $\int \frac{x^4 + 1 }{x^6 + 1}$ ? The numerator is a irreducible polynomial so I can't use partial fractions. I tried the substitutions $t = x^2, t=x^4$ and for the formula $\int u\,dv = uv - \int v\,du$ I tried using: $u=\frac{x^4 + 1 }{x^6 + 1} , \,dv=\,dx \\ u=\frac{1}{x^6 + 1} , \,dv= (x^4 + 1) \,dx \\u=x^4 + 1 , \,dv=\frac{\,dx}{x^6 + 1}$ But I always get more complicated integrals. Any hints are appreciated!",,"['real-analysis', 'integration', 'analysis', 'indefinite-integrals']"
91,Evaluating $ \int^{\infty}_0\frac{\ln x}{x^2+\pi^2} ~dx$,Evaluating, \int^{\infty}_0\frac{\ln x}{x^2+\pi^2} ~dx,"The question is Evaluate    $$ \int \limits^{\infty}_0\dfrac{\ln x}{x^2+\pi^2} dx $$ I have no idea what to do.Tried integration by parts, didn't work. Help would be appreciated. Thanks","The question is Evaluate    $$ \int \limits^{\infty}_0\dfrac{\ln x}{x^2+\pi^2} dx $$ I have no idea what to do.Tried integration by parts, didn't work. Help would be appreciated. Thanks",,"['integration', 'definite-integrals']"
92,A comprehensive book on graduate real analysis,A comprehensive book on graduate real analysis,,"I'm looking for a comprehensive book/a comprehensive list of books on graduate analysis that covers/cover these topics: Lebesgue measure and integration on $\mathbb{R}^d$ , the relationships between integrability and differentiability (it must also cover the theory of functions of bounded variation), complex analysis and fourier analysis.","I'm looking for a comprehensive book/a comprehensive list of books on graduate analysis that covers/cover these topics: Lebesgue measure and integration on , the relationships between integrability and differentiability (it must also cover the theory of functions of bounded variation), complex analysis and fourier analysis.",\mathbb{R}^d,"['real-analysis', 'integration', 'complex-analysis', 'fourier-analysis', 'real-numbers']"
93,"Find $\int_{0}^{\infty} \frac{\log(x) }{\sqrt{x} (x+1)^{2}}\,dx$",Find,"\int_{0}^{\infty} \frac{\log(x) }{\sqrt{x} (x+1)^{2}}\,dx","Need solve the next integral $$\int_{0}^{\infty} \frac{\log(x) }{\sqrt{x} (x+1)^{2}}\,dx$$ Tried something with Laurent’s series, but i can’t conclude anything. Thanks","Need solve the next integral Tried something with Laurent’s series, but i can’t conclude anything. Thanks","\int_{0}^{\infty} \frac{\log(x) }{\sqrt{x} (x+1)^{2}}\,dx","['integration', 'complex-analysis', 'improper-integrals']"
94,Compute $\lim\limits_{n\to \infty} \left(n^3 \int_{n} ^{2n}\frac{x dx} {1+x^5}\right) $,Compute,\lim\limits_{n\to \infty} \left(n^3 \int_{n} ^{2n}\frac{x dx} {1+x^5}\right) ,"Compute $\lim\limits_{n\to \infty} \left(n^3 \int_{n} ^{2n}\frac{x dx} {1+x^5}\right) $ . I tried to apply the first mean value theorem for  definite integrals and then apply the squeeze theorem, but it didn't work. The answer given by the book is $\frac{7}{24}$ ,but I can't see how to get to it.","Compute . I tried to apply the first mean value theorem for  definite integrals and then apply the squeeze theorem, but it didn't work. The answer given by the book is ,but I can't see how to get to it.",\lim\limits_{n\to \infty} \left(n^3 \int_{n} ^{2n}\frac{x dx} {1+x^5}\right)  \frac{7}{24},"['integration', 'limits', 'definite-integrals']"
95,Finding Limit of an Integral: $\lim_{n\to\infty}\int_a^b f(x)\sin^3{(nx)} \:dx$,Finding Limit of an Integral:,\lim_{n\to\infty}\int_a^b f(x)\sin^3{(nx)} \:dx,"Suppose $f:[a,b]\to\mathbb{R}$ is continuous. Determine if the following limit exists $$\lim_{n\to\infty}\int_a^b f(x)\sin^3{(nx)} \:dx.$$ As $f(x)$ and $\sin^3{(nx)}$ are continuous, so their product is Riemann integrable. However $\lim_{n\to\infty} f(x)\sin^3{(nx)} $ does not exist, so it's not uniformly convergence and we cannot pass the limit inside the integral. It also doesn't satisfy in the conditions of Dini Theorem. I don't know how to make a valid argument for this problem, but I think by what I said the limit doesn't exist. I appreciate any help.","Suppose is continuous. Determine if the following limit exists As and are continuous, so their product is Riemann integrable. However does not exist, so it's not uniformly convergence and we cannot pass the limit inside the integral. It also doesn't satisfy in the conditions of Dini Theorem. I don't know how to make a valid argument for this problem, but I think by what I said the limit doesn't exist. I appreciate any help.","f:[a,b]\to\mathbb{R} \lim_{n\to\infty}\int_a^b f(x)\sin^3{(nx)} \:dx. f(x) \sin^3{(nx)} \lim_{n\to\infty} f(x)\sin^3{(nx)} ","['integration', 'analysis', 'limits', 'continuity', 'uniform-convergence']"
96,Limit $\lim_{n\to \infty} \frac{1}{2n} \log{2n\choose n}$ [duplicate],Limit  [duplicate],\lim_{n\to \infty} \frac{1}{2n} \log{2n\choose n},"This question already has answers here : Evaluation of $\lim_{n\rightarrow \infty}\frac{1}{2n}\cdot \ln \binom{2n}{n}$ (2 answers) Closed 10 years ago . $\lim_{n\to \infty} \frac{1}{2n} \log{2n\choose n}$ I could not approach it beyond these simple steps, $\lim_{n\to \infty} \frac{1}{2n} \log(\frac{2n!}{(n!)^2})$ $=\lim_{n\to \infty} \frac{1}{2n} [\log(2n)+\cdots +\log(n+1)-\log(n)-\cdots-\log1]$ $=\lim_{n\to \infty} (\log(2n)^{1/2n}+\cdots+\log(n+1)^{1/2n}-\log(n)^{1/2n}-\cdots-\log1^{1/2n})$ Now,I understand that I have to create a sum of limit and produce an integration or use the formula $\lim_{n\to \infty} \log(1+\frac1x)^x=e$ but I cannot do it. Please help!","This question already has answers here : Evaluation of $\lim_{n\rightarrow \infty}\frac{1}{2n}\cdot \ln \binom{2n}{n}$ (2 answers) Closed 10 years ago . $\lim_{n\to \infty} \frac{1}{2n} \log{2n\choose n}$ I could not approach it beyond these simple steps, $\lim_{n\to \infty} \frac{1}{2n} \log(\frac{2n!}{(n!)^2})$ $=\lim_{n\to \infty} \frac{1}{2n} [\log(2n)+\cdots +\log(n+1)-\log(n)-\cdots-\log1]$ $=\lim_{n\to \infty} (\log(2n)^{1/2n}+\cdots+\log(n+1)^{1/2n}-\log(n)^{1/2n}-\cdots-\log1^{1/2n})$ Now,I understand that I have to create a sum of limit and produce an integration or use the formula $\lim_{n\to \infty} \log(1+\frac1x)^x=e$ but I cannot do it. Please help!",,"['real-analysis', 'integration', 'limits']"
97,Calculate $\int\left( \sqrt{\tan x}+\sqrt{\cot x}\right)dx$ [duplicate],Calculate  [duplicate],\int\left( \sqrt{\tan x}+\sqrt{\cot x}\right)dx,This question already has answers here : Which is the easiest way to evaluate $\int \limits_{0}^{\pi/2} (\sqrt{\tan x} +\sqrt{\cot x})$? (8 answers) Closed 10 years ago . How to calculate following integration? $$\int\left( \sqrt{\tan x}+\sqrt{\cot x}\right)dx$$,This question already has answers here : Which is the easiest way to evaluate $\int \limits_{0}^{\pi/2} (\sqrt{\tan x} +\sqrt{\cot x})$? (8 answers) Closed 10 years ago . How to calculate following integration? $$\int\left( \sqrt{\tan x}+\sqrt{\cot x}\right)dx$$,,"['integration', 'indefinite-integrals', 'trigonometric-integrals']"
98,An integral Lobachevsky calculated incorrectly $\int_{0}^{\infty}\frac{(e^x-e^{-x})x}{e^{2x}+e^{-2x}+2\cos(2a)}dx$,An integral Lobachevsky calculated incorrectly,\int_{0}^{\infty}\frac{(e^x-e^{-x})x}{e^{2x}+e^{-2x}+2\cos(2a)}dx,"In a recent lecture a professor told a story about the integral below. Lobachevsky calculated this integral at first time incorrectly. Following the publication of the integral, Ostrogradsky sent a letter with correct answer to Lobachevsky. What is the right answer? $$I(a)=\int_{0}^{\infty}\frac{(e^x-e^{-x})x}{e^{2x}+e^{-2x}+2\cos(2a)}dx$$ withe $0\leq a \leq \pi$ .","In a recent lecture a professor told a story about the integral below. Lobachevsky calculated this integral at first time incorrectly. Following the publication of the integral, Ostrogradsky sent a letter with correct answer to Lobachevsky. What is the right answer? withe .",I(a)=\int_{0}^{\infty}\frac{(e^x-e^{-x})x}{e^{2x}+e^{-2x}+2\cos(2a)}dx 0\leq a \leq \pi,['integration']
99,Integrate $\int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx.$,Integrate,\int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx.,"I am not able to solve the following integration, $$\int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx.$$ I have tried in different ways but most good one I think, \begin{align*} \int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx &= \int_{0}^{\pi} \frac{\sin x}{\sin x (1+3^{\cos x})} \\ &= \int_{-1}^{1} \frac{dz}{\sqrt{1-z^2} (1+3^{z})}.\end{align*} Now how could I proceed. Please help me.","I am not able to solve the following integration, I have tried in different ways but most good one I think, Now how could I proceed. Please help me.",\int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx. \begin{align*} \int_{0}^{\pi} \frac{1}{1+3^{\cos x}} dx &= \int_{0}^{\pi} \frac{\sin x}{\sin x (1+3^{\cos x})} \\ &= \int_{-1}^{1} \frac{dz}{\sqrt{1-z^2} (1+3^{z})}.\end{align*},"['integration', 'definite-integrals']"
