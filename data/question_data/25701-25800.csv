,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Proving T is self-adjoint iff $\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}$",Proving T is self-adjoint iff,"\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}","I'm struggling with proving $\left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle \iff \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}$ for all $x,y \in H$ where $H$ is a Hilbert space over $\Bbb{C}$ , and $T$ is a bounded linear operator from $H$ to $H$ . I've proven $\left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle \implies \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}$ as follows: $$\left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle$$ $$\left \langle Tx,y \right \rangle=\overline{\left \langle Ty,x \right \rangle}$$ So $Im(\left \langle Tx,y \right \rangle)=-Im(\left \langle Ty,x \right \rangle)$ , so the imaginary parts of $\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle$ cancel out and the result is a real number. For the other direction $\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R} \implies \left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle$ , I've gotten this far: $$\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}$$ $$\left \langle Tx,y \right \rangle + \overline{\left \langle x,Ty \right \rangle} \in \Bbb{R}$$ $$Im\left \langle Tx,y \right \rangle=-Im\overline{\left \langle x,Ty \right \rangle}$$ $$Im\left \langle Tx,y \right \rangle=Im{\left \langle x,Ty \right \rangle}$$ So I've proved $\left \langle Tx,y \right \rangle$ and $\left \langle x,Ty \right \rangle$ have the same imaginary parts but I'm stuck on proving they have the same real parts. I've been substituting the polarization identity and parallelogram law, equating the imaginary parts, etc, for a while, but haven't gotten anything useful. Any sort of push in the right direction would be hugely appreciated. Thank you :)","I'm struggling with proving for all where is a Hilbert space over , and is a bounded linear operator from to . I've proven as follows: So , so the imaginary parts of cancel out and the result is a real number. For the other direction , I've gotten this far: So I've proved and have the same imaginary parts but I'm stuck on proving they have the same real parts. I've been substituting the polarization identity and parallelogram law, equating the imaginary parts, etc, for a while, but haven't gotten anything useful. Any sort of push in the right direction would be hugely appreciated. Thank you :)","\left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle \iff \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R} x,y \in H H \Bbb{C} T H H \left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle \implies \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R} \left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle \left \langle Tx,y \right \rangle=\overline{\left \langle Ty,x \right \rangle} Im(\left \langle Tx,y \right \rangle)=-Im(\left \langle Ty,x \right \rangle) \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R} \implies \left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R} \left \langle Tx,y \right \rangle + \overline{\left \langle x,Ty \right \rangle} \in \Bbb{R} Im\left \langle Tx,y \right \rangle=-Im\overline{\left \langle x,Ty \right \rangle} Im\left \langle Tx,y \right \rangle=Im{\left \langle x,Ty \right \rangle} \left \langle Tx,y \right \rangle \left \langle x,Ty \right \rangle","['linear-algebra', 'hilbert-spaces', 'inner-products']"
1,Is there a space that does not include or need a coordinate system?,Is there a space that does not include or need a coordinate system?,,"A coordinate system (such as cartesian and polar coordinate systems) is expected in a Euclidean space. However, is there a space that does not include or need a coordinate system? I read that a Vector space does not need a coordinate system although a coordinate system can be defined if needed. Is there any other space that does not need/include a coordinate system? Is there any space that does not accept a coordinate system?","A coordinate system (such as cartesian and polar coordinate systems) is expected in a Euclidean space. However, is there a space that does not include or need a coordinate system? I read that a Vector space does not need a coordinate system although a coordinate system can be defined if needed. Is there any other space that does not need/include a coordinate system? Is there any space that does not accept a coordinate system?",,"['linear-algebra', 'vector-spaces', 'hilbert-spaces', 'banach-spaces', 'coordinate-systems']"
2,How do we know that nullspace and row space of a matrix are orthogonal complements?,How do we know that nullspace and row space of a matrix are orthogonal complements?,,"I'm following along Ch. 4 entitled ""Orthogonality"" in Gilbert Strang's Introduction to Linear Algebra . Here are some of the initial results in that chapter Definition: two subspaces of a vector space are orthogonal if every vector in the first subspace is perpendicular to every vector in the second subspace Every vector $x$ in the nullspace of a matrix $A$ is perpendicular to every row of $A$ . Similarly, every vector $y$ in the nullspace of $A^T$ is perpendicular to every column of $A$ . Definition: The orthogonal complement of a subspace $V$ contains every vector that is perpendicular to $V$ By this definition, the nullspace is the orthogonal complement of the row space. Every $x$ that is perpendicular to the rows satisfies $Ax=0$ . My question is about this last quote. How do we know that all vectors not in the nullspace are in the row space? Every vector $x$ that satisfies $Ax=0$ is in the nullspace of $A$ , and is perpendicular to every row of $A$ . Thus, $x$ is also perpendicular to linear combinations of rows of $A$ and is thus perpendicular to every vector in the row space. If $A$ is $m$ by $n$ with rank $r$ , the nullspace has dimension $n-r$ and the row space has dimension $r$ . Vectors in the nullspace and in the row space are in $\mathbb{R}^n$ . How do we know that not only are these two subspaces orthogonal, they are also orthogonal complements because there is no vector in $\mathbb{R}^n$ that is perpendicular to the vectors in the nullspace that is not in the row space, and there is no vector perpendicular to the vectors in the row space that is not in the null space?","I'm following along Ch. 4 entitled ""Orthogonality"" in Gilbert Strang's Introduction to Linear Algebra . Here are some of the initial results in that chapter Definition: two subspaces of a vector space are orthogonal if every vector in the first subspace is perpendicular to every vector in the second subspace Every vector in the nullspace of a matrix is perpendicular to every row of . Similarly, every vector in the nullspace of is perpendicular to every column of . Definition: The orthogonal complement of a subspace contains every vector that is perpendicular to By this definition, the nullspace is the orthogonal complement of the row space. Every that is perpendicular to the rows satisfies . My question is about this last quote. How do we know that all vectors not in the nullspace are in the row space? Every vector that satisfies is in the nullspace of , and is perpendicular to every row of . Thus, is also perpendicular to linear combinations of rows of and is thus perpendicular to every vector in the row space. If is by with rank , the nullspace has dimension and the row space has dimension . Vectors in the nullspace and in the row space are in . How do we know that not only are these two subspaces orthogonal, they are also orthogonal complements because there is no vector in that is perpendicular to the vectors in the nullspace that is not in the row space, and there is no vector perpendicular to the vectors in the row space that is not in the null space?",x A A y A^T A V V x Ax=0 x Ax=0 A A x A A m n r n-r r \mathbb{R}^n \mathbb{R}^n,"['linear-algebra', 'vector-spaces', 'orthogonality']"
3,How does changing a bilinear form change its associated orthogonal group?,How does changing a bilinear form change its associated orthogonal group?,,"I'm trying to understand bilinear forms and how they can be 'equivalent' and how this 'equivalence' translates to their associated orthogonal groups. Fix a field $F$ . I'm comfortable translating between a quadratic form $Q$ (homogenous degree $2$ polynomial in $\ell$ variables), its bilinear form on $F^\ell$ $B_Q$ , and the symmetric $\ell \times \ell$ matrix $A_Q$ . Each of these 'representations' has a group of matrices which preserve them, for example $SO(Q,F) := \{g \in SL_(\ell,F)  |  Q(gv) = Q(v) \; \forall v \in F^\ell\}$ . Each of these representations has an associated group of matrices preserving it and I can show these groups are literally equal. On the other hand, there are operations on the vector space that seem to change the bilinear form into things more pleasant to deal with ('diagonalizing the quadratic form') and some notation ( $SO(p,q)$ ) that seems to indicate that all of these are determined by the 'signature' of the quadratic form, etc. It seems like this should be explained by changing the underlying basis for the vector space, but I can't quite see it. Since my context is reading about these orthogonal groups as Lie groups (Morris Arithmetic Groups), I'd like to know how changing the form affects the group and why. It seems like it should be conjugation by a change of basis matrix, but again, I can't really explain why or find a reference. Explicitly, I'm looking for an explanation of how/when quadratic forms can be considered 'equivalent' (for example, why does the signature characterize them?), what effect that has on the orthogonal groups, and hopefully how it translates between the various representations listed above. I know that's a lot, so a reference would also be good.","I'm trying to understand bilinear forms and how they can be 'equivalent' and how this 'equivalence' translates to their associated orthogonal groups. Fix a field . I'm comfortable translating between a quadratic form (homogenous degree polynomial in variables), its bilinear form on , and the symmetric matrix . Each of these 'representations' has a group of matrices which preserve them, for example . Each of these representations has an associated group of matrices preserving it and I can show these groups are literally equal. On the other hand, there are operations on the vector space that seem to change the bilinear form into things more pleasant to deal with ('diagonalizing the quadratic form') and some notation ( ) that seems to indicate that all of these are determined by the 'signature' of the quadratic form, etc. It seems like this should be explained by changing the underlying basis for the vector space, but I can't quite see it. Since my context is reading about these orthogonal groups as Lie groups (Morris Arithmetic Groups), I'd like to know how changing the form affects the group and why. It seems like it should be conjugation by a change of basis matrix, but again, I can't really explain why or find a reference. Explicitly, I'm looking for an explanation of how/when quadratic forms can be considered 'equivalent' (for example, why does the signature characterize them?), what effect that has on the orthogonal groups, and hopefully how it translates between the various representations listed above. I know that's a lot, so a reference would also be good.","F Q 2 \ell F^\ell B_Q \ell \times \ell A_Q SO(Q,F) := \{g \in SL_(\ell,F)  |  Q(gv) = Q(v) \; \forall v \in F^\ell\} SO(p,q)","['linear-algebra', 'reference-request', 'lie-groups', 'quadratic-forms', 'bilinear-form']"
4,Prove that the product $\prod_{i=1}^n\left(a_i+b_j\right)$ is also constant for all $j$.,Prove that the product  is also constant for all .,\prod_{i=1}^n\left(a_i+b_j\right) j,"Let, $a_i, i=1,2, \ldots, n$ be distinct real numbers $b_1 b_2, \ldots, b_n$ be real numbers, such that the product $\prod_{j=1}^n\left(a_i+b_j\right)$ is the same for each $i$ . Prove that the product $\prod_{i=1}^n\left(a_i+b_j\right)$ is also constant for all $j$ . I am getting some issues here. It's given $a_i$ are distinct, so WLOG we can assume $a_1 <a_2<a_3< \dots <a_n$ and that would make $(a_1+b_1) < (a_2+b_1)$ and so on for all $a_i$ , but as we are given in the problem, $\prod_{j=1}^n\left(a_i+b_j\right)$ is same for all $i$ , we get $(a_1+b_1)(a_1+b_2) \dots (a_1+b_n)= (a_2+b_1)(a_2+b_2) \dots (a_2+b_n)$ . It's contradicting. Where am I going wrong?","Let, be distinct real numbers be real numbers, such that the product is the same for each . Prove that the product is also constant for all . I am getting some issues here. It's given are distinct, so WLOG we can assume and that would make and so on for all , but as we are given in the problem, is same for all , we get . It's contradicting. Where am I going wrong?","a_i, i=1,2, \ldots, n b_1 b_2, \ldots, b_n \prod_{j=1}^n\left(a_i+b_j\right) i \prod_{i=1}^n\left(a_i+b_j\right) j a_i a_1 <a_2<a_3< \dots <a_n (a_1+b_1) < (a_2+b_1) a_i \prod_{j=1}^n\left(a_i+b_j\right) i (a_1+b_1)(a_1+b_2) \dots (a_1+b_n)= (a_2+b_1)(a_2+b_2) \dots (a_2+b_n)","['linear-algebra', 'products']"
5,Does the Nullity Theorem hold in fields of characteristic 2?,Does the Nullity Theorem hold in fields of characteristic 2?,,"I'm playing around with involutory ( $M^2 = I$ ) matrices over finite fields with characteristic 2 ( $\mathbb{F}_{2^m}$ ). I came across the nullity theorem , which seems very useful to check if submatrices are (non)singular. It basically states (as far as I understand it), that the nullity of a submatrix in $M$ is equal to the complementary submatrix of the inverse ( $M^{-1}$ ). Out of caution I routinely check my assumptions and try to find counterexamples. I checked the 4x4 matrices for $m=6$ and found one which seems to contradict the theorem: (sagemath code) G=GF(2**6, repr='int') a=G(58.bits()) b=G(59.bits()) c=G(62.bits()) d=G(63.bits()) M=matrix(4,4,[b,a,d,c,d,c,a,d,d,d,c,a,d,d,d,b]) print(M) # [59 58 63 62] # [63 62 58 63] # [63 63 62 58] # [63 63 63 59]  print(M*M) # [1 0 0 0] # [0 1 0 0] # [0 0 1 0] # [0 0 0 1]  import itertools for d in [2,3]:     for p1 in itertools.combinations(range(4), d):         for p2 in itertools.combinations(range(4), d):             S=M.matrix_from_rows_and_columns(p1, p2)             if S.determinant() == 0:                 print('det', p1, p2)             if S.nullity() != 0:                 print('nul', p1, p2) # det (2, 3) (0, 1) # nul (2, 3) (0, 1)  print(M.matrix_from_rows_and_columns((2,3), (0,1))) # [63 63] # [63 63] As you can see, the matrix $M$ is clearly involutory, so $M = M^{-1}$ , but it also has one obviously singular submatrix. In addition, it ONLY contains this one singular and nullity non-zero submatrix! According to the nullity theorem, I would expect a second singular submatrix, as there should be a complementary submatrix in the inverse of $M$ . I checked various sources, incl. the original papers, but found no obvious reason, the theorem should not work over special fields. So my questions are: Is there a problem/error in the nullity theorem? If not, what is the reason it does not work over $\mathbb{F}_{2^m}$ ? Is there something (e.g. additional constraints) to make it work over $\mathbb{F}_{2^m}$ ?","I'm playing around with involutory ( ) matrices over finite fields with characteristic 2 ( ). I came across the nullity theorem , which seems very useful to check if submatrices are (non)singular. It basically states (as far as I understand it), that the nullity of a submatrix in is equal to the complementary submatrix of the inverse ( ). Out of caution I routinely check my assumptions and try to find counterexamples. I checked the 4x4 matrices for and found one which seems to contradict the theorem: (sagemath code) G=GF(2**6, repr='int') a=G(58.bits()) b=G(59.bits()) c=G(62.bits()) d=G(63.bits()) M=matrix(4,4,[b,a,d,c,d,c,a,d,d,d,c,a,d,d,d,b]) print(M) # [59 58 63 62] # [63 62 58 63] # [63 63 62 58] # [63 63 63 59]  print(M*M) # [1 0 0 0] # [0 1 0 0] # [0 0 1 0] # [0 0 0 1]  import itertools for d in [2,3]:     for p1 in itertools.combinations(range(4), d):         for p2 in itertools.combinations(range(4), d):             S=M.matrix_from_rows_and_columns(p1, p2)             if S.determinant() == 0:                 print('det', p1, p2)             if S.nullity() != 0:                 print('nul', p1, p2) # det (2, 3) (0, 1) # nul (2, 3) (0, 1)  print(M.matrix_from_rows_and_columns((2,3), (0,1))) # [63 63] # [63 63] As you can see, the matrix is clearly involutory, so , but it also has one obviously singular submatrix. In addition, it ONLY contains this one singular and nullity non-zero submatrix! According to the nullity theorem, I would expect a second singular submatrix, as there should be a complementary submatrix in the inverse of . I checked various sources, incl. the original papers, but found no obvious reason, the theorem should not work over special fields. So my questions are: Is there a problem/error in the nullity theorem? If not, what is the reason it does not work over ? Is there something (e.g. additional constraints) to make it work over ?",M^2 = I \mathbb{F}_{2^m} M M^{-1} m=6 M M = M^{-1} M \mathbb{F}_{2^m} \mathbb{F}_{2^m},"['linear-algebra', 'matrices', 'determinant', 'involutions']"
6,Question about basis transform and its direction,Question about basis transform and its direction,,"This question has been bothering me for a while. It's in connection with diagonalisation: I can never remember, whether it's $A = PDP^{-1}$ or $A = P^{-1}AP$ . I have an explanation but it's wrong because it leads to the wrong formula but I don't see the flaw. Can anyone please point out the mistake in the following argument? Let $A$ be some finite-dimensional linear map given as a matrix. Say, it is, as usually is the case, given with respect to the standard basis. Let's call this basis Basis 1. Let's assume it has as many different eigenvalues as its dimension $n$ so we can find $n$ distinct and linearly independent eigen vectors. They form a basis for the eigen space. Let's call this basis Basis 2. So far so good. Now, $P$ is the matrix containing these eigen vectors. Also, $P$ is a matrix that represents a change of basis: If we apply $P$ to the first standard basis vector $(1,0,0,\dots, 0)^T$ then we get the first column of $P$ which is the first eigen vector. So we have $$ P(e_i) = v_i$$ where $e_i$ are the basis vectors of Basis 1 and $v_i$ are the basis vectors of Basis 2. Therefore, $P$ transforms Basis 1 into Basis 2. Now to get the same linear map $A$ but with respect to Basis 2 I want to take a vector with respect to Basis 1 transform it into Basis 2 apply $D$ (i.e. the map $A$ but expressed w.r.t. Basis 2) transform the result from Basis 2 back into Basis 1 Therefore, expressing steps 1)-4), we should have $$ A = P^{-1}D P$$ But this is wrong: the correct formula is the other way around. So where is my mistake?","This question has been bothering me for a while. It's in connection with diagonalisation: I can never remember, whether it's or . I have an explanation but it's wrong because it leads to the wrong formula but I don't see the flaw. Can anyone please point out the mistake in the following argument? Let be some finite-dimensional linear map given as a matrix. Say, it is, as usually is the case, given with respect to the standard basis. Let's call this basis Basis 1. Let's assume it has as many different eigenvalues as its dimension so we can find distinct and linearly independent eigen vectors. They form a basis for the eigen space. Let's call this basis Basis 2. So far so good. Now, is the matrix containing these eigen vectors. Also, is a matrix that represents a change of basis: If we apply to the first standard basis vector then we get the first column of which is the first eigen vector. So we have where are the basis vectors of Basis 1 and are the basis vectors of Basis 2. Therefore, transforms Basis 1 into Basis 2. Now to get the same linear map but with respect to Basis 2 I want to take a vector with respect to Basis 1 transform it into Basis 2 apply (i.e. the map but expressed w.r.t. Basis 2) transform the result from Basis 2 back into Basis 1 Therefore, expressing steps 1)-4), we should have But this is wrong: the correct formula is the other way around. So where is my mistake?","A = PDP^{-1} A = P^{-1}AP A n n P P P (1,0,0,\dots, 0)^T P  P(e_i) = v_i e_i v_i P A D A  A = P^{-1}D P","['linear-algebra', 'linear-transformations']"
7,Sum of the components of a vector [duplicate],Sum of the components of a vector [duplicate],,"This question already has answers here : Every combination of $v = (1, -2, 1)$ and $w = (0, 1, -1)$ has components that add to ________. (2 answers) Closed 2 years ago . Introduction to Linear Algebra (Strang 5th ed) problem 6 page 8. What does it mean when components of a vector add to zero? The solution from the instructor's manual doesn't give additional clues, and maybe it doesn't mean more than that? Does someone have an insight on the sum of components of vectors? The problem set: The instructor's manual answer:","This question already has answers here : Every combination of $v = (1, -2, 1)$ and $w = (0, 1, -1)$ has components that add to ________. (2 answers) Closed 2 years ago . Introduction to Linear Algebra (Strang 5th ed) problem 6 page 8. What does it mean when components of a vector add to zero? The solution from the instructor's manual doesn't give additional clues, and maybe it doesn't mean more than that? Does someone have an insight on the sum of components of vectors? The problem set: The instructor's manual answer:",,"['linear-algebra', 'vectors']"
8,Matrix congruences,Matrix congruences,,"If A, B are two integer square matrices of the same size such that $A\equiv B\pmod n$ , is $A^p\equiv B^p \pmod{pn} $ for a prime p dividing n?","If A, B are two integer square matrices of the same size such that , is for a prime p dividing n?",A\equiv B\pmod n A^p\equiv B^p \pmod{pn} ,"['linear-algebra', 'p-adic-number-theory', 'matrix-congruences']"
9,"Finding a basis for the kernel of the following linear transformation: $T(p(x))=p'(x)$,$T:P_3[\mathbb{R}]\rightarrow P_3[\mathbb{R}]$","Finding a basis for the kernel of the following linear transformation: ,",T(p(x))=p'(x) T:P_3[\mathbb{R}]\rightarrow P_3[\mathbb{R}],"By given the following linear transformation: $T(p(x))=p'(x)$ , $T:P_3[\mathbb{R}]\rightarrow P_3[\mathbb{R}]$ , find a basis and the dimension of the kernel. $Solution.$ \begin{align*} \ker T & =\left\{p( x) \in P_{3}[\mathbb{R}]\Bigl| p'( x) =0\right\} \\ &=\left\{ax^{3} +bx^{2} +cx+d\Bigl| 3ax^{2} +2bx+c=0,a,b,c,d\in \mathbb{R}\right\}\\ & =\left\{ax^{3} +bx^{2} +cx+d\Bigl| c=-3ax^{2} -2bx,a,b,c,d\in \mathbb{R}\right\}\\ & =\left\{ax^{3} +bx^{2} +\left( -3ax^{2} -2bx\right) x+d\Bigl| a,b,d\in \mathbb{R}\right\}\\ & =\left\{a\left( x^{3} -2x^{2}\right) +b\left( x^{2} -2x\right) +d\Bigl| a,b,d\in \mathbb{R}\right\}\\ & =\operatorname{Span}\left\{x^{3} -2x^{2} ,x^{2} -2x,1\right\}\end{align*} in addition, the isomorphic vectors to the polynomials are linearly independent, can be easily checked and easy to see, so they are a basis for the kernel. Thus, $$B_{\ker T} =\left\{x^{3} -2x^{2} ,x^{2} -2x,1\right\} \Longrightarrow \dim\ker T=3$$ However, it is clear that this isn't true since the only polynomial who gives $p'(x)=0$ is $p(x)=d$ , so the basis is actually: $$B_{\ker T} =\left\{1\right\} \Longrightarrow \dim\ker T=1$$ and it's easy to see. Perhaps I'm finding here a basis for the $x$ 's? because I can't find why I get the wrong polynomials in the basis.","By given the following linear transformation: , , find a basis and the dimension of the kernel. in addition, the isomorphic vectors to the polynomials are linearly independent, can be easily checked and easy to see, so they are a basis for the kernel. Thus, However, it is clear that this isn't true since the only polynomial who gives is , so the basis is actually: and it's easy to see. Perhaps I'm finding here a basis for the 's? because I can't find why I get the wrong polynomials in the basis.","T(p(x))=p'(x) T:P_3[\mathbb{R}]\rightarrow P_3[\mathbb{R}] Solution. \begin{align*}
\ker T & =\left\{p( x) \in P_{3}[\mathbb{R}]\Bigl| p'( x) =0\right\} \\ &=\left\{ax^{3} +bx^{2} +cx+d\Bigl| 3ax^{2} +2bx+c=0,a,b,c,d\in \mathbb{R}\right\}\\
& =\left\{ax^{3} +bx^{2} +cx+d\Bigl| c=-3ax^{2} -2bx,a,b,c,d\in \mathbb{R}\right\}\\
& =\left\{ax^{3} +bx^{2} +\left( -3ax^{2} -2bx\right) x+d\Bigl| a,b,d\in \mathbb{R}\right\}\\
& =\left\{a\left( x^{3} -2x^{2}\right) +b\left( x^{2} -2x\right) +d\Bigl| a,b,d\in \mathbb{R}\right\}\\
& =\operatorname{Span}\left\{x^{3} -2x^{2} ,x^{2} -2x,1\right\}\end{align*} B_{\ker T} =\left\{x^{3} -2x^{2} ,x^{2} -2x,1\right\} \Longrightarrow \dim\ker T=3 p'(x)=0 p(x)=d B_{\ker T} =\left\{1\right\} \Longrightarrow \dim\ker T=1 x","['linear-algebra', 'linear-transformations']"
10,Why do we usually not need to find the eigenvalues of non-symmetric matrix,Why do we usually not need to find the eigenvalues of non-symmetric matrix,,"I came across this line in a class note I am reading: In numerical linear algebra, we usually don't need to find the eigenvalues of a non-symmetric matrix. Can someone explain why this is the case? I understand for symmetric matrix, there are many nice properties of eigenvalues. For example the eigenvalues of a real symmetric matrix are real. SVD comes from the eigenvalues of $A^TA$ which is symmetric, etc. But why are we so confident that we usually don't need to find the eigenvalues of non-symmetric matrix? Is it purely because of the nice properties of symmetric matrix that make us tend to formulate our problems that way?","I came across this line in a class note I am reading: In numerical linear algebra, we usually don't need to find the eigenvalues of a non-symmetric matrix. Can someone explain why this is the case? I understand for symmetric matrix, there are many nice properties of eigenvalues. For example the eigenvalues of a real symmetric matrix are real. SVD comes from the eigenvalues of which is symmetric, etc. But why are we so confident that we usually don't need to find the eigenvalues of non-symmetric matrix? Is it purely because of the nice properties of symmetric matrix that make us tend to formulate our problems that way?",A^TA,"['linear-algebra', 'matrices', 'numerical-methods', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
11,Condition on endomorphisms satisfying $\operatorname{Ker} f=\operatorname{Im} f$,Condition on endomorphisms satisfying,\operatorname{Ker} f=\operatorname{Im} f,"Let $E$ a finite dimensional vector space and $f \in \mathcal{L}(E)$ . Show that: $$\operatorname{Ker} f=\operatorname{Im} f \Leftrightarrow (f^2=0) \wedge (\exists h \in \mathcal{L}(E) \mid h \circ f + f \circ h=Id_E)$$ My attempt: I managed to show the $\Leftarrow$ implication. For $x \in \operatorname{Im} f$ , $x=f(y)$ , then $x \in \operatorname{Ker} f$ because $f(x)=f^2(y)=0$ so $\operatorname{Im} f \subset \operatorname{Ker} f$ . For $x \in \operatorname{Ker} f$ , we have $h \circ f(x) + f \circ h(x)=f \circ h(x)=x$ which shows that $x \in \operatorname{Im} f$ . So $\operatorname{Ker} f \subset \operatorname{Im} f \Rightarrow \operatorname{Ker} f = \operatorname{Im} f$ . Now, assuming $\operatorname{Ker} f = \operatorname{Im} f$ , for any $x$ , we have $f^2(x)=f(f(x))=0$ , hence $f^2=0$ . Yet, I can’t figure out the other part of the $\Rightarrow$ implication. I guess we would need to construct explicitly a function $h$ satisfying the equality. Generally in these kind of problems, a trick is to consider the decomposition $E=\operatorname{Ker} f + \operatorname{Im} f$ but it is here clearly not possible. Could anyone give me a hint on how to proceed?","Let a finite dimensional vector space and . Show that: My attempt: I managed to show the implication. For , , then because so . For , we have which shows that . So . Now, assuming , for any , we have , hence . Yet, I can’t figure out the other part of the implication. I guess we would need to construct explicitly a function satisfying the equality. Generally in these kind of problems, a trick is to consider the decomposition but it is here clearly not possible. Could anyone give me a hint on how to proceed?",E f \in \mathcal{L}(E) \operatorname{Ker} f=\operatorname{Im} f \Leftrightarrow (f^2=0) \wedge (\exists h \in \mathcal{L}(E) \mid h \circ f + f \circ h=Id_E) \Leftarrow x \in \operatorname{Im} f x=f(y) x \in \operatorname{Ker} f f(x)=f^2(y)=0 \operatorname{Im} f \subset \operatorname{Ker} f x \in \operatorname{Ker} f h \circ f(x) + f \circ h(x)=f \circ h(x)=x x \in \operatorname{Im} f \operatorname{Ker} f \subset \operatorname{Im} f \Rightarrow \operatorname{Ker} f = \operatorname{Im} f \operatorname{Ker} f = \operatorname{Im} f x f^2(x)=f(f(x))=0 f^2=0 \Rightarrow h E=\operatorname{Ker} f + \operatorname{Im} f,['linear-algebra']
12,Computation of Cholesky decomposition of Gram matrix from its components,Computation of Cholesky decomposition of Gram matrix from its components,,"Let's assume I have a tall matrix $\mathbf{X} \in \mathbb{C}^{m\times n}$ , where $m \gg n$ . I form the Gram matrix $\mathbf{A} = \mathbf{X}^*\mathbf{X}$ , where $\mathbf{A} \in \mathbb{C}^{n\times n}$ is Hermitian. As $\mathbf{A}$ is Hermitian, there exists a lower triangular matrix $\mathbf{L} \in \mathbb{C}^{n\times n}$ such that $\mathbf{A} = \mathbf{L}\mathbf{L}^*$ (where $\mathbf{L}$ is the Cholesky factor of $\mathbf{A}$ ). Is there a way to compute $\mathbf{L}$ without forming $\mathbf{A}$ first? I saw some things based on Lyapunov equation solvers, but I can't figure how this could help here.","Let's assume I have a tall matrix , where . I form the Gram matrix , where is Hermitian. As is Hermitian, there exists a lower triangular matrix such that (where is the Cholesky factor of ). Is there a way to compute without forming first? I saw some things based on Lyapunov equation solvers, but I can't figure how this could help here.",\mathbf{X} \in \mathbb{C}^{m\times n} m \gg n \mathbf{A} = \mathbf{X}^*\mathbf{X} \mathbf{A} \in \mathbb{C}^{n\times n} \mathbf{A} \mathbf{L} \in \mathbb{C}^{n\times n} \mathbf{A} = \mathbf{L}\mathbf{L}^* \mathbf{L} \mathbf{A} \mathbf{L} \mathbf{A},"['linear-algebra', 'matrices', 'matrix-decomposition', 'cholesky-decomposition']"
13,Show $ \begin{bmatrix} A & BC(A+BC) \\ I_n & 0 \end{bmatrix} $ and $\begin{bmatrix} A+BC & 0 \\ 0 & -CB \end{bmatrix}$ have same nonzero eigenvalues,Show  and  have same nonzero eigenvalues, \begin{bmatrix} A & BC(A+BC) \\ I_n & 0 \end{bmatrix}  \begin{bmatrix} A+BC & 0 \\ 0 & -CB \end{bmatrix},"Let $A\in\mathbb{R}^{n\times n}$ , $B\in\mathbb{R}^{n\times m}$ , and $C\in\mathbb{R}^{m\times n}$ .  Define \begin{equation} D_1=\begin{bmatrix} A & BC(A+BC) \\  I_n & 0 \end{bmatrix},\qquad D_2=\begin{bmatrix} A+BC & 0 \\ 0 & -CB \end{bmatrix} \end{equation} Show that $D_1$ and $D_2$ have the same nonzero eigenvalues. My attemp: If we check the claim numerically, we see that the nonzero eigenvalues of $D_1$ and $D_2$ are the same. Probably, there is a factorization of $D_1$ , for example $D_1=EF$ such that $D_2=FE$ .","Let , , and .  Define Show that and have the same nonzero eigenvalues. My attemp: If we check the claim numerically, we see that the nonzero eigenvalues of and are the same. Probably, there is a factorization of , for example such that .","A\in\mathbb{R}^{n\times n} B\in\mathbb{R}^{n\times m} C\in\mathbb{R}^{m\times n} \begin{equation}
D_1=\begin{bmatrix}
A & BC(A+BC) \\
 I_n & 0
\end{bmatrix},\qquad D_2=\begin{bmatrix} A+BC & 0 \\ 0 & -CB \end{bmatrix}
\end{equation} D_1 D_2 D_1 D_2 D_1 D_1=EF D_2=FE","['linear-algebra', 'matrices']"
14,Matrix exponential for non-commutative operator entries of matrix,Matrix exponential for non-commutative operator entries of matrix,,"I would like to find the matrix exponential $e^{iHt}$ of the Hermitian matrix $H$ where $$ H=\begin{pmatrix} \delta& \sqrt{2}a & 0\\ \sqrt{2}a^\dagger &0& \sqrt{2}a\\ 0 &\sqrt{2}a^\dagger&-\delta \end{pmatrix} $$ $$[a,a^\dagger]=a a^\dagger-a^\dagger a=1$$ $$[a,\delta]=[a^\dagger,\delta]=0.$$ The dagger $\dagger$ denotes complex conjugate. My thought is to make use of the definition of matrix exponential $e^{X}=\sum_{k=0}^{\infty} \frac{1}{k !} X^{k}$ and calculate $H^2$ and $H^3$ to find some pattern to exploit. For example, in the special case where $\delta=0$ , we can find a diagonal matrix $D$ $$ D=\begin{pmatrix} 4a^\dagger a+6 &0&0\\ 0&4a^\dagger a+2&0\\ 0&0&4a^\dagger a-2 \end{pmatrix} $$ such that $H^{2n+1}=D^{n}H$ and $H^{2n+2}=D^{n}H^2$ , with which we can obtain a neat result of $e^{i H t}$ . However, it seems that no such diagonal matrix exists for $\delta \neq 0$ case. Are there any other ways to calculating matrix exponential like this with non-commuting entries? Any input is much appreciated!","I would like to find the matrix exponential of the Hermitian matrix where The dagger denotes complex conjugate. My thought is to make use of the definition of matrix exponential and calculate and to find some pattern to exploit. For example, in the special case where , we can find a diagonal matrix such that and , with which we can obtain a neat result of . However, it seems that no such diagonal matrix exists for case. Are there any other ways to calculating matrix exponential like this with non-commuting entries? Any input is much appreciated!","e^{iHt} H 
H=\begin{pmatrix}
\delta& \sqrt{2}a & 0\\
\sqrt{2}a^\dagger &0& \sqrt{2}a\\
0 &\sqrt{2}a^\dagger&-\delta
\end{pmatrix}
 [a,a^\dagger]=a a^\dagger-a^\dagger a=1 [a,\delta]=[a^\dagger,\delta]=0. \dagger e^{X}=\sum_{k=0}^{\infty} \frac{1}{k !} X^{k} H^2 H^3 \delta=0 D 
D=\begin{pmatrix}
4a^\dagger a+6 &0&0\\
0&4a^\dagger a+2&0\\
0&0&4a^\dagger a-2
\end{pmatrix}
 H^{2n+1}=D^{n}H H^{2n+2}=D^{n}H^2 e^{i H t} \delta \neq 0","['linear-algebra', 'tensor-products', 'noncommutative-algebra', 'quantum-mechanics']"
15,what value of K does the system have a unique solution,what value of K does the system have a unique solution,,$\begin{cases}x_1 + kx_2 - x_3 = 2\\2x_1 - x_2 + kx_3 = 5\\x_1 + 10x_2 -6x_3= 1\\ \end{cases}$ I've been trying echelon form where i took $R_2 = R_2 - 2R_1$ and $R_3 = R_3-R_1$ So I have $\left[\begin{array}{ccc|c}1&K&-1&2\\2&-2&K&5\\1&10&-6&1\end{array}\right]$ I've been trying echelon form where i took $R_2 = R_2 - 2R_1$ and $R_3 = R_3-R_1$ and reduced it So I have $\left[\begin{array}{ccc|c}1&K&-1&2\\0&-1-2K&K+2&1\\0&10-K&-5&-1\end{array}\right]$ But now I am not sure how i could remove $10-K$ with $-1-2K$ any help would be appreciated,I've been trying echelon form where i took and So I have I've been trying echelon form where i took and and reduced it So I have But now I am not sure how i could remove with any help would be appreciated,"\begin{cases}x_1 + kx_2 - x_3 = 2\\2x_1 - x_2 + kx_3 = 5\\x_1 + 10x_2 -6x_3= 1\\
\end{cases} R_2 = R_2 - 2R_1 R_3 = R_3-R_1 \left[\begin{array}{ccc|c}1&K&-1&2\\2&-2&K&5\\1&10&-6&1\end{array}\right] R_2 = R_2 - 2R_1 R_3 = R_3-R_1 \left[\begin{array}{ccc|c}1&K&-1&2\\0&-1-2K&K+2&1\\0&10-K&-5&-1\end{array}\right] 10-K -1-2K",['linear-algebra']
16,Find the vector given the dot product and cross product of a set of vectors.,Find the vector given the dot product and cross product of a set of vectors.,,"This question comes from Ted Shifrin's Multivariable Mathematics . The question states: Given the nonzero vector $\textbf{a} \in \mathbb{R}^{3}$ , $\textbf{a} \cdot \textbf{x} = b \in \mathbb{R}$ , and $\textbf{a} \times \textbf{x} = \textbf{c} \in \mathbb{R}^{3}$ , can you determine the vector $\textbf{x} \in \mathbb{R}^{3}$ ? If so give a geometric construction of $\textbf{x}$ . I'm trying to visualize what the vector $\textbf{x}$ 's would look like in this scenario, but I think I'm getting confused by everything happening. So $\textbf{a} \cdot \textbf{x} = b$ is giving me the equation of a plane, in particular an affine plane from the one at the origin. As well $\textbf{a} \times \textbf{x} = \textbf{c}$ is giving me vector orthogonal to $\textbf{a}$ and $\textbf{x}$ respectively. The norm of this cross product also gives me the area of the parallelogram spanned by $\textbf{a}$ and $\textbf{x}$ . Even with all these properties I'm still having trouble figuring out how to solve for $\textbf{x}$ . Would I be able to get some assistance? If I'm lucky maybe the man himself actually might pop in to provide guidance.","This question comes from Ted Shifrin's Multivariable Mathematics . The question states: Given the nonzero vector , , and , can you determine the vector ? If so give a geometric construction of . I'm trying to visualize what the vector 's would look like in this scenario, but I think I'm getting confused by everything happening. So is giving me the equation of a plane, in particular an affine plane from the one at the origin. As well is giving me vector orthogonal to and respectively. The norm of this cross product also gives me the area of the parallelogram spanned by and . Even with all these properties I'm still having trouble figuring out how to solve for . Would I be able to get some assistance? If I'm lucky maybe the man himself actually might pop in to provide guidance.",\textbf{a} \in \mathbb{R}^{3} \textbf{a} \cdot \textbf{x} = b \in \mathbb{R} \textbf{a} \times \textbf{x} = \textbf{c} \in \mathbb{R}^{3} \textbf{x} \in \mathbb{R}^{3} \textbf{x} \textbf{x} \textbf{a} \cdot \textbf{x} = b \textbf{a} \times \textbf{x} = \textbf{c} \textbf{a} \textbf{x} \textbf{a} \textbf{x} \textbf{x},"['real-analysis', 'linear-algebra', 'multivariable-calculus', 'determinant', 'inner-products']"
17,Categorical description of matrix similarity,Categorical description of matrix similarity,,"I am wondering if there is a nice categorical description of matrix similarity, i.e. the equivalence relation on matrices given by $A\sim B \iff A=QBQ^{-1}$ , for some invertible $Q$ . In particular, I am considering the matrices as linear maps from a vector space into itself, which in turn forms a category $\mathrm{End}(V)$ with one object $V$ and arrows linear maps. We can then form the functor category $\mathrm{End}(V)^{\mathbb2},$ where $\mathbb2$ is the category with $2$ objects and $1$ arrow between them, i.e. this is the 2-category of linear maps and pairs of maps between them making the following commute: \begin{array}{ccccccccc} V & \xrightarrow{A} & V \\ \downarrow &   &  \downarrow \\ V & \xrightarrow{B} & V \end{array} So similarity between $A$ and $B$ is stronger than isomorphism in this category, as $(C,D):A\rightarrow B$ is an isomorphism if and only if $C$ and $D$ are linear isomorphisms, whereas similarity would require $C=D$ . Do similar matrices in this category obey some characterising or otherwise interesting property, or is isomorphism the strongest we can get?","I am wondering if there is a nice categorical description of matrix similarity, i.e. the equivalence relation on matrices given by , for some invertible . In particular, I am considering the matrices as linear maps from a vector space into itself, which in turn forms a category with one object and arrows linear maps. We can then form the functor category where is the category with objects and arrow between them, i.e. this is the 2-category of linear maps and pairs of maps between them making the following commute: So similarity between and is stronger than isomorphism in this category, as is an isomorphism if and only if and are linear isomorphisms, whereas similarity would require . Do similar matrices in this category obey some characterising or otherwise interesting property, or is isomorphism the strongest we can get?","A\sim B \iff A=QBQ^{-1} Q \mathrm{End}(V) V \mathrm{End}(V)^{\mathbb2}, \mathbb2 2 1 \begin{array}{ccccccccc} V & \xrightarrow{A} & V \\
\downarrow &   &  \downarrow \\
V & \xrightarrow{B} & V \end{array} A B (C,D):A\rightarrow B C D C=D","['linear-algebra', 'category-theory', 'universal-property']"
18,Why does the Minus-1 trick to get particular and general solutions work?,Why does the Minus-1 trick to get particular and general solutions work?,,"Minus-1 trick: A practical trick where the reduced row-echelon form of a system of equations is augmented with -1 rows. It is used to get particular and general solutions. While it is a handy trick, I was wondering why does it work. The following is an example of the minus-1 trick from the book Mathematics for Machine Learning Book","Minus-1 trick: A practical trick where the reduced row-echelon form of a system of equations is augmented with -1 rows. It is used to get particular and general solutions. While it is a handy trick, I was wondering why does it work. The following is an example of the minus-1 trick from the book Mathematics for Machine Learning Book",,"['linear-algebra', 'matrices']"
19,Generalising norms over arbitrary fields,Generalising norms over arbitrary fields,,"I am curious about the structure we require on arbitrary fields if we want to generalise the notion of norms on vector spaces over fields other than either $\mathbb{R}$ or $\mathbb{C}$ . Specifcally, given a vector space $V$ over $\mathbb{F}$ , what structure must be imposed on $\mathbb{F}$ , such that we have a map $\|\cdot\|: V \to \mathbb{F}$ s.t. for any $a \in \mathbb{F}$ and any $\bar u, \bar v\in V$ , $$\text{(i) }\|\bar u+ \bar v\| \leq \|\bar u\|+\|\bar v\|$$ $$\text{(ii) }\|\bar v\| = e \to \bar v = \boldsymbol{0}, \text{where } e \text{ is the additive identity of } \mathbb{F}$$ $$\|a\bar u\| = |a|\|\bar u\|$$ From this question here , it seems that positive definiteness requires that we have an ordered field. The accepted answer implies this is enough structure to generalise the inner product. Where I am getting somewhat confused is how the generalisation works for norms. Specifically, is it really enough to have just have an ordered field in order to satisfy the norm property of $\|a\bar x\| = |a|\|\bar x\|$ ? I know that having an ordered field allows us to generalise (i) and (ii), but  how do we define what the modulus of $a$ is? I think the linked question is relevant since if we generalise the inner product over an arbitrary field, and we get a norm out of those inner products, we have made some progress. How does ordering the field allow us to define the modulus of scalars in a vector space over an ordered field? Do we need to specify additional structure?","I am curious about the structure we require on arbitrary fields if we want to generalise the notion of norms on vector spaces over fields other than either or . Specifcally, given a vector space over , what structure must be imposed on , such that we have a map s.t. for any and any , From this question here , it seems that positive definiteness requires that we have an ordered field. The accepted answer implies this is enough structure to generalise the inner product. Where I am getting somewhat confused is how the generalisation works for norms. Specifically, is it really enough to have just have an ordered field in order to satisfy the norm property of ? I know that having an ordered field allows us to generalise (i) and (ii), but  how do we define what the modulus of is? I think the linked question is relevant since if we generalise the inner product over an arbitrary field, and we get a norm out of those inner products, we have made some progress. How does ordering the field allow us to define the modulus of scalars in a vector space over an ordered field? Do we need to specify additional structure?","\mathbb{R} \mathbb{C} V \mathbb{F} \mathbb{F} \|\cdot\|: V \to \mathbb{F} a \in \mathbb{F} \bar u, \bar v\in V \text{(i) }\|\bar u+ \bar v\| \leq \|\bar u\|+\|\bar v\| \text{(ii) }\|\bar v\| = e \to \bar v = \boldsymbol{0}, \text{where } e \text{ is the additive identity of } \mathbb{F} \|a\bar u\| = |a|\|\bar u\| \|a\bar x\| = |a|\|\bar x\| a","['linear-algebra', 'abstract-algebra']"
20,Balancing chemical equations - Matrix determinant is zero for specific reaction,Balancing chemical equations - Matrix determinant is zero for specific reaction,,"I am currently working on a program which takes a chemical equation as input and returns the balanced chemical equation instead. It finds the appropiate chemical coefficients with matrix operations, using the formula $x = A^{-1}b$ . I am using JAMA for calculations involving matrices. According to their docs, it calculates $A$ 's inverse using Eigenvalue Decomposition of both symmetric and nonsymmetric square matrices Let's take an example to better visualize this. Suppose the following chemical equation with denoted coefficients for every compound: $aCa(OH)_2 + bH_3PO_4 \rightarrow cCa_3(PO_4)_2 + dH_2O$ The computer creates an equation for every element: $P: b - 2c = 0$ $H: 2a + 3b - 2d = 0$ $Ca: a - 3c = 0$ The last equation needs to be $a = det(A)$ for three reasons. We need a square matrix (and that's why oxygen's equation is omitted) to compute it's inverse and determinant. Secondly, we need to give a numerical value to one of the variables to be able to get a numerical result, and thirdly (as far as I understood because I learned nothing about matrices in school yet) $adj(A)$ is multiplied by $1/det(A)$ to get $A^{-1}$ which causes the computed coefficients to sometimes be non-integer numbers. I am not sure of how or why setting $a$ to $det(A)$ works, neither this is the most efficient way of doing it. $A = \left[\begin{array}{cccc}0&1&-2&0\\2&3&0&-2\\1&0&-3&0\\1&0&0&0\end{array}\right]$ $b = \left[\begin{array}{c}0\\0\\0\\det(A)\end{array}\right]$ After dividing every number to the GCD the computed vector $x$ will be: $x = A^{-1}b = \left[\begin{array}{c}6\\4\\2\\12\end{array}\right]$ = $\left[\begin{array}{c}3\\2\\1\\6\end{array}\right]$ And of course, the balanced chemical equation is $3Ca(OH)_2 + 2H_3PO_4 \rightarrow Ca_3(PO_4)_2 + 6H_2O$ This program works perfectly for even more complex equations: $299H_2SO_4 + 10K_4Fe(CN)_6 + 122KMnO_4 \rightarrow 60CO_2 + 5Fe_2(SO_4)_3 + 188H_2O + 60HNO_3 + 162KHSO_4 + 122MnSO_4$ $9Fe_{36}Si_5 + 836H_3PO_4 + 192K_2Cr_2O_7 \rightarrow 324FePO_4 + 45SiO_2 + 128K_3PO_4 + 384CrPO_4 + 1254H_2O$ However, in the process of testing, I found a chemical equation that produces the following error: java.lang.RuntimeException: Matrix is singular. Let's write everything down: $aB_{10}H_{12}CNH_3 + bNiCl_2 + cNaOH \rightarrow dNa_4(B_{10}H_{10}CNH_2)_2Ni + eNaCl + fH_2O$ $B: 10a - 20d = 0$ $C: a - 2d = 0$ $Na: c - 4d - e = 0$ $H: 15a + c - 24d - 2f = 0$ $Cl: 2b - e = 0$ This time, the code omits the equation for three elements ( $N, Ni, O$ ) because it would result in an overdetermined system and a non-square matrix. $A = \left[\begin{array}{cccccc}10&0&0&-20&0&0\\1&0&0&-2&0&0\\0&0&1&-4&-1&0\\15&0&1&-24&0&-2\\0&2&0&0&-1&0\\1&0&0&0&0&0\end{array}\right]$ This is completely unexpected, both for me and the script. Using a matrix calculator, I found out that $A$ has no inverse: its determinant is zero. I analyzed the matrix and found that $10a - 20d = 0$ and $a - 2d = 0$ are really the same equation. I'm not sure if this has something to do with the matrix being singular. What should I do to make this work? Should I use a different method for finding solutions and representing my system of equations, or do I have to do something specific when $det(A) = 0$ ? I've read a little bit about Gaussian Elimination and LU Decomposition, but I don't seem to understand them very well. In case I have to use another method for solving the system, which one is the most suitable for this script? Also, I would be happy to get some details about it if possible. Keep in mind I'm a beginner in this matrix field. Any help is appreciated!","I am currently working on a program which takes a chemical equation as input and returns the balanced chemical equation instead. It finds the appropiate chemical coefficients with matrix operations, using the formula . I am using JAMA for calculations involving matrices. According to their docs, it calculates 's inverse using Eigenvalue Decomposition of both symmetric and nonsymmetric square matrices Let's take an example to better visualize this. Suppose the following chemical equation with denoted coefficients for every compound: The computer creates an equation for every element: The last equation needs to be for three reasons. We need a square matrix (and that's why oxygen's equation is omitted) to compute it's inverse and determinant. Secondly, we need to give a numerical value to one of the variables to be able to get a numerical result, and thirdly (as far as I understood because I learned nothing about matrices in school yet) is multiplied by to get which causes the computed coefficients to sometimes be non-integer numbers. I am not sure of how or why setting to works, neither this is the most efficient way of doing it. After dividing every number to the GCD the computed vector will be: = And of course, the balanced chemical equation is This program works perfectly for even more complex equations: However, in the process of testing, I found a chemical equation that produces the following error: java.lang.RuntimeException: Matrix is singular. Let's write everything down: This time, the code omits the equation for three elements ( ) because it would result in an overdetermined system and a non-square matrix. This is completely unexpected, both for me and the script. Using a matrix calculator, I found out that has no inverse: its determinant is zero. I analyzed the matrix and found that and are really the same equation. I'm not sure if this has something to do with the matrix being singular. What should I do to make this work? Should I use a different method for finding solutions and representing my system of equations, or do I have to do something specific when ? I've read a little bit about Gaussian Elimination and LU Decomposition, but I don't seem to understand them very well. In case I have to use another method for solving the system, which one is the most suitable for this script? Also, I would be happy to get some details about it if possible. Keep in mind I'm a beginner in this matrix field. Any help is appreciated!","x = A^{-1}b A aCa(OH)_2 + bH_3PO_4 \rightarrow cCa_3(PO_4)_2 + dH_2O P: b - 2c = 0 H: 2a + 3b - 2d = 0 Ca: a - 3c = 0 a = det(A) adj(A) 1/det(A) A^{-1} a det(A) A = \left[\begin{array}{cccc}0&1&-2&0\\2&3&0&-2\\1&0&-3&0\\1&0&0&0\end{array}\right] b = \left[\begin{array}{c}0\\0\\0\\det(A)\end{array}\right] x x = A^{-1}b = \left[\begin{array}{c}6\\4\\2\\12\end{array}\right] \left[\begin{array}{c}3\\2\\1\\6\end{array}\right] 3Ca(OH)_2 + 2H_3PO_4 \rightarrow Ca_3(PO_4)_2 + 6H_2O 299H_2SO_4 + 10K_4Fe(CN)_6 + 122KMnO_4 \rightarrow 60CO_2 + 5Fe_2(SO_4)_3 + 188H_2O + 60HNO_3 + 162KHSO_4 + 122MnSO_4 9Fe_{36}Si_5 + 836H_3PO_4 + 192K_2Cr_2O_7 \rightarrow 324FePO_4 + 45SiO_2 + 128K_3PO_4 + 384CrPO_4 + 1254H_2O aB_{10}H_{12}CNH_3 + bNiCl_2 + cNaOH \rightarrow dNa_4(B_{10}H_{10}CNH_2)_2Ni + eNaCl + fH_2O B: 10a - 20d = 0 C: a - 2d = 0 Na: c - 4d - e = 0 H: 15a + c - 24d - 2f = 0 Cl: 2b - e = 0 N, Ni, O A = \left[\begin{array}{cccccc}10&0&0&-20&0&0\\1&0&0&-2&0&0\\0&0&1&-4&-1&0\\15&0&1&-24&0&-2\\0&2&0&0&-1&0\\1&0&0&0&0&0\end{array}\right] A 10a - 20d = 0 a - 2d = 0 det(A) = 0","['linear-algebra', 'matrices', 'systems-of-equations', 'determinant', 'chemistry']"
21,"If $f$ is continuous and bounded, and $\text{span}\{x \mapsto f(x+k) \mid k \in \Bbb Z\}$ is finite-dimensional, what can we say about $f$?","If  is continuous and bounded, and  is finite-dimensional, what can we say about ?",f \text{span}\{x \mapsto f(x+k) \mid k \in \Bbb Z\} f,"If $f : \Bbb R \to \Bbb R$ is continuous and bounded, and $\text{span}\{x \mapsto f(x+k) \mid k \in \Bbb Z\}$ is finite-dimensional, what can we say about $f$ ? I found this problem difficult (I found examples of functions for which the hypothesis is true like $n$ -periodic functions, where $n \in \Bbb N^*$ , but I am struggling for the reverse implication). Can someone help me solve it and does someone have a reference from where it comes?","If is continuous and bounded, and is finite-dimensional, what can we say about ? I found this problem difficult (I found examples of functions for which the hypothesis is true like -periodic functions, where , but I am struggling for the reverse implication). Can someone help me solve it and does someone have a reference from where it comes?",f : \Bbb R \to \Bbb R \text{span}\{x \mapsto f(x+k) \mid k \in \Bbb Z\} f n n \in \Bbb N^*,"['real-analysis', 'calculus', 'linear-algebra']"
22,"Linear operator T on $ \mathbb{R}^5$. $T^4 \neq 0$, but $T^5 = 0$. What's the rank of $T^2$?","Linear operator T on . , but . What's the rank of ?", \mathbb{R}^5 T^4 \neq 0 T^5 = 0 T^2,"The problem: Linear operator T  on $ \mathbb{R}^5$ . $T^4 \neq 0$ , but $T^5 = 0$ . What's the rank of $T^2$ ? I am learning linear algebra and preparing for a test. For this problem, I am able to show that $range(T^4) \subseteq nullspace(T)$ since there exists $T^4x = y$ , and $T^5x = Ty = 0$ . $y$ is arbitrary and can represent the range of $T^4$ . Then I am stuck. I understand this is a duplicated question but I don't know Jordan canonical form or eigenspaces yet. So I couldn't understand the answer there. Could you help with it using only linear transformation theorems?","The problem: Linear operator T  on . , but . What's the rank of ? I am learning linear algebra and preparing for a test. For this problem, I am able to show that since there exists , and . is arbitrary and can represent the range of . Then I am stuck. I understand this is a duplicated question but I don't know Jordan canonical form or eigenspaces yet. So I couldn't understand the answer there. Could you help with it using only linear transformation theorems?", \mathbb{R}^5 T^4 \neq 0 T^5 = 0 T^2 range(T^4) \subseteq nullspace(T) T^4x = y T^5x = Ty = 0 y T^4,"['linear-algebra', 'linear-transformations']"
23,How to show that any polytope $P$ is spanned by the neighboring edges of any vertex $x$?,How to show that any polytope  is spanned by the neighboring edges of any vertex ?,P x,"Definitions: A subset $P \subset \mathbb R^n$ is a polytope if it is the convex hull of finitely many points. Let $P \subset \mathbb R^n$ be a polytope. A face is a subset $F\subset P$ of the form $$F=\arg\max\{cx : x \in P\}$$ for some $c \in \mathbb R^n$ . The dimension of a face is the dimension of its affine hull. A vertex is a zero dimensional face and an edge a one dimensional face. Two vertices $v, w$ are neighbors if their connecting line $\operatorname{conv}(\{v,w\})$ is an edge. Given a vertex $x$ define $$N(x) = \{y \in P: \text{ $y$ is a vertex neighboring $x$}\}$$ as the set of vertices that are neighbors of $x$ , and define $$E(x) = \{y-x: y \in N(x)\}$$ as the set of edge vectors pointing from $x$ to its neighbors. Question: Let $P \subset \mathbb R^n$ be a polytope and let $x$ be a vertex. Let $$E(x) = \{y-x: \text{ $y$ is a vertex neighboring $x$}\}$$ be the set of vectors that point from $x$ to its neighboring vertices. How can we show that for any $z \in P$ there exist coefficients $\lambda_v\ge 0$ such that $$ z = x + \sum_{v \in E(x)}\lambda_v v$$ The question can also be phrased as: How to show that the conical hull of $P-\{x\}$ , $$K=\operatorname{cone}(P-\{x\}):=\{\sum_{i=1}^k \alpha_i (z_i-x): z_i \in P, \alpha_i\ge0, k =1,2\dots, \}$$ is generated by the edge vectors $E(x)$ ? That is, show that $$K=\{\sum_{y \in N(x)} \alpha_y (y-x): \alpha_i\ge0 \}.$$ See also example and images below. I think Farkas' Lemma should lead to the answer somehow, but so far I've had no success in my proof attempts. Example: Consider $\mathbb R^2$ and let $P$ be the polytope that is the convex hull of the points $(0,0), (0,1), (1,0)$ . If we take the vertex $x=(0,0)$ then $N(x) = \{(0,1), (1,0)\} = E(x)$ and the set of vectors that are nonnegative linear combinations of elements of $E(x)$ is $\mathbb R^2$ . In particular, any $z \in P$ can be expressed as a nonnegative linear combinations of elements of $E(x)$ . Here is an image (the shaded region is the set of points $z = x + \sum_{v \in E(x)}\lambda_v v$ for some nonnegative $\lambda_v$ ): Here are two more images showing the idea for different polytopes: A polytope in $\mathbb R^2$ : A polytope in $\mathbb R^3$ :","Definitions: A subset is a polytope if it is the convex hull of finitely many points. Let be a polytope. A face is a subset of the form for some . The dimension of a face is the dimension of its affine hull. A vertex is a zero dimensional face and an edge a one dimensional face. Two vertices are neighbors if their connecting line is an edge. Given a vertex define as the set of vertices that are neighbors of , and define as the set of edge vectors pointing from to its neighbors. Question: Let be a polytope and let be a vertex. Let be the set of vectors that point from to its neighboring vertices. How can we show that for any there exist coefficients such that The question can also be phrased as: How to show that the conical hull of , is generated by the edge vectors ? That is, show that See also example and images below. I think Farkas' Lemma should lead to the answer somehow, but so far I've had no success in my proof attempts. Example: Consider and let be the polytope that is the convex hull of the points . If we take the vertex then and the set of vectors that are nonnegative linear combinations of elements of is . In particular, any can be expressed as a nonnegative linear combinations of elements of . Here is an image (the shaded region is the set of points for some nonnegative ): Here are two more images showing the idea for different polytopes: A polytope in : A polytope in :","P \subset \mathbb R^n P \subset \mathbb R^n F\subset P F=\arg\max\{cx : x \in P\} c \in \mathbb R^n v, w \operatorname{conv}(\{v,w\}) x N(x) = \{y \in P: \text{ y is a vertex neighboring x}\} x E(x) = \{y-x: y \in N(x)\} x P \subset \mathbb R^n x E(x) = \{y-x: \text{ y is a vertex neighboring x}\} x z \in P \lambda_v\ge 0  z = x + \sum_{v \in E(x)}\lambda_v v P-\{x\} K=\operatorname{cone}(P-\{x\}):=\{\sum_{i=1}^k \alpha_i (z_i-x): z_i \in P, \alpha_i\ge0, k =1,2\dots, \} E(x) K=\{\sum_{y \in N(x)} \alpha_y (y-x): \alpha_i\ge0 \}. \mathbb R^2 P (0,0), (0,1), (1,0) x=(0,0) N(x) = \{(0,1), (1,0)\} = E(x) E(x) \mathbb R^2 z \in P E(x) z = x + \sum_{v \in E(x)}\lambda_v v \lambda_v \mathbb R^2 \mathbb R^3","['linear-algebra', 'geometry', 'discrete-mathematics', 'linear-programming', 'polytopes']"
24,Can the trace of a positive matrix increase under a projection?,Can the trace of a positive matrix increase under a projection?,,"The question is concerned with symmetric matrices $\mathbb{S}_n$ as a real vector space. Let $X$ be a positive semidefinite symmetric matrix, and let $P : \mathbb{S}_n \to \mathcal{V}$ be a projection onto some subspace $\mathcal{V} \subset \mathbb{S}_n$ . Is it always the case that $\mathrm{trace}(P(X)) \leq \mathrm{trace}(X)$ ? I can see this being true when $\mathcal{V}$ has an orthonormal basis $\{A_i\}$ consisting of matrices which are all either trace $0$ or positive and trace $\leq 1$ . But in general I don't know anything about what kind of basis $\mathcal{V}$ would admit. Is it possible to come up with a counterexample?","The question is concerned with symmetric matrices as a real vector space. Let be a positive semidefinite symmetric matrix, and let be a projection onto some subspace . Is it always the case that ? I can see this being true when has an orthonormal basis consisting of matrices which are all either trace or positive and trace . But in general I don't know anything about what kind of basis would admit. Is it possible to come up with a counterexample?",\mathbb{S}_n X P : \mathbb{S}_n \to \mathcal{V} \mathcal{V} \subset \mathbb{S}_n \mathrm{trace}(P(X)) \leq \mathrm{trace}(X) \mathcal{V} \{A_i\} 0 \leq 1 \mathcal{V},"['linear-algebra', 'matrices', 'symmetric-matrices', 'matrix-analysis']"
25,INTUITION: Orthonormal columns implies orthonormal rows [closed],INTUITION: Orthonormal columns implies orthonormal rows [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question I was asked by a student and was stumped upon the very intuition of this statement. I searched on this site and found many proofs but little intuition. For example: Column Vectors orthogonal implies Row Vectors also orthogonal? and Intuition behind row vectors of orthonormal matrix being an orthonormal basis I am open to abstract linear algebra ideas but I don't think they help bring in much intuition. I am hoping for either a geometric intuition , a quick algebraic manipulation on vector components, or an intuitive explanation of the key step in the proof: $A^TA = AA^T = I$ . Edit: many comments went for the last option of the three. However, it was probably the hardest to gain any intuition with this option. I would personally prefer answers exploring the first two options, or something really really special about this last option.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question I was asked by a student and was stumped upon the very intuition of this statement. I searched on this site and found many proofs but little intuition. For example: Column Vectors orthogonal implies Row Vectors also orthogonal? and Intuition behind row vectors of orthonormal matrix being an orthonormal basis I am open to abstract linear algebra ideas but I don't think they help bring in much intuition. I am hoping for either a geometric intuition , a quick algebraic manipulation on vector components, or an intuitive explanation of the key step in the proof: . Edit: many comments went for the last option of the three. However, it was probably the hardest to gain any intuition with this option. I would personally prefer answers exploring the first two options, or something really really special about this last option.",A^TA = AA^T = I,"['linear-algebra', 'matrices', 'vector-spaces', 'intuition']"
26,"Prove $D \in \mathcal{L}(\mathcal{P}(\mathbf{R}),\mathcal{P}(\mathbf{R})) : \text{deg}(D(p)) = \text{deg}(p) - 1$ is surjective",Prove  is surjective,"D \in \mathcal{L}(\mathcal{P}(\mathbf{R}),\mathcal{P}(\mathbf{R})) : \text{deg}(D(p)) = \text{deg}(p) - 1","Suppose $D \in \mathcal{L}(\mathcal{P}(\mathbf{R}),\mathcal{P}(\mathbf{R}))$ is such that $\deg(D(p)) = \deg(p) - 1$ for every nonconstant polynomial $p \in \mathcal{P}(\mathbf{R})$ . Prove that $D$ is surjective. I have attempted an answer, however, I think it is incorrect: We can redefine this as a linear map between two finite dimentional vector spaces: $$ D \in \mathcal{L}(\mathcal{P}_m(\mathbf{R}),\mathcal{P}_{m-1}(\mathbf{R})) $$ for $m > 0$ . Let $(1, x, x^2 \ldots, x^{m-1})$ be a basis for $\mathcal{P}_{m-1}$ . We can extend this to a basis of $\mathcal{P}_m$ because $\mathcal{P}_{m-1} \subset \mathcal{P}_m$ : $$(1, x, x^2 \ldots, x^{m-1}, x^m)\text{.}$$ Then define $D$ : \begin{align} D(x^i) &= x^i, i = 0, \ldots, m - 1 \\ D(x^m) &= 0 \end{align} Clearly then, $\text{range}(D) = \mathcal{P}_{m-1}$ , as $(1, x, x^2 \ldots, x^{m-1})$ is a basis for $\text{range}(D)$ . Hence $D$ is surjective. The reason I think this answer is incorrect, is because I have chosen my own definition of $D$ , not proved it for an arbitrary $D$ . However, for similar questions, I often see the answers choose a specific mapping, and I struggle to know when that is acceptable and when it isn't.","Suppose is such that for every nonconstant polynomial . Prove that is surjective. I have attempted an answer, however, I think it is incorrect: We can redefine this as a linear map between two finite dimentional vector spaces: for . Let be a basis for . We can extend this to a basis of because : Then define : Clearly then, , as is a basis for . Hence is surjective. The reason I think this answer is incorrect, is because I have chosen my own definition of , not proved it for an arbitrary . However, for similar questions, I often see the answers choose a specific mapping, and I struggle to know when that is acceptable and when it isn't.","D \in \mathcal{L}(\mathcal{P}(\mathbf{R}),\mathcal{P}(\mathbf{R})) \deg(D(p)) = \deg(p) - 1 p \in \mathcal{P}(\mathbf{R}) D 
D \in \mathcal{L}(\mathcal{P}_m(\mathbf{R}),\mathcal{P}_{m-1}(\mathbf{R}))
 m > 0 (1, x, x^2 \ldots, x^{m-1}) \mathcal{P}_{m-1} \mathcal{P}_m \mathcal{P}_{m-1} \subset \mathcal{P}_m (1, x, x^2 \ldots, x^{m-1}, x^m)\text{.} D \begin{align}
D(x^i) &= x^i, i = 0, \ldots, m - 1 \\
D(x^m) &= 0
\end{align} \text{range}(D) = \mathcal{P}_{m-1} (1, x, x^2 \ldots, x^{m-1}) \text{range}(D) D D D","['linear-algebra', 'linear-transformations']"
27,Prove that commuting matrices over an algebraically closed field are simultaneously triangularizable.,Prove that commuting matrices over an algebraically closed field are simultaneously triangularizable.,,"Given an algebraically closed field $\mathbb K$ and matrices $A, B \in \mathbb K^{n \times n}$ such that $A B = B A$ , show that $A$ and $B$ are simultaneously triangularizable, i.e., show that there exists a matrix $T$ such that $T^{-1} A T$ and $T^{-1} B T$ are both upper triangular.","Given an algebraically closed field and matrices such that , show that and are simultaneously triangularizable, i.e., show that there exists a matrix such that and are both upper triangular.","\mathbb K A, B \in \mathbb K^{n \times n} A B = B A A B T T^{-1} A T T^{-1} B T","['linear-algebra', 'matrices', 'triangularization']"
28,"If for invertible matrices $A$ and $X$, $XAX^{-1}=A^2$ then eigenvalues of $A$ are $n^{th}$ roots of unity.","If for invertible matrices  and ,  then eigenvalues of  are  roots of unity.",A X XAX^{-1}=A^2 A n^{th},"Question: Let $A$ and $X$ be two complex invertible matrices such that $XAX^{-1}=A^2$ . Show that there exists a natural number $n$ such that each eigenvalue of $A$ is an $n^{th}$ root of unity. I can say from here, $\operatorname{det}(A)=1$ and I guess somehow I have to show $A^n=I$ , for some $n$ , which will give the result. But I have no idea how to show it from the fact that $A$ and $A^2$ are similar matrices. Any hint!!","Question: Let and be two complex invertible matrices such that . Show that there exists a natural number such that each eigenvalue of is an root of unity. I can say from here, and I guess somehow I have to show , for some , which will give the result. But I have no idea how to show it from the fact that and are similar matrices. Any hint!!",A X XAX^{-1}=A^2 n A n^{th} \operatorname{det}(A)=1 A^n=I n A A^2,['linear-algebra']
29,Easiest way to prove the Rouché–Capelli theorem,Easiest way to prove the Rouché–Capelli theorem,,"Rouché–Capelli theorem (Kronecker–Capelli theorem/Rouché–Fontené theorem/Rouché–Frobenius theorem/Frobenius theorem) states that for the non-homogeneous system Ax = b, $(i)$ $Ax = b$ has a unique solution if and only if $rank[A] = rank[A|b] = n$ $(ii)$ $Ax = b$ is inconsistent (i.e., no solution exists) if and only if $rank[A] < rank[A|b]$ $(iii)$ $Ax = b$ has infinitely many solutions if and only if $rank[A] = rank[A|b] < n$ How do I derive these conditions ? My Understanding $(i)$ $Ax=b$ has a unique solution $A^{-1}$ exists $\implies \boxed{x=A^{-1}b} \implies |A|\neq 0 \implies rank(A)=n$ If solution exists, then $\vec{A}_1x_1+\vec{A}_2x_2+....+\vec{A}_nx_n=b\implies b$ is a linear combination of the column vectors $\implies rank[A|b]=rank[A]=n$ $(ii)$ $Ax=b$ is inconsistent (i.e., no solution exists) $\boxed{|A|x=(adj A)b\implies |A|=0 \quad\&\quad adj A.b\neq 0}$ $|A|=0\implies rank[A]<n$ Is there a way to prove the last two conditions of the Rouché–Capelli theorem ?","Rouché–Capelli theorem (Kronecker–Capelli theorem/Rouché–Fontené theorem/Rouché–Frobenius theorem/Frobenius theorem) states that for the non-homogeneous system Ax = b, has a unique solution if and only if is inconsistent (i.e., no solution exists) if and only if has infinitely many solutions if and only if How do I derive these conditions ? My Understanding has a unique solution exists If solution exists, then is a linear combination of the column vectors is inconsistent (i.e., no solution exists) Is there a way to prove the last two conditions of the Rouché–Capelli theorem ?","(i) Ax = b rank[A] = rank[A|b] = n (ii) Ax = b rank[A] <
rank[A|b] (iii) Ax = b rank[A] = rank[A|b] < n (i) Ax=b A^{-1} \implies \boxed{x=A^{-1}b} \implies |A|\neq 0 \implies rank(A)=n \vec{A}_1x_1+\vec{A}_2x_2+....+\vec{A}_nx_n=b\implies b \implies rank[A|b]=rank[A]=n (ii) Ax=b \boxed{|A|x=(adj A)b\implies |A|=0 \quad\&\quad adj A.b\neq 0} |A|=0\implies rank[A]<n","['linear-algebra', 'matrix-rank']"
30,Does anyone recall this linear algebra survey of results?,Does anyone recall this linear algebra survey of results?,,"I'm not sure if math stackexchange is the appropriate place for this, but I am looking for a paper that consisted of results in linear algebra, since I've lost the pdf of it and can't recall the author. It was a survey of results, with either proofs or published references, in the field of finite dimensional linear algebra. It assumed familiarity with the subject, but not expert knowledge, and was motivated as a presentation of nontrivial results that deserve to be known more broadly. There were lots of results proved within, but I can only recall one specifically, which hopefully is obscure enough to track down the paper. The result was (if I recall correctly) that a linear operator is similar to its inverse if and only if it is the product of two involutions. Hopefully someone recognises something here, as it was a great reference for more advanced aspects of finite dimensional linear algebra.","I'm not sure if math stackexchange is the appropriate place for this, but I am looking for a paper that consisted of results in linear algebra, since I've lost the pdf of it and can't recall the author. It was a survey of results, with either proofs or published references, in the field of finite dimensional linear algebra. It assumed familiarity with the subject, but not expert knowledge, and was motivated as a presentation of nontrivial results that deserve to be known more broadly. There were lots of results proved within, but I can only recall one specifically, which hopefully is obscure enough to track down the paper. The result was (if I recall correctly) that a linear operator is similar to its inverse if and only if it is the product of two involutions. Hopefully someone recognises something here, as it was a great reference for more advanced aspects of finite dimensional linear algebra.",,"['linear-algebra', 'reference-request']"
31,"If $\mathrm{M,N}$ are $3\times 2, 2 \times 3$ matrices such that $\mathrm{MN}=$ is given. Then $\mathrm{det(NM)}$ is?",If  are  matrices such that  is given. Then  is?,"\mathrm{M,N} 3\times 2, 2 \times 3 \mathrm{MN}= \mathrm{det(NM)}","If $\mathrm{M,N}$ are $3\times 2, 2 \times 3$ matrices such that $\mathrm{MN}=\pmatrix{8& 2 & -2\\2& 5& 4\\-2& 4&5}$ , then $\mathrm{det(NM)}$ is? ( $\mathrm{NM}$ is invertible.) $\mathrm{det(MN)}$ must be (and is) zero. But how to find $\mathrm{det(NM)}$ ? Any hint?","If are matrices such that , then is? ( is invertible.) must be (and is) zero. But how to find ? Any hint?","\mathrm{M,N} 3\times 2, 2 \times 3 \mathrm{MN}=\pmatrix{8& 2 & -2\\2& 5& 4\\-2& 4&5} \mathrm{det(NM)} \mathrm{NM} \mathrm{det(MN)} \mathrm{det(NM)}","['linear-algebra', 'matrices']"
32,"If $\dim \ker T=4, \dim \ker T^3=9, \dim \ker T^4=11$. Then, Find $\dim \ker T^2$","If . Then, Find","\dim \ker T=4, \dim \ker T^3=9, \dim \ker T^4=11 \dim \ker T^2","Let $T: \mathbb C^{11} \rightarrow  \mathbb C^{11} $ be a linear transformation, such that $\dim \ker T=4, \dim \ker T^3=9, \dim \ker T^4=11$ . Then, Find $\dim \ker T^2.$ Attempt: We have : $\ker T \subset \ker T^2 \subset \ker T^3 \subset \ker T^4$ where $T$ is clearly nilpotent with index $4$ $\implies \dim \ker T <  \dim \ker T^2 <  \dim \ker T^3 <  \dim \ker T^4$ Thus, $\dim \ker T^2 $ can assume values from $\{5,6,7,8 \}$ . How do I move forward from here?","Let be a linear transformation, such that . Then, Find Attempt: We have : where is clearly nilpotent with index Thus, can assume values from . How do I move forward from here?","T: \mathbb C^{11} \rightarrow  \mathbb C^{11}  \dim \ker T=4, \dim \ker T^3=9, \dim \ker T^4=11 \dim \ker T^2. \ker T \subset \ker T^2 \subset \ker T^3 \subset \ker T^4 T 4 \implies \dim \ker T <  \dim \ker T^2 <  \dim \ker T^3 <  \dim \ker T^4 \dim \ker T^2  \{5,6,7,8 \}","['linear-algebra', 'linear-transformations']"
33,What does that fact that $AB = I_3$ tell us about $A$ and $B$?,What does that fact that  tell us about  and ?,AB = I_3 A B,"If we have to two matrices $A$ and $B$ such that, $A \in \mathbb{R^{3 \times 4}}$ and $B \in \mathbb{R^{4 \times 3}}$ . What does that fact that $AB = I_3$ tell us about $A$ and $B$ ?","If we have to two matrices and such that, and . What does that fact that tell us about and ?",A B A \in \mathbb{R^{3 \times 4}} B \in \mathbb{R^{4 \times 3}} AB = I_3 A B,"['linear-algebra', 'matrices']"
34,Two ways of interpreting matrix-vector multiplication?,Two ways of interpreting matrix-vector multiplication?,,"I have been trying to get an intuitive grasp of the matrix-vector multiplication operation. So far, I've consumed both 3Blue1Brown's videos on this topic as well as studied Gilbert Strang's textbook chapters relevant to matrix-vector multiplication. Both seem to offer two very different ways of intuitively looking at the product. 3Blue1Brown interprets the matrix as transforming the vector by performing operations such as ""stretching"", ""shrinking"", ""rotating"", etc. In other words, the thing being transformed is the vector. Strang, on the other hand, interprets this operation as the columns of the matrix being added together with the elements of the vector acting as coefficients. In other words, the things being transformed are the columns of the matrix. I see these as two very different interpretations of the same thing. Is there some way to reconcile these two things? Am I missing something painfully obvious?","I have been trying to get an intuitive grasp of the matrix-vector multiplication operation. So far, I've consumed both 3Blue1Brown's videos on this topic as well as studied Gilbert Strang's textbook chapters relevant to matrix-vector multiplication. Both seem to offer two very different ways of intuitively looking at the product. 3Blue1Brown interprets the matrix as transforming the vector by performing operations such as ""stretching"", ""shrinking"", ""rotating"", etc. In other words, the thing being transformed is the vector. Strang, on the other hand, interprets this operation as the columns of the matrix being added together with the elements of the vector acting as coefficients. In other words, the things being transformed are the columns of the matrix. I see these as two very different interpretations of the same thing. Is there some way to reconcile these two things? Am I missing something painfully obvious?",,"['linear-algebra', 'intuition']"
35,How can a vector represent velocity and a position as well?,How can a vector represent velocity and a position as well?,,"I'm relatively new to linear algebra. I've got a question about a question . I'm not looking for the exact answer (I'll try to find it by myself). I stumbled upon the following question : ""*At 12:00 pm, a spaceship is at position $$\begin{pmatrix} 3 \\ 2\\ 4 \end{pmatrix}$$ km away from the origin with respect to some 3 dimensional co ordinate system. The ship is travelling with velocity $$\begin{pmatrix} -1 \\ 2\\ -3 \end{pmatrix}$$ km/h What is the location of the spaceship after 2 hours have passed? "". I understand what a $$\begin{pmatrix} 3 \\ 2\\ 4 \end{pmatrix}$$ position means (in three dimensional Cartesian coordinate system with x, y, z axes).  I can point precisely the position on a paper. But what does a "" [-1,2,-3] velocity "" mean ? Again, I'm not looking for the precise answer. I'd just like to know how to approach the problem since a same format (vectors) is used to tackle two different concepts (namely, position and velocity ). How can I process to some calculations in this ""position"" I would say, aha. Thank you :) Alex","I'm relatively new to linear algebra. I've got a question about a question . I'm not looking for the exact answer (I'll try to find it by myself). I stumbled upon the following question : ""*At 12:00 pm, a spaceship is at position km away from the origin with respect to some 3 dimensional co ordinate system. The ship is travelling with velocity km/h What is the location of the spaceship after 2 hours have passed? "". I understand what a position means (in three dimensional Cartesian coordinate system with x, y, z axes).  I can point precisely the position on a paper. But what does a "" [-1,2,-3] velocity "" mean ? Again, I'm not looking for the precise answer. I'd just like to know how to approach the problem since a same format (vectors) is used to tackle two different concepts (namely, position and velocity ). How can I process to some calculations in this ""position"" I would say, aha. Thank you :) Alex","\begin{pmatrix}
3 \\
2\\
4
\end{pmatrix} \begin{pmatrix}
-1 \\
2\\
-3
\end{pmatrix} \begin{pmatrix}
3 \\
2\\
4
\end{pmatrix}",['linear-algebra']
36,Characterizing the dual cone of the squares of skew-symmetric matrices,Characterizing the dual cone of the squares of skew-symmetric matrices,,"Let $X$ be the set of all real $n \times n$ diagonal matrices $D$ satisfying $\langle D,B^2 \rangle \le 0$ for any (real) skew-symmetric matrix $B$ . (I am using the Frobenius Euclidean product here). $X$ is a convex cone. Can we give an explicit characterization of $X$ ? Comment: If we denote by $C$ the space of all  squares of skew-symmetric matrices, we can characterize its dual cone as follows: Since every square of a skew-symmetric matrix is symmetric, and the symmetric and the skew-symmetric matrices are orthogonal, we know that every skew-symmetric matrix belongs to the dual cone of $C$ . So, the question whether a given matrix $A$ belongs to the dual cone of $C$ depends solely on the symmetric part of $A$ . Since $C$ is invariant under orthogonal conjugation , we can orthogonally diagonalize $\text{sym}(A)$ and deduce that $A$ lies in $C^*$ if and only if the diagonal matrix whose entries are the eigenvalues of $\text{sym}(A)$ is in $C^*$ . Thus, the question reduces to determining the case of diagonal matrices. Edit: Omnomnomnom proved in this answer that every $D$ in $X$ has at most one negative entry, and the absolute value of the negative entry is less than or equal to the next-smallest entry. I have a strangely complicated proof for the converse, namely I can prove that every diagonal matrix satisfying the condition above is in $X$ . I would like to find a ""direct"" proof based on linear algebra\matrix analysis. (my proof is based on rather convoluted variational considerations).","Let be the set of all real diagonal matrices satisfying for any (real) skew-symmetric matrix . (I am using the Frobenius Euclidean product here). is a convex cone. Can we give an explicit characterization of ? Comment: If we denote by the space of all  squares of skew-symmetric matrices, we can characterize its dual cone as follows: Since every square of a skew-symmetric matrix is symmetric, and the symmetric and the skew-symmetric matrices are orthogonal, we know that every skew-symmetric matrix belongs to the dual cone of . So, the question whether a given matrix belongs to the dual cone of depends solely on the symmetric part of . Since is invariant under orthogonal conjugation , we can orthogonally diagonalize and deduce that lies in if and only if the diagonal matrix whose entries are the eigenvalues of is in . Thus, the question reduces to determining the case of diagonal matrices. Edit: Omnomnomnom proved in this answer that every in has at most one negative entry, and the absolute value of the negative entry is less than or equal to the next-smallest entry. I have a strangely complicated proof for the converse, namely I can prove that every diagonal matrix satisfying the condition above is in . I would like to find a ""direct"" proof based on linear algebra\matrix analysis. (my proof is based on rather convoluted variational considerations).","X n \times n D \langle D,B^2 \rangle \le 0 B X X C C A C A C \text{sym}(A) A C^* \text{sym}(A) C^* D X X","['linear-algebra', 'matrices', 'matrix-calculus', 'symmetric-matrices', 'dual-cone']"
37,Lyapunov equation: Relationship between the eigenvalues of P and Q,Lyapunov equation: Relationship between the eigenvalues of P and Q,,"Studying some problems in stability of linear/nonlinear systems I found this question would be interesting. I think this is related but I would like to address the problem from a more constructive point of view. Consider $A$ be a Hurwitz matrix (real part of eigenvalues strictly smaller than 0) and the classic Lyapunov equation: $A^{T}P + PA = -Q,\ Q>0$ We already know that the solution $P$ of the equation above is unique and positive definite. I was asking myself: can we choose $Q$ such that the eigenvalues of $P$ are in a certain region? If yes how? What are the constraints that the $A$ matrix (Hurwitz in this case) impose on this problem? Edit: I think that if $A$ is diagonalizable, then we have some interesting to say (will dig a bit more). Maybe this approach could be extended to the non diagonalizable case.","Studying some problems in stability of linear/nonlinear systems I found this question would be interesting. I think this is related but I would like to address the problem from a more constructive point of view. Consider be a Hurwitz matrix (real part of eigenvalues strictly smaller than 0) and the classic Lyapunov equation: We already know that the solution of the equation above is unique and positive definite. I was asking myself: can we choose such that the eigenvalues of are in a certain region? If yes how? What are the constraints that the matrix (Hurwitz in this case) impose on this problem? Edit: I think that if is diagonalizable, then we have some interesting to say (will dig a bit more). Maybe this approach could be extended to the non diagonalizable case.","A A^{T}P + PA = -Q,\ Q>0 P Q P A A","['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-equations', 'control-theory', 'stability-theory']"
38,Prove that $\varphi$ is continuous on the real numbers and $\varphi(x) = x$,Prove that  is continuous on the real numbers and,\varphi \varphi(x) = x,"Let $\varphi : \mathbb{R} \to \mathbb{R}$ be a bijection such that $\varphi(x) = x\;\forall x \in\mathbb{Q}$ and $\varphi(xy) = \varphi(x)\varphi(y)\wedge \varphi(x+y) = \varphi(x)+\varphi(y)\forall x,y\in\mathbb{R}$ . Prove that $\varphi(x)=x\;\forall x\in\mathbb{R}$ . Just to make sure I'm understanding everything right, I'm going to try to explain as much as I can in the following solution: Let $\varphi :\mathbb{R}\to\mathbb{R}$ be the said bijection. Then $\forall x\in\mathbb{R}$ such that $x>0$ , $\exists y\in \mathbb{R}$ such that $x=y^2\Rightarrow \varphi(x)=\varphi(y^2)=[\varphi(y)]^2>0$ . Since $x$ was arbitrary, $\varphi(x)$ preserves $>$ . Between any two distinct real numbers there is a rational number so for all distinct $a,b\in\mathbb{R},\exists r\in\mathbb{Q}$ such that $a<r<b$ . We thus have that $\varphi(a) < \varphi(r)=r<\varphi(b)$ . By the Archimedean property of the real numbers, $\forall\epsilon >0$ , there exists $a,b\in\mathbb{R}$ and $n\in\mathbb{N}$ such that $|a-b|\leq\frac{1}{10^n}<\epsilon$ . I think this is done, but I want to use the fact that between any two real numbers there is a rational number .","Let be a bijection such that and . Prove that . Just to make sure I'm understanding everything right, I'm going to try to explain as much as I can in the following solution: Let be the said bijection. Then such that , such that . Since was arbitrary, preserves . Between any two distinct real numbers there is a rational number so for all distinct such that . We thus have that . By the Archimedean property of the real numbers, , there exists and such that . I think this is done, but I want to use the fact that between any two real numbers there is a rational number .","\varphi : \mathbb{R} \to \mathbb{R} \varphi(x) = x\;\forall x \in\mathbb{Q} \varphi(xy) = \varphi(x)\varphi(y)\wedge \varphi(x+y) = \varphi(x)+\varphi(y)\forall x,y\in\mathbb{R} \varphi(x)=x\;\forall x\in\mathbb{R} \varphi :\mathbb{R}\to\mathbb{R} \forall x\in\mathbb{R} x>0 \exists y\in \mathbb{R} x=y^2\Rightarrow \varphi(x)=\varphi(y^2)=[\varphi(y)]^2>0 x \varphi(x) > a,b\in\mathbb{R},\exists r\in\mathbb{Q} a<r<b \varphi(a) < \varphi(r)=r<\varphi(b) \forall\epsilon >0 a,b\in\mathbb{R} n\in\mathbb{N} |a-b|\leq\frac{1}{10^n}<\epsilon",['calculus']
39,Why a Hamel basis?,Why a Hamel basis?,,"I think I understand what both Hamel basis and Schauder basis mean. But to me, the Schauder basis makes more sense intuitively than the Hamel basis does. For example, Fourier series, Bases for Hilbert Spaces used in Quantum Mechanics - are all Schauder bases. As I understand, even for simple infinite dimensional spaces ( e.g. $\ell^p$ ), a Hamel basis can have an uncountable cardinality and cannot be explicitly constructed. On the other hand, we can construct a Schauder basis for such spaces trivially. So, my question is that despite this fact, why is the concept of basis introduced as Hamel basis and not as Schauder basis? (Specifically, is there some objective reason like even though Schauder basis seems superficially simpler, its mathematics gets complex going ahead? Or maybe some properties defined for finite-dimensional vector spaces don't carry forward?)","I think I understand what both Hamel basis and Schauder basis mean. But to me, the Schauder basis makes more sense intuitively than the Hamel basis does. For example, Fourier series, Bases for Hilbert Spaces used in Quantum Mechanics - are all Schauder bases. As I understand, even for simple infinite dimensional spaces ( e.g. ), a Hamel basis can have an uncountable cardinality and cannot be explicitly constructed. On the other hand, we can construct a Schauder basis for such spaces trivially. So, my question is that despite this fact, why is the concept of basis introduced as Hamel basis and not as Schauder basis? (Specifically, is there some objective reason like even though Schauder basis seems superficially simpler, its mathematics gets complex going ahead? Or maybe some properties defined for finite-dimensional vector spaces don't carry forward?)",\ell^p,"['linear-algebra', 'schauder-basis', 'hamel-basis']"
40,Linear independence of real and imaginary parts of complex eigenvector,Linear independence of real and imaginary parts of complex eigenvector,,"I am told that for a matrix $A\in \mathbb{R}^{2,2}$ with complex eigenvalue $\lambda=a+ib$ and associated complex eigenvector $v \in \mathbb{C}^n$ then $A=PCP^{-1}$ where $P=\begin{bmatrix} \text{Re } v& \text{Im } v\end{bmatrix}$ and $C=\begin{bmatrix}a&-b\\b&a \end{bmatrix}$ . I know that $A$ must be invertible because it has 2 distinct eigenvalues; I know that $C$ must be invertible because $\text{det} C = a^2 + b^2 \ne 0$ , but am not sure how we know that $P$ is invertible. I know that $$Av = (a+ib)v =  av + ibv $$ Further calculations show that $$\text{Re}(Av)=A(\text{Re}v)=a\text{Re}v + b \text{Im}v$$ Can I state that because $A$ is invertible, $Av \ne 0$ and thus $\text{Re}(Av) \ne 0$ , which implies that $a\text{Re}v + b \text{Im}v = 0$ does not have a nontrivial solution and therefore $\text{Re}v, \text{Im}v$ must be linearly independent? Not sure if my reasoning is correct. Any help greatly appreciated!","I am told that for a matrix with complex eigenvalue and associated complex eigenvector then where and . I know that must be invertible because it has 2 distinct eigenvalues; I know that must be invertible because , but am not sure how we know that is invertible. I know that Further calculations show that Can I state that because is invertible, and thus , which implies that does not have a nontrivial solution and therefore must be linearly independent? Not sure if my reasoning is correct. Any help greatly appreciated!","A\in \mathbb{R}^{2,2} \lambda=a+ib v \in \mathbb{C}^n A=PCP^{-1} P=\begin{bmatrix} \text{Re } v& \text{Im } v\end{bmatrix} C=\begin{bmatrix}a&-b\\b&a \end{bmatrix} A C \text{det} C = a^2 + b^2 \ne 0 P Av = (a+ib)v =  av + ibv  \text{Re}(Av)=A(\text{Re}v)=a\text{Re}v + b \text{Im}v A Av \ne 0 \text{Re}(Av) \ne 0 a\text{Re}v + b \text{Im}v = 0 \text{Re}v, \text{Im}v","['linear-algebra', 'eigenvalues-eigenvectors']"
41,Inner product space two subspaces proofing question,Inner product space two subspaces proofing question,,"The question is as follows: Let $V$ be an inner product space of dimension $n$ , and let $U$ and $W$ be two $m$ -dimensional subspaces of $V$ . Assume there is some nonzero vector $v$ in $U$ , such      that $v$ is orthogonal to $W$ . (that is $\langle v,w\rangle =0$ for all $w$ in $W$ ).      Prove that $w$ is orthogonal to $U$ for some nonzero $w$ in $W$ . So I got a hint that it helps to fix a basis $B$ and $A$ for $V$ and $W$ and write $v$ as a linear combination of it. Then    explore what it means for $v$ to be perpendicular to every vector in $W$ . This is what I have done so far: Let $B= \{b_1,...,b_m\}$ and $A = \{a_1,...,a_m\}$ be basis for $U$ and $W$ . I found an $m\times m$ matrix $M = [\langle b_i,a_j\rangle]$ such that $\langle v,w \rangle = [w]_A M [v]_B = 0$ for any $w$ , so $[v]_B$ is in the nullspace of $M$ . From here, I don't know how to keep going. Can someone kindly suggest? Thanks, and sorry for the messy notation.","The question is as follows: Let be an inner product space of dimension , and let and be two -dimensional subspaces of . Assume there is some nonzero vector in , such      that is orthogonal to . (that is for all in ).      Prove that is orthogonal to for some nonzero in . So I got a hint that it helps to fix a basis and for and and write as a linear combination of it. Then    explore what it means for to be perpendicular to every vector in . This is what I have done so far: Let and be basis for and . I found an matrix such that for any , so is in the nullspace of . From here, I don't know how to keep going. Can someone kindly suggest? Thanks, and sorry for the messy notation.","V n U W m V v U v W \langle v,w\rangle =0 w W w U w W B A V W v v W B= \{b_1,...,b_m\} A = \{a_1,...,a_m\} U W m\times m M = [\langle b_i,a_j\rangle] \langle v,w \rangle = [w]_A M [v]_B = 0 w [v]_B M","['linear-algebra', 'inner-products']"
42,"Existence of open sets $U, V$ of matrices such that for every $A \in U$ there exists $B \in V$ such that $B^4 = A$",Existence of open sets  of matrices such that for every  there exists  such that,"U, V A \in U B \in V B^4 = A","Prove there exist non-empty open sets U and V of $n \times n$ matrices over $\mathbb{R}$ such that for every matrix $A \in U$ there exists exactly one matrix $B \in V$ such that $B^4 = A$ . I've tried to approach this problem in several ways, using characteristic polynomials, Jordan Canonical Form and facts from calculus but failed to get anything useful.","Prove there exist non-empty open sets U and V of matrices over such that for every matrix there exists exactly one matrix such that . I've tried to approach this problem in several ways, using characteristic polynomials, Jordan Canonical Form and facts from calculus but failed to get anything useful.",n \times n \mathbb{R} A \in U B \in V B^4 = A,"['real-analysis', 'linear-algebra', 'matrices']"
43,Matrix representation of Fractional Linear Transformation and the Identity Matrix?,Matrix representation of Fractional Linear Transformation and the Identity Matrix?,,"For $x \in \mathbb{R}$ , define the fractional linear transformation of $x$ as $f(x)$ where: $$f(x) = \frac{ax + b}{cx+d}$$ Then $f(x)$ has a matrix representation in $\mathbb{R}^2$ of $F$ where: $$F = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$$ Then the function $g(x)= \dfrac{1}{1-x}$ has the matrix representation of: $$G = \begin{pmatrix} 0 & 1 \\ -1 & 1 \end{pmatrix}$$ Observe that $g(g(g(x)))=x$ . Then why isn't $G^3=I$ ? In fact: $$G^3 = \begin{pmatrix} -1 & 0 \\ 0 & -1 \end{pmatrix} = -I$$ How does that make sense where $g^3(x)=x$ and not $-x$ ? Shouldn't the vector $$ v=\begin{pmatrix} x \\ y \end{pmatrix}$$ get mapped back to itself after 3 iterations of multiplying by $G$ ? What am I missing?","For , define the fractional linear transformation of as where: Then has a matrix representation in of where: Then the function has the matrix representation of: Observe that . Then why isn't ? In fact: How does that make sense where and not ? Shouldn't the vector get mapped back to itself after 3 iterations of multiplying by ? What am I missing?",x \in \mathbb{R} x f(x) f(x) = \frac{ax + b}{cx+d} f(x) \mathbb{R}^2 F F = \begin{pmatrix} a & b \\ c & d \end{pmatrix} g(x)= \dfrac{1}{1-x} G = \begin{pmatrix} 0 & 1 \\ -1 & 1 \end{pmatrix} g(g(g(x)))=x G^3=I G^3 = \begin{pmatrix} -1 & 0 \\ 0 & -1 \end{pmatrix} = -I g^3(x)=x -x  v=\begin{pmatrix} x \\ y \end{pmatrix} G,"['linear-algebra', 'matrices', 'linear-transformations']"
44,What might be the definition of a positively oriented chart in From Calculus to Cohomology?,What might be the definition of a positively oriented chart in From Calculus to Cohomology?,,"My book is From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave. I recently finished most of An Introduction to Manifolds by Loring W. Tu, so based on the preface of From Calculus to Cohomology, I started at Chapter 8. I don't believe I've missed anything since charts are first introduced in Chapter 8. Question : What's a positively oriented chart, first mentioned in Proposition 10.2 , please? Some context : I think this is relevant in answering my other question: Why is there a form with compact support on a connected oriented manifold with integral one but with support contained in a given open proper subset? I think I have to prove either the chart $(U, g: U \to g(U) = U')$ or some restriction $(W, g|_W:W \to g(W))$ , $W$ open in $U$ , is a ""positively oriented chart"" or at least an ""oriented chart"" in order to apply Proposition 10.2 My guesses : The definition of ""oriented chart"" in the book (see also previous definitions of orientation ) is meant to be ""positively oriented chart"" with ""negatively oriented chart"" to be for orientation-reversing. I mean that 1.1. a chart $(U,h:U \to U')$ is an oriented chart if and only if it is a member of an oriented atlas of an oriented smooth $n$ -dimensional manifold, and we sometimes omit $U$ and $U'$ and call $h$ , the coordinate map, an oriented chart (instead of something like ""oriented map"") 1.2 An oriented chart $(U,h:U \to U')$ , or just $h$ , is positively oriented if and only if $h:U \to U'$ is an orientation-preserving diffeomorphism if and only if $\det(D_q(h)) > 0$ if and only if $D_qh: T_qU = T_qM \to T_{h(q)}U' = T_{h(q)} \mathbb R^n$ is an orientation-preserving diffeomorphism of manifolds  (See here and here ) if and only if $D_qh: T_qU = T_qM \to T_{h(q)}U' = T_{h(q)} \mathbb R^n$ is an orientation-preserving vector space isomorphism of tangent spaces In Proposition 10.2 , what is meant by ""positively oriented chart"" is simply ""oriented chart"" if we go with the convention that ""oriented charts"" are ""positively oriented charts"", as originally in the book. I also tried looking up other books: An Introduction to Manifolds by Loring W. Tu: Based on Section 21.5 and Subsection 23.4 , I believe the definition for integration is for a chart in an ""oriented atlas"" of $M$ , where an ""oriented atlas"" is defined one where overlapping charts have positive Jacobian determinant. Thus, ""oriented atlas"" in An Introduction to Manifolds seems to be the same as ""positive atlas"" in From Calculus to Cohomology. Manifolds, Tensor Analysis, and Applications by Ralph Abraham, Jerrold E. Marsden, Tudor Ratiu : It seems a coordinate chart is defined as positively oriented if the coordinate chart's coordinate map has all its differentials to be orientation preserving ( as in vector spaces or as in manifolds, if we still have such equivalence of the 2 notions of orientation preserving ). If this is what is meant, then to clarify, do we, once again, have a notion, namely the notion of positively oriented chart, that is actually rooted in some prerequisite algebra notion ? I'm not sure this is (exactly) what Madsen and Tornehave mean because there is a difference in definition for manifolds. Update : Based on the proof of Theorem 11.9 , which relies on Lemma 11.8 , I think this might be the definition or at least equivalent to, implied by or implies the definition. Introduction to Smooth Manifolds by John M. Lee: It seems the definition is that for an oriented smooth $n$ -manifold $M$ with or without boundary, for a coordinate chart $(U,\varphi) = (U,x^1,...,x^n)$ in the differentiable structure of $M$ (see Tu Subsection 5.3 ), where $x^i=r^i \circ \varphi$ , where $r^1, ..., r^n$ are the standard coordinates on $\mathbb R^n$ , $(U,\varphi)$ is said to be positively oriented if the frame $\{\frac{\partial}{\partial x^1}, ..., \frac{\partial}{\partial x^n}\}$ is positively oriented. I think there's no explicit concept of ""manifold with boundary"" or ""frame"" in From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave so far, and so if we were to adopt this definition, ""if the frame $\{\frac{\partial}{\partial x^1}, ..., \frac{\partial}{\partial x^n}\}$ is positively oriented"" would be translated to ""if each element of the set $\{\frac{\partial}{\partial x^1}|_p, ..., \frac{\partial}{\partial x^n}|_p\}_{p \in M}$ is positively oriented"". Since each element is a basis of the tangent space $T_pM$ , based on Tu Subsection 21.3 (Tu says it was in Subsection 12.5, but I'm not sure that was explicit unless Subsection 12.5 was understood in the context of Proposition 8.9 ), and this is indeed defined after Definition 9.8","My book is From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave. I recently finished most of An Introduction to Manifolds by Loring W. Tu, so based on the preface of From Calculus to Cohomology, I started at Chapter 8. I don't believe I've missed anything since charts are first introduced in Chapter 8. Question : What's a positively oriented chart, first mentioned in Proposition 10.2 , please? Some context : I think this is relevant in answering my other question: Why is there a form with compact support on a connected oriented manifold with integral one but with support contained in a given open proper subset? I think I have to prove either the chart or some restriction , open in , is a ""positively oriented chart"" or at least an ""oriented chart"" in order to apply Proposition 10.2 My guesses : The definition of ""oriented chart"" in the book (see also previous definitions of orientation ) is meant to be ""positively oriented chart"" with ""negatively oriented chart"" to be for orientation-reversing. I mean that 1.1. a chart is an oriented chart if and only if it is a member of an oriented atlas of an oriented smooth -dimensional manifold, and we sometimes omit and and call , the coordinate map, an oriented chart (instead of something like ""oriented map"") 1.2 An oriented chart , or just , is positively oriented if and only if is an orientation-preserving diffeomorphism if and only if if and only if is an orientation-preserving diffeomorphism of manifolds  (See here and here ) if and only if is an orientation-preserving vector space isomorphism of tangent spaces In Proposition 10.2 , what is meant by ""positively oriented chart"" is simply ""oriented chart"" if we go with the convention that ""oriented charts"" are ""positively oriented charts"", as originally in the book. I also tried looking up other books: An Introduction to Manifolds by Loring W. Tu: Based on Section 21.5 and Subsection 23.4 , I believe the definition for integration is for a chart in an ""oriented atlas"" of , where an ""oriented atlas"" is defined one where overlapping charts have positive Jacobian determinant. Thus, ""oriented atlas"" in An Introduction to Manifolds seems to be the same as ""positive atlas"" in From Calculus to Cohomology. Manifolds, Tensor Analysis, and Applications by Ralph Abraham, Jerrold E. Marsden, Tudor Ratiu : It seems a coordinate chart is defined as positively oriented if the coordinate chart's coordinate map has all its differentials to be orientation preserving ( as in vector spaces or as in manifolds, if we still have such equivalence of the 2 notions of orientation preserving ). If this is what is meant, then to clarify, do we, once again, have a notion, namely the notion of positively oriented chart, that is actually rooted in some prerequisite algebra notion ? I'm not sure this is (exactly) what Madsen and Tornehave mean because there is a difference in definition for manifolds. Update : Based on the proof of Theorem 11.9 , which relies on Lemma 11.8 , I think this might be the definition or at least equivalent to, implied by or implies the definition. Introduction to Smooth Manifolds by John M. Lee: It seems the definition is that for an oriented smooth -manifold with or without boundary, for a coordinate chart in the differentiable structure of (see Tu Subsection 5.3 ), where , where are the standard coordinates on , is said to be positively oriented if the frame is positively oriented. I think there's no explicit concept of ""manifold with boundary"" or ""frame"" in From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave so far, and so if we were to adopt this definition, ""if the frame is positively oriented"" would be translated to ""if each element of the set is positively oriented"". Since each element is a basis of the tangent space , based on Tu Subsection 21.3 (Tu says it was in Subsection 12.5, but I'm not sure that was explicit unless Subsection 12.5 was understood in the context of Proposition 8.9 ), and this is indeed defined after Definition 9.8","(U, g: U \to g(U) = U') (W, g|_W:W \to g(W)) W U (U,h:U \to U') n U U' h (U,h:U \to U') h h:U \to U' \det(D_q(h)) > 0 D_qh: T_qU = T_qM \to T_{h(q)}U' = T_{h(q)} \mathbb R^n D_qh: T_qU = T_qM \to T_{h(q)}U' = T_{h(q)} \mathbb R^n M n M (U,\varphi) = (U,x^1,...,x^n) M x^i=r^i \circ \varphi r^1, ..., r^n \mathbb R^n (U,\varphi) \{\frac{\partial}{\partial x^1}, ..., \frac{\partial}{\partial x^n}\} \{\frac{\partial}{\partial x^1}, ..., \frac{\partial}{\partial x^n}\} \{\frac{\partial}{\partial x^1}|_p, ..., \frac{\partial}{\partial x^n}|_p\}_{p \in M} T_pM","['linear-algebra', 'general-topology']"
45,Find a basis $A$ of space $\mathbb R^{4}$ and basis $B$ of space $\mathbb R^{3}$,Find a basis  of space  and basis  of space,A \mathbb R^{4} B \mathbb R^{3},"Let $\varphi: \mathbb R^{4} \rightarrow \mathbb R^{3}$ : $$\varphi(x_{1},x_{2},x_{3},x_{4})=(x_{1}+x_{3}+x_{4},x_{1}+x_{2}+2x_{3}+3x_{4},x_{1}-x_{2}-x_{4})$$ Find a basis $A$ of space $\mathbb R^{4}$ and basis $B$ of space $\mathbb R^{3}$ such that $M(\varphi)^{B}_{A}={\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&0&0\end{bmatrix}}$ I think that I know how to do this task. However I really need an assessment if it is correct. My try: $M(\varphi)^{st}_{st}={\begin{bmatrix}1&0&1&1\\1&1&2&3\\1&-1&0&-1\end{bmatrix}}$ After elementary operations on the matrix $M(\varphi)^{st}_{st}$ I have system of equations: $\begin{cases}  x_{1}+x_{3}+x_{4}=0 \\  x_{2}+x_{3}+2x_{4}=0\end{cases}$ So $\ker \varphi=lin\left\{(-1,-1,1,0),(-1,-2,0,1)\right\}$ . This vectors are in a basis $A$ but I need two more vectors. $\dim(\ker\varphi)=2$ so $\dim(im \varphi)=2$ and it can be: $(1,1,1),(0,1,-1)$ (I take $2$ linearly independent vectors from the matrix $M(\varphi)^{st}_{st}$ columns). This vectors are in a basis $B$ . In this moment I have $A=\left\{\alpha_{1}, \alpha_{2},(-1,-1,1,0),(-1,-2,0,1)\right\}$ and $B=\left\{(1,1,1),(0,1,-1),\beta_{3}\right\}$ From $M(\varphi)^{B}_{A}$ I have: $\varphi(\alpha_{1})=1 \cdot \beta_{1}$ $\varphi(\alpha_{2})=1 \cdot \beta_{2}$ $\varphi(\alpha_{3})=0$ $\varphi(\alpha_{4})=0$ That is why: $\varphi(\alpha_{1})=(1,1,1)$ and $\alpha_{1}=(1,0,0,0)$ $\varphi(\alpha_{2})=(0,1,-1)$ and $\alpha_{2}=(0,1,0,0)$ $\beta_{3}$ I can choose anyway because I do not have any dependencies on it. It can be linerly indipendent from other basis vectors, so it can be $\beta=(0,0,1)$ I will be extremely grateful for checking this solution and indicating any errors.","Let : Find a basis of space and basis of space such that I think that I know how to do this task. However I really need an assessment if it is correct. My try: After elementary operations on the matrix I have system of equations: So . This vectors are in a basis but I need two more vectors. so and it can be: (I take linearly independent vectors from the matrix columns). This vectors are in a basis . In this moment I have and From I have: That is why: and and I can choose anyway because I do not have any dependencies on it. It can be linerly indipendent from other basis vectors, so it can be I will be extremely grateful for checking this solution and indicating any errors.","\varphi: \mathbb R^{4} \rightarrow \mathbb R^{3} \varphi(x_{1},x_{2},x_{3},x_{4})=(x_{1}+x_{3}+x_{4},x_{1}+x_{2}+2x_{3}+3x_{4},x_{1}-x_{2}-x_{4}) A \mathbb R^{4} B \mathbb R^{3} M(\varphi)^{B}_{A}={\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&0&0\end{bmatrix}} M(\varphi)^{st}_{st}={\begin{bmatrix}1&0&1&1\\1&1&2&3\\1&-1&0&-1\end{bmatrix}} M(\varphi)^{st}_{st} \begin{cases} 
x_{1}+x_{3}+x_{4}=0 \\ 
x_{2}+x_{3}+2x_{4}=0\end{cases} \ker \varphi=lin\left\{(-1,-1,1,0),(-1,-2,0,1)\right\} A \dim(\ker\varphi)=2 \dim(im \varphi)=2 (1,1,1),(0,1,-1) 2 M(\varphi)^{st}_{st} B A=\left\{\alpha_{1}, \alpha_{2},(-1,-1,1,0),(-1,-2,0,1)\right\} B=\left\{(1,1,1),(0,1,-1),\beta_{3}\right\} M(\varphi)^{B}_{A} \varphi(\alpha_{1})=1 \cdot \beta_{1} \varphi(\alpha_{2})=1 \cdot \beta_{2} \varphi(\alpha_{3})=0 \varphi(\alpha_{4})=0 \varphi(\alpha_{1})=(1,1,1) \alpha_{1}=(1,0,0,0) \varphi(\alpha_{2})=(0,1,-1) \alpha_{2}=(0,1,0,0) \beta_{3} \beta=(0,0,1)",['linear-algebra']
46,"If A is a real non-singular square matrix, then there exists a real matrix $B$ such that $e^B$ $=$ $A^2$","If A is a real non-singular square matrix, then there exists a real matrix  such that",B e^B = A^2,"If we consider the matrix exponential map on $M_n(\mathbb R)$ , then what will be the image set of the exponential map? I have seen this . From there I can say that $exp(M_n(\mathbb C))=GL_n(\mathbb C)$ . But what about the set $exp(M_n(\mathbb R))$ ? In the exercise 66 of this link I have $exp(M_n(\mathbb R))\neq GL_n(\mathbb R)$ . Even it says that : If A is a real non-singular square matrix, then there exists a real matrix B such that $e^B=A^2$ How can I prove this statement?  Any help please. Thanks in advance.","If we consider the matrix exponential map on , then what will be the image set of the exponential map? I have seen this . From there I can say that . But what about the set ? In the exercise 66 of this link I have . Even it says that : If A is a real non-singular square matrix, then there exists a real matrix B such that How can I prove this statement?  Any help please. Thanks in advance.",M_n(\mathbb R) exp(M_n(\mathbb C))=GL_n(\mathbb C) exp(M_n(\mathbb R)) exp(M_n(\mathbb R))\neq GL_n(\mathbb R) e^B=A^2,"['linear-algebra', 'matrices', 'group-theory', 'matrix-exponential']"
47,Non-orthogonal invariant subspaces,Non-orthogonal invariant subspaces,,"Let $\Gamma\subset\mathrm O(\Bbb R^n)$ be a finite group of orthogonal matrices. Let $U_1,U_2\subseteq\Bbb R^n$ be two irreducible invariant subspaces w.r.t. $\Gamma$ with $U_1\cap U_2=\{0\}$ , which are not orthogonal to each other, i.e. there are $u_i\in U_i$ with $\langle u_1,u_2\rangle \not=0$ . I was sceptic about the existence of such, but you can find examples here in a previous question of mine. Thinking a bit about such subspaces, I came to the following question: Question: Is it true, that: $\dim U_1=\dim U_2=:d$ . Every other $d$ -dimensional subspace $U\subset U_1\oplus U_2$ with $U\cap U_i=\{0\}$ is an irreducible invariant subspace as well. There are two orthogonal $d$ -dimensional irreducible invariant subspaces $\bar U_1,\bar U_2\subset U_1\oplus U_2$ . Update The second statement is not correct, but should be substituted by a different one. One version was given in the answer of Joppy . I can also think about something like this: every $u\in U_1\oplus U_2\setminus\{0\}$ is contained in exactly one $d$ -dimensional irreducible invariant subspace $U\subset U_1\oplus U_2$ .","Let be a finite group of orthogonal matrices. Let be two irreducible invariant subspaces w.r.t. with , which are not orthogonal to each other, i.e. there are with . I was sceptic about the existence of such, but you can find examples here in a previous question of mine. Thinking a bit about such subspaces, I came to the following question: Question: Is it true, that: . Every other -dimensional subspace with is an irreducible invariant subspace as well. There are two orthogonal -dimensional irreducible invariant subspaces . Update The second statement is not correct, but should be substituted by a different one. One version was given in the answer of Joppy . I can also think about something like this: every is contained in exactly one -dimensional irreducible invariant subspace .","\Gamma\subset\mathrm O(\Bbb R^n) U_1,U_2\subseteq\Bbb R^n \Gamma U_1\cap U_2=\{0\} u_i\in U_i \langle u_1,u_2\rangle \not=0 \dim U_1=\dim U_2=:d d U\subset U_1\oplus U_2 U\cap U_i=\{0\} d \bar U_1,\bar U_2\subset U_1\oplus U_2 u\in U_1\oplus U_2\setminus\{0\} d U\subset U_1\oplus U_2","['linear-algebra', 'matrices', 'group-theory', 'representation-theory', 'invariant-subspace']"
48,"If $V = \text{null}(T-\lambda I) \oplus \text{range}(T-\lambda I)$, then $T$ is diagonalizable?","If , then  is diagonalizable?",V = \text{null}(T-\lambda I) \oplus \text{range}(T-\lambda I) T,"$V$ is a finite-dimensional complex vector space and $T \in L(V)$ ( $L(V)$ is the set of all linear maps from $V$ to itself), and $\lambda$ is arbitrary in $\mathbb{C}$ . I know $T$ is diagonalizable if it has $\text{dim}(V)$ distinct eigenvalues, or if $V$ has a basis consisting of eigenvectors of $T$ . I was thinking it might also have something to do with eigenspaces (there are a few theorems with diagonalizability and eigenspaces in my text), since those specifically handle $\text{null}(T-\lambda I)$ , but then that would leave out the range. I have the definition of an eigenspace, direct sum, diagonalizability, and some conditions equivalent to diagonalizability, i.e. a matrix is diagonalizable if V consists of eigenvectors of T, there exist 1-dimensional subspaces of V that are invariant under T, where V is the direct sum of these 1D subspaces, V is the direct sum of eigenspaces of T corresponding to each eigenvalue, and the dimension of V is the same as the dimension of the sum of these eigenspaces. Also, the book is ""Linear Algebra Done Right"" by Sheldon Axler. I also have that $T$ is diagonalizable if it contains $dim(V)$ distinct eigenvalues.","is a finite-dimensional complex vector space and ( is the set of all linear maps from to itself), and is arbitrary in . I know is diagonalizable if it has distinct eigenvalues, or if has a basis consisting of eigenvectors of . I was thinking it might also have something to do with eigenspaces (there are a few theorems with diagonalizability and eigenspaces in my text), since those specifically handle , but then that would leave out the range. I have the definition of an eigenspace, direct sum, diagonalizability, and some conditions equivalent to diagonalizability, i.e. a matrix is diagonalizable if V consists of eigenvectors of T, there exist 1-dimensional subspaces of V that are invariant under T, where V is the direct sum of these 1D subspaces, V is the direct sum of eigenspaces of T corresponding to each eigenvalue, and the dimension of V is the same as the dimension of the sum of these eigenspaces. Also, the book is ""Linear Algebra Done Right"" by Sheldon Axler. I also have that is diagonalizable if it contains distinct eigenvalues.",V T \in L(V) L(V) V \lambda \mathbb{C} T \text{dim}(V) V T \text{null}(T-\lambda I) T dim(V),"['linear-algebra', 'linear-transformations', 'direct-sum']"
49,How can a function including a sin operation be linearly transformable for any offset?,How can a function including a sin operation be linearly transformable for any offset?,,"In the paper ""Attention is all you need"" the authors have chosen a function to encode the position of a word in a sequence (section 3.5). The following encoding is chosen: $ PE(pos, 2dim) = sin(pos / 10000 ^ {2dim/d_{model}} ) $ For the purposes of this question this function can be simplified to: $ PE(pos) = sin(pos) $ The text states that ""for any fixed offset $k$ , $PE(pos+k)$ can be represented as a linear function of $PE(pos)$ "". This did not seem obvious due to me due to the nonlinearity of the sine function. Other resources like Attention is all you need Explained mention this property but do not go deeper into it. I attempted to use linear regression techniques in Python to derive this, but  was unable to find a fitting linear transform. As $k$ increases and the sine waves resulting from the $PE(pos)$ function get out of sync, the correlation of the transformation and the truth decreases. Did I misapprehend the statement in the paper, or is my code or understanding of the underlying math here faulty?","In the paper ""Attention is all you need"" the authors have chosen a function to encode the position of a word in a sequence (section 3.5). The following encoding is chosen: For the purposes of this question this function can be simplified to: The text states that ""for any fixed offset , can be represented as a linear function of "". This did not seem obvious due to me due to the nonlinearity of the sine function. Other resources like Attention is all you need Explained mention this property but do not go deeper into it. I attempted to use linear regression techniques in Python to derive this, but  was unable to find a fitting linear transform. As increases and the sine waves resulting from the function get out of sync, the correlation of the transformation and the truth decreases. Did I misapprehend the statement in the paper, or is my code or understanding of the underlying math here faulty?"," PE(pos, 2dim) = sin(pos / 10000 ^ {2dim/d_{model}} )   PE(pos) = sin(pos)  k PE(pos+k) PE(pos) k PE(pos)","['linear-algebra', 'geometry', 'linear-transformations', 'nonlinear-analysis']"
50,"If $Q$ is a proper orthogonal transformation matrix, deduce that $\det(1-Q)=0$.","If  is a proper orthogonal transformation matrix, deduce that .",Q \det(1-Q)=0,"Show that if $Q$ is orthogonal transformation matrix, then $Q^t(Q-1)=(1-Q)^t$ . Deduce that if $Q$ is also proper, then $\det(1-Q)=0$ . Hence show that transformation has nonzero vector that has the same components in both coordinate system. I tried to solve this problem.I think I got the first part right, $$Q^t(Q -1)= Q^t  Q- Q^t=1- Q^t=(1- Q)^t$$ The second part, $$-Q ^t(1-Q)=(1-Q)^t$$ $$\det(-Q^t)\det(1-Q)=\det((1-Q)^t$$ $$(-1)^n\det(Q)\det(1-Q)=\det((1-Q)^t)$$ since the orthogonal matrix is proper which means $\det(Q)=1$ and for any matrix, its determinant equals the determinant of its transpose. $$(-1)^n\det(1-Q)=\det(1-Q)$$ So, it's always true for $\det(1-Q)=0$ But that's not what the question asks. I haven't done linear algebra for a while and I am not sure from the concepts I used, so I would be glad if you clarify any mistake I made.","Show that if is orthogonal transformation matrix, then . Deduce that if is also proper, then . Hence show that transformation has nonzero vector that has the same components in both coordinate system. I tried to solve this problem.I think I got the first part right, The second part, since the orthogonal matrix is proper which means and for any matrix, its determinant equals the determinant of its transpose. So, it's always true for But that's not what the question asks. I haven't done linear algebra for a while and I am not sure from the concepts I used, so I would be glad if you clarify any mistake I made.",Q Q^t(Q-1)=(1-Q)^t Q \det(1-Q)=0 Q^t(Q -1)= Q^t  Q- Q^t=1- Q^t=(1- Q)^t -Q ^t(1-Q)=(1-Q)^t \det(-Q^t)\det(1-Q)=\det((1-Q)^t (-1)^n\det(Q)\det(1-Q)=\det((1-Q)^t) \det(Q)=1 (-1)^n\det(1-Q)=\det(1-Q) \det(1-Q)=0,"['linear-algebra', 'orthogonality']"
51,Invert a symmetric banded matrix,Invert a symmetric banded matrix,,"I am interested in inverting a symmetric banded matrix with the following structure: \begin{equation}\mathbf A(\epsilon)= \left(\begin{array}{*{20}c} 2+\epsilon	&0	&  -1&0	&0&0& -1&0\\ 0	&2+\epsilon	& 0 &-1	&0&0&0&-1\\ -1	&0	& 2+\epsilon&0	& -1&0&0&0\\ 0	&-1	& 0&2+\epsilon	&0& -1&0&0\\ 0	&0	& -1&0	&2+\epsilon&0& -1&0\\ 0	&0	& 0& -1	&0&2+\epsilon&0& -1\\ -1	&0	& 0&0	& -1&0&2+\epsilon&0\\ 0	&-1	& 0&0	&0& -1&0&2+\epsilon\\ \end{array}\right) \end{equation} for an arbitrary separation between the bands. We consider here $\epsilon>0$ . The matrix would become singular for $\epsilon\to 0$ (as the sum of the elements of each row becomes zero). Is it possible to determine analytically $\mathbf A^{-1}(\epsilon)$ ? If so, how? Here is what I have been trying so far. I decomposed $\mathbf A$ in two (upper $\mathbf A_U$ and lower $\mathbf A_L$ ) triangular matrices which are both invertible if one splits the main diagonal in two symmetric contribitions with positive entries. I then tried to use some results to deal with the inverse of $(\mathbf A_U+\mathbf A_L)^{-1}$ . Moreover, I tried to reduce the inverse of this sum in the form $(\mathbf B+\mathbb I)^{-1}$ for a matrix $||\mathbf B||\ll1$ in order to expand the inverse of the last sum in series and at least obtaining a perurbative expression of the inverse.","I am interested in inverting a symmetric banded matrix with the following structure: for an arbitrary separation between the bands. We consider here . The matrix would become singular for (as the sum of the elements of each row becomes zero). Is it possible to determine analytically ? If so, how? Here is what I have been trying so far. I decomposed in two (upper and lower ) triangular matrices which are both invertible if one splits the main diagonal in two symmetric contribitions with positive entries. I then tried to use some results to deal with the inverse of . Moreover, I tried to reduce the inverse of this sum in the form for a matrix in order to expand the inverse of the last sum in series and at least obtaining a perurbative expression of the inverse.","\begin{equation}\mathbf A(\epsilon)=
\left(\begin{array}{*{20}c}
2+\epsilon	&0	&  -1&0	&0&0& -1&0\\
0	&2+\epsilon	& 0 &-1	&0&0&0&-1\\
-1	&0	& 2+\epsilon&0	& -1&0&0&0\\
0	&-1	& 0&2+\epsilon	&0& -1&0&0\\
0	&0	& -1&0	&2+\epsilon&0& -1&0\\
0	&0	& 0& -1	&0&2+\epsilon&0& -1\\
-1	&0	& 0&0	& -1&0&2+\epsilon&0\\
0	&-1	& 0&0	&0& -1&0&2+\epsilon\\
\end{array}\right)
\end{equation} \epsilon>0 \epsilon\to 0 \mathbf A^{-1}(\epsilon) \mathbf A \mathbf A_U \mathbf A_L (\mathbf A_U+\mathbf A_L)^{-1} (\mathbf B+\mathbb I)^{-1} ||\mathbf B||\ll1","['linear-algebra', 'matrices']"
52,"If $TS=ST$, then $S=\alpha T+\beta$.","If , then .",TS=ST S=\alpha T+\beta,"Let $T=\begin{pmatrix}a&b\\c&d\end{pmatrix}$ be a non-scalar matrix. If $S=\begin{pmatrix}e&f\\g&h\end{pmatrix}$ be such that $TS=ST$ . Why there exists $\alpha,\beta\in \mathbb{C}$ such that $$S=\alpha T+\beta I\;?$$ Note that $TS-ST=0$ is equivalent to $$\begin{bmatrix}bg-fc & af+bh-eb-fd\\ ce+dg-ga-hc & fc-bg\end{bmatrix} = \begin{bmatrix}0 & 0\\0 & 0\end{bmatrix}$$ This implies that $$\begin{cases} bg-fc = 0,\\ af+bh-eb-fd = 0,\\ ce+dg-ga-hc = 0,\\ fc-bg = 0. \end{cases}$$ Since $T$ is non scalar, then $b\neq 0$ or $c\neq 0$ or $a\neq d$ . However, I cannot find $\alpha$ and $\beta$ .","Let be a non-scalar matrix. If be such that . Why there exists such that Note that is equivalent to This implies that Since is non scalar, then or or . However, I cannot find and .","T=\begin{pmatrix}a&b\\c&d\end{pmatrix} S=\begin{pmatrix}e&f\\g&h\end{pmatrix} TS=ST \alpha,\beta\in \mathbb{C} S=\alpha T+\beta I\;? TS-ST=0 \begin{bmatrix}bg-fc & af+bh-eb-fd\\
ce+dg-ga-hc & fc-bg\end{bmatrix} = \begin{bmatrix}0 & 0\\0 & 0\end{bmatrix} \begin{cases}
bg-fc = 0,\\
af+bh-eb-fd = 0,\\
ce+dg-ga-hc = 0,\\
fc-bg = 0.
\end{cases} T b\neq 0 c\neq 0 a\neq d \alpha \beta",['linear-algebra']
53,Singular value inequality for sum of 2 matrices,Singular value inequality for sum of 2 matrices,,"I found a theorem mentioned in a couple of places, but could not find a proof.  The theorem states the following: Let $A, B \in \mathbb{F^{m,n}}$ , $p=min(m,n)$ with singular values $\sigma_1(A) \geqslant...\geqslant \sigma_p(A)$ and $\sigma_i(B) \geqslant...\geqslant \sigma_p(B)$ respectively, then $\sigma_{i+j-1}(A+B) \leqslant \sigma_i(A) + \sigma_j(B)$ . I am looking for a proof of the above. Thanks in advance.","I found a theorem mentioned in a couple of places, but could not find a proof.  The theorem states the following: Let , with singular values and respectively, then . I am looking for a proof of the above. Thanks in advance.","A, B \in \mathbb{F^{m,n}} p=min(m,n) \sigma_1(A) \geqslant...\geqslant \sigma_p(A) \sigma_i(B) \geqslant...\geqslant \sigma_p(B) \sigma_{i+j-1}(A+B) \leqslant \sigma_i(A) + \sigma_j(B)","['linear-algebra', 'inequality', 'singular-values']"
54,Is it possible to recognize when an endomorphism of a finite dimensional vector space is unitary for some choice of inner product?,Is it possible to recognize when an endomorphism of a finite dimensional vector space is unitary for some choice of inner product?,,"Let $V$ a finite dimensional vector space over $\mathbb{C}$ . Let $T\in GL(V)$ . Are there reasonable criteria for recognizing whether or not there is some inner product on $V$ w.r.t. to which $T$ is unitary? (equivalently, whether or not $T$ is similar to a unitary operator?)","Let a finite dimensional vector space over . Let . Are there reasonable criteria for recognizing whether or not there is some inner product on w.r.t. to which is unitary? (equivalently, whether or not is similar to a unitary operator?)",V \mathbb{C} T\in GL(V) V T T,['linear-algebra']
55,How to show that $\dim\ker(AB) \le \dim \ker A + \dim \ker B $?,How to show that ?,\dim\ker(AB) \le \dim \ker A + \dim \ker B ,"I want to show that $$ \dim \ker(AB) \le \dim \ker A + \dim \ker B. $$ My problem I thought that I can do that in this way: Let consider $x \in\ker B$ $$Bx  = 0$$ Let multiply this from left side by $A$ and we get: $$ABx = 0$$ so $$\ker B \subset\ker AB $$ so $$\dim \ker(B) \le \dim\ker AB$$ We can do the same thing with $\ker A$ let consider $ \vec{y} \in \operatorname{im}(AB) $ so $$ y = (AB)x $$ what is equivalent to $$ \vec{y} = A(B\vec{x}) = A\vec{w} $$ So $$ \vec{y} \in \operatorname{im}(AB) \rightarrow \vec{y} \in \operatorname{im}(A)$$ so $$ \operatorname{rank} AB \le  \operatorname{rank} A \leftrightarrow  \dim \ker A \le \dim \ker AB $$ But I am not sure what I should do later... edited I have seen this post $A, B$ are linear map and dim$null(A) = 3$, dim$null(B) = 5$ what about dim$null(AB)$ but I haven't got nothing like $\operatorname{im}(A|_{\operatorname{im}(B)})$ on my algebra lecture and I can't use that so I search for another proof (or similar without this trick)","I want to show that My problem I thought that I can do that in this way: Let consider Let multiply this from left side by and we get: so so We can do the same thing with let consider so what is equivalent to So so But I am not sure what I should do later... edited I have seen this post $A, B$ are linear map and dim$null(A) = 3$, dim$null(B) = 5$ what about dim$null(AB)$ but I haven't got nothing like on my algebra lecture and I can't use that so I search for another proof (or similar without this trick)", \dim \ker(AB) \le \dim \ker A + \dim \ker B.  x \in\ker B Bx  = 0 A ABx = 0 \ker B \subset\ker AB  \dim \ker(B) \le \dim\ker AB \ker A  \vec{y} \in \operatorname{im}(AB)   y = (AB)x   \vec{y} = A(B\vec{x}) = A\vec{w}   \vec{y} \in \operatorname{im}(AB) \rightarrow \vec{y} \in \operatorname{im}(A)  \operatorname{rank} AB \le  \operatorname{rank} A \leftrightarrow  \dim \ker A \le \dim \ker AB  \operatorname{im}(A|_{\operatorname{im}(B)}),"['linear-algebra', 'vector-spaces']"
56,Eigenvalues of a matrix product,Eigenvalues of a matrix product,,it is a well know result that for two $n\times n$ matrices $A$ and $B$ the eigenvalues of the product $AB$ are equal to those of the product $BA$ . Are there similar results for a product of $m>2$ matrices? E.g. does the product of four $n\times n$ matrices $ABCD$ have the same eigenvalues as $ACBD$ ? Thanks for your help!,it is a well know result that for two matrices and the eigenvalues of the product are equal to those of the product . Are there similar results for a product of matrices? E.g. does the product of four matrices have the same eigenvalues as ? Thanks for your help!,n\times n A B AB BA m>2 n\times n ABCD ACBD,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
57,Classification of $n\times n$ real matrices up to congruence,Classification of  real matrices up to congruence,n\times n,"As we known, for matrix similarity $A= P^{-1}B P$ , we can classify equivalence class in $M_n (\mathbb{C})$ by Jordan Canonical Form . Matrix congruence , $A = P^T B P$ with $P$ invertible, is also an equivalence relation. How does one classify the equivalence classes in $M_n(\mathbb{R})$ under this relation? For symmetric real matrices, it's easy, since we can use orthogonal transformation to diagonalize it, which gives both matrix similarity and matrix congruence. Then we use a scaling matrix to make the diagonal elements to be $\pm 1$ and $0$ . Therefore, for a symmetric real matrix, we classify the matrix by the number of $\pm1$ and $0$ . What about for general real matrices?","As we known, for matrix similarity , we can classify equivalence class in by Jordan Canonical Form . Matrix congruence , with invertible, is also an equivalence relation. How does one classify the equivalence classes in under this relation? For symmetric real matrices, it's easy, since we can use orthogonal transformation to diagonalize it, which gives both matrix similarity and matrix congruence. Then we use a scaling matrix to make the diagonal elements to be and . Therefore, for a symmetric real matrix, we classify the matrix by the number of and . What about for general real matrices?",A= P^{-1}B P M_n (\mathbb{C}) A = P^T B P P M_n(\mathbb{R}) \pm 1 0 \pm1 0,"['linear-algebra', 'abstract-algebra', 'matrix-analysis']"
58,"Does the functor $V\mapsto V^{**}/V$ (for $\dim V=\infty$) reflect monomorphisms, epimorphisms or isomorphisms?","Does the functor  (for ) reflect monomorphisms, epimorphisms or isomorphisms?",V\mapsto V^{**}/V \dim V=\infty,"Let $\textbf{Vect}_\infty$ be the category of infinite dimensional vector spaces (over some fixed ground field), and consider the endofunctor $F$ of $\textbf{Vect}_\infty$ whose effect on objects is given by $F(V):=V^{**}/V$ , and whose effect on morphisms is the obvious one. (Here $V^{**}/V$ is the cokernel of the canonical monomorphism $V\to V^{**}$ .) Does $F$ reflect monomorphisms? Does it reflect epimorphisms? Does it reflect isomorphisms? (Recall the $F$ reflect monomorphisms if the condition that $F(f)$ is a monomorphism implies that $f$ is also a monomorphism. The reflections of epimorphisms and isomorphisms are defined similarly.)","Let be the category of infinite dimensional vector spaces (over some fixed ground field), and consider the endofunctor of whose effect on objects is given by , and whose effect on morphisms is the obvious one. (Here is the cokernel of the canonical monomorphism .) Does reflect monomorphisms? Does it reflect epimorphisms? Does it reflect isomorphisms? (Recall the reflect monomorphisms if the condition that is a monomorphism implies that is also a monomorphism. The reflections of epimorphisms and isomorphisms are defined similarly.)",\textbf{Vect}_\infty F \textbf{Vect}_\infty F(V):=V^{**}/V V^{**}/V V\to V^{**} F F F(f) f,"['linear-algebra', 'category-theory', 'functors']"
59,A simple proof of this equality: $\det_{\mu\nu}\left(\frac 1 2 \text{Tr}\left[A\sigma_\mu A^\dagger \sigma_\nu \right]\right)=1$,A simple proof of this equality:,\det_{\mu\nu}\left(\frac 1 2 \text{Tr}\left[A\sigma_\mu A^\dagger \sigma_\nu \right]\right)=1,"Question Let $A\in \text{SL}(2,\mathbb{C})$ , so $\det(A)=1$ . Define the following (Pauli) matrices: $$\begin{align} \sigma_0=\begin{pmatrix}-1 & 0 \\ 0 & -1 \end{pmatrix} & &\sigma_1=\begin{pmatrix}0 & 1 \\ 1 & 0 \end{pmatrix} & \\ &\\ \sigma_2=\begin{pmatrix}0 & -i \\ i & 0 \end{pmatrix} & &\sigma_3=\begin{pmatrix}1 & 0 \\ 0 & -1 \end{pmatrix} &  \end{align} $$ Now define the following $4\times 4$ matrix. $$L_{\mu\nu}\equiv \frac{1}{2}\text{Tr}\left[A\sigma_\mu A^\dagger \sigma_\nu\right]$$ What I am trying to prove is $\det (L)=1$ , that's it. But I am having so much trouble. I have verified that it is true via Mathematica. The following is know exactly: $$\det (L)=\left|\det(A)\right|^4=1$$ Actually, it would be sufficient for my current purposes to show $\det(L)\geq 0$ , but even that is very hard for me to show. Mathematica Code. ClearAll[s0, s1, s2, s3, s, A]; (* Define the Pauli-Matrices and A \ matrix *) s0 = {{-1, 0}, {0, -1}}; s1 = {{0, 1}, {1, 0}}; s2 = {{0, -I}, {I, 0}}; s3 = {{1, 0}, {0, -1}}; s = {s0, s1, s2, s3}; A = {{a, b}, {c, d}};  L00 = 1/2*   Tr[A.s0.A\[ConjugateTranspose].s0];  (* Define L(A) through the \ equation LSubscript[(A)^\[Mu], \[Nu]] = 1/2Tr[Subscript[A\[Sigma], \ \[Mu]]A\[ConjugateTranspose]Subscript[\[Sigma], \[Nu]]] *) L01 =   1/2*Tr[A.s0.A\[ConjugateTranspose].s1]; L02 = 1/2*Tr[A.s0.A\[ConjugateTranspose].s2]; L03 = 1/2*Tr[A.s0.A\[ConjugateTranspose].s3]; L10 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s0]; L11 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s1]; L12 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s2]; L13 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s3]; L20 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s0]; L21 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s1]; L22 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s2]; L23 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s3]; L30 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s0]; L31 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s1]; L32 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s2]; L33 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s3]; L = {    {L00, L01, L02, L03},    {L10, L11, L12, L13},    {L20, L21, L22, L23},    {L30, L31, L32, L33}    };  TraditionalForm[  FullSimplify[   Det[L]]]  (* Evaluate the determinant explicitly, and put it in \ legible form *)","Question Let , so . Define the following (Pauli) matrices: Now define the following matrix. What I am trying to prove is , that's it. But I am having so much trouble. I have verified that it is true via Mathematica. The following is know exactly: Actually, it would be sufficient for my current purposes to show , but even that is very hard for me to show. Mathematica Code. ClearAll[s0, s1, s2, s3, s, A]; (* Define the Pauli-Matrices and A \ matrix *) s0 = {{-1, 0}, {0, -1}}; s1 = {{0, 1}, {1, 0}}; s2 = {{0, -I}, {I, 0}}; s3 = {{1, 0}, {0, -1}}; s = {s0, s1, s2, s3}; A = {{a, b}, {c, d}};  L00 = 1/2*   Tr[A.s0.A\[ConjugateTranspose].s0];  (* Define L(A) through the \ equation LSubscript[(A)^\[Mu], \[Nu]] = 1/2Tr[Subscript[A\[Sigma], \ \[Mu]]A\[ConjugateTranspose]Subscript[\[Sigma], \[Nu]]] *) L01 =   1/2*Tr[A.s0.A\[ConjugateTranspose].s1]; L02 = 1/2*Tr[A.s0.A\[ConjugateTranspose].s2]; L03 = 1/2*Tr[A.s0.A\[ConjugateTranspose].s3]; L10 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s0]; L11 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s1]; L12 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s2]; L13 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s3]; L20 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s0]; L21 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s1]; L22 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s2]; L23 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s3]; L30 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s0]; L31 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s1]; L32 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s2]; L33 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s3]; L = {    {L00, L01, L02, L03},    {L10, L11, L12, L13},    {L20, L21, L22, L23},    {L30, L31, L32, L33}    };  TraditionalForm[  FullSimplify[   Det[L]]]  (* Evaluate the determinant explicitly, and put it in \ legible form *)","A\in \text{SL}(2,\mathbb{C}) \det(A)=1 \begin{align}
\sigma_0=\begin{pmatrix}-1 & 0 \\ 0 & -1 \end{pmatrix} & &\sigma_1=\begin{pmatrix}0 & 1 \\ 1 & 0 \end{pmatrix} & \\
&\\
\sigma_2=\begin{pmatrix}0 & -i \\ i & 0 \end{pmatrix} & &\sigma_3=\begin{pmatrix}1 & 0 \\ 0 & -1 \end{pmatrix} & 
\end{align}
 4\times 4 L_{\mu\nu}\equiv \frac{1}{2}\text{Tr}\left[A\sigma_\mu A^\dagger \sigma_\nu\right] \det (L)=1 \det (L)=\left|\det(A)\right|^4=1 \det(L)\geq 0","['linear-algebra', 'physics', 'mathematical-physics']"
60,Question with vector space over finite field,Question with vector space over finite field,,"Problem Let $\mathcal{A}$ be a finite nonempty subset of a finite vector space $\mathcal{V}$ over a Galois field $GF(s)$ ( $s$ a prime power) and let $t$ be a positive integer such that (1) any $t$ vectors in $\mathcal{A}$ are linearly independent; (2) for any $v \in \mathcal{V}$ , there exists $a_1,\ldots,a_{t-1} \in GF(s)$ and $\alpha_1,\ldots,\alpha_{t-1} \in \mathcal{A}$ such that $$ v = a_1 \alpha_1 + \cdots + a_{t-1}\alpha_{t-1} $$ Then we call $\mathcal{A}$ a basis of $\mathcal{V}$ with parameter $t$ . Suppose $\mathcal{A}_1$ and $\mathcal{A}_2$ are two basis of $\mathcal{V}$ with parameter $t$ , then does $|\mathcal{A}_1| = |\mathcal{A}_2|$ hold? Background I came across this problem when I was considering this concrete example. Let $\mathcal{V} = GF(3)^3$ and $t=3$ , then we can take $$ \mathcal{A}_1 = \{(1,0,0),(0,1,0),(0,0,1),(1,1,1)\} $$ $$ \mathcal{A}_2 = \{(1,0,0),(1,1,0),(1,1,1),(1,2,1)\} $$ And they satisfy the conditions and have the same number of elements. I don't know if we can have general results as stated above. Any hints will be appreciated. Thanks!!! Updates & Observations (1) We can create such a $\mathcal{A}$ using a stepwise procedure. First let $\mathcal{C} := \varnothing$ and $\mathcal{S} := \mathcal{V}$ . Then at each step we take a nonzero vector $x \in \mathcal{S}$ , and update $$ \mathcal{C} \leftarrow \mathcal{C} \cup \{x\}, \quad \mathcal{S} \leftarrow \mathcal{S} \backslash \{\mbox{any }t-1\mbox{ linear combinations between }x\mbox{ and elements in }\mathcal{C}\} $$ and repeat the step until $\mathcal{S} = \varnothing$ . Finally, the set $\mathcal{C}$ will be what we desire.","Problem Let be a finite nonempty subset of a finite vector space over a Galois field ( a prime power) and let be a positive integer such that (1) any vectors in are linearly independent; (2) for any , there exists and such that Then we call a basis of with parameter . Suppose and are two basis of with parameter , then does hold? Background I came across this problem when I was considering this concrete example. Let and , then we can take And they satisfy the conditions and have the same number of elements. I don't know if we can have general results as stated above. Any hints will be appreciated. Thanks!!! Updates & Observations (1) We can create such a using a stepwise procedure. First let and . Then at each step we take a nonzero vector , and update and repeat the step until . Finally, the set will be what we desire.","\mathcal{A} \mathcal{V} GF(s) s t t \mathcal{A} v \in \mathcal{V} a_1,\ldots,a_{t-1} \in GF(s) \alpha_1,\ldots,\alpha_{t-1} \in \mathcal{A} 
v = a_1 \alpha_1 + \cdots + a_{t-1}\alpha_{t-1}
 \mathcal{A} \mathcal{V} t \mathcal{A}_1 \mathcal{A}_2 \mathcal{V} t |\mathcal{A}_1| = |\mathcal{A}_2| \mathcal{V} = GF(3)^3 t=3 
\mathcal{A}_1 = \{(1,0,0),(0,1,0),(0,0,1),(1,1,1)\}
 
\mathcal{A}_2 = \{(1,0,0),(1,1,0),(1,1,1),(1,2,1)\}
 \mathcal{A} \mathcal{C} := \varnothing \mathcal{S} := \mathcal{V} x \in \mathcal{S} 
\mathcal{C} \leftarrow \mathcal{C} \cup \{x\},
\quad
\mathcal{S} \leftarrow \mathcal{S} \backslash \{\mbox{any }t-1\mbox{ linear combinations between }x\mbox{ and elements in }\mathcal{C}\}
 \mathcal{S} = \varnothing \mathcal{C}","['linear-algebra', 'abstract-algebra', 'finite-fields']"
61,Column space and null space,Column space and null space,,"Let $A\in M_{5,7}(\mathbb{R})$ be a matrix such that $Ax=b$ has solution for every $b$ . I have to say what this information tells me about column- and null-space and rows of a matrix. The only thing I can think of is that column-space can have dimension $1$ to $5$ and null-space dimension can be deduced using rank-nullity theorem. Is there something more to see here?",Let be a matrix such that has solution for every . I have to say what this information tells me about column- and null-space and rows of a matrix. The only thing I can think of is that column-space can have dimension to and null-space dimension can be deduced using rank-nullity theorem. Is there something more to see here?,"A\in M_{5,7}(\mathbb{R}) Ax=b b 1 5","['linear-algebra', 'matrices', 'systems-of-equations']"
62,Classical Gram-Schmidt for matrix $A$,Classical Gram-Schmidt for matrix,A,"Let $$A=\begin{bmatrix}1 & 1 & 1\\ \epsilon & 0 & 0 \\ 0\ & \epsilon & 0 \\ 0 & 0 & \epsilon \end{bmatrix}.$$ On this page , this matrix $A$ is used to show the instability  of the classical Gram-Schmidt algorithm, using the criterion that $1+\epsilon =1$ . Furthermore, it can be shown that the output vectors from classical GS for $A$ are not orthogonal to each other. It seems that many websites briefly seem to only talk about the algorithm's shortcomings when running it on a computer. Is there any more ""general"" reasoning as to why the classical GS algorithm doesn't always produce orthonormal vectors, even ""on paper""? Is it because classical GS (in this case) does not account well for the approximation $\epsilon+1=1$ ? Would someone be able to explain this a little more in depth? Thanks","Let On this page , this matrix is used to show the instability  of the classical Gram-Schmidt algorithm, using the criterion that . Furthermore, it can be shown that the output vectors from classical GS for are not orthogonal to each other. It seems that many websites briefly seem to only talk about the algorithm's shortcomings when running it on a computer. Is there any more ""general"" reasoning as to why the classical GS algorithm doesn't always produce orthonormal vectors, even ""on paper""? Is it because classical GS (in this case) does not account well for the approximation ? Would someone be able to explain this a little more in depth? Thanks",A=\begin{bmatrix}1 & 1 & 1\\ \epsilon & 0 & 0 \\ 0\ & \epsilon & 0 \\ 0 & 0 & \epsilon \end{bmatrix}. A 1+\epsilon =1 A \epsilon+1=1,"['linear-algebra', 'numerical-linear-algebra', 'gram-schmidt']"
63,Geometric interpretation of why some matrices don't have eigenvalues,Geometric interpretation of why some matrices don't have eigenvalues,,"I don't understand how to geometrically interpret the formula $Av = \lambda v$ where $A$ is a matrix and $v, \lambda$ are the corresponding eigenvectors and eigenvalues. For instance, why does the matrix \begin{bmatrix}     0       & 1\\     -1     & 0  \end{bmatrix} not have any eigenvalues? How can I explain this, geometrically without just saying it's not the case because $\lambda^2+1=0$ doesn't have any real solutions?","I don't understand how to geometrically interpret the formula where is a matrix and are the corresponding eigenvectors and eigenvalues. For instance, why does the matrix not have any eigenvalues? How can I explain this, geometrically without just saying it's not the case because doesn't have any real solutions?","Av = \lambda v A v, \lambda \begin{bmatrix}
    0       & 1\\
    -1     & 0 
\end{bmatrix} \lambda^2+1=0","['linear-algebra', 'geometry', 'eigenvalues-eigenvectors', 'linear-transformations']"
64,Linear combination of Pauli matrices and projectors,Linear combination of Pauli matrices and projectors,,"Premise: this is exercise 2.60 of Quantum Computation and Quantum Information , by Nielsen and Chuang, where I'm currently stuck. Suppose $\vec{v}$ is any real three-dimensional unit vector, and $\sigma_{i}$ with $i \in \{1,2,3\}$ represents the Pauli matrices. Then we can define an Hermitian operator: $$ \begin{aligned} \vec{v}\cdot\vec{\sigma} &\equiv \sum_{i=1}^{3}v_{i}\sigma_{i} \\ &= v_{1} \sigma_{1} + v_{2} \sigma_{2} + v_{3} \sigma_{3}. \end{aligned} $$ Show that: $\vec{v}\cdot\vec{\sigma}$ has eigenvalues $\lambda_{\pm 1}=\pm 1$ . the projectors onto the corresponding eigenspaces are given by $P_{\pm}=(I \pm \vec{v}\cdot\vec{\sigma}) / 2$ . My straightforward idea was to develop the observable: $$ \begin{aligned} \vec{v}\cdot\vec{\sigma} &\equiv v_{1} X + v_{2} Y + v_{3} Z \\ &= v_{1} \left[\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right] + v_{2} \left[\begin{matrix} 0 & -i \\ i & 0 \end{matrix}\right] + v_{3} \left[\begin{matrix} 1 & 0 \\ 0 & -1 \end{matrix}\right] \\ &= \left[\begin{matrix} v_{3} & v_{1}-i v_{2} \\ v_{1}+i v_{2} & -v_{3} \end{matrix}\right] \end{aligned} $$ From which it is very easy to see that the eigenvalues are $\lambda_{\pm 1}=\pm 1$ and the corresponding eigenvectors are: $$ \lvert \lambda_{-1} \rangle = \left[ \begin{array}{c} -\frac{v_{1}-i v_{2}}{1+v_{3}} \\ 1 \end{array}\right]; \quad \lvert \lambda_{+1} \rangle = \left[ \begin{array}{c} \frac{v_{1}-i v_{2}}{1-v_{3}} \\ 1 \end{array}\right] $$ And then I would have constructed the projectors. My question is: from the eigenvectors I can see that the first component of $\lvert \lambda_{-1} \rangle$ is going to diverge when $v_{3}=-1$ , and also the first component of $\lvert \lambda_{+1} \rangle$ is going to diverge when $v_{3}=1$ . I'm losing generality in the definition of the vector $\vec{v}$ . This suggests my approach is wrong, and it is where I am stuck. Is there any other approach I could try to prove the given form of the projectors?","Premise: this is exercise 2.60 of Quantum Computation and Quantum Information , by Nielsen and Chuang, where I'm currently stuck. Suppose is any real three-dimensional unit vector, and with represents the Pauli matrices. Then we can define an Hermitian operator: Show that: has eigenvalues . the projectors onto the corresponding eigenspaces are given by . My straightforward idea was to develop the observable: From which it is very easy to see that the eigenvalues are and the corresponding eigenvectors are: And then I would have constructed the projectors. My question is: from the eigenvectors I can see that the first component of is going to diverge when , and also the first component of is going to diverge when . I'm losing generality in the definition of the vector . This suggests my approach is wrong, and it is where I am stuck. Is there any other approach I could try to prove the given form of the projectors?","\vec{v} \sigma_{i} i \in \{1,2,3\} 
\begin{aligned}
\vec{v}\cdot\vec{\sigma} &\equiv \sum_{i=1}^{3}v_{i}\sigma_{i} \\
&= v_{1} \sigma_{1} + v_{2} \sigma_{2} + v_{3} \sigma_{3}.
\end{aligned}
 \vec{v}\cdot\vec{\sigma} \lambda_{\pm 1}=\pm 1 P_{\pm}=(I \pm \vec{v}\cdot\vec{\sigma}) / 2 
\begin{aligned}
\vec{v}\cdot\vec{\sigma} &\equiv v_{1} X + v_{2} Y + v_{3} Z \\
&= v_{1} \left[\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right] + v_{2} \left[\begin{matrix} 0 & -i \\ i & 0 \end{matrix}\right] + v_{3} \left[\begin{matrix} 1 & 0 \\ 0 & -1 \end{matrix}\right] \\
&= \left[\begin{matrix} v_{3} & v_{1}-i v_{2} \\ v_{1}+i v_{2} & -v_{3} \end{matrix}\right]
\end{aligned}
 \lambda_{\pm 1}=\pm 1 
\lvert \lambda_{-1} \rangle = \left[ \begin{array}{c} -\frac{v_{1}-i v_{2}}{1+v_{3}} \\ 1 \end{array}\right]; \quad \lvert \lambda_{+1} \rangle = \left[ \begin{array}{c} \frac{v_{1}-i v_{2}}{1-v_{3}} \\ 1 \end{array}\right]
 \lvert \lambda_{-1} \rangle v_{3}=-1 \lvert \lambda_{+1} \rangle v_{3}=1 \vec{v}","['linear-algebra', 'quantum-mechanics', 'projection', 'quantum-computation', 'quantum-information']"
65,Proving there are as many generalized eigenvectors as algebraic multiplicity eigenvalue without the Jordan canonical form,Proving there are as many generalized eigenvectors as algebraic multiplicity eigenvalue without the Jordan canonical form,,"I'm reading some treatment of generalized eigenvectors in a differential equations book. They want to derive that there are as many generalized eigenvectors to a certain eigenvalue $\lambda_i$, as the algebraic multiplicity of this eigenvalue $\lambda_i$. This result is used to proof that there is a basis transformation putting the matrix in the Jordan canonical form. It goes as follows: let $p(x)=\prod_{i=1}^q(x-\lambda_i)^{m_i}$ be the characteristic polynomial of some $\mathbf{A}\in\mathbb{R}^{n\times n}$, thus $\sum_{i=1}^qm_i=n$, with $m_i$ the algebraic multiplicity of eigenvalue $\lambda_i$. Then, by Cayley-Hamilton, $$\mathbf{0}=\prod_{i=1}^q(\mathbf{A}-\lambda_i\mathbf{I})^{m_i},$$ and from here it is concluded that $S_i=\{\mathbf{v}:(\mathbf{A}-\lambda_i\mathbf{I})^{m_i}\mathbf{v}=\mathbf{0}\}$ is a vector subspace of dimension $m_i$. I don't get where this last assertion comes from. I have seen a (completely different) proof of the Jordan normal form theorem before. I would proof Cayley-Hamilton from the Jordan normal form theorem, and because eigensystems of similar matrices are equal, we can easily see that $S_i$ is a subspace of dimension $m_i$ using the Jordan normal form of $\mathbf{A}$. Can anyone explain me how this last conclusion is made without the Jordan normal form theorem?","I'm reading some treatment of generalized eigenvectors in a differential equations book. They want to derive that there are as many generalized eigenvectors to a certain eigenvalue $\lambda_i$, as the algebraic multiplicity of this eigenvalue $\lambda_i$. This result is used to proof that there is a basis transformation putting the matrix in the Jordan canonical form. It goes as follows: let $p(x)=\prod_{i=1}^q(x-\lambda_i)^{m_i}$ be the characteristic polynomial of some $\mathbf{A}\in\mathbb{R}^{n\times n}$, thus $\sum_{i=1}^qm_i=n$, with $m_i$ the algebraic multiplicity of eigenvalue $\lambda_i$. Then, by Cayley-Hamilton, $$\mathbf{0}=\prod_{i=1}^q(\mathbf{A}-\lambda_i\mathbf{I})^{m_i},$$ and from here it is concluded that $S_i=\{\mathbf{v}:(\mathbf{A}-\lambda_i\mathbf{I})^{m_i}\mathbf{v}=\mathbf{0}\}$ is a vector subspace of dimension $m_i$. I don't get where this last assertion comes from. I have seen a (completely different) proof of the Jordan normal form theorem before. I would proof Cayley-Hamilton from the Jordan normal form theorem, and because eigensystems of similar matrices are equal, we can easily see that $S_i$ is a subspace of dimension $m_i$ using the Jordan normal form of $\mathbf{A}$. Can anyone explain me how this last conclusion is made without the Jordan normal form theorem?",,"['linear-algebra', 'jordan-normal-form', 'generalized-eigenvector']"
66,Is there a sharper upper bound of the spectral norm of Hilbert matrix than $3+2\sqrt{2}$?,Is there a sharper upper bound of the spectral norm of Hilbert matrix than ?,3+2\sqrt{2},"$A_n$ is a real symmetric $n \times n$ matrix defined by $$ A_n =  \begin{bmatrix} 1 & \frac{1}{2} & \frac{1}{3} & \cdots &  \frac{1}{n} \\ \frac{1}{2} & \frac{1}{2} & \frac{1}{3} & \cdots &  \frac{1}{n} \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & \cdots &  \frac{1}{n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\  \frac{1}{n} & \frac{1}{n} & \frac{1}{n} & \cdots & \frac{1}{n} \\  \end{bmatrix} $$ Find an upper bound of the eigenvalues of $A_n$ as tight as possible. Let $A_n = U+U^T-D,$ where $U$ is the upper triangular part of $A_n$ and $D$ is the diagonal of $A_n$. Suppose $\lambda$ is the largest eigenvalue of $A_n = U^T U$. For any $X \in \mathbb{R}^n$,  $$X^T A_n X = X^T U^T U X = \left\Vert UX \right\Vert^2 \leq \lambda \left\Vert X \right\Vert^2 \implies \left\Vert UX \right\Vert \leq \sqrt{\lambda} \left\Vert X \right\Vert.$$ Since $U^T U$ is similar to $U U^T$, $\lambda$ is also the largest eigenvalue of $ U U^T $. Hence we also have $$ \left\Vert U^T X \right\Vert \leq \sqrt{\lambda} \left\Vert X \right\Vert.$$ Similarly, $$ \left\Vert D X \right\Vert \leq \left\Vert X \right\Vert.$$ Thus $$ \left\Vert A_n X \right\Vert = \left\Vert U X + U^T X - D X \right\Vert \leq \left\Vert U X \right\Vert + \left\Vert  U^T X \right\Vert + \left\Vert  D X \right\Vert \leq \left( 2 \sqrt{\lambda} + 1 \right) \left\Vert X \right\Vert.$$ Take $X$ to be the eigenvector of $A_n$ corresponding to $\lambda$, then $$ \left\Vert A_n X \right\Vert = \lambda \left\Vert X \right\Vert \leq \left( 2 \sqrt{\lambda} + 1 \right) \left\Vert X \right\Vert,$$ we have $\sqrt{\lambda} \leq 1+\sqrt{2} \implies \lambda \leq 3+2\sqrt{2}.$ Is there any tighter upper bound?","$A_n$ is a real symmetric $n \times n$ matrix defined by $$ A_n =  \begin{bmatrix} 1 & \frac{1}{2} & \frac{1}{3} & \cdots &  \frac{1}{n} \\ \frac{1}{2} & \frac{1}{2} & \frac{1}{3} & \cdots &  \frac{1}{n} \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & \cdots &  \frac{1}{n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\  \frac{1}{n} & \frac{1}{n} & \frac{1}{n} & \cdots & \frac{1}{n} \\  \end{bmatrix} $$ Find an upper bound of the eigenvalues of $A_n$ as tight as possible. Let $A_n = U+U^T-D,$ where $U$ is the upper triangular part of $A_n$ and $D$ is the diagonal of $A_n$. Suppose $\lambda$ is the largest eigenvalue of $A_n = U^T U$. For any $X \in \mathbb{R}^n$,  $$X^T A_n X = X^T U^T U X = \left\Vert UX \right\Vert^2 \leq \lambda \left\Vert X \right\Vert^2 \implies \left\Vert UX \right\Vert \leq \sqrt{\lambda} \left\Vert X \right\Vert.$$ Since $U^T U$ is similar to $U U^T$, $\lambda$ is also the largest eigenvalue of $ U U^T $. Hence we also have $$ \left\Vert U^T X \right\Vert \leq \sqrt{\lambda} \left\Vert X \right\Vert.$$ Similarly, $$ \left\Vert D X \right\Vert \leq \left\Vert X \right\Vert.$$ Thus $$ \left\Vert A_n X \right\Vert = \left\Vert U X + U^T X - D X \right\Vert \leq \left\Vert U X \right\Vert + \left\Vert  U^T X \right\Vert + \left\Vert  D X \right\Vert \leq \left( 2 \sqrt{\lambda} + 1 \right) \left\Vert X \right\Vert.$$ Take $X$ to be the eigenvector of $A_n$ corresponding to $\lambda$, then $$ \left\Vert A_n X \right\Vert = \lambda \left\Vert X \right\Vert \leq \left( 2 \sqrt{\lambda} + 1 \right) \left\Vert X \right\Vert,$$ we have $\sqrt{\lambda} \leq 1+\sqrt{2} \implies \lambda \leq 3+2\sqrt{2}.$ Is there any tighter upper bound?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'upper-lower-bounds']"
67,Geometric intuition behind an n-dimensional rotation matrix,Geometric intuition behind an n-dimensional rotation matrix,,"How do I derive an n-dimensional rotation matrix from a geometric perspective? I have read on wikipedia that it preserves distance so that $Q^TQ = I$ but the explanation to be honest isn't very clear. I've had a thorough look on Google and can't find a decent explanation that starts from the geometry first. Also, on Wikipedia (see here: https://en.wikipedia.org/wiki/Rotation_matrix ) it says that $det(Q) = 1$ but it isn't clear at all why! Thanks.","How do I derive an n-dimensional rotation matrix from a geometric perspective? I have read on wikipedia that it preserves distance so that $Q^TQ = I$ but the explanation to be honest isn't very clear. I've had a thorough look on Google and can't find a decent explanation that starts from the geometry first. Also, on Wikipedia (see here: https://en.wikipedia.org/wiki/Rotation_matrix ) it says that $det(Q) = 1$ but it isn't clear at all why! Thanks.",,"['linear-algebra', 'matrices', 'linear-transformations', 'rotations']"
68,How to Express the Summation of an Inner Product Space,How to Express the Summation of an Inner Product Space,,"I'm having an issue with the following problem from Ch. 6.1 (Inner Product Spaces and Norms) of Friedberg's Linear Algebra , 4th ed. $""$Let $\{v_1 ,v_2 ,...,v_k\}$ be an orthogonal set in V, and let $a_1 ,a_2 ,...,a_k$ be scalars.  Prove that $$||\sum_{i=1}^k a_i v_i||^{2} = \sum_{i=1}^k |a_i|^{2} ||v_i||^{2}.""$$ My attempt at a solution: $""$Recall that since $\{v_1 ,v_2 ,...,v_k\}$ is orthogonal, any two distinct vectors $v_1 ,v_j \in \{v_1 ,v_2 ,...,v_k\}$ would form the equality $<v_i ,v_j>=0$.  Let us pick a vector $v_i\in \{v_1 ,v_2 ,...,v_k\}$ such that $<v_i,v_i>\neq0$.  By the properties $||x||=\sqrt{<x,x>}$ and $\overline{\sum_{\ell=1}^n x_\ell}=\sum_{\ell=1}^n \overline{x_\ell}$ ('the conjugate of a sum is the sum of the conjugates'), we have $$||\sum_{i=1}^k a_i v_i||^{2} =<\sum_{i=1}^k a_i v_i,\sum_{i=1}^k a_i v_i>$$ $$=\sum_{i=1}^k a_i \sum_{i=1}^k \overline{a_i} <v_i,v_i>.$$ Note that as each $\sum_{i=1}^k a_i ,\sum_{i=1}^k \overline{a_i}$ simply denotes a scalar, we can use the properties $<cx,y>=c<x,y>$ and $<x,cy>=\bar c <x,y>.""$ At this point I am basically stuck.  If I make the argument... $$\sum_{i=1}^k a_i \sum_{i=1}^k \bar a_i=(a_1 + a_2 +...+ a_k)(\bar a_1 + \bar a_2 +...+\bar a_k)$$ $$=(a_1 + a_2 +...+ a_k) \overline{(a_1 + a_2 +...+ a_k)}$$ $$=|a_1 + a_2 +...+ a_k|^{2}$$ $$=|\sum_{i=1}^k a_i|^{2}$$ ... I get no further, because to the best of my knowledge, $|\sum_{i=1}^k a_i|^{2}\neq \sum_{i=1}^k |a_i|^{2}$ (because of the triangle inequality).  I feel I am probably making a few fatal errors throughout this proof.  Could anybody knowledgeable in Linear Algebra please shed some light for me?","I'm having an issue with the following problem from Ch. 6.1 (Inner Product Spaces and Norms) of Friedberg's Linear Algebra , 4th ed. $""$Let $\{v_1 ,v_2 ,...,v_k\}$ be an orthogonal set in V, and let $a_1 ,a_2 ,...,a_k$ be scalars.  Prove that $$||\sum_{i=1}^k a_i v_i||^{2} = \sum_{i=1}^k |a_i|^{2} ||v_i||^{2}.""$$ My attempt at a solution: $""$Recall that since $\{v_1 ,v_2 ,...,v_k\}$ is orthogonal, any two distinct vectors $v_1 ,v_j \in \{v_1 ,v_2 ,...,v_k\}$ would form the equality $<v_i ,v_j>=0$.  Let us pick a vector $v_i\in \{v_1 ,v_2 ,...,v_k\}$ such that $<v_i,v_i>\neq0$.  By the properties $||x||=\sqrt{<x,x>}$ and $\overline{\sum_{\ell=1}^n x_\ell}=\sum_{\ell=1}^n \overline{x_\ell}$ ('the conjugate of a sum is the sum of the conjugates'), we have $$||\sum_{i=1}^k a_i v_i||^{2} =<\sum_{i=1}^k a_i v_i,\sum_{i=1}^k a_i v_i>$$ $$=\sum_{i=1}^k a_i \sum_{i=1}^k \overline{a_i} <v_i,v_i>.$$ Note that as each $\sum_{i=1}^k a_i ,\sum_{i=1}^k \overline{a_i}$ simply denotes a scalar, we can use the properties $<cx,y>=c<x,y>$ and $<x,cy>=\bar c <x,y>.""$ At this point I am basically stuck.  If I make the argument... $$\sum_{i=1}^k a_i \sum_{i=1}^k \bar a_i=(a_1 + a_2 +...+ a_k)(\bar a_1 + \bar a_2 +...+\bar a_k)$$ $$=(a_1 + a_2 +...+ a_k) \overline{(a_1 + a_2 +...+ a_k)}$$ $$=|a_1 + a_2 +...+ a_k|^{2}$$ $$=|\sum_{i=1}^k a_i|^{2}$$ ... I get no further, because to the best of my knowledge, $|\sum_{i=1}^k a_i|^{2}\neq \sum_{i=1}^k |a_i|^{2}$ (because of the triangle inequality).  I feel I am probably making a few fatal errors throughout this proof.  Could anybody knowledgeable in Linear Algebra please shed some light for me?",,"['linear-algebra', 'summation', 'inner-products']"
69,How does linear algebra over the octonions and other division algebras work?,How does linear algebra over the octonions and other division algebras work?,,"An interesting question, which has been discussed in many forms on this site, is how many results from the study of linear algebra over vector spaces carries over when we allow the scalars to form an algebraic structure more general than a field. For example, if you stop requiring multiplicative commutativity and allow the scalars to form an arbitrary division ring, then a surprising amount of structure carries over unchanged, as discussed here : you lose the notion of the determinant and eigenvalues become more subtle, but you still have unique basis cardinality, Gaussian elimination, the Rouché-Capelli theorem, and matrix representations of arbitrarily linear maps between finite-dimensional modules. A similar story applies if you allow the scalars to form an arbitrary commutative ring. However, all hell can break loose if you drop commutativity and division: a module over an arbitrary ring does not necessary have an invariant basis number, so much of the structure of linear algebra over fields immediately falls apart. What happens if you further generalize the scalars' division ring to be a division algebra that is not associative? How much structure do you lose if you give up associativity but preserve division? To be concrete, consider generalized modules with the octonions as their ""scalars"". (A precise definition of what I mean is given in this question , together with the requirement $(rs)\cdot x = r \cdot (s \cdot x)$.) Do these modules have a notion of invariant basis number? If so, can arbitrary linear maps between finitely generated such generalized modules be represented by matrices? Are the matrix elements given by the usual formula $A(\hat{e}_j) = \sum_i A_{ij} \hat{e}_i$?","An interesting question, which has been discussed in many forms on this site, is how many results from the study of linear algebra over vector spaces carries over when we allow the scalars to form an algebraic structure more general than a field. For example, if you stop requiring multiplicative commutativity and allow the scalars to form an arbitrary division ring, then a surprising amount of structure carries over unchanged, as discussed here : you lose the notion of the determinant and eigenvalues become more subtle, but you still have unique basis cardinality, Gaussian elimination, the Rouché-Capelli theorem, and matrix representations of arbitrarily linear maps between finite-dimensional modules. A similar story applies if you allow the scalars to form an arbitrary commutative ring. However, all hell can break loose if you drop commutativity and division: a module over an arbitrary ring does not necessary have an invariant basis number, so much of the structure of linear algebra over fields immediately falls apart. What happens if you further generalize the scalars' division ring to be a division algebra that is not associative? How much structure do you lose if you give up associativity but preserve division? To be concrete, consider generalized modules with the octonions as their ""scalars"". (A precise definition of what I mean is given in this question , together with the requirement $(rs)\cdot x = r \cdot (s \cdot x)$.) Do these modules have a notion of invariant basis number? If so, can arbitrary linear maps between finitely generated such generalized modules be represented by matrices? Are the matrix elements given by the usual formula $A(\hat{e}_j) = \sum_i A_{ij} \hat{e}_i$?",,"['linear-algebra', 'abstract-algebra', 'modules', 'division-algebras', 'octonions']"
70,"Linear Transformation Question $ \operatorname{Hom}(V,W)$ and $ \operatorname{Hom}(W,V)$ satisfying conditions",Linear Transformation Question  and  satisfying conditions," \operatorname{Hom}(V,W)  \operatorname{Hom}(W,V)","Can I please get help on this questions. I'm not sure how to start it or end it. Let $V$ and $W$ be vector spaces over a field $F$. Let $\alpha$ be an element of $ \operatorname{Hom}(V,W)$ and $\beta$ be an element of $ \operatorname{Hom}(W,V)$ satisfy the condition  $\alpha\cdot\beta\cdot\alpha=\alpha$. If $w \in  \operatorname{Im}(\alpha)$, show that $\alpha^{-1}(w) = \{ \beta(w)+v - \beta \alpha(v)  \mid v \in V \}$.","Can I please get help on this questions. I'm not sure how to start it or end it. Let $V$ and $W$ be vector spaces over a field $F$. Let $\alpha$ be an element of $ \operatorname{Hom}(V,W)$ and $\beta$ be an element of $ \operatorname{Hom}(W,V)$ satisfy the condition  $\alpha\cdot\beta\cdot\alpha=\alpha$. If $w \in  \operatorname{Im}(\alpha)$, show that $\alpha^{-1}(w) = \{ \beta(w)+v - \beta \alpha(v)  \mid v \in V \}$.",,"['linear-algebra', 'linear-transformations']"
71,Prove that a linear combination of zero-sum vectors also sums to zero,Prove that a linear combination of zero-sum vectors also sums to zero,,"I have a matrix $A$ whose rows sum to zero, such that $\sum_j A_{ij} = 0, \forall i$. If I multiply it by any matrix, $B$, can it be proven that the resulting matrix, $C = BA$, must also have zero sum rows? I find that they are empirically. Is such a proof available as a reference in any text book?","I have a matrix $A$ whose rows sum to zero, such that $\sum_j A_{ij} = 0, \forall i$. If I multiply it by any matrix, $B$, can it be proven that the resulting matrix, $C = BA$, must also have zero sum rows? I find that they are empirically. Is such a proof available as a reference in any text book?",,['linear-algebra']
72,Does the following hold for any matrix $A$ with non-negative eigenvalues?,Does the following hold for any matrix  with non-negative eigenvalues?,A,"If $A\in\Re^{q\times{q}}$ is a square matrix with non-negative eigenvalues. Is it possible to show that $x^TAx\geq{0}$ for any non-zero vector $x$? I know this is obvious for positive semi-definte matrices but when it is not, is there a way to show that $x^TAx\geq{0}$? I know this holds for an eigenvector $x$ corresponding to eigenvalue $\lambda$, $Ax=\lambda{x}$ and $x^T{A}x=\lambda\|x\|^2\geq{0}$. But in case when $x$ is not an eigenvector is that claim true?","If $A\in\Re^{q\times{q}}$ is a square matrix with non-negative eigenvalues. Is it possible to show that $x^TAx\geq{0}$ for any non-zero vector $x$? I know this is obvious for positive semi-definte matrices but when it is not, is there a way to show that $x^TAx\geq{0}$? I know this holds for an eigenvector $x$ corresponding to eigenvalue $\lambda$, $Ax=\lambda{x}$ and $x^T{A}x=\lambda\|x\|^2\geq{0}$. But in case when $x$ is not an eigenvector is that claim true?",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
73,Kernel of nilpotent operators on infinite dimensional vector spaces,Kernel of nilpotent operators on infinite dimensional vector spaces,,Suppose $T$ is a nilpotent linear transformations on an infinite dimensional vector space. By nilpotent I mean that $T^k=0$ for some $k$. Does it follow that the kernel of $T$ must be infinite dimensional?,Suppose $T$ is a nilpotent linear transformations on an infinite dimensional vector space. By nilpotent I mean that $T^k=0$ for some $k$. Does it follow that the kernel of $T$ must be infinite dimensional?,,"['linear-algebra', 'functional-analysis', 'operator-theory']"
74,How does taking a real part of a complex matrix change the eigenvalues?,How does taking a real part of a complex matrix change the eigenvalues?,,"If $A\in \mathbb{C}^{n\times n}$ with eigenvalues $(\mu_1,\ldots,\mu_n)$, is there anything we can say about the eigenvalues of $T = \Re(A)$; let's call them $(\lambda_1,\ldots, \lambda_n)$? Especially, does it hold that $|\lambda_i|\leq|\mu_i|$? The last part holds for the largest eigenvalue via a norm argument, but I can't really come up with anything for the other eigenvalues.","If $A\in \mathbb{C}^{n\times n}$ with eigenvalues $(\mu_1,\ldots,\mu_n)$, is there anything we can say about the eigenvalues of $T = \Re(A)$; let's call them $(\lambda_1,\ldots, \lambda_n)$? Especially, does it hold that $|\lambda_i|\leq|\mu_i|$? The last part holds for the largest eigenvalue via a norm argument, but I can't really come up with anything for the other eigenvalues.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
75,"Show that $\langle v,w\rangle _1=c\langle v,w\rangle _2$ for some scalar $c$.",Show that  for some scalar .,"\langle v,w\rangle _1=c\langle v,w\rangle _2 c","Let $V$ be a vector space over $F$ and $\langle ,\rangle _1$ and $\langle ,\rangle _2$ be two inner products defined on it. It is given that $\langle v,w\rangle _1=0\iff \langle v ,w\rangle _2=0 \tag{H}.$ Show that $\langle v,w\rangle _1=c\langle  v,w\rangle _2$ for some scalar $c$ . Fix $w\in V$ . Define $f_1:V\to F$ by $f_1(v)=\langle v,w\rangle _1$ . and Define $f_2:V\to F$ by $f_2(v)=\langle v,w\rangle _2$ . $v\in \ker f_1\iff v\in \ker f_2$ . If $\ker f=V$ then choose $c=1$ and we are done. If $\ker f\neq V$ then $\exists v_0\in V$ such that $f_1(v_0)\neq 0\implies f_2(v_0)\neq 0$ . How to choose $c$ in this case? Please help me out.",Let be a vector space over and and be two inner products defined on it. It is given that Show that for some scalar . Fix . Define by . and Define by . . If then choose and we are done. If then such that . How to choose in this case? Please help me out.,"V F \langle ,\rangle _1 \langle ,\rangle _2 \langle v,w\rangle _1=0\iff \langle v ,w\rangle _2=0 \tag{H}. \langle v,w\rangle _1=c\langle  v,w\rangle _2 c w\in V f_1:V\to F f_1(v)=\langle v,w\rangle _1 f_2:V\to F f_2(v)=\langle v,w\rangle _2 v\in \ker f_1\iff v\in \ker f_2 \ker f=V c=1 \ker f\neq V \exists v_0\in V f_1(v_0)\neq 0\implies f_2(v_0)\neq 0 c","['linear-algebra', 'inner-products']"
76,Conjugation with Pauli matrices,Conjugation with Pauli matrices,,"Let $\{\sigma_j\}_{j=0}^3$ denote the Pauli basis of Hermitian matrices on $\mathbb C^2$ with $\sigma_0 := I$. Is it true that $$\frac{1}{4}\sum_{j=0}^3 \sigma_j A \sigma_j = \frac{\text{tr}(A)}{2}I$$ for any positive definite $2x2$ matrix $A$? If so, how would I go about showing this? I haven't been able to do so with the known properties of the Pauli matrices.","Let $\{\sigma_j\}_{j=0}^3$ denote the Pauli basis of Hermitian matrices on $\mathbb C^2$ with $\sigma_0 := I$. Is it true that $$\frac{1}{4}\sum_{j=0}^3 \sigma_j A \sigma_j = \frac{\text{tr}(A)}{2}I$$ for any positive definite $2x2$ matrix $A$? If so, how would I go about showing this? I haven't been able to do so with the known properties of the Pauli matrices.",,"['linear-algebra', 'matrices', 'vector-spaces']"
77,Why does $A^8=I_3$ implies $A^4=I_3$?,Why does  implies ?,A^8=I_3 A^4=I_3,"Let $A\in M_3(\mathbb{Q})$ such that $A^8=I_3$. Prove that $A^4=I_3$ (We denote by $m_X$ the minimal polynomial of matrix X and by $p_X$ its characteristic polynomial and by $gr \space X$ its degree) The solution is the following: $1)$ $m_a$ divides $P\in\mathbb{Q}[X]$, $$P(x)=x^8-1=(x-1)(x+1)(x^2+1)(x^4+1)$$ $2)$ Since $A^4\neq I_3$, then $p_A$ would have at least one common root with $x^4+1$ $3)$ Since $x^4+1$ is irreducible over $\mathbb{Q}[X]$ we have that $x^4+1|m_A$ $4)$ $A\in M_3(\mathbb{Q})$ so $gr\space m_A\leq3$ and $gr\space x^4+1=4$ , contradiction I understood only points $1)$ and $4)$. Can you please help me to understand $2)$ and $3)$? Are they even right?","Let $A\in M_3(\mathbb{Q})$ such that $A^8=I_3$. Prove that $A^4=I_3$ (We denote by $m_X$ the minimal polynomial of matrix X and by $p_X$ its characteristic polynomial and by $gr \space X$ its degree) The solution is the following: $1)$ $m_a$ divides $P\in\mathbb{Q}[X]$, $$P(x)=x^8-1=(x-1)(x+1)(x^2+1)(x^4+1)$$ $2)$ Since $A^4\neq I_3$, then $p_A$ would have at least one common root with $x^4+1$ $3)$ Since $x^4+1$ is irreducible over $\mathbb{Q}[X]$ we have that $x^4+1|m_A$ $4)$ $A\in M_3(\mathbb{Q})$ so $gr\space m_A\leq3$ and $gr\space x^4+1=4$ , contradiction I understood only points $1)$ and $4)$. Can you please help me to understand $2)$ and $3)$? Are they even right?",,"['linear-algebra', 'matrices', 'polynomials']"
78,Modified Gram Schmidt,Modified Gram Schmidt,,How does the Modified Gram Schmidt works? I want to use it but I am confused by the notations and I could not find any example online.,How does the Modified Gram Schmidt works? I want to use it but I am confused by the notations and I could not find any example online.,,"['linear-algebra', 'matrices', 'orthonormal', 'gram-schmidt']"
79,Find the kernel of a 4x4 matrix,Find the kernel of a 4x4 matrix,,"$$     \begin{pmatrix}     1 & 2 & 3 & 4\\     5 & 6 & 7 & 8\\     9 & 10 & 11 & 12\\     13 & 14 & 15 & 16\\     \end{pmatrix} $$ I am asked to find the kernel of the matrix $M$. After doing some row operation I get to  $$     \begin{pmatrix}     1 & 2 & 3 & 4\\     0 & -4 & -8 & -12\\     0 & 0 & 0 & 0\\     0 & 0 & 0 & 0\\     \end{pmatrix} $$ and for $x$ I find $x = \alpha + 2\beta$, whereas $y = -2\alpha -3\beta$ Therefore,  $$     \begin{pmatrix}     \alpha + 2\beta\\     -2\alpha - 3\beta\\     \alpha\\     \beta\\     \end{pmatrix} $$ When we take outside alpha and beta: we get two vectors: $$     \begin{pmatrix}     1\\     -2\\     1\\     0\\     \end{pmatrix} $$ and $$     \begin{pmatrix}     2\\     -3\\     0\\     1\\     \end{pmatrix} $$ which are linearly independent and form a basis of this $ker(M)$ Could you please confirm with me whether you get the same result? Thank you.","$$     \begin{pmatrix}     1 & 2 & 3 & 4\\     5 & 6 & 7 & 8\\     9 & 10 & 11 & 12\\     13 & 14 & 15 & 16\\     \end{pmatrix} $$ I am asked to find the kernel of the matrix $M$. After doing some row operation I get to  $$     \begin{pmatrix}     1 & 2 & 3 & 4\\     0 & -4 & -8 & -12\\     0 & 0 & 0 & 0\\     0 & 0 & 0 & 0\\     \end{pmatrix} $$ and for $x$ I find $x = \alpha + 2\beta$, whereas $y = -2\alpha -3\beta$ Therefore,  $$     \begin{pmatrix}     \alpha + 2\beta\\     -2\alpha - 3\beta\\     \alpha\\     \beta\\     \end{pmatrix} $$ When we take outside alpha and beta: we get two vectors: $$     \begin{pmatrix}     1\\     -2\\     1\\     0\\     \end{pmatrix} $$ and $$     \begin{pmatrix}     2\\     -3\\     0\\     1\\     \end{pmatrix} $$ which are linearly independent and form a basis of this $ker(M)$ Could you please confirm with me whether you get the same result? Thank you.",,"['linear-algebra', 'matrices']"
80,Matrix polynomial,Matrix polynomial,,"Let $p \in \mathbb{N}, p \geq 2$ even. Prove that $$\det(X^{p}+X^{p-1}+\dots+X+I)\geq 0$$   for all invertible matrices $X$ with real entries. The solution I found simply uses the fact that the polynomial $x^p+x^{p-1}+\dots+x+1$ doesn't have any real roots. Why is that enough?","Let $p \in \mathbb{N}, p \geq 2$ even. Prove that $$\det(X^{p}+X^{p-1}+\dots+X+I)\geq 0$$   for all invertible matrices $X$ with real entries. The solution I found simply uses the fact that the polynomial $x^p+x^{p-1}+\dots+x+1$ doesn't have any real roots. Why is that enough?",,"['linear-algebra', 'matrices', 'polynomials', 'determinant']"
81,Vectorization and transpose: how are $\text{vec}(W^T)$ and $\text{vec}(W)$ related?,Vectorization and transpose: how are  and  related?,\text{vec}(W^T) \text{vec}(W),"In solving for a gradient, I ended up with a differential that looks similar to: $$ dT = (a^T \otimes b^T)\ \text{vec}[d[W]^T] + (b^T \otimes c^T)\ \text{vec}[d[W]] $$ and I am trying to solve for $\frac{\partial T}{\partial \text{vec}W}$. The second term isn't a big problem, since we can just flip-flop the vec and d operators, but the first term needs to handle the transpose and I wasn't sure how to do it. Note: Although I hope it isn't nessesary, the actual equation I am solving for is $\frac{\partial T}{\partial \text{vec}W}$ with $T$ defined below: $$ T = (y - f_1(W_1f_0(W_0x)))^TB_0^Tf_0(W_0x) $$ using elementwise differentiable functions $f_i$, and non-square matricies $W_i$, and static vector $y$. Edit : Based on numerical experiments it seems that this relationship is true: $$ T = b^TW^Ta + c^TWb\\ dT_0 = (a^T \otimes b^T)\text{vec}[d[W]^T] \implies \nabla_{\text{vec} W} T_0 = (a^T \otimes b^T)\\ dT_1 = (b^T \otimes c^T)\text{vec}[d[W]] \implies \nabla_{\text{vec} W} T_1 = (c^T \otimes b^T)\\ $$ But this doesn't make any sense to me. Why would the non-transposed version flip the order of the kronecker product?","In solving for a gradient, I ended up with a differential that looks similar to: $$ dT = (a^T \otimes b^T)\ \text{vec}[d[W]^T] + (b^T \otimes c^T)\ \text{vec}[d[W]] $$ and I am trying to solve for $\frac{\partial T}{\partial \text{vec}W}$. The second term isn't a big problem, since we can just flip-flop the vec and d operators, but the first term needs to handle the transpose and I wasn't sure how to do it. Note: Although I hope it isn't nessesary, the actual equation I am solving for is $\frac{\partial T}{\partial \text{vec}W}$ with $T$ defined below: $$ T = (y - f_1(W_1f_0(W_0x)))^TB_0^Tf_0(W_0x) $$ using elementwise differentiable functions $f_i$, and non-square matricies $W_i$, and static vector $y$. Edit : Based on numerical experiments it seems that this relationship is true: $$ T = b^TW^Ta + c^TWb\\ dT_0 = (a^T \otimes b^T)\text{vec}[d[W]^T] \implies \nabla_{\text{vec} W} T_0 = (a^T \otimes b^T)\\ dT_1 = (b^T \otimes c^T)\text{vec}[d[W]] \implies \nabla_{\text{vec} W} T_1 = (c^T \otimes b^T)\\ $$ But this doesn't make any sense to me. Why would the non-transposed version flip the order of the kronecker product?",,"['linear-algebra', 'matrices', 'transpose', 'vectorization']"
82,"Neukirch, Proposition 2.11","Neukirch, Proposition 2.11",,"I am reading Neukirch's Algebraic Number Theory , and I do not understand the following step in the proof of Proposition 2.11. The proof can be found in this imgur album , but only the very end confuses me. The step that confuses me is on one of the last lines. He writes M as a product of two $n' \times n'$ matrices, each of whose entries is an $n \times n$ matrix. He then says, ""By changing indices the second matrix may be transformed to look like the first one,"" and writes, $$\det(M)^2 = \det(Q)^{2n'} \det((\sigma_{\ell}^{'}\omega_j^{'}))^{2n}.$$ I understand the $\det(Q)^{2n'}$ term, but I do not understand where the $\det((\sigma_{\ell}^{'}\omega_j^{'}))^{2n}$ comes from or what he means by ""changing indices"". It seems to me that because the second matrix is $n' \times n'$ as well, you would except an exponent of $2n'$, rather than $2n$. But I'm not sure how he's calculating it.","I am reading Neukirch's Algebraic Number Theory , and I do not understand the following step in the proof of Proposition 2.11. The proof can be found in this imgur album , but only the very end confuses me. The step that confuses me is on one of the last lines. He writes M as a product of two $n' \times n'$ matrices, each of whose entries is an $n \times n$ matrix. He then says, ""By changing indices the second matrix may be transformed to look like the first one,"" and writes, $$\det(M)^2 = \det(Q)^{2n'} \det((\sigma_{\ell}^{'}\omega_j^{'}))^{2n}.$$ I understand the $\det(Q)^{2n'}$ term, but I do not understand where the $\det((\sigma_{\ell}^{'}\omega_j^{'}))^{2n}$ comes from or what he means by ""changing indices"". It seems to me that because the second matrix is $n' \times n'$ as well, you would except an exponent of $2n'$, rather than $2n$. But I'm not sure how he's calculating it.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'number-theory', 'algebraic-number-theory']"
83,Why are pivot columns the basis of A?,Why are pivot columns the basis of A?,,"Why is it that when we determine the pivot columns of an m x n matrix $A$, the pivot columns form a basis for the $Range(A)$? I understand that the pivot columns are linearly independent (the reason why we chose them as the pivot columns), but how do we know that they also span the range of $A$? Also, why is that we're choosing the columns of $A$ to be the vectors that form a basis for $Range(A)$ rather than the columns of $U = rref(A)$?","Why is it that when we determine the pivot columns of an m x n matrix $A$, the pivot columns form a basis for the $Range(A)$? I understand that the pivot columns are linearly independent (the reason why we chose them as the pivot columns), but how do we know that they also span the range of $A$? Also, why is that we're choosing the columns of $A$ to be the vectors that form a basis for $Range(A)$ rather than the columns of $U = rref(A)$?",,['linear-algebra']
84,Inverse of a lower triangular matrix,Inverse of a lower triangular matrix,,"I got the following question to solve: Given the lower triangular matrix \begin{bmatrix}     A_{11} & 0 \\     A_{21} & A_{22}   \end{bmatrix} of size $n \times n$ (n is a power of 2) where $A_{11}$, $A_{21}$ and $A_{22}$ are matrices of size $(n/2) \times (n/2)$, show that the inverse is, \begin{bmatrix}     A_{11}^{-1} & 0 \\     -A_{22}^{-1}A_{21}A_{11} & A_{22}^{-1}   \end{bmatrix} how do I go about to solve this problem? Edit: the matrix is invertible. Edit: the second matrix should be changed to: \begin{bmatrix}     A_{11}^{-1} & 0 \\     -A_{22}^{-1}A_{21}A_{11}^{\color{red}{-1}} & A_{22}^{-1}   \end{bmatrix} The inverse was missing.","I got the following question to solve: Given the lower triangular matrix \begin{bmatrix}     A_{11} & 0 \\     A_{21} & A_{22}   \end{bmatrix} of size $n \times n$ (n is a power of 2) where $A_{11}$, $A_{21}$ and $A_{22}$ are matrices of size $(n/2) \times (n/2)$, show that the inverse is, \begin{bmatrix}     A_{11}^{-1} & 0 \\     -A_{22}^{-1}A_{21}A_{11} & A_{22}^{-1}   \end{bmatrix} how do I go about to solve this problem? Edit: the matrix is invertible. Edit: the second matrix should be changed to: \begin{bmatrix}     A_{11}^{-1} & 0 \\     -A_{22}^{-1}A_{21}A_{11}^{\color{red}{-1}} & A_{22}^{-1}   \end{bmatrix} The inverse was missing.",,"['linear-algebra', 'matrices', 'inverse']"
85,Finding dimension of the vector spaces of polynomials with two commuting variables as well as two non-commuting variables,Finding dimension of the vector spaces of polynomials with two commuting variables as well as two non-commuting variables,,"Let $V$ (resp. $W$) be the real vector space of all polynomials in two commuting (resp. noncommuting) variables with real coefficients and of degree strictly less than $100$. What are the dimensions of $V$ and $W$? My approach : To calculate dimension of $V$ We just have to count number of basis elements in the basis of $V$. The basis elements of $V$ are of the form $x^iy^j$ where $i,j\in \Bbb N \cup \{0\}$ and $i+j \le 99.$ Case-1)For $i=0$, we have $0 \le j \le 99.$ i.e we have $100$ choices for $j$. Case-2)For $i=1$, we have $0 \le j \le 98$. i.e. we have $99$ choices for $j$. Case-99)For $i=98$, we have $0 \le j \le 1$. i.e. we have $2$ choices for $j$. Case-100)For $i=99$, we have $j=0$ as the only $1$ choice. By addition rule, we get $\text {number of elements in the basis of V} = 1+2+3+...+100=\frac {100(101)}2=5050=\dim V.$ To calculate dimension of $W$ Again we count the number of basis elements in the basis of $W$. But here $x^iy^j \neq y^jx^i \; \forall \; i,j \ge 1.$ Thus we count number of basis elements for $x^i,y^j \; \forall i,j \ge 1$ only and then multiply the same by $2$. We first count number of elements in the list $x^0, x^1, x^2,...x^{99}$ which is $100$ and number of elements in the list $y^1,y^2,...,y^{99}$ which is $99$. For the $x^iy^j \;\;\; i,j \ge 1$ part we proceed as follows, Case-1)For $i=1,$ we have $1 \le j \le 98.$ i.e. we have $98$ choices for $j$. Case-2)For $i=2,$ we have $1 \le j \le 97.$ i.e. we have $97$ choices for $j$. Case-97)For $i=97$, we have $1 \le j \le 2$ i.e. we have $2$ choices for $j$. Case-98)For $i=98$, we have only $1$ choice for $j$ i.e. $j=1$. Therefore by addition rule we have $1+2+3+...+98=\frac {98(99)}2=4851$ choices. As per our argument above, we have considered only $x^iy^j$ part. Thus $y^jx^i$ part also has $4851$ choices. $\therefore \text {Total number of elements in basis of W}=199+2 \times 4851=9901=\dim W$ Is my approach correct? EDIT : As it has been pointed out in comments by @NickPavlov, I have messed up in non-commuting case. I have left out elements such as $xyx^2$, $yxyxyxy^5$ etc. How should I count this kind of elements?","Let $V$ (resp. $W$) be the real vector space of all polynomials in two commuting (resp. noncommuting) variables with real coefficients and of degree strictly less than $100$. What are the dimensions of $V$ and $W$? My approach : To calculate dimension of $V$ We just have to count number of basis elements in the basis of $V$. The basis elements of $V$ are of the form $x^iy^j$ where $i,j\in \Bbb N \cup \{0\}$ and $i+j \le 99.$ Case-1)For $i=0$, we have $0 \le j \le 99.$ i.e we have $100$ choices for $j$. Case-2)For $i=1$, we have $0 \le j \le 98$. i.e. we have $99$ choices for $j$. Case-99)For $i=98$, we have $0 \le j \le 1$. i.e. we have $2$ choices for $j$. Case-100)For $i=99$, we have $j=0$ as the only $1$ choice. By addition rule, we get $\text {number of elements in the basis of V} = 1+2+3+...+100=\frac {100(101)}2=5050=\dim V.$ To calculate dimension of $W$ Again we count the number of basis elements in the basis of $W$. But here $x^iy^j \neq y^jx^i \; \forall \; i,j \ge 1.$ Thus we count number of basis elements for $x^i,y^j \; \forall i,j \ge 1$ only and then multiply the same by $2$. We first count number of elements in the list $x^0, x^1, x^2,...x^{99}$ which is $100$ and number of elements in the list $y^1,y^2,...,y^{99}$ which is $99$. For the $x^iy^j \;\;\; i,j \ge 1$ part we proceed as follows, Case-1)For $i=1,$ we have $1 \le j \le 98.$ i.e. we have $98$ choices for $j$. Case-2)For $i=2,$ we have $1 \le j \le 97.$ i.e. we have $97$ choices for $j$. Case-97)For $i=97$, we have $1 \le j \le 2$ i.e. we have $2$ choices for $j$. Case-98)For $i=98$, we have only $1$ choice for $j$ i.e. $j=1$. Therefore by addition rule we have $1+2+3+...+98=\frac {98(99)}2=4851$ choices. As per our argument above, we have considered only $x^iy^j$ part. Thus $y^jx^i$ part also has $4851$ choices. $\therefore \text {Total number of elements in basis of W}=199+2 \times 4851=9901=\dim W$ Is my approach correct? EDIT : As it has been pointed out in comments by @NickPavlov, I have messed up in non-commuting case. I have left out elements such as $xyx^2$, $yxyxyxy^5$ etc. How should I count this kind of elements?",,"['linear-algebra', 'combinatorics', 'proof-verification', 'contest-math', 'alternative-proof']"
86,spectral radius and numerical radius of matrix,spectral radius and numerical radius of matrix,,"Let $\mathcal{H}$ be a complex finite dimensional Hilbert space ( $\dim\mathcal{H}=d$ ). Let $A\in M_d(\mathbb{C})$ . How can I show without using spectral considerations that $$\lim_{n\to\infty}\|A^n\|^{\frac{1}{n}} \le\displaystyle\sup_{\|x\|=1}|\langle Ax,x\rangle|?? $$ Thank you for your help!! This question is a motivation to the following one: It is well known that the spectral radius of a bounded linear operator $T$ acting on a complex infinite dimensional Hilbert space is given by $$r(T)=\lim_{n\to\infty}\|T^n\|^{\frac{1}{n}}.$$ I want to prove using the above formula (rather than the definition of the spectral radius) that $$r(T)=\lim_{n\to\infty}\|T^n\|^{\frac{1}{n}}\le \sup_{\|x\|=1}|\langle Tx.x\rangle|.$$",Let be a complex finite dimensional Hilbert space ( ). Let . How can I show without using spectral considerations that Thank you for your help!! This question is a motivation to the following one: It is well known that the spectral radius of a bounded linear operator acting on a complex infinite dimensional Hilbert space is given by I want to prove using the above formula (rather than the definition of the spectral radius) that,"\mathcal{H} \dim\mathcal{H}=d A\in M_d(\mathbb{C}) \lim_{n\to\infty}\|A^n\|^{\frac{1}{n}}
\le\displaystyle\sup_{\|x\|=1}|\langle Ax,x\rangle|??
 T r(T)=\lim_{n\to\infty}\|T^n\|^{\frac{1}{n}}. r(T)=\lim_{n\to\infty}\|T^n\|^{\frac{1}{n}}\le \sup_{\|x\|=1}|\langle Tx.x\rangle|.",['linear-algebra']
87,Condition number of a diagonal matrix,Condition number of a diagonal matrix,,"Let $\|\cdot\|$ be any norm on $\mathbb{C}^n$. Let $A\in \mathbb{C}^{n\times n}$ We define the matrix norm by $||A||=\max_{||x||=1}||Ax||$. If $A=diag(\lambda_1,...,\lambda_n)$ and it is invertible, then do we always have $$||A||\cdot||A^{-1}||=\frac{\max_i|\lambda_i|}{\min_i|\lambda_i|}?$$","Let $\|\cdot\|$ be any norm on $\mathbb{C}^n$. Let $A\in \mathbb{C}^{n\times n}$ We define the matrix norm by $||A||=\max_{||x||=1}||Ax||$. If $A=diag(\lambda_1,...,\lambda_n)$ and it is invertible, then do we always have $$||A||\cdot||A^{-1}||=\frac{\max_i|\lambda_i|}{\min_i|\lambda_i|}?$$",,"['linear-algebra', 'matrices', 'normed-spaces', 'condition-number']"
88,Prove $A$ positive definite $\Rightarrow$ $A$ invertible,Prove  positive definite   invertible,A \Rightarrow A,"At the demonstration of this, I couldn't understand why the following holds: ""$A$ positive definite $\Rightarrow$ $A$ invertible, because otherwise would exist $X\not=0$ satisfying $AX=0\Rightarrow X^TAX=0$ wich is a contradiction."" I understood the implications, but couldn't get why ""$A$ not invertible then exist $X\not=0$ satisfying $AX=0$"" If anyone could explain this to me or give another way of proving the initial problem, I'd be gratefull.","At the demonstration of this, I couldn't understand why the following holds: ""$A$ positive definite $\Rightarrow$ $A$ invertible, because otherwise would exist $X\not=0$ satisfying $AX=0\Rightarrow X^TAX=0$ wich is a contradiction."" I understood the implications, but couldn't get why ""$A$ not invertible then exist $X\not=0$ satisfying $AX=0$"" If anyone could explain this to me or give another way of proving the initial problem, I'd be gratefull.",,['linear-algebra']
89,Why are systems of linear equations of the form $A x = 0$ called homogeneous?,Why are systems of linear equations of the form  called homogeneous?,A x = 0,"Why are systems of linear equations of the form $A x = 0$ called homogeneous ? What is the motivation behind the word ""homogeneous""?","Why are systems of linear equations of the form $A x = 0$ called homogeneous ? What is the motivation behind the word ""homogeneous""?",,"['linear-algebra', 'systems-of-equations']"
90,When are the eigenvectors of an Hermitian matrix real?,When are the eigenvectors of an Hermitian matrix real?,,"Consider a Hermitian matrix $M=M^\dagger$. Clearly, its eigenvalues are real, but what is the condition for the eigenvectors to be real as well? Edit 1: I consider the entries of $M$ to be complex numbers. Moreover, I call a vector real if all its components are real numbers. Edit 2: As pointed out in the comments, if $v$ is a real eigenvector of $M$, then also is $i v$. For this reason I might need to emphasise that I do not care about ``global'' complex coefficients.","Consider a Hermitian matrix $M=M^\dagger$. Clearly, its eigenvalues are real, but what is the condition for the eigenvectors to be real as well? Edit 1: I consider the entries of $M$ to be complex numbers. Moreover, I call a vector real if all its components are real numbers. Edit 2: As pointed out in the comments, if $v$ is a real eigenvector of $M$, then also is $i v$. For this reason I might need to emphasise that I do not care about ``global'' complex coefficients.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'operator-algebras']"
91,Stopping Rules for Jacobi/Gauss-Seidel Iteration,Stopping Rules for Jacobi/Gauss-Seidel Iteration,,"Suppose that $A\mathbf{x}=\mathbf{b}$ is a diagonally dominant linear system. We can use iterative methods that produce a sequence of approximations $\mathbf{x}^1,\mathbf{x}^2,\dots$ that converge to $\mathbf{x}$. Doing this for a small system it seems intuitive to stop when the components $x^i_j$ and $x^{i+1}_j$ differ by a tolerance $\varepsilon$ giving solutions correct to $\varepsilon$. While playing around with larger systems it was clear that this could stop well before convergence and is a very poor stopping rule (and doesn't give accurate answers as claimed). Researching a little further I see some slightly more sophisticated stopping rules (in the below the norms might be max norms or 2-norms): When $\|\mathbf{x}^{i+1}-\mathbf{x}^i\|< \varepsilon.$ When $\|\mathbf{x}^{i+1}-\mathbf{x}^i\|< \varepsilon\|\mathbf{b}\|.$ When $\displaystyle \max_j\left|\frac{x_j^{i+1}-x_j^i}{x_j^{i+1}}\right|<\varepsilon$. When the residual $\|\mathbf{r}\|=\|\mathbf{b}-A\mathbf{x}^i\|<\varepsilon$. When the residual $\|\mathbf{r}\|=\|\mathbf{b}-A\mathbf{x}^i\|<\varepsilon\|\mathbf{b}\|$. I am interested in hearing the pros and cons of each. I am hoping to use one that is relatively easy to implement on VBA with perhaps 20 unknowns.","Suppose that $A\mathbf{x}=\mathbf{b}$ is a diagonally dominant linear system. We can use iterative methods that produce a sequence of approximations $\mathbf{x}^1,\mathbf{x}^2,\dots$ that converge to $\mathbf{x}$. Doing this for a small system it seems intuitive to stop when the components $x^i_j$ and $x^{i+1}_j$ differ by a tolerance $\varepsilon$ giving solutions correct to $\varepsilon$. While playing around with larger systems it was clear that this could stop well before convergence and is a very poor stopping rule (and doesn't give accurate answers as claimed). Researching a little further I see some slightly more sophisticated stopping rules (in the below the norms might be max norms or 2-norms): When $\|\mathbf{x}^{i+1}-\mathbf{x}^i\|< \varepsilon.$ When $\|\mathbf{x}^{i+1}-\mathbf{x}^i\|< \varepsilon\|\mathbf{b}\|.$ When $\displaystyle \max_j\left|\frac{x_j^{i+1}-x_j^i}{x_j^{i+1}}\right|<\varepsilon$. When the residual $\|\mathbf{r}\|=\|\mathbf{b}-A\mathbf{x}^i\|<\varepsilon$. When the residual $\|\mathbf{r}\|=\|\mathbf{b}-A\mathbf{x}^i\|<\varepsilon\|\mathbf{b}\|$. I am interested in hearing the pros and cons of each. I am hoping to use one that is relatively easy to implement on VBA with perhaps 20 unknowns.",,"['linear-algebra', 'numerical-methods', 'algorithms', 'normed-spaces', 'numerical-linear-algebra']"
92,Ergodicity and Simple Eigenvalue,Ergodicity and Simple Eigenvalue,,"It is a fact that a measure-preserving system $(X,\mathcal{B},\mu,T)$ is ergodic if and only if $1$ is a simple eigenvalue for the associated unitary operator $U_T: L^2(\mu) \to L^2(\mu)$ given by $U_Tf = f\circ T$ . I'm curious as to what a simple eigenvalue means here. I know $T$ is ergodic if and only if the only $T$ -invariant $L^2$ functions are the constants. So it is clear to me that $T$ is ergodic if and only if the eigenspace associated with the eigenvalue $1$ has dimension $1$ . However, for finite-dimensional transformations, a simple eigenvalue is an eigenvalue $\lambda$ for which if we completely factor the characteristic polynomial, then the exponent of the term with $\lambda$ is $1$ . Wikipedia seems to indicate that this is not necessarily the same as the dimension of the eigenspace corresponding to $\lambda$ . So what is a simple eigenvalue for a linear operator on infinite-dimensional space, and why does it coincide with the dimension for the eigenspace corresponding to the eigenvalue?","It is a fact that a measure-preserving system is ergodic if and only if is a simple eigenvalue for the associated unitary operator given by . I'm curious as to what a simple eigenvalue means here. I know is ergodic if and only if the only -invariant functions are the constants. So it is clear to me that is ergodic if and only if the eigenspace associated with the eigenvalue has dimension . However, for finite-dimensional transformations, a simple eigenvalue is an eigenvalue for which if we completely factor the characteristic polynomial, then the exponent of the term with is . Wikipedia seems to indicate that this is not necessarily the same as the dimension of the eigenspace corresponding to . So what is a simple eigenvalue for a linear operator on infinite-dimensional space, and why does it coincide with the dimension for the eigenspace corresponding to the eigenvalue?","(X,\mathcal{B},\mu,T) 1 U_T: L^2(\mu) \to L^2(\mu) U_Tf = f\circ T T T L^2 T 1 1 \lambda \lambda 1 \lambda","['real-analysis', 'linear-algebra', 'ergodic-theory']"
93,Decomposing a matrix into binary ones,Decomposing a matrix into binary ones,,"Let $A$ denote the set of all 3×2 binary matrices (those containing only 0's and 1's) in which the sum of each column adds up to 2. Can I decompose $$B=\begin{bmatrix} 3/4 & 1/2 \\ 3/4 & 3/4 \\ 1/2 & 3/4 \end{bmatrix}$$ into a linear combination of matrices in $A$? For example, $$B=1/2 \begin{bmatrix}1 & 1 \\ 0 & 0 \\ 1 & 1\end{bmatrix} + 1/2 \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 1\end{bmatrix}$$ or something like that. EDIT: I want to show there is at least a matrix that cannot be decomposed. How about  $$C=\begin{bmatrix} 3/4 & 2/3 \\ 3/4 & 2/3 \\ 1/2 & 2/3 \end{bmatrix}$$","Let $A$ denote the set of all 3×2 binary matrices (those containing only 0's and 1's) in which the sum of each column adds up to 2. Can I decompose $$B=\begin{bmatrix} 3/4 & 1/2 \\ 3/4 & 3/4 \\ 1/2 & 3/4 \end{bmatrix}$$ into a linear combination of matrices in $A$? For example, $$B=1/2 \begin{bmatrix}1 & 1 \\ 0 & 0 \\ 1 & 1\end{bmatrix} + 1/2 \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 1\end{bmatrix}$$ or something like that. EDIT: I want to show there is at least a matrix that cannot be decomposed. How about  $$C=\begin{bmatrix} 3/4 & 2/3 \\ 3/4 & 2/3 \\ 1/2 & 2/3 \end{bmatrix}$$",,"['linear-algebra', 'matrices']"
94,Counterexample to Schmidt Decomposition for 3 systems,Counterexample to Schmidt Decomposition for 3 systems,,"Problem 2.77 in Nielsen and Chuang's book asks for an example of a pure state $|\psi\rangle$ in a three part composite space $A\otimes B\otimes C$ such that $|\psi\rangle$ has no Schmidt decomposition. So suppose I start with $|\psi\rangle = \sum_{i,j,k}c_{ijk}|i_A\rangle|j_B\rangle|k_C\rangle$ for orthonormal bases $|i_A\rangle, |j_B\rangle, |k_C\rangle$. The proof for 2 spaces involves taking the singular value decomposition of the coefficient matrix. So in the 3 space case, I could take the SVD of each matrix $C_k$ (defined as $(C_k)_{ij}=c_{ijk}$), but they might not all have the same SVD, and I think that causes the problem. Using that, my best guess for a counterexample would be a state like  $$|\psi\rangle = (|00\rangle + |01\rangle +|10\rangle+|11\rangle)|0\rangle + (|00\rangle+|01\rangle - |10\rangle - |11\rangle)|1\rangle$$ (appropriately normalized), since the parts of the vector corresponding to the first two spaces can't be singular value decomposed with the same unitaries (I think). But I have no idea how to prove this: How do I know there isn't some crazy basis for the space $C$ that would still produce a Schmidt decomposition?","Problem 2.77 in Nielsen and Chuang's book asks for an example of a pure state $|\psi\rangle$ in a three part composite space $A\otimes B\otimes C$ such that $|\psi\rangle$ has no Schmidt decomposition. So suppose I start with $|\psi\rangle = \sum_{i,j,k}c_{ijk}|i_A\rangle|j_B\rangle|k_C\rangle$ for orthonormal bases $|i_A\rangle, |j_B\rangle, |k_C\rangle$. The proof for 2 spaces involves taking the singular value decomposition of the coefficient matrix. So in the 3 space case, I could take the SVD of each matrix $C_k$ (defined as $(C_k)_{ij}=c_{ijk}$), but they might not all have the same SVD, and I think that causes the problem. Using that, my best guess for a counterexample would be a state like  $$|\psi\rangle = (|00\rangle + |01\rangle +|10\rangle+|11\rangle)|0\rangle + (|00\rangle+|01\rangle - |10\rangle - |11\rangle)|1\rangle$$ (appropriately normalized), since the parts of the vector corresponding to the first two spaces can't be singular value decomposed with the same unitaries (I think). But I have no idea how to prove this: How do I know there isn't some crazy basis for the space $C$ that would still produce a Schmidt decomposition?",,"['linear-algebra', 'matrices', 'tensor-products', 'quantum-computation']"
95,Is $A^TA$ positive semi-definite for any real matrix $A$?,Is  positive semi-definite for any real matrix ?,A^TA A,The question is written in title. I read a theorem saying: Suppose $A\in \mathbb R^{n\times n}$ is symmetric. Then the following are equivalent. $A$ is positive semidefinite. Eigenvalues of $A$ are all non-negative. $A$ can be factored as $A=G^TG$ where $G$ is an $p\times n$ matrix for some $p$. The reason why I am asking is that I have a matrix $X$ and get some negative eigenvalues of $X^TX$ using either MatLab or R.,The question is written in title. I read a theorem saying: Suppose $A\in \mathbb R^{n\times n}$ is symmetric. Then the following are equivalent. $A$ is positive semidefinite. Eigenvalues of $A$ are all non-negative. $A$ can be factored as $A=G^TG$ where $G$ is an $p\times n$ matrix for some $p$. The reason why I am asking is that I have a matrix $X$ and get some negative eigenvalues of $X^TX$ using either MatLab or R.,,['linear-algebra']
96,"Proof or counterexample: Let $A$ be a square matrix, then:","Proof or counterexample: Let  be a square matrix, then:",A,"If $A$ is diagonalizable, then so is $A^2$ I answered yes. I argued that since $A$ is diagonalizable there exists an eigenbasis, and since $A^2$ has the same eigenvectors than $A$, and its eigenvalues are those of $A$ squared, there is also an eigenbasis for $A^2$, so it is diagonalizable If $A^2$ is diagonalizable, then so is $A$ I am pretty sure the answer is no, but I can't think of a counterexample. Thank you in advance","If $A$ is diagonalizable, then so is $A^2$ I answered yes. I argued that since $A$ is diagonalizable there exists an eigenbasis, and since $A^2$ has the same eigenvectors than $A$, and its eigenvalues are those of $A$ squared, there is also an eigenbasis for $A^2$, so it is diagonalizable If $A^2$ is diagonalizable, then so is $A$ I am pretty sure the answer is no, but I can't think of a counterexample. Thank you in advance",,"['linear-algebra', 'eigenvalues-eigenvectors']"
97,Signature of restriction of a bilinear form,Signature of restriction of a bilinear form,,"Assume I have a real bilinear form $b$ of signature $(p,q)$ on $E=\Bbb R^n$ , and a subspace $F$ of $E$ of dimension $d$ . What are the possible signatures $(\alpha, \beta)$ of the restriction of $b$ to $F \times F$ ? I saw the question possible signatures of bilinear form on subspaces , but it is unanswered. For instance, if $q=1, p=n-1$ , apparently the only possibilities for $b\vert_{F \times F}$ are $(\alpha, \beta) = (d,0), (d-1,1), (d-1,0)$ . I was able to show that if $\beta \geq 1$ , then we must have $(\alpha, \beta) = (d-1,1)$ , but how to do the case $\beta=0$ ?  What about more general cases for $p$ and $q$ ? (For instance, do we have $\alpha ≤ p, \beta ≤ q$ ?). Thank you!","Assume I have a real bilinear form of signature on , and a subspace of of dimension . What are the possible signatures of the restriction of to ? I saw the question possible signatures of bilinear form on subspaces , but it is unanswered. For instance, if , apparently the only possibilities for are . I was able to show that if , then we must have , but how to do the case ?  What about more general cases for and ? (For instance, do we have ?). Thank you!","b (p,q) E=\Bbb R^n F E d (\alpha, \beta) b F \times F q=1, p=n-1 b\vert_{F \times F} (\alpha, \beta) = (d,0), (d-1,1), (d-1,0) \beta \geq 1 (\alpha, \beta) = (d-1,1) \beta=0 p q \alpha ≤ p, \beta ≤ q","['linear-algebra', 'abstract-algebra', 'bilinear-form']"
98,Double orthogonal complement of a finite dimensional subspace,Double orthogonal complement of a finite dimensional subspace,,"Let $\mathbb{W}$ be a finite dimensional subspace of an inner product space $\mathbb{V}$. Prove or disprove the following: the double orthogonal complement of $\mathbb{W}$ is equal to itself, or, in other words, $(\mathbb{W}^\perp)^\perp = \mathbb{W}.$ We might begin a potential proof as follows. If $\vec{v} \in \mathbb{W}$, then $\langle \vec{v}, \vec{w}\rangle = \vec{0}$ for any vector $\vec{w} \in \mathbb{W}^\perp.$ Therefore, by definition, $\vec{v} \in (\mathbb{W}^\perp)^\perp$, and so $\mathbb{W} \subseteq (\mathbb{W}^\perp)^\perp.$ Now, it's easy to prove equality if $\mathbb{V}$ is finite dimensional. For instance, we can use the fact that $\dim \mathbb{W} + \dim \mathbb{W}^\perp = \dim \mathbb{V}$ to show that $\dim \mathbb{W} = \dim (\mathbb{W}^\perp)^\perp$ which implies the desired result. However, in this case, $\mathbb{V}$ is not necessarily finite dimensional and so this step is not valid. In the same vein, if we were to relax the condition that $\mathbb{W}$ must be finite dimensional, then the statement is false. Let $\mathbb{V}$ be the inner product space of all polynomials with real coefficients under the inner product $$ \langle a_0 + a_1x + \dots + a_n x^n, b_0 + b_1x + \dots + b_k x^k\rangle = a_0b_0 + a_1b_1 + \dots + a_jb_j $$ where $j$ is the lesser of $n$ and $k$. Then, the subspace  $$ \mathbb{W} = \{ f(x) \in \mathbb{V} \mid f(1) = 0\} $$ of $\mathbb{V}$ has orthogonal complement $\{ \vec{0}\}$ which shows that $(\mathbb{W}^\perp)^\perp = \mathbb{V} \neq \mathbb{W}$. The question, then, is: does there exist a counter example where $\mathbb{V}$ is infinite dimensional, but the subspace $\mathbb{W}$ is not? My intuition tells me yes, but I'm stuck.","Let $\mathbb{W}$ be a finite dimensional subspace of an inner product space $\mathbb{V}$. Prove or disprove the following: the double orthogonal complement of $\mathbb{W}$ is equal to itself, or, in other words, $(\mathbb{W}^\perp)^\perp = \mathbb{W}.$ We might begin a potential proof as follows. If $\vec{v} \in \mathbb{W}$, then $\langle \vec{v}, \vec{w}\rangle = \vec{0}$ for any vector $\vec{w} \in \mathbb{W}^\perp.$ Therefore, by definition, $\vec{v} \in (\mathbb{W}^\perp)^\perp$, and so $\mathbb{W} \subseteq (\mathbb{W}^\perp)^\perp.$ Now, it's easy to prove equality if $\mathbb{V}$ is finite dimensional. For instance, we can use the fact that $\dim \mathbb{W} + \dim \mathbb{W}^\perp = \dim \mathbb{V}$ to show that $\dim \mathbb{W} = \dim (\mathbb{W}^\perp)^\perp$ which implies the desired result. However, in this case, $\mathbb{V}$ is not necessarily finite dimensional and so this step is not valid. In the same vein, if we were to relax the condition that $\mathbb{W}$ must be finite dimensional, then the statement is false. Let $\mathbb{V}$ be the inner product space of all polynomials with real coefficients under the inner product $$ \langle a_0 + a_1x + \dots + a_n x^n, b_0 + b_1x + \dots + b_k x^k\rangle = a_0b_0 + a_1b_1 + \dots + a_jb_j $$ where $j$ is the lesser of $n$ and $k$. Then, the subspace  $$ \mathbb{W} = \{ f(x) \in \mathbb{V} \mid f(1) = 0\} $$ of $\mathbb{V}$ has orthogonal complement $\{ \vec{0}\}$ which shows that $(\mathbb{W}^\perp)^\perp = \mathbb{V} \neq \mathbb{W}$. The question, then, is: does there exist a counter example where $\mathbb{V}$ is infinite dimensional, but the subspace $\mathbb{W}$ is not? My intuition tells me yes, but I'm stuck.",,"['linear-algebra', 'vector-spaces', 'inner-products']"
99,Proof for $A^{-1}=\frac{adj(A_j)}{det(A)}$,Proof for,A^{-1}=\frac{adj(A_j)}{det(A)},"I have a proof for the method to find the inverse by taking the adjugate of the matrix over the determinant. I believe that it is correct; however, I would like to have it checked over before I submit it for bonus marks. The proof goes as follows: Let $A\in\mathbb{M}_{n\times{n}}(\mathbb{R})$ be an $n\times{n}$ invertible matrix. Let $A^{-1}=\begin{bmatrix}\vec{x_1}\cdots\vec{x_n}\end{bmatrix}$. $AA^{-1}=A\begin{bmatrix}\vec{x_1}\cdots\vec{x_n}\end{bmatrix}=\begin{bmatrix}A\vec{x_1}\\\vdots\\A\vec{x_n}\end{bmatrix}=\begin{bmatrix}\vec{e_1}^T\\\vdots\\\vec{e_n}^T\end{bmatrix}$. $\therefore A\vec{x_i}=\vec{e_i}^T\text{for } 1\leq{i}\leq{n}$ By Cramer's Rule, $x_{ji}=\frac{det(A_j)}{det(A)}\text{for }1\leq{i,j}\leq{n}$ where $A_j$ is the matrix formed by replacing the j th column of $A$ with the solution vector; in this case, $\vec{e_i}$. By properties of determinants, we can add a multiple of a column to another column without changing the determinant. $\therefore det(A_j)\\=det(\begin{bmatrix}\vec{a_1}\cdots\vec{a_{j-1}}\space\space\space\space\vec{e_i}\space\space\space\space\vec{a_{j+1}}\cdots\vec{a_n}\end{bmatrix})\\=det(\begin{bmatrix}\vec{a_1}-a_{1i}\vec{e_i}\cdots\vec{a_{j-1}}-a_{j-1,i}\vec{e_i}\space\space\space\space\vec{e_i}\space\space\space\space\vec{a_{j+1}}-a_{j+1,i}\vec{e_i}\cdots\vec{a_n}-a_{ni}\vec{e_i}\end{bmatrix})$ This matrix has all 0s in the i th row except for a 1 in the j th column. Thus, if we expand $det(A_j)$ along the i th row, we get: $det(A_j)\\=0\space det(A_{i,1})\times(-1)^{i+1}+\cdots+1\space det(A_{i,j})\times(-1)^{i+j}+\cdots+0\space det(A_{i,n})\times(-1)^{i+n}\\=det(A_{i,j})\times(-1)^{i+j}$ where $A{i,j}$ is the matrix formed by removing the i th row and j th column of A. $\therefore x_{ji}=\frac{det(A_j)}{det(A)}$ $\therefore A^{-1}=\frac{adj(A_j)}{det(A)}$ QED My questions are: - Can I use Cramer's rule in the proof? - Is this proof correct? - Are there any terminology issues or blatant inconsistencies/errors? Thanks.","I have a proof for the method to find the inverse by taking the adjugate of the matrix over the determinant. I believe that it is correct; however, I would like to have it checked over before I submit it for bonus marks. The proof goes as follows: Let $A\in\mathbb{M}_{n\times{n}}(\mathbb{R})$ be an $n\times{n}$ invertible matrix. Let $A^{-1}=\begin{bmatrix}\vec{x_1}\cdots\vec{x_n}\end{bmatrix}$. $AA^{-1}=A\begin{bmatrix}\vec{x_1}\cdots\vec{x_n}\end{bmatrix}=\begin{bmatrix}A\vec{x_1}\\\vdots\\A\vec{x_n}\end{bmatrix}=\begin{bmatrix}\vec{e_1}^T\\\vdots\\\vec{e_n}^T\end{bmatrix}$. $\therefore A\vec{x_i}=\vec{e_i}^T\text{for } 1\leq{i}\leq{n}$ By Cramer's Rule, $x_{ji}=\frac{det(A_j)}{det(A)}\text{for }1\leq{i,j}\leq{n}$ where $A_j$ is the matrix formed by replacing the j th column of $A$ with the solution vector; in this case, $\vec{e_i}$. By properties of determinants, we can add a multiple of a column to another column without changing the determinant. $\therefore det(A_j)\\=det(\begin{bmatrix}\vec{a_1}\cdots\vec{a_{j-1}}\space\space\space\space\vec{e_i}\space\space\space\space\vec{a_{j+1}}\cdots\vec{a_n}\end{bmatrix})\\=det(\begin{bmatrix}\vec{a_1}-a_{1i}\vec{e_i}\cdots\vec{a_{j-1}}-a_{j-1,i}\vec{e_i}\space\space\space\space\vec{e_i}\space\space\space\space\vec{a_{j+1}}-a_{j+1,i}\vec{e_i}\cdots\vec{a_n}-a_{ni}\vec{e_i}\end{bmatrix})$ This matrix has all 0s in the i th row except for a 1 in the j th column. Thus, if we expand $det(A_j)$ along the i th row, we get: $det(A_j)\\=0\space det(A_{i,1})\times(-1)^{i+1}+\cdots+1\space det(A_{i,j})\times(-1)^{i+j}+\cdots+0\space det(A_{i,n})\times(-1)^{i+n}\\=det(A_{i,j})\times(-1)^{i+j}$ where $A{i,j}$ is the matrix formed by removing the i th row and j th column of A. $\therefore x_{ji}=\frac{det(A_j)}{det(A)}$ $\therefore A^{-1}=\frac{adj(A_j)}{det(A)}$ QED My questions are: - Can I use Cramer's rule in the proof? - Is this proof correct? - Are there any terminology issues or blatant inconsistencies/errors? Thanks.",,"['linear-algebra', 'matrices', 'proof-verification']"
