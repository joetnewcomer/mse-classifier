,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solve $A^nx=b$ for an idempotent matrix,Solve  for an idempotent matrix,A^nx=b,"Let $$A=\begin{bmatrix}2& 3& -4\\ 0& 1 & 0\\ 0.5& 1.5 &-1\end{bmatrix},~ b=\begin{bmatrix}1\\ 0\\ 0\end{bmatrix}.$$ Show that $A$ is idempotent and solve the matrix equation $$A^nx=b$$ for each positive integer $n$.","Let $$A=\begin{bmatrix}2& 3& -4\\ 0& 1 & 0\\ 0.5& 1.5 &-1\end{bmatrix},~ b=\begin{bmatrix}1\\ 0\\ 0\end{bmatrix}.$$ Show that $A$ is idempotent and solve the matrix equation $$A^nx=b$$ for each positive integer $n$.",,['matrices']
1,"If $X^n$ is a diagonal matrix with distinct eigenvalues, then is $X$ also a diagonal matrix with distinct eigenvalues?","If  is a diagonal matrix with distinct eigenvalues, then is  also a diagonal matrix with distinct eigenvalues?",X^n X,"Assume that there exists an invertible matrix $P$ such that $P^{-1}X^nP$ is a diagonal matrix with distinct eigenvalues, then can I say that $P^{-1}XP$ is also a diagonal matrix with distinct eigenvalues? If so, how do I prove it?","Assume that there exists an invertible matrix $P$ such that $P^{-1}X^nP$ is a diagonal matrix with distinct eigenvalues, then can I say that $P^{-1}XP$ is also a diagonal matrix with distinct eigenvalues? If so, how do I prove it?",,"['linear-algebra', 'matrices']"
2,Geometrical meanings of diagonalizable and normal matrices,Geometrical meanings of diagonalizable and normal matrices,,"We know that normal matrices are diagonalizable, but the converse is not true. For example, see here . Since a diagonalizable matrix represents a scaling operation under certain basis, so I wonder what additional geometrical meanings a normal matrix processes to be distinguished from other diagonalizable matrices. In other words, how to geometrically interpret matrices that can be diagonalized by unitary and non-unitary matrices? Thanks!","We know that normal matrices are diagonalizable, but the converse is not true. For example, see here . Since a diagonalizable matrix represents a scaling operation under certain basis, so I wonder what additional geometrical meanings a normal matrix processes to be distinguished from other diagonalizable matrices. In other words, how to geometrically interpret matrices that can be diagonalized by unitary and non-unitary matrices? Thanks!",,"['linear-algebra', 'matrices', 'diagonalization', 'geometric-interpretation']"
3,Unimodular matrix definition?,Unimodular matrix definition?,,"I'm a bit confused. Based on Wikipedia : In mathematics, a unimodular matrix M is a square integer matrix   having determinant +1, 0 or −1. Equivalently, it is an integer matrix that is invertible over the integers. So determinant could be +1, 0 or −1. But a matrix is invertible only if determinat is non-zero! In fact, from Wolfram : A unimodular matrix is a real square matrix A with determinant det(A) = -1|+1. Which is right answer?","I'm a bit confused. Based on Wikipedia : In mathematics, a unimodular matrix M is a square integer matrix   having determinant +1, 0 or −1. Equivalently, it is an integer matrix that is invertible over the integers. So determinant could be +1, 0 or −1. But a matrix is invertible only if determinat is non-zero! In fact, from Wolfram : A unimodular matrix is a real square matrix A with determinant det(A) = -1|+1. Which is right answer?",,"['matrices', 'optimization', 'integer-programming', 'network-flow', 'operations-research']"
4,Solution to $AX=XB$ for $3\times3$ rotation matrices.,Solution to  for  rotation matrices.,AX=XB 3\times3,"Given the $3\times3$ rotation matrices $A$ and $B$, find the rotation matrix $X$ that satisfies $$AX=XB.$$","Given the $3\times3$ rotation matrices $A$ and $B$, find the rotation matrix $X$ that satisfies $$AX=XB.$$",,"['linear-algebra', 'matrices', 'matrix-equations', 'rotations']"
5,positive definite of hessian matrix around a point,positive definite of hessian matrix around a point,,"Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be a $C^2 $ function and $x^*$ be a point such that $\bigtriangledown^2f(x^*)$ is positive definite.Is it always true that,there exists a neighborhood around $x^*$ such that for all points $x$ in that neighborhood ,we have $\bigtriangledown ^2f(x)$ is positive definite?","Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be a $C^2 $ function and $x^*$ be a point such that $\bigtriangledown^2f(x^*)$ is positive definite.Is it always true that,there exists a neighborhood around $x^*$ such that for all points $x$ in that neighborhood ,we have $\bigtriangledown ^2f(x)$ is positive definite?",,"['matrices', 'functions']"
6,Diagonal dominance versus positive semi-definiteness,Diagonal dominance versus positive semi-definiteness,,"I know that for a symmetric matrix $A$, diagonal dominance, i.e. $$A_{ii} \ge \sum\limits_{j \ne i} |A_{ij}|$$ implies positive semi-definiteness. How about the other way? Does positive semi-definiteness imply diagonal dominance? Could you point to a proof or a counter example?","I know that for a symmetric matrix $A$, diagonal dominance, i.e. $$A_{ii} \ge \sum\limits_{j \ne i} |A_{ij}|$$ implies positive semi-definiteness. How about the other way? Does positive semi-definiteness imply diagonal dominance? Could you point to a proof or a counter example?",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
7,The matrix exponential: Any good books?,The matrix exponential: Any good books?,,"Looking for a book/article with a lucid exposition of the matrix exponential, preferably including the case of infinite matrices. Basic properties especially, but also differential equations are of interest.","Looking for a book/article with a lucid exposition of the matrix exponential, preferably including the case of infinite matrices. Basic properties especially, but also differential equations are of interest.",,"['matrices', 'reference-request', 'exponential-function', 'book-recommendation']"
8,Proving that matrix $B=C$,Proving that matrix,B=C,"Let $A,B$ be $n×n$ matrices such that $BA = I_n$, where $I_n$ is the identity matrix. $(1)$ Suppose that there exists $n×n$ matrix $C$ such that $AC = I_n$. Using properties of the matrix multiplication only (Theorem 2, sec. 2.1, p.113) show that $B = C$. $(2)$ Show that if $A, B$ satisfy $BA = I_n$, then C satisfying $AC = I_n$ exists. Hint: Consider equation $Ax ̄ = e_i$ for $e_i$ - an elements from the standard basis of $R^n$. Show that this equation has a unique solution for each $e_i$. Theorem 2 Attempt: $(1)$Ehhm... since $BA=I_n$ then $B$ is the inverse of $A$. Same with $C$. And since a matrix has a unique inverse $B=C$. But, I don't see how to prove it using the properties of multiplication only. Maybe something like this: $$A=B^{-1}I_n=B^{-1}\\A=I_nC^{-1}=C^{-1}\\C^{-1}=B^{-1}\\C=B$$ So, I am not sure about this one because I am using the inverse of a matrix, but I am told to only use the properties of multiplication. Hints please. $(2)$ Lets say that $BA=I_3$. Then, $\vec e_1=\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}\; ,\vec e_2=\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}\; ,\vec e_3=\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$. Then, $AC=I_3$, where $C=\begin{pmatrix} c_{11} & c_{12} & c_{13} \\ c_{21} & c_{22} & c_{23} \\ c_{31} & c_{32} & c_{33} \end{pmatrix}$. So, $A\begin{pmatrix} c_{11} \\ c_{21} \\ c_{31} \end{pmatrix}= \vec e_1$. I am trying to use that hint here($A\vec x=e_i$ has a unique solution).How do I show that this system is consistent and has a unique solution. Thanks.","Let $A,B$ be $n×n$ matrices such that $BA = I_n$, where $I_n$ is the identity matrix. $(1)$ Suppose that there exists $n×n$ matrix $C$ such that $AC = I_n$. Using properties of the matrix multiplication only (Theorem 2, sec. 2.1, p.113) show that $B = C$. $(2)$ Show that if $A, B$ satisfy $BA = I_n$, then C satisfying $AC = I_n$ exists. Hint: Consider equation $Ax ̄ = e_i$ for $e_i$ - an elements from the standard basis of $R^n$. Show that this equation has a unique solution for each $e_i$. Theorem 2 Attempt: $(1)$Ehhm... since $BA=I_n$ then $B$ is the inverse of $A$. Same with $C$. And since a matrix has a unique inverse $B=C$. But, I don't see how to prove it using the properties of multiplication only. Maybe something like this: $$A=B^{-1}I_n=B^{-1}\\A=I_nC^{-1}=C^{-1}\\C^{-1}=B^{-1}\\C=B$$ So, I am not sure about this one because I am using the inverse of a matrix, but I am told to only use the properties of multiplication. Hints please. $(2)$ Lets say that $BA=I_3$. Then, $\vec e_1=\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}\; ,\vec e_2=\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}\; ,\vec e_3=\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$. Then, $AC=I_3$, where $C=\begin{pmatrix} c_{11} & c_{12} & c_{13} \\ c_{21} & c_{22} & c_{23} \\ c_{31} & c_{32} & c_{33} \end{pmatrix}$. So, $A\begin{pmatrix} c_{11} \\ c_{21} \\ c_{31} \end{pmatrix}= \vec e_1$. I am trying to use that hint here($A\vec x=e_i$ has a unique solution).How do I show that this system is consistent and has a unique solution. Thanks.",,"['linear-algebra', 'matrices']"
9,Is the function $X \mapsto \mbox{trace} \left( X X^T \right)$ convex?,Is the function  convex?,X \mapsto \mbox{trace} \left( X X^T \right),"Is function $X \mapsto \mbox{trace} \left( X X^T \right)$ convex? A linear function of a quadratic should be convex, but I could not prove by definition.","Is function convex? A linear function of a quadratic should be convex, but I could not prove by definition.",X \mapsto \mbox{trace} \left( X X^T \right),"['matrices', 'convex-analysis', 'trace', 'matrix-norms', 'scalar-fields']"
10,When is the geometric multiplicity of an eigenvalue smaller than its algebraic multiplicity?,When is the geometric multiplicity of an eigenvalue smaller than its algebraic multiplicity?,,"I was kinda crushed to discover that two different matrices  with different properties can actually share the same characteristic polynomial ($-\lambda^3-3\lambda^2+4$): $A=\begin{pmatrix} 1 &  2& 2\\  -3 &-5  &-3 \\   3& 3 & 1 \end{pmatrix} , B=\begin{pmatrix} 2 & 4& 3\\  -4 &-6  &-3 \\   3& 3 & 1 \end{pmatrix}$ $A$ has an eigenline and an eigenplane (and thus an eigenbasis), whereas $B$ has two eigenlines (so no eigenbasis). The repeated eigenvalue -2 of B corresponds to an eigenspace with basis {(-1,1,0)}. When is the geometric multiplicity of an eigenvalue smaller than its algebraic multiplicity (as in case B)? Are there general conditions to look for? Thanks!","I was kinda crushed to discover that two different matrices  with different properties can actually share the same characteristic polynomial ($-\lambda^3-3\lambda^2+4$): $A=\begin{pmatrix} 1 &  2& 2\\  -3 &-5  &-3 \\   3& 3 & 1 \end{pmatrix} , B=\begin{pmatrix} 2 & 4& 3\\  -4 &-6  &-3 \\   3& 3 & 1 \end{pmatrix}$ $A$ has an eigenline and an eigenplane (and thus an eigenbasis), whereas $B$ has two eigenlines (so no eigenbasis). The repeated eigenvalue -2 of B corresponds to an eigenspace with basis {(-1,1,0)}. When is the geometric multiplicity of an eigenvalue smaller than its algebraic multiplicity (as in case B)? Are there general conditions to look for? Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
11,How does the Siamese method to construct any size of n-odd magic squares work?,How does the Siamese method to construct any size of n-odd magic squares work?,,"A Magic Square of order n is an arrangement of $n^2$ numbers, usually distinct integers, in a square, such that the n numbers in all rows, all columns, and both diagonals sum to the same constant. To construct Magic Squares of n-odd size, a method known as Siamese method is given on Wikipedia , the method is :: starting from the central box of the first row with the number 1 (or the first number of any arithmetic progression), the fundamental  movement for filling the boxes is diagonally up and right (↗), one step at a time. When a move would leave the square, it is wrapped around to  the last row or first column, respectively.  If a filled box is encountered, one moves vertically down one box (↓)     instead, then continuing as before. How does this method work?","A Magic Square of order n is an arrangement of $n^2$ numbers, usually distinct integers, in a square, such that the n numbers in all rows, all columns, and both diagonals sum to the same constant. To construct Magic Squares of n-odd size, a method known as Siamese method is given on Wikipedia , the method is :: starting from the central box of the first row with the number 1 (or the first number of any arithmetic progression), the fundamental  movement for filling the boxes is diagonally up and right (↗), one step at a time. When a move would leave the square, it is wrapped around to  the last row or first column, respectively.  If a filled box is encountered, one moves vertically down one box (↓)     instead, then continuing as before. How does this method work?",,"['matrices', 'recreational-mathematics', 'magic-square']"
12,"Show $A$ is ""real-equivalent"" to its transpose","Show  is ""real-equivalent"" to its transpose",A,"The problem statement: Let $A$ be a real $9\times 9$ matrix with transpose $B$. Prove that the matrices are real equivalent in the following sense: There exists a real invertible $9\times 9$ matrix $H$ such that $AH=HB$. Unfortunately, I don't have much of an attempt at a solution.  My first thought was to notice that $A$ and $B$ have the same Jordan normal form so they're similar. But the Jordan form may have complex entries as the eigenvalues may be complex. Also confusing is why the matrix is $9\times 9$.  How could this matter?  Nine is a perfect square and it's odd, but so are a lot of other numbers. Any help is welcomed.","The problem statement: Let $A$ be a real $9\times 9$ matrix with transpose $B$. Prove that the matrices are real equivalent in the following sense: There exists a real invertible $9\times 9$ matrix $H$ such that $AH=HB$. Unfortunately, I don't have much of an attempt at a solution.  My first thought was to notice that $A$ and $B$ have the same Jordan normal form so they're similar. But the Jordan form may have complex entries as the eigenvalues may be complex. Also confusing is why the matrix is $9\times 9$.  How could this matter?  Nine is a perfect square and it's odd, but so are a lot of other numbers. Any help is welcomed.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
13,Is it possible to determine if this matrix is ill-conditioned?,Is it possible to determine if this matrix is ill-conditioned?,,"I want to better understand ill-conditioning for matrices. Say we're given any matrix $A$, where some elements are $10^6$ in magnitude and some are $10^{-7}$ in magnitude. Does this guarantee that this matrix has a condition number greater than 100? Greater than 1000? Even though we haven't specified which elements of $A$ contain those values?","I want to better understand ill-conditioning for matrices. Say we're given any matrix $A$, where some elements are $10^6$ in magnitude and some are $10^{-7}$ in magnitude. Does this guarantee that this matrix has a condition number greater than 100? Greater than 1000? Even though we haven't specified which elements of $A$ contain those values?",,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra']"
14,Interesting Determinant,Interesting Determinant,,"Let $x_1,x_2,\ldots,x_n$ be $n$ real numbers that satisfy $x_1<x_2<\cdots<x_n$.  Define \begin{equation*} A=% \begin{bmatrix} 0 & x_{2}-x_{1} & \cdots & x_{n-1}-x_{1} & x_{n}-x_{1} \\  x_{2}-x_{1} & 0 & \cdots & x_{n-1}-x_{2} & x_{n}-x_{2} \\  \vdots & \vdots & \ddots & \vdots & \vdots \\  x_{n-1}-x_{1} & x_{n-1}-x_{2} & \cdots & 0 & x_{n}-x_{n-1} \\  x_{n}-x_{1} & x_{n}-x_{2} & \cdots & x_{n}-x_{n-1} & 0% \end{bmatrix}% \end{equation*} Could you determine the determinant of $A$ in term of $x_1,x_2,\ldots,x_n$? I make a several Calculation:  For $n=2$, we get \begin{equation*} A=% \begin{bmatrix} 0 & x_{2}-x_{1} \\  x_{2}-x_{1} & 0% \end{bmatrix}% \text{ and}\det (A)=-\left( x_{2}-x_{1}\right) ^{2} \end{equation*} For $n=3$, we get \begin{equation*} A=% \begin{bmatrix} 0 & x_{2}-x_{1} & x_{3}-x_{1} \\  x_{2}-x_{1} & 0 & x_{3}-x_{2} \\  x_{3}-x_{1} & x_{3}-x_{2} & 0% \end{bmatrix}% \text{ and}\det (A)=2\left( x_{2}-x_{1}\right) \left( x_{3}-x_{2}\right) \left( x_{3}-x_{1}\right)  \end{equation*} For $n=4,$ we get \begin{equation*} A=% \begin{bmatrix} 0 & x_{2}-x_{1} & x_{3}-x_{1} & x_{4}-x_{1} \\  x_{2}-x_{1} & 0 & x_{3}-x_{2} & x_{4}-x_{2} \\  x_{3}-x_{1} & x_{3}-x_{2} & 0 & x_{4}-x_{3} \\  x_{4}-x_{1} & x_{4}-x_{2} & x_{4}-x_{3} & 0% \end{bmatrix} \\% \text{ and} \\ \det (A)=-4\left( x_{4}-x_{1}\right) \left( x_{2}-x_{1}\right) \left( x_{3}-x_{2}\right) \left( x_{4}-x_{3}\right)  \end{equation*} Finally, I guess that the answer is $\det(A)=2^{n-2}\cdot (x_n-x_1)\cdot (x_2-x_1)\cdots (x_n-x_{n-1})$. But I don't know how to prove it.","Let $x_1,x_2,\ldots,x_n$ be $n$ real numbers that satisfy $x_1<x_2<\cdots<x_n$.  Define \begin{equation*} A=% \begin{bmatrix} 0 & x_{2}-x_{1} & \cdots & x_{n-1}-x_{1} & x_{n}-x_{1} \\  x_{2}-x_{1} & 0 & \cdots & x_{n-1}-x_{2} & x_{n}-x_{2} \\  \vdots & \vdots & \ddots & \vdots & \vdots \\  x_{n-1}-x_{1} & x_{n-1}-x_{2} & \cdots & 0 & x_{n}-x_{n-1} \\  x_{n}-x_{1} & x_{n}-x_{2} & \cdots & x_{n}-x_{n-1} & 0% \end{bmatrix}% \end{equation*} Could you determine the determinant of $A$ in term of $x_1,x_2,\ldots,x_n$? I make a several Calculation:  For $n=2$, we get \begin{equation*} A=% \begin{bmatrix} 0 & x_{2}-x_{1} \\  x_{2}-x_{1} & 0% \end{bmatrix}% \text{ and}\det (A)=-\left( x_{2}-x_{1}\right) ^{2} \end{equation*} For $n=3$, we get \begin{equation*} A=% \begin{bmatrix} 0 & x_{2}-x_{1} & x_{3}-x_{1} \\  x_{2}-x_{1} & 0 & x_{3}-x_{2} \\  x_{3}-x_{1} & x_{3}-x_{2} & 0% \end{bmatrix}% \text{ and}\det (A)=2\left( x_{2}-x_{1}\right) \left( x_{3}-x_{2}\right) \left( x_{3}-x_{1}\right)  \end{equation*} For $n=4,$ we get \begin{equation*} A=% \begin{bmatrix} 0 & x_{2}-x_{1} & x_{3}-x_{1} & x_{4}-x_{1} \\  x_{2}-x_{1} & 0 & x_{3}-x_{2} & x_{4}-x_{2} \\  x_{3}-x_{1} & x_{3}-x_{2} & 0 & x_{4}-x_{3} \\  x_{4}-x_{1} & x_{4}-x_{2} & x_{4}-x_{3} & 0% \end{bmatrix} \\% \text{ and} \\ \det (A)=-4\left( x_{4}-x_{1}\right) \left( x_{2}-x_{1}\right) \left( x_{3}-x_{2}\right) \left( x_{4}-x_{3}\right)  \end{equation*} Finally, I guess that the answer is $\det(A)=2^{n-2}\cdot (x_n-x_1)\cdot (x_2-x_1)\cdots (x_n-x_{n-1})$. But I don't know how to prove it.",,"['linear-algebra', 'matrices', 'determinant']"
15,Group theoretical characterization of diagonal matrices,Group theoretical characterization of diagonal matrices,,"Let $k$ be a field. Is there a group-theoretical characterization of the subgroup $D_n$ of diagonal matrices in $GL_n(k)$ ? For example, if $k = \mathbb{C}\;$ then $D_n$ is a maximal torus, but, of course, there are many of them.","Let $k$ be a field. Is there a group-theoretical characterization of the subgroup $D_n$ of diagonal matrices in $GL_n(k)$ ? For example, if $k = \mathbb{C}\;$ then $D_n$ is a maximal torus, but, of course, there are many of them.",,"['group-theory', 'matrices']"
16,Is there any relation between the principal eigenvalue of sub matrix and the original matrix?,Is there any relation between the principal eigenvalue of sub matrix and the original matrix?,,"I am wondering whether there is any relation between principal eigenvalue of sub matrix and the original matrix. In fact I am facing a problem which is to select $n$ rows and $n$ columns from the original non-negative matrix to construct a new matrix. The principal eigenvalue of the small matrix selected need to be close to certain constant. I have totally no idea how to start... I guess figuring out the relation maybe a good starting point of this problem. UPDATE: I set up a conceptual optimization problem, hope this can help on the understanding of my problem. $\min |\max (xAx')-\lambda^*|$ s.t. $\lVert x\rVert_2=1$ $x_i=[w_iv_i]$ $v_i>0$ $w_i=\{0,1\}$ $\sum_i w_i=n$ $A$ is the original matrix, $n$ is the number of rows/columns I used to construct the small matrix, $\lambda^*$ is the target constant of the principal eigenvalue of small matrix. What I want to know is the $w$ vector","I am wondering whether there is any relation between principal eigenvalue of sub matrix and the original matrix. In fact I am facing a problem which is to select $n$ rows and $n$ columns from the original non-negative matrix to construct a new matrix. The principal eigenvalue of the small matrix selected need to be close to certain constant. I have totally no idea how to start... I guess figuring out the relation maybe a good starting point of this problem. UPDATE: I set up a conceptual optimization problem, hope this can help on the understanding of my problem. $\min |\max (xAx')-\lambda^*|$ s.t. $\lVert x\rVert_2=1$ $x_i=[w_iv_i]$ $v_i>0$ $w_i=\{0,1\}$ $\sum_i w_i=n$ $A$ is the original matrix, $n$ is the number of rows/columns I used to construct the small matrix, $\lambda^*$ is the target constant of the principal eigenvalue of small matrix. What I want to know is the $w$ vector",,"['matrices', 'optimization', 'eigenvalues-eigenvectors', 'spectral-graph-theory']"
17,Eigenvalues of the matrix $(-1)^{i_1+i_2+\cdots+i_k+j_1+j_2+\cdots+j_k}$,Eigenvalues of the matrix,(-1)^{i_1+i_2+\cdots+i_k+j_1+j_2+\cdots+j_k},"$M_{[i],[j]}=(-1)^{i_1+i_2+\cdots+i_k+j_1+j_2+\cdots+j_k}$, where $1\le i_1<i_2<\cdots<i_k\le n$ and $1\le j_1<j_2<\cdots<j_k\le n$, can be taken to be an $\left(n\atop k\right)\times\left(n\atop k\right)$ dimensional square matrix, with indices $[i],[j]=[i_1,i_2,...,i_k],[j_1,j_2,...,j_k]$. By experiment in Maple for specific low-ish dimensional cases, all its eigenvalues are zero except for one, which takes the value $\left(n\atop k\right)$. What theorem or lemma can I cite to this effect for the general case, and in what book or paper? This emerged in some Fermionic quantum field computations. All I really care about is whether this matrix is positive semi-definite, which it pretty clearly is from an experimental mathematics point of view and because of the antisymmetry, so if citing for positive semi-definiteness is easier, please feel free to do that instead.","$M_{[i],[j]}=(-1)^{i_1+i_2+\cdots+i_k+j_1+j_2+\cdots+j_k}$, where $1\le i_1<i_2<\cdots<i_k\le n$ and $1\le j_1<j_2<\cdots<j_k\le n$, can be taken to be an $\left(n\atop k\right)\times\left(n\atop k\right)$ dimensional square matrix, with indices $[i],[j]=[i_1,i_2,...,i_k],[j_1,j_2,...,j_k]$. By experiment in Maple for specific low-ish dimensional cases, all its eigenvalues are zero except for one, which takes the value $\left(n\atop k\right)$. What theorem or lemma can I cite to this effect for the general case, and in what book or paper? This emerged in some Fermionic quantum field computations. All I really care about is whether this matrix is positive semi-definite, which it pretty clearly is from an experimental mathematics point of view and because of the antisymmetry, so if citing for positive semi-definiteness is easier, please feel free to do that instead.",,"['matrices', 'reference-request', 'representation-theory']"
18,How to construct magic squares of even order,How to construct magic squares of even order,,Could someone kindly point me to references on constructing magic squares of even order? Does a compact formula/algorithm exist?,Could someone kindly point me to references on constructing magic squares of even order? Does a compact formula/algorithm exist?,,"['matrices', 'reference-request', 'algorithms', 'recreational-mathematics']"
19,For what integers can this matrix be generated?,For what integers can this matrix be generated?,,"We call a positive integer $n$ ""good"" if there exists a $n \times n$ matrix such that: 1) Each element is either $0$ or $1$. 2) Sum of elements in each row is distinct. 3) Sum of elements in each column is the same of that in all other columns. For example 2 is good: $$\begin{matrix}0&0\\1&1\end{matrix}$$ Find the set $\mathbb G$ of good numbers. I have already tried it on small values of $n$, it seems that such a matrix exists for all $n$ ( that's at least for the values I tried ), is that right? And I'm looking for a concrete proof. Any help would be appreciated. Thanks in advance!","We call a positive integer $n$ ""good"" if there exists a $n \times n$ matrix such that: 1) Each element is either $0$ or $1$. 2) Sum of elements in each row is distinct. 3) Sum of elements in each column is the same of that in all other columns. For example 2 is good: $$\begin{matrix}0&0\\1&1\end{matrix}$$ Find the set $\mathbb G$ of good numbers. I have already tried it on small values of $n$, it seems that such a matrix exists for all $n$ ( that's at least for the values I tried ), is that right? And I'm looking for a concrete proof. Any help would be appreciated. Thanks in advance!",,"['matrices', 'induction']"
20,Eigenvectors of a normal matrix,Eigenvectors of a normal matrix,,"According to the spectral theorem every normal matrix can be represented as the product of a unitary matrix $\mathbb{U}$ and a diagonal matrix $\mathbb{D}$  $$\mathbb{A} = \mathbb{U}^H\mathbb{D}\mathbb{U}$$ meaning that every normal matrix is diagonalizable. Does it necessarily mean that the unitary matrix has to be composed from the eigenvectors of $\mathbb{A}$ ?  I presume that not, because then the eigenvectors of every normal matrix would form an orthonormal set (rows and columns of a unitary matrix are orthonormal in $\mathbb{C}^n$). So am I right that only the set of eigenvectors of a hermitian (or symmetric while in $\mathbb{R}^n$) matrix is orthonormal?","According to the spectral theorem every normal matrix can be represented as the product of a unitary matrix $\mathbb{U}$ and a diagonal matrix $\mathbb{D}$  $$\mathbb{A} = \mathbb{U}^H\mathbb{D}\mathbb{U}$$ meaning that every normal matrix is diagonalizable. Does it necessarily mean that the unitary matrix has to be composed from the eigenvectors of $\mathbb{A}$ ?  I presume that not, because then the eigenvectors of every normal matrix would form an orthonormal set (rows and columns of a unitary matrix are orthonormal in $\mathbb{C}^n$). So am I right that only the set of eigenvectors of a hermitian (or symmetric while in $\mathbb{R}^n$) matrix is orthonormal?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
21,Let $k$ be an odd integer. Let $A$ be a matrix such that $A^k = A + I$. Prove that $\det(A) > 0$.,Let  be an odd integer. Let  be a matrix such that . Prove that .,k A A^k = A + I \det(A) > 0,"Problem Let $k$ be an odd integer. Let $A$ be a matrix such that $A^{k} = A+ I$ . Prove that $\det(A) > 0$ . My attempt: We know that $\det(A) = \lambda_1\dots\lambda_n$ where $\lambda_1,\dots,\lambda_n$ are the complex eigenvalues of $A$ . We know that $\lambda_1,\dots,\lambda_n$ are roots of $X^{2k+1} - X -1$ . I can see graphically that the only real root of $X^{2k+1} -X -1$ is positive by plotting some graphs, and since the product of two conjugate complex numbers is positive, I think it has something to do with it. The issue is that if $\lambda$ is an eigenvalue of $A$ , I'm not sure that $\bar{\lambda}$ is an eigenvalue as well since $A$ possibly has complex coefficients (it's not specified in the statement).","Problem Let be an odd integer. Let be a matrix such that . Prove that . My attempt: We know that where are the complex eigenvalues of . We know that are roots of . I can see graphically that the only real root of is positive by plotting some graphs, and since the product of two conjugate complex numbers is positive, I think it has something to do with it. The issue is that if is an eigenvalue of , I'm not sure that is an eigenvalue as well since possibly has complex coefficients (it's not specified in the statement).","k A A^{k} = A+ I \det(A) > 0 \det(A) = \lambda_1\dots\lambda_n \lambda_1,\dots,\lambda_n A \lambda_1,\dots,\lambda_n X^{2k+1} - X -1 X^{2k+1} -X -1 \lambda A \bar{\lambda} A","['linear-algebra', 'matrices', 'determinant']"
22,"If $A,B.C$ are matrices and $ABC=X$ is it possible to find $AC$?",If  are matrices and  is it possible to find ?,"A,B.C ABC=X AC","Let $A,B,C$ be matrices. If $A=BC$ then $C= B^{-1}A$ (assuming $B$ is invertible) But what if $A,B,C$ are matrices  and if $ABC=X$ and $B$ is $n\times n$ invertible matrix, $A$ is an $m\times n$ matrix and $C$ is a $n \times m$ matrix is it possible to find $AC$ by Knowing $B$ and $X$ ? If it is possible, how to find it ? If it was not always possible, what are the necessary conditions on $A,B,C,X$ so that finding $AC$ becomes possible?","Let be matrices. If then (assuming is invertible) But what if are matrices  and if and is invertible matrix, is an matrix and is a matrix is it possible to find by Knowing and ? If it is possible, how to find it ? If it was not always possible, what are the necessary conditions on so that finding becomes possible?","A,B,C A=BC C= B^{-1}A B A,B,C ABC=X B n\times n A m\times n C n \times m AC B X A,B,C,X AC","['linear-algebra', 'matrices']"
23,Proving two matrices have the same eigenvalues when one is Hermitian matrix,Proving two matrices have the same eigenvalues when one is Hermitian matrix,,"I have a $A$ Hermitian matrix that can be written as $A = B + iC$ where $i$ is an imaginary number. Also, $B$ and $C$ are $n$ dim real-value matrices. Now, there is a matrix, $$   D = \begin{pmatrix} B & -C \\ C & B \end{pmatrix} $$ Here, I would like to prove that $A$ and $D$ have the same eigenvalues in this case. I have first attempted to prove that D is actually a symmetric matrix as below, $$   \begin{align*}      A = A^* = \bar{A}^T \\      A = B + iC = (B - iC)^T, B = B^T, C = -C^T \\      D = \begin{pmatrix} B & -C \\ C & B \end{pmatrix} = \begin{pmatrix} B^T & C^T \\ -C^T & B^T \end{pmatrix} = D^T   \end{align*} $$ However, I have no clue about writing the relation of eigenvalues with respect to matrices $A$ and $D$ . Can anyone give me some suggestions for the next step? Thank you for your time!","I have a Hermitian matrix that can be written as where is an imaginary number. Also, and are dim real-value matrices. Now, there is a matrix, Here, I would like to prove that and have the same eigenvalues in this case. I have first attempted to prove that D is actually a symmetric matrix as below, However, I have no clue about writing the relation of eigenvalues with respect to matrices and . Can anyone give me some suggestions for the next step? Thank you for your time!","A A = B + iC i B C n 
  D = \begin{pmatrix} B & -C \\ C & B \end{pmatrix}
 A D 
  \begin{align*}
     A = A^* = \bar{A}^T \\
     A = B + iC = (B - iC)^T, B = B^T, C = -C^T \\
     D = \begin{pmatrix} B & -C \\ C & B \end{pmatrix} = \begin{pmatrix} B^T & C^T \\ -C^T & B^T \end{pmatrix} = D^T
  \end{align*}
 A D","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
24,Rank of a matrix with $a_{ii}=0$ and $a_{ij}+a_{ji}=1$ [duplicate],Rank of a matrix with  and  [duplicate],a_{ii}=0 a_{ij}+a_{ji}=1,"This question already has an answer here : $a_{ii}=0, a_{ij}+a_{ji}=1, 1\leq i<j\leq n$. This matrix has rank $\geq n-1$? [closed] (1 answer) Closed 11 months ago . Let $A=(a_{ij})$ and an $n\times n$ real matrix such that $$a_{ii}=0~(1\le i\le n),\quad a_{ij}+a_{ji}=1~(1\le i<j\le n).$$ Then prove that ${\rm rank}(A)\ge n-1 $ . This is a problem I saw in an exercise book. It is clear that $A+A^{\rm T}$ is a matrix with all diagonal entries $0$ and off-diagonal entries $1$ . Can we get the ${\rm rank}(A)$ from it? Or how can we prove it in other ways? Thanks.","This question already has an answer here : $a_{ii}=0, a_{ij}+a_{ji}=1, 1\leq i<j\leq n$. This matrix has rank $\geq n-1$? [closed] (1 answer) Closed 11 months ago . Let and an real matrix such that Then prove that . This is a problem I saw in an exercise book. It is clear that is a matrix with all diagonal entries and off-diagonal entries . Can we get the from it? Or how can we prove it in other ways? Thanks.","A=(a_{ij}) n\times n a_{ii}=0~(1\le i\le n),\quad a_{ij}+a_{ji}=1~(1\le i<j\le n). {\rm rank}(A)\ge n-1  A+A^{\rm T} 0 1 {\rm rank}(A)","['linear-algebra', 'matrices', 'matrix-rank']"
25,Let $A\in M_3[\mathbb{R}]$ such that $A^{2019}+A=$,Let  such that,A\in M_3[\mathbb{R}] A^{2019}+A=,"Let $A\in M_3[\mathbb{R}]$ such that $A^{2019}+A=\begin{bmatrix} 2 & 2 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 2\end{bmatrix}.$ Find $tr(A^{2019})$ and $\det(A).$ My work: Let characteristic equation of $A$ be $x^3-tx^2+sx-d=(x-\lambda_1)(x-\lambda_2)(x-\lambda_3)=0$ $\implies x^{2019}=Q(x)(x-\lambda_1)(x-\lambda_2)(x-\lambda_3)+ax^2+bx+c,\text{ where }a\lambda_{i}^2+b\lambda_i+c=\lambda_i^{2019}\,\,\forall\,\, i=1,2,3.$ $\implies A^{2019}+A=aA^2+(b+1)A+cI_3=\begin{bmatrix} 2 & 2 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 2\end{bmatrix}$ $\implies a\times tr(A^2)+b\times tr(A)+3c=6$ $\implies a\sum\lambda_i^2+(b+1)\sum\lambda_i+3c=6$ $\implies \sum (a\lambda_i^2+(b+1)\lambda_i+c)=6$",Let such that Find and My work: Let characteristic equation of be,"A\in M_3[\mathbb{R}] A^{2019}+A=\begin{bmatrix}
2 & 2 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 2\end{bmatrix}. tr(A^{2019}) \det(A). A x^3-tx^2+sx-d=(x-\lambda_1)(x-\lambda_2)(x-\lambda_3)=0 \implies x^{2019}=Q(x)(x-\lambda_1)(x-\lambda_2)(x-\lambda_3)+ax^2+bx+c,\text{ where }a\lambda_{i}^2+b\lambda_i+c=\lambda_i^{2019}\,\,\forall\,\, i=1,2,3. \implies A^{2019}+A=aA^2+(b+1)A+cI_3=\begin{bmatrix}
2 & 2 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 2\end{bmatrix} \implies a\times tr(A^2)+b\times tr(A)+3c=6 \implies a\sum\lambda_i^2+(b+1)\sum\lambda_i+3c=6 \implies \sum (a\lambda_i^2+(b+1)\lambda_i+c)=6","['matrices', 'matrix-equations']"
26,Dot product between two vectors transformed by orthogonal matrices,Dot product between two vectors transformed by orthogonal matrices,,"I am reading through the ""Matrix Transformations"" chapter of this book and more specifically on Orthogonal Matrices. I understand their properties and understand that multiplying vectors by them is an affine transformation, so the dot product between two vectors will be the same as between the same vectors multiplied by the same orthogonal matrix $\mathbf{M}$ . However this proof from the book confuses me to no end: And more specifically this: $$\mathbf{(Ma)\cdot (Mb)=(Ma)^T (Mb)}$$ What exactly is happening on the right side of the equation? Is the transpose of $\mathbf{Ma}$ somehow the reciprocal of the dot operation?","I am reading through the ""Matrix Transformations"" chapter of this book and more specifically on Orthogonal Matrices. I understand their properties and understand that multiplying vectors by them is an affine transformation, so the dot product between two vectors will be the same as between the same vectors multiplied by the same orthogonal matrix . However this proof from the book confuses me to no end: And more specifically this: What exactly is happening on the right side of the equation? Is the transpose of somehow the reciprocal of the dot operation?",\mathbf{M} \mathbf{(Ma)\cdot (Mb)=(Ma)^T (Mb)} \mathbf{Ma},"['linear-algebra', 'matrices', 'orthogonal-matrices', 'transpose']"
27,Deflation of eigenvalue,Deflation of eigenvalue,,"Could you explain to me the deflation? For example if we have the matrix $A=\begin{pmatrix}1 & -0.5 & -1.5\\ -15 & -2.5 & 4.5\\ -15 & -4.5 & 2.5\end{pmatrix}$ how do we apply the deflation of the eigenvalue $\lambda_1=4$ ? $$$$ EDIT : I have done the following : We have the eigenvalue $\lambda_1=4$ . The corresponding eigenvector is : $v_1=\begin{pmatrix}-1\\ 3 \\ 1\end{pmatrix}$ , right? Since $v_1$ does not have $1$ as the component of largest modulus, we multiply $v_1$ by a permutation matrix $P$ which interchanges the largest element and the first element: We interchange rows $1$ and $2$ of $A$ : $v_1'=\begin{pmatrix}3 \\-1\\  1\end{pmatrix}$ $$\begin{pmatrix}-15 & -2.5 & 4.5\\1 & -0.5 & -1.5\\  -15 & -4.5 & 2.5\end{pmatrix}$$ We want to transform the vector into the vector $e_1$ , so we divide the first row by $3$ , add it to the second row and subtract it from the third row : $v_1''=\begin{pmatrix}1 \\0\\  0\end{pmatrix}$ $$\begin{pmatrix}-5 & -5/6 & 1.5\\-4 & -4/3 & 0\\  -10 & -11/3 & 1\end{pmatrix}$$ Since we have interchanged the rows $1$ and $2$ , now we have to interchange the columns $1$ and $2$ and so we get the matrix $$\begin{pmatrix} -5/6 & -5 &1.5\\-4/3 & -4 & 0\\   -11/3 & -10 &1\end{pmatrix}$$ Then $\lambda_1=4$ should be an eigenvalue of this matrix with $e_1$ the corresponding eigenvector, but this is not like that. What am I doing wrong? Is the deflated matrix equal to $$\begin{pmatrix} -4 & 0\\   -10 &1\end{pmatrix}$$ ?","Could you explain to me the deflation? For example if we have the matrix how do we apply the deflation of the eigenvalue ? EDIT : I have done the following : We have the eigenvalue . The corresponding eigenvector is : , right? Since does not have as the component of largest modulus, we multiply by a permutation matrix which interchanges the largest element and the first element: We interchange rows and of : We want to transform the vector into the vector , so we divide the first row by , add it to the second row and subtract it from the third row : Since we have interchanged the rows and , now we have to interchange the columns and and so we get the matrix Then should be an eigenvalue of this matrix with the corresponding eigenvector, but this is not like that. What am I doing wrong? Is the deflated matrix equal to ?",A=\begin{pmatrix}1 & -0.5 & -1.5\\ -15 & -2.5 & 4.5\\ -15 & -4.5 & 2.5\end{pmatrix} \lambda_1=4  \lambda_1=4 v_1=\begin{pmatrix}-1\\ 3 \\ 1\end{pmatrix} v_1 1 v_1 P 1 2 A v_1'=\begin{pmatrix}3 \\-1\\  1\end{pmatrix} \begin{pmatrix}-15 & -2.5 & 4.5\\1 & -0.5 & -1.5\\  -15 & -4.5 & 2.5\end{pmatrix} e_1 3 v_1''=\begin{pmatrix}1 \\0\\  0\end{pmatrix} \begin{pmatrix}-5 & -5/6 & 1.5\\-4 & -4/3 & 0\\  -10 & -11/3 & 1\end{pmatrix} 1 2 1 2 \begin{pmatrix} -5/6 & -5 &1.5\\-4/3 & -4 & 0\\   -11/3 & -10 &1\end{pmatrix} \lambda_1=4 e_1 \begin{pmatrix} -4 & 0\\   -10 &1\end{pmatrix},"['matrices', 'eigenvalues-eigenvectors', 'numerical-methods']"
28,Solve matrix equations of the form: $X^T A X = B_1$ and $X A X^T = B_2$,Solve matrix equations of the form:  and,X^T A X = B_1 X A X^T = B_2,"I have trouble solving the following two matrix equations for unknown $X \in \mathbb{R}^{n \times n}$ : $$X^T A X = B_1$$ $$X A X^T = B_2$$ where, $A$ , $B_1$ and $B_2$ are all $n \times n$ symmetric matrices such that $A^n = I_n$ , and $B_1 \neq B_2$ There is a solution for $X$ if $A$ and the $B$ 's are positive-definite, but is there an analytical solution for any matrices? Edit: Is there a way to solve this when $A^n \neq I_n$ ? I'm interested in the case where $A$ , $B_1$ , $B_2$ and $X$ are matrices of one-to-one mappings, i.e., $A_{ij}=A_{ji}=1$ if $i$ maps to $j$ , and $0$ otherwise. For example: $$A = \begin{bmatrix}     0 & 1 & 0 & 0 & 0 \\     1 & 0 & 0 & 0 & 0 \\     0 & 0 & 0 & 1  & 0\\     0 & 0 & 1 & 0  & 0\\     0 & 0 & 0 & 0  & 1 \end{bmatrix}$$","I have trouble solving the following two matrix equations for unknown : where, , and are all symmetric matrices such that , and There is a solution for if and the 's are positive-definite, but is there an analytical solution for any matrices? Edit: Is there a way to solve this when ? I'm interested in the case where , , and are matrices of one-to-one mappings, i.e., if maps to , and otherwise. For example:","X \in \mathbb{R}^{n \times n} X^T A X = B_1 X A X^T = B_2 A B_1 B_2 n \times n A^n = I_n B_1 \neq B_2 X A B A^n \neq I_n A B_1 B_2 X A_{ij}=A_{ji}=1 i j 0 A = \begin{bmatrix}
    0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1  & 0\\
    0 & 0 & 1 & 0  & 0\\
    0 & 0 & 0 & 0  & 1
\end{bmatrix}","['linear-algebra', 'matrices', 'quadratics', 'matrix-equations']"
29,Why is the rank of an element of a null space less than the dimension of that null space?,Why is the rank of an element of a null space less than the dimension of that null space?,,"I'm trying to understand a statement I came across in a paper. Say $A$ and $B$ are $n \times n $ -dimensional real matrices, and $B$ is in the null subspace of $A$ i.e. $AB=0$ (in fact $BA=0$ is also true in this case but I´m not sure if this is relevant here). The paper states that, since $B$ is in the null subspace of $A$ , $$ \textrm{rank}(B) \leq \textrm{dim}\big(\mathcal{N}(A)\big)\,,    $$ where $\mathcal{N(.)}$ returns the null subspace of the argument. How can one prove this inequality? I tried to show it with the rank-nullity theorem, to no success. I suspect the proof may be even simpler than this.","I'm trying to understand a statement I came across in a paper. Say and are -dimensional real matrices, and is in the null subspace of i.e. (in fact is also true in this case but I´m not sure if this is relevant here). The paper states that, since is in the null subspace of , where returns the null subspace of the argument. How can one prove this inequality? I tried to show it with the rank-nullity theorem, to no success. I suspect the proof may be even simpler than this.","A B n \times n  B A AB=0 BA=0 B A  \textrm{rank}(B) \leq \textrm{dim}\big(\mathcal{N}(A)\big)\,,     \mathcal{N(.)}","['linear-algebra', 'matrices', 'linear-transformations', 'matrix-rank']"
30,Condition number of random matrix gets worse as dimension grows,Condition number of random matrix gets worse as dimension grows,,"I observed experimentally that, when generating (pseudo) random matrices, their condition number increases with the dimension. Why? When using the norm induced by the Euclidean norm for vector spaces, the condition number is the absolute value of the ratio between the greatest and the smallest eigenvalue. It seems to me that, since increasing the dimension implies getting more eigenvalues, it is easier that this ratio increases.","I observed experimentally that, when generating (pseudo) random matrices, their condition number increases with the dimension. Why? When using the norm induced by the Euclidean norm for vector spaces, the condition number is the absolute value of the ratio between the greatest and the smallest eigenvalue. It seems to me that, since increasing the dimension implies getting more eigenvalues, it is easier that this ratio increases.",,"['linear-algebra', 'matrices', 'random-matrices', 'condition-number']"
31,"Similar matrices and conjugation outside of the center of GL(n,R) (a basic linear algebra question)","Similar matrices and conjugation outside of the center of GL(n,R) (a basic linear algebra question)",,"I'm teaching an introductory linear algebra course and have confused myself about something that is probably very straightforward; I would love someone to help me see where I'm going wrong. We have just started talking about eigenanalysis and have recently shown that two matrices are similar if and only if they represent the same linear transformation. That is, we have $A = P B P^{-1}$ if and only if $A$ and $B$ are both matrix representations of $T$ in the bases $\mathcal{B}_A$ and $\mathcal{B}_B$ . This is all fine; the proof is straightforward and totally unobjectionable. My confusion comes from the fact that conjugation ( $\phi_g: a \mapsto gag^{-1}$ ) is a nontrivial action for most subgroups of linear transformations. For an example, let's look at $r$ conjugated by $s$ in the dihedral group $D_n$ . I have $r^{-1} = s r s$ , and $r$ and $r^{-1}$ are not the same linear transformation for any $n > 2$ . But $s = s^{-1}$ , so we certainly have $r^{-1} = P r P^{-1}$ (matrix similarity) for a matrix representation of $D_n$ in whatever basis we might choose. It seems like these would then be a pair of similar matrices that represent different linear transformations, which is obviously not possible. I must be operating under some kind of silly misconception or misunderstanding some basic definition; can somebody tell me what it is?","I'm teaching an introductory linear algebra course and have confused myself about something that is probably very straightforward; I would love someone to help me see where I'm going wrong. We have just started talking about eigenanalysis and have recently shown that two matrices are similar if and only if they represent the same linear transformation. That is, we have if and only if and are both matrix representations of in the bases and . This is all fine; the proof is straightforward and totally unobjectionable. My confusion comes from the fact that conjugation ( ) is a nontrivial action for most subgroups of linear transformations. For an example, let's look at conjugated by in the dihedral group . I have , and and are not the same linear transformation for any . But , so we certainly have (matrix similarity) for a matrix representation of in whatever basis we might choose. It seems like these would then be a pair of similar matrices that represent different linear transformations, which is obviously not possible. I must be operating under some kind of silly misconception or misunderstanding some basic definition; can somebody tell me what it is?",A = P B P^{-1} A B T \mathcal{B}_A \mathcal{B}_B \phi_g: a \mapsto gag^{-1} r s D_n r^{-1} = s r s r r^{-1} n > 2 s = s^{-1} r^{-1} = P r P^{-1} D_n,"['linear-algebra', 'matrices', 'linear-transformations', 'similar-matrices']"
32,What is the difference between a column vector and tuple?,What is the difference between a column vector and tuple?,,"We sometimes right a column vector as $(x_1,x_2,x_3)$ with parentheses (to save space and not confuse it with a row vector), we also tend to write euclidean vectors in this way, in both cases it looks like a 'tuple', we know that $\mathbb R^n$ is defined using tuples, is there an equivalence between the two? Can we write a column vector as being equal to a tuple, perhaps in the column vector case this is just an alternative notation for a column vector and in fact not a tuple? If there was an equivalence the difficulty is that both a row and column vector could in theory be written as a tuple, and as we know the matrices are not equal.","We sometimes right a column vector as with parentheses (to save space and not confuse it with a row vector), we also tend to write euclidean vectors in this way, in both cases it looks like a 'tuple', we know that is defined using tuples, is there an equivalence between the two? Can we write a column vector as being equal to a tuple, perhaps in the column vector case this is just an alternative notation for a column vector and in fact not a tuple? If there was an equivalence the difficulty is that both a row and column vector could in theory be written as a tuple, and as we know the matrices are not equal.","(x_1,x_2,x_3) \mathbb R^n","['linear-algebra', 'matrices', 'notation']"
33,Is a 'column vector' actually a vector or a matrix?,Is a 'column vector' actually a vector or a matrix?,,"I've read this post and posted my own question , but I think I will write a more direct question related to this topic, I understand how row and column vectors can be used to represent vectors as answered in the question, but is it actually a vector, or a way of us trying to give the idea of components by putting them in a matrix? For example, Euclidian vectors have no concept of 'transpose', if the components are equal, its the same vector, yet row and column vectors are transpose of each other? A question I have is say we have: $y = \begin{bmatrix}            x_{1} \\            x_{2} \\            x_{3}          \end{bmatrix}$ Can we have also $y = [x_{1},x_{2},x_{3}]$ ? as then we have \begin{bmatrix}   x_{1} \\   x_{2} \\      x_{3}   \end{bmatrix} = $[x_{1},x_{2},x_{3}]$ and this gives $y=y^T$ which would be an incorrect result.","I've read this post and posted my own question , but I think I will write a more direct question related to this topic, I understand how row and column vectors can be used to represent vectors as answered in the question, but is it actually a vector, or a way of us trying to give the idea of components by putting them in a matrix? For example, Euclidian vectors have no concept of 'transpose', if the components are equal, its the same vector, yet row and column vectors are transpose of each other? A question I have is say we have: Can we have also ? as then we have = and this gives which would be an incorrect result.","y = \begin{bmatrix}
           x_{1} \\
           x_{2} \\
           x_{3}
         \end{bmatrix} y = [x_{1},x_{2},x_{3}] \begin{bmatrix}
  x_{1} \\
  x_{2} \\   
  x_{3}
  \end{bmatrix} [x_{1},x_{2},x_{3}] y=y^T","['linear-algebra', 'matrices', 'vector-spaces', 'soft-question']"
34,Prove $ \mbox{adj} (A) = \left[ \frac{\partial }{\partial a_{ij} } \det(A) \right]^T $,Prove, \mbox{adj} (A) = \left[ \frac{\partial }{\partial a_{ij} } \det(A) \right]^T ,"I have a bit of trouble proving an adjugate matrix equality for $A_{n\times n} = [a_{ij}]$ , $$ \mbox{adj} (A) = \left[ \frac{\partial }{\partial a_{ij} }  \det(A)  \right]^T, \quad i, j = 1,\dots,n $$ I tried by definition $$ \mbox{adj} (A) = \left[ (-1)^{i+j} \sum_{\sigma \in C_n } \left( \mbox{sgn} (\sigma)  \prod_{k=1\\k\neq i   }^n  a_{k,\sigma(k)} \right) \right]^T $$ where $C_n = \{\sigma \in S_n : \sigma(i) = j \} $ and $S_n$ is the set of all $n$ -permutation of the set $\{1,\dots,n\}$ . $sgn(\sigma)$ is $-1$ if permuatation $\sigma$ is odd , it's $1$ otherwise . $\sigma(k)$ returns the $k$ th element of $\sigma$ . The thing within the square bracket is the $(i,j)-$ cofactor . I pause here and look at the RHS of the statement $$ \left[ \frac{\partial }{\partial a_{ij} }  \det(A)  \right]^T  = \left[\sum_{\sigma\in C_n } \left( \mbox{sgn} (\sigma)   \prod_{k=1\\ k \neq i}^n  a_{k,\sigma(k)} \right) \right]^T $$ But I'm unable to deal with the signs ...","I have a bit of trouble proving an adjugate matrix equality for , I tried by definition where and is the set of all -permutation of the set . is if permuatation is odd , it's otherwise . returns the th element of . The thing within the square bracket is the cofactor . I pause here and look at the RHS of the statement But I'm unable to deal with the signs ...","A_{n\times n} = [a_{ij}]  \mbox{adj} (A) = \left[ \frac{\partial }{\partial a_{ij} }  \det(A)  \right]^T, \quad i, j = 1,\dots,n   \mbox{adj} (A) = \left[
(-1)^{i+j} \sum_{\sigma \in C_n } \left( \mbox{sgn} (\sigma)  \prod_{k=1\\k\neq i   }^n  a_{k,\sigma(k)} \right)
\right]^T
 C_n = \{\sigma \in S_n : \sigma(i) = j \}  S_n n \{1,\dots,n\} sgn(\sigma) -1 \sigma 1 \sigma(k) k \sigma (i,j)-  \left[ \frac{\partial }{\partial a_{ij} }  \det(A)  \right]^T 
= \left[\sum_{\sigma\in C_n } \left( \mbox{sgn} (\sigma)   \prod_{k=1\\ k \neq i}^n  a_{k,\sigma(k)} \right)
\right]^T ","['abstract-algebra', 'matrices', 'proof-writing', 'determinant', 'matrix-calculus']"
35,Inverse of a 2x2 matrix with an example,Inverse of a 2x2 matrix with an example,,"Need to find inverse of this matrix: $ \begin {bmatrix} 1 & 3/5\\ 0 & 1\\ \end {bmatrix} $ This is how it has been solved: $ \begin {bmatrix} 1 & 3/5\\ 0 & 1\\ \end {bmatrix} $ $ \begin {bmatrix} x_1 & x_2\\ x_3 & x_4\\ \end {bmatrix} $ = $ \begin {bmatrix} 1 & 0\\ 0 & 1\\ \end {bmatrix} $ - step 1 $ \begin {bmatrix} 1 & 0\\ 0 & 1\\ \end {bmatrix} $ $ \begin {bmatrix} x_1 & x_2\\ x_3 & x_4\\ \end {bmatrix} $ = $ \begin {bmatrix} 1 & -3/5\\ 0 & 1\\ \end {bmatrix} $ - step 2 From step 1 to step 2, -3/5 added to first row, second column of $ \begin {bmatrix} 1 & -3/5\\ 0 & 1\\ \end {bmatrix} $ Also, -3/5 to first row,second column of $ \begin {bmatrix} 1 & 0\\ 0 & 1\\ \end {bmatrix} $ My query is if it is correct to add or deduct a particular number from say first column, first row on the right side and on the left side and both leave the equation intact?","Need to find inverse of this matrix: This is how it has been solved: = - step 1 = - step 2 From step 1 to step 2, -3/5 added to first row, second column of Also, -3/5 to first row,second column of My query is if it is correct to add or deduct a particular number from say first column, first row on the right side and on the left side and both leave the equation intact?","
\begin {bmatrix}
1 & 3/5\\
0 & 1\\
\end {bmatrix}
 
\begin {bmatrix}
1 & 3/5\\
0 & 1\\
\end {bmatrix}
 
\begin {bmatrix}
x_1 & x_2\\
x_3 & x_4\\
\end {bmatrix}
 
\begin {bmatrix}
1 & 0\\
0 & 1\\
\end {bmatrix}
 
\begin {bmatrix}
1 & 0\\
0 & 1\\
\end {bmatrix}
 
\begin {bmatrix}
x_1 & x_2\\
x_3 & x_4\\
\end {bmatrix}
 
\begin {bmatrix}
1 & -3/5\\
0 & 1\\
\end {bmatrix}
 
\begin {bmatrix}
1 & -3/5\\
0 & 1\\
\end {bmatrix}
 
\begin {bmatrix}
1 & 0\\
0 & 1\\
\end {bmatrix}
","['linear-algebra', 'matrices', 'inverse', 'gaussian-elimination']"
36,Real Schur decomposition of orthogonal matrix,Real Schur decomposition of orthogonal matrix,,"The real Schur decomposition theorem states that for any matrix $A\in\mathbb R^{n\times n}$ , there exists an orthogonal matrix $Q$ and a ""quasitriangular"" matrix $T$ such that $A=QTQ^T$ . Here, ""quasitriangular"" means that $T$ has the form $$T=\begin{pmatrix}B_1&&&*\\&B_2&&\\&&\ddots&\\0&&&B_n\end{pmatrix},$$ where all $B_i$ are either $1\times1$ or $2\times2$ matrices. These matrices form the ""quasidiagonal"". I want to prove that in the specific case of real Schur decomposition of an orthogonal matrix, $T$ must be a ""quasidiagonal matrix"", i.e. all entries above the quasidiagonal are zero. This is claimed without proof in this answer . It's easy to see that $T$ must be orthogonal. In the special case where $T$ is upper triangular, it's simple to prove that the norm of the $i$ -th column equals that of the $i$ -th row, which then yields the desired result by a simple induction. However, my attempt at adapting this proof to the quasidiagonal case failed. This result seems to me like it should be well-known. A reference would also suffice.","The real Schur decomposition theorem states that for any matrix , there exists an orthogonal matrix and a ""quasitriangular"" matrix such that . Here, ""quasitriangular"" means that has the form where all are either or matrices. These matrices form the ""quasidiagonal"". I want to prove that in the specific case of real Schur decomposition of an orthogonal matrix, must be a ""quasidiagonal matrix"", i.e. all entries above the quasidiagonal are zero. This is claimed without proof in this answer . It's easy to see that must be orthogonal. In the special case where is upper triangular, it's simple to prove that the norm of the -th column equals that of the -th row, which then yields the desired result by a simple induction. However, my attempt at adapting this proof to the quasidiagonal case failed. This result seems to me like it should be well-known. A reference would also suffice.","A\in\mathbb R^{n\times n} Q T A=QTQ^T T T=\begin{pmatrix}B_1&&&*\\&B_2&&\\&&\ddots&\\0&&&B_n\end{pmatrix}, B_i 1\times1 2\times2 T T T i i","['linear-algebra', 'matrices', 'reference-request', 'orthogonal-matrices', 'schur-decomposition']"
37,Derivative of matrix w.r.t. itself,Derivative of matrix w.r.t. itself,,"This sounds like a joke, but i am actually interested and would like an answer, but what is the derivative of a matrix $C$ w.r.t. itself? $$ \text{What is: } \frac{\delta C}{\delta C}\text{?} $$ Is it a matrix with shape equal to $C$ and filled with ones?","This sounds like a joke, but i am actually interested and would like an answer, but what is the derivative of a matrix w.r.t. itself? Is it a matrix with shape equal to and filled with ones?","C 
\text{What is: } \frac{\delta C}{\delta C}\text{?}
 C","['calculus', 'matrices', 'derivatives', 'matrix-calculus']"
38,Prove a matrix has non-zero determinant,Prove a matrix has non-zero determinant,,"While working on a problem, I managed to reduce it to showing that the matrix $A = [a_{ij}]$ , where $2a_{ij} = \frac{1}{i+j-1} - \frac{(-1)^{i+j-1}}{i+j-1}$ , has non-zero determinant, for whatever size you choose. Here are a few cases: $$\det\begin{bmatrix} 1 & 0 & \frac{1}{3} \\ 0 & \frac{1}{3} & 0 \\ \frac{1}{3} & 0 & \frac{1}{5} \end{bmatrix} = \frac{4}{135};$$ $$\det\begin{bmatrix} 1 & 0 & \frac{1}{3} & 0 \\ 0 & \frac{1}{3} & 0 & \frac{1}{5}\\ \frac{1}{3} & 0 & \frac{1}{5} & 0 \\ 0 & \frac{1}{5} & 0 & \frac{1}{7} \end{bmatrix} = \frac{16}{23625}$$ I worked on a few cases using online calculators, but explicit determinant calculation in arbitrary sizes is very cumbersome. It seems to tend to $0$ , but to always be positive. In fact, I suspect this is a positive-definite matrix, but, again, couldn't quite prove it - and I recall having seen it in a computational setting before, maybe numerical integration, though I'm not sure... Anyhow, is there any technique I can use to show this has non-zero determinant for any size I pick? Thanks in advance!","While working on a problem, I managed to reduce it to showing that the matrix , where , has non-zero determinant, for whatever size you choose. Here are a few cases: I worked on a few cases using online calculators, but explicit determinant calculation in arbitrary sizes is very cumbersome. It seems to tend to , but to always be positive. In fact, I suspect this is a positive-definite matrix, but, again, couldn't quite prove it - and I recall having seen it in a computational setting before, maybe numerical integration, though I'm not sure... Anyhow, is there any technique I can use to show this has non-zero determinant for any size I pick? Thanks in advance!",A = [a_{ij}] 2a_{ij} = \frac{1}{i+j-1} - \frac{(-1)^{i+j-1}}{i+j-1} \det\begin{bmatrix} 1 & 0 & \frac{1}{3} \\ 0 & \frac{1}{3} & 0 \\ \frac{1}{3} & 0 & \frac{1}{5} \end{bmatrix} = \frac{4}{135}; \det\begin{bmatrix} 1 & 0 & \frac{1}{3} & 0 \\ 0 & \frac{1}{3} & 0 & \frac{1}{5}\\ \frac{1}{3} & 0 & \frac{1}{5} & 0 \\ 0 & \frac{1}{5} & 0 & \frac{1}{7} \end{bmatrix} = \frac{16}{23625} 0,"['linear-algebra', 'matrices', 'determinant', 'positive-matrices']"
39,Matrix Inverse of Outer Product of a vector with itself,Matrix Inverse of Outer Product of a vector with itself,,Suppose I have a vector $x \in \mathbb{R}^n$ and the matrix $$ A = \lambda x x^\top \qquad\lambda \in \mathbb{R}\backslash\{0\} $$ I would like to find an expression for its inverse. Is there a simple expression?,Suppose I have a vector and the matrix I would like to find an expression for its inverse. Is there a simple expression?,"x \in \mathbb{R}^n 
A = \lambda x x^\top \qquad\lambda \in \mathbb{R}\backslash\{0\}
",['linear-algebra']
40,Why does an affine transformation $A$ when constrained by $A^TA=\lambda^2I$ result in a similarity transformation?,Why does an affine transformation  when constrained by  result in a similarity transformation?,A A^TA=\lambda^2I,Why does an affine transformation $A$ when constrained by $A^TA=\lambda^2I$ result in a similarity transformation? I came across this when studying linear transformations in these notes which says: The similarity group is obtained from the affine group by requiring that $A$ be orthogonal: $A^TA=\lambda^2I$ Can't seem to wrap my head around this one.,Why does an affine transformation when constrained by result in a similarity transformation? I came across this when studying linear transformations in these notes which says: The similarity group is obtained from the affine group by requiring that be orthogonal: Can't seem to wrap my head around this one.,A A^TA=\lambda^2I A A^TA=\lambda^2I,"['linear-algebra', 'matrices', 'linear-transformations']"
41,Eigenvalues of a matrix in relation to another matrix,Eigenvalues of a matrix in relation to another matrix,,"Let $A,B \in \mathcal{M}_n(\mathbb{K}) $ (where $\mathbb{K} \in \{\mathbb{R},\mathbb{C} \} $ ) invertible matrices. Prove that if $\lambda = 0 $ is an eigenvalue of matrix $C$ , where $C = BA - AB ~$ then $ \lambda = 1 $ is an eigenvalue of matrix $D~$ where $D = B^{-1}A^{-1}BA$ I don't know where to start this proof, since I can't recall any eigenvalue properties that relate sum or diference of matrices.","Let (where ) invertible matrices. Prove that if is an eigenvalue of matrix , where then is an eigenvalue of matrix where I don't know where to start this proof, since I can't recall any eigenvalue properties that relate sum or diference of matrices.","A,B \in \mathcal{M}_n(\mathbb{K})  \mathbb{K} \in \{\mathbb{R},\mathbb{C} \}  \lambda = 0  C C = BA - AB ~  \lambda = 1  D~ D = B^{-1}A^{-1}BA","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
42,How to show this particular $4 \times 4$ matrix is positive definite?,How to show this particular  matrix is positive definite?,4 \times 4,"I am preparing for an exam in Numerical Analysis, and I am solving some practice problems. The question is to show that the following matrix $A$ is positive definite. I would only have about 10 minutes to work on this problem, so I am trying to solve this as fast as possible. We are also told that all the eigenvalues of $A$ are distinct (but this information may not be useful here; there is a part b to this question which might make use of this fact) $$A = \begin{bmatrix} 1 & -1 & 2 & 0\\ -1 & 4 & -1 & 1\\ 2 & -1 & 6 & -2\\ 0 & 1 & -2 & 4 \end{bmatrix}$$ My Attempts: My first tought is to use Greshgorin's Circle Theorem to show that all the eigenvalues are positive. However, this does not work because  the first Greshgorin disk contains negative reals. My second tought is to use Sylvester's Criterion. This is perhaps doable in under 10 minutes, but it is prone to mistakes (especially when going fast). I am also not sure if Sylvester's Criterion was taught in the class that this problem comes from.","I am preparing for an exam in Numerical Analysis, and I am solving some practice problems. The question is to show that the following matrix is positive definite. I would only have about 10 minutes to work on this problem, so I am trying to solve this as fast as possible. We are also told that all the eigenvalues of are distinct (but this information may not be useful here; there is a part b to this question which might make use of this fact) My Attempts: My first tought is to use Greshgorin's Circle Theorem to show that all the eigenvalues are positive. However, this does not work because  the first Greshgorin disk contains negative reals. My second tought is to use Sylvester's Criterion. This is perhaps doable in under 10 minutes, but it is prone to mistakes (especially when going fast). I am also not sure if Sylvester's Criterion was taught in the class that this problem comes from.","A A A = \begin{bmatrix}
1 & -1 & 2 & 0\\
-1 & 4 & -1 & 1\\
2 & -1 & 6 & -2\\
0 & 1 & -2 & 4
\end{bmatrix}","['linear-algebra', 'abstract-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra']"
43,"Find $\sup _\limits{Q \in M_{4\times 2} (\mathbb{R}), Q^{T} Q=I_{2}} \operatorname{tr}\left(Q^{T} A Q\right)$ [duplicate]",Find  [duplicate],"\sup _\limits{Q \in M_{4\times 2} (\mathbb{R}), Q^{T} Q=I_{2}} \operatorname{tr}\left(Q^{T} A Q\right)","This question already has answers here : Maximize $\mathrm{tr}(Q^TCQ)$ subject to $Q^TQ=I$ (5 answers) Closed 3 years ago . Let $$ A:=\left[\begin{array}{llll} 3 & 1 & 0 & 0 \\ 1 & 3 & 0 & 0 \\ 0 & 0 & 6 & 2 \\ 0 & 0 & 2 & 6 \end{array}\right] $$ Find $\sup _\limits{Q \in M_{4\times 2} (\mathbb{R}), Q^{T} Q=I_{2}} \operatorname{tr}\left(Q^{T} A Q\right)$ , where $M_{4 \times 2}(\mathbb{R})$ represents the set of all matrices of size $4\times 2$ . I know that $\mathrm{tr}A=\sum _i A_{ii}$ , but how can we deal with this upper bound? It is obvious that $Q^T AQ$ is a $2\times 2$ matrix, but I don't know how does the condition $Q^TQ=I_2$ help. Also, are there any backgrounds for this problem? I seldom see (linear algebra) problems asking the uppper bound for a trace and I hope I could get further information about these kind of problems (if possible).","This question already has answers here : Maximize $\mathrm{tr}(Q^TCQ)$ subject to $Q^TQ=I$ (5 answers) Closed 3 years ago . Let Find , where represents the set of all matrices of size . I know that , but how can we deal with this upper bound? It is obvious that is a matrix, but I don't know how does the condition help. Also, are there any backgrounds for this problem? I seldom see (linear algebra) problems asking the uppper bound for a trace and I hope I could get further information about these kind of problems (if possible).","
A:=\left[\begin{array}{llll}
3 & 1 & 0 & 0 \\
1 & 3 & 0 & 0 \\
0 & 0 & 6 & 2 \\
0 & 0 & 2 & 6
\end{array}\right]
 \sup _\limits{Q \in M_{4\times 2} (\mathbb{R}), Q^{T} Q=I_{2}} \operatorname{tr}\left(Q^{T} A Q\right) M_{4 \times 2}(\mathbb{R}) 4\times 2 \mathrm{tr}A=\sum _i A_{ii} Q^T AQ 2\times 2 Q^TQ=I_2","['linear-algebra', 'matrices', 'upper-lower-bounds', 'orthogonal-matrices']"
44,Prove that A is zero matrix,Prove that A is zero matrix,,"Let $A$ be an $n×n $ complex matrix such that the three matrices $A+I$ , $A^2+I $ , $ A^3+I$ are all  unitary .Prove that $ A$ is the zero matrix I try to show that $Trace( A^{\theta}A) =0$ where $A^{\theta }$ is conjugate transpose of matrix $A$ $\because $ $Trace( A^{\theta}A)$ = $|a_{11}|^2 + |a_{12}|^2....|a_{nn}|^2$ $A+I$ is unitary ,so $(A+I)^{\theta}(A+I)= I $ $\implies (A^ {\theta}+I)(A +I) =I $ $A^ {\theta}A+ A^ {\theta}+A = 0$ $ Trace( A^{\theta}A)= -( Trace( A^{\theta}+A))$ $ \implies  Trace( A^{\theta}A)=-2$ ( sum of real parts of each diagonal entry of A I don't know how to proceed further Please help","Let be an complex matrix such that the three matrices , , are all  unitary .Prove that is the zero matrix I try to show that where is conjugate transpose of matrix = is unitary ,so ( sum of real parts of each diagonal entry of A I don't know how to proceed further Please help",A n×n  A+I A^2+I   A^3+I  A Trace( A^{\theta}A) =0 A^{\theta } A \because  Trace( A^{\theta}A) |a_{11}|^2 + |a_{12}|^2....|a_{nn}|^2 A+I (A+I)^{\theta}(A+I)= I  \implies (A^ {\theta}+I)(A +I) =I  A^ {\theta}A+ A^ {\theta}+A = 0  Trace( A^{\theta}A)= -( Trace( A^{\theta}+A))  \implies  Trace( A^{\theta}A)=-2,"['linear-algebra', 'matrices', 'unitary-matrices']"
45,Conditions for this vectors to be linearly dependent,Conditions for this vectors to be linearly dependent,,"Consider $m<n$ positive definite matrices $A_1\dots,A_m\in\mathbb{R}^{n\times n}$ which are linearly independent matrices . This is, that there are no $c_1,\dots,c_n\in\mathbb{R}$ such that $\sum_{i=1}^m c_iA_i = 0$ unless $c_i=0, \forall i=1,\dots,m$ . How can I find vectors $x\in\mathbb{R}^n$ different from $0$ such that the set of vectors $$ A_1x,\dots,A_mx $$ become linearly dependent? This is, that there exists $d_1,\dots,d_n\in\mathbb{R}$ different from all $0$ ,such that $\sum_{i=1}^m d_iA_ix = 0$ . This question is motivated by an example as the following. Consider the matrices $$ A_i = \begin{bmatrix} M & 0_{3 \times 1}\\ 0_{1\times 3} & a_i \end{bmatrix} $$ where $a_i\in\mathbb{R}$ and $M\in\mathbb{R}^{3\times 3}$ , and $a_1,\dots,a_m$ are different from each other. Hence, if I take $$ x = \begin{bmatrix} x' \\ 0 \end{bmatrix} $$ with some $x'\in\mathbb{R}^3$ , then $A_ix = Mx', \forall i=1,\dots, m$ . Thus, in this case (which is something like a ""trivial example"") we have that the vectors $A_ix$ are linearly dependent.","Consider positive definite matrices which are linearly independent matrices . This is, that there are no such that unless . How can I find vectors different from such that the set of vectors become linearly dependent? This is, that there exists different from all ,such that . This question is motivated by an example as the following. Consider the matrices where and , and are different from each other. Hence, if I take with some , then . Thus, in this case (which is something like a ""trivial example"") we have that the vectors are linearly dependent.","m<n A_1\dots,A_m\in\mathbb{R}^{n\times n} c_1,\dots,c_n\in\mathbb{R} \sum_{i=1}^m c_iA_i = 0 c_i=0, \forall i=1,\dots,m x\in\mathbb{R}^n 0 
A_1x,\dots,A_mx
 d_1,\dots,d_n\in\mathbb{R} 0 \sum_{i=1}^m d_iA_ix = 0 
A_i = \begin{bmatrix}
M & 0_{3 \times 1}\\
0_{1\times 3} & a_i
\end{bmatrix}
 a_i\in\mathbb{R} M\in\mathbb{R}^{3\times 3} a_1,\dots,a_m 
x = \begin{bmatrix}
x' \\
0
\end{bmatrix}
 x'\in\mathbb{R}^3 A_ix = Mx', \forall i=1,\dots, m A_ix","['linear-algebra', 'matrices', 'vectors']"
46,Can a non-zero matrix have a zero characteristic polynomial?,Can a non-zero matrix have a zero characteristic polynomial?,,"Are there some field $\mathbb{F}$ , some $n \in \{1,2,\dots\}$ , and some non-zero $n \times n$ matrix $A$ over $\mathbb{F}$ , whose characteristic polynomial $p_A(t)$ is identically $0$ ? The same question was asked here in the past, and the answer explained that such a $p_A(t)$ was impossible, because a characteristic polynomial of an $n\times n$ matrix had degree $n$ . But this answer is unsatisfactory, because in some cases an identically zero polynomial has a positive degree: take for instance the polynomial $p(t) = t^5 + 4t$ in the field $\mathbb{Z}/5\mathbb{Z}$ of the integers modulo $5$ .","Are there some field , some , and some non-zero matrix over , whose characteristic polynomial is identically ? The same question was asked here in the past, and the answer explained that such a was impossible, because a characteristic polynomial of an matrix had degree . But this answer is unsatisfactory, because in some cases an identically zero polynomial has a positive degree: take for instance the polynomial in the field of the integers modulo .","\mathbb{F} n \in \{1,2,\dots\} n \times n A \mathbb{F} p_A(t) 0 p_A(t) n\times n n p(t) = t^5 + 4t \mathbb{Z}/5\mathbb{Z} 5","['linear-algebra', 'matrices', 'field-theory', 'characteristic-polynomial']"
47,Alternative rank test for controllability (purely linear-algebra problem),Alternative rank test for controllability (purely linear-algebra problem),,"Given $A \in \mathbb{R}^{n\times n}$ and $b\in\mathbb{R}^n$ , show that $$ \text{rank} \begin{bmatrix} (sI-A) \quad b \end{bmatrix} = n \quad \forall s \in \mathbb{C} \quad  \text{(the PBH criterion for controllability)} $$ if and only if the only $n\times n$ matrix $X$ such that $AX=XA$ and $Xb\equiv0$ is $X \equiv 0$ . I can only go as far as follows (for the $\Rightarrow$ direction): Suppose $q$ is a left eigenvector of $A$ , i.e., $qA=\lambda q\:$ for some $\lambda$ . From the PBH rank test, there is no left eigenvector of $A$ that is orthogonal to $b$ , i.e., $qb\equiv0$ . Now, given an $X$ such that $AX=XA$ and $Xb\equiv0$ . Left multiply them by $q$ gives $qAX=\lambda qX=qXA$ and $qXb\equiv0$ , which shows that $qX$ is also a left eigenvector of $A$ with the same eigenvalue and $(qX)b\equiv0$ . So by the PBH rank test, we must have $qX\equiv0$ . This holds for all left eigenvectors of $A$ . If $A$ is diagonalizable, then we can find a set of left eigenvectors that forms a basis for $\mathbb{R}^n$ , and thus $X\equiv0$ . However, A may not be diagonalizable.","Given and , show that if and only if the only matrix such that and is . I can only go as far as follows (for the direction): Suppose is a left eigenvector of , i.e., for some . From the PBH rank test, there is no left eigenvector of that is orthogonal to , i.e., . Now, given an such that and . Left multiply them by gives and , which shows that is also a left eigenvector of with the same eigenvalue and . So by the PBH rank test, we must have . This holds for all left eigenvectors of . If is diagonalizable, then we can find a set of left eigenvectors that forms a basis for , and thus . However, A may not be diagonalizable.","A \in \mathbb{R}^{n\times n} b\in\mathbb{R}^n  \text{rank} \begin{bmatrix} (sI-A) \quad b \end{bmatrix} = n \quad \forall s \in \mathbb{C}
\quad 
\text{(the PBH criterion for controllability)}
 n\times n X AX=XA Xb\equiv0 X \equiv 0 \Rightarrow q A qA=\lambda q\: \lambda A b qb\equiv0 X AX=XA Xb\equiv0 q qAX=\lambda qX=qXA qXb\equiv0 qX A (qX)b\equiv0 qX\equiv0 A A \mathbb{R}^n X\equiv0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-rank', 'linear-control']"
48,Can the eigenvalues of this block circulant matrix be found?,Can the eigenvalues of this block circulant matrix be found?,,"I have a matrix of the form $$ M = \begin{pmatrix} A & A^T & & & I\\ I & A & A^T & & \\ &  I & A  & \ddots &\\ & & \ddots & \ddots & A^T\\ A^T & & & I & A \end{pmatrix}$$ where $I$ is an $n \times n$ identity matrix and $A$ is an $n \times n$ -matrix given by $$ A = \begin{pmatrix} 0 & 1 & 0 & \dots & 0\\ \vdots & \ddots& \ddots & \ddots & \vdots\\ 0 & \dots & 0 & 1 & 0\\ 0 & \dots &  & 0 & 1\\ 0 & \dots & & & 0 \end{pmatrix}$$ that is a matrix which has ones on the super diagonal and zeros everywhere else. Is there some way to find the eigenvalues of this matrix? If there is, can it be generalized to a more complicated $A$ ? Since $A$ and $A^T$ don't commute, one cannot diagonalize them simultaneously (also, they are not even diagonalizable), otherwise that would have been a straightforward way to do it. I have tried computing the characteristic polynomial, but I cannot seem to find a way to simplify the determinant.","I have a matrix of the form where is an identity matrix and is an -matrix given by that is a matrix which has ones on the super diagonal and zeros everywhere else. Is there some way to find the eigenvalues of this matrix? If there is, can it be generalized to a more complicated ? Since and don't commute, one cannot diagonalize them simultaneously (also, they are not even diagonalizable), otherwise that would have been a straightforward way to do it. I have tried computing the characteristic polynomial, but I cannot seem to find a way to simplify the determinant.", M = \begin{pmatrix} A & A^T & & & I\\ I & A & A^T & & \\ &  I & A  & \ddots &\\ & & \ddots & \ddots & A^T\\ A^T & & & I & A \end{pmatrix} I n \times n A n \times n  A = \begin{pmatrix} 0 & 1 & 0 & \dots & 0\\ \vdots & \ddots& \ddots & \ddots & \vdots\\ 0 & \dots & 0 & 1 & 0\\ 0 & \dots &  & 0 & 1\\ 0 & \dots & & & 0 \end{pmatrix} A A A^T,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'circulant-matrices']"
49,Subalgebra of $M_n(\mathbb{C})$ generated by two elements (along with unity),Subalgebra of  generated by two elements (along with unity),M_n(\mathbb{C}),"Let $M_n(\mathbb{C})$ denote the algebra of $n\times n$ matrices over the field of complex numbers $\mathbb{C}$ . Let $h_1,h_2\in M_n(\mathbb{C})$ be two Hermitian matrices. Suppose that $h_1,h_2$ are ""relatively irreducible"", i.e. they don't have common invariant proper subspaces. What is the dimension of the subalgebra $\mathcal{A}$ generated by $e,h_1,h_2$ , where $e$ is the $n\times n$ identity matrix? Especially, what is the condition on $h_1,h_2$ such that $\mathcal{A}=M_n(\mathbb{C})$ ? [Note: it is easy to see that if $h_1,h_2$ can be simultaneously block diagonalized, then $e,h_1,h_2$ cannot generate $M_n(C)$ .] For example when $n=2$ , the Pauli matrices $\sigma^z,\sigma^x$ are enough to generate $M_2(\mathbb{C})$ . When $n=3$ , it is easy to check that the Gellmann matrices $$\lambda_1 = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix},~~\lambda_4 = \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix},$$ along with identity matrix $e$ could generate $M_3(\mathbb{C})$ . I'm curious about what can be said in general.","Let denote the algebra of matrices over the field of complex numbers . Let be two Hermitian matrices. Suppose that are ""relatively irreducible"", i.e. they don't have common invariant proper subspaces. What is the dimension of the subalgebra generated by , where is the identity matrix? Especially, what is the condition on such that ? [Note: it is easy to see that if can be simultaneously block diagonalized, then cannot generate .] For example when , the Pauli matrices are enough to generate . When , it is easy to check that the Gellmann matrices along with identity matrix could generate . I'm curious about what can be said in general.","M_n(\mathbb{C}) n\times n \mathbb{C} h_1,h_2\in M_n(\mathbb{C}) h_1,h_2 \mathcal{A} e,h_1,h_2 e n\times n h_1,h_2 \mathcal{A}=M_n(\mathbb{C}) h_1,h_2 e,h_1,h_2 M_n(C) n=2 \sigma^z,\sigma^x M_2(\mathbb{C}) n=3 \lambda_1 = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix},~~\lambda_4 = \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix}, e M_3(\mathbb{C})","['linear-algebra', 'abstract-algebra', 'matrices', 'noncommutative-algebra']"
50,Is there a name for the matrix operation $ABA^t$,Is there a name for the matrix operation,ABA^t,"I know in group theory, the operation $ABA^{-1}$ , i.e. the element A multiplied by the element B, and then multiplied by the inverse of A, is called conjugation. When dealing with matrices, there is a similar operation that happens frequently: $ABA^{t}$ , where instead of multiplying by the inverse of A, you multiply by the transpose of A. Does this operation or the resulting matrix have a name?","I know in group theory, the operation , i.e. the element A multiplied by the element B, and then multiplied by the inverse of A, is called conjugation. When dealing with matrices, there is a similar operation that happens frequently: , where instead of multiplying by the inverse of A, you multiply by the transpose of A. Does this operation or the resulting matrix have a name?",ABA^{-1} ABA^{t},"['linear-algebra', 'abstract-algebra', 'matrices']"
51,If $A$ and $B$ have non-negative eigen values then can we conclude that $A+B$ has non-negative eigen values?,If  and  have non-negative eigen values then can we conclude that  has non-negative eigen values?,A B A+B,Let $A$ and $B$ be $n\times n$ matrices whose all eigen values are non-negative real numbers. What can I say about the signs of the eigen values of $A+B$ ? First of all is it possible that the eigen values of $A+B$ may not be real? Of course if $A$ and $B$ are symmetric and have non-negative eigen values then it is easy to see that $A+B$ will also have non-negative eigen values (this is just equivalent to positive semidefiniteness). So I am only interested in the non-symmetric case. Thank you.,Let and be matrices whose all eigen values are non-negative real numbers. What can I say about the signs of the eigen values of ? First of all is it possible that the eigen values of may not be real? Of course if and are symmetric and have non-negative eigen values then it is easy to see that will also have non-negative eigen values (this is just equivalent to positive semidefiniteness). So I am only interested in the non-symmetric case. Thank you.,A B n\times n A+B A+B A B A+B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
52,$ Ax\cdot x>0$ and $Ay\cdot y>0$ implies $(Ax\cdot x)(Ay\cdot y)\geq (Ax\cdot y)^2$?,and  implies ?, Ax\cdot x>0 Ay\cdot y>0 (Ax\cdot x)(Ay\cdot y)\geq (Ax\cdot y)^2,"Let $A$ be a $n\times n$ symmetric real matrix. Assume that $x,y\in\mathbb{R}^n$ are such that $Ax\cdot x>0$ and $Ay\cdot y>0.$ Does this imply that $(Ax\cdot x)(Ay\cdot y)\geq (Ax\cdot y)^2$ ? The inequality clearly looks like a Cauchy-Schwarz (C-S) type inequality but applying C-S I couldn't arrive to the desired inequality. Besides, I am not even sure if it is even true. Does anyone have any thoughts?","Let be a symmetric real matrix. Assume that are such that and Does this imply that ? The inequality clearly looks like a Cauchy-Schwarz (C-S) type inequality but applying C-S I couldn't arrive to the desired inequality. Besides, I am not even sure if it is even true. Does anyone have any thoughts?","A n\times n x,y\in\mathbb{R}^n Ax\cdot x>0 Ay\cdot y>0. (Ax\cdot x)(Ay\cdot y)\geq (Ax\cdot y)^2","['linear-algebra', 'matrices', 'symmetric-matrices', 'cauchy-schwarz-inequality']"
53,Understanding the determinant of an infinite matrix,Understanding the determinant of an infinite matrix,,"The fredholm determinant of an infinite matrix $A$ is defined by $$ \det(I+A) = \exp(\text{tr}(\log(I+A))). $$ I'm trying to understand the formula and how to use it. To be concrete here are my questions: Let $$ A=I=\begin{pmatrix}1 & 0 & 0 & \ldots \\ 0 & 1 & 0 & \ddots  \\ \vdots & \ddots & \ddots &\ddots \\ \end{pmatrix} $$ ( $A$ is the infinite matrix with diagonal elements equal to 1 and 0's everywhere else). If $A$ were finite dimensional then mutlipyling the diagonal elements, the determinant equals 1. It seems natural that the infinite matrix should also have determinant equal to 1 but I don't see how the above formula gets this. What about a triangular matrix with diagonal elements  equal to 1? What about a diagonal matrix with elements not equal to 1? Why is the definition not $\det(A) = \exp(\text{tr}(\log(A)))$ ? Say $\det(I+A)=3$ then what is $\det(A)$ ?","The fredholm determinant of an infinite matrix is defined by I'm trying to understand the formula and how to use it. To be concrete here are my questions: Let ( is the infinite matrix with diagonal elements equal to 1 and 0's everywhere else). If were finite dimensional then mutlipyling the diagonal elements, the determinant equals 1. It seems natural that the infinite matrix should also have determinant equal to 1 but I don't see how the above formula gets this. What about a triangular matrix with diagonal elements  equal to 1? What about a diagonal matrix with elements not equal to 1? Why is the definition not ? Say then what is ?","A 
\det(I+A) = \exp(\text{tr}(\log(I+A))).
 
A=I=\begin{pmatrix}1 & 0 & 0 & \ldots \\
0 & 1 & 0 & \ddots  \\
\vdots & \ddots & \ddots &\ddots \\
\end{pmatrix}
 A A \det(A) = \exp(\text{tr}(\log(A))) \det(I+A)=3 \det(A)","['linear-algebra', 'matrices', 'functional-analysis', 'determinant']"
54,Find all matrices $A\in \mathbb{R}^{2\times2}$ such that $A^2=\bf{0}$,Find all matrices  such that,A\in \mathbb{R}^{2\times2} A^2=\bf{0},"Attempt: Let's consider $A =  \begin{bmatrix} a & b \\ c & d \end{bmatrix}$ . $$\begin{align} A^2 &=  \begin{bmatrix} a & b \\ c & d \end{bmatrix} \cdot \begin{bmatrix} a & b \\ c & d \end{bmatrix} \\&= \begin{bmatrix} a\cdot a+b\cdot c & a\cdot b + b\cdot d \\ c\cdot a + d\cdot c & c\cdot b + d\cdot d  \end{bmatrix}\\ &= \bf{0} \end{align}$$ This gives us the system of equations: $$\begin{align} a\cdot a+b\cdot c &= 0 \tag{1}\\ a\cdot b + b\cdot d  = b\cdot(a+d)&= 0 \tag{2}\\ c\cdot a + d\cdot c = c\cdot(a+d)&= 0 \tag{3}\\ c\cdot b + d\cdot d &= 0 \tag{4} \end{align}$$ Now from equation $(2)$ and $(3)$ , we have eight cases: $b = 0$ $c = 0$ $a+d = 0$ and 5 combinations of (1,2,3) which I won't bother listing. Case 1 ( $b=0$ ): $b=0$ implies $a = 0$ in equation $(1)$ and $d = 0$ in equation $(4)$ . This means that if $A = \begin{bmatrix}0&0\\c&0\end{bmatrix}$ then $A^2=\bf{0}$ . Case 2 ( $c=0$ ): From symmetry with $b$ , $c=0 \implies A=\begin{bmatrix}0&b\\0&0\end{bmatrix}$ . So, we only consider cases where $b\neq0$ and $c\neq 0 $ which leaves us only with case 3 ( $a+d=0$ ). Case 3 ( $a+d=0$ ): In equation (1), $a+d=0 \implies a\cdot d - b\cdot c = 0$ . So $A=\begin{bmatrix}a&b\\c&-a\end{bmatrix}$ is not invertible, $A^2 = \bf{0}$ . In summary, if $A$ has one of the following forms: $$\begin{bmatrix}0&b\\0&0\end{bmatrix}, \begin{bmatrix}0&0\\c&0\end{bmatrix}, \begin{bmatrix}a&b\\c&-a\end{bmatrix} \text{ (and not invertible) }$$ then $A^2=\bf{0}$ . Questions: Is this a correct proof? What is the standard proof? ""Strange question"": How can I know if there was only the 8 cases? As in, how do I know that only these 8 cases are relevant to $A^2 = \bf{0}$ ?","Attempt: Let's consider . This gives us the system of equations: Now from equation and , we have eight cases: and 5 combinations of (1,2,3) which I won't bother listing. Case 1 ( ): implies in equation and in equation . This means that if then . Case 2 ( ): From symmetry with , . So, we only consider cases where and which leaves us only with case 3 ( ). Case 3 ( ): In equation (1), . So is not invertible, . In summary, if has one of the following forms: then . Questions: Is this a correct proof? What is the standard proof? ""Strange question"": How can I know if there was only the 8 cases? As in, how do I know that only these 8 cases are relevant to ?","A = 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} \begin{align}
A^2 &= 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\cdot
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} \\&=
\begin{bmatrix}
a\cdot a+b\cdot c & a\cdot b + b\cdot d \\
c\cdot a + d\cdot c & c\cdot b + d\cdot d 
\end{bmatrix}\\ &= \bf{0}
\end{align} \begin{align}
a\cdot a+b\cdot c &= 0 \tag{1}\\
a\cdot b + b\cdot d  = b\cdot(a+d)&= 0 \tag{2}\\
c\cdot a + d\cdot c = c\cdot(a+d)&= 0 \tag{3}\\
c\cdot b + d\cdot d &= 0 \tag{4}
\end{align} (2) (3) b = 0 c = 0 a+d = 0 b=0 b=0 a = 0 (1) d = 0 (4) A = \begin{bmatrix}0&0\\c&0\end{bmatrix} A^2=\bf{0} c=0 b c=0 \implies A=\begin{bmatrix}0&b\\0&0\end{bmatrix} b\neq0 c\neq 0  a+d=0 a+d=0 a+d=0 \implies a\cdot d - b\cdot c = 0 A=\begin{bmatrix}a&b\\c&-a\end{bmatrix} A^2 = \bf{0} A \begin{bmatrix}0&b\\0&0\end{bmatrix},
\begin{bmatrix}0&0\\c&0\end{bmatrix},
\begin{bmatrix}a&b\\c&-a\end{bmatrix} \text{ (and not invertible) } A^2=\bf{0} A^2 = \bf{0}","['linear-algebra', 'matrices', 'proof-verification', 'matrix-equations', 'nilpotence']"
55,Quadratic matrix equation $XAX=B$ [duplicate],Quadratic matrix equation  [duplicate],XAX=B,This question already has an answer here : Matrix equation with symmetric and positive definite matrices [duplicate] (1 answer) Closed last year . Let $A$ and $B$ be two positive semidefinite $n \times n$ matrices. Does the following quadratic matrix equation have a solution in the set of real symmetric matrices? $$XAX=B$$ It's a special case of the Riccati equation. I want just to prove the existence of such real matrix $X$ .,This question already has an answer here : Matrix equation with symmetric and positive definite matrices [duplicate] (1 answer) Closed last year . Let and be two positive semidefinite matrices. Does the following quadratic matrix equation have a solution in the set of real symmetric matrices? It's a special case of the Riccati equation. I want just to prove the existence of such real matrix .,A B n \times n XAX=B X,"['matrices', 'quadratics', 'matrix-equations', 'matrix-calculus', 'symmetric-matrices']"
56,"If $\det(A-\lambda I)=\det(A^{-1}-\lambda I)$, then characteristic polynomial is coefficient symmetry","If , then characteristic polynomial is coefficient symmetry",\det(A-\lambda I)=\det(A^{-1}-\lambda I),"In one class, professor mentioned the following without proof, Suppose $A\in \mathbb{R}^{2N\times 2N}$ . $\det(A-\lambda I)=\det(A^{-1}-\lambda I)=p(\lambda)$ then if $$p(\lambda) = a_{2N}\lambda^{2N}+a_{2N-1}\lambda^{2N-1}+ \cdots +a_1\lambda^1 +a_0,$$ we have $a_{2N}=a_0, a_{2N-1}=a_1, \cdots.$ I refer to the following discussion Show that $\det(A-\lambda I)=\det(B-\lambda I)$ However, I still have no idea how to see it. Is there any good method without expanding the "" $\det$ "" to show it? Thanks in advanced.","In one class, professor mentioned the following without proof, Suppose . then if we have I refer to the following discussion Show that $\det(A-\lambda I)=\det(B-\lambda I)$ However, I still have no idea how to see it. Is there any good method without expanding the "" "" to show it? Thanks in advanced.","A\in \mathbb{R}^{2N\times 2N} \det(A-\lambda I)=\det(A^{-1}-\lambda I)=p(\lambda) p(\lambda) = a_{2N}\lambda^{2N}+a_{2N-1}\lambda^{2N-1}+ \cdots +a_1\lambda^1 +a_0, a_{2N}=a_0, a_{2N-1}=a_1, \cdots. \det","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
57,"If $A\in\Bbb R^{3\times3}$ is an invertible matrix such that $A^2=A$, then $\det(2A)=8$","If  is an invertible matrix such that , then",A\in\Bbb R^{3\times3} A^2=A \det(2A)=8,"True or false? ""If $\pmb{A\in\Bbb R^{3\times3}}$ is an invertible matrix such that $\pmb{A^2=A}$ , then $\pmb{\det(2A)=8}$ "". True. Proof. We start from $\det(2A)$ . Then, $2^3\det(A)$ . But we know that $A^2=AA=A$ , so $A=AA^{-1}$ because $A$ is an invertible matrix, hence $$8\det(A)=8\det(AA^{-1})=8\det(A)\det(A^{-1})=8\det(A)(\det(A))^{-1}=8\det(A)\frac{1}{\det(A)}=8,$$ which is where we wanted to go. $\square$ Is it correct? Thanks!!","True or false? ""If is an invertible matrix such that , then "". True. Proof. We start from . Then, . But we know that , so because is an invertible matrix, hence which is where we wanted to go. Is it correct? Thanks!!","\pmb{A\in\Bbb R^{3\times3}} \pmb{A^2=A} \pmb{\det(2A)=8} \det(2A) 2^3\det(A) A^2=AA=A A=AA^{-1} A 8\det(A)=8\det(AA^{-1})=8\det(A)\det(A^{-1})=8\det(A)(\det(A))^{-1}=8\det(A)\frac{1}{\det(A)}=8, \square","['matrices', 'proof-verification', 'determinant']"
58,How to differentiate a matrix equation w.r.t a vector?,How to differentiate a matrix equation w.r.t a vector?,,"I have a matrix equation that yields a scalar $$f(M) = MAM^T - 2 \sum_i^{N} \log(M_i)$$ Where $M$ is a $1 \times N$ row vector, and $A$ is an $N \times N$ matrix. As such, the result $f$ is a scalar. How does one take the derivative of $f$ w.r.t $M$ ? I have seen matrix cookbook define derivatives of matrices w.r.t specific index values, but I couldn't find a definition of differentiation w.r.t vectors. My intuition is something like $$\frac{\partial f}{\partial M} = 2(AM - M^{\circ -1})$$ with my reasoning being the two $M$ 's in the first term yield $2AM$ once differentiated, and the log term yields $M$ where each element is raised to the $-1$ power ( $\frac{d \log x}{dx} = x^{-1}$ ). Again, I'm not sure if I've done this correctly, and the fact that $f$ is a scalar makes it a bit more confusing.","I have a matrix equation that yields a scalar Where is a row vector, and is an matrix. As such, the result is a scalar. How does one take the derivative of w.r.t ? I have seen matrix cookbook define derivatives of matrices w.r.t specific index values, but I couldn't find a definition of differentiation w.r.t vectors. My intuition is something like with my reasoning being the two 's in the first term yield once differentiated, and the log term yields where each element is raised to the power ( ). Again, I'm not sure if I've done this correctly, and the fact that is a scalar makes it a bit more confusing.",f(M) = MAM^T - 2 \sum_i^{N} \log(M_i) M 1 \times N A N \times N f f M \frac{\partial f}{\partial M} = 2(AM - M^{\circ -1}) M 2AM M -1 \frac{d \log x}{dx} = x^{-1} f,"['calculus', 'linear-algebra', 'matrices', 'derivatives']"
59,Proper way to find change-of-basis matrix,Proper way to find change-of-basis matrix,,"From S.L Linear Algebra: In each one of the following cases, find $M^{\beta }_{\beta' }(id)$ . The vector space in each case is $\mathbb{R}^3$ . (a) $\beta = \{(1, 1, 0), (-1, 1, 1), (0, 1, 2)\}$ ; $\beta' = \{(2, 1, 1), (0, 0, 1), (-1, 1, 1)\}$ ... Definitions: $\beta$ and $\beta'$ : $\beta$ and $\beta'$ for some linear transformation $F$ , define the basis of vector space domain of $F$ and basis of vector space codomain of $F$ . In other words, for linear transformation: $F: V \rightarrow W$ $\beta$ implies a basis of $V$ and $\beta'$ implies a basis of $W$ . $M^{\beta }_{\beta' }(id)$ : Generally, $M^{\beta }_{\beta' }(F)$ for some linear transformation in the book is defined by a unique matrix $A$ having following the property: If $X$ is the (column) coordinate vector of an element $v$ of $V$ , relative to the basis $\beta$ , then $AX$ is the (column) coordinate vector of $F(v)$ , relative to the basis $\beta'$ . I'm not sure about the exact definition of $M^{\beta }_{\beta' }(id)$ , but generally book refers to $id$ as an identity map, hence I'm assuming that matrix $M^{\beta }_{\beta' }(id)$ is matrix associated with some identity map. Solution: There's a very interesting result in book: $$X_{\beta{}'}(v) = M^{\beta }_{\beta' }(id) X_{\beta{}}(v)$$ ( Equation 1 ) Note that $X_{\beta{}}(v)$ implies that the coordinate vector $X$ depends on $v$ and basis $\beta $ . Hence, $v= X \beta{}$ where $v \in V$ and $w = X \beta{}'$ where $w \in W$ , assuming that $Id: V \rightarrow W$ . But considering that identity map is both surjective (it's image is equal to codomain) and injective (trivial kernel), I assume we have $V = W$ and hence $Id: V \rightarrow V$ . According to our equation 1 and basis information, I can simply plug variables: $$\left( x_1(2, 1, 1), x_2(0, 0, 1), x_3(-1, 1, 1) \right)= \left(A_1x_1(1, 1, 0), A_2x_2(-1, 1, 1),  A_3x_3(0, 1, 2) \right)$$ (where $A = M^{\beta }_{\beta' }(id)$ and $A_1, A_2, A_3$ represent columns of $A$ , considering that it is a $3 \times 3$ matrix). Assuming that $x_1, x_2, x_3$ are scalars, we have: $$\left( (2x_1, x_1, x_1), (0, 0, x_2), (-x_3, x_3, x_3) \right)= \left(A_1(x_1, x_1, 0), A_2(-x_2, x_2, x_2),  A_3(0, x_3, x_3) \right)$$ This is where it gets little confusing, if I try to isolate column vectors of $A$ in this manner: $$\left( (2x_1 - x_1, x_1 - x_1, x_1 - 0), (0 + x_2, 0 - x_2, x_2 - x_2), (-x_3 - 0, x_3 - x_3, x_3 - x_3) \right)= (A_1, A_2, A_3)$$ Would it be a fundamental error? If not then we would have: $$\begin{pmatrix} x_1  & x_2   & -x_3 \\  0 & -x_2  & 0 \\  0 & 0  & 0 \end{pmatrix}$$ Which doesn't seem like a proper solution. What mistake did I make? Is there a better solution to this problem? I feel like I'm making a simple fundamental mistake.","From S.L Linear Algebra: In each one of the following cases, find . The vector space in each case is . (a) ; ... Definitions: and : and for some linear transformation , define the basis of vector space domain of and basis of vector space codomain of . In other words, for linear transformation: implies a basis of and implies a basis of . : Generally, for some linear transformation in the book is defined by a unique matrix having following the property: If is the (column) coordinate vector of an element of , relative to the basis , then is the (column) coordinate vector of , relative to the basis . I'm not sure about the exact definition of , but generally book refers to as an identity map, hence I'm assuming that matrix is matrix associated with some identity map. Solution: There's a very interesting result in book: ( Equation 1 ) Note that implies that the coordinate vector depends on and basis . Hence, where and where , assuming that . But considering that identity map is both surjective (it's image is equal to codomain) and injective (trivial kernel), I assume we have and hence . According to our equation 1 and basis information, I can simply plug variables: (where and represent columns of , considering that it is a matrix). Assuming that are scalars, we have: This is where it gets little confusing, if I try to isolate column vectors of in this manner: Would it be a fundamental error? If not then we would have: Which doesn't seem like a proper solution. What mistake did I make? Is there a better solution to this problem? I feel like I'm making a simple fundamental mistake.","M^{\beta }_{\beta' }(id) \mathbb{R}^3 \beta = \{(1, 1, 0), (-1, 1, 1), (0, 1, 2)\} \beta' = \{(2, 1, 1), (0, 0, 1), (-1, 1, 1)\} \beta \beta' \beta \beta' F F F F: V \rightarrow W \beta V \beta' W M^{\beta }_{\beta' }(id) M^{\beta }_{\beta' }(F) A X v V \beta AX F(v) \beta' M^{\beta }_{\beta' }(id) id M^{\beta }_{\beta' }(id) X_{\beta{}'}(v) = M^{\beta }_{\beta' }(id) X_{\beta{}}(v) X_{\beta{}}(v) X v \beta  v= X \beta{} v \in V w = X \beta{}' w \in W Id: V \rightarrow W V = W Id: V \rightarrow V \left( x_1(2, 1, 1), x_2(0, 0, 1), x_3(-1, 1, 1) \right)=
\left(A_1x_1(1, 1, 0), A_2x_2(-1, 1, 1),  A_3x_3(0, 1, 2) \right) A = M^{\beta }_{\beta' }(id) A_1, A_2, A_3 A 3 \times 3 x_1, x_2, x_3 \left( (2x_1, x_1, x_1), (0, 0, x_2), (-x_3, x_3, x_3) \right)=
\left(A_1(x_1, x_1, 0), A_2(-x_2, x_2, x_2),  A_3(0, x_3, x_3) \right) A \left( (2x_1 - x_1, x_1 - x_1, x_1 - 0), (0 + x_2, 0 - x_2, x_2 - x_2), (-x_3 - 0, x_3 - x_3, x_3 - x_3) \right)= (A_1, A_2, A_3) \begin{pmatrix}
x_1  & x_2   & -x_3 \\ 
0 & -x_2  & 0 \\ 
0 & 0  & 0
\end{pmatrix}","['linear-algebra', 'matrices', 'linear-transformations']"
60,A $5\times 5$ Matrix with No Eigenvalues.,A  Matrix with No Eigenvalues.,5\times 5,"It can be observed that a matrix of order $5$ over $\mathbb{R}$ has at least one eigenvalue in $\mathbb{R}$ . What if we consider a finite field? For example, over $\mathbb{Z}_2$ , a matrix having characteristic polynomial $x^4(x-1)+1$ cannot have an eigenvalue from $\mathbb{Z}_2$ . Can such a matrix exist?","It can be observed that a matrix of order over has at least one eigenvalue in . What if we consider a finite field? For example, over , a matrix having characteristic polynomial cannot have an eigenvalue from . Can such a matrix exist?",5 \mathbb{R} \mathbb{R} \mathbb{Z}_2 x^4(x-1)+1 \mathbb{Z}_2,"['linear-algebra', 'abstract-algebra', 'matrices', 'eigenvalues-eigenvectors', 'finite-fields']"
61,Determinant of a $3\times 3$ matrix in simplest form.,Determinant of a  matrix in simplest form.,3\times 3,"Let $\alpha$ and $\beta $ be fixed non-zero reals and $f(n)=\alpha^n+\beta^n$ with $$A=\begin{pmatrix} 3&1+f(1)&1+f(2)\\1+f(1)&1+f(2)&1+f(3)\\ 1+f(2)&1+f(3)&1+f(4) \end{pmatrix}.$$ How to find determinant of the matrix $A$ i.e. $|A|,$ in the simplest form? Firstly I thought that matrix may be of a particular form. But its neither circulant matrix nor Vandermonde type or tridiagonal type. Then I tried multilinear property on columns of given matrix as below: $$\det\begin{pmatrix} 3&1+f(1)&1+f(2)\\1+f(1)&1+f(2)&1+f(3)\\ 1+f(2)&1+f(3)&1+f(4) \end{pmatrix}=\det\begin{pmatrix} 3&1+f(1)&1\\1+f(1)&1+f(2)&1\\ 1+f(2)&1+f(3)&1 \end{pmatrix} +\det\begin{pmatrix} 3&1+f(1)&f(2)\\1+f(1)&1+f(2)&f(3)\\ 1+f(2)&1+f(3)&f(4) \end{pmatrix} $$ I further apply multilinear property on columns but didn't get my answer. Please help. Thanks.",Let and be fixed non-zero reals and with How to find determinant of the matrix i.e. in the simplest form? Firstly I thought that matrix may be of a particular form. But its neither circulant matrix nor Vandermonde type or tridiagonal type. Then I tried multilinear property on columns of given matrix as below: I further apply multilinear property on columns but didn't get my answer. Please help. Thanks.,"\alpha \beta  f(n)=\alpha^n+\beta^n A=\begin{pmatrix}
3&1+f(1)&1+f(2)\\1+f(1)&1+f(2)&1+f(3)\\ 1+f(2)&1+f(3)&1+f(4)
\end{pmatrix}. A |A|, \det\begin{pmatrix}
3&1+f(1)&1+f(2)\\1+f(1)&1+f(2)&1+f(3)\\ 1+f(2)&1+f(3)&1+f(4)
\end{pmatrix}=\det\begin{pmatrix}
3&1+f(1)&1\\1+f(1)&1+f(2)&1\\ 1+f(2)&1+f(3)&1
\end{pmatrix} +\det\begin{pmatrix}
3&1+f(1)&f(2)\\1+f(1)&1+f(2)&f(3)\\ 1+f(2)&1+f(3)&f(4)
\end{pmatrix} ","['matrices', 'determinant']"
62,Inverse of strictly diagonally dominant matrix,Inverse of strictly diagonally dominant matrix,,I have a matrix whose diagonal entries are positive whereas non-diagonal entries are negative.This matrix is also Strictly diagonally dominant. Can we say that all elements of the inverse of this matrix is strictly positive i.e $a_{ij}$ >0 .,I have a matrix whose diagonal entries are positive whereas non-diagonal entries are negative.This matrix is also Strictly diagonally dominant. Can we say that all elements of the inverse of this matrix is strictly positive i.e >0 .,a_{ij},"['linear-algebra', 'matrices']"
63,Absolute of all eigenvalues are always bounded by maximal singular value,Absolute of all eigenvalues are always bounded by maximal singular value,,"There are many discussions about the singular values and eigenvalues, such as What is the difference between Singular Value and Eigenvalue? . I want to ask the particular one in title. Usually, for a general square matrix, singular valure are not equal to eigenvalues. But singular values are alwyas nonnegative. My claim is $A\in \mathbb{R}^{n\times n}$ . $|\lambda_i(A)|\leq \sigma_\max(A)$ , for all $i$ . The reason is the definition of the maximal singular value of $A$ , which is $$\sigma_\max(A) = \|A\|_2 = \max_{\|x\|=1} \|Ax\|.$$ It reflects the maximal gain of $A$ . And this $x$ does not have to be the eigenvector of $A$ . However, the eigenvalue of $A$ is $$Av = \lambda v, \ \ \ \ \|v\|=1.$$ To get $\lambda_\max$ , we have to have the extra constraint $\|Av\|  = \|\lambda_\max v\|$ . So $|\lambda(A)|\leq \sigma_\max (A)$ . Here $A$ is any real matrix not necessarily symmetric.  I am not sure if I am correct.","There are many discussions about the singular values and eigenvalues, such as What is the difference between Singular Value and Eigenvalue? . I want to ask the particular one in title. Usually, for a general square matrix, singular valure are not equal to eigenvalues. But singular values are alwyas nonnegative. My claim is . , for all . The reason is the definition of the maximal singular value of , which is It reflects the maximal gain of . And this does not have to be the eigenvector of . However, the eigenvalue of is To get , we have to have the extra constraint . So . Here is any real matrix not necessarily symmetric.  I am not sure if I am correct.","A\in \mathbb{R}^{n\times n} |\lambda_i(A)|\leq \sigma_\max(A) i A \sigma_\max(A) = \|A\|_2 = \max_{\|x\|=1} \|Ax\|. A x A A Av = \lambda v, \ \ \ \ \|v\|=1. \lambda_\max \|Av\| 
= \|\lambda_\max v\| |\lambda(A)|\leq \sigma_\max (A) A","['matrices', 'eigenvalues-eigenvectors', 'singular-values']"
64,Frobenius and operator $2$-norm,Frobenius and operator -norm,2,"I have been studying about norms and for a given matrix $A$ , I haven't been able to understand the difference between Frobenius norm $\|A\|_F$ and operator $2$ -norm $\|A\|_2$ . Can someone help me understand the difference between them?","I have been studying about norms and for a given matrix , I haven't been able to understand the difference between Frobenius norm and operator -norm . Can someone help me understand the difference between them?",A \|A\|_F 2 \|A\|_2,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms', 'spectral-norm']"
65,"Intuition: If the columns of a matrix are colinear, then its rows are also colinear.","Intuition: If the columns of a matrix are colinear, then its rows are also colinear.",,"For simplicity's sake, I'm working with a 3x3 square matrix in which none of the column or row vectors is the zero vector. I tried graphing the columns of the matrix {{1,4,-3},{2,7,-5},{3,6,-3}} (where each group of $3$ is a row) and those $3$ vectors are on the same plane. I understand why that's the case; what I don't understand is why it necessarily implies that the $3$ row vectors are also on a single plane. I have some algebraic intuition for the 2d case. For example with vectors v = <1,2> and w = <3,6>, the fact that w = 3v means that the respective components of the two vectors are in the same proportion, and therefore the vectors x = <1,3> and y = <2,6> are also colinear. I tried to extend that kind of logic to the 3d case but I didn't manage to because of the increased complexity. For ex in the 3x3 matrix that I used, if column vectors are u, v and w, then u = v + w. Since one vector isn't simply a scalar multiple of another as in the 2d case, I can't seem to apply the same logic of proportions kept. I saw this question: For a square matrix, row vectors are linearly independent if and only if columns are. , but it doesn't give the kind of intuition I'm looking for.","For simplicity's sake, I'm working with a 3x3 square matrix in which none of the column or row vectors is the zero vector. I tried graphing the columns of the matrix {{1,4,-3},{2,7,-5},{3,6,-3}} (where each group of is a row) and those vectors are on the same plane. I understand why that's the case; what I don't understand is why it necessarily implies that the row vectors are also on a single plane. I have some algebraic intuition for the 2d case. For example with vectors v = <1,2> and w = <3,6>, the fact that w = 3v means that the respective components of the two vectors are in the same proportion, and therefore the vectors x = <1,3> and y = <2,6> are also colinear. I tried to extend that kind of logic to the 3d case but I didn't manage to because of the increased complexity. For ex in the 3x3 matrix that I used, if column vectors are u, v and w, then u = v + w. Since one vector isn't simply a scalar multiple of another as in the 2d case, I can't seem to apply the same logic of proportions kept. I saw this question: For a square matrix, row vectors are linearly independent if and only if columns are. , but it doesn't give the kind of intuition I'm looking for.",3 3 3,"['linear-algebra', 'matrices', 'vectors']"
66,Transforming a matrix to diagonal matrix,Transforming a matrix to diagonal matrix,,"Show that the matrix $$A = \begin{bmatrix}a&h\\h&b\end{bmatrix} ,\quad a \ne b$$ is transformed to diagonal matrix $D = P^{-1}AP$ , where $$P = \begin{bmatrix}\cos x& -\sin x\\\sin x& \cos x\end{bmatrix}$$ and $$\tan2x=\frac{2h}{(a-b)}$$ I understand that a $n \times n$ matrix $A$ is diagonalizable if there is a diagonal matrix $D$ such that $A$ is similar to $D$ , that is, if there is an invertible matrix $P$ such that $P^{-1}AP= D$ . Also,  columns of $P$ are $n $ linearly independent eigenvectors of $A$ and the diagonal entries of $D$ are the eigenvalues of $A$ corresponding to the eigenvectors in $P$ in the same order. I am unable to proceed solving the above problem with these leads.","Show that the matrix is transformed to diagonal matrix , where and I understand that a matrix is diagonalizable if there is a diagonal matrix such that is similar to , that is, if there is an invertible matrix such that . Also,  columns of are linearly independent eigenvectors of and the diagonal entries of are the eigenvalues of corresponding to the eigenvectors in in the same order. I am unable to proceed solving the above problem with these leads.","A = \begin{bmatrix}a&h\\h&b\end{bmatrix} ,\quad a \ne b D = P^{-1}AP P = \begin{bmatrix}\cos x& -\sin x\\\sin x& \cos x\end{bmatrix} \tan2x=\frac{2h}{(a-b)} n \times n A D A D P P^{-1}AP= D P n  A D A P","['linear-algebra', 'matrices']"
67,"Centre of the special linear group $SL_2(\mathbb R)$ or $SL(2,\mathbb R)$",Centre of the special linear group  or,"SL_2(\mathbb R) SL(2,\mathbb R)","Algebra by Michael Artin Def 2.5.12 Obviously $I$ and $-I$ are in the centre: $AI=IA,A(-I)=(-I)A$. How exactly do I go about doing this? I was thinking to solve for $j,f,g,h$ below $$\begin{bmatrix} a & b\\  c & d \end{bmatrix} \begin{bmatrix} j & f\\  g & h \end{bmatrix} = \begin{bmatrix} j & f\\  g & h \end{bmatrix} \begin{bmatrix} a & b\\  c & d \end{bmatrix}, ad-bc=1=jh-fg.$$ So, I plug in b*g=c*f, a*f+b*h=b*j+d*f, c*j+d*g=a*g+c*h, j*h-f*g=1, a*d-b*c=1 in Wolfram Alpha or here or here (or here ) to get a bunch of complicated solutions sets, some of which include the desired $\pm I$. Ugh, where can I find a proof online? Or if this is still folklore , how do I begin? Do I for example take cases $c=0, c \ne 0$ and then solve for the centre in each case('s subcases)? Another thing I thought was to suppose on the contrary that $f \ne 0$ and then arrive at a contradiction and then assume $f=0$ when supposing on the contrary that $g \ne 0$ and then assuming $f=g=0$ when supposing on the contrary that $j \ne \pm 1$ and then assuming $f=g=0, j = \pm 1$ when supposing on the contrary that $h \ne \pm 1$. Not looking for a full solution, just a little nudge in the right direction. I've been lost in subcases of subcases of cases (like here ) that I think I'm missing something elegant or simple. I guess I've been doing maths for quite awhile that I've forgotten how to do arithmetic . To clarify, I am looking for a way to do this by systems or at least nothing high level like using facts like these About the Center of the Special Linear Group $SL(n,F)$ Note that this chapter is on homomorphisms. The reader just finished cyclic groups. The reader didn't reach Lagrange's Theorem, fields, rings or even isomorphisms.","Algebra by Michael Artin Def 2.5.12 Obviously $I$ and $-I$ are in the centre: $AI=IA,A(-I)=(-I)A$. How exactly do I go about doing this? I was thinking to solve for $j,f,g,h$ below $$\begin{bmatrix} a & b\\  c & d \end{bmatrix} \begin{bmatrix} j & f\\  g & h \end{bmatrix} = \begin{bmatrix} j & f\\  g & h \end{bmatrix} \begin{bmatrix} a & b\\  c & d \end{bmatrix}, ad-bc=1=jh-fg.$$ So, I plug in b*g=c*f, a*f+b*h=b*j+d*f, c*j+d*g=a*g+c*h, j*h-f*g=1, a*d-b*c=1 in Wolfram Alpha or here or here (or here ) to get a bunch of complicated solutions sets, some of which include the desired $\pm I$. Ugh, where can I find a proof online? Or if this is still folklore , how do I begin? Do I for example take cases $c=0, c \ne 0$ and then solve for the centre in each case('s subcases)? Another thing I thought was to suppose on the contrary that $f \ne 0$ and then arrive at a contradiction and then assume $f=0$ when supposing on the contrary that $g \ne 0$ and then assuming $f=g=0$ when supposing on the contrary that $j \ne \pm 1$ and then assuming $f=g=0, j = \pm 1$ when supposing on the contrary that $h \ne \pm 1$. Not looking for a full solution, just a little nudge in the right direction. I've been lost in subcases of subcases of cases (like here ) that I think I'm missing something elegant or simple. I guess I've been doing maths for quite awhile that I've forgotten how to do arithmetic . To clarify, I am looking for a way to do this by systems or at least nothing high level like using facts like these About the Center of the Special Linear Group $SL(n,F)$ Note that this chapter is on homomorphisms. The reader just finished cyclic groups. The reader didn't reach Lagrange's Theorem, fields, rings or even isomorphisms.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'linear-groups']"
68,"Find two $2\times2$ real matrix $A$ and $B$ such that $A$, $B$ , $A+B$ are all invertible with $(A+B)^{-1}=A^{-1}+B^{-1}$","Find two  real matrix  and  such that ,  ,  are all invertible with",2\times2 A B A B A+B (A+B)^{-1}=A^{-1}+B^{-1},"Find two $2\times2$ real matrices $A$ and $B$ such that $A$, $B$ , $A+B$ are all invertible with $(A+B)^{-1}=A^{-1}+B^{-1}$ Tried to write the matrices as $$A=\pmatrix {a&b\\c&d},B=\pmatrix {e&f\\g&h}$$ and solve $(A+B)^{-1}=A^{-1}+B^{-1}$, But make it too complex. Any more convenient ways?","Find two $2\times2$ real matrices $A$ and $B$ such that $A$, $B$ , $A+B$ are all invertible with $(A+B)^{-1}=A^{-1}+B^{-1}$ Tried to write the matrices as $$A=\pmatrix {a&b\\c&d},B=\pmatrix {e&f\\g&h}$$ and solve $(A+B)^{-1}=A^{-1}+B^{-1}$, But make it too complex. Any more convenient ways?",,"['linear-algebra', 'matrices']"
69,Decomposing Square Matrix Into Two Matrices Which Are Transposes of Each Other,Decomposing Square Matrix Into Two Matrices Which Are Transposes of Each Other,,"Let $X$ be a real-valued square $n \times n$ matrix. Is there decomposition $X = \Lambda \Lambda^\top$ where $\Lambda$ is a a real-valued $n \times k$ matrix, that always exists?","Let $X$ be a real-valued square $n \times n$ matrix. Is there decomposition $X = \Lambda \Lambda^\top$ where $\Lambda$ is a a real-valued $n \times k$ matrix, that always exists?",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
70,Category of vector spaces and matrices,Category of vector spaces and matrices,,"I was brushing up on linear algebra and the following came to the mind. Consider a category whose nodes are n-dimensional vector spaces (n>0). Morphisms are matrices (transformations between vector spaces). This construction seems to be forming a category: let identities to be identity matrices. let composition to be composition of matrices. Composition is associative and identity rules of category are satisfied, so we have a category. Questions: Any reference studied this category? (any name associated with this category at all in the literature?) Is matrix addition associated with any categorical construction in this category? What do product, co-product, terminal, and initial objects mean in this category?","I was brushing up on linear algebra and the following came to the mind. Consider a category whose nodes are n-dimensional vector spaces (n>0). Morphisms are matrices (transformations between vector spaces). This construction seems to be forming a category: let identities to be identity matrices. let composition to be composition of matrices. Composition is associative and identity rules of category are satisfied, so we have a category. Questions: Any reference studied this category? (any name associated with this category at all in the literature?) Is matrix addition associated with any categorical construction in this category? What do product, co-product, terminal, and initial objects mean in this category?",,"['linear-algebra', 'matrices', 'category-theory']"
71,"For a matrix with all integer called entries, any integer eigenvalue will divide the determinant.","For a matrix with all integer called entries, any integer eigenvalue will divide the determinant.",,"I have to show that for a matrix with all integer valued entries, any integer eigenvalue will divide the determinant. I know that the product of the eigenvalues is the determinant, but there can be non integral eigenvalues too. Also I know the sum of the eigenvalues, which is the trace of the matrix, is also integral. Any hint will be good. Thank you.","I have to show that for a matrix with all integer valued entries, any integer eigenvalue will divide the determinant. I know that the product of the eigenvalues is the determinant, but there can be non integral eigenvalues too. Also I know the sum of the eigenvalues, which is the trace of the matrix, is also integral. Any hint will be good. Thank you.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
72,Show that the spectral radius satisfies $\rho(AB)\le \rho(A)\rho(B)$ for commuting matrices,Show that the spectral radius satisfies  for commuting matrices,\rho(AB)\le \rho(A)\rho(B),"For any square matrix $C$ with real entries, denote by $\rho(C)$ its spectral radius, i.e. the maximum magnitude of its eigenvalues. For symmetric matrices $A$ and $B$ with $AB=BA$ show that $$\rho(AB)\le \rho(A)\rho(B)$$ I think simultaneous diagonalization of $A$ and $B$ is to be used here, but couldn't find my way out. Also will the proposition hold if the condition of symmetry is dropped?","For any square matrix $C$ with real entries, denote by $\rho(C)$ its spectral radius, i.e. the maximum magnitude of its eigenvalues. For symmetric matrices $A$ and $B$ with $AB=BA$ show that $$\rho(AB)\le \rho(A)\rho(B)$$ I think simultaneous diagonalization of $A$ and $B$ is to be used here, but couldn't find my way out. Also will the proposition hold if the condition of symmetry is dropped?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
73,"For rotation matrix $A$, find $B = A^4- A^3 + A^2 - A$","For rotation matrix , find",A B = A^4- A^3 + A^2 - A,"Find the value of $B = A^4- A^3 + A^2 - A$ where $A$ is the matrix given below $$ A=     \left [ \begin{matrix}     \cos\alpha & \sin \alpha  \\     -\sin\alpha & \cos\alpha      \end{matrix} \right ] $$ It's actually quite simple when you look at it. Find $A$, $A^2$, $A^3$, $A^4$ by doing some matrix multiplication and then add and subtract following the question. I did all of the tedious work mentioned above. But the matrix $B$ I obtain is filled with garbage. I assuming either we must use some trigno mumbo jumbo identities, or there's something beautiful that I missed from the question. Edit: Could the sum of a geometric series help?","Find the value of $B = A^4- A^3 + A^2 - A$ where $A$ is the matrix given below $$ A=     \left [ \begin{matrix}     \cos\alpha & \sin \alpha  \\     -\sin\alpha & \cos\alpha      \end{matrix} \right ] $$ It's actually quite simple when you look at it. Find $A$, $A^2$, $A^3$, $A^4$ by doing some matrix multiplication and then add and subtract following the question. I did all of the tedious work mentioned above. But the matrix $B$ I obtain is filled with garbage. I assuming either we must use some trigno mumbo jumbo identities, or there's something beautiful that I missed from the question. Edit: Could the sum of a geometric series help?",,"['matrices', 'trigonometry', 'trigonometric-series']"
74,Finding an invertible matrix P and some matrix C,Finding an invertible matrix P and some matrix C,,"Find an invertible matrix $P$ and a matrix $C$ of the form $C=\begin{pmatrix}a & -b\\b & a\end{pmatrix}$ such that the given matrix $A$ has the form $A = PCP^{-1}$ $A=\begin{pmatrix}5 & -2\\1 & 3\end{pmatrix}$ The first thing i tried to do was to find the eigenvectors of matrix $A$ and i got these vectors (which i glued together to get matrix $P$ and  $P^{-1}$) $P=\begin{pmatrix}1+ i& 1-i\\1 & 1\end{pmatrix}$ $P^{-1}=\begin{pmatrix}\frac{1}{2i} & \frac{-1+i}{2i}\\-\frac{1}{2i} & \frac{1+i}{2i}\end{pmatrix}$ Im not sure how to find the matrix $C$, i thought at first i could plug in the eigenvalues in the $C$ matrix, but i don't think that is what they problem i asking me to do. Any help will be appreciated","Find an invertible matrix $P$ and a matrix $C$ of the form $C=\begin{pmatrix}a & -b\\b & a\end{pmatrix}$ such that the given matrix $A$ has the form $A = PCP^{-1}$ $A=\begin{pmatrix}5 & -2\\1 & 3\end{pmatrix}$ The first thing i tried to do was to find the eigenvectors of matrix $A$ and i got these vectors (which i glued together to get matrix $P$ and  $P^{-1}$) $P=\begin{pmatrix}1+ i& 1-i\\1 & 1\end{pmatrix}$ $P^{-1}=\begin{pmatrix}\frac{1}{2i} & \frac{-1+i}{2i}\\-\frac{1}{2i} & \frac{1+i}{2i}\end{pmatrix}$ Im not sure how to find the matrix $C$, i thought at first i could plug in the eigenvalues in the $C$ matrix, but i don't think that is what they problem i asking me to do. Any help will be appreciated",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse']"
75,Proving Fibonacci Identity,Proving Fibonacci Identity,,"in this question I am asked to prove that if $A=\begin{bmatrix}1 & 1 \\ 1 & 0 \end{bmatrix}$ then $A^n= \begin{bmatrix} F_{n+1} & F_n \\ F_n & F_{n-1} \end{bmatrix}$ which I have successfully proved by induction on n.  Then we are asked to prove that $F_{k+j} = F_kF_{j+1} + F_{k−1}F_j = F_{k+1}F_j + F_kF_{j−1}$ by comparing the identity $A^{k+j}= A^kA^j $ And then a similar question for $A^{k+j+l}= A^kA^jA^l $  , I have tried answering these questions by computing the matrix identities but it got too complicated and I got lost in the algebra, is there an easier way to prove this? Thank you!","in this question I am asked to prove that if $A=\begin{bmatrix}1 & 1 \\ 1 & 0 \end{bmatrix}$ then $A^n= \begin{bmatrix} F_{n+1} & F_n \\ F_n & F_{n-1} \end{bmatrix}$ which I have successfully proved by induction on n.  Then we are asked to prove that $F_{k+j} = F_kF_{j+1} + F_{k−1}F_j = F_{k+1}F_j + F_kF_{j−1}$ by comparing the identity $A^{k+j}= A^kA^j $ And then a similar question for $A^{k+j+l}= A^kA^jA^l $  , I have tried answering these questions by computing the matrix identities but it got too complicated and I got lost in the algebra, is there an easier way to prove this? Thank you!",,"['matrices', 'fibonacci-numbers']"
76,Let a and b be such that $-2A^2 + 4A -3I_2 = aA + bI_2$. To find $a + b$.,Let a and b be such that . To find .,-2A^2 + 4A -3I_2 = aA + bI_2 a + b,$A \in M_2$ with characteristic polynomial $p(x) = x^2 -3x - 5$. Let a and b be such that  $-2A^2 + 4A -3I_2 = aA + bI_2$. To find $a + b$.,$A \in M_2$ with characteristic polynomial $p(x) = x^2 -3x - 5$. Let a and b be such that  $-2A^2 + 4A -3I_2 = aA + bI_2$. To find $a + b$.,,"['linear-algebra', 'matrices', 'proof-verification']"
77,A question about a vector operation,A question about a vector operation,,"I was working on a question which defined a new operation between vectors. Let $\vec{u}=(u_i)_{i\in\lbrack n\rbrack},\vec{v}=(v_i)_{i\in\lbrack n\rbrack}\in\Bbb R^n$, define $\star:\Bbb R^n\times\Bbb R^n\to\mathcal{M}_n(\Bbb R)$ where $\vec{u}\star\vec{v}=A_{ij}=(u_iv_j)$ as a matrix. The question is as follows: If $\vec{u},\vec{v}$ are linearly independent, find the rank of the matrix $\vec{u}\star\vec{v}-\vec{v}\star\vec{u}$. What I did was try to conjecture the rank with a couple of common l.i. vectors. I chose $e_1, e_2$. This gave me the matrix with entries $1$ in $(1,2)$, $-1$ in $(2,1)$, and $0$ elsewhere. Thus if the question is true, the matrix's rank should be $2$. To prove this I tried checking what happened when $\vec{u},\vec{v}$ were linearly dependent. The only possibility is that $\vec{v}$ is a scalar multiple of $\vec{u}$, then $\vec{v}=\lambda\vec{u}$ for a real $\lambda$. Should $\lambda$ be $0$, the matrix's rank would be 1. Else the matrix would be the zero matrix. Now to prove the result, I think that if I prove that the rank is less than or equal to $2$ then I've got the proof. This is because the other cases are covered in what I just wrote. Still, I feel that even if it were true, my proof would be too flimsy and not elegant. I'm looking for any hints and help to approach the solution.","I was working on a question which defined a new operation between vectors. Let $\vec{u}=(u_i)_{i\in\lbrack n\rbrack},\vec{v}=(v_i)_{i\in\lbrack n\rbrack}\in\Bbb R^n$, define $\star:\Bbb R^n\times\Bbb R^n\to\mathcal{M}_n(\Bbb R)$ where $\vec{u}\star\vec{v}=A_{ij}=(u_iv_j)$ as a matrix. The question is as follows: If $\vec{u},\vec{v}$ are linearly independent, find the rank of the matrix $\vec{u}\star\vec{v}-\vec{v}\star\vec{u}$. What I did was try to conjecture the rank with a couple of common l.i. vectors. I chose $e_1, e_2$. This gave me the matrix with entries $1$ in $(1,2)$, $-1$ in $(2,1)$, and $0$ elsewhere. Thus if the question is true, the matrix's rank should be $2$. To prove this I tried checking what happened when $\vec{u},\vec{v}$ were linearly dependent. The only possibility is that $\vec{v}$ is a scalar multiple of $\vec{u}$, then $\vec{v}=\lambda\vec{u}$ for a real $\lambda$. Should $\lambda$ be $0$, the matrix's rank would be 1. Else the matrix would be the zero matrix. Now to prove the result, I think that if I prove that the rank is less than or equal to $2$ then I've got the proof. This is because the other cases are covered in what I just wrote. Still, I feel that even if it were true, my proof would be too flimsy and not elegant. I'm looking for any hints and help to approach the solution.",,"['linear-algebra', 'matrices', 'matrix-rank']"
78,Proving that the inverse of a matrix $H$ is itself,Proving that the inverse of a matrix  is itself,H,"Let $u$ be a column vector in $\mathbb{R}^n$, and let $H = I_n - uu^T$ where $I_n$ is the $n \times n$ identity matrix. Also, it is known that $H$ is symmetric. Prove that if $u^Tu = 2$, then the inverse of $H$ is itself. Theoretically speaking, I know the easiest way to approach this is to show that $HH = I_n$. Currently I'm stuck at the following and can't get past the last equation: $$HH = (I_n - uu^T)(I_n - uu^T) = I_n^2 - 2uu^T + (uu^T)^2 = \cdots$$ How can I solve this equation using $u^Tu = 2$? I just can't see where this piece of information can assist me in proving the desired equation. I can't manipulate the last part to suit my needs. Tips and tricks are greatly appreciated!","Let $u$ be a column vector in $\mathbb{R}^n$, and let $H = I_n - uu^T$ where $I_n$ is the $n \times n$ identity matrix. Also, it is known that $H$ is symmetric. Prove that if $u^Tu = 2$, then the inverse of $H$ is itself. Theoretically speaking, I know the easiest way to approach this is to show that $HH = I_n$. Currently I'm stuck at the following and can't get past the last equation: $$HH = (I_n - uu^T)(I_n - uu^T) = I_n^2 - 2uu^T + (uu^T)^2 = \cdots$$ How can I solve this equation using $u^Tu = 2$? I just can't see where this piece of information can assist me in proving the desired equation. I can't manipulate the last part to suit my needs. Tips and tricks are greatly appreciated!",,"['linear-algebra', 'matrices', 'vectors', 'inverse']"
79,"If $A$ and $B$ are $n×n$ matrices, $AB = -BA$ , and $n$ is odd, show that either $A$ or $B$ has no inverse. [duplicate]","If  and  are  matrices,  , and  is odd, show that either  or  has no inverse. [duplicate]",A B n×n AB = -BA n A B,"This question already has answers here : Proof if $AB+BA=0$ Then atleast one of the matrices are singular. (4 answers) Closed 6 years ago . If $A$ and $B$ are $n×n$ matrices, $AB = -BA$ , and $n$ is odd, show that either $A$ or $B$ has no inverse. I have no clue how to do this and any help/guidance would be appreciated! Thanks in advance! $$det(AB) = det(-BA)$$  $$det(AB)= det(-B)det(A)$$  $$det(AB) = (-1)^ndet(BA)$$ since n is odd  $$det(AB) = -det(BA)$$ $$det(A)det(B) = -det(B)det(A)$$ $$2det(A)det(B) = 0$$ Therefore $det(A)=0$ or $det(B)=0$","This question already has answers here : Proof if $AB+BA=0$ Then atleast one of the matrices are singular. (4 answers) Closed 6 years ago . If $A$ and $B$ are $n×n$ matrices, $AB = -BA$ , and $n$ is odd, show that either $A$ or $B$ has no inverse. I have no clue how to do this and any help/guidance would be appreciated! Thanks in advance! $$det(AB) = det(-BA)$$  $$det(AB)= det(-B)det(A)$$  $$det(AB) = (-1)^ndet(BA)$$ since n is odd  $$det(AB) = -det(BA)$$ $$det(A)det(B) = -det(B)det(A)$$ $$2det(A)det(B) = 0$$ Therefore $det(A)=0$ or $det(B)=0$",,"['linear-algebra', 'matrices', 'inverse']"
80,Basis of the image of the product of two matrices,Basis of the image of the product of two matrices,,"We have  $A \in \mathbb R ^{\mathrm {mxn} }$ and $B \in \mathbb R ^{\mathrm {nxp}}$ which are two matrices. It is said that $\{ b_1, b_2, ..., b_k\}$, where $k \leq q$, is a basis for $\mathrm{Im}(B)$ and that $\mathrm{Ker}(A)\cap   \mathrm{Im}(B) = \{ \vec{0}\}$. We have to show  that $\{ Ab_1, Ab_2, ..., Ab_k \}$ form a basis for $\mathrm{Im}(AB)$. I've tried fiddling with the rank nullity theorem, leading me to this: $$\dim (\mathrm{Im}(AB)) = \dim (\mathrm{Im}(B)) - \dim(\mathrm{Ker}(A)\cap   \mathrm{Im}(B)) $$ which makes sense since $\dim(\mathrm{Ker}(A)\cap   \mathrm{Im}(B)) = 0 $ (since it only contains the null vector) and since we are asked to show that the basis for  $\mathrm{Im}(AB)$ is $\{ Ab_1, Ab_2, ..., Ab_k \}$ (i.e a set with $k$ vectors). But, unfortunately I don't where to go from here. The only thing I've shown is that  $\dim (\mathrm{Im}(AB)) = k$ Another approach I tried to have to solve this problem was the following: $$AB\vec{x} = \vec{0} \\ \Leftrightarrow A(B\vec{x}) = \vec{0} \\ \Rightarrow B\vec{x}\in \mathrm{Ker}(A) \\ \text{and}\ B\vec{x}\in \mathrm{Im}(B)$$ but again I don't know where to go from here.","We have  $A \in \mathbb R ^{\mathrm {mxn} }$ and $B \in \mathbb R ^{\mathrm {nxp}}$ which are two matrices. It is said that $\{ b_1, b_2, ..., b_k\}$, where $k \leq q$, is a basis for $\mathrm{Im}(B)$ and that $\mathrm{Ker}(A)\cap   \mathrm{Im}(B) = \{ \vec{0}\}$. We have to show  that $\{ Ab_1, Ab_2, ..., Ab_k \}$ form a basis for $\mathrm{Im}(AB)$. I've tried fiddling with the rank nullity theorem, leading me to this: $$\dim (\mathrm{Im}(AB)) = \dim (\mathrm{Im}(B)) - \dim(\mathrm{Ker}(A)\cap   \mathrm{Im}(B)) $$ which makes sense since $\dim(\mathrm{Ker}(A)\cap   \mathrm{Im}(B)) = 0 $ (since it only contains the null vector) and since we are asked to show that the basis for  $\mathrm{Im}(AB)$ is $\{ Ab_1, Ab_2, ..., Ab_k \}$ (i.e a set with $k$ vectors). But, unfortunately I don't where to go from here. The only thing I've shown is that  $\dim (\mathrm{Im}(AB)) = k$ Another approach I tried to have to solve this problem was the following: $$AB\vec{x} = \vec{0} \\ \Leftrightarrow A(B\vec{x}) = \vec{0} \\ \Rightarrow B\vec{x}\in \mathrm{Ker}(A) \\ \text{and}\ B\vec{x}\in \mathrm{Im}(B)$$ but again I don't know where to go from here.",,"['linear-algebra', 'matrices', 'vector-spaces', 'matrix-rank']"
81,Matrix equation: why can't I simplify by multiplying by the inverse matrix?,Matrix equation: why can't I simplify by multiplying by the inverse matrix?,,"I know, it's standard stuff, but I could not find a good place where I could read about this. Please help me understand, or refer me to some good source where I could learn. Thank you very much! I want to solve the equation: $X^TX a = X^Ty$ Where $X$ is a matrix, $y$ and $a$ are vectors. Can I solve the equation by multiplying both sides by the inverse matrix of $X^T$? $\color{red}{(X^T)^{-1}} X^TX a = \color{red}{(X^T)^{-1}} X^Ty $ ? $Xa = y$ $a = X^{-1}y$ Or am I doing something wrong? I watched a course where they solved the equation to: $a = (X^T X)^{-1}X^Ty$ but I don't understand why they didn't simplify the equation first.","I know, it's standard stuff, but I could not find a good place where I could read about this. Please help me understand, or refer me to some good source where I could learn. Thank you very much! I want to solve the equation: $X^TX a = X^Ty$ Where $X$ is a matrix, $y$ and $a$ are vectors. Can I solve the equation by multiplying both sides by the inverse matrix of $X^T$? $\color{red}{(X^T)^{-1}} X^TX a = \color{red}{(X^T)^{-1}} X^Ty $ ? $Xa = y$ $a = X^{-1}y$ Or am I doing something wrong? I watched a course where they solved the equation to: $a = (X^T X)^{-1}X^Ty$ but I don't understand why they didn't simplify the equation first.",,"['matrices', 'matrix-equations']"
82,Why is dimension of solution space of homogeneous equations n-r?,Why is dimension of solution space of homogeneous equations n-r?,,"This throws me off track completely - its like pushing me out of moving train.I am referring to page 65 of Shilov (Linear algebra). The author clearly states that in Homogeneous system of linear equations: If the coefficient matrix has order k x n (k -> number of equations, n -> number of unknowns) r is rank of the matrix. Then linear solution space has dimension n-r. I would think if rank is r, then the number of linearly independent rows is r. So if x i denotes any solution for r+1st equation to kth equation, it should be totally describable by linearly independent solutions r i from i=1 to i=r (Again going by rank). Yet, the dimension of solution space L is given by n-r. Why? Update: What's being said is starting to makes sense. I am probably mixing up the concept of linear independence of columns of basis minor matrix (rank r) with dimension of solution space. I guess the fact that first r columns of coefficient matrix of rank r are linearly independent implies that other columns r+1 .... n is expressible in terms of r columns. That gives us freedom to arbitrarily choose solutions c r+1...n for dependent column variables with solutions c 1...r for linearly  independent column elements uniquely determinable (By Cramers rule). This ""freedom"" manifests as dimension n-r of solution space. Does the above sound coherent?","This throws me off track completely - its like pushing me out of moving train.I am referring to page 65 of Shilov (Linear algebra). The author clearly states that in Homogeneous system of linear equations: If the coefficient matrix has order k x n (k -> number of equations, n -> number of unknowns) r is rank of the matrix. Then linear solution space has dimension n-r. I would think if rank is r, then the number of linearly independent rows is r. So if x i denotes any solution for r+1st equation to kth equation, it should be totally describable by linearly independent solutions r i from i=1 to i=r (Again going by rank). Yet, the dimension of solution space L is given by n-r. Why? Update: What's being said is starting to makes sense. I am probably mixing up the concept of linear independence of columns of basis minor matrix (rank r) with dimension of solution space. I guess the fact that first r columns of coefficient matrix of rank r are linearly independent implies that other columns r+1 .... n is expressible in terms of r columns. That gives us freedom to arbitrarily choose solutions c r+1...n for dependent column variables with solutions c 1...r for linearly  independent column elements uniquely determinable (By Cramers rule). This ""freedom"" manifests as dimension n-r of solution space. Does the above sound coherent?",,"['linear-algebra', 'matrices', 'matrix-rank', 'homogeneous-equation']"
83,"Proving that any element of $\text{SL}(2,\mathbb{R})$ can be expressed as $\pm\exp(z)$.",Proving that any element of  can be expressed as .,"\text{SL}(2,\mathbb{R}) \pm\exp(z)","I would like to show that any element $N\in\text{SL}(2,\mathbb{R})$ can be represented in the following form \begin{align} N=\pm\exp(z)~~~~\text{ for } ~~~~~z\in\mathfrak{sl}(2,\mathbb{R}). \end{align} The Lie algebra $\mathfrak{sl}(2,\mathbb{R})$ of $\text{SL}(2,\mathbb{R})$ is that of real traceless $2\times 2$ matrices. I (for personal reasons) choose to use  the basis \begin{align} \tau_1=\begin{pmatrix}0&&-1\\-1&&0\end{pmatrix},~~~\tau_2=\begin{pmatrix}1&&0\\0&&-1\end{pmatrix},~~~\tau_3=\begin{pmatrix}0&&1\\-1&&0\end{pmatrix}. \end{align} Any element  $z\in\mathfrak{sl}(2,\mathbb{R})$ can then be expressed as a linear combination of these matrices; $z=z^m\tau_m$ with $z^m$ the local coordinates of $\text{SL}(2,\mathbb{R})$ and I have used the Einstein summation convention. Thus, in this basis an arbitrary $z$ is  \begin{align} z=\begin{pmatrix} z_2&&z_3-z_1\\-z_1-z_3&&-z_2\end{pmatrix}. \end{align} To compute the exponential first observe that $z^2=\omega^2 I$ where $\omega^2:=z_1^2+z_2^2-z_3^2$ and $I$ is the $2\times 2$ identity. I think we should split it into three cases where $\omega^2=0, \omega^2<0$ and $\omega^2>0$. I will only do the case $\omega^2>0$ for brevity. Computing the exponential explicitly we have \begin{align} \exp(z)&=\exp\bigg(\begin{pmatrix} z_2&&z_3-z_1\\-z_1-z_3&&-z_2\end{pmatrix}\bigg)\\ &=I+z+\frac{1}{2!}\omega^2I+\frac{1}{3!}\omega^2z+\frac{1}{4!}\omega^4I+\frac{1}{5!}\omega^4z+\cdots\\ &=\sum_{n=0}^{\infty}\frac{\omega^{2n}}{(2n)!}I+\sum_{n=0}^{\infty}\frac{\omega^{2n}}{(2n+1)!}z\\ &=\cosh(\omega)I+\frac{1}{\omega}\sinh(\omega)z\\ &=\begin{pmatrix} \cosh(\omega)+\frac{z_2}{\omega}\sinh(\omega)&&\frac{z_3-z_1}{\omega}\sinh(\omega)\\\\-\frac{z_3+z_1}{\omega}\sinh(\omega)&&\cosh(\omega)-\frac{z_2}{\omega}\sinh(\omega)\end{pmatrix}\tag{1}. \end{align} Now I need to show that any $N$ can be written as $\pm(1)$ but I am not sure how to do this part. I thought I could try and write  \begin{align} N=\begin{pmatrix} a&&b\\c&&d\end{pmatrix} ,~~~~ ad-bc=1 \end{align} and solve the resulting system of equations but I don't think this is how it is done.","I would like to show that any element $N\in\text{SL}(2,\mathbb{R})$ can be represented in the following form \begin{align} N=\pm\exp(z)~~~~\text{ for } ~~~~~z\in\mathfrak{sl}(2,\mathbb{R}). \end{align} The Lie algebra $\mathfrak{sl}(2,\mathbb{R})$ of $\text{SL}(2,\mathbb{R})$ is that of real traceless $2\times 2$ matrices. I (for personal reasons) choose to use  the basis \begin{align} \tau_1=\begin{pmatrix}0&&-1\\-1&&0\end{pmatrix},~~~\tau_2=\begin{pmatrix}1&&0\\0&&-1\end{pmatrix},~~~\tau_3=\begin{pmatrix}0&&1\\-1&&0\end{pmatrix}. \end{align} Any element  $z\in\mathfrak{sl}(2,\mathbb{R})$ can then be expressed as a linear combination of these matrices; $z=z^m\tau_m$ with $z^m$ the local coordinates of $\text{SL}(2,\mathbb{R})$ and I have used the Einstein summation convention. Thus, in this basis an arbitrary $z$ is  \begin{align} z=\begin{pmatrix} z_2&&z_3-z_1\\-z_1-z_3&&-z_2\end{pmatrix}. \end{align} To compute the exponential first observe that $z^2=\omega^2 I$ where $\omega^2:=z_1^2+z_2^2-z_3^2$ and $I$ is the $2\times 2$ identity. I think we should split it into three cases where $\omega^2=0, \omega^2<0$ and $\omega^2>0$. I will only do the case $\omega^2>0$ for brevity. Computing the exponential explicitly we have \begin{align} \exp(z)&=\exp\bigg(\begin{pmatrix} z_2&&z_3-z_1\\-z_1-z_3&&-z_2\end{pmatrix}\bigg)\\ &=I+z+\frac{1}{2!}\omega^2I+\frac{1}{3!}\omega^2z+\frac{1}{4!}\omega^4I+\frac{1}{5!}\omega^4z+\cdots\\ &=\sum_{n=0}^{\infty}\frac{\omega^{2n}}{(2n)!}I+\sum_{n=0}^{\infty}\frac{\omega^{2n}}{(2n+1)!}z\\ &=\cosh(\omega)I+\frac{1}{\omega}\sinh(\omega)z\\ &=\begin{pmatrix} \cosh(\omega)+\frac{z_2}{\omega}\sinh(\omega)&&\frac{z_3-z_1}{\omega}\sinh(\omega)\\\\-\frac{z_3+z_1}{\omega}\sinh(\omega)&&\cosh(\omega)-\frac{z_2}{\omega}\sinh(\omega)\end{pmatrix}\tag{1}. \end{align} Now I need to show that any $N$ can be written as $\pm(1)$ but I am not sure how to do this part. I thought I could try and write  \begin{align} N=\begin{pmatrix} a&&b\\c&&d\end{pmatrix} ,~~~~ ad-bc=1 \end{align} and solve the resulting system of equations but I don't think this is how it is done.",,"['matrices', 'lie-groups', 'lie-algebras', 'matrix-exponential']"
84,How to check if a non-negative matrix is primitive (in the stochastic sense)?,How to check if a non-negative matrix is primitive (in the stochastic sense)?,,"A square non-negative matrix $A$ is primitive if $A^k$ has all entries positive for some positive integer $k$. Is there an algorithm to do this check in practice? Given a non-negative square matrix $A$ of finite dimension $N$, can we determine if it is primitive? Note: By non-negative matrix $A$, I mean that all its entries are non-negative.","A square non-negative matrix $A$ is primitive if $A^k$ has all entries positive for some positive integer $k$. Is there an algorithm to do this check in practice? Given a non-negative square matrix $A$ of finite dimension $N$, can we determine if it is primitive? Note: By non-negative matrix $A$, I mean that all its entries are non-negative.",,"['matrices', 'stochastic-processes']"
85,Show a matrix is positive definite,Show a matrix is positive definite,,"I was looking at UCLA's Department of Mathematics's Fall 2015 Basic Qualifying Exam, and question #12 is the following. Show that the following matrix is positive definite. $$M=\begin{pmatrix} 2 & 1 & 1 & \ldots & 1\\ 1 & 3 & 1 & \ldots & 1\\ 1 & 1 & 4 & \ldots & 1\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ 1 & 1& 1& \ldots &n+1 \end{pmatrix}$$ I wasn't super sure how one should proceed; showing $x^TMx>0$ for all $x \neq 0$ seems like a pain, and I was curious if there was a slick way to do this. Other ways would be to write $M = AA^T$ or go through the characteristic polynomial, but I'm not super sure about those either. If anyone can give some hints, that would be great!","I was looking at UCLA's Department of Mathematics's Fall 2015 Basic Qualifying Exam, and question #12 is the following. Show that the following matrix is positive definite. $$M=\begin{pmatrix} 2 & 1 & 1 & \ldots & 1\\ 1 & 3 & 1 & \ldots & 1\\ 1 & 1 & 4 & \ldots & 1\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ 1 & 1& 1& \ldots &n+1 \end{pmatrix}$$ I wasn't super sure how one should proceed; showing $x^TMx>0$ for all $x \neq 0$ seems like a pain, and I was curious if there was a slick way to do this. Other ways would be to write $M = AA^T$ or go through the characteristic polynomial, but I'm not super sure about those either. If anyone can give some hints, that would be great!",,"['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices']"
86,Symmetric matrix as sum,Symmetric matrix as sum,,"Let $B$ be a symmetric matrix, and $A$ an invertible matrix (or course both square matrices with the same order). Show that there is a matrix $X$ such that $$B= AX^T + A^T X$$","Let $B$ be a symmetric matrix, and $A$ an invertible matrix (or course both square matrices with the same order). Show that there is a matrix $X$ such that $$B= AX^T + A^T X$$",,"['linear-algebra', 'matrices', 'matrix-equations']"
87,Proof of a matrix exponential identity,Proof of a matrix exponential identity,,"Let $A,B \in M_n(\mathbb{R})$, and suppose $[A,[A,B]] = [B,[A,B]] = 0$, where $[A,B] = BA-AB$. We want to show $$ e^{At}e^{Bt}=e^{(A+B)t}e^{[A,B]t^2/2}, \quad \forall t\in \mathbb{R}. $$ I'm given a hint to show that $x(t) =e^{-(A+B)t}e^{Bt}e^{At}x_0$ is a solution to the ODE $x'= t[A,B]x$ for each $x_0\in \mathbb{R}$. I haven't been able to prove the hint, let alone see why being a solution helps prove the identity. Taking the derivative of $x(t)$ didn't lead to much, and moreover there's no $t$ outside of the exponent, and substituting it into the RHS of the ODE also didn't help. The RHS of the identity to be shown solves $x' = (A+B+t[A,B])x$, which maybe gives some intuition about the ODE in the hint, but I'm not really sure where to go.","Let $A,B \in M_n(\mathbb{R})$, and suppose $[A,[A,B]] = [B,[A,B]] = 0$, where $[A,B] = BA-AB$. We want to show $$ e^{At}e^{Bt}=e^{(A+B)t}e^{[A,B]t^2/2}, \quad \forall t\in \mathbb{R}. $$ I'm given a hint to show that $x(t) =e^{-(A+B)t}e^{Bt}e^{At}x_0$ is a solution to the ODE $x'= t[A,B]x$ for each $x_0\in \mathbb{R}$. I haven't been able to prove the hint, let alone see why being a solution helps prove the identity. Taking the derivative of $x(t)$ didn't lead to much, and moreover there's no $t$ outside of the exponent, and substituting it into the RHS of the ODE also didn't help. The RHS of the identity to be shown solves $x' = (A+B+t[A,B])x$, which maybe gives some intuition about the ODE in the hint, but I'm not really sure where to go.",,"['matrices', 'ordinary-differential-equations', 'matrix-exponential']"
88,Reconciling the 'column' and the 'row' pictures for matrices,Reconciling the 'column' and the 'row' pictures for matrices,,"I understand the column picture (namely considering a matrix equation $Ax=b$ as a vector equation with each column of the matrix $A$ scalarly multiplied by each coefficient of the vector $x$ to give the vector $b$) and the row picture (intersection of the planes, or hyperplanes/lines according to the dimensions). But since these are equivalent, there has to be some way to show this equivalency ? What I mean is, the vectors in the column picture must somehow directly correspond to the planes in the row picture, and the procedures we follow in either of the pictures must be equivalent. Is there an easy way to visualise this equivalency ? I do not mean equivalence, in terms of determining the solutions, that is, to determine if the planes are parallel based on the column vectors. What I mean is to get to a system/visualisation, which can connect the two pictures. The row and the column pictures are as mentioned in the Introduction to Linear Algebra by Gilbert Strang.","I understand the column picture (namely considering a matrix equation $Ax=b$ as a vector equation with each column of the matrix $A$ scalarly multiplied by each coefficient of the vector $x$ to give the vector $b$) and the row picture (intersection of the planes, or hyperplanes/lines according to the dimensions). But since these are equivalent, there has to be some way to show this equivalency ? What I mean is, the vectors in the column picture must somehow directly correspond to the planes in the row picture, and the procedures we follow in either of the pictures must be equivalent. Is there an easy way to visualise this equivalency ? I do not mean equivalence, in terms of determining the solutions, that is, to determine if the planes are parallel based on the column vectors. What I mean is to get to a system/visualisation, which can connect the two pictures. The row and the column pictures are as mentioned in the Introduction to Linear Algebra by Gilbert Strang.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-equations']"
89,QR Factorization for Solving Least Squares,QR Factorization for Solving Least Squares,,"By solving Least Squares, we use  $RX = Q^Tb$ What's the benefit for solving least squares using QR factorization instead of solving the normal equations? and Why?","By solving Least Squares, we use  $RX = Q^Tb$ What's the benefit for solving least squares using QR factorization instead of solving the normal equations? and Why?",,"['matrices', 'least-squares']"
90,"Representations of $S O( n )$ coming from $GL( N ,\mathbb{ R})$",Representations of  coming from,"S O( n ) GL( N ,\mathbb{ R})","I would like to show that ""the finite-dimensional spinor representation of $SO(N)$ does not arise from a finite-dimensional representation of $GL(N)$"" , as stated here and here . Apparently we should take this to mean that you can't find/embed the double cover of $SO(n)$ in $GL(n,\mathbb{R})$. (If needed, the original statement is here ). I am hoping what's done below is correct, any comments/fixes/illustrations/generalizations (even just in the comments section) would be greatly appreciated. I am not clear on the very end (posted a pic of that bit of the proof) and the generalization so comments on that would be helpful! Consider first the group of rotations $SO(2)$,  $$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos(\theta) && - \sin(\theta) \\ \sin(\theta) && \cos(\theta) \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}$$ of in the $(x,y)$ plane. We can explicitly derive the double cover by defining  $$\begin{aligned}  \zeta_0 &= \pm \sqrt{\frac{x-iy}{2}} \\  \zeta_1 &= \pm \sqrt{ \frac{-x-iy}{2} }  \end{aligned},$$ they transform under rotations as  $$\begin{aligned}  \zeta_0' &= \pm \sqrt{\frac{x'-iy'}{2}} = \pm \sqrt{\frac{x \cos(\theta) - y \sin(\theta) - i x \sin(\theta) - i y \cos(\theta)}{2}} \\  &= \pm \sqrt{\frac{(x - i y)\cos(\theta) - i(x - iy) \sin(\theta)}{2}} = \pm  \sqrt{\frac{(x - i y)e^{-i \theta}}{2}} = e^{-i \theta/2} \zeta_0 \\  \zeta_1' &= \pm \sqrt{ \frac{-x'-iy'}{2} } = \pm \sqrt{ \frac{-x \cos(\theta) + y \sin(\theta) - i x \sin(\theta) - i y \cos(\theta)}{2} }  \\  &= \pm \sqrt{ \frac{-(x + iy) \cos(\theta) - i(x + i y)\sin(\theta) }{2} } = \pm \sqrt{ \frac{-(x + iy) e^{i \theta} }{2} } = e^{i \theta/2}\zeta_1 \end{aligned},$$ or $$ \begin{bmatrix} \zeta_0' \\ \zeta_1' \end{bmatrix} = \begin{bmatrix} e^{-i\theta/2} && 0 \\ 0 && e^{i \theta/2} \end{bmatrix} \begin{bmatrix} \zeta_0 \\ \zeta_1 \end{bmatrix}$$  so that after a rotation by $2 \pi$ the signs change $$ \begin{aligned}  \zeta_0' &= \pm \sqrt{\frac{x'-iy'}{2}} = \pm \sqrt{\frac{e^{-i 2 \pi}(x-iy)}{2}} = \mp \sqrt{\frac{x-iy}{2}} \\  \zeta_1' &= \pm \sqrt{ \frac{-x'-iy'}{2} } = \mp \sqrt{ \frac{-x-iy}{2} }  \\  \begin{bmatrix} e^{-i\theta/2 - i \pi} && 0 \\ 0 && e^{i \theta/2 + i \pi} \end{bmatrix} &= \begin{bmatrix} - e^{-i\theta/2} && 0 \\ 0 && - e^{i \theta/2} \end{bmatrix} \end{aligned}.$$ In other words, to every $(x,y)$ there corresponds both a $(\zeta_0,\zeta_1)$ and a $(-\zeta_0,-\zeta_1)$, or  to every rotation matrix $$\begin{bmatrix} \cos(\theta) && - \sin(\theta) \\ \sin(\theta) && \cos(\theta) \end{bmatrix}  $$ there correspond the two matrices $$\begin{bmatrix} e^{-i\theta/2} && 0 \\ 0 && e^{i \theta/2} \end{bmatrix}, \begin{bmatrix} - e^{-i\theta/2} && 0 \\ 0 && - e^{i \theta/2} \end{bmatrix}.$$ Thus we have a 2-valued representation of the group of rotations, in the sense that one rotation $R$ in $SO(2)$ maps to two elements above. Now consider the elements of $A = \begin{bmatrix} a && b \\ c && d \end{bmatrix} \in SL(2,\mathbb{R}), \det(A) = 1$. We wish to know whether a representation of $A$ acting on a finite number of variables has the above 2-valued representation of $SO(2)$ as a sub-representation, that is, whether $SL(2,\mathbb{R})$ has multi-valued representations in the sense that $A$ in $SL(2,\mathbb{R})$ corresponds to multiple elements of a representation of $SL(2,\mathbb{R})$. If we were seeking to find multi-valued representations $\rho$ of $SL(2,\mathbb{C})$ then we would immediately see they do not exist since $SL(2,\mathbb{C})$ is simply connected. If we could find multi-valued representations of $SL(2,\mathbb{C})$ then defining a closed path $A(t)$ in $SL(2,\mathbb{C})$ we'd see $A(0) = A(1)$ while $\rho(A(0)) \neq \rho(A(1))$. On deforming the closed path down to a point, we have both $\rho(A(0)) = \rho(A(1))$ and $\rho(A(0)) \neq \rho(A(1))$ which is a contradiction. Since $SO(2)$ is not simply connected, we can't use this proof. Analyzing $A \in SL(2,\mathbb{R})$ we see $\det(A) = 1$ allowing us to view elements as rotations and so we see the closed loops in $SL(2,\mathbb{R})$ are rotations about a circle thus the fundamental group (set of equivalence classes of closed loops) is $\mathbb{Z}$. If we show the universal cover of $SL(2,\mathbb{R})$ with center $\mathbb{Z}$ is connected (in the page below) then this or it's representations cannot be used to obtain the above multi-valued representation. This shows that ""the finite-dimensional spinor representation of $SO(2)$ does not arise from a finite-dimensional representation of $GL(2)$."" For $N > 2$, I'm not sure why the above doesn't prove it by only focusing on two variables, indeed one source I read does this, but others seem to generalize it by modifying the end (e.g. the center is not $\mathbb{Z}$). Assuming the above is okay, this part is a bit unclear, the page it's taken from is below in case: It seems that the way to do this is to analyze $$y' = \frac{ay + b}{cy + d}$$ with $\det(A) > 0$ for $y \in \mathbb{R}$, set $y' = \tan(x') , y = \tan(x)$ and then differentiate  $$\tan(x') = \frac{ a \tan(x) + b}{c \tan(x) + d}$$ to find regions of increase/decrease, then study the $x = 0$ case and shift the parameters by $\lambda$ $$\tan(x') = \frac{b + \lambda b_0}{d + \lambda d_0}$$ from $0$ to $\infty$ then $-\infty$ to $0$ to find the same $(a,b,c,d)$ give $x + \pi$ and repeating this shows to every $x$ we get an infinite number of $x'$'s.","I would like to show that ""the finite-dimensional spinor representation of $SO(N)$ does not arise from a finite-dimensional representation of $GL(N)$"" , as stated here and here . Apparently we should take this to mean that you can't find/embed the double cover of $SO(n)$ in $GL(n,\mathbb{R})$. (If needed, the original statement is here ). I am hoping what's done below is correct, any comments/fixes/illustrations/generalizations (even just in the comments section) would be greatly appreciated. I am not clear on the very end (posted a pic of that bit of the proof) and the generalization so comments on that would be helpful! Consider first the group of rotations $SO(2)$,  $$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos(\theta) && - \sin(\theta) \\ \sin(\theta) && \cos(\theta) \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}$$ of in the $(x,y)$ plane. We can explicitly derive the double cover by defining  $$\begin{aligned}  \zeta_0 &= \pm \sqrt{\frac{x-iy}{2}} \\  \zeta_1 &= \pm \sqrt{ \frac{-x-iy}{2} }  \end{aligned},$$ they transform under rotations as  $$\begin{aligned}  \zeta_0' &= \pm \sqrt{\frac{x'-iy'}{2}} = \pm \sqrt{\frac{x \cos(\theta) - y \sin(\theta) - i x \sin(\theta) - i y \cos(\theta)}{2}} \\  &= \pm \sqrt{\frac{(x - i y)\cos(\theta) - i(x - iy) \sin(\theta)}{2}} = \pm  \sqrt{\frac{(x - i y)e^{-i \theta}}{2}} = e^{-i \theta/2} \zeta_0 \\  \zeta_1' &= \pm \sqrt{ \frac{-x'-iy'}{2} } = \pm \sqrt{ \frac{-x \cos(\theta) + y \sin(\theta) - i x \sin(\theta) - i y \cos(\theta)}{2} }  \\  &= \pm \sqrt{ \frac{-(x + iy) \cos(\theta) - i(x + i y)\sin(\theta) }{2} } = \pm \sqrt{ \frac{-(x + iy) e^{i \theta} }{2} } = e^{i \theta/2}\zeta_1 \end{aligned},$$ or $$ \begin{bmatrix} \zeta_0' \\ \zeta_1' \end{bmatrix} = \begin{bmatrix} e^{-i\theta/2} && 0 \\ 0 && e^{i \theta/2} \end{bmatrix} \begin{bmatrix} \zeta_0 \\ \zeta_1 \end{bmatrix}$$  so that after a rotation by $2 \pi$ the signs change $$ \begin{aligned}  \zeta_0' &= \pm \sqrt{\frac{x'-iy'}{2}} = \pm \sqrt{\frac{e^{-i 2 \pi}(x-iy)}{2}} = \mp \sqrt{\frac{x-iy}{2}} \\  \zeta_1' &= \pm \sqrt{ \frac{-x'-iy'}{2} } = \mp \sqrt{ \frac{-x-iy}{2} }  \\  \begin{bmatrix} e^{-i\theta/2 - i \pi} && 0 \\ 0 && e^{i \theta/2 + i \pi} \end{bmatrix} &= \begin{bmatrix} - e^{-i\theta/2} && 0 \\ 0 && - e^{i \theta/2} \end{bmatrix} \end{aligned}.$$ In other words, to every $(x,y)$ there corresponds both a $(\zeta_0,\zeta_1)$ and a $(-\zeta_0,-\zeta_1)$, or  to every rotation matrix $$\begin{bmatrix} \cos(\theta) && - \sin(\theta) \\ \sin(\theta) && \cos(\theta) \end{bmatrix}  $$ there correspond the two matrices $$\begin{bmatrix} e^{-i\theta/2} && 0 \\ 0 && e^{i \theta/2} \end{bmatrix}, \begin{bmatrix} - e^{-i\theta/2} && 0 \\ 0 && - e^{i \theta/2} \end{bmatrix}.$$ Thus we have a 2-valued representation of the group of rotations, in the sense that one rotation $R$ in $SO(2)$ maps to two elements above. Now consider the elements of $A = \begin{bmatrix} a && b \\ c && d \end{bmatrix} \in SL(2,\mathbb{R}), \det(A) = 1$. We wish to know whether a representation of $A$ acting on a finite number of variables has the above 2-valued representation of $SO(2)$ as a sub-representation, that is, whether $SL(2,\mathbb{R})$ has multi-valued representations in the sense that $A$ in $SL(2,\mathbb{R})$ corresponds to multiple elements of a representation of $SL(2,\mathbb{R})$. If we were seeking to find multi-valued representations $\rho$ of $SL(2,\mathbb{C})$ then we would immediately see they do not exist since $SL(2,\mathbb{C})$ is simply connected. If we could find multi-valued representations of $SL(2,\mathbb{C})$ then defining a closed path $A(t)$ in $SL(2,\mathbb{C})$ we'd see $A(0) = A(1)$ while $\rho(A(0)) \neq \rho(A(1))$. On deforming the closed path down to a point, we have both $\rho(A(0)) = \rho(A(1))$ and $\rho(A(0)) \neq \rho(A(1))$ which is a contradiction. Since $SO(2)$ is not simply connected, we can't use this proof. Analyzing $A \in SL(2,\mathbb{R})$ we see $\det(A) = 1$ allowing us to view elements as rotations and so we see the closed loops in $SL(2,\mathbb{R})$ are rotations about a circle thus the fundamental group (set of equivalence classes of closed loops) is $\mathbb{Z}$. If we show the universal cover of $SL(2,\mathbb{R})$ with center $\mathbb{Z}$ is connected (in the page below) then this or it's representations cannot be used to obtain the above multi-valued representation. This shows that ""the finite-dimensional spinor representation of $SO(2)$ does not arise from a finite-dimensional representation of $GL(2)$."" For $N > 2$, I'm not sure why the above doesn't prove it by only focusing on two variables, indeed one source I read does this, but others seem to generalize it by modifying the end (e.g. the center is not $\mathbb{Z}$). Assuming the above is okay, this part is a bit unclear, the page it's taken from is below in case: It seems that the way to do this is to analyze $$y' = \frac{ay + b}{cy + d}$$ with $\det(A) > 0$ for $y \in \mathbb{R}$, set $y' = \tan(x') , y = \tan(x)$ and then differentiate  $$\tan(x') = \frac{ a \tan(x) + b}{c \tan(x) + d}$$ to find regions of increase/decrease, then study the $x = 0$ case and shift the parameters by $\lambda$ $$\tan(x') = \frac{b + \lambda b_0}{d + \lambda d_0}$$ from $0$ to $\infty$ then $-\infty$ to $0$ to find the same $(a,b,c,d)$ give $x + \pi$ and repeating this shows to every $x$ we get an infinite number of $x'$'s.",,"['matrices', 'representation-theory', 'lie-groups', 'lie-algebras', 'spin-geometry']"
91,Peculiar way of writing eigendecomposition of a real symmetric matrix.,Peculiar way of writing eigendecomposition of a real symmetric matrix.,,"Look at this screenshot: Now, the original matrix is a precision/covariance matrix and therefore real symmetric, and that allows him to do those algebraic  manipulations, but that's not my point. Look at the rightmost side, and tell me if I'm wrong: it seems he writes the eigendecomposition as sum of matrices , given that we have a product of columns times rows inside the summation. Am I grasping it right?","Look at this screenshot: Now, the original matrix is a precision/covariance matrix and therefore real symmetric, and that allows him to do those algebraic  manipulations, but that's not my point. Look at the rightmost side, and tell me if I'm wrong: it seems he writes the eigendecomposition as sum of matrices , given that we have a product of columns times rows inside the summation. Am I grasping it right?",,"['linear-algebra', 'matrices', 'statistics']"
92,Change in eigenvalues when rank-1 matrix is added to positive definite matrix,Change in eigenvalues when rank-1 matrix is added to positive definite matrix,,"Let $v \in \mathbb{R}^n$ be a column vector and $A, B$ are positive definite matrices with eigenvalues $\lambda_A^1 \geq \dots \geq \lambda_A^n$ and $ \lambda_B^1 \geq \dots \geq \lambda_B^n$ respectively. We also know that a) $ \lambda_A^1 \leq \lambda_B^1 $ and $ \lambda_A^n \geq \lambda_B^n $ , b) $\operatorname{Trace} (A) = \operatorname{Trace} (B)$ c) $ \det(A) \leq \det(B) $ What can we say about eigenvalues (or change in eigenvalues) of following matrices (apart from Cauchy interlacing theorem): $ \hat{A} = A + vv^T$ $ \hat{B} = B + vv^T$ Is there any relationship between $ \frac{\det(\hat{A})}{\det(A)} $ and $  \frac{\det(\hat{B})}{\det(B)}  $ ?","Let be a column vector and are positive definite matrices with eigenvalues and respectively. We also know that a) and , b) c) What can we say about eigenvalues (or change in eigenvalues) of following matrices (apart from Cauchy interlacing theorem): Is there any relationship between and ?","v \in \mathbb{R}^n A, B \lambda_A^1 \geq \dots \geq \lambda_A^n  \lambda_B^1 \geq \dots \geq \lambda_B^n  \lambda_A^1 \leq \lambda_B^1   \lambda_A^n \geq \lambda_B^n  \operatorname{Trace} (A) = \operatorname{Trace} (B)  \det(A) \leq \det(B)   \hat{A} = A + vv^T  \hat{B} = B + vv^T  \frac{\det(\hat{A})}{\det(A)}    \frac{\det(\hat{B})}{\det(B)}  ","['real-analysis', 'linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'perturbation-theory']"
93,How to calculate this pretty determinant smartly?,How to calculate this pretty determinant smartly?,,"Let $$f(x):=a_1+a_2 \sin(x)+a_3 \sin(x)^2$$ $$g(x):=b_1+b_2 \sin(x)+b_3 \sin(x)^2$$ $$h(x):=c_1+c_2 \sin(x)+c_3 \sin(x)^2$$ then Mathematica shows that the determinant of the matrix $$\left(\begin{matrix} f(x) & g(x)& h(x) \\ f'(x) & g'(x) & h'(x) \\ f''(x) & g''(x) & h''(x) \end{matrix} \right)$$ is just $$2 (-a_3 b_2 c_1 + a_2 b_3 c_1 + a_3 b_1 c_2 - a_1 b_3 c_2 - a_2 b_1 c_3 +     a_1 b_2 c_3) \cos(x)^2$$ which is a suprisingly simple result given that the expressions from the chain-rule may become rather cumbersome. I would like to know, is there a smart way to conclude this result without explicitly calculating everything and regrouping terms in order to see the result?","Let $$f(x):=a_1+a_2 \sin(x)+a_3 \sin(x)^2$$ $$g(x):=b_1+b_2 \sin(x)+b_3 \sin(x)^2$$ $$h(x):=c_1+c_2 \sin(x)+c_3 \sin(x)^2$$ then Mathematica shows that the determinant of the matrix $$\left(\begin{matrix} f(x) & g(x)& h(x) \\ f'(x) & g'(x) & h'(x) \\ f''(x) & g''(x) & h''(x) \end{matrix} \right)$$ is just $$2 (-a_3 b_2 c_1 + a_2 b_3 c_1 + a_3 b_1 c_2 - a_1 b_3 c_2 - a_2 b_1 c_3 +     a_1 b_2 c_3) \cos(x)^2$$ which is a suprisingly simple result given that the expressions from the chain-rule may become rather cumbersome. I would like to know, is there a smart way to conclude this result without explicitly calculating everything and regrouping terms in order to see the result?",,"['linear-algebra', 'matrices']"
94,Evaluation of a trace - how does it depend on the inner product being used?,Evaluation of a trace - how does it depend on the inner product being used?,,"Let's say that I am looking at the space of matrices $M_{n\times n}(\mathbb{R})$. If I have some linear function $F : M_{n\times n}(\mathbb{R}) \to M_{n\times n}(\mathbb{R})$, then I can take it's ""trace"" by taking a basis $\{ E_{ab} \}_{a,b=1}^{n} \subset M_{n\times n}(\mathbb{R})$ and evaluating the following: $$ \mathrm{Tr}(F) = \sum_{a,b=1}^{n} \left< E_{ab}, F(E_{ab}) \right>\ \ \ =?\ \ \  \sum_{a,b=1}^{n} \mathrm{Tr}\left[ F(E_{ab})^{T}E_{ab}\right] $$ I'm assuming that the inner product on $M_{n\times n}(\mathbb{R})$ I should be using is $ \left< A,B \right> = \mathrm{Tr}(B^{T}A)$. But here's my question: obviously $ \left< A,B \right>_{\prime} = \frac{\pi}{1789} \mathrm{Tr}(A^{T}B)$ is a valid inner product as well! But this different choice of inner product would change the value of $\mathrm{Tr}(F)$! I would have liked to think that $\mathrm{Tr}(F)$ is invariant under the choice of our inner product for some reason...since this is not the case,  is the trace of a function like $F$ defined in terms the inner product $\left< A,B \right> = \mathrm{Tr}(B^{T}A)$? What's going on here?","Let's say that I am looking at the space of matrices $M_{n\times n}(\mathbb{R})$. If I have some linear function $F : M_{n\times n}(\mathbb{R}) \to M_{n\times n}(\mathbb{R})$, then I can take it's ""trace"" by taking a basis $\{ E_{ab} \}_{a,b=1}^{n} \subset M_{n\times n}(\mathbb{R})$ and evaluating the following: $$ \mathrm{Tr}(F) = \sum_{a,b=1}^{n} \left< E_{ab}, F(E_{ab}) \right>\ \ \ =?\ \ \  \sum_{a,b=1}^{n} \mathrm{Tr}\left[ F(E_{ab})^{T}E_{ab}\right] $$ I'm assuming that the inner product on $M_{n\times n}(\mathbb{R})$ I should be using is $ \left< A,B \right> = \mathrm{Tr}(B^{T}A)$. But here's my question: obviously $ \left< A,B \right>_{\prime} = \frac{\pi}{1789} \mathrm{Tr}(A^{T}B)$ is a valid inner product as well! But this different choice of inner product would change the value of $\mathrm{Tr}(F)$! I would have liked to think that $\mathrm{Tr}(F)$ is invariant under the choice of our inner product for some reason...since this is not the case,  is the trace of a function like $F$ defined in terms the inner product $\left< A,B \right> = \mathrm{Tr}(B^{T}A)$? What's going on here?",,"['linear-algebra', 'matrices', 'inner-products', 'trace']"
95,Relation between SVD and EVD,Relation between SVD and EVD,,"Given SVD decomposition $ A = U \Sigma V^T $ (where $U$ and $V$ are orthonormal and $ \Sigma $ is a diagonal matrix), I wish to prove that $ AA^T=U\Sigma \Sigma ^TU^T $ is the EVD decomposition of $ AA^T  $ (same goes for $ A^TA=V^T \Sigma ^T \Sigma V $ ). It's easy to see that indeed $ AA^T=U\Sigma \Sigma ^TU^T $ . But I don't understand why the values on $ \Sigma \Sigma ^T $ 's diagonal are $ AA^T $ 's eigenvalues.","Given SVD decomposition (where and are orthonormal and is a diagonal matrix), I wish to prove that is the EVD decomposition of (same goes for ). It's easy to see that indeed . But I don't understand why the values on 's diagonal are 's eigenvalues.", A = U \Sigma V^T  U V  \Sigma   AA^T=U\Sigma \Sigma ^TU^T   AA^T    A^TA=V^T \Sigma ^T \Sigma V   AA^T=U\Sigma \Sigma ^TU^T   \Sigma \Sigma ^T   AA^T ,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd']"
96,How one can prove that the Moore-Penrose pseudoinverse can be calculated by using the limit concept?,How one can prove that the Moore-Penrose pseudoinverse can be calculated by using the limit concept?,,"How one can prove that   the Moore-Penrose pseudoinverse can be calculated by using the limit concept as $$H^+=lim_{\delta \to 0^+} (\delta I+H^*H)^{-1}H^*$$, where $H^*$ denotes the Hermitian matrix. I know this limit exists even if $A^*A$ is not invertible. Moreover, since $(H^*HH^*+\delta H^*)=H^*(HH^*+\delta I)=(H^*H+\delta I) H^*$, and $(H^*HH^*+\delta H^*)$ and $H^*(HH^*+\delta I)H^*$ have inverses when $\delta > 0$ hence $$H^+=lim_{\delta \to 0^+} (\delta I+H^*H)^{-1}H^*$$ $$=lim_{\delta \to 0^+} H^*(\delta I+HH^*)^{-1}.$$ But, I couldn't go further to prove the limit definition is also Moore-Penrose pseudoinverse.i.e. $lim_{\delta \to 0^+} (\delta I+H^*H)^{-1}H^*=H^+=(H^*H)^{-1}H^*.$ I was wondering if anyone could help me?","How one can prove that   the Moore-Penrose pseudoinverse can be calculated by using the limit concept as $$H^+=lim_{\delta \to 0^+} (\delta I+H^*H)^{-1}H^*$$, where $H^*$ denotes the Hermitian matrix. I know this limit exists even if $A^*A$ is not invertible. Moreover, since $(H^*HH^*+\delta H^*)=H^*(HH^*+\delta I)=(H^*H+\delta I) H^*$, and $(H^*HH^*+\delta H^*)$ and $H^*(HH^*+\delta I)H^*$ have inverses when $\delta > 0$ hence $$H^+=lim_{\delta \to 0^+} (\delta I+H^*H)^{-1}H^*$$ $$=lim_{\delta \to 0^+} H^*(\delta I+HH^*)^{-1}.$$ But, I couldn't go further to prove the limit definition is also Moore-Penrose pseudoinverse.i.e. $lim_{\delta \to 0^+} (\delta I+H^*H)^{-1}H^*=H^+=(H^*H)^{-1}H^*.$ I was wondering if anyone could help me?",,"['matrices', 'pseudoinverse']"
97,"Given that $A$ is a projection, $A$ is Hermitian if and only if $AA^\ast A=A$","Given that  is a projection,  is Hermitian if and only if",A A AA^\ast A=A,"In this question, $A^\ast$ is the conjugate transpose of $A$. I am asked to show that if $A$ is a projection matrix, that $A$ is Hermitian if and only if $A=AA^\ast A$. One direction is easy--if $A$ is Hermitian, the result is trivial. So what about the converse? The assumption that $A=AA^\ast A$ and that $A$ is a projection gives us some identities: $$A^2=A,$$ $$A^\ast=A^\ast AA^\ast,$$ $$A=A(A^\ast)^2A,$$ etc.... But I am at a loss. How can I use these to prove the converse: If $A=AA^\ast A$ and $A$ is a projection, then $A$ is Hermitian?","In this question, $A^\ast$ is the conjugate transpose of $A$. I am asked to show that if $A$ is a projection matrix, that $A$ is Hermitian if and only if $A=AA^\ast A$. One direction is easy--if $A$ is Hermitian, the result is trivial. So what about the converse? The assumption that $A=AA^\ast A$ and that $A$ is a projection gives us some identities: $$A^2=A,$$ $$A^\ast=A^\ast AA^\ast,$$ $$A=A(A^\ast)^2A,$$ etc.... But I am at a loss. How can I use these to prove the converse: If $A=AA^\ast A$ and $A$ is a projection, then $A$ is Hermitian?",,['matrices']
98,Conversion of upper triangle linear index from index on symmetrical array,Conversion of upper triangle linear index from index on symmetrical array,,"Say we have a symmetrical matrix of the following form: A = [[0,1,2],      [1,0,2],      [2,2,0]] If we take the upper triangle of A and flatten it we get: B = [0,1,2,0,2,0] Is there a known formula that could take an index for A in the form of (i,j) and convert it to a value k that corresponds to the location in B for that index. For example: A[0,1] = B[1] = 1  A[1,0] = B[1] = 1 A[2,0] = B[2] = 2 A[2,1] = B[4] = 2 In addition what is the method for deriving this formula? Perhaps my brain just isn't working today, but I can't seem to remember how to go about doing this. I have found something similar here , but that is for the triangle with an offset of 1 and I would like to include the diagonal in my conversion.","Say we have a symmetrical matrix of the following form: A = [[0,1,2],      [1,0,2],      [2,2,0]] If we take the upper triangle of A and flatten it we get: B = [0,1,2,0,2,0] Is there a known formula that could take an index for A in the form of (i,j) and convert it to a value k that corresponds to the location in B for that index. For example: A[0,1] = B[1] = 1  A[1,0] = B[1] = 1 A[2,0] = B[2] = 2 A[2,1] = B[4] = 2 In addition what is the method for deriving this formula? Perhaps my brain just isn't working today, but I can't seem to remember how to go about doing this. I have found something similar here , but that is for the triangle with an offset of 1 and I would like to include the diagonal in my conversion.",,"['matrices', 'symmetric-matrices']"
99,Matrix Group generated by Roots of Unity,Matrix Group generated by Roots of Unity,,The question is essentially as follows: let $W$ be a primitive $n$-th root of unity where $n$ is an odd integer. Let $G$ be the subgroup (of $GL(n)$) of all 2x2 matrices generated by matrices $$\begin{bmatrix}0&-1\\1&0\end{bmatrix} \ \ \text{and} \ \  \begin{bmatrix} W&0\\0&W^{-1}\end{bmatrix}.$$ Prove that $G$ has order $4n.$,The question is essentially as follows: let $W$ be a primitive $n$-th root of unity where $n$ is an odd integer. Let $G$ be the subgroup (of $GL(n)$) of all 2x2 matrices generated by matrices $$\begin{bmatrix}0&-1\\1&0\end{bmatrix} \ \ \text{and} \ \  \begin{bmatrix} W&0\\0&W^{-1}\end{bmatrix}.$$ Prove that $G$ has order $4n.$,,"['abstract-algebra', 'matrices', 'group-theory']"
