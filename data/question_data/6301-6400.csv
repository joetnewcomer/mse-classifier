,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"How to solve $\int \frac{\,dx}{(x^3 + x + 1)^3}$?",How to solve ?,"\int \frac{\,dx}{(x^3 + x + 1)^3}","How to solve $$\int \frac{\,dx}{(x^3 + x + 1)^3}$$ ? Wolfram Alpha gives me something I am not familiar with. I thought that the idea was using partial fractions because $x^3$ and $x$ are bijections, there must be a real root but it seems that Wolfram Alpha is using numerical methods to approximate the root so it's not a ""nice"" number. I can't thing of any substitution which could help me nor any formula to transform this denominator. The formula $\int u\,dv = uv - \int v\,du$ also yields a more complicated integral: $$ \,dv = \,dx \implies v = x \\ u = \frac{1}{(x^3 + x + 1)^3} \implies du = -3\frac{3x^2 + 1}{(x^3 + x + 1)^4} \,dx \\ \int \frac{\,dx}{(x^3 + x + 1)^3} = \frac{x}{(x^3 + x + 1)^3} + 3\int \frac{3x^3 + x}{(x^3 + x + 1)^4} \,dx \\  = \frac{x}{(x^3 + x + 1)^3} + 3\int \frac{3x^3 + x \pm 2x \pm 3}{(x^3 + x + 1)^4}\,dx  \\ = \frac{x}{(x^3 + x + 1)^3} + 3\int \frac{3x^3 + 3x + 3}{(x^3 + x + 1)^4}\,dx + 3\int \frac{- 2x - 3}{(x^3 + x + 1)^4}\,dx \\ = \frac{x}{(x^3 + x + 1)^3} + 9\int \frac{\,dx}{(x^3 + x + 1)^3} - 3\int \frac{2x + 3}{(x^3 + x + 1)^4} \,dx  \\ I = \int \frac{\,dx}{(x^3 + x + 1)^3} \implies \\ -8I = \frac{x}{(x^3 + x + 1)^3} - 3\int \frac{2x + 3}{(x^3 + x + 1)^4} \,dx$$ Is this one really as complicated as Wolfram Alpha ""tells"" me or is there some sort of ""trick"" which can be applied?","How to solve $$\int \frac{\,dx}{(x^3 + x + 1)^3}$$ ? Wolfram Alpha gives me something I am not familiar with. I thought that the idea was using partial fractions because $x^3$ and $x$ are bijections, there must be a real root but it seems that Wolfram Alpha is using numerical methods to approximate the root so it's not a ""nice"" number. I can't thing of any substitution which could help me nor any formula to transform this denominator. The formula $\int u\,dv = uv - \int v\,du$ also yields a more complicated integral: $$ \,dv = \,dx \implies v = x \\ u = \frac{1}{(x^3 + x + 1)^3} \implies du = -3\frac{3x^2 + 1}{(x^3 + x + 1)^4} \,dx \\ \int \frac{\,dx}{(x^3 + x + 1)^3} = \frac{x}{(x^3 + x + 1)^3} + 3\int \frac{3x^3 + x}{(x^3 + x + 1)^4} \,dx \\  = \frac{x}{(x^3 + x + 1)^3} + 3\int \frac{3x^3 + x \pm 2x \pm 3}{(x^3 + x + 1)^4}\,dx  \\ = \frac{x}{(x^3 + x + 1)^3} + 3\int \frac{3x^3 + 3x + 3}{(x^3 + x + 1)^4}\,dx + 3\int \frac{- 2x - 3}{(x^3 + x + 1)^4}\,dx \\ = \frac{x}{(x^3 + x + 1)^3} + 9\int \frac{\,dx}{(x^3 + x + 1)^3} - 3\int \frac{2x + 3}{(x^3 + x + 1)^4} \,dx  \\ I = \int \frac{\,dx}{(x^3 + x + 1)^3} \implies \\ -8I = \frac{x}{(x^3 + x + 1)^3} - 3\int \frac{2x + 3}{(x^3 + x + 1)^4} \,dx$$ Is this one really as complicated as Wolfram Alpha ""tells"" me or is there some sort of ""trick"" which can be applied?",,"['calculus', 'real-analysis', 'integration', 'indefinite-integrals']"
1,Show that $\sum_{n \geqslant 0} |a_n z^n|<1$ with $|z|<1/3$ .,Show that  with  .,\sum_{n \geqslant 0} |a_n z^n|<1 |z|<1/3,"Assume that $\sum_{n \geqslant 0} a_n z^n$ converges for $|z|<1$ and $$\left|\sum_{n \geqslant 0} a_n z^n \right|<1$$    Show that for all $z \in \mathbb{C}$, with $|z|<1/3$ we have $$\sum_{n \geqslant 0} |a_n z^n|<1$$ I am stuck with this exercise, could someone give me some hint please ?","Assume that $\sum_{n \geqslant 0} a_n z^n$ converges for $|z|<1$ and $$\left|\sum_{n \geqslant 0} a_n z^n \right|<1$$    Show that for all $z \in \mathbb{C}$, with $|z|<1/3$ we have $$\sum_{n \geqslant 0} |a_n z^n|<1$$ I am stuck with this exercise, could someone give me some hint please ?",,['real-analysis']
2,Counterexamples for Hölder's inequality when $p$ and $q$ are not conjugate.,Counterexamples for Hölder's inequality when  and  are not conjugate.,p q,"Hölder's inequality shows that, when $$ \frac{1}{p} + \frac{1}{q} = 1,$$ and $f\in L^p$ and $g\in L^q$, then $$\Vert f\,g\Vert_1 \le \Vert f \Vert_p \Vert g \Vert_q.$$ Is there an example of this inequality failing for $p=q=1$?  I have been unable to find one in the usual places (analysis texts, and various combinations of search terms online).","Hölder's inequality shows that, when $$ \frac{1}{p} + \frac{1}{q} = 1,$$ and $f\in L^p$ and $g\in L^q$, then $$\Vert f\,g\Vert_1 \le \Vert f \Vert_p \Vert g \Vert_q.$$ Is there an example of this inequality failing for $p=q=1$?  I have been unable to find one in the usual places (analysis texts, and various combinations of search terms online).",,"['real-analysis', 'analysis', 'banach-spaces', 'examples-counterexamples', 'normed-spaces']"
3,"Limit of a trigonometric integral: $\lim\limits_{x \to \infty} \int_{0}^{\pi} \frac{\sin (t)}{1+\cos^{2}(xt)} \, \mathrm dt$",Limit of a trigonometric integral:,"\lim\limits_{x \to \infty} \int_{0}^{\pi} \frac{\sin (t)}{1+\cos^{2}(xt)} \, \mathrm dt","For all $x \in \mathbb{R}$, let  $$ {\rm f}\left(x\right) =\int_{0}^{\pi}\frac{\sin\left(t\right)}{1 + \cos^{2}\left(xt\right)}\,{\rm d}t $$ Compute the limit when $x\rightarrow +\infty$. My attempt : I tried the substitution $u=\sin(t)$ (and $u=\cos^2(xt)$) but it seems worse: $$ \int _{0}^{1}\!{\frac {u}{ \left( 1+ \left( \cos \left( x\arcsin  \left( u \right)  \right)  \right) ^{2} \right) \sqrt {1-{u}^{2}}}}{du} $$ I tried to use a subsequence $(x_n)$ which is tends to $+\infty$ and use the dominated convergence theorem but it didn't work either. Sorry if my attempt doesn't make much sense. Thank you in advance.","For all $x \in \mathbb{R}$, let  $$ {\rm f}\left(x\right) =\int_{0}^{\pi}\frac{\sin\left(t\right)}{1 + \cos^{2}\left(xt\right)}\,{\rm d}t $$ Compute the limit when $x\rightarrow +\infty$. My attempt : I tried the substitution $u=\sin(t)$ (and $u=\cos^2(xt)$) but it seems worse: $$ \int _{0}^{1}\!{\frac {u}{ \left( 1+ \left( \cos \left( x\arcsin  \left( u \right)  \right)  \right) ^{2} \right) \sqrt {1-{u}^{2}}}}{du} $$ I tried to use a subsequence $(x_n)$ which is tends to $+\infty$ and use the dominated convergence theorem but it didn't work either. Sorry if my attempt doesn't make much sense. Thank you in advance.",,"['calculus', 'real-analysis']"
4,Why any open set of $\mathbb{R}$ is the countable union of disjoint intervals.,Why any open set of  is the countable union of disjoint intervals.,\mathbb{R},"Every nonempty set in $\mathbb{R}$ is the countable union of open intervals. I read the following proof, but I still want to know why either the $I_q$'s are the same or are disjoint. If someone could provide a nice explanation it would be greatly appreciated. The following is the proof I found on this page - Any open subset of $\Bbb R$ is a at most countable union of disjoint open intervals. [Collecting Proofs] Let $U \subseteq R$ be open and let $x \in U$. Either $x$ is rational or irrational. If $x$ is rational, define \begin{align}I_x = \bigcup\limits_{\substack{I\text{ an open interval} \\ x~\in~I~\subseteq~U}} I,\end{align} which, as a union of non-disjoint open intervals (each $I$ contains $x$),  is an open interval subset to $U$. If $x$ is irrational, by openness of $U$ there is $\varepsilon > 0$ such that $(x - \varepsilon, x + \varepsilon) \subseteq U$, and there exists rational $y \in (x - \varepsilon, x + \varepsilon) \subseteq I_y$ (by the definition of $I_y$). Hence $x \in I_y$. So any $x \in U$ is in $I_q$ for some $q \in U \cap \mathbb{Q}$, and so \begin{align}U \subseteq \bigcup\limits_{q~\in~U \cap~\mathbb{Q}} I_q.\end{align} But $I_q \subseteq U$ for each $q \in U \cap \mathbb{Q}$; thus \begin{align}U = \bigcup\limits_{q~\in~U \cap~\mathbb{Q}} I_q, \end{align} which is a countable union of open intervals.","Every nonempty set in $\mathbb{R}$ is the countable union of open intervals. I read the following proof, but I still want to know why either the $I_q$'s are the same or are disjoint. If someone could provide a nice explanation it would be greatly appreciated. The following is the proof I found on this page - Any open subset of $\Bbb R$ is a at most countable union of disjoint open intervals. [Collecting Proofs] Let $U \subseteq R$ be open and let $x \in U$. Either $x$ is rational or irrational. If $x$ is rational, define \begin{align}I_x = \bigcup\limits_{\substack{I\text{ an open interval} \\ x~\in~I~\subseteq~U}} I,\end{align} which, as a union of non-disjoint open intervals (each $I$ contains $x$),  is an open interval subset to $U$. If $x$ is irrational, by openness of $U$ there is $\varepsilon > 0$ such that $(x - \varepsilon, x + \varepsilon) \subseteq U$, and there exists rational $y \in (x - \varepsilon, x + \varepsilon) \subseteq I_y$ (by the definition of $I_y$). Hence $x \in I_y$. So any $x \in U$ is in $I_q$ for some $q \in U \cap \mathbb{Q}$, and so \begin{align}U \subseteq \bigcup\limits_{q~\in~U \cap~\mathbb{Q}} I_q.\end{align} But $I_q \subseteq U$ for each $q \in U \cap \mathbb{Q}$; thus \begin{align}U = \bigcup\limits_{q~\in~U \cap~\mathbb{Q}} I_q, \end{align} which is a countable union of open intervals.",,"['real-analysis', 'analysis', 'measure-theory']"
5,Integral inequality $\int_0^1\log \left(f(x)\right)dx\leq \log\left(\int_0^1f(x)dx\right)$,Integral inequality,\int_0^1\log \left(f(x)\right)dx\leq \log\left(\int_0^1f(x)dx\right),How to prove this inequality $$\int_0^1\log \left(f(x)\right)dx\leq \log\left(\int_0^1f(x)dx\right)$$ for $f>0$.,How to prove this inequality $$\int_0^1\log \left(f(x)\right)dx\leq \log\left(\int_0^1f(x)dx\right)$$ for $f>0$.,,"['real-analysis', 'integration', 'inequality', 'integral-inequality']"
6,Extending exponentiation to reals,Extending exponentiation to reals,,"I've been reading through a course on exponential functions, starting from integer-valued exponents to rational ones as in: $x^r$ from $r\in \Bbb{N}$ to $\Bbb{Z}$, and combining them to rigorously construct for $r\in\ \Bbb{Q}$. Still, this book adresses high-schoolers, and therefore ""summons"" an extension of the exponential notation for real exponents. Is there any formal basis for this extension? Could it be related to the density of $\Bbb{Q}$ in $\Bbb{R}$ ? Or the limit of a sequence $(x^{(r_n)})$ where $(r_n)_{n=1}$ is a sequence of rationals that converge to $r$ a non-rational exponent?","I've been reading through a course on exponential functions, starting from integer-valued exponents to rational ones as in: $x^r$ from $r\in \Bbb{N}$ to $\Bbb{Z}$, and combining them to rigorously construct for $r\in\ \Bbb{Q}$. Still, this book adresses high-schoolers, and therefore ""summons"" an extension of the exponential notation for real exponents. Is there any formal basis for this extension? Could it be related to the density of $\Bbb{Q}$ in $\Bbb{R}$ ? Or the limit of a sequence $(x^{(r_n)})$ where $(r_n)_{n=1}$ is a sequence of rationals that converge to $r$ a non-rational exponent?",,"['real-analysis', 'exponentiation']"
7,How does the series $\sum_{n=1}^\infty \frac{(-1)^n \cos(n^2 x)}{n}$ behave?,How does the series  behave?,\sum_{n=1}^\infty \frac{(-1)^n \cos(n^2 x)}{n},"The title is the question: I'm trying to understand the behavior of the series $$\star \qquad \sum_{n=1}^\infty \frac{(-1)^n \cos(n^2 x)}{n},\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad\qquad \qquad $$ where $0\leq x \leq 1$.  All I know for sure about it is that it converges conditionally at $x=0$ to a known value (I think it's $-\ln 2$).  I don't even know how to check its convergence for other values of $x$ (it may be possible to compute it for certain rational multiples of $\pi$).  I plotted the partial sum going up to $n=1000$ using Maple and the partial sum only goes from about $-2.8$ to $0.4$.  It also looks continuous but very jagged.  This suggests that the series may converge at least pointwise to a continuous, nowhere-differntiable function.  The maximum value of the absolute value of the derivative of the 1000th partial sum is about $21249$, according to Maple17's Mamimize command, but I am suspicious because Maple says the maximum occurs at $x=0.2000002655$, suspiciously close to $0.2$.  But if the derivative of the 1000th partial sum really is that big, it suggests that if the series converges to a function, that function might be nowhere-continuous. I did some Googling, looking for information on continuous, nowhere-differentiable functions, hoping to find my series.  There seem to be a huge number of them that are well-known.  The best-known such functions seem to be ""Weierstrass functions"", but for such functions the denominator is something like $2^n$, which grows much faster than $n$.  At http://epubl.luth.se/1402-1617/2003/320/LTU-EX-03320-SE.pdf , there is posted a thesis about continuous, nowhere-differentiable functions.  On p. 23 of the .pdf (not the author's page number, but the page number of the .pdf document), the author discusses ""Riemann's function"", defined by $\sum_{k=1}^\infty \frac{1}{k^2}\sin(k^2 x)$.  This is very similar to my series, but again the denominator decays more rapidly than the one I'm considering. So, to make a long story short, this is what I'd like to know: For what values of $x$ does $\star$ converge? For what values of $x$ does $\star$ converge absolutely? If $\star$ converges for all $x$ (EDIT: NO: see accepted answer) , does it converge merely pointwise or uniformly? (see answer) If $\star$ converges at least pointwise (EDIT: NO: see accepted answer), where is the resulting function differentiable? EDIT: I accepted an excellent answer that answers most of my questions, and I will not unaccept it.  However, I am still a little curious about the complete answers to my first and second questions above.  I will upvote informative, original comments and answers.","The title is the question: I'm trying to understand the behavior of the series $$\star \qquad \sum_{n=1}^\infty \frac{(-1)^n \cos(n^2 x)}{n},\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad\qquad \qquad $$ where $0\leq x \leq 1$.  All I know for sure about it is that it converges conditionally at $x=0$ to a known value (I think it's $-\ln 2$).  I don't even know how to check its convergence for other values of $x$ (it may be possible to compute it for certain rational multiples of $\pi$).  I plotted the partial sum going up to $n=1000$ using Maple and the partial sum only goes from about $-2.8$ to $0.4$.  It also looks continuous but very jagged.  This suggests that the series may converge at least pointwise to a continuous, nowhere-differntiable function.  The maximum value of the absolute value of the derivative of the 1000th partial sum is about $21249$, according to Maple17's Mamimize command, but I am suspicious because Maple says the maximum occurs at $x=0.2000002655$, suspiciously close to $0.2$.  But if the derivative of the 1000th partial sum really is that big, it suggests that if the series converges to a function, that function might be nowhere-continuous. I did some Googling, looking for information on continuous, nowhere-differentiable functions, hoping to find my series.  There seem to be a huge number of them that are well-known.  The best-known such functions seem to be ""Weierstrass functions"", but for such functions the denominator is something like $2^n$, which grows much faster than $n$.  At http://epubl.luth.se/1402-1617/2003/320/LTU-EX-03320-SE.pdf , there is posted a thesis about continuous, nowhere-differentiable functions.  On p. 23 of the .pdf (not the author's page number, but the page number of the .pdf document), the author discusses ""Riemann's function"", defined by $\sum_{k=1}^\infty \frac{1}{k^2}\sin(k^2 x)$.  This is very similar to my series, but again the denominator decays more rapidly than the one I'm considering. So, to make a long story short, this is what I'd like to know: For what values of $x$ does $\star$ converge? For what values of $x$ does $\star$ converge absolutely? If $\star$ converges for all $x$ (EDIT: NO: see accepted answer) , does it converge merely pointwise or uniformly? (see answer) If $\star$ converges at least pointwise (EDIT: NO: see accepted answer), where is the resulting function differentiable? EDIT: I accepted an excellent answer that answers most of my questions, and I will not unaccept it.  However, I am still a little curious about the complete answers to my first and second questions above.  I will upvote informative, original comments and answers.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
8,"proving $\lim\limits_{n\to\infty} \int_{0}^{1} f(x^n)dx = f(0)$ when f is continous on [0,1]","proving  when f is continous on [0,1]",\lim\limits_{n\to\infty} \int_{0}^{1} f(x^n)dx = f(0),"$$\lim\limits_{n\to\infty} \int_{0}^{1} f(x^n)dx = f(0)$$  when f is continuous on $[0,1]$ I know it can be proved using bounded convergence theorem but,  I wanna know proof using only basic properties of riemann integral and fundamental theorem of calculus and MVT for integrals ...   Thank you.","$$\lim\limits_{n\to\infty} \int_{0}^{1} f(x^n)dx = f(0)$$  when f is continuous on $[0,1]$ I know it can be proved using bounded convergence theorem but,  I wanna know proof using only basic properties of riemann integral and fundamental theorem of calculus and MVT for integrals ...   Thank you.",,"['real-analysis', 'riemann-sum']"
9,What Does it Mean for a Function to have Finite Support?,What Does it Mean for a Function to have Finite Support?,,"I am working on a problem that states: Let $f$ be integrable over $\mathbb{R}$ and $\varepsilon > 0$. Show that there is a simple function $\eta$ on $\mathbb{R}$ which has finite support and $\int_{\mathbb{R}} \lvert f - \eta \rvert < \varepsilon$ What does it mean for a simple function $\eta$ to have finite support? I do not need help with the actual problem, just the meaning. Unfortunately, Royden's fourth edition of 'Real Analysis' does not describe this.","I am working on a problem that states: Let $f$ be integrable over $\mathbb{R}$ and $\varepsilon > 0$. Show that there is a simple function $\eta$ on $\mathbb{R}$ which has finite support and $\int_{\mathbb{R}} \lvert f - \eta \rvert < \varepsilon$ What does it mean for a simple function $\eta$ to have finite support? I do not need help with the actual problem, just the meaning. Unfortunately, Royden's fourth edition of 'Real Analysis' does not describe this.",,['real-analysis']
10,Measure theory singles out the countable cardinal. Why?,Measure theory singles out the countable cardinal. Why?,,"In some elementary analysis courses, we discussed what would fail without countable additivity, although it's not as if there would be some contradiction.  It would merely be ""not nice.""  We'd lose continuity under sequential monotone limits.  Dunford and Schwartz say some things about finitely additive set functions.  What happens when instead of notions like ""$\sigma$ algebra"" and countable additivity, you replace every occurrence of countability with some larger cardinal?  So the reals may not be quite the right place to carry out such a theory because if you have an arbitrary sized disjoint union of ""measurable"" sets in the proposed theory, then unless all but countable many have ""measure 0"" we would have that the measure would have to be infinity or undefined in the signed case all the time.  But what happens if more general values of the measures are taken, like say in a Banach space? To the extent that the following question can be answered beyond ""just because it worked out that way"", why is it that our modern mathematical theories basically only care about scalar measures that are countably additive, or perhaps in strange situations banach space valued measures, but still countably additive?  Why not finite, or why not bigger?","In some elementary analysis courses, we discussed what would fail without countable additivity, although it's not as if there would be some contradiction.  It would merely be ""not nice.""  We'd lose continuity under sequential monotone limits.  Dunford and Schwartz say some things about finitely additive set functions.  What happens when instead of notions like ""$\sigma$ algebra"" and countable additivity, you replace every occurrence of countability with some larger cardinal?  So the reals may not be quite the right place to carry out such a theory because if you have an arbitrary sized disjoint union of ""measurable"" sets in the proposed theory, then unless all but countable many have ""measure 0"" we would have that the measure would have to be infinity or undefined in the signed case all the time.  But what happens if more general values of the measures are taken, like say in a Banach space? To the extent that the following question can be answered beyond ""just because it worked out that way"", why is it that our modern mathematical theories basically only care about scalar measures that are countably additive, or perhaps in strange situations banach space valued measures, but still countably additive?  Why not finite, or why not bigger?",,"['real-analysis', 'analysis', 'measure-theory']"
11,"""Constrained"" numerical solutions of ODEs with conservation laws?","""Constrained"" numerical solutions of ODEs with conservation laws?",,"I know little about numerical methods and I was considering the following problem that possibly has standard solution in the literature. Suppose you have an ODE for wich we already know that it must satisfy a conservation law for some ""energy"" (for example hamiltonian dynamical systems such as $\frac 1 2 m \dot{x}^2+U(x)=\mbox{const}$). Then if we try to solve it with standard Runge Kutta methods we are likely to have errors for the energy at each step, which possibly will sum up. But what if I want to sacrifice a little accuracy for the ""position"" in order to have a greater accuracy for the energy? (This could be relevant if for example I am interested in having good approximation of long time asimptotic behaviors which should not be affected by errors in short time dynamics). Are there numerical methods which tries to give the best approximation given the constrain of keeping the energy ""strictly"" fixed?","I know little about numerical methods and I was considering the following problem that possibly has standard solution in the literature. Suppose you have an ODE for wich we already know that it must satisfy a conservation law for some ""energy"" (for example hamiltonian dynamical systems such as $\frac 1 2 m \dot{x}^2+U(x)=\mbox{const}$). Then if we try to solve it with standard Runge Kutta methods we are likely to have errors for the energy at each step, which possibly will sum up. But what if I want to sacrifice a little accuracy for the ""position"" in order to have a greater accuracy for the energy? (This could be relevant if for example I am interested in having good approximation of long time asimptotic behaviors which should not be affected by errors in short time dynamics). Are there numerical methods which tries to give the best approximation given the constrain of keeping the energy ""strictly"" fixed?",,"['real-analysis', 'ordinary-differential-equations', 'numerical-methods', 'dynamical-systems']"
12,Hölder- continuous function [duplicate],Hölder- continuous function [duplicate],,"This question already has answers here : Function on $[a,b]$ that satisfies a Hölder condition of order $\alpha > 1 $ is constant (2 answers) Closed 6 years ago . $f:I \rightarrow \mathbb R$ is said to be Hölder continuous if $\exists \alpha>0$ such that $|f(x)-f(y)| \leq M|x-y|^\alpha$, $ \forall x,y \in I$, $0<\alpha\leq1$. Prove that $f$ Hölder continuous $\Rightarrow$ $f$ uniformly continuous and if $\alpha>1$, then f is constant. In order to prove that $f$ Hölder continuous $\Rightarrow$ $f$ uniformly continuous, it is enough to note that $|f(x)-f(y)| \leq M |x-y|^\alpha \leq M|x-y|$, since $\alpha \leq 1$. This implies that f is Lipschitz $\Rightarrow$ f is uniformly continuous. But how can I prove that if $\alpha >1$, then $f$ is constant?","This question already has answers here : Function on $[a,b]$ that satisfies a Hölder condition of order $\alpha > 1 $ is constant (2 answers) Closed 6 years ago . $f:I \rightarrow \mathbb R$ is said to be Hölder continuous if $\exists \alpha>0$ such that $|f(x)-f(y)| \leq M|x-y|^\alpha$, $ \forall x,y \in I$, $0<\alpha\leq1$. Prove that $f$ Hölder continuous $\Rightarrow$ $f$ uniformly continuous and if $\alpha>1$, then f is constant. In order to prove that $f$ Hölder continuous $\Rightarrow$ $f$ uniformly continuous, it is enough to note that $|f(x)-f(y)| \leq M |x-y|^\alpha \leq M|x-y|$, since $\alpha \leq 1$. This implies that f is Lipschitz $\Rightarrow$ f is uniformly continuous. But how can I prove that if $\alpha >1$, then $f$ is constant?",,['real-analysis']
13,"If $f:I\to\mathbb{R}$ is $1{-}1$ and continuous, then $f$ is strictly monotone on $I$.","If  is  and continuous, then  is strictly monotone on .",f:I\to\mathbb{R} 1{-}1 f I,"Suppose that $I\subseteq\mathbb{R}$ is nonempty. If $f:I\to\mathbb{R}$ is $1{-}1$ and continuous, then $f$ is strictly monotone on $I$. The answer in the back of the book$^1$, which I found after writing the following proof, says this is false (no explanation). However, I cannot find an error in my proof or think of a viable counterexample. What am I overlooking? Suppose not, and there exists some $x_1,x_2,x_3\in I$ with $x_1<x_2<x_3$ such that $f(x_1)\leq f(x_2)$ and $f(x_2)\geq f(x_3)$, or $f(x_1)\geq f(x_2)$ and $f(x_2)\leq f(x_3)$. Without loss of generality, we may assume that $f(x_1)\leq f(x_2)$ and $f(x_2)\geq f(x_3)$. If $f(x_1)=f(x_2)$ or $f(x_2)=f(x_3)$ then $f$ would not be $1{-}1$; thus, $f(x_1)<f(x_2)$ and $f(x_2)>f(x_3)$. We are left with two possible cases. Case 1. Suppose $f(x_1)<f(x_3)$. Then $f(x_1)<f(x_3)<f(x_2)$. By the Intermediate Value Theorem, there exists point $x_0\in (x_1, x_2)$ such that $f(x_0)=f(x_3)$ with $x_0\neq x_3$ since $x_3\notin (x_1,x_2)$, a contradiction since $f$ is $1{-}1$. Case 2. Suppose $f(x_1)>f(x_3)$. Then $f(x_3)<f(x_1)<f(x_2)$. By the Intermediate Value Theorem, there exists point $x_0\in (x_2,x_3)$ such that $f(x_0)=f(x_1)$ with $x_0\neq x_1$ since $x_1\notin (x_2,x_3)$, a contradiction since $f$ is $1{-}1$. If we assume that $f(x_1)\geq f(x_2)$ and $f(x_2)\leq f(x_3)$ we are lead to similar contradictions; thus, $f$ is strictly monotone. $\blacksquare$ 1: An Introduction to Analysis by William R. Wade, 4th edition","Suppose that $I\subseteq\mathbb{R}$ is nonempty. If $f:I\to\mathbb{R}$ is $1{-}1$ and continuous, then $f$ is strictly monotone on $I$. The answer in the back of the book$^1$, which I found after writing the following proof, says this is false (no explanation). However, I cannot find an error in my proof or think of a viable counterexample. What am I overlooking? Suppose not, and there exists some $x_1,x_2,x_3\in I$ with $x_1<x_2<x_3$ such that $f(x_1)\leq f(x_2)$ and $f(x_2)\geq f(x_3)$, or $f(x_1)\geq f(x_2)$ and $f(x_2)\leq f(x_3)$. Without loss of generality, we may assume that $f(x_1)\leq f(x_2)$ and $f(x_2)\geq f(x_3)$. If $f(x_1)=f(x_2)$ or $f(x_2)=f(x_3)$ then $f$ would not be $1{-}1$; thus, $f(x_1)<f(x_2)$ and $f(x_2)>f(x_3)$. We are left with two possible cases. Case 1. Suppose $f(x_1)<f(x_3)$. Then $f(x_1)<f(x_3)<f(x_2)$. By the Intermediate Value Theorem, there exists point $x_0\in (x_1, x_2)$ such that $f(x_0)=f(x_3)$ with $x_0\neq x_3$ since $x_3\notin (x_1,x_2)$, a contradiction since $f$ is $1{-}1$. Case 2. Suppose $f(x_1)>f(x_3)$. Then $f(x_3)<f(x_1)<f(x_2)$. By the Intermediate Value Theorem, there exists point $x_0\in (x_2,x_3)$ such that $f(x_0)=f(x_1)$ with $x_0\neq x_1$ since $x_1\notin (x_2,x_3)$, a contradiction since $f$ is $1{-}1$. If we assume that $f(x_1)\geq f(x_2)$ and $f(x_2)\leq f(x_3)$ we are lead to similar contradictions; thus, $f$ is strictly monotone. $\blacksquare$ 1: An Introduction to Analysis by William R. Wade, 4th edition",,[]
14,Intuition behind the difference between derived sets and closed sets?,Intuition behind the difference between derived sets and closed sets?,,"I missed the lecture from my Analysis class where my professor talked about derived sets. Furthermore, nothing about derived sets is in my textbook. Upon looking in many topology textbooks, few even have the term ""derived set"" in their index and many books only say ""$A'$ is the set of limit points of $A$"". But I am really confused on the difference between $A'$ and $\bar{A}$. For example, Let $A=\{(x,y)∈ \mathbb{R}^{2} ∣x^2+y^2<1\}$, certainly $(1,0) \in A'$, but shouldn't $(0,0)∈A'$ too? In this way it would seem that $A \subseteq A'$. The definition of a limit point is a point $x$ such that every neighborhood of $x$ contains a point of $A$ other than $x$ itself. Then wouldn't $(0,0)$ fit this criterion? If I am wrong, why? And if I am not, can someone please give me some more intuitive examples that clearly illustrate, the subtle difference between $A'$, and $\bar{A}$?","I missed the lecture from my Analysis class where my professor talked about derived sets. Furthermore, nothing about derived sets is in my textbook. Upon looking in many topology textbooks, few even have the term ""derived set"" in their index and many books only say ""$A'$ is the set of limit points of $A$"". But I am really confused on the difference between $A'$ and $\bar{A}$. For example, Let $A=\{(x,y)∈ \mathbb{R}^{2} ∣x^2+y^2<1\}$, certainly $(1,0) \in A'$, but shouldn't $(0,0)∈A'$ too? In this way it would seem that $A \subseteq A'$. The definition of a limit point is a point $x$ such that every neighborhood of $x$ contains a point of $A$ other than $x$ itself. Then wouldn't $(0,0)$ fit this criterion? If I am wrong, why? And if I am not, can someone please give me some more intuitive examples that clearly illustrate, the subtle difference between $A'$, and $\bar{A}$?",,"['real-analysis', 'general-topology', 'elementary-set-theory', 'intuition']"
15,functional equations for trigonometric functions: $f(x+y)=f(x)f(y)-g(x)g(y)$ and $g(x+y)=f(x)g(y)+g(x)f(y)$,functional equations for trigonometric functions:  and,f(x+y)=f(x)f(y)-g(x)g(y) g(x+y)=f(x)g(y)+g(x)f(y),"It is well known that the following system of functional equations: $\begin{cases} f(x+y)=f(x)f(y)-g(x)g(y) \\ g(x+y)=f(x)g(y)+g(x)f(y) \end{cases}$ admit the solution $(f,g)=(\cos,\sin)$. Are there any other solutions (besides the trivial $(0,0)$)? i.e. do these identities characterize the trigonometric functions? If not, what additional identities are required?","It is well known that the following system of functional equations: $\begin{cases} f(x+y)=f(x)f(y)-g(x)g(y) \\ g(x+y)=f(x)g(y)+g(x)f(y) \end{cases}$ admit the solution $(f,g)=(\cos,\sin)$. Are there any other solutions (besides the trivial $(0,0)$)? i.e. do these identities characterize the trigonometric functions? If not, what additional identities are required?",,"['real-analysis', 'trigonometry', 'functional-equations']"
16,Tietze extension theorem for complex valued functions,Tietze extension theorem for complex valued functions,,"Why is this theorem always only stated for real valued functions, and not for complex valued functions? Thanks.","Why is this theorem always only stated for real valued functions, and not for complex valued functions? Thanks.",,"['real-analysis', 'general-topology', 'analysis']"
17,$\sum a_n$ converges $\iff \sum (\sqrt{1+a_n}-1)$ converges,converges  converges,\sum a_n \iff \sum (\sqrt{1+a_n}-1),Let $a_n\in\mathbb{R}_{\geq 0}$ and assume $\sum {a_n}^2$ converges. Show that: $\sum a_n$ converges $\iff \sum (\sqrt{1+a_n}-1)$ converges For $\Rightarrow$ I think I need to show that $\sqrt{1+a_n}-1 \leq {a_n}^2 +a_n$. Or something like that ? Any hints ?,Let $a_n\in\mathbb{R}_{\geq 0}$ and assume $\sum {a_n}^2$ converges. Show that: $\sum a_n$ converges $\iff \sum (\sqrt{1+a_n}-1)$ converges For $\Rightarrow$ I think I need to show that $\sqrt{1+a_n}-1 \leq {a_n}^2 +a_n$. Or something like that ? Any hints ?,,"['real-analysis', 'sequences-and-series']"
18,"Find sets $E_1, E_2,\dots$ of finite outer measure s.t. $E_k \searrow E$ and $\lim |E_k|_e > |E|_e$",Find sets  of finite outer measure s.t.  and,"E_1, E_2,\dots E_k \searrow E \lim |E_k|_e > |E|_e","I am trying to show existence of sets $E_1, E_2,\ldots$ s.t. $E_k \searrow E$, $|E_k|_e$ (outer measures of $E_k$'s) are finite and $\lim |E_k|_e > |E|_e$ strictly. I took a nonmeasurable subset of $[0,1)$ and tried to find a sequence of nonmeasurable sets decreasing to empty set. We know that all these sets have positive outer measure, but is it possible that their limit is positive? (or is there such a decreasing sequence?)","I am trying to show existence of sets $E_1, E_2,\ldots$ s.t. $E_k \searrow E$, $|E_k|_e$ (outer measures of $E_k$'s) are finite and $\lim |E_k|_e > |E|_e$ strictly. I took a nonmeasurable subset of $[0,1)$ and tried to find a sequence of nonmeasurable sets decreasing to empty set. We know that all these sets have positive outer measure, but is it possible that their limit is positive? (or is there such a decreasing sequence?)",,"['real-analysis', 'measure-theory']"
19,$\lim_{p\to \infty}\Vert f\Vert_{p}=\Vert f\Vert_{\infty}$? [duplicate],? [duplicate],\lim_{p\to \infty}\Vert f\Vert_{p}=\Vert f\Vert_{\infty},"This question already has answers here : Closed 11 years ago . Possible Duplicate: Limit of $L^p$ norm On the $L_p$ spaces, when is $$\lim_{p\to \infty}\| f\|_{p}=\| f\|_{\infty}$$ true? Or what condition is necessary for this?","This question already has answers here : Closed 11 years ago . Possible Duplicate: Limit of $L^p$ norm On the $L_p$ spaces, when is $$\lim_{p\to \infty}\| f\|_{p}=\| f\|_{\infty}$$ true? Or what condition is necessary for this?",,"['real-analysis', 'measure-theory']"
20,$\int_{0}^{\pi}\left|\frac{\sin nx}{x}\right|~dx \geq \frac{2}{\pi}(1+\frac{1}{2}+ \cdots +\frac{1}{n})$,,\int_{0}^{\pi}\left|\frac{\sin nx}{x}\right|~dx \geq \frac{2}{\pi}(1+\frac{1}{2}+ \cdots +\frac{1}{n}),Show that  $$\int_{0}^{\pi}\left|\frac{\sin nx}{x}\right|~dx \geqslant \frac{2}{\pi}\left(1+\frac{1}{2}+ \dots +\frac{1}{n}\right)$$ I know $0 \leqslant \left|\sin nx\right| \leqslant 1 $. But with this I can't solve. Please help.,Show that  $$\int_{0}^{\pi}\left|\frac{\sin nx}{x}\right|~dx \geqslant \frac{2}{\pi}\left(1+\frac{1}{2}+ \dots +\frac{1}{n}\right)$$ I know $0 \leqslant \left|\sin nx\right| \leqslant 1 $. But with this I can't solve. Please help.,,"['real-analysis', 'inequality', 'integration']"
21,Linear combinations of sequences of uniformly integrable functions,Linear combinations of sequences of uniformly integrable functions,,"Let $\{f_n\}$ and $\{g_n\}$ be sequences of uniformly integrable functions on $E$.  Show that for $\alpha$, $\beta$, the sequences $\alpha f_n + \beta g_n$ are also uniformly integrable. Attempt at a proof: Since both sequences are uniformly integrable I can find a $\delta=\min\{\delta_{g_n}, \delta_{f_n}\}$ such that $|\int_{A_{f_n}\cup A_{g_n}} f_n +g_n|< \epsilon$. I would like to know how to show this if I knew that $f_n$ would converge pointwise to $f$.  I'm not given this detail however. Edit: A family $\mathscr{F}$ of measurable functions on $E$ is said to be uniformly integrable over $E$ provided for each $\epsilon >0$, there is a $\delta >0$ such that for each $f \in \mathscr{F}$, if $A \subseteq E$ is measurable and $m(A)<\delta$, the $\int_A |f|< \epsilon$.","Let $\{f_n\}$ and $\{g_n\}$ be sequences of uniformly integrable functions on $E$.  Show that for $\alpha$, $\beta$, the sequences $\alpha f_n + \beta g_n$ are also uniformly integrable. Attempt at a proof: Since both sequences are uniformly integrable I can find a $\delta=\min\{\delta_{g_n}, \delta_{f_n}\}$ such that $|\int_{A_{f_n}\cup A_{g_n}} f_n +g_n|< \epsilon$. I would like to know how to show this if I knew that $f_n$ would converge pointwise to $f$.  I'm not given this detail however. Edit: A family $\mathscr{F}$ of measurable functions on $E$ is said to be uniformly integrable over $E$ provided for each $\epsilon >0$, there is a $\delta >0$ such that for each $f \in \mathscr{F}$, if $A \subseteq E$ is measurable and $m(A)<\delta$, the $\int_A |f|< \epsilon$.",,['real-analysis']
22,Proving Baire's theorem: The intersection of a sequence of dense open subsets of a complete metric space is nonempty,Proving Baire's theorem: The intersection of a sequence of dense open subsets of a complete metric space is nonempty,,"The following is problem 3.22 from Rudin's Princples of Mathematical Analysis : Suppose $X$ is a nonempty complete metric space, and $\{G_n\}$ is a sequence of dense open subsets of $X$. Prove Baire's theorem, namely, that $\bigcap_{n=1}^\infty G_n$ is not empty. Hint: find a shrinking sequence of neighbourhoods $E_n$ such that $\overline{E}_n\subset G_n$. Here's what I've tried so far: Let $\{r_n\}$ be a Cauchy sequence of positive real numbers converging to $0$. Fix $x\in X$ and define $E_i=\{g\in G_i:d(g,x)<r_i\}$, which is nonempty since $G_i$ is dense. I would like to show that for all $i$, $\overline{E}_i\subset G_i$ (I had convinced myself that this would be true but I am now having doubts). Let $e\in \overline{E}_i$. Then either $e\in E_i$ or $e$ is a limit point of $E_i$. If $e\in E_i$ then $e\in G_i$. Otherwise, every neighbourhood of $e$ contains a point in $E_i$. I thought that I should be able to then choose some point $e'\in E_i$ in a neighbourhood of $e$ and, since $G_i$ is open, it'll have a neighbourhood $N\subset G_i$ which contains $e$, but this is proving to be difficult and I'm worried that it's not true. If I can show that this is true then the rest will follow from results I've already proven. Does my approach make any sense? Incidentally, as a secondary question, what type of a thing would $G_n$ be? A sequence of dense open subsets seems weird to me—at first I was thinking of some sequence of infinite subsets of rational numbers in the real numbers but I realized that those aren't open. Is there anything which would be familiar to my little undergrad brain which would be analogous to this problem?","The following is problem 3.22 from Rudin's Princples of Mathematical Analysis : Suppose $X$ is a nonempty complete metric space, and $\{G_n\}$ is a sequence of dense open subsets of $X$. Prove Baire's theorem, namely, that $\bigcap_{n=1}^\infty G_n$ is not empty. Hint: find a shrinking sequence of neighbourhoods $E_n$ such that $\overline{E}_n\subset G_n$. Here's what I've tried so far: Let $\{r_n\}$ be a Cauchy sequence of positive real numbers converging to $0$. Fix $x\in X$ and define $E_i=\{g\in G_i:d(g,x)<r_i\}$, which is nonempty since $G_i$ is dense. I would like to show that for all $i$, $\overline{E}_i\subset G_i$ (I had convinced myself that this would be true but I am now having doubts). Let $e\in \overline{E}_i$. Then either $e\in E_i$ or $e$ is a limit point of $E_i$. If $e\in E_i$ then $e\in G_i$. Otherwise, every neighbourhood of $e$ contains a point in $E_i$. I thought that I should be able to then choose some point $e'\in E_i$ in a neighbourhood of $e$ and, since $G_i$ is open, it'll have a neighbourhood $N\subset G_i$ which contains $e$, but this is proving to be difficult and I'm worried that it's not true. If I can show that this is true then the rest will follow from results I've already proven. Does my approach make any sense? Incidentally, as a secondary question, what type of a thing would $G_n$ be? A sequence of dense open subsets seems weird to me—at first I was thinking of some sequence of infinite subsets of rational numbers in the real numbers but I realized that those aren't open. Is there anything which would be familiar to my little undergrad brain which would be analogous to this problem?",,"['real-analysis', 'general-topology', 'metric-spaces', 'baire-category']"
23,"union of two simply connected open , with open and non empty intersection in $R^2$","union of two simply connected open , with open and non empty intersection in",R^2,"Let $D_1,D_2$ be two simply connected open subsets of $\mathbb{R}^2$. Let's suppose that it's intersection is nonempty and connected. Then $ D_1\cup D_2$ is simply connected. I have no idea how can I do this.","Let $D_1,D_2$ be two simply connected open subsets of $\mathbb{R}^2$. Let's suppose that it's intersection is nonempty and connected. Then $ D_1\cup D_2$ is simply connected. I have no idea how can I do this.",,['real-analysis']
24,Locally continuously differentiable implies locally Lipschitz,Locally continuously differentiable implies locally Lipschitz,,"I am interesting in the following result: Let $X$ be a normed space and $f : X \to \mathbb{R}$. If $f$ is continuously differentiable in a neighborhood $V$ of a point $x_0 \in X$, then $f$ is locally Lipschitz at $x_0$. Using mean value theorem, for all $x,y$ in an open ball $B \subset V$, there exists $z \in [x,y]$ such that $f(x)-f(y)= \langle f'(z), x-y \rangle$ so $|f(x)-f(y)| \leq \max\limits_{z \in B} ||f'(z)||. ||x-y||$. In finite dimension, we can suppose that the closure of $B$ is in $V$ so that $\max\limits_{z \in B} ||f'(z)|| < + \infty$ because $z \mapsto f'(z)$ is continuous and the closure of $B$ is compact. But what happens in infinite dimension? Is the result still true?","I am interesting in the following result: Let $X$ be a normed space and $f : X \to \mathbb{R}$. If $f$ is continuously differentiable in a neighborhood $V$ of a point $x_0 \in X$, then $f$ is locally Lipschitz at $x_0$. Using mean value theorem, for all $x,y$ in an open ball $B \subset V$, there exists $z \in [x,y]$ such that $f(x)-f(y)= \langle f'(z), x-y \rangle$ so $|f(x)-f(y)| \leq \max\limits_{z \in B} ||f'(z)||. ||x-y||$. In finite dimension, we can suppose that the closure of $B$ is in $V$ so that $\max\limits_{z \in B} ||f'(z)|| < + \infty$ because $z \mapsto f'(z)$ is continuous and the closure of $B$ is compact. But what happens in infinite dimension? Is the result still true?",,"['real-analysis', 'holder-spaces']"
25,Evaluating: $\int \frac{t}{\cos{t}} dt$,Evaluating:,\int \frac{t}{\cos{t}} dt,"How would you evaluate the following indefinite integral? In fact, I did evaluate  $\int \frac{\cos{t}}{t} dt$ by parametric integration and then I thought of this variant. $$\int \frac{t}{\cos{t}} dt$$ Here you may find the result given by W|A.","How would you evaluate the following indefinite integral? In fact, I did evaluate  $\int \frac{\cos{t}}{t} dt$ by parametric integration and then I thought of this variant. $$\int \frac{t}{\cos{t}} dt$$ Here you may find the result given by W|A.",,"['calculus', 'real-analysis', 'integration', 'indefinite-integrals']"
26,Do all analytic and $2\pi$ periodic functions have a finite Fourier series?,Do all analytic and  periodic functions have a finite Fourier series?,2\pi,"Consider a function $f:\mathbb{R}\to\mathbb{R}$ which is periodic with period $2\pi$. Let us impose the condition that $f$ is analytic. Now does that imply that $f$ has a finite Fourier series? PS : Although this question seems to be related to this , I couldn't find anything that I can understand there","Consider a function $f:\mathbb{R}\to\mathbb{R}$ which is periodic with period $2\pi$. Let us impose the condition that $f$ is analytic. Now does that imply that $f$ has a finite Fourier series? PS : Although this question seems to be related to this , I couldn't find anything that I can understand there",,"['real-analysis', 'fourier-series', 'analyticity']"
27,Fatou's Lemma and Jensen's inequality,Fatou's Lemma and Jensen's inequality,,"I would like to know if there is any relationship between Fatou's lemma and Jensen's inequality. On paper, I find a similarity in their expressions. Fatou's Lemma: $$\int (\liminf_{n \to \infty}f_n)\le \liminf_{n \to \infty}\int (f_n) $$ Jensen's inequality: $$\mathbb{E}(f(X)) \le f(\mathbb{E} (X))$$ if $f(x)$ is concave in $x$. Could someone provide me with some insight explaining this similarity?","I would like to know if there is any relationship between Fatou's lemma and Jensen's inequality. On paper, I find a similarity in their expressions. Fatou's Lemma: $$\int (\liminf_{n \to \infty}f_n)\le \liminf_{n \to \infty}\int (f_n) $$ Jensen's inequality: $$\mathbb{E}(f(X)) \le f(\mathbb{E} (X))$$ if $f(x)$ is concave in $x$. Could someone provide me with some insight explaining this similarity?",,"['real-analysis', 'measure-theory', 'probability-theory']"
28,"Is an increasing, bounded and continuous function on $[a,+\infty)$ uniformly continuous?","Is an increasing, bounded and continuous function on  uniformly continuous?","[a,+\infty)","Supose $f$ is increasing, bounded and continuous on $[a,+\infty)$. Is  $f$ uniformly continuous ? I think yes. how to prove that? My idea is to show there exists $X$ , $f$ is uniformly continuous on $[X,+\infty)$. How to fix such $X$?","Supose $f$ is increasing, bounded and continuous on $[a,+\infty)$. Is  $f$ uniformly continuous ? I think yes. how to prove that? My idea is to show there exists $X$ , $f$ is uniformly continuous on $[X,+\infty)$. How to fix such $X$?",,"['calculus', 'real-analysis']"
29,Mean value theorem for vector valued functions,Mean value theorem for vector valued functions,,"In the general situation of $f:S\to R^m$ where $S\subset R^n$. There is a form of the mean value theorem: $a\cdot (f(y)-f(x))=a\cdot (f'(z)(y-x))$ which requires a vector $a$ and dot products. Is it possible to create a generalization of the mean value theorem that doesn't involve dot products. Something like $$f(y)-f(x) = cf'(z)(y-x)$$ where $c$ is some real number. The idea being that the path $f(t(y-x)+ x)$ where $t\in [0,1]$ should have a tangent $f'(z)(y-x)$ that is parallel to $f(y)-f(x)$. Now there is no reason that they should have the same length though and so a constant scaling factor $c$ is needed. And to avoid some pathological cases maybe some conditions are needed such as $f(y)\neq f(x)$.","In the general situation of $f:S\to R^m$ where $S\subset R^n$. There is a form of the mean value theorem: $a\cdot (f(y)-f(x))=a\cdot (f'(z)(y-x))$ which requires a vector $a$ and dot products. Is it possible to create a generalization of the mean value theorem that doesn't involve dot products. Something like $$f(y)-f(x) = cf'(z)(y-x)$$ where $c$ is some real number. The idea being that the path $f(t(y-x)+ x)$ where $t\in [0,1]$ should have a tangent $f'(z)(y-x)$ that is parallel to $f(y)-f(x)$. Now there is no reason that they should have the same length though and so a constant scaling factor $c$ is needed. And to avoid some pathological cases maybe some conditions are needed such as $f(y)\neq f(x)$.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
30,Using the Mean Value Theorem to Evaluate an Integral of a Sequence of Functions?,Using the Mean Value Theorem to Evaluate an Integral of a Sequence of Functions?,,"The following statement should be true, I think, but I'm having a hell of a time trying to prove it: Let $f_n$ be a $C^1$ function on $[0,a]$, satisfying $f_n = 1$ on $[1/n,a]$ and $0\le f_n \le 1$ and $f_n(0)=0$. Let $\phi$ be a continuous function on $[0,a].$ I want to say that $\lim_{n\to\infty} \int_0^a f_n'(x) \phi(x) dx = \phi(0)$. If $\phi$ is $C^1$, I can just do integration by parts to prove this. But I'm not sure what to do if $\phi$ is just continuous. This is what I have so far. Using the mean value theorem, we have $\int_0^a f_n'(x)\phi(x) dx = \int_0^{1/n} f_n'(x)\phi(x) dx= \frac{1}{n}f_n'(c_n) \phi(c_n)$ where $0<c_n<1/n$. I think I need to use the mean value theorem on $f_n$ and continuity of $f_n'$ to show that $\frac{1}{n}f_n'(c_n)$ goes to 1 as $n\to\infty$, but I can't seem to figure out how to do this... can someone throw me a hint?","The following statement should be true, I think, but I'm having a hell of a time trying to prove it: Let $f_n$ be a $C^1$ function on $[0,a]$, satisfying $f_n = 1$ on $[1/n,a]$ and $0\le f_n \le 1$ and $f_n(0)=0$. Let $\phi$ be a continuous function on $[0,a].$ I want to say that $\lim_{n\to\infty} \int_0^a f_n'(x) \phi(x) dx = \phi(0)$. If $\phi$ is $C^1$, I can just do integration by parts to prove this. But I'm not sure what to do if $\phi$ is just continuous. This is what I have so far. Using the mean value theorem, we have $\int_0^a f_n'(x)\phi(x) dx = \int_0^{1/n} f_n'(x)\phi(x) dx= \frac{1}{n}f_n'(c_n) \phi(c_n)$ where $0<c_n<1/n$. I think I need to use the mean value theorem on $f_n$ and continuity of $f_n'$ to show that $\frac{1}{n}f_n'(c_n)$ goes to 1 as $n\to\infty$, but I can't seem to figure out how to do this... can someone throw me a hint?",,['real-analysis']
31,Asymptotic Behavior of Iterated Sums,Asymptotic Behavior of Iterated Sums,,"Given the integral identity \begin{align} \int_{0}^{t} \cdots \int_{0}^{t - t_{1} - \dots - t_{n -1}} 1 \ dt_1 \cdots dt_n = \frac{t^{n}}{n!}, \end{align} I believe it is true that  \begin{align} \sum_{i_{1} = 0}^{\lfloor t \rfloor} \cdots \sum_{i_{n} = 0}^{\lfloor t - i_1 - \cdots - i_{n-1} \rfloor} 1 = \frac{t^{n}}{n!} + O(t^{n-1}), \end{align} where $\lfloor \cdot \rfloor$ is the floor function. However, I'm having difficulty in justifying the exponent $n - 1$. Is it indeed the case? Any help would certainly be appreciated. Thanks!","Given the integral identity \begin{align} \int_{0}^{t} \cdots \int_{0}^{t - t_{1} - \dots - t_{n -1}} 1 \ dt_1 \cdots dt_n = \frac{t^{n}}{n!}, \end{align} I believe it is true that  \begin{align} \sum_{i_{1} = 0}^{\lfloor t \rfloor} \cdots \sum_{i_{n} = 0}^{\lfloor t - i_1 - \cdots - i_{n-1} \rfloor} 1 = \frac{t^{n}}{n!} + O(t^{n-1}), \end{align} where $\lfloor \cdot \rfloor$ is the floor function. However, I'm having difficulty in justifying the exponent $n - 1$. Is it indeed the case? Any help would certainly be appreciated. Thanks!",,"['real-analysis', 'asymptotics']"
32,Lebesgue Decomposition Theorem,Lebesgue Decomposition Theorem,,"The usual statement of the Lebesgue Decomposition Theorem says that given two $\sigma$-finite measures $\mu$ and $\nu$ on a measure space, we can decompose $\nu = \nu_1 + \nu_2$, where $\nu_1$ is absolutely continuous with respect to $\mu$ and $\nu_2$ and $\mu$ are mutually singular. Wikipedia ( link text ) says that there is a ""refinement"" of this result, where the singular part $\nu_2$ can be further decomposed into a discrete measure and a singular continuous measure. I understand what a discrete measure is, but what exactly is the definition of a singular continuous measure? I was also wondering if anyone knew of a reference for this refined result, since I haven't been able to find it anywhere.","The usual statement of the Lebesgue Decomposition Theorem says that given two $\sigma$-finite measures $\mu$ and $\nu$ on a measure space, we can decompose $\nu = \nu_1 + \nu_2$, where $\nu_1$ is absolutely continuous with respect to $\mu$ and $\nu_2$ and $\mu$ are mutually singular. Wikipedia ( link text ) says that there is a ""refinement"" of this result, where the singular part $\nu_2$ can be further decomposed into a discrete measure and a singular continuous measure. I understand what a discrete measure is, but what exactly is the definition of a singular continuous measure? I was also wondering if anyone knew of a reference for this refined result, since I haven't been able to find it anywhere.",,['real-analysis']
33,Is the Gamma function superadditive?,Is the Gamma function superadditive?,,"A function $f$ is superadditive if $f(x) + f(y) \le f(x+y)$. The question is: Does a real number $a$ exists such that for all  real numbers with $x, y\ \ge \ a $ $$ \Gamma(x) + \Gamma(y) \le \Gamma(x+y) \quad ?$$","A function $f$ is superadditive if $f(x) + f(y) \le f(x+y)$. The question is: Does a real number $a$ exists such that for all  real numbers with $x, y\ \ge \ a $ $$ \Gamma(x) + \Gamma(y) \le \Gamma(x+y) \quad ?$$",,['real-analysis']
34,Baby Rudin Theorem 1.19 Step 5,Baby Rudin Theorem 1.19 Step 5,,"I have no background in pure mathematics, and I'm trying to learn how to be more rigorous in general. To help with this, I am trying to make everything more explicit as I progress through Rudin's Principles of Mathematical Analysis. I am currently going through theorem 1.19's proof where Rudin constructs $\mathbb{R}$ from $\mathbb{Q}$ . In particular, in step 5, Rudin states that it is obvious that if $\alpha , \beta , \gamma \in \mathbb{R}$ and $\alpha < \beta$ , then $\alpha + \gamma < \beta + \gamma$ . So far, the book has defined $\alpha , \beta , \gamma$ as subsets of $\mathbb{Q}$ called cuts, and has shown that cuts respect the field axioms of addition. I am intuitively convinced that the set $\beta + \gamma$ contains elements that $\alpha + \gamma$ does not, but am struggling with how to formalize it. The following is my thought process on my (failed) attempt. I know that: $\beta > \alpha \implies \exists x \in \beta : x \notin \alpha$ , and I think the next step is the take some value $y \in \gamma$ and show that $y + x \in \beta + \gamma$ while $y + x \notin \alpha + \gamma$ . I originally thought this would work by taking $y = \sup \gamma$ , but $\sup \gamma$ is a set, not a rational number. When I try to think about taking $y < \sup \gamma$ (edit: sorry, this should say $y \in \gamma$ not $y < \sup \gamma$ ), it's clear to me that $y + x \in \beta + \gamma$ , but it's no longer clear to me that $y + x \notin \alpha + \gamma$ . Any help would be greatly appreciated!","I have no background in pure mathematics, and I'm trying to learn how to be more rigorous in general. To help with this, I am trying to make everything more explicit as I progress through Rudin's Principles of Mathematical Analysis. I am currently going through theorem 1.19's proof where Rudin constructs from . In particular, in step 5, Rudin states that it is obvious that if and , then . So far, the book has defined as subsets of called cuts, and has shown that cuts respect the field axioms of addition. I am intuitively convinced that the set contains elements that does not, but am struggling with how to formalize it. The following is my thought process on my (failed) attempt. I know that: , and I think the next step is the take some value and show that while . I originally thought this would work by taking , but is a set, not a rational number. When I try to think about taking (edit: sorry, this should say not ), it's clear to me that , but it's no longer clear to me that . Any help would be greatly appreciated!","\mathbb{R} \mathbb{Q} \alpha , \beta , \gamma \in \mathbb{R} \alpha < \beta \alpha + \gamma < \beta + \gamma \alpha , \beta , \gamma \mathbb{Q} \beta + \gamma \alpha + \gamma \beta > \alpha \implies \exists x \in \beta : x \notin \alpha y \in \gamma y + x \in \beta + \gamma y + x \notin \alpha + \gamma y = \sup \gamma \sup \gamma y < \sup \gamma y \in \gamma y < \sup \gamma y + x \in \beta + \gamma y + x \notin \alpha + \gamma","['real-analysis', 'proof-writing', 'real-numbers', 'rational-numbers']"
35,Continuity of a multivariable function: doubts on how to reason,Continuity of a multivariable function: doubts on how to reason,,"I have some doubts concerning the continuity of a function in many variables. I will present a specific example, which I worked out (hopefully in a good way), and I will then ask you some questions because I'm really struggling with the ""deep"" theoretical sense of these procedures. Say I have the function $$f(x, y, z) = \begin{cases} (x + 2y + 3z)\ln(x^2 + 2y^2 + 3z^2) & (x, y, z) \neq (0, 0, 0) \\\\\\ C & (x, y, z) = (0, 0, 0)\end{cases}$$ It's asked if/when the function is continuous at the origin. First way I thought I could restrict to a particular path, like $x = y$ and $x = z$ , reducing to a one dimensional problem: $$f(x) = \begin{cases} 6x\ln(6x^2) & x \neq 0 \\\\ C & x = 0 \end{cases}$$ Here I can clearly study the limits when $x\to 0^+$ and $x\to 0^-$ and conclude that the function is continuous at the origin iff $C = 0$ . Alternatively, I can take a sequence $x_n = 1/n$ and operate that way. *Question 1: how can I be sure about this? I mean, how can I be sure that choosing some other path I wouldn't obtain something strange? This is a more general question actually, meant in general when applying the method of restriction to some path. Second way Our professor taught us the ""distance method"", that is: a function $f(x)$ is continuous at $x_0$ iff there exist a function $h(d)$ , a function of a distance $d$ (distance meant as in the Euclidean way, I guess) such that $$|f(x) - f(x_0)| \leq h(d)$$ with $h(d) \to 0$ for $d \to 0$ . This opened the abysmal world of majorizations and estimations, and I actually really need clarification from someone who got this well, in order to not sink in this oblivion. For example, using this, here is what I did. Please, read until the end (question included). $$|f(x, y, z) - \underbrace{f(0, 0, 0)}_{\text{supposed zero}}| \leq |x + 2y + 3z||\ln(x^2 + 2y^2 + 3z^2)|$$ Now I did: $|x + 2y + 3z| \leq |3x + 3y + 3z|$ and the same in the logaritm $$\leq  |3x + 3y + 3z||\ln(3x^2 + 3y^2 + 3z^2)|$$ At this point I did: $|3x + 3y + 3z| = 3(|x+y+z|) \leq 3(|x| + |y| + |z|)$ and also after that: $3(|x| + |y| + |z|) \leq 3(x^2 + y^2 + z^2)$ This allowed me to call a distance function $h(d^2) = x^2 + y^2 + z^2$ where $d^2$ would be the Euclidean distance squared ( $d^2 = x^2+y^2+z^2)$ , so in the end: $$\leq 3h(d)|\ln(3h(d))| \to 0 \qquad \text{as}\ d\to 0$$ So apparently I have managed to find such a function that, when shrunk, compress the function into it's value at the origin, which is zero. Hence again $C$ must be zero. And here I am stuck: How to really majorize? I mean things like $x < x^2$ are true when $|x| > 1$ where as the counterpart $x > x^2$ are true when $|x| < 1$ . Since I'm analyzing limits to zero, should I beware of such conditions from the beginning , or can I just think about finding a big ball $B$ , hence majorizing as if $(x, y, z) >> 0$ (large enought) and in the end shrinking letting $(x, y, z) \to (0, 0, 0)$ ? Isn't it always possible to compute such a majorization, even if a function is not continuous? How can I be sure with this method? Sorry for this poem, but THANK YOU SO MUCH to whoever will reply to this.","I have some doubts concerning the continuity of a function in many variables. I will present a specific example, which I worked out (hopefully in a good way), and I will then ask you some questions because I'm really struggling with the ""deep"" theoretical sense of these procedures. Say I have the function It's asked if/when the function is continuous at the origin. First way I thought I could restrict to a particular path, like and , reducing to a one dimensional problem: Here I can clearly study the limits when and and conclude that the function is continuous at the origin iff . Alternatively, I can take a sequence and operate that way. *Question 1: how can I be sure about this? I mean, how can I be sure that choosing some other path I wouldn't obtain something strange? This is a more general question actually, meant in general when applying the method of restriction to some path. Second way Our professor taught us the ""distance method"", that is: a function is continuous at iff there exist a function , a function of a distance (distance meant as in the Euclidean way, I guess) such that with for . This opened the abysmal world of majorizations and estimations, and I actually really need clarification from someone who got this well, in order to not sink in this oblivion. For example, using this, here is what I did. Please, read until the end (question included). Now I did: and the same in the logaritm At this point I did: and also after that: This allowed me to call a distance function where would be the Euclidean distance squared ( , so in the end: So apparently I have managed to find such a function that, when shrunk, compress the function into it's value at the origin, which is zero. Hence again must be zero. And here I am stuck: How to really majorize? I mean things like are true when where as the counterpart are true when . Since I'm analyzing limits to zero, should I beware of such conditions from the beginning , or can I just think about finding a big ball , hence majorizing as if (large enought) and in the end shrinking letting ? Isn't it always possible to compute such a majorization, even if a function is not continuous? How can I be sure with this method? Sorry for this poem, but THANK YOU SO MUCH to whoever will reply to this.","f(x, y, z) = \begin{cases} (x + 2y + 3z)\ln(x^2 + 2y^2 + 3z^2) & (x, y, z) \neq (0, 0, 0) \\\\\\ C & (x, y, z) = (0, 0, 0)\end{cases} x = y x = z f(x) = \begin{cases} 6x\ln(6x^2) & x \neq 0 \\\\ C & x = 0 \end{cases} x\to 0^+ x\to 0^- C = 0 x_n = 1/n f(x) x_0 h(d) d |f(x) - f(x_0)| \leq h(d) h(d) \to 0 d \to 0 |f(x, y, z) - \underbrace{f(0, 0, 0)}_{\text{supposed zero}}| \leq |x + 2y + 3z||\ln(x^2 + 2y^2 + 3z^2)| |x + 2y + 3z| \leq |3x + 3y + 3z| \leq  |3x + 3y + 3z||\ln(3x^2 + 3y^2 + 3z^2)| |3x + 3y + 3z| = 3(|x+y+z|) \leq 3(|x| + |y| + |z|) 3(|x| + |y| + |z|) \leq 3(x^2 + y^2 + z^2) h(d^2) = x^2 + y^2 + z^2 d^2 d^2 = x^2+y^2+z^2) \leq 3h(d)|\ln(3h(d))| \to 0 \qquad \text{as}\ d\to 0 C x < x^2 |x| > 1 x > x^2 |x| < 1 B (x, y, z) >> 0 (x, y, z) \to (0, 0, 0)","['real-analysis', 'multivariable-calculus', 'solution-verification', 'continuity', 'majorization']"
36,Possible closed form of $\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z$,Possible closed form of,\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z,"Reviewing the link , I consider to evaluate $$ \int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z. $$ And I quickly discover $$ \int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z\\ =\frac{9\sqrt{2}\pi }{4}\int_{\sqrt{\frac53}}^{\sqrt{2} }  \frac{\arctan(x)}{\left ( 2x^2-3 \right )\sqrt{3x^2-5}  }\text{d}x -\frac{\pi^3}{\sqrt{6} }+\frac{3\pi^2}{2\sqrt{6} }\arctan(2\sqrt{6} ). $$ I don't know whether this is helpful or not. But I instinctively know it has a sufficient simple result, which only appears $\pi,\arctan$ and some quadratic irrationals. Hopefully you are glad to reach for my hand.","Reviewing the link , I consider to evaluate And I quickly discover I don't know whether this is helpful or not. But I instinctively know it has a sufficient simple result, which only appears and some quadratic irrationals. Hopefully you are glad to reach for my hand.","
\int_{0}^{1} \int_{0}^{1} \int_{0}^{1}
\frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z.
 
\int_{0}^{1} \int_{0}^{1} \int_{0}^{1}
\frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z\\
=\frac{9\sqrt{2}\pi }{4}\int_{\sqrt{\frac53}}^{\sqrt{2} } 
\frac{\arctan(x)}{\left ( 2x^2-3 \right )\sqrt{3x^2-5}  }\text{d}x
-\frac{\pi^3}{\sqrt{6} }+\frac{3\pi^2}{2\sqrt{6} }\arctan(2\sqrt{6} ).
 \pi,\arctan","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'multiple-integral']"
37,Convergence of $ \sum_{n=1}^{\infty} \frac{\cos (2n+\sqrt{n})}{\sqrt{n}} $,Convergence of, \sum_{n=1}^{\infty} \frac{\cos (2n+\sqrt{n})}{\sqrt{n}} ,"As you undestand from the topic title, I am wondering how to determine whether the series $$ \sum_{n=1}^{\infty} \frac{\cos (2n+\sqrt{n})}{\sqrt{n}}  $$ converges or not. Previously there have been many similar problems and I tried them all did not work. [Link1]:( Convergence of $\sum \frac{ \cos\sqrt n} {\sqrt n}$ ) Estimating the number of n to identify a Cauchy lower bound is a good approach, but  when inside the $\cos$ is $2n+\sqrt{n}$ , the range of $n$ is not as much as expected failed to estimate the lower boundary. [Link2]:( Does $\sum_{n=1}^\infty \frac{\cos{(\sqrt{n})}}{n}$ converge? )I followed the idea of Robert Z's answer. Although its corresponding integral is convergent, the summation does not converge due to the 1st order term in the first step of estimating $|f(n)-f(x)|$ . Any help may yield help and look forward to your reply.","As you undestand from the topic title, I am wondering how to determine whether the series converges or not. Previously there have been many similar problems and I tried them all did not work. [Link1]:( Convergence of $\sum \frac{ \cos\sqrt n} {\sqrt n}$ ) Estimating the number of n to identify a Cauchy lower bound is a good approach, but  when inside the is , the range of is not as much as expected failed to estimate the lower boundary. [Link2]:( Does $\sum_{n=1}^\infty \frac{\cos{(\sqrt{n})}}{n}$ converge? )I followed the idea of Robert Z's answer. Although its corresponding integral is convergent, the summation does not converge due to the 1st order term in the first step of estimating . Any help may yield help and look forward to your reply.", \sum_{n=1}^{\infty} \frac{\cos (2n+\sqrt{n})}{\sqrt{n}}   \cos 2n+\sqrt{n} n |f(n)-f(x)|,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
38,Show that there exists a $c : f(c) = g(c)$,Show that there exists a,c : f(c) = g(c),"I'll try to present a solution for this problem, and I hope I can receive feedback on what went wrong, if something went wrong of course. Let $f, g : [a, b] \to \Bbb R$ be continuous functions and $\int_{a}^{b} f(x) dx = \int_{a}^{b} g(x) dx$ . Show that there exists $c \in [a, b]$ such that $f(c) = g(c)$ . Solution Let's define $h(x) = \int_{a}^{x}f(x)dx-\int_{a}^{x}g(x)dx$ $h(x)$ is continous, since $f(x)$ and $g(x)$ is continous. I hope this argument is correct. We see $h(a) = h(b) = 0$ . Applying Rolle's Theorem, we get that $\exists \xi \in (a,b) : h'(\xi) = 0$ In other terms, $f(\xi) = g(\xi)$ $\square$ Thanks!","I'll try to present a solution for this problem, and I hope I can receive feedback on what went wrong, if something went wrong of course. Let be continuous functions and . Show that there exists such that . Solution Let's define is continous, since and is continous. I hope this argument is correct. We see . Applying Rolle's Theorem, we get that In other terms, Thanks!","f, g : [a, b] \to \Bbb R \int_{a}^{b} f(x) dx = \int_{a}^{b} g(x) dx c \in [a, b] f(c) = g(c) h(x) = \int_{a}^{x}f(x)dx-\int_{a}^{x}g(x)dx h(x) f(x) g(x) h(a) = h(b) = 0 \exists \xi \in (a,b) : h'(\xi) = 0 f(\xi) = g(\xi) \square",['real-analysis']
39,Convergence of a rearrangement of a series,Convergence of a rearrangement of a series,,"Let $\sum a_n$ be a convergent series and $f$ is a bijection on $\mathbb{N}$ . Suppose $(f(n) -n)$ is a bounded sequence the rearrangement series $\sum a_{f(n)}$ converges to same limit. Suppose $m_n=\sup\{|a_k| : k> n\}$ . If the sequence $(m_n |f(n) -n|) $ converges to $0$ , then the rearrangement series converges to same limit. Since $(f(n) -n)$ is bounded sequence, the terms can be grouped in some way such that the limit does not change. But how it can be done. $2$ seems consequence of $1$ . How to do it?","Let be a convergent series and is a bijection on . Suppose is a bounded sequence the rearrangement series converges to same limit. Suppose . If the sequence converges to , then the rearrangement series converges to same limit. Since is bounded sequence, the terms can be grouped in some way such that the limit does not change. But how it can be done. seems consequence of . How to do it?",\sum a_n f \mathbb{N} (f(n) -n) \sum a_{f(n)} m_n=\sup\{|a_k| : k> n\} (m_n |f(n) -n|)  0 (f(n) -n) 2 1,"['real-analysis', 'sequences-and-series']"
40,Extreme points of the unit ball of a Banach space,Extreme points of the unit ball of a Banach space,,"Let $X$ be a Banach space and let $B_X$ denote the closed unit ball of $X$ . Let $\mathrm{Ext}(B_X)$ denote the set of all extreme points of the unit ball $B_X$ . Whenever $X$ is finite-dimensional, it is known that $\mathrm{Ext}(B_X)\neq \emptyset.$ Are there instances, where $\mathrm{Ext}(B_X)=\emptyset$ ? Does completeness plays any role here? Is it true that we will always have $\mathrm{Ext}(B_X)\neq \emptyset$ for $X$ is Banach and $\mathrm{Ext}(B_X)$ may be empty in case of $X$ is not complete?","Let be a Banach space and let denote the closed unit ball of . Let denote the set of all extreme points of the unit ball . Whenever is finite-dimensional, it is known that Are there instances, where ? Does completeness plays any role here? Is it true that we will always have for is Banach and may be empty in case of is not complete?",X B_X X \mathrm{Ext}(B_X) B_X X \mathrm{Ext}(B_X)\neq \emptyset. \mathrm{Ext}(B_X)=\emptyset \mathrm{Ext}(B_X)\neq \emptyset X \mathrm{Ext}(B_X) X,"['real-analysis', 'functional-analysis', 'convex-analysis', 'banach-spaces', 'normed-spaces']"
41,"If $f''(x) \le 0, \, \forall x \neq 0$ and $f$ is minimum at $0$, prove $f'(0)$ doesn't exist.","If  and  is minimum at , prove  doesn't exist.","f''(x) \le 0, \, \forall x \neq 0 f 0 f'(0)","A classic example of a function being differentiable everywhere except at one point is the function $f(x) = \sqrt{\lvert x \rvert}$ . This function is not differentiable at $x=0$ as is shown in this answer . I was thinking about what was the required behavior for a function to have around $x=0$ for the derivative to be nonexistent. I believe the requirements are: The function needs to be continuous, since otherwise differentiability $\implies$ continuity would prove the statement is trivially true. The function has to be concave for all $x \neq 0$ . The function needs to be minimum at the point $x=0$ . To me, this intuitively seems to guarantee the ""pointiness"" at $x=0$ that makes the function non-differentiable. Inspired by the above I wanted to generalize the result. I propose the following theorem: Given a continous function $f:\mathbb{R} \to \mathbb{R}$ , if $f''(x) \le 0 $ on some interval $0<|x|< a$ , for some $a\in (0,\infty]$ , and $f$ is minimum at $0$ , then $f'(0)$ doesn't exist. Here is my attempt at proving the statement: We analyze the limit $\lim_{h \to 0^+}$ and $\lim_{h \to 0^-}$ separately. We see that $\lim_{h \to 0^+} \frac{f(0 + h) - f(0)}{h} > 0$ since $h>0$ and $f(h)-f(0)> 0$ using that the function is minimum at $0$ . Similarly, $\lim_{h \to 0^-} \frac{f(0 + h) - f(0)}{h} < 0$ since $h<0$ and $f(h)-f(0)> 0$ using that the function is minimum at $0$ . So since the limit from the right is positive, but the limit from the left is negative, then the limit doesn't exist. QED. This attempt troubles me because I didn't use the condition that the function has to be concave for all $x \neq 0$ , so if the proof were correct this would mean that the theorem would also hold for functions like $x^2$ which are also minimum at $x=0$ , but this is clearly wrong! I can't seem to find exactly what's wrong with the argument, but because of the lack of my use of the hypothesis I know it is indeed wrong. Can anyone tell me how I could correct my proof to make it valid? Edit: The case where $f''(x) =0$ has a family of counterexamples as Theo Bendit pointed out in his answer. However, the question was inspired by functions that have similar behavior to $f(x) = \sqrt{\lvert x \rvert}$ , so any suggestions on how to prove the statement with $f''(x) <0$ instead of $f''(x) \le 0$ are greatly appreciated.","A classic example of a function being differentiable everywhere except at one point is the function . This function is not differentiable at as is shown in this answer . I was thinking about what was the required behavior for a function to have around for the derivative to be nonexistent. I believe the requirements are: The function needs to be continuous, since otherwise differentiability continuity would prove the statement is trivially true. The function has to be concave for all . The function needs to be minimum at the point . To me, this intuitively seems to guarantee the ""pointiness"" at that makes the function non-differentiable. Inspired by the above I wanted to generalize the result. I propose the following theorem: Given a continous function , if on some interval , for some , and is minimum at , then doesn't exist. Here is my attempt at proving the statement: We analyze the limit and separately. We see that since and using that the function is minimum at . Similarly, since and using that the function is minimum at . So since the limit from the right is positive, but the limit from the left is negative, then the limit doesn't exist. QED. This attempt troubles me because I didn't use the condition that the function has to be concave for all , so if the proof were correct this would mean that the theorem would also hold for functions like which are also minimum at , but this is clearly wrong! I can't seem to find exactly what's wrong with the argument, but because of the lack of my use of the hypothesis I know it is indeed wrong. Can anyone tell me how I could correct my proof to make it valid? Edit: The case where has a family of counterexamples as Theo Bendit pointed out in his answer. However, the question was inspired by functions that have similar behavior to , so any suggestions on how to prove the statement with instead of are greatly appreciated.","f(x) = \sqrt{\lvert x \rvert} x=0 x=0 \implies x \neq 0 x=0 x=0 f:\mathbb{R} \to \mathbb{R} f''(x) \le 0  0<|x|< a a\in (0,\infty] f 0 f'(0) \lim_{h \to 0^+} \lim_{h \to 0^-} \lim_{h \to 0^+} \frac{f(0 + h) - f(0)}{h} > 0 h>0 f(h)-f(0)> 0 0 \lim_{h \to 0^-} \frac{f(0 + h) - f(0)}{h} < 0 h<0 f(h)-f(0)> 0 0 x \neq 0 x^2 x=0 f''(x) =0 f(x) = \sqrt{\lvert x \rvert} f''(x) <0 f''(x) \le 0","['real-analysis', 'calculus', 'derivatives', 'solution-verification', 'alternative-proof']"
42,Is $\prod_{m=1}^{\infty}\frac{\Gamma(2mx+x+1)}{(2mx)^{2x+1}\Gamma(2mx-x)}=\frac{2x^{x+\frac 12}}{\pi^x\ \Gamma(x+1)}$ also for $x\notin\mathbb Z^+$?,Is  also for ?,\prod_{m=1}^{\infty}\frac{\Gamma(2mx+x+1)}{(2mx)^{2x+1}\Gamma(2mx-x)}=\frac{2x^{x+\frac 12}}{\pi^x\ \Gamma(x+1)} x\notin\mathbb Z^+,"By calling upon the roots of unity, it can be proved that for $n\in\mathbb Z^+$ , $$\prod_{k=1}^{n-1} \sin\frac{k\pi}{2n}= \frac{\sqrt n}{2^{n-1}} $$ (see, for example, this .) Using the infinite product representation for $\sin x$ , the LHS becomes $$\prod_{k=1}^n \frac{k\pi}{2n}\prod_{m=1}^{\infty}\left( 1-\frac{k^2}{4m^2n^2}\right)\\ = n!\left(\frac{\pi}{2n} \right)^n \prod_{m=1}^{\infty}\prod_{k=1}^n \frac{(2mn-k)(2mn+k)}{4m^2n^2}\\ = n!\left(\frac{\pi}{2n}\right)^n \prod_{m=1}^{\infty}\frac{\Gamma(2mn+n+1)}{(2mn)^{2n+1}\Gamma(2mn-n)} $$ and therefore $$  \prod_{m=1}^{\infty}\frac{\Gamma(2mx+x+1)}{(2mx)^{2x+1}\Gamma(2mx-x)}= \frac{2x^{x+\frac 12}}{\pi^x \ \Gamma(x+1)} $$ for $x\in\mathbb Z^+$ . Naturally, I wondered whether this result also holds for other $x\notin\mathbb Z^+$ , and indeed on checking I saw that this is true for $x=\frac 12, \frac 32, \frac 13$ or even $x=\pi$ , so I suspect that it holds atleast for all $x\gt 0$ . How can this be proven/disproven?","By calling upon the roots of unity, it can be proved that for , (see, for example, this .) Using the infinite product representation for , the LHS becomes and therefore for . Naturally, I wondered whether this result also holds for other , and indeed on checking I saw that this is true for or even , so I suspect that it holds atleast for all . How can this be proven/disproven?","n\in\mathbb Z^+ \prod_{k=1}^{n-1} \sin\frac{k\pi}{2n}= \frac{\sqrt n}{2^{n-1}}  \sin x \prod_{k=1}^n \frac{k\pi}{2n}\prod_{m=1}^{\infty}\left( 1-\frac{k^2}{4m^2n^2}\right)\\ = n!\left(\frac{\pi}{2n} \right)^n \prod_{m=1}^{\infty}\prod_{k=1}^n \frac{(2mn-k)(2mn+k)}{4m^2n^2}\\ = n!\left(\frac{\pi}{2n}\right)^n \prod_{m=1}^{\infty}\frac{\Gamma(2mn+n+1)}{(2mn)^{2n+1}\Gamma(2mn-n)}    \prod_{m=1}^{\infty}\frac{\Gamma(2mx+x+1)}{(2mx)^{2x+1}\Gamma(2mx-x)}= \frac{2x^{x+\frac 12}}{\pi^x \ \Gamma(x+1)}  x\in\mathbb Z^+ x\notin\mathbb Z^+ x=\frac 12, \frac 32, \frac 13 x=\pi x\gt 0","['real-analysis', 'trigonometry', 'gamma-function', 'infinite-product']"
43,Help with $ \lim \frac{n\pi}{4} - \left( \frac{n^2}{n^2+1^2} +\frac{n^2}{n^2+2^2} + \cdots + \frac{n^2}{n^2+n^2} \right) $,Help with, \lim \frac{n\pi}{4} - \left( \frac{n^2}{n^2+1^2} +\frac{n^2}{n^2+2^2} + \cdots + \frac{n^2}{n^2+n^2} \right) ,"I have proved that $$\lim_{n\to \infty} \left( \frac{n}{n^2+1^2} +\frac{n}{n^2+2^2} + \cdots + \frac{n}{n^2+n^2}  \right)  = \frac{\pi}{4},$$ by using the Riemann's sum of $\arctan$ on $[0,1]$ . Now I'm interested in computing the next limit $$ L=\lim_{n\to \infty} \left[ \frac{n\pi}{4} - \left( \frac{n^2}{n^2+1^2} +\frac{n^2}{n^2+2^2} + \cdots + \frac{n^2}{n^2+n^2}  \right) \right] $$ Note that $$ L= \lim n  \left[ \frac{\pi}{4} - \left( \frac{n}{n^2+1^2} +\frac{n}{n^2+2^2} + \cdots + \frac{n}{n^2+n^2}  \right) \right] =[\infty \cdot 0].$$ I can't manage to calculate $L$ . How can this indeterminate form be solved? Also, by using computer, I suspect that $L$ exists and is equal to $1/4$ . Any idea or hint is welcome! (Actually, I dont need a complete solution, just a good start point). Have a nice day you all!","I have proved that by using the Riemann's sum of on . Now I'm interested in computing the next limit Note that I can't manage to calculate . How can this indeterminate form be solved? Also, by using computer, I suspect that exists and is equal to . Any idea or hint is welcome! (Actually, I dont need a complete solution, just a good start point). Have a nice day you all!","\lim_{n\to \infty} \left( \frac{n}{n^2+1^2} +\frac{n}{n^2+2^2} + \cdots + \frac{n}{n^2+n^2}  \right)  = \frac{\pi}{4}, \arctan [0,1]  L=\lim_{n\to \infty} \left[ \frac{n\pi}{4} - \left( \frac{n^2}{n^2+1^2} +\frac{n^2}{n^2+2^2} + \cdots + \frac{n^2}{n^2+n^2}  \right) \right]   L= \lim n  \left[ \frac{\pi}{4} - \left( \frac{n}{n^2+1^2} +\frac{n}{n^2+2^2} + \cdots + \frac{n}{n^2+n^2}  \right) \right] =[\infty \cdot 0]. L L 1/4","['real-analysis', 'limits', 'riemann-sum']"
44,Is this Integration From First Principles notation standard outside of A Level Maths?,Is this Integration From First Principles notation standard outside of A Level Maths?,,"I teach A Level Maths in the UK. We are required to do some 'introduction' to integral from first principles as part of the specification ( link , page 25 is the interesting part). In a previous exam question (Paper 2, June 2018), this was essentially the question: Suppose we have the curve $y= \sqrt{x}$ The point $P(x,y)$ lies on the curve. Consider a rectangle with height $y$ and width $\delta x$ . Calculate $\displaystyle \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \,\delta x$ The answer involves us recognising $$ \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \, \delta x = \int_4^9 \sqrt{x} \, dx$$ and evaluating the integral. Is this notation standard? To me, it doesn't make sense. How can you have $x=4$ to $9$ as the limits on the sum, for example? A sum only works over integral values. In addition, one could easily give meaning to the limit as $\displaystyle \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \, \delta x = \lim_{\delta x \rightarrow 0}( \sqrt{4}(\delta x) + \sqrt{5} (\delta x) + \cdots + \sqrt{9} (\delta x))$ which should be $0$ as $\delta x \rightarrow 0$ . The reason I am so confused is that this question and notation appears on a Pearson A Level Maths Exam paper. It is a regulated qualification. There are very qualified people out there that have deemed this to make sense and be used to assess the understanding of thousands of A Level students in the UK.","I teach A Level Maths in the UK. We are required to do some 'introduction' to integral from first principles as part of the specification ( link , page 25 is the interesting part). In a previous exam question (Paper 2, June 2018), this was essentially the question: Suppose we have the curve The point lies on the curve. Consider a rectangle with height and width . Calculate The answer involves us recognising and evaluating the integral. Is this notation standard? To me, it doesn't make sense. How can you have to as the limits on the sum, for example? A sum only works over integral values. In addition, one could easily give meaning to the limit as which should be as . The reason I am so confused is that this question and notation appears on a Pearson A Level Maths Exam paper. It is a regulated qualification. There are very qualified people out there that have deemed this to make sense and be used to assess the understanding of thousands of A Level students in the UK.","y= \sqrt{x} P(x,y) y \delta x \displaystyle \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \,\delta x  \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \, \delta x = \int_4^9 \sqrt{x} \, dx x=4 9 \displaystyle \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \, \delta x = \lim_{\delta x \rightarrow 0}( \sqrt{4}(\delta x) + \sqrt{5} (\delta x) + \cdots + \sqrt{9} (\delta x)) 0 \delta x \rightarrow 0","['real-analysis', 'notation', 'riemann-integration', 'riemann-sum']"
45,Writing Zeta Function In Terms Of The J-Function,Writing Zeta Function In Terms Of The J-Function,,"I am reading through John Derbyshire's ""Prime Obsession"" and am struggling to understand his argument for why $\frac{1}{s} \log{\zeta(s)}=\int_{0}^{\infty} J(x)x^{-s-1}dx$ where $J(x)$ is defined as $\pi(x)+\frac{1}{2}\pi(\sqrt{x})+\frac{1}{3}\pi(\sqrt[3]{x})+\frac{1}{4}\pi(\sqrt[4]{x})+\frac{1}{5}\pi(\sqrt[5]{x})+...$ Here is what I am getting so far: I know $\zeta(s)={ \prod_{p} \left(1-p^{-s}\right)^{-1}}$ . Taking the logarithm, $\log\left(\zeta(s)\right)=-\log(1-\frac{1}{2^s})-\log(1-\frac{1}{3^s})-\log(1-\frac{1}{5^s})+...$ Recall $S=\sum_{k=0}^{n-1}a\cdot r^k=\frac{1}{1-r}$ whenever $a=1$ and $r\in(-1,1)$ . Taking the integral, we have $\int{\frac{1}{1-r}}=\int{1+r+r^2+r^3+...}$ , and $-\log(1-r)=r+\frac{r^2}{2}+\frac{r^3}{3}+\frac{r^4}{4}+...$ . Then since $0 < \lvert \frac{1}{p^s} \rvert<1$ , we can write each term in Euler's product formula as an infinite sum. For example, $-\log(1-\frac{1}{2^s})=\frac{1}{2^s}+\left(\frac{1}{2}\cdot\left(\frac{1}{2^s}\right)^2\right)+\left(\frac{1}{3}\cdot\left(\frac{1}{2^s}\right)^3\right)+\left(\frac{1}{4}\cdot\left(\frac{1}{2^s}\right)^4\right)\dots$ Any term in this infinite sum of infinite sums can be written as an integral. For example, $\left(\frac{1}{3}\cdot\left(\frac{1}{2^s}\right)^3\right)=\frac{1}{3}\times\frac{1}{2^{3s}}=\frac{1}{3}\cdot{s}\cdot \int_{2^3}^{\infty}x^{-s-1}\: dx$ since $\int_{2^3}^{\infty} x^{-s-1}dx=\left(\frac{1}{s}\cdot\frac{-1}{x^s}\right)\biggr\rvert_{8}^{\infty}=\left(0\right)-\left(\frac{1}{s}\cdot\frac{-1}{8^s}\right)=\frac{1}{s}\times\frac{1}{8^s}$ which is precisely $\frac{s}{3}$ multiples of $\frac{1}{3}\times\frac{1}{2^{3s}}$ . This is where I am not following. Derbyshire says that this specific term forms a ""strip"" under the J-Function. Even though the J-Function is a step function, if you think of the integral as area under the curve, the example in the previous step should not be rectangular. Another point that I don't understand is why $\int_{0}^{\infty} J(x)x^{-s-1}dx=\left[\int_{2}^{\infty} \left(\frac{1}{1}\cdot x^{-s-1} dx\right)+\int_{2^2}^{\infty} \left(\frac{1}{2}\cdot x^{-s-1} dx\right)+\int_{2^3}^{\infty} \left(\frac{1}{3}\cdot x^{-s-1} dx\right)+...\right]+\left[\int_{3}^{\infty} \left(\frac{1}{1}\cdot x^{-s-1} dx\right)+\int_{3^2}^{\infty} \left(\frac{1}{2}\cdot x^{-s-1} dx\right)+\int_{3^3}^{\infty} \left(\frac{1}{3}\cdot x^{-s-1} dx\right)+...\right]+\left[\int_{5}^{\infty} \left(\frac{1}{1}\cdot x^{-s-1} dx\right)+\int_{5^2}^{\infty} \left(\frac{1}{2}\cdot x^{-s-1} dx\right)+\int_{5^3}^{\infty} \left(\frac{1}{3}\cdot x^{-s-1} dx\right)+...\right]+...$ . Any insights into this problem?","I am reading through John Derbyshire's ""Prime Obsession"" and am struggling to understand his argument for why where is defined as Here is what I am getting so far: I know . Taking the logarithm, Recall whenever and . Taking the integral, we have , and . Then since , we can write each term in Euler's product formula as an infinite sum. For example, Any term in this infinite sum of infinite sums can be written as an integral. For example, since which is precisely multiples of . This is where I am not following. Derbyshire says that this specific term forms a ""strip"" under the J-Function. Even though the J-Function is a step function, if you think of the integral as area under the curve, the example in the previous step should not be rectangular. Another point that I don't understand is why . Any insights into this problem?","\frac{1}{s} \log{\zeta(s)}=\int_{0}^{\infty} J(x)x^{-s-1}dx J(x) \pi(x)+\frac{1}{2}\pi(\sqrt{x})+\frac{1}{3}\pi(\sqrt[3]{x})+\frac{1}{4}\pi(\sqrt[4]{x})+\frac{1}{5}\pi(\sqrt[5]{x})+... \zeta(s)={ \prod_{p} \left(1-p^{-s}\right)^{-1}} \log\left(\zeta(s)\right)=-\log(1-\frac{1}{2^s})-\log(1-\frac{1}{3^s})-\log(1-\frac{1}{5^s})+... S=\sum_{k=0}^{n-1}a\cdot r^k=\frac{1}{1-r} a=1 r\in(-1,1) \int{\frac{1}{1-r}}=\int{1+r+r^2+r^3+...} -\log(1-r)=r+\frac{r^2}{2}+\frac{r^3}{3}+\frac{r^4}{4}+... 0 < \lvert \frac{1}{p^s} \rvert<1 -\log(1-\frac{1}{2^s})=\frac{1}{2^s}+\left(\frac{1}{2}\cdot\left(\frac{1}{2^s}\right)^2\right)+\left(\frac{1}{3}\cdot\left(\frac{1}{2^s}\right)^3\right)+\left(\frac{1}{4}\cdot\left(\frac{1}{2^s}\right)^4\right)\dots \left(\frac{1}{3}\cdot\left(\frac{1}{2^s}\right)^3\right)=\frac{1}{3}\times\frac{1}{2^{3s}}=\frac{1}{3}\cdot{s}\cdot \int_{2^3}^{\infty}x^{-s-1}\: dx \int_{2^3}^{\infty} x^{-s-1}dx=\left(\frac{1}{s}\cdot\frac{-1}{x^s}\right)\biggr\rvert_{8}^{\infty}=\left(0\right)-\left(\frac{1}{s}\cdot\frac{-1}{8^s}\right)=\frac{1}{s}\times\frac{1}{8^s} \frac{s}{3} \frac{1}{3}\times\frac{1}{2^{3s}} \int_{0}^{\infty} J(x)x^{-s-1}dx=\left[\int_{2}^{\infty} \left(\frac{1}{1}\cdot x^{-s-1} dx\right)+\int_{2^2}^{\infty} \left(\frac{1}{2}\cdot x^{-s-1} dx\right)+\int_{2^3}^{\infty} \left(\frac{1}{3}\cdot x^{-s-1} dx\right)+...\right]+\left[\int_{3}^{\infty} \left(\frac{1}{1}\cdot x^{-s-1} dx\right)+\int_{3^2}^{\infty} \left(\frac{1}{2}\cdot x^{-s-1} dx\right)+\int_{3^3}^{\infty} \left(\frac{1}{3}\cdot x^{-s-1} dx\right)+...\right]+\left[\int_{5}^{\infty} \left(\frac{1}{1}\cdot x^{-s-1} dx\right)+\int_{5^2}^{\infty} \left(\frac{1}{2}\cdot x^{-s-1} dx\right)+\int_{5^3}^{\infty} \left(\frac{1}{3}\cdot x^{-s-1} dx\right)+...\right]+...","['real-analysis', 'number-theory', 'riemann-zeta']"
46,"If $T:(\mathbb{R}^2,\|\cdot\|_p) \to (\mathbb{R}^2,\|\cdot\|_q)$ is an onto linear isometry, then must it be $p=q$?","If  is an onto linear isometry, then must it be ?","T:(\mathbb{R}^2,\|\cdot\|_p) \to (\mathbb{R}^2,\|\cdot\|_q) p=q","Question: Let $p,q\in [1,\infty)$ and suppose that that $T:(\mathbb{R}^2,\|\cdot\|_p) \to (\mathbb{R}^2,\|\cdot\|_q)$ is an onto linear isometry. Must it be $p=q$ ? I think it is true as isometry preserves extreme points. However, it would be good if there is an elementary arguments.","Question: Let and suppose that that is an onto linear isometry. Must it be ? I think it is true as isometry preserves extreme points. However, it would be good if there is an elementary arguments.","p,q\in [1,\infty) T:(\mathbb{R}^2,\|\cdot\|_p) \to (\mathbb{R}^2,\|\cdot\|_q) p=q","['real-analysis', 'functional-analysis', 'lp-spaces', 'isometry']"
47,Why do additional Taylor terms lead to an improved approximation of a function?,Why do additional Taylor terms lead to an improved approximation of a function?,,"I hope my question becomes clear: I understand what a Taylor polynomial does. It approximates an analytical function in the point $x=a$ in a way that the n'th order Taylor polynomial matches the function up to its n'th derivative in the point $x=a$ . Something I could never answer myself though is: Why does each additional Taylor term improve the approximation of the function in the vicinity of the point $x=a$ ? Can't it be that a Taylor polynomial provides a worse approximation in the vicinity of $x=a$ when taking more Taylor terms (i.e. choosing a later truncation for $n$ )? If this is the case: Is it also safe to say that truncating a Taylor series at larger $n$ also improves the approximation of $f(x)$ for points far away from $x=a$ ? And a last question: Why is an analytic function $f(x)$ almost always equal to the Taylor series of the form $f(x)=\sum_{n=0}^{\infty} \frac{f^{(n)}}{n!} (x-a)!$ Are there function where this is not the case? And is there a clear cut proof, why an infinite Taylor series equals an analytic function in general?","I hope my question becomes clear: I understand what a Taylor polynomial does. It approximates an analytical function in the point in a way that the n'th order Taylor polynomial matches the function up to its n'th derivative in the point . Something I could never answer myself though is: Why does each additional Taylor term improve the approximation of the function in the vicinity of the point ? Can't it be that a Taylor polynomial provides a worse approximation in the vicinity of when taking more Taylor terms (i.e. choosing a later truncation for )? If this is the case: Is it also safe to say that truncating a Taylor series at larger also improves the approximation of for points far away from ? And a last question: Why is an analytic function almost always equal to the Taylor series of the form Are there function where this is not the case? And is there a clear cut proof, why an infinite Taylor series equals an analytic function in general?",x=a x=a x=a x=a n n f(x) x=a f(x) f(x)=\sum_{n=0}^{\infty} \frac{f^{(n)}}{n!} (x-a)!,"['real-analysis', 'taylor-expansion', 'approximation']"
48,Can Abel's test and Dirichlet test be used interchangably?,Can Abel's test and Dirichlet test be used interchangably?,,I have seen in most cases in series of constants and in series of functions that where Abel's test of convergence applies Dirichlet's test also applies and vice versa. This makes me to think whether the two tests are equivalent in the sense that  one is derivable from other.Is it true? Or there are cases where we can apply one but cannot apply other. Also is it so that one is more general and other is a corollary out of it.,I have seen in most cases in series of constants and in series of functions that where Abel's test of convergence applies Dirichlet's test also applies and vice versa. This makes me to think whether the two tests are equivalent in the sense that  one is derivable from other.Is it true? Or there are cases where we can apply one but cannot apply other. Also is it so that one is more general and other is a corollary out of it.,,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'metric-spaces', 'uniform-convergence']"
49,"$\{a_n\}$ be a sequence such that $ a_{n+1}^2-2a_na_{n+1}-a_n=0$, then $\sum_1^{\infty}\frac{a_n}{3^n}$ lies in...","be a sequence such that , then  lies in...",\{a_n\}  a_{n+1}^2-2a_na_{n+1}-a_n=0 \sum_1^{\infty}\frac{a_n}{3^n},"Let $\{a_n\}$ be a sequence of positive real numbers such that $a_1 =1,\ \  a_{n+1}^2-2a_na_{n+1}-a_n=0, \ \ \forall n\geq 1$ . Then the sum of the series $\sum_1^{\infty}\frac{a_n}{3^n}$ lies in... (A) $(1,2]$ , (B) $(2,3]$ , (C) $(3,4]$ , (D) $(4,5]$ . Solution attempt: Firstly, we figure out what $\frac{a_{n+1}}{a_n}$ is going to look like. We get, from the recursive formula, $\frac{a_{n+1}}{a_n}=1+\sqrt{1+\frac{1}{a_n^2}}$ (remembering the fact that $a_n>0$ , the other root is rejected). We know that, if $\lim_{n \to \infty}\frac{a_{n+1}}{a_n}>1$ , then $\lim a_n \to \infty$ . Further, $(a_{n+1}-a_n)= \sqrt{a_n(a_n+1)}>0$ . (Again, the other root is rejected due to the same reason). Hence, $(a_n)$ increases monotonically. Therefore, the largest value of $\frac{a_{n+1}}{a_n}$ is approximately $1+\sqrt{1+\frac{1}{1}} \approx 2.15$ Now, the sum can be approximated as $\displaystyle\frac{\frac{1}{3}}{1-\frac{2.15}{3}} \approx 1.3$ (In actuality, $\mathbb{sum}< 1.3$ ). So, option $(A)$ is the correct answer. Is the procedure correct? I have been noticing a handful of this type of questions (based on approximations) lately, and the goal is to find out where the sum / the limit of the sequence might lie. Is there any ""definitive"" approach that exploits the recursive formula and gives us the value, or does the approach varies from problem to problem?","Let be a sequence of positive real numbers such that . Then the sum of the series lies in... (A) , (B) , (C) , (D) . Solution attempt: Firstly, we figure out what is going to look like. We get, from the recursive formula, (remembering the fact that , the other root is rejected). We know that, if , then . Further, . (Again, the other root is rejected due to the same reason). Hence, increases monotonically. Therefore, the largest value of is approximately Now, the sum can be approximated as (In actuality, ). So, option is the correct answer. Is the procedure correct? I have been noticing a handful of this type of questions (based on approximations) lately, and the goal is to find out where the sum / the limit of the sequence might lie. Is there any ""definitive"" approach that exploits the recursive formula and gives us the value, or does the approach varies from problem to problem?","\{a_n\} a_1 =1,\ \  a_{n+1}^2-2a_na_{n+1}-a_n=0, \ \ \forall n\geq 1 \sum_1^{\infty}\frac{a_n}{3^n} (1,2] (2,3] (3,4] (4,5] \frac{a_{n+1}}{a_n} \frac{a_{n+1}}{a_n}=1+\sqrt{1+\frac{1}{a_n^2}} a_n>0 \lim_{n \to \infty}\frac{a_{n+1}}{a_n}>1 \lim a_n \to \infty (a_{n+1}-a_n)= \sqrt{a_n(a_n+1)}>0 (a_n) \frac{a_{n+1}}{a_n} 1+\sqrt{1+\frac{1}{1}} \approx 2.15 \displaystyle\frac{\frac{1}{3}}{1-\frac{2.15}{3}} \approx 1.3 \mathbb{sum}< 1.3 (A)","['real-analysis', 'sequences-and-series', 'proof-verification', 'convergence-divergence', 'approximation']"
50,A multi variable function that satisfies 3 conditions,A multi variable function that satisfies 3 conditions,,"Let $f(x,y)$ be multi variable function that is defined when $\frac{x}{2}<y <x $ , and I want to know if there is such a function that satisfies : 1 ) $f(x-1,y) > f(x,y)$ 2 ) $f(x-1,y-\ln x) > f(x,y)(1-\frac{1}{x})$ 3 ) $|f(x,y) - \ln x| < \frac{1}{\sqrt{x}}$ when $|y- x| < 3\ln x$ , this condition can be loosened to say $\frac{1}{x^{\frac{1}{3}}}$ or more (but for a fixed constant power). I think such function do not exist, I just don't have a proof. Please give a proof that such function does not exist or an example for such a function Edit : I am interested as $x \to \infty$ how the function behave, also i am assuming that $f(x-1,y),f(x-1,y-\ln x),f(x,y)$ are all defined. For example the function $f(x,y) = \ln (x) +1-\frac{x}{y}$ does satisfy the conditions $1,3$ but does not satisfy $2$ .","Let be multi variable function that is defined when , and I want to know if there is such a function that satisfies : 1 ) 2 ) 3 ) when , this condition can be loosened to say or more (but for a fixed constant power). I think such function do not exist, I just don't have a proof. Please give a proof that such function does not exist or an example for such a function Edit : I am interested as how the function behave, also i am assuming that are all defined. For example the function does satisfy the conditions but does not satisfy .","f(x,y) \frac{x}{2}<y <x  f(x-1,y) > f(x,y) f(x-1,y-\ln x) > f(x,y)(1-\frac{1}{x}) |f(x,y) - \ln x| < \frac{1}{\sqrt{x}} |y- x| < 3\ln x \frac{1}{x^{\frac{1}{3}}} x \to \infty f(x-1,y),f(x-1,y-\ln x),f(x,y) f(x,y) = \ln (x) +1-\frac{x}{y} 1,3 2",['real-analysis']
51,Find $\displaystyle \min_{f \in \mathcal{A}} \int_{0}^{1} (1+x^{2})(f(x))^{2} dx $,Find,\displaystyle \min_{f \in \mathcal{A}} \int_{0}^{1} (1+x^{2})(f(x))^{2} dx ,"Find $$ \min_{f \in \mathcal{A}} \int_{0}^{1} (1+x^{2})(f(x))^{2} dx $$ where $$ \mathcal{A}=\left\{ f \in C[a,b] \ | \ \int_{0}^{1} f(x) dx = 1 \right\}.$$ I used the Hölder's inequality to try to solve the problem but I do not know how to reduce the term with $(f (x))^{4}$ , i.e, $$\int_{0}^{1} (1+x^{2})(f(x))^{2} dx   \leq \left( \int_{0}^{1} (1+x^{2})dx \right)^{\frac{1}{2}} \left(  \int_{0}^{1} (f(x))^{4} dx \right)^{\frac{1}{2}} $$ I appreciate any help.","Find where I used the Hölder's inequality to try to solve the problem but I do not know how to reduce the term with , i.e, I appreciate any help."," \min_{f \in \mathcal{A}} \int_{0}^{1} (1+x^{2})(f(x))^{2} dx   \mathcal{A}=\left\{ f \in C[a,b] \ | \ \int_{0}^{1} f(x) dx = 1 \right\}. (f (x))^{4} \int_{0}^{1} (1+x^{2})(f(x))^{2} dx 
 \leq \left( \int_{0}^{1} (1+x^{2})dx \right)^{\frac{1}{2}} \left(  \int_{0}^{1} (f(x))^{4} dx \right)^{\frac{1}{2}} ",['real-analysis']
52,Finding a $f(x)=\sum_{k\geq 0}a_k x^{4k+2}$ such that $f(x) \to 0$ as $x \to \infty$,Finding a  such that  as,f(x)=\sum_{k\geq 0}a_k x^{4k+2} f(x) \to 0 x \to \infty,"To provide a little context for this question, I recently proved that for a suitable function $f(x)$ the following identity holds $$\sum_{k=0}^{\infty}f'(k)=-2\sum_{n=1}^{\infty}\frac{f'(n)}{e^{2\pi n}-1}$$ The criteria for suitability of the function $f(x)$ essentially comes down to two major factors.  Firstly, $f(x)$ must possess a power series expansion of the form $$f(x)=\sum_{k=0}^{\infty}a_k x^{4k+2}$$ such the series $$\sum_{k=0}^{\infty}\frac{\vert{a_k}\vert}{(2\pi)^{4k}}(4k+2)! < \infty$$ Secondly, we must have $f(x) \to 0$ as $x \to \infty$ . After proving this, I wanted to find a specific example so that I could compute both of the series to ensure the identity actually holds and I didn't just make some error in my proof.  I didn't think this would be too difficult of a task, but frustratingly I have not been able to come up with a single example! My first thought was to try something Bessel-like as $J_v(x) \to 0$ as $x \to \infty$ .  Along these lines I figured the most sensible function to start with was the Wright function $z^2\phi(4,3,-z^4)$ where $$\phi(\alpha,\beta,z) = \sum_{n=0}^{\infty}\frac{z^n}{n!\Gamma(\alpha n + \beta)}$$ so that $$z^2\phi(4,3,-z^4) = \sum_{n=0}^{\infty}\frac{(-1)^n}{n!(4n+2)!}z^{4n+2}$$ Unfortunately, I quickly became aware of the fact that the asymptotics of more general hypergeometric functions can become quite complicated.  More specifically to my needs, $z^2\phi(4,3,-z^4)$ does not go to $0$ as $z \to \infty$ . This paper gives the asymptotics for the Wright function. I am wondering, does anyone have any suggestions on constructing a function with my desired properties?  Or showing that such a function does indeed exist?","To provide a little context for this question, I recently proved that for a suitable function the following identity holds The criteria for suitability of the function essentially comes down to two major factors.  Firstly, must possess a power series expansion of the form such the series Secondly, we must have as . After proving this, I wanted to find a specific example so that I could compute both of the series to ensure the identity actually holds and I didn't just make some error in my proof.  I didn't think this would be too difficult of a task, but frustratingly I have not been able to come up with a single example! My first thought was to try something Bessel-like as as .  Along these lines I figured the most sensible function to start with was the Wright function where so that Unfortunately, I quickly became aware of the fact that the asymptotics of more general hypergeometric functions can become quite complicated.  More specifically to my needs, does not go to as . This paper gives the asymptotics for the Wright function. I am wondering, does anyone have any suggestions on constructing a function with my desired properties?  Or showing that such a function does indeed exist?","f(x) \sum_{k=0}^{\infty}f'(k)=-2\sum_{n=1}^{\infty}\frac{f'(n)}{e^{2\pi n}-1} f(x) f(x) f(x)=\sum_{k=0}^{\infty}a_k x^{4k+2} \sum_{k=0}^{\infty}\frac{\vert{a_k}\vert}{(2\pi)^{4k}}(4k+2)! < \infty f(x) \to 0 x \to \infty J_v(x) \to 0 x \to \infty z^2\phi(4,3,-z^4) \phi(\alpha,\beta,z) = \sum_{n=0}^{\infty}\frac{z^n}{n!\Gamma(\alpha n + \beta)} z^2\phi(4,3,-z^4) = \sum_{n=0}^{\infty}\frac{(-1)^n}{n!(4n+2)!}z^{4n+2} z^2\phi(4,3,-z^4) 0 z \to \infty","['real-analysis', 'sequences-and-series', 'special-functions']"
53,Is there a trick to solve $\int_{-1}^1 \frac{P(t)}{\sqrt{1-t^2}}{\rm d}t=a[P(x_1)+P(x_2)+P(x_3)]$?,Is there a trick to solve ?,\int_{-1}^1 \frac{P(t)}{\sqrt{1-t^2}}{\rm d}t=a[P(x_1)+P(x_2)+P(x_3)],"I found this question in some old exam: Find 4 reals $a, x_1, x_2, x_3$ such that the equality $$\int_{-1}^1 \frac{P(t)}{\sqrt{1-t^2}}{\rm d}t=a[P(x_1)+P(x_2)+P(x_3)]$$ is true for all polynomial with degree less or equal 3. My problem is not to compute these reals for a particular example but rather I didn't understand the idea behind this equality. In fact there is a similar question in this exam to prove that $\int_{-1}^1 \frac{f(t)}{\sqrt{1-t^2}}{\rm d}t=\frac{\pi}{3}[f(x_1)+f(x_2)+f(x_3)]$ for all polynomial $f$ with degree less or equal 5; this what make me sure that there is a trick behind these issues. Is there any explanation?",I found this question in some old exam: Find 4 reals such that the equality is true for all polynomial with degree less or equal 3. My problem is not to compute these reals for a particular example but rather I didn't understand the idea behind this equality. In fact there is a similar question in this exam to prove that for all polynomial with degree less or equal 5; this what make me sure that there is a trick behind these issues. Is there any explanation?,"a, x_1, x_2, x_3 \int_{-1}^1 \frac{P(t)}{\sqrt{1-t^2}}{\rm d}t=a[P(x_1)+P(x_2)+P(x_3)] \int_{-1}^1 \frac{f(t)}{\sqrt{1-t^2}}{\rm d}t=\frac{\pi}{3}[f(x_1)+f(x_2)+f(x_3)] f","['real-analysis', 'integration', 'polynomials']"
54,How to prove $f(x) = 4x^{3} + 4x - 6$ has exactly one real root?,How to prove  has exactly one real root?,f(x) = 4x^{3} + 4x - 6,"How can I show that $f(x) = 4x^{3} + 4x - 6$ has exactly one real root? I think the best way is to show $f'(x) = 12x^2 + 4 > 0$ for all $x \in \mathbb{R}$ . Thus, $f'(x)$ has zero real roots. Thus, $f(x)$ has at most one real root. I thought about trying to show that if $f$ is a polynomial and $f'$ has $n$ real roots, then $f$ has $n + 1$ roots by using Rolle's Theorem or Mean Value Theorem, but I don't think this fact, in general, is true. I would need to prove this statement. Can someone please help me prove this fact?","How can I show that has exactly one real root? I think the best way is to show for all . Thus, has zero real roots. Thus, has at most one real root. I thought about trying to show that if is a polynomial and has real roots, then has roots by using Rolle's Theorem or Mean Value Theorem, but I don't think this fact, in general, is true. I would need to prove this statement. Can someone please help me prove this fact?",f(x) = 4x^{3} + 4x - 6 f'(x) = 12x^2 + 4 > 0 x \in \mathbb{R} f'(x) f(x) f f' n f n + 1,[]
55,Lebesgue Dominated Convergence Theorem with Convergence in Measure,Lebesgue Dominated Convergence Theorem with Convergence in Measure,,"I have an issue with the solution to the following problem. I now want to prove that the Lebesgue Dominated Convergence Theorem still works when the condition $\{ f_{n} \}$ converges to $f$ a.e. is replaced by $\{ f_{n} \}$ converges to $f$ in measure. Relax, I know a lot of people have asked this question already. But what I do not understand is that the solutions I found online insist to work with a sub-subsequence instead of just a subsequence. For instance, the following is an expressed answer I summarise from someone: We can take a subsequence $\{f_{n_{k}} \}$ converges to $f$ in measure. Then, by a proved proposition, there is a sub-subsequence $\{f_{n_{k_{l}}} \}$ converges to $f$ a.e.. Then it reduces to the usual LDCT conditions and we can have the conclusion. But how about I simplify a little bit? Can't I just take a subsequence $\{f_{n_{k}} \}$ which converges to $f$ a.e. and have the conclusion? I fail to see the need to take a subsequence that converges to $f$ in measure first. Is that really necessary?","I have an issue with the solution to the following problem. I now want to prove that the Lebesgue Dominated Convergence Theorem still works when the condition converges to a.e. is replaced by converges to in measure. Relax, I know a lot of people have asked this question already. But what I do not understand is that the solutions I found online insist to work with a sub-subsequence instead of just a subsequence. For instance, the following is an expressed answer I summarise from someone: We can take a subsequence converges to in measure. Then, by a proved proposition, there is a sub-subsequence converges to a.e.. Then it reduces to the usual LDCT conditions and we can have the conclusion. But how about I simplify a little bit? Can't I just take a subsequence which converges to a.e. and have the conclusion? I fail to see the need to take a subsequence that converges to in measure first. Is that really necessary?",\{ f_{n} \} f \{ f_{n} \} f \{f_{n_{k}} \} f \{f_{n_{k_{l}}} \} f \{f_{n_{k}} \} f f,['real-analysis']
56,Pointwise convergence of sequence $(f_n)_n$ of functions to $f$ and changing limits,Pointwise convergence of sequence  of functions to  and changing limits,(f_n)_n f,"My analysis notes contains the following question: if $(f_n)_n$ is a sequence of functions of $A \subset \mathbb{R} \to \mathbb{R}$ and $a \in \mathbb{R} \cup \{-\infty, +\infty\}$ an accumulation point in $A$. Assume that for all $n$, $\lim_{ x \to a}f_n(x) = L_n$ exists and is finite. Suppose $(f_n)_n$ converges pointwise to $f:A \to \mathbb{R}$. 1) Does $\lim_{x \to a} f(x)$ exists? 2) Does $\lim_{n \to \infty} L_n$ exists? 3) Can we change limits (in case both limits exist)? If know all questions should be no. I have found examples for 1) and 3). However, I can not find an example for 2). I know I should look for a sequence of functions which does not converge uniformly. Any hints would be appreciated. My solutions to 1)  Define $$f_n: \mathbb{R} \to \mathbb{R}: x \mapsto \begin{cases} -1 &\text{ if } x \leq -1/n\\ nx & \text{ if } -1/n < x < 1/n\\ 1 &\text{ if } x \geq 1/n\end{cases}.$$ This functions converges to the function $f$ which equals to -1 for $x > 0$, 0 for $x = 0$ and $1$ for $x > 0$. The limit in zero does not exist. and 3): define $$f_n: \mathbb{R} \to \mathbb{R}: x \mapsto \frac{(nx)^2}{1 + (nx)^2}.$$ This sequence converges to $f$ which equals 1 everywhere, except for $x = 0$, where it equals to $0$. We have that  $$1 = \lim_{x \to 0} \lim_{n \to \infty} f_n(x) \neq \lim_{n \to \infty} \lim_{x \to 0} f_n(x) = 0$$","My analysis notes contains the following question: if $(f_n)_n$ is a sequence of functions of $A \subset \mathbb{R} \to \mathbb{R}$ and $a \in \mathbb{R} \cup \{-\infty, +\infty\}$ an accumulation point in $A$. Assume that for all $n$, $\lim_{ x \to a}f_n(x) = L_n$ exists and is finite. Suppose $(f_n)_n$ converges pointwise to $f:A \to \mathbb{R}$. 1) Does $\lim_{x \to a} f(x)$ exists? 2) Does $\lim_{n \to \infty} L_n$ exists? 3) Can we change limits (in case both limits exist)? If know all questions should be no. I have found examples for 1) and 3). However, I can not find an example for 2). I know I should look for a sequence of functions which does not converge uniformly. Any hints would be appreciated. My solutions to 1)  Define $$f_n: \mathbb{R} \to \mathbb{R}: x \mapsto \begin{cases} -1 &\text{ if } x \leq -1/n\\ nx & \text{ if } -1/n < x < 1/n\\ 1 &\text{ if } x \geq 1/n\end{cases}.$$ This functions converges to the function $f$ which equals to -1 for $x > 0$, 0 for $x = 0$ and $1$ for $x > 0$. The limit in zero does not exist. and 3): define $$f_n: \mathbb{R} \to \mathbb{R}: x \mapsto \frac{(nx)^2}{1 + (nx)^2}.$$ This sequence converges to $f$ which equals 1 everywhere, except for $x = 0$, where it equals to $0$. We have that  $$1 = \lim_{x \to 0} \lim_{n \to \infty} f_n(x) \neq \lim_{n \to \infty} \lim_{x \to 0} f_n(x) = 0$$",,"['real-analysis', 'convergence-divergence', 'pointwise-convergence']"
57,Is normalizing automorphism in $\mathbb{R}^n$ by Jacobian still an automorphism?,Is normalizing automorphism in  by Jacobian still an automorphism?,\mathbb{R}^n,"Let $n \in \mathbb{N}$. Suppose $f: \mathbb{R}^n \to \mathbb{R}^n$ is a $C^\infty$ diffeomorphism. Let $J_f: \mathbb{R}^n \to \text{M} (n, \mathbb{R})$ be the Jacobian of $f$. Is the map $F: \mathbb{R}^n \to \mathbb{R}^n$ defined by $F(x) = [J_f(x)]^{-1}\cdot f(x)$ a $C^\infty$ diffeomorphism?","Let $n \in \mathbb{N}$. Suppose $f: \mathbb{R}^n \to \mathbb{R}^n$ is a $C^\infty$ diffeomorphism. Let $J_f: \mathbb{R}^n \to \text{M} (n, \mathbb{R})$ be the Jacobian of $f$. Is the map $F: \mathbb{R}^n \to \mathbb{R}^n$ defined by $F(x) = [J_f(x)]^{-1}\cdot f(x)$ a $C^\infty$ diffeomorphism?",,"['calculus', 'real-analysis', 'differential-topology']"
58,Calculate $\lim_{n \rightarrow \infty}n^x (a_1 a_2\dots a_n)^{\frac{1}{n}}$.,Calculate .,\lim_{n \rightarrow \infty}n^x (a_1 a_2\dots a_n)^{\frac{1}{n}},"Suppose that $\{a_n\}$ is  a sequence such that $\displaystyle\lim_{n \rightarrow\infty} {n^x}a_n=a $ for some real $\,x$ . Calculate $$\lim_{n \rightarrow \infty}n^x (a_1\,a_2\dots\,a_n)^{\frac{1}{n}}$$ My attempts : I take $a_1=a_2 = \dots =a_n = a$ after that $\lim_{n \rightarrow \infty}$ $n^x (a_1\,a_2 \dots \,a_n)^{\frac{1}{n}}=  \infty \, a = \infty$ Is it correct ?? or not Please help me. Any hints/soluion.....",Suppose that is  a sequence such that for some real . Calculate My attempts : I take after that Is it correct ?? or not Please help me. Any hints/soluion.....,"\{a_n\} \displaystyle\lim_{n \rightarrow\infty} {n^x}a_n=a  \,x \lim_{n \rightarrow \infty}n^x (a_1\,a_2\dots\,a_n)^{\frac{1}{n}} a_1=a_2 = \dots =a_n = a \lim_{n \rightarrow \infty} n^x (a_1\,a_2 \dots \,a_n)^{\frac{1}{n}}=  \infty \, a = \infty","['real-analysis', 'calculus', 'limits', 'analysis', 'exponential-function']"
59,"Non-empty, disjoint subsets of $\Bbb R^2,$ both isometric to their union?","Non-empty, disjoint subsets of  both isometric to their union?","\Bbb R^2,","This question was posed to me by a friend who tells me the answer is yes, but I cannot see why. Does there exist two nonempty, disjoint sets $A, B \subset \mathbb{R}^2$ such that $A$ and $B$ are both isometric to their union? I cannot for the life of me come up with a way of constructing both sets - I've noticed that the measure of both has to be $0$, but that's about it.","This question was posed to me by a friend who tells me the answer is yes, but I cannot see why. Does there exist two nonempty, disjoint sets $A, B \subset \mathbb{R}^2$ such that $A$ and $B$ are both isometric to their union? I cannot for the life of me come up with a way of constructing both sets - I've noticed that the measure of both has to be $0$, but that's about it.",,"['real-analysis', 'metric-spaces']"
60,Characterization of solutions to $f' = f(1-f)$,Characterization of solutions to,f' = f(1-f),"The sigmoid function $f(x) = \frac{1}{1+e^{-x}}$ has the property that $$f'(x) = f(x)(1-f(x))~~~ and ~~~f(0) = \frac 12$$ My question: is $f$ the unique function from $\mathbb R$ to $(0,1)$, perhaps up to some kind of scaling, that satisfies $f' = f(1-f)$? I don't have much experience with differential equations so a nonlinear one like this is beyond anything I've done before. In case it helps, my motivation for this is that this property makes the log likelihood a lot easier in a logistic regression, and I'm wondering if assuming that the inverse link function satisfies this property is equivalent to just taking it to be $f$.","The sigmoid function $f(x) = \frac{1}{1+e^{-x}}$ has the property that $$f'(x) = f(x)(1-f(x))~~~ and ~~~f(0) = \frac 12$$ My question: is $f$ the unique function from $\mathbb R$ to $(0,1)$, perhaps up to some kind of scaling, that satisfies $f' = f(1-f)$? I don't have much experience with differential equations so a nonlinear one like this is beyond anything I've done before. In case it helps, my motivation for this is that this property makes the log likelihood a lot easier in a logistic regression, and I'm wondering if assuming that the inverse link function satisfies this property is equivalent to just taking it to be $f$.",,"['real-analysis', 'ordinary-differential-equations']"
61,Arzela Ascoli counterexamples,Arzela Ascoli counterexamples,,"I am looking for some examples that show that the Arzela Ascoli theorem is ""tight"". i.e. is there a sequence of functions that is uniformly bounded and equicontinuous on a noncompact set that would not have a uniformly convergent subsequence. Also is there an example of a uniformly bounded non-equicontinuous sequence on a compact set that does not have a convergent subsequence, and similarly by removing the uniformly bounded condition","I am looking for some examples that show that the Arzela Ascoli theorem is ""tight"". i.e. is there a sequence of functions that is uniformly bounded and equicontinuous on a noncompact set that would not have a uniformly convergent subsequence. Also is there an example of a uniformly bounded non-equicontinuous sequence on a compact set that does not have a convergent subsequence, and similarly by removing the uniformly bounded condition",,['real-analysis']
62,Find: $\int_0^\pi x^2\ln(\sin(x))dx$,Find:,\int_0^\pi x^2\ln(\sin(x))dx,"I've been working on a few log-sine integrals. So far I have found $$\int_0^\pi \ln(\sin(x))dx=-\pi\ln(2)$$ $$\int_0^\pi x\ln(\sin(x))dx=-\frac{\pi^2\ln(2)}{2}$$ ...but I am struggling with the integral $$\int_0^\pi x^2\ln(\sin(x))dx$$ and I can't figure it out... however, I do know that the answer will contain $\zeta(3)$. Any hints?","I've been working on a few log-sine integrals. So far I have found $$\int_0^\pi \ln(\sin(x))dx=-\pi\ln(2)$$ $$\int_0^\pi x\ln(\sin(x))dx=-\frac{\pi^2\ln(2)}{2}$$ ...but I am struggling with the integral $$\int_0^\pi x^2\ln(\sin(x))dx$$ and I can't figure it out... however, I do know that the answer will contain $\zeta(3)$. Any hints?",,"['calculus', 'real-analysis', 'trigonometry', 'definite-integrals', 'logarithms']"
63,"If $f_n \to f$ uniformly and $g_n\to g$ uniformly, then $f_n\circ g_n \to f\circ g$ uniformly?","If  uniformly and  uniformly, then  uniformly?",f_n \to f g_n\to g f_n\circ g_n \to f\circ g,"Assume that $f_n, g_n$ are continuous. If $f_n \to f$ uniformly and $g_n\to g$ uniformly. Does it imply that $f_n\circ g_n \to f\circ g$ uniformly? I think it is true but I have no idea to prove it. Can anyone help me? Thank you in advance! Edit: Is it true that $f_n\circ g_n \to f\circ g$ pointwise?","Assume that $f_n, g_n$ are continuous. If $f_n \to f$ uniformly and $g_n\to g$ uniformly. Does it imply that $f_n\circ g_n \to f\circ g$ uniformly? I think it is true but I have no idea to prove it. Can anyone help me? Thank you in advance! Edit: Is it true that $f_n\circ g_n \to f\circ g$ pointwise?",,"['calculus', 'real-analysis', 'analysis', 'limits', 'uniform-convergence']"
64,Is $\sum_{k=1}^n \cos \sqrt k= o(n)$ as $n\to \infty$?,Is  as ?,\sum_{k=1}^n \cos \sqrt k= o(n) n\to \infty,"In the solution @Jack D'Aurizio: gave the estimate $$\sum_{k=1}^{n} \cos k^2 = \mathcal{O}(\sqrt{n}\log n)$$ and mentioned the Weyl's inequalities $$\sum_{k=1}^n \cos( f(k) ) = \mathcal{O}(F(n))$$ if $f$ is polynomial function. I wonder if one can give some nontrivial estimates for other kind of functions, for instance $$\sum_{k=1}^n \cos \sqrt{k} = \mathcal{O}(F(n))$$ for some $F(n) = \mathcal{o}(n)$ .","In the solution @Jack D'Aurizio: gave the estimate and mentioned the Weyl's inequalities if is polynomial function. I wonder if one can give some nontrivial estimates for other kind of functions, for instance for some .",\sum_{k=1}^{n} \cos k^2 = \mathcal{O}(\sqrt{n}\log n) \sum_{k=1}^n \cos( f(k) ) = \mathcal{O}(F(n)) f \sum_{k=1}^n \cos \sqrt{k} = \mathcal{O}(F(n)) F(n) = \mathcal{o}(n),"['real-analysis', 'asymptotics']"
65,Give a sequence such that root test works while ratio test fails,Give a sequence such that root test works while ratio test fails,,"Question : Give a sequence $(a_n)_{n=1}^\infty$ with $a_n>0$ such that root test works while ratio test does not work, that is,    $$\lim_{n\rightarrow\infty}(a_n)^{\frac{1}{n}} \text{ exists}$$   while    $$\lim_{n\rightarrow\infty}\frac{a_{n+1}}{a_n} \text{ does not exist}.$$ My attempt: Define a sequence $(a_n)_{n=1}^\infty$ such that      $$ a_n =  \begin{cases}        2^{n-1} & \text{if }n \text{ is odd,} \\ 	  2^{n+1} & \text{if }n \text{ is even}.    \end{cases} 	$$ Odd subsequence $(a_{n_j})_{j=1}^\infty$ implies that $\lim_{j\rightarrow\infty}(a_{n_j})^{\frac{1}{n_j}} = \lim_{j\rightarrow\infty} 2^{1-\frac{1}{n_j}} = 2 $ while even subsequence $(a_{n_k})_{k=1}^\infty$ implies that $\lim_{k\rightarrow\infty}(a_{n_k})^{\frac{1}{n_k}} = \lim_{k\rightarrow\infty} 2^{1+\frac{1}{n_k}} = 2.$     It follows that $\lim_{n\rightarrow\infty}(a_n)^{\frac{1}{n}} = 2,$ that is, root test works.      However, for odd subsequence $(a_{n_j})_{j=1}^\infty,$     $$\lim_{j\rightarrow\infty}\frac{a_{n_j+1}}{a_{n_j}} = \lim_{n\rightarrow\infty} \frac{2^{n_j+1}}{2^{n_j-1}} = \lim_{n\rightarrow\infty} 4 = 4.$$     For even subsequence $(a_{n_k})_{k=1}^\infty,$     $$\lim_{k\rightarrow\infty}\frac{a_{n_k+1}}{a_{n_k}} = \lim_{k\rightarrow\infty} \frac{2^{n_k-1}}{2^{n_k+1}} = \lim_{k\rightarrow\infty} \frac{1}{4} = \frac{1}{4}.$$ Therefore, $\lim_{n\rightarrow\infty}\frac{a_{n+1}}{a_n}$ does not exist, that is, ratio test does not work. Does my example work?","Question : Give a sequence $(a_n)_{n=1}^\infty$ with $a_n>0$ such that root test works while ratio test does not work, that is,    $$\lim_{n\rightarrow\infty}(a_n)^{\frac{1}{n}} \text{ exists}$$   while    $$\lim_{n\rightarrow\infty}\frac{a_{n+1}}{a_n} \text{ does not exist}.$$ My attempt: Define a sequence $(a_n)_{n=1}^\infty$ such that      $$ a_n =  \begin{cases}        2^{n-1} & \text{if }n \text{ is odd,} \\ 	  2^{n+1} & \text{if }n \text{ is even}.    \end{cases} 	$$ Odd subsequence $(a_{n_j})_{j=1}^\infty$ implies that $\lim_{j\rightarrow\infty}(a_{n_j})^{\frac{1}{n_j}} = \lim_{j\rightarrow\infty} 2^{1-\frac{1}{n_j}} = 2 $ while even subsequence $(a_{n_k})_{k=1}^\infty$ implies that $\lim_{k\rightarrow\infty}(a_{n_k})^{\frac{1}{n_k}} = \lim_{k\rightarrow\infty} 2^{1+\frac{1}{n_k}} = 2.$     It follows that $\lim_{n\rightarrow\infty}(a_n)^{\frac{1}{n}} = 2,$ that is, root test works.      However, for odd subsequence $(a_{n_j})_{j=1}^\infty,$     $$\lim_{j\rightarrow\infty}\frac{a_{n_j+1}}{a_{n_j}} = \lim_{n\rightarrow\infty} \frac{2^{n_j+1}}{2^{n_j-1}} = \lim_{n\rightarrow\infty} 4 = 4.$$     For even subsequence $(a_{n_k})_{k=1}^\infty,$     $$\lim_{k\rightarrow\infty}\frac{a_{n_k+1}}{a_{n_k}} = \lim_{k\rightarrow\infty} \frac{2^{n_k-1}}{2^{n_k+1}} = \lim_{k\rightarrow\infty} \frac{1}{4} = \frac{1}{4}.$$ Therefore, $\lim_{n\rightarrow\infty}\frac{a_{n+1}}{a_n}$ does not exist, that is, ratio test does not work. Does my example work?",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'examples-counterexamples']"
66,Logical suite and inequalities,Logical suite and inequalities,,"This is a related problem (see here ) We have the following inequalities : For $n=3$ with $a,b,c$ real numbers the following inequality holds.   $$\frac{1}{(a-b)^2}+\frac{1}{(c-b)^2}+\frac{1}{(a-c)^2}+(c-b)^2+(c-a)^2+(a-b)^2\geq \sqrt{54}$$ For $n=4$ with $a,b,c,d$ real numbers $$\frac{1}{(a-d)^2}+\frac{1}{(d-b)^2}+\frac{1}{(d-c)^2}+\frac{1}{(a-b)^2}+\frac{1}{(c-b)^2}+\frac{1}{(a-c)^2}+(d-b)^2+(c-d)^2+(a-d)^2+(a-b)^2+(b-c)^2+(a-c)^2\geq \sqrt{288}$$ If we continue with $n=5,6,7\cdots$ there is a logical suite but I don't know how to prove this and what is the following numbers . Thanks.","This is a related problem (see here ) We have the following inequalities : For $n=3$ with $a,b,c$ real numbers the following inequality holds.   $$\frac{1}{(a-b)^2}+\frac{1}{(c-b)^2}+\frac{1}{(a-c)^2}+(c-b)^2+(c-a)^2+(a-b)^2\geq \sqrt{54}$$ For $n=4$ with $a,b,c,d$ real numbers $$\frac{1}{(a-d)^2}+\frac{1}{(d-b)^2}+\frac{1}{(d-c)^2}+\frac{1}{(a-b)^2}+\frac{1}{(c-b)^2}+\frac{1}{(a-c)^2}+(d-b)^2+(c-d)^2+(a-d)^2+(a-b)^2+(b-c)^2+(a-c)^2\geq \sqrt{288}$$ If we continue with $n=5,6,7\cdots$ there is a logical suite but I don't know how to prove this and what is the following numbers . Thanks.",,"['real-analysis', 'inequality', 'contest-math']"
67,sup of Norm equals sup of inner product,sup of Norm equals sup of inner product,,"Given $V$ a pre-Hilbert space and T a self-ajoint linear Operator $V \to V$ show that$$ \sup \|Tx\| =\sup |\langle Tx,x \rangle | \in \mathbb{R} \cup \{ \infty  \}$$ for all $ \|x\| =1$. I know that $\| x \| = \sup |\langle x,y \rangle | $ over all $y \in V$. from cauchy-schwarz $|\langle Tx,x \rangle | \le \|Tx\| \| x \| = \|Tx\| $  But I cannot derive the wanted + I dont know how to bring the self-ajointness in. Greetings.","Given $V$ a pre-Hilbert space and T a self-ajoint linear Operator $V \to V$ show that$$ \sup \|Tx\| =\sup |\langle Tx,x \rangle | \in \mathbb{R} \cup \{ \infty  \}$$ for all $ \|x\| =1$. I know that $\| x \| = \sup |\langle x,y \rangle | $ over all $y \in V$. from cauchy-schwarz $|\langle Tx,x \rangle | \le \|Tx\| \| x \| = \|Tx\| $  But I cannot derive the wanted + I dont know how to bring the self-ajointness in. Greetings.",,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'inner-products']"
68,"On Complete Separable metrizable spaces, do the compact sets generate the Borel $\sigma$-algebra?","On Complete Separable metrizable spaces, do the compact sets generate the Borel -algebra?",\sigma,"On Complete Separable metrizable spaces, do the compact sets generate the Borel $\sigma$-algebra? I don't think this is true. Can anyone help me with a counterexample?","On Complete Separable metrizable spaces, do the compact sets generate the Borel $\sigma$-algebra? I don't think this is true. Can anyone help me with a counterexample?",,"['real-analysis', 'general-topology', 'measure-theory', 'borel-sets']"
69,$L^p$ implies polynomial decay?,implies polynomial decay?,L^p,"Suppose $f$ is uniformly continuous on $\mathbb{R}^n$. Question: If $f \in L^p(\mathbb{R}^n)$, must there exist a $q > 0$ such that $|f(x)| \leq C|x|^{-q}$ for all large x (or perhaps a.e. large x)? I know $f \in L^p(\mathbb{R}^n)$ implies $f(x) \to 0$ as $|x| \to \infty$. And I know $|f(x)| \leq C|x|^{-q}$ implies $f \in L^p(\mathbb{R}^n)$ when $pq > n$. Remark: One could ask a similar question about infinite series.","Suppose $f$ is uniformly continuous on $\mathbb{R}^n$. Question: If $f \in L^p(\mathbb{R}^n)$, must there exist a $q > 0$ such that $|f(x)| \leq C|x|^{-q}$ for all large x (or perhaps a.e. large x)? I know $f \in L^p(\mathbb{R}^n)$ implies $f(x) \to 0$ as $|x| \to \infty$. And I know $|f(x)| \leq C|x|^{-q}$ implies $f \in L^p(\mathbb{R}^n)$ when $pq > n$. Remark: One could ask a similar question about infinite series.",,"['real-analysis', 'analysis', 'measure-theory']"
70,"If $a_n$ is sequence of positive numbers such that $\sum a_n$ converges, then does $\sum \frac {{(a_n)}^{\frac 14}}{n^{\frac 45}}$ converge?","If  is sequence of positive numbers such that  converges, then does  converge?",a_n \sum a_n \sum \frac {{(a_n)}^{\frac 14}}{n^{\frac 45}},"Failed attempts : Let $x_n= \frac {{(a_n)}^{\frac 14}}{n^{\frac 45}}$. 1) By limit comparison test - Taking $y_n=a_n$, $\lim \frac {x_n}{y_n}=\lim \frac 1{n^{\frac 45} {a_n}^{\frac 34}}$ (Leads nowhere.) Taking $y_n=\frac 1n$, $\lim \frac {x_n}{y_n}=\lim \frac 1{n^{\frac 15} {a_n}^{\frac 14}}$ (Leads nowhere). Similarly taking $y_n={a_n}^2,\frac 1{n^2}$ doesn't work. Still searching for $y_n$ that works. 2) By comparison test - $\frac {{(a_n)}^{\frac 14}}{n^{\frac 45}} \le {(a_n)}^{\frac 14}$ (Leads nowhere since we can't say whether $\sum{a_n}^{\frac 14}$ converges). 3) By ratio test - $\lim\frac {{(a_{n+1})}^{\frac 14}}{{n+1}^{\frac 45}} \frac {n^{\frac 45}}{{a_n}^{\frac 14}}=\lim \frac {{a_{n+1}}^{\frac 14}}{{a_n}^{\frac 14}}$ (Leads nowhere since we don't know whether the last limit is less than $1$) Where am I making mistakes? Can you provide some hints?","Failed attempts : Let $x_n= \frac {{(a_n)}^{\frac 14}}{n^{\frac 45}}$. 1) By limit comparison test - Taking $y_n=a_n$, $\lim \frac {x_n}{y_n}=\lim \frac 1{n^{\frac 45} {a_n}^{\frac 34}}$ (Leads nowhere.) Taking $y_n=\frac 1n$, $\lim \frac {x_n}{y_n}=\lim \frac 1{n^{\frac 15} {a_n}^{\frac 14}}$ (Leads nowhere). Similarly taking $y_n={a_n}^2,\frac 1{n^2}$ doesn't work. Still searching for $y_n$ that works. 2) By comparison test - $\frac {{(a_n)}^{\frac 14}}{n^{\frac 45}} \le {(a_n)}^{\frac 14}$ (Leads nowhere since we can't say whether $\sum{a_n}^{\frac 14}$ converges). 3) By ratio test - $\lim\frac {{(a_{n+1})}^{\frac 14}}{{n+1}^{\frac 45}} \frac {n^{\frac 45}}{{a_n}^{\frac 14}}=\lim \frac {{a_{n+1}}^{\frac 14}}{{a_n}^{\frac 14}}$ (Leads nowhere since we don't know whether the last limit is less than $1$) Where am I making mistakes? Can you provide some hints?",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'convergence-divergence']"
71,"Fractional part of $1+\frac{1}{2}+\dots+\frac{1}{n}$ dense in $(0,1)$",Fractional part of  dense in,"1+\frac{1}{2}+\dots+\frac{1}{n} (0,1)","Is the sequence $(x_n)_{n=1}^\infty$, where $x_n$ is the fractional part of $1+\frac{1}{2}+\dots+\frac1n$, dense in $(0,1)$? The fractional part of a number $y$ is defined as $y-\lfloor y\rfloor$. For a sequence like $a,2a,3a,\dots$ where $a$ is an irrational number, it is known that the fractional part sequence is dense. (I think there's even a name for this result, but I can't recall.) The proof uses a pigeonhole-style argument to show that the sequence must fall into any small interval of $(0,1)$ and relies on the linearity of the sequence, which we don't have in our sequence.","Is the sequence $(x_n)_{n=1}^\infty$, where $x_n$ is the fractional part of $1+\frac{1}{2}+\dots+\frac1n$, dense in $(0,1)$? The fractional part of a number $y$ is defined as $y-\lfloor y\rfloor$. For a sequence like $a,2a,3a,\dots$ where $a$ is an irrational number, it is known that the fractional part sequence is dense. (I think there's even a name for this result, but I can't recall.) The proof uses a pigeonhole-style argument to show that the sequence must fall into any small interval of $(0,1)$ and relies on the linearity of the sequence, which we don't have in our sequence.",,"['real-analysis', 'sequences-and-series']"
72,Idea behind pattern seen in topology,Idea behind pattern seen in topology,,"In topology, I have come across a ""pattern"" that involves a subsequence or subset of a sequence or set. For example, this definition of a second countable space uses: A topological space $T$ is second countable if there exists some countable collection $\mathcal{U} = \{U_i\}_{i=1}^{\infty}$ of open subsets of $T$ such that any open subset of $T$ can be written as a union of elements of some subfamily of $\mathcal{U}$. Another variant of that ""pattern"" can be found in: Let $\{x_n\}$ be a bounded sequence such that every convergent subsequence converges to $L$. Then $\lim_{n\to\infty}x_n = L.$ What is the idea behind that pattern of a subsequence/subset of sequence/set? I realize that the two examples are different, I am not saying that they are the same, but it feels to me that they share an important idea of "" a sub-thing of a thing does this or that "" in their construction, and it is that idea/thought process that I'd like to get at. As a bonus question: in which fields/sub-fields of maths is that idea more prevalent/used? Does it show up only in e.g (some branch of) topology, or is it also used in e.g. (some branch of) algebra?","In topology, I have come across a ""pattern"" that involves a subsequence or subset of a sequence or set. For example, this definition of a second countable space uses: A topological space $T$ is second countable if there exists some countable collection $\mathcal{U} = \{U_i\}_{i=1}^{\infty}$ of open subsets of $T$ such that any open subset of $T$ can be written as a union of elements of some subfamily of $\mathcal{U}$. Another variant of that ""pattern"" can be found in: Let $\{x_n\}$ be a bounded sequence such that every convergent subsequence converges to $L$. Then $\lim_{n\to\infty}x_n = L.$ What is the idea behind that pattern of a subsequence/subset of sequence/set? I realize that the two examples are different, I am not saying that they are the same, but it feels to me that they share an important idea of "" a sub-thing of a thing does this or that "" in their construction, and it is that idea/thought process that I'd like to get at. As a bonus question: in which fields/sub-fields of maths is that idea more prevalent/used? Does it show up only in e.g (some branch of) topology, or is it also used in e.g. (some branch of) algebra?",,"['real-analysis', 'general-topology']"
73,"Is there a way to decompose a polynomial $xy+f(x,y)$ into the product of two convergent power series $x+g(x,y)$ and $y+h(x,y)$?",Is there a way to decompose a polynomial  into the product of two convergent power series  and ?,"xy+f(x,y) x+g(x,y) y+h(x,y)","Suppose we have a polynomial $xy+f(x,y)$, where $f(x,y)$ is a polynomial in $\mathbb C[x,y]$ whose the lowest degree term has degree at least 3. My question is, are we always able to decompose $xy+f(x,y)$ into the product of two convergent power series $x+g(x,y)$ and $y+h(x,y)$ in a neighborhood of $(0,0)$, where terms in $g$ and $h$ have order higher than 1? I have no idea about the convergence of power series with two or more variables. Any solution or reference will be appreciated! Please also note that it's not enough to simply decompose $xy+f(x,y)$ formally into two $x+g(x,y)$ and $y+h(x,y)$. We also need the convergence of $g$ and $h$!","Suppose we have a polynomial $xy+f(x,y)$, where $f(x,y)$ is a polynomial in $\mathbb C[x,y]$ whose the lowest degree term has degree at least 3. My question is, are we always able to decompose $xy+f(x,y)$ into the product of two convergent power series $x+g(x,y)$ and $y+h(x,y)$ in a neighborhood of $(0,0)$, where terms in $g$ and $h$ have order higher than 1? I have no idea about the convergence of power series with two or more variables. Any solution or reference will be appreciated! Please also note that it's not enough to simply decompose $xy+f(x,y)$ formally into two $x+g(x,y)$ and $y+h(x,y)$. We also need the convergence of $g$ and $h$!",,"['real-analysis', 'complex-analysis', 'algebraic-geometry', 'convergence-divergence', 'power-series']"
74,How can I prove that this sequence is monotonic?,How can I prove that this sequence is monotonic?,,"I have a sequence $(u_n)$ that is defined as: $u_0 = 2$, $u_{n+1} =\frac{u_n}{2} + \frac{1}{u_n}$ I have tried to prove that it is monotonic using induction but I wasn't able to succeed. How can I prove it easily ? Thank you","I have a sequence $(u_n)$ that is defined as: $u_0 = 2$, $u_{n+1} =\frac{u_n}{2} + \frac{1}{u_n}$ I have tried to prove that it is monotonic using induction but I wasn't able to succeed. How can I prove it easily ? Thank you",,['real-analysis']
75,"Why is it true that $\lim_{n\to\infty} \left(\int_a^b \left|f(x)\right|^ndx\right)^{1/n} = \text{sup}\{\left|f(x)\right| : x \in [a,b]\}$? [duplicate]",Why is it true that ? [duplicate],"\lim_{n\to\infty} \left(\int_a^b \left|f(x)\right|^ndx\right)^{1/n} = \text{sup}\{\left|f(x)\right| : x \in [a,b]\}","This question already has answers here : If $f(x)$ is continuous on $[a,b]$ and $M=\max \; |f(x)|$, is $M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}$? (2 answers) Closed 7 years ago . Can someone explain the intuition behind this/ what the proof might look for this? If $f$ is a continuous, real-valued function on $[a,b]$, then \begin{equation} \lim_{n\to\infty} \left(\int_a^b \left|f(x)\right|^ndx\right)^{1/n} = \text{sup}\{\left|f(x)\right| : x \in [a,b]\} \end{equation} It seems like it's a way of averaging $\left|f(x)\right|$, similar to how the Lyapunov number is an average of all the slopes of an orbit in a dynamical system.","This question already has answers here : If $f(x)$ is continuous on $[a,b]$ and $M=\max \; |f(x)|$, is $M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}$? (2 answers) Closed 7 years ago . Can someone explain the intuition behind this/ what the proof might look for this? If $f$ is a continuous, real-valued function on $[a,b]$, then \begin{equation} \lim_{n\to\infty} \left(\int_a^b \left|f(x)\right|^ndx\right)^{1/n} = \text{sup}\{\left|f(x)\right| : x \in [a,b]\} \end{equation} It seems like it's a way of averaging $\left|f(x)\right|$, similar to how the Lyapunov number is an average of all the slopes of an orbit in a dynamical system.",,"['real-analysis', 'analysis']"
76,Infinite Union/Intersection vs Infinite summation,Infinite Union/Intersection vs Infinite summation,,"I have a question for you. It's well known that I can't sum an infinite number of ""number"". Indeed, the series $$ \sum_{k=1}^\infty a_k $$ is defined as a Limit. However, I can make the infinite union/intersection of sets. Someone can explain me why ?","I have a question for you. It's well known that I can't sum an infinite number of ""number"". Indeed, the series $$ \sum_{k=1}^\infty a_k $$ is defined as a Limit. However, I can make the infinite union/intersection of sets. Someone can explain me why ?",,"['real-analysis', 'measure-theory', 'elementary-set-theory', 'limsup-and-liminf']"
77,Two different expansions of $\frac{z}{1-z}$,Two different expansions of,\frac{z}{1-z},"This is exercise 21 of Chapter 1 from Stein and Shakarchi's Complex Analysis. Show that for $|z|<1$ one has $$\frac{z}{1-z^2}+\frac{z^2}{1-z^4}+\cdots +\frac{z^{2^n}}{1-z^{2^{n+1}}}+\cdots =\frac{z}{1-z}$$and $$\frac{z}{1+z}+\frac{2z^2}{1+z^2}+\cdots \frac{2^k z^{2^k}}{1+z^{2^k}}+\cdots =\frac{z}{1-z}.$$ Justify any change in the order of summation. [Hint: Use the dyadic expansion of an integer and the fact that $2^{k+1}-1=1+2+2^2+\cdots +2^k$.] I don't really know how to work this through. I know that $\frac{z}{1-z}=\sum_{n=1}^\infty z^n$ and each $n$ can be represented as a dyadic expansion, but I don't know how to progress from here. Any hints solutions or suggestions would be appreciated.","This is exercise 21 of Chapter 1 from Stein and Shakarchi's Complex Analysis. Show that for $|z|<1$ one has $$\frac{z}{1-z^2}+\frac{z^2}{1-z^4}+\cdots +\frac{z^{2^n}}{1-z^{2^{n+1}}}+\cdots =\frac{z}{1-z}$$and $$\frac{z}{1+z}+\frac{2z^2}{1+z^2}+\cdots \frac{2^k z^{2^k}}{1+z^{2^k}}+\cdots =\frac{z}{1-z}.$$ Justify any change in the order of summation. [Hint: Use the dyadic expansion of an integer and the fact that $2^{k+1}-1=1+2+2^2+\cdots +2^k$.] I don't really know how to work this through. I know that $\frac{z}{1-z}=\sum_{n=1}^\infty z^n$ and each $n$ can be represented as a dyadic expansion, but I don't know how to progress from here. Any hints solutions or suggestions would be appreciated.",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'analysis', 'power-series']"
78,Confusion on Baby Rudin problem 2.16,Confusion on Baby Rudin problem 2.16,,"I am having a difficult time proving problem 2.16 (specifically that $E$ is closed) in Rudin's Principles of Mathematical Analysis . I realize that this question has been asked before here , but I believe my question is different enough to warrant a new question. The problem asks: Regard $\mathbb{Q}$, the set of all rational numbers, as a metric space with $d(p,q) = \vert{p-q}\vert$.  Let $E$ be the set of all $p \in \mathbb{Q}$ such that $2<p^2<3$.  Show that $E$ is closed and bounded in $\mathbb{Q}$, but that $E$ is not compact.  Is $E$ open? My attempt: By definition $E$ is closed if every limit point of $E$ is an element of $E$. Thus, we want to show that if a point $p$ is a limit point of $E$, $p \in E$, that is $2<p^2<3$. To do this, let $p$ be a limit point of $E$. We then know that $$\forall r> 0, \exists q :d(p,q) < r$$ In other words $$ \forall r> 0, \exists q : q-r < p < q + r$$ Now squaring the inequality gives $$ q^2-2qr + r^2 < p^2 < q^2 + 2qr + r^2 $$ Thus I would like to find an $r$ such that $2<p^2<3$ as desired.  Unfortunately I am having a hard time constructing such an $r$.  Can anyone point me in the right direction? Moreover, I've seen similar problem to this one, in which the set $E$ is proven to be closed by showing that the complement of $E$ is open.  Is this usually the easier route to take?  In this case it would seem to require more work. I just have a hard time understanding where solvers obtain their $r$ values from, they seem to come out of thin air...","I am having a difficult time proving problem 2.16 (specifically that $E$ is closed) in Rudin's Principles of Mathematical Analysis . I realize that this question has been asked before here , but I believe my question is different enough to warrant a new question. The problem asks: Regard $\mathbb{Q}$, the set of all rational numbers, as a metric space with $d(p,q) = \vert{p-q}\vert$.  Let $E$ be the set of all $p \in \mathbb{Q}$ such that $2<p^2<3$.  Show that $E$ is closed and bounded in $\mathbb{Q}$, but that $E$ is not compact.  Is $E$ open? My attempt: By definition $E$ is closed if every limit point of $E$ is an element of $E$. Thus, we want to show that if a point $p$ is a limit point of $E$, $p \in E$, that is $2<p^2<3$. To do this, let $p$ be a limit point of $E$. We then know that $$\forall r> 0, \exists q :d(p,q) < r$$ In other words $$ \forall r> 0, \exists q : q-r < p < q + r$$ Now squaring the inequality gives $$ q^2-2qr + r^2 < p^2 < q^2 + 2qr + r^2 $$ Thus I would like to find an $r$ such that $2<p^2<3$ as desired.  Unfortunately I am having a hard time constructing such an $r$.  Can anyone point me in the right direction? Moreover, I've seen similar problem to this one, in which the set $E$ is proven to be closed by showing that the complement of $E$ is open.  Is this usually the easier route to take?  In this case it would seem to require more work. I just have a hard time understanding where solvers obtain their $r$ values from, they seem to come out of thin air...",,"['real-analysis', 'general-topology']"
79,Why any rational number can be written as $P/Q$?,Why any rational number can be written as ?,P/Q,"Why any rational number can be written as $P/Q$? and how to prove that where $p,q$ are integers and at least one of $p,q$ is odd? I can see that at least one of $p,q$ is odd, but I don't how to write down the proof properly. I did is: Let $x$ be a rational number. Case 1: $x$ is integer then $x = x/1$ where $1$ is an odd number. Case 2: $x$ is not integer write $x = m/n$ if $m,n$ are both even, then the $2$ cancels out, and repeat the case $2$ process until at least one of the two numbers is odd.","Why any rational number can be written as $P/Q$? and how to prove that where $p,q$ are integers and at least one of $p,q$ is odd? I can see that at least one of $p,q$ is odd, but I don't how to write down the proof properly. I did is: Let $x$ be a rational number. Case 1: $x$ is integer then $x = x/1$ where $1$ is an odd number. Case 2: $x$ is not integer write $x = m/n$ if $m,n$ are both even, then the $2$ cancels out, and repeat the case $2$ process until at least one of the two numbers is odd.",,"['real-analysis', 'number-theory', 'rational-numbers']"
80,Proving the uniqueness property of Lebesgue measure,Proving the uniqueness property of Lebesgue measure,,"I'm having trouble in showing the following statement: The Lebesgue measure is the only map $E \to m(E)$ from the class of measurable sets to $[0,+\infty]$, which satisfies the following properties: $(i)$ Empty set property : $m(\phi)=0$ $(ii)$ Countable additivity : For disjoint measurable sets $E_1,E_2,\ldots$ ; $$m\bigg(\bigcup_{n=1}^{\infty}E_n\bigg)=\sum_{n=1}^{\infty}m(E_n)$$ $(iii)$ Translation invariance : For any measurable set $E \subset \mathbb{R}^d$ and for any $x \in \mathbb{R}^d$, $E+x := \{y+x|y \in E\}$ is measurable and $m(E+x)=m(E)$. $(iv)$ Normalization : $m([0,1]^d)=1$, i.e. measure of the unit hypercube is $1$. My approach: To go via Lebesgue outer measure . We already know that the only mapping from the class of all subsets of $\mathbb{R}^d$ (which obviously contains the class of all measurable sets in $\mathbb{R}^d$) to $[0,+\infty]$, satisfying $(i)$, a weaker version of $(ii)$ (only subadditivity ) and $(iv)$, is the Lebesgue outer measure $m^*(\cdot)$. Then the job left is to show that with strict additivity and translation invariance , the Lebesgue outer measure $m^*(\cdot)$ is upgraded to the Lebesgue measure $m(\cdot)$, which I cannot complete. Any help is greatly appreciated!","I'm having trouble in showing the following statement: The Lebesgue measure is the only map $E \to m(E)$ from the class of measurable sets to $[0,+\infty]$, which satisfies the following properties: $(i)$ Empty set property : $m(\phi)=0$ $(ii)$ Countable additivity : For disjoint measurable sets $E_1,E_2,\ldots$ ; $$m\bigg(\bigcup_{n=1}^{\infty}E_n\bigg)=\sum_{n=1}^{\infty}m(E_n)$$ $(iii)$ Translation invariance : For any measurable set $E \subset \mathbb{R}^d$ and for any $x \in \mathbb{R}^d$, $E+x := \{y+x|y \in E\}$ is measurable and $m(E+x)=m(E)$. $(iv)$ Normalization : $m([0,1]^d)=1$, i.e. measure of the unit hypercube is $1$. My approach: To go via Lebesgue outer measure . We already know that the only mapping from the class of all subsets of $\mathbb{R}^d$ (which obviously contains the class of all measurable sets in $\mathbb{R}^d$) to $[0,+\infty]$, satisfying $(i)$, a weaker version of $(ii)$ (only subadditivity ) and $(iv)$, is the Lebesgue outer measure $m^*(\cdot)$. Then the job left is to show that with strict additivity and translation invariance , the Lebesgue outer measure $m^*(\cdot)$ is upgraded to the Lebesgue measure $m(\cdot)$, which I cannot complete. Any help is greatly appreciated!",,['real-analysis']
81,Show that there exists no strictly increasing function $f:\mathbb{N}\rightarrow\mathbb{N}$ with $f(2)=3$...,Show that there exists no strictly increasing function  with ...,f:\mathbb{N}\rightarrow\mathbb{N} f(2)=3,"Full exercise: Show that there exists no strictly increasing function $f:\mathbb{N}\rightarrow\mathbb{N}$ with $f(2)=3$ which has the property that $f(mn)=f(m)f(n)$. This is one of the first exercises in Putnam and Beyond , in the section dedicated to the proof by contradiction. I am quite familiar with the proof technique and feel comfortable with the mechanics of the problem but I find a nice trick here quite elusive. If anyone sees the elephant in the room a bit of subtle guidance would be much appreciated. I have the solution on hand but I would rather not look at it (where's the fun?). If you have any general suggestions that come to mind on how to handle proofs of this nature, especially those involving multiplicative homomorphisms between subsets of $\mathbb{R}$, I am all ears. Many thanks! P.S. I forgot to mention that I have tried using the fact that $f$ increasing implies $f(n+1)>f(n)$ for all $n\in\mathbb{N}$ in several ways to produce a contradiction of the $f(2)=3$ condition. Mainly I used the factorization of $n^2-1$ to get $f(n+1)f(n-1)<f(n)f(n)$ from the inequality $f(n^2)>f(n^2-1)$ but did not find anything very helpful in this approach.","Full exercise: Show that there exists no strictly increasing function $f:\mathbb{N}\rightarrow\mathbb{N}$ with $f(2)=3$ which has the property that $f(mn)=f(m)f(n)$. This is one of the first exercises in Putnam and Beyond , in the section dedicated to the proof by contradiction. I am quite familiar with the proof technique and feel comfortable with the mechanics of the problem but I find a nice trick here quite elusive. If anyone sees the elephant in the room a bit of subtle guidance would be much appreciated. I have the solution on hand but I would rather not look at it (where's the fun?). If you have any general suggestions that come to mind on how to handle proofs of this nature, especially those involving multiplicative homomorphisms between subsets of $\mathbb{R}$, I am all ears. Many thanks! P.S. I forgot to mention that I have tried using the fact that $f$ increasing implies $f(n+1)>f(n)$ for all $n\in\mathbb{N}$ in several ways to produce a contradiction of the $f(2)=3$ condition. Mainly I used the factorization of $n^2-1$ to get $f(n+1)f(n-1)<f(n)f(n)$ from the inequality $f(n^2)>f(n^2-1)$ but did not find anything very helpful in this approach.",,"['real-analysis', 'proof-writing', 'contest-math']"
82,"Steps for proving that a sequence converges, using the epsilon definition of convergence","Steps for proving that a sequence converges, using the epsilon definition of convergence",,"I haven't been able to find any sources that clearly and methodically state the approach for proving the convergence of a sequence, using the epsilon definition of convergence. At best, I have been able to find vague, unjustified demonstrations. However, this does nothing to help me learn. I want to be able to generalise this method across all convergence problems that I encounter. I would like someone to state the steps and associated reasoning involved in proving that a sequence converges, using the epsilon definition of convergence. Please specify the reasoning behind each step of the methodology, to assist in justifying your calculations. I would like the 'why' and 'how' behind each step of such a proof. I have the sequence $ \{a_n\}_{n=1}^{\infty}$, where $a_n = \dfrac{(-1)^{n+1}}{n}$, $L = 0$. From what I have read, we want to prove that for any $\epsilon > 0$, there exists some $N > 0$, such that if $n > N$, $|a_n - L| < \epsilon$. However, as alluded to above, I do not fully appreciate or understand what this is saying. Thank you.","I haven't been able to find any sources that clearly and methodically state the approach for proving the convergence of a sequence, using the epsilon definition of convergence. At best, I have been able to find vague, unjustified demonstrations. However, this does nothing to help me learn. I want to be able to generalise this method across all convergence problems that I encounter. I would like someone to state the steps and associated reasoning involved in proving that a sequence converges, using the epsilon definition of convergence. Please specify the reasoning behind each step of the methodology, to assist in justifying your calculations. I would like the 'why' and 'how' behind each step of such a proof. I have the sequence $ \{a_n\}_{n=1}^{\infty}$, where $a_n = \dfrac{(-1)^{n+1}}{n}$, $L = 0$. From what I have read, we want to prove that for any $\epsilon > 0$, there exists some $N > 0$, such that if $n > N$, $|a_n - L| < \epsilon$. However, as alluded to above, I do not fully appreciate or understand what this is saying. Thank you.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
83,Relationship between Taylor and Weierstrass theorem,Relationship between Taylor and Weierstrass theorem,,"I'm not a mathematician, but these two theorems sound related to me. Taylor's theorem. Every k -times differentiable function can be approximated in a neighborhood around a given point by a k -th order polynomial to an arbitrary degree. Weierstrass theorem . Every continuous function defined on a closed interval $[a, b]$ can be approximated to an arbitrary degree by a polynomial function. (The statements are probably not precise.) I always wondered what is the underlying relationship between those two? Does one imply the other, or are they each special cases of some more general result?","I'm not a mathematician, but these two theorems sound related to me. Taylor's theorem. Every k -times differentiable function can be approximated in a neighborhood around a given point by a k -th order polynomial to an arbitrary degree. Weierstrass theorem . Every continuous function defined on a closed interval $[a, b]$ can be approximated to an arbitrary degree by a polynomial function. (The statements are probably not precise.) I always wondered what is the underlying relationship between those two? Does one imply the other, or are they each special cases of some more general result?",,"['real-analysis', 'approximation-theory']"
84,"Real Analysis, Folland Proposition 2.11/Exercise 10 Measurable Functions","Real Analysis, Folland Proposition 2.11/Exercise 10 Measurable Functions",,"Question Proposition 2.11 (Exercise 10) - The following implications are valid if and only if the measure is complete: a.) If $f$ is measurable and $f = g$ $\mu$-a.e., then $g$ is measurable. b.) If $f_n$ is measurable for $n\in \mathbb{N}$ and $f_n\rightarrow f$ $\mu$-a.e., then $f$ is measurable. Attempted Proof a.) We want to show that for every Borel set $B\subset \mathbb{R}$, $g^{-1}(B)$ is measurable. Suppose $\mu$ is complete, since $f = g$ $\mu$-a.e., there exists a measurable set $E$ such that $\mu(E) = 0$, in fact, for all $x\notin E$ $f(x) = g(x)$. Then $$g^{-1}(B) = (g^{-1}(B)\cap E)\cup(f^{-1}(B)\setminus E)$$ since $f$ is measurable we have that $f^{-1}(B)$ is measurable, and since $\mu$ is complete, we have $g^{-1}(B)\cap E$ is measurable. Thus $g^{-1}(B)$ is measurable. Now suppose part a.) holds. Let $N\subset E$, where $E$ has measure zero. Let $f = 1_{E}$ and $g = 1_{N}$ then $f = g$ a.e., so $g$ is measurable. Thus, $g^{-1}(\{1\}) = N$ is measurable. Therefore $\mu$ is complete. Attempted proof b.) We are given $f_n$ to be measurable for $n\in\mathbb{N}$, and $f_n\rightarrow f$ a.e., from proposition 2.7 we can let $$\hat{f} = \lim_{n\rightarrow \infty}\sup f_n$$ Since $f_n$ is measurable then so is $\hat{f}$. Thus given the fact that $f_n\rightarrow f$ e.e. then we have $\hat{f} = f$ a.e., so by part a.) $f$ is measurable. I am not sure how to proceed with the converse of part b.). Any suggestions is greatly appreciated. Background Information Proposition 2.7 - If $\{f_j\}$ is a sequence of $\overline{\mathbb{R}}$-valued measurable functions on $(X,M)$, then the functions \begin{align*} g_1(x) &= \sup_{j}f_j(x) \ \ \ \ \ &g_3(x) = \lim_{j\rightarrow \infty}\sup f_j(x)\\ g_2(x) &= \inf_{j}f_j(x) \ \ \ \ \ &g_4(x) = \lim_{j\rightarrow \infty}\inf f_j(x) \end{align*}   are all measurable. If $f(x) = \lim_{j\rightarrow \infty}f_j(x)$ exists for every $x\in X$, then $f$ is measurable.","Question Proposition 2.11 (Exercise 10) - The following implications are valid if and only if the measure is complete: a.) If $f$ is measurable and $f = g$ $\mu$-a.e., then $g$ is measurable. b.) If $f_n$ is measurable for $n\in \mathbb{N}$ and $f_n\rightarrow f$ $\mu$-a.e., then $f$ is measurable. Attempted Proof a.) We want to show that for every Borel set $B\subset \mathbb{R}$, $g^{-1}(B)$ is measurable. Suppose $\mu$ is complete, since $f = g$ $\mu$-a.e., there exists a measurable set $E$ such that $\mu(E) = 0$, in fact, for all $x\notin E$ $f(x) = g(x)$. Then $$g^{-1}(B) = (g^{-1}(B)\cap E)\cup(f^{-1}(B)\setminus E)$$ since $f$ is measurable we have that $f^{-1}(B)$ is measurable, and since $\mu$ is complete, we have $g^{-1}(B)\cap E$ is measurable. Thus $g^{-1}(B)$ is measurable. Now suppose part a.) holds. Let $N\subset E$, where $E$ has measure zero. Let $f = 1_{E}$ and $g = 1_{N}$ then $f = g$ a.e., so $g$ is measurable. Thus, $g^{-1}(\{1\}) = N$ is measurable. Therefore $\mu$ is complete. Attempted proof b.) We are given $f_n$ to be measurable for $n\in\mathbb{N}$, and $f_n\rightarrow f$ a.e., from proposition 2.7 we can let $$\hat{f} = \lim_{n\rightarrow \infty}\sup f_n$$ Since $f_n$ is measurable then so is $\hat{f}$. Thus given the fact that $f_n\rightarrow f$ e.e. then we have $\hat{f} = f$ a.e., so by part a.) $f$ is measurable. I am not sure how to proceed with the converse of part b.). Any suggestions is greatly appreciated. Background Information Proposition 2.7 - If $\{f_j\}$ is a sequence of $\overline{\mathbb{R}}$-valued measurable functions on $(X,M)$, then the functions \begin{align*} g_1(x) &= \sup_{j}f_j(x) \ \ \ \ \ &g_3(x) = \lim_{j\rightarrow \infty}\sup f_j(x)\\ g_2(x) &= \inf_{j}f_j(x) \ \ \ \ \ &g_4(x) = \lim_{j\rightarrow \infty}\inf f_j(x) \end{align*}   are all measurable. If $f(x) = \lim_{j\rightarrow \infty}f_j(x)$ exists for every $x\in X$, then $f$ is measurable.",,"['real-analysis', 'measure-theory']"
85,"Show that $(L^{p},\|\|_{p})$ is a Banach space.",Show that  is a Banach space.,"(L^{p},\|\|_{p})","Show that $(L^{p},\|\|_{p})$ is a Banach space. My approach: I prove the statement for $(L^{1},\|\|_{1})$ , of the following way, first all, is easy show that $\|\|_{1}$ is a norm. So, $(L^{1},\|\|_{1})$ is vector space. To show that is a Banach space, note that Prop. 1: Let $(f_{n})_{n\in\mathbb{N}}\subset\mathcal{L}^{1}$ (where $\mathcal{L}^{1}=\mathcal{L}^{1}(X,\tau,\mu)$ is the space of all integrals function), such that $\sum_{i}{\|f_{n}\|}_{1}<\infty$ , then the sequences $\left(\sum_{n=1}^{N}{f_{n}}\right)_{n\in\mathbb{N}}$ converges almost everywhere to the integral function, that we called $\sum_{n\in\mathbb{N}}{f_{n}}$ . Furthermore $$\sum_{n\in\mathbb{N}}{\int{f_{n}d\mu}}=\int{\sum_{n\in\mathbb{N}}{f_{n}d\mu}}\quad\lim_{N\to\infty}{\|\sum_{n=1}^{N}{f_{n}}-\sum_{n\in\mathbb{N}}{f_{n}}\|_{1}}=0$$ Prop. 2: Let $(E,\|\|)$ a normed vector space. Then $E$ is a Banach space if and only if for all $(e_{k})_{k\in\mathbb{N}}\subset E$ such that $\sum_{k\in\mathbb{N}}{\|e_{k}\|}<\infty$ , $\left(\sum_{k=1}^{n}{e_{k}}\right)_{n\in\mathbb{N}}$ converges in $E$ We take the proposition 2. Let $(f_{n})_{n\in\mathbb{N}}\subset L^{1}$ such that $\sum_{n}{\|\hat{f}_{n}\|_{1}}<\infty$ . Let $f_{n}\in\hat{f}_{n}$ , then $\|f_{n}\|_{1}=\|\hat{f}_{n}\|_{1}$ and then $\sum_{n}{\|f_{n}\|_{1}}<\infty$ , by prop. 1, there exist $F\in\mathcal{L}^{1}$ such that $\|\sum_{n=1}^{N}{f_{n}-F\|_{1}}\to 0$ , and then $\|\sum_{n=1}^{N}{\hat{f}_{n}-F\|_{1}}\to 0$ . Finally note that $\hat{F}\in L^{1}$ is a Banach space. This was my answer for $L^{1}$ space, but how I prove the general statement for $L^{p}$ (I want to do a similar response). Edit: I'm stuck in the problem, any idea or hint is appreciated.Thanks!!","Show that is a Banach space. My approach: I prove the statement for , of the following way, first all, is easy show that is a norm. So, is vector space. To show that is a Banach space, note that Prop. 1: Let (where is the space of all integrals function), such that , then the sequences converges almost everywhere to the integral function, that we called . Furthermore Prop. 2: Let a normed vector space. Then is a Banach space if and only if for all such that , converges in We take the proposition 2. Let such that . Let , then and then , by prop. 1, there exist such that , and then . Finally note that is a Banach space. This was my answer for space, but how I prove the general statement for (I want to do a similar response). Edit: I'm stuck in the problem, any idea or hint is appreciated.Thanks!!","(L^{p},\|\|_{p}) (L^{1},\|\|_{1}) \|\|_{1} (L^{1},\|\|_{1}) (f_{n})_{n\in\mathbb{N}}\subset\mathcal{L}^{1} \mathcal{L}^{1}=\mathcal{L}^{1}(X,\tau,\mu) \sum_{i}{\|f_{n}\|}_{1}<\infty \left(\sum_{n=1}^{N}{f_{n}}\right)_{n\in\mathbb{N}} \sum_{n\in\mathbb{N}}{f_{n}} \sum_{n\in\mathbb{N}}{\int{f_{n}d\mu}}=\int{\sum_{n\in\mathbb{N}}{f_{n}d\mu}}\quad\lim_{N\to\infty}{\|\sum_{n=1}^{N}{f_{n}}-\sum_{n\in\mathbb{N}}{f_{n}}\|_{1}}=0 (E,\|\|) E (e_{k})_{k\in\mathbb{N}}\subset E \sum_{k\in\mathbb{N}}{\|e_{k}\|}<\infty \left(\sum_{k=1}^{n}{e_{k}}\right)_{n\in\mathbb{N}} E (f_{n})_{n\in\mathbb{N}}\subset L^{1} \sum_{n}{\|\hat{f}_{n}\|_{1}}<\infty f_{n}\in\hat{f}_{n} \|f_{n}\|_{1}=\|\hat{f}_{n}\|_{1} \sum_{n}{\|f_{n}\|_{1}}<\infty F\in\mathcal{L}^{1} \|\sum_{n=1}^{N}{f_{n}-F\|_{1}}\to 0 \|\sum_{n=1}^{N}{\hat{f}_{n}-F\|_{1}}\to 0 \hat{F}\in L^{1} L^{1} L^{p}","['real-analysis', 'functional-analysis', 'measure-theory', 'proof-verification', 'lp-spaces']"
86,Provide examples or explain why it is impossible,Provide examples or explain why it is impossible,,"a) A continuous function defined on an open interval with range equal to a closed interval. My example: $f(x)=\frac{1}{2}\sin(4\pi x)+\frac{1}{2}$ on $(0,1)$ to $[0,1]$. Note: I am not considering $\mathbb{R}$ an interval. b) A continuous function defined on a closed interval with range equal to an open interval. I think this is impossible if we exclude $\mathbb{R}$. Edit: we must also exclude unbounded intervals. By the Extreme Value Theorem, any  continuous function on a compact set attains a maximum and a minimum. Yet, the set of the points in an open interval doesn't include its supremum and infimum, a contradiction. c) A continuous function defined on an open interval with range equal to an unbounded closed set different from $\mathbb{R}$. My example: $f(x)=\sqrt{|x|}$ on $\mathbb{R}$ to [0,$\infty$). Is there another function that works and has a different domain than $\mathbb{R}$? d) A continuous defined on all of $\mathbb{R}$ with range equal to $\mathbb{Q}$. I was thinking maybe map the natural numbers to $\mathbb{Q}$ and use the rest to ""fill in the gaps."" Evidently, I need most help with d). Thanks in advance!","a) A continuous function defined on an open interval with range equal to a closed interval. My example: $f(x)=\frac{1}{2}\sin(4\pi x)+\frac{1}{2}$ on $(0,1)$ to $[0,1]$. Note: I am not considering $\mathbb{R}$ an interval. b) A continuous function defined on a closed interval with range equal to an open interval. I think this is impossible if we exclude $\mathbb{R}$. Edit: we must also exclude unbounded intervals. By the Extreme Value Theorem, any  continuous function on a compact set attains a maximum and a minimum. Yet, the set of the points in an open interval doesn't include its supremum and infimum, a contradiction. c) A continuous function defined on an open interval with range equal to an unbounded closed set different from $\mathbb{R}$. My example: $f(x)=\sqrt{|x|}$ on $\mathbb{R}$ to [0,$\infty$). Is there another function that works and has a different domain than $\mathbb{R}$? d) A continuous defined on all of $\mathbb{R}$ with range equal to $\mathbb{Q}$. I was thinking maybe map the natural numbers to $\mathbb{Q}$ and use the rest to ""fill in the gaps."" Evidently, I need most help with d). Thanks in advance!",,"['real-analysis', 'general-topology']"
87,Is there a closed form for these polynomials?,Is there a closed form for these polynomials?,,"Let $P_0(x)=1, P_{-1}(x)=0$ and define via recursion $P_{n+1}(x)=xP_{n}(x)-P_{n-1}(x)$. The first few polynomials are $$ P_0(x)= 1\\ P_1(x) = x \\ P_2(x) = x^2-1 \\ P_3(x)= x^3 -2 x\\ P_4(x) = x^4 - 3 x^2 +1 \\ P_5(x) = x^5 - 4 x^3 +3 x$$ It appears the polynomials are always the form: $$P_n(x)=\sum_{k=0}^{\lfloor n/2\rfloor+1} (-1)^k\ \ f(n,k)\ x^{n-2k}$$ Where for example one can calculate $$f(n,0)=1 \\ f(n,1)=n-1 \\ f(n,2)=\frac{(n-2)(n-3)}{2}$$ Is there a closed form for these polynomials? Aside from that I am specifically interested in whether or not $\sum_{n=0}^\infty |P_n(x)|^2$ diverges for every $x \in \mathbb{C}$.","Let $P_0(x)=1, P_{-1}(x)=0$ and define via recursion $P_{n+1}(x)=xP_{n}(x)-P_{n-1}(x)$. The first few polynomials are $$ P_0(x)= 1\\ P_1(x) = x \\ P_2(x) = x^2-1 \\ P_3(x)= x^3 -2 x\\ P_4(x) = x^4 - 3 x^2 +1 \\ P_5(x) = x^5 - 4 x^3 +3 x$$ It appears the polynomials are always the form: $$P_n(x)=\sum_{k=0}^{\lfloor n/2\rfloor+1} (-1)^k\ \ f(n,k)\ x^{n-2k}$$ Where for example one can calculate $$f(n,0)=1 \\ f(n,1)=n-1 \\ f(n,2)=\frac{(n-2)(n-3)}{2}$$ Is there a closed form for these polynomials? Aside from that I am specifically interested in whether or not $\sum_{n=0}^\infty |P_n(x)|^2$ diverges for every $x \in \mathbb{C}$.",,"['real-analysis', 'polynomials', 'closed-form']"
88,"Can the function $f(x,y) = \frac{xy}{\sqrt{x^2+y^2}}$ be defined so that f is differentiable at the origin?",Can the function  be defined so that f is differentiable at the origin?,"f(x,y) = \frac{xy}{\sqrt{x^2+y^2}}","Can the function $f(x,y) = \frac{xy}{\sqrt{x^2+y^2}}$ be defined so that $f$ is continuous and differentiable at the origin? I redefined the function piecewise so that $f=0$ at the origin and $f = \frac{xy}{\sqrt{x^2+y^2}}$ otherwise. This made the function continuous since $\displaystyle\lim_{(x,y) \to (0,0) } \frac{xy}{\sqrt{x^2+y^2}} = 0$. However, I'm having a difficult time of proving whether or not f is differentiable. I'm pretty sure it's not. I tried to do this by showing that the partial derivatives are not continuous at the origin, but I got stuck. Thanks!","Can the function $f(x,y) = \frac{xy}{\sqrt{x^2+y^2}}$ be defined so that $f$ is continuous and differentiable at the origin? I redefined the function piecewise so that $f=0$ at the origin and $f = \frac{xy}{\sqrt{x^2+y^2}}$ otherwise. This made the function continuous since $\displaystyle\lim_{(x,y) \to (0,0) } \frac{xy}{\sqrt{x^2+y^2}} = 0$. However, I'm having a difficult time of proving whether or not f is differentiable. I'm pretty sure it's not. I tried to do this by showing that the partial derivatives are not continuous at the origin, but I got stuck. Thanks!",,"['real-analysis', 'limits', 'multivariable-calculus', 'derivatives', 'continuity']"
89,How can I prove $x_{n+1} = e^{-x_n}$ is convergent?,How can I prove  is convergent?,x_{n+1} = e^{-x_n},"I'm doing a practice problem which asks to prove that the sequence defined by $x_{n+1} = e^{-x_n}$ is convergent (or rather ""study the convergence of $(x_n)$""). So I'd like to try and find some sufficient condition on $x_0$ for the sequence to converge. I can see that $e^{-x}$ is $k$-lipschitzian with $k<1$ on $[a, \infty[$ for $a>0$. But the problem is that $e^{-x}$ does not map $[a, \infty[$ into itself. I started trying to find values of $a, b$ such that $[e^{-b}, e^{-a}]\subseteq[a, b]$, but then I wondered if maybe there was some simpler approach that I was missing.","I'm doing a practice problem which asks to prove that the sequence defined by $x_{n+1} = e^{-x_n}$ is convergent (or rather ""study the convergence of $(x_n)$""). So I'd like to try and find some sufficient condition on $x_0$ for the sequence to converge. I can see that $e^{-x}$ is $k$-lipschitzian with $k<1$ on $[a, \infty[$ for $a>0$. But the problem is that $e^{-x}$ does not map $[a, \infty[$ into itself. I started trying to find values of $a, b$ such that $[e^{-b}, e^{-a}]\subseteq[a, b]$, but then I wondered if maybe there was some simpler approach that I was missing.",,"['real-analysis', 'fixed-point-theorems']"
90,Is there standard terminology to describe the not-quite-a-limit behavior of ${\tan( \log x) \over x}$ as $x$ approaches infinity?,Is there standard terminology to describe the not-quite-a-limit behavior of  as  approaches infinity?,{\tan( \log x) \over x} x,"Suppose I want to describe the long term behavior of ${\tan(\log x) \over x}$ as x increases towards positive real infinity. Now, $$\lim_{x \rightarrow \infty}{\tan(\log x) \over x}$$ obviously doesn't exist.  So it would be wrong to say its limit is 0. But in some very slightly looser sense, the term obviously approaches 0 aside from the very occasional vertical asymptote.  If you were to pick a point at ""random"" far, far down the number line (I'm being very imprecise here, I know), it would be an $\epsilon$ from 0 with a probability approaching 1 as the random range you were pulling from got larger.  Various summability methods would also make this fact clear. Is there either standard terminology for getting this idea across, or standard notation for expressing it?","Suppose I want to describe the long term behavior of ${\tan(\log x) \over x}$ as x increases towards positive real infinity. Now, $$\lim_{x \rightarrow \infty}{\tan(\log x) \over x}$$ obviously doesn't exist.  So it would be wrong to say its limit is 0. But in some very slightly looser sense, the term obviously approaches 0 aside from the very occasional vertical asymptote.  If you were to pick a point at ""random"" far, far down the number line (I'm being very imprecise here, I know), it would be an $\epsilon$ from 0 with a probability approaching 1 as the random range you were pulling from got larger.  Various summability methods would also make this fact clear. Is there either standard terminology for getting this idea across, or standard notation for expressing it?",,"['calculus', 'real-analysis', 'limits', 'terminology', 'asymptotics']"
91,A misunderstanding concerning $\pi$,A misunderstanding concerning,\pi,"The very well-known expression $$\frac {\pi} {4} = 1 - \frac {1} {3} + \frac {1} {5} - \frac {1} {7} + \cdots$$ puts me face to face with a contradictory position. Let $$s_N = \sum_{k = 0}^{N} \frac {1} {4k + 1} - \sum_{k = 0}^{N} \frac {1} {4k + 3}.$$ Then it is obvious that $$\frac {\pi} {4} = \lim_{N \to \infty} s_N.$$ By Euler-MacLaurin summation formula, $$\sum_{k = 0}^{N} \frac {1} {4k + 1} = \int_{0}^{N} \frac {dx} {4x + 1} + \frac {1} {2} \left(1 + \frac {1} {4N + 1} \right) + o \left (\frac {1} {N^2} \right)$$ and $$\sum_{k = 0}^{N} \frac {1} {4k + 3} = \int_{0}^{N} \frac {dx} {4x + 3} + \frac {1} {2} \left(\frac {1} {3} + \frac {1} {4N + 3} \right) + o \left (\frac {1} {N^2} \right).$$ We then have $$s_N = \frac {1} {4} \log \left(3 - \frac {6} {4N + 3} \right) + \frac {1} {3} + \frac {1} {(4N + 1) (4N + 3)} + o \left (\frac {1} {N^2} \right)$$ and $$\lim_{N \to \infty} s_N = \frac {\log 3} {4} + \frac {1} {3}.$$ But $\frac {\log 3} {4} + \frac {1} {3} \ne \frac {\pi} {4}$. How come? Where have I done the mistake?","The very well-known expression $$\frac {\pi} {4} = 1 - \frac {1} {3} + \frac {1} {5} - \frac {1} {7} + \cdots$$ puts me face to face with a contradictory position. Let $$s_N = \sum_{k = 0}^{N} \frac {1} {4k + 1} - \sum_{k = 0}^{N} \frac {1} {4k + 3}.$$ Then it is obvious that $$\frac {\pi} {4} = \lim_{N \to \infty} s_N.$$ By Euler-MacLaurin summation formula, $$\sum_{k = 0}^{N} \frac {1} {4k + 1} = \int_{0}^{N} \frac {dx} {4x + 1} + \frac {1} {2} \left(1 + \frac {1} {4N + 1} \right) + o \left (\frac {1} {N^2} \right)$$ and $$\sum_{k = 0}^{N} \frac {1} {4k + 3} = \int_{0}^{N} \frac {dx} {4x + 3} + \frac {1} {2} \left(\frac {1} {3} + \frac {1} {4N + 3} \right) + o \left (\frac {1} {N^2} \right).$$ We then have $$s_N = \frac {1} {4} \log \left(3 - \frac {6} {4N + 3} \right) + \frac {1} {3} + \frac {1} {(4N + 1) (4N + 3)} + o \left (\frac {1} {N^2} \right)$$ and $$\lim_{N \to \infty} s_N = \frac {\log 3} {4} + \frac {1} {3}.$$ But $\frac {\log 3} {4} + \frac {1} {3} \ne \frac {\pi} {4}$. How come? Where have I done the mistake?",,['real-analysis']
92,"Discuss the convergence of $\int_0^\infty x \sin e^x \, dx$",Discuss the convergence of,"\int_0^\infty x \sin e^x \, dx","$$\int_0^\infty x \sin e^x \, dx$$ I have tried applying the Dirichlet test, Comparison Principle, integration by parts and substitution, but all have failed. None of these prove that the integral is divergent though, so I'm not really sure how to show that this converges/diverges. My work: Dirichlet: Fails, because neither $f(x)=x$ nor $g(x)=\sin e^x$ goes to zero Comparison: Fails. $\sin e^x \le1$, therefore $\int_0^\infty x \, dx\ge \int_0^\infty x \sin e^x \, dx$. However, $\int_0^\infty x \, dx$ does not converge, so this idea is unhelpful IBP: $\int_a^b FG'=(F(b)G(b)-F(a)G(a))-\int_a^bGF'$ $$F=x,\quad F'= dx,\quad G' = \sin e^x, \quad G =\text{?}$$ Substitution: $u(x)=e^x$ $du=e^x$ therefore: $$\int_0^\infty x \sin e^x \, dx=\int_0^{\infty} \ln {u(x)} \sin u(x) \, dx$$ From here, you can use IBP, resulting in: $$F=\ln(u), \quad F'=\frac 1x, \quad G'= \sin u(x), \quad G = -\cos u(x)\cdot u'(x)$$ $$-\ln u(x) \cos u(x) u'(x)|_0^\infty-\int_0^\infty \frac{-\cos u(x) u'(x)}{u(x)}$$ But I feel like this integral is far too complicated for the scope of the question. Additionally, $\ln\infty$ would go to infinity anyway, so I feel like that is not an acceptable way to solve the problem. Graphically, my calculator says that the integral should be equal to 0.411229, a number which appears to have no numerical significance. Is there any other way to integrate this function?","$$\int_0^\infty x \sin e^x \, dx$$ I have tried applying the Dirichlet test, Comparison Principle, integration by parts and substitution, but all have failed. None of these prove that the integral is divergent though, so I'm not really sure how to show that this converges/diverges. My work: Dirichlet: Fails, because neither $f(x)=x$ nor $g(x)=\sin e^x$ goes to zero Comparison: Fails. $\sin e^x \le1$, therefore $\int_0^\infty x \, dx\ge \int_0^\infty x \sin e^x \, dx$. However, $\int_0^\infty x \, dx$ does not converge, so this idea is unhelpful IBP: $\int_a^b FG'=(F(b)G(b)-F(a)G(a))-\int_a^bGF'$ $$F=x,\quad F'= dx,\quad G' = \sin e^x, \quad G =\text{?}$$ Substitution: $u(x)=e^x$ $du=e^x$ therefore: $$\int_0^\infty x \sin e^x \, dx=\int_0^{\infty} \ln {u(x)} \sin u(x) \, dx$$ From here, you can use IBP, resulting in: $$F=\ln(u), \quad F'=\frac 1x, \quad G'= \sin u(x), \quad G = -\cos u(x)\cdot u'(x)$$ $$-\ln u(x) \cos u(x) u'(x)|_0^\infty-\int_0^\infty \frac{-\cos u(x) u'(x)}{u(x)}$$ But I feel like this integral is far too complicated for the scope of the question. Additionally, $\ln\infty$ would go to infinity anyway, so I feel like that is not an acceptable way to solve the problem. Graphically, my calculator says that the integral should be equal to 0.411229, a number which appears to have no numerical significance. Is there any other way to integrate this function?",,"['calculus', 'real-analysis', 'integration', 'convergence-divergence']"
93,Convergence of $\sum_{n=0}^{\infty} \left(\frac{1+\frac 12+\ldots+\frac 1n}{n}\right)^p$,Convergence of,\sum_{n=0}^{\infty} \left(\frac{1+\frac 12+\ldots+\frac 1n}{n}\right)^p,"Prove that $$\sum_{n=0}^{\infty}  \left(\frac{1+\frac 12+\ldots+\frac 1n}{n}\right)^p$$ converges if $p>1$ and diverges when $0<p\leqslant1$ . My attempt : Since the terms of the series $\sum_{n=0}^{\infty} \frac1n$ are monotonically decreasing and positive, I applied Cauchy's Condensation theorem , i.e, if $\sum a(n)$ and $\sum2^na(2^n)$ will converge and diverge together. Applying this I got, $\sum\left(1-\frac{1}{2^n}\right)^p$ , but I am stuck now. Don't know how to proceed further.","Prove that converges if and diverges when . My attempt : Since the terms of the series are monotonically decreasing and positive, I applied Cauchy's Condensation theorem , i.e, if and will converge and diverge together. Applying this I got, , but I am stuck now. Don't know how to proceed further.",\sum_{n=0}^{\infty}  \left(\frac{1+\frac 12+\ldots+\frac 1n}{n}\right)^p p>1 0<p\leqslant1 \sum_{n=0}^{\infty} \frac1n \sum a(n) \sum2^na(2^n) \sum\left(1-\frac{1}{2^n}\right)^p,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
94,Uniformly integrable implies integrable?,Uniformly integrable implies integrable?,,"The term ""uniformly integrable"" sounds (to a layman like me) to be stronger than integrable. Just like how uniformly convergent is stronger than simply being convergent. However, from the definition of uniformly integrable, I can't seem to see how if a family of measurable functions is uniformly integrable, then each function is integrable ($\int f_n <\infty$). Thanks for any help! I am sure it is quite trivial but I can't see it immediately. I post the definition of uniformly integrable as stated in my lecture notes: A family $\mathcal{H}$ of measurable functions on $E$ is said to be uniformly integrable over $E$ provided for each $\epsilon >0$, there is a $\delta>0$ such that for all $f\in\mathcal{H}$, if $A\subseteq E$ is measurable with $m(A)<\delta$, then $\int_A |f|<\epsilon$.","The term ""uniformly integrable"" sounds (to a layman like me) to be stronger than integrable. Just like how uniformly convergent is stronger than simply being convergent. However, from the definition of uniformly integrable, I can't seem to see how if a family of measurable functions is uniformly integrable, then each function is integrable ($\int f_n <\infty$). Thanks for any help! I am sure it is quite trivial but I can't see it immediately. I post the definition of uniformly integrable as stated in my lecture notes: A family $\mathcal{H}$ of measurable functions on $E$ is said to be uniformly integrable over $E$ provided for each $\epsilon >0$, there is a $\delta>0$ such that for all $f\in\mathcal{H}$, if $A\subseteq E$ is measurable with $m(A)<\delta$, then $\int_A |f|<\epsilon$.",,"['real-analysis', 'analysis', 'measure-theory']"
95,Show that $2 \int f^2 \leq \int |f'| \cdot \int |f|$,Show that,2 \int f^2 \leq \int |f'| \cdot \int |f|,"Let $f(x)$ be a continuously differentiable function defined on closed interval $[0, 1]$ for which$$\int_0^1 f(x)\,dx = 0.$$How do I show that$$2 \int_0^1 f(x)^2\,dx \le \int_0^1 |f'(x)|\,dx \cdot \int_0^1 |f(x)|\,dx?$$","Let $f(x)$ be a continuously differentiable function defined on closed interval $[0, 1]$ for which$$\int_0^1 f(x)\,dx = 0.$$How do I show that$$2 \int_0^1 f(x)^2\,dx \le \int_0^1 |f'(x)|\,dx \cdot \int_0^1 |f(x)|\,dx?$$",,"['calculus', 'real-analysis']"
96,How to compute$\int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx$,How to compute,\int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx,"How to compute   $$\int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx$$   I'm interested in more ways of computing this integral. My Thoughts \begin{align} \int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx&=\int_{0}^{1} \:\ln \left(x\right)\frac{x}{\left(x^2+1\right)^2}dx\\ &=\int_{0}^{1} \:\ln \left(x\right)\frac{x}{\left(x^2+1\right)^2}dx\\ \mathrm{Apply\:Integration\:By\:Parts}: \end{align} $$\fbox{$u=\ln \left(x\right),\:\:u'=\frac{1}{x},\:\:v'=\frac{1}{\left(x^2+1\right)^2}x,\:\:v=-\frac{1}{2\left(x^2+1\right)}$ }$$ \begin{align} \int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx& =\ln \left(x\right)\left(-\frac{1}{2\left(x^2+1\right)}\right)\biggl|_{0}^{1}-\int_{0}^{1} \frac{1}{x}\left(-\frac{1}{2\left(x^2+1\right)}\right)dx\\ &=-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1}+\int_{0}^{1} \:\frac{1}{2x\left(x^2+1\right)}dx\\ &=-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1}+\dfrac{1}{2}\int_{0}^{1} \:\frac{1}{x\left(x^2+1\right)}dx\\ \end{align} now let's calculate: \begin{align} \int_{0}^{1} \frac{1}{x\left(x^2+1\right)}dx&=\int_{0}^{1} \frac{1}{x\left(x^2+1\right)}dx\\ &=\int_{0}^{1} \frac{1}{x}-\frac{x}{x^2+1}dx\\ &=\ln \left(x\right)\biggl|_{0}^{1}-\dfrac{1}{2}\int_{0}^{1} \frac{2x}{x^2+1}dx\\ &=\ln \left(x\right)\biggl|_{0}^{1}-\dfrac{1}{2}\int_{0}^{1} \frac{(x^2+1)'}{x^2+1}dx\\ &=\ln \left(x\right)\biggl|_{0}^{1}-\frac{1}{2}\ln \left(x^2+1\right)\biggl|_{0}^{1}\\ \int_{0}^{1} \frac{1}{x\left(x^2+1\right)}dx&=\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)\biggl|_{0}^{1}\\ \end{align} then $$\fbox{$\int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx=-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1}+\dfrac{1}{2}\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)\biggl|_{0}^{1}$}$$ \begin{align} \int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx&=-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1}+\dfrac{1}{2}\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)\biggl|_{0}^{1} \\ &=\frac{1}{2}\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1} \\ \end{align} or the limit of  \begin{align} \lim _{x\to \:0+}\left(\frac{1}{2}\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\right)&=\lim _{x\to \:0+}\left(\frac{2x^2\ln \left(x\right)-x^2\ln \left(x^2+1\right)-\ln \left(x^2+1\right)}{4\left(x^2+1\right)}\right)\\ &=\dfrac{0}{4}=0 \end{align} and $$\frac{1}{2}\left(\ln \left(1\right)-\frac{1}{2}\ln \left(1^2+1\right)\right)-\frac{\ln \left(1\right)}{2\left(1^2+1\right)}=\dfrac{-\ln(2)}{4} $$ Finaly $$\fbox{$\int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx=\dfrac{-\ln(2)}{4} $} $$","How to compute   $$\int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx$$   I'm interested in more ways of computing this integral. My Thoughts \begin{align} \int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx&=\int_{0}^{1} \:\ln \left(x\right)\frac{x}{\left(x^2+1\right)^2}dx\\ &=\int_{0}^{1} \:\ln \left(x\right)\frac{x}{\left(x^2+1\right)^2}dx\\ \mathrm{Apply\:Integration\:By\:Parts}: \end{align} $$\fbox{$u=\ln \left(x\right),\:\:u'=\frac{1}{x},\:\:v'=\frac{1}{\left(x^2+1\right)^2}x,\:\:v=-\frac{1}{2\left(x^2+1\right)}$ }$$ \begin{align} \int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx& =\ln \left(x\right)\left(-\frac{1}{2\left(x^2+1\right)}\right)\biggl|_{0}^{1}-\int_{0}^{1} \frac{1}{x}\left(-\frac{1}{2\left(x^2+1\right)}\right)dx\\ &=-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1}+\int_{0}^{1} \:\frac{1}{2x\left(x^2+1\right)}dx\\ &=-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1}+\dfrac{1}{2}\int_{0}^{1} \:\frac{1}{x\left(x^2+1\right)}dx\\ \end{align} now let's calculate: \begin{align} \int_{0}^{1} \frac{1}{x\left(x^2+1\right)}dx&=\int_{0}^{1} \frac{1}{x\left(x^2+1\right)}dx\\ &=\int_{0}^{1} \frac{1}{x}-\frac{x}{x^2+1}dx\\ &=\ln \left(x\right)\biggl|_{0}^{1}-\dfrac{1}{2}\int_{0}^{1} \frac{2x}{x^2+1}dx\\ &=\ln \left(x\right)\biggl|_{0}^{1}-\dfrac{1}{2}\int_{0}^{1} \frac{(x^2+1)'}{x^2+1}dx\\ &=\ln \left(x\right)\biggl|_{0}^{1}-\frac{1}{2}\ln \left(x^2+1\right)\biggl|_{0}^{1}\\ \int_{0}^{1} \frac{1}{x\left(x^2+1\right)}dx&=\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)\biggl|_{0}^{1}\\ \end{align} then $$\fbox{$\int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx=-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1}+\dfrac{1}{2}\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)\biggl|_{0}^{1}$}$$ \begin{align} \int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx&=-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1}+\dfrac{1}{2}\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)\biggl|_{0}^{1} \\ &=\frac{1}{2}\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\biggl|_{0}^{1} \\ \end{align} or the limit of  \begin{align} \lim _{x\to \:0+}\left(\frac{1}{2}\left(\ln \left(x\right)-\frac{1}{2}\ln \left(x^2+1\right)\right)-\frac{\ln \left(x\right)}{2\left(x^2+1\right)}\right)&=\lim _{x\to \:0+}\left(\frac{2x^2\ln \left(x\right)-x^2\ln \left(x^2+1\right)-\ln \left(x^2+1\right)}{4\left(x^2+1\right)}\right)\\ &=\dfrac{0}{4}=0 \end{align} and $$\frac{1}{2}\left(\ln \left(1\right)-\frac{1}{2}\ln \left(1^2+1\right)\right)-\frac{\ln \left(1\right)}{2\left(1^2+1\right)}=\dfrac{-\ln(2)}{4} $$ Finaly $$\fbox{$\int_{0}^{1}\dfrac{x\ln(x)}{(x^2+1)^2}dx=\dfrac{-\ln(2)}{4} $} $$",,"['calculus', 'real-analysis', 'integration', 'contest-math']"
97,Proving a convergence relationship between two sequences,Proving a convergence relationship between two sequences,,Let $a_{n}$ a sequence of real numbers. Let $\sigma_n= \frac{a_1+a_2+...+a_n}{n}$. Suppose that $\lim_{n\to \infty} \sigma_n=A.$ Prove that $$\lim_{n \to \infty}\frac{1}{\log n}  \sum_{j=1}^{n}\frac{a_j}{j}=A$$,Let $a_{n}$ a sequence of real numbers. Let $\sigma_n= \frac{a_1+a_2+...+a_n}{n}$. Suppose that $\lim_{n\to \infty} \sigma_n=A.$ Prove that $$\lim_{n \to \infty}\frac{1}{\log n}  \sum_{j=1}^{n}\frac{a_j}{j}=A$$,,"['real-analysis', 'sequences-and-series', 'limits', 'real-numbers']"
98,"Orthonormal basis for $\mathcal{L}^2([0,1])$",Orthonormal basis for,"\mathcal{L}^2([0,1])","$\textbf{Theorem:}$ The orthonormal family $\{e_n(x):\, n\in\mathbb{N}\}$ , where $e_n(x)=e^{2\pi inx}$ , is a basis for $\mathcal{L}^2([0,1])$ . In this case, $\{e_n(x):\, n\in\mathbb{N}\}$ being a basis would mean that any $f\in\mathcal{L}^2([0,1])$ can be written in the form $$f=\sum^\infty_{k=0} \hat{f}(k)e_k(x)$$ where $$\hat{f}(k)=\langle f,e_k\rangle =\int_{[0,1]} f(x)\overline{e_k(x)} \ \text{d}x$$ I am attempting to get a solution in which we can say $$\left\vert\left\vert f-\sum^k_{k=0}\hat{f}(k)e_k(x)\right\vert\right\vert\rightarrow 0 \ \ \text{as} \ \ n\rightarrow \infty$$ via Parsevals and Plancheral Identities, but I have been unable to do so. Any hints please?","The orthonormal family , where , is a basis for . In this case, being a basis would mean that any can be written in the form where I am attempting to get a solution in which we can say via Parsevals and Plancheral Identities, but I have been unable to do so. Any hints please?","\textbf{Theorem:} \{e_n(x):\, n\in\mathbb{N}\} e_n(x)=e^{2\pi inx} \mathcal{L}^2([0,1]) \{e_n(x):\, n\in\mathbb{N}\} f\in\mathcal{L}^2([0,1]) f=\sum^\infty_{k=0} \hat{f}(k)e_k(x) \hat{f}(k)=\langle f,e_k\rangle =\int_{[0,1]} f(x)\overline{e_k(x)} \ \text{d}x \left\vert\left\vert f-\sum^k_{k=0}\hat{f}(k)e_k(x)\right\vert\right\vert\rightarrow 0 \ \ \text{as} \ \ n\rightarrow \infty","['real-analysis', 'functional-analysis', 'fourier-analysis', 'hilbert-spaces', 'fourier-series']"
99,If $D$ is a dense linear subspace of $X$ then $D\to Y$ extends to $X\to Y$ uniquely,If  is a dense linear subspace of  then  extends to  uniquely,D X D\to Y X\to Y,"I am trying to prove the following, but I am not confident in my work. Let $D$ be a linear subspace of a normed space $X$ that is dense   in $X$. Let $Y$ be a Banach space. Show that any bounded linear operator $T : D \to Y$ has a unique extension to a bounded linear operator $X \to  Y.$ What I tried. I can prove the lemma Lemma: Assume that $D$ is a dense subset of $X_1$, that $(X_2, d_2)$ is complete and that $f : D → M_2$ is a uniformly continuous mapping. Then there exists a unique continuous mapping $F : M_1 → M_2$ such that $F|_D = f$. Bounded linear operators are uniformly continuous the existence and uniqueness of a continuous extension follows from the Lemma. If I am correct, I can use the proof of the lemma to construct the desired extension.","I am trying to prove the following, but I am not confident in my work. Let $D$ be a linear subspace of a normed space $X$ that is dense   in $X$. Let $Y$ be a Banach space. Show that any bounded linear operator $T : D \to Y$ has a unique extension to a bounded linear operator $X \to  Y.$ What I tried. I can prove the lemma Lemma: Assume that $D$ is a dense subset of $X_1$, that $(X_2, d_2)$ is complete and that $f : D → M_2$ is a uniformly continuous mapping. Then there exists a unique continuous mapping $F : M_1 → M_2$ such that $F|_D = f$. Bounded linear operators are uniformly continuous the existence and uniqueness of a continuous extension follows from the Lemma. If I am correct, I can use the proof of the lemma to construct the desired extension.",,"['real-analysis', 'functional-analysis', 'proof-verification']"
