,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,a matrix of rank $r$ satisfies a polynomial of degree $r+1$.,a matrix of rank  satisfies a polynomial of degree .,r r+1,"Let $M$ be an $n\times n$ matrix with coefficients in $\mathbb C$. Suppose $M$ has rank $r$ with $r<n$. Prove there is a polynomial $P(x)$ with degree $r+1$ and coefficients in $\mathbb C$ such that $P(M)$ is the zero matrix. I think this has to do with the minimal/characteristic polynomials, although I have not yet seen this in school. I would appreciate a comprehensive solution, hopefully referencing the theorems as they are used.","Let $M$ be an $n\times n$ matrix with coefficients in $\mathbb C$. Suppose $M$ has rank $r$ with $r<n$. Prove there is a polynomial $P(x)$ with degree $r+1$ and coefficients in $\mathbb C$ such that $P(M)$ is the zero matrix. I think this has to do with the minimal/characteristic polynomials, although I have not yet seen this in school. I would appreciate a comprehensive solution, hopefully referencing the theorems as they are used.",,"['linear-algebra', 'polynomials', 'complex-numbers', 'contest-math', 'minimal-polynomials']"
1,Eigenvalues of linear operator $F(A) = AB + BA$,Eigenvalues of linear operator,F(A) = AB + BA,"Let $B$ be the $n \times n$ square matrix; $\lambda_1, \lambda_2, \dots, \lambda_n$ are its pairwise distinct eigenvalues. For all $n \times n$ matrix $A$ let me define $F(A) = AB + BA$. We can consider $F$ as a linear operator, because $F(\alpha X + \beta Y) = \alpha F(x) + \beta F(y)$. What eigenvalues does $F$ have? Any help would be appreciated.","Let $B$ be the $n \times n$ square matrix; $\lambda_1, \lambda_2, \dots, \lambda_n$ are its pairwise distinct eigenvalues. For all $n \times n$ matrix $A$ let me define $F(A) = AB + BA$. We can consider $F$ as a linear operator, because $F(\alpha X + \beta Y) = \alpha F(x) + \beta F(y)$. What eigenvalues does $F$ have? Any help would be appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
2,Prove Cauchy-Schwarz equality.,Prove Cauchy-Schwarz equality.,,My professor asked me to prove the equality in Cauchy-Schwarz inequality. The equality holds iff the vectors $v$ and $u$ are linearly dependent. I am able to show the equality using the fact $v$ and $u$ are linearly dependent. But I don't know how to show the converse (i.e Showing the linear dependence using the equality).,My professor asked me to prove the equality in Cauchy-Schwarz inequality. The equality holds iff the vectors $v$ and $u$ are linearly dependent. I am able to show the equality using the fact $v$ and $u$ are linearly dependent. But I don't know how to show the converse (i.e Showing the linear dependence using the equality).,,"['linear-algebra', 'inequality']"
3,Solve for $X$ in $Y = X^TAX$.,Solve for  in .,X Y = X^TAX,"Suppose $$ Y = X^TAX, $$ where $Y$ and $A$ are both known $n\times n$, real, symmetric matrices.  The unknown matrix $X$ is restricted to $n\times n$. I think there should be at least one real valued solution for $X$.  How do I solve for $X$?","Suppose $$ Y = X^TAX, $$ where $Y$ and $A$ are both known $n\times n$, real, symmetric matrices.  The unknown matrix $X$ is restricted to $n\times n$. I think there should be at least one real valued solution for $X$.  How do I solve for $X$?",,['linear-algebra']
4,What new insights does numerical analysis give on linear algebra?,What new insights does numerical analysis give on linear algebra?,,"I know linear algebra decently well, but I've never taken a numerical analysis course. However, I've heard that it provides a good intuition for the subject. Assuming that I'm already familiar with most linear algebra concepts and matrix decompositions, in what way would a numerical analysis course benefit my understanding? Are there any concrete examples of something valuable from a computational perspective that one wouldn't get in a more abstract setting?","I know linear algebra decently well, but I've never taken a numerical analysis course. However, I've heard that it provides a good intuition for the subject. Assuming that I'm already familiar with most linear algebra concepts and matrix decompositions, in what way would a numerical analysis course benefit my understanding? Are there any concrete examples of something valuable from a computational perspective that one wouldn't get in a more abstract setting?",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
5,Eigenvalues and Eigenvectors of Large Matrix,Eigenvalues and Eigenvectors of Large Matrix,,"Computing eigenvalues and eigenvectors of a $2\times2$ matrix is easy by solving the characteristic equation. However, things get complicated if the matrix is larger. Let's assume I have this matrix with computed eigenvalues and eigenvectors: $$\begin{pmatrix}12&4\\3&7\end{pmatrix}$$ Then, I have this $4\times4$ matrix that contains two duplicates of this matrix in it: $$\begin{pmatrix}12&4&0&0\\3&7&0&0\\0&0&12&4\\0&0&3&7\end{pmatrix}$$ To find the eigenvalues, I would have to solve an equation of $4$th degree and have to calculate a huge determinant. But I think there should be an easier way to calculate it. I have $2$ questions here: Is there a trick that I can use here to calculate them, knowing the eigenvalues of above $2\times2$ matrix already? How would swapping the rows or columns of my $4\times4$ matrix change the eigenvalues? Please feel free to answer any of the two. I am hoping that an easier solution exists to this.","Computing eigenvalues and eigenvectors of a $2\times2$ matrix is easy by solving the characteristic equation. However, things get complicated if the matrix is larger. Let's assume I have this matrix with computed eigenvalues and eigenvectors: $$\begin{pmatrix}12&4\\3&7\end{pmatrix}$$ Then, I have this $4\times4$ matrix that contains two duplicates of this matrix in it: $$\begin{pmatrix}12&4&0&0\\3&7&0&0\\0&0&12&4\\0&0&3&7\end{pmatrix}$$ To find the eigenvalues, I would have to solve an equation of $4$th degree and have to calculate a huge determinant. But I think there should be an easier way to calculate it. I have $2$ questions here: Is there a trick that I can use here to calculate them, knowing the eigenvalues of above $2\times2$ matrix already? How would swapping the rows or columns of my $4\times4$ matrix change the eigenvalues? Please feel free to answer any of the two. I am hoping that an easier solution exists to this.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
6,How to compute the SVD of $2\times 2$ matrices?,How to compute the SVD of  matrices?,2\times 2,"What's an efficient algorithm to get the SVD of $2\times 2$ matrices? I've found papers about doing SVD on $2\times 2$ triangular matrices, and I've seen the analytic formula to get the singular values of a $2\times 2$ matrix. But how to use either of these to get the SVD of an arbitrary $2\times 2$ matrix? Are the general algorithms built on these, or are these just some special cases?","What's an efficient algorithm to get the SVD of $2\times 2$ matrices? I've found papers about doing SVD on $2\times 2$ triangular matrices, and I've seen the analytic formula to get the singular values of a $2\times 2$ matrix. But how to use either of these to get the SVD of an arbitrary $2\times 2$ matrix? Are the general algorithms built on these, or are these just some special cases?",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd']"
7,Derive a rotation from a 2D rotation matrix,Derive a rotation from a 2D rotation matrix,,"I have a rotation 2D rotation matrix. I know that this matrix will always ever only be a rotation matrix. $$\left[ \begin{array}{@{}cc} \cos a & -\sin a \\ \sin a & \cos a \\ \end{array} \right]$$ How can I extract the rotation from this matrix? The less steps, the better, since this will be done on a computer and I don't want it to constantly be doing a lot of computations!","I have a rotation 2D rotation matrix. I know that this matrix will always ever only be a rotation matrix. $$\left[ \begin{array}{@{}cc} \cos a & -\sin a \\ \sin a & \cos a \\ \end{array} \right]$$ How can I extract the rotation from this matrix? The less steps, the better, since this will be done on a computer and I don't want it to constantly be doing a lot of computations!",,"['linear-algebra', 'matrices', 'rotations']"
8,Distance between real finite dimensional linear subspaces,Distance between real finite dimensional linear subspaces,,"Is there a usual distance between linear subspaces ($V,W$) of an n-dimensional normed vector space with inner product? In the case of hyper-planes one could use the angle (based on the inner product of the vector space). What can be used in the case of subspaces with lower dimension (not necessarily equal)? e.g. $dim(V)= n-2$ and $dim(W) = n-4$ Thanks","Is there a usual distance between linear subspaces ($V,W$) of an n-dimensional normed vector space with inner product? In the case of hyper-planes one could use the angle (based on the inner product of the vector space). What can be used in the case of subspaces with lower dimension (not necessarily equal)? e.g. $dim(V)= n-2$ and $dim(W) = n-4$ Thanks",,"['linear-algebra', 'normed-spaces', 'inner-products']"
9,Sending vector space to dual is a functor,Sending vector space to dual is a functor,,"In the category of finite dimensional vector spaces over a field and linear maps between them, the map that sends each space to its dual and linear map to its transpose is a functor, right? But this doesn't make sense to me.  Call the map $F$, and let $f: V \to W$ and $g: W \to Z$ be linear maps.  So, $$ F(g \circ f) = (g \circ f)^{*} :Z^* \to V^*$$ needs to be  $F(g) \circ F(f)$ but this is not defined, $F(f) \circ F(g)$ is defined however.  Can someone tell me what I'm missing here?","In the category of finite dimensional vector spaces over a field and linear maps between them, the map that sends each space to its dual and linear map to its transpose is a functor, right? But this doesn't make sense to me.  Call the map $F$, and let $f: V \to W$ and $g: W \to Z$ be linear maps.  So, $$ F(g \circ f) = (g \circ f)^{*} :Z^* \to V^*$$ needs to be  $F(g) \circ F(f)$ but this is not defined, $F(f) \circ F(g)$ is defined however.  Can someone tell me what I'm missing here?",,"['linear-algebra', 'abstract-algebra', 'category-theory']"
10,The rank of skew-symmetric matrix is even,The rank of skew-symmetric matrix is even,,I know that the rank of a skew-symmetric matrix is even.  I just need to find a published proof for it.  Could anyone direct me to a source that could help me?,I know that the rank of a skew-symmetric matrix is even.  I just need to find a published proof for it.  Could anyone direct me to a source that could help me?,,"['linear-algebra', 'matrices', 'reference-request']"
11,Canonical isomorphism between vector spaces,Canonical isomorphism between vector spaces,,"We can identify spaces $V$ and $V^{**}$ by canonical isomorphism: $$A:V\to V^{**},$$ $$Av(f)=f(v),$$ for any $f\in V^*$. But why we cannot identify $V$ and $V^*$ by $e^{*}_{i}(e_j)=\delta_{ij}$ (I understand that after change the basis of $V$ operator $B: V\to V^*$ will be changed)? What means that spaces $V$ and $V^{**}$ are identical? How we can use it?","We can identify spaces $V$ and $V^{**}$ by canonical isomorphism: $$A:V\to V^{**},$$ $$Av(f)=f(v),$$ for any $f\in V^*$. But why we cannot identify $V$ and $V^*$ by $e^{*}_{i}(e_j)=\delta_{ij}$ (I understand that after change the basis of $V$ operator $B: V\to V^*$ will be changed)? What means that spaces $V$ and $V^{**}$ are identical? How we can use it?",,"['linear-algebra', 'abstract-algebra']"
12,Outer product of a vector with itself,Outer product of a vector with itself,,"Is there a special name for an outer product of a vector with itself? Is it a special case of a Gramian? I've seen them a thousand times, but I have no idea if such product has a name. Update : The case of outer product I'm talking about is $\vec{u}\vec{u}^T$ where $\vec{u}$ is a column vector. Does is have a name in the form of something of $\vec{u}$? Cheers!","Is there a special name for an outer product of a vector with itself? Is it a special case of a Gramian? I've seen them a thousand times, but I have no idea if such product has a name. Update : The case of outer product I'm talking about is $\vec{u}\vec{u}^T$ where $\vec{u}$ is a column vector. Does is have a name in the form of something of $\vec{u}$? Cheers!",,['linear-algebra']
13,Does every operator have a hermitian adjoint?,Does every operator have a hermitian adjoint?,,"If we think of operators as matrices, every matrix can be transposed and its elements can be complex-conjugated. But the identification of the hermitian adjoint with the transpose conjugate comes from inner products. Namely, the hermitian adjoint of $\hat{T}$ is $\hat{T}^{\dagger}$ where $\hat{T}^{\dagger}$ satisfies $$(u,\hat{T}v) = (\hat{T}^{\dagger}u,v).$$ If $T_{ij} = (i,\hat{T}j),$ then $$T_{ij}^{\dagger} = (i,\hat{T}^{\dagger}j) = (j,\hat{T}i)^* = T_{ji}^*.$$ But how do we know that the map $\hat{T}^{\dagger}$ which satisfies the desired relationship exists in the first place? Can we show it without thinking of operators as matrices, just using the inner product? Also, does the relationship $$\big(\hat{T}^{\dagger}\big)^{\dagger} = \hat{T}$$ always hold? Since it seems to be used in the derivation of $T_{ij}^{\dagger}.$","If we think of operators as matrices, every matrix can be transposed and its elements can be complex-conjugated. But the identification of the hermitian adjoint with the transpose conjugate comes from inner products. Namely, the hermitian adjoint of is where satisfies If then But how do we know that the map which satisfies the desired relationship exists in the first place? Can we show it without thinking of operators as matrices, just using the inner product? Also, does the relationship always hold? Since it seems to be used in the derivation of","\hat{T} \hat{T}^{\dagger} \hat{T}^{\dagger} (u,\hat{T}v) = (\hat{T}^{\dagger}u,v). T_{ij} = (i,\hat{T}j), T_{ij}^{\dagger} = (i,\hat{T}^{\dagger}j) = (j,\hat{T}i)^* = T_{ji}^*. \hat{T}^{\dagger} \big(\hat{T}^{\dagger}\big)^{\dagger} = \hat{T} T_{ij}^{\dagger}.","['quantum-mechanics', 'linear-algebra']"
14,Prove that the Euclidean distance is no more than the spherical distance,Prove that the Euclidean distance is no more than the spherical distance,,"In an $n$ -dimensional Euclidean space, for any unit vector $$\boldsymbol{y}=(y_1,y_2,\cdots, y_n)\in\mathbb{S}^{n-1}=\{\boldsymbol{x}\in\mathbb{R}^n:\|\boldsymbol{x}\|_2=1\},$$ we can express $\boldsymbol{y}$ in the spherical coordinate system via $\phi(\boldsymbol{y})=(\phi_1(\boldsymbol{y}), \phi_2(\boldsymbol{y}), \ldots, \phi_{n-1}(\boldsymbol{y}))^{\mathrm{T}}\in\mathbb{R}^{n-1}$ s.t. $$ \begin{aligned} y_1 &=\cos \phi_1 \\ y_2 &=\sin \phi_1 \cos \phi_2 \\ y_3 &=\sin \phi_1 \sin \phi_2 \cos \phi_3 \\ & ~\,\,\vdots \\ y_{n-1} &=\sin \phi_1 \cdots \sin \phi_{n-2} \cos \phi_{n-1} \\ y_{n} &=\sin \phi_1 \cdots \sin \phi_{n-2} \sin \phi_{n-1}, \end{aligned} $$ where $0 \leq \phi_{n-1}<2 \pi$ , and $0 \leq \phi_{i}\le \pi$ , $\forall\,i=1,2,\ldots,n-2$ . My question is, if the Euclidean distance is no more than the spherical distance, i.e., if one has $$ \boxed{\|\boldsymbol{x}-\boldsymbol{y}\|_2 \leq\|\phi(\boldsymbol{x})-\phi(\boldsymbol{y})\|_2}. $$ I have been thinking this for several days, but am still unable to prove it. What is easy to show is that $\|\boldsymbol{x}-\boldsymbol{y}\|_2 \leq\|\phi(\boldsymbol{x})-\phi(\boldsymbol{y})\|_1$ , which simply follows from the fact that the Euclidean distance is the shortest between any two points, and $|\phi_i(\boldsymbol{x})-\phi_i(\boldsymbol{y})|$ is exactly the length of the arc used to align $\boldsymbol{x}$ and $\boldsymbol{y}$ along the $i$ -th spherical coordinate, which is longer than the corresponding chord. If this is not true, then does it hold for any two close vectors $\boldsymbol{x}$ and $\boldsymbol{y}$ in the sense that $\underset{i=1,2,\ldots,n}{\max}|\phi_i(\boldsymbol{x})-\phi_i(\boldsymbol{y})|\le\delta$ for some small $\delta$ ?","In an -dimensional Euclidean space, for any unit vector we can express in the spherical coordinate system via s.t. where , and , . My question is, if the Euclidean distance is no more than the spherical distance, i.e., if one has I have been thinking this for several days, but am still unable to prove it. What is easy to show is that , which simply follows from the fact that the Euclidean distance is the shortest between any two points, and is exactly the length of the arc used to align and along the -th spherical coordinate, which is longer than the corresponding chord. If this is not true, then does it hold for any two close vectors and in the sense that for some small ?","n \boldsymbol{y}=(y_1,y_2,\cdots, y_n)\in\mathbb{S}^{n-1}=\{\boldsymbol{x}\in\mathbb{R}^n:\|\boldsymbol{x}\|_2=1\}, \boldsymbol{y} \phi(\boldsymbol{y})=(\phi_1(\boldsymbol{y}), \phi_2(\boldsymbol{y}), \ldots, \phi_{n-1}(\boldsymbol{y}))^{\mathrm{T}}\in\mathbb{R}^{n-1} 
\begin{aligned}
y_1 &=\cos \phi_1 \\
y_2 &=\sin \phi_1 \cos \phi_2 \\
y_3 &=\sin \phi_1 \sin \phi_2 \cos \phi_3 \\
& ~\,\,\vdots \\
y_{n-1} &=\sin \phi_1 \cdots \sin \phi_{n-2} \cos \phi_{n-1} \\
y_{n} &=\sin \phi_1 \cdots \sin \phi_{n-2} \sin \phi_{n-1},
\end{aligned}
 0 \leq \phi_{n-1}<2 \pi 0 \leq \phi_{i}\le \pi \forall\,i=1,2,\ldots,n-2 
\boxed{\|\boldsymbol{x}-\boldsymbol{y}\|_2 \leq\|\phi(\boldsymbol{x})-\phi(\boldsymbol{y})\|_2}.
 \|\boldsymbol{x}-\boldsymbol{y}\|_2 \leq\|\phi(\boldsymbol{x})-\phi(\boldsymbol{y})\|_1 |\phi_i(\boldsymbol{x})-\phi_i(\boldsymbol{y})| \boldsymbol{x} \boldsymbol{y} i \boldsymbol{x} \boldsymbol{y} \underset{i=1,2,\ldots,n}{\max}|\phi_i(\boldsymbol{x})-\phi_i(\boldsymbol{y})|\le\delta \delta","['linear-algebra', 'geometry', 'inequality', 'spherical-coordinates', 'spherical-geometry']"
15,Example that M* is not reflexive,Example that M* is not reflexive,,"Let $R$ be a noetherian ring. Set $(-)^\ast={\rm Hom}_R(-,R)$ . For each $R$ -module $N$ , let $\pi_N:N\rightarrow N^{\ast\ast}$ be the map which maps $n\in N$ to $(f\mapsto f(n))$ . $N$ is called reflexive if $\pi_N$ is isomorphism. Question: Does there exist a finitely generated $R$ -module $M$ such that $M^\ast$ is not reflexive? I guess the answer is yes. But I can’t find any example. For each $R$ -module $N$ , we can check directly that the composition $N^\ast\xrightarrow{\pi_{N^\ast}}N^{\ast\ast\ast}\xrightarrow{(\pi_N)^\ast}N^\ast$ is identity. In particular, $\pi_{N^\ast}$ is always invective.  I searched the internet. It is proved in Yoshino’s paper that if $R$ is Gorenstein in depth one, then $M^\ast$ is reflexive for each finitely generated $R$ -module $M$ ; see Lemma 4.4 of HOMOTOPY CATEGORIES OF UNBOUNDED COMPLEXES OF PROJECTIVE MODULES . Thank you in advance.","Let be a noetherian ring. Set . For each -module , let be the map which maps to . is called reflexive if is isomorphism. Question: Does there exist a finitely generated -module such that is not reflexive? I guess the answer is yes. But I can’t find any example. For each -module , we can check directly that the composition is identity. In particular, is always invective.  I searched the internet. It is proved in Yoshino’s paper that if is Gorenstein in depth one, then is reflexive for each finitely generated -module ; see Lemma 4.4 of HOMOTOPY CATEGORIES OF UNBOUNDED COMPLEXES OF PROJECTIVE MODULES . Thank you in advance.","R (-)^\ast={\rm Hom}_R(-,R) R N \pi_N:N\rightarrow N^{\ast\ast} n\in N (f\mapsto f(n)) N \pi_N R M M^\ast R N N^\ast\xrightarrow{\pi_{N^\ast}}N^{\ast\ast\ast}\xrightarrow{(\pi_N)^\ast}N^\ast \pi_{N^\ast} R M^\ast R M","['linear-algebra', 'abstract-algebra', 'commutative-algebra', 'homological-algebra', 'cohen-macaulay']"
16,Almost every square matrix satisfies Cayley-Hamilton Theorem,Almost every square matrix satisfies Cayley-Hamilton Theorem,,"I was watching Steve Brunton's lecture and he pointed out that Cayley Hamilton theorem is not true for every single square matrix, but it's true for almost every of them: Someone pointed out to me that this might not actually be true for every single square matrix $A$ . So, almost every matrix $A$ satisfies its own characteristic equation. I don't want to get into the edge cases where this is not true. You can look this up in a linear algebra book and find out if this is true everywhere, but basically this is true for most matrices, OK? I think it might actually be true for every matrix... Could you clarify what is the matrix which does not satisfy the theorem?","I was watching Steve Brunton's lecture and he pointed out that Cayley Hamilton theorem is not true for every single square matrix, but it's true for almost every of them: Someone pointed out to me that this might not actually be true for every single square matrix . So, almost every matrix satisfies its own characteristic equation. I don't want to get into the edge cases where this is not true. You can look this up in a linear algebra book and find out if this is true everywhere, but basically this is true for most matrices, OK? I think it might actually be true for every matrix... Could you clarify what is the matrix which does not satisfy the theorem?",A A,"['linear-algebra', 'control-theory']"
17,Two permutation matrices represent conjugate permutations iff they have same characteristic polynomial.,Two permutation matrices represent conjugate permutations iff they have same characteristic polynomial.,,"I was told that Two permutation matrices represent conjugate permutations iff they have same characteristic polynomial (where the conjugacy is considered only in $S_{n}$ ). The first implication is clear to me i.e. permutation matrices representing conjugate permutations, being similar matrices, have same characteristic polynomial. But, I do not understand why is it necessary that if two permutation matrices have same characteristic polynomial they should represent conjugate permutation? I was told to see newton's identities. But I do not see anything relating characteristic polynomial with permutation matrices in them. Anyhow, I did it and I have written answer. But, I would really like to know a whether there exists a proof that uses newtons identities or algaebraic manipulations.","I was told that Two permutation matrices represent conjugate permutations iff they have same characteristic polynomial (where the conjugacy is considered only in ). The first implication is clear to me i.e. permutation matrices representing conjugate permutations, being similar matrices, have same characteristic polynomial. But, I do not understand why is it necessary that if two permutation matrices have same characteristic polynomial they should represent conjugate permutation? I was told to see newton's identities. But I do not see anything relating characteristic polynomial with permutation matrices in them. Anyhow, I did it and I have written answer. But, I would really like to know a whether there exists a proof that uses newtons identities or algaebraic manipulations.",S_{n},"['linear-algebra', 'abstract-algebra', 'group-theory', 'characteristic-polynomial', 'permutation-matrices']"
18,Are there many ways to decompose a matrix into a sum of rank-$1$ matrices?,Are there many ways to decompose a matrix into a sum of rank- matrices?,1,"Are there many ways to decompose a matrix, ${\bf{A}} \in \mathbb{R}^{n \times n}$ , into a sum of rank- $1$ matrices? I ask because I know that if a matrix is symmetric and diagonalizable, $\bf{A}=\Phi\Lambda\Phi^{T}$ , then you can use the eigendecomposition to form a sum of rank- $1$ matrices: $$\bf{A} = \sum_{i=1}^{n}\lambda_i \phi_i\phi_i^T\,.$$ It is also true that you can always use the singular value decomposition (SVD) to do this, as well: $$\bf{A} = \sum_{i=1}^n \sigma_i \bf{u}_i\bf{v}_i^T\,.$$ So, unless these happen to be equal, which is not true in general, then I have found two ways already. Are there infinitely many ways? What is an example of another decomposition of this form?","Are there many ways to decompose a matrix, , into a sum of rank- matrices? I ask because I know that if a matrix is symmetric and diagonalizable, , then you can use the eigendecomposition to form a sum of rank- matrices: It is also true that you can always use the singular value decomposition (SVD) to do this, as well: So, unless these happen to be equal, which is not true in general, then I have found two ways already. Are there infinitely many ways? What is an example of another decomposition of this form?","{\bf{A}} \in \mathbb{R}^{n \times n} 1 \bf{A}=\Phi\Lambda\Phi^{T} 1 \bf{A} = \sum_{i=1}^{n}\lambda_i \phi_i\phi_i^T\,. \bf{A} = \sum_{i=1}^n \sigma_i \bf{u}_i\bf{v}_i^T\,.","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd']"
19,Computing the matrix powers of a non-diagonalizable matrix,Computing the matrix powers of a non-diagonalizable matrix,,Define \begin{equation} A = \begin{pmatrix} \frac{1}{2} &\frac{1}{2} & 0\\ 0& \frac{3}{4} & \frac{1}{4}\\ 0& \frac{1}{4} & \frac{3}{4} \end{pmatrix}. \end{equation} Note that the sum of the dimensions of the eigenspaces of $A$ is only two. $A$ is thus not diagonalizable. How can we compute $A^n$ ?,Define Note that the sum of the dimensions of the eigenspaces of is only two. is thus not diagonalizable. How can we compute ?,"\begin{equation}
A = \begin{pmatrix} \frac{1}{2} &\frac{1}{2} & 0\\ 0& \frac{3}{4} & \frac{1}{4}\\ 0& \frac{1}{4} & \frac{3}{4} \end{pmatrix}.
\end{equation} A A A^n","['linear-algebra', 'matrices', 'matrix-exponential']"
20,Parametrization of unitary matrices,Parametrization of unitary matrices,,Does anyone know a simple way to parametrize the space of $n\times n$ complex unitary matrices into a set of independent complex numbers in some complex-rectangle? Specifically the mapping and inverse mapping that does this. Like a generalized Euler angle.,Does anyone know a simple way to parametrize the space of complex unitary matrices into a set of independent complex numbers in some complex-rectangle? Specifically the mapping and inverse mapping that does this. Like a generalized Euler angle.,n\times n,"['linear-algebra', 'matrices', 'reference-request', 'complex-numbers', 'unitary-matrices']"
21,Is the distance between two elements of a normed vector space at least the distance between their projections on the unit ball?,Is the distance between two elements of a normed vector space at least the distance between their projections on the unit ball?,,"I am stuck on the following question which I am having some difficulty with. Any help is much appreciated. Let $X$ be a normed vector space and define the projection onto the unit ball as $\pi(x)$ = $x/||x||$ for $x \in X \setminus \{0\}$. Let $x,y \in X \setminus \{0\}$ with $||x||,||y|| \geq 1$. Must it be the case that we have $||\pi(x) − \pi(y)|| \leq ||x − y||$? I can't seem to find a counterexample so am trying to go for a proof. So far the only observation I have been able to make is this: WLOG we can take $||x|| \leq ||y||$. We may also WLOG take $||x|| = 1$ as otherwise we can replace $x$ with $x' = x/||x||$ and $y$ with $y' = y/||x||$ which does not affect the L.H.S of our inequality which reducing the R.H.S which only makes the inequality more strict. Hence the problem reduces to showing $||x-\pi(y)|| \leq ||x-y||$ with $||x|| = 1$ and $||y|| \geq 1$. If we have $||y|| \geq 3$ then we can use the triangle inequality twice to show this. However I can't find a proof using just $||y|| \geq 1$. How should I proceed?","I am stuck on the following question which I am having some difficulty with. Any help is much appreciated. Let $X$ be a normed vector space and define the projection onto the unit ball as $\pi(x)$ = $x/||x||$ for $x \in X \setminus \{0\}$. Let $x,y \in X \setminus \{0\}$ with $||x||,||y|| \geq 1$. Must it be the case that we have $||\pi(x) − \pi(y)|| \leq ||x − y||$? I can't seem to find a counterexample so am trying to go for a proof. So far the only observation I have been able to make is this: WLOG we can take $||x|| \leq ||y||$. We may also WLOG take $||x|| = 1$ as otherwise we can replace $x$ with $x' = x/||x||$ and $y$ with $y' = y/||x||$ which does not affect the L.H.S of our inequality which reducing the R.H.S which only makes the inequality more strict. Hence the problem reduces to showing $||x-\pi(y)|| \leq ||x-y||$ with $||x|| = 1$ and $||y|| \geq 1$. If we have $||y|| \geq 3$ then we can use the triangle inequality twice to show this. However I can't find a proof using just $||y|| \geq 1$. How should I proceed?",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'normed-spaces']"
22,Singular Value Decomposition in terms of Change of Basis?,Singular Value Decomposition in terms of Change of Basis?,,"I do see how SVD can be understood in terms of rotating. But it is hard for me to understand SVD in terms of change of basis. So, let me start from Diagonalization to clarify my question. When $B=\{v_1, \dots, v_n\}$ where $v_i$ are eigenvectors, $A=PDP^T$. Then $PDP^Tx$ means that $x$ becomes $[x]_B$ (that is, $P^Tx$) and scaling operates on $[x]_B$ (that is, D) and then the result is transformed into the original coordinate (that is, P). On the other hand, understanding SVD in terms of change of basis is really hard for me. Let me say that $A=U\Sigma V^T$. Then, given $U\Sigma V^Tx$, $V^T$ transforms x into the coordinate system consisting of column vectors of V. Scaling operates in that coordinate system ($\Sigma$). But after that, how can one transform the vector under coordinate system governed by V into the original coordinate system? $U$ is different from $V$! Thank you!","I do see how SVD can be understood in terms of rotating. But it is hard for me to understand SVD in terms of change of basis. So, let me start from Diagonalization to clarify my question. When $B=\{v_1, \dots, v_n\}$ where $v_i$ are eigenvectors, $A=PDP^T$. Then $PDP^Tx$ means that $x$ becomes $[x]_B$ (that is, $P^Tx$) and scaling operates on $[x]_B$ (that is, D) and then the result is transformed into the original coordinate (that is, P). On the other hand, understanding SVD in terms of change of basis is really hard for me. Let me say that $A=U\Sigma V^T$. Then, given $U\Sigma V^Tx$, $V^T$ transforms x into the coordinate system consisting of column vectors of V. Scaling operates in that coordinate system ($\Sigma$). But after that, how can one transform the vector under coordinate system governed by V into the original coordinate system? $U$ is different from $V$! Thank you!",,['linear-algebra']
23,"What does the vector space R^[0,1] mean?","What does the vector space R^[0,1] mean?",,"While reading a book Linear Algebra Done Right , I came to knew that a vector space $\mathbf{R}^n$ represents a space with dimensions as $(x_1, x_2, ...,x_n)$, but there were other vector spaces that I could not understand. There was a statement as Ref: 1.35 The set of continuous real-valued functions on the interval $[0,1]$ is a subspace of $\mathbf{R}^{[0,1]}$ What kind of space does $\mathbf{R}^{[0,1]}$ represent? Is this a space that can continuously be from $0$ dimension to $1$ dimension? Another statement, Ref: 1.35 The set of differentiable real-valued functions on $\mathbf{R}$ is a subspace of $\mathbf{R}^\mathbf{R}$ What kind of space is $\mathbf{R}^\mathbf{R}$? Similarly, there were other subspaces as, $\mathbf{R}^{(0,\ 3)}$ and $\mathbf{R}^{(-4,\ 4)}$ Explain me how can I visualize such spaces. If you can explain with the proof too, that will be great.","While reading a book Linear Algebra Done Right , I came to knew that a vector space $\mathbf{R}^n$ represents a space with dimensions as $(x_1, x_2, ...,x_n)$, but there were other vector spaces that I could not understand. There was a statement as Ref: 1.35 The set of continuous real-valued functions on the interval $[0,1]$ is a subspace of $\mathbf{R}^{[0,1]}$ What kind of space does $\mathbf{R}^{[0,1]}$ represent? Is this a space that can continuously be from $0$ dimension to $1$ dimension? Another statement, Ref: 1.35 The set of differentiable real-valued functions on $\mathbf{R}$ is a subspace of $\mathbf{R}^\mathbf{R}$ What kind of space is $\mathbf{R}^\mathbf{R}$? Similarly, there were other subspaces as, $\mathbf{R}^{(0,\ 3)}$ and $\mathbf{R}^{(-4,\ 4)}$ Explain me how can I visualize such spaces. If you can explain with the proof too, that will be great.",,"['linear-algebra', 'functions', 'vector-spaces', 'notation']"
24,Is a Hamel basis dense?,Is a Hamel basis dense?,,"Consider a basis $B$ of the vector space $\Bbb{R}$ over the field $\Bbb{Q}$ , known as a Hamel basis. Is $B$ necessarily dense in $\Bbb{R}$ ? It seems hard to answer this question since we can't actually construct a Hamel basis and I don't know any of its properties other than that it exists!","Consider a basis of the vector space over the field , known as a Hamel basis. Is necessarily dense in ? It seems hard to answer this question since we can't actually construct a Hamel basis and I don't know any of its properties other than that it exists!",B \Bbb{R} \Bbb{Q} B \Bbb{R},"['linear-algebra', 'hamel-basis']"
25,"Determinant of the matrix with $a_{i,j} = (i+j)^2$ [closed]",Determinant of the matrix with  [closed],"a_{i,j} = (i+j)^2","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Determinant of the matrix with $ a_{i,j} = (i+j)^2 $ I was trying to solve, but it is impossible","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Determinant of the matrix with $ a_{i,j} = (i+j)^2 $ I was trying to solve, but it is impossible",,"['linear-algebra', 'determinant']"
26,Why is the winding number of a matrix an integer?,Why is the winding number of a matrix an integer?,,I am told in literature that if we have a continous map from a circle to unitary matrix $$M : S^1 \to U(m)$$ then a winding number can be defined: $$\nu=\frac{i}{2\pi}\int_0^{2\pi}dt\text{Tr}[M^{-1}(t)\partial_tM(t)]$$ Notice $M$ is a matrix. This number is an integer. Why would this number be an integer?,I am told in literature that if we have a continous map from a circle to unitary matrix then a winding number can be defined: Notice is a matrix. This number is an integer. Why would this number be an integer?,M : S^1 \to U(m) \nu=\frac{i}{2\pi}\int_0^{2\pi}dt\text{Tr}[M^{-1}(t)\partial_tM(t)] M,"['linear-algebra', 'general-topology', 'matrices', 'algebraic-topology', 'winding-number']"
27,Norm preservation properties of a unitary matrix,Norm preservation properties of a unitary matrix,,"Tried to prove the following facts: If $B \in M_{n\times n}(\mathbb{C})$ is unitary  (i.e. $ B^{-1} = B^{*}$ ), then: $\forall x\in \mathbb{C}^n :\| Bx \|_2 = \| x \|_2  $ $\forall A\in M_{n\times n}(\mathbb{C})$ : $ \| BA \|_F = \| A \|_F  $ (where $ \| . \|_F $ denotes Frobenius norm). $\forall A,B\in M_{n\times n}(\mathbb{C}): \| AB \|_{ \infty} \leq \| A \|_{ \infty}\| B \|_{ \infty}   $ In (1),tried to conduct direct algebraic manipulations from the definitions of the norms, but I obtained no results.    In (2) the use of definition $ \| B \|_{ \infty} := max\{\frac{\|Bx\|}{\|x\|}: x\in \mathbb{C}^n \setminus \{0\} \}$ and the inequality $\|AB\| \leq \|A\|\|B\|$ gives straightforward result. However, the use of definition $\|B\|\ _{\infty} :=max\{ \sum_{j=1}^{n}|b_{ij}|: i\in \{1,...n\}\}$ does not give direct results. I would be thankful for hints/advices!","Tried to prove the following facts: If is unitary  (i.e. ), then: : (where denotes Frobenius norm). In (1),tried to conduct direct algebraic manipulations from the definitions of the norms, but I obtained no results.    In (2) the use of definition and the inequality gives straightforward result. However, the use of definition does not give direct results. I would be thankful for hints/advices!","B \in M_{n\times n}(\mathbb{C})  B^{-1} = B^{*} \forall x\in \mathbb{C}^n :\| Bx \|_2 = \| x \|_2   \forall A\in M_{n\times n}(\mathbb{C})  \| BA \|_F = \| A \|_F    \| . \|_F  \forall A,B\in M_{n\times n}(\mathbb{C}): \| AB \|_{ \infty} \leq \| A \|_{ \infty}\| B \|_{ \infty}     \| B \|_{ \infty} := max\{\frac{\|Bx\|}{\|x\|}: x\in \mathbb{C}^n \setminus \{0\} \} \|AB\| \leq \|A\|\|B\| \|B\|\ _{\infty} :=max\{ \sum_{j=1}^{n}|b_{ij}|: i\in \{1,...n\}\}","['linear-algebra', 'matrices']"
28,"What does $[A,B]=0$ mean in matrix theory?",What does  mean in matrix theory?,"[A,B]=0","This is probably very simple but I can't seem to find an answer anywhere. I'm being asked to prove $e^{tA}e^{tB} = e^{t(A+B)}$ for all $t \in \mathbb{R}$ iff $[A,B]=0$ However, I am unsure what $[A,B]=0$ even means! Any help?","This is probably very simple but I can't seem to find an answer anywhere. I'm being asked to prove $e^{tA}e^{tB} = e^{t(A+B)}$ for all $t \in \mathbb{R}$ iff $[A,B]=0$ However, I am unsure what $[A,B]=0$ even means! Any help?",,"['linear-algebra', 'matrices', 'notation', 'matrix-exponential']"
29,Prove that the trace of the matrix product $U'AU$ is maximized by setting $U$'s columns to $A$'s eigenvectors,Prove that the trace of the matrix product  is maximized by setting 's columns to 's eigenvectors,U'AU U A,"I have an expression which I want to maximize: $$\mbox{trace} (U^T\Sigma U)$$ $\Sigma \in R^{d \times d}$ is symmetric and positive definite $U \in R^{d \times r}$ its columns are orthonormal. I am following a proof which states that U is optimal when its columns are the eigenvectors of $\Sigma$: let $\Sigma = V\Delta V^T$, the eigen-decomposition; let $B=V^TU$ then: $$U=VB$$ $$U^T\Sigma U= B^TV^TV\Delta V^TVB = B^T\Delta B$$ the last statement is mathematically correct. However, how does this proof to me that the columns of U are the eigenvectors of $\Sigma$? For some background: I am trying to understand a derivation of PCA. The point is that when this trace is maximized the objective function will be minimized.","I have an expression which I want to maximize: $$\mbox{trace} (U^T\Sigma U)$$ $\Sigma \in R^{d \times d}$ is symmetric and positive definite $U \in R^{d \times r}$ its columns are orthonormal. I am following a proof which states that U is optimal when its columns are the eigenvectors of $\Sigma$: let $\Sigma = V\Delta V^T$, the eigen-decomposition; let $B=V^TU$ then: $$U=VB$$ $$U^T\Sigma U= B^TV^TV\Delta V^TVB = B^T\Delta B$$ the last statement is mathematically correct. However, how does this proof to me that the columns of U are the eigenvectors of $\Sigma$? For some background: I am trying to understand a derivation of PCA. The point is that when this trace is maximized the objective function will be minimized.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
30,Are the elements of a module also called vectors?,Are the elements of a module also called vectors?,,"Are the elements of a module also called vectors? Or if someone says 'vector', are they talking only about a vector space? If no context is given, are there some standard assumptions?","Are the elements of a module also called vectors? Or if someone says 'vector', are they talking only about a vector space? If no context is given, are there some standard assumptions?",,"['linear-algebra', 'abstract-algebra', 'modules', 'terminology']"
31,Matrix exponential for Jordan canonical form,Matrix exponential for Jordan canonical form,,"Let $X$ be a real $n \times n$ matrix, then there is a Jordan decomposition such that $X = D+N$ where $D$ is diagonalisable and $N$ is nilpotent. Then, I was wondering whether the following is correct. $$ e^{tX}(x) = \sum_{k=0}^{m} \frac{t^k N^k}{k!} \left(e^{t \lambda_1} \alpha_1 v_1+\cdots+e^{t \lambda_n} \alpha_n v_n \right).$$ Here $x = \sum_{i=1}^{n } \alpha_i v_i$ and $v_i$ are the eigenvectors of the diagonalisable matrix, $\lambda_i$ are the eigenvalues of $D$ and $m$ is the degree up to which $N^k$ is still non-zero. Is this correct or am I doing something wrong? Cause I could not find a general equation for this matrix exponential, so I tried my best. (Thus, I am only asking for a verification or correction of this answer.) If anything is unclear, please let me know.","Let be a real matrix, then there is a Jordan decomposition such that where is diagonalisable and is nilpotent. Then, I was wondering whether the following is correct. Here and are the eigenvectors of the diagonalisable matrix, are the eigenvalues of and is the degree up to which is still non-zero. Is this correct or am I doing something wrong? Cause I could not find a general equation for this matrix exponential, so I tried my best. (Thus, I am only asking for a verification or correction of this answer.) If anything is unclear, please let me know.",X n \times n X = D+N D N  e^{tX}(x) = \sum_{k=0}^{m} \frac{t^k N^k}{k!} \left(e^{t \lambda_1} \alpha_1 v_1+\cdots+e^{t \lambda_n} \alpha_n v_n \right). x = \sum_{i=1}^{n } \alpha_i v_i v_i \lambda_i D m N^k,"['linear-algebra', 'matrices', 'jordan-normal-form', 'matrix-exponential']"
32,understanding vector-matrix-vector operation in linear algebra,understanding vector-matrix-vector operation in linear algebra,,"This operation comes up a lot in linear systems: $x^{T}Ax$ where $A$ is a (square) matrix and $x$ is a column vector. example: $A = \begin{bmatrix}-1 & 0 & 0\\0 & -2 & 0\\0 & 0 & -3\end{bmatrix}$ $x = \begin{bmatrix}-5\\-6\\-8\end{bmatrix}$ the operation $Ax$ is an application of a linear transform (represented by the matrix) to a vector, which is typically the variables in the linear system. That's clear. What does $x^TAx$ intuitively mean then? The result in the above example is: $x^TAx = \begin{bmatrix}95\end{bmatrix}$","This operation comes up a lot in linear systems: $x^{T}Ax$ where $A$ is a (square) matrix and $x$ is a column vector. example: $A = \begin{bmatrix}-1 & 0 & 0\\0 & -2 & 0\\0 & 0 & -3\end{bmatrix}$ $x = \begin{bmatrix}-5\\-6\\-8\end{bmatrix}$ the operation $Ax$ is an application of a linear transform (represented by the matrix) to a vector, which is typically the variables in the linear system. That's clear. What does $x^TAx$ intuitively mean then? The result in the above example is: $x^TAx = \begin{bmatrix}95\end{bmatrix}$",,"['linear-algebra', 'matrices', 'linear-transformations']"
33,Diagonalize tri-diagonal symmetric matrix,Diagonalize tri-diagonal symmetric matrix,,"How to diagonalize the following matrix? \begin{pmatrix} 2  & -1 & 0 & 0 & 0 & \cdots \\ -1 & 2  & -1 & 0 & 0 & \cdots \\ 0 & -1 & 2  & -1 & 0 & \cdots \\ \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\ \cdots & 0 & 0 & -1 & 2  & -1 \\ \cdots & 0 & 0 & 0 & -1 & 2  \\ \end{pmatrix} It seems like we need to first compute eigenvalues and eigenvectors for this matrix, like the following: \begin{vmatrix} 2 - \lambda & -1 & 0 & 0 & 0 & \cdots \\ -1 & 2 - \lambda & -1 & 0 & 0 & \cdots \\ 0 & -1 & 2 - \lambda & -1 & 0 & \cdots \\ \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\ \cdots & 0 & 0 & -1 & 2 - \lambda & -1 \\ \cdots & 0 & 0 & 0 & -1 & 2 - \lambda \\ \end{vmatrix} But I just don't know what to do next.","How to diagonalize the following matrix? \begin{pmatrix} 2  & -1 & 0 & 0 & 0 & \cdots \\ -1 & 2  & -1 & 0 & 0 & \cdots \\ 0 & -1 & 2  & -1 & 0 & \cdots \\ \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\ \cdots & 0 & 0 & -1 & 2  & -1 \\ \cdots & 0 & 0 & 0 & -1 & 2  \\ \end{pmatrix} It seems like we need to first compute eigenvalues and eigenvectors for this matrix, like the following: \begin{vmatrix} 2 - \lambda & -1 & 0 & 0 & 0 & \cdots \\ -1 & 2 - \lambda & -1 & 0 & 0 & \cdots \\ 0 & -1 & 2 - \lambda & -1 & 0 & \cdots \\ \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\ \cdots & 0 & 0 & -1 & 2 - \lambda & -1 \\ \cdots & 0 & 0 & 0 & -1 & 2 - \lambda \\ \end{vmatrix} But I just don't know what to do next.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'tridiagonal-matrices']"
34,"Connection between linear independence, non-/trivial and x solutions","Connection between linear independence, non-/trivial and x solutions",,"I am having a hard time remembering which goes hand in hand with what. The math questions I get always include words like trivial etc. 1 solution no solution infinite amount of solutions And then we have the two types of set of vectors linearly independent linearly dependent A set of vectors is linearly independent iff the system of equations are satisfied when all vector scalars are = 0 (making all vectors zero vectors). This results in 1 solution , the solution is trivial ? A set of vectors is linearly dependent when there are an infinite amount of solutions to the system of equations. This is non-trivial ? Where does no solution come in? I understand that if there is no solution, then all of the vectors do not intersect at a specific coordinate(which is the solution to the system of equations). But does that mean the set of vectors is linearly independent or dependent?","I am having a hard time remembering which goes hand in hand with what. The math questions I get always include words like trivial etc. 1 solution no solution infinite amount of solutions And then we have the two types of set of vectors linearly independent linearly dependent A set of vectors is linearly independent iff the system of equations are satisfied when all vector scalars are = 0 (making all vectors zero vectors). This results in 1 solution , the solution is trivial ? A set of vectors is linearly dependent when there are an infinite amount of solutions to the system of equations. This is non-trivial ? Where does no solution come in? I understand that if there is no solution, then all of the vectors do not intersect at a specific coordinate(which is the solution to the system of equations). But does that mean the set of vectors is linearly independent or dependent?",,['linear-algebra']
35,Mysterious Proof about Induced Norms (was: Uniqueness of SVD),Mysterious Proof about Induced Norms (was: Uniqueness of SVD),,"In order to prove non-uniqueness of singular vectors when a repeated singular value is present, the book (Trefethen), argues as follows: Let $\sigma$ be the first singular value of A, and $v_{1}$ the corresponding singular vector. Let $w$ be another linearly independent vector such that $||Aw||=\sigma$, and construct a third vector $v_{2}$ belonging to span of $v_{1}$ and $w$, and orthogonal to $v_{1}$. All three vectors are unitary, so $w=av_{1}+bv_{2}$ with $|a|^2+|b|^2=1$, and $v_{2}$ is constructed (Gram-Schmidt style) as follows: $$ {v}_{2}= \dfrac{{w}-({v}_{1}^{T} w ){v}_{1}}{|| {w}_{1}-({v}_{1}^{T} {w} ){v}_{1} ||_{2}}$$ Now, Trefethen says, $||A||=\sigma$, so $||Av_{2}||\le \sigma$ but this must be an equality (and so $v_{2}$ is another singular vector relative to $\sigma$), since otherwise we would have $||Aw||<\sigma$, in contrast with the hypothesis. How that? I cannot see any elementary application of triangle inequality or Schwarz inequality  to prove this claim. I am pretty well convinced of partial non-uniqueness of SVD in certain situations. Other proofs are feasible, but I wish to undestand this specific algebraic step of this specific proof. Thanks.","In order to prove non-uniqueness of singular vectors when a repeated singular value is present, the book (Trefethen), argues as follows: Let $\sigma$ be the first singular value of A, and $v_{1}$ the corresponding singular vector. Let $w$ be another linearly independent vector such that $||Aw||=\sigma$, and construct a third vector $v_{2}$ belonging to span of $v_{1}$ and $w$, and orthogonal to $v_{1}$. All three vectors are unitary, so $w=av_{1}+bv_{2}$ with $|a|^2+|b|^2=1$, and $v_{2}$ is constructed (Gram-Schmidt style) as follows: $$ {v}_{2}= \dfrac{{w}-({v}_{1}^{T} w ){v}_{1}}{|| {w}_{1}-({v}_{1}^{T} {w} ){v}_{1} ||_{2}}$$ Now, Trefethen says, $||A||=\sigma$, so $||Av_{2}||\le \sigma$ but this must be an equality (and so $v_{2}$ is another singular vector relative to $\sigma$), since otherwise we would have $||Aw||<\sigma$, in contrast with the hypothesis. How that? I cannot see any elementary application of triangle inequality or Schwarz inequality  to prove this claim. I am pretty well convinced of partial non-uniqueness of SVD in certain situations. Other proofs are feasible, but I wish to undestand this specific algebraic step of this specific proof. Thanks.",,"['linear-algebra', 'matrices', 'svd']"
36,How to generalize symmetry for higher-dimensional arrays?,How to generalize symmetry for higher-dimensional arrays?,,"@BrianM.Scott 's answer to this question Q: 3-dimensional array suggests that there is no standard concept of symmetry for 3-, 4-, N-dimensional arrays, in constrast to the case for 2-D arrays, as in linear algebra for matrices.  Are there alternative definitions of symmetry for higher-dimensional arrays?  Are there specific definitions that are widely used in certain contexts, e.g. in tensor calculus? (I don't have a specific need; I'm trying to help implement a symmetry test for a matrix library [ core.matrix for the Clojure language].  Since the library allows higher-dimensional arrays, there's a question about whether there is a natural choice for what the symmetry test should return for higher-dimensional arrays.)","@BrianM.Scott 's answer to this question Q: 3-dimensional array suggests that there is no standard concept of symmetry for 3-, 4-, N-dimensional arrays, in constrast to the case for 2-D arrays, as in linear algebra for matrices.  Are there alternative definitions of symmetry for higher-dimensional arrays?  Are there specific definitions that are widely used in certain contexts, e.g. in tensor calculus? (I don't have a specific need; I'm trying to help implement a symmetry test for a matrix library [ core.matrix for the Clojure language].  Since the library allows higher-dimensional arrays, there's a question about whether there is a natural choice for what the symmetry test should return for higher-dimensional arrays.)",,"['linear-algebra', 'matrices', 'tensors']"
37,Computing the Frobenius normal form,Computing the Frobenius normal form,,"I was wondering whether someone could give me an example how one actually determines the Frobenius normal form of a given matrix. Further, it seems hard to find an example where the new basis is calculated so that a given matrix is in Frobenius normal form. I really tried to find an example where this is done, but most books just use this form as a theoretical example rather than actually calculating this form? A short summary how you would proceed to calculate the Frobenius normal form and the basis would be more than enough, too.","I was wondering whether someone could give me an example how one actually determines the Frobenius normal form of a given matrix. Further, it seems hard to find an example where the new basis is calculated so that a given matrix is in Frobenius normal form. I really tried to find an example where this is done, but most books just use this form as a theoretical example rather than actually calculating this form? A short summary how you would proceed to calculate the Frobenius normal form and the basis would be more than enough, too.",,['linear-algebra']
38,Two terms that I want to understand: weakest topology and jointly continuous (in the following context).,Two terms that I want to understand: weakest topology and jointly continuous (in the following context).,,"I was reading an article online, please help me to understand the following lines (in bold letters). -  Topological structure: If (V, ‖·‖) is a normed vector space, the norm ‖·‖ induces a metric and therefore a topology on V.the distance between two vectors u and v is given by ‖u−v‖. This topology is precisely the weakest topology which makes ‖·‖ continuous and which is compatible with the linear structure of V in the following sense: 1.The vector addition + : V × V → V is jointly continuous with respect to this topology. This follows directly from the triangle inequality. 2.The scalar multiplication · : K × V → V, where K is the underlying scalar field of V, is jointly continuous. This follows from the triangle inequality and homogeneity of the norm. Please explain me the Weak topology and how does it makes norm ‖·‖ continuous. what does it mean by ""Addition + : V × V → V is jointly continuous with respect to this topology"" Thank you so much in advance.","I was reading an article online, please help me to understand the following lines (in bold letters). -  Topological structure: If (V, ‖·‖) is a normed vector space, the norm ‖·‖ induces a metric and therefore a topology on V.the distance between two vectors u and v is given by ‖u−v‖. This topology is precisely the weakest topology which makes ‖·‖ continuous and which is compatible with the linear structure of V in the following sense: 1.The vector addition + : V × V → V is jointly continuous with respect to this topology. This follows directly from the triangle inequality. 2.The scalar multiplication · : K × V → V, where K is the underlying scalar field of V, is jointly continuous. This follows from the triangle inequality and homogeneity of the norm. Please explain me the Weak topology and how does it makes norm ‖·‖ continuous. what does it mean by ""Addition + : V × V → V is jointly continuous with respect to this topology"" Thank you so much in advance.",,"['linear-algebra', 'general-topology', 'functional-analysis', 'normed-spaces']"
39,Prove that direct sum of linear transformations is a block matrix,Prove that direct sum of linear transformations is a block matrix,,"In linear algebra, I'm facing a question I cannot quite formalize into a full proof. I want to prove that a direct sum of linear transformations is a block matrix. Here's the formal question: Assume $V$ is a vector space with $U_1,\ldots,U_n$ as subspaces such that $V = U_1\oplus\cdots\oplus U_n$ . Assume that each $T_i: U_i\to U_i$ is a linear transformation. Assume that $T = T_1 \oplus T_2 \oplus\cdots\oplus T_n$ . How do I formally prove that $T$ can be represented as follows? $$\begin{bmatrix}A_1&&&0\\&A_2&&\\&&\dots\\0&&&A_m\end{bmatrix}$$ where $A_1,\ldots,A_m$ are square matrices representing $T_1,\ldots,T_n$ accordingly. Thanks!","In linear algebra, I'm facing a question I cannot quite formalize into a full proof. I want to prove that a direct sum of linear transformations is a block matrix. Here's the formal question: Assume is a vector space with as subspaces such that . Assume that each is a linear transformation. Assume that . How do I formally prove that can be represented as follows? where are square matrices representing accordingly. Thanks!","V U_1,\ldots,U_n V = U_1\oplus\cdots\oplus U_n T_i: U_i\to U_i T = T_1 \oplus T_2 \oplus\cdots\oplus T_n T \begin{bmatrix}A_1&&&0\\&A_2&&\\&&\dots\\0&&&A_m\end{bmatrix} A_1,\ldots,A_m T_1,\ldots,T_n","['linear-algebra', 'vector-spaces']"
40,Pullbacks and transpose map,Pullbacks and transpose map,,"Given maifolds $M,N$ and a smooth map $\phi:M \to N$, and a smooth function $f:N \to \mathbb{R}$, we have the pullback of $\phi$ by $f$ to be the function $\phi^* f = f \circ \phi : M \to \mathbb{R}$. Similarly, given a linear map $T:V \to W$, we get a transpose map $T^*: W^* \to V^*$ such that $T^*g = g \circ T$. I just noticed that these ideas are really similar. Is there something more ""going on""? It can't be a coincidence, since mathematicians have decided to use basically the same notation. My question is: is there a general way to encapsulate this concept? I imagine (perhaps incorrectly) that such an answer would involve category theory (I am aware of the so-called ""dual functor"" associated to vector spaces, which I understand is related to this topic) If possible, could someone point me to a reference without too much category theory? Thanks. p.s. any references would be good, so even if they do contain tons of category theory, that's OK.","Given maifolds $M,N$ and a smooth map $\phi:M \to N$, and a smooth function $f:N \to \mathbb{R}$, we have the pullback of $\phi$ by $f$ to be the function $\phi^* f = f \circ \phi : M \to \mathbb{R}$. Similarly, given a linear map $T:V \to W$, we get a transpose map $T^*: W^* \to V^*$ such that $T^*g = g \circ T$. I just noticed that these ideas are really similar. Is there something more ""going on""? It can't be a coincidence, since mathematicians have decided to use basically the same notation. My question is: is there a general way to encapsulate this concept? I imagine (perhaps incorrectly) that such an answer would involve category theory (I am aware of the so-called ""dual functor"" associated to vector spaces, which I understand is related to this topic) If possible, could someone point me to a reference without too much category theory? Thanks. p.s. any references would be good, so even if they do contain tons of category theory, that's OK.",,"['linear-algebra', 'reference-request', 'differential-geometry', 'category-theory']"
41,Change of Basis vs. Linear Transformation,Change of Basis vs. Linear Transformation,,"If i understand it correctly, change of basis is just a specific case of a linear transformation.  Specifically given a vector space $V$ over a field $F$ such that $\dim V=n$, change of basis is just a transformation from $F^n$ to $F^n$. Does change of basis in and of itself have practical uses that are separate from linear transformations?  What I mean is separate from linear transformations that do more than just change the basis of a vector in it's own vector space.","If i understand it correctly, change of basis is just a specific case of a linear transformation.  Specifically given a vector space $V$ over a field $F$ such that $\dim V=n$, change of basis is just a transformation from $F^n$ to $F^n$. Does change of basis in and of itself have practical uses that are separate from linear transformations?  What I mean is separate from linear transformations that do more than just change the basis of a vector in it's own vector space.",,"['linear-algebra', 'soft-question']"
42,Prove the inequality $\frac{k(k+1)}{2}\left(\frac{a_1^2}{k} + \frac{a_2^2}{k-1} + \ldots + \frac{a_k^2}{1}\right) \geq (a_1 + a_2 + \ldots + a_k)^2$,Prove the inequality,\frac{k(k+1)}{2}\left(\frac{a_1^2}{k} + \frac{a_2^2}{k-1} + \ldots + \frac{a_k^2}{1}\right) \geq (a_1 + a_2 + \ldots + a_k)^2,"I need to prove that $$\frac{k(k+1)}{2}\left(\frac{a_1^2}{k} + \frac{a_2^2}{k-1} + \ldots + \frac{a_k^2}{1}\right) \geq (a_1 + a_2 + \ldots + a_k)^2\;,$$ where $a_1, a_2, \dots, a_k$ is some set of reals. Firstly: Can I presume without the loss of generality that $a_1 \leq a_2 \leq \ldots \leq a_n$ ? This is how far I got: I used the formula $\left \langle a,b \right \rangle \leq |a||b|$: $$\begin{align*}\left \langle a,1 \right \rangle &\leq |a||1|\\ (a_1 + a_2 + \ldots + a_k) &\leq \sqrt{(a_1^2 + a_2^2 + \ldots + a_k^2)}\sqrt{k} \end{align*}$$ Square it: $$(a_1 + a_2 + \ldots + a_k)^2 \leq k(a_1^2 + a_2^2 + \ldots + a_k^2)$$ Now I have to prove that: $$\frac{k(k+1)}{2}\left(\frac{a_1^2}{k} + \frac{a_2^2}{k-1} + \ldots + \frac{a_k^2}{1}\right) \geq k(a_1^2 + a_2^2 + ... + a_k^2)$$ But I'm not sure how. Any pointers?","I need to prove that $$\frac{k(k+1)}{2}\left(\frac{a_1^2}{k} + \frac{a_2^2}{k-1} + \ldots + \frac{a_k^2}{1}\right) \geq (a_1 + a_2 + \ldots + a_k)^2\;,$$ where $a_1, a_2, \dots, a_k$ is some set of reals. Firstly: Can I presume without the loss of generality that $a_1 \leq a_2 \leq \ldots \leq a_n$ ? This is how far I got: I used the formula $\left \langle a,b \right \rangle \leq |a||b|$: $$\begin{align*}\left \langle a,1 \right \rangle &\leq |a||1|\\ (a_1 + a_2 + \ldots + a_k) &\leq \sqrt{(a_1^2 + a_2^2 + \ldots + a_k^2)}\sqrt{k} \end{align*}$$ Square it: $$(a_1 + a_2 + \ldots + a_k)^2 \leq k(a_1^2 + a_2^2 + \ldots + a_k^2)$$ Now I have to prove that: $$\frac{k(k+1)}{2}\left(\frac{a_1^2}{k} + \frac{a_2^2}{k-1} + \ldots + \frac{a_k^2}{1}\right) \geq k(a_1^2 + a_2^2 + ... + a_k^2)$$ But I'm not sure how. Any pointers?",,"['linear-algebra', 'inequality']"
43,A basis for $k(X)$ regarded as a vector space over $k$,A basis for  regarded as a vector space over,k(X) k,"Can anyone give an explicit basis of the $k$-vector space $k(X) = \operatorname{Quot}(k[X])$ of rational functions over $k$? The dimension is given by $$\dim_k k(X) = \max(|k|, |\mathbb N|).$$ If $k$ is infinite, this follows from $|k| \leqslant |k(X)| \leqslant |k[X] \times k[X]| = |k \times k| = |k|$ and the linear independence of $\lbrace \frac{1}{X - \alpha} \mid \alpha \in k\rbrace.$ If $k$ is finite, the result can be obtained similarly from $|k(X)|=|\mathbb N|$ and the linear independence of the monomials $X^n$, $n \geqslant 0$.","Can anyone give an explicit basis of the $k$-vector space $k(X) = \operatorname{Quot}(k[X])$ of rational functions over $k$? The dimension is given by $$\dim_k k(X) = \max(|k|, |\mathbb N|).$$ If $k$ is infinite, this follows from $|k| \leqslant |k(X)| \leqslant |k[X] \times k[X]| = |k \times k| = |k|$ and the linear independence of $\lbrace \frac{1}{X - \alpha} \mid \alpha \in k\rbrace.$ If $k$ is finite, the result can be obtained similarly from $|k(X)|=|\mathbb N|$ and the linear independence of the monomials $X^n$, $n \geqslant 0$.",,"['linear-algebra', 'vector-spaces', 'extension-field']"
44,"""Splitting"" determinant of a matrix","""Splitting"" determinant of a matrix",,"There is a thing that I don't understand about how the determinant of a matrix could be split this way: $$ \begin{vmatrix} a & b\\  c & d \end{vmatrix}=  \begin{vmatrix} a & 0\\  c & d \end{vmatrix} + \begin{vmatrix} 0 & b\\  c & d \end{vmatrix} $$ I read that this ""split"" is possible because of some linearity property but I still cannot figure out how the ""split"" is formed. It could be even split further like: $$ \begin{vmatrix} a & 0\\  c & d \end{vmatrix} = \begin{vmatrix} a & 0\\  c & 0 \end{vmatrix} + \begin{vmatrix} a & 0\\  0 & d \end{vmatrix} $$ How do these ""splitting"" go about? Thanks for any help.","There is a thing that I don't understand about how the determinant of a matrix could be split this way: $$ \begin{vmatrix} a & b\\  c & d \end{vmatrix}=  \begin{vmatrix} a & 0\\  c & d \end{vmatrix} + \begin{vmatrix} 0 & b\\  c & d \end{vmatrix} $$ I read that this ""split"" is possible because of some linearity property but I still cannot figure out how the ""split"" is formed. It could be even split further like: $$ \begin{vmatrix} a & 0\\  c & d \end{vmatrix} = \begin{vmatrix} a & 0\\  c & 0 \end{vmatrix} + \begin{vmatrix} a & 0\\  0 & d \end{vmatrix} $$ How do these ""splitting"" go about? Thanks for any help.",,"['linear-algebra', 'matrices']"
45,Determinants and volume of parallelotopes,Determinants and volume of parallelotopes,,The absolute value of a $2 \times 2$ matrix determinant is the area of a corresponding parallelogram with the $2$ row vectors as sides. The absolute value of a $3 \times 3$ matrix determinant is the volume of a corresponding parallelepiped with the $3$ row vectors as sides. Can it be generalized to $n-D$? The absolute value of an $n \times n$ matrix determinant is the volume of a corresponding $n-$parallelotope?,The absolute value of a $2 \times 2$ matrix determinant is the area of a corresponding parallelogram with the $2$ row vectors as sides. The absolute value of a $3 \times 3$ matrix determinant is the volume of a corresponding parallelepiped with the $3$ row vectors as sides. Can it be generalized to $n-D$? The absolute value of an $n \times n$ matrix determinant is the volume of a corresponding $n-$parallelotope?,,"['linear-algebra', 'geometry', 'determinant']"
46,Why do we only focus on finite-dimensional vector space?,Why do we only focus on finite-dimensional vector space?,,"I have self-learned (theoretical) linear algebra and read various books about this topic, such as Linear Algebra Done Right (Sheldon Axler) and Linear Algebra (Stephen Friedberg et al.) . Also, I think I am familiar with set theory and the axiom of choice since I have completed chapters 3 and 8 of Terence Tao’s book Analysis I . My question: Why these books focus on finite-dimensional vector space but not arbitrary ones? What is the main difference between finite-dimensional vector spaces and infinite-dimensional ones? I will be grateful for any help you can provide.","I have self-learned (theoretical) linear algebra and read various books about this topic, such as Linear Algebra Done Right (Sheldon Axler) and Linear Algebra (Stephen Friedberg et al.) . Also, I think I am familiar with set theory and the axiom of choice since I have completed chapters 3 and 8 of Terence Tao’s book Analysis I . My question: Why these books focus on finite-dimensional vector space but not arbitrary ones? What is the main difference between finite-dimensional vector spaces and infinite-dimensional ones? I will be grateful for any help you can provide.",,"['linear-algebra', 'vector-spaces']"
47,"Suppose $\{v,Av,\cdots,A^{n-1}v\}$ is linearly independent. Prove that if $B$ is any matrix which commutes with $A$, then $B$ is a polynomial in $A$.","Suppose  is linearly independent. Prove that if  is any matrix which commutes with , then  is a polynomial in .","\{v,Av,\cdots,A^{n-1}v\} B A B A","Question: Let $A$ be an $n\times n$ square matrix and $v$ a column vector.  Suppose $\{v,Av,\cdots,A^{n-1}v\}$ is linearly independent.  Prove that if $B$ is any matrix which commutes with $A$ , then $B$ is a polynomial in $A$ . Thoughts: Let $A$ be $n\times n$ .  Wouldn't the matricies that commute with $A$ be a subspace of $\{v,Av,\cdots,A^{n-1}v\}$ (not sure how to formally prove this)?  And since $\{v,Av,\cdots,A^{n-1}v\}$ is linearly independent, then if $B$ were a polynomial in $A$ , it would have degree $\leq n-1$ , but I am a bit lost in seeing the details of the proof.  Any help is greatly appreciated!  Also, this is not a homework problem of mine, I have just been recently going over some old linear algebra stuff, and I just want to make sure I am seeing the details to better understand some of this stuff :)","Question: Let be an square matrix and a column vector.  Suppose is linearly independent.  Prove that if is any matrix which commutes with , then is a polynomial in . Thoughts: Let be .  Wouldn't the matricies that commute with be a subspace of (not sure how to formally prove this)?  And since is linearly independent, then if were a polynomial in , it would have degree , but I am a bit lost in seeing the details of the proof.  Any help is greatly appreciated!  Also, this is not a homework problem of mine, I have just been recently going over some old linear algebra stuff, and I just want to make sure I am seeing the details to better understand some of this stuff :)","A n\times n v \{v,Av,\cdots,A^{n-1}v\} B A B A A n\times n A \{v,Av,\cdots,A^{n-1}v\} \{v,Av,\cdots,A^{n-1}v\} B A \leq n-1","['linear-algebra', 'abstract-algebra', 'vector-spaces']"
48,Turning an Abelian Group into a Vector Space,Turning an Abelian Group into a Vector Space,,"This question is inspired by the question Are these vector spaces? but is free-standing. Suppose $\mathbb{F}$ is a field, and that $X$ is the multiplicative group of $\mathbb{F}$ . Let us write this abelian group $X$ additively. To be precise the set is $\mathbb{F}\setminus\{0\}$ , the zero element is $\hat{0}:=1$ , the addition operation is $x \hat{+}y:=xy$ , and the negative operation is $\widehat{-}x:=x^{-1}$ . Let $V$ be the abelian group of $n$ -tuples $X^n$ with the usual co-ordinatewise operations. Question : Can we define an $\mathbb{F}$ -scalar multiplication on $V$ so that $V$ becomes an $\mathbb{F}$ -vector space? By looking at $-1$ in $\mathbb{F}$ , which satisfies $(-1)\hat{+}(-1)=(-1)^2=1=\hat{0}$ we see that if there is to be any chance of turning $V$ into a vector space then the field $\mathbb{F}$ must have characteristic $2$ . More than that, a similar argument on $(2^k -1)$ -th roots of unity will show that $\mathbb{F}$ has no finite subfields $\mathbb{F}_{2^k}$ except $\mathbb{F}_2$ . Beyond that I cannot go.","This question is inspired by the question Are these vector spaces? but is free-standing. Suppose is a field, and that is the multiplicative group of . Let us write this abelian group additively. To be precise the set is , the zero element is , the addition operation is , and the negative operation is . Let be the abelian group of -tuples with the usual co-ordinatewise operations. Question : Can we define an -scalar multiplication on so that becomes an -vector space? By looking at in , which satisfies we see that if there is to be any chance of turning into a vector space then the field must have characteristic . More than that, a similar argument on -th roots of unity will show that has no finite subfields except . Beyond that I cannot go.",\mathbb{F} X \mathbb{F} X \mathbb{F}\setminus\{0\} \hat{0}:=1 x \hat{+}y:=xy \widehat{-}x:=x^{-1} V n X^n \mathbb{F} V V \mathbb{F} -1 \mathbb{F} (-1)\hat{+}(-1)=(-1)^2=1=\hat{0} V \mathbb{F} 2 (2^k -1) \mathbb{F} \mathbb{F}_{2^k} \mathbb{F}_2,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'commutative-algebra', 'modules']"
49,"Puzzle group of $4\times 4$ ""flip"" game","Puzzle group of  ""flip"" game",4\times 4,"I have been goofing around with the game ""flip,"" which can be played at the following link . The puzzle consists of an $n\times n$ grid of squares that are either black or white, and when one clicks on one of the squares, its color and the colors of each of its four neighbors are toggled. It is easy to show that any board configuration is solvable for the $2\times 2$ and $3\times 3$ versions of the puzzle. Thus, since game moves commute, the groups corresponding to these puzzles are $\mathbb Z_2^4$ and $\mathbb Z_2^9$ respectively. However, I have discovered that the $4\times 4$ game is not so simple; there exist some board configurations that are not solvable. This can be demonstrated as follows. Suppose we color the board like this: One may easily verify that any move toggles an even number of the red-colored squares; thus, in any solvable puzzles, the number of black squares in the red region is even. This narrows down the group of solvable puzzles to $\mathbb Z_2^{15}$ ; however, since reflections/rotations of this coloring produce further restrictions, the group must be even smaller than this. In fact, using this strategy, I have narrowed it down to a subgroup of $\mathbb Z_2^{14}$ , and I suspect it can be narrowed down even further to $\mathbb Z_2^{13}$ . My question is: what is the puzzle group of the $4\times 4$ puzzle? I know this problem can be reduced to finding the rank of a $16\times 16$ matrix, but... surely there's a better way.","I have been goofing around with the game ""flip,"" which can be played at the following link . The puzzle consists of an grid of squares that are either black or white, and when one clicks on one of the squares, its color and the colors of each of its four neighbors are toggled. It is easy to show that any board configuration is solvable for the and versions of the puzzle. Thus, since game moves commute, the groups corresponding to these puzzles are and respectively. However, I have discovered that the game is not so simple; there exist some board configurations that are not solvable. This can be demonstrated as follows. Suppose we color the board like this: One may easily verify that any move toggles an even number of the red-colored squares; thus, in any solvable puzzles, the number of black squares in the red region is even. This narrows down the group of solvable puzzles to ; however, since reflections/rotations of this coloring produce further restrictions, the group must be even smaller than this. In fact, using this strategy, I have narrowed it down to a subgroup of , and I suspect it can be narrowed down even further to . My question is: what is the puzzle group of the puzzle? I know this problem can be reduced to finding the rank of a matrix, but... surely there's a better way.",n\times n 2\times 2 3\times 3 \mathbb Z_2^4 \mathbb Z_2^9 4\times 4 \mathbb Z_2^{15} \mathbb Z_2^{14} \mathbb Z_2^{13} 4\times 4 16\times 16,"['linear-algebra', 'group-theory', 'logic', 'abelian-groups', 'puzzle']"
50,"Prove that the polar decomposition of normal matrices, $A=SU$, is such that $SU=US$","Prove that the polar decomposition of normal matrices, , is such that",A=SU SU=US,Assume $A$ is a normal matrix. Suppose $A=SU$ is a polar decomposition of $A$ . Prove that $SU=US$ . I have no idea to prove this. $A$ is normal then $AA^*=A^*A$ . And then we have $$ SS^*=U^*S^*SU. $$ But I don't know how to continue.,Assume is a normal matrix. Suppose is a polar decomposition of . Prove that . I have no idea to prove this. is normal then . And then we have But I don't know how to continue.,"A A=SU A SU=US A AA^*=A^*A 
SS^*=U^*S^*SU.
","['linear-algebra', 'matrices', 'matrix-decomposition']"
51,Condition number for polynomial interpolation matrix,Condition number for polynomial interpolation matrix,,"We want to interpolate a function $\,f:\mathbb{R}\to\mathbb{R}$ on the interval $[0,1]$ with, say, monomials. Assume we have set $\left\{x_i\right\}_{i=0}^{n}$ of $n+1$ points $x_i\in\left[0,1\right],\; i = 0,\dots, n,$ which are not uniformly distributed and for which we know values $\,f_i = f\left(x_i\right)$ of function $\,f$. Following standard interpolation techniques, we write approximating polynomial $P_n(x)$ as linear combination of monomials: $$ P_n\left(x\right) = a_0 + a_1 x + a_2 x^2 + \dots + a_nx^n  = \sum_{i=0}^{n} a_ix^i $$ where $a_i\in \mathbb R,\; i=0,\dots,n$ are unknown coefficients we need to determine. Estimating monomials at points $\left\{x_i\right\}_{i=0}^{n}$ yields system of linear equations $$ \underbrace{ \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^n \\ 1 & x_2 & x_2^2 & \cdots & x_2^n \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^n \\ \end{bmatrix}}_{\quad\;\style{display: inline-block; transform-origin: 50% 50% 0px; transform: rotate(30deg); }{:= \boldsymbol{M}} } \cdot \underbrace{ \begin{bmatrix}a_0 \\ a_1 \\ \vdots \\ a_n\end{bmatrix} }_ {\quad\style{display: inline-block; transform-origin: 50% 50% 0px; transform: rotate(30deg); }{:= \overline{\boldsymbol{a}}} } = \underbrace{\begin{bmatrix}f_0 \\ f_1 \\ \vdots \\ f_n\end{bmatrix} }_ {\quad\style{display: inline-block; transform-origin: 50% 50% 0px; transform: rotate(30deg); }{:= \overline{\boldsymbol{f}}}  } %\qquad \iff \qquad %\boldsymbol{M} \cdot\overline{\boldsymbol{a}} = \overline{\boldsymbol{f}} $$ which we can rewrite as $\;\boldsymbol{M} \cdot\overline{\boldsymbol{a}} = \overline{\boldsymbol{f}}$. I am trying to figure out how does condition number $\,\kappa\left(\boldsymbol{M}\right)$ of the matrix depends on the minimal distance $h$ between points $\left\{ x_i \right\}_{i=0}^{n}$: $$h=\min_{i,j=0\ldots n} \left\lVert x_i - x_j\right\rVert.$$ Intuitively $\kappa\left(\boldsymbol{M}\right)$ should grow as $h\to0$, which matches results of numerical experiments. However, I am clueless about the exact type of relation between $\kappa$ and $h$. Any help is appreciated.","We want to interpolate a function $\,f:\mathbb{R}\to\mathbb{R}$ on the interval $[0,1]$ with, say, monomials. Assume we have set $\left\{x_i\right\}_{i=0}^{n}$ of $n+1$ points $x_i\in\left[0,1\right],\; i = 0,\dots, n,$ which are not uniformly distributed and for which we know values $\,f_i = f\left(x_i\right)$ of function $\,f$. Following standard interpolation techniques, we write approximating polynomial $P_n(x)$ as linear combination of monomials: $$ P_n\left(x\right) = a_0 + a_1 x + a_2 x^2 + \dots + a_nx^n  = \sum_{i=0}^{n} a_ix^i $$ where $a_i\in \mathbb R,\; i=0,\dots,n$ are unknown coefficients we need to determine. Estimating monomials at points $\left\{x_i\right\}_{i=0}^{n}$ yields system of linear equations $$ \underbrace{ \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^n \\ 1 & x_2 & x_2^2 & \cdots & x_2^n \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^n \\ \end{bmatrix}}_{\quad\;\style{display: inline-block; transform-origin: 50% 50% 0px; transform: rotate(30deg); }{:= \boldsymbol{M}} } \cdot \underbrace{ \begin{bmatrix}a_0 \\ a_1 \\ \vdots \\ a_n\end{bmatrix} }_ {\quad\style{display: inline-block; transform-origin: 50% 50% 0px; transform: rotate(30deg); }{:= \overline{\boldsymbol{a}}} } = \underbrace{\begin{bmatrix}f_0 \\ f_1 \\ \vdots \\ f_n\end{bmatrix} }_ {\quad\style{display: inline-block; transform-origin: 50% 50% 0px; transform: rotate(30deg); }{:= \overline{\boldsymbol{f}}}  } %\qquad \iff \qquad %\boldsymbol{M} \cdot\overline{\boldsymbol{a}} = \overline{\boldsymbol{f}} $$ which we can rewrite as $\;\boldsymbol{M} \cdot\overline{\boldsymbol{a}} = \overline{\boldsymbol{f}}$. I am trying to figure out how does condition number $\,\kappa\left(\boldsymbol{M}\right)$ of the matrix depends on the minimal distance $h$ between points $\left\{ x_i \right\}_{i=0}^{n}$: $$h=\min_{i,j=0\ldots n} \left\lVert x_i - x_j\right\rVert.$$ Intuitively $\kappa\left(\boldsymbol{M}\right)$ should grow as $h\to0$, which matches results of numerical experiments. However, I am clueless about the exact type of relation between $\kappa$ and $h$. Any help is appreciated.",,"['linear-algebra', 'interpolation', 'condition-number']"
52,Vector orthogonal to hyperplane,Vector orthogonal to hyperplane,,"Finding the vector perpendicular to the plane Why is weight vector orthogonal to decision plane in neural networks I am thinking now about hyperplanes and orthogonal vectors. My problem is following one: Definition of hyperplane: $$H=\left\{ x\ \epsilon\ R^2 : u^Tx=v \right\}$$ Consider for instance simple case of hyperplane in $R^2$ (straight line): $$H=\left\{ x\ \epsilon\ R^2 : u^Tx=3 \right\}$$ so that $x_{1}-x_{2}=3$ so $u=\left(\begin{array}{c}1\\ -1\end{array}\right)$ take any point from that straight line : $x=\left(\begin{array}{c}4\\ 1\end{array}\right)$ $$u^Tx\neq0$$ So we conclude that points on this line (hyperplane) are not orhogonal to vector of coefficients $u$, and this is the case (they are othogonal) only if: $$H=\left\{ x\ \epsilon\ R^2 : u^Tx=0 \right\}$$ In case of plane we have following definition of hyperplane : $$H=\left\{ x\ \epsilon\ R^3 : u^Tx=v \right\}$$ then similarily $x$ and $u$ are orthogonal only in case $v=0$ I came across following definition which confused me a little bit: $$H=\left\{ x\ \epsilon\ R^n : u^Tx=v \right\}=\left\{ x\ \epsilon\ R^n : u^T(x-a)=0 \right\}$$ where $a$ is any point on hyperplane ( so $u^Ta-v=0$). Therefore, the hyperplane H consists of the points x for which $<u, x — a> = 0$.   In other words, the hyperplane H consists of the points x for which the vectors u and x-a are orthogonal So this is the case such that $(x-a)\ \epsilon\ R^n$ and $v=0$, thats why $(x-a)$ is orthogonal to $u$. So why in  literature about for instance about Suppor Vector Machines, we assume that vector of coefficients $u$ is orthogonal to separating hyperplane, for cases where $v\neq0$? Shouldn't it be like if $u$ is orthogonal to some hyperplane, then $u$ is orthogonal to every vector $x$ in such hyperplane? I certainly misuse some concepts, so could you provide step by step proof of the fact that vector of coefficients $u$ is orthogonal to any hyperplane?","Finding the vector perpendicular to the plane Why is weight vector orthogonal to decision plane in neural networks I am thinking now about hyperplanes and orthogonal vectors. My problem is following one: Definition of hyperplane: $$H=\left\{ x\ \epsilon\ R^2 : u^Tx=v \right\}$$ Consider for instance simple case of hyperplane in $R^2$ (straight line): $$H=\left\{ x\ \epsilon\ R^2 : u^Tx=3 \right\}$$ so that $x_{1}-x_{2}=3$ so $u=\left(\begin{array}{c}1\\ -1\end{array}\right)$ take any point from that straight line : $x=\left(\begin{array}{c}4\\ 1\end{array}\right)$ $$u^Tx\neq0$$ So we conclude that points on this line (hyperplane) are not orhogonal to vector of coefficients $u$, and this is the case (they are othogonal) only if: $$H=\left\{ x\ \epsilon\ R^2 : u^Tx=0 \right\}$$ In case of plane we have following definition of hyperplane : $$H=\left\{ x\ \epsilon\ R^3 : u^Tx=v \right\}$$ then similarily $x$ and $u$ are orthogonal only in case $v=0$ I came across following definition which confused me a little bit: $$H=\left\{ x\ \epsilon\ R^n : u^Tx=v \right\}=\left\{ x\ \epsilon\ R^n : u^T(x-a)=0 \right\}$$ where $a$ is any point on hyperplane ( so $u^Ta-v=0$). Therefore, the hyperplane H consists of the points x for which $<u, x — a> = 0$.   In other words, the hyperplane H consists of the points x for which the vectors u and x-a are orthogonal So this is the case such that $(x-a)\ \epsilon\ R^n$ and $v=0$, thats why $(x-a)$ is orthogonal to $u$. So why in  literature about for instance about Suppor Vector Machines, we assume that vector of coefficients $u$ is orthogonal to separating hyperplane, for cases where $v\neq0$? Shouldn't it be like if $u$ is orthogonal to some hyperplane, then $u$ is orthogonal to every vector $x$ in such hyperplane? I certainly misuse some concepts, so could you provide step by step proof of the fact that vector of coefficients $u$ is orthogonal to any hyperplane?",,"['linear-algebra', 'multivariable-calculus', 'optimization']"
53,Find dimension of the space of all linear maps from $\Bbb R^{13}$ to $\mathbb R^{31}$,Find dimension of the space of all linear maps from  to,\Bbb R^{13} \mathbb R^{31},"Let $V$ be a subspace of $\mathbb R^{13}$ of dimension $6$ and $W$ be a subspace of $\Bbb R^{31}$ of dimension $29$. What's the dimension of the space of all linear maps from $\Bbb R^{13}$ to $\mathbb R^{31}$ whose kernel contains $V$ and whose image is contained in $W$. Here,  $ \dim(V)=6$ and $\dim(W)=29$. Also , $V\subset \ker(T)$ and $\text{Im}(T) \subset W$. So, $\dim(\text{Im}(T))\le 29$ and $\dim(\ker(T))\ge 6$. Now, $\dim(\ker(T))+\dim(\text{Im}(T))=13=\begin{cases}6+7\\7+6\\8+5\\9+4\\10+3\\11+2\\12+1\end{cases}$ So there are $7$ cases. So dimension of the space of all linear maps from $\Bbb R^{13}$ to $\mathbb R^{31}$ is $7$. Is it correct or there are something wrong?","Let $V$ be a subspace of $\mathbb R^{13}$ of dimension $6$ and $W$ be a subspace of $\Bbb R^{31}$ of dimension $29$. What's the dimension of the space of all linear maps from $\Bbb R^{13}$ to $\mathbb R^{31}$ whose kernel contains $V$ and whose image is contained in $W$. Here,  $ \dim(V)=6$ and $\dim(W)=29$. Also , $V\subset \ker(T)$ and $\text{Im}(T) \subset W$. So, $\dim(\text{Im}(T))\le 29$ and $\dim(\ker(T))\ge 6$. Now, $\dim(\ker(T))+\dim(\text{Im}(T))=13=\begin{cases}6+7\\7+6\\8+5\\9+4\\10+3\\11+2\\12+1\end{cases}$ So there are $7$ cases. So dimension of the space of all linear maps from $\Bbb R^{13}$ to $\mathbb R^{31}$ is $7$. Is it correct or there are something wrong?",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
54,Proving positive definiteness of matrix $a_{ij}=\frac{2x_ix_j}{x_i + x_j}$,Proving positive definiteness of matrix,a_{ij}=\frac{2x_ix_j}{x_i + x_j},"I'm trying to prove that the matrix with entries $\left\{\frac{2x_ix_j}{x_i + x_j}\right\}_{ij}$ is positive definite for all n, where n is the number of rows/columns. I was able to prove it for the 2x2 case by showing the determinant is always positive. However, once I extend it to the 3x3 case I run into trouble. I found a question here whose chosen answer gave a condition for positive definiteness of the extended matrix, and after evaluating the condition and maximizing it via software, the inequality turned out to hold indeed, but I just can't show it. Furthermore, it would be way more complicated when I go to 4x4 and higher. I think I should somehow use induction here to show it for all n, but I think I'm missing something. Any help is appreciated. Edit: Actually the mistake is mine, turns out there are indeed no squares in the denominator, so it turns out user141614's first answer is what I really needed. Thanks a lot! Should I just accept this answer, or should it be changed back to his first answer then I accept it?","I'm trying to prove that the matrix with entries $\left\{\frac{2x_ix_j}{x_i + x_j}\right\}_{ij}$ is positive definite for all n, where n is the number of rows/columns. I was able to prove it for the 2x2 case by showing the determinant is always positive. However, once I extend it to the 3x3 case I run into trouble. I found a question here whose chosen answer gave a condition for positive definiteness of the extended matrix, and after evaluating the condition and maximizing it via software, the inequality turned out to hold indeed, but I just can't show it. Furthermore, it would be way more complicated when I go to 4x4 and higher. I think I should somehow use induction here to show it for all n, but I think I'm missing something. Any help is appreciated. Edit: Actually the mistake is mine, turns out there are indeed no squares in the denominator, so it turns out user141614's first answer is what I really needed. Thanks a lot! Should I just accept this answer, or should it be changed back to his first answer then I accept it?",,"['linear-algebra', 'matrices', 'positive-definite']"
55,Proving the intersection of distinct eigenspaces is trivial,Proving the intersection of distinct eigenspaces is trivial,,"Suppose $\lambda_1$ and $\lambda_2$ are different eigenvalues of $T$. Prove $E_{\lambda_1} \cap E_{\lambda_2}= \{\vec0\}$.  I have a basic idea of what to do. Since both eigenvalues are distinct, doesn't that mean the basis for each space are linearly independent of each other so that no vector in one is in the span of the basis of the other? I'm just looking on how to formalize these ideas.","Suppose $\lambda_1$ and $\lambda_2$ are different eigenvalues of $T$. Prove $E_{\lambda_1} \cap E_{\lambda_2}= \{\vec0\}$.  I have a basic idea of what to do. Since both eigenvalues are distinct, doesn't that mean the basis for each space are linearly independent of each other so that no vector in one is in the span of the basis of the other? I'm just looking on how to formalize these ideas.",,['linear-algebra']
56,Image of dual map is annihilator of kernel,Image of dual map is annihilator of kernel,,"Suppose $T:V\to W$ and that $V$ is finite-dimensional. I want to prove that $$\text{Im }T'=(\ker T)^0$$ where $T'$ is the dual/transpose map and $(\ker T)^0$ is the annihilator of the kernel. I know that $\phi \in V'$ is an annihilator of $\ker T$ if and only if $$\phi(v)=0 \space \forall v\in \ker T$$ if and only if $$\phi(v)=0 \space \forall v \in V \text{ such that } T(v)=0$$ Now I also know that $\phi \in \text{Im }T'\subset V'$ if and only if $$\phi = f \circ T \text{ for some } f \in W'$$ I can see that if $v\in \ker T$, then this implies $\phi(v) = f(T(v)) = 0$, so $\phi \in (\ker T)^0.$ However, I'm having trouble showing the other way, that if $\phi \in (\ker T)^0$, then $\phi \in \text{Im }T'.$ Could anybody help?","Suppose $T:V\to W$ and that $V$ is finite-dimensional. I want to prove that $$\text{Im }T'=(\ker T)^0$$ where $T'$ is the dual/transpose map and $(\ker T)^0$ is the annihilator of the kernel. I know that $\phi \in V'$ is an annihilator of $\ker T$ if and only if $$\phi(v)=0 \space \forall v\in \ker T$$ if and only if $$\phi(v)=0 \space \forall v \in V \text{ such that } T(v)=0$$ Now I also know that $\phi \in \text{Im }T'\subset V'$ if and only if $$\phi = f \circ T \text{ for some } f \in W'$$ I can see that if $v\in \ker T$, then this implies $\phi(v) = f(T(v)) = 0$, so $\phi \in (\ker T)^0.$ However, I'm having trouble showing the other way, that if $\phi \in (\ker T)^0$, then $\phi \in \text{Im }T'.$ Could anybody help?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'duality-theorems']"
57,The $\exp \circ \log$ function acts as the identity on unipotent matrices.,The  function acts as the identity on unipotent matrices.,\exp \circ \log,"I am working through the exercises in ""Lie Groups, Lie Algebras, and Representations"" - Hall and can't complete exercise 9 of chapter 2 using the provided hint. In chapter 2, Hall defines the matrix exponential as $$e^X = \sum_{m=0}^\infty \frac{X^m}{m!}$$ and the matrix logarithm as  $$\log A = \sum_{m=1}^\infty (-1)^{m+1}\frac{(A - I)^m}{m}$$ Exercise 9c asks me to show that $\exp(\log(A)) = A$ when $A$ is unipotent.  The hint provided is: Let $A(t) = I + t(A - I)$.  Show that $\exp(\log(A(t))$ depends polynomially on $t$ and that $\exp(\log(A(t))) = A(t)$ for all sufficiently small $t$. I have already have a brute force proof that explicitly composes $e$ with $log$ to demonstrates the claim, and I have verified the hint is true, but I would like to understand how to conclude the statement from the hint. Verification of the hint for interested future readers: Theorem 2.8 shows that for $||A - I|| < 1$ (Hilbert-Schmidt Norm) that $e^{\log A} = A$.  The matrix in question $A(t)$ has norm $||A(t) - I|| = ||t(A - I)||$ which can be made arbitrarily small for small $t$.","I am working through the exercises in ""Lie Groups, Lie Algebras, and Representations"" - Hall and can't complete exercise 9 of chapter 2 using the provided hint. In chapter 2, Hall defines the matrix exponential as $$e^X = \sum_{m=0}^\infty \frac{X^m}{m!}$$ and the matrix logarithm as  $$\log A = \sum_{m=1}^\infty (-1)^{m+1}\frac{(A - I)^m}{m}$$ Exercise 9c asks me to show that $\exp(\log(A)) = A$ when $A$ is unipotent.  The hint provided is: Let $A(t) = I + t(A - I)$.  Show that $\exp(\log(A(t))$ depends polynomially on $t$ and that $\exp(\log(A(t))) = A(t)$ for all sufficiently small $t$. I have already have a brute force proof that explicitly composes $e$ with $log$ to demonstrates the claim, and I have verified the hint is true, but I would like to understand how to conclude the statement from the hint. Verification of the hint for interested future readers: Theorem 2.8 shows that for $||A - I|| < 1$ (Hilbert-Schmidt Norm) that $e^{\log A} = A$.  The matrix in question $A(t)$ has norm $||A(t) - I|| = ||t(A - I)||$ which can be made arbitrarily small for small $t$.",,"['linear-algebra', 'matrices']"
58,How to check if a point is in the direction of the normal of a plane?,How to check if a point is in the direction of the normal of a plane?,,"I have a plane, defined by a normal vector $n$ and a point $p$. I also have a point $a = (x, y, z)$. Based on this information, how do I know if the point $a$ exists somewhere past the plane in the direction of the normal?","I have a plane, defined by a normal vector $n$ and a point $p$. I also have a point $a = (x, y, z)$. Based on this information, how do I know if the point $a$ exists somewhere past the plane in the direction of the normal?",,"['linear-algebra', 'geometry', '3d']"
59,Angle between two 4D vectors,Angle between two 4D vectors,,"I was wondering if there is any difference between finding the angle between two 4D vectors as opposed to finding the angle between two 3D vectors? I have $u = (1, 0, 1, 0)$ , $v = (-3, -3, -3, -3)$ and used the dot product to find the angle between them. We have that: $u\cdot v = -6$ $||u|| = \sqrt 2$ $||v|| = \sqrt{36}$ Then, $$\cos(\theta) = \frac{u . v} {||u||\cdot ||v||}=\frac{-6 }{ \sqrt 6 \cdot \sqrt{36}} = -0.71,$$ so $$\theta = \cos^{-1} (-0.71)$$ And the angle between $u$ and $v$ is $135$ degrees or $2.36$ radians.","I was wondering if there is any difference between finding the angle between two 4D vectors as opposed to finding the angle between two 3D vectors? I have $u = (1, 0, 1, 0)$ , $v = (-3, -3, -3, -3)$ and used the dot product to find the angle between them. We have that: $u\cdot v = -6$ $||u|| = \sqrt 2$ $||v|| = \sqrt{36}$ Then, $$\cos(\theta) = \frac{u . v} {||u||\cdot ||v||}=\frac{-6 }{ \sqrt 6 \cdot \sqrt{36}} = -0.71,$$ so $$\theta = \cos^{-1} (-0.71)$$ And the angle between $u$ and $v$ is $135$ degrees or $2.36$ radians.",,['linear-algebra']
60,If $A^2 =0$ then possible rank of $A$,If  then possible rank of,A^2 =0 A,"Let, $A$ be a non zero matrix of order $8$ with $A^2 =0.$ Then one of the possible value for rank of $A$ is (a) $5$ (b) $4$ (c) $6$ (d) $8$ . Attempt : As , $A^2=0$ , so $A$ is a nilpotent matrix of order $2$ . So , characteristic polynomial of $A$ is of the form $x^2f(x)$ , where $f(x)$ is a polynomial of degree $6$ . So , possible values of $rank(A)$ are $4,5,6$ . Am I correct ?","Let, be a non zero matrix of order with Then one of the possible value for rank of is (a) (b) (c) (d) . Attempt : As , , so is a nilpotent matrix of order . So , characteristic polynomial of is of the form , where is a polynomial of degree . So , possible values of are . Am I correct ?","A 8 A^2 =0. A 5 4 6 8 A^2=0 A 2 A x^2f(x) f(x) 6 rank(A) 4,5,6","['linear-algebra', 'vector-spaces']"
61,Determinant of the linear map given by conjugation.,Determinant of the linear map given by conjugation.,,"Let $S$ denote the space of skew-symmetric $n\times n$ real matrices, where every element $A\in S$ satisfies $A^T+A = 0$. Let $M$ denote an orthogonal $n\times n$ matrix, and $L_M$ denotes the linear map $$L_M: A\mapsto MAM^{-1}.$$ What is the determinant of $L_M$ as an endomorphism of $S$?","Let $S$ denote the space of skew-symmetric $n\times n$ real matrices, where every element $A\in S$ satisfies $A^T+A = 0$. Let $M$ denote an orthogonal $n\times n$ matrix, and $L_M$ denotes the linear map $$L_M: A\mapsto MAM^{-1}.$$ What is the determinant of $L_M$ as an endomorphism of $S$?",,"['linear-algebra', 'abstract-algebra', 'determinant']"
62,How to prove $\sum_{i=1}^k(\frac{1}{\alpha_i}\prod_{j\neq i}^k\frac{\alpha_j}{\alpha_j-\alpha_i})=\sum_{i=1}^k\frac{1}{\alpha_i}$?,How to prove ?,\sum_{i=1}^k(\frac{1}{\alpha_i}\prod_{j\neq i}^k\frac{\alpha_j}{\alpha_j-\alpha_i})=\sum_{i=1}^k\frac{1}{\alpha_i},"How to prove $\sum_{i=1}^k(\frac{1}{\alpha_i}\prod_{j\neq i}^k\frac{\alpha_j}{\alpha_j-\alpha_i})=\sum_{i=1}^k\frac{1}{\alpha_i}$? Where $\alpha_1, \alpha_2,\ldots, \alpha_k$  are $k$ distinct positive numbers.","How to prove $\sum_{i=1}^k(\frac{1}{\alpha_i}\prod_{j\neq i}^k\frac{\alpha_j}{\alpha_j-\alpha_i})=\sum_{i=1}^k\frac{1}{\alpha_i}$? Where $\alpha_1, \alpha_2,\ldots, \alpha_k$  are $k$ distinct positive numbers.",,"['linear-algebra', 'polynomials']"
63,Show $\ker(\alpha)=\ker(\alpha)^2 \ \iff \ \ker(\alpha)\cap \mathrm{Im}(\alpha)=\{0\}$,Show,\ker(\alpha)=\ker(\alpha)^2 \ \iff \ \ker(\alpha)\cap \mathrm{Im}(\alpha)=\{0\},"Let $V$ be a vector space over a field $F$ and let $\alpha$ be an element of $\operatorname{End}(V)$ . Show $\ker(\alpha)=\ker(\alpha^2)$ iff $\ker(\alpha)$ and $\operatorname{im}(\alpha)$ are linearly disjoint. so, i know that $\ker(\alpha)\subseteq \ker(\alpha^2)$ is always true If $v$ is an element of $\ker(\alpha)\implies \alpha(r)=0_V$ $\implies\alpha(\alpha(r))=\alpha(0)=0_V$ $\implies \alpha^2(v)=0_V \implies v \in \ker(\alpha^2)$ $\ker(\alpha)=ker(\alpha^2)\Leftrightarrow \ker(\alpha^2) \subseteq \ker(\alpha)$ Now, I know that I'm suppose to take some $u$ element of $\ker(\alpha)\cap \operatorname{im}(\alpha)$ and show that = $0_V$ Then I'm suppose to take $u$ element of $\ker(\alpha^2)$ and show $u$ element of $\ker(\alpha)$ where $\alpha^2(u)=0$ , and $\alpha(\alpha(u))=0$ Im just not sure how to proceed from here.","Let be a vector space over a field and let be an element of . Show iff and are linearly disjoint. so, i know that is always true If is an element of Now, I know that I'm suppose to take some element of and show that = Then I'm suppose to take element of and show element of where , and Im just not sure how to proceed from here.",V F \alpha \operatorname{End}(V) \ker(\alpha)=\ker(\alpha^2) \ker(\alpha) \operatorname{im}(\alpha) \ker(\alpha)\subseteq \ker(\alpha^2) v \ker(\alpha)\implies \alpha(r)=0_V \implies\alpha(\alpha(r))=\alpha(0)=0_V \implies \alpha^2(v)=0_V \implies v \in \ker(\alpha^2) \ker(\alpha)=ker(\alpha^2)\Leftrightarrow \ker(\alpha^2) \subseteq \ker(\alpha) u \ker(\alpha)\cap \operatorname{im}(\alpha) 0_V u \ker(\alpha^2) u \ker(\alpha) \alpha^2(u)=0 \alpha(\alpha(u))=0,"['linear-algebra', 'vector-spaces', 'vector-fields']"
64,The map that sends $A$ to its greatest eigenvalue is continuous.,The map that sends  to its greatest eigenvalue is continuous.,A,"The map $f:S_n(\mathbb R)\to \mathbb R$ such that $f(M)$ is the greatest eigenvalue of $M$ is continuous ( $S_n(\mathbb R)$ is the set of symmetric matrices) I need to prove this result in order to solve a broader problem. I never feel comfortable when proving continuity of such maps. I'd say that the map that takes $M$ to $\chi_M$ is continuous (why?), and the map that sends a real polynomial with only real roots $Q$ to its greatest root is continuous (why again?). For the first question, I may argue that the coefficients of $\chi_M$ are polynomials in the coefficients of $M$ (doesn't sound very rigorous though)... The original problem is the following Let $\delta$ and $M$ be positive reals. Let $S_n(\mathbb R)^{++}$ denote the set of real positive definite matrices. Prove the set $\{ A\in S_n(\mathbb R)^{++} | \det A \geq \delta \;\;\text{and}\; \; sp(A)\subset [0,M]\}$ is compact.","The map such that is the greatest eigenvalue of is continuous ( is the set of symmetric matrices) I need to prove this result in order to solve a broader problem. I never feel comfortable when proving continuity of such maps. I'd say that the map that takes to is continuous (why?), and the map that sends a real polynomial with only real roots to its greatest root is continuous (why again?). For the first question, I may argue that the coefficients of are polynomials in the coefficients of (doesn't sound very rigorous though)... The original problem is the following Let and be positive reals. Let denote the set of real positive definite matrices. Prove the set is compact.","f:S_n(\mathbb R)\to \mathbb R f(M) M S_n(\mathbb R) M \chi_M Q \chi_M M \delta M S_n(\mathbb R)^{++} \{ A\in S_n(\mathbb R)^{++} | \det A \geq \delta \;\;\text{and}\; \; sp(A)\subset [0,M]\}","['linear-algebra', 'matrices', 'continuity']"
65,A quick way to estimate eigenvector/eigenvalue of a matrix,A quick way to estimate eigenvector/eigenvalue of a matrix,,"Is there a quick way to give a raw estimation of an eigenvector/eigenvalue of a matrix? By ""quick"" I mean some method which can be computed without a computer or paper and pencil...something you could do in your head","Is there a quick way to give a raw estimation of an eigenvector/eigenvalue of a matrix? By ""quick"" I mean some method which can be computed without a computer or paper and pencil...something you could do in your head",,"['linear-algebra', 'eigenvalues-eigenvectors']"
66,A is a Hermitian projection if and only if it is an orthogonal projection,A is a Hermitian projection if and only if it is an orthogonal projection,,"I need to figure out this property of Hermitian / Orthogonal projections ""A is a Hermitian projection if and only if it is an orthogonal projection"" Your assistance will be highly appreciated. Thank You","I need to figure out this property of Hermitian / Orthogonal projections ""A is a Hermitian projection if and only if it is an orthogonal projection"" Your assistance will be highly appreciated. Thank You",,"['linear-algebra', 'matrices']"
67,Linear algebra questions that a high-schooler could explore,Linear algebra questions that a high-schooler could explore,,Are there any deep/significant concepts in linear algebra that are not overly complicated that a high schooler could explore in depth?,Are there any deep/significant concepts in linear algebra that are not overly complicated that a high schooler could explore in depth?,,"['linear-algebra', 'soft-question']"
68,"Affine group, semi-direct product and linear transformations","Affine group, semi-direct product and linear transformations",,"According to wikipedia the Affine group is the semi-direct product of a vector space $V$ and the general linear group $GL(V)$. Here is the definition of the semi-direct product in terms of matrices and vectors as given in the wikipedia link: \begin{equation}   (M_1, v_1)\cdot(M_2, v_2) = (M_1 M_2, v_1 + M_1v_2) \end{equation} By definition, I know the affine group is the set of all invertible affine maps and I know that the general linear group is the set of all invertible linear maps. I also know that an affine map is just a linear map without preserving the origin, and hence all linear maps are affine maps. It seems reasonable to me that affine maps can be constructed from linear maps, but I do not see everything from the above definition of semi-direct product. The only other idea I have is relating the difference between linear maps and affine maps, which is: translations. So I am thinking that the second coordinate vector gives us our translation, and the first coordinate gives us our linear transformation, however why not switch $v_1$ and $v_2$ in the second coordinate? Or why is $M_1$ in the second coordinate and not $M_2$? Specifically, How does the above semi-direct product give us all invertible affine maps? Why not just a direct product or some other product? Why is the equation exactly as specified? Why not switch $v_1$ and $v_2$ in the second coordinate or $M_1$ and $M_2$? I guess the second coordinate confuses me the most since it is what makes it different then a direct product, so please explain why the second coordinate takes that form. Please share anything else you think is useful in understanding semi-direct products, how they arise (constructing groups?) and how they work in this specific example. Please let me know where I am right and wrong :) Thanks for all the help","According to wikipedia the Affine group is the semi-direct product of a vector space $V$ and the general linear group $GL(V)$. Here is the definition of the semi-direct product in terms of matrices and vectors as given in the wikipedia link: \begin{equation}   (M_1, v_1)\cdot(M_2, v_2) = (M_1 M_2, v_1 + M_1v_2) \end{equation} By definition, I know the affine group is the set of all invertible affine maps and I know that the general linear group is the set of all invertible linear maps. I also know that an affine map is just a linear map without preserving the origin, and hence all linear maps are affine maps. It seems reasonable to me that affine maps can be constructed from linear maps, but I do not see everything from the above definition of semi-direct product. The only other idea I have is relating the difference between linear maps and affine maps, which is: translations. So I am thinking that the second coordinate vector gives us our translation, and the first coordinate gives us our linear transformation, however why not switch $v_1$ and $v_2$ in the second coordinate? Or why is $M_1$ in the second coordinate and not $M_2$? Specifically, How does the above semi-direct product give us all invertible affine maps? Why not just a direct product or some other product? Why is the equation exactly as specified? Why not switch $v_1$ and $v_2$ in the second coordinate or $M_1$ and $M_2$? I guess the second coordinate confuses me the most since it is what makes it different then a direct product, so please explain why the second coordinate takes that form. Please share anything else you think is useful in understanding semi-direct products, how they arise (constructing groups?) and how they work in this specific example. Please let me know where I am right and wrong :) Thanks for all the help",,"['linear-algebra', 'group-theory', 'affine-geometry', 'semidirect-product']"
69,Support of a vector,Support of a vector,,"What is the support of a signed vector? By signed vector, I mean a vector which is determined by considering the signs of the coefficients of the entries of another vector.","What is the support of a signed vector? By signed vector, I mean a vector which is determined by considering the signs of the coefficients of the entries of another vector.",,"['linear-algebra', 'terminology', 'matroids']"
70,Does an Inner Product Always Induce a Metric?,Does an Inner Product Always Induce a Metric?,,"I am working with linear algebra over finite fields, specifically $F_2$. In class my professor has explained that every inner product induces a norm, $\sqrt{\left < v,v \right>}$ which in turn induces a metric. All of this has seemed pretty obvious to me. Considering the vector space $V={F_2}^2$, with the standard sum of the product of each coordinate inner product, it seems to me that this is not true (probably because I am not understanding something right). The inner product is a function from $V \times V \rightarrow V$ which means that the under the induced metric, the magnitude of $\left(1,1 \right)^T$ is zero, though it is not the zero vector, contradicting the definition of a metric. Can somebody please clear up my misunderstanding?","I am working with linear algebra over finite fields, specifically $F_2$. In class my professor has explained that every inner product induces a norm, $\sqrt{\left < v,v \right>}$ which in turn induces a metric. All of this has seemed pretty obvious to me. Considering the vector space $V={F_2}^2$, with the standard sum of the product of each coordinate inner product, it seems to me that this is not true (probably because I am not understanding something right). The inner product is a function from $V \times V \rightarrow V$ which means that the under the induced metric, the magnitude of $\left(1,1 \right)^T$ is zero, though it is not the zero vector, contradicting the definition of a metric. Can somebody please clear up my misunderstanding?",,"['linear-algebra', 'metric-spaces', 'inner-products']"
71,Sum of elements of inverse matrix,Sum of elements of inverse matrix,,"Assume NxN matrix A of complex values. I want to calculated the sum of all elements of its inverse. Does anybody have any good idea how to do this? The problem is that calculating the inverse is computationally expensive and since I am looking only for the sum of its elements, I thought there might be something smarter to do. Note: the real part of A is diagonal while the imaginary is a 2x2 block matrix of symmetric submatrices. Thanks","Assume NxN matrix A of complex values. I want to calculated the sum of all elements of its inverse. Does anybody have any good idea how to do this? The problem is that calculating the inverse is computationally expensive and since I am looking only for the sum of its elements, I thought there might be something smarter to do. Note: the real part of A is diagonal while the imaginary is a 2x2 block matrix of symmetric submatrices. Thanks",,"['linear-algebra', 'matrices', 'inverse', 'block-matrices']"
72,symmetric matrices that aren't diagonalizable by a SPECIAL orthogonal matrix,symmetric matrices that aren't diagonalizable by a SPECIAL orthogonal matrix,,"Is there a $2\times 2$ symmetric matrix that can't be diagonalized by a special orthogonal matrix? The spectral theorem guarantees an orthogonal matrix, but both the algebra and the geometry suggest this fact degenerates, in the two-dimensional case, to the existence of a special orthogonal matrix that will do the trick. (Geometrically I can't see why we would need a reflection to bring a quadratic form into standard form; algebraically there appears to always be a solution for a rotation by $\theta$, at least if I did my algebra right.) In higher dimensions, what's the (right way to think about the) geometric obstruction to an orientation-preserving change-of-basis for the diagonalization? (I'm assuming that for $n>2$ there are indeed $n\times n$ symmetric matrices that require a reflection, even though I can't quickly write one down.) I'm thinking over the reals, if that wasn't clear.","Is there a $2\times 2$ symmetric matrix that can't be diagonalized by a special orthogonal matrix? The spectral theorem guarantees an orthogonal matrix, but both the algebra and the geometry suggest this fact degenerates, in the two-dimensional case, to the existence of a special orthogonal matrix that will do the trick. (Geometrically I can't see why we would need a reflection to bring a quadratic form into standard form; algebraically there appears to always be a solution for a rotation by $\theta$, at least if I did my algebra right.) In higher dimensions, what's the (right way to think about the) geometric obstruction to an orientation-preserving change-of-basis for the diagonalization? (I'm assuming that for $n>2$ there are indeed $n\times n$ symmetric matrices that require a reflection, even though I can't quickly write one down.) I'm thinking over the reals, if that wasn't clear.",,"['linear-algebra', 'geometry']"
73,Finding an orthogonal basis from a column space,Finding an orthogonal basis from a column space,,"I'm having issues with understanding one of the exercises I'm making. I have to find an orthogonal basis for the column space of $A$, where: $$A = \begin{bmatrix} 0 & 2 & 3 & -4 & 1\\ 0 & 0 & 2 & 3 & 4 \\ 2 & 2 & -5 & 2 & 4\\ 2 & 0 & -6 & 9 & 7 \end{bmatrix}.$$ The first question was to find a basis of the column space of $A$, clearly this is simply the first $3$ column vectors (by reducing it to row echelon form, and finding the leading $1$'s). However, then I had to find an orthogonal basis out of the column space of $A$, and here is where I get lost. I started off with finding the first vector: $$u_1 = \begin{bmatrix}0\\0\\2\\2\\\end{bmatrix}.$$ Then I thought I would find the second vector like this: $$u_2 = \begin{bmatrix}2\\0\\2\\0\\\end{bmatrix}-\left(\begin{bmatrix}2\\0\\2\\0\\\end{bmatrix}\cdot\begin{bmatrix}0\\0\\2\\2\\\end{bmatrix}\right)*\begin{bmatrix}0\\0\\2\\2\\\end{bmatrix} = \begin{bmatrix}2\\0\\2\\0\\\end{bmatrix}-4*\begin{bmatrix}0\\0\\2\\2\\\end{bmatrix} = \begin{bmatrix}2\\0\\-6\\-8\\\end{bmatrix}.$$ However, according to the result sheet we were given, instead of having a $4$, I should have $\frac{4}{8}$. I somehow can not figure out what I am missing, since the dot product of the two vectors clearly is $4$. Also, as a second question: if I had to find a orthonormal basis I would only have to take the orthogonal vectors found here, and multiply them by their $1$/length, correct?","I'm having issues with understanding one of the exercises I'm making. I have to find an orthogonal basis for the column space of $A$, where: $$A = \begin{bmatrix} 0 & 2 & 3 & -4 & 1\\ 0 & 0 & 2 & 3 & 4 \\ 2 & 2 & -5 & 2 & 4\\ 2 & 0 & -6 & 9 & 7 \end{bmatrix}.$$ The first question was to find a basis of the column space of $A$, clearly this is simply the first $3$ column vectors (by reducing it to row echelon form, and finding the leading $1$'s). However, then I had to find an orthogonal basis out of the column space of $A$, and here is where I get lost. I started off with finding the first vector: $$u_1 = \begin{bmatrix}0\\0\\2\\2\\\end{bmatrix}.$$ Then I thought I would find the second vector like this: $$u_2 = \begin{bmatrix}2\\0\\2\\0\\\end{bmatrix}-\left(\begin{bmatrix}2\\0\\2\\0\\\end{bmatrix}\cdot\begin{bmatrix}0\\0\\2\\2\\\end{bmatrix}\right)*\begin{bmatrix}0\\0\\2\\2\\\end{bmatrix} = \begin{bmatrix}2\\0\\2\\0\\\end{bmatrix}-4*\begin{bmatrix}0\\0\\2\\2\\\end{bmatrix} = \begin{bmatrix}2\\0\\-6\\-8\\\end{bmatrix}.$$ However, according to the result sheet we were given, instead of having a $4$, I should have $\frac{4}{8}$. I somehow can not figure out what I am missing, since the dot product of the two vectors clearly is $4$. Also, as a second question: if I had to find a orthonormal basis I would only have to take the orthogonal vectors found here, and multiply them by their $1$/length, correct?",,"['linear-algebra', 'matrices']"
74,How does composition affect eigendecomposition?,How does composition affect eigendecomposition?,,What relationship is there between the eigenvalues and vectors of linear operator $T$ and the composition $A T$ or $T A$? I'm also interested in analogous results for SVD.,What relationship is there between the eigenvalues and vectors of linear operator $T$ and the composition $A T$ or $T A$? I'm also interested in analogous results for SVD.,,"['linear-algebra', 'eigenvalues-eigenvectors']"
75,Does there exist a vector space with 30 elements?,Does there exist a vector space with 30 elements?,,Does there exist a vector space with 30 elements? How to determine whether there exist any vector space of particular cardinality?,Does there exist a vector space with 30 elements? How to determine whether there exist any vector space of particular cardinality?,,['linear-algebra']
76,An algebra of nilpotent linear transformations is triangularizable,An algebra of nilpotent linear transformations is triangularizable,,"How to prove ""An algebra of nilpotent linear transformations is triangularizable"" using linear algebra only?","How to prove ""An algebra of nilpotent linear transformations is triangularizable"" using linear algebra only?",,['linear-algebra']
77,Does the existence of a $\mathbb{Q}$-basis for $\mathbb{R}$ imply that choice holds up to $\frak c$?,Does the existence of a -basis for  imply that choice holds up to ?,\mathbb{Q} \mathbb{R} \frak c,"The axiom of choice is, for ZF, equivalent to the statement that every vector space has a basis. The implication of AoC by the existence of a basis for any vector space is shown in this paper . The proof of that theorem can be summed up as following: One adjoins all members of the sets $X_i$ from which one needs choice as variables to an arbitrary field $k$ creating the field of rational functions $k(X)$ over that field. Then a subfield $K$ of that field is defined through a variation of homogeneity that considers the index of the $X_i$ from which variables are occurring and finally the span of the variables as a vector space $V$ over $K$ is considered. A basis for $V$ over $K$ is chosen, and the construction is such that for all $x \in X_i$, the same set of basis elements occurs with non-zero coefficient in their $K$-representation. This fact is then used together with a similarity of the coefficients occuring to pick a unique $x_i \in X_i$ and an element was chosen as desired. Now, this comment made me curious if there can be a $\mathbb{Q}$-basis of $\mathbb{R}$ when choice fails at size continuum or below. That separates into three potentially hard (or, perhaps more likely, easily negatively answered) questions: a) Can we conclude anything from the existence of one specific basis? The construction of the critical field in the proof relies on the structure of the set family we choose from, and so it seems that these fields will not be isomorphic for different families of sets. But that doesn't say there cannot be a different proof where they are. Or it might also be possible to choose that one guaranteed basis for a family such we can imply choice for all other set families of the same cardinality - but then, does such a family exist? Any direct proof of the existence of well-orderings I know use multiple instances of choice, but perhaps one carefully chosen one can be enough? b) Assuming a) is positively answered, can we make any conclusions about lower cardinalities? Or, under ZF more strongly, can we actually say something about all subsets even if they are not comparable - can they still exist if we have choice for all families of sets of cardinality $\frak c$? c) Even if a) is answered positively, could a $\mathbb{Q}$-basis for $\mathbb{R}$ be used as this basis? This would then obviously require a different proof. To make the question precise, consider three versions of ""hold up to $\frak c$"": Do we have choice for families of sets where both the index set and all sets are subsets of $\mathbb{R}$? And, not necessarily equivalently, do we have choice for families of sets where both the index set and all sets have defined cardinalities $\le\frak c$ or perhaps $<\frak c$?","The axiom of choice is, for ZF, equivalent to the statement that every vector space has a basis. The implication of AoC by the existence of a basis for any vector space is shown in this paper . The proof of that theorem can be summed up as following: One adjoins all members of the sets $X_i$ from which one needs choice as variables to an arbitrary field $k$ creating the field of rational functions $k(X)$ over that field. Then a subfield $K$ of that field is defined through a variation of homogeneity that considers the index of the $X_i$ from which variables are occurring and finally the span of the variables as a vector space $V$ over $K$ is considered. A basis for $V$ over $K$ is chosen, and the construction is such that for all $x \in X_i$, the same set of basis elements occurs with non-zero coefficient in their $K$-representation. This fact is then used together with a similarity of the coefficients occuring to pick a unique $x_i \in X_i$ and an element was chosen as desired. Now, this comment made me curious if there can be a $\mathbb{Q}$-basis of $\mathbb{R}$ when choice fails at size continuum or below. That separates into three potentially hard (or, perhaps more likely, easily negatively answered) questions: a) Can we conclude anything from the existence of one specific basis? The construction of the critical field in the proof relies on the structure of the set family we choose from, and so it seems that these fields will not be isomorphic for different families of sets. But that doesn't say there cannot be a different proof where they are. Or it might also be possible to choose that one guaranteed basis for a family such we can imply choice for all other set families of the same cardinality - but then, does such a family exist? Any direct proof of the existence of well-orderings I know use multiple instances of choice, but perhaps one carefully chosen one can be enough? b) Assuming a) is positively answered, can we make any conclusions about lower cardinalities? Or, under ZF more strongly, can we actually say something about all subsets even if they are not comparable - can they still exist if we have choice for all families of sets of cardinality $\frak c$? c) Even if a) is answered positively, could a $\mathbb{Q}$-basis for $\mathbb{R}$ be used as this basis? This would then obviously require a different proof. To make the question precise, consider three versions of ""hold up to $\frak c$"": Do we have choice for families of sets where both the index set and all sets are subsets of $\mathbb{R}$? And, not necessarily equivalently, do we have choice for families of sets where both the index set and all sets have defined cardinalities $\le\frak c$ or perhaps $<\frak c$?",,"['linear-algebra', 'set-theory', 'model-theory', 'axiom-of-choice']"
78,"Determinant of matrix $(x_j^{n-i}- x_j^{2n-i})_{i,j=1}^{n}$",Determinant of matrix,"(x_j^{n-i}- x_j^{2n-i})_{i,j=1}^{n}","Good evening all, I am determined to determine this determinant: $$D = \det{\left[x_j^{n-i} - x_j^{2n-i}\right]_{i,j=1}^{n}}$$ Looking at the smaller cases, leads me to believe that $$D = \prod_{1 \leq i < j \leq n}\left(x_i-x_j\right)\prod_{i=1}^n \left(1-{x_i}^n\right)$$ although I am having trouble showing this. I know that, since the determinant is an alternating function in the variables $x_1,\dots x_n$ it follows that $$ \frac{D}{\displaystyle\prod_{1 \leq i < j \leq n}\left(x_i-x_j\right)} $$ is a symmetric polynomial of degree $n^2$ (the degree of D minus the degree of the Vandermonde part). How can I show that this symmetric polynomial is exactly $\prod_{i=1}^n \left(1-{x_i}^n\right)$ ? Your help is, as always, much appreciated.","Good evening all, I am determined to determine this determinant: $$D = \det{\left[x_j^{n-i} - x_j^{2n-i}\right]_{i,j=1}^{n}}$$ Looking at the smaller cases, leads me to believe that $$D = \prod_{1 \leq i < j \leq n}\left(x_i-x_j\right)\prod_{i=1}^n \left(1-{x_i}^n\right)$$ although I am having trouble showing this. I know that, since the determinant is an alternating function in the variables $x_1,\dots x_n$ it follows that $$ \frac{D}{\displaystyle\prod_{1 \leq i < j \leq n}\left(x_i-x_j\right)} $$ is a symmetric polynomial of degree $n^2$ (the degree of D minus the degree of the Vandermonde part). How can I show that this symmetric polynomial is exactly $\prod_{i=1}^n \left(1-{x_i}^n\right)$ ? Your help is, as always, much appreciated.",,"['linear-algebra', 'determinant']"
79,Checking axioms of Vector Spaces,Checking axioms of Vector Spaces,,"Currently I am studying a section from my book on vector spaces. I'm having issues in understanding how I am supposed to prove some of the questions in the Exercises section, such as: In each of the following, determine whether the set, together with the indicated operations, is a vector space. If it is not, identify at least one of the ten vector space axioms that fails. 13. $M_{4,6}$ with the standard operations. 14. $M_{1,1}$ with the standard operations. 15. The set of all third-degree polynomials with the standard operations. 16. The set of all fifth-degree polynomials with the standard operations. 17. The set of all first-degree polynomial functions $ax+b$, $a\neq 0$, whose graphs pass through the origin with the standard operations. 18. The set of all quadratic functions whose graphs pass through the origin with the standard operations. I don't know how exactly to identify which axiom fails. Here are the axioms: $\mathbf{u}+\mathbf{v}$ is in $V$. Closure under addition. $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$. Commutative property. $\mathbf{u}+(\mathbf{v}+\mathbf{w}) = (\mathbf{u}+\mathbf{v})+\mathbf{w}$. Associative property. $V$ has a zero vector $\mathbf{0}$ such that for every $\mathbf{u}\in V$, $\mathbf{u}+\mathbf{0}=\mathbf{u}$. Additive identity. For every $\mathbf{u}\in V$, there is a vector in $V$ denoted by $-\mathbf{u}$ such that $\mathbf{u}+(-\mathbf{u}) = \mathbf{0}$. Additive inverse. $c\mathbf{u}$ is in $V$. Closure under scalar multiplication. $c(\mathbf{u}+\mathbf{v}) = c\mathbf{u}+c\mathbf{v}$. Distributive property. $(c+d)\mathbf{u}=c\mathbf{u}+d\mathbf{u}$. Distributive property. $c(d\mathbf{u})= (cd)\mathbf{u}$. Associative property. $1(\mathbf{u}) =\mathbf{u}$. Scalar identity. Say we look at question 17: In the answer book it says that axiom 4 fails, but I don't see how that is possible. Say if you have 4x+1, by axiom 4 you are supposed to add 0 to the vector u, so you would get 4x+1+0=4x+1, which is true.... I'm really not sure how to get my head around these type of problems. Can someone give me a coherent explanation? Am I supposed to test by hand each axiom, or just in my head?","Currently I am studying a section from my book on vector spaces. I'm having issues in understanding how I am supposed to prove some of the questions in the Exercises section, such as: In each of the following, determine whether the set, together with the indicated operations, is a vector space. If it is not, identify at least one of the ten vector space axioms that fails. 13. $M_{4,6}$ with the standard operations. 14. $M_{1,1}$ with the standard operations. 15. The set of all third-degree polynomials with the standard operations. 16. The set of all fifth-degree polynomials with the standard operations. 17. The set of all first-degree polynomial functions $ax+b$, $a\neq 0$, whose graphs pass through the origin with the standard operations. 18. The set of all quadratic functions whose graphs pass through the origin with the standard operations. I don't know how exactly to identify which axiom fails. Here are the axioms: $\mathbf{u}+\mathbf{v}$ is in $V$. Closure under addition. $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$. Commutative property. $\mathbf{u}+(\mathbf{v}+\mathbf{w}) = (\mathbf{u}+\mathbf{v})+\mathbf{w}$. Associative property. $V$ has a zero vector $\mathbf{0}$ such that for every $\mathbf{u}\in V$, $\mathbf{u}+\mathbf{0}=\mathbf{u}$. Additive identity. For every $\mathbf{u}\in V$, there is a vector in $V$ denoted by $-\mathbf{u}$ such that $\mathbf{u}+(-\mathbf{u}) = \mathbf{0}$. Additive inverse. $c\mathbf{u}$ is in $V$. Closure under scalar multiplication. $c(\mathbf{u}+\mathbf{v}) = c\mathbf{u}+c\mathbf{v}$. Distributive property. $(c+d)\mathbf{u}=c\mathbf{u}+d\mathbf{u}$. Distributive property. $c(d\mathbf{u})= (cd)\mathbf{u}$. Associative property. $1(\mathbf{u}) =\mathbf{u}$. Scalar identity. Say we look at question 17: In the answer book it says that axiom 4 fails, but I don't see how that is possible. Say if you have 4x+1, by axiom 4 you are supposed to add 0 to the vector u, so you would get 4x+1+0=4x+1, which is true.... I'm really not sure how to get my head around these type of problems. Can someone give me a coherent explanation? Am I supposed to test by hand each axiom, or just in my head?",,['linear-algebra']
80,Two introductory linear algebra problems,Two introductory linear algebra problems,,"I remember when I was in Moscow one of my homework questions was: Is there a $2\times 4$ matrix whose $2\times 2$ minors are: a) $(2,3,4,5,6,7)$ b) $(3,4,5,6,7,8)$ c) $(5,6,7,8,9,10)$ This problem is supposed to be an easy application of some basic result in multilinear algebra, but I still do not know how to solve it. I also want to ask the question for $n\times m$ matrix and $n\times n$ minors in general, for which we will have $\displaystyle C^{m}_{n}=\frac{m!}{n!(m-n)!}$ numbers to choose. I only know this is somehow related to intersection varieties, but the problem is really tiny , so there should be some easy solution of it. The second problem, which appeared in my last year's final, was this: Let $A$ and $B$ be two matrices with $m$ rows and $n\ge m$ columns. Prove that $$\det(AB^{t})=\sum_{I}\det A_{I} \det B_{I},$$ where the sum is running over all increasing sequences $I=(i_{1},i_{2},\dots,i_{m})\subset (1,2,\dots,n)$ and $A_{I},B_{I}$ mean $m\times m$ -submatrices formed by $I$ -columns. This problem gives me the same feeling that it is supposed to be elementary and solvable by simple tools, but I could not unravel it via standard tools available. I feel there must be some better way to do it than expanding right side to equal the left side, etc. It has been one year and I still do not know how to solve it; so I decided to ask in here. This is from past exam so I think it is ok to ask online now.","I remember when I was in Moscow one of my homework questions was: Is there a matrix whose minors are: a) b) c) This problem is supposed to be an easy application of some basic result in multilinear algebra, but I still do not know how to solve it. I also want to ask the question for matrix and minors in general, for which we will have numbers to choose. I only know this is somehow related to intersection varieties, but the problem is really tiny , so there should be some easy solution of it. The second problem, which appeared in my last year's final, was this: Let and be two matrices with rows and columns. Prove that where the sum is running over all increasing sequences and mean -submatrices formed by -columns. This problem gives me the same feeling that it is supposed to be elementary and solvable by simple tools, but I could not unravel it via standard tools available. I feel there must be some better way to do it than expanding right side to equal the left side, etc. It has been one year and I still do not know how to solve it; so I decided to ask in here. This is from past exam so I think it is ok to ask online now.","2\times 4 2\times 2 (2,3,4,5,6,7) (3,4,5,6,7,8) (5,6,7,8,9,10) n\times m n\times n \displaystyle C^{m}_{n}=\frac{m!}{n!(m-n)!} A B m n\ge m \det(AB^{t})=\sum_{I}\det A_{I} \det B_{I}, I=(i_{1},i_{2},\dots,i_{m})\subset (1,2,\dots,n) A_{I},B_{I} m\times m I","['linear-algebra', 'matrices', 'multilinear-algebra']"
81,"Solve a linear equation system, while enforcing a unit vector solution","Solve a linear equation system, while enforcing a unit vector solution",,"I have an equation system of the form Aix + Biy + Ciz = Di, where (x,y,z) is a unit vector, and (Ai, Bi, Ci, Di) are sets of measurements from a noisy system (with typically 3-5 independant readings). My first intuition to solve this problem was to pose this as an overdetermined linear equation system AX = B where X = (x,y,z), and to solve for X. However, with that approach, I have no way to enforce that the solution for vector X is a unit vector. Is there an elegant (or standard) solution to that problem, or should I simply dive into non-linear equation solving solutions?","I have an equation system of the form Aix + Biy + Ciz = Di, where (x,y,z) is a unit vector, and (Ai, Bi, Ci, Di) are sets of measurements from a noisy system (with typically 3-5 independant readings). My first intuition to solve this problem was to pose this as an overdetermined linear equation system AX = B where X = (x,y,z), and to solve for X. However, with that approach, I have no way to enforce that the solution for vector X is a unit vector. Is there an elegant (or standard) solution to that problem, or should I simply dive into non-linear equation solving solutions?",,['linear-algebra']
82,Proving a matrix identity involving inverses,Proving a matrix identity involving inverses,,"Assuming that all matrix inverses involved below exist, show that $$(A-B)^{-1}=A^{-1} + A^{-1}(B^{-1} - A^{-1})^{-1}A^{-1}.$$ In particular $$\left(I+A\right)^{-1}=I-\left(A^{-1}+I\right)^{-1}$$ and $$\left\vert\left(I+A\right)^{-1}+\left(I+A^{-1}\right)^{-1}\right\vert=1.$$ Here's what I tried to do but I can't simplify that underlined part at all: So I thought I should maybe try finding some sort of infinite geometric series in whose summation the $(AB^{-1} - I)$ term appears but that doesn't seem to help either and I'm not certain if that's mathematically justified either. What should I do next? Also, I've just started studying linear algebra so I would really appreciate an answer using only elementary properties of matrices.","Assuming that all matrix inverses involved below exist, show that In particular and Here's what I tried to do but I can't simplify that underlined part at all: So I thought I should maybe try finding some sort of infinite geometric series in whose summation the term appears but that doesn't seem to help either and I'm not certain if that's mathematically justified either. What should I do next? Also, I've just started studying linear algebra so I would really appreciate an answer using only elementary properties of matrices.",(A-B)^{-1}=A^{-1} + A^{-1}(B^{-1} - A^{-1})^{-1}A^{-1}. \left(I+A\right)^{-1}=I-\left(A^{-1}+I\right)^{-1} \left\vert\left(I+A\right)^{-1}+\left(I+A^{-1}\right)^{-1}\right\vert=1. (AB^{-1} - I),"['linear-algebra', 'matrices', 'determinant', 'inverse']"
83,inner products on polynomials,inner products on polynomials,,"So, I was studying about orthogonal polynomials and saw general examples of inner products on $\mathbb{R}[x]$ , mostly of the forms $$\langle f,g\rangle=\int f(x)g(x)q(x)dx,$$ for some kind of density $q$ ; and an inner product of the form $$\langle f,g\rangle=\sum_{k=1}^n f(x_k)g(x_k),$$ for some fixed set of $x_1,...,x_n$ (mostly the eigenvalues of some matrix). As it happens, in both cases we have that the product is of the form $$\langle f,g\rangle=\int fgd\mu,$$ for some measure $\mu$ defined on the borelians (the first case the absolut continuous $d\mu =qdm$ and the second the dirac measure supported on $x_j$ ) My intuition is that for all cases there should be some measure, which we can define from the inner product. This intuition comes from Riez-Markov theorem, but there is no topological structure to be used here, so I cannot use it. so the question is: Is it true that every positive semi-definide inner product on $\mathbb{R}[x]$ is of the kind $$\langle f,g\rangle=\int fgd\mu,$$ for some measure $\mu$ that can be determined by the inner product? If not I would really apreciate a counterexample, as I am new to this area...","So, I was studying about orthogonal polynomials and saw general examples of inner products on , mostly of the forms for some kind of density ; and an inner product of the form for some fixed set of (mostly the eigenvalues of some matrix). As it happens, in both cases we have that the product is of the form for some measure defined on the borelians (the first case the absolut continuous and the second the dirac measure supported on ) My intuition is that for all cases there should be some measure, which we can define from the inner product. This intuition comes from Riez-Markov theorem, but there is no topological structure to be used here, so I cannot use it. so the question is: Is it true that every positive semi-definide inner product on is of the kind for some measure that can be determined by the inner product? If not I would really apreciate a counterexample, as I am new to this area...","\mathbb{R}[x] \langle f,g\rangle=\int f(x)g(x)q(x)dx, q \langle f,g\rangle=\sum_{k=1}^n f(x_k)g(x_k), x_1,...,x_n \langle f,g\rangle=\int fgd\mu, \mu d\mu =qdm x_j \mathbb{R}[x] \langle f,g\rangle=\int fgd\mu, \mu","['linear-algebra', 'functional-analysis', 'measure-theory', 'polynomials', 'orthogonal-polynomials']"
84,Linear Algebra Done Right: Notation 1.23,Linear Algebra Done Right: Notation 1.23,,"I recently started reading Linear Algebra Done Right by Axler, and I find it great up until Notation 1.23, where the first bullet point states: If $S$ is a set then $F^S$ denotes the set of functions from $S$ to $F$ I would really like to know what this means; in other words, does this mean that for all $f \in S$ , then $f \in F$ , but the only difference is that $F$ is a vector field?","I recently started reading Linear Algebra Done Right by Axler, and I find it great up until Notation 1.23, where the first bullet point states: If is a set then denotes the set of functions from to I would really like to know what this means; in other words, does this mean that for all , then , but the only difference is that is a vector field?",S F^S S F f \in S f \in F F,"['linear-algebra', 'vector-spaces', 'notation']"
85,Interesting/Useful tricks in linear algebra. [closed],Interesting/Useful tricks in linear algebra. [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question This may be an opinion based-based question, since everyone understands things differently, but what are some interesting/useful tricks in linear algebra? For example, if we know that the determinant of a matrix is equal to 0 0 , then we that the matrix is not invertible, the rows and columns of the matrix are linearly dependent, and so on. Are there any others, that would be useful to have on a cheatsheet for a linear algebra exam?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question This may be an opinion based-based question, since everyone understands things differently, but what are some interesting/useful tricks in linear algebra? For example, if we know that the determinant of a matrix is equal to 0 0 , then we that the matrix is not invertible, the rows and columns of the matrix are linearly dependent, and so on. Are there any others, that would be useful to have on a cheatsheet for a linear algebra exam?",,"['linear-algebra', 'matrices', 'vector-spaces', 'big-list']"
86,Proving Laplace expansion using exterior algebra,Proving Laplace expansion using exterior algebra,,"Let $A = (a_{ij})$ be an $n\times n$ matrix with entries in a ring $R$ , $M$ a free $R$ -module of rank $n$ with an ordered basis $(e_i)_{i \leq n}$ and $\phi\colon M\to M$ is an endomorphism which $A$ represents (such that $\phi(e_j) = \sum_{i = 1}^n a_{ij}e_i$ fop all $j$ ). Then $\bigwedge^n M$ has rank $1$ with a basis $\{e_1\wedge ... \wedge e_n\}$ and there is a unique scalar $r \in R$ such that $$\left(\bigwedge^n \phi\right)(e_1\wedge ... \wedge e_n) = \phi(e_1)\wedge ... \wedge \phi(e_n) = r(e_1\wedge ... \wedge e_n).$$ This scalar is precisely the determinant $\det(A)$ of $A$ . It can be easily shown that the value of $\det(A)$ doesn't depend on the choice of $M$ . I take this as the definition of a determinant. From this other definitions can be deduced as theorems, including the formula $$\det(A) = \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)a_{\sigma(1)1}...a_{\sigma(n)n}.$$ I understand that there is a shorter proof of the Laplace expansion of a determinant using the definition of a determinant in question. However, a book I consulted has a serious gap in the proof. I will state what I want to prove: Let $A = (a_{ij})$ be an $n\times n$ matrix with entries in a commutative ring $R$ . Denote by $A_{ij}$ the $(n-1)\times(n-1)$ matrix obtained from $A$ by deleting the $i$ -th row and the $j$ -th column of $A$ . Then, for all $i$ , $$\det(A) = \sum_{j = 1}^n (-1)^{i + j} a_{ij}\det(A_{ij}).$$","Let be an matrix with entries in a ring , a free -module of rank with an ordered basis and is an endomorphism which represents (such that fop all ). Then has rank with a basis and there is a unique scalar such that This scalar is precisely the determinant of . It can be easily shown that the value of doesn't depend on the choice of . I take this as the definition of a determinant. From this other definitions can be deduced as theorems, including the formula I understand that there is a shorter proof of the Laplace expansion of a determinant using the definition of a determinant in question. However, a book I consulted has a serious gap in the proof. I will state what I want to prove: Let be an matrix with entries in a commutative ring . Denote by the matrix obtained from by deleting the -th row and the -th column of . Then, for all ,",A = (a_{ij}) n\times n R M R n (e_i)_{i \leq n} \phi\colon M\to M A \phi(e_j) = \sum_{i = 1}^n a_{ij}e_i j \bigwedge^n M 1 \{e_1\wedge ... \wedge e_n\} r \in R \left(\bigwedge^n \phi\right)(e_1\wedge ... \wedge e_n) = \phi(e_1)\wedge ... \wedge \phi(e_n) = r(e_1\wedge ... \wedge e_n). \det(A) A \det(A) M \det(A) = \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)a_{\sigma(1)1}...a_{\sigma(n)n}. A = (a_{ij}) n\times n R A_{ij} (n-1)\times(n-1) A i j A i \det(A) = \sum_{j = 1}^n (-1)^{i + j} a_{ij}\det(A_{ij}).,"['linear-algebra', 'abstract-algebra', 'modules', 'determinant']"
87,"When solving a linear system, why SVD is preferred over QR to make the solution more stable?","When solving a linear system, why SVD is preferred over QR to make the solution more stable?",,"I have seen many posts stating that SVD is more stable as a preprocessing for solving least square or linear system problem than QR. Certainly QR is less expensive than SVD, so I guess it makes sense. But why?","I have seen many posts stating that SVD is more stable as a preprocessing for solving least square or linear system problem than QR. Certainly QR is less expensive than SVD, so I guess it makes sense. But why?",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra', 'least-squares', 'svd']"
88,If $B$ is nilpotent and $AB=BA$ then $\det(A+B) = \det(A)$ [duplicate],If  is nilpotent and  then  [duplicate],B AB=BA \det(A+B) = \det(A),"This question already has answers here : If $\,A^k=0$ and $AB=BA$, then $\,\det(A+B)=\det B$ (3 answers) Closed 5 years ago . The following stumps me: Let $\mathbb K$ be a field. Let $A, B \in \mathbb K^{n \times n}$ where $B$ is nilpotent and commutes with $A$ , i.e., $A B = B A$ . Show that $$ \det(A+B)=\det(A) $$ I have no idea how to approach this I thought perhaps raise both sides to a power but nothing works. Thanks for all help.","This question already has answers here : If $\,A^k=0$ and $AB=BA$, then $\,\det(A+B)=\det B$ (3 answers) Closed 5 years ago . The following stumps me: Let be a field. Let where is nilpotent and commutes with , i.e., . Show that I have no idea how to approach this I thought perhaps raise both sides to a power but nothing works. Thanks for all help.","\mathbb K A, B \in \mathbb K^{n \times n} B A A B = B A  \det(A+B)=\det(A) ","['linear-algebra', 'matrices', 'determinant', 'nilpotence']"
89,Find real number $a$ such that matrix $A$ is NOT diagonalisable,Find real number  such that matrix  is NOT diagonalisable,a A,Consider the matrix: $\begin{bmatrix}2 & a & -1\\0 & 2 & 1\\-1 & 8 & -1\end{bmatrix}$ Find all $a \in \mathbb R$ such that $A$ is not diagonalisable. I've never thought of such a problem before. What is the systematic way to make a matrix non-diagonalisable?,Consider the matrix: $\begin{bmatrix}2 & a & -1\\0 & 2 & 1\\-1 & 8 & -1\end{bmatrix}$ Find all $a \in \mathbb R$ such that $A$ is not diagonalisable. I've never thought of such a problem before. What is the systematic way to make a matrix non-diagonalisable?,,"['linear-algebra', 'matrices', 'diagonalization']"
90,Is a linear map (transformation) always a matrix multiplication,Is a linear map (transformation) always a matrix multiplication,,"I am studying linear maps. It is defined as a linear map $L$ which transforms a vector from dimension $n$ to dimension $k$ $L:\mathbb{R}^n \rightarrow \mathbb{R}^k$ This seems to me as a matrix multiplication (from $x$ to $y$): $y = Ax$ My question is, is this correct, and further, can a linear map always be written as a matrix multiplication?","I am studying linear maps. It is defined as a linear map $L$ which transforms a vector from dimension $n$ to dimension $k$ $L:\mathbb{R}^n \rightarrow \mathbb{R}^k$ This seems to me as a matrix multiplication (from $x$ to $y$): $y = Ax$ My question is, is this correct, and further, can a linear map always be written as a matrix multiplication?",,"['linear-algebra', 'linear-transformations']"
91,Let $A$ be a symmetric matrix of order $n$ and $A^2=0$ . Is it necessarily true that $A=0$,Let  be a symmetric matrix of order  and  . Is it necessarily true that,A n A^2=0 A=0,"Let $A$ be a symmetric matrix of order $n$ and $A^2=0$ . Is it necessarily true that $A=0$ . My approach : I tried to experiment with some $2\times 2$ matrices but never gotten any far . Now, Wikipedia says that there exists a diagonal matrix $D$ and orthogonal matrix $Q$ such that $D=Q^t A Q$ . So $D^2=Q^t A^2 Q=0$ . As $D$ is diagonal matrix with real entry we get $D=0$ . So $A=QDQ^t =0 $  I think my proof is correct . I just want know if there is any way to prove this without citing any big theorems or in more elementary way . The problem is quoted from a part of the web-text that only used elementary definitions like what a symmetric matrix is . So i'm curious if there is an elementary solution to the problem .","Let $A$ be a symmetric matrix of order $n$ and $A^2=0$ . Is it necessarily true that $A=0$ . My approach : I tried to experiment with some $2\times 2$ matrices but never gotten any far . Now, Wikipedia says that there exists a diagonal matrix $D$ and orthogonal matrix $Q$ such that $D=Q^t A Q$ . So $D^2=Q^t A^2 Q=0$ . As $D$ is diagonal matrix with real entry we get $D=0$ . So $A=QDQ^t =0 $  I think my proof is correct . I just want know if there is any way to prove this without citing any big theorems or in more elementary way . The problem is quoted from a part of the web-text that only used elementary definitions like what a symmetric matrix is . So i'm curious if there is an elementary solution to the problem .",,"['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations', 'matrix-equations']"
92,Matlab code to compute the smallest nonzero singular value of the matrix without using SVD,Matlab code to compute the smallest nonzero singular value of the matrix without using SVD,,"I want to compute the smallest nonzero singular value of the matrix A, which is defined as follows. Let $B =  rand(500, 250)$, $A = B*B^t$, where $t$ denotes the transpose of the matrix. I found the following matlab code to compute singular values of the matrix A which is based on the Singular value decomposition of the matrix. svds = svd(A);                              s = min(svds);  % smallest singular value I want to know is there any other efficient way to smallest singular value? Thank you in advance","I want to compute the smallest nonzero singular value of the matrix A, which is defined as follows. Let $B =  rand(500, 250)$, $A = B*B^t$, where $t$ denotes the transpose of the matrix. I found the following matlab code to compute singular values of the matrix A which is based on the Singular value decomposition of the matrix. svds = svd(A);                              s = min(svds);  % smallest singular value I want to know is there any other efficient way to smallest singular value? Thank you in advance",,"['linear-algebra', 'matrices', 'matlab', 'numerical-linear-algebra', 'singular-values']"
93,Why isn't every eigenvalue of a stochastic matrix equal to 1?,Why isn't every eigenvalue of a stochastic matrix equal to 1?,,"In this question, we see a proof that the largest eigenvalue of a stochastic matrix is equal to 1: Proof that the largest eigenvalue of a stochastic matrix is 1 However, I think I've found a proof that every eigenvalue of a stochastic matrix is equal to 1.  Can you tell me where my proof is wrong? Proof: Suppose ${\bf r}$ is an eigenvector of the column stochastic matrix $M$ (i.e. $M{\bf r} = \lambda {\bf r}$ for some $\lambda$), and assume without loss of generality that the entries of ${\bf r}$ sum to $1$.  Then $$M{\bf r} = \begin{bmatrix} M_{11}\\ M_{21}\\ \vdots\\ M_{n1} \end{bmatrix} r_1 + \begin{bmatrix} M_{12}\\ M_{22}\\ \vdots\\ M_{n2} \end{bmatrix} r_2 + \dots + \begin{bmatrix} M_{1n}\\ M_{2n}\\ \vdots\\ M_{nn} \end{bmatrix} r_n$$ Since $M$ is column stochastic, each column must sum to $1$, so the sum of the entries in $M {\bf r}$ is just $1 \cdot r_1 + 1 \cdot r_2 + \dots + 1 \cdot r_n = 1$.  Therefore, $\lambda$ must be $1$, and $M$ can only have one eigenvalue. Thanks!","In this question, we see a proof that the largest eigenvalue of a stochastic matrix is equal to 1: Proof that the largest eigenvalue of a stochastic matrix is 1 However, I think I've found a proof that every eigenvalue of a stochastic matrix is equal to 1.  Can you tell me where my proof is wrong? Proof: Suppose ${\bf r}$ is an eigenvector of the column stochastic matrix $M$ (i.e. $M{\bf r} = \lambda {\bf r}$ for some $\lambda$), and assume without loss of generality that the entries of ${\bf r}$ sum to $1$.  Then $$M{\bf r} = \begin{bmatrix} M_{11}\\ M_{21}\\ \vdots\\ M_{n1} \end{bmatrix} r_1 + \begin{bmatrix} M_{12}\\ M_{22}\\ \vdots\\ M_{n2} \end{bmatrix} r_2 + \dots + \begin{bmatrix} M_{1n}\\ M_{2n}\\ \vdots\\ M_{nn} \end{bmatrix} r_n$$ Since $M$ is column stochastic, each column must sum to $1$, so the sum of the entries in $M {\bf r}$ is just $1 \cdot r_1 + 1 \cdot r_2 + \dots + 1 \cdot r_n = 1$.  Therefore, $\lambda$ must be $1$, and $M$ can only have one eigenvalue. Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
94,What is the difference between the projection onto the column space and projection onto row space?,What is the difference between the projection onto the column space and projection onto row space?,,"If I see a question that asks ""find the projection a vector $b$ onto a matrix $A$"" I would either solve by using $A^TA\hat x =A^Tb$ and then the projection would equal $A\hat x$, and if the matrix $A$ was orthogonal then I would use $proj_bA = \frac{b \cdot q_1}{q_1 \cdot q_1}q_1 + ... + \frac{b \cdot q_k}{q_k \cdot q_k}q_k$ where $q_k$ represents the $k^{th}$ vector in matrix $A$. My question is what if a question says find the projection of some vector $b$ onto the column/row space of matrix $A$? What does this mean and what would I need to do differently to calculate it?","If I see a question that asks ""find the projection a vector $b$ onto a matrix $A$"" I would either solve by using $A^TA\hat x =A^Tb$ and then the projection would equal $A\hat x$, and if the matrix $A$ was orthogonal then I would use $proj_bA = \frac{b \cdot q_1}{q_1 \cdot q_1}q_1 + ... + \frac{b \cdot q_k}{q_k \cdot q_k}q_k$ where $q_k$ represents the $k^{th}$ vector in matrix $A$. My question is what if a question says find the projection of some vector $b$ onto the column/row space of matrix $A$? What does this mean and what would I need to do differently to calculate it?",,['linear-algebra']
95,Adjoint of a bounded linear operator is bounded,Adjoint of a bounded linear operator is bounded,,"Suppose $X$ and $Y$ are normed spaces over $\mathbb{R}$ and suppose $T: X \rightarrow Y$ is a bounded linear map. I want to prove that the adjoint map $T^\star : Y^\star \rightarrow X^\star$ is bounded as a linear map also, but I get super confused when trying to deal with the norms on the dual spaces since there are four norms, to deal with and I am unsure whether it is the operator norm of $T^\star$ or the dual norms i need to take? I suppose the question is that since $\Vert T(x) \Vert_X \leq M \Vert x \Vert_Y$ it should be possible to find a $K$ s.t. $\Vert T^\star(l) \Vert_X^{\star} \leq K \Vert l \Vert_Y^{\star}$, but expanding these seems a little unfeasible.","Suppose $X$ and $Y$ are normed spaces over $\mathbb{R}$ and suppose $T: X \rightarrow Y$ is a bounded linear map. I want to prove that the adjoint map $T^\star : Y^\star \rightarrow X^\star$ is bounded as a linear map also, but I get super confused when trying to deal with the norms on the dual spaces since there are four norms, to deal with and I am unsure whether it is the operator norm of $T^\star$ or the dual norms i need to take? I suppose the question is that since $\Vert T(x) \Vert_X \leq M \Vert x \Vert_Y$ it should be possible to find a $K$ s.t. $\Vert T^\star(l) \Vert_X^{\star} \leq K \Vert l \Vert_Y^{\star}$, but expanding these seems a little unfeasible.",,"['linear-algebra', 'functional-analysis']"
96,linearly independent generalized eigenvectors,linearly independent generalized eigenvectors,,"I'm self-studying Axler's Linear Algebra Done Right and I am not understanding one step of the proof of 8.13 (Linearly independent generalized eigenvectors). It is the same step as the one that ""yields"" 1.4.65 in the proof of Lemma 1.4.63 in this book , which also leaves it unexplained. We are multiplying both sides of the equation $\sum a_i v_i = 0$, where each $v_i$ is a generalized eigenvector, by $(T-\lambda_j)^k\prod_{i \ne j} (T - \lambda_i)^n$, where each $\lambda_i$ is the eigenvalue corresponding to $v_i$, and somehow getting: $a_j (T-\lambda_j)^k\prod_{i \ne j} (T - \lambda_i)^n v_j = 0$ (i.e., all terms of $\sum a_i v_i$ disappear except for the one contained $v_j$) I understand that each $v_i$ is an element of $null(T-\lambda_i)^n$ and so its term would disappear if the operator were applied directly to the $v_i$, but only the final $(T-\lambda_i)^n$ applies directly to each $v_i$. Are these transformations commutative for some reason? I'm likely missing something trivial here and would appreciate your insights. Thanks!","I'm self-studying Axler's Linear Algebra Done Right and I am not understanding one step of the proof of 8.13 (Linearly independent generalized eigenvectors). It is the same step as the one that ""yields"" 1.4.65 in the proof of Lemma 1.4.63 in this book , which also leaves it unexplained. We are multiplying both sides of the equation $\sum a_i v_i = 0$, where each $v_i$ is a generalized eigenvector, by $(T-\lambda_j)^k\prod_{i \ne j} (T - \lambda_i)^n$, where each $\lambda_i$ is the eigenvalue corresponding to $v_i$, and somehow getting: $a_j (T-\lambda_j)^k\prod_{i \ne j} (T - \lambda_i)^n v_j = 0$ (i.e., all terms of $\sum a_i v_i$ disappear except for the one contained $v_j$) I understand that each $v_i$ is an element of $null(T-\lambda_i)^n$ and so its term would disappear if the operator were applied directly to the $v_i$, but only the final $(T-\lambda_i)^n$ applies directly to each $v_i$. Are these transformations commutative for some reason? I'm likely missing something trivial here and would appreciate your insights. Thanks!",,"['linear-algebra', 'eigenvalues-eigenvectors']"
97,Is the absolute value of a P.D.S. matrix P.D.S.,Is the absolute value of a P.D.S. matrix P.D.S.,,"Suppose that $A$ is a positive definite symmetric matrix (P.D.S.). Now consider the matrix $|A|$, the matrix arrived at by taking the absolute value of all the entries of $A$. Is $|A|$ also P.D.S.? I have been trying to construct a counter example, but I can't seem to get one. Can someone proved a proof or counterexample?","Suppose that $A$ is a positive definite symmetric matrix (P.D.S.). Now consider the matrix $|A|$, the matrix arrived at by taking the absolute value of all the entries of $A$. Is $|A|$ also P.D.S.? I have been trying to construct a counter example, but I can't seem to get one. Can someone proved a proof or counterexample?",,"['linear-algebra', 'matrices']"
98,Finding orthogonal matrix that maps one vector to another,Finding orthogonal matrix that maps one vector to another,,"Let $w, v \in \mathbb{R}^k$ be two known vectors such that $||w|| = ||v||$ ($|| . ||$ is the usual Euclidean norm). My questions are related with the problem of finding $Q$ orthogonal such that $v = Q w$. I know that if we take $u = (w-v)/||w-v||$, then $Q = I-2uu'$ (Householder reflection) will be a solution. My questions are: 1) Is there any explicit way to characterize the solutions $Q$? For instance, what would be another concrete ""formula"" for a solution $Q$, different than the Householder reflection above? 2) Say that we have a matrix with eigenvalues $\Lambda$. If $Q_1$ is another solution (i.e. $Q_1$ is orthogonal and $v = Qw = Q_1 w$, $Q_1 \neq Q$), is it true that $Q D Q' = Q_1 D Q_1'$? 3) In essence, does the restriction $Q w = v$ for $Q$ orthogonal ""tie down"" the structure/properties of $Q$ in any interesting way? Any reference or pointers are much appreciated... Thanks in advance!","Let $w, v \in \mathbb{R}^k$ be two known vectors such that $||w|| = ||v||$ ($|| . ||$ is the usual Euclidean norm). My questions are related with the problem of finding $Q$ orthogonal such that $v = Q w$. I know that if we take $u = (w-v)/||w-v||$, then $Q = I-2uu'$ (Householder reflection) will be a solution. My questions are: 1) Is there any explicit way to characterize the solutions $Q$? For instance, what would be another concrete ""formula"" for a solution $Q$, different than the Householder reflection above? 2) Say that we have a matrix with eigenvalues $\Lambda$. If $Q_1$ is another solution (i.e. $Q_1$ is orthogonal and $v = Qw = Q_1 w$, $Q_1 \neq Q$), is it true that $Q D Q' = Q_1 D Q_1'$? 3) In essence, does the restriction $Q w = v$ for $Q$ orthogonal ""tie down"" the structure/properties of $Q$ in any interesting way? Any reference or pointers are much appreciated... Thanks in advance!",,"['linear-algebra', 'matrices']"
99,"Question about ""baffling"" umbral calculus result","Question about ""baffling"" umbral calculus result",,"I am reading a paper here and I've come to a particular passage that is confusing me. It comes on page 2 of the attached paper and it deals with the binomial theorem...  The passage lays the groundwork of how umbrals were linear functionals before the spread of linear algebra but the statement that confuses me is at the end of the paragraph and states: Before knowledge of linear algebra became widespread, the action of a linear functional (written using physicist notation as $\langle L\mid x^n\rangle=a_n$) would be conceived of as raising the index $n$ to a power, and then ""treating"" the sequence $a_n$ as a sequence of powers $a^n$, while reserving the right to lower the index at the proper time.  No precise rules for lowering indices were stated, nor could they be, as long as the underlying conceptual framework was missing.  A baffingly difficulty in the calculus of umbrae was the important $rule$   $$(a+a)^n=\sum_{k=0}^{n}\binom{n}{k}a^ka^{n-k}$$   which seemed to imply $a+a\neq 2a$ It is the last statement I am confused at.  Why is this implication here?  How is it a result?","I am reading a paper here and I've come to a particular passage that is confusing me. It comes on page 2 of the attached paper and it deals with the binomial theorem...  The passage lays the groundwork of how umbrals were linear functionals before the spread of linear algebra but the statement that confuses me is at the end of the paragraph and states: Before knowledge of linear algebra became widespread, the action of a linear functional (written using physicist notation as $\langle L\mid x^n\rangle=a_n$) would be conceived of as raising the index $n$ to a power, and then ""treating"" the sequence $a_n$ as a sequence of powers $a^n$, while reserving the right to lower the index at the proper time.  No precise rules for lowering indices were stated, nor could they be, as long as the underlying conceptual framework was missing.  A baffingly difficulty in the calculus of umbrae was the important $rule$   $$(a+a)^n=\sum_{k=0}^{n}\binom{n}{k}a^ka^{n-k}$$   which seemed to imply $a+a\neq 2a$ It is the last statement I am confused at.  Why is this implication here?  How is it a result?",,"['linear-algebra', 'number-theory', 'binomial-theorem', 'umbral-calculus']"
