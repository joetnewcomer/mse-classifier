,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Let $A$ be a $n× n$ real matrix with $A^2 = A^T$. Show that every real eigenvalue of $A$ is either $0$ or $1$.,Let  be a  real matrix with . Show that every real eigenvalue of  is either  or .,A n× n A^2 = A^T A 0 1,Let $A$ be a $n×n$ real matrix with $A^2 = A^T$. Show that every real eigenvalue of $A$ is either $0$ or $1$. My thoughts: $A^2 = A^T$ $\implies$ $A.A=A^T$ $\implies$$(A.A)^T=A$ $\implies$ $A^TA^T=A$ $\implies$$A^2A^2=A$ $\implies$ $A^4-A=0$ . so the real root of the equation $x^4-x=0$ are $0$ & $1$. Am I right?,Let $A$ be a $n×n$ real matrix with $A^2 = A^T$. Show that every real eigenvalue of $A$ is either $0$ or $1$. My thoughts: $A^2 = A^T$ $\implies$ $A.A=A^T$ $\implies$$(A.A)^T=A$ $\implies$ $A^TA^T=A$ $\implies$$A^2A^2=A$ $\implies$ $A^4-A=0$ . so the real root of the equation $x^4-x=0$ are $0$ & $1$. Am I right?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
1,"are there any ""deep"" reasons for representing linear systems as $Ax=b$ instead of $xA=b$?","are there any ""deep"" reasons for representing linear systems as  instead of ?",Ax=b xA=b,"Nowadays we represent the system of $m$ linear equations $$\sum_{i=1}^na_{1i}x_i=y_1$$ $$\sum_{i=1}^na_{2i}x_i=y_2$$ $$\vdots$$ $$\sum_{i=1}^na_{mi}x_i=y_m$$ as $\mathbf{Ax}=\mathbf{y}$, where $(A)_{ij}=a_{ij}$ is an $m\times n$ matrix, $\mathbf{x}$ is an $n\times 1$ column vector, and $\mathbf{y}$ is an $m\times 1$ column vector. Call this the ""column picture."" But we could just as well have represented it by the transposed equation $$\mathbf{y}^T=\mathbf{x}^T\mathbf{A}^T$$ where we now deal with row vectors rather than column vectors. Call this the ""row picture."" I have two questions: (1) Can anyone point to a specific historical reference in which a linear system of equations was represented using the row picture? (2) Are there any deep reasons for preferring the column picture to the row picture? Or is it fair for us to describe the column picture as totally arbitrary?","Nowadays we represent the system of $m$ linear equations $$\sum_{i=1}^na_{1i}x_i=y_1$$ $$\sum_{i=1}^na_{2i}x_i=y_2$$ $$\vdots$$ $$\sum_{i=1}^na_{mi}x_i=y_m$$ as $\mathbf{Ax}=\mathbf{y}$, where $(A)_{ij}=a_{ij}$ is an $m\times n$ matrix, $\mathbf{x}$ is an $n\times 1$ column vector, and $\mathbf{y}$ is an $m\times 1$ column vector. Call this the ""column picture."" But we could just as well have represented it by the transposed equation $$\mathbf{y}^T=\mathbf{x}^T\mathbf{A}^T$$ where we now deal with row vectors rather than column vectors. Call this the ""row picture."" I have two questions: (1) Can anyone point to a specific historical reference in which a linear system of equations was represented using the row picture? (2) Are there any deep reasons for preferring the column picture to the row picture? Or is it fair for us to describe the column picture as totally arbitrary?",,['linear-algebra']
2,Maximal dimension of subspace of matrices whose products have zero trace,Maximal dimension of subspace of matrices whose products have zero trace,,"In the space of all real matrices with dimension $n$, find the maximal possible dimension of a subspace $V$ such that $\forall X,Y\in V,\, \operatorname{tr}(XY)=0$.","In the space of all real matrices with dimension $n$, find the maximal possible dimension of a subspace $V$ such that $\forall X,Y\in V,\, \operatorname{tr}(XY)=0$.",,"['linear-algebra', 'matrices']"
3,Positive semidefinite inner product,Positive semidefinite inner product,,"If S,Z are positive semidefinite. We now know that $y^TSy=\sum_{i}\sum_{j}S_{ij}y_{i}y_{j}\geq 0 $, same goes for Z. We also know that we can write S as $S=\sum_{i}\lambda_{i}x_{i}x_{i}^{T}$, with $\lambda_{i}$ the eigenvalues and $x_{i}$ eigenvectors of the matrix S. In other words $S_{ij}=\sum_{k}\lambda_{k}x_{ik}x_{jk}$ by construction. I want to show that $$\sum_{i}\sum_{j}S_{ij}Z_{ij}\geq0$$ Could anyone help me show this?? I would also like to know why the equality of this statement only holds if and only if $S \cdot Z=0$","If S,Z are positive semidefinite. We now know that $y^TSy=\sum_{i}\sum_{j}S_{ij}y_{i}y_{j}\geq 0 $, same goes for Z. We also know that we can write S as $S=\sum_{i}\lambda_{i}x_{i}x_{i}^{T}$, with $\lambda_{i}$ the eigenvalues and $x_{i}$ eigenvectors of the matrix S. In other words $S_{ij}=\sum_{k}\lambda_{k}x_{ik}x_{jk}$ by construction. I want to show that $$\sum_{i}\sum_{j}S_{ij}Z_{ij}\geq0$$ Could anyone help me show this?? I would also like to know why the equality of this statement only holds if and only if $S \cdot Z=0$",,"['linear-algebra', 'inner-products']"
4,Understanding matrices whose powers have positive entries,Understanding matrices whose powers have positive entries,,"A regular matrix $A$ is described as a square matrix that for all positive integer $n$, is such that $A^n$ has positive entries. How then would I prove something is regular? I mean I can prove something is irregular if $A^2$ has some 0 or negative entries; but I cant prove regularity since I cant solve $A^n$ for all integers $n$. My thoughts are that if a matrix $A$ is diagonalisable as $A=PD^{-1}P$ then it is 'regular,' since then all $A^k$ exist; but does this also imply all entries of $A^k$ are positive? Any hints?","A regular matrix $A$ is described as a square matrix that for all positive integer $n$, is such that $A^n$ has positive entries. How then would I prove something is regular? I mean I can prove something is irregular if $A^2$ has some 0 or negative entries; but I cant prove regularity since I cant solve $A^n$ for all integers $n$. My thoughts are that if a matrix $A$ is diagonalisable as $A=PD^{-1}P$ then it is 'regular,' since then all $A^k$ exist; but does this also imply all entries of $A^k$ are positive? Any hints?",,"['linear-algebra', 'matrices', 'positive-matrices']"
5,"Matrix inverse property, show that $(I + uv^T)^{-1} = I - \frac{uv^T}{1+v^Tu}$","Matrix inverse property, show that",(I + uv^T)^{-1} = I - \frac{uv^T}{1+v^Tu},"Let $u, v \in \mathbb{R}^N, u^Tv \neq -1$. Thereby $I +uv^T \in \mathbb{R}^{N \times N}$ is invertible. Show that: $$(I + uv^T)^{-1} = I - \frac{uv^T}{1+v^Tu}$$ I'm lost, why did the denominator get $uv^T$ as $v^T u$? Where did this $1$ come from? Any hints are very appreciated!","Let $u, v \in \mathbb{R}^N, u^Tv \neq -1$. Thereby $I +uv^T \in \mathbb{R}^{N \times N}$ is invertible. Show that: $$(I + uv^T)^{-1} = I - \frac{uv^T}{1+v^Tu}$$ I'm lost, why did the denominator get $uv^T$ as $v^T u$? Where did this $1$ come from? Any hints are very appreciated!",,['linear-algebra']
6,Can we construct a basis for Hermitian matrices made of positive semidefinite ones?,Can we construct a basis for Hermitian matrices made of positive semidefinite ones?,,"let us consider $n\times n$ hermitian matrices.  They form a real space. Now we know that any such matrix $A$ can be written as  $A=A_+-A_-$, where $A_\pm$ are positive semidefinite matrices. Thus we can say that the (real) linear combination of positive semi-definite matrices spans the space of hermitian matrices. My question is, can we construct any basis for the space of hermitian matrices such that each basis element is a positive semidefinite matrix. please help or refer some literature  for it. ADDED: seeing the comment of Joriki I have decided to add a few more lines regarding my earlier (failed) approach, in the hope that someone can help me (in completing the line of argument, if possible; or by finding a fault in my argument). I can diagonalise and separate the positive and negative part. now let $A=UDU^*$, where $U$ is an unitary operator and $D$ is the diagonal matrix. This again can be written as $A=UD_1U^*-UD_2U^*$ where $D_i$ are diagonal matrices with all entries $\geq0$. hence, if we take a such a positive diagonal matrices and only consider unitary group action on it, we are going to find all the hermitian operators. in particular, i tried to take diagonal matrix $D_j$ ($j$-th entry $1$, others $0$) and applied unitary group. this method seemed to fail here, as i could not get any meaningful basis out of these actions.","let us consider $n\times n$ hermitian matrices.  They form a real space. Now we know that any such matrix $A$ can be written as  $A=A_+-A_-$, where $A_\pm$ are positive semidefinite matrices. Thus we can say that the (real) linear combination of positive semi-definite matrices spans the space of hermitian matrices. My question is, can we construct any basis for the space of hermitian matrices such that each basis element is a positive semidefinite matrix. please help or refer some literature  for it. ADDED: seeing the comment of Joriki I have decided to add a few more lines regarding my earlier (failed) approach, in the hope that someone can help me (in completing the line of argument, if possible; or by finding a fault in my argument). I can diagonalise and separate the positive and negative part. now let $A=UDU^*$, where $U$ is an unitary operator and $D$ is the diagonal matrix. This again can be written as $A=UD_1U^*-UD_2U^*$ where $D_i$ are diagonal matrices with all entries $\geq0$. hence, if we take a such a positive diagonal matrices and only consider unitary group action on it, we are going to find all the hermitian operators. in particular, i tried to take diagonal matrix $D_j$ ($j$-th entry $1$, others $0$) and applied unitary group. this method seemed to fail here, as i could not get any meaningful basis out of these actions.",,"['linear-algebra', 'matrices', 'functional-analysis']"
7,understanding the symmetric/Hermitian matrix/operator,understanding the symmetric/Hermitian matrix/operator,,"The orthogonal/unitary matrix/operator can be seen as an isometric operation or transformation, which preserves the inner product (""length"" and ""angle""). But I have difficulties to intuitively understand the symmetric/Hermitian matrix/operator. I mean what kind of operation or transformation can be interpreted as the symmetric/Hermitian matrix/operator? Thanks!","The orthogonal/unitary matrix/operator can be seen as an isometric operation or transformation, which preserves the inner product (""length"" and ""angle""). But I have difficulties to intuitively understand the symmetric/Hermitian matrix/operator. I mean what kind of operation or transformation can be interpreted as the symmetric/Hermitian matrix/operator? Thanks!",,"['linear-algebra', 'matrices', 'functional-analysis', 'operator-theory']"
8,G graph of diameter d implies an adjacency matrix with at least d+1 distinct eigenvalues!,G graph of diameter d implies an adjacency matrix with at least d+1 distinct eigenvalues!,,"In reading the well known book on Algebraic graph theory I came across a theorem that could be stated in the following way: If $G$ is a graph of diameter $d$ then the adjacency matrix of $G$ has at least $d+1$ distinct eigenvalues. The proof shows that if $A$ is the adjacency matrix of a graph $G$ that has diameter $d$, then $I,A,A^2, \ldots, A^d$ are linearly independent. And then the main theorem somehow follows from here. I assume what is used is the fact that if $A$ is a symmetric matrix and $I,A,\ldots,A^d$ are linearly independent, then $A$ has at least $d+1$ distinct eigenvalues? Am I right? If yes, how can one quickly prove this fact?","In reading the well known book on Algebraic graph theory I came across a theorem that could be stated in the following way: If $G$ is a graph of diameter $d$ then the adjacency matrix of $G$ has at least $d+1$ distinct eigenvalues. The proof shows that if $A$ is the adjacency matrix of a graph $G$ that has diameter $d$, then $I,A,A^2, \ldots, A^d$ are linearly independent. And then the main theorem somehow follows from here. I assume what is used is the fact that if $A$ is a symmetric matrix and $I,A,\ldots,A^d$ are linearly independent, then $A$ has at least $d+1$ distinct eigenvalues? Am I right? If yes, how can one quickly prove this fact?",,"['linear-algebra', 'group-theory', 'matrices', 'eigenvalues-eigenvectors', 'algebraic-graph-theory']"
9,Computing determinant of a matrix with non-zero values on three diagonals,Computing determinant of a matrix with non-zero values on three diagonals,,"let $A$ be an $n\times n$ matrix with entries $a_{ij}$ such that $a_{ij}=2$ if $i=j$. $a_{ij}=1$ if $|i-j|=2$ and $a_{ij}=0$ otherwise. compute the determinant of $A$. using the famous formula $\det A=\sum_{i=1}^{n}(-1)^{i+j}a_{ij}\det A^{(ij)}$ where $A{(ij)}$ is the submatrix obtaining from $A$ by omiting it's $i$th row and $j$th colomn, I reached to the formula $\det A=\frac{1}{4}n^2+n+\frac{7}{8}+\frac{1}{8}(-1)^n$. is it correct?","let $A$ be an $n\times n$ matrix with entries $a_{ij}$ such that $a_{ij}=2$ if $i=j$. $a_{ij}=1$ if $|i-j|=2$ and $a_{ij}=0$ otherwise. compute the determinant of $A$. using the famous formula $\det A=\sum_{i=1}^{n}(-1)^{i+j}a_{ij}\det A^{(ij)}$ where $A{(ij)}$ is the submatrix obtaining from $A$ by omiting it's $i$th row and $j$th colomn, I reached to the formula $\det A=\frac{1}{4}n^2+n+\frac{7}{8}+\frac{1}{8}(-1)^n$. is it correct?",,"['linear-algebra', 'matrices', 'determinant']"
10,Invariants of a matrix,Invariants of a matrix,,"I'm teaching a course in physics, and I need a simple and intuitive proof that a matrix ($3\times3$, but it doesn't matter) has exactly 1 invariant which is linear in its entries, 2 that are quadratic, etc. When I say ""invariance"" I mean under orthonormal transformation of the axes $A\to Q A Q^{-1}$ for orthonormal $Q$. For example, that every quadratic invariant scalar is a combination of $\operatorname{tr}(A)^2$ and $\operatorname{tr}(A^2)$. The proof for the linear case is trivial (and intuitive) but I can't find a generalization for the quadratic case.","I'm teaching a course in physics, and I need a simple and intuitive proof that a matrix ($3\times3$, but it doesn't matter) has exactly 1 invariant which is linear in its entries, 2 that are quadratic, etc. When I say ""invariance"" I mean under orthonormal transformation of the axes $A\to Q A Q^{-1}$ for orthonormal $Q$. For example, that every quadratic invariant scalar is a combination of $\operatorname{tr}(A)^2$ and $\operatorname{tr}(A^2)$. The proof for the linear case is trivial (and intuitive) but I can't find a generalization for the quadratic case.",,"['linear-algebra', 'invariance']"
11,Can any set of $n$ relatively prime elements be extended to an invertible matrix?,Can any set of  relatively prime elements be extended to an invertible matrix?,n,"Say you're given a ordered set of $n$ relatively prime elements, $a_1,\dots,a_n$ in a principal ideal domain $D$. If I relabel these elements $a_{11},\dots,a_{1n}$ in the same order, is it possible to find some remaining $a_{kj}$ in $D$ such that $(a_{kj})$ is an invertible $n\times n$ matrix over $D$?","Say you're given a ordered set of $n$ relatively prime elements, $a_1,\dots,a_n$ in a principal ideal domain $D$. If I relabel these elements $a_{11},\dots,a_{1n}$ in the same order, is it possible to find some remaining $a_{kj}$ in $D$ such that $(a_{kj})$ is an invertible $n\times n$ matrix over $D$?",,"['linear-algebra', 'principal-ideal-domains']"
12,When does an eigenvector of a matrix contain only a constant?,When does an eigenvector of a matrix contain only a constant?,,"When I compute the eigenvectors of a certain matrix, the first of them is composed entirely of a single constant. What properties of a matrix lead to this result? Update By ""a vector composed entirely of a constant"" I mean n repetitions of a constant comprising a length- n vector.","When I compute the eigenvectors of a certain matrix, the first of them is composed entirely of a single constant. What properties of a matrix lead to this result? Update By ""a vector composed entirely of a constant"" I mean n repetitions of a constant comprising a length- n vector.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
13,Eigenvalues of anti-circulant matrices using 1-circulant matrices,Eigenvalues of anti-circulant matrices using 1-circulant matrices,,"Is there any theorem to find the eigenvalues of any anti-circulant matrix using the equivalent (with the same first row) circulant matrix. I found out that, for any anti-circulant matrix, the eigenvalues (taken as $\mu$) of the anti-circulant matrix can be written as, \begin{equation} \mu = \pm \mid{\lambda_j}\mid \label{mu_alpha} \end{equation} where $\lambda_j$ is an eigenvalue of 1-circulant matrix with the same first row. This seems valid since any anti-circulant matrix should be symmetric resulting in real eigenvalues. Can anyone send me a link to any reference which has this proof..? or can you please comment if you think that this should not be correct ?","Is there any theorem to find the eigenvalues of any anti-circulant matrix using the equivalent (with the same first row) circulant matrix. I found out that, for any anti-circulant matrix, the eigenvalues (taken as $\mu$) of the anti-circulant matrix can be written as, \begin{equation} \mu = \pm \mid{\lambda_j}\mid \label{mu_alpha} \end{equation} where $\lambda_j$ is an eigenvalue of 1-circulant matrix with the same first row. This seems valid since any anti-circulant matrix should be symmetric resulting in real eigenvalues. Can anyone send me a link to any reference which has this proof..? or can you please comment if you think that this should not be correct ?",,"['linear-algebra', 'matrices', 'reference-request', 'eigenvalues-eigenvectors']"
14,What's the fastest way to take powers of a square matrix?,What's the fastest way to take powers of a square matrix?,,"So I know that you can use the Strassen Algorithm to multiply two matrices in seven operations, but what about multiplying two matrices that are exactly the same. Is there a faster way to go about doing that (ie. by reducing the number of multiplications per iteration to something less than 7) ?","So I know that you can use the Strassen Algorithm to multiply two matrices in seven operations, but what about multiplying two matrices that are exactly the same. Is there a faster way to go about doing that (ie. by reducing the number of multiplications per iteration to something less than 7) ?",,"['linear-algebra', 'algorithms', 'matrices']"
15,Norm of triangular matrix with constant rows $\approx \sqrt{d \log 2}$?,Norm of triangular matrix with constant rows ?,\approx \sqrt{d \log 2},"Suppose I have a triangular $d\times d$ matrix $A$ with constant rows normalized to have Euclidean norm 1. Empirically it appears that operator norm (largest singular value) of such matrix is $\sqrt{d \log 2}$ , for large $d$ , why? For instance, for $d=5$ $$A=\left( \begin{array}{ccccc}  1 & 0 & 0 & 0 & 0 \\  \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0 & 0 & 0 \\  \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & 0 & 0 \\  \frac{1}{2} & \frac{1}{2} & \frac{1}{2} & \frac{1}{2} & 0 \\  \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} \\ \end{array} \right)$$ with norm $1.94787$ . For larger values of $d$ , $\sqrt{d \log 2}$ appears to make a nice fit: Related earlier question about singular spectrum of such matrix -- Why do singular values decay as O(1/k) for this triangular matrix? Notebook: forum-triangular-singularvalues.nb","Suppose I have a triangular matrix with constant rows normalized to have Euclidean norm 1. Empirically it appears that operator norm (largest singular value) of such matrix is , for large , why? For instance, for with norm . For larger values of , appears to make a nice fit: Related earlier question about singular spectrum of such matrix -- Why do singular values decay as O(1/k) for this triangular matrix? Notebook: forum-triangular-singularvalues.nb","d\times d A \sqrt{d \log 2} d d=5 A=\left(
\begin{array}{ccccc}
 1 & 0 & 0 & 0 & 0 \\
 \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0 & 0 & 0 \\
 \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & 0 & 0 \\
 \frac{1}{2} & \frac{1}{2} & \frac{1}{2} & \frac{1}{2} & 0 \\
 \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} \\
\end{array}
\right) 1.94787 d \sqrt{d \log 2}",['linear-algebra']
16,Is it true that finite-dimensional vector spaces can have infinite sequences as elements?,Is it true that finite-dimensional vector spaces can have infinite sequences as elements?,,"I recently encountered the following definition of a finite-dimensional vector space in Axler's Linear Algebra Done Right : A vector space is called finite-dimensional if some list of vectors in it spans the space . This I was then thinking about $V = \{(x, 0, 0, \cdots): x \in \mathbb{R} \}$ i.e., the set of all infinite sequences whose first value is some real number and for which all other values are 0. The book defines ${\mathbb{R}}^{\infty} = \{(x_1, x_2, \cdots): x_j \in \mathbb{R} \text{ for } j = 1, 2, \cdots \}.$ He defines addition and scalar multiplication with ${\mathbb{R}}^{\infty}$ as you'd expect: $(x_1, x_2, \cdots) + (y_1, y_2, \cdots) = (x_1+y_1, x_2+y_2, \cdots)$ and $\lambda(x_1, x_2, \cdots) = (\lambda x_1, \lambda x_2, \cdots)$ . Clearly $V$ is a subspace of $\mathbb{R}^{\infty}$ . Now since any $v = (x, 0, 0 \cdots) \in V$ can be written $v = x(1, 0, 0, \cdots)$ where $x \in \mathbb{R}$ and $(1, 0, 0, \cdots) \in V$ , $(1, 0, 0, \cdots)$ alone spans $V$ . Since this list of just one vector in $V$ spans $V$ , I am inclined to think that $V$ is finite-dimensional. This does not mesh with any pre-existing intuition I had about what finite-dimensional vector spaces were, nor can I find any examples like this of a finite-dimensional vector space online, which leads me to think there is an error in my reasoning. Is this correct? If not, can someone point me to the error in my reasoning?","I recently encountered the following definition of a finite-dimensional vector space in Axler's Linear Algebra Done Right : A vector space is called finite-dimensional if some list of vectors in it spans the space . This I was then thinking about i.e., the set of all infinite sequences whose first value is some real number and for which all other values are 0. The book defines He defines addition and scalar multiplication with as you'd expect: and . Clearly is a subspace of . Now since any can be written where and , alone spans . Since this list of just one vector in spans , I am inclined to think that is finite-dimensional. This does not mesh with any pre-existing intuition I had about what finite-dimensional vector spaces were, nor can I find any examples like this of a finite-dimensional vector space online, which leads me to think there is an error in my reasoning. Is this correct? If not, can someone point me to the error in my reasoning?","V = \{(x, 0, 0, \cdots): x \in \mathbb{R} \} {\mathbb{R}}^{\infty} = \{(x_1, x_2, \cdots): x_j \in \mathbb{R} \text{ for } j = 1, 2, \cdots \}. {\mathbb{R}}^{\infty} (x_1, x_2, \cdots) + (y_1, y_2, \cdots) = (x_1+y_1, x_2+y_2, \cdots) \lambda(x_1, x_2, \cdots) = (\lambda x_1, \lambda x_2, \cdots) V \mathbb{R}^{\infty} v = (x, 0, 0 \cdots) \in V v = x(1, 0, 0, \cdots) x \in \mathbb{R} (1, 0, 0, \cdots) \in V (1, 0, 0, \cdots) V V V V","['linear-algebra', 'vector-spaces', 'vectors']"
17,"Is there any sort of relationship between the terms ""base"" ( number systems) and ""basis"" ( linear algebra) as both have similar definitions.","Is there any sort of relationship between the terms ""base"" ( number systems) and ""basis"" ( linear algebra) as both have similar definitions.",,"I was going through the topic of number systems today when I went to see the proper definition of a base. It said ""the number of digits or combination of digits that a system of counting uses to represent numbers."" I remembered seeing the term basis in linear algebra having a similar definition: ""a set B of vectors in a vector space V is called a basis if every element of V may be written in a unique way as a finite linear combination of elements of B."" So, are the two topics related in any way? Or  Are they the same thing? Or are they totally different things?","I was going through the topic of number systems today when I went to see the proper definition of a base. It said ""the number of digits or combination of digits that a system of counting uses to represent numbers."" I remembered seeing the term basis in linear algebra having a similar definition: ""a set B of vectors in a vector space V is called a basis if every element of V may be written in a unique way as a finite linear combination of elements of B."" So, are the two topics related in any way? Or  Are they the same thing? Or are they totally different things?",,"['linear-algebra', 'soft-question', 'terminology', 'number-systems']"
18,Submanifolds on 2x2 matrices with trace=0.,Submanifolds on 2x2 matrices with trace=0.,,"Let $S=\{X\in\text{Mat}_2(\mathbb{R}) : \text{tr}(X)=0\}$ . The group $\text{SL}_2(\mathbb{R})$ acts on $S$ by conjugation, that is $g \cdot X = gXg^{-1}$ . Describe the orbits of this action. When are the orbits submanifolds of S and what is their dimension? My attempt thus far: For 1, the orbits are given by matrices with the same Jordan Form, but I'm not sure how to approach 2. My idea is that for any $A \in \text{Orb}(X)$ , $\text{det}(A) = \text{det}(X)$ , so the orbit is given by the submanifold defined by $f^{-1}(0)$ , $f:S \rightarrow \mathbb{R}$ , $f(A) = \text{det}(A)-\text{det}(X)$ . Then in order for the orbit to be a submanifold, $Df_A$ must be of constant rank on $\text{Orb}(X)$ , but I am unsure of how to show this.","Let . The group acts on by conjugation, that is . Describe the orbits of this action. When are the orbits submanifolds of S and what is their dimension? My attempt thus far: For 1, the orbits are given by matrices with the same Jordan Form, but I'm not sure how to approach 2. My idea is that for any , , so the orbit is given by the submanifold defined by , , . Then in order for the orbit to be a submanifold, must be of constant rank on , but I am unsure of how to show this.",S=\{X\in\text{Mat}_2(\mathbb{R}) : \text{tr}(X)=0\} \text{SL}_2(\mathbb{R}) S g \cdot X = gXg^{-1} A \in \text{Orb}(X) \text{det}(A) = \text{det}(X) f^{-1}(0) f:S \rightarrow \mathbb{R} f(A) = \text{det}(A)-\text{det}(X) Df_A \text{Orb}(X),"['linear-algebra', 'multivariable-calculus', 'derivatives', 'smooth-manifolds']"
19,showing that $G$ does not have an embedding in $GL_n(F)$ for any $n\ge 1$ and field $F$,showing that  does not have an embedding in  for any  and field,G GL_n(F) n\ge 1 F,"Let $G$ denote the group of bijective maps $g : \mathbb{Z}\to \mathbb{Z}$ such that $g$ fixes all but finitely many integers. Show that there does not exist a field $F$ and an $n\ge 1$ so that $G$ is isomorphic to a subgroup of (embeds in) $GL_n(F),$ the set of invertible $n\times n$ matrices with entries in $F$ . I know that for any field $F$ , every finite group embeds into $GL_n(F)$ for some $n\ge 1.$ I think it might be useful to use a contradiction here. So suppose there exists an $n\ge 1$ and a field $F$ so that $\phi : G\to S$ is an isomorphism, where $S$ is a subgroup of $GL_n(F),$ denoted $S\leq GL_n(F).$ There are various properties of isomorphisms that may be useful; for instance, the order of $g\in G$ equals the order of $\phi(g)$ in $S$ , $S$ is abelian if and only if $G$ is abelian, etc.  I'm not sure how to use the properties of $G$ and $\phi$ to obtain a contradiction here though. Edit: I was wondering if @JyrkiLahtonen could elaborate on his answer? I think I mostly understand it, but I don't get a few details. Below is my understanding of his answer. $G_2$ is abelian because the permutations $\sigma_i$ and $\sigma_j$ commute for any $i$ and $j$ and any element of $G_2$ is of the form $\sigma_{i_1}^{a_{i_1}}\cdots \sigma_{i_n}^{a_{i_n}}$ for some $i_j \ge 1, a_{i_j} \in \mathbb{Z}\,\forall i_j$ (so $\sigma_{a}^{b}\sigma_{c}^{d} = \sigma_{c}^{d}\sigma_{a}^{b}$ for any $b,d \in\mathbb{Z}, a,c\ge 1$ ). It is an infinite direct sum of cyclic groups of order two because for any $i$ the cyclic group $\langle \sigma_i \rangle = \{e, \sigma_i\}$ has trivial intersection with the subgroup $\langle \{ \langle \sigma_j\rangle : j\neq i\}\rangle$ , $G_2$ is abelian so every subgroup $\langle \sigma_i\rangle$ of $G_2$ is normal in $G_2$ , and $G_2 = \langle \{ \langle \sigma_j\rangle : j\ge 1\} \rangle = \langle e,\sigma_1,\sigma_2,\cdots \rangle = \langle \sigma_1,\sigma_2,\cdots \rangle.$ The vectors $\frac{1}2(x+\phi(\sigma_i)(x))$ and $\frac{1}2(x-\phi(\sigma_i)(x))$ are eigenvectors of $\phi(\sigma_i)$ with corresponding eigenvalues +1 and $-1$ . Since $x$ was arbitrary, this shows that every vector of $V$ is a linear combination of eigenvectors of $\phi(\sigma_i),$ so the set of eigenvectors of $\phi(\sigma_i)$ spans $V$ . Hence we can obtain a basis of eigenvectors of $\phi(\sigma_i)$ for $V$ (e.g. we start with an eigenvector $v_1$ and then if $V\neq \mathrm{span} \{v_1\}$ , we pick an eigenvector $v_2 \in V\neq \mathrm{span} \{v_1\}$ and continue until we get a basis). Since $V$ has an ordered basis consisting of eigenvectors of $\phi(\sigma_i)$ , the matrix $\phi(\sigma_i)$ is diagonalizable. $N$ is the number of matrices. The above shows that the space $V$ has a basis consisting of eigenvectors of $\phi(\sigma_i)$ for fixed $i$ . Every element of $V$ can be written as a linear combination of two eigenvectors of $\phi(\sigma_{k+1})$ corresponding to the eigenvalues $+1$ and $-1$ . Also, these eigenspaces are disjoint because if $0\neq w\in V_+\cap V_-,$ then $w$ is an eigenvector of $\phi(\sigma_{k+1})$ with eigenvalue $1$ and $-1$ , which isn't possible. So $V= V_+\oplus V_-.$ The transformations $\phi(\sigma_j), 1\leq j\leq k$ commute with $\phi(\sigma_{k+1})$ because $\phi(\sigma_j)\phi(\sigma_{k+1}) = \phi(\sigma_j \sigma_{k+1}) = \phi(\sigma_{k+1}\sigma_j) = \phi(\sigma_{k+1})\phi(\sigma_j).$ Fix $1\leq j\leq k.$ Let $v_1$ and $v_2$ be the eigenvectors of $\phi(\sigma_{k+1})$ corresponding to eigenvalues $1$ and $-1$ . Then $\phi(\sigma_j)(v_1) = \phi(\sigma_j)(\phi(\sigma_{k+1})(v_1)) = \phi(\sigma_{k+1})(\phi(\sigma_j)(v_1)),$ so $\phi(\sigma_j)(v_1)$ is an eigenvector of $\phi(\sigma_{k+1})$ with eigenvalue $1.$ Hence $\phi(\sigma_j)(V_+) \subseteq V_+.$ Similarly $\phi(\sigma_j)^{-1}(V_+)\subseteq V_+$ so $V_+ = \phi(\sigma_j) (V_+).$ Similarly $\phi(\sigma_j)(V_-) = V_-.$ So does the induction hypothesis apply for $N=k$ ? An eigenvector $v$ is a shared eigenvector of all $\phi(\sigma_i)$ if it's an eigenvector of every $\phi(\sigma_i),$ right? What does ""they must all respect the decomposition (*)"" mean precisely? Hence if $v$ is a shared eigenvector of all $\phi(\sigma_j)$ 's in the basis for $V_-,$ for any $1\leq j\leq k, \phi(\sigma_{k+1})(v) = \phi(\sigma_{k+1})(-\phi(\sigma_j)(v)) = -\phi(\sigma_j)(\phi(\sigma_{k+1})(v))$ , so $\phi(\sigma_{k+1})(v)$ is an eigenvector of $\phi(\sigma_j)$ with eigenvalue -1. But is $v$ an eigenvector of $\phi(\sigma_{k+1})$ with eigenvalue $-1$ ? How does the claim follow from the fact that $GL_n(F)$ contains only $2^n$ diagonal matrices with entries $\pm 1$ ? The statement is clearly true, but each $\phi(\sigma_j)$ is diagonalizable, so there is an invertible matrix $P_j$ so that $P_j^{-1} \phi(\sigma_j) P_j$ is diagonal for all $j$ . Why can we adjoin a primitive third root of unity, $\omega$ to the field $F$ ? Does this just mean we replace $F$ with $F\cup \{\omega\}$ ?","Let denote the group of bijective maps such that fixes all but finitely many integers. Show that there does not exist a field and an so that is isomorphic to a subgroup of (embeds in) the set of invertible matrices with entries in . I know that for any field , every finite group embeds into for some I think it might be useful to use a contradiction here. So suppose there exists an and a field so that is an isomorphism, where is a subgroup of denoted There are various properties of isomorphisms that may be useful; for instance, the order of equals the order of in , is abelian if and only if is abelian, etc.  I'm not sure how to use the properties of and to obtain a contradiction here though. Edit: I was wondering if @JyrkiLahtonen could elaborate on his answer? I think I mostly understand it, but I don't get a few details. Below is my understanding of his answer. is abelian because the permutations and commute for any and and any element of is of the form for some (so for any ). It is an infinite direct sum of cyclic groups of order two because for any the cyclic group has trivial intersection with the subgroup , is abelian so every subgroup of is normal in , and The vectors and are eigenvectors of with corresponding eigenvalues +1 and . Since was arbitrary, this shows that every vector of is a linear combination of eigenvectors of so the set of eigenvectors of spans . Hence we can obtain a basis of eigenvectors of for (e.g. we start with an eigenvector and then if , we pick an eigenvector and continue until we get a basis). Since has an ordered basis consisting of eigenvectors of , the matrix is diagonalizable. is the number of matrices. The above shows that the space has a basis consisting of eigenvectors of for fixed . Every element of can be written as a linear combination of two eigenvectors of corresponding to the eigenvalues and . Also, these eigenspaces are disjoint because if then is an eigenvector of with eigenvalue and , which isn't possible. So The transformations commute with because Fix Let and be the eigenvectors of corresponding to eigenvalues and . Then so is an eigenvector of with eigenvalue Hence Similarly so Similarly So does the induction hypothesis apply for ? An eigenvector is a shared eigenvector of all if it's an eigenvector of every right? What does ""they must all respect the decomposition (*)"" mean precisely? Hence if is a shared eigenvector of all 's in the basis for for any , so is an eigenvector of with eigenvalue -1. But is an eigenvector of with eigenvalue ? How does the claim follow from the fact that contains only diagonal matrices with entries ? The statement is clearly true, but each is diagonalizable, so there is an invertible matrix so that is diagonal for all . Why can we adjoin a primitive third root of unity, to the field ? Does this just mean we replace with ?","G g : \mathbb{Z}\to \mathbb{Z} g F n\ge 1 G GL_n(F), n\times n F F GL_n(F) n\ge 1. n\ge 1 F \phi : G\to S S GL_n(F), S\leq GL_n(F). g\in G \phi(g) S S G G \phi G_2 \sigma_i \sigma_j i j G_2 \sigma_{i_1}^{a_{i_1}}\cdots \sigma_{i_n}^{a_{i_n}} i_j \ge 1, a_{i_j} \in \mathbb{Z}\,\forall i_j \sigma_{a}^{b}\sigma_{c}^{d} = \sigma_{c}^{d}\sigma_{a}^{b} b,d \in\mathbb{Z}, a,c\ge 1 i \langle \sigma_i \rangle = \{e, \sigma_i\} \langle \{ \langle \sigma_j\rangle : j\neq i\}\rangle G_2 \langle \sigma_i\rangle G_2 G_2 G_2 = \langle \{ \langle \sigma_j\rangle : j\ge 1\} \rangle = \langle e,\sigma_1,\sigma_2,\cdots \rangle = \langle \sigma_1,\sigma_2,\cdots \rangle. \frac{1}2(x+\phi(\sigma_i)(x)) \frac{1}2(x-\phi(\sigma_i)(x)) \phi(\sigma_i) -1 x V \phi(\sigma_i), \phi(\sigma_i) V \phi(\sigma_i) V v_1 V\neq \mathrm{span} \{v_1\} v_2 \in V\neq \mathrm{span} \{v_1\} V \phi(\sigma_i) \phi(\sigma_i) N V \phi(\sigma_i) i V \phi(\sigma_{k+1}) +1 -1 0\neq w\in V_+\cap V_-, w \phi(\sigma_{k+1}) 1 -1 V= V_+\oplus V_-. \phi(\sigma_j), 1\leq j\leq k \phi(\sigma_{k+1}) \phi(\sigma_j)\phi(\sigma_{k+1}) = \phi(\sigma_j \sigma_{k+1}) = \phi(\sigma_{k+1}\sigma_j) = \phi(\sigma_{k+1})\phi(\sigma_j). 1\leq j\leq k. v_1 v_2 \phi(\sigma_{k+1}) 1 -1 \phi(\sigma_j)(v_1) = \phi(\sigma_j)(\phi(\sigma_{k+1})(v_1)) = \phi(\sigma_{k+1})(\phi(\sigma_j)(v_1)), \phi(\sigma_j)(v_1) \phi(\sigma_{k+1}) 1. \phi(\sigma_j)(V_+) \subseteq V_+. \phi(\sigma_j)^{-1}(V_+)\subseteq V_+ V_+ = \phi(\sigma_j) (V_+). \phi(\sigma_j)(V_-) = V_-. N=k v \phi(\sigma_i) \phi(\sigma_i), v \phi(\sigma_j) V_-, 1\leq j\leq k, \phi(\sigma_{k+1})(v) = \phi(\sigma_{k+1})(-\phi(\sigma_j)(v)) = -\phi(\sigma_j)(\phi(\sigma_{k+1})(v)) \phi(\sigma_{k+1})(v) \phi(\sigma_j) v \phi(\sigma_{k+1}) -1 GL_n(F) 2^n \pm 1 \phi(\sigma_j) P_j P_j^{-1} \phi(\sigma_j) P_j j \omega F F F\cup \{\omega\}","['linear-algebra', 'abstract-algebra', 'group-theory', 'group-isomorphism']"
20,Can orthogonal matrix with positive diagonal have -1 in its spectrum?,Can orthogonal matrix with positive diagonal have -1 in its spectrum?,,"Let $O\in \mathbb{R}^{n\times n}$ be an orthogonal matrix, i.e. $O^tO=I=OO^t$ . Suppose its diagonal entries $\{O_{jj}\}_{j\in \{1,...,n\}}$ are (strictly) positive. Can $-1$ then be included in the spectrum of $O$ ? Note that if the diagonal is required to be non-negative in stead of positive, then $$\begin{pmatrix} 0&-1 & 0\\ 0 & 0 & -1\\ -1 & 0& 0 \end{pmatrix}$$ provides a counterexample, since it has a non-negative diagonal yet includes -1 in its spectrum. Apart from that, a straightforward calculation like (suppose, seeking a contradiction, that $v\in \mathbb{R}^n$ is a normalized eigenvector with eigenvalue -1) $$-1=\langle v,-v\rangle=\langle v,Ov\rangle \geq \sum_{j=1}^n\left(|O_{jj}|v_j^2 - \left|\sum_{k\neq j}O_{jk}v_jv_k\right|\right)> -\left(\sum_{j=1}^n\sum_{k\neq j}O_{jk}^2\right)^{1/2}\geq-n^{1/2}$$ doesn't suffice to resolve the problem (because it didn't result in the desired contradiction). Likewise, writing out $\langle e_j,Ov\rangle = -\langle e_j,v\rangle$ doesn't seem to yield anything. I am aware that my statement, if true, would imply for odd $n$ that a positive diagonal would imply $\det O=1$ . Also, note that there is a related 'hybrid' open question: what if the diagonal of $O$ is non-negative and non-zero? EDIT: there are also counterexamples to the hybrid problem just mentioned: take $$\begin{pmatrix} \sin(\theta) & \cos(\theta) & 0\\ 0 & 0 & -1\\ \cos(\theta) & -\sin(\theta)& 0 \end{pmatrix}$$ for an angle $\theta \in (0,\pi)$ . (to quicker analyse examples in odd $n$ , like $n=3$ here, it helps to prove the auxiliary lemma $\det O = -1 \Rightarrow -1 \in \sigma(O)$ )","Let be an orthogonal matrix, i.e. . Suppose its diagonal entries are (strictly) positive. Can then be included in the spectrum of ? Note that if the diagonal is required to be non-negative in stead of positive, then provides a counterexample, since it has a non-negative diagonal yet includes -1 in its spectrum. Apart from that, a straightforward calculation like (suppose, seeking a contradiction, that is a normalized eigenvector with eigenvalue -1) doesn't suffice to resolve the problem (because it didn't result in the desired contradiction). Likewise, writing out doesn't seem to yield anything. I am aware that my statement, if true, would imply for odd that a positive diagonal would imply . Also, note that there is a related 'hybrid' open question: what if the diagonal of is non-negative and non-zero? EDIT: there are also counterexamples to the hybrid problem just mentioned: take for an angle . (to quicker analyse examples in odd , like here, it helps to prove the auxiliary lemma )","O\in \mathbb{R}^{n\times n} O^tO=I=OO^t \{O_{jj}\}_{j\in \{1,...,n\}} -1 O \begin{pmatrix}
0&-1 & 0\\
0 & 0 & -1\\
-1 & 0& 0
\end{pmatrix} v\in \mathbb{R}^n -1=\langle v,-v\rangle=\langle v,Ov\rangle \geq \sum_{j=1}^n\left(|O_{jj}|v_j^2 - \left|\sum_{k\neq j}O_{jk}v_jv_k\right|\right)> -\left(\sum_{j=1}^n\sum_{k\neq j}O_{jk}^2\right)^{1/2}\geq-n^{1/2} \langle e_j,Ov\rangle = -\langle e_j,v\rangle n \det O=1 O \begin{pmatrix}
\sin(\theta) & \cos(\theta) & 0\\
0 & 0 & -1\\
\cos(\theta) & -\sin(\theta)& 0
\end{pmatrix} \theta \in (0,\pi) n n=3 \det O = -1 \Rightarrow -1 \in \sigma(O)","['linear-algebra', 'inequality', 'orthogonal-matrices']"
21,Is there a way to determine the eigenvectors of a matrix without working out the eigenvalues?,Is there a way to determine the eigenvectors of a matrix without working out the eigenvalues?,,"Normally, I would first work out the eigenvalues of a matrix and use them to determine the eigenvectors. However, is it possible to go the other way around? Is there any way to determine the eigenvectors of a matrix without working out the eigenvalues?","Normally, I would first work out the eigenvalues of a matrix and use them to determine the eigenvectors. However, is it possible to go the other way around? Is there any way to determine the eigenvectors of a matrix without working out the eigenvalues?",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
22,A Kronecker Product identity,A Kronecker Product identity,,"I want to show the Kronecker Product identity listed on Wikipedia : $$\begin{align} \mathrm{vec}(AXB) =(B^T \otimes A) \mathrm{vec}(X) \\ \tag{1} \end{align}$$ Wikipedia does not cite references for the proof for (1), and I want to verify. Before the calculation, agree with some convention. Let us denote $a\; \mathrm{mod}\; b$ for the remainder of $a$ modulo $b$ (so that $9\; \mathrm{mod}\; 4 =1$ ). This is unlike the usual practice, but otherwise the expression gets too verbose. Moreover let us denote the $(i,j)$ -th entry of $A$ to be $A_{i,j}$ , and $i$ -th entry of $v$ to be $v_{i}$ . Also say that $A \in \mathbb{R}^{M \cdot N}$ if it is a $M$ by $N$ matrix, by slightly abusing the meaning of $\mathbb{R}$ . If we stick to only addition and multiplication, once the proof works, it works for any field. Firstly, restate the formal definition of Kronecker product and vectorization, for later use. For $X' \in \mathbb{R}^{N_2 \cdot N_3}$ $$\begin{align} (\mathrm{vec}(X'))_{m_1} =X'_{m_1\; \mathrm{mod}\; N_1, \lceil m_1/N_1 \rceil} \\ \tag{2} \end{align}$$ For $A' \in \mathbb{R}^{N_1 \cdot N_2}, B' \in \mathbb{R}^{N_3 \cdot N_4}$ , $$\begin{align} (A' \otimes B')_{m_1, m_2} =A'_{\lceil m_1/N_3 \rceil, \lceil m_2/N_4 \rceil} B'_{m_1\; \mathrm{mod}\; N_3, m_2\; \mathrm{mod}\; N_4} \\ \tag{3} \end{align}$$ Now, let $A \in \mathbb{R}^{N_1 \cdot N_2},\; X \in \mathbb{R}^{N_2 \cdot N_3},\; B \in \mathbb{R}^{N_3 \cdot N_4},$ , We try to evaluate lhs and rhs of (1). $$\begin{align} &(\mathrm{vec}(AXB))_{m_1} \\ =&(AXB)_{m_1\; \mathrm{mod}\; N_1, \lceil m_1/N_1 \rceil} \\ =&\sum_{n_2=1}^{N_2} \sum_{n_3=1}^{N_3} A_{m_1\; \mathrm{mod}\; N_1, n_2} X_{n_2, n_3} B_{n_3, \lceil m_1/N_1 \rceil} \\ \tag{4} \end{align}$$ $$\begin{align} &((B^T \otimes A) \mathrm{vec}(X) )_{m_1} \\ =&\sum_{m_2=1}^{N_2 N_3} (B^T \otimes A)_{m_1, m_2} \mathrm{vec}(X)_{m_2} \\ =&\sum_{m_2=1}^{N_2 N_3} (B^T)_{\lceil m_1/N_1 \rceil, \lceil m_2/N_2 \rceil} A_{m_1\; \mathrm{mod}\; N_1, m_2\; \mathrm{mod}\; N_2} \mathrm{vec}(X)_{m_2} \\  =&\sum_{m_2=1}^{N_2 N_3} B_{\lceil m_2/N_2 \rceil, \lceil m_1/N_1 \rceil} A_{m_1\; \mathrm{mod}\; N_1, m_2\; \mathrm{mod}\; N_2} X_{m_2\; \mathrm{mod}\; N_2, \lceil m_2/N_2 \rceil} \\ \tag{5} \end{align}$$ It would seem that we are close, but it is not obvious that (4) and (5) are equal. Although $A$ 's row index $m_1\; \mathrm{mod}\; N_1$ and $B$ 's column index $\lceil m_1/N_1 \rceil$ agree, the other do not. It is suggestive that maybe if we ""sum away"" $n_2, n_3$ , we might find them to agree, but $A,X,B$ are all free, and we have nothing to exploit to carry out that summation.","I want to show the Kronecker Product identity listed on Wikipedia : Wikipedia does not cite references for the proof for (1), and I want to verify. Before the calculation, agree with some convention. Let us denote for the remainder of modulo (so that ). This is unlike the usual practice, but otherwise the expression gets too verbose. Moreover let us denote the -th entry of to be , and -th entry of to be . Also say that if it is a by matrix, by slightly abusing the meaning of . If we stick to only addition and multiplication, once the proof works, it works for any field. Firstly, restate the formal definition of Kronecker product and vectorization, for later use. For For , Now, let , We try to evaluate lhs and rhs of (1). It would seem that we are close, but it is not obvious that (4) and (5) are equal. Although 's row index and 's column index agree, the other do not. It is suggestive that maybe if we ""sum away"" , we might find them to agree, but are all free, and we have nothing to exploit to carry out that summation.","\begin{align}
\mathrm{vec}(AXB)
=(B^T \otimes A) \mathrm{vec}(X) \\ \tag{1}
\end{align} a\; \mathrm{mod}\; b a b 9\; \mathrm{mod}\; 4 =1 (i,j) A A_{i,j} i v v_{i} A \in \mathbb{R}^{M \cdot N} M N \mathbb{R} X' \in \mathbb{R}^{N_2 \cdot N_3} \begin{align}
(\mathrm{vec}(X'))_{m_1}
=X'_{m_1\; \mathrm{mod}\; N_1, \lceil m_1/N_1 \rceil} \\ \tag{2}
\end{align} A' \in \mathbb{R}^{N_1 \cdot N_2}, B' \in \mathbb{R}^{N_3 \cdot N_4} \begin{align}
(A' \otimes B')_{m_1, m_2}
=A'_{\lceil m_1/N_3 \rceil, \lceil m_2/N_4 \rceil}
B'_{m_1\; \mathrm{mod}\; N_3, m_2\; \mathrm{mod}\; N_4} \\ \tag{3}
\end{align} A \in \mathbb{R}^{N_1 \cdot N_2},\; X \in \mathbb{R}^{N_2 \cdot N_3},\; B \in \mathbb{R}^{N_3 \cdot N_4}, \begin{align}
&(\mathrm{vec}(AXB))_{m_1} \\
=&(AXB)_{m_1\; \mathrm{mod}\; N_1, \lceil m_1/N_1 \rceil} \\
=&\sum_{n_2=1}^{N_2} \sum_{n_3=1}^{N_3}
A_{m_1\; \mathrm{mod}\; N_1, n_2} X_{n_2, n_3} B_{n_3, \lceil m_1/N_1 \rceil} \\ \tag{4}
\end{align} \begin{align}
&((B^T \otimes A) \mathrm{vec}(X) )_{m_1} \\
=&\sum_{m_2=1}^{N_2 N_3} (B^T \otimes A)_{m_1, m_2} \mathrm{vec}(X)_{m_2} \\
=&\sum_{m_2=1}^{N_2 N_3} (B^T)_{\lceil m_1/N_1 \rceil, \lceil m_2/N_2 \rceil}
A_{m_1\; \mathrm{mod}\; N_1, m_2\; \mathrm{mod}\; N_2} \mathrm{vec}(X)_{m_2} \\ 
=&\sum_{m_2=1}^{N_2 N_3} B_{\lceil m_2/N_2 \rceil, \lceil m_1/N_1 \rceil}
A_{m_1\; \mathrm{mod}\; N_1, m_2\; \mathrm{mod}\; N_2} X_{m_2\; \mathrm{mod}\; N_2, \lceil m_2/N_2 \rceil} \\ \tag{5}
\end{align} A m_1\; \mathrm{mod}\; N_1 B \lceil m_1/N_1 \rceil n_2, n_3 A,X,B","['linear-algebra', 'matrices', 'kronecker-product']"
23,Show that the product of the Jacobian and the inverse Jacobian is 1,Show that the product of the Jacobian and the inverse Jacobian is 1,,"I have seen the following fact in a textbook, but am having trouble proving it. If the Jacobian (""stretch factor"" for change-of-variables) is given by $\left | \frac{\partial (x,y)}{\partial (u,v)} \right | =  \begin{vmatrix}  \frac{\partial x}{\partial u}&  \frac{\partial y}{\partial u}\\    \frac{\partial x}{\partial v}&   \frac{\partial y}{\partial v} \end{vmatrix}$ , with $x=x(u,v)$ and $y=y(u,v)$ , then $\left | \frac{\partial (x,y)}{\partial (u,v)} \right | \left | \frac{\partial (u,v)}{\partial (x,y)} \right | = 1$ . I am attempting to show this is true by multiplying the matrices and then taking the determinant of the product, but I do not understand how the product becomes equivalent to $\begin{bmatrix}  1& 0\\    0& 1 \end{bmatrix}$ . Thanks!","I have seen the following fact in a textbook, but am having trouble proving it. If the Jacobian (""stretch factor"" for change-of-variables) is given by , with and , then . I am attempting to show this is true by multiplying the matrices and then taking the determinant of the product, but I do not understand how the product becomes equivalent to . Thanks!","\left | \frac{\partial (x,y)}{\partial (u,v)} \right | =  \begin{vmatrix}
 \frac{\partial x}{\partial u}&  \frac{\partial y}{\partial u}\\ 
  \frac{\partial x}{\partial v}&   \frac{\partial y}{\partial v}
\end{vmatrix} x=x(u,v) y=y(u,v) \left | \frac{\partial (x,y)}{\partial (u,v)} \right | \left | \frac{\partial (u,v)}{\partial (x,y)} \right | = 1 \begin{bmatrix}
 1& 0\\ 
  0& 1
\end{bmatrix}","['linear-algebra', 'multivariable-calculus', 'jacobian']"
24,Eigenpolynomial of a linear operator,Eigenpolynomial of a linear operator,,"Let $V$ be a $n$-dimensional vector space over a field $F$, let $A\in \text{End}(V)$. Let $q\in F[x]$ be an irreducible polynomial and  $$ V_q:=\{x∈V:q(A)x=0\}. $$ I wish to prove that  $V_q$ is not trivial iff $q$ divides the characteristic polynomial $\chi$ of the operator $A$.","Let $V$ be a $n$-dimensional vector space over a field $F$, let $A\in \text{End}(V)$. Let $q\in F[x]$ be an irreducible polynomial and  $$ V_q:=\{x∈V:q(A)x=0\}. $$ I wish to prove that  $V_q$ is not trivial iff $q$ divides the characteristic polynomial $\chi$ of the operator $A$.",,['linear-algebra']
25,$M^2=I_n$ implies $M$ diagonalizable,implies  diagonalizable,M^2=I_n M,Let $M$ be an  $n \times n$ matrix  such that $M^{2}=I_n$; does this imply that $M$ is similar to a diagonal matrix $D$? How can we prove this?,Let $M$ be an  $n \times n$ matrix  such that $M^{2}=I_n$; does this imply that $M$ is similar to a diagonal matrix $D$? How can we prove this?,,"['linear-algebra', 'matrices', 'diagonalization']"
26,Generate integer matrices with integer eigenvalues,Generate integer matrices with integer eigenvalues,,"I want to generate $500$ random integer matrices with integer eigenvalues. Thanks to this post , I know how to generate a random matrix with whole eigenvalues: Generate a diagonal matrix $D$ with the desired (integer) eigenvalues. Generate an invertible matrix $A$ of the same size as $D$. Record the matrix $A^{-1} D A$. However, the problem is that this generates a lot of float matrices, and I'd actually like to have both integer matrices and integer eigenvalues. The float values are introduced by A.inverse() . According to this post , inverse matrices have whole integer values only when the determinant of the original matrix is 1 or -1 (and therefore an orthogonal matrix). I tried using the C++ library AlgLib, which has a rmatrixrndorthogonal function that uses G.W. Stewart's 1980 algorithm to generate a random uniformly distributed (Haar) orthogonal matrix. I also tried using R 's pracma library, which has a randortho function that also generates orthogonal matrices. However, both functions generate matrices with float values. Is there a way for me to generate orthogonal, integer matrices?","I want to generate $500$ random integer matrices with integer eigenvalues. Thanks to this post , I know how to generate a random matrix with whole eigenvalues: Generate a diagonal matrix $D$ with the desired (integer) eigenvalues. Generate an invertible matrix $A$ of the same size as $D$. Record the matrix $A^{-1} D A$. However, the problem is that this generates a lot of float matrices, and I'd actually like to have both integer matrices and integer eigenvalues. The float values are introduced by A.inverse() . According to this post , inverse matrices have whole integer values only when the determinant of the original matrix is 1 or -1 (and therefore an orthogonal matrix). I tried using the C++ library AlgLib, which has a rmatrixrndorthogonal function that uses G.W. Stewart's 1980 algorithm to generate a random uniformly distributed (Haar) orthogonal matrix. I also tried using R 's pracma library, which has a randortho function that also generates orthogonal matrices. However, both functions generate matrices with float values. Is there a way for me to generate orthogonal, integer matrices?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'random-matrices', 'orthogonal-matrices']"
27,Difficulty in linear algebra question.,Difficulty in linear algebra question.,,"Let $a_{ij}=a_ia_j$ $1\leq i,j\leq n$ where $a_1,... a_n$ are real numbers. Let $A=(a_{ij})$ be the $n \times  n $ matrix . Then It is spoosible to choose $a_1,... a_n$ so as to mke A non singular. The matrix is positive definite if $(a_1,...,a_n)$ is a nonzero vector. The matrix A is positive definite for all $(a_1,...,a_n)$. For all $(a_1,...,a_n)$ zero is an eigen value of A. My attempt: Option 1 is false because if we choose $a_1=\frac{1}{\sqrt{2}} ,    a_2=\sqrt{2}$ then $2\times 2$ matrix $A$ is singular. Option 2 is also false because if we take $a_1=1  \ a_2=2 $ then $2\times2$ matrix $A$ is not positive definite. Option 3 is also false. But I am not getting option 4. Could you please help me? Thanks in advance.","Let $a_{ij}=a_ia_j$ $1\leq i,j\leq n$ where $a_1,... a_n$ are real numbers. Let $A=(a_{ij})$ be the $n \times  n $ matrix . Then It is spoosible to choose $a_1,... a_n$ so as to mke A non singular. The matrix is positive definite if $(a_1,...,a_n)$ is a nonzero vector. The matrix A is positive definite for all $(a_1,...,a_n)$. For all $(a_1,...,a_n)$ zero is an eigen value of A. My attempt: Option 1 is false because if we choose $a_1=\frac{1}{\sqrt{2}} ,    a_2=\sqrt{2}$ then $2\times 2$ matrix $A$ is singular. Option 2 is also false because if we take $a_1=1  \ a_2=2 $ then $2\times2$ matrix $A$ is not positive definite. Option 3 is also false. But I am not getting option 4. Could you please help me? Thanks in advance.",,"['linear-algebra', 'matrices', 'positive-definite', 'positive-semidefinite']"
28,Reference for trace/norm inequality,Reference for trace/norm inequality,,"I'm looking for a reference for a matrix-norm inequality that I used in this answer , which has a few equivalent forms.  I will use notation that applies to complex vector spaces with a sesquilinear inner product, but of course the same applies over real matrices. The statement is as follows: Take $A,B \in \Bbb F^{n \times n}$.  Then    $$\vert\operatorname{tr}(A^*B)\vert \leq \sigma_1(A)\sum_{i=1}^n \sigma_i(B) = \|A\| \operatorname{tr}|B|$$   where $\sigma_i$ denotes the $i$th singular value, $|B| = (B^*B)^{1/2}$, and $\|\cdot\|$ denotes the spectral norm (induced Euclidean norm). I did manage to find some references, but they're overkill, and the texts themselves are not readily accessible to the faint of heart (Bhatia's text is dense and Pedersen's is not about matrices in particular). A suitable reference would be greatly appreciated.","I'm looking for a reference for a matrix-norm inequality that I used in this answer , which has a few equivalent forms.  I will use notation that applies to complex vector spaces with a sesquilinear inner product, but of course the same applies over real matrices. The statement is as follows: Take $A,B \in \Bbb F^{n \times n}$.  Then    $$\vert\operatorname{tr}(A^*B)\vert \leq \sigma_1(A)\sum_{i=1}^n \sigma_i(B) = \|A\| \operatorname{tr}|B|$$   where $\sigma_i$ denotes the $i$th singular value, $|B| = (B^*B)^{1/2}$, and $\|\cdot\|$ denotes the spectral norm (induced Euclidean norm). I did manage to find some references, but they're overkill, and the texts themselves are not readily accessible to the faint of heart (Bhatia's text is dense and Pedersen's is not about matrices in particular). A suitable reference would be greatly appreciated.",,"['linear-algebra', 'matrices', 'reference-request', 'numerical-linear-algebra']"
29,Prove that $AB = 0$ implies $\det\left(A + B\right)^2 = \det\left(A - B\right)^2$.,Prove that  implies .,AB = 0 \det\left(A + B\right)^2 = \det\left(A - B\right)^2,"Given matrices $A, B \in \mathcal{M}_{n \times n}\left(\mathbb{R}\right)$, prove that if $AB = 0$, $\det\left(A + B\right)^2 = \det\left(A - B\right)^2$. This is based on a quiz question from a few weeks ago that I never quite solved, and I forgot my instructors' solutions as well. Here are possible ideas I had: Show that $AB = BA$ (except it's not true). Show that $BA = -BA$—wait, isn't this equivalent to (1)? Show that $\forall i,j\, \left(A_{ij} = 0\right) \mathrm{or} \left(B_{ij}=0\right)$ so that $\det\left(A + B\right) = \left(-1\right)^k\det\left(A - B\right)$ for some $k$. This is definitely a sufficient condition, but I'm not sure if it's necessary. We have learned diagonalization, but we have not learned the Jordan canonical form yet, so it would be ideal if you avoided that kind of answer.","Given matrices $A, B \in \mathcal{M}_{n \times n}\left(\mathbb{R}\right)$, prove that if $AB = 0$, $\det\left(A + B\right)^2 = \det\left(A - B\right)^2$. This is based on a quiz question from a few weeks ago that I never quite solved, and I forgot my instructors' solutions as well. Here are possible ideas I had: Show that $AB = BA$ (except it's not true). Show that $BA = -BA$—wait, isn't this equivalent to (1)? Show that $\forall i,j\, \left(A_{ij} = 0\right) \mathrm{or} \left(B_{ij}=0\right)$ so that $\det\left(A + B\right) = \left(-1\right)^k\det\left(A - B\right)$ for some $k$. This is definitely a sufficient condition, but I'm not sure if it's necessary. We have learned diagonalization, but we have not learned the Jordan canonical form yet, so it would be ideal if you avoided that kind of answer.",,"['linear-algebra', 'matrices', 'determinant']"
30,A linear transformation preserving volume,A linear transformation preserving volume,,"This is ex. from Munkers book: \ find a linear  transformation $h: \mathbb{R}^n \rightarrow \mathbb{R}^n$ that preserves volumes but is not an isometry. It's clear that $n$ should be greater than 1, but even in  case $n=2$ I'm not able to provide an example.","This is ex. from Munkers book: \ find a linear  transformation $h: \mathbb{R}^n \rightarrow \mathbb{R}^n$ that preserves volumes but is not an isometry. It's clear that $n$ should be greater than 1, but even in  case $n=2$ I'm not able to provide an example.",,"['linear-algebra', 'volume']"
31,Notion of a normal operator,Notion of a normal operator,,"I understand that a normal operator is an operator such that $$ AA^\dagger = A^\dagger A $$ where $\dagger$ is the conjugate transpose. However, what is the most intuitive way to ""characterise"" this? For example, $SO(3)$ is the group of rotations in $\mathbb R^3$ A unitary matrix is one that represents an isometry A hermitian matrix is a generalization of symmetric, and is the ""nicest"" (diagonalizable, real eigenvalues, etc) of all matrices over $\mathbb C$ I was hoping for some sort of intuitive explanation of why I would care about normal matrices over $\mathbb C$ (maybe other reasons than the spectral theorem? Something more fundamental / geometric perhaps)","I understand that a normal operator is an operator such that $$ AA^\dagger = A^\dagger A $$ where $\dagger$ is the conjugate transpose. However, what is the most intuitive way to ""characterise"" this? For example, $SO(3)$ is the group of rotations in $\mathbb R^3$ A unitary matrix is one that represents an isometry A hermitian matrix is a generalization of symmetric, and is the ""nicest"" (diagonalizable, real eigenvalues, etc) of all matrices over $\mathbb C$ I was hoping for some sort of intuitive explanation of why I would care about normal matrices over $\mathbb C$ (maybe other reasons than the spectral theorem? Something more fundamental / geometric perhaps)",,"['linear-algebra', 'functional-analysis', 'representation-theory']"
32,Prove/disprove: $\text{det}(A+B)\geq\text{det}(A)+\text{det}(B)$,Prove/disprove:,\text{det}(A+B)\geq\text{det}(A)+\text{det}(B),"Let $A,B$ be non-neg $n$ by $n$ matrices over $\mathbb{C}$. Show $\text{det}(A+B)\geq\text{det}(A)+\text{det}(B)$. I attempted this problem in preparation for qualifying exams, but I'm a little unsure of the solution. My thought is to use the minmax theorem. Let $\eta_i,\lambda_i$ and $\zeta_i$ are the ith eigenvalues of $A$, $B$, and $A+B$. The determinant of each matrix is the product of the respective eigenvalues, and since the matrices are all positive, each eigenvalue is $>0$ and so each determinant is as well. Using the Rayleigh quotient, we have $R_{A+B}(v)=\frac{<v,Av>+<v,Bv>}{<v,v>}=R_A(v)+R_B(v)$ for each $v\neq 0$. Since the Rayleigh coefficients determinant the eigenvalues in increasing order, I would think this means that $\eta_i+\lambda_i=\zeta_i$. Hence, $\text{det}(A+B)=\prod_{i=1}^n\zeta_i=\prod_{i=1}^n(\eta_i+\lambda_i)\geq\prod_{i=1}^n\eta_i+\prod_{i=1}^n\lambda_i=\text{det}(A)+\text{det}(B)$. Does anyone see a flaw in this argument? If it is correct, does anyone know perhaps an easier argument?","Let $A,B$ be non-neg $n$ by $n$ matrices over $\mathbb{C}$. Show $\text{det}(A+B)\geq\text{det}(A)+\text{det}(B)$. I attempted this problem in preparation for qualifying exams, but I'm a little unsure of the solution. My thought is to use the minmax theorem. Let $\eta_i,\lambda_i$ and $\zeta_i$ are the ith eigenvalues of $A$, $B$, and $A+B$. The determinant of each matrix is the product of the respective eigenvalues, and since the matrices are all positive, each eigenvalue is $>0$ and so each determinant is as well. Using the Rayleigh quotient, we have $R_{A+B}(v)=\frac{<v,Av>+<v,Bv>}{<v,v>}=R_A(v)+R_B(v)$ for each $v\neq 0$. Since the Rayleigh coefficients determinant the eigenvalues in increasing order, I would think this means that $\eta_i+\lambda_i=\zeta_i$. Hence, $\text{det}(A+B)=\prod_{i=1}^n\zeta_i=\prod_{i=1}^n(\eta_i+\lambda_i)\geq\prod_{i=1}^n\eta_i+\prod_{i=1}^n\lambda_i=\text{det}(A)+\text{det}(B)$. Does anyone see a flaw in this argument? If it is correct, does anyone know perhaps an easier argument?",,"['linear-algebra', 'proof-verification']"
33,Why all vector space have a span set?,Why all vector space have a span set?,,"I thought about this question, but I don't sure if my proof is correct. In the book, he put this question like a observation of span sets' definition, so I tried proof this. My attempt: Suppose that exists a vector space $V$ such that there isn't a span set $S = \{ v_1, v_2, \ldots, v_n \}$, so exists a $v_{n+1} \in V$ such that $v_{n+1} \neq \sum_{i=1}^{i=n} v_i$, so $S \cup \{ v_{n+1} \}$ can be a span set of $V$, but can be exists $v_{n+2} \in V$ such that $v_{n+2} \neq \sum_{i=1}^{i=n+1},  v_i$, so $S \cup \{ v_{n+1}, v_{n+2} \}$ can be a span set of $V$ and we can be in this cycle infinitely. My doubt is what ensures that doesn't exists infinitely many vectors that can be span by a span set?","I thought about this question, but I don't sure if my proof is correct. In the book, he put this question like a observation of span sets' definition, so I tried proof this. My attempt: Suppose that exists a vector space $V$ such that there isn't a span set $S = \{ v_1, v_2, \ldots, v_n \}$, so exists a $v_{n+1} \in V$ such that $v_{n+1} \neq \sum_{i=1}^{i=n} v_i$, so $S \cup \{ v_{n+1} \}$ can be a span set of $V$, but can be exists $v_{n+2} \in V$ such that $v_{n+2} \neq \sum_{i=1}^{i=n+1},  v_i$, so $S \cup \{ v_{n+1}, v_{n+2} \}$ can be a span set of $V$ and we can be in this cycle infinitely. My doubt is what ensures that doesn't exists infinitely many vectors that can be span by a span set?",,"['linear-algebra', 'proof-verification']"
34,Importance of the homogeneity assumption in definition of linear map,Importance of the homogeneity assumption in definition of linear map,,"Let $V$ and $W$ be vector spaces over field $F$.  A function $f: V \rightarrow W$ is said to be linear if for any two vectors $x$ and $y$ in $V$ and any scalar $\alpha\in F$, the following two conditions are satisfied: $f(x + y) = f(x) + f(y)$ $f(\alpha x) = \alpha f(x)$ Let $F$ be a field of real numbers. Is it possible to construct $f$ such that the first condition is satified but not the second one?","Let $V$ and $W$ be vector spaces over field $F$.  A function $f: V \rightarrow W$ is said to be linear if for any two vectors $x$ and $y$ in $V$ and any scalar $\alpha\in F$, the following two conditions are satisfied: $f(x + y) = f(x) + f(y)$ $f(\alpha x) = \alpha f(x)$ Let $F$ be a field of real numbers. Is it possible to construct $f$ such that the first condition is satified but not the second one?",,['linear-algebra']
35,Finding all left inverses of a matrix,Finding all left inverses of a matrix,,"I have to find all left inverses of a matrix $$A = \begin{bmatrix}  2&-1 \\  5 & 3\\   -2& 1 \end{bmatrix}$$ I created a matrix to the left of $A$, $$\begin{bmatrix} a &b  &c \\  d &e &f  \end{bmatrix}  \begin{bmatrix}  2&-1 \\  5 & 3\\   -2& 1 \end{bmatrix} = \begin{bmatrix}  1&0 \\   0&1  \end{bmatrix}$$ and I got the following system of equations: \begin{array} {lcl} 2a+5b-2c & = & 1 \\-a+3b+c & = & 0 \\ 2d+5e-2f & = & 0 \\ -d+3e+f & = & 1 \end{array} After this step, I am unsure how to continue or form those equations into a solvable matrix, and create a left inverse matrix from the answers of these equations.","I have to find all left inverses of a matrix $$A = \begin{bmatrix}  2&-1 \\  5 & 3\\   -2& 1 \end{bmatrix}$$ I created a matrix to the left of $A$, $$\begin{bmatrix} a &b  &c \\  d &e &f  \end{bmatrix}  \begin{bmatrix}  2&-1 \\  5 & 3\\   -2& 1 \end{bmatrix} = \begin{bmatrix}  1&0 \\   0&1  \end{bmatrix}$$ and I got the following system of equations: \begin{array} {lcl} 2a+5b-2c & = & 1 \\-a+3b+c & = & 0 \\ 2d+5e-2f & = & 0 \\ -d+3e+f & = & 1 \end{array} After this step, I am unsure how to continue or form those equations into a solvable matrix, and create a left inverse matrix from the answers of these equations.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'inverse']"
36,L2 Norm of Pseudo-Inverse Relation with Minimum Singular Value,L2 Norm of Pseudo-Inverse Relation with Minimum Singular Value,,"Consider a matrix $A \in\mathbb R^{n\times m}$ with $n>m$. It has full column rank, i.e. $\operatorname{rank}(A)=m$. Its left pseudo-inverse is given by; $$A^{-1}_\text{left}=(A^TA)^{-1}A^T $$ From two different results during my studies, I have realized the following: $$ \|A^{-1}_\text{left}\|_2 = \frac{1}{\sigma_{\min}(A)} $$ just like the case as if $A$ is square invertible matrix. I have seen a similar question , however I couldn't relate the answer with the equality given above. My question is: How can we show that the L2 norm of left pseudo-inverse of $A$ is related to its minimum singular value? Thank you in advance for your help.","Consider a matrix $A \in\mathbb R^{n\times m}$ with $n>m$. It has full column rank, i.e. $\operatorname{rank}(A)=m$. Its left pseudo-inverse is given by; $$A^{-1}_\text{left}=(A^TA)^{-1}A^T $$ From two different results during my studies, I have realized the following: $$ \|A^{-1}_\text{left}\|_2 = \frac{1}{\sigma_{\min}(A)} $$ just like the case as if $A$ is square invertible matrix. I have seen a similar question , however I couldn't relate the answer with the equality given above. My question is: How can we show that the L2 norm of left pseudo-inverse of $A$ is related to its minimum singular value? Thank you in advance for your help.",,"['linear-algebra', 'matrices', 'normed-spaces', 'pseudoinverse']"
37,Zauner's conjecture,Zauner's conjecture,,"The conjecture is as follow: In $\mathbb{C}^{n}$, there exists $\{v_1,\cdots,v_{n^2}\}$ such that the following holds:  $$ \left| \left \langle v_i, v_j \right \rangle \right|  = \begin{cases} 1  & i = j\\ \frac{1}{n+1} & i \ne j\end{cases}$$ I have a prove for when $n = 2$, basically what I did is just assuming without loss of generality that one of the vectors is $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$, and brute force the rest of the vectors. I'm curious where this construction fails when $n \ge 3$, or has the conjecture already been proven? I can't seem to find literature that it has been proven on the Internet though.","The conjecture is as follow: In $\mathbb{C}^{n}$, there exists $\{v_1,\cdots,v_{n^2}\}$ such that the following holds:  $$ \left| \left \langle v_i, v_j \right \rangle \right|  = \begin{cases} 1  & i = j\\ \frac{1}{n+1} & i \ne j\end{cases}$$ I have a prove for when $n = 2$, basically what I did is just assuming without loss of generality that one of the vectors is $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$, and brute force the rest of the vectors. I'm curious where this construction fails when $n \ge 3$, or has the conjecture already been proven? I can't seem to find literature that it has been proven on the Internet though.",,['linear-algebra']
38,Why is this theorem also a proof that matrix multiplication is associative?,Why is this theorem also a proof that matrix multiplication is associative?,,"The author remarks that this theorem, which is basically all about what happens if we compose linear transformations, also gives a proof that matrix multiplication is associative: Let $V$, $W$, and $Z$ be finite-dimensional vector spaces over the field $F$; let $T$ be a linear transformation from $V$ into $W$ and $U$ a linear transformation from $W$ into $Z$. If $\mathfrak{B}$, $\mathfrak{B^{'}}$, and $\mathfrak{B^{''}}$ are ordered basis for the spaces $V$, $W$, $Z$, respectively, if $A$ is the matrix of $T$ relative to the pair $\mathfrak{B}$, $\mathfrak{B^{'}}$, and $B$ is the matrix of $U$ relative to the pair $\mathfrak{B^{'}}$, $\mathfrak{B^{''}}$, then the matrix of the composition $UT$ relative to the pair $\mathfrak{B}$, $\mathfrak{B^{''}}$ is the product matrix $C=BA$. However, I see no reason why that's true...","The author remarks that this theorem, which is basically all about what happens if we compose linear transformations, also gives a proof that matrix multiplication is associative: Let $V$, $W$, and $Z$ be finite-dimensional vector spaces over the field $F$; let $T$ be a linear transformation from $V$ into $W$ and $U$ a linear transformation from $W$ into $Z$. If $\mathfrak{B}$, $\mathfrak{B^{'}}$, and $\mathfrak{B^{''}}$ are ordered basis for the spaces $V$, $W$, $Z$, respectively, if $A$ is the matrix of $T$ relative to the pair $\mathfrak{B}$, $\mathfrak{B^{'}}$, and $B$ is the matrix of $U$ relative to the pair $\mathfrak{B^{'}}$, $\mathfrak{B^{''}}$, then the matrix of the composition $UT$ relative to the pair $\mathfrak{B}$, $\mathfrak{B^{''}}$ is the product matrix $C=BA$. However, I see no reason why that's true...",,['linear-algebra']
39,Symmetric permutation matrix,Symmetric permutation matrix,,"I am trying to prove that an $ n \times n $ permutation matrix $ P $  that is formed by switching two rows of an $ n \times n $ identity matrix will always be symmetric. This is what I am trying to use thus far but I can't quite figure out how to piece it all together: A matrix is symmetric if it is its own transpose. The transpose of the identity matrix is still the identity matrix. Any permutation $ P$  of the identity matrix satisfies $ P(P^T)=I$ (where $ P^T $ is the transpose of $ P$ ). A permutation matrix is always nonsingular and has a determinant of $ \pm 1$ . Basic transpose property: For matrices $ A $ and $ B$ , $       (AB)^T=(B^T)(A^T)$ Any help/advice would be greatly appreciated!","I am trying to prove that an $ n \times n $ permutation matrix $ P $  that is formed by switching two rows of an $ n \times n $ identity matrix will always be symmetric. This is what I am trying to use thus far but I can't quite figure out how to piece it all together: A matrix is symmetric if it is its own transpose. The transpose of the identity matrix is still the identity matrix. Any permutation $ P$  of the identity matrix satisfies $ P(P^T)=I$ (where $ P^T $ is the transpose of $ P$ ). A permutation matrix is always nonsingular and has a determinant of $ \pm 1$ . Basic transpose property: For matrices $ A $ and $ B$ , $       (AB)^T=(B^T)(A^T)$ Any help/advice would be greatly appreciated!",,"['linear-algebra', 'matrices', 'permutation-matrices']"
40,Prove that congruent matrices have the same rank.,Prove that congruent matrices have the same rank.,,Can someone prove that two similar matrices have the same rank? Thanks a lot.,Can someone prove that two similar matrices have the same rank? Thanks a lot.,,['linear-algebra']
41,Row and column operations and matrix similarity,Row and column operations and matrix similarity,,"Take for example the following matrix: $$ A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} $$ The elementary matrix equivalent to changing the first row with the second is $$ E = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}  $$ multiplied from the left. The elementary matrix equivalent to changing the first column with the second is the same matrix $E$ multiplied from the right. After a quick check, I found that $E = E^{-1}$ . Given that, I concluded that: $$ B = \begin{pmatrix} 5 & 4 & 6 \\ 2 & 1 & 3 \\ 8 & 7 &9\end{pmatrix} = EAE^{-1} $$ and therefore $A\sim B$ . Is this comprehensive? Does changing rows and column necessarily make the outcome similar to the original? Thanks","Take for example the following matrix: The elementary matrix equivalent to changing the first row with the second is multiplied from the left. The elementary matrix equivalent to changing the first column with the second is the same matrix multiplied from the right. After a quick check, I found that . Given that, I concluded that: and therefore . Is this comprehensive? Does changing rows and column necessarily make the outcome similar to the original? Thanks","
A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix}
 
E = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix} 
 E E = E^{-1} 
B = \begin{pmatrix} 5 & 4 & 6 \\ 2 & 1 & 3 \\ 8 & 7 &9\end{pmatrix} = EAE^{-1}
 A\sim B","['linear-algebra', 'matrices']"
42,Proving isomorphism between linear maps and matrices,Proving isomorphism between linear maps and matrices,,"Theorem: Let $V$ and $W$ be finite dimensional vectorspaces over the same field $F$, with dimensions $n$ and $m$ respectively. Suppose also that $\beta$ and $\gamma$ are ordered bases for resp. $V$ and $W$. Then the function $\Psi : \mathcal{L}(V, W) \rightarrow M_{m \times n}(F)$, defined as $\Psi(T) = [T]_{\beta}^{\gamma}$ for an arbitrary $T \in \mathcal{L}(V,W)$, is an isomorphism. Attempt at proof: We need to show that it is bijective, and hence an isomorphism. This means that for every $m \times n$-matrix $A$ we need to find an unique linear map $T: V \rightarrow W$ such that $\Psi(T)=A.$  So let $\beta = \left\{v_1, v_2, \ldots, v_n\right\}$ and $\gamma = \left\{w_1, w_2, \ldots, w_m\right\}$ be ordered bases for $V$ and $W$. Then we know already that there exists an unique linear map $T: V \rightarrow W$ such that for $1 \leq j \leq n$ \begin{align*} T(v_j) = \sum_{i=1}^{m} a_{ij} w_i \end{align*} But this means that $[T]_{\beta}^{\gamma} = A$, so $\Psi(T) = A$. Hence $\Psi$ is an isomorphism. Can someone check if my proof is sound and valid? If not, where did I go wrong? Thanks in advance.","Theorem: Let $V$ and $W$ be finite dimensional vectorspaces over the same field $F$, with dimensions $n$ and $m$ respectively. Suppose also that $\beta$ and $\gamma$ are ordered bases for resp. $V$ and $W$. Then the function $\Psi : \mathcal{L}(V, W) \rightarrow M_{m \times n}(F)$, defined as $\Psi(T) = [T]_{\beta}^{\gamma}$ for an arbitrary $T \in \mathcal{L}(V,W)$, is an isomorphism. Attempt at proof: We need to show that it is bijective, and hence an isomorphism. This means that for every $m \times n$-matrix $A$ we need to find an unique linear map $T: V \rightarrow W$ such that $\Psi(T)=A.$  So let $\beta = \left\{v_1, v_2, \ldots, v_n\right\}$ and $\gamma = \left\{w_1, w_2, \ldots, w_m\right\}$ be ordered bases for $V$ and $W$. Then we know already that there exists an unique linear map $T: V \rightarrow W$ such that for $1 \leq j \leq n$ \begin{align*} T(v_j) = \sum_{i=1}^{m} a_{ij} w_i \end{align*} But this means that $[T]_{\beta}^{\gamma} = A$, so $\Psi(T) = A$. Hence $\Psi$ is an isomorphism. Can someone check if my proof is sound and valid? If not, where did I go wrong? Thanks in advance.",,"['linear-algebra', 'proof-verification', 'linear-transformations']"
43,Are all symmetric and skew-symmetric matrices diagonalizable?,Are all symmetric and skew-symmetric matrices diagonalizable?,,"I know from a theorem that every hermitian and skew-hermitian matrix is similar to a diagonal matrix. But, is this fact also true for symmetric and skew-symmetric matrices? And, symmetric matrices have real eigenvalues, what about symmetric matrices that have complex entries?","I know from a theorem that every hermitian and skew-hermitian matrix is similar to a diagonal matrix. But, is this fact also true for symmetric and skew-symmetric matrices? And, symmetric matrices have real eigenvalues, what about symmetric matrices that have complex entries?",,"['linear-algebra', 'matrices']"
44,Is the Square Root of an Inverse Matrix Equal to the Inverse of the Square Root Matrix?,Is the Square Root of an Inverse Matrix Equal to the Inverse of the Square Root Matrix?,,"I know in general that if a matrix $A$ is positive definite, then there exists a (unique?) square root matrix $B$, which is also positive definite, such that $BB=A$. Therefore, suppose $A$ is positive definite. It is invertible, and its inverse is also positive definite. Therefore I know there exists $C$ (possibly unique?) so that $CC=A^{-1}$. For simplicity, I will call $C=A^{-1/2}$, so that in this notation $A^{-1/2}A^{-1/2}=A^{-1}$. My question is, is it also true that $A^{-1/2}AA^{-1/2}=I$? Or is this not necessarily true? I have posted an attempted solution below -- please let me know what you think. Thanks! Edit It occurs to me that at the essence of this question is whether or not $B=C^{-1}$; is this necessarily true? That is, is square root of the inverse of a matrix equal to the inverse of the square root of the matrix? (This also partly depends on whether or not $B$ and $C$ are themselves invertible; would this be true?) Edit 2 There's now a related topic which asks what kind of matrix decomposition we're using here (and the differences between these different approaches to matrix decomposition. That question can be found here .","I know in general that if a matrix $A$ is positive definite, then there exists a (unique?) square root matrix $B$, which is also positive definite, such that $BB=A$. Therefore, suppose $A$ is positive definite. It is invertible, and its inverse is also positive definite. Therefore I know there exists $C$ (possibly unique?) so that $CC=A^{-1}$. For simplicity, I will call $C=A^{-1/2}$, so that in this notation $A^{-1/2}A^{-1/2}=A^{-1}$. My question is, is it also true that $A^{-1/2}AA^{-1/2}=I$? Or is this not necessarily true? I have posted an attempted solution below -- please let me know what you think. Thanks! Edit It occurs to me that at the essence of this question is whether or not $B=C^{-1}$; is this necessarily true? That is, is square root of the inverse of a matrix equal to the inverse of the square root of the matrix? (This also partly depends on whether or not $B$ and $C$ are themselves invertible; would this be true?) Edit 2 There's now a related topic which asks what kind of matrix decomposition we're using here (and the differences between these different approaches to matrix decomposition. That question can be found here .",,"['linear-algebra', 'matrices', 'regression', 'least-squares', 'matrix-decomposition']"
45,"A question about the proof of ""a symmetric matrix has real eigenvalues"".","A question about the proof of ""a symmetric matrix has real eigenvalues"".",,"This question is in reference to this proof of the Spectral Theorem. Let $A$ be a symmetric matrix. Take $u$ to be an orthonormal vector (such that $u^Tu=1$). Then if $u$ is an eigenvector of $A$, then $Au=\lambda u$. This implies that $\lambda=u^TAu$. The article says that this proves that $\lambda$ is real. I don't follow why this is. Both $u$ and $A$ can contain complex entries! Why would $u^TAu$ have to be real?","This question is in reference to this proof of the Spectral Theorem. Let $A$ be a symmetric matrix. Take $u$ to be an orthonormal vector (such that $u^Tu=1$). Then if $u$ is an eigenvector of $A$, then $Au=\lambda u$. This implies that $\lambda=u^TAu$. The article says that this proves that $\lambda$ is real. I don't follow why this is. Both $u$ and $A$ can contain complex entries! Why would $u^TAu$ have to be real?",,['linear-algebra']
46,Solving recurrence relation: Product form,Solving recurrence relation: Product form,,"Please help in finding the solution of this recursion. $$f(n)=\frac{f(n-1) \cdot f(n-2)}{n},$$ where $ f(1)=1$ and $f(2)=2$.","Please help in finding the solution of this recursion. $$f(n)=\frac{f(n-1) \cdot f(n-2)}{n},$$ where $ f(1)=1$ and $f(2)=2$.",,"['linear-algebra', 'sequences-and-series', 'algorithms', 'recurrence-relations', 'recursive-algorithms']"
47,Solving $X+X^T=tr(X)M$,Solving,X+X^T=tr(X)M,"Let $M$ be a $n\times n$ complex matrix. Solve the equation $X+X^T=tr(X)M$ where $X$ is a $n\times n$ complex matrix. I've done some case-checking. Suppose $X$ is a solution. if $tr(X)=0$ , then $X$ is skew-symetric, and any skew-symetric matrix satisfies the equation if $tr(X)\neq 0$ , if $tr(M)\neq 2$ , there's a contradiction. If $tr(M)=2$ , and $M$ is not symetric, we reach a contradiction. if $tr(M)=2$ and $M$ is symetric, I don't know what to say. I've prefered an abstract approach so far, should I start looking at what happens with entries ?","Let be a complex matrix. Solve the equation where is a complex matrix. I've done some case-checking. Suppose is a solution. if , then is skew-symetric, and any skew-symetric matrix satisfies the equation if , if , there's a contradiction. If , and is not symetric, we reach a contradiction. if and is symetric, I don't know what to say. I've prefered an abstract approach so far, should I start looking at what happens with entries ?",M n\times n X+X^T=tr(X)M X n\times n X tr(X)=0 X tr(X)\neq 0 tr(M)\neq 2 tr(M)=2 M tr(M)=2 M,"['linear-algebra', 'matrices']"
48,Does adding two linear equations will result in a line which will pass through an intersection of the linear equations?,Does adding two linear equations will result in a line which will pass through an intersection of the linear equations?,,I was wondering why it is almost impossible to find a geometrical explanation of why adding two linear equations helps us to find a solution of a system of linear equations? Am I right that adding two linear equations will result in an equation of a line which will pass through a point where two linear equations intersect? If it is right then I completely don't understand why such a crucial point in understanding how to solve systems of equations is never properly taught even at a university level.,I was wondering why it is almost impossible to find a geometrical explanation of why adding two linear equations helps us to find a solution of a system of linear equations? Am I right that adding two linear equations will result in an equation of a line which will pass through a point where two linear equations intersect? If it is right then I completely don't understand why such a crucial point in understanding how to solve systems of equations is never properly taught even at a university level.,,"['linear-algebra', 'systems-of-equations']"
49,Lower bound on the minimum eigenvalue of sum of two matrices,Lower bound on the minimum eigenvalue of sum of two matrices,,Assume that $A$ is a symmetric positive definite matrix and $B$ is a symmetric (can potentially have negative entries). Is the following bound correct? $$ \lambda_{\min}(A+B) \geq \lambda_{\min}(A) + \lambda_{\min}(B) $$,Assume that is a symmetric positive definite matrix and is a symmetric (can potentially have negative entries). Is the following bound correct?,A B  \lambda_{\min}(A+B) \geq \lambda_{\min}(A) + \lambda_{\min}(B) ,"['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
50,Trace and determinant of composition of a left-multiplication and a right-multiplication on a space of matrices,Trace and determinant of composition of a left-multiplication and a right-multiplication on a space of matrices,,Determine the trace and determinant of the linear operator (on the space $\mathbb{F^{n\times n}}$) that sends the matrix $M\to AMB$ where $A$ and $B$ are $n\times n$ matricies,Determine the trace and determinant of the linear operator (on the space $\mathbb{F^{n\times n}}$) that sends the matrix $M\to AMB$ where $A$ and $B$ are $n\times n$ matricies,,"['linear-algebra', 'matrices', 'determinant', 'trace']"
51,Prove that the following matrices cannot represent the linear transformation $T$ in ANY basis,Prove that the following matrices cannot represent the linear transformation  in ANY basis,T,"$T: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ defined as $T(x,y,z) = (2x,z,y)$ is a linear transformation. I need to prove that the following matrices cannot represent $T$ in ANY basis: $$\begin{bmatrix} 2 & 0 & 1 \\ 0 & 1 & 0\\ 2 & 0 & 1 \end{bmatrix}$$ $$\begin{bmatrix} 1 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 2 & 1\end{bmatrix}$$ My attempt for the first matrix was to assume (negatively) existence of a basis in which the given matrix is the representation, and left multiplying it by $(x,y,z)^{T}$ to get $(2x+z,y,2x+z)^{T}$. Can I conclude that given it is not in the form $(2x,z,y)^{T}$, there is NO such basis? I want to know if this is correct and, in addition (Or alternatively), learn other ways to solve this exercise. Thank you.","$T: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ defined as $T(x,y,z) = (2x,z,y)$ is a linear transformation. I need to prove that the following matrices cannot represent $T$ in ANY basis: $$\begin{bmatrix} 2 & 0 & 1 \\ 0 & 1 & 0\\ 2 & 0 & 1 \end{bmatrix}$$ $$\begin{bmatrix} 1 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 2 & 1\end{bmatrix}$$ My attempt for the first matrix was to assume (negatively) existence of a basis in which the given matrix is the representation, and left multiplying it by $(x,y,z)^{T}$ to get $(2x+z,y,2x+z)^{T}$. Can I conclude that given it is not in the form $(2x,z,y)^{T}$, there is NO such basis? I want to know if this is correct and, in addition (Or alternatively), learn other ways to solve this exercise. Thank you.",,"['linear-algebra', 'matrices', 'transformation']"
52,"If $A$ is invertible, then the $QR$-factorization is unique if we require that the diagonal elements of $R$ are positive.","If  is invertible, then the -factorization is unique if we require that the diagonal elements of  are positive.",A QR R,"Let $A=QR$ be a $QR$ -decomposition. Then the wikipedia article regarding $QR$ decomposition states that If $A$ is invertible, then the factorization is unique if we require   that the diagonal elements of $R$ are positive. I don't understand the relation of diagonal elements being positive to uniqueness. Can someone explain what happens if they are non-negative, say some of them are zero, and what happens if they are all real.","Let be a -decomposition. Then the wikipedia article regarding decomposition states that If is invertible, then the factorization is unique if we require   that the diagonal elements of are positive. I don't understand the relation of diagonal elements being positive to uniqueness. Can someone explain what happens if they are non-negative, say some of them are zero, and what happens if they are all real.",A=QR QR QR A R,"['linear-algebra', 'matrices']"
53,Calculating the Moore-Penrose pseudoinverse,Calculating the Moore-Penrose pseudoinverse,,"I have a problem with a project requiring me to calculate the Moore-Penrose pseudo inverse. I've also posted about this on StackOverflow, where you can see my progress. From what I understand from Planet Math you can simply compute the pseudoinverse only the first formula which I can understand, but it also says that this is for general cases , and you have to do SVD (singular value decomposition) and the formula becomes much complicated ( the second formula ) which I don't understand... I mean, What is V? What is S? I'm confused. How can I calculate SVD? Can you please help me building the code/algorithm, or just a simple advice? Why there are two pseudo inverse formulas? Left pseudo inverse formula $$A_\text{left} = (A^TA)^{-1}A^T$$ Right pseudo inverse formula $$A_\text{right}=A^T(AA^T)^{-1}$$ Thank you very much, Daniel.","I have a problem with a project requiring me to calculate the Moore-Penrose pseudo inverse. I've also posted about this on StackOverflow, where you can see my progress. From what I understand from Planet Math you can simply compute the pseudoinverse only the first formula which I can understand, but it also says that this is for general cases , and you have to do SVD (singular value decomposition) and the formula becomes much complicated ( the second formula ) which I don't understand... I mean, What is V? What is S? I'm confused. How can I calculate SVD? Can you please help me building the code/algorithm, or just a simple advice? Why there are two pseudo inverse formulas? Left pseudo inverse formula $$A_\text{left} = (A^TA)^{-1}A^T$$ Right pseudo inverse formula $$A_\text{right}=A^T(AA^T)^{-1}$$ Thank you very much, Daniel.",,"['linear-algebra', 'matrices', 'inverse', 'pseudoinverse']"
54,Proof of the finite number of Bravais lattices?,Proof of the finite number of Bravais lattices?,,"I've been taught that there are a finite number of Bravais lattices in 1, 2 and 3 dimensions. I am wondering if there is a proof of this fact. Maybe this is obvious and I am only missing certain key assumptions that are made in setting the problem?","I've been taught that there are a finite number of Bravais lattices in 1, 2 and 3 dimensions. I am wondering if there is a proof of this fact. Maybe this is obvious and I am only missing certain key assumptions that are made in setting the problem?",,"['linear-algebra', 'geometry', 'integer-lattices']"
55,"One dimensional subspaces of a 3 dimensional vector space over $\mathbb Z/3\mathbb Z$, multiple choice","One dimensional subspaces of a 3 dimensional vector space over , multiple choice",\mathbb Z/3\mathbb Z,Let $V$ be a 3-dimentional vector space over the field $F_3=\Bbb Z/3 \Bbb Z$  of $3$ elements.the number of distinct 1 dimentional subspaces of $V$ is $13$ $26$ $9$ $15$,Let $V$ be a 3-dimentional vector space over the field $F_3=\Bbb Z/3 \Bbb Z$  of $3$ elements.the number of distinct 1 dimentional subspaces of $V$ is $13$ $26$ $9$ $15$,,['linear-algebra']
56,An $r\times r$ submatrix of independent rows and independent columns is invertible (Michael Artin's book).,An  submatrix of independent rows and independent columns is invertible (Michael Artin's book).,r\times r,"Let $A$ be an $m \times n$ matrix of rank $r$, let $I$ be a set of row indices such that the corresponding rows of $A$ are independent and let $J$ be a set of $r$ column indices such that the corresponding columns of $A$ are independent.  Let $M$ denote the $r \times r$ submatrix obtained by taking rows from $I$ and columns from $J$.  Then $M$ is invertible. So far I've got: Let $B$ be the $m \times r$ matrix obtain from taking the rows from $J$.  $B$ is row-reducible to having a pivot in each column or else $Ax = 0$ has more than one solution, a contradiction.  Also, the dimension of the row space of $B$ equals $r$ as well.","Let $A$ be an $m \times n$ matrix of rank $r$, let $I$ be a set of row indices such that the corresponding rows of $A$ are independent and let $J$ be a set of $r$ column indices such that the corresponding columns of $A$ are independent.  Let $M$ denote the $r \times r$ submatrix obtained by taking rows from $I$ and columns from $J$.  Then $M$ is invertible. So far I've got: Let $B$ be the $m \times r$ matrix obtain from taking the rows from $J$.  $B$ is row-reducible to having a pivot in each column or else $Ax = 0$ has more than one solution, a contradiction.  Also, the dimension of the row space of $B$ equals $r$ as well.",,"['linear-algebra', 'matrices']"
57,Does assigning a different inner product to a vector space in $\mathbb{R^n}$ change the meaning of the determinant on that space?,Does assigning a different inner product to a vector space in  change the meaning of the determinant on that space?,\mathbb{R^n},"We just started talking about inner product spaces and and how one can assign a different notion of length and angle on a vector space. Since the determinant in $\mathbb{R^n}$ captures the notion of ""$n$-dimensional oriented volume"", does this mean that a different notion of length will give a different meaning to the determinant? Is the ""length"" defined by an inner product even connected to the ""volume"" defined by the determinant?","We just started talking about inner product spaces and and how one can assign a different notion of length and angle on a vector space. Since the determinant in $\mathbb{R^n}$ captures the notion of ""$n$-dimensional oriented volume"", does this mean that a different notion of length will give a different meaning to the determinant? Is the ""length"" defined by an inner product even connected to the ""volume"" defined by the determinant?",,"['linear-algebra', 'determinant', 'inner-products']"
58,Are there non nilpotent operators with spectrum 0?,Are there non nilpotent operators with spectrum 0?,,"If a linear operator on a vector space V is nilpotent, then its spectrum is 0. Makes me wonder, are there also operators with spectrum 0 that are not nilpotent? Necessarily such an operator is not invertible, but I can't construct any examples and was hoping to see one.","If a linear operator on a vector space V is nilpotent, then its spectrum is 0. Makes me wonder, are there also operators with spectrum 0 that are not nilpotent? Necessarily such an operator is not invertible, but I can't construct any examples and was hoping to see one.",,"['linear-algebra', 'operator-theory']"
59,Torsion-free divisible group,Torsion-free divisible group,,"I have some confusion. Let $G$ be a torsion-free divisible abelian group. Then, $G$ is a $\mathbb Q$ -vector space. If $G$ has a finite dimension as a vector space then can we write $G$ as a finite direct sum of copies of $\mathbb Q$ ?","I have some confusion. Let be a torsion-free divisible abelian group. Then, is a -vector space. If has a finite dimension as a vector space then can we write as a finite direct sum of copies of ?",G G \mathbb Q G G \mathbb Q,"['linear-algebra', 'group-theory']"
60,"Prove that $\{\vec x_i\}\subset\mathbb R^d$ is affinely independent iff $\{(1,\vec x_i)\}$ is linearly independent",Prove that  is affinely independent iff  is linearly independent,"\{\vec x_i\}\subset\mathbb R^d \{(1,\vec x_i)\}","I guess I'm having some trouble getting my head around the notion of affine independence.  As I've been taught, a set of vectors $\{\vec{x_1},\ldots,\vec{x_n}\}\subset \mathbb{R}^d$ is affinely independent if its affine hull has dimension $n-1$ . I want instead to think about linearity in $\mathbb{R}^{d+1}$ , and I'm told I can. The claim is that my set of vectors is affinely independent if and only if $\{\hat{x_i}\}$ is linearly independent, where $\hat{x_i}=(1,\vec{x_i})$ .  Any tips for proving such a thing?","I guess I'm having some trouble getting my head around the notion of affine independence.  As I've been taught, a set of vectors is affinely independent if its affine hull has dimension . I want instead to think about linearity in , and I'm told I can. The claim is that my set of vectors is affinely independent if and only if is linearly independent, where .  Any tips for proving such a thing?","\{\vec{x_1},\ldots,\vec{x_n}\}\subset \mathbb{R}^d n-1 \mathbb{R}^{d+1} \{\hat{x_i}\} \hat{x_i}=(1,\vec{x_i})","['linear-algebra', 'affine-geometry', 'convex-geometry']"
61,Cauchy-Schwarz for metrics with arbitrary signatures,Cauchy-Schwarz for metrics with arbitrary signatures,,"When the norm of a vector is always greater than or equal to zero, the Cauchy-Schwarz inequality holds, but what if we look at a metric with an arbitrary signature? Then the inner product of a vector with itself could be negative. Is there any Cauchy-Schwarz inequality for an arbitrary metric? I suspect it would look something like this: $$g_{ij}^2\leq |g_{ii}g_{jj}|,$$ but I am not sure if that is necessarily the case (let alone how to go about proving it). Here, $g$ is the matrix representation of the metric tensor.","When the norm of a vector is always greater than or equal to zero, the Cauchy-Schwarz inequality holds, but what if we look at a metric with an arbitrary signature? Then the inner product of a vector with itself could be negative. Is there any Cauchy-Schwarz inequality for an arbitrary metric? I suspect it would look something like this: $$g_{ij}^2\leq |g_{ii}g_{jj}|,$$ but I am not sure if that is necessarily the case (let alone how to go about proving it). Here, $g$ is the matrix representation of the metric tensor.",,"['linear-algebra', 'metric-spaces', 'normed-spaces', 'inner-products', 'general-relativity']"
62,Smallest eigenvalues of Sum of Two Positive Matrices,Smallest eigenvalues of Sum of Two Positive Matrices,,"Let $C = A + B$, where $A$, $B$, and $C$ are positive definite matrices. In addition, $C$ is fixed. Let $\lambda (A)$, $\lambda (B)$, and $\lambda (C)$ be smallest eigenvalues of $A$, $B$, and $C$, respectively. Is there any result about the smallest eigenvalues of $C$ in comparison with the sum of smallest eigenvalues of $A$ and $B$? Is it true that : $\lambda (A)$ + $\lambda (B)$ < $\lambda (C)$ ? Moreover, what is the smallest possible value of $\lambda (A)$ + $\lambda (B)$ given a fixed $C$, and under what condition does this happen?  Many thanks! Xuan ------------------------------ Post Edit --------------------------- Question about $\lambda_{min} (A+B) > \lambda_{min} (A) + \lambda_{min} (B) $ can be seen from Weyl's inequality. The remaining question is about the smallest attainable value of $\lambda_{min} (A) + \lambda_{min} (B) $ given a fixed $C$?","Let $C = A + B$, where $A$, $B$, and $C$ are positive definite matrices. In addition, $C$ is fixed. Let $\lambda (A)$, $\lambda (B)$, and $\lambda (C)$ be smallest eigenvalues of $A$, $B$, and $C$, respectively. Is there any result about the smallest eigenvalues of $C$ in comparison with the sum of smallest eigenvalues of $A$ and $B$? Is it true that : $\lambda (A)$ + $\lambda (B)$ < $\lambda (C)$ ? Moreover, what is the smallest possible value of $\lambda (A)$ + $\lambda (B)$ given a fixed $C$, and under what condition does this happen?  Many thanks! Xuan ------------------------------ Post Edit --------------------------- Question about $\lambda_{min} (A+B) > \lambda_{min} (A) + \lambda_{min} (B) $ can be seen from Weyl's inequality. The remaining question is about the smallest attainable value of $\lambda_{min} (A) + \lambda_{min} (B) $ given a fixed $C$?",,"['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors', 'summation']"
63,Explanation of a cross product result,Explanation of a cross product result,,"In my book the result $$(u\times v)\cdot(x\times y)=\begin{vmatrix} u\cdot x & v\cdot x \\u \cdot y & v \cdot y\end{vmatrix},$$ where u, v, x and y are arbitrary vectors, is stated (here '$\cdot$' means the dot product and '$\times$' is the cross product). The book very briefly says that this can be easily done by observing that both sides are linear in u, v, x and y. I know that if I expand and simply the LHS using the components of a vector the result will be true. However, I don't really understand what it means when the book says ' both sides are linear in u, v, x and y ' and how by noticing this fact, makes this relation easier to prove. Any help will be greatly appreciated.","In my book the result $$(u\times v)\cdot(x\times y)=\begin{vmatrix} u\cdot x & v\cdot x \\u \cdot y & v \cdot y\end{vmatrix},$$ where u, v, x and y are arbitrary vectors, is stated (here '$\cdot$' means the dot product and '$\times$' is the cross product). The book very briefly says that this can be easily done by observing that both sides are linear in u, v, x and y. I know that if I expand and simply the LHS using the components of a vector the result will be true. However, I don't really understand what it means when the book says ' both sides are linear in u, v, x and y ' and how by noticing this fact, makes this relation easier to prove. Any help will be greatly appreciated.",,"['linear-algebra', 'determinant', 'cross-product']"
64,Computing very high powers of a particular Jordan block,Computing very high powers of a particular Jordan block,,"Let $J$ be the following $k-by-k$ Jordan block: $$ J:= \begin{bmatrix} e^{i \theta} & 1 & \\  & e^{i \theta} & 1 \\  & & \ddots & \ddots \\  & & & \ddots & 1 \\  & & & & e^{i \theta} \end{bmatrix}. $$ Is there an efficient way to compute the action of large powers of $J$, ie: $$J^n x$$ to within tolerance $\epsilon$, when $n$ is extremely large? Notes, The goal is to find a computational procedure $f_n$, so that $||f_n(x)-J^nx||<\epsilon$ for all $x$, and computing $f_n(x)$ takes (significantly) less work than iteratively applying $J$ over and over. Feel free to make $n$ depend on $k$ and $\epsilon$ and let $n$ it be as large as you want - I'm interested in the asymptotic behavior. $\theta/2\pi$ could be irrational","Let $J$ be the following $k-by-k$ Jordan block: $$ J:= \begin{bmatrix} e^{i \theta} & 1 & \\  & e^{i \theta} & 1 \\  & & \ddots & \ddots \\  & & & \ddots & 1 \\  & & & & e^{i \theta} \end{bmatrix}. $$ Is there an efficient way to compute the action of large powers of $J$, ie: $$J^n x$$ to within tolerance $\epsilon$, when $n$ is extremely large? Notes, The goal is to find a computational procedure $f_n$, so that $||f_n(x)-J^nx||<\epsilon$ for all $x$, and computing $f_n(x)$ takes (significantly) less work than iteratively applying $J$ over and over. Feel free to make $n$ depend on $k$ and $\epsilon$ and let $n$ it be as large as you want - I'm interested in the asymptotic behavior. $\theta/2\pi$ could be irrational",,"['linear-algebra', 'matrices', 'asymptotics', 'numerical-linear-algebra']"
65,Linear dependence of linear functionals,Linear dependence of linear functionals,,"Problem: Let V be a vector space over a field F and let $\alpha$ and $\beta$ be linear functionals on $V$. If $\ker(\beta)\subset\ker(\alpha)$, show $\alpha = k\beta$, for some $k\in F$. A proposed solution is in the answers below.","Problem: Let V be a vector space over a field F and let $\alpha$ and $\beta$ be linear functionals on $V$. If $\ker(\beta)\subset\ker(\alpha)$, show $\alpha = k\beta$, for some $k\in F$. A proposed solution is in the answers below.",,['linear-algebra']
66,Unitary orbit of the Jordan matrices,Unitary orbit of the Jordan matrices,,"Let $$ \mathcal{J}=\{A\in M_n(\mathbb{C}):\ A \text{ is a Jordan matrix}\} $$ Then it is well-known that the similary orbit of $\mathcal{J}$ is all of $M_n(\mathbb{C})$ . What is the unitary orbit of $\mathcal{J}$ ? Is it dense? It cannot be all of $M_n(\mathbb{C})$ , because every matrix in $\mathcal{J}$ and its unitary conjugates have the property that eigenvectors corresponding to different eigenvalues are orthogonal to each other.","Let Then it is well-known that the similary orbit of is all of . What is the unitary orbit of ? Is it dense? It cannot be all of , because every matrix in and its unitary conjugates have the property that eigenvectors corresponding to different eigenvalues are orthogonal to each other.","
\mathcal{J}=\{A\in M_n(\mathbb{C}):\ A \text{ is a Jordan matrix}\}
 \mathcal{J} M_n(\mathbb{C}) \mathcal{J} M_n(\mathbb{C}) \mathcal{J}","['linear-algebra', 'functional-analysis']"
67,Define two differents vector space structures over a field on an abelian group,Define two differents vector space structures over a field on an abelian group,,"Exercise 3 from Roman's book ""Advanced Linear Algebra"". The author asks us to ""find an abelian group $V$ and a field $\mathbb{F}$ for which $V$ is a vector space over $\mathbb{F}$ in at least two different ways, that is, there are two different definitions of scalar multiplications making $V$ a vector space over $\mathbb{F}$."" I would appreciate any hint in order to solve this question.","Exercise 3 from Roman's book ""Advanced Linear Algebra"". The author asks us to ""find an abelian group $V$ and a field $\mathbb{F}$ for which $V$ is a vector space over $\mathbb{F}$ in at least two different ways, that is, there are two different definitions of scalar multiplications making $V$ a vector space over $\mathbb{F}$."" I would appreciate any hint in order to solve this question.",,['linear-algebra']
68,Proving: $2n (\sum_{i=1}^{2n} a_i^2) \geq (\sum_{i=1}^{2n} a_i)^2+(\sum_{i=1}^{2n} a_i (-1)^i)^2$,Proving:,2n (\sum_{i=1}^{2n} a_i^2) \geq (\sum_{i=1}^{2n} a_i)^2+(\sum_{i=1}^{2n} a_i (-1)^i)^2,"Let $a_i\in \mathbb{R}$ and $n$ an integer. How do you prove: $$2n \left(\sum_{i=1}^{2n} a_i^2\right) \geq \left(\sum_{i=1}^{2n} a_i\right)^2+\left(\sum_{i=1}^{2n} a_i (-1)^i\right)^2?$$ This is how far I have got. I said let V be a inner product space with orthonormal basis $\{u_1, \dots ,u_{2n}\}$ and let, $u,v \in V$ and $u= \sum_{i=1}^{2n}a_iu_i$ and $v= \sum_{i=1}^{2n}a_iu_i$. Therefore as $$\langle u,v \rangle = \sum_{i=1}^{2n} a_ib_i,\quad \langle u,u \rangle = \sum_{i=1}^{2n} a_i^2,\quad \langle v,v \rangle = \sum_{i=1}^{2n} b_i^2,$$ and from the Cauchy–Schwarz Inequality $|\langle u,v \rangle|^2 \leq \langle u,u \rangle\langle v,v \rangle$ we know that $$\left(\sum_{i=1}^{2n} a_i^2\right)\left(\sum_{i=1}^{2n} b_i^2\right) \geq \left(\sum_{i=1}^{2n} a_ib_i\right)^2$$ So letting $b_i=1$ for all $i$ we see $$2n\left(\sum_{i=1}^{2n} a_i^2\right) \geq \left(\sum_{i=1}^{2n} a_i\right)^2$$ Clearly though $(\sum_{i=1}^{2n} a_i (-1)^i)^2 \geq 0$ so we can't simply move on from this step, I feel I need to back up a bit to continue, but i've got a bit lost, if anyone could help it'd be great! EDIT: Ok, so let $b_i=(-1)^i$ then $$\left(\sum_{i=1}^{2n} a_i^2\right)\left(\sum_{i=1}^{2n} ((-1)^2)^i\right) \geq \left(\sum_{i=1}^{2n} a_i(-1)^i\right)^2$$ $$\Rightarrow 2n\left(\sum_{i=1}^{2n} a_i^2\right) \geq \left(\sum_{i=1}^{2n} a_i(-1)^i\right)^2$$ $$\Rightarrow 4n\left(\sum_{i=1}^{2n} a_i^2\right) \geq \left(\sum_{i=1}^{2n} a_i\right)^2 + \left(\sum_{i=1}^{2n} a_i(-1)^i\right)^2.$$ Maybe the upper indices on the summation signs for $2n$ are typos in the question? Maybe they were meant to be $n$, then this holds?","Let $a_i\in \mathbb{R}$ and $n$ an integer. How do you prove: $$2n \left(\sum_{i=1}^{2n} a_i^2\right) \geq \left(\sum_{i=1}^{2n} a_i\right)^2+\left(\sum_{i=1}^{2n} a_i (-1)^i\right)^2?$$ This is how far I have got. I said let V be a inner product space with orthonormal basis $\{u_1, \dots ,u_{2n}\}$ and let, $u,v \in V$ and $u= \sum_{i=1}^{2n}a_iu_i$ and $v= \sum_{i=1}^{2n}a_iu_i$. Therefore as $$\langle u,v \rangle = \sum_{i=1}^{2n} a_ib_i,\quad \langle u,u \rangle = \sum_{i=1}^{2n} a_i^2,\quad \langle v,v \rangle = \sum_{i=1}^{2n} b_i^2,$$ and from the Cauchy–Schwarz Inequality $|\langle u,v \rangle|^2 \leq \langle u,u \rangle\langle v,v \rangle$ we know that $$\left(\sum_{i=1}^{2n} a_i^2\right)\left(\sum_{i=1}^{2n} b_i^2\right) \geq \left(\sum_{i=1}^{2n} a_ib_i\right)^2$$ So letting $b_i=1$ for all $i$ we see $$2n\left(\sum_{i=1}^{2n} a_i^2\right) \geq \left(\sum_{i=1}^{2n} a_i\right)^2$$ Clearly though $(\sum_{i=1}^{2n} a_i (-1)^i)^2 \geq 0$ so we can't simply move on from this step, I feel I need to back up a bit to continue, but i've got a bit lost, if anyone could help it'd be great! EDIT: Ok, so let $b_i=(-1)^i$ then $$\left(\sum_{i=1}^{2n} a_i^2\right)\left(\sum_{i=1}^{2n} ((-1)^2)^i\right) \geq \left(\sum_{i=1}^{2n} a_i(-1)^i\right)^2$$ $$\Rightarrow 2n\left(\sum_{i=1}^{2n} a_i^2\right) \geq \left(\sum_{i=1}^{2n} a_i(-1)^i\right)^2$$ $$\Rightarrow 4n\left(\sum_{i=1}^{2n} a_i^2\right) \geq \left(\sum_{i=1}^{2n} a_i\right)^2 + \left(\sum_{i=1}^{2n} a_i(-1)^i\right)^2.$$ Maybe the upper indices on the summation signs for $2n$ are typos in the question? Maybe they were meant to be $n$, then this holds?",,"['linear-algebra', 'inner-products']"
69,Find the equation of the plane passing through a point and a vector orthogonal,Find the equation of the plane passing through a point and a vector orthogonal,,"I have come across this question that I need a tip for. Find the equation (general form) of the plane passing through the point $P(3,1,6)$ that is orthogonal to the vector $v=(1,7,-2)$. I would be able to do this if it said ""parallel to the vector"" I would set the equation up as $(x,y,x) = (3,1,6) + t(1,7,-2)$ and go from there. I don't get where I can get an orthogonal vector. Normally when I am finding an orthogonal vector I have two other vectors and do the cross product to find it. I am thinking somehow I have to get three points on the plane, but I'm not sure how to go about doing that. Any pointers? thanks in advance.","I have come across this question that I need a tip for. Find the equation (general form) of the plane passing through the point $P(3,1,6)$ that is orthogonal to the vector $v=(1,7,-2)$. I would be able to do this if it said ""parallel to the vector"" I would set the equation up as $(x,y,x) = (3,1,6) + t(1,7,-2)$ and go from there. I don't get where I can get an orthogonal vector. Normally when I am finding an orthogonal vector I have two other vectors and do the cross product to find it. I am thinking somehow I have to get three points on the plane, but I'm not sure how to go about doing that. Any pointers? thanks in advance.",,"['linear-algebra', 'vector-spaces', 'parametric']"
70,"Prove $\|T(x)\| = \|x\|$ iff $\langle T(x),T(y)\rangle = \langle x,y\rangle$",Prove  iff,"\|T(x)\| = \|x\| \langle T(x),T(y)\rangle = \langle x,y\rangle","Let T be a linear operator on an inner product space $V$ . Prove that $\lVert T(x)\rVert = \lVert x\rVert$ for all $x$ in $V$ iff $\langle T(x), T(y)\rangle = \langle x,y\rangle\;\forall x,y\in V$ . I tried using $\lVert T(x-y)\rVert = \lVert x-y\rVert$ and I got stuck when I got to $\langle T(x),T(y)\rangle + \langle T(y),T(x)\rangle = \langle x,y\rangle  + \langle y,x\rangle $ Please give me some idea how to proceed from there or is there any other way to prove this? Thank you.",Let T be a linear operator on an inner product space . Prove that for all in iff . I tried using and I got stuck when I got to Please give me some idea how to proceed from there or is there any other way to prove this? Thank you.,"V \lVert T(x)\rVert = \lVert x\rVert x V \langle T(x), T(y)\rangle = \langle x,y\rangle\;\forall x,y\in V \lVert T(x-y)\rVert = \lVert x-y\rVert \langle T(x),T(y)\rangle + \langle T(y),T(x)\rangle = \langle x,y\rangle  + \langle y,x\rangle ","['linear-algebra', 'inner-products']"
71,Error in argument regarding the Cayley Hamilton theorem,Error in argument regarding the Cayley Hamilton theorem,,"I cannot spot the mistake in the following argument regarding the Cayley Hamilton theorem: Let $A\in M_n$, then, $$\begin{align*} P_A(t)&=\det(tI-A)\\ &\implies P_A(A)=\det(AI-A)\\ &\implies P_A(A)=\det(0)\\ &\implies P_A(A)=0 \end{align*}$$","I cannot spot the mistake in the following argument regarding the Cayley Hamilton theorem: Let $A\in M_n$, then, $$\begin{align*} P_A(t)&=\det(tI-A)\\ &\implies P_A(A)=\det(AI-A)\\ &\implies P_A(A)=\det(0)\\ &\implies P_A(A)=0 \end{align*}$$",,"['linear-algebra', 'matrices']"
72,Is the determinant of a zero divisor zero?,Is the determinant of a zero divisor zero?,,"Suppose that $A$ is a zero divisor in the ring of $(n\times n)$-matrices over the ring $R$. Is $\det(A) =0$ if $R$ is a field? Is $\det(A) =0$ if $R$ is an integral domain? It's not necessarily true if $R$ has zero-divisors. Take for example $R=\mathbf{Z}/2^{n+1}\mathbf{Z}$ and $A=\textrm{diag}(2,2,\ldots,2)$. Then $A^{n+1}=0$ but $\det A = 2^n \neq 0$. Of course, if $R$ is a reduced ring and $A$ is nilpotent, we have that $\det(A) =0$. In fact, $A^m=0$ for some $m>0$. Thus, $\det(A)^m = \det(A^m) =0$. Thus $\det(A) $ is nilpotent in $R$. Therefore, we have that $\det(A) =0$.","Suppose that $A$ is a zero divisor in the ring of $(n\times n)$-matrices over the ring $R$. Is $\det(A) =0$ if $R$ is a field? Is $\det(A) =0$ if $R$ is an integral domain? It's not necessarily true if $R$ has zero-divisors. Take for example $R=\mathbf{Z}/2^{n+1}\mathbf{Z}$ and $A=\textrm{diag}(2,2,\ldots,2)$. Then $A^{n+1}=0$ but $\det A = 2^n \neq 0$. Of course, if $R$ is a reduced ring and $A$ is nilpotent, we have that $\det(A) =0$. In fact, $A^m=0$ for some $m>0$. Thus, $\det(A)^m = \det(A^m) =0$. Thus $\det(A) $ is nilpotent in $R$. Therefore, we have that $\det(A) =0$.",,"['linear-algebra', 'matrices', 'ring-theory', 'determinant']"
73,Orthogonal basis for $\operatorname{Tr}(AB)$,Orthogonal basis for,\operatorname{Tr}(AB),"I recently stumbled across this bilinear form: $\beta(A,B)=\operatorname{Tr}(AB)$ for $A,B \in \mathbb{R}^{n,n}$. I am searching for an orthogonal basis. With many difficulties I could find one for $\mathbb{R}^{2,2}$ namely: $$B_{2,2}=\left\{\left( \begin{array}{cc}  \frac{1}{\sqrt{2}} & 0 \\  0 & \frac{1}{\sqrt{2}} \end{array} \right),\left( \begin{array}{cc}  0 & \frac{1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}} & 0 \end{array} \right),\left( \begin{array}{cc}  \frac{1}{\sqrt{2}} & 0 \\  0 & -\frac{1}{\sqrt{2}} \end{array} \right),\left( \begin{array}{cc}  0 & -\frac{1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}} & 0 \end{array} \right)\right\}$$ This form was also made that the representation matrix of $\beta$ is very elegant: $$M_\beta=\left( \begin{array}{cccc}  \beta \left(b_1,b_1\right) & \beta \left(b_1,b_2\right) & \beta \left(b_1,b_3\right)    & \beta \left(b_1,b_4\right) \\  \beta \left(b_2,b_1\right) & \beta \left(b_2,b_2\right) & \beta \left(b_2,b_3\right)    & \beta \left(b_2,b_4\right) \\  \beta \left(b_3,b_1\right) & \beta \left(b_3,b_2\right) & \beta \left(b_3,b_3\right)    & \beta \left(b_3,b_4\right) \\  \beta \left(b_4,b_1\right) & \beta \left(b_4,b_2\right) & \beta \left(b_4,b_3\right)    & \beta \left(b_4,b_4\right) \end{array} \right)=\left( \begin{array}{cccc}  1 & 0 & 0 & 0 \\  0 & 1 & 0 & 0 \\  0 & 0 & 1 & 0 \\  0 & 0 & 0 & -1 \end{array} \right)$$ So I can read the positive index of inertia (basically the number of 1s on the diagonal) which is in this case $n_+=3$. I am looking for such bases for higher dimensions of $\mathbb{R}^{n,n}$ but could not succeed. Thank you in advance.","I recently stumbled across this bilinear form: $\beta(A,B)=\operatorname{Tr}(AB)$ for $A,B \in \mathbb{R}^{n,n}$. I am searching for an orthogonal basis. With many difficulties I could find one for $\mathbb{R}^{2,2}$ namely: $$B_{2,2}=\left\{\left( \begin{array}{cc}  \frac{1}{\sqrt{2}} & 0 \\  0 & \frac{1}{\sqrt{2}} \end{array} \right),\left( \begin{array}{cc}  0 & \frac{1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}} & 0 \end{array} \right),\left( \begin{array}{cc}  \frac{1}{\sqrt{2}} & 0 \\  0 & -\frac{1}{\sqrt{2}} \end{array} \right),\left( \begin{array}{cc}  0 & -\frac{1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}} & 0 \end{array} \right)\right\}$$ This form was also made that the representation matrix of $\beta$ is very elegant: $$M_\beta=\left( \begin{array}{cccc}  \beta \left(b_1,b_1\right) & \beta \left(b_1,b_2\right) & \beta \left(b_1,b_3\right)    & \beta \left(b_1,b_4\right) \\  \beta \left(b_2,b_1\right) & \beta \left(b_2,b_2\right) & \beta \left(b_2,b_3\right)    & \beta \left(b_2,b_4\right) \\  \beta \left(b_3,b_1\right) & \beta \left(b_3,b_2\right) & \beta \left(b_3,b_3\right)    & \beta \left(b_3,b_4\right) \\  \beta \left(b_4,b_1\right) & \beta \left(b_4,b_2\right) & \beta \left(b_4,b_3\right)    & \beta \left(b_4,b_4\right) \end{array} \right)=\left( \begin{array}{cccc}  1 & 0 & 0 & 0 \\  0 & 1 & 0 & 0 \\  0 & 0 & 1 & 0 \\  0 & 0 & 0 & -1 \end{array} \right)$$ So I can read the positive index of inertia (basically the number of 1s on the diagonal) which is in this case $n_+=3$. I am looking for such bases for higher dimensions of $\mathbb{R}^{n,n}$ but could not succeed. Thank you in advance.",,"['linear-algebra', 'matrices']"
74,two point line form in 3d,two point line form in 3d,,the two-point form for a line in 2d is  $$y-y_1 = \left(\frac{y_2-y_1}{x_2-x_1}\right)(x-x_1);$$  what is it for 3d lines/planes?,the two-point form for a line in 2d is  $$y-y_1 = \left(\frac{y_2-y_1}{x_2-x_1}\right)(x-x_1);$$  what is it for 3d lines/planes?,,"['linear-algebra', 'geometry', 'euclidean-geometry']"
75,Why is this not an isomorphism?,Why is this not an isomorphism?,,"Let $T(f(t))$= $\begin{bmatrix} f(0) & f(1)\\  f(2) & f(3) \end{bmatrix}$ from $P_2$ to $\mathbb{R}^{2\times 2}$. To show that it is not an isomorphism, I need to show that either kernel of the transformation is not equal to the zero element only, or that the image is not the whole target space. I am struggling in showing that either of these is false, dealing with polynomials in transformations is very counter-intuitive. Thanks for help!","Let $T(f(t))$= $\begin{bmatrix} f(0) & f(1)\\  f(2) & f(3) \end{bmatrix}$ from $P_2$ to $\mathbb{R}^{2\times 2}$. To show that it is not an isomorphism, I need to show that either kernel of the transformation is not equal to the zero element only, or that the image is not the whole target space. I am struggling in showing that either of these is false, dealing with polynomials in transformations is very counter-intuitive. Thanks for help!",,[]
76,Need to compute values of all the entries of a $3 \times 3$ matrix?,Need to compute values of all the entries of a  matrix?,3 \times 3,"I have a Image processing application in which I have a Matrix equation as below: $A \cdot R=I$ where, $A = 3 \times 3$ matrix (Constants) $R = 3 \times 1$ matrix (Column Vector) Lets call this as actual output. $I = 3 \times 1$ matrix . Lets call $I$ as Ideal output I know the values of matrix $I$, and $R$. I have to find what matrix $A$, if post multiplied by $R$ would give me matrix $I$. How can I set his situation up in matrix algebra and solve it compute $A$? Any pointers would be helpful. Thank You. -AD.","I have a Image processing application in which I have a Matrix equation as below: $A \cdot R=I$ where, $A = 3 \times 3$ matrix (Constants) $R = 3 \times 1$ matrix (Column Vector) Lets call this as actual output. $I = 3 \times 1$ matrix . Lets call $I$ as Ideal output I know the values of matrix $I$, and $R$. I have to find what matrix $A$, if post multiplied by $R$ would give me matrix $I$. How can I set his situation up in matrix algebra and solve it compute $A$? Any pointers would be helpful. Thank You. -AD.",,"['linear-algebra', 'matrices']"
77,Is there such a thing as a vector without a basis?,Is there such a thing as a vector without a basis?,,"My professor claims that there is no such thing as a vector without a basis, but I claim that there exists ""raw"" vectors. For example, let's say you have a basis in $\mathbb{R}$ : $$\beta = \{\langle 1, 0 \rangle, \langle 0, 1 \rangle\}$$ Then the vectors in $\beta$ , are they with respect to a basis, or are they ""raw"" vectors? Perhaps my terminology is not correct, but I hope my meaning is clear, that vectors can exist without a basis, for example $m \in M_{2 \times 2}(\mathbb{F})$ . I think you would write it as a coordinate vector if it were in terms of a basis, and as a matrix in ""raw"" format.","My professor claims that there is no such thing as a vector without a basis, but I claim that there exists ""raw"" vectors. For example, let's say you have a basis in : Then the vectors in , are they with respect to a basis, or are they ""raw"" vectors? Perhaps my terminology is not correct, but I hope my meaning is clear, that vectors can exist without a basis, for example . I think you would write it as a coordinate vector if it were in terms of a basis, and as a matrix in ""raw"" format.","\mathbb{R} \beta = \{\langle 1, 0 \rangle, \langle 0, 1 \rangle\} \beta m \in M_{2 \times 2}(\mathbb{F})","['linear-algebra', 'matrices', 'vector-spaces', 'vectors']"
78,"Mutually independent vectors with small, integer coefficients","Mutually independent vectors with small, integer coefficients",,"Consider a family $\cal V$ of $n$ vectors $v_1,\ldots,v_n$ in ${\mathbb N}^3$ (here ${\mathbb N}$ denotes the set of positive integers, excluding $0$ ). Say that $\cal V$ is strongly independent if any three vectors $v_i,v_j,v_k$ in $\cal V$ with $i<j<k$ are linearly independent. Also, define the size of $\cal V$ to be $\max_{v\in {\cal V}} ||v||_{\infty}$ , where $||(x,y,z)||_{\infty}=\max(|x|,|y|,|z|)$ . Using a ""Van der Monde"" construction with $v_k=(1,k,k^2)$ , one achieves a strongly independent family with size $n^2$ . Question. For every $n\geq 3$ , is there a strongly independent family with size at most $n$ ? The answer is yes for $n\leq 9$ , by considering the family $v_k=(1,k,a_k)$ where $a_k$ is the $k$ -th element in $1, 2, 1, 2, 4, 3, 4, 3, 8$ .","Consider a family of vectors in (here denotes the set of positive integers, excluding ). Say that is strongly independent if any three vectors in with are linearly independent. Also, define the size of to be , where . Using a ""Van der Monde"" construction with , one achieves a strongly independent family with size . Question. For every , is there a strongly independent family with size at most ? The answer is yes for , by considering the family where is the -th element in .","\cal V n v_1,\ldots,v_n {\mathbb N}^3 {\mathbb N} 0 \cal V v_i,v_j,v_k \cal V i<j<k \cal V \max_{v\in {\cal V}} ||v||_{\infty} ||(x,y,z)||_{\infty}=\max(|x|,|y|,|z|) v_k=(1,k,k^2) n^2 n\geq 3 n n\leq 9 v_k=(1,k,a_k) a_k k 1, 2, 1, 2, 4, 3, 4, 3, 8","['linear-algebra', 'arithmetic', 'examples-counterexamples']"
79,Duality of linear independence and span,Duality of linear independence and span,,"I have observed the following phenomenon in linear algebra. Let $V$ , $W$ be two vector spaces over a field $F$ . Let $T:V\to W$ be a linear map. On object level, we have If $S'\subseteq S\subseteq V$ subsets, then $S$ is a linearly independent set $\implies$ $S'$ is a linearly independent set. $S'$ is a spanning set $\implies$ $S$ is a spanning set. On morphism level, we have If $S\subseteq V$ is a subset, then If $T$ is injective on $S$ , then $T(S)$ is a linearly independent set $\implies$ $S$ is a linearly independent set. If $T$ is surjective, then $S$ is a spanning set $\implies$ $T(S)$ is a spanning set. I have a strong feeling linear independence is ""dual"" to span, but cannot find a categorical formulation about this anywhere. Is there a systematic way to formulate this phenomenon in category theory? Thanks a lot in advance.","I have observed the following phenomenon in linear algebra. Let , be two vector spaces over a field . Let be a linear map. On object level, we have If subsets, then is a linearly independent set is a linearly independent set. is a spanning set is a spanning set. On morphism level, we have If is a subset, then If is injective on , then is a linearly independent set is a linearly independent set. If is surjective, then is a spanning set is a spanning set. I have a strong feeling linear independence is ""dual"" to span, but cannot find a categorical formulation about this anywhere. Is there a systematic way to formulate this phenomenon in category theory? Thanks a lot in advance.",V W F T:V\to W S'\subseteq S\subseteq V S \implies S' S' \implies S S\subseteq V T S T(S) \implies S T S \implies T(S),"['linear-algebra', 'vector-spaces', 'category-theory', 'linear-transformations']"
80,FInding the number of eigenvalues of the given $\mathbb{C}$ linear transformation.,FInding the number of eigenvalues of the given  linear transformation.,\mathbb{C},"$T: \mathbb{C}[x] → \mathbb{C}[x]$ be the $\mathbb{C}$ -linear transformation defined on the complex vector space $\mathbb{C}[x]$ of one variable complex polynomials by $T (f(x)) = f(x + 1)$ . How many eigenvalues does T have? The basis of $\mathbb{C}[x]$ over $\mathbb{C}$ are $\{1,x,x^2,\cdots , x^{n} ,\cdots\}$ . Then, $$T(1) = 1$$ $$T(x)= x+1 $$ $$T(x^2) = (x+1)^2 = x^2 + 2x + 1$$ $$T(x^n) = (x+1)^n = x^n +  (n_{C_1})x^{n-1}+ \cdots 1$$ Now my intuition is that all the diagonal entries of the matrix will be $1$ and it will be an upper triangularized matrix then the eigen-value of this linear transformation is $1$ .We need to find the number of times it occurs Let $f(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_0$ be a polynomial such that $T(f(x)) = f(x) \implies f(x+1) = f(x)$ from this equation we can conclude that the only possible polynomial that will satisfy this equation is $c$ where $c \in \mathbb{C}$ . Hence , $T(1) = 1$ so $1$ is the only possible eigen-vector and $1$ is the only eigen-value. Is my way of approaching the problem correct?","be the -linear transformation defined on the complex vector space of one variable complex polynomials by . How many eigenvalues does T have? The basis of over are . Then, Now my intuition is that all the diagonal entries of the matrix will be and it will be an upper triangularized matrix then the eigen-value of this linear transformation is .We need to find the number of times it occurs Let be a polynomial such that from this equation we can conclude that the only possible polynomial that will satisfy this equation is where . Hence , so is the only possible eigen-vector and is the only eigen-value. Is my way of approaching the problem correct?","T: \mathbb{C}[x] → \mathbb{C}[x] \mathbb{C} \mathbb{C}[x] T (f(x)) = f(x + 1) \mathbb{C}[x] \mathbb{C} \{1,x,x^2,\cdots , x^{n} ,\cdots\} T(1) = 1 T(x)= x+1  T(x^2) = (x+1)^2 = x^2 + 2x + 1 T(x^n) = (x+1)^n = x^n +  (n_{C_1})x^{n-1}+ \cdots 1 1 1 f(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_0 T(f(x)) = f(x) \implies f(x+1) = f(x) c c \in \mathbb{C} T(1) = 1 1 1","['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
81,Does real dimension equal rational dimension?,Does real dimension equal rational dimension?,,"Say $v_1,v_2,\dots,v_k\in\mathbb{Q}^n$ . Let $V$ be the subspace spanned by these vectors and let $W\subseteq\mathbb{R}^n$ be the vector subspace in $\mathbb{R}^n$ spanned by these vectors. Is it true that $\dim_\mathbb{Q} V=\dim_\mathbb{R} W$ ? The equality seems very obvious and it's in fact easy to prove it using induction on $n$ : If $n=1$ , then the equality holds as either both $V$ and $W$ are $0$ or $V=\mathbb{Q}$ and $W=\mathbb{R}$ . For $n>1$ , if $\dim V<n$ , then the theorem holds via induction. Hence I assume that $\dim V=n$ so $V=\mathbb{Q}^n$ . But in this case $W$ also must be equal to $\mathbb{R}^n$ and hence the dimensions are equal. Is there a more natural/intuitive way to see why this equality holds?","Say . Let be the subspace spanned by these vectors and let be the vector subspace in spanned by these vectors. Is it true that ? The equality seems very obvious and it's in fact easy to prove it using induction on : If , then the equality holds as either both and are or and . For , if , then the theorem holds via induction. Hence I assume that so . But in this case also must be equal to and hence the dimensions are equal. Is there a more natural/intuitive way to see why this equality holds?","v_1,v_2,\dots,v_k\in\mathbb{Q}^n V W\subseteq\mathbb{R}^n \mathbb{R}^n \dim_\mathbb{Q} V=\dim_\mathbb{R} W n n=1 V W 0 V=\mathbb{Q} W=\mathbb{R} n>1 \dim V<n \dim V=n V=\mathbb{Q}^n W \mathbb{R}^n","['linear-algebra', 'vector-spaces']"
82,Prove that rank(A) = rank(A|C) [duplicate],Prove that rank(A) = rank(A|C) [duplicate],,"This question already has answers here : Existence of solution for a linear system mod 2 (3 answers) Closed 3 years ago . I have a problem in which I am trying to prove over GF(2) that a binary symmetric matrix (A) with a diagonal of ones has a rank always equal to the rank of its augmented matrix with a ones vector (C) $$   C=\left[\begin{array} \\     1 \\     \vdots \\     1   \end{array}\right] $$ To clarify, such matrix is constructed like so: $$   A=\left[\begin{array}{rrrr} 1 & a_{1,1} & a_{1,2} & \dots & a_{1,n} \\ a_{1,1} & 1 & a_{2,1} & \ddots & \vdots \\ a_{1,2} & a_{2,1} & \ddots & a_{n-1,n-1} & a_{n-1,n} \\ \vdots & \ddots & a_{n-1,n-1} & 1 & a_{n,n} \\ a_{1,n} & \dots & a_{n-1,n} & a_{n,n} & 1   \end{array}\right] $$ For example, a 3 by 3 matrix like this has a rank of 2: $$   A=\left[\begin{array}{rrr}     1 & 1 & 0 \\     1 & 1 & 0 \\     0 & 0 & 1   \end{array}\right] $$ When we augment it with a ones vector, we get this matrix which also has a rank of 2: $$   A|C=\left[\begin{array}{rrr|r}     1 & 1 & 0 & 1 \\     1 & 1 & 0 & 1 \\     0 & 0 & 1 & 1   \end{array}\right] $$ Cleary rank(A) = rank(A|C) over GF(2). Why is this always true for such type of matrices? If you have a proof, an idea, or a suggestion on how to proceed, please let me know. Any help is appreciated.","This question already has answers here : Existence of solution for a linear system mod 2 (3 answers) Closed 3 years ago . I have a problem in which I am trying to prove over GF(2) that a binary symmetric matrix (A) with a diagonal of ones has a rank always equal to the rank of its augmented matrix with a ones vector (C) To clarify, such matrix is constructed like so: For example, a 3 by 3 matrix like this has a rank of 2: When we augment it with a ones vector, we get this matrix which also has a rank of 2: Cleary rank(A) = rank(A|C) over GF(2). Why is this always true for such type of matrices? If you have a proof, an idea, or a suggestion on how to proceed, please let me know. Any help is appreciated.","
  C=\left[\begin{array} \\
    1 \\
    \vdots \\
    1
  \end{array}\right]
 
  A=\left[\begin{array}{rrrr}
1 & a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
a_{1,1} & 1 & a_{2,1} & \ddots & \vdots \\
a_{1,2} & a_{2,1} & \ddots & a_{n-1,n-1} & a_{n-1,n} \\
\vdots & \ddots & a_{n-1,n-1} & 1 & a_{n,n} \\
a_{1,n} & \dots & a_{n-1,n} & a_{n,n} & 1
  \end{array}\right]
 
  A=\left[\begin{array}{rrr}
    1 & 1 & 0 \\
    1 & 1 & 0 \\
    0 & 0 & 1
  \end{array}\right]
 
  A|C=\left[\begin{array}{rrr|r}
    1 & 1 & 0 & 1 \\
    1 & 1 & 0 & 1 \\
    0 & 0 & 1 & 1
  \end{array}\right]
","['linear-algebra', 'systems-of-equations', 'matrix-rank']"
83,Why should the trace of a 3d rotation matrix have these properties?,Why should the trace of a 3d rotation matrix have these properties?,,"On the Wikipedia article about Rotation Matrices ( https://en.wikipedia.org/wiki/Rotation_matrix#Determining_the_angle ), the article states that the trace of the matrix will be equal to 1 + 2 cos(theta), where the theta represents the angle of the rotation in axis/angle form. How is this property found?  There doesn't appear to be any derivation on the site, and I can't see any reason why it might be the case.","On the Wikipedia article about Rotation Matrices ( https://en.wikipedia.org/wiki/Rotation_matrix#Determining_the_angle ), the article states that the trace of the matrix will be equal to 1 + 2 cos(theta), where the theta represents the angle of the rotation in axis/angle form. How is this property found?  There doesn't appear to be any derivation on the site, and I can't see any reason why it might be the case.",,"['linear-algebra', 'matrices', 'rotations', 'trace', 'orthogonal-matrices']"
84,"A symmetric, diagonally dominant matrix A with real positive diagonal entries is positive definite","A symmetric, diagonally dominant matrix A with real positive diagonal entries is positive definite",,"Suppose $A \in \mathbb{R}^{n\times n}$ is symmetric and diagonally dominant with positive diagonal entries. I have to prove that $A$ is positive definite but without using theorems, just algebraically. I´ve started with: $$x^T A x = \sum_{i=1}^n a_{ii} x_i^2 + \sum_{i=j} a_{ij} x_i x_j > \sum_{i=1}^n \sum_{i\neq j} |a_{ij}| x_i^2 + \sum_{i\neq j} a_{ij} x_i x_j$$ but I could much further. I was thiking about: $$ \sum_{1=1}^n \sum_{j>i} |a_{ij}| (x_i^2 + x_j^2) + 2 a_{ij} x_i x_j$$ but I´m not sure how to continue.","Suppose is symmetric and diagonally dominant with positive diagonal entries. I have to prove that is positive definite but without using theorems, just algebraically. I´ve started with: but I could much further. I was thiking about: but I´m not sure how to continue.",A \in \mathbb{R}^{n\times n} A x^T A x = \sum_{i=1}^n a_{ii} x_i^2 + \sum_{i=j} a_{ij} x_i x_j > \sum_{i=1}^n \sum_{i\neq j} |a_{ij}| x_i^2 + \sum_{i\neq j} a_{ij} x_i x_j  \sum_{1=1}^n \sum_{j>i} |a_{ij}| (x_i^2 + x_j^2) + 2 a_{ij} x_i x_j,"['linear-algebra', 'matrices', 'positive-definite']"
85,When is rank(A+B)=rank(A)+rank(B) for matrices?,When is rank(A+B)=rank(A)+rank(B) for matrices?,,I know that for matrices $$\operatorname{rank}(A+B)\leq \operatorname{rank}(A) + \operatorname{rank}(B)$$ but when does the equality hold?,I know that for matrices but when does the equality hold?,\operatorname{rank}(A+B)\leq \operatorname{rank}(A) + \operatorname{rank}(B),"['linear-algebra', 'matrices', 'matrix-rank']"
86,Finding the determinant of a tridiagonal matrix,Finding the determinant of a tridiagonal matrix,,$$\begin{vmatrix}x&1&0&0&⋯\\-n&x-2&2&0&⋯\\0&-(n-1)&x-4&3&⋯\\⋮&⋱&⋱&⋱&⋮\\0&⋯&-2&x-2(n-1)&n\\0&0&⋯&-1&x-2n\end{vmatrix}_{(n+1)×(n+1)}$$ Find the value of the above determinant. This problem comes from an advanced algebra book. I want to solve it with elementary transformation knowledge. I have been trying to solve it for a long time.,Find the value of the above determinant. This problem comes from an advanced algebra book. I want to solve it with elementary transformation knowledge. I have been trying to solve it for a long time.,\begin{vmatrix}x&1&0&0&⋯\\-n&x-2&2&0&⋯\\0&-(n-1)&x-4&3&⋯\\⋮&⋱&⋱&⋱&⋮\\0&⋯&-2&x-2(n-1)&n\\0&0&⋯&-1&x-2n\end{vmatrix}_{(n+1)×(n+1)},"['linear-algebra', 'determinant', 'tridiagonal-matrices']"
87,Alternate inner products on Euclidean space?,Alternate inner products on Euclidean space?,,"After reading about inner products as a generalization of the dot product, I was hoping to be able to prove that the dot product is in some sense the unique inner product in Euclidean space (e.g., up to constant scaling). But it seems that there are a whole bunch of alternative inner products in $\mathbb{R}^2$ with nonzero cross-terms between basis vectors, for example, $\langle (a, b)^\intercal, (x, y)^\intercal \rangle = ax + by + 0.5(ay + bx)$ . Unless I've made a mistake, this satisfies symmetry, linearity, and positive-definiteness. Is there a sense in which the dot product is the canonical inner product on Euclidean space? Or do we just pick it because the implied norm matches our notion of distance?","After reading about inner products as a generalization of the dot product, I was hoping to be able to prove that the dot product is in some sense the unique inner product in Euclidean space (e.g., up to constant scaling). But it seems that there are a whole bunch of alternative inner products in with nonzero cross-terms between basis vectors, for example, . Unless I've made a mistake, this satisfies symmetry, linearity, and positive-definiteness. Is there a sense in which the dot product is the canonical inner product on Euclidean space? Or do we just pick it because the implied norm matches our notion of distance?","\mathbb{R}^2 \langle (a, b)^\intercal, (x, y)^\intercal \rangle = ax + by + 0.5(ay + bx)","['linear-algebra', 'inner-products']"
88,Vector space basics: scalar times a nonzero vector = zero implies scalar = zero?,Vector space basics: scalar times a nonzero vector = zero implies scalar = zero?,,"I'm working through a linear algebra text, starting right from the axioms. So far I've understood and proved for myself that, for some vector space $V$ over a field $\mathbb{F}$ : the additive identity (zero vector) $\mathbf{0}$ in a vector space is unique the additive inverses in a vector space are unique scalar zero times any vector is the zero vector: $\forall \mathbf{v} \in V: 0 \mathbf{v} = \mathbf{0}$ any scalar times the zero vector is the zero vector: $\forall a \in \mathbb{F}: a \mathbf{0} = \mathbf{0}$ However, I'm stuck on the converse of the third statement: suppose $a \mathbf{v} = \mathbf{0}$ and $\mathbf{v} \neq \mathbf{0}$ . Show that $a$ must be equal to $0$ . In other words, show that the scalar zero is the only element of $\mathbb{F}$ that allows for rule of inference number 3 in the above list. It seems like such a simple thing but I'm not used to proving super basic statements like this axiomatically. My attempt so far is something like: $$\begin{align}a\mathbf{v} &= \mathbf{0} \quad &\text{(1)} \\ a \mathbf{v} + \mathbf{0} &= \mathbf{0} \quad &\text{(vector additive identity)} \\ a \mathbf{v} + a \mathbf{v} &= \mathbf{0} \quad &\text{(substitute from 1)} \\ (a + a) \mathbf{v} &= \mathbf{0} \quad &\text{(distributive property)} \\ (2a)\mathbf{v} &= \mathbf{0} \\ (2a)\mathbf{v} &= a \mathbf{v} \quad &\text{(substitute from 1)} \\ \mathrm{``therefore""} \quad 2a &= a \\ a &= 0 \end{align} $$ But I'm not sure I'm ""allowed"" to do that second-to-last step yet, given the things proved so far. I think it might just be a circular argument. Is it? EDIT: I figured it out with some prodding; turned out I had all the pieces in front of me already but didn't realize it. Here it is for completeness: Suppose $a \in \mathbb{F}$ , $\mathbf{v} \in V$ , and $a \mathbf{v} = \mathbf{0}$ . Either $a = 0$ or $a \neq 0$ . In the case where $a \neq 0$ : $$\begin{align} a \mathbf{v} &= \mathbf{0} \\ \frac{1}{a} ( a \mathbf{v} ) &= \frac{1}{a} \mathbf{0} \\ (\frac{1}{a} a) \mathbf{v} & = \mathbf{0} \\ 1\mathbf{v} &= \mathbf{0} \\ \mathbf{v} &= \mathbf{0}  \end{align}$$ But then suppose further that $\mathbf{v} \neq \mathbf{0}$ . Then $a = 0$ by the contrapositive.","I'm working through a linear algebra text, starting right from the axioms. So far I've understood and proved for myself that, for some vector space over a field : the additive identity (zero vector) in a vector space is unique the additive inverses in a vector space are unique scalar zero times any vector is the zero vector: any scalar times the zero vector is the zero vector: However, I'm stuck on the converse of the third statement: suppose and . Show that must be equal to . In other words, show that the scalar zero is the only element of that allows for rule of inference number 3 in the above list. It seems like such a simple thing but I'm not used to proving super basic statements like this axiomatically. My attempt so far is something like: But I'm not sure I'm ""allowed"" to do that second-to-last step yet, given the things proved so far. I think it might just be a circular argument. Is it? EDIT: I figured it out with some prodding; turned out I had all the pieces in front of me already but didn't realize it. Here it is for completeness: Suppose , , and . Either or . In the case where : But then suppose further that . Then by the contrapositive.","V \mathbb{F} \mathbf{0} \forall \mathbf{v} \in V: 0 \mathbf{v} = \mathbf{0} \forall a \in \mathbb{F}: a \mathbf{0} = \mathbf{0} a \mathbf{v} = \mathbf{0} \mathbf{v} \neq \mathbf{0} a 0 \mathbb{F} \begin{align}a\mathbf{v} &= \mathbf{0} \quad &\text{(1)} \\
a \mathbf{v} + \mathbf{0} &= \mathbf{0} \quad &\text{(vector additive identity)} \\
a \mathbf{v} + a \mathbf{v} &= \mathbf{0} \quad &\text{(substitute from 1)} \\
(a + a) \mathbf{v} &= \mathbf{0} \quad &\text{(distributive property)} \\
(2a)\mathbf{v} &= \mathbf{0} \\
(2a)\mathbf{v} &= a \mathbf{v} \quad &\text{(substitute from 1)} \\
\mathrm{``therefore""} \quad 2a &= a \\
a &= 0
\end{align}
 a \in \mathbb{F} \mathbf{v} \in V a \mathbf{v} = \mathbf{0} a = 0 a \neq 0 a \neq 0 \begin{align}
a \mathbf{v} &= \mathbf{0} \\
\frac{1}{a} ( a \mathbf{v} ) &= \frac{1}{a} \mathbf{0} \\
(\frac{1}{a} a) \mathbf{v} & = \mathbf{0} \\
1\mathbf{v} &= \mathbf{0} \\
\mathbf{v} &= \mathbf{0}
 \end{align} \mathbf{v} \neq \mathbf{0} a = 0","['linear-algebra', 'proof-verification', 'axioms']"
89,Actual computational complexity of solving a linear system accounting for numerical accuracy (digit),Actual computational complexity of solving a linear system accounting for numerical accuracy (digit),,"Solving a system of linear equations is solving for $n \times 1$ vector $x$ out of $Ax = b$ , where $A$ is $n \times n$ matrix. Suppose that $A$ 's entries have $k$ digits at maximum, in binary or decimal. Similarly, entries of $b$ also has $k$ digits at maximum. It is said that computational complexity of solving lienar equation is $O(n^3)$ , but this does not account for numerical issues - such as actually turning out $x$ to be accurate up to $y$ digits. What would be actual computational complexity accounting for $k$ and $y$ ?","Solving a system of linear equations is solving for vector out of , where is matrix. Suppose that 's entries have digits at maximum, in binary or decimal. Similarly, entries of also has digits at maximum. It is said that computational complexity of solving lienar equation is , but this does not account for numerical issues - such as actually turning out to be accurate up to digits. What would be actual computational complexity accounting for and ?",n \times 1 x Ax = b A n \times n A k b k O(n^3) x y k y,"['linear-algebra', 'computational-complexity', 'numerical-linear-algebra']"
90,Mutually commuting matrix with $A_i^2=0$,Mutually commuting matrix with,A_i^2=0,"Let $A_1,\dots,A_n$ be mutually commuting $m\times m$ matrices such that $A_i^2=0$ for all $1\le i \le n$ . If $m<2^n$ , prove that $A_1 A_2\cdots A_n=0$ Since $A_i^2=0$ So $\operatorname{Im}(A)\subset \ker(A) \implies \dim(\ker(A))\ge \frac m 2$ I think we have to use it to prove ..but don't know how go through ...","Let be mutually commuting matrices such that for all . If , prove that Since So I think we have to use it to prove ..but don't know how go through ...","A_1,\dots,A_n m\times m A_i^2=0 1\le i \le n m<2^n A_1 A_2\cdots A_n=0 A_i^2=0 \operatorname{Im}(A)\subset \ker(A) \implies \dim(\ker(A))\ge \frac m 2","['linear-algebra', 'matrices', 'nilpotence']"
91,What does it mean geometrically to add two matrices?,What does it mean geometrically to add two matrices?,,"If you think of matrix-vector multiplication geometrically as a linear transformation to a new coordinate system and matrix-matrix multiplication as the composition of two separate linear transformations, what does it mean to add two matrices together? Would it make sense to think of it in terms of adding each basis vector separately to create a new set of basis vectors?","If you think of matrix-vector multiplication geometrically as a linear transformation to a new coordinate system and matrix-matrix multiplication as the composition of two separate linear transformations, what does it mean to add two matrices together? Would it make sense to think of it in terms of adding each basis vector separately to create a new set of basis vectors?",,"['linear-algebra', 'matrices']"
92,"How to check whether a representation $G\to\mathrm{GL}(n,\Bbb R)$ is irreducible?",How to check whether a representation  is irreducible?,"G\to\mathrm{GL}(n,\Bbb R)","I know there is a very beautiful theory for representations over $\Bbb C$ , especially the character theory makes it almost trivial to check whether a given representation $G\to\mathrm{GL}(n,\Bbb C)$ is irreducible. But how can I check this in a similarly algorithmic fashion for representations over $\Bbb R$ ? I am specifically interested in the case of finite groups. Question : Given a finite group $G$ and a representation $\rho:G\to\mathrm{GL}(n,\Bbb R)$ . How to determine (algorithmically) whether $\rho$ is irreducible? Note I I am aware of Frobenius-Schur indicator but I cannot understand whether and how it helps me for my question. At first, I do not have a representation over $\Bbb C$ to start with. And I am not really interested in transforming my irreducible representation over $\Bbb R$ into one or more irreducible representation over $\Bbb C$ . Note II I avoid using the term ""real representation"" as it seems to have the meaning of a representation over $\Bbb C$ with a real valued character. I am not very familiar with the connection of this term and ""representations over $\Bbb R$ "" that I use. But please enlighten me.","I know there is a very beautiful theory for representations over , especially the character theory makes it almost trivial to check whether a given representation is irreducible. But how can I check this in a similarly algorithmic fashion for representations over ? I am specifically interested in the case of finite groups. Question : Given a finite group and a representation . How to determine (algorithmically) whether is irreducible? Note I I am aware of Frobenius-Schur indicator but I cannot understand whether and how it helps me for my question. At first, I do not have a representation over to start with. And I am not really interested in transforming my irreducible representation over into one or more irreducible representation over . Note II I avoid using the term ""real representation"" as it seems to have the meaning of a representation over with a real valued character. I am not very familiar with the connection of this term and ""representations over "" that I use. But please enlighten me.","\Bbb C G\to\mathrm{GL}(n,\Bbb C) \Bbb R G \rho:G\to\mathrm{GL}(n,\Bbb R) \rho \Bbb C \Bbb R \Bbb C \Bbb C \Bbb R","['linear-algebra', 'group-theory', 'finite-groups', 'representation-theory']"
93,Exercise books in linear algebra,Exercise books in linear algebra,,"I'm going to find some Exercise books in linear algebra  and I was wondering if there are some exercise books (that is, books with solved problems and exercises)  The books I'm searching for should be: full of hard , non-obvious , non-common , and thought-provoking problems; rich of complete , step by step , rigorous , and enlightening solutions; Thanks in advance","I'm going to find some Exercise books in linear algebra  and I was wondering if there are some exercise books (that is, books with solved problems and exercises)  The books I'm searching for should be: full of hard , non-obvious , non-common , and thought-provoking problems; rich of complete , step by step , rigorous , and enlightening solutions; Thanks in advance",,"['linear-algebra', 'reference-request', 'book-recommendation']"
94,Let $A$ be a $5 \times 5$ matrix such that $A^2=0$. Then how to compute the maximum rank for such A?,Let  be a  matrix such that . Then how to compute the maximum rank for such A?,A 5 \times 5 A^2=0,"Attempt : Suppose $A$ has a non-zero eigenvalue $\lambda$. Then corresponding to it's non-zero eigen vector $X$, we have $AX=\lambda X \Rightarrow A^2X=\lambda^2 X\Rightarrow 0=\lambda^2 X$. Which is a contradiction. Hence $\lambda=0$ with algebraic multiplicity $5$. Looking at all the Jordan normal forms $J$ of $A$, I found one $J$ which gives $J^2=0$ whose $\text {Rank} J=2$. (There is other Jordan normal form with rank$=1$) Here is that J := $$J =          \begin{pmatrix}         0 & 1 & 0 & 0 & 0 \\         0 & 0 & 0 & 0 & 0 \\         0 & 0 & 0 & 1 & 0 \\         0 & 0 & 0 & 0 & 0 \\         0 & 0 & 0 & 0 & 0 \\         \end{pmatrix} $$ I found that Jordan normal forms with ranks $3,4$ don't satisfy $J^2=0$. So the least upper bound for the rank is $2$. Is my attempt correct? Thanks.","Attempt : Suppose $A$ has a non-zero eigenvalue $\lambda$. Then corresponding to it's non-zero eigen vector $X$, we have $AX=\lambda X \Rightarrow A^2X=\lambda^2 X\Rightarrow 0=\lambda^2 X$. Which is a contradiction. Hence $\lambda=0$ with algebraic multiplicity $5$. Looking at all the Jordan normal forms $J$ of $A$, I found one $J$ which gives $J^2=0$ whose $\text {Rank} J=2$. (There is other Jordan normal form with rank$=1$) Here is that J := $$J =          \begin{pmatrix}         0 & 1 & 0 & 0 & 0 \\         0 & 0 & 0 & 0 & 0 \\         0 & 0 & 0 & 1 & 0 \\         0 & 0 & 0 & 0 & 0 \\         0 & 0 & 0 & 0 & 0 \\         \end{pmatrix} $$ I found that Jordan normal forms with ranks $3,4$ don't satisfy $J^2=0$. So the least upper bound for the rank is $2$. Is my attempt correct? Thanks.",,"['linear-algebra', 'matrices', 'proof-verification', 'matrix-rank', 'jordan-normal-form']"
95,"Characteristic polynomial of $T:M_n(\mathbb{F}) \rightarrow M_n(\mathbb{F}) ,\ \ TX = AX \ \ (A\in M_n(\mathbb{F}))$",Characteristic polynomial of,"T:M_n(\mathbb{F}) \rightarrow M_n(\mathbb{F}) ,\ \ TX = AX \ \ (A\in M_n(\mathbb{F}))","Question - how would I proceed to find the characteristic polynomial of $T:M_n(\mathbb{F}) \rightarrow M_n(\mathbb{F}) ,\ \ TX = AX  \ \ (A\in M_n(\mathbb{F}))$ ? What I've been trying: Given the the standard base $\{E_{11}, E_{12}, \dots, E_{nn}\}$ of $M_n(\mathbb{F})$ in which ( $E_{ij})_{kl} =\left\{\begin{matrix}  1,& k=i \ \ and \ \ l=j \\ 0, &otherwise \end{matrix}\right.$ $T$ can be represented by the following $n^2\times n^2$ matrix: $$[T] = \begin{pmatrix} (A)_{11}I_n&(A)_{12}I_n&\cdots&(A)_{1n}I_n\\ (A)_{21}I_n&(A)_{22}I_n&\cdots&(A)_{2n}I_n\\ \vdots&\vdots&\ddots&\vdots\\ (A)_{n1}I_n&(A)_{n2}I_n&\cdots&(A)_{nn}I_n \end{pmatrix} $$ Now, from from here I'd like to calculate $det([T]-tI_{n^2})$ , and this is the point where I got stuck. I'd be glad for ideas on how to proceed from here, or ideas for other ways to tackle the problem.","Question - how would I proceed to find the characteristic polynomial of ? What I've been trying: Given the the standard base of in which ( can be represented by the following matrix: Now, from from here I'd like to calculate , and this is the point where I got stuck. I'd be glad for ideas on how to proceed from here, or ideas for other ways to tackle the problem.","T:M_n(\mathbb{F}) \rightarrow M_n(\mathbb{F}) ,\ \ TX = AX  \ \ (A\in M_n(\mathbb{F})) \{E_{11}, E_{12}, \dots, E_{nn}\} M_n(\mathbb{F}) E_{ij})_{kl} =\left\{\begin{matrix}
 1,& k=i \ \ and \ \ l=j \\ 0, &otherwise
\end{matrix}\right. T n^2\times n^2 [T] = \begin{pmatrix}
(A)_{11}I_n&(A)_{12}I_n&\cdots&(A)_{1n}I_n\\
(A)_{21}I_n&(A)_{22}I_n&\cdots&(A)_{2n}I_n\\
\vdots&\vdots&\ddots&\vdots\\
(A)_{n1}I_n&(A)_{n2}I_n&\cdots&(A)_{nn}I_n
\end{pmatrix}  det([T]-tI_{n^2})","['linear-algebra', 'matrices']"
96,How to show this inequality $\sum\sqrt{\frac{x}{x+2y+z}}\le 2$,How to show this inequality,\sum\sqrt{\frac{x}{x+2y+z}}\le 2,"Let $x,y,z,w>0$ show that $$\sqrt{\dfrac{x}{x+2y+z}}+\sqrt{\dfrac{y}{y+2z+w}}+\sqrt{\dfrac{z}{z+2w+x}}+\sqrt{\dfrac{w}{w+2x+y}}\le 2$$ I tried C-S, but without success.","Let $x,y,z,w>0$ show that $$\sqrt{\dfrac{x}{x+2y+z}}+\sqrt{\dfrac{y}{y+2z+w}}+\sqrt{\dfrac{z}{z+2w+x}}+\sqrt{\dfrac{w}{w+2x+y}}\le 2$$ I tried C-S, but without success.",,"['linear-algebra', 'inequality', 'substitution', 'cauchy-schwarz-inequality']"
97,Subordinate matrix norm of $1$-norm,Subordinate matrix norm of -norm,1,"Show that for the vector norm $$\|x\|_1=\sum_{i=1}^n |x_i|$$ the subordinate matrix norm is $$ \|A\|_1=\max_{1\leq j\leq n} \sum_{i=1}^n |a_{ij}| $$ I've managed to narrow it down doing the following: $$ \|A\|_1=\sup_{\|u\|_1=1} \left\{\sum_{i=1}^n |(Au)_i|\right\} = \sup_{\|u\|_1=1} \left\{\sum_{i=1}^n \left|\sum_{j=1}^n a_{ij}u_j\right|\right\} $$ Now intuitively I can see why the subordinate norm is what it is. I just can't come up with a mathematical way of proving it from here. I know $u_j=1$ when the sum in the sub. matrix norm is largest, however how do I prove that doing this makes the maximum equal to the supremum?","Show that for the vector norm the subordinate matrix norm is I've managed to narrow it down doing the following: Now intuitively I can see why the subordinate norm is what it is. I just can't come up with a mathematical way of proving it from here. I know when the sum in the sub. matrix norm is largest, however how do I prove that doing this makes the maximum equal to the supremum?","\|x\|_1=\sum_{i=1}^n |x_i| 
\|A\|_1=\max_{1\leq j\leq n} \sum_{i=1}^n |a_{ij}|
 
\|A\|_1=\sup_{\|u\|_1=1} \left\{\sum_{i=1}^n |(Au)_i|\right\} = \sup_{\|u\|_1=1} \left\{\sum_{i=1}^n \left|\sum_{j=1}^n a_{ij}u_j\right|\right\}
 u_j=1","['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
98,"Given $(A-I)^2 = 0$, can we say det$(A)=1$ and tr$(A)=n$?","Given , can we say det and tr?",(A-I)^2 = 0 (A)=1 (A)=n,"Given $(A-I)^2 = 0$, can we say det$(A)=1$ and tr$(A)=n$? First I showed when $A$ is two by two, this is true. Since $\lambda_1 + \lambda_2 = 2$ and $\lambda_1 \lambda_2=1$, then from substitution $\lambda_1 = \frac{1}{\lambda_2}$, this gives $(\lambda_2-1)^2 = 1$, and therefore $\lambda_1 = \lambda_2 = 1$. I tried the case when $A$ is $3\times 3$ by first fixing the third eigenvalue and solve for the other two, and it also worked. How would I approach such problem when $A$ is $n\times n$.","Given $(A-I)^2 = 0$, can we say det$(A)=1$ and tr$(A)=n$? First I showed when $A$ is two by two, this is true. Since $\lambda_1 + \lambda_2 = 2$ and $\lambda_1 \lambda_2=1$, then from substitution $\lambda_1 = \frac{1}{\lambda_2}$, this gives $(\lambda_2-1)^2 = 1$, and therefore $\lambda_1 = \lambda_2 = 1$. I tried the case when $A$ is $3\times 3$ by first fixing the third eigenvalue and solve for the other two, and it also worked. How would I approach such problem when $A$ is $n\times n$.",,"['linear-algebra', 'determinant', 'trace']"
99,Which non-negative matrices have negative eigenvalues?,Which non-negative matrices have negative eigenvalues?,,"It's easy to proof by counterexample that non-negative matrices can have negative eigenvalues. For example, the following matrix has -1 as an eigenvalue: $$ A =  \begin{bmatrix}   0 & 0 & 0 & 1 \\   1 & 0 & 0 & 0 \\   0 & 1 & 0 & 0 \\   0 & 0 & 1 & 0 \\  \end{bmatrix} $$ However, which are the properties of those matrices, is there a generalization of them? Thanks!","It's easy to proof by counterexample that non-negative matrices can have negative eigenvalues. For example, the following matrix has -1 as an eigenvalue: However, which are the properties of those matrices, is there a generalization of them? Thanks!","
A =
 \begin{bmatrix}
  0 & 0 & 0 & 1 \\
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
 \end{bmatrix}
","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
