,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Lipschitz continuity of square root of sum of matrix squares,Lipschitz continuity of square root of sum of matrix squares,,"Let $B\in \mathbb{R}^{n\times n}$ be a positive semi-definite matrix (i.e., $B \in \mathbb{S}^n_{\ge 0}$ ),   and consider the map $$ \mathbb{R}^{n\times n}\ni A \mapsto f(A):=(AA^\top +B)^{1/2}\in \mathbb{S}^n_{\ge 0}, $$ where $(\cdot)^{1/2}$ is unique positive semidefinite matrix square root. Is it possible to prove the map is Lipschitz continuous? How about local Lipschitz continuity? The answer is affirmative for one-dimensional case. By discussing whether $B=0$ or not, one can easily show $f(A)$ is Lipschitz continuous. In a multidimensional setting, if $B$ is positive definite, then the matrix square root $(\cdot)^{1/2}$ is Lipschitz continuous with the Lipschitz constant depending on the minimum eigenvalue of $B$ , and hence the map $f$ is locally Lipschitz. It is not clear to me how to relax the positive definite condition of $B$ for the locally Lipschitz continuity.","Let be a positive semi-definite matrix (i.e., ),   and consider the map where is unique positive semidefinite matrix square root. Is it possible to prove the map is Lipschitz continuous? How about local Lipschitz continuity? The answer is affirmative for one-dimensional case. By discussing whether or not, one can easily show is Lipschitz continuous. In a multidimensional setting, if is positive definite, then the matrix square root is Lipschitz continuous with the Lipschitz constant depending on the minimum eigenvalue of , and hence the map is locally Lipschitz. It is not clear to me how to relax the positive definite condition of for the locally Lipschitz continuity.","B\in \mathbb{R}^{n\times n} B \in \mathbb{S}^n_{\ge 0} 
\mathbb{R}^{n\times n}\ni A \mapsto f(A):=(AA^\top +B)^{1/2}\in \mathbb{S}^n_{\ge 0},
 (\cdot)^{1/2} B=0 f(A) B (\cdot)^{1/2} B f B","['linear-algebra', 'matrices', 'matrix-decomposition', 'lipschitz-functions']"
1,Kronecker Product of Normal Matrices,Kronecker Product of Normal Matrices,,"Theorem. If $A\in M_m$ and $B\in M_n$ are both normal, so is $A\otimes B$ . The converse is true if $A\otimes B\ne 0$ . Proof. Suppose that $A\in M_m$ and $B\in M_n$ are both normal. Then \begin{align*}     (A\otimes B)^*(A\otimes B)=(A^*\otimes B^*)(A\otimes B)&=(A^*A)\otimes(B^*B) \\ &=(AA^*)\otimes(BB^*) \\ &=(A\otimes B)(A^*\otimes B^*)=(A\otimes B)(A\otimes B)^*.  \end{align*} Therefore, $A\otimes B$ is normal. Now suppose that $A\otimes B$ is nonzero and normal. Then both $A$ and $B$ are nonzero. Then we have \begin{equation*}     (A^*A)\otimes(B^*B)=(AA^*)\otimes(BB^*).  \end{equation*} Consider the $i$ -th diagonal block of these two products: \begin{equation*}     (A^*A)_{ii}(B^*B)=\left(\sum_{j=1}^{n}{|a_{ji}|^2}\right)(B^*B)=\left(\sum_{j=1}^{n}{|a_{ij}|^2}\right)(BB^*)=(AA^*)_{ii}(BB^*).  \end{equation*} Summing over all $i$ 's, we can get \begin{equation*}     \sum_{i=1}^{n}{\left(\sum_{j=1}^{n}{|a_{ji}|^2}\right)(B^*B)}=\sum_{i=1}^{n}{\left(\sum_{j=1}^{n}{|a_{ij}|^2}\right)(BB^*)},  \end{equation*} That is, $$\operatorname{tr}(A^*A)\cdot(B^*B)=\operatorname{tr}(AA^*)\cdot(BB^*).$$ Since $A$ is nonzero, $\operatorname{tr}(A^*A)=\operatorname{tr}(AA^*)\ne 0$ , which implies that $B^*B=BB^*$ and hence $A^*A=AA^*$ . $\blacksquare$ Can anyone please help to check if my proof is valid? For the converse part, I am not confident whether such method indeed works. Besides, if you have any alternative elegant proofs for this theorem, I would be extraordinarily happy to hear from you!","Theorem. If and are both normal, so is . The converse is true if . Proof. Suppose that and are both normal. Then Therefore, is normal. Now suppose that is nonzero and normal. Then both and are nonzero. Then we have Consider the -th diagonal block of these two products: Summing over all 's, we can get That is, Since is nonzero, , which implies that and hence . Can anyone please help to check if my proof is valid? For the converse part, I am not confident whether such method indeed works. Besides, if you have any alternative elegant proofs for this theorem, I would be extraordinarily happy to hear from you!","A\in M_m B\in M_n A\otimes B A\otimes B\ne 0 A\in M_m B\in M_n \begin{align*}
    (A\otimes B)^*(A\otimes B)=(A^*\otimes B^*)(A\otimes B)&=(A^*A)\otimes(B^*B) \\
&=(AA^*)\otimes(BB^*) \\
&=(A\otimes B)(A^*\otimes B^*)=(A\otimes B)(A\otimes B)^*. 
\end{align*} A\otimes B A\otimes B A B \begin{equation*}
    (A^*A)\otimes(B^*B)=(AA^*)\otimes(BB^*). 
\end{equation*} i \begin{equation*}
    (A^*A)_{ii}(B^*B)=\left(\sum_{j=1}^{n}{|a_{ji}|^2}\right)(B^*B)=\left(\sum_{j=1}^{n}{|a_{ij}|^2}\right)(BB^*)=(AA^*)_{ii}(BB^*). 
\end{equation*} i \begin{equation*}
    \sum_{i=1}^{n}{\left(\sum_{j=1}^{n}{|a_{ji}|^2}\right)(B^*B)}=\sum_{i=1}^{n}{\left(\sum_{j=1}^{n}{|a_{ij}|^2}\right)(BB^*)}, 
\end{equation*} \operatorname{tr}(A^*A)\cdot(B^*B)=\operatorname{tr}(AA^*)\cdot(BB^*). A \operatorname{tr}(A^*A)=\operatorname{tr}(AA^*)\ne 0 B^*B=BB^* A^*A=AA^* \blacksquare","['linear-algebra', 'matrices', 'solution-verification', 'kronecker-product']"
2,"Given any symmetric matrix $A$, how to find a diagonal matrix $B$ such that $B \succeq A$?","Given any symmetric matrix , how to find a diagonal matrix  such that ?",A B B \succeq A,"Given any symmetric (not necessarily PSD) matrix $A$ , how can we find a good diagonal matrix $B$ such that $A \preceq B$ ? We want each of the diagonal elements of $B$ to be as small as possible. One possible such matrix I can think of is $\mathrm{diag}(\|A_i\|_1)$ , but I don't think it is tight enough. Can we find a better matrix than this? Also, I hope that $B$ depends on the $\ell_2$ norms of $A_i$ instead of the $\ell_1$ norms, or the eigenstructure of the matrix $A$ . It would be very useful if there are solutions that only needs eigenvalues or singular values, so it is possible to approximate it in almost linear time to the dimension. The background for this problem is ""coordinate-wise smoothness"" in convex and non-convex optimization. The global smoothness of a function can be defined as $\sup\|\nabla^2f(x)\|_2$ , that is to find a constant $L$ such that $-LI \preceq \nabla^2 f(x) \preceq LI$ . I'm looking for a similar way to find the coordinate-wise smoothness efficiently.","Given any symmetric (not necessarily PSD) matrix , how can we find a good diagonal matrix such that ? We want each of the diagonal elements of to be as small as possible. One possible such matrix I can think of is , but I don't think it is tight enough. Can we find a better matrix than this? Also, I hope that depends on the norms of instead of the norms, or the eigenstructure of the matrix . It would be very useful if there are solutions that only needs eigenvalues or singular values, so it is possible to approximate it in almost linear time to the dimension. The background for this problem is ""coordinate-wise smoothness"" in convex and non-convex optimization. The global smoothness of a function can be defined as , that is to find a constant such that . I'm looking for a similar way to find the coordinate-wise smoothness efficiently.",A B A \preceq B B \mathrm{diag}(\|A_i\|_1) B \ell_2 A_i \ell_1 A \sup\|\nabla^2f(x)\|_2 L -LI \preceq \nabla^2 f(x) \preceq LI,"['linear-algebra', 'matrices', 'convex-optimization', 'positive-semidefinite', 'spectral-norm']"
3,Solutions for $A^2=B$,Solutions for,A^2=B,Show that there exists a neighborhood $U$ of $I$ in $\mbox{Mat}(2 \times 2)$ such that for all $B\in U$ there are at least two solutions in $\mbox{Mat}(2 \times 2)$ for the equation $A^2 = B$ . I have this question for homework and I would be happy to get a hint.  I think it uses the implicit function theorem.,Show that there exists a neighborhood of in such that for all there are at least two solutions in for the equation . I have this question for homework and I would be happy to get a hint.  I think it uses the implicit function theorem.,U I \mbox{Mat}(2 \times 2) B\in U \mbox{Mat}(2 \times 2) A^2 = B,"['calculus', 'matrices', 'matrix-equations', 'matrix-calculus', 'implicit-function-theorem']"
4,Derivative w.r.t. the weight matrix in a linear layer?,Derivative w.r.t. the weight matrix in a linear layer?,,"I'm deriving the back-propagation equations in a neural network. I have a single linear layer. I want to calculate the derivative of the loss function w.r.t. the weight matrix $W$ . The Loss is some function on the output $Y$ , $L(Y)$ . $Y$ is given as $Y=XW^T$ where $Y, X, W$ are all matrices. The answer is as below: $$ \frac{\partial L}{\partial W} = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial W} =?=(\frac{\partial L}{\partial Y})^T X $$ I now want to derive that answer using index notation but am completely stuck. I first start by rewriting $Y$ for a single element: $$ Y_{ij} = \sum^k X_{i,k}W_{j,k} $$ But I don't really know how to complete the derivation using only index notation. And why does $\partial L / \partial Y$ become transposed, while it is before $\partial Y / \partial W$ . I can derive that its necessary by investigating the shapes of the matrices, but that's not really sound reasoning imo.","I'm deriving the back-propagation equations in a neural network. I have a single linear layer. I want to calculate the derivative of the loss function w.r.t. the weight matrix . The Loss is some function on the output , . is given as where are all matrices. The answer is as below: I now want to derive that answer using index notation but am completely stuck. I first start by rewriting for a single element: But I don't really know how to complete the derivation using only index notation. And why does become transposed, while it is before . I can derive that its necessary by investigating the shapes of the matrices, but that's not really sound reasoning imo.","W Y L(Y) Y Y=XW^T Y, X, W 
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial W} =?=(\frac{\partial L}{\partial Y})^T X
 Y 
Y_{ij} = \sum^k X_{i,k}W_{j,k}
 \partial L / \partial Y \partial Y / \partial W","['matrices', 'partial-derivative', 'jacobian', 'neural-networks']"
5,Be $M \in M_2(\mathbb{R})$ with $det(M) = 1$,Be  with,M \in M_2(\mathbb{R}) det(M) = 1,"Be $M \in M_2(\mathbb{R})$ with $\det(M) = 1$ If $|\text{tr}(M)| < 2$ , Prove that exist $P \in M_2(\mathbb{R})$ with $\det(P)=1$ and exist $\alpha \in \mathbb{R}$ such that: $ M = P\cdot \begin{bmatrix} \cos(\alpha) & -\sin(\alpha)  \\ \sin(\alpha) & \cos(\alpha) \end{bmatrix} \cdot P^{-1}$ $\\$ My attempt to solution: I could not solve the problem. And I do not know how to continue. $1)$ I started analising the caracteristic polynomial of M: $P(x)_M = x^2-\text{tr}(M)\cdot x + \det(M)$ And we know that $\det(M) = 1$ and $|\text{tr}(M)| < 2$ , implies that $\Delta = (\text{tr}(M))^2-4 \det(M)<0$ So, both eigenvalues of $M$ are complexes and distinct, and one is conjugate of another. $2)$ This matrix remember me the rotation matrix, but i don't know how to use that fact. I do not know what to do. I tried for a long time, but I really could not resolve.","Be with If , Prove that exist with and exist such that: My attempt to solution: I could not solve the problem. And I do not know how to continue. I started analising the caracteristic polynomial of M: And we know that and , implies that So, both eigenvalues of are complexes and distinct, and one is conjugate of another. This matrix remember me the rotation matrix, but i don't know how to use that fact. I do not know what to do. I tried for a long time, but I really could not resolve.",M \in M_2(\mathbb{R}) \det(M) = 1 |\text{tr}(M)| < 2 P \in M_2(\mathbb{R}) \det(P)=1 \alpha \in \mathbb{R}  M = P\cdot \begin{bmatrix} \cos(\alpha) & -\sin(\alpha)  \\ \sin(\alpha) & \cos(\alpha) \end{bmatrix} \cdot P^{-1} \\ 1) P(x)_M = x^2-\text{tr}(M)\cdot x + \det(M) \det(M) = 1 |\text{tr}(M)| < 2 \Delta = (\text{tr}(M))^2-4 \det(M)<0 M 2),['matrices']
6,Special linear group over a quotient of a polynomial ring,Special linear group over a quotient of a polynomial ring,,"I'm interested in the structure of groups of the form $SL_2(A)$ where $A$ is a ring of the form $A=\mathbb{F}_p[x]/(x^2)$ , or more generally $A=\mathbb{F}_p[x]/(x^n)$ . It is clear that for $n=1$ we just recover $SL_2(\mathbb{F}_p)$ which is well-understood. Also if $p(x) \in \mathbb{F}_p[x]$ is irreducible then $SL_2(\mathbb{F}_p[x]/(p(x))) = SL_2(\mathbb{F}_q)$ for some finite field $\mathbb{F}_q$ , and again there exists a vast literature on these groups. My question is whether special linear groups over these slightly different (local) rings $\mathbb{F}_p[x]/(x^n)$ have been studied at all, and in particular their subgroups structure. Any references would be greatly appreciated.","I'm interested in the structure of groups of the form where is a ring of the form , or more generally . It is clear that for we just recover which is well-understood. Also if is irreducible then for some finite field , and again there exists a vast literature on these groups. My question is whether special linear groups over these slightly different (local) rings have been studied at all, and in particular their subgroups structure. Any references would be greatly appreciated.",SL_2(A) A A=\mathbb{F}_p[x]/(x^2) A=\mathbb{F}_p[x]/(x^n) n=1 SL_2(\mathbb{F}_p) p(x) \in \mathbb{F}_p[x] SL_2(\mathbb{F}_p[x]/(p(x))) = SL_2(\mathbb{F}_q) \mathbb{F}_q \mathbb{F}_p[x]/(x^n),"['abstract-algebra', 'matrices', 'group-theory']"
7,$A$ upper triangular $n\times n$ over $\mathbb{R}$. Show that $I-A$ is invertible and express inverse of $I-A$ as a function of $A$.,upper triangular  over . Show that  is invertible and express inverse of  as a function of .,A n\times n \mathbb{R} I-A I-A A,"Let $A$ be a strictly upper triangular $n\times n$ matrix with real entries.  Show that $I-A$ is invertible and express the inverse of $I-A$ as a function of $A$ . $A$ is upper triangular, so the diagonal is all $0$ , everything below the diagonal is all $0$ , and then we have entries in the upper part, say, $a_{1,2}$ , as the entry in row $1$ column $2$ .  If we consider $I-A$ , then $I-A$ has $1$ in every entry of the diagonal, only $0$ below the diagonal, and negative whatever the entry of $A$ was in the upper part.  So, det $(I-A)=1n=1$ , so $I-A$ is invertible.  Consider $$X=I+A+A^2+ \cdots +A^{n-1}$$ Then, I want to show that $(I-A)X=X(I-A)=I$ . I can see why this ""might"" work, since we would have $I$ and then add all the powers of $A$ , then subtract all the powers of $A$ ... but are there any unexpected issues I should be careful of here?  Thank you much!","Let be a strictly upper triangular matrix with real entries.  Show that is invertible and express the inverse of as a function of . is upper triangular, so the diagonal is all , everything below the diagonal is all , and then we have entries in the upper part, say, , as the entry in row column .  If we consider , then has in every entry of the diagonal, only below the diagonal, and negative whatever the entry of was in the upper part.  So, det , so is invertible.  Consider Then, I want to show that . I can see why this ""might"" work, since we would have and then add all the powers of , then subtract all the powers of ... but are there any unexpected issues I should be careful of here?  Thank you much!","A n\times n I-A I-A A A 0 0 a_{1,2} 1 2 I-A I-A 1 0 A (I-A)=1n=1 I-A X=I+A+A^2+ \cdots +A^{n-1} (I-A)X=X(I-A)=I I A A","['linear-algebra', 'matrices', 'determinant', 'inverse']"
8,How to prove this inequality about Kronecker product?,How to prove this inequality about Kronecker product?,,"I encounter the following problem when I study ridge regression. Problem. Let $\{d_j\}_{j=1}^\infty$ be a sequence of positive integers. Let $\{\psi_{i,j}\}_{i,j=1}^\infty$ be a collection of vectors where $\psi_{i,j}\in\mathbb{R}^{d_j}$ and $\|\psi_{i,j}\|_2\le 1$ . For positive integers $n_0, n_1$ and $n_2$ ( $n_1\ge n_2$ ), we define \begin{equation} \begin{aligned}     \Lambda_1=\sum_{i\in[n_0]}\left(\bigotimes_{j\in[n_1]}\psi_{i,j}\right)\left(\bigotimes_{j\in[n_1]}\psi_{i,j}\right)^\top+\lambda I_1,\\     \Lambda_2=\sum_{i\in[n_0]}\left(\bigotimes_{j\in[n_2]}\psi_{i,j}\right)\left(\bigotimes_{j\in[n_2]}\psi_{i,j}\right)^\top+\lambda I_2, \end{aligned} \end{equation} where $\otimes$ is the Kronecker product, $I_1$ and $I_2$ are identity matrices and $\lambda\in\mathbb{R}_+$ . Let $\{\varphi_k\}_{k=1}^\infty$ be a sequence of vectors where $\varphi_k\in\mathbb{R}^{d_k}$ and $\|\varphi_k\|_2\ge 1$ , and we define \begin{equation}     \phi_1=\bigotimes_{k\in[n_1]} \varphi_k,     \quad     \phi_2=\bigotimes_{k\in[n_2]} \varphi_k. \end{equation} Then, show that the following holds \begin{equation}     \phi_1^\top\Lambda_1^{-1}\phi_1\ge \phi_2^\top\Lambda_2^{-1}\phi_2. \end{equation} I have verified it via programming, and no counterexample was found. Therefore, I believe it is probably true. However, I can only prove the case where $n_0=1$ . The main idea of my proof for $n_0=1$ is the following: Clearly, it suffices to prove that it holds when $n_1=2$ and $n_2=1$ . We diagonalize $\Lambda_1^{-1}$ and $\Lambda_2^{-1}$ . By diagonalization, we can translate $\phi_1^\top\Lambda_1^{-1}\phi_1$ and $\phi_2^\top\Lambda_2^{-1}\phi_2$ into a combination of eigenvalues, and finally obtain the desired inequality. I do not how to handle the case where $n_0>1$ since there are fundamental differences--we can not easily diagonalize the matrices. Hence, I am stuck... Any help or hint would be appreciated. Thanks in advance.","I encounter the following problem when I study ridge regression. Problem. Let be a sequence of positive integers. Let be a collection of vectors where and . For positive integers and ( ), we define where is the Kronecker product, and are identity matrices and . Let be a sequence of vectors where and , and we define Then, show that the following holds I have verified it via programming, and no counterexample was found. Therefore, I believe it is probably true. However, I can only prove the case where . The main idea of my proof for is the following: Clearly, it suffices to prove that it holds when and . We diagonalize and . By diagonalization, we can translate and into a combination of eigenvalues, and finally obtain the desired inequality. I do not how to handle the case where since there are fundamental differences--we can not easily diagonalize the matrices. Hence, I am stuck... Any help or hint would be appreciated. Thanks in advance.","\{d_j\}_{j=1}^\infty \{\psi_{i,j}\}_{i,j=1}^\infty \psi_{i,j}\in\mathbb{R}^{d_j} \|\psi_{i,j}\|_2\le 1 n_0, n_1 n_2 n_1\ge n_2 \begin{equation}
\begin{aligned}
    \Lambda_1=\sum_{i\in[n_0]}\left(\bigotimes_{j\in[n_1]}\psi_{i,j}\right)\left(\bigotimes_{j\in[n_1]}\psi_{i,j}\right)^\top+\lambda I_1,\\
    \Lambda_2=\sum_{i\in[n_0]}\left(\bigotimes_{j\in[n_2]}\psi_{i,j}\right)\left(\bigotimes_{j\in[n_2]}\psi_{i,j}\right)^\top+\lambda I_2,
\end{aligned}
\end{equation} \otimes I_1 I_2 \lambda\in\mathbb{R}_+ \{\varphi_k\}_{k=1}^\infty \varphi_k\in\mathbb{R}^{d_k} \|\varphi_k\|_2\ge 1 \begin{equation}
    \phi_1=\bigotimes_{k\in[n_1]} \varphi_k,
    \quad
    \phi_2=\bigotimes_{k\in[n_2]} \varphi_k.
\end{equation} \begin{equation}
    \phi_1^\top\Lambda_1^{-1}\phi_1\ge \phi_2^\top\Lambda_2^{-1}\phi_2.
\end{equation} n_0=1 n_0=1 n_1=2 n_2=1 \Lambda_1^{-1} \Lambda_2^{-1} \phi_1^\top\Lambda_1^{-1}\phi_1 \phi_2^\top\Lambda_2^{-1}\phi_2 n_0>1","['linear-algebra', 'matrices', 'regression', 'symmetric-matrices', 'kronecker-product']"
9,A particular relation between two matrices,A particular relation between two matrices,,"I'd like to consider the consequences of the following relation between two symmetric square matrices $A$ and $B$ $$SA^{-1}S^T=B$$ $$S^TB^{-1}S=A$$ There are no inversion conditions on the matrix $S$ . Now, say we have the eigenvalues/vectors of $A$ , can we say anything about the eigenvalues/vectors of $B$ ? Edit: I might also add the conditions on $S$ $$S^TS=aA^2$$ $$SS^T=bB^2$$ where $a$ and $b$ are real numbers.","I'd like to consider the consequences of the following relation between two symmetric square matrices and There are no inversion conditions on the matrix . Now, say we have the eigenvalues/vectors of , can we say anything about the eigenvalues/vectors of ? Edit: I might also add the conditions on where and are real numbers.",A B SA^{-1}S^T=B S^TB^{-1}S=A S A B S S^TS=aA^2 SS^T=bB^2 a b,"['linear-algebra', 'matrices']"
10,"For which $2\times 3$ matrices $A,B$ does $\operatorname{rref}(A) + \operatorname{rref}(B) = \operatorname{rref}(A+B)$ hold?",For which  matrices  does  hold?,"2\times 3 A,B \operatorname{rref}(A) + \operatorname{rref}(B) = \operatorname{rref}(A+B)","Let $A$ and $B$ be $2\times 3$ matrices. For which $A$ and $B$ does the following equation hold? $$\operatorname{rref}(A)+ \operatorname{rref}(B) = \operatorname{rref}(A+B)$$ ( $\operatorname{rref}$ is the operation that makes a matrix in reduced row echelon form .) This problem is from Strang's Introduction to Linear Algebra book, and he says about this problem in the book that, it is silly. I don't know why he said that but I couldn't figure this out for a while. I'd appreciate your help. Thanks from now.","Let and be matrices. For which and does the following equation hold? ( is the operation that makes a matrix in reduced row echelon form .) This problem is from Strang's Introduction to Linear Algebra book, and he says about this problem in the book that, it is silly. I don't know why he said that but I couldn't figure this out for a while. I'd appreciate your help. Thanks from now.",A B 2\times 3 A B \operatorname{rref}(A)+ \operatorname{rref}(B) = \operatorname{rref}(A+B) \operatorname{rref},"['linear-algebra', 'matrices']"
11,Traversing a maze from one cell to another,Traversing a maze from one cell to another,,"I have maze (10*10) as shown below: Each cell has unique value ranging from [0,15] which actually defines its walls (see the image below). For example, value of first cell (r=0, c=0) is 7 and so on. Now I need to travel from entry point to exit point. Entry point is always at the left side of maze which is created by removing any wall. Same is done on the right side of maze where the exit is. These points will be provided by indicating with an arrow in the image. I know how we actually traverse from one point to another inside rectangular grid. It contains values in binary form where $1$ signify block to that cell and $0$ signify that path is possible from that cell. PS: You will be only provided a 10*10 matrix with values filled accordingly and you will need to get row and column number as a path from entry point to exit point.","I have maze (10*10) as shown below: Each cell has unique value ranging from [0,15] which actually defines its walls (see the image below). For example, value of first cell (r=0, c=0) is 7 and so on. Now I need to travel from entry point to exit point. Entry point is always at the left side of maze which is created by removing any wall. Same is done on the right side of maze where the exit is. These points will be provided by indicating with an arrow in the image. I know how we actually traverse from one point to another inside rectangular grid. It contains values in binary form where signify block to that cell and signify that path is possible from that cell. PS: You will be only provided a 10*10 matrix with values filled accordingly and you will need to get row and column number as a path from entry point to exit point.",1 0,"['matrices', 'algorithms', 'puzzle']"
12,Limit of regularized inverse matrix,Limit of regularized inverse matrix,,"consider a (quadratic, but otherwise arbitrary) matrix $A$ . I am interested in the long-time behaviour of $$ r(t) = (I + t\cdot A)^{-1} r_0,$$ i.e. what is $\lim_{t\to\infty} r(t)$ ? An interesting arrangement (by inserting $(I+t\cdot A - t\cdot A)$ ) is the following: $$r(t) = r_0 - A(t^{-1}I + A)^{-1}r_0$$ The second part looks fairly similar to the limit definition of the Moore-Penrose pseudoinverse: Given a matrix $B$ , the M-P pseudoinverse is given by $B^+ = \lim_{t\to\infty} B^T(\frac1 t I + BB^T)$ . But as you can see, this doesn't exactly match this form (the problem being that A is not symmetric so it cannot be interpreted as a term of the form $BB^T$ ). My hypothesis is something along the lines of ""the components of $r$ in the kernel of $A$ stay constant, while the components in the orthogonal complement converge to 0"".","consider a (quadratic, but otherwise arbitrary) matrix . I am interested in the long-time behaviour of i.e. what is ? An interesting arrangement (by inserting ) is the following: The second part looks fairly similar to the limit definition of the Moore-Penrose pseudoinverse: Given a matrix , the M-P pseudoinverse is given by . But as you can see, this doesn't exactly match this form (the problem being that A is not symmetric so it cannot be interpreted as a term of the form ). My hypothesis is something along the lines of ""the components of in the kernel of stay constant, while the components in the orthogonal complement converge to 0"".","A  r(t) = (I + t\cdot A)^{-1} r_0, \lim_{t\to\infty} r(t) (I+t\cdot A - t\cdot A) r(t) = r_0 - A(t^{-1}I + A)^{-1}r_0 B B^+ = \lim_{t\to\infty} B^T(\frac1 t I + BB^T) BB^T r A","['linear-algebra', 'matrices', 'inverse', 'pseudoinverse']"
13,Does an identity exist for distributing the inverse for a product including nonsquare matrices?,Does an identity exist for distributing the inverse for a product including nonsquare matrices?,,"For example, if $A$ and $B$ are invertible square matrices, we can write $(AB)^{-1} = B^{-1} A^{-1}$ . Now, consider $A$ is an $n \times n$ matrix and $C$ is an $n \times m$ matrix. If $A$ is invertible, does an identity exist for distributing the inverse inside parenthesis of a product of matrices including a nonsquare matrix such as $C$ ? For example, if $(C^T A C)^{-1}$ exists, does some identity exist for $(C^T A C)^{-1}$ ?","For example, if and are invertible square matrices, we can write . Now, consider is an matrix and is an matrix. If is invertible, does an identity exist for distributing the inverse inside parenthesis of a product of matrices including a nonsquare matrix such as ? For example, if exists, does some identity exist for ?",A B (AB)^{-1} = B^{-1} A^{-1} A n \times n C n \times m A C (C^T A C)^{-1} (C^T A C)^{-1},"['linear-algebra', 'matrices', 'inverse']"
14,What is an eigenmatrix and what is its use?,What is an eigenmatrix and what is its use?,,"I stumbled upon a paper using the term eigenmatrix which I never heard of before. Sadly I found little to none literature to it. Even my books at home don't know the term. I believe that an eigenmatrix $E$ of some $(n\times n)$ matrix $A$ with eigenvalues $\lambda_1,\ldots,\lambda_n$ is of form: $E=\begin{align}\left( \begin{array}{rrrr} \lambda_1 & 0 & 0 \\ 0 & \ddots & 0  \\ 0 & 0 & \lambda_n  \end{array}\right)\end{align} $ where each $\lambda_i$ is represented as often as its multiplicity. If my definition up to this point is correct, my questions are: What is this matrix used for? How does it look like if $A$ doesn't have a full set of (real) eigenvalues? Do eigenvectors play any role, if so what? (I ask this primarily because the paper I read states that "" $E$ is an eigenmatrix of $A$ corresponding to an eigenvector."") How can one calculate an eigenmatrix of some square matrix $A$ ? If my definition wasn't correct, feel free to tell me what an eigenmatrix is.","I stumbled upon a paper using the term eigenmatrix which I never heard of before. Sadly I found little to none literature to it. Even my books at home don't know the term. I believe that an eigenmatrix of some matrix with eigenvalues is of form: where each is represented as often as its multiplicity. If my definition up to this point is correct, my questions are: What is this matrix used for? How does it look like if doesn't have a full set of (real) eigenvalues? Do eigenvectors play any role, if so what? (I ask this primarily because the paper I read states that "" is an eigenmatrix of corresponding to an eigenvector."") How can one calculate an eigenmatrix of some square matrix ? If my definition wasn't correct, feel free to tell me what an eigenmatrix is.","E (n\times n) A \lambda_1,\ldots,\lambda_n E=\begin{align}\left( \begin{array}{rrrr}
\lambda_1 & 0 & 0 \\
0 & \ddots & 0  \\
0 & 0 & \lambda_n 
\end{array}\right)\end{align}  \lambda_i A E A A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
15,Trace-logarithm inequality $\operatorname{tr}\log (A) \leq \operatorname{tr}(A-I)$ for matrices $A$ with strictly positive eigenvalues,Trace-logarithm inequality  for matrices  with strictly positive eigenvalues,\operatorname{tr}\log (A) \leq \operatorname{tr}(A-I) A,"I recently came across Klein's inequality , which states that for any Hermitian matrices $A, B$ of the same size and any differentiable concave function $f :(0,\infty) \to \mathbb R$ , we have $$\operatorname{Tr}\left[f(A)-f(B)-(A-B) f^{\prime}(B)\right] \leq 0$$ where $f(A)$ is the induced map defined on the eigenvalues and corresponding projectors $P$ as $f(A) \equiv \sum_{j} f\left(\lambda_{j}\right) P_{j},$ given the spectral decomposition $A=\sum_{j} \lambda_{j} P_{j}$ . When $f= \log$ and $B=I$ Klein's identity seems to give $$\operatorname{Tr}\left[\log(A)-(A-I)\right] \leq 0$$ This resembles the usual identity that we have in $1D$ namely, $\log x \leq x-1$ . Similarly, for any matrix $A$ satisfying $\|A-I\|<1$ in operator norm we have, by the power series expression of $\log$ around $I$ : $$\log (A)=\sum_{k=1}^{\infty}(-1)^{k+1} \frac{(A-I)^{k}}{k}=(A-I)-\frac{(A-I)^{2}}{2}+\frac{(A-I)^{3}}{3} \cdots$$ Thus we can see in this case as well that $\operatorname{Tr}\log (A) \leq \operatorname{Tr}(A-I)$ . Recall that a complex matrix has a logarithm if and only if it is invertible. When the matrix has no negative real eigenvalues, then there is a unique logarithm that has eigenvalues all lying in the strip $\{z \in \mathbf{C} \mid-\pi<\operatorname{lm} z<\pi\}.$ This logarithm is known as the principal logarithm. My question is: to what extent can we generalise this result? For example, does it hold that for any matrix $A$ with strictly positive eigenvalues we have $\operatorname{Tr}\log (A) \leq \operatorname{Tr}(A-I)$ ? If it helps we can add the requirement that $A$ is a product of two positive definite matrices.","I recently came across Klein's inequality , which states that for any Hermitian matrices of the same size and any differentiable concave function , we have where is the induced map defined on the eigenvalues and corresponding projectors as given the spectral decomposition . When and Klein's identity seems to give This resembles the usual identity that we have in namely, . Similarly, for any matrix satisfying in operator norm we have, by the power series expression of around : Thus we can see in this case as well that . Recall that a complex matrix has a logarithm if and only if it is invertible. When the matrix has no negative real eigenvalues, then there is a unique logarithm that has eigenvalues all lying in the strip This logarithm is known as the principal logarithm. My question is: to what extent can we generalise this result? For example, does it hold that for any matrix with strictly positive eigenvalues we have ? If it helps we can add the requirement that is a product of two positive definite matrices.","A, B f :(0,\infty) \to \mathbb R \operatorname{Tr}\left[f(A)-f(B)-(A-B) f^{\prime}(B)\right] \leq 0 f(A) P f(A) \equiv \sum_{j} f\left(\lambda_{j}\right) P_{j}, A=\sum_{j} \lambda_{j} P_{j} f= \log B=I \operatorname{Tr}\left[\log(A)-(A-I)\right] \leq 0 1D \log x \leq x-1 A \|A-I\|<1 \log I \log (A)=\sum_{k=1}^{\infty}(-1)^{k+1} \frac{(A-I)^{k}}{k}=(A-I)-\frac{(A-I)^{2}}{2}+\frac{(A-I)^{3}}{3} \cdots \operatorname{Tr}\log (A) \leq \operatorname{Tr}(A-I) \{z \in \mathbf{C} \mid-\pi<\operatorname{lm} z<\pi\}. A \operatorname{Tr}\log (A) \leq \operatorname{Tr}(A-I) A","['linear-algebra', 'matrices', 'inequality', 'logarithms', 'trace']"
16,Linear operators well-defined by matrices in $l^2$ are bounded,Linear operators well-defined by matrices in  are bounded,l^2,"Let $\{a_{jk}\}$ be  an infinite matrix such that corresponding mapping $$A:(x_i) \mapsto (\sum_{j=1}^\infty a_{ij}x_j)$$ is well defined linear operator $A:l^2\to l^2$ . I need  help with showing that this operator will be bounded. I guess it means that i need to check if a unit sphere maps to something bounded, so i need to manage to get some inequality on coefficients of matrix that will allow to write a straight sequence of inequalities and get desired bound. But I don't understand how to get bound from operator being well defined.","Let be  an infinite matrix such that corresponding mapping is well defined linear operator . I need  help with showing that this operator will be bounded. I guess it means that i need to check if a unit sphere maps to something bounded, so i need to manage to get some inequality on coefficients of matrix that will allow to write a straight sequence of inequalities and get desired bound. But I don't understand how to get bound from operator being well defined.",\{a_{jk}\} A:(x_i) \mapsto (\sum_{j=1}^\infty a_{ij}x_j) A:l^2\to l^2,"['matrices', 'functional-analysis']"
17,Confused with this SVD problem: Does it matter which singular vectors you choose?,Confused with this SVD problem: Does it matter which singular vectors you choose?,,"I am trying to decompose the following matrix using the Singular Value Decomposition (SVD): $$A = \begin{bmatrix}     4 & 4\\     -3 & 3\\     \end{bmatrix} = U\Sigma V^T$$ Here is my work (I know this is far from the most efficient way to do SVD, but please follow along my way): Finding $\Sigma$ and $V$ : $$A^T A = \begin{bmatrix}25 & 7\\7 & 25\\\end{bmatrix} \quad\text{with } \lambda_1 = 32, v_1 = \begin{bmatrix}1/\sqrt2\\1/\sqrt2\\\end{bmatrix} \quad\text{and } \lambda_2 = 18, v_2 = \begin{bmatrix}1/\sqrt2\\-1/\sqrt2\\\end{bmatrix}$$ So, $$V = \begin{bmatrix}1/\sqrt2 & 1/\sqrt2\\1/\sqrt2 & -1/\sqrt2\\\end{bmatrix} \quad \text{and} \quad \Sigma = \begin{bmatrix} \sqrt{32} & 0 \\ 0 & \sqrt{18}\\\end{bmatrix} $$ Finding $U$ : $$A A^T = \begin{bmatrix}32 & 0\\0 & 18\\\end{bmatrix} \quad\text{with } \lambda_1 = 32, u_1 = \begin{bmatrix}1\\0\\\end{bmatrix} \quad\text{and } \lambda_2 = 18, u_2 = \begin{bmatrix}0\\1\\\end{bmatrix}$$ So, $$U = \begin{bmatrix}32 & 0\\0 & 18\\\end{bmatrix} $$ However, $$U \Sigma V^T = \begin{bmatrix}4 & 4\\3 & -3\\\end{bmatrix} \neq A$$ Did I do something wrong? Next attempt: This time, I used $u_2 = \begin{bmatrix}0\\-1\\\end{bmatrix}$ instead of $\begin{bmatrix}0\\1\\\end{bmatrix}$ . So, $U = \begin{bmatrix}1 & 0\\0 & -1\\\end{bmatrix}$ . Now, it seems to work: $$U \Sigma V^T = \begin{bmatrix}4 & 4\\-3 & 3\\\end{bmatrix} = A.$$ So my question is: Does it matter which singular vectors you choose for $U$ and $V$ ? In other words, if you find a singular vector $x$ with unit length, how do you know to choose $x$ or $-x$ ? I know that in  Eigenvalue decomposition, it didn't matter because you can change the diagonal matrix $\Lambda$ accordingly. What about in SVD?","I am trying to decompose the following matrix using the Singular Value Decomposition (SVD): Here is my work (I know this is far from the most efficient way to do SVD, but please follow along my way): Finding and : So, Finding : So, However, Did I do something wrong? Next attempt: This time, I used instead of . So, . Now, it seems to work: So my question is: Does it matter which singular vectors you choose for and ? In other words, if you find a singular vector with unit length, how do you know to choose or ? I know that in  Eigenvalue decomposition, it didn't matter because you can change the diagonal matrix accordingly. What about in SVD?","A = \begin{bmatrix}
    4 & 4\\
    -3 & 3\\
    \end{bmatrix} = U\Sigma V^T \Sigma V A^T A = \begin{bmatrix}25 & 7\\7 & 25\\\end{bmatrix} \quad\text{with } \lambda_1 = 32, v_1 = \begin{bmatrix}1/\sqrt2\\1/\sqrt2\\\end{bmatrix} \quad\text{and } \lambda_2 = 18, v_2 = \begin{bmatrix}1/\sqrt2\\-1/\sqrt2\\\end{bmatrix} V = \begin{bmatrix}1/\sqrt2 & 1/\sqrt2\\1/\sqrt2 & -1/\sqrt2\\\end{bmatrix} \quad \text{and} \quad \Sigma = \begin{bmatrix} \sqrt{32} & 0 \\ 0 & \sqrt{18}\\\end{bmatrix}  U A A^T = \begin{bmatrix}32 & 0\\0 & 18\\\end{bmatrix} \quad\text{with } \lambda_1 = 32, u_1 = \begin{bmatrix}1\\0\\\end{bmatrix} \quad\text{and } \lambda_2 = 18, u_2 = \begin{bmatrix}0\\1\\\end{bmatrix} U = \begin{bmatrix}32 & 0\\0 & 18\\\end{bmatrix}  U \Sigma V^T = \begin{bmatrix}4 & 4\\3 & -3\\\end{bmatrix} \neq A u_2 = \begin{bmatrix}0\\-1\\\end{bmatrix} \begin{bmatrix}0\\1\\\end{bmatrix} U = \begin{bmatrix}1 & 0\\0 & -1\\\end{bmatrix} U \Sigma V^T = \begin{bmatrix}4 & 4\\-3 & 3\\\end{bmatrix} = A. U V x x -x \Lambda","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'svd']"
18,Understanding a computer vision gravity vector detection algorithm/ Solving for an eigenvector,Understanding a computer vision gravity vector detection algorithm/ Solving for an eigenvector,,"Supposing we have a depth picture where objects from the picture have estimates $v \in \Bbb R^3$ of their surface normals, vectors on walls that point horizontally, and vectors on flat surfaces that point either up to the sky or down to the ground. A computer vision algorithm I am reading about available here on page $2$ , right side of the page, explores an iterative algorithm to estimate a gravity vector. The paper is titled Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images . At step $i$ , after obtaining an estimate for our vector, say $g_i$ , we classify our local estimates of surface normal estimates from an image into two categories: $N_{||}$ , the set of vectors within an angle-error $d$ of being parallel to $g_i$ $N_{\perp}$ , the set of vectors within an angle-error $d$ of being perpendicular to $g_i$ , where initially $d$ is broad enough that almost all surface normals will belong to one of the two categories (but iteratively $d$ decreases). The algorithm treats the concatenation of the two categories of surface normals as column vectors of the corresponding matrices $M_{||}, M_{\perp}$ . Next, we solve for a new estimate $g_i^{*}$ , where the authors state without proof that solving for $\min_{ \{g:\|g\|_2=1 \}}\left(\sum\limits_{n \in N_\perp} \cos^2(\theta(n,g)) + \sum\limits_{n \in N_{\|}}\sin^2(\theta(n,g))\right)$ is equivalent to finding the eigenvector with the smallest eigenvalue of the $3 \times 3$ matrix: $M_\perp M_\perp^T- M_{\|}M_{\|}^T$ , where $\theta(n,g)$ is the angle between vectors $n,g \in \Bbb R^3$ . The first expression makes perfect sense, we are computing an optimal unit vector that is perpendicular to our horizontal surface normals, and parallel to the up facing surface normals. I am not sure why solving for the eigenvector solution with minimal eigenvalue is equivalent, and after some attempts manually to write examples I have not progressed any further. Any insights appreciated.","Supposing we have a depth picture where objects from the picture have estimates of their surface normals, vectors on walls that point horizontally, and vectors on flat surfaces that point either up to the sky or down to the ground. A computer vision algorithm I am reading about available here on page , right side of the page, explores an iterative algorithm to estimate a gravity vector. The paper is titled Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images . At step , after obtaining an estimate for our vector, say , we classify our local estimates of surface normal estimates from an image into two categories: , the set of vectors within an angle-error of being parallel to , the set of vectors within an angle-error of being perpendicular to , where initially is broad enough that almost all surface normals will belong to one of the two categories (but iteratively decreases). The algorithm treats the concatenation of the two categories of surface normals as column vectors of the corresponding matrices . Next, we solve for a new estimate , where the authors state without proof that solving for is equivalent to finding the eigenvector with the smallest eigenvalue of the matrix: , where is the angle between vectors . The first expression makes perfect sense, we are computing an optimal unit vector that is perpendicular to our horizontal surface normals, and parallel to the up facing surface normals. I am not sure why solving for the eigenvector solution with minimal eigenvalue is equivalent, and after some attempts manually to write examples I have not progressed any further. Any insights appreciated.","v \in \Bbb R^3 2 i g_i N_{||} d g_i N_{\perp} d g_i d d M_{||}, M_{\perp} g_i^{*} \min_{ \{g:\|g\|_2=1 \}}\left(\sum\limits_{n \in N_\perp} \cos^2(\theta(n,g)) + \sum\limits_{n \in N_{\|}}\sin^2(\theta(n,g))\right) 3 \times 3 M_\perp M_\perp^T- M_{\|}M_{\|}^T \theta(n,g) n,g \in \Bbb R^3","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'computer-vision']"
19,Why do we need scalar-by-matrix derivative?,Why do we need scalar-by-matrix derivative?,,"We all know that there are such types of derivative: And scalar-by-matrix derivative $\frac{\partial y}{\partial \textbf{X}}  $ is defined as follow: \begin{pmatrix} \partial y / \partial x_{11} & \partial y / \partial x_{21} & \ldots  &\partial y / \partial x_{p1}\\ \partial y / \partial x_{12} & \partial y / \partial x_{22} & \ldots &\partial y / \partial x_{p2}\\ \vdots & \vdots & \ddots & \vdots \\ \partial y / \partial x_{1q} & \partial y / \partial x_{2q} & \ldots &\partial y / \partial x_{pq}\\ \end{pmatrix} So, why do we need this if we can just put all variables of $y$ in vector and take scalar-by-vector derivative? Probably, I failed to get the main idea about scalar-by-matrix derivative, can you give me some numerical example? I can't find any concrete example in any textbooks.","We all know that there are such types of derivative: And scalar-by-matrix derivative is defined as follow: So, why do we need this if we can just put all variables of in vector and take scalar-by-vector derivative? Probably, I failed to get the main idea about scalar-by-matrix derivative, can you give me some numerical example? I can't find any concrete example in any textbooks.","\frac{\partial y}{\partial \textbf{X}}   \begin{pmatrix}
\partial y / \partial x_{11} & \partial y / \partial x_{21} & \ldots  &\partial y / \partial x_{p1}\\
\partial y / \partial x_{12} & \partial y / \partial x_{22} & \ldots &\partial y / \partial x_{p2}\\
\vdots & \vdots & \ddots & \vdots \\
\partial y / \partial x_{1q} & \partial y / \partial x_{2q} & \ldots &\partial y / \partial x_{pq}\\
\end{pmatrix} y","['matrices', 'derivatives', 'matrix-calculus']"
20,Closest matrix that achieves positive semidefinite condition,Closest matrix that achieves positive semidefinite condition,,"Suppose we have two symmetric positive semidefinite $n$ dimensional matrices $A$ and $B$ . We use the notation $X\leq Y$ means that $Y-X$ is positive semidefinite. Suppose $A \not\leq B$ i.e. $B-A$ has at least one negative eigenvalue. We are interested in perturbing $A$ to some positive semidefinite $\tilde{A}$ such that $\tilde{A} \leq B$ while minimizing $|A-\tilde{A}|_1$ where $|\cdot|_1$ is the nuclear norm and defined by $$|X|_1 := \text{Tr} \left( \sqrt{X^\dagger X} \right)$$ and $X^\dagger$ is the transpose conjugate of $X$ . To make things simpler, I will now consider the case where $A$ is a rank- $1$ matrix. Is it true that $$\tilde{A} = \lambda A$$ for some $\lambda < 1$ ? An immediate corollary is that $\tilde{A}\leq A$ . EDIT: After a bit of searching, I found a result for the same question but where the norm considered is the induced 2-norm (spectral norm) or the Frobenius norm. For the induced 2-norm (spectral norm), it holds that $\tilde{A} = A - \lambda I$ where $\lambda$ is the smallest positive number such that $\tilde{A}\leq B$ is true. So for this case, my conjecture that $\tilde{A} = \lambda A$ is false but the statement $\tilde{A}\leq A$ is true. For the Frobenius norm case, we first write the polar decomposition of $B-A = UH$ . Then $B -\tilde{A} = \frac{1}{2}(B - A + H)$ is the solution. Since $H= ((B-A)^\dagger(B-A))^{1/2}\geq B-A$ , one can again conclude that $\tilde{A}\leq A$ I do not know what happens for the 1-norm though. EDIT 2: Here is another look at the problem that almost works. Suppose the solution $\tilde{A}\not\leq A$ . We prove that there exists some $A'$ such that $A'\leq B, A'\leq A$ and $|A'-A|_1\leq|\tilde{A}-A|_1$ . Let us diagonalize $\tilde{A}-A = ZDZ^\dagger = ZD^{+}Z^\dagger + ZD^{-}Z^\dagger$ where $D$ is diagonal, $D^{\pm}$ is also diagonal and includes the nonnegative and negative eigenvalues respectively. By assumption $\tilde{A}\leq B \implies A + ZD^{+}Z^\dagger + ZD^{-}Z^\dagger \leq B$ . Define $A':= A + ZD^{-}Z^\dagger$ . Since $ZD^{+}Z^\dagger$ is positive semidefinite, it holds that $A' = A + ZD^{-}Z^\dagger \leq B$ . Since $ZD^{-}Z^\dagger$ is negative definite, it follows that $A'\leq A$ . Finally, $|A' - A|_1 = |ZD^{-}Z^\dagger|_1 = |D^{-}|_1 \leq |D^{+}+D^{-}|_1 =  |Z(D^{+}+D^{-})Z^\dagger|_1 = |\tilde{A} - A|_1$ EDIT 3 Unfortunately, the $A'$ constructed is not positive semidefinite in general.","Suppose we have two symmetric positive semidefinite dimensional matrices and . We use the notation means that is positive semidefinite. Suppose i.e. has at least one negative eigenvalue. We are interested in perturbing to some positive semidefinite such that while minimizing where is the nuclear norm and defined by and is the transpose conjugate of . To make things simpler, I will now consider the case where is a rank- matrix. Is it true that for some ? An immediate corollary is that . EDIT: After a bit of searching, I found a result for the same question but where the norm considered is the induced 2-norm (spectral norm) or the Frobenius norm. For the induced 2-norm (spectral norm), it holds that where is the smallest positive number such that is true. So for this case, my conjecture that is false but the statement is true. For the Frobenius norm case, we first write the polar decomposition of . Then is the solution. Since , one can again conclude that I do not know what happens for the 1-norm though. EDIT 2: Here is another look at the problem that almost works. Suppose the solution . We prove that there exists some such that and . Let us diagonalize where is diagonal, is also diagonal and includes the nonnegative and negative eigenvalues respectively. By assumption . Define . Since is positive semidefinite, it holds that . Since is negative definite, it follows that . Finally, EDIT 3 Unfortunately, the constructed is not positive semidefinite in general.","n A B X\leq Y Y-X A \not\leq B B-A A \tilde{A} \tilde{A} \leq B |A-\tilde{A}|_1 |\cdot|_1 |X|_1 := \text{Tr} \left( \sqrt{X^\dagger X} \right) X^\dagger X A 1 \tilde{A} = \lambda A \lambda < 1 \tilde{A}\leq A \tilde{A} = A - \lambda I \lambda \tilde{A}\leq B \tilde{A} = \lambda A \tilde{A}\leq A B-A = UH B -\tilde{A} = \frac{1}{2}(B - A + H) H= ((B-A)^\dagger(B-A))^{1/2}\geq B-A \tilde{A}\leq A \tilde{A}\not\leq A A' A'\leq B, A'\leq A |A'-A|_1\leq|\tilde{A}-A|_1 \tilde{A}-A = ZDZ^\dagger = ZD^{+}Z^\dagger + ZD^{-}Z^\dagger D D^{\pm} \tilde{A}\leq B \implies A + ZD^{+}Z^\dagger + ZD^{-}Z^\dagger \leq B A':= A + ZD^{-}Z^\dagger ZD^{+}Z^\dagger A' = A + ZD^{-}Z^\dagger \leq B ZD^{-}Z^\dagger A'\leq A |A' - A|_1 = |ZD^{-}Z^\dagger|_1 = |D^{-}|_1 \leq |D^{+}+D^{-}|_1 =  |Z(D^{+}+D^{-})Z^\dagger|_1 = |\tilde{A} - A|_1 A'","['linear-algebra', 'matrices', 'positive-semidefinite', 'nuclear-norm']"
21,How to calculate the coefficients in this matrix decomposition,How to calculate the coefficients in this matrix decomposition,,"This recent answer contained an interesting Kronecker decomposition of the form $$\eqalign{ &A = \sum_{i,j} C_{ij}\otimes E_{ij} \;\in\;{\mathbb R}^{mp\times nq} \\ &C_{ij} \in{\mathbb R}^{m\times n}\quad\big({\rm Coefficient\,Matrices}\big) \\ &E_{ij} \in{\mathbb R}^{p\times q}\;\quad\big({\rm Standard\,Basis\,Matrices}\big) }$$ This decomposition has two trivial cases. When $m=n=1$ , the coefficients are simply scalars equal to the components of the matrix $$C_{ij}=A_{ij}$$ When $p=q=1$ , then there is only one matrix-valued coefficient equal to the whole matrix $$C_{11}=A$$ But what is the algorithm/formula to calculate the coefficient matrices in the general case?","This recent answer contained an interesting Kronecker decomposition of the form This decomposition has two trivial cases. When , the coefficients are simply scalars equal to the components of the matrix When , then there is only one matrix-valued coefficient equal to the whole matrix But what is the algorithm/formula to calculate the coefficient matrices in the general case?","\eqalign{
&A = \sum_{i,j} C_{ij}\otimes E_{ij} \;\in\;{\mathbb R}^{mp\times nq} \\
&C_{ij} \in{\mathbb R}^{m\times n}\quad\big({\rm Coefficient\,Matrices}\big) \\
&E_{ij} \in{\mathbb R}^{p\times q}\;\quad\big({\rm Standard\,Basis\,Matrices}\big)
} m=n=1 C_{ij}=A_{ij} p=q=1 C_{11}=A","['linear-algebra', 'matrices', 'matrix-decomposition']"
22,"Is there a $4$-by-$4$, rank $3$, positive semidefinite matrix with $a_{ii}=3$, $|a_{12}|\neq 1$, and principal minors having minimal eigenvalue $1$?","Is there a -by-, rank , positive semidefinite matrix with , , and principal minors having minimal eigenvalue ?",4 4 3 a_{ii}=3 |a_{12}|\neq 1 1,"Could anyone help me search for a positive semidefinite matrix $\left(a_{i,j}\right)_{4\times4}$ of rank 3 with $a_{i,i}=3$ and its all 3 by 3 principal minor matrices having minimal eigenvalue $\lambda_{\min}=1$ , but $\left|a_{1,2}\right|\ne1$ , or could anyone explain why such a matrix would not exist? Thanks a lot.","Could anyone help me search for a positive semidefinite matrix of rank 3 with and its all 3 by 3 principal minor matrices having minimal eigenvalue , but , or could anyone explain why such a matrix would not exist? Thanks a lot.","\left(a_{i,j}\right)_{4\times4} a_{i,i}=3 \lambda_{\min}=1 \left|a_{1,2}\right|\ne1","['linear-algebra', 'matrices', 'positive-semidefinite', 'combinatorial-geometry', 'discrete-geometry']"
23,Relation of row sums to largest eigenvalue,Relation of row sums to largest eigenvalue,,"I know that the largest eigenvalue of a graph is bounded between the minimal and maximal row sum of the matrix. If I have a $0-1$ symetric matrix (an adjacency matrix) and I know $k$ of the rows have at least $k$ zeros in them (more specifically, I know $k$ is the maximum size of an all zeros principal sub-matrix of the adjacency matrix), is there anything else I can tell about the largest eigenvalue (or others eigenvalues)? Is there anything I can tell about the eigenvalues which implies I have that kind of principal sub-matrix? Thanks!","I know that the largest eigenvalue of a graph is bounded between the minimal and maximal row sum of the matrix. If I have a symetric matrix (an adjacency matrix) and I know of the rows have at least zeros in them (more specifically, I know is the maximum size of an all zeros principal sub-matrix of the adjacency matrix), is there anything else I can tell about the largest eigenvalue (or others eigenvalues)? Is there anything I can tell about the eigenvalues which implies I have that kind of principal sub-matrix? Thanks!",0-1 k k k,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-graph-theory']"
24,Infinity norm of matrix defined using integral of Lagrange polynomials.,Infinity norm of matrix defined using integral of Lagrange polynomials.,,"Fix $n \geq 2$ an integer and let $x_{k+1} = k/n , k = 0, 1,\cdots, n-1.$ Let now $L_k$ be the Lagrange polynomial given by $$L_k(x) = \prod_{j =1, j \neq k}^{n} \frac{x-x_j}{x_k - x_j}.$$ Define the matrix $A = [a_{i,j}]$ whose entries are given by $$a_{i,j} := \int_{0}^{x_i}L_j(x)\, dx.$$ I'm interested in finding the infinity norm of $A$ $$\|A\|_\infty := \max_{ 1\leq i\leq n} \sum_{j=1}^n |a_{i,j}|.$$ My attempt: It seems (I used Mathematica for $n = 2, ..., 8$ ) that the maximum is reached in the last line of the matrix where we have $a_{n,j} \geq 0.$ Thus,  we have $$\|A\|_\infty = \sum_{j=1}^n |a_{n,j}| = \sum_{j=1}^n\int_{x_1}^{x_n}L_j(x).$$ It's clear that $\sum_{j=1}^nL_j(x) =1.$ Then, we have $$\|A\|_\infty = n-1.$$ Thank you for any hint.","Fix an integer and let Let now be the Lagrange polynomial given by Define the matrix whose entries are given by I'm interested in finding the infinity norm of My attempt: It seems (I used Mathematica for ) that the maximum is reached in the last line of the matrix where we have Thus,  we have It's clear that Then, we have Thank you for any hint.","n \geq 2 x_{k+1} = k/n , k = 0, 1,\cdots, n-1. L_k L_k(x) = \prod_{j =1, j \neq k}^{n} \frac{x-x_j}{x_k - x_j}. A = [a_{i,j}] a_{i,j} := \int_{0}^{x_i}L_j(x)\, dx. A \|A\|_\infty := \max_{ 1\leq i\leq n} \sum_{j=1}^n |a_{i,j}|. n = 2, ..., 8 a_{n,j} \geq 0. \|A\|_\infty = \sum_{j=1}^n |a_{n,j}| = \sum_{j=1}^n\int_{x_1}^{x_n}L_j(x). \sum_{j=1}^nL_j(x) =1. \|A\|_\infty = n-1.","['linear-algebra', 'matrices', 'polynomials']"
25,Derivative of matrix exponential $\exp(A+xB)$ at $x=0$,Derivative of matrix exponential  at,\exp(A+xB) x=0,"Consider two (Hermitian) matrices $A$ and $B$ . Is there a nice expression for  the following? $$ \boxed{ \frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0} = \; ? }$$ Of course, if $A$ and $B$ commute, this is simply $B \exp{(A)}$ . One thing I tried was the Suzuki-Trotter formula: \begin{align} \boxed{\frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0}}  &= \frac{\mathrm d}{\mathrm d x} \left. \left( \lim_{N \to \infty} \left[ \exp\left( \frac{A}{N} \right) \exp \left( x \frac{B}{N} \right) \right]^N \right) \right|_{x=0} \\ &= \lim_{N\to \infty}  \sum_{n=1}^N \exp\left( \frac{n}{N} A \right) \frac{B}{N}  \exp\left( \frac{N-n}{N} A \right) \\ &= \left( \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^N e^{\frac{n}{N}A }B\; e^{-\frac{n}{N}A } \right)  e^A \\\ &= \boxed{ \int_0^1 e^{t A} B \;e^{(1-t)A} \; \mathrm d t } \; . \end{align} Is this as close as it gets to a closed form? One thing we can do is go to the eigenbasis of $A$ , such that we can explicitly perform the integration over $t$ . If we index the eigenvectors of $A$ by $i$ , with corresponding eigenvalues $\lambda_i$ , then we can express the answer in this basis: \begin{equation} \boxed{ \left( \frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0} \right)_{ij} = \frac{e^{\lambda_i}-e^{\lambda_j}}{\lambda_i-\lambda_j} B_{ij}} \;, \end{equation} where $(\cdot)_{ij}$ are the entries of a matrix in the eigenbasis of $A$ . (Note that if $\lambda_i = \lambda_j$ , we replace $\frac{e^{\lambda_i}-e^{\lambda_j}}{\lambda_i-\lambda_j} \to e^{\lambda_i}$ , which is also consistent with l'Hopital's rule.)","Consider two (Hermitian) matrices and . Is there a nice expression for  the following? Of course, if and commute, this is simply . One thing I tried was the Suzuki-Trotter formula: Is this as close as it gets to a closed form? One thing we can do is go to the eigenbasis of , such that we can explicitly perform the integration over . If we index the eigenvectors of by , with corresponding eigenvalues , then we can express the answer in this basis: where are the entries of a matrix in the eigenbasis of . (Note that if , we replace , which is also consistent with l'Hopital's rule.)","A B  \boxed{ \frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0} = \; ? } A B B \exp{(A)} \begin{align}
\boxed{\frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0}} 
&= \frac{\mathrm d}{\mathrm d x} \left. \left( \lim_{N \to \infty} \left[ \exp\left( \frac{A}{N} \right) \exp \left( x \frac{B}{N} \right) \right]^N \right) \right|_{x=0} \\
&= \lim_{N\to \infty}  \sum_{n=1}^N \exp\left( \frac{n}{N} A \right) \frac{B}{N}  \exp\left( \frac{N-n}{N} A \right) \\
&= \left( \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^N e^{\frac{n}{N}A }B\; e^{-\frac{n}{N}A } \right)  e^A \\\
&= \boxed{ \int_0^1 e^{t A} B \;e^{(1-t)A} \; \mathrm d t } \; .
\end{align} A t A i \lambda_i \begin{equation}
\boxed{ \left( \frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0} \right)_{ij} = \frac{e^{\lambda_i}-e^{\lambda_j}}{\lambda_i-\lambda_j} B_{ij}} \;,
\end{equation} (\cdot)_{ij} A \lambda_i = \lambda_j \frac{e^{\lambda_i}-e^{\lambda_j}}{\lambda_i-\lambda_j} \to e^{\lambda_i}","['integration', 'matrices', 'derivatives', 'matrix-calculus', 'matrix-exponential']"
26,Homomorphisms between matrix groups,Homomorphisms between matrix groups,,"I was having a discussion with my friend, who is a physics major, but nevertheless enjoys taking math courses. He believes that a non-trivial homomorphism as I describe below cannot exist. My gut tells me that that is not true, but I am not able to find an example. Could anyone define a group homomorphism from a matrix group that is a subgroup of $GL_n(\mathbb{Z}[x, x^{-1}])$ to a matrix group that is a subgroup of $GL_n(\mathbb{Z})$ ?","I was having a discussion with my friend, who is a physics major, but nevertheless enjoys taking math courses. He believes that a non-trivial homomorphism as I describe below cannot exist. My gut tells me that that is not true, but I am not able to find an example. Could anyone define a group homomorphism from a matrix group that is a subgroup of to a matrix group that is a subgroup of ?","GL_n(\mathbb{Z}[x, x^{-1}]) GL_n(\mathbb{Z})","['linear-algebra', 'matrices', 'group-theory', 'group-homomorphism']"
27,Convex maximization problem (existence and uniqueness),Convex maximization problem (existence and uniqueness),,"Short summary: Let $n > k \geq 2$ . I am given an arbitrary full column rank $n \times k$ matrix $W$ whose rows sum up to $1$ . I am trying to find a square matrix $R$ such that $W R$ has certain properties: component-wise, it should be  non-negative and each of its rows should sum up to $1$ . $R$ maximizes the sum of column-wise variances of $W R$ . Does such an $R$ exist? If so, is it unique up to permutation of columns? Detailed statement of the problem: Let $W\in\mathbb{R}^{n \times k}$ (where $n > k$ ) be a full-rank matrix which fulfills $W\mathbb{1}_k = \mathbb{1}_n$ .  Here, $\mathbb{1}_k$ is the vector of ones with length $k$ . Let $f_W: \mathbb{R}^{k\times k}\rightarrow \mathbb{R}$ be the sum of column-wise variances of $W R$ , i.e., $$f_W(R) = \sum\limits_{l=1}^k \frac{1}{n-1} \sum\limits_{i=1}^n \left(\left(W R\right)_{i,l} - \frac{1}{n} \sum\limits_{j=1}^n \left(W R\right)_{j,l}\right)^2 $$ If I calculated correctly, the Hessian of each of the $k$ summands of $f_W$ equals $2$ times the covariance matrix of $W$ , and therefore $f_W$ is convex as the sum of convex functions. I am wondering whether the optimization problem $$\begin{array}{ll} \text{maximize} & f_W(R)\\ \text{subject to} & R\mathbb{1}_k = \mathbb{1}_k\\ & W R \geq 0\end{array}$$ where the inequality is component-wise, has a solution and whether this solution is unique up to permutation of columns. I think the answer is ""yes"" if $k=2$ , but I have not been able to say much about the general case $k > 2$ . Can anyone give me a hint on how to tackle this? Two other expressions for the objective function: We can also write $f_W$ as $$f_W(R) = \frac{1}{n-1} \left(\sum\limits_{i=1}^n \sum\limits_{l=1}^k \left(\left(WR\right)_{i,l} - \frac{1}{k}\right)^2 - n\sum\limits_{l=1}^k \left(\frac{1}{n} \sum\limits_{j=1}^n \left(W R\right)_{j,l} - \frac{1}{k}\right)^2\right).$$ So, to maximize $f_W$ , the entries of $WR$ should be as far from $\frac{1}{k}$ as possible under the constraints while its column means should be close to $\frac{1}{k}$ . Another expression for $f_W$ is $$f_W(R) = \frac{1}{n-1} \left(\|WR\|^2 - \frac{1}{n}\sum\limits_{l=1}^k \left(\sum\limits_{j=1}^n \left(WR\right)_{j,l}\right)^2\right) = \frac{1}{n-1} \left(\|WR\|^2 - \frac{1}{n} \|\mathbb{1}^\top_n WR\|^2\right),$$ where $\|\cdot\|$ denotes the Frobenius norm.","Short summary: Let . I am given an arbitrary full column rank matrix whose rows sum up to . I am trying to find a square matrix such that has certain properties: component-wise, it should be  non-negative and each of its rows should sum up to . maximizes the sum of column-wise variances of . Does such an exist? If so, is it unique up to permutation of columns? Detailed statement of the problem: Let (where ) be a full-rank matrix which fulfills .  Here, is the vector of ones with length . Let be the sum of column-wise variances of , i.e., If I calculated correctly, the Hessian of each of the summands of equals times the covariance matrix of , and therefore is convex as the sum of convex functions. I am wondering whether the optimization problem where the inequality is component-wise, has a solution and whether this solution is unique up to permutation of columns. I think the answer is ""yes"" if , but I have not been able to say much about the general case . Can anyone give me a hint on how to tackle this? Two other expressions for the objective function: We can also write as So, to maximize , the entries of should be as far from as possible under the constraints while its column means should be close to . Another expression for is where denotes the Frobenius norm.","n > k \geq 2 n \times k W 1 R W R 1 R W R R W\in\mathbb{R}^{n \times k} n > k W\mathbb{1}_k = \mathbb{1}_n \mathbb{1}_k k f_W: \mathbb{R}^{k\times k}\rightarrow \mathbb{R} W R f_W(R) = \sum\limits_{l=1}^k \frac{1}{n-1} \sum\limits_{i=1}^n \left(\left(W R\right)_{i,l} - \frac{1}{n} \sum\limits_{j=1}^n \left(W R\right)_{j,l}\right)^2  k f_W 2 W f_W \begin{array}{ll} \text{maximize} & f_W(R)\\ \text{subject to} & R\mathbb{1}_k = \mathbb{1}_k\\ & W R \geq 0\end{array} k=2 k > 2 f_W f_W(R) = \frac{1}{n-1} \left(\sum\limits_{i=1}^n \sum\limits_{l=1}^k \left(\left(WR\right)_{i,l} - \frac{1}{k}\right)^2 - n\sum\limits_{l=1}^k \left(\frac{1}{n} \sum\limits_{j=1}^n \left(W R\right)_{j,l} - \frac{1}{k}\right)^2\right). f_W WR \frac{1}{k} \frac{1}{k} f_W f_W(R) = \frac{1}{n-1} \left(\|WR\|^2 - \frac{1}{n}\sum\limits_{l=1}^k \left(\sum\limits_{j=1}^n \left(WR\right)_{j,l}\right)^2\right) = \frac{1}{n-1} \left(\|WR\|^2 - \frac{1}{n} \|\mathbb{1}^\top_n WR\|^2\right), \|\cdot\|","['matrices', 'optimization']"
28,dimension of column space and null space,dimension of column space and null space,,"I am trying to answer a series of questions given the dimension of a matrix. Suppose that $A$ is a $6 \times 12$ matrix. The column space is a subspace of $\mathbb{R}^n$ . What is n? $n = 6$ because there can only be 6 pivot columns. The null space is a subspace of $\mathbb{R}^m$ . What is m? $m = 12$ ? Not so sure about this question. Is it possible to have rank = 4, dimension of null space = 8? $rank \leq min(m,n)$ for $m \times n$ matrix, rank + nullity = number of columns. It is possible. Is it possible to have rank = 8, dimension of null space = 4? rank + nullity = number of columns but $rank \nleq min(m,n)$ . It is not possible. Are my answers valid for the three questions that I answered? I am a bit confused with the second question. Any help would be great. Thank you for reading.","I am trying to answer a series of questions given the dimension of a matrix. Suppose that is a matrix. The column space is a subspace of . What is n? because there can only be 6 pivot columns. The null space is a subspace of . What is m? ? Not so sure about this question. Is it possible to have rank = 4, dimension of null space = 8? for matrix, rank + nullity = number of columns. It is possible. Is it possible to have rank = 8, dimension of null space = 4? rank + nullity = number of columns but . It is not possible. Are my answers valid for the three questions that I answered? I am a bit confused with the second question. Any help would be great. Thank you for reading.","A 6 \times 12 \mathbb{R}^n n = 6 \mathbb{R}^m m = 12 rank \leq min(m,n) m \times n rank \nleq min(m,n)",['linear-algebra']
29,Positive semidefiniteness of block matrix when diagonal blocks are not invertible,Positive semidefiniteness of block matrix when diagonal blocks are not invertible,,"Let $$M =\left[\begin{array}{cc} A & B\\ B^{T} & D\end{array}\right]$$ where blocks $A$ and $D$ are not invertible, but both are positive semidefinite. Are there conditions such that $M$ is positive semi-definite? For example, consider the case where $$A=\left[\begin{array}{cc} a & -a\\-a & a\end{array}\right], \qquad B=\left[\begin{array}{cc} b_{1} & b_{2}\\ b_{2} & b_{1}\end{array}\right], \qquad D=\left[\begin{array}{cc}0 & 0\\0 & 0\end{array}\right]$$ where $a, b_1, b_2 \in \Bbb R$ . I read that if $D$ is invertible and $A-BD^{-1}B^T$ is positive semidefinite then $M$ is positive semidefinite (using Schur complements). I am wondering if there is a way to show positive semidefiniteness when neither $A$ nor $D$ is invertible; especially, when the matrices $A$ , $B$ , and $D$ could be written in the form given in the example.","Let where blocks and are not invertible, but both are positive semidefinite. Are there conditions such that is positive semi-definite? For example, consider the case where where . I read that if is invertible and is positive semidefinite then is positive semidefinite (using Schur complements). I am wondering if there is a way to show positive semidefiniteness when neither nor is invertible; especially, when the matrices , , and could be written in the form given in the example.","M =\left[\begin{array}{cc} A & B\\ B^{T} & D\end{array}\right] A D M A=\left[\begin{array}{cc} a & -a\\-a & a\end{array}\right], \qquad B=\left[\begin{array}{cc} b_{1} & b_{2}\\ b_{2} & b_{1}\end{array}\right], \qquad D=\left[\begin{array}{cc}0 & 0\\0 & 0\end{array}\right] a, b_1, b_2 \in \Bbb R D A-BD^{-1}B^T M A D A B D","['matrices', 'positive-definite', 'positive-semidefinite', 'block-matrices', 'schur-complement']"
30,Creating an Gaussian Orthogonal Matrix,Creating an Gaussian Orthogonal Matrix,,"I am working on a project where I need to create a special type of orthogonal matrix, such that all of the rows and all columns are orthogonal, but each entry in the matrix is drawn from a Gaussian distribution with a mean of $0$ and variance of $1$ . Is this possible, and if so what would be an easy way to do it?","I am working on a project where I need to create a special type of orthogonal matrix, such that all of the rows and all columns are orthogonal, but each entry in the matrix is drawn from a Gaussian distribution with a mean of and variance of . Is this possible, and if so what would be an easy way to do it?",0 1,"['linear-algebra', 'matrices', 'normal-distribution', 'orthogonal-matrices', 'random-matrices']"
31,The normal form of a skew symmetric matrix,The normal form of a skew symmetric matrix,,"In Werner Greub's book Linear Algebra , 4th ed. on p. 230, he gives this proof of the normal form for a skew transformation on a finite-dimensional real inner product space . (Note Greub's convention for the matrix of a transformation is the transpose of that normally used with left-hand notation.) I believe this proof is incorrect because it is not true in general that the $a_n$ defined form an orthonormal basis of the space. For example in $\mathbb{R}^4$ , if we define the transformation $\psi$ by $$e_1\mapsto e_2\qquad e_2\mapsto -e_1\qquad e_3\mapsto e_4\qquad e_4\mapsto -e_3$$ where $e_i$ is the $i$ -th standard basis vector, then $\psi$ is skew and $\varphi=\psi^2=-\iota$ is diagonalized by the standard basis. If we follow the proof for this example, we get $a_1=e_1$ , $a_2=\psi e_1=e_2$ , $a_3=e_2$ , and $a_4=\psi e_2=-e_1$ , so the $a_n$ do not form a basis of $\mathbb{R}^4$ . Does anyone see a way to salvage this proof while still retaining its spirit (in particular, avoiding use of complex numbers)?","In Werner Greub's book Linear Algebra , 4th ed. on p. 230, he gives this proof of the normal form for a skew transformation on a finite-dimensional real inner product space . (Note Greub's convention for the matrix of a transformation is the transpose of that normally used with left-hand notation.) I believe this proof is incorrect because it is not true in general that the defined form an orthonormal basis of the space. For example in , if we define the transformation by where is the -th standard basis vector, then is skew and is diagonalized by the standard basis. If we follow the proof for this example, we get , , , and , so the do not form a basis of . Does anyone see a way to salvage this proof while still retaining its spirit (in particular, avoiding use of complex numbers)?",a_n \mathbb{R}^4 \psi e_1\mapsto e_2\qquad e_2\mapsto -e_1\qquad e_3\mapsto e_4\qquad e_4\mapsto -e_3 e_i i \psi \varphi=\psi^2=-\iota a_1=e_1 a_2=\psi e_1=e_2 a_3=e_2 a_4=\psi e_2=-e_1 a_n \mathbb{R}^4,"['linear-algebra', 'matrices', 'proof-verification', 'linear-transformations', 'skew-mapping']"
32,"What is the derivative of $ \|X^TW-Y\|_{2,1}$ w.r.t. $W$? How to compute it?",What is the derivative of  w.r.t. ? How to compute it?," \|X^TW-Y\|_{2,1} W","$W$ is a variable. $\|X^TW-Y\|_{2,1}$ is not smooth due to the $\|\cdot\|_{2,1}$ -norm. In order to be differentiable, $\|X^TW-Y\|_{2,1}$ is relaxed to $2\operatorname{Tr}((X^TW-Y)^TD(X^TW-Y))$ , where $$D_{ii} = \frac{1}{2\|(X^TW-Y)_i\|_2+\varepsilon}$$ and $\varepsilon$ denotes a small constant. $X \in \mathbb{R}^{d \times n}$ , $Y \in \mathbb{R}^{n \times l}$ and $W \in \mathbb{R}^{d\times l}$ . Note that: the norm $\|\cdot\|_{2,1}$ of a matrix $W \in \mathbb{R}^{d \times l}$ is defined as $$ \Vert W \Vert_{2,1}  = \sum_{i=1}^d \Vert w^{i} \Vert_2 = \sum_{i=1}^d \left( \sum_{j=1}^l |w_{ij}|^2 \right)^{1/2} $$ where $w^i$ denotes $i^\text{th}$ row of $W$ , $w_{ij}$ denotes a element of $W$ . Some papers as follows: Multi-Label Informed Feature Selection Efficient and Robust Feature Selection via Joint $l_{2,1}$ -Norms Minimization","is a variable. is not smooth due to the -norm. In order to be differentiable, is relaxed to , where and denotes a small constant. , and . Note that: the norm of a matrix is defined as where denotes row of , denotes a element of . Some papers as follows: Multi-Label Informed Feature Selection Efficient and Robust Feature Selection via Joint -Norms Minimization","W \|X^TW-Y\|_{2,1} \|\cdot\|_{2,1} \|X^TW-Y\|_{2,1} 2\operatorname{Tr}((X^TW-Y)^TD(X^TW-Y)) D_{ii} = \frac{1}{2\|(X^TW-Y)_i\|_2+\varepsilon} \varepsilon X \in \mathbb{R}^{d \times n} Y \in \mathbb{R}^{n \times l} W \in \mathbb{R}^{d\times l} \|\cdot\|_{2,1} W \in \mathbb{R}^{d \times l} 
\Vert W \Vert_{2,1} 
= \sum_{i=1}^d \Vert w^{i} \Vert_2
= \sum_{i=1}^d \left( \sum_{j=1}^l |w_{ij}|^2 \right)^{1/2}
 w^i i^\text{th} W w_{ij} W l_{2,1}","['real-analysis', 'matrices', 'derivatives', 'matrix-calculus', 'matrix-norms']"
33,"Proof for a bound $\epsilon > m$ such that $A^TXA \preceq \epsilon X$, where $X \succ 0$.","Proof for a bound  such that , where .",\epsilon > m A^TXA \preceq \epsilon X X \succ 0,"I'm interested in the following problem: Let $A,X$ be matrices in $\mathbb{R}^{n\times n}$ with $X$ symmetric and positive definite ( $X \succ 0$ ). Proof a lower bound $0 < m < \epsilon$ so that $$ A^TXA \preceq \epsilon X.$$ As illustrated in an related question $m = \lambda_\max(A)^2$ is not sufficient even for $A \succeq 0$ . I came up with the following bound based on inequalities for the trace of matrix product : If $ \epsilon > \lambda_\max(AA^T)\frac{\lambda_\max(X)}{\lambda_\min(X)},$ then $ A^TXA \preceq \epsilon X.$ The proof is given below. Questions: First, I'm happy if anyone could point out flaws in the proof below. Are there better bounds for $\epsilon$ such that the matrix inequality holds? Is there a simpler proof of the given solution? Proof: Let $\rho(\cdot)$ denote the spectral radius of a matrix and let $||\cdot||_F$ be the Frobenius norm. Then \begin{align} A^TXA \preceq \epsilon X &\iff \rho (A^TXA (\epsilon X)^{-1}) < 1 \\ &\iff \underset{k\rightarrow \infty}{\lim} ||(A^TXA (\epsilon X)^{-1})^k||_F = 0. \end{align} We use the following inequality : Let $A,B$ be matrices in $\mathbb{R}^{n\times n}$ with $B \succeq 0$ . Then $$ \text{trace}(AB) \le \lambda_\max \left(\frac{A+A^T}{2}\right) \text{trace}(B) $$ We have \begin{align} ||(A^TXA (\epsilon X)^{-1})^k||_F &= \sqrt{ \text{trace} \left( (A^TXA (\epsilon X)^{-1})^k \right)^T \left( (A^TXA (\epsilon X)^{-1})^k \right)} \\ &= \frac{1}{\epsilon^k} \sqrt{ \text{trace} \left( X^{-1} A^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X A X^{-1} \right)} \\ &= \frac{1}{\epsilon^k} \sqrt{ \text{trace} \left( X^{-2} \underbrace{A^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X A}_{\text{positive semidefinte by definition}} \right)} \\ &\le \frac{1}{\epsilon^k} \sqrt{ \lambda_\min(X)^2 \text{trace} \left( AA^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X \right)} \\ & \ \ \vdots \\ &\le \left(\frac{1}{\epsilon} \lambda_\max(AA^T) \frac{\lambda_\max(X)}{\lambda_\min(X)} \right)^k n \end{align} The first equality applies the definition of the Frobenius norm. In the second equality we use that $X$ is symmetric and so is $X^{-1}$ . In the third equality we apply the cyclic property of the trace and observe that the right part is positive semidefinite per construction. The first inequality applies the trace inequality, uses the fact the eigenvalues of the square of a symmetric matrix are the eigenvalues of that matrix squared and that the eigenvalues of the inverse of a positive definite matrix are the reciprocals of the eigenvalues of that matrix. For the last inequality we apply the proceeding approach successively for $X^{-2}, AA^T, X^2$ , $A^TA$ and use that the eigenvalues of $AA^T$ and $A^TA$ are equal. Hence if $\frac{1}{\epsilon} \lambda_\max(AA^T) \frac{\lambda_\max(X)}{\lambda_\min(X)} < 1$ , then $\underset{k\rightarrow \infty}{\lim} ||(A^TXA (\epsilon X)^{-1})^k||_F = 0$ .","I'm interested in the following problem: Let be matrices in with symmetric and positive definite ( ). Proof a lower bound so that As illustrated in an related question is not sufficient even for . I came up with the following bound based on inequalities for the trace of matrix product : If then The proof is given below. Questions: First, I'm happy if anyone could point out flaws in the proof below. Are there better bounds for such that the matrix inequality holds? Is there a simpler proof of the given solution? Proof: Let denote the spectral radius of a matrix and let be the Frobenius norm. Then We use the following inequality : Let be matrices in with . Then We have The first equality applies the definition of the Frobenius norm. In the second equality we use that is symmetric and so is . In the third equality we apply the cyclic property of the trace and observe that the right part is positive semidefinite per construction. The first inequality applies the trace inequality, uses the fact the eigenvalues of the square of a symmetric matrix are the eigenvalues of that matrix squared and that the eigenvalues of the inverse of a positive definite matrix are the reciprocals of the eigenvalues of that matrix. For the last inequality we apply the proceeding approach successively for , and use that the eigenvalues of and are equal. Hence if , then .","A,X \mathbb{R}^{n\times n} X X \succ 0 0 < m < \epsilon  A^TXA \preceq \epsilon X. m = \lambda_\max(A)^2 A \succeq 0  \epsilon > \lambda_\max(AA^T)\frac{\lambda_\max(X)}{\lambda_\min(X)},  A^TXA \preceq \epsilon X. \epsilon \rho(\cdot) ||\cdot||_F \begin{align}
A^TXA \preceq \epsilon X &\iff \rho (A^TXA (\epsilon X)^{-1}) < 1 \\
&\iff \underset{k\rightarrow \infty}{\lim} ||(A^TXA (\epsilon X)^{-1})^k||_F = 0.
\end{align} A,B \mathbb{R}^{n\times n} B \succeq 0  \text{trace}(AB) \le \lambda_\max \left(\frac{A+A^T}{2}\right) \text{trace}(B)  \begin{align}
||(A^TXA (\epsilon X)^{-1})^k||_F &= \sqrt{ \text{trace} \left( (A^TXA (\epsilon X)^{-1})^k \right)^T \left( (A^TXA (\epsilon X)^{-1})^k \right)} \\
&= \frac{1}{\epsilon^k} \sqrt{ \text{trace} \left( X^{-1} A^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X A X^{-1} \right)} \\
&= \frac{1}{\epsilon^k} \sqrt{ \text{trace} \left( X^{-2} \underbrace{A^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X A}_{\text{positive semidefinte by definition}} \right)} \\
&\le \frac{1}{\epsilon^k} \sqrt{ \lambda_\min(X)^2 \text{trace} \left( AA^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X \right)} \\
& \ \ \vdots \\
&\le \left(\frac{1}{\epsilon} \lambda_\max(AA^T) \frac{\lambda_\max(X)}{\lambda_\min(X)} \right)^k n
\end{align} X X^{-1} X^{-2}, AA^T, X^2 A^TA AA^T A^TA \frac{1}{\epsilon} \lambda_\max(AA^T) \frac{\lambda_\max(X)}{\lambda_\min(X)} < 1 \underset{k\rightarrow \infty}{\lim} ||(A^TXA (\epsilon X)^{-1})^k||_F = 0","['linear-algebra', 'matrices', 'proof-verification', 'alternative-proof', 'spectral-radius']"
34,A property involving two commuting 3-by-3 matrices with non-negative entries,A property involving two commuting 3-by-3 matrices with non-negative entries,,"Consider real numbers $m,n,p,q\ge 0$ and the following two matrices $$M_1= \left(\begin{matrix} 0&0&1\\0&m&n\\1&n&q \end{matrix} \right), \ \ M_2= \left(\begin{matrix} 0&1&0\\1&p&m\\0&m&n \end{matrix} \right).$$ Let $\chi_i$ be the characteristic polynomials of $M_i$ , then: $$\chi_1(x) = -(x^3-(q+m)x^2+(qm-1-n^2)x+m)$$ $$\chi_2(x) = -(x^3-(p+n)x^2+(pn-1-m^2)x+n)$$ Consider the case where these matrices commute, which is equivalent to $$(\star)\qquad m^2 + n^2 = mq +np + 1.$$ Remark : The relation $(\star)$ and the polynomials $\chi_i$ already appeared in this answer of Max Alekseyev . Let us assume that $m \le n$ . Then the above equation forces $n>0$ . Then we can take $p=(m^2 + n^2 - mq - 1)/n$ . Then, these matrices are self-adjoint and commuting, so simultaneously diagonalizable. Thus, there is an invertible matrix $P$ and (real) eigenvalues $\alpha_i, \beta_i, \gamma_i$ (the roots of $\chi_i$ ) such that: $$P^{-1}M_iP= \left(\begin{matrix} \alpha_i&0&0\\0&\beta_i&0\\0&0&\gamma_i \end{matrix} \right)$$ By positivity $\max(\alpha_i, \beta_i, \gamma_i) = \Vert M_i \Vert$ . Let us assume that $\Vert M_i \Vert = \gamma_i$ . We can also assume (up to permutation) that $\alpha_1 = min(\alpha_1, \beta_1, \alpha_2, \beta_2)$ . Question : For which $(m,n,q)$ we have $\frac{\alpha_1^3}{\gamma_1} + \frac{\alpha_2^3}{\gamma_2} + 1 <0$ ? Investigation I used SageMath to make some computation when $q=0$ , and it seems that if $m < n$ then the expected property holds. Just with $(m,n,q) = (1,n,0)$ , SageMath provides complicated formula for $\alpha_i, \beta_i, \gamma_i$ : $\alpha_1 = -\frac{1}{3}(\frac{1}{2})^{2/3}(3n^2 + 4)\zeta u^{-1/3} - \frac{1}{6}(\frac{1}{2})^{1/3}u^{1/3} \overline{\zeta}  + 1/3$ $\beta_1 = -\frac{1}{3}(\frac{1}{2})^{2/3}(3n^2 + 4) \overline{\zeta} u^{-1/3} - \frac{1}{6}(\frac{1}{2})^{1/3}u^{1/3}\zeta  + 1/3$ $\gamma_1 = \frac{2}{3}(\frac{1}{2})^{2/3}(3n^2 + 4)u^{-1/3} + \frac{1}{3}(\frac{1}{2})^{1/3}u^{1/3} + 1/3$ $\alpha_2= -\frac{1}{3}(\frac{1}{2})^{2/3}(n^2 + 6) \overline{\zeta} v^{-1/3} - \frac{1}{6}(\frac{1}{2})^{1/3}v^{1/3}\zeta  + \frac{2}{3}n$ $\beta_2= -\frac{1}{3}(\frac{1}{2})^{2/3}(n^2 + 6)\zeta v^{-1/3} - \frac{1}{6}(\frac{1}{2})^{1/3}v^{1/3} \overline{\zeta}  + \frac{2}{3}n$ $\gamma_2= \frac{2}{3}(\frac{1}{2})^{2/3}(n^2 + 6)v^{-1/3} + \frac{2}{3}n + \frac{1}{3}(\frac{1}{2})^{1/3}v^{1/3}$ with $\zeta = (i\sqrt{3} + 1)$ , $u = 9n^2 + 9in(\frac{4}{3}n^4 + \frac{13}{3}n^2 + \frac{32}{3})^{1/2} - 16$ and $v = 16n^3 - 18(n^2 - 2)n - 27n + 9i(\frac{4}{3}n^4 + \frac{13}{3}n^2 + \frac{32}{3})^{1/2}$ . We observe with a computer and large $n$ that $\frac{1}{n}(\frac{\alpha_1(n)^3}{\gamma_1(n)} + \frac{\alpha_2(n)^3}{\gamma_2(n)} + 1) \to -2$ Example : for $n=2$ we have $$\left(\begin{matrix} \alpha_1 & \beta_1 & \gamma_1\\ \alpha_2 & \beta_2 & \gamma_2 \end{matrix} \right) \simeq \left(\begin{matrix} -1.903211925911 & 0.1939365664746 & 2.709275359436 \\ 1.311107817465 & -0.4811943040920 & 3.170086486626 \end{matrix} \right)$$ and $\frac{\alpha_1(n)^3}{\gamma_1(n)} + \frac{\alpha_2(n)^3}{\gamma_2(n)} + 1 \simeq -0.83357599939<0.$ If we plot for $n=2,3,\dots, 50$ , we get Now it seems complicated to use such formulas (at least by hand) to answer the question.","Consider real numbers and the following two matrices Let be the characteristic polynomials of , then: Consider the case where these matrices commute, which is equivalent to Remark : The relation and the polynomials already appeared in this answer of Max Alekseyev . Let us assume that . Then the above equation forces . Then we can take . Then, these matrices are self-adjoint and commuting, so simultaneously diagonalizable. Thus, there is an invertible matrix and (real) eigenvalues (the roots of ) such that: By positivity . Let us assume that . We can also assume (up to permutation) that . Question : For which we have ? Investigation I used SageMath to make some computation when , and it seems that if then the expected property holds. Just with , SageMath provides complicated formula for : with , and . We observe with a computer and large that Example : for we have and If we plot for , we get Now it seems complicated to use such formulas (at least by hand) to answer the question.","m,n,p,q\ge 0 M_1= \left(\begin{matrix} 0&0&1\\0&m&n\\1&n&q \end{matrix} \right), \ \ M_2= \left(\begin{matrix} 0&1&0\\1&p&m\\0&m&n \end{matrix} \right). \chi_i M_i \chi_1(x) = -(x^3-(q+m)x^2+(qm-1-n^2)x+m) \chi_2(x) = -(x^3-(p+n)x^2+(pn-1-m^2)x+n) (\star)\qquad m^2 + n^2 = mq +np + 1. (\star) \chi_i m \le n n>0 p=(m^2 + n^2 - mq - 1)/n P \alpha_i, \beta_i, \gamma_i \chi_i P^{-1}M_iP= \left(\begin{matrix} \alpha_i&0&0\\0&\beta_i&0\\0&0&\gamma_i \end{matrix} \right) \max(\alpha_i, \beta_i, \gamma_i) = \Vert M_i \Vert \Vert M_i \Vert = \gamma_i \alpha_1 = min(\alpha_1, \beta_1, \alpha_2, \beta_2) (m,n,q) \frac{\alpha_1^3}{\gamma_1} + \frac{\alpha_2^3}{\gamma_2} + 1 <0 q=0 m < n (m,n,q) = (1,n,0) \alpha_i, \beta_i, \gamma_i \alpha_1 = -\frac{1}{3}(\frac{1}{2})^{2/3}(3n^2 + 4)\zeta u^{-1/3} - \frac{1}{6}(\frac{1}{2})^{1/3}u^{1/3} \overline{\zeta}  + 1/3 \beta_1 = -\frac{1}{3}(\frac{1}{2})^{2/3}(3n^2 + 4) \overline{\zeta} u^{-1/3} - \frac{1}{6}(\frac{1}{2})^{1/3}u^{1/3}\zeta  + 1/3 \gamma_1 = \frac{2}{3}(\frac{1}{2})^{2/3}(3n^2 + 4)u^{-1/3} + \frac{1}{3}(\frac{1}{2})^{1/3}u^{1/3} + 1/3 \alpha_2= -\frac{1}{3}(\frac{1}{2})^{2/3}(n^2 + 6) \overline{\zeta} v^{-1/3} - \frac{1}{6}(\frac{1}{2})^{1/3}v^{1/3}\zeta  + \frac{2}{3}n \beta_2= -\frac{1}{3}(\frac{1}{2})^{2/3}(n^2 + 6)\zeta v^{-1/3} - \frac{1}{6}(\frac{1}{2})^{1/3}v^{1/3} \overline{\zeta}  + \frac{2}{3}n \gamma_2= \frac{2}{3}(\frac{1}{2})^{2/3}(n^2 + 6)v^{-1/3} + \frac{2}{3}n + \frac{1}{3}(\frac{1}{2})^{1/3}v^{1/3} \zeta = (i\sqrt{3} + 1) u = 9n^2 + 9in(\frac{4}{3}n^4 + \frac{13}{3}n^2 + \frac{32}{3})^{1/2} - 16 v = 16n^3 - 18(n^2 - 2)n - 27n + 9i(\frac{4}{3}n^4 + \frac{13}{3}n^2 + \frac{32}{3})^{1/2} n \frac{1}{n}(\frac{\alpha_1(n)^3}{\gamma_1(n)} + \frac{\alpha_2(n)^3}{\gamma_2(n)} + 1) \to -2 n=2 \left(\begin{matrix} \alpha_1 & \beta_1 & \gamma_1\\ \alpha_2 & \beta_2 & \gamma_2 \end{matrix} \right) \simeq \left(\begin{matrix} -1.903211925911 & 0.1939365664746 & 2.709275359436 \\ 1.311107817465 & -0.4811943040920 & 3.170086486626 \end{matrix} \right) \frac{\alpha_1(n)^3}{\gamma_1(n)} + \frac{\alpha_2(n)^3}{\gamma_2(n)} + 1 \simeq -0.83357599939<0. n=2,3,\dots, 50","['calculus', 'matrices', 'matrix-calculus', 'sagemath', 'symbolic-computation']"
35,Isotropic tensor functions that map antisymmetric tensors to zero (Navier-Stokes derivation),Isotropic tensor functions that map antisymmetric tensors to zero (Navier-Stokes derivation),,"I'm going through Chorin and Marsden's derivation of the Navier-Stokes equations in A Mathematical Introduction to Fluid Mechanics . There are three assumptions made about the Cauchy stress tensor $\pmb\sigma$ (which I am paraphrasing): 1) $\pmb \sigma$ depends only on the velocity gradient $\nabla \mathbf{u}$ . So written as a linear transformation $\pmb \sigma = \pmb \sigma (\nabla \mathbf{u})$ 2) $\pmb \sigma$ is an isotropic tensor function (rotationally invariant) so that $$\pmb \sigma (U \ \nabla \mathbf u \ U^T)= U \ \pmb \sigma (\nabla \mathbf u) U^T$$ for any proper orthogonal matrix $U$ . 3) $\pmb \sigma$ is symmetric. After stating these assumptions, they conclude that because of 3), it follows from 1) and 2) that $\pmb \sigma$ can depend only on the symmetric part of $\nabla \mathbf u$ . In other words, if you write $$\nabla \mathbf u = D +W$$ where $D=\frac 1 2 (\nabla \mathbf u + \nabla \mathbf u^T) $ (the symmetric part of $\nabla \mathbf u$ ) and $W = \frac 1 2 (\nabla \mathbf u - \nabla \mathbf u^T) $ (the antisymmetric part of $\nabla \mathbf u$ ), then $\pmb \sigma (W)= \mathbf 0$ , i.e. $\pmb \sigma$ maps antisymmetric second order tensors to the zero second order tensor. I am not sure how the above argument works and would appreciate some help in seeing it. Note : I've seen arguments which use the fact that $\pmb \sigma$ must satisfy in component form $$\sigma_{ij}= -p \delta_{ij} + C_{ijrs}  u_{r,s}$$ where $p$ is the pressure, and it ends up that the fourth order tensor $\mathbf C$ must take a special form (because it is isotropic). From this special form, you can end up deducing that $\pmb \sigma$ maps antisymmetric second order tensors to the zero second order tensor. I'm interested in seeing an argument that doesn't rely on the component analysis and also doesn't jump immediately to the special representation of isotropic second order tensor functions in terms of its principal invariants.","I'm going through Chorin and Marsden's derivation of the Navier-Stokes equations in A Mathematical Introduction to Fluid Mechanics . There are three assumptions made about the Cauchy stress tensor (which I am paraphrasing): 1) depends only on the velocity gradient . So written as a linear transformation 2) is an isotropic tensor function (rotationally invariant) so that for any proper orthogonal matrix . 3) is symmetric. After stating these assumptions, they conclude that because of 3), it follows from 1) and 2) that can depend only on the symmetric part of . In other words, if you write where (the symmetric part of ) and (the antisymmetric part of ), then , i.e. maps antisymmetric second order tensors to the zero second order tensor. I am not sure how the above argument works and would appreciate some help in seeing it. Note : I've seen arguments which use the fact that must satisfy in component form where is the pressure, and it ends up that the fourth order tensor must take a special form (because it is isotropic). From this special form, you can end up deducing that maps antisymmetric second order tensors to the zero second order tensor. I'm interested in seeing an argument that doesn't rely on the component analysis and also doesn't jump immediately to the special representation of isotropic second order tensor functions in terms of its principal invariants.","\pmb\sigma \pmb \sigma \nabla \mathbf{u} \pmb \sigma = \pmb \sigma (\nabla \mathbf{u}) \pmb \sigma \pmb \sigma (U \ \nabla \mathbf u \ U^T)= U \ \pmb \sigma (\nabla \mathbf u) U^T U \pmb \sigma \pmb \sigma \nabla \mathbf u \nabla \mathbf u = D +W D=\frac 1 2 (\nabla \mathbf u + \nabla \mathbf u^T)  \nabla \mathbf u W = \frac 1 2 (\nabla \mathbf u - \nabla \mathbf u^T)  \nabla \mathbf u \pmb \sigma (W)= \mathbf 0 \pmb \sigma \pmb \sigma \sigma_{ij}= -p \delta_{ij} + C_{ijrs}  u_{r,s} p \mathbf C \pmb \sigma","['matrices', 'linear-transformations', 'physics', 'tensors', 'fluid-dynamics']"
36,What is this matrix of binomial distributions?,What is this matrix of binomial distributions?,,"I have come across the following type of $(n+1) \times (n+1)$ matrix: $$C^n = \left( \begin{array}{cccc} 	\beta^n(0)	& 0			& \dots & 0	 \\ 	\beta^n(1)	& \beta^{n-1}(0)	& \dots & 0 \\ 	\vdots	& \vdots		& \ddots & \vdots \\ 	\beta^n(n)	& \beta^{n-1}(n-1)& \dots & \beta^0(0) \\ 	\end{array} \right)$$ where $\beta^n(k) = {n \choose k} p^k(1-p)^{n-k}, k = 0,1,\dots, n$ is the vector of binomial distribution probabilities, and we define $\beta^0(0) = 1$ . For example, for $n=3$ we have the matrix $$ C^3 = \left( \begin{array}{cccc} 	(1-p)^3		& 0			& 0 		& 0 \\ 	3 p(1-p)^2	& (1-p)^2		& 0		&0 \\ 	3 p^2(1-p)	& 2 p(1-p)	& 1-p 	& 0 \\ 	p^3			& p^2		& p		& 1 \\ 	\end{array} \right) $$ This matrix seems to have some interesting properties: it is clearly invertible; the lower right $n\times n$ submatrix of $C^n$ is $C^{n-1}$ ; and it appears that these matrices commute with convolution of probabilities, in the sense that $$  C^{m+n} (x \otimes y) = (C^m x) \otimes (C^n y)$$ where $x \in \mathbb{R}^n$ and $y\in \mathbb{R}^m$ are discrete probability distribution vectors and $x \otimes y$ is the usual convolution operation. Is anyone familiar with this kind of matrix? I would be grateful for any suggestions for literature/information. Have been searching for most of the day, but I'm probably not using the right keywords ... I am aware of the related Pascal matrices, but this seems a bit different.","I have come across the following type of matrix: where is the vector of binomial distribution probabilities, and we define . For example, for we have the matrix This matrix seems to have some interesting properties: it is clearly invertible; the lower right submatrix of is ; and it appears that these matrices commute with convolution of probabilities, in the sense that where and are discrete probability distribution vectors and is the usual convolution operation. Is anyone familiar with this kind of matrix? I would be grateful for any suggestions for literature/information. Have been searching for most of the day, but I'm probably not using the right keywords ... I am aware of the related Pascal matrices, but this seems a bit different.","(n+1) \times (n+1) C^n = \left( \begin{array}{cccc}
	\beta^n(0)	& 0			& \dots & 0	 \\
	\beta^n(1)	& \beta^{n-1}(0)	& \dots & 0 \\
	\vdots	& \vdots		& \ddots & \vdots \\
	\beta^n(n)	& \beta^{n-1}(n-1)& \dots & \beta^0(0) \\
	\end{array} \right) \beta^n(k) = {n \choose k} p^k(1-p)^{n-k}, k = 0,1,\dots, n \beta^0(0) = 1 n=3  C^3 = \left( \begin{array}{cccc}
	(1-p)^3		& 0			& 0 		& 0 \\
	3 p(1-p)^2	& (1-p)^2		& 0		&0 \\
	3 p^2(1-p)	& 2 p(1-p)	& 1-p 	& 0 \\
	p^3			& p^2		& p		& 1 \\
	\end{array} \right)  n\times n C^n C^{n-1}   C^{m+n} (x \otimes y) = (C^m x) \otimes (C^n y) x \in \mathbb{R}^n y\in \mathbb{R}^m x \otimes y","['matrices', 'convolution', 'binomial-distribution']"
37,Conjugate transpose of matrix is the adjoint intuition,Conjugate transpose of matrix is the adjoint intuition,,"I'm having a bit of trouble understanding this fact from Linear Algebra Done Right Let T $\in \mathcal{L}(V, W)$ Suppose $e_{1}, \dots, e_{n}$ is an   orthonormal basis of $V$ and $f_{1}, \ldots, f_{m}$ is an orthonormal   basis of $W$ . Then then adjoint $T^{*}$ is the conjugate transpose of $T$ in $\langle T v, w\rangle=\left\langle v, T^{*} w\right\rangle$ I went through the proof and did an example but I can't get a clearer picture as to why this must be true intuitively or geometrically. What is so special about a transpose that makes the transformation act in this bridge-like fashion? Also, I am also not able to appreciate the fact that we have an orthonormal basis, while it is important in the proof, what would go wrong if we didnt? Here's the entire proof for reference:","I'm having a bit of trouble understanding this fact from Linear Algebra Done Right Let T Suppose is an   orthonormal basis of and is an orthonormal   basis of . Then then adjoint is the conjugate transpose of in I went through the proof and did an example but I can't get a clearer picture as to why this must be true intuitively or geometrically. What is so special about a transpose that makes the transformation act in this bridge-like fashion? Also, I am also not able to appreciate the fact that we have an orthonormal basis, while it is important in the proof, what would go wrong if we didnt? Here's the entire proof for reference:","\in \mathcal{L}(V, W) e_{1}, \dots, e_{n} V f_{1}, \ldots, f_{m} W T^{*} T \langle T v, w\rangle=\left\langle v, T^{*} w\right\rangle","['linear-algebra', 'abstract-algebra', 'matrices']"
38,Unobserved values in SVD and matrix factorization problems,Unobserved values in SVD and matrix factorization problems,,"In class, in the context of matrix completion (recommender Systems, collaborative filtering), we have seen that the Eckart-Young theorem says the best rank- $k$ approximation to a matrix $A$ is given by taking the dominant first $k < \mbox{rank}(A)$ singular values. Thus, if we have a rating matrix, like in the Netflix data, it was mentioned that the rating matrix has many unobserved values that are not defined. The task is to infer these entries. One solution approach we have seen is via Matrix Factorization using Alternating Least Squares. Why can't we use SVD to solve this problem? Since with the Eckart-Young theorem it is guaranteed that we get the best rank- $k$ approximation of the rating matrix, it seems that unobserved values incur a problem, we cannot just set the unobserved values to $0$ or the mean of the data, but why is that? Does it have to do with the rank-k constraint in the minimization problem $$ A_k = \arg \min_{\mbox{rank}(B)=k} \left\Vert A - B \right\Vert_F^2 $$ because with the rank constrained it is a non-convex problem and Eckart-Young theorem does not guarantee the best rank- $k$ approximation anymore if there are unobserved values in $A$ ?","In class, in the context of matrix completion (recommender Systems, collaborative filtering), we have seen that the Eckart-Young theorem says the best rank- approximation to a matrix is given by taking the dominant first singular values. Thus, if we have a rating matrix, like in the Netflix data, it was mentioned that the rating matrix has many unobserved values that are not defined. The task is to infer these entries. One solution approach we have seen is via Matrix Factorization using Alternating Least Squares. Why can't we use SVD to solve this problem? Since with the Eckart-Young theorem it is guaranteed that we get the best rank- approximation of the rating matrix, it seems that unobserved values incur a problem, we cannot just set the unobserved values to or the mean of the data, but why is that? Does it have to do with the rank-k constraint in the minimization problem because with the rank constrained it is a non-convex problem and Eckart-Young theorem does not guarantee the best rank- approximation anymore if there are unobserved values in ?",k A k < \mbox{rank}(A) k 0  A_k = \arg \min_{\mbox{rank}(B)=k} \left\Vert A - B \right\Vert_F^2  k A,"['linear-algebra', 'matrices', 'optimization', 'svd', 'non-convex-optimization']"
39,Evaluating a determinant with multiple variables and multiplicative inverses,Evaluating a determinant with multiple variables and multiplicative inverses,,"I just asked a question on math stack exchange and doing some progress on it: Proof of Pascal's Theorem (on circles) using complex numbers. . I did some complex number geometry, got the intersection points and now all it remains is to evaluate a determinant which should evaluate to zero. Here's the determinant: $$    \frac{i}{4} \begin{vmatrix}     \frac{ab(d+e) - de(a+b)}{ab-de} & \frac{\frac{1}{ab}(\frac 1d+\frac1e) - \frac{1}{de}(\frac1a+\frac1b)}{\frac{1}{ab}-\frac{1}{de}} & 1 \\     \frac{bc(e+f) - ef(b+c)}{bc-ef} & \frac{\frac{1}{bc}(\frac1e+\frac1f) - \frac{1}{ef}(\frac1b+\frac1c)}{\frac{1}{bc}-\frac{1}{ef}} & 1 \\     \frac{cd(f+a) - fa(c+d)}{cd-fa} & \frac{\frac{1}{cd}(\frac1f+\frac1a) - \frac{1}{fa}(\frac1c+\frac1d)}{\frac{1}{cd}-\frac{1}{fa}} & 1 \\     \end{vmatrix} $$ where $a, b, c, d, e, f$ are points $A, B, C, D, E, F$ on the unit circle. I don't want to evaluate this by straight up multiplication and minors. Please explain how to solve this determinant using its properties. Thanks!","I just asked a question on math stack exchange and doing some progress on it: Proof of Pascal's Theorem (on circles) using complex numbers. . I did some complex number geometry, got the intersection points and now all it remains is to evaluate a determinant which should evaluate to zero. Here's the determinant: where are points on the unit circle. I don't want to evaluate this by straight up multiplication and minors. Please explain how to solve this determinant using its properties. Thanks!","
   \frac{i}{4} \begin{vmatrix}
    \frac{ab(d+e) - de(a+b)}{ab-de} & \frac{\frac{1}{ab}(\frac 1d+\frac1e) - \frac{1}{de}(\frac1a+\frac1b)}{\frac{1}{ab}-\frac{1}{de}} & 1 \\
    \frac{bc(e+f) - ef(b+c)}{bc-ef} & \frac{\frac{1}{bc}(\frac1e+\frac1f) - \frac{1}{ef}(\frac1b+\frac1c)}{\frac{1}{bc}-\frac{1}{ef}} & 1 \\
    \frac{cd(f+a) - fa(c+d)}{cd-fa} & \frac{\frac{1}{cd}(\frac1f+\frac1a) - \frac{1}{fa}(\frac1c+\frac1d)}{\frac{1}{cd}-\frac{1}{fa}} & 1 \\
    \end{vmatrix}
 a, b, c, d, e, f A, B, C, D, E, F","['matrices', 'complex-numbers', 'determinant', 'complex-geometry']"
40,Bounding the size of the inverse of $I+AB$ when $A$ and $B$ are both PSD,Bounding the size of the inverse of  when  and  are both PSD,I+AB A B,"If $A$ and $B$ are both positive semi-definite matrices, is it possible to show that $$\left\Vert \left(I+AB\right)^{-1}\right\Vert _{2}\leq1$$ where $\left\Vert \cdot\right\Vert _{2}$ is the operator norm?","If and are both positive semi-definite matrices, is it possible to show that where is the operator norm?",A B \left\Vert \left(I+AB\right)^{-1}\right\Vert _{2}\leq1 \left\Vert \cdot\right\Vert _{2},"['matrices', 'inverse', 'positive-semidefinite', 'matrix-norms', 'spectral-norm']"
41,"Prove $\det \left[\begin{smallmatrix} A&B\\\\C&D\\ \end{smallmatrix}\right] =\det(AD-BC)$ for $A,B,C,D$ upper triangular complex matrices",Prove  for  upper triangular complex matrices,"\det \left[\begin{smallmatrix} A&B\\\\C&D\\ \end{smallmatrix}\right] =\det(AD-BC) A,B,C,D","Let $A,B,C$ and $D$ be upper-triangular $n \times n$ complex matrices. Let $$E=\begin{bmatrix} A&B\\C&D\\ \end{bmatrix}$$ Prove $\det(E)=\det(AD-BC)$ . I did this problem in the case that the matrices commute but I cannot figure out this case.",Let and be upper-triangular complex matrices. Let Prove . I did this problem in the case that the matrices commute but I cannot figure out this case.,"A,B,C D n \times n E=\begin{bmatrix} A&B\\C&D\\ \end{bmatrix} \det(E)=\det(AD-BC)","['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
42,"Find a rank one non negative matrix $C$ such that the Matrix $B+C$ will have eigenvalues $13,2,-1$",Find a rank one non negative matrix  such that the Matrix  will have eigenvalues,"C B+C 13,2,-1","I have been given the matrix $$B = \begin{pmatrix} 1 & 3 & 3 \\ 2 & 3 & 2\\ 2 & 1 & 4\end{pmatrix}$$ and I've been given that its eigenvalues are: $7, 2, -1$ Firsly I was asked to find the column and row eigenvectors corresponding to the Perron eigenvalue. I know the Perron eigenvalue is $7$ so was able to find the eigenvectors to be $$ v = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, \ w^{T} =\begin{pmatrix} \frac{5}{9} & \frac{2}{3} & 1 \end{pmatrix}$$ The next part asks: Find a rank $1$ non negative matrix C such that the matrix $B+C$ will have eigenvalues $13,2,-1$ . I know from a theorem I've done in class that since $C$ is a rank $1$ matrix it can be expressed as $vy^{T}$ such that $B+C$ has the same eigenvalues as $B$ except that $\lambda_{1}$ of $B$ is replaced by $\lambda_{1} +y^{T}v$ . In this example then I have let $y = \begin{pmatrix} a & b & c \end{pmatrix}^{T}$ and from this I have $13 = 7 + \begin{pmatrix} a & b & c \end{pmatrix}\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$ , therefore $a + b + c = 6$ . I'm uncertain how to solve to find $C$ from this, do I need to use the row eigenvector?","I have been given the matrix and I've been given that its eigenvalues are: Firsly I was asked to find the column and row eigenvectors corresponding to the Perron eigenvalue. I know the Perron eigenvalue is so was able to find the eigenvectors to be The next part asks: Find a rank non negative matrix C such that the matrix will have eigenvalues . I know from a theorem I've done in class that since is a rank matrix it can be expressed as such that has the same eigenvalues as except that of is replaced by . In this example then I have let and from this I have , therefore . I'm uncertain how to solve to find from this, do I need to use the row eigenvector?","B = \begin{pmatrix} 1 & 3 & 3 \\ 2 & 3 & 2\\ 2 & 1 & 4\end{pmatrix} 7, 2, -1 7  v = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, \ w^{T} =\begin{pmatrix} \frac{5}{9} & \frac{2}{3} & 1 \end{pmatrix} 1 B+C 13,2,-1 C 1 vy^{T} B+C B \lambda_{1} B \lambda_{1} +y^{T}v y = \begin{pmatrix} a & b & c \end{pmatrix}^{T} 13 = 7 + \begin{pmatrix} a & b & c \end{pmatrix}\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} a + b + c = 6 C","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
43,Determinant of a matrix with positive diagonal entries is greater than 1,Determinant of a matrix with positive diagonal entries is greater than 1,,Let $A$ be a $n\times n$ matrix with entries on its diagonal are positive and other entries are negative with sum of entries in every column is 1. Prove that $\det(A) > 1.$ I got no idea to begin with. Any suggestion or hint?,Let be a matrix with entries on its diagonal are positive and other entries are negative with sum of entries in every column is 1. Prove that I got no idea to begin with. Any suggestion or hint?,A n\times n \det(A) > 1.,"['linear-algebra', 'matrices', 'determinant']"
44,Number of matrices with each row and column having exactly one 1.,Number of matrices with each row and column having exactly one 1.,,"Consider a square matrix of order $n = 5$ such that $a_{ij} = 0 ~  \forall ~ i+j = n+1; a_{ij} \in \{0,1\}$ . In each row as well as in each   column there is only one non zero element. Then number of such   matrices is? First we note that right diagonal has only $0$ s. Then I tried it this way: We choose a place for one among each row and mark the other places in that column  and same row as forbidden (i.e. no more one's). So for first column  we have 4 choices then 3 choices then 2 then 1 and then 2s. Thus, Number of ways = $4\times 3 \times 2\times 1\times 2 = 48$ But  its erroneous because the number of choices change if we place 1 above 0 in each attempt. What's the correct way to solve this question? Answer given is: 44","Consider a square matrix of order such that . In each row as well as in each   column there is only one non zero element. Then number of such   matrices is? First we note that right diagonal has only s. Then I tried it this way: We choose a place for one among each row and mark the other places in that column  and same row as forbidden (i.e. no more one's). So for first column  we have 4 choices then 3 choices then 2 then 1 and then 2s. Thus, Number of ways = But  its erroneous because the number of choices change if we place 1 above 0 in each attempt. What's the correct way to solve this question? Answer given is: 44","n = 5 a_{ij} = 0 ~
 \forall ~ i+j = n+1; a_{ij} \in \{0,1\} 0 4\times 3 \times 2\times 1\times 2 = 48","['matrices', 'permutations', 'combinations']"
45,The first element of a matrix power,The first element of a matrix power,,"Let $A \in \Bbb R^{n \times n}$ be the following block matrix: $$A:= \begin{bmatrix} a^T & \alpha\\ I_{n-1} & 0_{n-1} \end{bmatrix}$$ where $a, 0_{n-1} \in \Bbb R^{n-1}$ are vectors and $\alpha$ is a scalar. Then, is there a closed form for $$e_1^T A^k e_1,\quad k\in\Bbb N$$ i.e., the top left element of $A^k$ ? By closed form, I mean something like the combination of sums ( $\sum$ ) or products ( $\prod$ ) or anything else that would make solving for $k$ the equation $$e_1^T A^k e_1= 1/2$$ easier. Such a $k$ is called the half life of an AR(p) process whose coefficients are the first row of $A$ . Explanation for the time-series tag: This problem arises from computing the impulse response function, and therefore the half life, of a general AR process. Edit: $A$ is diagonalisable, non singular, and has spectral radius < 1 for what it's worth. So one numerical way to solve the equation $e_1^T A^k e_1= 1/2$ I can think about is to first diagonalise it as $A=QDQ^{-1}$ , then we can define for any real number $k$ the matrix power $A^k$ as $$A^k=QD^kQ^{-1}$$ And once we obtain $Q$ , $D$ numerically, we can expect to get a polynomial in $\lambda_{1,\cdots, n}^k$ where $\lambda$ are eigenvalues of $A$ , and thus is solvable by software.","Let be the following block matrix: where are vectors and is a scalar. Then, is there a closed form for i.e., the top left element of ? By closed form, I mean something like the combination of sums ( ) or products ( ) or anything else that would make solving for the equation easier. Such a is called the half life of an AR(p) process whose coefficients are the first row of . Explanation for the time-series tag: This problem arises from computing the impulse response function, and therefore the half life, of a general AR process. Edit: is diagonalisable, non singular, and has spectral radius < 1 for what it's worth. So one numerical way to solve the equation I can think about is to first diagonalise it as , then we can define for any real number the matrix power as And once we obtain , numerically, we can expect to get a polynomial in where are eigenvalues of , and thus is solvable by software.","A \in \Bbb R^{n \times n} A:=
\begin{bmatrix}
a^T & \alpha\\
I_{n-1} & 0_{n-1}
\end{bmatrix} a, 0_{n-1} \in \Bbb R^{n-1} \alpha e_1^T A^k e_1,\quad k\in\Bbb N A^k \sum \prod k e_1^T A^k e_1= 1/2 k A A e_1^T A^k e_1= 1/2 A=QDQ^{-1} k A^k A^k=QD^kQ^{-1} Q D \lambda_{1,\cdots, n}^k \lambda A","['linear-algebra', 'matrices', 'time-series', 'block-matrices']"
46,Matrices Inequality Proof,Matrices Inequality Proof,,"Recently, I read a paper and there is a step which turns out not obvious to me. The statement is as follows: All matrices here are real matrices. $F$ is an arbitrary square matrix. $\Psi$ is a symmetric positive definite matrix. Let $$\lambda_{\max}(A)\equiv\text{The maximum eigenvalue of symmetric matrix A}$$ (The ambiguity comes when $A$ is not symmetric. Here I guess if $A$ is not symmetric, then $\lambda_{\max}(A)=\sqrt{\text{Maximum eigenvalue of }A^TA}$ ). Then the following inequality holds: For all $x\in \mathbb R^n$ $$x^T(I-F)^T\Psi(I-F)x\le\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))x^T\Psi x $$ rewrite it, $$x^T\Big[(I-F)^T\Psi(I-F)-\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))\Psi\Big]x\le0 $$ or $$x^T\Psi\Big[\Psi^{-1}(I-F)^T\Psi(I-F)-\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))I\Big]x\le0\tag{*} $$ and if $\Psi$ commutes with $(I-F)^T\Psi(I-F)$ , then, $\Psi^{-1},\,(I-F)^T\Psi(I-F)$ can be simultaneously diagnolized. Then $$\Psi^{-1}(I-F)^T\Psi(I-F)-\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))I $$ is negatively semi-definite and diagnolized under certain basis, same as $\Psi$ . Then under the basis, since $\Psi$ is positive definite, $\Psi\Big[\Psi^{-1}(I-F)^T\Psi(I-F)-\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))I\Big]$ is negative semi-definite $\Rightarrow$ the inequality holds. However, in general, $\Psi$ may not commute with $(I-F)^T\Psi(I-F)$ . Are there any answers to that?","Recently, I read a paper and there is a step which turns out not obvious to me. The statement is as follows: All matrices here are real matrices. is an arbitrary square matrix. is a symmetric positive definite matrix. Let (The ambiguity comes when is not symmetric. Here I guess if is not symmetric, then ). Then the following inequality holds: For all rewrite it, or and if commutes with , then, can be simultaneously diagnolized. Then is negatively semi-definite and diagnolized under certain basis, same as . Then under the basis, since is positive definite, is negative semi-definite the inequality holds. However, in general, may not commute with . Are there any answers to that?","F \Psi \lambda_{\max}(A)\equiv\text{The maximum eigenvalue of symmetric matrix A} A A \lambda_{\max}(A)=\sqrt{\text{Maximum eigenvalue of }A^TA} x\in \mathbb R^n x^T(I-F)^T\Psi(I-F)x\le\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))x^T\Psi x
 x^T\Big[(I-F)^T\Psi(I-F)-\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))\Psi\Big]x\le0
 x^T\Psi\Big[\Psi^{-1}(I-F)^T\Psi(I-F)-\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))I\Big]x\le0\tag{*}
 \Psi (I-F)^T\Psi(I-F) \Psi^{-1},\,(I-F)^T\Psi(I-F) \Psi^{-1}(I-F)^T\Psi(I-F)-\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))I
 \Psi \Psi \Psi\Big[\Psi^{-1}(I-F)^T\Psi(I-F)-\lambda_\max(\Psi^{-1}(I-F)^T\Psi(I-F))I\Big] \Rightarrow \Psi (I-F)^T\Psi(I-F)","['matrices', 'inequality', 'lie-groups', 'positive-semidefinite']"
47,"Minimal ""dominating"" rank-$1$ matrix","Minimal ""dominating"" rank- matrix",1,"Given a matrix ${\bf A} \in \Bbb R^{n \times n}$ , I would like to find a minimal  rank- $1$ matrix ${\bf B} \in \Bbb R^{n \times n}$ such that the Frobenius norm of $ \| {\bf A} - {\bf B} \|_{\text{F}} $ is minimal under the constraint of $ a_{ij} \leq b_{ij}$ for all $i, j \in \{ 1, \dots, n \}$ . Also, is there a name for the element-wise  ""domination"" constraint?","Given a matrix , I would like to find a minimal  rank- matrix such that the Frobenius norm of is minimal under the constraint of for all . Also, is there a name for the element-wise  ""domination"" constraint?","{\bf A} \in \Bbb R^{n \times n} 1 {\bf B} \in \Bbb R^{n \times n}  \| {\bf A} - {\bf B} \|_{\text{F}}   a_{ij} \leq b_{ij} i, j \in \{ 1, \dots, n \}","['linear-algebra', 'matrices', 'optimization', 'rank-1-matrices']"
48,"A difficulty in understanding the definition of ""Spaces of Matrix Elements.""","A difficulty in understanding the definition of ""Spaces of Matrix Elements.""",,"The definition is given below: If the definition of the Spaces of Matrix Elements is as given below: But I do not understand why: 1- Any linear combination of matrix elements can be expressed invariantly (without using coordinates) as given in Eq.(3), could anyone explain this for me please? 2-Also I do not understand the paragraph under eq.(3), and why it is c_{ji} and not c_{ij} , could anyone explain this for me with a concrete example please? 3-Could anyone give me a concrete example describing the difference between the space of matrix elements of the representation $T$ and the matrix elements of $T$ ? Thank you!","The definition is given below: If the definition of the Spaces of Matrix Elements is as given below: But I do not understand why: 1- Any linear combination of matrix elements can be expressed invariantly (without using coordinates) as given in Eq.(3), could anyone explain this for me please? 2-Also I do not understand the paragraph under eq.(3), and why it is c_{ji} and not c_{ij} , could anyone explain this for me with a concrete example please? 3-Could anyone give me a concrete example describing the difference between the space of matrix elements of the representation and the matrix elements of ? Thank you!",T T,"['linear-algebra', 'matrices', 'representation-theory']"
49,Is there any inequality between 2-norm condition number and Frobenius norm condition number for rectangular matrix?,Is there any inequality between 2-norm condition number and Frobenius norm condition number for rectangular matrix?,,"What I have found in [1] Condition number inequality between Frobenius norm and 2-norm for square matrix, Consider a full rank matrix $X \in \mathbb{C}^{n \times m}$ , $m=n$ , then we can have, $$n - 2 + \frac{1}{\kappa_2(X)} + \kappa_2(X) \le \kappa_F(X).$$ Question: Does this inequality works for the case when $m \neq n$ ? If not, have you seen other relationship between them? Update: The following paper [2] mentioned that it is a natural extension to non-rectangular case, I don't get why. read the two equations below equation (3.7) in [2] . Ref: [1] Smith, Russell A. ""The condition numbers of the matrix eigenvalue problem."" Numerische Mathematik 10.3 (1967): 232-240. [2] Bazán, F. S. V. (2000). Conditioning of Rectangular Vandermonde Matrices with Nodes in the Unit Disk. SIAM Journal on Matrix Analysis and Applications, 21(2), 679–693. https://doi.org/10.1137/S0895479898336021","What I have found in [1] Condition number inequality between Frobenius norm and 2-norm for square matrix, Consider a full rank matrix , , then we can have, Question: Does this inequality works for the case when ? If not, have you seen other relationship between them? Update: The following paper [2] mentioned that it is a natural extension to non-rectangular case, I don't get why. read the two equations below equation (3.7) in [2] . Ref: [1] Smith, Russell A. ""The condition numbers of the matrix eigenvalue problem."" Numerische Mathematik 10.3 (1967): 232-240. [2] Bazán, F. S. V. (2000). Conditioning of Rectangular Vandermonde Matrices with Nodes in the Unit Disk. SIAM Journal on Matrix Analysis and Applications, 21(2), 679–693. https://doi.org/10.1137/S0895479898336021",X \in \mathbb{C}^{n \times m} m=n n - 2 + \frac{1}{\kappa_2(X)} + \kappa_2(X) \le \kappa_F(X). m \neq n,"['linear-algebra', 'matrices', 'inequality', 'condition-number', 'matrix-analysis']"
50,Does this specific SO(4) matrix have to be block-diagonal?,Does this specific SO(4) matrix have to be block-diagonal?,,"So I have a specific real $4\times4$ matrix $\mathbf{P}$ given by \begin{align} \mathbf{P}= \begin{pmatrix} p_{11} & -p_{21} & p_{13} &-p_{23}\\ p_{21} & p_{11} & p_{23} & p_{13}\\p_{31} & -p_{41}& p_{33} & -p_{43}\\p_{41} & p_{31} & p_{43} & p_{33}. \end{pmatrix}, \end{align} and I'm confident that if this matrix is in $SO(4)$ then it must be block-diagonal OR anti-block-diagonal i.e. if $\mathbf{P}\in SO(4)$ then either $p_{11}=p_{21}=p_{33}=p_{43}=0$ or $p_{13}=p_{23}=p_{31}=p_{41}=0$ , but I can't seem to show this... I've tried picking one column and generating a set of orthonormal vectors to fill the matrix, but this just gives examples where it works and I'd like to show it generally.  Is this possible?  Am I just missing something trivial here?","So I have a specific real matrix given by and I'm confident that if this matrix is in then it must be block-diagonal OR anti-block-diagonal i.e. if then either or , but I can't seem to show this... I've tried picking one column and generating a set of orthonormal vectors to fill the matrix, but this just gives examples where it works and I'd like to show it generally.  Is this possible?  Am I just missing something trivial here?","4\times4 \mathbf{P} \begin{align}
\mathbf{P}=
\begin{pmatrix}
p_{11} & -p_{21} & p_{13} &-p_{23}\\ p_{21} & p_{11} & p_{23} & p_{13}\\p_{31} & -p_{41}& p_{33} & -p_{43}\\p_{41} & p_{31} & p_{43} & p_{33}.
\end{pmatrix},
\end{align} SO(4) \mathbf{P}\in SO(4) p_{11}=p_{21}=p_{33}=p_{43}=0 p_{13}=p_{23}=p_{31}=p_{41}=0","['matrices', 'lie-groups', 'orthonormal']"
51,Eigenvalues of $A+v_1d^T$ where $Av_1 = \lambda_1 v_1$ (shift of first eigenvalue),Eigenvalues of  where  (shift of first eigenvalue),A+v_1d^T Av_1 = \lambda_1 v_1,"I have trouble to solve the following problem: Let $A\in \mathbf{R}^{n\times n}$ , $\lambda_1,\ldots,\lambda_n$ are eigenvalues of $A$ , and $A v_1=\lambda_1v_1$ . Let $d\in \mathbf{R}^n$ , then the eigenvalues of $A+v_1d^T$ are $\lambda_1+d^Tv_1, \lambda_2,\ldots,\lambda_n$ . Note: $A$ may not be diagonalizable. I know the following facts: $v_1d^T$ is of rank $1$ . eigenvalues of $v_1d^T$ are $v_1^Td, 0,\ldots, 0$ . But I still have no idea to prove this theorem. Please help me, thanks!","I have trouble to solve the following problem: Let , are eigenvalues of , and . Let , then the eigenvalues of are . Note: may not be diagonalizable. I know the following facts: is of rank . eigenvalues of are . But I still have no idea to prove this theorem. Please help me, thanks!","A\in \mathbf{R}^{n\times n} \lambda_1,\ldots,\lambda_n A A v_1=\lambda_1v_1 d\in \mathbf{R}^n A+v_1d^T \lambda_1+d^Tv_1, \lambda_2,\ldots,\lambda_n A v_1d^T 1 v_1d^T v_1^Td, 0,\ldots, 0","['matrices', 'eigenvalues-eigenvectors']"
52,"Inverse of sum of two marices, one being diagonal and other unitary.","Inverse of sum of two marices, one being diagonal and other unitary.",,"$C = A+D$ , $A$ being square matrix and $D$ a full rank diagonal matrix. Is there any easy way to compute $C^{-1}$ from $A^{-1}$ and $D$ Edit 2: (important edit) Iam interested in this question, because my matrix $A$ is huge and so is $C$ . So computing inverse of $C$ is not practical, but luckily the matrix $A$ is unitary, so $A^{-1} = A^*$ , so I easily have $A^{-1}$ , and finding ways to use it to get $C^{-1}$ .",", being square matrix and a full rank diagonal matrix. Is there any easy way to compute from and Edit 2: (important edit) Iam interested in this question, because my matrix is huge and so is . So computing inverse of is not practical, but luckily the matrix is unitary, so , so I easily have , and finding ways to use it to get .",C = A+D A D C^{-1} A^{-1} D A C C A A^{-1} = A^* A^{-1} C^{-1},"['linear-algebra', 'matrices']"
53,closed formula for determinant,closed formula for determinant,,"Consider the following matrix $$ \begin{equation}A_{r-1} :=  \begin{bmatrix} \frac{1}{x_{1}}	& -p	& \dots	 & 0  &\dots &0    \\ -q	& \frac{1}{x_{2}} & -p &0 	& \dots  & 0 	  \\ 0	& -q & \frac{1}{x_{3}} &-p 	& ~...  & 0 	  \\ 0	& 0 &-q &\frac{1}{x_{4}} &-p  	& 0 	  \\ 0  &\vdots	 	 & \ddots & -q & \frac{1}{x_{r-2}}  & -p \\ 0 	&0 &0 &\dots &-q	 &\frac{1}{x_{r-1}} \end{bmatrix} \end{equation} $$ where $x_{i},p,q \in \mathbb{R}$ , $x_{i} \neq 0$ for all $i = 1,2,...,r-1$ and $r \in \mathbb{N}$ , $r \geq 3$ . I want to find a closed formula for $\det(A)$ . For $r=3$ we have $$ \begin{equation} \det(A) =  \begin{vmatrix} \frac{1}{x_1} & -p   \\ -q & \frac{1}{x_2} \\ \end{vmatrix} = \frac{1 -pq(x_1 x_2)}{x_1 x_2}\end{equation} . $$ For $r = 4$ we have $$ \begin{equation} \det(A) =  \begin{vmatrix} \frac{1}{x_1} & -p & 0   \\ -q & \frac{1}{x_2} & -p \\ 0  &-q &\frac{1}{x_3}  \\ \end{vmatrix} = \frac{1 -pq(x_1 x_2 + x_2 x_3)}{x_1 x_2 x_3} \end{equation} . $$ Up to this point I think the formula is given by $$ \det(A) = \frac{1 - pq(x_{r-1}x_{r-2} + x_{r-2}x_{r-3})}{x_{1}x_{2}...x_{r-1}}. $$ But this is not correct. For $r = 5$ I get $$ \begin{equation} \det(A) =  \begin{vmatrix} \frac{1}{x_1} & -p & 0 &0   \\ -q & \frac{1}{x_2} & -p &0 \\ 0  &-q &\frac{1}{x_3} &-p  \\ 0 & 0 &-q &\frac{1}{x_4 } \\ \end{vmatrix} = \frac{1 -pq(x_1 x_2 + x_2 x_3 + x_3 x_4) + p^2 q^2x_1 x_2 x_3x_4}{x_1 x_2 x_3 x_4}. \end{equation} . $$ Has someone has an idea how to find a closed formula? Thanks for any hints!","Consider the following matrix where , for all and , . I want to find a closed formula for . For we have For we have Up to this point I think the formula is given by But this is not correct. For I get Has someone has an idea how to find a closed formula? Thanks for any hints!","
\begin{equation}A_{r-1} := 
\begin{bmatrix}
\frac{1}{x_{1}}	& -p	& \dots	 & 0  &\dots &0    \\
-q	& \frac{1}{x_{2}} & -p &0 	& \dots  & 0 	  \\
0	& -q & \frac{1}{x_{3}} &-p 	& ~...  & 0 	  \\
0	& 0 &-q &\frac{1}{x_{4}} &-p  	& 0 	  \\
0  &\vdots	 	 & \ddots & -q & \frac{1}{x_{r-2}}  & -p \\
0 	&0 &0 &\dots &-q	 &\frac{1}{x_{r-1}}
\end{bmatrix}
\end{equation}
 x_{i},p,q \in \mathbb{R} x_{i} \neq 0 i = 1,2,...,r-1 r \in \mathbb{N} r \geq 3 \det(A) r=3 
\begin{equation}
\det(A) = 
\begin{vmatrix}
\frac{1}{x_1} & -p   \\
-q & \frac{1}{x_2} \\
\end{vmatrix} = \frac{1 -pq(x_1 x_2)}{x_1
x_2}\end{equation} .
 r = 4 
\begin{equation}
\det(A) = 
\begin{vmatrix}
\frac{1}{x_1} & -p & 0   \\
-q & \frac{1}{x_2} & -p \\
0  &-q &\frac{1}{x_3}  \\
\end{vmatrix} = \frac{1 -pq(x_1 x_2 + x_2 x_3)}{x_1 x_2 x_3}
\end{equation} .
 
\det(A) = \frac{1 - pq(x_{r-1}x_{r-2} + x_{r-2}x_{r-3})}{x_{1}x_{2}...x_{r-1}}.
 r = 5 
\begin{equation}
\det(A) = 
\begin{vmatrix}
\frac{1}{x_1} & -p & 0 &0   \\
-q & \frac{1}{x_2} & -p &0 \\
0  &-q &\frac{1}{x_3} &-p  \\
0 & 0 &-q &\frac{1}{x_4 } \\
\end{vmatrix} = \frac{1 -pq(x_1 x_2 + x_2 x_3 + x_3 x_4) + p^2 q^2x_1 x_2 x_3x_4}{x_1 x_2 x_3 x_4}.
\end{equation} .
","['calculus', 'matrices', 'determinant']"
54,Matrices of functions commute,Matrices of functions commute,,"Given two $n\times n$ matrices  $A(z), B(z)$ with entire functions entries; with $A$ and $B$  invertible on $\mathbb{C}$, and $A$ is normal (i.e., $A(z)A^{*}(z)=A^{*}(z)A(z)$ on $\mathbb{C}$). If $A(z)B(z)=B(z)A(z)$ , and $A(z)B^{*}(z)=B^{*}(z)A(z)$  for all $z\in \mathbb{C}$, does this imply that $$A(z)B(w)=B(w)A(z) , \quad \text{for all}\; z,w\in \mathbb{C}?$$ Added: May be showing if $A(z)A(w)=A(w)A(z)$ could help!","Given two $n\times n$ matrices  $A(z), B(z)$ with entire functions entries; with $A$ and $B$  invertible on $\mathbb{C}$, and $A$ is normal (i.e., $A(z)A^{*}(z)=A^{*}(z)A(z)$ on $\mathbb{C}$). If $A(z)B(z)=B(z)A(z)$ , and $A(z)B^{*}(z)=B^{*}(z)A(z)$  for all $z\in \mathbb{C}$, does this imply that $$A(z)B(w)=B(w)A(z) , \quad \text{for all}\; z,w\in \mathbb{C}?$$ Added: May be showing if $A(z)A(w)=A(w)A(z)$ could help!",,"['matrices', 'complex-analysis']"
55,"Regarding a proof of: ""if $A,B \in M_n(\mathbb{k})$ are diagonalizable and commute, they are simultaneously diagonalizable"".","Regarding a proof of: ""if  are diagonalizable and commute, they are simultaneously diagonalizable"".","A,B \in M_n(\mathbb{k})","As the title states, I'm looking for a proof of the following, Proposition. Let $A, B \in M_n(\mathbb{k})$ be commuting diagonalizable matrices, so that $AB = BA$. Therefore, $A$ and $B$ can be diagonalized in the same basis. with these additional requirements: no usage of minimal polynomials, and as elementary an argument as possible. Looking for similar questions, I stumbled upon this answer. It proves that eigenvalues of $A$ are $B$-invariant and vice versa. If these were one dimensional, then by restricting $A$ or $B$ as functions to the eigenspaces of the other, we see that they share all eigenvectors (although possibly with different eigenvalues) and thus any base of them will diagonalize both matrices simultaneously. However, the case for eigenspaces of arbitrary dimension is left as an exercise. Any hints on how to proceed? Edit: upon reading this answer, I think the question can be reduced to: how can we show that given an eigenspace $E_\lambda$, $B : E_\lambda \to E_\lambda$ is diagonalizable? If this is answered, then since $$ \mathbb{k}^n = E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_n} $$ with $\lambda_1, \dots, \lambda_n$ the eigenvalues of $A$, and each restriction of $B$ to $E_{\lambda_i}$ can be diagonalized on a basis $B_i = \{v^i_1 , \dots, v^i_{k_i}\}$, the basis $\mathcal{B} = \cup_{i=1}^nB_i$ of $\mathbb{k}^n$ consists of eigenvectors of $B$ that are also eigenvectors of $A$, precisely because each $v_j^i \in E_{\lambda_i}$. Thus, each element of $\mathcal{B}$ would be an eigenvector for both $A$ and $B$, which implies that $\mathcal{B}$ diagonalizes the matrices at the same time. In short, if I have thought about this correctly, my question reduces to: how can one show that a $B$-invariant eigenspace of $A$ has a basis of eigenvectors of $B$?","As the title states, I'm looking for a proof of the following, Proposition. Let $A, B \in M_n(\mathbb{k})$ be commuting diagonalizable matrices, so that $AB = BA$. Therefore, $A$ and $B$ can be diagonalized in the same basis. with these additional requirements: no usage of minimal polynomials, and as elementary an argument as possible. Looking for similar questions, I stumbled upon this answer. It proves that eigenvalues of $A$ are $B$-invariant and vice versa. If these were one dimensional, then by restricting $A$ or $B$ as functions to the eigenspaces of the other, we see that they share all eigenvectors (although possibly with different eigenvalues) and thus any base of them will diagonalize both matrices simultaneously. However, the case for eigenspaces of arbitrary dimension is left as an exercise. Any hints on how to proceed? Edit: upon reading this answer, I think the question can be reduced to: how can we show that given an eigenspace $E_\lambda$, $B : E_\lambda \to E_\lambda$ is diagonalizable? If this is answered, then since $$ \mathbb{k}^n = E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_n} $$ with $\lambda_1, \dots, \lambda_n$ the eigenvalues of $A$, and each restriction of $B$ to $E_{\lambda_i}$ can be diagonalized on a basis $B_i = \{v^i_1 , \dots, v^i_{k_i}\}$, the basis $\mathcal{B} = \cup_{i=1}^nB_i$ of $\mathbb{k}^n$ consists of eigenvectors of $B$ that are also eigenvectors of $A$, precisely because each $v_j^i \in E_{\lambda_i}$. Thus, each element of $\mathcal{B}$ would be an eigenvector for both $A$ and $B$, which implies that $\mathcal{B}$ diagonalizes the matrices at the same time. In short, if I have thought about this correctly, my question reduces to: how can one show that a $B$-invariant eigenspace of $A$ has a basis of eigenvectors of $B$?",,"['linear-algebra', 'matrices', 'proof-explanation']"
56,Find the set of points where two functions are equal,Find the set of points where two functions are equal,,"Given the function: $$f(M, \vec{p}, d) = \sum_{i=1}^m log(\frac{\mathbb{I}(|\vec{M}_i - \vec{p}|_2<d)}{|\vec{M}_i - \vec{p}|_2})$$ Where $M \in \mathbb{R}^{m \times n}$ is a matrix, $\vec{p} \in \mathbb{R}^{n}$ is a vector describing a point, $d \in \mathbb{R}$ is a minimum distance scalar, and $|\vec{M}_i - \vec{p}|_2$ is the euclidean distance between two points. Also when $\mathbb{I}(|\vec{M}_i - \vec{p}|_2<d)$ is true 1 is returned, otherwise its zero. If I had two different matrices and plugged them into this function, how would I find the $n$ dimensional points where the two functions are equal to each other? As simple example let us look at the matrices $A$ and $B$: $$A =     \begin{bmatrix}     0.3 & 0.3 \\     0.3 & 0.4  \\     0.4 & 0.3 \\     \end{bmatrix} B =     \begin{bmatrix}     0.6 & 0.6 \\     0.6 & 0.7  \\     0.7 & 0.6 \\     \end{bmatrix} $$ And the minimum distance is: $$ d=0.3 $$ How would one go about finding the set of points where $f(A, p) = f(B, p)$? I know you would make them equal to one another like: $$\sum_{i=1}^3 log(\frac{\mathbb{I}(|\vec{A}_i - \vec{p}|_2<0.3)}{|\vec{A}_i - \vec{p}|_2}) = \sum_{i=1}^3 log(\frac{\mathbb{I}(|\vec{B}_i - \vec{p}|_2<0.3)}{|\vec{B}_i - \vec{p}|_2})$$ But I don't know how to solve this. Also bare in mind that I need this to work for examples with points of higher dimensionality ($n > 0$), and when the size of the $m$ dimension of the two matrices are not equal. To help you understand the problem more clearly I have created a visual aid: A graph describing the intersection of the two functions, where the red line indicates where the two functions would be equal. If you require any further details feel free to ask.","Given the function: $$f(M, \vec{p}, d) = \sum_{i=1}^m log(\frac{\mathbb{I}(|\vec{M}_i - \vec{p}|_2<d)}{|\vec{M}_i - \vec{p}|_2})$$ Where $M \in \mathbb{R}^{m \times n}$ is a matrix, $\vec{p} \in \mathbb{R}^{n}$ is a vector describing a point, $d \in \mathbb{R}$ is a minimum distance scalar, and $|\vec{M}_i - \vec{p}|_2$ is the euclidean distance between two points. Also when $\mathbb{I}(|\vec{M}_i - \vec{p}|_2<d)$ is true 1 is returned, otherwise its zero. If I had two different matrices and plugged them into this function, how would I find the $n$ dimensional points where the two functions are equal to each other? As simple example let us look at the matrices $A$ and $B$: $$A =     \begin{bmatrix}     0.3 & 0.3 \\     0.3 & 0.4  \\     0.4 & 0.3 \\     \end{bmatrix} B =     \begin{bmatrix}     0.6 & 0.6 \\     0.6 & 0.7  \\     0.7 & 0.6 \\     \end{bmatrix} $$ And the minimum distance is: $$ d=0.3 $$ How would one go about finding the set of points where $f(A, p) = f(B, p)$? I know you would make them equal to one another like: $$\sum_{i=1}^3 log(\frac{\mathbb{I}(|\vec{A}_i - \vec{p}|_2<0.3)}{|\vec{A}_i - \vec{p}|_2}) = \sum_{i=1}^3 log(\frac{\mathbb{I}(|\vec{B}_i - \vec{p}|_2<0.3)}{|\vec{B}_i - \vec{p}|_2})$$ But I don't know how to solve this. Also bare in mind that I need this to work for examples with points of higher dimensionality ($n > 0$), and when the size of the $m$ dimension of the two matrices are not equal. To help you understand the problem more clearly I have created a visual aid: A graph describing the intersection of the two functions, where the red line indicates where the two functions would be equal. If you require any further details feel free to ask.",,"['matrices', 'functions', 'vector-spaces', 'summation', 'euclidean-domain']"
57,Quick questions about summation notation used in my book,Quick questions about summation notation used in my book,,"My Questions In the screenshot below, is the summation notation in the orange (top) box and blue (bottom) box the exact same? So in a $3$ by $3$ matrix you'd only want to sum $a_{12} + a_{13} + a_{23}$ . Is it correct to say that the orange box is exactly the same as $$\sum_{i = 1}^{n-1} \sum_{j = i + 1}^n P(A_i A_j)?$$ Is it correct to say that the BLUE box is exactly the same as $$\sum_{(i,j):i<j} E[\min(X_i,X_j)]?$$ Why would you choose one notation or the other? It seems the blue box is faster to write. Is it correct to say that the following will always hold if $i$ and $j$ span the exact same indices and $a_{ij} = a_{ji}$ for all $i$ and $j$ ? $$\sum_{i \neq j} a_{ij} = 2\sum_{i<j} a_{ij}$$ Thank you for your time and patience.","My Questions In the screenshot below, is the summation notation in the orange (top) box and blue (bottom) box the exact same? So in a by matrix you'd only want to sum . Is it correct to say that the orange box is exactly the same as Is it correct to say that the BLUE box is exactly the same as Why would you choose one notation or the other? It seems the blue box is faster to write. Is it correct to say that the following will always hold if and span the exact same indices and for all and ? Thank you for your time and patience.","3 3 a_{12} + a_{13} + a_{23} \sum_{i = 1}^{n-1} \sum_{j = i + 1}^n P(A_i A_j)? \sum_{(i,j):i<j} E[\min(X_i,X_j)]? i j a_{ij} = a_{ji} i j \sum_{i \neq j} a_{ij} = 2\sum_{i<j} a_{ij}","['linear-algebra', 'matrices', 'algebra-precalculus', 'notation']"
58,Single Value Decomposition from Eigenvectors,Single Value Decomposition from Eigenvectors,,"Attempting to solve the SVD of this matrix: \begin{bmatrix}     2       & 0 \\     0       & -3 \\     0       & 0 \end{bmatrix} I know that one possible (my understanding is the SVD is not necessarily a unique result) result is: $$ \begin{bmatrix}     1    & 0    & 0 \\     0    & -1   & 0 \\     0    & 0    & 1 \end{bmatrix} \cdot \begin{bmatrix}     2       & 0 \\     0       & 3 \\     0       & 0 \end{bmatrix} \cdot \begin{bmatrix}     1    & 0 \\     0    & 1 \end{bmatrix}^T = \begin{bmatrix}     2       & 0 \\     0       & -3 \\     0       & 0 \end{bmatrix} $$ Which I have verified. I am trying to solve this problem myself, and am getting a different result; in particular, one that does not satisfy $A=U\Sigma V^T$. Therefore, I am trying to understand the hole in my approach. Currently, my result looks like this: $$ \begin{bmatrix}     0    & 0    & 1 \\     1    & 0    & 0 \\     0    & 1    & 0 \end{bmatrix} \cdot \begin{bmatrix}     3    & 0 \\     0    & 2 \\     0    & 0  \end{bmatrix} \cdot \begin{bmatrix}     0    & 1 \\     1    & 0  \end{bmatrix}^T = \begin{bmatrix}     0    & 0 \\     0    & 3 \\     2    & 0  \end{bmatrix} $$ This is my approach, given a matrix $M$ 1. Construct $MM^T$ and $M^TM$ $$ MM^T = \begin{bmatrix}     4    & 0    & 0 \\     0    & 9    & 0 \\     0    & 0    & 0 \end{bmatrix} \quad\&\quad M^TM = \begin{bmatrix}     4    & 0 \\     0    & 9 \\ \end{bmatrix} $$ 2. Solve for the eigenvalues and eigenvectors of both $MM^T$ and $M^TM$ This is accomplished via QR Factorization. Eigenvectors are listed in order of highest magnitude eigenvalue to lowest magnitude eigenvalue. The results are as follows: $$ MM^T \enspace eigenvalues =  \begin{bmatrix}     9    & 4    & 0 \end{bmatrix} $$ $$ MM^T \enspace eigenvectors =  \begin{bmatrix}     0 \\     1 \\     0 \end{bmatrix} \begin{bmatrix}     0 \\     0 \\     1 \end{bmatrix} \begin{bmatrix}     1 \\     0 \\     0 \end{bmatrix} $$ $$ M^TM \enspace eigenvalues =  \begin{bmatrix}     9    & 4  \end{bmatrix} $$ $$ M^TM \enspace eigenvectors =  \begin{bmatrix}     0 \\     1  \end{bmatrix} \begin{bmatrix}     1 \\     0  \end{bmatrix} $$ 3. Construct $U$ using the eigenvalue-ordered eigenvectors of $MM^T$ as the columns of the matrix. Construct $V$ in the same way, but with $M^TM$. I would also apply Gram-Schmidt orthonormalization here, but these matrices are already orthonormal. $$ U =  \begin{bmatrix}     0    & 0    & 1 \\     1    & 0    & 0 \\     0    & 1    & 0 \end{bmatrix} $$ $$ V = \begin{bmatrix}     0    & 1 \\     1    & 0  \end{bmatrix} $$ 4. Finally, construct $\Sigma$ by making a diagonal matrix whose values are the square roots of the non-zero eigenvalues from $MM^T$ or $M^TM$. Presumably, I should preserve the eigenvalue order when I do this? $$ \Sigma = \begin{bmatrix}     3    & 0 \\     0    & 2 \\     0    & 0  \end{bmatrix} $$ So my question is, if anyone can aid me in identifying either the mistake I've made, or the hole in my approach to this problem. Assuming that I haven't made a simple error, I believe I am missing some sort of insight as to the construction of these matrices (because you can see, for instance, that the known solution has very similar values in differing orders and signs).","Attempting to solve the SVD of this matrix: \begin{bmatrix}     2       & 0 \\     0       & -3 \\     0       & 0 \end{bmatrix} I know that one possible (my understanding is the SVD is not necessarily a unique result) result is: $$ \begin{bmatrix}     1    & 0    & 0 \\     0    & -1   & 0 \\     0    & 0    & 1 \end{bmatrix} \cdot \begin{bmatrix}     2       & 0 \\     0       & 3 \\     0       & 0 \end{bmatrix} \cdot \begin{bmatrix}     1    & 0 \\     0    & 1 \end{bmatrix}^T = \begin{bmatrix}     2       & 0 \\     0       & -3 \\     0       & 0 \end{bmatrix} $$ Which I have verified. I am trying to solve this problem myself, and am getting a different result; in particular, one that does not satisfy $A=U\Sigma V^T$. Therefore, I am trying to understand the hole in my approach. Currently, my result looks like this: $$ \begin{bmatrix}     0    & 0    & 1 \\     1    & 0    & 0 \\     0    & 1    & 0 \end{bmatrix} \cdot \begin{bmatrix}     3    & 0 \\     0    & 2 \\     0    & 0  \end{bmatrix} \cdot \begin{bmatrix}     0    & 1 \\     1    & 0  \end{bmatrix}^T = \begin{bmatrix}     0    & 0 \\     0    & 3 \\     2    & 0  \end{bmatrix} $$ This is my approach, given a matrix $M$ 1. Construct $MM^T$ and $M^TM$ $$ MM^T = \begin{bmatrix}     4    & 0    & 0 \\     0    & 9    & 0 \\     0    & 0    & 0 \end{bmatrix} \quad\&\quad M^TM = \begin{bmatrix}     4    & 0 \\     0    & 9 \\ \end{bmatrix} $$ 2. Solve for the eigenvalues and eigenvectors of both $MM^T$ and $M^TM$ This is accomplished via QR Factorization. Eigenvectors are listed in order of highest magnitude eigenvalue to lowest magnitude eigenvalue. The results are as follows: $$ MM^T \enspace eigenvalues =  \begin{bmatrix}     9    & 4    & 0 \end{bmatrix} $$ $$ MM^T \enspace eigenvectors =  \begin{bmatrix}     0 \\     1 \\     0 \end{bmatrix} \begin{bmatrix}     0 \\     0 \\     1 \end{bmatrix} \begin{bmatrix}     1 \\     0 \\     0 \end{bmatrix} $$ $$ M^TM \enspace eigenvalues =  \begin{bmatrix}     9    & 4  \end{bmatrix} $$ $$ M^TM \enspace eigenvectors =  \begin{bmatrix}     0 \\     1  \end{bmatrix} \begin{bmatrix}     1 \\     0  \end{bmatrix} $$ 3. Construct $U$ using the eigenvalue-ordered eigenvectors of $MM^T$ as the columns of the matrix. Construct $V$ in the same way, but with $M^TM$. I would also apply Gram-Schmidt orthonormalization here, but these matrices are already orthonormal. $$ U =  \begin{bmatrix}     0    & 0    & 1 \\     1    & 0    & 0 \\     0    & 1    & 0 \end{bmatrix} $$ $$ V = \begin{bmatrix}     0    & 1 \\     1    & 0  \end{bmatrix} $$ 4. Finally, construct $\Sigma$ by making a diagonal matrix whose values are the square roots of the non-zero eigenvalues from $MM^T$ or $M^TM$. Presumably, I should preserve the eigenvalue order when I do this? $$ \Sigma = \begin{bmatrix}     3    & 0 \\     0    & 2 \\     0    & 0  \end{bmatrix} $$ So my question is, if anyone can aid me in identifying either the mistake I've made, or the hole in my approach to this problem. Assuming that I haven't made a simple error, I believe I am missing some sort of insight as to the construction of these matrices (because you can see, for instance, that the known solution has very similar values in differing orders and signs).",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd']"
59,Which function on $\mathbf{X}$ has gradient $\mathbf{A} \mathbf{A}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} $?,Which function on  has gradient ?,\mathbf{X} \mathbf{A} \mathbf{A}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} ,"I learned from The Matrix Cookbook that the gradient of the $\log \det$ function is given by \begin{equation} \nabla \log \text{det}(\mathbf{X}^\top \mathbf{X})=2\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}, \end{equation} where $\mathbf{X}\in\mathbb{R}^{n\times r}$. I wonder which function will give the gradient \begin{equation} 2\mathbf{A} \mathbf{A}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}, \end{equation} for some matrix $\mathbf{A}\in \mathbb{R}^{n\times r}$.","I learned from The Matrix Cookbook that the gradient of the $\log \det$ function is given by \begin{equation} \nabla \log \text{det}(\mathbf{X}^\top \mathbf{X})=2\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}, \end{equation} where $\mathbf{X}\in\mathbb{R}^{n\times r}$. I wonder which function will give the gradient \begin{equation} 2\mathbf{A} \mathbf{A}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}, \end{equation} for some matrix $\mathbf{A}\in \mathbb{R}^{n\times r}$.",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus']"
60,"what is ranks do in Singular value decomposition, if rank = k, others than k first singular values set to 0 or gone?","what is ranks do in Singular value decomposition, if rank = k, others than k first singular values set to 0 or gone?",,"hi lets assume we have matrix A with 4 rows x 3 columns, when we input to svd it become: U = 4x4 , S = 4x3 and VT = 3x3. after that i specify that rank k=2 so what exactly happen to the other than the first k ranks? is it set to zero or completely gone like U = 4x2 S=2x2 and VT = 2x3? both cases resulting the same rows and columns, right? which is 4x3, but does it affect the value of the reduced matrix?","hi lets assume we have matrix A with 4 rows x 3 columns, when we input to svd it become: U = 4x4 , S = 4x3 and VT = 3x3. after that i specify that rank k=2 so what exactly happen to the other than the first k ranks? is it set to zero or completely gone like U = 4x2 S=2x2 and VT = 2x3? both cases resulting the same rows and columns, right? which is 4x3, but does it affect the value of the reduced matrix?",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd']"
61,Is the determinant of this symmetric matrix non-negative?,Is the determinant of this symmetric matrix non-negative?,,"Let $a_i \in [-1,1]$ and define the $n\times n$ -matrix $M$ by $$ M_{i,j} = \begin{cases}     1-|a_i - a_j|,& \text{if } a_ia_j>0 \\     0,              & \text{otherwise}. \end{cases} $$ $M$ is clearly symmetric with $1$ s on the diagonal. Can we prove that it has a non-negative determinant ( $\det(M) \ge 0$ ) or that it is PSD? I was unable to find a counter-example numerically. If $a_i \in [0,1]$ for each $i$ , the ""otherwise"" case would be of no effect and I can show that the matrix is PSD. However, I'm not sure how to deal with the general case above. Hints are also appreciated.","Let and define the -matrix by is clearly symmetric with s on the diagonal. Can we prove that it has a non-negative determinant ( ) or that it is PSD? I was unable to find a counter-example numerically. If for each , the ""otherwise"" case would be of no effect and I can show that the matrix is PSD. However, I'm not sure how to deal with the general case above. Hints are also appreciated.","a_i \in [-1,1] n\times n M 
M_{i,j} = \begin{cases}
    1-|a_i - a_j|,& \text{if } a_ia_j>0 \\
    0,              & \text{otherwise}.
\end{cases}
 M 1 \det(M) \ge 0 a_i \in [0,1] i","['linear-algebra', 'matrices']"
62,A real upper-triangular matrix commutes with its transpose then the matrix is diagonal.,A real upper-triangular matrix commutes with its transpose then the matrix is diagonal.,,"Let $A$ be a real, upper-triangular, $n\times n$  matrix  that commutes with its transpose. How can I show that $A$ is diagonal? I want to show that $A-A^t =0$ but I can't. Please help me.","Let $A$ be a real, upper-triangular, $n\times n$  matrix  that commutes with its transpose. How can I show that $A$ is diagonal? I want to show that $A-A^t =0$ but I can't. Please help me.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'diagonalization']"
63,Maximal diagonalization of a matrix by permutation matrices,Maximal diagonalization of a matrix by permutation matrices,,"I found an interesting problem based on a project I'm working on at my job. I'd like to share and see if anyone either knows if it is well-known or if anyone has any algorithms or techniques for approaching it: Let $M$ be an $n \times n$ square matrix. It suffices to consider $M$ to have entries of $0$ and $1$ only and for it to be fairly sparse. Let's further suppose that $M$ is block-diagonal and that the blocks are $B_1, B_2, \cdots, B_k$, where each block matrix $B_i$ is a $m_i \times m_i$ square matrix. We define the ""diagonality"" of $M$ to be the smallest sum $\sum_{i = 1}^k m_i^2$ we can achieve We have to choose the ""smallest"" such value since we usually have some choice with the size of the $B_i$'s. A $2 \times 2$ identity matrix, for instance, can be viewed as having either one $2 \times 2$ block or two $1 \times 1$ blocks, giving scores of $2^2 + 2^2 = 8$ and $1^2 = 1$ respectively. And so we choose $1$ in this case. So a diagonal matrix will always have diagonality $n$ while a matrix of all $1$'s will have a diagonality of $n^2$. The nontrivial upper triangular $2 \times 2$ matrix will have diagonality $4$. Etc. My question is then this: Starting with a fixed matrix $M$, I suppose I am allowed to permute the rows and columns at whim. Equivalently, I am allowed to pre- or post-multiply by any permutation matrix. In doing this, how would I go finding the permutations of the rows and/or columns that minimize the diagonality of the resulting matrix? For instance, let's take the $3 \times 3$ matrix $$ \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix} $$ This starts off having diagonality $9$. I could permute row $1$ and row $2$ to get: \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 0 & 1 \\ \end{bmatrix} And then I can permute column $1$ and column $2$ to get: \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \\ \end{bmatrix} And the result now has diagonality $5$ rather than $9$. Obviously, the setup here is my own, so I imagine that if this kind of  thing has been explored, it's been explored in greater generality. I'd love to know if anyone can lend me some insight into how I might go about minimizing this quantity.","I found an interesting problem based on a project I'm working on at my job. I'd like to share and see if anyone either knows if it is well-known or if anyone has any algorithms or techniques for approaching it: Let $M$ be an $n \times n$ square matrix. It suffices to consider $M$ to have entries of $0$ and $1$ only and for it to be fairly sparse. Let's further suppose that $M$ is block-diagonal and that the blocks are $B_1, B_2, \cdots, B_k$, where each block matrix $B_i$ is a $m_i \times m_i$ square matrix. We define the ""diagonality"" of $M$ to be the smallest sum $\sum_{i = 1}^k m_i^2$ we can achieve We have to choose the ""smallest"" such value since we usually have some choice with the size of the $B_i$'s. A $2 \times 2$ identity matrix, for instance, can be viewed as having either one $2 \times 2$ block or two $1 \times 1$ blocks, giving scores of $2^2 + 2^2 = 8$ and $1^2 = 1$ respectively. And so we choose $1$ in this case. So a diagonal matrix will always have diagonality $n$ while a matrix of all $1$'s will have a diagonality of $n^2$. The nontrivial upper triangular $2 \times 2$ matrix will have diagonality $4$. Etc. My question is then this: Starting with a fixed matrix $M$, I suppose I am allowed to permute the rows and columns at whim. Equivalently, I am allowed to pre- or post-multiply by any permutation matrix. In doing this, how would I go finding the permutations of the rows and/or columns that minimize the diagonality of the resulting matrix? For instance, let's take the $3 \times 3$ matrix $$ \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix} $$ This starts off having diagonality $9$. I could permute row $1$ and row $2$ to get: \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 0 & 1 \\ \end{bmatrix} And then I can permute column $1$ and column $2$ to get: \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \\ \end{bmatrix} And the result now has diagonality $5$ rather than $9$. Obviously, the setup here is my own, so I imagine that if this kind of  thing has been explored, it's been explored in greater generality. I'd love to know if anyone can lend me some insight into how I might go about minimizing this quantity.",,"['matrices', 'algorithms', 'discrete-optimization']"
64,If all the $k$-minors of a matrix are non-zero then its rank is greater than $k$?,If all the -minors of a matrix are non-zero then its rank is greater than ?,k k,"Let $A$ be a $d \times d$ real matrix, and let $1<k<d$. Suppose that all the $k$-minors of $A$ are non-zero. Is it necessarily true that $\text{rank}(A)>k$? I am looking for a counter-example. When $k=1$, this is obviously false; take $A=\begin{pmatrix} 1 & 2 \\\ 1 & 2 \end{pmatrix}$. All the entries of $A$ (the $1$-minors) are non-zero, but $\text{rank}(A)=1$. Is there an easy way to generalize this example to $k>1$? Comment: In the converse direction, we do know that $\text{rank}(A)>k$ implies there are many non-zero $k$-minors; There exist a non-zero $k+1$-minor; omitting any row and column participating in this minor, we obtain a non-zero $k$-minor. Thus we have at least $(k+1)^2$ non-zero $k$-minors. This lower bound is sharp; there exist matrices of rank $k+1$ with exactly $(k+1)^2$ non-zero $k$-minors- e.g. take $\text{Id}_{k+1 \times k+1} \otimes O_{d-k-1  \times d-k-1}$.","Let $A$ be a $d \times d$ real matrix, and let $1<k<d$. Suppose that all the $k$-minors of $A$ are non-zero. Is it necessarily true that $\text{rank}(A)>k$? I am looking for a counter-example. When $k=1$, this is obviously false; take $A=\begin{pmatrix} 1 & 2 \\\ 1 & 2 \end{pmatrix}$. All the entries of $A$ (the $1$-minors) are non-zero, but $\text{rank}(A)=1$. Is there an easy way to generalize this example to $k>1$? Comment: In the converse direction, we do know that $\text{rank}(A)>k$ implies there are many non-zero $k$-minors; There exist a non-zero $k+1$-minor; omitting any row and column participating in this minor, we obtain a non-zero $k$-minor. Thus we have at least $(k+1)^2$ non-zero $k$-minors. This lower bound is sharp; there exist matrices of rank $k+1$ with exactly $(k+1)^2$ non-zero $k$-minors- e.g. take $\text{Id}_{k+1 \times k+1} \otimes O_{d-k-1  \times d-k-1}$.",,"['linear-algebra', 'matrices', 'determinant', 'matrix-rank']"
65,rank of a rectangular Vandermonde matrix,rank of a rectangular Vandermonde matrix,,"Let the $m\times (n+1)$ rectangular Vandermonde matrix be $V$. More specifically, the matrix $V$ has the following form. $V=\begin{pmatrix} 1 & a_1 & \cdots & a_1^{n} \\  1 & a_2 & \cdots & a_2^{n}\\  \vdots& \vdots & \ddots  &\vdots \\  1 & a_m & \cdots & a_m^{n}  \end{pmatrix}$ , where $m \geq n$. I want a proof that $ \operatorname{rank}(V)=n$, if and only if the $a_i$ take exactly $n$ different values. Can you recommend me any paper or book that has a formal proof?","Let the $m\times (n+1)$ rectangular Vandermonde matrix be $V$. More specifically, the matrix $V$ has the following form. $V=\begin{pmatrix} 1 & a_1 & \cdots & a_1^{n} \\  1 & a_2 & \cdots & a_2^{n}\\  \vdots& \vdots & \ddots  &\vdots \\  1 & a_m & \cdots & a_m^{n}  \end{pmatrix}$ , where $m \geq n$. I want a proof that $ \operatorname{rank}(V)=n$, if and only if the $a_i$ take exactly $n$ different values. Can you recommend me any paper or book that has a formal proof?",,"['linear-algebra', 'matrices', 'matrix-rank', 'geometric-progressions']"
66,How to prove that inverse matrix is element-wise greater than the other,How to prove that inverse matrix is element-wise greater than the other,,"Suppose we have two matrices $A$ and $B$ (both have size $n\times n$), where both $A$ and $B$ are positive definite. We want to determine if every element of $A^{-1}$ is greater than the corresponding element of $B^{-1}$. However, we cannot directly calculate $A^{-1}$ and $B^{-1}$. Of course, the comparison result will depend on the specific forms of $A$ and $B$. But I want to know if one can tell if $A^{-1}$ is element-wise greater than $B^{-1}$ by just observing $A$ and $B$. Or in other words, what properties should $A$ and $B$ have so that $A^{-1}$ is element-wise greater than $B^{-1}$?","Suppose we have two matrices $A$ and $B$ (both have size $n\times n$), where both $A$ and $B$ are positive definite. We want to determine if every element of $A^{-1}$ is greater than the corresponding element of $B^{-1}$. However, we cannot directly calculate $A^{-1}$ and $B^{-1}$. Of course, the comparison result will depend on the specific forms of $A$ and $B$. But I want to know if one can tell if $A^{-1}$ is element-wise greater than $B^{-1}$ by just observing $A$ and $B$. Or in other words, what properties should $A$ and $B$ have so that $A^{-1}$ is element-wise greater than $B^{-1}$?",,"['linear-algebra', 'matrices', 'inverse', 'positive-definite']"
67,Finding Rational Canonical Form,Finding Rational Canonical Form,,"So the question I want to answer is the following: Ggive an example of a $10 \times 10$ matrix (over $\mathbb{R}$ ) whose minimal polynomial is $(x^{4}-2)(x+2)^{2}$ and is not similar to any matrix with rational entries. Here is my approach and the questions I have. We have that by the decomposition of modules, we have $$\mathbb{R}[x]/(a_{1}) \bigoplus \cdots \bigoplus \mathbb{R}[x]/(a_{m})$$ where $a_{1}|a_{2}|\dots|a_{m}$ . I read from Dummit and Foote that the minimal polynomial is the largest invariant factor so we get that $a_{m}$ is the minimal polynomial. So my attempt is to consider $$ \frac{\mathbb{R}[x]}{(x-\sqrt[4]{2})}\bigoplus \frac{\mathbb{R}[x]}{(x^{2}-\sqrt{2})(x+2)} \bigoplus \frac{\mathbb{R}[x]}{(x^{4}-2)(x+2)^{2}} $$ The conditions above are satisfied since clearly $a_{1}|a_{2}|a_{3}$ and $a_{3}$ is the minimal polyomial. From what I read in Dummit and Foote, the rational form is the companion matrices of these 3 factors. So my first question is, for the 3rd factor, $\frac{\mathbb{R}[x]}{(x^{4}-2)(x+2)^{2}}$ , my confusion is: Since we are quotienting by a degree 6, would this be a 6 by 6 companion matrix for our polynomial $(x^{4}-2)(x+2)^{2}$ or would it be a 4x4 companion matrix for $(x^{4}-2)$ followed by another 2x2 block which is the companion matrix of $(x+2)^{2}$ . Or since we have that our matrix is over $\mathbb{R}$ , our minimal polynomial factors as $$(x^{4}-2)(x+2)^{2}=(x-\sqrt[4]{2})(x+\sqrt[4]{2})(x^{2}+2)(x+2)^{2}$$ so we actually have $1\times1$ , $1\times1$ , $2\times2$ , and $2\times2$ companion matrices. As for my second question, unless there is a faster way, if there was a rational matrix,B, that was similar to my rational form, call it A, then $$A=PBP^{-1}$$ But then $$\det(A) = \det(B)$$ so I just need to show that my matrix (I haven't computed this long determinant for $10 \times 10$ since I am not sure what my matrix is as from my 1st question), has determinant that is not in $\mathbb{Q}$ right? Since the determinant of $B$ is in $\mathbb{Q}$ , my hope is by including those 4th roots of $2$ , I get $\det(A) \not \in \mathbb{Q}$ .","So the question I want to answer is the following: Ggive an example of a matrix (over ) whose minimal polynomial is and is not similar to any matrix with rational entries. Here is my approach and the questions I have. We have that by the decomposition of modules, we have where . I read from Dummit and Foote that the minimal polynomial is the largest invariant factor so we get that is the minimal polynomial. So my attempt is to consider The conditions above are satisfied since clearly and is the minimal polyomial. From what I read in Dummit and Foote, the rational form is the companion matrices of these 3 factors. So my first question is, for the 3rd factor, , my confusion is: Since we are quotienting by a degree 6, would this be a 6 by 6 companion matrix for our polynomial or would it be a 4x4 companion matrix for followed by another 2x2 block which is the companion matrix of . Or since we have that our matrix is over , our minimal polynomial factors as so we actually have , , , and companion matrices. As for my second question, unless there is a faster way, if there was a rational matrix,B, that was similar to my rational form, call it A, then But then so I just need to show that my matrix (I haven't computed this long determinant for since I am not sure what my matrix is as from my 1st question), has determinant that is not in right? Since the determinant of is in , my hope is by including those 4th roots of , I get .",10 \times 10 \mathbb{R} (x^{4}-2)(x+2)^{2} \mathbb{R}[x]/(a_{1}) \bigoplus \cdots \bigoplus \mathbb{R}[x]/(a_{m}) a_{1}|a_{2}|\dots|a_{m} a_{m}  \frac{\mathbb{R}[x]}{(x-\sqrt[4]{2})}\bigoplus \frac{\mathbb{R}[x]}{(x^{2}-\sqrt{2})(x+2)} \bigoplus \frac{\mathbb{R}[x]}{(x^{4}-2)(x+2)^{2}}  a_{1}|a_{2}|a_{3} a_{3} \frac{\mathbb{R}[x]}{(x^{4}-2)(x+2)^{2}} (x^{4}-2)(x+2)^{2} (x^{4}-2) (x+2)^{2} \mathbb{R} (x^{4}-2)(x+2)^{2}=(x-\sqrt[4]{2})(x+\sqrt[4]{2})(x^{2}+2)(x+2)^{2} 1\times1 1\times1 2\times2 2\times2 A=PBP^{-1} \det(A) = \det(B) 10 \times 10 \mathbb{Q} B \mathbb{Q} 2 \det(A) \not \in \mathbb{Q},"['linear-algebra', 'abstract-algebra', 'matrices', 'minimal-polynomials']"
68,Finding a Jordan basis of a $3\times 3$ matrix,Finding a Jordan basis of a  matrix,3\times 3,"Find a Jordan basis for the following matrix: $$A=     \begin{pmatrix}     1 & -3 & 4 \\     4 & -7 & 8 \\     6 & -7 & 7 \\     \end{pmatrix} $$ Hey everyone. First I have found the characteristic polynomial which is $(x-3)(x+1)^2$. Then i've found a basis for: $$\ker(3I-A)=\ker\begin{pmatrix}2 & 3 & -4 \\-4 & 10 & -8 \\-6 & 7 & -4 \\\end{pmatrix}$$ $B_1= \{(\frac{1}{2},1,1)^T\} $. Then, for the next Jordan block I've calculated $\ker(-I-A)=\ker \begin{pmatrix}     -2 & 3 & -4 \\     -4 & 6 & -8 \\     -6 & 7 & -8 \\     \end{pmatrix} $ and found a basis for this subspace- $B_2= \{(1,2,1)^T\}. \dim(\ker(-I-A))=1\neq a_m(\lambda)=2$ so we find a basis for $\ker(-I-A)^2 \Rightarrow B_3=\{(1,1,0)^T,(0,1,1)^T\}$ and choose $e_1=(1,0,0)^T \ (e_1 \notin Sp(B_2)) $ to complete $B_3$ to a basis of $\mathbb{R^3}$. Hence, our first chain is the vector $v_1= \{(\frac{1}{2},1,1)^T\}$, and the second chain is $\{(-I-A)e_1, e_1\}=\{(-2,-4-6)^T, (1,0,0)^T\} $ therefor our basis is $B= \{(\frac{1}{2},1,1)^T, (-2,-4-6)^T, (1,0,0)^T \} $ But $P^{-1}AP $ where $P=\begin{pmatrix}     \frac{1}{2} & -2 & 1 \\     1 & -4 & 0 \\     1 & -6 & 0 \\     \end{pmatrix}$ equals $\begin{pmatrix}     3 & -32 & 0 \\     0 & -1 & -1 \\     0 & 0 & -1 \\     \end{pmatrix} \neq \begin{pmatrix}     3 & 0 & 0 \\     0 & -1 & 1 \\     0 & 0 & -1 \\     \end{pmatrix}$ I've done numerous tries with different vectors other than $e_1$ yet I did not achieve the matrix' Jordan form. I would be happy if you could help me find my mistakes. Thanks in advance :)","Find a Jordan basis for the following matrix: $$A=     \begin{pmatrix}     1 & -3 & 4 \\     4 & -7 & 8 \\     6 & -7 & 7 \\     \end{pmatrix} $$ Hey everyone. First I have found the characteristic polynomial which is $(x-3)(x+1)^2$. Then i've found a basis for: $$\ker(3I-A)=\ker\begin{pmatrix}2 & 3 & -4 \\-4 & 10 & -8 \\-6 & 7 & -4 \\\end{pmatrix}$$ $B_1= \{(\frac{1}{2},1,1)^T\} $. Then, for the next Jordan block I've calculated $\ker(-I-A)=\ker \begin{pmatrix}     -2 & 3 & -4 \\     -4 & 6 & -8 \\     -6 & 7 & -8 \\     \end{pmatrix} $ and found a basis for this subspace- $B_2= \{(1,2,1)^T\}. \dim(\ker(-I-A))=1\neq a_m(\lambda)=2$ so we find a basis for $\ker(-I-A)^2 \Rightarrow B_3=\{(1,1,0)^T,(0,1,1)^T\}$ and choose $e_1=(1,0,0)^T \ (e_1 \notin Sp(B_2)) $ to complete $B_3$ to a basis of $\mathbb{R^3}$. Hence, our first chain is the vector $v_1= \{(\frac{1}{2},1,1)^T\}$, and the second chain is $\{(-I-A)e_1, e_1\}=\{(-2,-4-6)^T, (1,0,0)^T\} $ therefor our basis is $B= \{(\frac{1}{2},1,1)^T, (-2,-4-6)^T, (1,0,0)^T \} $ But $P^{-1}AP $ where $P=\begin{pmatrix}     \frac{1}{2} & -2 & 1 \\     1 & -4 & 0 \\     1 & -6 & 0 \\     \end{pmatrix}$ equals $\begin{pmatrix}     3 & -32 & 0 \\     0 & -1 & -1 \\     0 & 0 & -1 \\     \end{pmatrix} \neq \begin{pmatrix}     3 & 0 & 0 \\     0 & -1 & 1 \\     0 & 0 & -1 \\     \end{pmatrix}$ I've done numerous tries with different vectors other than $e_1$ yet I did not achieve the matrix' Jordan form. I would be happy if you could help me find my mistakes. Thanks in advance :)",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
69,Interpreting the Jordan Normal Form,Interpreting the Jordan Normal Form,,"What's the best way to interpret Jordan Normal Form (e.g. in terms of a linear map)? For instance, how should we interpret those $1$'s?","What's the best way to interpret Jordan Normal Form (e.g. in terms of a linear map)? For instance, how should we interpret those $1$'s?",,"['matrices', 'matrix-decomposition', 'jordan-normal-form']"
70,Question about eigenvalues when a sequence of matrix converges,Question about eigenvalues when a sequence of matrix converges,,"Let ${A_n}$ be a sequence of $p \times p$ symmetric positive semi-definite matrix and $A_n$ converges to a matrix $A$, that is every elements of $A_n$ converges to corresponding element of $A$. Then can I say anything about the relationship between the eigenvaues of $A_n$ and $A$? For example, If the norm of $A_n-A$ is small enough, can I make the eigenvalues of $A_n$ close enough to that of $A$?","Let ${A_n}$ be a sequence of $p \times p$ symmetric positive semi-definite matrix and $A_n$ converges to a matrix $A$, that is every elements of $A_n$ converges to corresponding element of $A$. Then can I say anything about the relationship between the eigenvaues of $A_n$ and $A$? For example, If the norm of $A_n-A$ is small enough, can I make the eigenvalues of $A_n$ close enough to that of $A$?",,"['matrices', 'convergence-divergence', 'eigenvalues-eigenvectors', 'normed-spaces']"
71,Upper triangular matrices (Solving),Upper triangular matrices (Solving),,"If $T=\begin{bmatrix}A & C\\0 & B\end{bmatrix}$ is upper triangular with $A$ and $B$ upper triangular as well ($C$ is an arbitrary matrix), then what must $R$ be so that: $$ S^{-1}TS = \begin{bmatrix}A & 0\\0 & B\end{bmatrix} \,, $$ where $S = \begin{bmatrix}I & R\\0 & I\end{bmatrix}$? My attempt I tried to compute the equivalent statement $TS = S\begin{bmatrix}A & 0\\0 & B\end{bmatrix}$ first, which gave me: $$ \begin{gather*} \begin{bmatrix}A & C\\0 & B\end{bmatrix}\begin{bmatrix}I & R\\0 & I\end{bmatrix} = \begin{bmatrix}I & R\\0 & I\end{bmatrix}\begin{bmatrix}A & 0\\0 & B\end{bmatrix} \\ \begin{bmatrix}A & AR+C\\0 & B\end{bmatrix} = \begin{bmatrix}A & RB\\0 & B\end{bmatrix} \\ \Rightarrow AR+C = RB \end{gather*} $$ then I'm stuck. How do I extract matrix $R$ from this? I think there's probably a need to use the fact that $A$ and $B$ are upper triangular, but I don't know how. Note that this is the question as posed, I do not have the dimensions of any of the matrices.","If $T=\begin{bmatrix}A & C\\0 & B\end{bmatrix}$ is upper triangular with $A$ and $B$ upper triangular as well ($C$ is an arbitrary matrix), then what must $R$ be so that: $$ S^{-1}TS = \begin{bmatrix}A & 0\\0 & B\end{bmatrix} \,, $$ where $S = \begin{bmatrix}I & R\\0 & I\end{bmatrix}$? My attempt I tried to compute the equivalent statement $TS = S\begin{bmatrix}A & 0\\0 & B\end{bmatrix}$ first, which gave me: $$ \begin{gather*} \begin{bmatrix}A & C\\0 & B\end{bmatrix}\begin{bmatrix}I & R\\0 & I\end{bmatrix} = \begin{bmatrix}I & R\\0 & I\end{bmatrix}\begin{bmatrix}A & 0\\0 & B\end{bmatrix} \\ \begin{bmatrix}A & AR+C\\0 & B\end{bmatrix} = \begin{bmatrix}A & RB\\0 & B\end{bmatrix} \\ \Rightarrow AR+C = RB \end{gather*} $$ then I'm stuck. How do I extract matrix $R$ from this? I think there's probably a need to use the fact that $A$ and $B$ are upper triangular, but I don't know how. Note that this is the question as posed, I do not have the dimensions of any of the matrices.",,"['linear-algebra', 'matrices']"
72,On reducibility over $\mathbb{Z}$ of a special class of polynomials .,On reducibility over  of a special class of polynomials .,\mathbb{Z},"Definitions Let positive integer number $n$ be referred to as $p$-composite if there exist such positive integer numbers $k_1$ and $k_2$ that $$ n=k_1+k_2+2k_1k_2\equiv k_1*k_2. $$ Let $\mathbb{K}_n$ be the set of all distinct ordered pairs $(k_1,k_2)$, such that $n=k_1*k_2$. Let the cardinality of the set $\mathbb{K}_n$ incremented by 1 be referred to as composition index ${\cal I}_n$ of the number $n$. Based on numerical evidence I propose the following Conjecture The polynomial $$ P_n(x)=\sum_{i=0}^n(-1)^{\left\lfloor\frac{i+1}{2}\right\rfloor}\binom{\left\lfloor\frac{n+i}{2}\right\rfloor}{i}x^i $$  is reducible to product of exactly ${\cal I}_n$ irreducible polynomials over $\mathbb{Z}$. Particularly this means that if $n$ is not $p$-composite $({\cal I}_n=1)$ the polynomial $P_n(x)$ is irreducible. I would be thankful for any hint on proving or disproving the conjecture. The following information may appear useful: $P_n$ is characteristic polynomial of $n\times n$ dimensional integer matrix $M$ introduced in my previous question . (based on numerical evidence) The polynomial $P_{k_1}((-1)^{k_2}x)$ divides the polynomial $P_{k_1*k_2}(x)$. Notes added: The first $p$-composite number is $\small 4$ with $\small {\cal I}_4=2$, the next $\small7$ with $\small{\cal I}_7=3$ and so on. As pointed out by Ewan Delanoy, $\small {\cal I}_n$ is just the decremented by 1 number of all distinct divisors of $\small 2n+1$. More generally if $\small d | 2n+1$, $\small k=\frac{d-1}{2}$ is ""$\small p$-divisor"" of $\small n$.","Definitions Let positive integer number $n$ be referred to as $p$-composite if there exist such positive integer numbers $k_1$ and $k_2$ that $$ n=k_1+k_2+2k_1k_2\equiv k_1*k_2. $$ Let $\mathbb{K}_n$ be the set of all distinct ordered pairs $(k_1,k_2)$, such that $n=k_1*k_2$. Let the cardinality of the set $\mathbb{K}_n$ incremented by 1 be referred to as composition index ${\cal I}_n$ of the number $n$. Based on numerical evidence I propose the following Conjecture The polynomial $$ P_n(x)=\sum_{i=0}^n(-1)^{\left\lfloor\frac{i+1}{2}\right\rfloor}\binom{\left\lfloor\frac{n+i}{2}\right\rfloor}{i}x^i $$  is reducible to product of exactly ${\cal I}_n$ irreducible polynomials over $\mathbb{Z}$. Particularly this means that if $n$ is not $p$-composite $({\cal I}_n=1)$ the polynomial $P_n(x)$ is irreducible. I would be thankful for any hint on proving or disproving the conjecture. The following information may appear useful: $P_n$ is characteristic polynomial of $n\times n$ dimensional integer matrix $M$ introduced in my previous question . (based on numerical evidence) The polynomial $P_{k_1}((-1)^{k_2}x)$ divides the polynomial $P_{k_1*k_2}(x)$. Notes added: The first $p$-composite number is $\small 4$ with $\small {\cal I}_4=2$, the next $\small7$ with $\small{\cal I}_7=3$ and so on. As pointed out by Ewan Delanoy, $\small {\cal I}_n$ is just the decremented by 1 number of all distinct divisors of $\small 2n+1$. More generally if $\small d | 2n+1$, $\small k=\frac{d-1}{2}$ is ""$\small p$-divisor"" of $\small n$.",,"['matrices', 'elementary-number-theory', 'polynomials', 'irreducible-polynomials']"
73,Nonsingular matrix problem,Nonsingular matrix problem,,"Let $A$, $B$ be $n\times n(n\ge 2)$ nonsingular matrices with real entries. a)If $A^{-1}+B^{-1}=(A+B)^{-1}$,then prove that $\det A=\det B$ b)Find the examples of matrices $A$,$B$ satisfying $A^{-1}+B^{-1}=(A+B)^{-1}$. c)Find the examples of matrices $A,B$ with complex entries such that $A^{-1}+B^{-1}=(A+B)^{-1}$, but $\det A\ne \det B$ I tried the first part I think it may be done by some multiplication right and left side but i failed.And for example i can't derive any example.Is their any process for thinking this type of problem??","Let $A$, $B$ be $n\times n(n\ge 2)$ nonsingular matrices with real entries. a)If $A^{-1}+B^{-1}=(A+B)^{-1}$,then prove that $\det A=\det B$ b)Find the examples of matrices $A$,$B$ satisfying $A^{-1}+B^{-1}=(A+B)^{-1}$. c)Find the examples of matrices $A,B$ with complex entries such that $A^{-1}+B^{-1}=(A+B)^{-1}$, but $\det A\ne \det B$ I tried the first part I think it may be done by some multiplication right and left side but i failed.And for example i can't derive any example.Is their any process for thinking this type of problem??",,"['matrices', 'determinant']"
74,The Determinant of a Special Vandermonde Matrix,The Determinant of a Special Vandermonde Matrix,,"Consider the following  Vandermonde matrix  $$ V_n = \begin{pmatrix}   1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\   1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\   1 & x_n & x_n^2 & \cdots & x_n^{n-2} & x_n^{n-1} \end{pmatrix}. $$ It is well-known [ 1 ] that the determinant of $V_n$ is defined by  $$ \displaystyle V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({x_j - x_i}\right)\tag{1} $$ Let $V_{n-1}^{(i,j)}$ be a square matrix such that it is obtained by the removing  $i$th row and $j$th column of $V_n$. My question: Is it possible to get a closed-form for the determinant of $V_{n-1}^{(i,j)}$ similar to $(1)$. My try: If $j=n$ then $V_{n-1}^{(i,n)}$ is a Vandermonde matrix and there is a closed-form for its  determinant as $(1)$. Thanks for any suggestions. Edit: I think the general case of the proposed question is as follows; what is the closed-form of the determinant of the next matrix $$ w_n = \begin{pmatrix}   1 & x_1^{i_1} & x_1^{i_2} & \cdots & x_1^{i_{n-2}} & x_1^{i_{n-1}} \\   1 & x_2^{i_1} & x_2^{i_2} & \cdots & x_2^{i_{n-2}} & x_2^{i_{n-1}} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\   1 & x_n^{i_1} & x_n^{i_2} & \cdots & x_n^{i_{n-2}} & x_n^{i_{n-1}} \end{pmatrix}. $$ where $i_t$, for $1\leq t \leq n-1$, are positive integer numbers such that  $i_1<i_2<\cdots<i_{n-2}<i_{n-1}$.","Consider the following  Vandermonde matrix  $$ V_n = \begin{pmatrix}   1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\   1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\   1 & x_n & x_n^2 & \cdots & x_n^{n-2} & x_n^{n-1} \end{pmatrix}. $$ It is well-known [ 1 ] that the determinant of $V_n$ is defined by  $$ \displaystyle V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({x_j - x_i}\right)\tag{1} $$ Let $V_{n-1}^{(i,j)}$ be a square matrix such that it is obtained by the removing  $i$th row and $j$th column of $V_n$. My question: Is it possible to get a closed-form for the determinant of $V_{n-1}^{(i,j)}$ similar to $(1)$. My try: If $j=n$ then $V_{n-1}^{(i,n)}$ is a Vandermonde matrix and there is a closed-form for its  determinant as $(1)$. Thanks for any suggestions. Edit: I think the general case of the proposed question is as follows; what is the closed-form of the determinant of the next matrix $$ w_n = \begin{pmatrix}   1 & x_1^{i_1} & x_1^{i_2} & \cdots & x_1^{i_{n-2}} & x_1^{i_{n-1}} \\   1 & x_2^{i_1} & x_2^{i_2} & \cdots & x_2^{i_{n-2}} & x_2^{i_{n-1}} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\   1 & x_n^{i_1} & x_n^{i_2} & \cdots & x_n^{i_{n-2}} & x_n^{i_{n-1}} \end{pmatrix}. $$ where $i_t$, for $1\leq t \leq n-1$, are positive integer numbers such that  $i_1<i_2<\cdots<i_{n-2}<i_{n-1}$.",,"['linear-algebra', 'matrices', 'determinant']"
75,Determining all real numbers $a$ for which the limit $\lim\limits_{n \to \infty} a^nA^n$ exists and is non-zero for a $3 \times 3$ matrix $A$.,Determining all real numbers  for which the limit  exists and is non-zero for a  matrix .,a \lim\limits_{n \to \infty} a^nA^n 3 \times 3 A,"Q. Let $A$ be a $3 \times 3$ matrix $$A=     \left(\begin{matrix}     1 & -1 & 0 \\     -1 & 2 &-1 \\     0 & -1 & 1 \\     \end{matrix}\right) $$ Determine all real numbers $a$ for which the limit  $\lim\limits_{n \to \infty} a^nA^n$ exists and is non-zero.[For a sequence of $3 \times 3$ matrices $\{B_n\}$ and a $3 \times 3$ matrix $B$, $\lim\limits_{n \to \infty} B_n = B$ means that, for all vectors $x \in \Bbb R^3$, we have $\lim\limits_{n \to \infty} B_nx=Bx$ in $\Bbb R^3$.] My approach : I found eigenvalues of $A$ to be $0,1,3$ with eigenvectors $(1,1,1)$,$(1,0,-1)$ and $(1,-2,1)$ respectively. Let $x \in \Bbb R^3$, then $x=b(1,1,1)+c(1,0,-1)+d(1,-2,1)$ for some unique scalars $b,c,d$ since the eigenvectors form a basis of $\Bbb R^3$. Thus, $\lim\limits_{n \to \infty} a^n A^n x=\lim\limits_{n \to \infty} a^n A^n[b(1,1,1)+c(1,0,-1)+d(1,-2,1)]=\lim\limits_{n \to \infty} [a^n\;b\; A^n(1,1,1)+a^n\;c\;A^n(1,0,-1)+a^n\;d\;A^n(1,-2,1)]=\lim\limits_{n \to \infty} [a^n\;b\; 0^n(1,1,1)+a^n\;c\;1^n(1,0,-1)+a^n\;d\;3^n(1,-2,1)]=(0,0,0)+c \lim\limits_{n \to \infty} a^n(1,0,-1)+d\lim\limits_{n \to \infty} (3a)^n(1,-2,1)$ which exists. Since $\lim\limits_{n \to \infty} a^n A^n$ exists, so from second term of the last expression, $|a| \le 1$ and from third term of the last expression $|3a| \ \le 1 \Rightarrow |a| \le \frac 13$. This implies that $|a| \le \frac 13$. But if $|a| \lt \frac 13$, then $\lim\limits_{n \to \infty} a^n A^nx=(0,0,0) \; \forall \; x \in \Bbb R^3.$ $\therefore a=\frac 13$. Is this approach correct? Particularly I am concerned with the basis I used in the beginning. Since everything in the question is in the standard basis, did I do right by writing $x$ in the eigenbasis? Another approach could be finding modal matrix $P$ and using $A^n=PD^nP^{-1}$ where $D$ is the diagonal matrix $     \left(\begin{matrix}     0 & 0 & 0 \\     0 & 1 & 0 \\     0 & 0 & 3 \\     \end{matrix}\right) $ which also yields the same answer $a=\frac 13$.","Q. Let $A$ be a $3 \times 3$ matrix $$A=     \left(\begin{matrix}     1 & -1 & 0 \\     -1 & 2 &-1 \\     0 & -1 & 1 \\     \end{matrix}\right) $$ Determine all real numbers $a$ for which the limit  $\lim\limits_{n \to \infty} a^nA^n$ exists and is non-zero.[For a sequence of $3 \times 3$ matrices $\{B_n\}$ and a $3 \times 3$ matrix $B$, $\lim\limits_{n \to \infty} B_n = B$ means that, for all vectors $x \in \Bbb R^3$, we have $\lim\limits_{n \to \infty} B_nx=Bx$ in $\Bbb R^3$.] My approach : I found eigenvalues of $A$ to be $0,1,3$ with eigenvectors $(1,1,1)$,$(1,0,-1)$ and $(1,-2,1)$ respectively. Let $x \in \Bbb R^3$, then $x=b(1,1,1)+c(1,0,-1)+d(1,-2,1)$ for some unique scalars $b,c,d$ since the eigenvectors form a basis of $\Bbb R^3$. Thus, $\lim\limits_{n \to \infty} a^n A^n x=\lim\limits_{n \to \infty} a^n A^n[b(1,1,1)+c(1,0,-1)+d(1,-2,1)]=\lim\limits_{n \to \infty} [a^n\;b\; A^n(1,1,1)+a^n\;c\;A^n(1,0,-1)+a^n\;d\;A^n(1,-2,1)]=\lim\limits_{n \to \infty} [a^n\;b\; 0^n(1,1,1)+a^n\;c\;1^n(1,0,-1)+a^n\;d\;3^n(1,-2,1)]=(0,0,0)+c \lim\limits_{n \to \infty} a^n(1,0,-1)+d\lim\limits_{n \to \infty} (3a)^n(1,-2,1)$ which exists. Since $\lim\limits_{n \to \infty} a^n A^n$ exists, so from second term of the last expression, $|a| \le 1$ and from third term of the last expression $|3a| \ \le 1 \Rightarrow |a| \le \frac 13$. This implies that $|a| \le \frac 13$. But if $|a| \lt \frac 13$, then $\lim\limits_{n \to \infty} a^n A^nx=(0,0,0) \; \forall \; x \in \Bbb R^3.$ $\therefore a=\frac 13$. Is this approach correct? Particularly I am concerned with the basis I used in the beginning. Since everything in the question is in the standard basis, did I do right by writing $x$ in the eigenbasis? Another approach could be finding modal matrix $P$ and using $A^n=PD^nP^{-1}$ where $D$ is the diagonal matrix $     \left(\begin{matrix}     0 & 0 & 0 \\     0 & 1 & 0 \\     0 & 0 & 3 \\     \end{matrix}\right) $ which also yields the same answer $a=\frac 13$.",,"['linear-algebra', 'matrices', 'limits', 'proof-verification', 'contest-math']"
76,Matrix exponential in the time propagation by the free Dirac equation,Matrix exponential in the time propagation by the free Dirac equation,,"I am trying to evaluate $$\exp(-ic \alpha pt /\hslash - i\beta mc^2t/\hslash),$$ where  $$ \alpha =  \begin{pmatrix} 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0  \end{pmatrix}, \quad \beta = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & -1  \end{pmatrix}. $$ The most basic property of these matrices is, besides $\beta$ being diagonal, is $\alpha^2 = \beta^2 = \mathbb{I}_4$. We also have $\alpha \beta + \beta \alpha = 0$. This matrix exponential arises in the study of time propagation by the 1-D free Dirac equation in relativistic quantum mechanics, which reads $$ i\hslash \partial_t \psi(p,t) = (c\alpha p + \beta mc^2) \psi(p,0),$$ where $\psi$ denotes the spinor in momentum space. This leads to the matrix exponential given above such that  $$\psi(p,t) = \exp(-ic \alpha pt /\hslash - i\beta mc^2t/\hslash) \psi(p,0).$$ In the appendix of a publication [1], the solution is directly given by (with $\hslash = 1$) $$\left( \mathbb{I}_4 cos(Et) - i\frac{c\alpha p + \beta mc^2}{E} sin(Et)\right), \quad E = \sqrt{p^2c^2 + m^2 c^4},$$ which looks fairly simple, and is reminiscent of Euler's formula. However, I was still not able to derive the solution with the few methods I know. For instance, the case $X = A + N$ ,where $A$ is diagonal, $N$ is nilpotent and $AN = NA$ does not apply here because of $\alpha$, which is not nilpotent, and $\alpha$ and $\beta$ do not commute, either. Can you see it at a glance? [1] : see equations B.3 and C.3 on pages 39 and 41 in https://arxiv.org/abs/1107.4650 .","I am trying to evaluate $$\exp(-ic \alpha pt /\hslash - i\beta mc^2t/\hslash),$$ where  $$ \alpha =  \begin{pmatrix} 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0  \end{pmatrix}, \quad \beta = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & -1  \end{pmatrix}. $$ The most basic property of these matrices is, besides $\beta$ being diagonal, is $\alpha^2 = \beta^2 = \mathbb{I}_4$. We also have $\alpha \beta + \beta \alpha = 0$. This matrix exponential arises in the study of time propagation by the 1-D free Dirac equation in relativistic quantum mechanics, which reads $$ i\hslash \partial_t \psi(p,t) = (c\alpha p + \beta mc^2) \psi(p,0),$$ where $\psi$ denotes the spinor in momentum space. This leads to the matrix exponential given above such that  $$\psi(p,t) = \exp(-ic \alpha pt /\hslash - i\beta mc^2t/\hslash) \psi(p,0).$$ In the appendix of a publication [1], the solution is directly given by (with $\hslash = 1$) $$\left( \mathbb{I}_4 cos(Et) - i\frac{c\alpha p + \beta mc^2}{E} sin(Et)\right), \quad E = \sqrt{p^2c^2 + m^2 c^4},$$ which looks fairly simple, and is reminiscent of Euler's formula. However, I was still not able to derive the solution with the few methods I know. For instance, the case $X = A + N$ ,where $A$ is diagonal, $N$ is nilpotent and $AN = NA$ does not apply here because of $\alpha$, which is not nilpotent, and $\alpha$ and $\beta$ do not commute, either. Can you see it at a glance? [1] : see equations B.3 and C.3 on pages 39 and 41 in https://arxiv.org/abs/1107.4650 .",,"['matrices', 'matrix-decomposition', 'matrix-exponential']"
77,Are the eigenvalues of Laplacian times Diagonal times Laplacian matrix non-negative?,Are the eigenvalues of Laplacian times Diagonal times Laplacian matrix non-negative?,,"Consider two (not identical) symmetric Laplacian matrices $L_1$ and $L_2$ and a diagonal matrix $D > 0$. My question is if the eigenvalues of the product $A =L_1 D L_2$ have non-negative real-part. It is well known that the product of two positive semi-definite matrices have eigenvalues with non-negative real part. (see e.g. the book by Bernstein: Matrix Mathematics, Theory, Facts, and Formulas). Extensive simulations show that $A$ has indeed eigenvalues with non-negative real part and that this not true in general if $L_1$ and $L_2$ were `only' positive semi-definite matrices. However, I have been, trying similarity transformations and Sylvester's law of inertia, unable to find a proof. Of course, a counterexample would be very useful as well.","Consider two (not identical) symmetric Laplacian matrices $L_1$ and $L_2$ and a diagonal matrix $D > 0$. My question is if the eigenvalues of the product $A =L_1 D L_2$ have non-negative real-part. It is well known that the product of two positive semi-definite matrices have eigenvalues with non-negative real part. (see e.g. the book by Bernstein: Matrix Mathematics, Theory, Facts, and Formulas). Extensive simulations show that $A$ has indeed eigenvalues with non-negative real part and that this not true in general if $L_1$ and $L_2$ were `only' positive semi-definite matrices. However, I have been, trying similarity transformations and Sylvester's law of inertia, unable to find a proof. Of course, a counterexample would be very useful as well.",,"['linear-algebra', 'matrices', 'spectral-graph-theory', 'graph-laplacian']"
78,On inequalities for norms of matrices,On inequalities for norms of matrices,,"I have a matrix $A \in \mathbb{R}^{n \times n}$ and would like to know about the relationship between the $\| A \|_\infty$ (i.e., the maximum element of the matrix) and the operator-induced norm $\| A \|$. I know that the following upper-bound holds (from Matrix Norm Inequality ): $ \| A \|_\infty \leq \sqrt{n} \| A \| $? But, I am trying to find a lower-bound? (Would the lower-bound possibly be comprised of the minimum singular value times some factor of $n$?) Also, I need this lower bound to have a norm that has the sub-multiplicative property: given square matrices $A,B \Rightarrow \| A B \|_{\infty} \geq \| A \|_p \| B \|_p $ But, is there an appropriate norm/$p$ that suits this?","I have a matrix $A \in \mathbb{R}^{n \times n}$ and would like to know about the relationship between the $\| A \|_\infty$ (i.e., the maximum element of the matrix) and the operator-induced norm $\| A \|$. I know that the following upper-bound holds (from Matrix Norm Inequality ): $ \| A \|_\infty \leq \sqrt{n} \| A \| $? But, I am trying to find a lower-bound? (Would the lower-bound possibly be comprised of the minimum singular value times some factor of $n$?) Also, I need this lower bound to have a norm that has the sub-multiplicative property: given square matrices $A,B \Rightarrow \| A B \|_{\infty} \geq \| A \|_p \| B \|_p $ But, is there an appropriate norm/$p$ that suits this?",,"['matrices', 'normed-spaces']"
79,"Any known bound on difference between Frobinus norm and 2-norm, especially for a positive definite matrix?","Any known bound on difference between Frobinus norm and 2-norm, especially for a positive definite matrix?",,"I know the definition of these two norms. Given a matrix ${\bf A} \in \Bbb R^{n\times n}$ or $\Bbb C^{n\times n}$, Frobinus norm $\|{\bf A}\|_F$ is simply the vector 2-norm applied on the vectorized $\bf A$, and $\|{\bf A}\|_2=\max_{{\bf x}:\|{\bf x}\|_2=1}\|{\bf Ax}\|_2$ can be interpreted as the maximum ""stretching"" power of linear operator $\bf A$ on the unit circle w.r.t. vector 2-norm, and it can be shown that $\|{\bf A}\|_2=\sqrt {\max (\sigma({\bf A}^*{\bf A}))}$ where ${\bf A}^*$ is the conjugate transpose of ${\bf A}$, and $\sigma (\cdot)$ denotes the set of all eigenvalues of the matrix. I am curious that if there is any known bound on their difference $|\|{\bf A}\|_F-\|{\bf A}\|_2|$. Especially, if $\bf A$ is positive definite, $\|{\bf A}\|_2$ is the largest eigenvalue of $\bf A$, then in what situation we can use $\|{\bf A}\|_F$ to estimate $\|{\bf A}\|_2$, i.e. the largest eigenvalue?","I know the definition of these two norms. Given a matrix ${\bf A} \in \Bbb R^{n\times n}$ or $\Bbb C^{n\times n}$, Frobinus norm $\|{\bf A}\|_F$ is simply the vector 2-norm applied on the vectorized $\bf A$, and $\|{\bf A}\|_2=\max_{{\bf x}:\|{\bf x}\|_2=1}\|{\bf Ax}\|_2$ can be interpreted as the maximum ""stretching"" power of linear operator $\bf A$ on the unit circle w.r.t. vector 2-norm, and it can be shown that $\|{\bf A}\|_2=\sqrt {\max (\sigma({\bf A}^*{\bf A}))}$ where ${\bf A}^*$ is the conjugate transpose of ${\bf A}$, and $\sigma (\cdot)$ denotes the set of all eigenvalues of the matrix. I am curious that if there is any known bound on their difference $|\|{\bf A}\|_F-\|{\bf A}\|_2|$. Especially, if $\bf A$ is positive definite, $\|{\bf A}\|_2$ is the largest eigenvalue of $\bf A$, then in what situation we can use $\|{\bf A}\|_F$ to estimate $\|{\bf A}\|_2$, i.e. the largest eigenvalue?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
80,"$A,B \in M(n,\mathbb C)$ with $AB=BA$ , then does there exist $P \in M(n,\mathbb C)$ and $f(X) , g(X) \in \mathbb C[X]$ such that $A=f(P), B=g(P)$? [duplicate]","with  , then does there exist  and  such that ? [duplicate]","A,B \in M(n,\mathbb C) AB=BA P \in M(n,\mathbb C) f(X) , g(X) \in \mathbb C[X] A=f(P), B=g(P)","This question already has answers here : Can commuting matrices $X,Y$ always be written as polynomials of some matrix $A$? (3 answers) Closed 6 years ago . Let $A,B \in M(n,\mathbb C)$ ; if $\exists P \in M(n, \mathbb C)$ and $f(X) , g(X) \in \mathbb C[X]$ such that $A=f(P) , B=g(P)$ then $AB=BA$ . Now suppose $AB=BA$ ; then does there exist $P \in M(n,\mathbb C)$ and $f(X) , g(X) \in \mathbb C[X]$ such that $A=f(P)$ and $ B=g(P)$ ? If this is not true in general , then what if we assume that both $A,B$ are diagonalizable ? Is it true then ?","This question already has answers here : Can commuting matrices $X,Y$ always be written as polynomials of some matrix $A$? (3 answers) Closed 6 years ago . Let $A,B \in M(n,\mathbb C)$ ; if $\exists P \in M(n, \mathbb C)$ and $f(X) , g(X) \in \mathbb C[X]$ such that $A=f(P) , B=g(P)$ then $AB=BA$ . Now suppose $AB=BA$ ; then does there exist $P \in M(n,\mathbb C)$ and $f(X) , g(X) \in \mathbb C[X]$ such that $A=f(P)$ and $ B=g(P)$ ? If this is not true in general , then what if we assume that both $A,B$ are diagonalizable ? Is it true then ?",,"['linear-algebra', 'matrices']"
81,Eigenvalues and eigenvectors of a tridiagonal block Toeplitz matrix,Eigenvalues and eigenvectors of a tridiagonal block Toeplitz matrix,,"Let $T$ be the $2 N \times 2 N$ matrix defined by $$ T =  \begin{pmatrix} A && B\\ -B^* && -A^* \end{pmatrix} $$ where $*$ is entry wise complex conjugation, $A$ is a Hermitian $N \times N$ tridiagonal Toeplitz matrix $$ A = \begin{pmatrix} 	a & \alpha& 0 & \dots & 0\\ 	\alpha^* & a & \alpha & \vdots & \vdots\\ 	0 & \alpha^* & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & a & \alpha\\ 	0 & \dots & \dots & \alpha^* & a 	\end{pmatrix} $$ with $a$ real and $\alpha$ in general complex and $B$ is the symmetric tridiagonal Toeplitz matrix $$ B = \begin{pmatrix} 	b & \beta& 0 & \dots & 0\\ 	\beta & b & \beta & \vdots & \vdots\\ 	0 & \beta & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & b & \beta\\ 	0 & \dots & \dots & \beta & b 	\end{pmatrix} $$ where $b$ is real and $\beta$ is complex. I want to find the eigenvalues of eigenvectors of $T$. Here's what I have so far. If $\alpha$ is real, then the solution is quite simple since $A$ is symmetric. Thus, $T$ can be written as $$ T = i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes A $$ where $\sigma_i$ are the usual Pauli matrices. It is well known that symmetric tridiagonal Toeplitz matrices all the same eigenvectors and their eigenvalues are particularly simple. We can simultaneously diagonalize $Re(B), Im(B),$ and $A$ and rewrite $T$ as a sum of $2 \times 2$ block matrices $$ T = \sum_{n =1}^N  \begin{pmatrix} a+ 2 \alpha \cos(\frac{\pi n}{(N+1)}) && b+\beta \cos(\frac{\pi n}{(N+1)})\\ -(b+\beta^* \cos(\frac{\pi n}{(N+1)})) && -(a+ 2 \alpha \cos(\frac{\pi n}{(N+1)})) \end{pmatrix} $$ And finding the eigenvalues and eigenvectors becomes diagonalizing a $2 \times 2$ matrix. Now if $\alpha$ is complex, we encounter a problem. $T$ can now be written as $$ T = Id \otimes Im(A) +i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes Re(A) $$ by hermicity of $A$, $Im(A)$ is an antisymmetric tridiagonal Toeplitz matrix which doesn't commute with $Re(A), Re(B), Im(B)$ (but it almost commutes in that the only non zero elements of the commutator are at the top left and bottom right of the matrix). So I tried something else. In a different basis (well really just swapping the tensor product) we get  $$ T = Im(A) \otimes Id + Im(B) \otimes i \sigma_x + Re(B) \otimes i\sigma_y+ Re(A) \otimes \sigma_z $$ Which we can write as a block tridiagonal Toeplitz matrix $$ T  = \begin{pmatrix} 	C & D& 0 & \dots & 0\\ 	E & C & D & \vdots & \vdots\\ 	0 & E & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & C & D\\ 	0 & \dots & \dots & E & C 	\end{pmatrix} $$ with $C, D, E$ the $2 \times 2$ matrices. $$ C =  \begin{pmatrix} a && b\\ -b && -a \end{pmatrix} \: \: \: D = \begin{pmatrix} \alpha^* && \beta\\ -\beta^* && -\alpha \end{pmatrix} \: \: \: E = \begin{pmatrix} \alpha && \beta\\ -\beta^* && -\alpha^* \end{pmatrix} $$ And so the eigenvalue problem becomes a three term difference equation  $$ E\begin{pmatrix}x_{j-1} \\ y_{j-1}\end{pmatrix} + C\begin{pmatrix}x_{j} \\ y_{j}\end{pmatrix} + D\begin{pmatrix}x_{j+1} \\ y_{j+1}\end{pmatrix} = \lambda \begin{pmatrix} x_{j} \\ y_{j} \end{pmatrix} $$ with boundary conditions $x_0 = x_{N+1} = y_0 = y_{N+1}$. Now, like in the regular tridiagonal Topeplitz matrix case, I make an ansatz of $x_j = x r^j$ and $y_j = y r^j$ the difference equation becomes \begin{equation} (rE+(C-\lambda)+r^{-1}D ) \begin{pmatrix} x \\y \end{pmatrix} = 0 \end{equation} For a non trivial solution to our equation we need the determinant to vanish, which is a polynomial of degree four in $r$ and we assume for the moment that there are four distinct roots $r_k$, $k = {1,2,3,4}$. Thus the general solution is of the form $$ \begin{pmatrix} x_j \\ y_j  \end{pmatrix} = \sum_{k = 1}^4 c_k \lambda^j_k  \begin{pmatrix} x_k \\ y_k \end{pmatrix} $$ where $(x_k,y_k)$ is in the kernel of the difference equation above. We need to find the $c_k$ which satisfy the boundary conditions which equivalent to finding a non trivial solution to $$ \begin{pmatrix} x_1 & x_2 & x_3 & x_4 \\ y_1 & y_2 & y_3 & y_4 \\ \lambda_1^{N+1} x_1 & \lambda_2^{N+1} x_2 & \lambda_3^{N+1} x_3 & \lambda_4^{N+1} x_4 \\ \lambda_1^{N+1} y_1 & \lambda_2^{N+1} y_2 & \lambda_3^{N+1} y_3 & \lambda_4^{N+1} y_4 \end{pmatrix} \begin{pmatrix} c_1\\ c_2\\ c_3\\ c_4 \end{pmatrix} = \begin{pmatrix} 0\\ 0\\ 0\\ 0 \end{pmatrix} $$ For a nontrivial solution, we need the determinant to vanish, which imposes a conditions on the roots $\lambda_k$. It is at this point that I'm stuck and not sure what to do, because the determinant involves the eigenvectors $(x_k,y_k)$ which is involved and confusing. Any help would be greatly appreciated!","Let $T$ be the $2 N \times 2 N$ matrix defined by $$ T =  \begin{pmatrix} A && B\\ -B^* && -A^* \end{pmatrix} $$ where $*$ is entry wise complex conjugation, $A$ is a Hermitian $N \times N$ tridiagonal Toeplitz matrix $$ A = \begin{pmatrix} 	a & \alpha& 0 & \dots & 0\\ 	\alpha^* & a & \alpha & \vdots & \vdots\\ 	0 & \alpha^* & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & a & \alpha\\ 	0 & \dots & \dots & \alpha^* & a 	\end{pmatrix} $$ with $a$ real and $\alpha$ in general complex and $B$ is the symmetric tridiagonal Toeplitz matrix $$ B = \begin{pmatrix} 	b & \beta& 0 & \dots & 0\\ 	\beta & b & \beta & \vdots & \vdots\\ 	0 & \beta & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & b & \beta\\ 	0 & \dots & \dots & \beta & b 	\end{pmatrix} $$ where $b$ is real and $\beta$ is complex. I want to find the eigenvalues of eigenvectors of $T$. Here's what I have so far. If $\alpha$ is real, then the solution is quite simple since $A$ is symmetric. Thus, $T$ can be written as $$ T = i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes A $$ where $\sigma_i$ are the usual Pauli matrices. It is well known that symmetric tridiagonal Toeplitz matrices all the same eigenvectors and their eigenvalues are particularly simple. We can simultaneously diagonalize $Re(B), Im(B),$ and $A$ and rewrite $T$ as a sum of $2 \times 2$ block matrices $$ T = \sum_{n =1}^N  \begin{pmatrix} a+ 2 \alpha \cos(\frac{\pi n}{(N+1)}) && b+\beta \cos(\frac{\pi n}{(N+1)})\\ -(b+\beta^* \cos(\frac{\pi n}{(N+1)})) && -(a+ 2 \alpha \cos(\frac{\pi n}{(N+1)})) \end{pmatrix} $$ And finding the eigenvalues and eigenvectors becomes diagonalizing a $2 \times 2$ matrix. Now if $\alpha$ is complex, we encounter a problem. $T$ can now be written as $$ T = Id \otimes Im(A) +i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes Re(A) $$ by hermicity of $A$, $Im(A)$ is an antisymmetric tridiagonal Toeplitz matrix which doesn't commute with $Re(A), Re(B), Im(B)$ (but it almost commutes in that the only non zero elements of the commutator are at the top left and bottom right of the matrix). So I tried something else. In a different basis (well really just swapping the tensor product) we get  $$ T = Im(A) \otimes Id + Im(B) \otimes i \sigma_x + Re(B) \otimes i\sigma_y+ Re(A) \otimes \sigma_z $$ Which we can write as a block tridiagonal Toeplitz matrix $$ T  = \begin{pmatrix} 	C & D& 0 & \dots & 0\\ 	E & C & D & \vdots & \vdots\\ 	0 & E & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & C & D\\ 	0 & \dots & \dots & E & C 	\end{pmatrix} $$ with $C, D, E$ the $2 \times 2$ matrices. $$ C =  \begin{pmatrix} a && b\\ -b && -a \end{pmatrix} \: \: \: D = \begin{pmatrix} \alpha^* && \beta\\ -\beta^* && -\alpha \end{pmatrix} \: \: \: E = \begin{pmatrix} \alpha && \beta\\ -\beta^* && -\alpha^* \end{pmatrix} $$ And so the eigenvalue problem becomes a three term difference equation  $$ E\begin{pmatrix}x_{j-1} \\ y_{j-1}\end{pmatrix} + C\begin{pmatrix}x_{j} \\ y_{j}\end{pmatrix} + D\begin{pmatrix}x_{j+1} \\ y_{j+1}\end{pmatrix} = \lambda \begin{pmatrix} x_{j} \\ y_{j} \end{pmatrix} $$ with boundary conditions $x_0 = x_{N+1} = y_0 = y_{N+1}$. Now, like in the regular tridiagonal Topeplitz matrix case, I make an ansatz of $x_j = x r^j$ and $y_j = y r^j$ the difference equation becomes \begin{equation} (rE+(C-\lambda)+r^{-1}D ) \begin{pmatrix} x \\y \end{pmatrix} = 0 \end{equation} For a non trivial solution to our equation we need the determinant to vanish, which is a polynomial of degree four in $r$ and we assume for the moment that there are four distinct roots $r_k$, $k = {1,2,3,4}$. Thus the general solution is of the form $$ \begin{pmatrix} x_j \\ y_j  \end{pmatrix} = \sum_{k = 1}^4 c_k \lambda^j_k  \begin{pmatrix} x_k \\ y_k \end{pmatrix} $$ where $(x_k,y_k)$ is in the kernel of the difference equation above. We need to find the $c_k$ which satisfy the boundary conditions which equivalent to finding a non trivial solution to $$ \begin{pmatrix} x_1 & x_2 & x_3 & x_4 \\ y_1 & y_2 & y_3 & y_4 \\ \lambda_1^{N+1} x_1 & \lambda_2^{N+1} x_2 & \lambda_3^{N+1} x_3 & \lambda_4^{N+1} x_4 \\ \lambda_1^{N+1} y_1 & \lambda_2^{N+1} y_2 & \lambda_3^{N+1} y_3 & \lambda_4^{N+1} y_4 \end{pmatrix} \begin{pmatrix} c_1\\ c_2\\ c_3\\ c_4 \end{pmatrix} = \begin{pmatrix} 0\\ 0\\ 0\\ 0 \end{pmatrix} $$ For a nontrivial solution, we need the determinant to vanish, which imposes a conditions on the roots $\lambda_k$. It is at this point that I'm stuck and not sure what to do, because the determinant involves the eigenvectors $(x_k,y_k)$ which is involved and confusing. Any help would be greatly appreciated!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'tridiagonal-matrices']"
82,Eigenvalues of a matrix with binomial entries,Eigenvalues of a matrix with binomial entries,,"I am trying to determine the eigenvalues of the following matrix: $$M_{ij} = 4^{-j}\binom{2j}{i}$$ where it is understood that the binomial coefficient $\binom{m}{k}$ is zero if $k<0$ or $k>m$. Here $i,j$ go from $0$ to $N$, therefore the matrix is $(N+1)\times(N+1)$. If an exact expression is not available, I would content myself with approximations valid for large $N$. Moreover, I am mostly interested in the largest positive eigenvalue and its corresponding eigenvector.","I am trying to determine the eigenvalues of the following matrix: $$M_{ij} = 4^{-j}\binom{2j}{i}$$ where it is understood that the binomial coefficient $\binom{m}{k}$ is zero if $k<0$ or $k>m$. Here $i,j$ go from $0$ to $N$, therefore the matrix is $(N+1)\times(N+1)$. If an exact expression is not available, I would content myself with approximations valid for large $N$. Moreover, I am mostly interested in the largest positive eigenvalue and its corresponding eigenvector.",,"['matrices', 'eigenvalues-eigenvectors', 'binomial-coefficients']"
83,Which properties do elementary transformations on a matrices not preserve?,Which properties do elementary transformations on a matrices not preserve?,,"In the book of Linear Algebra by Werner Greub at page 96, it is given that; The elementary transformations on a matrix are: (1.1.) Interchange of two vectors $X_i$ and $X_j(i\not =j).$ (1.2.) Interchange of two vectors $Y_k$ and $Y_l(k\not =).$ (11.1.) Adding to a vector $X_i$ an arbitrary multiple of a vector xj(j\not =i). (11.2.) Adding to a vector $Y_k$ an arbitrary multiple of a vector $Y_l (l \not = k)$ . It is easy to see that the four above transformations have the following effect on the matrix $M (\phi)$ : (1.1.) Interchange of the rows i and j. (1.2.) Interchange of the columns k and I. (11.1.) Replacement of the row-vector $a_i$ by $a_i+$ $\lambda a_j(j\not =i).$ (11.2.) Replacement of the column-vector $b_k$ by $b_k+\lambda b_l(l\not=k).$ I have no problem in understanding the operations, but what I'm asking is that what do those operations preserve, especially, in the view that they are matrix representations of linear mappings of vector spaces ? For example, though I haven't studied in details, in Gaussian elimination, basically we are applying these elementary transformations to the coefficient matrix of a system of equations, and in the book, it says that after those operations, the given system is equivalent to the old one. I don't know how because I don't know what the transformations do and do not preserve when we applied them to a matrix. Another example, in 11.2 when we add $\lambda b_l$ to the kth row, $\phi$ maps the kth bases to $\phi_{old} (x_k) + \lambda \phi (x_l)$ , so the image has change, hence the map has changed, at least according to me. Edit: For those who stumbled with these elementary transformation as I was, please see this question for insight.","In the book of Linear Algebra by Werner Greub at page 96, it is given that; The elementary transformations on a matrix are: (1.1.) Interchange of two vectors and (1.2.) Interchange of two vectors and (11.1.) Adding to a vector an arbitrary multiple of a vector xj(j\not =i). (11.2.) Adding to a vector an arbitrary multiple of a vector . It is easy to see that the four above transformations have the following effect on the matrix : (1.1.) Interchange of the rows i and j. (1.2.) Interchange of the columns k and I. (11.1.) Replacement of the row-vector by (11.2.) Replacement of the column-vector by I have no problem in understanding the operations, but what I'm asking is that what do those operations preserve, especially, in the view that they are matrix representations of linear mappings of vector spaces ? For example, though I haven't studied in details, in Gaussian elimination, basically we are applying these elementary transformations to the coefficient matrix of a system of equations, and in the book, it says that after those operations, the given system is equivalent to the old one. I don't know how because I don't know what the transformations do and do not preserve when we applied them to a matrix. Another example, in 11.2 when we add to the kth row, maps the kth bases to , so the image has change, hence the map has changed, at least according to me. Edit: For those who stumbled with these elementary transformation as I was, please see this question for insight.",X_i X_j(i\not =j). Y_k Y_l(k\not =). X_i Y_k Y_l (l \not = k) M (\phi) a_i a_i+ \lambda a_j(j\not =i). b_k b_k+\lambda b_l(l\not=k). \lambda b_l \phi \phi_{old} (x_k) + \lambda \phi (x_l),"['linear-algebra', 'matrices', 'linear-transformations', 'transformation', 'gaussian-elimination']"
84,Relation between minimal polynomials of $AA^T$ and $A^TA$,Relation between minimal polynomials of  and,AA^T A^TA,"Let $A$ be a real $m\times n$ matrix with $m<n$, of full row rank. Let $f$ be the minimal polynomial of $AA^T$, and $g$ be the minimal polynomial of $A^TA$. Prove that $g=xf$. What I know is that $A, A^T, AA^T, A^TA$ all have the same rank, so $AA^T$ must be invertible and $A^TA$ is not invertible, thus having eigenvalue 0, which means that $g$ must have a factor $x$. However, I don't know how to proceed further. Does anyone have idea?","Let $A$ be a real $m\times n$ matrix with $m<n$, of full row rank. Let $f$ be the minimal polynomial of $AA^T$, and $g$ be the minimal polynomial of $A^TA$. Prove that $g=xf$. What I know is that $A, A^T, AA^T, A^TA$ all have the same rank, so $AA^T$ must be invertible and $A^TA$ is not invertible, thus having eigenvalue 0, which means that $g$ must have a factor $x$. However, I don't know how to proceed further. Does anyone have idea?",,"['linear-algebra', 'matrices', 'minimal-polynomials']"
85,Easy way to find the eigenvalues of this matrix,Easy way to find the eigenvalues of this matrix,,"I have a $n \times n$ matrix A that we can write as $A = I + x(B - I) = (1-x)I + xB$, where $I$ is the identity matrix and $B$ is a $n \times n$ matrix with $b_{ij} =1$ for $0 \leq i,j \leq n$. Furthermore $x \in [-1,1]$. The eigenvalues of $A$ are given by $(1-x)$ with multiplicity n-1 and $(1-x) +nx$ with multiplicity 1. I checked this for $n=3$, by writing the determinant out. My question is, is there a more convenient way to see that the eigenvalues are indeed given by $(1-x)$ with multiplicity n-1 and $(1-x) +nx$ with multiplicity 1 for general $n$?","I have a $n \times n$ matrix A that we can write as $A = I + x(B - I) = (1-x)I + xB$, where $I$ is the identity matrix and $B$ is a $n \times n$ matrix with $b_{ij} =1$ for $0 \leq i,j \leq n$. Furthermore $x \in [-1,1]$. The eigenvalues of $A$ are given by $(1-x)$ with multiplicity n-1 and $(1-x) +nx$ with multiplicity 1. I checked this for $n=3$, by writing the determinant out. My question is, is there a more convenient way to see that the eigenvalues are indeed given by $(1-x)$ with multiplicity n-1 and $(1-x) +nx$ with multiplicity 1 for general $n$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
86,On the derivative of the matrix exponential,On the derivative of the matrix exponential,,"I have the following set up, for column vectors $\alpha,\beta$ and square matrix $A$ I am looking at the derivative $\frac{\partial\alpha^T e^{Ax}\beta}{\partial x}=\alpha^T Ae^{Ax}\beta=\alpha^T e^{Ax}A\beta$, where $e^{Ax}$ represents the matrix exponential. Can I write this in terms of the original function? So, $$\frac{\partial\alpha^T e^{Ax}\beta}{\partial x}=\alpha^T Ae^{Ax}\beta=X\alpha^T e^{Ax}\beta$$ for some $X$? Under what circumstances is this possible? Thank you!","I have the following set up, for column vectors $\alpha,\beta$ and square matrix $A$ I am looking at the derivative $\frac{\partial\alpha^T e^{Ax}\beta}{\partial x}=\alpha^T Ae^{Ax}\beta=\alpha^T e^{Ax}A\beta$, where $e^{Ax}$ represents the matrix exponential. Can I write this in terms of the original function? So, $$\frac{\partial\alpha^T e^{Ax}\beta}{\partial x}=\alpha^T Ae^{Ax}\beta=X\alpha^T e^{Ax}\beta$$ for some $X$? Under what circumstances is this possible? Thank you!",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'matrix-exponential']"
87,Is the pseudoinverse matrix the solution to the least squares problem?,Is the pseudoinverse matrix the solution to the least squares problem?,,"I'm trying to verify that, given a matrix M, the pseudo-inverse $$M^{+}=(M^TM)^{-1}M^T$$ is the solution for the least squares.. but something went wrong and I can't undestand why... $$e=\frac{1}{2}||y-Mx||=\frac{1}{2}(y-Mx)^T(y-Mx)\\ =\frac{1}{2}(y^Ty-y^TMx-x^TM^Ty+x^TM^TMx)=\\ =\frac{1}{2}(y^Ty-2y^TMx+x^TM^TMx) $$ so $$\frac{de}{dx}=\frac{1}{2}(-2y^TM+x^TM^TM)=0\\ x^TM^TM=2y^TM\\M^TMx=2M^Ty\\x=2(M^TM)^{-1}M^Ty$$ why can't I cancel the factor '2'?","I'm trying to verify that, given a matrix M, the pseudo-inverse $$M^{+}=(M^TM)^{-1}M^T$$ is the solution for the least squares.. but something went wrong and I can't undestand why... $$e=\frac{1}{2}||y-Mx||=\frac{1}{2}(y-Mx)^T(y-Mx)\\ =\frac{1}{2}(y^Ty-y^TMx-x^TM^Ty+x^TM^TMx)=\\ =\frac{1}{2}(y^Ty-2y^TMx+x^TM^TMx) $$ so $$\frac{de}{dx}=\frac{1}{2}(-2y^TM+x^TM^TM)=0\\ x^TM^TM=2y^TM\\M^TMx=2M^Ty\\x=2(M^TM)^{-1}M^Ty$$ why can't I cancel the factor '2'?",,"['linear-algebra', 'matrices', 'inverse', 'least-squares', 'pseudoinverse']"
88,Matrix algebra linear transformation question,Matrix algebra linear transformation question,,"Let $A = 4 \times 4$ matrix:  $\begin{bmatrix}  3 & 2 &  10 &  -6 \\   1  & 0  &  2 & -4 \\   0 &  1  & 2  & 3 \\   1  &  4 &  10 &  8   \end{bmatrix}$, let $b = 4 \times 1$  matrix:  $\begin{bmatrix}  -1  \\  3  \\  -1  \\  4  \\ \end{bmatrix}$ Is $b$ in the range of linear transformation $x \rightarrow Ax$? Why so or why not? I'm not really sure what the question is asking. Any help would be greatly appreciated. Sorry I don't know how to properly format the mathematical equations on this website yet, I did my best to make it legible.","Let $A = 4 \times 4$ matrix:  $\begin{bmatrix}  3 & 2 &  10 &  -6 \\   1  & 0  &  2 & -4 \\   0 &  1  & 2  & 3 \\   1  &  4 &  10 &  8   \end{bmatrix}$, let $b = 4 \times 1$  matrix:  $\begin{bmatrix}  -1  \\  3  \\  -1  \\  4  \\ \end{bmatrix}$ Is $b$ in the range of linear transformation $x \rightarrow Ax$? Why so or why not? I'm not really sure what the question is asking. Any help would be greatly appreciated. Sorry I don't know how to properly format the mathematical equations on this website yet, I did my best to make it legible.",,['linear-algebra']
89,How to find size of each Jordan Block,How to find size of each Jordan Block,,"Let $A$ be a $7\times 7$ matrix satisfying $2A^2-A^4=I$ .If $A$ has two distinct eigenvalues and each eigenvalue has geometric multipicity $3$,then find the number of non-zero entries in the Jordan Canonical Form of $A$. Since $2A^2-A^4=I$ so it is a annihilating polynomial of $A$. The minimal polynomial of $A$ must divide $2x^2-x^4-1=0\implies (x^2-1)^2=0$. Since it has two distinct eigenvalues so that will be $-1,1$. But how to find the size of the Jordan Block corresponding to eigen values $1,-1$. Its quite clear that the minimal polynomial will be $(x-1)^m(x+1)^n;m,n>1$ since the matrix is not diagonalizable. Please give some hints on how to solve the problem.","Let $A$ be a $7\times 7$ matrix satisfying $2A^2-A^4=I$ .If $A$ has two distinct eigenvalues and each eigenvalue has geometric multipicity $3$,then find the number of non-zero entries in the Jordan Canonical Form of $A$. Since $2A^2-A^4=I$ so it is a annihilating polynomial of $A$. The minimal polynomial of $A$ must divide $2x^2-x^4-1=0\implies (x^2-1)^2=0$. Since it has two distinct eigenvalues so that will be $-1,1$. But how to find the size of the Jordan Block corresponding to eigen values $1,-1$. Its quite clear that the minimal polynomial will be $(x-1)^m(x+1)^n;m,n>1$ since the matrix is not diagonalizable. Please give some hints on how to solve the problem.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
90,Wedge product of matrix-valued differential forms,Wedge product of matrix-valued differential forms,,"Below you can find the original question, but here is the main underlying calculation that seems bizarrely impossible to find written down anywhere: $$ \begin{pmatrix}   a&b\\c&d \end{pmatrix} \wedge \begin{pmatrix}   e&f\\g&h \end{pmatrix} \overset{?}{=} ah-fc $$ The bounty on this question is really for this one calculation, and the original question below is just for some (historical) context. Let $X$ be a complex $n$ -manifold with a rank- $r$ vector bundle $E$ and open cover $\{U_\alpha\}$ such that $E|_{U_\alpha}\cong\mathbb{C}^r$ . Say we have two $r\times r$ -matrices $P,Q$ of differential $1$ -forms, i.e. $P\in\Omega^1_{U_\alpha}\otimes\operatorname{End}(E|_{U_\alpha})$ ; $Q\in\Omega^1_{U_\beta}\otimes\operatorname{End}(E|_{U_\beta})$ . Question 1: What is the wedge product $P\wedge Q$ ? I believe it should just be matrix multiplication, but using the wedge product component-wise, i.e. $$(P\wedge Q)_{ij}=\sum_{k}(P_{ik}\wedge Q_{kj})$$ but (having read the article on vector-valued differential forms on the infallible Wikipedia) we should have that $$P\wedge Q\in\Omega^2_{U_{\alpha\beta}}\otimes\operatorname{End}(E|_{U_\alpha})\otimes \operatorname{End}(E|_{U_\beta}).$$ Then, using the fact that the $E|_{U_\alpha}$ are finite-dimensional vector spaces, we see that $P\wedge Q$ should be an $r^2\times r^2$ -matrix of differential $2$ -forms (since $\operatorname{End}(V)\otimes\operatorname{End}(W)\cong\operatorname{End}(V\otimes W)$ for f.d. vector spaces). In this case, the wedge product should be given by something like the Kronecker product of $P$ and $Q$ . Question 2: Now let $M$ be an $r\times r$ matrix of holomorphic functions (i.e. $0$ -forms). What can we say about $P\wedge MQ$ ? It seems clear that we should always have $(MP)\wedge Q=M(P\wedge Q)$ ; $P\wedge MQ=PM\wedge Q$ ; $P\wedge Q=-(Q^t\wedge P^t)^t$ (or something similar, depending on the answer to Question 1). However, using these two 'facts' it doesn't seem possible to 'pull out the $M$ ' from an expression of the form $P\wedge MQ$ . Is there a way of doing so? Matrix non-commutativity gets in the way here, but are there restrictions we can place on $M$ , $P$ , or $Q$ to get some nice result?","Below you can find the original question, but here is the main underlying calculation that seems bizarrely impossible to find written down anywhere: The bounty on this question is really for this one calculation, and the original question below is just for some (historical) context. Let be a complex -manifold with a rank- vector bundle and open cover such that . Say we have two -matrices of differential -forms, i.e. ; . Question 1: What is the wedge product ? I believe it should just be matrix multiplication, but using the wedge product component-wise, i.e. but (having read the article on vector-valued differential forms on the infallible Wikipedia) we should have that Then, using the fact that the are finite-dimensional vector spaces, we see that should be an -matrix of differential -forms (since for f.d. vector spaces). In this case, the wedge product should be given by something like the Kronecker product of and . Question 2: Now let be an matrix of holomorphic functions (i.e. -forms). What can we say about ? It seems clear that we should always have ; ; (or something similar, depending on the answer to Question 1). However, using these two 'facts' it doesn't seem possible to 'pull out the ' from an expression of the form . Is there a way of doing so? Matrix non-commutativity gets in the way here, but are there restrictions we can place on , , or to get some nice result?","
\begin{pmatrix}
  a&b\\c&d
\end{pmatrix}
\wedge
\begin{pmatrix}
  e&f\\g&h
\end{pmatrix}
\overset{?}{=}
ah-fc
 X n r E \{U_\alpha\} E|_{U_\alpha}\cong\mathbb{C}^r r\times r P,Q 1 P\in\Omega^1_{U_\alpha}\otimes\operatorname{End}(E|_{U_\alpha}) Q\in\Omega^1_{U_\beta}\otimes\operatorname{End}(E|_{U_\beta}) P\wedge Q (P\wedge Q)_{ij}=\sum_{k}(P_{ik}\wedge Q_{kj}) P\wedge Q\in\Omega^2_{U_{\alpha\beta}}\otimes\operatorname{End}(E|_{U_\alpha})\otimes \operatorname{End}(E|_{U_\beta}). E|_{U_\alpha} P\wedge Q r^2\times r^2 2 \operatorname{End}(V)\otimes\operatorname{End}(W)\cong\operatorname{End}(V\otimes W) P Q M r\times r 0 P\wedge MQ (MP)\wedge Q=M(P\wedge Q) P\wedge MQ=PM\wedge Q P\wedge Q=-(Q^t\wedge P^t)^t M P\wedge MQ M P Q","['matrices', 'differential-geometry', 'differential-forms']"
91,The power of matrix $A$,The power of matrix,A,"Let $A = \begin{pmatrix}0 & a\\ b&c\\ \end{pmatrix}$ with $a, b, c \in \mathbb{R}$. My question is that: ``How can we compute $A^n$ for any $n\in \mathbb{N}$? In fact, one had that $A^2 - cA - abI_2 = 0$ or $A^2 = cA + abI_2$. Then I obtained \begin{eqnarray} A^2 &=& cA + abI_2\\  A^3 &=& (c^2 +ab)A + abcI_2\\ .....&&....................\\ \end{eqnarray}","Let $A = \begin{pmatrix}0 & a\\ b&c\\ \end{pmatrix}$ with $a, b, c \in \mathbb{R}$. My question is that: ``How can we compute $A^n$ for any $n\in \mathbb{N}$? In fact, one had that $A^2 - cA - abI_2 = 0$ or $A^2 = cA + abI_2$. Then I obtained \begin{eqnarray} A^2 &=& cA + abI_2\\  A^3 &=& (c^2 +ab)A + abcI_2\\ .....&&....................\\ \end{eqnarray}",,"['matrices', 'matrix-equations', 'matrix-calculus', 'matrix-decomposition', 'matrix-rank']"
92,Find an integer quartic root of a $3\times 3$ matrix,Find an integer quartic root of a  matrix,3\times 3,"Find a $3 \times 3 $ matrix $X$ with integer coefficients such that   \begin{align*} X^{4} &= 3 \begin{bmatrix} 2 &-1 &-1 \\ -1 &2 &-1 \\ -1 &-1 &2  \end{bmatrix}. \end{align*} My attempt. Let us consider the matrix  \begin{align*} A &= 3 \begin{bmatrix} 2 &-1 &-1 \\ -1 &2 &-1 \\ -1 &-1 &2  \end{bmatrix} \\ &= \begin{bmatrix} 6 &-3 &-3 \\ -3 &6 &-3 \\ -3 &-3 &6  \end{bmatrix} \end{align*} Calculate the roots of characteristic polynomial, i.e calculate the eigenspace $AZ=\lambda Z$, this is given for the equation system $A-\lambda I=0$, where $I$ is $3 \times 3$ identity matrix.  \begin{align*} \begin{vmatrix} 6-\lambda & -3 & -3 \\ -3 & 6-\lambda & -3 \\ -3 & -3 & 6-\lambda \end{vmatrix} &= -\lambda \left( \lambda-9\right)^{2} \end{align*} Therefore, the polynomial function, the zero $\lambda=9$ has multiplicity $2$, and $\lambda=0$ has multiplicity $1$ and these special values are called the eigenvalues of the matrix $A$. We need to know the dimension of the eigenspace generated by this eigenvalue. Thus, solve the system $\left(A-3I\right)Z=0$ where $Z^{T}=\left(x,y,z \right)$ in order to find the eigenvectors.  (1) For $\lambda =0$, then $\left(A-3I\right)Z=0Z$. Thus, $x=y=z=0$. Hence, $v_{1}= \left(1,1,1\right)^{T}$ is an eigenvector corresponding to $\lambda=0$. (2) For $\lambda=9$. Then, we choose $x=0$, $y=1$, then $z=-1$. Hence, $v_{2}= \left(0,1,-1\right)^{T}$. Also, choose $x=1$, $y=-1$, then $z=0$, hence, $v_{3}= \left(1,-1,0\right)^{T}$. Furthermore, $v_{2}$ and $v_{3}$ are eigenvector corresponding to $\lambda=9$. Thus, we have the matrix $S=\left[v_{1} \ v_{2} \ v_{3} \right]$. Then,  \begin{align*} S &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \end{align*} and its inverse  \begin{align*} S^{-1} &=  \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}  \end{align*} Thus, $A=SJS^{-1}$, where  \begin{align*} J &=  \begin{bmatrix} 0 &0 &0 \\ 0 &9 &0 \\ 0 &0 &9  \end{bmatrix}  \end{align*} where $J$ is the Jordan canonical form of $A$. Hence, $\displaystyle X=SJ^{1/4} S^{-1}$ \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &9^{1/4} &0 \\ 0 &0 &9^{1/4}  \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ \end{align*} Now, $9^{1/4}= \sqrt[]{3}, \ - \ \sqrt[]{3}, \ \sqrt[]{3} \ i$, and $\ - \ \sqrt[]{3} \ i$. All these four values can be utilized, for $9^{1/4}$ and accordingly values of $X$ can be changed. All combination can be calculated to find the values of $X$. \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &\sqrt[]{3} &0 \\ 0 &0 &\sqrt[]{3}  \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ &=\begin{bmatrix} 2/\sqrt[]{3} & -1/\sqrt[]{3} & -1/\sqrt[]{3} \\ -1/\sqrt[]{3} & 2/\sqrt[]{3} & -1/\sqrt[]{3} \\ -1/\sqrt[]{3} & -1/\sqrt[]{3} & 2/\sqrt[]{3} \end{bmatrix} \end{align*} \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &- \ \sqrt[]{3} &0 \\ 0 &0 &- \ \sqrt[]{3}  \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ &=\begin{bmatrix} -2/\sqrt[]{3} & 1/\sqrt[]{3} & 1/\sqrt[]{3} \\ 1/\sqrt[]{3} & -2/\sqrt[]{3} & 1/\sqrt[]{3} \\ 1/\sqrt[]{3} & 1/\sqrt[]{3} & -2/\sqrt[]{3} \end{bmatrix} \end{align*} \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &\sqrt[]{3} \ i &0 \\ 0 &0 & \sqrt[]{3} \ i \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ &=\begin{bmatrix} 2i/\sqrt[]{3} & -i/\sqrt[]{3} & -i/\sqrt[]{3} \\ -i/\sqrt[]{3} & 2i/\sqrt[]{3} &-i/\sqrt[]{3} \\ -i/\sqrt[]{3} & -i/\sqrt[]{3} & 2i/\sqrt[]{3} \end{bmatrix} \end{align*} \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &  - \ \sqrt[]{3} \ i &0 \\ 0 &0 &  - \ \sqrt[]{3} \ i \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ &=\begin{bmatrix} -2i/\sqrt[]{3} & i/\sqrt[]{3} & i/\sqrt[]{3} \\ i/\sqrt[]{3} & -2i/\sqrt[]{3} &i/\sqrt[]{3} \\ i/\sqrt[]{3} & i/\sqrt[]{3} & -2i/\sqrt[]{3} \end{bmatrix} \end{align*} However, you can see that non of $X$'s have integer coefficients. Any idea where I have messed up something! Any help would be appreciated!","Find a $3 \times 3 $ matrix $X$ with integer coefficients such that   \begin{align*} X^{4} &= 3 \begin{bmatrix} 2 &-1 &-1 \\ -1 &2 &-1 \\ -1 &-1 &2  \end{bmatrix}. \end{align*} My attempt. Let us consider the matrix  \begin{align*} A &= 3 \begin{bmatrix} 2 &-1 &-1 \\ -1 &2 &-1 \\ -1 &-1 &2  \end{bmatrix} \\ &= \begin{bmatrix} 6 &-3 &-3 \\ -3 &6 &-3 \\ -3 &-3 &6  \end{bmatrix} \end{align*} Calculate the roots of characteristic polynomial, i.e calculate the eigenspace $AZ=\lambda Z$, this is given for the equation system $A-\lambda I=0$, where $I$ is $3 \times 3$ identity matrix.  \begin{align*} \begin{vmatrix} 6-\lambda & -3 & -3 \\ -3 & 6-\lambda & -3 \\ -3 & -3 & 6-\lambda \end{vmatrix} &= -\lambda \left( \lambda-9\right)^{2} \end{align*} Therefore, the polynomial function, the zero $\lambda=9$ has multiplicity $2$, and $\lambda=0$ has multiplicity $1$ and these special values are called the eigenvalues of the matrix $A$. We need to know the dimension of the eigenspace generated by this eigenvalue. Thus, solve the system $\left(A-3I\right)Z=0$ where $Z^{T}=\left(x,y,z \right)$ in order to find the eigenvectors.  (1) For $\lambda =0$, then $\left(A-3I\right)Z=0Z$. Thus, $x=y=z=0$. Hence, $v_{1}= \left(1,1,1\right)^{T}$ is an eigenvector corresponding to $\lambda=0$. (2) For $\lambda=9$. Then, we choose $x=0$, $y=1$, then $z=-1$. Hence, $v_{2}= \left(0,1,-1\right)^{T}$. Also, choose $x=1$, $y=-1$, then $z=0$, hence, $v_{3}= \left(1,-1,0\right)^{T}$. Furthermore, $v_{2}$ and $v_{3}$ are eigenvector corresponding to $\lambda=9$. Thus, we have the matrix $S=\left[v_{1} \ v_{2} \ v_{3} \right]$. Then,  \begin{align*} S &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \end{align*} and its inverse  \begin{align*} S^{-1} &=  \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}  \end{align*} Thus, $A=SJS^{-1}$, where  \begin{align*} J &=  \begin{bmatrix} 0 &0 &0 \\ 0 &9 &0 \\ 0 &0 &9  \end{bmatrix}  \end{align*} where $J$ is the Jordan canonical form of $A$. Hence, $\displaystyle X=SJ^{1/4} S^{-1}$ \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &9^{1/4} &0 \\ 0 &0 &9^{1/4}  \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ \end{align*} Now, $9^{1/4}= \sqrt[]{3}, \ - \ \sqrt[]{3}, \ \sqrt[]{3} \ i$, and $\ - \ \sqrt[]{3} \ i$. All these four values can be utilized, for $9^{1/4}$ and accordingly values of $X$ can be changed. All combination can be calculated to find the values of $X$. \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &\sqrt[]{3} &0 \\ 0 &0 &\sqrt[]{3}  \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ &=\begin{bmatrix} 2/\sqrt[]{3} & -1/\sqrt[]{3} & -1/\sqrt[]{3} \\ -1/\sqrt[]{3} & 2/\sqrt[]{3} & -1/\sqrt[]{3} \\ -1/\sqrt[]{3} & -1/\sqrt[]{3} & 2/\sqrt[]{3} \end{bmatrix} \end{align*} \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &- \ \sqrt[]{3} &0 \\ 0 &0 &- \ \sqrt[]{3}  \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ &=\begin{bmatrix} -2/\sqrt[]{3} & 1/\sqrt[]{3} & 1/\sqrt[]{3} \\ 1/\sqrt[]{3} & -2/\sqrt[]{3} & 1/\sqrt[]{3} \\ 1/\sqrt[]{3} & 1/\sqrt[]{3} & -2/\sqrt[]{3} \end{bmatrix} \end{align*} \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &\sqrt[]{3} \ i &0 \\ 0 &0 & \sqrt[]{3} \ i \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ &=\begin{bmatrix} 2i/\sqrt[]{3} & -i/\sqrt[]{3} & -i/\sqrt[]{3} \\ -i/\sqrt[]{3} & 2i/\sqrt[]{3} &-i/\sqrt[]{3} \\ -i/\sqrt[]{3} & -i/\sqrt[]{3} & 2i/\sqrt[]{3} \end{bmatrix} \end{align*} \begin{align*} X&=SJ^{1/4}S^{-1} \\ A &=  \begin{bmatrix} 1 &0 &1 \\ 1 &1 &-1 \\ 1 &-1 &0  \end{bmatrix}  \begin{bmatrix} 0 &0 &0 \\ 0 &  - \ \sqrt[]{3} \ i &0 \\ 0 &0 &  - \ \sqrt[]{3} \ i \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & -2/3 \\ 2/3 & -1/3 & -1/3  \end{bmatrix}\\ &=\begin{bmatrix} -2i/\sqrt[]{3} & i/\sqrt[]{3} & i/\sqrt[]{3} \\ i/\sqrt[]{3} & -2i/\sqrt[]{3} &i/\sqrt[]{3} \\ i/\sqrt[]{3} & i/\sqrt[]{3} & -2i/\sqrt[]{3} \end{bmatrix} \end{align*} However, you can see that non of $X$'s have integer coefficients. Any idea where I have messed up something! Any help would be appreciated!",,"['linear-algebra', 'matrices', 'canonical-transformation']"
93,Proof concerning eigenvalues,Proof concerning eigenvalues,,"I have the following problem which I don't know how to solve, any help is appreciated. Let $A,B$ and $C$ be $n \times n$ matrices. Suppose that $B$ and $C$ are symmetric. Consider the matrix   $$ M = \begin{bmatrix} A & B \\ C & -A^T \end{bmatrix} $$ Show that if $\lambda$ is an eigenvalue of $M$ then so is $-\lambda$. My idea: I know that for $\lambda$ to be an eigenvalue of $A$, $det(A-λI)$ has to be zero, then you can work out this determinant and get the eigenvalues. However I don't know if you need to solve this matrix like that since its entries aren't numbers but matrices. Besides that, I'm quite sure this isn't the way to tackle this problem since you don't need the values of the eigenvectors but you just have to show that if $\lambda$ is an eigenvalue of M then so is $-\lambda$. Thanks in advance :)","I have the following problem which I don't know how to solve, any help is appreciated. Let $A,B$ and $C$ be $n \times n$ matrices. Suppose that $B$ and $C$ are symmetric. Consider the matrix   $$ M = \begin{bmatrix} A & B \\ C & -A^T \end{bmatrix} $$ Show that if $\lambda$ is an eigenvalue of $M$ then so is $-\lambda$. My idea: I know that for $\lambda$ to be an eigenvalue of $A$, $det(A-λI)$ has to be zero, then you can work out this determinant and get the eigenvalues. However I don't know if you need to solve this matrix like that since its entries aren't numbers but matrices. Besides that, I'm quite sure this isn't the way to tackle this problem since you don't need the values of the eigenvectors but you just have to show that if $\lambda$ is an eigenvalue of M then so is $-\lambda$. Thanks in advance :)",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
94,Scaling a 3d projection matrix to be equal to another projection matrix,Scaling a 3d projection matrix to be equal to another projection matrix,,"This problem has been eating at me for a little while now, and it's extremely frustrating. First off, let me begin with explaining the matrices I am using: Unity calculates culling matrices incorrectly. From the matrix, it seems to generate the actual occlusion culling itself scaled proportionally to the projection matrix, at least when it comes to the near clipping plane. This can typically be fixed by simply calculating a new projection matrix with half the near clip plane distance, but my problem is unfortunately not that simple. I want to calculate an oblique culling matrix (that is, with the near plane not perpendicular to the camera), and this near-clip-doubling is making it impossible to do as no matter what method I use to calculate the matrix, it is proportionally incorrect. I cannot figure out how to counter-compute against the scaling being caused by Unity's incorrect calculation, as I am simply not familiar enough with linear algebra and matrix math to intuitively understand the problem. Here is the code that is being used to calculate the oblique matrix //clipPlane is a vector4 in camera space (direction + distance from camera) Matrix4x4 CalculateObliqueOcclusion(Matrix4x4 projection, Vector4 clipPlane, float signSide) {     //signSide determines whether it is the near or the far clipping plane that moves to the clipPlane     Vector4 q = projection.inverse * new Vector4(          signSide,          signSide,         1.0f,         1.0f     );     Vector4 c = clipPlane * (2F / (Vector4.Dot(clipPlane, q)));     //Left/Right sizing     projection[2] = c.x - projection[3];     //Top/Down sizing     projection[6] = c.y - projection[7];     //Near/Far clipping plane adjustment. Used to move far clipping plane.     projection[10] = (c.z - projection[11]);     //Scale adjustment- used to move near clipping plane     projection[14] = (c.w - projection[15]);     return projection; } Here is a video demonstrating the issue. the clipping plane is being generated correctly off of the object, but as you can see, the culling matrix is wildly out of control. Occlusion matrix being calculated incorrectly In this example, the invisible ""box"" that I am grabbing is being used to calculate the near plane of the projection matrix to be used — this code is working properly. The green box is a rendering of the volume of the occlusion culling matrix, which as you can see appears to scale with the rear plane by a factor of two.","This problem has been eating at me for a little while now, and it's extremely frustrating. First off, let me begin with explaining the matrices I am using: Unity calculates culling matrices incorrectly. From the matrix, it seems to generate the actual occlusion culling itself scaled proportionally to the projection matrix, at least when it comes to the near clipping plane. This can typically be fixed by simply calculating a new projection matrix with half the near clip plane distance, but my problem is unfortunately not that simple. I want to calculate an oblique culling matrix (that is, with the near plane not perpendicular to the camera), and this near-clip-doubling is making it impossible to do as no matter what method I use to calculate the matrix, it is proportionally incorrect. I cannot figure out how to counter-compute against the scaling being caused by Unity's incorrect calculation, as I am simply not familiar enough with linear algebra and matrix math to intuitively understand the problem. Here is the code that is being used to calculate the oblique matrix //clipPlane is a vector4 in camera space (direction + distance from camera) Matrix4x4 CalculateObliqueOcclusion(Matrix4x4 projection, Vector4 clipPlane, float signSide) {     //signSide determines whether it is the near or the far clipping plane that moves to the clipPlane     Vector4 q = projection.inverse * new Vector4(          signSide,          signSide,         1.0f,         1.0f     );     Vector4 c = clipPlane * (2F / (Vector4.Dot(clipPlane, q)));     //Left/Right sizing     projection[2] = c.x - projection[3];     //Top/Down sizing     projection[6] = c.y - projection[7];     //Near/Far clipping plane adjustment. Used to move far clipping plane.     projection[10] = (c.z - projection[11]);     //Scale adjustment- used to move near clipping plane     projection[14] = (c.w - projection[15]);     return projection; } Here is a video demonstrating the issue. the clipping plane is being generated correctly off of the object, but as you can see, the culling matrix is wildly out of control. Occlusion matrix being calculated incorrectly In this example, the invisible ""box"" that I am grabbing is being used to calculate the near plane of the projection matrix to be used — this code is working properly. The green box is a rendering of the volume of the occlusion culling matrix, which as you can see appears to scale with the rear plane by a factor of two.",,"['linear-algebra', 'matrices', '3d', 'matrix-equations', 'projection-matrices']"
95,An explicit realization of the similarity of the transpose of a matrix in function field.,An explicit realization of the similarity of the transpose of a matrix in function field.,,"Let $K=\mathbb{F}(a,b,c,d)$ be the field of rational functions in four variables over a field $\mathbb{F}$. The matrix $$ A=\left( {\begin{array}{cc}    a & b \\       c & d \      \end{array} } \right)$$ over $K$ is conjugate to its transpose. Hence there exists an invertible matrix $P$ over $K$ such that $A^t=PAP^{-1}$. What is an explicit formula for $P$, in terms of $a,b,c,d$ ? Can we choose $P$ to have polynomial entries in $a,b,c,d$ ?","Let $K=\mathbb{F}(a,b,c,d)$ be the field of rational functions in four variables over a field $\mathbb{F}$. The matrix $$ A=\left( {\begin{array}{cc}    a & b \\       c & d \      \end{array} } \right)$$ over $K$ is conjugate to its transpose. Hence there exists an invertible matrix $P$ over $K$ such that $A^t=PAP^{-1}$. What is an explicit formula for $P$, in terms of $a,b,c,d$ ? Can we choose $P$ to have polynomial entries in $a,b,c,d$ ?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
96,Real matrices such that $A^2=-I_n$,Real matrices such that,A^2=-I_n,"Find all $n\times n$ matrices with real entries such that $A^2=-I_n$. If $A$ is such a matrix, since $(\det A)^2 = (-1)^n$, $n$ must be even. Furthermore, $A$ annihilates $X^2+1 = (X-i)(X+i)$, so $A$ is diagonalizable over $\mathbb C$, with eigenvalues in $\{-i,i\}$. Let us write $A=PDP^{-1}$ where $D$ is a diagonal matrix with entries in $\{-i,i\}$ and $P$ is complex and non-singular. Since the trace of $A$ is real, there must be the same number of $i$ and $-i$ on $D$'s diagonal. Although interesting, this doesn't give a very explicit description of $A$. I'm not even sure any product $$P\begin{pmatrix} i\\ &\!\!i\\ &&\ddots\\ &&&-i\\ &&&&-i \end{pmatrix}P^{-1}$$ yields a matrix with real entries, that's why I think something much more specific can be said about $A$.","Find all $n\times n$ matrices with real entries such that $A^2=-I_n$. If $A$ is such a matrix, since $(\det A)^2 = (-1)^n$, $n$ must be even. Furthermore, $A$ annihilates $X^2+1 = (X-i)(X+i)$, so $A$ is diagonalizable over $\mathbb C$, with eigenvalues in $\{-i,i\}$. Let us write $A=PDP^{-1}$ where $D$ is a diagonal matrix with entries in $\{-i,i\}$ and $P$ is complex and non-singular. Since the trace of $A$ is real, there must be the same number of $i$ and $-i$ on $D$'s diagonal. Although interesting, this doesn't give a very explicit description of $A$. I'm not even sure any product $$P\begin{pmatrix} i\\ &\!\!i\\ &&\ddots\\ &&&-i\\ &&&&-i \end{pmatrix}P^{-1}$$ yields a matrix with real entries, that's why I think something much more specific can be said about $A$.",,"['linear-algebra', 'matrices', 'matrix-equations']"
97,Triangle inequality with spectral norm,Triangle inequality with spectral norm,,"Let $A$ be a real square matrix, I have to prove that $$\left \| A \right \|=\sqrt{\lambda_{\text{max}}(A^*A)}$$ defines a norm. I don't know how to prove the triangle inequality. I have already proved that $\|A\|=\|A\|_2=\sup_{\|x\|_2=1} \|Ax\|_2$, but the exercise is to prove without using it.","Let $A$ be a real square matrix, I have to prove that $$\left \| A \right \|=\sqrt{\lambda_{\text{max}}(A^*A)}$$ defines a norm. I don't know how to prove the triangle inequality. I have already proved that $\|A\|=\|A\|_2=\sup_{\|x\|_2=1} \|Ax\|_2$, but the exercise is to prove without using it.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'normed-spaces', 'spectral-norm']"
98,Matrix multiplication using Galois field,Matrix multiplication using Galois field,,"$$\begin{bmatrix}1 &1 &6\\4& 3& 2\\5 &2& 2\\5& 3& 4\\4& 2& 4\end{bmatrix}\begin{bmatrix}4\\5\\6\end{bmatrix} = \begin{bmatrix}3\\5\\4\\3\\2\end{bmatrix}. $$ I am not getting that how come this result is possible ? [Editor's comment #1: The question makes sense, but the asker forgot to explain their notation - possibly because they have not been exposed to any alternatives (happens regrettably often when programmers and/or telecommunication majors are introduced to finite fields). Below please find an elaboration of my educated guess, JL.] Here the notation is using a common way of writing polynomials with binary coefficients as integers. We are working over the field $GF(2^3)$ aka $\Bbb{F}_8$ defined as $\Bbb{F}_2[\alpha]$, where $\alpha$ is a zero of an irreducible cubic. We then compactly represent an arbitrary element  $$ z=a_0+a_1\alpha+a_2\alpha^2\in GF(8), \ a_0,a_1,a_2\in GF(2), $$ as the sequence of bits $$ z=a_2a_1a_0 $$ that is then (internally to the computer program) stored as the integer $$ i(z)=(a_2a_1a_0)_2 $$ in base two. For example, the element $\alpha^2+\alpha$ is converted to $6$, because $$ \alpha^2+\alpha=1\cdot\alpha^2+1\cdot\alpha^1+0\cdot\alpha^0=110_2=6. $$ [Editor's comment #2: A popular choice is that $\alpha$ is a zero  of the polynomial $x^3+x+1$. In other words, we have the equation $$ \alpha^3+\alpha+1=0. $$ Unless I made a mistake, the first component of the matrix product above matches with this minimal polynomial in the sense that the calculation $$ 1\cdot4+1\cdot5+6\cdot6=1\cdot\alpha^2+1\cdot(1+\alpha^2)+(\alpha+\alpha^2)^2=\alpha+1=3,$$ is true when $\alpha^3+\alpha+1=0$, JL.]","$$\begin{bmatrix}1 &1 &6\\4& 3& 2\\5 &2& 2\\5& 3& 4\\4& 2& 4\end{bmatrix}\begin{bmatrix}4\\5\\6\end{bmatrix} = \begin{bmatrix}3\\5\\4\\3\\2\end{bmatrix}. $$ I am not getting that how come this result is possible ? [Editor's comment #1: The question makes sense, but the asker forgot to explain their notation - possibly because they have not been exposed to any alternatives (happens regrettably often when programmers and/or telecommunication majors are introduced to finite fields). Below please find an elaboration of my educated guess, JL.] Here the notation is using a common way of writing polynomials with binary coefficients as integers. We are working over the field $GF(2^3)$ aka $\Bbb{F}_8$ defined as $\Bbb{F}_2[\alpha]$, where $\alpha$ is a zero of an irreducible cubic. We then compactly represent an arbitrary element  $$ z=a_0+a_1\alpha+a_2\alpha^2\in GF(8), \ a_0,a_1,a_2\in GF(2), $$ as the sequence of bits $$ z=a_2a_1a_0 $$ that is then (internally to the computer program) stored as the integer $$ i(z)=(a_2a_1a_0)_2 $$ in base two. For example, the element $\alpha^2+\alpha$ is converted to $6$, because $$ \alpha^2+\alpha=1\cdot\alpha^2+1\cdot\alpha^1+0\cdot\alpha^0=110_2=6. $$ [Editor's comment #2: A popular choice is that $\alpha$ is a zero  of the polynomial $x^3+x+1$. In other words, we have the equation $$ \alpha^3+\alpha+1=0. $$ Unless I made a mistake, the first component of the matrix product above matches with this minimal polynomial in the sense that the calculation $$ 1\cdot4+1\cdot5+6\cdot6=1\cdot\alpha^2+1\cdot(1+\alpha^2)+(\alpha+\alpha^2)^2=\alpha+1=3,$$ is true when $\alpha^3+\alpha+1=0$, JL.]",,"['linear-algebra', 'matrices', 'finite-fields', 'matlab']"
99,Eigenvalues for symmetric and skew-symmetric part of a matrix,Eigenvalues for symmetric and skew-symmetric part of a matrix,,"Every real matrix $A$ can be decomposed into symmetric and skew-symmetric part. Symmetric matrix has only real numbers as its eigenvalues (including $0$) and skew-symmetric matrix has only imaginary values (also including $0$). Could we infer from separate calculations of eigenvalues for symmetric and skew-symmetric about eigenvalues for matrix $A$ ? If so then can the same be said about eigenvectors calculated separately for these two parts of matrix and their relevance for eigenvectors of the whole matrix $A$? Edit (after 1 day) If the answer for the questions above is too difficult to obtain in a general case maybe it would be possible to answer for a particular case: why in the case of orthogonal matrices $R$ we can write down: $EGV(sym(R))+EGV(sk(R))= EGV(R)$ where $EGV$ is here a vector obtained from respectively ordered values of eigenvalues for symmetric part of $R$, skew-sym. part of $R$ and full matrix $R$. Example: for 3-D rotation matrix we have ( it's hard to believe that it is just a coincidence) $EGV(sym(R)) =\begin{bmatrix} \cos(\theta)\\ \cos(\theta)\\  1 \end{bmatrix}  ,  EGV(sk(R)) \begin{bmatrix} i\sin(\theta)\\ -i\sin(\theta)\\ 0 \end{bmatrix},  EGV(R)=\begin{bmatrix} \cos(\theta)+i\sin(\theta)\\ \cos(\theta)-i\sin(\theta)\\  1 \end{bmatrix}$ Of course it would be better if it were proved starting from general properties of orthogonal matrix without calculating exact values of eigenvalues as I showed in the example above.","Every real matrix $A$ can be decomposed into symmetric and skew-symmetric part. Symmetric matrix has only real numbers as its eigenvalues (including $0$) and skew-symmetric matrix has only imaginary values (also including $0$). Could we infer from separate calculations of eigenvalues for symmetric and skew-symmetric about eigenvalues for matrix $A$ ? If so then can the same be said about eigenvectors calculated separately for these two parts of matrix and their relevance for eigenvectors of the whole matrix $A$? Edit (after 1 day) If the answer for the questions above is too difficult to obtain in a general case maybe it would be possible to answer for a particular case: why in the case of orthogonal matrices $R$ we can write down: $EGV(sym(R))+EGV(sk(R))= EGV(R)$ where $EGV$ is here a vector obtained from respectively ordered values of eigenvalues for symmetric part of $R$, skew-sym. part of $R$ and full matrix $R$. Example: for 3-D rotation matrix we have ( it's hard to believe that it is just a coincidence) $EGV(sym(R)) =\begin{bmatrix} \cos(\theta)\\ \cos(\theta)\\  1 \end{bmatrix}  ,  EGV(sk(R)) \begin{bmatrix} i\sin(\theta)\\ -i\sin(\theta)\\ 0 \end{bmatrix},  EGV(R)=\begin{bmatrix} \cos(\theta)+i\sin(\theta)\\ \cos(\theta)-i\sin(\theta)\\  1 \end{bmatrix}$ Of course it would be better if it were proved starting from general properties of orthogonal matrix without calculating exact values of eigenvalues as I showed in the example above.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'rotations']"
