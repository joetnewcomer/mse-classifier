,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,The projection of a point onto a convex set is unique with respect to any norm,The projection of a point onto a convex set is unique with respect to any norm,,"Given a convex set $C$, the projection of a point $z$ onto $C$ is a point $x$ in $C$ that minimizes $\|z- x\|$. Say the minimum is achieved at $x^*\in C$. My textbook shows such $x^*$ is unique under the euclidean norm, as shown below. My guess is the uniqueness should hold regardless what norm is chosen, but I have trouble proving it. Thanks!","Given a convex set $C$, the projection of a point $z$ onto $C$ is a point $x$ in $C$ that minimizes $\|z- x\|$. Say the minimum is achieved at $x^*\in C$. My textbook shows such $x^*$ is unique under the euclidean norm, as shown below. My guess is the uniqueness should hold regardless what norm is chosen, but I have trouble proving it. Thanks!",,"['linear-algebra', 'optimization']"
1,common eigenvectors of commuting operators,common eigenvectors of commuting operators,,"I am pretty sure that my problem has already discussed, but I didn't find. So, the question is how to prove that two commuting operators have a common eigenvector. The first note is following: Let $A$ be the operator such that $Av = \lambda v$ (we are considering algebraically closed case, so such $v$ surely exists). Then $ABv = BAv = \lambda Bv$, so $Bv$ is eigenvector of $A$ with eigenvalue $\lambda$ as well. But I am not sure, what to do next.","I am pretty sure that my problem has already discussed, but I didn't find. So, the question is how to prove that two commuting operators have a common eigenvector. The first note is following: Let $A$ be the operator such that $Av = \lambda v$ (we are considering algebraically closed case, so such $v$ surely exists). Then $ABv = BAv = \lambda Bv$, so $Bv$ is eigenvector of $A$ with eigenvalue $\lambda$ as well. But I am not sure, what to do next.",,['linear-algebra']
2,Prove that the product of two positive linear operators is positive if and only if they commute.,Prove that the product of two positive linear operators is positive if and only if they commute.,,"Having problem in the following problems on positive forms: $1)$ Prove that the product of two positive linear operators is positive if and  only if they commute. I am able to do one direction that if the product of two positive linear operators is positive then they commute. But unable to do the opposite direction. Let $T,S$ be two positive linear operators and they commute , i.e. $ST = TS$ . To show the product of two positive linear operators is positive we have to show that $\langle TS\alpha,\alpha\rangle > 0$ for any $\alpha \neq0$ and $(TS)^* = TS$ . I have shown the part $(TS)^* = TS$ . I need help to show that $\langle TS\alpha,\alpha\rangle > 0$ for any $\alpha \neq0$ . $2)$ Let $V$ be a finite-dimensional inner product space and $Ε$ the orthogonal  projection of $V$ onto some subspace. $(a)$ Prove that, for any positive number $c$ , the operator $cI + Ε$ is positive. $(b)$ Express in terms of $Ε$ a self-adjoint linear operator $Τ$ such that $T^2 = I + E$ . In this I am able to do part $(a)$ but unable to the second part. Can anyone give me any lead to the problems?","Having problem in the following problems on positive forms: Prove that the product of two positive linear operators is positive if and  only if they commute. I am able to do one direction that if the product of two positive linear operators is positive then they commute. But unable to do the opposite direction. Let be two positive linear operators and they commute , i.e. . To show the product of two positive linear operators is positive we have to show that for any and . I have shown the part . I need help to show that for any . Let be a finite-dimensional inner product space and the orthogonal  projection of onto some subspace. Prove that, for any positive number , the operator is positive. Express in terms of a self-adjoint linear operator such that . In this I am able to do part but unable to the second part. Can anyone give me any lead to the problems?","1) T,S ST = TS \langle TS\alpha,\alpha\rangle > 0 \alpha \neq0 (TS)^* = TS (TS)^* = TS \langle TS\alpha,\alpha\rangle > 0 \alpha \neq0 2) V Ε V (a) c cI + Ε (b) Ε Τ T^2 = I + E (a)","['linear-algebra', 'operator-theory', 'inner-products', 'positive-definite']"
3,Prove the set of matrices with one Jordan block is not dense in $M_n(\mathbb{C}).$,Prove the set of matrices with one Jordan block is not dense in,M_n(\mathbb{C}).,"I would like to show that the set of matrices with one Jordan block is not dense in $M_n(\mathbb{C}),$ the set of all $3$ x $3$ matrices with complex entries. I have done proofs showing that invertible matrices are dense, and diagonal matrices are dense, but I've been struggling with proofs showing that a given subset is not dense.  Another one I've had trouble with was showing the $SL(2,\mathbb{C})$ is not dense too. The only reasonable approach seems to begin with assuming that the subset is dense, and reaching a contradiction.  Intuitively, one should be able to state that $I$ is not the limit point of a sequence of Jordan blocks, but I have had trouble stating this rigorously.  Any tips/suggestions/tricks?","I would like to show that the set of matrices with one Jordan block is not dense in $M_n(\mathbb{C}),$ the set of all $3$ x $3$ matrices with complex entries. I have done proofs showing that invertible matrices are dense, and diagonal matrices are dense, but I've been struggling with proofs showing that a given subset is not dense.  Another one I've had trouble with was showing the $SL(2,\mathbb{C})$ is not dense too. The only reasonable approach seems to begin with assuming that the subset is dense, and reaching a contradiction.  Intuitively, one should be able to state that $I$ is not the limit point of a sequence of Jordan blocks, but I have had trouble stating this rigorously.  Any tips/suggestions/tricks?",,"['linear-algebra', 'general-topology']"
4,Double dual space is isomorphic to vector space - Intuition,Double dual space is isomorphic to vector space - Intuition,,"The recent topics I studied were linear functionals and dual spaces. I like to think about a linear functional as a stack of hyperplanes like it is described here . In ""Finite dimensional vector spaces"" by Paul Halmos I read that every vector space is isomorphic to its double dual. I wonder if there is an intuitive way to see that they are isomorphic? Also I am not sure how I could graphically or geometrically think about the double dual space (in the sense I think about the dual space as a stack of hyperplanes in every direction). Is there a way to visualize the double dual space? Maybe then the isomorphism would become clearer. I guess there is some intuition behind it since someone had to think about it first before he or she invented the concept (double dual space). I hope my question makes sense? Thanks for any responses!","The recent topics I studied were linear functionals and dual spaces. I like to think about a linear functional as a stack of hyperplanes like it is described here . In ""Finite dimensional vector spaces"" by Paul Halmos I read that every vector space is isomorphic to its double dual. I wonder if there is an intuitive way to see that they are isomorphic? Also I am not sure how I could graphically or geometrically think about the double dual space (in the sense I think about the dual space as a stack of hyperplanes in every direction). Is there a way to visualize the double dual space? Maybe then the isomorphism would become clearer. I guess there is some intuition behind it since someone had to think about it first before he or she invented the concept (double dual space). I hope my question makes sense? Thanks for any responses!",,"['linear-algebra', 'intuition', 'duality-theorems', 'dual-spaces']"
5,Intuitive understanding of the matrix of a linear transformation,Intuitive understanding of the matrix of a linear transformation,,Is it accurate to say that a matrix $M(T)$ of the linear map $T:V\to W$ encodes the linear map into a series of numbers by showing how the linear map applied to the basis vectors of $V$ can be expressed as basis vectors of $W$? Is this a healthy way to imagine and intuit the matrix of a linear map?,Is it accurate to say that a matrix $M(T)$ of the linear map $T:V\to W$ encodes the linear map into a series of numbers by showing how the linear map applied to the basis vectors of $V$ can be expressed as basis vectors of $W$? Is this a healthy way to imagine and intuit the matrix of a linear map?,,"['linear-algebra', 'matrices', 'linear-transformations']"
6,Solution(s) to dot product of vectors,Solution(s) to dot product of vectors,,"I have some questions about the uniqueness of matrices when post- and pre-multiplied with vectors (inner product). Say we have two vectors $\vec{a}$ and $\vec{b}$, whose inner product is a scalar, known to satisfy the following equation involving matrix $\left[C\right]$: $$ \vec{a} \cdot \vec{b} = \vec{a}^T \vec{b} = \vec{a}^T \left[C\right] \vec{b} $$ In this case, is $\left[C\right]$ guaranteed to be the identity matrix? Can it be anything else? Why? Along the same lines, is it ever possible to ""eliminate"" vectors from an equation? For example, if we also have a matrix $\left[D\right]$ that satisfies the equation: $$ \left[C\right] \vec{b} = \left[D\right] \vec{b} $$ Could we just post-multiply each side by $\vec{b}^{-1}$ to obtain $\left[C\right]$ = $\left[D\right]$? Is this valid under any set of conditions? Thanks","I have some questions about the uniqueness of matrices when post- and pre-multiplied with vectors (inner product). Say we have two vectors $\vec{a}$ and $\vec{b}$, whose inner product is a scalar, known to satisfy the following equation involving matrix $\left[C\right]$: $$ \vec{a} \cdot \vec{b} = \vec{a}^T \vec{b} = \vec{a}^T \left[C\right] \vec{b} $$ In this case, is $\left[C\right]$ guaranteed to be the identity matrix? Can it be anything else? Why? Along the same lines, is it ever possible to ""eliminate"" vectors from an equation? For example, if we also have a matrix $\left[D\right]$ that satisfies the equation: $$ \left[C\right] \vec{b} = \left[D\right] \vec{b} $$ Could we just post-multiply each side by $\vec{b}^{-1}$ to obtain $\left[C\right]$ = $\left[D\right]$? Is this valid under any set of conditions? Thanks",,['linear-algebra']
7,Does there exist a multiplicative $f:\mathbb{Q}^+\to\mathbb{Q}^+$ such that $f\neq x\mapsto x^a$ for all $a$?,Does there exist a multiplicative  such that  for all ?,f:\mathbb{Q}^+\to\mathbb{Q}^+ f\neq x\mapsto x^a a,"If we consider the functional equation: $f:\mathbb{Q}^+\to\mathbb{R}$ such that $$ f(xy)=f(x)f(y) $$ for all $x,y\in\mathbb{Q}^+$ I think, I have constructed a solution which is not of the form $x\mapsto x^a$. Namely, if we consider $V=\mathbb{R}$ as $\mathbb{Q}$-vector space, we can find a base $B$ of $V$ such that $\log(2),\log(3)\in B$. If we set $f(x)=\exp(\lambda(\log(2),\log(x)))$ where $\lambda(b,x)$ for $b\in B$ and $x\in \mathbb{R}$ denotes the coefficient of $b$ in the base $B$ representation of $x$. First of all, is this construction correct? Now, what if we impose the codomain to be $\mathbb{Q}^+$, i.e. $f:\mathbb{Q}^+\to\mathbb{Q}^+$? Can we still find non-ordinary solutions? Intuitively, I would say yes, but the above construction fails in this case. How to proceed?","If we consider the functional equation: $f:\mathbb{Q}^+\to\mathbb{R}$ such that $$ f(xy)=f(x)f(y) $$ for all $x,y\in\mathbb{Q}^+$ I think, I have constructed a solution which is not of the form $x\mapsto x^a$. Namely, if we consider $V=\mathbb{R}$ as $\mathbb{Q}$-vector space, we can find a base $B$ of $V$ such that $\log(2),\log(3)\in B$. If we set $f(x)=\exp(\lambda(\log(2),\log(x)))$ where $\lambda(b,x)$ for $b\in B$ and $x\in \mathbb{R}$ denotes the coefficient of $b$ in the base $B$ representation of $x$. First of all, is this construction correct? Now, what if we impose the codomain to be $\mathbb{Q}^+$, i.e. $f:\mathbb{Q}^+\to\mathbb{Q}^+$? Can we still find non-ordinary solutions? Intuitively, I would say yes, but the above construction fails in this case. How to proceed?",,"['linear-algebra', 'proof-verification', 'functional-equations', 'group-homomorphism']"
8,Prove that the determinant of an invertible matrix $A$ is equal to $±1$ when all of the entries of $A$ and $A^{−1}$ are integers.,Prove that the determinant of an invertible matrix  is equal to  when all of the entries of  and  are integers.,A ±1 A A^{−1},"Prove that the determinant of an invertible matrix $A$ is equal to $±1$ when all of the entries of $A$ and $A^{−1}$ are integers. I can explain the answer but would like help translating it into a concise proof with equations. My explanation: The fact that $\det(A) = ±1$ implies that when we perform Gaussian elimination on $A$, we never have to multiply rows by scalars. This means that for each column, the pivot entry is created by the previous column’s row operations and can be brought into place by swapping rows. (And the first column must already contain a $1$). Therefore, we never need to multiply by a non-integral value to perform Gaussian elimination.","Prove that the determinant of an invertible matrix $A$ is equal to $±1$ when all of the entries of $A$ and $A^{−1}$ are integers. I can explain the answer but would like help translating it into a concise proof with equations. My explanation: The fact that $\det(A) = ±1$ implies that when we perform Gaussian elimination on $A$, we never have to multiply rows by scalars. This means that for each column, the pivot entry is created by the previous column’s row operations and can be brought into place by swapping rows. (And the first column must already contain a $1$). Therefore, we never need to multiply by a non-integral value to perform Gaussian elimination.",,"['linear-algebra', 'proof-verification', 'determinant', 'alternative-proof']"
9,the vector space of Magic Squares,the vector space of Magic Squares,,"Can anyone offer help? I have no clue how to do this problem. Magical squares are 3 by 3 matrices with the following properties: the sum of all numbers in each row, and in each column, and in each diagonal is equal. This number is called the magical number. (i)Prove that the set of magical squares forms a vector space with the usual matrix addition and scalar-matrix product. (ii) Find a basis of the vector space of magical squares and determine its dimension.","Can anyone offer help? I have no clue how to do this problem. Magical squares are 3 by 3 matrices with the following properties: the sum of all numbers in each row, and in each column, and in each diagonal is equal. This number is called the magical number. (i)Prove that the set of magical squares forms a vector space with the usual matrix addition and scalar-matrix product. (ii) Find a basis of the vector space of magical squares and determine its dimension.",,['linear-algebra']
10,"Cyclic Modules, Characteristic Polynomial and Minimal Polynomial","Cyclic Modules, Characteristic Polynomial and Minimal Polynomial",,"Suppose that $\mathrm{dim}_{F}M<\infty$ for $F$ a field and $M$ an $F$ vector space. Let $T$ be a linear transformation on $M$. Show that $M$ is cyclic (as an $F[x]$ module) if and only if $m(x)$ is the characteristic polynomial of $T$, for $m(x)$ being the minimal polynomial of $T$. How would one be able to show this? I'm not sure on how to start with either direction. We know that the torsion of $M$ would just be $M$ (since $m(T)=0$) if we consider $M$ as an $F[x]$ module with $x$ being represented as the action of $T$ (i.e. $p(x) \cdot v=p(T)v$). Would the Cayley-Hamilton theorem help in this case? Thanks for the help.","Suppose that $\mathrm{dim}_{F}M<\infty$ for $F$ a field and $M$ an $F$ vector space. Let $T$ be a linear transformation on $M$. Show that $M$ is cyclic (as an $F[x]$ module) if and only if $m(x)$ is the characteristic polynomial of $T$, for $m(x)$ being the minimal polynomial of $T$. How would one be able to show this? I'm not sure on how to start with either direction. We know that the torsion of $M$ would just be $M$ (since $m(T)=0$) if we consider $M$ as an $F[x]$ module with $x$ being represented as the action of $T$ (i.e. $p(x) \cdot v=p(T)v$). Would the Cayley-Hamilton theorem help in this case? Thanks for the help.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'modules', 'characteristic-polynomial']"
11,Problem of determinant when $A^{-1}+B^{-1}=(A+B)^{-1}$,Problem of determinant when,A^{-1}+B^{-1}=(A+B)^{-1},"I have two $4\times 4$ real matrices $A$ and $B$, and it is known that $A^{-1}+B^{-1}=(A+B)^{-1}$ ($A$, $B$ and $A+B$ are invertible). How can I prove that $\det (A)=\det (B)$?","I have two $4\times 4$ real matrices $A$ and $B$, and it is known that $A^{-1}+B^{-1}=(A+B)^{-1}$ ($A$, $B$ and $A+B$ are invertible). How can I prove that $\det (A)=\det (B)$?",,"['linear-algebra', 'matrices', 'determinant']"
12,Is a sinc-distance matrix positive semidefinite?,Is a sinc-distance matrix positive semidefinite?,,"I've been trying to crack this problem for days but I can't find a way around it. Given a set of unique $N$ points $X = \{x_1,\dots,x_N\}, x_i \in R^3$ , the associated sinc-distance matrix $S \in R^{n\times n}$ is $S(i,j) = \sin(|x_i-x_j|)/(|x_i-x_j|)$ . My question is if this matrix is positive semidefinite. I've ran numerical tests and the matrix always appears PSD, but I haven't been able to prove it formally. Things I've tried: Try to prove it using minors and so: no luck, sinc's are difficult to work with in order to simplify the form of the determinants. Try proving that $\forall y$ , $y^TSy\geq 0$ , that ends up being $\sum_{i=1}^N \sum_{j=1}^N y_i y_j \sin(|x_i-x_j|)/(|x_i-x_j|) \geq 0$ . No luck either. My intuition is that the triangle inequality for distances has to play a role somewhere in it, but can't find it. Try a constructive approach. We know that for the $N=2$ case, the matrix is PD, as the diagonal elements are 1 (sinc of 0) and the non-diagonal are $<1$ , so the matrix is strictly diagonally dominant (also the determinant and trace are both positive, so $S \succ 0$ . Then prove that adding an element to the set doesn't make the resulting matrix indefinite. I've tried using Schur complements for this task but no luck yet. Also for any set $X$ , we can multiply all elements by a scalar $\alpha$ . As $\alpha$ goes to 0, all the elements in the set also go to 0 and $S \to \vec{1} \vec{1}^T$ , which is PSD rank 1. As $\alpha$ goes to infinity, so do the points and so do the distances, so $S \to I$ . So changing $\alpha$ , the matrix moves in a 1D curve in the group $\mathcal{S}$ of symmetric matrices and the curve starts in the boundary of the cone of PSD matrices and ends in the middle of the cone, so it seems natural that it doesn't leave the cone, but I can't prove it. Any help would be greatly appreciated! Looking forward to the discussion! Thanks!","I've been trying to crack this problem for days but I can't find a way around it. Given a set of unique points , the associated sinc-distance matrix is . My question is if this matrix is positive semidefinite. I've ran numerical tests and the matrix always appears PSD, but I haven't been able to prove it formally. Things I've tried: Try to prove it using minors and so: no luck, sinc's are difficult to work with in order to simplify the form of the determinants. Try proving that , , that ends up being . No luck either. My intuition is that the triangle inequality for distances has to play a role somewhere in it, but can't find it. Try a constructive approach. We know that for the case, the matrix is PD, as the diagonal elements are 1 (sinc of 0) and the non-diagonal are , so the matrix is strictly diagonally dominant (also the determinant and trace are both positive, so . Then prove that adding an element to the set doesn't make the resulting matrix indefinite. I've tried using Schur complements for this task but no luck yet. Also for any set , we can multiply all elements by a scalar . As goes to 0, all the elements in the set also go to 0 and , which is PSD rank 1. As goes to infinity, so do the points and so do the distances, so . So changing , the matrix moves in a 1D curve in the group of symmetric matrices and the curve starts in the boundary of the cone of PSD matrices and ends in the middle of the cone, so it seems natural that it doesn't leave the cone, but I can't prove it. Any help would be greatly appreciated! Looking forward to the discussion! Thanks!","N X = \{x_1,\dots,x_N\}, x_i \in R^3 S \in R^{n\times n} S(i,j) = \sin(|x_i-x_j|)/(|x_i-x_j|) \forall y y^TSy\geq 0 \sum_{i=1}^N \sum_{j=1}^N y_i y_j \sin(|x_i-x_j|)/(|x_i-x_j|) \geq 0 N=2 <1 S \succ 0 X \alpha \alpha S \to \vec{1} \vec{1}^T \alpha S \to I \alpha \mathcal{S}","['linear-algebra', 'matrices', 'quadratic-programming', 'positive-semidefinite']"
13,"In common tongue, what is the differences between sparse and dense matrices?","In common tongue, what is the differences between sparse and dense matrices?",,"What are the differences with sparse and dense matrices in practice, so as to offer some insight to new learners on a more intuitive level. Obviously everyone knows about the dictionary definition of sparse and dense matrices (a definition based on the portion of zero/non-zero elements) But why are they so important from a mathematical application/optimization/problem solving point of view? Is it that a lot of neat algorithms are defined such that they can only be operated on a problem if it satisfies such and such criteria, and some guy just proved that sparse | dense matrices tends to satisfy the aforementioned criteria really well Or is it to do with the limited amount of computer memory available in real life, and that we must somehow ""compress"" matrices for faster computation - as such sparse matrices would be more desired Or is it just a fuzzy guideline word that mathematicians use, as opposed to strict criterion fulfilling definitions that imply X properties about the matrices (e.g. make sure the matrix is sparse and not dense because too many elements/variables too long to compute - or something to that nature?) TLDR Question: Is the only major difference as a result of computational limitation and resource savings or are there fundamental mathematical differences between the two that make one uniquely operable and the other not TLDR Answer: So essentially it revolves around our ability to compute something. so there really isn't some ""fundamental"" difference (like the difference between the first derivative or a second derivative of a function). but its just a thing that rose out of technical limitations in real life during computation.","What are the differences with sparse and dense matrices in practice, so as to offer some insight to new learners on a more intuitive level. Obviously everyone knows about the dictionary definition of sparse and dense matrices (a definition based on the portion of zero/non-zero elements) But why are they so important from a mathematical application/optimization/problem solving point of view? Is it that a lot of neat algorithms are defined such that they can only be operated on a problem if it satisfies such and such criteria, and some guy just proved that sparse | dense matrices tends to satisfy the aforementioned criteria really well Or is it to do with the limited amount of computer memory available in real life, and that we must somehow ""compress"" matrices for faster computation - as such sparse matrices would be more desired Or is it just a fuzzy guideline word that mathematicians use, as opposed to strict criterion fulfilling definitions that imply X properties about the matrices (e.g. make sure the matrix is sparse and not dense because too many elements/variables too long to compute - or something to that nature?) TLDR Question: Is the only major difference as a result of computational limitation and resource savings or are there fundamental mathematical differences between the two that make one uniquely operable and the other not TLDR Answer: So essentially it revolves around our ability to compute something. so there really isn't some ""fundamental"" difference (like the difference between the first derivative or a second derivative of a function). but its just a thing that rose out of technical limitations in real life during computation.",,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra', 'sparse-matrices']"
14,If A is invertible and $||B-A|| < ||A^{-1}||^{-1}$ prove $B$ is invertible.,If A is invertible and  prove  is invertible.,||B-A|| < ||A^{-1}||^{-1} B,Just having really hard time trying to proof : If $A$ is invertible and $||B-A|| < ||A^{-1}||^{-1}$ prove $B$ is invertible. It is related to Neumann Series but i don't understand how to proof with math. Thanks for your help and time. Brian Ignacio,Just having really hard time trying to proof : If $A$ is invertible and $||B-A|| < ||A^{-1}||^{-1}$ prove $B$ is invertible. It is related to Neumann Series but i don't understand how to proof with math. Thanks for your help and time. Brian Ignacio,,"['linear-algebra', 'matrices', 'normed-spaces']"
15,The affine special linear group acts doubly transitive,The affine special linear group acts doubly transitive,,"Let $F$ be a field and $d \ge 2$. Denote by $ASL_d(F)$ the affine special linear group , i.e. the group of all transformation on $F^d$ with $t_{A,v}(u) = Au + v$ and $\det A = 1$. I want to show that this group is $2$-transitive (also called doubly transitive) on the points from $F^d$. For this I have to show that for $x,y, x', y'$ with $x \ne y$ and $x' \ne y'$ I can find an element $t_{A,v} \in ASL_d(F)$ such that $$  x' = t_{A,v}(x) = Ax + v \qquad\mbox{and}\qquad  y' = t_{A,v}(y) = Ay + v. $$ Any ideas or hints how to solve this? I have solved it in the case $d = 2$ by a cumbersome solution of the equations involved, with the restriction on the determinant for $d = 2$ with the above I have five equations and six unknows (the elements of the matrix and of the translation vector $v$), so this could be solved. But my solution is quite messy and involves a lot of rearrangemnt and equation handling, so any short solution would be preferred?","Let $F$ be a field and $d \ge 2$. Denote by $ASL_d(F)$ the affine special linear group , i.e. the group of all transformation on $F^d$ with $t_{A,v}(u) = Au + v$ and $\det A = 1$. I want to show that this group is $2$-transitive (also called doubly transitive) on the points from $F^d$. For this I have to show that for $x,y, x', y'$ with $x \ne y$ and $x' \ne y'$ I can find an element $t_{A,v} \in ASL_d(F)$ such that $$  x' = t_{A,v}(x) = Ax + v \qquad\mbox{and}\qquad  y' = t_{A,v}(y) = Ay + v. $$ Any ideas or hints how to solve this? I have solved it in the case $d = 2$ by a cumbersome solution of the equations involved, with the restriction on the determinant for $d = 2$ with the above I have five equations and six unknows (the elements of the matrix and of the translation vector $v$), so this could be solved. But my solution is quite messy and involves a lot of rearrangemnt and equation handling, so any short solution would be preferred?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'linear-transformations', 'group-actions']"
16,eigenvalues of A - aI in terms of eigenvalues of A,eigenvalues of A - aI in terms of eigenvalues of A,,I am stuck with this question of my assignment where given that A is nxn square matrix and a be a scalar it is asked to - Find the eigenvalues of A -  aI in terms of eigenvalues of A. A and A -  aI have same eigenvectors. I am not able to generalise the relation. Kindly help me out!,I am stuck with this question of my assignment where given that A is nxn square matrix and a be a scalar it is asked to - Find the eigenvalues of A -  aI in terms of eigenvalues of A. A and A -  aI have same eigenvectors. I am not able to generalise the relation. Kindly help me out!,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
17,"Prove that if $B$ is similar to $A$, then $B^T$ is similar to $A^T$ .","Prove that if  is similar to , then  is similar to  .",B A B^T A^T,"If two matrix ($A$ and $B$) are similar if there exists an invertible matrix $P$, such that: $$ B=P^{-1} A P $$ I'm thinking if I can prove that $A$, $B$ , $A^T$ and $B^T$ have the same characteristic polynomial then that would prove the above statement. However, I am a little unsure how to put the whole proof together.","If two matrix ($A$ and $B$) are similar if there exists an invertible matrix $P$, such that: $$ B=P^{-1} A P $$ I'm thinking if I can prove that $A$, $B$ , $A^T$ and $B^T$ have the same characteristic polynomial then that would prove the above statement. However, I am a little unsure how to put the whole proof together.",,"['linear-algebra', 'matrices', 'proof-writing']"
18,"If $X + X^T$ is positive definite, is $X^{-1} + X^{-T}$ also positive definite?","If  is positive definite, is  also positive definite?",X + X^T X^{-1} + X^{-T},Is it true or is there a counterexample?,Is it true or is there a counterexample?,,['linear-algebra']
19,An inequality related to the cosine theorem,An inequality related to the cosine theorem,,"Let $A,B,C$ be the three angles in the a triangle (with length $a,b,c$). Can we show that  $$x^2+y^2+z^2\geq 2x y \cos A+2xz\cos B+2yz\cos C?$$ for all $x,y,z\in\Bbb R$. I do not see whether it is true, but I find it in some book. I suspect it is false. My idea is that if $x=a$, $y=b$, $z=c$, then the cosine theorem implies that the inequality is in fact equality... Hence the matrix  $$\begin{pmatrix}1&-\cos A&-\cos B\\-\cos A&1&-\cos C\\ -\cos B&-\cos C&1\end{pmatrix}$$ is positive semidefinite.  However, there may be some minors (say $-\cos A$) be negative, contradicting to the fact that $M$ is positive semi-definite $\Leftrightarrow$ all minors $\geq 0$...","Let $A,B,C$ be the three angles in the a triangle (with length $a,b,c$). Can we show that  $$x^2+y^2+z^2\geq 2x y \cos A+2xz\cos B+2yz\cos C?$$ for all $x,y,z\in\Bbb R$. I do not see whether it is true, but I find it in some book. I suspect it is false. My idea is that if $x=a$, $y=b$, $z=c$, then the cosine theorem implies that the inequality is in fact equality... Hence the matrix  $$\begin{pmatrix}1&-\cos A&-\cos B\\-\cos A&1&-\cos C\\ -\cos B&-\cos C&1\end{pmatrix}$$ is positive semidefinite.  However, there may be some minors (say $-\cos A$) be negative, contradicting to the fact that $M$ is positive semi-definite $\Leftrightarrow$ all minors $\geq 0$...",,"['linear-algebra', 'geometry', 'inequality', 'vectors', 'triangles']"
20,The definition of $\oplus$,The definition of,\oplus,"I would like to understand why the books give two different concepts to $\oplus$ between vector spaces: See: Concept 1: $W=V_1\oplus V_2=\{(v_1,v_2) \mid v_1\in V_1, v_2\in V_2\}$. Concept 2: $W=V_1\oplus V_2=\{v_1+v_2 \mid v_1\in V_1, v_2\in V_2\}$, where $V_1\cap V_2=\{0\}$. Are they equivalent? I'm thinking to prove this is an isomorphism $(v_1,v_2)\mapsto v_1+v_2$. Thanks","I would like to understand why the books give two different concepts to $\oplus$ between vector spaces: See: Concept 1: $W=V_1\oplus V_2=\{(v_1,v_2) \mid v_1\in V_1, v_2\in V_2\}$. Concept 2: $W=V_1\oplus V_2=\{v_1+v_2 \mid v_1\in V_1, v_2\in V_2\}$, where $V_1\cap V_2=\{0\}$. Are they equivalent? I'm thinking to prove this is an isomorphism $(v_1,v_2)\mapsto v_1+v_2$. Thanks",,['linear-algebra']
21,"Linear Algebra - four ""true or false"" questions about matrices and linear systems","Linear Algebra - four ""true or false"" questions about matrices and linear systems",,"I'm reviewing for my linear algebra course, and have four ""true or false"" questions that I'm struggling to prove. I've included my approach to the solutions in brackets below them: 1) If $A^2 = B^2$, then A = B or A = -B, where A and B are nxn matrices (Not sure how to approach this one at all) 2) Every 3x3 skew symmetric matrix is singular (Pretty sure I have this one correct: Because this is a skew symmetric matrix, $\det(A) = \det(A^T) = \det(-A) = (-1)^n\det(A)$, and when n is odd $\det(A) = -\det(A)$, so $2\det(A) = 0$ and therefore $\det(A) = 0$. As such, the answer is ""False"" because it is only singular when n is odd) 3) Any system of n linear equations in n variables has at most n solutions (A system can have infinitely many solutions if the determinant is zero, right? I just don't know how to prove it) 4) For a square matrix A, A is invertible if and only if $AA^T$ is (Not sure how to approach this one, either)","I'm reviewing for my linear algebra course, and have four ""true or false"" questions that I'm struggling to prove. I've included my approach to the solutions in brackets below them: 1) If $A^2 = B^2$, then A = B or A = -B, where A and B are nxn matrices (Not sure how to approach this one at all) 2) Every 3x3 skew symmetric matrix is singular (Pretty sure I have this one correct: Because this is a skew symmetric matrix, $\det(A) = \det(A^T) = \det(-A) = (-1)^n\det(A)$, and when n is odd $\det(A) = -\det(A)$, so $2\det(A) = 0$ and therefore $\det(A) = 0$. As such, the answer is ""False"" because it is only singular when n is odd) 3) Any system of n linear equations in n variables has at most n solutions (A system can have infinitely many solutions if the determinant is zero, right? I just don't know how to prove it) 4) For a square matrix A, A is invertible if and only if $AA^T$ is (Not sure how to approach this one, either)",,"['linear-algebra', 'matrices', 'determinant', 'systems-of-equations']"
22,Is $\mathbb{R}$ a subspace of $\mathbb{R}^2$?,Is  a subspace of ?,\mathbb{R} \mathbb{R}^2,"I think I've been confusing myself about the language of subspaces and so on.  This is a rather basic question, so please bare with me.  I'm wondering why we do not (or perhaps ""we"" do, and I just don't know about it) say that $\mathbb{ R } $ is a subspace of $\mathbb{ R }^2 $.  It's elementary to prove that the set $$ S:= \left\{ c \cdot \mathbf{x} \mid c \in \mathbb{ R }, \mathbf{x} \in \mathbb{ R }^2  \right\}$$ is a vector subspace of $\mathbb{ R } ^2$.  What is confusing me is that there seems to be an isomorphism between the set $S$ and $\mathbb{ R } $: \begin{align*} \varphi: S &\rightarrow \mathbb{ R }   \\  c \cdot \mathbf{x} &\mapsto c \\ \end{align*} If this is indeed true, as I believe it is having checked that $\varphi$ gives an isomorphism, wouldn't we say that $\mathbb{ R } $ is a subspace of $\mathbb{ R } ^2$? Any help sorting out this (language) problem will be greatly appreciated!","I think I've been confusing myself about the language of subspaces and so on.  This is a rather basic question, so please bare with me.  I'm wondering why we do not (or perhaps ""we"" do, and I just don't know about it) say that $\mathbb{ R } $ is a subspace of $\mathbb{ R }^2 $.  It's elementary to prove that the set $$ S:= \left\{ c \cdot \mathbf{x} \mid c \in \mathbb{ R }, \mathbf{x} \in \mathbb{ R }^2  \right\}$$ is a vector subspace of $\mathbb{ R } ^2$.  What is confusing me is that there seems to be an isomorphism between the set $S$ and $\mathbb{ R } $: \begin{align*} \varphi: S &\rightarrow \mathbb{ R }   \\  c \cdot \mathbf{x} &\mapsto c \\ \end{align*} If this is indeed true, as I believe it is having checked that $\varphi$ gives an isomorphism, wouldn't we say that $\mathbb{ R } $ is a subspace of $\mathbb{ R } ^2$? Any help sorting out this (language) problem will be greatly appreciated!",,['linear-algebra']
23,Derivative of Hadamard product,Derivative of Hadamard product,,What is the derivative of Hadamard product of two matrices with respect to one of them? I.e. what is $D(AB)$ with respect to $A$?,What is the derivative of Hadamard product of two matrices with respect to one of them? I.e. what is $D(AB)$ with respect to $A$?,,"['linear-algebra', 'matrices', 'hadamard-product']"
24,A gap in Halmos' definition of dimension? And how to repair?,A gap in Halmos' definition of dimension? And how to repair?,,"In Halmos' Finite-Dimensional Vector Spaces , section I.8 has a proof of the Steinitz exchange lemma , which says that if $V$ is a vector space, $S$ is a finite independent subset of $V$, and $T$ is a finite generating subset of $V$, then the cardinality of $T$ is not less than the cardinality of $S$. Having proved this, Halmos goes on to assert that ""The number of elements in any basis of a finite-dimensional vector space $V$ is the same as in any other basis."" But this is not quite right, because the lemma only proves that any two finite bases have the same number of elements. I don't see where it has been ruled out that there might still somehow be infinite bases lurking in a vector space with a finite basis. There is a theorem in section I.7 which says that any independent set can be extended to a basis. Halmos assumes that the independent set is finite, but I think the proof still goes through if we assume it possibly infinite, so that if there are any infinite independent subsets, we can extend it to an infinite basis. So it seems we need to rule out the possibility that a vector space with a finite basis might have an infinite independent set. Have I made a mistake anywhere in the above? And if not, can we repair the gap in this theorem easily? I took a quick look at Roman's Advanced Linear Algebra and it seems to have the same defect of assuming that in a vector space with a finite spanning set, all bases must be finite. I'm not able to find where this is proved. Roman has a proof that works for arbitrary vector spaces, but it uses cardinal arithmetic, which I am not familiar with. Is there any easier way?","In Halmos' Finite-Dimensional Vector Spaces , section I.8 has a proof of the Steinitz exchange lemma , which says that if $V$ is a vector space, $S$ is a finite independent subset of $V$, and $T$ is a finite generating subset of $V$, then the cardinality of $T$ is not less than the cardinality of $S$. Having proved this, Halmos goes on to assert that ""The number of elements in any basis of a finite-dimensional vector space $V$ is the same as in any other basis."" But this is not quite right, because the lemma only proves that any two finite bases have the same number of elements. I don't see where it has been ruled out that there might still somehow be infinite bases lurking in a vector space with a finite basis. There is a theorem in section I.7 which says that any independent set can be extended to a basis. Halmos assumes that the independent set is finite, but I think the proof still goes through if we assume it possibly infinite, so that if there are any infinite independent subsets, we can extend it to an infinite basis. So it seems we need to rule out the possibility that a vector space with a finite basis might have an infinite independent set. Have I made a mistake anywhere in the above? And if not, can we repair the gap in this theorem easily? I took a quick look at Roman's Advanced Linear Algebra and it seems to have the same defect of assuming that in a vector space with a finite spanning set, all bases must be finite. I'm not able to find where this is proved. Roman has a proof that works for arbitrary vector spaces, but it uses cardinal arithmetic, which I am not familiar with. Is there any easier way?",,['linear-algebra']
25,Finding a basis for a given subspace of $\Bbb R^4$,Finding a basis for a given subspace of,\Bbb R^4,"Find a basis for the subspace $ W = \{(x, y, z, w) \in\Bbb R^4 : y − 2z + w = 0\}$. What is $\dim(W)$? I don't seem to understand how to solve this problem. I just don't know where to start I am not given an A matrix to work with. possibly the b matrix is $[0, 1, -2,1]$ but is that enough to find the basis?","Find a basis for the subspace $ W = \{(x, y, z, w) \in\Bbb R^4 : y − 2z + w = 0\}$. What is $\dim(W)$? I don't seem to understand how to solve this problem. I just don't know where to start I am not given an A matrix to work with. possibly the b matrix is $[0, 1, -2,1]$ but is that enough to find the basis?",,"['linear-algebra', 'matrices', 'vector-spaces']"
26,What commutes with a matrix in Jordan canonical form?,What commutes with a matrix in Jordan canonical form?,,"The question I would like answered is the following: Given a matrix $G$ and that $G$ commutes with another matrix $X$, that is $[G, X] = 0$, what is $X$? Or more generally, what properties of $X$ may we infer? I understand however that this question is really too vague, so here's a more specific question: If $G$ is in Jordan canonical form, does $[G, X] = 0$ imply that $X$ has the same Jordan canonical form? Or still more specific, if $G$ is diagonal with no two diagonal entries the same, does $[G, X] = 0$ imply that $X$ is diagonal? I have convinced myself that the answer to the latter question is ‘yes’, but a simple proof eludes me.","The question I would like answered is the following: Given a matrix $G$ and that $G$ commutes with another matrix $X$, that is $[G, X] = 0$, what is $X$? Or more generally, what properties of $X$ may we infer? I understand however that this question is really too vague, so here's a more specific question: If $G$ is in Jordan canonical form, does $[G, X] = 0$ imply that $X$ has the same Jordan canonical form? Or still more specific, if $G$ is diagonal with no two diagonal entries the same, does $[G, X] = 0$ imply that $X$ is diagonal? I have convinced myself that the answer to the latter question is ‘yes’, but a simple proof eludes me.",,"['linear-algebra', 'matrices', 'matrix-equations', 'jordan-normal-form']"
27,Find an invertible matrix $P$ and a diagonal matrix $D$ such that $D=P^{−1}AP$?,Find an invertible matrix  and a diagonal matrix  such that ?,P D D=P^{−1}AP,"I have a matrix $A=\begin{bmatrix} -5 & -1 & 2\\ 2 & 0 & -2\\ -6 & -1 & 3\end{bmatrix}$, and I need to find an invertible matrix P and a diagonal matrix D such that $D = P^{-1}AP$. I've found the eigenvalues for the matrix and they are $-3, 1, 0$, so I know the D matrix, but I can't seem to figure out the eigenvectors for the P matrix.","I have a matrix $A=\begin{bmatrix} -5 & -1 & 2\\ 2 & 0 & -2\\ -6 & -1 & 3\end{bmatrix}$, and I need to find an invertible matrix P and a diagonal matrix D such that $D = P^{-1}AP$. I've found the eigenvalues for the matrix and they are $-3, 1, 0$, so I know the D matrix, but I can't seem to figure out the eigenvectors for the P matrix.",,['linear-algebra']
28,Proving any linear map on a subspace of $V$ can be extended to a linear map on $V$,Proving any linear map on a subspace of  can be extended to a linear map on,V V,"Suppose V is a finite dimensional vector space. I am trying to prove that any linear map on subspace of V can be extended to linear map on V. Basically, showing that if $U$ is a subspace of $V$ and $S \in L(U,W)$, then there exists a $T \in L(V,W)$ such that $Tu=Su$ for all $u \in U$. I attempted it by taking the basis of the subspace U with Dim(m) and extending it to basis of V with Dim(n). Since mapping of all vectors of basis of W exists and taking the remaining m-n vectors as zero - we will get a linear map to an element that is in V by defining $T \in L(V,W)$ as: $T(a_1u_1 + ...+ a_mu_m+b_1v_1 + ...+b_nv_n)=a_1Su_1 + ...+a_mSu_m$ Then $Tu=Su \space  \forall u \in U$ What I dont get is this: 1) Please correct me if I'm wrong but a linear map on V should be defined for all elements of V. But does the extension inside $T$ not limit to vectors in V with $v_m+1=.....=v_n=0$ thereby excluding vectors in V where these are not zero? 2) How does the $Su$'s hit every value in $W$? 3) Is there a more intuitive way or step by step explained way of how I need the above equation and condition? I have no idea why it was chosen as thus. Thanks!","Suppose V is a finite dimensional vector space. I am trying to prove that any linear map on subspace of V can be extended to linear map on V. Basically, showing that if $U$ is a subspace of $V$ and $S \in L(U,W)$, then there exists a $T \in L(V,W)$ such that $Tu=Su$ for all $u \in U$. I attempted it by taking the basis of the subspace U with Dim(m) and extending it to basis of V with Dim(n). Since mapping of all vectors of basis of W exists and taking the remaining m-n vectors as zero - we will get a linear map to an element that is in V by defining $T \in L(V,W)$ as: $T(a_1u_1 + ...+ a_mu_m+b_1v_1 + ...+b_nv_n)=a_1Su_1 + ...+a_mSu_m$ Then $Tu=Su \space  \forall u \in U$ What I dont get is this: 1) Please correct me if I'm wrong but a linear map on V should be defined for all elements of V. But does the extension inside $T$ not limit to vectors in V with $v_m+1=.....=v_n=0$ thereby excluding vectors in V where these are not zero? 2) How does the $Su$'s hit every value in $W$? 3) Is there a more intuitive way or step by step explained way of how I need the above equation and condition? I have no idea why it was chosen as thus. Thanks!",,['linear-algebra']
29,$A\in\mathrm{M}_{n\times n}(\mathbb{C})\implies A^TA$ is diagonalisable?,is diagonalisable?,A\in\mathrm{M}_{n\times n}(\mathbb{C})\implies A^TA,"I have been set some work to do over the holidays, and one of the questions gives a hint that is as follows: $A\in\mathrm{M}_{n\times n}(\mathbb{C})\implies A^TA\text{ is diagonalisable}$. I know that $A\in\mathrm{M}_{n\times n}(\mathbb{R})\implies A^TA\text{ is diagonalisable}$ $A\in\mathrm{M}_{n\times n}(\mathbb{C})\implies A^*A\text{ is diagonalisable}$ where the former can be thought of as a particular case of the latter. Both those statements are true because $A^*A$ is self adjoint, and we can then apply the Spectral Theorem for normal operators. But is the statement at the top of my question true, or has the lecturer simply mistyped one of the two facts that I've written? If it is true, I can't see how to prove it, so any hints would be appreciated.","I have been set some work to do over the holidays, and one of the questions gives a hint that is as follows: $A\in\mathrm{M}_{n\times n}(\mathbb{C})\implies A^TA\text{ is diagonalisable}$. I know that $A\in\mathrm{M}_{n\times n}(\mathbb{R})\implies A^TA\text{ is diagonalisable}$ $A\in\mathrm{M}_{n\times n}(\mathbb{C})\implies A^*A\text{ is diagonalisable}$ where the former can be thought of as a particular case of the latter. Both those statements are true because $A^*A$ is self adjoint, and we can then apply the Spectral Theorem for normal operators. But is the statement at the top of my question true, or has the lecturer simply mistyped one of the two facts that I've written? If it is true, I can't see how to prove it, so any hints would be appreciated.",,"['linear-algebra', 'matrices']"
30,Triangular matrices and commutators,Triangular matrices and commutators,,"From Humphreys' Introduction to Lie Algebras and Representation Theory : We conclude this subsection by mentioning several other subalgebras of $gl(n,F)$ which play an important subsidiary role for us. Let $t(n,F)$ be the set of upper triangular matrices $(a_{ij})$, $a_{ij}=0$ if $i>j$. Let $n(n,F)$ be the strictly upper triangular matrices ($a_{ij}=0$ if $i\geq j$). Finally, let $o(n,F)$ be the set of all diagonal matrices . It is trivial to check that each of these is closed under the bracket. Notice also that $t(n,F)=o(n,F)+n(n,F)$ (vector space direct sum), with $[o(n,F),n(n,F)]=n(n,F)$, hence $[t(n,F),t(n,F)]=n(n,F)$. (If $H,K$ are subalgebras of $L$, $[H K]$ denotes the subspace of $L$ spanned by commutators $[xy]$, $x\in H, y\in K$.) I am confused about why $[t(n,F),t(n,F)]=n(n,F)$. If we have two upper triangular matrices $X,Y$, then $XY$ is also upper triangular, and so $XY-YX$ is upper triangular, so it should stay in $t(n,F)$. But I don't see why $XY-YX$ should be strictly upper triangular.","From Humphreys' Introduction to Lie Algebras and Representation Theory : We conclude this subsection by mentioning several other subalgebras of $gl(n,F)$ which play an important subsidiary role for us. Let $t(n,F)$ be the set of upper triangular matrices $(a_{ij})$, $a_{ij}=0$ if $i>j$. Let $n(n,F)$ be the strictly upper triangular matrices ($a_{ij}=0$ if $i\geq j$). Finally, let $o(n,F)$ be the set of all diagonal matrices . It is trivial to check that each of these is closed under the bracket. Notice also that $t(n,F)=o(n,F)+n(n,F)$ (vector space direct sum), with $[o(n,F),n(n,F)]=n(n,F)$, hence $[t(n,F),t(n,F)]=n(n,F)$. (If $H,K$ are subalgebras of $L$, $[H K]$ denotes the subspace of $L$ spanned by commutators $[xy]$, $x\in H, y\in K$.) I am confused about why $[t(n,F),t(n,F)]=n(n,F)$. If we have two upper triangular matrices $X,Y$, then $XY$ is also upper triangular, and so $XY-YX$ is upper triangular, so it should stay in $t(n,F)$. But I don't see why $XY-YX$ should be strictly upper triangular.",,"['linear-algebra', 'abstract-algebra']"
31,How does one combine proportionality?,How does one combine proportionality?,,"this is something that often comes up in both Physics and Mathematics, in my A Levels. Here is the crux of the problem. So, you have something like this : $A \propto B$ which means that $A = kB \tag{1}$ Fine, then you get something like : $A \propto L^2$ which means that $A = k'L^2 \tag{2}$ Okay, so from $(1)$ and $(2)$ that they derive : $$A \propto BL^2$$ Now how does that work? How do we derive from the properties in $(1)$ and $(2)$ , that $A \propto BL^2$ . Thanks in advance.","this is something that often comes up in both Physics and Mathematics, in my A Levels. Here is the crux of the problem. So, you have something like this : which means that Fine, then you get something like : which means that Okay, so from and that they derive : Now how does that work? How do we derive from the properties in and , that . Thanks in advance.",A \propto B A = kB \tag{1} A \propto L^2 A = k'L^2 \tag{2} (1) (2) A \propto BL^2 (1) (2) A \propto BL^2,['linear-algebra']
32,Dual basis existence and uniqueness.,Dual basis existence and uniqueness.,,"In Wikipedia, on Dual Basis they say: ""Algebraically, a dual set always exists, and gives an injection from $V$ into $V^*$. However, a dual basis exists if and only if a vector space is finite dimensional, and each basis has a unique dual basis..."" The following afterwards I can't understand. Could you please give me some direct proof for that interesting result of existence and uniqueness of dual basis in finite dimensional vector space? Thanks.","In Wikipedia, on Dual Basis they say: ""Algebraically, a dual set always exists, and gives an injection from $V$ into $V^*$. However, a dual basis exists if and only if a vector space is finite dimensional, and each basis has a unique dual basis..."" The following afterwards I can't understand. Could you please give me some direct proof for that interesting result of existence and uniqueness of dual basis in finite dimensional vector space? Thanks.",,"['linear-algebra', 'reference-request']"
33,Underdetermined Linear Systems and the Least Squares Solution,Underdetermined Linear Systems and the Least Squares Solution,,"I have an underdetermined linear system, with 3 equations and four unknows. I also know an initial guess for these 4 unknows. The article I am reading says: We can solve the system using the least squares method, starting form a guess. I don't know how can I do this. Thanks.","I have an underdetermined linear system, with 3 equations and four unknows. I also know an initial guess for these 4 unknows. The article I am reading says: We can solve the system using the least squares method, starting form a guess. I don't know how can I do this. Thanks.",,"['linear-algebra', 'numerical-linear-algebra', 'least-squares']"
34,Is the sum of two normal operators normal?,Is the sum of two normal operators normal?,,A normal operator is defined as    $AA^{*}\ =A^{*}A$ Where A is an operator how do i show the sum of two normal operators is normal?  Or find a counter example that shows this is false?,A normal operator is defined as    $AA^{*}\ =A^{*}A$ Where A is an operator how do i show the sum of two normal operators is normal?  Or find a counter example that shows this is false?,,"['linear-algebra', 'operator-theory', 'orthonormal']"
35,A linear transformation $T:V\to V$ is one-to-one if and only if it is onto,A linear transformation  is one-to-one if and only if it is onto,T:V\to V,"Let $V$ be a finite dimensional vector space.  Show that a linear transformation $T\colon V \to V$ is one-to-one if and only if it is onto. The hint I was given was that one only needs to show that $T(e_1),\dots,T(e_n)$ is a basis whenever $e_1,\dots,e_n$ is a basis. My first attempt is as follows: Let $x \in V$.  Then  $$Tx=[T(x_1)e_1+\dots+T(x_n)e_n]=[x_1T(e_1),\dots,x_nT(e_n)]=x\cdot T(e).$$  I really want $T(e_n)$ to be a basis of $V$, but I'm not sure that it is.  It seems like it is obviously a basis because $T$ maps from $V$ to $V$. Is there a property of linear transformations that makes this apparent?","Let $V$ be a finite dimensional vector space.  Show that a linear transformation $T\colon V \to V$ is one-to-one if and only if it is onto. The hint I was given was that one only needs to show that $T(e_1),\dots,T(e_n)$ is a basis whenever $e_1,\dots,e_n$ is a basis. My first attempt is as follows: Let $x \in V$.  Then  $$Tx=[T(x_1)e_1+\dots+T(x_n)e_n]=[x_1T(e_1),\dots,x_nT(e_n)]=x\cdot T(e).$$  I really want $T(e_n)$ to be a basis of $V$, but I'm not sure that it is.  It seems like it is obviously a basis because $T$ maps from $V$ to $V$. Is there a property of linear transformations that makes this apparent?",,"['linear-algebra', 'vector-spaces']"
36,What are the generators for $SL_n(\mathbb{R})$ (Michael Artin's Algebra book),What are the generators for  (Michael Artin's Algebra book),SL_n(\mathbb{R}),"The book asks you to prove that $SL_n(\mathbb{R})$ is generated by elementary (row operation) matrices in which one nonzero off-diagonal entry is added to the identity matrix.  For example, $$ \begin{bmatrix} 1 & a \\ 0 & 1 \end{bmatrix} $$ acts by left multiplication on $2\times2$ matrices by adding $a$ times (row 2) to (row 1).   Considering a simple example: $$ M= \begin{bmatrix} a & 0  \\  0 & 1/a \end{bmatrix}, a \neq 0$$ you can see that the matrix $M$ does belong to $SL_n(\mathbb{R})$.  However, the elementary matrices composing $M$ are of the below type and not of the first type (nonzero off-diagonal entry). $$ \begin{bmatrix}  c & 0 \\   0 & 1  \end{bmatrix} $$ i.e. one nonzero diagonal entry added to the identity matrix.  So  $$M =  \begin{bmatrix} a & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1/a \end{bmatrix} $$ So $M$ clearly isn't generated by the first type.  What is going wrong?","The book asks you to prove that $SL_n(\mathbb{R})$ is generated by elementary (row operation) matrices in which one nonzero off-diagonal entry is added to the identity matrix.  For example, $$ \begin{bmatrix} 1 & a \\ 0 & 1 \end{bmatrix} $$ acts by left multiplication on $2\times2$ matrices by adding $a$ times (row 2) to (row 1).   Considering a simple example: $$ M= \begin{bmatrix} a & 0  \\  0 & 1/a \end{bmatrix}, a \neq 0$$ you can see that the matrix $M$ does belong to $SL_n(\mathbb{R})$.  However, the elementary matrices composing $M$ are of the below type and not of the first type (nonzero off-diagonal entry). $$ \begin{bmatrix}  c & 0 \\   0 & 1  \end{bmatrix} $$ i.e. one nonzero diagonal entry added to the identity matrix.  So  $$M =  \begin{bmatrix} a & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1/a \end{bmatrix} $$ So $M$ clearly isn't generated by the first type.  What is going wrong?",,"['linear-algebra', 'matrices']"
37,Can a subspace be written as the direct sum of its intersections with subspaces generated by partitions of a basis?,Can a subspace be written as the direct sum of its intersections with subspaces generated by partitions of a basis?,,"Say $V$ is a vector space with some basis $\mathcal{B}=\{b_i\}_{i\in I}$, and let $\{B_1,\dots,B_n\}$ be a partition of $\mathcal{B}$. In general, for $S$ a subspace it is not true that $$ S=\bigoplus_{i=1}^n (S\cap\langle B_i\rangle). $$ For example, take $V=\mathbb{R}^2$ as a real vector space, and $S=\langle(1,1)\rangle$, with $B_1=\{e_1\}$ and $B_2=\{e_2\}$. Then $S\cap\langle B_i\rangle=\{0\}$ for $i=1,2$, so no equality can hold. If we add the condition that $S\cap\langle B_i\rangle\neq\{0\}$ for all $i$, can the above equality be shown to hold? Or is it still impossible? It is clear that the $\supseteq$ containment holds in all cases.","Say $V$ is a vector space with some basis $\mathcal{B}=\{b_i\}_{i\in I}$, and let $\{B_1,\dots,B_n\}$ be a partition of $\mathcal{B}$. In general, for $S$ a subspace it is not true that $$ S=\bigoplus_{i=1}^n (S\cap\langle B_i\rangle). $$ For example, take $V=\mathbb{R}^2$ as a real vector space, and $S=\langle(1,1)\rangle$, with $B_1=\{e_1\}$ and $B_2=\{e_2\}$. Then $S\cap\langle B_i\rangle=\{0\}$ for $i=1,2$, so no equality can hold. If we add the condition that $S\cap\langle B_i\rangle\neq\{0\}$ for all $i$, can the above equality be shown to hold? Or is it still impossible? It is clear that the $\supseteq$ containment holds in all cases.",,['linear-algebra']
38,The inverse of the adjacency matrix of an undirected cycle,The inverse of the adjacency matrix of an undirected cycle,,"Is there an expression for $A^{-1}$, where $A_{n \times n}$ is the adjacency matrix of an undirected cycle $C_n$, in terms of $A$? I want this expression because I want to compute $A^{-1}$ without actually inverting $A$. As one answer suggests, $A$ is non-invertible for certain values of $n$ (namely when $n$ is a multiple of $4$).","Is there an expression for $A^{-1}$, where $A_{n \times n}$ is the adjacency matrix of an undirected cycle $C_n$, in terms of $A$? I want this expression because I want to compute $A^{-1}$ without actually inverting $A$. As one answer suggests, $A$ is non-invertible for certain values of $n$ (namely when $n$ is a multiple of $4$).",,"['linear-algebra', 'graph-theory']"
39,Necessary condition for positive-semidefiniteness -- is it sufficient?,Necessary condition for positive-semidefiniteness -- is it sufficient?,,"Suppose that I have a symmetric square $n\times n$ matrix $A$ such that: $a_{ii}\geq 0$ for all $1\leq i\leq n$, and that $a_{ii}a_{jj} - a_{ij}^2 \geq 0$ for all $1\leq i\leq n$ and $i < j \leq n$. Clearly this is a necessary condition for positive semidefiniteness because of Sylvester's criterion, and the fact that $P^TAP$ is positive semidefinite for any positive semidefinite $A$ and permutation matrix $P$. This question hints that this condition is not sufficient. Can you list any simple counterexamples to the claim that this condition is sufficient for $A$ to be positive semidefinite? Thanks. P.S. I won't be offended if you flag this as a duplicate, but it's the counterexamples I'm interested in, so please consider that before you flag.","Suppose that I have a symmetric square $n\times n$ matrix $A$ such that: $a_{ii}\geq 0$ for all $1\leq i\leq n$, and that $a_{ii}a_{jj} - a_{ij}^2 \geq 0$ for all $1\leq i\leq n$ and $i < j \leq n$. Clearly this is a necessary condition for positive semidefiniteness because of Sylvester's criterion, and the fact that $P^TAP$ is positive semidefinite for any positive semidefinite $A$ and permutation matrix $P$. This question hints that this condition is not sufficient. Can you list any simple counterexamples to the claim that this condition is sufficient for $A$ to be positive semidefinite? Thanks. P.S. I won't be offended if you flag this as a duplicate, but it's the counterexamples I'm interested in, so please consider that before you flag.",,"['linear-algebra', 'matrices']"
40,A non-degenerate trace implies dual basis [updated],A non-degenerate trace implies dual basis [updated],,"I found a better proof of the theorem in Serge Lang - Algebraic Number Theory but I put in bold the parts I don't understand. Hoping for any explanations of these points. The trace $Tr : L \to K$ is linear and nondegenerate in the sense that there exists an $x \in L$ such that $Tr(x) \not = 0$. If $\alpha$ is a nonzero element of $L$ then $x \mapsto Tr(\alpha x)$ is an element of the dual space of $L$ (as a $K$-vector space) and it is a homomorphism from $L$ to the dual space. Since kernel is trivial it follows that $L$ is isomorphic to it's dual under the bilinear form $(x,y) \mapsto Tr(xy)$. I don't understand this part, trivial kernel only gives injectivity. By rank-nullity I know the image of the map has the same dimension as $L$ but I would also need to know that a-priori to have surjectivity? Let $\{w'_1,\ldots,w'_n\}$ be the dual basis of $\{w_1,\ldots,w_n\}$ satisfying $Tr(w'_i w_j) = \delta_{ij}$. I suppose the $w'_1$ are elements of $L$ which represent elements of the dual space, but I don't really understand what they are and how we can make sure the $\delta_{ij}$ condition is satisfied . Let $c \not = 0$ be an element of $A$ such that each $cw'_i$ is integral.","I found a better proof of the theorem in Serge Lang - Algebraic Number Theory but I put in bold the parts I don't understand. Hoping for any explanations of these points. The trace $Tr : L \to K$ is linear and nondegenerate in the sense that there exists an $x \in L$ such that $Tr(x) \not = 0$. If $\alpha$ is a nonzero element of $L$ then $x \mapsto Tr(\alpha x)$ is an element of the dual space of $L$ (as a $K$-vector space) and it is a homomorphism from $L$ to the dual space. Since kernel is trivial it follows that $L$ is isomorphic to it's dual under the bilinear form $(x,y) \mapsto Tr(xy)$. I don't understand this part, trivial kernel only gives injectivity. By rank-nullity I know the image of the map has the same dimension as $L$ but I would also need to know that a-priori to have surjectivity? Let $\{w'_1,\ldots,w'_n\}$ be the dual basis of $\{w_1,\ldots,w_n\}$ satisfying $Tr(w'_i w_j) = \delta_{ij}$. I suppose the $w'_1$ are elements of $L$ which represent elements of the dual space, but I don't really understand what they are and how we can make sure the $\delta_{ij}$ condition is satisfied . Let $c \not = 0$ be an element of $A$ such that each $cw'_i$ is integral.",,"['linear-algebra', 'commutative-algebra', 'algebraic-number-theory', 'trace']"
41,Basic trace inequality,Basic trace inequality,,"Suppose $A$ and $B$ are self-adjoint matrices. Why is it true that $$Tr(A^2) \le Tr(Ae^{-tB}Ae^{tB})$$ for $t\in\mathbb R$, where $e^x$ denotes the matrix exponential?","Suppose $A$ and $B$ are self-adjoint matrices. Why is it true that $$Tr(A^2) \le Tr(Ae^{-tB}Ae^{tB})$$ for $t\in\mathbb R$, where $e^x$ denotes the matrix exponential?",,['linear-algebra']
42,exp(X)=A has a solution if A is sufficiently close to identity matrix,exp(X)=A has a solution if A is sufficiently close to identity matrix,,Can anyone give hint for proving this? I think some kind of inverse thm argument would work. But I wasn't able to make it accurate... such as continuity of mapping $X \mapsto \exp(X)$ or this mapping has full rank near $A$ if $A$ is sufficiently close to identity...,Can anyone give hint for proving this? I think some kind of inverse thm argument would work. But I wasn't able to make it accurate... such as continuity of mapping $X \mapsto \exp(X)$ or this mapping has full rank near $A$ if $A$ is sufficiently close to identity...,,"['linear-algebra', 'multivariable-calculus']"
43,Cauchy's functional equation for $\mathbb R^n$,Cauchy's functional equation for,\mathbb R^n,"Suppose $f(x+y)=f(x)+f(y)$. If $f:\mathbb R\to \mathbb R$ and is measurable, then $f(x)=cx$. This is referred to as Cauchy's functional equation . Suppose $f:\mathbb R^n\to \mathbb R^n$ instead. Does it still hold that $f$ is linear? Wikipedia says that Hilbert's fifth problem is a generalization of this functional equation, but I can't parse that page well enough to understand how it relates.","Suppose $f(x+y)=f(x)+f(y)$. If $f:\mathbb R\to \mathbb R$ and is measurable, then $f(x)=cx$. This is referred to as Cauchy's functional equation . Suppose $f:\mathbb R^n\to \mathbb R^n$ instead. Does it still hold that $f$ is linear? Wikipedia says that Hilbert's fifth problem is a generalization of this functional equation, but I can't parse that page well enough to understand how it relates.",,"['linear-algebra', 'functional-equations']"
44,Cross-product technique to find the eigenspaces of a $3\times 3$ matrix,Cross-product technique to find the eigenspaces of a  matrix,3\times 3,"$1)$ For each distinct real eigenvalue $\lambda$ of a $3 \times 3$ matrix $A$, it turns out that the cross product of the transpose of any two linearly independent rows of $A-\lambda I$ gives a corresponding eigenvector (and thus easily the corresponding eigenspace, since in this case the eigenspace is an eigenline). But why does this method work? $2)$ I think the above may be generalisable to any $3\times 3$ matrix with only real eigenvalues: Substitute an eigenvalue of $A$ into $A-\lambda I$. Then take the cross products of the transpose of any two pairs of rows of $A-\lambda I$. Only two possibilities exist: $(a)$ If only one is nonzero, that gives a corresponding eigenvector and hence easily the eigenspace. ($b$)If both are zero, then the eigenspace is the plane orthogonal to any row of $A-\lambda I$. Is this generalisation valid, and if so, why does the method work? $3)$ How about for the final case whereby $2$ complex eigenvalues exist??","$1)$ For each distinct real eigenvalue $\lambda$ of a $3 \times 3$ matrix $A$, it turns out that the cross product of the transpose of any two linearly independent rows of $A-\lambda I$ gives a corresponding eigenvector (and thus easily the corresponding eigenspace, since in this case the eigenspace is an eigenline). But why does this method work? $2)$ I think the above may be generalisable to any $3\times 3$ matrix with only real eigenvalues: Substitute an eigenvalue of $A$ into $A-\lambda I$. Then take the cross products of the transpose of any two pairs of rows of $A-\lambda I$. Only two possibilities exist: $(a)$ If only one is nonzero, that gives a corresponding eigenvector and hence easily the eigenspace. ($b$)If both are zero, then the eigenspace is the plane orthogonal to any row of $A-\lambda I$. Is this generalisation valid, and if so, why does the method work? $3)$ How about for the final case whereby $2$ complex eigenvalues exist??",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
45,Relationship: Rank of a matrix $\leftrightarrow$ # of eigenvalues,Relationship: Rank of a matrix  # of eigenvalues,\leftrightarrow,"Can someone tell, why the number of the nonzero eigenvalues (counted according to their algebraic multiplicities) of a matrix of type $A^{*}A$, where $A$ is an arbitrary real or complexvalued matrix, is equal to the rank of $A$ ? Here , in step 2, it seems to me that exactly this assertion is made, and I can't quite understand it. I get, that if  $$ \text{rank}\left(A^{*}A\right)=r $$ then $$ \ker\left(A^{*}A\right)=n-r, $$ if it happens that $A^{*}A$ is an $n\times n$ matrix. But how can I conclude from this, that the multiplicity of the $0$ eigenvalue is $n-r$, i.e. that there are $n-r$ that are mapped to $0$ (couldn't it by that only linear combination of $n-r$ eigenvectors are mapped to $0$, since the kernel doesn't have to necessarily be spanned by the eigenvectors themselves, as far as I know) ?","Can someone tell, why the number of the nonzero eigenvalues (counted according to their algebraic multiplicities) of a matrix of type $A^{*}A$, where $A$ is an arbitrary real or complexvalued matrix, is equal to the rank of $A$ ? Here , in step 2, it seems to me that exactly this assertion is made, and I can't quite understand it. I get, that if  $$ \text{rank}\left(A^{*}A\right)=r $$ then $$ \ker\left(A^{*}A\right)=n-r, $$ if it happens that $A^{*}A$ is an $n\times n$ matrix. But how can I conclude from this, that the multiplicity of the $0$ eigenvalue is $n-r$, i.e. that there are $n-r$ that are mapped to $0$ (couldn't it by that only linear combination of $n-r$ eigenvectors are mapped to $0$, since the kernel doesn't have to necessarily be spanned by the eigenvectors themselves, as far as I know) ?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-rank']"
46,Show that $\det(A+A^t)\neq p$ where $p$ is a prime number and $A \in M_{p \times p}(\mathbb{Z})$,Show that  where  is a prime number and,\det(A+A^t)\neq p p A \in M_{p \times p}(\mathbb{Z}),"I've stumbled across the above question. Usually, I don't spend my time on such problems, though it's a nice question in my opinion. My idea : Case $p=2$ is easy, follows from $x^2 \equiv 2 \;\;\; (\text{mod} \;\; 4)$ has no integer solution. Assume the contrary and $p$ is an odd prime number. Since, $A+A^t$ is symmetric, then is has only real eigenvalues, so if $\det(xI-(A+A^t)) \in \mathbb{Z}[x]$ is the characteristic polynomial, it factors $(x-\lambda_1) \cdots (x-\lambda_p)$ where $\lambda_i$'s are real numbers. By rational root test, if any of $\lambda_i$ is rational, then it will be an integer. Therefore, if we show that non of them is irrational (hence, all $\lambda_i$ are integers), we may then conclude as follows; $\sum_i \lambda_i=-tr(A+A^t)=2k$ where $k \in \mathbb{Z},$ and $\lambda_1 \cdots \lambda_p=-p.$ Now, $p$ is odd , then wlog, $\lambda_p=-p$ and $\lambda_1=\cdots=\lambda_{2t}=-1$ and $\lambda_{2t}=\cdots=\lambda_{p-1}=1,$ but $\sum_i \lambda_i=-2t+p-1-2t-p$ which is odd, $\lambda_p=p$ and $\lambda_1= \cdots=\lambda_{2t+1}=-1$ and $\lambda_{2t+1}=\cdots=\lambda_{p-1}=1,$ but again $\sum_i \lambda_i=-2t-1+p-1-2t-1+p$ which is odd, hence contradiction. Now, we're left to show that (in my words) Lemma : Given $p$ integers and irrational numbers, where $p$ is an odd prime number. We have that $$\sum_i\lambda_i=2k \in \mathbb{Z}, $$ $$\sum_{i<j}\lambda_i\lambda_j \in \mathbb{Z},$$ $$.$$ $$.$$ $$.$$ $$\lambda_1\cdots \lambda_p=-p \in \mathbb{Z}.$$ Then, non of the $\lambda_i$ is irrational. An idea to prove the lamma : Since any symmetric polynomial in $p$ variables $\lambda_i$ with integer coefficient can be expressed in terms of elementary symmetric polynomials above, then any symmetric (polynomial with integer coefficient) expression of $\lambda_i$'s will be an integer. Is it possible to conclude from here that non of the $\lambda_i$ is irrational? P.S. I'd appreciate any other ideas for solving it.","I've stumbled across the above question. Usually, I don't spend my time on such problems, though it's a nice question in my opinion. My idea : Case $p=2$ is easy, follows from $x^2 \equiv 2 \;\;\; (\text{mod} \;\; 4)$ has no integer solution. Assume the contrary and $p$ is an odd prime number. Since, $A+A^t$ is symmetric, then is has only real eigenvalues, so if $\det(xI-(A+A^t)) \in \mathbb{Z}[x]$ is the characteristic polynomial, it factors $(x-\lambda_1) \cdots (x-\lambda_p)$ where $\lambda_i$'s are real numbers. By rational root test, if any of $\lambda_i$ is rational, then it will be an integer. Therefore, if we show that non of them is irrational (hence, all $\lambda_i$ are integers), we may then conclude as follows; $\sum_i \lambda_i=-tr(A+A^t)=2k$ where $k \in \mathbb{Z},$ and $\lambda_1 \cdots \lambda_p=-p.$ Now, $p$ is odd , then wlog, $\lambda_p=-p$ and $\lambda_1=\cdots=\lambda_{2t}=-1$ and $\lambda_{2t}=\cdots=\lambda_{p-1}=1,$ but $\sum_i \lambda_i=-2t+p-1-2t-p$ which is odd, $\lambda_p=p$ and $\lambda_1= \cdots=\lambda_{2t+1}=-1$ and $\lambda_{2t+1}=\cdots=\lambda_{p-1}=1,$ but again $\sum_i \lambda_i=-2t-1+p-1-2t-1+p$ which is odd, hence contradiction. Now, we're left to show that (in my words) Lemma : Given $p$ integers and irrational numbers, where $p$ is an odd prime number. We have that $$\sum_i\lambda_i=2k \in \mathbb{Z}, $$ $$\sum_{i<j}\lambda_i\lambda_j \in \mathbb{Z},$$ $$.$$ $$.$$ $$.$$ $$\lambda_1\cdots \lambda_p=-p \in \mathbb{Z}.$$ Then, non of the $\lambda_i$ is irrational. An idea to prove the lamma : Since any symmetric polynomial in $p$ variables $\lambda_i$ with integer coefficient can be expressed in terms of elementary symmetric polynomials above, then any symmetric (polynomial with integer coefficient) expression of $\lambda_i$'s will be an integer. Is it possible to conclude from here that non of the $\lambda_i$ is irrational? P.S. I'd appreciate any other ideas for solving it.",,"['linear-algebra', 'abstract-algebra', 'number-theory', 'matrices']"
47,Graphs with eigenvalues of large multiplicity,Graphs with eigenvalues of large multiplicity,,"For a strongly regular graph, there are exactly 3 eigenvalues, all nonzero (I believe).  One has multiplicity 1, which means the other two have pretty high multiplicities.  There are tables that give these eigenvalues and multiplicities: http://www.win.tue.nl/~aeb/graphs/srg/srgtab1-50.html For example, the Schlaefli graph is order 27 but has an eigenvalue of order 20. My question is, are there other known graphs (families, types, or just single graphs) that have large multiplicities of eigenvalues?  When I check a random graph in Sage, it seems the max multiplicity is mostly 1.","For a strongly regular graph, there are exactly 3 eigenvalues, all nonzero (I believe).  One has multiplicity 1, which means the other two have pretty high multiplicities.  There are tables that give these eigenvalues and multiplicities: http://www.win.tue.nl/~aeb/graphs/srg/srgtab1-50.html For example, the Schlaefli graph is order 27 but has an eigenvalue of order 20. My question is, are there other known graphs (families, types, or just single graphs) that have large multiplicities of eigenvalues?  When I check a random graph in Sage, it seems the max multiplicity is mostly 1.",,"['linear-algebra', 'graph-theory', 'eigenvalues-eigenvectors', 'algebraic-graph-theory']"
48,What is the Jordan canonical form of $A^{2}$ if we know that of $A$?,What is the Jordan canonical form of  if we know that of ?,A^{2} A,"Let $A\in M_{n}$ have Jordan canonical form $J_{n_1}(\lambda_{1})\oplus\cdots\oplus J_{n_k}(\lambda_{k})$. If $A$ is non-singular ($\lambda_i\neq 0$), what is the Jordan canonical form of $A^{2}$? I can prove that if the eigenvalues of $A$ are $\sigma(A)=\{\lambda_{1},\dots, \lambda_{n} \}$ then $\sigma(A^{2})=\{\lambda_{1}^{2},\dots, \lambda_{n}^{2} \}$, for this reason I have been trying to attack this problem using this fact, but I am getting nowhere. How should I proceed?","Let $A\in M_{n}$ have Jordan canonical form $J_{n_1}(\lambda_{1})\oplus\cdots\oplus J_{n_k}(\lambda_{k})$. If $A$ is non-singular ($\lambda_i\neq 0$), what is the Jordan canonical form of $A^{2}$? I can prove that if the eigenvalues of $A$ are $\sigma(A)=\{\lambda_{1},\dots, \lambda_{n} \}$ then $\sigma(A^{2})=\{\lambda_{1}^{2},\dots, \lambda_{n}^{2} \}$, for this reason I have been trying to attack this problem using this fact, but I am getting nowhere. How should I proceed?",,"['linear-algebra', 'matrices']"
49,Do these vectors form a basis?,Do these vectors form a basis?,,"I'm researching a potential algorithm, and I'm hoping that someone can verify my calculations. I have sets of vectors in $\mathbb{R}^6$ that I can use.  They have a corresponding value associated with them, but this relation is not necessarily a simple one.  I'd like to find the value associated with the vector $(0,0,1,0,0,0)$.  So I'm wondering if I can perform linear algebra, using the vectors that I can create, to do so. One vector I can create is $(a,a,a,0,0,0)$ for any real $a$.  A second is $(b,b,b,b,b,b)$. I can also create vectors of the form $(c^0,c^1,c^2,c^0,c^1,c^2)$ for some real number $c$, where $c^k$ is just simply $c$ taken to the $k$th power.  A second form that I can create is $(0, d, 2d, 0, 0, 0)$ for real $d$. I am wondering if these vectors form a complete basis for $\mathbb{R}^6$. In case it helps, I can use as many vectors as I want, adding and/or subtracting them, as long as they are of the forms above. My Question What I really want to know is, can I find the corresponding value for $(0,0,1,0,0,0)$?","I'm researching a potential algorithm, and I'm hoping that someone can verify my calculations. I have sets of vectors in $\mathbb{R}^6$ that I can use.  They have a corresponding value associated with them, but this relation is not necessarily a simple one.  I'd like to find the value associated with the vector $(0,0,1,0,0,0)$.  So I'm wondering if I can perform linear algebra, using the vectors that I can create, to do so. One vector I can create is $(a,a,a,0,0,0)$ for any real $a$.  A second is $(b,b,b,b,b,b)$. I can also create vectors of the form $(c^0,c^1,c^2,c^0,c^1,c^2)$ for some real number $c$, where $c^k$ is just simply $c$ taken to the $k$th power.  A second form that I can create is $(0, d, 2d, 0, 0, 0)$ for real $d$. I am wondering if these vectors form a complete basis for $\mathbb{R}^6$. In case it helps, I can use as many vectors as I want, adding and/or subtracting them, as long as they are of the forms above. My Question What I really want to know is, can I find the corresponding value for $(0,0,1,0,0,0)$?",,['linear-algebra']
50,Geometric and analytic multiplicity of a linear operator,Geometric and analytic multiplicity of a linear operator,,"If I understand correctly, the analytic multiplicity of a linear operator say $T:V\to V$ is the amount of times $\lambda$ shows up as a root in the characteristic polynomial (Assuming you have a matrix $A$ representing $T$ with respect to some basis of $V$, then the characteristic equation is $\det(A-\lambda I)$). I understand how to find this, but what exactly does this mean for the linear operator? Also, how do we find the geometric multiplicity of $T$?  What is it's significance?  I tried looking it up on Wikipedia but they are using some notation and words that I am not familiar with; the definitions there are usually quite formal.  Am I correct in thinking that it is the greatest amount of linearly independent vectors of the eigenvalues of $T$?  Do all the eigenvalues of a linear operator have the same amount of linearly independent vectors? Edit: Here is a specific example. We have the matrix, say $A$, with $1$s all along the diagonal and above, and $0$s below.  Since this matrix is upper triangular, the eigenvalues of $A$ are $1$.  Looking at the equations we have to satisfy to find the eigenvector(s) of an eigenvalue $1$, we get: \begin{align*} x_1 + x_2 + \cdots + x_n &= \lambda x_1\\ x_2 + \cdots + x_n &= \lambda x_2\\ &\vdots\\ x_n &=\lambda x_n \end{align*} It seems like we need to do some inspection here, it would be hard (or at least I can't see how) to show this formally.  It seems to me like there are two possible linear independant eigenvectors here, $$\displaystyle \vec 0$$ and $$\displaystyle (1,0,...,0)$$ So the geometric multiplicity is $1$?","If I understand correctly, the analytic multiplicity of a linear operator say $T:V\to V$ is the amount of times $\lambda$ shows up as a root in the characteristic polynomial (Assuming you have a matrix $A$ representing $T$ with respect to some basis of $V$, then the characteristic equation is $\det(A-\lambda I)$). I understand how to find this, but what exactly does this mean for the linear operator? Also, how do we find the geometric multiplicity of $T$?  What is it's significance?  I tried looking it up on Wikipedia but they are using some notation and words that I am not familiar with; the definitions there are usually quite formal.  Am I correct in thinking that it is the greatest amount of linearly independent vectors of the eigenvalues of $T$?  Do all the eigenvalues of a linear operator have the same amount of linearly independent vectors? Edit: Here is a specific example. We have the matrix, say $A$, with $1$s all along the diagonal and above, and $0$s below.  Since this matrix is upper triangular, the eigenvalues of $A$ are $1$.  Looking at the equations we have to satisfy to find the eigenvector(s) of an eigenvalue $1$, we get: \begin{align*} x_1 + x_2 + \cdots + x_n &= \lambda x_1\\ x_2 + \cdots + x_n &= \lambda x_2\\ &\vdots\\ x_n &=\lambda x_n \end{align*} It seems like we need to do some inspection here, it would be hard (or at least I can't see how) to show this formally.  It seems to me like there are two possible linear independant eigenvectors here, $$\displaystyle \vec 0$$ and $$\displaystyle (1,0,...,0)$$ So the geometric multiplicity is $1$?",,['linear-algebra']
51,What is known about merely-orthogonal matrices?,What is known about merely-orthogonal matrices?,,"I'm interested in square matrices whose columns are orthogonal, but not necessarily orthonormal, non-zero vectors. Answers to other questions on this topic have noted that such matrices do not have an agreed name and that the natural name of ""orthogonal matrix"" means a matrix with orthonormal columns. So I'm going to use the name ""merely-orthogonal"" for these matrices and ""orthonormal"" for those matrices where $Q^TQ=I$ , avoiding ""orthogonal matrix"" entirely. The answer to one question usefully notes that if $M$ is merely-orthogonal then there exists an invertible diagonal matrix $D$ and an orthonormal matrix $Q$ such that $M=QD$ . The elements of $D$ are easily found as the norms of each column. This also immediately implies that $$M^TM = (QD)^TQD = D^TQ^TQD = D^T D = D^2$$ which is diagonal and has non-zeros on the diagonal. And hence, $M$ is invertible with $$M^{-1} = D^{-2}M^T$$ It seems like merely-orthogonal matrices should form a group under multiplication and that there should be an analog to the $QR$ decomposition (call it the $MR$ decomposition) where for any matrix $A$ , $A = MR$ with $M$ merely-orthogonal and $R$ upper triangular. It seems to me that the $MR$ decomposition would not require the underlying field to be algebraically closed, like the $QR$ decomposition does but would be computable over the rational numbers. So my questions are: Has anyone studied merely-orthogonal matrices and established these or other results? Am I right that merely-orthogonal matrices form a group? Am I right that $MR$ decompositions can be done over the rationals?","I'm interested in square matrices whose columns are orthogonal, but not necessarily orthonormal, non-zero vectors. Answers to other questions on this topic have noted that such matrices do not have an agreed name and that the natural name of ""orthogonal matrix"" means a matrix with orthonormal columns. So I'm going to use the name ""merely-orthogonal"" for these matrices and ""orthonormal"" for those matrices where , avoiding ""orthogonal matrix"" entirely. The answer to one question usefully notes that if is merely-orthogonal then there exists an invertible diagonal matrix and an orthonormal matrix such that . The elements of are easily found as the norms of each column. This also immediately implies that which is diagonal and has non-zeros on the diagonal. And hence, is invertible with It seems like merely-orthogonal matrices should form a group under multiplication and that there should be an analog to the decomposition (call it the decomposition) where for any matrix , with merely-orthogonal and upper triangular. It seems to me that the decomposition would not require the underlying field to be algebraically closed, like the decomposition does but would be computable over the rational numbers. So my questions are: Has anyone studied merely-orthogonal matrices and established these or other results? Am I right that merely-orthogonal matrices form a group? Am I right that decompositions can be done over the rationals?",Q^TQ=I M D Q M=QD D M^TM = (QD)^TQD = D^TQ^TQD = D^T D = D^2 M M^{-1} = D^{-2}M^T QR MR A A = MR M R MR QR MR,"['linear-algebra', 'orthogonal-matrices']"
52,Why is this true of matrices? Linearly dependent rows make linearly dependent columns,Why is this true of matrices? Linearly dependent rows make linearly dependent columns,,"Here’s a cool math thing about matrices that I don’t understand why it’s true: If one row of the matrix is a linear combination of other rows, then one of the columns will be a linear combination of other columns. For example: $$ \begin{bmatrix} 3 & 7 & 2 \\ 1 & 2 & 3 \\ 4 & 9 & 5 \end{bmatrix} $$ The third row is just the first row plus the second row. The third column is 17 times the first column -7 times the second column. (I had to use Wolfram Alpha to solve that.) This is just one example, but it’s always true. Should I just accept that this is the case, or can anyone provide insight into why this is the case?","Here’s a cool math thing about matrices that I don’t understand why it’s true: If one row of the matrix is a linear combination of other rows, then one of the columns will be a linear combination of other columns. For example: The third row is just the first row plus the second row. The third column is 17 times the first column -7 times the second column. (I had to use Wolfram Alpha to solve that.) This is just one example, but it’s always true. Should I just accept that this is the case, or can anyone provide insight into why this is the case?"," \begin{bmatrix}
3 & 7 & 2 \\
1 & 2 & 3 \\
4 & 9 & 5
\end{bmatrix} ","['linear-algebra', 'matrices']"
53,False proof: Every linear operator (matrix) has an eigenvalue.,False proof: Every linear operator (matrix) has an eigenvalue.,,"Below is a proof that any linear operator must have an eigenvalue. The proof obviously contains a mistake because the statement is wrong. But I do not see the mistake. Please point it out if you see it. Assumption: $V$ is a finitely dimensional $K$ -linear space ( $K$ is a field with infinitely many elements, e.g. $\mathbb R$ ), $V \ne \{0\}$ , $L:V \to V$ is a linear operator. Claim: $L$ must have an eigenvalue $\lambda \in K$ , i.e. there must be $\lambda \in K$ and $0 \ne v \in V$ such that $L(v)=\lambda v$ . Proof: Assume that $L$ does not have an eigenvalue. Then for all $\lambda \in K$ it should hold: $L-\lambda I$ is bijective ( $I$ is the identity operator). So we have an infinite family of bijective linear operators $S:=(L-\lambda I)_{\lambda \in K}$ in the linear space $H$ of linear operators $V\to V$ that is finitely dimensional ( $\dim H = (\dim V)^2$ ). $S$ is a linear independent family. Indeed, let $a_1,\ldots,a_n \in K$ such that $0=\sum_{i=1}^n a_i (L-\lambda_i I)=(\sum_{i=1}^n a_i)L - (\sum_{i=1}^n a_i\lambda_i) I$ . If not all $a_i$ are $0$ , the $S \ni L - \frac{ \sum_{i=1}^n a_i\lambda_i}{\sum_{i=1}^n a_i} I=0$ . Therefore $S$ contains a zero operator. This contradicts the statement above that all elements in $S$ are bijective and $V \ne \{0\}$ . So we have found an infinite linearly independent family $S$ in a finitely dimensional linear space $H$ . That is a contradiction. This $L$ has an eigenvalue. Addition : $H$ is finitely dimensional Proof: fix a basis of $V$ say $e_1,\ldots,e_m$ . Consider linear operator $L_{i,j}$ for which hold $L(e_i)=e_j$ and $L(e_k)=0$ for $k \ne i$ . Than $H$ is panned by the finite family $(L_{i,j})_{j,i = 1,\ldots m}$","Below is a proof that any linear operator must have an eigenvalue. The proof obviously contains a mistake because the statement is wrong. But I do not see the mistake. Please point it out if you see it. Assumption: is a finitely dimensional -linear space ( is a field with infinitely many elements, e.g. ), , is a linear operator. Claim: must have an eigenvalue , i.e. there must be and such that . Proof: Assume that does not have an eigenvalue. Then for all it should hold: is bijective ( is the identity operator). So we have an infinite family of bijective linear operators in the linear space of linear operators that is finitely dimensional ( ). is a linear independent family. Indeed, let such that . If not all are , the . Therefore contains a zero operator. This contradicts the statement above that all elements in are bijective and . So we have found an infinite linearly independent family in a finitely dimensional linear space . That is a contradiction. This has an eigenvalue. Addition : is finitely dimensional Proof: fix a basis of say . Consider linear operator for which hold and for . Than is panned by the finite family","V K K \mathbb R V \ne \{0\} L:V \to V L \lambda \in K \lambda \in K 0 \ne v \in V L(v)=\lambda v L \lambda \in K L-\lambda I I S:=(L-\lambda I)_{\lambda \in K} H V\to V \dim H = (\dim V)^2 S a_1,\ldots,a_n \in K 0=\sum_{i=1}^n a_i (L-\lambda_i I)=(\sum_{i=1}^n a_i)L - (\sum_{i=1}^n a_i\lambda_i) I a_i 0 S \ni L - \frac{ \sum_{i=1}^n a_i\lambda_i}{\sum_{i=1}^n a_i} I=0 S S V \ne \{0\} S H L H V e_1,\ldots,e_m L_{i,j} L(e_i)=e_j L(e_k)=0 k \ne i H (L_{i,j})_{j,i = 1,\ldots m}","['linear-algebra', 'eigenvalues-eigenvectors', 'fake-proofs']"
54,Characterization of basis in terms of universal property: axiom of choice,Characterization of basis in terms of universal property: axiom of choice,,"I wonder if the proof of the following statement requires the axiom of choice: (Characterization of basis in terms of universal property) Let $V$ be a vector space, and let $S$ be a non-empty subset of $U$ . Show that $S$ is a basis for $V$ if and only if for every vector space $W$ and every function $f : S → W$ , there exists a unique linear transformation $\tilde{f} : V → W$ such that $\tilde{f}(x) = f(x)$ for all $x \in S$ . The proof of the 'if' direction given in this answer certainly uses the axiom of choice, specifically this part: On the other hand, suppose $E$ is linearly independent but not spanning. $\require{color} \colorbox{yellow}{Extend $E$ to a basis $E'$}$ , with $x\in E'\setminus E$ . Any $f:E\to Y$ extends to distinct functions $f_0,f_1:E'\to Y$ defined by $f_0(e)=f_1(e)=f(e)$ for $e\in E$ , $f_0(e)=f_1(e)=0$ for $e\in E'\setminus (E\cup\{x\})$ , and $f_0(x)=0$ , $f_1(x)=1$ . Any linear extension of either $f_1$ or of $f_2$ will be a linear extension of $f$ . By the forward direction you know that each $f_1$ and $f_2$ have linear extensions. Since $f_1\ne f_2$ , these will be distinct linear extensions of $f$ . I don't see any way to circumvent it. However, there is an obvious argument using Yoneda lemma that seemingly does not use the axiom of choice: Let $F,G: \operatorname{Set}\rightleftarrows \operatorname{Vec}_k$ be the free-forgetful adjunction.The universal property implies that $\operatorname{Set}(S,G(W))\cong \operatorname{Vec}_k(V,W)$ for every vector space $W$ naturally in $W$ (naturally follows from uniqueness of $\tilde{f}$ ). On the other hand, $\operatorname{Set}(S,G(W))\cong \operatorname{Vec}_k(F(S),W)$ . Thus, the representable functors $\operatorname{Vec}_k(V,-)$ and $\operatorname{Vec}_k(F(S),-)$ are naturally isomorphic and hence $V\cong F(S)$ by a corollary of Yoneda lemma and it follows that $S$ is a basis of $V$ . I would like to know whether the proof of the statement requires axiom of choice and if yes, when does the above argument uses the axiom of choice, and if no, how to modify the first proof to get around it.","I wonder if the proof of the following statement requires the axiom of choice: (Characterization of basis in terms of universal property) Let be a vector space, and let be a non-empty subset of . Show that is a basis for if and only if for every vector space and every function , there exists a unique linear transformation such that for all . The proof of the 'if' direction given in this answer certainly uses the axiom of choice, specifically this part: On the other hand, suppose is linearly independent but not spanning. , with . Any extends to distinct functions defined by for , for , and , . Any linear extension of either or of will be a linear extension of . By the forward direction you know that each and have linear extensions. Since , these will be distinct linear extensions of . I don't see any way to circumvent it. However, there is an obvious argument using Yoneda lemma that seemingly does not use the axiom of choice: Let be the free-forgetful adjunction.The universal property implies that for every vector space naturally in (naturally follows from uniqueness of ). On the other hand, . Thus, the representable functors and are naturally isomorphic and hence by a corollary of Yoneda lemma and it follows that is a basis of . I would like to know whether the proof of the statement requires axiom of choice and if yes, when does the above argument uses the axiom of choice, and if no, how to modify the first proof to get around it.","V S U S V W f : S → W \tilde{f} : V → W \tilde{f}(x) = f(x) x \in S E \require{color} \colorbox{yellow}{Extend E to a basis E'} x\in E'\setminus E f:E\to Y f_0,f_1:E'\to Y f_0(e)=f_1(e)=f(e) e\in E f_0(e)=f_1(e)=0 e\in E'\setminus (E\cup\{x\}) f_0(x)=0 f_1(x)=1 f_1 f_2 f f_1 f_2 f_1\ne f_2 f F,G: \operatorname{Set}\rightleftarrows \operatorname{Vec}_k \operatorname{Set}(S,G(W))\cong \operatorname{Vec}_k(V,W) W W \tilde{f} \operatorname{Set}(S,G(W))\cong \operatorname{Vec}_k(F(S),W) \operatorname{Vec}_k(V,-) \operatorname{Vec}_k(F(S),-) V\cong F(S) S V","['linear-algebra', 'category-theory', 'set-theory', 'axiom-of-choice', 'universal-property']"
55,Computationally efficient way to compute projection matrix,Computationally efficient way to compute projection matrix,,Is there some computationally efficient way of computing $(A^\top A)^{-1}$ ? I would like to know this cause I want to compute the projection matrix $$ P = A(A^\top A)^{-1} A^\top $$ efficiently.,Is there some computationally efficient way of computing ? I would like to know this cause I want to compute the projection matrix efficiently.,"(A^\top A)^{-1} 
P = A(A^\top A)^{-1} A^\top
","['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'projection-matrices']"
56,"If a matrix $A \in \mathbb{R}^{N\times N}$ is both row and column diagonally dominant, will it satisfy $(x^{2p-1})^T A x \geq 0, p \geq 1$?","If a matrix  is both row and column diagonally dominant, will it satisfy ?","A \in \mathbb{R}^{N\times N} (x^{2p-1})^T A x \geq 0, p \geq 1","If a matrix $A = \{a_{i,j}\} \in \mathbb{R}^{N\times N}$ is both  row and column diagonally dominant with non-negative diagonal entries, i.e. $a_{i,i} \geq  0$ , $\forall i = 1, \cdots, N$ $a_{i,i} \geq \sum_{j = 1,\cdots, N; j\neq i} |a_{i,j}|$ , $\forall i = 1, \cdots, N$ $a_{i,i} \geq \sum_{l = 1,\cdots, N; l\neq i} |a_{l, i}|$ , $\forall i = 1, \cdots, N$ will it satisfy $x^T A x \geq 0, \forall \mathbf{x} \in \mathbb{R}^N$ ? EDIT True, answered by Minus One-Twelfth $(\mathbf{x}^{(2p-1)})^T A \mathbf{x} \geq 0$ , where $p \geq 2$ is an interger? EDIT : $\mathbf x^{2p-1} = [x_1^{2p-1}, x_2^{2p-1}, \cdots, x_N^{2p-1}]^T$ . Thank you very much! I wrote a short matlab code to verify this: N = 5; for i = 1:100000     A = 2*rand(N, N) - 1; % random value in [-1, 1]     rowsum = sum(abs(A), 2) - abs(diag(A));     columnsum = sum(abs(A), 1)' - abs(diag(A));     v = max(rowsum, columnsum);     A = A - diag(diag(A)) + diag(v); % column/row diagonally dominant     xv = 4*rand(N, 100000) - 2; % random vector in [-2, 2]     p = 1;     minvalue = min(dot((xv.^(2*p-1)),  A * xv))     if minvalue < 0         fprintf('wrong!\n');         pause;     end end","If a matrix is both  row and column diagonally dominant with non-negative diagonal entries, i.e. , , , will it satisfy ? EDIT True, answered by Minus One-Twelfth , where is an interger? EDIT : . Thank you very much! I wrote a short matlab code to verify this: N = 5; for i = 1:100000     A = 2*rand(N, N) - 1; % random value in [-1, 1]     rowsum = sum(abs(A), 2) - abs(diag(A));     columnsum = sum(abs(A), 1)' - abs(diag(A));     v = max(rowsum, columnsum);     A = A - diag(diag(A)) + diag(v); % column/row diagonally dominant     xv = 4*rand(N, 100000) - 2; % random vector in [-2, 2]     p = 1;     minvalue = min(dot((xv.^(2*p-1)),  A * xv))     if minvalue < 0         fprintf('wrong!\n');         pause;     end end","A = \{a_{i,j}\} \in \mathbb{R}^{N\times N} a_{i,i} \geq  0 \forall i = 1, \cdots, N a_{i,i} \geq \sum_{j = 1,\cdots, N; j\neq i} |a_{i,j}| \forall i = 1, \cdots, N a_{i,i} \geq \sum_{l = 1,\cdots, N; l\neq i} |a_{l, i}| \forall i = 1, \cdots, N x^T A x \geq 0, \forall \mathbf{x} \in \mathbb{R}^N (\mathbf{x}^{(2p-1)})^T A \mathbf{x} \geq 0 p \geq 2 \mathbf x^{2p-1} = [x_1^{2p-1}, x_2^{2p-1}, \cdots, x_N^{2p-1}]^T","['linear-algebra', 'inequality', 'numerical-linear-algebra', 'quadratic-forms', 'positive-definite']"
57,An $n\times n$ matrix that has exactly one $1$ and one $-1$ in each row and column and others are $0$,An  matrix that has exactly one  and one  in each row and column and others are,n\times n 1 -1 0,I came across the following Question Assume an $n\times n$ matrix that has exactly one $1$ and one $-1$ in each row and column and others are $0$ . Prove that there is a way that we can change the places of rows and columns in which it gives the negative of the matrix. MY TRY- Call such matrix A. All we need to do is to find some permutation matrices $P_{1}$ and $P_{2}$ such that $$P_{1}AP_{2} = -A$$ $A$ can be written as a difference of two permutation matrices i.e. $$A = P-Q$$ where P and Q are some permutation matrices Example of one such matrix of order $3\times3$ $$ \begin{pmatrix} 1 & 0 & -1 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{pmatrix} = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\  0 & 0 & 1 \end{pmatrix}-\begin{pmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\  0 & 1 & 0 \end{pmatrix}.$$ We could first turn every such matrix $A$ by multiplying by appropriate permutation matrices to the form $I-R$ :- $$P^{T}A = P^{T}(P-Q) = I-R$$ Clearly the permutation matrix R shouldn't have $1$ at the same position as in $I$ . R lies in the class of traceless permutation matrices . Now If we are able to find matrices permutation $P_{1}$ and $P_{2}$ such that $$P_{1}(I-R)P_{2} = (R-I) = -(I-R)$$ we'll have $$P_{1}P^{T}AP_{2} = -P^{T}A \implies PP_{1}P^{T}AP_{2} = -A $$ and we would be done. But how could I proceed now to find $P_{1}$ and $P_{2}$ ? Would we need some extra equation from the fact that $R$ is a traceless permutation matrix? It was great to see other approaches to solve the problem by Michael Hoppe and user1551. But I am curious to see how would it be if we go this way?,I came across the following Question Assume an matrix that has exactly one and one in each row and column and others are . Prove that there is a way that we can change the places of rows and columns in which it gives the negative of the matrix. MY TRY- Call such matrix A. All we need to do is to find some permutation matrices and such that can be written as a difference of two permutation matrices i.e. where P and Q are some permutation matrices Example of one such matrix of order We could first turn every such matrix by multiplying by appropriate permutation matrices to the form :- Clearly the permutation matrix R shouldn't have at the same position as in . R lies in the class of traceless permutation matrices . Now If we are able to find matrices permutation and such that we'll have and we would be done. But how could I proceed now to find and ? Would we need some extra equation from the fact that is a traceless permutation matrix? It was great to see other approaches to solve the problem by Michael Hoppe and user1551. But I am curious to see how would it be if we go this way?,"n\times n 1 -1 0 P_{1} P_{2} P_{1}AP_{2} = -A A A = P-Q 3\times3  \begin{pmatrix}
1 & 0 & -1 \\
-1 & 1 & 0 \\
0 & -1 & 1
\end{pmatrix} = \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
 0 & 0 & 1
\end{pmatrix}-\begin{pmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
 0 & 1 & 0
\end{pmatrix}. A I-R P^{T}A = P^{T}(P-Q) = I-R 1 I P_{1} P_{2} P_{1}(I-R)P_{2} = (R-I) = -(I-R) P_{1}P^{T}AP_{2} = -P^{T}A \implies PP_{1}P^{T}AP_{2} = -A  P_{1} P_{2} R","['linear-algebra', 'matrices', 'permutations', 'trace', 'permutation-matrices']"
58,Positive-definiteness of matrix with $|a_{ij}| \leq \frac{1}{ij}$,Positive-definiteness of matrix with,|a_{ij}| \leq \frac{1}{ij},"Let $A$ be a $N\times N$ -matrix with elements $$ a_{ii}=1 \quad\text{and}\quad a_{ij} = \frac{1}{ij}  \quad\text{for}~ i\neq j. $$ Then $A$ is positive-definite, as can be easily seen from $$ x^T A x = \sum_i x_i^2 + \sum_{i \neq j} \frac{x_i x_j}{ij}  \geq \sum_i \frac{x_i^2}{i^2} + \sum_{i \neq j} \frac{x_i x_j}{ij}  = \left(\sum_i \frac{x_i}{i}\right)^2 \geq 0. $$ Assume now that $A$ is a real symmetric $N\times N$ -matrix with elements $$ \tag{1} a_{ii}=1 \quad\text{and}\quad |a_{ij}| \leq \frac{1}{ij}  \quad\text{for}~ i\neq j. $$ Is it possible to show that $A$ is also positive-definite (or positive-semidefinite)? Here I posted a refinement of this question by assuming $0 \leq a_{ij} \leq \frac{1}{ij}$ for $i \neq j$ in $(1)$ .","Let be a -matrix with elements Then is positive-definite, as can be easily seen from Assume now that is a real symmetric -matrix with elements Is it possible to show that is also positive-definite (or positive-semidefinite)? Here I posted a refinement of this question by assuming for in .","A N\times N 
a_{ii}=1 \quad\text{and}\quad
a_{ij} = \frac{1}{ij}  \quad\text{for}~ i\neq j.
 A 
x^T A x = \sum_i x_i^2 + \sum_{i \neq j} \frac{x_i x_j}{ij} 
\geq
\sum_i \frac{x_i^2}{i^2} + \sum_{i \neq j} \frac{x_i x_j}{ij} 
=
\left(\sum_i \frac{x_i}{i}\right)^2 \geq 0.
 A N\times N 
\tag{1}
a_{ii}=1 \quad\text{and}\quad
|a_{ij}| \leq \frac{1}{ij}  \quad\text{for}~ i\neq j.
 A 0 \leq a_{ij} \leq \frac{1}{ij} i \neq j (1)","['linear-algebra', 'matrices', 'positive-definite', 'positive-semidefinite']"
59,"If $\alpha=\sqrt[3]{2}$ and $p,q,r\in\mathbb{Q}$ then show $p+q\alpha+r\alpha^2$ is a subfield of $\mathbb{C}$",If  and  then show  is a subfield of,"\alpha=\sqrt[3]{2} p,q,r\in\mathbb{Q} p+q\alpha+r\alpha^2 \mathbb{C}","If $\alpha=\sqrt[3]{2}$ and $p,q,r\in\mathbb{Q}$ then show $p+q\alpha+r\alpha^2$ is a subfield of $\mathbb{C}$ . For context, this is number $5$ in Chapter $1$ of Ian Stewart's Galois Theory. At this point in the text, we have only learned how to solve cubics and quartics, while introducing subring and subfield language. First to show $$R=\{p+q\alpha+r\alpha^2: p,q,r\in\mathbb{Q} \wedge \alpha=\sqrt[3]{2}\}$$ is a subfield  we show $R$ is a subring of $\mathbb{C}$ and then finish by showing $\forall x\in R,  \exists x^{-1}\in R$ . Note that clearly $R\subset\mathbb{C}$ since $p+q\alpha+r\alpha^2$ is a real number for all rational $p,q,r.$ Take $p=1,q=0,r=0$ to see $1\in R$ . If $p_1+q_1\alpha+r_1\alpha^2\in R$ and $p_2+q_2\alpha+r_2\alpha^2\in R$ , then $$\left(p_1+q_1\alpha+r_1\alpha^2\right)+\left(p_2+q_2\alpha+r_2\alpha^2\right)=\left(p_1+p_2\right)+\left(q_1+q_2\right)\alpha+\left(r_1+r_2\right)\alpha^2\in R$$ $$-(p_1+q_1\alpha+r_1\alpha^2)=-p_2-q_2\alpha-r_2\alpha^2\in R$$ $$\left(p_1+q_1\alpha+r_1\alpha^2\right)\left(p_2+q_2\alpha+r_2\alpha^2\right)$$ $$=\left(p_1p_2+2q_1r_2+2q_2r_1\right)+\left(p_1q_2+p_2q_1+2r_1r_2\right)\alpha+\left(p_1r_2+q_1q_2+p_2r_1\right)\alpha^2\in R$$ The preceding argument follows from the facts that the rationals are closed under addition and multiplication. The above also shows $R$ is a subring of $\mathbb{C}$ . To complete the proof that $R$ is a subfield , we find an expression for the inverse $$(p_1+q_1\alpha+r_1\alpha^2)^{-1}$$ Here is where I run into issues. My first thought was to set the product $\left(p_1+q_1\alpha+r_1\alpha^2\right)\left(p_2+q_2\alpha+r_2\alpha^2\right)$ equal to $1$ : $$\left(p_1+q_1\alpha+r_1\alpha^2\right)\left(p_2+q_2\alpha+r_2\alpha^2\right)=1\implies$$ $$p_1p_2+2q_1r_2+2q_2r_1=1$$ $$p_1q_2+p_2q_1+2r_1r_2=0$$ $$p_1r_2+q_1q_2+p_2r_1=0$$ which is equivalent to $$\begin{pmatrix} p_2&2r_2&2q_2\\ q_2&p_2&2r_2\\r_2&q_2&p_2\end{pmatrix}\begin{pmatrix}p_1\\q_1\\r_1\end{pmatrix}=\begin{pmatrix}1\\0\\0\end{pmatrix}$$ If I could find an explicit inverse for the above $3\times3$ matrix, the problem would be solved yielding exact expressions for $p_1,q_1, r_1$ in terms of $p_2,q_2,r_2$ . However, I am not seeing a way to guarantee the determinant is nonzero for as long as $p_2,q_2,r_2\neq0$ . I noticed the matrix is Toeplitz, but I don't know if that tells us anything about invertibility. Any help with finding the inverse element here without resorting to high power machinery in these answers Describe the subfields of $\mathbb{C}$ of the form: $\mathbb{Q}(\alpha)$ where $\alpha$ is the real cube root of $2$. and How to show that $\mathbb{Q}(\alpha) = \left\{ p+q\alpha+r\alpha^2 \mid p, q, r\in \mathbb{Q} \right\}$, where $\alpha$ is the real cube root of $2$? is much appreciated.","If and then show is a subfield of . For context, this is number in Chapter of Ian Stewart's Galois Theory. At this point in the text, we have only learned how to solve cubics and quartics, while introducing subring and subfield language. First to show is a subfield  we show is a subring of and then finish by showing . Note that clearly since is a real number for all rational Take to see . If and , then The preceding argument follows from the facts that the rationals are closed under addition and multiplication. The above also shows is a subring of . To complete the proof that is a subfield , we find an expression for the inverse Here is where I run into issues. My first thought was to set the product equal to : which is equivalent to If I could find an explicit inverse for the above matrix, the problem would be solved yielding exact expressions for in terms of . However, I am not seeing a way to guarantee the determinant is nonzero for as long as . I noticed the matrix is Toeplitz, but I don't know if that tells us anything about invertibility. Any help with finding the inverse element here without resorting to high power machinery in these answers Describe the subfields of $\mathbb{C}$ of the form: $\mathbb{Q}(\alpha)$ where $\alpha$ is the real cube root of $2$. and How to show that $\mathbb{Q}(\alpha) = \left\{ p+q\alpha+r\alpha^2 \mid p, q, r\in \mathbb{Q} \right\}$, where $\alpha$ is the real cube root of $2$? is much appreciated.","\alpha=\sqrt[3]{2} p,q,r\in\mathbb{Q} p+q\alpha+r\alpha^2 \mathbb{C} 5 1 R=\{p+q\alpha+r\alpha^2: p,q,r\in\mathbb{Q} \wedge \alpha=\sqrt[3]{2}\} R \mathbb{C} \forall x\in R,  \exists x^{-1}\in R R\subset\mathbb{C} p+q\alpha+r\alpha^2 p,q,r. p=1,q=0,r=0 1\in R p_1+q_1\alpha+r_1\alpha^2\in R p_2+q_2\alpha+r_2\alpha^2\in R \left(p_1+q_1\alpha+r_1\alpha^2\right)+\left(p_2+q_2\alpha+r_2\alpha^2\right)=\left(p_1+p_2\right)+\left(q_1+q_2\right)\alpha+\left(r_1+r_2\right)\alpha^2\in R -(p_1+q_1\alpha+r_1\alpha^2)=-p_2-q_2\alpha-r_2\alpha^2\in R \left(p_1+q_1\alpha+r_1\alpha^2\right)\left(p_2+q_2\alpha+r_2\alpha^2\right) =\left(p_1p_2+2q_1r_2+2q_2r_1\right)+\left(p_1q_2+p_2q_1+2r_1r_2\right)\alpha+\left(p_1r_2+q_1q_2+p_2r_1\right)\alpha^2\in R R \mathbb{C} R (p_1+q_1\alpha+r_1\alpha^2)^{-1} \left(p_1+q_1\alpha+r_1\alpha^2\right)\left(p_2+q_2\alpha+r_2\alpha^2\right) 1 \left(p_1+q_1\alpha+r_1\alpha^2\right)\left(p_2+q_2\alpha+r_2\alpha^2\right)=1\implies p_1p_2+2q_1r_2+2q_2r_1=1 p_1q_2+p_2q_1+2r_1r_2=0 p_1r_2+q_1q_2+p_2r_1=0 \begin{pmatrix}
p_2&2r_2&2q_2\\ q_2&p_2&2r_2\\r_2&q_2&p_2\end{pmatrix}\begin{pmatrix}p_1\\q_1\\r_1\end{pmatrix}=\begin{pmatrix}1\\0\\0\end{pmatrix} 3\times3 p_1,q_1, r_1 p_2,q_2,r_2 p_2,q_2,r_2\neq0","['linear-algebra', 'abstract-algebra']"
60,"Languages, students, interpretation of $AA^{\tau}\;\&\;A^{\tau}A$","Languages, students, interpretation of",AA^{\tau}\;\&\;A^{\tau}A,"Student $A$ speaks French & German, student $B$ speaks English, French   & Italian, student $C$ speaks English, Italian & Spanish and student $D$ speaks all the languages mentioned except French. Write a matrix   such that rows represent those $4$ students, while columns represent   the languages they speak. If a person $i$ speaks a language $j$ , set $a_{ij}$ =1, otherwise set $a_{ij}=0$ . Interpret the meaning of the   matrices $AA^{\tau}\;\&\;A^{\tau}A$ . My attempt: columns are: $E,F,G,I\;\&\;S$ One of my dilemmas is if the symmetry of $AA^{\tau}\;\&\;A^{\tau}A$ is relevant here and how to use what I've gotten: $$A=\begin{bmatrix}0&1&1&0&0\\1&1&0&1&0\\1&0&0&1&1\\1&0&1&1&1\end{bmatrix},A^{\tau}=\begin{bmatrix}0&1&1&1\\1&1&0&0\\1&0&0&1\\0&1&1&1\\0&0&1&1\end{bmatrix}$$ $$AA^{\tau}=\begin{bmatrix}2&1&0&1\\1&3&2&2\\0&2&3&3\\1&2&3&4\end{bmatrix},\;A^{\tau}A=\begin{bmatrix}3&1&1&3&2\\1&2&1&1&0\\1&1&2&1&1\\3&1&1&3&2\\2&0&1&2&2\end{bmatrix}$$ I've already read posts and articles on the topic of Gramian matrix , but we haven't covered it this semester yet. I translated this from Croatian word by word and thought the interpretation should be some kind of a relation or number of combinations I'm not able to see at the moment. I'm not sure if this task explicitly involves the $\text{Gramian matrix}$ . How to interpret the given matrices in the context of students and languages they speak?","Student speaks French & German, student speaks English, French   & Italian, student speaks English, Italian & Spanish and student speaks all the languages mentioned except French. Write a matrix   such that rows represent those students, while columns represent   the languages they speak. If a person speaks a language , set =1, otherwise set . Interpret the meaning of the   matrices . My attempt: columns are: One of my dilemmas is if the symmetry of is relevant here and how to use what I've gotten: I've already read posts and articles on the topic of Gramian matrix , but we haven't covered it this semester yet. I translated this from Croatian word by word and thought the interpretation should be some kind of a relation or number of combinations I'm not able to see at the moment. I'm not sure if this task explicitly involves the . How to interpret the given matrices in the context of students and languages they speak?","A B C D 4 i j a_{ij} a_{ij}=0 AA^{\tau}\;\&\;A^{\tau}A E,F,G,I\;\&\;S AA^{\tau}\;\&\;A^{\tau}A A=\begin{bmatrix}0&1&1&0&0\\1&1&0&1&0\\1&0&0&1&1\\1&0&1&1&1\end{bmatrix},A^{\tau}=\begin{bmatrix}0&1&1&1\\1&1&0&0\\1&0&0&1\\0&1&1&1\\0&0&1&1\end{bmatrix} AA^{\tau}=\begin{bmatrix}2&1&0&1\\1&3&2&2\\0&2&3&3\\1&2&3&4\end{bmatrix},\;A^{\tau}A=\begin{bmatrix}3&1&1&3&2\\1&2&1&1&0\\1&1&2&1&1\\3&1&1&3&2\\2&0&1&2&2\end{bmatrix} \text{Gramian matrix}","['linear-algebra', 'matrices']"
61,Prove that matrix $A\in \mathbb{R}^{n \times n}$ is invertible if $A^T = p(A)$,Prove that matrix  is invertible if,A\in \mathbb{R}^{n \times n} A^T = p(A),I have to prove that a matrix $A\in \mathbb{R}^{n \times n}$ is invertible if $A^T = p(A)$ where $p(A)$ is a polynomial with non-zero last coefficient. I've tried to use that if $A^T=p(A)$ then $A=p(A^T)$ and to look at $$AA^T = (a_0E+a_1A^T+...+a_m(A^T)^m)(a_0E+a_1A+...+a_mA^m)$$ and somehow to get the expression like $cE=A(...)$ or $cE=A^T(...)$ which will prove what I need but failed.,I have to prove that a matrix is invertible if where is a polynomial with non-zero last coefficient. I've tried to use that if then and to look at and somehow to get the expression like or which will prove what I need but failed.,A\in \mathbb{R}^{n \times n} A^T = p(A) p(A) A^T=p(A) A=p(A^T) AA^T = (a_0E+a_1A^T+...+a_m(A^T)^m)(a_0E+a_1A+...+a_mA^m) cE=A(...) cE=A^T(...),"['linear-algebra', 'matrices', 'transpose']"
62,"Why are singular values of ""complex"" matrices always real and non-negative?","Why are singular values of ""complex"" matrices always real and non-negative?",,"I've already read the following related questions on math.SE: Why can't singular values be complex numbers? Clarification on the SVD of a complex matrix Why are singular values always non-negative? Convention on non-negative singular values? The conclusion they seem to agree on is the following: For $A \in \mathbb{R}^{m\times n} $ , its singular values are real non-negative . I do not see, however, how can this be the case for $A \in \mathbb{C}^{m\times n}$ , even though in most answers, people say it is the same as for real matrices. Usual argument for singular values of $A  \in \mathbb{R}^{m\times n} $ being real non-negative: The SVD of $A$ is: $$ A = U S V^T $$ We have $$ B = A^T A = V S^T U^T U S V^T = V S^T S V^T $$ where $S^T S$ is diagonal with elements $\sigma_i^2$ , where $\sigma_i$ are the singular values of $A$ . Now, $\sigma_i^2$ are real-nonegative because they can be seen as the eigenvalues $\lambda_i=\sigma_i^2$ of the symmetric, positive-definite, matrix $B = V \Lambda V^T$ (i.e., $\Lambda = S^T S$ ). Using $\lambda_i=\sigma_i^2 \ge 0$ to solve for $\sigma_i$ , we find that: $\sigma_i$ is real because if it were complex, then the only way $\sigma_i^2$ would be real is if $\sigma_i$ is real or pure imaginary. However, in the latter case we get $\sigma_i^2$ negative, which contradicts $\sigma_i^2 \ge 0$ . $\sigma_i$ is the square-root of $\lambda_i$ , which can be positive or negative. By convention, however, we take the positive square-root. Hence, $$ \sigma_i \text{ are real-nonnegative themselves.} $$ A try for a similar argument for singular values of $A  \in \mathbb{C}^{m\times n} $ : The SVD of $A$ is: $$ A = U S V^H $$ We have $$ B = A^H A = V S^H U^H U S V^H = V S^H S V^H $$ where $S^H S$ is diagonal with elements $|\sigma_i|^2$ , where $\sigma_i$ are the singular values of $A$ . Now, $|\sigma_i|^2$ are real-nonnegative because of the modulus square, and they can also be seen as the eigenvalues $\lambda_i=|\sigma_i|^2$ of the Hermitian, positive-definite, matrix $B = V \Lambda V^H$ (i.e., $\Lambda = S^H S$ ). Using $\lambda_i=|\sigma_i|^2 \ge 0$ , how can one solve for $\sigma_i$ and prove it is real-nonnegative ? Particularly, what prevents $\sigma_i$ from being complex?","I've already read the following related questions on math.SE: Why can't singular values be complex numbers? Clarification on the SVD of a complex matrix Why are singular values always non-negative? Convention on non-negative singular values? The conclusion they seem to agree on is the following: For , its singular values are real non-negative . I do not see, however, how can this be the case for , even though in most answers, people say it is the same as for real matrices. Usual argument for singular values of being real non-negative: The SVD of is: We have where is diagonal with elements , where are the singular values of . Now, are real-nonegative because they can be seen as the eigenvalues of the symmetric, positive-definite, matrix (i.e., ). Using to solve for , we find that: is real because if it were complex, then the only way would be real is if is real or pure imaginary. However, in the latter case we get negative, which contradicts . is the square-root of , which can be positive or negative. By convention, however, we take the positive square-root. Hence, A try for a similar argument for singular values of : The SVD of is: We have where is diagonal with elements , where are the singular values of . Now, are real-nonnegative because of the modulus square, and they can also be seen as the eigenvalues of the Hermitian, positive-definite, matrix (i.e., ). Using , how can one solve for and prove it is real-nonnegative ? Particularly, what prevents from being complex?","A \in \mathbb{R}^{m\times n}  A \in \mathbb{C}^{m\times n} A  \in \mathbb{R}^{m\times n}  A 
A = U S V^T
 
B = A^T A = V S^T U^T U S V^T = V S^T S V^T
 S^T S \sigma_i^2 \sigma_i A \sigma_i^2 \lambda_i=\sigma_i^2 B = V \Lambda V^T \Lambda = S^T S \lambda_i=\sigma_i^2 \ge 0 \sigma_i \sigma_i \sigma_i^2 \sigma_i \sigma_i^2 \sigma_i^2 \ge 0 \sigma_i \lambda_i 
\sigma_i \text{ are real-nonnegative themselves.}
 A  \in \mathbb{C}^{m\times n}  A 
A = U S V^H
 
B = A^H A = V S^H U^H U S V^H = V S^H S V^H
 S^H S |\sigma_i|^2 \sigma_i A |\sigma_i|^2 \lambda_i=|\sigma_i|^2 B = V \Lambda V^H \Lambda = S^H S \lambda_i=|\sigma_i|^2 \ge 0 \sigma_i \sigma_i","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd']"
63,Invariant Polynomials under Rotations,Invariant Polynomials under Rotations,,"Consider a real polynomial $P(x, y)$ in two variables. It is called invariant with respect to the rotation by an angle $\alpha$ if $$ P (x \cos(\alpha) − y \sin(\alpha), x \sin(\alpha) + y \cos(\alpha)) = P (x, y) $$ for all real $x$ and $y$ .  How do we find the dimension of the real vector space formed by all polynomials in  two variables of total degree not greater than $d$ invariant with respect to the  rotation by $2\pi/n$ ?",Consider a real polynomial in two variables. It is called invariant with respect to the rotation by an angle if for all real and .  How do we find the dimension of the real vector space formed by all polynomials in  two variables of total degree not greater than invariant with respect to the  rotation by ?,"P(x, y) \alpha 
P (x \cos(\alpha) − y \sin(\alpha), x \sin(\alpha) + y \cos(\alpha)) = P (x, y)
 x y d 2\pi/n","['linear-algebra', 'combinatorics', 'vector-spaces', 'rotations', 'group-actions']"
64,how to find the eigenvalues of permutation matrices?,how to find the eigenvalues of permutation matrices?,,I searched online but didnt find a clear explanation to how can I find the eigenvalues and eigenvectors of permutation matrices. can someone help? for example: $$     \begin{pmatrix}     0 & 0 & 0 & 1 & 0 \\     0 & 0 & 1 & 0 & 0 \\     0 & 0 & 0 & 0 & 1 \\     1 & 0 & 0 & 0 & 0\\     0 & 1 & 0 & 0 & 0\\     \end{pmatrix}. $$,I searched online but didnt find a clear explanation to how can I find the eigenvalues and eigenvectors of permutation matrices. can someone help? for example:,"
    \begin{pmatrix}
    0 & 0 & 0 & 1 & 0 \\
    0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 & 1 \\
    1 & 0 & 0 & 0 & 0\\
    0 & 1 & 0 & 0 & 0\\
    \end{pmatrix}.
","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
65,How to find $\lim\limits_{n\to\infty} A^n$?,How to find ?,\lim\limits_{n\to\infty} A^n,"Given $4\times4$ matrix, $$A= \begin{bmatrix} 0.4&0.3&0.2&0.1\\ 0.1&0.4&0.3&0.2\\ 0.3&0.2&0.1&0.4\\ 0.2&0.1&0.4&0.3 \end{bmatrix} .$$ Can we prove that $$\lim\limits_{n\to\infty} A^n= \begin{bmatrix} 0.25&0.25&0.25&0.25\\ 0.25&0.25&0.25&0.25\\ 0.25&0.25&0.25&0.25\\ 0.25&0.25&0.25&0.25 \end{bmatrix}$$ manually? I tried to find $A^2$ , and $A^3$ for a long long time. Any ways to find $\lim\limits_{n\to\infty} A^n$ ?","Given matrix, Can we prove that manually? I tried to find , and for a long long time. Any ways to find ?","4\times4 A=
\begin{bmatrix}
0.4&0.3&0.2&0.1\\
0.1&0.4&0.3&0.2\\
0.3&0.2&0.1&0.4\\
0.2&0.1&0.4&0.3
\end{bmatrix}
. \lim\limits_{n\to\infty} A^n=
\begin{bmatrix}
0.25&0.25&0.25&0.25\\
0.25&0.25&0.25&0.25\\
0.25&0.25&0.25&0.25\\
0.25&0.25&0.25&0.25
\end{bmatrix} A^2 A^3 \lim\limits_{n\to\infty} A^n","['linear-algebra', 'matrices']"
66,$L^2$ norm of a matrix: Is this statement true?,norm of a matrix: Is this statement true?,L^2,"I am following Nocedal and Wright's Numerical Optimization book for self study. In the Appendix section of the book, the following matrix norms are defined: They defined the $l_2$ norm of the matrix $A$ as the largest eigenvalue of $(A^TA)^{1/2}$ . But I have also seen the following definition: $||A||_2 =\max_{i:n} \sqrt\lambda_i$ where $\lambda_i$ is the i. eigenvalue of the matrix $A^TA$ . (source: http://www.maths.lth.se/na/courses/FMN081/FMN081-06/lecture6.pdf ) I am not sure how these two definitions are equal. $A^TA$ is a symmetric positive definite matrix, hence it has positive eigenvalues. Assume that $\lambda_i$ is its largest eigenvalue. $A^TA$ has a unique positive definite square root with the eigenvalues $\sqrt{\lambda_i}$ . Considering only this PD square root matrix, Nocedal's definition is correct. But there can be other square root matrices of $A^TA$ as well, for which different eigenvalues are the largest. And if $A^TA$ has repeating eigenvalues, it will have infinitely many square roots. Hence I think there is an ambiguity in the Nocedal's definition. Am I missing something here? How can be the book's definition correct?","I am following Nocedal and Wright's Numerical Optimization book for self study. In the Appendix section of the book, the following matrix norms are defined: They defined the norm of the matrix as the largest eigenvalue of . But I have also seen the following definition: where is the i. eigenvalue of the matrix . (source: http://www.maths.lth.se/na/courses/FMN081/FMN081-06/lecture6.pdf ) I am not sure how these two definitions are equal. is a symmetric positive definite matrix, hence it has positive eigenvalues. Assume that is its largest eigenvalue. has a unique positive definite square root with the eigenvalues . Considering only this PD square root matrix, Nocedal's definition is correct. But there can be other square root matrices of as well, for which different eigenvalues are the largest. And if has repeating eigenvalues, it will have infinitely many square roots. Hence I think there is an ambiguity in the Nocedal's definition. Am I missing something here? How can be the book's definition correct?",l_2 A (A^TA)^{1/2} ||A||_2 =\max_{i:n} \sqrt\lambda_i \lambda_i A^TA A^TA \lambda_i A^TA \sqrt{\lambda_i} A^TA A^TA,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms', 'spectral-norm']"
67,Find $\operatorname{Tr}(A^{2018})$ if $\det(A^2-2018I_2)=0$,Find  if,\operatorname{Tr}(A^{2018}) \det(A^2-2018I_2)=0,"Find $\operatorname{Tr}(A^{2018})$ if $\det(A^2-2018I_2)=0, A\in M_2(\mathbb{Q})$ My attempt: Let $A^2=\begin{bmatrix}a&b\\c&d\end{bmatrix}$ then $B=A^2-2018I_2=\begin{bmatrix}a-2018&b\\c&d-2018\end{bmatrix}$ then $\det(B)=(a-2018)(d-2018)-bc$ then we have: $a = 2018$ or $d=2018$ AND $b=0$ or $c=0$. And I took the possible candidate for $A^2$ when all of these happen at the same time so: $a=2018,d=2018,b=0,c=0\implies \operatorname{Tr}(A^{2018})=2\times2018^{1009}$ by induction. Did I do everything right? I feel like my solution is not really that good. Also can you give me some advice that might help me in these kind of situation with problems like this?","Find $\operatorname{Tr}(A^{2018})$ if $\det(A^2-2018I_2)=0, A\in M_2(\mathbb{Q})$ My attempt: Let $A^2=\begin{bmatrix}a&b\\c&d\end{bmatrix}$ then $B=A^2-2018I_2=\begin{bmatrix}a-2018&b\\c&d-2018\end{bmatrix}$ then $\det(B)=(a-2018)(d-2018)-bc$ then we have: $a = 2018$ or $d=2018$ AND $b=0$ or $c=0$. And I took the possible candidate for $A^2$ when all of these happen at the same time so: $a=2018,d=2018,b=0,c=0\implies \operatorname{Tr}(A^{2018})=2\times2018^{1009}$ by induction. Did I do everything right? I feel like my solution is not really that good. Also can you give me some advice that might help me in these kind of situation with problems like this?",,"['linear-algebra', 'matrices']"
68,Why does the inner product require the base field be $\mathbb{R}$ or $\mathbb{C}$?,Why does the inner product require the base field be  or ?,\mathbb{R} \mathbb{C},"I've often wondered why it seems inner products only ever use $\mathbb{R}$ or $\mathbb{C}$ for the base field of their vector space, or sub-fields like the algebraic or construct-able numbers. According to Wikipedia, these are the only fields we can use which have the right properties. Why? Why not use other, stranger, more complicated fields? What breaks down if we do so, and is there a meaningful generalization? What does it say about the inner product to have these restrictions?","I've often wondered why it seems inner products only ever use $\mathbb{R}$ or $\mathbb{C}$ for the base field of their vector space, or sub-fields like the algebraic or construct-able numbers. According to Wikipedia, these are the only fields we can use which have the right properties. Why? Why not use other, stranger, more complicated fields? What breaks down if we do so, and is there a meaningful generalization? What does it say about the inner product to have these restrictions?",,"['linear-algebra', 'vector-spaces', 'field-theory', 'inner-products']"
69,"How to show a rational polynomial is irreducible in $\mathbb{Q}[a,b,c]$?",How to show a rational polynomial is irreducible in ?,"\mathbb{Q}[a,b,c]","How to show that a rational polynomial is irreducible in $\mathbb{Q}[a,b,c]$? For example, I try to show this polynomial $$p(a,b,c)=a(a+c)(a+b)+b(b+c)(b+a)+c(c+a)(c+b)-4(a+b)(a+c)(b+c)(*)$$ is irreducible, where $a,b,c\in \mathbb{Q}$. The related problem is Ask for the rational roots of $\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}=4.$ . Could I consider the points $(*)$ intersect with $L_{\infty}$ are three? $L_{\infty}$ is the infinity line in a projective space $\mathbb{C}P^2$.","How to show that a rational polynomial is irreducible in $\mathbb{Q}[a,b,c]$? For example, I try to show this polynomial $$p(a,b,c)=a(a+c)(a+b)+b(b+c)(b+a)+c(c+a)(c+b)-4(a+b)(a+c)(b+c)(*)$$ is irreducible, where $a,b,c\in \mathbb{Q}$. The related problem is Ask for the rational roots of $\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}=4.$ . Could I consider the points $(*)$ intersect with $L_{\infty}$ are three? $L_{\infty}$ is the infinity line in a projective space $\mathbb{C}P^2$.",,['linear-algebra']
70,Calculating a determinant.,Calculating a determinant.,,"$D_n$=\begin{vmatrix} a & 0 & 0 & \cdots  &0&0& n-1 \\ 0 & a & 0 & \cdots &0&0& n-2\\ 0 & 0 & a & \ddots &0&0& n-3 \\ \vdots & \vdots & \ddots & \ddots & \ddots&\vdots&\vdots \\ \vdots & \vdots & \vdots & \ddots & \ddots& 0&2 \\ 0 & \cdots & \cdots & \cdots &0&a&1  \\ n-1 & n-2 & n-3 & \cdots & 2 & 1& a\\ \end{vmatrix} I tried getting the eigenvalues for A = \begin{vmatrix} 0 & 0 & 0 & \cdots  &0&0& n-1 \\ 0 & 0 & 0 & \cdots &0&0& n-2\\ 0 & 0 & 0  & \ddots &0&0& n-3 \\ \vdots & \vdots & \ddots & \ddots & \ddots&\vdots&\vdots \\ \vdots & \vdots & \vdots & \ddots & \ddots& 0&2 \\ 0 & \cdots & \cdots & \cdots &0&0&1  \\ n-1 & n-2 & n-3 & \cdots & 2 & 1& 0\\ \end{vmatrix} For $a=0$  , the rank of the matrix is $2$ , hence $\dim(\ker(A)) = n-2 $ $m(0)>=n-2$ However, I was not able to determine the other eigenvalues. Testing for different values of n : for $n=2$ : $D_2 = a^2-1$ for $n=3$ : $D_3 = a^3 -5a$ $D_n$ seems to be equal to $a^n - a^{n-2}\sum_{i=1}^{n-1}i^2$ . However I'm aware that testing for different values of $n$ is not enough to generalize the formula. Thanks in advance.","$D_n$=\begin{vmatrix} a & 0 & 0 & \cdots  &0&0& n-1 \\ 0 & a & 0 & \cdots &0&0& n-2\\ 0 & 0 & a & \ddots &0&0& n-3 \\ \vdots & \vdots & \ddots & \ddots & \ddots&\vdots&\vdots \\ \vdots & \vdots & \vdots & \ddots & \ddots& 0&2 \\ 0 & \cdots & \cdots & \cdots &0&a&1  \\ n-1 & n-2 & n-3 & \cdots & 2 & 1& a\\ \end{vmatrix} I tried getting the eigenvalues for A = \begin{vmatrix} 0 & 0 & 0 & \cdots  &0&0& n-1 \\ 0 & 0 & 0 & \cdots &0&0& n-2\\ 0 & 0 & 0  & \ddots &0&0& n-3 \\ \vdots & \vdots & \ddots & \ddots & \ddots&\vdots&\vdots \\ \vdots & \vdots & \vdots & \ddots & \ddots& 0&2 \\ 0 & \cdots & \cdots & \cdots &0&0&1  \\ n-1 & n-2 & n-3 & \cdots & 2 & 1& 0\\ \end{vmatrix} For $a=0$  , the rank of the matrix is $2$ , hence $\dim(\ker(A)) = n-2 $ $m(0)>=n-2$ However, I was not able to determine the other eigenvalues. Testing for different values of n : for $n=2$ : $D_2 = a^2-1$ for $n=3$ : $D_3 = a^3 -5a$ $D_n$ seems to be equal to $a^n - a^{n-2}\sum_{i=1}^{n-1}i^2$ . However I'm aware that testing for different values of $n$ is not enough to generalize the formula. Thanks in advance.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
71,Evaluate a determinant with $\sin$,Evaluate a determinant with,\sin,"Evaluate $$\begin{vmatrix} \sin x_1 & \sin x_2 & \dots & \sin x_n \\ \sin 2x_1 & \sin 2x_2 & \dots & \sin 2x_n \\ \vdots & \vdots & & \vdots \\ \sin nx_1 & \sin nx_2 & \dots & \sin nx_n \end{vmatrix}$$   where $x_1,x_2,\dots,x_n \in \mathbb{R}$. I tried to somehow expand $\sin kx_i$ in terms of $\sin x_i$ and $\cos x_i$, but things got really complicated and I couldn't go further. Also, I tried substracting the first column from the others, but nothing came out of it.","Evaluate $$\begin{vmatrix} \sin x_1 & \sin x_2 & \dots & \sin x_n \\ \sin 2x_1 & \sin 2x_2 & \dots & \sin 2x_n \\ \vdots & \vdots & & \vdots \\ \sin nx_1 & \sin nx_2 & \dots & \sin nx_n \end{vmatrix}$$   where $x_1,x_2,\dots,x_n \in \mathbb{R}$. I tried to somehow expand $\sin kx_i$ in terms of $\sin x_i$ and $\cos x_i$, but things got really complicated and I couldn't go further. Also, I tried substracting the first column from the others, but nothing came out of it.",,"['linear-algebra', 'matrices', 'determinant']"
72,Is there any relationship between 'invertible' and 'diagonalizable'? [duplicate],Is there any relationship between 'invertible' and 'diagonalizable'? [duplicate],,"This question already has an answer here : Is there any connection between a matrix being invertible and being diagonalizable? (1 answer) Closed 6 years ago . I'am a linear algebra beginner and I am confusing with invertible and diagonalizable. From my understanding, invertible means non-singular and any of eigenvalue must not be 0. Diagonalizable means there must be N linearly independent eigenvectors.(Eventhough eigenvalue has 0, it seems possible to have N linearly independent eigenvectors. Right?) Above are just single fraction of their property, and I cannot imagine bigger picture than this. Is there any intuitive relation or theorem between 'invertible' and 'diagonalizable'?","This question already has an answer here : Is there any connection between a matrix being invertible and being diagonalizable? (1 answer) Closed 6 years ago . I'am a linear algebra beginner and I am confusing with invertible and diagonalizable. From my understanding, invertible means non-singular and any of eigenvalue must not be 0. Diagonalizable means there must be N linearly independent eigenvectors.(Eventhough eigenvalue has 0, it seems possible to have N linearly independent eigenvectors. Right?) Above are just single fraction of their property, and I cannot imagine bigger picture than this. Is there any intuitive relation or theorem between 'invertible' and 'diagonalizable'?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'inverse', 'diagonalization']"
73,"Eigenvalues of a product of matrices, involving Moore-Penrose pseudo inverse","Eigenvalues of a product of matrices, involving Moore-Penrose pseudo inverse",,"I have recently came across the following question in my linear algebra research, which I suspect has a basic straightforward answer: Suppose we have a real square matrix $ A $, and a certain real square matrix $ B $, which is not necessarily invertible. If $ B $ is invertible, then the matrices $ A $ and $ BAB^{-1} $ and $ B^{-1}AB $ have the same eigenvalues due to similarity. However, if $ B $ is not invertible, then what about the matrices $ BAB^{\dagger} $ and $ B^{\dagger}AB $  ($ \dagger $ denotes Moore-Penrose pseudoinverse) Do they share eigenvalues with $ A $? Is there an analogy here? I really have no ability to answer this question, I am not an expert on the pseudo inverse of matrices, so I am hoping someone here can provide the answer, which I suspect exists. I thank all helpers.","I have recently came across the following question in my linear algebra research, which I suspect has a basic straightforward answer: Suppose we have a real square matrix $ A $, and a certain real square matrix $ B $, which is not necessarily invertible. If $ B $ is invertible, then the matrices $ A $ and $ BAB^{-1} $ and $ B^{-1}AB $ have the same eigenvalues due to similarity. However, if $ B $ is not invertible, then what about the matrices $ BAB^{\dagger} $ and $ B^{\dagger}AB $  ($ \dagger $ denotes Moore-Penrose pseudoinverse) Do they share eigenvalues with $ A $? Is there an analogy here? I really have no ability to answer this question, I am not an expert on the pseudo inverse of matrices, so I am hoping someone here can provide the answer, which I suspect exists. I thank all helpers.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse', 'pseudoinverse']"
74,A $3 \times 3$ matrix with one eigenvalue and one eigenvector?,A  matrix with one eigenvalue and one eigenvector?,3 \times 3,"Suppose we have to construct a $3 \times 3$ matrix with only one eigenvalue which has only one linearly independent eigenvector, what should be our approach? I was asked this in an interview, so first thing that came on my mind was to look for a matrix with $\lambda=2$ on the diagonal such that char poly comes out to be $(\lambda-2)^3=0$. I was going for something like $A=\begin{bmatrix}2&0&0\\0&2&1\\1&0&2\end{bmatrix}$. It has only one eigenvalue i.e. $\lambda =2$ but when I find $A-\lambda I= \begin{bmatrix}0&0&0\\0&0&1\\1&0&0\end{bmatrix}$ it's only element in null space is zero. And I end up with nothing. I was interrupted in between and asked another question? Was I completely in a wrong direction?","Suppose we have to construct a $3 \times 3$ matrix with only one eigenvalue which has only one linearly independent eigenvector, what should be our approach? I was asked this in an interview, so first thing that came on my mind was to look for a matrix with $\lambda=2$ on the diagonal such that char poly comes out to be $(\lambda-2)^3=0$. I was going for something like $A=\begin{bmatrix}2&0&0\\0&2&1\\1&0&2\end{bmatrix}$. It has only one eigenvalue i.e. $\lambda =2$ but when I find $A-\lambda I= \begin{bmatrix}0&0&0\\0&0&1\\1&0&0\end{bmatrix}$ it's only element in null space is zero. And I end up with nothing. I was interrupted in between and asked another question? Was I completely in a wrong direction?",,['linear-algebra']
75,Square root of Matrix $A=\begin{bmatrix} 1 &2 \\ 3&4 \end{bmatrix}$,Square root of Matrix,A=\begin{bmatrix} 1 &2 \\ 3&4 \end{bmatrix},Find Square root of Matrix $A=\begin{bmatrix} 1 &2 \\   3&4  \end{bmatrix}$ without using concept of Eigen Values and Eigen Vectors I assumed its square root as $$B=\begin{bmatrix} a &b \\   c&d  \end{bmatrix}$$ hence $$B^2=A$$ $\implies$ $$\begin{bmatrix} a^2+bc &b(a+d) \\   c(a+d)&d^2+bc  \end{bmatrix}=\begin{bmatrix} 1 &2 \\   3&4  \end{bmatrix}$$ So $$a^2+bc=1 \tag{1}$$ $$b(a+d)=2 \tag{2}$$ $$c(a+d)=3 \tag{3}$$ $$d^2+bc=4 \tag{4}$$  From $(2)$ and $(3)$ we get $$\frac{b}{c}=\frac{2}{3}$$ Let $b=2k $ and $c=3k$ Then $$a^2=1-6k^2$$ and $$d^2=4-6k^2$$  So $$B=\begin{bmatrix} \sqrt{1-6k^2} &2k\\   3k&\sqrt{4-6k^2}  \end{bmatrix}$$ So $$B^2=\begin{bmatrix} 1 &2k \left(\sqrt{1-6k^2}+\sqrt{4-6k^2}\right) \\   3k \left(\sqrt{1-6k^2}+\sqrt{4-6k^2}\right)&4  \end{bmatrix}=\begin{bmatrix} 1 &2 \\   3&4  \end{bmatrix}$$ Now we have to solve for $k$ using equation $$ \left(\sqrt{1-6k^2}+\sqrt{4-6k^2}\right)=\frac{1}{k} \tag{5}$$ Also $$\left(\sqrt{1-6k^2}+\sqrt{4-6k^2}\right) \times \left(\sqrt{4-6k^2}-\sqrt{1-6k^2}\right)=3$$ So from $(5)$ $$\left(\sqrt{4-6k^2}-\sqrt{1-6k^2}\right)=3k \tag{6}$$ Subtracting $(6)$ from $(5)$ we get $$2\sqrt{1-6k^2}=\frac{1}{k}-3k$$  Squaring Both sides we get $$33k^4-10k^2+1=0$$ we get $$k^2=\frac{5 \pm i \sqrt{8}}{33}$$ From this we get $k$. is there any other simpler way to find ?,Find Square root of Matrix $A=\begin{bmatrix} 1 &2 \\   3&4  \end{bmatrix}$ without using concept of Eigen Values and Eigen Vectors I assumed its square root as $$B=\begin{bmatrix} a &b \\   c&d  \end{bmatrix}$$ hence $$B^2=A$$ $\implies$ $$\begin{bmatrix} a^2+bc &b(a+d) \\   c(a+d)&d^2+bc  \end{bmatrix}=\begin{bmatrix} 1 &2 \\   3&4  \end{bmatrix}$$ So $$a^2+bc=1 \tag{1}$$ $$b(a+d)=2 \tag{2}$$ $$c(a+d)=3 \tag{3}$$ $$d^2+bc=4 \tag{4}$$  From $(2)$ and $(3)$ we get $$\frac{b}{c}=\frac{2}{3}$$ Let $b=2k $ and $c=3k$ Then $$a^2=1-6k^2$$ and $$d^2=4-6k^2$$  So $$B=\begin{bmatrix} \sqrt{1-6k^2} &2k\\   3k&\sqrt{4-6k^2}  \end{bmatrix}$$ So $$B^2=\begin{bmatrix} 1 &2k \left(\sqrt{1-6k^2}+\sqrt{4-6k^2}\right) \\   3k \left(\sqrt{1-6k^2}+\sqrt{4-6k^2}\right)&4  \end{bmatrix}=\begin{bmatrix} 1 &2 \\   3&4  \end{bmatrix}$$ Now we have to solve for $k$ using equation $$ \left(\sqrt{1-6k^2}+\sqrt{4-6k^2}\right)=\frac{1}{k} \tag{5}$$ Also $$\left(\sqrt{1-6k^2}+\sqrt{4-6k^2}\right) \times \left(\sqrt{4-6k^2}-\sqrt{1-6k^2}\right)=3$$ So from $(5)$ $$\left(\sqrt{4-6k^2}-\sqrt{1-6k^2}\right)=3k \tag{6}$$ Subtracting $(6)$ from $(5)$ we get $$2\sqrt{1-6k^2}=\frac{1}{k}-3k$$  Squaring Both sides we get $$33k^4-10k^2+1=0$$ we get $$k^2=\frac{5 \pm i \sqrt{8}}{33}$$ From this we get $k$. is there any other simpler way to find ?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'radicals']"
76,Is there a ring isomorphism between $M_n(D)$ and $M_m(D)$ where $n\neq m$ and $D$ is a division ring?,Is there a ring isomorphism between  and  where  and  is a division ring?,M_n(D) M_m(D) n\neq m D,Is there a ring isomorphism between $M_n(D)$ and $M_m(D)$ where $n\neq m$ and $D$ is a division ring? I know that this is impossible if we talk about left $D$-vector space homomorphisms (because of dimension over $D$ must be the same). What if we think about just a ring isomorphism?,Is there a ring isomorphism between $M_n(D)$ and $M_m(D)$ where $n\neq m$ and $D$ is a division ring? I know that this is impossible if we talk about left $D$-vector space homomorphisms (because of dimension over $D$ must be the same). What if we think about just a ring isomorphism?,,"['linear-algebra', 'abstract-algebra', 'ring-theory']"
77,"If $A \in M_3(\mathbb C)$ and $\operatorname{tr}(A)=\operatorname{tr}(A^{-1})=0$ and $\det(A)=1$, then $A^3=I_3$","If  and  and , then",A \in M_3(\mathbb C) \operatorname{tr}(A)=\operatorname{tr}(A^{-1})=0 \det(A)=1 A^3=I_3,Assume that we have a $3\times 3$ matrix like $A$ in which the coordinates come from the set of complex numbers ($\mathbb C$). We know that $\operatorname{tr}(A)=\operatorname{tr}(A^{-1})=0$ and $\det(A)=1$ . Prove that $A^3=I_3$ . I don't know what's the relationship between $A^3$ and the trace !,Assume that we have a $3\times 3$ matrix like $A$ in which the coordinates come from the set of complex numbers ($\mathbb C$). We know that $\operatorname{tr}(A)=\operatorname{tr}(A^{-1})=0$ and $\det(A)=1$ . Prove that $A^3=I_3$ . I don't know what's the relationship between $A^3$ and the trace !,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace']"
78,Derivation of the Curl formula in cartesian coordinates.,Derivation of the Curl formula in cartesian coordinates.,,"By calculating the circulation per area of a vector field $$F(x,y,z) = F_x(x,y,z)\vec{x} + F_y(x,y,z)\vec{y} + F_z(x,y,z)\vec{z}$$ in a small rectangle around $(x_0, y_0, z_0)$ on the $xy$ plane, it can be shown the limit as the sides of the rectangle approach zero is $$\left(\frac{\partial F_y(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial y}\right)$$ The same calculation however is not that straightforward if the rectangle does not lie in the $xy$, $yz$, or $xz$ planes. Now if $\vec{n}$ is the normal of the plane, I thought that by performing a change of basis such that $\vec{n} \rightarrow \vec{z'} $ and by following the previous calculations we could show that the limit of the circulation per area is $$ \left(\frac{\partial F_{y'}(x'_0, y'_0, z'_0)}{\partial x'} - \frac{\partial F_{x'}(x_0, y_0, z_0)}{\partial y'}\right) $$ This is also the inner product of the curl of the vector field and the normal $\vec{n}$ As such the two should be equal: $$\left(\frac{\partial F_{y'}(x'_0, y'_0, z'_0)}{\partial x'} - \frac{\partial F_{x'}(x'_0, y'_0, z'_0)}{\partial y'}\right) = \\ \left[\left(\frac{\partial F_z(x_0, y_0, z_0)}{\partial y} - \frac{\partial F_y(x_0, y_0, z_0)}{\partial z} \right)\vec{x} + \left(\frac{\partial F_z(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial z} \right)\vec{y} +  \left(\frac{\partial F_y(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial y} \right)\vec{z}\right] \cdot \vec{n} $$ I've been trying to prove the above equality for some time without success, specifically I am not sure how to handle the transformations correctly. Any help with this is much appreciated!","By calculating the circulation per area of a vector field $$F(x,y,z) = F_x(x,y,z)\vec{x} + F_y(x,y,z)\vec{y} + F_z(x,y,z)\vec{z}$$ in a small rectangle around $(x_0, y_0, z_0)$ on the $xy$ plane, it can be shown the limit as the sides of the rectangle approach zero is $$\left(\frac{\partial F_y(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial y}\right)$$ The same calculation however is not that straightforward if the rectangle does not lie in the $xy$, $yz$, or $xz$ planes. Now if $\vec{n}$ is the normal of the plane, I thought that by performing a change of basis such that $\vec{n} \rightarrow \vec{z'} $ and by following the previous calculations we could show that the limit of the circulation per area is $$ \left(\frac{\partial F_{y'}(x'_0, y'_0, z'_0)}{\partial x'} - \frac{\partial F_{x'}(x_0, y_0, z_0)}{\partial y'}\right) $$ This is also the inner product of the curl of the vector field and the normal $\vec{n}$ As such the two should be equal: $$\left(\frac{\partial F_{y'}(x'_0, y'_0, z'_0)}{\partial x'} - \frac{\partial F_{x'}(x'_0, y'_0, z'_0)}{\partial y'}\right) = \\ \left[\left(\frac{\partial F_z(x_0, y_0, z_0)}{\partial y} - \frac{\partial F_y(x_0, y_0, z_0)}{\partial z} \right)\vec{x} + \left(\frac{\partial F_z(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial z} \right)\vec{y} +  \left(\frac{\partial F_y(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial y} \right)\vec{z}\right] \cdot \vec{n} $$ I've been trying to prove the above equality for some time without success, specifically I am not sure how to handle the transformations correctly. Any help with this is much appreciated!",,"['linear-algebra', 'multivariable-calculus', 'derivatives', 'differential-topology']"
79,Hamel basis for the vector space of real numbers over rational numbers and closedness of the basis under inversion,Hamel basis for the vector space of real numbers over rational numbers and closedness of the basis under inversion,,"Let $\mathcal B$ be a Hamel basis for $\mathbb R$ over $\mathbb Q$. Then is it true that for every $0\ne a \in \mathbb R , \exists y \in \mathcal B$ such that $\dfrac a y \notin \mathcal B$ ? If the answer to the above in general is no then can we take $a$ to be $1$, that is, Does there exist a Hamel basis $\mathcal B$ for $\mathbb R$  over $\mathbb Q$ such that $a\in \mathcal B \implies \dfrac 1a \in \mathcal B$ ? See Hamel basis for $\mathbb{R}$ over $\mathbb{Q}$ cannot be closed under scalar multiplication by $a \ne 0,1$","Let $\mathcal B$ be a Hamel basis for $\mathbb R$ over $\mathbb Q$. Then is it true that for every $0\ne a \in \mathbb R , \exists y \in \mathcal B$ such that $\dfrac a y \notin \mathcal B$ ? If the answer to the above in general is no then can we take $a$ to be $1$, that is, Does there exist a Hamel basis $\mathcal B$ for $\mathbb R$  over $\mathbb Q$ such that $a\in \mathcal B \implies \dfrac 1a \in \mathcal B$ ? See Hamel basis for $\mathbb{R}$ over $\mathbb{Q}$ cannot be closed under scalar multiplication by $a \ne 0,1$",,['linear-algebra']
80,"Two vector spaces with same dimension and same basis, are identical?","Two vector spaces with same dimension and same basis, are identical?",,"Let $V$ subspace of $W$ and both have same dimension and same basis. Then can we safely say that $V= W$ ? I believe yes. For example there may be an element $x \in V$ written as a linear combination of the basis elements. This linear combination is unique. Now let's take another vector $y \in W$. The $y$ is written as a linear combination of the basis vectors. We equate the two linear combinations, and since the basis elements are linearly independent, we get that $x=y$. Thus $V=W$. Do you agree?","Let $V$ subspace of $W$ and both have same dimension and same basis. Then can we safely say that $V= W$ ? I believe yes. For example there may be an element $x \in V$ written as a linear combination of the basis elements. This linear combination is unique. Now let's take another vector $y \in W$. The $y$ is written as a linear combination of the basis vectors. We equate the two linear combinations, and since the basis elements are linearly independent, we get that $x=y$. Thus $V=W$. Do you agree?",,"['linear-algebra', 'matrices', 'vector-spaces']"
81,Congruent matrices - why do we require invertiblility?,Congruent matrices - why do we require invertiblility?,,"If $A$, $B$ $\in K^{n \times n}$ are $n \times n$ matrices over a field $K$, then we say that $A$ and $B$ are congruent if there exists an invertible $P \in GL(n, K)$ such that $B = P^TAP$, where $P^T$ denotes the transpose of $P$. Why do we require $P$ to be invertible? Congruence of matrices is usually compared to similarity of matrices, where we say that $A$ and $B$ are similar if there exists an invertible $P \in GL(n, K)$ such that $B = P^{-1}AP$. Here it is obvious why we need $P$ to be invertible. However, in the case of congruent matrices, the invertibility requirement doesn't seem to be obvious.","If $A$, $B$ $\in K^{n \times n}$ are $n \times n$ matrices over a field $K$, then we say that $A$ and $B$ are congruent if there exists an invertible $P \in GL(n, K)$ such that $B = P^TAP$, where $P^T$ denotes the transpose of $P$. Why do we require $P$ to be invertible? Congruence of matrices is usually compared to similarity of matrices, where we say that $A$ and $B$ are similar if there exists an invertible $P \in GL(n, K)$ such that $B = P^{-1}AP$. Here it is obvious why we need $P$ to be invertible. However, in the case of congruent matrices, the invertibility requirement doesn't seem to be obvious.",,"['linear-algebra', 'matrices', 'inverse', 'matrix-congruences']"
82,Riesz Representation Theorem in Linear Algebra,Riesz Representation Theorem in Linear Algebra,,"Let $\mathbb{V}$ be a finite dimensional inner product space and $\alpha : \mathbb{V} \rightarrow \mathbb{R}$ a linear functional. Prove that there is a unique vector $\overrightarrow v_{0} \in \mathbb{V}$ such that $\alpha(\overrightarrow v)=\langle\overrightarrow v,\overrightarrow v_{0}\rangle$ for all $\overrightarrow v \in \mathbb{V}$. My approach : I suppose that there is exists another vector $\overrightarrow w_{0} \in \mathbb{V}$ that satisfies the same property. We get $\langle\overrightarrow v,\overrightarrow v_{0}-\overrightarrow w_{0}\rangle=0$ and I need to show that $\overrightarrow v_{0}=\overrightarrow w_{0}$ somehow. Any tips on how to do that? I tried taking an orthonormal basis for $\mathbb{V}$ but that didn't help in the end.","Let $\mathbb{V}$ be a finite dimensional inner product space and $\alpha : \mathbb{V} \rightarrow \mathbb{R}$ a linear functional. Prove that there is a unique vector $\overrightarrow v_{0} \in \mathbb{V}$ such that $\alpha(\overrightarrow v)=\langle\overrightarrow v,\overrightarrow v_{0}\rangle$ for all $\overrightarrow v \in \mathbb{V}$. My approach : I suppose that there is exists another vector $\overrightarrow w_{0} \in \mathbb{V}$ that satisfies the same property. We get $\langle\overrightarrow v,\overrightarrow v_{0}-\overrightarrow w_{0}\rangle=0$ and I need to show that $\overrightarrow v_{0}=\overrightarrow w_{0}$ somehow. Any tips on how to do that? I tried taking an orthonormal basis for $\mathbb{V}$ but that didn't help in the end.",,"['linear-algebra', 'riesz-representation-theorem']"
83,"On the definition of the volume form in general vector spaces as given in Spivak, Calculus on Manifolds","On the definition of the volume form in general vector spaces as given in Spivak, Calculus on Manifolds",,"For a vector space $V$ denote by $\Lambda^k(V)$ the space of alternating $k$-tensors, or alternating $k$-fold multilinear maps on $V$. I have some difficulty following the intention of the author in the part where he introduced the volume form of a finite-dimensional vector space $V$ (page 83): The fact that $\dim \Lambda^n(\mathbb R^n) = 1$ is probably not new to you, since $\det$ is often defined as the unique element $\omega \in \Lambda^n(\mathbb R^n)$ such that $\omega(e_1, \ldots, e_n) = 1$. For a general vector space $V$ there is no extra criterion of this sort to distinguish a particular $\omega \in \Lambda^n(V)$. Suppose, however, that an inner product $T$ for $V$ is given. If $v_1, \ldots, v_n$ and $w_1, \ldots, w_n$ are two bases which are orthonormal with respect to $T$, and the matrix $A = (a_{ij})$ is defined by $w_i = \sum_{j=1}^n a_{ij} v_j$, then   $$  \delta_{ij} = T(w_i, w_j) = \sum_{k,l = 1}^n a_{ik}a_{jl} T(v_k, v_j) = \sum_{k=1}^n a_{ik}a_{jk}. $$   In other words, if $A^T$ denotes the transpose of the matrix $A$, then we have $A \cdot A^T = I$, so $\det A = \pm 1$. It folows as $\omega(w_1,\ldots, w_n) = \det(A)\cdot \omega(v_1, \ldots, v_n)$ that if $\omega \in \Lambda^n(V)$ satisfies $\omega(v_1, \ldots, v_n) \in \{-1,1\}$, then $\omega(w_1, \ldots, w_n) \in \{-1,1\}$. If an orientation $\mu$ for $V$ has also been given, it follows that there is a unique $\omega \in \Lambda^n(V)$ such that $\omega(v_1, \ldots, v_n) = 1$ whenever $v_1, \ldots, v_n$ is an orthonormal basis such that $[v_1,\ldots, v_n] = \mu$. This unique $\omega$ is called the volume element of $V$. First why he claims that for a general (finite-dimensional) $V$ there is no extra criterion as for $\mathbb R^n$; if we choose some basis $v_1, \ldots, v_n$ of $V$ then the requirement $\omega(v_1, \ldots, v_n) = 1$ gives me a unique $n$-form (as the value on every permutation of the basis vectors is then also prescribed). Of course, this choice is relative to a basis, but that was the choice in $\mathbb R^n$ also. And the orientation is coded in the basis. So why all these complicated derivations using the given orientation and an inner product, when simply a basis is enough? Also if we have a basis of a finite-dimensional vector space, we also get an inner product by multiplying the coefficient w.r.t. this basis and summing. Of course the geometric meaning of such an inner product is questionable, as it no longer corresponds to angles, length and projections as in the $\mathbb R^n$ setting with the standard inner product if the $v_i$ are not orthonormal. But for mere existence of an inner product such that the basis is orthonormal w.r.t. it, it is enough. I can image that such definitions are not very meaningful from a geometrical point of view, if we take an arbitrary basis then built our volume form by the mere requirement that it takes the value one on this basis, we have no ready-to-use interpretation of it as an actual volume, as for it we need orthonormality and a notion of length, angle and so on already given by a prescribed inner product. (of course by the above construction of inner product an angle of two vectors w.r.t. to this constructed inner product corresponds to the ""real"" geometric angle if we view the coordinates as coordinates of the standard basis in $\mathbb R^n$). So I guess that what he might want to say is ""there is no sort to distinguish any particular form that has also any meaningful geometrical interpretation, as for example given by some inner product and orientation, after which we can talk about (oriented) length, areas, volumes"". But this is just a guess of mine; as how we are interested in ""real world interpretations"" in formal mathematics and its definition is here not clear to me. Maybe what he want to say is that the essential ingredients for a volume form (as derived from $\mathbb R^3$ and its interpretation of volume) is that we need an orthonormal basis (hence an inner product), which also fixes an orientation, such that a meaningful volume form could be introduced.","For a vector space $V$ denote by $\Lambda^k(V)$ the space of alternating $k$-tensors, or alternating $k$-fold multilinear maps on $V$. I have some difficulty following the intention of the author in the part where he introduced the volume form of a finite-dimensional vector space $V$ (page 83): The fact that $\dim \Lambda^n(\mathbb R^n) = 1$ is probably not new to you, since $\det$ is often defined as the unique element $\omega \in \Lambda^n(\mathbb R^n)$ such that $\omega(e_1, \ldots, e_n) = 1$. For a general vector space $V$ there is no extra criterion of this sort to distinguish a particular $\omega \in \Lambda^n(V)$. Suppose, however, that an inner product $T$ for $V$ is given. If $v_1, \ldots, v_n$ and $w_1, \ldots, w_n$ are two bases which are orthonormal with respect to $T$, and the matrix $A = (a_{ij})$ is defined by $w_i = \sum_{j=1}^n a_{ij} v_j$, then   $$  \delta_{ij} = T(w_i, w_j) = \sum_{k,l = 1}^n a_{ik}a_{jl} T(v_k, v_j) = \sum_{k=1}^n a_{ik}a_{jk}. $$   In other words, if $A^T$ denotes the transpose of the matrix $A$, then we have $A \cdot A^T = I$, so $\det A = \pm 1$. It folows as $\omega(w_1,\ldots, w_n) = \det(A)\cdot \omega(v_1, \ldots, v_n)$ that if $\omega \in \Lambda^n(V)$ satisfies $\omega(v_1, \ldots, v_n) \in \{-1,1\}$, then $\omega(w_1, \ldots, w_n) \in \{-1,1\}$. If an orientation $\mu$ for $V$ has also been given, it follows that there is a unique $\omega \in \Lambda^n(V)$ such that $\omega(v_1, \ldots, v_n) = 1$ whenever $v_1, \ldots, v_n$ is an orthonormal basis such that $[v_1,\ldots, v_n] = \mu$. This unique $\omega$ is called the volume element of $V$. First why he claims that for a general (finite-dimensional) $V$ there is no extra criterion as for $\mathbb R^n$; if we choose some basis $v_1, \ldots, v_n$ of $V$ then the requirement $\omega(v_1, \ldots, v_n) = 1$ gives me a unique $n$-form (as the value on every permutation of the basis vectors is then also prescribed). Of course, this choice is relative to a basis, but that was the choice in $\mathbb R^n$ also. And the orientation is coded in the basis. So why all these complicated derivations using the given orientation and an inner product, when simply a basis is enough? Also if we have a basis of a finite-dimensional vector space, we also get an inner product by multiplying the coefficient w.r.t. this basis and summing. Of course the geometric meaning of such an inner product is questionable, as it no longer corresponds to angles, length and projections as in the $\mathbb R^n$ setting with the standard inner product if the $v_i$ are not orthonormal. But for mere existence of an inner product such that the basis is orthonormal w.r.t. it, it is enough. I can image that such definitions are not very meaningful from a geometrical point of view, if we take an arbitrary basis then built our volume form by the mere requirement that it takes the value one on this basis, we have no ready-to-use interpretation of it as an actual volume, as for it we need orthonormality and a notion of length, angle and so on already given by a prescribed inner product. (of course by the above construction of inner product an angle of two vectors w.r.t. to this constructed inner product corresponds to the ""real"" geometric angle if we view the coordinates as coordinates of the standard basis in $\mathbb R^n$). So I guess that what he might want to say is ""there is no sort to distinguish any particular form that has also any meaningful geometrical interpretation, as for example given by some inner product and orientation, after which we can talk about (oriented) length, areas, volumes"". But this is just a guess of mine; as how we are interested in ""real world interpretations"" in formal mathematics and its definition is here not clear to me. Maybe what he want to say is that the essential ingredients for a volume form (as derived from $\mathbb R^3$ and its interpretation of volume) is that we need an orthonormal basis (hence an inner product), which also fixes an orientation, such that a meaningful volume form could be introduced.",,"['linear-algebra', 'differential-geometry', 'determinant', 'differential-forms']"
84,Vector spaces: Is (the) scalar multiplication unique?,Vector spaces: Is (the) scalar multiplication unique?,,"Notation Consider an arbitrary vector space $(V, \oplus, \odot)$ over a field $F$ with vector addition $\oplus : V \times V \to V$ and scalar multiplication $\odot : F \times V \to V$, both satisfying all the axioms defining a vector space. Background Let us fix the field $F$ and the set $V$. It is obvious that the vector addition does not have to be unique. Any binary operation $\oplus$ that makes $(V, \oplus)$ an Abelian group, would actually work. But I am not so sure if this is also true for (the) scalar multiplication. Question Let us fix the field $F$ and the Abelian group $(V, \oplus)$. Is the action of $F$ on $(V, \oplus)$, i.e., the scalar multiplication $\odot$ satisfying the axioms of a vector space, a unique operation?","Notation Consider an arbitrary vector space $(V, \oplus, \odot)$ over a field $F$ with vector addition $\oplus : V \times V \to V$ and scalar multiplication $\odot : F \times V \to V$, both satisfying all the axioms defining a vector space. Background Let us fix the field $F$ and the set $V$. It is obvious that the vector addition does not have to be unique. Any binary operation $\oplus$ that makes $(V, \oplus)$ an Abelian group, would actually work. But I am not so sure if this is also true for (the) scalar multiplication. Question Let us fix the field $F$ and the Abelian group $(V, \oplus)$. Is the action of $F$ on $(V, \oplus)$, i.e., the scalar multiplication $\odot$ satisfying the axioms of a vector space, a unique operation?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
85,"Proof of ""Singular values of a normal matrix are the absolute values of its eigenvalues""","Proof of ""Singular values of a normal matrix are the absolute values of its eigenvalues""",,"I want a simple proof of this fact using only definitions and basic facts. I've searched for it for some time and I couldn't find a satisfying proof. So I attempted to do it myself. Let $A \in \mathbb{C}^{n \times n}$ and $A^H$ be conjugate transpose of $A$. Let $A$ be normal, i.e. $A A^H = A^H A$. The singular values of $A$ are defined as $\sigma \in \mathbb{R}^{\geq 0}$ such that $$ \begin{align*} A v &= \sigma u \\ A^H u &= \sigma v \end{align*} $$ where $u^H u = v^H v = 1$. $u$ and $v$ are called left and right singular vectors respectively. Now multiplying the first equation with $A^H$ and the second equation with $A$ from the left we obtain $$ \begin{align*} A^H A v &= \sigma A^H u = \sigma^2 v \\ A A^H u &= \sigma A v = \sigma^2 u \end{align*} $$ Since $A$ is normal we obtain $$ \begin{align*} A A^H v &= \sigma^2 v \\ A A^H u &= \sigma^2 u \end{align*} $$ I believe we can conclude that either $u=v$ or $u=-v$ if singular values are distinct. So by the definition we can see that either $\lambda=\sigma$ or $\lambda=-\sigma$ is also an eigenvalue of $A$. So $\sigma = | \lambda |$. Edit: As @levap pointed out we can only conclude that $u = e^{i \theta} v, \theta \in [0, 2 \pi)$. Then we see that $$ \begin{align*} Av &= \sigma e^{i \theta} v \\ A^H v &= \sigma e^{-i \theta} v \end{align*} $$ So, we can say that $\lambda = \sigma e^{i \theta}$ is an eigenvalue of $A$. Also, by the lemma given below $\sigma^2 \geq 0$, so $\sigma \in \mathbb{R}$ and we can always select $\sigma \geq 0$ (using $-u$ in the definition if $\sigma \leq 0$). Therefore, we can conclude that $\sigma = |\lambda|$. Also, $v$ is an eigenvector of $A^H$ with $\bar{\lambda}$. Lemma. Eigenvalues of $AA^H$ are real and non-negative. Proof. $0 \leq \lVert A^H v \rVert_2 = v^H A A^H v = \sigma^2 v^H v = \sigma^2$. What happens if singular values are not distinct?","I want a simple proof of this fact using only definitions and basic facts. I've searched for it for some time and I couldn't find a satisfying proof. So I attempted to do it myself. Let $A \in \mathbb{C}^{n \times n}$ and $A^H$ be conjugate transpose of $A$. Let $A$ be normal, i.e. $A A^H = A^H A$. The singular values of $A$ are defined as $\sigma \in \mathbb{R}^{\geq 0}$ such that $$ \begin{align*} A v &= \sigma u \\ A^H u &= \sigma v \end{align*} $$ where $u^H u = v^H v = 1$. $u$ and $v$ are called left and right singular vectors respectively. Now multiplying the first equation with $A^H$ and the second equation with $A$ from the left we obtain $$ \begin{align*} A^H A v &= \sigma A^H u = \sigma^2 v \\ A A^H u &= \sigma A v = \sigma^2 u \end{align*} $$ Since $A$ is normal we obtain $$ \begin{align*} A A^H v &= \sigma^2 v \\ A A^H u &= \sigma^2 u \end{align*} $$ I believe we can conclude that either $u=v$ or $u=-v$ if singular values are distinct. So by the definition we can see that either $\lambda=\sigma$ or $\lambda=-\sigma$ is also an eigenvalue of $A$. So $\sigma = | \lambda |$. Edit: As @levap pointed out we can only conclude that $u = e^{i \theta} v, \theta \in [0, 2 \pi)$. Then we see that $$ \begin{align*} Av &= \sigma e^{i \theta} v \\ A^H v &= \sigma e^{-i \theta} v \end{align*} $$ So, we can say that $\lambda = \sigma e^{i \theta}$ is an eigenvalue of $A$. Also, by the lemma given below $\sigma^2 \geq 0$, so $\sigma \in \mathbb{R}$ and we can always select $\sigma \geq 0$ (using $-u$ in the definition if $\sigma \leq 0$). Therefore, we can conclude that $\sigma = |\lambda|$. Also, $v$ is an eigenvector of $A^H$ with $\bar{\lambda}$. Lemma. Eigenvalues of $AA^H$ are real and non-negative. Proof. $0 \leq \lVert A^H v \rVert_2 = v^H A A^H v = \sigma^2 v^H v = \sigma^2$. What happens if singular values are not distinct?",,"['linear-algebra', 'proof-verification', 'eigenvalues-eigenvectors', 'svd']"
86,Proof that an $n \times n$ real symmetric matrix has $n$ real eigenvalues [closed],Proof that an  real symmetric matrix has  real eigenvalues [closed],n \times n n,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question How to show that an $n \times n$ real symmetric matrix has $n$ real eigenvalues $$ \lambda_n \geqslant \dots \geqslant  \lambda_1 $$  with corresponding eigenvectors $\mathbf v_n, \dots,\mathbf v_1$ such that $$ \mathbf v_i^T \mathbf v_j = \delta_{ij}. $$ I am struggling a bit on how to start this proof. I tried looking online but could not find anything. Perhaps I am looking in the wrong direction, would it be the same to prove that such a matrix would have $n$ real orthogonal eigenvectors and thus $n$ real eigenvalues? Hints on where to start this proof would be appreciated.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question How to show that an $n \times n$ real symmetric matrix has $n$ real eigenvalues $$ \lambda_n \geqslant \dots \geqslant  \lambda_1 $$  with corresponding eigenvectors $\mathbf v_n, \dots,\mathbf v_1$ such that $$ \mathbf v_i^T \mathbf v_j = \delta_{ij}. $$ I am struggling a bit on how to start this proof. I tried looking online but could not find anything. Perhaps I am looking in the wrong direction, would it be the same to prove that such a matrix would have $n$ real orthogonal eigenvectors and thus $n$ real eigenvalues? Hints on where to start this proof would be appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
87,Any set with more elements than the dimension of vector space is linearly dependent,Any set with more elements than the dimension of vector space is linearly dependent,,"Let $V$ be a vector space with dimension $n$ such that $\{v_1,\cdots,v_n\}$ is its basis. Take $A\equiv\{a_1,\cdots,a_p\}\subset V$ with $p>n$. How do can I show that $A$ is linearly dependent without having to do all those summations . Is there an easier more direct proof than that? Thanks for helping!!!","Let $V$ be a vector space with dimension $n$ such that $\{v_1,\cdots,v_n\}$ is its basis. Take $A\equiv\{a_1,\cdots,a_p\}\subset V$ with $p>n$. How do can I show that $A$ is linearly dependent without having to do all those summations . Is there an easier more direct proof than that? Thanks for helping!!!",,['linear-algebra']
88,Equivalent definitions of an orthogonal matrix.,Equivalent definitions of an orthogonal matrix.,,"I wish to show that the following definitions of an $n \times n$ real matrix $Q$ are equivalent: $QQ^T=I$, $Qx\cdot Qx=x\cdot x$ for all $x\in \mathbb{R}^n$. I found it easy to show that $(1) \Rightarrow (2)$: Suppose that $QQ^T=I$. Then, we have that $$Qx\cdot Qx=(Qx)^TQx=x^TQ^TQx=x^TIx=x^Tx=x\cdot x.$$ However, I am unsure about how to show that $(2) \Rightarrow (1)$.","I wish to show that the following definitions of an $n \times n$ real matrix $Q$ are equivalent: $QQ^T=I$, $Qx\cdot Qx=x\cdot x$ for all $x\in \mathbb{R}^n$. I found it easy to show that $(1) \Rightarrow (2)$: Suppose that $QQ^T=I$. Then, we have that $$Qx\cdot Qx=(Qx)^TQx=x^TQ^TQx=x^TIx=x^Tx=x\cdot x.$$ However, I am unsure about how to show that $(2) \Rightarrow (1)$.",,['linear-algebra']
89,"Erwin Kreyszig's Introductory Functional Analysis With Applications, Section 2.9, Problem 12","Erwin Kreyszig's Introductory Functional Analysis With Applications, Section 2.9, Problem 12",,"If $f_1, \ldots, f_p$ are linear functionals on an $n$-dimensional vector space $X$, where $p<n$, then how to show that there is a vector $x \ne 0$ in $X$ such that $f_1(x) = 0, \ldots, f_p(x)=0$? What consequences does this result have with respect to linear equations? My feeling is that one of the consequences of this result with respect to linear equations is the following fact: Any system of $p$ homogeneous linear equations in $n$ unknowns, where $p < n$, must have a non-trivial solution. Am I right? And if so, then what other consequences does our result entail?","If $f_1, \ldots, f_p$ are linear functionals on an $n$-dimensional vector space $X$, where $p<n$, then how to show that there is a vector $x \ne 0$ in $X$ such that $f_1(x) = 0, \ldots, f_p(x)=0$? What consequences does this result have with respect to linear equations? My feeling is that one of the consequences of this result with respect to linear equations is the following fact: Any system of $p$ homogeneous linear equations in $n$ unknowns, where $p < n$, must have a non-trivial solution. Am I right? And if so, then what other consequences does our result entail?",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'systems-of-equations']"
90,Can the transpose of a matrix be expressed in row/column operations?,Can the transpose of a matrix be expressed in row/column operations?,,"Suppose that $A$ is a matrix, can we get its transpose, $A^T$, by performing row and/or column operations to $A$?","Suppose that $A$ is a matrix, can we get its transpose, $A^T$, by performing row and/or column operations to $A$?",,['linear-algebra']
91,Prove that $A \circ B = AB$ if and only if both $A$ and $B$ are diagonal,Prove that  if and only if both  and  are diagonal,A \circ B = AB A B,"Definition. Hadamard product. Let $A,B \in \mathbb{C}^{m \times n}$. The Hadamard product of $A$ and $B$ is defined by $[A \circ B]_{ij} = [A]_{ij}[B]_{ij}$ for all $i = 1, \dots, m$, $j = 1, \dots, n$. Remark. See details in this Hadamard product wiki article . There is the following remark in Million's paper in Chapter 2: We can relate the Hadamard product with matrix multiplication via   considering diagonal matrices, since $A \circ  B = AB$ if and only if   both $A$ and $B$ are diagonal. So there is a theorem, that $A \circ  B = AB$ if and only if both $A$ and $B$ are diagonal , but I don't know how to prove it, and I didn't find it in the literature, because not many books have written in this topic. Edit. In this theorem probably $m=n$. Million didn't write about it.","Definition. Hadamard product. Let $A,B \in \mathbb{C}^{m \times n}$. The Hadamard product of $A$ and $B$ is defined by $[A \circ B]_{ij} = [A]_{ij}[B]_{ij}$ for all $i = 1, \dots, m$, $j = 1, \dots, n$. Remark. See details in this Hadamard product wiki article . There is the following remark in Million's paper in Chapter 2: We can relate the Hadamard product with matrix multiplication via   considering diagonal matrices, since $A \circ  B = AB$ if and only if   both $A$ and $B$ are diagonal. So there is a theorem, that $A \circ  B = AB$ if and only if both $A$ and $B$ are diagonal , but I don't know how to prove it, and I didn't find it in the literature, because not many books have written in this topic. Edit. In this theorem probably $m=n$. Million didn't write about it.",,"['linear-algebra', 'matrices', 'hadamard-product']"
92,"If $v_1, \dots, v_m$ are linearly independent, then there is $w$ such that $\langle w, v_j \rangle > 0$ for all $j$","If  are linearly independent, then there is  such that  for all","v_1, \dots, v_m w \langle w, v_j \rangle > 0 j","Suppose $v_1, \dots v_m$ is a linearly independent list in $V$ . Show that there exists $w \in V$ such that $\langle w, v_j \rangle > 0$ for all $j \in {1, \dots ,m}$ . I understand this question is saying given a linearly independent list, there is $w \in V$ such that the vector $w$ is not orthogonal to any $v$ in that linearly independent set. I'm also confused as to why it is significant that the inner product be greater than zero and instead of just $\neq 0$ . Can someone give me a hint on how to do this problem? I know that $\langle v, v \rangle >0$ for all $v$ not equal to zero, and since $v_1, \dots v_m$ is linearly independent, then none of the $v_j$ will be zero, but it is impossible to have w equal to all $v_j$ ?","Suppose is a linearly independent list in . Show that there exists such that for all . I understand this question is saying given a linearly independent list, there is such that the vector is not orthogonal to any in that linearly independent set. I'm also confused as to why it is significant that the inner product be greater than zero and instead of just . Can someone give me a hint on how to do this problem? I know that for all not equal to zero, and since is linearly independent, then none of the will be zero, but it is impossible to have w equal to all ?","v_1, \dots v_m V w \in V \langle w, v_j \rangle > 0 j \in {1, \dots ,m} w \in V w v \neq 0 \langle v, v \rangle >0 v v_1, \dots v_m v_j v_j","['linear-algebra', 'vector-spaces', 'vectors', 'inner-products']"
93,Prove that $A$ is diagonalizable iff $\mbox{tr} A\neq 0$,Prove that  is diagonalizable iff,A \mbox{tr} A\neq 0,"Prove that $A$ is diagonalizable if and only if $\mbox{tr} A\neq 0$. $A$ is an $n\times n$ matrix over $\mathbb{C}$, and $\mbox{rk} A=1$. If $p(t)$ is the characteristic polynomial of $A$, I know that $a(n-1)\neq0$ because $\mbox{tr} A = (-1)^{n+1}a(n-1).$ I also know that $\dim\ker(A-0\cdot I)=\dim\ker A=n-\mbox{rk} A=n-1$ (so the geometric multiplicity of $t=0$ as an eigenvalue is $n-1$). Though I don't know how to continue from here (on both directions). Any suggestions? Thanks","Prove that $A$ is diagonalizable if and only if $\mbox{tr} A\neq 0$. $A$ is an $n\times n$ matrix over $\mathbb{C}$, and $\mbox{rk} A=1$. If $p(t)$ is the characteristic polynomial of $A$, I know that $a(n-1)\neq0$ because $\mbox{tr} A = (-1)^{n+1}a(n-1).$ I also know that $\dim\ker(A-0\cdot I)=\dim\ker A=n-\mbox{rk} A=n-1$ (so the geometric multiplicity of $t=0$ as an eigenvalue is $n-1$). Though I don't know how to continue from here (on both directions). Any suggestions? Thanks",,"['linear-algebra', 'diagonalization', 'trace']"
94,Every vector space has a basis using minimal spanning set.,Every vector space has a basis using minimal spanning set.,,"We have seen the argument for proving the above statment using Zorn's Lemma by asserting the existence of a maximal linearly independent set which serves a basis. In finite dimensional vector space, a basis is same as a maximal linearly independent and also same as a minimal spanning set. Does the notion of minimal spanning set make sense for arbitrary vector spaces? Moreover, can the statement that every vector space has a basis be proved using the partially ordered set $\Sigma = \lbrace  A \subset V \vert Span(A) =V \rbrace $ with the partial order $A \leq B$ iff $B \subset A $? Can one say intersection of a chain of spanning sets in this poset is also a spanning set?","We have seen the argument for proving the above statment using Zorn's Lemma by asserting the existence of a maximal linearly independent set which serves a basis. In finite dimensional vector space, a basis is same as a maximal linearly independent and also same as a minimal spanning set. Does the notion of minimal spanning set make sense for arbitrary vector spaces? Moreover, can the statement that every vector space has a basis be proved using the partially ordered set $\Sigma = \lbrace  A \subset V \vert Span(A) =V \rbrace $ with the partial order $A \leq B$ iff $B \subset A $? Can one say intersection of a chain of spanning sets in this poset is also a spanning set?",,"['linear-algebra', 'abstract-algebra', 'modules']"
95,A union of two subspaces not equal to the vector space. [duplicate],A union of two subspaces not equal to the vector space. [duplicate],,"This question already has answers here : Union of two vector subspaces not a subspace? (8 answers) Closed 10 years ago . Let $L,M$ two subspaces of the vector space, $V$ such that both $L,M \ne V$. Prove: $L\cup M \ne V$. I think this is a case of a proof by contradiction. Lets assume $L \cup M = V$. Hence, $$\dim(V) = \dim(L) + \dim(M) - \dim(L\cap M)$$ How to proceed?","This question already has answers here : Union of two vector subspaces not a subspace? (8 answers) Closed 10 years ago . Let $L,M$ two subspaces of the vector space, $V$ such that both $L,M \ne V$. Prove: $L\cup M \ne V$. I think this is a case of a proof by contradiction. Lets assume $L \cup M = V$. Hence, $$\dim(V) = \dim(L) + \dim(M) - \dim(L\cap M)$$ How to proceed?",,"['linear-algebra', 'vector-spaces']"
96,Why is this a good picture of a covector?,Why is this a good picture of a covector?,,"I'm reading a book about applied differential geometry and the author says: ""suppose $V$ is a finite dimensional vector space. For a given covector $\omega \in V^\ast$, the set $\hat{\omega}$, of vectors such that $\omega(v)=1$, that is $\hat{\omega}=\{v\in V : \omega(v)=1\}$ form a hyperplane in the vector space, and provide a faithful representation for $\omega$."" Now, why is this a good representation of a covector? I've heard that a covector can be thought of as a family of hyperplanes and it's value on a vector are then the number of hyperplanes crossed. This seems to relate to this idea, but how? Is this the idea that ""if $v\in \hat{\omega}$, then $v$ crosses $\omega$ once if $\dfrac{1}{k}v\in \hat{\omega}$ then $v$ crosses $\omega$ $k$-times""? I'm really failing to get the geometrical intuition behind those objects. Thanks very much in advance.","I'm reading a book about applied differential geometry and the author says: ""suppose $V$ is a finite dimensional vector space. For a given covector $\omega \in V^\ast$, the set $\hat{\omega}$, of vectors such that $\omega(v)=1$, that is $\hat{\omega}=\{v\in V : \omega(v)=1\}$ form a hyperplane in the vector space, and provide a faithful representation for $\omega$."" Now, why is this a good representation of a covector? I've heard that a covector can be thought of as a family of hyperplanes and it's value on a vector are then the number of hyperplanes crossed. This seems to relate to this idea, but how? Is this the idea that ""if $v\in \hat{\omega}$, then $v$ crosses $\omega$ once if $\dfrac{1}{k}v\in \hat{\omega}$ then $v$ crosses $\omega$ $k$-times""? I'm really failing to get the geometrical intuition behind those objects. Thanks very much in advance.",,"['linear-algebra', 'vector-spaces', 'visualization']"
97,Avoid dividing by zero with just variables and basic operators,Avoid dividing by zero with just variables and basic operators,,"I am working on stats for a sports team, and one of the stats I have the ratio of Shots and Shots on Target (Which I call SOTP ). So, for instance, if a player has 2 shots, and one's on target, their SOTP ratio is 1/2 or 50%. Now I have a problem that I can't seem to solve. The software that generates the stats lets me put in numbers, variables, or basic operators ( +, -, /, * , and ** , which represents ^ ). So for the SOTP I have $SOTP = SHOTSONTARGET/SHOTS$. My problem arises when a player has zero shots since the denominator is zero. If this is the case, the software automatically returns nothing, which works until I need to use the SOTP ratio in another operation. For instance, I have something that generates points based off passes, goals, etc, but when SOTP is equal to nothing, the points return nothing, even if the player has points. So my question is, is there anything I can do to avoid the zero in the denominator by just using basic operators, constants, and variables? Thanks for your help.","I am working on stats for a sports team, and one of the stats I have the ratio of Shots and Shots on Target (Which I call SOTP ). So, for instance, if a player has 2 shots, and one's on target, their SOTP ratio is 1/2 or 50%. Now I have a problem that I can't seem to solve. The software that generates the stats lets me put in numbers, variables, or basic operators ( +, -, /, * , and ** , which represents ^ ). So for the SOTP I have $SOTP = SHOTSONTARGET/SHOTS$. My problem arises when a player has zero shots since the denominator is zero. If this is the case, the software automatically returns nothing, which works until I need to use the SOTP ratio in another operation. For instance, I have something that generates points based off passes, goals, etc, but when SOTP is equal to nothing, the points return nothing, even if the player has points. So my question is, is there anything I can do to avoid the zero in the denominator by just using basic operators, constants, and variables? Thanks for your help.",,"['linear-algebra', 'statistics', 'algorithms', 'computer-science', 'ratio']"
98,Eigenvalues of a matrix satisfying a polynomial,Eigenvalues of a matrix satisfying a polynomial,,"The theorem of Cayley-Hamilton says that a matrix satisfies it's characteristic polynomial. But can we also make a statement about the eigenvalues if a matrix satisfies a monic polynomial in general? So let's say that we have a matrix $A \in \mathbb{C}^{m,m}$ and a monic polynomial $p$ of degree $n$ (with $n<m$) for which $p(A)=0$. What can be said about the eigenvalues of $A$ with this information?","The theorem of Cayley-Hamilton says that a matrix satisfies it's characteristic polynomial. But can we also make a statement about the eigenvalues if a matrix satisfies a monic polynomial in general? So let's say that we have a matrix $A \in \mathbb{C}^{m,m}$ and a monic polynomial $p$ of degree $n$ (with $n<m$) for which $p(A)=0$. What can be said about the eigenvalues of $A$ with this information?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
99,Projection into a subspace?,Projection into a subspace?,,"Let $S$ be a nonzero subspace with orthogonal basis $(v_1, \ldots, v_k)$ . Then the projection of $u$ onto $S$ is given by: $$\operatorname{proj}_S u = \frac {v_1 \dot{} u}{\operatorname{norm} (v_1)^2}v_1 + \cdots + \frac {v_k \dot{} u}{\operatorname{norm}(v_k)^2}v_k$$ Why does the basis have to be orthogonal? Doesn't the same formula allow you to ""extract"" the components even if the basis isn't orthogonal?","Let be a nonzero subspace with orthogonal basis . Then the projection of onto is given by: Why does the basis have to be orthogonal? Doesn't the same formula allow you to ""extract"" the components even if the basis isn't orthogonal?","S (v_1, \ldots, v_k) u S \operatorname{proj}_S u = \frac {v_1 \dot{} u}{\operatorname{norm} (v_1)^2}v_1 + \cdots + \frac {v_k \dot{} u}{\operatorname{norm}(v_k)^2}v_k","['linear-algebra', 'matrices', 'vector-spaces']"
