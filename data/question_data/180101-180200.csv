,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Basic question from vector analysis - Louis Brand Ch1, problem 2","Basic question from vector analysis - Louis Brand Ch1, problem 2",,"I am an open university second-year math undergrad student. I am taking a first course on multivariable calculus - I am solving problems from Vector analysis by Louis Brand to gain a physics-based intuition to what's going on. I would like someone to verify my simple proof - or possible suggest alternate line of thought. Show that $\vec{AB}+\vec{CD}=2\vec{MN}$, where $M$ and $N$ are the mid-points of $AC$ and $BD$. Solution. $\begin{aligned} \vec{MN} &= \vec{AN}-\vec{AM}\\ &= (\vec{AB} + \vec{BN}) - \frac{1}{2}\vec{AC}\\ &= \vec{AB} + \frac{1}{2}\vec{BD} - \frac{1}{2} \vec{AC}\\ &= \vec{AB} + \frac{1}{2}(\vec{BC} + \vec{CD}) - \frac{1}{2}(\vec{AB} + \vec{BC}) \\ &= \frac{1}{2}(\vec{AB} + \vec{CD})\\ 2\vec{MN} &= \vec{AB} + \vec{CD} \end{aligned}$","I am an open university second-year math undergrad student. I am taking a first course on multivariable calculus - I am solving problems from Vector analysis by Louis Brand to gain a physics-based intuition to what's going on. I would like someone to verify my simple proof - or possible suggest alternate line of thought. Show that $\vec{AB}+\vec{CD}=2\vec{MN}$, where $M$ and $N$ are the mid-points of $AC$ and $BD$. Solution. $\begin{aligned} \vec{MN} &= \vec{AN}-\vec{AM}\\ &= (\vec{AB} + \vec{BN}) - \frac{1}{2}\vec{AC}\\ &= \vec{AB} + \frac{1}{2}\vec{BD} - \frac{1}{2} \vec{AC}\\ &= \vec{AB} + \frac{1}{2}(\vec{BC} + \vec{CD}) - \frac{1}{2}(\vec{AB} + \vec{BC}) \\ &= \frac{1}{2}(\vec{AB} + \vec{CD})\\ 2\vec{MN} &= \vec{AB} + \vec{CD} \end{aligned}$",,"['multivariable-calculus', 'proof-verification', 'vectors']"
1,Dual spaces and gradients and subgradients,Dual spaces and gradients and subgradients,,"Suppose we have some function $f:{\mathbb R}^n \rightarrow \mathbb{R}$. Its gradient is defined as the vector which gives the directional derivative via $(v,\nabla f )=D_{v}f$ for any direction $v$. Could, or should, we think of $\nabla f$ as something belonging to the dual space of the domain of $f$? And if yes, what is the idea of going about this in this way? In particular are there some geometric ideas involved? I ran into this idea while learning about subgradients and generalised subgradients, which are defined as functionals on the space of the domain of $f$.","Suppose we have some function $f:{\mathbb R}^n \rightarrow \mathbb{R}$. Its gradient is defined as the vector which gives the directional derivative via $(v,\nabla f )=D_{v}f$ for any direction $v$. Could, or should, we think of $\nabla f$ as something belonging to the dual space of the domain of $f$? And if yes, what is the idea of going about this in this way? In particular are there some geometric ideas involved? I ran into this idea while learning about subgradients and generalised subgradients, which are defined as functionals on the space of the domain of $f$.",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
2,integrate $2\sqrt{1-x^2/4-y^2/9}$,integrate,2\sqrt{1-x^2/4-y^2/9},"I am trying to find the volume of the ellipsoid $E$ given by $x^2/4 + y^2/9 + z^2 \leq 1$ by computing $\iiint_E dV$ I ended up with $$\int_{-2}^{2}\int_{-3\sqrt{1-x^2/4}}^{3\sqrt{1-x^2/4}}\int_{-\sqrt{1-x^2/4-y^2/9}}^{\sqrt{1-x^2/4-y^2/9}}1dzdydx$$ After doing the inner integral I get $$\int_{-2}^{2}\int_{-3\sqrt{1-x^2/4}}^{3\sqrt{1-x^2/4}}2\sqrt{1-x^2/4-y^2/9}\space dydx$$ I am not sure about how to figure out the next one The hint is to use trig substitution, though I am not sure if it is talking about this step or the next one","I am trying to find the volume of the ellipsoid $E$ given by $x^2/4 + y^2/9 + z^2 \leq 1$ by computing $\iiint_E dV$ I ended up with $$\int_{-2}^{2}\int_{-3\sqrt{1-x^2/4}}^{3\sqrt{1-x^2/4}}\int_{-\sqrt{1-x^2/4-y^2/9}}^{\sqrt{1-x^2/4-y^2/9}}1dzdydx$$ After doing the inner integral I get $$\int_{-2}^{2}\int_{-3\sqrt{1-x^2/4}}^{3\sqrt{1-x^2/4}}2\sqrt{1-x^2/4-y^2/9}\space dydx$$ I am not sure about how to figure out the next one The hint is to use trig substitution, though I am not sure if it is talking about this step or the next one",,"['multivariable-calculus', 'volume']"
3,Approximation argument for Poisson integral formula,Approximation argument for Poisson integral formula,,"In the PDE book, for a harmonic function $u \in C^2(B_R(0))\cap C^1(\overline B_R(0)),$ we have the following Poisson integral formula   $$ u(y)=\frac{R^2-|y|^2}{n w_n R}\int_{\partial B_R(y)}\frac{u(x)}{|x-y|^n}ds_x $$   An approximation argument shows that the Poisson integral formula continues to hold for $u \in C^2(B_R(0))\cap C(\overline B_R(0)).$ What is the approximation argument? For a harmonic function $u \in C(\overline B_R(0))$, is there a sequence of harmonic functions $u_k \in  C^1(\overline B_R(0))$ such that $u_k \to u$ in $C(\overline B_R(0))$? I have no idea for harmonic functions. May I use convolution? Please let me know if you have any hint or comment for it. Thanks in advance!","In the PDE book, for a harmonic function $u \in C^2(B_R(0))\cap C^1(\overline B_R(0)),$ we have the following Poisson integral formula   $$ u(y)=\frac{R^2-|y|^2}{n w_n R}\int_{\partial B_R(y)}\frac{u(x)}{|x-y|^n}ds_x $$   An approximation argument shows that the Poisson integral formula continues to hold for $u \in C^2(B_R(0))\cap C(\overline B_R(0)).$ What is the approximation argument? For a harmonic function $u \in C(\overline B_R(0))$, is there a sequence of harmonic functions $u_k \in  C^1(\overline B_R(0))$ such that $u_k \to u$ in $C(\overline B_R(0))$? I have no idea for harmonic functions. May I use convolution? Please let me know if you have any hint or comment for it. Thanks in advance!",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'vector-analysis', 'linear-pde']"
4,Directional derivative of $\frac{xy^2}{x^2+y^6}$,Directional derivative of,\frac{xy^2}{x^2+y^6},"Let $f:\mathbb{R}^2\rightarrow\mathbb{R}, (x,y)\mapsto\begin{cases}\frac{xy^2}{x^2+y^6},\,&(x,y)\neq(0,0)\\0,\,&\text{else}\end{cases}$. Show that for every direction $v\in\mathbb{R}^2\setminus\{0\}$ there exists a directional derivative $D_vf(0,0)$. Is $f$ continuous in $(0,0)$? Is $f$ differentiable in $(0,0)$? I'm stuck on this exercise. I tried applying the definition, i.e. showing that $\lim_{t\rightarrow 0}\frac{f(\xi+tv)-f(\xi)}{t}$ exists, but it's getting me nowhere. Can anybody tell me the right approach here or what I'm not seeing?","Let $f:\mathbb{R}^2\rightarrow\mathbb{R}, (x,y)\mapsto\begin{cases}\frac{xy^2}{x^2+y^6},\,&(x,y)\neq(0,0)\\0,\,&\text{else}\end{cases}$. Show that for every direction $v\in\mathbb{R}^2\setminus\{0\}$ there exists a directional derivative $D_vf(0,0)$. Is $f$ continuous in $(0,0)$? Is $f$ differentiable in $(0,0)$? I'm stuck on this exercise. I tried applying the definition, i.e. showing that $\lim_{t\rightarrow 0}\frac{f(\xi+tv)-f(\xi)}{t}$ exists, but it's getting me nowhere. Can anybody tell me the right approach here or what I'm not seeing?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
5,If component functions are injective then is the function injective?,If component functions are injective then is the function injective?,,"If we have injective continuously differentiable function $f$, and if we define continuously differentiable function $g(x,y)=(f(x),y)$, then does it follow that $g$ is injective? Thanks","If we have injective continuously differentiable function $f$, and if we define continuously differentiable function $g(x,y)=(f(x),y)$, then does it follow that $g$ is injective? Thanks",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'proof-writing']"
6,$|f(x)-f(y)|\le C |g(x)-g(y)|$ and integrability of $g$ implies that of $f$,and integrability of  implies that of,|f(x)-f(y)|\le C |g(x)-g(y)| g f,"Let $S\subset \mathbb R^n$ be a closed rectangle and $f,g: S\to \mathbb R$ functions such that $$|f(x)-f(y)|\le C |g(x)-g(y)|$$ for all $x,y\in S$. Show that if $g$ is Riemann integrable, then so if $f$, and if $f$ is, then so if $|f|$. There is an integrability criterion which I don't know how to connect with the given inequality. If $P$ is a partition, then $U(f,P)-L(f,P) < \epsilon $. Or should I use something different? Let $P$ be a partition of $S$ such that $(U(g,P) - L(g,P)<\epsilon/C$. On each subrectangle of partition $s$, $\sup_s f-\inf_s f=f(x_s)-f(y_s)$ (the sup and inf are attained because $f$ is continuous on the compact $s$). For each $s$, $|f(x_s)-f(y_s)|\le C |g(x_s)-g(y_s)| \le C (\sup_s g-\inf_s g)$. Thus $$U(f,P)-L(f,P)=\sum_s (\sup_s f-\inf_s f)\operatorname{vol}(s)\le C \sum_s(\sup_s g-\inf_s g)\operatorname{vol}(s)=C (U(g,P) - L(g,P)) < \epsilon$$","Let $S\subset \mathbb R^n$ be a closed rectangle and $f,g: S\to \mathbb R$ functions such that $$|f(x)-f(y)|\le C |g(x)-g(y)|$$ for all $x,y\in S$. Show that if $g$ is Riemann integrable, then so if $f$, and if $f$ is, then so if $|f|$. There is an integrability criterion which I don't know how to connect with the given inequality. If $P$ is a partition, then $U(f,P)-L(f,P) < \epsilon $. Or should I use something different? Let $P$ be a partition of $S$ such that $(U(g,P) - L(g,P)<\epsilon/C$. On each subrectangle of partition $s$, $\sup_s f-\inf_s f=f(x_s)-f(y_s)$ (the sup and inf are attained because $f$ is continuous on the compact $s$). For each $s$, $|f(x_s)-f(y_s)|\le C |g(x_s)-g(y_s)| \le C (\sup_s g-\inf_s g)$. Thus $$U(f,P)-L(f,P)=\sum_s (\sup_s f-\inf_s f)\operatorname{vol}(s)\le C \sum_s(\sup_s g-\inf_s g)\operatorname{vol}(s)=C (U(g,P) - L(g,P)) < \epsilon$$",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
7,Find the volume common to sphere $x^2+y^2+z^2$ and the cylinder $x^2+y^2<ax$,Find the volume common to sphere  and the cylinder,x^2+y^2+z^2 x^2+y^2<ax,"Find the volume common to sphere $x^2+y^2+z^2<1$ and the cylinder $x^2+y^2<ax$ I set up the following integral : $$I=2\cdot \iiint_{z=0}^{\sqrt{a^2-x^2-y^2}} dz\,dy\,dz = 2\cdot\iint_E\sqrt{a^2-x^2-y^2} \, dy\,dx$$ where $E:x^2+y^2=ax$ Now under polar coordinates  $E:r=a\cos(\theta)$ and so $0\leq r \leq a\cos(\theta)$ and $-\pi/2\leq \theta \leq \pi/2$ $$I= 2\cdot\int_{\theta=-\pi/2}^{\pi/2}\,\int_{r=0}^{a\cos(\theta)} \sqrt{a^2-r^2}\,r\,dr\,d\theta=2\cdot (1/2)\cdot(2/3)\int_{\theta=-\pi/2}^{\pi/2}a^3\cdot(1-\sin^3(\theta))d\theta = \frac{2a^3}{3}\cdot\{\int_{\theta=-\pi/2}^{\pi/2} d\theta - \int_{\theta=-\pi/2}^{\pi/2}\sin^3(\theta)d\theta\} = \frac{2\pi a^3}{3}$$ However answer given to me is $$\frac{2a^3}{3}\cdot(\pi - \frac43)$$ Where am I making the mistake? Or is it the case that the answer given to me is incorrect?","Find the volume common to sphere $x^2+y^2+z^2<1$ and the cylinder $x^2+y^2<ax$ I set up the following integral : $$I=2\cdot \iiint_{z=0}^{\sqrt{a^2-x^2-y^2}} dz\,dy\,dz = 2\cdot\iint_E\sqrt{a^2-x^2-y^2} \, dy\,dx$$ where $E:x^2+y^2=ax$ Now under polar coordinates  $E:r=a\cos(\theta)$ and so $0\leq r \leq a\cos(\theta)$ and $-\pi/2\leq \theta \leq \pi/2$ $$I= 2\cdot\int_{\theta=-\pi/2}^{\pi/2}\,\int_{r=0}^{a\cos(\theta)} \sqrt{a^2-r^2}\,r\,dr\,d\theta=2\cdot (1/2)\cdot(2/3)\int_{\theta=-\pi/2}^{\pi/2}a^3\cdot(1-\sin^3(\theta))d\theta = \frac{2a^3}{3}\cdot\{\int_{\theta=-\pi/2}^{\pi/2} d\theta - \int_{\theta=-\pi/2}^{\pi/2}\sin^3(\theta)d\theta\} = \frac{2\pi a^3}{3}$$ However answer given to me is $$\frac{2a^3}{3}\cdot(\pi - \frac43)$$ Where am I making the mistake? Or is it the case that the answer given to me is incorrect?",,"['calculus', 'multivariable-calculus', 'multiple-integral']"
8,Finding the absolute maximum,Finding the absolute maximum,,"You are in charge of manufacturing the snazzy new mobile tablets that everyone wants to own.  The revenue function, in dollars, is given by $R(s,t) = 8s+6t-s^2-2t^2+2st$  , s denotes ""steel"" model and t denotes ""titanium"" model, both in units of million (assume that you make positive but a finite number of products). I have to determine the quantity of both products for maximum revenue. My understanding: So, I think the question is asking for the global maximum point. I found the critical point and it has only one, (11,7). Now, I think we need to assume that the lowest boundary for s and t is 0 and the upper boundary is also something (I don't know what to assume). And I'm stuck here.","You are in charge of manufacturing the snazzy new mobile tablets that everyone wants to own.  The revenue function, in dollars, is given by $R(s,t) = 8s+6t-s^2-2t^2+2st$  , s denotes ""steel"" model and t denotes ""titanium"" model, both in units of million (assume that you make positive but a finite number of products). I have to determine the quantity of both products for maximum revenue. My understanding: So, I think the question is asking for the global maximum point. I found the critical point and it has only one, (11,7). Now, I think we need to assume that the lowest boundary for s and t is 0 and the upper boundary is also something (I don't know what to assume). And I'm stuck here.",,['multivariable-calculus']
9,How to show that $|\nabla f|\ge \sqrt 2$?,How to show that ?,|\nabla f|\ge \sqrt 2,"Let $[0,1]^2\subset U\subset \mathbb R^2$ where $U$ is open, and let $f: U\to \mathbb R$ be a differentiable function with $f(0,0)=3$ and $f(1,1)=1$. Prove that $|\nabla f|\ge \sqrt 2$ somewhere in $U$. The only idea I had is to apply Taylor's formula but it only gives something like $-2=f(1,1)-f(0,0)=f_x(1,1)+f_y(1,1)$, which holds at a point, and I guess ""somewhere"" means ""on a subset"". Moreover, I don't know how to get hold of $f_x^2+f_y^2=||\nabla f|| ^2$. Any hints please? (If I only use hints as opposed to reading a solution I hope I will learn more.)","Let $[0,1]^2\subset U\subset \mathbb R^2$ where $U$ is open, and let $f: U\to \mathbb R$ be a differentiable function with $f(0,0)=3$ and $f(1,1)=1$. Prove that $|\nabla f|\ge \sqrt 2$ somewhere in $U$. The only idea I had is to apply Taylor's formula but it only gives something like $-2=f(1,1)-f(0,0)=f_x(1,1)+f_y(1,1)$, which holds at a point, and I guess ""somewhere"" means ""on a subset"". Moreover, I don't know how to get hold of $f_x^2+f_y^2=||\nabla f|| ^2$. Any hints please? (If I only use hints as opposed to reading a solution I hope I will learn more.)",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
10,Calculating volume using cylindrical coordinates.,Calculating volume using cylindrical coordinates.,,"A solid $D$ is defined by the following inequalities: $$\begin{align}x^2+y^2+(z-1)^2 &\le 1\\ z^2 &\le x^2+y^2\end{align}$$ Calculate the volume of $D$. Attempt to solve: $$\begin{align}x&=r\cos\theta \\ y&=r\sin\theta\end{align}$$ Plugging in these values into the first equation we get : $$\begin{align} r^2+(z-1)^2&=1 \\ (z-1)^2&=1-r^2 \\ z&=1\pm\sqrt{1-r^2} \end{align}$$ Since $z^2=r^2$ from the 2nd inequality, we'll have $z=\pm r$. Solving for $r$: $$r=1+\sqrt{1-r^2} \implies r=1$$ and $0<\theta<2\pi$. However, I'm confused about how to define the limits of $z$ since it could be equal to $1+\sqrt{1-r^2}$ or $1+\sqrt{1-r^2}$. Thanks in advance.","A solid $D$ is defined by the following inequalities: $$\begin{align}x^2+y^2+(z-1)^2 &\le 1\\ z^2 &\le x^2+y^2\end{align}$$ Calculate the volume of $D$. Attempt to solve: $$\begin{align}x&=r\cos\theta \\ y&=r\sin\theta\end{align}$$ Plugging in these values into the first equation we get : $$\begin{align} r^2+(z-1)^2&=1 \\ (z-1)^2&=1-r^2 \\ z&=1\pm\sqrt{1-r^2} \end{align}$$ Since $z^2=r^2$ from the 2nd inequality, we'll have $z=\pm r$. Solving for $r$: $$r=1+\sqrt{1-r^2} \implies r=1$$ and $0<\theta<2\pi$. However, I'm confused about how to define the limits of $z$ since it could be equal to $1+\sqrt{1-r^2}$ or $1+\sqrt{1-r^2}$. Thanks in advance.",,"['calculus', 'integration', 'multivariable-calculus', 'cylindrical-coordinates']"
11,Neural Network Gradient Descent of Cost Function Misunderstanding,Neural Network Gradient Descent of Cost Function Misunderstanding,,"I've taken an interest in neural networks recently and have been progressing rather well but came to a standstill while learning about gradient descent (I've done multivariable calculus previously). Specifically, I struggle with this: Say our neural network is designed to recognise digits 0-9, and we have the MSE Cost function which, given a certain vector of weights and biases, after a large number of training examples, will spit out the average 'cost' as a scalar. I have read that now one must compute the gradient -$\nabla{C}$ in order to find the vector of greatest descent, but I am confused on this: How does one graph the cost function/find an explicit formula for the cost function/compute $\nabla{C}$ - these would all require one to try an infinite number of weights and biases surely...??","I've taken an interest in neural networks recently and have been progressing rather well but came to a standstill while learning about gradient descent (I've done multivariable calculus previously). Specifically, I struggle with this: Say our neural network is designed to recognise digits 0-9, and we have the MSE Cost function which, given a certain vector of weights and biases, after a large number of training examples, will spit out the average 'cost' as a scalar. I have read that now one must compute the gradient -$\nabla{C}$ in order to find the vector of greatest descent, but I am confused on this: How does one graph the cost function/find an explicit formula for the cost function/compute $\nabla{C}$ - these would all require one to try an infinite number of weights and biases surely...??",,"['multivariable-calculus', 'optimization', 'machine-learning', 'gradient-descent', 'neural-networks']"
12,How to compute this derivative of a square root of a sum?,How to compute this derivative of a square root of a sum?,,"I have the function $$f(\mathbf{x}) = \sqrt{\frac{1}{n}\sum_{i=1}^n\left(\log_e(x_i+1)-c_i\right)^2}$$ where $c_i$ is a constant, and I want to find $f'(\mathbf{x})$, more explicitly, $$\frac{\partial f}{\partial x_i}$$ the derivative of $f$ with respect to $x_i$, the $i$'th variable. I know I'll have to apply the chain rule here, but with the sum and the square root, I am unsure of the correct order of the steps.","I have the function $$f(\mathbf{x}) = \sqrt{\frac{1}{n}\sum_{i=1}^n\left(\log_e(x_i+1)-c_i\right)^2}$$ where $c_i$ is a constant, and I want to find $f'(\mathbf{x})$, more explicitly, $$\frac{\partial f}{\partial x_i}$$ the derivative of $f$ with respect to $x_i$, the $i$'th variable. I know I'll have to apply the chain rule here, but with the sum and the square root, I am unsure of the correct order of the steps.",,"['multivariable-calculus', 'derivatives']"
13,What's wrong with my answer about this integral?,What's wrong with my answer about this integral?,,"Problem : Let $D=\{x^{2}+y^{2} \leq1\}$ and $ f_{xx}+f_{yy}=1$ for $(x,y)\in D$. Prove that $$\iint_{D}(xf_{x}+yf_{y})dxdy=\frac{\pi}{4}$$ My solution : $\iint_{D}(xf_{x}+yf_{y})dxdy=\int_{0}^{1}dr\int_{0}^{2\pi}[rcos\theta f_{x}(rcos\theta,rsin\theta)+rsin\theta f_{y}(rcos\theta,rsin\theta)]rd\theta\=\int_{0}^{1}dr[\int_{L_{r}}(xf_{x}+yf_{y})ds]=\int_{0}^{1}dr[\int_{L_{r}}(f_{x}dy-f_{y}dx)ds]=\int_{0}^{1}dr[\iint_{D_{r}}(f_{xx}+f_{yy}dx)dS]=\int_{0}^{1}dr[\pi r^{2}]=\frac{\pi}{3}\quad???$ where $L_{r} $ represents a circle with radius $r$ in $D$ and $D_{r}$  is the interior of $L_{r}$. However, I don't know what's wrong with my solution...Could anyone help me?   Please..","Problem : Let $D=\{x^{2}+y^{2} \leq1\}$ and $ f_{xx}+f_{yy}=1$ for $(x,y)\in D$. Prove that $$\iint_{D}(xf_{x}+yf_{y})dxdy=\frac{\pi}{4}$$ My solution : $\iint_{D}(xf_{x}+yf_{y})dxdy=\int_{0}^{1}dr\int_{0}^{2\pi}[rcos\theta f_{x}(rcos\theta,rsin\theta)+rsin\theta f_{y}(rcos\theta,rsin\theta)]rd\theta\=\int_{0}^{1}dr[\int_{L_{r}}(xf_{x}+yf_{y})ds]=\int_{0}^{1}dr[\int_{L_{r}}(f_{x}dy-f_{y}dx)ds]=\int_{0}^{1}dr[\iint_{D_{r}}(f_{xx}+f_{yy}dx)dS]=\int_{0}^{1}dr[\pi r^{2}]=\frac{\pi}{3}\quad???$ where $L_{r} $ represents a circle with radius $r$ in $D$ and $D_{r}$  is the interior of $L_{r}$. However, I don't know what's wrong with my solution...Could anyone help me?   Please..",,"['calculus', 'multivariable-calculus', 'multiple-integral']"
14,Use Green's Theorem to evaluate a line integral,Use Green's Theorem to evaluate a line integral,,"Evaluate the line integral $\int_cy^4\ dx+2xy^3\ dy$ where $C$ is the ellipse $x^2+2y^2=2$. My attempt: First, I need Green's Theorem: $\int_cP\ dx+Q\ dy = \int\int_D\big(\frac{\partial{Q}}{\partial{x}}-\frac{\partial{P}}{\partial{y}}\big)\ dA$ where $C$ is a positively oriented, smooth, closed curve and let $D$ be the region bound by $C$ in the plane. Therefore, my region $D$ is the ellipse $\frac{x^2}{(\sqrt{2})^2}+y^2=1$. My bounds for $D$ would then be $-\sqrt{2}\leq x \leq \sqrt{2},\space and \space 0\leq y \leq\sqrt{1-\frac{x^2}{2}}$. I realize though that this is only the upper half of the ellipse, so I multiply my integral by $2$ $\therefore$ $$2\int_{-\sqrt{2}}^{\sqrt{2}}\int_0^{\sqrt{1-\frac{x^2}{2}}}-2y^3\ dy\ dx$$ However, after evaluating this integral, my result is $\frac{22\sqrt{2}}{15}$ when it should be $0$. Is it incorrect to multiply the integral by $2$ when I should instead be integrating in the $y$ direction from $$-{\sqrt{1-\frac{x^2}{2}}}\leq y \leq{\sqrt{1-\frac{x^2}{2}}}$$","Evaluate the line integral $\int_cy^4\ dx+2xy^3\ dy$ where $C$ is the ellipse $x^2+2y^2=2$. My attempt: First, I need Green's Theorem: $\int_cP\ dx+Q\ dy = \int\int_D\big(\frac{\partial{Q}}{\partial{x}}-\frac{\partial{P}}{\partial{y}}\big)\ dA$ where $C$ is a positively oriented, smooth, closed curve and let $D$ be the region bound by $C$ in the plane. Therefore, my region $D$ is the ellipse $\frac{x^2}{(\sqrt{2})^2}+y^2=1$. My bounds for $D$ would then be $-\sqrt{2}\leq x \leq \sqrt{2},\space and \space 0\leq y \leq\sqrt{1-\frac{x^2}{2}}$. I realize though that this is only the upper half of the ellipse, so I multiply my integral by $2$ $\therefore$ $$2\int_{-\sqrt{2}}^{\sqrt{2}}\int_0^{\sqrt{1-\frac{x^2}{2}}}-2y^3\ dy\ dx$$ However, after evaluating this integral, my result is $\frac{22\sqrt{2}}{15}$ when it should be $0$. Is it incorrect to multiply the integral by $2$ when I should instead be integrating in the $y$ direction from $$-{\sqrt{1-\frac{x^2}{2}}}\leq y \leq{\sqrt{1-\frac{x^2}{2}}}$$",,"['multivariable-calculus', 'greens-theorem', 'iterated-integrals']"
15,Lagrange multipliers: find minimum with constraints,Lagrange multipliers: find minimum with constraints,,"Find the minimum of $f(x,y,z) = z$ subject to the constraints $x + y + z = 1$ and $x^2 + y^2 = 1.$ So far, I have $$(0,0,1)=\lambda(1,1,1)+\mu(2x,2y,0)$$ $$0=\lambda+2\mu x \implies 2\mu x=-1 \implies x=-1/2\mu$$ $$0=\lambda+2\mu y \implies 2\mu y=-1 \implies y=-1/2\mu$$ $$1=\lambda$$ I feel like I'm not on the right path as there is no $z$ variable. Any input is appreciated.","Find the minimum of $f(x,y,z) = z$ subject to the constraints $x + y + z = 1$ and $x^2 + y^2 = 1.$ So far, I have $$(0,0,1)=\lambda(1,1,1)+\mu(2x,2y,0)$$ $$0=\lambda+2\mu x \implies 2\mu x=-1 \implies x=-1/2\mu$$ $$0=\lambda+2\mu y \implies 2\mu y=-1 \implies y=-1/2\mu$$ $$1=\lambda$$ I feel like I'm not on the right path as there is no $z$ variable. Any input is appreciated.",,['multivariable-calculus']
16,"Finding $a_i:U→\mathbb R$ of class $C^{k-1}$ such that $f(x,y)=\sum\limits_{j=0}^ia_j(x,y)x^jy^{i-j}$",Finding  of class  such that,"a_i:U→\mathbb R C^{k-1} f(x,y)=\sum\limits_{j=0}^ia_j(x,y)x^jy^{i-j}","Let $f: U \to \mathbb{R}$ of class $C^{k}$ ($i \leq k \leq \infty$) in the convex open $U \subset \mathbb{R}^{2}$ containing $(0,0)$. Suppose that $f$ all partial derivatives of order $\leq i$ vanish in $(0,0)$. Prove that there exists functions $a_{0},..., a_{i}: U \to \mathbb{R}$ of class $C^{k-1}$, such that $$f(x,y) = \sum_{j=0}^{i}a_{j}(x,y)x^{j}y^{i-j}. \quad \forall (x,y) \in U$$ I didn't have a good idea to start solving, but I don't want a complete solution, I just want some hints to get started. My professor hint me to use Taylor Formula, I tried to do this, but I couldn't see how to use.","Let $f: U \to \mathbb{R}$ of class $C^{k}$ ($i \leq k \leq \infty$) in the convex open $U \subset \mathbb{R}^{2}$ containing $(0,0)$. Suppose that $f$ all partial derivatives of order $\leq i$ vanish in $(0,0)$. Prove that there exists functions $a_{0},..., a_{i}: U \to \mathbb{R}$ of class $C^{k-1}$, such that $$f(x,y) = \sum_{j=0}^{i}a_{j}(x,y)x^{j}y^{i-j}. \quad \forall (x,y) \in U$$ I didn't have a good idea to start solving, but I don't want a complete solution, I just want some hints to get started. My professor hint me to use Taylor Formula, I tried to do this, but I couldn't see how to use.",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
17,Directional Derivatives...,Directional Derivatives...,,"There is a proof for how the limit definition of the direction derivative can be expressed as $\nabla f\cdot u$ that I found online where we define a function $g(h) = f(x+ha, y+hb)$ and find the derivative of it at $h=0$ which is equal to the limit definition of the directional derivative. Then we use the chain rule and evaluate that at $h=0$ too, and the two expressions of $g'(0)$ which is equal to the limit def. of a directional derivative. What I don't undetstand is the first step. Why do we create a function $g(h) = f(x_0 + ha, y_0 + hb)$ and when we find the derivative of it, why do we evaluate it at $h=0$? Also why do we evalautate the chain rule expression at $h=0$ too?","There is a proof for how the limit definition of the direction derivative can be expressed as $\nabla f\cdot u$ that I found online where we define a function $g(h) = f(x+ha, y+hb)$ and find the derivative of it at $h=0$ which is equal to the limit definition of the directional derivative. Then we use the chain rule and evaluate that at $h=0$ too, and the two expressions of $g'(0)$ which is equal to the limit def. of a directional derivative. What I don't undetstand is the first step. Why do we create a function $g(h) = f(x_0 + ha, y_0 + hb)$ and when we find the derivative of it, why do we evaluate it at $h=0$? Also why do we evalautate the chain rule expression at $h=0$ too?",,"['calculus', 'limits']"
18,$\Bbb R^2 \to \Bbb R$ limit problem from Hubbard & Hubbard (Exercise 1.5.24),limit problem from Hubbard & Hubbard (Exercise 1.5.24),\Bbb R^2 \to \Bbb R,"For what positive integers a, b, c, and d, where $c \le d$, does the limit as $\begin{pmatrix} x\\ y\\ \end{pmatrix} \to \begin{pmatrix} 0\\ 0\\ \end{pmatrix}$ of $$\frac{x^ay^b}{x^{2c}+y^{2d}} \quad \text{exist?}$$ This is a minor modification of problem 1.5.24 on p 104 of Hubbard & Hubbard's Vector Calculus, Linear Algebra, and Differential Forms:  A Unified Approach (5th edition,  ISBN 9780971576681, pub by Matrix Editions). H & H said a, b, c, and d are non-negative integers and didn't specify $\,$ $c\le d$ , but $c\le d$ may be assumed without loss of generality, and if we allow any exponent to be 0 there are lots of trivial cases to deal with, all of which I can solve. This is what I've proven so far: 1) $\,$If $\,a \gt c \,$ and $\, b \gt d\,$  the limit is 0.$\,$ But I don't know what happens in general if $ \,a \le c \,$ or $ \,b \le d$. 2) $\,$ If $a \gt 2c \,$ or $\, b \gt 2d\,$ the limit is 0. $\,$  But I don't what happens if $\, a \le 2c \,$  and  $\,b \le 2d$. 3) $\,$  If $ \,a+b \le 2c \,$ the limit does not exist, and if  $\, a+b \gt 2d \,$ the limit is 0.  $\,$ But I don't know what $\quad$ happens if  $\, 2c \lt a+b \le 2d$. 4) If $\,ad + cb \le 2cd \,$ the limit does not exist, which implies that if  $\,a \le c \,$  and $\, b \le d \,$ then the limit $\quad$ also does not exist. $\,$But I don't know what happens if  $ \,ad + cb > 2cd$. Since at this point in the book H & H haven't even defined partial derivatives, I'm hoping for a proof that requires nothing fancier than AP BC calculus and the basic limit definitions and properties of limits for functions from $R^n$ into $R^m$. But any proof or help will be greatly appreciated. Thanks!","For what positive integers a, b, c, and d, where $c \le d$, does the limit as $\begin{pmatrix} x\\ y\\ \end{pmatrix} \to \begin{pmatrix} 0\\ 0\\ \end{pmatrix}$ of $$\frac{x^ay^b}{x^{2c}+y^{2d}} \quad \text{exist?}$$ This is a minor modification of problem 1.5.24 on p 104 of Hubbard & Hubbard's Vector Calculus, Linear Algebra, and Differential Forms:  A Unified Approach (5th edition,  ISBN 9780971576681, pub by Matrix Editions). H & H said a, b, c, and d are non-negative integers and didn't specify $\,$ $c\le d$ , but $c\le d$ may be assumed without loss of generality, and if we allow any exponent to be 0 there are lots of trivial cases to deal with, all of which I can solve. This is what I've proven so far: 1) $\,$If $\,a \gt c \,$ and $\, b \gt d\,$  the limit is 0.$\,$ But I don't know what happens in general if $ \,a \le c \,$ or $ \,b \le d$. 2) $\,$ If $a \gt 2c \,$ or $\, b \gt 2d\,$ the limit is 0. $\,$  But I don't what happens if $\, a \le 2c \,$  and  $\,b \le 2d$. 3) $\,$  If $ \,a+b \le 2c \,$ the limit does not exist, and if  $\, a+b \gt 2d \,$ the limit is 0.  $\,$ But I don't know what $\quad$ happens if  $\, 2c \lt a+b \le 2d$. 4) If $\,ad + cb \le 2cd \,$ the limit does not exist, which implies that if  $\,a \le c \,$  and $\, b \le d \,$ then the limit $\quad$ also does not exist. $\,$But I don't know what happens if  $ \,ad + cb > 2cd$. Since at this point in the book H & H haven't even defined partial derivatives, I'm hoping for a proof that requires nothing fancier than AP BC calculus and the basic limit definitions and properties of limits for functions from $R^n$ into $R^m$. But any proof or help will be greatly appreciated. Thanks!",,"['real-analysis', 'limits', 'multivariable-calculus']"
19,The maximum light intensity coming from two light bulbs on the $xy$-plane,The maximum light intensity coming from two light bulbs on the -plane,xy,"There is two light bulbs, one on $(-a,0,2)$ and the other on $(a,0,2)$, where $a>0$. The light intensity coming from two light bulbs on the $x$-axis and $y$-axis (where $z=0$) can be calculated using the following equation: $$f(x,y) = \frac{1}{(x+a)^2+y^2+2^2} + \frac{1}{(x-a)^2+y^2+2^2}.$$ My question is, how to find the maximum point(s) (or critical points) of $f$. Notes: There can be one or two critical points, depending on $a$.","There is two light bulbs, one on $(-a,0,2)$ and the other on $(a,0,2)$, where $a>0$. The light intensity coming from two light bulbs on the $x$-axis and $y$-axis (where $z=0$) can be calculated using the following equation: $$f(x,y) = \frac{1}{(x+a)^2+y^2+2^2} + \frac{1}{(x-a)^2+y^2+2^2}.$$ My question is, how to find the maximum point(s) (or critical points) of $f$. Notes: There can be one or two critical points, depending on $a$.",,"['calculus', 'multivariable-calculus']"
20,"Find the following limit : $\lim_{(x,y)\rightarrow(0,0)}xy \log\left|y\right|$",Find the following limit :,"\lim_{(x,y)\rightarrow(0,0)}xy \log\left|y\right|","I have been trying to solve this limit: $$\lim_{(x,y)\rightarrow(0,0)}xy\log\left|y\right|$$ I tried using polar coordinantes but i could not find the answer. I tried using iterate limits but i dont get any information . Any hint on how to solve this?","I have been trying to solve this limit: $$\lim_{(x,y)\rightarrow(0,0)}xy\log\left|y\right|$$ I tried using polar coordinantes but i could not find the answer. I tried using iterate limits but i dont get any information . Any hint on how to solve this?",,"['limits', 'multivariable-calculus']"
21,Outward/Inward flux of a conservative vector field over a closed loop,Outward/Inward flux of a conservative vector field over a closed loop,,"Due to path independence the line integral of a conservative vector field for a closed loop is 0--assuming this is over some simply connected region. In my calculus text book it says that ""conservative"" implies that it follows the principles of conservation of energy, hence does this imply the inward and outward flux over the same closed loop is also 0?","Due to path independence the line integral of a conservative vector field for a closed loop is 0--assuming this is over some simply connected region. In my calculus text book it says that ""conservative"" implies that it follows the principles of conservation of energy, hence does this imply the inward and outward flux over the same closed loop is also 0?",,"['calculus', 'multivariable-calculus']"
22,Is $\vec{F}\cdot \hat{n}$ zero on the boundary?,Is  zero on the boundary?,\vec{F}\cdot \hat{n},"I'm trying to do the following exercise Let $\Omega\subset\mathbb R^3$ be a connected bounded subset with differentiable boundary $\partial\Omega$ . If the divergence of $F:\mathbb R^3 \rightarrow \mathbb R^3$ is zero in $\Omega$ , then $$ \int_\Omega F^TJ^TFdx=0, $$ where $J$ means the jacobian of $F$ and $J^T$ the transpose matrix of $J$ . I have asked a solution for it here and @MarkViola has proved that the integral is equal to $$\int_{\partial\Omega} (F\cdot F)(F\cdot n) \, dS , $$ where $n$ is the normal vector to the surface defined by the boundary. Now, why is this integral also zero??? I can see that if $\operatorname{div}F=0$ on $\Omega$ then $$\int_{\partial \Omega} F\cdot n \, dS = 0. $$ But, does it imply that $F\cdot n =0$ on the boundary? If not, why the integral $$\int_{\partial\Omega} (F\cdot F)(F\cdot n) \, dS $$ is equal to zero? Remark I should remark that @Mark also gave an example for which $F\cdot n$ is not zero on the boundary and the integral is. The only problem I see in that case is that the boundary is $C^1$ a.e. but not actually $C^1$ .","I'm trying to do the following exercise Let be a connected bounded subset with differentiable boundary . If the divergence of is zero in , then where means the jacobian of and the transpose matrix of . I have asked a solution for it here and @MarkViola has proved that the integral is equal to where is the normal vector to the surface defined by the boundary. Now, why is this integral also zero??? I can see that if on then But, does it imply that on the boundary? If not, why the integral is equal to zero? Remark I should remark that @Mark also gave an example for which is not zero on the boundary and the integral is. The only problem I see in that case is that the boundary is a.e. but not actually .","\Omega\subset\mathbb R^3 \partial\Omega F:\mathbb R^3 \rightarrow \mathbb R^3 \Omega  \int_\Omega F^TJ^TFdx=0,  J F J^T J \int_{\partial\Omega} (F\cdot F)(F\cdot n) \, dS ,  n \operatorname{div}F=0 \Omega \int_{\partial \Omega} F\cdot n \, dS = 0.  F\cdot n =0 \int_{\partial\Omega} (F\cdot F)(F\cdot n) \, dS  F\cdot n C^1 C^1","['multivariable-calculus', 'definite-integrals']"
23,"Finding min and max of $f(x,y)=-(x-y)^2+x$ in the region $(x,y)\in [0,1]\times[0,1]$.",Finding min and max of  in the region .,"f(x,y)=-(x-y)^2+x (x,y)\in [0,1]\times[0,1]","I have a two variable function:  $$f(x,y)=-(x-y)^2+x\, .$$ I need to find its absolute minimum and maximum under the constraints: $(x,y)\in [0,1]\times[0,1]$. The partial derivatives of$f$ do not vanish in the same points and then the maximum (and minimum) have to be to find on the edge: $E_1=\{x\in[0,1], y=0\}$, $E_2=\{x\in[0,1], y=1\}$, $E_3=\{y\in[0,1], x=0\}$, $E_4=\{y\in[0,1], x=1\}$. Then we must consider $f(x,0)$, $f(x,1)$ etc and find min and max of them. My problems are in the corners of the square $(x,y)\in [0,1]\times[0,1]$. What shall I say about $(0,0)$, $(0,1)$, $(1,0)$ and $(1,1)$? Thanks in advance","I have a two variable function:  $$f(x,y)=-(x-y)^2+x\, .$$ I need to find its absolute minimum and maximum under the constraints: $(x,y)\in [0,1]\times[0,1]$. The partial derivatives of$f$ do not vanish in the same points and then the maximum (and minimum) have to be to find on the edge: $E_1=\{x\in[0,1], y=0\}$, $E_2=\{x\in[0,1], y=1\}$, $E_3=\{y\in[0,1], x=0\}$, $E_4=\{y\in[0,1], x=1\}$. Then we must consider $f(x,0)$, $f(x,1)$ etc and find min and max of them. My problems are in the corners of the square $(x,y)\in [0,1]\times[0,1]$. What shall I say about $(0,0)$, $(0,1)$, $(1,0)$ and $(1,1)$? Thanks in advance",,"['multivariable-calculus', 'optimization']"
24,Area of a parametric surface,Area of a parametric surface,,"Given the parametric surface $\Phi(\rho,\theta)=(\rho\cos\theta,\rho\sin\theta,\theta)$, $\rho\in]0,1]$, $\theta\in[0,4\pi]$ find its area. This is a kind of exercise i might find in my multivariable calculus exam. I'm struggling to find a way to solve it because there's no resolution method mentioned in the study material my prof gave me for this kind of exercise . I guess i need to do some sort of integration to solve this, but i don't know which one to look up for. I think an example could help me understand.","Given the parametric surface $\Phi(\rho,\theta)=(\rho\cos\theta,\rho\sin\theta,\theta)$, $\rho\in]0,1]$, $\theta\in[0,4\pi]$ find its area. This is a kind of exercise i might find in my multivariable calculus exam. I'm struggling to find a way to solve it because there's no resolution method mentioned in the study material my prof gave me for this kind of exercise . I guess i need to do some sort of integration to solve this, but i don't know which one to look up for. I think an example could help me understand.",,"['integration', 'multivariable-calculus', 'area']"
25,Check the continuity of a function of two variables,Check the continuity of a function of two variables,,"I've been trying to check the continuity of the following function: $$ f(x,y) =  \begin{cases}     \frac {(x-1)(y-4)^2}{(x-1)^2+\sin(y-4)}      & \quad \text{(x,y) } \ne \text{ (1,4)}\\     0  & \quad \text{(x,y) } = \text{ (1,4)}   \end{cases} $$ I've tried calculating the following $lim$ , as $t = x-1$ , and $ z = y-4 $ : $$ \lim_{{(t,z)\to(0,0)}}{ \frac {tz^2}{t^2+\sin(z)}} $$ I've tried choosing different paths: $ t=z$ and $t=z^2$ but both gave me the same result - $0$ . I'm not sure how to prove or disprove that the limit is $0$ . I'd appreciate your help, thanks!","I've been trying to check the continuity of the following function: $$ f(x,y) =  \begin{cases}     \frac {(x-1)(y-4)^2}{(x-1)^2+\sin(y-4)}      & \quad \text{(x,y) } \ne \text{ (1,4)}\\     0  & \quad \text{(x,y) } = \text{ (1,4)}   \end{cases} $$ I've tried calculating the following $lim$ , as $t = x-1$ , and $ z = y-4 $ : $$ \lim_{{(t,z)\to(0,0)}}{ \frac {tz^2}{t^2+\sin(z)}} $$ I've tried choosing different paths: $ t=z$ and $t=z^2$ but both gave me the same result - $0$ . I'm not sure how to prove or disprove that the limit is $0$ . I'd appreciate your help, thanks!",,"['limits', 'functions', 'multivariable-calculus', 'continuity']"
26,On the integral $\iint_{\partial B_1(0)}\frac{d\Omega}{\left(1+a\cdot r\right)\left(1+b\cdot r\right)}$,On the integral,\iint_{\partial B_1(0)}\frac{d\Omega}{\left(1+a\cdot r\right)\left(1+b\cdot r\right)},"Let $a, b$ be arbitrary vectors in $\mathbb{R}^3$ and define $\partial B_1(0):=\{r\in\mathbb{R}^3:|r|=1\}$ as the boundary of the three-dimensional unit sphere. I am trying to evaluate the following integral: $$I = \iint_{\partial B_1(0)}\frac{d\Omega}{\left(1+a\cdot r\right)\left(1+b\cdot r\right)}$$ So far, I've said that we can, given a vector $a$ and a fixed vector $b$, write the following: $$a = c b + x$$ where $c = \frac{a\cdot b}{|b|}$ and $x\in\mathbb{R}^3$ such that $x$ orthogonal to $b$. Then, if we choose our coordinate system so that the positive $z$-axis runs parallel to the vector $b$, then, considering spherical coordinates, the angle between $r$ and $b$ is given by $\theta$, and we have: \begin{align} I = \int_0^\pi d\theta\sin\theta\int_0^{2\pi}d\phi\frac{1}{\left(1+|a|\cos\theta\right)\left(1+c\cos\theta + x\cdot r\right)} \end{align} Now, I know that, since $x\cdot b = 0$, $x$ must lie in the $x-y-$plane, so we can express it as $(\rho\cos\zeta, \rho\sin\zeta, 0)$ for some modulus $\rho$ and some polar angle $\zeta$. Thus: $$x\cdot r = \rho\sin\theta\left(\cos\zeta\cos\phi+\sin\zeta\sin\phi\right) = \rho\sin\theta\cos\left(\phi-\zeta\right)$$ Letting $\phi-\zeta=:\xi$, we have $d\xi = d\phi$ and thus, since all periods of the cosine function are equal, we have: \begin{align} I&=\int_0^\pi d\theta\frac{\sin\theta}{1+|a|\cos\theta}\int_{-\zeta}^{2\pi-\zeta}\frac{d\xi}{1+c\cos\theta+\rho\sin\theta\cos\xi}\\ &=\int_{-1}^1 \frac{du}{1+|a|u}\int_{0}^{2\pi}\frac{d\xi}{1+cu+\rho\sqrt{1-u^2}\cos\xi} \end{align} And this is where I'm stumped. I've been thinking of maybe converting the second integral into a contour integral over the unit circle, which reduces to evaluating $$\oint_{|z|=1}\frac{dz}{z^2+\alpha z + 1}$$ where $\alpha = 2\frac{1+cu}{\rho\sqrt{1-u^2}}$, but this then leads to a very complicated dependancy of the residues on $u$, so I was maybe wondering if there was a simpler way. Is this even the right way to attack the integral $I$? Or is $I$ perhaps so complicated that it admits no simpler solution?","Let $a, b$ be arbitrary vectors in $\mathbb{R}^3$ and define $\partial B_1(0):=\{r\in\mathbb{R}^3:|r|=1\}$ as the boundary of the three-dimensional unit sphere. I am trying to evaluate the following integral: $$I = \iint_{\partial B_1(0)}\frac{d\Omega}{\left(1+a\cdot r\right)\left(1+b\cdot r\right)}$$ So far, I've said that we can, given a vector $a$ and a fixed vector $b$, write the following: $$a = c b + x$$ where $c = \frac{a\cdot b}{|b|}$ and $x\in\mathbb{R}^3$ such that $x$ orthogonal to $b$. Then, if we choose our coordinate system so that the positive $z$-axis runs parallel to the vector $b$, then, considering spherical coordinates, the angle between $r$ and $b$ is given by $\theta$, and we have: \begin{align} I = \int_0^\pi d\theta\sin\theta\int_0^{2\pi}d\phi\frac{1}{\left(1+|a|\cos\theta\right)\left(1+c\cos\theta + x\cdot r\right)} \end{align} Now, I know that, since $x\cdot b = 0$, $x$ must lie in the $x-y-$plane, so we can express it as $(\rho\cos\zeta, \rho\sin\zeta, 0)$ for some modulus $\rho$ and some polar angle $\zeta$. Thus: $$x\cdot r = \rho\sin\theta\left(\cos\zeta\cos\phi+\sin\zeta\sin\phi\right) = \rho\sin\theta\cos\left(\phi-\zeta\right)$$ Letting $\phi-\zeta=:\xi$, we have $d\xi = d\phi$ and thus, since all periods of the cosine function are equal, we have: \begin{align} I&=\int_0^\pi d\theta\frac{\sin\theta}{1+|a|\cos\theta}\int_{-\zeta}^{2\pi-\zeta}\frac{d\xi}{1+c\cos\theta+\rho\sin\theta\cos\xi}\\ &=\int_{-1}^1 \frac{du}{1+|a|u}\int_{0}^{2\pi}\frac{d\xi}{1+cu+\rho\sqrt{1-u^2}\cos\xi} \end{align} And this is where I'm stumped. I've been thinking of maybe converting the second integral into a contour integral over the unit circle, which reduces to evaluating $$\oint_{|z|=1}\frac{dz}{z^2+\alpha z + 1}$$ where $\alpha = 2\frac{1+cu}{\rho\sqrt{1-u^2}}$, but this then leads to a very complicated dependancy of the residues on $u$, so I was maybe wondering if there was a simpler way. Is this even the right way to attack the integral $I$? Or is $I$ perhaps so complicated that it admits no simpler solution?",,"['integration', 'multivariable-calculus', 'contour-integration']"
27,"$\lim_{(x,y)\rightarrow0} \frac{x\ln(1+x^3)}{y(x^2+y^2)}$ doesn't exist (?)",doesn't exist (?),"\lim_{(x,y)\rightarrow0} \frac{x\ln(1+x^3)}{y(x^2+y^2)}","Is what I did correct? We have: $\lim_{(x,y)\rightarrow0} \frac{x\ln(1+x^3)}{y(x^2+y^2)}$ We know that as $x\rightarrow 0$ $\frac{\ln(1+x^3)}{x^3}=1$ , can be proven using taylor series.. So we have: $\lim_{(x,y)\rightarrow0} \frac{\ln(1+x^3)}{x^3} \frac{x^4}{yx^2+y^3}$ Pose $y=Cx^2$ we get: $$\lim_{(x,y)\rightarrow0} \frac{\ln(1+x^3)}{x^3} \frac{x^4}{Cx^4+C^3x^6}=\lim_{(x,y)\rightarrow0} \frac{\ln(1+x^3)}{x^3} \frac{x^4}{x^4(C+C^3x^2)}=\lim_{(x,y)\rightarrow0} \frac{\ln(1+x^3)}{x^3} \frac{1}{(C+C^3x^2)}=\frac{1}{(C+C^3x^2)}$$ Which is dependent on the constant $C$, Hence the limit doesn't exist. What i'm doubtful about is the usage of $\lim_{x\rightarrow 0}\frac{\ln(1+x^3)}{x^3}=1$ in double variable limits, What i thought was since $x$ is still tending towards zero, and these terms are independent of y I can use it. Thanks for your help!","Is what I did correct? We have: $\lim_{(x,y)\rightarrow0} \frac{x\ln(1+x^3)}{y(x^2+y^2)}$ We know that as $x\rightarrow 0$ $\frac{\ln(1+x^3)}{x^3}=1$ , can be proven using taylor series.. So we have: $\lim_{(x,y)\rightarrow0} \frac{\ln(1+x^3)}{x^3} \frac{x^4}{yx^2+y^3}$ Pose $y=Cx^2$ we get: $$\lim_{(x,y)\rightarrow0} \frac{\ln(1+x^3)}{x^3} \frac{x^4}{Cx^4+C^3x^6}=\lim_{(x,y)\rightarrow0} \frac{\ln(1+x^3)}{x^3} \frac{x^4}{x^4(C+C^3x^2)}=\lim_{(x,y)\rightarrow0} \frac{\ln(1+x^3)}{x^3} \frac{1}{(C+C^3x^2)}=\frac{1}{(C+C^3x^2)}$$ Which is dependent on the constant $C$, Hence the limit doesn't exist. What i'm doubtful about is the usage of $\lim_{x\rightarrow 0}\frac{\ln(1+x^3)}{x^3}=1$ in double variable limits, What i thought was since $x$ is still tending towards zero, and these terms are independent of y I can use it. Thanks for your help!",,"['limits', 'multivariable-calculus']"
28,Rotation of a curl?,Rotation of a curl?,,"Given an arbitrary constant 3-D rotation matrix $\mathbf{R}$ and a 3-D vector field $\mathbf{A}$, how can I find the vector field $\mathbf{B}$ such that $$\nabla\times\mathbf{B}=\mathbf{R}\left(\nabla\times\mathbf{A}\right)$$ ? I followed two ways, without really finding a way out I wrote the LHS and RHS explicitly. If I write $\mathbf{R}$ as the matrix $$\mathbf{R} = \begin{pmatrix}a & b & c\\ d & e & f\\ g & h & i \end{pmatrix}$$ (which is obviously an overkill, since the actual degrees of freedom of an arbitrary rotation are just three), then my equation looks like the nasty \begin{align*} \partial_{y}B_{z}-\partial_{z}B_{y} & =a\left(\partial_{y}A_{z}-\partial_{z}A_{y}\right)+b\left(\partial_{z}A_{x}-\partial_{x}A_{z}\right)+c\left(\partial_{x}A_{y}-\partial_{y}A_{x}\right)\\ \partial_{z}B_{x}-\partial_{x}B_{z} & =d\left(\partial_{y}A_{z}-\partial_{z}A_{y}\right)+e\left(\partial_{z}A_{x}-\partial_{x}A_{z}\right)+f\left(\partial_{x}A_{y}-\partial_{y}A_{x}\right)\\ \partial_{x}B_{y}-\partial_{y}B_{x} & =g\left(\partial_{y}A_{z}-\partial_{z}A_{y}\right)+h\left(\partial_{z}A_{x}-\partial_{x}A_{z}\right)+i\left(\partial_{x}A_{y}-\partial_{y}A_{x}\right) \end{align*} which intimidates me quite a bit... Rearranging it acquires an interesting structure, \begin{align*} \partial_{y}B_{z}-\partial_{z}B_{y} & =\partial_{x}\left(cA_{y}-bA_{z}\right)+\partial_{y}\left(aA_{z}-cA_{x}\right)+\partial_{z}\left(bA_{x}-aA_{y}\right)\\ \partial_{z}B_{x}-\partial_{x}B_{z} & =\partial_{x}\left(fA_{y}-eA_{z}\right)+\partial_{y}\left(dA_{z}-fA_{x}\right)+\partial_{z}\left(eA_{x}-dA_{y}\right)\\ \partial_{x}B_{y}-\partial_{y}B_{x} & =\partial_{x}\left(iA_{y}-hA_{z}\right)+\partial_{y}\left(gA_{z}-iA_{x}\right)+\partial_{z}\left(hA_{x}-gA_{y}\right) \end{align*} but I don't really know how to continue.. I don't think that replacing the actual rotation matrix elements would even help.. Then I tried a different, let's say geometric, way. I Fourier transformed both sides of my equation and, as (1) the Fourier transform of the $\nabla$ operator is $i\mathbf{k}$ and (2) the Fourier transform commutes with a rotation matrix, I obtained $$i\mathbf{k}\times\tilde{\mathbf{B}}=\mathbf{R}\left(i\mathbf{k}\times\tilde{\mathbf{A}}\right)$$ which is a nice expression involving just vectors and not nasty partial derivatives. Now, I can rewrite this as $$\mathbf{k}\times\tilde{\mathbf{B}}=\mathbf{R}\left(\mathbf{k}\right)\times\mathbf{R}\left(\tilde{\mathbf{A}}\right)$$ but this expression, although simple and nice, does not really help me find $\mathbf{B}$... Perhaps I am just rusted and I am missing something simple....","Given an arbitrary constant 3-D rotation matrix $\mathbf{R}$ and a 3-D vector field $\mathbf{A}$, how can I find the vector field $\mathbf{B}$ such that $$\nabla\times\mathbf{B}=\mathbf{R}\left(\nabla\times\mathbf{A}\right)$$ ? I followed two ways, without really finding a way out I wrote the LHS and RHS explicitly. If I write $\mathbf{R}$ as the matrix $$\mathbf{R} = \begin{pmatrix}a & b & c\\ d & e & f\\ g & h & i \end{pmatrix}$$ (which is obviously an overkill, since the actual degrees of freedom of an arbitrary rotation are just three), then my equation looks like the nasty \begin{align*} \partial_{y}B_{z}-\partial_{z}B_{y} & =a\left(\partial_{y}A_{z}-\partial_{z}A_{y}\right)+b\left(\partial_{z}A_{x}-\partial_{x}A_{z}\right)+c\left(\partial_{x}A_{y}-\partial_{y}A_{x}\right)\\ \partial_{z}B_{x}-\partial_{x}B_{z} & =d\left(\partial_{y}A_{z}-\partial_{z}A_{y}\right)+e\left(\partial_{z}A_{x}-\partial_{x}A_{z}\right)+f\left(\partial_{x}A_{y}-\partial_{y}A_{x}\right)\\ \partial_{x}B_{y}-\partial_{y}B_{x} & =g\left(\partial_{y}A_{z}-\partial_{z}A_{y}\right)+h\left(\partial_{z}A_{x}-\partial_{x}A_{z}\right)+i\left(\partial_{x}A_{y}-\partial_{y}A_{x}\right) \end{align*} which intimidates me quite a bit... Rearranging it acquires an interesting structure, \begin{align*} \partial_{y}B_{z}-\partial_{z}B_{y} & =\partial_{x}\left(cA_{y}-bA_{z}\right)+\partial_{y}\left(aA_{z}-cA_{x}\right)+\partial_{z}\left(bA_{x}-aA_{y}\right)\\ \partial_{z}B_{x}-\partial_{x}B_{z} & =\partial_{x}\left(fA_{y}-eA_{z}\right)+\partial_{y}\left(dA_{z}-fA_{x}\right)+\partial_{z}\left(eA_{x}-dA_{y}\right)\\ \partial_{x}B_{y}-\partial_{y}B_{x} & =\partial_{x}\left(iA_{y}-hA_{z}\right)+\partial_{y}\left(gA_{z}-iA_{x}\right)+\partial_{z}\left(hA_{x}-gA_{y}\right) \end{align*} but I don't really know how to continue.. I don't think that replacing the actual rotation matrix elements would even help.. Then I tried a different, let's say geometric, way. I Fourier transformed both sides of my equation and, as (1) the Fourier transform of the $\nabla$ operator is $i\mathbf{k}$ and (2) the Fourier transform commutes with a rotation matrix, I obtained $$i\mathbf{k}\times\tilde{\mathbf{B}}=\mathbf{R}\left(i\mathbf{k}\times\tilde{\mathbf{A}}\right)$$ which is a nice expression involving just vectors and not nasty partial derivatives. Now, I can rewrite this as $$\mathbf{k}\times\tilde{\mathbf{B}}=\mathbf{R}\left(\mathbf{k}\right)\times\mathbf{R}\left(\tilde{\mathbf{A}}\right)$$ but this expression, although simple and nice, does not really help me find $\mathbf{B}$... Perhaps I am just rusted and I am missing something simple....",,"['multivariable-calculus', 'vector-analysis', 'curl']"
29,"What does $D_{12}f$ mean compared to $D_{1}f$, for example?","What does  mean compared to , for example?",D_{12}f D_{1}f,"What does $D_{12}f$ mean in the context of partial derivatives, with $f: \mathbb{R}^2 \to \mathbb{R}$, for example? This is used in Rudin's Principles of Mathematical Analysis. It is clear to me what that $D_1 f$ is the partial derivative of $f$ with respect to $x$. Is $D_{12}f$ a row vector $ \begin{bmatrix} D_1f & D_2f \end{bmatrix} $?","What does $D_{12}f$ mean in the context of partial derivatives, with $f: \mathbb{R}^2 \to \mathbb{R}$, for example? This is used in Rudin's Principles of Mathematical Analysis. It is clear to me what that $D_1 f$ is the partial derivative of $f$ with respect to $x$. Is $D_{12}f$ a row vector $ \begin{bmatrix} D_1f & D_2f \end{bmatrix} $?",,"['multivariable-calculus', 'notation', 'partial-derivative']"
30,True or False: No Parametrized Surface is Closed in $\mathbb{R}^3$,True or False: No Parametrized Surface is Closed in,\mathbb{R}^3,"Is no parametrized surface (topologically) closed in $\mathbb{R}^3$? I use the definition of Fitzpatrick's Advanced Calculus (2009) . So far, all the examples I've seen have not been closed -- for example, $\{(x,y,z)\in\mathbb{R}^3:x^2+y^2+z^2=1,z>0\}$ or $\{(x,y,z)\in\mathbb{R}^3:x^2+y^2=z^2,z>0,(x-1)^2+y^2<1\}.$","Is no parametrized surface (topologically) closed in $\mathbb{R}^3$? I use the definition of Fitzpatrick's Advanced Calculus (2009) . So far, all the examples I've seen have not been closed -- for example, $\{(x,y,z)\in\mathbb{R}^3:x^2+y^2+z^2=1,z>0\}$ or $\{(x,y,z)\in\mathbb{R}^3:x^2+y^2=z^2,z>0,(x-1)^2+y^2<1\}.$",,"['general-topology', 'multivariable-calculus', 'surfaces']"
31,How to pullback a form,How to pullback a form,,"I am trying to make sense of pullbacks of forms as described in Guillemin and Pollack. If $f:X\rightarrow Y$ is a smooth map and $\omega$ is a $p$-form on $Y$, we define a $p$-form $f^*\omega$ on $X$ as follows. If $f(x)=y$, then $f$ induces a derivative map $df_x:T_x(X)\rightarrow T_y(Y)$. Since $\omega(y)$ is an alternating $p$-tensor on $T_y(Y)$, we can pull it back to $T_x(X)$ using the transpose $(df_x)^*$. We then define the pullback of $\omega$ by $f$ by $f^*\omega(x)=(df_x)^*\omega[f(x)]$. (Note: In general, we define the transpose as $A^*T(v_1,...,v_p)=T(Av_1,...,Av_p)$) I am having trouble actually applying this definition to real examples. Guillemin and Pollack give none. For example, if $f:(\pi/2,\pi/2)\rightarrow \mathbb{R}$ is given by $f(t)=\sin t$ and $\omega=dx/\sqrt{1-x^2}$, then I begin as follows. We have that $df_t=f'(t)=\cos t$. Then $f^*\omega(t)=(\cos t)^*\omega$. Is this correct? How do I proceed?","I am trying to make sense of pullbacks of forms as described in Guillemin and Pollack. If $f:X\rightarrow Y$ is a smooth map and $\omega$ is a $p$-form on $Y$, we define a $p$-form $f^*\omega$ on $X$ as follows. If $f(x)=y$, then $f$ induces a derivative map $df_x:T_x(X)\rightarrow T_y(Y)$. Since $\omega(y)$ is an alternating $p$-tensor on $T_y(Y)$, we can pull it back to $T_x(X)$ using the transpose $(df_x)^*$. We then define the pullback of $\omega$ by $f$ by $f^*\omega(x)=(df_x)^*\omega[f(x)]$. (Note: In general, we define the transpose as $A^*T(v_1,...,v_p)=T(Av_1,...,Av_p)$) I am having trouble actually applying this definition to real examples. Guillemin and Pollack give none. For example, if $f:(\pi/2,\pi/2)\rightarrow \mathbb{R}$ is given by $f(t)=\sin t$ and $\omega=dx/\sqrt{1-x^2}$, then I begin as follows. We have that $df_t=f'(t)=\cos t$. Then $f^*\omega(t)=(\cos t)^*\omega$. Is this correct? How do I proceed?",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
32,Applying Green's theorem on scalar fields,Applying Green's theorem on scalar fields,,"Consider the vector field $\mathbf F =  \langle x+x^3e^y, -3x^2e^y \rangle$ and let $C$ be the circle $x^2 +y^2=5^2$. Let $\mathbf u$ be the unit vector $\dfrac{1}{\sqrt{x^2+y^2}}\langle x,y\rangle$ and $f(x,y) = \mathbf F  \cdot \mathbf u$. Evaluate the line integral: $$\int_C f(x,y) \,\mathrm{d}s,$$ where $\mathrm{d}s=\Vert\mathbf r'(t)\Vert \,\mathrm{d}t$. So I noticed that this is a integral for a scalar field, which then I decided to parameterise the curve $C:\mathbf r(t)=\langle 5\cos(t),5\sin(t)\rangle,t\in[0,2\pi]$, then $\mathbf r'(t)=\langle -5\sin(t),5\cos(t)\rangle $ and $\Vert\mathbf r'(t)\Vert=5$. So,$$ \int_C f(x,y)\,\mathrm{d}s = \int_{0}^{2\pi}\frac{1}{\sqrt{x^2+y^2}}\langle x,y\rangle\cdot\langle x+x^3e^y, -3x^2e^y\rangle \Vert\mathbf r'(t)\Vert\,\mathrm{d}t.$$ Simplification yields:$$ \int_{0}^{2\pi}(25\cos^2(t)+625\cos^4(t)e^{5\sin(t)} -375\cos^2(t)\sin(t)e^{5\sin(t)}) \,\mathrm{d}t,$$ which is kind of impossible to integrate. I am not sure if there is an easier way to do this question. Upon checking the answer I noticed that Green's theorem was used, I am not sure on how Green's theorem is applicable to a line integral on a scalar field. (I was only thought on how to apply it onto a vector field). Any help on this clarification is really appreciated!","Consider the vector field $\mathbf F =  \langle x+x^3e^y, -3x^2e^y \rangle$ and let $C$ be the circle $x^2 +y^2=5^2$. Let $\mathbf u$ be the unit vector $\dfrac{1}{\sqrt{x^2+y^2}}\langle x,y\rangle$ and $f(x,y) = \mathbf F  \cdot \mathbf u$. Evaluate the line integral: $$\int_C f(x,y) \,\mathrm{d}s,$$ where $\mathrm{d}s=\Vert\mathbf r'(t)\Vert \,\mathrm{d}t$. So I noticed that this is a integral for a scalar field, which then I decided to parameterise the curve $C:\mathbf r(t)=\langle 5\cos(t),5\sin(t)\rangle,t\in[0,2\pi]$, then $\mathbf r'(t)=\langle -5\sin(t),5\cos(t)\rangle $ and $\Vert\mathbf r'(t)\Vert=5$. So,$$ \int_C f(x,y)\,\mathrm{d}s = \int_{0}^{2\pi}\frac{1}{\sqrt{x^2+y^2}}\langle x,y\rangle\cdot\langle x+x^3e^y, -3x^2e^y\rangle \Vert\mathbf r'(t)\Vert\,\mathrm{d}t.$$ Simplification yields:$$ \int_{0}^{2\pi}(25\cos^2(t)+625\cos^4(t)e^{5\sin(t)} -375\cos^2(t)\sin(t)e^{5\sin(t)}) \,\mathrm{d}t,$$ which is kind of impossible to integrate. I am not sure if there is an easier way to do this question. Upon checking the answer I noticed that Green's theorem was used, I am not sure on how Green's theorem is applicable to a line integral on a scalar field. (I was only thought on how to apply it onto a vector field). Any help on this clarification is really appreciated!",,"['multivariable-calculus', 'vector-analysis', 'greens-theorem', 'scalar-fields']"
33,How to change the order of integration for this?,How to change the order of integration for this?,,"So I can change order of integration for simple functions through the use of diagram but how do i do it for $$\int_{0}^{\pi}\int_{0}^{\sin x}f(x, y)dydx?$$ So y goes from 0 to 1 but the functions needs to be split at $\pi/2$ when we consider the $x$ direction so in the $x$ direction it goes from $x=\sin^{-1}y$ to $\pi/2$ and then $\pi/2$ to $x=\sin^{-1}y$ or something? This question has been asked before but i don't think the asker does it the way i do so didn't see a good answer","So I can change order of integration for simple functions through the use of diagram but how do i do it for $$\int_{0}^{\pi}\int_{0}^{\sin x}f(x, y)dydx?$$ So y goes from 0 to 1 but the functions needs to be split at $\pi/2$ when we consider the $x$ direction so in the $x$ direction it goes from $x=\sin^{-1}y$ to $\pi/2$ and then $\pi/2$ to $x=\sin^{-1}y$ or something? This question has been asked before but i don't think the asker does it the way i do so didn't see a good answer",,"['integration', 'multivariable-calculus']"
34,Differential Operators as vectors,Differential Operators as vectors,,Do differential operators form a vector space and if so can we safely say that the del operator is a vector?,Do differential operators form a vector space and if so can we safely say that the del operator is a vector?,,"['multivariable-calculus', 'differential-geometry', 'vector-spaces', 'operator-theory']"
35,Find the mass of the parallelepipid,Find the mass of the parallelepipid,,"""A parallelepiped is described by the vectors $(2,2,3),(2,4,3)$ and $(0,1,5)$ given that the density $= 2x+2y$, find the mass of the parallelepiped."" I can find the volume just fine, but setting up the integral to find the mass is giving me a lot of trouble. this is my best guess so far... $$20 \int_0^2  \int_0^4 \int_0^5  (2x+2y) dzdxdy$$ The vectors do stem from the origin","""A parallelepiped is described by the vectors $(2,2,3),(2,4,3)$ and $(0,1,5)$ given that the density $= 2x+2y$, find the mass of the parallelepiped."" I can find the volume just fine, but setting up the integral to find the mass is giving me a lot of trouble. this is my best guess so far... $$20 \int_0^2  \int_0^4 \int_0^5  (2x+2y) dzdxdy$$ The vectors do stem from the origin",,['multivariable-calculus']
36,"Let $g$ be a function such that $\underline{\int_{y\in B}}f(x,y)\leq g(x)\leq \overline{\int_{y\in B}}f(x,y)$ for all $x\in A$. Show that if $f$",Let  be a function such that  for all . Show that if,"g \underline{\int_{y\in B}}f(x,y)\leq g(x)\leq \overline{\int_{y\in B}}f(x,y) x\in A f","(a) Exercise 1 of that section is : Let $f,g: Q\to \mathbb{R}$ be bounded function such that $f(x)\leq g(x)$ for $x\in Q$. Show that $\underline{\int_{Q}}f\leq \underline{\int_{Q}}g$ and $\overline{\int_{Q}}f\leq \overline{\int_{Q}}f$ , I think of using the exercise to conclude that $\underline{\int_{x\in A}}\underline{\int_{y\in B}}f(x,y)\leq \underline{\int_{x\in A}}g(x)$ and $\overline{\int_{x\in A}}g(x)\leq \overline{\int_{x\in A}}\overline{\int_{y\in B}}f(x,y)$ for all $x\in A$, But fubini's theorem tells me that $\int_{Q}f=\int_{x\in A}\underline{\int_{y\in B}}f(x,y)=\int_{x\in A}\overline{\int_{y\in B}}f(x,y)=\underline{\int_{x\in A}}\underline{\int_{y\in B}}f(x,y)\overline{\int_{x\in A}}\overline{\int_{y\in B}}f(x,y)$, with which $\overline{\int_{x\in A}}g(x)=\int_{Q}f=\underline{\int_{x\in A}}g(x)$, with this I could not conclude that $g$ is integrable over $A$? I need help for (b) and (c), could someone help me please? Thank you.","(a) Exercise 1 of that section is : Let $f,g: Q\to \mathbb{R}$ be bounded function such that $f(x)\leq g(x)$ for $x\in Q$. Show that $\underline{\int_{Q}}f\leq \underline{\int_{Q}}g$ and $\overline{\int_{Q}}f\leq \overline{\int_{Q}}f$ , I think of using the exercise to conclude that $\underline{\int_{x\in A}}\underline{\int_{y\in B}}f(x,y)\leq \underline{\int_{x\in A}}g(x)$ and $\overline{\int_{x\in A}}g(x)\leq \overline{\int_{x\in A}}\overline{\int_{y\in B}}f(x,y)$ for all $x\in A$, But fubini's theorem tells me that $\int_{Q}f=\int_{x\in A}\underline{\int_{y\in B}}f(x,y)=\int_{x\in A}\overline{\int_{y\in B}}f(x,y)=\underline{\int_{x\in A}}\underline{\int_{y\in B}}f(x,y)\overline{\int_{x\in A}}\overline{\int_{y\in B}}f(x,y)$, with which $\overline{\int_{x\in A}}g(x)=\int_{Q}f=\underline{\int_{x\in A}}g(x)$, with this I could not conclude that $g$ is integrable over $A$? I need help for (b) and (c), could someone help me please? Thank you.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'vector-analysis']"
37,In what sense is the length of a parameterized curve an area?,In what sense is the length of a parameterized curve an area?,,A little confusion on my part. Study of multi variable calculus and we are using the formula for length of a parameterized curve. The equation makes intuitive sense and I can work it OK. But I also recall using the same integral with out the parameterizing to find the length of a curve where the first term of the square root in just one. The former formula is the general case. Now for the question:  I had just previously used the integral for completing the quadrature i.e.  Find the area under a curve.   Is the single integral used for finding both area and length ?  I guess I am trying to unify the concepts in my mind to understand the context of how they are used and know the difference. Thank you.,A little confusion on my part. Study of multi variable calculus and we are using the formula for length of a parameterized curve. The equation makes intuitive sense and I can work it OK. But I also recall using the same integral with out the parameterizing to find the length of a curve where the first term of the square root in just one. The former formula is the general case. Now for the question:  I had just previously used the integral for completing the quadrature i.e.  Find the area under a curve.   Is the single integral used for finding both area and length ?  I guess I am trying to unify the concepts in my mind to understand the context of how they are used and know the difference. Thank you.,,"['integration', 'multivariable-calculus']"
38,Curves and Surfaces,Curves and Surfaces,,"Suppose $$S=\{(x,y,z) \in \mathbb{R}^3 : x^2+y^2+(z-1)^2=1\}$$ and                $$T=\left\{(x,y,z) \in \mathbb{R}^3 : \frac{(z+1)^2}{4}=x^2+y^2, z \geq-1 \right\}.$$ (1) Find the $z$-coordinates of the points of intersection of $S$ and $T$ and sketch the projection into the $xy$-plane of the curves of intersection. I tried to solve simultaneously and found that $z=1/5, 1$. Not sure if this is correct. Also am unsure about the sketch. Thanks!","Suppose $$S=\{(x,y,z) \in \mathbb{R}^3 : x^2+y^2+(z-1)^2=1\}$$ and                $$T=\left\{(x,y,z) \in \mathbb{R}^3 : \frac{(z+1)^2}{4}=x^2+y^2, z \geq-1 \right\}.$$ (1) Find the $z$-coordinates of the points of intersection of $S$ and $T$ and sketch the projection into the $xy$-plane of the curves of intersection. I tried to solve simultaneously and found that $z=1/5, 1$. Not sure if this is correct. Also am unsure about the sketch. Thanks!",,"['calculus', 'multivariable-calculus']"
39,Finding potential for a vector field,Finding potential for a vector field,,Find the potential function for the vector field $$\frac{16x}z\hat{\mathbf i}+\frac{18y}z\hat{\mathbf j}+\left(1-\frac{8x^2 +9y^2}{z^2}\right)\hat{\mathbf k}$$ which has no constant terms. So far I have $$f = \frac8z x^2 +\frac9z y^2 +g(z).$$ The $z$ term kinda messes things up and I'm not really able to get any further.,Find the potential function for the vector field which has no constant terms. So far I have The term kinda messes things up and I'm not really able to get any further.,\frac{16x}z\hat{\mathbf i}+\frac{18y}z\hat{\mathbf j}+\left(1-\frac{8x^2 +9y^2}{z^2}\right)\hat{\mathbf k} f = \frac8z x^2 +\frac9z y^2 +g(z). z,"['integration', 'multivariable-calculus', 'partial-derivative']"
40,Finding First Variation,Finding First Variation,,"Given the functional $ I(U) = \int_0^1 F(u'(x),u(x), x) dx$ and: $F(p,u,x)= \sqrt{p^2+u^2}$ how would you find the first variation using say the definition on Wikipedia (Gateaux Derivative) and what's the purpose of it? I know the Euler-Lagrange tell you the functions which make the given functional stationary, but I'm not quite certain how to relate these two ideas. Thanks","Given the functional $ I(U) = \int_0^1 F(u'(x),u(x), x) dx$ and: $F(p,u,x)= \sqrt{p^2+u^2}$ how would you find the first variation using say the definition on Wikipedia (Gateaux Derivative) and what's the purpose of it? I know the Euler-Lagrange tell you the functions which make the given functional stationary, but I'm not quite certain how to relate these two ideas. Thanks",,"['multivariable-calculus', 'calculus-of-variations', 'euler-lagrange-equation']"
41,Finding equations of all lines that lie entirely on a surface and passes through a point,Finding equations of all lines that lie entirely on a surface and passes through a point,,"Let $S$ be the surface $x^2 + y^2 - z^2 = 1$ and $P(1,1,-1)$ be a point on $S$. Find the equations for all lines passing through the point P which lie entirely on the surface $S$. At point $P$, the equation of the tangent plane of the surface $S$ is: $x + y + z =1$ (1) The line equation $L: r = <1 + at, 1 + bt, -1+ct>$ must satisfy both equation (1) and the equation of the surface. Substituting $L$ into the tangent plane equation yields $(a + b + c)t = 0$ While subsittuting $L$ into the surface equation yields $2(a + b -c) + (a^2 + b^2 -c^2)t = 0$ We must have $a + b +c =0 , a + b -c = 0, a^2 + b^2 -c^2 = 0$, which then solving yields $a = b = c = 0$. I might have done something wrong while working on this question, thanks @Arthur for the help and information for me to improve my question. Any help on this will be appreciated!","Let $S$ be the surface $x^2 + y^2 - z^2 = 1$ and $P(1,1,-1)$ be a point on $S$. Find the equations for all lines passing through the point P which lie entirely on the surface $S$. At point $P$, the equation of the tangent plane of the surface $S$ is: $x + y + z =1$ (1) The line equation $L: r = <1 + at, 1 + bt, -1+ct>$ must satisfy both equation (1) and the equation of the surface. Substituting $L$ into the tangent plane equation yields $(a + b + c)t = 0$ While subsittuting $L$ into the surface equation yields $2(a + b -c) + (a^2 + b^2 -c^2)t = 0$ We must have $a + b +c =0 , a + b -c = 0, a^2 + b^2 -c^2 = 0$, which then solving yields $a = b = c = 0$. I might have done something wrong while working on this question, thanks @Arthur for the help and information for me to improve my question. Any help on this will be appreciated!",,"['multivariable-calculus', 'surfaces']"
42,$\frac{\partial^2 u}{\partial x^2}=\frac{\partial^2 u}{\partial t^2}\implies \frac{\partial^2 v}{\partial \xi\partial \eta}=0$,,\frac{\partial^2 u}{\partial x^2}=\frac{\partial^2 u}{\partial t^2}\implies \frac{\partial^2 v}{\partial \xi\partial \eta}=0,"I'm reading ""Fourier analysis, an introduction"" by Elias M. And Rami Shakarchi and in chapter one it said: The function $u(x,t)$ satisfied the equation $$\dfrac{\partial^2 u}{\partial x^2}=\dfrac{\partial^2 u}{\partial t^2}$$ Now let's set $2$ new variables: $\xi=x+t$ and $\eta=x-t$. We now define the function $v$ such that $v(\xi,\eta)=u(x,t)$. I understand this but in the book from this they conclude $2$ things that I'm failed to understand how: $$\dfrac{\partial^2 v}{\partial \xi\partial \eta}=0$$ And $$v(\xi,\eta)=F(\xi)+G(\eta)$$($F,G$ are twice differentiable functions) How from the change of variables they got to this? I would be happy with either a hint or an answer, thank you very much.","I'm reading ""Fourier analysis, an introduction"" by Elias M. And Rami Shakarchi and in chapter one it said: The function $u(x,t)$ satisfied the equation $$\dfrac{\partial^2 u}{\partial x^2}=\dfrac{\partial^2 u}{\partial t^2}$$ Now let's set $2$ new variables: $\xi=x+t$ and $\eta=x-t$. We now define the function $v$ such that $v(\xi,\eta)=u(x,t)$. I understand this but in the book from this they conclude $2$ things that I'm failed to understand how: $$\dfrac{\partial^2 v}{\partial \xi\partial \eta}=0$$ And $$v(\xi,\eta)=F(\xi)+G(\eta)$$($F,G$ are twice differentiable functions) How from the change of variables they got to this? I would be happy with either a hint or an answer, thank you very much.",,"['multivariable-calculus', 'partial-derivative', 'change-of-variable']"
43,Derivative in the direction of u+v is equal to the sum of the derivative in the direction of u and the derivative in the direction of v,Derivative in the direction of u+v is equal to the sum of the derivative in the direction of u and the derivative in the direction of v,,"I am trying to prove that $D_{u+v}f(a)=D_{u}f(a)+D_{v}f(a)$, where $a \in \mathbb{R}^n$ for any two   vectors $u,v$ in $\mathbb{R}^n$. I try to use the definition and so forth but I could not prove it. I need to prove this only by using the definition, which is $$D_{v}f(a)=\lim_{t \to 0} \frac{f(a+tv)-f(a)}{t}$$.","I am trying to prove that $D_{u+v}f(a)=D_{u}f(a)+D_{v}f(a)$, where $a \in \mathbb{R}^n$ for any two   vectors $u,v$ in $\mathbb{R}^n$. I try to use the definition and so forth but I could not prove it. I need to prove this only by using the definition, which is $$D_{v}f(a)=\lim_{t \to 0} \frac{f(a+tv)-f(a)}{t}$$.",,"['calculus', 'multivariable-calculus', 'derivatives']"
44,How do I parametrise the surface $x^2 + y^2 + z^2 -2x =0$?,How do I parametrise the surface ?,x^2 + y^2 + z^2 -2x =0,How do I parametrise the surface $x^2 + y^2 + z^2 -2x =0$? This is the beginning of a question where we have to work out a tangent plane at a point. I understand the rest of this method but somehow cannot work out how to parametrise tricky surfaces. The other surfaces we have to parametrise are $x^2 + y^2 - z^2 = 2y + 2z$ where $−1 ≤ z ≤ 0$ and $(4 − x^2 + y^2)^2 + z^2 = 1$. Obviously I don't expect someone to parametrise all these surfaces for me but any help would be appreciated!,How do I parametrise the surface $x^2 + y^2 + z^2 -2x =0$? This is the beginning of a question where we have to work out a tangent plane at a point. I understand the rest of this method but somehow cannot work out how to parametrise tricky surfaces. The other surfaces we have to parametrise are $x^2 + y^2 - z^2 = 2y + 2z$ where $−1 ≤ z ≤ 0$ and $(4 − x^2 + y^2)^2 + z^2 = 1$. Obviously I don't expect someone to parametrise all these surfaces for me but any help would be appreciated!,,"['multivariable-calculus', 'surfaces']"
45,"Prove $\frac{x^{n}}{x+y^3}+\frac{y^{n}}{y+x^3} \geqslant \frac{2^{4-n}}{5}$ for $x, y > 0$ with $x+y=1$",Prove  for  with,"\frac{x^{n}}{x+y^3}+\frac{y^{n}}{y+x^3} \geqslant \frac{2^{4-n}}{5} x, y > 0 x+y=1","Positive real numbers $x$ , $y$ satisfy $x+y=1$ . For any integer $n \geqslant 2$ , show that $$\frac{x^{n}}{x+y^3}+\frac{y^{n}}{y+x^3} \geqslant \frac{2^{4-n}}{5}.$$ So far, I've tried some elementary ways, such as AM-GM, Holder, induction. Maybe certain method mentioned above is feasible, but I've made little progress, please help.","Positive real numbers , satisfy . For any integer , show that So far, I've tried some elementary ways, such as AM-GM, Holder, induction. Maybe certain method mentioned above is feasible, but I've made little progress, please help.",x y x+y=1 n \geqslant 2 \frac{x^{n}}{x+y^3}+\frac{y^{n}}{y+x^3} \geqslant \frac{2^{4-n}}{5}.,"['algebra-precalculus', 'multivariable-calculus', 'inequality', 'polynomials', 'jensen-inequality']"
46,If functions agree at all but finitely many points then the integrals are the same,If functions agree at all but finitely many points then the integrals are the same,,"Exercise 3-2 from Calculus on Manifolds by Spivak: Let $A\subset R^n,\ f:A\rightarrow R$ an integrable (in the sense of   Darboux) function. Let $g=f$ except at finitely many points. Prove   that $g$ is also integrable and $\int_Af=\int_A g$. Note that there is a similar question show that $g$ is integrable, with $f=g$ except in finite set and $f$ integrable but the answers assume the knowledge of measure theory whereas Spivak doesn't. I guess I need to use the criterion saying that $f$ is integrable iff there is a partition $P$ of $A$ such that $U(f,P)-L(f,P)< \epsilon$ for any $\epsilon < 0$. But I don't know how to apply it to both functions. I thought about considering $f-g$ (which should be integrable except finitely many points), but Spivak doesn't even state that the sum of two integrable functions is integrable, so perhaps I'm not supposed to use this. (Even if I consider $f-g$, I don't know how to proceed).","Exercise 3-2 from Calculus on Manifolds by Spivak: Let $A\subset R^n,\ f:A\rightarrow R$ an integrable (in the sense of   Darboux) function. Let $g=f$ except at finitely many points. Prove   that $g$ is also integrable and $\int_Af=\int_A g$. Note that there is a similar question show that $g$ is integrable, with $f=g$ except in finite set and $f$ integrable but the answers assume the knowledge of measure theory whereas Spivak doesn't. I guess I need to use the criterion saying that $f$ is integrable iff there is a partition $P$ of $A$ such that $U(f,P)-L(f,P)< \epsilon$ for any $\epsilon < 0$. But I don't know how to apply it to both functions. I thought about considering $f-g$ (which should be integrable except finitely many points), but Spivak doesn't even state that the sum of two integrable functions is integrable, so perhaps I'm not supposed to use this. (Even if I consider $f-g$, I don't know how to proceed).",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
47,Formula involving cross products $\nabla \times (F \times G)$?,Formula involving cross products ?,\nabla \times (F \times G),"I was knowing this formula for vectors $a,b,c$ : $a \times (b \times c) = (a\cdot c)b - (a\cdot b)c$ But I was thinking of the differential operator as a vector and hence I thought the above formula would also work for $\nabla \times (F \times G) $ but it did not rather we have the following: $\nabla \times (F \times G)= (\nabla \cdot  G)F - (F\cdot \nabla)G + (G\cdot \nabla)F - (\nabla\cdot F)G$",I was knowing this formula for vectors : But I was thinking of the differential operator as a vector and hence I thought the above formula would also work for but it did not rather we have the following:,"a,b,c a \times (b \times c) = (a\cdot c)b - (a\cdot b)c \nabla \times (F \times G)  \nabla \times (F \times G)= (\nabla \cdot  G)F - (F\cdot \nabla)G + (G\cdot \nabla)F - (\nabla\cdot F)G","['multivariable-calculus', 'vector-analysis']"
48,Directional derivative of a function (Munkres),Directional derivative of a function (Munkres),,"Let $A\in \mathbb{R^n}$; let $f:A\rightarrow \mathbb R^n.$ Show that if $f'(a,u)$ exists, then $f'(a,cu)$ exists and equals $cf'(a,u).$ This exercise is from Munkres. I suppose $a\in \mathbb R^n$ and $c\in \mathbb R.$ I tried to calculate $f'(a,cu)$ and I got this: $f'(a,cu)=\lim_{t\to 0}\frac{ f(a+t(cu))-f(a)}{t}=\lim_{t\to 0}\frac{ f(a+(tc)u)-f(a)}{t}$ I took $r=ct.$ Then, $r\to 0$ since $t\to 0.$ Then $\lim_{t\to 0}\frac{ f(a+(tc)u)-f(a)}{t}=\lim_{r\to 0}\frac{ f(a+ru)-f(a)}{r}$ and the last limit exists since $f'(a,u)$ does. Is my argument correct? And how can I show the equality? Thanks in advance!","Let $A\in \mathbb{R^n}$; let $f:A\rightarrow \mathbb R^n.$ Show that if $f'(a,u)$ exists, then $f'(a,cu)$ exists and equals $cf'(a,u).$ This exercise is from Munkres. I suppose $a\in \mathbb R^n$ and $c\in \mathbb R.$ I tried to calculate $f'(a,cu)$ and I got this: $f'(a,cu)=\lim_{t\to 0}\frac{ f(a+t(cu))-f(a)}{t}=\lim_{t\to 0}\frac{ f(a+(tc)u)-f(a)}{t}$ I took $r=ct.$ Then, $r\to 0$ since $t\to 0.$ Then $\lim_{t\to 0}\frac{ f(a+(tc)u)-f(a)}{t}=\lim_{r\to 0}\frac{ f(a+ru)-f(a)}{r}$ and the last limit exists since $f'(a,u)$ does. Is my argument correct? And how can I show the equality? Thanks in advance!",,"['real-analysis', 'analysis', 'multivariable-calculus']"
49,"Double integral to polar coordinates, bounds","Double integral to polar coordinates, bounds",,"I want to find $\int_0^2 \int_ {\sqrt{2x-x^2}}^{\sqrt{4-x^2}} \sqrt{4-x^2-y^2}dydx$. My idea is to do a transformation to polar coordinates; $\iint \sqrt{4-r^2} rdrd\theta$, but I'm unsure about the bounds. Plotting the bounds in $f(x,y)$ I realize I'm dealing with the area between a smaller circle centered at (1, 0) and r=1 and a bigger circle centered at (0,0) with r = 2. (Under the ball $\sqrt{4-x^2-y^2}$ Any ideas? Thank you","I want to find $\int_0^2 \int_ {\sqrt{2x-x^2}}^{\sqrt{4-x^2}} \sqrt{4-x^2-y^2}dydx$. My idea is to do a transformation to polar coordinates; $\iint \sqrt{4-r^2} rdrd\theta$, but I'm unsure about the bounds. Plotting the bounds in $f(x,y)$ I realize I'm dealing with the area between a smaller circle centered at (1, 0) and r=1 and a bigger circle centered at (0,0) with r = 2. (Under the ball $\sqrt{4-x^2-y^2}$ Any ideas? Thank you",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
50,Maxima and minima of function with limiting condition (lagrange multiplier),Maxima and minima of function with limiting condition (lagrange multiplier),,"problem I have function: $$ f(x,y,z)=x+2y+\frac{z^2}{2} $$ and i would want to find it's minima and maxima with condition $x^2+y^2+z^2=5$ Attempt to solve I can form Lagrange function with the limiting condition and the function itself. Lagrange function is defined as $$ L(x,y,z,\lambda)=f(x)+\lambda(g(x)) $$ $g(x)$ would be limiting condition. $$ L(x,y,z,\lambda)= x+2y+\frac{z^2}{2} + \lambda(x^2+y^2+z^2-5)$$ in order to find when this function has critical points i need to compute gradient. $$ \nabla L(x,y,\lambda)=\begin{bmatrix} 1+2x\lambda \\ 2+2y\lambda \\ z+2z\lambda \\ x^2+y^2+z^2-5 \end{bmatrix} $$ $$ \begin{cases} 1+2x\lambda = 0 \\ 2+2y\lambda = 0 \\ z+2z\lambda =0 \\ x^2+y^2+z^2-5=0\end{cases} $$ $$ x=-\frac{1}{2\lambda} $$ $$ y=-\frac{2}{2\lambda} $$ $$ z=0 $$ $$ (-\frac{1}{2\lambda})^2+(-\frac{2}{2\lambda})^2+(0)^2-5=0 $$ $$ \frac{1}{4\lambda^2} + \frac{4}{4\lambda^2}-5=0 $$ $$ \frac{5}{4\lambda^2}-5=0 $$ $$ \frac{5}{4\lambda^2}=5 $$ $$ 20\lambda^2=5 $$ $$ \lambda = \sqrt{\frac{5}{20}}=\sqrt{\frac{1}{4}}=\frac{\sqrt{1}}{\sqrt{4}}=\frac{1}{2}$$ since for all $n\ge 0$  $\sqrt{\frac{m}{n}}=\frac{\sqrt{m}}{\sqrt{n}}$ Now i can compute values for $x$ & $y$ $$ 1+2(\cdot\frac{1}{2})x=0$$ $$ x=-1$$ $$ 2+2y\cdot (\frac{1}{2})=0 $$ $$ y=-2 $$ now for $z$ $$ z+2\cdot(\frac{1}{2})z=0$$ $$ z=0 $$ I found one such critical point $(-1,-2,0)$ now to compute what values this point has: $$ f(-1,-2,0)=-1-2\cdot 2+ \frac{0^2}{2}=-5 $$ I don't know what tools i have for checking if this critical point is either maxima or minima but it certainly would look like a minima. I think there is version of hessian matrix called ""bordered hessian"" but this wasn't covered on our lecture so i am suppose to know only about the regular hessian which isn't sufficient way to tell on this if the point is minima or maxima. Also how do i find the another critical point (maxima) or how do i know if such point doesn't exist ?","problem I have function: $$ f(x,y,z)=x+2y+\frac{z^2}{2} $$ and i would want to find it's minima and maxima with condition $x^2+y^2+z^2=5$ Attempt to solve I can form Lagrange function with the limiting condition and the function itself. Lagrange function is defined as $$ L(x,y,z,\lambda)=f(x)+\lambda(g(x)) $$ $g(x)$ would be limiting condition. $$ L(x,y,z,\lambda)= x+2y+\frac{z^2}{2} + \lambda(x^2+y^2+z^2-5)$$ in order to find when this function has critical points i need to compute gradient. $$ \nabla L(x,y,\lambda)=\begin{bmatrix} 1+2x\lambda \\ 2+2y\lambda \\ z+2z\lambda \\ x^2+y^2+z^2-5 \end{bmatrix} $$ $$ \begin{cases} 1+2x\lambda = 0 \\ 2+2y\lambda = 0 \\ z+2z\lambda =0 \\ x^2+y^2+z^2-5=0\end{cases} $$ $$ x=-\frac{1}{2\lambda} $$ $$ y=-\frac{2}{2\lambda} $$ $$ z=0 $$ $$ (-\frac{1}{2\lambda})^2+(-\frac{2}{2\lambda})^2+(0)^2-5=0 $$ $$ \frac{1}{4\lambda^2} + \frac{4}{4\lambda^2}-5=0 $$ $$ \frac{5}{4\lambda^2}-5=0 $$ $$ \frac{5}{4\lambda^2}=5 $$ $$ 20\lambda^2=5 $$ $$ \lambda = \sqrt{\frac{5}{20}}=\sqrt{\frac{1}{4}}=\frac{\sqrt{1}}{\sqrt{4}}=\frac{1}{2}$$ since for all $n\ge 0$  $\sqrt{\frac{m}{n}}=\frac{\sqrt{m}}{\sqrt{n}}$ Now i can compute values for $x$ & $y$ $$ 1+2(\cdot\frac{1}{2})x=0$$ $$ x=-1$$ $$ 2+2y\cdot (\frac{1}{2})=0 $$ $$ y=-2 $$ now for $z$ $$ z+2\cdot(\frac{1}{2})z=0$$ $$ z=0 $$ I found one such critical point $(-1,-2,0)$ now to compute what values this point has: $$ f(-1,-2,0)=-1-2\cdot 2+ \frac{0^2}{2}=-5 $$ I don't know what tools i have for checking if this critical point is either maxima or minima but it certainly would look like a minima. I think there is version of hessian matrix called ""bordered hessian"" but this wasn't covered on our lecture so i am suppose to know only about the regular hessian which isn't sufficient way to tell on this if the point is minima or maxima. Also how do i find the another critical point (maxima) or how do i know if such point doesn't exist ?",,"['multivariable-calculus', 'lagrange-multiplier', 'maxima-minima']"
51,Find a unit vector that minimizes the directional derivative at a point,Find a unit vector that minimizes the directional derivative at a point,,"Find a unit normed vector (direction) $d$ that minimizes the directional derivative, where $\nabla f(x^*)=(12,24)^T$. My question: Is the directional derivative minimized by selecting a vector orthogonal to the gradient? Is so would $d=(-2,1)^T$","Find a unit normed vector (direction) $d$ that minimizes the directional derivative, where $\nabla f(x^*)=(12,24)^T$. My question: Is the directional derivative minimized by selecting a vector orthogonal to the gradient? Is so would $d=(-2,1)^T$",,"['multivariable-calculus', 'optimization']"
52,Applicability of gradient theorem in the calculation of flux.,Applicability of gradient theorem in the calculation of flux.,,"I have a small confusion with the applicability of the fundamental theorem of line integral.  The theorem states that if $F=P\vec{i}+Q\vec{j}\;$  is a gradient field ($\nabla g$)  of some function $g$ ,then line integrals over any curve from $P$ to $Q$ under the influence of  $\vec{F}$ is just $g(Q)- g(P)$. Am I allowed to use this to calculate flux . In calculating flux in a planar   vector field, we are evaluating line integral . So I think we can use this theorem to calculate flux. But prof. Denis in his lecture says that we can't use this theorem to calculate flux. Refer the following lecture: 45:00 - 46:30 min https://www.youtube.com/watch?v=PnPIqh7Frlw&list=PL4C4C8A7D06566F38&index=24 Transcript: So, important: this is only for work. There is no statement like that for flux, ok, so don't try to apply this in a problem about flux. More precisely , suppose I have a force field $\vec{F}=y\vec{i}-x\vec{j}$. I want to calculate the flux across the curve $C$ which is a straight line from $(0,0)$ to $(1,1)$. My approach: $\int _{\scriptsize C}\vec{F}\cdot \vec{n}\;dS$=$\int_{\scriptsize C} xdx+ydy=\int_{\scriptsize C} d(\frac{1}{2}x^2+\frac{1}{2}y^2)=[\frac{1}{2}x^2+\frac{1}{2}y^2]_{(0,0)}^{(1,1)}=1  $ I think I can use the gradient theorem here because  the proof of gradient theorem is independent of work and flux. So My questions: Whether my approach is correct or not? If correct, What does Prof. Denis try to say in that clip.( see the frames from 45:00-46:30 in the above video.) More precisely: why doesn't it contradict with my approach? How to calculate the Flux of the Vector field $$\vec{F}=\bigg(2\pi x+\dfrac{2x^2y^2}{\pi}\bigg)\vec{i}+\bigg(2\pi xy-\cfrac{4y}{\pi}\bigg)\vec{j}$$  along the outward normal, across the ellipse $x^2+16y^2=4.$ I think the question is long but anyhow I am forced to ask these three questions. Thanks in advance.","I have a small confusion with the applicability of the fundamental theorem of line integral.  The theorem states that if $F=P\vec{i}+Q\vec{j}\;$  is a gradient field ($\nabla g$)  of some function $g$ ,then line integrals over any curve from $P$ to $Q$ under the influence of  $\vec{F}$ is just $g(Q)- g(P)$. Am I allowed to use this to calculate flux . In calculating flux in a planar   vector field, we are evaluating line integral . So I think we can use this theorem to calculate flux. But prof. Denis in his lecture says that we can't use this theorem to calculate flux. Refer the following lecture: 45:00 - 46:30 min https://www.youtube.com/watch?v=PnPIqh7Frlw&list=PL4C4C8A7D06566F38&index=24 Transcript: So, important: this is only for work. There is no statement like that for flux, ok, so don't try to apply this in a problem about flux. More precisely , suppose I have a force field $\vec{F}=y\vec{i}-x\vec{j}$. I want to calculate the flux across the curve $C$ which is a straight line from $(0,0)$ to $(1,1)$. My approach: $\int _{\scriptsize C}\vec{F}\cdot \vec{n}\;dS$=$\int_{\scriptsize C} xdx+ydy=\int_{\scriptsize C} d(\frac{1}{2}x^2+\frac{1}{2}y^2)=[\frac{1}{2}x^2+\frac{1}{2}y^2]_{(0,0)}^{(1,1)}=1  $ I think I can use the gradient theorem here because  the proof of gradient theorem is independent of work and flux. So My questions: Whether my approach is correct or not? If correct, What does Prof. Denis try to say in that clip.( see the frames from 45:00-46:30 in the above video.) More precisely: why doesn't it contradict with my approach? How to calculate the Flux of the Vector field $$\vec{F}=\bigg(2\pi x+\dfrac{2x^2y^2}{\pi}\bigg)\vec{i}+\bigg(2\pi xy-\cfrac{4y}{\pi}\bigg)\vec{j}$$  along the outward normal, across the ellipse $x^2+16y^2=4.$ I think the question is long but anyhow I am forced to ask these three questions. Thanks in advance.",,"['multivariable-calculus', 'vector-analysis', 'greens-theorem']"
53,The partial derivative of the complex quadratic statement,The partial derivative of the complex quadratic statement,,"Assume a complex-valued function $ h\left( {a,\theta } \right) = f\left( {a,\theta } \right) + ig\left( {a,\theta } \right) $  where $ a = \left[ {\begin{array}{*{20}{c}}   {{a_1}}& \cdots &{{a_n}}  \end{array}} \right] $  and $ \theta  = \left[ {\begin{array}{*{20}{c}}   {{\theta _1}}& \cdots &{{\theta _n}}  \end{array}} \right] $  has been represented by a quadratic matrix form $$ h\left( {a,\theta } \right) = {x^H}Ax = \left[ {\begin{array}{*{20}{c}}   {{a_1}{e^{ - j{\theta _1}}}}& \cdots &{{a_n}{e^{ - j{\theta _n}}}}  \end{array}} \right]A\left[ {\begin{array}{*{20}{c}}   {{a_1}{e^{j{\theta _1}}}} \\     \vdots  \\    {{a_n}{e^{j{\theta _n}}}}  \end{array}} \right] $$ where $A$ is a complex n by n matrix, neither Hermitian nor skew-Hermitian so the value of $ {x^H}Ax $ is a complex number.  What are the closed forms of $\frac{{\partial f\left( {a,\theta } \right)}}{{\partial \theta }} + i\frac{{\partial g\left( {a,\theta } \right)}}{{\partial \theta }} = \frac{{\partial {x^H}Ax}}{{\partial \theta }}$ and $\frac{{\partial f\left( {a,\theta } \right)}}{{\partial a}} + i\frac{{\partial g\left( {a,\theta } \right)}}{{\partial a}} = \frac{{\partial {x^H}Ax}}{{\partial a }}$ in terms of the matrix $A$ and vector $x$. Actually, I could not use the Wirtinger derivatives because I do not know how to use them in polar from.","Assume a complex-valued function $ h\left( {a,\theta } \right) = f\left( {a,\theta } \right) + ig\left( {a,\theta } \right) $  where $ a = \left[ {\begin{array}{*{20}{c}}   {{a_1}}& \cdots &{{a_n}}  \end{array}} \right] $  and $ \theta  = \left[ {\begin{array}{*{20}{c}}   {{\theta _1}}& \cdots &{{\theta _n}}  \end{array}} \right] $  has been represented by a quadratic matrix form $$ h\left( {a,\theta } \right) = {x^H}Ax = \left[ {\begin{array}{*{20}{c}}   {{a_1}{e^{ - j{\theta _1}}}}& \cdots &{{a_n}{e^{ - j{\theta _n}}}}  \end{array}} \right]A\left[ {\begin{array}{*{20}{c}}   {{a_1}{e^{j{\theta _1}}}} \\     \vdots  \\    {{a_n}{e^{j{\theta _n}}}}  \end{array}} \right] $$ where $A$ is a complex n by n matrix, neither Hermitian nor skew-Hermitian so the value of $ {x^H}Ax $ is a complex number.  What are the closed forms of $\frac{{\partial f\left( {a,\theta } \right)}}{{\partial \theta }} + i\frac{{\partial g\left( {a,\theta } \right)}}{{\partial \theta }} = \frac{{\partial {x^H}Ax}}{{\partial \theta }}$ and $\frac{{\partial f\left( {a,\theta } \right)}}{{\partial a}} + i\frac{{\partial g\left( {a,\theta } \right)}}{{\partial a}} = \frac{{\partial {x^H}Ax}}{{\partial a }}$ in terms of the matrix $A$ and vector $x$. Actually, I could not use the Wirtinger derivatives because I do not know how to use them in polar from.",,"['multivariable-calculus', 'complex-numbers']"
54,The derivative of complex quadratic form,The derivative of complex quadratic form,,Is there any way to represent the derivative of this complex quadratic statement into a compact matrix form? $${x^{{*^T}}}Ax = \left[ {\begin{array}{*{20}{c}}   {{a_1}{e^{ - j{\theta _1}}}}& \cdots &{{a_n}{e^{ - j{\theta _n}}}}  \end{array}} \right]A\left[ {\begin{array}{*{20}{c}}   {{a_1}{e^{j{\theta _1}}}} \\     \vdots  \\    {{a_n}{e^{j{\theta _n}}}}  \end{array}} \right]$$,Is there any way to represent the derivative of this complex quadratic statement into a compact matrix form? $${x^{{*^T}}}Ax = \left[ {\begin{array}{*{20}{c}}   {{a_1}{e^{ - j{\theta _1}}}}& \cdots &{{a_n}{e^{ - j{\theta _n}}}}  \end{array}} \right]A\left[ {\begin{array}{*{20}{c}}   {{a_1}{e^{j{\theta _1}}}} \\     \vdots  \\    {{a_n}{e^{j{\theta _n}}}}  \end{array}} \right]$$,,"['matrices', 'multivariable-calculus']"
55,Tangent plane when gradient is zero,Tangent plane when gradient is zero,,"I'm attempting to find the tangent plane of the function $f(x,y,z)=xz+2y^2z^2$ at $(x,y,z)=(-1,1,0).$ The partial derivatives are $z, 4yz^2$, and $4zy^2$ when done in respect to $x, y$ and $z$ respectively. But they're all zero at $(-1,1,0)$ (i.e the gradient is zero). Does this mean that a tangent plane doesn't exist at this point?","I'm attempting to find the tangent plane of the function $f(x,y,z)=xz+2y^2z^2$ at $(x,y,z)=(-1,1,0).$ The partial derivatives are $z, 4yz^2$, and $4zy^2$ when done in respect to $x, y$ and $z$ respectively. But they're all zero at $(-1,1,0)$ (i.e the gradient is zero). Does this mean that a tangent plane doesn't exist at this point?",,"['multivariable-calculus', 'partial-derivative', 'plane-curves']"
56,Constant vector and differential geometry,Constant vector and differential geometry,,"I have been asked to solve the following problem for my differential geometry class: Let u ( t ) = ($u_1$( t ), $u_2$ ( t ), $u_3$ ( t )) and v ( t ) = ( $v_1$ ( t ), $v_2$ ( t ), $v_3$ ( t )) be differentiable maps from the interval ( a , b ) into $\mathbb{R}^3$. If the derivatives u'(t) and v'(t) satisfy the conditions: $$u'(t) = au(t) +bv(t) $$ $$ v'(t) = cu(t) - av(t) $$ where a, b, and c are constants, show that u ( t ) ∧ v ( t ) is a constant vector. I am having some issues with this problem. I think part of that comes from the fact that I don't really understand what the constant vector. If anybody could answer that and offer some advice on how to begin that would be great. I guess I also don't really understand why we would need to the derivative condition as well.","I have been asked to solve the following problem for my differential geometry class: Let u ( t ) = ($u_1$( t ), $u_2$ ( t ), $u_3$ ( t )) and v ( t ) = ( $v_1$ ( t ), $v_2$ ( t ), $v_3$ ( t )) be differentiable maps from the interval ( a , b ) into $\mathbb{R}^3$. If the derivatives u'(t) and v'(t) satisfy the conditions: $$u'(t) = au(t) +bv(t) $$ $$ v'(t) = cu(t) - av(t) $$ where a, b, and c are constants, show that u ( t ) ∧ v ( t ) is a constant vector. I am having some issues with this problem. I think part of that comes from the fact that I don't really understand what the constant vector. If anybody could answer that and offer some advice on how to begin that would be great. I guess I also don't really understand why we would need to the derivative condition as well.",,"['multivariable-calculus', 'differential-geometry', 'cross-product']"
57,Find the value of the function (Chain rule),Find the value of the function (Chain rule),,"I have the function $u=xy+\ln{y^2}$ where $x=f(st)$ and $y=g(s+t)$ i have to find $u_s(-1,1)$ knowing that $g(0)=g'(0)=2,\ f(-1)=1$ and $f'(-1)=-1$. So long this is what i have done: $$u_s=\frac{du}{dx}\frac{dx}{ds}+\frac{du}{dy}\frac{dy}{ds}$$ $$u_s=y \frac{dx}{ds}+\Bigr(x+\frac2 y\Bigr)\frac{dy}{ds}$$ $$u_s=g(s+t)f_s(st)t+\Bigr(f(st)+\frac{2}{g(s+t)}\Bigr)g_s(s+t)$$ Is this right? And... how do i find $u_s(-1,1)$ exactly?","I have the function $u=xy+\ln{y^2}$ where $x=f(st)$ and $y=g(s+t)$ i have to find $u_s(-1,1)$ knowing that $g(0)=g'(0)=2,\ f(-1)=1$ and $f'(-1)=-1$. So long this is what i have done: $$u_s=\frac{du}{dx}\frac{dx}{ds}+\frac{du}{dy}\frac{dy}{ds}$$ $$u_s=y \frac{dx}{ds}+\Bigr(x+\frac2 y\Bigr)\frac{dy}{ds}$$ $$u_s=g(s+t)f_s(st)t+\Bigr(f(st)+\frac{2}{g(s+t)}\Bigr)g_s(s+t)$$ Is this right? And... how do i find $u_s(-1,1)$ exactly?",,"['multivariable-calculus', 'chain-rule']"
58,"Is $f$ continuous at$(0,0)$?",Is  continuous at?,"f (0,0)","$f:\Bbb R^2\to \Bbb R$ given by $$ f(x, y) = \cases{\dfrac{2(x^3+y^3)}{x^2 + y} & if $(x, y)\neq (0,0)$\\ 0 & if $(x, y) = (0,0)$ } $$ It looks continuous, but my friend said it isn't. I tried to show discontinuity by taking various paths, but was only met with failure.  I couldn't prove it is continuous either. Please help.","$f:\Bbb R^2\to \Bbb R$ given by $$ f(x, y) = \cases{\dfrac{2(x^3+y^3)}{x^2 + y} & if $(x, y)\neq (0,0)$\\ 0 & if $(x, y) = (0,0)$ } $$ It looks continuous, but my friend said it isn't. I tried to show discontinuity by taking various paths, but was only met with failure.  I couldn't prove it is continuous either. Please help.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
59,"Show that the following curve, $\alpha (s)$ is a circle centered at the origin","Show that the following curve,  is a circle centered at the origin",\alpha (s),"Let $\alpha (s)$ for $s \in \mathbb{R}$ be a curve in $\mathbb{R}^3$, parametrized by arc length which does not pass through the origin. Suppose for every $s\in \mathbb{R}$, the straight line through $\alpha(s)$ parallel to $\bf{n}$$(s)$ always passes through the origin $(0,0,0)$, Show $\alpha (s)$ is a circle centered at the origin. I dug around and found out that if the curvature is constant and torsion is $0$, then the curve is a circle. But now I have the following questions. It makes sense for a circle to have constant curvature part, but I don't quite understand how torsion works aside from it's definition.","Let $\alpha (s)$ for $s \in \mathbb{R}$ be a curve in $\mathbb{R}^3$, parametrized by arc length which does not pass through the origin. Suppose for every $s\in \mathbb{R}$, the straight line through $\alpha(s)$ parallel to $\bf{n}$$(s)$ always passes through the origin $(0,0,0)$, Show $\alpha (s)$ is a circle centered at the origin. I dug around and found out that if the curvature is constant and torsion is $0$, then the curve is a circle. But now I have the following questions. It makes sense for a circle to have constant curvature part, but I don't quite understand how torsion works aside from it's definition.",,"['multivariable-calculus', 'differential-geometry']"
60,Is complex analysis same difficulty as vector analysis/multivariable calculus? [closed],Is complex analysis same difficulty as vector analysis/multivariable calculus? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 years ago . Improve this question In vector calculus, you need some geometric intuition and find some integrals. I scanned a book on complex analysis and realized that they're aren't many pictures as we see in a multivariable textbook and there is always some substitution of variables going on when computing some integral. Is learning complex analysis in terms of computing integrals and derivatives just as easy as learning computational calculus? Why don't complex analysis books have as many pictures as calculus books do? Because they're are less pictures in complex analysis books and tons of substitutions I can't understand, I'm guessing complex analysis isn't as easy as calculus.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 years ago . Improve this question In vector calculus, you need some geometric intuition and find some integrals. I scanned a book on complex analysis and realized that they're aren't many pictures as we see in a multivariable textbook and there is always some substitution of variables going on when computing some integral. Is learning complex analysis in terms of computing integrals and derivatives just as easy as learning computational calculus? Why don't complex analysis books have as many pictures as calculus books do? Because they're are less pictures in complex analysis books and tons of substitutions I can't understand, I'm guessing complex analysis isn't as easy as calculus.",,"['complex-analysis', 'multivariable-calculus']"
61,Use Stokes' Theorem to evaluate line integral,Use Stokes' Theorem to evaluate line integral,,"For the vector field $\mathbf{F}(x,y,z) = \langle 5y+\sin x, z^2+\cos y, x^3\rangle$, I need to find the integral using Stokes' Theorem $\int_C\mathbf{F}\cdot\mathrm{d}\mathbf{r}$ where $C$ is the curve $\mathbf{r}(t)=\langle \sin t, \cos t, 2\sin t\cos t\rangle, t\in[0,2\pi]$. I have gotten the curl as $\langle -2z,-3x^2,-5\rangle$. But I was troubled in parametrising the surface. When I suggest that the curve is contained in the surface parametrised by $\mathbf{r}(u,v)=\langle u\sin v, u\cos v, 2u\sin v\cos v\rangle$,   I have $\mathbf{r}_u\times\mathbf{r}_v=\langle 2u\cos^3 v, 2u\sin^3 v,-u\rangle$ which changes direction in the domain of $v\in[0,2\pi]$. How should I correctly parametrise the surface? Thank you very much.","For the vector field $\mathbf{F}(x,y,z) = \langle 5y+\sin x, z^2+\cos y, x^3\rangle$, I need to find the integral using Stokes' Theorem $\int_C\mathbf{F}\cdot\mathrm{d}\mathbf{r}$ where $C$ is the curve $\mathbf{r}(t)=\langle \sin t, \cos t, 2\sin t\cos t\rangle, t\in[0,2\pi]$. I have gotten the curl as $\langle -2z,-3x^2,-5\rangle$. But I was troubled in parametrising the surface. When I suggest that the curve is contained in the surface parametrised by $\mathbf{r}(u,v)=\langle u\sin v, u\cos v, 2u\sin v\cos v\rangle$,   I have $\mathbf{r}_u\times\mathbf{r}_v=\langle 2u\cos^3 v, 2u\sin^3 v,-u\rangle$ which changes direction in the domain of $v\in[0,2\pi]$. How should I correctly parametrise the surface? Thank you very much.",,['multivariable-calculus']
62,Finding the general solution of an equation.,Finding the general solution of an equation.,,"Write the equation $f_{xx}-f_{yy} = 0$ in terms of the variables $u=x+y$, $v=x-y$ and calculate the general solution. We define $g(u,v)=f(x,y)$, then, by the chain rule: $$f_x=g_u+g_v,f_y=g_u-g_v$$ Using the chain rule again $$f_{xx}=g_{uu}+g_{vu}+g_{uv}+g_{vv},f_{yy}=g_{uu}-g_{uv}-g_{vu}+g_{vv}$$ Therefore $$f_{xx}-f_{yy}=4g_{uv}$$  $$0=4g_{uv}$$ This means $g_u$ doesn't depend on $v$ and $g$... I understand everything up until this point: ... has the form $\phi(u)+\psi(v)$ with $\phi$, $\psi$ functions of one variable. Therefore the general solution of $f_{xx}-f_{yy}=0$ is $$f(x,y)=\phi(x+y)+\psi(x-y)$$ I don't understand why $g$ has the form $\phi(u)+\psi(v)$ and how $g_u$ not depending on $v$ reflects on that solution","Write the equation $f_{xx}-f_{yy} = 0$ in terms of the variables $u=x+y$, $v=x-y$ and calculate the general solution. We define $g(u,v)=f(x,y)$, then, by the chain rule: $$f_x=g_u+g_v,f_y=g_u-g_v$$ Using the chain rule again $$f_{xx}=g_{uu}+g_{vu}+g_{uv}+g_{vv},f_{yy}=g_{uu}-g_{uv}-g_{vu}+g_{vv}$$ Therefore $$f_{xx}-f_{yy}=4g_{uv}$$  $$0=4g_{uv}$$ This means $g_u$ doesn't depend on $v$ and $g$... I understand everything up until this point: ... has the form $\phi(u)+\psi(v)$ with $\phi$, $\psi$ functions of one variable. Therefore the general solution of $f_{xx}-f_{yy}=0$ is $$f(x,y)=\phi(x+y)+\psi(x-y)$$ I don't understand why $g$ has the form $\phi(u)+\psi(v)$ and how $g_u$ not depending on $v$ reflects on that solution",,"['multivariable-calculus', 'partial-differential-equations', 'wave-equation']"
63,Time derivative of a line integral,Time derivative of a line integral,,"Question : What is the derivative of $$ \frac{d}{dt} \int_{(x_0,y_0,z_0)}^{(x,y,z)} \vec{F}(x,y,z,t) \cdot d\vec{r}$$ Now in order to compute any single integral, I need to give you a path. So far, all we know is that we are integrating $\vec{F}$ from the point $(x_0, y_0, z_0)$ to the point $(x, y, z)$ along some path. Let me specify that path. What is the derivative of $$ \frac{d}{dt} \int_{t_0}^{t} \vec{F}(c(t),t) \cdot \vec{c}\;'(t)dt $$ where $c(t) = (x(t), y(t), z(t))$ is the path and $\vec{c}\;'(t) = (x'(t), y'(t), z'(t))$ is the tangent vector to the path. Thoughts : Derivatives require a path. You can't take a derivative unless you know what path you are taking the derivative over. Likewise, integrals/antiderivatives require a path. You can't create an antiderivative unless you know what path you are doing the integral over. If I created the antiderivative $$\int_{t_0}^{t} \vec{F}(c(t)) \cdot \vec{c}\;'(t)dt $$ This would be the antiderivative over the path $c(t)$. Therefore if I take the derivative $d/dt$, this indicates the derivative over the same path $c(t)$. And since we know derivatives and antiderivatives are inverse operations ( over the same path ), $$ \frac{d}{dt} \int_{t_0}^{t} \vec{F}(c(t)) \cdot \vec{c}\;'(t)dt = \vec{F}(c(t)) \cdot \vec{c}\;'(t)$$ That's what I think at least. My problem is that my integrand involves an extra $t$ (I'm asking this because in order to understand time-dependent forces and energy consequences in physics, I'd like to understand derivatives of line integrals). Useful break down of my question : These would be useful to solve. Just to make the problem look simpler, what is $$\frac{d}{dt} \int_{t_0}^{t} f(x(t),y(t))x'(t)dt $$ This is the antiderivative over the path $x(t)$. But the derivative $d/dt$ is with respect to the path $(x(t), y(t))$. So would this become, using the chain rule $$ \frac{\partial }{\partial x} \Bigg [\int_{t_0}^{t} f(x(t),y(t))x'(t)dt\Bigg]\frac{dx}{dt} + \frac{\partial}{\partial y}\Bigg[\int_{t_0}^{t} f(x(t),y(t))x'(t)dt\Bigg]\frac{dy}{dt}$$ Is the second bracket zero? Another useful, simpler looking, problem : $$\frac{d}{dt}\int_{t_0}^t f(x(t),t)x'(t)dt $$ Which might become: $$ \frac{\partial }{\partial x} \Bigg [\int_{t_0}^{t} f(x(t), t)x'(t)dt\Bigg]\frac{dx}{dt} + \frac{\partial}{\partial t}\Bigg[\int_{t_0}^{t} f(x(t), t)x'(t)dt\Bigg]\frac{dt}{dt}$$ Is the first bracket just $f(x(t),t)$? If I write this first bracket as  $$ \int_{x_0}^x f(x, t)dx$$ Then a $\partial/\partial x$ of this antiderivative should return $f(x,t)$. However I'm getting confused by the abstraction of everything and the actual computation . This last integral seems to imply that the path is just the $x$-axis. But the real path is $x(t)$. You need to do the integral with all $t$'s then. But I still feel like $$ \frac{\partial }{\partial x}\int_{t_0}^{t} f(x(t), t) x'(t)dt = f(x(t),t) ?$$ If you read all of this, thank you. Any help would be greatly appreciated","Question : What is the derivative of $$ \frac{d}{dt} \int_{(x_0,y_0,z_0)}^{(x,y,z)} \vec{F}(x,y,z,t) \cdot d\vec{r}$$ Now in order to compute any single integral, I need to give you a path. So far, all we know is that we are integrating $\vec{F}$ from the point $(x_0, y_0, z_0)$ to the point $(x, y, z)$ along some path. Let me specify that path. What is the derivative of $$ \frac{d}{dt} \int_{t_0}^{t} \vec{F}(c(t),t) \cdot \vec{c}\;'(t)dt $$ where $c(t) = (x(t), y(t), z(t))$ is the path and $\vec{c}\;'(t) = (x'(t), y'(t), z'(t))$ is the tangent vector to the path. Thoughts : Derivatives require a path. You can't take a derivative unless you know what path you are taking the derivative over. Likewise, integrals/antiderivatives require a path. You can't create an antiderivative unless you know what path you are doing the integral over. If I created the antiderivative $$\int_{t_0}^{t} \vec{F}(c(t)) \cdot \vec{c}\;'(t)dt $$ This would be the antiderivative over the path $c(t)$. Therefore if I take the derivative $d/dt$, this indicates the derivative over the same path $c(t)$. And since we know derivatives and antiderivatives are inverse operations ( over the same path ), $$ \frac{d}{dt} \int_{t_0}^{t} \vec{F}(c(t)) \cdot \vec{c}\;'(t)dt = \vec{F}(c(t)) \cdot \vec{c}\;'(t)$$ That's what I think at least. My problem is that my integrand involves an extra $t$ (I'm asking this because in order to understand time-dependent forces and energy consequences in physics, I'd like to understand derivatives of line integrals). Useful break down of my question : These would be useful to solve. Just to make the problem look simpler, what is $$\frac{d}{dt} \int_{t_0}^{t} f(x(t),y(t))x'(t)dt $$ This is the antiderivative over the path $x(t)$. But the derivative $d/dt$ is with respect to the path $(x(t), y(t))$. So would this become, using the chain rule $$ \frac{\partial }{\partial x} \Bigg [\int_{t_0}^{t} f(x(t),y(t))x'(t)dt\Bigg]\frac{dx}{dt} + \frac{\partial}{\partial y}\Bigg[\int_{t_0}^{t} f(x(t),y(t))x'(t)dt\Bigg]\frac{dy}{dt}$$ Is the second bracket zero? Another useful, simpler looking, problem : $$\frac{d}{dt}\int_{t_0}^t f(x(t),t)x'(t)dt $$ Which might become: $$ \frac{\partial }{\partial x} \Bigg [\int_{t_0}^{t} f(x(t), t)x'(t)dt\Bigg]\frac{dx}{dt} + \frac{\partial}{\partial t}\Bigg[\int_{t_0}^{t} f(x(t), t)x'(t)dt\Bigg]\frac{dt}{dt}$$ Is the first bracket just $f(x(t),t)$? If I write this first bracket as  $$ \int_{x_0}^x f(x, t)dx$$ Then a $\partial/\partial x$ of this antiderivative should return $f(x,t)$. However I'm getting confused by the abstraction of everything and the actual computation . This last integral seems to imply that the path is just the $x$-axis. But the real path is $x(t)$. You need to do the integral with all $t$'s then. But I still feel like $$ \frac{\partial }{\partial x}\int_{t_0}^{t} f(x(t), t) x'(t)dt = f(x(t),t) ?$$ If you read all of this, thank you. Any help would be greatly appreciated",,"['calculus', 'multivariable-calculus']"
64,Intuition with graphs of parametarizations,Intuition with graphs of parametarizations,,"I have trouble intuitively understanding why a certain graph belongs to a parametarization in a certain number of parameters. When I ask myself why the graph of the function $f(x) = y$ is a curve, it's because if it were a surface it would fail the vertical line test (for any one input there would need to be multiple outputs). Also, the graph of $f(x, y) = z$ is a surface and not a volume because for any 2 inputs there would need to be multiple outputs. I can't seem to develop that kind of intuition for parametarizations though. Why is the graph of $\vec{r}(t) = (x(t), y(t), z(t))$ a curve, and the graph of   $\vec{r}(u, v) = (x(u, v), y(u, v), z(u, v))$ a surface? Is there more advanced math involved behind the curtain that prevents the intuition?","I have trouble intuitively understanding why a certain graph belongs to a parametarization in a certain number of parameters. When I ask myself why the graph of the function $f(x) = y$ is a curve, it's because if it were a surface it would fail the vertical line test (for any one input there would need to be multiple outputs). Also, the graph of $f(x, y) = z$ is a surface and not a volume because for any 2 inputs there would need to be multiple outputs. I can't seem to develop that kind of intuition for parametarizations though. Why is the graph of $\vec{r}(t) = (x(t), y(t), z(t))$ a curve, and the graph of   $\vec{r}(u, v) = (x(u, v), y(u, v), z(u, v))$ a surface? Is there more advanced math involved behind the curtain that prevents the intuition?",,['multivariable-calculus']
65,Prove that there is only one $y=h(x)$ so that $y+x^2y^3+x+y^5x^4=1$,Prove that there is only one  so that,y=h(x) y+x^2y^3+x+y^5x^4=1,Prove that $\forall x\in \mathbb{R}$ there is only one $y=h(x)$ so that  $$y+x^2y^3+x+y^5x^4=1$$ I am currenty studying multi-variable calculus on my own and I have no clue how to solve this problem. Which results of multi-variable calculus do I have to study to solve it? I would appreciate if someone could solve it or share similiar problems to this one.,Prove that $\forall x\in \mathbb{R}$ there is only one $y=h(x)$ so that  $$y+x^2y^3+x+y^5x^4=1$$ I am currenty studying multi-variable calculus on my own and I have no clue how to solve this problem. Which results of multi-variable calculus do I have to study to solve it? I would appreciate if someone could solve it or share similiar problems to this one.,,['multivariable-calculus']
66,Difficulty with the multivariate chain rule,Difficulty with the multivariate chain rule,,"I am having a minor issue with the chain rule. I remember its properties but I just cannot quite remember if I am doing it correctly. If someone could just check if this identity is correct that would be great. It is the last thing I appealed to in a proof Ive been working on for leisure and I want to make sure I don't embarrass myself by rememvering wrong. $(f(g(x),h(x)))' = f'(g(x),h(x)) * g'(x) + f'(g(x),h(x)) * h'(x)$ I know that each derivative of $f$ in above is the partial derivative with respect to each argument, but is that right? I just want to make sure I am not forgetting something vital.","I am having a minor issue with the chain rule. I remember its properties but I just cannot quite remember if I am doing it correctly. If someone could just check if this identity is correct that would be great. It is the last thing I appealed to in a proof Ive been working on for leisure and I want to make sure I don't embarrass myself by rememvering wrong. $(f(g(x),h(x)))' = f'(g(x),h(x)) * g'(x) + f'(g(x),h(x)) * h'(x)$ I know that each derivative of $f$ in above is the partial derivative with respect to each argument, but is that right? I just want to make sure I am not forgetting something vital.",,"['multivariable-calculus', 'chain-rule']"
67,Is this continuity proof valid?,Is this continuity proof valid?,,"$g:\mathbb{R}^2 \rightarrow \mathbb{R}$ differentiable and: $$g(x, 1) = 4$$ $$g(0, y) = 4$$ $$g(x, x + 1) = x^2 + 4.$$ $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ such that: $$f(x, y) = \begin{cases}        \frac{g(x, y) - 4}{\sqrt{x^{2} + (y - 1)^{2}}} & (x, y) \neq (0, 1) \\       0 & (x, y) = (0, 1)    \end{cases} $$ I have to prove $f$ is continuous. Now, I know that if $g$ is differentiable then it is continuous, then: $$\lim_{(x,y)\to(0,1)} g(x, y) = g(0,1) = 4$$ So in $$\lim_{(x,y)\to(0,1)} \frac{g(x, y) - 4}{\sqrt{x^2 + (y - 1)^2}}$$ Is it valid to say that as I distribute the limit I get 0 because: $$ \frac{0}{\lim_{(x,y)\to(0,1)} \sqrt{x^2 + (y - 1)^2}}$$ is $0$? Thanks in advance! PS: Sorry for my English.","$g:\mathbb{R}^2 \rightarrow \mathbb{R}$ differentiable and: $$g(x, 1) = 4$$ $$g(0, y) = 4$$ $$g(x, x + 1) = x^2 + 4.$$ $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ such that: $$f(x, y) = \begin{cases}        \frac{g(x, y) - 4}{\sqrt{x^{2} + (y - 1)^{2}}} & (x, y) \neq (0, 1) \\       0 & (x, y) = (0, 1)    \end{cases} $$ I have to prove $f$ is continuous. Now, I know that if $g$ is differentiable then it is continuous, then: $$\lim_{(x,y)\to(0,1)} g(x, y) = g(0,1) = 4$$ So in $$\lim_{(x,y)\to(0,1)} \frac{g(x, y) - 4}{\sqrt{x^2 + (y - 1)^2}}$$ Is it valid to say that as I distribute the limit I get 0 because: $$ \frac{0}{\lim_{(x,y)\to(0,1)} \sqrt{x^2 + (y - 1)^2}}$$ is $0$? Thanks in advance! PS: Sorry for my English.",,"['limits', 'multivariable-calculus', 'continuity']"
68,"Wrong Wolfram Alpha result for $\lim_{(x,y)\to(0,0)}\frac{xy^4}{x^4+x^2+y^4}$?",Wrong Wolfram Alpha result for ?,"\lim_{(x,y)\to(0,0)}\frac{xy^4}{x^4+x^2+y^4}","I'm trying to solve this limit:   $$ \lim_{(x,y)\to(0,0)}\frac{xy^4}{x^4+x^2+y^4} $$ Here's my attempt: $$0 \le |\frac{xy^4}{x^4+x^2+y^4} - 0| = \frac{|x|y^4}{x^4+x^2+y^4},$$ and since $x^4+x^2 \ge0$ then $\frac{y^4}{x^4+x^2+y^4} \le 1$ so  $$ \frac{|x|y^4}{x^4+x^2+y^4} \le |x|,$$ so  $$ 0 \le \lim_{(x,y)\to(0,0)}|\frac{xy^4}{x^4+x^2+y^4} - 0| \le \lim_{(x,y)\to(0,0)} |x| = 0, $$ and using the squeeze theorem the limit is $0$. But if I input the limit in wolfram alpha, it says that the limit doesn't exist. Here is the link to the limit in Wolfram Alpha.","I'm trying to solve this limit:   $$ \lim_{(x,y)\to(0,0)}\frac{xy^4}{x^4+x^2+y^4} $$ Here's my attempt: $$0 \le |\frac{xy^4}{x^4+x^2+y^4} - 0| = \frac{|x|y^4}{x^4+x^2+y^4},$$ and since $x^4+x^2 \ge0$ then $\frac{y^4}{x^4+x^2+y^4} \le 1$ so  $$ \frac{|x|y^4}{x^4+x^2+y^4} \le |x|,$$ so  $$ 0 \le \lim_{(x,y)\to(0,0)}|\frac{xy^4}{x^4+x^2+y^4} - 0| \le \lim_{(x,y)\to(0,0)} |x| = 0, $$ and using the squeeze theorem the limit is $0$. But if I input the limit in wolfram alpha, it says that the limit doesn't exist. Here is the link to the limit in Wolfram Alpha.",,"['limits', 'multivariable-calculus', 'wolfram-alpha']"
69,Evaluate $\int ydx + zdy + xdz$ using Stokes' Theorem?,Evaluate  using Stokes' Theorem?,\int ydx + zdy + xdz,"Evaluate $\int ydx + zdy + xdz$ where $C $ is intersection of $x+y=2$   and $x^2+y^2+z^2=2(x+y) $ traversed counterclockwise as viewed from   origin I am using Stokes' theorem to solve this question so We want $\int \int curl F.N \; dS$ where $N$ is the normal unit vector to surface S, where S is a surface bounded by $C$ $F = \langle  y,z,x\rangle$ $curl  F  = \langle -1,-1,-1 \rangle $ I take $S $  on the plane $x+y = 2$ $\nabla (x+y) = \langle 1,1,0 \rangle  = A(say)$ Then unit normal vector $N =  \langle -1/\sqrt2,-1/\sqrt2, 0\rangle $ {Multiplied by $-1$ because we are viewing it from origin } $curl F.N= \sqrt(2)$ Now intersection of $x+y=2 $ and $x^2+y^2+z^2 = 2(x+y) $ gives $x^2 + y^2 + z^2 = 4$ To get projection onto $xy$ plane $z=0$ we get $x^2 +y^2 =4$ Now I am stuck in finding $dS$ How do I get dS = $\sqrt{z_x^2 +z_y^2 +1}dA$  where $A:x^2 +y^2 =4$ This is because my $S:x+y=2$ has no $z$ term","Evaluate $\int ydx + zdy + xdz$ where $C $ is intersection of $x+y=2$   and $x^2+y^2+z^2=2(x+y) $ traversed counterclockwise as viewed from   origin I am using Stokes' theorem to solve this question so We want $\int \int curl F.N \; dS$ where $N$ is the normal unit vector to surface S, where S is a surface bounded by $C$ $F = \langle  y,z,x\rangle$ $curl  F  = \langle -1,-1,-1 \rangle $ I take $S $  on the plane $x+y = 2$ $\nabla (x+y) = \langle 1,1,0 \rangle  = A(say)$ Then unit normal vector $N =  \langle -1/\sqrt2,-1/\sqrt2, 0\rangle $ {Multiplied by $-1$ because we are viewing it from origin } $curl F.N= \sqrt(2)$ Now intersection of $x+y=2 $ and $x^2+y^2+z^2 = 2(x+y) $ gives $x^2 + y^2 + z^2 = 4$ To get projection onto $xy$ plane $z=0$ we get $x^2 +y^2 =4$ Now I am stuck in finding $dS$ How do I get dS = $\sqrt{z_x^2 +z_y^2 +1}dA$  where $A:x^2 +y^2 =4$ This is because my $S:x+y=2$ has no $z$ term",,['multivariable-calculus']
70,"Calculate the area of the region bounded by $z=0, z=1,$, and $(z+1)\sqrt{x^2+y^2}=1$","Calculate the area of the region bounded by , and","z=0, z=1, (z+1)\sqrt{x^2+y^2}=1","Calculate the volume of the region bounded by $z=0, z=1,$, and $(z+1)\sqrt{x^2+y^2}=1$ The integral is $\int_{B}z\text{ dV}$ The area is like the thing between the top two green places. The first place is $z=1$, second is $z=0$ Clearly we have $0\leq z\leq 1$, but I'm not sure what to bound next? Should I be using cylindrical? Would it be correct in saying $0\leq r\leq \dfrac{1}{z+1}$","Calculate the volume of the region bounded by $z=0, z=1,$, and $(z+1)\sqrt{x^2+y^2}=1$ The integral is $\int_{B}z\text{ dV}$ The area is like the thing between the top two green places. The first place is $z=1$, second is $z=0$ Clearly we have $0\leq z\leq 1$, but I'm not sure what to bound next? Should I be using cylindrical? Would it be correct in saying $0\leq r\leq \dfrac{1}{z+1}$",,"['calculus', 'integration', 'multivariable-calculus']"
71,Does anyone have a nice formula for repeatedly differentiating a composite?,Does anyone have a nice formula for repeatedly differentiating a composite?,,"If $$u\colon \mathbb{R}^n \to \mathbb{R},$$ $$f\colon \mathbb{R} \to \mathbb{R}$$ $$x=(x_1,\ldots,x_n)\in \mathbb{R}^n, \nabla=(\partial_{x_1},\ldots,\partial_{x_n})$$ is there a nice formula for $$\nabla^k(f(u(x)))?$$ This seems to be something I have to work with a lot...","If $$u\colon \mathbb{R}^n \to \mathbb{R},$$ $$f\colon \mathbb{R} \to \mathbb{R}$$ $$x=(x_1,\ldots,x_n)\in \mathbb{R}^n, \nabla=(\partial_{x_1},\ldots,\partial_{x_n})$$ is there a nice formula for $$\nabla^k(f(u(x)))?$$ This seems to be something I have to work with a lot...",,"['calculus', 'multivariable-calculus']"
72,"For which $u$ does the derivative $f'(u,0)$ of $f(x,y)=x^3/(x^2+y^2)$ exist?",For which  does the derivative  of  exist?,"u f'(u,0) f(x,y)=x^3/(x^2+y^2)","Let $f: \mathbb{R}^2\to \mathbb{R}$ be defined by setting $$f(x,y)=\begin{cases}\frac{x^3}{x^2+y^2} & (x,y)\neq (0,0)\\0 & (x,y)=(0,0)\end{cases}$$ (a) For which vectors $u\ne 0$ does $f'(0; u)$ exist? Evaluate it when it exists. (b) Do $D_1f$ and $D_2f$ exist at $0$? (c) Is $f$ differentiable at $0$? (d) Is $f$ continuous at $0$? I have thought a lot about this problem: For (a), be $u\neq 0, u:=(h,k)$, so $\lim_{t\to 0}\frac{f(0+tu)-f(0,0)}{t}=\lim_{t\to 0}\frac{f(th,tk)}{t}=\lim_{t\to 0}\frac{(th)^3}{(th)^2+(tk)^2}\frac{1}{t}=\frac{h^3}{h^2+k^2}$, then $f'(0,u)$ exists for all $u\neq 0$. For (b), $\lim_{t\to 0}\frac{f(0+te_1)-f(0)}{t}=\lim_{t\to 0}\frac{f(t,0)}{t}=\lim_{t\to 0}\frac{t^3}{t^3}=\lim_{t\to 0}1=1$ so $D_1f(0,0)=1$ and $\lim_{t\to 0}\frac{f(0+te_2)-f(0)}{t}=\lim_{t\to 0}0=0$ so $D_2f(0,0)=0$ I am having problems solving (c) and (d), could someone help me please? Thank you very much.","Let $f: \mathbb{R}^2\to \mathbb{R}$ be defined by setting $$f(x,y)=\begin{cases}\frac{x^3}{x^2+y^2} & (x,y)\neq (0,0)\\0 & (x,y)=(0,0)\end{cases}$$ (a) For which vectors $u\ne 0$ does $f'(0; u)$ exist? Evaluate it when it exists. (b) Do $D_1f$ and $D_2f$ exist at $0$? (c) Is $f$ differentiable at $0$? (d) Is $f$ continuous at $0$? I have thought a lot about this problem: For (a), be $u\neq 0, u:=(h,k)$, so $\lim_{t\to 0}\frac{f(0+tu)-f(0,0)}{t}=\lim_{t\to 0}\frac{f(th,tk)}{t}=\lim_{t\to 0}\frac{(th)^3}{(th)^2+(tk)^2}\frac{1}{t}=\frac{h^3}{h^2+k^2}$, then $f'(0,u)$ exists for all $u\neq 0$. For (b), $\lim_{t\to 0}\frac{f(0+te_1)-f(0)}{t}=\lim_{t\to 0}\frac{f(t,0)}{t}=\lim_{t\to 0}\frac{t^3}{t^3}=\lim_{t\to 0}1=1$ so $D_1f(0,0)=1$ and $\lim_{t\to 0}\frac{f(0+te_2)-f(0)}{t}=\lim_{t\to 0}0=0$ so $D_2f(0,0)=0$ I am having problems solving (c) and (d), could someone help me please? Thank you very much.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
73,Evaluate $\iint_{D}e^\frac{x+y}{x-y}$ on the region $D$ using a change of variables,Evaluate  on the region  using a change of variables,\iint_{D}e^\frac{x+y}{x-y} D,"Evaluate $\displaystyle\iint_{D}e^\dfrac{x+y}{x-y}$ on the region $D$ using a change of variables Here is the region: The region is bounded by the following: $y=x-2, y=x-1, x=0, y=0$. A sub that is obvious is let $u=y-x$, then $-2\leq u\leq -1$. Now I have to make one more sub that takes care of the $x=0, y=0$, so I made the sub, let $v=\frac{y}{y-x}$. Then if $x=0\to v=1$, and if $y=0\to v=0$. Therefore $0\leq v\leq 1$. Before I calculate the Jacobian, I notice that I cannot write $e^\frac{x+y}{x-y}$ in terms of $u,v$. The closest I can get is $e^\frac{x+y}{-u}$, but the $x+y$ is still left? Have I made an incorrect sub?","Evaluate $\displaystyle\iint_{D}e^\dfrac{x+y}{x-y}$ on the region $D$ using a change of variables Here is the region: The region is bounded by the following: $y=x-2, y=x-1, x=0, y=0$. A sub that is obvious is let $u=y-x$, then $-2\leq u\leq -1$. Now I have to make one more sub that takes care of the $x=0, y=0$, so I made the sub, let $v=\frac{y}{y-x}$. Then if $x=0\to v=1$, and if $y=0\to v=0$. Therefore $0\leq v\leq 1$. Before I calculate the Jacobian, I notice that I cannot write $e^\frac{x+y}{x-y}$ in terms of $u,v$. The closest I can get is $e^\frac{x+y}{-u}$, but the $x+y$ is still left? Have I made an incorrect sub?",,"['calculus', 'integration', 'multivariable-calculus', 'change-of-variable']"
74,"Integrating with respect to arc length (ds) vs. dx, or dy","Integrating with respect to arc length (ds) vs. dx, or dy",,"I'm learning line integrals. And I understand the concept of integration with respect to $ds$, but I don't know what $dx$ and $dy$ actually mean. In my calc textbook, it just says that $dx$ is the same thing as $x^{'}(t)dt$ and likewise for dy being equal to $y^{'}(t)dt$. But what do they mean physically. Intuitively I know that ds is the arc length, or a certain curtain in a sense. And I also know that if ds is present I just use the arc length formula, but for dx and dy I don't have to. I looked at the other questions but I never understood that well.","I'm learning line integrals. And I understand the concept of integration with respect to $ds$, but I don't know what $dx$ and $dy$ actually mean. In my calc textbook, it just says that $dx$ is the same thing as $x^{'}(t)dt$ and likewise for dy being equal to $y^{'}(t)dt$. But what do they mean physically. Intuitively I know that ds is the arc length, or a certain curtain in a sense. And I also know that if ds is present I just use the arc length formula, but for dx and dy I don't have to. I looked at the other questions but I never understood that well.",,"['calculus', 'integration', 'multivariable-calculus', 'vector-spaces']"
75,"If $\phi : M = \mathbb{R}^n \setminus \{0\} \to \mathbb{R}^n$ is defined by $\phi(x) = x + \frac{x}{|x|}$, find $\phi^{-1}$","If  is defined by , find",\phi : M = \mathbb{R}^n \setminus \{0\} \to \mathbb{R}^n \phi(x) = x + \frac{x}{|x|} \phi^{-1},"If $\phi : M = \mathbb{R}^n \setminus \{0\} \to \phi[M] \subseteq \mathbb{R}^n$ is defined by $\phi(x) = x + \frac{x}{|x|}$, find $\phi^{-1}$ I'm not sure how to go about finding $\phi^{-1}$ here. $\phi$ is clearly injective, and bijective on $M$, so $\phi^{-1}$ must exist, but I'm not entirely sure how to go about algebraically finding the inverse $\phi^{-1}(x)$. How can I go about doing so? If it helps the boundary of $\phi[M]$ is $S^{n-1}$.","If $\phi : M = \mathbb{R}^n \setminus \{0\} \to \phi[M] \subseteq \mathbb{R}^n$ is defined by $\phi(x) = x + \frac{x}{|x|}$, find $\phi^{-1}$ I'm not sure how to go about finding $\phi^{-1}$ here. $\phi$ is clearly injective, and bijective on $M$, so $\phi^{-1}$ must exist, but I'm not entirely sure how to go about algebraically finding the inverse $\phi^{-1}(x)$. How can I go about doing so? If it helps the boundary of $\phi[M]$ is $S^{n-1}$.",,"['functions', 'multivariable-calculus', 'inverse-function']"
76,"Calculate $\iiint_B x \text{ d}V$, where $B$ is tetrahedron with vertices $(0,0,0),(0,1,0),(0,0,1),(1,0,0)$","Calculate , where  is tetrahedron with vertices","\iiint_B x \text{ d}V B (0,0,0),(0,1,0),(0,0,1),(1,0,0)","Calculate $\displaystyle \iiint_B x\text{ d}V$, let $dV=dx\, dy\, dz$ (in that order) where $B$ is tetrahedron with vertices $(0,0,0),(0,1,0),(0,0,1),(1,0,0)$ Here is a picture: Lets fix $z$. Then $0\leq z\leq 1$, and we want to consider the projection on the $xy$ plane. We can see that $0\leq x\leq 1-y$, but what about $y$? $y$ must be a function of $z$ and all I see is that $y$ goes from $0$ to $1$?","Calculate $\displaystyle \iiint_B x\text{ d}V$, let $dV=dx\, dy\, dz$ (in that order) where $B$ is tetrahedron with vertices $(0,0,0),(0,1,0),(0,0,1),(1,0,0)$ Here is a picture: Lets fix $z$. Then $0\leq z\leq 1$, and we want to consider the projection on the $xy$ plane. We can see that $0\leq x\leq 1-y$, but what about $y$? $y$ must be a function of $z$ and all I see is that $y$ goes from $0$ to $1$?",,"['calculus', 'integration', 'multivariable-calculus', 'volume']"
77,Are these mixed partial derivatives equal?,Are these mixed partial derivatives equal?,,"Let's say I have a set of $m$ functions depending on $n$ variables $$x^{i} = x^{i}(s^{1}, ... , s^{n})$$ I use latin indices to denote $(1, ..., m)$ and greek letters to denote $(1, ..., n)$. I know $$\frac{\partial x^{i}}{\partial x^{j}} = \delta^{i}_{j}$$ $$ \implies \frac{\partial^{2} x^{i}}{\partial s^{\alpha} \partial x^{j}} = \frac{\partial (\delta^{i}_{j})}{\partial s^{\alpha}} = 0$$ What I'd like to know is whether commutativity of partial derivatives allows me to do this: $$\frac{\partial^{2} x^{i}}{\partial x^{j} \partial s^{\alpha}} = \frac{\partial^2 x^{i}}{\partial s^{\alpha} \partial x^{j}} = 0$$ And if I'm not allowed to do so, why? Thanks a lot, and I'm sorry if it is a silly question. Edit: For example, I get troubles when I'm in the simplest case, $m=1, n=1$. Let's say $x=s^2$ $$\frac{dx}{dx}=1$$ $$\implies \frac{d}{ds}\left( \frac{dx}{dx} \right) = 0$$ But when I do it in the opposite order: $$\frac{dx}{ds}=2s$$ $$\implies \frac{d}{dx}\left( \frac{dx}{ds} \right) = \frac{d}{dx}\left( 2s \right)$$ substituting $s = \sqrt{x}$ $$\frac{d(2s)}{dx} = \frac{d(2\sqrt{x})}{dx} = \frac{1}{\sqrt{x}} = \frac{1}{s}$$ What am I doing wrong?","Let's say I have a set of $m$ functions depending on $n$ variables $$x^{i} = x^{i}(s^{1}, ... , s^{n})$$ I use latin indices to denote $(1, ..., m)$ and greek letters to denote $(1, ..., n)$. I know $$\frac{\partial x^{i}}{\partial x^{j}} = \delta^{i}_{j}$$ $$ \implies \frac{\partial^{2} x^{i}}{\partial s^{\alpha} \partial x^{j}} = \frac{\partial (\delta^{i}_{j})}{\partial s^{\alpha}} = 0$$ What I'd like to know is whether commutativity of partial derivatives allows me to do this: $$\frac{\partial^{2} x^{i}}{\partial x^{j} \partial s^{\alpha}} = \frac{\partial^2 x^{i}}{\partial s^{\alpha} \partial x^{j}} = 0$$ And if I'm not allowed to do so, why? Thanks a lot, and I'm sorry if it is a silly question. Edit: For example, I get troubles when I'm in the simplest case, $m=1, n=1$. Let's say $x=s^2$ $$\frac{dx}{dx}=1$$ $$\implies \frac{d}{ds}\left( \frac{dx}{dx} \right) = 0$$ But when I do it in the opposite order: $$\frac{dx}{ds}=2s$$ $$\implies \frac{d}{dx}\left( \frac{dx}{ds} \right) = \frac{d}{dx}\left( 2s \right)$$ substituting $s = \sqrt{x}$ $$\frac{d(2s)}{dx} = \frac{d(2\sqrt{x})}{dx} = \frac{1}{\sqrt{x}} = \frac{1}{s}$$ What am I doing wrong?",,"['multivariable-calculus', 'partial-derivative', 'coordinate-systems']"
78,Confusion about differential of multivariable functions.,Confusion about differential of multivariable functions.,,"I have been given a function $F: \mathbb{R²} \to \mathbb{R³}$ and another function $\beta: J \to \mathbb{R³}: t \mapsto F(a + tx)$ where $x = v_1(1,0) + v_2(0,1) = (v_1,v_2)$ and $a$ is fixed. we can assume that all given functions are differentiable on their domain. I'm asked to find $\beta'(0)$ (this should be equal to $D_1F(a)v_1 + D_2F(a)v_2$) My attempt: Let $R(t) = a+tx$ Then $\beta'(0) = D\beta(0) = D(F\circ R)(0) = DF(R(0)) \circ DR(0)$ and then I'm stuck.","I have been given a function $F: \mathbb{R²} \to \mathbb{R³}$ and another function $\beta: J \to \mathbb{R³}: t \mapsto F(a + tx)$ where $x = v_1(1,0) + v_2(0,1) = (v_1,v_2)$ and $a$ is fixed. we can assume that all given functions are differentiable on their domain. I'm asked to find $\beta'(0)$ (this should be equal to $D_1F(a)v_1 + D_2F(a)v_2$) My attempt: Let $R(t) = a+tx$ Then $\beta'(0) = D\beta(0) = D(F\circ R)(0) = DF(R(0)) \circ DR(0)$ and then I'm stuck.",,['multivariable-calculus']
79,Calculating the volume of the unit ball in $\mathbb{R}^d$,Calculating the volume of the unit ball in,\mathbb{R}^d,How can I prove that: $$v_d= 2 v_{d-1} \int_{0} ^{1}(1-x^2)^{\frac{d-1}{2}}dx $$ without using polar coordinates. $v_d$ denotes the volume of the unit ball in $\mathbb{R}^d$,How can I prove that: $$v_d= 2 v_{d-1} \int_{0} ^{1}(1-x^2)^{\frac{d-1}{2}}dx $$ without using polar coordinates. $v_d$ denotes the volume of the unit ball in $\mathbb{R}^d$,,"['real-analysis', 'multivariable-calculus']"
80,"How to take the derivative of quadratic term that involves vectors, transposes, and matrices, with respect to a scalar","How to take the derivative of quadratic term that involves vectors, transposes, and matrices, with respect to a scalar",,"What are the proper steps to take the derivative with respect to $\alpha$ of $$\frac 1 2 (x - \alpha g)^T Q (x - \alpha g)$$ to get following? $$-(x-\alpha g)^T Q g$$ (where $\alpha$ is a scalar, $Q$ is a symmetric positive definite matrix, and $x$ and $g$ are vectors of proper size.) Background and further explanation: I saw this differentiation as part of a differentiation of a larger expression in Nocedal and Wright's book Numerical Optimization, just above (3.25). The authors skip explaining differentiation steps and obtain the minimizer $\alpha$, as that is their purpose. When I try to follow I see that the differentiation of this term must yield as above. Now my problem is, if I were to differentiate the term above I would not know that there would be a transpose there, or $Q$ would need to be in the middle. E.g. if everything were scalars the term would be $\frac 1 2 Q (x - \alpha g)^2$, and using the chain rule, I would obtain the derivative as $-Q(x-\alpha g)g$. Now that the vectors and transposes and matrices are involved, how should one apply the chain rule here properly?","What are the proper steps to take the derivative with respect to $\alpha$ of $$\frac 1 2 (x - \alpha g)^T Q (x - \alpha g)$$ to get following? $$-(x-\alpha g)^T Q g$$ (where $\alpha$ is a scalar, $Q$ is a symmetric positive definite matrix, and $x$ and $g$ are vectors of proper size.) Background and further explanation: I saw this differentiation as part of a differentiation of a larger expression in Nocedal and Wright's book Numerical Optimization, just above (3.25). The authors skip explaining differentiation steps and obtain the minimizer $\alpha$, as that is their purpose. When I try to follow I see that the differentiation of this term must yield as above. Now my problem is, if I were to differentiate the term above I would not know that there would be a transpose there, or $Q$ would need to be in the middle. E.g. if everything were scalars the term would be $\frac 1 2 Q (x - \alpha g)^2$, and using the chain rule, I would obtain the derivative as $-Q(x-\alpha g)g$. Now that the vectors and transposes and matrices are involved, how should one apply the chain rule here properly?",,"['multivariable-calculus', 'derivatives']"
81,"Evaluate the double integral for $f(x,y)=ye^x$ given $R = [2,4] \times [1,9]$",Evaluate the double integral for  given,"f(x,y)=ye^x R = [2,4] \times [1,9]","So I did first did it by integrating with respect to $y$ first then $x$ and eventually got the answer of $40(e^4-e^2)$, which is correct. But when I attempted to apply Fubini's theorem and switch the order of integration, I should get the same answer but I'm not. Possible made an error somewhere? What I did: \begin{align}\text{Double integral} &= \int_1^9 \int_2^4 ye^x\, dx\, dy \\ &= \int_1^9 \left.ye^x \right\vert_{x=2}^{x=4}\, dy \\ &= \int_1^9 (ye^4 - ye^2) \,dy\\ &= \left[\left[\frac{y^2}2\right]e^4 - \left[\frac{y^2}2\right]e^2 \right]_ {y=1}^{y=9}\\ &=  \end{align} and I got similar numbers as to when I integrated with $dx$ then $dy$, but couldn't get $40(e^4-e^2)$. Can someone show their algebra for after integrating?","So I did first did it by integrating with respect to $y$ first then $x$ and eventually got the answer of $40(e^4-e^2)$, which is correct. But when I attempted to apply Fubini's theorem and switch the order of integration, I should get the same answer but I'm not. Possible made an error somewhere? What I did: \begin{align}\text{Double integral} &= \int_1^9 \int_2^4 ye^x\, dx\, dy \\ &= \int_1^9 \left.ye^x \right\vert_{x=2}^{x=4}\, dy \\ &= \int_1^9 (ye^4 - ye^2) \,dy\\ &= \left[\left[\frac{y^2}2\right]e^4 - \left[\frac{y^2}2\right]e^2 \right]_ {y=1}^{y=9}\\ &=  \end{align} and I got similar numbers as to when I integrated with $dx$ then $dy$, but couldn't get $40(e^4-e^2)$. Can someone show their algebra for after integrating?",,['multivariable-calculus']
82,"Evaluate the triple integral $\iiint_E x\,dV$ where $E$ is bounded by the paraboloid $x=4y^2+4z^2$ and the plane $x=4$.",Evaluate the triple integral  where  is bounded by the paraboloid  and the plane .,"\iiint_E x\,dV E x=4y^2+4z^2 x=4","Evaluate the triple integral   $$\iiint_E x\,dV$$   where $E$ is bounded by the paraboloid $x=4y^2+4z^2$ and the plane $x=4$. I have been analyzing the part of my book where it evaluates triple integrals for paraboloids non stop, but I can't seem to figure out the method for setting it up. (and solving) I have a feeling one of the integrals will be the paraboloid given as an upper bound and the plane given as a lower bound, but I'm not sure how to get the other bounds without having to manually graph a bunch of points till i can see where everything intersects. I remember setting equations to each other to get intersections but I'm not sure how to apply that here. If someone could show me a detailed explanation of how to set this up (and solve) it would help a lot. Thanks. Edit: I have a feeling I'm supposed to put for my outer integral $x$ is from $0$ to $4$, and my inner integrals I use the $\pm$ solutions for $y$ and $z$. Is that right? But I'm not sure how to solve it from here.","Evaluate the triple integral   $$\iiint_E x\,dV$$   where $E$ is bounded by the paraboloid $x=4y^2+4z^2$ and the plane $x=4$. I have been analyzing the part of my book where it evaluates triple integrals for paraboloids non stop, but I can't seem to figure out the method for setting it up. (and solving) I have a feeling one of the integrals will be the paraboloid given as an upper bound and the plane given as a lower bound, but I'm not sure how to get the other bounds without having to manually graph a bunch of points till i can see where everything intersects. I remember setting equations to each other to get intersections but I'm not sure how to apply that here. If someone could show me a detailed explanation of how to set this up (and solve) it would help a lot. Thanks. Edit: I have a feeling I'm supposed to put for my outer integral $x$ is from $0$ to $4$, and my inner integrals I use the $\pm$ solutions for $y$ and $z$. Is that right? But I'm not sure how to solve it from here.",,"['calculus', 'integration', 'multivariable-calculus', 'euclidean-geometry']"
83,"A coordinate independent interpretation of $\vec u\cdot\nabla\vec v$ for vector fields $\vec u,\vec v$?",A coordinate independent interpretation of  for vector fields ?,"\vec u\cdot\nabla\vec v \vec u,\vec v","I'm having trouble conceptualizing what exactly is meant by the term $\mathbf u\cdot \nabla\mathbf v$ for vector fields $\mathbf u,\mathbf v$ on $\mathbb{R}^n$, say. I know that we can describe this literally as $$\mathbf u\cdot\nabla\mathbf v = (u^i\partial_iv^j)_j$$ but this doesn't exactly seem to be coordinate invariant. My difficulty is in coming up with change-of-variables formulae, since these seem fairly complicated.","I'm having trouble conceptualizing what exactly is meant by the term $\mathbf u\cdot \nabla\mathbf v$ for vector fields $\mathbf u,\mathbf v$ on $\mathbb{R}^n$, say. I know that we can describe this literally as $$\mathbf u\cdot\nabla\mathbf v = (u^i\partial_iv^j)_j$$ but this doesn't exactly seem to be coordinate invariant. My difficulty is in coming up with change-of-variables formulae, since these seem fairly complicated.",,"['multivariable-calculus', 'differential-geometry', 'fluid-dynamics']"
84,what is $d(y/x)$ in $ d\log(y/x)=\frac{d(y/x)}{(y/x)}$?,what is  in ?,d(y/x)  d\log(y/x)=\frac{d(y/x)}{(y/x)},"For example what is meant by $d(y/x)$ in right hand numerator in the following? $$ d\log(y/x)=\frac{d(y/x)}{(y/x)}$$ Is the same as $\frac{dy}{dx}$? If not, what does it mean?","For example what is meant by $d(y/x)$ in right hand numerator in the following? $$ d\log(y/x)=\frac{d(y/x)}{(y/x)}$$ Is the same as $\frac{dy}{dx}$? If not, what does it mean?",,"['calculus', 'multivariable-calculus', 'derivatives']"
85,Uniform convergence of difference quotient function implies differentiability in $\mathbb{R}^n$,Uniform convergence of difference quotient function implies differentiability in,\mathbb{R}^n,"This arises in the context of the proof of the Rademacher theorem. Suppose that $f: \mathbb{R}^n \to \mathbb{R}$ is a Lipschitz function. It therefore has a distributional gradient $L$ and suppose that $x$ is a point in the Lebesgue set of $L$. We can define $f_r(y) = \frac{f(x+ry)-f(x)}{r}$ and the claim is that if $f_r(y) \to \langle L(x), y\rangle$ uniformly as $r \to 0$ then $f$ is differentiable at $x$. Why is this true and why is uniform convergence necessary rather than just point wise convergence?","This arises in the context of the proof of the Rademacher theorem. Suppose that $f: \mathbb{R}^n \to \mathbb{R}$ is a Lipschitz function. It therefore has a distributional gradient $L$ and suppose that $x$ is a point in the Lebesgue set of $L$. We can define $f_r(y) = \frac{f(x+ry)-f(x)}{r}$ and the claim is that if $f_r(y) \to \langle L(x), y\rangle$ uniformly as $r \to 0$ then $f$ is differentiable at $x$. Why is this true and why is uniform convergence necessary rather than just point wise convergence?",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-differential-equations', 'lipschitz-functions']"
86,How to show $F_1/F_2$ is bounded?,How to show  is bounded?,F_1/F_2,"If $P(x,y)$ and $Q(x,y)$ are homogenous polynomials of degree $n$ and   $m$ respectively, then the function $f$ is defined as   $f(x,y)=\frac{P(x,y)}{Q(x,y)}$ [when $Q(x,y)\neq 0$] and 0 [when   $Q(x,y)=0$]. This type of a function will be continuous at $(0,0)$ if   $n>m$. I was thinking of proving it using $P(x,y)=x^nF_1(y/x)$ and $P(x,y)=x^mF_2(y/x)$ and writing $f(x,y)$ as $x^{n-m}(F_1/F_2)$. If $F_1$ by $F_2$ is bounded then we can say that the function $f(x,y)$ is continuous at $(0,0)$ since the double limit $(x,y)\to (0,0)$ exists there. Or perhaps even using polar coordinates might be useful. Any hints how to show $F_1/F_2$ is bounded?","If $P(x,y)$ and $Q(x,y)$ are homogenous polynomials of degree $n$ and   $m$ respectively, then the function $f$ is defined as   $f(x,y)=\frac{P(x,y)}{Q(x,y)}$ [when $Q(x,y)\neq 0$] and 0 [when   $Q(x,y)=0$]. This type of a function will be continuous at $(0,0)$ if   $n>m$. I was thinking of proving it using $P(x,y)=x^nF_1(y/x)$ and $P(x,y)=x^mF_2(y/x)$ and writing $f(x,y)$ as $x^{n-m}(F_1/F_2)$. If $F_1$ by $F_2$ is bounded then we can say that the function $f(x,y)$ is continuous at $(0,0)$ since the double limit $(x,y)\to (0,0)$ exists there. Or perhaps even using polar coordinates might be useful. Any hints how to show $F_1/F_2$ is bounded?",,['limits']
87,"Hey guys, I have a question about differentiability of the function of two variables","Hey guys, I have a question about differentiability of the function of two variables",,"Is the function $$ f(x, y) = \begin{cases}y^2 \over x^2 + y^2 & (x,y) \neq (0, 0) \newline  0 & (x, y) = (0, 0)\end{cases} $$ differentiable at $(0,0)$? The thing is that first we have checked for function's continuity and it turned out that $\lim_{k \to 0} f(k,0)=0$ and $\lim_{k \to 0} f(0,k) \to 1$ which basically implies a discontinuity of our initial function which in turn asserts the impossibility of our function being differentiable. But it seems too easy. Any help would be appreciated.","Is the function $$ f(x, y) = \begin{cases}y^2 \over x^2 + y^2 & (x,y) \neq (0, 0) \newline  0 & (x, y) = (0, 0)\end{cases} $$ differentiable at $(0,0)$? The thing is that first we have checked for function's continuity and it turned out that $\lim_{k \to 0} f(k,0)=0$ and $\lim_{k \to 0} f(0,k) \to 1$ which basically implies a discontinuity of our initial function which in turn asserts the impossibility of our function being differentiable. But it seems too easy. Any help would be appreciated.",,"['calculus', 'functional-analysis', 'multivariable-calculus', 'derivatives', 'continuity']"
88,"Connection of gradient, Jacobian and Hessian in Newton's method","Connection of gradient, Jacobian and Hessian in Newton's method",,"Suppose $f: \mathbb{R^n} \to \mathbb{R}$, the gradient of $f(\mathbf{x})$ is $$\mathop{\nabla} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial{f}}{\partial{x_1}} \\ \vdots \\ \frac{\partial{f}}{\partial{x_n}} \end{bmatrix}$$ The Jacobian matrix of $\mathop{\nabla} f(\mathbf{x})$ is $$\begin{align} \mathbf{D} (\mathop{\nabla} f(\mathbf{x})) &= \begin{bmatrix} \frac{{\partial^2}f}{{\partial}x_1^2} & \frac{{\partial^2}f}{{\partial}x_2{\partial}x_1} & \cdots & \frac{{\partial^2}f}{{\partial}x_n{\partial}x_1}\\ \frac{{\partial^2}f}{{\partial}x_1{\partial}x_2} & \frac{{\partial^2}f}{{\partial}x_2^2} & \cdots & \frac{{\partial^2}f}{{\partial}x_n{\partial}x_2}\\ \vdots & \vdots & \ddots & \vdots\\ \frac{{\partial^2}f}{{\partial}x_1{\partial}x_n} & \frac{{\partial^2}f}{{\partial}x_2{\partial}x_n} & \cdots & \frac{{\partial^2}f}{{\partial}x_n^2}\\  \end{bmatrix} \\ &= \mathbf{H}^T \end{align}$$ where H is the Hessian matrix, which is consistent with the definition in Wikipedia . The affine approximation of $\mathop{\nabla} f(\mathbf{x})$ around $x_n$ is $$\mathop{\nabla} f(\mathbf{x}) = \mathop{\nabla} f(\mathbf{x_n}) + \mathbf{D}  (\mathop{\nabla} f(\mathbf{x_n})) (x - x_n) = \mathop{\nabla} f(\mathbf{x_n}) + \mathbf{H}^T (x - x_n)$$ Setting $\mathop{\nabla} f(\mathbf{x}) = 0$ gives the Newton-Raphson update as $$x_{n+1} := x_n - \mathbf{H}^{-T} \mathop{\nabla} f(\mathbf{x_n}) $$ However, in Wikipedia the Newton-Raphson update is given as $x_{n+1} := x_n - \mathbf{H}^{-1} \mathop{\nabla} f(\mathbf{x_n})$. The Hessian matrix is not symmetric if the entry of the matrix is not continuous. Did I do anything wrong with my calculation? If not, does this mean we can generally treat Hessian matrix as symmetric in practice for optimization?","Suppose $f: \mathbb{R^n} \to \mathbb{R}$, the gradient of $f(\mathbf{x})$ is $$\mathop{\nabla} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial{f}}{\partial{x_1}} \\ \vdots \\ \frac{\partial{f}}{\partial{x_n}} \end{bmatrix}$$ The Jacobian matrix of $\mathop{\nabla} f(\mathbf{x})$ is $$\begin{align} \mathbf{D} (\mathop{\nabla} f(\mathbf{x})) &= \begin{bmatrix} \frac{{\partial^2}f}{{\partial}x_1^2} & \frac{{\partial^2}f}{{\partial}x_2{\partial}x_1} & \cdots & \frac{{\partial^2}f}{{\partial}x_n{\partial}x_1}\\ \frac{{\partial^2}f}{{\partial}x_1{\partial}x_2} & \frac{{\partial^2}f}{{\partial}x_2^2} & \cdots & \frac{{\partial^2}f}{{\partial}x_n{\partial}x_2}\\ \vdots & \vdots & \ddots & \vdots\\ \frac{{\partial^2}f}{{\partial}x_1{\partial}x_n} & \frac{{\partial^2}f}{{\partial}x_2{\partial}x_n} & \cdots & \frac{{\partial^2}f}{{\partial}x_n^2}\\  \end{bmatrix} \\ &= \mathbf{H}^T \end{align}$$ where H is the Hessian matrix, which is consistent with the definition in Wikipedia . The affine approximation of $\mathop{\nabla} f(\mathbf{x})$ around $x_n$ is $$\mathop{\nabla} f(\mathbf{x}) = \mathop{\nabla} f(\mathbf{x_n}) + \mathbf{D}  (\mathop{\nabla} f(\mathbf{x_n})) (x - x_n) = \mathop{\nabla} f(\mathbf{x_n}) + \mathbf{H}^T (x - x_n)$$ Setting $\mathop{\nabla} f(\mathbf{x}) = 0$ gives the Newton-Raphson update as $$x_{n+1} := x_n - \mathbf{H}^{-T} \mathop{\nabla} f(\mathbf{x_n}) $$ However, in Wikipedia the Newton-Raphson update is given as $x_{n+1} := x_n - \mathbf{H}^{-1} \mathop{\nabla} f(\mathbf{x_n})$. The Hessian matrix is not symmetric if the entry of the matrix is not continuous. Did I do anything wrong with my calculation? If not, does this mean we can generally treat Hessian matrix as symmetric in practice for optimization?",,"['multivariable-calculus', 'vector-analysis', 'newton-raphson', 'jacobian', 'hessian-matrix']"
89,"Find local maximum, minimum and saddle points of $f(x,y) = x^4 + y^4 - 4xy + 2$","Find local maximum, minimum and saddle points of","f(x,y) = x^4 + y^4 - 4xy + 2","Find maximum, minimum and saddle points of $f(x,y) = x^4 + y^4 - 4xy + 2$. For critical points, $f_{x} = f_{y} = 0$, $f_{x} = 3x³ - 4y$ and $f_{y} = 3y³ - 4x$. Therefore $f_{xx} = 9x²$, $f_{yy} = 9y²$, and $f_{xy} = f_{yx} = -4$. Hessian matrix determinant is positive for P($\frac{2}{\sqrt3},\frac{2}{\sqrt3},-14)$ and Q($\frac{-2}{\sqrt3}$,$\frac{-2}{\sqrt3}$,-14), so they are minimum points. And R(0,0,2) is a saddle point. Is this correct?","Find maximum, minimum and saddle points of $f(x,y) = x^4 + y^4 - 4xy + 2$. For critical points, $f_{x} = f_{y} = 0$, $f_{x} = 3x³ - 4y$ and $f_{y} = 3y³ - 4x$. Therefore $f_{xx} = 9x²$, $f_{yy} = 9y²$, and $f_{xy} = f_{yx} = -4$. Hessian matrix determinant is positive for P($\frac{2}{\sqrt3},\frac{2}{\sqrt3},-14)$ and Q($\frac{-2}{\sqrt3}$,$\frac{-2}{\sqrt3}$,-14), so they are minimum points. And R(0,0,2) is a saddle point. Is this correct?",,"['multivariable-calculus', 'partial-derivative', 'maxima-minima', 'a.m.-g.m.-inequality', 'hessian-matrix']"
90,"Let $f(x,y)$ be of class $C^2$. Find a formula for $\dfrac{\partial^2f}{\partial u\partial v}$ where $x=2u-3v$, $y=u+4v$","Let  be of class . Find a formula for  where ,","f(x,y) C^2 \dfrac{\partial^2f}{\partial u\partial v} x=2u-3v y=u+4v","Let $f(x,y)$ be of class $C^2$. Find a formula for $\dfrac{\partial^2f}{\partial u\partial v}$ where $x=2u-3v$, $y=u+4v$ Basically we know that: $$\dfrac{\partial^2f}{\partial u\partial v}=\dfrac{\partial}{\partial v}(\dfrac{\partial f}{\partial u})$$ Now, $$\dfrac{\partial f}{\partial u}=\dfrac{\partial f}{\partial x}*\dfrac{\partial x}{\partial u}+\dfrac{\partial f}{\partial y}*\dfrac{\partial y}{\partial u}=2\dfrac{\partial f}{\partial x}+1\dfrac{\partial f}{\partial y}$$ So now we must find: $$\dfrac{\partial}{\partial v}(2\dfrac{\partial f}{\partial x}+1\dfrac{\partial f}{\partial y})$$ But I don't know how to further simplify this? For example, what exactly is $2\dfrac{\partial}{\partial v}(\dfrac{\partial f}{\partial x})$. What is $\dfrac{\partial f}{\partial x}$?","Let $f(x,y)$ be of class $C^2$. Find a formula for $\dfrac{\partial^2f}{\partial u\partial v}$ where $x=2u-3v$, $y=u+4v$ Basically we know that: $$\dfrac{\partial^2f}{\partial u\partial v}=\dfrac{\partial}{\partial v}(\dfrac{\partial f}{\partial u})$$ Now, $$\dfrac{\partial f}{\partial u}=\dfrac{\partial f}{\partial x}*\dfrac{\partial x}{\partial u}+\dfrac{\partial f}{\partial y}*\dfrac{\partial y}{\partial u}=2\dfrac{\partial f}{\partial x}+1\dfrac{\partial f}{\partial y}$$ So now we must find: $$\dfrac{\partial}{\partial v}(2\dfrac{\partial f}{\partial x}+1\dfrac{\partial f}{\partial y})$$ But I don't know how to further simplify this? For example, what exactly is $2\dfrac{\partial}{\partial v}(\dfrac{\partial f}{\partial x})$. What is $\dfrac{\partial f}{\partial x}$?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
91,A General Process for Finding Correct Bounds on Double Integral?,A General Process for Finding Correct Bounds on Double Integral?,,"So I'm working on some multivariable calculus homework, and I can't seem to figure out why my professor takes this particular approach to the solution... The Question: $$ S = \{(x,y) \in R^2: 0 \leq x \leq 1, 0 \leq y \leq sin^{-1}x\} $$  And we have to evaluate $\int \int_{S} dA $ My professor's approach to this problem involves changing the integral bounds, so instead of the double integral setup looking like: $\int_{0}^{1} \int_{0}^{sin^{-1}}dydx$, it looks like $\int_{0}^{\pi/2} \int_{sin(y)}^{1}dxdy$ Can someone please explain how he got to this rearranged integral bounds setup, and additionally is there a general process for rewriting the integral bounds for a double integral?","So I'm working on some multivariable calculus homework, and I can't seem to figure out why my professor takes this particular approach to the solution... The Question: $$ S = \{(x,y) \in R^2: 0 \leq x \leq 1, 0 \leq y \leq sin^{-1}x\} $$  And we have to evaluate $\int \int_{S} dA $ My professor's approach to this problem involves changing the integral bounds, so instead of the double integral setup looking like: $\int_{0}^{1} \int_{0}^{sin^{-1}}dydx$, it looks like $\int_{0}^{\pi/2} \int_{sin(y)}^{1}dxdy$ Can someone please explain how he got to this rearranged integral bounds setup, and additionally is there a general process for rewriting the integral bounds for a double integral?",,"['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
92,Why not differentiate the Lagrangian w.r.t a lagrange multiplier?,Why not differentiate the Lagrangian w.r.t a lagrange multiplier?,,"I've heard from a reuptable source that it is problematic to differentiate the Lagrangian w.r.t the lagrange multiplier. I know that doing so is rather a waste of time since it just goves you back the constraints $h(x)=A$ that you started with, but this repitable source said that there is an additional reason why it is not only a waste of time but also problematic? Any hints as to why it is problematic?","I've heard from a reuptable source that it is problematic to differentiate the Lagrangian w.r.t the lagrange multiplier. I know that doing so is rather a waste of time since it just goves you back the constraints $h(x)=A$ that you started with, but this repitable source said that there is an additional reason why it is not only a waste of time but also problematic? Any hints as to why it is problematic?",,"['multivariable-calculus', 'lagrange-multiplier', 'constraints']"
93,Need some hint with limit with multiple variables,Need some hint with limit with multiple variables,,"The limit $$ \lim_{(x,y)\to(0,0)} \frac{x^{1/3}y^2}{x+y^3} $$ does exist since when we set $y=0$, $x=0$, and $x=y$ (it all equals to $0$). So now we need to evaluate the limit. The L'Hospital rule does not apply with multiple variables, so I am stuck of how to approach it. I understand that I need to somehow manipulate it, but I have no idea how. Any hints?","The limit $$ \lim_{(x,y)\to(0,0)} \frac{x^{1/3}y^2}{x+y^3} $$ does exist since when we set $y=0$, $x=0$, and $x=y$ (it all equals to $0$). So now we need to evaluate the limit. The L'Hospital rule does not apply with multiple variables, so I am stuck of how to approach it. I understand that I need to somehow manipulate it, but I have no idea how. Any hints?",,['limits']
94,Solving the equation $c=\dfrac{x^2+y^2-1}{x^2+(y+1)^2}$,Solving the equation,c=\dfrac{x^2+y^2-1}{x^2+(y+1)^2},"Solving the equation $c=\dfrac{x^2+y^2-1}{x^2+(y+1)^2}$ $$c=\dfrac{x^2+y^2-1}{x^2+(y+1)^2}$$ $$c{x^2+c(y+1)^2}={x^2+y^2-1}$$ $$c{x^2+cy^2+2cy+c}={x^2+y^2-1}\text{ [expanded]}$$ $$1+c=x^2-cx^2+y^2-cy^2-2cy\text{ [moved to other side]}$$ $$1+c=(1-c)x^2+\color{red}{(1-c)y^2-2cy}\text{ [factor, will complete square of red]}$$ $$1+c=(1-c)x^2+\color{red}{(1-c)(y^2-\dfrac{2cy}{c-1}+(\dfrac{2c}{c-1})^2)-(\dfrac{2c}{c-1})^2}\text{ [completed square]}$$ $$1+c+\color{red}{(\dfrac{2c}{c-1})^2}=(1-c)x^2+\color{red}{(1-c)(y-\dfrac{2c}{c-1})^2}\text{ [done]}$$ So we see that this is an ellipse, stretched in the $y$ direction by a factor of $1-c$, and translated both in $x$ and $y$. $c\neq 1$. Consider $c=2$ $19=-x^2-(y+16)^2$ $-19=x^2+(y+16)^2$ But this not a circle nor an ellipse. It's supposed to be a circle? What is the issue here?","Solving the equation $c=\dfrac{x^2+y^2-1}{x^2+(y+1)^2}$ $$c=\dfrac{x^2+y^2-1}{x^2+(y+1)^2}$$ $$c{x^2+c(y+1)^2}={x^2+y^2-1}$$ $$c{x^2+cy^2+2cy+c}={x^2+y^2-1}\text{ [expanded]}$$ $$1+c=x^2-cx^2+y^2-cy^2-2cy\text{ [moved to other side]}$$ $$1+c=(1-c)x^2+\color{red}{(1-c)y^2-2cy}\text{ [factor, will complete square of red]}$$ $$1+c=(1-c)x^2+\color{red}{(1-c)(y^2-\dfrac{2cy}{c-1}+(\dfrac{2c}{c-1})^2)-(\dfrac{2c}{c-1})^2}\text{ [completed square]}$$ $$1+c+\color{red}{(\dfrac{2c}{c-1})^2}=(1-c)x^2+\color{red}{(1-c)(y-\dfrac{2c}{c-1})^2}\text{ [done]}$$ So we see that this is an ellipse, stretched in the $y$ direction by a factor of $1-c$, and translated both in $x$ and $y$. $c\neq 1$. Consider $c=2$ $19=-x^2-(y+16)^2$ $-19=x^2+(y+16)^2$ But this not a circle nor an ellipse. It's supposed to be a circle? What is the issue here?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'circles', 'factoring']"
95,Help with Multivariable Limits,Help with Multivariable Limits,,"Currently struggling over a limit problem I've gone back and forth with for a few hours now. Any help would be appreciated. $$\lim_{(x,y) \to (0,0)}\left[\sin(x^2+y^2)\ln(x^2+y^2)\right]$$ I tried converting to polar coordinates, but the pesky $\ln (0)$ doesn't seem to go away. The only thing I can really do is prove it does exist, but I don't suppose that does much for me. Any help would be great.","Currently struggling over a limit problem I've gone back and forth with for a few hours now. Any help would be appreciated. I tried converting to polar coordinates, but the pesky doesn't seem to go away. The only thing I can really do is prove it does exist, but I don't suppose that does much for me. Any help would be great.","\lim_{(x,y) \to (0,0)}\left[\sin(x^2+y^2)\ln(x^2+y^2)\right] \ln (0)","['calculus', 'limits', 'multivariable-calculus']"
96,"Draw level curves for $f(x,y)=\frac{x^2+y^2}{xy}$",Draw level curves for,"f(x,y)=\frac{x^2+y^2}{xy}","Draw level curves for $f(x,y)=\dfrac{x^2+y^2}{xy}$ Let $z=\dfrac{x^2+y^2}{xy}$ $z=0\to x^2+y^2 = 0$, which is a circle of radius $0$, or nothing. $z=1\to x^2+y^2=xy\to \color{red}{x^2-xy+y^2=0}$ How do I even graph this? How do I know what it looks like. I tried plotting on Wolfram Alpha but the result is literally nothing.","Draw level curves for $f(x,y)=\dfrac{x^2+y^2}{xy}$ Let $z=\dfrac{x^2+y^2}{xy}$ $z=0\to x^2+y^2 = 0$, which is a circle of radius $0$, or nothing. $z=1\to x^2+y^2=xy\to \color{red}{x^2-xy+y^2=0}$ How do I even graph this? How do I know what it looks like. I tried plotting on Wolfram Alpha but the result is literally nothing.",,"['multivariable-calculus', 'curves']"
97,Parametric form of hyperplanes,Parametric form of hyperplanes,,"The standard definition of a hyperplane in $\mathbb{R}^n$ is a set $\mathbb{H}$ of the form $$\mathbb{H}=\left\{(x_1, x_2, \ldots, x_n) \in \mathbb{R}^n : \sum_{i=1}^n \alpha_ix_i = L \right\}$$ where $L$ is fixed, and the $\alpha_i$'s are fixed and not all $0$. We can also think of the hyperplane as a subset of $\mathbb{R}^n$ in ""parametric"" terms. In other words, for some linearly independent subset $l = \left\{\overline{y}_1, \overline{y}_2, \ldots, \overline{y}_{n-1}\right\} \subset \mathbb{R}^n$ and a ""base"" point $\overline{r} \in \mathbb{R}^n$ a hyperplane is the image of the function $g:\mathbb{R}^{n-1} \to \mathbb{R}^n$ given by $g(t_1, t_2, \ldots, t_{n-1}) = \left(\sum_{i=1}^{n-1} t_i\overline{y_i}\right) + \overline{r}$. Given the ""standard"" definition of the hyperplane above, is it possible to explicitly describe the plane parametrically in terms of the second definition (ie., find the set $l$ and $r$ corresponding to $\mathbb{H}$)?","The standard definition of a hyperplane in $\mathbb{R}^n$ is a set $\mathbb{H}$ of the form $$\mathbb{H}=\left\{(x_1, x_2, \ldots, x_n) \in \mathbb{R}^n : \sum_{i=1}^n \alpha_ix_i = L \right\}$$ where $L$ is fixed, and the $\alpha_i$'s are fixed and not all $0$. We can also think of the hyperplane as a subset of $\mathbb{R}^n$ in ""parametric"" terms. In other words, for some linearly independent subset $l = \left\{\overline{y}_1, \overline{y}_2, \ldots, \overline{y}_{n-1}\right\} \subset \mathbb{R}^n$ and a ""base"" point $\overline{r} \in \mathbb{R}^n$ a hyperplane is the image of the function $g:\mathbb{R}^{n-1} \to \mathbb{R}^n$ given by $g(t_1, t_2, \ldots, t_{n-1}) = \left(\sum_{i=1}^{n-1} t_i\overline{y_i}\right) + \overline{r}$. Given the ""standard"" definition of the hyperplane above, is it possible to explicitly describe the plane parametrically in terms of the second definition (ie., find the set $l$ and $r$ corresponding to $\mathbb{H}$)?",,"['linear-algebra', 'multivariable-calculus']"
98,Find a formula for the density at any point at any time.,Find a formula for the density at any point at any time.,,"Can anyone help me with this problem? Let the density per unit of volume in a cubical box of side length 2 vary directly as the distance from the center and inversely as $1+t^{2}$ where $t$ is the time. If the density at a corner of the box is 1 when $t=0$, find a formula for the density at any point at any time. What is the rate of change of the density at a point  $\frac{1}{2}$ unit from the center of the box at time $t = 1$ ?","Can anyone help me with this problem? Let the density per unit of volume in a cubical box of side length 2 vary directly as the distance from the center and inversely as $1+t^{2}$ where $t$ is the time. If the density at a corner of the box is 1 when $t=0$, find a formula for the density at any point at any time. What is the rate of change of the density at a point  $\frac{1}{2}$ unit from the center of the box at time $t = 1$ ?",,['multivariable-calculus']
99,"$f : \mathbb R^n\to\mathbb R$ is differentiable at $c$ if and only if $f(c+h)-f(c) = o(\langle v, h\rangle)$ for some $v$ : constant vector",is differentiable at  if and only if  for some  : constant vector,"f : \mathbb R^n\to\mathbb R c f(c+h)-f(c) = o(\langle v, h\rangle) v","This question is from Introduction to Mathematical analysis, Steven A. Douglass Exercise 8.3 (d). Let $f$ be a real-valued function defined on an open set $U$ in $\mathbb R^n$. Let $c$ be a point of $U$. Show that $f$ is differentiable at $c$ in $U$ if and only if $f(c+h) - f(c) = o(\langle v ,h\rangle)$ for some constant vector $v$ in $R^n$. (The vector $h$ is the variable.) ($\langle v,h\rangle$ denotes the inner product of $v$ and $h$.) I tried to solve the question, but I couldn't prove both direction, and here's what I tried. $f$ is differentiable at c $\Rightarrow$ for every $\varepsilon > 0,\  \exists \delta>0,\  |f(c+h)-f(c)- \langle \nabla f(c) , h\rangle| \le \varepsilon|h|$. Since I know that there exists some vector $v$ $s.t$ for every $\varepsilon > 0$, $\exists \delta>0,\ |f(c+h)-f(c)- \langle v , h\rangle| \le \varepsilon|h|$ $\Rightarrow$ $f$ is differentiable at $c$, I wanted to show the problem's condition for vector $v$ and the equation above are equivalent, but I think it doesn't work. From the differentiablity, I know that $|f(c+h)-f(c)|$ $\le$ $|\langle \nabla f, h\rangle|+ \varepsilon |h|$ as |h| goes to $0$. Of course, dividing by $|\langle \nabla f, h\rangle|$ does not satisfy the condition of the problem. Even if we divide the both side by $|\langle v , h\rangle|$ for general vector $v$, the term $\varepsilon \frac{|h|}{\langle v,h\rangle}$ is being arbitrarily large if vector $v$ is perpendicular to $h$, so I even doubt if the  theorem of the problem is true. Any ideas for the question?","This question is from Introduction to Mathematical analysis, Steven A. Douglass Exercise 8.3 (d). Let $f$ be a real-valued function defined on an open set $U$ in $\mathbb R^n$. Let $c$ be a point of $U$. Show that $f$ is differentiable at $c$ in $U$ if and only if $f(c+h) - f(c) = o(\langle v ,h\rangle)$ for some constant vector $v$ in $R^n$. (The vector $h$ is the variable.) ($\langle v,h\rangle$ denotes the inner product of $v$ and $h$.) I tried to solve the question, but I couldn't prove both direction, and here's what I tried. $f$ is differentiable at c $\Rightarrow$ for every $\varepsilon > 0,\  \exists \delta>0,\  |f(c+h)-f(c)- \langle \nabla f(c) , h\rangle| \le \varepsilon|h|$. Since I know that there exists some vector $v$ $s.t$ for every $\varepsilon > 0$, $\exists \delta>0,\ |f(c+h)-f(c)- \langle v , h\rangle| \le \varepsilon|h|$ $\Rightarrow$ $f$ is differentiable at $c$, I wanted to show the problem's condition for vector $v$ and the equation above are equivalent, but I think it doesn't work. From the differentiablity, I know that $|f(c+h)-f(c)|$ $\le$ $|\langle \nabla f, h\rangle|+ \varepsilon |h|$ as |h| goes to $0$. Of course, dividing by $|\langle \nabla f, h\rangle|$ does not satisfy the condition of the problem. Even if we divide the both side by $|\langle v , h\rangle|$ for general vector $v$, the term $\varepsilon \frac{|h|}{\langle v,h\rangle}$ is being arbitrarily large if vector $v$ is perpendicular to $h$, so I even doubt if the  theorem of the problem is true. Any ideas for the question?",,"['analysis', 'multivariable-calculus', 'derivatives']"
