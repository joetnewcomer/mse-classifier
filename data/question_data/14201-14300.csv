,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is there any identity for $\sum_{k=0}^{n-1}\tan(x+ka) $??,Is there any identity for ??,\sum_{k=0}^{n-1}\tan(x+ka) ,I found this series $$ \sum_{k=0}^{n-1}\tan\left(\theta+\frac{k\pi}{n}\right)=−n\cot\left(\frac{n\pi}{2}+n\theta\right) $$ but it's not what I need.,I found this series $$ \sum_{k=0}^{n-1}\tan\left(\theta+\frac{k\pi}{n}\right)=−n\cot\left(\frac{n\pi}{2}+n\theta\right) $$ but it's not what I need.,,"['calculus', 'sequences-and-series', 'trigonometry', 'trigonometric-series']"
1,What is the difference between a calculus and an algebra? [duplicate],What is the difference between a calculus and an algebra? [duplicate],,"This question already has answers here : What do Algebra and Calculus mean? (4 answers) Closed 9 years ago . You can have a lambda calculus , the calculus of the real numbers or a logical calculus but on the other hand you could also have an algebra of sets, a Lie algebra , or a linear algebra . Is there any convention which dictates whether something is an algebra or a calculus ? From my understanding, a calculus seems to give you information about objects you might encounter within a space, while an algebra gives you information about the interactions and structure within the space as a whole. Is this correct?","This question already has answers here : What do Algebra and Calculus mean? (4 answers) Closed 9 years ago . You can have a lambda calculus , the calculus of the real numbers or a logical calculus but on the other hand you could also have an algebra of sets, a Lie algebra , or a linear algebra . Is there any convention which dictates whether something is an algebra or a calculus ? From my understanding, a calculus seems to give you information about objects you might encounter within a space, while an algebra gives you information about the interactions and structure within the space as a whole. Is this correct?",,"['calculus', 'soft-question', 'terminology', 'algebras']"
2,The proof of $e^x \leq x + e^{x^2}$ [duplicate],The proof of  [duplicate],e^x \leq x + e^{x^2},This question already has answers here : proof of inequality $e^x\le x+e^{x^2}$ (4 answers) Closed 8 years ago . How can we prove the inequality $e^x \le x + e^{x^2}$ for $x\in\mathbb{R}$?,This question already has answers here : proof of inequality $e^x\le x+e^{x^2}$ (4 answers) Closed 8 years ago . How can we prove the inequality $e^x \le x + e^{x^2}$ for $x\in\mathbb{R}$?,,"['calculus', 'inequality', 'exponential-function']"
3,Non-divergence form of a 2nd order PDE,Non-divergence form of a 2nd order PDE,,"This might be a trivial question but I'm very rusty with regards to calculus and am new to PDEs. How would you write the following second order quasilinear equation in it's non-divergence form: The equation is: $$-\nabla\cdot\big(a(u,\nabla u)\big)+c(u,\nabla u) = g.$$ Based on the definition of a quasilinear second order pde defined in Lawrence Evans book it should be of the form: $$ - \sum_{i,j =1}^{n}a_{ij}(x,y,\nabla u) \frac{\partial^{2}u }{\partial x_{i}\partial x_{j}} + c(x,u,\nabla u) = g. $$ I think it is an application of the chain rule but I'm having difficulty getting a good form. What steps would you use to get this result? Thanks.","This might be a trivial question but I'm very rusty with regards to calculus and am new to PDEs. How would you write the following second order quasilinear equation in it's non-divergence form: The equation is: $$-\nabla\cdot\big(a(u,\nabla u)\big)+c(u,\nabla u) = g.$$ Based on the definition of a quasilinear second order pde defined in Lawrence Evans book it should be of the form: $$ - \sum_{i,j =1}^{n}a_{ij}(x,y,\nabla u) \frac{\partial^{2}u }{\partial x_{i}\partial x_{j}} + c(x,u,\nabla u) = g. $$ I think it is an application of the chain rule but I'm having difficulty getting a good form. What steps would you use to get this result? Thanks.",,"['calculus', 'functional-analysis', 'multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
4,$\sqrt{2\sqrt{2\sqrt{2\cdots}}}=2$,,\sqrt{2\sqrt{2\sqrt{2\cdots}}}=2,Show that $$\sqrt{2\sqrt{2\sqrt{2\cdots}}}=2$$ $$\sqrt{2}=\mathbf{2}^{1/2}$$ $$\sqrt{2\sqrt{2}}=\mathbf{2}^{1/2+1/2^2}$$ $$\sqrt{2\sqrt{2\sqrt{2}}}=\mathbf{2}^{1/2+1/2^2+1/2^3}$$ Show the limit of $$\mathbf{S}_{n}=\frac{1}{2}+\frac{1}{2^2}+\dotsb+\frac{1}{2^n}=1$$ when $n\to\infty$ $$\textbf{S}_{n}=\frac{1}{2}+\frac{1}{2^2}+...+\frac{1}{2^n}$$ $$\Rightarrow \frac{1}{2}\textbf{S}_{n}=\frac{1}{2}(\frac{1}{2}+\frac{1}{2^2}+...+\frac{1}{2^n})$$ $$\Rightarrow \frac{1}{2}\textbf{S}_{n}=(\frac{1}{2^2}+\frac{1}{2^3}+...+\frac{1}{2^{n+1}})$$ $$\Rightarrow (1)-(2)=\textbf{S}_{n}-\frac{1}{2}\textbf{S}_{n}=\frac{1}{2}-\frac{1}{2^{n+1}}$$ $$\Rightarrow \textbf{S}_{n}(1-\frac{1}{2})=\frac{1}{2}-\frac{1}{2^{n+1}}$$ $$\Rightarrow \frac{1}{2^{n+1}}\rightarrow\textbf{0}\quad\textit{when n}\rightarrow\infty$$ $$\Rightarrow \textbf{S}_{n}\rightarrow\textbf{1}\quad\textit{when n}\rightarrow\infty$$ $$\Rightarrow \lim_{n \to \infty}\textbf{2}^{\textbf{S}_{n}}=2\quad\textit{when n}\rightarrow\infty$$,Show that $$\sqrt{2\sqrt{2\sqrt{2\cdots}}}=2$$ $$\sqrt{2}=\mathbf{2}^{1/2}$$ $$\sqrt{2\sqrt{2}}=\mathbf{2}^{1/2+1/2^2}$$ $$\sqrt{2\sqrt{2\sqrt{2}}}=\mathbf{2}^{1/2+1/2^2+1/2^3}$$ Show the limit of $$\mathbf{S}_{n}=\frac{1}{2}+\frac{1}{2^2}+\dotsb+\frac{1}{2^n}=1$$ when $n\to\infty$ $$\textbf{S}_{n}=\frac{1}{2}+\frac{1}{2^2}+...+\frac{1}{2^n}$$ $$\Rightarrow \frac{1}{2}\textbf{S}_{n}=\frac{1}{2}(\frac{1}{2}+\frac{1}{2^2}+...+\frac{1}{2^n})$$ $$\Rightarrow \frac{1}{2}\textbf{S}_{n}=(\frac{1}{2^2}+\frac{1}{2^3}+...+\frac{1}{2^{n+1}})$$ $$\Rightarrow (1)-(2)=\textbf{S}_{n}-\frac{1}{2}\textbf{S}_{n}=\frac{1}{2}-\frac{1}{2^{n+1}}$$ $$\Rightarrow \textbf{S}_{n}(1-\frac{1}{2})=\frac{1}{2}-\frac{1}{2^{n+1}}$$ $$\Rightarrow \frac{1}{2^{n+1}}\rightarrow\textbf{0}\quad\textit{when n}\rightarrow\infty$$ $$\Rightarrow \textbf{S}_{n}\rightarrow\textbf{1}\quad\textit{when n}\rightarrow\infty$$ $$\Rightarrow \lim_{n \to \infty}\textbf{2}^{\textbf{S}_{n}}=2\quad\textit{when n}\rightarrow\infty$$,,['calculus']
5,General formula needed for this product rule expression (differential operator),General formula needed for this product rule expression (differential operator),,"Let $D_i^t$, $D_i^0$ for $i=1,\dots,n$ be differential operators. (For example $D_1^t = D_x^t$, $D_2^t = D_y^t,\dots$, where $x$, $y$ are the coordinates). Suppose I am given the identity $${D}_a^t (F_t u) = \sum_{j=1}^n F_t({D}_j^0 u){D}_a^t\varphi_j$$ where $\varphi_j$ are smooth functions and $F_t$ is some nice map. So $$ D^t_bD^t_a(F_t u) = \sum_j D^t_b\left(F_t({D}_j^0 u)\right){D}_a^t\varphi_j+\sum_j F_t({D}_j^0 u)D^t_b{D}_a^t\varphi_j $$ and because $${D}_b^t (F_t (D_j^0u)) = \sum_{k=1}^n F_t({D}_k^0 D_j^0u){D}_b^t\varphi_k,$$ we have $$D^t_bD^t_a(F_t u) =  \sum_{j,k=1}^n F_t({D}_k^0 D_j^0u){D}_b^t\varphi_k+\sum_j F_t({D}_j^0 u)D^t_b{D}_a^t\varphi_j .$$ My question is how do I generalise this and obtain a rule for $$D^t_{\alpha} (F_t u)$$ where $\alpha$ is a multiindex of order $n$ (or order $m$)? My intention is to put the derivatives on $u$ and put the $F_t$ outside, like I demonstrated above. Can anyone help me with getting the formula for this? It's really tedious to write out multiple derivatives so it's hard to tell for me.","Let $D_i^t$, $D_i^0$ for $i=1,\dots,n$ be differential operators. (For example $D_1^t = D_x^t$, $D_2^t = D_y^t,\dots$, where $x$, $y$ are the coordinates). Suppose I am given the identity $${D}_a^t (F_t u) = \sum_{j=1}^n F_t({D}_j^0 u){D}_a^t\varphi_j$$ where $\varphi_j$ are smooth functions and $F_t$ is some nice map. So $$ D^t_bD^t_a(F_t u) = \sum_j D^t_b\left(F_t({D}_j^0 u)\right){D}_a^t\varphi_j+\sum_j F_t({D}_j^0 u)D^t_b{D}_a^t\varphi_j $$ and because $${D}_b^t (F_t (D_j^0u)) = \sum_{k=1}^n F_t({D}_k^0 D_j^0u){D}_b^t\varphi_k,$$ we have $$D^t_bD^t_a(F_t u) =  \sum_{j,k=1}^n F_t({D}_k^0 D_j^0u){D}_b^t\varphi_k+\sum_j F_t({D}_j^0 u)D^t_b{D}_a^t\varphi_j .$$ My question is how do I generalise this and obtain a rule for $$D^t_{\alpha} (F_t u)$$ where $\alpha$ is a multiindex of order $n$ (or order $m$)? My intention is to put the derivatives on $u$ and put the $F_t$ outside, like I demonstrated above. Can anyone help me with getting the formula for this? It's really tedious to write out multiple derivatives so it's hard to tell for me.",,"['calculus', 'sobolev-spaces']"
6,Partial derivative of integral: Leibniz rule?,Partial derivative of integral: Leibniz rule?,,"The Leibniz rule is as follows: $$\frac{d}{d\alpha} \int_{a(\alpha)}^{b(\alpha)} f(x, \alpha) dx = \frac{db(\alpha)}{d\alpha} f(b(\alpha), \alpha) - \frac{da(\alpha)}{d\alpha} f(a(\alpha), \alpha) + \int^{b(\alpha)}_{a(\alpha)} \frac{\partial}{\partial\alpha} f(x, \alpha) dx$$ What I would like to know is how to apply the above formula for the case of the partial derivative: $$\frac{\partial}{\partial\alpha} \int_{a(\alpha)}^{b(\beta)} f(x, \alpha) dx.$$","The Leibniz rule is as follows: $$\frac{d}{d\alpha} \int_{a(\alpha)}^{b(\alpha)} f(x, \alpha) dx = \frac{db(\alpha)}{d\alpha} f(b(\alpha), \alpha) - \frac{da(\alpha)}{d\alpha} f(a(\alpha), \alpha) + \int^{b(\alpha)}_{a(\alpha)} \frac{\partial}{\partial\alpha} f(x, \alpha) dx$$ What I would like to know is how to apply the above formula for the case of the partial derivative: $$\frac{\partial}{\partial\alpha} \int_{a(\alpha)}^{b(\beta)} f(x, \alpha) dx.$$",,"['calculus', 'integration', 'multivariable-calculus', 'derivatives', 'leibniz-integral-rule']"
7,Is there a simple formula for this simple question about a circle?,Is there a simple formula for this simple question about a circle?,,"What is the average distance of the points within a circle of radius $r$ from a point a distance $d$ from the centre of the circle (with $d>r$, though a general solution without this constraint would be nice)? The question arose as an operational research simplification of a real problem in telecoms networks and is easy to approximate for any particular case. But several of my colleagues thought that such a simple problem should have a simple formula as the solution, but our combined brains never found one despite titanic effort. It looks like it might involve calculus. I'm interested in both how to approach the problem, but also a final algebraic solution that could be used in a spreadsheet (that is, I don't want to have to integrate anything).","What is the average distance of the points within a circle of radius $r$ from a point a distance $d$ from the centre of the circle (with $d>r$, though a general solution without this constraint would be nice)? The question arose as an operational research simplification of a real problem in telecoms networks and is easy to approximate for any particular case. But several of my colleagues thought that such a simple problem should have a simple formula as the solution, but our combined brains never found one despite titanic effort. It looks like it might involve calculus. I'm interested in both how to approach the problem, but also a final algebraic solution that could be used in a spreadsheet (that is, I don't want to have to integrate anything).",,"['calculus', 'circles']"
8,"Evaluating $\int_{-\infty}^{\infty}\frac{\arctan(\sin^2(x))}{x^2}\,dx$. My answer: $\pi\sqrt{2(\sqrt{2}-1)}$",Evaluating . My answer:,"\int_{-\infty}^{\infty}\frac{\arctan(\sin^2(x))}{x^2}\,dx \pi\sqrt{2(\sqrt{2}-1)}","I think I have evaluated the following integral to be: $$I=\int_{-\infty}^{\infty}\frac{\arctan(\sin^2(x))}{x^2}\,dx=\pi\sqrt{2(\sqrt{2}-1)} $$ I want to know if my answer is correct and if not was my method wrong? Here is what I did. I considered rewriting the integral over the whole real line as a sum of integrals that together span the real line. More specifically I did. $$I= \int_{-\infty}^{\infty}\frac{\arctan(\sin^2(x))}{x^2}\,dx=\sum_{n\in\mathbb{Z}}\int_{(2n-1)\frac{\pi}{2}}^{(2n+1)\frac{\pi}{2}}\frac{\arctan(\sin^2(x))}{x^2}\,dx $$ Then, considering we converge over this interval we make the substitution $x\longrightarrow{x+n\pi}$ Which yields: $$I=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\arctan(\sin^2(x))\sum_{n\in\mathbb{Z}}\frac{1}{(x+n\pi)^2}\,dx $$ And since $$\cot(x)=\sum_{n\in\mathbb{Z}}\frac{1}{x+n\pi}$$ It follows that $$\csc^2(x)=\sum_{n\in\mathbb{Z}}\frac{1}{(x+n\pi)^2}$$ So we finally get that $$I=2\int_{0}^{\frac{\pi}{2}}\frac{\arctan(\sin^2(x))}{\sin^2(x)}\,dx=\pi\sqrt{2(\sqrt{2}-1)} $$ Any thoughts?","I think I have evaluated the following integral to be: I want to know if my answer is correct and if not was my method wrong? Here is what I did. I considered rewriting the integral over the whole real line as a sum of integrals that together span the real line. More specifically I did. Then, considering we converge over this interval we make the substitution Which yields: And since It follows that So we finally get that Any thoughts?","I=\int_{-\infty}^{\infty}\frac{\arctan(\sin^2(x))}{x^2}\,dx=\pi\sqrt{2(\sqrt{2}-1)}
 I= \int_{-\infty}^{\infty}\frac{\arctan(\sin^2(x))}{x^2}\,dx=\sum_{n\in\mathbb{Z}}\int_{(2n-1)\frac{\pi}{2}}^{(2n+1)\frac{\pi}{2}}\frac{\arctan(\sin^2(x))}{x^2}\,dx
 x\longrightarrow{x+n\pi} I=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\arctan(\sin^2(x))\sum_{n\in\mathbb{Z}}\frac{1}{(x+n\pi)^2}\,dx
 \cot(x)=\sum_{n\in\mathbb{Z}}\frac{1}{x+n\pi} \csc^2(x)=\sum_{n\in\mathbb{Z}}\frac{1}{(x+n\pi)^2} I=2\int_{0}^{\frac{\pi}{2}}\frac{\arctan(\sin^2(x))}{\sin^2(x)}\,dx=\pi\sqrt{2(\sqrt{2}-1)}
","['calculus', 'integration', 'definite-integrals']"
9,Is it possible to prove the derivative of $e^x$ is $e^x$ using the limit definition of $e$ without using binomial expansion?,Is it possible to prove the derivative of  is  using the limit definition of  without using binomial expansion?,e^x e^x e,"I am teaching my students about the derivative of $e^x$ . I have walked through what is in our textbook and they have all happily believed me, but I would like to have a better explanation for why the derivative is $e^x$ . I know several other proofs exist like those that define $e$ using the slope at $0$ and those that use the natural log, but I'd like my proof to closely follow the method used in the textbook.  That is why this question and this question have not answered all of my questions. Here is what the textbook says. I understand everything until they decide to let $e^{\Delta x}\approx 1 + \Delta x$ . My question is why do they not directly use the limit definition of $e$ . Is it because they can't without introducing students to binomial expansion? I put $e^{x}$ outside the limit and then tried to substitute the definition of $e$ . But I think I made a mistake. Here is my work: $$e^{ x} \lim_{\Delta x\to\ 0} \frac{(\lim_{\Delta x\to\ 0}( 1+ \Delta x)^{\frac{1}{\Delta x}}) ^{\Delta x}-1}{\Delta x}$$ Using limit properties I know I can rewrite this as: $$e^{x} \lim_{\Delta x\to\ 0} \frac{(\lim_{\Delta x\to\ 0}( 1+ \Delta x))-1}{\Delta x}$$ This is where I get confused/stuck. If I resolve the inner limit first, I get: $$e^{x} \lim_{\Delta x\to\ 0} \frac{1-1}{\Delta x}$$ $$e^{x}\lim_{\Delta x\to\ 0} \frac{0}{\Delta x}$$ $$e^{x} \times 0 = 0$$ Is there a property of limits that would allow me to ""get rid"" of that inner limit?","I am teaching my students about the derivative of . I have walked through what is in our textbook and they have all happily believed me, but I would like to have a better explanation for why the derivative is . I know several other proofs exist like those that define using the slope at and those that use the natural log, but I'd like my proof to closely follow the method used in the textbook.  That is why this question and this question have not answered all of my questions. Here is what the textbook says. I understand everything until they decide to let . My question is why do they not directly use the limit definition of . Is it because they can't without introducing students to binomial expansion? I put outside the limit and then tried to substitute the definition of . But I think I made a mistake. Here is my work: Using limit properties I know I can rewrite this as: This is where I get confused/stuck. If I resolve the inner limit first, I get: Is there a property of limits that would allow me to ""get rid"" of that inner limit?",e^x e^x e 0 e^{\Delta x}\approx 1 + \Delta x e e^{x} e e^{ x} \lim_{\Delta x\to\ 0} \frac{(\lim_{\Delta x\to\ 0}( 1+ \Delta x)^{\frac{1}{\Delta x}}) ^{\Delta x}-1}{\Delta x} e^{x} \lim_{\Delta x\to\ 0} \frac{(\lim_{\Delta x\to\ 0}( 1+ \Delta x))-1}{\Delta x} e^{x} \lim_{\Delta x\to\ 0} \frac{1-1}{\Delta x} e^{x}\lim_{\Delta x\to\ 0} \frac{0}{\Delta x} e^{x} \times 0 = 0,"['calculus', 'derivatives', 'proof-explanation', 'exponential-function']"
10,Is this a rigorous derivation? What justifies this step? $\lim_{h\to 0} \frac{e^h -1}{h}$,Is this a rigorous derivation? What justifies this step?,\lim_{h\to 0} \frac{e^h -1}{h},"I'm trying to find the easiest way to prove that $\lim_{h\to 0} \frac{e^h-1}{h}=1$ . I found the following derivation, which seems the most straightforward: $$\lim_{h\to 0} \frac{e^h-1}{h} \overbrace{=}^* \lim_{h\to 0} \frac{((1+h)^\frac{1}{h})^h-1}{h}=\lim_{h\to 0}\frac{1+h-1}{h}=1$$ The step I'm curious about is the first one. I realize that $e=\lim_{h\to 0} (1+h)^\frac{1}{h}$ , but what is it that fully justifies substituting this limit inside the outer limit? If I remember correctly sometimes making such substitutions inside limits can lead to false results? Naively I would say that all I can say for sure is that $$\lim_{h\to 0} \frac{e^h-1}{h}=\lim_{h\to 0} \frac{(\lim_{k\to 0} (1+k)^\frac{1}{k})^h-1}{h}$$ Am I missing something trivial? [Added: I define $e = \lim_{n\to\infty} (1+\frac{1}{n})^n$ and then $e^x$ is defined per the usual way of exponentiation of real numbers. Therefore the above limit is a stepping stone in proving that $(e^x)'= e^x$ and so we can't use this derivative (or its consequences, such as the Taylor series of $e^x$ ) in order to prove this limit, so that we avoid a cyclic argument.]","I'm trying to find the easiest way to prove that . I found the following derivation, which seems the most straightforward: The step I'm curious about is the first one. I realize that , but what is it that fully justifies substituting this limit inside the outer limit? If I remember correctly sometimes making such substitutions inside limits can lead to false results? Naively I would say that all I can say for sure is that Am I missing something trivial? [Added: I define and then is defined per the usual way of exponentiation of real numbers. Therefore the above limit is a stepping stone in proving that and so we can't use this derivative (or its consequences, such as the Taylor series of ) in order to prove this limit, so that we avoid a cyclic argument.]",\lim_{h\to 0} \frac{e^h-1}{h}=1 \lim_{h\to 0} \frac{e^h-1}{h} \overbrace{=}^* \lim_{h\to 0} \frac{((1+h)^\frac{1}{h})^h-1}{h}=\lim_{h\to 0}\frac{1+h-1}{h}=1 e=\lim_{h\to 0} (1+h)^\frac{1}{h} \lim_{h\to 0} \frac{e^h-1}{h}=\lim_{h\to 0} \frac{(\lim_{k\to 0} (1+k)^\frac{1}{k})^h-1}{h} e = \lim_{n\to\infty} (1+\frac{1}{n})^n e^x (e^x)'= e^x e^x,"['calculus', 'limits', 'substitution']"
11,Proving $\lim\limits_{x\to0}{\frac{\sin(\frac{1}{x})}{\sin(\frac{1}{x})}}=1$,Proving,\lim\limits_{x\to0}{\frac{\sin(\frac{1}{x})}{\sin(\frac{1}{x})}}=1,"I am relatively new to calculus and I'm trying to understand it rigorously. For this question, assume that I only consider the functions to have purely real domains and ranges. I have seen that $\lim\limits_{x\to0}{\frac{\sin(\frac{1}{x})}{\sin(\frac{1}{x})}}=1$ is considered true. This is what my calculator and an online limit solver gave, so unless they are wrong, there must be a flaw in my reasoning. I do not understand it or know how to prove it using epsilon and delta. Here's what I've thought so far: For any function f, $\frac{f(x)} {f(x)}=1$ if f is nonzero and defined at x. If these conditions are met when $|x-c|\in(0, \delta)$ , the quotient is one, so any epsilon satisfies the condition. This means that to prove $\lim\limits_{x\to c}{\frac{f(x)}{f(x)}}=1$ , you just need to show that f is nonzero and defined when $|x-c|\in(0, \delta)$ . I don't believe this can be shown for $f(x)=\sin(\frac{1}{x})$ , since it has infinitely many x-intercepts within any $\delta$ . Have I made a mistake anywhere or failed to consider something? How would this be normally demonstrated?","I am relatively new to calculus and I'm trying to understand it rigorously. For this question, assume that I only consider the functions to have purely real domains and ranges. I have seen that is considered true. This is what my calculator and an online limit solver gave, so unless they are wrong, there must be a flaw in my reasoning. I do not understand it or know how to prove it using epsilon and delta. Here's what I've thought so far: For any function f, if f is nonzero and defined at x. If these conditions are met when , the quotient is one, so any epsilon satisfies the condition. This means that to prove , you just need to show that f is nonzero and defined when . I don't believe this can be shown for , since it has infinitely many x-intercepts within any . Have I made a mistake anywhere or failed to consider something? How would this be normally demonstrated?","\lim\limits_{x\to0}{\frac{\sin(\frac{1}{x})}{\sin(\frac{1}{x})}}=1 \frac{f(x)} {f(x)}=1 |x-c|\in(0, \delta) \lim\limits_{x\to c}{\frac{f(x)}{f(x)}}=1 |x-c|\in(0, \delta) f(x)=\sin(\frac{1}{x}) \delta","['calculus', 'limits', 'epsilon-delta']"
12,Want help with proving a calculus theory,Want help with proving a calculus theory,,"Let $f(x)$ have a second derivative on the closed interval $[-2,2]$ . If $\left| f(x) \right| \le 1$ and $\frac{1}{2}  (f^{\prime}(0))^2+f(0)^3>\frac{3}{2}  $ when $-2\le x\le2$ , now I need to prove that there must be a point $x_{0}$ on the interval $(-2,2)$ such that $f^{\prime \prime}\left(x_{0}\right)+3f\left(x_{0}\right)^2=0$ . (Series[1/2 (f'[x])^2 + f[x]^3, {x, 0, 1}]) // FullSimplify The above method does not reveal the nature of the problem and solve it cleverly. I want to use a more generic and heuristic method to verify the conclusion of this abstract function problem. What can I do to solve this problem? The source of this problem (张宇高等数学18讲):","Let have a second derivative on the closed interval . If and when , now I need to prove that there must be a point on the interval such that . (Series[1/2 (f'[x])^2 + f[x]^3, {x, 0, 1}]) // FullSimplify The above method does not reveal the nature of the problem and solve it cleverly. I want to use a more generic and heuristic method to verify the conclusion of this abstract function problem. What can I do to solve this problem? The source of this problem (张宇高等数学18讲):","f(x) [-2,2] \left| f(x) \right| \le 1 \frac{1}{2}  (f^{\prime}(0))^2+f(0)^3>\frac{3}{2}   -2\le x\le2 x_{0} (-2,2) f^{\prime \prime}\left(x_{0}\right)+3f\left(x_{0}\right)^2=0","['calculus', 'derivatives']"
13,Purely geometric proof of inverse trigonometric functions derivatives,Purely geometric proof of inverse trigonometric functions derivatives,,"Can you compute the derivatives of $\sin^{-1}(x),\cos^{-1}(x),$ and $\tan^{-1}(x)$ using only geometry? I know how to use geometry to find the derivatives of $\sin x$ and $\cos x$ like this: We can use the fact that we know the tangent of the circle to show that $\frac{d}{dx}\cos(x)=\sin(x)$ . Wondering if you can do the same with the inverse functions.",Can you compute the derivatives of and using only geometry? I know how to use geometry to find the derivatives of and like this: We can use the fact that we know the tangent of the circle to show that . Wondering if you can do the same with the inverse functions.,"\sin^{-1}(x),\cos^{-1}(x), \tan^{-1}(x) \sin x \cos x \frac{d}{dx}\cos(x)=\sin(x)","['calculus', 'geometry']"
14,Solve differential equation $f''''(x)=f'''(x)f''(x)f'(x)f(x)$,Solve differential equation,f''''(x)=f'''(x)f''(x)f'(x)f(x),"I met this DE recently, and I am utterly befuddled at how to solve it $$f''''(x)=f'''(x)f''(x)f'(x)f(x)$$ I tried this: $$\frac{f''''(x)}{f'''(x)}=f''(x)f'(x)f(x)$$ $$\ln|f'''(x)|=c_1+\int f(x)f'(x)f''(x)dx$$ I do not know how to solve the right side, though. Integration by parts? Plaese help.","I met this DE recently, and I am utterly befuddled at how to solve it I tried this: I do not know how to solve the right side, though. Integration by parts? Plaese help.",f''''(x)=f'''(x)f''(x)f'(x)f(x) \frac{f''''(x)}{f'''(x)}=f''(x)f'(x)f(x) \ln|f'''(x)|=c_1+\int f(x)f'(x)f''(x)dx,"['calculus', 'integration', 'ordinary-differential-equations']"
15,"Evaluating $\int_0^\pi \sqrt{\frac{\sin x}{\pi-x}} \, dx$",Evaluating,"\int_0^\pi \sqrt{\frac{\sin x}{\pi-x}} \, dx","$$ \int_0^\pi \sqrt{\frac{\sin x}{\pi-x}} \, dx $$ I have stumbled upon this integral and have no clue how to solve it.  I know the answer is around $2.2778$. I tried some expansions and some approximations without success. Can you guys figure it out?","$$ \int_0^\pi \sqrt{\frac{\sin x}{\pi-x}} \, dx $$ I have stumbled upon this integral and have no clue how to solve it.  I know the answer is around $2.2778$. I tried some expansions and some approximations without success. Can you guys figure it out?",,"['calculus', 'definite-integrals']"
16,How to prove that $K =\lim \limits_{n \to \infty}\left( \prod \limits_{k=1}^{n}a_k\right)^{1/n}\approx2.6854520010$?,How to prove that ?,K =\lim \limits_{n \to \infty}\left( \prod \limits_{k=1}^{n}a_k\right)^{1/n}\approx2.6854520010,"I was going through a list of important Mathematical Constants, when I saw the Khinchin's constant . It said that : If a real number $r$ is written as a simple continued fraction : $$r=a_0+\dfrac{1}{a_1+\dfrac{1}{a_2+\dfrac{1}{a_3+\dots}}}$$ , where $a_k$ are natural numbers $\forall \,\,k$ , then $\lim \limits_{n \to \infty} GM(a_1,a_2,\dots,a_n )= \left(\lim \limits_{n \to \infty} \prod \limits_{k=1}^{n}a_k\right)^{1/n}$ exists and is a constant $K \approx  2.6854520010$ , except for a set of measure $0$ . First obvious question is that why the value $a_0$ is not included in the Geometric Mean? I tried playing around with terms and juggling them but was unable to compute the limit. Also, is it necessary for $r$ to be ""written-able"" in the form of a continued fraction ? Thanks in Advance ! :-)","I was going through a list of important Mathematical Constants, when I saw the Khinchin's constant . It said that : If a real number is written as a simple continued fraction : , where are natural numbers , then exists and is a constant , except for a set of measure . First obvious question is that why the value is not included in the Geometric Mean? I tried playing around with terms and juggling them but was unable to compute the limit. Also, is it necessary for to be ""written-able"" in the form of a continued fraction ? Thanks in Advance ! :-)","r r=a_0+\dfrac{1}{a_1+\dfrac{1}{a_2+\dfrac{1}{a_3+\dots}}} a_k \forall \,\,k \lim \limits_{n \to \infty} GM(a_1,a_2,\dots,a_n )= \left(\lim \limits_{n \to \infty} \prod \limits_{k=1}^{n}a_k\right)^{1/n} K \approx  2.6854520010 0 a_0 r","['calculus', 'limits']"
17,matrix derivative of gradients,matrix derivative of gradients,,"UPDATE based on request below by KeD: Hi I am trying to take the functional derivative of the following integral I wrt  $X^H$; $\delta I/\delta X^H=0$, where $X$ is a complex square matrix and $X^H, \bar{X}$ are the hermitian transpose and complex conjugate of $X$ respectively.  The integral is given by$$ I = \int  \nabla_k X_{\alpha j} \nabla_k \bar{X}_{\alpha j} $$ where $c_1,c_2,c_3$ are real constants. I want to take the functional derivative $$ \frac{\delta I}{\delta X^H}= \frac{\delta I}{\delta X_{\alpha i}^H}=?   $$ however I don't know how to vary the gradient term.  The indices get confusing when varying the gradient terms. I am looking for a complete rigorous solution, thanks  a lot for your help! My approach so far is : $$ \delta I= \delta \int \nabla_k X_{\alpha j} \nabla_k \bar{X}_{\alpha j}=\int \delta \left( \nabla_k X_{\alpha j} \nabla_k \bar{X}_{\alpha j}\right) $$  $$ \delta I =\int \left(\nabla_k \delta X_{\alpha j} \nabla_k \bar{X}_{\alpha j}+\nabla_k X_{\alpha j} \nabla_k \delta (\bar{X}_{\alpha j})\right) $$ but is $\delta X_{\alpha j}=0$?? since I am varying wrt $X^H$, if so I have  $$ \delta I = \int\nabla_k X_{\alpha j} \nabla_k \delta (\bar{X}_{\alpha j})=-\int \nabla_k \nabla_k X_{\alpha j} \delta(\bar{X}_{\alpha j}) $$  where I integrated by parts on the last line to get the derivative off the $\delta$.  Thus I  get $$ \frac{\delta I}{\delta X^H}=- \nabla_k \nabla_k A_{\alpha j} $$ Is this correct?  THanks a lot!","UPDATE based on request below by KeD: Hi I am trying to take the functional derivative of the following integral I wrt  $X^H$; $\delta I/\delta X^H=0$, where $X$ is a complex square matrix and $X^H, \bar{X}$ are the hermitian transpose and complex conjugate of $X$ respectively.  The integral is given by$$ I = \int  \nabla_k X_{\alpha j} \nabla_k \bar{X}_{\alpha j} $$ where $c_1,c_2,c_3$ are real constants. I want to take the functional derivative $$ \frac{\delta I}{\delta X^H}= \frac{\delta I}{\delta X_{\alpha i}^H}=?   $$ however I don't know how to vary the gradient term.  The indices get confusing when varying the gradient terms. I am looking for a complete rigorous solution, thanks  a lot for your help! My approach so far is : $$ \delta I= \delta \int \nabla_k X_{\alpha j} \nabla_k \bar{X}_{\alpha j}=\int \delta \left( \nabla_k X_{\alpha j} \nabla_k \bar{X}_{\alpha j}\right) $$  $$ \delta I =\int \left(\nabla_k \delta X_{\alpha j} \nabla_k \bar{X}_{\alpha j}+\nabla_k X_{\alpha j} \nabla_k \delta (\bar{X}_{\alpha j})\right) $$ but is $\delta X_{\alpha j}=0$?? since I am varying wrt $X^H$, if so I have  $$ \delta I = \int\nabla_k X_{\alpha j} \nabla_k \delta (\bar{X}_{\alpha j})=-\int \nabla_k \nabla_k X_{\alpha j} \delta(\bar{X}_{\alpha j}) $$  where I integrated by parts on the last line to get the derivative off the $\delta$.  Thus I  get $$ \frac{\delta I}{\delta X^H}=- \nabla_k \nabla_k A_{\alpha j} $$ Is this correct?  THanks a lot!",,"['calculus', 'matrices', 'functional-analysis', 'derivatives', 'matrix-calculus']"
18,Details of Spivak's Proof of Stokes' Theorem,Details of Spivak's Proof of Stokes' Theorem,,"In Spivak's Calculus on Manifolds, the proof of Stokes Theorem on $\mathbb{R}^n$ begins as follows... It seems to me that there's something here which can be very confusing: When you pull back the $k-1$ form $f dx^1 \wedge ... \wedge \widehat{dx^i} \wedge ... \wedge dx^k$ along ${I^k}_{(i,\alpha)}$, the result is again a $k-1$ form, which should be integrated over a $(k-1)$-cube. However, in the line below, the integral is over $[0,1]^k$. It amounts to the same  thing, since ${I^k}_{(i,\alpha)}^*(f dx^1 \wedge ... \wedge \widehat{dx^i} \wedge ... \wedge dx^k) = f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1}$ and then $$ \begin{aligned}& \int_{[0,1]^{k-1}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1} \\ = & \int_{[0,1]}\left(\int_{[0,1]^{k-1}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1}\right)dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^{i}, ..., x^{k-1})\,dx^1 ... dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^{i+1}, ..., x^{k})\,dx^1 ... dx^k\end{aligned}$$ where the second line follows since the pulled back form is constant with respect to $x^k$ and the last line follows since we're working with the Riemann integral over $[0,1]^k$ so we're really just renaming variables. I think it's a bit of a stretch to ask the reader to 'note' that without any further indication as to why it's true. Spivak pulls a similar trick later on in the proof, which I noticed another StackExchange question on . After having run through the steps of the proof on a small example, I'm guessing that the reason for doing this is to avoid having to talk about renaming variables. So, my two questions are: Is there a simpler way to make sense of the 'note' which I addressed above? Am I correct in thinking that the extra integration is done to make the proof more concise and avoid discussion of renaming variables? Or is there some other reason I'm missing?","In Spivak's Calculus on Manifolds, the proof of Stokes Theorem on $\mathbb{R}^n$ begins as follows... It seems to me that there's something here which can be very confusing: When you pull back the $k-1$ form $f dx^1 \wedge ... \wedge \widehat{dx^i} \wedge ... \wedge dx^k$ along ${I^k}_{(i,\alpha)}$, the result is again a $k-1$ form, which should be integrated over a $(k-1)$-cube. However, in the line below, the integral is over $[0,1]^k$. It amounts to the same  thing, since ${I^k}_{(i,\alpha)}^*(f dx^1 \wedge ... \wedge \widehat{dx^i} \wedge ... \wedge dx^k) = f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1}$ and then $$ \begin{aligned}& \int_{[0,1]^{k-1}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1} \\ = & \int_{[0,1]}\left(\int_{[0,1]^{k-1}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1}\right)dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^{i}, ..., x^{k-1})\,dx^1 ... dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^{i+1}, ..., x^{k})\,dx^1 ... dx^k\end{aligned}$$ where the second line follows since the pulled back form is constant with respect to $x^k$ and the last line follows since we're working with the Riemann integral over $[0,1]^k$ so we're really just renaming variables. I think it's a bit of a stretch to ask the reader to 'note' that without any further indication as to why it's true. Spivak pulls a similar trick later on in the proof, which I noticed another StackExchange question on . After having run through the steps of the proof on a small example, I'm guessing that the reason for doing this is to avoid having to talk about renaming variables. So, my two questions are: Is there a simpler way to make sense of the 'note' which I addressed above? Am I correct in thinking that the extra integration is done to make the proof more concise and avoid discussion of renaming variables? Or is there some other reason I'm missing?",,"['calculus', 'vector-analysis', 'differential-forms', 'stokes-theorem']"
19,Which way should you run from the lions?,Which way should you run from the lions?,,"This is a fun problem that I saw somewhere on the internet a long time ago: Suppose you are at the center of an equilateral triangle with side length $s$. At each of its vertices, there is a lion which is determined to eat you. The lions start at a constant speed of $v_l$, and they are always running directly towards your current location. You start in the center, and can run at a constant speed $v_h$ (assume instantaneous acceleration for all parties). You are NOT enclosed in the triangle, you are free to try to run wherever you want. Which patch should you take in order to survive the longest time possible? How long can you survive? At first, because everybody's speed is constant, I thought we can just work with functions of $x$ for the paths of the lions and the human, and try to maximize the arc length of the lions' paths. However, I think it's much easier to work with the functions in parametric form, because the tangent lines to the lions' path at $t=t_0$ should go trough your position at $t_0$. Also, I think it's reasonable to assume that at $t=0$ your direction is straight towards the midpoint of one of the sides, because any other direction would cause you to meet one of the lions faster.","This is a fun problem that I saw somewhere on the internet a long time ago: Suppose you are at the center of an equilateral triangle with side length $s$. At each of its vertices, there is a lion which is determined to eat you. The lions start at a constant speed of $v_l$, and they are always running directly towards your current location. You start in the center, and can run at a constant speed $v_h$ (assume instantaneous acceleration for all parties). You are NOT enclosed in the triangle, you are free to try to run wherever you want. Which patch should you take in order to survive the longest time possible? How long can you survive? At first, because everybody's speed is constant, I thought we can just work with functions of $x$ for the paths of the lions and the human, and try to maximize the arc length of the lions' paths. However, I think it's much easier to work with the functions in parametric form, because the tangent lines to the lions' path at $t=t_0$ should go trough your position at $t_0$. Also, I think it's reasonable to assume that at $t=0$ your direction is straight towards the midpoint of one of the sides, because any other direction would cause you to meet one of the lions faster.",,"['calculus', 'optimization', 'parametric', 'parametrization', 'differential-games']"
20,Minimize the Area,Minimize the Area,,"The lower corner of a page is to folded to reach the opposite inner edge.  We have to find the width of the folded part if the Area of the folded part is minimum. Now how I proceeded: Let the width of the page be $1$. Let the folded part be $x$.  And the angle of the part folded be $\theta$. Now I found a relation between $x$ and $\theta$ and thus wrote the area entirely as a function  of tan($\theta$) and minimized it. What I want to know is of  other different  ways to do this. Here's a sketch : Just in case someone  wants to check their answers,  the width x comes out to be 2/3.","The lower corner of a page is to folded to reach the opposite inner edge.  We have to find the width of the folded part if the Area of the folded part is minimum. Now how I proceeded: Let the width of the page be $1$. Let the folded part be $x$.  And the angle of the part folded be $\theta$. Now I found a relation between $x$ and $\theta$ and thus wrote the area entirely as a function  of tan($\theta$) and minimized it. What I want to know is of  other different  ways to do this. Here's a sketch : Just in case someone  wants to check their answers,  the width x comes out to be 2/3.",,"['calculus', 'algebra-precalculus', 'derivatives']"
21,Is this epsilon-delta proof that $\sin(x)$ is continuous circular?,Is this epsilon-delta proof that  is continuous circular?,\sin(x),"Prove that $\lim_{x\to a}\sin x=\sin a$, where $a$ is any real number. Solution 13 here: https://www.math.ucdavis.edu/~kouba/CalcOneDIRECTORY/preclimsoldirectory/PrecLimSol.html#SOLUTION13 claims to prove that $\sin(x)$ is continous, in other words: for every real $a$, $\lim_{x\to a}\sin(x)=\sin(a)$. The solution uses the Mean Value Theorem, which only works if the function in question is differentiable on some interval; in particular the function must be continous on some interval. So the solution is assuming that $\sin(x)$ is continous in order to prove that it is continous. Is this solution bogus or am I missing something? Note: I am not asking you to provide a correct proof. I already know of a correct proof (which does not use MVT).","Prove that $\lim_{x\to a}\sin x=\sin a$, where $a$ is any real number. Solution 13 here: https://www.math.ucdavis.edu/~kouba/CalcOneDIRECTORY/preclimsoldirectory/PrecLimSol.html#SOLUTION13 claims to prove that $\sin(x)$ is continous, in other words: for every real $a$, $\lim_{x\to a}\sin(x)=\sin(a)$. The solution uses the Mean Value Theorem, which only works if the function in question is differentiable on some interval; in particular the function must be continous on some interval. So the solution is assuming that $\sin(x)$ is continous in order to prove that it is continous. Is this solution bogus or am I missing something? Note: I am not asking you to provide a correct proof. I already know of a correct proof (which does not use MVT).",,"['calculus', 'epsilon-delta']"
22,Sum of Bell Polynomials of the Second Kind,Sum of Bell Polynomials of the Second Kind,,"A problem of interest that has come up for me recently is solving the following. $$\frac{d^{n}}{dt^{n}}e^{g(t)}$$ There is a formula for a general $n$ -th order derivative of a composition as shown above: http://mathworld.wolfram.com/FaadiBrunosFormula.html In terms of the Bell Polynomials, we can write $$\frac{d^{n}}{dt^{n}}e^{g(t)}=e^{g(t)}\sum_{k=0}^{n}B_{n,k}(g'(t),g''(t),\cdots)$$ And the Bell polynomials of the second kind are shown in the Wolfram link above. I am wondering if there is a closed-form solution for the sum of the series of Bell Polynomials.","A problem of interest that has come up for me recently is solving the following. There is a formula for a general -th order derivative of a composition as shown above: http://mathworld.wolfram.com/FaadiBrunosFormula.html In terms of the Bell Polynomials, we can write And the Bell polynomials of the second kind are shown in the Wolfram link above. I am wondering if there is a closed-form solution for the sum of the series of Bell Polynomials.","\frac{d^{n}}{dt^{n}}e^{g(t)} n \frac{d^{n}}{dt^{n}}e^{g(t)}=e^{g(t)}\sum_{k=0}^{n}B_{n,k}(g'(t),g''(t),\cdots)","['calculus', 'sequences-and-series', 'polynomials', 'exponentiation', 'bell-numbers']"
23,Calculating the $k$th digit of $\pi$,Calculating the th digit of,k \pi,"I'm new to math.stackexchange so apologies in advance for any blunders: I am trying to calculate $\pi$ using the following technique here . Considering the above link says: The discovery of this formula came as a surprise. For centuries it had been assumed that there was no way to compute the $n$th digit of $\pi$ without calculating all of the preceding $n − 1$ digits. I won't pretend I understand the calculation and may have misunderstood, but from what I can gather, this formula should identify the $k$th  digit of $\pi$ independently (ie without having to calculate the previous $k-1$ digits): $$ \sum_{k=0}^\infty \left[ { 1 \over 16^k} \left( {120k^2 + 151k + 47 \over 512k^4 + 1024k^3 + 712k^2 + 194k + 15 } \right) \right]$$ The values I'm getting out are: k  result 0  3.133333 1  0.0080891331 2  0.0001649239 ....I realise that summing these gives me the correct digits of $\pi$ to an accuracy of $k$, however by calculating each of these in turn it seem that we are still ""calculating all of the preceding n-1 digits""?","I'm new to math.stackexchange so apologies in advance for any blunders: I am trying to calculate $\pi$ using the following technique here . Considering the above link says: The discovery of this formula came as a surprise. For centuries it had been assumed that there was no way to compute the $n$th digit of $\pi$ without calculating all of the preceding $n − 1$ digits. I won't pretend I understand the calculation and may have misunderstood, but from what I can gather, this formula should identify the $k$th  digit of $\pi$ independently (ie without having to calculate the previous $k-1$ digits): $$ \sum_{k=0}^\infty \left[ { 1 \over 16^k} \left( {120k^2 + 151k + 47 \over 512k^4 + 1024k^3 + 712k^2 + 194k + 15 } \right) \right]$$ The values I'm getting out are: k  result 0  3.133333 1  0.0080891331 2  0.0001649239 ....I realise that summing these gives me the correct digits of $\pi$ to an accuracy of $k$, however by calculating each of these in turn it seem that we are still ""calculating all of the preceding n-1 digits""?",,['calculus']
24,If $\lim_{n\to\infty} (a_{n+1}-a_n)=0$ and $|a_{n+2}-a_n|<\frac{1}{2^n}$ then $(a_n)$ converges,If  and  then  converges,\lim_{n\to\infty} (a_{n+1}-a_n)=0 |a_{n+2}-a_n|<\frac{1}{2^n} (a_n),"Let $(a_n)$ be a sequence such that $\lim_{n\to\infty} (a_{n+1}-a_n)=0$ and $|a_{n+2}-a_n|<\frac{1}{2^n}$ for all $n$ . I have to decide whether or not $(a_n)$ converges. My attempt: I think it converges. Let $b_n=a_{2n}, c_n=a_{2n-1}$ . Then: $$|b_{n+1}-b_n|=|a_{2n+2}-a_{2n}|<\frac{1}{2^{2n}}$$ $$|c_{n+1}-c_n|=|a_{2n+1}-a_{2n-1}|<\frac{1}{2^{2n-1}}$$ Thus $(b_n)$ and $(c_n)$ are Cauchy (proven in another question ) and converge. Because $(a_{2n}-a_{2n-1})$ is a subsequence of $(a_{n+1}-a_n)$ it also converges to $0$ . Thus $$\lim_{n\to\infty} (b_n-c_n)=\lim_{n\to\infty} (a_{2n}-a_{2n-1})=0$$ or $$\lim_{n\to\infty} b_n=\lim_{n\to\infty}c_n$$ Because the subsequences $(b_n)$ and $(c_n)$ cover the sequence $(a_n)$ and because they converge to the same point, $(a_n)$ converges. Is it correct? What do you think?","Let be a sequence such that and for all . I have to decide whether or not converges. My attempt: I think it converges. Let . Then: Thus and are Cauchy (proven in another question ) and converge. Because is a subsequence of it also converges to . Thus or Because the subsequences and cover the sequence and because they converge to the same point, converges. Is it correct? What do you think?","(a_n) \lim_{n\to\infty} (a_{n+1}-a_n)=0 |a_{n+2}-a_n|<\frac{1}{2^n} n (a_n) b_n=a_{2n}, c_n=a_{2n-1} |b_{n+1}-b_n|=|a_{2n+2}-a_{2n}|<\frac{1}{2^{2n}} |c_{n+1}-c_n|=|a_{2n+1}-a_{2n-1}|<\frac{1}{2^{2n-1}} (b_n) (c_n) (a_{2n}-a_{2n-1}) (a_{n+1}-a_n) 0 \lim_{n\to\infty} (b_n-c_n)=\lim_{n\to\infty} (a_{2n}-a_{2n-1})=0 \lim_{n\to\infty} b_n=\lim_{n\to\infty}c_n (b_n) (c_n) (a_n) (a_n)","['calculus', 'sequences-and-series', 'proof-verification']"
25,"How to integrate $\int \sec ^m(x) \tan ^n(x) \, dx$",How to integrate,"\int \sec ^m(x) \tan ^n(x) \, dx","$$\sec ^2(x)=\tan ^2(x)+1$$ $$\csc ^2(x)=\cot ^2(x)+1$$ We can evaluate integrals of the form: $$\int \sec ^m(x) \tan ^n(x) \, dx$$ $$\int \csc ^m(x) \cot ^n(x) \, dx$$ with substitution unless $m$ is odd and $n$ is even. What I am interested to know is why am I not able to solve this with substitution if $m$ is odd and $n$ is even. I am aware that I can solve it by integration by parts. But I do not see the underlying reason for why it is not possible when using substitution?","$$\sec ^2(x)=\tan ^2(x)+1$$ $$\csc ^2(x)=\cot ^2(x)+1$$ We can evaluate integrals of the form: $$\int \sec ^m(x) \tan ^n(x) \, dx$$ $$\int \csc ^m(x) \cot ^n(x) \, dx$$ with substitution unless $m$ is odd and $n$ is even. What I am interested to know is why am I not able to solve this with substitution if $m$ is odd and $n$ is even. I am aware that I can solve it by integration by parts. But I do not see the underlying reason for why it is not possible when using substitution?",,"['calculus', 'integration', 'indefinite-integrals', 'trigonometric-integrals']"
26,Function composition: $f^{653}(56)=?$,Function composition:,f^{653}(56)=?,"Let $f(x) = \frac1{(1-x)}$. Define the function $f^r$ to be $f^r(x) = f(f(f(...f(f(x)))))$. Find $f^{653}(56)$. What I've done: I started with r=1,2,3 and noticed the following pattern: $$f^r(x)= \left\{  \begin{array}{c} \frac1{1-x}, when \  r\equiv 1\pmod 3 \\  \frac{x-1}x, when \  r\equiv 2\pmod 3 \\ x, \ when \  r\equiv 0\pmod 3 \end{array} \right.  $$ As  $653\equiv 2\pmod 3$, $\\$ $f^{653}(56) = \frac{55}{56}$ BUT how can I prove that I'm right? By induction? I don't know what to do then, when I go from $r$ to $r+1$. Could you please share with me your reasoning by solving this problem? PS: This problem is from the book ""How to think like a mathematician"" by Kevin Houston.","Let $f(x) = \frac1{(1-x)}$. Define the function $f^r$ to be $f^r(x) = f(f(f(...f(f(x)))))$. Find $f^{653}(56)$. What I've done: I started with r=1,2,3 and noticed the following pattern: $$f^r(x)= \left\{  \begin{array}{c} \frac1{1-x}, when \  r\equiv 1\pmod 3 \\  \frac{x-1}x, when \  r\equiv 2\pmod 3 \\ x, \ when \  r\equiv 0\pmod 3 \end{array} \right.  $$ As  $653\equiv 2\pmod 3$, $\\$ $f^{653}(56) = \frac{55}{56}$ BUT how can I prove that I'm right? By induction? I don't know what to do then, when I go from $r$ to $r+1$. Could you please share with me your reasoning by solving this problem? PS: This problem is from the book ""How to think like a mathematician"" by Kevin Houston.",,"['calculus', 'algebra-precalculus', 'functions', 'induction']"
27,$x''= \frac{Ax+B}{Cx+D}$,,x''= \frac{Ax+B}{Cx+D},"Might there be a closed-form solution to the second-order differential equation below?$$x''(t)=\frac{Ax+B}{Cx+D}$$ If not, is there any way to get a power series approximation in terms of the variables , without having to use any initial conditions? Or absolutely anything somewhat close to the answer? (If it helps we have $x(0)=x'(0)=0$ but the constants $A,B,C,D$ are unknown and must be left as variables.)","Might there be a closed-form solution to the second-order differential equation below?$$x''(t)=\frac{Ax+B}{Cx+D}$$ If not, is there any way to get a power series approximation in terms of the variables , without having to use any initial conditions? Or absolutely anything somewhat close to the answer? (If it helps we have $x(0)=x'(0)=0$ but the constants $A,B,C,D$ are unknown and must be left as variables.)",,['calculus']
28,Solve $(\varepsilon-x)y=y'(-x+y^2-2x^2)$,Solve,(\varepsilon-x)y=y'(-x+y^2-2x^2),"I need some help, I have this ODE but can't solve it for $y(x)$ , I try every method I know, but with no success,please, somebody can help me? $$(\varepsilon-x)y=y'(-x+y^2-2x^2)$$ Thanks.","I need some help, I have this ODE but can't solve it for , I try every method I know, but with no success,please, somebody can help me? Thanks.",y(x) (\varepsilon-x)y=y'(-x+y^2-2x^2),"['calculus', 'ordinary-differential-equations']"
29,Differentiating $y=x^x$ with the formal definition of a derivative,Differentiating  with the formal definition of a derivative,y=x^x,"A friend and I were messing around with derivatives, and while we both know the procedure for finding the derivative of $y=x^x$ with logarithmic differentiation, i.e. $$y=x^x\\ \ln(y)=x\ln(x)\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}\dfrac{1}{y}=1+\ln(x) \\ \dfrac{\mathrm{d}y}{\mathrm{d}x}=y(1+\ln(x))\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}=x^x(1+\ln(x))$$ when we tried to do it with the formal definition of the derivative, we got this $$\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}\\ \lim_{h \to 0}\frac{x^{x+h}-x^x}{h}\\ \lim_{h \to 0}\frac{x^xx^h-x^x}{h}\\ \lim_{h \to 0}\frac{x^x(x^h-1)}{h}$$ then we pulled out the $x^x$ $$x^x\lim_{h \to 0}\frac{x^h-1}{h}$$ Then, using l'Hopital's rule, we differentiated with respect to $h$ , like this $$\lim_{h \to 0} \frac{x^h-1}{h}\\ \lim_{h \to 0} \frac{x^h\ln(x)}{1}$$ and since $x^h$ approaches zero, we get $\ln(x)$ for the limit Putting that together, we have $$\dfrac{\mathrm{d}y}{\mathrm{d}x}=x^x \ln(x)$$ and somewhere in between, we lost an $x^x$ . Where did I go wrong here? Am I not allowed to take the derivative with only respect to $h$ ?","A friend and I were messing around with derivatives, and while we both know the procedure for finding the derivative of with logarithmic differentiation, i.e. when we tried to do it with the formal definition of the derivative, we got this then we pulled out the Then, using l'Hopital's rule, we differentiated with respect to , like this and since approaches zero, we get for the limit Putting that together, we have and somewhere in between, we lost an . Where did I go wrong here? Am I not allowed to take the derivative with only respect to ?","y=x^x y=x^x\\
\ln(y)=x\ln(x)\\
\dfrac{\mathrm{d}y}{\mathrm{d}x}\dfrac{1}{y}=1+\ln(x) \\
\dfrac{\mathrm{d}y}{\mathrm{d}x}=y(1+\ln(x))\\
\dfrac{\mathrm{d}y}{\mathrm{d}x}=x^x(1+\ln(x)) \lim_{h \to 0}\frac{f(x+h)-f(x)}{h}\\
\lim_{h \to 0}\frac{x^{x+h}-x^x}{h}\\
\lim_{h \to 0}\frac{x^xx^h-x^x}{h}\\
\lim_{h \to 0}\frac{x^x(x^h-1)}{h} x^x x^x\lim_{h \to 0}\frac{x^h-1}{h} h \lim_{h \to 0} \frac{x^h-1}{h}\\
\lim_{h \to 0} \frac{x^h\ln(x)}{1} x^h \ln(x) \dfrac{\mathrm{d}y}{\mathrm{d}x}=x^x \ln(x) x^x h","['calculus', 'derivatives', 'logarithms']"
30,Calculus 2 Shell Method,Calculus 2 Shell Method,,"I have tried to use the standard formula for shell method and when I used this formula, the answer that I produced was incorrect. I do not know if it was a mathematical error or a formulaic error. If anybody could help with this problem, it would be greatly appreciated.","I have tried to use the standard formula for shell method and when I used this formula, the answer that I produced was incorrect. I do not know if it was a mathematical error or a formulaic error. If anybody could help with this problem, it would be greatly appreciated.",,"['calculus', 'integration']"
31,Understanding Integration techniques?,Understanding Integration techniques?,,Could someone give me a geometric interpretation of: a) Integration by Parts b) Integration by Substitution Thanks!,Could someone give me a geometric interpretation of: a) Integration by Parts b) Integration by Substitution Thanks!,,"['calculus', 'intuition']"
32,"Computing $\int\frac{7x^{13}+5x^{15}}{(x^7+x^2+1)^3}\,dx$",Computing,"\int\frac{7x^{13}+5x^{15}}{(x^7+x^2+1)^3}\,dx","Compute the indefinite integral $$ \int\frac{7x^{13}+5x^{15}}{(x^7+x^2+1)^3}\,dx $$ My Attempt: $$ \int\frac{7x^{13}+5x^{15}}{x^{21}(x^{-7}+x^{-5}+1)^3}\,dx = \int\frac{7x^{-8}+5x^{-6}}{(x^{-7}+x^{-5}+1)^3}\,dx $$ Let $t=(x^{-7}+x^{-5}+1)$ such that $$ \begin{align} dt&=(-7x^{-8}-5x^{-6})\,dx\\ -dt&=(7x^{-8}+5x^{-6})\,dx \end{align} $$ We can change the variables of the integral  to get $$ \begin{align} -\int \frac{1}{t^3}\,dt &=\frac{1}{2}\cdot\frac{1}{t^2}+C\\ &= \frac{1}{2}.\frac{1}{(x^{-7}+x^{-5}+1)^2}+C\\ &= \frac{x^{14}}{2.(1+x^2+x^7)^2}+C \end{align} $$ I'd prefer to compute the integral using methods of differentiation rather than integration, i.e. $$\frac{7x^{13}+5x^{15}}{(x^7+x^2+1)^3} = \frac{d}{dx} \left(\frac{ax^2+bx+c}{(x^7+x^2+1)^2}\right) $$ but I could not get the same answer as above.","Compute the indefinite integral $$ \int\frac{7x^{13}+5x^{15}}{(x^7+x^2+1)^3}\,dx $$ My Attempt: $$ \int\frac{7x^{13}+5x^{15}}{x^{21}(x^{-7}+x^{-5}+1)^3}\,dx = \int\frac{7x^{-8}+5x^{-6}}{(x^{-7}+x^{-5}+1)^3}\,dx $$ Let $t=(x^{-7}+x^{-5}+1)$ such that $$ \begin{align} dt&=(-7x^{-8}-5x^{-6})\,dx\\ -dt&=(7x^{-8}+5x^{-6})\,dx \end{align} $$ We can change the variables of the integral  to get $$ \begin{align} -\int \frac{1}{t^3}\,dt &=\frac{1}{2}\cdot\frac{1}{t^2}+C\\ &= \frac{1}{2}.\frac{1}{(x^{-7}+x^{-5}+1)^2}+C\\ &= \frac{x^{14}}{2.(1+x^2+x^7)^2}+C \end{align} $$ I'd prefer to compute the integral using methods of differentiation rather than integration, i.e. $$\frac{7x^{13}+5x^{15}}{(x^7+x^2+1)^3} = \frac{d}{dx} \left(\frac{ax^2+bx+c}{(x^7+x^2+1)^2}\right) $$ but I could not get the same answer as above.",,"['calculus', 'integration']"
33,Pretty solution to the trigonometric equation,Pretty solution to the trigonometric equation,,"Problem Consider the trigonometric equation: $$ a\sin x+b\cos x-\cos x\sin x=0\qquad(0\le x<2\pi)\tag{*} $$ try to analyze the number of solutions to equation (*) with parameters $a,b$, i.e, let $A=a^{2/3}+b^{2/3}-1$, we have: $A<0$, there are four distinct solutions. $A>0$, there are two distinct solutions. Endeavors Let $f(x)=a\sin x+b\cos x-\cos x\sin x$, we have $f^\prime(x)=a\cos x-b\sin x-\cos2x$. It seems no advance to calculate the derivative, because $f^\prime$ is as hard as $f$. Let $u=\cos x$ and $v=\sin x$, we have $u^2+v^2=1$ and $av+bu=uv$. We can work on these equations, but I prefer the trigonometric way, i.e, analyze the properties of $f(x)$. I want to illustrate some details about $f(x)$, which might be useful. Let $a=r\cos\phi$ and $b=r\sin\phi$, where $r=\sqrt{a^2+b^2}$, we have $f(x)=r\sin(x+\phi)-\frac12\sin2x$. It's a linear combination of $\sin(x+\phi)$ and $\sin2x$. I don't know whether there's a systematical way to deal with it. Any idea? Thanks!","Problem Consider the trigonometric equation: $$ a\sin x+b\cos x-\cos x\sin x=0\qquad(0\le x<2\pi)\tag{*} $$ try to analyze the number of solutions to equation (*) with parameters $a,b$, i.e, let $A=a^{2/3}+b^{2/3}-1$, we have: $A<0$, there are four distinct solutions. $A>0$, there are two distinct solutions. Endeavors Let $f(x)=a\sin x+b\cos x-\cos x\sin x$, we have $f^\prime(x)=a\cos x-b\sin x-\cos2x$. It seems no advance to calculate the derivative, because $f^\prime$ is as hard as $f$. Let $u=\cos x$ and $v=\sin x$, we have $u^2+v^2=1$ and $av+bu=uv$. We can work on these equations, but I prefer the trigonometric way, i.e, analyze the properties of $f(x)$. I want to illustrate some details about $f(x)$, which might be useful. Let $a=r\cos\phi$ and $b=r\sin\phi$, where $r=\sqrt{a^2+b^2}$, we have $f(x)=r\sin(x+\phi)-\frac12\sin2x$. It's a linear combination of $\sin(x+\phi)$ and $\sin2x$. I don't know whether there's a systematical way to deal with it. Any idea? Thanks!",,"['calculus', 'trigonometry']"
34,Directional derivative of vector field,Directional derivative of vector field,,"I am trying to compute the directional derivative of a vector field $V$ along a direction $U$. Actually, my vector field is initially only defined on a curve $\gamma(t)$ in a Riemannian manifold $(M, g)$, and I smoothly extend it on a tubular neighborhood of the curve in the following way : for each point in the neighborhood of $\gamma(t)$, I project it on $\gamma$ using any metric $h(u,v)$ and take the value of the vector field at that point : $V(t)$. The extension can be arbitrary for my purpose, so, $h(u,v)$ can be any Riemannian metric, not necessarily the same as $g$. What I thus need to compute, to get the directional derivative, is : $$ U V = \lim_{\epsilon \rightarrow 0} \frac{ V(\gamma^{-1}(\Pi (\gamma(t) + \epsilon U))) - V(t)}{\epsilon}$$ where $\Pi$ is the projection operator on the curve $\gamma$ and $V(t)$ is the vector field initially defined on $\gamma(t)$. I locally use a second order Taylor expansion of $\gamma(t+\delta) \approx \gamma(t) + \delta\dot\gamma(t) + \frac{\delta^2}{2}\ddot\gamma(t)$ I first compute the projection:  $$\gamma^{-1}(\Pi (\gamma(t) + \epsilon U)) = argmin_\delta \|\gamma(t+\delta) - (\gamma(t)+\epsilon U)\|_h^2$$ where I take the norm out of my arbitrary metric $h$. So, cancelling out the derivative over $\delta$ : $$ \frac{\partial}{\partial \delta} \|\gamma(t+\delta) - (\gamma(t)+\epsilon U)\|_h^2 = 0$$ $$\Rightarrow 2\,h(\dot\gamma+\delta\ddot\gamma, \delta\dot\gamma + \delta^2\ddot\gamma-\epsilon U) = 0$$ Keeping only the first order terms : $$ \Rightarrow \delta = \epsilon \frac{h(\dot\gamma, U)}{h(\dot\gamma, \dot\gamma)} = \gamma^{-1}(\Pi (\gamma(t) + \epsilon U))-t$$ Coming back to the original problem : $$ U V = \lim_{\epsilon \rightarrow 0} \frac{ V(t + \epsilon \frac{h(\dot\gamma, U)}{h( \dot\gamma, \dot\gamma)}) - V(t)}{\epsilon}$$ $$ \Rightarrow U V = \frac{h(\dot\gamma, U)}{h(\dot\gamma, \dot\gamma)} \dot V$$ In particular, when $V=\dot\gamma$, I get $U V = \frac{h(\dot\gamma, U)}{h(\dot\gamma, \dot\gamma)} \ddot\gamma$ Now, what worries me is that I want to compute the Lie bracket $[UV]=UV-VU$. Since I took $h$ to be a (symmetric) Riemannian metric, I necessarily get $[UV]=0$ for any curve, any metric, any extension... I guess that would be a wonderful theorem. But could you spot the bug ? Is it because I didn't use enough terms in the Taylor expansion (or because I truncated the second order terms when computing the result of the projection) ? Thanks!! [[EDIT: Arrrg, I realize that $[UV] \neq 0$ since $[UV]=\frac{h(\dot\gamma, U)}{h(\dot\gamma, \dot\gamma)} \dot V - \frac{h(\dot\gamma, V)}{h(\dot\gamma, \dot\gamma)} \dot U$ !! Could you however tell me if the way to proceed is correct? Thanks!! ]] Picture of the setting:","I am trying to compute the directional derivative of a vector field $V$ along a direction $U$. Actually, my vector field is initially only defined on a curve $\gamma(t)$ in a Riemannian manifold $(M, g)$, and I smoothly extend it on a tubular neighborhood of the curve in the following way : for each point in the neighborhood of $\gamma(t)$, I project it on $\gamma$ using any metric $h(u,v)$ and take the value of the vector field at that point : $V(t)$. The extension can be arbitrary for my purpose, so, $h(u,v)$ can be any Riemannian metric, not necessarily the same as $g$. What I thus need to compute, to get the directional derivative, is : $$ U V = \lim_{\epsilon \rightarrow 0} \frac{ V(\gamma^{-1}(\Pi (\gamma(t) + \epsilon U))) - V(t)}{\epsilon}$$ where $\Pi$ is the projection operator on the curve $\gamma$ and $V(t)$ is the vector field initially defined on $\gamma(t)$. I locally use a second order Taylor expansion of $\gamma(t+\delta) \approx \gamma(t) + \delta\dot\gamma(t) + \frac{\delta^2}{2}\ddot\gamma(t)$ I first compute the projection:  $$\gamma^{-1}(\Pi (\gamma(t) + \epsilon U)) = argmin_\delta \|\gamma(t+\delta) - (\gamma(t)+\epsilon U)\|_h^2$$ where I take the norm out of my arbitrary metric $h$. So, cancelling out the derivative over $\delta$ : $$ \frac{\partial}{\partial \delta} \|\gamma(t+\delta) - (\gamma(t)+\epsilon U)\|_h^2 = 0$$ $$\Rightarrow 2\,h(\dot\gamma+\delta\ddot\gamma, \delta\dot\gamma + \delta^2\ddot\gamma-\epsilon U) = 0$$ Keeping only the first order terms : $$ \Rightarrow \delta = \epsilon \frac{h(\dot\gamma, U)}{h(\dot\gamma, \dot\gamma)} = \gamma^{-1}(\Pi (\gamma(t) + \epsilon U))-t$$ Coming back to the original problem : $$ U V = \lim_{\epsilon \rightarrow 0} \frac{ V(t + \epsilon \frac{h(\dot\gamma, U)}{h( \dot\gamma, \dot\gamma)}) - V(t)}{\epsilon}$$ $$ \Rightarrow U V = \frac{h(\dot\gamma, U)}{h(\dot\gamma, \dot\gamma)} \dot V$$ In particular, when $V=\dot\gamma$, I get $U V = \frac{h(\dot\gamma, U)}{h(\dot\gamma, \dot\gamma)} \ddot\gamma$ Now, what worries me is that I want to compute the Lie bracket $[UV]=UV-VU$. Since I took $h$ to be a (symmetric) Riemannian metric, I necessarily get $[UV]=0$ for any curve, any metric, any extension... I guess that would be a wonderful theorem. But could you spot the bug ? Is it because I didn't use enough terms in the Taylor expansion (or because I truncated the second order terms when computing the result of the projection) ? Thanks!! [[EDIT: Arrrg, I realize that $[UV] \neq 0$ since $[UV]=\frac{h(\dot\gamma, U)}{h(\dot\gamma, \dot\gamma)} \dot V - \frac{h(\dot\gamma, V)}{h(\dot\gamma, \dot\gamma)} \dot U$ !! Could you however tell me if the way to proceed is correct? Thanks!! ]] Picture of the setting:",,"['calculus', 'differential-geometry', 'riemannian-geometry']"
35,How to solve $\int \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx$,How to solve,\int \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx,"One of my friends sent me a list of integrals (all without solutions )  one of those problems is: $$\int \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx$$ the numerator is $\frac{d}{dx}x^{2018}(1+x+x^2)$ and the denominator is $\left(x^2\left(x^{2018}(1+x+x^2)\right)\right)^2 +1$ so unless I am mistaken this integral is in the form of $\int\frac{f'(x)}{1+(x^2f(x))^2}dx$ which I don't know how to solve, maybe (If this problem is unsolvable ) there is a typo,  but I couldn't verify that whether this problem has a typo or not since wolfram alpha for some reason don't understand my input. Since this question seems to be incorrect I wounder what is the result of $\int_{- \infty}^{\infty} \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx$ Does this have a nice closed form ?","One of my friends sent me a list of integrals (all without solutions )  one of those problems is: the numerator is and the denominator is so unless I am mistaken this integral is in the form of which I don't know how to solve, maybe (If this problem is unsolvable ) there is a typo,  but I couldn't verify that whether this problem has a typo or not since wolfram alpha for some reason don't understand my input. Since this question seems to be incorrect I wounder what is the result of Does this have a nice closed form ?",\int \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx \frac{d}{dx}x^{2018}(1+x+x^2) \left(x^2\left(x^{2018}(1+x+x^2)\right)\right)^2 +1 \int\frac{f'(x)}{1+(x^2f(x))^2}dx \int_{- \infty}^{\infty} \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx,"['calculus', 'integration', 'definite-integrals', 'indefinite-integrals', 'closed-form']"
36,"Difficult calculus question that includes composite functions, primes, roots, etc","Difficult calculus question that includes composite functions, primes, roots, etc",,"Let $f(x)$ be a cubic whose coefficient of the leading term is positive and $g(x) = e^{\sin(\pi x)} - 1$ . The composite function $h(x) = g(f(x))$ is defined in the set of all numbers, and has a local maximum at $x=0$ . In the open interval $(0,3)$ the function $h(x)$ intersects the line $y= 1$ seven times. Given that $f(3) =1/2$ , $f'(3) = 0$ , and $f(2) = \frac{q}{p}$ , find the value of $p+q$ given that $p$ and $q$ are coprime natural numbers This is my working so far: Since $h(x)$ has a local maximum at $x=0$ , this means that at $x=0$ the derivative of $h(x) = 0$ , i.e. $$h'(0) = g'f(0) \cdot f'(0) = 0$$ We need to find the derivative of g(x) first to find an expression for the derivative of h(x), so $$\frac{d}{dx} g(x)$$ $$= \frac{d}{dx} (e^{\sin(\pi x)} - 1)$$ $$g'(x)= \pi \cos(\pi x) \cdot e^{\sin(\pi x)}$$ Hence $$h'(0) = \pi \cdot \cos(\pi f(0)) \cdot e^{\sin(\pi f(0))} \cdot f'(0) = 0$$ $$\cos(\pi f(0)) \cdot e^{\sin(\pi f(0))} \cdot f'(0) = 0$$ Given that any exponential function of the form $e^x$ is only defined for $x>0$ , $e^{\sin(\pi f(0))} \neq 0$ . So we are left with $$\cos(\pi f(0)) \cdot f'(0) = 0$$ Case 1: $$\cos(\pi f(0)) = 0$$ $$\pi f(0) = \frac{\pi}{2} + k\pi$$ where $k \in \mathbb{Z}$ $$f(0) = \frac{1}{2} + k$$ Because $f(x)$ is a cubic, we know it is of the form $f(x) = ax^3 + bx^2 + cx + d$ Hence, $$f(0) = d$$ So $d= \frac{1}{2} + k$ Case 2: $$f'(0) = 0$$ $f'(x) = 3ax^2 + 2bx + c$ , so $$f'(0) = c = 0$$ So $c=0$ Hence $f(x) = ax^3 + bx^2 + d$ , where $a>0$ and $d = \frac{1}{2} + k$ This is a part where I think I might have done something wrong. I assumed both $c=0$ and $d=\frac{1}{2} + k$ , but only one of them have to be true right? Because at first I thought both conditions would need to be true, but only one of $\cos(\pi f(0))$ or $f'(0)$ needs to be equal to $0$ for $h'(0) = 0$ to hold true (remember that $h'(0) = \cos(\pi f(0)) \cdot f'(0)$ ) Carrying on, $$f(3) = \frac{1}{2}$$ $$a \cdot 3^{3} + b \cdot 3^{2} + d= \frac{1}{2}$$ $$27a + 8b + d = \frac{1}{2}$$ $$f'(3) = 0$$ $$ 3a \cdot 3^{2} + 2b \cdot 3 = 0$$ $$ 27a + 6b = 0$$ $$f(2) = \frac{q}{p}$$ $$a \cdot 2^{3} + b \cdot 2^{2} + d= \frac{q}{p}$$ $$ 8a + 4b + d = \frac{q}{p}$$ So know we have the system of equations $$27a + 8b + d = \frac{1}{2}$$ $$ 27a + 6b = 0$$ $$ 8a + 4b + d = \frac{q}{p}$$ Let's eliminate $a$ from equations $1$ and $3 $ using equation $2$ . First, we solve equation $2$ for $27a$ : $ 27a = -6b $ Now we substitute $ -6b $ for $ 27a $ in equations $1$ and $3$ : $ -6b + 8b + d = \frac{1}{2} $ $ -\frac{6}{27} \cdot 8b + 4b + d = \frac{q}{p} $ Simplify those equations: 1. $  2b + d = \frac{1}{2} $ 3. $ -\frac{48}{27}b + 4b + d = \frac{q}{p} $ Let's simplify the coefficients in equation $3$ : $ -\frac{16}{9}b + 4b + d = \frac{q}{p} $ Multiply every term by 9 to get rid of the fraction: $ -16b + 36b + 9d = \frac{9q}{p} $ Now combine like terms: $ 20b + 9d = \frac{9q}{p} $ Now we have two equations: $ 2b + d = \frac{1}{2} $ $ 20b + 9d = \frac{9q}{p} $ We can multiply the first equation by 10 to align the coefficients of $b$ : $ 20b + 10d = 5 $ Now we have the system: $ 20b + 10d = 5 $ $ 20b + 9d = \frac{9q}{p} $ Subtract the second equation from the first: $ (20b + 10d) - (20b + 9d) = 5 - \frac{9q}{p} $ This simplifies to: $ d = 5 - \frac{9q}{p} $ Now we can substitute this value for $d$ back into one of the previous equations to find $b$ . Let's use the modified version of equation 1: $ 2b + (5 - \frac{9q}{p}) = \frac{1}{2} $ Solve for $b$ : $ 2b = \frac{1}{2} - (5 - \frac{9q}{p}) $ $ 2b = \frac{1}{2} - 5 + \frac{9q}{p} $ $ 2b = -\frac{9}{2} + \frac{9q}{p} $ $ b = -\frac{9}{4} + \frac{9q}{2p} $ Now we can substitute the value of $b$ back into equation 2 to solve for $a$ : $ 27a + 6b = 0 $ $ 27a + 6(-\frac{9}{4} + \frac{9q}{2p}) = 0 $ $ 27a - \frac{27}{2} + \frac{27q}{p} = 0 $ $ 27a = \frac{27}{2} - \frac{27q}{p} $ $ a = \frac{1}{2} - \frac{q}{p} $ Now we have expressions for $a$ , $b$ , and $d$ : $$ a = \frac{1}{2} - \frac{q}{p} $$ $$ b = -\frac{9}{4} + \frac{9q}{2p} $$ $$ d = 5 - \frac{9q}{p} $$ But remember that $d=\frac{1}{2} + k$ , where $k$ is an integer, so $$  \frac{1}{2} + k = 5 - \frac{9q}{p} $$ $$ k = \frac{9}{5} - \frac{9q}{p} $$ To make $k$ an integer, $\frac{9}{5} - \frac{9q}{p}$ must be an integer. This implies that $\frac{9q}{p}$ must be a fraction that can be expressed with a denominator of 5 since the first fraction has a denominator of 5. This is necessary for their difference to be an integer. So we can write: $$ \frac{9q}{p} = \frac{m}{5} $$ where $m$ is an integer such that when subtracted from $9$ , the result is a multiple of $5$ (ex. $m=4$ ). This is because when we subtract $\frac{m}{5}$ from $\frac{9}{5}$ , we get an integer. $$ 5 \cdot 9q = m \cdot p $$ Since $q$ and $p$ are coprime, $p$ must be a divisor of $5$ to satisfy this equation. The only natural number divisors of $5$ are $1$ and $5$ itself, but $1$ isn't prime , so $p=5$ If $p = 5$ , then $m = 9q$ Since we're looking for coprime $p$ and $q$ , since $p = 5$ , $q$ must not include the factor $5$ , and it can be any other natural number. $$f(x) = ax^3 + bx^2 + d$$ $$f(x)= (\frac{1}{2} - \frac{q}{p})x^3 + (-\frac{9}{4} + \frac{9q}{2p})x^2 + 5 - \frac{9q}{p}$$ $$ f(x)= (\frac{1}{2} - \frac{q}{5})x^3 + (-\frac{9}{4} + \frac{9q}{10})x^2 + 5 - \frac{9q}{5}$$ Now I'm stuck The number of different real roots of the equation $h(x) = 1$ in the open interval $(0,3)$ is $7$ . I don't even know what to do with this information 😭💀 I tried setting $h(x) = 1$ and solving for $x$ $$e^{\sin(\pi f(x))} - 1 = 1$$ $$e^{\sin(\pi f(x))} = 2$$ $$\sin(\pi f(x)) = \ln(2)$$ but this led nowhere again Please can someone help me solve this 😢","Let be a cubic whose coefficient of the leading term is positive and . The composite function is defined in the set of all numbers, and has a local maximum at . In the open interval the function intersects the line seven times. Given that , , and , find the value of given that and are coprime natural numbers This is my working so far: Since has a local maximum at , this means that at the derivative of , i.e. We need to find the derivative of g(x) first to find an expression for the derivative of h(x), so Hence Given that any exponential function of the form is only defined for , . So we are left with Case 1: where Because is a cubic, we know it is of the form Hence, So Case 2: , so So Hence , where and This is a part where I think I might have done something wrong. I assumed both and , but only one of them have to be true right? Because at first I thought both conditions would need to be true, but only one of or needs to be equal to for to hold true (remember that ) Carrying on, So know we have the system of equations Let's eliminate from equations and using equation . First, we solve equation for : Now we substitute for in equations and : Simplify those equations: 1. 3. Let's simplify the coefficients in equation : Multiply every term by 9 to get rid of the fraction: Now combine like terms: Now we have two equations: We can multiply the first equation by 10 to align the coefficients of : Now we have the system: Subtract the second equation from the first: This simplifies to: Now we can substitute this value for back into one of the previous equations to find . Let's use the modified version of equation 1: Solve for : Now we can substitute the value of back into equation 2 to solve for : Now we have expressions for , , and : But remember that , where is an integer, so To make an integer, must be an integer. This implies that must be a fraction that can be expressed with a denominator of 5 since the first fraction has a denominator of 5. This is necessary for their difference to be an integer. So we can write: where is an integer such that when subtracted from , the result is a multiple of (ex. ). This is because when we subtract from , we get an integer. Since and are coprime, must be a divisor of to satisfy this equation. The only natural number divisors of are and itself, but isn't prime , so If , then Since we're looking for coprime and , since , must not include the factor , and it can be any other natural number. Now I'm stuck The number of different real roots of the equation in the open interval is . I don't even know what to do with this information 😭💀 I tried setting and solving for but this led nowhere again Please can someone help me solve this 😢","f(x) g(x) = e^{\sin(\pi x)} - 1 h(x) = g(f(x)) x=0 (0,3) h(x) y= 1 f(3) =1/2 f'(3) = 0 f(2) = \frac{q}{p} p+q p q h(x) x=0 x=0 h(x) = 0 h'(0) = g'f(0) \cdot f'(0) = 0 \frac{d}{dx} g(x) = \frac{d}{dx} (e^{\sin(\pi x)} - 1) g'(x)= \pi \cos(\pi x) \cdot e^{\sin(\pi x)} h'(0) = \pi \cdot \cos(\pi f(0)) \cdot e^{\sin(\pi f(0))} \cdot f'(0) = 0 \cos(\pi f(0)) \cdot e^{\sin(\pi f(0))} \cdot f'(0) = 0 e^x x>0 e^{\sin(\pi f(0))} \neq 0 \cos(\pi f(0)) \cdot f'(0) = 0 \cos(\pi f(0)) = 0 \pi f(0) = \frac{\pi}{2} + k\pi k \in \mathbb{Z} f(0) = \frac{1}{2} + k f(x) f(x) = ax^3 + bx^2 + cx + d f(0) = d d= \frac{1}{2} + k f'(0) = 0 f'(x) = 3ax^2 + 2bx + c f'(0) = c = 0 c=0 f(x) = ax^3 + bx^2 + d a>0 d = \frac{1}{2} + k c=0 d=\frac{1}{2} + k \cos(\pi f(0)) f'(0) 0 h'(0) = 0 h'(0) = \cos(\pi f(0)) \cdot f'(0) f(3) = \frac{1}{2} a \cdot 3^{3} + b \cdot 3^{2} + d= \frac{1}{2} 27a + 8b + d = \frac{1}{2} f'(3) = 0  3a \cdot 3^{2} + 2b \cdot 3 = 0  27a + 6b = 0 f(2) = \frac{q}{p} a \cdot 2^{3} + b \cdot 2^{2} + d= \frac{q}{p}  8a + 4b + d = \frac{q}{p} 27a + 8b + d = \frac{1}{2}  27a + 6b = 0  8a + 4b + d = \frac{q}{p} a 1 3  2 2 27a  27a = -6b   -6b   27a  1 3  -6b + 8b + d = \frac{1}{2}   -\frac{6}{27} \cdot 8b + 4b + d = \frac{q}{p}    2b + d = \frac{1}{2}   -\frac{48}{27}b + 4b + d = \frac{q}{p}  3  -\frac{16}{9}b + 4b + d = \frac{q}{p}   -16b + 36b + 9d = \frac{9q}{p}   20b + 9d = \frac{9q}{p}   2b + d = \frac{1}{2}   20b + 9d = \frac{9q}{p}  b  20b + 10d = 5   20b + 10d = 5   20b + 9d = \frac{9q}{p}   (20b + 10d) - (20b + 9d) = 5 - \frac{9q}{p}   d = 5 - \frac{9q}{p}  d b  2b + (5 - \frac{9q}{p}) = \frac{1}{2}  b  2b = \frac{1}{2} - (5 - \frac{9q}{p})   2b = \frac{1}{2} - 5 + \frac{9q}{p}   2b = -\frac{9}{2} + \frac{9q}{p}   b = -\frac{9}{4} + \frac{9q}{2p}  b a  27a + 6b = 0   27a + 6(-\frac{9}{4} + \frac{9q}{2p}) = 0   27a - \frac{27}{2} + \frac{27q}{p} = 0   27a = \frac{27}{2} - \frac{27q}{p}   a = \frac{1}{2} - \frac{q}{p}  a b d  a = \frac{1}{2} - \frac{q}{p}   b = -\frac{9}{4} + \frac{9q}{2p}   d = 5 - \frac{9q}{p}  d=\frac{1}{2} + k k   \frac{1}{2} + k = 5 - \frac{9q}{p}   k = \frac{9}{5} - \frac{9q}{p}  k \frac{9}{5} - \frac{9q}{p} \frac{9q}{p}  \frac{9q}{p} = \frac{m}{5}  m 9 5 m=4 \frac{m}{5} \frac{9}{5}  5 \cdot 9q = m \cdot p  q p p 5 5 1 5 1 p=5 p = 5 m = 9q p q p = 5 q 5 f(x) = ax^3 + bx^2 + d f(x)= (\frac{1}{2} - \frac{q}{p})x^3 + (-\frac{9}{4} + \frac{9q}{2p})x^2 + 5 - \frac{9q}{p}  f(x)= (\frac{1}{2} - \frac{q}{5})x^3 + (-\frac{9}{4} + \frac{9q}{10})x^2 + 5 - \frac{9q}{5} h(x) = 1 (0,3) 7 h(x) = 1 x e^{\sin(\pi f(x))} - 1 = 1 e^{\sin(\pi f(x))} = 2 \sin(\pi f(x)) = \ln(2)","['calculus', 'functions', 'derivatives', 'polynomials']"
37,The Final Fantasy VIII leveling up problem,The Final Fantasy VIII leveling up problem,,"1. Introduction In the video game Final Fantasy VIII, you control six characters. Each character starts the game with the following amount of experience points: Character Initial EXP $C_1$ $6000$ $C_2$ $7000$ $C_3$ $7000$ $C_4$ $7000$ $C_5$ $10000$ $C_6$ $12000$ The goal is to engage in battles, win them and receive experience, until each character reaches $99000$ points. 2. Rules The following rules must be observed: Rule 1: Only three characters may engage in a battle at the same time. These are called active party members. The inactive party members do not receive experience points. The player can switch the group of active party members anytime between battles. $C_1$ is the group leader and must always be active . Rule 2: Only two of the three active party members may receive experience points after winning a battle. The player can choose which characters receive the points. The leader $C_1$ may or may not receive experience, as the player decides. Rule 3: One of the three active party members receives a bonus experience after winning a battle. The player decides which character receives the bonus points. 3. Mechanics The amount of experience received after each battle is given by the formula: $EXP=240\cdot(5\cdot (\dfrac{100-X}{X})+4)$ The variable $X$ stands for the average active party level. Considering $C_x$ , $C_y$ and $C_z$ are the active party members, then: $X=\dfrac{(C_x+C_y+C_z)}{3000}+1$ The amount of bonus experience received after each battle is given by the formula: $BONUS=30\cdot(5\cdot (\dfrac{100-Y}{Y})+4)$ The variable $Y$ stands for the character level that earns the bonus points. Considering $C_x$ the character to receive the bonus, then: $Y=\dfrac{C_x}{1000}+1$ 4. Example Since the amount of experience points the party receives after each battle lowers, as the characters earn more points, the general idea is to keep one or two low leveled characters in the active party, in order to boost the experience in each battle. Another general idea is that, since the group leader $C_1$ may not be switched out, it is best to level him up last, so that he drags down the average party level as long as possible. Quick example: Battles EXP Bonus Member 1 Member 2 Member 3 $0$ $0$ $0$ $C_1: 6000$ $C_2: 7000$ $C_3: 7000$ $1$ $16902$ $1845$ to $C_2$ $C_1: 6000$ $C_2: 25747$ $C_3: 23902$ $2$ $6075$ $595$ to $C_2$ $C_1: 6000$ $C_2: 32417$ $C_3: 29977$ $3$ $4977$ $424$ to $C_2$ $C_1: 6000$ $C_2: 37818$ $C_3: 34954$ Again, the player can change the active party members anytime, except for the group leader $C_1$ . Which is the minimum amount of battles required to complete the challenge?","1. Introduction In the video game Final Fantasy VIII, you control six characters. Each character starts the game with the following amount of experience points: Character Initial EXP The goal is to engage in battles, win them and receive experience, until each character reaches points. 2. Rules The following rules must be observed: Rule 1: Only three characters may engage in a battle at the same time. These are called active party members. The inactive party members do not receive experience points. The player can switch the group of active party members anytime between battles. is the group leader and must always be active . Rule 2: Only two of the three active party members may receive experience points after winning a battle. The player can choose which characters receive the points. The leader may or may not receive experience, as the player decides. Rule 3: One of the three active party members receives a bonus experience after winning a battle. The player decides which character receives the bonus points. 3. Mechanics The amount of experience received after each battle is given by the formula: The variable stands for the average active party level. Considering , and are the active party members, then: The amount of bonus experience received after each battle is given by the formula: The variable stands for the character level that earns the bonus points. Considering the character to receive the bonus, then: 4. Example Since the amount of experience points the party receives after each battle lowers, as the characters earn more points, the general idea is to keep one or two low leveled characters in the active party, in order to boost the experience in each battle. Another general idea is that, since the group leader may not be switched out, it is best to level him up last, so that he drags down the average party level as long as possible. Quick example: Battles EXP Bonus Member 1 Member 2 Member 3 to to to Again, the player can change the active party members anytime, except for the group leader . Which is the minimum amount of battles required to complete the challenge?",C_1 6000 C_2 7000 C_3 7000 C_4 7000 C_5 10000 C_6 12000 99000 C_1 C_1 EXP=240\cdot(5\cdot (\dfrac{100-X}{X})+4) X C_x C_y C_z X=\dfrac{(C_x+C_y+C_z)}{3000}+1 BONUS=30\cdot(5\cdot (\dfrac{100-Y}{Y})+4) Y C_x Y=\dfrac{C_x}{1000}+1 C_1 0 0 0 C_1: 6000 C_2: 7000 C_3: 7000 1 16902 1845 C_2 C_1: 6000 C_2: 25747 C_3: 23902 2 6075 595 C_2 C_1: 6000 C_2: 32417 C_3: 29977 3 4977 424 C_2 C_1: 6000 C_2: 37818 C_3: 34954 C_1,"['calculus', 'optimization']"
38,"N people in a pit are required to press their kill buttons one at a time, what percentage of the initial population is expected to live on?","N people in a pit are required to press their kill buttons one at a time, what percentage of the initial population is expected to live on?",,"I posed this problem for myself based on a simpler problem I saw on reddit, here is the more detailed version of my problem: The game master traps N people in a pit and equips them with a sort of kill button. When the button is pressed for the first time it will kill a random person, possibly even killing the button presser. The button cannot be pressed a second time (it simply wont do anything). The game master has each person take a turn pressing their button (if they are still alive). What percentage of the initial group of N people will remain? I am more specifically after $\lim_{n\to \infty} \frac{F(n)}{n}$ Where F(n) is the number of people who remain from an initial population of n people. I made a basic computer simulation of this scenario for an initial population of n people. As I plugged in bigger n values it became obvious that the percentage remaining was approaching 1/e. I've been trying to show on paper that $\lim_{n\to \infty} \frac{F(n)}{n} = \frac{1}{e}$ and can't seem to get it, help would be appreciated. Some useful information: A person can only press their button once. The people press their buttons one at a time. The button will always kill someone. (It will not try to kill an already dead person) The button can kill the button presser. Every living person has an equal chance of being chosen by the button. Information that I have gathered on the problem: The best case scenario happens when the kill buttons always choose to kill someone who has yet to use their button. Resulting in a remaining population of n/2. The worst case scenario is when the kill button kills the user every time, resulting in a remaining population of 0. The functional equation $$f(n,p) = \left(\frac{n}{p}\right)f(n-2,p-1)+\left(1-\frac{n}{p}\right)f(n-1,p-1)$$ With base cases: $$f(0,p)=p$$ $$f(1,p)=p-1$$ Represents the expected number of survivors for a given initial population of p where only the first n are assigned kill buttons. For which $f(n,n)$ is the same as $F(n)$ that I defined earlier. Easier reddit question: n people in a room randomly vote for $1$ person to be killed. When the voting period is over anybody with $1$ vote or more gets killed. What percentage of the initial n people survive? Important info: Multiple people can vote for the same person. Everyone gets $1$ vote. All of the deaths happen at the same time. Answer: The probability that someone votes for you is $\frac 1n$ , so the probability that someone does not vote for you is $1-\frac 1n$ . Then the probability that nobody votes for you is $\left(1-\frac{1}{n}\right)^n$ . The limit as n goes to infinity of that expression is also $\frac 1e$ . Why do these problems seem to have the same answer? My problem seems to be fundamentally different from the reddit one as: In my scenario one person kills one person, multiple people can't be responsible for a single death. In my scenario it is possible for someone to die before or after they kill someone themselves, where that isn't true in the case of the reddit problem.","I posed this problem for myself based on a simpler problem I saw on reddit, here is the more detailed version of my problem: The game master traps N people in a pit and equips them with a sort of kill button. When the button is pressed for the first time it will kill a random person, possibly even killing the button presser. The button cannot be pressed a second time (it simply wont do anything). The game master has each person take a turn pressing their button (if they are still alive). What percentage of the initial group of N people will remain? I am more specifically after Where F(n) is the number of people who remain from an initial population of n people. I made a basic computer simulation of this scenario for an initial population of n people. As I plugged in bigger n values it became obvious that the percentage remaining was approaching 1/e. I've been trying to show on paper that and can't seem to get it, help would be appreciated. Some useful information: A person can only press their button once. The people press their buttons one at a time. The button will always kill someone. (It will not try to kill an already dead person) The button can kill the button presser. Every living person has an equal chance of being chosen by the button. Information that I have gathered on the problem: The best case scenario happens when the kill buttons always choose to kill someone who has yet to use their button. Resulting in a remaining population of n/2. The worst case scenario is when the kill button kills the user every time, resulting in a remaining population of 0. The functional equation With base cases: Represents the expected number of survivors for a given initial population of p where only the first n are assigned kill buttons. For which is the same as that I defined earlier. Easier reddit question: n people in a room randomly vote for person to be killed. When the voting period is over anybody with vote or more gets killed. What percentage of the initial n people survive? Important info: Multiple people can vote for the same person. Everyone gets vote. All of the deaths happen at the same time. Answer: The probability that someone votes for you is , so the probability that someone does not vote for you is . Then the probability that nobody votes for you is . The limit as n goes to infinity of that expression is also . Why do these problems seem to have the same answer? My problem seems to be fundamentally different from the reddit one as: In my scenario one person kills one person, multiple people can't be responsible for a single death. In my scenario it is possible for someone to die before or after they kill someone themselves, where that isn't true in the case of the reddit problem.","\lim_{n\to \infty} \frac{F(n)}{n} \lim_{n\to \infty} \frac{F(n)}{n} = \frac{1}{e} f(n,p) = \left(\frac{n}{p}\right)f(n-2,p-1)+\left(1-\frac{n}{p}\right)f(n-1,p-1) f(0,p)=p f(1,p)=p-1 f(n,n) F(n) 1 1 1 \frac 1n 1-\frac 1n \left(1-\frac{1}{n}\right)^n \frac 1e","['calculus', 'probability', 'combinatorics', 'recreational-mathematics']"
39,$\int_0^{1398\pi} ({\sin^2x})^{\cos^2x}dx=?$,,\int_0^{1398\pi} ({\sin^2x})^{\cos^2x}dx=?,"$$\int_0^{1398\pi} ({\sin^2x})^{\cos^2x}dx=?$$ $f(x)=({\sin^2x})^{\cos^2x}$ is periodic with period equal to $\pi$ ,therefore we need to compute the integral in one period. The original question was: $$\int_0^{1398\pi} ({\sin^2x})^{\cos^2x}+({\cos^2x})^{\sin^2x}dx=?$$ Proposed by Jalil Hajimir to Romanian Math Magazine","is periodic with period equal to ,therefore we need to compute the integral in one period. The original question was: Proposed by Jalil Hajimir to Romanian Math Magazine",\int_0^{1398\pi} ({\sin^2x})^{\cos^2x}dx=? f(x)=({\sin^2x})^{\cos^2x} \pi \int_0^{1398\pi} ({\sin^2x})^{\cos^2x}+({\cos^2x})^{\sin^2x}dx=?,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'improper-integrals']"
40,"Prove or disprove: $\lim_{n\to\infty}(a_n n\ln n)=0$, where $a_n>0$, $a_{n+2}-a_{n+1}\geq a_{n+1}-a_n$, and $\sum_{k=1}^{n}a_n$ is bounded","Prove or disprove: , where , , and  is bounded",\lim_{n\to\infty}(a_n n\ln n)=0 a_n>0 a_{n+2}-a_{n+1}\geq a_{n+1}-a_n \sum_{k=1}^{n}a_n,"Suppose, for all $n \in \mathbb{N^+}$ , it holds that (1) $a_n>0$ ; (2) $a_{n+2}-a_{n+1}\geq a_{n+1}-a_n$ ; (3) $\sum_{k=1}^{n}a_n$ is bounded. Prove or disprove $$\lim\limits_{n \to \infty} (a_n\cdot n\cdot\ln n )=0$$ First, since (1) and (3), we can obtain $\sum_{n=1}^{\infty}a_n$ is convergent,hence $\lim\limits_{n \to \infty}{a_n}=0.$ Now,notice that $\sum_{k=1}^{n-1}(a_{k+1}-a_{k})=a_n-a_1$ , and let $n \to \infty$ , we can obtain $\sum_{n=1}^{\infty}(a_{n+1}-a_n)=-a_1$ .But how to go on with these? Maybe we may consider $$\ln n \sim \sum_{k=1}^{n}\frac{1}{k}, n\to \infty.$$","Suppose, for all , it holds that (1) ; (2) ; (3) is bounded. Prove or disprove First, since (1) and (3), we can obtain is convergent,hence Now,notice that , and let , we can obtain .But how to go on with these? Maybe we may consider","n \in \mathbb{N^+} a_n>0 a_{n+2}-a_{n+1}\geq a_{n+1}-a_n \sum_{k=1}^{n}a_n \lim\limits_{n \to \infty} (a_n\cdot n\cdot\ln n )=0 \sum_{n=1}^{\infty}a_n \lim\limits_{n \to \infty}{a_n}=0. \sum_{k=1}^{n-1}(a_{k+1}-a_{k})=a_n-a_1 n \to \infty \sum_{n=1}^{\infty}(a_{n+1}-a_n)=-a_1 \ln n \sim \sum_{k=1}^{n}\frac{1}{k}, n\to \infty.","['calculus', 'sequences-and-series', 'limits']"
41,Understanding the textbook: Expressing a triple integral in a different order,Understanding the textbook: Expressing a triple integral in a different order,,"I was going over my textbook and have trouble understanding this process: Why does the innermost integral have the bounds $\sqrt{y}$ and $1$ ? The shape on the $xz$ plane looks the same as the one on the $xy$ plane so I figured that you could replace $\sqrt{y}$ with $\sqrt{z}$ in the bounds. I did a quick calculation on my computer using $f(x,y,z)=xyz$ to check and the results were $0.0125$ and $0.02916667$ respectively. What am I missing here?",I was going over my textbook and have trouble understanding this process: Why does the innermost integral have the bounds and ? The shape on the plane looks the same as the one on the plane so I figured that you could replace with in the bounds. I did a quick calculation on my computer using to check and the results were and respectively. What am I missing here?,"\sqrt{y} 1 xz xy \sqrt{y} \sqrt{z} f(x,y,z)=xyz 0.0125 0.02916667","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
42,Is there a visualization for inverse trig functions as indefinite integrals,Is there a visualization for inverse trig functions as indefinite integrals,,"Examining the indefinite integral formulations of inverse trig functions I notice some things $$\arcsin(x)=\int_0^x \frac{1}{\sqrt{1-z^2}}dz$$ $$\arccos(x)=\int_x^1 \frac{1}{\sqrt{1-z^2}}dz$$ We can say that these functions ""split"" the range of integration $[0..1]$ at $x$ . Is there a visualization which expresses this relationship graphically? I mean, other than just drawing the graph, is there a visualization which meaningfully shows the relationship, in an insightful and intuitive way? Similarly, $$\arctan(x)=\int_0^x\frac{1}{z^2+1}dz$$ $$\mathrm{arccot}(x)=\int_x^\infty\frac{1}{z^2+1}dz$$ Again, these ""split"" the range $[0..\infty]$ at $x$ . Is there a digram for this? Similarly, $$\mathrm{arcsec}(x)=\int_1^x\frac{1}{z\sqrt{z^2-1}}dz$$ $$\mathrm{arccsc}(x)=\int_x^\infty\frac{1}{z\sqrt{z^2-1}}dz$$ These split the range $[1..\infty]$ at $x$ . Is there a diagram for this?","Examining the indefinite integral formulations of inverse trig functions I notice some things We can say that these functions ""split"" the range of integration at . Is there a visualization which expresses this relationship graphically? I mean, other than just drawing the graph, is there a visualization which meaningfully shows the relationship, in an insightful and intuitive way? Similarly, Again, these ""split"" the range at . Is there a digram for this? Similarly, These split the range at . Is there a diagram for this?",\arcsin(x)=\int_0^x \frac{1}{\sqrt{1-z^2}}dz \arccos(x)=\int_x^1 \frac{1}{\sqrt{1-z^2}}dz [0..1] x \arctan(x)=\int_0^x\frac{1}{z^2+1}dz \mathrm{arccot}(x)=\int_x^\infty\frac{1}{z^2+1}dz [0..\infty] x \mathrm{arcsec}(x)=\int_1^x\frac{1}{z\sqrt{z^2-1}}dz \mathrm{arccsc}(x)=\int_x^\infty\frac{1}{z\sqrt{z^2-1}}dz [1..\infty] x,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals', 'visualization']"
43,"Rigorous concise books that cover commonly taught mathematical topics since primary school up to ""calculus"" in high school","Rigorous concise books that cover commonly taught mathematical topics since primary school up to ""calculus"" in high school",,"I'll be doing some tutoring and (to help myself with the presentation of the material) I need to find a rigorous and concise book (or books) about the topics that are commonly taught since primary school until the end of high school (right before starting the ""calculus"" sequences). Does such books exist? Can you provide some examples?","I'll be doing some tutoring and (to help myself with the presentation of the material) I need to find a rigorous and concise book (or books) about the topics that are commonly taught since primary school until the end of high school (right before starting the ""calculus"" sequences). Does such books exist? Can you provide some examples?",,"['calculus', 'algebra-precalculus', 'reference-request', 'soft-question', 'euclidean-geometry']"
44,Equation for tangent line for $f(x) = 1/\sqrt{x}$ at $x=a$,Equation for tangent line for  at,f(x) = 1/\sqrt{x} x=a,"first time math stack-exchange-er here. I'm self-teaching single variable calculus using MIT's free online courses and I think I found a typo in the homework solution set (problem 1C-4 part d).  I'm not confident enough in my own abilities to know for sure if this is a mistake vs. my poor math skills. Could anyone tell me if the following equation is correct? The problem is as follows: Write an equation for the tangent line for the following functions: $$f(x) = \frac1{\sqrt{x}}\ \text{ at } x=a$$ I did the following.  First I found the derivative of $f(x)$: $$f'(x) = -\frac12 x^{-\frac32}\ $$ Then I plugged in a to get $f(a)$ and $f'(a)$ and used the point-slope method to find the equation for the tangent line: $$y-a^{-\frac12} = -\frac12 a^{-\frac32}(x-a)\ $$ Which I then simplified to: $$y=-\frac12a^{-\frac32}x  + \frac32 a^{-\frac12}\ $$ However, the solution set says the answer is: $$y=-a^{-\frac32}x  + \frac32 a^{-\frac12}\ $$ Is the solution set correct?  If so, where did the $-\frac12$ go? Thank you!","first time math stack-exchange-er here. I'm self-teaching single variable calculus using MIT's free online courses and I think I found a typo in the homework solution set (problem 1C-4 part d).  I'm not confident enough in my own abilities to know for sure if this is a mistake vs. my poor math skills. Could anyone tell me if the following equation is correct? The problem is as follows: Write an equation for the tangent line for the following functions: $$f(x) = \frac1{\sqrt{x}}\ \text{ at } x=a$$ I did the following.  First I found the derivative of $f(x)$: $$f'(x) = -\frac12 x^{-\frac32}\ $$ Then I plugged in a to get $f(a)$ and $f'(a)$ and used the point-slope method to find the equation for the tangent line: $$y-a^{-\frac12} = -\frac12 a^{-\frac32}(x-a)\ $$ Which I then simplified to: $$y=-\frac12a^{-\frac32}x  + \frac32 a^{-\frac12}\ $$ However, the solution set says the answer is: $$y=-a^{-\frac32}x  + \frac32 a^{-\frac12}\ $$ Is the solution set correct?  If so, where did the $-\frac12$ go? Thank you!",,"['calculus', 'algebra-precalculus', 'tangent-line']"
45,Solve the Differential equation $x^3 \frac{dy}{dx}=y^3+y^2\sqrt{x^2+y^2}$,Solve the Differential equation,x^3 \frac{dy}{dx}=y^3+y^2\sqrt{x^2+y^2},Solve the Differential equation $$x^3 \frac{dy}{dx}=y^3+y^2\sqrt{x^2+y^2}$$ i reduced the equation as $$x^3\frac{dy}{dx}=y^3\left(1+\sqrt{1+\left(\frac{x}{y}\right)^2}\right)$$ $\implies$ $$\frac{x^3}{y^3}\frac{dy}{dx}=\left(1+\sqrt{1+\left(\frac{x}{y}\right)^2}\right) \tag{1}$$ Next put $$\frac{x}{y}=v$$ we get $$ x=vy$$ then $$ \frac{dx}{dy}=v+y\frac{dv}{dy}$$ Then $(1)$ becomes $$\frac{v^3}{v+y\frac{dv}{dy}}=1+\sqrt{1+v^2}$$ Reciprocating we get $$\frac{v+y\frac{dv}{dy}}{v^3}=\frac{1}{\sqrt{1+v^2}+1}$$  Rationalizing RHS we get $$\frac{v+y\frac{dv}{dy}}{v^3}=\frac{\sqrt{1+v^2}-1}{v^2}$$ Rearranging we get $$\frac{dv}{v \times \left(\sqrt{1+v^2}-2\right)}=\frac{dy}{y}$$ EDIT: i am posting here the clue given by paul using Substitution $v=\tan z$: $$\int\frac{dv}{v \times \left(\sqrt{1+v^2}-2\right)}=\int\frac{\sec^2 z\: dz}{\tan z(\sec z-2)}=\int\frac{dz}{\sin z(1-2\cos z)}$$ So $$\int\frac{dz}{\sin z(1-2\cos z)}=\int \frac{\sin z\: dz}{\sin^2 z(1-2\cos z)}$$ Put $\cos z=t$ and use partial fractions,Solve the Differential equation $$x^3 \frac{dy}{dx}=y^3+y^2\sqrt{x^2+y^2}$$ i reduced the equation as $$x^3\frac{dy}{dx}=y^3\left(1+\sqrt{1+\left(\frac{x}{y}\right)^2}\right)$$ $\implies$ $$\frac{x^3}{y^3}\frac{dy}{dx}=\left(1+\sqrt{1+\left(\frac{x}{y}\right)^2}\right) \tag{1}$$ Next put $$\frac{x}{y}=v$$ we get $$ x=vy$$ then $$ \frac{dx}{dy}=v+y\frac{dv}{dy}$$ Then $(1)$ becomes $$\frac{v^3}{v+y\frac{dv}{dy}}=1+\sqrt{1+v^2}$$ Reciprocating we get $$\frac{v+y\frac{dv}{dy}}{v^3}=\frac{1}{\sqrt{1+v^2}+1}$$  Rationalizing RHS we get $$\frac{v+y\frac{dv}{dy}}{v^3}=\frac{\sqrt{1+v^2}-1}{v^2}$$ Rearranging we get $$\frac{dv}{v \times \left(\sqrt{1+v^2}-2\right)}=\frac{dy}{y}$$ EDIT: i am posting here the clue given by paul using Substitution $v=\tan z$: $$\int\frac{dv}{v \times \left(\sqrt{1+v^2}-2\right)}=\int\frac{\sec^2 z\: dz}{\tan z(\sec z-2)}=\int\frac{dz}{\sin z(1-2\cos z)}$$ So $$\int\frac{dz}{\sin z(1-2\cos z)}=\int \frac{\sin z\: dz}{\sin^2 z(1-2\cos z)}$$ Put $\cos z=t$ and use partial fractions,,"['calculus', 'algebra-precalculus', 'ordinary-differential-equations']"
46,Change of domain of a function to change variable in the limit,Change of domain of a function to change variable in the limit,,"I am trying to prove the following statement: $$ \lim_{x\to 1} \frac{\sin(x^2-1)}{x^2-1} = \lim_{h\to 0} \frac{\sin(h)}{h}$$ It might seem obvious but I have trouble going from one variable to the other. Please note that I do not know what the derivative is therefore I cannot use it to solve this problem. Now, the way I tried to complete this problem is shown next. Assume that $\lim_{h\to 0} \frac{\sin(h)}{h} = \lim_{h\to 0} f(h)$ exists and is equal to $\alpha$. Then I know that for each $\epsilon > 0 $ there is $\delta > 0$ so that if $0 < \vert h \vert < \delta$ then $\vert f(h) - \alpha \vert < \epsilon$. In general it is assumed that the domain of the function are all of the $h$ for which $f(h)$ makes sense. I want to use locality of limit. Therefore, I want to change domain of function $f(h)$ from $\mathbb{R} \setminus\{0\}$ to $(-1 ; +\infty)\setminus\{0\}$. Then, for each $\epsilon > 0 $,  I just take $ \delta ' = \mathrm{min} (1, \delta)$ so that if $0 < \vert h \vert < \delta '$ then the function is both defined and $\vert f(h) - \alpha \vert < \epsilon$. Now, because of my new domain, I want to say that every $h$ can be written as $h = x^2 - 1$ by finding some $x$. To make $x$ unique, I require that $x \geq 0$. Then, for each $\epsilon > 0 $ I can find $\delta' > 0$ so that if $0 < \vert x^2-1 \vert < \delta'$ then $\vert f(x^2-1) - \alpha \vert < \epsilon$. Now, I notice that if $\vert x^2-1 \vert = \vert x-1 \vert \vert x+1 \vert < \delta'$ then it must be true that $\vert x-1 \vert < \frac{\delta '}{\vert x+1 \vert} \leq \delta '$. Therefore I can say that for each $\epsilon > 0 $ I can find $\delta' > 0$ so that if $0 < \vert x-1 \vert < \delta'$ then $\vert f(x^2-1) - \alpha \vert < \epsilon$. But that is equivalent of saying that $\lim_{x\to 1} f(x^2-1) = \alpha$. Therefore, I think I have proved what I wanted. Questions : 1) Please can someone check my solution and justify all of the steps. 2) Maybe there is an easier solution? 3) How to deal with limits that tend $x$ to some $a$ but function is of the form $f(x^2)$ - by that I mean that there are only quadratic powers of $x$ in the values of the function. I am unsure about them because going to quadratic powers my domain changes and I do not know how to deal with it. Thanks!","I am trying to prove the following statement: $$ \lim_{x\to 1} \frac{\sin(x^2-1)}{x^2-1} = \lim_{h\to 0} \frac{\sin(h)}{h}$$ It might seem obvious but I have trouble going from one variable to the other. Please note that I do not know what the derivative is therefore I cannot use it to solve this problem. Now, the way I tried to complete this problem is shown next. Assume that $\lim_{h\to 0} \frac{\sin(h)}{h} = \lim_{h\to 0} f(h)$ exists and is equal to $\alpha$. Then I know that for each $\epsilon > 0 $ there is $\delta > 0$ so that if $0 < \vert h \vert < \delta$ then $\vert f(h) - \alpha \vert < \epsilon$. In general it is assumed that the domain of the function are all of the $h$ for which $f(h)$ makes sense. I want to use locality of limit. Therefore, I want to change domain of function $f(h)$ from $\mathbb{R} \setminus\{0\}$ to $(-1 ; +\infty)\setminus\{0\}$. Then, for each $\epsilon > 0 $,  I just take $ \delta ' = \mathrm{min} (1, \delta)$ so that if $0 < \vert h \vert < \delta '$ then the function is both defined and $\vert f(h) - \alpha \vert < \epsilon$. Now, because of my new domain, I want to say that every $h$ can be written as $h = x^2 - 1$ by finding some $x$. To make $x$ unique, I require that $x \geq 0$. Then, for each $\epsilon > 0 $ I can find $\delta' > 0$ so that if $0 < \vert x^2-1 \vert < \delta'$ then $\vert f(x^2-1) - \alpha \vert < \epsilon$. Now, I notice that if $\vert x^2-1 \vert = \vert x-1 \vert \vert x+1 \vert < \delta'$ then it must be true that $\vert x-1 \vert < \frac{\delta '}{\vert x+1 \vert} \leq \delta '$. Therefore I can say that for each $\epsilon > 0 $ I can find $\delta' > 0$ so that if $0 < \vert x-1 \vert < \delta'$ then $\vert f(x^2-1) - \alpha \vert < \epsilon$. But that is equivalent of saying that $\lim_{x\to 1} f(x^2-1) = \alpha$. Therefore, I think I have proved what I wanted. Questions : 1) Please can someone check my solution and justify all of the steps. 2) Maybe there is an easier solution? 3) How to deal with limits that tend $x$ to some $a$ but function is of the form $f(x^2)$ - by that I mean that there are only quadratic powers of $x$ in the values of the function. I am unsure about them because going to quadratic powers my domain changes and I do not know how to deal with it. Thanks!",,"['calculus', 'limits', 'limits-without-lhopital']"
47,Are upper and lower Lebesgue integrals equal for every $f$?,Are upper and lower Lebesgue integrals equal for every ?,f,"Is it true that lower and upper Lebesgue integrals are equal for every Lebesgue integrable function? If you look here you will see that: One nice feature of measurable functions is that the lower and upper   Lebesgue integrals can match, if one also assumes some boundedness. Exercise 11 Let $f: {\bf R}^d \rightarrow [0,+\infty]$ be measurable,   bounded, and vanishing outside of a set of finite measure. Show that   the lower and upper Lebesgue integrals of $f$ agree. (Hint: use   Exercise 4.) There is a converse to this statement, but we will defer   it to later notes. What happens if $f$ is allowed to be unbounded, or   is not supported inside a set of finite measure? So... what happens if $f$ is unbounded? Are upper and lower integrals equal? One person in the comments suggested it's true for unbounded functions as well (but he didn't provide a proof). Lebesgue defines area - then it's reasonable to expect that approximation of the area 'from above' and 'from below' should yield the same value (it's equivalent to saying that upper and lower Lebesgue integrals are equal - it would be nice if it was true). Based on the comments to my question, I'd say it depends on the definition of simple function - whether we want it to take finite valus, or countably many values. But, according on the definition in prof. Tao's article, what would be the correct answer to question raised in Exercise 11, quoted above? Apparently there are examples of Riemann integrable functions, whose upper Lebesgue integral doesn't equal (lower) Lebesgue integrals. But it's true that if both Riemann and Lebesgue integrals exist, they are equal.","Is it true that lower and upper Lebesgue integrals are equal for every Lebesgue integrable function? If you look here you will see that: One nice feature of measurable functions is that the lower and upper   Lebesgue integrals can match, if one also assumes some boundedness. Exercise 11 Let $f: {\bf R}^d \rightarrow [0,+\infty]$ be measurable,   bounded, and vanishing outside of a set of finite measure. Show that   the lower and upper Lebesgue integrals of $f$ agree. (Hint: use   Exercise 4.) There is a converse to this statement, but we will defer   it to later notes. What happens if $f$ is allowed to be unbounded, or   is not supported inside a set of finite measure? So... what happens if $f$ is unbounded? Are upper and lower integrals equal? One person in the comments suggested it's true for unbounded functions as well (but he didn't provide a proof). Lebesgue defines area - then it's reasonable to expect that approximation of the area 'from above' and 'from below' should yield the same value (it's equivalent to saying that upper and lower Lebesgue integrals are equal - it would be nice if it was true). Based on the comments to my question, I'd say it depends on the definition of simple function - whether we want it to take finite valus, or countably many values. But, according on the definition in prof. Tao's article, what would be the correct answer to question raised in Exercise 11, quoted above? Apparently there are examples of Riemann integrable functions, whose upper Lebesgue integral doesn't equal (lower) Lebesgue integrals. But it's true that if both Riemann and Lebesgue integrals exist, they are equal.",,"['calculus', 'measure-theory', 'lebesgue-integral']"
48,Need help proving an interval $\frac {1} {ek} \le \frac {1}{k} (1 - \frac {1}{k} )^{k-1} \le \frac {1}{2k}$,Need help proving an interval,\frac {1} {ek} \le \frac {1}{k} (1 - \frac {1}{k} )^{k-1} \le \frac {1}{2k},"I am trying to proof $$\frac {1} {ek}  \le  \frac {1}{k}   (1 - \frac {1}{k} )^{k-1} \le \frac {1}{2k} $$  for k>=2 to prove this I first multiply by k getting $$\frac {1} {e}  \le  \left(1 - \frac {1}{k} \right)^{k-1} \le \frac {1}{2} $$ then use case $k=2$ as a base case $$\frac {1} {e}  <=  \frac {1}{2}  <= \frac {1}{2} $$ which is good, then assumed $$\frac {1} {e}  <=  (1 - \frac {1}{k} )^{k-1}  <= \frac {1}{2} $$ to be true for any k>2 and try to prove for k+1 so I sustitute k+1 on k getting $$\frac {1} {e}  <=  (1 - \frac {1}{k+1} )^{k}  <= \frac {1}{2} $$ which equals $$\frac {1} {e}  <=  (\frac {k}{k+1} )^{k}  <= \frac {1}{2} $$ so I am trying to get $$ (\frac {k}{k+1} )^{k}  $$ to any of the original formulas to finish the prove but I have been unsuccesful. I have devoted a lot of time to it and dont see the solution, if anyone does thanks in aadvance. if you see any other choice that is easy to prove this pls let me know because I dont have to do it by induction","I am trying to proof $$\frac {1} {ek}  \le  \frac {1}{k}   (1 - \frac {1}{k} )^{k-1} \le \frac {1}{2k} $$  for k>=2 to prove this I first multiply by k getting $$\frac {1} {e}  \le  \left(1 - \frac {1}{k} \right)^{k-1} \le \frac {1}{2} $$ then use case $k=2$ as a base case $$\frac {1} {e}  <=  \frac {1}{2}  <= \frac {1}{2} $$ which is good, then assumed $$\frac {1} {e}  <=  (1 - \frac {1}{k} )^{k-1}  <= \frac {1}{2} $$ to be true for any k>2 and try to prove for k+1 so I sustitute k+1 on k getting $$\frac {1} {e}  <=  (1 - \frac {1}{k+1} )^{k}  <= \frac {1}{2} $$ which equals $$\frac {1} {e}  <=  (\frac {k}{k+1} )^{k}  <= \frac {1}{2} $$ so I am trying to get $$ (\frac {k}{k+1} )^{k}  $$ to any of the original formulas to finish the prove but I have been unsuccesful. I have devoted a lot of time to it and dont see the solution, if anyone does thanks in aadvance. if you see any other choice that is easy to prove this pls let me know because I dont have to do it by induction",,"['calculus', 'algebra-precalculus', 'proof-verification', 'proof-writing']"
49,Prove or disprove - Newton's method convergence in higher dimensions,Prove or disprove - Newton's method convergence in higher dimensions,,"It's not an exercise for uni or anything like that, just something that's been bothering me a bit and I can't seem to find useful information on the web on the matter. When talking about real valued scalar functions, we know that newton's method will surely converge to a root $s$ of $f(x)$ if our initial value $x_0$ is sufficiently close to the root, meaning, on the interval $(s-r,s+r)$ where $r=|s-x_0|$ the derivative $f'(x)$ is never zero (assuming $f'(s)$ is not zero). My question is, can we expand that criterion to higher dimension? Let $f: \mathbb R^{n} \to \mathbb R^n$ differentiable function with continuous partial derivatives. Let $s\in \mathbb R^n$ such that $f(s)=0$ and let $x_0 \in \mathbb R^n$ and $r=|s-x_0|$. Assume the jacobian of $f$ is invertible everywhere in the sphere with radius $r$ at epicenter $s$. Prove or disprove that newton's method will converge to $s$ if our initial value was $x_0$. Reminder: Newton's method is defined as $x_{n+1}=x_n-J^{-1}(x_n)f(x_n)$","It's not an exercise for uni or anything like that, just something that's been bothering me a bit and I can't seem to find useful information on the web on the matter. When talking about real valued scalar functions, we know that newton's method will surely converge to a root $s$ of $f(x)$ if our initial value $x_0$ is sufficiently close to the root, meaning, on the interval $(s-r,s+r)$ where $r=|s-x_0|$ the derivative $f'(x)$ is never zero (assuming $f'(s)$ is not zero). My question is, can we expand that criterion to higher dimension? Let $f: \mathbb R^{n} \to \mathbb R^n$ differentiable function with continuous partial derivatives. Let $s\in \mathbb R^n$ such that $f(s)=0$ and let $x_0 \in \mathbb R^n$ and $r=|s-x_0|$. Assume the jacobian of $f$ is invertible everywhere in the sphere with radius $r$ at epicenter $s$. Prove or disprove that newton's method will converge to $s$ if our initial value was $x_0$. Reminder: Newton's method is defined as $x_{n+1}=x_n-J^{-1}(x_n)f(x_n)$",,"['calculus', 'convergence-divergence', 'vector-spaces', 'numerical-methods', 'newton-raphson']"
50,Problem with differentiation under integral sign,Problem with differentiation under integral sign,,"Original problem: I have a problem in which i need to evaluate the integral: $$ \int_1^\infty \dfrac{\sqrt{r^2-1}e^{-\alpha r}}{r} dr\, $$ I have tried to evaluate it taking the $\alpha$ derivative, here i give you all the steps i have done: $$ \int_1^\infty \dfrac{\sqrt{r^2-1}e^{-\alpha r}}{r} dr\,=-\int d\alpha \int_1^\infty\sqrt{r^2-1}e^{-\alpha r}=-\int d\alpha \dfrac{K_1(\alpha)}{\alpha}\\ =-\frac{1}{4} G^{2,1}_{0,1}\left(\frac{\alpha}{2},\frac{1}{2} \middle| \begin{array}{c}  1 \\  -1/2,1/2,0 \\ \end{array} \right) $$ The two last steps in the last equation were obtained using Mathematica, and have been checked numerically, however i have calculated numerically the first integral for a few different values for $\alpha$ and it doesn't match the values the MeijerG function gives. So now i think i have two possibilities: -There is something wrong in the first step, the differentiation under integral sign doesn't work and i don't know why this could be the case. -It's a numerical problem. Mathematica could be giving me wrong values of the integral, but i think this can not be because i have been changing the precision and the method for the numerical integral, obtaining the same results. I want to know what is happening, i have tried similar examples with the same method and i have obtained the good results. Of course, if you have another way of evaluating this integral, don't hesitate, i would like to know it, but also i would like to know what is wrong here. EDIT: Comparison between numerical solution vs MeijerG solution (using differentiation under integral sign) as $\alpha$ functions: EDIT 2: I have found that the difference between the numeric integral and the analytic solution obtained using differentiation under integral sign is exactly $\pi/2$ for all $\alpha$. Actually I have found that for an integral of the form: $$ \int_z^\infty \dfrac{(r^2-z)^c e^{-\alpha r}}{r} dr\, $$ Being z a positive number and $0<c<1$. The differentiation under integral sign method gives the result up to a function f(z,c). For example: -For z=1 and c=1/2, this function is equal to $\pi/2$ -For z=1 and c=1/3 this function is equal to $\pi/\sqrt{3}$ -For c=1/2 this function is exactly $f(z,1/2)=-1+(1+\frac{\pi}{2})z$ With this i can continue with my calculations but, does anyone know why does this happen? Do you know a way for obtaining this function f(z,c) analytically?","Original problem: I have a problem in which i need to evaluate the integral: $$ \int_1^\infty \dfrac{\sqrt{r^2-1}e^{-\alpha r}}{r} dr\, $$ I have tried to evaluate it taking the $\alpha$ derivative, here i give you all the steps i have done: $$ \int_1^\infty \dfrac{\sqrt{r^2-1}e^{-\alpha r}}{r} dr\,=-\int d\alpha \int_1^\infty\sqrt{r^2-1}e^{-\alpha r}=-\int d\alpha \dfrac{K_1(\alpha)}{\alpha}\\ =-\frac{1}{4} G^{2,1}_{0,1}\left(\frac{\alpha}{2},\frac{1}{2} \middle| \begin{array}{c}  1 \\  -1/2,1/2,0 \\ \end{array} \right) $$ The two last steps in the last equation were obtained using Mathematica, and have been checked numerically, however i have calculated numerically the first integral for a few different values for $\alpha$ and it doesn't match the values the MeijerG function gives. So now i think i have two possibilities: -There is something wrong in the first step, the differentiation under integral sign doesn't work and i don't know why this could be the case. -It's a numerical problem. Mathematica could be giving me wrong values of the integral, but i think this can not be because i have been changing the precision and the method for the numerical integral, obtaining the same results. I want to know what is happening, i have tried similar examples with the same method and i have obtained the good results. Of course, if you have another way of evaluating this integral, don't hesitate, i would like to know it, but also i would like to know what is wrong here. EDIT: Comparison between numerical solution vs MeijerG solution (using differentiation under integral sign) as $\alpha$ functions: EDIT 2: I have found that the difference between the numeric integral and the analytic solution obtained using differentiation under integral sign is exactly $\pi/2$ for all $\alpha$. Actually I have found that for an integral of the form: $$ \int_z^\infty \dfrac{(r^2-z)^c e^{-\alpha r}}{r} dr\, $$ Being z a positive number and $0<c<1$. The differentiation under integral sign method gives the result up to a function f(z,c). For example: -For z=1 and c=1/2, this function is equal to $\pi/2$ -For z=1 and c=1/3 this function is equal to $\pi/\sqrt{3}$ -For c=1/2 this function is exactly $f(z,1/2)=-1+(1+\frac{\pi}{2})z$ With this i can continue with my calculations but, does anyone know why does this happen? Do you know a way for obtaining this function f(z,c) analytically?",,"['calculus', 'integration', 'derivatives']"
51,Equivalence of two distance function on a Riemannian manifold,Equivalence of two distance function on a Riemannian manifold,,"Let $(M,g)$ be a closed connected $m$ dimensional smooth Riemannian manifold and assume that it is isometrically embedded in a Euclidean space $\mathbb{R}^q$ by $\iota:M\to\mathbb{R}^q$. $|\ast|$ denotes the Euclidean norm in $\mathbb{R}^q$ and $d(x,y)$ denotes the geodesic distance between $x,y\in M$. Then, I think that it holds that \begin{eqnarray} \forall\varepsilon>0\ \exists\delta>0\ s.t.\ \forall x,y\in M\ \ \ 0<|y-x|<\delta\Rightarrow\ \dfrac{d(x,y)}{|y-x|}<1+\varepsilon.  \end{eqnarray} But I'm not able to varify the above statement. Can anyone help me? It is also glad if you show me the following weaker statement:  \begin{eqnarray} \exists C>0\ \forall x,y\in M\ \ \ d(x,y)\leq C|y-x|.  \end{eqnarray} Thank you.","Let $(M,g)$ be a closed connected $m$ dimensional smooth Riemannian manifold and assume that it is isometrically embedded in a Euclidean space $\mathbb{R}^q$ by $\iota:M\to\mathbb{R}^q$. $|\ast|$ denotes the Euclidean norm in $\mathbb{R}^q$ and $d(x,y)$ denotes the geodesic distance between $x,y\in M$. Then, I think that it holds that \begin{eqnarray} \forall\varepsilon>0\ \exists\delta>0\ s.t.\ \forall x,y\in M\ \ \ 0<|y-x|<\delta\Rightarrow\ \dfrac{d(x,y)}{|y-x|}<1+\varepsilon.  \end{eqnarray} But I'm not able to varify the above statement. Can anyone help me? It is also glad if you show me the following weaker statement:  \begin{eqnarray} \exists C>0\ \forall x,y\in M\ \ \ d(x,y)\leq C|y-x|.  \end{eqnarray} Thank you.",,"['calculus', 'inequality', 'manifolds', 'riemannian-geometry']"
52,Modified Hermite interpolation,Modified Hermite interpolation,,"I asked similar questions here and here , but I tried to formulate this one in a sharper way. Is anyone aware of some literature on polynomial interpolation where we sample the function and its derivatives, but perhaps we sample the derivative at a point where we have not sampled the function? For instance, given a function $f$, suppose we choose a polynomial $p$ of degree at most 3 where  $$p(a_1)=f(a_1)\\p(a_2)=f(a_2)\\p'(a_3)=f'(a_3)\\p'(a_4)=f'(a_4).$$ Finding such a polynomial $\alpha_3 x^3 + \alpha_2 x^2 + \alpha_1 x + \alpha_0$ is equivalent to solving  $$\left[ \begin{matrix} 1&a_1 & a_1^2 & a_1^3\\ 1&a_2 & a_2^2 & a_2^3\\ 0&1 & 2a_3 & 3a_3^2\\ 0&1 & 2a_4 & 3a_4^2 \end{matrix} \right] \left[ \begin{matrix} \alpha_0 \\ \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{matrix} \right] =  \left[ \begin{matrix} f(a_1) \\ f(a_2) \\ f'(a_3) \\ f'(a_4) \end{matrix} \right]. $$ I claim that the determinant of that matrix is $\frac{\partial^2}{\partial x_3\partial x_4}\left( \prod_{i<j}(x_i-x_j) \right) \left. \right|_{x_i=a_i\,\,\forall i}$. Therefore, $\frac{\partial^2}{\partial x_3\partial x_4}\left( \prod_{i<j}(x_i-x_j) \right) \left. \right|_{x_i=a_i\,\,\forall i}\neq 0$ is a necessary and sufficient condition for such a polynomial to exist uniquely. In particular, I am interested in estimating the error of such a polynomial interpolation, since I don't see how to modify the normal Hermite estimation.","I asked similar questions here and here , but I tried to formulate this one in a sharper way. Is anyone aware of some literature on polynomial interpolation where we sample the function and its derivatives, but perhaps we sample the derivative at a point where we have not sampled the function? For instance, given a function $f$, suppose we choose a polynomial $p$ of degree at most 3 where  $$p(a_1)=f(a_1)\\p(a_2)=f(a_2)\\p'(a_3)=f'(a_3)\\p'(a_4)=f'(a_4).$$ Finding such a polynomial $\alpha_3 x^3 + \alpha_2 x^2 + \alpha_1 x + \alpha_0$ is equivalent to solving  $$\left[ \begin{matrix} 1&a_1 & a_1^2 & a_1^3\\ 1&a_2 & a_2^2 & a_2^3\\ 0&1 & 2a_3 & 3a_3^2\\ 0&1 & 2a_4 & 3a_4^2 \end{matrix} \right] \left[ \begin{matrix} \alpha_0 \\ \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{matrix} \right] =  \left[ \begin{matrix} f(a_1) \\ f(a_2) \\ f'(a_3) \\ f'(a_4) \end{matrix} \right]. $$ I claim that the determinant of that matrix is $\frac{\partial^2}{\partial x_3\partial x_4}\left( \prod_{i<j}(x_i-x_j) \right) \left. \right|_{x_i=a_i\,\,\forall i}$. Therefore, $\frac{\partial^2}{\partial x_3\partial x_4}\left( \prod_{i<j}(x_i-x_j) \right) \left. \right|_{x_i=a_i\,\,\forall i}\neq 0$ is a necessary and sufficient condition for such a polynomial to exist uniquely. In particular, I am interested in estimating the error of such a polynomial interpolation, since I don't see how to modify the normal Hermite estimation.",,"['calculus', 'reference-request', 'polynomials', 'derivatives', 'interpolation']"
53,Transitioning to Higher Level Mathematics,Transitioning to Higher Level Mathematics,,"I am just finishing grade 12 pre-calculus at my school and have strong interest in math. The problem is, it seems some important elements of higher level math are not in my schools curriculum that are sometimes taught in pre-calculus. My school offers Calculus and uses Stewarts text. I am planning to take it but what I can't decide is whether or not I should be self studying logic / set theory / proofs before studying calculus at a post secondary level to be able to handle rigorous texts in post secondary like Spivak. If I was to just go ahead and take the Calculus that my school offers would I be adequately prepared for first year post secondary calculus / linear algebra courses or would most of the proofs / set theory be taught in first year p.s. courses?","I am just finishing grade 12 pre-calculus at my school and have strong interest in math. The problem is, it seems some important elements of higher level math are not in my schools curriculum that are sometimes taught in pre-calculus. My school offers Calculus and uses Stewarts text. I am planning to take it but what I can't decide is whether or not I should be self studying logic / set theory / proofs before studying calculus at a post secondary level to be able to handle rigorous texts in post secondary like Spivak. If I was to just go ahead and take the Calculus that my school offers would I be adequately prepared for first year post secondary calculus / linear algebra courses or would most of the proofs / set theory be taught in first year p.s. courses?",,"['calculus', 'algebra-precalculus', 'soft-question', 'self-learning', 'education']"
54,What number does $\sum_{k=1}^{\infty}\ln^k(1+\frac{1}{k})$ converge to?,What number does  converge to?,\sum_{k=1}^{\infty}\ln^k(1+\frac{1}{k}),What number does$$\sum_{k=1}^{\infty}\ln^k(1+\frac{1}{k})$$converge to? I think it converges by root test$$\lim_{k\to\infty}\left(\ln^{k}(1+\frac{1}{k})\right)^{\frac{1}{k}}=\lim_{k\to\infty}\ln(1+\frac{1}{k})=0$$but I don't know what tricks I can use to find what this series converges to.,What number does$$\sum_{k=1}^{\infty}\ln^k(1+\frac{1}{k})$$converge to? I think it converges by root test$$\lim_{k\to\infty}\left(\ln^{k}(1+\frac{1}{k})\right)^{\frac{1}{k}}=\lim_{k\to\infty}\ln(1+\frac{1}{k})=0$$but I don't know what tricks I can use to find what this series converges to.,,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'summation']"
55,Prove that $ \sum a^{b+c+d+e}>1$,Prove that, \sum a^{b+c+d+e}>1,"$a,b,c,d,e>o$. Show that $$ a^{b+c+d+e}+ b^{c+d+e+a}+c^{ d+e+a+b}+ d^{e+a+b+c}+e^{a+b+c+d}>1$$","$a,b,c,d,e>o$. Show that $$ a^{b+c+d+e}+ b^{c+d+e+a}+c^{ d+e+a+b}+ d^{e+a+b+c}+e^{a+b+c+d}>1$$",,"['calculus', 'inequality']"
56,Integral mean value theorem- relate a point in the derivative to an integral [duplicate],Integral mean value theorem- relate a point in the derivative to an integral [duplicate],,"This question already has answers here : Prove that $|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2}$ (2 answers) Closed 8 years ago . Question: Let $f:[a,b] \to \Bbb R$ be a continuously differentiable function s.t $f(a)=f(b)=0$ Prove that exists a point $c \in (a,b)$ such that $$ |f'(c)| \ge \frac 4{(b-a)^2} \int ^b_a f(x) dx $$ What we did: We tried using the integral mean value theorem and then using LaGrange theorem, but ended with a sum of 2 points $(f'(y_1) + f'(y_2))$ in $[a,b]$ that we didn't know how to connect to $|f'(c)|$. 2nd try (using Newton Leibnitz, and then LaGrange theorem twice): $|\frac 4{(b-a)^2}\int_a^b f(x)dx| = |\frac 4{(b-a)^2}(F(a)-F(b))|=  |\frac 4{(b-a)} \frac{(F(a)-F(b))}{b-a}| = |\frac 4{(b-a)} f(c_1)|= |\frac 4{(b-a)} \frac {(f(c_1)-f(a)) (c_1-a)}{c_1-a}|= |\frac {4(c_1-a)}{(b-a)} \frac {(f(c_1)-f(a))}{c_1-a}|= |\frac {4(c_1-a)}{(b-a)} f'(c)|$ our problem with this try is that we don't know how we can bound the last expression with f'(c)","This question already has answers here : Prove that $|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2}$ (2 answers) Closed 8 years ago . Question: Let $f:[a,b] \to \Bbb R$ be a continuously differentiable function s.t $f(a)=f(b)=0$ Prove that exists a point $c \in (a,b)$ such that $$ |f'(c)| \ge \frac 4{(b-a)^2} \int ^b_a f(x) dx $$ What we did: We tried using the integral mean value theorem and then using LaGrange theorem, but ended with a sum of 2 points $(f'(y_1) + f'(y_2))$ in $[a,b]$ that we didn't know how to connect to $|f'(c)|$. 2nd try (using Newton Leibnitz, and then LaGrange theorem twice): $|\frac 4{(b-a)^2}\int_a^b f(x)dx| = |\frac 4{(b-a)^2}(F(a)-F(b))|=  |\frac 4{(b-a)} \frac{(F(a)-F(b))}{b-a}| = |\frac 4{(b-a)} f(c_1)|= |\frac 4{(b-a)} \frac {(f(c_1)-f(a)) (c_1-a)}{c_1-a}|= |\frac {4(c_1-a)}{(b-a)} \frac {(f(c_1)-f(a))}{c_1-a}|= |\frac {4(c_1-a)}{(b-a)} f'(c)|$ our problem with this try is that we don't know how we can bound the last expression with f'(c)",,['calculus']
57,"Closed form of $\int_0^2\frac{1}{2+\sqrt{3\,e^x+3\,e^{-x}-2}}dx$",Closed form of,"\int_0^2\frac{1}{2+\sqrt{3\,e^x+3\,e^{-x}-2}}dx","Could you please help me to solve this integration problem? $$\int_0^2\frac{1}{2+\sqrt{3\,e^x+3\,e^{-x}-2}}dx$$ Its approximate numeric value is $0.419197813818367...$ , but I could not find an exact symbolic expression for it.","Could you please help me to solve this integration problem? $$\int_0^2\frac{1}{2+\sqrt{3\,e^x+3\,e^{-x}-2}}dx$$ Its approximate numeric value is $0.419197813818367...$ , but I could not find an exact symbolic expression for it.",,"['calculus', 'integration', 'definite-integrals', 'exponential-function', 'closed-form']"
58,Integrals of matrix functions,Integrals of matrix functions,,"I've stumbled across some math I've never really encountered before, and I would love it if someone could provide me with some useful references and texts on it. I'm dealing with integration over the space of matrices, as in Random Matrix Theory and such. Specifically, I think I've narrowed my confusion down to two different types of matrix integration, some of the form  \begin{equation}G=\int dA \,f(A)\end{equation} Where $A$ is a matrix of some sort, and $f(A)$ is also a matrix. Then there are integrals of the form  \begin{equation}H=\int dA \,f(|A|)\end{equation} Where $|A|$ is the determinant. This also includes integrals involving $\mathrm{tr}(A)$, i.e., integrals of scalar functions of matrices. Essentially, integrals of matrix functions of matrices, and integrals of scalar functions of matrices over some measure of matrices. Also, is there any good literature on extensions of contour integration and complex analysis to these sorts of matrix functions? Sorry for the long question.","I've stumbled across some math I've never really encountered before, and I would love it if someone could provide me with some useful references and texts on it. I'm dealing with integration over the space of matrices, as in Random Matrix Theory and such. Specifically, I think I've narrowed my confusion down to two different types of matrix integration, some of the form  \begin{equation}G=\int dA \,f(A)\end{equation} Where $A$ is a matrix of some sort, and $f(A)$ is also a matrix. Then there are integrals of the form  \begin{equation}H=\int dA \,f(|A|)\end{equation} Where $|A|$ is the determinant. This also includes integrals involving $\mathrm{tr}(A)$, i.e., integrals of scalar functions of matrices. Essentially, integrals of matrix functions of matrices, and integrals of scalar functions of matrices over some measure of matrices. Also, is there any good literature on extensions of contour integration and complex analysis to these sorts of matrix functions? Sorry for the long question.",,"['calculus', 'analysis', 'matrices', 'reference-request', 'integration']"
59,Navigating a Field of Sprinklers,Navigating a Field of Sprinklers,,"Everyday I walk to class and I have to walk through a sequence of sprinklers. I usually watch them for a second and try to plan a path in which I never have to stop or back track and will not get wet. It would be even better if I could maintain a constant speed. Question If we consider sprinkler system to be an $n \times n$ lattice where the water extends a length $L$ from the lattice point, has random initial direction and each rotates at a constant angular velocity $v$, what is the smallest  $L$ and $n$ such that there exists no smooth, path from $(0,0)$ (or more generally, from any $(a,b)$) to $(n,n)$ and $\frac{dx}{dt} \ge 0$, $\frac{dy}{dt} \ge 0$? Or, what conditions must we impose on this system to guarantee a solution (other than having them all rotating the same and having me sprint arbitrarily quickly)? Example sprinkler system : Observations Any case where $L \le .5$ is trivial since there is always a solution by following the outside of the circles formed by the sprinklers. Similarly, $L<\sqrt{2}n$ for any $n$. Of course there will not always be such an $n$ and $L$ like in the $2 \times 2$ case where there is a solution for all $L$ and any initial orientation. I feel like if $n$ and $L$ are sufficiently large, there will be no solution since you will eventually be caught inside of a polygon that is shrinking, but I do not have bounds on this conjecture.","Everyday I walk to class and I have to walk through a sequence of sprinklers. I usually watch them for a second and try to plan a path in which I never have to stop or back track and will not get wet. It would be even better if I could maintain a constant speed. Question If we consider sprinkler system to be an $n \times n$ lattice where the water extends a length $L$ from the lattice point, has random initial direction and each rotates at a constant angular velocity $v$, what is the smallest  $L$ and $n$ such that there exists no smooth, path from $(0,0)$ (or more generally, from any $(a,b)$) to $(n,n)$ and $\frac{dx}{dt} \ge 0$, $\frac{dy}{dt} \ge 0$? Or, what conditions must we impose on this system to guarantee a solution (other than having them all rotating the same and having me sprint arbitrarily quickly)? Example sprinkler system : Observations Any case where $L \le .5$ is trivial since there is always a solution by following the outside of the circles formed by the sprinklers. Similarly, $L<\sqrt{2}n$ for any $n$. Of course there will not always be such an $n$ and $L$ like in the $2 \times 2$ case where there is a solution for all $L$ and any initial orientation. I feel like if $n$ and $L$ are sufficiently large, there will be no solution since you will eventually be caught inside of a polygon that is shrinking, but I do not have bounds on this conjecture.",,"['calculus', 'recreational-mathematics']"
60,"Exercise 26 from Apostol's Calculus (p. 209, parts (c) & (d)))","Exercise 26 from Apostol's Calculus (p. 209, parts (c) & (d)))",,"This is a problem from Apostol's Calculus (p. 209 Ex. 26 (c) & (d)). The problem is to find a function $f$ with a continuous second derivative $f''$ satisfying the following conditions: (c) $f''(x) > 0 \quad \text{for every } x, \qquad f'(0) = 1, \qquad f(x) \leq 100 \quad \text{for all } x > 0$. (d) $f''(x) > 0 \quad \text{for every } x, \qquad f'(0) = 1, \qquad f(x) \leq 100 \quad \text{for all } x < 0$. So, for part (c) I do not think such a function can exist.  My proof is that $f''(x) > 0$ for every $x \implies f'(x)$ is increasing.  Since $f'(0) = 1$ this means $f'(x) > 1$ for $x > 0$. By the mean value theorem we have $f(b) - f(0) = f'(c) (b)$ for some $c \in (0,b)$.  Since $f'(c) > 1$ for all $c \in (0,b)$ we have $f(b) > b + f(0)$ for any $b > 0$.  So, just choose $b > 100 + |f(0)|$ to obtain $f(b) > 100$ contradicting that $f(x) \leq 100$ for all $x > 0$. Is this a sensible approach?  I feel like there should be a more straightforward way to get to this. For (d) it seems to me that there should be some function to satisfy these restrictions (since for $x < 0$ we can certainly let $f$ take on arbitrarily large negative values).  It isn't clear to me how to systematically identify such a function though. This problem comes from the exercises immediately following the statement and proofs of the first and second fundamental theorems of calculus, and a brief section on deducing properties of a function from its derivative; such as, a nonnegative derivative on an interval $\implies$ the function is increasing on the interval.","This is a problem from Apostol's Calculus (p. 209 Ex. 26 (c) & (d)). The problem is to find a function $f$ with a continuous second derivative $f''$ satisfying the following conditions: (c) $f''(x) > 0 \quad \text{for every } x, \qquad f'(0) = 1, \qquad f(x) \leq 100 \quad \text{for all } x > 0$. (d) $f''(x) > 0 \quad \text{for every } x, \qquad f'(0) = 1, \qquad f(x) \leq 100 \quad \text{for all } x < 0$. So, for part (c) I do not think such a function can exist.  My proof is that $f''(x) > 0$ for every $x \implies f'(x)$ is increasing.  Since $f'(0) = 1$ this means $f'(x) > 1$ for $x > 0$. By the mean value theorem we have $f(b) - f(0) = f'(c) (b)$ for some $c \in (0,b)$.  Since $f'(c) > 1$ for all $c \in (0,b)$ we have $f(b) > b + f(0)$ for any $b > 0$.  So, just choose $b > 100 + |f(0)|$ to obtain $f(b) > 100$ contradicting that $f(x) \leq 100$ for all $x > 0$. Is this a sensible approach?  I feel like there should be a more straightforward way to get to this. For (d) it seems to me that there should be some function to satisfy these restrictions (since for $x < 0$ we can certainly let $f$ take on arbitrarily large negative values).  It isn't clear to me how to systematically identify such a function though. This problem comes from the exercises immediately following the statement and proofs of the first and second fundamental theorems of calculus, and a brief section on deducing properties of a function from its derivative; such as, a nonnegative derivative on an interval $\implies$ the function is increasing on the interval.",,[]
61,Minimization of $\sum \frac{1}{n_k}\ln n_k >1 $ subject to $\sum \frac{1}{n_k}\simeq 1$,Minimization of  subject to,\sum \frac{1}{n_k}\ln n_k >1  \sum \frac{1}{n_k}\simeq 1,"Looking at an algorithm for minimizing $\sum_{k=1}^{m} \frac{1}{n_k}\ln n_k > 1$ subject to  $\sum_{k=1}^{m}\frac{1}{n_k} = 1$ in which $n_k$ are positive and in general non-sequential integers, I wondered about the more general problem of finding the minimum of $\sum_{k=1}^{m} \frac{1}{n_k}\ln n_k > 1$ subject to $\sum_{k=1}^{m}\frac{1}{n_k} \simeq 1$. For example: $\frac{1}{2}+ \frac{1}{3}+\frac{1}{6} = 1$, and $\frac{1}{2}\ln 2+ \frac{1}{3}\ln 3 + \frac{1}{6}\ln 6 \simeq 1.014$. We also have $\frac{1}{2}+\frac{1}{3}+\frac{1}{8} + \frac{1}{200}+\frac{1}{5000}   \simeq .96 $ with $\frac{1}{2}\ln 2 +\frac{1}{3}\ln 3 + \frac{1}{8}\ln 8 + \frac{1}{200}\ln 200 +\frac{1}{5000}\ln 5000   \simeq 1.0009$. Are there general ways of thinking about this? While I would think there are a finite number of solutions for $(\sum_{k=1}^{m} \frac{1}{n_k}\ln n_k - 1 )< \epsilon_1$ and $| \sum_{k=1}^{m}\frac{1}{n_k} - 1| \leq \epsilon_2$, and a countable number of solutions if m can be infinite, I don't see any systematic way of finding solutions even in the finite case. Thanks for any suggestions. Edit: typo corrected--sense of inequality in $\epsilon_1$ expression was backward. Should conform to question in title and first paragraph above.","Looking at an algorithm for minimizing $\sum_{k=1}^{m} \frac{1}{n_k}\ln n_k > 1$ subject to  $\sum_{k=1}^{m}\frac{1}{n_k} = 1$ in which $n_k$ are positive and in general non-sequential integers, I wondered about the more general problem of finding the minimum of $\sum_{k=1}^{m} \frac{1}{n_k}\ln n_k > 1$ subject to $\sum_{k=1}^{m}\frac{1}{n_k} \simeq 1$. For example: $\frac{1}{2}+ \frac{1}{3}+\frac{1}{6} = 1$, and $\frac{1}{2}\ln 2+ \frac{1}{3}\ln 3 + \frac{1}{6}\ln 6 \simeq 1.014$. We also have $\frac{1}{2}+\frac{1}{3}+\frac{1}{8} + \frac{1}{200}+\frac{1}{5000}   \simeq .96 $ with $\frac{1}{2}\ln 2 +\frac{1}{3}\ln 3 + \frac{1}{8}\ln 8 + \frac{1}{200}\ln 200 +\frac{1}{5000}\ln 5000   \simeq 1.0009$. Are there general ways of thinking about this? While I would think there are a finite number of solutions for $(\sum_{k=1}^{m} \frac{1}{n_k}\ln n_k - 1 )< \epsilon_1$ and $| \sum_{k=1}^{m}\frac{1}{n_k} - 1| \leq \epsilon_2$, and a countable number of solutions if m can be infinite, I don't see any systematic way of finding solutions even in the finite case. Thanks for any suggestions. Edit: typo corrected--sense of inequality in $\epsilon_1$ expression was backward. Should conform to question in title and first paragraph above.",,"['calculus', 'nonlinear-optimization']"
62,Does the Wronskian have anything to do with the product rule in calculus,Does the Wronskian have anything to do with the product rule in calculus,,Does the Wronskian have anything to do with the product rule in calculus. I ask this because i noticed the form looking similar to the product rule. $$W=g(x)f'(x)-g'(x)f(x)$$ where as the product rule is $$(f(x)g(x))' = g(x)f'(x)+g'(x)f(x)$$ they only differ by a plus operation.,Does the Wronskian have anything to do with the product rule in calculus. I ask this because i noticed the form looking similar to the product rule. $$W=g(x)f'(x)-g'(x)f(x)$$ where as the product rule is $$(f(x)g(x))' = g(x)f'(x)+g'(x)f(x)$$ they only differ by a plus operation.,,"['calculus', 'ordinary-differential-equations']"
63,How to find $ \int_0^1 \frac{x^n}{(1-x) \ln ^n(1-x)} d x? $,How to find, \int_0^1 \frac{x^n}{(1-x) \ln ^n(1-x)} d x? ,"Latest news Thanks to Gary Liang and metamorphy who had given me links of relevant materials so that the closed form of our integral can be found as $$ \boxed{\int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x =-\frac{n+1}{n !} \sum_{k=1}^n\left(\begin{array}{l} n \\ k \end{array}\right)(-1)^{n-k}(k+1)^{n-1} \ln (k+1)} $$ By this formula, we can evaluate the integral by finding the series instead of evaluating multiple integrals. Couple of days ago, I encountered the integral $$ J_2=\int_0^1 \frac{x^2}{(1-x) \ln ^2(1-x)} d x. $$ For convenience, I first transformed the integral by the substitution $x\mapsto 1-x$ and then made use of double integral. $$ \begin{aligned} J_2  &=\int_0^1 \frac{(1-x)^2}{x \ln ^2x} d x =-\int_0^1(1-x)^2 d\left(\frac{1}{\ln x}\right) \\&=2 \int_0^1 \frac{x-1}{\ln x} d x=2 \int_0^1 \int_0^1 x^t d t d x\\&=2 \int_0^1 \int_0^1 x^t d x d t= 2 \int_0^1\left[\frac{x^{t+1}}{t+1}\right]_0^1 d t\\&= 2 \int_0^1 \frac{1}{t+1} d t= 2 \ln 2\\ \end{aligned} $$ Then I tried generalise $J_2$ by raising the power by $2$ to $n+1$ and used similar technique to get $$J_{n+1}=\int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x= -\frac{n+1}{n}\int_0^1 \left(\frac{1-x}{\ln  x}\right)^{n} d x $$ Replacing $\phi$ by $1$ in my post , we have $$ \begin{aligned} J_{n+1}&= (-1)^{n+1}\frac{n+1}{n}\int_0^1\left(\frac{x-1}{\ln x}\right)^n d x \\ & = (-1)^{n+1}\frac{n+1}{n}\int_0^1 \left(\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text { integral signs }} x^{t_1+t_2+\ldots+t_n} d t_1 d t_2 \cdots d t_n\right) dx \\&= (-1)^{n+1}\frac{n+1}{n} \underbrace{\int_0^1\int_0^1\cdots \int_0^1}_{n \text { integral signs } }\left(\int_0^1 x^{t_1+t_2+\ldots+t_n} d x\right) d t_1 d t_2 \cdots d t_n \\ \int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x&= (-1)^{n+1}\frac{n+1}{n}\int_0^1\int_0^1 \cdots \int_0^1\frac{1}{1+t_1+t_2+\cdots+t_n} d t_1 d t_2 \cdots d t_n \blacksquare\\ \end{aligned} $$ For confirmation, we start with $$J_2=2\int_0^1 \frac{1}{1+t_1} d t_1=2\ln 2$$ $$ \begin{aligned} J_3&=-\frac{3}{2} \int_0^1 \int_0^1 \frac{1}{1+t_1+t_2} d t_1 d t_2 \\&=-\frac{3}{2} \int_0^1\left[\ln \left(1+t_1+t_2\right)\right]_0^1 d t_2 \\ &=-\frac{3}{2} \int_0^1\left[\ln \left(2+t_2\right)-\ln \left(1+t_2\right)\right] d t_2 \\ &=-\frac{3}{2}\left[\left(2+t_2\right)\left(\ln \left(2+t_2\right)-1\right) -\left(1+t_2\right)\left(\ln \left(1+t_2\right)-1\right)\right]\,_0^1\\ &=\frac{3}{2} \ln \left(\frac{16}{27}\right) \end{aligned} $$ and so on. We can find $J_{n}$ as long as we could repeatedly find $\int_0^1x^k\ln x dx$ . My question is how to simplify the multiple integral or obtain any other closed form.  Your comments and alternative methods are highly appreciated.","Latest news Thanks to Gary Liang and metamorphy who had given me links of relevant materials so that the closed form of our integral can be found as By this formula, we can evaluate the integral by finding the series instead of evaluating multiple integrals. Couple of days ago, I encountered the integral For convenience, I first transformed the integral by the substitution and then made use of double integral. Then I tried generalise by raising the power by to and used similar technique to get Replacing by in my post , we have For confirmation, we start with and so on. We can find as long as we could repeatedly find . My question is how to simplify the multiple integral or obtain any other closed form.  Your comments and alternative methods are highly appreciated.","
\boxed{\int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x =-\frac{n+1}{n !} \sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right)(-1)^{n-k}(k+1)^{n-1} \ln (k+1)}
  J_2=\int_0^1 \frac{x^2}{(1-x) \ln ^2(1-x)} d x.  x\mapsto 1-x  \begin{aligned}
J_2  &=\int_0^1 \frac{(1-x)^2}{x \ln ^2x} d x =-\int_0^1(1-x)^2 d\left(\frac{1}{\ln x}\right) \\&=2 \int_0^1 \frac{x-1}{\ln x} d x=2 \int_0^1 \int_0^1 x^t d t d x\\&=2 \int_0^1 \int_0^1 x^t d x d t= 2 \int_0^1\left[\frac{x^{t+1}}{t+1}\right]_0^1 d t\\&= 2 \int_0^1 \frac{1}{t+1} d t= 2 \ln 2\\ \end{aligned}
 J_2 2 n+1 J_{n+1}=\int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x= -\frac{n+1}{n}\int_0^1 \left(\frac{1-x}{\ln  x}\right)^{n} d x  \phi 1 
\begin{aligned}
J_{n+1}&= (-1)^{n+1}\frac{n+1}{n}\int_0^1\left(\frac{x-1}{\ln x}\right)^n d x \\
& = (-1)^{n+1}\frac{n+1}{n}\int_0^1 \left(\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text { integral signs }} x^{t_1+t_2+\ldots+t_n} d t_1 d t_2 \cdots d t_n\right) dx \\&= (-1)^{n+1}\frac{n+1}{n} \underbrace{\int_0^1\int_0^1\cdots \int_0^1}_{n \text { integral signs } }\left(\int_0^1 x^{t_1+t_2+\ldots+t_n} d x\right) d t_1 d t_2 \cdots d t_n
\\ \int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x&= (-1)^{n+1}\frac{n+1}{n}\int_0^1\int_0^1 \cdots \int_0^1\frac{1}{1+t_1+t_2+\cdots+t_n} d t_1 d t_2 \cdots d t_n \blacksquare\\
\end{aligned}
 J_2=2\int_0^1 \frac{1}{1+t_1} d t_1=2\ln 2 
\begin{aligned}
J_3&=-\frac{3}{2} \int_0^1 \int_0^1 \frac{1}{1+t_1+t_2} d t_1 d t_2 \\&=-\frac{3}{2} \int_0^1\left[\ln \left(1+t_1+t_2\right)\right]_0^1 d t_2 \\
&=-\frac{3}{2} \int_0^1\left[\ln \left(2+t_2\right)-\ln \left(1+t_2\right)\right] d t_2 \\
&=-\frac{3}{2}\left[\left(2+t_2\right)\left(\ln \left(2+t_2\right)-1\right) -\left(1+t_2\right)\left(\ln \left(1+t_2\right)-1\right)\right]\,_0^1\\
&=\frac{3}{2} \ln \left(\frac{16}{27}\right)
\end{aligned}
 J_{n} \int_0^1x^k\ln x dx","['calculus', 'integration', 'definite-integrals', 'multiple-integral']"
64,"If $|ax^2+bx+c|\le100$ for all $|x|\le 1$, What is the maxima for $|a|+|b|+|c|$","If  for all , What is the maxima for",|ax^2+bx+c|\le100 |x|\le 1 |a|+|b|+|c|,"Here is a similar problem posted before. I try to use this method to solve this problem. $$f(-1)=a-b+c, f(0)=c, f(1)=a+b+c$$ So we have $|c|=|f(0)|\le 100$ $$|2a|=|f(-1)-2f(0)+f(1)|\le|f(-1)|+2|f(0)|+|f(1)|\le400$$ So we have $|a|\le200$ But how to find an upper bound for $b$ ? Due to the symmetry, I make a guess for the maxima of $|a|+|b|+|c|$ occurring when $b=0$ , then we have $$y=200x^2-100~~~\text{or}~~~y=-200x^2+100$$ But is there a rigorous way to prove it?","Here is a similar problem posted before. I try to use this method to solve this problem. So we have So we have But how to find an upper bound for ? Due to the symmetry, I make a guess for the maxima of occurring when , then we have But is there a rigorous way to prove it?","f(-1)=a-b+c, f(0)=c, f(1)=a+b+c |c|=|f(0)|\le 100 |2a|=|f(-1)-2f(0)+f(1)|\le|f(-1)|+2|f(0)|+|f(1)|\le400 |a|\le200 b |a|+|b|+|c| b=0 y=200x^2-100~~~\text{or}~~~y=-200x^2+100","['calculus', 'algebra-precalculus']"
65,Is $\pi$! irrational? [closed],Is ! irrational? [closed],\pi,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The factorial value of $\pi$ can be found using the gamma function like so: $$\Gamma \left( \pi +1 \right) = \int\limits_0^\infty {s^{\pi} e^{ - s}\,\mathrm{d}s} ≈ 7.1880827$$ This, however, raises the question of if this number is irrational. It is well-known that $\pi$ is irrational, but is $\pi!$ irrational too?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The factorial value of can be found using the gamma function like so: This, however, raises the question of if this number is irrational. It is well-known that is irrational, but is irrational too?","\pi \Gamma \left( \pi +1 \right) = \int\limits_0^\infty {s^{\pi} e^{ - s}\,\mathrm{d}s} ≈ 7.1880827 \pi \pi!","['calculus', 'gamma-function', 'irrational-numbers', 'pi']"
66,Proving $\int_0^1 \frac{(\ln(x))^5}{1+x} \mathrm{d}x = -\frac{31\pi^6}{252}$,Proving,\int_0^1 \frac{(\ln(x))^5}{1+x} \mathrm{d}x = -\frac{31\pi^6}{252},"I would like to show the following identity: $$\boxed{ I :=  \int_0^1 \dfrac{(\ln(x))^5}{1+x} \mathrm{d}x = -\dfrac{31\pi^6}{252} }$$ Here is what I tried. The change of variables $u=1/x$ yields $$I= \int_1^{\infty} \dfrac{(\ln(x))^5}{1+1/u} \dfrac{1}{u^2} \mathrm{d}u = \int_1^{\infty} \dfrac{(\ln(x))^5}{u^2+u} \mathrm{d}u$$ Then $z=u-1$ gives $$I = \int_{0}^{\infty} \dfrac{(\ln(z+1))^5}{z^2+3z+2} \mathrm{d}z $$ with $z^2+3z+2=(z+1)(z+2)$. I wanted to use contour integration like here , but I was not sure how to proceed in this case. Anyway, the computations of the residues (of which ""well-chosen"" function? Maybe something like this ?) seem to be difficult. I believe that we can generalize to $\frac{(\ln(x))^n}{1+x}$, or maybe even more (e.g. $\frac{(\ln(x))^n}{ax^2+bx+c}$). Related computations are: (1) , (2) , (3) , (4) . Thank you for your detailed help.","I would like to show the following identity: $$\boxed{ I :=  \int_0^1 \dfrac{(\ln(x))^5}{1+x} \mathrm{d}x = -\dfrac{31\pi^6}{252} }$$ Here is what I tried. The change of variables $u=1/x$ yields $$I= \int_1^{\infty} \dfrac{(\ln(x))^5}{1+1/u} \dfrac{1}{u^2} \mathrm{d}u = \int_1^{\infty} \dfrac{(\ln(x))^5}{u^2+u} \mathrm{d}u$$ Then $z=u-1$ gives $$I = \int_{0}^{\infty} \dfrac{(\ln(z+1))^5}{z^2+3z+2} \mathrm{d}z $$ with $z^2+3z+2=(z+1)(z+2)$. I wanted to use contour integration like here , but I was not sure how to proceed in this case. Anyway, the computations of the residues (of which ""well-chosen"" function? Maybe something like this ?) seem to be difficult. I believe that we can generalize to $\frac{(\ln(x))^n}{1+x}$, or maybe even more (e.g. $\frac{(\ln(x))^n}{ax^2+bx+c}$). Related computations are: (1) , (2) , (3) , (4) . Thank you for your detailed help.",,"['calculus', 'integration', 'definite-integrals', 'contour-integration', 'closed-form']"
67,"Quadruple Integral $\int\limits_0^1\!\!\int\limits_0^1\!\!\int\limits_0^1\!\!\int\limits_0^1\frac1{1-xyzw}\,dw\,dz\,dy\,dx$",Quadruple Integral,"\int\limits_0^1\!\!\int\limits_0^1\!\!\int\limits_0^1\!\!\int\limits_0^1\frac1{1-xyzw}\,dw\,dz\,dy\,dx","In page 122 of the book Topics in Number Theory (1956) by William J. LeVeque , there is an exercise for evaluating the following integral in two ways. $$\int_0^1\!\!\!\int_0^1\frac1{1-xy}\,dy\,dx$$ First way is to write the integrand as a geometric series, $$\int_0^1\!\!\!\int_0^1\frac1{1-xy}\,dy\,dx=\int_0^1\!\!\!\int_0^1\left(\sum_{n=1}^\infty(xy)^{n-1}\right)\,dy\,dx=\sum_{n=1}^\infty\frac1{n^2}$$ and the second way by use of a suitable change of variables ( $y:=u-v,x:=u+v$ ) which is also published by Tom M. Apostol in this paper . Hence, the second way together with the result of the first way is a proof for the famous Basel problem , in fact to show that $\sum_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}6$ . Now, the main question is, if there is a suitable change of variables for the following integral $$\int_0^1\!\!\!\int_0^1\!\!\!\int_0^1\!\!\!\int_0^1\frac1{1-xyzw}\,dw\,dz\,dy\,dx~?$$ Unfortunately, I think a similar change of variables like ( $w:=p\pm q\pm r\pm s,\cdots$ ) doesn't work here, while I'm not really sure!","In page 122 of the book Topics in Number Theory (1956) by William J. LeVeque , there is an exercise for evaluating the following integral in two ways. First way is to write the integrand as a geometric series, and the second way by use of a suitable change of variables ( ) which is also published by Tom M. Apostol in this paper . Hence, the second way together with the result of the first way is a proof for the famous Basel problem , in fact to show that . Now, the main question is, if there is a suitable change of variables for the following integral Unfortunately, I think a similar change of variables like ( ) doesn't work here, while I'm not really sure!","\int_0^1\!\!\!\int_0^1\frac1{1-xy}\,dy\,dx \int_0^1\!\!\!\int_0^1\frac1{1-xy}\,dy\,dx=\int_0^1\!\!\!\int_0^1\left(\sum_{n=1}^\infty(xy)^{n-1}\right)\,dy\,dx=\sum_{n=1}^\infty\frac1{n^2} y:=u-v,x:=u+v \sum_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}6 \int_0^1\!\!\!\int_0^1\!\!\!\int_0^1\!\!\!\int_0^1\frac1{1-xyzw}\,dw\,dz\,dy\,dx~? w:=p\pm q\pm r\pm s,\cdots","['real-analysis', 'complex-analysis', 'multivariable-calculus', 'closed-form', 'multiple-integral']"
68,More on the log sine integral $\int_0^{\pi }\theta ^{3}\log^{3}\left ( 2\sin\frac{\theta }{2} \right )\mathrm{d}\theta$,More on the log sine integral,\int_0^{\pi }\theta ^{3}\log^{3}\left ( 2\sin\frac{\theta }{2} \right )\mathrm{d}\theta,"I. In this post , the OP asks about the particular log sine integral, $$\mathrm{Ls}_{7}^{\left ( 3 \right )} =-\int_{0}^{\pi }\theta ^{3}\log^{3}\left ( 2\sin\frac{\theta }{2} \right )\,\mathrm{d}\theta $$ and gives the monster evaluation, $$\small{\begin{align*} -\mathrm{Ls}_{7}^{\left ( 3 \right )}\left ( \pi  \right)&=\frac{9}{35}\log^72+\frac{4}{5}\pi ^{2} \log^52+9\zeta \left ( 3 \right )\log^42-\frac{31}{30}\pi ^{4}\log^32\\ &-\left [ 72\mathrm{Li}_5\left ( \frac{1}{2} \right )-\frac{9}{8}\zeta \left ( 5 \right )-\frac{51}{4}\pi ^{2}\zeta \left ( 3 \right ) \right ]\log^22\\ &+\left [ 72\,\color{red}{\mathrm{Li}_{5,1}}\left ( \frac{1}{2} \right )-216\mathrm{Li}_6\left ( \frac{1}{2} \right )+36\pi ^{2}\mathrm{Li}_4\left ( \frac{1}{2} \right ) \right ]\log2+72\,\color{red}{\mathrm{Li}_{6,1}}\left ( \frac{1}{2} \right )\\ &-216\mathrm{Li}_7\left ( \frac{1}{2} \right )+36\pi ^{2}\mathrm{Li}_5\left ( \frac{1}{2} \right )-\frac{1161}{32}\zeta \left ( 7 \right )-\frac{375}{32}\pi ^{2}\zeta \left ( 5 \right )+\frac{1}{10}\pi ^{4}\zeta \left ( 3 \right ) \end{align*}}$$ where I had to use small fonts to make it fit better. Update : Based on Przemo's answer , I just realized the multiple polylogarithm $\mathrm{Li}_{m,1}(z)$ is just a Nielsen generalized polylogarithm $S_{n,p}(z)$ in disguise, $$\color{red}{\mathrm{Li}_{m ,1}}\left ( z \right )=\sum_{k=1}^{\infty }\frac{z^{k}}{k^{m}}\sum_{j=1}^{k-1}\frac{1}{j} = S_{m-1,\,2}(z)$$ discussed below. II. The Nielsen generalized polylogarithm , $$S_{n,p}(z) = \frac{(-1)^{n+p-1}}{(n-1)!\,p!}\int_0^1\frac{(\ln t)^{n-1}\big(\ln(1-z\,t)\big)^p}{t}dt$$ Given $\color{blue}{z=-1}$ , for brevity let $S_{n,p}(-1) = S_{n,p}$ . Then I found the integral can be evaluated using only 6 $S_{n,p}$ with small coefficients, $$\frac1{18}\int_0^{\pi}x^3\ln^3\left(2\sin\tfrac{x}2\right)dx \\ \large{\color{blue}{=-10 S_{5,2}+14S_{4,3}-8S_{3,4}+\tfrac{\pi^2}6\Big(4S_{3,2}-9S_{2,3}+6S_{1,4}\Big)\\} =\, 0.3341049\dots}$$ However, since there is a linear relation between these six as, $$1260 S_{5,2}-1506 S_{4,3} + 1004 S_{3,4} -\tfrac{\pi^2}6\Big(157S_{3,2}-279S_{2,3}+558S_{1,4}\Big) = 0$$ then the number of terms can be reduced to $5$ . The last piece of the puzzle is to express these as polylogarithms. Note that, $$32S_{3,2}(-1) = 16\zeta(2)\zeta(3)-29\zeta(5)$$ $$32S_{2,3}(-1) = 16\zeta(2)\zeta(3)-31\zeta(5)+64S_{1,4}(-1)$$ $$30S_{1,4}(-1) = -a^5-5a^3\rm{Li}_2(\tfrac12)-15a^2\rm{Li}_3(\tfrac12) -30a\, \rm{Li}_4(\tfrac12)-30\rm{Li}_5(\tfrac12)+30\zeta(5)$$ where $a=\ln 2$ , and, $$128S_{5,2}(-1) = 64\zeta(2)\zeta(5)+112\zeta(3)\zeta(4)-251\zeta(7)$$ I've been trying to do so also for $S_{3,4}(-1)$ and $S_{4,3}(-1)$ but to no avail. The best I could do was, $$128S_{3,4}(-1)-192S_{4,3}(-1)=-64\zeta(2)\zeta(5)-160\zeta(3)\zeta(4)+315\zeta(7)$$ III. Question: For $z=-1$ , can we express $S_{3,4}(-1)$ and $S_{4,3}(-1)$ in terms of polylogarithms $\rm{Li}_m(x)$ ? In general, can we do so for all $S_{n,p}(-1)$ ? P.S. I know it can be done when $n = 1$ .","I. In this post , the OP asks about the particular log sine integral, and gives the monster evaluation, where I had to use small fonts to make it fit better. Update : Based on Przemo's answer , I just realized the multiple polylogarithm is just a Nielsen generalized polylogarithm in disguise, discussed below. II. The Nielsen generalized polylogarithm , Given , for brevity let . Then I found the integral can be evaluated using only 6 with small coefficients, However, since there is a linear relation between these six as, then the number of terms can be reduced to . The last piece of the puzzle is to express these as polylogarithms. Note that, where , and, I've been trying to do so also for and but to no avail. The best I could do was, III. Question: For , can we express and in terms of polylogarithms ? In general, can we do so for all ? P.S. I know it can be done when .","\mathrm{Ls}_{7}^{\left ( 3 \right )} =-\int_{0}^{\pi }\theta ^{3}\log^{3}\left ( 2\sin\frac{\theta }{2} \right )\,\mathrm{d}\theta  \small{\begin{align*}
-\mathrm{Ls}_{7}^{\left ( 3 \right )}\left ( \pi  \right)&=\frac{9}{35}\log^72+\frac{4}{5}\pi ^{2} \log^52+9\zeta \left ( 3 \right )\log^42-\frac{31}{30}\pi ^{4}\log^32\\
&-\left [ 72\mathrm{Li}_5\left ( \frac{1}{2} \right )-\frac{9}{8}\zeta \left ( 5 \right )-\frac{51}{4}\pi ^{2}\zeta \left ( 3 \right ) \right ]\log^22\\
&+\left [ 72\,\color{red}{\mathrm{Li}_{5,1}}\left ( \frac{1}{2} \right )-216\mathrm{Li}_6\left ( \frac{1}{2} \right )+36\pi ^{2}\mathrm{Li}_4\left ( \frac{1}{2} \right ) \right ]\log2+72\,\color{red}{\mathrm{Li}_{6,1}}\left ( \frac{1}{2} \right )\\
&-216\mathrm{Li}_7\left ( \frac{1}{2} \right )+36\pi ^{2}\mathrm{Li}_5\left ( \frac{1}{2} \right )-\frac{1161}{32}\zeta \left ( 7 \right )-\frac{375}{32}\pi ^{2}\zeta \left ( 5 \right )+\frac{1}{10}\pi ^{4}\zeta \left ( 3 \right )
\end{align*}} \mathrm{Li}_{m,1}(z) S_{n,p}(z) \color{red}{\mathrm{Li}_{m ,1}}\left ( z \right )=\sum_{k=1}^{\infty }\frac{z^{k}}{k^{m}}\sum_{j=1}^{k-1}\frac{1}{j} = S_{m-1,\,2}(z) S_{n,p}(z) = \frac{(-1)^{n+p-1}}{(n-1)!\,p!}\int_0^1\frac{(\ln t)^{n-1}\big(\ln(1-z\,t)\big)^p}{t}dt \color{blue}{z=-1} S_{n,p}(-1) = S_{n,p} S_{n,p} \frac1{18}\int_0^{\pi}x^3\ln^3\left(2\sin\tfrac{x}2\right)dx \\
\large{\color{blue}{=-10 S_{5,2}+14S_{4,3}-8S_{3,4}+\tfrac{\pi^2}6\Big(4S_{3,2}-9S_{2,3}+6S_{1,4}\Big)\\} =\, 0.3341049\dots} 1260 S_{5,2}-1506 S_{4,3} + 1004 S_{3,4} -\tfrac{\pi^2}6\Big(157S_{3,2}-279S_{2,3}+558S_{1,4}\Big) = 0 5 32S_{3,2}(-1) = 16\zeta(2)\zeta(3)-29\zeta(5) 32S_{2,3}(-1) = 16\zeta(2)\zeta(3)-31\zeta(5)+64S_{1,4}(-1) 30S_{1,4}(-1) = -a^5-5a^3\rm{Li}_2(\tfrac12)-15a^2\rm{Li}_3(\tfrac12) -30a\, \rm{Li}_4(\tfrac12)-30\rm{Li}_5(\tfrac12)+30\zeta(5) a=\ln 2 128S_{5,2}(-1) = 64\zeta(2)\zeta(5)+112\zeta(3)\zeta(4)-251\zeta(7) S_{3,4}(-1) S_{4,3}(-1) 128S_{3,4}(-1)-192S_{4,3}(-1)=-64\zeta(2)\zeta(5)-160\zeta(3)\zeta(4)+315\zeta(7) z=-1 S_{3,4}(-1) S_{4,3}(-1) \rm{Li}_m(x) S_{n,p}(-1) n = 1","['calculus', 'integration', 'closed-form', 'zeta-functions', 'polylogarithm']"
69,"Have any of you seen anything like this before? Where should someone go to publish a ""new"" function?","Have any of you seen anything like this before? Where should someone go to publish a ""new"" function?",,"I'm just some random bum calculus II student who was tinkering around with calculus and I think I came up with some new function on my own. so the tl;dr of my function is that you take some function and project that function onto another one. The function is called $b(f(x),g(x))$ where $f(x)$ is the function, and $g(x)$ will be treated as the axis. Here are some picture examples. $b(sin(.5x),5sin(.1x))$ $b(sin(x),.01x^2)$ If it turns out that this is something new, where should I send it to to spread the word? Thank you!","I'm just some random bum calculus II student who was tinkering around with calculus and I think I came up with some new function on my own. so the tl;dr of my function is that you take some function and project that function onto another one. The function is called $b(f(x),g(x))$ where $f(x)$ is the function, and $g(x)$ will be treated as the axis. Here are some picture examples. $b(sin(.5x),5sin(.1x))$ $b(sin(x),.01x^2)$ If it turns out that this is something new, where should I send it to to spread the word? Thank you!",,"['calculus', 'functions', 'proof-writing', 'publishing']"
70,Integrals that can't be solved by real methods [closed],Integrals that can't be solved by real methods [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question It seems that there are real integrals that are immune to all real methods of integration and one has to apply the residue theorem and contour integration. Here is my collection $$\int_0^1 x^{-x}(1-x)^{x-1}\sin \pi x\,\mathrm{d}x=\frac{\pi}{e}\tag {1}$$ $$\int_{-\infty}^{\infty}\frac{\mathrm{d}x}{(e^{x}-x)^{2}+{\pi}^{2}}=\frac{1}{1+W(1)}=\frac{1}{1+\Omega}\tag {2}$$ $$\int^{\pi/2}_{0}\cos(xt)\cos^y(t)\,\mathrm{d}t=\frac{\pi \Gamma(x+1)}{2^{y+1}\Gamma\left(\frac{x+y+2}{2}\right)\Gamma\left(\frac{2-x+y}{2}\right)}\tag{3}$$ $$\int_0^{1}\arctan\left(\frac{\mathrm{arctanh}\ x-\arctan{x}}{\pi+\mathrm{arctanh}\ x-\arctan{x}}\right)\frac{\mathrm{d}x}{x}=\frac{\pi}{8}\log\frac{\pi^2}{8} \tag {4}$$ $$\int_{-\infty}^{\infty} \frac{\mathrm{d}x}{(e^x+x+1)^2+\pi^2}=\frac{2}{3}\tag {5}$$ Are there any other examples ? References https://artofproblemsolving.com/community/c7h501365p2817263 Interesting integral related to the Omega Constant/Lambert W Function http://advancedintegrals.com/2017/04/integrating-a-function-around-three-branches-using-a-semi-circle-contour/ https://arxiv.org/pdf/1402.3830.pdf Integral: $\int_{-\infty}^{\infty} \frac{dx}{(e^x+x+1)^2+\pi^2}$","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question It seems that there are real integrals that are immune to all real methods of integration and one has to apply the residue theorem and contour integration. Here is my collection $$\int_0^1 x^{-x}(1-x)^{x-1}\sin \pi x\,\mathrm{d}x=\frac{\pi}{e}\tag {1}$$ $$\int_{-\infty}^{\infty}\frac{\mathrm{d}x}{(e^{x}-x)^{2}+{\pi}^{2}}=\frac{1}{1+W(1)}=\frac{1}{1+\Omega}\tag {2}$$ $$\int^{\pi/2}_{0}\cos(xt)\cos^y(t)\,\mathrm{d}t=\frac{\pi \Gamma(x+1)}{2^{y+1}\Gamma\left(\frac{x+y+2}{2}\right)\Gamma\left(\frac{2-x+y}{2}\right)}\tag{3}$$ $$\int_0^{1}\arctan\left(\frac{\mathrm{arctanh}\ x-\arctan{x}}{\pi+\mathrm{arctanh}\ x-\arctan{x}}\right)\frac{\mathrm{d}x}{x}=\frac{\pi}{8}\log\frac{\pi^2}{8} \tag {4}$$ $$\int_{-\infty}^{\infty} \frac{\mathrm{d}x}{(e^x+x+1)^2+\pi^2}=\frac{2}{3}\tag {5}$$ Are there any other examples ? References https://artofproblemsolving.com/community/c7h501365p2817263 Interesting integral related to the Omega Constant/Lambert W Function http://advancedintegrals.com/2017/04/integrating-a-function-around-three-branches-using-a-semi-circle-contour/ https://arxiv.org/pdf/1402.3830.pdf Integral: $\int_{-\infty}^{\infty} \frac{dx}{(e^x+x+1)^2+\pi^2}$",,"['calculus', 'integration', 'complex-analysis', 'contour-integration']"
71,How far has a chasing wasp flown as her target walks around a square?,How far has a chasing wasp flown as her target walks around a square?,,"I take a walk each morning along the sides of a square; each side is one mile. I start at one corner and walk at a constant speed. As I start on the walk, an unfriendly wasp always starts at the center of the square and starts chasing me, always flying directly in the direction from the wasp to myself. She must be flying a bit faster than I walk, since precisely when I complete the walk (having returned to the starting corner) the wasp meets me and greets me with an unfriendly sting. How far has the wasp flown in her chase? By supplying the wasp with a FitBit (or perhaps by numerically integrating the equations of motion) I can tell you that the answer is roughly 4.029 miles. But I would like to have either a closed form expression for the distance travelled, or failing that, a perturbative solution that will tell me how much the distance exceeds the 4 mile perimeter of the square, to at least first order in some small quantity.","I take a walk each morning along the sides of a square; each side is one mile. I start at one corner and walk at a constant speed. As I start on the walk, an unfriendly wasp always starts at the center of the square and starts chasing me, always flying directly in the direction from the wasp to myself. She must be flying a bit faster than I walk, since precisely when I complete the walk (having returned to the starting corner) the wasp meets me and greets me with an unfriendly sting. How far has the wasp flown in her chase? By supplying the wasp with a FitBit (or perhaps by numerically integrating the equations of motion) I can tell you that the answer is roughly 4.029 miles. But I would like to have either a closed form expression for the distance travelled, or failing that, a perturbative solution that will tell me how much the distance exceeds the 4 mile perimeter of the square, to at least first order in some small quantity.",,"['calculus', 'ordinary-differential-equations', 'word-problem']"
72,"More on $\sum_{n=1}^\infty\frac{(4n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256)^n}$",More on,"\sum_{n=1}^\infty\frac{(4n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256)^n}","Let, $$\alpha=2\sqrt[3]{1+\sqrt2}-\frac2{\sqrt[3]{1+\sqrt2}}$$ In this post , it was asked if, $$\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256)^n}=\frac{\sqrt3}{2\,\pi}\left(2\sqrt{\frac8{\sqrt\alpha}-\alpha}-2\sqrt\alpha-3\right)$$ I was curious about this result, and a little experimentation showed that, $$\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\color{blue}{\frac13}+n\right)\,n!^2\,(-256)^n}=\frac{\sqrt3}{2\,\pi}\small\left(\frac16\sqrt{\frac{16}{\sqrt{2\alpha+8}}-\frac{\alpha-8}{2}}+\frac{1}{12}\sqrt{2\alpha+8}-1\right)$$ More generally, defining, $$x=\frac{3}{4}+\frac{\pi}{2\sqrt3}\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256/c)^n}\tag1$$ $$y=3+2\sqrt3\,\pi\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\color{blue}{\frac13}+n\right)\,n!^2\,(-256/c)^n}\tag2$$ how do we show that $x,y$ in fact are algebraic numbers , being roots of the simple quartics, $$cx^4+4x=3\tag3$$ $$c\Big(\tfrac{y}{y+1}\Big)^4+4\Big(\tfrac{y}{y+1}\Big)=3\tag4$$","Let, $$\alpha=2\sqrt[3]{1+\sqrt2}-\frac2{\sqrt[3]{1+\sqrt2}}$$ In this post , it was asked if, $$\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256)^n}=\frac{\sqrt3}{2\,\pi}\left(2\sqrt{\frac8{\sqrt\alpha}-\alpha}-2\sqrt\alpha-3\right)$$ I was curious about this result, and a little experimentation showed that, $$\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\color{blue}{\frac13}+n\right)\,n!^2\,(-256)^n}=\frac{\sqrt3}{2\,\pi}\small\left(\frac16\sqrt{\frac{16}{\sqrt{2\alpha+8}}-\frac{\alpha-8}{2}}+\frac{1}{12}\sqrt{2\alpha+8}-1\right)$$ More generally, defining, $$x=\frac{3}{4}+\frac{\pi}{2\sqrt3}\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256/c)^n}\tag1$$ $$y=3+2\sqrt3\,\pi\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\color{blue}{\frac13}+n\right)\,n!^2\,(-256/c)^n}\tag2$$ how do we show that $x,y$ in fact are algebraic numbers , being roots of the simple quartics, $$cx^4+4x=3\tag3$$ $$c\Big(\tfrac{y}{y+1}\Big)^4+4\Big(\tfrac{y}{y+1}\Big)=3\tag4$$",,"['calculus', 'sequences-and-series', 'closed-form', 'hypergeometric-function']"
73,Improper Integral $\int_0^\infty\tan\left(\frac x{\sqrt{x^3+x^2}}\right)\frac{\ln(1+\sqrt x)}xdx$,Improper Integral,\int_0^\infty\tan\left(\frac x{\sqrt{x^3+x^2}}\right)\frac{\ln(1+\sqrt x)}xdx,This integral is from integral Find  $$\int_0^\infty\tan\left(\frac x{\sqrt{x^3+x^2}}\right)\frac{\ln(1+\sqrt x)}xdx$$ I have get $$\int_0^\infty\tan\left(\frac x{\sqrt{x^3+x^2}}\right)\frac{\ln(1+\sqrt x)}xdx=\int_0^{\infty}\tan\left(\frac1{\sqrt{x+1}}\right)\frac{\ln(1+\sqrt x)}{x}dx$$ Let$$\dfrac{1}{\sqrt{x+1}}=t$$ that $$I=\int_{0}^{1}\dfrac{\tan{t}\ln{\left(1+\sqrt{\frac{1}{t^2}-1}\right)}}{t-t^3}dt$$ This integral is have  closed form ?,This integral is from integral Find  $$\int_0^\infty\tan\left(\frac x{\sqrt{x^3+x^2}}\right)\frac{\ln(1+\sqrt x)}xdx$$ I have get $$\int_0^\infty\tan\left(\frac x{\sqrt{x^3+x^2}}\right)\frac{\ln(1+\sqrt x)}xdx=\int_0^{\infty}\tan\left(\frac1{\sqrt{x+1}}\right)\frac{\ln(1+\sqrt x)}{x}dx$$ Let$$\dfrac{1}{\sqrt{x+1}}=t$$ that $$I=\int_{0}^{1}\dfrac{\tan{t}\ln{\left(1+\sqrt{\frac{1}{t^2}-1}\right)}}{t-t^3}dt$$ This integral is have  closed form ?,,"['calculus', 'integration', 'analysis', 'improper-integrals', 'closed-form']"
74,Maple Code for Ricci Flow,Maple Code for Ricci Flow,,"I am trying to generate a Maple Simulation for Ricci Flow assuming general Solids of Revolution and such. I assume that there is a surface with the following parametrization: $$S:\left\{\begin{matrix}x=R(z,t)\cos\theta \\y=R(z,t)\sin\theta\\ z=z\end{matrix}\phantom{..}\begin{matrix}\theta\in[0..2\pi] \\ z\in[0..1]\end{matrix}\right.$$ And want to see how the surface is deformed by Ricci Flow. My Maple Code is as such: -I define my x,y,&z coordinates -I define the tangent vectors -I define the metric tensor -I define the Christoffel Symbols -I define the Ricci Tensor -I attempt at pdsolving the equation $$\frac{\partial}{\partial t}g_{i j}=-2R_{i j}$$ Everything goes lovely, but upon the execution of the final line, I get the following message: "" Error, (in pdsolve/numeric/plot) unable to compute solution for t>HFloat(0.0): Newton iteration is not converging "" Can someone please help me as to the fail in the way I set up my Maple Code? Thanks a lot everyone.","I am trying to generate a Maple Simulation for Ricci Flow assuming general Solids of Revolution and such. I assume that there is a surface with the following parametrization: $$S:\left\{\begin{matrix}x=R(z,t)\cos\theta \\y=R(z,t)\sin\theta\\ z=z\end{matrix}\phantom{..}\begin{matrix}\theta\in[0..2\pi] \\ z\in[0..1]\end{matrix}\right.$$ And want to see how the surface is deformed by Ricci Flow. My Maple Code is as such: -I define my x,y,&z coordinates -I define the tangent vectors -I define the metric tensor -I define the Christoffel Symbols -I define the Ricci Tensor -I attempt at pdsolving the equation $$\frac{\partial}{\partial t}g_{i j}=-2R_{i j}$$ Everything goes lovely, but upon the execution of the final line, I get the following message: "" Error, (in pdsolve/numeric/plot) unable to compute solution for t>HFloat(0.0): Newton iteration is not converging "" Can someone please help me as to the fail in the way I set up my Maple Code? Thanks a lot everyone.",,"['calculus', 'differential-geometry', 'partial-differential-equations', 'maple', 'ricci-flow']"
75,"prove that $\sin (\tan x)\geq x\;\forall x\in \left[0,\frac{\pi}{4}\right]$",prove that,"\sin (\tan x)\geq x\;\forall x\in \left[0,\frac{\pi}{4}\right]","Using the relation $2(1-\cos x)<x^2,x\neq 0$ or otherwise, prove that   $$ \sin (\tan x)\geq x\;\forall x\in \left[0,\frac{\pi}{4}\right] $$ My Attempt: Let $f(x) = \sin (\tan x)-x$. Then $f'(x) = \cos (\tan x)\cdot \sec^2 x-1$. Now using the given statement that $2(1-\cos x)<x^2$ for $x\neq0$, set $x=\tan x$ to get $$ \begin{align} 2(1-\cos (\tan x))&<\tan^2 x\\ \left[1-\cos (\tan x)\right]&<\frac{\tan^2 x}{2}\\ \cos (\tan x)&>\left[\frac{2-\tan^2 x}{2}\right]\\ \cos (\tan x)\cdot \sec^2 x-1&>\left[\frac{2-\tan^2 x}{2}\right]\cdot \sec^2 x-1\\ f'(x)&>\left[\frac{2-\tan^2 x}{2}\right]\cdot \sec^2 x-1\\ f'(x)&>\tan^2 x-\frac{\tan^2 x}{2}\cdot \sec^2 x\\ f'(x) \color{red}{=} \frac{\tan^2 x}{2}\cdot \left[2-\sec^2 x\right]&>0\;\; \forall x\in \left(0,\frac{\pi}{4}\right)\\ f'(x)&>0\;\; \forall x\in \left(0,\frac{\pi}{4}\right) \end{align} $$ Thus $f(x)$ is a strictly increasing function for $x\in \left(0,\frac{\pi}{4}\right)$. Furthermore, $$ \begin{align} f(x)&>f(0)\\ \sin (\tan x)-x&\geq 0\\ \sin (\tan x)-x&>0 \;\forall x\in \left(0,\frac{\pi}{4}\right) \end{align} $$ Now $f(0)=0$ and $\displaystyle f\left(\frac{\pi}{4}\right) = \sin (1)-\frac{\pi}{4}>0$. Therefore, $$ f(x)\geq 0\;\forall x \in  \left[0,\frac{\pi}{4}\right] $$ Can we solve this using any other method?","Using the relation $2(1-\cos x)<x^2,x\neq 0$ or otherwise, prove that   $$ \sin (\tan x)\geq x\;\forall x\in \left[0,\frac{\pi}{4}\right] $$ My Attempt: Let $f(x) = \sin (\tan x)-x$. Then $f'(x) = \cos (\tan x)\cdot \sec^2 x-1$. Now using the given statement that $2(1-\cos x)<x^2$ for $x\neq0$, set $x=\tan x$ to get $$ \begin{align} 2(1-\cos (\tan x))&<\tan^2 x\\ \left[1-\cos (\tan x)\right]&<\frac{\tan^2 x}{2}\\ \cos (\tan x)&>\left[\frac{2-\tan^2 x}{2}\right]\\ \cos (\tan x)\cdot \sec^2 x-1&>\left[\frac{2-\tan^2 x}{2}\right]\cdot \sec^2 x-1\\ f'(x)&>\left[\frac{2-\tan^2 x}{2}\right]\cdot \sec^2 x-1\\ f'(x)&>\tan^2 x-\frac{\tan^2 x}{2}\cdot \sec^2 x\\ f'(x) \color{red}{=} \frac{\tan^2 x}{2}\cdot \left[2-\sec^2 x\right]&>0\;\; \forall x\in \left(0,\frac{\pi}{4}\right)\\ f'(x)&>0\;\; \forall x\in \left(0,\frac{\pi}{4}\right) \end{align} $$ Thus $f(x)$ is a strictly increasing function for $x\in \left(0,\frac{\pi}{4}\right)$. Furthermore, $$ \begin{align} f(x)&>f(0)\\ \sin (\tan x)-x&\geq 0\\ \sin (\tan x)-x&>0 \;\forall x\in \left(0,\frac{\pi}{4}\right) \end{align} $$ Now $f(0)=0$ and $\displaystyle f\left(\frac{\pi}{4}\right) = \sin (1)-\frac{\pi}{4}>0$. Therefore, $$ f(x)\geq 0\;\forall x \in  \left[0,\frac{\pi}{4}\right] $$ Can we solve this using any other method?",,['calculus']
76,Why Are Fresnel Functions Used For Splines?,Why Are Fresnel Functions Used For Splines?,,"Why are Fresnel functions still used in the research and implementation of clothoid splines?  They cannot represent curves of constant curvature, which has led to a lot of research/implementation complexity.  For example, see this paper: http://www.mit.edu/~ibaran/curves/ $\dots$that constructs a clothoid spline by finding the shortest path through a graph of possible clothoid/arc/line combinations. Fresnels are derived by integrating the following curvature function: $k = 2s$ There are many superior alternatives; I've attached one alternative I came up with to this post, but see Raph Levien's PhD thesis for another: http://www.levien.com/phd/phd.html Why are fresnels still around?  Levien's work dates from 2007, and some guy in the early 90s also did research on (polynomial) curvature functions.  As far as I can tell, most of the modern clothoid literature is little more advanced than where the literature was in the late 60s – it’s as if people like Levien never published at all.  Is this a failure of the peer review system?  Or does the fault lie with the way clothoid research is fragmented between many different fields that don't really communicate with each other? Anyway, here’s the curve I came up with.  See Levien's thesis for a more in-depth discussion of why simple fresnels aren’t good enough (as well we his method, which is a bit better than the one here). Addendum. A Better Euler Spiral Formulation If our goal is to create a spline whose curvature function is piecewise linear, we can use this curvature function: $k = k_1*(1.0-s) + k_2*s$ This function interpolates two constants over the interval [0, 1]. If you integrate it, you get these replacements for the fresnel functions: $th(s) = s(k_2s - k_1s + 2k_1) / 2$ $x(s) = \int_{0}^{s} \sin(th(s))$ $y(s) = \int_{0}^{s} \cos(th(s))$ These functions can represent lines, arcs, and clothoids, and is far easier to work with than if all three types are treated separately.","Why are Fresnel functions still used in the research and implementation of clothoid splines?  They cannot represent curves of constant curvature, which has led to a lot of research/implementation complexity.  For example, see this paper: http://www.mit.edu/~ibaran/curves/ $\dots$that constructs a clothoid spline by finding the shortest path through a graph of possible clothoid/arc/line combinations. Fresnels are derived by integrating the following curvature function: $k = 2s$ There are many superior alternatives; I've attached one alternative I came up with to this post, but see Raph Levien's PhD thesis for another: http://www.levien.com/phd/phd.html Why are fresnels still around?  Levien's work dates from 2007, and some guy in the early 90s also did research on (polynomial) curvature functions.  As far as I can tell, most of the modern clothoid literature is little more advanced than where the literature was in the late 60s – it’s as if people like Levien never published at all.  Is this a failure of the peer review system?  Or does the fault lie with the way clothoid research is fragmented between many different fields that don't really communicate with each other? Anyway, here’s the curve I came up with.  See Levien's thesis for a more in-depth discussion of why simple fresnels aren’t good enough (as well we his method, which is a bit better than the one here). Addendum. A Better Euler Spiral Formulation If our goal is to create a spline whose curvature function is piecewise linear, we can use this curvature function: $k = k_1*(1.0-s) + k_2*s$ This function interpolates two constants over the interval [0, 1]. If you integrate it, you get these replacements for the fresnel functions: $th(s) = s(k_2s - k_1s + 2k_1) / 2$ $x(s) = \int_{0}^{s} \sin(th(s))$ $y(s) = \int_{0}^{s} \cos(th(s))$ These functions can represent lines, arcs, and clothoids, and is far easier to work with than if all three types are treated separately.",,"['calculus', 'geometry', 'special-functions', 'plane-curves', 'spline']"
77,Hard sum with harmonics numbers [closed],Hard sum with harmonics numbers [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove or disprove that $S=\displaystyle\sum_{n=1}^{\infty}\frac{{H_n^{2}}~{H_n^{(2)}}+3{H_n^{(4)}}}{n~2^n}=\frac{25}{16}\zeta(5)+\frac{7}{8}\zeta(2)\zeta(3)$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove or disprove that $S=\displaystyle\sum_{n=1}^{\infty}\frac{{H_n^{2}}~{H_n^{(2)}}+3{H_n^{(4)}}}{n~2^n}=\frac{25}{16}\zeta(5)+\frac{7}{8}\zeta(2)\zeta(3)$.",,"['calculus', 'integration', 'definite-integrals', 'harmonic-numbers', 'polylogarithm']"
78,GRE textbooks question - calculus and linear algebra,GRE textbooks question - calculus and linear algebra,,"I'll be taking the Math GRE subject test in a few months. I know that it is best to focus on being able to answer all the calculus, DEs, and linear algebra questions (and quickly). I'm a pure math student, so my computational skills and speed aren't so great because I'm out of practice. I'm wondering what texts are best to prepare with. I know Stewart is commonly recommended for preparing for the calculus questions. My university used Calculus: Early Transcendentals by Edwards & Penney. I can get Stewart out of the library, so I was just wondering which would be a better resource (especially in terms of practice problems). Is Stewart sufficient for differential equations? Secondly, what are some good linear algebra books? I have Sheldon Axler's Linear Algebra Done Right, but this doesn't teach much in the way of computational skills, so I need something that would be good for that. I have also purchased the Princeton Review's Cracking the GRE Subject Test, which I've heard is not awesome but still worth having.","I'll be taking the Math GRE subject test in a few months. I know that it is best to focus on being able to answer all the calculus, DEs, and linear algebra questions (and quickly). I'm a pure math student, so my computational skills and speed aren't so great because I'm out of practice. I'm wondering what texts are best to prepare with. I know Stewart is commonly recommended for preparing for the calculus questions. My university used Calculus: Early Transcendentals by Edwards & Penney. I can get Stewart out of the library, so I was just wondering which would be a better resource (especially in terms of practice problems). Is Stewart sufficient for differential equations? Secondly, what are some good linear algebra books? I have Sheldon Axler's Linear Algebra Done Right, but this doesn't teach much in the way of computational skills, so I need something that would be good for that. I have also purchased the Princeton Review's Cracking the GRE Subject Test, which I've heard is not awesome but still worth having.",,"['calculus', 'linear-algebra', 'reference-request', 'book-recommendation']"
79,Maximum and minimum of an integral under integral constraints.,Maximum and minimum of an integral under integral constraints.,,"Find the maximum and minimum of the following integral in terms of $f(x),a,C$: \begin{align}I=\int_{0}^{a} \frac{x}{f(x)}p(x)dx  \end{align} s.t.: 1) $\int_{0}^{a} p(x)dx=1$ 2) $\int_{0}^{a} f(x)p(x)dx=C$ Notes: 1) $p(x)$ is an unknown probability density function. 2) $x,f(x),p(x)\geq 0 \;,\; C>0$ 3) $f(0)=0$ 4) $\lim_{x \to 0}\frac{x}{f(x)}=0$","Find the maximum and minimum of the following integral in terms of $f(x),a,C$: \begin{align}I=\int_{0}^{a} \frac{x}{f(x)}p(x)dx  \end{align} s.t.: 1) $\int_{0}^{a} p(x)dx=1$ 2) $\int_{0}^{a} f(x)p(x)dx=C$ Notes: 1) $p(x)$ is an unknown probability density function. 2) $x,f(x),p(x)\geq 0 \;,\; C>0$ 3) $f(0)=0$ 4) $\lim_{x \to 0}\frac{x}{f(x)}=0$",,"['calculus', 'probability-distributions', 'optimization', 'linear-programming', 'calculus-of-variations']"
80,Am I thinking about infinitesimals correctly?,Am I thinking about infinitesimals correctly?,,"I was all set to begin Calculus 2 when I thought, ""I should have a more intuitive sense of what's happening with differentials before I move on."" I want to tell you what I've learned and ask you all to help me put it together more cohesively than I have done.  I'm just going to list what I know.  Please point out my inconsistencies.  And please forgive me if the following seems to ramble, I'm just sharing my thought process and I want it to have more rigour. I've seen Qiaochu Yuan saying a few times that $dy$ and $dx$ are not numbers.  I didn't know what he meant.  So, I read elsewhere, and learned about infinitesimals.  I read that an infinitesimal is a concept similar to zero, but not.  It is a small, unmeasurable value.  That there is no value you can multiply an infinitesimal by to get any real number.  Yet, it's not zero.  I read that this is an important distinction. So, when I read $\frac{dy}{dx}$, this is not a fraction: it should be read as one symbol, and interpreted as shorthand for $\lim\limits_{\delta x \to 0}\frac{\delta y}{\delta x}$. A problem I still have, though, and it's surprising even to me that I don't get this yet, is this: if I'm finding the gradient of a curve at a point, then I'm finding the gradient of the tangent line at that point.  I do this by setting up the quotient $\frac{\delta y}{\delta x}$ where both $\delta y$ and $\delta x$ are measureable values, real numbers.  as soon as I write $\lim\limits_{\delta x \to 0}$ in front of the quotient, I'm saying that the quotient coupled with the limit is not a quotient.  So $\delta y$ and $\delta x$ are still real numbers, but $\frac{dy}{dx}$ is a symbol that means the tangent, the limit, or $f'(x)$. So two things I'm not sure of at this point:  it seems that if $\frac{dy}{dx}$ is a symbol in its own right, then $dy$ and $dx$ are not independent symbols.  And if they are, I don't know what they mean.  I will guess that $dy = \lim\limits_{\delta x \to 0}\delta y$ and $dx = \lim\limits_{\delta x \to 0}\delta x$.  To say that $dx$ is infinitesimally small is to say that it is infinitely close to zero but not zero. But, if two points are infinitely close to one another, where the distance between them is an infinitesimal, they are still two points aren't they?  They can only be the same point if the distance between them is zero, and an infinitesimal is not zero.  And so, how can I find the gradient of the tangent line when the tangent is really a secant? I know this is important to understand, and at some point I thought I understood it intuitively.  But my understanding has slipped and I'm here.  I can tell myself ""it just is"" but it's not much of an argument and it's not satisfying.  I want to understand everything as much as I'm capable of doing so, and take nothing on faith. Once I understand this, I will be able to write the chain rule without multiplying by infinitesimals, then the same with the substitution rule, then I move onto Calculus 2.  Until I understand this, I can't move on. I'm guessing my problem comes from a lack of rigor in some textbooks, and that makes my overall understanding weakened.  If I try to solve my own problem, I might say that $\frac{dy}{dx}$ is not, in fact, the gradient of the tangent at all.  It is the gradient of the secant, but it is the value that is infinitely close to the tangent. It ""approaches"" the tangent.  But it still comes down to this: if two values are not the same, and their difference is infinitely small, what does that mean?  Sometimes they are considered the same, sometimes they are not.  Which is it?  If something is infinitely small, it has a value greater than zero, but it's not zero, it can't accumulate to become any real number. It's not a number.  It's a concept with the above properties and that's all there is to it.  Is that the answer? I understand the idea is to be able to get infinitely close to a value while avoiding a division by zero.  And knowing that is almost enough for me to move on.  But I think there's more for me to understand here, and a lot rests on it. Thanks for reading this far, if you did.  I appreciate your feedback. Also: another question.  If I have $dx$ and $du$ and they are both infinitesimals, then $1+dx = 1+du$, right?  Yet, $du$ and $dx$ are not the same, right?  In an integral, I can't just switch out a $dx$ for a $du$, yet they both represent the same concept.","I was all set to begin Calculus 2 when I thought, ""I should have a more intuitive sense of what's happening with differentials before I move on."" I want to tell you what I've learned and ask you all to help me put it together more cohesively than I have done.  I'm just going to list what I know.  Please point out my inconsistencies.  And please forgive me if the following seems to ramble, I'm just sharing my thought process and I want it to have more rigour. I've seen Qiaochu Yuan saying a few times that $dy$ and $dx$ are not numbers.  I didn't know what he meant.  So, I read elsewhere, and learned about infinitesimals.  I read that an infinitesimal is a concept similar to zero, but not.  It is a small, unmeasurable value.  That there is no value you can multiply an infinitesimal by to get any real number.  Yet, it's not zero.  I read that this is an important distinction. So, when I read $\frac{dy}{dx}$, this is not a fraction: it should be read as one symbol, and interpreted as shorthand for $\lim\limits_{\delta x \to 0}\frac{\delta y}{\delta x}$. A problem I still have, though, and it's surprising even to me that I don't get this yet, is this: if I'm finding the gradient of a curve at a point, then I'm finding the gradient of the tangent line at that point.  I do this by setting up the quotient $\frac{\delta y}{\delta x}$ where both $\delta y$ and $\delta x$ are measureable values, real numbers.  as soon as I write $\lim\limits_{\delta x \to 0}$ in front of the quotient, I'm saying that the quotient coupled with the limit is not a quotient.  So $\delta y$ and $\delta x$ are still real numbers, but $\frac{dy}{dx}$ is a symbol that means the tangent, the limit, or $f'(x)$. So two things I'm not sure of at this point:  it seems that if $\frac{dy}{dx}$ is a symbol in its own right, then $dy$ and $dx$ are not independent symbols.  And if they are, I don't know what they mean.  I will guess that $dy = \lim\limits_{\delta x \to 0}\delta y$ and $dx = \lim\limits_{\delta x \to 0}\delta x$.  To say that $dx$ is infinitesimally small is to say that it is infinitely close to zero but not zero. But, if two points are infinitely close to one another, where the distance between them is an infinitesimal, they are still two points aren't they?  They can only be the same point if the distance between them is zero, and an infinitesimal is not zero.  And so, how can I find the gradient of the tangent line when the tangent is really a secant? I know this is important to understand, and at some point I thought I understood it intuitively.  But my understanding has slipped and I'm here.  I can tell myself ""it just is"" but it's not much of an argument and it's not satisfying.  I want to understand everything as much as I'm capable of doing so, and take nothing on faith. Once I understand this, I will be able to write the chain rule without multiplying by infinitesimals, then the same with the substitution rule, then I move onto Calculus 2.  Until I understand this, I can't move on. I'm guessing my problem comes from a lack of rigor in some textbooks, and that makes my overall understanding weakened.  If I try to solve my own problem, I might say that $\frac{dy}{dx}$ is not, in fact, the gradient of the tangent at all.  It is the gradient of the secant, but it is the value that is infinitely close to the tangent. It ""approaches"" the tangent.  But it still comes down to this: if two values are not the same, and their difference is infinitely small, what does that mean?  Sometimes they are considered the same, sometimes they are not.  Which is it?  If something is infinitely small, it has a value greater than zero, but it's not zero, it can't accumulate to become any real number. It's not a number.  It's a concept with the above properties and that's all there is to it.  Is that the answer? I understand the idea is to be able to get infinitely close to a value while avoiding a division by zero.  And knowing that is almost enough for me to move on.  But I think there's more for me to understand here, and a lot rests on it. Thanks for reading this far, if you did.  I appreciate your feedback. Also: another question.  If I have $dx$ and $du$ and they are both infinitesimals, then $1+dx = 1+du$, right?  Yet, $du$ and $dx$ are not the same, right?  In an integral, I can't just switch out a $dx$ for a $du$, yet they both represent the same concept.",,"['calculus', 'infinity']"
81,Show that $\lim_{x \to a}{P(x)} = P(a)$ for any polynomial $P(x)$,Show that  for any polynomial,\lim_{x \to a}{P(x)} = P(a) P(x),"In my calculus textbook I was given the following Problem: If $P(x)$ is a polynomial, show that $\lim_{x \to a}{P(x)} = P(a)$ . I found the following solution here , were a proof was given using induction. In my textbook, the problem was given in an chapter regarding limit laws, a precise definition is only given later in the following chapter and inductions weren't used up until then. So I decided to try solve this problem using only the limit laws. In that, I am taking these for granted without proving them. Solution: Per definition we have a polynomial $P(x) = \sum_{i=0}^n k_i x^i$ were each $k_i$ is a constant and $k, x \in \Bbb{R}$ . Thus $P(a) = \sum_{i=0}^n k_i a^i$ . Now, using the limit laws for sum, multiplication and power we can write $$ \lim_{x \to a}{P(x)}  = \lim_{x \to a}{\sum_{i=0}^n k_i x^i}  = \sum_{i=0}^n{\lim_{x \to a}k_i \cdot \left(\lim_{x \to a}x\right)^i}  = \sum_{i=0}^n{k_i \cdot \left(\lim_{x \to a}x\right)^i}$$ and since $\lim_{x \to a}x = a$ we get $$ \lim_{x \to a}{P(x)}  = \sum_{i=0}^n{k_i \cdot \left(\lim_{x \to a}x\right)^i}  = \sum_{i=0}^n{k_i a^i}  = P(a) $$ $\blacksquare$ Is my reasoning correct? I am not sure, if I can use these manipulations on the sum without using induction. Thanks in advance for any comments and answers.","In my calculus textbook I was given the following Problem: If is a polynomial, show that . I found the following solution here , were a proof was given using induction. In my textbook, the problem was given in an chapter regarding limit laws, a precise definition is only given later in the following chapter and inductions weren't used up until then. So I decided to try solve this problem using only the limit laws. In that, I am taking these for granted without proving them. Solution: Per definition we have a polynomial were each is a constant and . Thus . Now, using the limit laws for sum, multiplication and power we can write and since we get Is my reasoning correct? I am not sure, if I can use these manipulations on the sum without using induction. Thanks in advance for any comments and answers.","P(x) \lim_{x \to a}{P(x)} = P(a) P(x) = \sum_{i=0}^n k_i x^i k_i k, x \in \Bbb{R} P(a) = \sum_{i=0}^n k_i a^i 
\lim_{x \to a}{P(x)}
 = \lim_{x \to a}{\sum_{i=0}^n k_i x^i}
 = \sum_{i=0}^n{\lim_{x \to a}k_i \cdot \left(\lim_{x \to a}x\right)^i}
 = \sum_{i=0}^n{k_i \cdot \left(\lim_{x \to a}x\right)^i} \lim_{x \to a}x = a 
\lim_{x \to a}{P(x)}
 = \sum_{i=0}^n{k_i \cdot \left(\lim_{x \to a}x\right)^i}
 = \sum_{i=0}^n{k_i a^i}
 = P(a)
 \blacksquare","['calculus', 'limits', 'polynomials', 'solution-verification', 'continuity']"
82,Other Idea to show an inequality $\dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt n}\geq \sqrt n$,Other Idea to show an inequality,\dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt n}\geq \sqrt n,"$$\dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt n}\geq \sqrt n$$ I want to prove this by Induction  $$n=1 \checkmark\\ n=k \to \dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt k}\geq \sqrt k\\ n=k+1 \to \dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt {k+1}}\geq \sqrt {k+1}$$ so $$\dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt k}+\dfrac{1}{\sqrt {k+1}}\geq \sqrt k+\dfrac{1}{\sqrt {k+1}}$$now we prove that $$\sqrt k+\dfrac{1}{\sqrt {k+1}} >\sqrt{k+1} \\\sqrt{k(k+1)}+1 \geq k+1 \\ k(k+1) \geq k^2 \\k+1 \geq k \checkmark$$ and the second method like below , and I want to know is there more Idia to show this proof  ? forexample  combinatorics proofs  , or using integrals ,or fourier series ,.... Is there a close form for this summation ? any help will be appreciated .","$$\dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt n}\geq \sqrt n$$ I want to prove this by Induction  $$n=1 \checkmark\\ n=k \to \dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt k}\geq \sqrt k\\ n=k+1 \to \dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt {k+1}}\geq \sqrt {k+1}$$ so $$\dfrac{1}{\sqrt 1}+\dfrac{1}{\sqrt 2}+\dfrac{1}{\sqrt 3}+\cdots+\dfrac{1}{\sqrt k}+\dfrac{1}{\sqrt {k+1}}\geq \sqrt k+\dfrac{1}{\sqrt {k+1}}$$now we prove that $$\sqrt k+\dfrac{1}{\sqrt {k+1}} >\sqrt{k+1} \\\sqrt{k(k+1)}+1 \geq k+1 \\ k(k+1) \geq k^2 \\k+1 \geq k \checkmark$$ and the second method like below , and I want to know is there more Idia to show this proof  ? forexample  combinatorics proofs  , or using integrals ,or fourier series ,.... Is there a close form for this summation ? any help will be appreciated .",,"['calculus', 'discrete-mathematics', 'inequality', 'alternative-proof', 'big-list']"
83,What is the $1469^\text{th}$ derivative of $x^{532}-5x^{37}-4$?,What is the  derivative of ?,1469^\text{th} x^{532}-5x^{37}-4,I'm doing some basic calculus exercises on higher derivatives. But I'm stuck at a problem. The question is to find the 1469th derivative of $f(x)=x^{532}-5x^{37}-4$. I've read something about using the general Leibniz rule and a binomial but I can't get my head around it.,I'm doing some basic calculus exercises on higher derivatives. But I'm stuck at a problem. The question is to find the 1469th derivative of $f(x)=x^{532}-5x^{37}-4$. I've read something about using the general Leibniz rule and a binomial but I can't get my head around it.,,"['calculus', 'derivatives']"
84,Why is the integral defined as the limit of the sum $\int_a^b f(x) dx = \lim_{n\to\infty}\sum_{i=1}^n f(x_i^*)\Delta x$? [duplicate],Why is the integral defined as the limit of the sum ? [duplicate],\int_a^b f(x) dx = \lim_{n\to\infty}\sum_{i=1}^n f(x_i^*)\Delta x,"This question already has answers here : What's the ""limit"" in the definition of Riemann integrals? (5 answers) Closed 6 years ago . I am failing to understand why the integral is defined as: $$\int_a^b f(x) dx = \lim_{n\to\infty}\sum_{i=1}^n f(x_i^*)\Delta x$$ instead of: $$\int_a^b f(x)dx=\sum_{i=1}^\infty f(x_i^*)\Delta x$$ Is the former just popular preference or is there something I am not conceptually understanding here?","This question already has answers here : What's the ""limit"" in the definition of Riemann integrals? (5 answers) Closed 6 years ago . I am failing to understand why the integral is defined as: $$\int_a^b f(x) dx = \lim_{n\to\infty}\sum_{i=1}^n f(x_i^*)\Delta x$$ instead of: $$\int_a^b f(x)dx=\sum_{i=1}^\infty f(x_i^*)\Delta x$$ Is the former just popular preference or is there something I am not conceptually understanding here?",,"['calculus', 'integration', 'limits', 'riemann-integration']"
85,Evaluating $\int_{0}^{1} \sqrt{1+x^2} \text{ d}x$,Evaluating,\int_{0}^{1} \sqrt{1+x^2} \text{ d}x,I'm learning integral. Here is my homework: $$\int_0^1 \sqrt{1+x^2}\;dx$$ I think this problem solve by change $x$ to other variable. Can you tell me how please. (just direction how to solve) thanks :),I'm learning integral. Here is my homework: $$\int_0^1 \sqrt{1+x^2}\;dx$$ I think this problem solve by change $x$ to other variable. Can you tell me how please. (just direction how to solve) thanks :),,"['calculus', 'integration', 'definite-integrals']"
86,"Looking for functions $f$ with $\int_{-\infty}^{\infty}f(x)\,dx = 1$. [closed]",Looking for functions  with . [closed],"f \int_{-\infty}^{\infty}f(x)\,dx = 1",Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question I am looking for functions and/or constants that when being integrated from minus infinity to infinity produce 1. I think the Dirac delta function is one example but perhaps there are some more? References  on useful material is also greatly appreciated.,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question I am looking for functions and/or constants that when being integrated from minus infinity to infinity produce 1. I think the Dirac delta function is one example but perhaps there are some more? References  on useful material is also greatly appreciated.,,"['big-list', 'calculus', 'analysis']"
87,How to integrate $\int\frac{1}{1+\cos{x}}\ dx$?,How to integrate ?,\int\frac{1}{1+\cos{x}}\ dx,"I've looked at wolfram alpha for a possible solution, but it does something that makes no sense to me... It says to let $u=\tan(\frac{x}{2})$ with no clear reason as to why... So what would be a first step to approaching this? I've thought of converting it to $\frac{\sec{x}}{\sec{x}+1}$, but that seems to be fruitless...","I've looked at wolfram alpha for a possible solution, but it does something that makes no sense to me... It says to let $u=\tan(\frac{x}{2})$ with no clear reason as to why... So what would be a first step to approaching this? I've thought of converting it to $\frac{\sec{x}}{\sec{x}+1}$, but that seems to be fruitless...",,"['calculus', 'integration']"
88,Odd $\sin/\cos$ integral,Odd  integral,\sin/\cos,"How to evaluate $$\int \frac{\sin^3 x}{\cos^5x}dx\ ?$$ I've tried various substitutions with $\sin x = u$ or $\cos x = u$, I've tried using Euler's formula which result in too heavy calculations and I've tried using $\sin^2x + \cos^2x = 1$ in various forms without success.","How to evaluate $$\int \frac{\sin^3 x}{\cos^5x}dx\ ?$$ I've tried various substitutions with $\sin x = u$ or $\cos x = u$, I've tried using Euler's formula which result in too heavy calculations and I've tried using $\sin^2x + \cos^2x = 1$ in various forms without success.",,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals']"
89,Can the limit of a product exist if neither of its factors exist?,Can the limit of a product exist if neither of its factors exist?,,"Show an example where neither $\lim\limits_{x\to c} f(x)$ or $\lim\limits_{x\to c} g(x)$ exists but $\lim\limits_{x\to c} f(x)g(x)$ exists. Sorry if this seems elementary, I have just started my degree... Thanks in advance.","Show an example where neither $\lim\limits_{x\to c} f(x)$ or $\lim\limits_{x\to c} g(x)$ exists but $\lim\limits_{x\to c} f(x)g(x)$ exists. Sorry if this seems elementary, I have just started my degree... Thanks in advance.",,"['calculus', 'limits', 'products']"
90,Why aren't all functions considered a power function of 1 and integrated using some sort of chain rule?,Why aren't all functions considered a power function of 1 and integrated using some sort of chain rule?,,"When integrating, say, $f(x) = x^{3} - 2x$, why do we go straight to integrating each term independently? Why is it not considered an implicit function of the power one and integrated as so? I'm not sure if that's a valid feat since I'm a beginner, but I was thinking: $\int \left [ f(x) \right ] ^{^{1}} dx$ would be $\frac{f(x)^{2}}{2}$ as another example to the usual: $\int f(x)^{n} dx  = \frac{f(x)^{n+1}}{n+1}$ where $n \neq -1 $. As for what's inside, the $f(x)$, as I said I'm a beginner, but I know of u-substitution and maybe we could use it and work it out from there? Is that possible or just plain wrong?","When integrating, say, $f(x) = x^{3} - 2x$, why do we go straight to integrating each term independently? Why is it not considered an implicit function of the power one and integrated as so? I'm not sure if that's a valid feat since I'm a beginner, but I was thinking: $\int \left [ f(x) \right ] ^{^{1}} dx$ would be $\frac{f(x)^{2}}{2}$ as another example to the usual: $\int f(x)^{n} dx  = \frac{f(x)^{n+1}}{n+1}$ where $n \neq -1 $. As for what's inside, the $f(x)$, as I said I'm a beginner, but I know of u-substitution and maybe we could use it and work it out from there? Is that possible or just plain wrong?",,"['calculus', 'integration']"
91,Convergence of infinite sum (factorial and root),Convergence of infinite sum (factorial and root),,"Find the convergence of the following sequence: $$\sum_{n=1}^{\infty}\left(\dfrac{n!}{(2n)!}\right)^{\frac{1}{n}}$$ I tried to use the ratio test. It wasn't useful. Also I used the logharitmic test (I'm not sure this is the standard name for it): $\lim_{n\to\infty}\dfrac{\ln\frac{1}{u_n}}{\ln n}=l$. If $l>1$,     $\sum_{n=1}^{\infty}u_n$ converges. It didn't work to well. Thanks for your help!","Find the convergence of the following sequence: $$\sum_{n=1}^{\infty}\left(\dfrac{n!}{(2n)!}\right)^{\frac{1}{n}}$$ I tried to use the ratio test. It wasn't useful. Also I used the logharitmic test (I'm not sure this is the standard name for it): $\lim_{n\to\infty}\dfrac{\ln\frac{1}{u_n}}{\ln n}=l$. If $l>1$,     $\sum_{n=1}^{\infty}u_n$ converges. It didn't work to well. Thanks for your help!",,"['calculus', 'sequences-and-series', 'logarithms']"
92,"If $f(x) = \cos x$, explain, without taking the derivative, how you would find the $f^{(99)}(x)$?","If , explain, without taking the derivative, how you would find the ?",f(x) = \cos x f^{(99)}(x),My theory: derivative of $\cos x = - \sin x $ derivative of $-\sin x = -\cos x $ derivative of $-\cos x = \sin x.$ cycle occurs three times but then what do you do?? Is there a good way to solve this?,My theory: derivative of $\cos x = - \sin x $ derivative of $-\sin x = -\cos x $ derivative of $-\cos x = \sin x.$ cycle occurs three times but then what do you do?? Is there a good way to solve this?,,"['calculus', 'trigonometry']"
93,is sin(x) / sin(x) = 1?,is sin(x) / sin(x) = 1?,,"I tested this in python using: import numpy as np import matplotlib.pyplot as plt  x = np.linspace(0, 10*2*np.pi, 10000) y = np.sin(x) plt.plot(y/y) plt.plot(y) Which produces: The blue line representing sin(x)/sin(x) appears to be y=1 However, I don't know if the values at the point where sin(x) crosses the x-axis really equals 1, 0, infinity or just undefined.","I tested this in python using: import numpy as np import matplotlib.pyplot as plt  x = np.linspace(0, 10*2*np.pi, 10000) y = np.sin(x) plt.plot(y/y) plt.plot(y) Which produces: The blue line representing sin(x)/sin(x) appears to be y=1 However, I don't know if the values at the point where sin(x) crosses the x-axis really equals 1, 0, infinity or just undefined.",,"['calculus', 'algebra-precalculus', 'discrete-mathematics']"
94,Is it possible to prove that the polynomial $x^4 - 2x^3 + 16$ doesn't have a real zero by writing it as a sum of squares?,Is it possible to prove that the polynomial  doesn't have a real zero by writing it as a sum of squares?,x^4 - 2x^3 + 16,"To be able to apply the argument principle to a certain problem, I have to prove that the polynomial $x^4-2x^3+16$ doesn't have a real zero. I think that I can do this by a non-satisfactory, derivative technique: The value of the polynomial seems to go to plus infinity as x approaches plus or minus infinity. Therefore there must be a globally minimal value at a critical point. Critical points are 0 and 3/2, where the value is positive. Therefore the polynomial can't have a real zero. A more satisfactory proof would be writing the polynomial as a sum of squares, but that seems difficult. I've tried to get rid of the disturbing $-2x^3$ term by writing the polynomial as $(x^2-x)^2 - x^2 + 16$ , but then an equally disturbing $-x^2$ term poppes up. What to do?","To be able to apply the argument principle to a certain problem, I have to prove that the polynomial doesn't have a real zero. I think that I can do this by a non-satisfactory, derivative technique: The value of the polynomial seems to go to plus infinity as x approaches plus or minus infinity. Therefore there must be a globally minimal value at a critical point. Critical points are 0 and 3/2, where the value is positive. Therefore the polynomial can't have a real zero. A more satisfactory proof would be writing the polynomial as a sum of squares, but that seems difficult. I've tried to get rid of the disturbing term by writing the polynomial as , but then an equally disturbing term poppes up. What to do?",x^4-2x^3+16 -2x^3 (x^2-x)^2 - x^2 + 16 -x^2,"['calculus', 'polynomials']"
95,How do you create an alternating series with the sign being the same twice in a row?,How do you create an alternating series with the sign being the same twice in a row?,,"I am working on a Taylor series question and I have created a series which alternates however, it does so in doubles. in other words it follows the following pattern: $x$, $x$, $-x$, $-x$, $x$, $x$,... My goal is to write this series using summation notation.  I know that to produce a normal alternative series I simply use $(-1)^n$. However, I've found this significantly harder to create in summation notation because any manipulation to the $(-1)^n$ part of the sum only moves the position where the alternating series starts. Any suggestions are appreciated! edit: please note that in my example, ""$x$"" is just an arbitrary function and each other $x$ does not equal the other $x$'s.","I am working on a Taylor series question and I have created a series which alternates however, it does so in doubles. in other words it follows the following pattern: $x$, $x$, $-x$, $-x$, $x$, $x$,... My goal is to write this series using summation notation.  I know that to produce a normal alternative series I simply use $(-1)^n$. However, I've found this significantly harder to create in summation notation because any manipulation to the $(-1)^n$ part of the sum only moves the position where the alternating series starts. Any suggestions are appreciated! edit: please note that in my example, ""$x$"" is just an arbitrary function and each other $x$ does not equal the other $x$'s.",,"['calculus', 'sequences-and-series', 'power-series', 'taylor-expansion']"
96,Why $e^x$ never equal $x$?,Why  never equal ?,e^x x,"Je veux savoir pourquoi $x=e^x$ n'a aucune solution dans $\Bbb R$. Lorsque j'ai essayé de tracer le graphe de la fonction $e^x$, j'ai trouvé en fait qu'elle est une fonction strictement croissante mais je ne sais pas quoi faire après. Merci mon ami. I would like to know why $x=e^x$ has no solution in $\Bbb R$? I tried to plot the function $e^x$ and I found that it is an increasing function but I do not know what to do next.","Je veux savoir pourquoi $x=e^x$ n'a aucune solution dans $\Bbb R$. Lorsque j'ai essayé de tracer le graphe de la fonction $e^x$, j'ai trouvé en fait qu'elle est une fonction strictement croissante mais je ne sais pas quoi faire après. Merci mon ami. I would like to know why $x=e^x$ has no solution in $\Bbb R$? I tried to plot the function $e^x$ and I found that it is an increasing function but I do not know what to do next.",,"['calculus', 'exponential-function', 'fixed-point-theorems']"
97,Find formula of sum $\sin (nx)$ [duplicate],Find formula of sum  [duplicate],\sin (nx),This question already has answers here : How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? (8 answers) Closed 9 years ago . I wonder if there is a way to calculate the $$S_n=\sin x + \sin 2x + … + \sin nx$$ but using only derivatives ?,This question already has answers here : How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? (8 answers) Closed 9 years ago . I wonder if there is a way to calculate the $$S_n=\sin x + \sin 2x + … + \sin nx$$ but using only derivatives ?,,"['calculus', 'complex-numbers']"
98,"Different methods, different answers.","Different methods, different answers.",,"If $\int_{\pi/2}^\theta\sin x\,dx=\sin2\theta$, then the value of $\theta$ satisfying $0<\theta<\pi$, is (a) $3\pi/2$ (b) $\pi/6$ (c) $5\pi/6$ (d) $\pi/2$ Method 1: I apply Leibniz rule and differentiate both the sides with respect to $\theta$. No option seems to be correct. Method 2: I integrate the left hand side and get to a conclusion of option d). Now which method to follow?","If $\int_{\pi/2}^\theta\sin x\,dx=\sin2\theta$, then the value of $\theta$ satisfying $0<\theta<\pi$, is (a) $3\pi/2$ (b) $\pi/6$ (c) $5\pi/6$ (d) $\pi/2$ Method 1: I apply Leibniz rule and differentiate both the sides with respect to $\theta$. No option seems to be correct. Method 2: I integrate the left hand side and get to a conclusion of option d). Now which method to follow?",,['calculus']
99,How to find the derivative of this function?,How to find the derivative of this function?,,"$$f(x) = \int_0^{\cos x}t^2 \, dt$$ I computed and got $f'(x) = - \cos^2(x) \sin(x)$. I checked the answer in the book and I got this wrong. But I think I did this correctly. I used part 1 of FTC to do this. Any ideas?","$$f(x) = \int_0^{\cos x}t^2 \, dt$$ I computed and got $f'(x) = - \cos^2(x) \sin(x)$. I checked the answer in the book and I got this wrong. But I think I did this correctly. I used part 1 of FTC to do this. Any ideas?",,"['calculus', 'derivatives']"
