,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Justify, without evaluating, that the determinant of the following matrix is zero","Justify, without evaluating, that the determinant of the following matrix is zero",,"I am currently stuck at this question and have no idea how to solve. I just started out learning linear and I'm really weak in this field. Justify, without evaluating, that the determinant of the following matrix is zero Here's the matrix A: $$\begin{pmatrix}      1 &  0  &  2 &  4\\     -2 & 3 &   8 &  6\\     -1 &  3 &  10 & 10\\      6 &  6 &  -3 &  7\\ \end{pmatrix}$$ I searched online but couldn't find something similar. What I found though was that if it was skew-symmetric ($A^t= -A$) then the determinant could directly be said to be equal to zero. But in this case it didn't work with me. Thank you.","I am currently stuck at this question and have no idea how to solve. I just started out learning linear and I'm really weak in this field. Justify, without evaluating, that the determinant of the following matrix is zero Here's the matrix A: $$\begin{pmatrix}      1 &  0  &  2 &  4\\     -2 & 3 &   8 &  6\\     -1 &  3 &  10 & 10\\      6 &  6 &  -3 &  7\\ \end{pmatrix}$$ I searched online but couldn't find something similar. What I found though was that if it was skew-symmetric ($A^t= -A$) then the determinant could directly be said to be equal to zero. But in this case it didn't work with me. Thank you.",,"['linear-algebra', 'matrices', 'determinant', 'inverse']"
1,Proving Eigenvalue squared is Eigenvalue of $A^2$,Proving Eigenvalue squared is Eigenvalue of,A^2,"The question is: Prove that if $\lambda$ is an eigenvalue of a matrix A with corresponding eigenvector x , then $\lambda^2$ is an eigenvalue of $A^2$ with corresponding eigenvector x . I assume I need to start with the equation $Ax=\lambda x$ and end up with $A^2 x=\lambda^2 x$ but between those I am kind of lost. I have manipulated the equations several different ways and just can't seem to end up where I need to be. Help would be greatly appreciated as I believe this will be on a test tomorrow.","The question is: Prove that if $\lambda$ is an eigenvalue of a matrix A with corresponding eigenvector x , then $\lambda^2$ is an eigenvalue of $A^2$ with corresponding eigenvector x . I assume I need to start with the equation $Ax=\lambda x$ and end up with $A^2 x=\lambda^2 x$ but between those I am kind of lost. I have manipulated the equations several different ways and just can't seem to end up where I need to be. Help would be greatly appreciated as I believe this will be on a test tomorrow.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
2,What are the eigenvalues of matrix that have all elements equal 1? [duplicate],What are the eigenvalues of matrix that have all elements equal 1? [duplicate],,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . As in subject: given a matrix $A$ of size $n$ with all elements equal exactly 1. What are the eigenvalues of that matrix ?","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . As in subject: given a matrix $A$ of size $n$ with all elements equal exactly 1. What are the eigenvalues of that matrix ?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
3,How do I find a dual basis given the following basis?,How do I find a dual basis given the following basis?,,"$V = \Bbb{R}^3$ and has basis $\mathcal{B} = \{\vec{e_1}-\vec{e_2},\vec{e_1}+\vec{e_2},\vec{e_3}\}$ How do I find the dual basis? This is not homework, but an example that I am struggling to grasp. This is a simple question, so I would really appreciate if you wouldn't skip any details no matter the triviality as there may be fundamental gaps in my understanding.","$V = \Bbb{R}^3$ and has basis $\mathcal{B} = \{\vec{e_1}-\vec{e_2},\vec{e_1}+\vec{e_2},\vec{e_3}\}$ How do I find the dual basis? This is not homework, but an example that I am struggling to grasp. This is a simple question, so I would really appreciate if you wouldn't skip any details no matter the triviality as there may be fundamental gaps in my understanding.",,"['linear-algebra', 'matrices', 'vector-spaces', 'vectors', 'linear-transformations']"
4,Why is it true that $\mathrm{adj}(A)A = \det(A) \cdot I$?,Why is it true that ?,\mathrm{adj}(A)A = \det(A) \cdot I,"This is a statement in linear algebra that I can't seem to understand the proof behind. For a square matrix $A$, why is: $$\mathrm{adj}(A)A = \det(A) \cdot I$$ Any explanation would be greatly appreciated.","This is a statement in linear algebra that I can't seem to understand the proof behind. For a square matrix $A$, why is: $$\mathrm{adj}(A)A = \det(A) \cdot I$$ Any explanation would be greatly appreciated.",,"['linear-algebra', 'determinant']"
5,Is the trace of the product of two positive semidefinite matrices always nonnegative?,Is the trace of the product of two positive semidefinite matrices always nonnegative?,,"Is $\mbox{tr}(XY) \geq 0$ for all $X, Y \in \Bbb S_+$ ?",Is for all ?,"\mbox{tr}(XY) \geq 0 X, Y \in \Bbb S_+","['linear-algebra', 'matrices', 'trace', 'positive-semidefinite']"
6,Diagonalization of a projection,Diagonalization of a projection,,"If I have a projection $T$ on a finite dimensional vector space $V$, how do I show that $T$ is diagonalizable?","If I have a projection $T$ on a finite dimensional vector space $V$, how do I show that $T$ is diagonalizable?",,"['linear-algebra', 'diagonalization', 'projection-matrices']"
7,What's the point of orthogonal diagonalisation?,What's the point of orthogonal diagonalisation?,,"I've learned the process of orthogonal diagonalisation in an algebra course I'm taking...but I just realised I have no idea what the point of it is. The definition is basically this: ""A matrix $A$ is orthogonally diagonalisable if there exists a matrix $P$ which is orthogonal and $D = P^tAP$ where $D$ is diagonal"". I don't understand the significance of this though...what is special/important about this relationship?","I've learned the process of orthogonal diagonalisation in an algebra course I'm taking...but I just realised I have no idea what the point of it is. The definition is basically this: ""A matrix is orthogonally diagonalisable if there exists a matrix which is orthogonal and where is diagonal"". I don't understand the significance of this though...what is special/important about this relationship?",A P D = P^tAP D,"['linear-algebra', 'matrices']"
8,"Dot product vs Matrix multiplication, is the later a special case of the first?","Dot product vs Matrix multiplication, is the later a special case of the first?",,"I seemed to have thoroughly confused myself today... Long story short, the question is simple. Is matrix multiplication just a special case of the dot product of two sets of vectors when the sets of vectors have the same cardinality and all vectors in both sets have the same length? I assume the answer is yes from reviewing the computation of matrix multiplication and the dot product .","I seemed to have thoroughly confused myself today... Long story short, the question is simple. Is matrix multiplication just a special case of the dot product of two sets of vectors when the sets of vectors have the same cardinality and all vectors in both sets have the same length? I assume the answer is yes from reviewing the computation of matrix multiplication and the dot product .",,"['linear-algebra', 'matrices', 'inner-products']"
9,Linear span of the empty set,Linear span of the empty set,,"Why is the span of the empty set defined to be $\{0\}$? It is known that the span of any nonempty set of vectors in a vector space $V$, gives a subspace of $V$, and it is stated in “Linear Algebra Done Right” by Axler, that to be consistent with this, the span of the empty set is defined to be $\{0\}$? Is this the only reason or does this definition prove useful in other ways later on?","Why is the span of the empty set defined to be $\{0\}$? It is known that the span of any nonempty set of vectors in a vector space $V$, gives a subspace of $V$, and it is stated in “Linear Algebra Done Right” by Axler, that to be consistent with this, the span of the empty set is defined to be $\{0\}$? Is this the only reason or does this definition prove useful in other ways later on?",,['linear-algebra']
10,What does shear mean?,What does shear mean?,,"As I understand it, the gradient of a vector field can be decomposed into parts that relate to the divergence, curl, and shear of the function.  I understand what divergence and curl are (both computationally and geometrically), but what does shear mean in this context?  Is it related to the shear mapping that is used in Linear Algebra?  What does it really mean and how do we use it?","As I understand it, the gradient of a vector field can be decomposed into parts that relate to the divergence, curl, and shear of the function.  I understand what divergence and curl are (both computationally and geometrically), but what does shear mean in this context?  Is it related to the shear mapping that is used in Linear Algebra?  What does it really mean and how do we use it?",,"['linear-algebra', 'multivariable-calculus', 'tensors']"
11,Importance of eigenvalues,Importance of eigenvalues,,I know how to find eigenvalues and eigenvectors.  But I don't know what to do with that. What is their use?  Can anyone explain it to me?,I know how to find eigenvalues and eigenvectors.  But I don't know what to do with that. What is their use?  Can anyone explain it to me?,,"['linear-algebra', 'soft-question', 'eigenvalues-eigenvectors']"
12,Is there a geometric meaning of the Frobenius norm?,Is there a geometric meaning of the Frobenius norm?,,"I have a positive definite matrix $A$. I am going to choose its Frobenius norm $\|A\|_F^2$ as a cost function and then minimize $\|A\|_F^2$. But I think I need to find a reason to convince people it is reasonable to choose $\|A\|_F^2$ as a cost function. So I'm wondering if there are some geometric meanings of the Frobenius norm. Thanks. Edit : here $A$ is a 3 by 3 matrix. In the problem I'm working on, people usually choose $\det A$ as a cost function since $\det A$ has an obvious geometric interpretation: the volume of the parallelepiped determined by $A$. Now I want to choose $\|A\|_F^2$ as a cost function because of the good properties of $\|A\|_F^2$. That's why I am interested in the geometric meaning of $\|A\|_F^2$.","I have a positive definite matrix $A$. I am going to choose its Frobenius norm $\|A\|_F^2$ as a cost function and then minimize $\|A\|_F^2$. But I think I need to find a reason to convince people it is reasonable to choose $\|A\|_F^2$ as a cost function. So I'm wondering if there are some geometric meanings of the Frobenius norm. Thanks. Edit : here $A$ is a 3 by 3 matrix. In the problem I'm working on, people usually choose $\det A$ as a cost function since $\det A$ has an obvious geometric interpretation: the volume of the parallelepiped determined by $A$. Now I want to choose $\|A\|_F^2$ as a cost function because of the good properties of $\|A\|_F^2$. That's why I am interested in the geometric meaning of $\|A\|_F^2$.",,"['linear-algebra', 'geometry', 'matrices', 'normed-spaces']"
13,Proving that the matrix exponential map is surjective onto the general linear group,Proving that the matrix exponential map is surjective onto the general linear group,,"Let $M_n(\mathbb{F})$ be the set of all $n\times n$ with entries in $\mathbb{F}$ and let $\exp:M_n(\mathbb{C})\to M_n(\mathbb{C})$ be defined by  $$ \exp(A)=\sum_{k=0}^{\infty}\frac{A^k}{k!},$$ for all $A\in M_n(\mathbb{C}).$ I want to prove that $\exp$ is a surjective map from $M_n(\mathbb{C})$ to $GL(n,\mathbb{C})=\left\{A\in M_n(\mathbb{C})\,\middle| \det(A)\neq0\right\}$, how do I go about that? I mean saying that $\exp:M_n(\mathbb{R})\to GL(n,\mathbb{R})$ is an analogous to saying $\exp:\mathbb{R}\to \mathbb{R}_{>0}$ and this is also pretty intuitive, since, in analogy with the case of numbers, $A^0=I\;\forall A$, so $\exp(0)=I+0+\frac{0^2}{2!}+\dots=I$, so even for $A=0$ we get $\det\left(\exp(A)\right)\neq0$ and so because of the first term we can never get a zero determinant. But I have no idea how to prove the subjectiveness.  Thanks in advance.","Let $M_n(\mathbb{F})$ be the set of all $n\times n$ with entries in $\mathbb{F}$ and let $\exp:M_n(\mathbb{C})\to M_n(\mathbb{C})$ be defined by  $$ \exp(A)=\sum_{k=0}^{\infty}\frac{A^k}{k!},$$ for all $A\in M_n(\mathbb{C}).$ I want to prove that $\exp$ is a surjective map from $M_n(\mathbb{C})$ to $GL(n,\mathbb{C})=\left\{A\in M_n(\mathbb{C})\,\middle| \det(A)\neq0\right\}$, how do I go about that? I mean saying that $\exp:M_n(\mathbb{R})\to GL(n,\mathbb{R})$ is an analogous to saying $\exp:\mathbb{R}\to \mathbb{R}_{>0}$ and this is also pretty intuitive, since, in analogy with the case of numbers, $A^0=I\;\forall A$, so $\exp(0)=I+0+\frac{0^2}{2!}+\dots=I$, so even for $A=0$ we get $\det\left(\exp(A)\right)\neq0$ and so because of the first term we can never get a zero determinant. But I have no idea how to prove the subjectiveness.  Thanks in advance.",,"['linear-algebra', 'group-theory', 'exponential-function']"
14,A problem on condition $\det(A+B)=\det(A)+\det(B)$,A problem on condition,\det(A+B)=\det(A)+\det(B),Let $A$ be a matrix $n\times n$ matrix such that for any matrix $B$ we have $\det(A+B)=\det(A)+\det(B)$. Does this imply that $A=0$? or $\det(A)=0$?,Let $A$ be a matrix $n\times n$ matrix such that for any matrix $B$ we have $\det(A+B)=\det(A)+\det(B)$. Does this imply that $A=0$? or $\det(A)=0$?,,"['linear-algebra', 'matrices', 'determinant']"
15,How does one show a matrix is irreducible and reducible?,How does one show a matrix is irreducible and reducible?,,How does one show a matrix is irreducible and reducible? An example would also be great. I know that a matrix is reducible if and only if it can be placed into block upper-triangular form. How do you find block upper-triangular form?,How does one show a matrix is irreducible and reducible? An example would also be great. I know that a matrix is reducible if and only if it can be placed into block upper-triangular form. How do you find block upper-triangular form?,,"['linear-algebra', 'matrices']"
16,Why is inverse of orthogonal matrix is its transpose?,Why is inverse of orthogonal matrix is its transpose?,,So the question is in the title. It's easy to prove when we know that there are real numbers in it and the dot product is standard. But why this works in the general case - when there are complex numbers inside and the dot product is something else?,So the question is in the title. It's easy to prove when we know that there are real numbers in it and the dot product is standard. But why this works in the general case - when there are complex numbers inside and the dot product is something else?,,"['linear-algebra', 'matrices', 'orthogonal-matrices']"
17,What is the relationship between the null space and the column space?,What is the relationship between the null space and the column space?,,"Just looking at some tutorial videos, I'm noticing somewhat of a trend... but it wasn't spelled out explicitly, so I'd like to verify if this theory of mine is correct... (forgive my horribly un-exact notation...) Given $m\in\mathbb{N}$ vectors in $\mathbb{R}^n$, the span of those vectors is $$a_1\vec{v}_1+a_2\vec{v}_2+a_3\vec{v}_3+a_4\vec{v}_4+\cdots+a_m\vec{v}_m$$ If the null space of those vectors is a span of just $p\in\mathbb{N}$ of the $m$ vectors, does that mean that the column space is a span of the other $m-p$ vectors? Basically, is it true that the column space ""added"" to the null space gives all of the original vectors?","Just looking at some tutorial videos, I'm noticing somewhat of a trend... but it wasn't spelled out explicitly, so I'd like to verify if this theory of mine is correct... (forgive my horribly un-exact notation...) Given $m\in\mathbb{N}$ vectors in $\mathbb{R}^n$, the span of those vectors is $$a_1\vec{v}_1+a_2\vec{v}_2+a_3\vec{v}_3+a_4\vec{v}_4+\cdots+a_m\vec{v}_m$$ If the null space of those vectors is a span of just $p\in\mathbb{N}$ of the $m$ vectors, does that mean that the column space is a span of the other $m-p$ vectors? Basically, is it true that the column space ""added"" to the null space gives all of the original vectors?",,['linear-algebra']
18,Is the proof of this lemma really necessary?,Is the proof of this lemma really necessary?,,"To prove the Cayley-Hamilton theorem in linear algebra, my professor said that a lemma was necessary: Lemma : Let $A \in M_n(\mathbb{K})$ be an $n\times n$ matrix over a field $\mathbb{K}$, let $b(t) \in M_n(\mathbb{K})[t]$ and $P(t) = b(t)[A-tI]$, then $P(A) = 0$ The theorem (which says that if $f$ is an endomorphism of V, then $f$ is a solution to its characteristic polynomial), was then proven thus: let $B(t) = \text{adj}[A-tI]$ and $P(t) = B(t)[A-tI]$, then $P(A)=0$ but also $P(t) = \delta I$ (where $\delta = \det(A-tI)$). Since $\delta = \chi_f(t)$, so $P(A) = 0 \Rightarrow \chi_f(A) = 0$. My question is: since we interpret the $P(t)$ of the theorem as a polynomial with matrix coefficients, isn't the whole thing kind of obvious for the properties of a polynomial ring? (Assuming we all know how to switch between matrices and endomorphisms)","To prove the Cayley-Hamilton theorem in linear algebra, my professor said that a lemma was necessary: Lemma : Let $A \in M_n(\mathbb{K})$ be an $n\times n$ matrix over a field $\mathbb{K}$, let $b(t) \in M_n(\mathbb{K})[t]$ and $P(t) = b(t)[A-tI]$, then $P(A) = 0$ The theorem (which says that if $f$ is an endomorphism of V, then $f$ is a solution to its characteristic polynomial), was then proven thus: let $B(t) = \text{adj}[A-tI]$ and $P(t) = B(t)[A-tI]$, then $P(A)=0$ but also $P(t) = \delta I$ (where $\delta = \det(A-tI)$). Since $\delta = \chi_f(t)$, so $P(A) = 0 \Rightarrow \chi_f(A) = 0$. My question is: since we interpret the $P(t)$ of the theorem as a polynomial with matrix coefficients, isn't the whole thing kind of obvious for the properties of a polynomial ring? (Assuming we all know how to switch between matrices and endomorphisms)",,['linear-algebra']
19,Row swap changing sign of determinant,Row swap changing sign of determinant,,"I was wondering if someone could help me clarify something regarding the effect of swapping two rows on the sign of the determinant. I know that if $A$ is an $n\times n$ matrix and $B$ is an $n\times n$ matrix obtained from $A$ by swapping two rows, then $$\det(B)=-\det(A)$$ but I don't know how to prove this. I have been looking for proofs at internet, and read in both in textbooks and lectures notes that are available that this result is very hard to prove and most approaches rely on induction and so was wondering if there is something wrong with using that $\det(AB)=\det(A)\det(B)$ and then writing $B=EA$ where $E$ is an elementary matrix swapping two rows and using this result to get $\det(B)=\det(E)\det(A)=-\det(A)$ (since showing that $\det(E)=-1$ in this case is not that hard).","I was wondering if someone could help me clarify something regarding the effect of swapping two rows on the sign of the determinant. I know that if is an matrix and is an matrix obtained from by swapping two rows, then but I don't know how to prove this. I have been looking for proofs at internet, and read in both in textbooks and lectures notes that are available that this result is very hard to prove and most approaches rely on induction and so was wondering if there is something wrong with using that and then writing where is an elementary matrix swapping two rows and using this result to get (since showing that in this case is not that hard).",A n\times n B n\times n A \det(B)=-\det(A) \det(AB)=\det(A)\det(B) B=EA E \det(B)=\det(E)\det(A)=-\det(A) \det(E)=-1,"['linear-algebra', 'matrices', 'permutations', 'determinant', 'permutation-matrices']"
20,Find the inverse of a submatrix of a given matrix,Find the inverse of a submatrix of a given matrix,,"I have a $A$ matrix $4 \times 4$. By delete the first row and first column of $A$, we have a matrix $B$ with sizes $3 \times 3$. Assume that I have the result of invertible A that denote $A^{-1}$ before. I want to use the result to calculate the inverse of $B.$ Is it possible? Have any method to do it? Thanks Let consider a simple case that A is invertible then B also invertible.","I have a $A$ matrix $4 \times 4$. By delete the first row and first column of $A$, we have a matrix $B$ with sizes $3 \times 3$. Assume that I have the result of invertible A that denote $A^{-1}$ before. I want to use the result to calculate the inverse of $B.$ Is it possible? Have any method to do it? Thanks Let consider a simple case that A is invertible then B also invertible.",,"['linear-algebra', 'matrices', 'algorithms', 'matrix-equations', 'matrix-decomposition']"
21,Linear dependency of polynomials question,Linear dependency of polynomials question,,"I have to determine whether the polynomials $p_1(x)=2-x^2$, $p_2(x)=3x$, $p_3(x)= x^2 +x-2$ are linearly dependent or independent but I am not sure how to start. Anyone care to enlighten me? Also I have to find out if it spans $P^{(2)}$.","I have to determine whether the polynomials $p_1(x)=2-x^2$, $p_2(x)=3x$, $p_3(x)= x^2 +x-2$ are linearly dependent or independent but I am not sure how to start. Anyone care to enlighten me? Also I have to find out if it spans $P^{(2)}$.",,['linear-algebra']
22,Is null vector always linearly dependent?,Is null vector always linearly dependent?,,"I'm trying to find the column space of $\begin{bmatrix}a&0\\b&0\\c&0\end{bmatrix}$, which I think is $span\left(~\begin{bmatrix}a\\b\\c\end{bmatrix}~\begin{bmatrix}0\\0\\0\end{bmatrix}~\right)$.  Since by definition a span needs to include the null vector, this is redundant.  Would it also be correct to say that this is redundant because $\begin{bmatrix}0\\0\\0\end{bmatrix}$ is linearly dependent on $\begin{bmatrix}a\\b\\c\end{bmatrix}$ (can multiply it by a scalar of 0 to reach the null vector)?  Or is that a misapplication of the concept of linear dependence?","I'm trying to find the column space of $\begin{bmatrix}a&0\\b&0\\c&0\end{bmatrix}$, which I think is $span\left(~\begin{bmatrix}a\\b\\c\end{bmatrix}~\begin{bmatrix}0\\0\\0\end{bmatrix}~\right)$.  Since by definition a span needs to include the null vector, this is redundant.  Would it also be correct to say that this is redundant because $\begin{bmatrix}0\\0\\0\end{bmatrix}$ is linearly dependent on $\begin{bmatrix}a\\b\\c\end{bmatrix}$ (can multiply it by a scalar of 0 to reach the null vector)?  Or is that a misapplication of the concept of linear dependence?",,"['linear-algebra', 'matrices', 'vector-spaces']"
23,Why is anti-symmetry a desirable quality in determinants?,Why is anti-symmetry a desirable quality in determinants?,,"I hear the determinant of matrix can be defined using 3 facts. 1. It is multilinear. 2. It is anti-symmetric. 3. It is scaled so the determinant of the identity is 1. But, I don't understand why anti-symmetric is there? Why do people want determinants to be anti-symmetric?","I hear the determinant of matrix can be defined using 3 facts. 1. It is multilinear. 2. It is anti-symmetric. 3. It is scaled so the determinant of the identity is 1. But, I don't understand why anti-symmetric is there? Why do people want determinants to be anti-symmetric?",,['linear-algebra']
24,Every integer vector in $\mathbb R^n$ with integer length is part of an orthogonal basis of $\mathbb R^n$,Every integer vector in  with integer length is part of an orthogonal basis of,\mathbb R^n \mathbb R^n,"Suppose $\vec x$ is a (non-zero) vector with integer coordinates in $\mathbb R^n$ such that $\|\vec x\| \in \mathbb Z$. Is it true that there is an orthogonal basis of $\mathbb R^n$ containing $\vec x$, consisting of vectors with integer coordinates, all with the same length? For example: let $\vec x = \left<2,10,11\right>$, so $\|\vec x\| = 15$. Then the vectors $\left<14,-5,2\right>$ and $\left<5,10,-10\right>$ complete a basis of $\mathbb R^3$. I've checked all such integer vectors in $\mathbb R^3$ with an integer length up to 17 and found no counter examples. Moreover, these can always be arranged as a symmetric matrix, possibly changing the order (or permuting the coordinates) and changing signs ( edit: not always; see answer below). For example, the vectors above can be arranged as: $$\begin{bmatrix}14&-5&2\\-5&-10&10\\2&10&11\end{bmatrix}$$ This is easily true if $n$ is even ( edit: rather, if $n=4$), you can simply permute the entries of $\vec x$ (altering signs appropriately) to find $n-1$ other vectors orthogonal to $\vec x$. In $\mathbb R^3$, finding a second integer vector is sufficient because the cross product of the two (divided by $\|\vec x\|$) will give the third vector of such a basis. Then it is straightforward to prove for special cases (like $\vec x = \left<1,2m,2m^2 \right>$, $m\in \mathbb Z$), but I can't think of a good reason why this should be true in general. Edited , hopefully for clarity, by o.p. The original phrasing and title referred to $\mathbb Z^n$.","Suppose $\vec x$ is a (non-zero) vector with integer coordinates in $\mathbb R^n$ such that $\|\vec x\| \in \mathbb Z$. Is it true that there is an orthogonal basis of $\mathbb R^n$ containing $\vec x$, consisting of vectors with integer coordinates, all with the same length? For example: let $\vec x = \left<2,10,11\right>$, so $\|\vec x\| = 15$. Then the vectors $\left<14,-5,2\right>$ and $\left<5,10,-10\right>$ complete a basis of $\mathbb R^3$. I've checked all such integer vectors in $\mathbb R^3$ with an integer length up to 17 and found no counter examples. Moreover, these can always be arranged as a symmetric matrix, possibly changing the order (or permuting the coordinates) and changing signs ( edit: not always; see answer below). For example, the vectors above can be arranged as: $$\begin{bmatrix}14&-5&2\\-5&-10&10\\2&10&11\end{bmatrix}$$ This is easily true if $n$ is even ( edit: rather, if $n=4$), you can simply permute the entries of $\vec x$ (altering signs appropriately) to find $n-1$ other vectors orthogonal to $\vec x$. In $\mathbb R^3$, finding a second integer vector is sufficient because the cross product of the two (divided by $\|\vec x\|$) will give the third vector of such a basis. Then it is straightforward to prove for special cases (like $\vec x = \left<1,2m,2m^2 \right>$, $m\in \mathbb Z$), but I can't think of a good reason why this should be true in general. Edited , hopefully for clarity, by o.p. The original phrasing and title referred to $\mathbb Z^n$.",,"['linear-algebra', 'matrices', 'elementary-number-theory', 'quadratic-forms', 'integer-lattices']"
25,"help me understand a line in an “$A^TA$ is positive, semi-definite” proof","help me understand a line in an “ is positive, semi-definite” proof",A^TA,"I am looking at a proof for why $A^TA$ is positive semi-definite when $A$ is $n\times n$ and it has this line. $$ v^TAA^Tv = A^Tv \cdot A^Tv ≥ 0. $$ I understand what $v^TAA^Tv$ means and the purpose of proving that it's nonnegative, etc... My problem is that I am a linear algebra novice and do not necessarily understand how the first part $v^TAA^Tv$ is equivalent to $A^Tv \cdot A^Tv$. I know that $a^Tb = a \cdot b$, but something else is going on, no? Appreciate any help!","I am looking at a proof for why $A^TA$ is positive semi-definite when $A$ is $n\times n$ and it has this line. $$ v^TAA^Tv = A^Tv \cdot A^Tv ≥ 0. $$ I understand what $v^TAA^Tv$ means and the purpose of proving that it's nonnegative, etc... My problem is that I am a linear algebra novice and do not necessarily understand how the first part $v^TAA^Tv$ is equivalent to $A^Tv \cdot A^Tv$. I know that $a^Tb = a \cdot b$, but something else is going on, no? Appreciate any help!",,"['linear-algebra', 'proof-writing']"
26,"If $ \operatorname{Tr}(M^k) = \operatorname{Tr}(N^k)\;\forall 1\leq k \leq n$, then how do we show the $M$ and $N$ have the same eigenvalues?","If , then how do we show the  and  have the same eigenvalues?", \operatorname{Tr}(M^k) = \operatorname{Tr}(N^k)\;\forall 1\leq k \leq n M N,"Let $M,N$ be $n \times n$ square matrices over an algebraically closed field with the properties that the trace of both matrices coincides along with all powers of the matrix. More specifically, suppose that $\mathrm{Tr}(M^k) = \mathrm{Tr}(N^k)$ for all $1\leq k \leq n$ . The following questions about eigenvalues is then natural and I was thinking it would be an application of Cayley-Hamilton but I am having trouble writing out a proof. How do we show that $M$ and $N$ have the same eigenvalues? Added (because this question is now target of many duplicates, it should state its hypotheses properly). Assume that all the mentioned values of $k$ are nonzero in the field considered; in other words either the field is of characteristic $0$ , or else its prime characteristic $p$ satisfies $p>n$ .","Let be square matrices over an algebraically closed field with the properties that the trace of both matrices coincides along with all powers of the matrix. More specifically, suppose that for all . The following questions about eigenvalues is then natural and I was thinking it would be an application of Cayley-Hamilton but I am having trouble writing out a proof. How do we show that and have the same eigenvalues? Added (because this question is now target of many duplicates, it should state its hypotheses properly). Assume that all the mentioned values of are nonzero in the field considered; in other words either the field is of characteristic , or else its prime characteristic satisfies .","M,N n \times n \mathrm{Tr}(M^k) = \mathrm{Tr}(N^k) 1\leq k \leq n M N k 0 p p>n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
27,Strictly diagonally dominant matrices are non singular,Strictly diagonally dominant matrices are non singular,,"I try to find a good proof for invertibility of strictly diagonally dominant matrices (defined by $|m_{ii}|>\sum_{j\ne i}|m_{ij}|$).  There is a proof of this in this paper but I'm wondering whether there are are better proof such as using determinant, etc to show that the matrix is non singular.","I try to find a good proof for invertibility of strictly diagonally dominant matrices (defined by $|m_{ii}|>\sum_{j\ne i}|m_{ij}|$).  There is a proof of this in this paper but I'm wondering whether there are are better proof such as using determinant, etc to show that the matrix is non singular.",,"['linear-algebra', 'matrices']"
28,Fast way to calculate Eigen of 2x2 matrix using a formula,Fast way to calculate Eigen of 2x2 matrix using a formula,,"I found this site: http://people.math.harvard.edu/~knill/teaching/math21b2004/exhibits/2dmatrices/index.html Which shows a very fast and simple way to get Eigen vectors for a 2x2 matrix. While harvard is quite respectable, I want to understand how this quick formula works and not take it on faith Part 1 calculating the Eigen values is quite clear, they are using the characteristic polynomial to get the Eigen values. Part 2, where they calculate the Eigen vectors is what I don't understand and have tried to prove but cannot. I understand that that what matters with Eigen vectors is the ratio, not the value. For example, an Eigen value of 2, with vector 3, 4, I could have any other vector, example 6, 8, or 12, 16, etc... any scalar multiple. In their example, given a matrix in the form a b c d, if b & c are zero, then the vectors are 1 0 and 0 1, which makes sense as you can scale these to any other size. I don't understand the other two cases (when b=0, or c=0), or I presume the case when b & c are non-zero. Can somebody offer an explanation or proof of this?","I found this site: http://people.math.harvard.edu/~knill/teaching/math21b2004/exhibits/2dmatrices/index.html Which shows a very fast and simple way to get Eigen vectors for a 2x2 matrix. While harvard is quite respectable, I want to understand how this quick formula works and not take it on faith Part 1 calculating the Eigen values is quite clear, they are using the characteristic polynomial to get the Eigen values. Part 2, where they calculate the Eigen vectors is what I don't understand and have tried to prove but cannot. I understand that that what matters with Eigen vectors is the ratio, not the value. For example, an Eigen value of 2, with vector 3, 4, I could have any other vector, example 6, 8, or 12, 16, etc... any scalar multiple. In their example, given a matrix in the form a b c d, if b & c are zero, then the vectors are 1 0 and 0 1, which makes sense as you can scale these to any other size. I don't understand the other two cases (when b=0, or c=0), or I presume the case when b & c are non-zero. Can somebody offer an explanation or proof of this?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
29,what is the geometrical interpretation to positive definite matrix,what is the geometrical interpretation to positive definite matrix,,"What is the geometrical interpretation of positive definite matrix ? (not necessarily symmetric) if $A$ is positive definite, what does it do to a vector $x$ (i.e. $Ax$)?","What is the geometrical interpretation of positive definite matrix ? (not necessarily symmetric) if $A$ is positive definite, what does it do to a vector $x$ (i.e. $Ax$)?",,"['linear-algebra', 'intuition']"
30,Rank of the outer product of two vectors,Rank of the outer product of two vectors,,"I have come across the statement that the rank of the outer product of two vectors is always $1$ , but why is that true?","I have come across the statement that the rank of the outer product of two vectors is always , but why is that true?",1,['linear-algebra']
31,The Dimension of the Symmetric $k$-tensors,The Dimension of the Symmetric -tensors,k,"I want to compute the dimension of the symmetric $k$-tensors. I know that a covariant $k$-tensor $T$ is called symmetric if it is unchanged under permutation of arguments. Also, I know that the dimension of covariant $k$-tensors is $n^k$ but how can I eliminate non-symmetric the cases? I found this but I could not get the intution. Also, this blog post answers my question but I don't see why we put | between different indices. Any concrete example would also help such as the symmetric covariant 2-tensors in $\mathbb{R^3}$, as I asked in this thread .","I want to compute the dimension of the symmetric $k$-tensors. I know that a covariant $k$-tensor $T$ is called symmetric if it is unchanged under permutation of arguments. Also, I know that the dimension of covariant $k$-tensors is $n^k$ but how can I eliminate non-symmetric the cases? I found this but I could not get the intution. Also, this blog post answers my question but I don't see why we put | between different indices. Any concrete example would also help such as the symmetric covariant 2-tensors in $\mathbb{R^3}$, as I asked in this thread .",,"['linear-algebra', 'differential-geometry', 'tensors']"
32,mathematical difference between column vectors and row vectors,mathematical difference between column vectors and row vectors,,"I'm writing a mathematical library; and I have an idea where I want to automatically turn column matrices and row matrices to vectors, with all of the mathematical properties of a vector. Answer I'm looking for: Someone with good mathematical reasoning explaining why: column matrices, column vectors, row matrices, row vectors should not be treated as the same thing. (The library will ofcourse understand operations like [[1,2],[3,4]] * [1,2], where [1,2] is a vector) or: some kind of showcase or example where it is impossible for a library that can't differentiate between row vectors and column vectors to know which one of several possible answers are correct. or: some kind of evidence that it is in fact possible to do this. please note: inner vector multiplication will be easily integrated by using a special function for that function rather than the * sign.","I'm writing a mathematical library; and I have an idea where I want to automatically turn column matrices and row matrices to vectors, with all of the mathematical properties of a vector. Answer I'm looking for: Someone with good mathematical reasoning explaining why: column matrices, column vectors, row matrices, row vectors should not be treated as the same thing. (The library will ofcourse understand operations like [[1,2],[3,4]] * [1,2], where [1,2] is a vector) or: some kind of showcase or example where it is impossible for a library that can't differentiate between row vectors and column vectors to know which one of several possible answers are correct. or: some kind of evidence that it is in fact possible to do this. please note: inner vector multiplication will be easily integrated by using a special function for that function rather than the * sign.",,"['linear-algebra', 'vector-analysis']"
33,Basis for tensor products,Basis for tensor products,,"Suppose $V_1$ and $V_2$ are $k$ -vector spaces with bases $(e_i)_{i \in I}$ and $(f_j)_{j \in J}$ , respectively. I've seen the claim that the collection of elements of the form $e_i \otimes f_j$ (with $\left(i,j\right) \in I \times J$ ) forms a basis for $V_1 \otimes V_2$ . But I seem to get stuck with the proof. My question: What's the easiest way to see that the above set is indeed linearly independent?","Suppose and are -vector spaces with bases and , respectively. I've seen the claim that the collection of elements of the form (with ) forms a basis for . But I seem to get stuck with the proof. My question: What's the easiest way to see that the above set is indeed linearly independent?","V_1 V_2 k (e_i)_{i \in I} (f_j)_{j \in J} e_i \otimes f_j \left(i,j\right) \in I \times J V_1 \otimes V_2","['linear-algebra', 'abstract-algebra', 'tensor-products', 'multilinear-algebra']"
34,Why can a system of linear equations be represented as a linear combination of vectors?,Why can a system of linear equations be represented as a linear combination of vectors?,,"I was watching Gilbert Strang's first Linear Algebra lecture, and the very first thing he does is relating the standard view of a system of linear equations as lines -in $\mathbb{R}^2$ of course- (what he calls the row picture) with the notion of taking a linear combination of the vectors given by the columns of the matrix (the column picture). Now, I can see it works in practice to reach the same solution, but I don't intuitively understand why this is the case. A priori, they seem like very different things, and it's mysterious that these two views somehow correspond to each other. Could anyone shed some light on this? I'd greatly appreciate it!","I was watching Gilbert Strang's first Linear Algebra lecture, and the very first thing he does is relating the standard view of a system of linear equations as lines -in $\mathbb{R}^2$ of course- (what he calls the row picture) with the notion of taking a linear combination of the vectors given by the columns of the matrix (the column picture). Now, I can see it works in practice to reach the same solution, but I don't intuitively understand why this is the case. A priori, they seem like very different things, and it's mysterious that these two views somehow correspond to each other. Could anyone shed some light on this? I'd greatly appreciate it!",,['linear-algebra']
35,Is there a surjective group homomorphism $\operatorname{GL}_{n}(k) \to \operatorname{GL}_{m}(k)$ where $n > m$?,Is there a surjective group homomorphism  where ?,\operatorname{GL}_{n}(k) \to \operatorname{GL}_{m}(k) n > m,"Does there exist a field $k$, two positive integers $n > m > 1$, and a surjective group homomorphism $\operatorname{GL}_{n}(k) \to \operatorname{GL}_{m}(k)$? Here $k$ can be any field, and $\operatorname{GL}_{n}(k)$ is viewed as an abstract group (as opposed to group scheme or Lie group), and this group homomorphism doesn't have to be ""algebraic"" or ""smooth"" in any sense. Note that if $m = 1$ then the determinant map gives a surjective map.","Does there exist a field $k$, two positive integers $n > m > 1$, and a surjective group homomorphism $\operatorname{GL}_{n}(k) \to \operatorname{GL}_{m}(k)$? Here $k$ can be any field, and $\operatorname{GL}_{n}(k)$ is viewed as an abstract group (as opposed to group scheme or Lie group), and this group homomorphism doesn't have to be ""algebraic"" or ""smooth"" in any sense. Note that if $m = 1$ then the determinant map gives a surjective map.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'algebraic-groups', 'general-linear-group']"
36,How to find the eigenvalues of a block-diagonal matrix?,How to find the eigenvalues of a block-diagonal matrix?,,The matrix $A$ below is a block diagonal matrix where each block $A_i$ is a  $4 \times 4$ matrix with known eigenvalues. $$A= \begin{pmatrix}A_1 & 0 & \cdots & 0 \\ 0 & A_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & A_n  \end{pmatrix}$$ How do I find the eigenvalues of the block diagonal matrix $A$? Does this mean that I will have $4 n$ eigenvalues? Am I correct in thinking that the eigenvalues of the block diagonal matrix $A$ above are just a list of the individual eigenvalues of each $A_i$ and not the product of everything?,The matrix $A$ below is a block diagonal matrix where each block $A_i$ is a  $4 \times 4$ matrix with known eigenvalues. $$A= \begin{pmatrix}A_1 & 0 & \cdots & 0 \\ 0 & A_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & A_n  \end{pmatrix}$$ How do I find the eigenvalues of the block diagonal matrix $A$? Does this mean that I will have $4 n$ eigenvalues? Am I correct in thinking that the eigenvalues of the block diagonal matrix $A$ above are just a list of the individual eigenvalues of each $A_i$ and not the product of everything?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
37,Upper Triangular Block Matrix Determinant by induction,Upper Triangular Block Matrix Determinant by induction,,"We want to prove that:  $$\det\begin{pmatrix}A & C \\ 0 & B\\ \end{pmatrix}= \det(A)\operatorname{det}(B),$$ where $A \in M_{m\times m}(R)$, $C \in M_{m\times n}(R)$,$B \in M_{n\times n}(R)$ and $R$ is a commutative ring with $1$. $\textbf{SOLUTION}:$ Let $L = \begin{pmatrix}A & C \\ 0 & B\\ \end{pmatrix},$ then its clear that $L \in M_{(m+n)\times(m+n)}(R)$ and the matrix $L$ can be represented as follows: $$L = \begin{pmatrix}\begin{bmatrix}  a_{11} &a_{12}  &\ldots   & a_{1m}  \\  a_{21} &a_{22}  &\ldots   & a_{2m}  \\  \vdots  & \ddots  & \ddots  &\vdots  \\   a_{11} &a_{12}  &\dots   & a_{mm}  \\ \end{bmatrix}  & \begin{bmatrix} c_{11} &c_{12}  &\ldots   & c_{1n}  \\  c_{21} &c_{22}  &\ldots   & a_{2n}  \\  \vdots  & \ddots  & \ddots  &\vdots  \\   c_{m1} &c_{m2}  &\dots   & c_{mn} \\ \end{bmatrix} \\ \\ \begin{bmatrix} 0 &0  &\ldots   & 0  \\  0 &0  &\ldots   & 0  \\  \vdots  & \ddots  & \ddots  &\vdots  \\   0 &0  &\dots   & 0 \\ \end{bmatrix}& \begin{bmatrix} b_{11} &b_{12}  &\ldots   & b_{1n}  \\  b_{21} &b_{22}  &\ldots   & b_{2n}  \\  \vdots  & \ddots  & \ddots  &\vdots  \\   b_{n1} &b_{n2}  &\dots   & b_{nn} \\ \end{bmatrix}\\ \end{pmatrix}$$ Now, for each fixed $i \in \{1,2,\ldots,m,m+1,\ldots,m+n\}$ ($i$ is the row), We have: $$\det(L) = \sum\limits_{j=1}^m (-1)^{i+j}\alpha_{ij}\det(L_{ij}) + \sum\limits_{j=m+1}^{m+n} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$ where: $$ \ \alpha_{i,j} = \begin{cases}        a_{i,j} & i,j = 1,\ldots , m \\       c_{i,j-m} & i = 1,\ldots , m & j = m+1, \ldots ,m+n \\       0 & i= m+1, \ldots ,m+n & j = 1,\ldots , m \\       b_{i-m,j-m} & i = m+1,\ldots ,m+n & j = m+1, \ldots, m+n    \end{cases} \\ $$ and $$ \\ L_{i,j} = \begin{cases}        A_{i,j} & i,j = 1,\ldots , m \\       C_{i,j-m} & i = 1,\ldots , m & j = m+1, \ldots,m+n \\       0 & i= m+1, \ldots ,m+n & j = 1,\ldots , m \\       B_{i-m,j-m} & i = m+1,\ldots ,m+n & j = m+1, \ldots ,m+n    \end{cases} \\ $$ $\textbf{FIRST CASE}$ Choose $i > m$, then $i \in \{m+1,\ldots, m+n \}$. Now if $n = 0$, the problem is trivial, since $\det(B) = 1$ and therefore:$$\det(L) = \det(A)\det(B) = \det(A)1 = \det(A).$$ Then, lets assume that  $$\det(L) = \det(A)\det(B),$$ holds for $n = k$.Then choose $i \in \{m+1,\ldots, m+k \}$, then $\forall j < m+1$, I get: $$(-1)^{i+j}\alpha_{ij}\det(L_{ij}) = 0,$$ Hence:  $$\det(L) = \sum\limits_{j=m+1}^{m+n} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$ but $i \in \{m+1,m+2, \ldots, m+k\}$ and $j \in \{m+1,m+2, \ldots, m+k\}$ therefore: $$\det(L) = \sum\limits_{j=m+1}^{m+k} (-1)^{i+j}b_{ij}\det(B_{ij})$$ Notice that the sub-matrix $B_{ij}$ has the form: $\begin{pmatrix}A & C' \\ 0 & B'\\ \end{pmatrix}$. On the other hand, $B'$ have size smaller than $B$, therefore by our induction hypothesis, we have: $$\det\begin{pmatrix}A & C' \\ 0 & B'\\ \end{pmatrix} = \det(A)\det(B')$$ $ \textbf{SECOND CASE} $ On the other hand, Choose $i \leq m$, if $m = 0$, then the problem is trivial, since $\det(A) = 1$ and therefore: $$\det(L) = \det(A)\det(B) = \det(B) = \det(B).$$ Then, lets assume that  $$\det(L) = \det(A)\det(B)$$ holds for $m = k$. Then choose $i \in \{1,\ldots, k \}$, then for all $j>k$, we have: $$(-1)^{i+j}\alpha_{ij}\det(L_{ij}) = 0,$$ therefore: $$\det(L) = \sum\limits_{j=1}^{k} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$ but $i \in \{1,2, \ldots, k\}$ and $j \in \{1,2, \ldots, k\}$, therefore: $$\det(L) = \sum\limits_{j=1}^k (-1)^{i+j}a_{ij}\det(A_{ij})$$ Now, the sub-matrix $A_{ij}$ has the form: $\begin{pmatrix}A' & C' \\ 0 & B\\ \end{pmatrix}$. On the other hand, $A'$ has size smaller than $A$, then by the induction hypothesis, we have: $$\det\begin{pmatrix}A' & C' \\ 0 & B\\ \end{pmatrix} = \det(A')\det(B) $$ I want to know if this is the right proof.","We want to prove that:  $$\det\begin{pmatrix}A & C \\ 0 & B\\ \end{pmatrix}= \det(A)\operatorname{det}(B),$$ where $A \in M_{m\times m}(R)$, $C \in M_{m\times n}(R)$,$B \in M_{n\times n}(R)$ and $R$ is a commutative ring with $1$. $\textbf{SOLUTION}:$ Let $L = \begin{pmatrix}A & C \\ 0 & B\\ \end{pmatrix},$ then its clear that $L \in M_{(m+n)\times(m+n)}(R)$ and the matrix $L$ can be represented as follows: $$L = \begin{pmatrix}\begin{bmatrix}  a_{11} &a_{12}  &\ldots   & a_{1m}  \\  a_{21} &a_{22}  &\ldots   & a_{2m}  \\  \vdots  & \ddots  & \ddots  &\vdots  \\   a_{11} &a_{12}  &\dots   & a_{mm}  \\ \end{bmatrix}  & \begin{bmatrix} c_{11} &c_{12}  &\ldots   & c_{1n}  \\  c_{21} &c_{22}  &\ldots   & a_{2n}  \\  \vdots  & \ddots  & \ddots  &\vdots  \\   c_{m1} &c_{m2}  &\dots   & c_{mn} \\ \end{bmatrix} \\ \\ \begin{bmatrix} 0 &0  &\ldots   & 0  \\  0 &0  &\ldots   & 0  \\  \vdots  & \ddots  & \ddots  &\vdots  \\   0 &0  &\dots   & 0 \\ \end{bmatrix}& \begin{bmatrix} b_{11} &b_{12}  &\ldots   & b_{1n}  \\  b_{21} &b_{22}  &\ldots   & b_{2n}  \\  \vdots  & \ddots  & \ddots  &\vdots  \\   b_{n1} &b_{n2}  &\dots   & b_{nn} \\ \end{bmatrix}\\ \end{pmatrix}$$ Now, for each fixed $i \in \{1,2,\ldots,m,m+1,\ldots,m+n\}$ ($i$ is the row), We have: $$\det(L) = \sum\limits_{j=1}^m (-1)^{i+j}\alpha_{ij}\det(L_{ij}) + \sum\limits_{j=m+1}^{m+n} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$ where: $$ \ \alpha_{i,j} = \begin{cases}        a_{i,j} & i,j = 1,\ldots , m \\       c_{i,j-m} & i = 1,\ldots , m & j = m+1, \ldots ,m+n \\       0 & i= m+1, \ldots ,m+n & j = 1,\ldots , m \\       b_{i-m,j-m} & i = m+1,\ldots ,m+n & j = m+1, \ldots, m+n    \end{cases} \\ $$ and $$ \\ L_{i,j} = \begin{cases}        A_{i,j} & i,j = 1,\ldots , m \\       C_{i,j-m} & i = 1,\ldots , m & j = m+1, \ldots,m+n \\       0 & i= m+1, \ldots ,m+n & j = 1,\ldots , m \\       B_{i-m,j-m} & i = m+1,\ldots ,m+n & j = m+1, \ldots ,m+n    \end{cases} \\ $$ $\textbf{FIRST CASE}$ Choose $i > m$, then $i \in \{m+1,\ldots, m+n \}$. Now if $n = 0$, the problem is trivial, since $\det(B) = 1$ and therefore:$$\det(L) = \det(A)\det(B) = \det(A)1 = \det(A).$$ Then, lets assume that  $$\det(L) = \det(A)\det(B),$$ holds for $n = k$.Then choose $i \in \{m+1,\ldots, m+k \}$, then $\forall j < m+1$, I get: $$(-1)^{i+j}\alpha_{ij}\det(L_{ij}) = 0,$$ Hence:  $$\det(L) = \sum\limits_{j=m+1}^{m+n} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$ but $i \in \{m+1,m+2, \ldots, m+k\}$ and $j \in \{m+1,m+2, \ldots, m+k\}$ therefore: $$\det(L) = \sum\limits_{j=m+1}^{m+k} (-1)^{i+j}b_{ij}\det(B_{ij})$$ Notice that the sub-matrix $B_{ij}$ has the form: $\begin{pmatrix}A & C' \\ 0 & B'\\ \end{pmatrix}$. On the other hand, $B'$ have size smaller than $B$, therefore by our induction hypothesis, we have: $$\det\begin{pmatrix}A & C' \\ 0 & B'\\ \end{pmatrix} = \det(A)\det(B')$$ $ \textbf{SECOND CASE} $ On the other hand, Choose $i \leq m$, if $m = 0$, then the problem is trivial, since $\det(A) = 1$ and therefore: $$\det(L) = \det(A)\det(B) = \det(B) = \det(B).$$ Then, lets assume that  $$\det(L) = \det(A)\det(B)$$ holds for $m = k$. Then choose $i \in \{1,\ldots, k \}$, then for all $j>k$, we have: $$(-1)^{i+j}\alpha_{ij}\det(L_{ij}) = 0,$$ therefore: $$\det(L) = \sum\limits_{j=1}^{k} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$ but $i \in \{1,2, \ldots, k\}$ and $j \in \{1,2, \ldots, k\}$, therefore: $$\det(L) = \sum\limits_{j=1}^k (-1)^{i+j}a_{ij}\det(A_{ij})$$ Now, the sub-matrix $A_{ij}$ has the form: $\begin{pmatrix}A' & C' \\ 0 & B\\ \end{pmatrix}$. On the other hand, $A'$ has size smaller than $A$, then by the induction hypothesis, we have: $$\det\begin{pmatrix}A' & C' \\ 0 & B\\ \end{pmatrix} = \det(A')\det(B) $$ I want to know if this is the right proof.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory', 'determinant']"
38,Meaning of the inverse of the Laplacian matrix,Meaning of the inverse of the Laplacian matrix,,"Given an undirected graph $G = (V,E)$ , let $\bf A$ and $\cal L_{\bf A}$ denote its adjacency matrix and its Laplacian matrix, respectively. $\cal L_{\bf A}(i,i)$ is the degree of vertex ${{\bf{v}}_i}$ , and $\mathcal L_{\bf A}(i,j) = -1$ if vertices ${{\bf{v}}_i}$ and ${{\bf{v}}_j}$ are connected. I know the Laplacian matrix is exactly the discrete Laplace operator. Given a function $\phi : V \to \Bbb R$ , and supposing that $\bf A$ is indexed by $\bf v$ , i.e., $\bf A$$(i,j) = 1$ iff vertices ${{\bf{v}}_i}$ and ${{\bf{v}}_j}$ are connected, then $$\Delta \phi=\cal L_{\bf A}{\phi(\bf v)}$$ What I need to know is the meaning of the inverse (or pseudoinverse if it is not invertible) of the Laplacian. Is there any meaningful interpretation of the inverse Laplacian ? Is there any concrete meaning for $\mathcal L^{-1}_{\bf A}(i,j)$ ?","Given an undirected graph , let and denote its adjacency matrix and its Laplacian matrix, respectively. is the degree of vertex , and if vertices and are connected. I know the Laplacian matrix is exactly the discrete Laplace operator. Given a function , and supposing that is indexed by , i.e., iff vertices and are connected, then What I need to know is the meaning of the inverse (or pseudoinverse if it is not invertible) of the Laplacian. Is there any meaningful interpretation of the inverse Laplacian ? Is there any concrete meaning for ?","G = (V,E) \bf A \cal L_{\bf A} \cal L_{\bf A}(i,i) {{\bf{v}}_i} \mathcal L_{\bf A}(i,j) = -1 {{\bf{v}}_i} {{\bf{v}}_j} \phi : V \to \Bbb R \bf A \bf v \bf A(i,j) = 1 {{\bf{v}}_i} {{\bf{v}}_j} \Delta \phi=\cal L_{\bf A}{\phi(\bf v)} \mathcal L^{-1}_{\bf A}(i,j)","['linear-algebra', 'matrices', 'graph-theory', 'pseudoinverse', 'graph-laplacian']"
39,Why do elementary row operations preserve linear dependence between matrix columns?,Why do elementary row operations preserve linear dependence between matrix columns?,,"I know that I can find a basis for the column space of a matrix $A$ by reducing the matrix to reduced row echelon form $J$. The columns of $A$ corresponding to the linearly independent columns of $J$ then form a basis for $Col(A)$, because linear dependence is preserved under elementary row operations. I can't figure out why this is true though, and a google search returns nothing, so I'm sure it's simple. Can someone give me a proof?","I know that I can find a basis for the column space of a matrix $A$ by reducing the matrix to reduced row echelon form $J$. The columns of $A$ corresponding to the linearly independent columns of $J$ then form a basis for $Col(A)$, because linear dependence is preserved under elementary row operations. I can't figure out why this is true though, and a google search returns nothing, so I'm sure it's simple. Can someone give me a proof?",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
40,Prove that $\det(AB-BA)=0$,Prove that,\det(AB-BA)=0,"Let $A,B$ be two $3 \times 3$ matrices with complex entries such that $$(A-B)^2=O_3$$ Prove that $$\det(AB-BA)=0$$ I tried to prove this with ranks. I denoted $X=A-B$ and thus $X^2=O_3$ which means that $\det X=0$ and $\operatorname{rank}X \leq 2$ . Then, I wrote $AB-BA=(X+B)B-B(X+B)=XB-BX$ and finally I used $\operatorname{rank}(M \pm N) \leq \operatorname{rank}M+\operatorname{rank}N$ and Frobenius's inequality in order to get $$\operatorname{rank}(XB-BX) \leq \operatorname{rank}BX+\operatorname{rank}XB \leq \operatorname{rank}X+\operatorname{rank}BXB$$ and if we knew that $\operatorname{rank}BXB=0$ , the problem would be solved. However, I don't quite know if the latter is true.","Let be two matrices with complex entries such that Prove that I tried to prove this with ranks. I denoted and thus which means that and . Then, I wrote and finally I used and Frobenius's inequality in order to get and if we knew that , the problem would be solved. However, I don't quite know if the latter is true.","A,B 3 \times 3 (A-B)^2=O_3 \det(AB-BA)=0 X=A-B X^2=O_3 \det X=0 \operatorname{rank}X \leq 2 AB-BA=(X+B)B-B(X+B)=XB-BX \operatorname{rank}(M \pm N) \leq \operatorname{rank}M+\operatorname{rank}N \operatorname{rank}(XB-BX) \leq \operatorname{rank}BX+\operatorname{rank}XB \leq \operatorname{rank}X+\operatorname{rank}BXB \operatorname{rank}BXB=0","['linear-algebra', 'matrices', 'determinant', 'matrix-rank']"
41,Tricks for quickly reading off the eigenvalues of a matrix,Tricks for quickly reading off the eigenvalues of a matrix,,"I noticed that some mathematicians have an uncanny ability to identify the eigenvalues of matrices without doing much in the way of computation. For instance, one might notice that all the rows have the same sum, from which it follows that the sum must be an eigenvalue. There can be no simple way to find eigenvalues of arbitrary matrices, but what special cases might we look for where finding the eigenvalues are easy? What tricks do you know?","I noticed that some mathematicians have an uncanny ability to identify the eigenvalues of matrices without doing much in the way of computation. For instance, one might notice that all the rows have the same sum, from which it follows that the sum must be an eigenvalue. There can be no simple way to find eigenvalues of arbitrary matrices, but what special cases might we look for where finding the eigenvalues are easy? What tricks do you know?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'big-list']"
42,Motivation for Jordan Canonical Form,Motivation for Jordan Canonical Form,,"I took linear algebra and understood the proof that linear operators on a vector space over an algebraically closed field have a Jordan Canonical Form. Why should I care about this theorem? I understand that it can be useful in doing some computations, but it seems that these computations are quite rare. Indeed, I am not puzzled by diagonalization or triangularization at all. They both have practical and theoretical uses, but even more than that, they just seem like nice things to have. Can someone explain why Jordan Canonical Form is a ""nice thing to have""?","I took linear algebra and understood the proof that linear operators on a vector space over an algebraically closed field have a Jordan Canonical Form. Why should I care about this theorem? I understand that it can be useful in doing some computations, but it seems that these computations are quite rare. Indeed, I am not puzzled by diagonalization or triangularization at all. They both have practical and theoretical uses, but even more than that, they just seem like nice things to have. Can someone explain why Jordan Canonical Form is a ""nice thing to have""?",,"['linear-algebra', 'matrices', 'soft-question', 'jordan-normal-form']"
43,"How to show $\ell^2_p$ not isometric to $\ell^2_{p'}$ unless $p\in\{1,2,\infty\}$?",How to show  not isometric to  unless ?,"\ell^2_p \ell^2_{p'} p\in\{1,2,\infty\}","Let $\ell^n_p$ denote $\mathbb{R}^n$ with the $p$-norm; assume $n>1$.  Now it's well-known that for $p\ne q$, $\ell^n_p$ and $\ell^n_q$ are never isometric unless $n=2$ and $\{p,q\}=\{1,\infty\}$, but I realized recently I didn't actually know how to prove this. I can prove most of it - the only case I still need to rule out is the case where $n=2$ and $q=p'$ (i.e. $\frac{1}{1-1/p}$).  Now of course, it is indeed possible that $q=p'$, because $p$ could be $2$ (so $p=q$) or $1$ or $\infty$ (the sole exceptional case).  But the problem then is showing that $p$ must actually be one of those three. Furthermore I can show that if there is such an isometry, then, after applying a translation and some signed permutation matrix, it must be $\lambda R$, where $\lambda=2^{1/p-1/2}=2^{1/2-1/p'}$ and $R$ is rotation by $\pi/4$.  So actually all we need to do is, given $p\notin \{1,2,\infty\}$, to find a point $v$ such that $||\lambda R v||_{p'}\ne ||v||_p$. Now, graphing things, we can see that in fact any point that is not on the axes or the lines $y=\pm x$ will work; but I do not know how to prove it for even a single specified point, even, say, $(1,2)$, because the resulting equation for $p$ (assuming $p\ne 1,\infty$, of course) is just absolutely terrible, and attempting to, say, determine when each side is larger by differentiating seems to only make things worse. Addendum Nov 21 : I should perhaps point out - obviously the point can vary with $p$, but I have no idea how given $p$ one might pick a specific point that makes it easier to prove - especially since it is after all true for nearly any point! So how is this case handled? And is there by any chance a nicer approach that I'm missing?","Let $\ell^n_p$ denote $\mathbb{R}^n$ with the $p$-norm; assume $n>1$.  Now it's well-known that for $p\ne q$, $\ell^n_p$ and $\ell^n_q$ are never isometric unless $n=2$ and $\{p,q\}=\{1,\infty\}$, but I realized recently I didn't actually know how to prove this. I can prove most of it - the only case I still need to rule out is the case where $n=2$ and $q=p'$ (i.e. $\frac{1}{1-1/p}$).  Now of course, it is indeed possible that $q=p'$, because $p$ could be $2$ (so $p=q$) or $1$ or $\infty$ (the sole exceptional case).  But the problem then is showing that $p$ must actually be one of those three. Furthermore I can show that if there is such an isometry, then, after applying a translation and some signed permutation matrix, it must be $\lambda R$, where $\lambda=2^{1/p-1/2}=2^{1/2-1/p'}$ and $R$ is rotation by $\pi/4$.  So actually all we need to do is, given $p\notin \{1,2,\infty\}$, to find a point $v$ such that $||\lambda R v||_{p'}\ne ||v||_p$. Now, graphing things, we can see that in fact any point that is not on the axes or the lines $y=\pm x$ will work; but I do not know how to prove it for even a single specified point, even, say, $(1,2)$, because the resulting equation for $p$ (assuming $p\ne 1,\infty$, of course) is just absolutely terrible, and attempting to, say, determine when each side is larger by differentiating seems to only make things worse. Addendum Nov 21 : I should perhaps point out - obviously the point can vary with $p$, but I have no idea how given $p$ one might pick a specific point that makes it easier to prove - especially since it is after all true for nearly any point! So how is this case handled? And is there by any chance a nicer approach that I'm missing?",,"['linear-algebra', 'analysis']"
44,Is there a Way to Think of the Adjugate Matrix Invariantly.,Is there a Way to Think of the Adjugate Matrix Invariantly.,,"Given a square matrix $M$ with entries from a field $F$, the adjugate of $M$ is defined as the transpose of the cofactor matrix. Is there an interpretation of this concept in terms of linear operators on vector spaces? As an example of what I am trying to ask , consider the operation of taking the transpose of a matrix (with entries from a field). This can be thought of in terms of linear operators in the following way: Let $T:V\to V$ be a linear operator on a finite dimensional vector space $V$. We define the transpose of $T$ as the linear map $T^t:V^*\to V^*$ which sends a member $\omega\in V^*$ to the member $(v\mapsto \omega(Tv))$ of $V^*$. Now if $\mathcal B$ is a basis of $V$ and $M$ is the matrix representation of $T$ with respect to the basis $\mathcal B$, then the matrix representation of $T^t$ with respect to the dual basis of $\mathcal B$ is same as the matrix transpose of $M$.","Given a square matrix $M$ with entries from a field $F$, the adjugate of $M$ is defined as the transpose of the cofactor matrix. Is there an interpretation of this concept in terms of linear operators on vector spaces? As an example of what I am trying to ask , consider the operation of taking the transpose of a matrix (with entries from a field). This can be thought of in terms of linear operators in the following way: Let $T:V\to V$ be a linear operator on a finite dimensional vector space $V$. We define the transpose of $T$ as the linear map $T^t:V^*\to V^*$ which sends a member $\omega\in V^*$ to the member $(v\mapsto \omega(Tv))$ of $V^*$. Now if $\mathcal B$ is a basis of $V$ and $M$ is the matrix representation of $T$ with respect to the basis $\mathcal B$, then the matrix representation of $T^t$ with respect to the dual basis of $\mathcal B$ is same as the matrix transpose of $M$.",,"['linear-algebra', 'matrices']"
45,Determinant of a finite-dimensional matrix in terms of trace,Determinant of a finite-dimensional matrix in terms of trace,,"I have noticed that for the case of $1 \times 1$, $2 \times 2$ and $3 \times 3$ matrices $A$, $B$, I can write the determinant of their commutator $C=[A,B]$ in terms of traces: $1 \times 1$  matrices $A$, $B$: $$\det(C)=\text{tr}(C)$$ $2 \times 2$ matrices $A$, $B$: $$\det(C)=-\frac{1}{2}\text{tr}(C^2)$$ $3 \times 3$  matrices $A$, $B$: $$\det(C)=\frac{1}{3}\text{tr}(C^3)$$ But I can't find a simple formula for $4 \times 4$  matrices--I have no idea how to generalize this general finite-dimensional matrices: I can't quite understand the origin of this pattern for low-dimensional matrices. So my question is can this be generalized to: Larger $n\times n$ matrices, $n>3$? Anticommutators $C=\{A,B\} = AB+BA$?","I have noticed that for the case of $1 \times 1$, $2 \times 2$ and $3 \times 3$ matrices $A$, $B$, I can write the determinant of their commutator $C=[A,B]$ in terms of traces: $1 \times 1$  matrices $A$, $B$: $$\det(C)=\text{tr}(C)$$ $2 \times 2$ matrices $A$, $B$: $$\det(C)=-\frac{1}{2}\text{tr}(C^2)$$ $3 \times 3$  matrices $A$, $B$: $$\det(C)=\frac{1}{3}\text{tr}(C^3)$$ But I can't find a simple formula for $4 \times 4$  matrices--I have no idea how to generalize this general finite-dimensional matrices: I can't quite understand the origin of this pattern for low-dimensional matrices. So my question is can this be generalized to: Larger $n\times n$ matrices, $n>3$? Anticommutators $C=\{A,B\} = AB+BA$?",,"['linear-algebra', 'matrices', 'determinant', 'trace']"
46,Which vectors can give zero inner products forever,Which vectors can give zero inner products forever,,"For even positive integer $n$, consider an $n$-dimensional vector $v$ such that $v \in \{-1,0,1\}^n$.  Now consider an infinite dimensional vector $w$ with $w_i \in \{-1,1\}$ and define $I_k = \sum_{i=1}^n v_i w_{i+k}$. In other words, $I_k$ is the  inner product of $v$ with the $k$th subvector of $w$. For how many vectors $v$ does there exist at least one vector $w$ such that $I_k=0$ for all $k$? I can see three categories of $v$ which will work. Clearly $v = 0$ is one such vector.  Any $w$ will give zero inner products for ever. Any vector where there are an even number of  non-zero elements which are all next to each other and are all $1$ or all $-1$s. Let us call the number of $1$s or $-1$s $x$.  In this case we just need a $w$ where every window of length $x$ has the same number of $1$s and $-1$s. Any vector which has the same number of $1$s as $-1$s.  In this case $w$ which is all $1$s or all $-1$s will do. But how many others are there apart from these? I now see I missed out a whole class of vectors $v$ which give zero inner products when aligned with $w = (1,-1,1,-1,1,\dots)$.  For example $v= (-1,-1,-1,0,0,1)$.","For even positive integer $n$, consider an $n$-dimensional vector $v$ such that $v \in \{-1,0,1\}^n$.  Now consider an infinite dimensional vector $w$ with $w_i \in \{-1,1\}$ and define $I_k = \sum_{i=1}^n v_i w_{i+k}$. In other words, $I_k$ is the  inner product of $v$ with the $k$th subvector of $w$. For how many vectors $v$ does there exist at least one vector $w$ such that $I_k=0$ for all $k$? I can see three categories of $v$ which will work. Clearly $v = 0$ is one such vector.  Any $w$ will give zero inner products for ever. Any vector where there are an even number of  non-zero elements which are all next to each other and are all $1$ or all $-1$s. Let us call the number of $1$s or $-1$s $x$.  In this case we just need a $w$ where every window of length $x$ has the same number of $1$s and $-1$s. Any vector which has the same number of $1$s as $-1$s.  In this case $w$ which is all $1$s or all $-1$s will do. But how many others are there apart from these? I now see I missed out a whole class of vectors $v$ which give zero inner products when aligned with $w = (1,-1,1,-1,1,\dots)$.  For example $v= (-1,-1,-1,0,0,1)$.",,['linear-algebra']
47,Intuition behind the definition of linear transformation,Intuition behind the definition of linear transformation,,"I have studied that given vector spaces $V_1$ and $V_2$, a function $T:V_1 \rightarrow V_2$ is called a linear transformation of $V_1$ into $V_2$, if following two properties are true for all $u, v \in V_1$ and scalar $c$: $(1)$:  $T(u+v) = T(u) + T(v)$ and $(2):  T(cu) = c T(u) $. My questions are $1$: What is the geometrical interpretation of properties $1$ and $2$ which says that $T$ preserves additivity and scalar multiplication . I am not able to see this geometrically. What is the meaning of preserving additivity and scalar multiplication. $2$: At some place I have studied that a linear transformation will be linear if it sends each line to line and planes to planes and so on. How can we interpret this based on these two properties. I need help to understand this. Thank you very much for your time..","I have studied that given vector spaces $V_1$ and $V_2$, a function $T:V_1 \rightarrow V_2$ is called a linear transformation of $V_1$ into $V_2$, if following two properties are true for all $u, v \in V_1$ and scalar $c$: $(1)$:  $T(u+v) = T(u) + T(v)$ and $(2):  T(cu) = c T(u) $. My questions are $1$: What is the geometrical interpretation of properties $1$ and $2$ which says that $T$ preserves additivity and scalar multiplication . I am not able to see this geometrically. What is the meaning of preserving additivity and scalar multiplication. $2$: At some place I have studied that a linear transformation will be linear if it sends each line to line and planes to planes and so on. How can we interpret this based on these two properties. I need help to understand this. Thank you very much for your time..",,"['linear-algebra', 'intuition', 'linear-transformations']"
48,Understanding Linear Algebra Geometrically - Reference Request,Understanding Linear Algebra Geometrically - Reference Request,,"I know geometry and I know linear algebra but when I understand a linear algebraic concept geometrically, my head just explodes and things just become so much clearer and easier to understand...not to mention easier to remember or figure out its properties and explain them to others. Here are a few examples. Orthogonal matrices - If you think of an orthogonal matrix as a rotation then some of its properties are obvious. Orthogonal matrices are always invertible because rotations can simply be reversed. They always preserve the Euclidean norm because rotating a vector doesn't change its length. Orthogonal matrices forming a group is also easy to see because it is easy to see them satisfying the group axioms. Determinant - The determinant of a linear transformation can be understood as follows. Start with the (chosen) basis of your domain. It forms a parallelepiped. Call it $P$. It has a certain volume $V(P)$. Now apply your linear transformation $T$ to the chosen basis. A new parallelepiped $T(P)$ is formed and its volume in the range space (embedded in the codomain) is now $V(T(P))$. The determinant (in absolute value) is the ratio of the new volume to the old one. This intuitively explains, for example, why the determinant is zero for non-invertible transformations. The dimension of such a transformation will always be strictly less than the dimension of the domain/codomain so the volume of the transformed parallelepiped will always be zero. I always imagine a parallelepiped in $\mathbb{R}^3$ collapsing onto a plane. This also explains why the determinant of an orthogonal matrix is always $\pm1$ because rotating a parallelepiped won't change its volume. In addition, it kind of helps with the Jacobian determinant and why is the Jacobian ""necessary"" when transforming variables. Singular value decomposition - Every matrix having an SVD says the fantastical fact that any linear transformation can be considered a rotation, then a dilation (different directions by different factors), and then a rotation again. Projection matrices - Imagine an arbitrary vector's shadow onto a line or a plane. I imagine a vector collapsing onto its shadow and properties like $P^2=P$ are immediate for any projector operator $P$. Take this and run with it. My question is, can anyone point to some good reading material where a geometric interpretation of various linear algebra concepts is offered ? This could be anyone's class/teaching notes, published papers, something from recreational mathematics, or just a good book.","I know geometry and I know linear algebra but when I understand a linear algebraic concept geometrically, my head just explodes and things just become so much clearer and easier to understand...not to mention easier to remember or figure out its properties and explain them to others. Here are a few examples. Orthogonal matrices - If you think of an orthogonal matrix as a rotation then some of its properties are obvious. Orthogonal matrices are always invertible because rotations can simply be reversed. They always preserve the Euclidean norm because rotating a vector doesn't change its length. Orthogonal matrices forming a group is also easy to see because it is easy to see them satisfying the group axioms. Determinant - The determinant of a linear transformation can be understood as follows. Start with the (chosen) basis of your domain. It forms a parallelepiped. Call it $P$. It has a certain volume $V(P)$. Now apply your linear transformation $T$ to the chosen basis. A new parallelepiped $T(P)$ is formed and its volume in the range space (embedded in the codomain) is now $V(T(P))$. The determinant (in absolute value) is the ratio of the new volume to the old one. This intuitively explains, for example, why the determinant is zero for non-invertible transformations. The dimension of such a transformation will always be strictly less than the dimension of the domain/codomain so the volume of the transformed parallelepiped will always be zero. I always imagine a parallelepiped in $\mathbb{R}^3$ collapsing onto a plane. This also explains why the determinant of an orthogonal matrix is always $\pm1$ because rotating a parallelepiped won't change its volume. In addition, it kind of helps with the Jacobian determinant and why is the Jacobian ""necessary"" when transforming variables. Singular value decomposition - Every matrix having an SVD says the fantastical fact that any linear transformation can be considered a rotation, then a dilation (different directions by different factors), and then a rotation again. Projection matrices - Imagine an arbitrary vector's shadow onto a line or a plane. I imagine a vector collapsing onto its shadow and properties like $P^2=P$ are immediate for any projector operator $P$. Take this and run with it. My question is, can anyone point to some good reading material where a geometric interpretation of various linear algebra concepts is offered ? This could be anyone's class/teaching notes, published papers, something from recreational mathematics, or just a good book.",,"['linear-algebra', 'matrices', 'geometry', 'reference-request', 'geometric-interpretation']"
49,Angular distribution of lines passing through two squares.,Angular distribution of lines passing through two squares.,,"Let's say I've got two squares with side length $d$ that are held parallel at a distance $m$ apart. Suppose that particles are randomly falling from above in such a way that the polar angle $\varphi$ of the trajectory of the particles has a probability distribution proportional to $\operatorname{cos}^2(\varphi)$, while the azimuthal angle is uniformly distributed. A particle is admissible if it passes through both squares. Can we find a closed form expression for the probability distribution of the polar angle of admissible particles?  (or at least a good numerical solution?) My attempt at a stronger formulation: First, let's normalize the problem by letting $\alpha=m/d$ and assuming each square has unit side length.  We may assume that the point at which the particle passes through Panel $1$ (the upper panel) is uniformly distributed, that is, there are a uniformly distributed variables $X_1$ and $Y_1$, each between $0$ and $1$, and the point of contact on the plane of panel $1$ is $$\mathfrak{X}_1=\left(\begin{array}{c}X_1\\Y_1\\ \alpha\end{array}\right).$$  In order to be admissible, a particle must pass through Panel $1$, and we are only interested in admissible particles, so this is our universe of particles. Let's find the point of contact $\mathfrak{X}_2$ on the plane of Panel $2$ (the lower panel).  We need $$\left(\begin{array}{c}X_1+t\operatorname{sin}(\varphi)\operatorname{cos}(\theta)\\ Y_1+t\operatorname{sin}(\varphi)\operatorname{sin}(\theta)\\ \alpha+t\operatorname{cos}(\varphi)\end{array}\right)=\left(\begin{array}{c}X_2\\Y_2\\0\end{array}\right)$$ so $t=-\alpha\operatorname{sec}(\varphi)$, from which we have $$\mathfrak{X}_2=\left(\begin{array}{c}X_1-\alpha\operatorname{tan}(\varphi)\operatorname{cos}(\theta)\\Y_1-\alpha\operatorname{tan}(\varphi)\operatorname{sin}(\theta)\\0\end{array}\right).$$ So, the question is, what is the probability distribution of $\varphi$ for the lines satisfying $$\begin{array}{rcl}0 \leq& X_1-\alpha\operatorname{tan}(\varphi)\operatorname{cos}(\theta) &\leq  1 \\ 0 \leq& Y_1-\alpha\operatorname{tan}(\varphi)\operatorname{sin}(\theta) &\leq 1 \end{array} $$ given that $$\begin{array}{rl}X_1 \text{ and }Y_1& \text{are uniformly distributed from }0\text{ to }1 \\ \theta &\text{is uniformly distributed from }0\text{ to }2\pi \\ \varphi & \text{has probability distribution }\frac{4}{\pi}\operatorname{cos}^2\varphi\text{ from }\frac{\pi}{2}\text{ to }\pi\end{array}$$ ? EDIT 2: So I ran a simulation, yielding the following histogram (displayed below with $3$ different bin widths).  I haven't fit a curve to this yet, but maybe it will give us a hint.  Here $\alpha$ is about $6$.","Let's say I've got two squares with side length $d$ that are held parallel at a distance $m$ apart. Suppose that particles are randomly falling from above in such a way that the polar angle $\varphi$ of the trajectory of the particles has a probability distribution proportional to $\operatorname{cos}^2(\varphi)$, while the azimuthal angle is uniformly distributed. A particle is admissible if it passes through both squares. Can we find a closed form expression for the probability distribution of the polar angle of admissible particles?  (or at least a good numerical solution?) My attempt at a stronger formulation: First, let's normalize the problem by letting $\alpha=m/d$ and assuming each square has unit side length.  We may assume that the point at which the particle passes through Panel $1$ (the upper panel) is uniformly distributed, that is, there are a uniformly distributed variables $X_1$ and $Y_1$, each between $0$ and $1$, and the point of contact on the plane of panel $1$ is $$\mathfrak{X}_1=\left(\begin{array}{c}X_1\\Y_1\\ \alpha\end{array}\right).$$  In order to be admissible, a particle must pass through Panel $1$, and we are only interested in admissible particles, so this is our universe of particles. Let's find the point of contact $\mathfrak{X}_2$ on the plane of Panel $2$ (the lower panel).  We need $$\left(\begin{array}{c}X_1+t\operatorname{sin}(\varphi)\operatorname{cos}(\theta)\\ Y_1+t\operatorname{sin}(\varphi)\operatorname{sin}(\theta)\\ \alpha+t\operatorname{cos}(\varphi)\end{array}\right)=\left(\begin{array}{c}X_2\\Y_2\\0\end{array}\right)$$ so $t=-\alpha\operatorname{sec}(\varphi)$, from which we have $$\mathfrak{X}_2=\left(\begin{array}{c}X_1-\alpha\operatorname{tan}(\varphi)\operatorname{cos}(\theta)\\Y_1-\alpha\operatorname{tan}(\varphi)\operatorname{sin}(\theta)\\0\end{array}\right).$$ So, the question is, what is the probability distribution of $\varphi$ for the lines satisfying $$\begin{array}{rcl}0 \leq& X_1-\alpha\operatorname{tan}(\varphi)\operatorname{cos}(\theta) &\leq  1 \\ 0 \leq& Y_1-\alpha\operatorname{tan}(\varphi)\operatorname{sin}(\theta) &\leq 1 \end{array} $$ given that $$\begin{array}{rl}X_1 \text{ and }Y_1& \text{are uniformly distributed from }0\text{ to }1 \\ \theta &\text{is uniformly distributed from }0\text{ to }2\pi \\ \varphi & \text{has probability distribution }\frac{4}{\pi}\operatorname{cos}^2\varphi\text{ from }\frac{\pi}{2}\text{ to }\pi\end{array}$$ ? EDIT 2: So I ran a simulation, yielding the following histogram (displayed below with $3$ different bin widths).  I haven't fit a curve to this yet, but maybe it will give us a hint.  Here $\alpha$ is about $6$.",,"['linear-algebra', 'probability', 'trigonometry', 'probability-distributions', 'physics']"
50,Count the number of bases in a subset,Count the number of bases in a subset,,"Consider $\mathbb{R}^n$ as a vector space over $\mathbb{R}$. Consider the subset $\mathrm{S}^n = \{(x_1,\ldots,x_n) \in \mathbb{R}^n | x_i = 0 \; \mathrm{or} \; 1\;\forall i = 1,\ldots,n\}$. How many bases of $\mathbb{R}^n$ does $\mathrm{S}^n$ contain? e.g. for n = 2, $\mathrm{S}^2 = \{(0,0),(0,1),(1,0),(1,1)\}$ and it contains three bases namely $\{(0,1),(1,0)\},\;\{(0,1),(1,1)\},\textrm{ and }\{(1,0),(1,1)\}$. I want a general expression for n.","Consider $\mathbb{R}^n$ as a vector space over $\mathbb{R}$. Consider the subset $\mathrm{S}^n = \{(x_1,\ldots,x_n) \in \mathbb{R}^n | x_i = 0 \; \mathrm{or} \; 1\;\forall i = 1,\ldots,n\}$. How many bases of $\mathbb{R}^n$ does $\mathrm{S}^n$ contain? e.g. for n = 2, $\mathrm{S}^2 = \{(0,0),(0,1),(1,0),(1,1)\}$ and it contains three bases namely $\{(0,1),(1,0)\},\;\{(0,1),(1,1)\},\textrm{ and }\{(1,0),(1,1)\}$. I want a general expression for n.",,['linear-algebra']
51,"The normalizer of $\mathrm{GL}(n,\mathbf Z)$ in $\mathrm{GL}(n,\mathbf Q)$",The normalizer of  in,"\mathrm{GL}(n,\mathbf Z) \mathrm{GL}(n,\mathbf Q)","It seems that the normalizer of $H=\mathrm{GL}(n,\mathbf Z)$ in $G=\mathrm{GL}(n,\mathbf Q)$ is ""almost"" equal to itself, that is,  $$ N_G(\mathrm{GL}(n,\mathbf Z))=Z(G) \cdot \mathrm{GL}(n,\mathbf Z) $$  where $Z(G)$ is the centre of $G.$  Is there a simple proof/disproof of this fact? More generally, for which integral domains $R$ it is known that $\mathrm{GL}(n,R)$ ""almost"" coincides with its normalizer in the group $\mathrm{GL}(n,Q(R))$ where $Q(R)$ is the quotient field of $R?$","It seems that the normalizer of $H=\mathrm{GL}(n,\mathbf Z)$ in $G=\mathrm{GL}(n,\mathbf Q)$ is ""almost"" equal to itself, that is,  $$ N_G(\mathrm{GL}(n,\mathbf Z))=Z(G) \cdot \mathrm{GL}(n,\mathbf Z) $$  where $Z(G)$ is the centre of $G.$  Is there a simple proof/disproof of this fact? More generally, for which integral domains $R$ it is known that $\mathrm{GL}(n,R)$ ""almost"" coincides with its normalizer in the group $\mathrm{GL}(n,Q(R))$ where $Q(R)$ is the quotient field of $R?$",,"['linear-algebra', 'group-theory']"
52,"Show that $\phi: \mathbb{R}_3[x]\rightarrow\mathbb{R}^3, \phi(p):=[p(-1), p(0), p(1)] $ is a linear transformation",Show that  is a linear transformation,"\phi: \mathbb{R}_3[x]\rightarrow\mathbb{R}^3, \phi(p):=[p(-1), p(0), p(1)] ","Let $\mathbb{R}_3[x]$ be a vector space of polynomials p with degree $\leq3$ and show that $\phi: \mathbb{R}_3[x]\rightarrow\mathbb{R}^3, \phi(p):=[p(-1), p(0), p(1)] $ is a linear transformation. Now I know that transformation is linear if these two conditions are true: A linear transformation between two vector spaces $V$ and $W$ is a map $T:V\rightarrow W$ such that the following hold: $T(v_1+v_2)=T(v_1)+T(v_2)$ for any vectors $v_1$ and $v_2$ in $V$, and $T(\alpha v)=\alpha T(v)$ for any scalar $\alpha$. Let $p(x)=\alpha p_1(x)+\beta p_2(x)$, now we have $\phi(p(x)) = \phi(\alpha p_1(x)+\beta p_2(x))=\left[\begin{array}{c}\alpha p_1(-1)+\beta p_2(-1)\\\alpha p_1(0)+\beta p_2(0)\\\alpha p_1(1)+\beta p_2(1)\end{array}\right]=\alpha\left[\begin{array}{c} p_1(-1)\\p_1(0)\\p_1(1)\end{array}\right]+\beta\left[\begin{array}{c} p_2(-1)\\p_2(0)\\p_2(1)\end{array}\right]$ Does this prove that the transformation is linear?","Let $\mathbb{R}_3[x]$ be a vector space of polynomials p with degree $\leq3$ and show that $\phi: \mathbb{R}_3[x]\rightarrow\mathbb{R}^3, \phi(p):=[p(-1), p(0), p(1)] $ is a linear transformation. Now I know that transformation is linear if these two conditions are true: A linear transformation between two vector spaces $V$ and $W$ is a map $T:V\rightarrow W$ such that the following hold: $T(v_1+v_2)=T(v_1)+T(v_2)$ for any vectors $v_1$ and $v_2$ in $V$, and $T(\alpha v)=\alpha T(v)$ for any scalar $\alpha$. Let $p(x)=\alpha p_1(x)+\beta p_2(x)$, now we have $\phi(p(x)) = \phi(\alpha p_1(x)+\beta p_2(x))=\left[\begin{array}{c}\alpha p_1(-1)+\beta p_2(-1)\\\alpha p_1(0)+\beta p_2(0)\\\alpha p_1(1)+\beta p_2(1)\end{array}\right]=\alpha\left[\begin{array}{c} p_1(-1)\\p_1(0)\\p_1(1)\end{array}\right]+\beta\left[\begin{array}{c} p_2(-1)\\p_2(0)\\p_2(1)\end{array}\right]$ Does this prove that the transformation is linear?",,"['linear-algebra', 'transformation']"
53,How can I show that $\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^n = \begin{pmatrix} 1 & n \\ 0 & 1\end{pmatrix}$?,How can I show that ?,\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^n = \begin{pmatrix} 1 & n \\ 0 & 1\end{pmatrix},"Well, the original task was to figure out what the following expression evaluates to for any $n$. $$\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^{\large n}$$ By trying out different values of $n$, I found the pattern: $$\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^{\large n} = \begin{pmatrix} 1 & n \\ 0 & 1\end{pmatrix}$$ But I have yet to figure out how to prove it algebraically. Suggestions?","Well, the original task was to figure out what the following expression evaluates to for any $n$. $$\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^{\large n}$$ By trying out different values of $n$, I found the pattern: $$\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^{\large n} = \begin{pmatrix} 1 & n \\ 0 & 1\end{pmatrix}$$ But I have yet to figure out how to prove it algebraically. Suggestions?",,['linear-algebra']
54,Is it possible to have a $3 \times 3$ matrix that is both orthogonal and skew-symmetric?,Is it possible to have a  matrix that is both orthogonal and skew-symmetric?,3 \times 3,Is it possible to have a $3 \times 3$ matrix that is both orthogonal and skew-symmetric? I know it has something to do with the odd order of the matrix and it is not possible to have such a matrix. But what is the reason?,Is it possible to have a $3 \times 3$ matrix that is both orthogonal and skew-symmetric? I know it has something to do with the odd order of the matrix and it is not possible to have such a matrix. But what is the reason?,,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'skew-symmetric-matrices']"
55,A finite-dimensional vector space cannot be covered by finitely many proper subspaces?,A finite-dimensional vector space cannot be covered by finitely many proper subspaces?,,"Let $V$ be a finite-dimensional vector space, $V_i$ is a proper subspace of $V$ for every $1\leq i\leq m$ for some integer $m$. In my linear algebra text, I've seen a result that $V$ can never be covered by $\{V_i\}$, but I don't know how to prove it correctly. I've written down my false proof below: First we may prove the result when $V_i$ is a codimension-1 subspace. Since $codim(V_i)=1$, we can pick a vector $e_i\in V$ s.t. $V_i\oplus\mathcal{L}(e_i)=V$, where $\mathcal{L}(v)$ is the linear subspace span by $v$. Then we choose $e=e_1+\cdots+e_m$, I want to show that none of $V_i$ contains $e$ but I failed. Could you tell me a simple and corrected proof to this result? Ideas of proof are also welcome~ Remark : As @Jim Conant mentioned that this is possible for finite field, I assume the base field of $V$ to be a number field.","Let $V$ be a finite-dimensional vector space, $V_i$ is a proper subspace of $V$ for every $1\leq i\leq m$ for some integer $m$. In my linear algebra text, I've seen a result that $V$ can never be covered by $\{V_i\}$, but I don't know how to prove it correctly. I've written down my false proof below: First we may prove the result when $V_i$ is a codimension-1 subspace. Since $codim(V_i)=1$, we can pick a vector $e_i\in V$ s.t. $V_i\oplus\mathcal{L}(e_i)=V$, where $\mathcal{L}(v)$ is the linear subspace span by $v$. Then we choose $e=e_1+\cdots+e_m$, I want to show that none of $V_i$ contains $e$ but I failed. Could you tell me a simple and corrected proof to this result? Ideas of proof are also welcome~ Remark : As @Jim Conant mentioned that this is possible for finite field, I assume the base field of $V$ to be a number field.",,['linear-algebra']
56,More Theoretical and Less Computational Linear Algebra Textbook,More Theoretical and Less Computational Linear Algebra Textbook,,"I found what seems to be a good linear algebra book.  However, I want a more theoretical as opposed to computational linear algebra book.  The book is Linear Algebra with Applications 7th edition by Gareth Williams.  How high quality is this?  Will it provide me with a good background in linear algebra?","I found what seems to be a good linear algebra book.  However, I want a more theoretical as opposed to computational linear algebra book.  The book is Linear Algebra with Applications 7th edition by Gareth Williams.  How high quality is this?  Will it provide me with a good background in linear algebra?",,"['linear-algebra', 'reference-request']"
57,General expression for determinant of a block-diagonal matrix,General expression for determinant of a block-diagonal matrix,,"Consider having a matrix whose structure is the following: $$ A = \begin{pmatrix} a_{1,1} & a_{1,2} & a_{1,3} & 0 & 0 & 0 & 0 & 0 & 0\\ a_{2,1} & a_{2,2} & a_{2,3} & 0 & 0 & 0 & 0 & 0 & 0\\ a_{3,1} & a_{3,2} & a_{3,3} & 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & a_{4,4} & a_{4,5} & a_{4,6} & 0 & 0 & 0\\ 0 & 0 & 0 & a_{5,4} & a_{5,5} & a_{5,6} & 0 & 0 & 0\\ 0 & 0 & 0 & a_{6,4} & a_{6,5} & a_{6,6} & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0 & a_{7,7} & a_{7,8} & a_{7,9}\\ 0 & 0 & 0 & 0 & 0 & 0 & a_{8,7} & a_{8,8} & a_{8,9}\\ 0 & 0 & 0 & 0 & 0 & 0 & a_{9,7} & a_{9,8} & a_{9,9}\\ \end{pmatrix} $$ Question. What about its determinant $|A|$?. Another question I was wondering that maybe matrix $A$ can be expressed as a product of particular matrices to have such a structure... maybe using these matrices: $$ A_1 =  \begin{pmatrix} a_{1,1} & a_{1,2} & a_{1,3}\\ a_{2,1} & a_{2,2} & a_{2,3}\\ a_{3,1} & a_{3,2} & a_{3,3}\\ \end{pmatrix} $$ $$ A_2 =  \begin{pmatrix} a_{4,4} & a_{4,5} & a_{4,6}\\ a_{5,4} & a_{5,5} & a_{5,6}\\ a_{6,4} & a_{6,5} & a_{6,6}\\ \end{pmatrix} $$ $$ A_2 =  \begin{pmatrix} a_{7,7} & a_{7,8} & a_{7,9}\\ a_{8,7} & a_{8,8} & a_{8,9}\\ a_{9,7} & a_{9,8} & a_{9,9}\\ \end{pmatrix} $$ I can arrange $A$ as a compination of those: $A = f(A_1,A_2,A_3)$ Kronecker product One possibility can be the Kronecker product: $$ A= \begin{pmatrix} 1 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 0\\ \end{pmatrix} \otimes A_1 + \begin{pmatrix} 0 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 0\\ \end{pmatrix} \otimes A_2 + \begin{pmatrix} 0 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 1\\ \end{pmatrix} \cdot A_3 $$ But what about the determinant??? There are sums in this case which is not good...","Consider having a matrix whose structure is the following: $$ A = \begin{pmatrix} a_{1,1} & a_{1,2} & a_{1,3} & 0 & 0 & 0 & 0 & 0 & 0\\ a_{2,1} & a_{2,2} & a_{2,3} & 0 & 0 & 0 & 0 & 0 & 0\\ a_{3,1} & a_{3,2} & a_{3,3} & 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & a_{4,4} & a_{4,5} & a_{4,6} & 0 & 0 & 0\\ 0 & 0 & 0 & a_{5,4} & a_{5,5} & a_{5,6} & 0 & 0 & 0\\ 0 & 0 & 0 & a_{6,4} & a_{6,5} & a_{6,6} & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0 & a_{7,7} & a_{7,8} & a_{7,9}\\ 0 & 0 & 0 & 0 & 0 & 0 & a_{8,7} & a_{8,8} & a_{8,9}\\ 0 & 0 & 0 & 0 & 0 & 0 & a_{9,7} & a_{9,8} & a_{9,9}\\ \end{pmatrix} $$ Question. What about its determinant $|A|$?. Another question I was wondering that maybe matrix $A$ can be expressed as a product of particular matrices to have such a structure... maybe using these matrices: $$ A_1 =  \begin{pmatrix} a_{1,1} & a_{1,2} & a_{1,3}\\ a_{2,1} & a_{2,2} & a_{2,3}\\ a_{3,1} & a_{3,2} & a_{3,3}\\ \end{pmatrix} $$ $$ A_2 =  \begin{pmatrix} a_{4,4} & a_{4,5} & a_{4,6}\\ a_{5,4} & a_{5,5} & a_{5,6}\\ a_{6,4} & a_{6,5} & a_{6,6}\\ \end{pmatrix} $$ $$ A_2 =  \begin{pmatrix} a_{7,7} & a_{7,8} & a_{7,9}\\ a_{8,7} & a_{8,8} & a_{8,9}\\ a_{9,7} & a_{9,8} & a_{9,9}\\ \end{pmatrix} $$ I can arrange $A$ as a compination of those: $A = f(A_1,A_2,A_3)$ Kronecker product One possibility can be the Kronecker product: $$ A= \begin{pmatrix} 1 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 0\\ \end{pmatrix} \otimes A_1 + \begin{pmatrix} 0 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 0\\ \end{pmatrix} \otimes A_2 + \begin{pmatrix} 0 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 1\\ \end{pmatrix} \cdot A_3 $$ But what about the determinant??? There are sums in this case which is not good...",,"['linear-algebra', 'matrices', 'determinant', 'exterior-algebra', 'block-matrices']"
58,Are determinants always real?,Are determinants always real?,,"I've just realized that I'm not sure about the answer to this. Are determinants always real-valued? Determinants can be calculated as the product of eigenvalues.  Eigenvalues can be complex-valued.  Thus surely determinants can be complex-valued?  Yet, I've never calculated a complex-valued determinant.  So have I just not encountered one yet, or are determinants always real for some reason? Note: I'm inclined to think they must be real because you can say that a determinant is just a generalized volume of a parallelotope made by the column vectors as the sides.  But then again, how do you construct a parallelotope out of complex vectors?","I've just realized that I'm not sure about the answer to this. Are determinants always real-valued? Determinants can be calculated as the product of eigenvalues.  Eigenvalues can be complex-valued.  Thus surely determinants can be complex-valued?  Yet, I've never calculated a complex-valued determinant.  So have I just not encountered one yet, or are determinants always real for some reason? Note: I'm inclined to think they must be real because you can say that a determinant is just a generalized volume of a parallelotope made by the column vectors as the sides.  But then again, how do you construct a parallelotope out of complex vectors?",,['linear-algebra']
59,Is a linear vector space a vector space?,Is a linear vector space a vector space?,,"On the first page of the classical book ""Ordinary Differential Equations"" by Jack Hale (Revised Edition, 1980) there is the following definition: An abstract linear vector space (or linear space ) $\mathcal{X}$ over $\mathbb{R}$ is a collection of elements $\{x,y,\ldots\}$ such that for each $x,y \in \mathcal{X}$, the sum $x+y$ is defined, $x+y \in \mathcal{X}$, $x+y=y+x$ and there is an element $0 \in \mathcal{X}$ such that $x+0=x$ for all $x \in \mathcal{X}$. Also, for any number $a,b \in\mathbb{R}$, scalar multiplication $ax$ is defined, $ax \in \mathcal{X}$ and $1 \cdot x = x$, $(ab)x=a(bx)=b(ax)$, $(a+b)x=ax+bx$ for all $x,y \in \mathcal{X}$. The terminology linear vector space is the same as vector space (i.e., without the adjective linear )? I am asking this because a classical axiom of vector spaces is missing here: given an $x \in \mathcal{X}$ there is an element $z \in \mathcal{X}$ such that $x+z=0$, where the element $0$ was defined above. Question improvement : with respect to the definition of vector space , more axioms seem to be missing too, namely the associativity under $+$ and the scalar distributivity as $a(x+y) = ax + ay$. This was mentioned by more than one comment/post of contributors. Why is that?","On the first page of the classical book ""Ordinary Differential Equations"" by Jack Hale (Revised Edition, 1980) there is the following definition: An abstract linear vector space (or linear space ) $\mathcal{X}$ over $\mathbb{R}$ is a collection of elements $\{x,y,\ldots\}$ such that for each $x,y \in \mathcal{X}$, the sum $x+y$ is defined, $x+y \in \mathcal{X}$, $x+y=y+x$ and there is an element $0 \in \mathcal{X}$ such that $x+0=x$ for all $x \in \mathcal{X}$. Also, for any number $a,b \in\mathbb{R}$, scalar multiplication $ax$ is defined, $ax \in \mathcal{X}$ and $1 \cdot x = x$, $(ab)x=a(bx)=b(ax)$, $(a+b)x=ax+bx$ for all $x,y \in \mathcal{X}$. The terminology linear vector space is the same as vector space (i.e., without the adjective linear )? I am asking this because a classical axiom of vector spaces is missing here: given an $x \in \mathcal{X}$ there is an element $z \in \mathcal{X}$ such that $x+z=0$, where the element $0$ was defined above. Question improvement : with respect to the definition of vector space , more axioms seem to be missing too, namely the associativity under $+$ and the scalar distributivity as $a(x+y) = ax + ay$. This was mentioned by more than one comment/post of contributors. Why is that?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
60,Expected Value of a Determinant,Expected Value of a Determinant,,"Suppose that I construct an $n \times n$ matrix $A$ such that each entry of $A$ is a random integer in the range $[1, \, n]$. I'd like to calculate the expected value of $\det(A)$. My conjecture is that the answer is zero, though I could very well be incorrect. Running some numerical experiments with different values for $n$ and a large number of trials, it seems that $\mathbb{E}[\det(A)]$ is normally in the range $[0.25, \, 0.7]$, so I'm starting to lose faith in my intuition that it is zero. Could anyone lend some advice on how to approach this problem and what strategies I may want to consider applying?","Suppose that I construct an $n \times n$ matrix $A$ such that each entry of $A$ is a random integer in the range $[1, \, n]$. I'd like to calculate the expected value of $\det(A)$. My conjecture is that the answer is zero, though I could very well be incorrect. Running some numerical experiments with different values for $n$ and a large number of trials, it seems that $\mathbb{E}[\det(A)]$ is normally in the range $[0.25, \, 0.7]$, so I'm starting to lose faith in my intuition that it is zero. Could anyone lend some advice on how to approach this problem and what strategies I may want to consider applying?",,"['linear-algebra', 'probability', 'matrices', 'determinant']"
61,Text suggestion for linear algebra and geometry,Text suggestion for linear algebra and geometry,,"I want to study more linear algebra over the summer, specifically relating it to geometry. I was originally going to read Shafarevich's Linear Algebra & Geometry , after a recommendation, but it has no exercises. Can anyone suggest a similar text? As for my related background, I learned linear algebra from Hubbard's Vector Calculus text, I've worked through most of Axler's LADR , and through chapter 5 or 6 of Artin.","I want to study more linear algebra over the summer, specifically relating it to geometry. I was originally going to read Shafarevich's Linear Algebra & Geometry , after a recommendation, but it has no exercises. Can anyone suggest a similar text? As for my related background, I learned linear algebra from Hubbard's Vector Calculus text, I've worked through most of Axler's LADR , and through chapter 5 or 6 of Artin.",,"['linear-algebra', 'geometry']"
62,The rank of a linear transformation/matrix,The rank of a linear transformation/matrix,,"I'm terribly confused on the concept of ""rank of a linear transformation"". My book keeps using it, but it doesn't clarify what it means (or at least I haven't been able to find it). Is it the same as the rank of the matrix? For example, if A is a mxn matrix, what would be the rank(A)?","I'm terribly confused on the concept of ""rank of a linear transformation"". My book keeps using it, but it doesn't clarify what it means (or at least I haven't been able to find it). Is it the same as the rank of the matrix? For example, if A is a mxn matrix, what would be the rank(A)?",,"['linear-algebra', 'matrices', 'linear-transformations']"
63,Can there be a multiplicative linear operator for real square matrices?,Can there be a multiplicative linear operator for real square matrices?,,"Let me be more precise: Is there an $f: \mathbb R^{n \times n}\to \mathbb R$ such that for any square matrices $A$ and $B$ of the same order $n \times n$ we have: $f(A+B) = f(A) + f(B)$ $f(kA) = kf(A), k \in \mathbb R$ $f(AB) = f(A)f(B)$ Can there be such an $f$ ? It is clear that $f(0) = 0, f(-A) = -f(A), f(I) =1 $ (or $0$ ). Do we get some contradiction from this? All nilpotent matrices have $f=0$ EDIT: A few interesting corollaries that one of your answers allowed me to arrive to: There is also no operator that satisfies only $1$ and $3$ to all square matrices but the trivial one $f \equiv 0$ . If an operator satisfies $1$ and $2$ , then it must be a random (but fixed) linear combination of the elements of the matrix, like the trace or just any random element of fixed coordinates.","Let me be more precise: Is there an such that for any square matrices and of the same order we have: Can there be such an ? It is clear that (or ). Do we get some contradiction from this? All nilpotent matrices have EDIT: A few interesting corollaries that one of your answers allowed me to arrive to: There is also no operator that satisfies only and to all square matrices but the trivial one . If an operator satisfies and , then it must be a random (but fixed) linear combination of the elements of the matrix, like the trace or just any random element of fixed coordinates.","f: \mathbb R^{n \times n}\to \mathbb R A B n \times n f(A+B) = f(A) + f(B) f(kA) = kf(A), k \in \mathbb R f(AB) = f(A)f(B) f f(0) = 0, f(-A) = -f(A), f(I) =1  0 f=0 1 3 f \equiv 0 1 2","['linear-algebra', 'matrices', 'linear-transformations']"
64,What does orthogonality mean in function space?,What does orthogonality mean in function space?,,"The functions $x$ and $x^2 - {1\over2}$ are orthogonal with respect to their inner product on the interval [0, 1]. However, when you graph the two functions, they do not look orthogonal at all. So what does it truly mean for two functions to be orthogonal?","The functions $x$ and $x^2 - {1\over2}$ are orthogonal with respect to their inner product on the interval [0, 1]. However, when you graph the two functions, they do not look orthogonal at all. So what does it truly mean for two functions to be orthogonal?",,"['linear-algebra', 'functions', 'inner-products', 'orthogonal-polynomials']"
65,Proving that a matrix is invertible without using determinants,Proving that a matrix is invertible without using determinants,,"Prove if $A$, $B$, and $C$ are square matrices and $ABC = I$, then $B$ is invertible and  $B^{-1}= CA$. I know that this proof can be done by taking the determinant of $ABC=I$ and showing that $A$, $B$, and $C$ are invertible and then finding the inverse of $B$. However, in this chapter of the book, we have not yet learned determinants so I would like to solve the problem without determinants. My proof method involves using a contradiction and is as follows: Assume ${C^{-1}}$ does not exist, then $\exists$ $x$ $\neq$ $0$ such that $Cx = 0$. $ABCx =Ix$ $AB0 = x$ $0=x$, which is a contradiction since we know that $x$ $\neq$ $0$, and therefore ${C^{-1}}$ exists. $AB$${C^{-1}}$ $=I$${C^{-1}}$ $AB=$${C^{-1}}$ WLOG, B is invertible $CAB =C$${C^{-1}}$ $CAB = I$ $CAB$${B^{-1}}$ $=I$${B^{-1}}$ $CA=$${B^{-1}}$ My question is if it is correct to assume ${C^{-1}}$ does not exist since the proof does not mention anything about ${C^{-1}}$ existing or not.","Prove if $A$, $B$, and $C$ are square matrices and $ABC = I$, then $B$ is invertible and  $B^{-1}= CA$. I know that this proof can be done by taking the determinant of $ABC=I$ and showing that $A$, $B$, and $C$ are invertible and then finding the inverse of $B$. However, in this chapter of the book, we have not yet learned determinants so I would like to solve the problem without determinants. My proof method involves using a contradiction and is as follows: Assume ${C^{-1}}$ does not exist, then $\exists$ $x$ $\neq$ $0$ such that $Cx = 0$. $ABCx =Ix$ $AB0 = x$ $0=x$, which is a contradiction since we know that $x$ $\neq$ $0$, and therefore ${C^{-1}}$ exists. $AB$${C^{-1}}$ $=I$${C^{-1}}$ $AB=$${C^{-1}}$ WLOG, B is invertible $CAB =C$${C^{-1}}$ $CAB = I$ $CAB$${B^{-1}}$ $=I$${B^{-1}}$ $CA=$${B^{-1}}$ My question is if it is correct to assume ${C^{-1}}$ does not exist since the proof does not mention anything about ${C^{-1}}$ existing or not.",,"['linear-algebra', 'proof-verification', 'alternative-proof']"
66,How to calculate the determinant of all-ones matrix minus the identity? [duplicate],How to calculate the determinant of all-ones matrix minus the identity? [duplicate],,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 3 years ago . How do I calculate the determinant of the following $n\times n$ matrices $$\begin {bmatrix} 0 & 1 & \ldots & 1 \\ 1 & 0 & \ldots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & ... & 0 \end {bmatrix}$$ and the same matrix but one of columns replaced only with $1$ s? In the  above matrix all off-diagonal elements are $1$ and diagonal elements are $0$ .","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 3 years ago . How do I calculate the determinant of the following matrices and the same matrix but one of columns replaced only with s? In the  above matrix all off-diagonal elements are and diagonal elements are .","n\times n \begin {bmatrix}
0 & 1 & \ldots & 1 \\
1 & 0 & \ldots & 1 \\
\vdots & \vdots & \ddots & \vdots \\
1 & 1 & ... & 0
\end {bmatrix} 1 1 0","['linear-algebra', 'matrices', 'determinant']"
67,Does swapping columns of a matrix cause the rows of the inverse matrix to be swapped?,Does swapping columns of a matrix cause the rows of the inverse matrix to be swapped?,,"This question came up while I was performing some computation on a few matrices on an unrelated problem in computer science. Let $A$ be an invertible matrix with columns $A_1, \dots A_n$ . Let $B$ be its inverse, with rows $B_1, \dots, B_n$ . Now construct a new matrix $\hat{A}$ by swapping two columns $A_i$ and $A_j$ . Let $\hat{B}$ be the inverse of $\hat{A}$ . In my specific computations, I noticed that $\hat{B}$ was nothing more than $B$ with the rows $B_i$ and $B_j$ swapped. Is this always true? I doubt it was a coincidence because the numbers were quite random. Note: I was working with real numbers but I'd be interested to know if the field makes any difference.","This question came up while I was performing some computation on a few matrices on an unrelated problem in computer science. Let be an invertible matrix with columns . Let be its inverse, with rows . Now construct a new matrix by swapping two columns and . Let be the inverse of . In my specific computations, I noticed that was nothing more than with the rows and swapped. Is this always true? I doubt it was a coincidence because the numbers were quite random. Note: I was working with real numbers but I'd be interested to know if the field makes any difference.","A A_1, \dots A_n B B_1, \dots, B_n \hat{A} A_i A_j \hat{B} \hat{A} \hat{B} B B_i B_j","['linear-algebra', 'inverse']"
68,Derivative of the $l_p$ norm [closed],Derivative of the  norm [closed],l_p,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question The $l_p$-norm of the vector $\mathbf{x}$ is defined as $$\Vert \mathbf{x} \Vert_p = \left(\sum_i |x_i|^p\right)^{1/p}$$ I want to calculate the following derivative. Any hint is appreciated. $$\frac{\partial}{\partial \mathbf{x}}\Vert \mathbf{x} \Vert_p $$ Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question The $l_p$-norm of the vector $\mathbf{x}$ is defined as $$\Vert \mathbf{x} \Vert_p = \left(\sum_i |x_i|^p\right)^{1/p}$$ I want to calculate the following derivative. Any hint is appreciated. $$\frac{\partial}{\partial \mathbf{x}}\Vert \mathbf{x} \Vert_p $$ Thanks.",,"['linear-algebra', 'vectors']"
69,Alternative definition of the determinant of a square matrix and its advantages?,Alternative definition of the determinant of a square matrix and its advantages?,,"Usually, the definition of the determinant of a $n\times n$ matrix $A=(a_{ij})$ is as the following: $$\det(A):=\sum_{\sigma\in S_n}\text{sgn}(\sigma)\prod_{i=1}^na_{i,\sigma(i)}.$$ In Gilbert Strang's Linear Algebra and Its Application , the author points out that it can be understood as the volume of a box in $n$-dimensional space. However, I am wondering if it can be defined in this way. Here are my questions : [ EDIT : Can the determinant be defined as the volume of a box in $n$-dimensional space?] Are there other definitions of the determinant for a square matrix? What is the ""advantages"" of the different definition? [ EDIT : For instance, as Qiaochu said in the comment, how easy is it to prove that $\det(AB)=\det(A)\det(B)$ with that definition?]","Usually, the definition of the determinant of a $n\times n$ matrix $A=(a_{ij})$ is as the following: $$\det(A):=\sum_{\sigma\in S_n}\text{sgn}(\sigma)\prod_{i=1}^na_{i,\sigma(i)}.$$ In Gilbert Strang's Linear Algebra and Its Application , the author points out that it can be understood as the volume of a box in $n$-dimensional space. However, I am wondering if it can be defined in this way. Here are my questions : [ EDIT : Can the determinant be defined as the volume of a box in $n$-dimensional space?] Are there other definitions of the determinant for a square matrix? What is the ""advantages"" of the different definition? [ EDIT : For instance, as Qiaochu said in the comment, how easy is it to prove that $\det(AB)=\det(A)\det(B)$ with that definition?]",,['linear-algebra']
70,"Wrong but fun and/or useful ""proofs"" in linear algebra","Wrong but fun and/or useful ""proofs"" in linear algebra",,"I wonder if anyone can share wrong but useful and/or fun proofs in linear algebra. It can not only be fun, but also useful for someone who learns the subject. You are welcome to add explanations but please hide them in order not to spoil the fun. Let me start from my own ""proof"" that every square matrix has zero determinant (I am sure that ""proof"" was discovered many many times). ""Theorem"". Every square matrix $A$ over any field $K$ has zero determinant. ""Proof"". If $A$ has two equal rows or columns we are done. Otherwise, we will construct them. First let add all the rows except the first row to the first row. Next let add all the rows except the last row to the last row. Clearly, now first and last row are equal since both or them are sum of every row in $A$ , so $\det(A) = 0$ . $\Box$ After we added rows to the first row of $A$ it changed, but the ""proof"" assumes that the first row is the same.","I wonder if anyone can share wrong but useful and/or fun proofs in linear algebra. It can not only be fun, but also useful for someone who learns the subject. You are welcome to add explanations but please hide them in order not to spoil the fun. Let me start from my own ""proof"" that every square matrix has zero determinant (I am sure that ""proof"" was discovered many many times). ""Theorem"". Every square matrix over any field has zero determinant. ""Proof"". If has two equal rows or columns we are done. Otherwise, we will construct them. First let add all the rows except the first row to the first row. Next let add all the rows except the last row to the last row. Clearly, now first and last row are equal since both or them are sum of every row in , so . After we added rows to the first row of it changed, but the ""proof"" assumes that the first row is the same.",A K A A \det(A) = 0 \Box A,['linear-algebra']
71,Rigorously proving that a change-of-basis matrix is always invertible,Rigorously proving that a change-of-basis matrix is always invertible,,"How can we prove that all change-of-basis matrices are invertible? The trivial case when it's a change of basis for $\mathbb{R^{n}}$ is easily demonstratable using, for example, determinants. But I am struggling to rigorously show this for all bases, for example for a two-dimensional subspace of $\mathbb{R^{4}}$. I am sure that there are many ways to go about this proof, and I would be very appreciative for as many ways of demonstration as possible, to back up my intuition!","How can we prove that all change-of-basis matrices are invertible? The trivial case when it's a change of basis for $\mathbb{R^{n}}$ is easily demonstratable using, for example, determinants. But I am struggling to rigorously show this for all bases, for example for a two-dimensional subspace of $\mathbb{R^{4}}$. I am sure that there are many ways to go about this proof, and I would be very appreciative for as many ways of demonstration as possible, to back up my intuition!",,[]
72,Transpose of product of matrices [duplicate],Transpose of product of matrices [duplicate],,This question already has answers here : How to prove $(AB)^T=B^T A^T$ (5 answers) Closed 5 years ago . How do you prove the following fact about the transpose of a product of matrices? Also can you give some intuition as to why it is so. $(AB)^T = B^TA^T$,This question already has answers here : How to prove $(AB)^T=B^T A^T$ (5 answers) Closed 5 years ago . How do you prove the following fact about the transpose of a product of matrices? Also can you give some intuition as to why it is so.,(AB)^T = B^TA^T,[]
73,What does it mean to work without a basis?,What does it mean to work without a basis?,,"When reading proofs or definitions on Wikipedia, I'm accustomed to seeing both a basis-dependent discussion and basis-free discussion. Take, for example, this page on the tensor product, which has a ""free vector space"" discussion alongside a matrix representation discussion. While I understand not all vector spaces have a canonical basis, it's not formally clear to me what it means for a proof or definition to be basis-dependent vs. basis-free, and why it's better to be basis-free in the first place. 1) If I'm writing a proof or defining an object, what rules or criteria must I follow to be properly basis free? And, once I know what a basis-dependent proof or definition looks like, what is the strategy from generalizing it to a basis-independent proof or definition? 2) If all vector spaces can be represented by a (not necessarily canonical) basis, can't we always represent operators and members of that that space with matrices and linearly independent sums? My larger question, is then, if we're really making no assumptions when write down matrices or element-wise operations, why is it bad or ungentlemanly to choose a basis without loss of generality?","When reading proofs or definitions on Wikipedia, I'm accustomed to seeing both a basis-dependent discussion and basis-free discussion. Take, for example, this page on the tensor product, which has a ""free vector space"" discussion alongside a matrix representation discussion. While I understand not all vector spaces have a canonical basis, it's not formally clear to me what it means for a proof or definition to be basis-dependent vs. basis-free, and why it's better to be basis-free in the first place. 1) If I'm writing a proof or defining an object, what rules or criteria must I follow to be properly basis free? And, once I know what a basis-dependent proof or definition looks like, what is the strategy from generalizing it to a basis-independent proof or definition? 2) If all vector spaces can be represented by a (not necessarily canonical) basis, can't we always represent operators and members of that that space with matrices and linearly independent sums? My larger question, is then, if we're really making no assumptions when write down matrices or element-wise operations, why is it bad or ungentlemanly to choose a basis without loss of generality?",,['linear-algebra']
74,Find the rotation axis and angle of a matrix,Find the rotation axis and angle of a matrix,,$$A=\frac{1}{9} \begin{pmatrix} -7 & 4 & 4\\  4 & -1 & 8\\  4 & 8 & -1 \end{pmatrix}$$ How do I prove that A is a rotation ? How do I find the rotation axis and the rotation angle ?,$$A=\frac{1}{9} \begin{pmatrix} -7 & 4 & 4\\  4 & -1 & 8\\  4 & 8 & -1 \end{pmatrix}$$ How do I prove that A is a rotation ? How do I find the rotation axis and the rotation angle ?,,['linear-algebra']
75,Prove that an involutory matrix has eigenvalues $\pm 1$,Prove that an involutory matrix has eigenvalues,\pm 1,"I'm trying to prove that an involutory matrix (a matrix where $A=A^{-1}$ ) has only eigenvalues $\pm 1$ . I've been able to prove that $\det(A) = \pm 1$ , but that only shows that the product of the eigenvalues is equal to $\pm 1$ , not the eigenvalues themselves. Does anybody have an idea for how the proof might go?","I'm trying to prove that an involutory matrix (a matrix where ) has only eigenvalues . I've been able to prove that , but that only shows that the product of the eigenvalues is equal to , not the eigenvalues themselves. Does anybody have an idea for how the proof might go?",A=A^{-1} \pm 1 \det(A) = \pm 1 \pm 1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'involutions']"
76,Prove that determinant complex conjugate is complex conjugate of determinant,Prove that determinant complex conjugate is complex conjugate of determinant,,"Prove that determinant complex conjugate is complex conjugate of determinant. I know that there is an induction proof for this. However, I am wondering if there is a more elegant and simple solution that shows this fact. Thanks.","Prove that determinant complex conjugate is complex conjugate of determinant. I know that there is an induction proof for this. However, I am wondering if there is a more elegant and simple solution that shows this fact. Thanks.",,['linear-algebra']
77,Diagonalizable matrix $A$ invertible also?,Diagonalizable matrix  invertible also?,A,"If a matrix $A$ is diagonalizable, is $A$ invertible? I know that $P^{-1}AP = \text{some diagonal matrix}$ and therefore $P$ is invertible, but not sure of $A$ itself.","If a matrix $A$ is diagonalizable, is $A$ invertible? I know that $P^{-1}AP = \text{some diagonal matrix}$ and therefore $P$ is invertible, but not sure of $A$ itself.",,"['linear-algebra', 'matrices']"
78,Does taking the dot product of two column vectors involve converting one of the vectors into row vectors first?,Does taking the dot product of two column vectors involve converting one of the vectors into row vectors first?,,"If you have two vectors living in subspace $V$ and you want to take dot product, it seems that you cannot technically do this operation because if you write both vectors in matrix form, they would both be column vectors living in the same subspace. In order to take the dot product, you would need to convert one of the vectors into a row vector which lives in a completely different dual subspace $V^*$ and then take the dot product of this dual space vector with the column vector. Is all of this true?","If you have two vectors living in subspace and you want to take dot product, it seems that you cannot technically do this operation because if you write both vectors in matrix form, they would both be column vectors living in the same subspace. In order to take the dot product, you would need to convert one of the vectors into a row vector which lives in a completely different dual subspace and then take the dot product of this dual space vector with the column vector. Is all of this true?",V V^*,"['linear-algebra', 'vector-spaces', 'inner-products']"
79,Does eigenvectors of a matrix change during matrix operations?,Does eigenvectors of a matrix change during matrix operations?,,If I have a matrix $A$ with two eigenvectors $x$ and $y$.  What will be the eigenvectors of $$A^2 - 3A + 4I ?$$ I know that if we take powers of $A$ then the eigenvectors remain unchanged. But I am not quite sure about the above matrix .,If I have a matrix $A$ with two eigenvectors $x$ and $y$.  What will be the eigenvectors of $$A^2 - 3A + 4I ?$$ I know that if we take powers of $A$ then the eigenvectors remain unchanged. But I am not quite sure about the above matrix .,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
80,Maximize the value of $v^{T}Av$,Maximize the value of,v^{T}Av,"Let $A$ be a symmetric, real matrix. The goal is to find a unit vector $v$ such that the value $v^{T}Av$ is maximized, and minimized. The answer is that $v$ should be the eigenvector of $A$ with largest eigenvalue, and smallest eigenvalue. Could anyone give an explanation why? What do eigenvectors have to do with maximizing such a number? Please make it as 'step by step' as possible, express it using very basic algebra (if possible). I don't quite understand MooS' answer.","Let $A$ be a symmetric, real matrix. The goal is to find a unit vector $v$ such that the value $v^{T}Av$ is maximized, and minimized. The answer is that $v$ should be the eigenvector of $A$ with largest eigenvalue, and smallest eigenvalue. Could anyone give an explanation why? What do eigenvectors have to do with maximizing such a number? Please make it as 'step by step' as possible, express it using very basic algebra (if possible). I don't quite understand MooS' answer.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'non-convex-optimization', 'qcqp']"
81,Determinant of a triangular matrix,Determinant of a triangular matrix,,"Using the cofactor expansion, explain why the determinant of a triangular matrix is the product of the elements on its diagonal. Is it the fact that there are $0$ 's in the $L$ or $U$ part of the matrix and that somehow comes into play to where only the diagonal is accounted for? I'm not quite sure.","Using the cofactor expansion, explain why the determinant of a triangular matrix is the product of the elements on its diagonal. Is it the fact that there are 's in the or part of the matrix and that somehow comes into play to where only the diagonal is accounted for? I'm not quite sure.",0 L U,"['linear-algebra', 'matrices', 'determinant']"
82,Why is the inverse of an orthogonal matrix equal to its transpose? [duplicate],Why is the inverse of an orthogonal matrix equal to its transpose? [duplicate],,This question already has answers here : Why is inverse of orthogonal matrix is its transpose? (5 answers) Closed 2 years ago . I don't get why that's the case. Or is it a definition? The way the concept was presented to me was that an orthogonal matrix has orthonormal columns. And that's it.,This question already has answers here : Why is inverse of orthogonal matrix is its transpose? (5 answers) Closed 2 years ago . I don't get why that's the case. Or is it a definition? The way the concept was presented to me was that an orthogonal matrix has orthonormal columns. And that's it.,,"['linear-algebra', 'matrices']"
83,Computing the Jordan Form of a Matrix,Computing the Jordan Form of a Matrix,,"I apologize if this has already been answered, but I've seen multiple examples of how to compute Jordan Canonical Forms of a matrix, and I still don't really get it. Could someone help me out with this? What I know for certain is that I must start off by finding my eigenvalues, and corresponding eigenvectors. OR, (how it was taught in class from my understanding), I can simply plug in the eigenvalues into my original matrix and find the rank. I have no clue what to do from there though... I also know that my Jordan Normal Forms should look like these: $$\begin{pmatrix} \lambda_1 & 0 & 0\\ 0 & \lambda_2 & 0\\ 0 & 0 & \lambda_3\\ \end{pmatrix}$$ or $$\begin{pmatrix} \lambda_1 & 1 & 0\\ 0 & \lambda_1 & 0\\ 0 & 0 & \lambda_2\\ \end{pmatrix}$$ And if we switch 1 and 2, then the 1 will be on the other side of the top. Lastly, $$\begin{pmatrix} \lambda & 1 & 0\\ 0 & \lambda & 1\\ 0 & 0 & \lambda\\ \end{pmatrix}$$ I've seen from many sources that if given a matrix J (specifically 3x3) that is our Jordan normal form, and we have our matrix A, then there is some P such that $PAP^{-1}=J$. Here's an example matrix if I could possibly get an explanation on how this works through an example: $$\begin{pmatrix} -7 & 8 & 2\\ -4 & 5 & 1\\ -23 & 21 & 7\\ \end{pmatrix}$$ I don't know how to fill the information in the middle. For instance, what do I do after I find the rank of my matrix or what do I do once I find my rank? Sorry if I made mistakes, very tired, and please try to make this as coherent as possible, because I'm so confused. This is an Advanced Linear Algebra course. Any help is greatly appreciated!","I apologize if this has already been answered, but I've seen multiple examples of how to compute Jordan Canonical Forms of a matrix, and I still don't really get it. Could someone help me out with this? What I know for certain is that I must start off by finding my eigenvalues, and corresponding eigenvectors. OR, (how it was taught in class from my understanding), I can simply plug in the eigenvalues into my original matrix and find the rank. I have no clue what to do from there though... I also know that my Jordan Normal Forms should look like these: $$\begin{pmatrix} \lambda_1 & 0 & 0\\ 0 & \lambda_2 & 0\\ 0 & 0 & \lambda_3\\ \end{pmatrix}$$ or $$\begin{pmatrix} \lambda_1 & 1 & 0\\ 0 & \lambda_1 & 0\\ 0 & 0 & \lambda_2\\ \end{pmatrix}$$ And if we switch 1 and 2, then the 1 will be on the other side of the top. Lastly, $$\begin{pmatrix} \lambda & 1 & 0\\ 0 & \lambda & 1\\ 0 & 0 & \lambda\\ \end{pmatrix}$$ I've seen from many sources that if given a matrix J (specifically 3x3) that is our Jordan normal form, and we have our matrix A, then there is some P such that $PAP^{-1}=J$. Here's an example matrix if I could possibly get an explanation on how this works through an example: $$\begin{pmatrix} -7 & 8 & 2\\ -4 & 5 & 1\\ -23 & 21 & 7\\ \end{pmatrix}$$ I don't know how to fill the information in the middle. For instance, what do I do after I find the rank of my matrix or what do I do once I find my rank? Sorry if I made mistakes, very tired, and please try to make this as coherent as possible, because I'm so confused. This is an Advanced Linear Algebra course. Any help is greatly appreciated!",,"['linear-algebra', 'matrices']"
84,Connection between eigenvalues and eigenvectors of a matrix in different bases,Connection between eigenvalues and eigenvectors of a matrix in different bases,,If you have a matrix $A$ you can find its eigenvalues and eigenvectors. If you represent this matrix relative to another basis $\mathcal{D}$ you can again find its eigenvectors and eigenvectors. My questions What is the connection between the eigenvalues and eigenvectors of this same matrix in different bases? Why is this so? And how do you interpret the eigenvalues and eigenvectors in these different bases? Could you please also provide some intuition and an example? Thank you! EDIT I created a follow-up question with a more complicated example here: Example of eigenvectors in different bases (follow-up question),If you have a matrix $A$ you can find its eigenvalues and eigenvectors. If you represent this matrix relative to another basis $\mathcal{D}$ you can again find its eigenvectors and eigenvectors. My questions What is the connection between the eigenvalues and eigenvectors of this same matrix in different bases? Why is this so? And how do you interpret the eigenvalues and eigenvectors in these different bases? Could you please also provide some intuition and an example? Thank you! EDIT I created a follow-up question with a more complicated example here: Example of eigenvectors in different bases (follow-up question),,"['linear-algebra', 'eigenvalues-eigenvectors']"
85,Importance of Linear Algebra [closed],Importance of Linear Algebra [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question In one of his online lectures Benedict Gross comments that one can never have too much Linear Algebra.  Also, looking around it seems like I can find comments to the effect that Linear Algebra has more importance than other sub-disciplines of mathematics.  If true, why is Linear Algebra the most important sub-discipline of mathematics?  If Benedict Gross is correct, why can't one have enough Linear Algebra?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question In one of his online lectures Benedict Gross comments that one can never have too much Linear Algebra.  Also, looking around it seems like I can find comments to the effect that Linear Algebra has more importance than other sub-disciplines of mathematics.  If true, why is Linear Algebra the most important sub-discipline of mathematics?  If Benedict Gross is correct, why can't one have enough Linear Algebra?",,"['linear-algebra', 'soft-question']"
86,Is the matrix $A$ diagonalizable if $A^2=I$,Is the matrix  diagonalizable if,A A^2=I,"If $A$ is an involutory matrix, i.e. $A^2=I$, then is $A$ diagonalizable?","If $A$ is an involutory matrix, i.e. $A^2=I$, then is $A$ diagonalizable?",,"['linear-algebra', 'matrices']"
87,Rigorous Text in Multivariable Calculus and Linear Algebra,Rigorous Text in Multivariable Calculus and Linear Algebra,,"So I'm wanting a solid math book for Christmas.  I have a solid background in Calculus and am currently working through baby Rudin.  I really want a rigorous book dealing with multivariable calculus and linear algebra.  How well does Apostol II do this?  Would this be a good continuation book from my current study?  If not, what book would you suggest? EDIT:  I really like Apostol's Multivariable Calculus and Linear Algebra book and its format.  Will this book accomplish my purpose?  Do you think it is a good book?","So I'm wanting a solid math book for Christmas.  I have a solid background in Calculus and am currently working through baby Rudin.  I really want a rigorous book dealing with multivariable calculus and linear algebra.  How well does Apostol II do this?  Would this be a good continuation book from my current study?  If not, what book would you suggest? EDIT:  I really like Apostol's Multivariable Calculus and Linear Algebra book and its format.  Will this book accomplish my purpose?  Do you think it is a good book?",,"['linear-algebra', 'analysis', 'reference-request', 'self-learning']"
88,Intuitive explanation of the Fundamental Theorem of Linear Algebra,Intuitive explanation of the Fundamental Theorem of Linear Algebra,,"Can someone explain intuitively what the Fundamental Theorem of Linear Algebra states? and why specifically it is called the above? Specifically, what makes it 'Fundamental' in the broad scope of the theory.","Can someone explain intuitively what the Fundamental Theorem of Linear Algebra states? and why specifically it is called the above? Specifically, what makes it 'Fundamental' in the broad scope of the theory.",,"['linear-algebra', 'intuition']"
89,Why does the Gaussian-Jordan elimination work when finding the inverse matrix?,Why does the Gaussian-Jordan elimination work when finding the inverse matrix?,,"In order to find the inverse matrix $A^{-1}$, one can apply Gaussian-Jordan elimination to the augmented matrix $$(A \mid I)$$ to obtain $$(I \mid C),$$ where $C$ is indeed $A^{-1}$. However, I fail to see why this actually works, and reading this answer didn't really clear things up for me.","In order to find the inverse matrix $A^{-1}$, one can apply Gaussian-Jordan elimination to the augmented matrix $$(A \mid I)$$ to obtain $$(I \mid C),$$ where $C$ is indeed $A^{-1}$. However, I fail to see why this actually works, and reading this answer didn't really clear things up for me.",,"['linear-algebra', 'matrices', 'inverse', 'gaussian-elimination']"
90,What is the difference between linear transformation and linear operator?,What is the difference between linear transformation and linear operator?,,"What is the difference between linear transformation and linear operator ? In our linear algebra class, we learned that, if $$\textbf{T}:\textbf{V}\rightarrow\textbf{W}\quad\vec{v},\vec{u}\in\textbf{V}$$ $$\textbf{T}(\vec{v}+\vec{u})=\textbf{T}\vec{v}+\textbf{T}\vec{u}$$ $$\textbf{T}(c\vec{v})=c\textbf{T}(\vec{v})\quad\textbf{c}\in\mathbb{R}$$ then $\textbf{T}$ is linear transformation from $\textbf{V}$ to $\textbf{W}$ .","What is the difference between linear transformation and linear operator ? In our linear algebra class, we learned that, if then is linear transformation from to .","\textbf{T}:\textbf{V}\rightarrow\textbf{W}\quad\vec{v},\vec{u}\in\textbf{V} \textbf{T}(\vec{v}+\vec{u})=\textbf{T}\vec{v}+\textbf{T}\vec{u} \textbf{T}(c\vec{v})=c\textbf{T}(\vec{v})\quad\textbf{c}\in\mathbb{R} \textbf{T} \textbf{V} \textbf{W}","['linear-algebra', 'operator-theory', 'linear-transformations']"
91,Existence of the Pfaffian?,Existence of the Pfaffian?,,"Consider a square skew-symmetric $n\times n$ matrix $A$. We know that $\det(A)=\det(A^T)=(-1)^n\det(A)$, so if $n$ is odd, the determinant vanishes. If $n$ is even, my book claims that the determinant is the square of a polynomial function of the entries, and Wikipedia confirms this. The polynomial in question is called the Pfaffian . I was wondering if there was an easy (clean, conceptual) way to show that this is the case, without mucking around with the symmetric group.","Consider a square skew-symmetric $n\times n$ matrix $A$. We know that $\det(A)=\det(A^T)=(-1)^n\det(A)$, so if $n$ is odd, the determinant vanishes. If $n$ is even, my book claims that the determinant is the square of a polynomial function of the entries, and Wikipedia confirms this. The polynomial in question is called the Pfaffian . I was wondering if there was an easy (clean, conceptual) way to show that this is the case, without mucking around with the symmetric group.",,"['linear-algebra', 'matrices', 'pfaffian', 'skew-symmetric-matrices']"
92,Any neat way to calculate this Vandermonde-like determinant?,Any neat way to calculate this Vandermonde-like determinant?,,"Let $x_i,i\in\{1,\cdots,n\}$ be real numbers, and $s_k=x_1^k+\cdots+x_n^k$, I'm asked to calculate $$     |S|:=     \begin{vmatrix}       s_0 & s_1 & s_2 & \cdots & s_{n-1}\\       s_1 & s_2 & s_3 & \cdots & s_n\\       s_2 & s_3 & s_4 & \cdots & s_{n+1}\\       \vdots & \vdots & \vdots & \ddots & \vdots \\       s_{n-1} & s_{n} & s_{n+1} & \cdots & s_{2n-2}     \end{vmatrix} $$ and to prove that $|S|\ge 0$ for all possible real $x_i$. I found that $$ |S|=\det[(v_1+\cdots v_n), (x_1v_1+\cdots+x_nv_n),\cdots,(x_1^{n-1}v_1+\cdots+x_n^{n-1}v_n)],\quad\text{where}\, v_j=\begin{bmatrix}   1 \\ x_j \\ \vdots\\ x_j^{n-1} \end{bmatrix} $$ Due to multilinearity of the $\det$ function, I sense it might have something to do with Vandermonde determinant. In fact, it must have the form $$|S|=(\det[v_1,\cdots, v_n])\cdot \text{something}$$ But that ""something"" involves many cyclic sums and is therefore a horrible mess.. Anyway, is there a neat way to calculate this tricky determinant? Thanks!","Let $x_i,i\in\{1,\cdots,n\}$ be real numbers, and $s_k=x_1^k+\cdots+x_n^k$, I'm asked to calculate $$     |S|:=     \begin{vmatrix}       s_0 & s_1 & s_2 & \cdots & s_{n-1}\\       s_1 & s_2 & s_3 & \cdots & s_n\\       s_2 & s_3 & s_4 & \cdots & s_{n+1}\\       \vdots & \vdots & \vdots & \ddots & \vdots \\       s_{n-1} & s_{n} & s_{n+1} & \cdots & s_{2n-2}     \end{vmatrix} $$ and to prove that $|S|\ge 0$ for all possible real $x_i$. I found that $$ |S|=\det[(v_1+\cdots v_n), (x_1v_1+\cdots+x_nv_n),\cdots,(x_1^{n-1}v_1+\cdots+x_n^{n-1}v_n)],\quad\text{where}\, v_j=\begin{bmatrix}   1 \\ x_j \\ \vdots\\ x_j^{n-1} \end{bmatrix} $$ Due to multilinearity of the $\det$ function, I sense it might have something to do with Vandermonde determinant. In fact, it must have the form $$|S|=(\det[v_1,\cdots, v_n])\cdot \text{something}$$ But that ""something"" involves many cyclic sums and is therefore a horrible mess.. Anyway, is there a neat way to calculate this tricky determinant? Thanks!",,"['linear-algebra', 'matrices', 'determinant', 'hankel-matrices']"
93,Is a bra the adjoint of a ket?,Is a bra the adjoint of a ket?,,"The instructor in my quantum computation course sometimes uses the equivalence $$(\left|a\right>)^\dagger\equiv\left<a\right|$$ I understand that this is true for the typical matrix implementation of Dirac's bra-ket notation, and taking $x^\dagger\equiv\bar{x}^T$; but does it follow from the definition of an inner product space and the adjoint? Should I be able to derive this as a fact from definition of an inner product space and the adjoint of linear operators on that space or is it simply a notational convention?","The instructor in my quantum computation course sometimes uses the equivalence $$(\left|a\right>)^\dagger\equiv\left<a\right|$$ I understand that this is true for the typical matrix implementation of Dirac's bra-ket notation, and taking $x^\dagger\equiv\bar{x}^T$; but does it follow from the definition of an inner product space and the adjoint? Should I be able to derive this as a fact from definition of an inner product space and the adjoint of linear operators on that space or is it simply a notational convention?",,"['linear-algebra', 'notation', 'inner-products', 'quantum-computation']"
94,Linear Algebra: determine whether the sets span the same subspace,Linear Algebra: determine whether the sets span the same subspace,,"So I am stuck on 51 here: 51. Determine whether the sets $S_1$ and $S_2$ span the same subspace of $\mathbb{R}^3$:   $$\begin{align*} S_1 &= \Bigl\{ (1,2,-1),\ (0,1,1),\ (2,5,-1)\Bigr\}\\ S_2 &= \Bigl\{ (-2,-6,0),\ (1,1,-2)\Bigr\} \end{align*}$$ What I did to solve it was to multiply each vector in set $S_1$ by $C$, add the vectors together and set them to zero. Once I reduced the resulting matrix I got the following result: $$\begin{align*} c_1&=-2t\\ c_2&=-t\\ c_3&=t \end{align*}$$ So this result would be linearly dependent. Then I did the same thing for the second set and I got that $c_1=c_2=0$ which means that it is linearly independent. In conclusion, I said that set $S_1$ and $S_2$ does not span the same subspace of $\mathbb{R}^3$. The book says they do. Could anyone point out where I went wrong? Thanks.","So I am stuck on 51 here: 51. Determine whether the sets $S_1$ and $S_2$ span the same subspace of $\mathbb{R}^3$:   $$\begin{align*} S_1 &= \Bigl\{ (1,2,-1),\ (0,1,1),\ (2,5,-1)\Bigr\}\\ S_2 &= \Bigl\{ (-2,-6,0),\ (1,1,-2)\Bigr\} \end{align*}$$ What I did to solve it was to multiply each vector in set $S_1$ by $C$, add the vectors together and set them to zero. Once I reduced the resulting matrix I got the following result: $$\begin{align*} c_1&=-2t\\ c_2&=-t\\ c_3&=t \end{align*}$$ So this result would be linearly dependent. Then I did the same thing for the second set and I got that $c_1=c_2=0$ which means that it is linearly independent. In conclusion, I said that set $S_1$ and $S_2$ does not span the same subspace of $\mathbb{R}^3$. The book says they do. Could anyone point out where I went wrong? Thanks.",,['linear-algebra']
95,Continuity of matrix inversion,Continuity of matrix inversion,,"Show that the set $U \subset \mathbb{R}^{n^{2}}$ of matrices $A$ with $\det(A) \neq 0$ is open. Let $A^{-1}$ be the inverse of the matrix $A$ . Show that the mapping $A \mapsto A^{-1}$ is continuous from $U$ to $U$ . My solution to the first part is that $\det(A)$ can be expressed as a polynomial in the entries of $A$ , and since polynomial functions are continuous we have that the determinant function is continuous from which we can say the given set is indeed open. Any thoughts on the next part?","Show that the set of matrices with is open. Let be the inverse of the matrix . Show that the mapping is continuous from to . My solution to the first part is that can be expressed as a polynomial in the entries of , and since polynomial functions are continuous we have that the determinant function is continuous from which we can say the given set is indeed open. Any thoughts on the next part?",U \subset \mathbb{R}^{n^{2}} A \det(A) \neq 0 A^{-1} A A \mapsto A^{-1} U U \det(A) A,"['linear-algebra', 'matrices', 'analysis', 'continuity', 'inverse']"
96,"Where does the Pythagorean theorem ""fit"" within modern mathematics?","Where does the Pythagorean theorem ""fit"" within modern mathematics?",,"I am interested in how today's professional mathematicians view the Pythagorean theorem, in terms of how the theorem fits within the axiomatic framework of mathematics.  I often come across textbooks that define length by the Pythagorean theorem, so that the theorem is in essence a definition or axiom.  In more modern mathematics such as linear algebra, is the Pythagorean theorem generally just used as the definition of length?  Is it more conventional today to treat the Pythagorean theorem as a definition (or axiom) rather than a theorem?  Are there any modern proofs of the Pythagorean theorem that don't rely on Euclidean geometry (like a proof that utilizes linear algebra/the dot product, etc.)?","I am interested in how today's professional mathematicians view the Pythagorean theorem, in terms of how the theorem fits within the axiomatic framework of mathematics.  I often come across textbooks that define length by the Pythagorean theorem, so that the theorem is in essence a definition or axiom.  In more modern mathematics such as linear algebra, is the Pythagorean theorem generally just used as the definition of length?  Is it more conventional today to treat the Pythagorean theorem as a definition (or axiom) rather than a theorem?  Are there any modern proofs of the Pythagorean theorem that don't rely on Euclidean geometry (like a proof that utilizes linear algebra/the dot product, etc.)?",,"['linear-algebra', 'euclidean-geometry']"
97,$\ker T\subset \ker S\Rightarrow S=rT$ when $S$ and $T$ are linear functionals,when  and  are linear functionals,\ker T\subset \ker S\Rightarrow S=rT S T,"I would like only a hint to the following exercise: Let $V$ be a vector space over the field $K$, and $T$, $S$ linear functionals on V such that $Tv=0\Rightarrow Sv=0$. Prove that there exists $r\in K$ such that $S=rT$. I know how to prove this when $V$ is finite dimensional. I show that if there is no such constant $r$ then $n-2=\operatorname{dim}\textrm{ }(\ker\textrm{ }T\textrm{ }\cap \ker\textrm{ }S)=\operatorname{dim}\ker T=n-1$, a contradiction. But this approach doesn't seem to help at all for the stated problem.","I would like only a hint to the following exercise: Let $V$ be a vector space over the field $K$, and $T$, $S$ linear functionals on V such that $Tv=0\Rightarrow Sv=0$. Prove that there exists $r\in K$ such that $S=rT$. I know how to prove this when $V$ is finite dimensional. I show that if there is no such constant $r$ then $n-2=\operatorname{dim}\textrm{ }(\ker\textrm{ }T\textrm{ }\cap \ker\textrm{ }S)=\operatorname{dim}\ker T=n-1$, a contradiction. But this approach doesn't seem to help at all for the stated problem.",,['linear-algebra']
98,Change of Basis Confusion,Change of Basis Confusion,,"I am horribly confused by the cluster of terminology and operations surrounding ""change of basis"" operations. Finding alternate references on this topic only seems to add to the confusion as there doesn't appear to be a consistent approach to defining and notating these operations. Perhaps someone will be able to clarify just one simple aspect of this which is as follows: Let $u = \{u_1, \dots, u_n \}$ and and $w = \{w_1, \dots, w_n\}$ be bases for a vector space $V$. Then, necessarily, there exists a unique linear operator $T:V \rightarrow V$ such that $T(u_i) = w_i$. Now, the most natural thing in the world to call the matrix of this operator is the change of basis matrix from $u$ to $w$. Give this operator a vector in $u$ and it spits out a vector in $w$. Now, whether it is correct I don't know, but I've seen the matrix of this operator called the change of basis matrix from $w$ to $u$, reversing the target and source bases. This latter interpretation makes no sense because because it takes vectors in $u$ and produces vectors in $w$! I've seen this interpretation in more than one place so it can't just be a fluke. So...which is it?","I am horribly confused by the cluster of terminology and operations surrounding ""change of basis"" operations. Finding alternate references on this topic only seems to add to the confusion as there doesn't appear to be a consistent approach to defining and notating these operations. Perhaps someone will be able to clarify just one simple aspect of this which is as follows: Let $u = \{u_1, \dots, u_n \}$ and and $w = \{w_1, \dots, w_n\}$ be bases for a vector space $V$. Then, necessarily, there exists a unique linear operator $T:V \rightarrow V$ such that $T(u_i) = w_i$. Now, the most natural thing in the world to call the matrix of this operator is the change of basis matrix from $u$ to $w$. Give this operator a vector in $u$ and it spits out a vector in $w$. Now, whether it is correct I don't know, but I've seen the matrix of this operator called the change of basis matrix from $w$ to $u$, reversing the target and source bases. This latter interpretation makes no sense because because it takes vectors in $u$ and produces vectors in $w$! I've seen this interpretation in more than one place so it can't just be a fluke. So...which is it?",,"['linear-algebra', 'matrices', 'change-of-basis']"
99,Closed Subspaces of Vector Spaces,Closed Subspaces of Vector Spaces,,"Question: In Functional Analysis we can note things like: every closed subspace of a Banach space is Banach.  In this case, what does ""closed subspace"" mean? Does this mean closed under the norm topology? Or does this mean closed in the sense that multiplication of scalars and addition of vectors is closed? Or does this mean closed with respect to limits? I'm reviewing this material and I realized that even though I have this in my notes a number of times I am unsure of what this actually is.  I thought it was the second statement above, but the third statement makes the ""every closed subspace of a banach space is banach"" statement easy to prove.","Question: In Functional Analysis we can note things like: every closed subspace of a Banach space is Banach.  In this case, what does ""closed subspace"" mean? Does this mean closed under the norm topology? Or does this mean closed in the sense that multiplication of scalars and addition of vectors is closed? Or does this mean closed with respect to limits? I'm reviewing this material and I realized that even though I have this in my notes a number of times I am unsure of what this actually is.  I thought it was the second statement above, but the third statement makes the ""every closed subspace of a banach space is banach"" statement easy to prove.",,"['linear-algebra', 'general-topology']"
