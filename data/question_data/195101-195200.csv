,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Explicit inverse of $x\mapsto\frac{1}{\alpha} (x+1)^{\frac{\alpha - 1}{\alpha}} | x - 1 |^{\frac{1}{\alpha}} \frac{\alpha(x - 1) + 2}{(x- 1)(x + 1)}$,Explicit inverse of,x\mapsto\frac{1}{\alpha} (x+1)^{\frac{\alpha - 1}{\alpha}} | x - 1 |^{\frac{1}{\alpha}} \frac{\alpha(x - 1) + 2}{(x- 1)(x + 1)},"In the very same spirit as my previous question I am trying to find an explicit representation of the convex conjugate $f^*$ of the function $$ f \colon \mathbb R \to [0, \infty], \qquad x \mapsto | 1 - x |^{\frac{1}{a}} (1 + x)^{\frac{\alpha - 1}{\alpha}} + \iota_{[0, \infty)}(x), $$ where for any set $A$ , the convex indicator function $\iota_{A}(x)$ is equal to $0$ for $x \in A$ and equal to $\infty$ else and $\alpha \in (0, 1)$ . Up to edge cases, this first boils down to finding the inverse of $$ f'(x) = \frac{1}{\alpha} (x+1)^{\frac{\alpha - 1}{\alpha}} | x - 1 |^{\frac{1}{\alpha}} \frac{\alpha(x - 1) + 2}{(x- 1)(x + 1)} = \frac{1}{\alpha} \left| \frac{x-1}{x+1}\right|^{\frac{1}{\alpha}} \left(\alpha + \frac{2}{(x- 1)}\right) $$ (to obtain $f^*(y) = y (f')^{-1}(y) - f((f')^{-1}(y))$ .) Due to the last term in parenthesis, I do not know how to invert $f'$ , like it was done in this answer . We have that $f'$ is strictly monotonically increasing and continuous, so that it is bijective onto its range and hence invertible. For $\alpha = \frac{2}{3}$ , WolframAlpha gives an explicit inverse, which as a quite complicated expression , whereas for $\alpha = \frac{1}{\sqrt{2}}$ , it can't find an answer in terms of standard mathematical functions.","In the very same spirit as my previous question I am trying to find an explicit representation of the convex conjugate of the function where for any set , the convex indicator function is equal to for and equal to else and . Up to edge cases, this first boils down to finding the inverse of (to obtain .) Due to the last term in parenthesis, I do not know how to invert , like it was done in this answer . We have that is strictly monotonically increasing and continuous, so that it is bijective onto its range and hence invertible. For , WolframAlpha gives an explicit inverse, which as a quite complicated expression , whereas for , it can't find an answer in terms of standard mathematical functions.","f^* 
f \colon \mathbb R \to [0, \infty], \qquad x \mapsto | 1 - x |^{\frac{1}{a}} (1 + x)^{\frac{\alpha - 1}{\alpha}} + \iota_{[0, \infty)}(x),
 A \iota_{A}(x) 0 x \in A \infty \alpha \in (0, 1) 
f'(x)
= \frac{1}{\alpha} (x+1)^{\frac{\alpha - 1}{\alpha}} | x - 1 |^{\frac{1}{\alpha}} \frac{\alpha(x - 1) + 2}{(x- 1)(x + 1)}
= \frac{1}{\alpha} \left| \frac{x-1}{x+1}\right|^{\frac{1}{\alpha}} \left(\alpha + \frac{2}{(x- 1)}\right)
 f^*(y) = y (f')^{-1}(y) - f((f')^{-1}(y)) f' f' \alpha = \frac{2}{3} \alpha = \frac{1}{\sqrt{2}}","['real-analysis', 'derivatives', 'special-functions', 'closed-form', 'inverse-function']"
1,Intuition behind Picard's Thorem requiring f to be Lipschitz in the *second* variable (rather than the first variable),Intuition behind Picard's Thorem requiring f to be Lipschitz in the *second* variable (rather than the first variable),,"These are the statements of Picard's Theorem I've been taught Let $f:R\to\mathbb R$ be a function defined on the rectangle $R:= \{(x,y):|x-a|\leq h, |y-b|\leq k\}$ which satisfies f is continuous in R with bound M and $Mh\leq k$ f satisfies a Lipschitz condition in R ie. $\exists L>0:|f(x,y_1)-f(x,y_2)| \leq L|y_1-y_2| \  \forall x\in [a-h,a+h], \forall y_1,y_2 \in [b-k,b+k]$ Then the IVP $y'(x)=f(x,y(x))$ with $y(a)=b$ has a unique solution y on the interval [a-h,a+h] Suppose that $f:R\to\mathbb R$ is a continuous function on an unbounded rectangle $R = [c,d] \times \mathbb{R}$ which satisfies a global Lipschitz condition on R ie. $\exists L>0:|f(x,y_1)-f(x,y_2)| \leq L|y_1-y_2| \  \forall x\in [a-h,a+h], \forall y_1,y_2 \in \mathbb R$ . Then for any $a\in[c,d]$ and any $b\in\mathbb R$ the IVP has a unique solution that is defined on all of [c,d] As in the subject, I don't think I understand the intuition about why the Lipschitz condition is placed on the second variable rather than the first. I can follow the algebra but don't fully understand why it works. Further to this, in the global existence version of Picard's theorem, why do we look for a global Lipschitz condition in the second variable when we want a global solution for all x","These are the statements of Picard's Theorem I've been taught Let be a function defined on the rectangle which satisfies f is continuous in R with bound M and f satisfies a Lipschitz condition in R ie. Then the IVP with has a unique solution y on the interval [a-h,a+h] Suppose that is a continuous function on an unbounded rectangle which satisfies a global Lipschitz condition on R ie. . Then for any and any the IVP has a unique solution that is defined on all of [c,d] As in the subject, I don't think I understand the intuition about why the Lipschitz condition is placed on the second variable rather than the first. I can follow the algebra but don't fully understand why it works. Further to this, in the global existence version of Picard's theorem, why do we look for a global Lipschitz condition in the second variable when we want a global solution for all x","f:R\to\mathbb R R:= \{(x,y):|x-a|\leq h, |y-b|\leq k\} Mh\leq k \exists L>0:|f(x,y_1)-f(x,y_2)| \leq L|y_1-y_2| \  \forall x\in [a-h,a+h], \forall y_1,y_2 \in [b-k,b+k] y'(x)=f(x,y(x)) y(a)=b f:R\to\mathbb R R = [c,d] \times \mathbb{R} \exists L>0:|f(x,y_1)-f(x,y_2)| \leq L|y_1-y_2| \  \forall x\in [a-h,a+h], \forall y_1,y_2 \in \mathbb R a\in[c,d] b\in\mathbb R","['ordinary-differential-equations', 'derivatives']"
2,Prove that there is a zero of a derivative $x_0$ such that $|p(x_0)| < \frac{2}{3} |x_0|$,Prove that there is a zero of a derivative  such that,x_0 |p(x_0)| < \frac{2}{3} |x_0|,"Let $p(x)$ be $a_5 x^5 + a_4 x^4 + a_3 x^3 + a_2 x^2 + x, a_5 \neq 0$ , all zeros of $p'$ are from $\mathbb{R}$ . I need to prove that there is $x_0$ a zero of a derivative such that $|p(x_0)| < \frac{2}{3} |x_0|$ . $$p'(x) = 5 a_5 x^4 + 4 a_4 x^3 + 3 a_3 x^2 + 2 a_2 x + 1$$ From here I don't know what to do. I've tried assuming the opposite, but it got me nowhere. It's the first time I've come across this type of exercise, how should I approach this?","Let be , all zeros of are from . I need to prove that there is a zero of a derivative such that . From here I don't know what to do. I've tried assuming the opposite, but it got me nowhere. It's the first time I've come across this type of exercise, how should I approach this?","p(x) a_5 x^5 + a_4 x^4 + a_3 x^3 + a_2 x^2 + x, a_5 \neq 0 p' \mathbb{R} x_0 |p(x_0)| < \frac{2}{3} |x_0| p'(x) = 5 a_5 x^4 + 4 a_4 x^3 + 3 a_3 x^2 + 2 a_2 x + 1","['derivatives', 'polynomials']"
3,How could I prove that $g$ is differentiable?,How could I prove that  is differentiable?,g,"Let $x_1, \dots, x_{k - 1} \in \mathbb{R}^n$ be fixed and let $f \colon U \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m$ a $k$ times differentiable function. I want to prove that $g \colon U \longrightarrow \mathbb{R}^m$ given by $g(z) = D^{k - 1}f(z)\left(x_1, \dots, x_{k - 1}\right)$ is differentiable and that $Dg(z)\left(x_k\right) = D^kf(z)(x_k, x_1, \dots, x_{k - 1})$ . I think the proof I've got has a mistake. Here it is: $g$ can be written as the composition of $$D^{k - 1}f \colon U \longrightarrow \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right)$$ and $$\lambda \colon \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right) \longrightarrow \mathbb{R}^m,$$ where $\lambda(T) = T\left(x_1, \dots, x_{k - 1}\right)$ . Since $\lambda$ is linear, by the chain rule we can write, for $z \in U$ , $$Dg(z) = D\left(\lambda \circ D^{k - 1}(f)\right)(z) = D\lambda\left(D^{k - 1}f(z)\right) \circ D\left(D^{k - 1}f\right)(z) = \\$$ $$= \lambda \circ D\left[D^{k - 1}f\right](z) \in \mathcal{L}\left(\mathbb{R}^n, \mathbb{R}^m\right).$$ Consequently, for $x_k \in \mathbb{R}^n$ , $D\left[D^{k - 1}f\right](z)\left(x_k\right) \in \mathcal{L}\left(\mathbb{R}^n, \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right)\right) \equiv \mathcal{L}^k\left(\mathbb{R}^n; \mathbb{R}^m\right)$ , and we can write $$Dg(z)\left(x_k\right) = \lambda\left(D\left(D^{k - 1}f\right)(z)\left(x_k\right)\right) = D\left(D^{k - 1}f\right)(z)\left(x_k\right)\left(x_1, \dots, x_{k - 1}\right) = \\$$ $$= D^kf(z)\left(x_k, x_1, \dots, x_{k - 1}\right)$$ (we identify $D(Df)(z)(h)(k)$ with $D^2f(z)(h, k)$ ). I don't understand the last equalities. I don't really understand what is $\lambda\left(D\left(D^{k - 1}f\right)(z)\left(x_k\right)\right)$ , I mean, is that even defined? $D\left(D^{k - 1}f\right)(z)\left(x_k\right) \in \mathcal{L}^k\left(\mathbb{R}^n; \mathbb{R}^m\right)$ , so $\lambda$ shouldn't be defined there. It is really confusing, but could you help me? Thanks!","Let be fixed and let a times differentiable function. I want to prove that given by is differentiable and that . I think the proof I've got has a mistake. Here it is: can be written as the composition of and where . Since is linear, by the chain rule we can write, for , Consequently, for , , and we can write (we identify with ). I don't understand the last equalities. I don't really understand what is , I mean, is that even defined? , so shouldn't be defined there. It is really confusing, but could you help me? Thanks!","x_1, \dots, x_{k - 1} \in \mathbb{R}^n f \colon U \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m k g \colon U \longrightarrow \mathbb{R}^m g(z) = D^{k - 1}f(z)\left(x_1, \dots, x_{k - 1}\right) Dg(z)\left(x_k\right) = D^kf(z)(x_k, x_1, \dots, x_{k - 1}) g D^{k - 1}f \colon U \longrightarrow \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right) \lambda \colon \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right) \longrightarrow \mathbb{R}^m, \lambda(T) = T\left(x_1, \dots, x_{k - 1}\right) \lambda z \in U Dg(z) = D\left(\lambda \circ D^{k - 1}(f)\right)(z) = D\lambda\left(D^{k - 1}f(z)\right) \circ D\left(D^{k - 1}f\right)(z) = \\ = \lambda \circ D\left[D^{k - 1}f\right](z) \in \mathcal{L}\left(\mathbb{R}^n, \mathbb{R}^m\right). x_k \in \mathbb{R}^n D\left[D^{k - 1}f\right](z)\left(x_k\right) \in \mathcal{L}\left(\mathbb{R}^n, \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right)\right) \equiv \mathcal{L}^k\left(\mathbb{R}^n; \mathbb{R}^m\right) Dg(z)\left(x_k\right) = \lambda\left(D\left(D^{k - 1}f\right)(z)\left(x_k\right)\right) = D\left(D^{k - 1}f\right)(z)\left(x_k\right)\left(x_1, \dots, x_{k - 1}\right) = \\ = D^kf(z)\left(x_k, x_1, \dots, x_{k - 1}\right) D(Df)(z)(h)(k) D^2f(z)(h, k) \lambda\left(D\left(D^{k - 1}f\right)(z)\left(x_k\right)\right) D\left(D^{k - 1}f\right)(z)\left(x_k\right) \in \mathcal{L}^k\left(\mathbb{R}^n; \mathbb{R}^m\right) \lambda",['real-analysis']
4,Please help me with this calculus question [closed],Please help me with this calculus question [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question The coefficients of the quadratic function $f(x)$ including the constant term, are all rational and $f(x)$ has a local maximum at $x = 0$ . Let $g(x) =|f'(x)|e^{f(x)}$ have a maximum value of $4\sqrt{e}$ . If $g(x) = 4\sqrt{e}$ has rational solutions then: a) $\int_{-1}^0g(x)dx=e-\frac{1}{e^{7}}$ b) The value of the sgn $(f(0))=-1$ (where sgn is the sign function) c) $g(x)$ is not differentiable at one value of $x$ d) The value of $g\left(\tan{\frac{\pi}{4}}\right)=\frac{2}{e^7}$ (Since, I don't yet know how to write integrals on this website) The entire question is as follows","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question The coefficients of the quadratic function including the constant term, are all rational and has a local maximum at . Let have a maximum value of . If has rational solutions then: a) b) The value of the sgn (where sgn is the sign function) c) is not differentiable at one value of d) The value of (Since, I don't yet know how to write integrals on this website) The entire question is as follows",f(x) f(x) x = 0 g(x) =|f'(x)|e^{f(x)} 4\sqrt{e} g(x) = 4\sqrt{e} \int_{-1}^0g(x)dx=e-\frac{1}{e^{7}} (f(0))=-1 g(x) x g\left(\tan{\frac{\pi}{4}}\right)=\frac{2}{e^7},"['calculus', 'integration', 'derivatives']"
5,Use of elemental length in volume of a truncated cone,Use of elemental length in volume of a truncated cone,,"The typical way to compute the volume of a truncated cone is to slice into discs and calculate the volume of a differential cylinder. While doing that we first take the area of the disc $\pi f(x)^2$ and then multiply with thickness which we assume to be $dx$ and then get the final area is $\pi \int f(x)^2dx$ . But why should $dx$ be the thickness. As I have indicated in the picture,we are basically taking a tiny step along the surface which is $dl$ which again can be written as $\sqrt{(dy)^2+(dx)^2}$ . That changes the entire integration with a factor of $1+\sqrt{(\frac{dy}{dx})^2}$ and hence changes the volume? So which integral should be correct and why should $dx$ be the thickness and not $dL$ ? This issue is arising because if we were to calculate the surface area of this solid,we would have actually used $dL$ instead of $dx$ . So why are we taking two different thickness while accounting for the same differential cylinder? An image or animation will be greatly helpful in pointing out the difference,","The typical way to compute the volume of a truncated cone is to slice into discs and calculate the volume of a differential cylinder. While doing that we first take the area of the disc and then multiply with thickness which we assume to be and then get the final area is . But why should be the thickness. As I have indicated in the picture,we are basically taking a tiny step along the surface which is which again can be written as . That changes the entire integration with a factor of and hence changes the volume? So which integral should be correct and why should be the thickness and not ? This issue is arising because if we were to calculate the surface area of this solid,we would have actually used instead of . So why are we taking two different thickness while accounting for the same differential cylinder? An image or animation will be greatly helpful in pointing out the difference,",\pi f(x)^2 dx \pi \int f(x)^2dx dx dl \sqrt{(dy)^2+(dx)^2} 1+\sqrt{(\frac{dy}{dx})^2} dx dL dL dx,"['derivatives', 'definite-integrals', 'volume', 'infinitesimals']"
6,Can You Prove that a Function Is Holomorphic Without Appealing to Cauchy-Integral Formula,Can You Prove that a Function Is Holomorphic Without Appealing to Cauchy-Integral Formula,,"Suppose that the function $f: U \rightarrow \mathbb{C}$ is a holomorphic function on some domain $U$ . Assume that $U$ is simply connected connected and that the point $z_{0} \in U$ . Define a function $g(z)$ as follows: \ \ $$g(z) = \frac{f(z) - f(z_{0})}{z-z_{0}} \ if \ z \neq z_{0}$$ $$g(z) = f’(z_{0})$$ Is it possible to show, WITHOUT assuming the Cauchy Integral Formula, that $g(z)$ is complex differentiable (holomorphic) at $z=z_{0}$ . More generally, can it be shown that if a function $F: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ is real differentiable, satisfies Cauchy-Riemann-Equations on $U-\{ z_{0} \}$ , and if $\lim_{z \rightarrow z_{0}} F(z)$ is finite, then $F$ is real differentiable at $z = z_{0}$ and $F$ satisfies the Cauchy-Riemann Equations at $z = z_{0}$ . My initial thoughts were that we could use the continuity of the partial derivatives to show that the Cauchy-Riemann Equations must be satisfied and $z = z_{0}$ . We could also use the fact that $F$ is real differentiable on $U - \{ z_{0} \}$ along with continuity to show that $F$ is also real-differentiable at $z = z_{0}$ .","Suppose that the function is a holomorphic function on some domain . Assume that is simply connected connected and that the point . Define a function as follows: \ \ Is it possible to show, WITHOUT assuming the Cauchy Integral Formula, that is complex differentiable (holomorphic) at . More generally, can it be shown that if a function is real differentiable, satisfies Cauchy-Riemann-Equations on , and if is finite, then is real differentiable at and satisfies the Cauchy-Riemann Equations at . My initial thoughts were that we could use the continuity of the partial derivatives to show that the Cauchy-Riemann Equations must be satisfied and . We could also use the fact that is real differentiable on along with continuity to show that is also real-differentiable at .",f: U \rightarrow \mathbb{C} U U z_{0} \in U g(z) g(z) = \frac{f(z) - f(z_{0})}{z-z_{0}} \ if \ z \neq z_{0} g(z) = f’(z_{0}) g(z) z=z_{0} F: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2} U-\{ z_{0} \} \lim_{z \rightarrow z_{0}} F(z) F z = z_{0} F z = z_{0} z = z_{0} F U - \{ z_{0} \} F z = z_{0},"['complex-analysis', 'derivatives']"
7,"Find $f(x)$ : $ f'(x) = f(x)^2 + f^{-1}(x) + \int_{x}^{-\infty} \frac{e^t}{t} \, dt $",Find  :,"f(x)  f'(x) = f(x)^2 + f^{-1}(x) + \int_{x}^{-\infty} \frac{e^t}{t} \, dt ","\begin{align} f'(x) &= f(x)^2 + f^{-1}(x) + \int_{x}^{-\infty} \frac{e^t}{t} \, dt \end{align} How to find $f(x)$ What i do so far \begin{align} f'(x) &= f(x)^2 + f^{-1}(x) + \int_{x}^{-\infty} \frac{e^t}{t} , dt \end{align} \begin{align} (f^{-1})'(x) &= \frac{1}{f'(f^{-1}(x))} \end{align} \begin{align} f'(x) &= f(x)^2 + \frac{1}{f'(f^{-1}(x))} + \int_{x}^{-\infty} \frac{e^t}{t} , dt \end{align} u = f(x): \begin{align} u &= f(x) \ f'(x) &= \frac{du}{dx} \end{align} \begin{align} \frac{du}{dx} &= u^2 + \frac{1}{f'\left(f^{-1}(u)\right)} + \int_{x}^{-\infty} \frac{e^t}{t} , dt \end{align} \begin{align}\frac{du}{dx} &= u^2 + \frac{1}{f'\left(f^{-1}(u)\right)} - \frac{e^x}{x} \end{align}",How to find What i do so far u = f(x):,"\begin{align}
f'(x) &= f(x)^2 + f^{-1}(x) + \int_{x}^{-\infty} \frac{e^t}{t} \, dt
\end{align} f(x) \begin{align} f'(x) &= f(x)^2 + f^{-1}(x) + \int_{x}^{-\infty} \frac{e^t}{t} , dt \end{align} \begin{align} (f^{-1})'(x) &= \frac{1}{f'(f^{-1}(x))} \end{align} \begin{align} f'(x) &= f(x)^2 + \frac{1}{f'(f^{-1}(x))} + \int_{x}^{-\infty} \frac{e^t}{t} , dt \end{align} \begin{align} u &= f(x) \ f'(x) &= \frac{du}{dx} \end{align} \begin{align} \frac{du}{dx} &= u^2 + \frac{1}{f'\left(f^{-1}(u)\right)} + \int_{x}^{-\infty} \frac{e^t}{t} , dt \end{align} \begin{align}\frac{du}{dx} &= u^2 + \frac{1}{f'\left(f^{-1}(u)\right)} - \frac{e^x}{x} \end{align}","['ordinary-differential-equations', 'derivatives', 'solution-verification', 'exponential-function', 'inverse-function']"
8,Solving $f'(x)g(x)-f(x)g'(x)=c \in \mathbb{C}-\{0\}$,Solving,f'(x)g(x)-f(x)g'(x)=c \in \mathbb{C}-\{0\},"Let $c \in \mathbb{C}$ . How to find a general form of $f=f(x),g=g(x) \in \mathbb{C}[x]$ that satisfy $f'(x)g(x)-f(x)g'(x)=c$ ? I think I can solve this algebraically by writing $f=a_nx^n+\cdots+a_1x+a_0$ and $g=b_mx^m+\cdots+b_1x+b_0$ , where $n,m \geq 0$ , $a_i,b_j \in \mathbb{C}$ , with $a_nb_m \neq 0$ . Then $c=f'g-fg'=(a_nx^n+\cdots+a_1x+a_0)'(b_mx^m+\cdots+b_1x+b_0)-(a_nx^n+\cdots+a_1x+a_0)(b_mx^m+\cdots+b_1x+b_0)'= (na_nx^{n-1}+\cdots+a_1)(b_mx^m+\cdots+b_1x+b_0)-(a_nx^n+\cdots+a_1x+a_0)(mb_mx^{m-1}+\cdots+b_1)=a_nb_m(n-m)x^{n+m-1}+\cdots$ . There are two cases: (1) $n+m-1=0$ and $n-m \neq 0$ : Then $n+m=1$ and $n-m \neq 0$ . This implies $n=1,m=0$ or $n=0,m=1$ . Therefore, $f=a_1x+a_0, g=b_0$ with $a_1b_0=c$ or $f=a_0, g=b_1x+b_0$ with $-a_0b_1=c$ . (2) $n+m-1 > 0$ , hence $n-m=0$ , so $n=m$ , and also we should check all the other conditions until $a_1b_0-a_0b_1=c$ . This yields, for example: $f=x+a, g=x+b$ with $b-a=c$ . However, I wonder if there is a solution involving differential equations . I wish to integrate both sides, but do not know how: Divide both sides of $f'(x)g(x)-f(x)g'(x)=c$ by $g^2(x)$ ( $c \neq 0$ implies that $g(x) \neq 0$ ), so $(\frac{f}{g})'=\frac{f'(x)g(x)-f(x)g'(x)}{g^2(x)}=\frac{c}{g^2(x)}$ . But now I am not sure if it will help multiplying both sides by $-g'(x)$ since this will 'ruin' the LHS. Perhaps this will help somehow. Any help in solving this equation not algebraically is welcome; thank you! Edit: In case (2) I have not brought all the details; here is an example where $n=m=2$ : $f=x^2+ax+b$ and $g=x^2+cx+d$ , there are no such $f,g$ of degree $2$ (the original $c$ is now changed to $\tilde{c}$ , since we already use $c$ as one of the coefficients of $g$ ): $\tilde{c}=f'g-fg'=(c-a)x^2+2(d-b)x+(ad-bc)$ , hence: $c=a$ , $d=b$ and $ad-bc=\tilde{c} \neq 0$ , but applying the first and second conditions in the third yield: $0 \neq \tilde{c}=ad-bc=ab-ba=0$ , a contradiction. Similarly, I have checked $n=m=3$ , writing $f=x^3+ax^2+bx+c$ , $g=x^3+dx^2+ex+f$ , and again the last condition, based on former conditions, yields a contradiction. So it seems that a general form is necessarily that of case (1) : $f=a_1x+a_0, g=b_0$ with $a_1b_0=c$ or $f=a_0, g=b_1x+b_0$ with $-a_0b_1=c$ . (Since case (2) is impossible).","Let . How to find a general form of that satisfy ? I think I can solve this algebraically by writing and , where , , with . Then . There are two cases: (1) and : Then and . This implies or . Therefore, with or with . (2) , hence , so , and also we should check all the other conditions until . This yields, for example: with . However, I wonder if there is a solution involving differential equations . I wish to integrate both sides, but do not know how: Divide both sides of by ( implies that ), so . But now I am not sure if it will help multiplying both sides by since this will 'ruin' the LHS. Perhaps this will help somehow. Any help in solving this equation not algebraically is welcome; thank you! Edit: In case (2) I have not brought all the details; here is an example where : and , there are no such of degree (the original is now changed to , since we already use as one of the coefficients of ): , hence: , and , but applying the first and second conditions in the third yield: , a contradiction. Similarly, I have checked , writing , , and again the last condition, based on former conditions, yields a contradiction. So it seems that a general form is necessarily that of case (1) : with or with . (Since case (2) is impossible).","c \in \mathbb{C} f=f(x),g=g(x) \in \mathbb{C}[x] f'(x)g(x)-f(x)g'(x)=c f=a_nx^n+\cdots+a_1x+a_0 g=b_mx^m+\cdots+b_1x+b_0 n,m \geq 0 a_i,b_j \in \mathbb{C} a_nb_m \neq 0 c=f'g-fg'=(a_nx^n+\cdots+a_1x+a_0)'(b_mx^m+\cdots+b_1x+b_0)-(a_nx^n+\cdots+a_1x+a_0)(b_mx^m+\cdots+b_1x+b_0)'=
(na_nx^{n-1}+\cdots+a_1)(b_mx^m+\cdots+b_1x+b_0)-(a_nx^n+\cdots+a_1x+a_0)(mb_mx^{m-1}+\cdots+b_1)=a_nb_m(n-m)x^{n+m-1}+\cdots n+m-1=0 n-m \neq 0 n+m=1 n-m \neq 0 n=1,m=0 n=0,m=1 f=a_1x+a_0, g=b_0 a_1b_0=c f=a_0, g=b_1x+b_0 -a_0b_1=c n+m-1 > 0 n-m=0 n=m a_1b_0-a_0b_1=c f=x+a, g=x+b b-a=c f'(x)g(x)-f(x)g'(x)=c g^2(x) c \neq 0 g(x) \neq 0 (\frac{f}{g})'=\frac{f'(x)g(x)-f(x)g'(x)}{g^2(x)}=\frac{c}{g^2(x)} -g'(x) n=m=2 f=x^2+ax+b g=x^2+cx+d f,g 2 c \tilde{c} c g \tilde{c}=f'g-fg'=(c-a)x^2+2(d-b)x+(ad-bc) c=a d=b ad-bc=\tilde{c} \neq 0 0 \neq \tilde{c}=ad-bc=ab-ba=0 n=m=3 f=x^3+ax^2+bx+c g=x^3+dx^2+ex+f f=a_1x+a_0, g=b_0 a_1b_0=c f=a_0, g=b_1x+b_0 -a_0b_1=c","['ordinary-differential-equations', 'derivatives', 'polynomials', 'commutative-algebra']"
9,If $f'(x)$ is continuous at $a$ then prove that $\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a)$,If  is continuous at  then prove that,f'(x) a \lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a),"Prove that, $\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a)$ if $f'(x)$ is continuous at $a.$ This was how the question was presented in a regional book focusing upon Mean Value Theorems and no other details were given. My solution is as follows: We have, $$\lim_{x\to a}\frac{f(x)-f(a)}{x-a}=f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}=\lim_{h\to 0}\frac{f(a-h)-f(a)}{-h}=\lim_{h\to  0}\frac{f(a)-f(a-h)}{h}.$$ We note that, $$\lim_{h\to 0}\frac{f(a+h)-f(a)-f(a-h)+f(a)}{h}=\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{h}=2f'(a)\implies \lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a),$$ as required. However, the solution given in the book is, as follows: From Lagrange's Mean Value Theorem, we have, $f(a+h)=f(a)+hf'(a+\theta h),\theta\in (0,1)$ and $f(a)=f(a-h)+hf'(a-\theta ' h),\theta '\in (0,1).$ Adding these relations and rearranging we have, $$f(a+h)-f(a-h)=h(f'(a+\theta h)+f'(a-\theta ' h))\implies \frac{f(a+h)-f(a-h)}{2h}=\frac 12[f'(a+\theta h)+f'(a-\theta ' h)].$$ Taking limit $h\to 0$ on both sides, we have, $$\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=\frac 12\lim_{h\to 0}[f'(a+\theta h)+f'(a-\theta ' h)]=\frac 12\times 2f'(a)=f'(a).$$ However, I feel the approach given in the book has a major flaw. First of all, how did they apply Lagrange's Mean Value Theorem just like that without checking whether $f$ is continuous at $[a,a+h]$ and $[a-h,a].$ Also, in order to apply this theorem we need to have $f$ being differentiable in $(a,a+h)$ and $(a-h,a).$ They never guaranteed such claims holding true. I think, this makes the proof given in the book incorrect. Also, the information that, $f'(x)$ is continuous at $a$ seems totally useless and unnecessary! Lastly, is my solution at the beginning a valid one?","Prove that, if is continuous at This was how the question was presented in a regional book focusing upon Mean Value Theorems and no other details were given. My solution is as follows: We have, We note that, as required. However, the solution given in the book is, as follows: From Lagrange's Mean Value Theorem, we have, and Adding these relations and rearranging we have, Taking limit on both sides, we have, However, I feel the approach given in the book has a major flaw. First of all, how did they apply Lagrange's Mean Value Theorem just like that without checking whether is continuous at and Also, in order to apply this theorem we need to have being differentiable in and They never guaranteed such claims holding true. I think, this makes the proof given in the book incorrect. Also, the information that, is continuous at seems totally useless and unnecessary! Lastly, is my solution at the beginning a valid one?","\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a) f'(x) a. \lim_{x\to a}\frac{f(x)-f(a)}{x-a}=f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}=\lim_{h\to 0}\frac{f(a-h)-f(a)}{-h}=\lim_{h\to  0}\frac{f(a)-f(a-h)}{h}. \lim_{h\to 0}\frac{f(a+h)-f(a)-f(a-h)+f(a)}{h}=\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{h}=2f'(a)\implies \lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a), f(a+h)=f(a)+hf'(a+\theta h),\theta\in (0,1) f(a)=f(a-h)+hf'(a-\theta ' h),\theta '\in (0,1). f(a+h)-f(a-h)=h(f'(a+\theta h)+f'(a-\theta ' h))\implies \frac{f(a+h)-f(a-h)}{2h}=\frac 12[f'(a+\theta h)+f'(a-\theta ' h)]. h\to 0 \lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=\frac 12\lim_{h\to 0}[f'(a+\theta h)+f'(a-\theta ' h)]=\frac 12\times 2f'(a)=f'(a). f [a,a+h] [a-h,a]. f (a,a+h) (a-h,a). f'(x) a","['real-analysis', 'limits', 'derivatives', 'solution-verification', 'alternative-proof']"
10,Proving that there exists a $\xi\in \mathbb{R} $ such that $\ln f(\xi) = \xi$.,Proving that there exists a  such that .,\xi\in \mathbb{R}  \ln f(\xi) = \xi,"Given a function $f(x)$ that is differentiable over $(-\infty,+\infty)$ , and $|f'(x)| < k f(x)$ for some $0 < k < 1$ . Prove that there exists a $\xi\in  \mathbb{R}$ such that $\ln f(\xi) = \xi$ . I attempt a proof by contradiction but couldn't proceed. First, we assume that $F(x) = \ln f(x) - x<0$ ( or $>0$ ) consistently. Since $$\frac{f'(x)}{f(x)} = \frac{d}{dx} \ln f(x)$$ we obtain $$|\frac{d}{dx} \ln f(x)| < k < 1,$$ which lead to $F'(x) < 0$ consistently, indicating a strictly decreasing function. However, at this point, I struggle to establish $\ln f(x) < x$ consistently, resulting in a contradiction. I would appreciate it if someone could provide some insights. Thank you very much.","Given a function that is differentiable over , and for some . Prove that there exists a such that . I attempt a proof by contradiction but couldn't proceed. First, we assume that ( or ) consistently. Since we obtain which lead to consistently, indicating a strictly decreasing function. However, at this point, I struggle to establish consistently, resulting in a contradiction. I would appreciate it if someone could provide some insights. Thank you very much.","f(x) (-\infty,+\infty) |f'(x)| < k f(x) 0 < k < 1 \xi\in  \mathbb{R} \ln f(\xi) = \xi F(x) = \ln f(x) - x<0 >0 \frac{f'(x)}{f(x)} = \frac{d}{dx} \ln f(x) |\frac{d}{dx} \ln f(x)| < k < 1, F'(x) < 0 \ln f(x) < x","['analysis', 'derivatives']"
11,Must every Cauchy-Riemann condition be fulfilled simultaneously?,Must every Cauchy-Riemann condition be fulfilled simultaneously?,,"Working through problems in my complex analysis book, and I have to determine where the derivative exists for a function. I know that the derivative can exist only along a certain curve, however I don't know if that must be true simultaneously or not. For instance, one such function I've been given is $f(z) = \frac{ix + 1}{y}$ which gives $u(x,y) = \frac{1}{y}$ and $v(x,y) = \frac{x}{y}$ , and for the derivatives, you get $u_x = 0, v_y = \frac{-x}{y^2}, u_y = \frac{-1}{y^2}, and -v_x = \frac{-1}{y}$ . I know that this implies the derivative exists when $\frac{-x}{y^2} = 0$ (i.e. $x = 0$ ) and when $\frac{-1}{y^2} = \frac{-1}{y}$ (i.e. $y = 1$ ), but I don't know if this implies the derivative exists when EITHER of those things is true (derivative exists at $(0, a)$ and $(b, i)$ ) or only when BOTH of those things are true (derivative exists at $(0, i)$ ). I'm only asking for clarity on that front. Thank you!","Working through problems in my complex analysis book, and I have to determine where the derivative exists for a function. I know that the derivative can exist only along a certain curve, however I don't know if that must be true simultaneously or not. For instance, one such function I've been given is which gives and , and for the derivatives, you get . I know that this implies the derivative exists when (i.e. ) and when (i.e. ), but I don't know if this implies the derivative exists when EITHER of those things is true (derivative exists at and ) or only when BOTH of those things are true (derivative exists at ). I'm only asking for clarity on that front. Thank you!","f(z) = \frac{ix + 1}{y} u(x,y) = \frac{1}{y} v(x,y) = \frac{x}{y} u_x = 0, v_y = \frac{-x}{y^2}, u_y = \frac{-1}{y^2}, and -v_x = \frac{-1}{y} \frac{-x}{y^2} = 0 x = 0 \frac{-1}{y^2} = \frac{-1}{y} y = 1 (0, a) (b, i) (0, i)","['complex-analysis', 'derivatives', 'complex-numbers', 'partial-derivative', 'cauchy-riemann-equations']"
12,Where exactly does the integral definition of the gradient come from?,Where exactly does the integral definition of the gradient come from?,,"In the book ""Essential mathematical methods for physicists"" from Weber and Arfken, they define the integral form of the gradient,divergence and curl, althougth they give sections before an explanation of the upper parts of the divergence and curl definition, I don´t understand where does the upper part of the gradient definition come from and also I don´t understand why the integral of $d\tau$ and its limit appears in all the three definitions.","In the book ""Essential mathematical methods for physicists"" from Weber and Arfken, they define the integral form of the gradient,divergence and curl, althougth they give sections before an explanation of the upper parts of the divergence and curl definition, I don´t understand where does the upper part of the gradient definition come from and also I don´t understand why the integral of and its limit appears in all the three definitions.",d\tau,"['field-theory', 'derivatives', 'vector-fields']"
13,Applying the limit definition of a derivative on a radical function $x^{2/3}$,Applying the limit definition of a derivative on a radical function,x^{2/3},"I'm trying to find the derivative of the following using the limit definition of a derivative: $$f(x)=x^{2/3}.$$ I know that the derivative of $f(x)$ is $\frac23x^{-1/3}$ by the power rule, but I can't figure out how to do it with the limit definition. I've tried multiplying by a conjugate and can't get it that way. The professor said to use the difference of cubes formula to solve it, but I can't figure out how that plays into this question. The question is due 9/6/23, so any answers before that are greatly welcomed.","I'm trying to find the derivative of the following using the limit definition of a derivative: I know that the derivative of is by the power rule, but I can't figure out how to do it with the limit definition. I've tried multiplying by a conjugate and can't get it that way. The professor said to use the difference of cubes formula to solve it, but I can't figure out how that plays into this question. The question is due 9/6/23, so any answers before that are greatly welcomed.",f(x)=x^{2/3}. f(x) \frac23x^{-1/3},"['calculus', 'limits', 'derivatives', 'radicals', 'limits-without-lhopital']"
14,FTC II Type Question,FTC II Type Question,,"Suppose we define the function G(x) = $\int_{0}^{x^2} [y^2 \int_{0}^{y} f(t)dt] dy$ , and let's name $\phi(y) = y^2 \int_{0}^{y} f(t)dt$ . Then G'(x) = $\phi(x^2)*2x = 2x^5 \int_{0}^{x^2} f(t)dt$ . Going further, $\phi ' (x^2) = \frac{d}{dx} \left( x^4 \int_{0}^{x^2} f(t)dt \right) = 4x^3 \cdot \phi (x^2) + x^4\cdot f(x^2) \cdot 2x$ . Thus $G''(x) = \phi ' (x^2) \cdot 2x + 2 \phi (x^2) = 2x ( 4x^3 \cdot \phi (x^2) + 2x^5 \cdot f(x^2) ) + 2\phi (x^2).$ Is my math correct here? Did I apply the FTCII correctly?","Suppose we define the function G(x) = , and let's name . Then G'(x) = . Going further, . Thus Is my math correct here? Did I apply the FTCII correctly?",\int_{0}^{x^2} [y^2 \int_{0}^{y} f(t)dt] dy \phi(y) = y^2 \int_{0}^{y} f(t)dt \phi(x^2)*2x = 2x^5 \int_{0}^{x^2} f(t)dt \phi ' (x^2) = \frac{d}{dx} \left( x^4 \int_{0}^{x^2} f(t)dt \right) = 4x^3 \cdot \phi (x^2) + x^4\cdot f(x^2) \cdot 2x G''(x) = \phi ' (x^2) \cdot 2x + 2 \phi (x^2) = 2x ( 4x^3 \cdot \phi (x^2) + 2x^5 \cdot f(x^2) ) + 2\phi (x^2).,"['calculus', 'derivatives', 'solution-verification', 'chain-rule']"
15,Question on equality in ODEs,Question on equality in ODEs,,"I've been doing some reading in differential geometry and it's caused me some confusion over the objects involved in differential equations. Consider $f:\mathbb{R}\rightarrow\mathbb{R}$ and a first order ODE $$\frac{df}{dt}\Bigg|_t=g(f,t)$$ The R.H.S evaluates on $\mathbb{R}\rightarrow\mathbb{R}$ while the L.H.S $\frac{df}{dt}\Big|_t=f_*(\frac{d}{dt}\Big|_t)$ evaluates on $\mathbb{R}\rightarrow T\mathbb{R}$ so it seems to me that we stating that unlike objects are equal. It seems to me that on the L.H.S we are really interested in the 'natural' map $v(t)\frac{d}{dt}\Big|_t\mapsto v(t)$ , is this correct and should it be made explicit? (and without this is a diffeomorphism more appropriate than an equality?) It's worth mentioning that the partial derivative of $f:M\rightarrow\mathbb{R}$ with coordinate system $x$ is defined by a in terms of the directional derivative $$\frac{\partial f}{\partial x^i}\Bigg|_p := D_i(f\circ x^{-1})(x(p))$$ and perhaps this is from where my confusion is arising. If I plug this definition into the above problem I get $\frac{df}{dt}\Big|_t=f'(t)$ which appears to resolve my issue, but I've chosen to reject this answer! This definition appears to be useful in that it tells explains how to compute the value of the tangent vector, but still leaves me befuddled with respect to the objects at play. Is there something else that I am missing? Anything to alleviate my confusion will be much appreciated. Note: The notation that I'm using (should be) consistent with that used by Spivak. Edit 1: From the answers I've received it seems that we do make this identification is between $\mathbb{R}$ and $T\mathbb{R}$ implicitly. Mathematical 'Identifications' such as these feel very 'algebraic' to me - which leads me to a second (and less well formed) question: Is there a nice way to describe this using some algebraic theory of morphisms?","I've been doing some reading in differential geometry and it's caused me some confusion over the objects involved in differential equations. Consider and a first order ODE The R.H.S evaluates on while the L.H.S evaluates on so it seems to me that we stating that unlike objects are equal. It seems to me that on the L.H.S we are really interested in the 'natural' map , is this correct and should it be made explicit? (and without this is a diffeomorphism more appropriate than an equality?) It's worth mentioning that the partial derivative of with coordinate system is defined by a in terms of the directional derivative and perhaps this is from where my confusion is arising. If I plug this definition into the above problem I get which appears to resolve my issue, but I've chosen to reject this answer! This definition appears to be useful in that it tells explains how to compute the value of the tangent vector, but still leaves me befuddled with respect to the objects at play. Is there something else that I am missing? Anything to alleviate my confusion will be much appreciated. Note: The notation that I'm using (should be) consistent with that used by Spivak. Edit 1: From the answers I've received it seems that we do make this identification is between and implicitly. Mathematical 'Identifications' such as these feel very 'algebraic' to me - which leads me to a second (and less well formed) question: Is there a nice way to describe this using some algebraic theory of morphisms?","f:\mathbb{R}\rightarrow\mathbb{R} \frac{df}{dt}\Bigg|_t=g(f,t) \mathbb{R}\rightarrow\mathbb{R} \frac{df}{dt}\Big|_t=f_*(\frac{d}{dt}\Big|_t) \mathbb{R}\rightarrow T\mathbb{R} v(t)\frac{d}{dt}\Big|_t\mapsto v(t) f:M\rightarrow\mathbb{R} x \frac{\partial f}{\partial x^i}\Bigg|_p := D_i(f\circ x^{-1})(x(p)) \frac{df}{dt}\Big|_t=f'(t) \mathbb{R} T\mathbb{R}","['abstract-algebra', 'ordinary-differential-equations', 'derivatives', 'differential-geometry', 'partial-differential-equations']"
16,Problem 6.1 in Loring Tu: Why is the identity function on a particular manifold not smooth?,Problem 6.1 in Loring Tu: Why is the identity function on a particular manifold not smooth?,,"I'm currently reading from Loring Tu's Introduction to Manifolds , and I've come across the following problem: My confusion lies primarily with the last comment mentioned in Tu's hint: the identity map $\mathbb{R'} \to \mathbb{R}$ is not smooth (after looking in Tu's list of errata for the book, he meant to say $\mathbb{R'} \to \mathbb{R}$ instead of $\mathbb{R} \to \mathbb{R}$ ). This perplexes me a little bit for the following reasons: Let $i: \mathbb{R'} \to \mathbb{R}$ be the map given by $i(x) = x$ . Tu gives a very clear set of criteria to determine when such a map (an $\mathbb{R}$ -valued function defined on a manifold) is smooth: So, at least in my mind, we have the atlas $ \{ (\mathbb{R}, \psi) \} $ on $\mathbb{R'}$ where $\psi(x) = x^{1/3}$ . It's not necessarily the smooth structure on $\mathbb{R'}$ , but it's an atlas contained in the smooth structure. It is clear that the inverse of $\psi$ is the map $\psi^{-1}: \mathbb{R} \to \mathbb{R'}$ given by $\psi^{-1}(x) = x^3$ . By proposition $6.3(\text{ii})$ , $i(x) = x$ will be $C^{\infty}$ if the map $i \circ \psi^{-1}: \mathbb{R} \to \mathbb{R}$ is $C^\infty$ . But $i \circ \psi^{-1}(x)$ is precisely $i \circ \psi^{-1}(x) = x^3$ which is clearly smooth. What am I missing here? I've been trying to find which assumptions I'm making are incorrect, but I can't see them. Any help with this would be greatly appreciated! (Note, I'm not asking for help solving problem 6.1 as I've already found another diffeomorphism between $\mathbb{R}$ and $\mathbb{R'}$ . I would just like clarity on identity's lack of smoothness).","I'm currently reading from Loring Tu's Introduction to Manifolds , and I've come across the following problem: My confusion lies primarily with the last comment mentioned in Tu's hint: the identity map is not smooth (after looking in Tu's list of errata for the book, he meant to say instead of ). This perplexes me a little bit for the following reasons: Let be the map given by . Tu gives a very clear set of criteria to determine when such a map (an -valued function defined on a manifold) is smooth: So, at least in my mind, we have the atlas on where . It's not necessarily the smooth structure on , but it's an atlas contained in the smooth structure. It is clear that the inverse of is the map given by . By proposition , will be if the map is . But is precisely which is clearly smooth. What am I missing here? I've been trying to find which assumptions I'm making are incorrect, but I can't see them. Any help with this would be greatly appreciated! (Note, I'm not asking for help solving problem 6.1 as I've already found another diffeomorphism between and . I would just like clarity on identity's lack of smoothness).","\mathbb{R'} \to \mathbb{R} \mathbb{R'} \to \mathbb{R} \mathbb{R} \to \mathbb{R} i: \mathbb{R'} \to \mathbb{R} i(x) = x \mathbb{R}  \{ (\mathbb{R}, \psi) \}  \mathbb{R'} \psi(x) = x^{1/3} \mathbb{R'} \psi \psi^{-1}: \mathbb{R} \to \mathbb{R'} \psi^{-1}(x) = x^3 6.3(\text{ii}) i(x) = x C^{\infty} i \circ \psi^{-1}: \mathbb{R} \to \mathbb{R} C^\infty i \circ \psi^{-1}(x) i \circ \psi^{-1}(x) = x^3 \mathbb{R} \mathbb{R'}","['derivatives', 'differential-topology', 'smooth-manifolds']"
17,Motivation for Connections over smooth manifolds,Motivation for Connections over smooth manifolds,,"I was reading about connections on Wikipedia and I found this section about ""Motivation"": My question is the following one. If I look at the vector bundle $E$ like a manifold, then locally I have charts and I can think of $X: M \rightarrow E$ like a smooth map between open subsets of $\mathbb{R}^n$ and $\mathbb{R}^m$ for some $n$ and $m$ . Hence, I should naturally have a definition of derivatives. Am I wrong? What is the point of that argument? Thanks in advance","I was reading about connections on Wikipedia and I found this section about ""Motivation"": My question is the following one. If I look at the vector bundle like a manifold, then locally I have charts and I can think of like a smooth map between open subsets of and for some and . Hence, I should naturally have a definition of derivatives. Am I wrong? What is the point of that argument? Thanks in advance",E X: M \rightarrow E \mathbb{R}^n \mathbb{R}^m n m,"['derivatives', 'differential-geometry', 'euclidean-geometry', 'vector-bundles', 'connections']"
18,"Finding the derivative of a function with two absolute values within it, using a piecewise function","Finding the derivative of a function with two absolute values within it, using a piecewise function",,"I'm trying to solve some a problem relating to absolute values. I found online the strategy for solving similar functions from here: https://www.youtube.com/watch?v=eIHtq67nh7w&list=PLGbL7EvScmU7DfRNwONW7JDDcmB98Gjcs&index=22&t=305s&ab_channel=ProfRobBob Using the strategy shown: $y = |2x-3|+1$ solve for $0$ , to find where break point in the line will be: $0 = 2x-3 -> x = 3/2$ By substituting values around the break point, for example: When: $x=0 $ $y = |2(0) -3| + 1 $ $y = |-3| + 1 $ $y = 4$ (absolute value needs to be taken when x < 3/2) When: $x=3 $ $y = |2(3) -3| +1 $ $y = |3| + 1 $ $y = 4$ (The result is the same regardless of taking the absolute value when x> 3/2) We can then represent the first equation to make it piecewise: $y =   2x - 3 + 1 ; x >= 3/2$ $y =  -2x + 3 + 1 ; x < 3/2$ derivatives would be: $y  = 2; x >=3/2  $ $y  =-2; x < 3/2$ HOWEVER, I am trying to solve a function like this: $y = |x^2 - 3| + |x + 1|$ And Im not sure how to go about it. I tried doing the same as before: $0 = x^2 -1 + x + 4   ->  -3 = x^2 +x$ But this doesn't seem like the right approach, any advice on what to do?","I'm trying to solve some a problem relating to absolute values. I found online the strategy for solving similar functions from here: https://www.youtube.com/watch?v=eIHtq67nh7w&list=PLGbL7EvScmU7DfRNwONW7JDDcmB98Gjcs&index=22&t=305s&ab_channel=ProfRobBob Using the strategy shown: solve for , to find where break point in the line will be: By substituting values around the break point, for example: When: (absolute value needs to be taken when x < 3/2) When: (The result is the same regardless of taking the absolute value when x> 3/2) We can then represent the first equation to make it piecewise: derivatives would be: HOWEVER, I am trying to solve a function like this: And Im not sure how to go about it. I tried doing the same as before: But this doesn't seem like the right approach, any advice on what to do?",y = |2x-3|+1 0 0 = 2x-3 -> x = 3/2 x=0  y = |2(0) -3| + 1  y = |-3| + 1  y = 4 x=3  y = |2(3) -3| +1  y = |3| + 1  y = 4 y =   2x - 3 + 1 ; x >= 3/2 y =  -2x + 3 + 1 ; x < 3/2 y  = 2; x >=3/2   y  =-2; x < 3/2 y = |x^2 - 3| + |x + 1| 0 = x^2 -1 + x + 4   ->  -3 = x^2 +x,"['derivatives', 'absolute-value', 'piecewise-continuity']"
19,Derivative of the derivative of a neural network w.r.t. itself,Derivative of the derivative of a neural network w.r.t. itself,,"I'm trying to find the following derivative: \begin{equation} \frac{\partial^2 f}{\partial f \partial x} \end{equation} where $f$ is a neural network, and $x$ is an input. To be more accurate, let's say $f$ is a 2-layer neural net with a 1-D output and 1-D input. Then, we can write $f$ as: $$ f(x) = tanh(xW_1 + b_1)W_2 + b  $$ I tried deriving the formula for $\frac{\partial f}{\partial x}$ and got the following: $$ \frac{\partial f}{\partial x} = W_2^T\times (W_1^T\circ tanh'(xW_1 + b_1)) $$ where $\circ$ is the element-wise product. Now, I need to take its derivative w.r.t. $f$ itself. Any ideas on how to proceed from here? Edit: As @NinadMunshi pointed out, I intend to find the following derivative after a variable change: $$ z = tanh(xW_1 + b_1)W_2 + b\\ g = \frac{\partial f}{\partial x} = W_2^T\times (W_1^T\circ tanh'(xW_1 + b_1))\\ \frac{\partial g}{\partial z} = ? $$","I'm trying to find the following derivative: where is a neural network, and is an input. To be more accurate, let's say is a 2-layer neural net with a 1-D output and 1-D input. Then, we can write as: I tried deriving the formula for and got the following: where is the element-wise product. Now, I need to take its derivative w.r.t. itself. Any ideas on how to proceed from here? Edit: As @NinadMunshi pointed out, I intend to find the following derivative after a variable change:","\begin{equation}
\frac{\partial^2 f}{\partial f \partial x}
\end{equation} f x f f 
f(x) = tanh(xW_1 + b_1)W_2 + b 
 \frac{\partial f}{\partial x} 
\frac{\partial f}{\partial x} = W_2^T\times (W_1^T\circ tanh'(xW_1 + b_1))
 \circ f 
z = tanh(xW_1 + b_1)W_2 + b\\
g = \frac{\partial f}{\partial x} = W_2^T\times (W_1^T\circ tanh'(xW_1 + b_1))\\
\frac{\partial g}{\partial z} = ?
","['derivatives', 'partial-derivative', 'neural-networks']"
20,Differentiable and Non-decreasing with $f' = 0$ a.e. But Not Constant,Differentiable and Non-decreasing with  a.e. But Not Constant,f' = 0,"Context / Motivation In this question , the OP asks for a proof that if $f : [0,1] \to [0,1]$ is increasing and differentiable with $f(0) = 0$ and $f(1) = 1$ , then the length of the graph $L(f)$ is less than $2$ . As is shown in the answers to that question, if we only know that $f$ is non-decreasing, $f(0) = 0$ and $f(1) = 1$ , we may conclude $L(f) \leq 2$ . However, additional assumptions are required for the strict inequality. If we additionally suppose, for example, that $f' > 0$ on some interval in $[0,1]$ (assuming nothing about the differentiability of $f$ outside of this interval), then we may easily obtain the strict inequality $L(f) < 2$ . By contrast, if we instead suppose that $f$ is differentiable almost everywhere on $[0,1]$ , we cannot guarantee strict inequality. The famous Cantor function has $f(0) = 0$ , $f(1) = 1$ , is non-decreasing, and is differentiable almost everywhere, yet $L(f) = 2$ . The above considerations made me curious how weak the hypotheses on $f$ could be. What seems to have gone wrong with the Cantor function is that the function was not differentiable everywhere, only almost everywhere. This made me wonder whether it would be enough to assume that $f$ is differentiable everywhere. What could go wrong in this case? The only way to still have $L(f) = 2$ would be if $f' = 0$ a.e. But is this possible? Can we have a non-decreasing differentiable function with $f' = 0$ a.e., $f(0) = 0$ , and $f(1) = 1$ ? My Question Does there exist a function $f : [0,1] \to [0,1]$ such that $f$ is non-decreasing $f$ is differentiable everywhere (this rules out the Cantor function) $f' = 0$ almost everywhere $f(0) = 0$ and $f(1) = 1$ Note that through re-scaling and translation this is the same as the question in the title. I have only asked it this way here to be consistent with with the context/motivation above.","Context / Motivation In this question , the OP asks for a proof that if is increasing and differentiable with and , then the length of the graph is less than . As is shown in the answers to that question, if we only know that is non-decreasing, and , we may conclude . However, additional assumptions are required for the strict inequality. If we additionally suppose, for example, that on some interval in (assuming nothing about the differentiability of outside of this interval), then we may easily obtain the strict inequality . By contrast, if we instead suppose that is differentiable almost everywhere on , we cannot guarantee strict inequality. The famous Cantor function has , , is non-decreasing, and is differentiable almost everywhere, yet . The above considerations made me curious how weak the hypotheses on could be. What seems to have gone wrong with the Cantor function is that the function was not differentiable everywhere, only almost everywhere. This made me wonder whether it would be enough to assume that is differentiable everywhere. What could go wrong in this case? The only way to still have would be if a.e. But is this possible? Can we have a non-decreasing differentiable function with a.e., , and ? My Question Does there exist a function such that is non-decreasing is differentiable everywhere (this rules out the Cantor function) almost everywhere and Note that through re-scaling and translation this is the same as the question in the title. I have only asked it this way here to be consistent with with the context/motivation above.","f : [0,1] \to [0,1] f(0) = 0 f(1) = 1 L(f) 2 f f(0) = 0 f(1) = 1 L(f) \leq 2 f' > 0 [0,1] f L(f) < 2 f [0,1] f(0) = 0 f(1) = 1 L(f) = 2 f f L(f) = 2 f' = 0 f' = 0 f(0) = 0 f(1) = 1 f : [0,1] \to [0,1] f f f' = 0 f(0) = 0 f(1) = 1","['derivatives', 'examples-counterexamples', 'monotone-functions']"
21,Derivative with respect to inner function with differing inputs?,Derivative with respect to inner function with differing inputs?,,"There's a conceptual deal with derivatives of composite functions that I'm really struggling to understand. Let $g(t)$ be some continuous, differentiable function. $f(t) = g(t) + g(t + 1) + g(t^2)$ Now, $\frac{df}{dt}$ is trivial to solve with the chain rule. What's $\frac{\bf df}{\bf dg}$ ? Naïvely, treating $g$ as just the input into $f$ , it seems analogous to $\frac{d}{dx}(x+x+x) = 3$ . But in the equation for $f$ above, it doesn't seem right that the three $g$ terms, which could all represent vastly different quantities for any given input, should have the same instantaneous rate of change. Is it really true that for an infinitesimal change to the output of $g$ , the output of $f$ simply changes by 3 times as much, for any value of x? Or am I wrong? Or am I asking a completely nonsensical question? (For context, I'm working on some homebrewed alternative neural network structures, and I'm treating some neurons as compositions of functions. For error measurement purposes, it would be great to take derivatives of the outer functions with respect to the inner function alone.)","There's a conceptual deal with derivatives of composite functions that I'm really struggling to understand. Let be some continuous, differentiable function. Now, is trivial to solve with the chain rule. What's ? Naïvely, treating as just the input into , it seems analogous to . But in the equation for above, it doesn't seem right that the three terms, which could all represent vastly different quantities for any given input, should have the same instantaneous rate of change. Is it really true that for an infinitesimal change to the output of , the output of simply changes by 3 times as much, for any value of x? Or am I wrong? Or am I asking a completely nonsensical question? (For context, I'm working on some homebrewed alternative neural network structures, and I'm treating some neurons as compositions of functions. For error measurement purposes, it would be great to take derivatives of the outer functions with respect to the inner function alone.)",g(t) f(t) = g(t) + g(t + 1) + g(t^2) \frac{df}{dt} \frac{\bf df}{\bf dg} g f \frac{d}{dx}(x+x+x) = 3 f g g f,"['calculus', 'derivatives', 'continuity']"
22,Problem with First-Order nonlinear Differential Equation,Problem with First-Order nonlinear Differential Equation,,"I am trying to derive an analytical expression for a solution to the following differential equation: $$\frac{\mathrm dy}{\mathrm dx} - Be^{-y} = \cos(2x) + D,$$ where $B$ and $D$ are constants. Can someone point me in the right direction as to how this can be done? If it cannot be solved analytically, is there an approximation that can be made (e.g. power series expansion) to allow an approx. analytical expression to be derived? I am not interested in solving this problem numerically. Thank You","I am trying to derive an analytical expression for a solution to the following differential equation: where and are constants. Can someone point me in the right direction as to how this can be done? If it cannot be solved analytically, is there an approximation that can be made (e.g. power series expansion) to allow an approx. analytical expression to be derived? I am not interested in solving this problem numerically. Thank You","\frac{\mathrm dy}{\mathrm dx} - Be^{-y} = \cos(2x) + D, B D","['integration', 'ordinary-differential-equations', 'derivatives']"
23,How to differentiate an expression with a variable and parameters,How to differentiate an expression with a variable and parameters,,I came across the expression of $\sqrt{x^2+a^2}$ with $a$ as a non zero real number parameter. I thought that I would simply treat $a^2$ as simply one integer when doing derivations but my instructor said that I was wrong. Can anyone please help explain why this is the case and inform me how to treat parameters in f(x) expressions? thanks a lot!,I came across the expression of with as a non zero real number parameter. I thought that I would simply treat as simply one integer when doing derivations but my instructor said that I was wrong. Can anyone please help explain why this is the case and inform me how to treat parameters in f(x) expressions? thanks a lot!,\sqrt{x^2+a^2} a a^2,"['calculus', 'derivatives', 'parametric']"
24,How to compare the time complexity of $n^n$ and $2^{n\log_2(n^2)}?$,How to compare the time complexity of  and,n^n 2^{n\log_2(n^2)}?,"Here is what I did for this, using $\lim\limits_{x\to\infty}\dfrac{f(x)}{g(x)}$ to get the result. $$2^{n\log_2(n^2)}=2^n\cdot2^{\log_2(n^2)}=2^n\cdot n^2$$ $$\frac{f(x)}{g(x)}=\frac{n^n}{2^n\cdot n^2}=\frac{n^{n-2}}{2^n}$$ then I stuck here, how should I compare $n^{n-2}$ and $2^n\;?$","Here is what I did for this, using to get the result. then I stuck here, how should I compare and",\lim\limits_{x\to\infty}\dfrac{f(x)}{g(x)} 2^{n\log_2(n^2)}=2^n\cdot2^{\log_2(n^2)}=2^n\cdot n^2 \frac{f(x)}{g(x)}=\frac{n^n}{2^n\cdot n^2}=\frac{n^{n-2}}{2^n} n^{n-2} 2^n\;?,"['limits', 'derivatives']"
25,Confusion on definition of differential,Confusion on definition of differential,,"I’m having a hard time understanding a certain section in In Ordinary Differential Equations by Tenenbaum and Pollard. Specifically, in section 6, they state the following: Given a function $y=f(x)$ , They define the differential dy as: $dy=f’(x) \Delta x $ (6.14) And denote the function dy as $(dy)(x, \Delta x)$ as it is a function of $x$ and $\Delta x.$ They then state that if $y=\hat{x}$ , where  (and this distinction is completely lost on me in the sense of not understanding why it is necessary), “we place the symbol ^ over x so that $y=\hat{x}$ will define a function that assigns to each value of the independent variable x the same unique value to the dependent variable y.” They then say that if $y=\hat{x}$ , then $(dy)(x, \Delta x) = (d \hat{x})(x, \Delta x) = \Delta x.$ (6.22) All of this is fine I suppose but I don’t understand the logic at all behind the next step: “If I’m (6.14) we replace $\Delta x$ by its value as given in (6.22), it becomes $(dy)(x, \Delta x) = f’(x) (d \hat{x})(x, \Delta x)$ (6.24) This part doesn’t make any sense to me as they are simultaneously stating that $y=f(x)$ and $y=\hat{x}$ by saying so, unless I misunderstood something. And finally, this last part also completely evades me: They state that the above relation is correct (still not sure why based on their reasoning), however “it became customary to write (6.24) in the more familiar form $dy = f’(x) dx $ or $\frac{dy}{dx} = f’(x)$ . This last part especially confuses me because now it can no longer be said they were defining dy as a function, but were using its definition as a limit in the conventional sense of a derivative. I’ve seen posts about why this symbolic manipulation of dy and dx turns out to be ok because of the chain rule, but I’m more just lost on any and all of the steps they took in introducing this differential. If anyone could help clarify that would be great, thank you!","I’m having a hard time understanding a certain section in In Ordinary Differential Equations by Tenenbaum and Pollard. Specifically, in section 6, they state the following: Given a function , They define the differential dy as: (6.14) And denote the function dy as as it is a function of and They then state that if , where  (and this distinction is completely lost on me in the sense of not understanding why it is necessary), “we place the symbol ^ over x so that will define a function that assigns to each value of the independent variable x the same unique value to the dependent variable y.” They then say that if , then (6.22) All of this is fine I suppose but I don’t understand the logic at all behind the next step: “If I’m (6.14) we replace by its value as given in (6.22), it becomes (6.24) This part doesn’t make any sense to me as they are simultaneously stating that and by saying so, unless I misunderstood something. And finally, this last part also completely evades me: They state that the above relation is correct (still not sure why based on their reasoning), however “it became customary to write (6.24) in the more familiar form or . This last part especially confuses me because now it can no longer be said they were defining dy as a function, but were using its definition as a limit in the conventional sense of a derivative. I’ve seen posts about why this symbolic manipulation of dy and dx turns out to be ok because of the chain rule, but I’m more just lost on any and all of the steps they took in introducing this differential. If anyone could help clarify that would be great, thank you!","y=f(x) dy=f’(x) \Delta x  (dy)(x, \Delta x) x \Delta x. y=\hat{x} y=\hat{x} y=\hat{x} (dy)(x, \Delta x) = (d \hat{x})(x, \Delta x) = \Delta x. \Delta x (dy)(x, \Delta x) = f’(x) (d \hat{x})(x, \Delta x) y=f(x) y=\hat{x} dy = f’(x) dx  \frac{dy}{dx} = f’(x)","['calculus', 'ordinary-differential-equations', 'derivatives']"
26,finding the values of a and b(real numbers) that makes the function differentiable at any point of its domain,finding the values of a and b(real numbers) that makes the function differentiable at any point of its domain,,"I'm given this function: $$ f(x) = \begin{cases} (lnx)^4  & 0<x< e \\ ax+b & x≥e \end{cases} $$ and I'm asked for which real values of 'a' and 'b' the function is differentiable at any point I understand that for every x≠1 the function is differentiable. and now I'm trying the find the values of 'a' and 'b' that make the function differentiable at x=e this is what I did so far: first I demanded that the function will be Continuous at x=e, meaning I demand that $\lim\limits_{x \to e^{-}} f(x) = \lim\limits_{x \to e^{+}} f(x) = f(e)$ and I got the equation: $ae+b = e$ next, I demanded that the function will be differentiable at x = e, I demanded that $\lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h} = \lim\limits_{h \to 0^{+}} \frac{f(e+h)-f(e)}{h}$ so far I got: $\lim\limits_{h \to 0^{+}} \frac{f(e+h)-f(e)}{h} = \lim\limits_{h \to 0^{+}} \frac{(a(e+h)+b) - (ae+b)}{h} = \lim\limits_{h \to 0^{+}} \frac{(ae+ah+b) - (ae+b)}{h} = \lim\limits_{h \to 0^{+}} \frac{ah}{h} = \lim\limits_{h \to 0^{+}} a = a$ but when I tried to calculate $\lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h}$ I got stuck, so far I got $\lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h} = \lim\limits_{h \to 0^{-}} \frac{(ln(e+h))^4 - (ae+b)}{h} = ???$ now I don't know how to solve this, I'll be glad of any help you can give me, should I calculate the derivative function at $0<x<e$ and at $x>e$ using the chain rule and then demand that $f'(1+) = f'(1-)  $ or should I stick to the limit definition of the derivative?","I'm given this function: and I'm asked for which real values of 'a' and 'b' the function is differentiable at any point I understand that for every x≠1 the function is differentiable. and now I'm trying the find the values of 'a' and 'b' that make the function differentiable at x=e this is what I did so far: first I demanded that the function will be Continuous at x=e, meaning I demand that and I got the equation: next, I demanded that the function will be differentiable at x = e, I demanded that so far I got: but when I tried to calculate I got stuck, so far I got now I don't know how to solve this, I'll be glad of any help you can give me, should I calculate the derivative function at and at using the chain rule and then demand that or should I stick to the limit definition of the derivative?","
f(x) = \begin{cases}
(lnx)^4  & 0<x< e \\
ax+b & x≥e
\end{cases}
 \lim\limits_{x \to e^{-}} f(x)
= \lim\limits_{x \to e^{+}} f(x) = f(e) ae+b = e \lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h}
= \lim\limits_{h \to 0^{+}} \frac{f(e+h)-f(e)}{h} \lim\limits_{h \to 0^{+}} \frac{f(e+h)-f(e)}{h} = \lim\limits_{h \to 0^{+}} \frac{(a(e+h)+b) - (ae+b)}{h} = \lim\limits_{h \to 0^{+}} \frac{(ae+ah+b) - (ae+b)}{h} = \lim\limits_{h \to 0^{+}} \frac{ah}{h} = \lim\limits_{h \to 0^{+}} a = a \lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h} \lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h} = \lim\limits_{h \to 0^{-}} \frac{(ln(e+h))^4 - (ae+b)}{h} = ??? 0<x<e x>e f'(1+) = f'(1-)  ","['calculus', 'limits', 'derivatives', 'parametric', 'piecewise-continuity']"
27,Second derivative calculation,Second derivative calculation,,Consider $\boldsymbol{\psi}(\boldsymbol{q})\in R^2$ is a function of vector $\boldsymbol{q}(t)\in R^3$ which is a function of time $t\in R^+$ . The first time derivative of function $\boldsymbol{\psi}$ can be defined as following equation using chain rule: $\dot{\boldsymbol{\psi}}=\frac{\partial{\boldsymbol{\psi}}}{\partial{\boldsymbol{q}}}\dot{q}$ . In this equation $\frac{\partial{\boldsymbol{\psi}}}{\partial{\boldsymbol{q}}}\in R^{2\times3}$ is the Jacobian matrix. How it is possible to define the second time derivative of $\ddot{\boldsymbol{\psi}}$ using chain rule? For example if the function $\psi$ and $q$ were scalars we could define its second derivative easily by the following equation $\ddot{\psi}=\frac{\partial^2{\psi}}{\partial{q^2}} \dot{q}^2+\frac{\partial{\psi}}{\partial{q}}\ddot{q}$ . But what about this case? Is there any similar equation for vector forms of $\boldsymbol{\psi} \in R^2$ and $\boldsymbol{q}(t) \in R^3$ ?,Consider is a function of vector which is a function of time . The first time derivative of function can be defined as following equation using chain rule: . In this equation is the Jacobian matrix. How it is possible to define the second time derivative of using chain rule? For example if the function and were scalars we could define its second derivative easily by the following equation . But what about this case? Is there any similar equation for vector forms of and ?,\boldsymbol{\psi}(\boldsymbol{q})\in R^2 \boldsymbol{q}(t)\in R^3 t\in R^+ \boldsymbol{\psi} \dot{\boldsymbol{\psi}}=\frac{\partial{\boldsymbol{\psi}}}{\partial{\boldsymbol{q}}}\dot{q} \frac{\partial{\boldsymbol{\psi}}}{\partial{\boldsymbol{q}}}\in R^{2\times3} \ddot{\boldsymbol{\psi}} \psi q \ddot{\psi}=\frac{\partial^2{\psi}}{\partial{q^2}} \dot{q}^2+\frac{\partial{\psi}}{\partial{q}}\ddot{q} \boldsymbol{\psi} \in R^2 \boldsymbol{q}(t) \in R^3,"['derivatives', 'partial-derivative', 'chain-rule', 'jacobian']"
28,Minimum area of the inside of a parallelepiped,Minimum area of the inside of a parallelepiped,,"A container with parallelepiped shape, empty inside, and square base, has capacity of 32 litres. We want to cover the inside of the container (without the top surface), with a sheet. What are: the sides $x$ the height $h$ of that parallelepiped if the area of sheet must be minimum? MY ATTEMPT My attempt was to use these two equations: $x^2h=0.032$ (it's the area of the parallelepiped in meters squared) $D(4xh+x^2)=0$ (the area of the sheet must be the equal to the 5 faces of the parallelepiped, and I must find its minimum, thus the derivative equal to zero) I then obtain, from the first equation, $h=\frac{0.032}{x^2}$ and then I substitute this value of $h$ in $4xh+x^2$ , obtaining $\frac{0.128}{x}+x^2$ . Then I do the derivative of this last expression, obtaining $\frac{0.128}{x^2}+2x$ and put $=0$ to find the minimum, obtaining $x^3=\frac{0.128}{2}$ DRAWING OF THE PROBLEM Note that the face of the parallelepiped with the yellow diagonal lines is the one we shouldn't consider, since it is the removed top surface. The faces with the red arrow are the one with area $xh$ , the base (orange) has area $x^2$ since it's a square","A container with parallelepiped shape, empty inside, and square base, has capacity of 32 litres. We want to cover the inside of the container (without the top surface), with a sheet. What are: the sides the height of that parallelepiped if the area of sheet must be minimum? MY ATTEMPT My attempt was to use these two equations: (it's the area of the parallelepiped in meters squared) (the area of the sheet must be the equal to the 5 faces of the parallelepiped, and I must find its minimum, thus the derivative equal to zero) I then obtain, from the first equation, and then I substitute this value of in , obtaining . Then I do the derivative of this last expression, obtaining and put to find the minimum, obtaining DRAWING OF THE PROBLEM Note that the face of the parallelepiped with the yellow diagonal lines is the one we shouldn't consider, since it is the removed top surface. The faces with the red arrow are the one with area , the base (orange) has area since it's a square",x h x^2h=0.032 D(4xh+x^2)=0 h=\frac{0.032}{x^2} h 4xh+x^2 \frac{0.128}{x}+x^2 \frac{0.128}{x^2}+2x =0 x^3=\frac{0.128}{2} xh x^2,"['derivatives', 'optimization', 'solution-verification', 'euclidean-geometry']"
29,"Differentiability of $f(x,y) = |xy|$",Differentiability of,"f(x,y) = |xy|","I know there is also a questions concerning this task. However I have some questions about my approaches to solve this taks. Thanks for your help in advance! We consider the function $f: \mathbb{R}^{2} \longrightarrow \mathbb{R}$ , defined by $$f(x, y):=|x y|$$ a) Determine all points $u=(u_{1}, u_{2}) \in \mathbb{R}^{2}$ at which the function $f$ has partial derivatives $\frac{\partial f}{\partial x}(u)$ and $\frac{\partial f}{\partial y}(u)$ . b) Determine all points $u=(u_{1}, u_{2}) \in \mathbb{R}^{2}$ at which the function $f$ is differentiable. Question to a)\ When I compute the partial derivatives for $\frac{\partial f}{\partial x}(u)$ and $\frac{\partial f}{\partial y}(u)$ I have $$\frac{\partial f}{\partial x}(u) = \frac{|y|x}{|x|}$$ and $$\frac{\partial f}{\partial y}(u) = \frac{|x|y}{|y|}$$ We see that we can't have $(0,y) \in \mathbb{R}^{2}$ for $\frac{\partial f}{\partial x}(u)$ and $(x,0)\in \mathbb{R}^{2}$ for $\frac{\partial f}{\partial y}(u)$ . And the partial derivatives also don't exist in $(0,0) \in \mathbb{R}^{2}$ . I feel like this doesn't solve the task correctly. Is there anything I'm missing? Question to b)\ b) In order to find differentiability for $f(x,y)$ in $u=(u_1,u_2)$ we have to find the points where the partial derivatives exist and are continuous. So we have to deal with our special points $(0,0),(x,0),(0,y) \in \mathbb{R}^{2}$ Let $(x,y)=(0,0)$ , then $$\begin{aligned}\lim \limits_{(h_{1},h_{2}) \rightarrow(0,0)} \frac{f(h_{1}, h_{2})-f(0,0)}{\|h\|}&=\lim \limits_{(h_{1},h_{2})\rightarrow(0,0)} \frac{|h_{1}| \cdot|h_{2}|}{\sqrt{h_{1}^{2}+h_{2}^{2}}} \\ &\leq \lim \limits_{(h_{1},h_{2}) \rightarrow(0,0)}|h_{1}| \cdot|h_{2}| =0\end{aligned}$$ So $f(x,y)$ is differentiable in $u=(0,0)$ . Let $(x,y)= (0,y)$ , then $$\lim \limits_{h \rightarrow 0^{+}} \frac{f(h, y)-f(0, y)}{h}=\lim \limits_{h \rightarrow 0^{+}} \frac{|h||y|}{h}=|y|$$ but $$\lim \limits_{h \rightarrow 0^{-}} \frac{f(h, y)-f(0, y)}{h}=\lim \limits_{h \rightarrow 0^{-}} \frac{|h||y|}{h}=-|y|$$ So $f$ is not differentiable in $(0,y) \in \mathbb{R}^{2}$ . Let $(x, y)=(x, 0)$ , then $$\lim \limits_{h \rightarrow 0^{+}} \frac{f(x, h)-f(x, 0)}{h}=\lim \limits_{h \rightarrow 0^{+}} \frac{|h||x|}{h}=|x|$$ but $$\lim \limits_{h \rightarrow 0^{-}} \frac{f(x, h)-f(x, 0)}{h}=\lim \limits_{h \rightarrow 0^{-}} \frac{|h||x|}{h}=-|x|$$ So $f$ is not differentiable in $(x, 0) \in \mathbb{R}^{2}$ . So $f$ is $\forall(x, y) \in \mathbb{R}^{2} \backslash((x, 0),(0, y))$ differentiable. Let me know what you think, thanks for your help!","I know there is also a questions concerning this task. However I have some questions about my approaches to solve this taks. Thanks for your help in advance! We consider the function , defined by a) Determine all points at which the function has partial derivatives and . b) Determine all points at which the function is differentiable. Question to a)\ When I compute the partial derivatives for and I have and We see that we can't have for and for . And the partial derivatives also don't exist in . I feel like this doesn't solve the task correctly. Is there anything I'm missing? Question to b)\ b) In order to find differentiability for in we have to find the points where the partial derivatives exist and are continuous. So we have to deal with our special points Let , then So is differentiable in . Let , then but So is not differentiable in . Let , then but So is not differentiable in . So is differentiable. Let me know what you think, thanks for your help!","f: \mathbb{R}^{2} \longrightarrow \mathbb{R} f(x, y):=|x y| u=(u_{1}, u_{2}) \in \mathbb{R}^{2} f \frac{\partial f}{\partial x}(u) \frac{\partial f}{\partial y}(u) u=(u_{1}, u_{2}) \in \mathbb{R}^{2} f \frac{\partial f}{\partial x}(u) \frac{\partial f}{\partial y}(u) \frac{\partial f}{\partial x}(u) = \frac{|y|x}{|x|} \frac{\partial f}{\partial y}(u) = \frac{|x|y}{|y|} (0,y) \in \mathbb{R}^{2} \frac{\partial f}{\partial x}(u) (x,0)\in \mathbb{R}^{2} \frac{\partial f}{\partial y}(u) (0,0) \in \mathbb{R}^{2} f(x,y) u=(u_1,u_2) (0,0),(x,0),(0,y) \in \mathbb{R}^{2} (x,y)=(0,0) \begin{aligned}\lim \limits_{(h_{1},h_{2}) \rightarrow(0,0)} \frac{f(h_{1}, h_{2})-f(0,0)}{\|h\|}&=\lim \limits_{(h_{1},h_{2})\rightarrow(0,0)} \frac{|h_{1}| \cdot|h_{2}|}{\sqrt{h_{1}^{2}+h_{2}^{2}}} \\ &\leq \lim \limits_{(h_{1},h_{2}) \rightarrow(0,0)}|h_{1}| \cdot|h_{2}| =0\end{aligned} f(x,y) u=(0,0) (x,y)= (0,y) \lim \limits_{h \rightarrow 0^{+}} \frac{f(h, y)-f(0, y)}{h}=\lim \limits_{h \rightarrow 0^{+}} \frac{|h||y|}{h}=|y| \lim \limits_{h \rightarrow 0^{-}} \frac{f(h, y)-f(0, y)}{h}=\lim \limits_{h \rightarrow 0^{-}} \frac{|h||y|}{h}=-|y| f (0,y) \in \mathbb{R}^{2} (x, y)=(x, 0) \lim \limits_{h \rightarrow 0^{+}} \frac{f(x, h)-f(x, 0)}{h}=\lim \limits_{h \rightarrow 0^{+}} \frac{|h||x|}{h}=|x| \lim \limits_{h \rightarrow 0^{-}} \frac{f(x, h)-f(x, 0)}{h}=\lim \limits_{h \rightarrow 0^{-}} \frac{|h||x|}{h}=-|x| f (x, 0) \in \mathbb{R}^{2} f \forall(x, y) \in \mathbb{R}^{2} \backslash((x, 0),(0, y))","['analysis', 'derivatives', 'solution-verification', 'partial-derivative']"
30,"If you have an inflection point, is there a three-term A.P. in the domain that the function maps to a three-term A.P.?","If you have an inflection point, is there a three-term A.P. in the domain that the function maps to a three-term A.P.?",,"Let $f:\mathbb{R}\to\mathbb{R}$ be a twice-differentiable function with a point of inflection at $\ x=0.\ $ Specifically, $\ f''(0)=0,\ $ and there exists $\ b>0, c>0\ $ such that $\  f''(x) < 0\ $ on $\ (-b,0),\ $ and $\ f''(x) > 0\ $ on $\ (0,c).\ $ Does there exist $\ a<0,\ d > 0,\ $ such that $\ f(a), f(a+d),  f(a+2d)\ $ forms an arithmetic progression, in particular, $\ f(a+d) -  f(a) = f(a+2d) - f(a+d)\ ?$ I think the answer is yes. In particular, I think there exists $\ \mu < 0\ $ such that the result is true for every $\ a\in [\mu,0).\ $ However, I don't know how to prove this. Instinctively I think we can apply the Intermediate Value theorem on $\ f'(x),\ $ but I'm not sure how to implement this.","Let be a twice-differentiable function with a point of inflection at Specifically, and there exists such that on and on Does there exist such that forms an arithmetic progression, in particular, I think the answer is yes. In particular, I think there exists such that the result is true for every However, I don't know how to prove this. Instinctively I think we can apply the Intermediate Value theorem on but I'm not sure how to implement this.","f:\mathbb{R}\to\mathbb{R} \ x=0.\  \ f''(0)=0,\  \ b>0, c>0\  \
 f''(x) < 0\  \ (-b,0),\  \ f''(x) > 0\  \ (0,c).\  \ a<0,\ d > 0,\  \ f(a), f(a+d),
 f(a+2d)\  \ f(a+d) -
 f(a) = f(a+2d) - f(a+d)\ ? \ \mu < 0\  \ a\in [\mu,0).\  \ f'(x),\ ","['real-analysis', 'calculus', 'derivatives', 'mean-value-theorem']"
31,Is there a norm for $\nabla$ operator? What is the meaning of $\sqrt{\Delta}$? [duplicate],Is there a norm for  operator? What is the meaning of ? [duplicate],\nabla \sqrt{\Delta},"This question already has answers here : Square root of a Hermitian operator exists (4 answers) Intuitive understanding of the operator norm? (1 answer) What's the difference between the operator norm and the sup norm (1 answer) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Considering the del operator in cartesian coordinate. $$\nabla=\left(\dfrac{\partial}{\partial x_1},...,\dfrac{\partial}{\partial x_n}\right)$$ Since technically it's a vector, does its norm have any meaning? I considered is ""norm"" as the square root of the Laplacian: $$\left\Vert\nabla\right\Vert_{\mathbb{R}^n}=\sqrt{\Delta}=\sqrt{\dfrac{\partial^2}{\partial x_1^2}+...+\dfrac{\partial^2}{\partial x_n^2}}$$ Is it correct to think of this operator as ""the operator that applied twice behaves like the Laplacian""? If this is wrong, it is possible to make the norm of a vector of operators? If yes, what are the properties of this norm and what normed space are we in? In this case I considered $\mathbb{R}^n$ and technically this is not a norm since the result is not a non-negative number, so I was curious to know: If anyway this operator makes sense (in case what is the meaning). If there is a space that can associate a numeric value to the norm of this operator (even if the norm was infinite causing the operator to be unbounded). If this space exists, does the result depend on the coordinate system i'm using and the length of the vector?","This question already has answers here : Square root of a Hermitian operator exists (4 answers) Intuitive understanding of the operator norm? (1 answer) What's the difference between the operator norm and the sup norm (1 answer) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Considering the del operator in cartesian coordinate. Since technically it's a vector, does its norm have any meaning? I considered is ""norm"" as the square root of the Laplacian: Is it correct to think of this operator as ""the operator that applied twice behaves like the Laplacian""? If this is wrong, it is possible to make the norm of a vector of operators? If yes, what are the properties of this norm and what normed space are we in? In this case I considered and technically this is not a norm since the result is not a non-negative number, so I was curious to know: If anyway this operator makes sense (in case what is the meaning). If there is a space that can associate a numeric value to the norm of this operator (even if the norm was infinite causing the operator to be unbounded). If this space exists, does the result depend on the coordinate system i'm using and the length of the vector?","\nabla=\left(\dfrac{\partial}{\partial x_1},...,\dfrac{\partial}{\partial x_n}\right) \left\Vert\nabla\right\Vert_{\mathbb{R}^n}=\sqrt{\Delta}=\sqrt{\dfrac{\partial^2}{\partial x_1^2}+...+\dfrac{\partial^2}{\partial x_n^2}} \mathbb{R}^n","['functional-analysis', 'derivatives', 'normed-spaces', 'vector-analysis', 'laplacian']"
32,"Show that for every $\epsilon>0$, there exists $x_{\epsilon}$ real number such that $|f(x_{\epsilon})|<\epsilon$","Show that for every , there exists  real number such that",\epsilon>0 x_{\epsilon} |f(x_{\epsilon})|<\epsilon,"We have $f:\mathbb R\to \mathbb R$ a function that has a primitive $F:\mathbb R\to (0,\infty)$ . Show that for every $\epsilon>0$ , there exists $x_{\epsilon}$ such that $|f(x_{\epsilon})|<\epsilon$ . I supposed that there exists an $\epsilon>0$ such that for every $x$ real number we have $|f(x)|>\epsilon$ . So $f(x)\in (-\infty,-\epsilon)\cup(\epsilon,\infty)$ . But f has Darboux property because it has $F$ as a primitive. So $f(x)<-\epsilon$ or $f(x)>\epsilon$ for every $x$ real number. In the case when $f(x)<-\epsilon$ for every $x$ , we have $f(x)<0$ for every $x$ so $F$ is strictly decreasing and it has limits at $\pm\infty$ (I don't know if this helps). I tried using Lagrange theorem. If we take $a<b$ then $F(a)>F(b)>0$ and by Lagrange theorem we get that $\frac{F(b)-F(a)}{b-a}=f(c)<-\epsilon$ . I dont know how to finish the problem and how to get to a contradiction. I feel like I need to use some limit or derivative close to $\epsilon$ to get a contradiction.","We have a function that has a primitive . Show that for every , there exists such that . I supposed that there exists an such that for every real number we have . So . But f has Darboux property because it has as a primitive. So or for every real number. In the case when for every , we have for every so is strictly decreasing and it has limits at (I don't know if this helps). I tried using Lagrange theorem. If we take then and by Lagrange theorem we get that . I dont know how to finish the problem and how to get to a contradiction. I feel like I need to use some limit or derivative close to to get a contradiction.","f:\mathbb R\to \mathbb R F:\mathbb R\to (0,\infty) \epsilon>0 x_{\epsilon} |f(x_{\epsilon})|<\epsilon \epsilon>0 x |f(x)|>\epsilon f(x)\in (-\infty,-\epsilon)\cup(\epsilon,\infty) F f(x)<-\epsilon f(x)>\epsilon x f(x)<-\epsilon x f(x)<0 x F \pm\infty a<b F(a)>F(b)>0 \frac{F(b)-F(a)}{b-a}=f(c)<-\epsilon \epsilon","['integration', 'derivatives', 'epsilon-delta', 'mean-value-theorem']"
33,Intersection of perpendicular tangent lines - generalization of directrix?,Intersection of perpendicular tangent lines - generalization of directrix?,,"This is a funny little problem that I came up with. For a differentiable function $f$ , define a locus of points $P$ as follows: Let $m$ be an arbitrary tangent line to $f$ , and let $n$ be another tangent line that is perpendicular to $m$ . A point is in $P$ iff it is the intersection of $m,n$ . For a parabola, $P$ is the directrix (you can prove this with algebra or with geometry). But for other types of graphs, it gets a little weirder. Some examples: For $f(x)=x^3$ , $P= \emptyset$ since $f'(x) \geq 0 \space \forall x$ . For $f(x) = \sin(x)$ , $P$ is a lattice of points consisting of the intersections of tangent lines having a slope of 1 and -1. The specific problem I'm taking aim at is how to get an explicit description of what $P$ would look like for $f(x) = x^4$ . I have drawn it in Geogebra and it looks like an inverted bell curve, which makes sense, but I can't get an actual equation to describe it. The same algebraic techniques used to show that $P$ will be the directrix when $f$ is a parabola end up with awful equations that I'm not even sure have a closed form solution. The curve just looks so... I dunno... nice? I feel like there must be a much simpler answer than the pages of power functions that assail me every time I try this.","This is a funny little problem that I came up with. For a differentiable function , define a locus of points as follows: Let be an arbitrary tangent line to , and let be another tangent line that is perpendicular to . A point is in iff it is the intersection of . For a parabola, is the directrix (you can prove this with algebra or with geometry). But for other types of graphs, it gets a little weirder. Some examples: For , since . For , is a lattice of points consisting of the intersections of tangent lines having a slope of 1 and -1. The specific problem I'm taking aim at is how to get an explicit description of what would look like for . I have drawn it in Geogebra and it looks like an inverted bell curve, which makes sense, but I can't get an actual equation to describe it. The same algebraic techniques used to show that will be the directrix when is a parabola end up with awful equations that I'm not even sure have a closed form solution. The curve just looks so... I dunno... nice? I feel like there must be a much simpler answer than the pages of power functions that assail me every time I try this.","f P m f n m P m,n P f(x)=x^3 P= \emptyset f'(x) \geq 0 \space \forall x f(x) = \sin(x) P P f(x) = x^4 P f","['derivatives', 'locus']"
34,Fundamental theorem of calculus with the derivative on the inside?,Fundamental theorem of calculus with the derivative on the inside?,,I know that: $$\frac{d}{dx}\int_{a}^{g(x)} f(t)dt = f(g(x))*g'(x)$$ But what about: $$\int_{a}^{g(x)}\frac{d}{dx}f(x)dx$$ An example of this would be: $$\int_{3}^{t^3}\frac{d}{dx}\frac{x}{x-2}dx$$ Do we apply the chain rule with the $t^3$ ? This is the question and solution:,I know that: But what about: An example of this would be: Do we apply the chain rule with the ? This is the question and solution:,\frac{d}{dx}\int_{a}^{g(x)} f(t)dt = f(g(x))*g'(x) \int_{a}^{g(x)}\frac{d}{dx}f(x)dx \int_{3}^{t^3}\frac{d}{dx}\frac{x}{x-2}dx t^3,"['calculus', 'integration', 'derivatives', 'leibniz-integral-rule']"
35,Derivative of simple matrix product,Derivative of simple matrix product,,"I have been trying for the past few days to find a solution to this very simple problem, yet every time I seem to hit a wall. (PS: please excuse the poor notations, it's my first time using this website). Let's say we have $C = AB$ , $A$ and $B$ matrices so that $C$ is defined. What is $\frac{\partial C}{\partial A}$ and $\frac{\partial C}{\partial B}$ , i.e., the derivatives of $C$ w.r.t $A$ and $B$ respectively? Let us assume size $A = m \times n$ , size $B = n \times p$ , size $C = m \times p$ By doing $C_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}$ for $i = 1, \dots, m$ and $j = 1, \dots, p$ Then $\frac{\partial C_{ij}}{\partial a_{ls}} = \frac{\partial (\sum_{k=1}^{n} a_{ik} b_{kj})}{\partial a_{ls}} = b_{sj}$ for $i = l$ Then shouldn't $\frac{\partial C}{\partial A}$ be the rate of change for each $C_{ij}$ w.r.t each element of $A$ ? So that $\frac{\partial C}{\partial A}$ is a matrix $D$ of size $C$ where each $D_{ij}$ is a matrix of size $A$ . For instance $C = AB$ , given $A$ of size $1 \times 3$ and $B$ of size $3 \times 2$ . $A = $$\begin{pmatrix}a_1 & a_2 & a_3 \end{pmatrix}$ $B = $$\begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \\ b_{31} & b_{32}\end{pmatrix}$ $C = \begin{pmatrix} a_1 b_{11} + a_2 b_{21} + a_3 b_{31} & a_1 b_{12} + a_2 b_{22} + a_3 b_{32} \end{pmatrix}$ Then $\frac{\partial C_1}{\partial B} = \frac{\partial (a_1 b_{11} + a_2 b_{21} + a_3 b_{31})}{\partial B} = \begin{pmatrix} a_1 & 0 & a_2 & 0 & a_3 & 0 \end{pmatrix}$ and $\frac{\partial C_2}{\partial B} = \frac{\partial (a_1 b_{12} + a_2 b_{22} + a_3 b_{32})}{\partial B} = \begin{pmatrix} 0 & a_1 & 0 & a_2 & 0 & a_3 \end{pmatrix}$ So that $\frac{\partial C}{\partial B} = \begin{pmatrix} a_1 & 0 & a_2 & 0 & a_3 & 0 \\ 0 & a_1 & 0 & a_2 & 0 & a_3 \end{pmatrix}$ , is of size $2 \times 6$ . $\frac{\partial C}{\partial A} = B^T$ . However, in the machine learning equations, it is given that $\frac{\partial C}{\partial B} = A^T$ . To be more precise, the context of this query is for machine learning and backpropagation. We have $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \hat{Y}} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial \text{out}}{\partial W}$ , where $L$ is a scalar function (the loss), $\hat{Y}$ is a $1 \times 2$ matrix (the activated output), $\text{out}$ is a $1 \times 2$ vector (pre-activation), and $W$ is a $3 \times 2$ matrix (the weights). If you could also explain why we actually write: $\frac{\partial L}{\partial W} = \frac{\partial \text{out}}{\partial W} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial L}{\partial \hat{Y}}$ , instead of the forward notation used earlier $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \hat{Y}} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial \text{out}}{\partial W}$ Is there an explanation beyond ""cause it fits for the matrix product so we rearrange the terms in the chain rule""? Asking Chat-GPT these questions yields different answers every time. Thank you for your time. Edit : the formatting was done using Chat-GPT. I also checked the link : Not understanding derivative of a matrix-matrix product. and it seems that the result AX is a Kronecker product, which also seems to not be the right size. The part where I usually get stuck at is computing every $D_{ij}$ for $D = \frac{\partial C}{\partial B}$ . Other EDIT : Here is more context for the chain derivation above. $L(Y, \hat{Y}) = \frac{1}{2m} \sum_{i=1}^{m}(Y^{(i)} - \hat{Y}^{(i)})^2$ where $\hat{Y}$ is the predicted out $ \hat{Y} = \sigma_{o}(out) $ $ out = h_{2} W $ where $\sigma_{o}$ is the activation function for the out layer, $h_{2}$ is of size $1 \times 3$ , $W$ of size $3 \times 2$ , $out$ of size $1 \times 2$ and $\hat{y}$ of size $1 \times 2$ Here the activation function $\sigma_{o}(X)$ is applied element-wise to $X$ , a matrix.","I have been trying for the past few days to find a solution to this very simple problem, yet every time I seem to hit a wall. (PS: please excuse the poor notations, it's my first time using this website). Let's say we have , and matrices so that is defined. What is and , i.e., the derivatives of w.r.t and respectively? Let us assume size , size , size By doing for and Then for Then shouldn't be the rate of change for each w.r.t each element of ? So that is a matrix of size where each is a matrix of size . For instance , given of size and of size . Then and So that , is of size . . However, in the machine learning equations, it is given that . To be more precise, the context of this query is for machine learning and backpropagation. We have , where is a scalar function (the loss), is a matrix (the activated output), is a vector (pre-activation), and is a matrix (the weights). If you could also explain why we actually write: , instead of the forward notation used earlier Is there an explanation beyond ""cause it fits for the matrix product so we rearrange the terms in the chain rule""? Asking Chat-GPT these questions yields different answers every time. Thank you for your time. Edit : the formatting was done using Chat-GPT. I also checked the link : Not understanding derivative of a matrix-matrix product. and it seems that the result AX is a Kronecker product, which also seems to not be the right size. The part where I usually get stuck at is computing every for . Other EDIT : Here is more context for the chain derivation above. where is the predicted out where is the activation function for the out layer, is of size , of size , of size and of size Here the activation function is applied element-wise to , a matrix.","C = AB A B C \frac{\partial C}{\partial A} \frac{\partial C}{\partial B} C A B A = m \times n B = n \times p C = m \times p C_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj} i = 1, \dots, m j = 1, \dots, p \frac{\partial C_{ij}}{\partial a_{ls}} = \frac{\partial (\sum_{k=1}^{n} a_{ik} b_{kj})}{\partial a_{ls}} = b_{sj} i = l \frac{\partial C}{\partial A} C_{ij} A \frac{\partial C}{\partial A} D C D_{ij} A C = AB A 1 \times 3 B 3 \times 2 A = \begin{pmatrix}a_1 & a_2 & a_3 \end{pmatrix} B = \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \\ b_{31} & b_{32}\end{pmatrix} C = \begin{pmatrix} a_1 b_{11} + a_2 b_{21} + a_3 b_{31} & a_1 b_{12} + a_2 b_{22} + a_3 b_{32} \end{pmatrix} \frac{\partial C_1}{\partial B} = \frac{\partial (a_1 b_{11} + a_2 b_{21} + a_3 b_{31})}{\partial B} = \begin{pmatrix} a_1 & 0 & a_2 & 0 & a_3 & 0 \end{pmatrix} \frac{\partial C_2}{\partial B} = \frac{\partial (a_1 b_{12} + a_2 b_{22} + a_3 b_{32})}{\partial B} = \begin{pmatrix} 0 & a_1 & 0 & a_2 & 0 & a_3 \end{pmatrix} \frac{\partial C}{\partial B} = \begin{pmatrix} a_1 & 0 & a_2 & 0 & a_3 & 0 \\ 0 & a_1 & 0 & a_2 & 0 & a_3 \end{pmatrix} 2 \times 6 \frac{\partial C}{\partial A} = B^T \frac{\partial C}{\partial B} = A^T \frac{\partial L}{\partial W} = \frac{\partial L}{\partial \hat{Y}} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial \text{out}}{\partial W} L \hat{Y} 1 \times 2 \text{out} 1 \times 2 W 3 \times 2 \frac{\partial L}{\partial W} = \frac{\partial \text{out}}{\partial W} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial L}{\partial \hat{Y}} \frac{\partial L}{\partial W} = \frac{\partial L}{\partial \hat{Y}} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial \text{out}}{\partial W} D_{ij} D = \frac{\partial C}{\partial B} L(Y, \hat{Y}) = \frac{1}{2m} \sum_{i=1}^{m}(Y^{(i)} - \hat{Y}^{(i)})^2 \hat{Y}  \hat{Y} = \sigma_{o}(out)   out = h_{2} W  \sigma_{o} h_{2} 1 \times 3 W 3 \times 2 out 1 \times 2 \hat{y} 1 \times 2 \sigma_{o}(X) X","['matrices', 'derivatives', 'partial-derivative', 'matrix-calculus', 'machine-learning']"
36,Solve the differential equation : $(x^2y-2xy^2)dx-(x^3-3x^2y)dy=0.$,Solve the differential equation :,(x^2y-2xy^2)dx-(x^3-3x^2y)dy=0.,"Solve the differential equation : $(x^2y-2xy^2)dx-(x^3-3x^2y)dy=0.$ My solution goes like this: This is a first order and first degree differential equation of the form $Mdx+Ndy=0,$ where $M=x^2y-2xy^2,N=-(x^3-3x^2y)=3x^2y-x^3.$ Now, since, $Mx+Ny=x^2y^2\neq 0,$ and homogeneous as well, so the integrating factor will be $\frac{1}{Mx+Ny}=\frac{1}{x^2y^2}.$ Multiplying $\frac{1}{x^2y^2}$ on both sides of the given equation we get, an exact differential equation i.e $$\frac{1}{x^2y^2}[(x^2y-2xy^2)dx-(x^3-3x^2y)dy]=0.$$ Now, the solution of this exact differential equation is $$\int [(\frac{x^2y-2xy^2}{x^2y^2})]dx+\int  \frac 3y dy=c\implies \frac xy - 2\log x+3\log y=c,$$ where $c$ is an arbitary constant. Is the above solution correct? If not, where is it going wrong ?","Solve the differential equation : My solution goes like this: This is a first order and first degree differential equation of the form where Now, since, and homogeneous as well, so the integrating factor will be Multiplying on both sides of the given equation we get, an exact differential equation i.e Now, the solution of this exact differential equation is where is an arbitary constant. Is the above solution correct? If not, where is it going wrong ?","(x^2y-2xy^2)dx-(x^3-3x^2y)dy=0. Mdx+Ndy=0, M=x^2y-2xy^2,N=-(x^3-3x^2y)=3x^2y-x^3. Mx+Ny=x^2y^2\neq 0, \frac{1}{Mx+Ny}=\frac{1}{x^2y^2}. \frac{1}{x^2y^2} \frac{1}{x^2y^2}[(x^2y-2xy^2)dx-(x^3-3x^2y)dy]=0. \int [(\frac{x^2y-2xy^2}{x^2y^2})]dx+\int  \frac 3y dy=c\implies \frac xy - 2\log x+3\log y=c, c","['calculus', 'ordinary-differential-equations', 'derivatives', 'solution-verification']"
37,"Showing that $f \in \mathrm{BV}[a,b] \implies \int_a^b |f'| \le V[f;a,b]$",Showing that,"f \in \mathrm{BV}[a,b] \implies \int_a^b |f'| \le V[f;a,b]","Problem Statement: If $f$ is of bounded variation on $[a,b]$ , show that $$\newcommand{\nc}{\newcommand} \nc{\R}{\mathbb{R}} \nc{\BV}{\mathrm{BV}} \nc{\PP}{\mathcal{P}} \nc{\abs}[1]{\left|#1\right|} \nc{\set}[1]{\left\{ #1 \right\}} \nc{\para}[1]{\left( #1 \right)} \nc{\br}[1]{\left[ #1 \right]} \nc{\ve}{\varepsilon} \nc{\vp}{\varphi} \int_a^b \abs{f'} \le V[a;b] $$ This is the first part of Problem $7.9$ in Measure & Integral: An Introduction to Real Analysis by Richard Wheeden and Antoni Zygmund. Notes / Definitions: Here, the authors choose to denote total variation by $V[a;b]$ ; other common notations include $V[f;a,b]$ (specifying $f$ ), $\mathrm{TV}[f;a,b]$ , $V_a^b(f)$ , and so on. Integration is meant in the Lebesgue sense; we are dealing with functions $f : [a,b] \to \R$ . Given $f : [a,b] \to \R$ , we define its variation by $$ V[f;a,b] := \sup_{P \in \PP_{a,b}} \sum_i \abs{ f(x_i) - f(x_{i-1}) } $$ We may associate a variation to a given partition by $$ V(f;P) := \sum_i \abs{ f(x_i) - f(x_{i-1}) } $$ If $V[f;a,b] < \infty$ , then $f$ is of bounded variation and we say $f \in \BV[a,b]$ . Some Thoughts: It is known that $\BV[a,b]$ functions are differentiable a.e. with derivative in $L^1$ (Corollary $7.23$ of the source text), so the well-definedness of the integral is a non-issue. I've had a couple of ideas that went nowhere, however. Jordan Decomposition: We use that $\BV[a,b]$ functions may be written as the difference of monotone-increasing functions, and that monotone functions are differentiable a.e. Note that, since $f \in \BV[a,b]$ , we may write $f = \vp - \psi$ for $\vp,\psi$ monotone-increasing functions (and hence differentiable a.e.). Note that this means $\vp',\psi' \ge 0$ . Then $$\begin{align*} \abs{f'} &= \abs{\vp' - \psi'} \\ &\le \abs{\vp'} + \abs{\psi'} \\ &= \vp' + \psi' \end{align*}$$ giving us $$\begin{align*} \int_a^b \abs{f'} &\le \int_a^b \vp' + \int_a^b \psi' \\ &\le \vp(b) - \vp(a) + \psi(b) - \psi(a) \\ &\le V(\vp;P) + V(\psi;P) \end{align*}$$ for any partition $P = \set{x_i}_{i=0}^n$ . As $\vp,\psi$ are increasing, we may drop the absolute values in their variation: $$\begin{align*} V(\vp;P) + V(\psi;P) &= \sum_i \abs{ \vp(x_i) - \vp(x_{i-1}) } + \sum_i \abs{ \psi(x_i) - \psi(x_{i-1}) }\\ &= \sum_i  \vp(x_i) - \vp(x_{i-1})  + \sum_i   \psi(x_i) - \psi(x_{i-1})  \\ &= \sum_i  \vp(x_i) - \vp(x_{i-1})  +    \psi(x_i) - \psi(x_{i-1})  \\ &= V(\vp+\psi;P) \end{align*}$$ This does not help us, as $f \ne \vp + \psi$ . This approach via decomposition is noted in this MSE post and related ones, but not to a sufficient level of detail to grasp what I need to finish the proof. Limit Definitions: Consider a sequence of partitions $P_n := \set{x_i^{(n)}}_{i=0}^{k_n}$ of $[a,b]$ , ordered in the usual sense. For simplicity, let $$\begin{align*} \Delta_n x_i &:= x_i^{(n)} - x_{i-1}^{(n)} \\ \Delta_n f &:= f \para{ x_i^{(n)} } - f \para{ x_i^{(n)} } \end{align*}$$ Then for each $i$ , by the mean value theorem, $\exists \xi_i^{(n)} \in \br{ x_{i-1}^{(n)} , x_{i}^{(n)} }$ such that $$ \frac{\Delta_n f}{\Delta_n x_i} = f' \para{ \xi_i^{(n)} } $$ and hence $$ \abs{\Delta_n f} = \abs{f' \para{ \xi_i^{(n)} } \Delta_n x_i}= \abs{f' \para{ \xi_i^{(n)} }} \Delta_n x_i $$ and thus $$ \sum_i \abs{\Delta_n f}  = \sum_i \abs{f' \para{ \xi_i^{(n)} }} \Delta_n x_i $$ Taking the limit $n \to \infty$ then gives $$ V[f;a,b] = \int_a^b \abs{f'} $$ However, this is obviously stronger than necessary and does not clearly use that $f \in \BV[a,b]$ . I suspect the core issue lies with the existence of the $\xi_i^{(n)}$ ; perhaps there is a scenario in which they do not always exist? (I suppose issues on a zero-measure dense set might be enough to break this argument.) Still, there is an appeal in proving this for first principles, so it's hard for me to let go of this approach. Does anyone have ideas -- be it in terms of alternative approaches, or ways to salvage these?","Problem Statement: If is of bounded variation on , show that This is the first part of Problem in Measure & Integral: An Introduction to Real Analysis by Richard Wheeden and Antoni Zygmund. Notes / Definitions: Here, the authors choose to denote total variation by ; other common notations include (specifying ), , , and so on. Integration is meant in the Lebesgue sense; we are dealing with functions . Given , we define its variation by We may associate a variation to a given partition by If , then is of bounded variation and we say . Some Thoughts: It is known that functions are differentiable a.e. with derivative in (Corollary of the source text), so the well-definedness of the integral is a non-issue. I've had a couple of ideas that went nowhere, however. Jordan Decomposition: We use that functions may be written as the difference of monotone-increasing functions, and that monotone functions are differentiable a.e. Note that, since , we may write for monotone-increasing functions (and hence differentiable a.e.). Note that this means . Then giving us for any partition . As are increasing, we may drop the absolute values in their variation: This does not help us, as . This approach via decomposition is noted in this MSE post and related ones, but not to a sufficient level of detail to grasp what I need to finish the proof. Limit Definitions: Consider a sequence of partitions of , ordered in the usual sense. For simplicity, let Then for each , by the mean value theorem, such that and hence and thus Taking the limit then gives However, this is obviously stronger than necessary and does not clearly use that . I suspect the core issue lies with the existence of the ; perhaps there is a scenario in which they do not always exist? (I suppose issues on a zero-measure dense set might be enough to break this argument.) Still, there is an appeal in proving this for first principles, so it's hard for me to let go of this approach. Does anyone have ideas -- be it in terms of alternative approaches, or ways to salvage these?","f [a,b] \newcommand{\nc}{\newcommand}
\nc{\R}{\mathbb{R}}
\nc{\BV}{\mathrm{BV}}
\nc{\PP}{\mathcal{P}}
\nc{\abs}[1]{\left|#1\right|}
\nc{\set}[1]{\left\{ #1 \right\}}
\nc{\para}[1]{\left( #1 \right)}
\nc{\br}[1]{\left[ #1 \right]}
\nc{\ve}{\varepsilon}
\nc{\vp}{\varphi}
\int_a^b \abs{f'} \le V[a;b]
 7.9 V[a;b] V[f;a,b] f \mathrm{TV}[f;a,b] V_a^b(f) f : [a,b] \to \R f : [a,b] \to \R 
V[f;a,b] := \sup_{P \in \PP_{a,b}} \sum_i \abs{ f(x_i) - f(x_{i-1}) }
 
V(f;P) := \sum_i \abs{ f(x_i) - f(x_{i-1}) }
 V[f;a,b] < \infty f f \in \BV[a,b] \BV[a,b] L^1 7.23 \BV[a,b] f \in \BV[a,b] f = \vp - \psi \vp,\psi \vp',\psi' \ge 0 \begin{align*}
\abs{f'} &= \abs{\vp' - \psi'} \\
&\le \abs{\vp'} + \abs{\psi'} \\
&= \vp' + \psi'
\end{align*} \begin{align*}
\int_a^b \abs{f'} &\le \int_a^b \vp' + \int_a^b \psi' \\
&\le \vp(b) - \vp(a) + \psi(b) - \psi(a) \\
&\le V(\vp;P) + V(\psi;P)
\end{align*} P = \set{x_i}_{i=0}^n \vp,\psi \begin{align*}
V(\vp;P) + V(\psi;P)
&= \sum_i \abs{ \vp(x_i) - \vp(x_{i-1}) } + \sum_i \abs{ \psi(x_i) - \psi(x_{i-1}) }\\
&= \sum_i  \vp(x_i) - \vp(x_{i-1})  + \sum_i   \psi(x_i) - \psi(x_{i-1})  \\
&= \sum_i  \vp(x_i) - \vp(x_{i-1})  +    \psi(x_i) - \psi(x_{i-1})  \\
&= V(\vp+\psi;P)
\end{align*} f \ne \vp + \psi P_n := \set{x_i^{(n)}}_{i=0}^{k_n} [a,b] \begin{align*}
\Delta_n x_i &:= x_i^{(n)} - x_{i-1}^{(n)} \\
\Delta_n f &:= f \para{ x_i^{(n)} } - f \para{ x_i^{(n)} }
\end{align*} i \exists \xi_i^{(n)} \in \br{ x_{i-1}^{(n)} , x_{i}^{(n)} } 
\frac{\Delta_n f}{\Delta_n x_i} = f' \para{ \xi_i^{(n)} }
 
\abs{\Delta_n f} = \abs{f' \para{ \xi_i^{(n)} } \Delta_n x_i}= \abs{f' \para{ \xi_i^{(n)} }} \Delta_n x_i
 
\sum_i \abs{\Delta_n f}  = \sum_i \abs{f' \para{ \xi_i^{(n)} }} \Delta_n x_i
 n \to \infty 
V[f;a,b] = \int_a^b \abs{f'}
 f \in \BV[a,b] \xi_i^{(n)}","['real-analysis', 'integration', 'derivatives', 'lebesgue-integral', 'bounded-variation']"
38,Bijection but not diffeomorphism,Bijection but not diffeomorphism,,"Let $f:\mathbb{R}\to \mathbb{R}$ be the function defined as $$f(x) = \begin{cases} x+x^2\sin \frac{1}{x}, & \text{if }x\neq 0 \\ 0, & \text{if }x=0. \end{cases}$$ I want to prove the following: there are neighborhoods $U$ of $0$ and $V$ of $0$ such that $f:U\to V$ is a bijection. there is no neighborhood of $0$ where the function $f$ is diffeomorphism. My approach: I think that it should be not so difficult but I am missing something. First of all, one can show that $$f'(x) = \begin{cases} 1+2x\sin \frac{1}{x}-\cos \frac{1}{x}, & \text{if }x\neq 0 \\ 1, & \text{if }x=0. \end{cases}$$ Also $f'(\frac{1}{2\pi n})=0$ , i.e, one can find arbitrary small zero of $f'$ . I am wondering can anyone answer my questions? Thank you!","Let be the function defined as I want to prove the following: there are neighborhoods of and of such that is a bijection. there is no neighborhood of where the function is diffeomorphism. My approach: I think that it should be not so difficult but I am missing something. First of all, one can show that Also , i.e, one can find arbitrary small zero of . I am wondering can anyone answer my questions? Thank you!","f:\mathbb{R}\to \mathbb{R} f(x) =
\begin{cases}
x+x^2\sin \frac{1}{x}, & \text{if }x\neq 0 \\
0, & \text{if }x=0.
\end{cases} U 0 V 0 f:U\to V 0 f f'(x) =
\begin{cases}
1+2x\sin \frac{1}{x}-\cos \frac{1}{x}, & \text{if }x\neq 0 \\
1, & \text{if }x=0.
\end{cases} f'(\frac{1}{2\pi n})=0 f'","['real-analysis', 'derivatives']"
39,Complex Derivative: $\frac{d}{dz} (z+\overline{z} -z^3+z\overline{z})$ [closed],Complex Derivative:  [closed],\frac{d}{dz} (z+\overline{z} -z^3+z\overline{z}),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question How can I compute the following complex derivative $$\frac{d}{dz} (z+\overline{z} -z^3+z\overline{z})$$ I know that $\frac{d}{dz} (z\overline{z})=z$ but not sure about the above. Original problem: $\displaystyle \frac{\partial}{\partial \bar{z}}(z+\bar{z}-z^3+z\bar{z})$ . $\displaystyle \frac{\partial}{\partial \bar{z}}((z+\bar{z}+16z^8-\bar{z})(-z^4+z^7))$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question How can I compute the following complex derivative I know that but not sure about the above. Original problem: .",\frac{d}{dz} (z+\overline{z} -z^3+z\overline{z}) \frac{d}{dz} (z\overline{z})=z \displaystyle \frac{\partial}{\partial \bar{z}}(z+\bar{z}-z^3+z\bar{z}) \displaystyle \frac{\partial}{\partial \bar{z}}((z+\bar{z}+16z^8-\bar{z})(-z^4+z^7)),"['complex-analysis', 'derivatives', 'complex-numbers', 'partial-derivative']"
40,$\frac{\partial \text{score}(x; \lambda)}{\partial \lambda}$.,.,\frac{\partial \text{score}(x; \lambda)}{\partial \lambda},Apparently I forgot how to do take a derivative.. This is the score of some Poisson distribution random variables: $\text{score}(x; \lambda) = \frac{x}{\lambda} - 1$ . Now I want to take the derivative according to the parameter $\lambda$ . $\frac{\partial score(x; \lambda)}{\partial \lambda}$ .,Apparently I forgot how to do take a derivative.. This is the score of some Poisson distribution random variables: . Now I want to take the derivative according to the parameter . .,\text{score}(x; \lambda) = \frac{x}{\lambda} - 1 \lambda \frac{\partial score(x; \lambda)}{\partial \lambda},"['derivatives', 'fisher-information']"
41,How to find the derivative w.r.t. lower limits?,How to find the derivative w.r.t. lower limits?,,"How to find $\frac{d}{dy} \int_{y}^{\infty} \int_{2y}^{\infty} y f(x_2)dx_2 f(x_1)dx_1$ , where $x_1$ and $x_2$ are two independent continuous random variables and $f(x_1)$ and $f(x_2)$ are their PDFs. Can I still use Leibniz's rule?","How to find , where and are two independent continuous random variables and and are their PDFs. Can I still use Leibniz's rule?",\frac{d}{dy} \int_{y}^{\infty} \int_{2y}^{\infty} y f(x_2)dx_2 f(x_1)dx_1 x_1 x_2 f(x_1) f(x_2),"['probability', 'integration', 'derivatives', 'leibniz-integral-rule']"
42,On the differentiability of increasing functions,On the differentiability of increasing functions,,"I'm trying to follow the proof of this website http://mathonline.wikidot.com/lebesgue-s-theorem-for-the-differentiability-of-monotone-fun about the differentiability a.e. of increasing functions. Although, there's something I don't understand. It says: ""Now note that for each $k\in\{1,2,...,n\}$ we have that $E\cap[c_k,d_k]\subseteq\{x\in[c_k,d_k]:\overline{D}f(x)\geq|\alpha|\}$ "". I don't know how to obtain that absolute value of $\alpha$ . The only thing I know about that set is the following: $$E\cap[c_k,d_k]=\{x\in[c_k,d_k]:\underline{D}f(x)<\beta<\alpha<\overline{D}f(x)<+\infty\}.$$ Thanks in advance.","I'm trying to follow the proof of this website http://mathonline.wikidot.com/lebesgue-s-theorem-for-the-differentiability-of-monotone-fun about the differentiability a.e. of increasing functions. Although, there's something I don't understand. It says: ""Now note that for each we have that "". I don't know how to obtain that absolute value of . The only thing I know about that set is the following: Thanks in advance.","k\in\{1,2,...,n\} E\cap[c_k,d_k]\subseteq\{x\in[c_k,d_k]:\overline{D}f(x)\geq|\alpha|\} \alpha E\cap[c_k,d_k]=\{x\in[c_k,d_k]:\underline{D}f(x)<\beta<\alpha<\overline{D}f(x)<+\infty\}.","['calculus', 'derivatives', 'lebesgue-measure', 'monotone-functions', 'bounded-variation']"
43,"$F$ is Lipschitz $\Longrightarrow$ $F(x)=F(a)+\int_{a}^{x}f(t)\,dt$",is Lipschitz,"F \Longrightarrow F(x)=F(a)+\int_{a}^{x}f(t)\,dt","According to the Fundamental Theorem of Lebesgue Integral Calculus, the following statement holds: Let $F:[a,b]\to\mathbb{R}$ . Then $F$ is absolutely continuous iff there exists a function $f:[a,b]\to\mathbb{R}$ which is Lebesgue integrable such that: $F(x)=F(a)+\int_{a}^{x}f(t)\,dt$ I am wondering whether the following statement of similar nature holds as well: Let $F:[a,b]\to\mathbb{R}$ . Then $F$ is Lipschitz continuous iff there exists a function $f:[a,b]\to\mathbb{R}$ which is Riemann integrable such that: $F(x)=F(a)+\int_{a}^{x}f(t)\,dt$ Note that the implication "" $\Leftarrow$ "" is already well known, and is usually mentioned as a lemma in preparation for the Fundamental Theorem of Calculus. Is it indeed true that for every Lipschitz continuous function $F:[a,b]\to\mathbb{R}$ there exists a function $f:[a,b]\to\mathbb{R}$ which is Riemann integrable such that: $F(x)=F(a)+\int_{a}^{x}f(t)\,dt$ ? My Attempt: It is already well known that a Lipschitz continuous function is differentiable almost everywhere (and the derivative is clearly bounded). Thus if we only show that this derivative is Riemann integrable, the problem can be solved at once with an existing geralized version of the Newton-Leibniz Theorem. So far, I was not successful in showing the Riemann integrability of that derivative...","According to the Fundamental Theorem of Lebesgue Integral Calculus, the following statement holds: Let . Then is absolutely continuous iff there exists a function which is Lebesgue integrable such that: I am wondering whether the following statement of similar nature holds as well: Let . Then is Lipschitz continuous iff there exists a function which is Riemann integrable such that: Note that the implication "" "" is already well known, and is usually mentioned as a lemma in preparation for the Fundamental Theorem of Calculus. Is it indeed true that for every Lipschitz continuous function there exists a function which is Riemann integrable such that: ? My Attempt: It is already well known that a Lipschitz continuous function is differentiable almost everywhere (and the derivative is clearly bounded). Thus if we only show that this derivative is Riemann integrable, the problem can be solved at once with an existing geralized version of the Newton-Leibniz Theorem. So far, I was not successful in showing the Riemann integrability of that derivative...","F:[a,b]\to\mathbb{R} F f:[a,b]\to\mathbb{R} F(x)=F(a)+\int_{a}^{x}f(t)\,dt F:[a,b]\to\mathbb{R} F f:[a,b]\to\mathbb{R} F(x)=F(a)+\int_{a}^{x}f(t)\,dt \Leftarrow F:[a,b]\to\mathbb{R} f:[a,b]\to\mathbb{R} F(x)=F(a)+\int_{a}^{x}f(t)\,dt","['calculus', 'integration', 'derivatives', 'lebesgue-integral', 'lipschitz-functions']"
44,What is wrong with this proof of the derivative product rule?,What is wrong with this proof of the derivative product rule?,,"I was trying to prove the derivative product rule, but I got a wrong result and nothing seems wrong with my proof. If someone could help me, I'd really appreciate it. Thanks  in advance $f(x) = g(x)h(x)$ Prove $f^\prime (x) = g(x)^\prime h(x) + g(x)h(x)^\prime$ My proof: \begin{align} f^\prime (x) &= \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} \\ &= \lim_{h \to 0} \frac{g(x+h)h(x+h) - g(x)h(x)}{h}\\  &=\lim_{h \to 0} \left(\frac{g(x+h)}{h} \cdot h(x+h)\right)-\lim_{h \to 0} \left(\frac{g(x)}{h} \cdot h(x) \right)\\ &= \lim_{h \to 0} \frac{g(x+h)}{h} \cdot \lim_{h \to 0} h(x+h) - \lim_{h \to 0} \frac{g(x)}{h} \cdot \lim_{h \to 0} h(x)\\ &= h(x) \cdot \lim_{h \to 0} \frac{g(x+h)}{h} - h(x) \cdot \lim_{h \to 0} \frac{g(x)}{h} \\ &= h(x) \left( \lim_{h \to 0} \frac{g(x+h)}{h} - \lim_{h \to 0} \frac{g(x)}{h} \right)\\ &= h(x) \cdot \lim_{h \to 0} \frac{g(x+h)-g(x)}{h} \\ &= h(x) \cdot g^\prime (x) \end{align}","I was trying to prove the derivative product rule, but I got a wrong result and nothing seems wrong with my proof. If someone could help me, I'd really appreciate it. Thanks  in advance Prove My proof:","f(x) = g(x)h(x) f^\prime (x) = g(x)^\prime h(x) + g(x)h(x)^\prime \begin{align}
f^\prime (x)
&= \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} \\
&= \lim_{h \to 0} \frac{g(x+h)h(x+h) - g(x)h(x)}{h}\\ 
&=\lim_{h \to 0} \left(\frac{g(x+h)}{h} \cdot h(x+h)\right)-\lim_{h \to 0} \left(\frac{g(x)}{h} \cdot h(x) \right)\\
&= \lim_{h \to 0} \frac{g(x+h)}{h} \cdot \lim_{h \to 0} h(x+h) - \lim_{h \to 0} \frac{g(x)}{h} \cdot \lim_{h \to 0} h(x)\\
&= h(x) \cdot \lim_{h \to 0} \frac{g(x+h)}{h} - h(x) \cdot \lim_{h \to 0} \frac{g(x)}{h} \\
&= h(x) \left( \lim_{h \to 0} \frac{g(x+h)}{h} - \lim_{h \to 0} \frac{g(x)}{h} \right)\\
&= h(x) \cdot \lim_{h \to 0} \frac{g(x+h)-g(x)}{h} \\
&= h(x) \cdot g^\prime (x)
\end{align}","['calculus', 'limits', 'derivatives', 'solution-verification']"
45,Proving the existence and unicity of a stationary point,Proving the existence and unicity of a stationary point,,"Last question for today I promise. I have the function $f(x) = x^3\ln(x) - x$ . I have to find its absolute or local max and min, if any. So I studied the derivative but it's not a function I can properly solve, when imposing it equals to zero. So I thought about proving things in different analytical way. The fact is that I am stuck in certan parts. So furst of all I have $$f'(x) = 3x^2\ln(x) + x^2 - 1$$ which is continuous in its domain (the same of $f(x)$ ). I can see ""by hand"" that $x = 1$ is a solution of $f'(x) = 0$ . The fact is that I want indeed to prove that $x = 1$ is a solution, that is there exist a solution. Then I want to prove it's unique. And then I would like to show that that solution is indeed $x = 1$ . So I thought about considering $g(x) = f'(x)$ and hence studying in terms of $g(x)$ , that is: $g'(x) = 5x + 6x\ln(x)$ , observing that $g'(x) > 0$ for $x > e^{-5/6}$ . This means $g(x)$ is increasing for $x = e^{-5/6}$ and consequently $f(x)$ is convex in that same interval. Since there is a change in the monotonicity of $g(x) = f'(x)$ , then $f(x)$ has a minimum (due to how the monotonicity changes) and only one, due to having shown there is only one solution for $g'(x)$ , since the solution $x = 0$ is trashed because of Fermat's theorem). Then I showed there exists only one minimum. How can I show that this minima is indeed at $x = 1$ without just saying that ""it's obvious""?","Last question for today I promise. I have the function . I have to find its absolute or local max and min, if any. So I studied the derivative but it's not a function I can properly solve, when imposing it equals to zero. So I thought about proving things in different analytical way. The fact is that I am stuck in certan parts. So furst of all I have which is continuous in its domain (the same of ). I can see ""by hand"" that is a solution of . The fact is that I want indeed to prove that is a solution, that is there exist a solution. Then I want to prove it's unique. And then I would like to show that that solution is indeed . So I thought about considering and hence studying in terms of , that is: , observing that for . This means is increasing for and consequently is convex in that same interval. Since there is a change in the monotonicity of , then has a minimum (due to how the monotonicity changes) and only one, due to having shown there is only one solution for , since the solution is trashed because of Fermat's theorem). Then I showed there exists only one minimum. How can I show that this minima is indeed at without just saying that ""it's obvious""?",f(x) = x^3\ln(x) - x f'(x) = 3x^2\ln(x) + x^2 - 1 f(x) x = 1 f'(x) = 0 x = 1 x = 1 g(x) = f'(x) g(x) g'(x) = 5x + 6x\ln(x) g'(x) > 0 x > e^{-5/6} g(x) x = e^{-5/6} f(x) g(x) = f'(x) f(x) g'(x) x = 0 x = 1,"['derivatives', 'optimization', 'solution-verification', 'maxima-minima']"
46,Solving for third derivative of implicit differentiation,Solving for third derivative of implicit differentiation,,"My professor gave us an activity of, to me what feels like, a vague third derivative implicit differentiation. We were taught up to second, but now I am feeling lost. I am tasked to find $\frac{d^3y}{dx^3}$ , $x^2$ + $y^2$ = $a^2$ . I treated $a^2$ as a constant like what tutorial videos online did, and arrived at a different answer: $\frac{x}{27y^5}$ . None of the choices match this in the question given to me, they all include a in them. Am I looking at $a^2$ differently here?","My professor gave us an activity of, to me what feels like, a vague third derivative implicit differentiation. We were taught up to second, but now I am feeling lost. I am tasked to find , + = . I treated as a constant like what tutorial videos online did, and arrived at a different answer: . None of the choices match this in the question given to me, they all include a in them. Am I looking at differently here?",\frac{d^3y}{dx^3} x^2 y^2 a^2 a^2 \frac{x}{27y^5} a^2,['derivatives']
47,Is there any simple set of properties that uniquely characterizes differentiation in the space of complex functions?,Is there any simple set of properties that uniquely characterizes differentiation in the space of complex functions?,,"The transformation of differentiation is a linear operator over the vector space of entire functions (call this space $\mathbb{C}^E.$ ) Is there any simple set of properties that uniquely determines this linear operator that uses only the field structure of the complex numbers and the vector space structure of $\mathbb{C}^E$ and how they ""interact"" with each other, rather than using an ""absolute value"" that cannot be defined in terms of the field structure alone?","The transformation of differentiation is a linear operator over the vector space of entire functions (call this space ) Is there any simple set of properties that uniquely determines this linear operator that uses only the field structure of the complex numbers and the vector space structure of and how they ""interact"" with each other, rather than using an ""absolute value"" that cannot be defined in terms of the field structure alone?",\mathbb{C}^E. \mathbb{C}^E,"['complex-analysis', 'derivatives', 'soft-question', 'linear-transformations', 'analytic-functions']"
48,Prove that the solution exists for all time.,Prove that the solution exists for all time.,,"Suppose that $x^{\prime}=-x+f(x)$ where $f(x)$ is Lipschitz continuous and $0<f(x)<1$ for all $x \in R$ . Prove that the solution exists for all time. I am reading from Teschl ODE From the notation of page 37-38( I am basically using Picard Lindeloff theorem). If I choose any $T,\delta >>0$ then $M=max_{(t,x)\in V} |f(t,x)|\leq \delta+1$ and then $\frac{\delta}{M}=\frac{\delta}{\delta+1}$ (approx) and then $T_0=\min\{T,\frac{\delta}{M}\}\leq 1$ . Hence, I am stuck about how to show that the solution exists for all time. I am confused about the fallacy here as the function $-x+f(x)$ is globally Lipschitz the solution should exist for all $t$ . Please help me or give me a proof.","Suppose that where is Lipschitz continuous and for all . Prove that the solution exists for all time. I am reading from Teschl ODE From the notation of page 37-38( I am basically using Picard Lindeloff theorem). If I choose any then and then (approx) and then . Hence, I am stuck about how to show that the solution exists for all time. I am confused about the fallacy here as the function is globally Lipschitz the solution should exist for all . Please help me or give me a proof.","x^{\prime}=-x+f(x) f(x) 0<f(x)<1 x \in R T,\delta >>0 M=max_{(t,x)\in V} |f(t,x)|\leq \delta+1 \frac{\delta}{M}=\frac{\delta}{\delta+1} T_0=\min\{T,\frac{\delta}{M}\}\leq 1 -x+f(x) t","['real-analysis', 'ordinary-differential-equations', 'derivatives', 'dynamical-systems', 'lipschitz-functions']"
49,An example of $f$ and $a$ such that $f$ is Gâteaux but not Fréchet differentiable at $a$,An example of  and  such that  is Gâteaux but not Fréchet differentiable at,f a f a,"I'm reading this lecture note about differentiability. Let $(X, |\cdot|_X)$ and $(Y, |\cdot|_Y)$ be normed spaces. Let $A$ be an open subset of $X$ and $f: A \to Y$ . The directional derivative $f^{\prime}(a) (v)$ of $f$ at $a \in A$ along direction $v \in X$ is the limit (if exists) $$ f^{\prime}(a)(v) :=\lim _{t \rightarrow 0} \frac{f(a+t v)-f(a)}{t}. $$ We shall say that $f$ is: Gâteaux differentiable at $a$ if there exists $x^{*} \in X^{*}$ such that $f^{\prime}(a)(v)=x^{*}(v)$ for each $v \in X$ (that is, $f^{\prime}(a)$ is everywhere defined, real-valued, linear and continuous); Then $x^{*}$ is called the Gâteaux differential (or derivative ) of $f$ at $a$ , and is denoted by $d f(a)$ . Fréchet differentiable at $a$ if there exists $x^{*} \in X^{*}$ such that $$ \lim _{h \to 0} \frac{f(a+h)-f(a)-x^{*}(h)}{|h|_X}=0 . $$ Then $x^{*}$ is called the Fréchet differential (or derivative ) of $f$ at $a$ , and is denoted by $\partial f(a)$ . Could you provide an example of $f$ and $a$ such that $f$ is Gâteaux but not Fréchet differentiable at $a$ ? I only find an example in which $f^{\prime}(a)$ exists and is continuous, but not linear. For example, consider the real-valued function $F$ of two real variables defined by $$ F(x, y)= \begin{cases}\frac{x^3}{x^2+y^2} & \text { if }(x, y) \neq(0,0), \\ 0 & \text { if }(x, y)=(0,0).\end{cases} $$ Then $$ d F(0,0) (a, b)=\left\{\begin{array}{ll} \frac{F(\tau a, \tau b)-0}{\tau} & \text{if } (a, b) \neq(0,0), \\ 0 & \text{if } (a, b)=(0,0). \end{array}= \begin{cases}\frac{a^3}{a^2+b^2} & \text{if }  (a, b) \neq(0,0), \\ 0 &\text{if } (a, b)=(0,0).\end{cases}\right. $$","I'm reading this lecture note about differentiability. Let and be normed spaces. Let be an open subset of and . The directional derivative of at along direction is the limit (if exists) We shall say that is: Gâteaux differentiable at if there exists such that for each (that is, is everywhere defined, real-valued, linear and continuous); Then is called the Gâteaux differential (or derivative ) of at , and is denoted by . Fréchet differentiable at if there exists such that Then is called the Fréchet differential (or derivative ) of at , and is denoted by . Could you provide an example of and such that is Gâteaux but not Fréchet differentiable at ? I only find an example in which exists and is continuous, but not linear. For example, consider the real-valued function of two real variables defined by Then","(X, |\cdot|_X) (Y, |\cdot|_Y) A X f: A \to Y f^{\prime}(a) (v) f a \in A v \in X 
f^{\prime}(a)(v) :=\lim _{t \rightarrow 0} \frac{f(a+t v)-f(a)}{t}.
 f a x^{*} \in X^{*} f^{\prime}(a)(v)=x^{*}(v) v \in X f^{\prime}(a) x^{*} f a d f(a) a x^{*} \in X^{*} 
\lim _{h \to 0} \frac{f(a+h)-f(a)-x^{*}(h)}{|h|_X}=0 .
 x^{*} f a \partial f(a) f a f a f^{\prime}(a) F 
F(x, y)= \begin{cases}\frac{x^3}{x^2+y^2} & \text { if }(x, y) \neq(0,0), \\ 0 & \text { if }(x, y)=(0,0).\end{cases}
 
d F(0,0) (a, b)=\left\{\begin{array}{ll}
\frac{F(\tau a, \tau b)-0}{\tau} & \text{if } (a, b) \neq(0,0), \\
0 & \text{if } (a, b)=(0,0).
\end{array}= \begin{cases}\frac{a^3}{a^2+b^2} & \text{if }  (a, b) \neq(0,0), \\
0 &\text{if } (a, b)=(0,0).\end{cases}\right.
","['derivatives', 'examples-counterexamples', 'frechet-derivative', 'gateaux-derivative']"
50,How can I solve this Cauchy equation $(x - 4)^{2}y'' - 5(x - 4)y' + 9y = 4 - x$?,How can I solve this Cauchy equation ?,(x - 4)^{2}y'' - 5(x - 4)y' + 9y = 4 - x,"I am trying to solve the following Cauchy-Euler equation $$(x - 4)^{2}y'' - 5(x - 4)y' + 9y = 4 - x$$ My first step is substituting $x - 4$ by $t$ and the equation becomes $$t^{2}y'' - 5ty' + 9y = -t$$ before finding the value of $m$ I need to use the chain rule so the equation becomes $\mathrm{d}x/\mathrm{d}t$ .  So I need help in this step. I know the chain rule but i don't know how to apply it and use it in this type of equations, so any suggestions?","I am trying to solve the following Cauchy-Euler equation My first step is substituting by and the equation becomes before finding the value of I need to use the chain rule so the equation becomes .  So I need help in this step. I know the chain rule but i don't know how to apply it and use it in this type of equations, so any suggestions?",(x - 4)^{2}y'' - 5(x - 4)y' + 9y = 4 - x x - 4 t t^{2}y'' - 5ty' + 9y = -t m \mathrm{d}x/\mathrm{d}t,"['calculus', 'ordinary-differential-equations', 'derivatives']"
51,"If it is known that $\lim _{h\to 0}(\frac{f(h)}{h}-f'(0))=0$, does this implies that $\lim _{h\to 0}(\frac{\frac{f(h)}{h}-f'(0)}{h})=0$?","If it is known that , does this implies that ?",\lim _{h\to 0}(\frac{f(h)}{h}-f'(0))=0 \lim _{h\to 0}(\frac{\frac{f(h)}{h}-f'(0)}{h})=0,"So I am working on a problem as such: Suppose that $f$ is a function such that $f(0)=0$ and $f'(x)$ and $f''(x)$ exist for any real number $x$ . Let $g$ be a function  with $g(0)\!=\!\!f'(0)$ and $g(x)\!=\!\!f(x)/x$ for $x\neq0$ . Given that $g(x)$ is continuous for any real number $x$ , prove that $g'(x)$ exists for all $x\in\Bbb R$ . My approach to this problem is to divide the case into two case, which is for $x=0$ and $x\neq0$ . The function $g(x)$ can be written into a piecewise as such: $$g(x)=\begin{cases}        f(x)/x \;\;\;\;\;\; x\neq0 \\       f'(0)\;\;\;\;\;\;\;\;\;x=0    \end{cases}$$ For $x\neq0$ : $$g(x)=f(x)/x$$ $$g'(x)=\frac{f'(x)(x)+f(x)}{x^2}$$ Since $f'(x)$ exists for all real number $x$ , this implies that $f(x)$ exists for all real number $x$ . And since $x$ can never be $0$ , then $g'(x)$ exists for all $x\neq0$ . My question is for the case when $x\neq0$ My approach is to use the definition of limit, which is to find out whether $\;\lim\limits_{h\to 0}\dfrac{g(0+h)-g(0)}h\;$ exists or no. If it does, then the limit exists. Using what we know from the question,we can solve this equation: $$\lim\limits_{h\to 0}\dfrac{\frac{f(h)}{h}-f'(0)}{h}$$ And this is where my question comes from. Since we know that $g(x)$ is continous, we can say that $\;\lim\limits_{h\to 0}\left(\dfrac{f(h)}{h}-f'(0)\right)=0$ . But does this imply that $\;\lim\limits_{h\to 0}\dfrac{\frac{f(h)}{h}-f'(0)}{h}=0\;$ and hence $g'(0)$ exists? I am thinking of using the L'Hopital rule for the case $\mathbf{0/0}$ , but is it appropriate in this problem? I have tried it but seems that it does not take me anywhere. If no, how can I solve this problem?","So I am working on a problem as such: Suppose that is a function such that and and exist for any real number . Let be a function  with and for . Given that is continuous for any real number , prove that exists for all . My approach to this problem is to divide the case into two case, which is for and . The function can be written into a piecewise as such: For : Since exists for all real number , this implies that exists for all real number . And since can never be , then exists for all . My question is for the case when My approach is to use the definition of limit, which is to find out whether exists or no. If it does, then the limit exists. Using what we know from the question,we can solve this equation: And this is where my question comes from. Since we know that is continous, we can say that . But does this imply that and hence exists? I am thinking of using the L'Hopital rule for the case , but is it appropriate in this problem? I have tried it but seems that it does not take me anywhere. If no, how can I solve this problem?","f f(0)=0 f'(x) f''(x) x g g(0)\!=\!\!f'(0) g(x)\!=\!\!f(x)/x x\neq0 g(x) x g'(x) x\in\Bbb R x=0 x\neq0 g(x) g(x)=\begin{cases} 
      f(x)/x \;\;\;\;\;\; x\neq0 \\
      f'(0)\;\;\;\;\;\;\;\;\;x=0
   \end{cases} x\neq0 g(x)=f(x)/x g'(x)=\frac{f'(x)(x)+f(x)}{x^2} f'(x) x f(x) x x 0 g'(x) x\neq0 x\neq0 \;\lim\limits_{h\to 0}\dfrac{g(0+h)-g(0)}h\; \lim\limits_{h\to 0}\dfrac{\frac{f(h)}{h}-f'(0)}{h} g(x) \;\lim\limits_{h\to 0}\left(\dfrac{f(h)}{h}-f'(0)\right)=0 \;\lim\limits_{h\to 0}\dfrac{\frac{f(h)}{h}-f'(0)}{h}=0\; g'(0) \mathbf{0/0}","['calculus', 'limits', 'derivatives', 'continuity']"
52,differentiability of a distance function,differentiability of a distance function,,"Let $A\subset\mathbb{R}$ be a nonempty set, and define the function: $$f:\mathbb{R}\to\mathbb{R}\quad,\quad f(x):=\inf_{a\in A}|x-a|$$ My question is -  Is there an explicit characterization of all points $x\in\overline{A}$ at which $f$ is differentiable? My attempt Claim: Let $x\in\overline{A}$ . Then $f$ is differentiable at $x$ iff for every sequence of points ${\{y_n\}}_{n=1}^\infty\subset\mathbb{R}$ s.t. $y_n\to x$ and $y_n\neq x$ for every $n$ , there exists a sequence ${\{a_n\}}_{n=1}^\infty\subset A$ s.t. $\left|\frac{y_n-a_n}{y_n-x}\right|\to0$ . $\Rightarrow$ : $f$ is differentiable at $x\in\overline{A}$ , thus there is a sequence of points ${\{c_n\}}_{n=1}^\infty\subset A$ s.t. $c_n\to x$ , and so: $$f'(x)=\lim_{n\to\infty}\frac{f(c_n)-f(x)}{c_n-x}=\lim_{n\to\infty}\frac{0-0}{c_n-x}=0$$ Let ${\{y_n\}}_{n=1}^\infty\subset\mathbb{R}$ be a sequence s.t. $y_n\to x$ and $y_n\neq x$ for every $n$ . By properties of $f$ , for every $n$ there exists a point $a_n\in A$ s.t. $|y_n-a_n|<f(y_n)+\left|\frac{y_n-x}{n}\right|$ , and so: $$\left|\frac{y_n-a_n}{y_n-x}\right|\leq\frac{f(y_n)+\left|\frac{y_n-x}{n}\right|}{|y_n-x|}=\left|\frac{f(y_n)-f(x)}{y_n-x}\right|+\frac{1}{n}\to f'(x)=0$$ $\Leftarrow$ - Let ${\{y_n\}}_{n=1}^\infty\subset\mathbb{R}$ be a sequence s.t. $y_n\to x$ and $y_n\neq x$ for every $n$ . We assume there exists a sequence ${\{a_n\}}_{n=1}^\infty\subset A$ s.t. $\left|\frac{y_n-a_n}{y_n-x}\right|\to0$ . Then we conclude: $$\left|\frac{f(y_n)-f(x)}{y_n-x}\right|=\frac{f(y_n)}{|y_n-x|}\leq\left|\frac{y_n-a_n}{y_n-x}\right|\to0$$ Thus $f$ is differentiable at $x$ (and $f'(x)=0$ ). In my opinion, this is quite a nice property, because in some sense it means that not only that $x$ is a limit point of $A$ , but also that we can ""approximate"" every sequence that converges to $x$ with a sequence of elements of $A$ . On the other hand, It doesn't give us a clear idea of how $A$ really behaves around $x$ . Is there any characterization of the points $x\in\overline{A}$ at which $f$ is differentiable which is more straightforward?","Let be a nonempty set, and define the function: My question is -  Is there an explicit characterization of all points at which is differentiable? My attempt Claim: Let . Then is differentiable at iff for every sequence of points s.t. and for every , there exists a sequence s.t. . : is differentiable at , thus there is a sequence of points s.t. , and so: Let be a sequence s.t. and for every . By properties of , for every there exists a point s.t. , and so: - Let be a sequence s.t. and for every . We assume there exists a sequence s.t. . Then we conclude: Thus is differentiable at (and ). In my opinion, this is quite a nice property, because in some sense it means that not only that is a limit point of , but also that we can ""approximate"" every sequence that converges to with a sequence of elements of . On the other hand, It doesn't give us a clear idea of how really behaves around . Is there any characterization of the points at which is differentiable which is more straightforward?","A\subset\mathbb{R} f:\mathbb{R}\to\mathbb{R}\quad,\quad f(x):=\inf_{a\in A}|x-a| x\in\overline{A} f x\in\overline{A} f x {\{y_n\}}_{n=1}^\infty\subset\mathbb{R} y_n\to x y_n\neq x n {\{a_n\}}_{n=1}^\infty\subset A \left|\frac{y_n-a_n}{y_n-x}\right|\to0 \Rightarrow f x\in\overline{A} {\{c_n\}}_{n=1}^\infty\subset A c_n\to x f'(x)=\lim_{n\to\infty}\frac{f(c_n)-f(x)}{c_n-x}=\lim_{n\to\infty}\frac{0-0}{c_n-x}=0 {\{y_n\}}_{n=1}^\infty\subset\mathbb{R} y_n\to x y_n\neq x n f n a_n\in A |y_n-a_n|<f(y_n)+\left|\frac{y_n-x}{n}\right| \left|\frac{y_n-a_n}{y_n-x}\right|\leq\frac{f(y_n)+\left|\frac{y_n-x}{n}\right|}{|y_n-x|}=\left|\frac{f(y_n)-f(x)}{y_n-x}\right|+\frac{1}{n}\to f'(x)=0 \Leftarrow {\{y_n\}}_{n=1}^\infty\subset\mathbb{R} y_n\to x y_n\neq x n {\{a_n\}}_{n=1}^\infty\subset A \left|\frac{y_n-a_n}{y_n-x}\right|\to0 \left|\frac{f(y_n)-f(x)}{y_n-x}\right|=\frac{f(y_n)}{|y_n-x|}\leq\left|\frac{y_n-a_n}{y_n-x}\right|\to0 f x f'(x)=0 x A x A A x x\in\overline{A} f","['limits', 'analysis', 'derivatives']"
53,Prove that $P_j(x)$ and $P_k(x)$ are relatively prime for all positive integers $j\neq k$,Prove that  and  are relatively prime for all positive integers,P_j(x) P_k(x) j\neq k,"Let for $n\ge 1, P_n(x)= 1+2x+3x^2+\cdots + nx^{n-1}$ . Prove that for any distinct positive integers j and k, $P_j(x)$ and $P_k(x)$ are relatively prime. The above problem is 2014 Putnam A5. Solutions can be found here . I have the following questions about the solutions: In the first solution, how did they compute that $w^n = nw - n+1$ ? I tried using the fact that $z$ is not a nonnegative real number and $P_i(z)=P_j(z)=0,$ but I wasn't able to deduce this result. In the second solution, I can't understand the proof of Corollary 2. In particular, how can one apply lemma 1 to the polynomial $f(x/R)$ if $x/R$ isn't necessarily a root of $f$ ? Also, even if $x/R$ were a root of $f,$ it doesn't seem like the resulting coefficients of the polynomial would be increasing, which is a requirement of lemma 1. I'm not sure how they get the bound $|z|\ge r$ , for similar reasons. I tried proving a variant of Corollary 2 where $a_i/a_{i+1}$ is replaced by $a_{i+1}/a_i$ in the definitions of $r$ and $R$ , but even if this corollary holds, it doesn't seem like it's useful for the given problem.","Let for . Prove that for any distinct positive integers j and k, and are relatively prime. The above problem is 2014 Putnam A5. Solutions can be found here . I have the following questions about the solutions: In the first solution, how did they compute that ? I tried using the fact that is not a nonnegative real number and but I wasn't able to deduce this result. In the second solution, I can't understand the proof of Corollary 2. In particular, how can one apply lemma 1 to the polynomial if isn't necessarily a root of ? Also, even if were a root of it doesn't seem like the resulting coefficients of the polynomial would be increasing, which is a requirement of lemma 1. I'm not sure how they get the bound , for similar reasons. I tried proving a variant of Corollary 2 where is replaced by in the definitions of and , but even if this corollary holds, it doesn't seem like it's useful for the given problem.","n\ge 1, P_n(x)= 1+2x+3x^2+\cdots + nx^{n-1} P_j(x) P_k(x) w^n = nw - n+1 z P_i(z)=P_j(z)=0, f(x/R) x/R f x/R f, |z|\ge r a_i/a_{i+1} a_{i+1}/a_i r R","['calculus', 'derivatives', 'polynomials', 'contest-math', 'gcd-and-lcm']"
54,Form the differential equations of the family of circles touching two given parallel lines $y=\pm a$,Form the differential equations of the family of circles touching two given parallel lines,y=\pm a,"Form the differential equations of the family of circles touching two given parallel lines $y=\pm a$ My Attempt: Let the family of circles be $(x-h)^2+y^2=a^2$ , with $(h,0)$ being the centre and $a$ being the radius. Differentiating w.r.t. x, we get, $$2(x-h)+2yy'=0\\ \implies yy'+x=h$$ Differentiating again, I get, $$yy''+(y')^2+1=0$$ But the answer given is $$y^2(1+(y')^2)=a^2$$","Form the differential equations of the family of circles touching two given parallel lines My Attempt: Let the family of circles be , with being the centre and being the radius. Differentiating w.r.t. x, we get, Differentiating again, I get, But the answer given is","y=\pm a (x-h)^2+y^2=a^2 (h,0) a 2(x-h)+2yy'=0\\ \implies yy'+x=h yy''+(y')^2+1=0 y^2(1+(y')^2)=a^2","['calculus', 'ordinary-differential-equations', 'derivatives']"
55,Multiple Solutions? Suppose ${x^2}+{y^2} = 45$ and $x=2y$ ... find $\frac{dy}{dt}$,Multiple Solutions? Suppose  and  ... find,{x^2}+{y^2} = 45 x=2y \frac{dy}{dt},"Suppose ${x^2}+{y^2} = 45$ and $x=2y$ for positive values of $x$ and $y$ find $\frac{dy}{dt}$ when $\frac{dx}{dt}=2$ I am new to implicit differntiation and related rates. When I attempted to solve this problem, I was met with multiple solutions. It is my first time working with two seperate equations and was wondering why two solutions is possible and what they would mean in context. Thank you. Solution 1: $$\frac{d}{dt}[x=2y]$$ $$\implies\frac{dx}{dt}=2\frac{dy}{dt}$$ $$\implies2=2\frac{dy}{dt}$$ $$\implies\frac{dy}{dt}=1$$ Solution 2: $$\frac{d}{dt}[{x^2}+{y^2} = 45]$$ $$\implies2x\frac{dx}{dt}+2y\frac{dy}{dt}=0$$ $$\implies2(2y)(2)+2y\frac{dy}{dt}=0$$ $$\implies4+\frac{dy}{dt}=0$$ $$\implies\frac{dy}{dt}=-4$$ Solution 3: (admittedly the same as sol. 2) $${x^2}+{y^2} = 45 \quad \Bbb{and} \quad x=2y$$ $$\implies{(2y)^2}+{y^2} = 45$$ $$\implies {5y^2} = 45$$ $$\implies {y} = 3 \Bbb {\quad [ignore\, \pm\, because\, of\, original\, question]}$$ $$\Bbb{and}\quad{x=6}$$ $$\Bbb{we\,recall} \quad 2x\frac{dx}{dt}+2y\frac{dy}{dt}=0$$ $$\implies 2(6)(2)+2(3)\frac{dy}{dt}=0$$ $$\implies \frac{dy}{dt}=-\frac{24}{6}=-4$$","Suppose and for positive values of and find when I am new to implicit differntiation and related rates. When I attempted to solve this problem, I was met with multiple solutions. It is my first time working with two seperate equations and was wondering why two solutions is possible and what they would mean in context. Thank you. Solution 1: Solution 2: Solution 3: (admittedly the same as sol. 2)","{x^2}+{y^2} = 45 x=2y x y \frac{dy}{dt} \frac{dx}{dt}=2 \frac{d}{dt}[x=2y] \implies\frac{dx}{dt}=2\frac{dy}{dt} \implies2=2\frac{dy}{dt} \implies\frac{dy}{dt}=1 \frac{d}{dt}[{x^2}+{y^2} = 45] \implies2x\frac{dx}{dt}+2y\frac{dy}{dt}=0 \implies2(2y)(2)+2y\frac{dy}{dt}=0 \implies4+\frac{dy}{dt}=0 \implies\frac{dy}{dt}=-4 {x^2}+{y^2} = 45 \quad \Bbb{and} \quad x=2y \implies{(2y)^2}+{y^2} = 45 \implies {5y^2} = 45 \implies {y} = 3 \Bbb {\quad [ignore\, \pm\, because\, of\, original\, question]} \Bbb{and}\quad{x=6} \Bbb{we\,recall} \quad 2x\frac{dx}{dt}+2y\frac{dy}{dt}=0 \implies 2(6)(2)+2(3)\frac{dy}{dt}=0 \implies \frac{dy}{dt}=-\frac{24}{6}=-4","['calculus', 'derivatives', 'implicit-differentiation', 'related-rates']"
56,"Differentiation under the ingtegral sign for a $W^{1, \infty}$ function",Differentiation under the ingtegral sign for a  function,"W^{1, \infty}","I have the following question. If $u\in L^1([0, T]; W^{1, \infty}(\Omega))$ , then for every $t\in[0, T]$ can we conclude that the function $$ w^t:\Omega\longrightarrow\mathbb R,\qquad x\longmapsto\int_0^t u(s, x)ds $$ belongs to $W^{1, \infty}(\Omega)$ and $\frac{\partial w^t}{\partial x}=\int_0^t \frac{\partial}{\partial x}u(s, x)ds$ ? I'm pretty sure that the answer is positive but I cannot find a book where to find it or some similar result which can confirm it. Could you help me? Thank you","I have the following question. If , then for every can we conclude that the function belongs to and ? I'm pretty sure that the answer is positive but I cannot find a book where to find it or some similar result which can confirm it. Could you help me? Thank you","u\in L^1([0, T]; W^{1, \infty}(\Omega)) t\in[0, T] 
w^t:\Omega\longrightarrow\mathbb R,\qquad x\longmapsto\int_0^t u(s, x)ds
 W^{1, \infty}(\Omega) \frac{\partial w^t}{\partial x}=\int_0^t \frac{\partial}{\partial x}u(s, x)ds","['functional-analysis', 'derivatives', 'sobolev-spaces']"
57,How can I find functions satisfying the Bessel equation $\frac{d^2}{dt^2}y(t)+p(t)\frac d{dt}y(t)+q(t)y(t)=0$?,How can I find functions satisfying the Bessel equation ?,\frac{d^2}{dt^2}y(t)+p(t)\frac d{dt}y(t)+q(t)y(t)=0,"We are tasked with finding the functions $u(t), Q(t)$ such that the conversion $y(t) = u(t)v(t)$ takes us from \begin{align} \dfrac{d^2}{dt^2}y(t) + p(t)\dfrac{d}{dt}y(t) + q(t)y(t) = 0 \end{align} to the form \begin{align} \dfrac{d^2}{dt^2}u(t) + Q(t)u(t)=0. \end{align} From then on we must use this to solve $t^2\dfrac{d^2}{dt^2}y(t) + t\dfrac{d}{dt}y(t) + \left(t^2-\dfrac{1}{4}\right)y(t) = 0$ . My take was this: I started differentiating (misusing the notation to make it easier to read) \begin{align} &y(t)=u(t)v(t)\\ &y'(t)=u'(t)v(t)+u(t)v'(t)\\ &y''(t)=u''(t)v(t) + 2u'(t)v'(t) + u(t)v''(t) \end{align} thus the original equation gives \begin{align} u''(t) v(t) + 2u'(t)v'(t) + u(t)v''(t)+ p(t) \left[ u'(t)v(t)+u(t)v'(t) \right] + q(t)u(t)v(t) = 0 \end{align} dividing by $v(t)$ (we can assume it's nonzero) we have \begin{align} \boxed{u''(t)} + \dfrac{2u'(t)v'(t)}{v(t)} + \boxed{u(t)} \dfrac{v''(t)}{v(t)} + p(t)u'(t) + \boxed{u(t)}\dfrac{p(t)v'(t)}{v(t)} + \boxed{u(t)}q(t)=0. \end{align} All the boxed terms give us (by common factor) the $Q(t)$ we ask for. However the remaining non-boxed terms must be zero right? So let's ask the equation to zero this for us: \begin{align} \dfrac{2u'(t)v'(t)}{v(t)} + p(t)u'(t) = 0\\ \implies u'(t) \left[ 2\dfrac{v'(t)}{v(t)} + p(t) \right] = 0\\ \implies u(t) = c \ \lor \ 2\left( \ln v(t) \right)' + p(t) = 0\\ \implies u(t) = c \ \lor \ v(t) = e^{-\int \left(\dfrac{p(t)}{2}dt\right)}. \end{align} This is where I have arrived, and I am not sure how to proceed. Can we assume that $u(t)$ cannot be constant? What about $v(t)$ ? Nowhere is this asked to find right? Any help would be appreciated.","We are tasked with finding the functions such that the conversion takes us from to the form From then on we must use this to solve . My take was this: I started differentiating (misusing the notation to make it easier to read) thus the original equation gives dividing by (we can assume it's nonzero) we have All the boxed terms give us (by common factor) the we ask for. However the remaining non-boxed terms must be zero right? So let's ask the equation to zero this for us: This is where I have arrived, and I am not sure how to proceed. Can we assume that cannot be constant? What about ? Nowhere is this asked to find right? Any help would be appreciated.","u(t), Q(t) y(t) = u(t)v(t) \begin{align}
\dfrac{d^2}{dt^2}y(t) + p(t)\dfrac{d}{dt}y(t) + q(t)y(t) = 0
\end{align} \begin{align}
\dfrac{d^2}{dt^2}u(t) + Q(t)u(t)=0.
\end{align} t^2\dfrac{d^2}{dt^2}y(t) + t\dfrac{d}{dt}y(t) + \left(t^2-\dfrac{1}{4}\right)y(t) = 0 \begin{align}
&y(t)=u(t)v(t)\\
&y'(t)=u'(t)v(t)+u(t)v'(t)\\
&y''(t)=u''(t)v(t) + 2u'(t)v'(t) + u(t)v''(t)
\end{align} \begin{align}
u''(t) v(t) + 2u'(t)v'(t) + u(t)v''(t)+ p(t) \left[ u'(t)v(t)+u(t)v'(t) \right] + q(t)u(t)v(t) = 0
\end{align} v(t) \begin{align}
\boxed{u''(t)} + \dfrac{2u'(t)v'(t)}{v(t)} + \boxed{u(t)} \dfrac{v''(t)}{v(t)} + p(t)u'(t) + \boxed{u(t)}\dfrac{p(t)v'(t)}{v(t)} + \boxed{u(t)}q(t)=0.
\end{align} Q(t) \begin{align}
\dfrac{2u'(t)v'(t)}{v(t)} + p(t)u'(t) = 0\\
\implies u'(t) \left[ 2\dfrac{v'(t)}{v(t)} + p(t) \right] = 0\\
\implies u(t) = c \ \lor \ 2\left( \ln v(t) \right)' + p(t) = 0\\
\implies u(t) = c \ \lor \ v(t) = e^{-\int \left(\dfrac{p(t)}{2}dt\right)}.
\end{align} u(t) v(t)","['ordinary-differential-equations', 'derivatives']"
58,"Spivak, Ch. 20, Problem 18: Use Taylor's Theorem to prove that $\lim\limits_{x\to a} \frac{f(x)-P_{n,a}(x)}{(x-a)^n}=0$","Spivak, Ch. 20, Problem 18: Use Taylor's Theorem to prove that","\lim\limits_{x\to a} \frac{f(x)-P_{n,a}(x)}{(x-a)^n}=0","The following is a problem from Spivak's Calculus Chapter 20, Problem 18 : Deduce Theorem $1$ as a corollary of Taylor's Theorem, with any form of the remainder. (The catch is that it will be necessary to assume one more derivative than in the hypotheses for Theorem 1). Here is Theorem $1$ Theorem 1 Suppose that $f$ is a function for which $$f'(a),...,f^{(n)}(a)$$ all exist. Let $$a_k=\frac{f^{(k)}(a)}{k!}, 0\leq k\leq n$$ and define $$P_{n,a}(x)=a_0+a_1(x-a)+...+a_n(x-a)^n$$ Then $$\lim\limits_{x\to a} \frac{f(x)-P_{n,a}(x)}{(x-a)^n}=0$$ and here is Taylor's Theorem Theorem 4 (Taylor's Theorem) Suppose that $f',...,f^{(n+1)}$ are defined on $[a,x]$ , and that $R_{n,a}(x)$ is defined by $$f(x)=f(a)+f'(a)(x-a)+...+\frac{f^{(n)}(a)}{n!}(x-a)^n+R_{n,a}(x)$$ Then $$R_{n,a}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1},\text{ for some }  t \text{ in } (a,x)$$ (Lagrange form of the remainder). My primary question is: why can or can't we use the following proof Assume we know Taylor's theorem is true and assume the first $n+1$ derivatives of $f$ exist on $[a,x]$ . Taylor's theorem tells us that $$f(x)-P_{n,a}(x)=R_{n,a}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1},  \text{ for } t\in (a,x)$$ Then, $$\lim\limits_{x\to a}  \frac{f(x)-P_{n,a}(x)}{(x-a)^n}=\lim\limits_{x\to a}  \frac{f^{(n+1)(t)}}{(n+1)!}(x-a)=0$$ In addition, let me just show the solution manual solution and my own understanding of it in more steps Solution Manual Suppose $|f^{(n+1)}|$ is bounded, by some $M$ , on some interval around $a$ . Then for $x$ in this interval we have $$|R_{n,a}(x)|=\frac{|f^{(n+1)}(t)|}{n!}|x-a|^{n+1}\tag{1}$$ so $$\frac{|R_{n,a}(x)|}{|x-a|^n}\leq M|x-a|\tag{2}$$ so $$\lim\limits_{x\to a} \frac{R_{n,a}(x)}{(x-a)^n}=0\tag{3}$$ A similar proof works for the integral form of the remainder and for the Cauchy form. Here is my understanding of this proof in more steps Suppose we know Taylor's theorem is true, and assume the assumptions of that theorem are true on some interval $[a,x]$ . One of those assumptions is that $f',...,f^{(n+1)}$ are defined on $[a,x]$ . Thus, $f^{(n+1)}$ is continuous on $[a,x]$ , and hence bounded on this interval. Thus, there exists some $M>0$ , such that for any $x_1\in[a,x]$ we have $|f^{(n+1)}(x_1)|\leq M$ . In addition, Taylor's theorem tells us that there is some $t\in  (a,x)$ such that $$R_{n,a}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1}\tag{4}$$ Second question: why does $(1)$ have the denominator as $n!$ and not $(n+1)!$ as in $(4)$ ? Now we take the absolute value of both sides of $(4)$ $$|R_{n,a}(x)|=\frac{|f^{(n+1)}(t)|}{(n+1)!}|(x-a)|^{n+1}$$ $$0\leq \frac{|R_{n,a}(x)|}{|x-a|^{n}}=\frac{|f^{(n+1)}(t)|}{(n+1)!}|x-a|$$ $$\leq \frac{M}{(n+1)!}|x-a|\leq M|x-a|$$ Hence $$\lim\limits_{x\to a}  \frac{|R_{n,a}(x)|}{|x-a|^{n}}=\lim\limits_{x\to a}  \frac{|f(x)-P_{n,a}(x)|}{|x-a|^{n}} =0$$ $$\implies \lim\limits_{x\to a} \frac{f(x)-P_{n,a}(x)}{(x-a)^{n}} =0$$ which is the result of Theorem 1.","The following is a problem from Spivak's Calculus Chapter 20, Problem 18 : Deduce Theorem as a corollary of Taylor's Theorem, with any form of the remainder. (The catch is that it will be necessary to assume one more derivative than in the hypotheses for Theorem 1). Here is Theorem Theorem 1 Suppose that is a function for which all exist. Let and define Then and here is Taylor's Theorem Theorem 4 (Taylor's Theorem) Suppose that are defined on , and that is defined by Then (Lagrange form of the remainder). My primary question is: why can or can't we use the following proof Assume we know Taylor's theorem is true and assume the first derivatives of exist on . Taylor's theorem tells us that Then, In addition, let me just show the solution manual solution and my own understanding of it in more steps Solution Manual Suppose is bounded, by some , on some interval around . Then for in this interval we have so so A similar proof works for the integral form of the remainder and for the Cauchy form. Here is my understanding of this proof in more steps Suppose we know Taylor's theorem is true, and assume the assumptions of that theorem are true on some interval . One of those assumptions is that are defined on . Thus, is continuous on , and hence bounded on this interval. Thus, there exists some , such that for any we have . In addition, Taylor's theorem tells us that there is some such that Second question: why does have the denominator as and not as in ? Now we take the absolute value of both sides of Hence which is the result of Theorem 1.","1 1 f f'(a),...,f^{(n)}(a) a_k=\frac{f^{(k)}(a)}{k!}, 0\leq k\leq n P_{n,a}(x)=a_0+a_1(x-a)+...+a_n(x-a)^n \lim\limits_{x\to a} \frac{f(x)-P_{n,a}(x)}{(x-a)^n}=0 f',...,f^{(n+1)} [a,x] R_{n,a}(x) f(x)=f(a)+f'(a)(x-a)+...+\frac{f^{(n)}(a)}{n!}(x-a)^n+R_{n,a}(x) R_{n,a}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1},\text{ for some }
 t \text{ in } (a,x) n+1 f [a,x] f(x)-P_{n,a}(x)=R_{n,a}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1},
 \text{ for } t\in (a,x) \lim\limits_{x\to a}
 \frac{f(x)-P_{n,a}(x)}{(x-a)^n}=\lim\limits_{x\to a}
 \frac{f^{(n+1)(t)}}{(n+1)!}(x-a)=0 |f^{(n+1)}| M a x |R_{n,a}(x)|=\frac{|f^{(n+1)}(t)|}{n!}|x-a|^{n+1}\tag{1} \frac{|R_{n,a}(x)|}{|x-a|^n}\leq M|x-a|\tag{2} \lim\limits_{x\to a} \frac{R_{n,a}(x)}{(x-a)^n}=0\tag{3} [a,x] f',...,f^{(n+1)} [a,x] f^{(n+1)} [a,x] M>0 x_1\in[a,x] |f^{(n+1)}(x_1)|\leq M t\in
 (a,x) R_{n,a}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1}\tag{4} (1) n! (n+1)! (4) (4) |R_{n,a}(x)|=\frac{|f^{(n+1)}(t)|}{(n+1)!}|(x-a)|^{n+1} 0\leq \frac{|R_{n,a}(x)|}{|x-a|^{n}}=\frac{|f^{(n+1)}(t)|}{(n+1)!}|x-a| \leq \frac{M}{(n+1)!}|x-a|\leq M|x-a| \lim\limits_{x\to a}
 \frac{|R_{n,a}(x)|}{|x-a|^{n}}=\lim\limits_{x\to a}
 \frac{|f(x)-P_{n,a}(x)|}{|x-a|^{n}} =0 \implies \lim\limits_{x\to a} \frac{f(x)-P_{n,a}(x)}{(x-a)^{n}} =0","['calculus', 'limits', 'derivatives', 'solution-verification', 'taylor-expansion']"
59,determining the start velocity and angle based on the goals(End point) angle and coordinates.,determining the start velocity and angle based on the goals(End point) angle and coordinates.,,"======== I'm trying to figure out the start angle and velocity of an parabola, When i know only the start coordinates and the end point coordinates and the angle which it reaches the endpoint. I'm tasked with figuring this out by using derivative method. So the known parameters are: Start point (0,0) end point (6,4) end point angle (-18) unknown are: velocity = unknown. start angle =unkown. This is a homework assiment thats been bothering me for months now. Here is a picture of what i've been working with:","======== I'm trying to figure out the start angle and velocity of an parabola, When i know only the start coordinates and the end point coordinates and the angle which it reaches the endpoint. I'm tasked with figuring this out by using derivative method. So the known parameters are: Start point (0,0) end point (6,4) end point angle (-18) unknown are: velocity = unknown. start angle =unkown. This is a homework assiment thats been bothering me for months now. Here is a picture of what i've been working with:",,['derivatives']
60,"Spivak, Chapter 20, Problem 11: Using Taylor's Theorem Instead of L'Hôpital´s Rule to compute limit.","Spivak, Chapter 20, Problem 11: Using Taylor's Theorem Instead of L'Hôpital´s Rule to compute limit.",,"The following is a problem from Chapter 20 of Spivak's Calculus Calculations of this sort may be used to evaluate limits that we might otherwise try to find through laborious use of l'Hopital's Rule. Find the following (a) $\lim\limits_{x\to 0}  \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0}  \frac{N(x)}{D(x)}$ Hint: First find $P_{3,0,N}(x)$ and $P_{3,0,D}(x)$ for the numerator and denominator $N(x)$ and $D(x)$ . ""Calculations of this sort"" refers to the previous problem $10$ in which we compute Taylor polynomials of functions $f+g$ , $f\cdot g$ , and $f\circ g$ , using formulas derived in problem $9$ . Here is the solution manual solution $$\lim\limits_{x\to 0} \frac{N(x)}{D(x)}=\lim\limits_{x\to  0}\frac{\frac{1}{6}x^3+R_{3,0,N}(x)}{\frac{1}{6}x^3+R_{3,0,D}(x)}=\lim\limits_{x\to  0}\frac{\frac{1}{6}+\frac{R_{3,0,N}(x)}{x^3}}{\frac{1}{6}+\frac{R_{3,0,D}(x)}{x^3}}=1$$ since the limits involving $R$ terms are $0$ My question is about the intermediate steps in this proof. Here is my attempt at writing them out. Using Taylor's Theorem, we can easily see that $$N(x)=P_{3,0,N}(x)+R_{3,0,N}(x)$$ $$=\frac{x^3}{3!}+\frac{e^t}{4!}x^4, t\in (0,x)$$ $$D(x)=P_{3,0,D}(x)+R_{3,0,N}(x)$$ $$=\frac{x^3}{3!}+\frac{(-\sin{u})}{4!}x^4,u\in (0,x)$$ Therefore, we have $$\lim\limits_{x\to 0}  \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0}  \frac{N(x)}{D(x)}$$ $$=\lim\limits_{x\to 0} \frac{\frac{x^3}{3!}+\frac{e^t}{4!}x^4}{\frac{x^3}{3!}+\frac{(-\sin{u})}{4!}x^4}$$ $$=\lim\limits_{x\to 0} \frac{\frac{1}{6}+\frac{e^t x}{4!}}{\frac{1}{6}-\frac{x\sin{(u)}}{4!}}$$ $$=1$$ Are these intermediate steps correct? Seems like this was all a lot more work than applying L'Hopital directly to the original limit. $$\lim\limits_{x\to 0} \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0} \frac{e^x-1}{\sin{x}}=\lim\limits_{x\to 0} \frac{e^x}{\cos{x}}=1$$","The following is a problem from Chapter 20 of Spivak's Calculus Calculations of this sort may be used to evaluate limits that we might otherwise try to find through laborious use of l'Hopital's Rule. Find the following (a) Hint: First find and for the numerator and denominator and . ""Calculations of this sort"" refers to the previous problem in which we compute Taylor polynomials of functions , , and , using formulas derived in problem . Here is the solution manual solution since the limits involving terms are My question is about the intermediate steps in this proof. Here is my attempt at writing them out. Using Taylor's Theorem, we can easily see that Therefore, we have Are these intermediate steps correct? Seems like this was all a lot more work than applying L'Hopital directly to the original limit.","\lim\limits_{x\to 0}
 \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0}
 \frac{N(x)}{D(x)} P_{3,0,N}(x) P_{3,0,D}(x) N(x) D(x) 10 f+g f\cdot g f\circ g 9 \lim\limits_{x\to 0} \frac{N(x)}{D(x)}=\lim\limits_{x\to
 0}\frac{\frac{1}{6}x^3+R_{3,0,N}(x)}{\frac{1}{6}x^3+R_{3,0,D}(x)}=\lim\limits_{x\to
 0}\frac{\frac{1}{6}+\frac{R_{3,0,N}(x)}{x^3}}{\frac{1}{6}+\frac{R_{3,0,D}(x)}{x^3}}=1 R 0 N(x)=P_{3,0,N}(x)+R_{3,0,N}(x) =\frac{x^3}{3!}+\frac{e^t}{4!}x^4, t\in (0,x) D(x)=P_{3,0,D}(x)+R_{3,0,N}(x) =\frac{x^3}{3!}+\frac{(-\sin{u})}{4!}x^4,u\in (0,x) \lim\limits_{x\to 0}
 \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0}
 \frac{N(x)}{D(x)} =\lim\limits_{x\to 0} \frac{\frac{x^3}{3!}+\frac{e^t}{4!}x^4}{\frac{x^3}{3!}+\frac{(-\sin{u})}{4!}x^4} =\lim\limits_{x\to 0} \frac{\frac{1}{6}+\frac{e^t x}{4!}}{\frac{1}{6}-\frac{x\sin{(u)}}{4!}} =1 \lim\limits_{x\to 0} \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0} \frac{e^x-1}{\sin{x}}=\lim\limits_{x\to 0} \frac{e^x}{\cos{x}}=1","['calculus', 'integration', 'derivatives', 'solution-verification', 'taylor-expansion']"
61,Obtaining $\zeta(4)$ from specific derived expression,Obtaining  from specific derived expression,\zeta(4),"I started from the expansion of cot(x) by the Mittag-Leffler theorem, where: $$\cot(x) = \frac{1}{x} + \sum_{k = 1}^{\infty} \frac{2x}{x^{2} - k^2 \pi^2}.$$ After  splitting up $\cot(x)$ , taking the derivative of both sides, and some algebra, I eventually was left with the following expression: $$-1 = 6\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} + 4x^2    \left [ \sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 - 4x^2\sum_{k = 1}^{\infty} \frac{1}{({x^{2} - k^2 \pi^2})^2} $$ where $x = 0$ gives $\zeta(2)$ . I am now trying to obtain the exact value for $\zeta(4)$ through this expression. To do this, I am attempting to once again take the derivative and then set $x = 0$ . My work is shown below. $$-1 = 6\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} + 4x^2    \left [ \sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 - 4x^2\sum_{k = 1}^{\infty} \frac{1}{({x^{2} - k^2 \pi^2})^2} \Longrightarrow $$ $\Longrightarrow 0 = -12x\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} + 8x\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 + 8x^2\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] \times \frac{d}{dx}\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] - 8x \left [\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} \right ] \left [ \displaystyle\sum_{k = 1}^{\infty} \frac{-4x}{(x^{2} - k^2 \pi^2)^3} \right ] \Longrightarrow $ Dividing by $x$ : $\Longrightarrow0 =-12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 + \\8x\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] \times \frac{d}{dx}\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] - 8x \left [\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} \right ] \left [ \displaystyle\sum_{k = 1}^{\infty} \frac{-4}{(x^{2} - k^2 \pi^2)^3} \right  ] \Longrightarrow $ Setting $x = 0$ : $\Longrightarrow 0 = -12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(- k^2 \pi^2)^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{- k^2 \pi^2} \right ] ^2 \Longrightarrow $ Multiplying everything by $\pi^4$ : $\Longrightarrow 0 = -12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(- k^2 )^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{- k^2 } \right ] ^2 \Longrightarrow $ $\Longrightarrow 0 = 3\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(k^2 )^2} - 2\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{ k^2 } \right ] ^2 \Longrightarrow $ Solving for ζ(4): $\Longrightarrow 0 = 3\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(k^2 )^2} - 2\left [ \frac{\pi^2}{6} \right ] ^2 \Longrightarrow $ $\Longrightarrow \displaystyle\sum_{k = 1}^{\infty} \frac{1}{k^4 } = \frac{\pi^4}{54}$ However, my answer is wrong. The correct result would be $\frac{\pi^4}{90}$ . Any help on this would be greatly appreciated.","I started from the expansion of cot(x) by the Mittag-Leffler theorem, where: After  splitting up , taking the derivative of both sides, and some algebra, I eventually was left with the following expression: where gives . I am now trying to obtain the exact value for through this expression. To do this, I am attempting to once again take the derivative and then set . My work is shown below. Dividing by : Setting : Multiplying everything by : Solving for ζ(4): However, my answer is wrong. The correct result would be . Any help on this would be greatly appreciated.","\cot(x) = \frac{1}{x} + \sum_{k = 1}^{\infty} \frac{2x}{x^{2} - k^2 \pi^2}. \cot(x) -1 = 6\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} + 4x^2 
  \left [ \sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 - 4x^2\sum_{k = 1}^{\infty} \frac{1}{({x^{2} - k^2 \pi^2})^2}  x = 0 \zeta(2) \zeta(4) x = 0 -1 = 6\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} + 4x^2 
  \left [ \sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 - 4x^2\sum_{k = 1}^{\infty} \frac{1}{({x^{2} - k^2 \pi^2})^2} \Longrightarrow  \Longrightarrow 0 = -12x\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} + 8x\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 + 8x^2\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] \times \frac{d}{dx}\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] - 8x \left [\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} \right ] \left [ \displaystyle\sum_{k = 1}^{\infty} \frac{-4x}{(x^{2} - k^2 \pi^2)^3} \right ] \Longrightarrow
 x \Longrightarrow0 =-12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 + \\8x\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] \times \frac{d}{dx}\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] - 8x \left [\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} \right ] \left [ \displaystyle\sum_{k = 1}^{\infty} \frac{-4}{(x^{2} - k^2 \pi^2)^3} \right  ] \Longrightarrow
 x = 0 \Longrightarrow 0 = -12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(- k^2 \pi^2)^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{- k^2 \pi^2} \right ] ^2 \Longrightarrow
 \pi^4 \Longrightarrow 0 = -12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(- k^2 )^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{- k^2 } \right ] ^2 \Longrightarrow
 \Longrightarrow 0 = 3\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(k^2 )^2} - 2\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{ k^2 } \right ] ^2 \Longrightarrow
 \Longrightarrow 0 = 3\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(k^2 )^2} - 2\left [ \frac{\pi^2}{6} \right ] ^2 \Longrightarrow
 \Longrightarrow \displaystyle\sum_{k = 1}^{\infty} \frac{1}{k^4 } = \frac{\pi^4}{54} \frac{\pi^4}{90}","['derivatives', 'riemann-zeta', 'mittag-leffler-function']"
62,Who said differential dx is a very small quantity?,Who said differential dx is a very small quantity?,,"Given a differentiable function $f(x)$ in a point $x_0$ , by definition, the differential $df$ is the difference in ordinates of the tangent line to curve in $x_0$ , evaluated at the point $x_0 + dx$ . Now, $dx$ can be small or big, and if it is small enough, we know that this difference approximates: $f(x_0 + dx) - f(x_0)$ . Question is: it's $dx$ just a quantity which can be taken big or small? (often taken small enough in applications)?","Given a differentiable function in a point , by definition, the differential is the difference in ordinates of the tangent line to curve in , evaluated at the point . Now, can be small or big, and if it is small enough, we know that this difference approximates: . Question is: it's just a quantity which can be taken big or small? (often taken small enough in applications)?",f(x) x_0 df x_0 x_0 + dx dx f(x_0 + dx) - f(x_0) dx,"['integration', 'derivatives']"
63,Can we approximate a real-analytic function and its derivatives by non-analytic smooth functions in the following way?,Can we approximate a real-analytic function and its derivatives by non-analytic smooth functions in the following way?,,"Let $C^\omega(\Bbb R)$ denote the set of all real-analytic functions on $\Bbb R$ . I was trying the following question: $(\mathscr{Q1})$ Let $f \in C^\omega(\Bbb R) \cap L^2(\Bbb R)$ such that $f^{(2k)} \in L^2(\Bbb R) \:, \forall k \ge 1$ . Then given $\varepsilon >0$ does there exist $f_{\varepsilon} \in C^\infty(\Bbb R) \setminus C^\omega(\Bbb R)$ such that $\|f^{(2k)}- f^{(2k)}_{\varepsilon}\|_{L^2(\Bbb R)} < \varepsilon ,\:  \forall k = 0,1,2,\dots \:?$ My most naive approach was to obtain $f_{\varepsilon}$ by modifying $f$ on some set of measure $0$ , while keeping its smoothness intact. But of course, it does not work, because then $f-f_{\varepsilon}$ would be a $C^\infty$ function which is non-zero only on a set of zero measure, which is not possible! Then I thought about considering a convolution with non-analytic mollifier or approximate identity kind of argument, but I don't know how to preserve the norm closeness at the derivatives level. Is there some Sobolev density result which would be useful here? $(\mathscr{Q2})$ The higher dimensional analogue of $(\mathscr{Q1})$ with the even derivatives replaced by the iterates of the Laplacian? Thanks in advance for help!","Let denote the set of all real-analytic functions on . I was trying the following question: Let such that . Then given does there exist such that My most naive approach was to obtain by modifying on some set of measure , while keeping its smoothness intact. But of course, it does not work, because then would be a function which is non-zero only on a set of zero measure, which is not possible! Then I thought about considering a convolution with non-analytic mollifier or approximate identity kind of argument, but I don't know how to preserve the norm closeness at the derivatives level. Is there some Sobolev density result which would be useful here? The higher dimensional analogue of with the even derivatives replaced by the iterates of the Laplacian? Thanks in advance for help!","C^\omega(\Bbb R) \Bbb R (\mathscr{Q1}) f \in C^\omega(\Bbb R) \cap L^2(\Bbb R) f^{(2k)} \in L^2(\Bbb R) \:, \forall k \ge 1 \varepsilon >0 f_{\varepsilon} \in C^\infty(\Bbb R) \setminus C^\omega(\Bbb R) \|f^{(2k)}- f^{(2k)}_{\varepsilon}\|_{L^2(\Bbb R)} < \varepsilon ,\:  \forall k = 0,1,2,\dots \:? f_{\varepsilon} f 0 f-f_{\varepsilon} C^\infty (\mathscr{Q2}) (\mathscr{Q1})","['real-analysis', 'derivatives', 'lp-spaces', 'approximation-theory']"
64,Does derivative of antiderivative always equal the integrand function?,Does derivative of antiderivative always equal the integrand function?,,"For some reals $a<b$ we consider $F:(a,b)\to\mathbb{R}$ where $F(x):=\int\limits_{a}^xf(t)dt$ and assume that $F$ is continously differentiable. (We don't know if $f$ is continuous) Can we conclude that $F'(x)=f(x)$ for all $x\in (a,b)$ ? For some $x_0\in(a,b)$ we choose $$f(t)=\begin{cases}0, & t\in(a,b)\setminus\{x_0\}\\2,& t=x_0.\end{cases}$$ and $g(t):=\hat{0}$ . Due to properties of the Riemann integral $$\int\limits_a^{x_0}f(t)dt=\int\limits_a^{x_0}g(t)dt=0.$$ Then \begin{align*} &\left|\frac{\int\limits_{a}^xf(t)~dt-\int\limits_{a}^xf(t)~dt}{x_0-x}\right|=\left|\frac{\int\limits_{x_0}^xf(t)~dt}{x_0-x}\right|=\left|\frac{\int\limits_{x_0}^xg(t)~dt}{x_0-x}\right|=0,\text{ where }x\neq x_0\\ &\implies F'(x_0)=0. \end{align*} As $x_0$ was chosen arbitrarily $F'(x)=0$ for all $x\in(a,b)$ . Hence $F'$ is continuous but $F'\neq f$ . Is this correct?",For some reals we consider where and assume that is continously differentiable. (We don't know if is continuous) Can we conclude that for all ? For some we choose and . Due to properties of the Riemann integral Then As was chosen arbitrarily for all . Hence is continuous but . Is this correct?,"a<b F:(a,b)\to\mathbb{R} F(x):=\int\limits_{a}^xf(t)dt F f F'(x)=f(x) x\in (a,b) x_0\in(a,b) f(t)=\begin{cases}0, & t\in(a,b)\setminus\{x_0\}\\2,& t=x_0.\end{cases} g(t):=\hat{0} \int\limits_a^{x_0}f(t)dt=\int\limits_a^{x_0}g(t)dt=0. \begin{align*}
&\left|\frac{\int\limits_{a}^xf(t)~dt-\int\limits_{a}^xf(t)~dt}{x_0-x}\right|=\left|\frac{\int\limits_{x_0}^xf(t)~dt}{x_0-x}\right|=\left|\frac{\int\limits_{x_0}^xg(t)~dt}{x_0-x}\right|=0,\text{ where }x\neq x_0\\
&\implies F'(x_0)=0.
\end{align*} x_0 F'(x)=0 x\in(a,b) F' F'\neq f","['real-analysis', 'integration', 'derivatives', 'riemann-integration']"
65,Deriving Schwarzian Action in SYK Theory,Deriving Schwarzian Action in SYK Theory,,"I am trying to derive the Schwarzian action for the $q=4$ SYK model following ""An introduction to the SYK model"" by V. Rosenhaus. I understand that we have the solution $$G(\tau_1, \tau_2) =  \frac{b {\rm sgn}(\tau_1-\tau_2)}{ J ^{2\Delta}} \frac{f^\prime (\tau_1) ^\Delta f^\prime(\tau_2)^\Delta}{\vert f(\tau_1)-f(\tau_2)\vert^{2\Delta}} .\tag{3.4}$$ My problem is deriving the expansion of $G$ . Supposedly, we change coordinates from $(\tau_1, \tau_2)$ to $(\tau_+, \tau_-)$ where $\tau_+= \frac{\tau_1+\tau_2}{2}$ and $\tau_-=\tau_1-\tau_2$ . Taylor expand around $\tau_+$ $$G(\tau_1, \tau_2) = \frac{b {\rm sgn}(\tau_1-\tau_2)}{ \vert J (\tau_1-\tau_2) \vert^{2\Delta}} \left(1+ \frac{\Delta}{6} (\tau_1-\tau_2)^2 {\rm Sch}(f(\tau_+), \tau_+)+ O(\tau_1-\tau_2)^3 \right) .\tag{3.5}$$ It would be appreciated if someone could help show how this expansion is done or point to a resource where this has been detailed.","I am trying to derive the Schwarzian action for the SYK model following ""An introduction to the SYK model"" by V. Rosenhaus. I understand that we have the solution My problem is deriving the expansion of . Supposedly, we change coordinates from to where and . Taylor expand around It would be appreciated if someone could help show how this expansion is done or point to a resource where this has been detailed.","q=4 G(\tau_1, \tau_2) =  \frac{b {\rm sgn}(\tau_1-\tau_2)}{ J ^{2\Delta}} \frac{f^\prime (\tau_1) ^\Delta f^\prime(\tau_2)^\Delta}{\vert f(\tau_1)-f(\tau_2)\vert^{2\Delta}} .\tag{3.4} G (\tau_1, \tau_2) (\tau_+, \tau_-) \tau_+= \frac{\tau_1+\tau_2}{2} \tau_-=\tau_1-\tau_2 \tau_+ G(\tau_1, \tau_2) = \frac{b {\rm sgn}(\tau_1-\tau_2)}{ \vert J (\tau_1-\tau_2) \vert^{2\Delta}} \left(1+ \frac{\Delta}{6} (\tau_1-\tau_2)^2 {\rm Sch}(f(\tau_+), \tau_+)+ O(\tau_1-\tau_2)^3 \right) .\tag{3.5}","['derivatives', 'taylor-expansion', 'physics', 'conformal-geometry', 'conformal-field-theory']"
66,Is a distance function differentiable on $\Bbb R^n$?,Is a distance function differentiable on ?,\Bbb R^n,"Let $C\subset \Bbb R^n$ be closed and convex. Define $d_X\colon \Bbb R^n\to \Bbb R$ by the formula $d_X(x):=\mathrm{dist}(x,C)$ . Is it differentiable outside $ C$ ? How to prove it? What's the derivative? Moreover, if it's true that it is not differentiable for any closed convex set $C$ and any $x\in\partial C$ ? (this isn't covered by my proof below and I'll be happy to see the answer to this question). Context: In the question the derivative of $d_X$ is considered. In the discussion there was a doubt if this function is always differentiable and the smooth boundary is assumed to force differentiability. While trying to answer the question I realized that the question of differentiability is interesting on its own and putting my proof as the answer to that question may obfuscate both the proof of differentiability of $d_X$ and the answer of that question. Short googling gave the following statement in an article from Transactions of the AMS : 'For convex $C$ , the differentiability of $d_C$ everywhere outside of $C$ is well known' (page 1 line -4). So it turns out that the condition on $\partial X$ is not needed! Of course if $x\in \mathrm{int}\,C$ then $\nabla d_X(x)=0$ .","Let be closed and convex. Define by the formula . Is it differentiable outside ? How to prove it? What's the derivative? Moreover, if it's true that it is not differentiable for any closed convex set and any ? (this isn't covered by my proof below and I'll be happy to see the answer to this question). Context: In the question the derivative of is considered. In the discussion there was a doubt if this function is always differentiable and the smooth boundary is assumed to force differentiability. While trying to answer the question I realized that the question of differentiability is interesting on its own and putting my proof as the answer to that question may obfuscate both the proof of differentiability of and the answer of that question. Short googling gave the following statement in an article from Transactions of the AMS : 'For convex , the differentiability of everywhere outside of is well known' (page 1 line -4). So it turns out that the condition on is not needed! Of course if then .","C\subset \Bbb R^n d_X\colon \Bbb R^n\to \Bbb R d_X(x):=\mathrm{dist}(x,C)  C C x\in\partial C d_X d_X C d_C C \partial X x\in \mathrm{int}\,C \nabla d_X(x)=0","['real-analysis', 'derivatives']"
67,"Spivak, Ch. 18, Problem 21: $f$ satisfies $f'=cf$ for some number $c$, show that $f(x)=ke^{cx}$ for some $k$.","Spivak, Ch. 18, Problem 21:  satisfies  for some number , show that  for some .",f f'=cf c f(x)=ke^{cx} k,"Suppose that on some interval the function $f$ satisfies $f'=cf$ for some number $c$ . (a) Assuming that $f$ is never $0$ , use Problem 20(b) to prove that $|f(x)|=le^{cx}$ for some number $l>0$ . It follows that $f(x)=ke^{cx}$ for some $k$ . (b) Show that this result holds without the added assumption that $f$ is never $0$ . Hint: show that $f$ can't be $0$ at the endpoint of an open interval on which it is nowhere $0$ . I am interested specifically in the solution to part $(b)$ . A question about this problem has been asked before . However, I am not fully satisfied with the accepted answer where it pertains to item $(b)$ . Though I understand the reasoning in that answer, I felt that the final part was left a bit vague we can continue the same argument by replacing $a$ with $(a+p)$ (or $(a-p)$ ) and thereby extend the region where $f$ does not vanish. Using the same argument repeatedly we can show that $f$ does not vanish at any point in $I$ . How exactly do we use the same argument repeatedly (ie, I would like to see what that looks like formally)? I think I reached a similar point in my own attempt at a proof, and that is why I am asking this question. Here is my attempt Our only assumption is that $$f'=cf\tag{1}$$ on some interval. Let $[a,b]$ be the interval. $(1)$ implies that $f$ is differentiable and hence continuous on $[a,b]$ . First, there is a trivial case: $f(x)=0=0\cdot e^{cx}$ . Now let's consider the case where $f\neq 0$ at some point $x_1$ . Suppose $f(x_1)>0$ . Then, by continuity, there is an interval $(m,n)$ around $x_1$ such that $f(x)>0$ for $x\in (m,n)$ . By part $(a)$ , $f(x)=ke^{cx}$ for $k>0$ on $(m,n)$ . Suppose $f(n)\leq0$ . But $\lim\limits_{x\to n^-} ke^{cx}=ke^{cn}>0\neq f(n)$ . Thus $f$ is discontinuous at $n$ , which contradicts our assumptions. Therefore, $f(n)>0$ . Analogously, we can show that $f(m)>0$ . So, if the above is correct (is it?), I am at a similar point to the linked answer. I need to extend my argument such that $m$ and $n$ move outwards to encompass all of $[a,b]$ . How do I finish this proof? Edit: I had forgotten to check the solution manual, but I just did and it is also kind of terse. It uses the same argument I used above, but leaves the part I am asking about as a hint. It says On an interval where $f$ is non-zero it has the form $f(x)=ke^{cx}$ . But this can't approach $0$ at the endpoint of the interval; so $f$ couldn't be $0$ at the endpoints. This proves that if $f$ is non-zero at one point $x_0$ , then it is nowhere $0$ (consider $\sup\{x>x_0:f(x)\neq 0\}$ and $\inf\{x<x_0:f(x)\neq 0\}$","Suppose that on some interval the function satisfies for some number . (a) Assuming that is never , use Problem 20(b) to prove that for some number . It follows that for some . (b) Show that this result holds without the added assumption that is never . Hint: show that can't be at the endpoint of an open interval on which it is nowhere . I am interested specifically in the solution to part . A question about this problem has been asked before . However, I am not fully satisfied with the accepted answer where it pertains to item . Though I understand the reasoning in that answer, I felt that the final part was left a bit vague we can continue the same argument by replacing with (or ) and thereby extend the region where does not vanish. Using the same argument repeatedly we can show that does not vanish at any point in . How exactly do we use the same argument repeatedly (ie, I would like to see what that looks like formally)? I think I reached a similar point in my own attempt at a proof, and that is why I am asking this question. Here is my attempt Our only assumption is that on some interval. Let be the interval. implies that is differentiable and hence continuous on . First, there is a trivial case: . Now let's consider the case where at some point . Suppose . Then, by continuity, there is an interval around such that for . By part , for on . Suppose . But . Thus is discontinuous at , which contradicts our assumptions. Therefore, . Analogously, we can show that . So, if the above is correct (is it?), I am at a similar point to the linked answer. I need to extend my argument such that and move outwards to encompass all of . How do I finish this proof? Edit: I had forgotten to check the solution manual, but I just did and it is also kind of terse. It uses the same argument I used above, but leaves the part I am asking about as a hint. It says On an interval where is non-zero it has the form . But this can't approach at the endpoint of the interval; so couldn't be at the endpoints. This proves that if is non-zero at one point , then it is nowhere (consider and","f f'=cf c f 0 |f(x)|=le^{cx} l>0 f(x)=ke^{cx} k f 0 f 0 0 (b) (b) a (a+p) (a-p) f f I f'=cf\tag{1} [a,b] (1) f [a,b] f(x)=0=0\cdot e^{cx} f\neq 0 x_1 f(x_1)>0 (m,n) x_1 f(x)>0 x\in (m,n) (a) f(x)=ke^{cx} k>0 (m,n) f(n)\leq0 \lim\limits_{x\to n^-} ke^{cx}=ke^{cn}>0\neq f(n) f n f(n)>0 f(m)>0 m n [a,b] f f(x)=ke^{cx} 0 f 0 f x_0 0 \sup\{x>x_0:f(x)\neq 0\} \inf\{x<x_0:f(x)\neq 0\}","['calculus', 'derivatives', 'solution-verification', 'logarithms', 'exponential-function']"
68,"Spivak, Ch. 15, Problem *26c: show $\lim\limits_{\lambda\to \infty} \int_a^b f(x)\sin{(\lambda x)}dx=0$ for any $f$ integrable on $[a,b]$.","Spivak, Ch. 15, Problem *26c: show  for any  integrable on .","\lim\limits_{\lambda\to \infty} \int_a^b f(x)\sin{(\lambda x)}dx=0 f [a,b]","The following problem is from Ch. 15 of Spivak's Calculus . My question is about understanding the solution manual solution to item $(c)$ . I've asked a separate question about my own attempt at a solution which differs from the solution manual . *26. It is an excellent test of intuition to predict the value of $$\lim\limits_{\lambda\to \infty} \int_a^b f(x)\sin{(\lambda x)}dx$$ Continuous functions should be the most accessible to intuition, but once you get the right idea for a proof the limit can easily be established for any integrable $f$ (a) Show that $\lim\limits_{\lambda\to \infty} \int_a^b \sin{(\lambda  x)}dx=0$ , by computing the integral explicitly. (b) Show that if $s$ is a step function on $[a,b]$ , then $\lim\limits_{\lambda\to \infty} \int_a^b s(x)\sin{(\lambda x)}dx=0$ . (c) Finally, use Problem 13-26 to show that $\lim\limits_{\lambda\to  \infty} \int_a^b f(x)\sin{(\lambda x)}dx=0$ for any function $f$ which is integrable on $[a,b]$ . This result, like Problem 12, plays an important role in the theory of Fourier series; it is known as the Riemann-Lebesgue Lemma. Item $(a)$ is just the limit of a definite integral $$\lim\limits_{\lambda\to \infty} \int_a^b \sin{(\lambda x)}dx$$ $$=\lim\limits_{\lambda\to \infty} -\frac{1}{\lambda}\left ( \cos{(\lambda d)}-\cos{(\lambda c)} \right )=0$$ For item $(b)$ , note that a step function is defined based on a partition $P=\{t_0,...,t_n\}$ of $[a,b]$ . Then $$\lim\limits_{\lambda\to \infty}\int_a^b s(x)\sin{(\lambda x)}dx=\lim\limits_{\lambda\to \infty} \sum_{i=1}^n s_i\int_{t_{i-1}}^{t_i} \sin{(\lambda x)}dx=0$$ For the items above, my solution coincided with the solution manual. For item $(c)$ my solution was different . For now I'd like to understand the solution manual solution (specifically the last part of it). Here it is Solution Manual Solution to $(c)$ For any $\epsilon>0$ there is, by Problem 13-16, a step function $s\leq f$ with $$\int_a^b [f(x)-s(x)]dx<\epsilon$$ Now $$\left | \int_a^b f(x)\sin{(\lambda x)}dx-\int_a^b s(x)\sin{(\lambda  x)}dx \right |\tag{1}$$ $$=\left | \int_a^b [f(x)-s(x)]\sin{(\lambda x)} dx \right  |$$ $$\leq \int_a^b [f(x)-s(x)]\cdot |\sin{(\lambda x)} |dx$$ $$\leq \int_a^b [f(x)-s(x)]dx<\epsilon\tag{2}$$ So far so good Part $(b)$ then shows that $$\lim\limits_{\lambda\to \infty} \left | \int_a^b f(x)\sin{(\lambda  x)}dx \right |<\epsilon$$ Since this is true for every $\epsilon>0$ , the limit must be $0$ . I'd like to understand this last part. I think what happened is that from $(1)$ we had $$\left | \int_a^b f(x)\sin{(\lambda x)}dx\right | -\left | \int_a^b s(x)\sin{(\lambda  x)}dx \right |\leq\left | \int_a^b f(x)\sin{(\lambda x)}dx-\int_a^b s(x)\sin{(\lambda  x)}dx \right |<\epsilon$$ and then we took the limit $$\lim\limits_{\lambda\to \infty}\left | \int_a^b f(x)\sin{(\lambda x)}dx\right | -\lim\limits_{\lambda\to \infty}\left | \int_a^b s(x)\sin{(\lambda  x)}dx \right |<\epsilon$$ But since from $(b)$ we know that $\lim\limits_{\lambda\to \infty}\left | \int_a^b s(x)\sin{(\lambda  x)}dx\right |=0$ we have $$\lim\limits_{\lambda\to \infty}\left | \int_a^b f(x)\sin{(\lambda x)}dx\right |<\epsilon$$ Is this filling in of the steps correct?","The following problem is from Ch. 15 of Spivak's Calculus . My question is about understanding the solution manual solution to item . I've asked a separate question about my own attempt at a solution which differs from the solution manual . *26. It is an excellent test of intuition to predict the value of Continuous functions should be the most accessible to intuition, but once you get the right idea for a proof the limit can easily be established for any integrable (a) Show that , by computing the integral explicitly. (b) Show that if is a step function on , then . (c) Finally, use Problem 13-26 to show that for any function which is integrable on . This result, like Problem 12, plays an important role in the theory of Fourier series; it is known as the Riemann-Lebesgue Lemma. Item is just the limit of a definite integral For item , note that a step function is defined based on a partition of . Then For the items above, my solution coincided with the solution manual. For item my solution was different . For now I'd like to understand the solution manual solution (specifically the last part of it). Here it is Solution Manual Solution to For any there is, by Problem 13-16, a step function with Now So far so good Part then shows that Since this is true for every , the limit must be . I'd like to understand this last part. I think what happened is that from we had and then we took the limit But since from we know that we have Is this filling in of the steps correct?","(c) \lim\limits_{\lambda\to \infty} \int_a^b f(x)\sin{(\lambda x)}dx f \lim\limits_{\lambda\to \infty} \int_a^b \sin{(\lambda
 x)}dx=0 s [a,b] \lim\limits_{\lambda\to \infty} \int_a^b s(x)\sin{(\lambda x)}dx=0 \lim\limits_{\lambda\to
 \infty} \int_a^b f(x)\sin{(\lambda x)}dx=0 f [a,b] (a) \lim\limits_{\lambda\to \infty} \int_a^b \sin{(\lambda x)}dx =\lim\limits_{\lambda\to \infty} -\frac{1}{\lambda}\left ( \cos{(\lambda d)}-\cos{(\lambda c)} \right )=0 (b) P=\{t_0,...,t_n\} [a,b] \lim\limits_{\lambda\to \infty}\int_a^b s(x)\sin{(\lambda x)}dx=\lim\limits_{\lambda\to \infty} \sum_{i=1}^n s_i\int_{t_{i-1}}^{t_i} \sin{(\lambda x)}dx=0 (c) (c) \epsilon>0 s\leq f \int_a^b [f(x)-s(x)]dx<\epsilon \left | \int_a^b f(x)\sin{(\lambda x)}dx-\int_a^b s(x)\sin{(\lambda
 x)}dx \right |\tag{1} =\left | \int_a^b [f(x)-s(x)]\sin{(\lambda x)} dx \right
 | \leq \int_a^b [f(x)-s(x)]\cdot |\sin{(\lambda x)} |dx \leq \int_a^b [f(x)-s(x)]dx<\epsilon\tag{2} (b) \lim\limits_{\lambda\to \infty} \left | \int_a^b f(x)\sin{(\lambda
 x)}dx \right |<\epsilon \epsilon>0 0 (1) \left | \int_a^b f(x)\sin{(\lambda x)}dx\right | -\left | \int_a^b s(x)\sin{(\lambda
 x)}dx \right |\leq\left | \int_a^b f(x)\sin{(\lambda x)}dx-\int_a^b s(x)\sin{(\lambda
 x)}dx \right |<\epsilon \lim\limits_{\lambda\to \infty}\left | \int_a^b f(x)\sin{(\lambda x)}dx\right | -\lim\limits_{\lambda\to \infty}\left | \int_a^b s(x)\sin{(\lambda
 x)}dx \right |<\epsilon (b) \lim\limits_{\lambda\to \infty}\left | \int_a^b s(x)\sin{(\lambda
 x)}dx\right |=0 \lim\limits_{\lambda\to \infty}\left | \int_a^b f(x)\sin{(\lambda x)}dx\right |<\epsilon","['calculus', 'integration', 'limits', 'derivatives', 'proof-explanation']"
69,"$f=f(u)$ and $u=u(x,t)$, does partial derivatives be exchanged, $\frac{d}{dx}\left(\frac{df}{du}\right) = \frac{d}{du}\left(\frac{df}{dx}\right)$?","and , does partial derivatives be exchanged, ?","f=f(u) u=u(x,t) \frac{d}{dx}\left(\frac{df}{du}\right) = \frac{d}{du}\left(\frac{df}{dx}\right)","I am working with compressible Navier–Stokes equations. When calculating the derivatives to linearize the equation, I got confused about one term. If $f=f(u)$ , and $u=u(x,t)$ , are the two derivatives equal, i.e., $\frac{d}{dx}\left(\frac{df}{du}\right) = \frac{d}{du}\left(\frac{df}{dx}\right)$ ? I've tested a simple scalar case, $f=u^2$ and $u=\sqrt x$ . In this case, the partial derivatives are not exahcngeable. But I was wondering if there is any requirement, by satisfying which makes them exchangable?","I am working with compressible Navier–Stokes equations. When calculating the derivatives to linearize the equation, I got confused about one term. If , and , are the two derivatives equal, i.e., ? I've tested a simple scalar case, and . In this case, the partial derivatives are not exahcngeable. But I was wondering if there is any requirement, by satisfying which makes them exchangable?","f=f(u) u=u(x,t) \frac{d}{dx}\left(\frac{df}{du}\right) = \frac{d}{du}\left(\frac{df}{dx}\right) f=u^2 u=\sqrt x","['calculus', 'derivatives', 'partial-differential-equations', 'mathematical-physics', 'fluid-dynamics']"
70,Stuck on problems about differential / derivatives,Stuck on problems about differential / derivatives,,"For the real number $t$ that $t \ge 6 $ , let $f(x) = \frac{1}{t} \left( \frac{1}{8}x^3 + \frac{t^2}{8}x + 2 \right)$ . Let the sum of all real numbers $k$ satisfying the following condition be $g(t)$ : function $\{f(x)-x\}^2$ has an extreme value at $x=k$ . For the real number $p$ where $g(p) = -1$ , $$\int_{6}^{p}g'(t)(8t-t^2)dt = ?$$ My attempt: Since the function $h(x) = \{f(x)-x\}^2$ has an extreme value at $x=k$ , by differentiation, $h'(k) = 2\{ f(k)-k \}\left( f'(k)-1 \right) =0$ . So, $f(k) = k$ or $f'(k) = 1$ . $h''(k) \ne 0$ because it cannot be an inflection point. Applying it, if $f(k) = k$ , $f'(k) \ne 1$ . if $f'(k) = 1$ , $f(k) \ne k$ . And I'm stuck. It seems like I have to use the equation of $f(x)$ , but it gives me tons of equations with messy variables. A little hint would be really helpful. (+ Additional approach) : Applying step 4. to $f(x)$ , we can get $t(8-t) = k^2 + \frac{16}{k}$ ( $k\ne 2$ when $t=6$ ). By multiplying $k$ to each side, $k^3 - t(8-t)k + 16 = 0$ . This is a cubic equation with respect to $k$ , so I thought I could use $\alpha + \beta + \gamma = -\frac{b}{a}$ to get $g(t)$ . But as you can see here, it becomes $0$ since it also counts imaginary roots.","For the real number that , let . Let the sum of all real numbers satisfying the following condition be : function has an extreme value at . For the real number where , My attempt: Since the function has an extreme value at , by differentiation, . So, or . because it cannot be an inflection point. Applying it, if , . if , . And I'm stuck. It seems like I have to use the equation of , but it gives me tons of equations with messy variables. A little hint would be really helpful. (+ Additional approach) : Applying step 4. to , we can get ( when ). By multiplying to each side, . This is a cubic equation with respect to , so I thought I could use to get . But as you can see here, it becomes since it also counts imaginary roots.",t t \ge 6  f(x) = \frac{1}{t} \left( \frac{1}{8}x^3 + \frac{t^2}{8}x + 2 \right) k g(t) \{f(x)-x\}^2 x=k p g(p) = -1 \int_{6}^{p}g'(t)(8t-t^2)dt = ? h(x) = \{f(x)-x\}^2 x=k h'(k) = 2\{ f(k)-k \}\left( f'(k)-1 \right) =0 f(k) = k f'(k) = 1 h''(k) \ne 0 f(k) = k f'(k) \ne 1 f'(k) = 1 f(k) \ne k f(x) f(x) t(8-t) = k^2 + \frac{16}{k} k\ne 2 t=6 k k^3 - t(8-t)k + 16 = 0 k \alpha + \beta + \gamma = -\frac{b}{a} g(t) 0,"['derivatives', 'polynomials', 'cubics']"
71,"Spivak, Ch. 14, Prob 12d : Prove if $f'$ is periodic with period $a$ and $f$ is periodic (with some period not necessarily $=a$), then $f(a)=f(0)$.","Spivak, Ch. 14, Prob 12d : Prove if  is periodic with period  and  is periodic (with some period not necessarily ), then .",f' a f =a f(a)=f(0),"In Spivak's Calculus , there is the following problem in Ch. 14 ""Fundamental Theorem of Calculus"" A function $f$ is periodic, with period $a$ , if $f(x+a)=f(x)$ for all $x$ . *(d) Prove that if $f'$ is periodic with period $a$ and $f$ is periodic (with some period not necessarily $=a$ ), then $f(a)=f(0)$ . The solution manual solution (with my filling in some intermediate steps) is as follows Let $g(x)=f(x+a)-f(x)$ . Then $g'(x)=f'(x+a)-f'(x)=0$ , because $f'$ is periodic. Therefore, $g$ is constant and $g(x)=g(0)$ , so $$f(x+a)-f(x)=f(a)-f(0)$$ $$f(x+a)=f(x)+f(a)-f(0)$$ $$f(na)=f[(n-1)a]+f(a)-f(0)$$ $$=f[(n-2)a+a]+f(a)-f(0)$$ $$=f[(n-2)a]+2f(a)-2f(0)$$ $$=f(0)+nf(a)-nf(0)$$ $$=n(f(a)-f(0))+f(0)$$ Since $f$ must be bounded (since it is periodic), then $f(a)=f(0)$ , otherwise $f$ would be unbounded. Is there an interesting interpretation for this result? f apparently can have some period $b\neq a$ which we didn't need to make use of in the proof above. Doesn't the fact that $g$ is constant mean that $f$ has a period of $a$ ? $$g(x)=f(x+a)-f(x)=0, \text{ for all } x $$ $$\implies f(x+a)=f(x), \text{ for all }x$$ When I tried to solve this problem I started on the assumption that $f$ had a period $b\neq a$ $$f'(x+a)=f'(x)$$ $$f(x)=\int_0^x f' + f(0)$$ $$f(x+b)=\int_0^{x+b}f’ +f(0)=f(x)$$ But didn't get much further than this.","In Spivak's Calculus , there is the following problem in Ch. 14 ""Fundamental Theorem of Calculus"" A function is periodic, with period , if for all . *(d) Prove that if is periodic with period and is periodic (with some period not necessarily ), then . The solution manual solution (with my filling in some intermediate steps) is as follows Let . Then , because is periodic. Therefore, is constant and , so Since must be bounded (since it is periodic), then , otherwise would be unbounded. Is there an interesting interpretation for this result? f apparently can have some period which we didn't need to make use of in the proof above. Doesn't the fact that is constant mean that has a period of ? When I tried to solve this problem I started on the assumption that had a period But didn't get much further than this.","f a f(x+a)=f(x) x f' a f =a f(a)=f(0) g(x)=f(x+a)-f(x) g'(x)=f'(x+a)-f'(x)=0 f' g g(x)=g(0) f(x+a)-f(x)=f(a)-f(0) f(x+a)=f(x)+f(a)-f(0) f(na)=f[(n-1)a]+f(a)-f(0) =f[(n-2)a+a]+f(a)-f(0) =f[(n-2)a]+2f(a)-2f(0) =f(0)+nf(a)-nf(0) =n(f(a)-f(0))+f(0) f f(a)=f(0) f b\neq a g f a g(x)=f(x+a)-f(x)=0, \text{ for all } x  \implies f(x+a)=f(x), \text{ for all }x f b\neq a f'(x+a)=f'(x) f(x)=\int_0^x f' + f(0) f(x+b)=\int_0^{x+b}f’ +f(0)=f(x)","['real-analysis', 'integration', 'derivatives', 'periodic-functions']"
72,"Find all continuous functions $f$ satisfying $\int\limits_0^x f=(f(x))^2+C$, for some constant $C\neq 0$. Why is the assumption $C\neq 0$ necessary?","Find all continuous functions  satisfying , for some constant . Why is the assumption  necessary?",f \int\limits_0^x f=(f(x))^2+C C\neq 0 C\neq 0,"The following is a problem from Chapter 14 ""The Fundamental Theorem of Calculus"" from Spivak's Calculus (a) Find all continuous functions $f$ satisfying $$\int\limits_0^x f=(f(x))^2+C, \text{ for some constant } C \neq 0\tag{1}$$ assuming that $f$ has at most one $0$ . (b) Also find a solution that is $0$ on an interval $(-\infty, b]$ with $0<b$ , but non-zero for $x>b$ (c) Finally, for $C=0$ and any interval $[a,b]$ with $a<0<b$ , find a solution that is $0$ on $[a,b]$ , but non-zero elsewhere. My question is: why the assumption that $C \neq 0$ ? The solution to this problem doesn't seem to require that assumption. Here is my solution to part $(a)$ First, note that $$\int_0^0 f = 0 = (f(0))^2+C \implies f(0)=\pm \sqrt{-C}$$ Therefore, if $f$ satisfies $(1)$ then $C<0$ . Assume $f$ is continuous. Then we can apply the FTC1 to $(1)$ $$f(x)=2f(x)f'(x)$$ $$f(x)(1-2f'(x))=0\tag{2}$$ By assumption, $f$ is zero at most at one point. Assume $f(x_0)=0$ . Then at all $x\neq x_0$ we have $$f'(x)=\frac{1}{2}\tag{3}$$ But we know that a function of the form $f(x)=\frac{x}{2}+b$ satisfies $(3)$ . Therefore, we can use the FTC2 to obtain $$\int_0^x \frac{1}{2}=f(x)-f(0)=\frac{x}{2}$$ $$f(x)=\frac{x}{2}+f(0)$$ $$f(x)=\frac{x}{2}\pm \sqrt{-C}=\frac{x}{2}\pm k, k\in\mathbb{R}$$ This represents the set of all lines with slope $1/2$ . If we choose $C=0$ then we have $$f(x)=\frac{x}{2}$$ $$\int_0^x \frac{x}{2}dx = \frac{x^2}{4}=\left ( \frac{x}{2} \right )^2=(f(x))^2$$ So the question remains: why the assumption that $C\neq 0$ ? Here is the solution to part $(b)$ For this item, we are dropping the assumption that $f$ is zero at only one point. As before, we have eq. $(2)$ . If $f(x)=0$ for all $x \in (-\infty,b]$ , then for $x>b$ we have $f'(x)=\frac{1}{2}$ . This means that $$f(x)=\begin{cases} 0, \text{ if } x\leq b \\ \frac{x}{2}+k, \text{ if } x>b, \text{ with } k \in \mathbb{R} \end{cases}$$ Part (c) is similar, but now $f$ is zero on an interval $[a,b]$ with $a<0<b$ . By the same steps as before, we have that $f'(x)=\frac{1}{2}$ for $x \in (-\infty,a) \bigcup (b,\infty)$ .","The following is a problem from Chapter 14 ""The Fundamental Theorem of Calculus"" from Spivak's Calculus (a) Find all continuous functions satisfying assuming that has at most one . (b) Also find a solution that is on an interval with , but non-zero for (c) Finally, for and any interval with , find a solution that is on , but non-zero elsewhere. My question is: why the assumption that ? The solution to this problem doesn't seem to require that assumption. Here is my solution to part First, note that Therefore, if satisfies then . Assume is continuous. Then we can apply the FTC1 to By assumption, is zero at most at one point. Assume . Then at all we have But we know that a function of the form satisfies . Therefore, we can use the FTC2 to obtain This represents the set of all lines with slope . If we choose then we have So the question remains: why the assumption that ? Here is the solution to part For this item, we are dropping the assumption that is zero at only one point. As before, we have eq. . If for all , then for we have . This means that Part (c) is similar, but now is zero on an interval with . By the same steps as before, we have that for .","f \int\limits_0^x f=(f(x))^2+C, \text{ for some constant } C \neq 0\tag{1} f 0 0 (-\infty, b] 0<b x>b C=0 [a,b] a<0<b 0 [a,b] C \neq 0 (a) \int_0^0 f = 0 = (f(0))^2+C \implies f(0)=\pm \sqrt{-C} f (1) C<0 f (1) f(x)=2f(x)f'(x) f(x)(1-2f'(x))=0\tag{2} f f(x_0)=0 x\neq x_0 f'(x)=\frac{1}{2}\tag{3} f(x)=\frac{x}{2}+b (3) \int_0^x \frac{1}{2}=f(x)-f(0)=\frac{x}{2} f(x)=\frac{x}{2}+f(0) f(x)=\frac{x}{2}\pm \sqrt{-C}=\frac{x}{2}\pm k, k\in\mathbb{R} 1/2 C=0 f(x)=\frac{x}{2} \int_0^x \frac{x}{2}dx = \frac{x^2}{4}=\left ( \frac{x}{2} \right )^2=(f(x))^2 C\neq 0 (b) f (2) f(x)=0 x \in (-\infty,b] x>b f'(x)=\frac{1}{2} f(x)=\begin{cases} 0, \text{ if } x\leq b \\ \frac{x}{2}+k, \text{ if } x>b, \text{ with } k \in \mathbb{R} \end{cases} f [a,b] a<0<b f'(x)=\frac{1}{2} x \in (-\infty,a) \bigcup (b,\infty)","['calculus', 'integration', 'derivatives', 'proof-explanation']"
73,"Should the vertical ""at bar"" go before or after the function being differentiated?","Should the vertical ""at bar"" go before or after the function being differentiated?",,Suppose I want to calculate the derivative of a long function at a particular point $a$ . Is it more common to write $$ \frac{d}{dx} \left( x^2 \sin(x)^{(3x-1)^2} + \frac{e^x}{2 x^3 -2 x^2 +1} -3 \log(x)\right) \Bigg|_{x=a} $$ or $$ \frac{d}{dx}\Bigg|_{x=a} \left( x^2 \sin(x)^{(3x-1)^2} + \frac{e^x}{2 x^3 -2 x^2 +1} -3 \log(x) \right)? $$ Does either notational convention have any advantages for clarity?,Suppose I want to calculate the derivative of a long function at a particular point . Is it more common to write or Does either notational convention have any advantages for clarity?,"a 
\frac{d}{dx} \left( x^2 \sin(x)^{(3x-1)^2} + \frac{e^x}{2 x^3 -2 x^2 +1} -3 \log(x)\right) \Bigg|_{x=a}
 
\frac{d}{dx}\Bigg|_{x=a} \left( x^2 \sin(x)^{(3x-1)^2} + \frac{e^x}{2 x^3 -2 x^2 +1} -3 \log(x) \right)?
","['derivatives', 'notation']"
74,Time derivative of the blend of a pair of quaternion curves,Time derivative of the blend of a pair of quaternion curves,,"I have two curves ${\bf q}_0(t), {\bf q}_1(t)$ . Each curve maps time $t$ to a unit quaternion. Construction of these curves is not important here, although we do have the respective time derivatives for the curves given by $\dot{\bf q}_0(t), \dot{\bf q}_1(t)$ . My goal is to find a unit quaternion and its time derivate of the two curves blended by some parameter $p \in [0, 1]$ . Using a slerp for the blended unit quaternion gives ${\bf q} = {\bf q}_0 ({\bf q}^*_0 {\bf q}_1)^p$ , where ${\bf q}^*$ is the quaternion conjugate, and ${\bf q}^p = e^{p \log({\bf q})}$ . For the sake of this discussion, assume $p$ is not time dependent. It will be in the final solution but for now let's not be bothered by that. My question is how to compute the time derivative $\dot{\bf q}$ . Intuitively, it appears to be something of the form $\dot{\bf q} = ((1 - p)\dot{\bf q}_0{\bf q}^*_0 + p\dot{\bf q}_1 {\bf q}^*_1) {\bf q}$ , which is linearly interpolated angular velocities times blend result. This formula obviously is correct for $p = 0$ and for $p = 1$ . I'm trying to find some mathematical backing for the general case. Naively, taking $\dot{\bf q} = \dot{\bf q}_0({\bf q}^*_0 {\bf q}_1)^p + {\bf q}_0 p ({\bf q}^*_0 {\bf q}_1)^{p - 1} (\dot{\bf q}^*_0 {\bf q}_1 + {\bf q}^*_0 \dot{\bf q}_1)$ can't be correct in the general case, since the product operator does not commute. I could use some help in finding the proper time derivative. In particular, I would like to know whether there is a general formula for $\frac{d}{dt}{\bf q}(t)^p$ , for constant $p \in [0, 1]$ . Perhaps someone with a grasp of Lie groups/algebras could shed some light on this.  Thanks for your help.","I have two curves . Each curve maps time to a unit quaternion. Construction of these curves is not important here, although we do have the respective time derivatives for the curves given by . My goal is to find a unit quaternion and its time derivate of the two curves blended by some parameter . Using a slerp for the blended unit quaternion gives , where is the quaternion conjugate, and . For the sake of this discussion, assume is not time dependent. It will be in the final solution but for now let's not be bothered by that. My question is how to compute the time derivative . Intuitively, it appears to be something of the form , which is linearly interpolated angular velocities times blend result. This formula obviously is correct for and for . I'm trying to find some mathematical backing for the general case. Naively, taking can't be correct in the general case, since the product operator does not commute. I could use some help in finding the proper time derivative. In particular, I would like to know whether there is a general formula for , for constant . Perhaps someone with a grasp of Lie groups/algebras could shed some light on this.  Thanks for your help.","{\bf q}_0(t), {\bf q}_1(t) t \dot{\bf q}_0(t), \dot{\bf q}_1(t) p \in [0, 1] {\bf q} = {\bf q}_0 ({\bf q}^*_0 {\bf q}_1)^p {\bf q}^* {\bf q}^p = e^{p \log({\bf q})} p \dot{\bf q} \dot{\bf q} = ((1 - p)\dot{\bf q}_0{\bf q}^*_0 + p\dot{\bf q}_1 {\bf q}^*_1) {\bf q} p = 0 p = 1 \dot{\bf q} = \dot{\bf q}_0({\bf q}^*_0 {\bf q}_1)^p + {\bf q}_0 p ({\bf q}^*_0 {\bf q}_1)^{p - 1} (\dot{\bf q}^*_0 {\bf q}_1 + {\bf q}^*_0 \dot{\bf q}_1) \frac{d}{dt}{\bf q}(t)^p p \in [0, 1]","['derivatives', 'lie-groups', 'lie-algebras', 'interpolation', 'quaternions']"
75,Assume $A$ is open convex and $f$ convex continuous. Then $f$ is $L$-lipschitz on $A$ if and only if $\partial f(A) \subset L B_{X^*}$,Assume  is open convex and  convex continuous. Then  is -lipschitz on  if and only if,A f f L A \partial f(A) \subset L B_{X^*},"This thread is meant to record a question that I feel interesting during my self-study. I'm very happy to receive your suggestion and comments. See: SE blog: Answer own Question and MSE meta: Answer own Question . Let $A$ be a subset of a normed space $X$ and $f:A \to \mathbb R$ . The subdifferential of $f$ at $a \in A$ is the set $$ \partial f(a)=\left\{x^* \in X^* \mid f(x) - f(a) \ge \langle x^*, x-a \rangle \text { for each } x \in A\right\}. $$ The elements of $\partial f(a)$ are called subgradients of $f$ at $a$ . The multivalued mapping $\partial f: A \to \mathcal P(X^*), x \mapsto \partial f(x)$ , is called the subdifferential of $f$ . The image $\partial f(E)$ of a set $E \subset A$ is the set $$ \partial f(E)=\bigcup_{x \in A} \partial f(x) . $$ Theorem: Assume $A$ is open convex and $f$ convex continuous. Then $f$ is $L$ -lipschitz on $A$ if and only if $\partial f(A) \subset L B_{X^*}$ .","This thread is meant to record a question that I feel interesting during my self-study. I'm very happy to receive your suggestion and comments. See: SE blog: Answer own Question and MSE meta: Answer own Question . Let be a subset of a normed space and . The subdifferential of at is the set The elements of are called subgradients of at . The multivalued mapping , is called the subdifferential of . The image of a set is the set Theorem: Assume is open convex and convex continuous. Then is -lipschitz on if and only if .","A X f:A \to \mathbb R f a \in A 
\partial f(a)=\left\{x^* \in X^* \mid f(x) - f(a) \ge \langle x^*, x-a \rangle \text { for each } x \in A\right\}.
 \partial f(a) f a \partial f: A \to \mathcal P(X^*), x \mapsto \partial f(x) f \partial f(E) E \subset A 
\partial f(E)=\bigcup_{x \in A} \partial f(x) .
 A f f L A \partial f(A) \subset L B_{X^*}","['functional-analysis', 'derivatives', 'normed-spaces', 'convex-analysis']"
76,Does continuity of $f(z)$ imply that of $\overline{f(\overline{z})}$?,Does continuity of  imply that of ?,f(z) \overline{f(\overline{z})},"I was stuck on a problem with complex-valued functions. Here is the question: If a function $f(z)$ is continuous at a point $z=z_0$ , then does this imply that the induced function $\overline{f(\overline{z})}$ is continuous at $z=z_0$ ? Much help required. Thank you so much!!","I was stuck on a problem with complex-valued functions. Here is the question: If a function is continuous at a point , then does this imply that the induced function is continuous at ? Much help required. Thank you so much!!",f(z) z=z_0 \overline{f(\overline{z})} z=z_0,"['complex-analysis', 'derivatives', 'partial-differential-equations', 'differential']"
77,Let $f$ be Lipschitz around $a \in \mathbb R^d$. Then $f$ is Fréchet differentiable at $a$ if and only if $f$ is Gâteaux differentiable at $a$,Let  be Lipschitz around . Then  is Fréchet differentiable at  if and only if  is Gâteaux differentiable at,f a \in \mathbb R^d f a f a,"This thread is meant to record a question that I feel interesting during my self-study. I'm very happy to receive your suggestion and comments. See: SE blog: Answer own Question and MSE meta: Answer own Question . Anyway, it is written as problem. Let $X$ be a normed space, $A \subset X$ an open set, $f: A \rightarrow \mathbb{R}$ a function, and $a \in A$ a point. For a ""direction"" $v \in X$ , we shall consider the directional derivative $f^{\prime}(a, v)$ , which are defined by: $$ \begin{aligned} f^{\prime}(a, v) &=\lim _{t \rightarrow 0} \frac{f(a+t v)-f(a)}{t} \end{aligned} $$ We shall say that $f$ is: Gâteaux differentiable at $a$ if $f^{\prime}(a, \cdot) \in X^{*}$ , i.e., $f^{\prime}(a, \cdot)$ is everywhere defined, real-valued, linear and continuous); Fréchet differentiable at $a$ if there exists $x^{*} \in X^{*}$ such that $$ \lim _{\|h\| \rightarrow 0} \frac{f(a+h)-f(a)-x^{*}(h)}{\|h\|}=0 . $$ Theorem: Let $X, A, f, a$ be as above. Assume that $X=\mathbb{R}^{d}$ and $f$ is Lipschitz on some neighborhood of $a$ . Then $f$ is Fréchet differentiable at $a$ if and only if $f$ is Gâteaux differentiable at $a$ .","This thread is meant to record a question that I feel interesting during my self-study. I'm very happy to receive your suggestion and comments. See: SE blog: Answer own Question and MSE meta: Answer own Question . Anyway, it is written as problem. Let be a normed space, an open set, a function, and a point. For a ""direction"" , we shall consider the directional derivative , which are defined by: We shall say that is: Gâteaux differentiable at if , i.e., is everywhere defined, real-valued, linear and continuous); Fréchet differentiable at if there exists such that Theorem: Let be as above. Assume that and is Lipschitz on some neighborhood of . Then is Fréchet differentiable at if and only if is Gâteaux differentiable at .","X A \subset X f: A \rightarrow \mathbb{R} a \in A v \in X f^{\prime}(a, v) 
\begin{aligned}
f^{\prime}(a, v) &=\lim _{t \rightarrow 0} \frac{f(a+t v)-f(a)}{t}
\end{aligned}
 f a f^{\prime}(a, \cdot) \in X^{*} f^{\prime}(a, \cdot) a x^{*} \in X^{*} 
\lim _{\|h\| \rightarrow 0} \frac{f(a+h)-f(a)-x^{*}(h)}{\|h\|}=0 .
 X, A, f, a X=\mathbb{R}^{d} f a f a f a","['real-analysis', 'derivatives', 'lipschitz-functions']"
78,"How do we rigorously prove that for $n>1$, $(1+x)^{n-1}<1$ for $-1<x<0$?","How do we rigorously prove that for ,  for ?",n>1 (1+x)^{n-1}<1 -1<x<0,"Given $n>1$ and $$(1+x)^{n-1}<1$$ Intuitively I can see that for $x \in (-1,0)$ , we have $1+x<1$ , and if we raise that to any power then it will be smaller than 1. How do we prove this rigorously? For context on how I came upon this question, the following is a problem from Spivak's Calculus, Ch. 11 on ""Significance of Derivatives"". Use derivatives to prove that if $n \geq 1$ , then $$(1+x)^n > 1+nx, \text{ for } -1<x<0 \text{ and } x>0$$ (notice that equality holds for $x=0$ ) The solution in the solution manual is a bit terse Let $g(x)=(1+x)^n-(1+nx)$ . Then $g(0)=0$ , but $$g'(x)=n(1+x)^{n-1}-n\tag{1}$$ Since $n-1 \neq 0$ this means that $$\begin{align}g'(x) & < 0 \text{ for } -1<x<0, \\ & >0 \text{ for }  x>0 \end{align}\tag{2}$$ Thus $g(x)>0$ for $-1<x<0$ and $x>0$ I've been reading this book as a self-contained exposition of mathematical concepts that build upon one another chapter by chapter. I can't recall among the theorems I've seen so far a justification for the step from $(1)$ to $(2)$ . What theorem(s) justifies making this step? In particular, we reach $$(1+x)^{n-1}<1$$","Given and Intuitively I can see that for , we have , and if we raise that to any power then it will be smaller than 1. How do we prove this rigorously? For context on how I came upon this question, the following is a problem from Spivak's Calculus, Ch. 11 on ""Significance of Derivatives"". Use derivatives to prove that if , then (notice that equality holds for ) The solution in the solution manual is a bit terse Let . Then , but Since this means that Thus for and I've been reading this book as a self-contained exposition of mathematical concepts that build upon one another chapter by chapter. I can't recall among the theorems I've seen so far a justification for the step from to . What theorem(s) justifies making this step? In particular, we reach","n>1 (1+x)^{n-1}<1 x \in (-1,0) 1+x<1 n \geq 1 (1+x)^n > 1+nx, \text{ for } -1<x<0 \text{ and } x>0 x=0 g(x)=(1+x)^n-(1+nx) g(0)=0 g'(x)=n(1+x)^{n-1}-n\tag{1} n-1 \neq 0 \begin{align}g'(x) & < 0 \text{ for } -1<x<0, \\ & >0 \text{ for }
 x>0 \end{align}\tag{2} g(x)>0 -1<x<0 x>0 (1) (2) (1+x)^{n-1}<1","['calculus', 'algebra-precalculus', 'derivatives', 'inequality']"
79,Show existence of bounded linear functional,Show existence of bounded linear functional,,"To solve problem about a bounded linear functional, I am having a problem with the Hahn Banach Theorem. Problem is: For $n \in \mathbb{N}$ and $1 \leq p < \infty$ , let $X_n \subset L^p([0,1]$ be a collection of polynomials of degree $\leq n$ and let $X = \cup_{n=1}^\infty X_n$ . Is there a bounded linear functional $\Lambda$ on $L^p([0,1]$ such that $\Lambda(f) = f'(0)$ for all $f \in X$ ? First I show that for fixed $n \in \mathbb{N}$ , Consider $\Lambda : X_n \rightarrow \mathbb{R}$ which satisfies $\Lambda(f) = f'(0)$ . Since $f \in X_n$ , we can express $f$ as $f(x) = a_nx^n + \cdots + a_1x + a_0$ . $a_i$ are real numbers. By using fact that a derivative as a linear functional $(D)$ is linear and bounded, we know that $\Lambda$ is a bounded linear functional. Therefore we can use the Hahn Banach Theorem, that there exists an extension $\Lambda^* : L^p([0,1]) \rightarrow \mathbb{R}$ which satisfies $\Lambda^*(f) = f'(0)$ (Since $X_n$ is subspace in $L^p([0,1]))$ . But I am not sure how to extend this result to $X$ . Is it enough to show that $X$ is a subspace in $L^p([0,1])$ ?","To solve problem about a bounded linear functional, I am having a problem with the Hahn Banach Theorem. Problem is: For and , let be a collection of polynomials of degree and let . Is there a bounded linear functional on such that for all ? First I show that for fixed , Consider which satisfies . Since , we can express as . are real numbers. By using fact that a derivative as a linear functional is linear and bounded, we know that is a bounded linear functional. Therefore we can use the Hahn Banach Theorem, that there exists an extension which satisfies (Since is subspace in . But I am not sure how to extend this result to . Is it enough to show that is a subspace in ?","n \in \mathbb{N} 1 \leq p < \infty X_n \subset L^p([0,1] \leq n X = \cup_{n=1}^\infty X_n \Lambda L^p([0,1] \Lambda(f) = f'(0) f \in X n \in \mathbb{N} \Lambda : X_n \rightarrow \mathbb{R} \Lambda(f) = f'(0) f \in X_n f f(x) = a_nx^n + \cdots + a_1x + a_0 a_i (D) \Lambda \Lambda^* : L^p([0,1]) \rightarrow \mathbb{R} \Lambda^*(f) = f'(0) X_n L^p([0,1])) X X L^p([0,1])","['real-analysis', 'functional-analysis', 'derivatives', 'lp-spaces', 'hahn-banach-theorem']"
80,Uniform Convergence of Difference Quotient to Derivative using Compactness,Uniform Convergence of Difference Quotient to Derivative using Compactness,,"I have a question about a proof to a question in another discussion. This is regarding the uniform convergence of the difference quotient to the derivative of a function. Here is a link to the discussion. The question in the discussion was: Let $f:[0,1] \to \mathbb{R}$ be continuously differentiable. Prove that, for every $\epsilon > 0$ , there exists $\delta > 0$ such that $0 < |h| < \delta$ implies $\left| \frac{f(x+h) - f(x)}{h} - f'(x) \right| < \epsilon$ for all appropriate $x$ . And the answer provided by Hagen von Eitzen was: Let $\epsilon>0$ be given. For $\delta>0$ let $$U_\delta = \left\{a\in [0,1]\colon 0<|h|<\delta\Rightarrow \left|\frac{f(a+h)-f(a)}h-f'(a)\right|<\epsilon\right\}$$ Clearly, $\delta<\delta'$ implies $U_{\delta'}\subseteq U_\delta$ . By continuity of $f$ and $f'$ , $U_\delta$ is open and by definition of $f'$ , $$[0,1]=\bigcup _{\delta>0}U_\delta.$$ Since $[0,1]$ is compact, there is a finite subcover, i.e. there is a single $\delta>0$ such that $[0,1]=U_\delta$ . My question is: why is $U_{\delta}$ open? I tried creating some examples and this does appear to hold for those examples, but I couldn't see how to extend it to a general proof. I do understand how all of the other parts in the proof work though. It would also be appreciated if the use of the mean value theorem can be avoided as this was requested in the discussion when the question was asked. However, if no other reasonable approach is known to show that $U_{\delta}$ is open, then the use of the mean value theorem or other similar theorems is welcomed.","I have a question about a proof to a question in another discussion. This is regarding the uniform convergence of the difference quotient to the derivative of a function. Here is a link to the discussion. The question in the discussion was: Let be continuously differentiable. Prove that, for every , there exists such that implies for all appropriate . And the answer provided by Hagen von Eitzen was: Let be given. For let Clearly, implies . By continuity of and , is open and by definition of , Since is compact, there is a finite subcover, i.e. there is a single such that . My question is: why is open? I tried creating some examples and this does appear to hold for those examples, but I couldn't see how to extend it to a general proof. I do understand how all of the other parts in the proof work though. It would also be appreciated if the use of the mean value theorem can be avoided as this was requested in the discussion when the question was asked. However, if no other reasonable approach is known to show that is open, then the use of the mean value theorem or other similar theorems is welcomed.","f:[0,1] \to \mathbb{R} \epsilon > 0 \delta > 0 0 < |h| < \delta \left| \frac{f(x+h) - f(x)}{h} - f'(x) \right| < \epsilon x \epsilon>0 \delta>0 U_\delta = \left\{a\in [0,1]\colon 0<|h|<\delta\Rightarrow \left|\frac{f(a+h)-f(a)}h-f'(a)\right|<\epsilon\right\} \delta<\delta' U_{\delta'}\subseteq U_\delta f f' U_\delta f' [0,1]=\bigcup _{\delta>0}U_\delta. [0,1] \delta>0 [0,1]=U_\delta U_{\delta} U_{\delta}","['derivatives', 'uniform-convergence']"
81,Can you find a derivative of a function $f(x)$ with respect to something that isn't $x$?,Can you find a derivative of a function  with respect to something that isn't ?,f(x) x,"I am not sure how to phrase this exactly, but an example of what I'm talking about is finding the derivative of $x^4$ with respect to, say, $x^2$ . I was just thinking, maybe you could use some substitution to find the answer, and make $x^2=a$ , and thus find $d/dx$ of $a^2$ with respect to $a$ , and hence $d/d(x^2)$ of $x^4$ would be $2x^2$ . Can you do this? Or is it illegal?","I am not sure how to phrase this exactly, but an example of what I'm talking about is finding the derivative of with respect to, say, . I was just thinking, maybe you could use some substitution to find the answer, and make , and thus find of with respect to , and hence of would be . Can you do this? Or is it illegal?",x^4 x^2 x^2=a d/dx a^2 a d/d(x^2) x^4 2x^2,['derivatives']
82,How to graph the elasticity function ( knowing the - linear-demand function and the price function )? What goes wrong in my Desmos graph?,How to graph the elasticity function ( knowing the - linear-demand function and the price function )? What goes wrong in my Desmos graph?,,"My goal is to visualize the graph of the elasticity function for a linear demand curve . The problem I face is that the elasticity function graph I came up with looks unfamiliar. I suppose my formula for the elasticity function contains a mistake, but I can't locate it. Here is what I've done ( and I add a Desmos image below). (1) I start with a demand function ( with price as independent variable) : $$D(x)= a-bx$$ . (2) I transform this function into a price function ( with demand as independent variable), in order to obtain the traditonal demand curve ( with demand on the X axis and price on the Y axis) : $$P(x)= - \frac 1b x +\frac ab$$ . (3) I use the calculus version of the elasticity function, namely : $$ \epsilon_{\small P}= \frac {\mathit dD(P)} {\mathit dP} \times \frac {\mathit P} {\mathit D}$$ wich ( so it seems) should yield $$ \Large\epsilon(x) = D'(x) \frac {P(x)}{Q(x)}$$ and finally ( since $D'(x)=-b$ here) $$ \Large\epsilon(x) = -b \frac {P(x)}{Q(x)}$$ . But, as I said above, the graph of my alledged elasticity function looks unfamiliar. In particular, it seems to me that the elasticity should be equal to $1$ for the X-value of the middle point on the demand curve. Desmos ( https://www.desmos.com/calculator/fp7elscgtq )  :","My goal is to visualize the graph of the elasticity function for a linear demand curve . The problem I face is that the elasticity function graph I came up with looks unfamiliar. I suppose my formula for the elasticity function contains a mistake, but I can't locate it. Here is what I've done ( and I add a Desmos image below). (1) I start with a demand function ( with price as independent variable) : . (2) I transform this function into a price function ( with demand as independent variable), in order to obtain the traditonal demand curve ( with demand on the X axis and price on the Y axis) : . (3) I use the calculus version of the elasticity function, namely : wich ( so it seems) should yield and finally ( since here) . But, as I said above, the graph of my alledged elasticity function looks unfamiliar. In particular, it seems to me that the elasticity should be equal to for the X-value of the middle point on the demand curve. Desmos ( https://www.desmos.com/calculator/fp7elscgtq )  :",D(x)= a-bx P(x)= - \frac 1b x +\frac ab  \epsilon_{\small P}= \frac {\mathit dD(P)} {\mathit dP} \times \frac {\mathit P} {\mathit D}  \Large\epsilon(x) = D'(x) \frac {P(x)}{Q(x)} D'(x)=-b  \Large\epsilon(x) = -b \frac {P(x)}{Q(x)} 1,"['calculus', 'derivatives', 'soft-question', 'graphing-functions', 'economics']"
83,Root finding and automatic differentiation,Root finding and automatic differentiation,,"Consider the equation $z = f (z, x)$ . We would like to find $z^{\star}$ for $f$ such that $z^{\star} = f (z^{\star}, x)$ . One way to do this problem is through naive iteration: $z^{(k + 1)} = f (z^{(k)}, x)$ ; stop when $z^{(k + 1)} \approx z^{(k)}$ . A faster way is to arrange the equation as $g (z) = f (z, x) - z$ . This allows us to use Newton's root finding method: $$z^{(k + 1)} = z^{(k)} - \left( \frac{\partial g (z^{(k)})}{\partial z}    \right)^{- 1} \cdot g (z^{(k)})$$ Having found $z^{\star}$ , let's say we would like to find the the derivative of some loss function $l$ with respect to $x$ . This can be done as: $$\frac{\partial l}{\partial x} = \frac{\partial l}{\partial z^{\star}}    \frac{\partial z^{\star}}{\partial x} = \frac{\partial l}{\partial    z^{\star}} \cdot - \left( \frac{\partial g}{\partial z^{\star}} \right)^{-    1} \frac{\partial g}{\partial x}$$ Let's say we would to do this using a software package that implements automatic differentiation like PyTorch or JAX. Automatic differentiation has a foward pass and a backward pass. In the forward pass we simply iterate through $z^{(k + 1)} = z^{(k)} - J (z^{(k)})^{- 1} \cdot g (z^{(k)})$ and save the output of each iteration. In the backward pass, we evaluate the derivative $\frac{\partial l}{\partial x}$ through each of the iterations we had done in the forward pass. We can think of this as unrolling the forward pass and passing $\frac{\partial l}{\partial x}$ from the output to the input. What I've just described is the standard way automatic differentiation is used with the backpropagation algorithm. The problem here is that we need to backpropagate through all the steps we did in the forward phase. This is not only time consuming, but requires us to store the outputs of all the iterations since they are needed in the backward pass. I was reading this tutorial: http://implicit-layers-tutorial.org/implicit_functions/ , where the author says, if we do implicit differentiation, then we don't need to save the intermediate values because the only Jacobian (the term $\frac{\partial g}{\partial z^{\star}}$ ) we need is the Jacobian at the solution point. This is kind of a big deal because it means if you can reformulate your function as an implicit function, then the backward phase of the backpropagation becomes free. My issue is I don't fully understand why we can avoid backpropagating through the solver by defining the function as an implicit function. I would appreciate if someone can enlighten me.","Consider the equation . We would like to find for such that . One way to do this problem is through naive iteration: ; stop when . A faster way is to arrange the equation as . This allows us to use Newton's root finding method: Having found , let's say we would like to find the the derivative of some loss function with respect to . This can be done as: Let's say we would to do this using a software package that implements automatic differentiation like PyTorch or JAX. Automatic differentiation has a foward pass and a backward pass. In the forward pass we simply iterate through and save the output of each iteration. In the backward pass, we evaluate the derivative through each of the iterations we had done in the forward pass. We can think of this as unrolling the forward pass and passing from the output to the input. What I've just described is the standard way automatic differentiation is used with the backpropagation algorithm. The problem here is that we need to backpropagate through all the steps we did in the forward phase. This is not only time consuming, but requires us to store the outputs of all the iterations since they are needed in the backward pass. I was reading this tutorial: http://implicit-layers-tutorial.org/implicit_functions/ , where the author says, if we do implicit differentiation, then we don't need to save the intermediate values because the only Jacobian (the term ) we need is the Jacobian at the solution point. This is kind of a big deal because it means if you can reformulate your function as an implicit function, then the backward phase of the backpropagation becomes free. My issue is I don't fully understand why we can avoid backpropagating through the solver by defining the function as an implicit function. I would appreciate if someone can enlighten me.","z = f (z, x) z^{\star} f z^{\star} = f (z^{\star}, x) z^{(k + 1)} = f (z^{(k)},
x) z^{(k + 1)} \approx z^{(k)} g (z) = f (z, x) - z z^{(k + 1)} = z^{(k)} - \left( \frac{\partial g (z^{(k)})}{\partial z}
   \right)^{- 1} \cdot g (z^{(k)}) z^{\star} l x \frac{\partial l}{\partial x} = \frac{\partial l}{\partial z^{\star}}
   \frac{\partial z^{\star}}{\partial x} = \frac{\partial l}{\partial
   z^{\star}} \cdot - \left( \frac{\partial g}{\partial z^{\star}} \right)^{-
   1} \frac{\partial g}{\partial x} z^{(k + 1)} = z^{(k)} - J (z^{(k)})^{-
1} \cdot g (z^{(k)}) \frac{\partial l}{\partial x} \frac{\partial l}{\partial x} \frac{\partial
g}{\partial z^{\star}}","['derivatives', 'machine-learning', 'newton-raphson', 'backpropagation']"
84,"$y = \sin^{-1}x + (\sin^{-1}x)^2$, find the value of $\frac{d^{2r + 1}y}{dx^{2r + 1}}$ at x = 0",", find the value of  at x = 0",y = \sin^{-1}x + (\sin^{-1}x)^2 \frac{d^{2r + 1}y}{dx^{2r + 1}},Question: If $y = \sin^{-1}x + (\sin^{-1}x)^2$ show that the value of $\frac{d^{2r + 1}y}{dx^{2r + 1}}$ when x = 0 is $\frac{1}{2r}(\frac{(2r)!}{r!})^{2}$ I got $Dy = \frac{1 + 2\sin^{-1}x}{\sqrt{1-x^2}}$ and I know that $D^{2r + 1}y$ is $D^{2r}(Dy)$ but I'm clueless as to how to move further from this point. What am I supposed to do? Please provide a solution if you have idea.,Question: If show that the value of when x = 0 is I got and I know that is but I'm clueless as to how to move further from this point. What am I supposed to do? Please provide a solution if you have idea.,y = \sin^{-1}x + (\sin^{-1}x)^2 \frac{d^{2r + 1}y}{dx^{2r + 1}} \frac{1}{2r}(\frac{(2r)!}{r!})^{2} Dy = \frac{1 + 2\sin^{-1}x}{\sqrt{1-x^2}} D^{2r + 1}y D^{2r}(Dy),"['calculus', 'derivatives']"
85,Derivative of the smallest eigenvalue,Derivative of the smallest eigenvalue,,"Suppose $S_0, S$ are known real symmetric matrices and we have the function $f(x) := \lambda_{0}(S_0+x S)$ where $\lambda_0$ is the smallest eigenvalue. By symmetry, the eigenvalues are real and I believe the smallest eigenvalue is a smooth function of $x$ . How can I find the derivative $\frac{df(x)}{dx}$","Suppose are known real symmetric matrices and we have the function where is the smallest eigenvalue. By symmetry, the eigenvalues are real and I believe the smallest eigenvalue is a smooth function of . How can I find the derivative","S_0, S f(x) := \lambda_{0}(S_0+x S) \lambda_0 x \frac{df(x)}{dx}","['calculus', 'linear-algebra', 'derivatives', 'eigenvalues-eigenvectors']"
86,Solving a 2nd order nonlinear ODE through substitution based linearisation,Solving a 2nd order nonlinear ODE through substitution based linearisation,,"I have the following nonlinear differential equation, $$y y''-(y')^2-yy'(\alpha y-\beta)=0,\qquad (\ast)$$ and I was wondering whether it would be possible to linearise it? Whilst browsing the web, I came across this equation: $$y y''-(y')^2+f(x)yy'+g(x)y^2=0,$$ which I link here, which may be solved through the substitution $u=y'/y$ . They are astoundingly similar and I was therefore curious as to whether my equation $(\ast)$ can also be solved through a similar process of linearisation. I have not made the breakthrough so far and any help would be appreciated.","I have the following nonlinear differential equation, and I was wondering whether it would be possible to linearise it? Whilst browsing the web, I came across this equation: which I link here, which may be solved through the substitution . They are astoundingly similar and I was therefore curious as to whether my equation can also be solved through a similar process of linearisation. I have not made the breakthrough so far and any help would be appreciated.","y y''-(y')^2-yy'(\alpha y-\beta)=0,\qquad (\ast) y y''-(y')^2+f(x)yy'+g(x)y^2=0, u=y'/y (\ast)","['calculus', 'ordinary-differential-equations', 'derivatives']"
87,Tangents to a function and its inverse,Tangents to a function and its inverse,,"I came across the following problem: Find all $x$ values such that the tangent to the function $f(x) = \frac{1}{x^2+1} + (1-2x)^{1/3}$ where $x \ge 0$ at that $x$ value is perpendicular to the tangent of its inverse function $f^{-1}(x)$ at that $x$ value. I know that a formula for the  derivative of the inverse function is $(f^{-1})^{'}(x) = \frac{1}{f'(f^{-1}(x))}$ and we want this expression for $(f^{-1})^{'}(x)$ to be the negative reciprocal of $f'(x)$ for the tangent of the function to be perpendicular to the tangent of its inverse at $x$ . This gives the equation $f'(x) = -f'(f^{-1}(x))$ . However, it seems the only way to solve this equation for $x$ is to find $f^{-1}(x)$ explicitly and I can't seem to find the inverse of $f$ because it is so complicated. I tried the standard swapping $y$ and $x$ and solving for $y$ trick. I even tried Wolfram Alpha but it didn't seem to find an answer in terms of elementary functions. Is there a way to solve this problem without explicitly finding the inverse or, if not, how would one go about finding the inverse of $f(x) = \frac{1}{x^2+1} + (1-2x)^{1/3}$ where $x \ge 0$ ?","I came across the following problem: Find all values such that the tangent to the function where at that value is perpendicular to the tangent of its inverse function at that value. I know that a formula for the  derivative of the inverse function is and we want this expression for to be the negative reciprocal of for the tangent of the function to be perpendicular to the tangent of its inverse at . This gives the equation . However, it seems the only way to solve this equation for is to find explicitly and I can't seem to find the inverse of because it is so complicated. I tried the standard swapping and and solving for trick. I even tried Wolfram Alpha but it didn't seem to find an answer in terms of elementary functions. Is there a way to solve this problem without explicitly finding the inverse or, if not, how would one go about finding the inverse of where ?",x f(x) = \frac{1}{x^2+1} + (1-2x)^{1/3} x \ge 0 x f^{-1}(x) x (f^{-1})^{'}(x) = \frac{1}{f'(f^{-1}(x))} (f^{-1})^{'}(x) f'(x) x f'(x) = -f'(f^{-1}(x)) x f^{-1}(x) f y x y f(x) = \frac{1}{x^2+1} + (1-2x)^{1/3} x \ge 0,"['calculus', 'derivatives', 'inverse-function']"
88,Relation of derivative of $\operatorname{sinc}(z)$ and derivative of $\cos$.,Relation of derivative of  and derivative of .,\operatorname{sinc}(z) \cos,"From this paper, in Eq. (4), it is asserted that (LHS being (3) and RHS being (4)) $$ (-1)^{n} z^{n} \frac{d^{n} \operatorname{sinc}(z)}{(z d z)^{n}} = (-1)^{n+1} z^{n+1} \frac{d^{n+1} \cos (z)}{(z d z)^{n+1}} $$ with $\operatorname{sinc}(z) = \sin(z)/z$ and $n \in \mathbb{N}_0$ and $z\in \mathbb{C}$ . The notation employed in the cited paper is a bit ambiguous but in other literature (or here on SE ), as far as I see it, $$\frac{d^n}{(zdz)^n} f(z) := \left(\frac{1}{z} \frac{d}{d z}\right)^{n}  := \underbrace{ \left(\frac{1}{z} \frac{d}{d z}\right)  \left(\frac{1}{z} \frac{d}{d z}\right) \cdots \left(\frac{1}{z} \frac{d}{d z}\right)}_{\text{n times}} f(z) $$ I can't verify this though. When I substitute $n=1$ to test, I get for the LHS: $-\frac{\operatorname{cos}(z)}{z}+\frac{\operatorname{sin}(z)}{z^{2}}$ but for the RHS $z \left(\frac{\sin (z)}{z^2}-\frac{\cos (z)}{z}\right)$ . Hence it feels like in the equation above, the $z^{n+1}$ on the RHS should actually be just $z$ . Is this an error in the paper or am I missing something?","From this paper, in Eq. (4), it is asserted that (LHS being (3) and RHS being (4)) with and and . The notation employed in the cited paper is a bit ambiguous but in other literature (or here on SE ), as far as I see it, I can't verify this though. When I substitute to test, I get for the LHS: but for the RHS . Hence it feels like in the equation above, the on the RHS should actually be just . Is this an error in the paper or am I missing something?","
(-1)^{n} z^{n} \frac{d^{n} \operatorname{sinc}(z)}{(z d z)^{n}}
=
(-1)^{n+1} z^{n+1} \frac{d^{n+1} \cos (z)}{(z d z)^{n+1}}
 \operatorname{sinc}(z) = \sin(z)/z n \in \mathbb{N}_0 z\in \mathbb{C} \frac{d^n}{(zdz)^n} f(z)
:= \left(\frac{1}{z} \frac{d}{d z}\right)^{n} 
:= \underbrace{
\left(\frac{1}{z} \frac{d}{d z}\right) 
\left(\frac{1}{z} \frac{d}{d z}\right)
\cdots
\left(\frac{1}{z} \frac{d}{d z}\right)}_{\text{n times}}
f(z)
 n=1 -\frac{\operatorname{cos}(z)}{z}+\frac{\operatorname{sin}(z)}{z^{2}} z \left(\frac{\sin (z)}{z^2}-\frac{\cos (z)}{z}\right) z^{n+1} z","['calculus', 'derivatives', 'trigonometry']"
89,Partial Derivative of Sigmoid,Partial Derivative of Sigmoid,,"As it has been stated elsewhere, the derivative of sigmoid is $\sigma$ (x)(1- $\sigma$ (x)). So with that being said I just would like for verification that when taking the partial derivative to a sigmoid function that I am correct in my thinking.  So lets say we had a function f(x) = $\sigma$ ( $w_1$ x + $b_1$ ) and the goal is to take the partial derivative of f(x) with respect to $w_1$ .  We have: $\frac{\partial f}{\partial w_1}$ = $\sigma$ ( $w_1$ x + $b_1$ ) Based off the knowledge of what the derivative of sigmoid is, can we rewrite the problem as? $$\\$$ $\sigma$ ( $w_1$ x + $b_1$ )(1- $\sigma$ ( $w_1$ x + $b_1$ )) $\frac{\partial f}{\partial w_1}($$w_1$ x + $b_1$ ) If so, then proceeding on: $\sigma$ ( $w_1$ x + $b_1$ )(1- $\sigma$ ( $w_1$ x + $b_1$ ))(1*x + 0) Thus the final answer is: $\sigma$ ( $w_1$ x + $b_1$ )(1- $\sigma$ ( $w_1$ x + $b_1$ ))(x) Is this correct thinking?  Thanks!","As it has been stated elsewhere, the derivative of sigmoid is (x)(1- (x)). So with that being said I just would like for verification that when taking the partial derivative to a sigmoid function that I am correct in my thinking.  So lets say we had a function f(x) = ( x + ) and the goal is to take the partial derivative of f(x) with respect to .  We have: = ( x + ) Based off the knowledge of what the derivative of sigmoid is, can we rewrite the problem as? ( x + )(1- ( x + )) x + ) If so, then proceeding on: ( x + )(1- ( x + ))(1*x + 0) Thus the final answer is: ( x + )(1- ( x + ))(x) Is this correct thinking?  Thanks!",\sigma \sigma \sigma w_1 b_1 w_1 \frac{\partial f}{\partial w_1} \sigma w_1 b_1 \\ \sigma w_1 b_1 \sigma w_1 b_1 \frac{\partial f}{\partial w_1}(w_1 b_1 \sigma w_1 b_1 \sigma w_1 b_1 \sigma w_1 b_1 \sigma w_1 b_1,"['derivatives', 'partial-derivative', 'machine-learning']"
90,"Proof explanation: Bounding of $\mathcal{C}^1[a,b]$ functions which vanish at $a,b$ (among other conditions)",Proof explanation: Bounding of  functions which vanish at  (among other conditions),"\mathcal{C}^1[a,b] a,b","Claim: Let $f,g \in \mathcal{C}^1[a,b]$ be such that: $f(a) = f(b) = g(a) = g(b) = 0$ $g(x) > 0$ on $(a,b)$ $g'(a) > 0$ $g'(b) < 0 \newcommand{\o}{\Omega} \newcommand{\oo}{\overline{\Omega}} \newcommand{\pp}{\partial_{\mathbf{n}}} \newcommand{\p}{\partial} \newcommand{\CC}{\mathcal{C}} $ Then $\exists k > 0$ whereby $f(x) \le k \cdot g(x)$ for all $x \in [a,b]$ . Context: This claim arises as Lemma $3.7$ in Mingxin Wang's Nonlinear Second Order Parabolic Equations (ISBN-10: 0367711982). The original claim and proof are made for $n$ -dimensional space; I'm just trying to intuit my way around the one-dimensional case for now. Below, define $\partial_{\mathbf{n}} f$ as follows: $\mathbf{n}$ represents the outward normal vector for $f$ , in the case of $\partial \Omega$ (where $\Omega \subseteq \mathbb{R}^n$ is a bounded domain); then $\partial_{\mathbf{n}} f$ represents the derivative with respect to this vector. The claim and proof from the text go (essentially) as so: Lemma $3.7$ : Let $\o$ be of class $\CC^1$ , let $u,v \in \CC^1(\oo)$ be such that $u,v \equiv 0$ on $\p \o$ $v(x) > 0$ on $\o$ $\pp v < 0$ on $\p \o$ Then $\exists k > 0$ where $u(x) \le k \cdot v(x)$ in $\oo$ . Proof ( quoted verbatim ): Owing to $u,v \in \CC^1(\oo)$ and $\pp v < 0$ on $\p \o$ , there exists $k_1 > 0$ for which $$ \pp u - k_1 \pp v > 0 $$ on $\p \o$ . Noting that $u - k_1 v = 0$ on $\p\o$ , we can find a $\o$ -neighborhood $V$ of $\p\o$ such that $u-k_1 \le 0$ in $V$ . Because $u,v \in \CC^1(\o\setminus V)$ and $v > 0$ in $\o\setminus V$ , there is a positive constant $k_2$ such that $u(x) \le k_2 v(x)$ in $\o\setminus V$ . Take $k := k_1 + k_2$ . Then the desired conclusion holds. A translation to the one-dimensional case would essentially be this: Lemma $3.7$ (in $\mathbb{R}$ ): Let $\o = [a,b] = \oo$ , let $f,g \in \CC^1[a,b]$ be such that $f(x) = g(x) = 0$ for $x \in \{a,b\}$ $g(x) > 0$ on $(a,b)$ $g'(a) > 0$ and $g'(b) < 0$ Then $\exists k > 0$ where $f(x) \le k \cdot g(x)$ in $[a,b]$ . Proof: Owing to $f,g \in \CC^1[a,b]$ and the conditions on $g'$ , there exists $k_1 > 0$ for which: $f'(a) - k_1 g'(a) > 0$ $f'(b) - k_1 g'(b) < 0$ Interjection: Why does such a $k_1$ exist? And are these the correct derivative conditions in the bullets? (I'm not totally comfortable with the outer normal derivative thing.) Proof ( cont. ): Noting that $f(x) - k_1 g(x) = 0$ for $x = a,b$ , we can find an $\o$ -neighborhood $V$ of $\p \o = \{a,b\}$ where $f(x) - k_1 g(x) \le 0$ in $V$ . Interjection: So essentially we may take a ball of radius $\varepsilon$ at each of $x=a,x=b$ , and the text essentially claims that $f(x) - k_1 g(x) \le 0$ in these balls for appropriately small $\varepsilon > 0$ . How is this ensured? Why can we never have a positive value instead? Proof ( cont. ): Because $f,g \in \CC^1([a,b] \setminus V)$ and $g(x) > 0$ in $(a,b) \setminus V$ , then $\exists k_2 >0$ such that $f(x) \le k_2 g(x)$ for all $x \in (a,b)  \setminus V$ . Interjection: How is this $k_2$ 's existence justified? I'm guessing just take $\inf \{ v(x) \mid x \in (a,b)  \setminus V \}$ and scale accordingly, since $v$ is positive? But that seems too easy... Proof ( cont. ): Take $k := k_1 + k_2$ ; then the desired result holds. Can anyone enlighten me on these details?","Claim: Let be such that: on Then whereby for all . Context: This claim arises as Lemma in Mingxin Wang's Nonlinear Second Order Parabolic Equations (ISBN-10: 0367711982). The original claim and proof are made for -dimensional space; I'm just trying to intuit my way around the one-dimensional case for now. Below, define as follows: represents the outward normal vector for , in the case of (where is a bounded domain); then represents the derivative with respect to this vector. The claim and proof from the text go (essentially) as so: Lemma : Let be of class , let be such that on on on Then where in . Proof ( quoted verbatim ): Owing to and on , there exists for which on . Noting that on , we can find a -neighborhood of such that in . Because and in , there is a positive constant such that in . Take . Then the desired conclusion holds. A translation to the one-dimensional case would essentially be this: Lemma (in ): Let , let be such that for on and Then where in . Proof: Owing to and the conditions on , there exists for which: Interjection: Why does such a exist? And are these the correct derivative conditions in the bullets? (I'm not totally comfortable with the outer normal derivative thing.) Proof ( cont. ): Noting that for , we can find an -neighborhood of where in . Interjection: So essentially we may take a ball of radius at each of , and the text essentially claims that in these balls for appropriately small . How is this ensured? Why can we never have a positive value instead? Proof ( cont. ): Because and in , then such that for all . Interjection: How is this 's existence justified? I'm guessing just take and scale accordingly, since is positive? But that seems too easy... Proof ( cont. ): Take ; then the desired result holds. Can anyone enlighten me on these details?","f,g \in \mathcal{C}^1[a,b] f(a) = f(b) = g(a) = g(b) = 0 g(x) > 0 (a,b) g'(a) > 0 g'(b) < 0
\newcommand{\o}{\Omega}
\newcommand{\oo}{\overline{\Omega}}
\newcommand{\pp}{\partial_{\mathbf{n}}}
\newcommand{\p}{\partial}
\newcommand{\CC}{\mathcal{C}}
 \exists k > 0 f(x) \le k \cdot g(x) x \in [a,b] 3.7 n \partial_{\mathbf{n}} f \mathbf{n} f \partial \Omega \Omega \subseteq \mathbb{R}^n \partial_{\mathbf{n}} f 3.7 \o \CC^1 u,v \in \CC^1(\oo) u,v \equiv 0 \p \o v(x) > 0 \o \pp v < 0 \p \o \exists k > 0 u(x) \le k \cdot v(x) \oo u,v \in \CC^1(\oo) \pp v < 0 \p \o k_1 > 0 
\pp u - k_1 \pp v > 0
 \p \o u - k_1 v = 0 \p\o \o V \p\o u-k_1 \le 0 V u,v \in \CC^1(\o\setminus V) v > 0 \o\setminus V k_2 u(x) \le k_2 v(x) \o\setminus V k := k_1 + k_2 3.7 \mathbb{R} \o = [a,b] = \oo f,g \in \CC^1[a,b] f(x) = g(x) = 0 x \in \{a,b\} g(x) > 0 (a,b) g'(a) > 0 g'(b) < 0 \exists k > 0 f(x) \le k \cdot g(x) [a,b] f,g \in \CC^1[a,b] g' k_1 > 0 f'(a) - k_1 g'(a) > 0 f'(b) - k_1 g'(b) < 0 k_1 f(x) - k_1 g(x) = 0 x = a,b \o V \p \o = \{a,b\} f(x) - k_1 g(x) \le 0 V \varepsilon x=a,x=b f(x) - k_1 g(x) \le 0 \varepsilon > 0 f,g \in \CC^1([a,b] \setminus V) g(x) > 0 (a,b) \setminus V \exists k_2 >0 f(x) \le k_2 g(x) x \in (a,b)  \setminus V k_2 \inf \{ v(x) \mid x \in (a,b)  \setminus V \} v k := k_1 + k_2","['real-analysis', 'derivatives', 'continuity', 'proof-explanation', 'upper-lower-bounds']"
91,existence of a bounded $C^\infty$ function,existence of a bounded  function,C^\infty,"Prove that there exists a bounded $C^\infty$ function (i.e. infinitely differentiable) $f:\mathbb{R}\to\mathbb{R}$ so that $\lim\limits_{n\to\infty} f^{(n)}(0) = \infty$ . The function $g:(-1,1)\to \mathbb{R}$ given by $g(x)=1/(1-x)$ has derivatives satisfying $g^{(n)}(0) = n!$ , so we can let $f(x) = g(x)h(x)$ for $|x| \leq \frac{1}2$ and $f(x) = 0$ for $|x| \ge 1/2$ , provided we can find a function $h : \mathbb{R}\to [0,1]$ that is in $C^\infty$ with $h(x) = 1$ for $|x|\leq \frac{1}4$ and $h(x) = 0$ for $|x|\ge \frac{1}2$ . Then $f'(0) = g'(0)h(0) + g(0) h'(0) = g'(0) = 0!$ , and one can show by induction that $f^{(n)}(x) = \displaystyle\sum_{i=0}^n {n\choose i} g^{(n-i)}(x)h^{(i)}(x),$ from which it follows that $f^{(n)}(0) = g(0) = n!$ for each x. How do I find $h$ ? I tried using $\arctan$ but I can't even get a differentiable function (at least on all of $\mathbb{R}$ satisfying the constraints.","Prove that there exists a bounded function (i.e. infinitely differentiable) so that . The function given by has derivatives satisfying , so we can let for and for , provided we can find a function that is in with for and for . Then , and one can show by induction that from which it follows that for each x. How do I find ? I tried using but I can't even get a differentiable function (at least on all of satisfying the constraints.","C^\infty f:\mathbb{R}\to\mathbb{R} \lim\limits_{n\to\infty} f^{(n)}(0) = \infty g:(-1,1)\to \mathbb{R} g(x)=1/(1-x) g^{(n)}(0) = n! f(x) = g(x)h(x) |x| \leq \frac{1}2 f(x) = 0 |x| \ge 1/2 h : \mathbb{R}\to [0,1] C^\infty h(x) = 1 |x|\leq \frac{1}4 h(x) = 0 |x|\ge \frac{1}2 f'(0) = g'(0)h(0) + g(0) h'(0) = g'(0) = 0! f^{(n)}(x) = \displaystyle\sum_{i=0}^n {n\choose i} g^{(n-i)}(x)h^{(i)}(x), f^{(n)}(0) = g(0) = n! h \arctan \mathbb{R}","['real-analysis', 'calculus', 'limits', 'derivatives']"
92,Derivative of matrices involving transpose and inverse,Derivative of matrices involving transpose and inverse,,"I have an equation which looks like this: $$Z = [{A(A^TA + \lambda I)^{-1}A^TB - B}]^T[{A(A^TA + \lambda I)^{-1}A^TB - B}]$$ Here, $\lambda$ is scalar ( $\lambda > 0$ ) and $I$ indenty matrix such that $$\mathbf{\lambda I}=\begin{bmatrix} \lambda & 0\\ 0 & \lambda\\ \end{bmatrix}$$ I want to find $\frac{\partial Z}{\partial \lambda}$ in order to prove that if $\lambda_1 \geq \lambda_2$ then $Z_1 \geq Z_2$ . Here, is what I tried $\frac{\partial Z}{\partial \lambda} = \frac{\partial}{\partial \lambda} [{A(A^TA + \lambda I)^{-1}A^TB - B}]^T[{A(A^TA + \lambda I)^{-1}A^TB - B}]$ $\frac{\partial Z}{\partial \lambda} = 2\frac{\partial}{\partial \lambda} [{A(A^TA + \lambda I)^{-1}A^TB - B}]$ ...( $\frac{\partial X^TX}{\partial X}$ = 2X) $\frac{\partial Z}{\partial \lambda} = 2[\frac{\partial}{\partial \lambda} (A) * [(A^TA + \lambda I)^{-1}A^TB] + A[\frac{\partial}{\partial \lambda} (A^TA + \lambda I)^{-1} * (A^TB) + (A^TA + \lambda I)^{-1} * \frac{\partial}{\partial \lambda}A^TB]  - \frac{\partial}{\partial \lambda}B]$ ...( $\frac{\partial MN}{\partial X}$ = MN' + M'N) $\frac{\partial Z}{\partial \lambda} = 2[0 * [(A^TA + \lambda I)^{-1}A^TB] + A[\frac{\partial}{\partial \lambda} (A^TA + \lambda I)^{-1} * (A^TB) + (A^TA + \lambda I)^{-1} * 0]- 0]$ ...( $\frac{\partial A}{\partial \lambda}$ = 0, $\frac{\partial B}{\partial \lambda}$ = 0, $\frac{\partial A^TB}{\partial \lambda}$ = 0) $\frac{\partial Z}{\partial \lambda} = 2A\frac{\partial}{\partial \lambda} (A^TA + \lambda I)^{-1} * (A^TB)$ $\frac{\partial Z}{\partial \lambda} = -2A(A^TA + \lambda I)^{-1}\frac{\partial}{\partial \lambda} (A^TA + \lambda I) * (A^TA + \lambda I)^{-1}*(A^TB)$ ...( $\frac{\partial M^{-1}}{\partial X} = -M^{-1}M'M^{-1}$ ) $\frac{\partial Z}{\partial \lambda} = -2A(A^TA + \lambda I)^{-1}I(A^TA + \lambda I)^{-1}(A^TB)$ ...( $\frac{\partial (A^TA + \lambda I)}{\partial \lambda} = I$ ) I am unsure of this answer since I expected the derivative to be positive so that the above statement/proof can be proved.","I have an equation which looks like this: Here, is scalar ( ) and indenty matrix such that I want to find in order to prove that if then . Here, is what I tried ...( = 2X) ...( = MN' + M'N) ...( = 0, = 0, = 0) ...( ) ...( ) I am unsure of this answer since I expected the derivative to be positive so that the above statement/proof can be proved.","Z = [{A(A^TA + \lambda I)^{-1}A^TB - B}]^T[{A(A^TA + \lambda I)^{-1}A^TB - B}] \lambda \lambda > 0 I \mathbf{\lambda I}=\begin{bmatrix}
\lambda & 0\\
0 & \lambda\\
\end{bmatrix} \frac{\partial Z}{\partial \lambda} \lambda_1 \geq \lambda_2 Z_1 \geq Z_2 \frac{\partial Z}{\partial \lambda} = \frac{\partial}{\partial \lambda} [{A(A^TA + \lambda I)^{-1}A^TB - B}]^T[{A(A^TA + \lambda I)^{-1}A^TB - B}] \frac{\partial Z}{\partial \lambda} = 2\frac{\partial}{\partial \lambda} [{A(A^TA + \lambda I)^{-1}A^TB - B}] \frac{\partial X^TX}{\partial X} \frac{\partial Z}{\partial \lambda} = 2[\frac{\partial}{\partial \lambda} (A) * [(A^TA + \lambda I)^{-1}A^TB] + A[\frac{\partial}{\partial \lambda} (A^TA + \lambda I)^{-1} * (A^TB) + (A^TA + \lambda I)^{-1} * \frac{\partial}{\partial \lambda}A^TB]
 - \frac{\partial}{\partial \lambda}B] \frac{\partial MN}{\partial X} \frac{\partial Z}{\partial \lambda} = 2[0 * [(A^TA + \lambda I)^{-1}A^TB] + A[\frac{\partial}{\partial \lambda} (A^TA + \lambda I)^{-1} * (A^TB) + (A^TA + \lambda I)^{-1} * 0]- 0] \frac{\partial A}{\partial \lambda} \frac{\partial B}{\partial \lambda} \frac{\partial A^TB}{\partial \lambda} \frac{\partial Z}{\partial \lambda} = 2A\frac{\partial}{\partial \lambda} (A^TA + \lambda I)^{-1} * (A^TB) \frac{\partial Z}{\partial \lambda} = -2A(A^TA + \lambda I)^{-1}\frac{\partial}{\partial \lambda} (A^TA + \lambda I) * (A^TA + \lambda I)^{-1}*(A^TB) \frac{\partial M^{-1}}{\partial X} = -M^{-1}M'M^{-1} \frac{\partial Z}{\partial \lambda} = -2A(A^TA + \lambda I)^{-1}I(A^TA + \lambda I)^{-1}(A^TB) \frac{\partial (A^TA + \lambda I)}{\partial \lambda} = I","['calculus', 'matrices', 'derivatives', 'matrix-calculus']"
93,"Reference request for the space $C^k[a,b]$",Reference request for the space,"C^k[a,b]","I am looking for some notes/references that discuss basic properties of the space $C^k[a,b]$ of $k$ -times differentiable $\mathbb{C}$ -valued functions on an interval $[a,b]$ . I am not very familiar with this space, and the naive questions I would like to be able to answer are: Does $C^k$ here refer to the one-sided derivative? Or does it refer to functions on $[a,b]$ that can be extended to a $C^k$ function on a slightly larger open interval $(a-\varepsilon,b+\varepsilon)$ ? Is $C^k[a,b]$ a Banach $*$ -algebra with respect to the norm $$\|f\|_k=\sum_{i=0}^k\left\|\frac{d^i f}{dx^i}\right\|_\infty?$$ (Again, with derivative suitably interpreted.) Of course, answers to these questions are also welcome.","I am looking for some notes/references that discuss basic properties of the space of -times differentiable -valued functions on an interval . I am not very familiar with this space, and the naive questions I would like to be able to answer are: Does here refer to the one-sided derivative? Or does it refer to functions on that can be extended to a function on a slightly larger open interval ? Is a Banach -algebra with respect to the norm (Again, with derivative suitably interpreted.) Of course, answers to these questions are also welcome.","C^k[a,b] k \mathbb{C} [a,b] C^k [a,b] C^k (a-\varepsilon,b+\varepsilon) C^k[a,b] * \|f\|_k=\sum_{i=0}^k\left\|\frac{d^i f}{dx^i}\right\|_\infty?","['real-analysis', 'derivatives', 'banach-spaces', 'banach-algebras']"
94,"Prove that $f$ is a contraction iff $\exists K\in (0,1):\forall x \in \mathbb R, |f'(x)|\leq K$",Prove that  is a contraction iff,"f \exists K\in (0,1):\forall x \in \mathbb R, |f'(x)|\leq K","In an exercise my teacher asked the following: Let $f:\mathbb R\to \mathbb R $ be a function of class $C^1$ . Prove that $f$ is a contraction iff $\exists K\in (0,1):\forall x \in \mathbb R, |f'(x)|\leq K$ . I was able to prove this but I didn't use anywhere the fact that $f'$ is a continuous function and because of that, I'm starting the question of the validity of my proof. This is what I did: (1. contraction $\implies$ $|f'(x)|\leq K$ ) If $f$ is a contraction, then $\exists K\in (0,1)$ such that: $\forall x,y\in \mathbb R, |f(x)-f(y)|\leq K|x-y|$ , and this is the same as: $$\left|\frac{f(x)-f(y)}{x-y}\right|\leq K$$ Let $x \in \mathbb R$ , then: $$|f'(x)|=\left|\lim _{\alpha\to 0} \frac {f(x+\alpha ) - f(x)}{(x+\alpha)-x}\right|$$ $$ =\lim_{\alpha \to 0}\left| \frac {f(x+\alpha ) - f(x)}{(x+\alpha)-x}\right|\leq K$$ (2. $|f'(x)|\leq K \implies$ contraction) Let $f$ be a function such that $\forall x \in \mathbb R, |f'(x)| \leq K$ with $K \in (0,1)$ . Let $x,y \in \mathbb R$ with $x < y$ . Then, $\exists c \in (x,y)$ such that: $$\left| \frac{f(y)-f(x)}{y-x} \right|=|f'(c)|\leq K$$ So: $$|f(y)-f(x)|\leq K|y-x|$$ In the proof, I never used the fact that the function $f$ has a continuous derivative. Is the proof correct or did I make some kind of mistake that considering that $f'$ is continuous would fix?","In an exercise my teacher asked the following: Let be a function of class . Prove that is a contraction iff . I was able to prove this but I didn't use anywhere the fact that is a continuous function and because of that, I'm starting the question of the validity of my proof. This is what I did: (1. contraction ) If is a contraction, then such that: , and this is the same as: Let , then: (2. contraction) Let be a function such that with . Let with . Then, such that: So: In the proof, I never used the fact that the function has a continuous derivative. Is the proof correct or did I make some kind of mistake that considering that is continuous would fix?","f:\mathbb R\to \mathbb R  C^1 f \exists K\in (0,1):\forall x \in \mathbb R, |f'(x)|\leq K f' \implies |f'(x)|\leq K f \exists K\in (0,1) \forall x,y\in \mathbb R, |f(x)-f(y)|\leq K|x-y| \left|\frac{f(x)-f(y)}{x-y}\right|\leq K x \in \mathbb R |f'(x)|=\left|\lim _{\alpha\to 0} \frac {f(x+\alpha ) - f(x)}{(x+\alpha)-x}\right|  =\lim_{\alpha \to 0}\left| \frac {f(x+\alpha ) - f(x)}{(x+\alpha)-x}\right|\leq K |f'(x)|\leq K \implies f \forall x \in \mathbb R, |f'(x)| \leq K K \in (0,1) x,y \in \mathbb R x < y \exists c \in (x,y) \left| \frac{f(y)-f(x)}{y-x} \right|=|f'(c)|\leq K |f(y)-f(x)|\leq K|y-x| f f'","['real-analysis', 'derivatives', 'solution-verification', 'lipschitz-functions']"
95,Find an equation of the curve that satisfies the condition.,Find an equation of the curve that satisfies the condition.,,"My question is as follows; In the book Anton Calculus 12th edition, Question 56 in section 5.2 has a solution contradicting my own; I will break the reasoning down for my answer to aid a reader in helping me. Or, possibly the book's solution is incorrect. Problem: Find an equation of the curve that satisfies the given. At each point (x,y) on the curve, the slope equals three times the square of the distance between the point and the y-axis; the point (-1,2) is on the curve. Book Solution: \begin{align*} dy/dx=x^2,y&=\int x^2dx = x^3/3+C;\\ y&=2 \textrm{ when } x=-1,\textrm{ so } (-1)^{3}/3+C = 2, C=7/3\\ \textrm{thus } y&=x^{3}/3+7/3 \end{align*} My solution: Here is my first thought for solving this, possibly $\vec{v} = (x,y)$ then we get, $$\frac{dy}{dx} = 3\|\vec{v}||^{2}\implies y =\int3\|\vec{v}||^{2}dx $$ After doing said integration, solve the initial value problem to get $C_{1}$ . The above must be incorrect since this would mean that $\vec{v}$ is a vector from the origin; So I will do my best to break my thought process into an algorithm the reader can understand, which may aid in helping me. What I think I know. The labels (1, 2, ... 7) can be used to aid in replying to my question. (1) F(x) = y (2) ""at each point"", every (x,F(x)). (3) ""on the curve"", the equation F(x). (4) ""the slope"", $\frac{dy}{dx}$ (5) ""equals three times the square,"" three times a square is 3(x^2). (6) ""the distance between the point and the y-axis"", Since it's asserting the distance between the point and the y-axis, I think it's safe to assume what ever vector I come up with will not be going through the origin, so possibly if $P1 = (0,y)$ , and $P2 = (x,y)$ then $\vec{P1P2} = (x,y)-(0,y) = (x,y-y) = (x,0)$ (7) ""the point (-1,2) is on the curve"". This will be used to find our $C_{1}$ Here is my Second thought. Using $\vec{P1P2}$ from (6) \begin{align} \frac{dy}{dx} &= 3||\vec{P1P2}||^2\\ &= 3x^2 \\ \therefore y &= \int3x^2dx = 3\frac{1}{3}x^{3}=x^3+C_{1} \\ \end{align} Finally finding $C_{1}$ (7) \begin{align} y(-1) & = (-1)^3 + C_{1}\\ \implies 2 &= -1 +C_{1} \\ 3 &= C_{1} \end{align} So we now have, $F(x) = y = x^{3} + 3$ . Any help that can be provided is greatly appreciated; thanks in advance.","My question is as follows; In the book Anton Calculus 12th edition, Question 56 in section 5.2 has a solution contradicting my own; I will break the reasoning down for my answer to aid a reader in helping me. Or, possibly the book's solution is incorrect. Problem: Find an equation of the curve that satisfies the given. At each point (x,y) on the curve, the slope equals three times the square of the distance between the point and the y-axis; the point (-1,2) is on the curve. Book Solution: My solution: Here is my first thought for solving this, possibly then we get, After doing said integration, solve the initial value problem to get . The above must be incorrect since this would mean that is a vector from the origin; So I will do my best to break my thought process into an algorithm the reader can understand, which may aid in helping me. What I think I know. The labels (1, 2, ... 7) can be used to aid in replying to my question. (1) F(x) = y (2) ""at each point"", every (x,F(x)). (3) ""on the curve"", the equation F(x). (4) ""the slope"", (5) ""equals three times the square,"" three times a square is 3(x^2). (6) ""the distance between the point and the y-axis"", Since it's asserting the distance between the point and the y-axis, I think it's safe to assume what ever vector I come up with will not be going through the origin, so possibly if , and then (7) ""the point (-1,2) is on the curve"". This will be used to find our Here is my Second thought. Using from (6) Finally finding (7) So we now have, . Any help that can be provided is greatly appreciated; thanks in advance.","\begin{align*}
dy/dx=x^2,y&=\int x^2dx = x^3/3+C;\\
y&=2 \textrm{ when } x=-1,\textrm{ so } (-1)^{3}/3+C = 2, C=7/3\\
\textrm{thus } y&=x^{3}/3+7/3
\end{align*} \vec{v} = (x,y) \frac{dy}{dx} = 3\|\vec{v}||^{2}\implies y =\int3\|\vec{v}||^{2}dx  C_{1} \vec{v} \frac{dy}{dx} P1 = (0,y) P2 = (x,y) \vec{P1P2} = (x,y)-(0,y) = (x,y-y) = (x,0) C_{1} \vec{P1P2} \begin{align}
\frac{dy}{dx} &= 3||\vec{P1P2}||^2\\
&= 3x^2 \\
\therefore y &= \int3x^2dx = 3\frac{1}{3}x^{3}=x^3+C_{1} \\
\end{align} C_{1} \begin{align}
y(-1) & = (-1)^3 + C_{1}\\
\implies 2 &= -1 +C_{1} \\
3 &= C_{1}
\end{align} F(x) = y = x^{3} + 3","['calculus', 'derivatives', 'differential']"
96,Finding the error of $f''(x) \approx \frac{f(x+2h) - 2f(x+h) + f(x)}{h^2}~$,Finding the error of,f''(x) \approx \frac{f(x+2h) - 2f(x+h) + f(x)}{h^2}~,"I'm having some trouble with the following exercise: Deduce the following approximation: $$f''(x) \approx \frac{f(x+2h) - 2f(x+h) + f(x)}{h^2}$$ for small values of $h$ , and find an expression for the error commited when using this approximation. I was able to get to the expression but I don't know how to get an expression for the error. In my classnotes my teacher claimed that the error commited in the approximation: $$f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}$$ is $$E=-\frac {h^2}{12}f^{(4)}(t)$$ for some $t \in (x-h,x+h)$ , but my teacher didn't show us the proof and thus I have no reference to do this problem. How can I find an expression for the error?","I'm having some trouble with the following exercise: Deduce the following approximation: for small values of , and find an expression for the error commited when using this approximation. I was able to get to the expression but I don't know how to get an expression for the error. In my classnotes my teacher claimed that the error commited in the approximation: is for some , but my teacher didn't show us the proof and thus I have no reference to do this problem. How can I find an expression for the error?","f''(x) \approx \frac{f(x+2h) - 2f(x+h) + f(x)}{h^2} h f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2} E=-\frac {h^2}{12}f^{(4)}(t) t \in (x-h,x+h)","['derivatives', 'numerical-methods', 'numerical-calculus']"
97,"Use sandwich theorem to find $\lim_{x \to 0} f(x)$, when $|f(x)-1| \le x^2$","Use sandwich theorem to find , when",\lim_{x \to 0} f(x) |f(x)-1| \le x^2,"My approach: Using the definition of absolute function we know that, $$ 0 \le |f(x)-1| \le x^2$$ Applying $\displaystyle \lim_{x \to 0}$ on lower and upper bound, we get $$ \lim_{x \to 0}\,(0)=0 \qquad\text{ and }\qquad \displaystyle \lim_{x \to 0} \, (x^2)=0$$ So, by sandwich theorem, $$  \lim_{x \to 0} |f(x)-1|=0$$ Case I: $$\lim_{x \to 0} f(x)-1=0$$ $$\implies \displaystyle \lim_{x \to 0} f(x)=1$$ Case II: $$\displaystyle \lim_{x \to 0} 1-f(x)=0$$ $$\implies \displaystyle \lim_{x \to 0} f(x)=1$$ In either case, $\displaystyle \lim_{x \to 0} f(x)=1$ . This was a 5 mark question on a test and my calculus professor gave me 2 marks for this solution. He says that he deducted marks because I wrote $0 \le |f(x)-1| \le x^2$ instead of $-x^2 \le f(x)-1 \le x^2$ and then applying sandwich theorem. How is my approach wrong? Will my approach yield a wrong answer in a different scenario? Please explain.","My approach: Using the definition of absolute function we know that, Applying on lower and upper bound, we get So, by sandwich theorem, Case I: Case II: In either case, . This was a 5 mark question on a test and my calculus professor gave me 2 marks for this solution. He says that he deducted marks because I wrote instead of and then applying sandwich theorem. How is my approach wrong? Will my approach yield a wrong answer in a different scenario? Please explain."," 0 \le |f(x)-1| \le x^2 \displaystyle \lim_{x \to 0}  \lim_{x \to 0}\,(0)=0 \qquad\text{ and }\qquad \displaystyle \lim_{x \to 0} \, (x^2)=0   \lim_{x \to 0} |f(x)-1|=0 \lim_{x \to 0} f(x)-1=0 \implies \displaystyle \lim_{x \to 0} f(x)=1 \displaystyle \lim_{x \to 0} 1-f(x)=0 \implies \displaystyle \lim_{x \to 0} f(x)=1 \displaystyle \lim_{x \to 0} f(x)=1 0 \le |f(x)-1| \le x^2 -x^2 \le f(x)-1 \le x^2","['real-analysis', 'calculus', 'limits', 'derivatives', 'continuity']"
98,"Find Definite Integral $\int_{3}^{4} e^{2x} \,dx $",Find Definite Integral,"\int_{3}^{4} e^{2x} \,dx ","I am having problems with this question and I do not know if I am doing it correctly. Can someone assist me? $$\int_{3}^{4} e^{2x} \,dx $$ $$t = 2x $$ $$dt = 2dx $$ $$\frac {dt}{2} = dx $$ $$\int \frac{1}{2} \cdot e^t + C$$ $$F(b)-F(a) =$$ $$\frac{1}{2}\cdot e^8 - \frac{1}{2}\cdot e^6$$",I am having problems with this question and I do not know if I am doing it correctly. Can someone assist me?,"\int_{3}^{4} e^{2x} \,dx  t = 2x  dt = 2dx  \frac {dt}{2} = dx  \int \frac{1}{2} \cdot e^t + C F(b)-F(a) = \frac{1}{2}\cdot e^8 - \frac{1}{2}\cdot e^6","['integration', 'derivatives']"
99,Complex functions whose derivative is itself,Complex functions whose derivative is itself,,"If $f$ is a real function whose derivative is itself, then it is not difficult to see that it is of the form $f(x)=ce^{x}$ , $c \in \mathbb{R}$ . (If $c=0$ , then $f=0$ ). Indeed, assume that $g(x)$ satisfies $g'(x)=g(x)$ . Take $h(x):=g(x)e^{-x}$ . Then by the Chain Rule we have: $h'(x)=g'(x)e^{-x}-g(x)e^{-x}$ . Since $g(x)=g'(x)$ we obtain that $h'(x)=0$ . Then $h(x)=c$ , for some $c \in \mathbb{R}$ . Therefore, $c=h(x)=g(x)e^{-x}$ , so $g(x)=ce^{x}$ . Does the same proof (with adjustments) hold for complex functions, namely, if $g'(z)=g(z)$ , then $g(z)=ce^{iz}$ , $c \in \mathbb{C}$ ? See this question. Thank you very much!","If is a real function whose derivative is itself, then it is not difficult to see that it is of the form , . (If , then ). Indeed, assume that satisfies . Take . Then by the Chain Rule we have: . Since we obtain that . Then , for some . Therefore, , so . Does the same proof (with adjustments) hold for complex functions, namely, if , then , ? See this question. Thank you very much!",f f(x)=ce^{x} c \in \mathbb{R} c=0 f=0 g(x) g'(x)=g(x) h(x):=g(x)e^{-x} h'(x)=g'(x)e^{-x}-g(x)e^{-x} g(x)=g'(x) h'(x)=0 h(x)=c c \in \mathbb{R} c=h(x)=g(x)e^{-x} g(x)=ce^{x} g'(z)=g(z) g(z)=ce^{iz} c \in \mathbb{C},"['complex-analysis', 'ordinary-differential-equations', 'derivatives']"
