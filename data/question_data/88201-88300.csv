,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Equivalency of all norms on a finite dimensional vector space: compactness theorems vs. the open mapping theorem,Equivalency of all norms on a finite dimensional vector space: compactness theorems vs. the open mapping theorem,,"Going through my functional analysis course notes, I feel like there are two different proofs for the following theorem. In $\mathbb{R}^n$ (or $\mathbb{C}^n$ ), any two norms are equivalent. One uses the compactness-related extreme value theorem (i.e., a continuous function on compact set must achieve its maximum and minimum values), while the other uses the open mapping theorem (i.e, for every continuous linear mapping $T$ from a Banach space $X$ onto another Banach space $Y$ , and every $U \in X$ open, $T(U)$ is open). These two theorems use different hypothesis and are not equivalent. Therefore, I am suspicious that I am doing something wrong. My questions is whether both these proofs are correct, or if I am doing something wrong here . Common steps of both proofs Define (recall) the $\|.\|_\text{sup}$ norm as $\|x\| = \sup_{i} |x_i|$ . (*) Show that for any norm $\|.\|_b$ on $\mathbb{R}^n$ , there exist an $M_b > 0$ , such that for all $x \in \mathbb{R}^n$ , $\|x\|_b \leq M_b \|x\|_\text{sup}$ (see for example here on how to find $M_b$ ). Proof with extreme value theorem From (*) we deduce that any norm $\|x\|_b$ is continuous w.r.t the $\|x\|_\text{sup}$ norm. (+) Use the fact that the unit sphere (of the sup norm) is compact in $\mathbb R^n$ , (*), and the extreme value theorem to deduce that $\|x\|_b$ achieves a minimum $m_b$ on the unit sphere (of the sup norm). In other words, there exists $m_b > 0$ such that $\|x\|_b \geq m_b \|x\|_\text{sup}$ for all $x \in \mathbb R^n$ . Combining (+) and (*), we get that any norm $\|.\|_b$ and $\|.\|_\text{sup}$ are equivalent $\blacksquare$ Proof with the open mapping theorem This is the proof that I am not sure about. From the open mapping theorem, one can prove that (see for example here for a proof): Let $\|.\|_1$ and $\|.\|_2$ be norms on a Banach space $X$ , such that $\|x\|_1 \leq\|x\|_2$ . Then, the norms are equivalent. Now combine this with (*) with the fact that $\mathbb R^n$ is complete (Banach), and you get that any norm in $\mathbb{R}^n$ is equivalent to the $\|.\|_\text{sup}$ norm $\blacksquare$","Going through my functional analysis course notes, I feel like there are two different proofs for the following theorem. In (or ), any two norms are equivalent. One uses the compactness-related extreme value theorem (i.e., a continuous function on compact set must achieve its maximum and minimum values), while the other uses the open mapping theorem (i.e, for every continuous linear mapping from a Banach space onto another Banach space , and every open, is open). These two theorems use different hypothesis and are not equivalent. Therefore, I am suspicious that I am doing something wrong. My questions is whether both these proofs are correct, or if I am doing something wrong here . Common steps of both proofs Define (recall) the norm as . (*) Show that for any norm on , there exist an , such that for all , (see for example here on how to find ). Proof with extreme value theorem From (*) we deduce that any norm is continuous w.r.t the norm. (+) Use the fact that the unit sphere (of the sup norm) is compact in , (*), and the extreme value theorem to deduce that achieves a minimum on the unit sphere (of the sup norm). In other words, there exists such that for all . Combining (+) and (*), we get that any norm and are equivalent Proof with the open mapping theorem This is the proof that I am not sure about. From the open mapping theorem, one can prove that (see for example here for a proof): Let and be norms on a Banach space , such that . Then, the norms are equivalent. Now combine this with (*) with the fact that is complete (Banach), and you get that any norm in is equivalent to the norm",\mathbb{R}^n \mathbb{C}^n T X Y U \in X T(U) \|.\|_\text{sup} \|x\| = \sup_{i} |x_i| \|.\|_b \mathbb{R}^n M_b > 0 x \in \mathbb{R}^n \|x\|_b \leq M_b \|x\|_\text{sup} M_b \|x\|_b \|x\|_\text{sup} \mathbb R^n \|x\|_b m_b m_b > 0 \|x\|_b \geq m_b \|x\|_\text{sup} x \in \mathbb R^n \|.\|_b \|.\|_\text{sup} \blacksquare \|.\|_1 \|.\|_2 X \|x\|_1 \leq\|x\|_2 \mathbb R^n \mathbb{R}^n \|.\|_\text{sup} \blacksquare,"['real-analysis', 'functional-analysis']"
1,How to apply continuous functional calculus,How to apply continuous functional calculus,,"This is the statement I am using. Theorem 2.17, pg 34: Suppose that $A$ is a unital $C^*$ algebra and that $a$ is a normal element in $A$ , then there is a $*$ -isometric isomorphism $$C(\sigma(a)) \rightarrow C^*(\{1_A, a \}) \subseteq A$$ such that $id \mapsto a$ , where $id$ is the identity function on $\Bbb C$ . I want apply this result to a bounded operator $T$ on Hilbert space $H$ . If $T \in B(H)$ is normal, and $\sigma(a)$ is discrete, then the ""spike"" functions, $$\delta_\lambda(x):= \begin{cases} 1 \text{ if } \lambda =x \\  0 \text{ otherwise }  \end{cases} $$ Maps to the projection map $$\delta_\lambda(T)= P_\lambda$$ the projection map on to the eigenspaces. This seems intuitively true. But I don't know how to prove it. The problem may not  be well defined too, since we do not know that the eigenspaces are in fact closed - let us suppose this to be the case.","This is the statement I am using. Theorem 2.17, pg 34: Suppose that is a unital algebra and that is a normal element in , then there is a -isometric isomorphism such that , where is the identity function on . I want apply this result to a bounded operator on Hilbert space . If is normal, and is discrete, then the ""spike"" functions, Maps to the projection map the projection map on to the eigenspaces. This seems intuitively true. But I don't know how to prove it. The problem may not  be well defined too, since we do not know that the eigenspaces are in fact closed - let us suppose this to be the case.","A C^* a A * C(\sigma(a)) \rightarrow C^*(\{1_A, a \}) \subseteq A id \mapsto a id \Bbb C T H T \in B(H) \sigma(a) \delta_\lambda(x):=
\begin{cases}
1 \text{ if } \lambda =x \\ 
0 \text{ otherwise } 
\end{cases}
 \delta_\lambda(T)= P_\lambda","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'operator-algebras', 'functional-calculus']"
2,Is the L2 limit of a polynomial holomorphic?,Is the L2 limit of a polynomial holomorphic?,,"This is a follow-up to my question here . Let $D$ be the unit disk, and for each $n$ let $f_n\in L^2(D)$ be a polynomial in $z=x+iy$ with complex coefficients.  And suppose that $f_n\rightarrow f$ with respect to the $L^2(D)$ norm for some $f\in L^2(D)$ .  My question is, is it necessarily true that $f$ is holomorphic? If not, does anyone know of a counterexample?  I ask because this is true for uniform convergence.","This is a follow-up to my question here . Let be the unit disk, and for each let be a polynomial in with complex coefficients.  And suppose that with respect to the norm for some .  My question is, is it necessarily true that is holomorphic? If not, does anyone know of a counterexample?  I ask because this is true for uniform convergence.",D n f_n\in L^2(D) z=x+iy f_n\rightarrow f L^2(D) f\in L^2(D) f,"['complex-analysis', 'functional-analysis', 'hilbert-spaces', 'lp-spaces']"
3,Conditions on the domain for Sobolev embeddings,Conditions on the domain for Sobolev embeddings,,"I am reading the proof of the Sobolev embedding theorem presented in the book Sobolev Spaces by Robert A. Adams and John J. F. Fournier. I could not understand the proof for part II of the theorem. The purpose of the proof is to embed $W^{1,p}(\Omega)\hookrightarrow C^{0,\lambda}(\Omega)$ where $n<p\leq\infty$ , $0<\lambda\leq1-n/p$ , and $\Omega\subset\mathbb{R}^n$ is a connected open subset that satisfies the strong local Lipschitz condition (page 83, paragraph 4.9): In the actual proof, the authors reduced it to the following lemma: They proved the case in which $\Omega$ is a unit cube, and then proceeds thus: My Question I'm at a total loss. Where is the strong Lipschitz condition used? Why can we take such $P$ and $\delta_0,\delta_1$ ?","I am reading the proof of the Sobolev embedding theorem presented in the book Sobolev Spaces by Robert A. Adams and John J. F. Fournier. I could not understand the proof for part II of the theorem. The purpose of the proof is to embed where , , and is a connected open subset that satisfies the strong local Lipschitz condition (page 83, paragraph 4.9): In the actual proof, the authors reduced it to the following lemma: They proved the case in which is a unit cube, and then proceeds thus: My Question I'm at a total loss. Where is the strong Lipschitz condition used? Why can we take such and ?","W^{1,p}(\Omega)\hookrightarrow C^{0,\lambda}(\Omega) n<p\leq\infty 0<\lambda\leq1-n/p \Omega\subset\mathbb{R}^n \Omega P \delta_0,\delta_1","['functional-analysis', 'analysis', 'partial-differential-equations', 'sobolev-spaces']"
4,"The third dual can be ""decomposed"" with the dual and his annihilator","The third dual can be ""decomposed"" with the dual and his annihilator",,"My teacher says that is an easy exercise to see that $X^{***}= X^* \bigoplus J(X)^\bot$ . That is, $X^*$ is complemented in $X^{***}$ and $J(X)^\bot$ is the topological complement.  Here $J$ is the canonical James map $J:X\to X^{**}$ . ( I've already read other similar questions on this forum, but they did not help me )","My teacher says that is an easy exercise to see that . That is, is complemented in and is the topological complement.  Here is the canonical James map . ( I've already read other similar questions on this forum, but they did not help me )",X^{***}= X^* \bigoplus J(X)^\bot X^* X^{***} J(X)^\bot J J:X\to X^{**},"['functional-analysis', 'banach-spaces', 'normed-spaces']"
5,Projection onto the range of an operator in a nonseparable Hilbert space,Projection onto the range of an operator in a nonseparable Hilbert space,,"Let $A$ be a matrix with linearly-independent columns. Then $A(A^TA)^{-1}A^T$ is the orthogonal projection matrix onto the range/image of $A$ . This formula is legal because the linear independence of $A$ 's columns ensures that $A^TA$ is nonsingular. Now let $A$ be a linear operator in nonseparable Hilbert space $\cal H$ . We do not have an orthonormal basis for $\cal H$ and we cannot necessarily find a matrix representation of $A$ with linearly-independent columns. How do we find the orthogonal projection onto the image of $A$ ? My context is S.J. Bernau's The Square Root of a Positive Self-Adjoint Operator , just before lemma 16 on page 29. Bernau is proving the spectral theorem for unbounded self-adjoint operators. He shows how to construct the square root of any positive self-adjoint operator, and since $A^2$ is positive when $A$ is self-adjoint he defines an absolute value $|A|=\sqrt{A^2}$ from which he constructs the ""positive"" and ""negative"" parts of $A$ as $A^+ = {1\over2}(A+|A|)$ and $A^- = {1\over2}(A-|A|)$ . Then we can decompose $A = A^+ - A^-$ . If we repeat this reasoning with $A-\lambda I$ for arbitrary lambda, we get that $A - \lambda I = (A-\lambda I)^+ - (A-\lambda I)^-$ , and if we keep only the positive part, we get a function that is zero whenever (in the abstract spectral sense) $A\geq \lambda I$ . So the range of $(A-\lambda I)^+$ contains just those vectors that $A$ acts on with eigenvalue less than $\lambda$ (also in the abstract spectral sense), so if we can find a projection onto the range of $(A-\lambda I)^+$ we will have the projection matrix $E_\lambda$ that shows up in most statements of the spectral theorem. All of that reasoning makes sense. The trouble I have is that the paper just defines $E_\lambda$ as the projection onto the range of $(A-\lambda I)^+$ without constructing this projection. And we can't use the spectral theorem to do it because that is what we are trying to prove! So my precise question is: Let $B = (A-\lambda I)^+$ from the above problem.  We know that $B$ is a nonnegative self-adjoint operator (possibly unbounded) on some Hilbert space. How do I define an operator that is a projection onto the image of $B$ if I am not allowed to use the spectral theorem and I cannot just assume that my operator is a finite-dimensional matrix with linearly independent columns? We are free to use any special assumptions that apply to $B$ , even if they don't apply to arbitrary operators. Update: Riesz and Nagy make the same proof in section 108 of Functional Analysis . They just say ""Let $E_\lambda$ be the projection upon ${\frak L}_ \lambda$ "" as though this is something you can assume exists. So I'm adding the ""functional-analysis"" tag on the grounds that this question could be re-asked in the context of trying to understand the proof in a classic functional analysis book. Another couple of updates, since this was solved in the comments: Bernau is actually referring to the null space of $(A-\lambda I)^+$ , not its range. It is hard to tell because $\frak N$ and $\frak R$ look so similar. But my question is asking for the wrong projection operator! Riesz and Nagy have a proof in section 34 of Functional Analysis that any vector can be decomposed into the sum of an element of a subspace and a vector orthogonal to every element of the subspace. (This proof explicitly does not require the separability of the Hilbert space.) Then in section 105 they define the projection operator directly in terms of that decomposition. So this proof justifies the existence of orthogonal projections onto arbitrary subspaces, even for nonseparable spaces. So we at least have a proof that such an orthogonal projection exists. There is no tidy formula (like $P = A(A^TA)^{-1}A^T$ ) for the projection operator; I'm going to have to be happy with a formula-less proof that the operator is well-defined.","Let be a matrix with linearly-independent columns. Then is the orthogonal projection matrix onto the range/image of . This formula is legal because the linear independence of 's columns ensures that is nonsingular. Now let be a linear operator in nonseparable Hilbert space . We do not have an orthonormal basis for and we cannot necessarily find a matrix representation of with linearly-independent columns. How do we find the orthogonal projection onto the image of ? My context is S.J. Bernau's The Square Root of a Positive Self-Adjoint Operator , just before lemma 16 on page 29. Bernau is proving the spectral theorem for unbounded self-adjoint operators. He shows how to construct the square root of any positive self-adjoint operator, and since is positive when is self-adjoint he defines an absolute value from which he constructs the ""positive"" and ""negative"" parts of as and . Then we can decompose . If we repeat this reasoning with for arbitrary lambda, we get that , and if we keep only the positive part, we get a function that is zero whenever (in the abstract spectral sense) . So the range of contains just those vectors that acts on with eigenvalue less than (also in the abstract spectral sense), so if we can find a projection onto the range of we will have the projection matrix that shows up in most statements of the spectral theorem. All of that reasoning makes sense. The trouble I have is that the paper just defines as the projection onto the range of without constructing this projection. And we can't use the spectral theorem to do it because that is what we are trying to prove! So my precise question is: Let from the above problem.  We know that is a nonnegative self-adjoint operator (possibly unbounded) on some Hilbert space. How do I define an operator that is a projection onto the image of if I am not allowed to use the spectral theorem and I cannot just assume that my operator is a finite-dimensional matrix with linearly independent columns? We are free to use any special assumptions that apply to , even if they don't apply to arbitrary operators. Update: Riesz and Nagy make the same proof in section 108 of Functional Analysis . They just say ""Let be the projection upon "" as though this is something you can assume exists. So I'm adding the ""functional-analysis"" tag on the grounds that this question could be re-asked in the context of trying to understand the proof in a classic functional analysis book. Another couple of updates, since this was solved in the comments: Bernau is actually referring to the null space of , not its range. It is hard to tell because and look so similar. But my question is asking for the wrong projection operator! Riesz and Nagy have a proof in section 34 of Functional Analysis that any vector can be decomposed into the sum of an element of a subspace and a vector orthogonal to every element of the subspace. (This proof explicitly does not require the separability of the Hilbert space.) Then in section 105 they define the projection operator directly in terms of that decomposition. So this proof justifies the existence of orthogonal projections onto arbitrary subspaces, even for nonseparable spaces. So we at least have a proof that such an orthogonal projection exists. There is no tidy formula (like ) for the projection operator; I'm going to have to be happy with a formula-less proof that the operator is well-defined.","A A(A^TA)^{-1}A^T A A A^TA A \cal H \cal H A A A^2 A |A|=\sqrt{A^2} A A^+ = {1\over2}(A+|A|) A^- = {1\over2}(A-|A|) A = A^+ - A^- A-\lambda I A - \lambda I = (A-\lambda I)^+ - (A-\lambda I)^- A\geq \lambda I (A-\lambda I)^+ A \lambda (A-\lambda I)^+ E_\lambda E_\lambda (A-\lambda I)^+ B = (A-\lambda I)^+ B B B E_\lambda {\frak L}_
\lambda (A-\lambda I)^+ \frak N \frak R P = A(A^TA)^{-1}A^T","['functional-analysis', 'hilbert-spaces', 'spectral-theory']"
6,Term by term integration of $\int_0^1 \sum_{n=0}^{\infty}a_nx^n g(x)dx$,Term by term integration of,\int_0^1 \sum_{n=0}^{\infty}a_nx^n g(x)dx,"If $\displaystyle \sum_{n=0}^{\infty}a_nx^n$ converges uniformly to $f(x)$ on $[0,1]$ and $g(x)$ is any integrable function. Can we perform term by term integration, in other words, is the following true $$\displaystyle\int_0^1f(x)g(x)dx=\displaystyle\int_0^1 \sum_{n=0}^{\infty}a_nx^n g(x)dx=\displaystyle  \sum_{n=0}^{\infty}a_n\int_0^1x^n g(x)dx$$ I don't have series representation of $g(x)$ . What are the conditions on $f$ and $g$ that ensure such a term by term integration?","If converges uniformly to on and is any integrable function. Can we perform term by term integration, in other words, is the following true I don't have series representation of . What are the conditions on and that ensure such a term by term integration?","\displaystyle \sum_{n=0}^{\infty}a_nx^n f(x) [0,1] g(x) \displaystyle\int_0^1f(x)g(x)dx=\displaystyle\int_0^1 \sum_{n=0}^{\infty}a_nx^n g(x)dx=\displaystyle  \sum_{n=0}^{\infty}a_n\int_0^1x^n g(x)dx g(x) f g","['real-analysis', 'calculus']"
7,Strong and weak limits in a dense set for a sequence of bounded operators,Strong and weak limits in a dense set for a sequence of bounded operators,,"I am working through Lax's ""Functional Analysis"", and I'm trying to prove Theorem 6 of section 15.2, dealing with weak and strong convergence in the operator space. We denote by $s-\lim$ the fact that a sequence converges strongly (in norm), and by $w-\lim$ the weak convergence. Specifically, the theorem states: Let X, U be Banach spaces, $M_n$ a sequence of linear maps in $\textit{L}(X,U)$ uniformly bounded in norm: $\mid M_n\mid\leq c$ for all $n$ . Suppose further that $s-\lim M_nx$ exists for a dense set of $x$ in $X$ . Then, $M_n$ converges strongly, i.e., the $s-\lim$ exists for all $x \in X$ . I need to prove this result, and also an analogous one for weak convergence. My attempt at the strong convergence goes as follows: Let $D\subset X$ be the dense set for which $s-\lim M_nx$ exists. If $x \in X-D$ , since D is dense, there exists a sequence $(x_j)_{j\in \mathbb{N}} \subset D$ such that $x_j \to x$ as $j\to\infty$ . For each $j\in\mathbb{N}$ , let $y_j = s-\lim_{n\to\infty} M_n(x_j)$ . Then, we claim that the sequence $(y_j)_{j\in\mathbb{N}}$ is Cauchy in U. This follows from the fact that $(x_j)_{j\in\mathbb{N}}$ is Cauchy and $(M_n)$ is uniformly bounded in norm: $\mid y_j - y_k \mid = \mid \lim M_n(x_j) - \lim M_n(x_k) \mid = \mid \lim M_n(x_j-x_k) \mid = \lim \mid M_n(x_j-x_k)\mid \leq \liminf \mid M_n \mid \mid x_j - x_k\mid \leq c\mid x_j-x_k\mid$ Hence, since U is Banach, $y_j \to y$ as $j\to\infty$ for some $y\in U$ . Now, we claim that $M_nx \to y$ as $n\to\infty$ . This follows since: $\mid M_nx-y \mid \leq \mid M_nx-M_nx_j\mid + \mid M_nx_j-y_j\mid + \mid y_j-y \mid$ . Since the above is true $\forall j \in \mathbb{N}$ and we have that $x_j \to x$ and $y_j \to y$ , given $\varepsilon>0$ we find an $j_0$ such that $\mid x_j - x \mid < \frac{\varepsilon}{3c}$ and $\mid y_j -y \mid < \frac{\varepsilon}{3}$ for all $j\geq j_0$ . Then, we choose $n$ large enough such that $\mid M_nx_{j_0} - y_{j_0} \mid < \frac{\varepsilon}{3}$ , and the result follows. Firstly, I don't see where we use the completeness of X. Futhermore, I'm not sure this proof generalizes for weak convergence. For weak convergence, the weak limit would exist for a dense set $D\subset X$ . Then, we proceed in the same manner: if $x\in X-D$ , there exists a sequence such that $x_j\to x$ as $j\to\infty$ . Let $l \in U'$ be non-zero. Since the weak limit exists in D, we have that $M_nx_j$ converges weakly to $y_j \in U$ . Hence, $l(M_nx_j)\to l(y_j)$ for some $y_j \in U$ . We claim that $(l(y_j))_j$ is Cauchy in $\mathbb{K}$ . Again, this follows since: $\mid l(y_j) - l(y_k) \mid = \mid \lim_n l(M_nx_j) - \lim_n l(M_nx_k) \mid \leq \liminf \mid l \mid \mid M_n \mid \mid x_j - x_k \mid $ Hence, $l(y_j)\to l(y)$ for a certain $y\in U$ , again, this is because $l$ is surjective. Finally, we claim that $l(M_nx) \to l(y)$ , and the proof is analogous to the case above. However, I don't think that this is enough to conclude that the weak limit $w-\lim M_nx$ exists. This is because our choice of $y$ depends very much on the functional $l$ we chose to start with. Assuming that my proof on the strong limit is right, is there a better way to generalize it to prove the statement about weak limits? If not, how can I approach this problem?","I am working through Lax's ""Functional Analysis"", and I'm trying to prove Theorem 6 of section 15.2, dealing with weak and strong convergence in the operator space. We denote by the fact that a sequence converges strongly (in norm), and by the weak convergence. Specifically, the theorem states: Let X, U be Banach spaces, a sequence of linear maps in uniformly bounded in norm: for all . Suppose further that exists for a dense set of in . Then, converges strongly, i.e., the exists for all . I need to prove this result, and also an analogous one for weak convergence. My attempt at the strong convergence goes as follows: Let be the dense set for which exists. If , since D is dense, there exists a sequence such that as . For each , let . Then, we claim that the sequence is Cauchy in U. This follows from the fact that is Cauchy and is uniformly bounded in norm: Hence, since U is Banach, as for some . Now, we claim that as . This follows since: . Since the above is true and we have that and , given we find an such that and for all . Then, we choose large enough such that , and the result follows. Firstly, I don't see where we use the completeness of X. Futhermore, I'm not sure this proof generalizes for weak convergence. For weak convergence, the weak limit would exist for a dense set . Then, we proceed in the same manner: if , there exists a sequence such that as . Let be non-zero. Since the weak limit exists in D, we have that converges weakly to . Hence, for some . We claim that is Cauchy in . Again, this follows since: Hence, for a certain , again, this is because is surjective. Finally, we claim that , and the proof is analogous to the case above. However, I don't think that this is enough to conclude that the weak limit exists. This is because our choice of depends very much on the functional we chose to start with. Assuming that my proof on the strong limit is right, is there a better way to generalize it to prove the statement about weak limits? If not, how can I approach this problem?","s-\lim w-\lim M_n \textit{L}(X,U) \mid M_n\mid\leq c n s-\lim M_nx x X M_n s-\lim x \in X D\subset X s-\lim M_nx x \in X-D (x_j)_{j\in \mathbb{N}} \subset D x_j \to x j\to\infty j\in\mathbb{N} y_j = s-\lim_{n\to\infty} M_n(x_j) (y_j)_{j\in\mathbb{N}} (x_j)_{j\in\mathbb{N}} (M_n) \mid y_j - y_k \mid = \mid \lim M_n(x_j) - \lim M_n(x_k) \mid = \mid \lim M_n(x_j-x_k) \mid = \lim \mid M_n(x_j-x_k)\mid \leq \liminf \mid M_n \mid \mid x_j - x_k\mid \leq c\mid x_j-x_k\mid y_j \to y j\to\infty y\in U M_nx \to y n\to\infty \mid M_nx-y \mid \leq \mid M_nx-M_nx_j\mid + \mid M_nx_j-y_j\mid + \mid y_j-y \mid \forall j \in \mathbb{N} x_j \to x y_j \to y \varepsilon>0 j_0 \mid x_j - x \mid < \frac{\varepsilon}{3c} \mid y_j -y \mid < \frac{\varepsilon}{3} j\geq j_0 n \mid M_nx_{j_0} - y_{j_0} \mid < \frac{\varepsilon}{3} D\subset X x\in X-D x_j\to x j\to\infty l \in U' M_nx_j y_j \in U l(M_nx_j)\to l(y_j) y_j \in U (l(y_j))_j \mathbb{K} \mid l(y_j) - l(y_k) \mid = \mid \lim_n l(M_nx_j) - \lim_n l(M_nx_k) \mid \leq \liminf \mid l \mid \mid M_n \mid \mid x_j - x_k \mid  l(y_j)\to l(y) y\in U l l(M_nx) \to l(y) w-\lim M_nx y l","['functional-analysis', 'proof-verification', 'weak-convergence']"
8,Minimal conditions to ensure uniqueness of solutions of Schrödinger equation IVP,Minimal conditions to ensure uniqueness of solutions of Schrödinger equation IVP,,"Let us consider the free non-relativistic Schrödinger equation $$i\partial_t \psi =-\frac{1}{2}\partial_x^2 \psi=:H\psi.$$ Adapting Fritz John's pathological solution to the heat equation , I find that the non-zero smooth function $$\varphi:\mathbb{R}^2 \to \mathbb{C}:(x,t) \mapsto \sum_{n=0}^\infty f^{(n)}(t)\frac{x^{2n}(-2i)^n}{(2n)!}, \qquad f(t)\equiv e^{-1/t^2}$$ solves the free Schrödinger equation while reducing identically to zero as $t\to 0$ . This establishes that the Schrödinger equation, regarded as a PDE at face value, never offers a unique solution to an initial value problem. Traditionally, we add in the constraint that the solution of the Schrödinger equation ought to maneuver inside $L^2(\mathbb{R})$ in order to make the Born rule operable. However, usual treatments also add in strong-continuity-like ingredients so that we can finally handle the Schrödinger equation with a cosy and standard functional-analytic framework. However, the physical interpretation and requirement of these continuity ingredients is a bit obscure to me, certainly so since they are to a certain degree non-local (e.g. in the semigroup context , it is demanded that there is a dense core of ""classical solutions"" $t \mapsto \psi(t)$ characterized by $\forall t \in \mathbb{R}: H\psi(t) \in L^2(\mathbb{R})$ , which indeed has the flavour of a non-local weight condition). Q: Does Born's integrability condition $\psi(t)\in L^2(\mathbb{R})$ suffice to select unique solutions for the Schrödinger-equation-related IVP (or do we really need the additional $\partial_x^2\psi(t)\in L^2(\mathbb{R})$ or similar strong-continuity requirements)? EDIT(18/02/19): One is of course tempted to use $\psi \in L^2(\mathbb{R})$ to our advantage by allowing us to use the Fourier transform in the direction of $x$ : the Schrödinger equation then reads $i\partial_t \hat{\psi} = p^2 \hat{\psi}$ from where uniqueness seems easy to obtain. I'm unsure though what to say about the necessary ""differentiations under the integral sign"" and partial differentiations that are required along this line of thinking.","Let us consider the free non-relativistic Schrödinger equation Adapting Fritz John's pathological solution to the heat equation , I find that the non-zero smooth function solves the free Schrödinger equation while reducing identically to zero as . This establishes that the Schrödinger equation, regarded as a PDE at face value, never offers a unique solution to an initial value problem. Traditionally, we add in the constraint that the solution of the Schrödinger equation ought to maneuver inside in order to make the Born rule operable. However, usual treatments also add in strong-continuity-like ingredients so that we can finally handle the Schrödinger equation with a cosy and standard functional-analytic framework. However, the physical interpretation and requirement of these continuity ingredients is a bit obscure to me, certainly so since they are to a certain degree non-local (e.g. in the semigroup context , it is demanded that there is a dense core of ""classical solutions"" characterized by , which indeed has the flavour of a non-local weight condition). Q: Does Born's integrability condition suffice to select unique solutions for the Schrödinger-equation-related IVP (or do we really need the additional or similar strong-continuity requirements)? EDIT(18/02/19): One is of course tempted to use to our advantage by allowing us to use the Fourier transform in the direction of : the Schrödinger equation then reads from where uniqueness seems easy to obtain. I'm unsure though what to say about the necessary ""differentiations under the integral sign"" and partial differentiations that are required along this line of thinking.","i\partial_t \psi =-\frac{1}{2}\partial_x^2 \psi=:H\psi. \varphi:\mathbb{R}^2 \to \mathbb{C}:(x,t) \mapsto \sum_{n=0}^\infty f^{(n)}(t)\frac{x^{2n}(-2i)^n}{(2n)!}, \qquad f(t)\equiv e^{-1/t^2} t\to 0 L^2(\mathbb{R}) t \mapsto \psi(t) \forall t \in \mathbb{R}: H\psi(t) \in L^2(\mathbb{R}) \psi(t)\in L^2(\mathbb{R}) \partial_x^2\psi(t)\in L^2(\mathbb{R}) \psi \in L^2(\mathbb{R}) x i\partial_t \hat{\psi} = p^2 \hat{\psi}","['functional-analysis', 'reference-request', 'partial-differential-equations', 'mathematical-physics']"
9,Showing weakly continuous operators are continuous without using weak topology,Showing weakly continuous operators are continuous without using weak topology,,"Let $X$ and $Y$ be Banach spaces, and let $T:X\rightarrow Y$ be a linear map such that $f\circ T$ is continuous for all $f\in Y'$ .  Show that $T$ is continuous. Now I think this problem is trivial once you have the notion of weak topology.  But without that notion, I'm not sure how to approach this.  I tried using the inequalities $|(f\circ T)(x)|\leq(||f\circ T||)(||x||)$ and $|(f\circ T)(x)|\leq(||f|||)(||Tx||)$ , but I don't see how that gets us any closer to find what $||Tx||$ is less than or equal to.  Maybe a judicious choice of $f$ would help?","Let and be Banach spaces, and let be a linear map such that is continuous for all .  Show that is continuous. Now I think this problem is trivial once you have the notion of weak topology.  But without that notion, I'm not sure how to approach this.  I tried using the inequalities and , but I don't see how that gets us any closer to find what is less than or equal to.  Maybe a judicious choice of would help?",X Y T:X\rightarrow Y f\circ T f\in Y' T |(f\circ T)(x)|\leq(||f\circ T||)(||x||) |(f\circ T)(x)|\leq(||f|||)(||Tx||) ||Tx|| f,"['functional-analysis', 'operator-theory', 'banach-spaces', 'normed-spaces', 'weak-convergence']"
10,Are neural networks with bounded parameters a compact subset of the Banach space of continuous functions?,Are neural networks with bounded parameters a compact subset of the Banach space of continuous functions?,,"Let $d, n \in \mathbb{N}$ . Moreover, let $D \subset \mathbb{R}^d$ be compact and denote with $\mathcal{C}(D, \mathbb{R}^n) $ the set of continuous functions from $D$ to $\mathbb{R}^n$ . Then $\mathcal{C}(D, \mathbb{R}^n) $ is a Banach space with the usual maximum norm $\lVert f\rVert_{\infty} := \max_{x \in D} \lVert f(x) \rVert $ . Now let $\sigma : \mathbb{R} \rightarrow \mathbb{R}$ be a continuous (activation) function, $K>0$ and $\Gamma$ be a set of fixed hyperparameters for an artificial neural network such that the input layer has $d$ nodes and the output layer has $n$ nodes. Denote with $\mathcal{H}_{\sigma, \Gamma, K} \subset \mathcal{C}(D, \mathbb{R}^n)$ the set of all neural networks with activation function $\sigma$ and fixed architecture $\Gamma$ for which all $l \in \mathbb{N}$ parameters $\theta_1,...,\theta_l$ (the biases and edgeweights) are smaller or equal than $K$ in absolute value, i.e. $\lvert \theta_i \rvert \leq K$ for all parameters $\theta_i$ of the neural network. I want to rigorously show, that $\mathcal{H}_{\sigma, \Gamma, K}$ forms a compact topological subspace of the Banach space $(\mathcal{C}(D, \mathbb{R}^n), \lVert . \rVert_{\infty}) $ . One idea I have is to show that the mapping $j:[-K,K]^l \rightarrow \mathcal{H}_{\sigma, \Gamma, K}$ which maps a set of parameters $\Theta$ to a neural network $\Phi_{\Theta}$ is continuous. Since $[-K,K]^l$ is compact, this would imply compactness of $j([-K,K]^l) = \mathcal{H}_{\sigma, \Gamma, K}$ . So far, however, I have not been successful. I am grateful for any advice! Kind regards and thank you very much, Joker","Let . Moreover, let be compact and denote with the set of continuous functions from to . Then is a Banach space with the usual maximum norm . Now let be a continuous (activation) function, and be a set of fixed hyperparameters for an artificial neural network such that the input layer has nodes and the output layer has nodes. Denote with the set of all neural networks with activation function and fixed architecture for which all parameters (the biases and edgeweights) are smaller or equal than in absolute value, i.e. for all parameters of the neural network. I want to rigorously show, that forms a compact topological subspace of the Banach space . One idea I have is to show that the mapping which maps a set of parameters to a neural network is continuous. Since is compact, this would imply compactness of . So far, however, I have not been successful. I am grateful for any advice! Kind regards and thank you very much, Joker","d, n \in \mathbb{N} D \subset \mathbb{R}^d \mathcal{C}(D, \mathbb{R}^n)  D \mathbb{R}^n \mathcal{C}(D, \mathbb{R}^n)  \lVert f\rVert_{\infty} := \max_{x \in D} \lVert f(x) \rVert  \sigma : \mathbb{R} \rightarrow \mathbb{R} K>0 \Gamma d n \mathcal{H}_{\sigma, \Gamma, K} \subset \mathcal{C}(D, \mathbb{R}^n) \sigma \Gamma l \in \mathbb{N} \theta_1,...,\theta_l K \lvert \theta_i \rvert \leq K \theta_i \mathcal{H}_{\sigma, \Gamma, K} (\mathcal{C}(D, \mathbb{R}^n), \lVert . \rVert_{\infty})  j:[-K,K]^l \rightarrow \mathcal{H}_{\sigma, \Gamma, K} \Theta \Phi_{\Theta} [-K,K]^l j([-K,K]^l) = \mathcal{H}_{\sigma, \Gamma, K}","['functional-analysis', 'banach-spaces', 'compactness', 'neural-networks', 'artificial-intelligence']"
11,Proving that a subset of $l^{2}$ is compact,Proving that a subset of  is compact,l^{2},"Define $B \subset \ell^{2}$ by $$B = \{x \in \ell^{2}: \sum_{n=1}^{\infty}n|x_{n}|^{2} \leq 1 \}. $$ Show that $B$ is compact. I found this question while studying for an exam. I tried proving that $B$ was sequentially compact by taking an arbitrary sequence $\{ x^{(k)} \}_{k \geq 1} \in \ell^{2}$ , and using a diagonalization argument to extract a subsequence $\{ x^{k(j)} \}_{j \geq 1}$ with the property that $$\lim_{j \to \infty}x_{n}^{k(j)} = x_{n} \in \left[-\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}} \right],$$ for each $n \in \mathbb{N}$ . I proved that $x = (x_1,\dots,x_{n},\dots) \in B$ , but can't find a way to prove that my original sequence converges to $x$ in the $\ell^{2}$ norm. Is this approach correct so far, or would it be easier to show that $B$ is complete and totally bounded?","Define by Show that is compact. I found this question while studying for an exam. I tried proving that was sequentially compact by taking an arbitrary sequence , and using a diagonalization argument to extract a subsequence with the property that for each . I proved that , but can't find a way to prove that my original sequence converges to in the norm. Is this approach correct so far, or would it be easier to show that is complete and totally bounded?","B \subset \ell^{2} B = \{x \in \ell^{2}: \sum_{n=1}^{\infty}n|x_{n}|^{2} \leq 1 \}.  B B \{ x^{(k)} \}_{k \geq 1} \in \ell^{2} \{ x^{k(j)} \}_{j \geq 1} \lim_{j \to \infty}x_{n}^{k(j)} = x_{n} \in \left[-\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}} \right], n \in \mathbb{N} x = (x_1,\dots,x_{n},\dots) \in B x \ell^{2} B","['real-analysis', 'sequences-and-series', 'functional-analysis', 'compactness']"
12,"If for some sequence $a_n\to \infty$ the limit $\lim_{n\to\infty} f(a_nx)$ exists for all $x\in\mathbb R$, then $\lim_{x\to\infty} f(x)$ exists","If for some sequence  the limit  exists for all , then  exists",a_n\to \infty \lim_{n\to\infty} f(a_nx) x\in\mathbb R \lim_{x\to\infty} f(x),"A little bit of context. I was given a problem which went like if $X_n$ is normally distributed with mean $a_n$ and is converging in distribution to $X$ , then $a_n\to a$ for some $a\in\mathbb R$ and $X$ is normally distributed. The question is quite doable using characteristic functions, I guessed, until I ended up with the problem if both limits $$\lim_{n\to\infty}\cos(a_nt) \ \ \ \text{ and } \lim_{n\to\infty} \sin(a_nt)$$ for all $t\in\mathbb R$ then the limit $$\lim_{n\to\infty} a_n$$ exists as well and is finite. If I assume $a_n$ is bounded, then I don't have any problem proving the statement. But if $a_n$ is unbounded, then there is, without loss of generality, a subsequence $a_{n_k}\to\infty$ . Therefore my question which is a bit general: Question. Let $f:\mathbb R\to\mathbb R$ be a continuous function and $a_n$ be a sequence diverging to $\infty$ , if $$\lim_{n\to\infty} f(a_nt)$$ exists for all $t\in\mathbb R$ , do we have $\lim_{x\to\infty} f(x)$ exists as well? The result is well-known if $a_n=n$ . If we have proven this statement then my original claim is easy to prove.  I attempted to mimic the proof in this post , but I stopped where they say $(f(nmx))_{n\in\mathbb N}$ is a subsequence of $(f(nx))_{n\in\mathbb N}$ since in my particular case I don't have $(f(a_na_mx))_{n\in\mathbb N}$ is a subsequence of $(f(a_nx))_{n\in\mathbb N}$ . However deep inside, I believe there is a possibility for mimicing.","A little bit of context. I was given a problem which went like if is normally distributed with mean and is converging in distribution to , then for some and is normally distributed. The question is quite doable using characteristic functions, I guessed, until I ended up with the problem if both limits for all then the limit exists as well and is finite. If I assume is bounded, then I don't have any problem proving the statement. But if is unbounded, then there is, without loss of generality, a subsequence . Therefore my question which is a bit general: Question. Let be a continuous function and be a sequence diverging to , if exists for all , do we have exists as well? The result is well-known if . If we have proven this statement then my original claim is easy to prove.  I attempted to mimic the proof in this post , but I stopped where they say is a subsequence of since in my particular case I don't have is a subsequence of . However deep inside, I believe there is a possibility for mimicing.",X_n a_n X a_n\to a a\in\mathbb R X \lim_{n\to\infty}\cos(a_nt) \ \ \ \text{ and } \lim_{n\to\infty} \sin(a_nt) t\in\mathbb R \lim_{n\to\infty} a_n a_n a_n a_{n_k}\to\infty f:\mathbb R\to\mathbb R a_n \infty \lim_{n\to\infty} f(a_nt) t\in\mathbb R \lim_{x\to\infty} f(x) a_n=n (f(nmx))_{n\in\mathbb N} (f(nx))_{n\in\mathbb N} (f(a_na_mx))_{n\in\mathbb N} (f(a_nx))_{n\in\mathbb N},"['real-analysis', 'functional-analysis', 'analysis', 'baire-category']"
13,"The spaces $\ell^p, \; 1 \leq p < + \infty$ are separable. On the other side, $\ell^\infty$ is not.","The spaces  are separable. On the other side,  is not.","\ell^p, \; 1 \leq p < + \infty \ell^\infty","Exercise : Show that the spaces $\ell^p, \; 1 \leq p < + \infty$ are separable. Attempt : In order to show that $\ell^p$ is separable for $\ell^p, \; 1 \leq p < + \infty$ , we need to work over a set $D$ proving that it is countable and dense over $\ell^p$ , while showing that for $x \in \ell^P$ and $y \in D$ , it is $\|x-y\|<\varepsilon.$ Since $x \in \ell^p$ , where $x=(x_n)_{n}$ , this obviously means that : $$\sum_{n=1}^\infty |x_n|^p <\infty \implies \left( \sum_{n=1}^\infty |x_n|^p \right)^{1/p} < \infty \implies \left( \sum_{n=1}^\infty |x_n|^p \right)^{1/p}<\varepsilon$$ $$\Leftrightarrow$$ $$\sum_{n=1}^\infty |x_n|^p < \varepsilon^p, \quad \text{for some } \varepsilon >0$$ But that would mean that $\exists k \in \mathbb N$ , such that : $$\sum_{n=k+1}^\infty |x_n|^p < \left(\frac{\varepsilon}{2}\right)^p$$ Now, if I let $y=(y_n)_n$ be the sequence $$y_n = \begin{cases} x_n& n \leq k \\ 0& n> k \end{cases}$$ then, one can easily see that : $$\left\| x-y\right\|_p = \left( \sum_{n=k+1}^\infty |x_n|^p \right)^{1/p}<\frac{\varepsilon}{2}$$ Now, let $D_n$ be the set : $$D_n = \left\{\sum_{i=1}^n q_ie_i : q_i \in \mathbb Q \right\}$$ This set is countable as $D_n$ can be correlated to $\mathbb Q^n$ . Now, the union $$\bigcup_{n=1}^\infty D_n$$ is also countable as a union of countable sets. Now, it is : $$\|x-z\|_p \leq \|x-y\|_p + \|y-z\|_p < \varepsilon/2 + \varepsilon/2 \equiv \varepsilon$$ This means that $D = \bigcup_{n=1}^\infty D_n$ is countable and dense over $\ell^p$ which means that $\ell^p$ is separable. Question : How does one show that $\ell^\infty$ is not a separable space ?","Exercise : Show that the spaces are separable. Attempt : In order to show that is separable for , we need to work over a set proving that it is countable and dense over , while showing that for and , it is Since , where , this obviously means that : But that would mean that , such that : Now, if I let be the sequence then, one can easily see that : Now, let be the set : This set is countable as can be correlated to . Now, the union is also countable as a union of countable sets. Now, it is : This means that is countable and dense over which means that is separable. Question : How does one show that is not a separable space ?","\ell^p, \; 1 \leq p < + \infty \ell^p \ell^p, \; 1 \leq p < + \infty D \ell^p x \in \ell^P y \in D \|x-y\|<\varepsilon. x \in \ell^p x=(x_n)_{n} \sum_{n=1}^\infty |x_n|^p <\infty \implies \left( \sum_{n=1}^\infty |x_n|^p \right)^{1/p} < \infty \implies \left( \sum_{n=1}^\infty |x_n|^p \right)^{1/p}<\varepsilon \Leftrightarrow \sum_{n=1}^\infty |x_n|^p < \varepsilon^p, \quad \text{for some } \varepsilon >0 \exists k \in \mathbb N \sum_{n=k+1}^\infty |x_n|^p < \left(\frac{\varepsilon}{2}\right)^p y=(y_n)_n y_n = \begin{cases} x_n& n \leq k \\ 0& n> k \end{cases} \left\| x-y\right\|_p = \left( \sum_{n=k+1}^\infty |x_n|^p \right)^{1/p}<\frac{\varepsilon}{2} D_n D_n = \left\{\sum_{i=1}^n q_ie_i : q_i \in \mathbb Q \right\} D_n \mathbb Q^n \bigcup_{n=1}^\infty D_n \|x-z\|_p \leq \|x-y\|_p + \|y-z\|_p < \varepsilon/2 + \varepsilon/2 \equiv \varepsilon D = \bigcup_{n=1}^\infty D_n \ell^p \ell^p \ell^\infty","['real-analysis', 'functional-analysis', 'banach-spaces', 'normed-spaces', 'separable-spaces']"
14,Dense sets in $L_{\infty}$,Dense sets in,L_{\infty},"I know that for every probability measure $\mu$ on doubling metric space $X$ ,  the set of lipschitz functions (and therefore continuous functions) is dense in $L_1(\mu)$ . Is it true for $L_{\infty}$ as well? i.e. for every $\epsilon >0$ and a bounded function $g$ there is a continuous $f$ s.t. for all $x \in X$ we have $|f(x) - g(x)| <\epsilon $ (I assume $f:X \to \mathbb{R}$ ) If it is not true, is it correct with additional constraints on $X$ ?","I know that for every probability measure on doubling metric space ,  the set of lipschitz functions (and therefore continuous functions) is dense in . Is it true for as well? i.e. for every and a bounded function there is a continuous s.t. for all we have (I assume ) If it is not true, is it correct with additional constraints on ?",\mu X L_1(\mu) L_{\infty} \epsilon >0 g f x \in X |f(x) - g(x)| <\epsilon  f:X \to \mathbb{R} X,"['functional-analysis', 'measure-theory', 'metric-spaces']"
15,Power Series of an Operator,Power Series of an Operator,,"I'm working through some functional analysis problems and  am having trouble with the following. Let $f(z)=\sum_{n=0}^\infty a_n z^n$ be a power series with radius of convergence $R>0$ . Let $A\in\mathcal{B}(\mathcal{H})$ with $||A||<R.$ Show the following: (a) There is a $T\in\mathcal{B}(\mathcal{H})$ with $\langle Tx, y\rangle=\sum_{n=0}^\infty a_n\langle A^nx,y\rangle$ for $x,y\in\mathcal{H}.$ Here we denote $T$ by $f(A)$ . (b) That the partial sums of $f(A)$ converge to $T$ in the operator norm. (c) That $f(A)B=bf(A)$ whenever $B\in\mathcal{B}(\mathcal{H})$ with $BA=AB.$ (d) For $f(z)=e^z$ for $A$ self-adjoint we have $f(iA)$ is unitary. Solution attempts: (a) I think $T = \sum_{n=0}^\infty a_n A^n$ ,  but am  having trouble showing this is within $\mathcal{B}(\mathcal{H})$ . In particular, I have for $h\in\mathcal{H}$ with $||h||=1,$ $|| \sum_{n=0}^\infty a_nA^n h||\leq |a_0|||h||+|a_1|||Ah||+|a_2|||A^2h||+\cdots$ My thought was to continue this chain of inequalites until I got a series that converged, but since I am taking the absolute values of the coefficients I don't think that this will work. (b) I think I could get this part if I saw how part (a) is done. (c) $$     f(A)B = (\sum a_n A^n ) B = (a_0+a_1A+a_2A^2+\cdots)B\\     =a_0B+a_1AB+a_2AAB+\cdots = Ba_0+Ba_1A+Ba_2AA+\cdots \\     =B(\sum a_n A^n) = Bf(A)     $$ (d) $f(iA)=\sum \frac{1}{n!} (iA)^n\Rightarrow f(iA)^*= (\sum \frac{1}{n!}(iA)^n)^*=\sum \frac{1}{n!}(i^n)^*(A^n)^*=\sum \frac{1}{n!}(i^*)^n(A^*)^n=\sum \frac{1}{n!}(-i)^nA^n =f(-iA)$ . Since $A$ commutes with itself, we have $f(iA)f(-iA)=f(-iA)f(iA)=e^{iA}e^{-iA}=e^{0}=I$","I'm working through some functional analysis problems and  am having trouble with the following. Let be a power series with radius of convergence . Let with Show the following: (a) There is a with for Here we denote by . (b) That the partial sums of converge to in the operator norm. (c) That whenever with (d) For for self-adjoint we have is unitary. Solution attempts: (a) I think ,  but am  having trouble showing this is within . In particular, I have for with My thought was to continue this chain of inequalites until I got a series that converged, but since I am taking the absolute values of the coefficients I don't think that this will work. (b) I think I could get this part if I saw how part (a) is done. (c) (d) . Since commutes with itself, we have","f(z)=\sum_{n=0}^\infty a_n z^n R>0 A\in\mathcal{B}(\mathcal{H}) ||A||<R. T\in\mathcal{B}(\mathcal{H}) \langle Tx, y\rangle=\sum_{n=0}^\infty a_n\langle A^nx,y\rangle x,y\in\mathcal{H}. T f(A) f(A) T f(A)B=bf(A) B\in\mathcal{B}(\mathcal{H}) BA=AB. f(z)=e^z A f(iA) T = \sum_{n=0}^\infty a_n A^n \mathcal{B}(\mathcal{H}) h\in\mathcal{H} ||h||=1, || \sum_{n=0}^\infty a_nA^n h||\leq |a_0|||h||+|a_1|||Ah||+|a_2|||A^2h||+\cdots 
    f(A)B = (\sum a_n A^n ) B = (a_0+a_1A+a_2A^2+\cdots)B\\
    =a_0B+a_1AB+a_2AAB+\cdots = Ba_0+Ba_1A+Ba_2AA+\cdots \\
    =B(\sum a_n A^n) = Bf(A)
     f(iA)=\sum \frac{1}{n!} (iA)^n\Rightarrow f(iA)^*= (\sum \frac{1}{n!}(iA)^n)^*=\sum \frac{1}{n!}(i^n)^*(A^n)^*=\sum \frac{1}{n!}(i^*)^n(A^*)^n=\sum \frac{1}{n!}(-i)^nA^n =f(-iA) A f(iA)f(-iA)=f(-iA)f(iA)=e^{iA}e^{-iA}=e^{0}=I","['functional-analysis', 'power-series', 'matrix-exponential']"
16,"Find all solutions $T$ of $x^{2006} T = 0$ in the space of tempered distributions, $\mathcal{S}'(\mathbb{R})$","Find all solutions  of  in the space of tempered distributions,",T x^{2006} T = 0 \mathcal{S}'(\mathbb{R}),"I'm modeling my solution after this answer to a similar question . This is as far as I've gotten: Every $\phi \in \mathcal{S}(\mathbb{R})$ that vanishes at $0$ can be expressed as $\phi(x) = x \psi (x)$ . Then, $T\phi = xT(\psi) = 0$ by assumption. Fix $\chi \in \mathcal{S}(\mathbb{R})$ such that $\chi(0) = 1$ . Let $T\chi = a$ . Then, for any $\phi \in \mathcal{S}(\mathbb{R})$ , $$T\phi = T(\phi - \phi(0) \chi + \phi(0) \chi) = T(\phi - \phi(0) \chi) + T(\phi(0) \chi).$$ This is where I've gotten stuf. I'm not sure how, in the linked solution, the answer reduces from $T(\phi - \phi(0) \chi) + T(\phi(0) \chi)$ to $0 + a \phi(0)$ (my primary confusion is $T(\phi - \phi(0) \chi)  = 0$ ) nor how to adapt that for my own question. Any suggestions?","I'm modeling my solution after this answer to a similar question . This is as far as I've gotten: Every that vanishes at can be expressed as . Then, by assumption. Fix such that . Let . Then, for any , This is where I've gotten stuf. I'm not sure how, in the linked solution, the answer reduces from to (my primary confusion is ) nor how to adapt that for my own question. Any suggestions?",\phi \in \mathcal{S}(\mathbb{R}) 0 \phi(x) = x \psi (x) T\phi = xT(\psi) = 0 \chi \in \mathcal{S}(\mathbb{R}) \chi(0) = 1 T\chi = a \phi \in \mathcal{S}(\mathbb{R}) T\phi = T(\phi - \phi(0) \chi + \phi(0) \chi) = T(\phi - \phi(0) \chi) + T(\phi(0) \chi). T(\phi - \phi(0) \chi) + T(\phi(0) \chi) 0 + a \phi(0) T(\phi - \phi(0) \chi)  = 0,"['functional-analysis', 'distribution-theory', 'schwartz-space']"
17,Commutative Banach algebras and maximum ideal space,Commutative Banach algebras and maximum ideal space,,"Let $A, B$ be commutative unital Banach algebras and let $\varphi: A \rightarrow B$ be a continuous unital map such that $$\overline{\varphi(A)} = B$$ Let $$\varphi^{*}: \text{Max}(B) \rightarrow \text{Max}(A)$$ $$\varphi^{*}(m) = m(\varphi)$$ be the map from the space of maximal ideals of $B$ to the space of maximal ideals of $A$ induced by $\varphi$ . How to prove that $\varphi^{*}$ is a topologically injective map? (Recall that an operator $T: X \rightarrow Y$ is called topologically injective if $T: X \rightarrow \text{im}(T)$ is a homeomorphism) My progress on the problem is the following: first of all, the space of maximal ideals of a commutative Banach algebra $A$ can be identified with the space of continuous functionals of the form $m: A \rightarrow \mathbb{C}$ . Clearly the map above is continuous, since the pointwise convergence of a net $(n_{i})$ in $\text{Max}(B)$ implies the convergence of the net $ m_{i} \circ \varphi) $ (the space of continuous linear functional is endowed with the weak* topology) If we assume for the moment that the map is bijective, then the fact that a continuous bijective map between compact Hausdorff spaces is a homeomorphism, yields the result. (here the space of maximal ideals is compact in weak* topology since the algebra is unital) The suggested proposition looks like a relaxation of the aformentioned reasoning above though i cannot figure out an easy way to modify it to make it work. Are there any hints?","Let be commutative unital Banach algebras and let be a continuous unital map such that Let be the map from the space of maximal ideals of to the space of maximal ideals of induced by . How to prove that is a topologically injective map? (Recall that an operator is called topologically injective if is a homeomorphism) My progress on the problem is the following: first of all, the space of maximal ideals of a commutative Banach algebra can be identified with the space of continuous functionals of the form . Clearly the map above is continuous, since the pointwise convergence of a net in implies the convergence of the net (the space of continuous linear functional is endowed with the weak* topology) If we assume for the moment that the map is bijective, then the fact that a continuous bijective map between compact Hausdorff spaces is a homeomorphism, yields the result. (here the space of maximal ideals is compact in weak* topology since the algebra is unital) The suggested proposition looks like a relaxation of the aformentioned reasoning above though i cannot figure out an easy way to modify it to make it work. Are there any hints?","A, B \varphi: A \rightarrow B \overline{\varphi(A)} = B \varphi^{*}: \text{Max}(B) \rightarrow \text{Max}(A) \varphi^{*}(m) = m(\varphi) B A \varphi \varphi^{*} T: X \rightarrow Y T: X \rightarrow \text{im}(T) A m: A \rightarrow \mathbb{C} (n_{i}) \text{Max}(B)  m_{i} \circ \varphi) ","['functional-analysis', 'maximal-and-prime-ideals', 'banach-algebras']"
18,Explanation of proof that $c_{0}$ doesn't complemented in $l^{\infty}$,Explanation of proof that  doesn't complemented in,c_{0} l^{\infty},"I've stuck with a problem. We want to prove that $c_{0}$ doesn't complemented in $l^{\infty}$ . There is a proof given in Complement of $c_{0}$ in $l^{\infty}$ . The main problem I have connected with the second part of proof : ""Now fix $n$ and $k$ such that $I_{n,k}$ is uncountable. Let $J \subset I_{n,k}$ be finite and consider $y = \sum_{j \in J} \operatorname{sign}{[(Px_j)(n)]} \cdot x_j$ . Note that $$ (Py)(n) = \sum_{j \in J} \operatorname{sign}{[(Px_j)(n)]}\cdot (Px_j)(n) \geq \frac{\# J}{k} $$ by our choice of $y$ . Since $A_i \cap A_j$ is finite for $i \neq j$ , we can write $y = f + z$ where $f$ has finite support and $\|z\| \leq 1$ . Thus $P(y) = P(f) + P(z) = P(z)$ by hypothesis on $P$ and therefore $\|P(y)\| \leq \|P\| \|z\| \leq \|P\|$ . This yields $$\# J \leq \|P\| k$$ whence the absurdity that $I_{n,k}$ must be finite"". As it said we consider finite subset $J \subset I_{n,k}$ . And then prove that $\#J \le \|P\| k$ , so it shows that $J$ is finite . It confuses me. And even if we show that $J$ is finite, how it shows that $I_{n,k}$ is finite?","I've stuck with a problem. We want to prove that doesn't complemented in . There is a proof given in Complement of in . The main problem I have connected with the second part of proof : ""Now fix and such that is uncountable. Let be finite and consider . Note that by our choice of . Since is finite for , we can write where has finite support and . Thus by hypothesis on and therefore . This yields whence the absurdity that must be finite"". As it said we consider finite subset . And then prove that , so it shows that is finite . It confuses me. And even if we show that is finite, how it shows that is finite?","c_{0} l^{\infty} c_{0} l^{\infty} n k I_{n,k} J \subset I_{n,k} y = \sum_{j \in J} \operatorname{sign}{[(Px_j)(n)]} \cdot x_j 
(Py)(n) = \sum_{j \in J} \operatorname{sign}{[(Px_j)(n)]}\cdot (Px_j)(n) \geq \frac{\# J}{k}
 y A_i \cap A_j i \neq j y = f + z f \|z\| \leq 1 P(y) = P(f) + P(z) = P(z) P \|P(y)\| \leq \|P\| \|z\| \leq \|P\| \# J \leq \|P\| k I_{n,k} J \subset I_{n,k} \#J \le \|P\| k J J I_{n,k}","['real-analysis', 'functional-analysis', 'proof-explanation', 'banach-spaces']"
19,Show that Gamma-convergence isn't preserved under addition,Show that Gamma-convergence isn't preserved under addition,,"I need to show that $\Gamma-\lim F_k=F$ and $\Gamma-\lim G_k=G$ doesn't always imply $\Gamma-\lim(F_k + G_k)=F+G$ . Our definition of $\Gamma-$ convergence is the following: $F_k\;$ $\Gamma$ -converges to $F$ if 1) For all sequences $(u_k)$ with $u_k \rightarrow u \in X$ : $F(u) \leq \liminf F_k(u_k)$ and  2) $\;\forall u \in X: \exists$ a sequence $u_n \rightarrow u$ with $F(u)=\lim F_k(u_k)$ . We've seen in our lecture that for $F_k: \mathbb{R} \rightarrow \mathbb{R},\; F_k(x)=\sin(kx)$ follows $\Gamma-\lim \sin(kx)=-1$ . This is true because $F(x)=-1 \leq \liminf\, F_k(x_k)$ and for all $k \in \mathbb{Z}$ we have $F_k(\frac{3+4k}{2n}\pi)=-1$ and every $x\in \mathbb{R}$ can be approximated by $\frac{3+4k}{2n}\pi$ and so we obtain $F(x)=-1= \lim F_k(x_k)$ . I've thought that I could use this to create the counterexample I need. My idea was to show an analog to the example from the lecture that $\Gamma-\lim\sin^2(kx)=0$ and $\Gamma-\lim\cos^2(kx)=0$ but $\Gamma-\lim(\sin^2(kx)+\cos^2(kx))=\Gamma-\lim1=1 \neq 0$ . I'd like to ask if my reasoning is correct or am I somewhere mistaken?",I need to show that and doesn't always imply . Our definition of convergence is the following: -converges to if 1) For all sequences with : and  2) a sequence with . We've seen in our lecture that for follows . This is true because and for all we have and every can be approximated by and so we obtain . I've thought that I could use this to create the counterexample I need. My idea was to show an analog to the example from the lecture that and but . I'd like to ask if my reasoning is correct or am I somewhere mistaken?,"\Gamma-\lim F_k=F \Gamma-\lim G_k=G \Gamma-\lim(F_k + G_k)=F+G \Gamma- F_k\; \Gamma F (u_k) u_k \rightarrow u \in X F(u) \leq \liminf F_k(u_k) \;\forall u \in X: \exists u_n \rightarrow u F(u)=\lim F_k(u_k) F_k: \mathbb{R} \rightarrow \mathbb{R},\; F_k(x)=\sin(kx) \Gamma-\lim \sin(kx)=-1 F(x)=-1 \leq \liminf\, F_k(x_k) k \in \mathbb{Z} F_k(\frac{3+4k}{2n}\pi)=-1 x\in \mathbb{R} \frac{3+4k}{2n}\pi F(x)=-1= \lim F_k(x_k) \Gamma-\lim\sin^2(kx)=0 \Gamma-\lim\cos^2(kx)=0 \Gamma-\lim(\sin^2(kx)+\cos^2(kx))=\Gamma-\lim1=1 \neq 0","['functional-analysis', 'convergence-divergence']"
20,Convergence of a series given properties on uniformly bounded functions,Convergence of a series given properties on uniformly bounded functions,,"I have the following problem from ""Bollobas, Linear analysis. An introductory course"" Let $\phi_n:[0,1]\to\mathbb{R}^+$ $(n=1,2,\dots)$ be uniformly bounded continuous function such that $$\int_{0}^1\phi_n(x)dx\geq c$$ for some $c>0$ . Suppose $c_n\geq 0$ , $(n=1,2,\dots)$ and $$\sum_{n\geq 1}c_n\phi_n(x)<\infty$$ for every $x\in[0,1]$ . Prove that $$\sum_{n\geq 1}c_n<\infty$$ I don't know how to proceed. Any hints will be useful ! Thanks in advance","I have the following problem from ""Bollobas, Linear analysis. An introductory course"" Let be uniformly bounded continuous function such that for some . Suppose , and for every . Prove that I don't know how to proceed. Any hints will be useful ! Thanks in advance","\phi_n:[0,1]\to\mathbb{R}^+ (n=1,2,\dots) \int_{0}^1\phi_n(x)dx\geq c c>0 c_n\geq 0 (n=1,2,\dots) \sum_{n\geq 1}c_n\phi_n(x)<\infty x\in[0,1] \sum_{n\geq 1}c_n<\infty",['functional-analysis']
21,Is it a norm or not? Checking by 3 axioms.,Is it a norm or not? Checking by 3 axioms.,,"Task: On the set $C^2[a, b] := \lbrace f: [a, b]\to R \text{ are twice continuous differentiable function} \rbrace$ are defined functions $F_i :C^2[a, b]\to R, i \in\{ 1, 2, 3\}$ . We have 3 specific functions: $$F_1(f)=\max_{a\le t\le b} |f(t)|$$ $$F_2(f)=\max_{a\le t\le b} |f'(t)|$$ $$F_3(f)=\max_{a\le t\le b} |f''(t)|$$ Is it a norm of $C^2[a, b]$ : (a) $\Vert f\Vert =\vert f(b)-f(a)\vert  + F_2(f) + F_3(f)$ (b) $\Vert f\Vert =\vert f(b)\vert+\vert f(a)\vert + F_3(f)$ (c) $\Vert f\Vert = \int_{a}^ b \vert f(t)\vert dt   + F_3(f)$ My solution for (a) is: I need to check 3 axioms: 1) $\Vert f\Vert  \ge 0$ . (it is true because of absolute values) and $\Vert f\Vert  = 0 \iff f=0$ ( here I have some problems to show it is true ). 2) $\Vert \mathcal L f\Vert = \vert\mathcal L \vert \Vert f \Vert$ Proof: \begin{align*} \Vert \mathcal L f\Vert & = \vert\mathcal L f(b) - \mathcal L f(a) \vert + F_2(\mathcal L f) + F_3(\mathcal L f)\\ & = \vert\mathcal L (f(b) - f(a)) \vert + \mathcal L F_2(f) + \mathcal L F_3(f) \\ &= \vert \mathcal L \vert ( \vert (f(b) - f(a)) \vert + F_2(f) + F_3(f)) \\ &= \vert\mathcal L \vert \Vert f \Vert \end{align*} So it is true. 3) $\Vert  f+ g \Vert \le \vert f \vert + \vert g \vert$ . True, because of $ F_2(f+g) \le F_2(f) + F_2(g)$ . Question: The same I did with (b) and (c) and my result is that all of them are norms. Is my proof correct? What about (b) and (c)? Maybe I cannot notice something and make mistake? It looks for my strange that I got 3 norms here.","Task: On the set are defined functions . We have 3 specific functions: Is it a norm of : (a) (b) (c) My solution for (a) is: I need to check 3 axioms: 1) . (it is true because of absolute values) and ( here I have some problems to show it is true ). 2) Proof: So it is true. 3) . True, because of . Question: The same I did with (b) and (c) and my result is that all of them are norms. Is my proof correct? What about (b) and (c)? Maybe I cannot notice something and make mistake? It looks for my strange that I got 3 norms here.","C^2[a, b] := \lbrace f: [a, b]\to R \text{ are twice continuous differentiable function} \rbrace F_i :C^2[a, b]\to R, i \in\{ 1, 2, 3\} F_1(f)=\max_{a\le t\le b} |f(t)| F_2(f)=\max_{a\le t\le b} |f'(t)| F_3(f)=\max_{a\le t\le b} |f''(t)| C^2[a, b] \Vert f\Vert =\vert f(b)-f(a)\vert  + F_2(f) + F_3(f) \Vert f\Vert =\vert f(b)\vert+\vert f(a)\vert + F_3(f) \Vert f\Vert = \int_{a}^ b \vert f(t)\vert dt   + F_3(f) \Vert f\Vert  \ge 0 \Vert f\Vert  = 0 \iff f=0 \Vert \mathcal L f\Vert = \vert\mathcal L \vert \Vert f \Vert \begin{align*}
\Vert \mathcal L f\Vert & = \vert\mathcal L f(b) - \mathcal L f(a) \vert + F_2(\mathcal L f) + F_3(\mathcal L f)\\
& = \vert\mathcal L (f(b) - f(a)) \vert + \mathcal L F_2(f) + \mathcal L F_3(f) \\
&= \vert \mathcal L \vert ( \vert (f(b) - f(a)) \vert + F_2(f) + F_3(f)) \\
&= \vert\mathcal L \vert \Vert f \Vert
\end{align*} \Vert  f+ g \Vert \le \vert f \vert + \vert g \vert  F_2(f+g) \le F_2(f) + F_2(g)","['functional-analysis', 'functions', 'proof-verification', 'normed-spaces']"
22,Can one explicitly solve the shifted harmonic oscillator,Can one explicitly solve the shifted harmonic oscillator,,"The quantum harmonic oscillator is described by a Hamiltonian $$H=-\Delta + \left\lvert x \right\rvert^2.$$ By decomposing the eigenvalue problem $H\psi= E\psi$ into its angular and radial part one obtains essentially the ODE $$-\varphi''(r) + \frac{2}{r} \varphi'(r) + \frac{l(l+1)}{r^2} \varphi(r)+r^2\varphi(r)=E\varphi(r).$$ Please let me know if you have any questions. This ODE can be solved explicitly, i.e. one obtains the usual eigenfunctions and eigenvalues I wonder whether one can also solve the shifted version $$-\varphi''(r) + \frac{2}{r} \varphi'(r) + \frac{l(l+1)}{r^2} \varphi(r)+(r-r_0)^2\varphi(r)=E\varphi(r)$$ for some $r_0>0.$ It does not seem possible to map this one immediately back to the original one that's why I am asking. Please let me know if you have questions or remarks.","The quantum harmonic oscillator is described by a Hamiltonian By decomposing the eigenvalue problem into its angular and radial part one obtains essentially the ODE Please let me know if you have any questions. This ODE can be solved explicitly, i.e. one obtains the usual eigenfunctions and eigenvalues I wonder whether one can also solve the shifted version for some It does not seem possible to map this one immediately back to the original one that's why I am asking. Please let me know if you have questions or remarks.",H=-\Delta + \left\lvert x \right\rvert^2. H\psi= E\psi -\varphi''(r) + \frac{2}{r} \varphi'(r) + \frac{l(l+1)}{r^2} \varphi(r)+r^2\varphi(r)=E\varphi(r). -\varphi''(r) + \frac{2}{r} \varphi'(r) + \frac{l(l+1)}{r^2} \varphi(r)+(r-r_0)^2\varphi(r)=E\varphi(r) r_0>0.,"['real-analysis', 'functional-analysis']"
23,How to prove that a function belongs (or does not belong) to $H^{\frac12}_{00}$,How to prove that a function belongs (or does not belong) to,H^{\frac12}_{00},"Given a domain $\Omega$ , the space $H^{1/2}(\partial \Omega)$ can be defined as the image of the trace operator $\gamma: H^{1}(\Omega) \rightarrow H^{1/2}(\partial \Omega)$ , which (roughly speaking) is the linear operator that associates to each $\varphi \in D(\overline{\Omega})$ its restriction over $\partial \Omega$ i.e. $\gamma   \varphi = \varphi|_{\partial \Omega}$ . Note that extending the definition of $\gamma$ from $D(\overline{\Omega})$ to $H^1({\Omega})$ is legit because the latter is dense in the former. In my understanding, given a portion of the boundary $\Gamma \subset \partial \Omega$ , the space $H^{1/2}_{00}(\Gamma)$ is defined as the space of functions with support on $\Gamma$ whose trivial extension by zero outside of $\Gamma$ belongs to $H^{1/2}(\partial \Omega)$ . My question is: given a generic function on $\Gamma$ , how can one be sure that it belong to $H^{1/2}_{00}(\Gamma)$ ? I believe that this question can be linked to the well-posedness of the following problem. Let us consider the domain $\Omega = (0,1)^2$ and the problem $-\Delta u = f$ in $\Omega$ , $u = g$ on $\Gamma_D = \{1\} \times (0,1)$ and $u = 0$ on $\partial \Omega \setminus \Gamma_D$ . Does this equation admit a solution $u \in H^1(\Omega)$ for a general function $g$ ? If not, under which conditions does the solution live in such space?","Given a domain , the space can be defined as the image of the trace operator , which (roughly speaking) is the linear operator that associates to each its restriction over i.e. . Note that extending the definition of from to is legit because the latter is dense in the former. In my understanding, given a portion of the boundary , the space is defined as the space of functions with support on whose trivial extension by zero outside of belongs to . My question is: given a generic function on , how can one be sure that it belong to ? I believe that this question can be linked to the well-posedness of the following problem. Let us consider the domain and the problem in , on and on . Does this equation admit a solution for a general function ? If not, under which conditions does the solution live in such space?","\Omega H^{1/2}(\partial \Omega) \gamma: H^{1}(\Omega) \rightarrow H^{1/2}(\partial \Omega) \varphi \in D(\overline{\Omega}) \partial \Omega \gamma 
 \varphi = \varphi|_{\partial \Omega} \gamma D(\overline{\Omega}) H^1({\Omega}) \Gamma \subset \partial \Omega H^{1/2}_{00}(\Gamma) \Gamma \Gamma H^{1/2}(\partial \Omega) \Gamma H^{1/2}_{00}(\Gamma) \Omega = (0,1)^2 -\Delta u = f \Omega u = g \Gamma_D = \{1\} \times (0,1) u = 0 \partial \Omega \setminus \Gamma_D u \in H^1(\Omega) g","['functional-analysis', 'trace']"
24,"Show that if the norms $\| \cdot \|_1$ and $\| \cdot \|_2$ are equivalent, then $(X, \| \cdot \|_1)$ is Banach iff $(X, \| \cdot \|_2)$ is Banach","Show that if the norms  and  are equivalent, then  is Banach iff  is Banach","\| \cdot \|_1 \| \cdot \|_2 (X, \| \cdot \|_1) (X, \| \cdot \|_2)","Exercise : Show that if the norms $\| \cdot \|_1$ and $\| \cdot \|_2$ are equivalent, then the space $(X, \| \cdot \|_1)$ is a Banach space if and only if the space $(X, \| \cdot \|_2)$ is a Banach space. Attempt : Let $x_n$ be a Cauchy sequence in $(X, \| \cdot \|_2)$ . Let $\epsilon >0$ , then $\exists k_0 \in \mathbb N :$ $$\|x_k - x_l \|_2 < \frac{\epsilon}{2} \; \forall \; n,l \geq k_0$$ Every term $x_k$ is of the form $x_k = (x_n^k)_{n \in \mathbb N}=(x_1^k, x_2^k, \dots)$ . Thus : $$\|x_n^k - x_n^l\|_2 < \frac{\epsilon}{2} \; \forall \; k,l \geq k_0$$ which means that the sequence of real numbers $(x_n^k)_{n \in \mathbb N}$ is Cauchy $\forall n=1,2,\dots$ . Thus, it converges to some real number $x_n$ , which means that $\lim_{k \to \infty} x_n^k = x_n \forall n$ . Now, for $l \to \infty$ , we have that : $$\|x_n^k - x_n\|_2 < \frac{\epsilon}{2} \underset{n \to \infty}{\implies} \| x_k - x\|_2 < \epsilon $$ Thus $\lim_{k \to \infty} x_k = x$ which means that the sequence converges in $(X, \| \cdot \|_2)$ . But since the norms are equivalent, it holds that : $$c_1\|\cdot\|_1 \leq \|\cdot \|_2 \leq c_2\|\cdot \|_1$$ Thus, it would also be : $$c_1\|x_k - x_l\|_1<\frac{\epsilon}{2} \implies \|x_k - x_l\|_1 < \frac{\epsilon '}{2}$$ But that's the definition of a Cauchy sequence in $(X, \| \cdot \|_1)$ and thus a sequence $(x_n)$ is Cauchy in $(X, \| \cdot \|_1)$ if and only if it's Cauchy in $(X, \| \cdot \|_2)$ . This means that the sequence would also converge in $(X, \| \cdot \|_1)$ if and only if it converges in $(X, \| \cdot \|_2)$ . Thus $(X, \| \cdot \|_2)$ must be complete and thus a Banach space. Question : Is my solution mathematically rigorous enough and correct ? I would appreciate any comments or corrections as I am a beginner at Functional Analysis.","Exercise : Show that if the norms and are equivalent, then the space is a Banach space if and only if the space is a Banach space. Attempt : Let be a Cauchy sequence in . Let , then Every term is of the form . Thus : which means that the sequence of real numbers is Cauchy . Thus, it converges to some real number , which means that . Now, for , we have that : Thus which means that the sequence converges in . But since the norms are equivalent, it holds that : Thus, it would also be : But that's the definition of a Cauchy sequence in and thus a sequence is Cauchy in if and only if it's Cauchy in . This means that the sequence would also converge in if and only if it converges in . Thus must be complete and thus a Banach space. Question : Is my solution mathematically rigorous enough and correct ? I would appreciate any comments or corrections as I am a beginner at Functional Analysis.","\| \cdot \|_1 \| \cdot \|_2 (X, \| \cdot \|_1) (X, \| \cdot \|_2) x_n (X, \| \cdot \|_2) \epsilon >0 \exists k_0 \in \mathbb N : \|x_k - x_l \|_2 < \frac{\epsilon}{2} \; \forall \; n,l \geq k_0 x_k x_k = (x_n^k)_{n \in \mathbb N}=(x_1^k, x_2^k, \dots) \|x_n^k - x_n^l\|_2 < \frac{\epsilon}{2} \; \forall \; k,l \geq k_0 (x_n^k)_{n \in \mathbb N} \forall n=1,2,\dots x_n \lim_{k \to \infty} x_n^k = x_n \forall n l \to \infty \|x_n^k - x_n\|_2 < \frac{\epsilon}{2} \underset{n \to \infty}{\implies} \| x_k - x\|_2 < \epsilon  \lim_{k \to \infty} x_k = x (X, \| \cdot \|_2) c_1\|\cdot\|_1 \leq \|\cdot \|_2 \leq c_2\|\cdot \|_1 c_1\|x_k - x_l\|_1<\frac{\epsilon}{2} \implies \|x_k - x_l\|_1 < \frac{\epsilon '}{2} (X, \| \cdot \|_1) (x_n) (X, \| \cdot \|_1) (X, \| \cdot \|_2) (X, \| \cdot \|_1) (X, \| \cdot \|_2) (X, \| \cdot \|_2)","['real-analysis', 'functional-analysis', 'metric-spaces', 'banach-spaces', 'normed-spaces']"
25,radially unbounded functions,radially unbounded functions,,"Is the following function radially unbounded or not? $$V(x) = \frac{x_{1}^2}{1 + x_{1}^2} + x_{2}^2$$ I know that if $x_{2} \to \infty$ in which case $||x|| \to \infty$ and $V(x) \to \infty$ but if $x_{1} \to \infty$ which makes $||x|| \to \infty$ but then $V(x)$ does not go to $\infty$ . What I would like to know is that is a function radially unbounded if only one or more of its variables makes it go to infinity or should it be true for every variable that that function depends on? Edit: I am not a student of math. In a engineering subject, this popped up. I have seen other answers but can't get my head around it. So can you kindly answer the question in simpler words? instead of down voting the question.","Is the following function radially unbounded or not? I know that if in which case and but if which makes but then does not go to . What I would like to know is that is a function radially unbounded if only one or more of its variables makes it go to infinity or should it be true for every variable that that function depends on? Edit: I am not a student of math. In a engineering subject, this popped up. I have seen other answers but can't get my head around it. So can you kindly answer the question in simpler words? instead of down voting the question.",V(x) = \frac{x_{1}^2}{1 + x_{1}^2} + x_{2}^2 x_{2} \to \infty ||x|| \to \infty V(x) \to \infty x_{1} \to \infty ||x|| \to \infty V(x) \infty,"['real-analysis', 'functional-analysis', 'infinity', 'control-theory', 'lyapunov-functions']"
26,In a Banach space $x_n\text{cos}(nt)+ y_n\text{sin}{nt}\rightarrow 0$ implies both sequences individually go to $0$.,In a Banach space  implies both sequences individually go to .,x_n\text{cos}(nt)+ y_n\text{sin}{nt}\rightarrow 0 0,"Let $X$ be a Banach space, and let $\{x_n\}$ , $\{y_n\}$ be two sequences in $X$ . Suppose $x_n\text{cos}(nt)+ y_n\text{sin}(nt)\rightarrow 0$ as $n \rightarrow \infty$ for all $t$ in a non degenerate interval. Show that $\{x_n\}$ and $\{y_n\}$ both go to $0$ . I'm trying to show that both the sequences go to $0$ along each subsequence or at least show that each subsequence has a further subsequence along which both go to $0$ . To do this, I thought of somehow choosing the further subsequence such that for some $t$ in the given interval, $e^{int}$ is quite close to $1$ along the further subsequence which should give me information about $\{x_n\}$ . However, irrational rotations on the circle equidistribute which makes me clueless as to how to achieve this.","Let be a Banach space, and let , be two sequences in . Suppose as for all in a non degenerate interval. Show that and both go to . I'm trying to show that both the sequences go to along each subsequence or at least show that each subsequence has a further subsequence along which both go to . To do this, I thought of somehow choosing the further subsequence such that for some in the given interval, is quite close to along the further subsequence which should give me information about . However, irrational rotations on the circle equidistribute which makes me clueless as to how to achieve this.",X \{x_n\} \{y_n\} X x_n\text{cos}(nt)+ y_n\text{sin}(nt)\rightarrow 0 n \rightarrow \infty t \{x_n\} \{y_n\} 0 0 0 t e^{int} 1 \{x_n\},"['functional-analysis', 'dynamical-systems', 'ergodic-theory']"
27,"Show that a subspace is dense in $L^2[0,1]$",Show that a subspace is dense in,"L^2[0,1]","Let $f \in L^1[0,1]$, but $f \notin L^2[0,1]$. Consider the subspace $X$ of $L^2[0,1]$ such that $X= \{\phi \in L^2[0,1]: \int f \phi = 0\}$. Want to show that $X$ is dense in $L^2[0.1]$. I tried proving this by checking that $\langle f, g \rangle, \forall g \in X$ implies $f = 0$. However, this does not seem like a plausible way of doing this. I am also hinted by using the theory of densely defined operator, but I only know of this by its definition.","Let $f \in L^1[0,1]$, but $f \notin L^2[0,1]$. Consider the subspace $X$ of $L^2[0,1]$ such that $X= \{\phi \in L^2[0,1]: \int f \phi = 0\}$. Want to show that $X$ is dense in $L^2[0.1]$. I tried proving this by checking that $\langle f, g \rangle, \forall g \in X$ implies $f = 0$. However, this does not seem like a plausible way of doing this. I am also hinted by using the theory of densely defined operator, but I only know of this by its definition.",,['functional-analysis']
28,Why a countable set of numbers can characterize a function in Hilbert space?,Why a countable set of numbers can characterize a function in Hilbert space?,,"I'm a new graduate student major in physics; the only mathematical course I was formally trained is Real Analysis, so my following statement of my question may seem a little unprofessional, and I hope anyone who's willing to help could use as little math terminology as possible, I would really appreciate that. In my study of Quantum Mechanics, Hilbert space is defined as a collection of functions in a interval $I$ (bounded or maybe unbounded) which are square integrable on $I$, together with a definition of inner product. With out much proof, our professor claim that any functions in Hilbert space could be expanded as a linear combination of a set of complete orthogonal functions $\{u_n\}$ (since that's exactly how completeness is defined), with coefficient $a_n=\langle u_n \mid f\rangle$. My question is, if we are to describe the function $f$ in terms of the set of coefficients $(a_n)$, then it seems that the information we need is countable (forgive me for I couldn't give a proper definition of ""information""); while if we try to describe it in our original way, on $x$-axis, the information we need seems turn into uncountably infinite. So how to explain such a contradiction? Or maybe it's just that the ""information"" of later is also countable, but why?","I'm a new graduate student major in physics; the only mathematical course I was formally trained is Real Analysis, so my following statement of my question may seem a little unprofessional, and I hope anyone who's willing to help could use as little math terminology as possible, I would really appreciate that. In my study of Quantum Mechanics, Hilbert space is defined as a collection of functions in a interval $I$ (bounded or maybe unbounded) which are square integrable on $I$, together with a definition of inner product. With out much proof, our professor claim that any functions in Hilbert space could be expanded as a linear combination of a set of complete orthogonal functions $\{u_n\}$ (since that's exactly how completeness is defined), with coefficient $a_n=\langle u_n \mid f\rangle$. My question is, if we are to describe the function $f$ in terms of the set of coefficients $(a_n)$, then it seems that the information we need is countable (forgive me for I couldn't give a proper definition of ""information""); while if we try to describe it in our original way, on $x$-axis, the information we need seems turn into uncountably infinite. So how to explain such a contradiction? Or maybe it's just that the ""information"" of later is also countable, but why?",,"['functional-analysis', 'hilbert-spaces']"
29,Is this operator on $L^\infty$ injective / surjective?,Is this operator on  injective / surjective?,L^\infty,"$$ f \in L^\infty (0,1) \\ Tf(x) = \int_0^x e^{y-x}f(y)dy, x\ge0 $$ I've shown that T is a bounded linear operator from $L^\infty(0,\infty)$ into itself. I've computed its norm (it should be $\|T\| = 1$).  Now, I was wondering if it is injective and/or surjective. For injectivity , I have to show that $Tf = Tg \implies f=g \text{ in } L^\infty(0,1)$. This seems to be true $$  Tf(x) = Tg(x) \\  e^{-x}\int_0^x e^y f(y) dy = e^{-x}\int_0^xe^yg(y)dy \\ \int_0^x e^y f(y) dy = \int_0^xe^yg(y)dy  $$ Differentiating both sides with respect to $x$ and using the Fundamental Theorem of Calculus:  $$ e^x f(x) = e^x g(x) \; a.e.\\ f = g \; a.e. $$ Is this right? However, I do not know how to show surjectivity ( and I do not if it is surjective ) .  If it is surjective, then:  $$ \forall g \in L^\infty(0,\infty), \exists f \in L^\infty(0,\infty): Tf = g $$ Therefore, I have to solve the following for $f$:  $$ e^{-x} \int_0^x e^y f(y) dy = g(x) $$ I try:  $$ \int_0^x e^y f(y) dy = g(x)e^x \\ e^x f(x) = g'(x)e^x + g(x)e^x \\ f(x) = g(x) + g'(x) $$ The problem is that I'm writing $g'(x)$ without knowing if $g$ is differentiable (in general, it is not, I think). I do not know how to proceed. Can someone please help? Thank you.","$$ f \in L^\infty (0,1) \\ Tf(x) = \int_0^x e^{y-x}f(y)dy, x\ge0 $$ I've shown that T is a bounded linear operator from $L^\infty(0,\infty)$ into itself. I've computed its norm (it should be $\|T\| = 1$).  Now, I was wondering if it is injective and/or surjective. For injectivity , I have to show that $Tf = Tg \implies f=g \text{ in } L^\infty(0,1)$. This seems to be true $$  Tf(x) = Tg(x) \\  e^{-x}\int_0^x e^y f(y) dy = e^{-x}\int_0^xe^yg(y)dy \\ \int_0^x e^y f(y) dy = \int_0^xe^yg(y)dy  $$ Differentiating both sides with respect to $x$ and using the Fundamental Theorem of Calculus:  $$ e^x f(x) = e^x g(x) \; a.e.\\ f = g \; a.e. $$ Is this right? However, I do not know how to show surjectivity ( and I do not if it is surjective ) .  If it is surjective, then:  $$ \forall g \in L^\infty(0,\infty), \exists f \in L^\infty(0,\infty): Tf = g $$ Therefore, I have to solve the following for $f$:  $$ e^{-x} \int_0^x e^y f(y) dy = g(x) $$ I try:  $$ \int_0^x e^y f(y) dy = g(x)e^x \\ e^x f(x) = g'(x)e^x + g(x)e^x \\ f(x) = g(x) + g'(x) $$ The problem is that I'm writing $g'(x)$ without knowing if $g$ is differentiable (in general, it is not, I think). I do not know how to proceed. Can someone please help? Thank you.",,"['functional-analysis', 'measure-theory', 'operator-theory', 'integral-operators']"
30,"Show that the projection from $C[0,1]$ onto $P_n[0,1]$ under $L^q$ norm is nonlinear when $q>1$ and $q\neq 2$.",Show that the projection from  onto  under  norm is nonlinear when  and .,"C[0,1] P_n[0,1] L^q q>1 q\neq 2","Let $C[0,1]$ denote the space of real valued continuous functions defined on $[0,1]$. Let $P_n[0,1]$ the space of real polynomials with degree not greater than $n$ defined on $[0,1]$. Let $q>1$ denote a finite real number. For any $f\in C[0,1]$, one can show by compactness argument and uniform convexity of $L^q$ norm that there exists an  unique $P(f)\in P_n[0,1]$ such that \begin{equation} ||f-P(f)||_{L^q(0,1)}=\inf\limits_{p\in P_n[0,1]}||f-p||_{L^q(0,1)}. \end{equation} In this fashion, we define a projection operator: $f\in C[0,1]\mapsto P(f)\in P_n[0,1]$. How to show that $P(\cdot)$ is nonlinear when $q\neq 2$? Could anyone help me show this? I really don't know how to start it.","Let $C[0,1]$ denote the space of real valued continuous functions defined on $[0,1]$. Let $P_n[0,1]$ the space of real polynomials with degree not greater than $n$ defined on $[0,1]$. Let $q>1$ denote a finite real number. For any $f\in C[0,1]$, one can show by compactness argument and uniform convexity of $L^q$ norm that there exists an  unique $P(f)\in P_n[0,1]$ such that \begin{equation} ||f-P(f)||_{L^q(0,1)}=\inf\limits_{p\in P_n[0,1]}||f-p||_{L^q(0,1)}. \end{equation} In this fashion, we define a projection operator: $f\in C[0,1]\mapsto P(f)\in P_n[0,1]$. How to show that $P(\cdot)$ is nonlinear when $q\neq 2$? Could anyone help me show this? I really don't know how to start it.",,"['functional-analysis', 'polynomials', 'continuity', 'lp-spaces', 'projection']"
31,Prove the reduced Riesz representation theorem for finite-dimensional vector spaces with using only the concepts in linear algebra,Prove the reduced Riesz representation theorem for finite-dimensional vector spaces with using only the concepts in linear algebra,,"If someone did not study functional analysis but just studied linear algebra, how to let them understand the idea of Riesz representation theorem for finite-dimensional vector spaces? The Riesz representation theorem Wikipedia Let $H$ be a Hilbert space, and let $H^*$ denote its dual space, consisting of all continuous linear functionals from $H$ into the field $\mathbb{R}$ or $\mathbb{C}$. If $x$ is an element of $H$, then the function $\varphi_{x}$, for all $y$ in $H$ defined by:  \begin{align*} \varphi_x (y) = \langle y,x \rangle  \end{align*} where $\langle \cdot,\cdot \rangle$ denotes the inner product of the Hilbert space, is an element of $H^*$. The Riesz representation theorem states that every element of $H^*$ can be written uniquely in this form. This description is abstract to me. Since linear algebra is sort of the reduced functional analysis, at the very first step, I am thinking to understand the reduced Riesz representation theorem applied to linear algebra. In linear algebra, we intend to solve the problem of a linear system \begin{align*} A x = b \end{align*} where $A \in \mathbb{R}^m \times \mathbb{R}^n$ is an $m$ by $n$ matrix, $x \in \mathbb{R}^n$ is an $n$ by $1$ column vector and $b \in \mathbb{R}^m$ is an $m$ by $1$ column vector. The matrix $A$ transforms vectors in $\mathbb{R}^n$ to vectors in $\mathbb{R}^m$, thus we say $A: \mathbb{R}^n \to \mathbb{R}^m$. But the vector $b$ is actually in the column space of $A$, say $C(A) = \mathbb{R}^r \subset \mathbb{R}^m$, which has dimension $r$ that denotes the rank of $A$. Thus we can say $A: \mathbb{R}^n \to \mathbb{R}^r$. If we have an $m$ by $1$ column vector $y$, then we can write \begin{align} y^T A x = y^T b \end{align} We can rewrite it in the form of inner product \begin{align} \langle y,Ax \rangle = \langle y,b \rangle \end{align} And if we consider $b$ as a functional in the dual space of $\mathbb{R}^r$, denoted by $\varphi_{Ax}(\cdot) := \langle \cdot,b \rangle$, then \begin{align} \varphi_{Ax} (y) = \langle y, A x \rangle \end{align} Note that the mapping between $b$ and $\varphi_{Ax}$ is one-to-one. We say, every $b$ in $\mathbb{R}^r$ can be written uniquely in this form. It is very close to the equation in the Riesz representation theorem, but it seems we have to use $Ax$ instead of $x$, unless $A=I$ and $m=n=r$? I am trying to state the reduced version of the Riesz representation theorem in linear algebra, as follows: $\mathbb{R}^r$ is a Hilbert space, and its dual space $(\mathbb{R}^r)^*=\mathbb{R}^r$, consisting of all continuous linear functionals from $\mathbb{R}^r$ into the field $\mathbb{R}$. If $Ax$ is an element of $\mathbb{R}^r$, then the function $\varphi_{Ax}$, for all $y$ in $\mathbb{R}^r$ defined by:  \begin{align*} \varphi_{Ax} (y) = \langle y,Ax \rangle  \end{align*} where $\langle \cdot,\cdot \rangle$ denotes the inner product of the Hilbert space, is an element of $(\mathbb{R}^r)^*$. The Riesz representation theorem states that every element of $(\mathbb{R}^r)^*$ can be written uniquely in this form. That is, every vector $b$ in $\mathbb{R}^r$ can be represented by $\langle y,Ax \rangle$. This looks like a connection to the ""weak formulation"" of $Ax = b$, namely, we can find the solution $x \in \mathbb{R}^n$ of $Ax = b$, if for every ""test"" vector $y \in \mathbb{R}^m$ there holds $\varphi_{Ax} (y) = \langle y,Ax \rangle$. I am still not fully understand the theorem at this moment, so there might be something wrong stated above. Any comments? Could you provide me with a more clear structure of the reduced Riesz representation theorem in linear algebra? In addition, the proof of the Riesz representation theorem in textbooks usually take with a nullspace of $\varphi$ denoted by $\mathrm{ker}(\varphi)$ and its orthogonal space $\mathrm{ker}(\varphi)^{\perp}$. In linear algebra, we know the row space of a matrix is always orthogonal to its nullspace. Is there any connection between these two ideas? In other words, can we prove the reduced Riesz representation theorem for finite-dimensional vector spaces with using only the concepts in linear algebra?","If someone did not study functional analysis but just studied linear algebra, how to let them understand the idea of Riesz representation theorem for finite-dimensional vector spaces? The Riesz representation theorem Wikipedia Let $H$ be a Hilbert space, and let $H^*$ denote its dual space, consisting of all continuous linear functionals from $H$ into the field $\mathbb{R}$ or $\mathbb{C}$. If $x$ is an element of $H$, then the function $\varphi_{x}$, for all $y$ in $H$ defined by:  \begin{align*} \varphi_x (y) = \langle y,x \rangle  \end{align*} where $\langle \cdot,\cdot \rangle$ denotes the inner product of the Hilbert space, is an element of $H^*$. The Riesz representation theorem states that every element of $H^*$ can be written uniquely in this form. This description is abstract to me. Since linear algebra is sort of the reduced functional analysis, at the very first step, I am thinking to understand the reduced Riesz representation theorem applied to linear algebra. In linear algebra, we intend to solve the problem of a linear system \begin{align*} A x = b \end{align*} where $A \in \mathbb{R}^m \times \mathbb{R}^n$ is an $m$ by $n$ matrix, $x \in \mathbb{R}^n$ is an $n$ by $1$ column vector and $b \in \mathbb{R}^m$ is an $m$ by $1$ column vector. The matrix $A$ transforms vectors in $\mathbb{R}^n$ to vectors in $\mathbb{R}^m$, thus we say $A: \mathbb{R}^n \to \mathbb{R}^m$. But the vector $b$ is actually in the column space of $A$, say $C(A) = \mathbb{R}^r \subset \mathbb{R}^m$, which has dimension $r$ that denotes the rank of $A$. Thus we can say $A: \mathbb{R}^n \to \mathbb{R}^r$. If we have an $m$ by $1$ column vector $y$, then we can write \begin{align} y^T A x = y^T b \end{align} We can rewrite it in the form of inner product \begin{align} \langle y,Ax \rangle = \langle y,b \rangle \end{align} And if we consider $b$ as a functional in the dual space of $\mathbb{R}^r$, denoted by $\varphi_{Ax}(\cdot) := \langle \cdot,b \rangle$, then \begin{align} \varphi_{Ax} (y) = \langle y, A x \rangle \end{align} Note that the mapping between $b$ and $\varphi_{Ax}$ is one-to-one. We say, every $b$ in $\mathbb{R}^r$ can be written uniquely in this form. It is very close to the equation in the Riesz representation theorem, but it seems we have to use $Ax$ instead of $x$, unless $A=I$ and $m=n=r$? I am trying to state the reduced version of the Riesz representation theorem in linear algebra, as follows: $\mathbb{R}^r$ is a Hilbert space, and its dual space $(\mathbb{R}^r)^*=\mathbb{R}^r$, consisting of all continuous linear functionals from $\mathbb{R}^r$ into the field $\mathbb{R}$. If $Ax$ is an element of $\mathbb{R}^r$, then the function $\varphi_{Ax}$, for all $y$ in $\mathbb{R}^r$ defined by:  \begin{align*} \varphi_{Ax} (y) = \langle y,Ax \rangle  \end{align*} where $\langle \cdot,\cdot \rangle$ denotes the inner product of the Hilbert space, is an element of $(\mathbb{R}^r)^*$. The Riesz representation theorem states that every element of $(\mathbb{R}^r)^*$ can be written uniquely in this form. That is, every vector $b$ in $\mathbb{R}^r$ can be represented by $\langle y,Ax \rangle$. This looks like a connection to the ""weak formulation"" of $Ax = b$, namely, we can find the solution $x \in \mathbb{R}^n$ of $Ax = b$, if for every ""test"" vector $y \in \mathbb{R}^m$ there holds $\varphi_{Ax} (y) = \langle y,Ax \rangle$. I am still not fully understand the theorem at this moment, so there might be something wrong stated above. Any comments? Could you provide me with a more clear structure of the reduced Riesz representation theorem in linear algebra? In addition, the proof of the Riesz representation theorem in textbooks usually take with a nullspace of $\varphi$ denoted by $\mathrm{ker}(\varphi)$ and its orthogonal space $\mathrm{ker}(\varphi)^{\perp}$. In linear algebra, we know the row space of a matrix is always orthogonal to its nullspace. Is there any connection between these two ideas? In other words, can we prove the reduced Riesz representation theorem for finite-dimensional vector spaces with using only the concepts in linear algebra?",,"['linear-algebra', 'functional-analysis', 'analysis', 'partial-differential-equations', 'linear-transformations']"
32,Compactness / sequentially compact / reflexive space / separable spaces / weak topology,Compactness / sequentially compact / reflexive space / separable spaces / weak topology,,"I'm a bit confuse with all theses notions. Let $E$ a normed vector space of infinite dimension (also Banach, but it's probably not important). The theorem of Eberlin Smulian theorem says that : all bounded sequence that has a subsequence that converge weakly $\iff$ it's reflexive. (In fact it just says implication, but the converse is also true)... anyway. Q1) Does it mean that if $E$ is reflexive, then instead of the fact that the weak topology is not metrizable, the property $C\subset E$ is compact $\iff$ $C$ is sequentially compact hold ? Because in reflexive spaces, $\{x\in E\mid \|x\|\leq 1\}$ is compact. Eberlin Smulian theorem says that $\{x\in E\mid \|x\|\leq 1\}$ is compact $\iff$ it's sequentially compact. Can this be generalized for any compact $C$ ? Q2) If a set is separable, we know that $\{x\in E\mid \|x\|\leq 1\}$ is metrizable for the weak topology. In particular, can we conclude from this that If $E$ is separable, a set $C\subset E$ is compact $\iff$ it's sequentially compact.","I'm a bit confuse with all theses notions. Let $E$ a normed vector space of infinite dimension (also Banach, but it's probably not important). The theorem of Eberlin Smulian theorem says that : all bounded sequence that has a subsequence that converge weakly $\iff$ it's reflexive. (In fact it just says implication, but the converse is also true)... anyway. Q1) Does it mean that if $E$ is reflexive, then instead of the fact that the weak topology is not metrizable, the property $C\subset E$ is compact $\iff$ $C$ is sequentially compact hold ? Because in reflexive spaces, $\{x\in E\mid \|x\|\leq 1\}$ is compact. Eberlin Smulian theorem says that $\{x\in E\mid \|x\|\leq 1\}$ is compact $\iff$ it's sequentially compact. Can this be generalized for any compact $C$ ? Q2) If a set is separable, we know that $\{x\in E\mid \|x\|\leq 1\}$ is metrizable for the weak topology. In particular, can we conclude from this that If $E$ is separable, a set $C\subset E$ is compact $\iff$ it's sequentially compact.",,"['functional-analysis', 'weak-convergence', 'weak-topology']"
33,"Inequality involving $f,f',f''$",Inequality involving,"f,f',f''","Let $f:\mathbb{R}\to [0,+\infty)$ is strictly convex ($f''(x)\geq 0$) and is $\mathrm{C}^2$ with $\min f(x) = f(0) = 0$. If $f''(0) > 0$ then by Taylor's expansion around $0$, obviously  $$ \limsup_{x\to 0} \left|\frac{f(x)}{\left[f'(x)\right]^2} \right|<+\infty.$$ My questions are, If $f''(0) = 0$, what assumptions on $f$ do we need to ensure that $$ \limsup_{x\to 0} \left|\frac{f''(x)f(x)}{\left[f'(x)\right]^2} \right|<+\infty? \qquad(*)$$ Can we prove $(*)$ if $f\in \mathrm{C}^3$?    In other words, is it true that if $f\in C^3$ is strictly convex with $\min f = f(0) = 0$ then   $$ \limsup_{x\to 0} \frac{f''(x)f(x)}{f'(x)^2} < \infty?$$","Let $f:\mathbb{R}\to [0,+\infty)$ is strictly convex ($f''(x)\geq 0$) and is $\mathrm{C}^2$ with $\min f(x) = f(0) = 0$. If $f''(0) > 0$ then by Taylor's expansion around $0$, obviously  $$ \limsup_{x\to 0} \left|\frac{f(x)}{\left[f'(x)\right]^2} \right|<+\infty.$$ My questions are, If $f''(0) = 0$, what assumptions on $f$ do we need to ensure that $$ \limsup_{x\to 0} \left|\frac{f''(x)f(x)}{\left[f'(x)\right]^2} \right|<+\infty? \qquad(*)$$ Can we prove $(*)$ if $f\in \mathrm{C}^3$?    In other words, is it true that if $f\in C^3$ is strictly convex with $\min f = f(0) = 0$ then   $$ \limsup_{x\to 0} \frac{f''(x)f(x)}{f'(x)^2} < \infty?$$",,"['calculus', 'real-analysis', 'functional-analysis', 'analysis']"
34,A question on equivalence of Sobolev norm,A question on equivalence of Sobolev norm,,"Define $\|.\|_{T^k(\Omega)}$ as  $$ \|f\|_{T^k(\Omega)} =     \|f\|_{L^2(\Omega)} + \|(\sum\limits_{i=1}^d(\frac{\partial^{k}f}{\partial x_i^{k}})^2)^{\frac{1}{2}}\|_{L^2(\Omega)} $$ Is this norm equivalent to Sobolev norm, $\|.\|_{W^{k,2}(\Omega)}$, under two cases $\Omega$ is a bounded open subset of $\mathbb{R}^d$ $\Omega = \mathbb{R}^d$ PS : I read that it is true, for the case $d = 1$, but I have not able to find anything for $d\ge2$","Define $\|.\|_{T^k(\Omega)}$ as  $$ \|f\|_{T^k(\Omega)} =     \|f\|_{L^2(\Omega)} + \|(\sum\limits_{i=1}^d(\frac{\partial^{k}f}{\partial x_i^{k}})^2)^{\frac{1}{2}}\|_{L^2(\Omega)} $$ Is this norm equivalent to Sobolev norm, $\|.\|_{W^{k,2}(\Omega)}$, under two cases $\Omega$ is a bounded open subset of $\mathbb{R}^d$ $\Omega = \mathbb{R}^d$ PS : I read that it is true, for the case $d = 1$, but I have not able to find anything for $d\ge2$",,"['real-analysis', 'functional-analysis', 'sobolev-spaces']"
35,Why the Jauge of $C$ is $\inf\{r>0\mid v/r\in C\}$ and not $\sup\{r>0\mid rv\in K\}$?,Why the Jauge of  is  and not ?,C \inf\{r>0\mid v/r\in C\} \sup\{r>0\mid rv\in K\},"Why the Jauge $p:\mathbb R^n\to \mathbb R$ of a set $C\subset \mathbb R^n$ is defined as $$p(v)=\inf\{r>0\mid v/r\in C\}$$ and not as $$p(v)=\inf\{r>0\mid rv\notin C\} \quad \text{or}\quad p(v)=\sup\{r>0\mid rv\in C\}\ \ ?$$ Because both explain the same concept as : the first time we enter in $C$ or go out of $C$, and it looks more easy to work with $rv$ than with $\frac{v}{r}$. So I was wondering what is the motivation to use $\frac{v}{r}$ instead of $rv$.","Why the Jauge $p:\mathbb R^n\to \mathbb R$ of a set $C\subset \mathbb R^n$ is defined as $$p(v)=\inf\{r>0\mid v/r\in C\}$$ and not as $$p(v)=\inf\{r>0\mid rv\notin C\} \quad \text{or}\quad p(v)=\sup\{r>0\mid rv\in C\}\ \ ?$$ Because both explain the same concept as : the first time we enter in $C$ or go out of $C$, and it looks more easy to work with $rv$ than with $\frac{v}{r}$. So I was wondering what is the motivation to use $\frac{v}{r}$ instead of $rv$.",,['functional-analysis']
36,Finding orthonormal basis from orthogonal basis,Finding orthonormal basis from orthogonal basis,,"The following is example C.5 from Appendix C (Linear Spaces Review) of Introduction to Laplace Transforms and Fourier Series, Second Edition , by Phil Dyke: Example C.5 Show that $\{\sin(x),\cos(x)\}$ is an orthonormal basis for the inner product space $V=\{a\sin(x)+b\cos(x); a,b\in\mathbb R, 0\le x\le\pi\}$ using as inner product $$\langle f,g \rangle = \int_0^1 fg dx, \qquad f,g\in V$$    and determine an orthonormal basis. Solution $V$ is two dimensional and the set $\{\sin(x),\cos(x)\}$ is obviously a basis. We merely need to check orthogonality. First of all, $$\begin{align}\langle\sin(x),\cos(x)\rangle=\int_0^\pi\sin(x)\cos(x)\,dx&=\frac{1}{2}\int_0^\pi\sin(2x)\,dx\\ &=\left[-\frac{1}{4}\cos(2x)\right]_0^\pi \\ &=0.\end{align}$$ Hence orthogonality is established. Also, $$\langle\sin(x),\sin(x)\rangle=\int_0^\pi\sin^2(x)\,dx=\frac{\pi}{2}$$ and $$\langle\cos(x),\cos(x)\rangle=\int_0^\pi\cos^2(x)\,dx=\frac{\pi}{2}.$$ Therefore $$\left\{\sqrt{\dfrac{2}{\pi}}\sin(x),\sqrt{\dfrac{2}{\pi}}\cos(x)\right\}$$ is an orthonormal basis. I understand that, for orthonormality, we require that $\| \mathbf{a} \| = 1$. However, I'm unsure of how the orthonormal basis was found at the bottom of the proof? I would appreciate it if people could please take the time to clarify this.","The following is example C.5 from Appendix C (Linear Spaces Review) of Introduction to Laplace Transforms and Fourier Series, Second Edition , by Phil Dyke: Example C.5 Show that $\{\sin(x),\cos(x)\}$ is an orthonormal basis for the inner product space $V=\{a\sin(x)+b\cos(x); a,b\in\mathbb R, 0\le x\le\pi\}$ using as inner product $$\langle f,g \rangle = \int_0^1 fg dx, \qquad f,g\in V$$    and determine an orthonormal basis. Solution $V$ is two dimensional and the set $\{\sin(x),\cos(x)\}$ is obviously a basis. We merely need to check orthogonality. First of all, $$\begin{align}\langle\sin(x),\cos(x)\rangle=\int_0^\pi\sin(x)\cos(x)\,dx&=\frac{1}{2}\int_0^\pi\sin(2x)\,dx\\ &=\left[-\frac{1}{4}\cos(2x)\right]_0^\pi \\ &=0.\end{align}$$ Hence orthogonality is established. Also, $$\langle\sin(x),\sin(x)\rangle=\int_0^\pi\sin^2(x)\,dx=\frac{\pi}{2}$$ and $$\langle\cos(x),\cos(x)\rangle=\int_0^\pi\cos^2(x)\,dx=\frac{\pi}{2}.$$ Therefore $$\left\{\sqrt{\dfrac{2}{\pi}}\sin(x),\sqrt{\dfrac{2}{\pi}}\cos(x)\right\}$$ is an orthonormal basis. I understand that, for orthonormality, we require that $\| \mathbf{a} \| = 1$. However, I'm unsure of how the orthonormal basis was found at the bottom of the proof? I would appreciate it if people could please take the time to clarify this.",,"['linear-algebra', 'functional-analysis', 'orthonormal']"
37,Sequence of subspaces is dense. Limit of Operator norm,Sequence of subspaces is dense. Limit of Operator norm,,"Let $V \subset L^2(\Omega)$ be a Hilbertspace and $\{V_n\}$ a sequence of subspaces such that \begin{align*} V_1 \subset V_2 \subset \dots \quad \text{and} \quad \overline{\bigcup_{n \in \mathbb{N}} V_n} = V \, (\text{w.r.t. } V\text{-norm} ). \end{align*} For some $f\in L^2(\Omega)$ we define $\phi_n = \sup_{\| v_n\| = 1, v_n \in V_n} \int_\Omega f(x) v_n(x)\, dx$. How can I prove that \begin{align*} \lim_{n\to\infty} \phi_n = \sup_{\| v\| = 1, v \in V} \int_\Omega f(x) v(x)\, dx \end{align*}  holds? Is this convergence uniform?","Let $V \subset L^2(\Omega)$ be a Hilbertspace and $\{V_n\}$ a sequence of subspaces such that \begin{align*} V_1 \subset V_2 \subset \dots \quad \text{and} \quad \overline{\bigcup_{n \in \mathbb{N}} V_n} = V \, (\text{w.r.t. } V\text{-norm} ). \end{align*} For some $f\in L^2(\Omega)$ we define $\phi_n = \sup_{\| v_n\| = 1, v_n \in V_n} \int_\Omega f(x) v_n(x)\, dx$. How can I prove that \begin{align*} \lim_{n\to\infty} \phi_n = \sup_{\| v\| = 1, v \in V} \int_\Omega f(x) v(x)\, dx \end{align*}  holds? Is this convergence uniform?",,"['sequences-and-series', 'functional-analysis', 'normed-spaces', 'sobolev-spaces']"
38,Domain of sum of self-adjoint operators $A \otimes 1 + 1 \otimes B$?,Domain of sum of self-adjoint operators ?,A \otimes 1 + 1 \otimes B,"Thinking about some quantum mechanics issues, I stumbled across the following functional analysis problem which confuses me a lot. Let $A$, $B$ be self-adjoint, unbounded and positive operators with domains $\rm{dom}(A) \subset H_1$ and $\rm{dom}(B) \subset H_2$, where $H_1, H_2$ are some separable Hilbert spaces ($L^2$ of something, in fact). What is the domain of $A + B$, which denotes the closure of the operator $A \otimes 1 + 1 \otimes B$ definable on $\rm{dom}(A) \otimes \rm{dom}(B) \subset H_1 \otimes H_2$? I know that the closure can in principle lead to complications here. But I thought that the answer should be something like $\rm{dom}(A) \otimes H_2 \cap H_1 \otimes \rm{dom}(B)$ because both operators are positive and there cannot be any strange cancellations that would further enlarge the domain.","Thinking about some quantum mechanics issues, I stumbled across the following functional analysis problem which confuses me a lot. Let $A$, $B$ be self-adjoint, unbounded and positive operators with domains $\rm{dom}(A) \subset H_1$ and $\rm{dom}(B) \subset H_2$, where $H_1, H_2$ are some separable Hilbert spaces ($L^2$ of something, in fact). What is the domain of $A + B$, which denotes the closure of the operator $A \otimes 1 + 1 \otimes B$ definable on $\rm{dom}(A) \otimes \rm{dom}(B) \subset H_1 \otimes H_2$? I know that the closure can in principle lead to complications here. But I thought that the answer should be something like $\rm{dom}(A) \otimes H_2 \cap H_1 \otimes \rm{dom}(B)$ because both operators are positive and there cannot be any strange cancellations that would further enlarge the domain.",,"['functional-analysis', 'tensor-products', 'unbounded-operators']"
39,Find orbit of element in Banach space,Find orbit of element in Banach space,,"Let $\mathcal{B}_1,\mathcal{B}_2$ be some Banach spaces, then $$S=\left\{A:\mathcal{B}_1\to\mathcal{B}_2\big|\left\lVert A\right\rVert\leqslant1\right\}\subset\mathcal{L}\left(\mathcal{B}_1,\mathcal{B}_2\right)$$ is a closed uniball in a linear continuous operator space. Let $x\in\mathcal{B}_1$ be some fixed element. I have a task to find an orbit of such point, which is denoted as: $$\text{Orb}\left(x\right)=Sx=\left\{Ax\big|A\in S\right\}$$ I think, such orbit would be a closed ball of radius $\left\lVert x\right\rVert$ in $\mathcal{B}_2$, but I don't know how to prove it. The furthers I got is that $\text{Orb}\left(x\right)\in\left\{y\in\mathcal{B}_2\big|\left\lVert y\right\rVert\leqslant\left\lVert x\right\rVert\right\}$. What should I do next?","Let $\mathcal{B}_1,\mathcal{B}_2$ be some Banach spaces, then $$S=\left\{A:\mathcal{B}_1\to\mathcal{B}_2\big|\left\lVert A\right\rVert\leqslant1\right\}\subset\mathcal{L}\left(\mathcal{B}_1,\mathcal{B}_2\right)$$ is a closed uniball in a linear continuous operator space. Let $x\in\mathcal{B}_1$ be some fixed element. I have a task to find an orbit of such point, which is denoted as: $$\text{Orb}\left(x\right)=Sx=\left\{Ax\big|A\in S\right\}$$ I think, such orbit would be a closed ball of radius $\left\lVert x\right\rVert$ in $\mathcal{B}_2$, but I don't know how to prove it. The furthers I got is that $\text{Orb}\left(x\right)\in\left\{y\in\mathcal{B}_2\big|\left\lVert y\right\rVert\leqslant\left\lVert x\right\rVert\right\}$. What should I do next?",,"['functional-analysis', 'operator-theory', 'banach-spaces']"
40,Closed set in metric space,Closed set in metric space,,"In $C\left( {\left[ {0,1} \right]} \right)$, we define metric  $$d\left( {f,g} \right) = \mathop {\max }\limits_{t \in \left[ {0,1} \right]} t\left| {f\left( t \right) - g\left( t \right)} \right|.$$ We define $A = \left\{ {f \in C\left( {\left[ {0,1} \right]} \right):f\left( 0 \right) = f\left( 1 \right)} \right\}$. Is $A$ an open or closed set in $C\left( {\left[ {0,1} \right]} \right)$ with metric $d$??","In $C\left( {\left[ {0,1} \right]} \right)$, we define metric  $$d\left( {f,g} \right) = \mathop {\max }\limits_{t \in \left[ {0,1} \right]} t\left| {f\left( t \right) - g\left( t \right)} \right|.$$ We define $A = \left\{ {f \in C\left( {\left[ {0,1} \right]} \right):f\left( 0 \right) = f\left( 1 \right)} \right\}$. Is $A$ an open or closed set in $C\left( {\left[ {0,1} \right]} \right)$ with metric $d$??",,['functional-analysis']
41,Projection on Hilbert spaces,Projection on Hilbert spaces,,"Let $\mathcal{H}$ be a Hilbert space, $P$ and $Q$ orthogonal projections, and $\psi\in\mathcal{H}$ a unit vector. Let $R$ be the orthogonal projection onto $\overline{\text{span}(\text{im}P\cup\text{im}Q)}$. I need to show that $\langle\psi,R\psi\rangle\leq\langle\psi,(P+Q)\psi\rangle$. I feel this is extremely obvious but can't figure out how to begin the proof. Any help is much appreciated!","Let $\mathcal{H}$ be a Hilbert space, $P$ and $Q$ orthogonal projections, and $\psi\in\mathcal{H}$ a unit vector. Let $R$ be the orthogonal projection onto $\overline{\text{span}(\text{im}P\cup\text{im}Q)}$. I need to show that $\langle\psi,R\psi\rangle\leq\langle\psi,(P+Q)\psi\rangle$. I feel this is extremely obvious but can't figure out how to begin the proof. Any help is much appreciated!",,"['functional-analysis', 'hilbert-spaces']"
42,Finding a certain sequence for a discontinuous linear map,Finding a certain sequence for a discontinuous linear map,,"Let $X,Y$ be normed spaces and $T:X\to Y$ linear and discontinuous. Hence $T$ is discontinuous at every point. Then for every $x\in X$ there exists a sequence $(x_n)\subseteq X$ such that $(x_n)$ converges to $x$ and $(T(x_n))$ doesn't converge to $T(x)$. My question is: Can we choose above a sequence $(x_n)$ such that $(T(x_n))$ is bounded? Since $(x_n)$ converges to $x$ then $(x_n)$ is bounded. But since $T$ is discontinuous it sends every ball to an unbounded set, so I'd like to know if such a sequence can be found carefully. Any ideas? Thank you.","Let $X,Y$ be normed spaces and $T:X\to Y$ linear and discontinuous. Hence $T$ is discontinuous at every point. Then for every $x\in X$ there exists a sequence $(x_n)\subseteq X$ such that $(x_n)$ converges to $x$ and $(T(x_n))$ doesn't converge to $T(x)$. My question is: Can we choose above a sequence $(x_n)$ such that $(T(x_n))$ is bounded? Since $(x_n)$ converges to $x$ then $(x_n)$ is bounded. But since $T$ is discontinuous it sends every ball to an unbounded set, so I'd like to know if such a sequence can be found carefully. Any ideas? Thank you.",,['functional-analysis']
43,Basic functional analysis question: equivalence of norms.,Basic functional analysis question: equivalence of norms.,,"Suppose we have two norms on a vector space such that a linear functional is continuous with respect to one if and only if it is continuous with respect to the other. Show that the two norms are equivalent. Two norms $||.||_1 $ and $||.||_2$ are equivalent if there are some constants $C_1,C_2$ such that $C_1||.||_1≤||.||_2≤C_2||.||_1$ Hints, please.","Suppose we have two norms on a vector space such that a linear functional is continuous with respect to one if and only if it is continuous with respect to the other. Show that the two norms are equivalent. Two norms $||.||_1 $ and $||.||_2$ are equivalent if there are some constants $C_1,C_2$ such that $C_1||.||_1≤||.||_2≤C_2||.||_1$ Hints, please.",,['functional-analysis']
44,A map with a small bound on the ratio of Dini derivatives is injective on the unit ball in Hilbert spaces,A map with a small bound on the ratio of Dini derivatives is injective on the unit ball in Hilbert spaces,,"Let $f$ be a continuous map defined from the unit ball $U$ of a Hilbert space $E$ to another Hilbert space $F$. The Dini derivatives are defined as (for $x\in U$) $$D^+f(x)=\limsup_{y\to x}\frac{\Vert f(y)-f(x)\Vert}{\Vert y-x\Vert}$$ and $$D^-f(x)=\liminf_{y\to x}\frac{\Vert f(y)-f(x)\Vert}{\Vert y-x\Vert}$$ I have read that if on $U$ we have $0<m\leq D^-f(x)\leq D^+f(x)\leq M<\infty$ and $$k=M/m<\sqrt{(1+\sqrt{5})/2}$$ then one can prove that $f$ is a homeomorphism from $U$ to $f(U)$ More precisely, for $x, y\in U$ we have  $$\Vert f(x)-f(y)\Vert\geq \mu\Vert x-y\Vert$$ where $$\mu=m\frac{(1+k\sqrt{k^2-1})}{1+\sqrt{k^2-1}}$$ Does anybody know a proof of this strange result or a reference for a proof of it (that seems to work only because of the geometry of Hilbert space, not in a more general Banach setting)?","Let $f$ be a continuous map defined from the unit ball $U$ of a Hilbert space $E$ to another Hilbert space $F$. The Dini derivatives are defined as (for $x\in U$) $$D^+f(x)=\limsup_{y\to x}\frac{\Vert f(y)-f(x)\Vert}{\Vert y-x\Vert}$$ and $$D^-f(x)=\liminf_{y\to x}\frac{\Vert f(y)-f(x)\Vert}{\Vert y-x\Vert}$$ I have read that if on $U$ we have $0<m\leq D^-f(x)\leq D^+f(x)\leq M<\infty$ and $$k=M/m<\sqrt{(1+\sqrt{5})/2}$$ then one can prove that $f$ is a homeomorphism from $U$ to $f(U)$ More precisely, for $x, y\in U$ we have  $$\Vert f(x)-f(y)\Vert\geq \mu\Vert x-y\Vert$$ where $$\mu=m\frac{(1+k\sqrt{k^2-1})}{1+\sqrt{k^2-1}}$$ Does anybody know a proof of this strange result or a reference for a proof of it (that seems to work only because of the geometry of Hilbert space, not in a more general Banach setting)?",,"['functional-analysis', 'hilbert-spaces']"
45,Confusion about proof of Unit Balls are not compact in Infinite Dimensional Normed Spaces with Riesz's Lemma,Confusion about proof of Unit Balls are not compact in Infinite Dimensional Normed Spaces with Riesz's Lemma,,"We can construct a sequence such that $\|x_n-x_m\|\gt 1/2$ via using Riesz's Lemma. It's not Cauchy sequence and thus it's not a convergent sequence. My question : In my notes ""since the sequence is not convergent, it doesn't have a convergent subsequence"" has been written. But when $(-1)^n$ is not convergent, its subsequence is $(-1)^{2n}$ is convergent to $1$. How can we say it doesn't have a convergent subsequence? Where am I wrong, I couldn't realize. Thanks a lot","We can construct a sequence such that $\|x_n-x_m\|\gt 1/2$ via using Riesz's Lemma. It's not Cauchy sequence and thus it's not a convergent sequence. My question : In my notes ""since the sequence is not convergent, it doesn't have a convergent subsequence"" has been written. But when $(-1)^n$ is not convergent, its subsequence is $(-1)^{2n}$ is convergent to $1$. How can we say it doesn't have a convergent subsequence? Where am I wrong, I couldn't realize. Thanks a lot",,"['functional-analysis', 'normed-spaces', 'cauchy-sequences']"
46,Space of Linear and Bounded Operators is a Banach Space,Space of Linear and Bounded Operators is a Banach Space,,"I have some questions about proof of the theorem below is written in my notes. If someone help me about it I will be appreciated. Thanks Theorem : Let $(X,\|.\|_X)$ is a normed space and $(Y,\|.\|_Y)$ is a Banach space then $(B(X,Y),\|.\|_{op})$ is a Banach space I have problem about some writtens : 1) $\|T_n(x)-T_m(x)\|_Y \leq \|T_n-T_m\|_{op}$ $\|T_n(x)-T_m(x)\|_Y \leq sup_{\|x\|_X \leq 1}\|T_n(x)-T_m(x)\|_Y$ it is true only for some $x \in X$ not for all. How can we use it? 2) $T_n(x) \to y , \exists y \in Y$ since $Y$ is a Banach space.It's OK for me but after this statement ""Since $\forall x \in X$  $\exists y \in Y$ we can write $lim_{n\to \infty}T_n(x)=T(x)$"" is written. How can we write it? Thanks in advance :)","I have some questions about proof of the theorem below is written in my notes. If someone help me about it I will be appreciated. Thanks Theorem : Let $(X,\|.\|_X)$ is a normed space and $(Y,\|.\|_Y)$ is a Banach space then $(B(X,Y),\|.\|_{op})$ is a Banach space I have problem about some writtens : 1) $\|T_n(x)-T_m(x)\|_Y \leq \|T_n-T_m\|_{op}$ $\|T_n(x)-T_m(x)\|_Y \leq sup_{\|x\|_X \leq 1}\|T_n(x)-T_m(x)\|_Y$ it is true only for some $x \in X$ not for all. How can we use it? 2) $T_n(x) \to y , \exists y \in Y$ since $Y$ is a Banach space.It's OK for me but after this statement ""Since $\forall x \in X$  $\exists y \in Y$ we can write $lim_{n\to \infty}T_n(x)=T(x)$"" is written. How can we write it? Thanks in advance :)",,"['functional-analysis', 'analysis', 'operator-theory', 'banach-spaces', 'normed-spaces']"
47,spectrum and point spectrum of $(Af)(x)=(2+\cos x)f(x)$,spectrum and point spectrum of,(Af)(x)=(2+\cos x)f(x),"Consider $A:L^2[-\frac{\pi}{2},\frac{\pi}{2}]\to L^2[-\frac{\pi}{2},\frac{\pi} {2}]$ such that $(Af)(x)=(2+\cos x)f(x)$. Do we have $||A||\in \sigma(A)$ and $||A||\in\sigma_p(A)$? My attempt: I know that $||A||\in \sigma_p$ iff there exists a nonzero $f\in L^2[-\frac{\pi}{2},\frac{\pi}{2}]$ such that $Af=||A||f$. First, I showed that $A$ is selfadjoint. Then $$||Af||^2=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}|(2+\cos x)f(x)|^2 dx \le \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}|3f(x)|^2 dx$$ This implies $||A||\le 3.$ Let $S_\epsilon=\{x\in [-\frac{\pi}{2},\frac{\pi}{2}] : 2+\cos x \ge 3-\epsilon\}$ and $f_\epsilon(x)=\mathbb{1}_{S_{\epsilon}}$. So, we can show that $||Af||\le (3-\epsilon)||f||$. This impiles $||A||=3$. If $||A||=3 \in \sigma_p$, then $Af=(2+\cos x)f(x)=3f(x)$ for some $f\in L^2[-\frac{\pi}{2},\frac{\pi}{2}]$. How to prove or disprove such an $f$ exists? Can we just say the above equation implies $2+\cos x=f(x)$, which is a contradiction? How about $||A||\in \sigma(A)$? How to prove or disprove it? Do I need to check whether $A$ is compact? Thank you very much.","Consider $A:L^2[-\frac{\pi}{2},\frac{\pi}{2}]\to L^2[-\frac{\pi}{2},\frac{\pi} {2}]$ such that $(Af)(x)=(2+\cos x)f(x)$. Do we have $||A||\in \sigma(A)$ and $||A||\in\sigma_p(A)$? My attempt: I know that $||A||\in \sigma_p$ iff there exists a nonzero $f\in L^2[-\frac{\pi}{2},\frac{\pi}{2}]$ such that $Af=||A||f$. First, I showed that $A$ is selfadjoint. Then $$||Af||^2=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}|(2+\cos x)f(x)|^2 dx \le \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}|3f(x)|^2 dx$$ This implies $||A||\le 3.$ Let $S_\epsilon=\{x\in [-\frac{\pi}{2},\frac{\pi}{2}] : 2+\cos x \ge 3-\epsilon\}$ and $f_\epsilon(x)=\mathbb{1}_{S_{\epsilon}}$. So, we can show that $||Af||\le (3-\epsilon)||f||$. This impiles $||A||=3$. If $||A||=3 \in \sigma_p$, then $Af=(2+\cos x)f(x)=3f(x)$ for some $f\in L^2[-\frac{\pi}{2},\frac{\pi}{2}]$. How to prove or disprove such an $f$ exists? Can we just say the above equation implies $2+\cos x=f(x)$, which is a contradiction? How about $||A||\in \sigma(A)$? How to prove or disprove it? Do I need to check whether $A$ is compact? Thank you very much.",,"['functional-analysis', 'operator-theory', 'spectral-theory']"
48,$L^1$ convergence implies pointwise a.e. uniform bound of a subsequence,convergence implies pointwise a.e. uniform bound of a subsequence,L^1,"Is the following statement true ? Let $f_n$ be a sequence of non-negative functions in $L^1(X,m)$, where $(X,\Sigma,\mu)$ is a probability space, such that $\|f_n - f\|_{L^1(X)}\to 0 $ for some $f \in L^1(X)$. Then there exists a subsequence $f_{n_k}$ and a function $g \in L^1(X,\mu)$ such that $$f_{n_k}(x) \le g(x) \quad \text{for $\mu$-a.e  } x \in X  .$$ I know that there exists a subsequence that converges almost everywhere, but I don't really know how to use this information. Any suggestions?","Is the following statement true ? Let $f_n$ be a sequence of non-negative functions in $L^1(X,m)$, where $(X,\Sigma,\mu)$ is a probability space, such that $\|f_n - f\|_{L^1(X)}\to 0 $ for some $f \in L^1(X)$. Then there exists a subsequence $f_{n_k}$ and a function $g \in L^1(X,\mu)$ such that $$f_{n_k}(x) \le g(x) \quad \text{for $\mu$-a.e  } x \in X  .$$ I know that there exists a subsequence that converges almost everywhere, but I don't really know how to use this information. Any suggestions?",,"['real-analysis', 'functional-analysis', 'analysis', 'measure-theory']"
49,Find continuos function $x$ with $x(0)=0$ such that $\|x-y\| \geq 1$ where $y(0)=0$ and $\int_0^1 y(t) dt = 0$,Find continuos function  with  such that  where  and,x x(0)=0 \|x-y\| \geq 1 y(0)=0 \int_0^1 y(t) dt = 0,"Consider $C[0,1]$ space of continuos functions with the uniform norm. Let $X = \{ x \in C[0,1]: x(0)=0\}$ and $Y = \{ y \in X: \int_0^1 y(t) dt = 0 \}$ subspaces of  $C[0,1]$. How can I show that $\exists x \in X$, $\|x\|=1$, such that $$\|x-y\| \geq 1 ~~\forall y \in Y?$$ Edit: Sorry guys, the question is: How can I prove that doesn't exists $x \in X$, such that $||x||=1$ and $||x-y|| \geq 1 ~~\forall y \in Y$","Consider $C[0,1]$ space of continuos functions with the uniform norm. Let $X = \{ x \in C[0,1]: x(0)=0\}$ and $Y = \{ y \in X: \int_0^1 y(t) dt = 0 \}$ subspaces of  $C[0,1]$. How can I show that $\exists x \in X$, $\|x\|=1$, such that $$\|x-y\| \geq 1 ~~\forall y \in Y?$$ Edit: Sorry guys, the question is: How can I prove that doesn't exists $x \in X$, such that $||x||=1$ and $||x-y|| \geq 1 ~~\forall y \in Y$",,['functional-analysis']
50,$\lim_{h\to 0} \frac{ f(a+ h) - 2f(a) + f(a-h) }{h^2}$ and a polynomial function with degree at most 2,and a polynomial function with degree at most 2,\lim_{h\to 0} \frac{ f(a+ h) - 2f(a) + f(a-h) }{h^2},"Let $f_1(x)$ be a contionous polynomial funtion $f:\mathbb{R}\to\mathbb{R}$ with degree at most 2. Let $$\lim_{h\to 0} \frac{ f_1(a+ h) - 2f_1(a) + f_1(a-h) }{h^2}=f_2(a).$$ Prove that if the limit above exist for all $a$s, then $f_2(a)$ is constant! This problem comes from a problem also posted in this site. But I can't prove it even with the solutions posted to this: a similar problem Please help! I am very thankful for every solution!!","Let $f_1(x)$ be a contionous polynomial funtion $f:\mathbb{R}\to\mathbb{R}$ with degree at most 2. Let $$\lim_{h\to 0} \frac{ f_1(a+ h) - 2f_1(a) + f_1(a-h) }{h^2}=f_2(a).$$ Prove that if the limit above exist for all $a$s, then $f_2(a)$ is constant! This problem comes from a problem also posted in this site. But I can't prove it even with the solutions posted to this: a similar problem Please help! I am very thankful for every solution!!",,"['real-analysis', 'functional-analysis', 'derivatives', 'polynomials']"
51,Show that $\Vert T\Vert=\max\limits_{1\leq i\leq n}\vert d_{i}\vert $,Show that,\Vert T\Vert=\max\limits_{1\leq i\leq n}\vert d_{i}\vert ,"Please give me a hint for the following question. The Problem: (a) Let $D\in M_{nn}(\mathbb{R})$ be a diagonal matrix and $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ be the linear operator associated with $D,$ $Tx=Dx$ for all $x\in\mathbb{R}^{n}.$ Show that    $$\Vert T\Vert=\max\limits_{1\leq i\leq n}\vert d_{i}\vert $$   where $d_{1}, d_{2}, \cdots, d_{n}$ are the entries on the diagonal of $D$.     For this part of the question I have proved that  $\Vert T\Vert\leq\max\limits_{1\leq i\leq n}\vert d_{i}\vert $ For this part of the question I have proved that  $\Vert T\Vert\leq\max\limits_{1\leq i\leq n}\vert d_{i}\vert $. (b) Suppose $A\in M_{nn}(\mathbb{R}^{n})$ is symetric (i.e.,$A^{T}=A$) and $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is the linear operator associated with $A$. Show that    $$\Vert T\Vert=\max\{ \vert \lambda\vert; \lambda\; is\; the\; eigenvalue\; of\; A\}. $$","Please give me a hint for the following question. The Problem: (a) Let $D\in M_{nn}(\mathbb{R})$ be a diagonal matrix and $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ be the linear operator associated with $D,$ $Tx=Dx$ for all $x\in\mathbb{R}^{n}.$ Show that    $$\Vert T\Vert=\max\limits_{1\leq i\leq n}\vert d_{i}\vert $$   where $d_{1}, d_{2}, \cdots, d_{n}$ are the entries on the diagonal of $D$.     For this part of the question I have proved that  $\Vert T\Vert\leq\max\limits_{1\leq i\leq n}\vert d_{i}\vert $ For this part of the question I have proved that  $\Vert T\Vert\leq\max\limits_{1\leq i\leq n}\vert d_{i}\vert $. (b) Suppose $A\in M_{nn}(\mathbb{R}^{n})$ is symetric (i.e.,$A^{T}=A$) and $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is the linear operator associated with $A$. Show that    $$\Vert T\Vert=\max\{ \vert \lambda\vert; \lambda\; is\; the\; eigenvalue\; of\; A\}. $$",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'operator-theory', 'linear-transformations']"
52,Scaling the NS equation: supercriticality and energy estimate,Scaling the NS equation: supercriticality and energy estimate,,"There is a ‘rescaling transformation’ that is particularly significant for the Navier–Stokes equations when they are posed on the whole space, but is also important in the local regularity theory. Suppose first that $u(x,t)$ is a solution of the linear heat equation $$\partial_t u-\Delta u=0,~~x\in \mathbb{R}^3$$ It is simple to check that for any $\lambda > 0$ and any $\alpha \in \mathbb{R}$ the function $$u_{\lambda, \alpha}=\lambda^\alpha u(\lambda x,\lambda^2 t)$$ is again a solution of the heat equation. When we consider the Navier–Stokes equations we need the nonlinear term $(u\cdot \nabla)u$ to transform in the same way as $\partial_t u$ and $\Delta u$; it is easy to check that this requires the choice  $\alpha =1$. Therefore if $u(x, 0) = u_0(x)$ gives rise to a solution $u(x, t )$ of the Navier–Stokes equations on $\mathbb{R}^3$, with corresponding pressure $p(x, t )$, then the rescaled functions $$\lambda u(\lambda x,\lambda^2 t)~~and ~~\lambda^2p(\lambda x,\lambda^2 t)$$  still solve the equations, but with rescaled initial data $u_{0,\lambda}(x) = \lambda u_0(\lambda x)$. For $\lambda$ this corresponds to shrinking (spatial) distances by a factor of $\lambda^{-1}$ and increasing the speed by a factor of $\lambda$; it is clear that the time =(distance/speed) should therefore shrink by a factor of $\lambda^{−2}$. Note that if an initial condition $u_0$ gives rise to the solution $u(x, t )$ and the rescaled initial condition $u_{0,\lambda}$ gives rise to the solution $u_\lambda(x, t )$ then $$u_\lambda \mbox{ is regular on} [0, T_\lambda] \Leftrightarrow u \mbox{ is regular on}  [0, \lambda^2T_{\lambda}].$$ Spaces of functions in which the norm is unchanged by the rescaling $$u(x) \rightarrow \lambda u(\lambda x)$$ are termed ‘critical spaces’. These are the natural spaces in which to try to prove ‘small data’ results, i.e. the global existence of smooth solutions when the norm of the initial condition is small, since the norm of the data is unaffected by the aformentioned rescaling transformation. We give two examples of such spaces: the Sobolev space $\dot{H}^{1/2}$ and the Lebesgue space $L^3$. Critical spaces are important since in some cases local existence in a critical space can be used to deduce global existence. **   Now, let us take the same scaling transformation in the context of **supercritical space such as $L^2$ and blow up this fine-scale behaviour by $\lambda$ to create a coarse-scale solution to Navier-Stokes. Given that the fine-scale solution could (in the worst-case scenario) be as bad as an arbitrary smooth vector field with kinetic energy and cumulative energy dissipation at most $E$, the rescaled unit-scale solution can be as bad as an arbitrary smooth vector field with kinetic energy and cumulative energy dissipation at most $E \lambda$, as a simple change-of-variables shows. Note that the control given by our two key quantities has worsened by a factor of $\lambda$; because of this worsening, we say that these quantities are supercritical – they become increasingly useless for controlling the solution as one moves to finer and finer scales. This should be contrasted with critical quantities (such as the energy for two-dimensional Navier-Stokes), which are invariant under scaling and thus control all scales equally well (or equally poorly), and subcritical quantities, control of which becomes increasingly powerful at fine scales (and increasingly useless at very coarse scales). Knowing that the energy estimate for the solution $u$ is given by $$ \underbrace{\sup_{0 \leq t < T} \frac{1}{2} \int_{{\Bbb R}^3} |u(t,x)|^2\ dx}_{\mbox{kinetic energy}}+  \underbrace{\frac{1}{2}\int_0^T \int_{{\Bbb R}^3} |\nabla u(t,x)|^2\ dx dt}_{\mbox{cumulative energy dissipation}}\leq \underbrace{\|u_0\|_{L^2({\Bbb R}^3)}^2}_{E}$$ Would you please write the same energy estimate associated to $\lambda u$ and explain in mathematical terms how exactly in that case the energy estimate will  fail to control the kinetic and cumulative dissapation energy in contrast with critical spaces case. References: Why global regularity for NS equations is hard, Terry Tao, what's new blog Robinson, J., Rodrigo, J., & Sadowski, W. (2016). The Three-Dimensional Navier–Stokes Equations: Classical Theory (Cambridge Studies in Advanced Mathematics). Cambridge: Cambridge University Press. doi:10.1017/CBO9781139095143 Thanks","There is a ‘rescaling transformation’ that is particularly significant for the Navier–Stokes equations when they are posed on the whole space, but is also important in the local regularity theory. Suppose first that $u(x,t)$ is a solution of the linear heat equation $$\partial_t u-\Delta u=0,~~x\in \mathbb{R}^3$$ It is simple to check that for any $\lambda > 0$ and any $\alpha \in \mathbb{R}$ the function $$u_{\lambda, \alpha}=\lambda^\alpha u(\lambda x,\lambda^2 t)$$ is again a solution of the heat equation. When we consider the Navier–Stokes equations we need the nonlinear term $(u\cdot \nabla)u$ to transform in the same way as $\partial_t u$ and $\Delta u$; it is easy to check that this requires the choice  $\alpha =1$. Therefore if $u(x, 0) = u_0(x)$ gives rise to a solution $u(x, t )$ of the Navier–Stokes equations on $\mathbb{R}^3$, with corresponding pressure $p(x, t )$, then the rescaled functions $$\lambda u(\lambda x,\lambda^2 t)~~and ~~\lambda^2p(\lambda x,\lambda^2 t)$$  still solve the equations, but with rescaled initial data $u_{0,\lambda}(x) = \lambda u_0(\lambda x)$. For $\lambda$ this corresponds to shrinking (spatial) distances by a factor of $\lambda^{-1}$ and increasing the speed by a factor of $\lambda$; it is clear that the time =(distance/speed) should therefore shrink by a factor of $\lambda^{−2}$. Note that if an initial condition $u_0$ gives rise to the solution $u(x, t )$ and the rescaled initial condition $u_{0,\lambda}$ gives rise to the solution $u_\lambda(x, t )$ then $$u_\lambda \mbox{ is regular on} [0, T_\lambda] \Leftrightarrow u \mbox{ is regular on}  [0, \lambda^2T_{\lambda}].$$ Spaces of functions in which the norm is unchanged by the rescaling $$u(x) \rightarrow \lambda u(\lambda x)$$ are termed ‘critical spaces’. These are the natural spaces in which to try to prove ‘small data’ results, i.e. the global existence of smooth solutions when the norm of the initial condition is small, since the norm of the data is unaffected by the aformentioned rescaling transformation. We give two examples of such spaces: the Sobolev space $\dot{H}^{1/2}$ and the Lebesgue space $L^3$. Critical spaces are important since in some cases local existence in a critical space can be used to deduce global existence. **   Now, let us take the same scaling transformation in the context of **supercritical space such as $L^2$ and blow up this fine-scale behaviour by $\lambda$ to create a coarse-scale solution to Navier-Stokes. Given that the fine-scale solution could (in the worst-case scenario) be as bad as an arbitrary smooth vector field with kinetic energy and cumulative energy dissipation at most $E$, the rescaled unit-scale solution can be as bad as an arbitrary smooth vector field with kinetic energy and cumulative energy dissipation at most $E \lambda$, as a simple change-of-variables shows. Note that the control given by our two key quantities has worsened by a factor of $\lambda$; because of this worsening, we say that these quantities are supercritical – they become increasingly useless for controlling the solution as one moves to finer and finer scales. This should be contrasted with critical quantities (such as the energy for two-dimensional Navier-Stokes), which are invariant under scaling and thus control all scales equally well (or equally poorly), and subcritical quantities, control of which becomes increasingly powerful at fine scales (and increasingly useless at very coarse scales). Knowing that the energy estimate for the solution $u$ is given by $$ \underbrace{\sup_{0 \leq t < T} \frac{1}{2} \int_{{\Bbb R}^3} |u(t,x)|^2\ dx}_{\mbox{kinetic energy}}+  \underbrace{\frac{1}{2}\int_0^T \int_{{\Bbb R}^3} |\nabla u(t,x)|^2\ dx dt}_{\mbox{cumulative energy dissipation}}\leq \underbrace{\|u_0\|_{L^2({\Bbb R}^3)}^2}_{E}$$ Would you please write the same energy estimate associated to $\lambda u$ and explain in mathematical terms how exactly in that case the energy estimate will  fail to control the kinetic and cumulative dissapation energy in contrast with critical spaces case. References: Why global regularity for NS equations is hard, Terry Tao, what's new blog Robinson, J., Rodrigo, J., & Sadowski, W. (2016). The Three-Dimensional Navier–Stokes Equations: Classical Theory (Cambridge Studies in Advanced Mathematics). Cambridge: Cambridge University Press. doi:10.1017/CBO9781139095143 Thanks",,"['functional-analysis', 'partial-differential-equations', 'regularity-theory-of-pdes', 'parabolic-pde']"
53,Proving the infinite direct sum of orthogonal closed subspaces of a hilbert space is a closed linear subspace,Proving the infinite direct sum of orthogonal closed subspaces of a hilbert space is a closed linear subspace,,"Suppose that $\left\{ \mathcal{H}_n \, \big| \, n\in\mathbb{N} \right\}$ is a set of orthogonal closed subspaces of a Hilbert space $\mathcal{H}$. We define the infinite direct sum   $$\bigoplus_{n=1}^{\infty} \mathcal{H}_n := \left\{ \sum_{n=1}^{\infty} x_n \, \big| \, x_n \in \mathcal{H}_n \, \text{and} \, \sum_{n=1}^{\infty} \|\ x_n \|^2 < \infty \right\} . $$    Prove that $\oplus_{n=1}^{\infty} \mathcal{H}_n$ is a closed linear subspace of $\mathcal{H}$. I feel like I proved this in a round about manner and am looking for critiques or a 'better' way to prove the result. Here is my proof: Without loss of generality let $\left\{ e_k \, \big| \, k \in A \right\}$ be an orthonormal set in the Hilbert space $\mathcal{H}$. Then, for each $\mathcal{H}_n$ there is some $A_n$ such that $\left\{ e_k \, \big| \, k\in A_n \right\}$ is a orthonormal set. Additionally, $\displaystyle\bigcup_{n=1}^{\infty} A_n = A$.  Let  $$\mathcal{G}:=\bigoplus_{n=1}^{\infty} \mathcal{H}_n := \left\{ \sum_{n=1}^{\infty} x_n \, \big| \, x_n \in \mathcal{H}_n \, \text{and} \, \sum_{n=1}^{\infty} \|\ x_n \|^2 < \infty \right\} . $$ First we note that $\mathcal{G}$ is well-defined since $\sum_{n=1}^{\infty} x_n$, for $x_n \in \mathcal{H}_n$ converges if and only if $\sum_{n=1}^{\infty} \|\ x_n \|^2 $ converges, which we have by construction of $\mathcal{G}$. Note that the convergent unordered sums can be added term by term, suppose that  $$\sum_{n \in A} x_n := x, \quad \sum_{n \in A} y_n := y$$ Take $\epsilon>0$, there are finite sets $I, J \subset A$, such that  $$\|\ \sum_{k \in I}  x_k - x \|\ < \frac{\epsilon}{2}, \quad \quad \|\ \sum_{k \in J} y_k - y \|\ < \frac{\epsilon}{2}.$$ It follows that if $I \cup J \subset K$ is a finite subset of $A$, then  \begin{align*} \|\ \sum_{k \in K} (x_k + y_k) - (x+y) \|\ &\leq \|\ \sum_{k \in K} x_k - x \|\ + \|\ \sum_{k \in K} y_k - y \|\ \\ &< \frac{\epsilon}{2}+\frac{\epsilon}{2} \\ &=\epsilon. \end{align*} If $x,y \in \mathcal{G}$, with $$x=\sum_{k\in A} x_k e_k , \quad y=\sum_{k \in A} y_k e_k $$ then $$x+y = \sum_{k \in A} (x_k + y_k)e_k.$$ Now since $$|x_k + y_k|^2 \leq (|x_k| + |y_k|)^2 \leq 2(\max\left\{|x_k|,|y_k|\right\})^2 \leq 4(|x_k|^2 + |y_k|^2),$$ it follows that $$\sum_{k \in A} |x_k + y_k|^2 \leq 4\left( \sum_{k \in A} |x_k|^2 + \sum_{k\in A} |y_k|^2 \right) < \infty,$$ so $x+y\in \mathcal{G}$. Similarly, we also have that $\lambda x \in \mathcal{G}$ for all $\lambda \in \mathbb{C}$ and $x\in\mathcal{G}$, so $\mathcal{G}$ is a linear subspace. Now to prove that $\mathcal{G}$ is closed, first note that $\overline{\mathcal{G}} \cap \mathcal{G}^{\perp} = \left\{0\right\}$ because if $x\in\overline{\mathcal{G}}\cap\mathcal{G}^{\perp}$, then $\mathcal{G}=\overline{\mathcal{G}}^{\perp}$ we have $x\perp x$ so $x=0$. Now suppose that $x \in \mathcal{G}$. Let $$y:=\sum_{k\in A} \langle e_k , x \rangle e_k.$$ By Bessel's inequality, $$\sum_{k \in A} | \langle e_k , x \rangle |^2 \leq \|\ x \|^2,$$ so $y\in\mathcal{G}$. Moreover, $\langle e_k , x \rangle = \langle e_k , y \rangle$ for all $k\in A$, which implies that $x-y\in\mathcal{G}^{\perp}$. Since $x,y\in\overline{\mathcal{G}}$, it follows that $x-y\in\overline{\mathcal{G}}\cap\mathcal{G}^{\perp}$, so $x=y$, and $x\in\mathcal{G}$, which implies that $\mathcal{G}$ is closed.","Suppose that $\left\{ \mathcal{H}_n \, \big| \, n\in\mathbb{N} \right\}$ is a set of orthogonal closed subspaces of a Hilbert space $\mathcal{H}$. We define the infinite direct sum   $$\bigoplus_{n=1}^{\infty} \mathcal{H}_n := \left\{ \sum_{n=1}^{\infty} x_n \, \big| \, x_n \in \mathcal{H}_n \, \text{and} \, \sum_{n=1}^{\infty} \|\ x_n \|^2 < \infty \right\} . $$    Prove that $\oplus_{n=1}^{\infty} \mathcal{H}_n$ is a closed linear subspace of $\mathcal{H}$. I feel like I proved this in a round about manner and am looking for critiques or a 'better' way to prove the result. Here is my proof: Without loss of generality let $\left\{ e_k \, \big| \, k \in A \right\}$ be an orthonormal set in the Hilbert space $\mathcal{H}$. Then, for each $\mathcal{H}_n$ there is some $A_n$ such that $\left\{ e_k \, \big| \, k\in A_n \right\}$ is a orthonormal set. Additionally, $\displaystyle\bigcup_{n=1}^{\infty} A_n = A$.  Let  $$\mathcal{G}:=\bigoplus_{n=1}^{\infty} \mathcal{H}_n := \left\{ \sum_{n=1}^{\infty} x_n \, \big| \, x_n \in \mathcal{H}_n \, \text{and} \, \sum_{n=1}^{\infty} \|\ x_n \|^2 < \infty \right\} . $$ First we note that $\mathcal{G}$ is well-defined since $\sum_{n=1}^{\infty} x_n$, for $x_n \in \mathcal{H}_n$ converges if and only if $\sum_{n=1}^{\infty} \|\ x_n \|^2 $ converges, which we have by construction of $\mathcal{G}$. Note that the convergent unordered sums can be added term by term, suppose that  $$\sum_{n \in A} x_n := x, \quad \sum_{n \in A} y_n := y$$ Take $\epsilon>0$, there are finite sets $I, J \subset A$, such that  $$\|\ \sum_{k \in I}  x_k - x \|\ < \frac{\epsilon}{2}, \quad \quad \|\ \sum_{k \in J} y_k - y \|\ < \frac{\epsilon}{2}.$$ It follows that if $I \cup J \subset K$ is a finite subset of $A$, then  \begin{align*} \|\ \sum_{k \in K} (x_k + y_k) - (x+y) \|\ &\leq \|\ \sum_{k \in K} x_k - x \|\ + \|\ \sum_{k \in K} y_k - y \|\ \\ &< \frac{\epsilon}{2}+\frac{\epsilon}{2} \\ &=\epsilon. \end{align*} If $x,y \in \mathcal{G}$, with $$x=\sum_{k\in A} x_k e_k , \quad y=\sum_{k \in A} y_k e_k $$ then $$x+y = \sum_{k \in A} (x_k + y_k)e_k.$$ Now since $$|x_k + y_k|^2 \leq (|x_k| + |y_k|)^2 \leq 2(\max\left\{|x_k|,|y_k|\right\})^2 \leq 4(|x_k|^2 + |y_k|^2),$$ it follows that $$\sum_{k \in A} |x_k + y_k|^2 \leq 4\left( \sum_{k \in A} |x_k|^2 + \sum_{k\in A} |y_k|^2 \right) < \infty,$$ so $x+y\in \mathcal{G}$. Similarly, we also have that $\lambda x \in \mathcal{G}$ for all $\lambda \in \mathbb{C}$ and $x\in\mathcal{G}$, so $\mathcal{G}$ is a linear subspace. Now to prove that $\mathcal{G}$ is closed, first note that $\overline{\mathcal{G}} \cap \mathcal{G}^{\perp} = \left\{0\right\}$ because if $x\in\overline{\mathcal{G}}\cap\mathcal{G}^{\perp}$, then $\mathcal{G}=\overline{\mathcal{G}}^{\perp}$ we have $x\perp x$ so $x=0$. Now suppose that $x \in \mathcal{G}$. Let $$y:=\sum_{k\in A} \langle e_k , x \rangle e_k.$$ By Bessel's inequality, $$\sum_{k \in A} | \langle e_k , x \rangle |^2 \leq \|\ x \|^2,$$ so $y\in\mathcal{G}$. Moreover, $\langle e_k , x \rangle = \langle e_k , y \rangle$ for all $k\in A$, which implies that $x-y\in\mathcal{G}^{\perp}$. Since $x,y\in\overline{\mathcal{G}}$, it follows that $x-y\in\overline{\mathcal{G}}\cap\mathcal{G}^{\perp}$, so $x=y$, and $x\in\mathcal{G}$, which implies that $\mathcal{G}$ is closed.",,"['real-analysis', 'functional-analysis', 'proof-verification', 'hilbert-spaces']"
54,Does the multiplication operator $T_f$ given by $T_f\phi = f\phi$ have a discrete spectrum? Can a multiplication operator have continuous spectrum?,Does the multiplication operator  given by  have a discrete spectrum? Can a multiplication operator have continuous spectrum?,T_f T_f\phi = f\phi,"Consider the multiplication operator $T_f:L^2(\mathbb{R}) \to L^2(\mathbb{R})$ given by $$ \phi(x) \mapsto (T_f\phi)(x) = f(x)\phi(x), $$ where $$ f(x) = \begin{cases} 1, \quad x \ge 0, \\ 0, \quad x < 0. \end{cases} $$ So I checked and found that the spectrum of this operator is $\sigma(T_f) = \sigma_p(T_f) = \{0,1\}$. That is, $(T_f-\lambda I)$ is not injective when $\lambda = 0,1$, and hence the spectrum is simply the point spectrum. Now I have read that the point spectrum $\sigma_p$ has a subset $\sigma_d$ called the discrete spectrum. Are $0$ and $1$ in the discrete spectrum $\sigma_d$ or are they in $\sigma_p\setminus \sigma_d$? One of the conditions for $\lambda$ to be in $\sigma_d$ is that 'the root subspace $L_\lambda(A)$ corresponding to $\lambda$ is finite-dimensional. It seems to me that the eigenfunctions corresponding to, for example, $\lambda = 0$ form an infinite dimensional subspace. I.e. $\phi(x) \in \mathbb{R}$ for $x <0$ and $\phi(x) = 0$ for $x \ge 0$. So this means that $0$ and $1$ are not in the discrete spectrum? Is it possible to construct a simple explicit example of a multiplication operator that has a continuous spectrum? In this case $(T_f-\lambda I)$ should be injective but only have a dense image (i.e. not the whole space). Can a multiplication operator have this type of spectrum?","Consider the multiplication operator $T_f:L^2(\mathbb{R}) \to L^2(\mathbb{R})$ given by $$ \phi(x) \mapsto (T_f\phi)(x) = f(x)\phi(x), $$ where $$ f(x) = \begin{cases} 1, \quad x \ge 0, \\ 0, \quad x < 0. \end{cases} $$ So I checked and found that the spectrum of this operator is $\sigma(T_f) = \sigma_p(T_f) = \{0,1\}$. That is, $(T_f-\lambda I)$ is not injective when $\lambda = 0,1$, and hence the spectrum is simply the point spectrum. Now I have read that the point spectrum $\sigma_p$ has a subset $\sigma_d$ called the discrete spectrum. Are $0$ and $1$ in the discrete spectrum $\sigma_d$ or are they in $\sigma_p\setminus \sigma_d$? One of the conditions for $\lambda$ to be in $\sigma_d$ is that 'the root subspace $L_\lambda(A)$ corresponding to $\lambda$ is finite-dimensional. It seems to me that the eigenfunctions corresponding to, for example, $\lambda = 0$ form an infinite dimensional subspace. I.e. $\phi(x) \in \mathbb{R}$ for $x <0$ and $\phi(x) = 0$ for $x \ge 0$. So this means that $0$ and $1$ are not in the discrete spectrum? Is it possible to construct a simple explicit example of a multiplication operator that has a continuous spectrum? In this case $(T_f-\lambda I)$ should be injective but only have a dense image (i.e. not the whole space). Can a multiplication operator have this type of spectrum?",,"['functional-analysis', 'operator-theory', 'spectral-theory']"
55,"Calculate the distance in $C[0,1]$ from the function $x(t) = t^2$ to the linear hull of the functions $y(t) = t$ and $z(t) = \sin t.$",Calculate the distance in  from the function  to the linear hull of the functions  and,"C[0,1] x(t) = t^2 y(t) = t z(t) = \sin t.","The following question is taken from ' Banach space Theory: The Basis for Linear and Nonlinear Analysis ', Chapter $1,$ question $1.8.$ Question: Calculate the distance in $C[0,1]$ from the function $x(t) = t^2$ to the linear hull of the functions $y(t) = t$ and $z(t) = \sin t.$ So we are asked to calculate  $$\inf_{a,b\in\mathbb{R}} \max_{t\in[0,1]}|t^2-at-b\sin(t)|.$$ Fix $a,b\in\mathbb{R}$ and denote  $$f(t) = t^2-at-b\sin(t).$$ By differentiation, we obtain  $$f'(t) = 2t-a-b\cos(t),$$ To find stationary point, we set  $$f'(t) = 2t - a-b\cos(t)=0,$$ which implies that  $$2t-a=b\cos(t).$$ But I have trouble solving the equation above. Any hint would be appreciated.","The following question is taken from ' Banach space Theory: The Basis for Linear and Nonlinear Analysis ', Chapter $1,$ question $1.8.$ Question: Calculate the distance in $C[0,1]$ from the function $x(t) = t^2$ to the linear hull of the functions $y(t) = t$ and $z(t) = \sin t.$ So we are asked to calculate  $$\inf_{a,b\in\mathbb{R}} \max_{t\in[0,1]}|t^2-at-b\sin(t)|.$$ Fix $a,b\in\mathbb{R}$ and denote  $$f(t) = t^2-at-b\sin(t).$$ By differentiation, we obtain  $$f'(t) = 2t-a-b\cos(t),$$ To find stationary point, we set  $$f'(t) = 2t - a-b\cos(t)=0,$$ which implies that  $$2t-a=b\cos(t).$$ But I have trouble solving the equation above. Any hint would be appreciated.",,"['calculus', 'real-analysis', 'functional-analysis']"
56,Adjoint of the attached space in operator algebras,Adjoint of the attached space in operator algebras,,"J. Conway considers in his book ""A course in operator theory"" a linear subspace $\mathcal S \subset B(\mathcal H)$ of bounded operators acting on a Hilbert space. He defines the attached space as $$ \mathrm{Ref} \ \mathcal S = \{ T \in B(\mathcal H) \ | \ \forall h \in \mathcal H \ : \ Th \in \mathrm{cl} (\mathcal S h)  \}, $$ where $\mathrm{cl}$ denotes closure of a set and $\mathcal S h$ is the set of all vectors of the form $Ah$ with $A \in \mathcal S$. He leaves as a simple exercise to prove that $\mathrm{Ref} \ (\mathcal S^*)=(\mathrm{Ref} \ \mathcal S)^*$ - that is space attached to the set of adjoint operators consists precisely of adjoints of elements in the attached space. I have no idea how to proceed here. It seems that it is necessary to somehow relate ranges of an operator and its adjoint, but such relation certainly doesn't exist - instead range is connected with kernel. Explanation of this equality or hints with be appreciated","J. Conway considers in his book ""A course in operator theory"" a linear subspace $\mathcal S \subset B(\mathcal H)$ of bounded operators acting on a Hilbert space. He defines the attached space as $$ \mathrm{Ref} \ \mathcal S = \{ T \in B(\mathcal H) \ | \ \forall h \in \mathcal H \ : \ Th \in \mathrm{cl} (\mathcal S h)  \}, $$ where $\mathrm{cl}$ denotes closure of a set and $\mathcal S h$ is the set of all vectors of the form $Ah$ with $A \in \mathcal S$. He leaves as a simple exercise to prove that $\mathrm{Ref} \ (\mathcal S^*)=(\mathrm{Ref} \ \mathcal S)^*$ - that is space attached to the set of adjoint operators consists precisely of adjoints of elements in the attached space. I have no idea how to proceed here. It seems that it is necessary to somehow relate ranges of an operator and its adjoint, but such relation certainly doesn't exist - instead range is connected with kernel. Explanation of this equality or hints with be appreciated",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'operator-algebras']"
57,Counterexample to Restrictions of Radon Measures are Radon,Counterexample to Restrictions of Radon Measures are Radon,,"Define a Radon measure to be inner regular on open sets, outer regular on Borel sets, and finite on compact sets, as in Folland's Real Analysis.  A restriction of a $\sigma$-finite Radon measure to a Borel measurable subspace is again a Radon measure.  However, this is not necessarily true for a non-$\sigma$-finite measure.  I am trying to understand the counterexample given in problem 7.13 of Folland. $X = \mathbb{R}\times \mathbb{R}$ where the first copy of $\mathbb{R}$ has the usual topology and the second copy has the discrete topology.  $\mu$ is the unique Radon measure on $X$ induced by the functional on $C_{c}(X)$ defined by $f\mapsto \sum_{y}\int f(x,y)dx$ where $dx$ denotes integration with the Lebesgue measure.  According to an answer in the post Reconciling several different definitions of Radon measures , $\mu$ is given by (1) If $E$ has only countably many nonempty horizontal slices $E^{y}$, then $\mu(E)$ is the sum of the Lebesgue measure of each slice $E^{y}$ (2) If there are uncountably many nonempty horizontal slices $E^{y}$, then $\mu(E) = \infty$ The claim is that if we restrict $\mu$ to $\mathbb{R}\backslash \{0\}\times \mathbb{R}$, it is no longer Radon.  Why is this true?","Define a Radon measure to be inner regular on open sets, outer regular on Borel sets, and finite on compact sets, as in Folland's Real Analysis.  A restriction of a $\sigma$-finite Radon measure to a Borel measurable subspace is again a Radon measure.  However, this is not necessarily true for a non-$\sigma$-finite measure.  I am trying to understand the counterexample given in problem 7.13 of Folland. $X = \mathbb{R}\times \mathbb{R}$ where the first copy of $\mathbb{R}$ has the usual topology and the second copy has the discrete topology.  $\mu$ is the unique Radon measure on $X$ induced by the functional on $C_{c}(X)$ defined by $f\mapsto \sum_{y}\int f(x,y)dx$ where $dx$ denotes integration with the Lebesgue measure.  According to an answer in the post Reconciling several different definitions of Radon measures , $\mu$ is given by (1) If $E$ has only countably many nonempty horizontal slices $E^{y}$, then $\mu(E)$ is the sum of the Lebesgue measure of each slice $E^{y}$ (2) If there are uncountably many nonempty horizontal slices $E^{y}$, then $\mu(E) = \infty$ The claim is that if we restrict $\mu$ to $\mathbb{R}\backslash \{0\}\times \mathbb{R}$, it is no longer Radon.  Why is this true?",,"['functional-analysis', 'measure-theory']"
58,"Proving that a family of exponential operators has a uniform bound, without semigroup theory.","Proving that a family of exponential operators has a uniform bound, without semigroup theory.",,"Set $T\in B(X)$ for some arbitrary Banach space $X$. Define the family of operators $\{e^{tT}\}_{t\in\mathbb R}$ via the holomorphic functional calculus. I have been able to prove, using basic facts about the functional calculus that $\frac{d}{dt}e^{tT}=Te^{tT}$, and that the family forms a group under multiplication. What I am now trying to prove is that if $\sigma(T)\subset\{z:Rez<0\}$, then $\sup_{t\geq 0} \|e^{tT}\|\leq M$ for some positive $M$. In the finite dimensional case, I know we can decompose into Jordan's normal form to get the result, but I do not know how to pursue that avenue for an arbitrary bounded operator. My idea is to somehow use the spectral mapping theorem. This gives us that the spectral radius $\rho (e^{tT})\to 0$ as $t\to \infty$, and that this convergence is exponential. Unfortunately, as $X$ need not be a Hilbert space and $T$ need not be normal we do not know that $\rho(e^{tT})=\|e^{tT}\|$. What I am hoping is that there is some way we can extend the estimation lemma to the holomorphic functional calculus, at least in certain cases of which this is one, which would then give the result, but I have not been able to find such a thing. From scouring the net I know that this is an archetypal problem of continuous semi-group theory, but I encountered this while studying Rudin's Functional Analysis , specifically in relation to his chapter on Banach algebras and the holomorphic functional calculus, which makes me believe that there is a simpler solution using basic theory from these areas. Any help would be appreciated.","Set $T\in B(X)$ for some arbitrary Banach space $X$. Define the family of operators $\{e^{tT}\}_{t\in\mathbb R}$ via the holomorphic functional calculus. I have been able to prove, using basic facts about the functional calculus that $\frac{d}{dt}e^{tT}=Te^{tT}$, and that the family forms a group under multiplication. What I am now trying to prove is that if $\sigma(T)\subset\{z:Rez<0\}$, then $\sup_{t\geq 0} \|e^{tT}\|\leq M$ for some positive $M$. In the finite dimensional case, I know we can decompose into Jordan's normal form to get the result, but I do not know how to pursue that avenue for an arbitrary bounded operator. My idea is to somehow use the spectral mapping theorem. This gives us that the spectral radius $\rho (e^{tT})\to 0$ as $t\to \infty$, and that this convergence is exponential. Unfortunately, as $X$ need not be a Hilbert space and $T$ need not be normal we do not know that $\rho(e^{tT})=\|e^{tT}\|$. What I am hoping is that there is some way we can extend the estimation lemma to the holomorphic functional calculus, at least in certain cases of which this is one, which would then give the result, but I have not been able to find such a thing. From scouring the net I know that this is an archetypal problem of continuous semi-group theory, but I encountered this while studying Rudin's Functional Analysis , specifically in relation to his chapter on Banach algebras and the holomorphic functional calculus, which makes me believe that there is a simpler solution using basic theory from these areas. Any help would be appreciated.",,"['functional-analysis', 'banach-algebras', 'functional-calculus']"
59,If $\dim (\operatorname{Ker}f)^{\perp} = 1$ then $f$ is continuous for $f$ linear functional on Hilbert space,If  then  is continuous for  linear functional on Hilbert space,\dim (\operatorname{Ker}f)^{\perp} = 1 f f,"I need to prove the following result: If $\dim (\operatorname{Ker}f)^{\perp} = 1$ then $f$ is continuous for $f$ linear   functional on Hilbert space. I have the following ingredients in mind: Use sequential characterization of continuity knowing that since $\operatorname{Ker}(f)$ is closed, projection theorem gives that $x_n = m_n + u_n \to x = m + u$ is equivalent to $m_n \to m \land u_n \to u$. Someone suggested me to further prove that $H = \operatorname{Ker}(f) \oplus \operatorname{Ker}(f)^{\perp} = \operatorname{Ker}(f) \oplus\operatorname{Img}(f)$ but I don't see how this is of any help. Any ideas?","I need to prove the following result: If $\dim (\operatorname{Ker}f)^{\perp} = 1$ then $f$ is continuous for $f$ linear   functional on Hilbert space. I have the following ingredients in mind: Use sequential characterization of continuity knowing that since $\operatorname{Ker}(f)$ is closed, projection theorem gives that $x_n = m_n + u_n \to x = m + u$ is equivalent to $m_n \to m \land u_n \to u$. Someone suggested me to further prove that $H = \operatorname{Ker}(f) \oplus \operatorname{Ker}(f)^{\perp} = \operatorname{Ker}(f) \oplus\operatorname{Img}(f)$ but I don't see how this is of any help. Any ideas?",,"['functional-analysis', 'hilbert-spaces', 'orthogonality']"
60,Proof verification: why this formula holds?,Proof verification: why this formula holds?,,"For $A= (A_1,\cdots,A_d)\in {\cal L}(E)^d$  such that $A_iA_j=A_jA_i$ for all $i,j$. Why   $$\sum_{f\in F(n,d)} A_{f}^*LA_{f}=\displaystyle\sum_{|\alpha|=n}\frac{n!}{\alpha!}{A^*}^{\alpha}LA^{\alpha},\,\;\forall\, L\in\mathcal{L}(E)?$$   Note that $F(n,d)$ denotes the set of all functions from $\{1,\cdots,n\}$ into  $\{1,\cdots,d\}$ and $A_f:=A_{f(1)}\cdots A_{f(n)}$, for $f\in F(n,d)$. Also $\alpha = (\alpha_1, \alpha_2,...,\alpha_d) \in \mathbb{Z}_+^d;\;\alpha!: =\alpha_1!\cdots\alpha_d!,\;|\alpha|:=\displaystyle\sum_{j=1}^d|\alpha_j|$; $A^*=(A_1^*,\cdots,A_d^*)$ and $A^\alpha:=A_1^{\alpha_1} A_2^{\alpha_2}\cdots A_d^{\alpha_d}$. I'm facing dificulties to understand the following proof: Proof $$\sum_{f\in F(n,d)} A_{f}^*LA_{f}=\displaystyle\sum_{|\alpha|=n}{n\choose \alpha}\ {A^*}^{\alpha}LA^{\alpha}\ .$$ It is just an instance of the expansion of the $n$-power of the sum of  $d$ commuting objects in a ring, $$(X_1+\dots X_d)^n=\sum_{\alpha\in\mathbb{N}^d\atop |a|=n} {n\choose \alpha}X^\alpha, $$ with $X:=X_1^{\alpha_1}X_2^{\alpha_2}\dots X_d^{\alpha_d}.$ So  if $X_j$ is the linear operator on $\mathcal{L}(H)$ defined by $L\mapsto A_j^*LA_j$ , we get the  desired  formula.","For $A= (A_1,\cdots,A_d)\in {\cal L}(E)^d$  such that $A_iA_j=A_jA_i$ for all $i,j$. Why   $$\sum_{f\in F(n,d)} A_{f}^*LA_{f}=\displaystyle\sum_{|\alpha|=n}\frac{n!}{\alpha!}{A^*}^{\alpha}LA^{\alpha},\,\;\forall\, L\in\mathcal{L}(E)?$$   Note that $F(n,d)$ denotes the set of all functions from $\{1,\cdots,n\}$ into  $\{1,\cdots,d\}$ and $A_f:=A_{f(1)}\cdots A_{f(n)}$, for $f\in F(n,d)$. Also $\alpha = (\alpha_1, \alpha_2,...,\alpha_d) \in \mathbb{Z}_+^d;\;\alpha!: =\alpha_1!\cdots\alpha_d!,\;|\alpha|:=\displaystyle\sum_{j=1}^d|\alpha_j|$; $A^*=(A_1^*,\cdots,A_d^*)$ and $A^\alpha:=A_1^{\alpha_1} A_2^{\alpha_2}\cdots A_d^{\alpha_d}$. I'm facing dificulties to understand the following proof: Proof $$\sum_{f\in F(n,d)} A_{f}^*LA_{f}=\displaystyle\sum_{|\alpha|=n}{n\choose \alpha}\ {A^*}^{\alpha}LA^{\alpha}\ .$$ It is just an instance of the expansion of the $n$-power of the sum of  $d$ commuting objects in a ring, $$(X_1+\dots X_d)^n=\sum_{\alpha\in\mathbb{N}^d\atop |a|=n} {n\choose \alpha}X^\alpha, $$ with $X:=X_1^{\alpha_1}X_2^{\alpha_2}\dots X_d^{\alpha_d}.$ So  if $X_j$ is the linear operator on $\mathcal{L}(H)$ defined by $L\mapsto A_j^*LA_j$ , we get the  desired  formula.",,"['functional-analysis', 'proof-verification', 'multinomial-coefficients']"
61,Characterization of the resolvent of a bounded operator on a Hilbert space,Characterization of the resolvent of a bounded operator on a Hilbert space,,"Let $E$ be a complex Hilbert space  with inner product $\langle\cdot\;| \;\cdot\rangle$ and the norm $\|\cdot\|$. If $A\in\mathcal{L}(E)$, why    \begin{align*} \rho(A):=\mathbb{C}\setminus\sigma(A) & = \{\lambda \in \mathbb{C}\colon \exists d>0; \|(\lambda \mathrm{Id}-A)y\|\geqslant d\|y\|,\;\forall\;y\in E\\  &\phantom{+++}\;\hbox{and}\;\;\mathrm{dist}(x, \mathrm{Im}(\lambda\mathrm{Id}-A))=0\,\forall x\in E\;\}? \end{align*}   where $\mathrm{dist}$ is the distance induced by $\|\cdot\|$.","Let $E$ be a complex Hilbert space  with inner product $\langle\cdot\;| \;\cdot\rangle$ and the norm $\|\cdot\|$. If $A\in\mathcal{L}(E)$, why    \begin{align*} \rho(A):=\mathbb{C}\setminus\sigma(A) & = \{\lambda \in \mathbb{C}\colon \exists d>0; \|(\lambda \mathrm{Id}-A)y\|\geqslant d\|y\|,\;\forall\;y\in E\\  &\phantom{+++}\;\hbox{and}\;\;\mathrm{dist}(x, \mathrm{Im}(\lambda\mathrm{Id}-A))=0\,\forall x\in E\;\}? \end{align*}   where $\mathrm{dist}$ is the distance induced by $\|\cdot\|$.",,"['functional-analysis', 'spectral-theory']"
62,$AB$ is compact iff $BA$ is,is compact iff  is,AB BA,"Is it in general true or false that for $A, B \in \mathcal L (\mathcal H)$, $A B \in \mathcal S^\infty (\mathcal H) \Longleftrightarrow B A \in \mathcal S^\infty (\mathcal H )$? I think it holds only for $A, B$ normal. Could someone tell me if I'm right? Proof (sketch) for normal operators: $A B \in \mathcal S^\infty (\mathcal H)$, then $\psi_n \rightharpoonup 0$ implies $A B \psi_n \to 0$, because $A, B$ normal, this implies $A^* B^* \psi_n = (BA)^* \psi_n \to 0$, i.e. $(BA)^* \in \mathcal S^\infty(\mathcal H)$ and this is equivalent to $BA \in \mathcal S^\infty (\mathcal H)$. Counterexample for the general case (I think this is one): $\mathcal H = \ell^2(\mathbb N)$ and denote projections in Bra-Ket notation like $\vert n \rangle \langle n \vert$. $$A := \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \frac{1}{2n+1} \vert 2n+1\rangle \langle 2n+1\vert + \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \vert 2n \rangle \langle 2n \vert$$ $$B := \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \vert 2n+1\rangle \langle n \vert$$ Then $AB$ is compact but $BA$ is not (since it maps norm-preserving on the infinite-dimensional subspace of even $n$s)...","Is it in general true or false that for $A, B \in \mathcal L (\mathcal H)$, $A B \in \mathcal S^\infty (\mathcal H) \Longleftrightarrow B A \in \mathcal S^\infty (\mathcal H )$? I think it holds only for $A, B$ normal. Could someone tell me if I'm right? Proof (sketch) for normal operators: $A B \in \mathcal S^\infty (\mathcal H)$, then $\psi_n \rightharpoonup 0$ implies $A B \psi_n \to 0$, because $A, B$ normal, this implies $A^* B^* \psi_n = (BA)^* \psi_n \to 0$, i.e. $(BA)^* \in \mathcal S^\infty(\mathcal H)$ and this is equivalent to $BA \in \mathcal S^\infty (\mathcal H)$. Counterexample for the general case (I think this is one): $\mathcal H = \ell^2(\mathbb N)$ and denote projections in Bra-Ket notation like $\vert n \rangle \langle n \vert$. $$A := \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \frac{1}{2n+1} \vert 2n+1\rangle \langle 2n+1\vert + \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \vert 2n \rangle \langle 2n \vert$$ $$B := \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \vert 2n+1\rangle \langle n \vert$$ Then $AB$ is compact but $BA$ is not (since it maps norm-preserving on the infinite-dimensional subspace of even $n$s)...",,"['functional-analysis', 'operator-theory', 'compact-operators']"
63,Definition of extreme point which involve two norm $1$ points,Definition of extreme point which involve two norm  points,1,"I migrate the following post from Maths Overflow. Let $E$ be a Banach space.  Denote $B_E$ and $S_E$ to be the unit ball and sphere of $E$ respectively. A common definition of extreme point of $B_E$ that I have came across is the following: Definition $1$ $x$ is an extreme point of $B_E$ if $$x = \frac{1}{2}(y+z)$$ for some $y,z\in B_E,$ then $x=y=z.$ I have seen another definition of extreme point which involves $y$ and $z$ to be norm $1.$ For example, Lei Li et al. paper, page $549,$ paragraph starts with 'On the other hand,...' and Botelho's et al. paper, page $823,$ first sentence of the proof of Theorem $3$ . They use the following definition of extreme point: Definition $2$ $x$ is an extreme point of $B_E$ if    $$x = \frac{1}{2} (y+z)$$ for some $y,z\in S_E,$ then $x=y=z.$ I can prove that Definition $1$ implies Definition $2.$ But I am not able to prove its converse.  So here is my question: Question: Does Definition $2$ imply Definition $1?$ If yes, can prove a proof or reference? Otherwise, can provide counterexample? ................................................................................................................................................................... Based on the discussion at Maths Overflow, it seems that Definition $2$ implies Definition $1.$ To prove it, it suffices to prove that Definition $2$ implies that $\|x\|=1.$","I migrate the following post from Maths Overflow. Let $E$ be a Banach space.  Denote $B_E$ and $S_E$ to be the unit ball and sphere of $E$ respectively. A common definition of extreme point of $B_E$ that I have came across is the following: Definition $1$ $x$ is an extreme point of $B_E$ if $$x = \frac{1}{2}(y+z)$$ for some $y,z\in B_E,$ then $x=y=z.$ I have seen another definition of extreme point which involves $y$ and $z$ to be norm $1.$ For example, Lei Li et al. paper, page $549,$ paragraph starts with 'On the other hand,...' and Botelho's et al. paper, page $823,$ first sentence of the proof of Theorem $3$ . They use the following definition of extreme point: Definition $2$ $x$ is an extreme point of $B_E$ if    $$x = \frac{1}{2} (y+z)$$ for some $y,z\in S_E,$ then $x=y=z.$ I can prove that Definition $1$ implies Definition $2.$ But I am not able to prove its converse.  So here is my question: Question: Does Definition $2$ imply Definition $1?$ If yes, can prove a proof or reference? Otherwise, can provide counterexample? ................................................................................................................................................................... Based on the discussion at Maths Overflow, it seems that Definition $2$ implies Definition $1.$ To prove it, it suffices to prove that Definition $2$ implies that $\|x\|=1.$",,"['real-analysis', 'functional-analysis', 'reference-request', 'banach-spaces']"
64,Unique bounding rectangle.,Unique bounding rectangle.,,"Let $K\in\mathbb{R}^{2}$ be compact and connected. Then we can always find rectangles that bound $K$. Moreover, we can find the minimum-area such a bounding rectangle could have. My question is this: Is it true that if there are two or more minimum-area bounding rectangles, then those rectangles are squares? Any pointing to the right direction would be much appreciated.","Let $K\in\mathbb{R}^{2}$ be compact and connected. Then we can always find rectangles that bound $K$. Moreover, we can find the minimum-area such a bounding rectangle could have. My question is this: Is it true that if there are two or more minimum-area bounding rectangles, then those rectangles are squares? Any pointing to the right direction would be much appreciated.",,"['real-analysis', 'geometry', 'functional-analysis']"
65,Good references(books or lecture notes) for self study on Banach Algebra.,Good references(books or lecture notes) for self study on Banach Algebra.,,"Can someone suggest some good references(books or lecture notes) for self study on Banach Algebra? By background being a first course in Functional Analysis and a course in Measure Theory. Most preferably other than some of known to be: Rudin,  Functional Analysis. Conway, Functional Analysis. Thank You.","Can someone suggest some good references(books or lecture notes) for self study on Banach Algebra? By background being a first course in Functional Analysis and a course in Measure Theory. Most preferably other than some of known to be: Rudin,  Functional Analysis. Conway, Functional Analysis. Thank You.",,"['functional-analysis', 'measure-theory', 'reference-request', 'soft-question', 'banach-algebras']"
66,A kind of Minkowski inequality for integral,A kind of Minkowski inequality for integral,,"Problem: Let $f$ be a measurable nonnegative function on $[0,1]^2$, and $1\leq r < p < \infty$. Then, show that   $$ \left(\int_{0}^{1}\left(\int_{0}^{1}f^{r}(x,y)dy\right)^{p/r} \right)^{1/p} \leq \left(\int_{0}^{1}\left(\int_{0}^{1}f^{p}(x,y)dy\right)^{r/p} \right)^{1/r}.$$   Hint : Let $s=\frac{p}{r}$ and  $F(x) = \int_{0}^{1}f^{r}(x,y)dy$ and consider appropriate function $h \in L_{s'}[0,1].$ My attempt: By following hints, I've got this result. For any $h \in L_{s'}[0,1],$ from the Tonelli's theorem, $$ \int_{0}^{1}F(x)|h(x)|dx = \int_{0}^{1}\int_{0}^{1}f^{r}(x,y)dy|h(x)|dx = \int_{0}^{1}\int_{0}^{1}f^{r}(x,y)|h(x)|dxdy, $$ and by using the Holder's inequality, $$ \int_{0}^{1}\int_{0}^{1}f^{r}(x,y)|h(x)|dxdy \leq ||h||_{s'}\int_{0}^{1}||f^{r}(x,y)||_{s}  = ||h||_{s'}\int_{0}^{1}\left(\int_{0}^{1}f^{p}(x,y)dx \right)^{r/p}dy.$$ However, now I have no idea how to proceed. I know that  $$||F||_{s}^{1/r} = \left(\int_{0}^{1}\left(\int_{0}^{1}f^{r}(x,y)dy\right)^{p/r} \right)^{1/p} $$ so I tried to choose suitable $h \in L_{s'}[0,1]$ with $||h||_{L_{s'}[0,1]}$ such that $$\int_{0}^{1}F(x)|h(x)|dx =||F||_{s}.$$ If we find such function, than $$||F||_{s} \leq \int_{0}^{1}\left(\int_{0}^{1}f^{p}(x,y)dx \right)^{r/p}dy$$ so we are done by taking $1/r$-th root. However I haven't find yet. If you gave me one more step for this problem, I would appreciate much about it.","Problem: Let $f$ be a measurable nonnegative function on $[0,1]^2$, and $1\leq r < p < \infty$. Then, show that   $$ \left(\int_{0}^{1}\left(\int_{0}^{1}f^{r}(x,y)dy\right)^{p/r} \right)^{1/p} \leq \left(\int_{0}^{1}\left(\int_{0}^{1}f^{p}(x,y)dy\right)^{r/p} \right)^{1/r}.$$   Hint : Let $s=\frac{p}{r}$ and  $F(x) = \int_{0}^{1}f^{r}(x,y)dy$ and consider appropriate function $h \in L_{s'}[0,1].$ My attempt: By following hints, I've got this result. For any $h \in L_{s'}[0,1],$ from the Tonelli's theorem, $$ \int_{0}^{1}F(x)|h(x)|dx = \int_{0}^{1}\int_{0}^{1}f^{r}(x,y)dy|h(x)|dx = \int_{0}^{1}\int_{0}^{1}f^{r}(x,y)|h(x)|dxdy, $$ and by using the Holder's inequality, $$ \int_{0}^{1}\int_{0}^{1}f^{r}(x,y)|h(x)|dxdy \leq ||h||_{s'}\int_{0}^{1}||f^{r}(x,y)||_{s}  = ||h||_{s'}\int_{0}^{1}\left(\int_{0}^{1}f^{p}(x,y)dx \right)^{r/p}dy.$$ However, now I have no idea how to proceed. I know that  $$||F||_{s}^{1/r} = \left(\int_{0}^{1}\left(\int_{0}^{1}f^{r}(x,y)dy\right)^{p/r} \right)^{1/p} $$ so I tried to choose suitable $h \in L_{s'}[0,1]$ with $||h||_{L_{s'}[0,1]}$ such that $$\int_{0}^{1}F(x)|h(x)|dx =||F||_{s}.$$ If we find such function, than $$||F||_{s} \leq \int_{0}^{1}\left(\int_{0}^{1}f^{p}(x,y)dx \right)^{r/p}dy$$ so we are done by taking $1/r$-th root. However I haven't find yet. If you gave me one more step for this problem, I would appreciate much about it.",,"['real-analysis', 'functional-analysis', 'lp-spaces']"
67,Under which conditions $\mathcal{L}^{S}(E)=\mathcal{L}(E)$?,Under which conditions ?,\mathcal{L}^{S}(E)=\mathcal{L}(E),"Let $E$ be a complex Hilbert space, $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$. For $S\in \mathcal{L}(E)^+$, we set $$\mathcal{L}^{S}(E)=\left\{A\in \mathcal{L}(E);\;\exists \,M> 0;\;\|S^{1/2}Ay\| \leq M\|S^{1/2}y\|,\;\forall\,y\in E\right\}.$$ If $SA=AS$ and $S^{1/2}$ is surjective then $\mathcal{L}^{S}(E)=\mathcal{L}(E)$. I want to get a more weaker conditions under which $\mathcal{L}^{S}(E)=\mathcal{L}(E)$. Do you think if $S$ is injective operator with a closed range, the above equality holds? Thank you!","Let $E$ be a complex Hilbert space, $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$. For $S\in \mathcal{L}(E)^+$, we set $$\mathcal{L}^{S}(E)=\left\{A\in \mathcal{L}(E);\;\exists \,M> 0;\;\|S^{1/2}Ay\| \leq M\|S^{1/2}y\|,\;\forall\,y\in E\right\}.$$ If $SA=AS$ and $S^{1/2}$ is surjective then $\mathcal{L}^{S}(E)=\mathcal{L}(E)$. I want to get a more weaker conditions under which $\mathcal{L}^{S}(E)=\mathcal{L}(E)$. Do you think if $S$ is injective operator with a closed range, the above equality holds? Thank you!",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'operator-algebras']"
68,Is a quotient of topological vector space a topological vector space,Is a quotient of topological vector space a topological vector space,,"I should prove or disprove that quotient of topological vector space is a topological vector space. I think that it is. May I reduce my prove to proof of following statement? $X$ - topological vector space, $M \subset X$ - linear subspace of $X$. Let $A$ - vector addition in $X/M$, W - neighbourhood of origin $o$ in $X/M$. Then i should prove that $A^{-1}(W)$ is a neighbourhood of $(o, o)$ in $X/M \times X/M$. But I have no idea how can I prove this fact.","I should prove or disprove that quotient of topological vector space is a topological vector space. I think that it is. May I reduce my prove to proof of following statement? $X$ - topological vector space, $M \subset X$ - linear subspace of $X$. Let $A$ - vector addition in $X/M$, W - neighbourhood of origin $o$ in $X/M$. Then i should prove that $A^{-1}(W)$ is a neighbourhood of $(o, o)$ in $X/M \times X/M$. But I have no idea how can I prove this fact.",,"['abstract-algebra', 'functional-analysis', 'topological-vector-spaces']"
69,Convexity of a functional on a Sobolev space,Convexity of a functional on a Sobolev space,,"Denote a specific Sobolev space by $$W^{2,2}(a,b) = \left\lbrace x\in L_2(a,b) : x'\in AC[a,b],\quad x''\in L_2(a,b) \right\rbrace $$ where $AC[a,b]$ is the class of absolutely continuous functions on $[a,b]\subset\mathbb R$. Given smoothing problems in the context of theory of splines (and, as I've witnessed, in statistics) the objective is to minimize the following expression over all $f\in W^{2,2}$: $$M(f) := \left [ \sum_{j=0}^n w_j(f(x_j)-y_j)^2 + \int_a^bf''(x)^2w(x)dx\right ]^{1/2} $$ where $x_0<x_1<\ldots <x_n$ (in $[a,b]$), $w,w_j>0$ (weights) and $y_j\in\mathbb R$ are provided and then the (unique?) minimizer of $M$ is christened the smoothing spline. What about the functional $M : W^{2,2}\to\mathbb R$ itself, though? Is it, perhaps, convex? Even strictly convex? Specifically, for convexity: $$\forall u,v\in W^{2,2}, \forall t\in (0,1), M(tu +(1-t)v)\overset{?}\leq tM(u)+(1-t)M(v).$$ Strict convexity means that whenever $u\neq v$ the inequality is strict. I haven't been able to find any discussion on this particular property of $M$ and it doesn't seem all that obvious either. A thought. Perhaps, we could obtain $$M^2(tu+(1-t)v)\leq t^2M^2(u)+(1-t)^2M^2(v) $$ and state that perhaps $2t(1-t)M(u)M(v)>0$ for $u\neq v$. That won't work. If $u = 0$ with $y_j\equiv 0$, then $M(u) = 0$ and at the same time $$M((1-t)v) = (1-t)^2M(v)< (1-t)M(v), $$ since $0<t<1$, provided $M(v)>0$.","Denote a specific Sobolev space by $$W^{2,2}(a,b) = \left\lbrace x\in L_2(a,b) : x'\in AC[a,b],\quad x''\in L_2(a,b) \right\rbrace $$ where $AC[a,b]$ is the class of absolutely continuous functions on $[a,b]\subset\mathbb R$. Given smoothing problems in the context of theory of splines (and, as I've witnessed, in statistics) the objective is to minimize the following expression over all $f\in W^{2,2}$: $$M(f) := \left [ \sum_{j=0}^n w_j(f(x_j)-y_j)^2 + \int_a^bf''(x)^2w(x)dx\right ]^{1/2} $$ where $x_0<x_1<\ldots <x_n$ (in $[a,b]$), $w,w_j>0$ (weights) and $y_j\in\mathbb R$ are provided and then the (unique?) minimizer of $M$ is christened the smoothing spline. What about the functional $M : W^{2,2}\to\mathbb R$ itself, though? Is it, perhaps, convex? Even strictly convex? Specifically, for convexity: $$\forall u,v\in W^{2,2}, \forall t\in (0,1), M(tu +(1-t)v)\overset{?}\leq tM(u)+(1-t)M(v).$$ Strict convexity means that whenever $u\neq v$ the inequality is strict. I haven't been able to find any discussion on this particular property of $M$ and it doesn't seem all that obvious either. A thought. Perhaps, we could obtain $$M^2(tu+(1-t)v)\leq t^2M^2(u)+(1-t)^2M^2(v) $$ and state that perhaps $2t(1-t)M(u)M(v)>0$ for $u\neq v$. That won't work. If $u = 0$ with $y_j\equiv 0$, then $M(u) = 0$ and at the same time $$M((1-t)v) = (1-t)^2M(v)< (1-t)M(v), $$ since $0<t<1$, provided $M(v)>0$.",,"['functional-analysis', 'optimization', 'lebesgue-integral', 'spline', 'functional-inequalities']"
70,Continuous integral kernels give rise to completely continuous integral operators,Continuous integral kernels give rise to completely continuous integral operators,,"Let $X$ be a compact space, and $K : X\times X\rightarrow \mathbb{C}$ a continuous function. Then how can I see that the integral operator $T_K : L^2(X)\rightarrow L^2(X)$ given by $$T_Kf(x) := \int_X K(x,y)f(y)dy$$ is ""completely continuous""? Also, what is the proper definition of ""completely continuous""? (I've been taking it to mean a compact operator"". This is a footnote in Gelfand-Piatetskii-Shapiro's book Representation theory and automorphic functions .","Let $X$ be a compact space, and $K : X\times X\rightarrow \mathbb{C}$ a continuous function. Then how can I see that the integral operator $T_K : L^2(X)\rightarrow L^2(X)$ given by $$T_Kf(x) := \int_X K(x,y)f(y)dy$$ is ""completely continuous""? Also, what is the proper definition of ""completely continuous""? (I've been taking it to mean a compact operator"". This is a footnote in Gelfand-Piatetskii-Shapiro's book Representation theory and automorphic functions .",,"['real-analysis', 'complex-analysis']"
71,Spectral Decomposition: $A\psi = \lambda \psi \implies f(A)\psi = f(\lambda)\psi$,Spectral Decomposition:,A\psi = \lambda \psi \implies f(A)\psi = f(\lambda)\psi,"I'm reading Simon & Reed's Functional Analysis, and attempting to write out the proof of the functional calculus form of the Spectral Theorem. See this link for their construction of $f(A)$ where $f \in \mathbb{B}(\mathbb{R})$ a bounded Borel function on $\mathbb{R}$: Continuity of the functional calculus form of the Spectral Theorem Let $A \in L(H)$ be self-adjoint. My question is this: How do I show that $A\psi = \lambda \psi \implies f(A)\psi = f(\lambda)\psi$? This argument is easy when proving the same statement in the continuous calculus because when dealing with a continuous $f$ it may be approximated by a sequence $p_n$ that converges to it uniformly, and hence point-wise too. To my knowledge, a bounded Borel function, $f$, has the property that $\exists \{f_n\} \subset C(\sigma(A))$ s.t $$\lim_n \int_{\sigma(A)}|f_n-f|du_\psi = 0.$$ Also I found that we have point-wise convergence a.e$[u_\psi]$. Using the definition of $f(A)$ I can show that $$\exists ~~\lim_n f_n(\lambda) \in \mathbb{C}$$ but not that it necessarily equals $f(\lambda)$. I wanted this to complete the attempt of: $(\psi, f(A)\psi) = \int_{\sigma(A)} fdu_\psi = \lim_n \int_{\sigma(A)}f_ndu_\psi = \lim_n(\psi, f_n(A)\psi) = \lim_n(\psi, f_n(\lambda)\psi)$","I'm reading Simon & Reed's Functional Analysis, and attempting to write out the proof of the functional calculus form of the Spectral Theorem. See this link for their construction of $f(A)$ where $f \in \mathbb{B}(\mathbb{R})$ a bounded Borel function on $\mathbb{R}$: Continuity of the functional calculus form of the Spectral Theorem Let $A \in L(H)$ be self-adjoint. My question is this: How do I show that $A\psi = \lambda \psi \implies f(A)\psi = f(\lambda)\psi$? This argument is easy when proving the same statement in the continuous calculus because when dealing with a continuous $f$ it may be approximated by a sequence $p_n$ that converges to it uniformly, and hence point-wise too. To my knowledge, a bounded Borel function, $f$, has the property that $\exists \{f_n\} \subset C(\sigma(A))$ s.t $$\lim_n \int_{\sigma(A)}|f_n-f|du_\psi = 0.$$ Also I found that we have point-wise convergence a.e$[u_\psi]$. Using the definition of $f(A)$ I can show that $$\exists ~~\lim_n f_n(\lambda) \in \mathbb{C}$$ but not that it necessarily equals $f(\lambda)$. I wanted this to complete the attempt of: $(\psi, f(A)\psi) = \int_{\sigma(A)} fdu_\psi = \lim_n \int_{\sigma(A)}f_ndu_\psi = \lim_n(\psi, f_n(A)\psi) = \lim_n(\psi, f_n(\lambda)\psi)$",,"['functional-analysis', 'spectral-theory', 'functional-calculus']"
72,"If $V$ is Volterra Operator, proof that $\|V^n\| = \frac{1}{(n-1)!}$","If  is Volterra Operator, proof that",V \|V^n\| = \frac{1}{(n-1)!},"Define the Volterra Operator $V:C([0,1])\to C([0,1])$ given by $$ Vf(x) = \int_0^x f(t) \, dt, \quad f \in C([0,1]). $$ I'm asked to proof that $\|V^n\| = \frac{1}{(n-1)!}$. My attempt: First, I'm gonna proof that  $V^n f (x) = \frac{1}{(n-1)!} \, \int_0^x (x-t)^{n-1} f(t) \, dt  \quad (I)$. Which holds for $n=1$, assume that $(I)$ holds for some $n > 1$. Then, $$ V^{n+1} f(x) = \int_0^x V^n f(t) dt = \int_0^x \left [ \frac{1}{(n-1)!}\int_0^t (t-s)^{n-1} f(s) ds \right ] dt =\\ \frac{1}{(n-1)!} \int_0^x f(s)\left [\int_s^x (t-s)^{n-1} \right ] ds  = \frac{1}{n!} \int_0^x (x-s)^n f(s) ds.$$ $$|V^n f (x)| \leq \frac{1}{(n-1)!} \, \int_0^x |x-t|^{n-1} |f(t)| \, dt \leq \frac{\|f\|_0}{(n-1)!} \, \int_0^x (x-t)^{n-1}  \, dt = \frac{\|f\|_0}{n!} x^n $$ Hence, $\|V^n f \| \leq \frac{1}{n!} \, \|f\|_0 \Rightarrow \|V^n\| \leq \frac{1}{n!}$ and $V^n 1 (x) = \frac{x^n}{n!}$, i.e $\| V^n 1 \|_0 = \frac{1}{n!}$. With this I conclude that $\|V^n \| = \frac{1}{n!}$ and not $\frac{1}{(n-1)!}$. What Am I doing wrong in this exercise?","Define the Volterra Operator $V:C([0,1])\to C([0,1])$ given by $$ Vf(x) = \int_0^x f(t) \, dt, \quad f \in C([0,1]). $$ I'm asked to proof that $\|V^n\| = \frac{1}{(n-1)!}$. My attempt: First, I'm gonna proof that  $V^n f (x) = \frac{1}{(n-1)!} \, \int_0^x (x-t)^{n-1} f(t) \, dt  \quad (I)$. Which holds for $n=1$, assume that $(I)$ holds for some $n > 1$. Then, $$ V^{n+1} f(x) = \int_0^x V^n f(t) dt = \int_0^x \left [ \frac{1}{(n-1)!}\int_0^t (t-s)^{n-1} f(s) ds \right ] dt =\\ \frac{1}{(n-1)!} \int_0^x f(s)\left [\int_s^x (t-s)^{n-1} \right ] ds  = \frac{1}{n!} \int_0^x (x-s)^n f(s) ds.$$ $$|V^n f (x)| \leq \frac{1}{(n-1)!} \, \int_0^x |x-t|^{n-1} |f(t)| \, dt \leq \frac{\|f\|_0}{(n-1)!} \, \int_0^x (x-t)^{n-1}  \, dt = \frac{\|f\|_0}{n!} x^n $$ Hence, $\|V^n f \| \leq \frac{1}{n!} \, \|f\|_0 \Rightarrow \|V^n\| \leq \frac{1}{n!}$ and $V^n 1 (x) = \frac{x^n}{n!}$, i.e $\| V^n 1 \|_0 = \frac{1}{n!}$. With this I conclude that $\|V^n \| = \frac{1}{n!}$ and not $\frac{1}{(n-1)!}$. What Am I doing wrong in this exercise?",,['functional-analysis']
73,Solving the following problem of convexity,Solving the following problem of convexity,,"Let $E$ be a complex Hilbert space. Let $A=(A_1,\cdots,A_d)\in \mathcal{L}(E)^d$. Consider \begin{eqnarray*} W_{max}(A) &=&\{\alpha\in \mathbb{C}^d:\;\exists\,(z_n)\subset E\;\;\hbox{such that}\;\|z_n\|=1,\displaystyle\lim_{n\rightarrow+\infty}\langle A_j z_n,z_n\rangle=\alpha_j,\\ &&\phantom{++++++++++}\;\hbox{and}\;\displaystyle\lim_{n\rightarrow+\infty}\|A_jz_n\|\rightarrow \|A_j\|,\;\forall j=1,\cdots,d \}. \end{eqnarray*} It is well known if $d=1$, we have $W_{Max}(A)$ is convex. If $d\geq2$, is $W_{Max}(A)$ convex?? Thank you for your help.","Let $E$ be a complex Hilbert space. Let $A=(A_1,\cdots,A_d)\in \mathcal{L}(E)^d$. Consider \begin{eqnarray*} W_{max}(A) &=&\{\alpha\in \mathbb{C}^d:\;\exists\,(z_n)\subset E\;\;\hbox{such that}\;\|z_n\|=1,\displaystyle\lim_{n\rightarrow+\infty}\langle A_j z_n,z_n\rangle=\alpha_j,\\ &&\phantom{++++++++++}\;\hbox{and}\;\displaystyle\lim_{n\rightarrow+\infty}\|A_jz_n\|\rightarrow \|A_j\|,\;\forall j=1,\cdots,d \}. \end{eqnarray*} It is well known if $d=1$, we have $W_{Max}(A)$ is convex. If $d\geq2$, is $W_{Max}(A)$ convex?? Thank you for your help.",,['functional-analysis']
74,"$\lim_{(x,y)\to(0,0)} (y-\sin(y))/(x^2+y^2)$",,"\lim_{(x,y)\to(0,0)} (y-\sin(y))/(x^2+y^2)","$$ f(x,y) = \begin{cases} 0 & \text{if } (x,y)=(0,0), \\[6pt] \dfrac{y-\sin y}{x^2+y^2} & \text{otherwise.} \end{cases} $$ is $\lim\limits_{(x,y)\to(0,0)} \dfrac{y-\sin y}{x^2+y^2} = 0 \text{ ?}$ according to wolfarm no but can someone show me why?","$$ f(x,y) = \begin{cases} 0 & \text{if } (x,y)=(0,0), \\[6pt] \dfrac{y-\sin y}{x^2+y^2} & \text{otherwise.} \end{cases} $$ is $\lim\limits_{(x,y)\to(0,0)} \dfrac{y-\sin y}{x^2+y^2} = 0 \text{ ?}$ according to wolfarm no but can someone show me why?",,"['functional-analysis', 'limits', 'functions']"
75,Division in $\Bbb R^n$,Division in,\Bbb R^n,"What is the most natural generalization of division from $\Bbb R$ to $\Bbb R^n $? In fact my original question concerns any normed space $X$. How one can divide two vectors in $X$ ? However, question is still non trivial in $ \Bbb R^n $. I want at least this division be continuous w.r.t  the norm of $X$. Motivation : Let $\{x_n\}$ and $\{y_n\}$ be two sequences in $X$, with $y_n$ be nice enough for all $n \in \Bbb N$. I want to find a good notion of division such that if $x_n \to x$ and $y_n \to y$ then we get $$\frac{x_n}{y_n}  \to \frac{x}{y} $$ For example, this is possible in all $\Bbb R^{n^2}$, viewing its members as square matrices then if $\{y_n\} \subseteq \Bbb R^{n^2} $ be the sequence of non singular matrices, we have that conclusion!","What is the most natural generalization of division from $\Bbb R$ to $\Bbb R^n $? In fact my original question concerns any normed space $X$. How one can divide two vectors in $X$ ? However, question is still non trivial in $ \Bbb R^n $. I want at least this division be continuous w.r.t  the norm of $X$. Motivation : Let $\{x_n\}$ and $\{y_n\}$ be two sequences in $X$, with $y_n$ be nice enough for all $n \in \Bbb N$. I want to find a good notion of division such that if $x_n \to x$ and $y_n \to y$ then we get $$\frac{x_n}{y_n}  \to \frac{x}{y} $$ For example, this is possible in all $\Bbb R^{n^2}$, viewing its members as square matrices then if $\{y_n\} \subseteq \Bbb R^{n^2} $ be the sequence of non singular matrices, we have that conclusion!",,"['real-analysis', 'linear-algebra', 'functional-analysis']"
76,Weak convergence of a sequence,Weak convergence of a sequence,,"Consider the sequence $(x_n)$ in $(c_0,\|.\|_{\infty})$, where $x_n=e_1+e_2+\ldots +e_n;  e_n=(0,0,\ldots,1,0,\ldots)$ for all $n\in \mathbb N$. I want to show that $(f(x_n))$ converges in $\mathbb K$ but $(x_n)$ does not converge weakly in $(c_0,\|.\|)$. Let $f\in c_0^*$. Since $c_0^*$ is $(c_0,\|.\|)$ isometrically isomorphic to $\ell^1$, therefore there exists $y\in \ell^1$ such that $f(x)=\sum\limits_{n=1}^{\infty}x(n)y(n)$ for all $x\in c_0$. Then $f(e_n)=\sum\limits_{m=1}^{\infty}e_n(m)y(m)=y(1)+\ldots+y(m)\to \sum\limits_{m=1}^{\infty}y(m)$. But how to show that $(x_n)$ does not converge weakly in $c_0$? Please suggest anything.","Consider the sequence $(x_n)$ in $(c_0,\|.\|_{\infty})$, where $x_n=e_1+e_2+\ldots +e_n;  e_n=(0,0,\ldots,1,0,\ldots)$ for all $n\in \mathbb N$. I want to show that $(f(x_n))$ converges in $\mathbb K$ but $(x_n)$ does not converge weakly in $(c_0,\|.\|)$. Let $f\in c_0^*$. Since $c_0^*$ is $(c_0,\|.\|)$ isometrically isomorphic to $\ell^1$, therefore there exists $y\in \ell^1$ such that $f(x)=\sum\limits_{n=1}^{\infty}x(n)y(n)$ for all $x\in c_0$. Then $f(e_n)=\sum\limits_{m=1}^{\infty}e_n(m)y(m)=y(1)+\ldots+y(m)\to \sum\limits_{m=1}^{\infty}y(m)$. But how to show that $(x_n)$ does not converge weakly in $c_0$? Please suggest anything.",,"['functional-analysis', 'weak-convergence']"
77,How do we take the Frechet derivative here? A question concernig a paper on the Kuramoto model,How do we take the Frechet derivative here? A question concernig a paper on the Kuramoto model,,"In the paper ""DYNAMICAL ASPECTS OF MEAN FIELD PLANE ROTATORS AND THE KURAMOTO MODEL"" by  L. Bertini, G. Giacomin, AND K. Pakdaman we read $$\partial_t q_t(\theta)=\frac12\frac{\partial^2q_t(\theta)}{\partial\theta^2}+K\frac{\partial}{\partial\theta}\left[\left(\int_{\mathbb{S}}\sin(\theta-\theta')q_t(\theta')\,\mathrm{d}\theta'\right)q_t(\theta)\right],\tag{1.9}$$ and further on, we see $1.3.$ The gradient flow viewpoint. For our purposes the following fact is of crucial importance: $(1.9)$ can be reqritten in the gradient form $$\partial_t q_t(\theta)=\nabla\left[q_t(\theta)\nabla\left(\dfrac{\delta\mathcal{F}(q_t)}{\delta q_t(\theta)}\right)\right],\tag{1.18}$$ where we use $\nabla$ for $\partial_\theta$ for visual impact, $\delta\mathcal{G}(q)/\delta q(\theta)$ is the standard $L^2$ Frechet derivative of the function $\mathcal{G}$ and $$\mathcal{F}(q):=\frac{1}{2}\int_{\mathbb{S}} q(\theta) \log q(\theta)\,\mathrm{d}\theta-\frac K2\int_{\mathbb{S}^2}\cos(\theta-\theta')q(\theta)q(\theta')\,\mathrm{d}\theta\,\mathrm{d}\theta'.\tag{1.19}$$ I am having trouble finding (1.18). I think the difficulty lies in computing the Frechet derivative. Attempt When computing a Frechet derivative we are looking for a transformation $A$ such that $$\frac{\|f(x + v) - f(x) - Av\|}{\|h\|} \xrightarrow[h \to 0]{} 0 $$ The norms here are $L^2$ norms on the space of functions on $S = [0,2\pi)$ with the Lebesgue measure. So $h$ is a vector in $L^2(S)$. A first question is: What do we mean when we say $ \frac{\delta \mathcal{F(q_t)}}{\delta q_t(\theta)}$? Are we taking the derivative in the direction of the function $q_t$? More precisely  are we computing $$\lim_{h \to 0}\frac{\mathcal{F}(q_t + hq_t) - \mathcal{F}(q_t)}{h} ? \tag{*}$$ In this case we obtain: $$ \lim_{h\to 0} \frac{1}{2} \frac{1}{h}\int_S (q_t(\theta) + h q_t(\theta)) \log(q_t(\theta) + h q_t(\theta)) - q_t(\theta) \log (q_t(\theta)) d\theta\\ - \frac{K}{2}\frac{1}{h}\int_{S^2} \cos(\theta - \theta')\{ [q_t(\theta) + hq_t(\theta)][q_t(\theta') + hq_t(\theta')] - q_t(\theta) q_t(\theta')\} d\theta d\theta'  \\ = \frac{1}{2} \int_S q_t(\theta) +  q_t(\theta) \log(q_t(\theta))\, d\theta \\ - \frac{K}{2}\int_{S^2} \cos(\theta - \theta')\{ 2q_t(\theta) q_t(\theta')\} d\theta d\theta'  $$ However, when computing the derivative $\partial_\theta  \frac{\delta \mathcal{F(q_t)}}{\delta q_t(\theta)}$ this is zero, once the value above does not depend on $\theta$ since we integrated on $\theta$. So this derivative makes no sense for our purposes. Maybe we should note that  the denominator of $ \frac{\delta \mathcal{F(q_t)}}{\delta q_t(\theta)}$ has a $\theta$. So this should mean that we are differentiating in a direction that depends on $\theta$. Maybe we should compute $$ \lim_{h\to 0} \frac{1}{2} \frac{1}{h}\int_S (q_t(u) + h q_t(\theta)) \log(q_t(u) + h q_t(\theta)) - q_t(u) \log (q_t(u)) du\\ - \frac{K}{2}\frac{1}{h}\int_{S^2} \cos(u - u')\{ [q_t(u) + hq_t(\theta)][q_t(u') + hq_t(\theta')] - q_t(u) q_t(u')\} du du'  \\ = \frac{1}{2} \int_S q_t(\theta) +  q_t(\theta) \log(q_t(u))\, du \\ - \frac{K}{2}\int_{S^2} \cos(u -u')\{ q_t(\theta)q_t(u) + q_t(u') q_t(\theta)\} du du'  $$ Still I don't see how this could be compatible with (1.9) Any ideas?","In the paper ""DYNAMICAL ASPECTS OF MEAN FIELD PLANE ROTATORS AND THE KURAMOTO MODEL"" by  L. Bertini, G. Giacomin, AND K. Pakdaman we read $$\partial_t q_t(\theta)=\frac12\frac{\partial^2q_t(\theta)}{\partial\theta^2}+K\frac{\partial}{\partial\theta}\left[\left(\int_{\mathbb{S}}\sin(\theta-\theta')q_t(\theta')\,\mathrm{d}\theta'\right)q_t(\theta)\right],\tag{1.9}$$ and further on, we see $1.3.$ The gradient flow viewpoint. For our purposes the following fact is of crucial importance: $(1.9)$ can be reqritten in the gradient form $$\partial_t q_t(\theta)=\nabla\left[q_t(\theta)\nabla\left(\dfrac{\delta\mathcal{F}(q_t)}{\delta q_t(\theta)}\right)\right],\tag{1.18}$$ where we use $\nabla$ for $\partial_\theta$ for visual impact, $\delta\mathcal{G}(q)/\delta q(\theta)$ is the standard $L^2$ Frechet derivative of the function $\mathcal{G}$ and $$\mathcal{F}(q):=\frac{1}{2}\int_{\mathbb{S}} q(\theta) \log q(\theta)\,\mathrm{d}\theta-\frac K2\int_{\mathbb{S}^2}\cos(\theta-\theta')q(\theta)q(\theta')\,\mathrm{d}\theta\,\mathrm{d}\theta'.\tag{1.19}$$ I am having trouble finding (1.18). I think the difficulty lies in computing the Frechet derivative. Attempt When computing a Frechet derivative we are looking for a transformation $A$ such that $$\frac{\|f(x + v) - f(x) - Av\|}{\|h\|} \xrightarrow[h \to 0]{} 0 $$ The norms here are $L^2$ norms on the space of functions on $S = [0,2\pi)$ with the Lebesgue measure. So $h$ is a vector in $L^2(S)$. A first question is: What do we mean when we say $ \frac{\delta \mathcal{F(q_t)}}{\delta q_t(\theta)}$? Are we taking the derivative in the direction of the function $q_t$? More precisely  are we computing $$\lim_{h \to 0}\frac{\mathcal{F}(q_t + hq_t) - \mathcal{F}(q_t)}{h} ? \tag{*}$$ In this case we obtain: $$ \lim_{h\to 0} \frac{1}{2} \frac{1}{h}\int_S (q_t(\theta) + h q_t(\theta)) \log(q_t(\theta) + h q_t(\theta)) - q_t(\theta) \log (q_t(\theta)) d\theta\\ - \frac{K}{2}\frac{1}{h}\int_{S^2} \cos(\theta - \theta')\{ [q_t(\theta) + hq_t(\theta)][q_t(\theta') + hq_t(\theta')] - q_t(\theta) q_t(\theta')\} d\theta d\theta'  \\ = \frac{1}{2} \int_S q_t(\theta) +  q_t(\theta) \log(q_t(\theta))\, d\theta \\ - \frac{K}{2}\int_{S^2} \cos(\theta - \theta')\{ 2q_t(\theta) q_t(\theta')\} d\theta d\theta'  $$ However, when computing the derivative $\partial_\theta  \frac{\delta \mathcal{F(q_t)}}{\delta q_t(\theta)}$ this is zero, once the value above does not depend on $\theta$ since we integrated on $\theta$. So this derivative makes no sense for our purposes. Maybe we should note that  the denominator of $ \frac{\delta \mathcal{F(q_t)}}{\delta q_t(\theta)}$ has a $\theta$. So this should mean that we are differentiating in a direction that depends on $\theta$. Maybe we should compute $$ \lim_{h\to 0} \frac{1}{2} \frac{1}{h}\int_S (q_t(u) + h q_t(\theta)) \log(q_t(u) + h q_t(\theta)) - q_t(u) \log (q_t(u)) du\\ - \frac{K}{2}\frac{1}{h}\int_{S^2} \cos(u - u')\{ [q_t(u) + hq_t(\theta)][q_t(u') + hq_t(\theta')] - q_t(u) q_t(u')\} du du'  \\ = \frac{1}{2} \int_S q_t(\theta) +  q_t(\theta) \log(q_t(u))\, du \\ - \frac{K}{2}\int_{S^2} \cos(u -u')\{ q_t(\theta)q_t(u) + q_t(u') q_t(\theta)\} du du'  $$ Still I don't see how this could be compatible with (1.9) Any ideas?",,['functional-analysis']
78,"Completing an orthornomal sequence to obtain orthonormal basis in $ L^2[a,b] $",Completing an orthornomal sequence to obtain orthonormal basis in," L^2[a,b] ","i am stumped with the following question :  Suppose we have an orthonormal sequence $ \left \{ e_{n} , n \in \mathbb{N} \right.\left.  \right \} $  in $ L^2[a,b] $ can we add countable many functions such that we obtain an orthonormal basis  [ $ L^2[a,b]=\overline{span\left \{ \left \{ e_{n},n \in \mathbb{N}\left.  \right \} \cup \left \{ x_{j},j \in \mathbb{N}\left.  \right \} \right. \right. \right.}   $ ]  for $ L^2[a,b] $ ? Can we generalize for any separable Hilbert space ?","i am stumped with the following question :  Suppose we have an orthonormal sequence $ \left \{ e_{n} , n \in \mathbb{N} \right.\left.  \right \} $  in $ L^2[a,b] $ can we add countable many functions such that we obtain an orthonormal basis  [ $ L^2[a,b]=\overline{span\left \{ \left \{ e_{n},n \in \mathbb{N}\left.  \right \} \cup \left \{ x_{j},j \in \mathbb{N}\left.  \right \} \right. \right. \right.}   $ ]  for $ L^2[a,b] $ ? Can we generalize for any separable Hilbert space ?",,"['functional-analysis', 'hilbert-spaces', 'lp-spaces']"
79,Singular value decomposition of a specific integral operator,Singular value decomposition of a specific integral operator,,"I want to determine the singular value decomposition of the integral operator $$L^2(0,1) \to L^2(0,1) ; f \mapsto Af(\cdot) = \int_0^\cdot f(y)dy.$$ Its adjungate is given by $$A^*f(\cdot) = \int_\cdot^1 f(y) dy. $$ Hence $A^*Af(x) = \int_x^1 \int_0^y f(z)dzdy.$ Assuming that $f$ is smooth yields $$\lambda f''(x) = A^*Af''(x) = \int_x^1\int_0^y f''(z)dzdy = -f(x) +f(1) - (1-x)f'(0).$$ Clearly, $\lambda_j = ((j-1/2)\pi)^{-2}$ and $v_j(x)= \sqrt{2}\cos((j-1/2)\pi x)$ are solutions of the above differential equation. Do more solutions exist? Since $A$ is injective the Eigenfunctions should span all of $L^2(0,1)$ . We have $v_j(1)=0$ for all $j$ , but this is merely a point and should not matter in $L^2(0,1)$ . Moreover, the $v_j$ are even for some $j$ and odd for the other.","I want to determine the singular value decomposition of the integral operator Its adjungate is given by Hence Assuming that is smooth yields Clearly, and are solutions of the above differential equation. Do more solutions exist? Since is injective the Eigenfunctions should span all of . We have for all , but this is merely a point and should not matter in . Moreover, the are even for some and odd for the other.","L^2(0,1) \to L^2(0,1) ; f \mapsto Af(\cdot) = \int_0^\cdot f(y)dy. A^*f(\cdot) = \int_\cdot^1 f(y) dy.  A^*Af(x) = \int_x^1 \int_0^y f(z)dzdy. f \lambda f''(x) = A^*Af''(x) = \int_x^1\int_0^y f''(z)dzdy = -f(x) +f(1) - (1-x)f'(0). \lambda_j = ((j-1/2)\pi)^{-2} v_j(x)= \sqrt{2}\cos((j-1/2)\pi x) A L^2(0,1) v_j(1)=0 j L^2(0,1) v_j j","['functional-analysis', 'ordinary-differential-equations', 'integral-operators', 'schauder-basis']"
80,Continuous spectrum of unbounded operator,Continuous spectrum of unbounded operator,,"I would like to ask about continuous spectrum of unbounded, densely defined closed   operator. Let $A\colon X\supset\mathcal{D}_A\to X$, where X is Banach space, $\overline{\mathcal{D}_A}=X$ be a unbounded linear operator. When I read some books I find two a bit different definitions of continuous spectrum: (a) $\sigma_c(A)=\{\lambda\in\mathbb{C}\, |\,\lambda I-A \textrm{ is injective }, \overline{R(\lambda I -A)}=X,\, R(\lambda I -A)\neq X \}$ (b) $\sigma_c(A)=\{\lambda\in\mathbb{C}\, |\,\lambda I-A \textrm{ is injective }, \overline{R(\lambda I -A)}=X,\, (\lambda I-A)^{-1} \textrm{ is unbounded} \}.$ Could you explain me  why that definitions are equivalent?","I would like to ask about continuous spectrum of unbounded, densely defined closed   operator. Let $A\colon X\supset\mathcal{D}_A\to X$, where X is Banach space, $\overline{\mathcal{D}_A}=X$ be a unbounded linear operator. When I read some books I find two a bit different definitions of continuous spectrum: (a) $\sigma_c(A)=\{\lambda\in\mathbb{C}\, |\,\lambda I-A \textrm{ is injective }, \overline{R(\lambda I -A)}=X,\, R(\lambda I -A)\neq X \}$ (b) $\sigma_c(A)=\{\lambda\in\mathbb{C}\, |\,\lambda I-A \textrm{ is injective }, \overline{R(\lambda I -A)}=X,\, (\lambda I-A)^{-1} \textrm{ is unbounded} \}.$ Could you explain me  why that definitions are equivalent?",,"['functional-analysis', 'operator-theory', 'spectral-theory']"
81,"$W^{1,p}_0 \cap C^\infty \subset C^\infty_0$?",?,"W^{1,p}_0 \cap C^\infty \subset C^\infty_0","Is this true: $W^{1,p}_0 \cap C^\infty \subset C^\infty_0$? If this is true, how to prove it? If not, what is a counter-example? Notation: Denote $C^\infty_0$ the set of all real-valued smooth function $f$ on  $\mathbb R$ such that $\lim_{x\to \pm\infty} f(x)=0$, and denote $C^\infty_c$ the set of all real-valued smooth function $f$ on $\mathbb R$such that the support of $f$ is compact. Finally, define $W^{1,p}_0$ is the completion of $C^\infty_c$ with respect to the Sobolev norm $||\cdot||_{W^{1,p}}$","Is this true: $W^{1,p}_0 \cap C^\infty \subset C^\infty_0$? If this is true, how to prove it? If not, what is a counter-example? Notation: Denote $C^\infty_0$ the set of all real-valued smooth function $f$ on  $\mathbb R$ such that $\lim_{x\to \pm\infty} f(x)=0$, and denote $C^\infty_c$ the set of all real-valued smooth function $f$ on $\mathbb R$such that the support of $f$ is compact. Finally, define $W^{1,p}_0$ is the completion of $C^\infty_c$ with respect to the Sobolev norm $||\cdot||_{W^{1,p}}$",,"['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations', 'sobolev-spaces']"
82,Is there some general way to characterize real symmetrical functions?,Is there some general way to characterize real symmetrical functions?,,"I am looking for a way to rewrite a function $f: \mathbb{R}^2 \mapsto \mathbb{R}$ that is symmetric in its two arguments. That is: $$ f(x,y) = f(y,x) $$ At first I was thinking that since $x$ and $y$ are interchangeable, their influence on the value of $f$ must be the same, hence somewhere within $f$ they must be mapped by some auxiliary function $a$ and then aggregated somehow. For example by summation or multiplication: $$ f(x,y) = a(x) + a(y) \\  \text{  or}\\ f(x,y) = a(x)a(y) $$ We can see that if we compose $f$ with any $g : \mathbb{R} \mapsto \mathbb{R}$, the symmetry remains. Hence if we take the logarithm of the lower equation, we can transform multiplication into addition. This would suggest a general form: $$ f(x,y) = g(a(x) + a(y)) $$ But the problem is that if we add two symmetric functions, we also get a symmetric function, but the given general form does not stand up to this task: $$ f_1(x,y) + f_2(x,y) = g_1(a_1(x) + a_1(y)) + g_2(a_2(x) + a_2(y)) $$ Is there some general way to characterize $f$? We can assume any number of finite derivatives of $f$.","I am looking for a way to rewrite a function $f: \mathbb{R}^2 \mapsto \mathbb{R}$ that is symmetric in its two arguments. That is: $$ f(x,y) = f(y,x) $$ At first I was thinking that since $x$ and $y$ are interchangeable, their influence on the value of $f$ must be the same, hence somewhere within $f$ they must be mapped by some auxiliary function $a$ and then aggregated somehow. For example by summation or multiplication: $$ f(x,y) = a(x) + a(y) \\  \text{  or}\\ f(x,y) = a(x)a(y) $$ We can see that if we compose $f$ with any $g : \mathbb{R} \mapsto \mathbb{R}$, the symmetry remains. Hence if we take the logarithm of the lower equation, we can transform multiplication into addition. This would suggest a general form: $$ f(x,y) = g(a(x) + a(y)) $$ But the problem is that if we add two symmetric functions, we also get a symmetric function, but the given general form does not stand up to this task: $$ f_1(x,y) + f_2(x,y) = g_1(a_1(x) + a_1(y)) + g_2(a_2(x) + a_2(y)) $$ Is there some general way to characterize $f$? We can assume any number of finite derivatives of $f$.",,"['functional-analysis', 'functions', 'symmetric-functions']"
83,Can left-shift be approximated by polynomials of right shift?,Can left-shift be approximated by polynomials of right shift?,,"Consider the left shift operator $L$ and the right shift $R$ on $l^2(\mathbb Z)$. Then both are unitary operators and inverse of each other. I have two question about them: Is there $x\ne 0$ such that $Lx$ is in the closure of the span of $(R^nx)$? And can we show in addition that there numbers $(a_n)$ such that  $$ Lx = \sum_{n=0}^\infty a_n R^n x \quad ? $$ It is easy to see that the claim is false if $x$ is a one-sided sequence, i.e., there is $K$ such that ($x_k=0$ for  all $k<-K$) or ($x_k=0$ for  all $k>K$) holds. Now such sequences are dense in $l^2(\mathbb Z)$. But I was not able to show that no $x\ne0$ can be written as above. Is there a nice proof of this claim? Or is there an example of $x$, which works? This question was inspired by this question and the example given there. Edit: As commented by David C. Ullrich, the original version of the post contained two non-equivalent question. Both questions have been answered below. Thanks.","Consider the left shift operator $L$ and the right shift $R$ on $l^2(\mathbb Z)$. Then both are unitary operators and inverse of each other. I have two question about them: Is there $x\ne 0$ such that $Lx$ is in the closure of the span of $(R^nx)$? And can we show in addition that there numbers $(a_n)$ such that  $$ Lx = \sum_{n=0}^\infty a_n R^n x \quad ? $$ It is easy to see that the claim is false if $x$ is a one-sided sequence, i.e., there is $K$ such that ($x_k=0$ for  all $k<-K$) or ($x_k=0$ for  all $k>K$) holds. Now such sequences are dense in $l^2(\mathbb Z)$. But I was not able to show that no $x\ne0$ can be written as above. Is there a nice proof of this claim? Or is there an example of $x$, which works? This question was inspired by this question and the example given there. Edit: As commented by David C. Ullrich, the original version of the post contained two non-equivalent question. Both questions have been answered below. Thanks.",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
84,A $H^{-1}$ norm of a $L^2$ function,A  norm of a  function,H^{-1} L^2,"Let $P\in L^2(\Omega)$ (where $\Omega=[0,2\pi]^n$),  be a function such that : $\forall x=(x_1,...,x_n)\in \Omega,\;P(x)=\sum_{j_1=-\infty}^{+\infty}...\sum_{j_n=-\infty}^{+\infty}a_{j}e^{ij.x}$ with $j.x=j_1x_1+...+j_nx_n$. How can I figure out $$\left \| P \right \|_{H^{-1}(\Omega)}\; \text{and}\;\; \left \| \nabla P \right \|_{(H^{-1}(\Omega))^n}\;,$$  I will appreciate if you could help me . Thanks. PS: I want to prove that: $$\left \| P \right \|_{L^{2}(\Omega)}\; \leq c\big(\left \| P \right \|_{H^{-1}(\Omega)}+\; \left \| \nabla P \right \|_{(H^{-1}(\Omega))^n}\;\big).$$ Directly using the calculations, and without using any particular inequality (in particular the inequality of $Ne\check{c}as$.) Here what I did: This is not an answer but it's the idea that i have, if some one can complete it: Let's try to characterize $\left \| P \right \|_{H^{-1}(Q)}$ and $\left \| \nabla P \right \|_{(H^{-1}(Q))^d}$. We have : $$\left \| P \right \|_{H^{-1}(Q)}\geqslant \frac{\left \langle P,\phi \right \rangle}{ \left \| \phi \right \|_{H^1(Q)}}\;,\:\: \forall \phi \in H_0^1(Q)$$  In particular for each $N\in \mathbb N$, Let $$\phi_N(x)=\sum_{j\in[-N,N]^d} (1+\left | j \right |^2)^{-1}\;\overline{a_j}\;e^{i\,j.x} \;,\;\forall x\in Q$$  We have: \begin{align*}    \left \| \phi_N \right \|_{H^1(Q)}&=(\sum_{j\in[-N,N]^d }\; (1+\left | j \right |^2)(1+\left | j \right |^2)^{-2}\;\left | a_j \right |^2)^\frac{1}{2} \\     &=  (\sum_{j\in[-N,N]^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2} \end{align*} So, \begin{align*} \left \| P \right \|_{H^{-1}(Q)} &\geqslant \frac{\left \langle P,\phi_N \right \rangle}{ \left \| \phi_N \right \|_{H^1}}\\ &=\frac{\sum_{j\in[-N,N]^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2}{(\sum_{j\in[-N,N]^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2}}\\ &=(\sum_{j\in[-N,N]^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2}\;,\;\forall N\in\mathbb N \end{align*} So, $\left \| P \right \|_{H^{-1}(Q)} \geqslant (\sum_{j\in\mathbb Z^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2}$ I want to use the same idea (Find $\phi _N$) to prove that: $\left \| \nabla P \right \|_{(H^{-1}(Q))^d} \geqslant (\sum_{j\in\mathbb Z^d}\; \left | j \right |^2(1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2}$ Can you help me?","Let $P\in L^2(\Omega)$ (where $\Omega=[0,2\pi]^n$),  be a function such that : $\forall x=(x_1,...,x_n)\in \Omega,\;P(x)=\sum_{j_1=-\infty}^{+\infty}...\sum_{j_n=-\infty}^{+\infty}a_{j}e^{ij.x}$ with $j.x=j_1x_1+...+j_nx_n$. How can I figure out $$\left \| P \right \|_{H^{-1}(\Omega)}\; \text{and}\;\; \left \| \nabla P \right \|_{(H^{-1}(\Omega))^n}\;,$$  I will appreciate if you could help me . Thanks. PS: I want to prove that: $$\left \| P \right \|_{L^{2}(\Omega)}\; \leq c\big(\left \| P \right \|_{H^{-1}(\Omega)}+\; \left \| \nabla P \right \|_{(H^{-1}(\Omega))^n}\;\big).$$ Directly using the calculations, and without using any particular inequality (in particular the inequality of $Ne\check{c}as$.) Here what I did: This is not an answer but it's the idea that i have, if some one can complete it: Let's try to characterize $\left \| P \right \|_{H^{-1}(Q)}$ and $\left \| \nabla P \right \|_{(H^{-1}(Q))^d}$. We have : $$\left \| P \right \|_{H^{-1}(Q)}\geqslant \frac{\left \langle P,\phi \right \rangle}{ \left \| \phi \right \|_{H^1(Q)}}\;,\:\: \forall \phi \in H_0^1(Q)$$  In particular for each $N\in \mathbb N$, Let $$\phi_N(x)=\sum_{j\in[-N,N]^d} (1+\left | j \right |^2)^{-1}\;\overline{a_j}\;e^{i\,j.x} \;,\;\forall x\in Q$$  We have: \begin{align*}    \left \| \phi_N \right \|_{H^1(Q)}&=(\sum_{j\in[-N,N]^d }\; (1+\left | j \right |^2)(1+\left | j \right |^2)^{-2}\;\left | a_j \right |^2)^\frac{1}{2} \\     &=  (\sum_{j\in[-N,N]^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2} \end{align*} So, \begin{align*} \left \| P \right \|_{H^{-1}(Q)} &\geqslant \frac{\left \langle P,\phi_N \right \rangle}{ \left \| \phi_N \right \|_{H^1}}\\ &=\frac{\sum_{j\in[-N,N]^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2}{(\sum_{j\in[-N,N]^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2}}\\ &=(\sum_{j\in[-N,N]^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2}\;,\;\forall N\in\mathbb N \end{align*} So, $\left \| P \right \|_{H^{-1}(Q)} \geqslant (\sum_{j\in\mathbb Z^d}\; (1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2}$ I want to use the same idea (Find $\phi _N$) to prove that: $\left \| \nabla P \right \|_{(H^{-1}(Q))^d} \geqslant (\sum_{j\in\mathbb Z^d}\; \left | j \right |^2(1+\left | j \right |^2)^{-1}\;\left | a_j \right |^2)^\frac{1}{2}$ Can you help me?",,"['calculus', 'real-analysis', 'functional-analysis', 'normed-spaces', 'sobolev-spaces']"
85,"If $f\in L^2([0,1]^2)$, what can we say about $g(x)=f(x,x)$?","If , what can we say about ?","f\in L^2([0,1]^2) g(x)=f(x,x)","Suppose that $L^2([0,1]^2)$ is the space of equivalence classes of square integrable functions $f:[0,1]^2\to\mathbb R$ with the usual norm given by $$ \|f\|_2=\biggl(\int_0^1\int_0^1|f(x,y)|^2dxdy\biggr)^{1/2}. $$ With an abuse of notation, the function and the equivalence class is denoted by the same symbol $f$ (instead of using $f$ for the function and $[f]$ for its equivalence class). Suppose that $f\in L^2([0,1]^2)$ and set $g(x)=f(x,x)$ for each $x\in[0,1]$. What can we say about $g$? Can we say that $g\in L^2([0,1])$? Suppose that $f,f_1,f_2,\ldots\in L^2([0,1]^2)$ such that $f_n\to f$  as $n\to\infty$ in $L^2([0,1]^2)$. Set $g(x)=f(x,x)$, $g_1=f_1(x,x),g_2=f_2(x,x),\ldots$ for each $x\in[0,1]$. Can we say that $g_n\to g$ as $n\to\infty$ in $L^2([0,1])$? Any help is much appreciated!","Suppose that $L^2([0,1]^2)$ is the space of equivalence classes of square integrable functions $f:[0,1]^2\to\mathbb R$ with the usual norm given by $$ \|f\|_2=\biggl(\int_0^1\int_0^1|f(x,y)|^2dxdy\biggr)^{1/2}. $$ With an abuse of notation, the function and the equivalence class is denoted by the same symbol $f$ (instead of using $f$ for the function and $[f]$ for its equivalence class). Suppose that $f\in L^2([0,1]^2)$ and set $g(x)=f(x,x)$ for each $x\in[0,1]$. What can we say about $g$? Can we say that $g\in L^2([0,1])$? Suppose that $f,f_1,f_2,\ldots\in L^2([0,1]^2)$ such that $f_n\to f$  as $n\to\infty$ in $L^2([0,1]^2)$. Set $g(x)=f(x,x)$, $g_1=f_1(x,x),g_2=f_2(x,x),\ldots$ for each $x\in[0,1]$. Can we say that $g_n\to g$ as $n\to\infty$ in $L^2([0,1])$? Any help is much appreciated!",,"['functional-analysis', 'measure-theory', 'lp-spaces']"
86,convergence of argmax,convergence of argmax,,"Let $f$ and $g$ be two continuous functions on an interval $[a,b] \subset \mathbb{R}$. Define for every $k \in \mathbb{N}$ $f^k(x)=(1-\varepsilon_k)f(x)+\varepsilon_kg(x)$ in which $\varepsilon_k \rightarrow 0$ as $k \rightarrow \infty$. Suppose that function $f$ has a unique maximizer $x=y$. How can I show that there is a sequence of  $\{y^k\}_{k=1}^{\infty}$ in which $y^k$ is a maximizer of $f^k$ for every $k$, such that $y^k \rightarrow y$ when $k \rightarrow \infty$?","Let $f$ and $g$ be two continuous functions on an interval $[a,b] \subset \mathbb{R}$. Define for every $k \in \mathbb{N}$ $f^k(x)=(1-\varepsilon_k)f(x)+\varepsilon_kg(x)$ in which $\varepsilon_k \rightarrow 0$ as $k \rightarrow \infty$. Suppose that function $f$ has a unique maximizer $x=y$. How can I show that there is a sequence of  $\{y^k\}_{k=1}^{\infty}$ in which $y^k$ is a maximizer of $f^k$ for every $k$, such that $y^k \rightarrow y$ when $k \rightarrow \infty$?",,['functional-analysis']
87,Reflexive closure of Banach space,Reflexive closure of Banach space,,"Given a Banach space $E$, need there exist a reflexive Banach space $\overline{E}$ and a map $T: E \to \overline{E}$ such that any map $S: E \to X$, where $X$ is a reflexive Banach space, factors through $\overline{E}$ via $T$? This $\overline{E}$ would then be a sort of reflexive ""closure"" or ""envelope"" of $E$. My initial thought was to look at the colimit of  $$E \hookrightarrow E^{**} \hookrightarrow E^{****} \hookrightarrow \dots$$ but this is just a wild guess. For it to work, I'd need to know that the double dual commutes with colimits in the category of Banach spaces, and that seems doubtful to me.","Given a Banach space $E$, need there exist a reflexive Banach space $\overline{E}$ and a map $T: E \to \overline{E}$ such that any map $S: E \to X$, where $X$ is a reflexive Banach space, factors through $\overline{E}$ via $T$? This $\overline{E}$ would then be a sort of reflexive ""closure"" or ""envelope"" of $E$. My initial thought was to look at the colimit of  $$E \hookrightarrow E^{**} \hookrightarrow E^{****} \hookrightarrow \dots$$ but this is just a wild guess. For it to work, I'd need to know that the double dual commutes with colimits in the category of Banach spaces, and that seems doubtful to me.",,"['functional-analysis', 'banach-spaces']"
88,Unbounded operator between normed spaces,Unbounded operator between normed spaces,,"For every infinite sequence $x = (x_1, x_2, x_3, ...)$ of complex numbers define $S(x)$ by $S(x_1, x_2, x_3, ...) = (x_1, 2x_2, 3x_3, ...)$. Is $S$ in $\mathcal{L}(\mathcal l^1, \mathcal l^\infty)$? I argue that $S$ is unbounded and hence not in $\mathcal{L}(\mathcal l^1, \mathcal l^\infty)$. Proof: Firstly, using $|| x||_\infty \geq || x||_1$, we have that $$||S(x)||_\infty = \sup_n|S(x_n)| = \sup_n|n\cdot x_n|= n\cdot|| x||_\infty \geq n\cdot||x||_1.$$ This means that $$\frac{||S(x)||_\infty}{|| x||_1} \geq n \rightarrow\infty$$ and hence, $S$ is unbounded with $||\cdot||_\infty$ norm and not a member of $\mathcal{L}(\mathcal l^1, \mathcal l^\infty)$ .","For every infinite sequence $x = (x_1, x_2, x_3, ...)$ of complex numbers define $S(x)$ by $S(x_1, x_2, x_3, ...) = (x_1, 2x_2, 3x_3, ...)$. Is $S$ in $\mathcal{L}(\mathcal l^1, \mathcal l^\infty)$? I argue that $S$ is unbounded and hence not in $\mathcal{L}(\mathcal l^1, \mathcal l^\infty)$. Proof: Firstly, using $|| x||_\infty \geq || x||_1$, we have that $$||S(x)||_\infty = \sup_n|S(x_n)| = \sup_n|n\cdot x_n|= n\cdot|| x||_\infty \geq n\cdot||x||_1.$$ This means that $$\frac{||S(x)||_\infty}{|| x||_1} \geq n \rightarrow\infty$$ and hence, $S$ is unbounded with $||\cdot||_\infty$ norm and not a member of $\mathcal{L}(\mathcal l^1, \mathcal l^\infty)$ .",,"['calculus', 'functional-analysis', 'proof-verification', 'normed-spaces']"
89,Determine generator of $C_0$-semigroup,Determine generator of -semigroup,C_0,"I try to solve the following problem: Let $X$ locally compact and $a \in C(X)$ such that $\text{Re } a \leq w$. Further let $T(t)f := e^{ta}f$ for all $f \in C_0(X)$ and $t \geq 0$. Determine the generator of $(T(t))_{t \geq 0}$. I figured out that the generator must be the multiplication operator of $a$ defined by $M_a f = a f$ for all $f \in C_0(X)$. So I need to show that $$ \lim_{t \to 0} \left\Vert \frac{T(t)f - f}{t} - M_a f \right\Vert_\infty = 0 \qquad \text{for all } f \in C_0(X).$$ So let $\epsilon > 0$, $(t_n)_{n \in \mathbb N}$ in $\mathbb R_+$ with $t_n \to 0$ and $f \in C_0(X)$. So I need a $N \in \mathbb N$ with $$ \left\Vert \frac{T(t_n)f - f}{t} - M_a f \right\Vert_\infty  = \sup_{x \in X} \left\vert \frac{1}{t_n}(e^{t_n a(x)} - 1) - a(x) \right\vert \vert f \vert < \epsilon \qquad \text{for all } n \geq N$$ So its enough to show that $\left\vert \frac{1}{t_n}(e^{t_n a(x)} - 1) - a(x) \right\vert < \epsilon/\Vert f \Vert_\infty$ for all $n \geq N$ and $x \in X$. By L'Hopital I know that $\frac{1}{t}(e^{t a(x)} - 1) \to a(x)$ for $t \to 0$. Thus for a $x \in X$ I get by continuity: $$\forall \epsilon > 0\ \exists \delta > 0\ \forall \vert t \vert < \delta: \left\vert\frac{1}{t}(e^{t a(x)} - 1) - a(x)  \right\vert < \epsilon. $$ At this point I don't know how to proceed because I think that $\delta$ depends on the $x$, so that I don't get the estimate for all $x \in X$. Another idea of mine is to show  $$ \lim_{t \to 0} \left\Vert \frac{T(t)f - f}{t} - M_a f \right\Vert_\infty = 0 \qquad \text{for all } f \in C_c(X)$$ and use the fact that $C_c(X)$ is dense in $C_0(X)$. But I figured out that I would have the same issues with that idea... I would appreciate some hints on the topic :)","I try to solve the following problem: Let $X$ locally compact and $a \in C(X)$ such that $\text{Re } a \leq w$. Further let $T(t)f := e^{ta}f$ for all $f \in C_0(X)$ and $t \geq 0$. Determine the generator of $(T(t))_{t \geq 0}$. I figured out that the generator must be the multiplication operator of $a$ defined by $M_a f = a f$ for all $f \in C_0(X)$. So I need to show that $$ \lim_{t \to 0} \left\Vert \frac{T(t)f - f}{t} - M_a f \right\Vert_\infty = 0 \qquad \text{for all } f \in C_0(X).$$ So let $\epsilon > 0$, $(t_n)_{n \in \mathbb N}$ in $\mathbb R_+$ with $t_n \to 0$ and $f \in C_0(X)$. So I need a $N \in \mathbb N$ with $$ \left\Vert \frac{T(t_n)f - f}{t} - M_a f \right\Vert_\infty  = \sup_{x \in X} \left\vert \frac{1}{t_n}(e^{t_n a(x)} - 1) - a(x) \right\vert \vert f \vert < \epsilon \qquad \text{for all } n \geq N$$ So its enough to show that $\left\vert \frac{1}{t_n}(e^{t_n a(x)} - 1) - a(x) \right\vert < \epsilon/\Vert f \Vert_\infty$ for all $n \geq N$ and $x \in X$. By L'Hopital I know that $\frac{1}{t}(e^{t a(x)} - 1) \to a(x)$ for $t \to 0$. Thus for a $x \in X$ I get by continuity: $$\forall \epsilon > 0\ \exists \delta > 0\ \forall \vert t \vert < \delta: \left\vert\frac{1}{t}(e^{t a(x)} - 1) - a(x)  \right\vert < \epsilon. $$ At this point I don't know how to proceed because I think that $\delta$ depends on the $x$, so that I don't get the estimate for all $x \in X$. Another idea of mine is to show  $$ \lim_{t \to 0} \left\Vert \frac{T(t)f - f}{t} - M_a f \right\Vert_\infty = 0 \qquad \text{for all } f \in C_c(X)$$ and use the fact that $C_c(X)$ is dense in $C_0(X)$. But I figured out that I would have the same issues with that idea... I would appreciate some hints on the topic :)",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'semigroup-of-operators']"
90,Do commutative Banach algebras with totally disconnected spectrum have linearly dense idempotents?,Do commutative Banach algebras with totally disconnected spectrum have linearly dense idempotents?,,"Suppose that $A$ is a unital commutative Banach algebra. It is a nice application of the Shilov idempotent theorem that if the spectrum of $A$ is totally disconnected, then $A$ is regular. Can we show that idempotents in $A$ are linearly dense if the spectum of $A$ is totally disconnected? I think it must be known. This is really asking for the converse to this fact .","Suppose that $A$ is a unital commutative Banach algebra. It is a nice application of the Shilov idempotent theorem that if the spectrum of $A$ is totally disconnected, then $A$ is regular. Can we show that idempotents in $A$ are linearly dense if the spectum of $A$ is totally disconnected? I think it must be known. This is really asking for the converse to this fact .",,"['functional-analysis', 'reference-request', 'operator-algebras', 'banach-algebras']"
91,"closed ideals of $C(X,\mathcal{B})$",closed ideals of,"C(X,\mathcal{B})","Question:- Let $X$ be a compact Hausdroff space and $\mathcal{B}$ be a simple C*-algebra. Prove that the closed ideals of $\mathcal{B}\otimes_{min} C(X) \cong^* C(X,\mathcal{B})$ are of the form $\mathcal{B}\otimes J$ where $J$ is a closed ideal of $C(X)$. Proof:- Let $I$ be a proper closed ideal of $C(X,\mathcal{B})$ and set  $$E = \{x\in X : f(x) = 0 \text{ for all } f\in I\}.$$ Then $E$ is a closed subset of $X$ and $I\subseteq I(E) := \{f\in C(X,\mathcal{B}) : f(x) = 0 \text{ for all } x\in E\}$. Indeed, let $x$ be a limit point of $E$. Suppose there exist $f\in I$ such that $f(x)\neq 0$. Then, by continuity, there exists an open set $U$ containing $x$, such that $f(y)\neq 0$ for all $y\in U$. Since $U$ is neighborhood of $x$, there is $q\in E$ and $q\neq x$, for which $f(q)\neq 0$, which is a contradiction. Therefore, $x\in E$. claim : $I(E)\subseteq I$. Assume that $X\cap E = \emptyset$. Then, for every point $p\in X$, there exists a continuous function $f\in I$ such that $f(p)\neq 0$. Then, by continuity, there must exist an open set $U$ containing $p$ so that $f(q)\neq 0$ for all $q\in U$. Thus, we may assign to each point $p\in X$ a continuous function $f\in I$ and an open set $U$ of $X$ such that $f(q)\neq 0$ for all $q\in U$. Since this collection of open sets covers $X$, which is compact, there must exists a finite subcover which also covers $X$. Call this subcover $U_1,\cdots, U_n$ and the corresponding functions $f_1, \cdots, f_n$. Consider the function $g$ defined as  $$g(x) = f_1(x) f^*_1(x)+\cdots+f_n(x) f^*_n(x).$$ Since $I$ is an ideal, $g\in I$. For every point $p\in X$, there exists an integer $i$ between $1$ and $n$ such that $f_i(p)\neq 0$. This implies that $g(p) \neq 0$. Also, $g$ is positive element of $\mathcal{B}$. how to proceed further...... if I prove that $g^{-1}$ exist, then $g.g^{-1}\in I$, which implies, $I = C(X,\mathcal{B})$, a contradiction (as $I$ is proper).","Question:- Let $X$ be a compact Hausdroff space and $\mathcal{B}$ be a simple C*-algebra. Prove that the closed ideals of $\mathcal{B}\otimes_{min} C(X) \cong^* C(X,\mathcal{B})$ are of the form $\mathcal{B}\otimes J$ where $J$ is a closed ideal of $C(X)$. Proof:- Let $I$ be a proper closed ideal of $C(X,\mathcal{B})$ and set  $$E = \{x\in X : f(x) = 0 \text{ for all } f\in I\}.$$ Then $E$ is a closed subset of $X$ and $I\subseteq I(E) := \{f\in C(X,\mathcal{B}) : f(x) = 0 \text{ for all } x\in E\}$. Indeed, let $x$ be a limit point of $E$. Suppose there exist $f\in I$ such that $f(x)\neq 0$. Then, by continuity, there exists an open set $U$ containing $x$, such that $f(y)\neq 0$ for all $y\in U$. Since $U$ is neighborhood of $x$, there is $q\in E$ and $q\neq x$, for which $f(q)\neq 0$, which is a contradiction. Therefore, $x\in E$. claim : $I(E)\subseteq I$. Assume that $X\cap E = \emptyset$. Then, for every point $p\in X$, there exists a continuous function $f\in I$ such that $f(p)\neq 0$. Then, by continuity, there must exist an open set $U$ containing $p$ so that $f(q)\neq 0$ for all $q\in U$. Thus, we may assign to each point $p\in X$ a continuous function $f\in I$ and an open set $U$ of $X$ such that $f(q)\neq 0$ for all $q\in U$. Since this collection of open sets covers $X$, which is compact, there must exists a finite subcover which also covers $X$. Call this subcover $U_1,\cdots, U_n$ and the corresponding functions $f_1, \cdots, f_n$. Consider the function $g$ defined as  $$g(x) = f_1(x) f^*_1(x)+\cdots+f_n(x) f^*_n(x).$$ Since $I$ is an ideal, $g\in I$. For every point $p\in X$, there exists an integer $i$ between $1$ and $n$ such that $f_i(p)\neq 0$. This implies that $g(p) \neq 0$. Also, $g$ is positive element of $\mathcal{B}$. how to proceed further...... if I prove that $g^{-1}$ exist, then $g.g^{-1}\in I$, which implies, $I = C(X,\mathcal{B})$, a contradiction (as $I$ is proper).",,"['functional-analysis', 'operator-algebras', 'c-star-algebras']"
92,Prove a linear operator is continuous wrt weak topology iff it is continuous at $0$ wrt weak topology,Prove a linear operator is continuous wrt weak topology iff it is continuous at  wrt weak topology,0,"Let $\mathbb E$ and $\mathbb F$ be two Banach spaces and $T:\mathbb E\to \mathbb F$ a linear operator. Use the definition of the weak topology to show that $T$ is continuous with respect $\mathbb E$ being equipped with the weak topology $\sigma(\mathbb E,\mathbb E^*)$ and $\mathbb F$ being equipped with  the weak topology $\sigma(\mathbb F,\mathbb F^*)$ if and only if it is continuous at $0\in \mathbb E$ with respect to these weak topologies. I think the idea behind is that continuity (weak) implies continuity (strong) and we know function is continuous iff it is continuous at one point. so this is also continuous(strong) at $0$, then again continuity(strong) implies continuity(weak) and we are done. I am not sure whether my rough sketch is right or wrong? Also couldn't quite get the way of perfectly writing it. Any help would be highly appreciated. Thanks in advance.","Let $\mathbb E$ and $\mathbb F$ be two Banach spaces and $T:\mathbb E\to \mathbb F$ a linear operator. Use the definition of the weak topology to show that $T$ is continuous with respect $\mathbb E$ being equipped with the weak topology $\sigma(\mathbb E,\mathbb E^*)$ and $\mathbb F$ being equipped with  the weak topology $\sigma(\mathbb F,\mathbb F^*)$ if and only if it is continuous at $0\in \mathbb E$ with respect to these weak topologies. I think the idea behind is that continuity (weak) implies continuity (strong) and we know function is continuous iff it is continuous at one point. so this is also continuous(strong) at $0$, then again continuity(strong) implies continuity(weak) and we are done. I am not sure whether my rough sketch is right or wrong? Also couldn't quite get the way of perfectly writing it. Any help would be highly appreciated. Thanks in advance.",,['functional-analysis']
93,Considering operators on the direct sum of Hilbert spaces as operator valued matrices,Considering operators on the direct sum of Hilbert spaces as operator valued matrices,,"If $(\mathcal H_i)_{i \in I}$ is a family of Hilbert spaces, we can form their Hilbert space direct sum $$ \tilde{\mathcal H} := \oplus^{\ell^2}_{i \in I} \mathcal H_i $$ where the $\ell^2$ expresses that norms of elements of $\tilde{\mathcal H}$ have to summable with respect to the 2-norm. Now, for simplicity, let $\mathcal H_i = \mathcal H$ for one fixed Hilbert space and let $$ \tilde{\mathcal H} := \oplus^{\ell^2}_{i \in I} \mathcal H = \ell^2(I, \mathcal H), $$ using the notation of Bochner-Lebesgue spaces ( I think I will have to restrict $\mathcal H$ to be a separable space, right? ). Let $\mathfrak B(\mathcal H)$ denote the space of all bounded operators on $\mathcal H$. From the algebraical viewpoint we have that  $$ \mathfrak B(\ell^2(I, \mathcal H)) \subseteq M_I(\mathfrak B(\mathcal H)) := \left\{ (x_{ij})_{ij} \colon x_{ij} \in \mathfrak B(\mathcal H) \right\}, $$ i.e. every element of $\mathfrak B(\ell^2(I,\mathcal H))$ is a ""matrix"" with entries in $\mathfrak B(\mathcal H)$. Can we characterize an element $x$ of $\mathfrak B(\ell^2(I,\mathcal H))$ in terms of its ""entries"" $x_{ij}$? Here are my thoughts: Let $x = (x_{ij})_{ij}$ and $\xi = \oplus_i \xi_i \in \tilde{\mathcal H}$. If we want $x \in \mathfrak B(\tilde{\mathcal H})$ the following calculation could provide some necessary conditions: $$ \| x\xi\|_2^2 = \left\| \bigoplus_i \left(\sum_j x_{ij} \xi_j\right) \right\|_2^2 = \sum_i \left\| \sum_j x_{ij} \xi_j \right\|^2 \leq \\  \sum_i \sum_j \|x_{ij}\|^2 \|\xi_j\|^2  \leq \underbrace{\sum_i \left( \sup_j \|x_{ij}\| \right)^2}_{=: C < \infty} \sum_j \|\xi_j\|^2, $$ where for the last inequality to hold I assumed that for each $i \in I$ the family $(x_{ij})_{j \in I}$ is uniformly bounded and that the family $s_i := \sup_j \|x_{ij}\|$ is in $\ell^2$. This would give me something like $x \in \ell^2( I, \ell^\infty(I, \mathfrak B(\mathcal H))$, but this is clearly not an algebra. I am happy about comments on my approach. Maybe someone can share a reference on this matter?","If $(\mathcal H_i)_{i \in I}$ is a family of Hilbert spaces, we can form their Hilbert space direct sum $$ \tilde{\mathcal H} := \oplus^{\ell^2}_{i \in I} \mathcal H_i $$ where the $\ell^2$ expresses that norms of elements of $\tilde{\mathcal H}$ have to summable with respect to the 2-norm. Now, for simplicity, let $\mathcal H_i = \mathcal H$ for one fixed Hilbert space and let $$ \tilde{\mathcal H} := \oplus^{\ell^2}_{i \in I} \mathcal H = \ell^2(I, \mathcal H), $$ using the notation of Bochner-Lebesgue spaces ( I think I will have to restrict $\mathcal H$ to be a separable space, right? ). Let $\mathfrak B(\mathcal H)$ denote the space of all bounded operators on $\mathcal H$. From the algebraical viewpoint we have that  $$ \mathfrak B(\ell^2(I, \mathcal H)) \subseteq M_I(\mathfrak B(\mathcal H)) := \left\{ (x_{ij})_{ij} \colon x_{ij} \in \mathfrak B(\mathcal H) \right\}, $$ i.e. every element of $\mathfrak B(\ell^2(I,\mathcal H))$ is a ""matrix"" with entries in $\mathfrak B(\mathcal H)$. Can we characterize an element $x$ of $\mathfrak B(\ell^2(I,\mathcal H))$ in terms of its ""entries"" $x_{ij}$? Here are my thoughts: Let $x = (x_{ij})_{ij}$ and $\xi = \oplus_i \xi_i \in \tilde{\mathcal H}$. If we want $x \in \mathfrak B(\tilde{\mathcal H})$ the following calculation could provide some necessary conditions: $$ \| x\xi\|_2^2 = \left\| \bigoplus_i \left(\sum_j x_{ij} \xi_j\right) \right\|_2^2 = \sum_i \left\| \sum_j x_{ij} \xi_j \right\|^2 \leq \\  \sum_i \sum_j \|x_{ij}\|^2 \|\xi_j\|^2  \leq \underbrace{\sum_i \left( \sup_j \|x_{ij}\| \right)^2}_{=: C < \infty} \sum_j \|\xi_j\|^2, $$ where for the last inequality to hold I assumed that for each $i \in I$ the family $(x_{ij})_{j \in I}$ is uniformly bounded and that the family $s_i := \sup_j \|x_{ij}\|$ is in $\ell^2$. This would give me something like $x \in \ell^2( I, \ell^\infty(I, \mathfrak B(\mathcal H))$, but this is clearly not an algebra. I am happy about comments on my approach. Maybe someone can share a reference on this matter?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'operator-algebras', 'bochner-spaces']"
94,Questions about the proof of the Sturm oscillation theorem,Questions about the proof of the Sturm oscillation theorem,,"I'm trying to understand the proof of the Sturm oscillation theorem and I hit the roadblock. Theorem: Let $E_0<E_1<\dots$ be the eigenvalues of $H=-\frac{d^2}{dx^2}+V(x)$ on $L^2(0,a)$ with boundary conditions $u(0)=u(a)=0$. Then $u(x,E_n)$ has exactly $n$ zeros in $(0,a)$. Part of the proof: Suppose $u_n$ has $m$ zeros $x_1<\dots<x_m$ in $(0,a)$. Let $v_0,\dots,v_m$ be the function $u_n$ restricted successively to $(0,x_1),(x_1,x_2),...,(x_m,a)$. The $v$'s are continuous and piecewise $C^1$ with $v_l(0)=v_l(a)=0$. Thus they lie in the quadratic form domain of $H$ and  $$ <v_j,Hv_k>=\int_0^a v'_j v'_k + \int_0^a Vv_jv_k=\delta_{jk}E\int_0^a v_j^2dx $$ since if $j=k$, we can integrate by parts and use $-u''+Vu=Eu$. It follows that for any $v$ in the span of $v_j$'s, $<v,Hv>=E\|v\|^2$, so by the variational principle, $H$ has at least $m+1$ eigenvalues in $(-\infty, E_n]$, that is, $n+1\geq m+1$ Questions: Why is it possible to express $<v_j,Hv_k>$ as in the formula above and how the expression is integrated by parts? What is the variational principle mentioned above that allows to determine the number of eigenvalues in the interval $(-\infty, E_n]$?","I'm trying to understand the proof of the Sturm oscillation theorem and I hit the roadblock. Theorem: Let $E_0<E_1<\dots$ be the eigenvalues of $H=-\frac{d^2}{dx^2}+V(x)$ on $L^2(0,a)$ with boundary conditions $u(0)=u(a)=0$. Then $u(x,E_n)$ has exactly $n$ zeros in $(0,a)$. Part of the proof: Suppose $u_n$ has $m$ zeros $x_1<\dots<x_m$ in $(0,a)$. Let $v_0,\dots,v_m$ be the function $u_n$ restricted successively to $(0,x_1),(x_1,x_2),...,(x_m,a)$. The $v$'s are continuous and piecewise $C^1$ with $v_l(0)=v_l(a)=0$. Thus they lie in the quadratic form domain of $H$ and  $$ <v_j,Hv_k>=\int_0^a v'_j v'_k + \int_0^a Vv_jv_k=\delta_{jk}E\int_0^a v_j^2dx $$ since if $j=k$, we can integrate by parts and use $-u''+Vu=Eu$. It follows that for any $v$ in the span of $v_j$'s, $<v,Hv>=E\|v\|^2$, so by the variational principle, $H$ has at least $m+1$ eigenvalues in $(-\infty, E_n]$, that is, $n+1\geq m+1$ Questions: Why is it possible to express $<v_j,Hv_k>$ as in the formula above and how the expression is integrated by parts? What is the variational principle mentioned above that allows to determine the number of eigenvalues in the interval $(-\infty, E_n]$?",,"['functional-analysis', 'operator-theory', 'sturm-liouville']"
95,Fourier transform of $e^{-i|x|^2}$,Fourier transform of,e^{-i|x|^2},"I am trying to calculate the Fourier transform of $f(x)=e^{-i|x|^2}$ for $x\in\mathbb{R}^n$. Roughly speaking, the Fresnel integral implies that $$\hat{f}(\xi)=(2i)^{-n/2}e^{i|\xi|^2/4}$$ where the Fourier transform of a function $g$ is defined as $$\hat{g}(\xi)=(2\pi)^{-n/2}\int_{\mathbb{R}^n}g(x)e^{-ix\cdot\xi}dx,\ \ \xi\in\mathbb{R}^n.$$ However, as this post says, $f$ is not in $L^1$ and the Fresnel integral holds in the sense of improper Riemann integral not in the sense of Lebesgue integral. I think this equality $\hat{f}(\xi)=(2i)^{-n/2}e^{i|\xi|^2/4}$  is true in the sense of distributions and the proof of this calculation is based on distribution theory, but I cannot go further. How can we prove this? I appreciate any advice.","I am trying to calculate the Fourier transform of $f(x)=e^{-i|x|^2}$ for $x\in\mathbb{R}^n$. Roughly speaking, the Fresnel integral implies that $$\hat{f}(\xi)=(2i)^{-n/2}e^{i|\xi|^2/4}$$ where the Fourier transform of a function $g$ is defined as $$\hat{g}(\xi)=(2\pi)^{-n/2}\int_{\mathbb{R}^n}g(x)e^{-ix\cdot\xi}dx,\ \ \xi\in\mathbb{R}^n.$$ However, as this post says, $f$ is not in $L^1$ and the Fresnel integral holds in the sense of improper Riemann integral not in the sense of Lebesgue integral. I think this equality $\hat{f}(\xi)=(2i)^{-n/2}e^{i|\xi|^2/4}$  is true in the sense of distributions and the proof of this calculation is based on distribution theory, but I cannot go further. How can we prove this? I appreciate any advice.",,"['functional-analysis', 'distribution-theory', 'fourier-transform']"
96,Dense subspaces of $L^p$,Dense subspaces of,L^p,"Let $B$ a separable Banach space and $\nu$ a Borel probability measure on $B$.  Let us consider the space $C^1_b(B)$ of the continuously differentiable functions bounded and with bounded derivative.  Is it true that $C^1_b(B)$ is dense in $L^p(B,\mu)$ for every $p \ne \infty$?","Let $B$ a separable Banach space and $\nu$ a Borel probability measure on $B$.  Let us consider the space $C^1_b(B)$ of the continuously differentiable functions bounded and with bounded derivative.  Is it true that $C^1_b(B)$ is dense in $L^p(B,\mu)$ for every $p \ne \infty$?",,"['functional-analysis', 'measure-theory', 'geometric-measure-theory']"
97,Using bump functions to create desired function,Using bump functions to create desired function,,"Let $\lambda \in C^\infty(\mathbb{R})$ satisfy $$ \begin{cases} \lambda(x) = 0, &\text{if $x \leq 0$} \\ \lambda(x) > 1 &\text{if $x > 1$}\\ \lambda''(x) \geq 100\lambda'(x)  &\text{for all $x$}\\ \lambda''(x) > 0  &\text{if  $x > 0$}\\ \lambda'(x) > 100 &\text{if $\lambda(x) > 1/2$}.  \end{cases} $$ The paper I am reading says ""such a function obviously exists.""  Why is this obvious?  It looks like we will have to construct bump functions.  In my previous classes, whenever we dealt with a bump function, the only specification we put on the bump function was its support.  When we construct bump functions, can we specify restrictions on the first and second derivatives?  Is there a general procedure for problems like these?","Let $\lambda \in C^\infty(\mathbb{R})$ satisfy $$ \begin{cases} \lambda(x) = 0, &\text{if $x \leq 0$} \\ \lambda(x) > 1 &\text{if $x > 1$}\\ \lambda''(x) \geq 100\lambda'(x)  &\text{for all $x$}\\ \lambda''(x) > 0  &\text{if  $x > 0$}\\ \lambda'(x) > 100 &\text{if $\lambda(x) > 1/2$}.  \end{cases} $$ The paper I am reading says ""such a function obviously exists.""  Why is this obvious?  It looks like we will have to construct bump functions.  In my previous classes, whenever we dealt with a bump function, the only specification we put on the bump function was its support.  When we construct bump functions, can we specify restrictions on the first and second derivatives?  Is there a general procedure for problems like these?",,"['real-analysis', 'functional-analysis', 'analysis']"
98,Application of the Spectral Theorem,Application of the Spectral Theorem,,Suppose $T$ is a compact self-adjoint operator and $f$ is a unit vector such that $\| (T -3)f \| \leq 1/2$. Denote by $p$ the orthogonal projection onto the direct sum of eigenspaces of $T$ with eigenvalue $2 \leq \lambda \leq 4$. I want to show that $$\| p f \| \geq \frac{\sqrt{3}}{2}.$$ I'm unsure of how to consider the norm of $p$ and how to relate it to the eigenvalues. Perhaps $$\| Tf \| = \lambda \| p \circ T f \|?$$,Suppose $T$ is a compact self-adjoint operator and $f$ is a unit vector such that $\| (T -3)f \| \leq 1/2$. Denote by $p$ the orthogonal projection onto the direct sum of eigenspaces of $T$ with eigenvalue $2 \leq \lambda \leq 4$. I want to show that $$\| p f \| \geq \frac{\sqrt{3}}{2}.$$ I'm unsure of how to consider the norm of $p$ and how to relate it to the eigenvalues. Perhaps $$\| Tf \| = \lambda \| p \circ T f \|?$$,,"['real-analysis', 'functional-analysis']"
99,*-homomorphism between concrete von Neumann algebras is SOT-SOT continuous iff it is WOT-WOT continuous,*-homomorphism between concrete von Neumann algebras is SOT-SOT continuous iff it is WOT-WOT continuous,,"Let $\mathcal H, \mathcal K$ be Hilbert spaces and $M \subseteq B(\mathcal H)$ a (concrete) von Neumann algebra (Here, $B(\mathcal H)$ denotes the algebra of bounded operators on $\mathcal H$). Furthermore, let $$\pi \colon M \to B(\mathcal K)$$ be a *-Homomorphism. Note that $\pi$ is automatically a contractive, i.e. especially bounded, operator. I want to prove that $\pi$ is strong-strong (SOT-SOT) continuous if and only if $\pi$ is weak-weak (WOT-WOT) continuous. strong-strong means that $B(\mathcal H)$ as well as $B(\mathcal K)$ carry the strong operator topology (SOT). Same goes for the term weak-weak and the weak operator topology (WOT). I know that the topological duals of $B(\mathcal H)$ w.r.t the strong/weak operator topologies coincide, i.e. a linear functional on $B(\mathcal H)$ is strongly continuous if and only if it is weakly continuous. The same property is inherited by the sub-von Neumann Algebra $M$. Do you know how to prove this or can you share a reference on this matter? EDIT . I have asked another more general question which could yield an answer to this concrete problem. See here . However, I am not conviced that there is a closed-graph theorem for the strong and weak operator topologies. 2nd EDIT (06.04.2017) I think one can prove WOT-WOT implies SOT-SOT in a straigthforward way: It suffices to prove that if $x_i \overset{\mathrm{SOT}}{\to} 0$ then $\pi(x_i) \overset{\mathrm{SOT}}{\to} 0$. We know that $$x_i \overset{\mathrm{SOT}}{\to} 0 \iff x_i^* x_i \overset{\mathrm{WOT}}{\to} 0 \tag{$\ast$}.$$ Now if $x_i \overset{\mathrm{SOT}}{\to} 0$, since $\pi$ is WOT-WOT continuous, ($\ast$) gives that $$\pi(x_i)^* \pi(x_i) = \pi(x_i^* x_i) \overset{\mathrm{WOT}}{\to} 0$$ which, again by ($\ast$) implies that $\pi(x_i) \overset{\mathrm{SOT}}{\to} 0$.","Let $\mathcal H, \mathcal K$ be Hilbert spaces and $M \subseteq B(\mathcal H)$ a (concrete) von Neumann algebra (Here, $B(\mathcal H)$ denotes the algebra of bounded operators on $\mathcal H$). Furthermore, let $$\pi \colon M \to B(\mathcal K)$$ be a *-Homomorphism. Note that $\pi$ is automatically a contractive, i.e. especially bounded, operator. I want to prove that $\pi$ is strong-strong (SOT-SOT) continuous if and only if $\pi$ is weak-weak (WOT-WOT) continuous. strong-strong means that $B(\mathcal H)$ as well as $B(\mathcal K)$ carry the strong operator topology (SOT). Same goes for the term weak-weak and the weak operator topology (WOT). I know that the topological duals of $B(\mathcal H)$ w.r.t the strong/weak operator topologies coincide, i.e. a linear functional on $B(\mathcal H)$ is strongly continuous if and only if it is weakly continuous. The same property is inherited by the sub-von Neumann Algebra $M$. Do you know how to prove this or can you share a reference on this matter? EDIT . I have asked another more general question which could yield an answer to this concrete problem. See here . However, I am not conviced that there is a closed-graph theorem for the strong and weak operator topologies. 2nd EDIT (06.04.2017) I think one can prove WOT-WOT implies SOT-SOT in a straigthforward way: It suffices to prove that if $x_i \overset{\mathrm{SOT}}{\to} 0$ then $\pi(x_i) \overset{\mathrm{SOT}}{\to} 0$. We know that $$x_i \overset{\mathrm{SOT}}{\to} 0 \iff x_i^* x_i \overset{\mathrm{WOT}}{\to} 0 \tag{$\ast$}.$$ Now if $x_i \overset{\mathrm{SOT}}{\to} 0$, since $\pi$ is WOT-WOT continuous, ($\ast$) gives that $$\pi(x_i)^* \pi(x_i) = \pi(x_i^* x_i) \overset{\mathrm{WOT}}{\to} 0$$ which, again by ($\ast$) implies that $\pi(x_i) \overset{\mathrm{SOT}}{\to} 0$.",,"['functional-analysis', 'reference-request', 'operator-theory', 'topological-vector-spaces', 'von-neumann-algebras']"
