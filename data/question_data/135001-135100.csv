,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solving $ y' -3y = x \cdot e^x $,Solving, y' -3y = x \cdot e^x ,I'm struggling with this differential equation:  $ y' -3y = x \cdot e^x $ Step one: Solve the homogenous equation  $ y' -3y = 0 $  $$ y_0 = e^{3x} \cdot C $$ The approach for the particular solution $y_p$ should be $(c_1x+c_0)\cdot Ce^x$ This leads me to the following term:  $$ y_p = Ce^x(ax+b) $$ $$ y'_p= Ce^x\cdot(ax+b)+Ce^x\cdot a $$ $$ Ce^x\cdot(ax+b)+Ce^x\cdot a - 3Ce^x(ax+b) = x\cdot e^x $$ $$ C\cdot(ax+b)+C\cdot a - 3C(ax+b) = x $$ The C is a real problem here. After taking a look in my textbook I saw that the approach for $y_p$ is:  $$y_p = (ax + b)\cdot e^x $$  $$y'_p = e^x \cdot (ax +b)+e^x \cdot a $$ Plugging this into $ y' - 3y = x\cdot e^x $ and simplify gives: $$ x(-2a)+a-2b=x \Rightarrow a=-\frac{1}{2}; b=-\frac{1}{4}$$ And finally  $$ y= e^{3x}\cdot C - \frac{1}{4}(e^x+2xe^x) $$ When I omit the C I'm able to solve this but I don't know why this is correct as the approach for $g(x)$ would be $C\cdot e^{bx}$,I'm struggling with this differential equation:  $ y' -3y = x \cdot e^x $ Step one: Solve the homogenous equation  $ y' -3y = 0 $  $$ y_0 = e^{3x} \cdot C $$ The approach for the particular solution $y_p$ should be $(c_1x+c_0)\cdot Ce^x$ This leads me to the following term:  $$ y_p = Ce^x(ax+b) $$ $$ y'_p= Ce^x\cdot(ax+b)+Ce^x\cdot a $$ $$ Ce^x\cdot(ax+b)+Ce^x\cdot a - 3Ce^x(ax+b) = x\cdot e^x $$ $$ C\cdot(ax+b)+C\cdot a - 3C(ax+b) = x $$ The C is a real problem here. After taking a look in my textbook I saw that the approach for $y_p$ is:  $$y_p = (ax + b)\cdot e^x $$  $$y'_p = e^x \cdot (ax +b)+e^x \cdot a $$ Plugging this into $ y' - 3y = x\cdot e^x $ and simplify gives: $$ x(-2a)+a-2b=x \Rightarrow a=-\frac{1}{2}; b=-\frac{1}{4}$$ And finally  $$ y= e^{3x}\cdot C - \frac{1}{4}(e^x+2xe^x) $$ When I omit the C I'm able to solve this but I don't know why this is correct as the approach for $g(x)$ would be $C\cdot e^{bx}$,,['ordinary-differential-equations']
1,How can I solve the following higher order ODE?,How can I solve the following higher order ODE?,,"I am trying to solve a system of two second-order ODEs. After separating them, I obtained a fourth-order independent ODE as illustrated below. I wonder if there is a specific technique to solve it. $$y^{(4)}+\frac{a_1}{x} y^{(3)}+\frac{a_2}{x^2}y^{(2)}+a_3y^{(2)}+\frac{a_4}{x}y^{(1)}+a_5y=0$$","I am trying to solve a system of two second-order ODEs. After separating them, I obtained a fourth-order independent ODE as illustrated below. I wonder if there is a specific technique to solve it. $$y^{(4)}+\frac{a_1}{x} y^{(3)}+\frac{a_2}{x^2}y^{(2)}+a_3y^{(2)}+\frac{a_4}{x}y^{(1)}+a_5y=0$$",,['ordinary-differential-equations']
2,Solving homogeneous differential equation $\frac{dy}{dx}=\frac{x^2+8y^2}{3xy}$.,Solving homogeneous differential equation .,\frac{dy}{dx}=\frac{x^2+8y^2}{3xy},Solve the differential equation. Use the fact that the given equation is homogeneous   $$ \frac{dy}{dx}=\frac{x^2+8y^2}{3xy} $$ First I multiply the right side by $$ \frac{\frac{1}{x^2}}{\frac{1}{x^2}} $$ Then $$\frac{dy}{dx}=\frac{1+\frac{8y^2}{x^2}}{\frac{3y}{x}} $$ $$ Let: v=\frac{y}{x} $$ $$ \frac{dy}{dx}=v+x\frac{dv}{dx} $$ Then substitute $$ \frac{dy}{dx}=\frac{1+v^2}{3v}$$ $$ \frac{1+v^2}{3v}=v+x\frac{dv}{dx}$$ How do I find the solution from here?,Solve the differential equation. Use the fact that the given equation is homogeneous   $$ \frac{dy}{dx}=\frac{x^2+8y^2}{3xy} $$ First I multiply the right side by $$ \frac{\frac{1}{x^2}}{\frac{1}{x^2}} $$ Then $$\frac{dy}{dx}=\frac{1+\frac{8y^2}{x^2}}{\frac{3y}{x}} $$ $$ Let: v=\frac{y}{x} $$ $$ \frac{dy}{dx}=v+x\frac{dv}{dx} $$ Then substitute $$ \frac{dy}{dx}=\frac{1+v^2}{3v}$$ $$ \frac{1+v^2}{3v}=v+x\frac{dv}{dx}$$ How do I find the solution from here?,,['ordinary-differential-equations']
3,"Find particular solution of $y''+4y=12$ if the point $(0,5)$ has horizontal tangent line",Find particular solution of  if the point  has horizontal tangent line,"y''+4y=12 (0,5)","Find the particular solution of $$y''+4y=12$$ if the point $(0,5)$ has horizontal tangent line (parallel to the $x$-axis). I know the general solution of $y''+4y=12$ is $$y=C_1\cos{(2x)}+C_2\sin{(2x)}+3\quad\text{for some}~C_1,C_2\in\mathbb R.$$ Now we have to find the particular solution but I don't know if the initial conditions are correct: $$\begin{cases}y''+4y=12\\\color{red}{y(0)=5}\\\color{red}{y'(0)=k,\quad\text{for some}~k\in\mathbb R}.\end{cases}$$ Are the $\color{red}{\text{initial conditions}}$ correct? Thanks!","Find the particular solution of $$y''+4y=12$$ if the point $(0,5)$ has horizontal tangent line (parallel to the $x$-axis). I know the general solution of $y''+4y=12$ is $$y=C_1\cos{(2x)}+C_2\sin{(2x)}+3\quad\text{for some}~C_1,C_2\in\mathbb R.$$ Now we have to find the particular solution but I don't know if the initial conditions are correct: $$\begin{cases}y''+4y=12\\\color{red}{y(0)=5}\\\color{red}{y'(0)=k,\quad\text{for some}~k\in\mathbb R}.\end{cases}$$ Are the $\color{red}{\text{initial conditions}}$ correct? Thanks!",,['ordinary-differential-equations']
4,A conserved quantity reduces the dimension of the system?,A conserved quantity reduces the dimension of the system?,,"Suppose $\dot{x} = f(x)$ is a dynamical system on the state space $X$. My notes define a conservative system as one where there exists a (nontrivial) function $H: X \rightarrow \mathbb{R}$ such that $$\frac{d}{dt}H(x) = 0 = \nabla H(x) \cdot f(x)$$ If we then define $\Sigma_E := H^{-1}(E)$, $E \in \mathbb{R}$ as level sets of $H$, it follows that $\Sigma_E$ is an invariant set due to $H$ being constant along orbits. Immediately after this proof my notes puzzlingly state ""the dimension of the problem is reduced by one"". Later on when introducing Hamiltonian systems as a subset of conservative systems the classic example of the pendulum is given: \begin{align} &\dot{\theta} = p \\ &\dot{p} = -\sin(\theta)\\ \end{align} It is noted that there is the Hamiltonian $H(p,\theta) = \frac{1}{2}p^2 -\cos(\theta)$ and then shown how any Hamiltonian must be a conserved quantity. Immediately after it is written that ""it is useful to notice conserved quantities when they exist as they reduce the dimension of the problem"". Am I missing something obvious? In the case of the pendulum the state space is 2-dimensional, having the Hamiltonian and plotting its level sets gives us the entire phase portrait which is obviously still 2-dimensional. I know a Hamiltonian system has some very nice properties over a generic system, but I do not know one relating to reducing dimensions. I guess my main question really is ""why do we care about conserved quantities?"".","Suppose $\dot{x} = f(x)$ is a dynamical system on the state space $X$. My notes define a conservative system as one where there exists a (nontrivial) function $H: X \rightarrow \mathbb{R}$ such that $$\frac{d}{dt}H(x) = 0 = \nabla H(x) \cdot f(x)$$ If we then define $\Sigma_E := H^{-1}(E)$, $E \in \mathbb{R}$ as level sets of $H$, it follows that $\Sigma_E$ is an invariant set due to $H$ being constant along orbits. Immediately after this proof my notes puzzlingly state ""the dimension of the problem is reduced by one"". Later on when introducing Hamiltonian systems as a subset of conservative systems the classic example of the pendulum is given: \begin{align} &\dot{\theta} = p \\ &\dot{p} = -\sin(\theta)\\ \end{align} It is noted that there is the Hamiltonian $H(p,\theta) = \frac{1}{2}p^2 -\cos(\theta)$ and then shown how any Hamiltonian must be a conserved quantity. Immediately after it is written that ""it is useful to notice conserved quantities when they exist as they reduce the dimension of the problem"". Am I missing something obvious? In the case of the pendulum the state space is 2-dimensional, having the Hamiltonian and plotting its level sets gives us the entire phase portrait which is obviously still 2-dimensional. I know a Hamiltonian system has some very nice properties over a generic system, but I do not know one relating to reducing dimensions. I guess my main question really is ""why do we care about conserved quantities?"".",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-dynamics', 'integrable-systems']"
5,"If $\Vert f(x)-f(y) \Vert\leq k\Vert x-y\Vert,\;\;\forall\;x,y\in\Bbb{R}^n,$ then $x(t,x_0)=x_0,\forall \;t\geq 0,$",If  then,"\Vert f(x)-f(y) \Vert\leq k\Vert x-y\Vert,\;\;\forall\;x,y\in\Bbb{R}^n, x(t,x_0)=x_0,\forall \;t\geq 0,","We consider the following O.D.E \begin{align}(1)\;\;\;\begin{cases}x'(t)=f(x(t)) & t\geq 0,\\x(0)=x_0\in \Bbb{R}^n&\end{cases}\end{align} where \begin{align}f: \Bbb{R}^{n}\to \Bbb{R}^{n}\end{align} Assuming that \begin{align}\Vert f(x)-f(y) \Vert\leq  k\Vert x-y\Vert,\;\;\forall\;x,y\in\Bbb{R}^n.\end{align} Assuming that $f(x_0)=0.$ Please, do I prove that $x(t,x_0)=x_0,\forall \;t\geq 0,$ where $x(\cdot\,,x_0)$ is the solution of $(1)$. If there are references, I also would appreciate!","We consider the following O.D.E \begin{align}(1)\;\;\;\begin{cases}x'(t)=f(x(t)) & t\geq 0,\\x(0)=x_0\in \Bbb{R}^n&\end{cases}\end{align} where \begin{align}f: \Bbb{R}^{n}\to \Bbb{R}^{n}\end{align} Assuming that \begin{align}\Vert f(x)-f(y) \Vert\leq  k\Vert x-y\Vert,\;\;\forall\;x,y\in\Bbb{R}^n.\end{align} Assuming that $f(x_0)=0.$ Please, do I prove that $x(t,x_0)=x_0,\forall \;t\geq 0,$ where $x(\cdot\,,x_0)$ is the solution of $(1)$. If there are references, I also would appreciate!",,"['calculus', 'ordinary-differential-equations']"
6,How to understand partial differential equations in the sense of distribution?,How to understand partial differential equations in the sense of distribution?,,"I have just studied some elementary distribution theory. However, when attempting to apply them in solving partial differential equations I encounter the following confusion. Consider the heat equation: $$ \frac{\partial u(x,t)}{\partial t} - k^2\Delta u(x,t) = 0,$$ it is certainly clear what this equality means in the sense of ordinary functions. However, when considering the solution $u(x,t)$ as a distribution my textbook makes the following remark: ""...we assume that $ u(x,t) \in C( [0,\infty),S'(\mathbb{R}^n) ) $, i.e., $u(x,t)$ is continuous in $t$, $t \geq 0$, with values in $ S'(\mathbb{R}^n) $..."" My question is what does this mean exactly? I mean, as far as I have encountered distributions has nothing to do with variables unless they are identified with measures or functions. Thus in general I must think of the distribution $u(x,t)$ as a functional acting on certain space of functions. I can try to make sense of this by perhaps looking at a map $$ t \mapsto u(x,t) \in S'(\mathbb{R}^n) $$  which is continuous such that the argument $ x $ doesn't really play a role, hence I can consider $ u(x,.) $ as a distribution for every $t \in [0,1)$. Then again, if this is the case, how should I interpret derivative with respect to $t$? Any insight would be grateful! Thanks!","I have just studied some elementary distribution theory. However, when attempting to apply them in solving partial differential equations I encounter the following confusion. Consider the heat equation: $$ \frac{\partial u(x,t)}{\partial t} - k^2\Delta u(x,t) = 0,$$ it is certainly clear what this equality means in the sense of ordinary functions. However, when considering the solution $u(x,t)$ as a distribution my textbook makes the following remark: ""...we assume that $ u(x,t) \in C( [0,\infty),S'(\mathbb{R}^n) ) $, i.e., $u(x,t)$ is continuous in $t$, $t \geq 0$, with values in $ S'(\mathbb{R}^n) $..."" My question is what does this mean exactly? I mean, as far as I have encountered distributions has nothing to do with variables unless they are identified with measures or functions. Thus in general I must think of the distribution $u(x,t)$ as a functional acting on certain space of functions. I can try to make sense of this by perhaps looking at a map $$ t \mapsto u(x,t) \in S'(\mathbb{R}^n) $$  which is continuous such that the argument $ x $ doesn't really play a role, hence I can consider $ u(x,.) $ as a distribution for every $t \in [0,1)$. Then again, if this is the case, how should I interpret derivative with respect to $t$? Any insight would be grateful! Thanks!",,"['ordinary-differential-equations', 'partial-differential-equations', 'distribution-theory']"
7,Finding reason for removal of the logarithm in ODE,Finding reason for removal of the logarithm in ODE,,"I'm given an ODE $$y' = \frac{x + 2y + 1}{2x + 3},~y(5/2) = 1/4.$$ To solve this I'm doing the following: Solve system of equations $$2x_0 + 2y_0 = 1,$$ $$2x_0  = 3,$$ this gives me solutions $x_0 = \frac{3}{2}$, $y_0 = -\frac{1}{4}$. Then, I'm transforming variables $s = x + \frac{3}{2}$, $t = y - \frac{1}{4}$. Transform the ODE using those new variables to the form $$t' = \frac{s + 2t}{2s}$$ and using substitution of the form $t = s\cdot u(s)$ the equation is reduced to the $$u' = \frac{1}{2s} \implies u' = \frac{1}{2}\ln|C\cdot s|.$$ Using initial conditions constant C can be found using $y(5/2) = 1/4$, $s(5/2) = 4$, $t(4) = 0$, $u(4) = 0$, hence $0 = \ln|4C| \implies C = \pm \frac{1}{4}$. Putting it all together yields me $$y = \frac{1}{2}\left(x + \frac{3}{2}\right)\ln\left|\frac{x + 3/2}{4}\right| + 1/4$$ with two domains $I_1 = (-3/2, +\infty)$ and $I_2 = (-\infty, -3/2)$. But my textbook states that the answer is  $$y = \frac{1}{2}\left(x + \frac{3}{2}\right)\ln\frac{x + 3/2}{4} + 1/4,~I = (-3/2, +\infty)$$ Hence the following two questions: What was the reason for removal of absolute value in the logarithm? Why is mine domain of validity wrong?","I'm given an ODE $$y' = \frac{x + 2y + 1}{2x + 3},~y(5/2) = 1/4.$$ To solve this I'm doing the following: Solve system of equations $$2x_0 + 2y_0 = 1,$$ $$2x_0  = 3,$$ this gives me solutions $x_0 = \frac{3}{2}$, $y_0 = -\frac{1}{4}$. Then, I'm transforming variables $s = x + \frac{3}{2}$, $t = y - \frac{1}{4}$. Transform the ODE using those new variables to the form $$t' = \frac{s + 2t}{2s}$$ and using substitution of the form $t = s\cdot u(s)$ the equation is reduced to the $$u' = \frac{1}{2s} \implies u' = \frac{1}{2}\ln|C\cdot s|.$$ Using initial conditions constant C can be found using $y(5/2) = 1/4$, $s(5/2) = 4$, $t(4) = 0$, $u(4) = 0$, hence $0 = \ln|4C| \implies C = \pm \frac{1}{4}$. Putting it all together yields me $$y = \frac{1}{2}\left(x + \frac{3}{2}\right)\ln\left|\frac{x + 3/2}{4}\right| + 1/4$$ with two domains $I_1 = (-3/2, +\infty)$ and $I_2 = (-\infty, -3/2)$. But my textbook states that the answer is  $$y = \frac{1}{2}\left(x + \frac{3}{2}\right)\ln\frac{x + 3/2}{4} + 1/4,~I = (-3/2, +\infty)$$ Hence the following two questions: What was the reason for removal of absolute value in the logarithm? Why is mine domain of validity wrong?",,"['ordinary-differential-equations', 'absolute-value', 'initial-value-problems', 'homogeneous-equation']"
8,Linearization of differential system of equation,Linearization of differential system of equation,,"I would like to ask if I understand correctly the process of linearization for analyzing critical points. I was given differential equation: $\dot x = xy+1$ $\dot y = x+xy$ And my task was to linearize the system around stationary points. What I've done is: calculated critical point (I think the result is point $T(1,-1)$) and then I calculated Jacobian matrix and it's eigenvalues and corresponding eigenvectors and after that I've got that the point $T$ is saddle and then I drew the picture. I am not quite sure where can I check if steps that I've done are correct. Is there some program available for drawing phase portraits or online tool. Or if someone could solve the task and tell whether are my steps correct and if I'm missing something.","I would like to ask if I understand correctly the process of linearization for analyzing critical points. I was given differential equation: $\dot x = xy+1$ $\dot y = x+xy$ And my task was to linearize the system around stationary points. What I've done is: calculated critical point (I think the result is point $T(1,-1)$) and then I calculated Jacobian matrix and it's eigenvalues and corresponding eigenvectors and after that I've got that the point $T$ is saddle and then I drew the picture. I am not quite sure where can I check if steps that I've done are correct. Is there some program available for drawing phase portraits or online tool. Or if someone could solve the task and tell whether are my steps correct and if I'm missing something.",,"['linear-algebra', 'ordinary-differential-equations', 'jacobian', 'linearization']"
9,Recommended Books for differential equations?,Recommended Books for differential equations?,,"I am planning to take Differential equations next semester, but due to a timetable issue I want to study most of it this summer in my spare time to make it easier. These are the Topics that will be included, which I think represent about half of the Differential equations in other universities: Ordinary differential equations. Explicitly solvable equations, exact and linear equations. Well-posedness of the initial value problem, existence, uniqueness, continuous dependence on initial values. Approximate solution methods. Linear systems of equations, variational system.  Elements of stability theory, stability, asymptotic stability, Lyapunov functions, stability by the linear approximation. Phase portraits of planar autonomous equations. Laplace transform, application to solve differential equations. Discrete-time dynamical systems. I am not only looking for textbooks with rigorous exercise sets, any books are welcome, heavy in theory ones as well.","I am planning to take Differential equations next semester, but due to a timetable issue I want to study most of it this summer in my spare time to make it easier. These are the Topics that will be included, which I think represent about half of the Differential equations in other universities: Ordinary differential equations. Explicitly solvable equations, exact and linear equations. Well-posedness of the initial value problem, existence, uniqueness, continuous dependence on initial values. Approximate solution methods. Linear systems of equations, variational system.  Elements of stability theory, stability, asymptotic stability, Lyapunov functions, stability by the linear approximation. Phase portraits of planar autonomous equations. Laplace transform, application to solve differential equations. Discrete-time dynamical systems. I am not only looking for textbooks with rigorous exercise sets, any books are welcome, heavy in theory ones as well.",,"['ordinary-differential-equations', 'analysis', 'book-recommendation', 'advice']"
10,Stable limit cycle,Stable limit cycle,,"For a system of two differential equations, the eigenvalues determine the stability of the system. When the eigenvalues are positive, the system is unstable. In case of complex eigenvalues, the system is unstable when the real parts of the eigenvalues are positive. In the latter case, how do we mathematically show that the system enters a stable limit cycle when the fixed point is unstable? Considering the following non-linear system, Bier et al.'s model of yeast glycolysis \begin{align} \frac{dA}{dt} &= 2k_1GA - \frac{k_pA}{A+K_m}\\\\ \frac{dG}{dt} &= V_{in}-k_1GA\end{align} where $G$ refers to glucose and $A$ refers to ATP. The values of the parameters are $V_{in}=0.36$, $k_1=0.02$, and $k_p=6$. When $K_m=13$, the following behavior is observed ref. From the Jacobian of the matrix, the eigenvalues at the fixed point are $0.0040 + 0.1132i$ and $0.0040 - 0.1132i$. Here, the real parts of both eigenvalues are positive and we observe that the phase portrait shows a limit cycle. I would like to understand how complex eigenvalues with positive real parts are mathematically related to limit cycles. Any help would be much appreciated","For a system of two differential equations, the eigenvalues determine the stability of the system. When the eigenvalues are positive, the system is unstable. In case of complex eigenvalues, the system is unstable when the real parts of the eigenvalues are positive. In the latter case, how do we mathematically show that the system enters a stable limit cycle when the fixed point is unstable? Considering the following non-linear system, Bier et al.'s model of yeast glycolysis \begin{align} \frac{dA}{dt} &= 2k_1GA - \frac{k_pA}{A+K_m}\\\\ \frac{dG}{dt} &= V_{in}-k_1GA\end{align} where $G$ refers to glucose and $A$ refers to ATP. The values of the parameters are $V_{in}=0.36$, $k_1=0.02$, and $k_p=6$. When $K_m=13$, the following behavior is observed ref. From the Jacobian of the matrix, the eigenvalues at the fixed point are $0.0040 + 0.1132i$ and $0.0040 - 0.1132i$. Here, the real parts of both eigenvalues are positive and we observe that the phase portrait shows a limit cycle. I would like to understand how complex eigenvalues with positive real parts are mathematically related to limit cycles. Any help would be much appreciated",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'chemistry']"
11,Wronskian zero and linear dependence,Wronskian zero and linear dependence,,"Let $y''(x)+p(x)y'(x)+q(x)y(x)=0$ such that $p(x),q(x)$ are continuous. Let $y_1(x),y_2(x)$ be two solutions of the equation. Suppose that $y_1(x),y_2(x)$ both attain a maximum at a point $x_0$. Prove that $y_1,y_2$ are linearly dependent. Is it possible to say that $y_1'(x_0)=y_2'(x_0)=0$ and then, using Abel's formula, we get that $W[y_1,y_2,x]\equiv 0$ and so $y_1,y_2$ are linearly dependent? Moreover, in general can we say that if $p,q$ are continuous and $W[y_1,y_2,x]\equiv 0$ then the solutions $y_1,y_2$ are dependent? Edit: $W[y_1,y_2,x]=y_1(x)y_2'(x)-y_2(x)y_1'(x)\equiv 0$, so define $f(x)=\frac{y_2(x)}{y_1(x)}$. Then $f'(x)=\frac{y_1(x)y_2'(x)-y_2(x)y_1'(x)}{(y_1(x))^2}\equiv 0$. So $\exists c\in\mathbb{R}: f(x)\equiv c$ and finally $\exists c\in\mathbb{R}: y_2(x)\equiv c\cdot y_1(x)$. But, for this we need $y_1(x)\neq 0$.","Let $y''(x)+p(x)y'(x)+q(x)y(x)=0$ such that $p(x),q(x)$ are continuous. Let $y_1(x),y_2(x)$ be two solutions of the equation. Suppose that $y_1(x),y_2(x)$ both attain a maximum at a point $x_0$. Prove that $y_1,y_2$ are linearly dependent. Is it possible to say that $y_1'(x_0)=y_2'(x_0)=0$ and then, using Abel's formula, we get that $W[y_1,y_2,x]\equiv 0$ and so $y_1,y_2$ are linearly dependent? Moreover, in general can we say that if $p,q$ are continuous and $W[y_1,y_2,x]\equiv 0$ then the solutions $y_1,y_2$ are dependent? Edit: $W[y_1,y_2,x]=y_1(x)y_2'(x)-y_2(x)y_1'(x)\equiv 0$, so define $f(x)=\frac{y_2(x)}{y_1(x)}$. Then $f'(x)=\frac{y_1(x)y_2'(x)-y_2(x)y_1'(x)}{(y_1(x))^2}\equiv 0$. So $\exists c\in\mathbb{R}: f(x)\equiv c$ and finally $\exists c\in\mathbb{R}: y_2(x)\equiv c\cdot y_1(x)$. But, for this we need $y_1(x)\neq 0$.",,['ordinary-differential-equations']
12,Coordinate transformation of ODEs,Coordinate transformation of ODEs,,"I have the following system of ODEs: $x' = \alpha x - y - x(x^2 + y^2), y' = x + \alpha y - y(x^2+y^2)$, where $\alpha \in \mathbb{R}$ is a free parameter. I have to make a coordinate transformation using polar coordinates, $x = r \cos \theta, y = r \sin \theta$. I recognize that the $(x^2 + y^2) = r^2$. I am having difficulty in substituting the derivatives. I get that: $x'(t) = r'(t) \cos (\theta (t))-r(t) \theta '(t) \sin (\theta (t))$, and $y'(t) =r'(t) \sin (\theta (t))+r(t) \theta '(t) \cos (\theta (t))$ The problem as you can see is that this autonomous system of ODEs that originally just had an $x'(t)$ and $y'(t)$, now has a $r'(t)$ and $\theta'(t)$ in one ODE. I am unable to write down a system of ODEs as: $r'(t) = ... $ and $\theta'(t) =...$","I have the following system of ODEs: $x' = \alpha x - y - x(x^2 + y^2), y' = x + \alpha y - y(x^2+y^2)$, where $\alpha \in \mathbb{R}$ is a free parameter. I have to make a coordinate transformation using polar coordinates, $x = r \cos \theta, y = r \sin \theta$. I recognize that the $(x^2 + y^2) = r^2$. I am having difficulty in substituting the derivatives. I get that: $x'(t) = r'(t) \cos (\theta (t))-r(t) \theta '(t) \sin (\theta (t))$, and $y'(t) =r'(t) \sin (\theta (t))+r(t) \theta '(t) \cos (\theta (t))$ The problem as you can see is that this autonomous system of ODEs that originally just had an $x'(t)$ and $y'(t)$, now has a $r'(t)$ and $\theta'(t)$ in one ODE. I am unable to write down a system of ODEs as: $r'(t) = ... $ and $\theta'(t) =...$",,"['ordinary-differential-equations', 'dynamical-systems', 'coordinate-systems', 'polar-coordinates']"
13,How can I determine the type of bifurcation I have found?,How can I determine the type of bifurcation I have found?,,"I was tasked with finding and classifying the bifurcation of the following system: $$\frac{dx}{dt} = rx(x+1)-1$$ I have found that the bifurcation occurs when $r=1$, with equilibrium points at $x=0$ and $x=\frac{1}{r}-1$, but I am not sure how to determine the type of bifurcation it is. I have created a plot of the equilibrium points against r, but it looks nothing like any other plots I have seen that give the type of bifurcation. Is there a way to find the type of bifurcation it is analytically, or how can I otherwise determine the type of the bifurcation?","I was tasked with finding and classifying the bifurcation of the following system: $$\frac{dx}{dt} = rx(x+1)-1$$ I have found that the bifurcation occurs when $r=1$, with equilibrium points at $x=0$ and $x=\frac{1}{r}-1$, but I am not sure how to determine the type of bifurcation it is. I have created a plot of the equilibrium points against r, but it looks nothing like any other plots I have seen that give the type of bifurcation. Is there a way to find the type of bifurcation it is analytically, or how can I otherwise determine the type of the bifurcation?",,"['ordinary-differential-equations', 'dynamical-systems', 'bifurcation']"
14,"Solution of the differential equation $2\,y\, dx +(3x^2+y^2)\, dy =0$",Solution of the differential equation,"2\,y\, dx +(3x^2+y^2)\, dy =0","Please give me some hint to solve first order first degree differential equation:$2\,y\,dx +(3x^2+y^2)\, dy =0$. This cannot be solved by variable separable method. This is not exact or linear equation either. Putting $y=vx $ will not help. So I am clueless now. A small hint will be sufficient.","Please give me some hint to solve first order first degree differential equation:$2\,y\,dx +(3x^2+y^2)\, dy =0$. This cannot be solved by variable separable method. This is not exact or linear equation either. Putting $y=vx $ will not help. So I am clueless now. A small hint will be sufficient.",,['ordinary-differential-equations']
15,Hints on how to solve $(x+y^2)dy = ydx$?,Hints on how to solve ?,(x+y^2)dy = ydx,I'm looking for hints on how to solve the differential equation: $(x+y^2)dy = ydx$ . I tried finding an integrating factor and dividing both sides by $y$ but that didn't work.,I'm looking for hints on how to solve the differential equation: $(x+y^2)dy = ydx$ . I tried finding an integrating factor and dividing both sides by $y$ but that didn't work.,,"['calculus', 'ordinary-differential-equations']"
16,Nonlinear differential equation with absolute value,Nonlinear differential equation with absolute value,,"I am looking to solve the following nonlinear differential equation: $$y'(t) = |y(t)| + \cos(t),$$ for all $t$ in $[0,4]$ with $y(0) = -1.5$. I am really confused on how to approach it because of the absolute value.","I am looking to solve the following nonlinear differential equation: $$y'(t) = |y(t)| + \cos(t),$$ for all $t$ in $[0,4]$ with $y(0) = -1.5$. I am really confused on how to approach it because of the absolute value.",,"['ordinary-differential-equations', 'absolute-value']"
17,Solve $\operatorname{x}\operatorname{dy}-\operatorname{y}\operatorname{dx}=\operatorname{x}\sqrt{x^2-\operatorname{y^2}}\operatorname{dy} $,Solve,\operatorname{x}\operatorname{dy}-\operatorname{y}\operatorname{dx}=\operatorname{x}\sqrt{x^2-\operatorname{y^2}}\operatorname{dy} ,"How do I show that $$\operatorname{x}\operatorname{dy}-\operatorname{y}\operatorname{dx}=\operatorname{x}\sqrt{x^2-\operatorname{y^2}}\operatorname{dy} $$ has a primitive of: $$ \operatorname{y=x}\sin(y+C) $$ I must be missing something pretty clear because I'm pretty sure this isn't supposed to be a difficult problem. I've tried finding an integrating factor to change it into an exact equation, but nothing is popping out at me, nor is it homogeneous. I can't seem to write it as a Bernoulli Equation, and can't seem to find a way. I'm sure I'm just missing a glaring problem considering how simple the solution is. Thanks in advance!","How do I show that $$\operatorname{x}\operatorname{dy}-\operatorname{y}\operatorname{dx}=\operatorname{x}\sqrt{x^2-\operatorname{y^2}}\operatorname{dy} $$ has a primitive of: $$ \operatorname{y=x}\sin(y+C) $$ I must be missing something pretty clear because I'm pretty sure this isn't supposed to be a difficult problem. I've tried finding an integrating factor to change it into an exact equation, but nothing is popping out at me, nor is it homogeneous. I can't seem to write it as a Bernoulli Equation, and can't seem to find a way. I'm sure I'm just missing a glaring problem considering how simple the solution is. Thanks in advance!",,"['ordinary-differential-equations', 'trigonometry']"
18,Non linear differential equation $\sin(xy) =\frac {dy}{dx}$,Non linear differential equation,\sin(xy) =\frac {dy}{dx},A friend of mine set the question $\sin(xy) =\dfrac {dy}{dx}$ but after quite a long time I have made no headway (so far I have tried $v=xy$ and the solving it as a $1$ st order differential equations and also differentiating and trying to solve as a second order ( $3$ rd order makes things even messier). If anyone is able to tell me whether this is doable that would be fantastic! Edit: he said he had a closed form solution but if there was an infinite sum solution that would also be fine Edit a lot lot later. It turned out he was trolling me and the equation is not solvable,A friend of mine set the question but after quite a long time I have made no headway (so far I have tried and the solving it as a st order differential equations and also differentiating and trying to solve as a second order ( rd order makes things even messier). If anyone is able to tell me whether this is doable that would be fantastic! Edit: he said he had a closed form solution but if there was an infinite sum solution that would also be fine Edit a lot lot later. It turned out he was trolling me and the equation is not solvable,\sin(xy) =\dfrac {dy}{dx} v=xy 1 3,"['calculus', 'ordinary-differential-equations']"
19,Differential equation for non-ordinary homogenous equation,Differential equation for non-ordinary homogenous equation,,"What is the solution to the differential equation: $$y''+y^\alpha=0$$ where $\alpha\neq 1,0$? Tried finding solutions on websites with differential equation calculators but they gave nothing. So does it even exist?","What is the solution to the differential equation: $$y''+y^\alpha=0$$ where $\alpha\neq 1,0$? Tried finding solutions on websites with differential equation calculators but they gave nothing. So does it even exist?",,"['calculus', 'ordinary-differential-equations']"
20,How do you solve $(D^2+1)y=4\cos{x}$?,How do you solve ?,(D^2+1)y=4\cos{x},"I'm stuck on this question: $$(D^2+1)y=4\cos{x}$$ where $D^2$ denotes the differential operator $\frac{d^2}{dx^2}$ As far as I know for trig functions, I'm supposed to assume $y=A\sin{x}+B\cos{x}$ and substitute to get $A$ and $B$. But however, for such, $$(D^2+1)y=0$$ for all values of $A$ and $B$. I don't know what other types of $y$ I should assume. I'm basically clueless here, so any hints would be great.","I'm stuck on this question: $$(D^2+1)y=4\cos{x}$$ where $D^2$ denotes the differential operator $\frac{d^2}{dx^2}$ As far as I know for trig functions, I'm supposed to assume $y=A\sin{x}+B\cos{x}$ and substitute to get $A$ and $B$. But however, for such, $$(D^2+1)y=0$$ for all values of $A$ and $B$. I don't know what other types of $y$ I should assume. I'm basically clueless here, so any hints would be great.",,['ordinary-differential-equations']
21,Bounded solution to general nonautonomous ODE,Bounded solution to general nonautonomous ODE,,"Suppose that for each $t\in\mathbf{R}$ we have an $n\times n$ matrix $A(t)$ and that there exists an $m\geq 0$ in such a way that for each $|t|\geq m$ the matrix $A(t)$ is positive definite. The family of matrices $A$ depends continuously on the parameter $t$. Moreover, the matrix norm of $A(t)$ is bounded by some constant. Now consider the following ODE $$\dot{x}(t)=A(t)x(t).$$ Then from the Cauchy-Lipschitz theorem, it follows that for each $y_0\in\mathbf{R}^n$ there exists a unique solution $x(t)$ of this ODE with $x(0)=y_0$. My question is: can I conclude that $x=0$ is the only bounded solution? For $A$ independent of $t$ this is true, as we can compute the solution exactly and just see that any non-trivial solution is unbounded. For $n=1$, it is also true: for some fixed $t>m$ we can look at any solution $x$ with $x(t)>0$ (if $x(t)<0$ just look at $-x$), then the equation and the positive definiteness implies $x'(t)>0$. Hence $x$ is increasing at that point, which means that it grows to infinity as $t\rightarrow\infty$. Since any non-trivial solution to this ODE is non-zero at some $t>m$ it follows that $x=0$ is the only bounded solution. However, in the general non-autonomous $n\times n$ case I do not see how this works precisely. I do not necessarily need a proof, a reference to a book or paper is fine as well. Update: Thanks to humanStampedist I discovered I at least need to assume that $A(t)\rightarrow A^{\pm}$ as $t\rightarrow\pm\infty$ for some positive definite matrices $A^{\pm}$. However, assuming this, I still do not know the solution, so more help is needed.","Suppose that for each $t\in\mathbf{R}$ we have an $n\times n$ matrix $A(t)$ and that there exists an $m\geq 0$ in such a way that for each $|t|\geq m$ the matrix $A(t)$ is positive definite. The family of matrices $A$ depends continuously on the parameter $t$. Moreover, the matrix norm of $A(t)$ is bounded by some constant. Now consider the following ODE $$\dot{x}(t)=A(t)x(t).$$ Then from the Cauchy-Lipschitz theorem, it follows that for each $y_0\in\mathbf{R}^n$ there exists a unique solution $x(t)$ of this ODE with $x(0)=y_0$. My question is: can I conclude that $x=0$ is the only bounded solution? For $A$ independent of $t$ this is true, as we can compute the solution exactly and just see that any non-trivial solution is unbounded. For $n=1$, it is also true: for some fixed $t>m$ we can look at any solution $x$ with $x(t)>0$ (if $x(t)<0$ just look at $-x$), then the equation and the positive definiteness implies $x'(t)>0$. Hence $x$ is increasing at that point, which means that it grows to infinity as $t\rightarrow\infty$. Since any non-trivial solution to this ODE is non-zero at some $t>m$ it follows that $x=0$ is the only bounded solution. However, in the general non-autonomous $n\times n$ case I do not see how this works precisely. I do not necessarily need a proof, a reference to a book or paper is fine as well. Update: Thanks to humanStampedist I discovered I at least need to assume that $A(t)\rightarrow A^{\pm}$ as $t\rightarrow\pm\infty$ for some positive definite matrices $A^{\pm}$. However, assuming this, I still do not know the solution, so more help is needed.",,['ordinary-differential-equations']
22,Stability of Euler's Method for non-linear ODE,Stability of Euler's Method for non-linear ODE,,"Consider the ODE $$y'(t) = \lambda y(t), \quad y(t_0) =y_0.$$ Euler's method $y_{i+1}=y_i+h\lambda y_i $ is stable (meaning that the solution decays or stays constant as $ i \to \infty$) provided that $ |1 + h\lambda| \leq 1 $. This idea can be extended to systems of $n$ dimensions by placing a similar condition on the maximum coefficient instead of just $\lambda$. This question is about non-linear, $n$ dimensional systems. Under what conditions is Euler's method stable for the system $$\mathbf{y}'(t)=\mathbf{F}(\mathbf{y}(t)), \quad \mathbf{y}(t_0)= \mathbf{y}_0$$? I suspect this will involve the Jacobian of the non-linear term. The only resource I've found is on slide 18 of these lecture slides , but the notation isn't explained (what is $l_{k+1}$?). It was probably introduced in a previous lecture. The integral of the Jacobian that appears here is interesting, it looks like some kind of weighted average between the numerical and the true solution. Where does this come from?","Consider the ODE $$y'(t) = \lambda y(t), \quad y(t_0) =y_0.$$ Euler's method $y_{i+1}=y_i+h\lambda y_i $ is stable (meaning that the solution decays or stays constant as $ i \to \infty$) provided that $ |1 + h\lambda| \leq 1 $. This idea can be extended to systems of $n$ dimensions by placing a similar condition on the maximum coefficient instead of just $\lambda$. This question is about non-linear, $n$ dimensional systems. Under what conditions is Euler's method stable for the system $$\mathbf{y}'(t)=\mathbf{F}(\mathbf{y}(t)), \quad \mathbf{y}(t_0)= \mathbf{y}_0$$? I suspect this will involve the Jacobian of the non-linear term. The only resource I've found is on slide 18 of these lecture slides , but the notation isn't explained (what is $l_{k+1}$?). It was probably introduced in a previous lecture. The integral of the Jacobian that appears here is interesting, it looks like some kind of weighted average between the numerical and the true solution. Where does this come from?",,"['ordinary-differential-equations', 'numerical-methods', 'linearization']"
23,Solve this nonlinear second order ODE: $yy'+xyy''+1=0$ [closed],Solve this nonlinear second order ODE:  [closed],yy'+xyy''+1=0,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Please could you help me  to solve the differential equation  $$yy'+xyy''+1=0$$ with initial conditions $y(0)=1, y'(0)=-1$. Thank you in advance.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Please could you help me  to solve the differential equation  $$yy'+xyy''+1=0$$ with initial conditions $y(0)=1, y'(0)=-1$. Thank you in advance.",,"['ordinary-differential-equations', 'generating-functions']"
24,How can a point that makes the ODE undefined be included into a domain of any solution?,How can a point that makes the ODE undefined be included into a domain of any solution?,,"To illustrate the problem that I'm having, consider the following ODE: $$y' = \frac{y}{1+x}$$ To solve this ODE, my professor had done the followings: Assuming $y(x) \not = 0$, and $x \not = -1$ (actually, he never stated these), then we can arrange the ODE as $$\frac{dy}{y} = \frac{dx}{1+x} \\ \ln|y| = \ln|x+1| + \ln|c| \\ y = c(x+1). \quad \text{(The General solution)}$$ And later, while introducing some initial values to this ODE, he gave the case where $y(-1) = 0$, and claimed that since when $x = -1$, for any values of $c\in \mathbb{R}$, we get different solutions from the ""general solution"", there exists infinitely many solutions satisfying this initial condition. However, (even if we skip how we found $y = c(x+1)$) to verify that $y = c(x+1)$ is indeed a solution (for some $c$) to this ODE, when we plug it in to the ODE, in order to cancel the terms $1+x$ in the denominator and in the numerator, we need to assume that $x \not =-1$, otherwise it would also mean we are calcelling zero's, which is not valid since, in the algebra that we are doing, zero do not have any multiplicative inverse.Therefore, any solution of the form $y = c_0(x+1)$ cannot contain the the point $x = -1$, so how can we say that ""there are infinitely many solution of this ODE satisfying the initial condition $y(-1) = 0$"" ? With my logic, there exists no such solution satisfying this initial condition, so which one of us is right, and why? A Further Question: As we have done it in this particular case, to reach some solutions of a given ODE, we need to do some arrangements to the form of the ODE, such as collection same variables to the same side, but this generally requires some assumptions, such as $y \not= 0$, so in such cases, the result that we get should not be applicable to the cases, for example, in the above case, it there was any other point other than $x = -1$ where $y = 0$, say $y(x_1) = 0$ the solutions that we got from the equation $y = c(x+1)$ should not contain that point $(x_1, 0)$ also, right ?","To illustrate the problem that I'm having, consider the following ODE: $$y' = \frac{y}{1+x}$$ To solve this ODE, my professor had done the followings: Assuming $y(x) \not = 0$, and $x \not = -1$ (actually, he never stated these), then we can arrange the ODE as $$\frac{dy}{y} = \frac{dx}{1+x} \\ \ln|y| = \ln|x+1| + \ln|c| \\ y = c(x+1). \quad \text{(The General solution)}$$ And later, while introducing some initial values to this ODE, he gave the case where $y(-1) = 0$, and claimed that since when $x = -1$, for any values of $c\in \mathbb{R}$, we get different solutions from the ""general solution"", there exists infinitely many solutions satisfying this initial condition. However, (even if we skip how we found $y = c(x+1)$) to verify that $y = c(x+1)$ is indeed a solution (for some $c$) to this ODE, when we plug it in to the ODE, in order to cancel the terms $1+x$ in the denominator and in the numerator, we need to assume that $x \not =-1$, otherwise it would also mean we are calcelling zero's, which is not valid since, in the algebra that we are doing, zero do not have any multiplicative inverse.Therefore, any solution of the form $y = c_0(x+1)$ cannot contain the the point $x = -1$, so how can we say that ""there are infinitely many solution of this ODE satisfying the initial condition $y(-1) = 0$"" ? With my logic, there exists no such solution satisfying this initial condition, so which one of us is right, and why? A Further Question: As we have done it in this particular case, to reach some solutions of a given ODE, we need to do some arrangements to the form of the ODE, such as collection same variables to the same side, but this generally requires some assumptions, such as $y \not= 0$, so in such cases, the result that we get should not be applicable to the cases, for example, in the above case, it there was any other point other than $x = -1$ where $y = 0$, say $y(x_1) = 0$ the solutions that we got from the equation $y = c(x+1)$ should not contain that point $(x_1, 0)$ also, right ?",,"['ordinary-differential-equations', 'singularity', 'fundamental-solution']"
25,Understanding high and low frequency eigenmodes.,Understanding high and low frequency eigenmodes.,,"I'm studying basic iterative methods and about solving PDEs with basic iterative methods. I came across the following text in my lecture notes ""In this section we revisit the convergence of BIMs applied to the discrete Poisson equation and reveal what causes their slow convergence. To this end we consider solving the linear system $$A^hu^h = f^h$$ resulting from finite difference discretisation of a PDE. The eigenmodes can be subdivided into low and high frequency modes. The low frequency modes are slowly varying grid vectors that correspond to the small eigenvalues of $A^h\in\mathbb{R}^{n\times n}$. We more precisely have that in one and two dimensions (of the PDE) the low frequency modes are the eigenvectors $v^{h,[k]}$ for $1\leq k\leq n/2$ and $v^{h,[kl]}$ for $1\leq k,l\leq n/2,$ respectively. The remaining eigenmodes are called the high frequency modes and correspond to oscillatory grid vectors."" I have never heard about eigenmodes or slowly varying grid vectors before, and this text doesn't make a lot of sense to me. Question: What are high and low frequency eigenmodes in this example? What does it mean for a vector to be slowly varying or oscillating, and why do the slowly varying grid vectors correspond to the small eigenvector? What do the superscripts above the eigenvectors in the one or two dimensional case mean? Thanks! Thanks in advance!","I'm studying basic iterative methods and about solving PDEs with basic iterative methods. I came across the following text in my lecture notes ""In this section we revisit the convergence of BIMs applied to the discrete Poisson equation and reveal what causes their slow convergence. To this end we consider solving the linear system $$A^hu^h = f^h$$ resulting from finite difference discretisation of a PDE. The eigenmodes can be subdivided into low and high frequency modes. The low frequency modes are slowly varying grid vectors that correspond to the small eigenvalues of $A^h\in\mathbb{R}^{n\times n}$. We more precisely have that in one and two dimensions (of the PDE) the low frequency modes are the eigenvectors $v^{h,[k]}$ for $1\leq k\leq n/2$ and $v^{h,[kl]}$ for $1\leq k,l\leq n/2,$ respectively. The remaining eigenmodes are called the high frequency modes and correspond to oscillatory grid vectors."" I have never heard about eigenmodes or slowly varying grid vectors before, and this text doesn't make a lot of sense to me. Question: What are high and low frequency eigenmodes in this example? What does it mean for a vector to be slowly varying or oscillating, and why do the slowly varying grid vectors correspond to the small eigenvector? What do the superscripts above the eigenvectors in the one or two dimensional case mean? Thanks! Thanks in advance!",,"['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'eigenvalues-eigenvectors', 'algorithms']"
26,Solving a differential equation with a complex number as a coefficient,Solving a differential equation with a complex number as a coefficient,,"I am trying to solve the following differential equation; \begin{equation} y'' - iy = 0 \end{equation} By following the usual method of solving, I get my characteristic equation $\lambda^{2} - i = 0$, which then gives me the general solution of  \begin{equation} y = cos(\sqrt{i}x) + sin(\sqrt{i}x) \end{equation} I want to know if there is any way I can remove the $i$ term from inside the brackets as to get real solutions. I have tried using an expression for $e^{ix}$ but cannot cancel out the $i$ and the $\sqrt{i}$ I also tried this with a similar ODE, as shown below, \begin{equation} y'' + iy = 0 \end{equation} Where I got a characteristic equation of $\lambda^{2} + i = 0$, giving me a general solution of \begin{equation} y=e^{i\sqrt{i}x} + e^{-i\sqrt{i}x} \end{equation} However, I am still not sure how to cancel this out so that in cos/sin form I have no complex term inside the brackets. Boundary conditions do not affect the problem at this stage - I'm simply interested in removing the complex term from the brackets. I hope this is clear enough - any help would be appreciated.","I am trying to solve the following differential equation; \begin{equation} y'' - iy = 0 \end{equation} By following the usual method of solving, I get my characteristic equation $\lambda^{2} - i = 0$, which then gives me the general solution of  \begin{equation} y = cos(\sqrt{i}x) + sin(\sqrt{i}x) \end{equation} I want to know if there is any way I can remove the $i$ term from inside the brackets as to get real solutions. I have tried using an expression for $e^{ix}$ but cannot cancel out the $i$ and the $\sqrt{i}$ I also tried this with a similar ODE, as shown below, \begin{equation} y'' + iy = 0 \end{equation} Where I got a characteristic equation of $\lambda^{2} + i = 0$, giving me a general solution of \begin{equation} y=e^{i\sqrt{i}x} + e^{-i\sqrt{i}x} \end{equation} However, I am still not sure how to cancel this out so that in cos/sin form I have no complex term inside the brackets. Boundary conditions do not affect the problem at this stage - I'm simply interested in removing the complex term from the brackets. I hope this is clear enough - any help would be appreciated.",,"['ordinary-differential-equations', 'complex-numbers']"
27,"Show that $x_1' = x_2, \space x_2' = -2x_1 + x_2(2-5x_1^2 - 3x_2^2)$ has a bounded solution",Show that  has a bounded solution,"x_1' = x_2, \space x_2' = -2x_1 + x_2(2-5x_1^2 - 3x_2^2)","Consider the dynamical system :   $$x_1' = x_2, \space \space x_2' = -2x_1 + x_2(2-5x_1^2 - 3x_2^2)$$   where $x_1,x_2 \in \mathbb R$. Using the functional $V(x_1,x_2) = \frac{1}{2} x_1^2 + \frac{1}{2} x_2^2$, prove that for any given inital value, the initial value problem has a bounded solution. Attempt : Take the derivative over the solution curves for the given functional, which is : $$\dot{V}(x_1,x_2) = -x_1x_2 + x_2^2(2-5x_1^2-3x_2^2)$$ I will try ""increase"" this expression via inequalities (which is a standard way of handling such problems), so that I can provide a compact form with $V(x_1,x_2)$ in expressions which we know it's positive : $$\dot{V}(x_1,x_2)\leq-x_1x_2 + x_2^2(2-3x_1^2-3x_2^2)\leq\frac{1}{2}x_1^2 + \frac{1}{2}x_2^2 + x_2^2(2-x_1^2 - x_2^2)$$ Now, I have reduced the coefficients inside the parenthesis, since $x_1^2,x_2^2 \geq0$ which means that a smaller coefficient leads to a bigger quality. The inequality derived for the expression at front is a simple application of $(x_1+x_2)^2 \geq 0$. However, I cannot seem how I should continue from now on, since that $x_2^2$ in front of the parenthesis is bugging me and I do not know how to convert it while keeping the inequality in a form that would produce an expression of $V$. Any tips ?","Consider the dynamical system :   $$x_1' = x_2, \space \space x_2' = -2x_1 + x_2(2-5x_1^2 - 3x_2^2)$$   where $x_1,x_2 \in \mathbb R$. Using the functional $V(x_1,x_2) = \frac{1}{2} x_1^2 + \frac{1}{2} x_2^2$, prove that for any given inital value, the initial value problem has a bounded solution. Attempt : Take the derivative over the solution curves for the given functional, which is : $$\dot{V}(x_1,x_2) = -x_1x_2 + x_2^2(2-5x_1^2-3x_2^2)$$ I will try ""increase"" this expression via inequalities (which is a standard way of handling such problems), so that I can provide a compact form with $V(x_1,x_2)$ in expressions which we know it's positive : $$\dot{V}(x_1,x_2)\leq-x_1x_2 + x_2^2(2-3x_1^2-3x_2^2)\leq\frac{1}{2}x_1^2 + \frac{1}{2}x_2^2 + x_2^2(2-x_1^2 - x_2^2)$$ Now, I have reduced the coefficients inside the parenthesis, since $x_1^2,x_2^2 \geq0$ which means that a smaller coefficient leads to a bigger quality. The inequality derived for the expression at front is a simple application of $(x_1+x_2)^2 \geq 0$. However, I cannot seem how I should continue from now on, since that $x_2^2$ in front of the parenthesis is bugging me and I do not know how to convert it while keeping the inequality in a form that would produce an expression of $V$. Any tips ?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
28,"Solving $y''+\dfrac{\varepsilon y'}{y^2} - y' = 0, \, y(-\infty)=1$ and $y(\infty) = \varepsilon$",Solving  and,"y''+\dfrac{\varepsilon y'}{y^2} - y' = 0, \, y(-\infty)=1 y(\infty) = \varepsilon","$y''+\dfrac{\varepsilon y'}{y^2} - y' = 0, \, y(-\infty)=1$ and $y(\infty) = \varepsilon >0$. Above is an ODE that was in an asymptotic analysis exam. A part of the question asked us to solve it exactly on the domain $(-\infty, \infty)$, where I got into troubles. I wrote the equation as: $$\left(\dfrac{\varepsilon}{y}\right)' = y''-y'=(y'-y)'$$ So we have $\dfrac{\varepsilon}{y} - y'+y = C_0$ and then $\dfrac{ydy}{y^2-yC_0+\varepsilon} = dx$, which would give a closed formula for the general solution. But this is where the trouble is: the antiderivative of the LHS will be very different functions depending on the relationships between $\varepsilon$ and $C_0$; therefore, it would seem like we need to determine $C_0$ to begin with. But that is impossible because of the initial conditions not involving any $y'.$ How should I proceed from here?","$y''+\dfrac{\varepsilon y'}{y^2} - y' = 0, \, y(-\infty)=1$ and $y(\infty) = \varepsilon >0$. Above is an ODE that was in an asymptotic analysis exam. A part of the question asked us to solve it exactly on the domain $(-\infty, \infty)$, where I got into troubles. I wrote the equation as: $$\left(\dfrac{\varepsilon}{y}\right)' = y''-y'=(y'-y)'$$ So we have $\dfrac{\varepsilon}{y} - y'+y = C_0$ and then $\dfrac{ydy}{y^2-yC_0+\varepsilon} = dx$, which would give a closed formula for the general solution. But this is where the trouble is: the antiderivative of the LHS will be very different functions depending on the relationships between $\varepsilon$ and $C_0$; therefore, it would seem like we need to determine $C_0$ to begin with. But that is impossible because of the initial conditions not involving any $y'.$ How should I proceed from here?",,"['ordinary-differential-equations', 'asymptotics', 'initial-value-problems']"
29,Solve $z''+z=x$,Solve,z''+z=x,"Considering the ansats $z=ax+b$. If I'm missing one degree in the LHS in the differential equation, shouln't I multiply my ansatz by $x$ and get the ansatz and instead get $ax^2+bx$? If not, when does this apply?","Considering the ansats $z=ax+b$. If I'm missing one degree in the LHS in the differential equation, shouln't I multiply my ansatz by $x$ and get the ansatz and instead get $ax^2+bx$? If not, when does this apply?",,"['calculus', 'ordinary-differential-equations']"
30,"Can I go further for this problem? If $f(x) = f'(x) + \int_0^1 f(x) \,dx$ find $f((a+b)/2)$",Can I go further for this problem? If  find,"f(x) = f'(x) + \int_0^1 f(x) \,dx f((a+b)/2)","Let $f :(0,1) \to \mathbb{R}$ be differentiable for all $x\in(0,1)$ and satifies \begin{equation} f(x) = f'(x) + \int_0^1 f(x) \,dx\tag{1}\end{equation} for all $x\in (0,1)$. If there exist $a,b\in(0,1)$ such that $f(a) = f(b) = (a + b)/2$, determine the value of $f((a + b)/2)$. My Approach Let $\displaystyle \int_0^1 f(x) dx = S$. Since $f$ is differentiable(and hence continue) we have a differential equation $$f'(x) - f(x) = -S$$ whose  homogenous solution is $f_h(x) = ce^x$ for some constant $c$. Since the particular solution is $f_p(x) = S$, it follows that $c =0$. From this we conclude that the solution is  $f(x) = S$ for all $x\in (0,1)$. In other word, $f(x)$ is constant for $x\in (0,1)$ and hence $$f((a + b)/2) =f(a) = f(b) = (a + b)/2.$$ Is this correct? Am I on the right track? I'm not really sure about my answer though. I think we can get further or maybe there is another approach to this problem.","Let $f :(0,1) \to \mathbb{R}$ be differentiable for all $x\in(0,1)$ and satifies \begin{equation} f(x) = f'(x) + \int_0^1 f(x) \,dx\tag{1}\end{equation} for all $x\in (0,1)$. If there exist $a,b\in(0,1)$ such that $f(a) = f(b) = (a + b)/2$, determine the value of $f((a + b)/2)$. My Approach Let $\displaystyle \int_0^1 f(x) dx = S$. Since $f$ is differentiable(and hence continue) we have a differential equation $$f'(x) - f(x) = -S$$ whose  homogenous solution is $f_h(x) = ce^x$ for some constant $c$. Since the particular solution is $f_p(x) = S$, it follows that $c =0$. From this we conclude that the solution is  $f(x) = S$ for all $x\in (0,1)$. In other word, $f(x)$ is constant for $x\in (0,1)$ and hence $$f((a + b)/2) =f(a) = f(b) = (a + b)/2.$$ Is this correct? Am I on the right track? I'm not really sure about my answer though. I think we can get further or maybe there is another approach to this problem.",,"['real-analysis', 'ordinary-differential-equations']"
31,Solution for $\displaystyle f'\left(x\right)=\cos\left(f\left(x\right)\right)$,Solution for,\displaystyle f'\left(x\right)=\cos\left(f\left(x\right)\right),"Let $f$ be defined for $x \in \mathbb{R}$ by $$ f\left(x\right)=2\text{arctan}\left(e^x\right)-\frac{\pi}{2} $$ I've shown that $f$ is odd and satisfies for $x \in \mathbb{R}$ $$f'\left(x\right)=\cos\left(f\left(x\right)\right)$$ To prove it, i've used that $$ \cos\left(f\left(x\right)\right)=2\sin\left(\text{arctan}\left(e^x\right)\right)\cos\left(\text{arctan}\left(e^x\right)\right) $$ And then use that $$ \cos\left(\text{arctan}\left(e^x\right)\right)=\frac{1}{\sqrt{1+e^{2x}}} $$ I've two questions : $\bullet$ Is that the unique solution for $f(0)=0$ ? I've tried to prove it by supposing to different solutions and trying to prove there are infact equals with trigonometric formula but it does not seem to work. $\bullet$ Is there another way ( even wiser or faster ) to prove it ? Thanks for those who take time to answer.","Let $f$ be defined for $x \in \mathbb{R}$ by $$ f\left(x\right)=2\text{arctan}\left(e^x\right)-\frac{\pi}{2} $$ I've shown that $f$ is odd and satisfies for $x \in \mathbb{R}$ $$f'\left(x\right)=\cos\left(f\left(x\right)\right)$$ To prove it, i've used that $$ \cos\left(f\left(x\right)\right)=2\sin\left(\text{arctan}\left(e^x\right)\right)\cos\left(\text{arctan}\left(e^x\right)\right) $$ And then use that $$ \cos\left(\text{arctan}\left(e^x\right)\right)=\frac{1}{\sqrt{1+e^{2x}}} $$ I've two questions : $\bullet$ Is that the unique solution for $f(0)=0$ ? I've tried to prove it by supposing to different solutions and trying to prove there are infact equals with trigonometric formula but it does not seem to work. $\bullet$ Is there another way ( even wiser or faster ) to prove it ? Thanks for those who take time to answer.",,"['ordinary-differential-equations', 'functions', 'trigonometry']"
32,Linear Dependency of two functions,Linear Dependency of two functions,,"How the functions $f(x) = x^2$ and $g(x) = x|x|$  are linearly independent for $-\infty \lt x \lt \infty$? My Try:- I first divided the interval in two parts $x \le 0$ and $ x \gt 0$. Then I calculated the Wronskian of the two functions separately which came out to be zero in both the cases. Then why these functions are linearly independent? Is is  so because we are not able to find any nonzero constants c and k for which $cf(x)  + kg(x) \ne 0 $. And if answer is yes, then why do they contradict with Wronskian?","How the functions $f(x) = x^2$ and $g(x) = x|x|$  are linearly independent for $-\infty \lt x \lt \infty$? My Try:- I first divided the interval in two parts $x \le 0$ and $ x \gt 0$. Then I calculated the Wronskian of the two functions separately which came out to be zero in both the cases. Then why these functions are linearly independent? Is is  so because we are not able to find any nonzero constants c and k for which $cf(x)  + kg(x) \ne 0 $. And if answer is yes, then why do they contradict with Wronskian?",,"['calculus', 'ordinary-differential-equations']"
33,solution of the integral equation,solution of the integral equation,,"let $y(x)$ be the solution of the integral equation $$ y(x) = x-\int _0 ^x xt^2y(t)dt, x>0$$ Then value of $y(\sqrt 2)?$","let $y(x)$ be the solution of the integral equation $$ y(x) = x-\int _0 ^x xt^2y(t)dt, x>0$$ Then value of $y(\sqrt 2)?$",,"['ordinary-differential-equations', 'integral-equations']"
34,Help solving variable separable ODE: $y' = \frac{1}{2} a y^2 + b y - 1$ with $y(0)=0$,Help solving variable separable ODE:  with,y' = \frac{1}{2} a y^2 + b y - 1 y(0)=0,"I am studying for an exam about ODEs and I am struggling with one of the past exam questions. The past exam shows one exercise which asks us to solve: $$y' = \frac{1}{2} a y^2 + b y - 1$$with $y(0)=0$ The solution is given as $$y(x) = \frac{2 \left( e^{\Gamma x} - 1 \right)}{(b + \Gamma)(e^{\Gamma x} - 1) + 2\Gamma},$$ with $\Gamma = \sqrt{b^2 + 2a}$ I am really getting stuck at this exercise and would love to have someone show me how this solution is derived. One thing I did find out is that this ODE is variable separable. That is, $$y' = g(x)h(y) = (1) \cdot (\frac{1}{2} ay^2 + by - 1),$$ and therefore the solution would result from solving $$\int \frac{1}{\frac{1}{2} ay^2 + by - 1} dy = \int dx + C,$$ where C is clearly zero because $y(0) = 0$. I am now getting stuck at solving the left integral. Could anyone please show me the steps? UPDATE So I came quite far with @LutzL solution, however my answers seems to slightly deviate from the solution given above. These are the steps I performed (continuing from @LutzL's answer): You complete the square $\frac12ay^2+by-1=\frac12a(y+\frac ba)^2-1-\frac{b^2}{2a}$ and use this to inspire the change of coordinates $u=ay+b$ leading to $$ \int \frac{dy}{\frac12ay^2+by-1}=\int\frac{2\,du}{u^2-2a-b^2} $$ and for that your integral tables should give a form using the inverse hyperbolic tangent. Or you perform a partial fraction decomposition for $$ \frac{2Γ}{u^2-Γ^2}=-\frac{1}{u+Γ}+\frac{1}{u-Γ} $$ and find the corresponding logarithmic anti-derivatives, $$ \ln|u-Γ|-\ln|u+Γ|=Γx+c,\\ \frac{u-Γ}{u+Γ}=Ce^{Γx},\ C=\pm e^c $$ which you now can easily solve for $u$ and then $y$. Given that $y(0) = 0$ we have $u(y(0)) = u(0) = b$ and therefore the final equation becomes $$\frac{u(0)-Γ}{u(0)+Γ}= \frac{b-Γ}{b+Γ}=Ce^{Γ\cdot0} = Ce^{Γ \cdot 0} = C$$ Now by first isolating $u$ I get $$u - \Gamma = u C e^{\Gamma x} + \Gamma C e^{\Gamma x} \Rightarrow \\ u \left( 1 - C e^{ \Gamma x} \right) = \Gamma \left( 1 + C e^{\Gamma x} \right) \Rightarrow \\ u = \frac{\Gamma \left( 1 + C e^{\Gamma x} \right)}{\left( 1 - C e^{ \Gamma x} \right)}$$ Now substituting u and C gives $$ay + b= \frac{\Gamma \left( 1 + \frac{b-Γ}{b+Γ} e^{\Gamma x} \right)}{\left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)} \Rightarrow \\ y = \frac{\Gamma \left( 1 + \frac{b-Γ}{b+Γ} e^{\Gamma x} \right) - b \left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)}{a \left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)}$$ Now using the fact that $\Gamma = \sqrt{b^2 + 2a} \Rightarrow a = \frac{(\Gamma + b)(\Gamma - b)}{2}$ we get that $$y = \frac{2 \left( \Gamma \left( 1 + \frac{b-\Gamma }{b+\Gamma } e^{\Gamma x} \right) - b \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right) \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)}  \\ = \frac{2 \left( (\Gamma - b) + (\Gamma + b) \frac{b-\Gamma }{b+ \Gamma } e^{\Gamma x} \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)} \\ = \frac{2 \left( (\Gamma - b) - (\Gamma + b) \frac{\Gamma - b }{b+ \Gamma } e^{\Gamma x} \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)}$$ Now cancelling the terms $(b + \Gamma)$ and $(b - \Gamma)$ wherever possible and multiplying denominator and nominator by -1 gives $$y = \frac{2 \left(  e^{\Gamma x} - 1 \right)}{(\Gamma + b) \left( \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} - 1 \right)} $$ So clearly, I got the nominator right, but I can not seem to get the denominator to equal $(b + \Gamma)(e^{\Gamma x} - 1) + 2\Gamma$. Can someone rescue me and show me what I did wrong? Maybe it helps if I say that x is always positive?","I am studying for an exam about ODEs and I am struggling with one of the past exam questions. The past exam shows one exercise which asks us to solve: $$y' = \frac{1}{2} a y^2 + b y - 1$$with $y(0)=0$ The solution is given as $$y(x) = \frac{2 \left( e^{\Gamma x} - 1 \right)}{(b + \Gamma)(e^{\Gamma x} - 1) + 2\Gamma},$$ with $\Gamma = \sqrt{b^2 + 2a}$ I am really getting stuck at this exercise and would love to have someone show me how this solution is derived. One thing I did find out is that this ODE is variable separable. That is, $$y' = g(x)h(y) = (1) \cdot (\frac{1}{2} ay^2 + by - 1),$$ and therefore the solution would result from solving $$\int \frac{1}{\frac{1}{2} ay^2 + by - 1} dy = \int dx + C,$$ where C is clearly zero because $y(0) = 0$. I am now getting stuck at solving the left integral. Could anyone please show me the steps? UPDATE So I came quite far with @LutzL solution, however my answers seems to slightly deviate from the solution given above. These are the steps I performed (continuing from @LutzL's answer): You complete the square $\frac12ay^2+by-1=\frac12a(y+\frac ba)^2-1-\frac{b^2}{2a}$ and use this to inspire the change of coordinates $u=ay+b$ leading to $$ \int \frac{dy}{\frac12ay^2+by-1}=\int\frac{2\,du}{u^2-2a-b^2} $$ and for that your integral tables should give a form using the inverse hyperbolic tangent. Or you perform a partial fraction decomposition for $$ \frac{2Γ}{u^2-Γ^2}=-\frac{1}{u+Γ}+\frac{1}{u-Γ} $$ and find the corresponding logarithmic anti-derivatives, $$ \ln|u-Γ|-\ln|u+Γ|=Γx+c,\\ \frac{u-Γ}{u+Γ}=Ce^{Γx},\ C=\pm e^c $$ which you now can easily solve for $u$ and then $y$. Given that $y(0) = 0$ we have $u(y(0)) = u(0) = b$ and therefore the final equation becomes $$\frac{u(0)-Γ}{u(0)+Γ}= \frac{b-Γ}{b+Γ}=Ce^{Γ\cdot0} = Ce^{Γ \cdot 0} = C$$ Now by first isolating $u$ I get $$u - \Gamma = u C e^{\Gamma x} + \Gamma C e^{\Gamma x} \Rightarrow \\ u \left( 1 - C e^{ \Gamma x} \right) = \Gamma \left( 1 + C e^{\Gamma x} \right) \Rightarrow \\ u = \frac{\Gamma \left( 1 + C e^{\Gamma x} \right)}{\left( 1 - C e^{ \Gamma x} \right)}$$ Now substituting u and C gives $$ay + b= \frac{\Gamma \left( 1 + \frac{b-Γ}{b+Γ} e^{\Gamma x} \right)}{\left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)} \Rightarrow \\ y = \frac{\Gamma \left( 1 + \frac{b-Γ}{b+Γ} e^{\Gamma x} \right) - b \left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)}{a \left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)}$$ Now using the fact that $\Gamma = \sqrt{b^2 + 2a} \Rightarrow a = \frac{(\Gamma + b)(\Gamma - b)}{2}$ we get that $$y = \frac{2 \left( \Gamma \left( 1 + \frac{b-\Gamma }{b+\Gamma } e^{\Gamma x} \right) - b \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right) \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)}  \\ = \frac{2 \left( (\Gamma - b) + (\Gamma + b) \frac{b-\Gamma }{b+ \Gamma } e^{\Gamma x} \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)} \\ = \frac{2 \left( (\Gamma - b) - (\Gamma + b) \frac{\Gamma - b }{b+ \Gamma } e^{\Gamma x} \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)}$$ Now cancelling the terms $(b + \Gamma)$ and $(b - \Gamma)$ wherever possible and multiplying denominator and nominator by -1 gives $$y = \frac{2 \left(  e^{\Gamma x} - 1 \right)}{(\Gamma + b) \left( \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} - 1 \right)} $$ So clearly, I got the nominator right, but I can not seem to get the denominator to equal $(b + \Gamma)(e^{\Gamma x} - 1) + 2\Gamma$. Can someone rescue me and show me what I did wrong? Maybe it helps if I say that x is always positive?",,"['integration', 'ordinary-differential-equations']"
35,how to solve the differential equation : $\frac{d^2{y}}{dx^{2}} = \frac{1}{y^3}$,how to solve the differential equation :,\frac{d^2{y}}{dx^{2}} = \frac{1}{y^3},"As I am a beginner in Differential equations, I have no idea how to solve the following differential equation : $$\frac{d^2{y}}{dx^{2}} = \frac{1}{y^3}$$ Thanks in advance for helping solving the DE.","As I am a beginner in Differential equations, I have no idea how to solve the following differential equation : $$\frac{d^2{y}}{dx^{2}} = \frac{1}{y^3}$$ Thanks in advance for helping solving the DE.",,[]
36,Why is the exponential function used function used when solving linear 2nd order homogeneous differential equations?,Why is the exponential function used function used when solving linear 2nd order homogeneous differential equations?,,"In my textbook the introduction to solving linear 2nd order homogeneous DE's begins with a general form: $ay''+by'+ cy =0$ Then they say: ""a solution must have the property that its second derivative is expressible as a linear combination of its first and zeroth derivatives. This suggests a solution of the form:"" $y=e^{rt}$ By this doesn't seem intuitive to me. How is the second derivative of the exponential function expressible as a linear combination of its first and zeroth derivative? Would that not mean the following: $r^2e^{rt}=re^{rt}+e^{rt}$ Which isn't true. So what could they mean by that?","In my textbook the introduction to solving linear 2nd order homogeneous DE's begins with a general form: $ay''+by'+ cy =0$ Then they say: ""a solution must have the property that its second derivative is expressible as a linear combination of its first and zeroth derivatives. This suggests a solution of the form:"" $y=e^{rt}$ By this doesn't seem intuitive to me. How is the second derivative of the exponential function expressible as a linear combination of its first and zeroth derivative? Would that not mean the following: $r^2e^{rt}=re^{rt}+e^{rt}$ Which isn't true. So what could they mean by that?",,['ordinary-differential-equations']
37,Urns: solving a differential system,Urns: solving a differential system,,"I'm working on an urn model where two urns, say A and B, are interacting in that at each step, a ball is drawn from each of them and then replaced adding a ball of the color drawn from the other urn. So if my draw vector at time $t$ is $X_t=(X_t^a,X_t^b)=(W,B)$, I'll add a black ball to urn A and a white to urn B. Now, if $Z$ represents the proportion of black balls, I end up having the following dynamics: \begin{cases} \mathbb{E}\left[\Delta Z_a^t|\mathcal{F}_t\right]=\cfrac{1}{S_a^t+1}\left[Z_b^t-\cfrac{1}{S_a^t}\right]\\ \mathbb{E}\left[\Delta Z_b^t|\mathcal{F}_t\right] =\cfrac{1}{S_b^t+1}\left[Z_a^t-\cfrac{1}{S_b^t}\right] \end{cases} with S being the total number of balls in the considered urn. Quite naively, I tried to solve the following differential system: $$ \begin{cases} x'(t)=\frac{1}{t+1}\left[y(t)-\frac{1}{t}\right] \\ y'(t)=\frac{1}{t+1}\left[x(t)-\frac{1}{t}\right] \end{cases} $$ assuming that each urn is randomly initialized with one ball. The problem is: if I solve this system I end up having functions that are not probabilities at all. I'm not proficient in stochastic calculus so I don't know whether my method is good but I need to add a constraint to my system or if I should solve SDEs rather than ODEs. Do you know how I can get around this? EDIT: actually a mistake was hidden in the computation of the dynamics. The right system is: $$ \begin{cases} \mathbb{E}\left[\Delta Z_a^t|\mathcal{F}_t\right]=\cfrac{1}{S_a^t+1}\left[Z_b^t-Z_a^t\right]\\ \mathbb{E}\left[\Delta Z_b^t|\mathcal{F}_t\right] =\cfrac{1}{S_b^t+1}\left[Z_a^t-Z_b^t\right] \end{cases} $$ So the ordinary differential system associated is $$ \begin{cases} x'(t)=\frac{1}{t+1}\left[y(t)-x(t)\right] \\ y'(t)=\frac{1}{t+1}\left[x(t)-y(t)\right] \end{cases} $$ Which makes much more sense but seems tougher to solve.","I'm working on an urn model where two urns, say A and B, are interacting in that at each step, a ball is drawn from each of them and then replaced adding a ball of the color drawn from the other urn. So if my draw vector at time $t$ is $X_t=(X_t^a,X_t^b)=(W,B)$, I'll add a black ball to urn A and a white to urn B. Now, if $Z$ represents the proportion of black balls, I end up having the following dynamics: \begin{cases} \mathbb{E}\left[\Delta Z_a^t|\mathcal{F}_t\right]=\cfrac{1}{S_a^t+1}\left[Z_b^t-\cfrac{1}{S_a^t}\right]\\ \mathbb{E}\left[\Delta Z_b^t|\mathcal{F}_t\right] =\cfrac{1}{S_b^t+1}\left[Z_a^t-\cfrac{1}{S_b^t}\right] \end{cases} with S being the total number of balls in the considered urn. Quite naively, I tried to solve the following differential system: $$ \begin{cases} x'(t)=\frac{1}{t+1}\left[y(t)-\frac{1}{t}\right] \\ y'(t)=\frac{1}{t+1}\left[x(t)-\frac{1}{t}\right] \end{cases} $$ assuming that each urn is randomly initialized with one ball. The problem is: if I solve this system I end up having functions that are not probabilities at all. I'm not proficient in stochastic calculus so I don't know whether my method is good but I need to add a constraint to my system or if I should solve SDEs rather than ODEs. Do you know how I can get around this? EDIT: actually a mistake was hidden in the computation of the dynamics. The right system is: $$ \begin{cases} \mathbb{E}\left[\Delta Z_a^t|\mathcal{F}_t\right]=\cfrac{1}{S_a^t+1}\left[Z_b^t-Z_a^t\right]\\ \mathbb{E}\left[\Delta Z_b^t|\mathcal{F}_t\right] =\cfrac{1}{S_b^t+1}\left[Z_a^t-Z_b^t\right] \end{cases} $$ So the ordinary differential system associated is $$ \begin{cases} x'(t)=\frac{1}{t+1}\left[y(t)-x(t)\right] \\ y'(t)=\frac{1}{t+1}\left[x(t)-y(t)\right] \end{cases} $$ Which makes much more sense but seems tougher to solve.",,"['probability', 'ordinary-differential-equations', 'recurrence-relations', 'markov-chains', 'polya-urn-model']"
38,Deriving the equation of motion of a pendulum from energy conservation,Deriving the equation of motion of a pendulum from energy conservation,,"The kinetic energy of a simple pendulum is: $$K=\frac{1}{2}mL^2(\frac{d\theta}{dt})^2$$ The potential energy of the pendulum is: $$U=mgL(1-cos\theta)$$ The total energy of the pendulum is therefore: $$E=K+U=K=\frac{1}{2}mL^2(\frac{d\theta}{dt})^2+mgL(1-cos\theta)$$ The total energy of the system is constant, therefore: $$\frac{dE}{dt}=0$$ I am given to understand that taking the derivative of the total energy with respect to $t$ should allow me to rearrage for the equation of motion of a pendulum, $$\frac{d^2\theta}{dt^2}+\frac{g}{L}sin\theta=0$$ but I don't see how.  Taking the derivative of the third equation returns $0$, which isn't very useful.  Am I missing something?","The kinetic energy of a simple pendulum is: $$K=\frac{1}{2}mL^2(\frac{d\theta}{dt})^2$$ The potential energy of the pendulum is: $$U=mgL(1-cos\theta)$$ The total energy of the pendulum is therefore: $$E=K+U=K=\frac{1}{2}mL^2(\frac{d\theta}{dt})^2+mgL(1-cos\theta)$$ The total energy of the system is constant, therefore: $$\frac{dE}{dt}=0$$ I am given to understand that taking the derivative of the total energy with respect to $t$ should allow me to rearrage for the equation of motion of a pendulum, $$\frac{d^2\theta}{dt^2}+\frac{g}{L}sin\theta=0$$ but I don't see how.  Taking the derivative of the third equation returns $0$, which isn't very useful.  Am I missing something?",,"['calculus', 'ordinary-differential-equations']"
39,Solving nonhomogenous wave equation using Fourier transform,Solving nonhomogenous wave equation using Fourier transform,,"Consider this nonhomogenous wave equation: $$ \begin{cases}u_{tt}-u_{xx}=\sin(x)\\u(x,0)=\sin(x) \\ u_{t}(x,0)=0\end{cases}$$ Applying d'Alembert's formula and Duhamel's principle, one can easy show that $u(x,t)=\sin(x)$. My problem with this equation is that I must solve it using Fourier transform (as defined here ), because applying FT on equation yields $$\hat u_{tt}(k,t)+4\pi^{2}k^2\hat u(k,t)= (\delta_{1/2\pi}-\delta_{-1/2\pi})/2$$ How to solve last equation? I've got problem finding particular solution.","Consider this nonhomogenous wave equation: $$ \begin{cases}u_{tt}-u_{xx}=\sin(x)\\u(x,0)=\sin(x) \\ u_{t}(x,0)=0\end{cases}$$ Applying d'Alembert's formula and Duhamel's principle, one can easy show that $u(x,t)=\sin(x)$. My problem with this equation is that I must solve it using Fourier transform (as defined here ), because applying FT on equation yields $$\hat u_{tt}(k,t)+4\pi^{2}k^2\hat u(k,t)= (\delta_{1/2\pi}-\delta_{-1/2\pi})/2$$ How to solve last equation? I've got problem finding particular solution.",,"['ordinary-differential-equations', 'partial-differential-equations', 'distribution-theory', 'fourier-transform', 'wave-equation']"
40,Relationship between the Picard-Fuchs differential equation and the hypergeometric differential equation,Relationship between the Picard-Fuchs differential equation and the hypergeometric differential equation,,"Consider the Picard-Fuchs differential equation: $$\frac{d^2 \Omega}{dJ^2} + \frac{1}{J} \frac{d\Omega}{dJ} + \frac{31J - 4}{144J^2 (1 - J)^2} \Omega = 0.$$ The author of this article claims that the solutions to this equation are those to the hypergeometric differential equation $$J(1 - J) \frac{d^2 \Omega}{dJ^2} + [c - (a + b + 1)J] \frac{d\Omega}{dJ} - ab\Omega = 0$$ with $a = 11/12$, $b = 11/12$, $c = 4/3$, multiplied by $J^{1/6} (1 - J)^{3/4}$. My question is: how does one know that? How can one look the Picard-Fuchs differential equation above and know that its solutions are related to the solutions of the hypergeometric differential equation? Is it because the Picard-Fuchs differential equation is related to Riemann's differential equation?","Consider the Picard-Fuchs differential equation: $$\frac{d^2 \Omega}{dJ^2} + \frac{1}{J} \frac{d\Omega}{dJ} + \frac{31J - 4}{144J^2 (1 - J)^2} \Omega = 0.$$ The author of this article claims that the solutions to this equation are those to the hypergeometric differential equation $$J(1 - J) \frac{d^2 \Omega}{dJ^2} + [c - (a + b + 1)J] \frac{d\Omega}{dJ} - ab\Omega = 0$$ with $a = 11/12$, $b = 11/12$, $c = 4/3$, multiplied by $J^{1/6} (1 - J)^{3/4}$. My question is: how does one know that? How can one look the Picard-Fuchs differential equation above and know that its solutions are related to the solutions of the hypergeometric differential equation? Is it because the Picard-Fuchs differential equation is related to Riemann's differential equation?",,"['ordinary-differential-equations', 'elliptic-curves', 'modular-forms', 'hypergeometric-function', 'modular-function']"
41,Modeling population growth with variable rate in a differential equation,Modeling population growth with variable rate in a differential equation,,If you consider the classic differential equation used in textbooks to model animal population growth and forget adding in the carrying capacity terms: $dP/dt$ = rP It would seem that the above DE would benefit from some variable rate r instead of some fixed rate r in order to achieve a more accurate model. I tried googling around for this and I'm guessing I simply wasn't using the best keywords. I'm wondering if someone could give an example of a DE where a variable rate r is used and how one usually solves such an equation. Thanks,If you consider the classic differential equation used in textbooks to model animal population growth and forget adding in the carrying capacity terms: $dP/dt$ = rP It would seem that the above DE would benefit from some variable rate r instead of some fixed rate r in order to achieve a more accurate model. I tried googling around for this and I'm guessing I simply wasn't using the best keywords. I'm wondering if someone could give an example of a DE where a variable rate r is used and how one usually solves such an equation. Thanks,,['ordinary-differential-equations']
42,A partial differential equation with trig functions,A partial differential equation with trig functions,,"Let $\xi:\mathbb{R}^3\rightarrow \mathbb{R}$ be a function $\xi(x,y,z)$. I have the following PDE: $$ \cos(\xi)\partial_y\xi = \sin(\xi)\partial_x\xi $$ which is equivalent to $\tan(\xi)=\xi_y/\xi_x$. Clearly, $\xi_x = \xi_y = 0$ (where $\partial_i\xi=\xi_i$) is a solution, which would imply $\xi(x,y,z) = \eta(z)$ is only a function of $z$. But are there any other solutions? If not, how to prove it? My thoughts so far: obviously,  \begin{align} \xi_x &= \cos(\xi)\\ \xi_y &= \sin(\xi) \end{align}  together describe a solution. We can solve these separately to get: $$ \xi = 2\arctan(\exp(y+f_1(x,z))\;\;\;\&\;\;\;  \xi=2\arctan(\tanh([1/2][x+f_2(y,z)])) $$ respectively.  This implies that $$ \exp(y+f_1(x,z)) = \tanh([x+f_2(y,z)]/2) $$ Not sure if this line of thinking is useful though.","Let $\xi:\mathbb{R}^3\rightarrow \mathbb{R}$ be a function $\xi(x,y,z)$. I have the following PDE: $$ \cos(\xi)\partial_y\xi = \sin(\xi)\partial_x\xi $$ which is equivalent to $\tan(\xi)=\xi_y/\xi_x$. Clearly, $\xi_x = \xi_y = 0$ (where $\partial_i\xi=\xi_i$) is a solution, which would imply $\xi(x,y,z) = \eta(z)$ is only a function of $z$. But are there any other solutions? If not, how to prove it? My thoughts so far: obviously,  \begin{align} \xi_x &= \cos(\xi)\\ \xi_y &= \sin(\xi) \end{align}  together describe a solution. We can solve these separately to get: $$ \xi = 2\arctan(\exp(y+f_1(x,z))\;\;\;\&\;\;\;  \xi=2\arctan(\tanh([1/2][x+f_2(y,z)])) $$ respectively.  This implies that $$ \exp(y+f_1(x,z)) = \tanh([x+f_2(y,z)]/2) $$ Not sure if this line of thinking is useful though.",,"['ordinary-differential-equations', 'trigonometry', 'partial-differential-equations', 'partial-derivative']"
43,Solve the integral equation of convolution (Abel),Solve the integral equation of convolution (Abel),,"Solve the integral equation, $$I = \int_{0}^{t} \frac{f(\tau)}{\sqrt{t - \tau}}d\tau = \sqrt{2g}T$$ where $T, g$ are constants. Find $f(t)$ I see that $$I = f(t) * g(t)$$ where $f(t)$ we need to find and $g(t) = \frac{1}{\sqrt{t}}$ Using the convolution theorem, we see that: $$\sqrt{2g}T = \mathcal{L}^{-1}\{ \mathcal{L}(f) \mathcal{L}(g) \} $$ Let $F(s) = \mathcal{L}(f)$ and we know $\mathcal{L}(g) = \frac{\sqrt{\pi}}{2s^{3/2}}$ therefore, $$\sqrt{2g}T = \frac{\sqrt{\pi}}{2}\mathcal{L}^{-1} \{\frac{F(s)}{s^{3/2}}\}$$ Now, I got stuck. Can someone provide some help?","Solve the integral equation, $$I = \int_{0}^{t} \frac{f(\tau)}{\sqrt{t - \tau}}d\tau = \sqrt{2g}T$$ where $T, g$ are constants. Find $f(t)$ I see that $$I = f(t) * g(t)$$ where $f(t)$ we need to find and $g(t) = \frac{1}{\sqrt{t}}$ Using the convolution theorem, we see that: $$\sqrt{2g}T = \mathcal{L}^{-1}\{ \mathcal{L}(f) \mathcal{L}(g) \} $$ Let $F(s) = \mathcal{L}(f)$ and we know $\mathcal{L}(g) = \frac{\sqrt{\pi}}{2s^{3/2}}$ therefore, $$\sqrt{2g}T = \frac{\sqrt{\pi}}{2}\mathcal{L}^{-1} \{\frac{F(s)}{s^{3/2}}\}$$ Now, I got stuck. Can someone provide some help?",,"['calculus', 'ordinary-differential-equations', 'laplace-transform']"
44,"for the cauchy problem , determine unique solution ,or no solution , or infinitely many solution","for the cauchy problem , determine unique solution ,or no solution , or infinitely many solution",,"for the cauchy problem , determine unique solution ,or no solution , or infinitely many solution $u_x-6u_y=y$ with the date $u(x,y)=e^x$  on the line $y=-6x+2$ My attemp t: given $u_x-6u_y=y$ then $\frac{dx}{1}=\frac{dy}{-6}=\frac{du}{u}$ there fore the general solution  $\phi(6x+y, y^2+12u)=0$ and hence implicit  equation $y^2+12u=F(6x+y)$ but i cant go for further","for the cauchy problem , determine unique solution ,or no solution , or infinitely many solution $u_x-6u_y=y$ with the date $u(x,y)=e^x$  on the line $y=-6x+2$ My attemp t: given $u_x-6u_y=y$ then $\frac{dx}{1}=\frac{dy}{-6}=\frac{du}{u}$ there fore the general solution  $\phi(6x+y, y^2+12u)=0$ and hence implicit  equation $y^2+12u=F(6x+y)$ but i cant go for further",,"['ordinary-differential-equations', 'partial-differential-equations', 'cauchy-problem']"
45,Hermite polynomials for negative integers,Hermite polynomials for negative integers,,"I have an equation which is similar to Hermite's differential equation $y''(x)+xy'(x) +\lambda y(x)=0$ with decaying boundary conditions at infinity $y\rightarrow 0$ as $x\rightarrow\pm\infty$. I believe the eigenvalues take the form $\lambda_n =-n$ where $n=0,1,2,...$ from the power series solution. The eigenfunction which satisfies the boundary condition is $\phi_n(x) = A_n e^{-y^2/2}H_{\lambda_n-1}\left(\frac{y}{\sqrt 2}\right)$. All of which are Hermite polynomials for negative integers. The recurrence relation $2 n H_{n-1}(x) = 2x H_n(x)-H_{n+1}(x)$ is no helpful since it gives indeterminate values. But I have seen them that these polynomial of non-negative orders are expressed in terms of error function, but I could not find any reference to this. Is there any way to derive them in terms of error function? It would be helpful if someone clarifies about the orthogonality condition for negative integers. Is it the same as Hermite's polynomials of positive integers?","I have an equation which is similar to Hermite's differential equation $y''(x)+xy'(x) +\lambda y(x)=0$ with decaying boundary conditions at infinity $y\rightarrow 0$ as $x\rightarrow\pm\infty$. I believe the eigenvalues take the form $\lambda_n =-n$ where $n=0,1,2,...$ from the power series solution. The eigenfunction which satisfies the boundary condition is $\phi_n(x) = A_n e^{-y^2/2}H_{\lambda_n-1}\left(\frac{y}{\sqrt 2}\right)$. All of which are Hermite polynomials for negative integers. The recurrence relation $2 n H_{n-1}(x) = 2x H_n(x)-H_{n+1}(x)$ is no helpful since it gives indeterminate values. But I have seen them that these polynomial of non-negative orders are expressed in terms of error function, but I could not find any reference to this. Is there any way to derive them in terms of error function? It would be helpful if someone clarifies about the orthogonality condition for negative integers. Is it the same as Hermite's polynomials of positive integers?",,"['ordinary-differential-equations', 'orthogonal-polynomials', 'eigenfunctions', 'error-function', 'hermite-polynomials']"
46,Calculate the curve that is orthogonal to the given curve?,Calculate the curve that is orthogonal to the given curve?,,Given the equation $y = Cy^{2} - 3x^{4}$ find a curve orthogonal to the given curve The steps I did so far are: Isolating C: $C = (y + 3x^{4})/y^{2}$ Using symbolab's implicit differentiation calculator: $y' = -(12x^{3}y)/(-6x^{4}-y)$ Taking the negative reciprocal: $y' = (-6x^{4} -y)/(12x^{3}y)$ Simplifying the right hand side: $y'= -x/(2y) - 1/(12x^{3})$ At this point I'm completely confused as the y is in the denominator and y' is already isolated so separation of variables and Bernoulli's does not seem to work. Is there another way to do this equation or did I just make a simple mistake? Thanks in advance,Given the equation $y = Cy^{2} - 3x^{4}$ find a curve orthogonal to the given curve The steps I did so far are: Isolating C: $C = (y + 3x^{4})/y^{2}$ Using symbolab's implicit differentiation calculator: $y' = -(12x^{3}y)/(-6x^{4}-y)$ Taking the negative reciprocal: $y' = (-6x^{4} -y)/(12x^{3}y)$ Simplifying the right hand side: $y'= -x/(2y) - 1/(12x^{3})$ At this point I'm completely confused as the y is in the denominator and y' is already isolated so separation of variables and Bernoulli's does not seem to work. Is there another way to do this equation or did I just make a simple mistake? Thanks in advance,,"['calculus', 'ordinary-differential-equations']"
47,Proving a set is positive invariant for a dynamical system,Proving a set is positive invariant for a dynamical system,,"I have the following dynamical system: $$ \begin{align} \dot{x}&=-x-2y^2, \\ \dot{y}&=-x^2y-y^3. \end{align} $$ My task is to show that, for the dynamical system, the set $$S=\left\{ (x,y) \in \mathbb{R}^2:x \leq0 \right\}$$ is positive invariant. My first thought is to use a Liapunov function defined by $$L:S \to \mathbb{R}, \: \: L(x,y)=-x,$$ which is positive definite. However, calculating $\dot{L}$ gives $$\dot{L}=L_x\dot{x}+L_y\dot{y}=-\left(-x-2y^2\right)=x+2y^2,$$ from which I cannot seem to deduce anything. Any help would be great!","I have the following dynamical system: $$ \begin{align} \dot{x}&=-x-2y^2, \\ \dot{y}&=-x^2y-y^3. \end{align} $$ My task is to show that, for the dynamical system, the set $$S=\left\{ (x,y) \in \mathbb{R}^2:x \leq0 \right\}$$ is positive invariant. My first thought is to use a Liapunov function defined by $$L:S \to \mathbb{R}, \: \: L(x,y)=-x,$$ which is positive definite. However, calculating $\dot{L}$ gives $$\dot{L}=L_x\dot{x}+L_y\dot{y}=-\left(-x-2y^2\right)=x+2y^2,$$ from which I cannot seem to deduce anything. Any help would be great!",,"['ordinary-differential-equations', 'dynamical-systems', 'set-invariance']"
48,very hard differential equations,very hard differential equations,,"Hi I would need help with solving the two following differential equations: $\dot{N} =\left(a\frac{\dot{x}(t)}{x(t)} -  b\frac{\dot{y}(t)}{y(t)} - c \frac{\dot{z}(t)}{z(t)} \right) \alpha N$ $\dot{N} =\left(a\frac{\dot{x}(t)}{x(t)} -  b\frac{\dot{y}(t)}{y(t)} - c \frac{\dot{z}(t)}{z(t)} \right) \alpha(t)N$ I also know that $N = (px(t) - ry(t) - sz(t))\frac{1}{\alpha+\kappa} $ and $N = (px(t) - ry(t) - sz(t))\frac{1}{\alpha(t)+\kappa}$ respectively - although I am not sure if this information is crucial or not. The way I was thinking to approach this problem is that I know there is a class of separable differential equations: $\frac{dx}{dt}= F(t)g(x)$ And I know how to solve these, but I have no idea if 1. or 2. can be expressed in such form. I think that this should be possible since everything in brackets is a function of t, but I don't think that it is rigorous to just say that $F(t)=\left(a\frac{\dot{x}(t)}{x(t)} -  b\frac{\dot{y}(t)}{y(t)} - c \frac{\dot{z}(t)}{z(t)} \right)$. But I don't know if then the rule for separable differential equation still applies as this would make F a compound function. I also tried to work with inverse functions $t(x)=x^{-1}(t)$, $t(y)=y^{-1}(t)$ and $t(z)=z^{-1}(t)$ and their derivatives, but I feel that just confused me even more. I have never encountered a complex problem such as this so any help would be much appreciated.","Hi I would need help with solving the two following differential equations: $\dot{N} =\left(a\frac{\dot{x}(t)}{x(t)} -  b\frac{\dot{y}(t)}{y(t)} - c \frac{\dot{z}(t)}{z(t)} \right) \alpha N$ $\dot{N} =\left(a\frac{\dot{x}(t)}{x(t)} -  b\frac{\dot{y}(t)}{y(t)} - c \frac{\dot{z}(t)}{z(t)} \right) \alpha(t)N$ I also know that $N = (px(t) - ry(t) - sz(t))\frac{1}{\alpha+\kappa} $ and $N = (px(t) - ry(t) - sz(t))\frac{1}{\alpha(t)+\kappa}$ respectively - although I am not sure if this information is crucial or not. The way I was thinking to approach this problem is that I know there is a class of separable differential equations: $\frac{dx}{dt}= F(t)g(x)$ And I know how to solve these, but I have no idea if 1. or 2. can be expressed in such form. I think that this should be possible since everything in brackets is a function of t, but I don't think that it is rigorous to just say that $F(t)=\left(a\frac{\dot{x}(t)}{x(t)} -  b\frac{\dot{y}(t)}{y(t)} - c \frac{\dot{z}(t)}{z(t)} \right)$. But I don't know if then the rule for separable differential equation still applies as this would make F a compound function. I also tried to work with inverse functions $t(x)=x^{-1}(t)$, $t(y)=y^{-1}(t)$ and $t(z)=z^{-1}(t)$ and their derivatives, but I feel that just confused me even more. I have never encountered a complex problem such as this so any help would be much appreciated.",,"['ordinary-differential-equations', 'multivariable-calculus']"
49,Closed-form solution for a second order ODE,Closed-form solution for a second order ODE,,"I have a series of second order ODE's that are in the form: $$ Ay''+By'+Cy=\frac{1}{1+e^{-t}} $$ ($y'' = \frac{d^2}{dt^2}y(t)$,  $y' = \frac{d}{dt}y(t)$ ) The initial conditions are known: $y(0)=y_0$, $y'(0)=y'_0$ Clearly, the general solution for the homogeneous form $Ay''+By'+Cy=0$ is trivial; I'm stuck in finding the particular solution. Is there a hope that I can come up with a closed-form solution parametrized by $A$, $B$, $C$ and the initial conditions (All in $\mathbb{R}$)? Background: Initially I was trying to solve the ODE with the step function (the heavy-side function: $u(t)=1~if~t>0; ~0~ow$) on the right-hand side and a different set of coefficients on the left-hand side: $$ (A_1t+A2)y''+(B_1+B2)y'+(C_1t+C2)y=\frac{1}{1+e^{-t}} $$ But, I thought maybe I could find the solution with a smooth function on the RHS, and a simpler ODE in the first place. Therefore, a solution to the original (either with affine or with constant coefficients) problem will be helpful too. Thanks for any help!","I have a series of second order ODE's that are in the form: $$ Ay''+By'+Cy=\frac{1}{1+e^{-t}} $$ ($y'' = \frac{d^2}{dt^2}y(t)$,  $y' = \frac{d}{dt}y(t)$ ) The initial conditions are known: $y(0)=y_0$, $y'(0)=y'_0$ Clearly, the general solution for the homogeneous form $Ay''+By'+Cy=0$ is trivial; I'm stuck in finding the particular solution. Is there a hope that I can come up with a closed-form solution parametrized by $A$, $B$, $C$ and the initial conditions (All in $\mathbb{R}$)? Background: Initially I was trying to solve the ODE with the step function (the heavy-side function: $u(t)=1~if~t>0; ~0~ow$) on the right-hand side and a different set of coefficients on the left-hand side: $$ (A_1t+A2)y''+(B_1+B2)y'+(C_1t+C2)y=\frac{1}{1+e^{-t}} $$ But, I thought maybe I could find the solution with a smooth function on the RHS, and a simpler ODE in the first place. Therefore, a solution to the original (either with affine or with constant coefficients) problem will be helpful too. Thanks for any help!",,['ordinary-differential-equations']
50,"""Explosion in finite time"" in ODE and manifolds","""Explosion in finite time"" in ODE and manifolds",,"We know that, if $F:\mathbb{R}^n\to\mathbb{R}^n$ is regular enough, then a curve $c:t\to\mathbb{R}^n$ solution of the ODE  $$\left\{\begin{array}{l}c'(t)=F(c(t))\\ c(0)=x\in\mathbb{R}^n\end{array}\right.$$ check the following alternative: $c$ is defined on whole $\mathbb{R}$, or $c$ is defined on $]t_\star,t^\star[$, with $t^\star<+\infty$, and for all compact $K\subset\mathbb{R}^n$ it exists a time $t_K\in]t_\star,t^\star[$ such that for all $t\in[t_K,t^\star[$, $c(t)\in \mathbb{R}^n\setminus K$, or ""same as 2. with $t_\star$"". My first question is: does this result have a common name in english litterature? In France we call it ""explosion in finite time"" or ""getting out of all compact sets"", but these seem not to be actual translations. My second question is: does this alternative hold in differential manifold theory? More precisely, if I have a $\mathcal{C}^\infty$ vector field $X$ defined on a smooth manifold $M$, does a solution $c:t\to M$ of  $$\left\{\begin{array}{l}c'(t)=X_{c(t)}\\ c(0)=p\in M\end{array}\right.$$ check exclusively one of the three conditions? If yes, I would be glad to have a sketch of proof/a reference. Thank you all for your attention!","We know that, if $F:\mathbb{R}^n\to\mathbb{R}^n$ is regular enough, then a curve $c:t\to\mathbb{R}^n$ solution of the ODE  $$\left\{\begin{array}{l}c'(t)=F(c(t))\\ c(0)=x\in\mathbb{R}^n\end{array}\right.$$ check the following alternative: $c$ is defined on whole $\mathbb{R}$, or $c$ is defined on $]t_\star,t^\star[$, with $t^\star<+\infty$, and for all compact $K\subset\mathbb{R}^n$ it exists a time $t_K\in]t_\star,t^\star[$ such that for all $t\in[t_K,t^\star[$, $c(t)\in \mathbb{R}^n\setminus K$, or ""same as 2. with $t_\star$"". My first question is: does this result have a common name in english litterature? In France we call it ""explosion in finite time"" or ""getting out of all compact sets"", but these seem not to be actual translations. My second question is: does this alternative hold in differential manifold theory? More precisely, if I have a $\mathcal{C}^\infty$ vector field $X$ defined on a smooth manifold $M$, does a solution $c:t\to M$ of  $$\left\{\begin{array}{l}c'(t)=X_{c(t)}\\ c(0)=p\in M\end{array}\right.$$ check exclusively one of the three conditions? If yes, I would be glad to have a sketch of proof/a reference. Thank you all for your attention!",,"['ordinary-differential-equations', 'smooth-manifolds', 'translation-request']"
51,How to solve a differential equation including $\ddot{x}\dot{x}$ term?,How to solve a differential equation including  term?,\ddot{x}\dot{x},"I have the following differential equation $$ m \ddot{x} \dot{x} + f \dot{x} = p $$ where $m > 0$ , $f > 0$ , $p > 0$ are constants. How to solve it analytically?","I have the following differential equation where , , are constants. How to solve it analytically?", m \ddot{x} \dot{x} + f \dot{x} = p  m > 0 f > 0 p > 0,['ordinary-differential-equations']
52,Solving a differential equation with wronskians,Solving a differential equation with wronskians,,"So I am asked to find a solution to this ODE here and I feel like I am missing something very obvious. I am asked to find the general solution of: $x^2y""-3xy'+4y=\frac{x^2}{ln(x)}, y>1$ So I first tried to find the homogeneous solution which was just a cauchy Euler equation: $x^2y""-3xy'+4y=0$ if I solve that, I get $y_{h}(x)=c_1 x^2 + c_2 x^2 ln(x)$ Then I tried to use variation of parameters to solve the particular solution. I obtain: $W=\begin{bmatrix}   x^2 & x^2\ln(x) \\  2x & x+2x\ln(x) \\ \end{bmatrix}$ The wronskian ends up becoming $W=x^3$ so things worked out really nicely. If I try to find the particular solution though, I can't integrate one of the integrals. $Y_p(t)=-y_1\int \frac{y_2g(x)}{W}+y_2\int \frac{y_1g(x)}{W} dx$ $Y_p(t)=-x^2\int x dx +x^2\ln(x)\int \frac{x}{ln(x)} dx$ but the second integral can't be done so either I made a mistake or there is another way to solve this. I can't even use Laplace transforms since I don't have initial conditions so I am a little lost here... Thanks!","So I am asked to find a solution to this ODE here and I feel like I am missing something very obvious. I am asked to find the general solution of: $x^2y""-3xy'+4y=\frac{x^2}{ln(x)}, y>1$ So I first tried to find the homogeneous solution which was just a cauchy Euler equation: $x^2y""-3xy'+4y=0$ if I solve that, I get $y_{h}(x)=c_1 x^2 + c_2 x^2 ln(x)$ Then I tried to use variation of parameters to solve the particular solution. I obtain: $W=\begin{bmatrix}   x^2 & x^2\ln(x) \\  2x & x+2x\ln(x) \\ \end{bmatrix}$ The wronskian ends up becoming $W=x^3$ so things worked out really nicely. If I try to find the particular solution though, I can't integrate one of the integrals. $Y_p(t)=-y_1\int \frac{y_2g(x)}{W}+y_2\int \frac{y_1g(x)}{W} dx$ $Y_p(t)=-x^2\int x dx +x^2\ln(x)\int \frac{x}{ln(x)} dx$ but the second integral can't be done so either I made a mistake or there is another way to solve this. I can't even use Laplace transforms since I don't have initial conditions so I am a little lost here... Thanks!",,"['ordinary-differential-equations', 'laplace-transform']"
53,Can a Homogeneous Differential Equation be Nonlinear?,Can a Homogeneous Differential Equation be Nonlinear?,,"First of all, I am not asking about $v=y/x$ transformation kinda homogeneous. Can we say  this nonlinear differential equation is homogeneous $$y^{\prime}=ty^2.$$ Here there is no term without $y$, this is okay. But this is nonlinear equation. I saw here some discussions and some people said it is just for linear equations. I want to give two links: Dummies Series (assumes it can be nonlinear) (I also see some universities documents too). Wolfram Math World (assumes it has to be linear) Which one is the definition: 1) No term without $y$, 2) Linear and No term without $y$. 3) $y$ is a solution, then $\lambda y$ is solution for all $\lambda \in \mathbb R$. Considering the references, I will use the 3rd one. This option is also meaningful.","First of all, I am not asking about $v=y/x$ transformation kinda homogeneous. Can we say  this nonlinear differential equation is homogeneous $$y^{\prime}=ty^2.$$ Here there is no term without $y$, this is okay. But this is nonlinear equation. I saw here some discussions and some people said it is just for linear equations. I want to give two links: Dummies Series (assumes it can be nonlinear) (I also see some universities documents too). Wolfram Math World (assumes it has to be linear) Which one is the definition: 1) No term without $y$, 2) Linear and No term without $y$. 3) $y$ is a solution, then $\lambda y$ is solution for all $\lambda \in \mathbb R$. Considering the references, I will use the 3rd one. This option is also meaningful.",,"['ordinary-differential-equations', 'reference-request', 'definition']"
54,Limits for the solution of the non-linear ODE,Limits for the solution of the non-linear ODE,,"Consider the ODE $$y''+y'+y^3=0$$ I need to prove that $$\lim_{x\rightarrow \infty} y(x) = 0$$ and $$\lim_{x\rightarrow \infty} y'(x) = 0.$$ Well, introducing the change of variables such as $x_1=y,x_2=y'$ I get the system of equations nonlinear in $x_1, x_2$. My question is, if I linearize this system around $(0,0)$ and analyze the behaviour of the linearized system there, would I be correct to infer that the behaviour is the same for a nonlinear (original) system? Say, for a solution to the linearized system the limits above hold true. Would they hold true for the original system as well then?","Consider the ODE $$y''+y'+y^3=0$$ I need to prove that $$\lim_{x\rightarrow \infty} y(x) = 0$$ and $$\lim_{x\rightarrow \infty} y'(x) = 0.$$ Well, introducing the change of variables such as $x_1=y,x_2=y'$ I get the system of equations nonlinear in $x_1, x_2$. My question is, if I linearize this system around $(0,0)$ and analyze the behaviour of the linearized system there, would I be correct to infer that the behaviour is the same for a nonlinear (original) system? Say, for a solution to the linearized system the limits above hold true. Would they hold true for the original system as well then?",,"['ordinary-differential-equations', 'nonlinear-system', 'stability-in-odes']"
55,Notation regarding integral curves and vector fields,Notation regarding integral curves and vector fields,,"trying to solve some integral curve of vector field I am really confused about the notation used in many textbook. I can see problem of this sort : Find the integral curves of the following vector field : \begin{align} X(x,y) = x^2\frac{\partial}{\partial x} + xy\frac{\partial}{\partial y} \end{align} So to set up the notation in this setting, a curve should be $\gamma : \mathbb{R} \rightarrow \mathbb{R}^2$ and any vector field of the curve can be written as : \begin{align} X_{{\gamma},p} = \frac{\partial }{\partial x^i}.(x^i\circ\gamma)'(0) \end{align} Where $\gamma(0) = p$. So if we want to find the integral curve of the vector field we have to make sure at any point of the curve (any $t$) the tangent vector of the curve is the same as the vector field evaluated at $\gamma(t)$. The problem is when equating the two equation we end up with something like $x^2 = (x^i\circ\gamma)'(0)$ which do not make sense. Are we abusing notation in this sort of problems where $x^2$ is actually the composition of the coordinate function $x$ with the curve at the specific point of interest ? If this is the case, how inaccurate is the drawing of the vector field in 2D ? ( I can see in my textbook the drawing of the vector field where at each $(x,y)$ an arrow is drawn with direction $(x^2, xy)$. I can see some other textbook which define the curve to be $\gamma(t) = (x(t), y(t))$ which again do not make sense as it is conflicting with the coordinate function definition $x : \mathbb{R^2} \rightarrow \mathbb{R}$. I am asking this question because the books go ahead solving an ODE where they have no problem solving stuff like : $x' = x^2$ and $y' = xy$. So is this abusing the notation ?","trying to solve some integral curve of vector field I am really confused about the notation used in many textbook. I can see problem of this sort : Find the integral curves of the following vector field : \begin{align} X(x,y) = x^2\frac{\partial}{\partial x} + xy\frac{\partial}{\partial y} \end{align} So to set up the notation in this setting, a curve should be $\gamma : \mathbb{R} \rightarrow \mathbb{R}^2$ and any vector field of the curve can be written as : \begin{align} X_{{\gamma},p} = \frac{\partial }{\partial x^i}.(x^i\circ\gamma)'(0) \end{align} Where $\gamma(0) = p$. So if we want to find the integral curve of the vector field we have to make sure at any point of the curve (any $t$) the tangent vector of the curve is the same as the vector field evaluated at $\gamma(t)$. The problem is when equating the two equation we end up with something like $x^2 = (x^i\circ\gamma)'(0)$ which do not make sense. Are we abusing notation in this sort of problems where $x^2$ is actually the composition of the coordinate function $x$ with the curve at the specific point of interest ? If this is the case, how inaccurate is the drawing of the vector field in 2D ? ( I can see in my textbook the drawing of the vector field where at each $(x,y)$ an arrow is drawn with direction $(x^2, xy)$. I can see some other textbook which define the curve to be $\gamma(t) = (x(t), y(t))$ which again do not make sense as it is conflicting with the coordinate function definition $x : \mathbb{R^2} \rightarrow \mathbb{R}$. I am asking this question because the books go ahead solving an ODE where they have no problem solving stuff like : $x' = x^2$ and $y' = xy$. So is this abusing the notation ?",,"['ordinary-differential-equations', 'differential-geometry', 'vectors']"
56,Differential equation in polar coordinates,Differential equation in polar coordinates,,I have the following system: $\frac{dx}{dt} = 3x + y - x(x^2+y^2)$ $\frac{dy}{dt} = -x +3y -y(x^2+y^2)$ Converting this to polar coordinates gives us: $\frac{dr}{dt} = r(3-r^2)$ $\frac{d\theta}{dt} = -1$ This gives us a solution $\theta(t) = -t + \theta_0$. What would the solution for $r(t)$ be though?,I have the following system: $\frac{dx}{dt} = 3x + y - x(x^2+y^2)$ $\frac{dy}{dt} = -x +3y -y(x^2+y^2)$ Converting this to polar coordinates gives us: $\frac{dr}{dt} = r(3-r^2)$ $\frac{d\theta}{dt} = -1$ This gives us a solution $\theta(t) = -t + \theta_0$. What would the solution for $r(t)$ be though?,,"['analysis', 'ordinary-differential-equations', 'polar-coordinates']"
57,A Differential Equation with Dirac Delta as the Non-homogeneity Term,A Differential Equation with Dirac Delta as the Non-homogeneity Term,,"I'm currently studying Computational Mechanics, and an important step to solve problems is knowing how to express physical interactions in terms of mathematical equations. Consider for example, the problem of achieving the nodal displacement of a bar that is clamped on a wall, and there's a punctual force $P$ applied in the middle of its length. $\hspace{75pt}$ The differential equation that rules the system is: $$\frac{d^2u}{dx^2}=-\frac{P}{EA}\delta\left(x-L\right)$$ where: $u$ is the displacement; $E$ is the Young's modulus; $A$ is the sectional area of the bar; $L$ is half of the length of the bar; $\delta$ is the Dirac delta function. I intend to know if my solution of this problem is mathematically right: I just integrated two times the differential equation and got:   $$ u(x)= \begin{cases} \hphantom{-}C_1x+D_1 &\text{if x}<L,\\[2ex] \left(-\frac{P}{EA}+C_1\right)(x-L)+D_2&\text{if x}>L. \end{cases} $$   where $C_1$, $D_1$ and $D_2$ are constants of integration.   Now we just need to look for boundary conditions.   The clamping condition gives:    $$u(0)=0$$   The continuity of the bar gives:    $$u(L^-)=u(L^+)$$   The clamping is on the left side of the bar, so the force $P$ will only be applied on the immediate left surroundings of $L$, i.e. $L^-$ ($P$ it's the complement action of the reaction on the wall). So,   $$P=EA\frac{du}{dx}\bigg{|}_{(x=L^-)}$$   with these conditions I got:   $$ u(x)= \begin{cases} \hphantom{-}\frac{P}{EA}x &\text{if x}<L,\\[2ex] \frac{P}{EA}L &\text{if x}>L. \end{cases} $$ My main difficulty in these kind of problems is how to deal with the Dirac delta functions, and how to deal with the boundary conditions in respect to the concentrated applied forces. This requires much more than pure mathematical analysis. I'm not 100% confident of what I did because I didn't compare the results with known literature. The bibliography of my course doesn't talk about the solution of differential equations of this kind. Do you know any good books that treat this type of problems?","I'm currently studying Computational Mechanics, and an important step to solve problems is knowing how to express physical interactions in terms of mathematical equations. Consider for example, the problem of achieving the nodal displacement of a bar that is clamped on a wall, and there's a punctual force $P$ applied in the middle of its length. $\hspace{75pt}$ The differential equation that rules the system is: $$\frac{d^2u}{dx^2}=-\frac{P}{EA}\delta\left(x-L\right)$$ where: $u$ is the displacement; $E$ is the Young's modulus; $A$ is the sectional area of the bar; $L$ is half of the length of the bar; $\delta$ is the Dirac delta function. I intend to know if my solution of this problem is mathematically right: I just integrated two times the differential equation and got:   $$ u(x)= \begin{cases} \hphantom{-}C_1x+D_1 &\text{if x}<L,\\[2ex] \left(-\frac{P}{EA}+C_1\right)(x-L)+D_2&\text{if x}>L. \end{cases} $$   where $C_1$, $D_1$ and $D_2$ are constants of integration.   Now we just need to look for boundary conditions.   The clamping condition gives:    $$u(0)=0$$   The continuity of the bar gives:    $$u(L^-)=u(L^+)$$   The clamping is on the left side of the bar, so the force $P$ will only be applied on the immediate left surroundings of $L$, i.e. $L^-$ ($P$ it's the complement action of the reaction on the wall). So,   $$P=EA\frac{du}{dx}\bigg{|}_{(x=L^-)}$$   with these conditions I got:   $$ u(x)= \begin{cases} \hphantom{-}\frac{P}{EA}x &\text{if x}<L,\\[2ex] \frac{P}{EA}L &\text{if x}>L. \end{cases} $$ My main difficulty in these kind of problems is how to deal with the Dirac delta functions, and how to deal with the boundary conditions in respect to the concentrated applied forces. This requires much more than pure mathematical analysis. I'm not 100% confident of what I did because I didn't compare the results with known literature. The bibliography of my course doesn't talk about the solution of differential equations of this kind. Do you know any good books that treat this type of problems?",,"['ordinary-differential-equations', 'distribution-theory', 'dirac-delta', 'finite-element-method']"
58,Steady-state solution and initial conditions,Steady-state solution and initial conditions,,"Let's say that we have the following first order differential equation: $$\frac{d \rho(t)}{d t}=F(\rho(t))$$ with some given initial condition $\rho(0)$. I am interested in the steady-state solution $\rho_{ss}$, and to find it we set $\left.\frac{d\rho(t)}{d t}\right|_{t_{ss}}=0$ and solve the algebraic equation  $$F(\rho_{ss})=0$$ However, how in this situation do we account for the initial condition $\rho(0)=\rho_0$, without solving the full differential equation? After solving the algebraic equation I get infinetely many solutions (actually two distinct solutions, but their superposition is also a solution). P.s. I don't know if this helps, but in my case there is a conserved quantity $Q$, which is the same for $\rho(0)$ and $\rho_{ss}$.","Let's say that we have the following first order differential equation: $$\frac{d \rho(t)}{d t}=F(\rho(t))$$ with some given initial condition $\rho(0)$. I am interested in the steady-state solution $\rho_{ss}$, and to find it we set $\left.\frac{d\rho(t)}{d t}\right|_{t_{ss}}=0$ and solve the algebraic equation  $$F(\rho_{ss})=0$$ However, how in this situation do we account for the initial condition $\rho(0)=\rho_0$, without solving the full differential equation? After solving the algebraic equation I get infinetely many solutions (actually two distinct solutions, but their superposition is also a solution). P.s. I don't know if this helps, but in my case there is a conserved quantity $Q$, which is the same for $\rho(0)$ and $\rho_{ss}$.",,"['ordinary-differential-equations', 'initial-value-problems', 'steady-state']"
59,"Partial differential equation $[\partial_t + v(x)\partial_x - \rho(x)] D(t,x) = 0$",Partial differential equation,"[\partial_t + v(x)\partial_x - \rho(x)] D(t,x) = 0","I'm stuck with the following problem I found (without a proof) in the Peskin and Schroeder textbook on quantum field theory (the differential equation mentioned below is equivalent to the Callan-Symanzik equation describing the evolution of the n-point correlation functions under variation of the energy scale at which the theory is defined): Show that the solution of the equation $$ [\partial_t + v(x)\partial_x - \rho(x)] D(t,x) = 0 $$ has the form $$ D(t,x) = D(0,X_t(x)) \exp\left(\int_0^t d t' \rho(X_{t'}(x)) \right), $$ where $X_t(x)$ is the solution of the equation $$ \partial_t X_t(x) = - v(X_t(x)) $$ with the initial condition $$ X_0(x) = x. $$ Let's compute e.g. $$ \partial_t \left[D(0,X_t(x)) \exp\left(\int_0^t d t' \rho(X_{t'}(x)) \right) \right]= \left[\partial_t D(0,X_t(x))\right] \exp\left(\int_0^t d t' \rho(X_{t'}(x)) \right)  + D(0,X_t(x)) \rho(X_t(x)) \exp\left(\int_0^t d t' \rho(X_{t'}(x)) \right)  $$ I don't see any possible simplifications with other terms. I wonder how to show that the solution of the mentioned differential equation is indeed of the above form.","I'm stuck with the following problem I found (without a proof) in the Peskin and Schroeder textbook on quantum field theory (the differential equation mentioned below is equivalent to the Callan-Symanzik equation describing the evolution of the n-point correlation functions under variation of the energy scale at which the theory is defined): Show that the solution of the equation $$ [\partial_t + v(x)\partial_x - \rho(x)] D(t,x) = 0 $$ has the form $$ D(t,x) = D(0,X_t(x)) \exp\left(\int_0^t d t' \rho(X_{t'}(x)) \right), $$ where $X_t(x)$ is the solution of the equation $$ \partial_t X_t(x) = - v(X_t(x)) $$ with the initial condition $$ X_0(x) = x. $$ Let's compute e.g. $$ \partial_t \left[D(0,X_t(x)) \exp\left(\int_0^t d t' \rho(X_{t'}(x)) \right) \right]= \left[\partial_t D(0,X_t(x))\right] \exp\left(\int_0^t d t' \rho(X_{t'}(x)) \right)  + D(0,X_t(x)) \rho(X_t(x)) \exp\left(\int_0^t d t' \rho(X_{t'}(x)) \right)  $$ I don't see any possible simplifications with other terms. I wonder how to show that the solution of the mentioned differential equation is indeed of the above form.",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative', 'mathematical-physics']"
60,Problem getting a diffeomorphism work on $\mathbb{R}^{3}$,Problem getting a diffeomorphism work on,\mathbb{R}^{3},"I am trying to come up with a diffeomorphism f the cube onto itself. The reason I am trying to do this is because for a study I am doing I need a way of deforming the cube (onto itself) and be able to perfectly ""bring back"" the deformed domain to the original, in order to test some numerical algorithm I am working. Clarification just in case I have the XY problem . This is a follow up question from Example of a diffeomorphism on $\mathbb{R}^{3}$ onto itself (or cube onto itself) .  I am trying to make a real example out of it. I start from defining ""a smooth (at least continuously differentiable) vector field $V(x,y,z)$ on the cube $Q$, which vanishes on the boundary of the cube. If I don't understand wrong this is  a valid $V$: \begin{align*} V(x,y,z) = \bigl(   &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L),\\   &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L),\\   &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L)\bigr), \end{align*} being $L$ the length of the cube. Then I solve the ODE  $$ \frac{d}{dt} h(x,y,z,t)= V(x,y,z) $$  with initial conditions  $h(x,y,z,0) = (x,y,z)$. If I am not wrong (I may be) the solution is: $$ \frac{d}{dt}[h_x(x,y,z,t), h_y(x,y,z,t), h_z(x,y,z,t)] = [V_x, V_y, V_z]. $$ And by integrating the right hand side from $t=0$ to $t=t_0$ $dt$ I get: $$ h_x(x,y,z,t) = x_0 + t\cdot\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L) $$ (same with the other two). This equation does indeed describe a map of the cube onto itself (automorphism?), that actually looks similar to the image in the original post (especially if you modify $L$ by e.g. $L/2$). However, I am supposed to find the inverse map if I find the solution to $h_x(x,y,z,-t)$, which, if I am not wrong, simply translates to  $$ h_x(x,y,z,-t)=x_0 - t\cdot\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L) $$ in my case. However, when numerically testing this, if I ""warp"" a grid with the first map, and then use this last one on those ""warped"" points, I do not get the original grid. So questions: What is wrong with my maths? What did I miss? What is the solution for this map? Is there one? If this is a bad way of doing the thing, what should I do? How can I get a map of the cube onto itself that I can invert ""perfectly"" (meaning I don't want a numerical approximation)? PD: MATLAB code that I am using for this: [x,y,z]=meshgrid(0:30,0:30,0:30);  L=30; t=1; x2=x - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L); y2=y - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L); z2=z - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L);  x3=x2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L); y3=y2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L); z3=z2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L);  for indz=1:31     clf %     plot(x(:,:,indz),y(:,:,indz),'r.');     mesh(x(:,:,indz),y(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','g','FaceColor','none');      hold on     mesh(x2(:,:,indz),y2(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','k','FaceColor','none');     mesh(x3(:,:,indz),y3(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','b','FaceColor','none');      view(2)     axis tight     drawnow     pause(0.2) end","I am trying to come up with a diffeomorphism f the cube onto itself. The reason I am trying to do this is because for a study I am doing I need a way of deforming the cube (onto itself) and be able to perfectly ""bring back"" the deformed domain to the original, in order to test some numerical algorithm I am working. Clarification just in case I have the XY problem . This is a follow up question from Example of a diffeomorphism on $\mathbb{R}^{3}$ onto itself (or cube onto itself) .  I am trying to make a real example out of it. I start from defining ""a smooth (at least continuously differentiable) vector field $V(x,y,z)$ on the cube $Q$, which vanishes on the boundary of the cube. If I don't understand wrong this is  a valid $V$: \begin{align*} V(x,y,z) = \bigl(   &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L),\\   &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L),\\   &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L)\bigr), \end{align*} being $L$ the length of the cube. Then I solve the ODE  $$ \frac{d}{dt} h(x,y,z,t)= V(x,y,z) $$  with initial conditions  $h(x,y,z,0) = (x,y,z)$. If I am not wrong (I may be) the solution is: $$ \frac{d}{dt}[h_x(x,y,z,t), h_y(x,y,z,t), h_z(x,y,z,t)] = [V_x, V_y, V_z]. $$ And by integrating the right hand side from $t=0$ to $t=t_0$ $dt$ I get: $$ h_x(x,y,z,t) = x_0 + t\cdot\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L) $$ (same with the other two). This equation does indeed describe a map of the cube onto itself (automorphism?), that actually looks similar to the image in the original post (especially if you modify $L$ by e.g. $L/2$). However, I am supposed to find the inverse map if I find the solution to $h_x(x,y,z,-t)$, which, if I am not wrong, simply translates to  $$ h_x(x,y,z,-t)=x_0 - t\cdot\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L) $$ in my case. However, when numerically testing this, if I ""warp"" a grid with the first map, and then use this last one on those ""warped"" points, I do not get the original grid. So questions: What is wrong with my maths? What did I miss? What is the solution for this map? Is there one? If this is a bad way of doing the thing, what should I do? How can I get a map of the cube onto itself that I can invert ""perfectly"" (meaning I don't want a numerical approximation)? PD: MATLAB code that I am using for this: [x,y,z]=meshgrid(0:30,0:30,0:30);  L=30; t=1; x2=x - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L); y2=y - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L); z2=z - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L);  x3=x2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L); y3=y2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L); z3=z2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L);  for indz=1:31     clf %     plot(x(:,:,indz),y(:,:,indz),'r.');     mesh(x(:,:,indz),y(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','g','FaceColor','none');      hold on     mesh(x2(:,:,indz),y2(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','k','FaceColor','none');     mesh(x3(:,:,indz),y3(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','b','FaceColor','none');      view(2)     axis tight     drawnow     pause(0.2) end",,"['ordinary-differential-equations', 'definite-integrals', 'numerical-methods', 'lie-groups']"
61,Antiderivative of $e^{2\arctan{x}}$,Antiderivative of,e^{2\arctan{x}},"Is there any way to integrate this: $$\int e^{2\arctan x}\, dx$$ I tried to solve it using integration by parts but I could not end the integral because it got very difficult. Then I tried to solve it using Mathematica but it returned a really weird expression. The expression is part of solving this equation $$ y' + \frac{y}{1+x^2}=e^{\arctan x}\ $$ $$ V = e^{-\int f(x) dx} = e^{-\int \frac{dx}{1+x^2}} = e^{-\arctan x}$$ $$R = e^{\arctan x} $$ $$U = \int \frac{e^{\arctan x}}{ e^{-\arctan x}} dx$$ $$ U = \int e^{2\arctan x} dx $$","Is there any way to integrate this: $$\int e^{2\arctan x}\, dx$$ I tried to solve it using integration by parts but I could not end the integral because it got very difficult. Then I tried to solve it using Mathematica but it returned a really weird expression. The expression is part of solving this equation $$ y' + \frac{y}{1+x^2}=e^{\arctan x}\ $$ $$ V = e^{-\int f(x) dx} = e^{-\int \frac{dx}{1+x^2}} = e^{-\arctan x}$$ $$R = e^{\arctan x} $$ $$U = \int \frac{e^{\arctan x}}{ e^{-\arctan x}} dx$$ $$ U = \int e^{2\arctan x} dx $$",,"['calculus', 'integration', 'ordinary-differential-equations', 'special-functions', 'indefinite-integrals']"
62,How to solve $y(x) y'(x) +C_1y(x)^2+C_2y(x)={1\over2}C_3$ [closed],How to solve  [closed],y(x) y'(x) +C_1y(x)^2+C_2y(x)={1\over2}C_3,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question $$y(x) y'(x) +C_1y(x)^2+C_2y(x)={1\over2}C_3 $$ C1, C2 , C3 are constants. how to solve this equation??","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question $$y(x) y'(x) +C_1y(x)^2+C_2y(x)={1\over2}C_3 $$ C1, C2 , C3 are constants. how to solve this equation??",,['ordinary-differential-equations']
63,logarithm transition for the population growth equation,logarithm transition for the population growth equation,,Analyzing the growth population equation I came across with the below transition $$\frac{d \ lnN}{dt}=\frac{dN}{N dt}$$ which I don't quite follow. Can anybody clarify this please?,Analyzing the growth population equation I came across with the below transition $$\frac{d \ lnN}{dt}=\frac{dN}{N dt}$$ which I don't quite follow. Can anybody clarify this please?,,"['ordinary-differential-equations', 'logarithms']"
64,How to solve the differential equation: $x \frac{d^2y}{dx^2}+2(3x+1)\frac{dy}{dx}+3y(3x+2)=18x$,How to solve the differential equation:,x \frac{d^2y}{dx^2}+2(3x+1)\frac{dy}{dx}+3y(3x+2)=18x,How to solve the differential equation $$x \frac{d^2y}{dx^2}+2(3x+1)\frac{dy}{dx}+3y(3x+2)=18x$$ I think I could let $u=xy$ but I don't know how to proceed it.,How to solve the differential equation $$x \frac{d^2y}{dx^2}+2(3x+1)\frac{dy}{dx}+3y(3x+2)=18x$$ I think I could let $u=xy$ but I don't know how to proceed it.,,"['calculus', 'ordinary-differential-equations']"
65,Analytical solution to coupled nonlinear ODEs,Analytical solution to coupled nonlinear ODEs,,"I am looking to solve several coupled nonlinear ODEs like this one: $\hspace{20mm} \frac{d x(t)}{dt} = C_1 \cdot x(t) + C_2 \cdot y(t) + C_3\cdot (x(t)^2 + y(t)^2) x(t),$ $\hspace{20mm} \frac{d y(t)}{dt} = C_1 \cdot y(t) - C_2 \cdot x(t) + C_3\cdot (x(t)^2 + y(t)^2) y(t).$ I have tried multiplying the first by $y(t)$ and the second by $x(t)$ and adding them or subtracting them, but with no luck. I have noticed that each system is almost symmetrical, is there any general procedure for such systems? If not, how do I solve this system? Context I am looking into Rain-Wind induced vibrations of stay cables. I am trying to approximate the model I have derived using a Multiple Time-Scale Perturbation Analysis. During this analysis, many coupled nonlinear ODEs need to be solved, which are almost symmetrical.","I am looking to solve several coupled nonlinear ODEs like this one: $\hspace{20mm} \frac{d x(t)}{dt} = C_1 \cdot x(t) + C_2 \cdot y(t) + C_3\cdot (x(t)^2 + y(t)^2) x(t),$ $\hspace{20mm} \frac{d y(t)}{dt} = C_1 \cdot y(t) - C_2 \cdot x(t) + C_3\cdot (x(t)^2 + y(t)^2) y(t).$ I have tried multiplying the first by $y(t)$ and the second by $x(t)$ and adding them or subtracting them, but with no luck. I have noticed that each system is almost symmetrical, is there any general procedure for such systems? If not, how do I solve this system? Context I am looking into Rain-Wind induced vibrations of stay cables. I am trying to approximate the model I have derived using a Multiple Time-Scale Perturbation Analysis. During this analysis, many coupled nonlinear ODEs need to be solved, which are almost symmetrical.",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'perturbation-theory']"
66,Could I learn linear algebra before or along with calculus? (Same with differential equations),Could I learn linear algebra before or along with calculus? (Same with differential equations),,"I want to plan the next few subjects I learn (by self-study) in mathematics. I have made it through the equivalent of maybe half a Calculus I class so far, but I would like to start a bit on linear algebra and/or differential equations (which I will not do until maybe I work on my calculus some more). Is it a good idea to take this path?","I want to plan the next few subjects I learn (by self-study) in mathematics. I have made it through the equivalent of maybe half a Calculus I class so far, but I would like to start a bit on linear algebra and/or differential equations (which I will not do until maybe I work on my calculus some more). Is it a good idea to take this path?",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'soft-question']"
67,Finding the stable and unstable manifold of this system,Finding the stable and unstable manifold of this system,,"Consider the system $$\begin{cases}\dot{x} = x \\ \dot{y} = -y + x^2\end{cases}$$ This has fixed point $\overline{X} = (0,0)$, which is a saddle point. The aim is to find the equation of the stable and unstable manifold for the system. My lecture notes finds the unstable manifold in the following way: Use the trick: $\dot{x}\frac{dy}{dx} = \dot{y}$. For the system this gives $x\frac{dy}{dx}=-y+x^2$. We insert our power series $y(x) = \sum^\infty_{k=2}a_kx^k$ into the equation to give $\sum^\infty_{k=2}a_kkx^k = \sum^\infty_{k=2}(-a_k)x^k + x^2$. Comparing coefficients gives $a_2 = \frac{1}{3}$ and $a_k = 0$, for all $k > 2$. Hence, $y(x) = \frac{x^2}{3}$. This is correct and I'm fine working through this. But I don't know how to use this to find the stable manifold (which turns out to be $x = 0$). I've tried interchanging $x$ and $y$ to try and end up with an equation in $x$ but I can't actually get anywhere with it. How would I go about finding the stable manifold?","Consider the system $$\begin{cases}\dot{x} = x \\ \dot{y} = -y + x^2\end{cases}$$ This has fixed point $\overline{X} = (0,0)$, which is a saddle point. The aim is to find the equation of the stable and unstable manifold for the system. My lecture notes finds the unstable manifold in the following way: Use the trick: $\dot{x}\frac{dy}{dx} = \dot{y}$. For the system this gives $x\frac{dy}{dx}=-y+x^2$. We insert our power series $y(x) = \sum^\infty_{k=2}a_kx^k$ into the equation to give $\sum^\infty_{k=2}a_kkx^k = \sum^\infty_{k=2}(-a_k)x^k + x^2$. Comparing coefficients gives $a_2 = \frac{1}{3}$ and $a_k = 0$, for all $k > 2$. Hence, $y(x) = \frac{x^2}{3}$. This is correct and I'm fine working through this. But I don't know how to use this to find the stable manifold (which turns out to be $x = 0$). I've tried interchanging $x$ and $y$ to try and end up with an equation in $x$ but I can't actually get anywhere with it. How would I go about finding the stable manifold?",,"['ordinary-differential-equations', 'manifolds', 'dynamical-systems']"
68,Solution of partial differential equation,Solution of partial differential equation,,"Solve the differential equation, $$ z=\frac{\partial z}{\partial x}x + \frac{\partial z}{\partial y}y+ (\frac{\partial z}{\partial x})^2 + \frac{\partial z}{\partial x}\frac{\partial z}{\partial y}+  (\frac{\partial z}{\partial y})^2.$$ Can you please solve and explain the solution assuming that I don't have any idea.","Solve the differential equation, $$ z=\frac{\partial z}{\partial x}x + \frac{\partial z}{\partial y}y+ (\frac{\partial z}{\partial x})^2 + \frac{\partial z}{\partial x}\frac{\partial z}{\partial y}+  (\frac{\partial z}{\partial y})^2.$$ Can you please solve and explain the solution assuming that I don't have any idea.",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative', 'implicit-differentiation']"
69,Laplace-Beltrami operator on a circle - Explanation,Laplace-Beltrami operator on a circle - Explanation,,"I would like to find the spectrum of a circle. I know that the Laplace-Beltrami eigenvalue problem for unit circle is equivalent to the regular Laplacian eigenvalue problem for  the interval of the length $2\pi$ with periodic Boundary conditions. In a sense, the Laplace-Beltrami operator (i.e. $∆ =\frac{1}{\sqrt{G}} \sum_{i,j=1}^n \frac{\partial}{\partial_i} (\sqrt{G} g^{ij} \frac{\partial}{\partial_i})$) on circle can be viewed as a Laplacian of a function depending on the arc length. I am not familiar with the use of the Laplace-Beltrami operator and Helmholtz equation. Some things I know is in the definition of page $1$ of pdf . The first formula of the pdf, the definition of the $Δ$ operator in manifolds in term of local coordinates $x_i$ and the metric $g$ and I'd say that if the manifold is a $1$-manifold homeomorphic to the circle, we can choose the coordinates such that $g$ is constant and $=1$, and it reduces again to the eigenvalue equation $h″=λh$ with $h:ℝ→ℂ$ and  periodic. Is there anyone could solve it in using these tools?","I would like to find the spectrum of a circle. I know that the Laplace-Beltrami eigenvalue problem for unit circle is equivalent to the regular Laplacian eigenvalue problem for  the interval of the length $2\pi$ with periodic Boundary conditions. In a sense, the Laplace-Beltrami operator (i.e. $∆ =\frac{1}{\sqrt{G}} \sum_{i,j=1}^n \frac{\partial}{\partial_i} (\sqrt{G} g^{ij} \frac{\partial}{\partial_i})$) on circle can be viewed as a Laplacian of a function depending on the arc length. I am not familiar with the use of the Laplace-Beltrami operator and Helmholtz equation. Some things I know is in the definition of page $1$ of pdf . The first formula of the pdf, the definition of the $Δ$ operator in manifolds in term of local coordinates $x_i$ and the metric $g$ and I'd say that if the manifold is a $1$-manifold homeomorphic to the circle, we can choose the coordinates such that $g$ is constant and $=1$, and it reduces again to the eigenvalue equation $h″=λh$ with $h:ℝ→ℂ$ and  periodic. Is there anyone could solve it in using these tools?",,['ordinary-differential-equations']
70,Solving a separable integral equation: $y(x) = 1+\int_{1}^{x} \frac{y(t)dt}{t(t+1)}dt$,Solving a separable integral equation:,y(x) = 1+\int_{1}^{x} \frac{y(t)dt}{t(t+1)}dt,Solving integral equation. My answer is wrong. Where do I make a mistake? $$y(x) = 1+\int_{1}^{x} \frac{y(t)dt}{t(t+1)}dt $$ $$ y'(x) = \frac{d}{dx} \int_{1}^{x} \frac{y(t)dt}{t(t+1)}dt$$ $$ \frac{dy}{dx} = \frac{y(x)}{x(x+1)}$$ $$ \frac{1}{y} = \frac{1}{x(x+1)}dx$$ $$ \int\frac{1}{y} = \int\frac{1}{x(x+1)}dx$$ $$ ln|y| = ln|x|-ln|x+1|+C$$ By y(1) = 1 from the original equation. $$ C = ln|2| $$ $$ ln|y| = ln|x|-ln|x+1|+ln|2|$$ $$ y = e^{ln|x|-ln|x+1|+ln|2|}$$ $$ y = -x(x+1)ln|2|$$ But the correct answer is $$ y = \frac{2x}{x+1}$$,Solving integral equation. My answer is wrong. Where do I make a mistake? $$y(x) = 1+\int_{1}^{x} \frac{y(t)dt}{t(t+1)}dt $$ $$ y'(x) = \frac{d}{dx} \int_{1}^{x} \frac{y(t)dt}{t(t+1)}dt$$ $$ \frac{dy}{dx} = \frac{y(x)}{x(x+1)}$$ $$ \frac{1}{y} = \frac{1}{x(x+1)}dx$$ $$ \int\frac{1}{y} = \int\frac{1}{x(x+1)}dx$$ $$ ln|y| = ln|x|-ln|x+1|+C$$ By y(1) = 1 from the original equation. $$ C = ln|2| $$ $$ ln|y| = ln|x|-ln|x+1|+ln|2|$$ $$ y = e^{ln|x|-ln|x+1|+ln|2|}$$ $$ y = -x(x+1)ln|2|$$ But the correct answer is $$ y = \frac{2x}{x+1}$$,,"['integration', 'ordinary-differential-equations', 'integral-equations']"
71,"How to solve the following system $\frac{\text{d}x}{\text{d} t} = -Ax + \frac{B}{y} - C$, $ \frac{\text{d}y}{\text{d} t} = -Dx + \frac{E}{y} - F$","How to solve the following system ,",\frac{\text{d}x}{\text{d} t} = -Ax + \frac{B}{y} - C  \frac{\text{d}y}{\text{d} t} = -Dx + \frac{E}{y} - F,"Is there a way to analytically solve the following ODE system? $$ \frac{\text{d}x}{\text{d} t} = -Ax +  B\left(\frac{1}{y} -1\right) \\ \frac{\text{d}y}{\text{d} t} = -Cx + D\left(\frac{1}{y} -1\right)  $$ Where $A,B,C,D>0$ and $x(0)=0,\ y(0)=y_0>0$. $B,D$ may also be treated as including a factor of epsilon, $\varepsilon\ll 1$, however my asymptotics hasn't gone anywhere so far.","Is there a way to analytically solve the following ODE system? $$ \frac{\text{d}x}{\text{d} t} = -Ax +  B\left(\frac{1}{y} -1\right) \\ \frac{\text{d}y}{\text{d} t} = -Cx + D\left(\frac{1}{y} -1\right)  $$ Where $A,B,C,D>0$ and $x(0)=0,\ y(0)=y_0>0$. $B,D$ may also be treated as including a factor of epsilon, $\varepsilon\ll 1$, however my asymptotics hasn't gone anywhere so far.",,['ordinary-differential-equations']
72,Defining sine and cosine via ODE's,Defining sine and cosine via ODE's,,"So I read in Simmons book on Differential Equations that via the equation y''+y=0 One can define s(x), c(x) as their solutions with some given initial conditions, that is s(0)=0 s'(0)=1 ; c(0)=1, c'(0)=0 These are of course sine and cosine, but you don't actually need to know it beforehand. Now, what the book says is that just with these information you can deduce all of the trigonometric identities, I've tried out the simplest ones: s'=c , c'= -s, s^2+c^2=1 and also that they are linearly independent, but I'm having trouble with the other identities such as: s(x+2pi)= s(x) ;  s(2x)=2s(x)c(x) ;  s(x+a)=s(x)c(a)+c(x)s(a) With pi defined as the point where s' crosses the x axis. Any help will be deeply appreciated.","So I read in Simmons book on Differential Equations that via the equation y''+y=0 One can define s(x), c(x) as their solutions with some given initial conditions, that is s(0)=0 s'(0)=1 ; c(0)=1, c'(0)=0 These are of course sine and cosine, but you don't actually need to know it beforehand. Now, what the book says is that just with these information you can deduce all of the trigonometric identities, I've tried out the simplest ones: s'=c , c'= -s, s^2+c^2=1 and also that they are linearly independent, but I'm having trouble with the other identities such as: s(x+2pi)= s(x) ;  s(2x)=2s(x)c(x) ;  s(x+a)=s(x)c(a)+c(x)s(a) With pi defined as the point where s' crosses the x axis. Any help will be deeply appreciated.",,"['ordinary-differential-equations', 'trigonometry']"
73,Solve this system of equations using elimination for $x(t)$ and $y(t)$,Solve this system of equations using elimination for  and,x(t) y(t),"I'm taking an online Differential Equations class and don't understand how to solve this system of equations using elimination. I tried the typical algebraic method but am running into trouble: $x'+y'-x=5$ $x+y'+y=1$ So $y=x'-2x-4$ $x=-5+x'+y'$ But I can't imagine this is how I leave the equation.  My guess is the equation should be in some form where I can use separation of variables or something, but I'm not entirely sure. Any help is greatly appreciated.","I'm taking an online Differential Equations class and don't understand how to solve this system of equations using elimination. I tried the typical algebraic method but am running into trouble: So But I can't imagine this is how I leave the equation.  My guess is the equation should be in some form where I can use separation of variables or something, but I'm not entirely sure. Any help is greatly appreciated.",x'+y'-x=5 x+y'+y=1 y=x'-2x-4 x=-5+x'+y',"['ordinary-differential-equations', 'systems-of-equations']"
74,Why are differential equations with sinusoidal source terms easier to solve than others?,Why are differential equations with sinusoidal source terms easier to solve than others?,,"I am a software engineer trying to wrap my tiny human brain around Fourier Transforms for a project I'm currently working on. Although I will ultimately use an open source Math library to do all the heavy lifting for me, I don't like to do anything without at least having a basic understanding of it, so I came here. My understanding is that linear differential equations with non-sinusoidal source terms are hard to solve (though I don't understand why). And so Fourier Transform helps convert these problems into several component linear differential equations with sinusoidal source terms, which are apparently easy to solve. So first, if I have misunderstood the motivation/reasoning behind the use of Fourier Transforms, please begin by correcting me! Assuming I'm more or less correct, there are a few mental blockers for me here: Does Fourier Transform only apply to linear differential equations? If so, why? How many smaller ""component"" functions (with sinusoidal source terms) does Fourier Transform produce? Moreover, why (from a 30,000 ft view) is solving something like this (don't get hung up on the specific functional definitions, I'm just providing these as straw men examples): L[y(t)] = 3t^2 + 4t + 8          // Non-sinusoidal source term harder to solve than something like this: L[y(t)] = 2sin(4*PI + t) - 20    // Sinusoidal source term ? Thanks for any-and-all clarification/help.","I am a software engineer trying to wrap my tiny human brain around Fourier Transforms for a project I'm currently working on. Although I will ultimately use an open source Math library to do all the heavy lifting for me, I don't like to do anything without at least having a basic understanding of it, so I came here. My understanding is that linear differential equations with non-sinusoidal source terms are hard to solve (though I don't understand why). And so Fourier Transform helps convert these problems into several component linear differential equations with sinusoidal source terms, which are apparently easy to solve. So first, if I have misunderstood the motivation/reasoning behind the use of Fourier Transforms, please begin by correcting me! Assuming I'm more or less correct, there are a few mental blockers for me here: Does Fourier Transform only apply to linear differential equations? If so, why? How many smaller ""component"" functions (with sinusoidal source terms) does Fourier Transform produce? Moreover, why (from a 30,000 ft view) is solving something like this (don't get hung up on the specific functional definitions, I'm just providing these as straw men examples): L[y(t)] = 3t^2 + 4t + 8          // Non-sinusoidal source term harder to solve than something like this: L[y(t)] = 2sin(4*PI + t) - 20    // Sinusoidal source term ? Thanks for any-and-all clarification/help.",,"['ordinary-differential-equations', 'soft-question', 'fourier-analysis', 'motivation', 'fourier-transform']"
75,Can we interchange derivatives when multiplied?,Can we interchange derivatives when multiplied?,,"Although this looks like a physics question, this is more of a Math question,  I was reading the Energy-Mass relationship derivation, it goes as follows, Force $F$ is given by $$F=\frac{d}{dt}(mv)$$ $$=\frac{dm}{dt}v+m\frac{dv}{dt}$$ And kinetic energy $K$ $$dK=F \cdot ds$$ So, $$dK=\frac{dm}{dt}v\cdot ds+m\frac{dv}{dt} \cdot ds$$ The next step what I don't understand $$dK=dm \cdot v \cdot \frac{ds}{dt}+m \cdot dv \cdot \frac{ds}{dt}$$ Can we really write something like for example $$\bigg(\frac{dx}{dt}dy \bigg) \space \text{as} \space \bigg (dx \frac{dy}{dt} \bigg)?$$ I dont think that is valid, or am I missing out something? Should I continue reading it or throw away the book?","Although this looks like a physics question, this is more of a Math question,  I was reading the Energy-Mass relationship derivation, it goes as follows, Force $F$ is given by $$F=\frac{d}{dt}(mv)$$ $$=\frac{dm}{dt}v+m\frac{dv}{dt}$$ And kinetic energy $K$ $$dK=F \cdot ds$$ So, $$dK=\frac{dm}{dt}v\cdot ds+m\frac{dv}{dt} \cdot ds$$ The next step what I don't understand $$dK=dm \cdot v \cdot \frac{ds}{dt}+m \cdot dv \cdot \frac{ds}{dt}$$ Can we really write something like for example $$\bigg(\frac{dx}{dt}dy \bigg) \space \text{as} \space \bigg (dx \frac{dy}{dt} \bigg)?$$ I dont think that is valid, or am I missing out something? Should I continue reading it or throw away the book?",,"['calculus', 'ordinary-differential-equations', 'derivatives', 'physics']"
76,Uniqueness of Solution for Boundary Valued Problems,Uniqueness of Solution for Boundary Valued Problems,,"Is it true that all boundary problems (ODE's n-order, PDE's, etc.) in any dimension always have an unique solution (if a solution exists)? If not, what are some counterexamples to this? Furthermore, is there a general set of restrictions that will allow the uniqueness of solution (ODE, PDE, etc.). For example, forcing both the derivative and the position function to have certain values at the boundary, or required the solution to be infinitely smooth and the space compact. Maybe it has to do with weak derivative, etc. My intuition is that the answer should be no. Suppose two non-identical solutions for the BVP exist. Then I can ""follow"" both equations ""out"" (away) from the Boundary to a point where they differ. This means the vector field points in two directions at this point, which is not allowed to happen as learned in elementary differential equation. Thank you.","Is it true that all boundary problems (ODE's n-order, PDE's, etc.) in any dimension always have an unique solution (if a solution exists)? If not, what are some counterexamples to this? Furthermore, is there a general set of restrictions that will allow the uniqueness of solution (ODE, PDE, etc.). For example, forcing both the derivative and the position function to have certain values at the boundary, or required the solution to be infinitely smooth and the space compact. Maybe it has to do with weak derivative, etc. My intuition is that the answer should be no. Suppose two non-identical solutions for the BVP exist. Then I can ""follow"" both equations ""out"" (away) from the Boundary to a point where they differ. This means the vector field points in two directions at this point, which is not allowed to happen as learned in elementary differential equation. Thank you.",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem']"
77,${d^2y}\over {dt^2}$ rather than ${dy}\over {dt}$ - what does it change$?$,rather than  - what does it change,{d^2y}\over {dt^2} {dy}\over {dt} ?,"I have the problem ""Find a positive value of $k$ for which $y = \sin(kt)$ satisfies $\frac{d^2y}{dt^2} + 9y = 0.$ Any example I can find for solving this equation either just uses $y'$ or has a typical $\frac{dy}{dt}$ So what exactly would be changed by doing this$?$","I have the problem ""Find a positive value of $k$ for which $y = \sin(kt)$ satisfies $\frac{d^2y}{dt^2} + 9y = 0.$ Any example I can find for solving this equation either just uses $y'$ or has a typical $\frac{dy}{dt}$ So what exactly would be changed by doing this$?$",,"['calculus', 'ordinary-differential-equations']"
78,Theorem about implicit solutions of differential equations?,Theorem about implicit solutions of differential equations?,,"I have a theorem in my book (in the context of separation of variables method) which I don't fully understand: Suppose $g=g(x)$ is continuous on $(a, b)$ and $h=h(y)$ is continuous on $(c, d)$. Let $G$ be an antiderivative of $g$ on $(a, b)$ and let $H$ be an antiderivative of $h$ on $(c, d)$. Let $x_0$ be an arbitrary point in $(a, b)$, and let $y_0$ be an arbitrary point in $(c, d)$ such that $h(y_0) \not = 0$, and define   $$ c=H(y_0)-G(x_0) \label{1}\tag{1} $$   Then there's a function $y=y(x)$ defined on some open interval $(a_1, b_1)$, where $a \le a_1 \lt x_0 \lt b_1 \le b$, such that $y(x_0)=y_0$ and    $$ H(y)=G(x)+c \label{2}\tag{2} $$    for $a_1<x<b_1$. Therefore $y$ is a solution of the initial value problem    \begin{align} h(y)y'&=g(x), \\  y(x_0) &=y_0 \label{3}\tag{3} \end{align}   It is convenient to say that $\eqref{2}$ with arbitrary $c$ is an implicit solution of $h(y)y'=g(x)$. If $c$ satisfies $\eqref{1}$ we'll say that $\eqref{2}$ is an implicit solution of the initial value problem $\eqref{3}$. However, keep in mind that for some choices of $c$ there may or may not be any differentiable functions that satisfy $\eqref{2}$. I don't think I really understand what this theorem is saying. My understanding of the theorem goes like this: If you have any function $g$ differentialbe somewhere and any function $h$ differentiable somewhere , then you can transform $H$ into $G$ plus a constant simply by plugging in an appropriate $y(x)$ in to $H$, which implies that you can transform any function into any other function by choosing an appropriate input. This seems wrong, so I think my interpretation is not correct here. I don't even see why this theorem is really needed. The book has been solving separation of variables problems since before this theorem by finding $H(y)$ and $G(x)$ such that $H'(y)=h(y)$ and $G'(x)=g(x)$. This is my first DE class so please try to keep the answers at an appropriate level.","I have a theorem in my book (in the context of separation of variables method) which I don't fully understand: Suppose $g=g(x)$ is continuous on $(a, b)$ and $h=h(y)$ is continuous on $(c, d)$. Let $G$ be an antiderivative of $g$ on $(a, b)$ and let $H$ be an antiderivative of $h$ on $(c, d)$. Let $x_0$ be an arbitrary point in $(a, b)$, and let $y_0$ be an arbitrary point in $(c, d)$ such that $h(y_0) \not = 0$, and define   $$ c=H(y_0)-G(x_0) \label{1}\tag{1} $$   Then there's a function $y=y(x)$ defined on some open interval $(a_1, b_1)$, where $a \le a_1 \lt x_0 \lt b_1 \le b$, such that $y(x_0)=y_0$ and    $$ H(y)=G(x)+c \label{2}\tag{2} $$    for $a_1<x<b_1$. Therefore $y$ is a solution of the initial value problem    \begin{align} h(y)y'&=g(x), \\  y(x_0) &=y_0 \label{3}\tag{3} \end{align}   It is convenient to say that $\eqref{2}$ with arbitrary $c$ is an implicit solution of $h(y)y'=g(x)$. If $c$ satisfies $\eqref{1}$ we'll say that $\eqref{2}$ is an implicit solution of the initial value problem $\eqref{3}$. However, keep in mind that for some choices of $c$ there may or may not be any differentiable functions that satisfy $\eqref{2}$. I don't think I really understand what this theorem is saying. My understanding of the theorem goes like this: If you have any function $g$ differentialbe somewhere and any function $h$ differentiable somewhere , then you can transform $H$ into $G$ plus a constant simply by plugging in an appropriate $y(x)$ in to $H$, which implies that you can transform any function into any other function by choosing an appropriate input. This seems wrong, so I think my interpretation is not correct here. I don't even see why this theorem is really needed. The book has been solving separation of variables problems since before this theorem by finding $H(y)$ and $G(x)$ such that $H'(y)=h(y)$ and $G'(x)=g(x)$. This is my first DE class so please try to keep the answers at an appropriate level.",,"['calculus', 'ordinary-differential-equations', 'implicit-differentiation']"
79,"Stable, periodic solution for $\dot x +x=f(t)$","Stable, periodic solution for",\dot x +x=f(t),"I'm having trouble on this problem from Strogatz.  Given a $T$-periodic, smooth function $f(t)$, is true that $\dot x +x=f(t)$ necessarily has a stable, $T$-periodic solution $x(t)$?  I must either prove this or provide a counter example. In my attempt to prove it, I have that the solution must be of the form $$x(t)=e^{-t} \int e^t f(t)dt$$ Can I generalize a $T$-periodic function in any way that would allow me to solve this integral in a general sense?","I'm having trouble on this problem from Strogatz.  Given a $T$-periodic, smooth function $f(t)$, is true that $\dot x +x=f(t)$ necessarily has a stable, $T$-periodic solution $x(t)$?  I must either prove this or provide a counter example. In my attempt to prove it, I have that the solution must be of the form $$x(t)=e^{-t} \int e^t f(t)dt$$ Can I generalize a $T$-periodic function in any way that would allow me to solve this integral in a general sense?",,"['ordinary-differential-equations', 'dynamical-systems']"
80,How to solve Schrödinger equation numerically with time dependent potential,How to solve Schrödinger equation numerically with time dependent potential,,"How to solve the Schrödinger equation with time dependent potential in 1D or 3D (if it is easier): $$i\hbar\dfrac{\partial \Psi}{\partial t}(x,t)=\left(-\dfrac{\hbar}{2m}\nabla^2-\frac{e^2}{x+\alpha}-exE(t)\right)\Psi(x,t)$$ where $E(t) = E_0 \exp(-t/\tau^2)sin(\omega_0 t)$ is a Gaussian pulse in time, $\alpha$ is a constant and $e$ is a constant (the electron charge). $\Psi(x,0)$ is hydrogen ground state. What would it mean to find the solution in a self consistent manner?","How to solve the Schrödinger equation with time dependent potential in 1D or 3D (if it is easier): $$i\hbar\dfrac{\partial \Psi}{\partial t}(x,t)=\left(-\dfrac{\hbar}{2m}\nabla^2-\frac{e^2}{x+\alpha}-exE(t)\right)\Psi(x,t)$$ where $E(t) = E_0 \exp(-t/\tau^2)sin(\omega_0 t)$ is a Gaussian pulse in time, $\alpha$ is a constant and $e$ is a constant (the electron charge). $\Psi(x,0)$ is hydrogen ground state. What would it mean to find the solution in a self consistent manner?",,"['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'physics', 'runge-kutta-methods']"
81,Definition of trajectory,Definition of trajectory,,"I am writing something that involves comparing the solutions of many different differential equations, and I need precise definitions of the terms trajectory and solution curve. Given a dynamical system $\dot{\textbf{x}}=F(\textbf{x})$ in $n$ dimensions, and an initial condition $\textbf{x}(t)=\textbf{0}$ Would the trajectory be a mapping from time to the phase plane? $T:\mathbb{R}\mapsto \mathbb{R}^n$? Would the trajectory be a set of points on the phase plane? Would the trajectory be a set of points in $\mathbb{R}^{n+1}$, containing both time $t$ and the vector $\textbf{x}(t)$? Also is the word trajectory a synonym of solution curve? how do they differ?","I am writing something that involves comparing the solutions of many different differential equations, and I need precise definitions of the terms trajectory and solution curve. Given a dynamical system $\dot{\textbf{x}}=F(\textbf{x})$ in $n$ dimensions, and an initial condition $\textbf{x}(t)=\textbf{0}$ Would the trajectory be a mapping from time to the phase plane? $T:\mathbb{R}\mapsto \mathbb{R}^n$? Would the trajectory be a set of points on the phase plane? Would the trajectory be a set of points in $\mathbb{R}^{n+1}$, containing both time $t$ and the vector $\textbf{x}(t)$? Also is the word trajectory a synonym of solution curve? how do they differ?",,"['ordinary-differential-equations', 'definition', 'dynamical-systems']"
82,Differential equation: $\ddot{y}(x) + \alpha\dot{y}^2(x) + \beta y(x) = 0$,Differential equation:,\ddot{y}(x) + \alpha\dot{y}^2(x) + \beta y(x) = 0,"I am interested in finding an approximate solution for this differential equation, since the exact analytic solution seems to not exist. I tried with Mathematica and it spits out nothing. $$\ddot{y}(x) + \alpha\dot{y}^2(x) + \beta y(x) = 0 ~~~~~~~ (\alpha, \beta) > 0$$ Clearly, I don't want an approximate solution like ""let's cut one of the three terms"". I was thinking about a good ansatz, but I found nothing really good. Also I got stuck while trying $$y(x) = A(x)\ e^{-kt}$$ Sounds a really ""simple"" problem but maybe it's not. Any idea?","I am interested in finding an approximate solution for this differential equation, since the exact analytic solution seems to not exist. I tried with Mathematica and it spits out nothing. $$\ddot{y}(x) + \alpha\dot{y}^2(x) + \beta y(x) = 0 ~~~~~~~ (\alpha, \beta) > 0$$ Clearly, I don't want an approximate solution like ""let's cut one of the three terms"". I was thinking about a good ansatz, but I found nothing really good. Also I got stuck while trying $$y(x) = A(x)\ e^{-kt}$$ Sounds a really ""simple"" problem but maybe it's not. Any idea?",,"['calculus', 'analysis', 'ordinary-differential-equations', 'numerical-methods']"
83,Step size in Euler's forward method,Step size in Euler's forward method,,I came across the following question. Kindly let me know if there is any generic solution to this type of question. $$ \frac{d^2 y}{dt^2} + 3 \frac{dy}{dt} + 2y = f(t) $$ Where $f(t)$ is an impulse function. What is a suitable step size for Euler forward difference method. 2 1.5 1 0.2,I came across the following question. Kindly let me know if there is any generic solution to this type of question. $$ \frac{d^2 y}{dt^2} + 3 \frac{dy}{dt} + 2y = f(t) $$ Where $f(t)$ is an impulse function. What is a suitable step size for Euler forward difference method. 2 1.5 1 0.2,,"['ordinary-differential-equations', 'numerical-methods']"
84,Asymptotic relation for the following series?,Asymptotic relation for the following series?,,"Questions Is the asymptotic relationship correct? How do I determine $c_1$ and $\kappa$? As, $|s| \to 0$ $$ \sum_{r=1}^\infty s^r \ln(r) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$ Is it possible to find $A_r$ using this relationship (for any of the r's)? Where, $$ \sum_{r=1}^\infty A_r(s) \ln(p_r) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$ Background Recently I found a technique as in a previous question: Number-theoretic asymptotic looks false but is true? This led me to another series with interesting properties: $$ K(s) = s \ln 1 + s^2 \ln 2 + s^3 \ln 3 +  \dots  =  \sum_{r=1}^\infty s^r \ln(r) $$ $$ \implies K(s) =  A_1(s) \ln 2 + A_2(s) \ln(3) + A_3(s) \ln 5 + \dots = \sum_{r=1}^\infty A_r(s) \ln(p_r) $$ Where $p_r$ is the r'th prime Now consider the following: $$ K(s) =  \sum_{r=1}^\infty s^r \ln(r) $$ $$ \implies \sum_{r=0}^\infty s^r K(s) = s \ln 1! + s^2 \ln 2! + s^3 \ln 3! + \dots $$ $$ \implies \frac{ K(s)}{1-s} = \sum_{r=1}^\infty s^r \ln r! $$ Using Stirling approximation: $$ \implies \frac{ K(s)}{1-s} = \sum_{r=1}^\infty s^r (r \ln(r) - r +\frac{1}{2}\ln(2 \pi) + \frac{1}{2} \ln r + \sum_{k=1}^\infty a_k/r^k)  $$ where $a_n$ is the coefficients of the $1/r$ error terms $$ \implies \frac{ K(s)}{1-s} = s \frac{dK}{ds} - \frac{s}{(1-s)^2} + \frac{\ln(2 \pi) s }{2(1-s)} + \frac{K}{2} + a_1 \text{Li}_1(s)  + a_2 \text{Li}_2(s)\dots $$ where $\text{Li}_r(s)$ is the $r$'th poly-logarithmic function. Taking $|s| \to 0 $ and using the asymptotic relation of $\text{Li}_r(s) \sim s$ $$ \implies \frac{ K(s)}{2} \sim s \frac{dK}{ds} - s + \frac{\ln(2 \pi) s }{2} +  \kappa s $$ whre $\kappa= \sum a_r$ and now solving the differential equation: $$ K(s) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$","Questions Is the asymptotic relationship correct? How do I determine $c_1$ and $\kappa$? As, $|s| \to 0$ $$ \sum_{r=1}^\infty s^r \ln(r) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$ Is it possible to find $A_r$ using this relationship (for any of the r's)? Where, $$ \sum_{r=1}^\infty A_r(s) \ln(p_r) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$ Background Recently I found a technique as in a previous question: Number-theoretic asymptotic looks false but is true? This led me to another series with interesting properties: $$ K(s) = s \ln 1 + s^2 \ln 2 + s^3 \ln 3 +  \dots  =  \sum_{r=1}^\infty s^r \ln(r) $$ $$ \implies K(s) =  A_1(s) \ln 2 + A_2(s) \ln(3) + A_3(s) \ln 5 + \dots = \sum_{r=1}^\infty A_r(s) \ln(p_r) $$ Where $p_r$ is the r'th prime Now consider the following: $$ K(s) =  \sum_{r=1}^\infty s^r \ln(r) $$ $$ \implies \sum_{r=0}^\infty s^r K(s) = s \ln 1! + s^2 \ln 2! + s^3 \ln 3! + \dots $$ $$ \implies \frac{ K(s)}{1-s} = \sum_{r=1}^\infty s^r \ln r! $$ Using Stirling approximation: $$ \implies \frac{ K(s)}{1-s} = \sum_{r=1}^\infty s^r (r \ln(r) - r +\frac{1}{2}\ln(2 \pi) + \frac{1}{2} \ln r + \sum_{k=1}^\infty a_k/r^k)  $$ where $a_n$ is the coefficients of the $1/r$ error terms $$ \implies \frac{ K(s)}{1-s} = s \frac{dK}{ds} - \frac{s}{(1-s)^2} + \frac{\ln(2 \pi) s }{2(1-s)} + \frac{K}{2} + a_1 \text{Li}_1(s)  + a_2 \text{Li}_2(s)\dots $$ where $\text{Li}_r(s)$ is the $r$'th poly-logarithmic function. Taking $|s| \to 0 $ and using the asymptotic relation of $\text{Li}_r(s) \sim s$ $$ \implies \frac{ K(s)}{2} \sim s \frac{dK}{ds} - s + \frac{\ln(2 \pi) s }{2} +  \kappa s $$ whre $\kappa= \sum a_r$ and now solving the differential equation: $$ K(s) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$",,"['number-theory', 'ordinary-differential-equations', 'proof-verification', 'prime-numbers', 'asymptotics']"
85,Solution of $y'=(1+x)(1+y)$,Solution of,y'=(1+x)(1+y),Differential equation: $$y'=(1+x)(1+y)$$ which is: $$y'=y(1+x)+1+x$$. Like this differential equation I try to solve by: $$y'(x)=a(x)y+b(x) $$ which solves non-homogenous differential equation. In this case a(x)=1+x and b(x)=1+x are similar. We have: $$y(x)=c(x)e^{\int a(x)dx}$$ and $$c(x)=\int b(x)e^{\int a(x)dx}dx$$ I am trying with this method but I can't get solution. What I got is: $$y(x)=e^{\frac{1}{2}x(x+2)}$$ but this doesn't work. Where is my mistake or is any better way to solve it?,Differential equation: $$y'=(1+x)(1+y)$$ which is: $$y'=y(1+x)+1+x$$. Like this differential equation I try to solve by: $$y'(x)=a(x)y+b(x) $$ which solves non-homogenous differential equation. In this case a(x)=1+x and b(x)=1+x are similar. We have: $$y(x)=c(x)e^{\int a(x)dx}$$ and $$c(x)=\int b(x)e^{\int a(x)dx}dx$$ I am trying with this method but I can't get solution. What I got is: $$y(x)=e^{\frac{1}{2}x(x+2)}$$ but this doesn't work. Where is my mistake or is any better way to solve it?,,['ordinary-differential-equations']
86,Modified Bessel differential equation,Modified Bessel differential equation,,"The modified Bessel differential equation is always presented as $$r^2 \frac{\partial^2 f(r)}{\partial r^2} + r\frac{\partial f(r)}{\partial r} - (r^2 + n^2)f(r) = 0$$ with solutions $$f(r) = AI_n(r) + BK_n(r)$$ But if I had $$r^2 \frac{\partial^2 f(r)}{\partial r^2} + r\frac{\partial f(r)}{\partial r} - (\alpha^2 r^2 + n^2)f(r) = 0$$ What form should have the solutions and how to prove it? It seems not to be a simple substitution $r' = \alpha r$, because $\alpha$ appears just one time in the second equation and not in all the $r$ terms (as I would expect instead).","The modified Bessel differential equation is always presented as $$r^2 \frac{\partial^2 f(r)}{\partial r^2} + r\frac{\partial f(r)}{\partial r} - (r^2 + n^2)f(r) = 0$$ with solutions $$f(r) = AI_n(r) + BK_n(r)$$ But if I had $$r^2 \frac{\partial^2 f(r)}{\partial r^2} + r\frac{\partial f(r)}{\partial r} - (\alpha^2 r^2 + n^2)f(r) = 0$$ What form should have the solutions and how to prove it? It seems not to be a simple substitution $r' = \alpha r$, because $\alpha$ appears just one time in the second equation and not in all the $r$ terms (as I would expect instead).",,"['calculus', 'ordinary-differential-equations', 'bessel-functions']"
87,Particular solution:$ (3+x) e^{-2x}$?,Particular solution:?, (3+x) e^{-2x},$y''+4y'+4y = (3+x) e^{-2x}$ So I'm working with undetermined coefficient and figured out solution for the left side. But what is the particular solution for the right side? I tried these but they don't work as all terms cancel to zero: $y = (Ax+B) x e^{-2x}$ $y = (Ax+B)e^{-2x}$,$y''+4y'+4y = (3+x) e^{-2x}$ So I'm working with undetermined coefficient and figured out solution for the left side. But what is the particular solution for the right side? I tried these but they don't work as all terms cancel to zero: $y = (Ax+B) x e^{-2x}$ $y = (Ax+B)e^{-2x}$,,"['ordinary-differential-equations', 'multivariable-calculus']"
88,"Existence of solution of $\frac{\partial f}{\partial t}=-\Delta f+|\nabla f|^2-R(x,t)$",Existence of solution of,"\frac{\partial f}{\partial t}=-\Delta f+|\nabla f|^2-R(x,t)","When $t=t_0$, $f(x,t)=f_0(x)\in L^2(U)$. $t\in [0,t_0]$ and  $U$ is a open subset of $R^n$.$R(x,t)$ is bounded and smooth about $x$ and $t$. I don't  whether suitable the conditions is ,if not, please correct it . How to show the existence of solution of  $\frac{\partial f}{\partial t}=-\Delta f+|\nabla f|^2-R(x,t)$ ?","When $t=t_0$, $f(x,t)=f_0(x)\in L^2(U)$. $t\in [0,t_0]$ and  $U$ is a open subset of $R^n$.$R(x,t)$ is bounded and smooth about $x$ and $t$. I don't  whether suitable the conditions is ,if not, please correct it . How to show the existence of solution of  $\frac{\partial f}{\partial t}=-\Delta f+|\nabla f|^2-R(x,t)$ ?",,"['analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'systems-of-equations']"
89,General solution of differntial equation $(x^2)(y)dx = ( (x^3) + (y^3))dy$,General solution of differntial equation,(x^2)(y)dx = ( (x^3) + (y^3))dy,"I tried this problem by many ways , i took $dy$ to other side and tried to form bernaulli equation but it is not helping and not giving me proper answer .","I tried this problem by many ways , i took $dy$ to other side and tried to form bernaulli equation but it is not helping and not giving me proper answer .",,['ordinary-differential-equations']
90,Wronskian zero with linearly independent solutions,Wronskian zero with linearly independent solutions,,"Any ideas how to go about proving this? Functions $\phi(x)$ and $\psi(x)$ are linearly independent on the interval $[\alpha,\beta]$, but their Wronskian $W(\phi,\psi)=0$ for some $x\in [\alpha, \beta]$. Prove that exist at least two points $x_1,x_2 \in [\alpha, \beta]$ such that $\phi(x_1)=\psi′(x_2)=0$. I've naturally started with considering the point at which the Wronskian vanishes, say $\phi(x_0)\psi'(x_0)-\phi'(x_0)\psi(x_0)=0$ and then took them to each side and arrive at the logs being equal when evaluated at x0 but I'm not sure if this makes sense and if it doesn, where to go from there? I'm not sure how the linear independence comes in to play? Should I'm be attempting this at a higher level rather than constructively? Any help would be greatly appreciated!","Any ideas how to go about proving this? Functions $\phi(x)$ and $\psi(x)$ are linearly independent on the interval $[\alpha,\beta]$, but their Wronskian $W(\phi,\psi)=0$ for some $x\in [\alpha, \beta]$. Prove that exist at least two points $x_1,x_2 \in [\alpha, \beta]$ such that $\phi(x_1)=\psi′(x_2)=0$. I've naturally started with considering the point at which the Wronskian vanishes, say $\phi(x_0)\psi'(x_0)-\phi'(x_0)\psi(x_0)=0$ and then took them to each side and arrive at the logs being equal when evaluated at x0 but I'm not sure if this makes sense and if it doesn, where to go from there? I'm not sure how the linear independence comes in to play? Should I'm be attempting this at a higher level rather than constructively? Any help would be greatly appreciated!",,"['ordinary-differential-equations', 'wronskian']"
91,"Hamiltonian question, find optimal controller, simple question","Hamiltonian question, find optimal controller, simple question",,"An object is moving with accordance to Newton's laws: $\begin{pmatrix} \dot{y} \\ \dot {v} \end{pmatrix} = \begin{pmatrix} v \\ u \end{pmatrix}$ where $y$ is the objects location and $v$ is its speed. Let $x=\begin{pmatrix} y \\ v \end{pmatrix}$ be the state variable. Assume our input controller (the acceleration) $u$ is such that $|u| \leq 1$. Using Jacobi-Hamilton (Hamiltonian) method, find an optimal controller that shifts $y(0),v(0)$ to the origin $(0,0)$ in minimal time $T$. What I did: Our cost function is $J(0)=\int_{0}^{T}1d\tau$ and our state equation is $\dot{x} = \begin{pmatrix} \dot{y} \\ \dot {v} \end{pmatrix} = \begin{pmatrix} v \\ u \end{pmatrix}$ And so our Hamiltonian function is $H(x,u,\lambda) = 1+\lambda^ T\dot{x}$ and if $\lambda = \begin{pmatrix} \lambda _1 \\ \lambda _2 \end{pmatrix}$ then $H=1+\lambda _1 v+\lambda _2 u$ Please note - This is also the result the teacher reached. So I doubt this is incorrect. The method says that $\dot{x} = H_\lambda$, $\dot {\lambda} = -H_x$, and $H_u = 0$. From the first equation we get $\dot{x} =\begin{pmatrix} \dot{y} \\ \dot {v} \end{pmatrix}=\begin{pmatrix} v \\ u \end{pmatrix}$ which we already know and is trivial. From the second equation (I think) we get $\dot {\lambda}  = \begin{pmatrix} \dot{\lambda _1 } \\ \dot {\lambda _2} \end{pmatrix}=-\begin{pmatrix} 0 \\ \lambda _1 \end{pmatrix}$ which is also problematic by itself since this is not solvable, but this isn't my main problem. My main problem is with $H_u=0$. this means that $\lambda _2 =0$. So where does the $u$ part come in? I'm supposed to find an optimal controller $u$, but nothing is dependent on $u$. it does not appear anywhere. Would appreciate any help, I can't understand the teachers solution either.","An object is moving with accordance to Newton's laws: $\begin{pmatrix} \dot{y} \\ \dot {v} \end{pmatrix} = \begin{pmatrix} v \\ u \end{pmatrix}$ where $y$ is the objects location and $v$ is its speed. Let $x=\begin{pmatrix} y \\ v \end{pmatrix}$ be the state variable. Assume our input controller (the acceleration) $u$ is such that $|u| \leq 1$. Using Jacobi-Hamilton (Hamiltonian) method, find an optimal controller that shifts $y(0),v(0)$ to the origin $(0,0)$ in minimal time $T$. What I did: Our cost function is $J(0)=\int_{0}^{T}1d\tau$ and our state equation is $\dot{x} = \begin{pmatrix} \dot{y} \\ \dot {v} \end{pmatrix} = \begin{pmatrix} v \\ u \end{pmatrix}$ And so our Hamiltonian function is $H(x,u,\lambda) = 1+\lambda^ T\dot{x}$ and if $\lambda = \begin{pmatrix} \lambda _1 \\ \lambda _2 \end{pmatrix}$ then $H=1+\lambda _1 v+\lambda _2 u$ Please note - This is also the result the teacher reached. So I doubt this is incorrect. The method says that $\dot{x} = H_\lambda$, $\dot {\lambda} = -H_x$, and $H_u = 0$. From the first equation we get $\dot{x} =\begin{pmatrix} \dot{y} \\ \dot {v} \end{pmatrix}=\begin{pmatrix} v \\ u \end{pmatrix}$ which we already know and is trivial. From the second equation (I think) we get $\dot {\lambda}  = \begin{pmatrix} \dot{\lambda _1 } \\ \dot {\lambda _2} \end{pmatrix}=-\begin{pmatrix} 0 \\ \lambda _1 \end{pmatrix}$ which is also problematic by itself since this is not solvable, but this isn't my main problem. My main problem is with $H_u=0$. this means that $\lambda _2 =0$. So where does the $u$ part come in? I'm supposed to find an optimal controller $u$, but nothing is dependent on $u$. it does not appear anywhere. Would appreciate any help, I can't understand the teachers solution either.",,"['linear-algebra', 'ordinary-differential-equations', 'optimization', 'dynamical-systems', 'control-theory']"
92,Separating the Variables Solution,Separating the Variables Solution,,"Problem: Solve the initial value problem  $$y′ = \frac{1+3x^2}{3y^2 −6y},\quad y(0) = 1$$ and determine the interval in which the solution is valid. Hint: To find the interval of definition, look for points where the integral curve has a vertical tangent. Hi! I have this problem from one of my homework's and I am stuck on the part of determining the interval. I have already separated the variables and found C using the initial condition and got: y^3 - 3y^2 = x + x^3 -2. Now I need to determine the interval for the solution. I tried to understand the hint but can't really figure what they mean by looking for points where the integral curve has a vertical tangent. Do I accomplish this by setting: dx/dy = 0 Thanks for the help!","Problem: Solve the initial value problem  $$y′ = \frac{1+3x^2}{3y^2 −6y},\quad y(0) = 1$$ and determine the interval in which the solution is valid. Hint: To find the interval of definition, look for points where the integral curve has a vertical tangent. Hi! I have this problem from one of my homework's and I am stuck on the part of determining the interval. I have already separated the variables and found C using the initial condition and got: y^3 - 3y^2 = x + x^3 -2. Now I need to determine the interval for the solution. I tried to understand the hint but can't really figure what they mean by looking for points where the integral curve has a vertical tangent. Do I accomplish this by setting: dx/dy = 0 Thanks for the help!",,['ordinary-differential-equations']
93,Harmonic Oscillators: Differential Equations,Harmonic Oscillators: Differential Equations,,"The book being used for this course is Differential Equations, Dynamical Systems, and an Introduction to Chaos by Morris W. Hirsch. The question is as follows. Suppose there are two masses $m_1$ and $m_2$ attached to springs and walls. The springs connecting $m_j$ to the walls both have spring constants $k_1$, while the springs connecting $m_1$ and $m_2$ has spring constant $k_2$. This coupling means that the motion of either mass affects the behavior of the other. Let $x_j$ denote the displacement of each mass from its rest position, and assume that both masses are equal to 1. The differential equation for these coupled oscillators are then given by   \begin{eqnarray} x_1^{""}& = & -(k_1+k_2)x_1+k_2x_2\\ x_2^{""}& = & k_2x_1-(k_1+k_2)x_2\\ \end{eqnarray} Write these equations as a first-order linear system. Determine the eigenvalues and eigenvectors of the corresponding matrix. Find the general solution. Let $\omega_1= \sqrt{k_1}$ and $\omega_2=\sqrt{k_1+2k_2}$. What can be said about the periodicity  of solutions relative to the $\omega_j$? Prove this. Now I just want to focus on the first part: to see if my linear system is correct. I first start by introducing the new variable $y_j=x^{'}_j$ for $j = 1,2$, so that the equations can be written as a system.  \begin{eqnarray} x^{'}_1 & =& y_1\\ y^{'}_1 & =& -(k_1+k_2)x_1+k_2x_2\\ x^{'}_2 & =& y_2\\ y^{'}_2 & =& k_2x_1-(k_1+k_2)x_2  \end{eqnarray} In matrix form, this system is $X^{'}=AX$ where $X=(x_1,y_1,x_2,y_2)$ and  \begin{align} A & = & \begin{bmatrix} 0 & 1&0 &0 \\ -(k_1+k_2)& 0& k_2& 0\\ 0& 0& 0& 1\\ k_2& 0& -(k_1+k_2)&0\\ \end{bmatrix} \end{align} Do I have the right linear system? Thanks for your time.","The book being used for this course is Differential Equations, Dynamical Systems, and an Introduction to Chaos by Morris W. Hirsch. The question is as follows. Suppose there are two masses $m_1$ and $m_2$ attached to springs and walls. The springs connecting $m_j$ to the walls both have spring constants $k_1$, while the springs connecting $m_1$ and $m_2$ has spring constant $k_2$. This coupling means that the motion of either mass affects the behavior of the other. Let $x_j$ denote the displacement of each mass from its rest position, and assume that both masses are equal to 1. The differential equation for these coupled oscillators are then given by   \begin{eqnarray} x_1^{""}& = & -(k_1+k_2)x_1+k_2x_2\\ x_2^{""}& = & k_2x_1-(k_1+k_2)x_2\\ \end{eqnarray} Write these equations as a first-order linear system. Determine the eigenvalues and eigenvectors of the corresponding matrix. Find the general solution. Let $\omega_1= \sqrt{k_1}$ and $\omega_2=\sqrt{k_1+2k_2}$. What can be said about the periodicity  of solutions relative to the $\omega_j$? Prove this. Now I just want to focus on the first part: to see if my linear system is correct. I first start by introducing the new variable $y_j=x^{'}_j$ for $j = 1,2$, so that the equations can be written as a system.  \begin{eqnarray} x^{'}_1 & =& y_1\\ y^{'}_1 & =& -(k_1+k_2)x_1+k_2x_2\\ x^{'}_2 & =& y_2\\ y^{'}_2 & =& k_2x_1-(k_1+k_2)x_2  \end{eqnarray} In matrix form, this system is $X^{'}=AX$ where $X=(x_1,y_1,x_2,y_2)$ and  \begin{align} A & = & \begin{bmatrix} 0 & 1&0 &0 \\ -(k_1+k_2)& 0& k_2& 0\\ 0& 0& 0& 1\\ k_2& 0& -(k_1+k_2)&0\\ \end{bmatrix} \end{align} Do I have the right linear system? Thanks for your time.",,"['ordinary-differential-equations', 'harmonic-analysis']"
94,"Solve the differential equation: $dy/dx=\sqrt y,\ y(0)=0$",Solve the differential equation:,"dy/dx=\sqrt y,\ y(0)=0",I am trying to solve this. But I see that this equation can not satisfy the LIPSCHITZ condition in the interval containing 0.  Can this be solved by separation of variables?,I am trying to solve this. But I see that this equation can not satisfy the LIPSCHITZ condition in the interval containing 0.  Can this be solved by separation of variables?,,['ordinary-differential-equations']
95,Differentiation of matrix exponential,Differentiation of matrix exponential,,"I'm with the following problem and I'd like to know if my answer makes sense. I'd appreciate any suggestion to improve it. Thanks :) Let $X(t)= \int_0^tA(t) dt$, where $A(t)$ is a continuous $n\times n$ matrix. Show that in general we can't conclude  \begin{align*}\frac{d}{dt} \exp(X(t))\not=A(t)\exp(X(t)) \tag{*}\end{align*}   But if $[A(t),A(s)]=0$ for all $t$ and $s$ in $\mathbb{R}$, where $[\cdot,\cdot]$ is the commutator, then the equality in $(*)$ is true. Since $\exp(X(t))$ is absolutely and uniform convergent for any $t$, we can develop in a power series expansion and the derivative is element by element. Thus \begin{align*}\frac{d}{dt} \exp(X(t))= A(t)+\frac{1}{2}\big(A(t)X(t)+X(t)A(t)\big)+\frac{1}{3!}\big(A(t)X^2(t)+X(t)A(t)X(t)+X^2(t)A(t)\big)+\ldots+\end{align*} and not necessarily $[A(t),X(t)]=0$ for all $t$, therefore the equality in $(*)$ does not hold for all matrices. Now suppose that $[A(t),A(s)]=0$ for all $t$ and $s$ in $\mathbb{R}$. We will show that $[A(t),X(t)]=0$. Let $\mathscr{P}$ a tagged partition of the subinterval $[0,t]$, so let $\{a_k\}_{k=0}^n$ a finite sequence of points which are a partition of $[0,t]$ and let  $\{t_k\}_{k=1}^n$ be the tags of $\mathscr{P}$, that is, $a_{i-1}\le t_i\le a_i$ holds for each $i=1,\ldots, n$. Then we will show that $$A(t)\mathscr{R}(A(t),\mathscr{P})=\mathscr{R}(A(t),\mathscr{P})A(t)$$ where $\mathscr{R}(A(t),\mathscr{P})$ is the Riemann sum corresponding to $A(t)$ and the  partition $\mathscr{P}$. Then \begin{align*}A(t)\sum_{k=1}^n\bigg( (a_k-a_{k-1}) A(t_k)\bigg)&=\sum_{k=1}^n \bigg((a_k-a_{k-1}) A(t)A(t_k)\bigg)\\ &=\sum_{k=1}^n\bigg( (a_k-a_{k-1}) A(t_k)A(t)\bigg)=\sum_{k=1}^n\bigg( (a_k-a_{k-1}) A(t_k)\bigg)A(t) \end{align*} Now let $\mathscr{P}$ a tagged partition of $[0,t]$, such that $\|\mathscr{R}(A(t),\mathscr{P})-X(t)\|<\epsilon$, where $\|\cdot \|$ is the operator norm. Then \begin{align*}\|A(t)X(t)-X(t)A(t)\|&\le \|A(X-\mathscr{R})\|+\|\mathscr{R}A-A\mathscr{R}\|+\|(\mathscr{R}-X)A\|\\ &\le \|A\| \|X-\mathscr{R}\|+\|\mathscr{R}-A\|\|A\|\\ &<2\|A\|\epsilon\end{align*} Letting $\epsilon \downarrow 0$, gives us $[A(t),X(t)]=0$ as desired.Then using the above result, is clear that the equality on $(*)$ holds.","I'm with the following problem and I'd like to know if my answer makes sense. I'd appreciate any suggestion to improve it. Thanks :) Let $X(t)= \int_0^tA(t) dt$, where $A(t)$ is a continuous $n\times n$ matrix. Show that in general we can't conclude  \begin{align*}\frac{d}{dt} \exp(X(t))\not=A(t)\exp(X(t)) \tag{*}\end{align*}   But if $[A(t),A(s)]=0$ for all $t$ and $s$ in $\mathbb{R}$, where $[\cdot,\cdot]$ is the commutator, then the equality in $(*)$ is true. Since $\exp(X(t))$ is absolutely and uniform convergent for any $t$, we can develop in a power series expansion and the derivative is element by element. Thus \begin{align*}\frac{d}{dt} \exp(X(t))= A(t)+\frac{1}{2}\big(A(t)X(t)+X(t)A(t)\big)+\frac{1}{3!}\big(A(t)X^2(t)+X(t)A(t)X(t)+X^2(t)A(t)\big)+\ldots+\end{align*} and not necessarily $[A(t),X(t)]=0$ for all $t$, therefore the equality in $(*)$ does not hold for all matrices. Now suppose that $[A(t),A(s)]=0$ for all $t$ and $s$ in $\mathbb{R}$. We will show that $[A(t),X(t)]=0$. Let $\mathscr{P}$ a tagged partition of the subinterval $[0,t]$, so let $\{a_k\}_{k=0}^n$ a finite sequence of points which are a partition of $[0,t]$ and let  $\{t_k\}_{k=1}^n$ be the tags of $\mathscr{P}$, that is, $a_{i-1}\le t_i\le a_i$ holds for each $i=1,\ldots, n$. Then we will show that $$A(t)\mathscr{R}(A(t),\mathscr{P})=\mathscr{R}(A(t),\mathscr{P})A(t)$$ where $\mathscr{R}(A(t),\mathscr{P})$ is the Riemann sum corresponding to $A(t)$ and the  partition $\mathscr{P}$. Then \begin{align*}A(t)\sum_{k=1}^n\bigg( (a_k-a_{k-1}) A(t_k)\bigg)&=\sum_{k=1}^n \bigg((a_k-a_{k-1}) A(t)A(t_k)\bigg)\\ &=\sum_{k=1}^n\bigg( (a_k-a_{k-1}) A(t_k)A(t)\bigg)=\sum_{k=1}^n\bigg( (a_k-a_{k-1}) A(t_k)\bigg)A(t) \end{align*} Now let $\mathscr{P}$ a tagged partition of $[0,t]$, such that $\|\mathscr{R}(A(t),\mathscr{P})-X(t)\|<\epsilon$, where $\|\cdot \|$ is the operator norm. Then \begin{align*}\|A(t)X(t)-X(t)A(t)\|&\le \|A(X-\mathscr{R})\|+\|\mathscr{R}A-A\mathscr{R}\|+\|(\mathscr{R}-X)A\|\\ &\le \|A\| \|X-\mathscr{R}\|+\|\mathscr{R}-A\|\|A\|\\ &<2\|A\|\epsilon\end{align*} Letting $\epsilon \downarrow 0$, gives us $[A(t),X(t)]=0$ as desired.Then using the above result, is clear that the equality on $(*)$ holds.",,"['ordinary-differential-equations', 'proof-verification']"
96,time derivative of the Jacobian matrix of a nonlinear transformation,time derivative of the Jacobian matrix of a nonlinear transformation,,"I have a dynamical system whose state is a vector $\mathbf{y}\in \mathbf{R}^m$. The vectors $\mathbf{y},\dot{\mathbf{y}},\ddot{\mathbf{y}}\in \mathbf{R}^m$ relate to the vectors $\mathbf{z},\dot{\mathbf{z}},\ddot{\mathbf{z}}\in \mathbf{R}^n$ through the transformations \begin{align} \mathbf{y}&=g(\mathbf{z})\\ \dot{\mathbf{y}}&=J(\mathbf{z})\dot{\mathbf{z}}\\ \ddot{\mathbf{y}}&=J(\mathbf{z})\ddot{\mathbf{z}} + \dot{J}(\mathbf{z})\dot{\mathbf{z}} \end{align} where $J$ is the Jacobian matrix of the nonlinear transformation $g$. What are the entries of $\dot{J}(\mathbf{z})$? For example, if $J_{ij}=\frac{\partial y_i}{\partial z_j}=a\sin(z_j)$, then what is $\dot{J}_{ij}$? (There is a similar but unanswered question here .)","I have a dynamical system whose state is a vector $\mathbf{y}\in \mathbf{R}^m$. The vectors $\mathbf{y},\dot{\mathbf{y}},\ddot{\mathbf{y}}\in \mathbf{R}^m$ relate to the vectors $\mathbf{z},\dot{\mathbf{z}},\ddot{\mathbf{z}}\in \mathbf{R}^n$ through the transformations \begin{align} \mathbf{y}&=g(\mathbf{z})\\ \dot{\mathbf{y}}&=J(\mathbf{z})\dot{\mathbf{z}}\\ \ddot{\mathbf{y}}&=J(\mathbf{z})\ddot{\mathbf{z}} + \dot{J}(\mathbf{z})\dot{\mathbf{z}} \end{align} where $J$ is the Jacobian matrix of the nonlinear transformation $g$. What are the entries of $\dot{J}(\mathbf{z})$? For example, if $J_{ij}=\frac{\partial y_i}{\partial z_j}=a\sin(z_j)$, then what is $\dot{J}_{ij}$? (There is a similar but unanswered question here .)",,"['ordinary-differential-equations', 'derivatives', 'partial-derivative']"
97,Find the Integrating Factor of $xdy-3ydx=\frac{x^4}{y}dy$,Find the Integrating Factor of,xdy-3ydx=\frac{x^4}{y}dy,"This is integrating factor by inspection, $xdy-3ydx=\frac{x^4}{y}dy$ I've been trying to look for the Integrating factor for this problem but I can't still get one right. I think I really need to use the $3ydx$ in the problem since it has the $dx$ but i can't remove the $-3$ any ideas how?","This is integrating factor by inspection, $xdy-3ydx=\frac{x^4}{y}dy$ I've been trying to look for the Integrating factor for this problem but I can't still get one right. I think I really need to use the $3ydx$ in the problem since it has the $dx$ but i can't remove the $-3$ any ideas how?",,['ordinary-differential-equations']
98,PDE: solving Fokker-Planck equation with initial and boundary condition,PDE: solving Fokker-Planck equation with initial and boundary condition,,"Here is the problem. We have the following simple PDE: \begin{equation} \frac{\partial p(x,t)}{\partial t}= - a\frac{\partial p(x,t)}{\partial x} + \frac{D}{2} \frac{ \partial^2 p(x,t) }{\partial x^2}, \quad0<x<L, \quad t>0   \end{equation}  where $a$ and $D$ are constants. We also  have the following initial and boundary conditions:  \begin{equation} p(x,0)=0\\ a p(x,t)- \frac{D}{2} \frac{\partial p(x,t)}{\partial x} = f(t) \quad at \quad x=0 \\ \frac{\partial^2 p(x,t)}{\partial x^2}=0 \quad at \quad x=L \end{equation} The expression, $\frac{\partial^2 p(x,t)}{\partial x^2}=0$, is not a standard well-known boundary condition and makes the PDE difficult to solve. I would be grateful if anyone have any idea for solving this PDE.","Here is the problem. We have the following simple PDE: \begin{equation} \frac{\partial p(x,t)}{\partial t}= - a\frac{\partial p(x,t)}{\partial x} + \frac{D}{2} \frac{ \partial^2 p(x,t) }{\partial x^2}, \quad0<x<L, \quad t>0   \end{equation}  where $a$ and $D$ are constants. We also  have the following initial and boundary conditions:  \begin{equation} p(x,0)=0\\ a p(x,t)- \frac{D}{2} \frac{\partial p(x,t)}{\partial x} = f(t) \quad at \quad x=0 \\ \frac{\partial^2 p(x,t)}{\partial x^2}=0 \quad at \quad x=L \end{equation} The expression, $\frac{\partial^2 p(x,t)}{\partial x^2}=0$, is not a standard well-known boundary condition and makes the PDE difficult to solve. I would be grateful if anyone have any idea for solving this PDE.",,"['ordinary-differential-equations', 'maple']"
99,Flow of time-dependent vector field,Flow of time-dependent vector field,,"Suppose $X_t$ is a time-dependent vector field, with flow $\phi_t$ . Thus, $$\frac{d}{dt} \phi_t = X_t(\phi_t).$$ Is it true that $$d \phi_t\big(X_t(x)\big) = X_t\big(\phi_t(x)\big)\;?$$ This is true when $X_t$ does not depend on $t$ .","Suppose is a time-dependent vector field, with flow . Thus, Is it true that This is true when does not depend on .",X_t \phi_t \frac{d}{dt} \phi_t = X_t(\phi_t). d \phi_t\big(X_t(x)\big) = X_t\big(\phi_t(x)\big)\;? X_t t,"['ordinary-differential-equations', 'differential-geometry', 'vector-fields']"
