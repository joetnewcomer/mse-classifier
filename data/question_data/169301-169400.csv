,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to prove P(A ∩ B ∩ C) = P(A|B ∩ C)P(B|C)P(C)?,How to prove P(A ∩ B ∩ C) = P(A|B ∩ C)P(B|C)P(C)?,,"How to prove P(A∩B∩C) = P(A|B∩C)P(B|C)P(C)? I've tried to approach this a couple times, but always end up getting stuck :( Any help is appreciated! :)","How to prove P(A∩B∩C) = P(A|B∩C)P(B|C)P(C)? I've tried to approach this a couple times, but always end up getting stuck :( Any help is appreciated! :)",,['statistics']
1,Finding an unbiased estimator for $\mu^2$,Finding an unbiased estimator for,\mu^2,"I'm currently studying my first course in statistics. I am going through some question and have come across one that has stumped me. The question is as follows: Suppose $\bar{X}_1, \ldots, \bar{X}_n$ are $n$ identically distributed independent random variables each with mean $\mu$ and variance $1$. Find an unbiased estimator for $\mu^2$. To my understanding an unbiased estimator for $\mu^2$ is one such that $E[\text{?}] = \ldots = \mu^2$. However, I don't know how to go about solving for $?$ Any help or hints will be greatly appreciated. Thank you for your time.","I'm currently studying my first course in statistics. I am going through some question and have come across one that has stumped me. The question is as follows: Suppose $\bar{X}_1, \ldots, \bar{X}_n$ are $n$ identically distributed independent random variables each with mean $\mu$ and variance $1$. Find an unbiased estimator for $\mu^2$. To my understanding an unbiased estimator for $\mu^2$ is one such that $E[\text{?}] = \ldots = \mu^2$. However, I don't know how to go about solving for $?$ Any help or hints will be greatly appreciated. Thank you for your time.",,['statistics']
2,"Central Limit Theorem, why $n \ge 30$?","Central Limit Theorem, why ?",n \ge 30,"This is what I think the technical statement of CLT is: If we consider $\overline{X}_{n}$ coming from a sample of $\mathcal n$ independent and identically distributed random variables $X_{i}$ with mean $\mu$ and variance $\sigma^{2}$ then as n $\rightarrow \infty$ $\sqrt n \left( \left(\frac{1}{n} \ \sum_{i=1}^{n} X_{i} \right) - \mu \right) \approx N(0,\sigma^{2})$ Question: I've read textbooks which claim $n \ge 30$ without offering any support for the claim, why $n \ge 30$?","This is what I think the technical statement of CLT is: If we consider $\overline{X}_{n}$ coming from a sample of $\mathcal n$ independent and identically distributed random variables $X_{i}$ with mean $\mu$ and variance $\sigma^{2}$ then as n $\rightarrow \infty$ $\sqrt n \left( \left(\frac{1}{n} \ \sum_{i=1}^{n} X_{i} \right) - \mu \right) \approx N(0,\sigma^{2})$ Question: I've read textbooks which claim $n \ge 30$ without offering any support for the claim, why $n \ge 30$?",,"['statistics', 'normal-distribution']"
3,"How can we show that ""almost surely"" equal random variables have the same distribution?","How can we show that ""almost surely"" equal random variables have the same distribution?",,"How can we show that ""almost surely"" equal random variables have the same distribution? We know  $X =\text{(a.s)} Y$. What I have so far: $$\begin{align*}\implies& P(X = Y) = 1 \\ \implies& P(X(c) = Y(c)) = 1&(\text{for all }c)\\ \implies&P(X \in A) = P(Y \in A),\end{align*}$$ where $A$ is the set $X(c).$ I don't know if I can just make the above statement though. Am I missing something?","How can we show that ""almost surely"" equal random variables have the same distribution? We know  $X =\text{(a.s)} Y$. What I have so far: $$\begin{align*}\implies& P(X = Y) = 1 \\ \implies& P(X(c) = Y(c)) = 1&(\text{for all }c)\\ \implies&P(X \in A) = P(Y \in A),\end{align*}$$ where $A$ is the set $X(c).$ I don't know if I can just make the above statement though. Am I missing something?",,"['statistics', 'proof-verification', 'random-variables']"
4,Deriving probability of exactly one event occuring,Deriving probability of exactly one event occuring,,"Show that for any events A and B, the probability that exactly one of them occur is   Pr(A) + Pr(B) − 2 Pr(A ∩ B). Solution: The probability that exactly one event occurs is $$ Pr ((A \cap B^c) \cup (A^c ∩ B)) = Pr(A ∩ B^c) + Pr(A^c ∩ B) \\ = (Pr(A) − Pr(A ∩ B)) + (Pr(B)− Pr(A ∩ B))\\ = Pr(A) + Pr(B) − 2 Pr(A ∩ B).$$ I am going through this question and I don't understand how $$P(A \cap B^c) = (Pr(A) − Pr(A ∩ B))$$ and $$P(A^c \cap B) = (Pr(B) − Pr(A ∩ B))$$ If someone could explain this to me, I would be very happy. Note: I understand why this works with a venn diagram, but I can't derive its formula...","Show that for any events A and B, the probability that exactly one of them occur is   Pr(A) + Pr(B) − 2 Pr(A ∩ B). Solution: The probability that exactly one event occurs is $$ Pr ((A \cap B^c) \cup (A^c ∩ B)) = Pr(A ∩ B^c) + Pr(A^c ∩ B) \\ = (Pr(A) − Pr(A ∩ B)) + (Pr(B)− Pr(A ∩ B))\\ = Pr(A) + Pr(B) − 2 Pr(A ∩ B).$$ I am going through this question and I don't understand how $$P(A \cap B^c) = (Pr(A) − Pr(A ∩ B))$$ and $$P(A^c \cap B) = (Pr(B) − Pr(A ∩ B))$$ If someone could explain this to me, I would be very happy. Note: I understand why this works with a venn diagram, but I can't derive its formula...",,"['probability', 'statistics', 'axioms']"
5,Value of constant k which makes the function $f(x)=\frac{k|x|}{(1+|x|)^4}$ a p.d.f.,Value of constant k which makes the function  a p.d.f.,f(x)=\frac{k|x|}{(1+|x|)^4},"Let $f(x)=\dfrac{k|x|}{(1+|x|)^4}$, $-\infty<x<\infty$. Then, what is the value for which f(x) is a probability density function ? f(x) will be a p.d.f. if $\displaystyle \int_{-\infty}^{\infty}f(x)dx=1$ $\displaystyle \int_{-\infty}^{\infty}\dfrac{k|x|}{(1+|x|)^4}dx=1$ |x| is an even function,so:$\qquad$ $\displaystyle 2\int_{0}^{\infty}\dfrac{kx}{(1+x)^4}dx=1$ How do I solve this? Please help.","Let $f(x)=\dfrac{k|x|}{(1+|x|)^4}$, $-\infty<x<\infty$. Then, what is the value for which f(x) is a probability density function ? f(x) will be a p.d.f. if $\displaystyle \int_{-\infty}^{\infty}f(x)dx=1$ $\displaystyle \int_{-\infty}^{\infty}\dfrac{k|x|}{(1+|x|)^4}dx=1$ |x| is an even function,so:$\qquad$ $\displaystyle 2\int_{0}^{\infty}\dfrac{kx}{(1+x)^4}dx=1$ How do I solve this? Please help.",,"['calculus', 'integration', 'statistics', 'probability-theory', 'probability-distributions']"
6,Random Sample vs Simple Random Sample,Random Sample vs Simple Random Sample,,"I am reading, just for fun, the book Essentials of Statististics of Mario Triola. I am trying to see the differences between Random Sample and Simple Random Sample. In the book I found these definitions: ""A simple random sample of n subjects is selected in a such way that every possible sample of the same size n has the same chance of being selected. In a random sample members from the population are selected in a such way that each individual member in the population has an equal chance of being selected."" I believe, but I am not sure, that in the random sample we need to be careful that the sample represent races, ages, economical situation, geographical location but in the simple random sample we do not consider that. Am I correct?","I am reading, just for fun, the book Essentials of Statististics of Mario Triola. I am trying to see the differences between Random Sample and Simple Random Sample. In the book I found these definitions: ""A simple random sample of n subjects is selected in a such way that every possible sample of the same size n has the same chance of being selected. In a random sample members from the population are selected in a such way that each individual member in the population has an equal chance of being selected."" I believe, but I am not sure, that in the random sample we need to be careful that the sample represent races, ages, economical situation, geographical location but in the simple random sample we do not consider that. Am I correct?",,"['probability', 'statistics']"
7,Standard deviation of $Z = 9 - 3Y^3$,Standard deviation of,Z = 9 - 3Y^3,If Y is a random variable with probability mass function: Y | Pr(Y = y) -1 | 0.4  0 | 0.5  1 | 0.1 I need to find the standard deviation of $Z = 9 - 3Y^3$. Approach: since it's the variance I know that 9 should not affect it and I essentially have $3^2 Var(Y^3)$ using one of the identities.  My understanding is that I should be able to use the delta method to find $Var(Y^3)$ but I am not sure how.  I don't think I can directly use the identity $Var(X) = E[X^2] - E[X]^2$ can I?,If Y is a random variable with probability mass function: Y | Pr(Y = y) -1 | 0.4  0 | 0.5  1 | 0.1 I need to find the standard deviation of $Z = 9 - 3Y^3$. Approach: since it's the variance I know that 9 should not affect it and I essentially have $3^2 Var(Y^3)$ using one of the identities.  My understanding is that I should be able to use the delta method to find $Var(Y^3)$ but I am not sure how.  I don't think I can directly use the identity $Var(X) = E[X^2] - E[X]^2$ can I?,,"['statistics', 'standard-deviation']"
8,"Let $X \sim \text{HGeom}(w,b,n)$, what is the distribution of $n-X$?","Let , what is the distribution of ?","X \sim \text{HGeom}(w,b,n) n-X","Let $X \sim \text{HGeom}(w,b,n)$, what is the distribution of $n-X$? The distribution of $X$ (e.g., number of white ($w$) balls in a sample of size $n$) is hypergeometric, so $$P(X=x) = \frac{\binom{w}{x}\binom{b}{n-x}}{\binom{w+b}{n}}.$$ If $X$ is the number of white balls, then $n-X$ must be the number of black balls. Let $Y=n-X$, then $P(Y=y)=P(n-X=y)=P(X=n-y)$, so plugging in, I get $$P(Y=y) = \frac{\binom{w}{n-y}\binom{b}{y}}{\binom{w+b}{n}}$$ Is this a correct proof (not only if the results is correct)? What kind of manipulations are actually allowed within the $P(\dots)$? I'm always tempted to do some stuff like $P(Y=y)=P(n-X=n-x)=P(X=x)$, which is obviously meaningless, but I can't figure out what is formally wrong with it. Why is it allowed to plug in $n-X$ for $Y$, but not $n-x$ for $y$?","Let $X \sim \text{HGeom}(w,b,n)$, what is the distribution of $n-X$? The distribution of $X$ (e.g., number of white ($w$) balls in a sample of size $n$) is hypergeometric, so $$P(X=x) = \frac{\binom{w}{x}\binom{b}{n-x}}{\binom{w+b}{n}}.$$ If $X$ is the number of white balls, then $n-X$ must be the number of black balls. Let $Y=n-X$, then $P(Y=y)=P(n-X=y)=P(X=n-y)$, so plugging in, I get $$P(Y=y) = \frac{\binom{w}{n-y}\binom{b}{y}}{\binom{w+b}{n}}$$ Is this a correct proof (not only if the results is correct)? What kind of manipulations are actually allowed within the $P(\dots)$? I'm always tempted to do some stuff like $P(Y=y)=P(n-X=n-x)=P(X=x)$, which is obviously meaningless, but I can't figure out what is formally wrong with it. Why is it allowed to plug in $n-X$ for $Y$, but not $n-x$ for $y$?",,"['probability', 'statistics']"
9,Expected value of lognormal distribution. [duplicate],Expected value of lognormal distribution. [duplicate],,"This question already has answers here : How to compute moments of log normal distribution? (2 answers) Closed 3 years ago . Hi I'm stuck on this question: Recall that $X$ is said to have a lognormal distribution with parameters $\mu$ and $\sigma^2$ if $\log(X)$ is normal with mean $\mu$ and variance $\sigma^2$. Suppose $X$ is such a lognormal random variable. Find $\mathrm{E}[X]$. Find $\mathrm{Var}(X)$. I know that the approach is to find the moment generating function and take the first derivative for the expected value and the second derivative for the variance (I think), but I can't seem to figure it out.  Thanks in advance.","This question already has answers here : How to compute moments of log normal distribution? (2 answers) Closed 3 years ago . Hi I'm stuck on this question: Recall that $X$ is said to have a lognormal distribution with parameters $\mu$ and $\sigma^2$ if $\log(X)$ is normal with mean $\mu$ and variance $\sigma^2$. Suppose $X$ is such a lognormal random variable. Find $\mathrm{E}[X]$. Find $\mathrm{Var}(X)$. I know that the approach is to find the moment generating function and take the first derivative for the expected value and the second derivative for the variance (I think), but I can't seem to figure it out.  Thanks in advance.",,"['statistics', 'normal-distribution', 'means']"
10,Generalised probability density functions using Dirac deltas,Generalised probability density functions using Dirac deltas,,"I just cannot understand how a discrete random variable $X$ may be represented using a generalised probability density function (p.d.f.) $f_X$ as, \begin{equation}\tag{1} f_X(x) = \sum_{x_k \in R_X} P(X = x_k)\delta(x - x_k) \end{equation} where $R_X = \{x_1, x_2, ...\}$ is the range of the discrete random variable $X$, $P(\cdot)$ is the probability mass function (p.m.f.) of $X$ and $\delta(x - x_k)$ is the Dirac delta function , $$\delta(x)  = \begin{cases}     \infty,&  x = 0\\     0,              & otherwise.\end{cases} $$ where by definition of the Dirac delta function, $$\int_{- \infty}^{+ \infty} \delta(x)dx = 1$$ I have seen (1) used in a number of online sources. What I do not understand is how (1) works if you end up multiplying infinities? Shouldn't (1) be rewritten as, \begin{equation}\tag{2} f_X(x) = \sum_{x_k \in R_X} P(X = x_k)\delta(x - x_k)dx \end{equation} where, $$\delta(x)dx  = \begin{cases}     1,&  x = 0\\     0,              & otherwise.\end{cases} $$ Two online sources where (1) is used: i. A book's supporting website ii. A tutorial publication (pdf)","I just cannot understand how a discrete random variable $X$ may be represented using a generalised probability density function (p.d.f.) $f_X$ as, \begin{equation}\tag{1} f_X(x) = \sum_{x_k \in R_X} P(X = x_k)\delta(x - x_k) \end{equation} where $R_X = \{x_1, x_2, ...\}$ is the range of the discrete random variable $X$, $P(\cdot)$ is the probability mass function (p.m.f.) of $X$ and $\delta(x - x_k)$ is the Dirac delta function , $$\delta(x)  = \begin{cases}     \infty,&  x = 0\\     0,              & otherwise.\end{cases} $$ where by definition of the Dirac delta function, $$\int_{- \infty}^{+ \infty} \delta(x)dx = 1$$ I have seen (1) used in a number of online sources. What I do not understand is how (1) works if you end up multiplying infinities? Shouldn't (1) be rewritten as, \begin{equation}\tag{2} f_X(x) = \sum_{x_k \in R_X} P(X = x_k)\delta(x - x_k)dx \end{equation} where, $$\delta(x)dx  = \begin{cases}     1,&  x = 0\\     0,              & otherwise.\end{cases} $$ Two online sources where (1) is used: i. A book's supporting website ii. A tutorial publication (pdf)",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'dirac-delta']"
11,Proof of the law of large numbers for higher moments,Proof of the law of large numbers for higher moments,,"Let us work on some probability space $<\Omega,\mathscr{A},\mathbb{P}>$: I'm looking for (independent) proofs of two proofs, of the generalised weak and strong law of large numbers respectively. That is I'm looking for proofs that the sample moments are consistency estimators of the moments of the distribution in question (given appropriate conditions thereon). In symbols: I'm looking for two proofs that: $m_k\overset{D}{\rightarrow} \mu_k$ and $m_k\overset{as}{\rightarrow} \mu_k$ (where m_k is the $k^{th}$sample moment and $\mu_k$ is the k$^{th}$ (assuming at-most that $\mu_{k+1}<\infty$)? It would be preferable if the proof of the weak law relied on characteristic functions. Thanks in advance","Let us work on some probability space $<\Omega,\mathscr{A},\mathbb{P}>$: I'm looking for (independent) proofs of two proofs, of the generalised weak and strong law of large numbers respectively. That is I'm looking for proofs that the sample moments are consistency estimators of the moments of the distribution in question (given appropriate conditions thereon). In symbols: I'm looking for two proofs that: $m_k\overset{D}{\rightarrow} \mu_k$ and $m_k\overset{as}{\rightarrow} \mu_k$ (where m_k is the $k^{th}$sample moment and $\mu_k$ is the k$^{th}$ (assuming at-most that $\mu_{k+1}<\infty$)? It would be preferable if the proof of the weak law relied on characteristic functions. Thanks in advance",,"['probability', 'statistics', 'probability-theory', 'fourier-analysis', 'law-of-large-numbers']"
12,CDF of independent variables,CDF of independent variables,,"I am given two independent variables $X$ and $Y$. Where $F_X(x)=F_Y(x)=x^4 \ 0\le x \le 1$. I am looking for CDF of $Z=max(X,Y)$ and $E(Z^2)$ I need some pointers, especially for $Z^2$. Thank you.","I am given two independent variables $X$ and $Y$. Where $F_X(x)=F_Y(x)=x^4 \ 0\le x \le 1$. I am looking for CDF of $Z=max(X,Y)$ and $E(Z^2)$ I need some pointers, especially for $Z^2$. Thank you.",,"['probability', 'statistics', 'probability-distributions']"
13,Determining the value of ECDF at a point using Matlab,Determining the value of ECDF at a point using Matlab,,"I have a data $X=[x_1,\dots,x_n].$ In Matlab, I know by using [f,x]=ecdf(X) plot(x,f) we will have the empirical distribution function based on $X$. Now, if $x$ is given, how will I know the value of my ECDF at this point?","I have a data $X=[x_1,\dots,x_n].$ In Matlab, I know by using [f,x]=ecdf(X) plot(x,f) we will have the empirical distribution function based on $X$. Now, if $x$ is given, how will I know the value of my ECDF at this point?",,"['statistics', 'matlab']"
14,In statistics what does mutually exclusive mean?,In statistics what does mutually exclusive mean?,,For homework a question related to Venn diagrams is 'Are the probabilities of having not used a spinner and not tossed a coin in the game mutually exclusive' Don't know what is meant by it so can't answer it with out your help,For homework a question related to Venn diagrams is 'Are the probabilities of having not used a spinner and not tossed a coin in the game mutually exclusive' Don't know what is meant by it so can't answer it with out your help,,['statistics']
15,Normal Distribution $E(X^4)$?,Normal Distribution ?,E(X^4),"So I have the Normal Distribution $f(z)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}.$ I know any $E(Z^{\mbox{ (any odd #)}})$ makes you integrate an odd function thus giving an answer of zero (i.e. $E(Z^1)$ and $E(Z^3)$ both $=0$). And I know computing $E(Z^2)$ shows some links to the Gamma function/distributions and helps the integration and gives the answer of $1.$ However, I'm having difficulty figuring $E(Z^4).$ I know $\displaystyle E(Z^4) = \int \left[z^4 \frac{1}{\sqrt{2\pi}}e^{-z^2/2}\right] dz,$ then I do  $\displaystyle 2 \cdot \frac{2}{\sqrt{2\pi}} \int \left[\frac{z^2}{2} e^{-z^2/2}\right] d\left(\frac{z^2}{2}\right).$ This is similar to the Gamma function $\Gamma(\alpha)= \int t^{\alpha-1} e^{-t}.$  However, my ""$t$"" in my problem $[(z^2)/2]$ doesn't have an $(\alpha-1)$, it's just to the first power. Any suggestions of how to manipulate it better to form a better gamma function to solve the problem for $E(X^4)$? Thanks a lot","So I have the Normal Distribution $f(z)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}.$ I know any $E(Z^{\mbox{ (any odd #)}})$ makes you integrate an odd function thus giving an answer of zero (i.e. $E(Z^1)$ and $E(Z^3)$ both $=0$). And I know computing $E(Z^2)$ shows some links to the Gamma function/distributions and helps the integration and gives the answer of $1.$ However, I'm having difficulty figuring $E(Z^4).$ I know $\displaystyle E(Z^4) = \int \left[z^4 \frac{1}{\sqrt{2\pi}}e^{-z^2/2}\right] dz,$ then I do  $\displaystyle 2 \cdot \frac{2}{\sqrt{2\pi}} \int \left[\frac{z^2}{2} e^{-z^2/2}\right] d\left(\frac{z^2}{2}\right).$ This is similar to the Gamma function $\Gamma(\alpha)= \int t^{\alpha-1} e^{-t}.$  However, my ""$t$"" in my problem $[(z^2)/2]$ doesn't have an $(\alpha-1)$, it's just to the first power. Any suggestions of how to manipulate it better to form a better gamma function to solve the problem for $E(X^4)$? Thanks a lot",,"['probability', 'statistics', 'normal-distribution']"
16,Counting Number of Possibilities using Inclusion-Exclusion,Counting Number of Possibilities using Inclusion-Exclusion,,"I have been tasked with answering the following combinatorics problem for a homework assignment: Consider the set of all six digit numbers that don’t begin with 0. How many of these have at least one 0, at least one 1, and at least one 2? The professor hinted that we use inclusion-exclusion to solve it, but I am not really seeing to accomplish this. I have tried writing down some patterns and then finding rearrangements of them within the rules, but I am not arriving at the correct answer of 59790 (I wrote a program to count the size of the set). Given that this is homework I would prefer some guidance and not a full solution. Thanks!","I have been tasked with answering the following combinatorics problem for a homework assignment: Consider the set of all six digit numbers that don’t begin with 0. How many of these have at least one 0, at least one 1, and at least one 2? The professor hinted that we use inclusion-exclusion to solve it, but I am not really seeing to accomplish this. I have tried writing down some patterns and then finding rearrangements of them within the rules, but I am not arriving at the correct answer of 59790 (I wrote a program to count the size of the set). Given that this is homework I would prefer some guidance and not a full solution. Thanks!",,"['combinatorics', 'statistics', 'inclusion-exclusion']"
17,Find the pdf of $X+Y$ (discrete case),Find the pdf of  (discrete case),X+Y,"Given the marginal pdfs: $$p_X(k)= e^{-\lambda}\frac{\lambda^k}{k!}\text{ and }p_Y(k)= e^{-\mu}\frac{\mu^k}{k!},\quad k=0,1,2,\ldots$$ find the pdf of $X+Y$. I know for $W=X+Y$, $p_W(w)= \sum_x p_X(x)p_Y(w-x)$ So far I have: $$p_W(w)= e^{-\lambda-\mu}\sum_x\frac{\lambda^x\mu^{w-x}}{x!(w-x)!} ,\ldots$$ but I am not seeing where this is coming from. If you're summing all $x$'s, wouldn't you get an infinite sum?","Given the marginal pdfs: $$p_X(k)= e^{-\lambda}\frac{\lambda^k}{k!}\text{ and }p_Y(k)= e^{-\mu}\frac{\mu^k}{k!},\quad k=0,1,2,\ldots$$ find the pdf of $X+Y$. I know for $W=X+Y$, $p_W(w)= \sum_x p_X(x)p_Y(w-x)$ So far I have: $$p_W(w)= e^{-\lambda-\mu}\sum_x\frac{\lambda^x\mu^{w-x}}{x!(w-x)!} ,\ldots$$ but I am not seeing where this is coming from. If you're summing all $x$'s, wouldn't you get an infinite sum?",,"['probability', 'statistics', 'random-variables']"
18,Is the Gamma Function a jointly sufficient statistic?,Is the Gamma Function a jointly sufficient statistic?,,"A random sample $X_{1},...,X_{n}$ are pulled from a gamma distribution. Are there jointly sufficient statistics based on these observations for the two unknown parameters? The definition of a gamma distribution is f(x;$\alpha$,$\beta$)=$\frac{x^{\alpha-1 }}{\beta ^\alpha \Gamma(x){}}e^{\frac{-x}{\beta }}$ I kind of understand what a Jointly Sufficient Statistic is however I am not sure what to do from here. Possibly taking the product $\prod_{i=1}^{n}$ in front of the distribution. Can anybody help? Thanks!","A random sample $X_{1},...,X_{n}$ are pulled from a gamma distribution. Are there jointly sufficient statistics based on these observations for the two unknown parameters? The definition of a gamma distribution is f(x;$\alpha$,$\beta$)=$\frac{x^{\alpha-1 }}{\beta ^\alpha \Gamma(x){}}e^{\frac{-x}{\beta }}$ I kind of understand what a Jointly Sufficient Statistic is however I am not sure what to do from here. Possibly taking the product $\prod_{i=1}^{n}$ in front of the distribution. Can anybody help? Thanks!",,"['statistics', 'probability-distributions', 'self-learning']"
19,Exponential growth's power,Exponential growth's power,,"If the number of Ebola cases are doubling every three weeks, how long before everyone in the world ($7$ billion population) will be infected? Sorry, I forgot to add that currently we have $3000$ cases. I'm just an English major but math layman. I think this may be too scary for many of us to consider. My friend offered this: $2^n=7000000000=7*10^9$ Solve for $n$ Then $3n=$number of weeks","If the number of Ebola cases are doubling every three weeks, how long before everyone in the world ($7$ billion population) will be infected? Sorry, I forgot to add that currently we have $3000$ cases. I'm just an English major but math layman. I think this may be too scary for many of us to consider. My friend offered this: $2^n=7000000000=7*10^9$ Solve for $n$ Then $3n=$number of weeks",,['statistics']
20,Monte Carlo p-test and early stopping,Monte Carlo p-test and early stopping,,"Say you have a coin with some probability $p$ of falling on heads.  You would like to determine if this probability is less than or equal to $0.05$ with some reasonable degree of confidence and stop as soon as you have that confidence. This is meant to be a model for doing any Monte Carlo statistical test where you would like to reject with a $p$-value of $0.05$ or lower and where tests are expensive so you want to stop as early as possible. What is known about the earliest time you can stop, depending on the   outcome of the coin tosses and your desired degree of confidence?","Say you have a coin with some probability $p$ of falling on heads.  You would like to determine if this probability is less than or equal to $0.05$ with some reasonable degree of confidence and stop as soon as you have that confidence. This is meant to be a model for doing any Monte Carlo statistical test where you would like to reject with a $p$-value of $0.05$ or lower and where tests are expensive so you want to stop as early as possible. What is known about the earliest time you can stop, depending on the   outcome of the coin tosses and your desired degree of confidence?",,['probability']
21,Log normal distribution - Where am I wrong?,Log normal distribution - Where am I wrong?,,"Let $X$ be a R.V whose pdf is given by  $$f(x)=p\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x-\mu_1)^2}{2\sigma_1^2}\right)+ (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x-\mu_2)^2}{2\sigma_2^2}\right)$$ clearly $X\sim pN(\mu_1,\sigma_1^2)+(1-p)N(\mu_2,\sigma_2^2)=N(p\mu_1+(1-p)\mu_2,p^2\sigma_1^2+(1-p)^2\sigma_2^2)$ therefore $\kappa=E(e^X)-1=e^{p\mu_1+(1-p)\mu_2+\frac{1}{2}(p^2\sigma_1^2+(1-p)^2\sigma_2^2)}-1$. However, if I let $Y=e^X$, then  Let $G(y)$ be the cdf of $Y$, then  $G(y)=P(Y<y)=P(e^X<y)=p(X<\ln(y))=F_X(\ln(y))$ there fore $g(y)=f(\ln(y))\frac{1}{y}$ so  the pdf  $g(y)$ of $Y$ is given by $$g(y)=p\frac{1}{y\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(\ln(y)-\mu_1)^2}{2\sigma_1^2} \right) + (1-p) \frac{1}{y\sqrt{2\pi\sigma_2^2}} \exp\left(-\frac{(\ln(y)-\mu_2)^2}{2\sigma_2^2}\right)$$ i.e $Y=p\times \mathrm{logNormal}(\mu_1,\sigma_1^2)+(1-p)\times \mathrm{logNormal} (\mu_2,\sigma_1^2)$ where $\mathrm{logNormal}(\mu_1,\sigma_1^2)$ means a log-normal distribution. hence $\kappa=E(Y)-1=pE(\mathrm{logNormal}(\mu_1,\sigma_1^2))+(1-p) E(\mathrm{logNormal} (\mu_2,\sigma_2^2))= pe^{\mu_1+\frac{\sigma_1^2}{2}}+(1-p)e^{\mu_2+\frac{\sigma_2^2}{2}}-1$. My question is : why I have different values for $\kappa$? Could some one explain for me where I am wrong ? thank you for your time.","Let $X$ be a R.V whose pdf is given by  $$f(x)=p\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x-\mu_1)^2}{2\sigma_1^2}\right)+ (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x-\mu_2)^2}{2\sigma_2^2}\right)$$ clearly $X\sim pN(\mu_1,\sigma_1^2)+(1-p)N(\mu_2,\sigma_2^2)=N(p\mu_1+(1-p)\mu_2,p^2\sigma_1^2+(1-p)^2\sigma_2^2)$ therefore $\kappa=E(e^X)-1=e^{p\mu_1+(1-p)\mu_2+\frac{1}{2}(p^2\sigma_1^2+(1-p)^2\sigma_2^2)}-1$. However, if I let $Y=e^X$, then  Let $G(y)$ be the cdf of $Y$, then  $G(y)=P(Y<y)=P(e^X<y)=p(X<\ln(y))=F_X(\ln(y))$ there fore $g(y)=f(\ln(y))\frac{1}{y}$ so  the pdf  $g(y)$ of $Y$ is given by $$g(y)=p\frac{1}{y\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(\ln(y)-\mu_1)^2}{2\sigma_1^2} \right) + (1-p) \frac{1}{y\sqrt{2\pi\sigma_2^2}} \exp\left(-\frac{(\ln(y)-\mu_2)^2}{2\sigma_2^2}\right)$$ i.e $Y=p\times \mathrm{logNormal}(\mu_1,\sigma_1^2)+(1-p)\times \mathrm{logNormal} (\mu_2,\sigma_1^2)$ where $\mathrm{logNormal}(\mu_1,\sigma_1^2)$ means a log-normal distribution. hence $\kappa=E(Y)-1=pE(\mathrm{logNormal}(\mu_1,\sigma_1^2))+(1-p) E(\mathrm{logNormal} (\mu_2,\sigma_2^2))= pe^{\mu_1+\frac{\sigma_1^2}{2}}+(1-p)e^{\mu_2+\frac{\sigma_2^2}{2}}-1$. My question is : why I have different values for $\kappa$? Could some one explain for me where I am wrong ? thank you for your time.",,"['calculus', 'probability', 'statistics', 'probability-distributions']"
22,Optimize rate of collection in counters,Optimize rate of collection in counters,,"Suppose you have $K$ counters. The value of these $K$ counters are all $0$. Every second, each counter has a $J$ chance of incrementing itself, up to a max value of $I$. Every second, you may choose to collect the values of all the counters. This resets all counters back to $0$ and prevents incrementation of counters for that second. Assume $K$ and $I$ are integers and $0 \le J \le 1$. Find the optimal time between collections in terms of $J$, $K$, and $I$, such that the sum of the collected values is greatest. For example, collecting every other second would not be ideal, as the incrementing time is essentially halved. Collecting too slowly would also not be ideal, as there is a chance that the max limit $I$ is reached and the counter attempts to increment itself but fails. Feel free to add/remove tags.","Suppose you have $K$ counters. The value of these $K$ counters are all $0$. Every second, each counter has a $J$ chance of incrementing itself, up to a max value of $I$. Every second, you may choose to collect the values of all the counters. This resets all counters back to $0$ and prevents incrementation of counters for that second. Assume $K$ and $I$ are integers and $0 \le J \le 1$. Find the optimal time between collections in terms of $J$, $K$, and $I$, such that the sum of the collected values is greatest. For example, collecting every other second would not be ideal, as the incrementing time is essentially halved. Collecting too slowly would also not be ideal, as there is a chance that the max limit $I$ is reached and the counter attempts to increment itself but fails. Feel free to add/remove tags.",,"['probability', 'statistics', 'optimization']"
23,$\overline{\theta}$ the maximum likelihood estimator of $\theta \implies$?,the maximum likelihood estimator of ?,\overline{\theta} \theta \implies,I can't understand how the following statement holds without any extra conditions on the function $g$: $\overline{\lambda}$ the maximum likelihood estimator of parameter $\lambda \implies g(\overline{\lambda})$ the maximum likelihood estimator of $g(\lambda)$ Intuitively it would seem that we would need $g$ to be monotionic for this to be true. How can it be shown that this statement holds for an arbitrary function $g$?,I can't understand how the following statement holds without any extra conditions on the function $g$: $\overline{\lambda}$ the maximum likelihood estimator of parameter $\lambda \implies g(\overline{\lambda})$ the maximum likelihood estimator of $g(\lambda)$ Intuitively it would seem that we would need $g$ to be monotionic for this to be true. How can it be shown that this statement holds for an arbitrary function $g$?,,"['probability', 'statistics']"
24,Expectation of Geometic distribution,Expectation of Geometic distribution,,"I have the following question : It costs $30$ cents per day to keep pigeons. Let $N$ be the number of pigeons kept and suppose that $N$ has the geometric distribution $Pr(N=n) = \frac1{10} (\frac9{10})^n (n=0,1,\dots).$ Which yields $30*\frac{0.9}{0.1} = 270=\$2.7$ That is fine. Now I have the one I don't know how to solve : Now suppose that they cost $35$ cents per head except for the first three which remain at $30$ cents per head. What is the expected cost per day. I have tried numerous things but I can't get their answer, I don't want to type all of these up because my table pc is slow. I have tried taking the difference between the first three against expected for the new price $35 * 9$ and calculating them all. I have their solution but do not understand it: $=5(9-3 + \frac3{10} + \frac2{10}(\frac9{10}) + \frac1{10}(\frac9{10})^2) = \frac{6561}{200} = 32.805$","I have the following question : It costs $30$ cents per day to keep pigeons. Let $N$ be the number of pigeons kept and suppose that $N$ has the geometric distribution $Pr(N=n) = \frac1{10} (\frac9{10})^n (n=0,1,\dots).$ Which yields $30*\frac{0.9}{0.1} = 270=\$2.7$ That is fine. Now I have the one I don't know how to solve : Now suppose that they cost $35$ cents per head except for the first three which remain at $30$ cents per head. What is the expected cost per day. I have tried numerous things but I can't get their answer, I don't want to type all of these up because my table pc is slow. I have tried taking the difference between the first three against expected for the new price $35 * 9$ and calculating them all. I have their solution but do not understand it: $=5(9-3 + \frac3{10} + \frac2{10}(\frac9{10}) + \frac1{10}(\frac9{10})^2) = \frac{6561}{200} = 32.805$",,"['probability', 'statistics']"
25,Is it possible to calculate the expectation of $\frac{a}{(b+x)^2}$ where x is gamma distributed?,Is it possible to calculate the expectation of  where x is gamma distributed?,\frac{a}{(b+x)^2},"Is it possible to calculate the expectation of $\frac{a}{(b+x)^2}$ where x is gamma distributed? Or more generally, can you calculate the distribution of $\frac{Na}{(b+\sum\limits_{i=1}^n x_i)^2}$ where the x's are IID gamma distributions? Sorry for not giving any context, it would be tough to do so without confusing the main question at hand. Thank you!","Is it possible to calculate the expectation of $\frac{a}{(b+x)^2}$ where x is gamma distributed? Or more generally, can you calculate the distribution of $\frac{Na}{(b+\sum\limits_{i=1}^n x_i)^2}$ where the x's are IID gamma distributions? Sorry for not giving any context, it would be tough to do so without confusing the main question at hand. Thank you!",,"['statistics', 'probability-distributions']"
26,Given two uniformly distributed independent random variables what should the PDF of multiplication of them? [duplicate],Given two uniformly distributed independent random variables what should the PDF of multiplication of them? [duplicate],,"This question already has answers here : Probability density function of a product of uniform random variables (2 answers) Closed 10 years ago . I am a probability noob and I was solving the following problem, but my answer doesn't match the book's. The length and width of panels used for interior doors(in inches) are denoted as Xand Y, respectively. Suppose that Xand Yare independent, continuous uniform random variables for 17.75<x<18.25 and 4.75<y<5.25 , respectively.determine the probability that thearea of a panel exceeds 90 squared inches. I thought the area random variable will have a minimum value of 17.75*4.75=84.3125 Maximum value of area will be 18.25*5.25=95.8125 Also if X and Y are uniformly distributed and independent then area should be uniformly distributed too. Every single value that area can take has equal chance. Please correct me here. Now with this when I integrate area(A) from 90 to 95.8125 for a constant PDF f(A)=1/(95.8125-84.3125), my answer comes out to be 0.5054. But the answer according to the solutions given at the back of book is 0.499. I am not able to understand where I am doing a mistake. Can anyone please help me understand where I am going wrong?","This question already has answers here : Probability density function of a product of uniform random variables (2 answers) Closed 10 years ago . I am a probability noob and I was solving the following problem, but my answer doesn't match the book's. The length and width of panels used for interior doors(in inches) are denoted as Xand Y, respectively. Suppose that Xand Yare independent, continuous uniform random variables for 17.75<x<18.25 and 4.75<y<5.25 , respectively.determine the probability that thearea of a panel exceeds 90 squared inches. I thought the area random variable will have a minimum value of 17.75*4.75=84.3125 Maximum value of area will be 18.25*5.25=95.8125 Also if X and Y are uniformly distributed and independent then area should be uniformly distributed too. Every single value that area can take has equal chance. Please correct me here. Now with this when I integrate area(A) from 90 to 95.8125 for a constant PDF f(A)=1/(95.8125-84.3125), my answer comes out to be 0.5054. But the answer according to the solutions given at the back of book is 0.499. I am not able to understand where I am doing a mistake. Can anyone please help me understand where I am going wrong?",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
27,"calculating $P(X_n=\max(X_1,X_2,\ldots, X_n))$",calculating,"P(X_n=\max(X_1,X_2,\ldots, X_n))","Suppose $X_1,X_2,\ldots X_{n-1}\sim U(0,1)$ , $X_n\sim \exp(\frac{1}{2})$ . Else, suppose $X_1,X_2\ldots, X_n$ are independent. How can I calculate $P(X_n=\max(X_1,X_2\ldots, X_n))$","Suppose , . Else, suppose are independent. How can I calculate","X_1,X_2,\ldots X_{n-1}\sim U(0,1) X_n\sim \exp(\frac{1}{2}) X_1,X_2\ldots, X_n P(X_n=\max(X_1,X_2\ldots, X_n))",['statistics']
28,Find the distribution of $X_1^2 + X_2^2$? [duplicate],Find the distribution of ? [duplicate],X_1^2 + X_2^2,"This question already has answers here : Let $X_1$ and $X_2$ are independent $N(0, \sigma^2)$ random variables. What is the distribution of $X_1^2 + X_2^2$? (3 answers) Closed 10 years ago . Let $X_1$ and $X_2$ are independent $N(0, \sigma^2)$ which means (mean = 0, variance = $\sigma^2$) random variables. What is the distribution of $X_1^2 + X_2^2$? My approach is that  $X_1\sim N(0, \sigma^2)$ and $X_2\sim N(0, \sigma^2)$. Transforming $X_1$ and $X_2$ into standard normal, $X_1/\sigma\sim N(0, 1)$ and $X_2/\sigma\sim N(0, 1)$. Then $X_1^2/\sigma$ and $X_2^2/\sigma$ have chi-squared distribution with 1 degree of freedom. Then I found the moment-generating function for $X_1^2$ and $X_2^2$;$$m_{X_1^2} = (1-2t)^{-1/2}$$ and $$m_{X_2^2} = (1-2t)^{-1/2}$$ So the moment generating function for $X_1^2 + X_2^2$ is $$m_{X_1^2}(t) m_{X_2^2}(t) = (1-2t)^{-2/2}$$ So $X_1^2 + X_2^2$ has a chi-squared distribution with 2 degrees of freedom. My question can I treat $X_1^2/\sigma$ + $X_2^2/\sigma$ as $X_1^2$ + $X_2^2$ like I did above?","This question already has answers here : Let $X_1$ and $X_2$ are independent $N(0, \sigma^2)$ random variables. What is the distribution of $X_1^2 + X_2^2$? (3 answers) Closed 10 years ago . Let $X_1$ and $X_2$ are independent $N(0, \sigma^2)$ which means (mean = 0, variance = $\sigma^2$) random variables. What is the distribution of $X_1^2 + X_2^2$? My approach is that  $X_1\sim N(0, \sigma^2)$ and $X_2\sim N(0, \sigma^2)$. Transforming $X_1$ and $X_2$ into standard normal, $X_1/\sigma\sim N(0, 1)$ and $X_2/\sigma\sim N(0, 1)$. Then $X_1^2/\sigma$ and $X_2^2/\sigma$ have chi-squared distribution with 1 degree of freedom. Then I found the moment-generating function for $X_1^2$ and $X_2^2$;$$m_{X_1^2} = (1-2t)^{-1/2}$$ and $$m_{X_2^2} = (1-2t)^{-1/2}$$ So the moment generating function for $X_1^2 + X_2^2$ is $$m_{X_1^2}(t) m_{X_2^2}(t) = (1-2t)^{-2/2}$$ So $X_1^2 + X_2^2$ has a chi-squared distribution with 2 degrees of freedom. My question can I treat $X_1^2/\sigma$ + $X_2^2/\sigma$ as $X_1^2$ + $X_2^2$ like I did above?",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
29,What is the meaning of this notation?,What is the meaning of this notation?,,"I am watching this video on maximum likelyhood estimation. I'm confused by the notation when the presenter says ""assume a set of distributions P-theta."" (the URL links to the relevant part of the video: What is the presenter saying here? I am confused by a bunch of things. I have seen set builder notation, but not this notation with a colon. Are they the same? Also he says that little theta ""ranges over"" big theta. What does it mean to ""range over"" something? Basically there are enough unfamiliar things going on here that I can't totally follow it from context. Is the idea that this statement builds a set of all of the distributions that are possible with all of the different parameters (thetas)? Is this the idea?","I am watching this video on maximum likelyhood estimation. I'm confused by the notation when the presenter says ""assume a set of distributions P-theta."" (the URL links to the relevant part of the video: What is the presenter saying here? I am confused by a bunch of things. I have seen set builder notation, but not this notation with a colon. Are they the same? Also he says that little theta ""ranges over"" big theta. What does it mean to ""range over"" something? Basically there are enough unfamiliar things going on here that I can't totally follow it from context. Is the idea that this statement builds a set of all of the distributions that are possible with all of the different parameters (thetas)? Is this the idea?",,"['probability', 'statistics', 'notation']"
30,General birth and death process,General birth and death process,,"hi i need some help to understand the following (from the general birth and death process).I'll give some context first , then i ask questions. Consider general birth and death process with birth rates $\{\lambda_n \}$ and death rates,where $\mu_o = 0$, and let $T_i$ denote the time,starting from state $i$, it takes the process to enter $i+1$, $i\geq 0$. We will recursively compute $E[T_i]$,$i\geq 1$,by starting with $i = 0$. Since $T_0$ is exponential with rate  $\lambda_0$ we have: $E[T_0]= \frac{1}{\lambda_0}$ For $i> 0$, we condition wheater the first transition takes the process into state $i-1$ or $i+1$, that is let: $I_i = \begin{cases} 1, &\mbox{if the first tranistion from $i$ is to $i+1$ }  \\  0 & \mbox{if the first transition from $i$ is to $i-1$}  \end{cases}$ and note that $E[T_i\vert I_i = 1] = \frac{1}{\lambda_i +\mu_i}$ $E[T_i\vert I_i = 0] = \frac{1}{\lambda_i +\mu_i} + E[T_{i-1}]+ E[T_{i}] $ $\textbf{questions}$ $E[T_i\vert I_i = 1] = \frac{1}{\lambda_i +\mu_i}$ how do i interpret this? it is clear from the text above that $E[T_i\vert I_i = 1]$ is the expected time for the process to enter state $i+1$,given that the first transition is from $i $ to $i+1$. but how is this equal to $\frac{1}{\lambda_i +\mu_i}$. Is this the mean for the minimum of two random variables with rates $\lambda_i$ and $\mu_i$?","hi i need some help to understand the following (from the general birth and death process).I'll give some context first , then i ask questions. Consider general birth and death process with birth rates $\{\lambda_n \}$ and death rates,where $\mu_o = 0$, and let $T_i$ denote the time,starting from state $i$, it takes the process to enter $i+1$, $i\geq 0$. We will recursively compute $E[T_i]$,$i\geq 1$,by starting with $i = 0$. Since $T_0$ is exponential with rate  $\lambda_0$ we have: $E[T_0]= \frac{1}{\lambda_0}$ For $i> 0$, we condition wheater the first transition takes the process into state $i-1$ or $i+1$, that is let: $I_i = \begin{cases} 1, &\mbox{if the first tranistion from $i$ is to $i+1$ }  \\  0 & \mbox{if the first transition from $i$ is to $i-1$}  \end{cases}$ and note that $E[T_i\vert I_i = 1] = \frac{1}{\lambda_i +\mu_i}$ $E[T_i\vert I_i = 0] = \frac{1}{\lambda_i +\mu_i} + E[T_{i-1}]+ E[T_{i}] $ $\textbf{questions}$ $E[T_i\vert I_i = 1] = \frac{1}{\lambda_i +\mu_i}$ how do i interpret this? it is clear from the text above that $E[T_i\vert I_i = 1]$ is the expected time for the process to enter state $i+1$,given that the first transition is from $i $ to $i+1$. but how is this equal to $\frac{1}{\lambda_i +\mu_i}$. Is this the mean for the minimum of two random variables with rates $\lambda_i$ and $\mu_i$?",,"['probability', 'statistics', 'markov-chains', 'markov-process']"
31,Why normal approximation to binomial distribution uses np> 5 as a condition,Why normal approximation to binomial distribution uses np> 5 as a condition,,I was reading about normal approximation to binomial distribution and I dunno how it works for cases when you say for example p is equal to 0.3 where p is probability of success. On most websites it is written that normal approximation to binomial distribution works well if average is greater than 5. I.e. np> 5 But I am unable to find where did this empirical formula came from? If n is quite large and probability of success is equal to .5 then i agree that normal approximation to binomial distribution is going to be quite accurate. But what about other cases? How can one say np> 5 is the condition for doing normal approximation?,I was reading about normal approximation to binomial distribution and I dunno how it works for cases when you say for example p is equal to 0.3 where p is probability of success. On most websites it is written that normal approximation to binomial distribution works well if average is greater than 5. I.e. np> 5 But I am unable to find where did this empirical formula came from? If n is quite large and probability of success is equal to .5 then i agree that normal approximation to binomial distribution is going to be quite accurate. But what about other cases? How can one say np> 5 is the condition for doing normal approximation?,,"['statistics', 'probability-distributions', 'normal-distribution']"
32,properties of least square estimators in regression,properties of least square estimators in regression,,$Y_i=\beta_0+\beta_1 X_i+\epsilon_i$ where $\epsilon_i$ is normally distributed with mean $0$ and variance $\sigma^2$   . The least square estimators of this model are $\hat\beta_0$ and $\hat\beta_1$. How can I show that $\hat\beta_0$ and $\hat\beta_1$ are linear functions of $y_i$? Also it says that both estimators are normally distributed.How come they normally distributed?I know that linear functions of normally distributed variables are also normally distributed.  . please explain this to me,$Y_i=\beta_0+\beta_1 X_i+\epsilon_i$ where $\epsilon_i$ is normally distributed with mean $0$ and variance $\sigma^2$   . The least square estimators of this model are $\hat\beta_0$ and $\hat\beta_1$. How can I show that $\hat\beta_0$ and $\hat\beta_1$ are linear functions of $y_i$? Also it says that both estimators are normally distributed.How come they normally distributed?I know that linear functions of normally distributed variables are also normally distributed.  . please explain this to me,,"['statistics', 'regression', 'regression-analysis']"
33,Does a proportion have to be a rational number?,Does a proportion have to be a rational number?,,"Does a proportion have to be a rational number? For example, Assume we have a square with side $2$ units. We are throwing a circle of radius $1$ unit over the square. Let $X$ be the area of the square covered by the circle relative to the area of the whole square. Hence, $X$ takes value in the interval $[0,{\pi  \over 4}]$. Is $X$ considered a proportion? Thanks in advance","Does a proportion have to be a rational number? For example, Assume we have a square with side $2$ units. We are throwing a circle of radius $1$ unit over the square. Let $X$ be the area of the square covered by the circle relative to the area of the whole square. Hence, $X$ takes value in the interval $[0,{\pi  \over 4}]$. Is $X$ considered a proportion? Thanks in advance",,"['statistics', 'random-variables', 'statistical-inference']"
34,Problem related to the exact distribution and the CLT,Problem related to the exact distribution and the CLT,,"I'm trying to solve a pretty straight forward problem but i can't find good info on the subjects necessary to solve it so i'm terribly stuck. I'll present it as follows and later try to explain my specific doubts about it. Here it goes: Let $X_{1}, \ldots , X_{15}$ be a simple sample from a  exponential population $X$ with mean $= 3$ and let $S = X_{1} + \cdots + X_{15}$. Find $P(S>38)$ using: The exact distribution of $S$ The normal approximation. In this case, explain why the of the central limit theorem is valid First :I can't find good information about the exact distribution, so i don't know where should I put the values from the problem, because I don't even know the pdf of the exact distribution Second I understand the intuition of the CLT in which  given a sufficiently large sample size from a population (with a finite level of variance), the mean of all samples from the same population will be approximately equal to the mean. BUT i can't relate that to the problem, it simply doesn't make sense to me how one connects to another. I know about the normal distribution, but what's the meaning of a 'normal approximation' ?","I'm trying to solve a pretty straight forward problem but i can't find good info on the subjects necessary to solve it so i'm terribly stuck. I'll present it as follows and later try to explain my specific doubts about it. Here it goes: Let $X_{1}, \ldots , X_{15}$ be a simple sample from a  exponential population $X$ with mean $= 3$ and let $S = X_{1} + \cdots + X_{15}$. Find $P(S>38)$ using: The exact distribution of $S$ The normal approximation. In this case, explain why the of the central limit theorem is valid First :I can't find good information about the exact distribution, so i don't know where should I put the values from the problem, because I don't even know the pdf of the exact distribution Second I understand the intuition of the CLT in which  given a sufficiently large sample size from a population (with a finite level of variance), the mean of all samples from the same population will be approximately equal to the mean. BUT i can't relate that to the problem, it simply doesn't make sense to me how one connects to another. I know about the normal distribution, but what's the meaning of a 'normal approximation' ?",,"['statistics', 'descriptive-statistics', 'central-limit-theorem']"
35,Finding probabilities with a Poisson distribution,Finding probabilities with a Poisson distribution,,"This is the question that I'm stuck with: An airline knows that overall 3% of passengers do not turn up for flights . The airline decides to adopt a policy of selling more tickets than there are seats on a flight. For an aircraft with 196 seats, the airline sold 200 tickets for a particular flight. By using a suitable approximation, find the probability that there is at least one empty seat on this flight. Here's what I did: As p is small, I can use a Poisson approximation $\therefore X$ ~ $P(6)$ where $X$ is the r.v. ""Number of people who don't turn up to flight"" If there is at least one empty seat on the flight, this means there must be at least one person who didn't show up, i.e. $X \geq 1$ $P(X \geq 1) = 1 - P(X = 0) \\ \space \space\space\space\space\space\space\space\space\space\space\space\space\space \space = 1 - 0.0025 \\ \space\space\space\space\space\space\space\space\space\space\space\space\space\space\space= 0.9975$ However, the mark scheme for this question says: $P(X > 4) = 1 - P(X \leq 4) \\ \space\space\space\space\space\space\space\space\space\space\space\space\space\space\space = 1 - 0.2851 \\ \space\space\space\space\space\space\space\space\space\space\space\space\space\space\space= 0.7149$ I don't understand where the four in this has come from? Is it something to do with the fact that the difference between actual seats on tickets sold is four? Thank you :)","This is the question that I'm stuck with: An airline knows that overall 3% of passengers do not turn up for flights . The airline decides to adopt a policy of selling more tickets than there are seats on a flight. For an aircraft with 196 seats, the airline sold 200 tickets for a particular flight. By using a suitable approximation, find the probability that there is at least one empty seat on this flight. Here's what I did: As p is small, I can use a Poisson approximation $\therefore X$ ~ $P(6)$ where $X$ is the r.v. ""Number of people who don't turn up to flight"" If there is at least one empty seat on the flight, this means there must be at least one person who didn't show up, i.e. $X \geq 1$ $P(X \geq 1) = 1 - P(X = 0) \\ \space \space\space\space\space\space\space\space\space\space\space\space\space\space \space = 1 - 0.0025 \\ \space\space\space\space\space\space\space\space\space\space\space\space\space\space\space= 0.9975$ However, the mark scheme for this question says: $P(X > 4) = 1 - P(X \leq 4) \\ \space\space\space\space\space\space\space\space\space\space\space\space\space\space\space = 1 - 0.2851 \\ \space\space\space\space\space\space\space\space\space\space\space\space\space\space\space= 0.7149$ I don't understand where the four in this has come from? Is it something to do with the fact that the difference between actual seats on tickets sold is four? Thank you :)",,"['probability', 'statistics']"
36,"maximum likelihood - estimate $\sigma^2$ in $N(0,\sigma^2)$",maximum likelihood - estimate  in,"\sigma^2 N(0,\sigma^2)","Prove, using maximum likelihood, the estimation of $\sigma^2$ where $X$ is $N(0,\sigma^2)$ There is no real statistics involved, just algebra and finding partial derivatives, so I tagged it algebra and calculus as well. $S_{xx}=n$ is obviously wrong. Attempt","Prove, using maximum likelihood, the estimation of $\sigma^2$ where $X$ is $N(0,\sigma^2)$ There is no real statistics involved, just algebra and finding partial derivatives, so I tagged it algebra and calculus as well. $S_{xx}=n$ is obviously wrong. Attempt",,"['calculus', 'algebra-precalculus', 'statistics']"
37,Calcute a chance that whole rectangle lays inside of the circle,Calcute a chance that whole rectangle lays inside of the circle,,"We are given circle with radius 1.  Point P lays somewhere on that circle picked from the uniform distribution. $\{P_x^2+P_y^2 = 1\}$ Point Q as well was randomly picked from the uniform distribution and lays inside the circle $\{Q_x^2+Q_y^2 \le 1\}$ Then we construct rectangle R with diagonal PQ and sides parallel to coordinate axes. Task is to find a chance that whole rectangle R lays inside the circle. At first I spent lots of time reasoning about line segment and circle intersection calculation but then I came up with thought that it's a huge overkill. Because that is enough to just check two other vertexes of the rectangle. (if point Q occurred to be near circle border, one of the adjacent vertexes going to be outside the circle) So all what we have to check, that none of those vertexes lies outside: $\{Q_x^2+P_y^2 \le 1\}$ $\{P_x^2+Q_y^2 \le 1\}$ But having figured out geometrical side of a problem, I got stuck with statistical one.","We are given circle with radius 1.  Point P lays somewhere on that circle picked from the uniform distribution. $\{P_x^2+P_y^2 = 1\}$ Point Q as well was randomly picked from the uniform distribution and lays inside the circle $\{Q_x^2+Q_y^2 \le 1\}$ Then we construct rectangle R with diagonal PQ and sides parallel to coordinate axes. Task is to find a chance that whole rectangle R lays inside the circle. At first I spent lots of time reasoning about line segment and circle intersection calculation but then I came up with thought that it's a huge overkill. Because that is enough to just check two other vertexes of the rectangle. (if point Q occurred to be near circle border, one of the adjacent vertexes going to be outside the circle) So all what we have to check, that none of those vertexes lies outside: $\{Q_x^2+P_y^2 \le 1\}$ $\{P_x^2+Q_y^2 \le 1\}$ But having figured out geometrical side of a problem, I got stuck with statistical one.",,"['geometry', 'statistics']"
38,Calculating the second moment of binomial random variable,Calculating the second moment of binomial random variable,,I don't understand how they got the equality E[Y+1]= np[(n-1)p+1]. https://i.sstatic.net/v9Yxs.jpg,I don't understand how they got the equality E[Y+1]= np[(n-1)p+1]. https://i.sstatic.net/v9Yxs.jpg,,"['probability', 'statistics']"
39,Find density of the smallest value in a sample,Find density of the smallest value in a sample,,"Let $X_1, X_2,..., X_n$ be a random sample from an infinite population with density function $f(x)$ and distribution function $F(x).$ Let $Y_{(1)}$ be the smallest value in the sample (the first order statistic). Express the density of $Y_{(1)}$ in terms of $n, f,$ and $F.$ This is for a homework problem. I know that I wanna use the distribution function method in order to solve this. $P(Y_{(1)}\le y)= 1 -P(Y_{(1)}\gt y)$ $= 1 -P(X_1,X_2,...X_n\gt y)$ $= 1 -P(X_1\gt y)P(X_2\gt y)\ ...P(X_n\gt y)$ $= 1 - [F(y)]^n $ Now where do I go from here?","Let $X_1, X_2,..., X_n$ be a random sample from an infinite population with density function $f(x)$ and distribution function $F(x).$ Let $Y_{(1)}$ be the smallest value in the sample (the first order statistic). Express the density of $Y_{(1)}$ in terms of $n, f,$ and $F.$ This is for a homework problem. I know that I wanna use the distribution function method in order to solve this. $P(Y_{(1)}\le y)= 1 -P(Y_{(1)}\gt y)$ $= 1 -P(X_1,X_2,...X_n\gt y)$ $= 1 -P(X_1\gt y)P(X_2\gt y)\ ...P(X_n\gt y)$ $= 1 - [F(y)]^n $ Now where do I go from here?",,['statistics']
40,How would you explain confidence intervals to a beginner with very weak algebra skills,How would you explain confidence intervals to a beginner with very weak algebra skills,,"Let us say that you are taking AP Statistics. The prerequisite is a passing grade of D or above in Algebra II. The kids that you are working with struggle with algebra and do not retain information very well. Even though you spent a month talking about z-scores and how to find them using the invNorm() function a lot of them are still confused as to what to do and need someone to spoonfeed them with simplified information that does not contain too many technical terms. Your challenge now is to teach them confidence intervals which involve dealing with a) the concept b) the math c) the interpretation and finally the d) misconceptions. As you can see, this is an uphill battle and what worsens it is the student apathy. You review how to find the corresponding z-scores of the middle 95th percentile using the invNorm() function. You explain that the first entry must be the area to the left however when they see $z_{\alpha}$ they think the area to the right. This has been an ongoing confusion for one month despite you repeating the same thing over weeks. Now you try giving them a motivating example: ""Lets say you wanted to figure out the population mean length of all the world's bald eagles' wingspan. This is our parameter. Remember a parameter is a numerical value assigned to a whole population. You take a sample of 100 bald eagles and measure their wingspan. We call this sample mean a point estimate. Do you guys think that this point estimate accurately reflects the population mean?"" (Introducing vocabulary) You transition into talking about 95% confidence intervals conceptually. They are lost. Then you present them this formula. They are now completely and hopelessly lost: $ \bar{x} - z_{\frac{\alpha}{2}}\sigma < \mu < \bar{x} + z_{\frac{\alpha}{2}}\sigma $ You draw a picture showing that the level of confidence is $1-\alpha$ and the remaining areas we don't want are $\frac{\alpha}{2}$ and $\frac{\alpha}{2}$. They don't get what's going on. Twenty minutes in, there are some students not paying attention anymore and doing another class' work.","Let us say that you are taking AP Statistics. The prerequisite is a passing grade of D or above in Algebra II. The kids that you are working with struggle with algebra and do not retain information very well. Even though you spent a month talking about z-scores and how to find them using the invNorm() function a lot of them are still confused as to what to do and need someone to spoonfeed them with simplified information that does not contain too many technical terms. Your challenge now is to teach them confidence intervals which involve dealing with a) the concept b) the math c) the interpretation and finally the d) misconceptions. As you can see, this is an uphill battle and what worsens it is the student apathy. You review how to find the corresponding z-scores of the middle 95th percentile using the invNorm() function. You explain that the first entry must be the area to the left however when they see $z_{\alpha}$ they think the area to the right. This has been an ongoing confusion for one month despite you repeating the same thing over weeks. Now you try giving them a motivating example: ""Lets say you wanted to figure out the population mean length of all the world's bald eagles' wingspan. This is our parameter. Remember a parameter is a numerical value assigned to a whole population. You take a sample of 100 bald eagles and measure their wingspan. We call this sample mean a point estimate. Do you guys think that this point estimate accurately reflects the population mean?"" (Introducing vocabulary) You transition into talking about 95% confidence intervals conceptually. They are lost. Then you present them this formula. They are now completely and hopelessly lost: $ \bar{x} - z_{\frac{\alpha}{2}}\sigma < \mu < \bar{x} + z_{\frac{\alpha}{2}}\sigma $ You draw a picture showing that the level of confidence is $1-\alpha$ and the remaining areas we don't want are $\frac{\alpha}{2}$ and $\frac{\alpha}{2}$. They don't get what's going on. Twenty minutes in, there are some students not paying attention anymore and doing another class' work.",,"['statistics', 'education']"
41,Reference request for stochastic process and applications,Reference request for stochastic process and applications,,"I am looking for a text book that will cover the following topics I hope someone could suggest me a good text book that will provide me a good guidance regarding the following; Generating functions, Convolution, Compounding, Random walks, Recurrent events, Discrete parameter Markov Chains, Continuous parameter Markov Chains, Birth and Death processes, Queuing processes. I have no idea of stochastic processes yet so any book with good explanations will be very useful. Thank you.","I am looking for a text book that will cover the following topics I hope someone could suggest me a good text book that will provide me a good guidance regarding the following; Generating functions, Convolution, Compounding, Random walks, Recurrent events, Discrete parameter Markov Chains, Continuous parameter Markov Chains, Birth and Death processes, Queuing processes. I have no idea of stochastic processes yet so any book with good explanations will be very useful. Thank you.",,"['statistics', 'probability-theory', 'reference-request', 'stochastic-processes']"
42,Distribution of the sample variance,Distribution of the sample variance,,"This is my first post to this great website :) It seems like an excellent place to learn. I have a question however that is bothering me as I cannot figure it out through my textbook. The sample variance $$ S^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n}\left(X_i - \bar{X}\right)^2 $$ Of course, we need to assume that $X_i$ is normal but here is where I am very confused. The next step in my notes shows this: $$X_i - \bar{X} = \sigma [(X_i - \mu) / \sigma - (\bar{X} - \mu) / \sigma]$$ Where did $\sigma$ come from? And can anyone help me with the proof of this? It appears that the distribution of the sample variance is that of a Chi-Squared R.V. with $n-1$ DoF. Thanks so much!","This is my first post to this great website :) It seems like an excellent place to learn. I have a question however that is bothering me as I cannot figure it out through my textbook. The sample variance $$ S^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n}\left(X_i - \bar{X}\right)^2 $$ Of course, we need to assume that $X_i$ is normal but here is where I am very confused. The next step in my notes shows this: $$X_i - \bar{X} = \sigma [(X_i - \mu) / \sigma - (\bar{X} - \mu) / \sigma]$$ Where did $\sigma$ come from? And can anyone help me with the proof of this? It appears that the distribution of the sample variance is that of a Chi-Squared R.V. with $n-1$ DoF. Thanks so much!",,"['statistics', 'statistical-inference']"
43,Variance deduction,Variance deduction,,"the definition of variance is  $V(X) = E((X-E(X))^2 )$ For a discrete random variable: if  we have put $Y = g(X)$ , where $g$ is a real function $E(Y) = E(g(X)) = \sum\limits_{k} g(k)p_X(k)$  , where $p_X(k) = P(X = k) $ how can i use this formula to show that: $V(X) = \sum\limits_{k} (k-E(X))^2 p_X(k)$","the definition of variance is  $V(X) = E((X-E(X))^2 )$ For a discrete random variable: if  we have put $Y = g(X)$ , where $g$ is a real function $E(Y) = E(g(X)) = \sum\limits_{k} g(k)p_X(k)$  , where $p_X(k) = P(X = k) $ how can i use this formula to show that: $V(X) = \sum\limits_{k} (k-E(X))^2 p_X(k)$",,"['probability', 'statistics', 'probability-distributions']"
44,Searching for a probability distribution appropriate for my task,Searching for a probability distribution appropriate for my task,,"I'm making a game (not important), but I'd like to have real probability distribution function (instead of classical dice notation). I like the normal distribution , but I would like to also shift the odds so to speak to a given side. My crude drawing (not necessarily accurate): I've tried to shift the function along x-axis, but that isn't it. Because sum must be always 100% (1.0) obviously. So the question is. Is there some way to make normal distribution behave like this? If not, is there some other parametric function (or distribution), that allows me to do it? regards, Kate Update : Solution in Wolfram Mathematica (for anyone interested in this): Manipulate[ListPlot[Table[{k, PDF[BetaBinomialDistribution[a, b, 50], k]}, {k, 0, 50}], Filling -> Axis], {a, 1, 3, 0.1}, {b, 3, 1, 0.1}]","I'm making a game (not important), but I'd like to have real probability distribution function (instead of classical dice notation). I like the normal distribution , but I would like to also shift the odds so to speak to a given side. My crude drawing (not necessarily accurate): I've tried to shift the function along x-axis, but that isn't it. Because sum must be always 100% (1.0) obviously. So the question is. Is there some way to make normal distribution behave like this? If not, is there some other parametric function (or distribution), that allows me to do it? regards, Kate Update : Solution in Wolfram Mathematica (for anyone interested in this): Manipulate[ListPlot[Table[{k, PDF[BetaBinomialDistribution[a, b, 50], k]}, {k, 0, 50}], Filling -> Axis], {a, 1, 3, 0.1}, {b, 3, 1, 0.1}]",,"['statistics', 'functions', 'probability-distributions', 'parametric']"
45,How to solve the transcendental equation $a^h=bh+c$ with a parameter,How to solve the transcendental equation  with a parameter,a^h=bh+c,"I've got a random Rayleigh variable $\xi$ with $p_\xi(x)=\frac{x}{\sigma^2}\exp\{-\frac{x}{2\sigma^2}\},x\geq0$ There are two hypotheses: $H_0:\sigma=\sigma_0$ and $H_1:\sigma=\sigma_1$ I have built the procedure for Wald sequential analysis and now I have to find $P\{\text{deny } H_0\Bigr|\sigma=a\}=\frac{1-B^h}{A^h-B^h}$ Now I have to find h from the following equation: $$\int\limits_0^{+\infty}\left (\frac{p_1(x)}{p_0(x)}\right)^hp(x;a)=1$$ There are given $\sigma_0=2.2$ and $\sigma_1=2.7$ in the task. After I had simplified the integral, I got the following equation: $$\left (\frac{22}{27}\right)^h=1-\frac{6125}{8809}ha^2$$, that I have to solve for $h$. I think I must get W-Lambert function somehow but I have no ideas so far. I would appreciate any hints.","I've got a random Rayleigh variable $\xi$ with $p_\xi(x)=\frac{x}{\sigma^2}\exp\{-\frac{x}{2\sigma^2}\},x\geq0$ There are two hypotheses: $H_0:\sigma=\sigma_0$ and $H_1:\sigma=\sigma_1$ I have built the procedure for Wald sequential analysis and now I have to find $P\{\text{deny } H_0\Bigr|\sigma=a\}=\frac{1-B^h}{A^h-B^h}$ Now I have to find h from the following equation: $$\int\limits_0^{+\infty}\left (\frac{p_1(x)}{p_0(x)}\right)^hp(x;a)=1$$ There are given $\sigma_0=2.2$ and $\sigma_1=2.7$ in the task. After I had simplified the integral, I got the following equation: $$\left (\frac{22}{27}\right)^h=1-\frac{6125}{8809}ha^2$$, that I have to solve for $h$. I think I must get W-Lambert function somehow but I have no ideas so far. I would appreciate any hints.",,"['statistics', 'transcendental-equations']"
46,Calculate the expectation of Inverse Bessel Process,Calculate the expectation of Inverse Bessel Process,,"Simply put, I want to calculate the integral:  $$(2\pi t)^{-3/2}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\frac{e^{-\frac{(x-a)^2+(y-b)^2+(z-c)^2}{2t}}}{\sqrt{x^2+y^2+z^2}}dzdydx$$ This integral is used to calculate the expecation of Inverse Bessel Process starts from $(a,b,c)$, and when $(a,b,c)=(1,0,0)$ the calculation is shown in a paper about local martingales and filtration shrinkage. The idea in the paper is to change of variable of polar coordinates, but the technique is  useful when $b=c=0$, however for arbitrary $b$ and $c$, I try the same technique, which integrates over $z$ and $y$ first, and run into integral like change of variable $$y=r\cos{\theta},z=r\sin{\theta}$$ $$\int_{0}^{2\pi}\int_{0}^{\infty}\frac{e^{-\frac{(r\cos{\theta}-b)^2+(r\sin{\theta}-c)^2}{2t}}}{\sqrt{x^2+r^2}}rdrd\theta$$ change of variable again $$\nu=\sqrt{x^2+r^2}$$ $$\int_{0}^{2\pi}\int_{0}^{\infty}e^{-\frac{\nu^2-x^2-2(b\cos{\theta}+c\sin{\theta})\sqrt{\nu^2-x^2}+b^2+c^2}{2t}}d\nu d\theta$$ Then I'm stuck here. I can't think of a change of variable that would further simplify this integral. Anybody can show me how to do the original integral? You can ignore my attempt and use a new method.","Simply put, I want to calculate the integral:  $$(2\pi t)^{-3/2}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\frac{e^{-\frac{(x-a)^2+(y-b)^2+(z-c)^2}{2t}}}{\sqrt{x^2+y^2+z^2}}dzdydx$$ This integral is used to calculate the expecation of Inverse Bessel Process starts from $(a,b,c)$, and when $(a,b,c)=(1,0,0)$ the calculation is shown in a paper about local martingales and filtration shrinkage. The idea in the paper is to change of variable of polar coordinates, but the technique is  useful when $b=c=0$, however for arbitrary $b$ and $c$, I try the same technique, which integrates over $z$ and $y$ first, and run into integral like change of variable $$y=r\cos{\theta},z=r\sin{\theta}$$ $$\int_{0}^{2\pi}\int_{0}^{\infty}\frac{e^{-\frac{(r\cos{\theta}-b)^2+(r\sin{\theta}-c)^2}{2t}}}{\sqrt{x^2+r^2}}rdrd\theta$$ change of variable again $$\nu=\sqrt{x^2+r^2}$$ $$\int_{0}^{2\pi}\int_{0}^{\infty}e^{-\frac{\nu^2-x^2-2(b\cos{\theta}+c\sin{\theta})\sqrt{\nu^2-x^2}+b^2+c^2}{2t}}d\nu d\theta$$ Then I'm stuck here. I can't think of a change of variable that would further simplify this integral. Anybody can show me how to do the original integral? You can ignore my attempt and use a new method.",,"['statistics', 'multivariable-calculus', 'stochastic-processes', 'normal-distribution', 'stochastic-calculus']"
47,Geometric Mean of an Interval,Geometric Mean of an Interval,,"Is there any way to calculate one? If it't not clear what I mean, here's an example: Starting with an initial value, keep multiplying it by some random amount on the interval [1, 2] at regular intervals, indefinitely. What if you want to compute the expected value of the product after a given amount of time. If it was randomly multiplying by a discrete set of values (1, 1.1, 1.2, ..., 2), then we'd need only take the 11th root of the product of these 11 values. (geometric mean) But if it's varying continuously on 1 + continuum, then I don't see a straightforward way to compute the expected factor. I wrote a program in python that tries to approximate this: parameters startInterval = 1 endInterval = 2 numSubIntervals = 1000000 variables product = 1 intervalStep = (float(endInterval) - float(startInterval))/numSubIntervals exponent = float(1)/numSubIntervals calculation for i in range(1, numSubIntervals+1):     product = product * pow((startInterval + intervalStep*i), exponent) output print ""geometric mean of the interval ("", startInterval, "", "", endInterval, "") is "", product And as far as I can test, this works. But is there some formula or more simplistic approach to this computation?","Is there any way to calculate one? If it't not clear what I mean, here's an example: Starting with an initial value, keep multiplying it by some random amount on the interval [1, 2] at regular intervals, indefinitely. What if you want to compute the expected value of the product after a given amount of time. If it was randomly multiplying by a discrete set of values (1, 1.1, 1.2, ..., 2), then we'd need only take the 11th root of the product of these 11 values. (geometric mean) But if it's varying continuously on 1 + continuum, then I don't see a straightforward way to compute the expected factor. I wrote a program in python that tries to approximate this: parameters startInterval = 1 endInterval = 2 numSubIntervals = 1000000 variables product = 1 intervalStep = (float(endInterval) - float(startInterval))/numSubIntervals exponent = float(1)/numSubIntervals calculation for i in range(1, numSubIntervals+1):     product = product * pow((startInterval + intervalStep*i), exponent) output print ""geometric mean of the interval ("", startInterval, "", "", endInterval, "") is "", product And as far as I can test, this works. But is there some formula or more simplistic approach to this computation?",,"['probability', 'statistics', 'average']"
48,Covariance matrix for 2 vectors with elements in the plane,Covariance matrix for 2 vectors with elements in the plane,,"I am given 2 vectors of points in the $R^2$ plane and I want to compute the covariance matrix of the two vectors. I know this will result in a 2x2 matrix. However, I am slightly confused. From Wikipedia, and from what I know, the covariance applies to vectors of random variables. In this case I do not have vectors of random variables, but rather a set of n samples from 2 random variables (one which yields the first vector and the other one which yields the second vector). I would like to use these samples in computing the variance, rather than estimating the two random variables in the underlying distributions. I believe I lack some sort of understanding in the matter, and an explanation (rather than a simple answer) would be more than welcome. Edit: this issue arises from me trying to understand the matlab function cov . Thank you!","I am given 2 vectors of points in the $R^2$ plane and I want to compute the covariance matrix of the two vectors. I know this will result in a 2x2 matrix. However, I am slightly confused. From Wikipedia, and from what I know, the covariance applies to vectors of random variables. In this case I do not have vectors of random variables, but rather a set of n samples from 2 random variables (one which yields the first vector and the other one which yields the second vector). I would like to use these samples in computing the variance, rather than estimating the two random variables in the underlying distributions. I believe I lack some sort of understanding in the matter, and an explanation (rather than a simple answer) would be more than welcome. Edit: this issue arises from me trying to understand the matlab function cov . Thank you!",,"['linear-algebra', 'statistics']"
49,$E[X_i]=\mu$ Statistics and Probability Mysteries,Statistics and Probability Mysteries,E[X_i]=\mu,"I realize I have over-flooded this forum with questions regarding probability recently, but I do not have access to a teacher, and have to learn these things by myself. I have made some good progress compared to what I knew about probability some weeks ago BUT, I am still very unsure of myself, and ... I have to say I found it difficult to find reliable sources on the topic, and text books or documents on the internet explaining things really clearly. For example, while learning (reading) about sampling distribution and CLT, I wanted to understand why $E[\bar X] = \mu$. I think I get it right here! So I understand this from all the books I read on the topic. You have a population (lets say finite) and you have lots elements in it which you call $x_1, x_2, ... x_N$ where $N$ is the total number of elements in the population (since the population is finite). Good. Now you can compute the population mean as: $\mu = {1\over N}\sum_{i=1}^N x_i$ The expected value equation is: $E[X] = \sum_{i=1} {p_i X_i}$ Imagine you have 10 cards labelled with numbers on them (0, 1, or 2), and that you have 3 0s, 5 1s, and 2 2s. Now you could compute the mean of this population by doing: $\mu = {1 \over 10}(0+0+1+1+1+1+1+2+2+2) = 1.1$$ and you could use the expected value formulation, knowing that assuming you know your population distribution before hand, you can say that your sample space is S={0,1,2} ( is that correct, your sample space are the possible outcomes? ). So the probability distribution would be Pr(0) = 0.2, Pr(1) = 0.5 and Pr(2) = 0.3. Thus: E[X] = 0.2 * 0 + 0.5 * 1 + 0.3 * 2 = 1.1$$ Great we get the same results. Now we don't want (because the population is too large or even infinite) to compute the population mean so we use a sample mean instead. We draw elements from the population, for instance $n$ of them and compute their average: $\bar X = {1 \over n} \sum_{i=1}^n X_i$ I think I am confused here because we use $X_i$ on the right inside instead of $x$. I understand it this way: imagine you randomly draw a number between 1 and 10, imagine it is r = 3, and this number indicates the card you need to pick from the population of 10 cards (you shuffled the cards before and put them on the table from left to right). The number on this card is 2. So what $X_i$ is in the above equation for the sample mean is this number 2. Correct? This is our random variable which we could write $X_1 = 2$ (we put the card back on the table, draw with replacement, shuffle the cards again, etc.) and we repeat this experience $n$ times to get the following series of Xs: $X_1 = 2, X_2 = 0, X_3 = 1, ... X_n = 2$. And finally we compute our sample mean which is an average of all these Xs: $\bar X = {1 \over n} (2 + 0 + 1 + ... 2)$ where the $1 \over n$ is there because again the way we drew the cards from the population was done in a ""uniform"" way (we didn't ""give more importance"" to cards on the left for instance when we drew them). So back to our problem, proving that $E[\bar X] = \mu$. I think I get it wrong here! Unfortunately I am lost here! So if I understand the equation for the expected mean correctly it's a sum of weighted random variables (where the weight is the probability associated with this random variable). So technically if we wanted to calculated the expected value of a sample mean (where the sample mean is already a mean of drawn elements from the population), we would need to calculate many samples means, weight them with their associated probability (which I understand in this case is close to a normal distribution) and add up the results. No ? $E[\bar X] = \sum_{i=1}^{NN} w_i \bar X_i$ where $\bar X_1 = {1 \over n} \sum_{i=1}^n X_i$, $\bar X_2 = {1 \over n} \sum_{i=1}^n X_i$, etc. So I think I am thinking wrong at this point because in books they define this expected value differently, they say since: $\bar X = {1 \over n} \sum_{i=1}^n X_i$ thus $E[\bar X] = E[{1 \over n} \sum_{i=1}^n X_i]$ $E[\bar X] = {1 \over n}E[X_1] + {1 \over n}E[X_2] + ... {1 \over n}E[X_n]$ and then they say something incredible in the sense to me not justified at all, which is that ""since the expected value of a random value is the mean"" (WHERE IS THAT PROVED?) EDIT: so I realise that I wrote above that $E[X]$ and the $\mu$ with my card example were the same, the problem I have with that, is that my example was only working with discrete r.v. and finite population. I believe to hold true, the $E[\bar X] = \mu$ equality should be proved for any population (finite or not) and a r.v that is either continuous or discrete. Does such a proof exist? then we have: $E[\bar X] = n \times {1 \over n} \mu = \mu$ So where I would really like to get some help, is really on the last part. 1) why do we compute the expected value of the mean as $E[\bar X] = E[{1 \over n} \sum_{i=1}^n X_i]$ rather than with the equation for the expected value $\sum_{i=1} {p_i X_i}$? 2) where is this coming from? ""Since $X_1, X_2, ... X_n$ are each random variable, their expected value will be equal to the probability mean $\mu$."" ????? I know the post is very long, but I hope this will be useful to many more people in the future, trying to understand why we use x rather than X, etc., etc. But if the real mathematicians out there, could please help me doing the last miles on my journey to ""starting to understand"" statistics it would be FANTASTIC! Many thanks in advance.","I realize I have over-flooded this forum with questions regarding probability recently, but I do not have access to a teacher, and have to learn these things by myself. I have made some good progress compared to what I knew about probability some weeks ago BUT, I am still very unsure of myself, and ... I have to say I found it difficult to find reliable sources on the topic, and text books or documents on the internet explaining things really clearly. For example, while learning (reading) about sampling distribution and CLT, I wanted to understand why $E[\bar X] = \mu$. I think I get it right here! So I understand this from all the books I read on the topic. You have a population (lets say finite) and you have lots elements in it which you call $x_1, x_2, ... x_N$ where $N$ is the total number of elements in the population (since the population is finite). Good. Now you can compute the population mean as: $\mu = {1\over N}\sum_{i=1}^N x_i$ The expected value equation is: $E[X] = \sum_{i=1} {p_i X_i}$ Imagine you have 10 cards labelled with numbers on them (0, 1, or 2), and that you have 3 0s, 5 1s, and 2 2s. Now you could compute the mean of this population by doing: $\mu = {1 \over 10}(0+0+1+1+1+1+1+2+2+2) = 1.1$$ and you could use the expected value formulation, knowing that assuming you know your population distribution before hand, you can say that your sample space is S={0,1,2} ( is that correct, your sample space are the possible outcomes? ). So the probability distribution would be Pr(0) = 0.2, Pr(1) = 0.5 and Pr(2) = 0.3. Thus: E[X] = 0.2 * 0 + 0.5 * 1 + 0.3 * 2 = 1.1$$ Great we get the same results. Now we don't want (because the population is too large or even infinite) to compute the population mean so we use a sample mean instead. We draw elements from the population, for instance $n$ of them and compute their average: $\bar X = {1 \over n} \sum_{i=1}^n X_i$ I think I am confused here because we use $X_i$ on the right inside instead of $x$. I understand it this way: imagine you randomly draw a number between 1 and 10, imagine it is r = 3, and this number indicates the card you need to pick from the population of 10 cards (you shuffled the cards before and put them on the table from left to right). The number on this card is 2. So what $X_i$ is in the above equation for the sample mean is this number 2. Correct? This is our random variable which we could write $X_1 = 2$ (we put the card back on the table, draw with replacement, shuffle the cards again, etc.) and we repeat this experience $n$ times to get the following series of Xs: $X_1 = 2, X_2 = 0, X_3 = 1, ... X_n = 2$. And finally we compute our sample mean which is an average of all these Xs: $\bar X = {1 \over n} (2 + 0 + 1 + ... 2)$ where the $1 \over n$ is there because again the way we drew the cards from the population was done in a ""uniform"" way (we didn't ""give more importance"" to cards on the left for instance when we drew them). So back to our problem, proving that $E[\bar X] = \mu$. I think I get it wrong here! Unfortunately I am lost here! So if I understand the equation for the expected mean correctly it's a sum of weighted random variables (where the weight is the probability associated with this random variable). So technically if we wanted to calculated the expected value of a sample mean (where the sample mean is already a mean of drawn elements from the population), we would need to calculate many samples means, weight them with their associated probability (which I understand in this case is close to a normal distribution) and add up the results. No ? $E[\bar X] = \sum_{i=1}^{NN} w_i \bar X_i$ where $\bar X_1 = {1 \over n} \sum_{i=1}^n X_i$, $\bar X_2 = {1 \over n} \sum_{i=1}^n X_i$, etc. So I think I am thinking wrong at this point because in books they define this expected value differently, they say since: $\bar X = {1 \over n} \sum_{i=1}^n X_i$ thus $E[\bar X] = E[{1 \over n} \sum_{i=1}^n X_i]$ $E[\bar X] = {1 \over n}E[X_1] + {1 \over n}E[X_2] + ... {1 \over n}E[X_n]$ and then they say something incredible in the sense to me not justified at all, which is that ""since the expected value of a random value is the mean"" (WHERE IS THAT PROVED?) EDIT: so I realise that I wrote above that $E[X]$ and the $\mu$ with my card example were the same, the problem I have with that, is that my example was only working with discrete r.v. and finite population. I believe to hold true, the $E[\bar X] = \mu$ equality should be proved for any population (finite or not) and a r.v that is either continuous or discrete. Does such a proof exist? then we have: $E[\bar X] = n \times {1 \over n} \mu = \mu$ So where I would really like to get some help, is really on the last part. 1) why do we compute the expected value of the mean as $E[\bar X] = E[{1 \over n} \sum_{i=1}^n X_i]$ rather than with the equation for the expected value $\sum_{i=1} {p_i X_i}$? 2) where is this coming from? ""Since $X_1, X_2, ... X_n$ are each random variable, their expected value will be equal to the probability mean $\mu$."" ????? I know the post is very long, but I hope this will be useful to many more people in the future, trying to understand why we use x rather than X, etc., etc. But if the real mathematicians out there, could please help me doing the last miles on my journey to ""starting to understand"" statistics it would be FANTASTIC! Many thanks in advance.",,"['probability', 'statistics', 'expectation']"
50,Distance of two normal distribution functions,Distance of two normal distribution functions,,"Let $\Phi(u)$ be the standard normal distribution function. We want to consider the following expression $\|\Phi(u/\sigma_l)-\Phi(u)\|_{\infty}$, where $\sigma_l\xrightarrow{l\rightarrow\infty}1$. For convenience let's say $\sigma_l=1-\frac{1}{l}$. Is it possible to approximate the expression above in the following form: \begin{align*}\|\Phi(u/\sigma_l)-\Phi(u)\|_{\infty}\leq f(l)\end{align*} where $f:\mathbb{N}\rightarrow\mathbb{R}$ with $\lim_{l\rightarrow\infty}f(l)=0$? Thanks a lot!","Let $\Phi(u)$ be the standard normal distribution function. We want to consider the following expression $\|\Phi(u/\sigma_l)-\Phi(u)\|_{\infty}$, where $\sigma_l\xrightarrow{l\rightarrow\infty}1$. For convenience let's say $\sigma_l=1-\frac{1}{l}$. Is it possible to approximate the expression above in the following form: \begin{align*}\|\Phi(u/\sigma_l)-\Phi(u)\|_{\infty}\leq f(l)\end{align*} where $f:\mathbb{N}\rightarrow\mathbb{R}$ with $\lim_{l\rightarrow\infty}f(l)=0$? Thanks a lot!",,"['statistics', 'probability-distributions']"
51,Basu's Theorem's application,Basu's Theorem's application,,"I can't solve a problem, for which I am suppose to use Basu's theorem. Suppose that $X$ and $Y$ are independent Exponential random variables with common parameter $\lambda$. I have to show that $X + Y$ and $\frac X{X + Y}$ are independent. Can you help me on this? Thank you!","I can't solve a problem, for which I am suppose to use Basu's theorem. Suppose that $X$ and $Y$ are independent Exponential random variables with common parameter $\lambda$. I have to show that $X + Y$ and $\frac X{X + Y}$ are independent. Can you help me on this? Thank you!",,"['statistics', 'random-variables', 'statistical-inference']"
52,Markov chain transition matrix,Markov chain transition matrix,,"if $P$ and $Q$ are $n \times n$ transition matrices for two Markov chain,  then product $R=PQ$ is also a transition matrix. is this true ? why is it ? looks like product of transition matrix means transition $P$ and $Q$.","if $P$ and $Q$ are $n \times n$ transition matrices for two Markov chain,  then product $R=PQ$ is also a transition matrix. is this true ? why is it ? looks like product of transition matrix means transition $P$ and $Q$.",,"['statistics', 'markov-chains']"
53,Propagation of Error,Propagation of Error,,"If I have a function $$f(x,y)=\sqrt{x^2+y^2}$$ with error in $x$ be $\Delta x$ and error in $y$ be $\Delta y$, then how do we calculate ${\Delta f}$? I know if we have $$f(x)=x^n$$, then I at least that $\frac{\Delta f}{f}=|n|\frac{\Delta x}{x}$. I have no idea how to proceed.","If I have a function $$f(x,y)=\sqrt{x^2+y^2}$$ with error in $x$ be $\Delta x$ and error in $y$ be $\Delta y$, then how do we calculate ${\Delta f}$? I know if we have $$f(x)=x^n$$, then I at least that $\frac{\Delta f}{f}=|n|\frac{\Delta x}{x}$. I have no idea how to proceed.",,"['statistics', 'error-propagation']"
54,Statistics notation,Statistics notation,,"I came across this notation and am not sure what it means. $\downarrow$ and $\uparrow$. For example. Let $A$ be a sample space and we can divide $A$ into $n$ disjoint sets. $A_N \subset A_{N-1}\dots A_2 \subset A_1$ be an infinite number of subsets such that $A_N \downarrow A$. Then $P(A_N)\to P(A)$ when $n\to\infty$. What does the down arrow mean? and conversely what does an up arrow mean? Edit: I misread the question, I have corrected the question above.","I came across this notation and am not sure what it means. $\downarrow$ and $\uparrow$. For example. Let $A$ be a sample space and we can divide $A$ into $n$ disjoint sets. $A_N \subset A_{N-1}\dots A_2 \subset A_1$ be an infinite number of subsets such that $A_N \downarrow A$. Then $P(A_N)\to P(A)$ when $n\to\infty$. What does the down arrow mean? and conversely what does an up arrow mean? Edit: I misread the question, I have corrected the question above.",,"['probability', 'statistics']"
55,Conditional Expectation for Geometric Series - Dice problem,Conditional Expectation for Geometric Series - Dice problem,,"A fair die is rolled repeatedly.  Let $X$ be the number of rolls needed to obtain a 5 and $Y$ the number of rolls needed to obtain a 6.  Calculate $E[X \mid Y=2]$. I found a similar post of this same question here: A question on conditional expectation But I still do not understand the solution after reading it.  If someone could hold my hand a little and provide a more thorough explanation I would really appreciate it. This is what's going through my head: Since $Y=2$, we roll a 6 on the second roll, meaning that we do not get a 6 on the first roll.  Therefore, the first roll has a uniform distribution of the set $\{1,2,3,4,5\}$.  We also know that $\Pr(X=2 \mid Y=2)=0$ because these events are mutually exclusive.  We can apply the definition of expected value now and I let $Z=X \mid (Y=2)$. $E[Z]=\sum\limits_{z=1,z \ge 3}^{\infty} z \cdot \Pr[Z] \tag{1}$ Now: $\Pr[Z=1]=\Pr[X=1 \mid Y=2] = \Pr[5 \text{ on first roll} \mid 6 \text{ on second roll}]=1/5$ $\Pr[Z=2]=0$ as mentioned above. Now I get confused when I calculate probabilities for $Z \ge 3$, and would appreciate some guidance: $\Pr[Z=3]=\Pr[X=3 \mid Y=2] =\Pr[5 \text{ on third roll} \mid 6 \text{ on second roll}]$ So to calculate this probability, I thought I'd break this case down: Question 1: I thought the first roll only can be from the set:  $\{1,2,3,4\}$.  Second roll = 6, and Third roll = 5.  But I think the solution to this probability is: $(4/5)(1)(1/6)$.  I don't see why the probability of the first roll is (4/5) because for this case, we can only get a 6 on the second roll.  Can someone please explain and perhaps illustrate another example like $Pr[Z=4]$? Question 2: My approach then was to find the equation for $\Pr[Z \ge 3]$ and then apply equation (1) to get the solution.  Is this the best approach?  I was reading another solution but did not understand it: Thanks in advance.","A fair die is rolled repeatedly.  Let $X$ be the number of rolls needed to obtain a 5 and $Y$ the number of rolls needed to obtain a 6.  Calculate $E[X \mid Y=2]$. I found a similar post of this same question here: A question on conditional expectation But I still do not understand the solution after reading it.  If someone could hold my hand a little and provide a more thorough explanation I would really appreciate it. This is what's going through my head: Since $Y=2$, we roll a 6 on the second roll, meaning that we do not get a 6 on the first roll.  Therefore, the first roll has a uniform distribution of the set $\{1,2,3,4,5\}$.  We also know that $\Pr(X=2 \mid Y=2)=0$ because these events are mutually exclusive.  We can apply the definition of expected value now and I let $Z=X \mid (Y=2)$. $E[Z]=\sum\limits_{z=1,z \ge 3}^{\infty} z \cdot \Pr[Z] \tag{1}$ Now: $\Pr[Z=1]=\Pr[X=1 \mid Y=2] = \Pr[5 \text{ on first roll} \mid 6 \text{ on second roll}]=1/5$ $\Pr[Z=2]=0$ as mentioned above. Now I get confused when I calculate probabilities for $Z \ge 3$, and would appreciate some guidance: $\Pr[Z=3]=\Pr[X=3 \mid Y=2] =\Pr[5 \text{ on third roll} \mid 6 \text{ on second roll}]$ So to calculate this probability, I thought I'd break this case down: Question 1: I thought the first roll only can be from the set:  $\{1,2,3,4\}$.  Second roll = 6, and Third roll = 5.  But I think the solution to this probability is: $(4/5)(1)(1/6)$.  I don't see why the probability of the first roll is (4/5) because for this case, we can only get a 6 on the second roll.  Can someone please explain and perhaps illustrate another example like $Pr[Z=4]$? Question 2: My approach then was to find the equation for $\Pr[Z \ge 3]$ and then apply equation (1) to get the solution.  Is this the best approach?  I was reading another solution but did not understand it: Thanks in advance.",,"['probability', 'statistics']"
56,Convergence almost everywhere?,Convergence almost everywhere?,,"Consider the following two statements about a random sequence $X_n$: (1) $X_n \stackrel{a.e.}{\rightarrow} X$. (2) $\mathrm{P}\{|X_n-X|>\epsilon, \ i.o.\} = 0, \ \forall  \epsilon>0$. (a.e. stands for ""almost everywhere"" and i.o.  stands for ""infinitely often"".) What is the difference between statements (1) and (2), or are they equivalent? Does one imply the other? I appreciate any help!","Consider the following two statements about a random sequence $X_n$: (1) $X_n \stackrel{a.e.}{\rightarrow} X$. (2) $\mathrm{P}\{|X_n-X|>\epsilon, \ i.o.\} = 0, \ \forall  \epsilon>0$. (a.e. stands for ""almost everywhere"" and i.o.  stands for ""infinitely often"".) What is the difference between statements (1) and (2), or are they equivalent? Does one imply the other? I appreciate any help!",,"['probability', 'statistics', 'measure-theory']"
57,Determining Convergence of Power Series,Determining Convergence of Power Series,,"Flip a fair coin until you get the first ""head"".  Let X represent the number of flips before the first head appears.  Calculate E[X]. So I solved this problem and you get a power series: $E[X] = 1*0.5 + 2*0.5^2 + 3*0.5^3+ ...$ This is basically of the form $\sum\limits_{i=0}^{\infty}x_i(0.5)^i.$ I flipped open my calculus book to review how to solve this series but I didn't find anything on how to calculate the limiting value for a power series, just the radius of convergence. I see for a sum of infinite geometric series, the value is: $S_n = a+ ax + ax^2 + ... + ax^n + ... = \cfrac{a}{1-x}$ for |x| < 1. Can someone please tell me the general approach if there is one for a power series? Thank you in advance.","Flip a fair coin until you get the first ""head"".  Let X represent the number of flips before the first head appears.  Calculate E[X]. So I solved this problem and you get a power series: $E[X] = 1*0.5 + 2*0.5^2 + 3*0.5^3+ ...$ This is basically of the form $\sum\limits_{i=0}^{\infty}x_i(0.5)^i.$ I flipped open my calculus book to review how to solve this series but I didn't find anything on how to calculate the limiting value for a power series, just the radius of convergence. I see for a sum of infinite geometric series, the value is: $S_n = a+ ax + ax^2 + ... + ax^n + ... = \cfrac{a}{1-x}$ for |x| < 1. Can someone please tell me the general approach if there is one for a power series? Thank you in advance.",,"['calculus', 'sequences-and-series', 'statistics', 'power-series']"
58,what does it mean when $\operatorname{E}[X^2]$ diverges?,what does it mean when  diverges?,\operatorname{E}[X^2],"is it possible for a random variable $X$, such that the expected value of $X^2$, $\operatorname{E}[X^2]$ is a divergent integral? If it is impossible, does that mean the probability density function of $X$ is wrong? (the integral of the probability density function over the support does not equal to $1$) Also, since $\operatorname{Var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X] ^ 2$, does $\operatorname{Var}[X]$ still exist in this case?","is it possible for a random variable $X$, such that the expected value of $X^2$, $\operatorname{E}[X^2]$ is a divergent integral? If it is impossible, does that mean the probability density function of $X$ is wrong? (the integral of the probability density function over the support does not equal to $1$) Also, since $\operatorname{Var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X] ^ 2$, does $\operatorname{Var}[X]$ still exist in this case?",,"['probability', 'statistics']"
59,bitwise XOR of two bit pattern having diferent length,bitwise XOR of two bit pattern having diferent length,,how two bit patterns can be XORed if they have different length e.g one bit pattern is 11000 and other is 11000000111111. I have to do bit comparison of bit patterns having length 5000 and 35000 so plz give me any formula or method to calculate it,how two bit patterns can be XORed if they have different length e.g one bit pattern is 11000 and other is 11000000111111. I have to do bit comparison of bit patterns having length 5000 and 35000 so plz give me any formula or method to calculate it,,"['real-analysis', 'statistics', 'logic', 'numerical-methods']"
60,Find the posterior distribution for an exponential prior and a Poisson likelihood,Find the posterior distribution for an exponential prior and a Poisson likelihood,,"I have a prior  $\lambda \sim \exp(1)$ and a likelihood $X \sim poisson(\lambda)$, and I observed in a sample of $n=5$ a mean  of $3$. What is the posterior distribution of $\lambda$? Here is my asnwer: $f(x|\lambda) = \frac{\lambda^{x}e^{-\lambda}}{x!}$ $f(\lambda) = \theta e^{-\theta \lambda}, \theta = 1 => f(\lambda) = e^{-\lambda }$ So, the posterior: $f(\lambda|x) \propto e^{-\lambda} \lambda^{x} e^{-\lambda +(-\lambda)} = \lambda^{x}e^{-2 \lambda} $ This posterior is some known distribution (e.g. exponential)?","I have a prior  $\lambda \sim \exp(1)$ and a likelihood $X \sim poisson(\lambda)$, and I observed in a sample of $n=5$ a mean  of $3$. What is the posterior distribution of $\lambda$? Here is my asnwer: $f(x|\lambda) = \frac{\lambda^{x}e^{-\lambda}}{x!}$ $f(\lambda) = \theta e^{-\theta \lambda}, \theta = 1 => f(\lambda) = e^{-\lambda }$ So, the posterior: $f(\lambda|x) \propto e^{-\lambda} \lambda^{x} e^{-\lambda +(-\lambda)} = \lambda^{x}e^{-2 \lambda} $ This posterior is some known distribution (e.g. exponential)?",,"['probability', 'statistics', 'probability-distributions', 'bayesian']"
61,Joint distribution of multiple binomial distributions,Joint distribution of multiple binomial distributions,,"In the picture below, how do they arrive at the joint density function? I understand how Binomial distributions work, but have never seen the joint distribution of them. The original file can be found here: Link","In the picture below, how do they arrive at the joint density function? I understand how Binomial distributions work, but have never seen the joint distribution of them. The original file can be found here: Link",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
62,average is higher than percentile 90 ?...,average is higher than percentile 90 ?...,,"I have a few hundreds of results from a test im running, and with a utility I have that analyzes these results I can see that the Average result value is higher than the percentile 90th value... This seems a little weird to me, since I know that most of the results recorder (exactly 89%) are below the percentile 90 value, therefore i figure this means that the 10% of the results above the percentile 90 value are probably very far from the median. What else can this tell me about my results ? Details on the results : average = 550, percentile 90 = 540. A valid results could be anything from 1 up to anything.","I have a few hundreds of results from a test im running, and with a utility I have that analyzes these results I can see that the Average result value is higher than the percentile 90th value... This seems a little weird to me, since I know that most of the results recorder (exactly 89%) are below the percentile 90 value, therefore i figure this means that the 10% of the results above the percentile 90 value are probably very far from the median. What else can this tell me about my results ? Details on the results : average = 550, percentile 90 = 540. A valid results could be anything from 1 up to anything.",,['statistics']
63,Is a Riemannian metric positive definite or positive semidefinite?,Is a Riemannian metric positive definite or positive semidefinite?,,"From Wikipedia The Fisher information matrix is a N x N positive semidefinite symmetric matrix, defining a Riemannian metric on the N-dimensional parameter space, But a Riemannian metric is defined to be a family of (positive definite) inner products. So I was wondering whether a Riemannian metric is positive definite or positive semidefinite? Thanks and regards!","From Wikipedia The Fisher information matrix is a N x N positive semidefinite symmetric matrix, defining a Riemannian metric on the N-dimensional parameter space, But a Riemannian metric is defined to be a family of (positive definite) inner products. So I was wondering whether a Riemannian metric is positive definite or positive semidefinite? Thanks and regards!",,"['probability-theory', 'statistics', 'riemannian-geometry', 'information-geometry']"
64,What makes terms of the sum in variance raise to the power of $2$ not $4$ or $6$?,What makes terms of the sum in variance raise to the power of  not  or ?,2 4 6,"If I change the power of the variance to $4$ or $6$, do the properties of the new formula remain the same as the old variance formula? If not what is the difference? Variance formula: $$\sigma^2=\frac{\sum(X-\mu)^2}{N}$$","If I change the power of the variance to $4$ or $6$, do the properties of the new formula remain the same as the old variance formula? If not what is the difference? Variance formula: $$\sigma^2=\frac{\sum(X-\mu)^2}{N}$$",,['statistics']
65,"Are the multiplications of i.i.d random variables , i.i.d?","Are the multiplications of i.i.d random variables , i.i.d?",,"If we know that $X_1$ and $X_2$ are i.i.d random variables, and $Z_1$ and $Z_2$ are also i.i.d random variables, can we say $X_1Z_1$  and $X_2Z_2$ are i.i.d random variables too? suppose that $X_1$ ,$X_2$, $Z_1$ and $Z_2$ are all independent random variables.","If we know that $X_1$ and $X_2$ are i.i.d random variables, and $Z_1$ and $Z_2$ are also i.i.d random variables, can we say $X_1Z_1$  and $X_2Z_2$ are i.i.d random variables too? suppose that $X_1$ ,$X_2$, $Z_1$ and $Z_2$ are all independent random variables.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'random-variables']"
66,Trying to work out $\operatorname{Var}(\bar{X})$ of Pareto distribution,Trying to work out  of Pareto distribution,\operatorname{Var}(\bar{X}),Pareto distribution with pdf $$f(x;\lambda)=\lambda x^{-(\lambda-1)} \;  $$ Using the standard equation for the variance I have that: $$\operatorname{Var}(\bar{X})=E[\bar{X^2}]-(E[\bar{X}])^2$$ I have $E[\bar{X}]=\frac{\lambda}{\lambda-1}$ but I'm having trouble with working out $E[\bar{X^2}]$ So far I've got: $$E[\bar{X^2}]=E\left[\left(\frac{1}{n}\sum X_i\right)^2\right] = \frac{1}{n^2} E\left[\left(\sum X_i\right)^2\right]$$ but I don't know how to go on from there. Thanks very much for you help!,Pareto distribution with pdf $$f(x;\lambda)=\lambda x^{-(\lambda-1)} \;  $$ Using the standard equation for the variance I have that: $$\operatorname{Var}(\bar{X})=E[\bar{X^2}]-(E[\bar{X}])^2$$ I have $E[\bar{X}]=\frac{\lambda}{\lambda-1}$ but I'm having trouble with working out $E[\bar{X^2}]$ So far I've got: $$E[\bar{X^2}]=E\left[\left(\frac{1}{n}\sum X_i\right)^2\right] = \frac{1}{n^2} E\left[\left(\sum X_i\right)^2\right]$$ but I don't know how to go on from there. Thanks very much for you help!,,"['statistics', 'probability-distributions']"
67,Test for Bernoulli distribution,Test for Bernoulli distribution,,"I'm thinking about the following problem. Assume that one is given a sequence of binary data $(x_i)_i\in\{0,1\}^n$ and that wants to determine whether or not this sequence has been generated by a Bernoulli-like random mechanism, such as coin tosses. My first idea was to use classical statistical test theory, but the null hypothesis $$ H_0=\{\text{data are not generated by a Bernoulli mechanism}\} $$ seems difficult to model mathematically. In particular, it includes the possibility that the data are not generated by any well-defined random mechanism. I have a feeling that any probabilistic reasoning breaks down, if one is not willing to make the assumption of an underlying distribution from which the observed data are sampled. My second idea was not to try and refute the (ill-defined) null-hypothesis that the data are not Bernoulli, but rather to extract from the data as much evidence as possible to show that it is consistent with the hypothesis that the data are generated by a Bernoulli-like mechanism. One example would be to subdivide (maybe randomly)  the  observed data into (maybe disjoint) subsets, count the number of 1's in each subset, and compare the empirical distribution of the number of 1's to a binomial distribution. This raises the question, how the subdivisions should be done, if the order of the data matters, and if the whole approach makes any sense. It seems to contradict the fact that the number of 1's is a sufficient test statistics for a repeated Bernoulli experiment. My question is thus the following: What methods can one use to establish that a sequence of binary data has been generated by a Bernoulli-like random mechanism? Alternatively, how can one refute the possibility that the data have been generated by some unspecified non-Bernoulli-like random mechanism or that they have not been generated by a well-defined random mechanism at all? I'm aware that this is not a very concrete question. Any input is appreciated.","I'm thinking about the following problem. Assume that one is given a sequence of binary data $(x_i)_i\in\{0,1\}^n$ and that wants to determine whether or not this sequence has been generated by a Bernoulli-like random mechanism, such as coin tosses. My first idea was to use classical statistical test theory, but the null hypothesis $$ H_0=\{\text{data are not generated by a Bernoulli mechanism}\} $$ seems difficult to model mathematically. In particular, it includes the possibility that the data are not generated by any well-defined random mechanism. I have a feeling that any probabilistic reasoning breaks down, if one is not willing to make the assumption of an underlying distribution from which the observed data are sampled. My second idea was not to try and refute the (ill-defined) null-hypothesis that the data are not Bernoulli, but rather to extract from the data as much evidence as possible to show that it is consistent with the hypothesis that the data are generated by a Bernoulli-like mechanism. One example would be to subdivide (maybe randomly)  the  observed data into (maybe disjoint) subsets, count the number of 1's in each subset, and compare the empirical distribution of the number of 1's to a binomial distribution. This raises the question, how the subdivisions should be done, if the order of the data matters, and if the whole approach makes any sense. It seems to contradict the fact that the number of 1's is a sufficient test statistics for a repeated Bernoulli experiment. My question is thus the following: What methods can one use to establish that a sequence of binary data has been generated by a Bernoulli-like random mechanism? Alternatively, how can one refute the possibility that the data have been generated by some unspecified non-Bernoulli-like random mechanism or that they have not been generated by a well-defined random mechanism at all? I'm aware that this is not a very concrete question. Any input is appreciated.",,"['probability', 'statistics', 'statistical-inference']"
68,"X has pdf $f(x) = \frac{x^{2}}{18}$ for -3<x<3, what is the pdf of $X^{2}$","X has pdf  for -3<x<3, what is the pdf of",f(x) = \frac{x^{2}}{18} X^{2},"So this was my solution: Say, $Z = X^{2}$, then $X=\pm \sqrt{Z}$ and, $$P(Z=z)=P(X = \sqrt{z}) + P(X = -\sqrt{z}) = \frac{z}{18} + \frac{z}{18} = \frac{z}{9}$$ for $$0<z<9$$ However: $$\int_{0}^{9}P(z) dz = \int_{0}^{9}\frac{z}{9}dz = \frac{9}{2} \neq 1$$ Where did I go wrong? Edit: I tried to do this problem intuitively. I looked through my textbook and found: $$f_{Y}(y) = f_{X}(g^{-1}(y))\left | \frac{dx}{dy} \right |$$ So updated question: why do we need to multiply by the jacobian? Is it because if you were to do this problem using the cdf and took its derivative to get the pdf, you would end up with that term which happens to be the jacobian? Is there a way to intuitvely understand why that term is there?","So this was my solution: Say, $Z = X^{2}$, then $X=\pm \sqrt{Z}$ and, $$P(Z=z)=P(X = \sqrt{z}) + P(X = -\sqrt{z}) = \frac{z}{18} + \frac{z}{18} = \frac{z}{9}$$ for $$0<z<9$$ However: $$\int_{0}^{9}P(z) dz = \int_{0}^{9}\frac{z}{9}dz = \frac{9}{2} \neq 1$$ Where did I go wrong? Edit: I tried to do this problem intuitively. I looked through my textbook and found: $$f_{Y}(y) = f_{X}(g^{-1}(y))\left | \frac{dx}{dy} \right |$$ So updated question: why do we need to multiply by the jacobian? Is it because if you were to do this problem using the cdf and took its derivative to get the pdf, you would end up with that term which happens to be the jacobian? Is there a way to intuitvely understand why that term is there?",,"['probability', 'statistics', 'transformation']"
69,Skewness - Zero,Skewness - Zero,,"Why is the skewness of the samples $[ 1,\,2,\,3,\,4,\, 5,\, 6,\, 7,\, 8 ,\, 9 ]$ zero? Of course, it follows from definition, $\displaystyle E \left[\left(\frac{x_i - \text{mean}}{\text{deviation}}\right)^3\right]$. But on the other hand a skewness of zero means a symmetrical distribution which is, obviously, not the case for the linear increasing values of the samples presented above?!","Why is the skewness of the samples $[ 1,\,2,\,3,\,4,\, 5,\, 6,\, 7,\, 8 ,\, 9 ]$ zero? Of course, it follows from definition, $\displaystyle E \left[\left(\frac{x_i - \text{mean}}{\text{deviation}}\right)^3\right]$. But on the other hand a skewness of zero means a symmetrical distribution which is, obviously, not the case for the linear increasing values of the samples presented above?!",,['statistics']
70,Calculating the 4th number from 3 related numbers,Calculating the 4th number from 3 related numbers,,"I was taught a long time ago this really useful trick to calculate an unknown value when you know 3 existing values that are related. An example would be something like: If there are 21 peas in 3 pods how many are there in 17 pods? This gives us 2 values that have a relation, the next value that fits a certain rule that would reveal the missing value. I was taught to write it down in a kind of 3 out of 4 square, and you would multiply diagonally and divide up and down. But I can't quite remember the exact rule and if this little trick has a name.","I was taught a long time ago this really useful trick to calculate an unknown value when you know 3 existing values that are related. An example would be something like: If there are 21 peas in 3 pods how many are there in 17 pods? This gives us 2 values that have a relation, the next value that fits a certain rule that would reveal the missing value. I was taught to write it down in a kind of 3 out of 4 square, and you would multiply diagonally and divide up and down. But I can't quite remember the exact rule and if this little trick has a name.",,['statistics']
71,Another textbook problem on probability,Another textbook problem on probability,,"Here's a problem in my textbook: ""From a set of 2n + 1 consecutively numbered tickets, three are selected at random without replacement. Find the probability that the numbers of the tickets form an arithmetic progression. [The order in which the tickets are selected does not matter.]"" I tried to solve it by first arbitrarily choosing 2 numbers from the first n+1 tickets. This is because in any 3 tickets in an arithmetic progression there must be at least 2 tickets from the first half of the pile. Once we've chosen our two numbers there is only one possible third ticket. Thus the probability must be 2C3/[(2n+1)C3]. However, the textbook gives n^2/[(2n+1)C3]. What went wrong?","Here's a problem in my textbook: ""From a set of 2n + 1 consecutively numbered tickets, three are selected at random without replacement. Find the probability that the numbers of the tickets form an arithmetic progression. [The order in which the tickets are selected does not matter.]"" I tried to solve it by first arbitrarily choosing 2 numbers from the first n+1 tickets. This is because in any 3 tickets in an arithmetic progression there must be at least 2 tickets from the first half of the pile. Once we've chosen our two numbers there is only one possible third ticket. Thus the probability must be 2C3/[(2n+1)C3]. However, the textbook gives n^2/[(2n+1)C3]. What went wrong?",,"['probability', 'statistics']"
72,Variance formulae,Variance formulae,,"I know that the variance formula is $$\sigma^2 = \frac{    \left( x_1 - \bar{x} \right) ^2 +    \left( x_2 - \bar{x} \right) ^2 +    \dots +    \left( x_n - \bar{x} \right) ^2    }{n}$$ Where $\sigma^2$ is the variance; $x_1,\ x_2,\ \dots,\ x_n$ are the statistical data, and $n$ is the number of data. My question is: how can I expand that formula to get this equivalent one: $$ \sigma^2 = \frac{x_1^2 + x_2^2 + \dots + x^2_n}{n} -\bar{x}^2$$ ?","I know that the variance formula is $$\sigma^2 = \frac{    \left( x_1 - \bar{x} \right) ^2 +    \left( x_2 - \bar{x} \right) ^2 +    \dots +    \left( x_n - \bar{x} \right) ^2    }{n}$$ Where $\sigma^2$ is the variance; $x_1,\ x_2,\ \dots,\ x_n$ are the statistical data, and $n$ is the number of data. My question is: how can I expand that formula to get this equivalent one: $$ \sigma^2 = \frac{x_1^2 + x_2^2 + \dots + x^2_n}{n} -\bar{x}^2$$ ?",,['statistics']
73,Relationship between variances in perfect correlation,Relationship between variances in perfect correlation,,"I have two random variables $X$ and $Y$ with mean and standard deviation $(\mu_1,\sigma_1)$ and $(\mu_2,\sigma_2)$ respectively. I know that for perfect correlation the relationship is given by a linear regression. I also know that positive perfect correlation establishes that $\mu_2$ will be linear function of $\mu_1$.  But is there a relationship between variances ? More specifically given $\mu_1,\sigma_1$ and $\mu_2$, can I evaluate what is the value of $\sigma_2$ ? You can assume normal distributions for $X$ and $Y$ if that helps.","I have two random variables $X$ and $Y$ with mean and standard deviation $(\mu_1,\sigma_1)$ and $(\mu_2,\sigma_2)$ respectively. I know that for perfect correlation the relationship is given by a linear regression. I also know that positive perfect correlation establishes that $\mu_2$ will be linear function of $\mu_1$.  But is there a relationship between variances ? More specifically given $\mu_1,\sigma_1$ and $\mu_2$, can I evaluate what is the value of $\sigma_2$ ? You can assume normal distributions for $X$ and $Y$ if that helps.",,"['statistics', 'correlation']"
74,How is it that the required sample size for a specified error and confidence is not dependent on population size?,How is it that the required sample size for a specified error and confidence is not dependent on population size?,,"When calculating confidence intervals for population parameters, the population size is never a factor, rather sample size and the estimated parameter are used. It seems to me very counter-intuitive that to assert with a certain confidence that a certain view has certain probability, one requires the same sample size  regardless of whether the population is 2K persons or 2M persons. Why is the confidence of estimated parameters of normal distributions independent of the population size?","When calculating confidence intervals for population parameters, the population size is never a factor, rather sample size and the estimated parameter are used. It seems to me very counter-intuitive that to assert with a certain confidence that a certain view has certain probability, one requires the same sample size  regardless of whether the population is 2K persons or 2M persons. Why is the confidence of estimated parameters of normal distributions independent of the population size?",,['statistics']
75,Rounding non-integer values to integer values without causing an 'overflow',Rounding non-integer values to integer values without causing an 'overflow',,"The following problem comes from a piece of software I am currently working on, but since it's partially a mathematical problem, I'll ask the question here. In my application I have a fixed value (e.g. 1000) which needs to be divided into multiple different values.  The problem is that the algorithm (in another application) that divides the value returns non-integer values, and the nature of my current application requires integer values. If the starting value is divided in two values, this shouldn't be a problem in most cases.  E.g. if 1000 is divided in 600.6 and 399.4, then I can safely round this and get 601 and 399, which still sum up to 1000. However, if the value is divided in more than 2 values, I cannot simply round the results, as this may cause an overflow or a slack.  E.g. if 1000 is divided in: 300.6 300.6 398.8 Then rounding these values give me: 301, 301 and 399, which sums up to 1001. In my application I could decide to round down everything, which would give me 300, 300 and 398.  Although the sum is not 1000 anymore, in my case a smaller value (slack) is more acceptible than a larger value (overflow). Are there any known tricks in number theory that provide better rouding mechanisms so this problem can be minimized?  E.g. by using information about statistical distribution of numbers/digits? Notice that: I can't change the original division algorithm I can't try to sum up the values myself and manually change one of the input values (in my situation the starting value could be divided in hundreds, thousands or even millions of different values) I can only change the rounding method Thanks in advance","The following problem comes from a piece of software I am currently working on, but since it's partially a mathematical problem, I'll ask the question here. In my application I have a fixed value (e.g. 1000) which needs to be divided into multiple different values.  The problem is that the algorithm (in another application) that divides the value returns non-integer values, and the nature of my current application requires integer values. If the starting value is divided in two values, this shouldn't be a problem in most cases.  E.g. if 1000 is divided in 600.6 and 399.4, then I can safely round this and get 601 and 399, which still sum up to 1000. However, if the value is divided in more than 2 values, I cannot simply round the results, as this may cause an overflow or a slack.  E.g. if 1000 is divided in: 300.6 300.6 398.8 Then rounding these values give me: 301, 301 and 399, which sums up to 1001. In my application I could decide to round down everything, which would give me 300, 300 and 398.  Although the sum is not 1000 anymore, in my case a smaller value (slack) is more acceptible than a larger value (overflow). Are there any known tricks in number theory that provide better rouding mechanisms so this problem can be minimized?  E.g. by using information about statistical distribution of numbers/digits? Notice that: I can't change the original division algorithm I can't try to sum up the values myself and manually change one of the input values (in my situation the starting value could be divided in hundreds, thousands or even millions of different values) I can only change the rounding method Thanks in advance",,"['number-theory', 'statistics']"
76,Going from binomial distribution to Poisson distribution,Going from binomial distribution to Poisson distribution,,"Why does the Poisson distribution $$\!f(k; \lambda)= \Pr(X=k)= \frac{\lambda^k \exp{(-\lambda})}{k!}$$ contain the exponential function $\exp$, while its relation to the binomial distribution would suggest it's all about powers of $2$?","Why does the Poisson distribution $$\!f(k; \lambda)= \Pr(X=k)= \frac{\lambda^k \exp{(-\lambda})}{k!}$$ contain the exponential function $\exp$, while its relation to the binomial distribution would suggest it's all about powers of $2$?",,"['probability', 'statistics', 'probability-distributions', 'binomial-coefficients']"
77,Probability density function of a complex-valued random variable,Probability density function of a complex-valued random variable,,"I'm trying to understand the concept of complex-valued random variables, but I'm struggling. If you consider two real-valued random variables $U$ and $V$ with values $u$ and $v$ and the joint random variable $UV$ with values $(u,v)$ then under the following transformation of random variable $UV$ to random variable $ZW$ $$z=u+iv$$ $$w=u$$ the probability density function of $ZW$ is (Jacobian determinant =1) $$p_{ZW}(z,w)=p_{UV}(z-iw,w)$$ and the marginal probability density function $p_{Z}(z)$ is then given by $$p_{Z}(z)=\int_{-\infty}^{\infty}{p_{UV}(z-iw,w)}dw$$ Is this the probability density function of complex-valued random variable $Z$?","I'm trying to understand the concept of complex-valued random variables, but I'm struggling. If you consider two real-valued random variables $U$ and $V$ with values $u$ and $v$ and the joint random variable $UV$ with values $(u,v)$ then under the following transformation of random variable $UV$ to random variable $ZW$ $$z=u+iv$$ $$w=u$$ the probability density function of $ZW$ is (Jacobian determinant =1) $$p_{ZW}(z,w)=p_{UV}(z-iw,w)$$ and the marginal probability density function $p_{Z}(z)$ is then given by $$p_{Z}(z)=\int_{-\infty}^{\infty}{p_{UV}(z-iw,w)}dw$$ Is this the probability density function of complex-valued random variable $Z$?",,"['probability', 'statistics']"
78,Measure-theoretic view of expectation of a Bernoulli sequence,Measure-theoretic view of expectation of a Bernoulli sequence,,"Problem: I have a good understanding of basic Bernoulli and Binomial RVs, but this was foundational work in statistics. I am attempting to try and apply my (minimal but increasing) knowledge of measure theory to a tangible concept. I have been working with simple functions, etc. and am trying to utilize only these tools to find expectation: if $f=\sum_{i=1}^m c_i1_{A_i}$ has distinct, finite c's and disjoint A's, then $\int f du=\sum_{i=1}^m c_i\mu(A_i)$ and if $f$ is measurable and $f_n \uparrow f$ then $\int f du=\lim_{n\rightarrow\infty}\int f_n d\mu$ I want to try and practice (read: learn how to) utilize these ideas on a measure space of an infinite number of Bernoulli trials. I define my space below: Work Take $(\Omega,\mathcal{B})=(\{0,1\}^{\infty},\mathcal{B}(\{0,1\}^{\infty}))$ and define an event $\omega\in\Omega$ as $\omega=(x_1,x_2,...)$ Then I defined a probability measure: $P(\{x_1\}$x{0,1}x...$)=\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$. From here I want to  find the expectation of RVs such as: 1- $Z(\omega)=\sum_{i=1}^nx_i$ 2- $Y=e^{sZ}$ (moment-generating function), and 3- $V(\omega)=\sum_{i=1}^{\infty}r^nx_n$ for positive r. Using comments below: $Z(\omega)=\sum_{i=1}^n 1_{A_i}, A_i\subset\Omega, A_i=\{(x_k)_{k\ge 1}\in\Omega|x_i=1\}$ $Y_n(x_1,...,x_n)=exp \left(s\sum_{i=1}^n x_i \right )$ $E_n(Y_n)=\sum_{(x_k)\in\Omega_n}exp \left(s\sum_{i=1}^n x_i \right )\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$ $E(Y)=(1-p+pe^s)^n$ $E(V)=\sum_{i=1}^n r^i p, \forall n$ $V(\omega)_n \uparrow V(\omega)\rightarrow E(V(\omega)_n)\uparrow E(V(\omega))$ I showed these to a friend and he had the following comments: 1-Each RV needs to be represented as a simple function, a limit of a nondecreasing sequence of non-negative functions, or a difference of two such limits (whose product is zero). For example, on $Z$, you need to compute $E(Z)=\sum_{i=1}^n P(A_i)$ and show how each $P(A_i)$ is derived from $P$ as being a unique probability measure satisfying the equality. Also, if the RV is a limit of simple functions, you have to find the expectation of the simple function in the sequence and take the limit. Given that I am learning this on my own from scratch, any explicit help would be greatly appreciated. No detail is too much!","Problem: I have a good understanding of basic Bernoulli and Binomial RVs, but this was foundational work in statistics. I am attempting to try and apply my (minimal but increasing) knowledge of measure theory to a tangible concept. I have been working with simple functions, etc. and am trying to utilize only these tools to find expectation: if $f=\sum_{i=1}^m c_i1_{A_i}$ has distinct, finite c's and disjoint A's, then $\int f du=\sum_{i=1}^m c_i\mu(A_i)$ and if $f$ is measurable and $f_n \uparrow f$ then $\int f du=\lim_{n\rightarrow\infty}\int f_n d\mu$ I want to try and practice (read: learn how to) utilize these ideas on a measure space of an infinite number of Bernoulli trials. I define my space below: Work Take $(\Omega,\mathcal{B})=(\{0,1\}^{\infty},\mathcal{B}(\{0,1\}^{\infty}))$ and define an event $\omega\in\Omega$ as $\omega=(x_1,x_2,...)$ Then I defined a probability measure: $P(\{x_1\}$x{0,1}x...$)=\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$. From here I want to  find the expectation of RVs such as: 1- $Z(\omega)=\sum_{i=1}^nx_i$ 2- $Y=e^{sZ}$ (moment-generating function), and 3- $V(\omega)=\sum_{i=1}^{\infty}r^nx_n$ for positive r. Using comments below: $Z(\omega)=\sum_{i=1}^n 1_{A_i}, A_i\subset\Omega, A_i=\{(x_k)_{k\ge 1}\in\Omega|x_i=1\}$ $Y_n(x_1,...,x_n)=exp \left(s\sum_{i=1}^n x_i \right )$ $E_n(Y_n)=\sum_{(x_k)\in\Omega_n}exp \left(s\sum_{i=1}^n x_i \right )\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$ $E(Y)=(1-p+pe^s)^n$ $E(V)=\sum_{i=1}^n r^i p, \forall n$ $V(\omega)_n \uparrow V(\omega)\rightarrow E(V(\omega)_n)\uparrow E(V(\omega))$ I showed these to a friend and he had the following comments: 1-Each RV needs to be represented as a simple function, a limit of a nondecreasing sequence of non-negative functions, or a difference of two such limits (whose product is zero). For example, on $Z$, you need to compute $E(Z)=\sum_{i=1}^n P(A_i)$ and show how each $P(A_i)$ is derived from $P$ as being a unique probability measure satisfying the equality. Also, if the RV is a limit of simple functions, you have to find the expectation of the simple function in the sequence and take the limit. Given that I am learning this on my own from scratch, any explicit help would be greatly appreciated. No detail is too much!",,"['statistics', 'measure-theory', 'probability-theory']"
79,Simple question on standard deviation and mean.,Simple question on standard deviation and mean.,,"This question has kinda stumped me: For a data set, the mean is 5 and the standard deviation is 1. There   are 10 values in the data set. Is the range of the data set bigger,   smaller or equal to 10? I don't want the answer, just the technique. Here's what I've tried (admittedly, not much): $$\sum_{i=0}^{9}a_i / 10= 5$$ $$\sigma = 1$$ Is there a relation between standard deviation and simple mean which I can use? Any help is appreciated. This is not homework, it's a practice qn for the GRE.","This question has kinda stumped me: For a data set, the mean is 5 and the standard deviation is 1. There   are 10 values in the data set. Is the range of the data set bigger,   smaller or equal to 10? I don't want the answer, just the technique. Here's what I've tried (admittedly, not much): $$\sum_{i=0}^{9}a_i / 10= 5$$ $$\sigma = 1$$ Is there a relation between standard deviation and simple mean which I can use? Any help is appreciated. This is not homework, it's a practice qn for the GRE.",,"['statistics', 'standard-deviation']"
80,An interesting problem about almost sure convergence,An interesting problem about almost sure convergence,,"Assume that $X_1,X_2,\ldots$ are independent random variables (not necessarily of the same distribution). Assume that that $Var[X_n]>0$ for all $n$. Assume also that $$\sum_{n=0}^\infty \frac{Var[X_n]}{n^2}<\infty,$$ that $$\frac{1}{n}\sum_{i=1}^n(X_i-E[X_i])\to 0 \textrm{ almost surely as $n\to\infty$},$$ that $$E[X_n]>0 \textrm{ for all $n$},$$ and that $$\liminf_{n\to\infty} E[X_n] > 0.$$ How can we prove that $$\sum_{i=1}^n X_i \to \infty\text{ almost surely as $n\to\infty$?}$$ This seems to intuitively make sense, but a formal proof escapes me. Also, what can we say if $E[X_n] = 0$ for all $n$ and $\lim_{n\to\infty} E[X_n] = 0$?","Assume that $X_1,X_2,\ldots$ are independent random variables (not necessarily of the same distribution). Assume that that $Var[X_n]>0$ for all $n$. Assume also that $$\sum_{n=0}^\infty \frac{Var[X_n]}{n^2}<\infty,$$ that $$\frac{1}{n}\sum_{i=1}^n(X_i-E[X_i])\to 0 \textrm{ almost surely as $n\to\infty$},$$ that $$E[X_n]>0 \textrm{ for all $n$},$$ and that $$\liminf_{n\to\infty} E[X_n] > 0.$$ How can we prove that $$\sum_{i=1}^n X_i \to \infty\text{ almost surely as $n\to\infty$?}$$ This seems to intuitively make sense, but a formal proof escapes me. Also, what can we say if $E[X_n] = 0$ for all $n$ and $\lim_{n\to\infty} E[X_n] = 0$?",,"['probability', 'statistics']"
81,Determine variance & mean,Determine variance & mean,,I am not sure how to determine the variance and mean for the following equation system. I'd greatly appreciate your help! $   f_\xi(x) = \left\{   \begin{array}{l l}     10k_1x & \quad \text{for $0<x<1$}\\     0 & \quad \text{for other}\\   \end{array} \right. $ $   f_\eta(x) = \left\{   \begin{array}{l l}     4k_2x & \quad \text{for $0<x<1$}\\     4k_2(2-x) & \quad \text{for $1\leq x<2$}\\   \end{array} \right. $ I have determined the following: $k_1 = \frac{1}{5}$ $k_2=\frac{2}{7}$ ... and I believe to have found their individual variance and mean: $E_\xi(x)=\frac{2}{3}$ $V_\xi(x)=\frac{1}{18}$ $E_\eta(x)=\frac{22}{21}$ $V_\eta(x)=\frac{145}{882}$ Now to the gist of my question: how do I calculate the variance and the mean for $10\xi+2\eta$? Thank you!,I am not sure how to determine the variance and mean for the following equation system. I'd greatly appreciate your help! $   f_\xi(x) = \left\{   \begin{array}{l l}     10k_1x & \quad \text{for $0<x<1$}\\     0 & \quad \text{for other}\\   \end{array} \right. $ $   f_\eta(x) = \left\{   \begin{array}{l l}     4k_2x & \quad \text{for $0<x<1$}\\     4k_2(2-x) & \quad \text{for $1\leq x<2$}\\   \end{array} \right. $ I have determined the following: $k_1 = \frac{1}{5}$ $k_2=\frac{2}{7}$ ... and I believe to have found their individual variance and mean: $E_\xi(x)=\frac{2}{3}$ $V_\xi(x)=\frac{1}{18}$ $E_\eta(x)=\frac{22}{21}$ $V_\eta(x)=\frac{145}{882}$ Now to the gist of my question: how do I calculate the variance and the mean for $10\xi+2\eta$? Thank you!,,"['statistics', 'integration']"
82,"Given series $A$ and a correlation, is it possible to randomly calculate a fitting series $B$?","Given series  and a correlation, is it possible to randomly calculate a fitting series ?",A B,"With reference to the original thread on Stackexchange , my question is as follows. Usually, one would enter two value-series and a script or program calculates the correlation. For instance, with $x = 5,3,6,7,4,2,9,5$ and $y = 4,3,4,8,3,2,10,5$, the correlation is $0.93439982209434$. For an educational website, I'm trying to find a way to let students: put in value series $x$, eg. $x = 5,3,6,7,4,2,9,5$ put in the correlation, eg. $0.9344$ put in upper and lower boundaries of $y$-series, eg. between $1$ and $10$ give back a random series which fits the citeria, eg. $y = 4,3,4,8,3,2,10,5$ The PHP script I have written to calculate the correlation can be found in the referred-to post on stackexchange. However, it was suggested mine was much more a mathematical than a programmatical question, hence this post. Would it be possible to execute this ""reverse correlation""?","With reference to the original thread on Stackexchange , my question is as follows. Usually, one would enter two value-series and a script or program calculates the correlation. For instance, with $x = 5,3,6,7,4,2,9,5$ and $y = 4,3,4,8,3,2,10,5$, the correlation is $0.93439982209434$. For an educational website, I'm trying to find a way to let students: put in value series $x$, eg. $x = 5,3,6,7,4,2,9,5$ put in the correlation, eg. $0.9344$ put in upper and lower boundaries of $y$-series, eg. between $1$ and $10$ give back a random series which fits the citeria, eg. $y = 4,3,4,8,3,2,10,5$ The PHP script I have written to calculate the correlation can be found in the referred-to post on stackexchange. However, it was suggested mine was much more a mathematical than a programmatical question, hence this post. Would it be possible to execute this ""reverse correlation""?",,"['statistics', 'correlation']"
83,Probability Questions,Probability Questions,,"I need help with part (1) of this problem and confirmation I've done the right thing for (2). 1) A survey carried out by a firm found 60% of clients buy products every month and 20% buy high-tech products. Of those who buy products every month, 30% buy high-tech products. (i) Are the events 'buy products every month' and 'buy high-tech products' independent? Justify your answer. (ii) what is the probability that a customer who buys high-tech products buys product every month? 2) It is known that 25% of all individuals in a large group are smokers. (i) Suppose 4 individuals are selected at random. What is the probability that exactly 2 of them are smokers? (ii) Suppose 400 individuals are selected at random. What is the probability that less than 113 of them are smokers? I'm completely clueless about (1) but for (2i) I worked it out to be 4C2 x 0.25$^2$ x 0.75$^2$ =0.2109 and for (2ii) I did a normal distribution Y~N(100,0.75) which gave me P(1.50 < Z)=0.9332. Hope I've done it right for 2 and need help for 1. Cheers","I need help with part (1) of this problem and confirmation I've done the right thing for (2). 1) A survey carried out by a firm found 60% of clients buy products every month and 20% buy high-tech products. Of those who buy products every month, 30% buy high-tech products. (i) Are the events 'buy products every month' and 'buy high-tech products' independent? Justify your answer. (ii) what is the probability that a customer who buys high-tech products buys product every month? 2) It is known that 25% of all individuals in a large group are smokers. (i) Suppose 4 individuals are selected at random. What is the probability that exactly 2 of them are smokers? (ii) Suppose 400 individuals are selected at random. What is the probability that less than 113 of them are smokers? I'm completely clueless about (1) but for (2i) I worked it out to be 4C2 x 0.25$^2$ x 0.75$^2$ =0.2109 and for (2ii) I did a normal distribution Y~N(100,0.75) which gave me P(1.50 < Z)=0.9332. Hope I've done it right for 2 and need help for 1. Cheers",,"['probability', 'statistics']"
84,design matrices,design matrices,,"Given a linear model $Y = X\beta + \epsilon$ with three treatments and six subjects where $X$ is the design matrix, suppose $X = \begin{matrix}1 & 1 & 0\\ 1 & 1 & 0\\ 1 & 0 & 1\\ 1 & 0 & 1\\ 1 & -1 & -1\\ 1 & -1 & -1 \end{matrix}$ and X'=   \begin{matrix}1 & 0 & 0\\ 1 & 0 & 0\\ 1 & 1 & 0\\ 1 & 1 & 0\\ 1 & 0 & 1\\ 1 & 0 & 1 \end{matrix} with response vector $Y=[Y_{11} Y_{12} Y_{21} Y_{22} Y_{31} Y_{32}]^{T}$ Do these two matrices give you the same model? I've had some trouble trying to figure out how, given that $\beta = (\beta_1 \beta_2 \beta_3)^T$, these two designs can give the equivalent model. Don't they have entirely different values of $\beta$ for each $Y_{ij}$? EDIT: The constraints are $\sum_{i=1}^{3}\beta_i = 0$","Given a linear model $Y = X\beta + \epsilon$ with three treatments and six subjects where $X$ is the design matrix, suppose $X = \begin{matrix}1 & 1 & 0\\ 1 & 1 & 0\\ 1 & 0 & 1\\ 1 & 0 & 1\\ 1 & -1 & -1\\ 1 & -1 & -1 \end{matrix}$ and X'=   \begin{matrix}1 & 0 & 0\\ 1 & 0 & 0\\ 1 & 1 & 0\\ 1 & 1 & 0\\ 1 & 0 & 1\\ 1 & 0 & 1 \end{matrix} with response vector $Y=[Y_{11} Y_{12} Y_{21} Y_{22} Y_{31} Y_{32}]^{T}$ Do these two matrices give you the same model? I've had some trouble trying to figure out how, given that $\beta = (\beta_1 \beta_2 \beta_3)^T$, these two designs can give the equivalent model. Don't they have entirely different values of $\beta$ for each $Y_{ij}$? EDIT: The constraints are $\sum_{i=1}^{3}\beta_i = 0$",,"['statistics', 'regression']"
85,How do you calculate probability of rolling all faces of a die after n number of rolls? [duplicate],How do you calculate probability of rolling all faces of a die after n number of rolls? [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Expected time to roll all 1 through 6 on a die Probability of picking all elements in a set Im pretty new to the stackexchange, and posted this is statistics, and then discovered this site, and thought it was much more appropriate, so here I go again: It is fairly easy to figure out what is the average rolls it would take to roll all faces of a die [1 + 6/5 + 6/4 + 6/3 + 6/2 + 6/1 = 14.7], but that got me thinking of a seemingly more complicated problem. Say you roll a die 1-5 times, the is the odds of ALL faces showing, is obviously 0. If you roll a die 6 times, the odds of all faces showing can easily be calculated like so: 1 * (5/6) * (4/6) * (3/6) * (2/6) * (1/6) = .0154 or 1.54% Now is where I get stuck. How to do 7, or more times, and calculate it with n. Any tips is helpful!","This question already has answers here : Closed 12 years ago . Possible Duplicate: Expected time to roll all 1 through 6 on a die Probability of picking all elements in a set Im pretty new to the stackexchange, and posted this is statistics, and then discovered this site, and thought it was much more appropriate, so here I go again: It is fairly easy to figure out what is the average rolls it would take to roll all faces of a die [1 + 6/5 + 6/4 + 6/3 + 6/2 + 6/1 = 14.7], but that got me thinking of a seemingly more complicated problem. Say you roll a die 1-5 times, the is the odds of ALL faces showing, is obviously 0. If you roll a die 6 times, the odds of all faces showing can easily be calculated like so: 1 * (5/6) * (4/6) * (3/6) * (2/6) * (1/6) = .0154 or 1.54% Now is where I get stuck. How to do 7, or more times, and calculate it with n. Any tips is helpful!",,"['probability', 'statistics', 'dice']"
86,Statistics: drawing 5 cards - probability for a pair and triplets,Statistics: drawing 5 cards - probability for a pair and triplets,,"I have to apologise if this question has been asked and answered in the past; if there is one field of Mathematics that I struggle to understand, it's definitely statistics. I'm to calculate the probability to acquire a pair and triplet of any sort. I'm only allowed to draw five cards from the stack. I am uncertain how to approach the problem - in what avenue to begin. Any help would be greatly appreciated, and please, the more you treat me as a dummy, all the better! :) Thanks","I have to apologise if this question has been asked and answered in the past; if there is one field of Mathematics that I struggle to understand, it's definitely statistics. I'm to calculate the probability to acquire a pair and triplet of any sort. I'm only allowed to draw five cards from the stack. I am uncertain how to approach the problem - in what avenue to begin. Any help would be greatly appreciated, and please, the more you treat me as a dummy, all the better! :) Thanks",,"['probability', 'statistics']"
87,How to decide what is the probability distribution in a Monte-Carlo simulation?,How to decide what is the probability distribution in a Monte-Carlo simulation?,,"For a Monte-Carlo integration of $$\int_\Omega P(x)f(x)\ \text d x,$$ there seems to be no apriori distinction if $f$ or $P$ is the probability function. So does it matter if I consider $$P, f, P f, \text{ or 1 over all of } \Omega$$ to be the distribution function I draw the random points $x$ from, given that I use the complement function as object to plug these values in?","For a Monte-Carlo integration of $$\int_\Omega P(x)f(x)\ \text d x,$$ there seems to be no apriori distinction if $f$ or $P$ is the probability function. So does it matter if I consider $$P, f, P f, \text{ or 1 over all of } \Omega$$ to be the distribution function I draw the random points $x$ from, given that I use the complement function as object to plug these values in?",,"['probability', 'statistics', 'numerical-methods', 'integration', 'monte-carlo']"
88,How to prove $\frac{(n-1)S^2}{\sigma^2}\sim \chi_{n-1}^2$? [duplicate],How to prove ? [duplicate],\frac{(n-1)S^2}{\sigma^2}\sim \chi_{n-1}^2,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Proof of $\frac{(n-1)S^2}{\sigma^2} \backsim \chi^2_{n-1}$ If $X_1,...,X_n$ is a random sample from a normal distribution and $S^2=\sum_{i=1}^n\frac{(X_i-\bar{X})^2}{n-1}$ Could anyone help me show that $\frac{(n-1)S^2}{\sigma^2}\sim \chi_{n-1}^2$ and that it's independent to $\bar{X}$ Many thanks!","This question already has answers here : Closed 12 years ago . Possible Duplicate: Proof of $\frac{(n-1)S^2}{\sigma^2} \backsim \chi^2_{n-1}$ If $X_1,...,X_n$ is a random sample from a normal distribution and $S^2=\sum_{i=1}^n\frac{(X_i-\bar{X})^2}{n-1}$ Could anyone help me show that $\frac{(n-1)S^2}{\sigma^2}\sim \chi_{n-1}^2$ and that it's independent to $\bar{X}$ Many thanks!",,"['statistics', 'probability-distributions']"
89,finding the limits of integration. joint probability,finding the limits of integration. joint probability,,"A candy company distributes boxes of chocolates with a mixture of creams, toffees, and cordials. Suppose that the weight of each box is 1 kilogram, but the individual weights of the creams, toffees, and cordials vary from box to box. For a randomly selected box, let X and Y represent the weights of the creams and the toffees, respectively, and suppose that the joint density function of these variables is f(x, y) =  24xy, 0 ≤ x ≤ 1, 0 ≤ y ≤ 1, x+ y ≤ 1, 0, elsewhere. (a) Find the probability that in a given box the cordials account for more than 1/2 of the weight. In letter a, it means x+y < 1/2, now how can we find the limits of the double integral? proper approach to solve this problem? thanks PS: there would be no problem if the limits can be easily deciphered. but for this one its asking P(X + Y < 1/2)","A candy company distributes boxes of chocolates with a mixture of creams, toffees, and cordials. Suppose that the weight of each box is 1 kilogram, but the individual weights of the creams, toffees, and cordials vary from box to box. For a randomly selected box, let X and Y represent the weights of the creams and the toffees, respectively, and suppose that the joint density function of these variables is f(x, y) =  24xy, 0 ≤ x ≤ 1, 0 ≤ y ≤ 1, x+ y ≤ 1, 0, elsewhere. (a) Find the probability that in a given box the cordials account for more than 1/2 of the weight. In letter a, it means x+y < 1/2, now how can we find the limits of the double integral? proper approach to solve this problem? thanks PS: there would be no problem if the limits can be easily deciphered. but for this one its asking P(X + Y < 1/2)",,"['probability', 'statistics']"
90,Finding the distribution of $\frac{1}{\sigma^2}\Big( \sum_i^m (X_i-\bar{X})^2+\sum_j^m (Y_i-\bar{Y})^2 \Big)$ where $X_i$ is from a normal sample,Finding the distribution of  where  is from a normal sample,\frac{1}{\sigma^2}\Big( \sum_i^m (X_i-\bar{X})^2+\sum_j^m (Y_i-\bar{Y})^2 \Big) X_i,"Let $X_1,...,X_m$ and $Y_1,...,Y_n$ be independent random samples from normal distributions $N(\mu_1,\sigma^2)$, $N(\mu_2,\sigma^2)$ respectively, where all parameters are unknown. Let $$S^2=(m+n-2)^{-1}\Big( \sum_i^m (X_i-\bar{X})^2+\sum_j^m (Y_i-\bar{Y})^2 \Big)$$ Can anyone help me find the distributions of: $$(m+n-2)S^2/\sigma^2\tag{1}$$ And $$\frac{\bar{X}-\bar{Y}-(\mu_1-\mu_2)}{\sqrt{S^2(\frac{1}{m}+\frac{1}{n})}}\tag{2}$$ For $(2)$ I am having no real luck at all. With $(1)$ I have the obvious step of writing $$(m+n-2)S^2/\sigma^2=\frac{1}{\sigma^2}\Big( \sum_i^m (X_i-\bar{X})^2+\sum_j^m (Y_i-\bar{Y})^2 \Big)$$ On the RHS is there some connection with the variance of $X$ and $Y$? $$(m+n-2)S^2/\sigma^2=\frac{1}{\sigma^2}\Big( \frac{\alpha^2}{m-1}+\frac{\beta^2}{n-1}\Big)$$ Where $\alpha^2,\beta^2$ are the sample variance for $X$ and $Y$ which we can relate to the addition of 2 chi square distributions? i.e. $\chi_{m-1}^2+\chi_{n-1}^2=\chi_{m+n-2}^2$","Let $X_1,...,X_m$ and $Y_1,...,Y_n$ be independent random samples from normal distributions $N(\mu_1,\sigma^2)$, $N(\mu_2,\sigma^2)$ respectively, where all parameters are unknown. Let $$S^2=(m+n-2)^{-1}\Big( \sum_i^m (X_i-\bar{X})^2+\sum_j^m (Y_i-\bar{Y})^2 \Big)$$ Can anyone help me find the distributions of: $$(m+n-2)S^2/\sigma^2\tag{1}$$ And $$\frac{\bar{X}-\bar{Y}-(\mu_1-\mu_2)}{\sqrt{S^2(\frac{1}{m}+\frac{1}{n})}}\tag{2}$$ For $(2)$ I am having no real luck at all. With $(1)$ I have the obvious step of writing $$(m+n-2)S^2/\sigma^2=\frac{1}{\sigma^2}\Big( \sum_i^m (X_i-\bar{X})^2+\sum_j^m (Y_i-\bar{Y})^2 \Big)$$ On the RHS is there some connection with the variance of $X$ and $Y$? $$(m+n-2)S^2/\sigma^2=\frac{1}{\sigma^2}\Big( \frac{\alpha^2}{m-1}+\frac{\beta^2}{n-1}\Big)$$ Where $\alpha^2,\beta^2$ are the sample variance for $X$ and $Y$ which we can relate to the addition of 2 chi square distributions? i.e. $\chi_{m-1}^2+\chi_{n-1}^2=\chi_{m+n-2}^2$",,['statistics']
91,Time Series and statistics,Time Series and statistics,,"Consider the time series: $X(t) = 2 + 3t + Z(t) $ where $Z(t)$ are Gaussian white noises from $\mathcal{N}(0,1)$ . Is $X(t)$ stationary - why or why not? Is $Y(t) = X(t) - X(t-1)$ stationary, why or why not? Let $V(t)= \frac{1}{2q+1}\sum_{j=-q}^q X(t-j)$ . What is the mean and auto-covariance function of $V(t)$ . My approach is that: I know a stationary process is one in which the statistical properties of a given series are constant, such as constant mean, auto-covariance etc. I know that the expected value or mean of the noise component is zero. How do I compute the expectation of $X(t)$ ? How do I show the statistical properties are constant or not constant?","Consider the time series: where are Gaussian white noises from . Is stationary - why or why not? Is stationary, why or why not? Let . What is the mean and auto-covariance function of . My approach is that: I know a stationary process is one in which the statistical properties of a given series are constant, such as constant mean, auto-covariance etc. I know that the expected value or mean of the noise component is zero. How do I compute the expectation of ? How do I show the statistical properties are constant or not constant?","X(t) = 2 + 3t + Z(t)  Z(t) \mathcal{N}(0,1) X(t) Y(t) = X(t) - X(t-1) V(t)= \frac{1}{2q+1}\sum_{j=-q}^q X(t-j) V(t) X(t)","['statistics', 'time-series']"
92,to find the distribution function,to find the distribution function,,"Let $X_1,X_2,X_3$  be mutually  stochastically independent random variables and let each of them  have  the following density function: $f(x)= 2x$ when $0\leq x\leq 1$ and $f(x)=0$ elsewhere. Let Y be the random variable defined as, $Y=\max(X_1, X_2, X_3)$, find (a) the  distribution function and (b) the  probability  function of the random variable $Y$. I saw somewhere that the distribution function of such variable to be given by  $F_Y(y)=P(Y\leq y)=P(\max (X_1,X_2,X_3)\leq y)$. If  this  is true, how  may I apply it in this question to find (a) and (b) above?","Let $X_1,X_2,X_3$  be mutually  stochastically independent random variables and let each of them  have  the following density function: $f(x)= 2x$ when $0\leq x\leq 1$ and $f(x)=0$ elsewhere. Let Y be the random variable defined as, $Y=\max(X_1, X_2, X_3)$, find (a) the  distribution function and (b) the  probability  function of the random variable $Y$. I saw somewhere that the distribution function of such variable to be given by  $F_Y(y)=P(Y\leq y)=P(\max (X_1,X_2,X_3)\leq y)$. If  this  is true, how  may I apply it in this question to find (a) and (b) above?",,"['statistics', 'probability-distributions']"
93,variance of a sum,variance of a sum,,"We draws 10 from a finite population of 30, without replacement! 15 members out of 30 have the number 1, 10 members have the number 2 and 5 members have the number 3. The Expected value is  $$\mathbb E(X)= 1 \cdot \frac12+2 \cdot \frac13+3 \cdot \frac 16 = \frac 53 $$ Variance $$V(X)= \left( 1 -\frac53 \right)^2 \cdot \frac12+ \left( 2-\frac53 \right)^2 \cdot \frac13+ \left( 3-\frac53 \right)^2 \cdot \frac16 = \frac59 $$ I need the Variance of $$X_{1}+...+X_{10}$$ and $$Var( \frac1{10}( X_{1}+...+X_{10}))$$ I found this formula $$V(X_{1}+...+X_{10})= \sum\limits_{i=1}^{10} \operatorname{Var}(X_i)+\sum\limits_{i\neq j}^{} \operatorname{Cov}(X_i,X_j)$$ But how i can solve this?  With Wolframalpha? thx","We draws 10 from a finite population of 30, without replacement! 15 members out of 30 have the number 1, 10 members have the number 2 and 5 members have the number 3. The Expected value is  $$\mathbb E(X)= 1 \cdot \frac12+2 \cdot \frac13+3 \cdot \frac 16 = \frac 53 $$ Variance $$V(X)= \left( 1 -\frac53 \right)^2 \cdot \frac12+ \left( 2-\frac53 \right)^2 \cdot \frac13+ \left( 3-\frac53 \right)^2 \cdot \frac16 = \frac59 $$ I need the Variance of $$X_{1}+...+X_{10}$$ and $$Var( \frac1{10}( X_{1}+...+X_{10}))$$ I found this formula $$V(X_{1}+...+X_{10})= \sum\limits_{i=1}^{10} \operatorname{Var}(X_i)+\sum\limits_{i\neq j}^{} \operatorname{Cov}(X_i,X_j)$$ But how i can solve this?  With Wolframalpha? thx",,"['probability', 'statistics']"
94,Limits of integration for random variable,Limits of integration for random variable,,"Suppose you have two random variables $X$ and $Y$. If $X \sim N(0,1)$,  $Y \sim N(0,1)$ and you want to find k s.t. $\mathbb P(X+Y >k)=0.01$, how would you do this? I am having a hard time finding the limits of integration. How would you generalize $\mathbb P(X+Y+Z+\cdots > k) =0.01$? I always get confused when problems involve multiple integrals.","Suppose you have two random variables $X$ and $Y$. If $X \sim N(0,1)$,  $Y \sim N(0,1)$ and you want to find k s.t. $\mathbb P(X+Y >k)=0.01$, how would you do this? I am having a hard time finding the limits of integration. How would you generalize $\mathbb P(X+Y+Z+\cdots > k) =0.01$? I always get confused when problems involve multiple integrals.",,['statistics']
95,Proving convergence in distribution identitity,Proving convergence in distribution identitity,,Suppose $\{X_n\}$ converges in distribution to $X$ and $\{Y_n\}$ converges in probability to $0$. How would I show that $\{X_n + Y_n\}$ converges in distribution to $X$? Thanks for the help.,Suppose $\{X_n\}$ converges in distribution to $X$ and $\{Y_n\}$ converges in probability to $0$. How would I show that $\{X_n + Y_n\}$ converges in distribution to $X$? Thanks for the help.,,"['probability', 'statistics']"
96,Probability of Median from a continuous distribution,Probability of Median from a continuous distribution,,"For a sample of size $n=3$ from a continuous probability distribution, what is $P(X_{(1)}<k<X_{(2)})$ where $k$ is the median of the distribution? What is $P(X_{(1)}<k<X_{(3)})$?  $X_{(i)},i=1,2,3$ are the ordered values of the sample. I'm having trouble trying to solve this question since the median is for the distribution and not the sample. The only explicit formulas for the median I know of are the median $k$ of any random variable $X$ satisfies $P(X≤k)≥1/2$ and $P(X≥k)≥1/2$, but I don't see how to apply that here.","For a sample of size $n=3$ from a continuous probability distribution, what is $P(X_{(1)}<k<X_{(2)})$ where $k$ is the median of the distribution? What is $P(X_{(1)}<k<X_{(3)})$?  $X_{(i)},i=1,2,3$ are the ordered values of the sample. I'm having trouble trying to solve this question since the median is for the distribution and not the sample. The only explicit formulas for the median I know of are the median $k$ of any random variable $X$ satisfies $P(X≤k)≥1/2$ and $P(X≥k)≥1/2$, but I don't see how to apply that here.",,"['probability', 'statistics']"
97,Expectation and random sample,Expectation and random sample,,"Let $X_1,X_2,...,X_n$  be a random sample of size $n$ from a population $N(\mu, \sigma^2)$ with $\sigma^2=4$. Further assume that $\mu \sim N(0,1)$. Find $E[\mu|X]$ and $E[(\mu - E[\mu|X])^2 |X]$. Find the precision of the normal distribution. Find a 95% credible interval for $\mu$. So far I understand what to do for 3, and I know that for 2 it is the inverse of the variance. But I cannot figure out 1. I have looked in my book, which is hard to read and I cannot find anything on how to do 1. If I figure out 1, then I can do 2. Please help with 1. Thank you so much!","Let $X_1,X_2,...,X_n$  be a random sample of size $n$ from a population $N(\mu, \sigma^2)$ with $\sigma^2=4$. Further assume that $\mu \sim N(0,1)$. Find $E[\mu|X]$ and $E[(\mu - E[\mu|X])^2 |X]$. Find the precision of the normal distribution. Find a 95% credible interval for $\mu$. So far I understand what to do for 3, and I know that for 2 it is the inverse of the variance. But I cannot figure out 1. I have looked in my book, which is hard to read and I cannot find anything on how to do 1. If I figure out 1, then I can do 2. Please help with 1. Thank you so much!",,['statistics']
98,How can I compute my final grade?,How can I compute my final grade?,,"In one of my courses in the university, I got: Final: (93 out of 100) [25% of the course] 2nd Major: (100 out of 100 + bouns 1) [12.5% of the course] 1st Major: (93 out of 100) [12.5% of the course] Lab [30% of the course]:     lab reports [11.25%]:         lab#1: (100 out of 100)         lab#2: (92 out of 100)         lab#3: (86 out of 100)         lab#4: (95 out of 100)         lab#5: (80 out of 100)         lab#6: (97 out of 100)      projects [11.25%]:         project#1: (100 out of 100)         project#2: (100 out of 100)         project#3: (100 out of 100)      Two lab test [7.5%]:         Lab Test#1: (82 out of 100)         Lab Test#2: (100 out of 100) Quizzes + Homework + Attendance [20% of the course]:      Quizzes [13%]:         Quiz#1: (100 out of 100)         Quiz#2: (85 out of 100)         Quiz#3: (85 out of 100)         Quiz#4: (80 out of 100)         Quiz#5: (100 out of 100)         Quiz#6: (95 out of 100)         Quiz#7: (100 out of 100)         Quiz#8: (80 out of 100)         Quiz#9: (100 out of 100)      Homework [3%]:         HW#1: (100 out of 100)         HW#2: (100 out of 100)         HW#3: (95 out of 100)         HW#4: (0 out of 100)         HW#5: (100 out of 100)         HW#6: (100 out of 100)         HW#7: (80 out of 100)         HW#8: (100 out of 100)         HW#9: (100 out of 100)      Attendance (3.93 out of 4) [4%] Could you please tell me how can I compute the final grade for the entire course? (I hope I get A+) p.s: btw, is this a wrong place to ask such question?","In one of my courses in the university, I got: Final: (93 out of 100) [25% of the course] 2nd Major: (100 out of 100 + bouns 1) [12.5% of the course] 1st Major: (93 out of 100) [12.5% of the course] Lab [30% of the course]:     lab reports [11.25%]:         lab#1: (100 out of 100)         lab#2: (92 out of 100)         lab#3: (86 out of 100)         lab#4: (95 out of 100)         lab#5: (80 out of 100)         lab#6: (97 out of 100)      projects [11.25%]:         project#1: (100 out of 100)         project#2: (100 out of 100)         project#3: (100 out of 100)      Two lab test [7.5%]:         Lab Test#1: (82 out of 100)         Lab Test#2: (100 out of 100) Quizzes + Homework + Attendance [20% of the course]:      Quizzes [13%]:         Quiz#1: (100 out of 100)         Quiz#2: (85 out of 100)         Quiz#3: (85 out of 100)         Quiz#4: (80 out of 100)         Quiz#5: (100 out of 100)         Quiz#6: (95 out of 100)         Quiz#7: (100 out of 100)         Quiz#8: (80 out of 100)         Quiz#9: (100 out of 100)      Homework [3%]:         HW#1: (100 out of 100)         HW#2: (100 out of 100)         HW#3: (95 out of 100)         HW#4: (0 out of 100)         HW#5: (100 out of 100)         HW#6: (100 out of 100)         HW#7: (80 out of 100)         HW#8: (100 out of 100)         HW#9: (100 out of 100)      Attendance (3.93 out of 4) [4%] Could you please tell me how can I compute the final grade for the entire course? (I hope I get A+) p.s: btw, is this a wrong place to ask such question?",,['statistics']
99,Name for resistance to change in data,Name for resistance to change in data,,"Suppose I have two poll questions that can be answered with a Yes or a No with the following results: Poll 1 Yes: $200$ No: $100$ Poll 2 Yes: $2$ No: $1$ Both polls have a $66\%$-$33\%$ split, but getting them to a $50\%$-$50\%$ split is much harder with the first poll than with the second poll. There would have to be $100$ new ""No""s for the first as opposed to $1$ new ""No"" for the second one, two orders of magnitude in difference. Is there a mathematical/statistical name for this ""resistance to change""?","Suppose I have two poll questions that can be answered with a Yes or a No with the following results: Poll 1 Yes: $200$ No: $100$ Poll 2 Yes: $2$ No: $1$ Both polls have a $66\%$-$33\%$ split, but getting them to a $50\%$-$50\%$ split is much harder with the first poll than with the second poll. There would have to be $100$ new ""No""s for the first as opposed to $1$ new ""No"" for the second one, two orders of magnitude in difference. Is there a mathematical/statistical name for this ""resistance to change""?",,"['reference-request', 'statistics']"
