,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Understanding the Musical Isomorphisms in Vector Spaces,Understanding the Musical Isomorphisms in Vector Spaces,,"I am trying to solidify my understanding of the muscial isomorphisms in the context of vector spaces. I believe I understand the definitions but would appreciate corrections if my understanding is not correct. Also, as I have had some difficulty tracking down related material, I would welcome any suggested references that would expand upon this material and related results. So, let $V$ be a finite-dimensional vector space over $\mathbb{R}$ with inner product $\langle \cdot, \cdot \rangle$ and let $V^*$ denote its dual. For each element $v \in V$, one can define a mapping $V \rightarrow V^*$ by $v \mapsto \langle \cdot, v \rangle$. By the Riesz representation theorem this mapping actually determines an isomorphism that allows us to identify each element in $V$ with a unique functional in $V^*$ The ""flat"" operator $\flat$ is defined by by $v^{\flat}(u) = \langle u , v \rangle$ for all $u \in V$ and is thus just the Riesz isomorphism in the direction $V \rightarrow V^*$ as defined above. On the other hand, given a linear functional $f_v \in V^*$, we know that there exists a unique $v \in V$ such that $f_v(u) = \langle u , v \rangle$ for all $u \in V$ and the ""sharp"" operator $\sharp$ is defined by  by $f_v^{\sharp} = v$ and this represents the other direction of the Riesz isomorphism. Is this understanding correct? Can anyone provide a reference to some examples/exercises that would explore these operators in a concrete way? The Wikipedia page on the topic isn't of much help. Update: I am adding a bounty to this question in hopes that someone will be able to provide examples/exercises (or references to such) that illustrate the use of the musical isomorphisms in the context of vector spaces.","I am trying to solidify my understanding of the muscial isomorphisms in the context of vector spaces. I believe I understand the definitions but would appreciate corrections if my understanding is not correct. Also, as I have had some difficulty tracking down related material, I would welcome any suggested references that would expand upon this material and related results. So, let $V$ be a finite-dimensional vector space over $\mathbb{R}$ with inner product $\langle \cdot, \cdot \rangle$ and let $V^*$ denote its dual. For each element $v \in V$, one can define a mapping $V \rightarrow V^*$ by $v \mapsto \langle \cdot, v \rangle$. By the Riesz representation theorem this mapping actually determines an isomorphism that allows us to identify each element in $V$ with a unique functional in $V^*$ The ""flat"" operator $\flat$ is defined by by $v^{\flat}(u) = \langle u , v \rangle$ for all $u \in V$ and is thus just the Riesz isomorphism in the direction $V \rightarrow V^*$ as defined above. On the other hand, given a linear functional $f_v \in V^*$, we know that there exists a unique $v \in V$ such that $f_v(u) = \langle u , v \rangle$ for all $u \in V$ and the ""sharp"" operator $\sharp$ is defined by  by $f_v^{\sharp} = v$ and this represents the other direction of the Riesz isomorphism. Is this understanding correct? Can anyone provide a reference to some examples/exercises that would explore these operators in a concrete way? The Wikipedia page on the topic isn't of much help. Update: I am adding a bounty to this question in hopes that someone will be able to provide examples/exercises (or references to such) that illustrate the use of the musical isomorphisms in the context of vector spaces.",,"['linear-algebra', 'reference-request', 'inner-products']"
1,"Is the determinant the ""only"" group homomorphism from $\mathrm{GL}_n(\mathbb R)$ to $\mathbb R^\times$?","Is the determinant the ""only"" group homomorphism from  to ?",\mathrm{GL}_n(\mathbb R) \mathbb R^\times,"This might be a dumb question; I know only enough group theory to be able to ask dumb questions. Ken W. Smith has pointed out that one way to get intuition about the determinant is to observe that it maps matrix multiplication to real multiplication. As it is continuous, too, this means that it is a Lie group homomorphism from $\mathrm{GL}_n(\mathbb R)$ to the multiplicative group of the nonzero reals, $\mathbb R^\times$. The natural question to ask, then, is whether it is the only such homomorphism. Obviously not: any function $\mathbf A \mapsto (\det\mathbf A)^k$ for $k\in\mathbb Z$ is also a homomorphism. But are all homomorphisms between the two groups of such a form? That is, Is every Lie group homomorphism from $\mathrm{GL}_n(\mathbb R)$ to $\mathbb R^\times$ identical to $\mathbf A \mapsto f(\det\mathbf A)$ for some homomorphism $f:\mathbb R^\times\to\mathbb R^\times$?","This might be a dumb question; I know only enough group theory to be able to ask dumb questions. Ken W. Smith has pointed out that one way to get intuition about the determinant is to observe that it maps matrix multiplication to real multiplication. As it is continuous, too, this means that it is a Lie group homomorphism from $\mathrm{GL}_n(\mathbb R)$ to the multiplicative group of the nonzero reals, $\mathbb R^\times$. The natural question to ask, then, is whether it is the only such homomorphism. Obviously not: any function $\mathbf A \mapsto (\det\mathbf A)^k$ for $k\in\mathbb Z$ is also a homomorphism. But are all homomorphisms between the two groups of such a form? That is, Is every Lie group homomorphism from $\mathrm{GL}_n(\mathbb R)$ to $\mathbb R^\times$ identical to $\mathbf A \mapsto f(\det\mathbf A)$ for some homomorphism $f:\mathbb R^\times\to\mathbb R^\times$?",,['linear-algebra']
2,LU Decomposition vs. Cholesky Decomposition,LU Decomposition vs. Cholesky Decomposition,,What is the difference between LU Decomposition and Cholesky Decomposition about using these methods to solving linear equation systems? Could you explain the difference with a simple example? Also could you explain the differences between these decomposition methods in: inverse of a matrix forward and backward substitution pivoting,What is the difference between LU Decomposition and Cholesky Decomposition about using these methods to solving linear equation systems? Could you explain the difference with a simple example? Also could you explain the differences between these decomposition methods in: inverse of a matrix forward and backward substitution pivoting,,"['linear-algebra', 'matrices', 'matrix-decomposition', 'lu-decomposition', 'cholesky-decomposition']"
3,Can the matrices $A$ and $I+A$ have the same determinant?,Can the matrices  and  have the same determinant?,A I+A,"Let $A\in\mathbb R^{n\times n}$ be an arbitrary matrix. Can $A$ and $I+A$ have the same determinant, if not how to prove it?  Furthermore, can $A$ and $I+A$ have the same eigenvalues?","Let $A\in\mathbb R^{n\times n}$ be an arbitrary matrix. Can $A$ and $I+A$ have the same determinant, if not how to prove it?  Furthermore, can $A$ and $I+A$ have the same eigenvalues?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
4,Fast(est) and intuitive ways to look at matrix multiplication?,Fast(est) and intuitive ways to look at matrix multiplication?,,"Most of the time I see matrix multiplication presented and defined , as a seemingly arbitrary sequence of operations. For example, the textbook I'm currently reading for a linear algebra course defines the product $AB$ as the $(i, j)$ entry in the $1 \times 1$ matrix that is the product of the ith row of $A$ and the jth column of $B$. Properties of matrix multiplication are subsequently proven based upon this definition. The definition is clear, but why the matrix product is useful is not clear to me as a student. A different textbook I'm referencing defines the product $AB$ in terms of linear combinations. The problem I have is doing matrix multiplication quickly by hand, particularly when the $A$ is $p \times 1$ and $B$ is $1 \times q$. I would like to know of how to look at or define matrix multiplication, in a manner which makes it easy (for the average student) to compute by hand, while being intuitive and consistent for use for later proofs. wikipedia has a great article .","Most of the time I see matrix multiplication presented and defined , as a seemingly arbitrary sequence of operations. For example, the textbook I'm currently reading for a linear algebra course defines the product $AB$ as the $(i, j)$ entry in the $1 \times 1$ matrix that is the product of the ith row of $A$ and the jth column of $B$. Properties of matrix multiplication are subsequently proven based upon this definition. The definition is clear, but why the matrix product is useful is not clear to me as a student. A different textbook I'm referencing defines the product $AB$ in terms of linear combinations. The problem I have is doing matrix multiplication quickly by hand, particularly when the $A$ is $p \times 1$ and $B$ is $1 \times q$. I would like to know of how to look at or define matrix multiplication, in a manner which makes it easy (for the average student) to compute by hand, while being intuitive and consistent for use for later proofs. wikipedia has a great article .",,"['linear-algebra', 'matrices']"
5,"Conjecture: If matrix $M$ has entries (left to right, then top to botom) $\sin 1,\sin 2,\sin 3,\dots,\sin (n^2)$, where $n\ge 3$, then $\det M = 0$.","Conjecture: If matrix  has entries (left to right, then top to botom) , where , then .","M \sin 1,\sin 2,\sin 3,\dots,\sin (n^2) n\ge 3 \det M = 0","According to my calculator, $\det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 \\ \sin 4 & \sin 5 & \sin 6 \\ \sin 7 & \sin 8 & \sin 9\end{bmatrix}=0$ $\det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 & \sin 4 \\ \sin 5 & \sin 6 & \sin 7  & \sin 8 \\ \sin 9 & \sin 10 & \sin 11 & \sin 12 \\ \sin 13 & \sin 14 & \sin 15 & \sin 16 \end{bmatrix}=0$ $\det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 & \sin 4 & \sin 5 \\ \sin 6 & \sin 7 & \sin 8  & \sin 9 & \sin 10 \\ \sin 11 & \sin 12 & \sin 13 & \sin 14  & \sin 15 \\ \sin 16 & \sin 17 & \sin 18 & \sin 19  & \sin 20 \\ \sin 21 & \sin 22 & \sin 23 & \sin 24 & \sin 25\end{bmatrix}=0$ I conjecture that, for $n\ge 3$ , $\det \begin{bmatrix} \sin 1 & \sin 2 & \sin 3 & \dots & \sin n \\ \sin (n+1) & \sin (n+2) & \sin (n+3) & \dots & \sin (2n) \\ \sin (2n+1) & \sin (2n+2) & \sin (2n+3) & \dots & \sin(3n) \\ \vdots & \vdots & \vdots & \vdots & \vdots \\ \sin ((n-1)n+1) & \sin ((n-1)n+2) & \sin ((n-1)n+3) & \dots & \sin (n^2) \end{bmatrix}=0$ Is my conjecture true? I have only been able to prove the case with $n=3$ . $\sin 5 + \sin 7 + [\sin 1 + \sin (-1)] + [\sin 3 + \sin (-3)]$ $=\sin 5 + \sin 7 + [\sin 1 + \sin (-1)] + [\sin 3 + \sin (-3)]$ Rearrange each side: $[\sin (-3) + \sin 5] + [\sin 1 + \sin 3] + [\sin (-1) + \sin 7]$ $=[\sin (-1) + \sin 3] + [\sin (-3) + \sin 7] + [\sin 1 + \sin 5]$ Use the product-to-sum formulas : $(\sin 1)(\cos 4) + (\sin 2)(\cos 1) + (\sin 3)(\cos 4)$ $=(\sin 1)(\cos 2) + (\sin 2)(\cos 5) + (\sin 3)(\cos 2)$ Subtract $(\sin 1)(\cos 14)+(\sin 2)(\cos 13)+(\sin 3)(\cos 12)$ from both sides: $(\sin 1)(\cos 4 - \cos 14) + (\sin 2)(\cos 1 - \cos 13) + (\sin 3)(\cos 4 - \cos 12)$ $=(\sin 1)(\cos 2 - \cos 14) + (\sin 2)(\cos 5 - \cos 13) + (\sin 3)(\cos 2 - \cos 12)$ Use the product-to-sum formulas again: $(\sin 1)(\sin 5)(\sin 9)+(\sin 2)(\sin 6)(\sin 7)+(\sin 3)(\sin 4)(\sin 8)$ $=(\sin 1)(\sin 6)(\sin 8)+(\sin 2)(\sin 4)(\sin 9)+(\sin 3)(\sin 5)(\sin 7)$ which is equivalent to $\det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 \\ \sin 4 & \sin 5 & \sin 6 \\ \sin 7 & \sin 8 & \sin 9\end{bmatrix}=0$","According to my calculator, I conjecture that, for , Is my conjecture true? I have only been able to prove the case with . Rearrange each side: Use the product-to-sum formulas : Subtract from both sides: Use the product-to-sum formulas again: which is equivalent to",\det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 \\ \sin 4 & \sin 5 & \sin 6 \\ \sin 7 & \sin 8 & \sin 9\end{bmatrix}=0 \det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 & \sin 4 \\ \sin 5 & \sin 6 & \sin 7  & \sin 8 \\ \sin 9 & \sin 10 & \sin 11 & \sin 12 \\ \sin 13 & \sin 14 & \sin 15 & \sin 16 \end{bmatrix}=0 \det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 & \sin 4 & \sin 5 \\ \sin 6 & \sin 7 & \sin 8  & \sin 9 & \sin 10 \\ \sin 11 & \sin 12 & \sin 13 & \sin 14  & \sin 15 \\ \sin 16 & \sin 17 & \sin 18 & \sin 19  & \sin 20 \\ \sin 21 & \sin 22 & \sin 23 & \sin 24 & \sin 25\end{bmatrix}=0 n\ge 3 \det \begin{bmatrix} \sin 1 & \sin 2 & \sin 3 & \dots & \sin n \\ \sin (n+1) & \sin (n+2) & \sin (n+3) & \dots & \sin (2n) \\ \sin (2n+1) & \sin (2n+2) & \sin (2n+3) & \dots & \sin(3n) \\ \vdots & \vdots & \vdots & \vdots & \vdots \\ \sin ((n-1)n+1) & \sin ((n-1)n+2) & \sin ((n-1)n+3) & \dots & \sin (n^2) \end{bmatrix}=0 n=3 \sin 5 + \sin 7 + [\sin 1 + \sin (-1)] + [\sin 3 + \sin (-3)] =\sin 5 + \sin 7 + [\sin 1 + \sin (-1)] + [\sin 3 + \sin (-3)] [\sin (-3) + \sin 5] + [\sin 1 + \sin 3] + [\sin (-1) + \sin 7] =[\sin (-1) + \sin 3] + [\sin (-3) + \sin 7] + [\sin 1 + \sin 5] (\sin 1)(\cos 4) + (\sin 2)(\cos 1) + (\sin 3)(\cos 4) =(\sin 1)(\cos 2) + (\sin 2)(\cos 5) + (\sin 3)(\cos 2) (\sin 1)(\cos 14)+(\sin 2)(\cos 13)+(\sin 3)(\cos 12) (\sin 1)(\cos 4 - \cos 14) + (\sin 2)(\cos 1 - \cos 13) + (\sin 3)(\cos 4 - \cos 12) =(\sin 1)(\cos 2 - \cos 14) + (\sin 2)(\cos 5 - \cos 13) + (\sin 3)(\cos 2 - \cos 12) (\sin 1)(\sin 5)(\sin 9)+(\sin 2)(\sin 6)(\sin 7)+(\sin 3)(\sin 4)(\sin 8) =(\sin 1)(\sin 6)(\sin 8)+(\sin 2)(\sin 4)(\sin 9)+(\sin 3)(\sin 5)(\sin 7) \det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 \\ \sin 4 & \sin 5 & \sin 6 \\ \sin 7 & \sin 8 & \sin 9\end{bmatrix}=0,"['linear-algebra', 'matrices', 'trigonometry', 'determinant', 'conjectures']"
6,Do eigenvalues depend on the choice of basis?,Do eigenvalues depend on the choice of basis?,,"Suppose we have a basis $B$ for an endomorphism $f$ that has eigenvalues $\lambda_{1},\dots,\lambda_{k}$. Do these eigenvalues change or stay the same if we change to another basis $B'$?","Suppose we have a basis $B$ for an endomorphism $f$ that has eigenvalues $\lambda_{1},\dots,\lambda_{k}$. Do these eigenvalues change or stay the same if we change to another basis $B'$?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'change-of-basis']"
7,Can every nonsingular $n\times n$ matrix with real entries be made singular by changing exactly one entry?,Can every nonsingular  matrix with real entries be made singular by changing exactly one entry?,n\times n,I was just thinking about this problem: Can every nonsingular $n\times n$ matrix with real entries be made singular by changing exactly one entry? Thanks for helping me.,I was just thinking about this problem: Can every nonsingular $n\times n$ matrix with real entries be made singular by changing exactly one entry? Thanks for helping me.,,"['linear-algebra', 'matrices']"
8,Interpreting the Cayley-Hamilton theorem,Interpreting the Cayley-Hamilton theorem,,"The statement of the Cayley-Hamilton Theorem is fairly straight-forward. I now know how to find characteristic polynomials from a given matrix (or at least a matrix with certain properties that I am unaware of!). I know that the eigenvalues of the matrix are roots of the polynomial. But what does having such a polynomial mean ? Wikipedia says that the characteristic polynomial ""...encodes several important properties of the matrix..."", but once we have switched to ""matrix form"" of the equation, what can we conclude? In other words, what does the Theorem do for us, besides allowing us to say, ""Hey, I know a matrix solution to this polynomial""?? Is there an abstraction of this in abstract algebra (rings, fields, etc.)? Thanks for your time.","The statement of the Cayley-Hamilton Theorem is fairly straight-forward. I now know how to find characteristic polynomials from a given matrix (or at least a matrix with certain properties that I am unaware of!). I know that the eigenvalues of the matrix are roots of the polynomial. But what does having such a polynomial mean ? Wikipedia says that the characteristic polynomial ""...encodes several important properties of the matrix..."", but once we have switched to ""matrix form"" of the equation, what can we conclude? In other words, what does the Theorem do for us, besides allowing us to say, ""Hey, I know a matrix solution to this polynomial""?? Is there an abstraction of this in abstract algebra (rings, fields, etc.)? Thanks for your time.",,"['linear-algebra', 'matrices', 'soft-question', 'cayley-hamilton', 'characteristic-polynomial']"
9,"Difference between epimorphism, isomorphism, endomorphism and automorphism (with examples)","Difference between epimorphism, isomorphism, endomorphism and automorphism (with examples)",,"Can somebody please explain me the difference between linear transformations such as epimorphism, isomorphism, endomorphism or automorphism? I would appreciate if somebody can explain the idea with examples or guide to some good source to clear the concept.","Can somebody please explain me the difference between linear transformations such as epimorphism, isomorphism, endomorphism or automorphism? I would appreciate if somebody can explain the idea with examples or guide to some good source to clear the concept.",,['linear-algebra']
10,Why is it called Sylvester's law of inertia?,Why is it called Sylvester's law of inertia?,,"In Sylvester's law of inertia , how does the name ""law of inertia"" fit with the statement of the theorem? I guess it's from physics, but I just don't see the connection.","In Sylvester's law of inertia , how does the name ""law of inertia"" fit with the statement of the theorem? I guess it's from physics, but I just don't see the connection.",,"['linear-algebra', 'matrices', 'soft-question', 'math-history']"
11,The intuition behind generalized eigenvectors,The intuition behind generalized eigenvectors,,An ordinary eigenvector can be viewed as a vector on which the operator acts by only stretching (without rotating) it. Is there a similar intuition behind generalized eigenvectors? EDIT: By generalized eigenvectors I'm referring to vectors in the kernel of $(T-\lambda I)^k$ for some $k$.,An ordinary eigenvector can be viewed as a vector on which the operator acts by only stretching (without rotating) it. Is there a similar intuition behind generalized eigenvectors? EDIT: By generalized eigenvectors I'm referring to vectors in the kernel of $(T-\lambda I)^k$ for some $k$.,,"['linear-algebra', 'intuition']"
12,"For $\det(A)=0$, how do we know if $A x = b$ has no solution or infinitely many solutions?","For , how do we know if  has no solution or infinitely many solutions?",\det(A)=0 A x = b,"Given a linear system of equations $A\vec x = \vec b$ , if the determinant $\det(A)$ is $0$ , then how do we know if the system has no solutions or infinitely many solutions? Two sub-questions: a) Using Cramer's Rule, the determinant of $A$ being zero means that a situation of ""Division by zero"" arises. Then, there being no solution is understandable as division by zero is not defined. But it confuses me how then, in any circumstance , the system can have solutions at all, since the formula in Cramer's rule is undefined. Is there an intuitive and insightful explanation? b) Given that $\det(A) = 0 $ , am I right to think that there are infinitely many solutions if and only if the system of equations is homogeneous, i.e. $\vec b = \vec 0$ ? Please explain why or why not.","Given a linear system of equations , if the determinant is , then how do we know if the system has no solutions or infinitely many solutions? Two sub-questions: a) Using Cramer's Rule, the determinant of being zero means that a situation of ""Division by zero"" arises. Then, there being no solution is understandable as division by zero is not defined. But it confuses me how then, in any circumstance , the system can have solutions at all, since the formula in Cramer's rule is undefined. Is there an intuitive and insightful explanation? b) Given that , am I right to think that there are infinitely many solutions if and only if the system of equations is homogeneous, i.e. ? Please explain why or why not.",A\vec x = \vec b \det(A) 0 A \det(A) = 0  \vec b = \vec 0,"['linear-algebra', 'matrices', 'systems-of-equations', 'determinant']"
13,What are the angle brackets in Linear Algebra?,What are the angle brackets in Linear Algebra?,,"In my linear algebra book, they have angle brackets around two different vectors, so it looks like this: $\langle\mathbf{u_2},\mathbf{v}_1\rangle$. They don't use angle brackets to define vectors, but use regular parenthesis instead. For the Gram-Schmidt process, they define $\mathbf{v}_1 = \mathbf{u}_1 = (1,1,1)$ and $\mathbf{v}_2 = \mathbf{u}_2 = \mathbf{u}_2 -  \dfrac{\langle\mathbf{u}_2, \mathbf{v}_1\rangle}{\|\mathbf{v}_1\|^2} \mathbf{v}_1$ where $\mathbf{u}_2 = (0,1,1)$ They conclude that that formula is equal to $(0,1,1) - \dfrac{2}{3}(1,1,1)$. What operation is the angle brackets to have that result?","In my linear algebra book, they have angle brackets around two different vectors, so it looks like this: $\langle\mathbf{u_2},\mathbf{v}_1\rangle$. They don't use angle brackets to define vectors, but use regular parenthesis instead. For the Gram-Schmidt process, they define $\mathbf{v}_1 = \mathbf{u}_1 = (1,1,1)$ and $\mathbf{v}_2 = \mathbf{u}_2 = \mathbf{u}_2 -  \dfrac{\langle\mathbf{u}_2, \mathbf{v}_1\rangle}{\|\mathbf{v}_1\|^2} \mathbf{v}_1$ where $\mathbf{u}_2 = (0,1,1)$ They conclude that that formula is equal to $(0,1,1) - \dfrac{2}{3}(1,1,1)$. What operation is the angle brackets to have that result?",,['linear-algebra']
14,Prove that every positive semidefinite matrix has nonnegative eigenvalues,Prove that every positive semidefinite matrix has nonnegative eigenvalues,,There is a theorem which states that every positive semidefinite matrix only has eigenvalues $\ge0$ How can I prove this theorem?,There is a theorem which states that every positive semidefinite matrix only has eigenvalues $\ge0$ How can I prove this theorem?,,"['linear-algebra', 'matrices', 'positive-semidefinite']"
15,Why the whole exterior algebra?,Why the whole exterior algebra?,,"So, I've been reading up on multilinear algebra a bit. In particular, I've been reading up on the construction of of the exterior algebra of a finite dimensional vector space $X$, say over $\mathbb{R}$.  $$ \Lambda(X) = \bigoplus_{n \geq 0} \Lambda^k(X) $$ I'm still at that frustrating early stage where the definitions seem very unmotivated. I'm hoping for some suggestions for improving my grip on them. Let me describe one particular thing which is bothering me in hopes that my concerns are easily dispelled.  I don't understand the point of having a product $\Lambda(X) \times \Lambda(X) \to \Lambda(X)$ instead of just paying attention to the products $\Lambda^k \times \Lambda^\ell(X) \to \Lambda^{k+\ell}(X)$ which seem to be all that is important. My problem may be that the only case I have any experience with is the case $X=X^*$ (a dual space) in which case the elements of $\Lambda^k(X^*)$ can be identified with alternating $k$-linear functionals $X^k \to \mathbb{R}$. It seems strange to me to want to put the forms of different ranks together in the same algebra. What is the use of an expression of mixed rank like  $$\omega = dx + dy \wedge dz$$  which is, I suppose, an element of $\Lambda((\mathbb{R}^3)^*)$? I think the thing which irritates me the most is that these mixed expressions do not even necessarily alternate! I mean, for Pete's sake, look! \begin{align*} \omega \wedge \omega & = (dx + dy \wedge dz) \wedge (dx + dy \wedge dz) \\ &= dx \wedge dy \wedge dz + dy \wedge dz \wedge dx \\ &= dx \wedge dy \wedge dz + (-1)^2 dx\wedge dy \wedge dz \\ &= 2 dx \wedge dy \wedge dz \\ &\neq 0 \end{align*} What's the point of considering all these extraneous elements whose wedge square isn't even zero? Now one answer to my question might be ""well, isn't it useful to consider polynomials which aren't of homogenuous degree?"". I don't think this is good enough for me though. Until I see why it is really useful to put the ""$\Lambda^k(X)$""s together into an algebra, I'm going to be wary of the object $\Lambda(X)$. Added: I noticed there is some relevant information at this thread .","So, I've been reading up on multilinear algebra a bit. In particular, I've been reading up on the construction of of the exterior algebra of a finite dimensional vector space $X$, say over $\mathbb{R}$.  $$ \Lambda(X) = \bigoplus_{n \geq 0} \Lambda^k(X) $$ I'm still at that frustrating early stage where the definitions seem very unmotivated. I'm hoping for some suggestions for improving my grip on them. Let me describe one particular thing which is bothering me in hopes that my concerns are easily dispelled.  I don't understand the point of having a product $\Lambda(X) \times \Lambda(X) \to \Lambda(X)$ instead of just paying attention to the products $\Lambda^k \times \Lambda^\ell(X) \to \Lambda^{k+\ell}(X)$ which seem to be all that is important. My problem may be that the only case I have any experience with is the case $X=X^*$ (a dual space) in which case the elements of $\Lambda^k(X^*)$ can be identified with alternating $k$-linear functionals $X^k \to \mathbb{R}$. It seems strange to me to want to put the forms of different ranks together in the same algebra. What is the use of an expression of mixed rank like  $$\omega = dx + dy \wedge dz$$  which is, I suppose, an element of $\Lambda((\mathbb{R}^3)^*)$? I think the thing which irritates me the most is that these mixed expressions do not even necessarily alternate! I mean, for Pete's sake, look! \begin{align*} \omega \wedge \omega & = (dx + dy \wedge dz) \wedge (dx + dy \wedge dz) \\ &= dx \wedge dy \wedge dz + dy \wedge dz \wedge dx \\ &= dx \wedge dy \wedge dz + (-1)^2 dx\wedge dy \wedge dz \\ &= 2 dx \wedge dy \wedge dz \\ &\neq 0 \end{align*} What's the point of considering all these extraneous elements whose wedge square isn't even zero? Now one answer to my question might be ""well, isn't it useful to consider polynomials which aren't of homogenuous degree?"". I don't think this is good enough for me though. Until I see why it is really useful to put the ""$\Lambda^k(X)$""s together into an algebra, I'm going to be wary of the object $\Lambda(X)$. Added: I noticed there is some relevant information at this thread .",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'category-theory', 'multilinear-algebra']"
16,Direct proof that nilpotent matrix has zero trace,Direct proof that nilpotent matrix has zero trace,,"Does anyone know a proof from the first principles that a nilpotent matrix has zero trace. No eigenvalues, no characteristic polynomials, just definition and basic facts about bases and matrices.","Does anyone know a proof from the first principles that a nilpotent matrix has zero trace. No eigenvalues, no characteristic polynomials, just definition and basic facts about bases and matrices.",,"['linear-algebra', 'matrices', 'trace', 'nilpotence']"
17,Can non-linear transformations be represented as Transformation Matrices?,Can non-linear transformations be represented as Transformation Matrices?,,"I just came back from an intense linear algebra lecture which showed that linear transformations could be represented by transformation matrices; with more generalization, it was later shown that affine transformations (linear + translation) could be represented by matrix multiplication as well. This got me to thinking about all those other transformations I've picked up over the past years I've been studying mathematics.  For example, polar transformations -- transforming $x$ and $y$ to two new variables $r$ and $\theta$. If you mapped $r$ to the $r$ axis and $\theta$ to the $y$ axis, you'd basically have a coordinate transformation.  A rather warped one, at that. Is there a way to represent this using a transformation matrix?  I've tried fiddling around with the numbers but everything I've tried to work with has fallen apart quite embarrassingly. More importantly, is there a way to, given a specific non-linear transformation, construct a transformation matrix from it?","I just came back from an intense linear algebra lecture which showed that linear transformations could be represented by transformation matrices; with more generalization, it was later shown that affine transformations (linear + translation) could be represented by matrix multiplication as well. This got me to thinking about all those other transformations I've picked up over the past years I've been studying mathematics.  For example, polar transformations -- transforming $x$ and $y$ to two new variables $r$ and $\theta$. If you mapped $r$ to the $r$ axis and $\theta$ to the $y$ axis, you'd basically have a coordinate transformation.  A rather warped one, at that. Is there a way to represent this using a transformation matrix?  I've tried fiddling around with the numbers but everything I've tried to work with has fallen apart quite embarrassingly. More importantly, is there a way to, given a specific non-linear transformation, construct a transformation matrix from it?",,"['linear-algebra', 'transformation']"
18,Intuition/meaning behind quadratic forms,Intuition/meaning behind quadratic forms,,"My professor just covered quadratic forms, but unfortunately did not give any intuition behind their meaning, so I'm hoping to get some of that cleared up. I know that we define a quadratic form as $Q(x) = x^T Ax$ , for some symmetric (i.e orthogonally diagonalizable) matrix $A$. Is there some significance to this other than that it is a ""cool"" transformation from $R^n \to R$ ? Is it special in some way? He also spoke about the Principal Axis Theorem. After looking on Wikipedia, it seems that the PAT that he described is wildly different from what most of the internet says. The professor said that the PAT tells us that any quadratic form $Q(x)$ can be ""transformed"" (what does that even mean???) into the quadratic form $Q(y) = y^T Dy$ with no cross product term (the cross product term is defined as the $x_1\cdot x_2$ term in the quadratic form), where $D$ is a diagonal matrix. His proof used the fact that $Q(x) = x^T Ax = (x^T Q)D(Q^T x) = y^T Dy$ for some $y$. What does the ""transformed quadratic form"" represent? Why is it significant? All my professor did was define these things, and didn't explain any intuition.","My professor just covered quadratic forms, but unfortunately did not give any intuition behind their meaning, so I'm hoping to get some of that cleared up. I know that we define a quadratic form as $Q(x) = x^T Ax$ , for some symmetric (i.e orthogonally diagonalizable) matrix $A$. Is there some significance to this other than that it is a ""cool"" transformation from $R^n \to R$ ? Is it special in some way? He also spoke about the Principal Axis Theorem. After looking on Wikipedia, it seems that the PAT that he described is wildly different from what most of the internet says. The professor said that the PAT tells us that any quadratic form $Q(x)$ can be ""transformed"" (what does that even mean???) into the quadratic form $Q(y) = y^T Dy$ with no cross product term (the cross product term is defined as the $x_1\cdot x_2$ term in the quadratic form), where $D$ is a diagonal matrix. His proof used the fact that $Q(x) = x^T Ax = (x^T Q)D(Q^T x) = y^T Dy$ for some $y$. What does the ""transformed quadratic form"" represent? Why is it significant? All my professor did was define these things, and didn't explain any intuition.",,"['linear-algebra', 'quadratic-forms']"
19,Prove the determinant of this matrix,Prove the determinant of this matrix,,"We have an $n\times n$ square matrix $\left(a_{i,j}\right)_{1\leq i\leq n, \ 1\leq j\leq n}$ such that all elements on main diagonal are zero, whereas the other elements are defined as follows: $$a_{i,j}=\begin{cases} 1,&\text{if } i+j \text{ belongs to the Fibonacci numbers,}\\ 0,&\text{if } i+j \text{ does not belong to the Fibonacci numbers}.\\ \end{cases}$$ We know that when $n$ is odd, the determinant of this matrix is zero. Now prove that when $n$ is even, the determinant of this matrix is $0$ or $1$ or $-1$. (Use induction or other methods.) Also posted on MO .","We have an $n\times n$ square matrix $\left(a_{i,j}\right)_{1\leq i\leq n, \ 1\leq j\leq n}$ such that all elements on main diagonal are zero, whereas the other elements are defined as follows: $$a_{i,j}=\begin{cases} 1,&\text{if } i+j \text{ belongs to the Fibonacci numbers,}\\ 0,&\text{if } i+j \text{ does not belong to the Fibonacci numbers}.\\ \end{cases}$$ We know that when $n$ is odd, the determinant of this matrix is zero. Now prove that when $n$ is even, the determinant of this matrix is $0$ or $1$ or $-1$. (Use induction or other methods.) Also posted on MO .",,"['linear-algebra', 'matrices', 'determinant']"
20,Why is an orthogonal matrix called orthogonal?,Why is an orthogonal matrix called orthogonal?,,"I know a square matrix is called orthogonal if its rows (and columns) are pairwise orthonormal But is there a deeper reason for this, or is it only an historical reason? I find it is very confusing and the term would let me assume, that a matrix is called orthogonal if its rows (and columns) are orthogonal and that it is called orthonormal if its rows (and columns) are orthonormal but apparently that's not conventional. I know that square matrices with orthogonal columns have no special interest, but thats not the point. If I read the term orthogonal matrix my first assumption is, that its rows (and columns) are orthogonal what is correct of course, but the more important property is that they are also orthonormal So, Question : Why do you call an orthogonal matrix orthogonal and not orthonormal ? Wouldn't this be more precisely and clearly?","I know a square matrix is called orthogonal if its rows (and columns) are pairwise orthonormal But is there a deeper reason for this, or is it only an historical reason? I find it is very confusing and the term would let me assume, that a matrix is called orthogonal if its rows (and columns) are orthogonal and that it is called orthonormal if its rows (and columns) are orthonormal but apparently that's not conventional. I know that square matrices with orthogonal columns have no special interest, but thats not the point. If I read the term orthogonal matrix my first assumption is, that its rows (and columns) are orthogonal what is correct of course, but the more important property is that they are also orthonormal So, Question : Why do you call an orthogonal matrix orthogonal and not orthonormal ? Wouldn't this be more precisely and clearly?",,"['linear-algebra', 'matrices', 'terminology', 'orthonormal']"
21,Matrix generated by prime numbers,Matrix generated by prime numbers,,"Let $p$ be the vector of dimension $n^2$ consisting of ordered prime numbers i.e. $p= [ 1 \ 2 \ 3 \ 5 \ 7   \ldots]^T$ and $A$ be the matrix of dimension $n\times{n}$ constructed with this vector by the following way: the first  column of $A$ is the first  $n$ prime numbers from  vector $v$ the second column of $A$ is the next   $n$ prime numbers from  vector $v$ etc. Examples of such matrices: $\begin{bmatrix} 1 & 3 \\ 2 & 5 \\ \end{bmatrix}$, $\begin{bmatrix}  1 &  5 &  13 \\   2 &  7 &  17   \\  3  & 11 &  19       \end{bmatrix}$, $\begin{bmatrix}  1 &   7 &  19 &  37 \\   2 &  11 &  23 &  41 \\  3  & 13 &  29 &  43 \\  5 &  17  & 31 &  47  \end{bmatrix}$,  $\dots$ Question: is it true that for any $n$ $ \ $ rank$(A)=n$ (i.e. columns of $A$ are linearly independent) or for some $n$ the statement above is not true?","Let $p$ be the vector of dimension $n^2$ consisting of ordered prime numbers i.e. $p= [ 1 \ 2 \ 3 \ 5 \ 7   \ldots]^T$ and $A$ be the matrix of dimension $n\times{n}$ constructed with this vector by the following way: the first  column of $A$ is the first  $n$ prime numbers from  vector $v$ the second column of $A$ is the next   $n$ prime numbers from  vector $v$ etc. Examples of such matrices: $\begin{bmatrix} 1 & 3 \\ 2 & 5 \\ \end{bmatrix}$, $\begin{bmatrix}  1 &  5 &  13 \\   2 &  7 &  17   \\  3  & 11 &  19       \end{bmatrix}$, $\begin{bmatrix}  1 &   7 &  19 &  37 \\   2 &  11 &  23 &  41 \\  3  & 13 &  29 &  43 \\  5 &  17  & 31 &  47  \end{bmatrix}$,  $\dots$ Question: is it true that for any $n$ $ \ $ rank$(A)=n$ (i.e. columns of $A$ are linearly independent) or for some $n$ the statement above is not true?",,"['linear-algebra', 'matrices', 'number-theory', 'prime-numbers']"
22,Compute the $n$-th power of triangular $3\times3$ matrix,Compute the -th power of triangular  matrix,n 3\times3,I have the following matrix $$ \begin{bmatrix} 1 & 2 & 3\\ 0 & 1 & 2\\ 0 & 0 & 1 \end{bmatrix} $$ and I am asked to compute its $n$-th power (to express each element as a function of $n$). I don't know at all what to do. I tried to compute some values manually to see some pattern and deduce a general expression but that didn't gave anything (especially for the top right). Thank you.,I have the following matrix $$ \begin{bmatrix} 1 & 2 & 3\\ 0 & 1 & 2\\ 0 & 0 & 1 \end{bmatrix} $$ and I am asked to compute its $n$-th power (to express each element as a function of $n$). I don't know at all what to do. I tried to compute some values manually to see some pattern and deduce a general expression but that didn't gave anything (especially for the top right). Thank you.,,"['linear-algebra', 'matrices']"
23,How to compute this determinant as quickly as possible (without using any software or calculator)?,How to compute this determinant as quickly as possible (without using any software or calculator)?,,"I sat an Algebra test yesterday, which consisted of 30 questions and a total time of 45 minutes (an average of 1 min 30 secs per question). One question of the test was this: Given the matrix: $$A=\begin{bmatrix}      -2 & 4 & 2 & 1 \\      4 & 2 & 1 & -2 \\      2 & 1 & -2 & 4 \\      1 & -2 & 4 & 2  \end{bmatrix}$$   Which of the following options is correct? (A) $A^{-1}=\dfrac{A}{25}$ (B) $A^{-1}=\dfrac{A}{5}$ (C) $A^{-1}=\dfrac{A}{15}$ (D) It has no inverse. I do know how to compute the inverse of a matrix. For example, using the Gauss-Jordan elimination method . Or for example, using this formula: $$A^{-1}=\dfrac{\text{Adj}(A^T)}{\text{det}(A)}$$ I calculated the determinant and it is $625$. However, this won't help me pick the correct option (of course I can eliminate option D, which is false). How in the world am I supposed to solve this problem in around 90-100 seconds, without using a calculator? Is there any shortcut or trick or something I missed? 90 seconds was the average time per question in the test. Given how little time I was given to solve the problem, this leads me to think that the structure of A could simplify the answer.","I sat an Algebra test yesterday, which consisted of 30 questions and a total time of 45 minutes (an average of 1 min 30 secs per question). One question of the test was this: Given the matrix: $$A=\begin{bmatrix}      -2 & 4 & 2 & 1 \\      4 & 2 & 1 & -2 \\      2 & 1 & -2 & 4 \\      1 & -2 & 4 & 2  \end{bmatrix}$$   Which of the following options is correct? (A) $A^{-1}=\dfrac{A}{25}$ (B) $A^{-1}=\dfrac{A}{5}$ (C) $A^{-1}=\dfrac{A}{15}$ (D) It has no inverse. I do know how to compute the inverse of a matrix. For example, using the Gauss-Jordan elimination method . Or for example, using this formula: $$A^{-1}=\dfrac{\text{Adj}(A^T)}{\text{det}(A)}$$ I calculated the determinant and it is $625$. However, this won't help me pick the correct option (of course I can eliminate option D, which is false). How in the world am I supposed to solve this problem in around 90-100 seconds, without using a calculator? Is there any shortcut or trick or something I missed? 90 seconds was the average time per question in the test. Given how little time I was given to solve the problem, this leads me to think that the structure of A could simplify the answer.",,"['linear-algebra', 'matrices', 'inverse']"
24,Why is determinant a multilinear function?,Why is determinant a multilinear function?,,"I am trying to understand (intuitive explanation will be fine) why determinant is a multilinear function and therefore to learn how elementary row operation affect the determinant. I understand that it has something to do with the definition of determinant by permutations, due to permutation being a bijection, in each product of the determinant there is just one entry from each row, but what's next?","I am trying to understand (intuitive explanation will be fine) why determinant is a multilinear function and therefore to learn how elementary row operation affect the determinant. I understand that it has something to do with the definition of determinant by permutations, due to permutation being a bijection, in each product of the determinant there is just one entry from each row, but what's next?",,"['linear-algebra', 'determinant']"
25,If $A^2 = I$ (Identity Matrix) then $A = \pm I$,If  (Identity Matrix) then,A^2 = I A = \pm I,"So I'm studying linear algebra and one of the self-study exercises has a set of true or false questions. One of the questions is this: If $A^2 = I$ (Identity Matrix), then $A = \pm I$ ? I'm pretty sure it is true but the answer says it's false. How can this be false (maybe it's a typography error in the book)?","So I'm studying linear algebra and one of the self-study exercises has a set of true or false questions. One of the questions is this: If (Identity Matrix), then ? I'm pretty sure it is true but the answer says it's false. How can this be false (maybe it's a typography error in the book)?",A^2 = I A = \pm I,"['linear-algebra', 'matrices', 'examples-counterexamples']"
26,"Using the Left-Inverse to ""Solve"" an Impossible System of Equations","Using the Left-Inverse to ""Solve"" an Impossible System of Equations",,"I was working with the following system of equations: $$\begin{split}     \begin{bmatrix}     4 & 0\\     0 & 5\\     0 & 0\\     \end{bmatrix}     \begin{bmatrix}     x_1\\     x_2\\     \end{bmatrix}      & =      \begin{bmatrix}     1\\     1\\     1\\     \end{bmatrix}     \end{split} $$ Clearly, this has no solution on account of the last rows in the coefficient and solution matrices. However, multiplying by the left-inverse of the coefficient matrix seems to imply a solution: $$\begin{split}     \begin{bmatrix}     \frac{1}{4} & 0 & b_{13}\\     0 & \frac{1}{5} & b_{23}\\     \end{bmatrix}     \begin{bmatrix}     4 & 0\\     0 & 5\\     0 & 0\\     \end{bmatrix}     \begin{bmatrix}     x_1\\     x_2\\     \end{bmatrix}      & =      \begin{bmatrix}     \frac{1}{4} & 0 & b_{13}\\     0 & \frac{1}{5} & b_{23}\\     \end{bmatrix}     \begin{bmatrix}     1\\     1\\     1\\     \end{bmatrix} \\     \begin{bmatrix}     x_1\\     x_2\\     \end{bmatrix}      & =      \begin{bmatrix}     \frac{1}{4} + b_{13}\\     \frac{1}{5} + b_{23}\\     \end{bmatrix}     \end{split} $$ Where $b_{13}$ and $b_{23}$ can be any number. As stated above, there is no solution to this system, so any solution obtained by the above method is wrong. What I am not understanding is why, after applying the rules of matrix multiplication, does it seem possible that there is a solution? Did I make a mistake somewhere in the process, and/or is there some fundamental nuance of linear algebra that I'm missing here?","I was working with the following system of equations: $$\begin{split}     \begin{bmatrix}     4 & 0\\     0 & 5\\     0 & 0\\     \end{bmatrix}     \begin{bmatrix}     x_1\\     x_2\\     \end{bmatrix}      & =      \begin{bmatrix}     1\\     1\\     1\\     \end{bmatrix}     \end{split} $$ Clearly, this has no solution on account of the last rows in the coefficient and solution matrices. However, multiplying by the left-inverse of the coefficient matrix seems to imply a solution: $$\begin{split}     \begin{bmatrix}     \frac{1}{4} & 0 & b_{13}\\     0 & \frac{1}{5} & b_{23}\\     \end{bmatrix}     \begin{bmatrix}     4 & 0\\     0 & 5\\     0 & 0\\     \end{bmatrix}     \begin{bmatrix}     x_1\\     x_2\\     \end{bmatrix}      & =      \begin{bmatrix}     \frac{1}{4} & 0 & b_{13}\\     0 & \frac{1}{5} & b_{23}\\     \end{bmatrix}     \begin{bmatrix}     1\\     1\\     1\\     \end{bmatrix} \\     \begin{bmatrix}     x_1\\     x_2\\     \end{bmatrix}      & =      \begin{bmatrix}     \frac{1}{4} + b_{13}\\     \frac{1}{5} + b_{23}\\     \end{bmatrix}     \end{split} $$ Where $b_{13}$ and $b_{23}$ can be any number. As stated above, there is no solution to this system, so any solution obtained by the above method is wrong. What I am not understanding is why, after applying the rules of matrix multiplication, does it seem possible that there is a solution? Did I make a mistake somewhere in the process, and/or is there some fundamental nuance of linear algebra that I'm missing here?",,['linear-algebra']
27,Is the proof of Pythagorean theorem using dot (inner) product circular?,Is the proof of Pythagorean theorem using dot (inner) product circular?,,"$x,y$ are perpendicular if and only if $x\cdot y=0$ . Now, $||x+y||^2=(x+y)\cdot (x+y)=(x\cdot x)+(x\cdot y)+(y\cdot x)+(y\cdot y)$ . The middle two terms are zero if and only if $x,y$ are perpendicular. So, $||x+y||^2=(x\cdot x)+(y\cdot y)=||x||^2+||y||^2$ if and only if $x,y$ are perpendicular.  ( I copied this ) I think this argument is circular because the property $x\cdot y=0 $ implies $x$ and $y$ are perpendicular comes from the Pythagorean theorem. Oh, it just came to mind that the property could be derived from the law of cosines. The law of cosines can be proved without the Pythagorean theorem, right, so the proof isn't circular? Another question : If the property comes from the Pythagorean theorem or cosine law, then how does the dot product give a condition for orthogonality for higher dimensions? Edit :  The following quote by Poincare hepled me regarding the question: Mathematics is the art of giving the same name to different things.","are perpendicular if and only if . Now, . The middle two terms are zero if and only if are perpendicular. So, if and only if are perpendicular.  ( I copied this ) I think this argument is circular because the property implies and are perpendicular comes from the Pythagorean theorem. Oh, it just came to mind that the property could be derived from the law of cosines. The law of cosines can be proved without the Pythagorean theorem, right, so the proof isn't circular? Another question : If the property comes from the Pythagorean theorem or cosine law, then how does the dot product give a condition for orthogonality for higher dimensions? Edit :  The following quote by Poincare hepled me regarding the question: Mathematics is the art of giving the same name to different things.","x,y x\cdot y=0 ||x+y||^2=(x+y)\cdot (x+y)=(x\cdot x)+(x\cdot y)+(y\cdot x)+(y\cdot y) x,y ||x+y||^2=(x\cdot x)+(y\cdot y)=||x||^2+||y||^2 x,y x\cdot y=0  x y","['linear-algebra', 'geometry', 'proof-verification', 'logic']"
28,How to diagonalize this matrix...,How to diagonalize this matrix...,,"Can someone show me step-by-step how to diagonalize this matrix? I'm trying to teach myself differential equations + linear algebra, but I'm stumped on how to do this. I'd really appreciate if someone would take the time to do this with me! $\begin{bmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & -3 \\ \end{bmatrix}$","Can someone show me step-by-step how to diagonalize this matrix? I'm trying to teach myself differential equations + linear algebra, but I'm stumped on how to do this. I'd really appreciate if someone would take the time to do this with me! $\begin{bmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & -3 \\ \end{bmatrix}$",,['linear-algebra']
29,Is the empty set linearly independent or linearly dependent?,Is the empty set linearly independent or linearly dependent?,,Is the empty set linearly independent or dependent?,Is the empty set linearly independent or dependent?,,"['linear-algebra', 'definition']"
30,Does every linear operator have a minimal polynomial?,Does every linear operator have a minimal polynomial?,,"I know that a linear operator $T$ defined on a finite-dimensional vector space has a minimal polynomial since, by Caley-Hamilton, $g(T)=0$, where $g$ is the characteristic polynomial.  Is there a linear operator defined on an infinite-dimensional vector space that has no minimal polynomial?","I know that a linear operator $T$ defined on a finite-dimensional vector space has a minimal polynomial since, by Caley-Hamilton, $g(T)=0$, where $g$ is the characteristic polynomial.  Is there a linear operator defined on an infinite-dimensional vector space that has no minimal polynomial?",,"['linear-algebra', 'minimal-polynomials']"
31,Proof of elementary row operations for matrices?,Proof of elementary row operations for matrices?,,"I'm taking a Linear Algebra course, and we just started talking about matrices. So we were introduced to the elementary row operations for matrices which say that we can do the following: Interchange two rows. Multiply a row with a nonzero number. Add a row to another one multiplied by a number. Now I understood from the lecture in class how to use these and all, but I want to understand the logic behind number 3.  Is there a mathematical proof that shows that by adding row $R_1$ to row $R_2$ we are not changing the system of equations? Thanks in advance","I'm taking a Linear Algebra course, and we just started talking about matrices. So we were introduced to the elementary row operations for matrices which say that we can do the following: Interchange two rows. Multiply a row with a nonzero number. Add a row to another one multiplied by a number. Now I understood from the lecture in class how to use these and all, but I want to understand the logic behind number 3.  Is there a mathematical proof that shows that by adding row $R_1$ to row $R_2$ we are not changing the system of equations? Thanks in advance",,"['linear-algebra', 'matrices']"
32,Understanding the singular value decomposition (SVD),Understanding the singular value decomposition (SVD),,"Please, would someone be so kind and explain what exactly happens when Singular Value Decomposition is applied on a matrix? What are singular values, left singular, and right singular vectors? I know they are matrices of specific form, I know how to calculate it but I cannot understand their meaning. I have recently been sort of catching up with Linear Algebra and matrix operations. I came across some techniques of matrix decomposition, particularly Singular Value Decomposition and I must admit I am having problem to understand the meaning of SVD. I read a bit about eigenvalues and eigenvectors only because I was interested in PCA and I came across diagonalizing a covariance matrix which determines its eigenvectors and eigenvalues (to be variances) towards those eigenvectors. I finally understood it but SVD gives me really hard time. thanks","Please, would someone be so kind and explain what exactly happens when Singular Value Decomposition is applied on a matrix? What are singular values, left singular, and right singular vectors? I know they are matrices of specific form, I know how to calculate it but I cannot understand their meaning. I have recently been sort of catching up with Linear Algebra and matrix operations. I came across some techniques of matrix decomposition, particularly Singular Value Decomposition and I must admit I am having problem to understand the meaning of SVD. I read a bit about eigenvalues and eigenvectors only because I was interested in PCA and I came across diagonalizing a covariance matrix which determines its eigenvectors and eigenvalues (to be variances) towards those eigenvectors. I finally understood it but SVD gives me really hard time. thanks",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd']"
33,Is matrix transpose a linear transformation?,Is matrix transpose a linear transformation?,,"This was the question posed to me. Does there exist a matrix $A$ for which $AM$ = $M^T$ for every $M$ . The answer to this is obviously no as I can vary the dimension of $M$ . But now this lead me to think , if I take , lets say only $2\times2$ matrix  into consideration. Now for a matrix $M$ , $A=M^TM^{-1}$ so $A$ is not fixed and depends on $M$ , but the operation follows all conditions of a linear transformation and I had read that any linear transformation can be represented as a matrix. So is the last statement wrong or my argument flawed?","This was the question posed to me. Does there exist a matrix for which = for every . The answer to this is obviously no as I can vary the dimension of . But now this lead me to think , if I take , lets say only matrix  into consideration. Now for a matrix , so is not fixed and depends on , but the operation follows all conditions of a linear transformation and I had read that any linear transformation can be represented as a matrix. So is the last statement wrong or my argument flawed?",A AM M^T M M 2\times2 M A=M^TM^{-1} A M,"['linear-algebra', 'matrices', 'linear-transformations']"
34,Can you use row and column operations interchangeably?,Can you use row and column operations interchangeably?,,"Is it possible to use row and column operations ""at the same time"" on a matrix $A$? So, for example, first subtracting $row_1$ from $row_2$, and then choosing to multiply $column_3$ by a constant $c$? Or do you have to ""stick to one method"" when reducing a matrix? If so, can somebody explain why ? Edit: so let me put this more clearly. Suppose you have $$\left( \begin{array}{ccc} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23}  \\ a_{31} & a_{32} & a_{33} \end{array} \right)$$ First I multiply row 2 by $\frac{a_{11}}{a_{21}}$, row 3 by $\frac{a_{11}}{a_{31}}$, then subtract row 1 from row 2 and row 3 so you end up with a matrix of a new general form: $$\left( \begin{array}{ccc} a_{11} & a_{12} & a_{13} \\ 0 & b_{22} & b_{23}  \\ 0 & b_{32} & b_{33} \end{array} \right)$$ with $b_{ij}$ the new coefficients which were attained by our row operations. My question is: could you then say, subtract column 1 from columns 2 and 3 in order to get: $$\left( \begin{array}{ccc} a_{11} & 0 & 0 \\ 0 & b_{22} & b_{23}  \\ 0 & b_{32} & b_{33} \end{array} \right)?$$ Edit2: How come that, according to the answers, this is incorrect, even though this is the exact same thing Serge Lang does in his book ""Introduction to Linear Algebra""? What's happening here? Are you telling me Serge Lang doesn't know how to reduce a matrix? Reminder: this is NOT a linear system we're solving for, and this is NOT an augmented matrix. It's just a matrix $A$. Edit3: here is some more context.","Is it possible to use row and column operations ""at the same time"" on a matrix $A$? So, for example, first subtracting $row_1$ from $row_2$, and then choosing to multiply $column_3$ by a constant $c$? Or do you have to ""stick to one method"" when reducing a matrix? If so, can somebody explain why ? Edit: so let me put this more clearly. Suppose you have $$\left( \begin{array}{ccc} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23}  \\ a_{31} & a_{32} & a_{33} \end{array} \right)$$ First I multiply row 2 by $\frac{a_{11}}{a_{21}}$, row 3 by $\frac{a_{11}}{a_{31}}$, then subtract row 1 from row 2 and row 3 so you end up with a matrix of a new general form: $$\left( \begin{array}{ccc} a_{11} & a_{12} & a_{13} \\ 0 & b_{22} & b_{23}  \\ 0 & b_{32} & b_{33} \end{array} \right)$$ with $b_{ij}$ the new coefficients which were attained by our row operations. My question is: could you then say, subtract column 1 from columns 2 and 3 in order to get: $$\left( \begin{array}{ccc} a_{11} & 0 & 0 \\ 0 & b_{22} & b_{23}  \\ 0 & b_{32} & b_{33} \end{array} \right)?$$ Edit2: How come that, according to the answers, this is incorrect, even though this is the exact same thing Serge Lang does in his book ""Introduction to Linear Algebra""? What's happening here? Are you telling me Serge Lang doesn't know how to reduce a matrix? Reminder: this is NOT a linear system we're solving for, and this is NOT an augmented matrix. It's just a matrix $A$. Edit3: here is some more context.",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
35,How to prove $(AB)^T=B^T A^T$,How to prove,(AB)^T=B^T A^T,"Given an $m\times n$ -matrix $A$ and an $n\times p$ -matrix $B$ . Prove that $(AB)^T = B^TA^T$ . Here is my attempt: Write the matrices $A$ and $B$ as $A = [a_{ij}]$ and $B = [b_{ij}]$ , meaning that their $\left(i,j\right)$ -th entries are $a_{ij}$ and $b_{ij}$ , respectively. Let $C=AB=[c_{ij}]$ , where $c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}$ , the standard multiplication definition . We want $(AB)^T = C^T = [c_{ji}]$ . That is the element in position $j,i$ is $\sum_{k=1}^n a_{ik}b_{kj}$ . For instance, if $i=2, j=3$ , then the element in $2,3$ of $C$ is that sum, but the element in position $3,2$ of the transpose is that sum. I need to get the same value for the element in position $3,2$ of the right side. The transpose matrices are $B^T=[b_{ji}], A^T=[a_{ji}]$ . They are size $p \times n$ and $n \times m$ . That is, they switch rows and columns. Let $D = B^T A^T = [d_{ji}]$ . I write the indices backwards because if I want the element in position $3,2$ , that is, $i=2, j=3$ just like on the other side. So I need the summation for $d_{ji}$ . But I get as $d_{ji} = \sum_{k=1}^n b_{jk}a_{ki}$ , which does not match.","Given an -matrix and an -matrix . Prove that . Here is my attempt: Write the matrices and as and , meaning that their -th entries are and , respectively. Let , where , the standard multiplication definition . We want . That is the element in position is . For instance, if , then the element in of is that sum, but the element in position of the transpose is that sum. I need to get the same value for the element in position of the right side. The transpose matrices are . They are size and . That is, they switch rows and columns. Let . I write the indices backwards because if I want the element in position , that is, just like on the other side. So I need the summation for . But I get as , which does not match.","m\times n A n\times p B (AB)^T = B^TA^T A B A = [a_{ij}] B = [b_{ij}] \left(i,j\right) a_{ij} b_{ij} C=AB=[c_{ij}] c_{ij} = \sum_{k=1}^n a_{ik}b_{kj} (AB)^T = C^T = [c_{ji}] j,i \sum_{k=1}^n a_{ik}b_{kj} i=2, j=3 2,3 C 3,2 3,2 B^T=[b_{ji}], A^T=[a_{ji}] p \times n n \times m D = B^T A^T = [d_{ji}] 3,2 i=2, j=3 d_{ji} d_{ji} = \sum_{k=1}^n b_{jk}a_{ki}","['linear-algebra', 'matrices', 'transpose']"
36,"The Center of $\operatorname{GL}(n,k)$",The Center of,"\operatorname{GL}(n,k)","The given question: Let $k$ be a ﬁeld and $n \in \mathbb{N}$. Show that the centre of $\operatorname{GL}(n, k)$ is $\lbrace\lambda I\mid λ ∈ k^∗\rbrace$. I have spent a while trying to prove this and have succeeded if $ k \subseteq \mathbb{R}$. So I imagine there is a nicer way to go about this. I have seen people saying take matrices where every element except one is zero in proving similar results but such matrices are not in $\operatorname{GL}(n,k)$ as they are not invertible. What I have done. I have said let $B \in$ Center then $B$ commutes with everything in $\operatorname{GL}(n,k)$. So take a permutation matrix, $P$ which swaps rows $i,j$ $P \neq I$. From this you can deduce that $B^T = B$ and that $B_{ii} = B_{jj}$. We can use the fact that $B$ is symmetric to find an orthogonal diagonalisation of $B$ by the spectral theorem. This gives $QBQ^T = D \rightarrow B=D$. And as $B_{ii} = B_{jj}$ we can say $B = k * I$. But the spectral theorem requires $B$ to be a real matrix. How do I prove this for a general $k$?.","The given question: Let $k$ be a ﬁeld and $n \in \mathbb{N}$. Show that the centre of $\operatorname{GL}(n, k)$ is $\lbrace\lambda I\mid λ ∈ k^∗\rbrace$. I have spent a while trying to prove this and have succeeded if $ k \subseteq \mathbb{R}$. So I imagine there is a nicer way to go about this. I have seen people saying take matrices where every element except one is zero in proving similar results but such matrices are not in $\operatorname{GL}(n,k)$ as they are not invertible. What I have done. I have said let $B \in$ Center then $B$ commutes with everything in $\operatorname{GL}(n,k)$. So take a permutation matrix, $P$ which swaps rows $i,j$ $P \neq I$. From this you can deduce that $B^T = B$ and that $B_{ii} = B_{jj}$. We can use the fact that $B$ is symmetric to find an orthogonal diagonalisation of $B$ by the spectral theorem. This gives $QBQ^T = D \rightarrow B=D$. And as $B_{ii} = B_{jj}$ we can say $B = k * I$. But the spectral theorem requires $B$ to be a real matrix. How do I prove this for a general $k$?.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'linear-groups']"
37,"The meaning of notation like $f\colon \mathbb R^2 \to \mathbb R$, $x \in \mathbb R^n$, and $x \in \mathbb R$.","The meaning of notation like , , and .",f\colon \mathbb R^2 \to \mathbb R x \in \mathbb R^n x \in \mathbb R,"I am in second year university and am taking linear algebra this semester.  Never having been a strong maths student, I am certainly struggling with some basic concepts and especially notation. I have tried searching on the web but have had difficulty in finding something which properly explains the meaning of notation like $$ f: \Bbb{R^2} \to \Bbb{R}$$ or the difference between $x\in \Bbb{R^n}$ and $x \in \Bbb{R}$ I can basically read these, and know the literal pronounciation of the symbols, but have no idea what they actually mean. The first one would be $f$ maps $\Bbb{R^2}$ to $\Bbb{R}$. What does this mean exactly? Is it saying that on an $(x,y)$ plane, the function $f$ returns a single number? E.g $f(x) = 3x^2$ $f(1) = 3$? Is the second one saying that $x$ is an element of a vector space with $n$ elements $(ax_1, bx_2,....,a_nx_n)$, whereas the first one is saying that $x$ is just some real number? I would really appreciate if someone could help me with this, either explaining it or referring me to a nice book that is appropriate at a beginner level.  Further more does this type of notation have any specific name?","I am in second year university and am taking linear algebra this semester.  Never having been a strong maths student, I am certainly struggling with some basic concepts and especially notation. I have tried searching on the web but have had difficulty in finding something which properly explains the meaning of notation like $$ f: \Bbb{R^2} \to \Bbb{R}$$ or the difference between $x\in \Bbb{R^n}$ and $x \in \Bbb{R}$ I can basically read these, and know the literal pronounciation of the symbols, but have no idea what they actually mean. The first one would be $f$ maps $\Bbb{R^2}$ to $\Bbb{R}$. What does this mean exactly? Is it saying that on an $(x,y)$ plane, the function $f$ returns a single number? E.g $f(x) = 3x^2$ $f(1) = 3$? Is the second one saying that $x$ is an element of a vector space with $n$ elements $(ax_1, bx_2,....,a_nx_n)$, whereas the first one is saying that $x$ is just some real number? I would really appreciate if someone could help me with this, either explaining it or referring me to a nice book that is appropriate at a beginner level.  Further more does this type of notation have any specific name?",,"['linear-algebra', 'functions', 'notation']"
38,How do I prove that $\det A= \det A^T$?,How do I prove that ?,\det A= \det A^T,"I found this exercise in Artin. It asks me to prove that $\det A= \det A^T$ where $A^T$ is the transpose of the matrix $A$ . Can anyone please comment whether my proof is correct or not? Attempted solution: If $\det A=0$ , the $A$ is non-invertible. We know that a matrix is invertible iff $A^T$ is invertible. As $A$ is non-invertible, so is $A^T$ and therefore $\det A^T=0$ . If the matrix is invertible, then $A=E_rE_{r-1}\dots E_1$ for a finite sequence of elementary row operations, $E_i$ . Using the fact that $(AB)^T=B^TA^T$ and using induction, we infer that $A^T=E_1^TE_2^T\dots E_r^T$ . So, $\det A^T=\det (E_1^T)\det(E_2^T)\dots \det(E_r^T)$ .As $E_i^T$ is an elementary row-operation as well of the same kind, $\det E_i^T=\det E_i$ . Using that, $\det A^T=\det (E_1E_2\dots E_r)=\det A$ .","I found this exercise in Artin. It asks me to prove that where is the transpose of the matrix . Can anyone please comment whether my proof is correct or not? Attempted solution: If , the is non-invertible. We know that a matrix is invertible iff is invertible. As is non-invertible, so is and therefore . If the matrix is invertible, then for a finite sequence of elementary row operations, . Using the fact that and using induction, we infer that . So, .As is an elementary row-operation as well of the same kind, . Using that, .",\det A= \det A^T A^T A \det A=0 A A^T A A^T \det A^T=0 A=E_rE_{r-1}\dots E_1 E_i (AB)^T=B^TA^T A^T=E_1^TE_2^T\dots E_r^T \det A^T=\det (E_1^T)\det(E_2^T)\dots \det(E_r^T) E_i^T \det E_i^T=\det E_i \det A^T=\det (E_1E_2\dots E_r)=\det A,['linear-algebra']
39,Is U=V in the SVD of a symmetric positive semidefinite matrix?,Is U=V in the SVD of a symmetric positive semidefinite matrix?,,"Consider the SVD of matrix $A$: $$A = U \Sigma V^\top$$ If $A$ is a symmetric, positive semidefinite real matrix, is there a guarantee that $U = V$? Second question (out of curiosity): what is the minimum necessary condition for  $U = V$?","Consider the SVD of matrix $A$: $$A = U \Sigma V^\top$$ If $A$ is a symmetric, positive semidefinite real matrix, is there a guarantee that $U = V$? Second question (out of curiosity): what is the minimum necessary condition for  $U = V$?",,"['linear-algebra', 'matrices', 'svd', 'symmetric-matrices', 'positive-semidefinite']"
40,Is every matrix conjugate to its transpose in a continuous way?,Is every matrix conjugate to its transpose in a continuous way?,,"It is well-known that every square matrix is conjugate to its transpose . This means (in the case of real matrices) that, for each $n\times n$ matrix $M$ with real entries, there is a matrix $S_M\in GL(n,\mathbb{R})$ such that ${S_M}^{-1}MS_M=M^T$ . My question is: can you choose $S_M$ in such a way that it depends continuously on $M$ ? In other words: Is there a continuous map $\psi\colon M_{n,n}(\mathbb{R})\longrightarrow GL(n,\mathbb{R})$ such that $$\bigl(\forall M\in M_{n\times n}(\mathbb{R})\bigr):\psi(M)^{-1}.M.\psi(M)=M^T?$$ My guess is that the answer is negative even for $n=2$ . Note that, for each individual matrix $M$ , there are plenty of choices for $S_M$ . For instance, if $n=2$ and $$M=\begin{bmatrix}x&y\\z&t\end{bmatrix},$$ then you can take $$S_M=\begin{bmatrix}az&bz\\bz&bt-bx+ay\end{bmatrix},$$ with $a$ and $b$ chosen such that $\det(S_M)\neq0$ but, of course, this will only work if $z\neq0$ . What if $z=0$ ? Then you can take $$S_M=\begin{bmatrix}-at+ax&ay\\ay&by\end{bmatrix}$$ and, again, $a$ and $b$ should be chosen such that $\det(S_M)\neq0$ ; the problem now is that, of course, this will only work if $y\neq0$ . And so on. This looks like the problem of finding a logarithm for each $z\in\mathbb{C}\setminus\{0\}$ : there are plenty of choices for each individual $z$ , but there is no continuous way of picking one.","It is well-known that every square matrix is conjugate to its transpose . This means (in the case of real matrices) that, for each matrix with real entries, there is a matrix such that . My question is: can you choose in such a way that it depends continuously on ? In other words: Is there a continuous map such that My guess is that the answer is negative even for . Note that, for each individual matrix , there are plenty of choices for . For instance, if and then you can take with and chosen such that but, of course, this will only work if . What if ? Then you can take and, again, and should be chosen such that ; the problem now is that, of course, this will only work if . And so on. This looks like the problem of finding a logarithm for each : there are plenty of choices for each individual , but there is no continuous way of picking one.","n\times n M S_M\in GL(n,\mathbb{R}) {S_M}^{-1}MS_M=M^T S_M M \psi\colon M_{n,n}(\mathbb{R})\longrightarrow GL(n,\mathbb{R}) \bigl(\forall M\in M_{n\times n}(\mathbb{R})\bigr):\psi(M)^{-1}.M.\psi(M)=M^T? n=2 M S_M n=2 M=\begin{bmatrix}x&y\\z&t\end{bmatrix}, S_M=\begin{bmatrix}az&bz\\bz&bt-bx+ay\end{bmatrix}, a b \det(S_M)\neq0 z\neq0 z=0 S_M=\begin{bmatrix}-at+ax&ay\\ay&by\end{bmatrix} a b \det(S_M)\neq0 y\neq0 z\in\mathbb{C}\setminus\{0\} z","['linear-algebra', 'matrices', 'continuity', 'transpose', 'similar-matrices']"
41,How do you write a differential operator as a matrix?,How do you write a differential operator as a matrix?,,How do you write a differential operator as a matrix? I'm very confused. Could someone please use examples to help me understand? Preferably with first and second-order linear differentiation.,How do you write a differential operator as a matrix? I'm very confused. Could someone please use examples to help me understand? Preferably with first and second-order linear differentiation.,,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
42,Balance chemical equations without trial and error?,Balance chemical equations without trial and error?,,"In my AP chemistry class, I often have to balance chemical equations like the following: $$ \mathrm{Al} + \text O_2 \to \mathrm{Al}_2 \mathrm O_3 $$ The goal is to make both side of the arrow have the same amount of atoms by adding compounds in the equation to each side. A solution: $$ 4 \mathrm{Al} + 3 \mathrm{ O_2} \to 2 \mathrm{Al}_2 \mathrm{ O_3} $$ When the subscripts become really large, or there are a lot of atoms involved, trial and error is impossible unless performed by a computer. What if some chemical equation can not be balanced?  (Do such equations exist?) I tried one for a long time only to realize the problem was wrong. My teacher said trial and error is the only way. Are there other methods?","In my AP chemistry class, I often have to balance chemical equations like the following: $$ \mathrm{Al} + \text O_2 \to \mathrm{Al}_2 \mathrm O_3 $$ The goal is to make both side of the arrow have the same amount of atoms by adding compounds in the equation to each side. A solution: $$ 4 \mathrm{Al} + 3 \mathrm{ O_2} \to 2 \mathrm{Al}_2 \mathrm{ O_3} $$ When the subscripts become really large, or there are a lot of atoms involved, trial and error is impossible unless performed by a computer. What if some chemical equation can not be balanced?  (Do such equations exist?) I tried one for a long time only to realize the problem was wrong. My teacher said trial and error is the only way. Are there other methods?",,"['linear-algebra', 'systems-of-equations', 'chemistry']"
43,Fake proofs using matrices,Fake proofs using matrices,,"Having gone through the 16-page-list of questions tagged fake-proofs , and going though both the relevant MSE Question and Wikipedia page , I didn't find a single fake proof that involved matrices . So the question (or challange) here is: what are some fake proof using matrices? In particular, the fake proof should use a property, an operation, ..., specific to matrices (or at least not present in $\mathbb{R}$ or $\mathbb{C}$), e.g. Noncommutativity (Non-)existence of an inverse Matrix sizes Operations as $\det$, $\text{trace}$, ... Eigenvalues and diagonalization Matrix decompositions and normal forms ... Note: It does not matter if the result being ""proven"" is correct or not. The fallacy in the proof itself is what matters. Examples: Proof that 1 = 0 Proof: it is a well-known fact that $(x+y)^2 = x^2 + 2xy + y^2$.    Now let   $$x = \begin{pmatrix}0 & 1\\ 0 & 0 \end{pmatrix},\;\;y = \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix}.$$   On the one hand, we have that   $$ (x+y)^2 = \begin{pmatrix}1 & 1\\ 0 & 0 \end{pmatrix}^2 = \begin{pmatrix}1 & 1\\ 0 & 0 \end{pmatrix},$$   on the other hand we have   $$x^2 + 2xy + y^2 = \begin{pmatrix}0 & 0\\ 0 & 0 \end{pmatrix} + 2\begin{pmatrix}0 & 0\\ 0 & 0 \end{pmatrix} + \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix} = \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix}.$$   Since two matrices are equal if and only if all their entries are equal, we conclude that $1 = 0$. The mistake here is that $x$ and $y$  do not commute. Thus $(x+y)^2 = x^2 + xy + yx + y^2 \neq x^2 + 2xy + y^2$. Proof that 2 = 0 Proof: We know that $\det (AB) = \det (BA)$, since $$\det (AB) = (\det A) (\det B) = (\det B) (\det A) = \det (BA).$$   Now consider the matrices   $$ A = \begin{pmatrix}1 & 0 & 1\\ 0 & 1 & 0 \end{pmatrix}, \; \; B = \begin{pmatrix}1 & 0\\ 0 & 1\\ 1 & 0 \end{pmatrix}.$$   We have that   $$AB = \begin{pmatrix}2 & 0\\ 0 & 1 \end{pmatrix}, \;\; BA = \begin{pmatrix}1 & 0 & 1\\ 0 & 1 & 0 \\1 & 0 & 1\end{pmatrix}.$$   Hence $\det (AB) = 2$ and $\det (BA) = 0$, therefore $2 = 0$. The mistake here is that $\det$ is defined for square matrices only, and thus $\det AB = \det BA$ only holds in general if $A$ and $B$ are square.","Having gone through the 16-page-list of questions tagged fake-proofs , and going though both the relevant MSE Question and Wikipedia page , I didn't find a single fake proof that involved matrices . So the question (or challange) here is: what are some fake proof using matrices? In particular, the fake proof should use a property, an operation, ..., specific to matrices (or at least not present in $\mathbb{R}$ or $\mathbb{C}$), e.g. Noncommutativity (Non-)existence of an inverse Matrix sizes Operations as $\det$, $\text{trace}$, ... Eigenvalues and diagonalization Matrix decompositions and normal forms ... Note: It does not matter if the result being ""proven"" is correct or not. The fallacy in the proof itself is what matters. Examples: Proof that 1 = 0 Proof: it is a well-known fact that $(x+y)^2 = x^2 + 2xy + y^2$.    Now let   $$x = \begin{pmatrix}0 & 1\\ 0 & 0 \end{pmatrix},\;\;y = \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix}.$$   On the one hand, we have that   $$ (x+y)^2 = \begin{pmatrix}1 & 1\\ 0 & 0 \end{pmatrix}^2 = \begin{pmatrix}1 & 1\\ 0 & 0 \end{pmatrix},$$   on the other hand we have   $$x^2 + 2xy + y^2 = \begin{pmatrix}0 & 0\\ 0 & 0 \end{pmatrix} + 2\begin{pmatrix}0 & 0\\ 0 & 0 \end{pmatrix} + \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix} = \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix}.$$   Since two matrices are equal if and only if all their entries are equal, we conclude that $1 = 0$. The mistake here is that $x$ and $y$  do not commute. Thus $(x+y)^2 = x^2 + xy + yx + y^2 \neq x^2 + 2xy + y^2$. Proof that 2 = 0 Proof: We know that $\det (AB) = \det (BA)$, since $$\det (AB) = (\det A) (\det B) = (\det B) (\det A) = \det (BA).$$   Now consider the matrices   $$ A = \begin{pmatrix}1 & 0 & 1\\ 0 & 1 & 0 \end{pmatrix}, \; \; B = \begin{pmatrix}1 & 0\\ 0 & 1\\ 1 & 0 \end{pmatrix}.$$   We have that   $$AB = \begin{pmatrix}2 & 0\\ 0 & 1 \end{pmatrix}, \;\; BA = \begin{pmatrix}1 & 0 & 1\\ 0 & 1 & 0 \\1 & 0 & 1\end{pmatrix}.$$   Hence $\det (AB) = 2$ and $\det (BA) = 0$, therefore $2 = 0$. The mistake here is that $\det$ is defined for square matrices only, and thus $\det AB = \det BA$ only holds in general if $A$ and $B$ are square.",,"['linear-algebra', 'matrices', 'soft-question', 'big-list', 'fake-proofs']"
44,History of dot product and cosine,History of dot product and cosine,,"The fact that the dot product and the cosine of the angle between two vectors are mutually computable is easy to show (see the two sides in the two answers at Dot product in coordinates ). But looking at the dot product, I would never have thought that it somehow captures something about the angle (and vice versa). How did the connection get discovered? Who were the major players? Did it just fall out of the development of matrix operations for linear algebra (or did the dot product come first) or are these only related by hindsight or what?","The fact that the dot product and the cosine of the angle between two vectors are mutually computable is easy to show (see the two sides in the two answers at Dot product in coordinates ). But looking at the dot product, I would never have thought that it somehow captures something about the angle (and vice versa). How did the connection get discovered? Who were the major players? Did it just fall out of the development of matrix operations for linear algebra (or did the dot product come first) or are these only related by hindsight or what?",,"['linear-algebra', 'math-history']"
45,Decomposable elements of $\Lambda^k(V)$,Decomposable elements of,\Lambda^k(V),"I have a conjecture. I have a problem proving or disproving it. Let $w \in \Lambda^k(V)$ be a $k$-vector. Then $W_w=\{v\in V: v\wedge w = 0 \}$ is a $k$-dimensional vector space if and only if $w$ is decomposable. For example, for $u=e_1\wedge e_2 + e_3 \wedge e_4$ we have $W_u = 0$.","I have a conjecture. I have a problem proving or disproving it. Let $w \in \Lambda^k(V)$ be a $k$-vector. Then $W_w=\{v\in V: v\wedge w = 0 \}$ is a $k$-dimensional vector space if and only if $w$ is decomposable. For example, for $u=e_1\wedge e_2 + e_3 \wedge e_4$ we have $W_u = 0$.",,"['linear-algebra', 'exterior-algebra']"
46,Eigenvalues of the sum of two matrices: one diagonal and the other not.,Eigenvalues of the sum of two matrices: one diagonal and the other not.,,"I'm starting by a simple remark: if $A$ is a $n\times n$ matrix and $\{\lambda_1,\ldots,\lambda_k\}$ are its eigenvalues, then the eigenvalues of matrix $I+A$ (where $I$ is the identity matrix) are $\{\lambda_1+1,\ldots,\lambda_k+1\}$. Moreover, if $\alpha\in\mathbb R$, the eigenvalues of $\alpha I+A$ are $\{\lambda_1+\alpha,\ldots,\lambda_k+\alpha\}$. Are there more general results for this topic? Specifically, if $A$ is a $n\times n$ matrix and $\{\lambda_1,\ldots,\lambda_k\}$ are its eigenvalues, what are the eigenvalues of $A+D$ (where $D$ is a diagonal matrix )? Edit (First): I was wondering if the solution is known in the case where the sum of the elements of every row of $A$ is $0$ and all the entries of $D$ is between $0$ and $1$. Edit (Second): I was wondering if the solution is known when the sum of the elements of every row of $A$ is $0$ and $D=\operatorname{diag}(1, 0,\dots,0)$.","I'm starting by a simple remark: if $A$ is a $n\times n$ matrix and $\{\lambda_1,\ldots,\lambda_k\}$ are its eigenvalues, then the eigenvalues of matrix $I+A$ (where $I$ is the identity matrix) are $\{\lambda_1+1,\ldots,\lambda_k+1\}$. Moreover, if $\alpha\in\mathbb R$, the eigenvalues of $\alpha I+A$ are $\{\lambda_1+\alpha,\ldots,\lambda_k+\alpha\}$. Are there more general results for this topic? Specifically, if $A$ is a $n\times n$ matrix and $\{\lambda_1,\ldots,\lambda_k\}$ are its eigenvalues, what are the eigenvalues of $A+D$ (where $D$ is a diagonal matrix )? Edit (First): I was wondering if the solution is known in the case where the sum of the elements of every row of $A$ is $0$ and all the entries of $D$ is between $0$ and $1$. Edit (Second): I was wondering if the solution is known when the sum of the elements of every row of $A$ is $0$ and $D=\operatorname{diag}(1, 0,\dots,0)$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
47,What's the geometric meaning of a negative determinant?,What's the geometric meaning of a negative determinant?,,"Geometrically, the determinant of a matrix is the signed volume of a unit cube after the transformation defined by the matrix is applied. However, I'm have trouble understanding what the ""signed"" mean here. Since effectively volumes are only positive (or zero but for now let's not worry about it). So what's the geometric meaning of a negative determinant? How should I understand the negative volume produced by applying such transformation?","Geometrically, the determinant of a matrix is the signed volume of a unit cube after the transformation defined by the matrix is applied. However, I'm have trouble understanding what the ""signed"" mean here. Since effectively volumes are only positive (or zero but for now let's not worry about it). So what's the geometric meaning of a negative determinant? How should I understand the negative volume produced by applying such transformation?",,"['linear-algebra', 'matrices', 'geometry', 'determinant']"
48,Name for matrices with orthogonal (not necessarily orthonormal) rows,Name for matrices with orthogonal (not necessarily orthonormal) rows,,"Is there a name for a matrix whose rows (or columns) are non-zero orthogonal vectors ? It seems to me that ""orthogonal matrix"" would be a good name, but this is already taken -- it refers to a matrix whose rows (or columns) form an orthonormal set of vectors.","Is there a name for a matrix whose rows (or columns) are non-zero orthogonal vectors ? It seems to me that ""orthogonal matrix"" would be a good name, but this is already taken -- it refers to a matrix whose rows (or columns) form an orthonormal set of vectors.",,"['linear-algebra', 'matrices', 'terminology', 'orthogonality']"
49,Let $A=(a_{ij})$ be an $n\times n$ matrix. Suppose that $A^2$ is diagonal. Must $A$ be diagonal?,Let  be an  matrix. Suppose that  is diagonal. Must  be diagonal?,A=(a_{ij}) n\times n A^2 A,"Let $A=(a_{ij})$ be an $n\times n$ matrix. Suppose that $A^2$ is diagonal? Must $A$ be diagonal. In other words, is it true that $$A^{2}\;\text{is diagonal}\;\Longrightarrow a_{ij}=0,\;i\neq j\;\;?$$","Let $A=(a_{ij})$ be an $n\times n$ matrix. Suppose that $A^2$ is diagonal? Must $A$ be diagonal. In other words, is it true that $$A^{2}\;\text{is diagonal}\;\Longrightarrow a_{ij}=0,\;i\neq j\;\;?$$",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
50,Inverse of an invertible upper triangular matrix of order 3,Inverse of an invertible upper triangular matrix of order 3,,"Prove that the inverse of an invertible upper triangular matrix of order 3 is invertible and upper triangular. I have checked all the similar questions but I couldn't understand any of them. I supposed random 3x3 upper triangular matrix and tried to find its inverse, but it came out lower triangular matrix, not the upper triangular. Can anyone please give me a suggestion, how to prove it? $$ A=\left(\begin{array}{rrr}% a&b&c\\% 0&d&e\\% 0&0&f\\% \end{array}\right)% $$ $$ x11=\left(\begin{array}{rrr}% d&e\\% 0&f\\% \end{array}\right)% =df, x12=-\left(\begin{array}{rrr}% 0&e\\% 0&f\\% \end{array}\right)% =0, x13=\left(\begin{array}{rrr}% 0&d\\% 0&0\\% \end{array}\right)% =0 $$ $$ x21=-\left(\begin{array}{rrr}% b&c\\% 0&f\\% \end{array}\right)% =-bf, X22=\left(\begin{array}{rrr}% a&c\\% 0&f\\% \end{array}\right)% =af, X23=-\left(\begin{array}{rrr}% a&b\\% 0&0\\% \end{array}\right)% =0 $$ $$ x31=\left(\begin{array}{rrr}% b&c\\% d&e\\% \end{array}\right)% =bc-cd, x32=-\left(\begin{array}{rrr}% a&c\\% 0&e\\% \end{array}\right)% =ac, x31=\left(\begin{array}{rrr}% a&b\\% 0&d\\% \end{array}\right)% =ad $$ $$ adjoint A = \left(\begin{array}{rrr}% df&0&0\\% -bf&af&0\\% bc-cd&-ac&ad\\% \end{array}\right)% $$ $$ det A = a\left(\begin{array}{rrr}% d&e\\% 0&f\\% \end{array}\right)% =adf $$ $$ Inverse-A =1/adf  \left(\begin{array}{rrr}% df&0&0\\% -bf&af&0\\% bc-cd&-ac&ad\\% \end{array}\right)% $$ It came out lower triangular matrix. Is there any way to make it upper triangular matrix?","Prove that the inverse of an invertible upper triangular matrix of order 3 is invertible and upper triangular. I have checked all the similar questions but I couldn't understand any of them. I supposed random 3x3 upper triangular matrix and tried to find its inverse, but it came out lower triangular matrix, not the upper triangular. Can anyone please give me a suggestion, how to prove it? $$ A=\left(\begin{array}{rrr}% a&b&c\\% 0&d&e\\% 0&0&f\\% \end{array}\right)% $$ $$ x11=\left(\begin{array}{rrr}% d&e\\% 0&f\\% \end{array}\right)% =df, x12=-\left(\begin{array}{rrr}% 0&e\\% 0&f\\% \end{array}\right)% =0, x13=\left(\begin{array}{rrr}% 0&d\\% 0&0\\% \end{array}\right)% =0 $$ $$ x21=-\left(\begin{array}{rrr}% b&c\\% 0&f\\% \end{array}\right)% =-bf, X22=\left(\begin{array}{rrr}% a&c\\% 0&f\\% \end{array}\right)% =af, X23=-\left(\begin{array}{rrr}% a&b\\% 0&0\\% \end{array}\right)% =0 $$ $$ x31=\left(\begin{array}{rrr}% b&c\\% d&e\\% \end{array}\right)% =bc-cd, x32=-\left(\begin{array}{rrr}% a&c\\% 0&e\\% \end{array}\right)% =ac, x31=\left(\begin{array}{rrr}% a&b\\% 0&d\\% \end{array}\right)% =ad $$ $$ adjoint A = \left(\begin{array}{rrr}% df&0&0\\% -bf&af&0\\% bc-cd&-ac&ad\\% \end{array}\right)% $$ $$ det A = a\left(\begin{array}{rrr}% d&e\\% 0&f\\% \end{array}\right)% =adf $$ $$ Inverse-A =1/adf  \left(\begin{array}{rrr}% df&0&0\\% -bf&af&0\\% bc-cd&-ac&ad\\% \end{array}\right)% $$ It came out lower triangular matrix. Is there any way to make it upper triangular matrix?",,"['linear-algebra', 'matrices']"
51,Matrix with zeros on diagonal and ones in other places is invertible,Matrix with zeros on diagonal and ones in other places is invertible,,Hi guys I am working with this and I am trying to prove to myself that n by n matrices of the type zero on the diagonal and 1 everywhere else are invertible. I ran some cases and looked at the determinant and came to the conclusion that we can easily find the determinant by using the following $\det(A)=(-1)^{n+1}(n-1)$. To prove this I do induction n=2 we have the $A=\begin{bmatrix} 0 & 1\\  1 & 0 \end{bmatrix}$ $\det(A)=-1$ and my formula gives me the same thing (-1)(2-1)=-1 Now assume if for $n \times n$ and $\det(A)=(-1)^{n+1}(n-1)$ Now to show for  a matrix B of size $n+1 \times n+1$. I am not sure I was thinking to take the determinant of the $n \times n$ minors but I am maybe someone can help me. Also is there an easier way to see this is invertible other than the determinant? I am curious.,Hi guys I am working with this and I am trying to prove to myself that n by n matrices of the type zero on the diagonal and 1 everywhere else are invertible. I ran some cases and looked at the determinant and came to the conclusion that we can easily find the determinant by using the following $\det(A)=(-1)^{n+1}(n-1)$. To prove this I do induction n=2 we have the $A=\begin{bmatrix} 0 & 1\\  1 & 0 \end{bmatrix}$ $\det(A)=-1$ and my formula gives me the same thing (-1)(2-1)=-1 Now assume if for $n \times n$ and $\det(A)=(-1)^{n+1}(n-1)$ Now to show for  a matrix B of size $n+1 \times n+1$. I am not sure I was thinking to take the determinant of the $n \times n$ minors but I am maybe someone can help me. Also is there an easier way to see this is invertible other than the determinant? I am curious.,,"['linear-algebra', 'matrices']"
52,What will be the value of the following determinant without expanding it?,What will be the value of the following determinant without expanding it?,,"$$\begin{vmatrix}a^2 & (a+1)^2 & (a+2)^2 & (a+3)^2 \\ b^2 & (b+1)^2 & (b+2)^2 & (b+3)^2 \\ c^2 & (c+1)^2 & (c+2)^2 & (c+3)^2 \\ d^2 & (d+1)^2 & (d+2)^2 & (d+3)^2\end{vmatrix} $$ I tried many column operations, mainly subtractions without any success.","$$\begin{vmatrix}a^2 & (a+1)^2 & (a+2)^2 & (a+3)^2 \\ b^2 & (b+1)^2 & (b+2)^2 & (b+3)^2 \\ c^2 & (c+1)^2 & (c+2)^2 & (c+3)^2 \\ d^2 & (d+1)^2 & (d+2)^2 & (d+3)^2\end{vmatrix} $$ I tried many column operations, mainly subtractions without any success.",,"['linear-algebra', 'determinant']"
53,What does it mean to represent a number in term of a $2\times2$ matrix?,What does it mean to represent a number in term of a  matrix?,2\times2,"Today my friend showed me that the imaginary number can be represented in term of a matrix $$i = \pmatrix{0&-1\\1&0}$$ This was very very confusing for me because I have never thought of it as a matrix. But it is apparent that the properties of imaginary number holds even in this representation, namely $i\cdot i = -1$ Even more confusing is that a bunch of quantities can be represented by matrices $$e^{i\theta} = \pmatrix{\cos\theta&-\sin\theta\\ \sin\theta&\cos\theta}$$ Naturally I wonder if we can perform this for any number. What is the big picture here? What is this operation called turning a number into a matrix. What is the deeper implication - how does knowing this help? 100 points to anyone who can answer this in a comprehensive way.","Today my friend showed me that the imaginary number can be represented in term of a matrix $$i = \pmatrix{0&-1\\1&0}$$ This was very very confusing for me because I have never thought of it as a matrix. But it is apparent that the properties of imaginary number holds even in this representation, namely $i\cdot i = -1$ Even more confusing is that a bunch of quantities can be represented by matrices $$e^{i\theta} = \pmatrix{\cos\theta&-\sin\theta\\ \sin\theta&\cos\theta}$$ Naturally I wonder if we can perform this for any number. What is the big picture here? What is this operation called turning a number into a matrix. What is the deeper implication - how does knowing this help? 100 points to anyone who can answer this in a comprehensive way.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'complex-numbers', 'rotations']"
54,Prove that the only eigenvalue of a nilpotent operator is 0?,Prove that the only eigenvalue of a nilpotent operator is 0?,,"I need to prove that: if a linear operator $\phi : V \rightarrow V$ on a vector space is nilpotent, then its only eigenvalue is $0$ . I know how to prove that this for a nilpotent matrix, but I'm not sure in the case of an operator. How would I be able to relate $\phi$ to a matrix? Note: A nilpotent operator $\phi$ has been defined as an operator that satisfies $\phi^{n} = 0$ for some $n \geq 1$ .","I need to prove that: if a linear operator on a vector space is nilpotent, then its only eigenvalue is . I know how to prove that this for a nilpotent matrix, but I'm not sure in the case of an operator. How would I be able to relate to a matrix? Note: A nilpotent operator has been defined as an operator that satisfies for some .",\phi : V \rightarrow V 0 \phi \phi \phi^{n} = 0 n \geq 1,"['linear-algebra', 'matrices', 'functions', 'eigenvalues-eigenvectors', 'nilpotence']"
55,Why are Vandermonde matrices invertible?,Why are Vandermonde matrices invertible?,,"A Vandermonde-matrix is a matrix of this form: $$\begin{pmatrix}  x_0^0 & \cdots & x_0^n \\ \vdots & \ddots & \vdots \\  x_n^0 & \cdots & x_n^n \end{pmatrix} \in \mathbb{R}^{(n+1) \times (n+1)}$$ . condition ☀ : $\forall i, j\in \{0, \dots, n\}: i\neq j \Rightarrow x_i \neq x_j$ Why are Vandermonde-matrices with ☀ always invertible? I have tried to find a short argument for that. I know some ways to show that in principle: rank is equal to dimension all lines / rows are linear independence determinant is not zero find inverse According to proofwiki , the determinant is $$\displaystyle V_n = \prod_{1 \le i < j \le n} \left({x_j - x_i}\right)$$ There are two proofs for this determinant, but I've wondered if there is a simpler way to show that such matrices are invertible.","A Vandermonde-matrix is a matrix of this form: . condition ☀ : Why are Vandermonde-matrices with ☀ always invertible? I have tried to find a short argument for that. I know some ways to show that in principle: rank is equal to dimension all lines / rows are linear independence determinant is not zero find inverse According to proofwiki , the determinant is There are two proofs for this determinant, but I've wondered if there is a simpler way to show that such matrices are invertible.","\begin{pmatrix}
 x_0^0 & \cdots & x_0^n \\
\vdots & \ddots & \vdots \\
 x_n^0 & \cdots & x_n^n
\end{pmatrix} \in \mathbb{R}^{(n+1) \times (n+1)} \forall i, j\in \{0, \dots, n\}: i\neq j \Rightarrow x_i \neq x_j \displaystyle V_n = \prod_{1 \le i < j \le n} \left({x_j - x_i}\right)","['linear-algebra', 'matrices', 'polynomials']"
56,Find the eigenvalues of a projection operator,Find the eigenvalues of a projection operator,,A projection operator $P$ is defined as $P^2$=$P$. Use this definition to find the eigenvalues of this operator. In this question is it necessary to define what the projection operator is? And won't the eigenvalue just be zero?,A projection operator $P$ is defined as $P^2$=$P$. Use this definition to find the eigenvalues of this operator. In this question is it necessary to define what the projection operator is? And won't the eigenvalue just be zero?,,"['linear-algebra', 'eigenvalues-eigenvectors']"
57,Why is orthogonal basis important?,Why is orthogonal basis important?,,Lets take the $\mathbb{R}^3$ space as example. Any point in the $\mathbb{R}^3$ space can be represented by 3 linearly independent vectors that need not be orthogonal to each other. What is that special quality of orthogonal basis (extending to orthonormal) that we choose them over non-orthogonal basis?,Lets take the $\mathbb{R}^3$ space as example. Any point in the $\mathbb{R}^3$ space can be represented by 3 linearly independent vectors that need not be orthogonal to each other. What is that special quality of orthogonal basis (extending to orthonormal) that we choose them over non-orthogonal basis?,,"['linear-algebra', 'vector-spaces', 'inner-products', 'orthogonality']"
58,Why a subspace of a vector space is useful,Why a subspace of a vector space is useful,,"I'm in a linear algebra class and am having a hard time wrapping my head around what subspaces of a vector space are useful for (among many other things!). My understanding of a vector space is that, simplistically, it defines a coordinate plane that you can plot points on and figure out some useful things about the relationship between vectors/points. I think what I'm curious about is more application of some of these ideas. Such as, is a subspace useful for a reason other than you don't have to look at the entire space something exists in (I guess one way I've been thinking about it is if you want to make a map of a city, you don't necessarily need to make a map of the state it's in) or am I even wrong about that much? Also, even though I feel like I should know this at this point, is if the subspace is linearly independent, is it still a subspace? If it is, what exactly does that describe and/or why is that still useful? If it's not, is it still useful for something? I think the most difficult part of this for me is I'm having a hard time being able to visualize what exactly we're talking about and I have a hard time thinking that abstractly. I know one or two examples of this might be too specific and doesn't generalize the concept enough, but I think if I have some example to relate back to when applying the idea to new things it might be helpful.","I'm in a linear algebra class and am having a hard time wrapping my head around what subspaces of a vector space are useful for (among many other things!). My understanding of a vector space is that, simplistically, it defines a coordinate plane that you can plot points on and figure out some useful things about the relationship between vectors/points. I think what I'm curious about is more application of some of these ideas. Such as, is a subspace useful for a reason other than you don't have to look at the entire space something exists in (I guess one way I've been thinking about it is if you want to make a map of a city, you don't necessarily need to make a map of the state it's in) or am I even wrong about that much? Also, even though I feel like I should know this at this point, is if the subspace is linearly independent, is it still a subspace? If it is, what exactly does that describe and/or why is that still useful? If it's not, is it still useful for something? I think the most difficult part of this for me is I'm having a hard time being able to visualize what exactly we're talking about and I have a hard time thinking that abstractly. I know one or two examples of this might be too specific and doesn't generalize the concept enough, but I think if I have some example to relate back to when applying the idea to new things it might be helpful.",,"['linear-algebra', 'vector-spaces']"
59,The inverse of a lower triangular matrix is lower triangular,The inverse of a lower triangular matrix is lower triangular,,"The inverse of a non-singular lower triangular matrix is lower triangular. Construct a proof of this fact as follows. Suppose that $L$ is a non-singular lower triangular matrix. If $b \in \mathbb{R^n}$ is such that $b_i = 0$ for $i = 1, . . . , k \leq n$ , and $y$ solves $Ly = b$ , then $y_i = 0$ for $i = 1, . . . , k \leq n$ . Hint : partition $L$ by the first $k$ rows and columns. Can someone tell me what exactly we are showing here and why it will prove that the inverse of any non-singular lower triangular matrix is lower triangular?","The inverse of a non-singular lower triangular matrix is lower triangular. Construct a proof of this fact as follows. Suppose that is a non-singular lower triangular matrix. If is such that for , and solves , then for . Hint : partition by the first rows and columns. Can someone tell me what exactly we are showing here and why it will prove that the inverse of any non-singular lower triangular matrix is lower triangular?","L b \in \mathbb{R^n} b_i = 0 i = 1, . . . , k \leq n y Ly = b y_i = 0 i = 1, . . . , k \leq n L k","['linear-algebra', 'matrices', 'proof-writing', 'inverse']"
60,"Is it true that any matrix can be decomposed into product of rotation, reflection, shear, scaling and projection matrices?","Is it true that any matrix can be decomposed into product of rotation, reflection, shear, scaling and projection matrices?",,"It seems to me that any linear transformation in ${\Bbb R}^{n \times m}$ is just a series of applications of rotation — actually i think any rotation can be achieved by applying two reflections, but not sure — reflection, shear, scaling and projection transformations. One or more of each kind in some order. This is how I have been imagining it to myself, but I was unable to find proof of this on the internet. Is this true? And if this is true, is there a way to find such a decomposition? EDIT: to make it clear, I am asking whether it is true that $\forall A \in {\Bbb R}^{n \times m}  $ , $$ A = \prod_{i=1}^{k} P_i $$ where $P_i$ is rotation, reflection, shear, scaling, or projection matrix in ${\Bbb R}^{n_i\times m_i}$ . Also, $n, m, k \in {\Bbb N}$ , and $n_i, m_i \in {\Bbb N}$ for all $I$ . And, if it is true, then how can we decompose it into that product?","It seems to me that any linear transformation in is just a series of applications of rotation — actually i think any rotation can be achieved by applying two reflections, but not sure — reflection, shear, scaling and projection transformations. One or more of each kind in some order. This is how I have been imagining it to myself, but I was unable to find proof of this on the internet. Is this true? And if this is true, is there a way to find such a decomposition? EDIT: to make it clear, I am asking whether it is true that , where is rotation, reflection, shear, scaling, or projection matrix in . Also, , and for all . And, if it is true, then how can we decompose it into that product?","{\Bbb R}^{n \times m} \forall A \in {\Bbb R}^{n \times m}    A = \prod_{i=1}^{k} P_i  P_i {\Bbb R}^{n_i\times m_i} n, m, k \in {\Bbb N} n_i, m_i \in {\Bbb N} I","['linear-algebra', 'matrices', 'linear-transformations', 'matrix-decomposition']"
61,Characterization of positive definite matrix with principal minors,Characterization of positive definite matrix with principal minors,,"A symmetric matrix $A$ is positive definite if $x^TAx>0$ for all $x\not=0$. However, such matrices can also be characterized by the positivity of the principal minors. A statement and proof can, for example, be found on wikipedia: http://en.wikipedia.org/wiki/Sylvester%27s_criterion However, the proof, as in most books I have seen, is very long and involved. This makes sense in a book where you wanted to prove the other theorems anyway. But there has to be a much better way to prove it. What is the ""proof from the book"" that positive definite matrices are characterized by their $n$ positive principal minors?","A symmetric matrix $A$ is positive definite if $x^TAx>0$ for all $x\not=0$. However, such matrices can also be characterized by the positivity of the principal minors. A statement and proof can, for example, be found on wikipedia: http://en.wikipedia.org/wiki/Sylvester%27s_criterion However, the proof, as in most books I have seen, is very long and involved. This makes sense in a book where you wanted to prove the other theorems anyway. But there has to be a much better way to prove it. What is the ""proof from the book"" that positive definite matrices are characterized by their $n$ positive principal minors?",,"['linear-algebra', 'matrices', 'determinant', 'alternative-proof', 'positive-definite']"
62,Similarity of real matrices over $\mathbb{C}$,Similarity of real matrices over,\mathbb{C},"$A  \underset{\mathbb{C}}{\sim} B \overset{\text{def}}{\iff} A=C^{-1}BC, \space C\in M_{n}(\mathbb{C})$ and similarly for $\underset{\mathbb{R}}{\sim}$. I want to prove that $ A \underset{\mathbb{C}}{\sim} B$ for $A,B \in M_{n}(\mathbb{R})$ therefore $A \underset{\mathbb{R}}{\sim} B$. My idea is that elementary divisors of $A,B$ over $\mathbb{C}$ are the same, and if $(x-z)^k$ is elementary divisor than $(x-\overline{z})^k$ is also elementary divisor $\implies$ $A,B$ have same elementary divisors over $\mathbb{R}$. But i think it's not clear.","$A  \underset{\mathbb{C}}{\sim} B \overset{\text{def}}{\iff} A=C^{-1}BC, \space C\in M_{n}(\mathbb{C})$ and similarly for $\underset{\mathbb{R}}{\sim}$. I want to prove that $ A \underset{\mathbb{C}}{\sim} B$ for $A,B \in M_{n}(\mathbb{R})$ therefore $A \underset{\mathbb{R}}{\sim} B$. My idea is that elementary divisors of $A,B$ over $\mathbb{C}$ are the same, and if $(x-z)^k$ is elementary divisor than $(x-\overline{z})^k$ is also elementary divisor $\implies$ $A,B$ have same elementary divisors over $\mathbb{R}$. But i think it's not clear.",,"['linear-algebra', 'matrices']"
63,Why is positive (semi-)definite only defined for symmetric matrices?,Why is positive (semi-)definite only defined for symmetric matrices?,,"When we are defining positive (semi-)definite matrices, we do so for symmetric matrices only. Why do we need symmetry in the definition?","When we are defining positive (semi-)definite matrices, we do so for symmetric matrices only. Why do we need symmetry in the definition?",,"['linear-algebra', 'matrices']"
64,Geometric interpretation of the cofactor expansion theorem,Geometric interpretation of the cofactor expansion theorem,,"I find the geometric interpretation of determinants to be really intuitive - they are the ""area"" created by the column vectors of the matrix. Could someone give me a geometric interpretation of the cofactor expansion theorem using the definition of the determinant as the ""area""? Thanks!","I find the geometric interpretation of determinants to be really intuitive - they are the ""area"" created by the column vectors of the matrix. Could someone give me a geometric interpretation of the cofactor expansion theorem using the definition of the determinant as the ""area""? Thanks!",,"['linear-algebra', 'matrices', 'vector-spaces', 'determinant']"
65,Determinant of a block upper triangular matrix [duplicate],Determinant of a block upper triangular matrix [duplicate],,This question already has answers here : Determinant of a block lower triangular matrix (7 answers) Closed 10 years ago . How prove the following equality for a block matrix? $$\det\left[\begin{array}[cc]\\A&C\\ 0&B\end{array}\right]=\det(A)\det(B)$$ I tried to use a proof by induction but I'm stuck. Is there a simpler method? Thanks for help.,This question already has answers here : Determinant of a block lower triangular matrix (7 answers) Closed 10 years ago . How prove the following equality for a block matrix? I tried to use a proof by induction but I'm stuck. Is there a simpler method? Thanks for help.,"\det\left[\begin{array}[cc]\\A&C\\
0&B\end{array}\right]=\det(A)\det(B)","['linear-algebra', 'matrices']"
66,The range of $T^*$ is the orthogonal complement of $\ker(T)$,The range of  is the orthogonal complement of,T^* \ker(T),"How can I prove that, if $V$ is a finite-dimensional vector space with inner product and $T$ a linear operator in $V$, then the range of $T^*$ is the orthogonal complement of the null space of $T$? I know what I must do (for a $v$ in the range of $T^*$, I have to show that $v\perp w$ for every $w$ in $\ker(T)$ and then do the opposite), but I don't know how to show that this inner product is zero.","How can I prove that, if $V$ is a finite-dimensional vector space with inner product and $T$ a linear operator in $V$, then the range of $T^*$ is the orthogonal complement of the null space of $T$? I know what I must do (for a $v$ in the range of $T^*$, I have to show that $v\perp w$ for every $w$ in $\ker(T)$ and then do the opposite), but I don't know how to show that this inner product is zero.",,"['linear-algebra', 'linear-transformations']"
67,Inner product on $C(\mathbb R)$,Inner product on,C(\mathbb R),"With axiom of choice it is possible to construct an inner product on $C(\mathbb R)$ . My question is, is it possible to explicitly construct an inner product on $C(\mathbb R)$ ? I.e. to give a closed formula to calculate the inner product? I know it is straight-forward to write down a scalar product using a Hamel basis. This is not the answer I am looking for. This question came to me, when a student asked me in the lecture today 'whether there are vector spaces without inner products'. So I tried to find scalar produces for function spaces. I think I managed to write one down for $L^1((0,1))$ . But I failed to construct one for $C(\mathbb R)$ .","With axiom of choice it is possible to construct an inner product on . My question is, is it possible to explicitly construct an inner product on ? I.e. to give a closed formula to calculate the inner product? I know it is straight-forward to write down a scalar product using a Hamel basis. This is not the answer I am looking for. This question came to me, when a student asked me in the lecture today 'whether there are vector spaces without inner products'. So I tried to find scalar produces for function spaces. I think I managed to write one down for . But I failed to construct one for .","C(\mathbb R) C(\mathbb R) L^1((0,1)) C(\mathbb R)","['linear-algebra', 'functional-analysis', 'inner-products', 'axiom-of-choice']"
68,Why Markov matrices always have 1 as an eigenvalue,Why Markov matrices always have 1 as an eigenvalue,,"Also called stochastic matrix. Let $A=[a_{ij}]$ - matrix over $\mathbb{R}$ $0\le a_{ij} \le 1 \forall i,j$ $\sum_{j}a_{ij}=1 \forall i$ i.e the sum along each column of $A$ is 1. I want to show $A$ has an eigenvalue of 1. The way I've seen this done is that $A^T$ clearly has an eigenvalue of 1, and the eigenvalues of $A^T$ are the same as those of $A$ . This proof, however, uses determinants, matrix transposes, and the characteristic polynomial of a matrix; none of which are particularly intuitive concepts. Does anyone have an intuitive, alternate proof (or sketch of proof)? My goal is to intuitively understand why, if $A$ defines transition probabilities of some Markov-chain, then $A$ has an eigenvalue of 1. I'm studying Google's PageRank algorithm.","Also called stochastic matrix. Let - matrix over i.e the sum along each column of is 1. I want to show has an eigenvalue of 1. The way I've seen this done is that clearly has an eigenvalue of 1, and the eigenvalues of are the same as those of . This proof, however, uses determinants, matrix transposes, and the characteristic polynomial of a matrix; none of which are particularly intuitive concepts. Does anyone have an intuitive, alternate proof (or sketch of proof)? My goal is to intuitively understand why, if defines transition probabilities of some Markov-chain, then has an eigenvalue of 1. I'm studying Google's PageRank algorithm.","A=[a_{ij}] \mathbb{R} 0\le a_{ij} \le 1 \forall i,j \sum_{j}a_{ij}=1 \forall i A A A^T A^T A A A","['linear-algebra', 'intuition', 'markov-chains', 'markov-process', 'alternative-proof']"
69,Prove that $ \det \left( A^{-1} \right) = \frac{1}{\det(A)} $,Prove that, \det \left( A^{-1} \right) = \frac{1}{\det(A)} ,"If I have a non-singular matrix $\bf A$ , how can I prove the following? $$ \det \left( {\bf A}^{-1} \right) = \frac{1}{\det({\bf A})} $$ I know that ${\bf A} {\bf A}^{-1} = {\bf I}$ , but I am not sure what to do with that knowledge.","If I have a non-singular matrix , how can I prove the following? I know that , but I am not sure what to do with that knowledge.",\bf A  \det \left( {\bf A}^{-1} \right) = \frac{1}{\det({\bf A})}  {\bf A} {\bf A}^{-1} = {\bf I},"['linear-algebra', 'matrices', 'determinant', 'inverse']"
70,The product of two positive definite matrices has real and positive eigenvalues? [duplicate],The product of two positive definite matrices has real and positive eigenvalues? [duplicate],,"This question already has answers here : Is the product of symmetric positive semidefinite matrices positive definite? (4 answers) Closed 3 years ago . Given two real positive definite (and therefore, symmetric) matrices $A$ and $B$, are all the eigenvalues of $AB$ real and positive? Wikipedia says $AB$ is positive definite if $A$ and $B$ are positive definite and commute, but I don't need $AB$ to be symmetric. Between the lines of this question the asking user somehow prove that yes, ""the eigenvalues of $AB$  are hence real and strictly positive"" but I couldn't understand if that is confirmed in the answer.","This question already has answers here : Is the product of symmetric positive semidefinite matrices positive definite? (4 answers) Closed 3 years ago . Given two real positive definite (and therefore, symmetric) matrices $A$ and $B$, are all the eigenvalues of $AB$ real and positive? Wikipedia says $AB$ is positive definite if $A$ and $B$ are positive definite and commute, but I don't need $AB$ to be symmetric. Between the lines of this question the asking user somehow prove that yes, ""the eigenvalues of $AB$  are hence real and strictly positive"" but I couldn't understand if that is confirmed in the answer.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
71,"Why is the orthogonal group $\operatorname{O}(2n,\mathbb R)$ not the direct product of $\operatorname{SO}(2n, \mathbb R)$ and $\mathbb Z_2$?",Why is the orthogonal group  not the direct product of  and ?,"\operatorname{O}(2n,\mathbb R) \operatorname{SO}(2n, \mathbb R) \mathbb Z_2","We know that when $n$ is odd, $\operatorname{O}_n(\mathbb R) \simeq \operatorname{SO}_n (\mathbb R) \times \mathbb Z_2$. However, this seems not true when $n$ is even . But I have no idea how to prove something is not a direct product. I have tried to verify some basic properties of direct product. For example, $\operatorname{SO}_n(\mathbb R)$ is a normal subgroup of $\operatorname{O}_n(\mathbb R)$, whenever $n$ is odd or even. But they are not helpful. So, is this statement true and how to prove it? Thank you!","We know that when $n$ is odd, $\operatorname{O}_n(\mathbb R) \simeq \operatorname{SO}_n (\mathbb R) \times \mathbb Z_2$. However, this seems not true when $n$ is even . But I have no idea how to prove something is not a direct product. I have tried to verify some basic properties of direct product. For example, $\operatorname{SO}_n(\mathbb R)$ is a normal subgroup of $\operatorname{O}_n(\mathbb R)$, whenever $n$ is odd or even. But they are not helpful. So, is this statement true and how to prove it? Thank you!",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'orthogonal-matrices']"
72,"Density and dimensionality of zeros in inverse square force fields of randomly distributed sources in (at least) 1, 2 and 3 dimensions?","Density and dimensionality of zeros in inverse square force fields of randomly distributed sources in (at least) 1, 2 and 3 dimensions?",,"Background: In this answer to Are there places in the Universe without gravity? in Astronomy SE I did a quick finite 2D calculation for 20 random sources to see if there was at least one zero, and without rigor convinced myself that there might always be some finite density of zeros. See image below which shows log 10 normalize force magnitude, the script is in the linked answer. Question: For a random distribution of discrete inverse square force sources (e.g. stars in space) of finite density , what is the density and dimensionality of zeros in the net force field relative to the density of sources? Please address the 1, 2 and 3 dimension cases at least. Results of a very quick exploration, looking for zero-dimensional zeros in 2D. The circa -14 values are simply a result of the cutoff in the minimization. The original script can be found in the linked answer. The labeled minima are the result of minimization from the lowest grid point value. I haven't made an attempt yet to look for all zeros .","Background: In this answer to Are there places in the Universe without gravity? in Astronomy SE I did a quick finite 2D calculation for 20 random sources to see if there was at least one zero, and without rigor convinced myself that there might always be some finite density of zeros. See image below which shows log 10 normalize force magnitude, the script is in the linked answer. Question: For a random distribution of discrete inverse square force sources (e.g. stars in space) of finite density , what is the density and dimensionality of zeros in the net force field relative to the density of sources? Please address the 1, 2 and 3 dimension cases at least. Results of a very quick exploration, looking for zero-dimensional zeros in 2D. The circa -14 values are simply a result of the cutoff in the minimization. The original script can be found in the linked answer. The labeled minima are the result of minimization from the lowest grid point value. I haven't made an attempt yet to look for all zeros .",,"['linear-algebra', 'physics', 'mathematical-physics', 'morse-theory']"
73,Prove $\mathbb{Z}$ is not a vector space over a field,Prove  is not a vector space over a field,\mathbb{Z},"This is an exercise from Chapter 3 of Golan's linear algebra book. Problem : Show $\mathbb{Z}$ is not a vector space over a field. Solution attempt :  Suppose there is a such a field and proceed by contradiction. I will write multiplication $FV$, where $F$ is in the field and $V$ is an element of $\mathbb{Z}$. First we rule out the case where the field has characteristic 2. We would have $$0=(1_F+1_F)1=1_F1+1_F1=2$$ a contradiction. Now, consider the case where the field does not have characteristic 2. Then there is an element $2^{-1}_F$ in the field, and $1=2_F(2^{-1}_F1)=2^{-1}_F1+2^{-1}_F1$ Now $2^{-1}_F1\in\mathbb{Z}$ as it is an element of the vector space, but there is no element $a\in\mathbb{Z}$ with $2a=1$, so we have a contradiction. Is this correct?","This is an exercise from Chapter 3 of Golan's linear algebra book. Problem : Show $\mathbb{Z}$ is not a vector space over a field. Solution attempt :  Suppose there is a such a field and proceed by contradiction. I will write multiplication $FV$, where $F$ is in the field and $V$ is an element of $\mathbb{Z}$. First we rule out the case where the field has characteristic 2. We would have $$0=(1_F+1_F)1=1_F1+1_F1=2$$ a contradiction. Now, consider the case where the field does not have characteristic 2. Then there is an element $2^{-1}_F$ in the field, and $1=2_F(2^{-1}_F1)=2^{-1}_F1+2^{-1}_F1$ Now $2^{-1}_F1\in\mathbb{Z}$ as it is an element of the vector space, but there is no element $a\in\mathbb{Z}$ with $2a=1$, so we have a contradiction. Is this correct?",,"['linear-algebra', 'vector-spaces']"
74,$\det\left(6(A^3+B^3+C^3)+I_{2}\right)\ge 5^2\det(A^2+B^2+C^2)$ for $2 \times 2$ matrices,for  matrices,\det\left(6(A^3+B^3+C^3)+I_{2}\right)\ge 5^2\det(A^2+B^2+C^2) 2 \times 2,"Motivated by this question I propose a simplification: Question Let matrices $A,B,C\in M_{2}(\mathbb{C})$ be Hermitian and positive definite, such that:$$A+B+C=I_2$$   Show that:   $$\det\left(6(A^3+B^3+C^3)+I_{2}\right)\ge 5^2\det(A^2+B^2+C^2)$$   where $I_{2}$ is the identity matrix. The original question is of unknown origin and I am hoping for a substantial simplification in the $2 \times 2$ case where the following expansion holds: $$\left[ \begin{array}{cc} x_1 & y + iz \\ y - iz & x_2\end{array}\right]  = \left[ \begin{array}{cc} x_1 & 0 \\ 0 & x_2\end{array}\right] + y\left[ \begin{array}{cc} 0 & 1  \\ 1  & 0\end{array}\right] + z\left[ \begin{array}{rc} 0 &  i \\  - i & 0\end{array}\right]$$ maybe with expansion to Pauli spin matrices this is solvable. There is a nice article by Knutson and Tao on Honeycombs that might be of assistnce: http://www.ams.org/journals/jams/1999-12-04/S0894-0347-99-00299-4/","Motivated by this question I propose a simplification: Question Let matrices $A,B,C\in M_{2}(\mathbb{C})$ be Hermitian and positive definite, such that:$$A+B+C=I_2$$   Show that:   $$\det\left(6(A^3+B^3+C^3)+I_{2}\right)\ge 5^2\det(A^2+B^2+C^2)$$   where $I_{2}$ is the identity matrix. The original question is of unknown origin and I am hoping for a substantial simplification in the $2 \times 2$ case where the following expansion holds: $$\left[ \begin{array}{cc} x_1 & y + iz \\ y - iz & x_2\end{array}\right]  = \left[ \begin{array}{cc} x_1 & 0 \\ 0 & x_2\end{array}\right] + y\left[ \begin{array}{cc} 0 & 1  \\ 1  & 0\end{array}\right] + z\left[ \begin{array}{rc} 0 &  i \\  - i & 0\end{array}\right]$$ maybe with expansion to Pauli spin matrices this is solvable. There is a nice article by Knutson and Tao on Honeycombs that might be of assistnce: http://www.ams.org/journals/jams/1999-12-04/S0894-0347-99-00299-4/",,"['linear-algebra', 'matrices', 'inequality', 'contest-math', 'determinant']"
75,Find the cardinality of a subset of $GL_n( \mathbb F_p)$,Find the cardinality of a subset of,GL_n( \mathbb F_p),"Let $m,n \in \mathbb N$ . Let $\mathbb F_p$ denote the prime field of characteristic $p$ . Consider the set $$ X_m = \{A \in GL_n( {\mathbb F_p}): A^m=1 \}$$ Compute the cardinality of $X_m$ . Its clear that $\vert X_m \vert < \infty$ since cardinality of $GL_n( \mathbb F_p)$ itself is $(p^n-1)(p^n-p)...(p^n-p^{n-1})$ . Moreover, suppose $A \in X_m$ then $(x^m-1)$ kills $A$ . First I tried to understand the case when $m=p$ . In this case if $A \in X_p$ then $(x^p-1)$ kills $A$ and since $x^p-1=(x-1)^p$ hence $(x-1)^n$ also kills $A$ . Also, minimal polynomial of $A$ is of the form $(x-1)^k$ for some $k \leq p$ . Any ideas to proceed further?","Let . Let denote the prime field of characteristic . Consider the set Compute the cardinality of . Its clear that since cardinality of itself is . Moreover, suppose then kills . First I tried to understand the case when . In this case if then kills and since hence also kills . Also, minimal polynomial of is of the form for some . Any ideas to proceed further?","m,n \in \mathbb N \mathbb F_p p  X_m = \{A \in GL_n( {\mathbb F_p}): A^m=1 \} X_m \vert X_m \vert < \infty GL_n( \mathbb F_p) (p^n-1)(p^n-p)...(p^n-p^{n-1}) A \in X_m (x^m-1) A m=p A \in X_p (x^p-1) A x^p-1=(x-1)^p (x-1)^n A A (x-1)^k k \leq p","['linear-algebra', 'combinatorics', 'matrices', 'finite-groups', 'representation-theory']"
76,Lowest dimensional faithful representation of a finite group,Lowest dimensional faithful representation of a finite group,,"How does one compute the lowest dimensional faithful representation of a finite group? This question originated in the context of given a finite group $G$ : trying to find the lowest dimensional shape whose rotational/reflection symmetries form $G$ . (Formally stated as finding the lowest dimensional faithful representation of $G$ into the orthogonal group $O(n)$ .) Now just listing out examples is hard, because even in just $\mathbb{R}^2$ outside of the symmetries of simple polygons we see things like $\mathbb{Z}_2 \times \mathbb{Z}_2 $ crop up, yet it appears that $\mathbb{Z}_2 \times \mathbb{Z}_2 \times \mathbb{Z}_2$ doesn't have any obvious two dimensional representations. Grouppropswiki doesn't even have a single representation for it at all. So I thought it was wise before I tackle the orthogonal group question I should know how to just, in general, find a low dimensional faithful representation. Some trivialities are that the dimension for a group $G$ will be less than or equal to the smallest $j$ such that $G \subset S_j$ since each symmetric group can be realized as the symmetries of a $j$ -dimensional simplex space due to Burnside. But this doesn't say much because even something like $\mathbb{D}_{\text{Graham's Number}}$ can be realized in $\mathbb{R}^2$ (and we still are dealing with isometries here, what about represenatations that transcend that!?)","How does one compute the lowest dimensional faithful representation of a finite group? This question originated in the context of given a finite group : trying to find the lowest dimensional shape whose rotational/reflection symmetries form . (Formally stated as finding the lowest dimensional faithful representation of into the orthogonal group .) Now just listing out examples is hard, because even in just outside of the symmetries of simple polygons we see things like crop up, yet it appears that doesn't have any obvious two dimensional representations. Grouppropswiki doesn't even have a single representation for it at all. So I thought it was wise before I tackle the orthogonal group question I should know how to just, in general, find a low dimensional faithful representation. Some trivialities are that the dimension for a group will be less than or equal to the smallest such that since each symmetric group can be realized as the symmetries of a -dimensional simplex space due to Burnside. But this doesn't say much because even something like can be realized in (and we still are dealing with isometries here, what about represenatations that transcend that!?)",G G G O(n) \mathbb{R}^2 \mathbb{Z}_2 \times \mathbb{Z}_2  \mathbb{Z}_2 \times \mathbb{Z}_2 \times \mathbb{Z}_2 G j G \subset S_j j \mathbb{D}_{\text{Graham's Number}} \mathbb{R}^2,"['linear-algebra', 'group-theory', 'finite-groups', 'representation-theory', 'linear-transformations']"
77,"If a matrix commutes with a set of other matrices, what conclusions can be drawn?","If a matrix commutes with a set of other matrices, what conclusions can be drawn?",,"I have a very specific example from a book on quantum mechanics by Schwabl, in which he states that an object which commutes with all four gamma matrices, $$         \begin{pmatrix}         1 & 0 & 0 & 0\\         0 & 1 & 0 & 0\\         0 & 0 & -1 & 0\\         0 & 0 & 0 & -1\\         \end{pmatrix}         \begin{pmatrix}         0 & 0 & 0 & 1\\         0 & 0 & 1 & 0\\         0 & -1 & 0 & 0\\         -1 & 0 & 0 & 0\\         \end{pmatrix}         \begin{pmatrix}         0 & 0 & 0 & -i\\         0 & 0 & i & 0\\         0 & i & 0 & 0\\         -i & 0 & 0 & 0\\         \end{pmatrix}         \begin{pmatrix}         0 & 0 & 1 & 0\\         0 & 0 & 0 & -1\\         -1 & 0 & 0 & 0\\         0 & 1 & 0 & 0\\         \end{pmatrix}, $$ must be a multiple times the unit matrix. These matrices don't seem to span all $4 \times 4$ matrices so why would this be the case? I have asked around but no one seems to know the answer.","I have a very specific example from a book on quantum mechanics by Schwabl, in which he states that an object which commutes with all four gamma matrices, $$         \begin{pmatrix}         1 & 0 & 0 & 0\\         0 & 1 & 0 & 0\\         0 & 0 & -1 & 0\\         0 & 0 & 0 & -1\\         \end{pmatrix}         \begin{pmatrix}         0 & 0 & 0 & 1\\         0 & 0 & 1 & 0\\         0 & -1 & 0 & 0\\         -1 & 0 & 0 & 0\\         \end{pmatrix}         \begin{pmatrix}         0 & 0 & 0 & -i\\         0 & 0 & i & 0\\         0 & i & 0 & 0\\         -i & 0 & 0 & 0\\         \end{pmatrix}         \begin{pmatrix}         0 & 0 & 1 & 0\\         0 & 0 & 0 & -1\\         -1 & 0 & 0 & 0\\         0 & 1 & 0 & 0\\         \end{pmatrix}, $$ must be a multiple times the unit matrix. These matrices don't seem to span all $4 \times 4$ matrices so why would this be the case? I have asked around but no one seems to know the answer.",,"['linear-algebra', 'matrices']"
78,Are $10\times 10$ matrices spanned by powers of a single matrix?,Are  matrices spanned by powers of a single matrix?,10\times 10,"I don't know how to answer this question: Is there a $10 \times 10$ matrix $A$ such that $$M_{10}(\mathbb{F})=\text{span}\{I,A,A^2,\ldots, A^{100}\}\textrm{,}$$ where $M_{10}(\mathbb{F})$ is the vector space of $10 \times 10$ matrices over $\mathbb{F}$? I think there is a connection to Jordan normal form.","I don't know how to answer this question: Is there a $10 \times 10$ matrix $A$ such that $$M_{10}(\mathbb{F})=\text{span}\{I,A,A^2,\ldots, A^{100}\}\textrm{,}$$ where $M_{10}(\mathbb{F})$ is the vector space of $10 \times 10$ matrices over $\mathbb{F}$? I think there is a connection to Jordan normal form.",,"['linear-algebra', 'matrices']"
79,why is the definition of the determinant so weird? [duplicate],why is the definition of the determinant so weird? [duplicate],,"This question already has answers here : What's an intuitive way to think about the determinant? (18 answers) Closed 7 years ago . I learned linear algebra from books of Friedberg, Gilbert & Strang, Anton etc. by myself. I dare say, that I learned all that stuff eagerly. Studying by myself, I could not intuitively understand the definition of the determinant (its even – odd manner). I could only memorize the definition, and then use it (or try to use it) to solve some related readers' homework problems or other exercises. As you know, the purpose of a determinant is literally to determine whether a given system of equations has a unique solution or not. In other words, the ""determinant"" will determine whether the row vectors (and equivalently, column vectors) of a given square matrix are independent or not. If those are mutually independent, then they can geometrically represent an $n$-dimensional quantity (for example, area in 2 dimensions or volume in 3D). If not, some of them are dependent, so they cannot form the $n$-dimensional quantity, and correspondingly the determinant is zero. A multiple of any row can be added to another, this kind of row elementary operation does not change the determinant value. The picture below illustrates an intuitive understanding of that, too. Writing the sides of the parallelogram as rows or columns of a square matrix, this transformation transforms it to another with the same value of the determinant. It can be transformed to Gauss–Jordan form, in this case, each of the row / column vectors are orthogonal because their inner products are all zero. (I tried this with the Gram–Schmidt process; however, intuitively, the result is surely the same.) Those vectors are orthogonal so it is very clear that just multiplication of the diagonal terms should give directly the aforementioned $n$-dimensional quantity, so that's the determinant in such case. I understand the determinant in this manner, and it makes sense intuitively. However the textbook definition mentioned above (defined in the ""even–odd"" manner) looks very weird to me. What is the motivation of that definition? And can it be generally derived from my intuition about the $n$-dimensional quantities? I succeeded in doing so for the $2\times2$ and $3\times3$ cases, but I cannot see any generalized relation. It seems to me that the definition of the determinant comes down magically, without enough logic. I was wondering if you could help me. Thank you in advance.","This question already has answers here : What's an intuitive way to think about the determinant? (18 answers) Closed 7 years ago . I learned linear algebra from books of Friedberg, Gilbert & Strang, Anton etc. by myself. I dare say, that I learned all that stuff eagerly. Studying by myself, I could not intuitively understand the definition of the determinant (its even – odd manner). I could only memorize the definition, and then use it (or try to use it) to solve some related readers' homework problems or other exercises. As you know, the purpose of a determinant is literally to determine whether a given system of equations has a unique solution or not. In other words, the ""determinant"" will determine whether the row vectors (and equivalently, column vectors) of a given square matrix are independent or not. If those are mutually independent, then they can geometrically represent an $n$-dimensional quantity (for example, area in 2 dimensions or volume in 3D). If not, some of them are dependent, so they cannot form the $n$-dimensional quantity, and correspondingly the determinant is zero. A multiple of any row can be added to another, this kind of row elementary operation does not change the determinant value. The picture below illustrates an intuitive understanding of that, too. Writing the sides of the parallelogram as rows or columns of a square matrix, this transformation transforms it to another with the same value of the determinant. It can be transformed to Gauss–Jordan form, in this case, each of the row / column vectors are orthogonal because their inner products are all zero. (I tried this with the Gram–Schmidt process; however, intuitively, the result is surely the same.) Those vectors are orthogonal so it is very clear that just multiplication of the diagonal terms should give directly the aforementioned $n$-dimensional quantity, so that's the determinant in such case. I understand the determinant in this manner, and it makes sense intuitively. However the textbook definition mentioned above (defined in the ""even–odd"" manner) looks very weird to me. What is the motivation of that definition? And can it be generally derived from my intuition about the $n$-dimensional quantities? I succeeded in doing so for the $2\times2$ and $3\times3$ cases, but I cannot see any generalized relation. It seems to me that the definition of the determinant comes down magically, without enough logic. I was wondering if you could help me. Thank you in advance.",,"['linear-algebra', 'determinant']"
80,Derivative of the nuclear norm,Derivative of the nuclear norm,,The nuclear norm is defined in the following way $$\|X\|_*=\mathrm{tr} \left(\sqrt{X^T X} \right)$$ I'm trying to take the derivative of the nuclear norm with respect to its argument $$\frac{\partial \|X\|_*}{\partial X}$$ Note that $\|X\|_*$ is a norm and is convex. I'm using this for some coordinate descent optimization algorithm. Thank you for your help.,The nuclear norm is defined in the following way $$\|X\|_*=\mathrm{tr} \left(\sqrt{X^T X} \right)$$ I'm trying to take the derivative of the nuclear norm with respect to its argument $$\frac{\partial \|X\|_*}{\partial X}$$ Note that $\|X\|_*$ is a norm and is convex. I'm using this for some coordinate descent optimization algorithm. Thank you for your help.,,"['linear-algebra', 'derivatives', 'matrix-calculus', 'nuclear-norm', 'proximal-operators']"
81,Derivation of the formula for Ordinary Least Squares Linear Regression,Derivation of the formula for Ordinary Least Squares Linear Regression,,"How was the formula for Ordinary Least Squares Linear Regression arrived at? Note I am not only looking for the proof, but also the derivation. Where did the formula come from?","How was the formula for Ordinary Least Squares Linear Regression arrived at? Note I am not only looking for the proof, but also the derivation. Where did the formula come from?",,['linear-algebra']
82,What does QR decomposition have to do with least squares method?,What does QR decomposition have to do with least squares method?,,"I know that QR decomposition is a mean to solve a system $Ax=b$ by doing $A = QR$ and then solving $Qy = b$ and then $Rx=y$ . I know that the least squares method is used to find $\min ||Ax-b||$ , that is, it can find the $x$ that is closest to solve $Ax=b$ or that solves it exactly. I often see QR decomposition in context of least squares but I can't see what they have in common.","I know that QR decomposition is a mean to solve a system by doing and then solving and then . I know that the least squares method is used to find , that is, it can find the that is closest to solve or that solves it exactly. I often see QR decomposition in context of least squares but I can't see what they have in common.",Ax=b A = QR Qy = b Rx=y \min ||Ax-b|| x Ax=b,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
83,How to calculate the determinant of a $4 \times 4$ matrix with multiple variables?,How to calculate the determinant of a  matrix with multiple variables?,4 \times 4,What is the determinant: $$ \begin{vmatrix}1& a & a^2 & a^4 \\ 1 & b & b^2 & b^4 \\ 1 & c & c^2 & c^4 \\1 & d & d^2 &d^4 \end{vmatrix} $$ Someone gave me the following hint Replace $d$ by a variable $x$; make use of the fact that the sum of the roots of a fourth-degree polynomial is equal to the coefficient of $x^3$ but I didn't get that.,What is the determinant: $$ \begin{vmatrix}1& a & a^2 & a^4 \\ 1 & b & b^2 & b^4 \\ 1 & c & c^2 & c^4 \\1 & d & d^2 &d^4 \end{vmatrix} $$ Someone gave me the following hint Replace $d$ by a variable $x$; make use of the fact that the sum of the roots of a fourth-degree polynomial is equal to the coefficient of $x^3$ but I didn't get that.,,['linear-algebra']
84,How to prove the distributive property of cross product,How to prove the distributive property of cross product,,"That is, how to prove the following identity: $$a \times (b+c) = a \times b + a \times c$$ where the $\times$ represents cross product of two vectors in 3-dimensional Euclidean space.","That is, how to prove the following identity: $$a \times (b+c) = a \times b + a \times c$$ where the $\times$ represents cross product of two vectors in 3-dimensional Euclidean space.",,['linear-algebra']
85,What can be said about a matrix which is both symmetric and orthogonal?,What can be said about a matrix which is both symmetric and orthogonal?,,"I tried to find matrices $A$ , which are both orthogonal and symmetric, this means $A = A^{-1} = A^T$ . I only found very special examples like $I$ , $-I$ or the matrix $$\begin{pmatrix}   0  &0& -1\\    0& -1&  0\\   -1&  0&  0 \end{pmatrix} $$ Can a matrix with the desired properties only contain the values $-1$ , $0$ and $1$ ?  Which matrices of a given size have the desired property?","I tried to find matrices , which are both orthogonal and symmetric, this means . I only found very special examples like , or the matrix Can a matrix with the desired properties only contain the values , and ?  Which matrices of a given size have the desired property?","A A = A^{-1} = A^T I -I \begin{pmatrix}
  0  &0& -1\\ 
  0& -1&  0\\
  -1&  0&  0
\end{pmatrix}  -1 0 1","['linear-algebra', 'matrices', 'symmetric-matrices', 'orthogonal-matrices']"
86,Is a function that preserves the cross product necessarily linear in $\mathbb R^3$? $f(a) \times f(b) = a \times b$ [closed],Is a function that preserves the cross product necessarily linear in ?  [closed],\mathbb R^3 f(a) \times f(b) = a \times b,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Assume that $f: \mathbb{R^3} \rightarrow \mathbb{R^3}$ is a function such that $$ f(a) \times f(b)=a \times b $$ for all $a,b \in \mathbb{R^3}$ , where '' $\times$ '' denotes the cross product in $\mathbb{R^3}$ . Does $f$ have to be a linear mapping?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Assume that is a function such that for all , where '' '' denotes the cross product in . Does have to be a linear mapping?","f: \mathbb{R^3} \rightarrow \mathbb{R^3} 
f(a) \times f(b)=a \times b
 a,b \in \mathbb{R^3} \times \mathbb{R^3} f","['linear-algebra', 'functional-equations', 'cross-product']"
87,Does an injective endomorphism of a finitely-generated free R-module have nonzero determinant?,Does an injective endomorphism of a finitely-generated free R-module have nonzero determinant?,,"Alternately, let $M$ be an $n \times n$ matrix with entries in a commutative ring $R$ . If $M$ has trivial kernel, is it true that $\det(M) \neq 0$ ? This math.SE question deals with the case that $R$ is a polynomial ring over a field. There it was observed that there is a straightforward proof when $R$ is an integral domain by passing to the fraction field. In the general case I have neither a proof nor a counterexample. Here are three general observations about properties that a counterexample $M$ (trivial kernel but zero determinant) must satisfy. First, recall that the adjugate $\text{adj}(M)$ of a matrix $M$ is a matrix whose entries are integer polynomials in those of $M$ and which satisfies $$M \text{adj}(M) = \det(M).$$ If $\det(M) = 0$ and $\text{adj}(M) \neq 0$ , then some column of $\text{adj}(M)$ lies in the kernel of $M$ . Thus: If $M$ is a counterexample, then $\text{adj}(M) = 0$ . When $n = 2$ , we have $\text{adj}(M) = 0 \Rightarrow M = 0$ , so this settles the $2 \times 2$ case. Second observation: recall that by Cayley-Hamilton $p(M) = 0$ where $p$ is the characteristic polynomial of $M$ . Write this as $$M^k q(M) = 0$$ where $q$ has nonzero constant term. If $q(M) \neq 0$ , then there exists some $v \in R^n$ such that $w = q(M) v \neq 0$ , hence $M^k w = 0$ and one of the vectors $w, Mw, M^2 w,\dots, M^{k-1} w$ necessarily lies in the kernel of $M$ . Thus if $M$ is a counterexample we must have $q(M) = 0$ where $q$ has nonzero constant term. Now for every prime ideal $P$ of $R$ , consider the induced action of $M$ on $F^n$ , where $F = \overline{ \text{Frac}(R/P) }$ . Then $q(\lambda) = 0$ for every eigenvalue $\lambda$ of $M$ . Since $\det(M) = 0$ , one of these eigenvalues over $F$ is $0$ , hence it follows that $q(0) \in P$ . Since this is true for all prime ideals, $q(0)$ lies in the intersection of all the prime ideals of $R$ , hence If $M$ is a counterexample and $q$ is defined as above, then $q(0)$ is nilpotent. This settles the question for reduced rings. Now, $\text{det}(M) = 0$ implies that the constant term of $p$ is equal to zero, and $\text{adj}(M) = 0$ implies that the linear term of $p$ is equal to zero. It follows that if $M$ is a counterexample, then $M^2 \mid p(M)$ . When $n = 3$ , this implies that $$q(M) = M - \lambda$$ where $\lambda$ is nilpotent, so $M$ is nilpotent and thus must have nontrivial kernel. So this settles the $3 \times 3$ case. Third observation: if $M$ is a counterexample, then it is a counterexample over the subring of $R$ generated by the entries of $M$ , so We may assume WLOG that $R$ is finitely-generated over $\mathbb{Z}$ .","Alternately, let be an matrix with entries in a commutative ring . If has trivial kernel, is it true that ? This math.SE question deals with the case that is a polynomial ring over a field. There it was observed that there is a straightforward proof when is an integral domain by passing to the fraction field. In the general case I have neither a proof nor a counterexample. Here are three general observations about properties that a counterexample (trivial kernel but zero determinant) must satisfy. First, recall that the adjugate of a matrix is a matrix whose entries are integer polynomials in those of and which satisfies If and , then some column of lies in the kernel of . Thus: If is a counterexample, then . When , we have , so this settles the case. Second observation: recall that by Cayley-Hamilton where is the characteristic polynomial of . Write this as where has nonzero constant term. If , then there exists some such that , hence and one of the vectors necessarily lies in the kernel of . Thus if is a counterexample we must have where has nonzero constant term. Now for every prime ideal of , consider the induced action of on , where . Then for every eigenvalue of . Since , one of these eigenvalues over is , hence it follows that . Since this is true for all prime ideals, lies in the intersection of all the prime ideals of , hence If is a counterexample and is defined as above, then is nilpotent. This settles the question for reduced rings. Now, implies that the constant term of is equal to zero, and implies that the linear term of is equal to zero. It follows that if is a counterexample, then . When , this implies that where is nilpotent, so is nilpotent and thus must have nontrivial kernel. So this settles the case. Third observation: if is a counterexample, then it is a counterexample over the subring of generated by the entries of , so We may assume WLOG that is finitely-generated over .","M n \times n R M \det(M) \neq 0 R R M \text{adj}(M) M M M \text{adj}(M) = \det(M). \det(M) = 0 \text{adj}(M) \neq 0 \text{adj}(M) M M \text{adj}(M) = 0 n = 2 \text{adj}(M) = 0 \Rightarrow M = 0 2 \times 2 p(M) = 0 p M M^k q(M) = 0 q q(M) \neq 0 v \in R^n w = q(M) v \neq 0 M^k w = 0 w, Mw, M^2 w,\dots, M^{k-1} w M M q(M) = 0 q P R M F^n F = \overline{ \text{Frac}(R/P) } q(\lambda) = 0 \lambda M \det(M) = 0 F 0 q(0) \in P q(0) R M q q(0) \text{det}(M) = 0 p \text{adj}(M) = 0 p M M^2 \mid p(M) n = 3 q(M) = M - \lambda \lambda M 3 \times 3 M R M R \mathbb{Z}","['linear-algebra', 'commutative-algebra', 'modules']"
88,Why are complex finite-dimensional irreducible representations of abelian groups one-dimensional?,Why are complex finite-dimensional irreducible representations of abelian groups one-dimensional?,,I'm supposed to show that each Complex finite-dimensional irreducible representation of an abelian group is one dimensional. For any map $\phi: V \rightarrow V$ it holds that $\phi(\rho(g)v) = \rho(g) \phi(v)$ . Also since the group $\rho(h) \rho(g) v = \rho(g) \rho(h) v$ . From a previous exercise I know that $\phi = \lambda \cdot id_V$ for some $\lambda \in \mathbb{C}$ . This transforms the previous equation into $\lambda \cdot id_V \cdot (\rho(g)v) = \rho(g) \lambda \cdot id_V \cdot v$ which implies that $\lambda \cdot id_V \cdot (\rho(g)v) =\lambda \cdot \rho(g) \cdot v$ . Now I'm not quite sure how to bring into play that $G$ is abelian. Could someone give me a hint? Cheers!,I'm supposed to show that each Complex finite-dimensional irreducible representation of an abelian group is one dimensional. For any map it holds that . Also since the group . From a previous exercise I know that for some . This transforms the previous equation into which implies that . Now I'm not quite sure how to bring into play that is abelian. Could someone give me a hint? Cheers!,\phi: V \rightarrow V \phi(\rho(g)v) = \rho(g) \phi(v) \rho(h) \rho(g) v = \rho(g) \rho(h) v \phi = \lambda \cdot id_V \lambda \in \mathbb{C} \lambda \cdot id_V \cdot (\rho(g)v) = \rho(g) \lambda \cdot id_V \cdot v \lambda \cdot id_V \cdot (\rho(g)v) =\lambda \cdot \rho(g) \cdot v G,"['linear-algebra', 'group-theory', 'representation-theory', 'abelian-groups']"
89,Motivation for linear transformations,Motivation for linear transformations,,"Sometimes I hate classes for mathematicians. It is not their precision and formality in building the concepts,  but they never give a motivation. So in the course my professor started right with the definition of vector space, assuming I suppose that everybody knows what he is talking about. Well, reading some books for beginners like me I've realized that vector spaces are actually a generalization of working with the properties of Euclidean spaces. Now the topic is about linear transformations. I understand the definition, they are special cases of mappings. Well, the thing is that I don't know why the definition has to be so, what is the real motivation for such a definition?. what is behind the meaning of 'linear'? My first impression is that maybe it has something to do with just preserving the operations of vectors, though I don't know why. What makes linear transformations to be special compared with those that are not linear? Sorry for this question, maybe it is too naive but it's really important to me.","Sometimes I hate classes for mathematicians. It is not their precision and formality in building the concepts,  but they never give a motivation. So in the course my professor started right with the definition of vector space, assuming I suppose that everybody knows what he is talking about. Well, reading some books for beginners like me I've realized that vector spaces are actually a generalization of working with the properties of Euclidean spaces. Now the topic is about linear transformations. I understand the definition, they are special cases of mappings. Well, the thing is that I don't know why the definition has to be so, what is the real motivation for such a definition?. what is behind the meaning of 'linear'? My first impression is that maybe it has something to do with just preserving the operations of vectors, though I don't know why. What makes linear transformations to be special compared with those that are not linear? Sorry for this question, maybe it is too naive but it's really important to me.",,"['linear-algebra', 'vector-spaces']"
90,"Matrix raised to a matrix: $M^N$, is this possible? with $M,N\in M_n(\Bbb K).$","Matrix raised to a matrix: , is this possible? with","M^N M,N\in M_n(\Bbb K).","I was wondering if there is such a valid operation as raising a matrix to the power of a matrix, e.g. vaguely, if $M$ is a matrix, is $$ M^N $$ valid, or is there at least something similar?  Would it be the components of the matrix raised to each component of the matrix it's raised to, resulting in again, another matrix? Thanks,","I was wondering if there is such a valid operation as raising a matrix to the power of a matrix, e.g. vaguely, if $M$ is a matrix, is $$ M^N $$ valid, or is there at least something similar?  Would it be the components of the matrix raised to each component of the matrix it's raised to, resulting in again, another matrix? Thanks,",,"['linear-algebra', 'matrices', 'complex-numbers', 'exponentiation']"
91,"If $\,A^k=0$ and $AB=BA$, then $\,\det(A+B)=\det B$","If  and , then","\,A^k=0 AB=BA \,\det(A+B)=\det B","Assume that the matrices $A,\: B\in \mathbb{R}^{n\times n}$ satisfy  $$ A^k=0,\,\, \text{for some $\,k\in \mathbb{Z^+}$}\quad\text{and}\quad AB=BA. $$ Prove that $$\det(A+B)=\det B.$$","Assume that the matrices $A,\: B\in \mathbb{R}^{n\times n}$ satisfy  $$ A^k=0,\,\, \text{for some $\,k\in \mathbb{Z^+}$}\quad\text{and}\quad AB=BA. $$ Prove that $$\det(A+B)=\det B.$$",,"['linear-algebra', 'matrices', 'determinant', 'matrix-equations', 'matrix-calculus']"
92,Matrix Inverses and Eigenvalues,Matrix Inverses and Eigenvalues,,"I was working on this problem here below, but seem to not know a precise or clean way to show the proof to this question below. I had about a few ways of doing it, but the statements/operations were pretty loosely used. The problem is as follows: Show that ${\bf A}^{-1}$ exists if and only if the eigenvalues $ \lambda _i$ ,  $1 \leq i \leq n$ of $\bf{A}$ are all non-zero, and then ${\bf A}^{-1}$ has the eigenvalues given by $ \frac{1}{\lambda _i}$, $1 \leq i \leq n$. Thanks.","I was working on this problem here below, but seem to not know a precise or clean way to show the proof to this question below. I had about a few ways of doing it, but the statements/operations were pretty loosely used. The problem is as follows: Show that ${\bf A}^{-1}$ exists if and only if the eigenvalues $ \lambda _i$ ,  $1 \leq i \leq n$ of $\bf{A}$ are all non-zero, and then ${\bf A}^{-1}$ has the eigenvalues given by $ \frac{1}{\lambda _i}$, $1 \leq i \leq n$. Thanks.",,"['linear-algebra', 'matrices']"
93,What is the purpose of Jordan Canonical Form?,What is the purpose of Jordan Canonical Form?,,"I don't claim at all to be an expert on this topic. In many (advanced) linear algebra textbooks for undergraduates, I usually find something about the ""Jordan Canonical Form"" of a matrix. What is the purpose of such a form? I have taken a usual first-course in linear algebra (did another semester with Axler, but I don't claim to be an expert) and have taken abstract algebra (most familiar with group and ring theory) and have briefly skimmed through linear algebra books covering this material, but I don't quite understand the ""big picture"" idea, i.e., why is this useful in application? One person once told me it is the ""most straightforward and useful algorithm for solving systems of linear equations, once you get beyond 3 variables or so,"" but maybe I'm missing something, since I usually don't see anything like what this person described to me in the linear algebra books I have. Most textbooks I've seen tend to have a more theoretical focus on this topic. Also, any suggested texts which have good coverage on this topic would be very helpful.","I don't claim at all to be an expert on this topic. In many (advanced) linear algebra textbooks for undergraduates, I usually find something about the ""Jordan Canonical Form"" of a matrix. What is the purpose of such a form? I have taken a usual first-course in linear algebra (did another semester with Axler, but I don't claim to be an expert) and have taken abstract algebra (most familiar with group and ring theory) and have briefly skimmed through linear algebra books covering this material, but I don't quite understand the ""big picture"" idea, i.e., why is this useful in application? One person once told me it is the ""most straightforward and useful algorithm for solving systems of linear equations, once you get beyond 3 variables or so,"" but maybe I'm missing something, since I usually don't see anything like what this person described to me in the linear algebra books I have. Most textbooks I've seen tend to have a more theoretical focus on this topic. Also, any suggested texts which have good coverage on this topic would be very helpful.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
94,Difference between $\mathbb{R}$-linear and $\mathbb{C}$-linear maps,Difference between -linear and -linear maps,\mathbb{R} \mathbb{C},Suppose we have a $f\colon \mathbb{C} \to \mathbb{C} $. I am confused when people say $f$ is $\mathbb{R}$-linear map or $\mathbb{C}$-linear map. What is the difference between these two ?,Suppose we have a $f\colon \mathbb{C} \to \mathbb{C} $. I am confused when people say $f$ is $\mathbb{R}$-linear map or $\mathbb{C}$-linear map. What is the difference between these two ?,,['linear-algebra']
95,What does a dot in a circle mean?,What does a dot in a circle mean?,,"I'm looking at some formulas involving matrices (in the context of machine learning, but I'm not sure it's relevant) and I came across $\odot$. What could this mean? The context is $M \odot N$, where $M$ is a matrix and $N$ might be a vector, or a matrix, or a scalar, it's a bit dense so it's hard to tell. I have reason to believe it may be the Hadamard product , is there anything else it could mean?","I'm looking at some formulas involving matrices (in the context of machine learning, but I'm not sure it's relevant) and I came across $\odot$. What could this mean? The context is $M \odot N$, where $M$ is a matrix and $N$ might be a vector, or a matrix, or a scalar, it's a bit dense so it's hard to tell. I have reason to believe it may be the Hadamard product , is there anything else it could mean?",,"['linear-algebra', 'notation']"
96,"If the product of two non-zero square matrices is zero, then both factors must be singular.","If the product of two non-zero square matrices is zero, then both factors must be singular.",,"In the textbook Contemporary Linear Algebra by Anton and Busby, there was a small question in section 3.2 page 101 concerning this. It asks if $A$ and $B$ are two non-zero square matrices such that $AB=0$, then $A$ and $B$ must both be singular. Why is this so? I can prove that if $A$ is non-singular then $B=I_nB=A^{-1}AB=0$, implying $B$ must be the zero matrix which is a contradiction. Similarly if $B$ is non-singular, then $A$ must be the zero matrix. Hence, both must be singular. But this doesn't really answer why, it just shows a contradiction for any case and hence must be the negation of our supposition that at least one is non-singular. I would like to know the essence and inherent property as to why they must be both singular (and why can't it be the case that only one is singular?) and what is the motivation for such a conclusion?","In the textbook Contemporary Linear Algebra by Anton and Busby, there was a small question in section 3.2 page 101 concerning this. It asks if $A$ and $B$ are two non-zero square matrices such that $AB=0$, then $A$ and $B$ must both be singular. Why is this so? I can prove that if $A$ is non-singular then $B=I_nB=A^{-1}AB=0$, implying $B$ must be the zero matrix which is a contradiction. Similarly if $B$ is non-singular, then $A$ must be the zero matrix. Hence, both must be singular. But this doesn't really answer why, it just shows a contradiction for any case and hence must be the negation of our supposition that at least one is non-singular. I would like to know the essence and inherent property as to why they must be both singular (and why can't it be the case that only one is singular?) and what is the motivation for such a conclusion?",,"['linear-algebra', 'matrices']"
97,Can the zero vector be an eigenvector for a matrix?,Can the zero vector be an eigenvector for a matrix?,,"I was checking over my work on WolfRamAlpha, and it says one of my eigenvalues (this one with multiplicity 2), has an eigenvector of (0,0,0). How can the zero vector be an eigenvector?","I was checking over my work on WolfRamAlpha, and it says one of my eigenvalues (this one with multiplicity 2), has an eigenvector of (0,0,0). How can the zero vector be an eigenvector?",,['linear-algebra']
98,Rank of sum of rank-$1$ matrices,Rank of sum of rank- matrices,1,If you sum a certain number of rank- $1$ matrices: $$X = u_1 u_1^T + u_2 u_2^T + \cdots + u_N u_N^T$$ Is the result guaranteed to be rank- $N$ assuming the individual $U$ vectors are linearly independent?,If you sum a certain number of rank- matrices: Is the result guaranteed to be rank- assuming the individual vectors are linearly independent?,1 X = u_1 u_1^T + u_2 u_2^T + \cdots + u_N u_N^T N U,"['linear-algebra', 'matrices', 'matrix-rank', 'rank-1-matrices']"
99,Why is there no generalization of the determinant to infinite dimensional vector spaces?,Why is there no generalization of the determinant to infinite dimensional vector spaces?,,"This question is to add to my understanding why the concept of a determinant does not extend to an infinite dimensional vector space. I am already aware of a couple facts which hint why this is so: The determinant of an endomorphism of a finite dimensional vector space with dimension $n$ can be defined in a basis-free way as the composition of these canonical maps: $$\mathrm{End}(V)\xrightarrow{\phi} \mathrm{End}(\Lambda^n V)\xrightarrow{\psi} K$$ defined by $\phi(A)=((x_1\wedge\cdots\wedge x_n)\mapsto(Ax_1\wedge\cdots\wedge Ax_n))$ and $\psi$ defined as the inverse of the map $\psi^{-1}:K\rightarrow \mathrm{End}(\Lambda^n V)$ defined by $\psi^{-1}(\lambda)=(x\mapsto \lambda x)$. This construction reveals why finite dimension is important: $\mathrm{End}(\Lambda^n V)$ need not be 1-dimensional otherwise for any $n$ if $V$ fails to be finite dimensional. And thus our last map fails to exist. Another reason that the determinant fails to extend to infinite dimensional spaces is that there are injective linear endomorphisms which do not have an inverse. Such maps may still have left inverses, but no right inverse. Such a pair is the right-shift and left-shift maps $$(x_1,x_2,\ldots)\mapsto(0,x_1,x_2,\ldots)\qquad (x_1,x_2,\ldots)\mapsto(x_2,x_3,\ldots)$$ where the left-shift is the left inverse of the right-shift; however, the left-shift remains non-invertible. A 'good' generalization of determinant would assign non-zero determinant to the first and zero to the last. This results in the determinant of the inverse not being the inverse of the determinant. Another way to see that the concept does not generalize is that if a determinant for an operator exists, you might expect it to be the product of the eigenvalues. In general, a linear endomorphism from an infinite dimensional space can have infinitely many eigenvalues. The previous fact suggests that we could define the determinant for a specific subset of $\mathrm{Aut}(V)$, namely those automorphisms which fix all but a finite number of 1-dimensional subspaces of $V$. But how far could we go with this generalization? Once you show that the determinant exists for a finite dimensional vector space, you can interpret the determinant as a nontrivial map which restricts to a group homomorphism from $\mathrm{Aut}(V)$ to $K^\times$ and assigns $0$ to the rest of the endomorphisms. Can we show that there is no nontrivial homomorphism from $\mathrm{Aut}(V)$ to $K^\times$ for an infinite-dimensional vector space? Much like we can show that the sign homomorphism does not extend to $S_{\Bbb N}$?","This question is to add to my understanding why the concept of a determinant does not extend to an infinite dimensional vector space. I am already aware of a couple facts which hint why this is so: The determinant of an endomorphism of a finite dimensional vector space with dimension $n$ can be defined in a basis-free way as the composition of these canonical maps: $$\mathrm{End}(V)\xrightarrow{\phi} \mathrm{End}(\Lambda^n V)\xrightarrow{\psi} K$$ defined by $\phi(A)=((x_1\wedge\cdots\wedge x_n)\mapsto(Ax_1\wedge\cdots\wedge Ax_n))$ and $\psi$ defined as the inverse of the map $\psi^{-1}:K\rightarrow \mathrm{End}(\Lambda^n V)$ defined by $\psi^{-1}(\lambda)=(x\mapsto \lambda x)$. This construction reveals why finite dimension is important: $\mathrm{End}(\Lambda^n V)$ need not be 1-dimensional otherwise for any $n$ if $V$ fails to be finite dimensional. And thus our last map fails to exist. Another reason that the determinant fails to extend to infinite dimensional spaces is that there are injective linear endomorphisms which do not have an inverse. Such maps may still have left inverses, but no right inverse. Such a pair is the right-shift and left-shift maps $$(x_1,x_2,\ldots)\mapsto(0,x_1,x_2,\ldots)\qquad (x_1,x_2,\ldots)\mapsto(x_2,x_3,\ldots)$$ where the left-shift is the left inverse of the right-shift; however, the left-shift remains non-invertible. A 'good' generalization of determinant would assign non-zero determinant to the first and zero to the last. This results in the determinant of the inverse not being the inverse of the determinant. Another way to see that the concept does not generalize is that if a determinant for an operator exists, you might expect it to be the product of the eigenvalues. In general, a linear endomorphism from an infinite dimensional space can have infinitely many eigenvalues. The previous fact suggests that we could define the determinant for a specific subset of $\mathrm{Aut}(V)$, namely those automorphisms which fix all but a finite number of 1-dimensional subspaces of $V$. But how far could we go with this generalization? Once you show that the determinant exists for a finite dimensional vector space, you can interpret the determinant as a nontrivial map which restricts to a group homomorphism from $\mathrm{Aut}(V)$ to $K^\times$ and assigns $0$ to the rest of the endomorphisms. Can we show that there is no nontrivial homomorphism from $\mathrm{Aut}(V)$ to $K^\times$ for an infinite-dimensional vector space? Much like we can show that the sign homomorphism does not extend to $S_{\Bbb N}$?",,['linear-algebra']
