,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A maximal ideal is always a prime ideal?,A maximal ideal is always a prime ideal?,,"A maximal ideal  is always a prime ideal, and the quotient ring  is always a field. In general, not all prime ideals are maximal. 1 In $2\mathbb{Z}$ , $4 \mathbb{Z} $ is a maximal ideal. Nevertheless it is not prime because $2 \cdot 2 \in 4\mathbb{Z}$ but $2 \notin 4\mathbb{Z}$ . What is that is misunderstand?","A maximal ideal  is always a prime ideal, and the quotient ring  is always a field. In general, not all prime ideals are maximal. 1 In , is a maximal ideal. Nevertheless it is not prime because but . What is that is misunderstand?",2\mathbb{Z} 4 \mathbb{Z}  2 \cdot 2 \in 4\mathbb{Z} 2 \notin 4\mathbb{Z},"['abstract-algebra', 'ideals', 'maximal-and-prime-ideals', 'rngs']"
1,Ring of order $p^2$ is commutative.,Ring of order  is commutative.,p^2,"I would like to show that ring of order $p^2$ is commutative. Taking $G=(R, +)$ as group, we have two possible isomorphism classes $\mathbb Z /p^2\mathbb Z$ and $\mathbb Z/ p\mathbb Z \times \mathbb Z /p\mathbb Z$. Since characterstic must divide the size of the group then we have two possibilities $p$ and $p^2$. Now IU don't understand how can I reason to say that the multiplication is commutative and how can I conclude for the case when characterstic is $p$?","I would like to show that ring of order $p^2$ is commutative. Taking $G=(R, +)$ as group, we have two possible isomorphism classes $\mathbb Z /p^2\mathbb Z$ and $\mathbb Z/ p\mathbb Z \times \mathbb Z /p\mathbb Z$. Since characterstic must divide the size of the group then we have two possibilities $p$ and $p^2$. Now IU don't understand how can I reason to say that the multiplication is commutative and how can I conclude for the case when characterstic is $p$?",,"['abstract-algebra', 'ring-theory', 'finite-rings']"
2,Intuition about the second isomorphism theorem,Intuition about the second isomorphism theorem,,"In group theory we have the second isomorphism theorem which can be stated as follows: Let $G$ be a group and let $S$ be a subgroup of $G$ and $N$ a normal subgroup of $G$ , then: The product $SN$ is a subgroup of $G$ . The intersection $S\cap N$ is a normal subgroup of $G$ . The quotient groups $SN/N$ and $S/(S\cap N)$ are isomorphic. I've seen this theorem some time now and I still can't grasp an intuition for it. I mean, it certainly is one important result, because as I've seen it is highlighted as one of the three isomorphism theorems. The first isomorphism theorem has a much more direct intuition though. We have groups $G$ and $H$ and a homomorphism $f:G\to H$ . If this $f$ is not injective we can quotient out what is stopping it from being injective and lift it to $G/\ker f$ as one isomorphism onto its image. Is there some nice interpretation like that for the second isormorphism theorem? How should we really understand this theorem?","In group theory we have the second isomorphism theorem which can be stated as follows: Let be a group and let be a subgroup of and a normal subgroup of , then: The product is a subgroup of . The intersection is a normal subgroup of . The quotient groups and are isomorphic. I've seen this theorem some time now and I still can't grasp an intuition for it. I mean, it certainly is one important result, because as I've seen it is highlighted as one of the three isomorphism theorems. The first isomorphism theorem has a much more direct intuition though. We have groups and and a homomorphism . If this is not injective we can quotient out what is stopping it from being injective and lift it to as one isomorphism onto its image. Is there some nice interpretation like that for the second isormorphism theorem? How should we really understand this theorem?",G S G N G SN G S\cap N G SN/N S/(S\cap N) G H f:G\to H f G/\ker f,"['abstract-algebra', 'group-theory', 'intuition', 'group-isomorphism']"
3,When can we say elements of tensor product are equal to $0$?,When can we say elements of tensor product are equal to ?,0,"I am learning about tensor products of modules, but there is a question which makes me very confused about it! If $E$ is a right $R$-module and $F$ is a left $R$-module, then suppose we have a balanced map (or bilinear map) $E\times F\to E\otimes F$. If some element $x\otimes y \in E\otimes F$ is $0$, then can we say $x$ or $y$ must be equal to $0$? I know if $x = 0$ or $y = 0$, then $x\otimes y$ is $0$. Are there other cases where $x\otimes y$ is $0$? Can someone give me a specific example? Really thank you!","I am learning about tensor products of modules, but there is a question which makes me very confused about it! If $E$ is a right $R$-module and $F$ is a left $R$-module, then suppose we have a balanced map (or bilinear map) $E\times F\to E\otimes F$. If some element $x\otimes y \in E\otimes F$ is $0$, then can we say $x$ or $y$ must be equal to $0$? I know if $x = 0$ or $y = 0$, then $x\otimes y$ is $0$. Are there other cases where $x\otimes y$ is $0$? Can someone give me a specific example? Really thank you!",,"['abstract-algebra', 'modules', 'tensor-products']"
4,The ring $ℤ/nℤ$ is a field if and only if $n$ is prime,The ring  is a field if and only if  is prime,ℤ/nℤ n,Let $n \in ℕ$. Show that the ring $ℤ/nℤ$ is a field if and only if $n$   is prime. Let $n$ prime. I need to show that if $\bar{a} \neq 0$ then $∃\bar b: \bar{a} \cdot \bar{b} = \bar{1}$. Any hints for this ? Suppose $ℤ/nℤ$ is a field. Therefore: for every $\bar{a} \neq 0$ $∃\bar b: \bar{a}\cdot \bar{b}=1$. How can I show that $n$ must be prime ?,Let $n \in ℕ$. Show that the ring $ℤ/nℤ$ is a field if and only if $n$   is prime. Let $n$ prime. I need to show that if $\bar{a} \neq 0$ then $∃\bar b: \bar{a} \cdot \bar{b} = \bar{1}$. Any hints for this ? Suppose $ℤ/nℤ$ is a field. Therefore: for every $\bar{a} \neq 0$ $∃\bar b: \bar{a}\cdot \bar{b}=1$. How can I show that $n$ must be prime ?,,"['abstract-algebra', 'ring-theory']"
5,Category Theory vs. Universal Algebra - Any References?,Category Theory vs. Universal Algebra - Any References?,,"After seeing the answer to the question Category theory, a branch of abstract algebra , I would like to ask Are there literature discussing the difference/indifference/comparison between category theory and universal algebra?","After seeing the answer to the question Category theory, a branch of abstract algebra , I would like to ask Are there literature discussing the difference/indifference/comparison between category theory and universal algebra?",,"['abstract-algebra', 'reference-request', 'soft-question', 'category-theory', 'universal-algebra']"
6,Describe all ring homomorphisms,Describe all ring homomorphisms,,Describe all ring homomorphisms of: a) $\mathbb{Z}$ into $\mathbb{Z}$ b) $\mathbb{Z}$ into $\mathbb{Z} \times \mathbb{Z}$ c) $\mathbb{Z} \times \mathbb{Z}$ into $\mathbb{Z}$ d) How many homomorphisms are there of $\mathbb{Z} \times \mathbb{Z} \times \mathbb{Z}$ into $\mathbb{Z}$ Note: These were past homework questions and my professor already gave out answers. I just need someone to help me understand and approach this type of problem. Thank you.,Describe all ring homomorphisms of: a) $\mathbb{Z}$ into $\mathbb{Z}$ b) $\mathbb{Z}$ into $\mathbb{Z} \times \mathbb{Z}$ c) $\mathbb{Z} \times \mathbb{Z}$ into $\mathbb{Z}$ d) How many homomorphisms are there of $\mathbb{Z} \times \mathbb{Z} \times \mathbb{Z}$ into $\mathbb{Z}$ Note: These were past homework questions and my professor already gave out answers. I just need someone to help me understand and approach this type of problem. Thank you.,,"['abstract-algebra', 'ring-theory']"
7,Group presentation for semidirect products,Group presentation for semidirect products,,"If $G$ and $H$ are groups with presentations $G=\langle X|R \rangle$ and $H=\langle Y| S \rangle$, then of course $G \times H$ has presentation $\langle X,Y | xy=yx \ \forall x \in X \ \text{and} \  y \in Y, R,S \rangle$.  Given two group presentations $G=\langle X|R \rangle$ and $H=\langle Y| S \rangle$ and a homomorphism $\phi: H \rightarrow \operatorname{Aut}(G)$, what is a presentation for $G \rtimes H$?  Is there a nice presentation, as in the direct product case?  Thanks!","If $G$ and $H$ are groups with presentations $G=\langle X|R \rangle$ and $H=\langle Y| S \rangle$, then of course $G \times H$ has presentation $\langle X,Y | xy=yx \ \forall x \in X \ \text{and} \  y \in Y, R,S \rangle$.  Given two group presentations $G=\langle X|R \rangle$ and $H=\langle Y| S \rangle$ and a homomorphism $\phi: H \rightarrow \operatorname{Aut}(G)$, what is a presentation for $G \rtimes H$?  Is there a nice presentation, as in the direct product case?  Thanks!",,"['abstract-algebra', 'group-theory', 'group-presentation', 'semidirect-product']"
8,An automorphism of the field of $p$-adic numbers,An automorphism of the field of -adic numbers,p,"Is an automorphism of the field $\mathbb{Q}_p$ of $p$-adic numbers the identity map? If yes, how can we prove it? Note :We don't assume an automorphism of $\mathbb{Q}_p$ is continuous.","Is an automorphism of the field $\mathbb{Q}_p$ of $p$-adic numbers the identity map? If yes, how can we prove it? Note :We don't assume an automorphism of $\mathbb{Q}_p$ is continuous.",,"['abstract-algebra', 'p-adic-number-theory']"
9,Examples proving why the tensor product does not distribute over direct products.,Examples proving why the tensor product does not distribute over direct products.,,"I recently read about the result that the tensor product distributes over direct sums. I was curious if it also distributes over direct products, but google tells me it doesn't. What are some simple counterexamples to why this property isn't true? I know that there is a natural homomorphism  $$ \left(\prod M_i\right)\otimes N\to \prod (M_i\otimes N) $$ given by $(\prod m_i)\otimes n\mapsto \prod (m_i\otimes n)$ when $M$ and $N$ are modules over some commutative ring $R$. Are there standard examples where this homomorphism is not injective/surjective and hence not an isomorphism?","I recently read about the result that the tensor product distributes over direct sums. I was curious if it also distributes over direct products, but google tells me it doesn't. What are some simple counterexamples to why this property isn't true? I know that there is a natural homomorphism  $$ \left(\prod M_i\right)\otimes N\to \prod (M_i\otimes N) $$ given by $(\prod m_i)\otimes n\mapsto \prod (m_i\otimes n)$ when $M$ and $N$ are modules over some commutative ring $R$. Are there standard examples where this homomorphism is not injective/surjective and hence not an isomorphism?",,"['abstract-algebra', 'modules', 'tensor-products']"
10,Applications of the Jordan-Hölder Theorem.,Applications of the Jordan-Hölder Theorem.,,"All books about group theory bring the Jordan-Hölder theorem, but does not give applications. I only saw in Rotman's book the Fundamental Theorem of Arithmetic being proved as a consequence of this theorem. What is a nice application of this theorem? Any references?","All books about group theory bring the Jordan-Hölder theorem, but does not give applications. I only saw in Rotman's book the Fundamental Theorem of Arithmetic being proved as a consequence of this theorem. What is a nice application of this theorem? Any references?",,"['abstract-algebra', 'group-theory', 'reference-request', 'modules']"
11,On the meaning of being algebraically closed,On the meaning of being algebraically closed,,"The definition of algebraic number is that $\alpha$ is an algebraic number if there is a nonzero polynomial $p(x)$ in $\mathbb{Q}[x]$ such that $p(\alpha)=0$. By algebraic closure, every nonconstant polynomial with algebraic coefficients has algebraic roots; then, there will be also a nonconstant polynomial with rational coefficients that has those roots. I feel uncomfortable with the idea that the root of a polynomial with algebraic coefficients is again algebraic; why are we sure that for every polynomial in $\mathbb{\bar{Q}}[x]$ we could find a polynomial in $\mathbb{Q}[x]$ that has the same roots? I apologize if I'm asking something really trivial or my question comes from a big misunderstanding of basic concepts.","The definition of algebraic number is that $\alpha$ is an algebraic number if there is a nonzero polynomial $p(x)$ in $\mathbb{Q}[x]$ such that $p(\alpha)=0$. By algebraic closure, every nonconstant polynomial with algebraic coefficients has algebraic roots; then, there will be also a nonconstant polynomial with rational coefficients that has those roots. I feel uncomfortable with the idea that the root of a polynomial with algebraic coefficients is again algebraic; why are we sure that for every polynomial in $\mathbb{\bar{Q}}[x]$ we could find a polynomial in $\mathbb{Q}[x]$ that has the same roots? I apologize if I'm asking something really trivial or my question comes from a big misunderstanding of basic concepts.",,"['abstract-algebra', 'field-theory']"
12,"Galois Group of $x^p - 2$, $p$ an odd prime","Galois Group of ,  an odd prime",x^p - 2 p,"Question is to determine the Galois group of $x^p-2$ for an odd prime $p$ . For finding the Galois group, we look for the splitting field of $x^p-2$ which can be seen as $\mathbb{Q}(\sqrt[p]{2},\zeta)$ where $\zeta$ is a primitive $p^{th}$ root of unity. Consider $\mathbb{Q}\subset \mathbb{Q}(\zeta) \subset \mathbb{Q}(\sqrt[p]{2},\zeta)$ .we know that $\mathbb{Q}(\sqrt[p]{2},\zeta)$ is Galois over $\mathbb{Q}(\zeta)$ ,  we find Corresponding Galois Group say $G_1$ . Consider $\mathbb{Q}\subset \mathbb{Q}(\sqrt[p]{2}) \subset \mathbb{Q}(\sqrt[p]{2},\zeta)$ . we know that $\mathbb{Q}(\sqrt[p]{2},\zeta)$ is galois over $ \mathbb{Q}(\sqrt[p]{2})$ , we find Corresponding Galois Group say $G_2$ . Then Galois Group of $\mathbb{Q}(\sqrt[p]{2},\zeta)$ would possibly be Product of these two subgroups $G_1$ and $G_2$ with some relation between the generators. For $Gal(\mathbb{Q}(\sqrt[p]{2},\zeta)/\mathbb{Q}(\zeta))$ , consider $\tau: \mathbb{Q}(\sqrt[p]{2},\zeta) \rightarrow \mathbb{Q}(\sqrt[p]{2},\zeta)$ fixing $\zeta$ and sending $\sqrt[p]{2} \rightarrow \sqrt[p]{2}\zeta$ . $\tau(\sqrt[p]{2})=\sqrt[p]{2}\zeta$ , $\tau^2(\sqrt[p]{2})=\tau(\tau(\sqrt[p]{2}))=\tau(\sqrt[p]{2}\zeta)=\tau(\sqrt[p]{2})\tau(\zeta)=\sqrt[p]{2}\zeta^2$ , For similar Reasons, $\tau^{p}(\sqrt[p]{2})=\sqrt[p]{2}\zeta^p=\sqrt[p]{2}$ . No power of $\tau$ less than $p$ gives identity as no power of $\zeta$ less than $p$ gives identity. So, $Gal(\mathbb{Q}(\sqrt[p]{2},\zeta)/\mathbb{Q}(\zeta)) \cong \mathbb{Z}_p \cong \big< \tau \big>$ . For $Gal(\mathbb{Q}(\sqrt[p]{2},\zeta)/\mathbb{Q}(\sqrt[p]{2}))$ , consider $\sigma : \mathbb{Q}(\sqrt[p]{2},\zeta) \rightarrow \mathbb{Q}(\sqrt[p]{2},\zeta)$ fixing $\sqrt[p]{2}$ and sending $\zeta \rightarrow \zeta^2$ $\sigma(\zeta)=\zeta^2$ $\sigma^2(\zeta)=\sigma(\sigma(\zeta))=\sigma(\zeta^2)=\zeta^{(2^2)}$ For similar reasons, $\sigma^{p-1}(\zeta)=\zeta^{(2^{p-1})}$ , as for every $a\in \mathbb{F}_p^\times$ , we have $a^{p-1}=1$ we have in particular $2^{p-1} \equiv~1~mod~p$ . So, $\sigma^{p-1}(\zeta)=\zeta^{(2^{p-1})}=\zeta$ (as $\zeta$ is a $p^{th}$ root of unity). No power of $\sigma$ less than $p-1$ gives identity as $2\in \mathbb{F}_p$ generates Multiplicative group, no power of $2$ less than $p-1$ can be equal to $1~mod~p$ . So, $Gal(\mathbb{Q}(\sqrt[p]{2},\zeta)/\mathbb{Q}(\sqrt[p]{2})) \cong \mathbb{Z}_{p-1} \cong \big< \sigma\big>$ . As $[\mathbb{Q}(\sqrt[p]{2},\zeta):\mathbb{Q}]=p(p-1)$ and $|\sigma|=p-1$ and $|\tau|=p$ i strongly feel Galois group should be possibly generated by $\sigma$ and $\tau$ with ""Some extra related conditions""But not very sure to confirm this. I am not able to go any further, I can see that $\sigma$ and $\tau$ do not commute with each other. I am unable to produce a know group which contain isomorphic copies of $\mathbb{Z}_{p-1}$ and $\mathbb{Z}_p$ as subgroups. I would be thankful if some one can help me out in this case. Thank You.","Question is to determine the Galois group of for an odd prime . For finding the Galois group, we look for the splitting field of which can be seen as where is a primitive root of unity. Consider .we know that is Galois over ,  we find Corresponding Galois Group say . Consider . we know that is galois over , we find Corresponding Galois Group say . Then Galois Group of would possibly be Product of these two subgroups and with some relation between the generators. For , consider fixing and sending . , , For similar Reasons, . No power of less than gives identity as no power of less than gives identity. So, . For , consider fixing and sending For similar reasons, , as for every , we have we have in particular . So, (as is a root of unity). No power of less than gives identity as generates Multiplicative group, no power of less than can be equal to . So, . As and and i strongly feel Galois group should be possibly generated by and with ""Some extra related conditions""But not very sure to confirm this. I am not able to go any further, I can see that and do not commute with each other. I am unable to produce a know group which contain isomorphic copies of and as subgroups. I would be thankful if some one can help me out in this case. Thank You.","x^p-2 p x^p-2 \mathbb{Q}(\sqrt[p]{2},\zeta) \zeta p^{th} \mathbb{Q}\subset \mathbb{Q}(\zeta) \subset \mathbb{Q}(\sqrt[p]{2},\zeta) \mathbb{Q}(\sqrt[p]{2},\zeta) \mathbb{Q}(\zeta) G_1 \mathbb{Q}\subset \mathbb{Q}(\sqrt[p]{2}) \subset \mathbb{Q}(\sqrt[p]{2},\zeta) \mathbb{Q}(\sqrt[p]{2},\zeta)  \mathbb{Q}(\sqrt[p]{2}) G_2 \mathbb{Q}(\sqrt[p]{2},\zeta) G_1 G_2 Gal(\mathbb{Q}(\sqrt[p]{2},\zeta)/\mathbb{Q}(\zeta)) \tau: \mathbb{Q}(\sqrt[p]{2},\zeta) \rightarrow \mathbb{Q}(\sqrt[p]{2},\zeta) \zeta \sqrt[p]{2} \rightarrow \sqrt[p]{2}\zeta \tau(\sqrt[p]{2})=\sqrt[p]{2}\zeta \tau^2(\sqrt[p]{2})=\tau(\tau(\sqrt[p]{2}))=\tau(\sqrt[p]{2}\zeta)=\tau(\sqrt[p]{2})\tau(\zeta)=\sqrt[p]{2}\zeta^2 \tau^{p}(\sqrt[p]{2})=\sqrt[p]{2}\zeta^p=\sqrt[p]{2} \tau p \zeta p Gal(\mathbb{Q}(\sqrt[p]{2},\zeta)/\mathbb{Q}(\zeta)) \cong \mathbb{Z}_p \cong \big< \tau \big> Gal(\mathbb{Q}(\sqrt[p]{2},\zeta)/\mathbb{Q}(\sqrt[p]{2})) \sigma : \mathbb{Q}(\sqrt[p]{2},\zeta) \rightarrow \mathbb{Q}(\sqrt[p]{2},\zeta) \sqrt[p]{2} \zeta \rightarrow \zeta^2 \sigma(\zeta)=\zeta^2 \sigma^2(\zeta)=\sigma(\sigma(\zeta))=\sigma(\zeta^2)=\zeta^{(2^2)} \sigma^{p-1}(\zeta)=\zeta^{(2^{p-1})} a\in \mathbb{F}_p^\times a^{p-1}=1 2^{p-1} \equiv~1~mod~p \sigma^{p-1}(\zeta)=\zeta^{(2^{p-1})}=\zeta \zeta p^{th} \sigma p-1 2\in \mathbb{F}_p 2 p-1 1~mod~p Gal(\mathbb{Q}(\sqrt[p]{2},\zeta)/\mathbb{Q}(\sqrt[p]{2})) \cong \mathbb{Z}_{p-1} \cong \big< \sigma\big> [\mathbb{Q}(\sqrt[p]{2},\zeta):\mathbb{Q}]=p(p-1) |\sigma|=p-1 |\tau|=p \sigma \tau \sigma \tau \mathbb{Z}_{p-1} \mathbb{Z}_p",['abstract-algebra']
13,"Group formed on Parabola similarly to how an Elliptic curve forms a group (by drawing lines, circles, intersecting, or taking tangent lines)","Group formed on Parabola similarly to how an Elliptic curve forms a group (by drawing lines, circles, intersecting, or taking tangent lines)",,"There's probably other ways of doing this, but I've found this to be the simplest way (group law) that does indeed work: To add points $A, B \in \{(x, f(x)) : x \in \Bbb{C}\} = G$ where $f$ is any parabola with vertex $E \in G$ , we treat $E$ as zero.  Now draw a line between $A, B$ and then draw a parallel line to this line that passes through $E$ .  The unique intersection point (other than $E$ , unless of course $A = B = E$ or $A = -B$ ) is then the value of the group law. I've checked all the axioms of a group using Geogebra.  This also works on a circle if I recall correctly. I'm wondering: How do we express $AB$ the abelian group law algebraically?","There's probably other ways of doing this, but I've found this to be the simplest way (group law) that does indeed work: To add points where is any parabola with vertex , we treat as zero.  Now draw a line between and then draw a parallel line to this line that passes through .  The unique intersection point (other than , unless of course or ) is then the value of the group law. I've checked all the axioms of a group using Geogebra.  This also works on a circle if I recall correctly. I'm wondering: How do we express the abelian group law algebraically?","A, B \in \{(x, f(x)) : x \in \Bbb{C}\} = G f E \in G E A, B E E A = B = E A = -B AB","['abstract-algebra', 'group-theory', 'conic-sections', 'elliptic-curves', 'geometric-construction']"
14,In a Dedekind domain every ideal is either principal or generated by two elements.,In a Dedekind domain every ideal is either principal or generated by two elements.,,Prove that in a Dedekind domain every ideal is either principal or generated by two elements. Help me some hints. Thanks a lot!,Prove that in a Dedekind domain every ideal is either principal or generated by two elements. Help me some hints. Thanks a lot!,,"['abstract-algebra', 'commutative-algebra', 'algebraic-number-theory', 'dedekind-domain']"
15,Tensor product of two finitely-generated modules over a local ring is zero,Tensor product of two finitely-generated modules over a local ring is zero,,"If $R$ is a local ring and $M$ and $N$ are finitely generated $R$-modules such that $M\otimes N=0$ then how does it follow from Nakayama's lemma that either $M=0$ or $N=0$? This is an exercise in Atiyah and Macdonald. The part I could not show in the hints is $({M{\otimes}_R N)}_{k}=0$ implies $M_{k}{\otimes }_{k} N_{k}=0$, where $k=R/\mathfrak m$ and $\mathfrak m$ is the maximal ideal of $R$.","If $R$ is a local ring and $M$ and $N$ are finitely generated $R$-modules such that $M\otimes N=0$ then how does it follow from Nakayama's lemma that either $M=0$ or $N=0$? This is an exercise in Atiyah and Macdonald. The part I could not show in the hints is $({M{\otimes}_R N)}_{k}=0$ implies $M_{k}{\otimes }_{k} N_{k}=0$, where $k=R/\mathfrak m$ and $\mathfrak m$ is the maximal ideal of $R$.",,"['abstract-algebra', 'commutative-algebra', 'tensor-products']"
16,A finite ring is a field if its units $\cup\ \{0\}$ comprise a field of characteristic $\ne 2$,A finite ring is a field if its units  comprise a field of characteristic,\cup\ \{0\} \ne 2,"Suppose $R$ is a finite ring (commutative ring with $1$) of characteristic $3$ and suppose that for every unit $u \in R\:,\ 1+u\ $ is also a unit or $0$. We need to show that $R$ is a field. Is this true if ${\rm char}(R) > 3$? Here is what I attempted to do. $\:$  First of all, $\:$ I noticed that the statement is not true if $\ R\ $ is infinite ($ \mathbb F_3[x]$ is an example of an infinite ring which is not a field but it satisfies all the required properties). Now, in a finite ring, a non-zero element is either a unit or a $\:0\:$ divisor, so I tried to show that $R$ has no $\:0\:$ divisors. Clearly, $R$ has no nonzero nilpotent elements (if $x$ is nilpotent, then $1+x$ is a unit, but then $1+(1+x)$ and $1+(2+x)$ is either a unit or $\:0\:.\:$ Hence $x$ is either a unit or $\:0\:,\:$ and since $x$ is nilpotent, it can't be a unit, so we must have $x = 0$). But this does not solve the problem, since $R$ could have elements that are $\:0\:$ divisors but not nilpotent (for example, $\ (1,0)\ $ is a $\:0\:$ divisor in $\ \mathbb Z/3\:\mathbb Z \times \mathbb Z/3\:\mathbb Z\ $ but it is not nilpotent). Another observation I made is that the set of units, together with $\:0\:$ forms a group under addition, so that $J = R^{*}$, together with $\:0\:$ is a subring of $R$. hence we may view $R$ as a $J$-module (and since $J$ is clearly a field, $R$ is a $J$-vector space). Another thing I tried is to show that $R$ has no proper nontrivial ideals. Viewing $R$ and $J$ as abelian groups, I noticed that a non-trivial ideal of $R$ can contain at most one element from each coset of $J$ in $R$, because if an ideal contains two distinct elements from the same coset of $J$ in $R$, this ideal would have to contain their difference, hence it'd have to contain a unit, hence it would not be a proper ideal. But again, I don't see how this observation leads to a solution. As for the last part, I suspect that this statement will remain true if ${\rm char}(R) > 3$. Since $1$ is a unit, it follows that $1,2,3,\ldots$ are all either units or $0$, which can only happen if ${\rm char}(R) = p$, a prime number (and then I suspect that $R$ will have to be a finite field), but again  I do not see how to prove (or disprove) this. By the way, this is not a homework problem. I am studying algebra on my own, and after thinking about it for a few days and making the observations I listed above, I still don't see how to finish the proof. I would appreciate your suggestions. Thank you in advance.","Suppose $R$ is a finite ring (commutative ring with $1$) of characteristic $3$ and suppose that for every unit $u \in R\:,\ 1+u\ $ is also a unit or $0$. We need to show that $R$ is a field. Is this true if ${\rm char}(R) > 3$? Here is what I attempted to do. $\:$  First of all, $\:$ I noticed that the statement is not true if $\ R\ $ is infinite ($ \mathbb F_3[x]$ is an example of an infinite ring which is not a field but it satisfies all the required properties). Now, in a finite ring, a non-zero element is either a unit or a $\:0\:$ divisor, so I tried to show that $R$ has no $\:0\:$ divisors. Clearly, $R$ has no nonzero nilpotent elements (if $x$ is nilpotent, then $1+x$ is a unit, but then $1+(1+x)$ and $1+(2+x)$ is either a unit or $\:0\:.\:$ Hence $x$ is either a unit or $\:0\:,\:$ and since $x$ is nilpotent, it can't be a unit, so we must have $x = 0$). But this does not solve the problem, since $R$ could have elements that are $\:0\:$ divisors but not nilpotent (for example, $\ (1,0)\ $ is a $\:0\:$ divisor in $\ \mathbb Z/3\:\mathbb Z \times \mathbb Z/3\:\mathbb Z\ $ but it is not nilpotent). Another observation I made is that the set of units, together with $\:0\:$ forms a group under addition, so that $J = R^{*}$, together with $\:0\:$ is a subring of $R$. hence we may view $R$ as a $J$-module (and since $J$ is clearly a field, $R$ is a $J$-vector space). Another thing I tried is to show that $R$ has no proper nontrivial ideals. Viewing $R$ and $J$ as abelian groups, I noticed that a non-trivial ideal of $R$ can contain at most one element from each coset of $J$ in $R$, because if an ideal contains two distinct elements from the same coset of $J$ in $R$, this ideal would have to contain their difference, hence it'd have to contain a unit, hence it would not be a proper ideal. But again, I don't see how this observation leads to a solution. As for the last part, I suspect that this statement will remain true if ${\rm char}(R) > 3$. Since $1$ is a unit, it follows that $1,2,3,\ldots$ are all either units or $0$, which can only happen if ${\rm char}(R) = p$, a prime number (and then I suspect that $R$ will have to be a finite field), but again  I do not see how to prove (or disprove) this. By the way, this is not a homework problem. I am studying algebra on my own, and after thinking about it for a few days and making the observations I listed above, I still don't see how to finish the proof. I would appreciate your suggestions. Thank you in advance.",,"['abstract-algebra', 'ring-theory', 'finite-rings']"
17,"Why is it called a 'ring', why is it called a 'field'?","Why is it called a 'ring', why is it called a 'field'?",,"The definitions of 'ring' and 'field' are pretty straightforward.  For a ring (e.g. integers): addition is commutative $( 1 + 2 = 2 + 1 )$ addition and multiplication are associative $(2 +(2+2)) = ((2 + 2) + 2)$ multiplication distributes over addition $( 2*(5 + 7) = 2*5 + 2*7 )$ each element has an additive inverse $( 2 + -2 = 0 )$ there exists an additive identity $( 2 + 0 = 2 )$ The word ring, though, has a concrete meaning in English.  Its round.  Usually when people name something, it is a metaphor of some kind.  Where's the metaphor for 'ring'?  What about 'field'?","The definitions of 'ring' and 'field' are pretty straightforward.  For a ring (e.g. integers): addition is commutative $( 1 + 2 = 2 + 1 )$ addition and multiplication are associative $(2 +(2+2)) = ((2 + 2) + 2)$ multiplication distributes over addition $( 2*(5 + 7) = 2*5 + 2*7 )$ each element has an additive inverse $( 2 + -2 = 0 )$ there exists an additive identity $( 2 + 0 = 2 )$ The word ring, though, has a concrete meaning in English.  Its round.  Usually when people name something, it is a metaphor of some kind.  Where's the metaphor for 'ring'?  What about 'field'?",,"['abstract-algebra', 'terminology', 'ring-theory', 'field-theory']"
18,Galois group of a reducible polynomial over $\mathbb {Q}$,Galois group of a reducible polynomial over,\mathbb {Q},"Let $f \in\mathbb {Q}[X]$ be reducible - for the sake of simplicity, $ f = gh$ with $g,h \in\mathbb {Q}[X]$ irreducible. Let L be the splitting field of f. Does $Gal(f) \simeq Gal(g) \times Gal(h)$ hold, were Gal(~) denotes the galois group of the splitting field of the respective polynomial? From what I understand, every element in the Galois group of g may be (not uniquely) extended to a homomorphism from L to $\overline{\mathbb{Q}}$ since L is an algebraic extension of $\mathbb{Q}$, and because L is galois ($\implies$normal), this extended homomorphism maps L to L, thus, it is an element of Gal(f). So, $Gal(g) \subset Gal(f)$. I also know that Gal(f) cannot act transitively on the roots of f since f is reducible. Since Gal(g) acts transitively on the roots of g and because Gal(g) embeds into Gal(f), there cannot be an automorphism in Gal(f) mapping a root of g to a root of h, because if such an automorphism existed, I could permute all the roots of f in any way I wanted, which is not possible. However, I don't see how I can solve the problem I stated in the beginning from here.","Let $f \in\mathbb {Q}[X]$ be reducible - for the sake of simplicity, $ f = gh$ with $g,h \in\mathbb {Q}[X]$ irreducible. Let L be the splitting field of f. Does $Gal(f) \simeq Gal(g) \times Gal(h)$ hold, were Gal(~) denotes the galois group of the splitting field of the respective polynomial? From what I understand, every element in the Galois group of g may be (not uniquely) extended to a homomorphism from L to $\overline{\mathbb{Q}}$ since L is an algebraic extension of $\mathbb{Q}$, and because L is galois ($\implies$normal), this extended homomorphism maps L to L, thus, it is an element of Gal(f). So, $Gal(g) \subset Gal(f)$. I also know that Gal(f) cannot act transitively on the roots of f since f is reducible. Since Gal(g) acts transitively on the roots of g and because Gal(g) embeds into Gal(f), there cannot be an automorphism in Gal(f) mapping a root of g to a root of h, because if such an automorphism existed, I could permute all the roots of f in any way I wanted, which is not possible. However, I don't see how I can solve the problem I stated in the beginning from here.",,"['abstract-algebra', 'field-theory', 'galois-theory']"
19,Visualising finite fields,Visualising finite fields,,"I'm interested in finding visual and/or physical approaches to understanding finite fields. I know of a few: V. I. Arnold has a few pictures of 'finite circles' and 'finite tori' in his book Dynamics, Statistics and Projective Geometry of Galois Fields . Also, N. Carter displays what you might call 'double Cayley diagrams' of the fields of order $4=2^2$ and $8=2^3$ in his book Visual Group Theory , which I reproduce here: The solid lines are the graph for addition and the dotted lines are the graph for multiplication. I like how you can see the structure of the additive group as a product of cyclic groups with the order of the characteristic, and if you look closer you can also see how the multiplicative group is cyclic. Are there any other interesting visual/physical ways of understanding finite fields?","I'm interested in finding visual and/or physical approaches to understanding finite fields. I know of a few: V. I. Arnold has a few pictures of 'finite circles' and 'finite tori' in his book Dynamics, Statistics and Projective Geometry of Galois Fields . Also, N. Carter displays what you might call 'double Cayley diagrams' of the fields of order $4=2^2$ and $8=2^3$ in his book Visual Group Theory , which I reproduce here: The solid lines are the graph for addition and the dotted lines are the graph for multiplication. I like how you can see the structure of the additive group as a product of cyclic groups with the order of the characteristic, and if you look closer you can also see how the multiplicative group is cyclic. Are there any other interesting visual/physical ways of understanding finite fields?",,"['abstract-algebra', 'intuition', 'finite-fields', 'visualization']"
20,Recovering a finite group's structure from the order of its elements.,Recovering a finite group's structure from the order of its elements.,,"Suppose you know the following two things about a group $G$ with $n$ elements: the order of each of the $n$ elements in $G$; $G$ is uniquely determined by the orders in (1). Question: How difficult is it to recover the group structure of $G$? In other words, what is the best way to use this information to construct a Cayley table for $G$? Note: (1) alone is not enough to uniquely determine a group. See this MO post for more. Information about identifying when (1) implies (2) would be welcomed as well.","Suppose you know the following two things about a group $G$ with $n$ elements: the order of each of the $n$ elements in $G$; $G$ is uniquely determined by the orders in (1). Question: How difficult is it to recover the group structure of $G$? In other words, what is the best way to use this information to construct a Cayley table for $G$? Note: (1) alone is not enough to uniquely determine a group. See this MO post for more. Information about identifying when (1) implies (2) would be welcomed as well.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups', 'computational-algebra']"
21,Prove that the multiplicative groups $\mathbb{R} - \{0\}$ and $\mathbb{C} - \{0\}$ are not isomorphic.,Prove that the multiplicative groups  and  are not isomorphic.,\mathbb{R} - \{0\} \mathbb{C} - \{0\},"Is my proof correct? I have made use of the fact isomorphism preserves order of elements, which I proved couple of exercises back. I am also interested in other ways of proving it. Is there a more explicit way or is this explicit enough? Problem Prove that the multiplicative groups $\mathbb{R} - \{0\}$ and $\mathbb{C} - \{0\}$ are not isomorphic. Solution Recall that isomorphism preserves order of elements and hence if there exists an isomorphism from $\phi: \mathbb{C}-\{0\} \mapsto \mathbb{R}-\{0\}$, then $x \in \mathbb{C} - \{0\}$, and $\phi(x) \in \mathbb{R} - \{0\}$, then $\vert x \vert = \vert \phi(x) \vert$. Now note that the element $i \in \mathbb{C} - \{0\}$ has order $4$. However, no element in $\mathbb{R}-\{0\}$ has order $4$. Hence, no isomorphism can exist. Hence, the multiplicative groups $\mathbb{R} - \{0\}$ and $\mathbb{C} - \{0\}$ are not isomorphic. Thanks","Is my proof correct? I have made use of the fact isomorphism preserves order of elements, which I proved couple of exercises back. I am also interested in other ways of proving it. Is there a more explicit way or is this explicit enough? Problem Prove that the multiplicative groups $\mathbb{R} - \{0\}$ and $\mathbb{C} - \{0\}$ are not isomorphic. Solution Recall that isomorphism preserves order of elements and hence if there exists an isomorphism from $\phi: \mathbb{C}-\{0\} \mapsto \mathbb{R}-\{0\}$, then $x \in \mathbb{C} - \{0\}$, and $\phi(x) \in \mathbb{R} - \{0\}$, then $\vert x \vert = \vert \phi(x) \vert$. Now note that the element $i \in \mathbb{C} - \{0\}$ has order $4$. However, no element in $\mathbb{R}-\{0\}$ has order $4$. Hence, no isomorphism can exist. Hence, the multiplicative groups $\mathbb{R} - \{0\}$ and $\mathbb{C} - \{0\}$ are not isomorphic. Thanks",,"['abstract-algebra', 'group-theory', 'proof-writing', 'proof-verification']"
22,"Prove that the map $\phi:S^3\times S^3\to{\bf GL}(4,\Bbb R)$ defined via quaternions as$\phi(p,q)(v)=pvq^{-1}$ has image ${\bf SO}(4)$",Prove that the map  defined via quaternions as has image,"\phi:S^3\times S^3\to{\bf GL}(4,\Bbb R) \phi(p,q)(v)=pvq^{-1} {\bf SO}(4)","I am interested in the map $\phi:S^3 \times S^3 \to GL_4(\mathbb{R})$ given as follows: Let $(p,q) \in S^3 \times S^3$. We identify $p$ and $q$ as real quaternions with unit norms and define $\phi(p,q)$ to be the map sending $v \in \mathbb{H}$ to the product $pvq^{-1}$, where $\mathbb{R}^4$ and $\mathbb{H}$ are identified in the obvious way as $\mathbb{R}$-vector spaces. It isn't hard to show that $\phi$ is a group homomorphism. I would like to prove the following in complete detail: 1) The image of $\phi$ is $SO(4)$. 2) The kernel of $\phi$ is $\{(1,1), (-1, -1)\}$ I proceed by noting that the maps $\phi(p,q)$ are clearly $\mathbb{R}$-linear, and since $p$ and $q$ have unit norm, these maps are also easily seen to be orthogonal. Thus, we have $\phi(S^3 \times S^3) \subseteq O(4)$. Now suppose $\phi(p,q)$ is the identity map on $\mathbb{R}^4$. That is, $pvq^{-1}=v\ \ \ \forall v \in \mathbb{H}$. Then for $v=1 \in \mathbb{H}$, we have $pq^{-1}=1$, and so $p=q$. By considering the equations $piq^{-1}=i$, $pjq^{-1}=j$, and $pkq^{-1}=k$, we conclude that $p=q \in \mathbb{R}$. Since $\mathbb{R} \cap S^3 = \{\pm1\}$, an easy compuation shows that the kernel of $\phi$ is $\{(1,1), (-1, -1)\} \cong \mathbb{Z}_2$. I am not very confident about the remainder of this argument. The map $\phi$ is smooth since the entries of the matrices $\phi(p,q)$ are polynomials in the $\{1, i, j, k\}$-coordinates of $p$ and $q$ realized as quaternions. Since $S^3 \times S^3$ is connected, we know that $\phi(S^3 \times S^3)$ is contained in the identity component of $O(4)$. That is, $\phi(S^3 \times S^3) \subseteq SO(4)$ (We could have also checked the determinant explicitly, but this is a bit tedious). Since the kernel acts freely and properly on $S^3 \times S^3$, we see that the image of $\phi$ is a smooth six-dimensional manifold. We can count the dimension of $SO(4)$ by considering its Lie algebra of skew-symmetric matrices. It is then easy to see that the dimension of $SO(4)$ is six. It follows that the image is an open submanfiold of $SO(4)$. Note also that $S^3 \times S^3$ is compact, and so is its image. Since $SO(4)$ is Hausdorff, it follows that the image of $\phi$ is also closed in $SO(4)$. Since $SO(4)$ is connected, it follows that the image of $\phi$ is all of $SO(4)$. My questions are as follows: 1) Is there anything wrong with this argument? Even picky corrections are much appreciated. 2) Can you identify any arguments that, while correct, you would prefer to do another way? 3) What more can be said about this map? Is there a common sense reason that these left and right quaternion multiplication actions produce all of $SO(4)$? Can we rephrase this argument in another language which might be more natural? This question is a little bit open ended, but only because I am not sure how to phrase it. The best answer would give me some clue about where this map fits into the general picture, if possible. Perhaps some application of the covering map would be appropriate here. Thanks in advance for any responses.","I am interested in the map $\phi:S^3 \times S^3 \to GL_4(\mathbb{R})$ given as follows: Let $(p,q) \in S^3 \times S^3$. We identify $p$ and $q$ as real quaternions with unit norms and define $\phi(p,q)$ to be the map sending $v \in \mathbb{H}$ to the product $pvq^{-1}$, where $\mathbb{R}^4$ and $\mathbb{H}$ are identified in the obvious way as $\mathbb{R}$-vector spaces. It isn't hard to show that $\phi$ is a group homomorphism. I would like to prove the following in complete detail: 1) The image of $\phi$ is $SO(4)$. 2) The kernel of $\phi$ is $\{(1,1), (-1, -1)\}$ I proceed by noting that the maps $\phi(p,q)$ are clearly $\mathbb{R}$-linear, and since $p$ and $q$ have unit norm, these maps are also easily seen to be orthogonal. Thus, we have $\phi(S^3 \times S^3) \subseteq O(4)$. Now suppose $\phi(p,q)$ is the identity map on $\mathbb{R}^4$. That is, $pvq^{-1}=v\ \ \ \forall v \in \mathbb{H}$. Then for $v=1 \in \mathbb{H}$, we have $pq^{-1}=1$, and so $p=q$. By considering the equations $piq^{-1}=i$, $pjq^{-1}=j$, and $pkq^{-1}=k$, we conclude that $p=q \in \mathbb{R}$. Since $\mathbb{R} \cap S^3 = \{\pm1\}$, an easy compuation shows that the kernel of $\phi$ is $\{(1,1), (-1, -1)\} \cong \mathbb{Z}_2$. I am not very confident about the remainder of this argument. The map $\phi$ is smooth since the entries of the matrices $\phi(p,q)$ are polynomials in the $\{1, i, j, k\}$-coordinates of $p$ and $q$ realized as quaternions. Since $S^3 \times S^3$ is connected, we know that $\phi(S^3 \times S^3)$ is contained in the identity component of $O(4)$. That is, $\phi(S^3 \times S^3) \subseteq SO(4)$ (We could have also checked the determinant explicitly, but this is a bit tedious). Since the kernel acts freely and properly on $S^3 \times S^3$, we see that the image of $\phi$ is a smooth six-dimensional manifold. We can count the dimension of $SO(4)$ by considering its Lie algebra of skew-symmetric matrices. It is then easy to see that the dimension of $SO(4)$ is six. It follows that the image is an open submanfiold of $SO(4)$. Note also that $S^3 \times S^3$ is compact, and so is its image. Since $SO(4)$ is Hausdorff, it follows that the image of $\phi$ is also closed in $SO(4)$. Since $SO(4)$ is connected, it follows that the image of $\phi$ is all of $SO(4)$. My questions are as follows: 1) Is there anything wrong with this argument? Even picky corrections are much appreciated. 2) Can you identify any arguments that, while correct, you would prefer to do another way? 3) What more can be said about this map? Is there a common sense reason that these left and right quaternion multiplication actions produce all of $SO(4)$? Can we rephrase this argument in another language which might be more natural? This question is a little bit open ended, but only because I am not sure how to phrase it. The best answer would give me some clue about where this map fits into the general picture, if possible. Perhaps some application of the covering map would be appropriate here. Thanks in advance for any responses.",,"['abstract-algebra', 'differential-geometry', 'lie-groups', 'quaternions', 'group-actions']"
23,When is the product of $n$ subgroups a subgroup?,When is the product of  subgroups a subgroup?,n,"Let $G$ be any group. It's a well-known result that if $H, K$ are subgroups of $G$, then $HK$ is a subgroup itself if and only if $HK = KH$. Now, I've always wondered about a generalization of this result, something along the lines of: Theorem : If $H_1, \ldots, H_n$ are subgroups of $G$, then $H_1H_2\dots{H_n}$ is a subgroup if and only if ($\star$) holds, where $(\star)$ is some condition on $H_1, \ldots, H_n$, preferably related to how the smaller products $H_{m_1}\ldots{}H_{m_k}$, for $k < n$, behave. Question 1 : Is there such a theorem? I do know, and its easy to prove, that if $H_iH_j = H_jH_i$ for every $i, j$, then the big product is a group, but this is not satisfying since it's far from necessary (just take one of the groups to be $G$, and you need no commutativity at all). Also, I've been told that there is no really satisfactory answer; if that is indeed the case, then my question would be why ? In particular: Question 2 : Are there really problematic counterexamples where you can see that the behavior of the smaller products has nothing to do with the big product, so that no such a theorem can ever exist? I would appreciate even an answer for the particular case $n = 3$. Thanks.","Let $G$ be any group. It's a well-known result that if $H, K$ are subgroups of $G$, then $HK$ is a subgroup itself if and only if $HK = KH$. Now, I've always wondered about a generalization of this result, something along the lines of: Theorem : If $H_1, \ldots, H_n$ are subgroups of $G$, then $H_1H_2\dots{H_n}$ is a subgroup if and only if ($\star$) holds, where $(\star)$ is some condition on $H_1, \ldots, H_n$, preferably related to how the smaller products $H_{m_1}\ldots{}H_{m_k}$, for $k < n$, behave. Question 1 : Is there such a theorem? I do know, and its easy to prove, that if $H_iH_j = H_jH_i$ for every $i, j$, then the big product is a group, but this is not satisfying since it's far from necessary (just take one of the groups to be $G$, and you need no commutativity at all). Also, I've been told that there is no really satisfactory answer; if that is indeed the case, then my question would be why ? In particular: Question 2 : Are there really problematic counterexamples where you can see that the behavior of the smaller products has nothing to do with the big product, so that no such a theorem can ever exist? I would appreciate even an answer for the particular case $n = 3$. Thanks.",,"['abstract-algebra', 'group-theory']"
24,Is there a group theoretic proof that $(\mathbf Z/(p))^\times$ is cyclic?,Is there a group theoretic proof that  is cyclic?,(\mathbf Z/(p))^\times,"Theorem: The group $(\mathbf Z/(p))^\times$ is cyclic for any prime $p$ . Most proofs make use of the fact that for $r\geq 1$ , there are at most $r$ solutions to the equation $x^r=1$ in $\mathbf Z/(p)$ , a result which doesn't seem — understandably — to have any group theoretic proofs. K. Conrad gives ten different proofs — and hints at some others — in his paper here . The first six make use of the previously mentioned fact, while the seventh proof makes extensive use of cyclotomic polynomials and is thus still not group-theoretic. I was also able to find a linear algebra based proof in the second chapter of Teoría Elemental de Grupos by Emilio Bujalance García, but still, no group theoretic proof to be found.","Theorem: The group is cyclic for any prime . Most proofs make use of the fact that for , there are at most solutions to the equation in , a result which doesn't seem — understandably — to have any group theoretic proofs. K. Conrad gives ten different proofs — and hints at some others — in his paper here . The first six make use of the previously mentioned fact, while the seventh proof makes extensive use of cyclotomic polynomials and is thus still not group-theoretic. I was also able to find a linear algebra based proof in the second chapter of Teoría Elemental de Grupos by Emilio Bujalance García, but still, no group theoretic proof to be found.",(\mathbf Z/(p))^\times p r\geq 1 r x^r=1 \mathbf Z/(p),"['abstract-algebra', 'group-theory', 'proof-writing', 'soft-question', 'cyclic-groups']"
25,Can a binary operation have an identity element when it is not associative and commutative? [duplicate],Can a binary operation have an identity element when it is not associative and commutative? [duplicate],,"This question already has answers here : Non-associative, non-commutative binary operation with a identity (8 answers) Closed 2 years ago . I tried getting the answers in similar questions, everyone says that it's not necessary, but if $e$ is the identity element for any binary operation $*$, which is not associative and commutative, how can $$a*e=a=e*a$$ when it is not commutative, i.e. $a*b \ne b*a$? Even if we get a value by solving $a*e=a$. Will we get the same value by solving $e*a=a$ ? Please provide an example.","This question already has answers here : Non-associative, non-commutative binary operation with a identity (8 answers) Closed 2 years ago . I tried getting the answers in similar questions, everyone says that it's not necessary, but if $e$ is the identity element for any binary operation $*$, which is not associative and commutative, how can $$a*e=a=e*a$$ when it is not commutative, i.e. $a*b \ne b*a$? Even if we get a value by solving $a*e=a$. Will we get the same value by solving $e*a=a$ ? Please provide an example.",,"['abstract-algebra', 'binary-operations']"
26,"If $G$ is a group of even order, prove it has an element $a\neq e$ satisfying $a^2=e$. [duplicate]","If  is a group of even order, prove it has an element  satisfying . [duplicate]",G a\neq e a^2=e,"This question already has answers here : Group of even order contains an element of order 2 (2 answers) Closed 7 years ago . If $G$ is a group of even order, prove it has an element $a \neq e$ satisfying $a^2 = e$ . My proof: Let $|G| = 2n$ . Since $G$ is finite, there exists, $a \in G$ such that $a^p = e$ and by Lagrange's Theorem, p divides 2n. By Euclid's lemma, since p does not divide 2, p divides n. Let $n = pk$ . Hence, $(a^n)^2 = (a^{pk})^2 = ((a^p)^k)^2 = (e^k)^2 = e$ . Therefore, $a^n$ is an element that satisfy the condition. Is my solution OK? For this problem, I am just wondering how I can solve this problem without using Lagrange's Theorem, as this problem is an exercise before the Lagrange's Theorem was taught.","This question already has answers here : Group of even order contains an element of order 2 (2 answers) Closed 7 years ago . If is a group of even order, prove it has an element satisfying . My proof: Let . Since is finite, there exists, such that and by Lagrange's Theorem, p divides 2n. By Euclid's lemma, since p does not divide 2, p divides n. Let . Hence, . Therefore, is an element that satisfy the condition. Is my solution OK? For this problem, I am just wondering how I can solve this problem without using Lagrange's Theorem, as this problem is an exercise before the Lagrange's Theorem was taught.",G a \neq e a^2 = e |G| = 2n G a \in G a^p = e n = pk (a^n)^2 = (a^{pk})^2 = ((a^p)^k)^2 = (e^k)^2 = e a^n,"['abstract-algebra', 'group-theory', 'solution-verification', 'finite-groups']"
27,Prove that the additive groups $\mathbb{Z}$ and $\mathbb{Q}$ are not isomorphic.,Prove that the additive groups  and  are not isomorphic.,\mathbb{Z} \mathbb{Q},"Is my proof below correct? What specific property of rationals did I exploit in my proof? It looks like the property I exploited is the following: Given any positive rational, I can always write it as sum of arbitrary number of positive rationals, whereas given any positive integer I cannot write it as a sum of arbitrary number of positive integers. Has it got to do with the fact that $\mathbb{Q}$ is a field? Problem Prove that the additive groups $\mathbb{Z}$ and $\mathbb{Q}$ are not isomorphic. Solution Let there exist an isomorphism between $\mathbb{Z}$ and $\mathbb{Q}$. Now consider the element $1_{\mathbb{Q}}$. We then have $\phi(1_{\mathbb{Q}}) = z \in \mathbb{Z}$. Since $\phi$ has to be a bijection, $z$ cannot be zero, since $\phi(0) = 0$. Now consider the element $\left(\dfrac1{z+1}\right)_{\mathbb{Q}}$. We now have $$z = \phi(1_{\mathbb{Q}}) = \phi\left(\underbrace{\left(\dfrac1{z+1}\right)_{\mathbb{Q}} + \left(\dfrac1{z+1}\right)_{\mathbb{Q}} + \cdots + \left(\dfrac1{z+1}\right)_{\mathbb{Q}}}_{z+1 \text{ times }} \right) = (z+1) \phi\left(\left(\dfrac1{z+1}\right)_{\mathbb{Q}}\right)$$ However, there is no element in $y \in \mathbb{Z}$ such that $(z+1)y = z$. First update Actually I realize that I complicated it unnecessarily. Instead, we can do like this. Since $\phi$ is an isomorphism, we have $\phi(q_{\mathbb{Q}}) = 1_{\mathbb{Z}}$ for some $q \in Q$. However, $$\phi(q) = \phi(q/2+q/2) = 2\phi(q/2)$$ And there is no $y \in Z$, such that $2y=1$. Hence, $\phi(q/2)$ remains unmapped. Thanks","Is my proof below correct? What specific property of rationals did I exploit in my proof? It looks like the property I exploited is the following: Given any positive rational, I can always write it as sum of arbitrary number of positive rationals, whereas given any positive integer I cannot write it as a sum of arbitrary number of positive integers. Has it got to do with the fact that $\mathbb{Q}$ is a field? Problem Prove that the additive groups $\mathbb{Z}$ and $\mathbb{Q}$ are not isomorphic. Solution Let there exist an isomorphism between $\mathbb{Z}$ and $\mathbb{Q}$. Now consider the element $1_{\mathbb{Q}}$. We then have $\phi(1_{\mathbb{Q}}) = z \in \mathbb{Z}$. Since $\phi$ has to be a bijection, $z$ cannot be zero, since $\phi(0) = 0$. Now consider the element $\left(\dfrac1{z+1}\right)_{\mathbb{Q}}$. We now have $$z = \phi(1_{\mathbb{Q}}) = \phi\left(\underbrace{\left(\dfrac1{z+1}\right)_{\mathbb{Q}} + \left(\dfrac1{z+1}\right)_{\mathbb{Q}} + \cdots + \left(\dfrac1{z+1}\right)_{\mathbb{Q}}}_{z+1 \text{ times }} \right) = (z+1) \phi\left(\left(\dfrac1{z+1}\right)_{\mathbb{Q}}\right)$$ However, there is no element in $y \in \mathbb{Z}$ such that $(z+1)y = z$. First update Actually I realize that I complicated it unnecessarily. Instead, we can do like this. Since $\phi$ is an isomorphism, we have $\phi(q_{\mathbb{Q}}) = 1_{\mathbb{Z}}$ for some $q \in Q$. However, $$\phi(q) = \phi(q/2+q/2) = 2\phi(q/2)$$ And there is no $y \in Z$, such that $2y=1$. Hence, $\phi(q/2)$ remains unmapped. Thanks",,"['abstract-algebra', 'group-theory', 'proof-verification', 'group-isomorphism']"
28,Do these “ultraweak” one-sided group axioms guarantee a group?,Do these “ultraweak” one-sided group axioms guarantee a group?,,"This post shows that the “left” group axioms, which only guarantee a left-identity and left-inverses, are sufficient to guarantee that a semigroup is a group. The same idea could be used to show that the “right” group axioms are also sufficient. These sets of axioms might be considered “weak” group axioms, but I am curious whether we can get weaker. Consider the following “ultraweak” axioms: Let $G$ be a set and $*$ be a binary operation on $G$ satisfying: $*$ is associative. There exists an ultraweak identity element $e\in G$ such that for all $x\in G,$ either $e*x = x$ or $x*e=x$ (that is, the “sidedness” of $e$ may differ for each element of $G$ ). For all $x \in G$ there exists an ultraweak inverse $x^{-1}\in G$ such that either $x^{-1} * x = e$ or $x*x^{-1}=e$ (that is, each element of $G$ has at least a one-sided inverse, where the side may differ for each element). Do these axioms guarantee that $(G,*)$ is a group? And if not, how much closer to these axioms can we get, starting from just the “weak” left or right axioms? [For example, maybe assuming an ultraweak identity element with left (or right) inverses is sufficient.] REVISED UPDATE: In the comments to the accepted answer by Vincent, @Yakk asks whether the following condition is sufficient to guarantee a group (assuming associativity of $*$ ): There exists an $e\in G$ such that for all $x\in G$ , either (1) $e*x=x$ and there exists an $x'\in G$ such that $x'*x=e$ , or (2) $x*e=x$ and there exists an $x'\in G$ such that $x*x'=e$ . At first I thought this was true due to the standard ""left identity + left inverses"" and ""right identity + right inverses"" cases applying element by element, but now I realize this reasoning is flawed (these proofs also require the one-sided inverse to have their own one-sided inverse with the same sidedness). So the question remains: Does the above condition, proposed by @Yakk, guarantee a group? Please provide a proof or counterexample. The answer to the revised update is “yes;” see here . There remains a further question about even weaker conditions, where the left and right identities can be different elements. I've asked that here .","This post shows that the “left” group axioms, which only guarantee a left-identity and left-inverses, are sufficient to guarantee that a semigroup is a group. The same idea could be used to show that the “right” group axioms are also sufficient. These sets of axioms might be considered “weak” group axioms, but I am curious whether we can get weaker. Consider the following “ultraweak” axioms: Let be a set and be a binary operation on satisfying: is associative. There exists an ultraweak identity element such that for all either or (that is, the “sidedness” of may differ for each element of ). For all there exists an ultraweak inverse such that either or (that is, each element of has at least a one-sided inverse, where the side may differ for each element). Do these axioms guarantee that is a group? And if not, how much closer to these axioms can we get, starting from just the “weak” left or right axioms? [For example, maybe assuming an ultraweak identity element with left (or right) inverses is sufficient.] REVISED UPDATE: In the comments to the accepted answer by Vincent, @Yakk asks whether the following condition is sufficient to guarantee a group (assuming associativity of ): There exists an such that for all , either (1) and there exists an such that , or (2) and there exists an such that . At first I thought this was true due to the standard ""left identity + left inverses"" and ""right identity + right inverses"" cases applying element by element, but now I realize this reasoning is flawed (these proofs also require the one-sided inverse to have their own one-sided inverse with the same sidedness). So the question remains: Does the above condition, proposed by @Yakk, guarantee a group? Please provide a proof or counterexample. The answer to the revised update is “yes;” see here . There remains a further question about even weaker conditions, where the left and right identities can be different elements. I've asked that here .","G * G * e\in G x\in G, e*x = x x*e=x e G x \in G x^{-1}\in G x^{-1} * x = e x*x^{-1}=e G (G,*) * e\in G x\in G e*x=x x'\in G x'*x=e x*e=x x'\in G x*x'=e","['abstract-algebra', 'group-theory', 'axioms', 'semigroups']"
29,Importance of group action in abstract algebra [closed],Importance of group action in abstract algebra [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question What are the important consequences of group action in abstract algebra? Why the action of a group on a set is defined?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question What are the important consequences of group action in abstract algebra? Why the action of a group on a set is defined?",,"['abstract-algebra', 'group-theory', 'group-actions']"
30,What are well-defined functions? [duplicate],What are well-defined functions? [duplicate],,"This question already has an answer here : ""Well defined"" function - What does it mean? (1 answer) Closed 5 years ago . What exactly makes a function well-defined? I have seen some proofs but they are too hand-wavy and I couldn't understand exactly what a well-defined function is.","This question already has an answer here : ""Well defined"" function - What does it mean? (1 answer) Closed 5 years ago . What exactly makes a function well-defined? I have seen some proofs but they are too hand-wavy and I couldn't understand exactly what a well-defined function is.",,['abstract-algebra']
31,"Structure of groups of order $pq$, where $p,q$ are distinct primes. [duplicate]","Structure of groups of order , where  are distinct primes. [duplicate]","pq p,q","This question already has an answer here : Structure of a group, $G$, of order $pq$ where $p, q$ are prime. [duplicate] (1 answer) Closed 5 years ago . I don't know about the Sylow Theorems. But I have been wondering about a proof of the fact that a group or order $pq$ where $p$ and $q$ are distinct primes must be cyclic. I can't quite work out the details, but here is the general idea. I would like help with filling in details. I assume that it is already known that $G$ has subgroup(s) of order $p$ and subgroup(s) of order $q$. If $G$ is a group of order $pq$ ($p\neq q$), then I know that $G$ has a subgroup $H$ of order $p$ and a subgroup $K$ of order $q$. Then $H\simeq \mathbb{Z}_p$ and $K\simeq \mathbb{Z}_q$. But then $H\oplus K \simeq \mathbb{Z}_{pq}$, so I would think that $H\oplus K \simeq G$. I guess one could do an internal direct product instead of an external direct product, but I don't know that $H$ and $K$ are normal subgroups. I am asking for help completing this argument. Edit: I see from the comments below that I might need to assume that the smaller prime does not divide the larger prime minus $1$. Or maybe it is enough to assume that the primes are greater than or equal to $3$ (Still distinct).","This question already has an answer here : Structure of a group, $G$, of order $pq$ where $p, q$ are prime. [duplicate] (1 answer) Closed 5 years ago . I don't know about the Sylow Theorems. But I have been wondering about a proof of the fact that a group or order $pq$ where $p$ and $q$ are distinct primes must be cyclic. I can't quite work out the details, but here is the general idea. I would like help with filling in details. I assume that it is already known that $G$ has subgroup(s) of order $p$ and subgroup(s) of order $q$. If $G$ is a group of order $pq$ ($p\neq q$), then I know that $G$ has a subgroup $H$ of order $p$ and a subgroup $K$ of order $q$. Then $H\simeq \mathbb{Z}_p$ and $K\simeq \mathbb{Z}_q$. But then $H\oplus K \simeq \mathbb{Z}_{pq}$, so I would think that $H\oplus K \simeq G$. I guess one could do an internal direct product instead of an external direct product, but I don't know that $H$ and $K$ are normal subgroups. I am asking for help completing this argument. Edit: I see from the comments below that I might need to assume that the smaller prime does not divide the larger prime minus $1$. Or maybe it is enough to assume that the primes are greater than or equal to $3$ (Still distinct).",,"['abstract-algebra', 'group-theory', 'direct-product']"
32,Why are higher-degree polynomial equations more difficult to solve?,Why are higher-degree polynomial equations more difficult to solve?,,"I am confused about the significance of the powers on equations. For example, in $ax = b$, intuitively $b$ is a value $x$ multiplied $a$ times. In $ax + b = c$, $c$ is a value $x$ multiplied $a$ times added to by $b$. In $ax^2 + bx + c = d$, $d$ is a value $x$ multiplied by itself and by another value $a$ added to by the $x$ multiplied by a value $b$ and that added to by another value $c$. This and so on. Even in the simple equation $x^2 = y$, $x$ is intuitively $\sqrt{y}$ and so on for arbitrary powers and roots. For the equation $x + x = y$, $x$ is placed doubly and $y$ is equal to that doubled x, so $x$ would be $y$ halved. But when a particular case like $x^2 + x = y$ arises and so on with all orders and polynomials of those orders (""higher order equations""), it cannot be solved so easily and the whole process begins to seem much more foreign and contrived. Why am I no longer able to intuitively solve for the unknown in such an equation when an exponent greater than one is used? Why doesn't the seamless reversing of operations to solve extend to higher order equations?","I am confused about the significance of the powers on equations. For example, in $ax = b$, intuitively $b$ is a value $x$ multiplied $a$ times. In $ax + b = c$, $c$ is a value $x$ multiplied $a$ times added to by $b$. In $ax^2 + bx + c = d$, $d$ is a value $x$ multiplied by itself and by another value $a$ added to by the $x$ multiplied by a value $b$ and that added to by another value $c$. This and so on. Even in the simple equation $x^2 = y$, $x$ is intuitively $\sqrt{y}$ and so on for arbitrary powers and roots. For the equation $x + x = y$, $x$ is placed doubly and $y$ is equal to that doubled x, so $x$ would be $y$ halved. But when a particular case like $x^2 + x = y$ arises and so on with all orders and polynomials of those orders (""higher order equations""), it cannot be solved so easily and the whole process begins to seem much more foreign and contrived. Why am I no longer able to intuitively solve for the unknown in such an equation when an exponent greater than one is used? Why doesn't the seamless reversing of operations to solve extend to higher order equations?",,"['abstract-algebra', 'algebra-precalculus']"
33,Proof that all abelian simple groups are cyclic groups of prime order,Proof that all abelian simple groups are cyclic groups of prime order,,"Just wanted some feedback to ensure I did not make any mistakes with this proof. Thanks! Since $G$ is abelian, every subgroup is normal. Since $G$ is simple, the only subgroups of $G$ are $1$ and $G$, and $|G| > 1$, so for some $x\in G$ we have $x\neq 1$ and $\langle x\rangle\leq G$, so $\langle x\rangle = G$. Suppose $x$ has either infinite order or even order. Then $\langle x^2\rangle \leq G$ but $\langle x^2\rangle \neq \langle x \rangle$, a contradiction. So $x$, and therefore $G$, has odd order, and by Feit-Thompson, $G\cong Z_p$ for some prime $p$. Edit: Thanks, I see that Feit-Thompson is too much. Since $G$ is abelian, every subgroup is normal. Since $G$ is simple, the only subgroups of $G$ are $1$ and $G$, and $|G| > 1$, so for some $x\in G$ we have $x\neq 1$ and $\langle x\rangle\leq G$, so $\langle x\rangle = G$. Suppose $x$ has infinite order. Then $\langle x^2\rangle \leq G$ but $\langle x^2\rangle \neq \langle x \rangle$, a contradiction. So $x$, and therefore $G$, has finite order. Suppose $x$ has composite order $n$ so for some $p > 1$ that divides $n$, $\langle x^p \rangle$ is a proper non-trivial subgroup of $G$, so $G$ is not simple. So $G$ is a cyclic group of prime order.","Just wanted some feedback to ensure I did not make any mistakes with this proof. Thanks! Since $G$ is abelian, every subgroup is normal. Since $G$ is simple, the only subgroups of $G$ are $1$ and $G$, and $|G| > 1$, so for some $x\in G$ we have $x\neq 1$ and $\langle x\rangle\leq G$, so $\langle x\rangle = G$. Suppose $x$ has either infinite order or even order. Then $\langle x^2\rangle \leq G$ but $\langle x^2\rangle \neq \langle x \rangle$, a contradiction. So $x$, and therefore $G$, has odd order, and by Feit-Thompson, $G\cong Z_p$ for some prime $p$. Edit: Thanks, I see that Feit-Thompson is too much. Since $G$ is abelian, every subgroup is normal. Since $G$ is simple, the only subgroups of $G$ are $1$ and $G$, and $|G| > 1$, so for some $x\in G$ we have $x\neq 1$ and $\langle x\rangle\leq G$, so $\langle x\rangle = G$. Suppose $x$ has infinite order. Then $\langle x^2\rangle \leq G$ but $\langle x^2\rangle \neq \langle x \rangle$, a contradiction. So $x$, and therefore $G$, has finite order. Suppose $x$ has composite order $n$ so for some $p > 1$ that divides $n$, $\langle x^p \rangle$ is a proper non-trivial subgroup of $G$, so $G$ is not simple. So $G$ is a cyclic group of prime order.",,"['abstract-algebra', 'group-theory', 'abelian-groups', 'cyclic-groups', 'simple-groups']"
34,counting the number of elements in a conjugacy class of $S_n$,counting the number of elements in a conjugacy class of,S_n,"I want to know if there is some systematic way (using some combinatorial argument) to find the number of elements of conjugacy classes of $S_n$ for some given $n$. For example, let's consider $S_5$. If the representative for the conjugacy class is an $m$-cycle then Dummit and Foote gives a formula on how to compute the number of elements in the conjugacy class. This is not a problem. But what about when the representative is not an $m$-cycle. As an example we can consider the conjugacy class that gives rise by the partition $2+3$ of $5$. A representative for the conjugacy class would be $(1 2)(3 4 5)$. How can I find the number of such elements?. Question? : Does $ {5\choose 2}\cdot { 3 \choose 3}\cdot 2$ give me what I want? Reasoning : For the first parenthesis I need to choose $2$ elements out of $5$ and for the second set of parenthesis I need to choose $3$ out of the remaining $3$ (noting that they can't be repeats). Finally we can permute these two parenthesis in two ways, thus giving me the above number. Is this reasoning correct?. If not how does one find the number of elements of such conjugacy classes. As always, any help is greatly appreciated.","I want to know if there is some systematic way (using some combinatorial argument) to find the number of elements of conjugacy classes of $S_n$ for some given $n$. For example, let's consider $S_5$. If the representative for the conjugacy class is an $m$-cycle then Dummit and Foote gives a formula on how to compute the number of elements in the conjugacy class. This is not a problem. But what about when the representative is not an $m$-cycle. As an example we can consider the conjugacy class that gives rise by the partition $2+3$ of $5$. A representative for the conjugacy class would be $(1 2)(3 4 5)$. How can I find the number of such elements?. Question? : Does $ {5\choose 2}\cdot { 3 \choose 3}\cdot 2$ give me what I want? Reasoning : For the first parenthesis I need to choose $2$ elements out of $5$ and for the second set of parenthesis I need to choose $3$ out of the remaining $3$ (noting that they can't be repeats). Finally we can permute these two parenthesis in two ways, thus giving me the above number. Is this reasoning correct?. If not how does one find the number of elements of such conjugacy classes. As always, any help is greatly appreciated.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'symmetric-groups']"
35,Looking for a clear definition of the geometric product,Looking for a clear definition of the geometric product,,"In brief: I'm looking for a clearly-worded definition 1 of the geometric product of two arbitrary multivectors in $\mathbb{G}^n$ . I'm having a hard time getting my bearings in the world of ""geometric algebra"", even though I'm using as my guide an introductory undergraduate-level 2 book ( Linear and  geometric algebra by Macdonald). Among the general problems that I'm running into is that most definitions and theorems that I find (either in this book, or online ) seem to apply to some multivectors (e.g. to $k$ -vectors, or to blades), not all.  Sometimes it is not clear to me whether a definition or result refers to all multivectors in $\mathbb{G}^n$ or only to a distinguished subset (e.g. blades), since these definitions/theorems are expressed in terms the word ""vector"".  This leads to the pervading doubt as to whether this word ""vector"" is being meant as synonymous with ""multivector""—i.e. an object in the so-called "" vector space $\mathbb{G}^n\;$ "")—, or with "" $1$ -vector"", or with "" $k$ -vector"", or something else entirely. (Hence the specification ""clearly-worded"" in my question above.  A more accurate specification would have been ""unambiguously-worded"", but it would have been puzzling on first encounter.) Case in point is the definition of the geometric product in $\mathbb{G}^n$ .  Macdonald gives a very partial definition of this product for ""vectors"" (and only in $\mathbb{G}^3$ ) 3 , but far as I can tell Macdonald never defines this product in general, even though he uses it freely throughout much of the book! I find this astonishing, to put it mildly.  But, please correct me if I'm wrong. 1 In his answer below Alan Macdonald writes ""I do not think it possible to give a quick definition of the general geometric product.""  In light of this remark, I want to stress that succinctness is not among the requirements in my specifications what I'm looking for. 2 The original version of this post incorrectly described this book as being written for ""high-school students"", but the author pointed out this error in his answer below .  I apologize for the (now-amended) inaccuracy. 3 On p. 82, Macdonald gives a definition for the geometric product of two $1$ -vectors in $\mathbb{G}^3$ , and later explicitly states: ""We have defined the geometric product of two vectors, but not for example, the geometric product of a vector and a bivector.  This will be taken up in the next chapter, where we will learn to take the geometric product of any two multivectors.""  As far as I can tell, however, the ""next chapter"", which is called simply $\mathbb{G}^n$ , never fulfills this promise.  Or at least, it never gives a definition for the geometric product of any two multivectors in $\mathbb{G}^n$ .","In brief: I'm looking for a clearly-worded definition 1 of the geometric product of two arbitrary multivectors in . I'm having a hard time getting my bearings in the world of ""geometric algebra"", even though I'm using as my guide an introductory undergraduate-level 2 book ( Linear and  geometric algebra by Macdonald). Among the general problems that I'm running into is that most definitions and theorems that I find (either in this book, or online ) seem to apply to some multivectors (e.g. to -vectors, or to blades), not all.  Sometimes it is not clear to me whether a definition or result refers to all multivectors in or only to a distinguished subset (e.g. blades), since these definitions/theorems are expressed in terms the word ""vector"".  This leads to the pervading doubt as to whether this word ""vector"" is being meant as synonymous with ""multivector""—i.e. an object in the so-called "" vector space "")—, or with "" -vector"", or with "" -vector"", or something else entirely. (Hence the specification ""clearly-worded"" in my question above.  A more accurate specification would have been ""unambiguously-worded"", but it would have been puzzling on first encounter.) Case in point is the definition of the geometric product in .  Macdonald gives a very partial definition of this product for ""vectors"" (and only in ) 3 , but far as I can tell Macdonald never defines this product in general, even though he uses it freely throughout much of the book! I find this astonishing, to put it mildly.  But, please correct me if I'm wrong. 1 In his answer below Alan Macdonald writes ""I do not think it possible to give a quick definition of the general geometric product.""  In light of this remark, I want to stress that succinctness is not among the requirements in my specifications what I'm looking for. 2 The original version of this post incorrectly described this book as being written for ""high-school students"", but the author pointed out this error in his answer below .  I apologize for the (now-amended) inaccuracy. 3 On p. 82, Macdonald gives a definition for the geometric product of two -vectors in , and later explicitly states: ""We have defined the geometric product of two vectors, but not for example, the geometric product of a vector and a bivector.  This will be taken up in the next chapter, where we will learn to take the geometric product of any two multivectors.""  As far as I can tell, however, the ""next chapter"", which is called simply , never fulfills this promise.  Or at least, it never gives a definition for the geometric product of any two multivectors in .",\mathbb{G}^n k \mathbb{G}^n \mathbb{G}^n\; 1 k \mathbb{G}^n \mathbb{G}^3 1 \mathbb{G}^3 \mathbb{G}^n \mathbb{G}^n,"['abstract-algebra', 'clifford-algebras', 'geometric-algebras']"
36,Is there a special name for a field where each number has a square root?,Is there a special name for a field where each number has a square root?,,"For example, not every number in the field of rational numbers with ordinary addition and multiplication has square root. Is there a special name for a field where each number has a square root? Can anyone help provide some reference? Thank you!","For example, not every number in the field of rational numbers with ordinary addition and multiplication has square root. Is there a special name for a field where each number has a square root? Can anyone help provide some reference? Thank you!",,"['abstract-algebra', 'field-theory', 'terminology']"
37,Choosing an advanced group theory text: concerns,Choosing an advanced group theory text: concerns,,"In this question, An Introduction to the Theory of Groups by Rotman is recommended twice as a good second-course group theory text.  However, after reading the reviews here , and seeing this pdf of what seems to be corrections to Rotman, I am pretty concerned about the apparently many errors in the text.  While some errors and their corrections may be pretty self-evident, I would hate to constantly be worrying about developing some misconception as a result of some fact that isn't presented quite as it should be. To those who have read An Introduction to the Theory of Groups by Rotman: do you feel the errors are a serious concern/disruption to the flow of the text, or is the book still pretty navigable? Also recommended in the question linked to above was A Course in the Theory of Groups by Robinson.  I really like the look of this text, based on the table of contents and the group-theoretic concepts that really interest me. The Robinson text is said (in the linked-to question) to 'move pretty quickly into deeper waters.'  My background consists of basic group theory, ring theory, field theory, a little Galois theory, linear algebra, topology, real analysis, and graph theory (I have seen a decent amount of material and have been self-studying for a few years now). Do you think that I can reasonably wade into these deeper waters without going in over my head and drowning?  I realize this is a little subjective since no one knows my exact abilities, but I love group theory and am definitely willing to put some real effort into it. Thanks in advance for the advice.","In this question, An Introduction to the Theory of Groups by Rotman is recommended twice as a good second-course group theory text.  However, after reading the reviews here , and seeing this pdf of what seems to be corrections to Rotman, I am pretty concerned about the apparently many errors in the text.  While some errors and their corrections may be pretty self-evident, I would hate to constantly be worrying about developing some misconception as a result of some fact that isn't presented quite as it should be. To those who have read An Introduction to the Theory of Groups by Rotman: do you feel the errors are a serious concern/disruption to the flow of the text, or is the book still pretty navigable? Also recommended in the question linked to above was A Course in the Theory of Groups by Robinson.  I really like the look of this text, based on the table of contents and the group-theoretic concepts that really interest me. The Robinson text is said (in the linked-to question) to 'move pretty quickly into deeper waters.'  My background consists of basic group theory, ring theory, field theory, a little Galois theory, linear algebra, topology, real analysis, and graph theory (I have seen a decent amount of material and have been self-studying for a few years now). Do you think that I can reasonably wade into these deeper waters without going in over my head and drowning?  I realize this is a little subjective since no one knows my exact abilities, but I love group theory and am definitely willing to put some real effort into it. Thanks in advance for the advice.",,"['abstract-algebra', 'group-theory', 'reference-request', 'soft-question', 'finite-groups']"
38,$(12)$ and $(123\dots n)$ are generators of $S_n$,and  are generators of,(12) (123\dots n) S_n,"Show that $S_n$ is generated by the set $ \{ (12),(123\dots n) \} $. I think I can see why this is true. My general plan is (1) to show that by applying various combinations of these two cycles you can get each transposition, and then (2) to show that each cycle is a product of transpositions. I'm just having trouble on the first step. Any ideas?","Show that $S_n$ is generated by the set $ \{ (12),(123\dots n) \} $. I think I can see why this is true. My general plan is (1) to show that by applying various combinations of these two cycles you can get each transposition, and then (2) to show that each cycle is a product of transpositions. I'm just having trouble on the first step. Any ideas?",,"['abstract-algebra', 'group-theory', 'permutations', 'finite-groups', 'symmetric-groups']"
39,"Proving $2,3,1+\sqrt{-5}$ and $1-\sqrt{-5}$ are irreducible in $\mathbb{Z}[\sqrt{-5}]$",Proving  and  are irreducible in,"2,3,1+\sqrt{-5} 1-\sqrt{-5} \mathbb{Z}[\sqrt{-5}]","Could anyone help me prove that $2,3,1+\sqrt{-5}$ and $1-\sqrt{-5}$ are irreducible in $\mathbb{Z}[\sqrt{-5}]$? As $6=2*3=(1+\sqrt{-5})(1-\sqrt{-5})$ so $\mathbb{Z}[\sqrt{-5}]$ is not a UFD. Therefore is not a PID or euclidean domain","Could anyone help me prove that $2,3,1+\sqrt{-5}$ and $1-\sqrt{-5}$ are irreducible in $\mathbb{Z}[\sqrt{-5}]$? As $6=2*3=(1+\sqrt{-5})(1-\sqrt{-5})$ so $\mathbb{Z}[\sqrt{-5}]$ is not a UFD. Therefore is not a PID or euclidean domain",,"['abstract-algebra', 'ring-theory']"
40,Proving that a subgroup of a finitely generated abelian group is finitely generated,Proving that a subgroup of a finitely generated abelian group is finitely generated,,"A question says: Using the isomorphism theorems or otherwise, prove that a subgroup of a finitely generated abelian group is finitely generated. I would say that for a finitely generated abelian group $G$, there exists elements $g_1,\dots, g_n$ such that a linear combination of them generates the whole group. Therefore as every element of a subgroup has an element in $G$ and so can be made by a linear combination of $g_1,\dots, g_n$. This means that $g_1,\dots, g_n$ span the whole subgroup and so there exists a subset of $g_1,\dots, g_n$ which generates the subgroup. This answer seems far too 'linear algebra-ish' rather than 'group theory-ish' and I can't seem to see how one would use the isomorphism theorems? Help would be appreciated!","A question says: Using the isomorphism theorems or otherwise, prove that a subgroup of a finitely generated abelian group is finitely generated. I would say that for a finitely generated abelian group $G$, there exists elements $g_1,\dots, g_n$ such that a linear combination of them generates the whole group. Therefore as every element of a subgroup has an element in $G$ and so can be made by a linear combination of $g_1,\dots, g_n$. This means that $g_1,\dots, g_n$ span the whole subgroup and so there exists a subset of $g_1,\dots, g_n$ which generates the subgroup. This answer seems far too 'linear algebra-ish' rather than 'group theory-ish' and I can't seem to see how one would use the isomorphism theorems? Help would be appreciated!",,['abstract-algebra']
41,What fields between the rationals and the reals allow to define the usual 2D distance?,What fields between the rationals and the reals allow to define the usual 2D distance?,,"Consider a field $K$ , let's say $K \subseteq \mathbb R$ . We can consider the 'plane' $K \times K$ . I am wondering in which cases the distance function $d: K \times K \to \mathbb R$ , defined as is normal by $d(x, y) = \sqrt{x^2 + y^2}$ , takes values in $K$ . Certainly this is not true for $\mathbb Q$ : we have $d(1, 1) = \sqrt{2} \notin \mathbb Q$ . If we take any $K$ which is closed under taking square roots of non-negative numbers, then certainly $d$ will take values in $K$ . However, a priori it might still be true that $a \in K$ positive has no square root, yet this does not provide an obstruction because there is no way to write $a = x^2 + y^2$ . Thus I am wondering: Are there fields $K \subseteq \mathbb R$ which do not have all square roots of positive numbers, yet are closed under $d$ ?","Consider a field , let's say . We can consider the 'plane' . I am wondering in which cases the distance function , defined as is normal by , takes values in . Certainly this is not true for : we have . If we take any which is closed under taking square roots of non-negative numbers, then certainly will take values in . However, a priori it might still be true that positive has no square root, yet this does not provide an obstruction because there is no way to write . Thus I am wondering: Are there fields which do not have all square roots of positive numbers, yet are closed under ?","K K \subseteq \mathbb R K \times K d: K \times K \to \mathbb R d(x, y) = \sqrt{x^2 + y^2} K \mathbb Q d(1, 1) = \sqrt{2} \notin \mathbb Q K d K a \in K a = x^2 + y^2 K \subseteq \mathbb R d","['abstract-algebra', 'field-theory']"
42,Do extension fields always belong to a bigger field?,Do extension fields always belong to a bigger field?,,"Let $F$ be a field, $E_1$ and $E_2$ are two distinct extension fields of $F$. Is it the case that we can always somehow find a field $G$ that contains both $E_1$ and $E_2$? In other words, could extensions of fields have different 'direction's such that they are incompatible? Edit: I began to think about this problem while reading a proof. $F$ is a field. $a$ and $b$ are algebraic over $F$. $p(x)$ and $q(x)$ are two polynomials in $F[x]$ of minimum degree that respectively make $a$ and $b$ a zero. The proof claims that there is an extension $K$ of $F$ such that all distinct zeros of $p(x)$ and $q(x)$ lie in $K$. For a single polynomial, I know this kind of field exists because of the existence of splitting field, why it is true for two polynomials?","Let $F$ be a field, $E_1$ and $E_2$ are two distinct extension fields of $F$. Is it the case that we can always somehow find a field $G$ that contains both $E_1$ and $E_2$? In other words, could extensions of fields have different 'direction's such that they are incompatible? Edit: I began to think about this problem while reading a proof. $F$ is a field. $a$ and $b$ are algebraic over $F$. $p(x)$ and $q(x)$ are two polynomials in $F[x]$ of minimum degree that respectively make $a$ and $b$ a zero. The proof claims that there is an extension $K$ of $F$ such that all distinct zeros of $p(x)$ and $q(x)$ lie in $K$. For a single polynomial, I know this kind of field exists because of the existence of splitting field, why it is true for two polynomials?",,"['abstract-algebra', 'field-theory', 'extension-field']"
43,Why $E\otimes_KE\cong EG$ implies that Galois theory works?,Why  implies that Galois theory works?,E\otimes_KE\cong EG,"I am reading the book Algebra, volume 1: Fields and Galois theory by Falko Lorenz. This is a part of statement in the book I do not fully appreciate. Suppose $E/K$ is Galois extension and $G$ the Galois group of $E/K$ . Let $EG$ be the group algebra of the finite group $G$ , considered as a $G$ -module (not as a ring). ""It is worthwhile remarking that $E\otimes_KE\cong EG$ can be viewed as a deep reason why Galois theory works."" Q: What is the implication above? I though $E\otimes_KE\cong EG$ 's proof has a major ingredient that the trace map is non degenerate.(i.e $E/K$ is separable.) Is this affording some representation of $G\to Aut_K(E)$ ? What is the author trying to express?","I am reading the book Algebra, volume 1: Fields and Galois theory by Falko Lorenz. This is a part of statement in the book I do not fully appreciate. Suppose is Galois extension and the Galois group of . Let be the group algebra of the finite group , considered as a -module (not as a ring). ""It is worthwhile remarking that can be viewed as a deep reason why Galois theory works."" Q: What is the implication above? I though 's proof has a major ingredient that the trace map is non degenerate.(i.e is separable.) Is this affording some representation of ? What is the author trying to express?",E/K G E/K EG G G E\otimes_KE\cong EG E\otimes_KE\cong EG E/K G\to Aut_K(E),"['abstract-algebra', 'galois-theory']"
44,"Prove that both $x+y$ and $xy$ are rational, under some conditions","Prove that both  and  are rational, under some conditions",x+y xy,"As a result of the answer I got for this question - Irrational solutions to some equations in two variables - I was wondering if the next statement is always true: Let $x,y$ be real, irrational numbers such that $x+y\ne0$.  And let $n_1,n_2,n_3$ be some positive integers (different from each other) such that $\gcd(n_1,n_2,n_3)=1$. Prove (or find a counter example) that if: $$x^{n_1}+y^{n_1}$$ $$x^{n_2}+y^{n_2}$$ $$x^{n_3}+y^{n_3}$$ are all rational numbers, then also both: $$x+y$$  $$xy$$ have to be rational numbers.","As a result of the answer I got for this question - Irrational solutions to some equations in two variables - I was wondering if the next statement is always true: Let $x,y$ be real, irrational numbers such that $x+y\ne0$.  And let $n_1,n_2,n_3$ be some positive integers (different from each other) such that $\gcd(n_1,n_2,n_3)=1$. Prove (or find a counter example) that if: $$x^{n_1}+y^{n_1}$$ $$x^{n_2}+y^{n_2}$$ $$x^{n_3}+y^{n_3}$$ are all rational numbers, then also both: $$x+y$$  $$xy$$ have to be rational numbers.",,"['abstract-algebra', 'number-theory', 'field-theory']"
45,How much do idempotent ultrafilters generate in terms of semigroups?,How much do idempotent ultrafilters generate in terms of semigroups?,,"It is known that the set of ultrafilters on, say, the natural numbers $\mathbb{N}$, can naturally be endowed with the structure of a compact topological left semigroup (which fails to be anything nicer in quite a spectacular fashion, e.g. it is badly uncommutative and right-discontinuous). An very nice class of ultrafilters are the idempotent ones, i.e. such that $p + p = p$. That they exist is already non-trivial ( Ellis theorem ) and useful (an extremely elegant proof of Hindman's theorem ). Since the situation is non-commutative, it cannot be hoped that if $p$ and $q$ are idempotent, then $p+q$ is again idempotent. However, there are a number of properties that are (1) interesting (in my humble opinion), (2) true of idempotents (3) preserved under sums. Hence, the following question feels natural: How large is the family of ultrafilters generated by the idempotents by taking sums? What if you allow limits as well? Is it in any significant way larger than the idempotents themselves? What if instead of idempotents, one looks at other special ultrafilters? Edit: Given that the question got a few upvotes, but no answers, I thought I should mention what I know about the problem. A first motivation, and a partial (negative) result is the following. Suppose $f : \mathbb{N} \to \mathbb{N}$ is a map. For $\alpha \in \mathbb{T} = \mathbb{R}/\mathbb{Z}$, we can consider the sequence $(f(n)\alpha)_{n\in \mathbb{N}}$. For any ultrafilter $p$ there is then a well defined notion of the generalised limit (with respect to this ultrafilter) given by the rule that: $p\!-\!\lim_n f(n) \alpha = \gamma$ if and only if for any open neighbourhood $U \subset \mathbb{T}$ of $\gamma$ we have $\{ n \ : \  f(n) \alpha \in U \} \in p$ (so, $f(n) \alpha$ is close to $\gamma$ for $p$-many $n$). It turns out that if $p$ is an idempotent and $f$ is a polynomial with $f(0) = 0$, then $p\!-\!\lim_n f(n) \alpha = 0$. (It is closely related to the other question asked here ). However, this is not quite the end of the story. It is true that $$(p+q)\!-\!\lim_k f(k) \alpha = p\!-\!\lim_n q\!-\!\lim_m f(n+m) \alpha$$ which can be used to conclude that if the claim holds for two ultrafilters $p,q$, then it also holds for their sum. What is even more, the map $p \mapsto  p\!-\!\lim_n f(n) \alpha$ is continuous. The bottom line is that the mentioned fact can be generalised from the idempotents to everything one can generate by taking sums and closures. There are the following two consequences. On one hand, if we knew something about how large is the closed sub-semigroup generated by the idempotents, there would be a result that would follow immediately. On the other hand, this shows that we surely can't generate everything, because there are ultrafilters $p$ with $p\!-\!\lim_n \alpha n \neq 0$ for any fixed $\alpha$. I would be grateful for any of the following: another reason why the idempotents don't generate all ultrafilters. any description of how much is it that the idempotents generate. answer to whether there exist ultrafilters for which it holds that $p\!-\!\lim_n f(n) \alpha = 0$ under the assumptions above, but $p$ is not generated by the idempotents. Edit: It turns out that the ultrafilters generated by the idempotents (allowing sums and limits) are still rather small fraction of the full space. In particular, if $A$ is the set of all such ultrafilters, then it takes at least $2^{\mathfrak{c}} = \# \beta \mathbb{Z}$ translates of $A$ (i.e. sets like $A+p$, $p+A$, or even $p+A+q$...). The argument is not difficult, but I am not sure if it makes sense to present it here. Edit: This has been cross-posted to MathOverflow .","It is known that the set of ultrafilters on, say, the natural numbers $\mathbb{N}$, can naturally be endowed with the structure of a compact topological left semigroup (which fails to be anything nicer in quite a spectacular fashion, e.g. it is badly uncommutative and right-discontinuous). An very nice class of ultrafilters are the idempotent ones, i.e. such that $p + p = p$. That they exist is already non-trivial ( Ellis theorem ) and useful (an extremely elegant proof of Hindman's theorem ). Since the situation is non-commutative, it cannot be hoped that if $p$ and $q$ are idempotent, then $p+q$ is again idempotent. However, there are a number of properties that are (1) interesting (in my humble opinion), (2) true of idempotents (3) preserved under sums. Hence, the following question feels natural: How large is the family of ultrafilters generated by the idempotents by taking sums? What if you allow limits as well? Is it in any significant way larger than the idempotents themselves? What if instead of idempotents, one looks at other special ultrafilters? Edit: Given that the question got a few upvotes, but no answers, I thought I should mention what I know about the problem. A first motivation, and a partial (negative) result is the following. Suppose $f : \mathbb{N} \to \mathbb{N}$ is a map. For $\alpha \in \mathbb{T} = \mathbb{R}/\mathbb{Z}$, we can consider the sequence $(f(n)\alpha)_{n\in \mathbb{N}}$. For any ultrafilter $p$ there is then a well defined notion of the generalised limit (with respect to this ultrafilter) given by the rule that: $p\!-\!\lim_n f(n) \alpha = \gamma$ if and only if for any open neighbourhood $U \subset \mathbb{T}$ of $\gamma$ we have $\{ n \ : \  f(n) \alpha \in U \} \in p$ (so, $f(n) \alpha$ is close to $\gamma$ for $p$-many $n$). It turns out that if $p$ is an idempotent and $f$ is a polynomial with $f(0) = 0$, then $p\!-\!\lim_n f(n) \alpha = 0$. (It is closely related to the other question asked here ). However, this is not quite the end of the story. It is true that $$(p+q)\!-\!\lim_k f(k) \alpha = p\!-\!\lim_n q\!-\!\lim_m f(n+m) \alpha$$ which can be used to conclude that if the claim holds for two ultrafilters $p,q$, then it also holds for their sum. What is even more, the map $p \mapsto  p\!-\!\lim_n f(n) \alpha$ is continuous. The bottom line is that the mentioned fact can be generalised from the idempotents to everything one can generate by taking sums and closures. There are the following two consequences. On one hand, if we knew something about how large is the closed sub-semigroup generated by the idempotents, there would be a result that would follow immediately. On the other hand, this shows that we surely can't generate everything, because there are ultrafilters $p$ with $p\!-\!\lim_n \alpha n \neq 0$ for any fixed $\alpha$. I would be grateful for any of the following: another reason why the idempotents don't generate all ultrafilters. any description of how much is it that the idempotents generate. answer to whether there exist ultrafilters for which it holds that $p\!-\!\lim_n f(n) \alpha = 0$ under the assumptions above, but $p$ is not generated by the idempotents. Edit: It turns out that the ultrafilters generated by the idempotents (allowing sums and limits) are still rather small fraction of the full space. In particular, if $A$ is the set of all such ultrafilters, then it takes at least $2^{\mathfrak{c}} = \# \beta \mathbb{Z}$ translates of $A$ (i.e. sets like $A+p$, $p+A$, or even $p+A+q$...). The argument is not difficult, but I am not sure if it makes sense to present it here. Edit: This has been cross-posted to MathOverflow .",,"['abstract-algebra', 'general-topology', 'semigroups', 'filters']"
46,Proving that $\mathbb{Z}[x]$ and $\mathbb{Q}[x]$ are not isomorphic.,Proving that  and  are not isomorphic.,\mathbb{Z}[x] \mathbb{Q}[x],"I am trying to prove that $\mathbb{Z}[x]$ and $\mathbb{Q}[x]$ are not isomorphic. I was thinking of arguing the following: Suppose there exists an isomorphism $\varphi: \mathbb{Z}[x]\rightarrow\mathbb{Q}[x]$. Because isomorphisms are by definition surjective, there exist $x, y \in\mathbb{Z}[x]$ such that $\varphi(x) = c \in \mathbb{Q}[x]$ and $\varphi(y) = d \in \mathbb{Q}[x]$ for any $c, d\in\mathbb{Q}[x]$. Because $\varphi$ is an isomorphism we must have $\varphi(x+y) = \varphi(x) + \varphi(y)$ for all $x, y \in \mathbb{Z}[x]$. Namely, because polynomial addition is defined componentwise, we must have that the constant term of $\varphi(a + b) = c_{0} + d_{0}$ (where $c_{0}, d_{0}$ are the constant terms of $c$ and $d$ respectively. I would then argue that because $\mathbb{Z}$ and $\mathbb{Q}$ are not isomorphic as additive groups, no such isomorphism $\varphi$ exists. Is this a valid proof? I've seen proofs that argue that because $\varphi(1) = 1$ for any homomorphism we have $1 = \varphi(2(1/2)) = 2(\varphi(1/2))$ so $\varphi(1/2)$ must be contained in $\mathbb{Z}[x]^{\times}$. Then because $\mathbb{Z}[x]^{\times} = \mathbb{Z}^{\times} = \{\pm1\}$ we have $2 \times \pm1 \neq1$, a contradiction. Is this any different than arguing that $\mathbb{Z}[x]$ and $\mathbb{Q}[x]$ each have a different number of units?","I am trying to prove that $\mathbb{Z}[x]$ and $\mathbb{Q}[x]$ are not isomorphic. I was thinking of arguing the following: Suppose there exists an isomorphism $\varphi: \mathbb{Z}[x]\rightarrow\mathbb{Q}[x]$. Because isomorphisms are by definition surjective, there exist $x, y \in\mathbb{Z}[x]$ such that $\varphi(x) = c \in \mathbb{Q}[x]$ and $\varphi(y) = d \in \mathbb{Q}[x]$ for any $c, d\in\mathbb{Q}[x]$. Because $\varphi$ is an isomorphism we must have $\varphi(x+y) = \varphi(x) + \varphi(y)$ for all $x, y \in \mathbb{Z}[x]$. Namely, because polynomial addition is defined componentwise, we must have that the constant term of $\varphi(a + b) = c_{0} + d_{0}$ (where $c_{0}, d_{0}$ are the constant terms of $c$ and $d$ respectively. I would then argue that because $\mathbb{Z}$ and $\mathbb{Q}$ are not isomorphic as additive groups, no such isomorphism $\varphi$ exists. Is this a valid proof? I've seen proofs that argue that because $\varphi(1) = 1$ for any homomorphism we have $1 = \varphi(2(1/2)) = 2(\varphi(1/2))$ so $\varphi(1/2)$ must be contained in $\mathbb{Z}[x]^{\times}$. Then because $\mathbb{Z}[x]^{\times} = \mathbb{Z}^{\times} = \{\pm1\}$ we have $2 \times \pm1 \neq1$, a contradiction. Is this any different than arguing that $\mathbb{Z}[x]$ and $\mathbb{Q}[x]$ each have a different number of units?",,"['abstract-algebra', 'ring-theory']"
47,Is the ideal generated by an irreducible polynomial prime?,Is the ideal generated by an irreducible polynomial prime?,,"$R$ is a commutative ring. $p(x)$ is an irreducible polynomial of $R[x]$. Is the ideal $(p(x))$ generated by $p(x)$ in $R[x]$ prime? If not, under what conditions of $R$ is $(p(x))$ prime? How about maximal?","$R$ is a commutative ring. $p(x)$ is an irreducible polynomial of $R[x]$. Is the ideal $(p(x))$ generated by $p(x)$ in $R[x]$ prime? If not, under what conditions of $R$ is $(p(x))$ prime? How about maximal?",,['abstract-algebra']
48,Algebraic Structures that do not respect isomorphism,Algebraic Structures that do not respect isomorphism,,"One of the first things a student learn in Algebra is isomorphism, and it seems many objects in algebra are defined up to isomorphism. It then comes as a mild shock (at least to me) that quotient groups do not respect isomorphism, in the sense that if $G$ is a group, and $H$ and $K$ are isomorphic normal subgroups, $G/H$ and $G/K$ may not be isomorphic. (see Isomorphic quotient groups ) My two questions are: 1) What other algebraic ""structures"" or ""operations"" do not respect isomorphism? 2) Philosophical (or heuristically), why are there algebraic structures that do not respect isomorphism? Is this supposed to be surprising or not surprising? To me $G/H$ not isomorphic to $G/K$, even though I understand the counterexample, is as surprising as $\frac{2}{1/2}\neq\frac{2}{0.5}$. Thanks for any help!","One of the first things a student learn in Algebra is isomorphism, and it seems many objects in algebra are defined up to isomorphism. It then comes as a mild shock (at least to me) that quotient groups do not respect isomorphism, in the sense that if $G$ is a group, and $H$ and $K$ are isomorphic normal subgroups, $G/H$ and $G/K$ may not be isomorphic. (see Isomorphic quotient groups ) My two questions are: 1) What other algebraic ""structures"" or ""operations"" do not respect isomorphism? 2) Philosophical (or heuristically), why are there algebraic structures that do not respect isomorphism? Is this supposed to be surprising or not surprising? To me $G/H$ not isomorphic to $G/K$, even though I understand the counterexample, is as surprising as $\frac{2}{1/2}\neq\frac{2}{0.5}$. Thanks for any help!",,"['abstract-algebra', 'group-theory', 'soft-question']"
49,Can someone explain ideals to me? [duplicate],Can someone explain ideals to me? [duplicate],,"This question already has answers here : Intuition behind ""ideal"" (9 answers) Closed 2 years ago . I'm struggling with the idea of ideals (both the definitions and the common notation). I'm in a basic collegiate algebra course, just looking for a bit of help. As simply defined as possible, if you could. thanks!","This question already has answers here : Intuition behind ""ideal"" (9 answers) Closed 2 years ago . I'm struggling with the idea of ideals (both the definitions and the common notation). I'm in a basic collegiate algebra course, just looking for a bit of help. As simply defined as possible, if you could. thanks!",,"['abstract-algebra', 'ring-theory', 'ideals']"
50,Is a semigroup $G$ with left identity and right inverses a group?,Is a semigroup  with left identity and right inverses a group?,G,"Hungerford's Algebra poses the question: Is it true that a semigroup $G$ that has a left identity element and in which every element has a right inverse is a group? Now, If both the identity and the inverse are of the same side, this is simple. For, instead of the above, say every element has a left inverse. For $a \in G$ denote this left inverse by $a^{-1}$. Then $$(aa^{-1})(aa^{-1}) = a(a^{-1}a)a^{-1} = aa^{-1}$$ and we can use the fact that $$cc = c \Longrightarrow c = 1$$ to get that inverses are in fact two-sided: $$ aa^{-1} = 1$$ From which it follows that $$a = 1 \cdot a = (aa^{-1})a = a (a^{-1}a) = a \cdot 1$$ as desired. But in the scenario given we cannot use $cc = c \Longrightarrow c = 1$, and I can see no other way to prove this. At the same time, I cannot find a counter-example. Is there a simple resolution to this question?","Hungerford's Algebra poses the question: Is it true that a semigroup $G$ that has a left identity element and in which every element has a right inverse is a group? Now, If both the identity and the inverse are of the same side, this is simple. For, instead of the above, say every element has a left inverse. For $a \in G$ denote this left inverse by $a^{-1}$. Then $$(aa^{-1})(aa^{-1}) = a(a^{-1}a)a^{-1} = aa^{-1}$$ and we can use the fact that $$cc = c \Longrightarrow c = 1$$ to get that inverses are in fact two-sided: $$ aa^{-1} = 1$$ From which it follows that $$a = 1 \cdot a = (aa^{-1})a = a (a^{-1}a) = a \cdot 1$$ as desired. But in the scenario given we cannot use $cc = c \Longrightarrow c = 1$, and I can see no other way to prove this. At the same time, I cannot find a counter-example. Is there a simple resolution to this question?",,"['abstract-algebra', 'group-theory', 'examples-counterexamples', 'semigroups']"
51,Prove that the preimage of a prime ideal is also prime.,Prove that the preimage of a prime ideal is also prime.,,"Let $f: R \rightarrow S$ be a ring homomorphism, with $R$ and $S$ commutative and $f(1)=1$ . If $P$ is a prime ideal of $S$ , show that the preimage $f^{-1}(P)$ is a prime ideal of $R$ . Define $g: S \rightarrow S/P$ with kernel $s$ . Let $h = g \circ f: R \rightarrow S/P$ . Since $h$ is a ring homomorphism, the kernel is an ideal of $R$ . Also, from the first isomorphism theorem, we know that $R/\ker(h) \cong S/P$ . Since $P$ is a prime ideal of $S$ , we know that $S/P$ is an integral domain. Since $R/\ker(h)$ is isomorphic to $S/P$ , it must also be an integral domain, which implies that the kernel of $h$ (which is the preimage of $P$ ) is a prime ideal of $R$ . Do you think my answer is correct? The reason why I was a bit skeptical of my answer is because I did not use the fact that $R$ and $S$ are commutative. So I'm wondering if I missed something $\dots$ Thank you in advance.","Let be a ring homomorphism, with and commutative and . If is a prime ideal of , show that the preimage is a prime ideal of . Define with kernel . Let . Since is a ring homomorphism, the kernel is an ideal of . Also, from the first isomorphism theorem, we know that . Since is a prime ideal of , we know that is an integral domain. Since is isomorphic to , it must also be an integral domain, which implies that the kernel of (which is the preimage of ) is a prime ideal of . Do you think my answer is correct? The reason why I was a bit skeptical of my answer is because I did not use the fact that and are commutative. So I'm wondering if I missed something Thank you in advance.",f: R \rightarrow S R S f(1)=1 P S f^{-1}(P) R g: S \rightarrow S/P s h = g \circ f: R \rightarrow S/P h R R/\ker(h) \cong S/P P S S/P R/\ker(h) S/P h P R R S \dots,['abstract-algebra']
52,Intersection of 2 subgroups must be a subgroup proof.,Intersection of 2 subgroups must be a subgroup proof.,,"Ok we have 2 subgroups of G defined as $H_1$ and $H_2$ the question wants us to prove $H_1$ intersect $H_2$ must also be a subgroup of G. This seems fairly intuitive, making as math usually hard to prove :) we know that for any element a in $H_1$ there exists $a^{-1}$ and that $H_1$ is closed. the same holds for $H_2$ so the intersection will only contain and element c in $H_1$ Intersect $H_2$ if c and $c^{-1}$ are in $H_1$ and $H_2$ additionally we know that $H_1$ and $H_2$ must contain $e$ the identity of G thus $H_1$ intersect $H_2$ cannot be empty. my problem lies with writing this out and proving closure on this intersection.","Ok we have 2 subgroups of G defined as $H_1$ and $H_2$ the question wants us to prove $H_1$ intersect $H_2$ must also be a subgroup of G. This seems fairly intuitive, making as math usually hard to prove :) we know that for any element a in $H_1$ there exists $a^{-1}$ and that $H_1$ is closed. the same holds for $H_2$ so the intersection will only contain and element c in $H_1$ Intersect $H_2$ if c and $c^{-1}$ are in $H_1$ and $H_2$ additionally we know that $H_1$ and $H_2$ must contain $e$ the identity of G thus $H_1$ intersect $H_2$ cannot be empty. my problem lies with writing this out and proving closure on this intersection.",,"['abstract-algebra', 'group-theory']"
53,Applications of the Isomorphism theorems,Applications of the Isomorphism theorems,,"In my study of groups, rings, modules etc, I've seen the three isomorphism theorems stated and proved many times. I use the first one ( $G/\ker \phi \cong \operatorname{im} \phi$ ) very often, but I can't recall having ever used the other two. Can anyone give some examples where they are used in a crucial way in some proof? For clarity, let us say that the 2nd one is : $(M/L)/(N/L) \cong M/N$ under the appropriate conditions, and the 3rd one is $(M+N)/N \cong M/(M\cap N).$","In my study of groups, rings, modules etc, I've seen the three isomorphism theorems stated and proved many times. I use the first one ( $G/\ker \phi \cong \operatorname{im} \phi$ ) very often, but I can't recall having ever used the other two. Can anyone give some examples where they are used in a crucial way in some proof? For clarity, let us say that the 2nd one is : $(M/L)/(N/L) \cong M/N$ under the appropriate conditions, and the 3rd one is $(M+N)/N \cong M/(M\cap N).$",,"['abstract-algebra', 'group-theory', 'ring-theory', 'modules', 'big-list']"
54,How fundamental is the fundamental theorem of algebra?,How fundamental is the fundamental theorem of algebra?,,"Despite its name, its often claimed that the fundamental theorem of algebra (which shows that the Complex numbers are algebraically closed - this is not to be confused with the claim that a polynomial of degree n has at most n roots) is not considered fundamental by algebraists as it's not needed for the development of modern algebra. My question is - what are the major uses of the theorem, and to which extent can they justify the claim that the theorem is fundamental for something ? An example I think of is the Jordan canonical form for matrices, but I don't think it suffices.","Despite its name, its often claimed that the fundamental theorem of algebra (which shows that the Complex numbers are algebraically closed - this is not to be confused with the claim that a polynomial of degree n has at most n roots) is not considered fundamental by algebraists as it's not needed for the development of modern algebra. My question is - what are the major uses of the theorem, and to which extent can they justify the claim that the theorem is fundamental for something ? An example I think of is the Jordan canonical form for matrices, but I don't think it suffices.",,"['abstract-algebra', 'polynomials', 'terminology', 'complex-numbers']"
55,"Why do we use this definition of ""algebraic integer""?","Why do we use this definition of ""algebraic integer""?",,"A number is an ""algebraic integer"" if it is the root to a monic polynomial with integer coefficients. Artin says (Algebra, p. 411): The concept of algebraic integer was one of the most important discoveries of number theory. It is not easy to explain quickly why it is the right definition to use, but roughly speaking, we can think of the leading coefficient of the primitive irreducible polynomials $f(x)$ as a ""denominator."" If $\alpha$ is the root of an integer polynomial $f(x)=dx^n+a_{n-1}x^{n-1}...$ then $d\alpha$ is an algebraic integer, because it is a root of the monic integer polynomial $x^n + a_{n-1}x^{n-1} + ... + d^{n-1}a_0$ . Thus we can ""clear the denominator"" in any algebraic number by multiplying it with a suitable integer to get an algebraic integer. When I first learned of algebraic integers, I looked online and saw some hints that maybe they were used to prove the Abel-Ruffini theorem. So I put off questioning their usage for a while; I now think I understand one proof of this theorem (the one at the end of Artin's Algebra) and it has nothing to do with algebraic integers (that I can tell). So basically: why is it important if a number is an algebraic integer? I think I understand what he's saying about the relationship between roots of integer polynomials and algebraic integers, but I fail to see why this is ""one of the most important discoveries of number theory.""","A number is an ""algebraic integer"" if it is the root to a monic polynomial with integer coefficients. Artin says (Algebra, p. 411): The concept of algebraic integer was one of the most important discoveries of number theory. It is not easy to explain quickly why it is the right definition to use, but roughly speaking, we can think of the leading coefficient of the primitive irreducible polynomials as a ""denominator."" If is the root of an integer polynomial then is an algebraic integer, because it is a root of the monic integer polynomial . Thus we can ""clear the denominator"" in any algebraic number by multiplying it with a suitable integer to get an algebraic integer. When I first learned of algebraic integers, I looked online and saw some hints that maybe they were used to prove the Abel-Ruffini theorem. So I put off questioning their usage for a while; I now think I understand one proof of this theorem (the one at the end of Artin's Algebra) and it has nothing to do with algebraic integers (that I can tell). So basically: why is it important if a number is an algebraic integer? I think I understand what he's saying about the relationship between roots of integer polynomials and algebraic integers, but I fail to see why this is ""one of the most important discoveries of number theory.""",f(x) \alpha f(x)=dx^n+a_{n-1}x^{n-1}... d\alpha x^n + a_{n-1}x^{n-1} + ... + d^{n-1}a_0,"['abstract-algebra', 'number-theory', 'commutative-algebra', 'algebraic-number-theory']"
56,Order of operations - why are they in the order they're in?,Order of operations - why are they in the order they're in?,,"I understand the order of operations, but why are they ordered the way they're ordered? Is there a particular reason why multiplication should have a higher precedence than subtraction, other than to prevent ambiguity? Edit: I'm a curious software developer that's relatively lousy at math. A simple explanation that your grandma could understand would be very welcome. :-)","I understand the order of operations, but why are they ordered the way they're ordered? Is there a particular reason why multiplication should have a higher precedence than subtraction, other than to prevent ambiguity? Edit: I'm a curious software developer that's relatively lousy at math. A simple explanation that your grandma could understand would be very welcome. :-)",,['abstract-algebra']
57,Spectrum of $\mathbb{Z}[x]$,Spectrum of,\mathbb{Z}[x],"Can someone point me towards a resource that proves that the spectrum of $\mathbb{Z}[x]$ consists of ideals $(p,f)$ where $p$ prime or zero and $f$ irred mod $p$? In particular I remember this can be proved simply using localizations, but can't quite remember how to do it! I definitely don't want a link to a long involved argument about polynomials, I can find quite enough of those! Many thanks in advance.","Can someone point me towards a resource that proves that the spectrum of $\mathbb{Z}[x]$ consists of ideals $(p,f)$ where $p$ prime or zero and $f$ irred mod $p$? In particular I remember this can be proved simply using localizations, but can't quite remember how to do it! I definitely don't want a link to a long involved argument about polynomials, I can find quite enough of those! Many thanks in advance.",,"['abstract-algebra', 'reference-request', 'algebraic-geometry', 'ring-theory', 'ideals']"
58,Finite groups with exactly one maximal subgroup,Finite groups with exactly one maximal subgroup,,"I was recently reading a proof in which the following property is used (and left as an exercise that I could not prove so far). Here is exactly how it is stated. Let $G$ be a finite group. Suppose it has exactly one maximal subgroup. Then $G$ is cyclic. Now, does this mean exactly one conjugacy class of maximal subgroups ? If it really means exactly one maximal subgroup , then what can we say about a finite group that possesses exactly one class of (at least two) maximal subgroups that are all conjugate to each other?","I was recently reading a proof in which the following property is used (and left as an exercise that I could not prove so far). Here is exactly how it is stated. Let $G$ be a finite group. Suppose it has exactly one maximal subgroup. Then $G$ is cyclic. Now, does this mean exactly one conjugacy class of maximal subgroups ? If it really means exactly one maximal subgroup , then what can we say about a finite group that possesses exactly one class of (at least two) maximal subgroups that are all conjugate to each other?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups']"
59,Every Transitive Permutation Group Has a Fixed Point Free Element,Every Transitive Permutation Group Has a Fixed Point Free Element,,"If $G$ acts transitively by permutations on a finite set $A$ with more than one element (i.e. $G$ is a transitive permutation subgroup of the symmetric group $S_A$). Why does $G$ necessarily contain an element which has no fixed points (i.e. $g$ such that $g \cdot a \neq a$ for any $a \in A$)? The hint I have is to think about, given $a \in A$, what fraction of elements of $G$ fixes $a$. I'm not sure how to go about this hint...","If $G$ acts transitively by permutations on a finite set $A$ with more than one element (i.e. $G$ is a transitive permutation subgroup of the symmetric group $S_A$). Why does $G$ necessarily contain an element which has no fixed points (i.e. $g$ such that $g \cdot a \neq a$ for any $a \in A$)? The hint I have is to think about, given $a \in A$, what fraction of elements of $G$ fixes $a$. I'm not sure how to go about this hint...",,"['abstract-algebra', 'group-theory', 'finite-groups']"
60,Morphisms in the category of natural transformations?,Morphisms in the category of natural transformations?,,"I am learning the basics of category theory, so this question is probably obvious to anyone who knows the subject. The resources I've seen all take the following approach: 0) A category is a collection of objects and morphisms between those objects that satisfy some rules. 1) A functor is a morphism in the category of categories. 2) A natural transformation is a morphism in the category of functors. But they all stop right there. What about: 3) the morphisms in the category of natural transformations? 4) Or the ""morphisms in the category of the morphisms in the category of natural transformations"" 5) ... Are these uninteresting? Why does the ""meta-ness"" stop at 2 levels deep?","I am learning the basics of category theory, so this question is probably obvious to anyone who knows the subject. The resources I've seen all take the following approach: 0) A category is a collection of objects and morphisms between those objects that satisfy some rules. 1) A functor is a morphism in the category of categories. 2) A natural transformation is a morphism in the category of functors. But they all stop right there. What about: 3) the morphisms in the category of natural transformations? 4) Or the ""morphisms in the category of the morphisms in the category of natural transformations"" 5) ... Are these uninteresting? Why does the ""meta-ness"" stop at 2 levels deep?",,"['category-theory', 'abstract-algebra']"
61,Why this $\sigma \pi \sigma^{-1}$ keeps appearing in my group theory book? (cycle decomposition),Why this  keeps appearing in my group theory book? (cycle decomposition),\sigma \pi \sigma^{-1},"I'm studying cycle decomposition in group theory. The exercises on my book keep saying things like: Find a permutation $\sigma$ such that $\sigma (123) \sigma^{-1} = (456)$ Prove that there is no permutation $\sigma$ such that $\sigma (1 2) \sigma^{-1} = (123)$ Show that $\pi(x_1 x_2 \cdots x_n)\pi^{-1} = (\pi(x_1)\cdots \pi(x_n))$ Why this  $\sigma \pi \sigma^{-1}$ appears every time in this book? Is this important for something? Could somebody give me a clear picture of how we arrive at this? Also, would be nice to know how to prove the third one, if somebody could help me.","I'm studying cycle decomposition in group theory. The exercises on my book keep saying things like: Find a permutation $\sigma$ such that $\sigma (123) \sigma^{-1} = (456)$ Prove that there is no permutation $\sigma$ such that $\sigma (1 2) \sigma^{-1} = (123)$ Show that $\pi(x_1 x_2 \cdots x_n)\pi^{-1} = (\pi(x_1)\cdots \pi(x_n))$ Why this  $\sigma \pi \sigma^{-1}$ appears every time in this book? Is this important for something? Could somebody give me a clear picture of how we arrive at this? Also, would be nice to know how to prove the third one, if somebody could help me.",,"['abstract-algebra', 'group-theory', 'permutations', 'finite-groups']"
62,Lie algebra: why does it have to be the tangent space at the IDENTITY of a Lie group?,Lie algebra: why does it have to be the tangent space at the IDENTITY of a Lie group?,,Why is the identity element so important in this construction. I looked up some books and notes but still do not see why. How could the construction started from tangent space of a element other than identity possibly fail to get a Lie algebra so that people can only get it from the tangent space of identity?,Why is the identity element so important in this construction. I looked up some books and notes but still do not see why. How could the construction started from tangent space of a element other than identity possibly fail to get a Lie algebra so that people can only get it from the tangent space of identity?,,"['abstract-algebra', 'group-theory']"
63,Why does $(A/I)\otimes_R (B/J)\cong(A\otimes_R B)/(I\otimes_R 1+1\otimes_R J)$?,Why does ?,(A/I)\otimes_R (B/J)\cong(A\otimes_R B)/(I\otimes_R 1+1\otimes_R J),"While reading, there is an isomorphism that I'm having trouble fulling seeing. If you have two algebras $A$ and $B$ over a commutative ring $R$, with $I$ and $J$ two sided ideals in $A$ and $B$, then you should have an isomorphism   $$ (A/I)\otimes (B/J)\cong (A\otimes B)/(I\otimes 1+1\otimes J). $$ Now there are bilinear maps from $A/I\times B/J\to (A\times B)/(I+J)$ and so the universal property of the tensor product gives unique maps $(A/I)\otimes (B/J)\to (A\times B)/(I+J)$. Does this somehow get to the isomorphism above, or am I completely off track? What is the quick way to see this? Thanks!","While reading, there is an isomorphism that I'm having trouble fulling seeing. If you have two algebras $A$ and $B$ over a commutative ring $R$, with $I$ and $J$ two sided ideals in $A$ and $B$, then you should have an isomorphism   $$ (A/I)\otimes (B/J)\cong (A\otimes B)/(I\otimes 1+1\otimes J). $$ Now there are bilinear maps from $A/I\times B/J\to (A\times B)/(I+J)$ and so the universal property of the tensor product gives unique maps $(A/I)\otimes (B/J)\to (A\times B)/(I+J)$. Does this somehow get to the isomorphism above, or am I completely off track? What is the quick way to see this? Thanks!",,"['abstract-algebra', 'tensor-products']"
64,Can all groups be thought of as the symmetries of a geometrical object?,Can all groups be thought of as the symmetries of a geometrical object?,,"It is often said that we can think of groups as the symmetries of some mathematical object. Usual examples involve geometrical objects, for instance we can think of $\mathbb{S}_3$ as the collection of all reflections and rotation symmetries of an equilateral triangle, similarly we can think of $D_8$ as the symmetry group of a square. Cayley's Theorem along with the fact that the symmetry group of a regular $n$ -simplex is isomorphic to $\mathbb{S}_{n+1}$ allows us to think of any finite group as a subset of the symmetry group of some geometrical object. Which brings me to the following questions: Can every finite group be represented as the collection of all symmetries of a geometrical object? That is, are all finite groups isomorphic to some Symmetry group? Can such a result (the representation of groups as distance-preserving transformations of some geometrical object) be extended to infinite groups? If so, how? Thanks in advance (:","It is often said that we can think of groups as the symmetries of some mathematical object. Usual examples involve geometrical objects, for instance we can think of as the collection of all reflections and rotation symmetries of an equilateral triangle, similarly we can think of as the symmetry group of a square. Cayley's Theorem along with the fact that the symmetry group of a regular -simplex is isomorphic to allows us to think of any finite group as a subset of the symmetry group of some geometrical object. Which brings me to the following questions: Can every finite group be represented as the collection of all symmetries of a geometrical object? That is, are all finite groups isomorphic to some Symmetry group? Can such a result (the representation of groups as distance-preserving transformations of some geometrical object) be extended to infinite groups? If so, how? Thanks in advance (:",\mathbb{S}_3 D_8 n \mathbb{S}_{n+1},"['abstract-algebra', 'group-theory', 'geometry', 'soft-question', 'symmetry']"
65,"What's the difference between geometric, exterior and multilinear algebra?","What's the difference between geometric, exterior and multilinear algebra?",,"I've studied what I think is geometric algebra, but can't seem to understand the difference between it and exterior and multilinear algebra. And is it linked to Clifford and Grassmann algebras in any way?","I've studied what I think is geometric algebra, but can't seem to understand the difference between it and exterior and multilinear algebra. And is it linked to Clifford and Grassmann algebras in any way?",,"['abstract-algebra', 'multilinear-algebra', 'exterior-algebra', 'clifford-algebras', 'geometric-algebras']"
66,Self-Studying Abstract Algebra; Aye or Nay? [closed],Self-Studying Abstract Algebra; Aye or Nay? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question I am a high schooler with a deep interest in mathematics, which is why I have self-studied Linear Algebra and have begun my self-study in Differential Equations. As I am a man who likes to plan ahead, I'm pondering what field of mathematics to plunge into once I've finished DE's. I am thinking of Abstract Algebra: it has always sounded mystical and intruiging to me for some reason. I have a couple of questions regarding AA: What exactly is Abstract Algebra? What does it study? Please use your own definition, no wikipedia definition please. What are its applications? Does it have a use for example in physics or chemistry, or is it as abstract as its name suggests? Would it be a logical step for a high schooler to self-study abstract algebra after studying LA and DE's, or is there a field of (post-high school) math 'better' or more useful to study prior to abstract algebra? What are some good books, pdfs, open courseware etc. on abstract algebra? links and names please.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question I am a high schooler with a deep interest in mathematics, which is why I have self-studied Linear Algebra and have begun my self-study in Differential Equations. As I am a man who likes to plan ahead, I'm pondering what field of mathematics to plunge into once I've finished DE's. I am thinking of Abstract Algebra: it has always sounded mystical and intruiging to me for some reason. I have a couple of questions regarding AA: What exactly is Abstract Algebra? What does it study? Please use your own definition, no wikipedia definition please. What are its applications? Does it have a use for example in physics or chemistry, or is it as abstract as its name suggests? Would it be a logical step for a high schooler to self-study abstract algebra after studying LA and DE's, or is there a field of (post-high school) math 'better' or more useful to study prior to abstract algebra? What are some good books, pdfs, open courseware etc. on abstract algebra? links and names please.",,"['abstract-algebra', 'reference-request', 'soft-question', 'self-learning']"
67,When is a tensor product of two commutative rings noetherian?,When is a tensor product of two commutative rings noetherian?,,"In particular, I'm told if $k$ is commutative (ring), $R$ and $S$ are commutative $k$-algebras such that $R$ is noetherian, and $S$ is a finitely generated $k$-algebra, then the tensor product $R\otimes_k S$ of $R$ and $S$ over $k$ is a noetherian ring.","In particular, I'm told if $k$ is commutative (ring), $R$ and $S$ are commutative $k$-algebras such that $R$ is noetherian, and $S$ is a finitely generated $k$-algebra, then the tensor product $R\otimes_k S$ of $R$ and $S$ over $k$ is a noetherian ring.",,"['abstract-algebra', 'commutative-algebra', 'noetherian']"
68,"Quotient of polynomials, PID but not Euclidean domain?","Quotient of polynomials, PID but not Euclidean domain?",,"While trying to look up examples of PIDs that are not Euclidean domains, I found a statement (without reference) on the Euclidean domain page of Wikipedia that $$\mathbb{R}[X,Y]/(X^2+Y^2+1)$$ is such a ring. After a good deal of searching, I have not been able to find any other (online) reference to this ring. Can anyone confirm this result? Is there a reference for it (paper, textbook or website)?","While trying to look up examples of PIDs that are not Euclidean domains, I found a statement (without reference) on the Euclidean domain page of Wikipedia that $$\mathbb{R}[X,Y]/(X^2+Y^2+1)$$ is such a ring. After a good deal of searching, I have not been able to find any other (online) reference to this ring. Can anyone confirm this result? Is there a reference for it (paper, textbook or website)?",,"['abstract-algebra', 'reference-request', 'commutative-algebra', 'principal-ideal-domains']"
69,Self teaching Galois Theory,Self teaching Galois Theory,,"At uni, I did a module in group theory which I really enjoyed. I also did one on number theory which had aspects of rings and fields in it and I enjoyed learning this. Now that I have finished uni, I was thinking that I would still like to learn all this stuff for two main reasons: $1)$ I like it and it was interesting and it's a fun ""hobby"" to have, and $2)$ If I wanted to apply for a masters in the future, it'd be good to show them that I have been doing some maths in my spare time. I spoke to a friend and told him I liked abstract algebra and in particular, I liked group theory. He said that if I liked that then Galois theory might be a good subject to look at, but I am a bit worried about going this advanced without knowing if I completely understand the basics. For example, I can create semi direct products for cyclic groups, but not really any other groups. I don't really get how to use cosets, etc. So I was thinking, would it be a good idea for me to first start on a basic book like ""A First course in Abstract Algebra"" as although there will be bits I understand quickly, there will be bits I have forgotten/didn't learn properly, etc. However, my friend said that it might be better to learn Galois theory with a good book as any basic bits I will remember again or if I genuinely don't know/remember, then I can look it up later. What would you think is the best idea? Apart from Galois theory, are there any other interesting subjects within the field of Group theory I could do?","At uni, I did a module in group theory which I really enjoyed. I also did one on number theory which had aspects of rings and fields in it and I enjoyed learning this. Now that I have finished uni, I was thinking that I would still like to learn all this stuff for two main reasons: $1)$ I like it and it was interesting and it's a fun ""hobby"" to have, and $2)$ If I wanted to apply for a masters in the future, it'd be good to show them that I have been doing some maths in my spare time. I spoke to a friend and told him I liked abstract algebra and in particular, I liked group theory. He said that if I liked that then Galois theory might be a good subject to look at, but I am a bit worried about going this advanced without knowing if I completely understand the basics. For example, I can create semi direct products for cyclic groups, but not really any other groups. I don't really get how to use cosets, etc. So I was thinking, would it be a good idea for me to first start on a basic book like ""A First course in Abstract Algebra"" as although there will be bits I understand quickly, there will be bits I have forgotten/didn't learn properly, etc. However, my friend said that it might be better to learn Galois theory with a good book as any basic bits I will remember again or if I genuinely don't know/remember, then I can look it up later. What would you think is the best idea? Apart from Galois theory, are there any other interesting subjects within the field of Group theory I could do?",,"['abstract-algebra', 'reference-request', 'soft-question', 'galois-theory']"
70,What's the motivation of the definition of primary ideals?,What's the motivation of the definition of primary ideals?,,$$xy\in\mathfrak q\:\Rightarrow\:\text{either $x\in\mathfrak q$ or $y^n\in\mathfrak q$ for some $n\gt0$}.$$ Primary ideals can be regard as the generalization of prime ideals and radical. But why it's defined like that? It's not symmetry. Why not define like that: $$xy\in\mathfrak q\:\Rightarrow\:\text{either $x^n\in\mathfrak q$ or $y^n\in\mathfrak q$ for some $n\gt0$}.$$,$$xy\in\mathfrak q\:\Rightarrow\:\text{either $x\in\mathfrak q$ or $y^n\in\mathfrak q$ for some $n\gt0$}.$$ Primary ideals can be regard as the generalization of prime ideals and radical. But why it's defined like that? It's not symmetry. Why not define like that: $$xy\in\mathfrak q\:\Rightarrow\:\text{either $x^n\in\mathfrak q$ or $y^n\in\mathfrak q$ for some $n\gt0$}.$$,,"['abstract-algebra', 'commutative-algebra', 'ring-theory', 'self-learning', 'ideals']"
71,Why are properties lost in the Cayley–Dickson construction?,Why are properties lost in the Cayley–Dickson construction?,,"Motivating question : What lies beyond the Sedenions? I'm aware that one can construct a hierarchy of number systems via the Cayley–Dickson process: $$\mathbb{R} \subset \mathbb{C} \subset \mathbb{H} \subset \mathbb{O} \subset \mathbb{S} \subset \ldots $$ ""Reals"" $\subset$ ""Complex"" $\subset$ ""Quaternions"" $\subset$ ""Octonions"" $\subset$ ""Sedenions"" $\subset$ $\ldots$ and that at each step you're given a multiplication table that tell how the elements interact. As you move up the ladder, certain ""nice"" properties are lost: ordering,  commutativity, associativity, multiplicative normedness, etc... Given the multiplication table, you can show that these properties don't hold. Eric Naslund noted that ""the first 4 are very special as they are the unique 4 normed division algebras over ℝ"", no surprise then that these $2^n$-ions have found quite a bit of use. I'm interested in the sequence itself however, irrespective of how useful a $2^{256}$-ion might be ( ducenti-quinquaginta-sex-ion ?). I feel like something deeper is going on here though that I don't understand. Why are these particular properties lost at each step? Is it possible to quantify the process such that, at the $2^n$-ion you can say something about the symmetry of the multiplication table*? * I'm making an ansatz that there is a connection between the symmetry of the multiplication table and these ""nice"" properties.","Motivating question : What lies beyond the Sedenions? I'm aware that one can construct a hierarchy of number systems via the Cayley–Dickson process: $$\mathbb{R} \subset \mathbb{C} \subset \mathbb{H} \subset \mathbb{O} \subset \mathbb{S} \subset \ldots $$ ""Reals"" $\subset$ ""Complex"" $\subset$ ""Quaternions"" $\subset$ ""Octonions"" $\subset$ ""Sedenions"" $\subset$ $\ldots$ and that at each step you're given a multiplication table that tell how the elements interact. As you move up the ladder, certain ""nice"" properties are lost: ordering,  commutativity, associativity, multiplicative normedness, etc... Given the multiplication table, you can show that these properties don't hold. Eric Naslund noted that ""the first 4 are very special as they are the unique 4 normed division algebras over ℝ"", no surprise then that these $2^n$-ions have found quite a bit of use. I'm interested in the sequence itself however, irrespective of how useful a $2^{256}$-ion might be ( ducenti-quinquaginta-sex-ion ?). I feel like something deeper is going on here though that I don't understand. Why are these particular properties lost at each step? Is it possible to quantify the process such that, at the $2^n$-ion you can say something about the symmetry of the multiplication table*? * I'm making an ansatz that there is a connection between the symmetry of the multiplication table and these ""nice"" properties.",,"['abstract-algebra', 'quaternions', 'octonions', 'sedenions', 'hypercomplex-numbers']"
72,Which fields satisfy the Freshman's Dream?,Which fields satisfy the Freshman's Dream?,,"It is well-known that the Freshman's Dream, $(a+b)^p = a^p + b^p$, holds in fields of characteristic $p$. For $p = 2$, in fact those are the only fields; for, $(a+b)^2 = a^2 + b^2 \Rightarrow 2ab = 0$ for each $a, b$, so in specific the field has characteristic $2$. But $(a+b)^3 = a^3 + b^3$ is satisfied by every element of $\mathbb{F}_2$, as well as in fields of characteristic $3$. In fact this is the only field of characteristic $\ne 3$ in which this identity holds; the identity is equivalent to $3ab(a+b) = 0$, which in characteristic $\ne 3$ means $ab(a+b) = 0$ or, letting $b = 1$, $a = 0,-1$. In which fields does the identity $(a+b)^p = a^p + b^p$ hold for every $a, b$ (with $p$ fixed)?","It is well-known that the Freshman's Dream, $(a+b)^p = a^p + b^p$, holds in fields of characteristic $p$. For $p = 2$, in fact those are the only fields; for, $(a+b)^2 = a^2 + b^2 \Rightarrow 2ab = 0$ for each $a, b$, so in specific the field has characteristic $2$. But $(a+b)^3 = a^3 + b^3$ is satisfied by every element of $\mathbb{F}_2$, as well as in fields of characteristic $3$. In fact this is the only field of characteristic $\ne 3$ in which this identity holds; the identity is equivalent to $3ab(a+b) = 0$, which in characteristic $\ne 3$ means $ab(a+b) = 0$ or, letting $b = 1$, $a = 0,-1$. In which fields does the identity $(a+b)^p = a^p + b^p$ hold for every $a, b$ (with $p$ fixed)?",,"['abstract-algebra', 'field-theory']"
73,"Why are ""algebras"" called algebras?","Why are ""algebras"" called algebras?",,"There's a mathematical object called an ""algebra"" (e.g. an algebra over a ring ), but why does this particular object have such an ""important"" name (which makes it sound like the most important concept in this huge area, abstract algebra), whereas the names of other important algebraic structures such as magmas, groups, rings, lattices and modules sound less important. I know some universal algebra and category theory, so I understand that ""algebras"" have many kin objects. But I can't understand why somebody decided to call these particular objects ""algebras"", although there seem to be many other good candidates for this grand name. Similarly, there are objects called ""numbers"" in number theory, ""sets"" in set theory, ""categories"" in category theory, and ""topologies"" in topology. However, in other areas, for example analysis, geometry and even mathematics, there is no object called an ""analysis"", a ""geometry"" or a ""mathematics"". Is it because there's no fundamental object in these areas which have unsurpassable importance over others? If there are any central objects in these areas which should be named by their significance, in the same way as the objects ""category"", ""topology"", and ""set"" in their respective areas, could you tell me them?","There's a mathematical object called an ""algebra"" (e.g. an algebra over a ring ), but why does this particular object have such an ""important"" name (which makes it sound like the most important concept in this huge area, abstract algebra), whereas the names of other important algebraic structures such as magmas, groups, rings, lattices and modules sound less important. I know some universal algebra and category theory, so I understand that ""algebras"" have many kin objects. But I can't understand why somebody decided to call these particular objects ""algebras"", although there seem to be many other good candidates for this grand name. Similarly, there are objects called ""numbers"" in number theory, ""sets"" in set theory, ""categories"" in category theory, and ""topologies"" in topology. However, in other areas, for example analysis, geometry and even mathematics, there is no object called an ""analysis"", a ""geometry"" or a ""mathematics"". Is it because there's no fundamental object in these areas which have unsurpassable importance over others? If there are any central objects in these areas which should be named by their significance, in the same way as the objects ""category"", ""topology"", and ""set"" in their respective areas, could you tell me them?",,"['abstract-algebra', 'soft-question', 'terminology']"
74,"Rotman's exercise 2.8 ""$S_n$ cannot be imbedded in $A_{n+1}$""","Rotman's exercise 2.8 "" cannot be imbedded in """,S_n A_{n+1},"This question is about the (in)famous Rotman's exercise 2.8 in "" An Introduction to the Theory of Groups ."" I've searched and found similar questions here and in MO, but none of them contains a valid proof. ( Does $S_n$ belong as a subgroup to $A_{n+1}$ ? ) According to Rotman, a valid proof can only use the concepts introduced up to this exercise: cycle permutations, factorization of permutations, odd and even permutations, semigroups, groups, homomorphism and subgroups. Cosets, Lagrange's theorem, normal subgroups, and so on are not yet introduced. I stress this point because all of the proofs I've seen use Lagrange or actions, on cosets. Now my attempt is to use exercise 2.7 (solved) which is about a proof that $A_n$ ( $n>2$ ) is generated by all the $3$ -cycles and exercise 2.4 (solved) "" if $S$ is a proper subgroup of $G$ then $\langle G \setminus S\rangle=G$ "" in this way: Suppose that for every $\phi  : S_n \to A_{n+1}$ imbeddings, all the $ 3$ -cycles are contained in $\operatorname{Im}\phi $ , then the assertion is proved by absurd. But I can't find a way to prove if it is possible or either find a counterexample to this kind of approach. If someone has another proof which use only basics concepts is well accepted of course, but I mainly need  some hints about correctness or not of my reasoning and how to proceed if it is correct. Thank you in advance.","This question is about the (in)famous Rotman's exercise 2.8 in "" An Introduction to the Theory of Groups ."" I've searched and found similar questions here and in MO, but none of them contains a valid proof. ( Does belong as a subgroup to ? ) According to Rotman, a valid proof can only use the concepts introduced up to this exercise: cycle permutations, factorization of permutations, odd and even permutations, semigroups, groups, homomorphism and subgroups. Cosets, Lagrange's theorem, normal subgroups, and so on are not yet introduced. I stress this point because all of the proofs I've seen use Lagrange or actions, on cosets. Now my attempt is to use exercise 2.7 (solved) which is about a proof that ( ) is generated by all the -cycles and exercise 2.4 (solved) "" if is a proper subgroup of then "" in this way: Suppose that for every imbeddings, all the -cycles are contained in , then the assertion is proved by absurd. But I can't find a way to prove if it is possible or either find a counterexample to this kind of approach. If someone has another proof which use only basics concepts is well accepted of course, but I mainly need  some hints about correctness or not of my reasoning and how to proceed if it is correct. Thank you in advance.",S_n A_{n+1} A_n n>2 3 S G \langle G \setminus S\rangle=G \phi  : S_n \to A_{n+1}  3 \operatorname{Im}\phi ,"['abstract-algebra', 'group-theory', 'finite-groups', 'symmetric-groups']"
75,"Show that in any group of order $23 \cdot 24$, the $23$-Sylow subgroup is normal.","Show that in any group of order , the -Sylow subgroup is normal.",23 \cdot 24 23,"Show that in any group of order $23 \cdot 24$, the $23$-Sylow subgroup is normal. Let $P_k$ denote the $k$-Sylow subgroup and let $n_3$ denote the number of conjugates of $P_k$. $n_2 \equiv 1 \mod 2$ and $n_2 | 69 \implies n_2= 1, 3, 23, 69$ $n_3 \equiv 1 \mod 3$ and $n_3 | 184 \implies n_3 = 1, 4, 184$ $n_{23} \equiv 1 \mod 23$ and $n_{23} | 24 \implies n_{23} = 1, 24$ Suppose for contradiction that $n_{23} = 24$. Then $N(P_{23})=552/24=23$. So the normalizer of $P_3$ only contains the identity and elements of order $23$. It is impossible for $n_2$ to equal $23$ or $69$, or for $n_3$ to equal $184$, since we would then have more than $552$ elements in G. Case 1: Let $n_2 = 1$. Then we have $P_2 \triangleleft G$, and so we have a subgroup $H=P_2P_{23}$ in G. We know that $|H|=\frac{|P_2||P_{23}|}{|P_2 \cap P_{23}} = 184$. We also know that the $23$-Sylow subgroup of $H$ is normal, so it's normalizer is of order $184$. Since a $p$-sylow subgroup is the largest subgroup of order $p^k$ for some $k$; and since the order of the $23$-Sylow subgroup of H and G both equal $23$, they must coincide. But that means that elements of orders not equal to $23$ normalize $P_{23}$, which is a contradiction. Case 2: Suppose that $n_3=1$. Then we have $P_3 \triangleleft G$, and so we have a subgroup $K=P_2P_{23}$ in $G$. Since $|K|=\frac{|P_3||P_{23}|}{|P_2 \cap P_{23}}= 69$, the $23$-sylow subgroup of $K$ is normal. Again, as in case 1, we have an element of order not equal to $23$ that normalizes $P_{23}$. Case 3: Let $n_3=4$ and $n_2=3$. But then $N(P_2)=184$. So this is the same as case 1, since we have a subgroup of G of order $184$. Do you think my answer is correct? Thanks in advance","Show that in any group of order $23 \cdot 24$, the $23$-Sylow subgroup is normal. Let $P_k$ denote the $k$-Sylow subgroup and let $n_3$ denote the number of conjugates of $P_k$. $n_2 \equiv 1 \mod 2$ and $n_2 | 69 \implies n_2= 1, 3, 23, 69$ $n_3 \equiv 1 \mod 3$ and $n_3 | 184 \implies n_3 = 1, 4, 184$ $n_{23} \equiv 1 \mod 23$ and $n_{23} | 24 \implies n_{23} = 1, 24$ Suppose for contradiction that $n_{23} = 24$. Then $N(P_{23})=552/24=23$. So the normalizer of $P_3$ only contains the identity and elements of order $23$. It is impossible for $n_2$ to equal $23$ or $69$, or for $n_3$ to equal $184$, since we would then have more than $552$ elements in G. Case 1: Let $n_2 = 1$. Then we have $P_2 \triangleleft G$, and so we have a subgroup $H=P_2P_{23}$ in G. We know that $|H|=\frac{|P_2||P_{23}|}{|P_2 \cap P_{23}} = 184$. We also know that the $23$-Sylow subgroup of $H$ is normal, so it's normalizer is of order $184$. Since a $p$-sylow subgroup is the largest subgroup of order $p^k$ for some $k$; and since the order of the $23$-Sylow subgroup of H and G both equal $23$, they must coincide. But that means that elements of orders not equal to $23$ normalize $P_{23}$, which is a contradiction. Case 2: Suppose that $n_3=1$. Then we have $P_3 \triangleleft G$, and so we have a subgroup $K=P_2P_{23}$ in $G$. Since $|K|=\frac{|P_3||P_{23}|}{|P_2 \cap P_{23}}= 69$, the $23$-sylow subgroup of $K$ is normal. Again, as in case 1, we have an element of order not equal to $23$ that normalizes $P_{23}$. Case 3: Let $n_3=4$ and $n_2=3$. But then $N(P_2)=184$. So this is the same as case 1, since we have a subgroup of G of order $184$. Do you think my answer is correct? Thanks in advance",,"['abstract-algebra', 'finite-groups']"
76,What are the fields with 4 elements? [closed],What are the fields with 4 elements? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . The community reviewed whether to reopen this question 10 months ago and left it closed: Original close reason(s) were not resolved Improve this question What are the  fields with 4 elements?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . The community reviewed whether to reopen this question 10 months ago and left it closed: Original close reason(s) were not resolved Improve this question What are the  fields with 4 elements?",,"['abstract-algebra', 'field-theory', 'finite-fields']"
77,Does every unital ring contain all the integers?,Does every unital ring contain all the integers?,,"Let us suppose there is a ring $R$ with the multiplicative identity $1$. We know that $1+r\in R$, where $r$ is any element of the ring $R$. Does this mean $1+1$ is also part of the ring, or does $r$ have to be an element of the ring different from $1$? Is $1+1$ called $2$ in the ring? Similarly, as $-1$ is also part of the ring, is $-1+ -1$ called $-2$ in the ring? If it is, then I suppose all integers are contained in every unital ring. These questions are very elementary. However, I read contradictory remarks in some places which tend to confuse me. So I thought it would be best to clear any doubts, however trivial the questions. Thanks in advance for your help!","Let us suppose there is a ring $R$ with the multiplicative identity $1$. We know that $1+r\in R$, where $r$ is any element of the ring $R$. Does this mean $1+1$ is also part of the ring, or does $r$ have to be an element of the ring different from $1$? Is $1+1$ called $2$ in the ring? Similarly, as $-1$ is also part of the ring, is $-1+ -1$ called $-2$ in the ring? If it is, then I suppose all integers are contained in every unital ring. These questions are very elementary. However, I read contradictory remarks in some places which tend to confuse me. So I thought it would be best to clear any doubts, however trivial the questions. Thanks in advance for your help!",,['abstract-algebra']
78,"Why is $\operatorname{Hom}(M,N)$ not necessarily an $R$ module?",Why is  not necessarily an  module?,"\operatorname{Hom}(M,N) R","Let $R$ be a ring, and $M,N$ be left $R$-modules. Then is it not true that $\operatorname{Hom}_R(M,N)$ has the structure of an $R$-module? I was reading the preface of the Homological Algebra book by Rotman and was quite surprised to learn that this is not the case. I think all the axioms for being a module are satisfied by $\operatorname{Hom}_R(M,N)$, but Rotman is very unlikely to make a mistake. What is it that I am missing? Under what circumstances is this true?","Let $R$ be a ring, and $M,N$ be left $R$-modules. Then is it not true that $\operatorname{Hom}_R(M,N)$ has the structure of an $R$-module? I was reading the preface of the Homological Algebra book by Rotman and was quite surprised to learn that this is not the case. I think all the axioms for being a module are satisfied by $\operatorname{Hom}_R(M,N)$, but Rotman is very unlikely to make a mistake. What is it that I am missing? Under what circumstances is this true?",,"['abstract-algebra', 'modules']"
79,Every group of order 255 is commutative,Every group of order 255 is commutative,,"There was previous task was same but with $N = 185$. And I prove it by showing that number of Sylow subgroups is 1 for every prime $p\mid N$. But there I have some options $N_5 \in \{1, 51\}$, $N_{17} = 1$, $N_3 \in \{1, 85\}$. I've tried to get contradiction from $N_5 = 51$ or $N_3=85$, but I didn't manage to do it I understand that it's impossible to have $N_5 = 51$ and $N_3=85$ at the same time.","There was previous task was same but with $N = 185$. And I prove it by showing that number of Sylow subgroups is 1 for every prime $p\mid N$. But there I have some options $N_5 \in \{1, 51\}$, $N_{17} = 1$, $N_3 \in \{1, 85\}$. I've tried to get contradiction from $N_5 = 51$ or $N_3=85$, but I didn't manage to do it I understand that it's impossible to have $N_5 = 51$ and $N_3=85$ at the same time.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups', 'sylow-theory']"
80,When can a homomorphism be determined entirely by its generators,When can a homomorphism be determined entirely by its generators,,"I read a text which says that: Just because a homomorphism $ϕ :G  →  H$  is determined by the image of its generators does not mean that any such image will work. e.g.: Suppose we try to define homomorphism $ϕ :Z_3  →  Z_4$  by $ϕ(1)=1$  , then we get $ϕ(0)=ϕ(1+1+1)=3$  which isn't possible as $ϕ(0)=0$. Does there exist some case in which a homomorphism is entirely determined by its generators?","I read a text which says that: Just because a homomorphism $ϕ :G  →  H$  is determined by the image of its generators does not mean that any such image will work. e.g.: Suppose we try to define homomorphism $ϕ :Z_3  →  Z_4$  by $ϕ(1)=1$  , then we get $ϕ(0)=ϕ(1+1+1)=3$  which isn't possible as $ϕ(0)=0$. Does there exist some case in which a homomorphism is entirely determined by its generators?",,"['group-theory', 'abstract-algebra']"
81,Why don't quaternions contradict the Fundamental Theorem of Algebra? [duplicate],Why don't quaternions contradict the Fundamental Theorem of Algebra? [duplicate],,"This question already has answers here : Why are the solutions of polynomial equations so unconstrained over the quaternions? (6 answers) Closed 7 years ago . I don't pretend to know anything much about the Fundamental Theorem of Algebra (FTA), but I do know what it states: for any polynomial with degree $n$, there are exactly $n$ solutions (roots). Well, when it comes to quaternions, apparently $i^2=j^2=k^2=-1$, but $i\ne j\ne k\ne i$. So now, we have apparently found three solutions to the second -degree polynomial $x^2=-1$. I'm not aware of the justification of the FTA, nor I am I aware of Hamilton's justification for quaternions. However, I know a contradiction when I see one. What am I missing here?","This question already has answers here : Why are the solutions of polynomial equations so unconstrained over the quaternions? (6 answers) Closed 7 years ago . I don't pretend to know anything much about the Fundamental Theorem of Algebra (FTA), but I do know what it states: for any polynomial with degree $n$, there are exactly $n$ solutions (roots). Well, when it comes to quaternions, apparently $i^2=j^2=k^2=-1$, but $i\ne j\ne k\ne i$. So now, we have apparently found three solutions to the second -degree polynomial $x^2=-1$. I'm not aware of the justification of the FTA, nor I am I aware of Hamilton's justification for quaternions. However, I know a contradiction when I see one. What am I missing here?",,"['abstract-algebra', 'polynomials', 'quaternions']"
82,"Geometrically, why do line bundles have inverses with respect to the tensor product?","Geometrically, why do line bundles have inverses with respect to the tensor product?",,"Geometrically, why do line bundles have inverses with respect to the tensor product? Here my thoughts on the problem so far, please excuse their scatteredness. I know algebraically, it is just because they are locally modules generated by $1$ element. Basically, it is just the fact that if one has a finitely generated module $M$ over a local ring $R$, there exists $M^{-1}$ with $M \otimes M^{-1} = R$ only when $M$ is free of rank $1$. This is not hard to prove. Geometrically, we are taking a line bundle; we are sort of ""straightening the lines"" to make it trivial. In fact, line bundles are just trivial neighborhoods with transition functions to $\mathbb{A}^1$. We just take the reciprocal of the transition function, and that gives us the inverse line bundle. So one might think of the following: the tensor product corresponds to product of modules, and hence looks nice geometrically precisely when there is no ""torsion"" in a non-precise sence, i.e. locally free modules? Locally projective modules? Flat? (Projective is stronger than flat.) So those are morally vector bundles, and can be straightened consistently precisely when the rank equals $1$. As a counterthought to the above, the above isn't exactly true. ""Straightening"" only really makes sense in the rank $1$ case because the structure sheaf is itself a line bundle and not a higher vector bundle. So one can not, for obvious reasons, have a rank $2$ vector bundle that inverts another rank $2$ vector bundle. But in the case of a rank $1$, it is straightening the bundle, which can be done. For higher dimensional vector bundles, we can think of an ""inverse"" as $O^r$, but even then not all guys would be invertible (i.e. anything that does not split probably should not be invertible like this). Everything I have so far is rather verbose. On an intuitive level with respect to the original question, we can think of nontrivial line bundles as structures that force their sections to have zeros or poles. If we have a section that is regular and has no vanishing everywhere, we can use it to trivialize the line bundle. A nice geometric picture of this is the line bundle of the Möbius strip over the circle. Alternatively, we can always construct an inverse by taking the dual of a line bundle and then using the adjoint properties of the tensor product to show its an inverse. If we take a section, there will have to be some point in which the section crosses over the zero section. If we can find some section that has poles where the sections have zeros, their tensor product, which will have sections behaving like the product of the $2$ original sections will be regular. So now, we have to visualize what taking the dual means, or sending zeros to poles. Geometrically, this is kind of like flipping over our line bundle and gluing everything together at infinity. This makes the zeros become poles. But this is all still very algebraic. Anyways, we certainly know how it works for higher dimensional things because the tensor product multiplies the dimension of the vector space. At this point, one might wonder, what do I mean by ""if we take a section, there will have to be some point in which the section crosses over the zero section."" Why does a global section have to intersect the zero section? Think about the Möbius strip. Draw a circle in the middle; this is isomorphic to the circle. Thus, the Möbius strip is a line bundle over the circle (well, we need to extend the Möbius strip out to infinity). Now, draw some line along the Möbius strip that tries to avoid intersecting the zero section (the circle we drew). This is impossible because when we wrap around the Möbius strip, we will be on the other side of the Möbius strip, and thus, to be a well-defined section, we must cross the zero section. This line represents a section and thus, every section has a zero. This is how we know that this line bundle is not trivial. So every section vanishes. In fact, we know that a line bundle is trivial if it has nonvanishing section. Since we can construct a trivialization by considering the map from $X$ (where $X$ is the variety or scheme we are working over) $L$ (the line bundle) given by $L$ maps to $X \times \mathbb{A}^1$ by $(x, y)$ maps to $(yf(x))$, where $f$ is the nonvanishing regular section. This gives an isomorphism. Hence, the zeros or poles of the section really gives us the interesting information of a line bundle. This is really the significance of this Picard group construction. To any set of poles and zeros, i.e. a divisor we may associate a line bundle $O(D)$, and this gives an isomorphism if we mod out by linear equivalence (i.e. multiplication by rational sections over the variety). Hence, the line bundle is uniquely determined by zeros and poles up to adding the zeros and poles achievable by rational functions on $X$. In general, the statement ""nontrivial if and only if vanishing sections"" is obvious by monodromy. And one could formally understand the tensor/Hom adjunction used in the Picard construction. But what does it mean really? The Picard group, the way I think of it, is the set of all divisors. Group of all divisors finite linear combinations of coding simon $1$ subvarieties, i.e. points if we are working over a curve. And these correspond to zeros and poles of functions. To which we could ask, is this accurate? Isn't the Picard group more analogous to the class group? And even then, that is not always an isomorphism. To which we would respond, yes that is true. The class group which is the group we are describing above quotients out by $Z(f)$, which is us looking at the valuation of $a$, rational function $f$ at the set of all divisors, i.e. we determine its order of vanishing at all the different possible zeros and poles. But one is correct that it is not always an isomorphism. However, for smooth complete curves, it most certainly is, and that is what most of this theory is used for. We already have some issues if we do not work over algebraically closed fields because then we can not factor our polynomial all the way. (Wikipedia gives the construction explicitly of line bundles $\implies$ class group element, and the thing about nonvanishing global sections being trivial makes it clear why it is well-defined up to principal divisors.) The bad behavior comes when it is not a factorial scheme. So we ask, what is an example of a scheme where the germs are not unique factorization domains? We will have issues... for example, look at Weil and Cartier divisors, as Weil does not always imply Cartier . Now, in spite of all this rambling, I feel like I still do not have a deep geometric understanding as to why line bundles have inverses with respect to the tensor product. It's quite possible I'm missing a relatively simple way of thinking about it. Could someone assess my statements and tell me if they are correct and/or the right way to think about this problem, and possibly contribute some of their own intuitions/explanations? Thanks in advance.","Geometrically, why do line bundles have inverses with respect to the tensor product? Here my thoughts on the problem so far, please excuse their scatteredness. I know algebraically, it is just because they are locally modules generated by $1$ element. Basically, it is just the fact that if one has a finitely generated module $M$ over a local ring $R$, there exists $M^{-1}$ with $M \otimes M^{-1} = R$ only when $M$ is free of rank $1$. This is not hard to prove. Geometrically, we are taking a line bundle; we are sort of ""straightening the lines"" to make it trivial. In fact, line bundles are just trivial neighborhoods with transition functions to $\mathbb{A}^1$. We just take the reciprocal of the transition function, and that gives us the inverse line bundle. So one might think of the following: the tensor product corresponds to product of modules, and hence looks nice geometrically precisely when there is no ""torsion"" in a non-precise sence, i.e. locally free modules? Locally projective modules? Flat? (Projective is stronger than flat.) So those are morally vector bundles, and can be straightened consistently precisely when the rank equals $1$. As a counterthought to the above, the above isn't exactly true. ""Straightening"" only really makes sense in the rank $1$ case because the structure sheaf is itself a line bundle and not a higher vector bundle. So one can not, for obvious reasons, have a rank $2$ vector bundle that inverts another rank $2$ vector bundle. But in the case of a rank $1$, it is straightening the bundle, which can be done. For higher dimensional vector bundles, we can think of an ""inverse"" as $O^r$, but even then not all guys would be invertible (i.e. anything that does not split probably should not be invertible like this). Everything I have so far is rather verbose. On an intuitive level with respect to the original question, we can think of nontrivial line bundles as structures that force their sections to have zeros or poles. If we have a section that is regular and has no vanishing everywhere, we can use it to trivialize the line bundle. A nice geometric picture of this is the line bundle of the Möbius strip over the circle. Alternatively, we can always construct an inverse by taking the dual of a line bundle and then using the adjoint properties of the tensor product to show its an inverse. If we take a section, there will have to be some point in which the section crosses over the zero section. If we can find some section that has poles where the sections have zeros, their tensor product, which will have sections behaving like the product of the $2$ original sections will be regular. So now, we have to visualize what taking the dual means, or sending zeros to poles. Geometrically, this is kind of like flipping over our line bundle and gluing everything together at infinity. This makes the zeros become poles. But this is all still very algebraic. Anyways, we certainly know how it works for higher dimensional things because the tensor product multiplies the dimension of the vector space. At this point, one might wonder, what do I mean by ""if we take a section, there will have to be some point in which the section crosses over the zero section."" Why does a global section have to intersect the zero section? Think about the Möbius strip. Draw a circle in the middle; this is isomorphic to the circle. Thus, the Möbius strip is a line bundle over the circle (well, we need to extend the Möbius strip out to infinity). Now, draw some line along the Möbius strip that tries to avoid intersecting the zero section (the circle we drew). This is impossible because when we wrap around the Möbius strip, we will be on the other side of the Möbius strip, and thus, to be a well-defined section, we must cross the zero section. This line represents a section and thus, every section has a zero. This is how we know that this line bundle is not trivial. So every section vanishes. In fact, we know that a line bundle is trivial if it has nonvanishing section. Since we can construct a trivialization by considering the map from $X$ (where $X$ is the variety or scheme we are working over) $L$ (the line bundle) given by $L$ maps to $X \times \mathbb{A}^1$ by $(x, y)$ maps to $(yf(x))$, where $f$ is the nonvanishing regular section. This gives an isomorphism. Hence, the zeros or poles of the section really gives us the interesting information of a line bundle. This is really the significance of this Picard group construction. To any set of poles and zeros, i.e. a divisor we may associate a line bundle $O(D)$, and this gives an isomorphism if we mod out by linear equivalence (i.e. multiplication by rational sections over the variety). Hence, the line bundle is uniquely determined by zeros and poles up to adding the zeros and poles achievable by rational functions on $X$. In general, the statement ""nontrivial if and only if vanishing sections"" is obvious by monodromy. And one could formally understand the tensor/Hom adjunction used in the Picard construction. But what does it mean really? The Picard group, the way I think of it, is the set of all divisors. Group of all divisors finite linear combinations of coding simon $1$ subvarieties, i.e. points if we are working over a curve. And these correspond to zeros and poles of functions. To which we could ask, is this accurate? Isn't the Picard group more analogous to the class group? And even then, that is not always an isomorphism. To which we would respond, yes that is true. The class group which is the group we are describing above quotients out by $Z(f)$, which is us looking at the valuation of $a$, rational function $f$ at the set of all divisors, i.e. we determine its order of vanishing at all the different possible zeros and poles. But one is correct that it is not always an isomorphism. However, for smooth complete curves, it most certainly is, and that is what most of this theory is used for. We already have some issues if we do not work over algebraically closed fields because then we can not factor our polynomial all the way. (Wikipedia gives the construction explicitly of line bundles $\implies$ class group element, and the thing about nonvanishing global sections being trivial makes it clear why it is well-defined up to principal divisors.) The bad behavior comes when it is not a factorial scheme. So we ask, what is an example of a scheme where the germs are not unique factorization domains? We will have issues... for example, look at Weil and Cartier divisors, as Weil does not always imply Cartier . Now, in spite of all this rambling, I feel like I still do not have a deep geometric understanding as to why line bundles have inverses with respect to the tensor product. It's quite possible I'm missing a relatively simple way of thinking about it. Could someone assess my statements and tell me if they are correct and/or the right way to think about this problem, and possibly contribute some of their own intuitions/explanations? Thanks in advance.",,"['abstract-algebra', 'geometry', 'algebraic-geometry', 'commutative-algebra']"
83,Do polynomials in two variables always factor in linear terms?,Do polynomials in two variables always factor in linear terms?,,"Consider a polynomial of one variable over $\Bbb C$: $$p(x)=a_0+a_1x+\cdots+a_nx^n,\quad a_i\in\Bbb C.$$ We know from the Fundamental Theorem of Algebra that there exists $c,\alpha_i\in\Bbb C$ such that $$p(x)=c(x-\alpha_1)\cdots(x-\alpha_n),$$ i.e. we can factor $p$ in linear terms . Now, what about polynomials $p(x,y)$ in two variables? Is it still true that we can factor $p(x,y)$ in linear terms? I.e. can we always write   $$p(x,y)=c(\alpha_1x+\beta_1y+\gamma_1)\cdots(\alpha_nx+\beta_ny+\gamma_n)$$   for some $c,\alpha_i,\beta_i,\gamma_i\in\Bbb C$?","Consider a polynomial of one variable over $\Bbb C$: $$p(x)=a_0+a_1x+\cdots+a_nx^n,\quad a_i\in\Bbb C.$$ We know from the Fundamental Theorem of Algebra that there exists $c,\alpha_i\in\Bbb C$ such that $$p(x)=c(x-\alpha_1)\cdots(x-\alpha_n),$$ i.e. we can factor $p$ in linear terms . Now, what about polynomials $p(x,y)$ in two variables? Is it still true that we can factor $p(x,y)$ in linear terms? I.e. can we always write   $$p(x,y)=c(\alpha_1x+\beta_1y+\gamma_1)\cdots(\alpha_nx+\beta_ny+\gamma_n)$$   for some $c,\alpha_i,\beta_i,\gamma_i\in\Bbb C$?",,"['abstract-algebra', 'polynomials']"
84,Is quotient of a ring by a power of a maximal ideal local?,Is quotient of a ring by a power of a maximal ideal local?,,"Say I have a commutative ring $R$ with a maximal ideal $m$. Then $m/m^k$ is a maximal ideal in $R/m^k$ for any $k$. Is it the only maximal ideal, i.e. is $R/m^k$ a local ring? This is a well known result for $k = 1$, as $R/m$ is a field. It seems to be true in other cases, e.g. $p\mathbb{Z}\subset \mathbb{Z}$ for prime $p$ and for $(x,y) \subset \mathbb{F}[x,y]$, the maximal ideal generated by $x$ and $y$ in a polynomial ring over the field $\mathbb{F}$. Equivalently, if an element $x\notin m$, is $x + m^k$ an invertible element in $R/m^k$?","Say I have a commutative ring $R$ with a maximal ideal $m$. Then $m/m^k$ is a maximal ideal in $R/m^k$ for any $k$. Is it the only maximal ideal, i.e. is $R/m^k$ a local ring? This is a well known result for $k = 1$, as $R/m$ is a field. It seems to be true in other cases, e.g. $p\mathbb{Z}\subset \mathbb{Z}$ for prime $p$ and for $(x,y) \subset \mathbb{F}[x,y]$, the maximal ideal generated by $x$ and $y$ in a polynomial ring over the field $\mathbb{F}$. Equivalently, if an element $x\notin m$, is $x + m^k$ an invertible element in $R/m^k$?",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'ideals']"
85,Khan academy for abstract algebra,Khan academy for abstract algebra,,"I am looking for instructional videos for abstract algebra, specifically topics including group theory, ring theory, isomorphic and homomorphic structures, and properties of groups and rings, and hopefully basic proofs with narations. Does anybody have links where I could find anything like this? KA and youtube have yield poor to no results. Thanks, Tom","I am looking for instructional videos for abstract algebra, specifically topics including group theory, ring theory, isomorphic and homomorphic structures, and properties of groups and rings, and hopefully basic proofs with narations. Does anybody have links where I could find anything like this? KA and youtube have yield poor to no results. Thanks, Tom",,"['abstract-algebra', 'group-theory', 'ring-theory']"
86,Is a finitely generated projective module a direct summand of a *finitely generated* free module?,Is a finitely generated projective module a direct summand of a *finitely generated* free module?,,"Let $R$ be a (not necessarily commutative) ring and $P$ a finitely generated projective $R$-module. Then there is an $R$-module $N$ such that $P \oplus N$ is free. Can $N$ always be chosen such that $P \oplus N$ is free and finitely generated ? Equivalently: Is there always a finitely generated $N$ such that $P \oplus N$ is free? If the answer is ""no"": What can be said about the rings $R$ such that this property is true?","Let $R$ be a (not necessarily commutative) ring and $P$ a finitely generated projective $R$-module. Then there is an $R$-module $N$ such that $P \oplus N$ is free. Can $N$ always be chosen such that $P \oplus N$ is free and finitely generated ? Equivalently: Is there always a finitely generated $N$ such that $P \oplus N$ is free? If the answer is ""no"": What can be said about the rings $R$ such that this property is true?",,"['abstract-algebra', 'ring-theory', 'modules', 'noncommutative-algebra', 'projective-module']"
87,"Can ""Taking algebraic closure"" be made into a functor?","Can ""Taking algebraic closure"" be made into a functor?",,"I am now confused with such problem as title goes. To be exact, the problem is Does there exist a functor from $A:\mathsf{Field}\to \mathsf{Field}$ with a natural transformation from identity functor $\iota: \operatorname{id}\to A$ such that for each $F$ , $A(F)$ is the algebraically closure of $F$ through $\iota_F:F\to A(F)$ ? It is not easy rather than first glimpse. Let me explain. Note that, the existence of algebraic closure only ensures that there exist a map from $\operatorname{Obj}(\mathsf{Field})$ to itself. Since the ""extension"" property is not unique, it is not generally true that we can extend the map to $\operatorname{Mor}(\mathsf{Field})$ for arbitrary choice of algebraic closure. For example, consider the fields $$\begin{array}{ccc} \mathbb{Q}[\sqrt[3]{2}, \sqrt{2}] &\to & \mathbb{Q}[\omega\sqrt[3]{2}, \sqrt{2}]\\ \uparrow &&\uparrow \\ \mathbb{Q}[\sqrt[3]{2}] & \to & \mathbb{Q}[\omega\sqrt[3]{2}] \end{array}$$ If we choose the algebraically closure of $\left[\begin{matrix}\mathbb{Q}[\sqrt[3]{2}, \sqrt{2}] & \mathbb{Q}[\omega\sqrt[3]{2}, \sqrt{2}]\\ & \mathbb{Q}[\omega\sqrt[3]{2}]\end{matrix}\right]$ by inclusion to $\overline{\mathbb{Q}}$ , and the closure of $\mathbb{Q}[\sqrt[3]{2}]\to \overline{\mathbb{Q}}$ by $\sqrt[3]{2}\mapsto \omega\sqrt[3]{2}$ .  We cannot extend a well-defined functor. Similar problem exists for transcendental extension, for example, square like this $$\begin{array}{ccc} \mathbb{C}[X,Y] &\to & \mathbb{C}[X^2,Y]\\ \uparrow &&\uparrow \\ \mathbb{C}[X] & \to & \mathbb{C}[X^2] \end{array}$$ A reasonable method is to avoid phenomenon above is as follow.  Fix an algebraically closed field $F$ , and take all of its subfields as ""skeleton"", then fix an isomorphism to a subfields of $F$ from all fields whose algebraic closure is $F$ up to an isomorphism. The isomorphic class of algebraically closure are completely dependen by its characteristic and the transcendental dimension over prime field $\mathbb{Q}$ or $\mathbb{F}_p$ . Now the problem is how to naturally chose extensions for endmorphisms. But unfortunately, the choice is fragile. For instance, consider the following diagram $$\begin{array}{ccccl} \mathbb{Q}[\sqrt{3}, \sqrt{2}] &\to & \mathbb{Q}[\sqrt{3}, \sqrt{2}] &: &\sqrt{3}\mapsto -\sqrt{3},\sqrt{2}\mapsto \pm \sqrt{2}\\ \uparrow &&\uparrow \\ \mathbb{Q}[\sqrt{3}] & \to & \mathbb{Q}[\sqrt{3}] &:&\sqrt{3}\mapsto -\sqrt{3} \end{array}$$ There is no suitable choice such that $$\begin{array}{ccccl} \overline{\mathbb{Q}} &\to & \overline{\mathbb{Q}} &: &\sqrt{3}\mapsto -\sqrt{3},\sqrt{2}\mapsto \pm \sqrt{2}\\ \parallel &&\parallel \\ \overline{\mathbb{Q}}& \to & \overline{\mathbb{Q}} &:&\sqrt{3}\mapsto -\sqrt{3} \end{array}$$ commutes for both $\pm=+$ and $\pm=-$ .","I am now confused with such problem as title goes. To be exact, the problem is Does there exist a functor from with a natural transformation from identity functor such that for each , is the algebraically closure of through ? It is not easy rather than first glimpse. Let me explain. Note that, the existence of algebraic closure only ensures that there exist a map from to itself. Since the ""extension"" property is not unique, it is not generally true that we can extend the map to for arbitrary choice of algebraic closure. For example, consider the fields If we choose the algebraically closure of by inclusion to , and the closure of by .  We cannot extend a well-defined functor. Similar problem exists for transcendental extension, for example, square like this A reasonable method is to avoid phenomenon above is as follow.  Fix an algebraically closed field , and take all of its subfields as ""skeleton"", then fix an isomorphism to a subfields of from all fields whose algebraic closure is up to an isomorphism. The isomorphic class of algebraically closure are completely dependen by its characteristic and the transcendental dimension over prime field or . Now the problem is how to naturally chose extensions for endmorphisms. But unfortunately, the choice is fragile. For instance, consider the following diagram There is no suitable choice such that commutes for both and .","A:\mathsf{Field}\to \mathsf{Field} \iota: \operatorname{id}\to A F A(F) F \iota_F:F\to A(F) \operatorname{Obj}(\mathsf{Field}) \operatorname{Mor}(\mathsf{Field}) \begin{array}{ccc}
\mathbb{Q}[\sqrt[3]{2}, \sqrt{2}] &\to & \mathbb{Q}[\omega\sqrt[3]{2}, \sqrt{2}]\\
\uparrow &&\uparrow \\
\mathbb{Q}[\sqrt[3]{2}] & \to & \mathbb{Q}[\omega\sqrt[3]{2}]
\end{array} \left[\begin{matrix}\mathbb{Q}[\sqrt[3]{2}, \sqrt{2}] & \mathbb{Q}[\omega\sqrt[3]{2}, \sqrt{2}]\\ & \mathbb{Q}[\omega\sqrt[3]{2}]\end{matrix}\right] \overline{\mathbb{Q}} \mathbb{Q}[\sqrt[3]{2}]\to \overline{\mathbb{Q}} \sqrt[3]{2}\mapsto \omega\sqrt[3]{2} \begin{array}{ccc}
\mathbb{C}[X,Y] &\to & \mathbb{C}[X^2,Y]\\
\uparrow &&\uparrow \\
\mathbb{C}[X] & \to & \mathbb{C}[X^2]
\end{array} F F F \mathbb{Q} \mathbb{F}_p \begin{array}{ccccl}
\mathbb{Q}[\sqrt{3}, \sqrt{2}] &\to & \mathbb{Q}[\sqrt{3}, \sqrt{2}] &: &\sqrt{3}\mapsto -\sqrt{3},\sqrt{2}\mapsto \pm \sqrt{2}\\
\uparrow &&\uparrow \\
\mathbb{Q}[\sqrt{3}] & \to & \mathbb{Q}[\sqrt{3}] &:&\sqrt{3}\mapsto -\sqrt{3}
\end{array} \begin{array}{ccccl}
\overline{\mathbb{Q}} &\to & \overline{\mathbb{Q}} &: &\sqrt{3}\mapsto -\sqrt{3},\sqrt{2}\mapsto \pm \sqrt{2}\\
\parallel &&\parallel \\
\overline{\mathbb{Q}}& \to & \overline{\mathbb{Q}} &:&\sqrt{3}\mapsto -\sqrt{3}
\end{array} \pm=+ \pm=-","['abstract-algebra', 'field-theory', 'category-theory']"
88,A finite group such that every element is conjugate to its square is trivial,A finite group such that every element is conjugate to its square is trivial,,"Suppose $G$ is a finite group such that $g$ is conjugate to $g^2$ for every $g\in G$. Here's a proof that $G$ is trivial.  First, observe that if $\lvert G\rvert$ is even, then $G$ contains an element $h$ of order $2$, in which case, $h$ is conjugate to $h^2=1$.  But this implies that $h=1$, so $h$ does not have order $2$.  By contradiction, $\lvert G\rvert$ is odd.  Then, by the Feit–Thompson theorem, $G$ is solvable.  In particular, this means that the derived series of $G$ terminates.  However, for any $g$ in $G$, there exists $a\in G$ such that $g^2=aga^{-1}$, i.e., $g=aga^{-1}g^{-1}\in G^{(1)}$.  It follows that $G^{(1)}=G$.  In fact, this shows that $G^{(n)}=G$ for all $n\geq 1$.  Since the derived series of $G$ terminates, this implies that $G$ must be trivial. While I'm convinced of the result, this proof is not particularly satisfying to me, since it relies on Feit-Thompson.  Is there an elementary proof that $G$ is trivial?","Suppose $G$ is a finite group such that $g$ is conjugate to $g^2$ for every $g\in G$. Here's a proof that $G$ is trivial.  First, observe that if $\lvert G\rvert$ is even, then $G$ contains an element $h$ of order $2$, in which case, $h$ is conjugate to $h^2=1$.  But this implies that $h=1$, so $h$ does not have order $2$.  By contradiction, $\lvert G\rvert$ is odd.  Then, by the Feit–Thompson theorem, $G$ is solvable.  In particular, this means that the derived series of $G$ terminates.  However, for any $g$ in $G$, there exists $a\in G$ such that $g^2=aga^{-1}$, i.e., $g=aga^{-1}g^{-1}\in G^{(1)}$.  It follows that $G^{(1)}=G$.  In fact, this shows that $G^{(n)}=G$ for all $n\geq 1$.  Since the derived series of $G$ terminates, this implies that $G$ must be trivial. While I'm convinced of the result, this proof is not particularly satisfying to me, since it relies on Feit-Thompson.  Is there an elementary proof that $G$ is trivial?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
89,Is $\mathbb Z[[X]]\otimes \mathbb Q$ isomorphic to $\mathbb Q[[X]]$?,Is  isomorphic to ?,\mathbb Z[[X]]\otimes \mathbb Q \mathbb Q[[X]],Is $\mathbb Z[[X]]\otimes \mathbb Q$ isomorphic to $\mathbb Q[[X]]$? Here tensor product is over the ring $\mathbb Z$ and $\mathbb Z[[X]] $ denotes formal power series over $\mathbb Z$. I think this is true if we take polynomial rings instead of power series. Any help in this regards will be appreciated.,Is $\mathbb Z[[X]]\otimes \mathbb Q$ isomorphic to $\mathbb Q[[X]]$? Here tensor product is over the ring $\mathbb Z$ and $\mathbb Z[[X]] $ denotes formal power series over $\mathbb Z$. I think this is true if we take polynomial rings instead of power series. Any help in this regards will be appreciated.,,"['abstract-algebra', 'commutative-algebra', 'tensor-products']"
90,Prove that a UFD is a PID if and only if every nonzero prime ideal is maximal [duplicate],Prove that a UFD is a PID if and only if every nonzero prime ideal is maximal [duplicate],,"This question already has answers here : A UFD is a PID if it has dimension $\le 1$ (nonzero prime ideals are maximal) (2 answers) Closed 7 months ago . Prove that a UFD is a PID if and only if every nonzero prime ideal is   maximal. The forward direction is standard, and the reverse direction is giving me trouble. In particular, I can prove that if every nonzero prime ideal is maximal then every maximal ideal is principal. From here, I know every ideal $I$ is contained in a maximal ideal, and that maximal ideal is principal, but I can't quite conclude that $I$ must also be principal. In particular, this is not true for sub-ideals of principal ideals in general rings, so I was looking at the set of all maximal ideals containing I, and trying to argue something about factorization but I can't quite get there. Any hints?","This question already has answers here : A UFD is a PID if it has dimension $\le 1$ (nonzero prime ideals are maximal) (2 answers) Closed 7 months ago . Prove that a UFD is a PID if and only if every nonzero prime ideal is   maximal. The forward direction is standard, and the reverse direction is giving me trouble. In particular, I can prove that if every nonzero prime ideal is maximal then every maximal ideal is principal. From here, I know every ideal $I$ is contained in a maximal ideal, and that maximal ideal is principal, but I can't quite conclude that $I$ must also be principal. In particular, this is not true for sub-ideals of principal ideals in general rings, so I was looking at the set of all maximal ideals containing I, and trying to argue something about factorization but I can't quite get there. Any hints?",,"['abstract-algebra', 'principal-ideal-domains', 'unique-factorization-domains']"
91,When $X^n-a$ is irreducible over F?,When  is irreducible over F?,X^n-a,"Let $F$ be a field, let $\omega$ be a primitive $n$ th root of unity in an algebraic closure of $F$ . If $a \in F$ is not an $m$ th power in $F(\omega)$ for any $m\gt 1$ that divides $n$ , how to show that $x^n -a$ is irreducible over $F$ ?","Let be a field, let be a primitive th root of unity in an algebraic closure of . If is not an th power in for any that divides , how to show that is irreducible over ?",F \omega n F a \in F m F(\omega) m\gt 1 n x^n -a F,"['abstract-algebra', 'polynomials', 'field-theory', 'irreducible-polynomials']"
92,How can I prove irreducibility of polynomial over a finite field?,How can I prove irreducibility of polynomial over a finite field?,,"I want to prove what $x^{10} +x^3+1$ is irreducible over a field $\mathbb F_{2}$ and $x^5$ + $x^4 +x^3 + x^2 +x -1$ is reducible over $\mathbb F_{3}$. As far as I know Eisenstein criteria won't help here. I have heard about a criteria of irreducibility which sounds like ""If $f(x)$ divides $x^{p^n-1}-1$ (where $n=\deg(f))$, $p$ is from $\mathbb F_{p}$ then $f(x)$ is irreducible."" But I don't know how to show if $f(x)$ can be divided by this or not.","I want to prove what $x^{10} +x^3+1$ is irreducible over a field $\mathbb F_{2}$ and $x^5$ + $x^4 +x^3 + x^2 +x -1$ is reducible over $\mathbb F_{3}$. As far as I know Eisenstein criteria won't help here. I have heard about a criteria of irreducibility which sounds like ""If $f(x)$ divides $x^{p^n-1}-1$ (where $n=\deg(f))$, $p$ is from $\mathbb F_{p}$ then $f(x)$ is irreducible."" But I don't know how to show if $f(x)$ can be divided by this or not.",,"['abstract-algebra', 'polynomials', 'finite-fields', 'irreducible-polynomials']"
93,Automorphism group of direct product of groups,Automorphism group of direct product of groups,,"I was working on a problem in group theory, which asks about the automorphism group of a direct product of groups. Okay, so I know that if $G,H$ are two groups whose orders are relatively prime, then $\operatorname{Aut}(G\times H)\cong\operatorname{Aut}(G)\times \operatorname{Aut}(H)$. I also know that if $G,H$ are abelian and simple and isomorphic, then $\operatorname{Aut}(G\times H)\cong \operatorname{GL}_2(\mathbf{Z}_p)$ where $p$ is the order of $G$. What I don't know is the following: what if $G,H$ are simple, yet not abelian? Let us for now focus on a more restricted case where $G\cong H$, in particular, $G=H$. What can we say about the group $\operatorname{Aut}(G\times H)$? (I heard something about this group being related to what is called ""wreath product"", which I do not know...) Thanks bunch in advance!","I was working on a problem in group theory, which asks about the automorphism group of a direct product of groups. Okay, so I know that if $G,H$ are two groups whose orders are relatively prime, then $\operatorname{Aut}(G\times H)\cong\operatorname{Aut}(G)\times \operatorname{Aut}(H)$. I also know that if $G,H$ are abelian and simple and isomorphic, then $\operatorname{Aut}(G\times H)\cong \operatorname{GL}_2(\mathbf{Z}_p)$ where $p$ is the order of $G$. What I don't know is the following: what if $G,H$ are simple, yet not abelian? Let us for now focus on a more restricted case where $G\cong H$, in particular, $G=H$. What can we say about the group $\operatorname{Aut}(G\times H)$? (I heard something about this group being related to what is called ""wreath product"", which I do not know...) Thanks bunch in advance!",,"['abstract-algebra', 'group-theory', 'automorphism-group', 'direct-product']"
94,Expressing a root of a polynomial as a rational function of another root,Expressing a root of a polynomial as a rational function of another root,,"Is there an easy way to tell how many roots $f(x)$ has in $\Bbb{Q}[x]/(f)$ given the coefficients of the polynomial $f$ in $\Bbb{Q}[x]$? Is there an easy way to find the roots as rational expressions in $x$? The easiest example is a pure quadratic: $X^2 + 7$ for instance.  If $A$ is a root, then so is $−A$.  Good ole $\pm\sqrt{−7}$. If the Galois group is abelian (like for any quadratic), then all of the roots can be expressed as polynomials in a given root.  However, I am not sure how to tell by looking at the polynomial if its Galois group is abelian, and even if it is, I am not sure how to find those rational expressions for the other roots. It might help to see some non-Abelian (non-Galois) examples: If $A$ is a root of $X^6 + 2X^4 − 8$, then $−A$ is also a root, but its other $4$ roots cannot be expressed as rational functions of $A$ (assuming I still understand Galois theory). Is there some easy way (not asking a CAS to calculate the Galois group) to see the other $4$ roots of of $X^6 + 2X^4 − 8$ cannot be expressed as rational functions of $A$? This one had the nice feature that it was a function of $X^2$, so it was easy to find two roots.  For $X^6 − 2X^5 + 3X^3 − 2X − 1$, I still have not found its other root (even using a CAS). If $A$ is a root of $X^6 − 2X^5 + 3X^3 − 2X − 1$, then what is a rational expression in $A$ for another root? This all first came up with the polynomial $x^4−4x^2+2$, where several distinct ad hoc arguments each sufficed, but I had no real understanding of how to even tell if my ad hoc arguments were worth trying on other polynomials.  If it helps, the roots are $A$, $−A$, $A^3−3A$, and $3A−A^3$. The context is hand calculations and reasonable degrees (say $\leq 10$), though I am not opposed to having a polynomial evaluation oracle that computes $f(g(x)) \mod f(x)$ in $1$ second (so ""try this finite and not too big list of possible roots"" is ok). If someone is interested, I am curious what the normalizer of a point stabilizer in the Galois group actually means in terms of Galois theory.  The index of the point stabilizer in its normalizer is the number of roots of $f$ in $\Bbb{Q}[x]/(f)$, but I'm not sure if it really means anything useful.","Is there an easy way to tell how many roots $f(x)$ has in $\Bbb{Q}[x]/(f)$ given the coefficients of the polynomial $f$ in $\Bbb{Q}[x]$? Is there an easy way to find the roots as rational expressions in $x$? The easiest example is a pure quadratic: $X^2 + 7$ for instance.  If $A$ is a root, then so is $−A$.  Good ole $\pm\sqrt{−7}$. If the Galois group is abelian (like for any quadratic), then all of the roots can be expressed as polynomials in a given root.  However, I am not sure how to tell by looking at the polynomial if its Galois group is abelian, and even if it is, I am not sure how to find those rational expressions for the other roots. It might help to see some non-Abelian (non-Galois) examples: If $A$ is a root of $X^6 + 2X^4 − 8$, then $−A$ is also a root, but its other $4$ roots cannot be expressed as rational functions of $A$ (assuming I still understand Galois theory). Is there some easy way (not asking a CAS to calculate the Galois group) to see the other $4$ roots of of $X^6 + 2X^4 − 8$ cannot be expressed as rational functions of $A$? This one had the nice feature that it was a function of $X^2$, so it was easy to find two roots.  For $X^6 − 2X^5 + 3X^3 − 2X − 1$, I still have not found its other root (even using a CAS). If $A$ is a root of $X^6 − 2X^5 + 3X^3 − 2X − 1$, then what is a rational expression in $A$ for another root? This all first came up with the polynomial $x^4−4x^2+2$, where several distinct ad hoc arguments each sufficed, but I had no real understanding of how to even tell if my ad hoc arguments were worth trying on other polynomials.  If it helps, the roots are $A$, $−A$, $A^3−3A$, and $3A−A^3$. The context is hand calculations and reasonable degrees (say $\leq 10$), though I am not opposed to having a polynomial evaluation oracle that computes $f(g(x)) \mod f(x)$ in $1$ second (so ""try this finite and not too big list of possible roots"" is ok). If someone is interested, I am curious what the normalizer of a point stabilizer in the Galois group actually means in terms of Galois theory.  The index of the point stabilizer in its normalizer is the number of roots of $f$ in $\Bbb{Q}[x]/(f)$, but I'm not sure if it really means anything useful.",,"['abstract-algebra', 'polynomials', 'galois-theory']"
95,"If $G \oplus H$ is isomorphic to a proper subgroup of itself, then must the same be true of one of $G$ and $H$?","If  is isomorphic to a proper subgroup of itself, then must the same be true of one of  and ?",G \oplus H G H,"Let $G$ and $H$ are groups. If $G \oplus H$ is isomorphic to a proper subgroup of itself, then must the same be true of one of $G$ and $H$ ? I found some examples of $G$ such that $G$ has no proper subgroup isomorphic to $G$ . For example, $\mathbb{Q}$ and $\mathbb{Q}\oplus \mathbb{Q}$ has no proper  subgroup isomorphic to each mother group. (The reason: If $f:\mathbb{Q}\oplus \mathbb{Q}\rightarrow\mathbb{Q}\oplus \mathbb{Q}$ is injective group homomorphism, then $f$ is also $\mathbb{Q}$ -module homomorphism, so $f$ is $\mathbb{Q}$ -linear map. So, injectivity of $f$ implies surjectivity of $f$ . This means $\mathbb{Q}\oplus \mathbb{Q}$ has no isomorphic subgroup.) I think there is counter-example of this claim but I can't choose one.. How to prove or take counter-example?","Let and are groups. If is isomorphic to a proper subgroup of itself, then must the same be true of one of and ? I found some examples of such that has no proper subgroup isomorphic to . For example, and has no proper  subgroup isomorphic to each mother group. (The reason: If is injective group homomorphism, then is also -module homomorphism, so is -linear map. So, injectivity of implies surjectivity of . This means has no isomorphic subgroup.) I think there is counter-example of this claim but I can't choose one.. How to prove or take counter-example?",G H G \oplus H G H G G G \mathbb{Q} \mathbb{Q}\oplus \mathbb{Q} f:\mathbb{Q}\oplus \mathbb{Q}\rightarrow\mathbb{Q}\oplus \mathbb{Q} f \mathbb{Q} f \mathbb{Q} f f \mathbb{Q}\oplus \mathbb{Q},"['abstract-algebra', 'group-theory', 'group-isomorphism', 'direct-product']"
96,"If $A[X] \cong B[X]$ as rings, are the degrees of irreducible polynomials the same in $A$ and in $B$?","If  as rings, are the degrees of irreducible polynomials the same in  and in ?",A[X] \cong B[X] A B,"First, I ask my question and then I add some explanations: Suppose that $A$ and $B$ are two commutative rings such that $A[X] \cong B[X]$ as rings. Denote by $D_A$ the set of all positive integers $n$ such that there exists an irreducible polynomial of degree $n$ over $A$ — the same for $D_B$. Is it true that $D_A = D_B$? Some time ago, I wanted to find many proofs (like here ) that $\Bbb Z[X]$ and $\Bbb R[X]$ are not isomorphic (obviously they are not because they don't even have the same cardinality, I know). I thought to the following argument: ""The irreducible polynomials in $\Bbb R[X]$ have degree $≤2$, while irreducible polynomials in $\Bbb Z[X]$ can have arbitrary large degree (for instance $X^n+2X^{n-1}+\cdots+2X+2$, by Eisenstein's criterion)"". But I wasn't sure of the correctness of this argument. The isomorphism $A[X] \cong B[X]$ is not required to preserve the degree. If it is preserved, then my claim should be true. I think that examples like this could prevent the isomorphism from preserving the degree. A possibly relevant question is What are the possible sets of degrees of irreducible polynomials over a field? , on MO. In particular, this can be interesting when $D_A$ and $D_B$ are infinite. Thank you for your comments!","First, I ask my question and then I add some explanations: Suppose that $A$ and $B$ are two commutative rings such that $A[X] \cong B[X]$ as rings. Denote by $D_A$ the set of all positive integers $n$ such that there exists an irreducible polynomial of degree $n$ over $A$ — the same for $D_B$. Is it true that $D_A = D_B$? Some time ago, I wanted to find many proofs (like here ) that $\Bbb Z[X]$ and $\Bbb R[X]$ are not isomorphic (obviously they are not because they don't even have the same cardinality, I know). I thought to the following argument: ""The irreducible polynomials in $\Bbb R[X]$ have degree $≤2$, while irreducible polynomials in $\Bbb Z[X]$ can have arbitrary large degree (for instance $X^n+2X^{n-1}+\cdots+2X+2$, by Eisenstein's criterion)"". But I wasn't sure of the correctness of this argument. The isomorphism $A[X] \cong B[X]$ is not required to preserve the degree. If it is preserved, then my claim should be true. I think that examples like this could prevent the isomorphism from preserving the degree. A possibly relevant question is What are the possible sets of degrees of irreducible polynomials over a field? , on MO. In particular, this can be interesting when $D_A$ and $D_B$ are infinite. Thank you for your comments!",,"['abstract-algebra', 'polynomials', 'ring-theory', 'irreducible-polynomials']"
97,Prove that the additive group $ℚ$ is not isomorphic with the multiplicative group $ℚ^*$. [duplicate],Prove that the additive group  is not isomorphic with the multiplicative group . [duplicate],ℚ ℚ^*,This question already has answers here : Group of positive rationals under multiplication not isomorphic to group of rationals (4 answers) Closed 5 years ago . Prove that the additive group $ℚ$ is not isomorphic with the multiplicative group $ℚ^*$. Prove that $ℚ^*_{>0}$ is not isomorphic with $ℚ$.,This question already has answers here : Group of positive rationals under multiplication not isomorphic to group of rationals (4 answers) Closed 5 years ago . Prove that the additive group $ℚ$ is not isomorphic with the multiplicative group $ℚ^*$. Prove that $ℚ^*_{>0}$ is not isomorphic with $ℚ$.,,"['abstract-algebra', 'group-theory']"
98,Quick way to find the number of the group homomorphisms $\phi:{\bf Z}_3\to{\bf Z}_6$?,Quick way to find the number of the group homomorphisms ?,\phi:{\bf Z}_3\to{\bf Z}_6,"Consider the following multiple choice problem: Let $H$ be the set of all group homomorphsims $\phi:{\bf Z}_3\to{\bf Z}_6$ . How many functions does $H$ contain? A.1  B.2 C.3 D.4 E.6 Since $1$ generates ${\bf Z}_3$ , one can analyze $\phi(1)$ case by case, which may be rather time consuming, for me, at least. Since this is a multiple choice problem, is there any quick way to solve it?","Consider the following multiple choice problem: Let be the set of all group homomorphsims . How many functions does contain? A.1  B.2 C.3 D.4 E.6 Since generates , one can analyze case by case, which may be rather time consuming, for me, at least. Since this is a multiple choice problem, is there any quick way to solve it?",H \phi:{\bf Z}_3\to{\bf Z}_6 H 1 {\bf Z}_3 \phi(1),['abstract-algebra']
99,Reference to self-study Abstract Algebra and Category Theory,Reference to self-study Abstract Algebra and Category Theory,,"I'm very interested in learning abstract algebra and category theory on my own. It seems a very powerful tool in math and it seems worthwile to take a time and learn about it. I just don't know even where to begin. Can someone point out for me what are good references to self-study those topics ? I'm really beginner, the only thing connected to algebra that I'm familiar with is linear algebra. Thanks very much in advance. Edit: Until now I've studied analytic geometry, single variable calculus, multivariable calculus, linear algebra, ordinary differential equations and I'm currently studying differential geometry and multilinear algebra.","I'm very interested in learning abstract algebra and category theory on my own. It seems a very powerful tool in math and it seems worthwile to take a time and learn about it. I just don't know even where to begin. Can someone point out for me what are good references to self-study those topics ? I'm really beginner, the only thing connected to algebra that I'm familiar with is linear algebra. Thanks very much in advance. Edit: Until now I've studied analytic geometry, single variable calculus, multivariable calculus, linear algebra, ordinary differential equations and I'm currently studying differential geometry and multilinear algebra.",,"['abstract-algebra', 'reference-request', 'category-theory']"
