,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Fourier transform of $1/ \sqrt{m^2+p_1^2+p_2^2+p_3^2}$,Fourier transform of,1/ \sqrt{m^2+p_1^2+p_2^2+p_3^2},"Let $m>0$ and consider the function $f:\mathbb R^3\to\mathbb C$ defined through $$ f(p_1,p_2,p_3) = \frac{1}{\sqrt{m^2+p_1^2+p_2^2+p_3^2}}.$$ I would like to compute the Fourier transform of $f$ . This particular function is of interest as one which naturally appears in some problems of special relativity. What I already know: Although $f$ is neither integrable nor square integrable, the Fourier transform of $f$ is well defined as the Fourier transform of a tempered distribution. Using symbolic calculus software, I expect that $\int_{-\infty}^{+\infty}f(p_1,p_2,p_3) e^{-i x_1p_1} dp_1 = 2K_0(x_1 \sqrt{m^2+p_2^2+p_3^2})$ , where $K_0$ is a modified Bessel function of the second kind. Questions: Is the Fourier transform of $f$ explicitly computable? If it is, how could I compute it?","Let and consider the function defined through I would like to compute the Fourier transform of . This particular function is of interest as one which naturally appears in some problems of special relativity. What I already know: Although is neither integrable nor square integrable, the Fourier transform of is well defined as the Fourier transform of a tempered distribution. Using symbolic calculus software, I expect that , where is a modified Bessel function of the second kind. Questions: Is the Fourier transform of explicitly computable? If it is, how could I compute it?","m>0 f:\mathbb R^3\to\mathbb C  f(p_1,p_2,p_3) = \frac{1}{\sqrt{m^2+p_1^2+p_2^2+p_3^2}}. f f f \int_{-\infty}^{+\infty}f(p_1,p_2,p_3) e^{-i x_1p_1} dp_1 = 2K_0(x_1 \sqrt{m^2+p_2^2+p_3^2}) K_0 f","['multivariable-calculus', 'definite-integrals', 'fourier-transform', 'multiple-integral']"
1,Why exactly can you change the order of integration in a double (and triple) integral?,Why exactly can you change the order of integration in a double (and triple) integral?,,"I'm currently studying multivariable Calculus doing double and triple integrals, and I'm slightly confused on why one can change the order of integration for a double integral. I have my own explanation, but it could be wrong. Is the following reasoning correct? Fubini's Theorem states that the double integral over a given 2D region where at least one of the variables has constants as their highest and lowest values (called a horizontally or vertically simple region, depending on which variable has the constants) is equal to the iterated integral where those constants are the outer integral's limits of integration (and the inner integral's limits are functions of the outer variable). My reasoning is: If the 2D region is describable as an equation of your two variables, say x and y, and the equation is separable, then you could simply solve for either x or y to get the inner integral's limits of integration, then see from your separated equation what endpoint values the other variable could take on. The 2D region doesn't necessarily have to have abrupt, flat lines as the endpoints for the outside-integral variable (as is depicted in pictures of these such regions in textbooks), but it does need to have endpoint values in terms of the outside variable that are constants. So, if the region doesn't have these abrupt lines to describe its endpoints for either variable, but either variable's endpoint values could be described as constants, then you could use any order of integration you like as long as your outer iterated integral has constants as the limits of integration. Is this correct, or does Fubini's Theorem explicitly state that any order of integration is usable (as long as the limits of integration are valid)? Also, in the proof for Fubini's Theorem for Triple Integrals, does the same logic essentially hold or is there another reason you can switch the order of integration? Thank you for taking the time to read this post!","I'm currently studying multivariable Calculus doing double and triple integrals, and I'm slightly confused on why one can change the order of integration for a double integral. I have my own explanation, but it could be wrong. Is the following reasoning correct? Fubini's Theorem states that the double integral over a given 2D region where at least one of the variables has constants as their highest and lowest values (called a horizontally or vertically simple region, depending on which variable has the constants) is equal to the iterated integral where those constants are the outer integral's limits of integration (and the inner integral's limits are functions of the outer variable). My reasoning is: If the 2D region is describable as an equation of your two variables, say x and y, and the equation is separable, then you could simply solve for either x or y to get the inner integral's limits of integration, then see from your separated equation what endpoint values the other variable could take on. The 2D region doesn't necessarily have to have abrupt, flat lines as the endpoints for the outside-integral variable (as is depicted in pictures of these such regions in textbooks), but it does need to have endpoint values in terms of the outside variable that are constants. So, if the region doesn't have these abrupt lines to describe its endpoints for either variable, but either variable's endpoint values could be described as constants, then you could use any order of integration you like as long as your outer iterated integral has constants as the limits of integration. Is this correct, or does Fubini's Theorem explicitly state that any order of integration is usable (as long as the limits of integration are valid)? Also, in the proof for Fubini's Theorem for Triple Integrals, does the same logic essentially hold or is there another reason you can switch the order of integration? Thank you for taking the time to read this post!",,"['integration', 'multivariable-calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
2,Showing that the intersection of a cylinder and a plane is an ellipse,Showing that the intersection of a cylinder and a plane is an ellipse,,"One of the questions in my homework was: ""Show that the curve $\vec{r}(t)=\cos t \vec{i}+\sin t \vec{j}+(1-\cos t)\vec{k}$ is an ellipse by showing that it is the intersection of a cylinder and a plane. Find equations for the cylinder and the plane."" It is easy to see that the curve is the intersection of the cylinder $x^2 + y^2 =1$ and the plane $x+z=1$ . Maybe I'm misunderstanding, but the question makes it sound like the hard part is showing that the curve is the intersection of a cylinder and a plane and that once they are found it is obvious that the curve is an ellipse. I have no idea how I can prove that the curve is an ellipse. Since it is not parallel to the $xy$ plane (or any other ""conventional"" planes), I am having a hard time showing that it satisfies the equation of an ellipse. Edit: I should add that I have not taken a linear algebra course yet, so please bear that in mind when posting a solution/hint. (I’m only adding this because many of the multivariable calculus hints online assume I am familiar with linear algebra.)","One of the questions in my homework was: ""Show that the curve is an ellipse by showing that it is the intersection of a cylinder and a plane. Find equations for the cylinder and the plane."" It is easy to see that the curve is the intersection of the cylinder and the plane . Maybe I'm misunderstanding, but the question makes it sound like the hard part is showing that the curve is the intersection of a cylinder and a plane and that once they are found it is obvious that the curve is an ellipse. I have no idea how I can prove that the curve is an ellipse. Since it is not parallel to the plane (or any other ""conventional"" planes), I am having a hard time showing that it satisfies the equation of an ellipse. Edit: I should add that I have not taken a linear algebra course yet, so please bear that in mind when posting a solution/hint. (I’m only adding this because many of the multivariable calculus hints online assume I am familiar with linear algebra.)",\vec{r}(t)=\cos t \vec{i}+\sin t \vec{j}+(1-\cos t)\vec{k} x^2 + y^2 =1 x+z=1 xy,['multivariable-calculus']
3,prove that $3(a+b+c) \geq 8(a b c)^{1 / 3}+\left(\frac{a^{3}+b^{3}+c^{3}}{3}\right)^{1 / 3}$,prove that,3(a+b+c) \geq 8(a b c)^{1 / 3}+\left(\frac{a^{3}+b^{3}+c^{3}}{3}\right)^{1 / 3},"Question - Suppose a,b,c are positive real numbers , prove that $3(a+b+c) \geq 8(a b c)^{1 / 3}+\left(\frac{a^{3}+b^{3}+c^{3}}{3}\right)^{1 / 3}$ (Thailand $2006$ ) My attempt - we can assume that $a+b+c=1$ so we have to prove that $3 \geq 8(a b c)^{1 / 3}+\left(\frac{a^{3}+b^{3}+c^{3}}{3}\right)^{1 / 3}$ but i am not able to show that it is true.. then i tried some AM-GM on RHS but none of them work, i think this is most different inequalities i have came across so i did not know where to go . any help will be appreciated thankyou","Question - Suppose a,b,c are positive real numbers , prove that (Thailand ) My attempt - we can assume that so we have to prove that but i am not able to show that it is true.. then i tried some AM-GM on RHS but none of them work, i think this is most different inequalities i have came across so i did not know where to go . any help will be appreciated thankyou",3(a+b+c) \geq 8(a b c)^{1 / 3}+\left(\frac{a^{3}+b^{3}+c^{3}}{3}\right)^{1 / 3} 2006 a+b+c=1 3 \geq 8(a b c)^{1 / 3}+\left(\frac{a^{3}+b^{3}+c^{3}}{3}\right)^{1 / 3},"['multivariable-calculus', 'inequality', 'contest-math', 'holder-inequality', 'uvw']"
4,Explanation of notation used in chain rule,Explanation of notation used in chain rule,,"Let $U\subseteq\mathbb{R}^{n}$ and $V\subseteq\mathbb{R}^{m}$ be open. Let $f:U\subseteq\mathbb{R}^{n}\to\mathbb{R}$ and $g_{1},g_{2},\dotsc,g_{n}\colon V\subseteq\mathbb{R}^{m}\to\mathbb{R}$ be $n$ functions such that: \begin{equation*}     (g_{1}(x),g_{2}(x),\dotsc,g_{n}(x))\in U, \quad\forall\, x\in V. \end{equation*} Furthermore let $x_{0}\in V$ , and let $j$ be a number in $\{1,\dotsc,n\}$ . Assume that $f$ is differentiable at $y_{0}=(g_{1}(x_{0}),g_{2}(x_{0}),\dotsc,g_{n}(x_{0}))$ and that the partial derivative $\frac{\partial g_{i}}{\partial x_{j}}(x_{0})$ exists for all $i=1,\dotsc,n$ . Then, the partial derivative of $f\circ (g_{1},g_{2},\dotsc,g_{n})$ exists w.r.t. the $j$ th coordinate of $x$ , and: \begin{equation*}     \frac{\partial (f\circ g)}{\partial x_{j}}(x_{0})=\sum_{i=1}^{n}\frac{\partial f}{\partial y_{i}}(y_{0})\frac{\partial g_{i}}{\partial x_{j}}(x_{0}).  \end{equation*} My question : Is it true that $y_{i}=g_{i}(x_{0})$ ? I know that it is a dumb question, but I am just curious. Thanks in advance.","Let and be open. Let and be functions such that: Furthermore let , and let be a number in . Assume that is differentiable at and that the partial derivative exists for all . Then, the partial derivative of exists w.r.t. the th coordinate of , and: My question : Is it true that ? I know that it is a dumb question, but I am just curious. Thanks in advance.","U\subseteq\mathbb{R}^{n} V\subseteq\mathbb{R}^{m} f:U\subseteq\mathbb{R}^{n}\to\mathbb{R} g_{1},g_{2},\dotsc,g_{n}\colon V\subseteq\mathbb{R}^{m}\to\mathbb{R} n \begin{equation*}
    (g_{1}(x),g_{2}(x),\dotsc,g_{n}(x))\in U, \quad\forall\, x\in V.
\end{equation*} x_{0}\in V j \{1,\dotsc,n\} f y_{0}=(g_{1}(x_{0}),g_{2}(x_{0}),\dotsc,g_{n}(x_{0})) \frac{\partial g_{i}}{\partial x_{j}}(x_{0}) i=1,\dotsc,n f\circ (g_{1},g_{2},\dotsc,g_{n}) j x \begin{equation*}
    \frac{\partial (f\circ g)}{\partial x_{j}}(x_{0})=\sum_{i=1}^{n}\frac{\partial f}{\partial y_{i}}(y_{0})\frac{\partial g_{i}}{\partial x_{j}}(x_{0}). 
\end{equation*} y_{i}=g_{i}(x_{0})","['real-analysis', 'multivariable-calculus', 'notation', 'partial-derivative', 'chain-rule']"
5,An inequality involving homogeneous polynomials,An inequality involving homogeneous polynomials,,"Let $x_1, x_2, \dots x_k \ge 0$ be non-negative real numbers. Does it follow that $$k \left( \sum_{i=1}^k x_i^3 \right)^2 \ge \left( \sum_{i=1}^k x_i^2 \right)^3 ? $$ This seems like something that might easily follow from standard inequalities like Jensen's inequality? (Now I am embarrassed that I hadn't really carefully tried Jensen before asking. As a penance, I will post a solution using Jensen's inequality.)","Let be non-negative real numbers. Does it follow that This seems like something that might easily follow from standard inequalities like Jensen's inequality? (Now I am embarrassed that I hadn't really carefully tried Jensen before asking. As a penance, I will post a solution using Jensen's inequality.)","x_1, x_2, \dots x_k \ge 0 k \left( \sum_{i=1}^k x_i^3 \right)^2 \ge \left( \sum_{i=1}^k x_i^2 \right)^3 ? ","['multivariable-calculus', 'inequality', 'symmetric-polynomials', 'holder-inequality', 'jensen-inequality']"
6,Maximum point of a function,Maximum point of a function,,"I am looking to find the local maximum value of the function $$f(x,y,z) = 9+ \frac{(x-y)^2}{xy} + \frac{(y-z)^2}{yz}+ \frac{(z-x)^2}{zx}$$ at a point within the cube $[a,b] \times [a,b] \times [a,b],$ where $0<a<b$ . I took the partial derivative of f with respect to x, y, and z. I set those to zero. I used the second-derivative test for multi-variable functions. The second-derivative test requires the computation of a 3 by 3 matrix. (Because the function is a three-variable function) $D= \det \pmatrix{f_{xx} & f_{xy} &f_{xz} \\ f_{yx} & f_{yy} &f_{yz} \\ f_{zx} & f_{zy} &f_{zz}}$ I found that the first dervatives equal to zero at $x=y=z$ . On the other hand, Wolfram Alpha confirms that the max for the $[1,3] \times [1,3] \times [1,3]$ is for $y=z=\frac{x}{3}$ and $x=\frac{y}{3}=\frac{z}{3}$ . I got confused","I am looking to find the local maximum value of the function at a point within the cube where . I took the partial derivative of f with respect to x, y, and z. I set those to zero. I used the second-derivative test for multi-variable functions. The second-derivative test requires the computation of a 3 by 3 matrix. (Because the function is a three-variable function) I found that the first dervatives equal to zero at . On the other hand, Wolfram Alpha confirms that the max for the is for and . I got confused","f(x,y,z) = 9+ \frac{(x-y)^2}{xy} + \frac{(y-z)^2}{yz}+ \frac{(z-x)^2}{zx} [a,b] \times [a,b] \times [a,b], 0<a<b D= \det \pmatrix{f_{xx} & f_{xy} &f_{xz} \\
f_{yx} & f_{yy} &f_{yz}
\\
f_{zx} & f_{zy} &f_{zz}} x=y=z [1,3] \times [1,3] \times [1,3] y=z=\frac{x}{3} x=\frac{y}{3}=\frac{z}{3}","['calculus', 'multivariable-calculus', 'optimization', 'partial-derivative', 'maxima-minima']"
7,Finding the maxima of a multivariable function using Lagrange's Multipliers,Finding the maxima of a multivariable function using Lagrange's Multipliers,,"I'm practicing Lagrange Multipliers (LM) $^{[1]}$ with the following self-made question: Given $a + b + c + d + e = 1$ , where $a, b, c, d, e \notin R^-$ . Find the maximum value of $ab + bc + cd + de$ I already know that the answer is $1/4^{[2]}$ . But, as an exercise, I want to use LM. My Attempt: Let, $$ f(a, \ ... \ ,e) = ab + bc + cd + de \\ g(a, \ ... \ ,e) = a + b + c + d + e = 1 $$ Then, define: $$ \mathcal{L}(a, \ ... \ ,e, \lambda) = f(a, \ ... \ ,e) - \lambda \cdot [g(a, \ ...\ ,e) - 1] \\ \therefore \mathcal{L}(a, ... ,e, \lambda) = ab + bc + cd + de - \lambda \cdot [a + b + c + d + e - 1] $$ Now, $\nabla\mathcal{L} = 0$ would give maxima / minima. On partial differentiation, we get, $$ \begin{align} b = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta a}) \tag 1\\  d = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta e}) \tag 2\\  a + c = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta b}) \\  c + e = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta d}) \\  b + d = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta c}) \tag 3 \end{align}$$ Here, equations $(1)$ , $(2)$ and $(3)$ seems contradicting. Why it is so? Note: I tested the same approach with $2$ , $3$ and $4$ variables and it gave me correct results. Why so? References: [1]: Lagrange multipliers, examples - Khan Academy [2]: If $a,b,c,d,e,f$ are non negative real numbers such that $a+b+c+d+e+f=1$, then find maximum value of $ab+bc+cd+de+ef$","I'm practicing Lagrange Multipliers (LM) with the following self-made question: Given , where . Find the maximum value of I already know that the answer is . But, as an exercise, I want to use LM. My Attempt: Let, Then, define: Now, would give maxima / minima. On partial differentiation, we get, Here, equations , and seems contradicting. Why it is so? Note: I tested the same approach with , and variables and it gave me correct results. Why so? References: [1]: Lagrange multipliers, examples - Khan Academy [2]: If $a,b,c,d,e,f$ are non negative real numbers such that $a+b+c+d+e+f=1$, then find maximum value of $ab+bc+cd+de+ef$","^{[1]} a + b + c + d + e = 1 a, b, c, d, e \notin R^- ab + bc + cd + de 1/4^{[2]}  f(a, \ ... \ ,e) = ab + bc + cd + de \\
g(a, \ ... \ ,e) = a + b + c + d + e = 1   \mathcal{L}(a, \ ... \ ,e, \lambda) = f(a, \ ... \ ,e) - \lambda \cdot [g(a, \ ...\ ,e) - 1] \\
\therefore \mathcal{L}(a, ... ,e, \lambda) = ab + bc + cd + de - \lambda \cdot [a + b + c + d + e - 1]  \nabla\mathcal{L} = 0  \begin{align}
b = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta a}) \tag 1\\ 
d = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta e}) \tag 2\\ 
a + c = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta b}) \\ 
c + e = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta d}) \\ 
b + d = \lambda \qquad (from \ \ \frac{\delta\mathcal{L}}{\delta c}) \tag 3
\end{align} (1) (2) (3) 2 3 4","['multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
8,A misunderstanding on the independence of orientation of line integrals?,A misunderstanding on the independence of orientation of line integrals?,,"I've found the following problem: Calculate the line integral of the first kind $$\int_C (x+y)ds$$ Where $C$ is the contour of the triangle $ABO$ with the vertices at $A=(1,0),B=(0,1),C=(0,0)$ . In the previous presentation of the subject, it says that the line integral of the first kind is independent of the orientation. So, we compute first from $A$ to $B$ by acknowledging that this path is given by the line $y=1-x$ and in the solution, the author computes: $$\int_{0}^{1}\sqrt{2} dx$$ Doing this way, we get $\int_{0}^{1}\sqrt{2} dx=\sqrt{2}$ but here we are computing the value from $B$ to $A$ and not from $A$ to $B$ .  We should compute it as $$\int_{1}^{0}\sqrt{2} dx$$ But doing this way, it would yield $-\sqrt{2}$ and our line integral should be independent of the path of integration. How come we get two different results? I may have understood something wrong. Notice that I computed only the segment from $A$ to $B$ but looking at the solution, it doesn't seems the situation is going to change if I compute for the whole triangle.","I've found the following problem: Calculate the line integral of the first kind Where is the contour of the triangle with the vertices at . In the previous presentation of the subject, it says that the line integral of the first kind is independent of the orientation. So, we compute first from to by acknowledging that this path is given by the line and in the solution, the author computes: Doing this way, we get but here we are computing the value from to and not from to .  We should compute it as But doing this way, it would yield and our line integral should be independent of the path of integration. How come we get two different results? I may have understood something wrong. Notice that I computed only the segment from to but looking at the solution, it doesn't seems the situation is going to change if I compute for the whole triangle.","\int_C (x+y)ds C ABO A=(1,0),B=(0,1),C=(0,0) A B y=1-x \int_{0}^{1}\sqrt{2} dx \int_{0}^{1}\sqrt{2} dx=\sqrt{2} B A A B \int_{1}^{0}\sqrt{2} dx -\sqrt{2} A B","['multivariable-calculus', 'definite-integrals']"
9,Double integration from polars to Cartesians,Double integration from polars to Cartesians,,"I have the following integral over the unit circle $$\int_0^{2\pi}d\varphi\int_0^1 rdr \ \varphi =\pi^2$$ where $\varphi$ is the azimuthal angle and $r$ is the radial distance. If I try to convert this into Cartesian coordinates, I get $$\int_{-1}^1dx\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}dy \arctan{\frac{y}{x}}=0$$ what am I doing wrong in the conversion from polars to Cartesians? Thanks.","I have the following integral over the unit circle where is the azimuthal angle and is the radial distance. If I try to convert this into Cartesian coordinates, I get what am I doing wrong in the conversion from polars to Cartesians? Thanks.",\int_0^{2\pi}d\varphi\int_0^1 rdr \ \varphi =\pi^2 \varphi r \int_{-1}^1dx\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}dy \arctan{\frac{y}{x}}=0,"['integration', 'multivariable-calculus', 'coordinate-systems', 'polar-coordinates']"
10,Total and partial derivative terminology in scalar- and vector-valued functions of scalars/vectors,Total and partial derivative terminology in scalar- and vector-valued functions of scalars/vectors,,"I have the following terminology questions which are often not well addressed in an undergraduate multivariable calculus course. While the questions are long, I expect their answers will be short. As background, say I have two functions that depend on scalar variables $w$ , $x$ , $y$ , and $z$ which each output a scalar or a vector - $f(w, x, y, z)$ and $\boldsymbol{f}(w, x, y, z)$ . I know that $\frac{\partial f}{\partial w}$ , $\frac{\partial f}{\partial x}$ , $\frac{\partial f}{\partial y}$ , and $\frac{\partial f}{\partial z}$ are all called partial derivatives of $f$ and they represent the change in $f$ when varying the variable in the denominator while keeping the others constant. I also know that I can group all 4 independent variables into a vector $\boldsymbol{r} = [w\, x\, y\, z]^T$ such that $f(\boldsymbol{r})$ and $\boldsymbol{f}(\boldsymbol{r})$ . I can alternatively divide this grouping into several vectors whose dimensions sum to the dimension of $\boldsymbol{r}$ . For example, $f(\boldsymbol{p}, \boldsymbol{q})$ or $\boldsymbol{f}(\boldsymbol{p}, \boldsymbol{q})$ where $\boldsymbol{p} = [w\, x]^T$ and $\boldsymbol{q} = [y\, z]^T$ . Are $\frac{\partial \boldsymbol{f}}{\partial w}$ , $\frac{\partial \boldsymbol{f}}{\partial x}$ , $\frac{\partial \boldsymbol{f}}{\partial y}$ , and $\frac{\partial \boldsymbol{f}}{\partial z}$ (which are vectors) all called partial derivatives of $\boldsymbol{f}$ ? Do they similarly represent change in $\boldsymbol{f}$ as a result of varying the variable in the denominator while keeping the others constant? Would $\frac{\partial f}{\partial \boldsymbol{r}}$ (which is a vector and often called $\nabla f$ or $\nabla f^T$ depending on convention) be called the total derivative of $f$ , and therefore would it be correct to use the notation $\frac{df}{d\boldsymbol{r}}$ ? I know this is a vector that points in the direction which most increases the value of $f$ and is not a number like the total derivative would be in single variable calculus. However I recognize that the number you get in single variable calculus is essentially a 1D gradient and plays exactly the same role. Would $\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{r}}$ (which is a matrix) be called the total derivative of $\boldsymbol{f}$ ? I have heard this called the pushforward, differential, or Jacobian, but is it also correct to think of it as a total derivative and therefore use the notation $\frac{d\boldsymbol{f}}{d\boldsymbol{r}}$ ? I know that this matrix basically contains the gradients of the components of $\boldsymbol f$ , which are scalar functions of $\boldsymbol r$ . Therefore, invoking the idea of the gradient, I might think of this matrix as some transformation which will most increase the magnitude of $\boldsymbol f$ , but given that the gradient of each component could point in arbitrary directions, I would not be able to choose an $\boldsymbol r$ which will align with all of these directions simultaneously. Is there an intuitive gradient-like interpretation of this matrix or should I only think of it as a mapping between small changes in input and output, or a first order approximation of $\boldsymbol{f}$ )? Would $\frac{\partial f}{\partial \boldsymbol{p}}$ , $\frac{\partial f}{\partial \boldsymbol{q}}$ (which are vectors) and $\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{p}}$ , $\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{q}}$ (which are matrices) also be called partial derivatives of $f$ and $\boldsymbol f$ ? I assume these describe the change in $f$ and $\boldsymbol f$ due to changes in one independent vector while keeping the other(s) constant. Is it also correct to say that they capture what the change in $f$ and $\boldsymbol f$ would be if you varied some group of the underlying scalar variables (in this case $w$ , $x$ or $y$ , $z$ ) while keeping the others constant? I recognize that some sort of union of these derivatives constitutes the derivatives asked about in the previous two questions. Can the gradient-like notion of pointing in the direction of greatest increase of the function be invoked here (at least in the case of $f$ )?","I have the following terminology questions which are often not well addressed in an undergraduate multivariable calculus course. While the questions are long, I expect their answers will be short. As background, say I have two functions that depend on scalar variables , , , and which each output a scalar or a vector - and . I know that , , , and are all called partial derivatives of and they represent the change in when varying the variable in the denominator while keeping the others constant. I also know that I can group all 4 independent variables into a vector such that and . I can alternatively divide this grouping into several vectors whose dimensions sum to the dimension of . For example, or where and . Are , , , and (which are vectors) all called partial derivatives of ? Do they similarly represent change in as a result of varying the variable in the denominator while keeping the others constant? Would (which is a vector and often called or depending on convention) be called the total derivative of , and therefore would it be correct to use the notation ? I know this is a vector that points in the direction which most increases the value of and is not a number like the total derivative would be in single variable calculus. However I recognize that the number you get in single variable calculus is essentially a 1D gradient and plays exactly the same role. Would (which is a matrix) be called the total derivative of ? I have heard this called the pushforward, differential, or Jacobian, but is it also correct to think of it as a total derivative and therefore use the notation ? I know that this matrix basically contains the gradients of the components of , which are scalar functions of . Therefore, invoking the idea of the gradient, I might think of this matrix as some transformation which will most increase the magnitude of , but given that the gradient of each component could point in arbitrary directions, I would not be able to choose an which will align with all of these directions simultaneously. Is there an intuitive gradient-like interpretation of this matrix or should I only think of it as a mapping between small changes in input and output, or a first order approximation of )? Would , (which are vectors) and , (which are matrices) also be called partial derivatives of and ? I assume these describe the change in and due to changes in one independent vector while keeping the other(s) constant. Is it also correct to say that they capture what the change in and would be if you varied some group of the underlying scalar variables (in this case , or , ) while keeping the others constant? I recognize that some sort of union of these derivatives constitutes the derivatives asked about in the previous two questions. Can the gradient-like notion of pointing in the direction of greatest increase of the function be invoked here (at least in the case of )?","w x y z f(w, x, y, z) \boldsymbol{f}(w, x, y, z) \frac{\partial f}{\partial w} \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} \frac{\partial f}{\partial z} f f \boldsymbol{r} = [w\, x\, y\, z]^T f(\boldsymbol{r}) \boldsymbol{f}(\boldsymbol{r}) \boldsymbol{r} f(\boldsymbol{p}, \boldsymbol{q}) \boldsymbol{f}(\boldsymbol{p}, \boldsymbol{q}) \boldsymbol{p} = [w\, x]^T \boldsymbol{q} = [y\, z]^T \frac{\partial \boldsymbol{f}}{\partial w} \frac{\partial \boldsymbol{f}}{\partial x} \frac{\partial \boldsymbol{f}}{\partial y} \frac{\partial \boldsymbol{f}}{\partial z} \boldsymbol{f} \boldsymbol{f} \frac{\partial f}{\partial \boldsymbol{r}} \nabla f \nabla f^T f \frac{df}{d\boldsymbol{r}} f \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{r}} \boldsymbol{f} \frac{d\boldsymbol{f}}{d\boldsymbol{r}} \boldsymbol f \boldsymbol r \boldsymbol f \boldsymbol r \boldsymbol{f} \frac{\partial f}{\partial \boldsymbol{p}} \frac{\partial f}{\partial \boldsymbol{q}} \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{p}} \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{q}} f \boldsymbol f f \boldsymbol f f \boldsymbol f w x y z f","['multivariable-calculus', 'terminology', 'partial-derivative', 'intuition']"
11,Volume of a sphere using cartesian coordinates,Volume of a sphere using cartesian coordinates,,"I'm preparing my calculus exam and I'm in doubt about how to generally compute triple integrals. I know that the cartesian equation of a sphere is $B_R=\{(x, y, z)|x^2+y^2+z^2=R^2\}$ , so (if I didn't want to use spherical coordinates, wich I'm aware is the best way and I already did that) it's volume would just be $\iiint_S \mathrm{d}x\mathrm{d}y\mathrm{d}z$ , but what would the extremes be? I know $-R \leq z \leq R$ and $-\sqrt{R^2-y^2-z^2} \leq x \leq\sqrt{R^2-y^2-z^2} $ , but what are the extremes for $y$ ? I can't describe it in terms of $x$ , so I have $$\text{Vol}(B_R) = \int_{-R}^{R} \int_?^?\int_{-\sqrt{R^2-y^2-z^2}}^\sqrt{R^2-y^2-z^2} \mathrm{d}x\mathrm{d}y\mathrm{d}z.$$ What should be there instead of the '?'","I'm preparing my calculus exam and I'm in doubt about how to generally compute triple integrals. I know that the cartesian equation of a sphere is , so (if I didn't want to use spherical coordinates, wich I'm aware is the best way and I already did that) it's volume would just be , but what would the extremes be? I know and , but what are the extremes for ? I can't describe it in terms of , so I have What should be there instead of the '?'","B_R=\{(x, y, z)|x^2+y^2+z^2=R^2\} \iiint_S \mathrm{d}x\mathrm{d}y\mathrm{d}z -R \leq z \leq R -\sqrt{R^2-y^2-z^2} \leq x \leq\sqrt{R^2-y^2-z^2}  y x \text{Vol}(B_R) = \int_{-R}^{R} \int_?^?\int_{-\sqrt{R^2-y^2-z^2}}^\sqrt{R^2-y^2-z^2} \mathrm{d}x\mathrm{d}y\mathrm{d}z.","['integration', 'multivariable-calculus', 'volume']"
12,Prove the vector calculus identity $\frac{1}{2}\mathbf{\nabla(\lVert u \rVert ^2) = (u \cdot \nabla)u+u \times (\nabla \times u )}$,Prove the vector calculus identity,\frac{1}{2}\mathbf{\nabla(\lVert u \rVert ^2) = (u \cdot \nabla)u+u \times (\nabla \times u )},My attempt: Consider the first component of both sides. $$LHS=\frac{1}{2}\frac{\partial}{\partial x_1}(u_1^2+u_2^2+u_3^2)=u_1 \frac{\partial u_1}{\partial x_1}+u_2 \frac{\partial u_2}{\partial x_1} + u_3 \frac{\partial u_3}{\partial x_1}$$ $$RHS=u_1\left( \frac{\partial u_1}{\partial x_1}+\frac{\partial u_2}{\partial x_2}+\frac{\partial u_3}{\partial x_3}\right)+u_2(\partial_1u_2-\partial_2u_1)-u_3(\partial_3u_1-\partial_1u_3)$$ Then I got stuck as I couldn't cancel out some of the terms and don't know how to proceed. Any help will be appreciated!,My attempt: Consider the first component of both sides. Then I got stuck as I couldn't cancel out some of the terms and don't know how to proceed. Any help will be appreciated!,LHS=\frac{1}{2}\frac{\partial}{\partial x_1}(u_1^2+u_2^2+u_3^2)=u_1 \frac{\partial u_1}{\partial x_1}+u_2 \frac{\partial u_2}{\partial x_1} + u_3 \frac{\partial u_3}{\partial x_1} RHS=u_1\left( \frac{\partial u_1}{\partial x_1}+\frac{\partial u_2}{\partial x_2}+\frac{\partial u_3}{\partial x_3}\right)+u_2(\partial_1u_2-\partial_2u_1)-u_3(\partial_3u_1-\partial_1u_3),"['multivariable-calculus', 'vector-analysis']"
13,Changing order of integration:$\int_0^\infty\int_{-\infty}^{-y}f(x)\mathrm dx\mathrm dy\Rightarrow\int_{-\infty}^0\int_0^{-x}f(x)\mathrm dy\mathrm dx$,Changing order of integration:,\int_0^\infty\int_{-\infty}^{-y}f(x)\mathrm dx\mathrm dy\Rightarrow\int_{-\infty}^0\int_0^{-x}f(x)\mathrm dy\mathrm dx,"Why does $$\int_{0}^{\infty} \int_{-\infty}^{-y} f(x)\mathrm dx \mathrm dy \Rightarrow \int_{-\infty}^{0} \int_{0}^{-x} f(x) \mathrm dy \mathrm dx$$ The title is pretty self explanatory. I couldn't see how to properly change the order of the left integeral to the right one. I'd love to hear your thoughts, thanks.","Why does The title is pretty self explanatory. I couldn't see how to properly change the order of the left integeral to the right one. I'd love to hear your thoughts, thanks.",\int_{0}^{\infty} \int_{-\infty}^{-y} f(x)\mathrm dx \mathrm dy \Rightarrow \int_{-\infty}^{0} \int_{0}^{-x} f(x) \mathrm dy \mathrm dx,"['integration', 'multivariable-calculus', 'change-of-variable']"
14,"Function Optimization with Non-linear Constraint, Lagrange Multipliers Fails","Function Optimization with Non-linear Constraint, Lagrange Multipliers Fails",,"I am trying to maximize the function $A(x,y)=\frac{1}{2}(x(12-x)+y(13-y))$ subject to the constraint $x^2+(12-x)^2-y^2-(13-y)^2=0$ . My attempt: $\begin{align*} \nabla A=\frac{1}{2}\langle 12-2x,\,13-2y\rangle &= \lambda\langle4x-24,\, -4y+26\rangle\\ \implies&\begin{cases} -x+6=\lambda(4x-24)\\-y+\frac{13}{2}=\lambda(-4y+26)\\x^2+(12-x)^2-y^2-(13-y)^2=0\end{cases}\end{align*}$ But clearly there is no solution due to the first two equations. Using Wolfram Alpha , however, yields a maximum at $\displaystyle \left(\frac{17}{2},\,\frac{13}{2}\right)$ being $A=36$ and shows a nice little graph.","I am trying to maximize the function subject to the constraint . My attempt: But clearly there is no solution due to the first two equations. Using Wolfram Alpha , however, yields a maximum at being and shows a nice little graph.","A(x,y)=\frac{1}{2}(x(12-x)+y(13-y)) x^2+(12-x)^2-y^2-(13-y)^2=0 \begin{align*} \nabla A=\frac{1}{2}\langle 12-2x,\,13-2y\rangle &= \lambda\langle4x-24,\, -4y+26\rangle\\ \implies&\begin{cases} -x+6=\lambda(4x-24)\\-y+\frac{13}{2}=\lambda(-4y+26)\\x^2+(12-x)^2-y^2-(13-y)^2=0\end{cases}\end{align*} \displaystyle \left(\frac{17}{2},\,\frac{13}{2}\right) A=36","['calculus', 'multivariable-calculus']"
15,If $f$ is a linear map. Show that $Df(a)=f(a)$,If  is a linear map. Show that,f Df(a)=f(a),Suppose $f:\Bbb R^n\to \Bbb R^m$ is a linear map. Show that $Df(a)=f(a)$ . Tried using limit definition: $$\lim\limits_{h \to 0}\frac{\Vert f(a+h)-f(a)-f(a)h\Vert}{\Vert h\Vert}$$ $$=\lim\limits_{h \to 0}\frac{\Vert f(a)+f(h)-f(a)-f(a)h\Vert}{\Vert h\Vert}$$ $$=\lim\limits_{h \to 0}\frac{\Vert f(h)-f(a)h\Vert}{\Vert h\Vert}$$ Want to show this is $0$ but can't see where to go from here. Unless I misunderstand what $Df(a)=f(a)$ means.,Suppose is a linear map. Show that . Tried using limit definition: Want to show this is but can't see where to go from here. Unless I misunderstand what means.,f:\Bbb R^n\to \Bbb R^m Df(a)=f(a) \lim\limits_{h \to 0}\frac{\Vert f(a+h)-f(a)-f(a)h\Vert}{\Vert h\Vert} =\lim\limits_{h \to 0}\frac{\Vert f(a)+f(h)-f(a)-f(a)h\Vert}{\Vert h\Vert} =\lim\limits_{h \to 0}\frac{\Vert f(h)-f(a)h\Vert}{\Vert h\Vert} 0 Df(a)=f(a),['multivariable-calculus']
16,How to calculate the surface area of parametric surface?,How to calculate the surface area of parametric surface?,,"Suppose you have the surface $z=3xy$ and you want to find the area that lies within the cylinder $x^2+y^2\leq 1$ . My homework is forcing me to use the parameterization $$\textbf{r}_1(s,t)= <s\cos(t), s\sin(t), 3s^2\sin(t)\cos(t)>$$ I am having a difficult time visualizing this parameterization, and I do not have any graphing software to graph the surface, but I want to make sure I understand this concept. This is quite obvious, but I want to be sure; using the above parameterization, I am not parameterizing the entire surface, right? If I wanted to, I assume the parameterization would be $$\textbf{r}_2(s,t) = <s,t,3st>$$ Instead, is $\textbf{r}_1$ just the parameterization adjusted for the region - the region being the cylinder $x^2+y^2\leq 1$ ? That is, are we just making a revolution around $z=3xy$ ? Any insight would be helpful.","Suppose you have the surface and you want to find the area that lies within the cylinder . My homework is forcing me to use the parameterization I am having a difficult time visualizing this parameterization, and I do not have any graphing software to graph the surface, but I want to make sure I understand this concept. This is quite obvious, but I want to be sure; using the above parameterization, I am not parameterizing the entire surface, right? If I wanted to, I assume the parameterization would be Instead, is just the parameterization adjusted for the region - the region being the cylinder ? That is, are we just making a revolution around ? Any insight would be helpful.","z=3xy x^2+y^2\leq 1 \textbf{r}_1(s,t)= <s\cos(t), s\sin(t), 3s^2\sin(t)\cos(t)> \textbf{r}_2(s,t) = <s,t,3st> \textbf{r}_1 x^2+y^2\leq 1 z=3xy","['multivariable-calculus', 'vectors', 'surfaces', 'parametrization']"
17,"Find the critical points of each of the functions below and classify them as a local maximum, minimum, or neither","Find the critical points of each of the functions below and classify them as a local maximum, minimum, or neither",,"$f ( x , y ) = \ln ( 2 + \sin ( x y ) )$ . Consider only the critical point $(0,0)$ . I have solved for the first and second partial derivatives and I see that they are both equal to $0$ at $(0,0)$ . One of the exercises in my textbook mentions the Hessian matrix and I think I should be using that, but I am not sure how it works. $f ( x , y ) = \left( x ^ { 2 } + 3 y ^ { 2 } \right) e ^ { 1 - x ^ { 2 } - y ^ { 2 } }$ For this problem, I took the first and second partial derivatives and I observed that the critical points are at $(0,1)$ , $(0,-1)$ , and $(0,0)$ . Do I have to use the Hessian here as well? How would that work?",". Consider only the critical point . I have solved for the first and second partial derivatives and I see that they are both equal to at . One of the exercises in my textbook mentions the Hessian matrix and I think I should be using that, but I am not sure how it works. For this problem, I took the first and second partial derivatives and I observed that the critical points are at , , and . Do I have to use the Hessian here as well? How would that work?","f ( x , y ) = \ln ( 2 + \sin ( x y ) ) (0,0) 0 (0,0) f ( x , y ) = \left( x ^ { 2 } + 3 y ^ { 2 } \right) e ^ { 1 - x ^ { 2 } - y ^ { 2 } } (0,1) (0,-1) (0,0)","['calculus', 'multivariable-calculus', 'vector-analysis']"
18,"The surface integral $\int_S z^2 \, dS$ over the cube $S$",The surface integral  over the cube,"\int_S z^2 \, dS S","Evaluate the integral $$\int_S z^2 \, dS ,$$ where $S$ is the surface of the cube $\{-1 < x < 1, -1 < y< 1, -1< z< 1\}$ . So I gather that it has six sides. So what I did was evaluate the surface integral for one side and got $8/3$ . So shouldn't the answer be $48/3$ ? The actual answer is $40/3$ .",Evaluate the integral where is the surface of the cube . So I gather that it has six sides. So what I did was evaluate the surface integral for one side and got . So shouldn't the answer be ? The actual answer is .,"\int_S z^2 \, dS , S \{-1 < x < 1, -1 < y< 1, -1< z< 1\} 8/3 48/3 40/3","['integration', 'multivariable-calculus', 'surface-integrals']"
19,Write this in matrix form?,Write this in matrix form?,,"I'm trying to work on my linear algebra skills. I came across this equation in Bishop PRML (Equation 4.93): $$ \nabla E(w) = \sum_{n=1}^N(\mathbf{w}^T\mathbf{\phi}_n-t_n)\mathbf{\phi}_n = \mathbf{\Phi}^T\mathbf{\Phi w} - \mathbf{\Phi^T t}$$ Where $\mathbf{w, \phi_n, t}$ are vectors and $\mathbf{\Phi}$ is a $N$ by $M$ matrix whose $n^{th}$ row is given by $\mathbf{\phi}_n^T$. The matrix form confuses me. I understand how he derives the $\mathbf{\Phi ^Tt}$ part, but really can't figure out how he came up with $\mathbf{\Phi^T\Phi w}$ I hope one of you can explain it to me :) Thanks P.S. Tips on how to structurally figure these matrix forms out in general are really appreciated.","I'm trying to work on my linear algebra skills. I came across this equation in Bishop PRML (Equation 4.93): $$ \nabla E(w) = \sum_{n=1}^N(\mathbf{w}^T\mathbf{\phi}_n-t_n)\mathbf{\phi}_n = \mathbf{\Phi}^T\mathbf{\Phi w} - \mathbf{\Phi^T t}$$ Where $\mathbf{w, \phi_n, t}$ are vectors and $\mathbf{\Phi}$ is a $N$ by $M$ matrix whose $n^{th}$ row is given by $\mathbf{\phi}_n^T$. The matrix form confuses me. I understand how he derives the $\mathbf{\Phi ^Tt}$ part, but really can't figure out how he came up with $\mathbf{\Phi^T\Phi w}$ I hope one of you can explain it to me :) Thanks P.S. Tips on how to structurally figure these matrix forms out in general are really appreciated.",,"['linear-algebra', 'multivariable-calculus', 'matrix-calculus']"
20,Intuition or motivation for the definition of an hypersurface. What are we actually trying to define?,Intuition or motivation for the definition of an hypersurface. What are we actually trying to define?,,"If we have $x^2 + y^2= 1 $ then we can solve for $y$ and $x$, at least in parts. The implicit function theorem gives us the conditions to solve these things. At this part of this book (Folland, about PDE), it says that if a hypersurface is given by that definition, we can use the implicit funciton theorem to solve for some variable in relation to the other $n-1$ other ones. So how does this definiton arise? I think the open sets $S$ and $V$ have something to do with locality of the solution, as we have inverse solutions for $x^2 + y^2 = 1$ only locally. The definition makes no sense to me, I'd like some help. UPDATE: It also talks about hyperplane without defining it. The only definition is of an hypersurface, which is also something I don't unerstand. What is one and another? Also, the implicit funciton theorem says that the jacobian must be invertible. The closest the book talks about this is when it says the gradient is nonvanishing, but I don't think it implies jacobian invertible. UPDATE : I'm trying to visualize it. The plane should be the subset $S$ of $\mathbb{R}^3$. $V$ is an open around $x_0$. If for every $x_0$ and every open $V$ we can find a function $\phi\in C^k(V)$ with $\nabla \phi$ nonvanishing on $S\cap V $ and $S\cap V  = \{x\in V: \phi(x) = 0\}$, then the plane is an hypersurface. There's no motivation anywhere on google (there's so little about the word hypersurface in general). So the main question is: what is this crazy definition suppose to define? What are the challenges in defining an hypersurface? (what even is an hypersurface?). Please remember that you're explaining to someone who have little background on this manifold thing etc (actually I don't even know what this term means at all) so a little background explanation would be good.","If we have $x^2 + y^2= 1 $ then we can solve for $y$ and $x$, at least in parts. The implicit function theorem gives us the conditions to solve these things. At this part of this book (Folland, about PDE), it says that if a hypersurface is given by that definition, we can use the implicit funciton theorem to solve for some variable in relation to the other $n-1$ other ones. So how does this definiton arise? I think the open sets $S$ and $V$ have something to do with locality of the solution, as we have inverse solutions for $x^2 + y^2 = 1$ only locally. The definition makes no sense to me, I'd like some help. UPDATE: It also talks about hyperplane without defining it. The only definition is of an hypersurface, which is also something I don't unerstand. What is one and another? Also, the implicit funciton theorem says that the jacobian must be invertible. The closest the book talks about this is when it says the gradient is nonvanishing, but I don't think it implies jacobian invertible. UPDATE : I'm trying to visualize it. The plane should be the subset $S$ of $\mathbb{R}^3$. $V$ is an open around $x_0$. If for every $x_0$ and every open $V$ we can find a function $\phi\in C^k(V)$ with $\nabla \phi$ nonvanishing on $S\cap V $ and $S\cap V  = \{x\in V: \phi(x) = 0\}$, then the plane is an hypersurface. There's no motivation anywhere on google (there's so little about the word hypersurface in general). So the main question is: what is this crazy definition suppose to define? What are the challenges in defining an hypersurface? (what even is an hypersurface?). Please remember that you're explaining to someone who have little background on this manifold thing etc (actually I don't even know what this term means at all) so a little background explanation would be good.",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
21,"$f:\mathbb{R}^4 \to \mathbb{R}$ is a Constant when $Xf = Yf = 0$ for Vector Fields $X,Y$",is a Constant when  for Vector Fields,"f:\mathbb{R}^4 \to \mathbb{R} Xf = Yf = 0 X,Y","I'm preparing for some comprehensive exams and this is a question from a previous year that I've been trying to solve. ""On $\mathbb{R}^4$, equipped with coordinates $(x,y,z,t)$, let $X,Y$ be vector fields given by $$X = \frac{\partial}{\partial x}+z \frac{\partial}{\partial y}, \hspace{5mm} Y=x \frac{\partial}{\partial z} + \frac{\partial}{\partial t}.$$ If $f: \mathbb{R}^4 \to \mathbb{R}$ is a smooth function satisfying $Xf = Yf = 0$, show that $f$ is constant."" Here's somethings I've tried. By definition, $Xf:=df(X)$. Thus, we first write out $df$ generically. $$df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y} dy + \frac{\partial f}{\partial z}dz + \frac{\partial f}{\partial t}dt.$$ We have that $df(X) = 0 = \frac{\partial f}{\partial x} + z \frac{\partial f}{\partial y}$ and $df(Y) = 0 = x \frac{\partial f}{\partial z} + \frac{\partial f}{\partial t}$. Observe that $[X,Y] = \frac{\partial}{\partial z}-x \frac{\partial}{\partial y}$. Since we know that $df([X,Y]) = 0$ as well from the assumptions of $Xf = Yf = 0$, then $df([X,Y]) = 0 = \frac{\partial f}{\partial z} - x \frac{\partial f}{\partial y}$. From here, I tried to use the three equations to show that each of the partial derivatives of $f$ are $0$, forcing $f$ to be constant. However, the relations aren't working out to show this so maybe I did some computations wrong though I've checked a few times... Another idea is to consider the distribution $D = \ker df$ and then try and apply Frobenius' theorem in some way. Any help is welcome!","I'm preparing for some comprehensive exams and this is a question from a previous year that I've been trying to solve. ""On $\mathbb{R}^4$, equipped with coordinates $(x,y,z,t)$, let $X,Y$ be vector fields given by $$X = \frac{\partial}{\partial x}+z \frac{\partial}{\partial y}, \hspace{5mm} Y=x \frac{\partial}{\partial z} + \frac{\partial}{\partial t}.$$ If $f: \mathbb{R}^4 \to \mathbb{R}$ is a smooth function satisfying $Xf = Yf = 0$, show that $f$ is constant."" Here's somethings I've tried. By definition, $Xf:=df(X)$. Thus, we first write out $df$ generically. $$df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y} dy + \frac{\partial f}{\partial z}dz + \frac{\partial f}{\partial t}dt.$$ We have that $df(X) = 0 = \frac{\partial f}{\partial x} + z \frac{\partial f}{\partial y}$ and $df(Y) = 0 = x \frac{\partial f}{\partial z} + \frac{\partial f}{\partial t}$. Observe that $[X,Y] = \frac{\partial}{\partial z}-x \frac{\partial}{\partial y}$. Since we know that $df([X,Y]) = 0$ as well from the assumptions of $Xf = Yf = 0$, then $df([X,Y]) = 0 = \frac{\partial f}{\partial z} - x \frac{\partial f}{\partial y}$. From here, I tried to use the three equations to show that each of the partial derivatives of $f$ are $0$, forcing $f$ to be constant. However, the relations aren't working out to show this so maybe I did some computations wrong though I've checked a few times... Another idea is to consider the distribution $D = \ker df$ and then try and apply Frobenius' theorem in some way. Any help is welcome!",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
22,"Evaluate $\iint_D x \sin (y -x^2) \,dA$.",Evaluate .,"\iint_D x \sin (y -x^2) \,dA","Let $D$ be the region, in the first quadrant of the $x,y$ plane, bounded by the curves $y = x^2$, $y = x^2+1$, $x+y=1$ and $x+y=2$. Using an appropriate change of variables, compute the integral $$\iint_D x \sin (y -x^2) \,dA.$$ I've been reviewing for an upcoming test and this problem was recommended to do for study -- I just can't get it. I've tried many changes of variables and nothing has worked. I would really appreciate a hint or a solution. Thanks in advance!","Let $D$ be the region, in the first quadrant of the $x,y$ plane, bounded by the curves $y = x^2$, $y = x^2+1$, $x+y=1$ and $x+y=2$. Using an appropriate change of variables, compute the integral $$\iint_D x \sin (y -x^2) \,dA.$$ I've been reviewing for an upcoming test and this problem was recommended to do for study -- I just can't get it. I've tried many changes of variables and nothing has worked. I would really appreciate a hint or a solution. Thanks in advance!",,"['calculus', 'integration', 'multivariable-calculus', 'change-of-variable']"
23,Leibniz rule with balls,Leibniz rule with balls,,"Let's say I have an integral: $$\int_{B_{t}} f(t,x) \mathrm{d}x$$ where $B_t$ is the ball of radius $t$ . And I would like to apply the Leibniz rule to compute the derivative of this. How would I do it? I know that the formula goes something like this: $$\frac{d}{dt}\int_{B_{t}} f(t,x) \mathrm{d}x = \int_{B_t} \frac{d}{dt}f(t,x) \mathrm{d}x + \int_{\partial B_t}g(t,x) \mathrm{d}S$$ I know that $g$ is supposed to have something to do with the velocity of the boundary (I saw this term used when discussing the Reynolds transport theorem). What does this mean exactly? How can I compute it for the case of the unit ball?",Let's say I have an integral: where is the ball of radius . And I would like to apply the Leibniz rule to compute the derivative of this. How would I do it? I know that the formula goes something like this: I know that is supposed to have something to do with the velocity of the boundary (I saw this term used when discussing the Reynolds transport theorem). What does this mean exactly? How can I compute it for the case of the unit ball?,"\int_{B_{t}} f(t,x) \mathrm{d}x B_t t \frac{d}{dt}\int_{B_{t}} f(t,x) \mathrm{d}x = \int_{B_t} \frac{d}{dt}f(t,x) \mathrm{d}x + \int_{\partial B_t}g(t,x) \mathrm{d}S g","['multivariable-calculus', 'derivatives', 'definite-integrals', 'vector-analysis', 'leibniz-integral-rule']"
24,"If $f: \mathbb{R}^n \to \mathbb{R}^n$ is one-to-one, is the differential necessarily one-to-one?","If  is one-to-one, is the differential necessarily one-to-one?",f: \mathbb{R}^n \to \mathbb{R}^n,"If $f: \mathbb{R}^n \to \mathbb{R}^n$ is one-to-one, $C^1$, and has everyhere non-singular Jacobian, then for any open set $U \subset \mathbb{R}^n$, the image $f(U)$ is open I proved the above theorem using the inverse function theorem. However, I stumbled upon a proof online that removed the necessity of the Jacobian being everywhere non-singular, instead only specifying $f$ to be one-to-one and $C^1$ The proof makes sense to me, except for one statement: ""an injective map has an injective derivative"" If $f: \mathbb{R}^n \to \mathbb{R}^n$ is one-to-one, is the differential necessarily one-to-one? Here is a link to the mentioned proof: https://gist.github.com/pervognsen/11251717","If $f: \mathbb{R}^n \to \mathbb{R}^n$ is one-to-one, $C^1$, and has everyhere non-singular Jacobian, then for any open set $U \subset \mathbb{R}^n$, the image $f(U)$ is open I proved the above theorem using the inverse function theorem. However, I stumbled upon a proof online that removed the necessity of the Jacobian being everywhere non-singular, instead only specifying $f$ to be one-to-one and $C^1$ The proof makes sense to me, except for one statement: ""an injective map has an injective derivative"" If $f: \mathbb{R}^n \to \mathbb{R}^n$ is one-to-one, is the differential necessarily one-to-one? Here is a link to the mentioned proof: https://gist.github.com/pervognsen/11251717",,"['real-analysis', 'multivariable-calculus', 'proof-verification']"
25,Finding when a double integral is convergent,Finding when a double integral is convergent,,"Given is the integral  $$\iint_{\mathbb{R}^2} \frac{1}{(1+x^2+y^2)^k}\,dx \, dy$$ the question asks for the values of $k$ for which the integral will converge, and in turn find the value which the integral converges to. Using $k=1$ shows that it diverges, but I'm not sure how I should go about finding the values for which it converges. Thanks in advance for any help.","Given is the integral  $$\iint_{\mathbb{R}^2} \frac{1}{(1+x^2+y^2)^k}\,dx \, dy$$ the question asks for the values of $k$ for which the integral will converge, and in turn find the value which the integral converges to. Using $k=1$ shows that it diverges, but I'm not sure how I should go about finding the values for which it converges. Thanks in advance for any help.",,"['calculus', 'integration', 'multivariable-calculus', 'improper-integrals']"
26,No possible Level Surface?,No possible Level Surface?,,"Given a function of three variables, is it possible to not have a level surface at all? Ex: I'm working on a problem that tells me to describe the level set (level surface) for $p(x,y,z) = e^{-x^2-y^2-4z^2}$. I manipulated the function to be $ln(k)=-x^2-y^2-4z^2$. I'm thinking the level surface must be a type of ellipsoid but I'm having trouble graphing it both by myself and my program doesn't seem to be able to graph it. Am I doing something wrong or is this just an odd case? Ex: I'm plugging in a number for the constant and trying to graph it. Should I possibly be moving z over and having x and y equal to z?","Given a function of three variables, is it possible to not have a level surface at all? Ex: I'm working on a problem that tells me to describe the level set (level surface) for $p(x,y,z) = e^{-x^2-y^2-4z^2}$. I manipulated the function to be $ln(k)=-x^2-y^2-4z^2$. I'm thinking the level surface must be a type of ellipsoid but I'm having trouble graphing it both by myself and my program doesn't seem to be able to graph it. Am I doing something wrong or is this just an odd case? Ex: I'm plugging in a number for the constant and trying to graph it. Should I possibly be moving z over and having x and y equal to z?",,"['multivariable-calculus', 'exponential-function', '3d']"
27,"Find the points that are closest and farthest from $(0,0)$ on the curve $3x^2-2xy+2y^2=5$",Find the points that are closest and farthest from  on the curve,"(0,0) 3x^2-2xy+2y^2=5","Find the points that are closest and farthest from $(0,0)$ on the curve $3x^2-2xy+2y^2=5$ My attempt: So, I'm looking to find global extrema of the function $f=x^2+y^2$ (since square root is a motonous function it has extrema at same points as this function). The set $S = \{ (x,y)\in \mathbb{R} : 3x^2 -2xy + 2y^2 -5 = 0\}$ is not compact, therefore I cannot guarantee that $f$ will have a minimum or a maximum on $S$. Anyways by applying the usual theorem, gradient of $f$ is colinear with the gradient of $g$. (gradient of g is $0$ only if $x=y=0$ and that point is not in $S$ therefore not relevant so the set containing only gradient of $g$ is linearly independent).Let the coefficient be noted by $\lambda$. I'm not sure how to go about this from now though. I have 3 equations that each contain $x,y,xy,\lambda x,\lambda y$: (if my calculations were right) $$\lambda (-3x+y) + x=0$$ $$\lambda (x-2y)+y=0$$ $$3x^2 -2xy + 2y^2 - 5 = 0$$ I'm stuck here and I'm not sure what to do.Hints would help! Thanks in advance!","Find the points that are closest and farthest from $(0,0)$ on the curve $3x^2-2xy+2y^2=5$ My attempt: So, I'm looking to find global extrema of the function $f=x^2+y^2$ (since square root is a motonous function it has extrema at same points as this function). The set $S = \{ (x,y)\in \mathbb{R} : 3x^2 -2xy + 2y^2 -5 = 0\}$ is not compact, therefore I cannot guarantee that $f$ will have a minimum or a maximum on $S$. Anyways by applying the usual theorem, gradient of $f$ is colinear with the gradient of $g$. (gradient of g is $0$ only if $x=y=0$ and that point is not in $S$ therefore not relevant so the set containing only gradient of $g$ is linearly independent).Let the coefficient be noted by $\lambda$. I'm not sure how to go about this from now though. I have 3 equations that each contain $x,y,xy,\lambda x,\lambda y$: (if my calculations were right) $$\lambda (-3x+y) + x=0$$ $$\lambda (x-2y)+y=0$$ $$3x^2 -2xy + 2y^2 - 5 = 0$$ I'm stuck here and I'm not sure what to do.Hints would help! Thanks in advance!",,"['multivariable-calculus', 'optimization', 'quadratics', 'lagrange-multiplier', 'discriminant']"
28,Calculating the surface of revolution of a cardioid.,Calculating the surface of revolution of a cardioid.,,"I have the cardioid $r=1+cos(t)$ for $0\leq{t}\leq{2\pi}$ and I want to calculate the surface of revolution of said curve. How can I calculate it? The parematrization of the cardioid is: $$x(t)=(1+cos(t))cos(t)$$ $$y(t)=(1+cos(t))sin(t)$$ and  $$\frac{dx}{dt}=\left(-2\cos\left(t\right)-1\right)\sin\left(t\right)$$ $$\frac{dy}{dt}=\cos\left(t\right)\left(\cos\left(t\right)+1\right)-\sin^2\left(t\right) $$ To calculate the surface of revolution I know I can use the formula (since I want to revolve it around the x-axis) $$2\pi \int_{a}^{b} y(t)\sqrt{\bigg(\frac{dx}{dt}\bigg)^2+\bigg(\frac{dy}{dt}\bigg)^2}  dt$$ However, when I do that integral, the result is $0$ (using online calculators ). Why is this wrong? Obviously the surface can't be 0, right? What's the correct answer?","I have the cardioid $r=1+cos(t)$ for $0\leq{t}\leq{2\pi}$ and I want to calculate the surface of revolution of said curve. How can I calculate it? The parematrization of the cardioid is: $$x(t)=(1+cos(t))cos(t)$$ $$y(t)=(1+cos(t))sin(t)$$ and  $$\frac{dx}{dt}=\left(-2\cos\left(t\right)-1\right)\sin\left(t\right)$$ $$\frac{dy}{dt}=\cos\left(t\right)\left(\cos\left(t\right)+1\right)-\sin^2\left(t\right) $$ To calculate the surface of revolution I know I can use the formula (since I want to revolve it around the x-axis) $$2\pi \int_{a}^{b} y(t)\sqrt{\bigg(\frac{dx}{dt}\bigg)^2+\bigg(\frac{dy}{dt}\bigg)^2}  dt$$ However, when I do that integral, the result is $0$ (using online calculators ). Why is this wrong? Obviously the surface can't be 0, right? What's the correct answer?",,"['multivariable-calculus', 'surfaces', 'parametrization', 'solid-of-revolution']"
29,How to use the chain rule in this tricky situation,How to use the chain rule in this tricky situation,,"I take funtions $\varphi: \mathbb{R}^n\rightarrow \mathbb{R}$ and $\gamma:\mathbb{R}^n \rightarrow \mathbb{R}^n $ such that $$\varphi \circ \gamma(x)=\frac{1}{2}(x_1^2+\ldots+x_r^2-x_{r+1}^2-\ldots -x_{n}^2)$$ I would like to prove that $$|\det(\gamma'(0))|=|\det (H\varphi)|^{-\frac{1}{2}}$$ Where $H\varphi$ is the Hessian matrix $$(\partial^2_{i,j} \varphi)$$ I think it's quite easy with the chain rule but I can't see how to use it propperly.","I take funtions $\varphi: \mathbb{R}^n\rightarrow \mathbb{R}$ and $\gamma:\mathbb{R}^n \rightarrow \mathbb{R}^n $ such that $$\varphi \circ \gamma(x)=\frac{1}{2}(x_1^2+\ldots+x_r^2-x_{r+1}^2-\ldots -x_{n}^2)$$ I would like to prove that $$|\det(\gamma'(0))|=|\det (H\varphi)|^{-\frac{1}{2}}$$ Where $H\varphi$ is the Hessian matrix $$(\partial^2_{i,j} \varphi)$$ I think it's quite easy with the chain rule but I can't see how to use it propperly.",,"['calculus', 'multivariable-calculus']"
30,How to calculate line integral over an ellipse with vector field undefined inside the ellipse?,How to calculate line integral over an ellipse with vector field undefined inside the ellipse?,,"Given vector fields:   $$ P=\frac{-y}{(x-1)^2+y^2}\\ Q=\frac{x-1}{(x-1)^2+y^2} $$   calculate $\oint_CP\,dx+Q\,dy$ over ellipse $\frac{x^2}{25}+\frac{y^2}{36}=1$. Define the domain where the function $U=\arctan\big(\frac{y}{x-1}\big)$ is the potential function to the given field. I tried doing parametrization for the curve that can de derived from ellipse: $$ c(t)=\langle 5\cos t,6\sin t\rangle $$ and plugging this directly into the integral. But very quickly the integral becomes quite complicated. I think there must be some trick here which I don't see. If there's a potential function then the field is conservative. But the problem is that the field is not defined at point $(1,0)$ which is inside the ellipse. For the same reason I don't think we can use Green's theorem as well.","Given vector fields:   $$ P=\frac{-y}{(x-1)^2+y^2}\\ Q=\frac{x-1}{(x-1)^2+y^2} $$   calculate $\oint_CP\,dx+Q\,dy$ over ellipse $\frac{x^2}{25}+\frac{y^2}{36}=1$. Define the domain where the function $U=\arctan\big(\frac{y}{x-1}\big)$ is the potential function to the given field. I tried doing parametrization for the curve that can de derived from ellipse: $$ c(t)=\langle 5\cos t,6\sin t\rangle $$ and plugging this directly into the integral. But very quickly the integral becomes quite complicated. I think there must be some trick here which I don't see. If there's a potential function then the field is conservative. But the problem is that the field is not defined at point $(1,0)$ which is inside the ellipse. For the same reason I don't think we can use Green's theorem as well.",,"['integration', 'multivariable-calculus', 'vector-fields']"
31,Coordinate free equation for electric field by a pure dipole.,Coordinate free equation for electric field by a pure dipole.,,"I am trying to derive $$\mathbf E(\mathbf r) =  \dfrac1{4\pi\varepsilon_0} \left(3(\bf p \cdot \mathbf {\hat {r}} ) \mathbf{\hat{r}} - \bf p \right)$$ From $$V(\mathbf r) =\dfrac{ \bf \hat r\cdot  p}{4\pi\varepsilon_0 r^2}  $$ Where $\bf p$ is the dipole vector defined by  $$\mathbf p = \int_\text{Volume} \mathbf r^\prime {\rho}{(}\mathbf r^\prime{)}{d\tau^\prime}$$ Where $d\tau^\prime$ is the volume element, Using $\mathbf E = {-\nabla V}$, $$\mathbf E  = -\dfrac1{4\pi\varepsilon_0}\left( \dfrac{\mathbf{ \hat{r}}}{r^2} \times (\nabla\times \mathbf p) + \mathbf p \times \left(\nabla\times \dfrac{\mathbf{ \hat{r}}}{r^2}\right) +   (\mathbf p\cdot\nabla)\dfrac{\mathbf{ \hat{r}}}{r^2} + \left(\dfrac{\mathbf{ \hat{r}}}{r^2}  \cdot\nabla\right) \mathbf p\right)$$. Now I don't know how to simplify this expression without using a specific coordinate system. Any hints ?","I am trying to derive $$\mathbf E(\mathbf r) =  \dfrac1{4\pi\varepsilon_0} \left(3(\bf p \cdot \mathbf {\hat {r}} ) \mathbf{\hat{r}} - \bf p \right)$$ From $$V(\mathbf r) =\dfrac{ \bf \hat r\cdot  p}{4\pi\varepsilon_0 r^2}  $$ Where $\bf p$ is the dipole vector defined by  $$\mathbf p = \int_\text{Volume} \mathbf r^\prime {\rho}{(}\mathbf r^\prime{)}{d\tau^\prime}$$ Where $d\tau^\prime$ is the volume element, Using $\mathbf E = {-\nabla V}$, $$\mathbf E  = -\dfrac1{4\pi\varepsilon_0}\left( \dfrac{\mathbf{ \hat{r}}}{r^2} \times (\nabla\times \mathbf p) + \mathbf p \times \left(\nabla\times \dfrac{\mathbf{ \hat{r}}}{r^2}\right) +   (\mathbf p\cdot\nabla)\dfrac{\mathbf{ \hat{r}}}{r^2} + \left(\dfrac{\mathbf{ \hat{r}}}{r^2}  \cdot\nabla\right) \mathbf p\right)$$. Now I don't know how to simplify this expression without using a specific coordinate system. Any hints ?",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'physics', 'mathematical-physics']"
32,Geometric intuition behind curl of vector field,Geometric intuition behind curl of vector field,,"If F = P i + Q j + R k is a vector field on $\mathbb{R}^{3}$, then the curl of F is defined by $$\operatorname{Curl}(F) = \nabla \times F$$ where $\nabla$ is the differential operator. Is there a geometric intuition behind the curl of a vector function F ? I would like to see a clear geometric diagram explaining the concept of the curl of F , as my book only gives the definition. Any help is appreciated!","If F = P i + Q j + R k is a vector field on $\mathbb{R}^{3}$, then the curl of F is defined by $$\operatorname{Curl}(F) = \nabla \times F$$ where $\nabla$ is the differential operator. Is there a geometric intuition behind the curl of a vector function F ? I would like to see a clear geometric diagram explaining the concept of the curl of F , as my book only gives the definition. Any help is appreciated!",,['multivariable-calculus']
33,Learn linear algebra in a week,Learn linear algebra in a week,,"I'am finishing my undergraduate degree in computer science and despite having had to take some math classes my math ability is still pretty poor. I struggled a lot with it which I believe was due to missing some pieces of knowledge that I needed to know and not seeing the big picture.  Now i'am studying neural networks and turns out they require quite some math(eg: the backpropagation algorithm uses the chain rule). Some people say you don't need the math but I don't think I will be able to to understand neural networks completely without it. And even if I could get away with I would probably still need the math later on. I have about a month that i can dedicate fully to this quest and I'm determined to learn linear algebra and multivariate calculus in this time frame. I will start with linear algebra (doesn't depend on calculus, right?) and I plan on learning from MIT 18.06 video lectures as well as doing the assignments. They have solutions so I should be able to easily track my progress. I also found this course from Berkeley Math 110. Linear Algebra . It doesn't have video lectures but it has more assignments with solutions so I can practice even more. As for textbooks MIT uses ""Introduction to Linear Algebra, Fourth Edition, Gilbert Strang"" and Berkeley uses ""Linear Algebra by S.H. Friedberg,A.L. Insel and L.E. Spence,Fourth Edition"". People here seem to say good things about them. I'am starting this journey tomorrow. In the mean time I'd like to get some advice. Do you have any advice in order to make this process smoother? Are there any other resources I should know about?","I'am finishing my undergraduate degree in computer science and despite having had to take some math classes my math ability is still pretty poor. I struggled a lot with it which I believe was due to missing some pieces of knowledge that I needed to know and not seeing the big picture.  Now i'am studying neural networks and turns out they require quite some math(eg: the backpropagation algorithm uses the chain rule). Some people say you don't need the math but I don't think I will be able to to understand neural networks completely without it. And even if I could get away with I would probably still need the math later on. I have about a month that i can dedicate fully to this quest and I'm determined to learn linear algebra and multivariate calculus in this time frame. I will start with linear algebra (doesn't depend on calculus, right?) and I plan on learning from MIT 18.06 video lectures as well as doing the assignments. They have solutions so I should be able to easily track my progress. I also found this course from Berkeley Math 110. Linear Algebra . It doesn't have video lectures but it has more assignments with solutions so I can practice even more. As for textbooks MIT uses ""Introduction to Linear Algebra, Fourth Edition, Gilbert Strang"" and Berkeley uses ""Linear Algebra by S.H. Friedberg,A.L. Insel and L.E. Spence,Fourth Edition"". People here seem to say good things about them. I'am starting this journey tomorrow. In the mean time I'd like to get some advice. Do you have any advice in order to make this process smoother? Are there any other resources I should know about?",,"['linear-algebra', 'multivariable-calculus', 'neural-networks']"
34,"function positive, if positive on axis","function positive, if positive on axis",,"I'm stuck on this little problem, and I'd appreciate some help shouldn't be too hard, but I can't really get my head around it: Let $f: \mathbb R^2 \to \mathbb R$ be a $C^1$ function such that \begin{cases} \frac{\partial f}{\partial t}(x,t) = \frac{\partial f}{\partial x}(x,t), & \forall \, (x,t) \in \mathbb R^2 \\ f(x,0) > 0, & \forall \, (x,0) \in \mathbb R^2 \end{cases} Show that $f(x,t) >0$, $\forall \, (x,t) \in \mathbb R^2$. By continuity I can show that there exists a region around the $x$-axis where this is true. I'm not sure though how to extend this idea!","I'm stuck on this little problem, and I'd appreciate some help shouldn't be too hard, but I can't really get my head around it: Let $f: \mathbb R^2 \to \mathbb R$ be a $C^1$ function such that \begin{cases} \frac{\partial f}{\partial t}(x,t) = \frac{\partial f}{\partial x}(x,t), & \forall \, (x,t) \in \mathbb R^2 \\ f(x,0) > 0, & \forall \, (x,0) \in \mathbb R^2 \end{cases} Show that $f(x,t) >0$, $\forall \, (x,t) \in \mathbb R^2$. By continuity I can show that there exists a region around the $x$-axis where this is true. I'm not sure though how to extend this idea!",,"['calculus', 'functions', 'multivariable-calculus']"
35,Derivative of vector consisting of euclidean distances,Derivative of vector consisting of euclidean distances,,"I have $g: \Bbb R^2 \to \Bbb R^4$ given by $g(x) = (\|c_1 - x\|, \dots, \|c_4 - x\|)$, where $c_1, \dots, c_4 \in \Bbb R^2$. I want to find $\left( \dfrac {\partial g} {\partial x_1}, \dfrac {\partial g} {\partial x_2} \right)$. Any hints?","I have $g: \Bbb R^2 \to \Bbb R^4$ given by $g(x) = (\|c_1 - x\|, \dots, \|c_4 - x\|)$, where $c_1, \dots, c_4 \in \Bbb R^2$. I want to find $\left( \dfrac {\partial g} {\partial x_1}, \dfrac {\partial g} {\partial x_2} \right)$. Any hints?",,"['calculus', 'multivariable-calculus', 'derivatives']"
36,Find the maximum of the value $F=x_{1}x_{2}x_{3}+x_{2}x_{3}x_{4}+\cdots+x_{n-2}x_{n-1}x_{n}+x_{n-1}x_{n}x_{1}$,Find the maximum of the value,F=x_{1}x_{2}x_{3}+x_{2}x_{3}x_{4}+\cdots+x_{n-2}x_{n-1}x_{n}+x_{n-1}x_{n}x_{1},"Let $x_{i}\ge 0$ and such $$x_{1}+x_{2}+\cdots+x_{n}=1$$   Find the maximum of the value   $$F=x_{1}x_{2}x_{3}+x_{2}x_{3}x_{4}+\cdots+x_{n-2}x_{n-1}x_{n}+x_{n-1}x_{n}x_{1}$$ case $1$. when $n=3$,then $$F=2x_{1}x_{2}x_{3}\le2\dfrac{(x_{1}+x_{2}+x_{3})^3}{27}=\dfrac{2}{27}$$ but for $n\ge 4$,I can't solve it","Let $x_{i}\ge 0$ and such $$x_{1}+x_{2}+\cdots+x_{n}=1$$   Find the maximum of the value   $$F=x_{1}x_{2}x_{3}+x_{2}x_{3}x_{4}+\cdots+x_{n-2}x_{n-1}x_{n}+x_{n-1}x_{n}x_{1}$$ case $1$. when $n=3$,then $$F=2x_{1}x_{2}x_{3}\le2\dfrac{(x_{1}+x_{2}+x_{3})^3}{27}=\dfrac{2}{27}$$ but for $n\ge 4$,I can't solve it",,"['multivariable-calculus', 'inequality', 'polynomials', 'optimization', 'a.m.-g.m.-inequality']"
37,How to prove $\int_{\mathbb R^2}\frac{dxdy}{(x^2+y^2+1)^{3/2}}=2\pi$,How to prove,\int_{\mathbb R^2}\frac{dxdy}{(x^2+y^2+1)^{3/2}}=2\pi,"I want to use polar coordinates to prove this: $$\int_{\mathbb R^2}\frac{dxdy}{(x^2+y^2+1)^{3/2}}=2\pi$$ I'm thinking to make the following substitution: $x^2+y^2=r^2, dx=-r\sin\theta d\theta, dy=r\cos\theta d\theta$ Am I in the right way? I need help how to proceed.","I want to use polar coordinates to prove this: $$\int_{\mathbb R^2}\frac{dxdy}{(x^2+y^2+1)^{3/2}}=2\pi$$ I'm thinking to make the following substitution: $x^2+y^2=r^2, dx=-r\sin\theta d\theta, dy=r\cos\theta d\theta$ Am I in the right way? I need help how to proceed.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
38,Intersection of a sphere and a surface,Intersection of a sphere and a surface,,"So I need to find an intersection of a sphere and a surface. The equation of the sphere is $x^2+y^2+z^2=r^2$ and surface is $a(xy+yz+xz)=xyz$ where $a \gt 0$. I am not even sure this is possible, I tried using spherical coordinates. I am solving an old exam and this is the part of a task: Show that tangent planes in the points where the above surface intersects the above sphere cut off the parts on coordinate axes whose sum is a constant. Thanks for your help!","So I need to find an intersection of a sphere and a surface. The equation of the sphere is $x^2+y^2+z^2=r^2$ and surface is $a(xy+yz+xz)=xyz$ where $a \gt 0$. I am not even sure this is possible, I tried using spherical coordinates. I am solving an old exam and this is the part of a task: Show that tangent planes in the points where the above surface intersects the above sphere cut off the parts on coordinate axes whose sum is a constant. Thanks for your help!",,"['multivariable-calculus', 'differential-geometry']"
39,"Why restrict the domain of polar coordinates, cylindrical coordinates, spherical, etc?","Why restrict the domain of polar coordinates, cylindrical coordinates, spherical, etc?",,"For a change of variables one needs the mapping to be injective. In the book I'm reading, we restrict the mapping of polar coordinates $g(r,\theta)$ to the domain $r>0$ and $0<\theta<2\pi$. However, why can't we also use $0\leq\theta<2\pi$ or $0<\theta\leq2\pi$ to define the domain? The same situation happens when we're talking about cylindrical, and spherical coordinates. In these cases, can we extend the domain in a similar manner? Any help would be appreciated.","For a change of variables one needs the mapping to be injective. In the book I'm reading, we restrict the mapping of polar coordinates $g(r,\theta)$ to the domain $r>0$ and $0<\theta<2\pi$. However, why can't we also use $0\leq\theta<2\pi$ or $0<\theta\leq2\pi$ to define the domain? The same situation happens when we're talking about cylindrical, and spherical coordinates. In these cases, can we extend the domain in a similar manner? Any help would be appreciated.",,"['multivariable-calculus', 'polar-coordinates', 'spherical-coordinates', 'cylindrical-coordinates', 'map-projections']"
40,Using the commutator to show unit vectors in polar coordinates are a noncoordinate basis?,Using the commutator to show unit vectors in polar coordinates are a noncoordinate basis?,,"I'm having trouble getting the algebra right here and don't know where I'm going wrong: Show that the unit basis vector fields for polar coordinates in the Euclidean plane, $$\hat{\mathbf{r}} = \cos\theta \hat{\mathbf{x}} + \sin\theta \hat{\mathbf{y}} \\ \hat{\mathbf{\theta}} = -\sin\theta \hat{\mathbf{x}} + \cos\theta \hat{\mathbf{y}} $$ where $\hat{\mathbf{x}}  = \partial/\partial x $, $\hat{\mathbf{y}}  = \partial/\partial y $, are a noncoordinate basis. I started by noting that $x=r\cos\theta$, $y=r\sin\theta$, therefore $\dfrac{\partial}{\partial x}\cos\theta = \dfrac{1}{r}$ and $\dfrac{\partial}{\partial x}\sin\theta= \dfrac{1}{r}$. Then I wrote out the commutator components: $$ [\hat{\mathbf{r}}, \hat{\mathbf{\theta}}] = \left[r^i {\partial\over\partial x^i}, \theta^j {\partial\over\partial x^j}\right] \\  = \left(r^i {\partial\theta^j\over\partial x^i}- \theta^i {\partial r^j\over\partial x^i}\right){\partial\over\partial x^j} \\ =-cos\theta{\partial \sin\theta \over \partial x}{\partial  \over \partial x} + \cos\theta{\partial \cos\theta \over \partial x}{\partial  \over \partial y}  -\sin\theta{\partial \sin\theta \over \partial y}{\partial  \over \partial x} + \sin\theta{\partial \cos\theta \over \partial y}{\partial  \over \partial y} \\ + \sin\theta{\partial cos\theta \over \partial x}{\partial  \over \partial x} + \sin\theta{\partial sin\theta \over \partial x}{\partial  \over \partial y} - \cos\theta{\partial cos\theta \over \partial y}{\partial  \over \partial x} - \cos\theta{\partial sin\theta \over \partial y}{\partial  \over \partial y} \\ = 0 + \cos\theta {1\over r}{\partial \over \partial y} - \sin\theta {1\over r}{\partial \over \partial x} + 0 + \sin\theta {1\over r}{\partial \over \partial x}  +0+0-\cos\theta {1\over r}{\partial \over \partial y} = 0 $$ but the answer is supposed to be non-zero, so I've included terms that I shouldn't have somewhere.","I'm having trouble getting the algebra right here and don't know where I'm going wrong: Show that the unit basis vector fields for polar coordinates in the Euclidean plane, $$\hat{\mathbf{r}} = \cos\theta \hat{\mathbf{x}} + \sin\theta \hat{\mathbf{y}} \\ \hat{\mathbf{\theta}} = -\sin\theta \hat{\mathbf{x}} + \cos\theta \hat{\mathbf{y}} $$ where $\hat{\mathbf{x}}  = \partial/\partial x $, $\hat{\mathbf{y}}  = \partial/\partial y $, are a noncoordinate basis. I started by noting that $x=r\cos\theta$, $y=r\sin\theta$, therefore $\dfrac{\partial}{\partial x}\cos\theta = \dfrac{1}{r}$ and $\dfrac{\partial}{\partial x}\sin\theta= \dfrac{1}{r}$. Then I wrote out the commutator components: $$ [\hat{\mathbf{r}}, \hat{\mathbf{\theta}}] = \left[r^i {\partial\over\partial x^i}, \theta^j {\partial\over\partial x^j}\right] \\  = \left(r^i {\partial\theta^j\over\partial x^i}- \theta^i {\partial r^j\over\partial x^i}\right){\partial\over\partial x^j} \\ =-cos\theta{\partial \sin\theta \over \partial x}{\partial  \over \partial x} + \cos\theta{\partial \cos\theta \over \partial x}{\partial  \over \partial y}  -\sin\theta{\partial \sin\theta \over \partial y}{\partial  \over \partial x} + \sin\theta{\partial \cos\theta \over \partial y}{\partial  \over \partial y} \\ + \sin\theta{\partial cos\theta \over \partial x}{\partial  \over \partial x} + \sin\theta{\partial sin\theta \over \partial x}{\partial  \over \partial y} - \cos\theta{\partial cos\theta \over \partial y}{\partial  \over \partial x} - \cos\theta{\partial sin\theta \over \partial y}{\partial  \over \partial y} \\ = 0 + \cos\theta {1\over r}{\partial \over \partial y} - \sin\theta {1\over r}{\partial \over \partial x} + 0 + \sin\theta {1\over r}{\partial \over \partial x}  +0+0-\cos\theta {1\over r}{\partial \over \partial y} = 0 $$ but the answer is supposed to be non-zero, so I've included terms that I shouldn't have somewhere.",,"['multivariable-calculus', 'differential-geometry', 'mathematical-physics']"
41,Why is the definition of derivative what it is?,Why is the definition of derivative what it is?,,"In our lectures, we've been taught the following: We say that $f:\mathbb{R}^3\to\mathbb{R}$ is differentiable at a point $X$,iff there exists $\alpha\in\mathbb{R}^3$ such that $$\epsilon (H)=\frac{f(X+H)-f(X)-\alpha\cdot H}{\|H\|}\to0$$ as $\|H\|\to0$ and the derivative is $\alpha$ But I can't understand why this should work? What is the intuition behind setting up $\epsilon(H)$ like this? Why the dot product ($\alpha\cdot H$)? What does $\alpha$ represent physically on the curve? Please help, thanks.","In our lectures, we've been taught the following: We say that $f:\mathbb{R}^3\to\mathbb{R}$ is differentiable at a point $X$,iff there exists $\alpha\in\mathbb{R}^3$ such that $$\epsilon (H)=\frac{f(X+H)-f(X)-\alpha\cdot H}{\|H\|}\to0$$ as $\|H\|\to0$ and the derivative is $\alpha$ But I can't understand why this should work? What is the intuition behind setting up $\epsilon(H)$ like this? Why the dot product ($\alpha\cdot H$)? What does $\alpha$ represent physically on the curve? Please help, thanks.",,"['calculus', 'multivariable-calculus', 'derivatives', 'definition', 'intuition']"
42,Evaluate the flux of the vector field $\vec F = -9\hat j- 3 \hat k$ on the surface $z=y$ bounded by the sphere $x^2+y^2+z^2=16$,Evaluate the flux of the vector field  on the surface  bounded by the sphere,\vec F = -9\hat j- 3 \hat k z=y x^2+y^2+z^2=16,"Evaluate the flux of the vector field $\vec F = -9\hat j- 3 \hat k$ on the surface $z=y$ bounded by the sphere $x^2+y^2+z^2=16$ My attempt: $$\iint_S \vec F \cdot \vec n dS = \iint_S (0,-9,-3) \cdot (0,1,-1) dS = -6\iint_S  dS = -6A$$ Where $A$ is the area of the surface. $A$ equal to the area of a circle with radius $4$, so $A= \pi \cdot 4^2 = 16 \pi$ Therefore the flux is: $$\iint_S \vec F \cdot \vec n dS = -6A = -96\pi$$ But the correct answer is $-48 \sqrt{2}\pi$. Where is my mistake?","Evaluate the flux of the vector field $\vec F = -9\hat j- 3 \hat k$ on the surface $z=y$ bounded by the sphere $x^2+y^2+z^2=16$ My attempt: $$\iint_S \vec F \cdot \vec n dS = \iint_S (0,-9,-3) \cdot (0,1,-1) dS = -6\iint_S  dS = -6A$$ Where $A$ is the area of the surface. $A$ equal to the area of a circle with radius $4$, so $A= \pi \cdot 4^2 = 16 \pi$ Therefore the flux is: $$\iint_S \vec F \cdot \vec n dS = -6A = -96\pi$$ But the correct answer is $-48 \sqrt{2}\pi$. Where is my mistake?",,"['multivariable-calculus', 'vector-analysis']"
43,"If a multiple integral is zero over some region, can I say the integrand is zero?","If a multiple integral is zero over some region, can I say the integrand is zero?",,"Consider the following problem Let the integral of a real function of 3 real variables $F(x,y,z)$ over some volume $V$ of $R^{3}$ vanish, $\int$$\int$$\int$$dxdydz$ $F(x,y,z)$$=0$ Now assume this continues to hold for any finite volume $V$ contained in a larger volume $V'$ of $R^{3}$. Can I say the integrand is necessarily zero over $V'$? This seems to be valid for definite integrals on a line (although I'm also not sure), and I feel the function would have to vanish, but I don't see how to prove this.","Consider the following problem Let the integral of a real function of 3 real variables $F(x,y,z)$ over some volume $V$ of $R^{3}$ vanish, $\int$$\int$$\int$$dxdydz$ $F(x,y,z)$$=0$ Now assume this continues to hold for any finite volume $V$ contained in a larger volume $V'$ of $R^{3}$. Can I say the integrand is necessarily zero over $V'$? This seems to be valid for definite integrals on a line (although I'm also not sure), and I feel the function would have to vanish, but I don't see how to prove this.",,"['calculus', 'multivariable-calculus', 'multiple-integral']"
44,Conjecture about Cal 1 derivatives?,Conjecture about Cal 1 derivatives?,,"Conjecture: Let $F\left(\vec{x}\right) : \Bbb{R}^n \to \Bbb{R}$ Define $g(t) = F(t, t, \dots, t)$ Then $$g^{\prime} (t) = \left(\sum_{i=1}^n \ { \partial F \over \partial  x_i}\right)\Big( \ \{ \ \vec{x} = \langle t,t,\dots,t\rangle \Big)$$ This theorem, if true, could greatly simplify finding the derivatives of most cal 1 functions, and functions in general. My first observation of this pattern was in finding the derivative of $x^x$ Informally, this function has two parts - a part which behaves like $x^a$, and a part which behaves like $a^x$. Treating $a$ as a constant and deriving these two functions with respect to $x$ yields $ax^{a-1}$ and $a^x \ln x$. Plugging $x$ back in for $a$ in these yields $x^x$ and $x^x \ln x$. Separately, these derivatives are incorrect, but their sum, $x^x + x^x \ln x$, is the correct derivative. This pattern held for several examples that I verified by hand, including $x^2 = x \cdot x $, $2x = x + x$, and even the function $\log_x (x)$, which is identical to $1$, yields the correct derivative $0$ when derived this method. And yet I don't know how to prove the conjecture in general. However, contained within this method are several other basic derivative rules. For example, the product rule and quotient rule can be proven with this method. Product rule proof: let $g(t) = a(t)b(t)$. Define $F(x,y) = a(x)b(y)$ Then ${\partial F \over \partial x} = a^{\prime}(x) \cdot b(y)\\ {\partial F \over \partial y} = b^{\prime}(y) \cdot a(x)$ So $g^{\prime}(t)= {\partial F \over \partial x} (t,t) + {\partial F \over \partial y} (t,t) = a^{\prime}(t)  \cdot b(t) + b^{\prime}(t) \cdot a(t)$ Similar proof for the quotient rule. This conjecture also must be true for all $F$ which are linear combinations of smaller, univariate functions,  $F = \sum_i \ a_i \ f_i (x_i)$, because then this method demonstrates the linearity of the derivative operation. This conjecture can also prove the power rule, ${d \over dx} x^n = nx^{n-1}$ Proof: Define $F(\vec{x}) = \prod_{i=1}^n \ x_i$, and thus $g(t) := F(t,t,\dots,t) = t^n$ Then $${\partial F \over \partial x_i} =\prod_{1 \ \le \ r \ \le \ n, \ \ r \ \ne \ i} \ \ x_r$$  Thus $${\partial F \over \partial x_i}(t,\dots,t) = t^{n-1} $$ So $$g^{\prime}(t) = \sum_i { \partial F \over \partial x_i} (t,\dots,t)  = \sum_{i=1}^n t^{n-1} = n \ t^{n-1}$$ I am very confident that this theorem could be true in general, but I can't think of a way to prove it in general. My professor was unconvinced when I showed him, and neither of us could think of any prior theorem this conjecture might be calling to. Is this conjecture true in general, or does there exist a counterexample function?","Conjecture: Let $F\left(\vec{x}\right) : \Bbb{R}^n \to \Bbb{R}$ Define $g(t) = F(t, t, \dots, t)$ Then $$g^{\prime} (t) = \left(\sum_{i=1}^n \ { \partial F \over \partial  x_i}\right)\Big( \ \{ \ \vec{x} = \langle t,t,\dots,t\rangle \Big)$$ This theorem, if true, could greatly simplify finding the derivatives of most cal 1 functions, and functions in general. My first observation of this pattern was in finding the derivative of $x^x$ Informally, this function has two parts - a part which behaves like $x^a$, and a part which behaves like $a^x$. Treating $a$ as a constant and deriving these two functions with respect to $x$ yields $ax^{a-1}$ and $a^x \ln x$. Plugging $x$ back in for $a$ in these yields $x^x$ and $x^x \ln x$. Separately, these derivatives are incorrect, but their sum, $x^x + x^x \ln x$, is the correct derivative. This pattern held for several examples that I verified by hand, including $x^2 = x \cdot x $, $2x = x + x$, and even the function $\log_x (x)$, which is identical to $1$, yields the correct derivative $0$ when derived this method. And yet I don't know how to prove the conjecture in general. However, contained within this method are several other basic derivative rules. For example, the product rule and quotient rule can be proven with this method. Product rule proof: let $g(t) = a(t)b(t)$. Define $F(x,y) = a(x)b(y)$ Then ${\partial F \over \partial x} = a^{\prime}(x) \cdot b(y)\\ {\partial F \over \partial y} = b^{\prime}(y) \cdot a(x)$ So $g^{\prime}(t)= {\partial F \over \partial x} (t,t) + {\partial F \over \partial y} (t,t) = a^{\prime}(t)  \cdot b(t) + b^{\prime}(t) \cdot a(t)$ Similar proof for the quotient rule. This conjecture also must be true for all $F$ which are linear combinations of smaller, univariate functions,  $F = \sum_i \ a_i \ f_i (x_i)$, because then this method demonstrates the linearity of the derivative operation. This conjecture can also prove the power rule, ${d \over dx} x^n = nx^{n-1}$ Proof: Define $F(\vec{x}) = \prod_{i=1}^n \ x_i$, and thus $g(t) := F(t,t,\dots,t) = t^n$ Then $${\partial F \over \partial x_i} =\prod_{1 \ \le \ r \ \le \ n, \ \ r \ \ne \ i} \ \ x_r$$  Thus $${\partial F \over \partial x_i}(t,\dots,t) = t^{n-1} $$ So $$g^{\prime}(t) = \sum_i { \partial F \over \partial x_i} (t,\dots,t)  = \sum_{i=1}^n t^{n-1} = n \ t^{n-1}$$ I am very confident that this theorem could be true in general, but I can't think of a way to prove it in general. My professor was unconvinced when I showed him, and neither of us could think of any prior theorem this conjecture might be calling to. Is this conjecture true in general, or does there exist a counterexample function?",,"['calculus', 'multivariable-calculus', 'derivatives', 'proof-verification', 'partial-derivative']"
45,Implicit derivation to find $\partial x/\partial v$?,Implicit derivation to find ?,\partial x/\partial v,"I saw this question: $$\begin{cases} x^2+y^2=u \\ x\sin y+y=v\end{cases}$$ What is the $\partial x/\partial v$? I think it should be $1/\sin(y)$ because $\partial v/\partial x=\sin y$, but the answer is $$\frac{-y}{x^2\cos y+x-y\sin y}$$ But why?","I saw this question: $$\begin{cases} x^2+y^2=u \\ x\sin y+y=v\end{cases}$$ What is the $\partial x/\partial v$? I think it should be $1/\sin(y)$ because $\partial v/\partial x=\sin y$, but the answer is $$\frac{-y}{x^2\cos y+x-y\sin y}$$ But why?",,"['calculus', 'multivariable-calculus']"
46,Calculate the line integral of a half circle as a standing unit circle?,Calculate the line integral of a half circle as a standing unit circle?,,"Calculate the line integral $$ \rm I=\int_{C}\mathbf{v}\cdot d\mathbf{r} \tag{01} $$ where $$ \mathbf{v}\left(x,y\right)=y\mathbf{i}+\left(-x\right)\mathbf{j} \tag{02} $$ and $C$ is the semicircle of radius $2$ centred at the origin from $(0,2)$ to $(0,-2)$ to the negative x axis (left half-plane). I have used the parametrisation of $\mathbf{r}\left(t\right) = (2\cos t, 2\sin t)$, $t \in [0,{\pi}]$. The answer I get is -4$\pi$ I have no idea if this is correct or not. Is my orientation correct, is my bound for $t$ correct, since this is a closed unit circle would it not be $[{\pi/2},{-\pi/2}]$, etc...","Calculate the line integral $$ \rm I=\int_{C}\mathbf{v}\cdot d\mathbf{r} \tag{01} $$ where $$ \mathbf{v}\left(x,y\right)=y\mathbf{i}+\left(-x\right)\mathbf{j} \tag{02} $$ and $C$ is the semicircle of radius $2$ centred at the origin from $(0,2)$ to $(0,-2)$ to the negative x axis (left half-plane). I have used the parametrisation of $\mathbf{r}\left(t\right) = (2\cos t, 2\sin t)$, $t \in [0,{\pi}]$. The answer I get is -4$\pi$ I have no idea if this is correct or not. Is my orientation correct, is my bound for $t$ correct, since this is a closed unit circle would it not be $[{\pi/2},{-\pi/2}]$, etc...",,"['multivariable-calculus', 'vectors']"
47,"Triple Integral $\iiint\limits_Wz\frac{e^{2x^2+2y^2}}2\,dx\,dy\,dz$ where $W=\{(x,y,z):x^2+y^2=1,0\le z\le4\}$",Triple Integral  where,"\iiint\limits_Wz\frac{e^{2x^2+2y^2}}2\,dx\,dy\,dz W=\{(x,y,z):x^2+y^2=1,0\le z\le4\}","They ask me find the following: $W$ is the solid bounded by the limited right circular cylinder: $$ x^2+y^2=1$$ and the planes: $$z=0, z=4$$ must calculate: $$\iiint_W z\frac{e^{2x^2+2y^2}}{2}\,dx\,dy\,dz$$ My procedure was as follows, I have in cylindrical coordinates: $$ x = r\cos(\theta ), y = r\sin(\theta ), z = z, r^2=x^2+y^2$$ therefore it poses work well $$\int_0^{2\pi}\int_0^1\int_0^4z\frac{e^{2r^{2}}}{2}r\,dz\,dr\,d\theta$$ but at this point I find problems to develop this integral as I could not develop any method; by parts or change by Fubini. Any advice will be of much help, thanks in advance!","They ask me find the following: $W$ is the solid bounded by the limited right circular cylinder: $$ x^2+y^2=1$$ and the planes: $$z=0, z=4$$ must calculate: $$\iiint_W z\frac{e^{2x^2+2y^2}}{2}\,dx\,dy\,dz$$ My procedure was as follows, I have in cylindrical coordinates: $$ x = r\cos(\theta ), y = r\sin(\theta ), z = z, r^2=x^2+y^2$$ therefore it poses work well $$\int_0^{2\pi}\int_0^1\int_0^4z\frac{e^{2r^{2}}}{2}r\,dz\,dr\,d\theta$$ but at this point I find problems to develop this integral as I could not develop any method; by parts or change by Fubini. Any advice will be of much help, thanks in advance!",,"['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
48,A clean way to obtain an (analytic or numeric) solution for this integral?,A clean way to obtain an (analytic or numeric) solution for this integral?,,"A friend and I have been looking at the crazy integral  $$\iiiint \limits^{\infty}_{-\infty}\exp\left[-(x-t)^2-(x-h)^2-(y+t)^2-(y-h)^2-10\right]\mathrm{d}V$$ and can't come up with a decent method on how to obtain a solution. ($x,y,t,h$ are all variables, not constants) Fubini's theorem would let it split into 4 integrals, but those aren't the cleanest either. I can't think of another method that will work (of course u-sub/parts, etc). Could residue theorem/contour integration work? Fourier integral? Any hints would be appreciated. Edit -- changed $y-t$ to $y+t$","A friend and I have been looking at the crazy integral  $$\iiiint \limits^{\infty}_{-\infty}\exp\left[-(x-t)^2-(x-h)^2-(y+t)^2-(y-h)^2-10\right]\mathrm{d}V$$ and can't come up with a decent method on how to obtain a solution. ($x,y,t,h$ are all variables, not constants) Fubini's theorem would let it split into 4 integrals, but those aren't the cleanest either. I can't think of another method that will work (of course u-sub/parts, etc). Could residue theorem/contour integration work? Fourier integral? Any hints would be appreciated. Edit -- changed $y-t$ to $y+t$",,"['integration', 'multivariable-calculus', 'improper-integrals']"
49,"Parametrize a curve, Multivariable Calculus","Parametrize a curve, Multivariable Calculus",,"I am stuck on what seems to be an easy exercise. We have $f(x,y) = x^2 + 4xy + y^2 \mbox{ for all } (x,y)$ in $\mathbb{R}^2.$ Now we are supposed to find a parametrization of the intersection curve between $f(x,y)$ and $z = x + 3y.$ I've been stuck for hours now, anyone got any ideas/tips? Thank you!","I am stuck on what seems to be an easy exercise. We have $f(x,y) = x^2 + 4xy + y^2 \mbox{ for all } (x,y)$ in $\mathbb{R}^2.$ Now we are supposed to find a parametrization of the intersection curve between $f(x,y)$ and $z = x + 3y.$ I've been stuck for hours now, anyone got any ideas/tips? Thank you!",,['multivariable-calculus']
50,Calculating double integral by converting to polar coordinates,Calculating double integral by converting to polar coordinates,,"Question: Evaluate the integral $$\int_0^1\int_x^1 \arctan\left(\frac{y}{x}\right) \ dy\,dx $$ My attempt: So I've converted the integral into polar coordinates, getting the integral $$\int_0^\frac{\pi}{4}\int_0^\frac{1}{\cos(\theta)} \theta r\,dr\,d\theta \ =\int_0^\frac{\pi}{4} \frac{\theta }{2\cos^2(\theta)} \, d\theta \ $$ However I have no idea where to go from here? Have I calculated the limits wrong or am I missing something? Thank you","Question: Evaluate the integral $$\int_0^1\int_x^1 \arctan\left(\frac{y}{x}\right) \ dy\,dx $$ My attempt: So I've converted the integral into polar coordinates, getting the integral $$\int_0^\frac{\pi}{4}\int_0^\frac{1}{\cos(\theta)} \theta r\,dr\,d\theta \ =\int_0^\frac{\pi}{4} \frac{\theta }{2\cos^2(\theta)} \, d\theta \ $$ However I have no idea where to go from here? Have I calculated the limits wrong or am I missing something? Thank you",,"['calculus', 'integration', 'multivariable-calculus']"
51,"Is $\frac{1}{xy}$ convex for $x,y>0$",Is  convex for,"\frac{1}{xy} x,y>0","Is the function $$f(x,y) = \frac{1}{xy}$$ convex for $x,y>0$. I computed the hessian but it is very complicated and I do not know how to show it is positive semi definite.","Is the function $$f(x,y) = \frac{1}{xy}$$ convex for $x,y>0$. I computed the hessian but it is very complicated and I do not know how to show it is positive semi definite.",,"['real-analysis', 'multivariable-calculus']"
52,Evaluate this integral using cylindrical coordinates,Evaluate this integral using cylindrical coordinates,,"Find the volume of the solid bounded above by the paraboloid of revolution $z^{2}=x^{2}+y^{2}$ And below by the $xy$ plane, and on the sides by the cylinder $x^{2}+y^{2}=2ax$ We take $a>0$. I'm struggling to understand what this would look like graphically, I understand how to find limits of integration for $x$ and $y$ but struggling to find them for $z$. So far i have equated the two terms, but i have got no where with that. Thanks","Find the volume of the solid bounded above by the paraboloid of revolution $z^{2}=x^{2}+y^{2}$ And below by the $xy$ plane, and on the sides by the cylinder $x^{2}+y^{2}=2ax$ We take $a>0$. I'm struggling to understand what this would look like graphically, I understand how to find limits of integration for $x$ and $y$ but struggling to find them for $z$. So far i have equated the two terms, but i have got no where with that. Thanks",,"['calculus', 'integration', 'multivariable-calculus']"
53,Second derivative test failure?,Second derivative test failure?,,"$f(x,y)=12x^2+12y^2+(x+y)^3$ Find all local maxima, minima and saddle points. I found $2$ critical points $(0,0)$ and $(-2,-2)$ but the second derivative test came out weird for the set of points $(-2,-2)$. I got $f_{xx}=0$ and the discriminant bigger than $0$. That doesn't imply anything (not maxima, not minima, not saddle point and test isn't even inconclusive). What do I do at this point?","$f(x,y)=12x^2+12y^2+(x+y)^3$ Find all local maxima, minima and saddle points. I found $2$ critical points $(0,0)$ and $(-2,-2)$ but the second derivative test came out weird for the set of points $(-2,-2)$. I got $f_{xx}=0$ and the discriminant bigger than $0$. That doesn't imply anything (not maxima, not minima, not saddle point and test isn't even inconclusive). What do I do at this point?",,"['multivariable-calculus', 'derivatives', 'optimization']"
54,Why does $ds=\frac{dxdy}{|n\cdot k|}$ for surface integrals?,Why does  for surface integrals?,ds=\frac{dxdy}{|n\cdot k|},I have come across the answer to a surface integral here: http://image.slidesharecdn.com/presentation1-130305202701-phpapp01/95/integral-permukaan-15-638.jpg?cb=1362515311 And at one stage it says: $$ds=\frac{dxdy}{|n\cdot k|}$$ I don't understand this step or where this formula comes from for surface integrals. Can anyone explain why this is the case and what $k$ is? Thanks.,I have come across the answer to a surface integral here: http://image.slidesharecdn.com/presentation1-130305202701-phpapp01/95/integral-permukaan-15-638.jpg?cb=1362515311 And at one stage it says: $$ds=\frac{dxdy}{|n\cdot k|}$$ I don't understand this step or where this formula comes from for surface integrals. Can anyone explain why this is the case and what $k$ is? Thanks.,,"['multivariable-calculus', 'surfaces']"
55,Is supremum over a compact domain of separately continuous function continuous?,Is supremum over a compact domain of separately continuous function continuous?,,"Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces, and consider the product metric space $(X\times Y,d)$ with a product metric $d$. Let $f(x,y):X\times Y\to \mathbb{R}$ be a separately continuous function. Suppose $X$: compact. Is $g(y)=\sup\limits_{x\in X}f(x,y)$ continuous? What if $Y$ is also compact? ++++++++++++++++++++++++++++++++++++++++++++++++++++ ++++++++++++++++++++++++++++++++++++++++++++++++++++ Suppose $X$ is not compact. Then, it fails even when $f$ is (jointly) continuous. A counter example is given http://at.yorku.ca/cgi-bin/bbqa?forum=ask_an_analyst_2005&task=show_msg&msg=4110.0001 .  That $x$ can go as far as possible does bad. Another counter example where $X$ is not compact is Supremum of continuous functions is continuous? . Suppose $X$ and $Y$ are compact, and $f$ is (jointly) continuous. Following the argument here: How prove this $g(x)=\sup{\{f(x,y)|0\le y\le 1\}}$ is continuous on $[0,1]$ it seems to hold, using $f$ being uniformly continuous on $X\times Y$. Go back to my question. In this case, we cannot use the uniform continuity. I tried doing the following and got stuck. From the compactness of $X$ and continuity in $x$, we can take $x^*_j\in\mathrm{argmax}_x f(x,y_j)$, ($j=1,2$), and $$ |g(y_1)-g(y_2)|\le |f(x_1^*,y_1)-f(x_1^*,y_2)|+|f(x_1^*,y_2)-f(x_2^*,y_2)|. $$ The first term is good as $f(x_1^*,\cdot)$ is continuous, but I could not do anything with the second term, and started to think maybe this is not true.","Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces, and consider the product metric space $(X\times Y,d)$ with a product metric $d$. Let $f(x,y):X\times Y\to \mathbb{R}$ be a separately continuous function. Suppose $X$: compact. Is $g(y)=\sup\limits_{x\in X}f(x,y)$ continuous? What if $Y$ is also compact? ++++++++++++++++++++++++++++++++++++++++++++++++++++ ++++++++++++++++++++++++++++++++++++++++++++++++++++ Suppose $X$ is not compact. Then, it fails even when $f$ is (jointly) continuous. A counter example is given http://at.yorku.ca/cgi-bin/bbqa?forum=ask_an_analyst_2005&task=show_msg&msg=4110.0001 .  That $x$ can go as far as possible does bad. Another counter example where $X$ is not compact is Supremum of continuous functions is continuous? . Suppose $X$ and $Y$ are compact, and $f$ is (jointly) continuous. Following the argument here: How prove this $g(x)=\sup{\{f(x,y)|0\le y\le 1\}}$ is continuous on $[0,1]$ it seems to hold, using $f$ being uniformly continuous on $X\times Y$. Go back to my question. In this case, we cannot use the uniform continuity. I tried doing the following and got stuck. From the compactness of $X$ and continuity in $x$, we can take $x^*_j\in\mathrm{argmax}_x f(x,y_j)$, ($j=1,2$), and $$ |g(y_1)-g(y_2)|\le |f(x_1^*,y_1)-f(x_1^*,y_2)|+|f(x_1^*,y_2)-f(x_2^*,y_2)|. $$ The first term is good as $f(x_1^*,\cdot)$ is continuous, but I could not do anything with the second term, and started to think maybe this is not true.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'metric-spaces', 'continuity']"
56,Why is the cancellation of partial derivatives like fractions justified in this example?,Why is the cancellation of partial derivatives like fractions justified in this example?,,"Suppose we have two functions $Q=Q(q,p)$ and $p=p(q,Q)$ (the context is not important here, but if you're wondering $(p,q)$ arise as coordinates in a Hamiltonian system, and $(P,Q)$ are alternative coordinates derived from a canonical transformation). I'm looking at an example and the final step is: \begin{equation} \dots=\frac{\partial{Q}}{\partial{p}}\Bigg{|}_q\frac{\partial{p}}{\partial{Q}}\Bigg{|}_q = 1 \end{equation} Now I have looked at various stackexchange posts and spent quite a while trying understand why you can treat normal derivatives like fractions, and why you can't do the same for partial derivatives, which perhaps seems contradictory to the last step above. I have tried to justify it in the following way, can someone please tell me if this is correct (I've taken out the vertical restriction bars to save time). We have: \begin{equation} dQ=\frac{\partial{Q}}{\partial{q}}dq + \frac{\partial{Q}}{\partial{p}}dp~~~~~,~~~~~dp=\frac{\partial{p}}{\partial{q}}dq + \frac{\partial{p}}{\partial{Q}}dQ \end{equation} Now, since $q$ is held constant, $dq=0$. Hence if we divide through by $dp$ in the left equation and $dQ$ in the right we get $\frac{\partial{Q}}{\partial{p}}=\frac{dQ}{dp}$ and $\frac{\partial{p}}{\partial{Q}}=\frac{dp}{dQ}$, and using the fact that we can cancel total derivatives like fractions (for reasons I'm not wanting to discuss here, unless they are important to the dicussion), we have: \begin{equation} \frac{\partial{Q}}{\partial{p}}\frac{\partial{p}}{\partial{Q}} = \frac{dQ}{dp}\frac{dp}{dQ}=1 \end{equation} Is this correct, and if so, am I overcomplicating things at all? Is there perhaps a more intuitive reason why in this case , we can cancel the partial derivatives like fractions? Thanks","Suppose we have two functions $Q=Q(q,p)$ and $p=p(q,Q)$ (the context is not important here, but if you're wondering $(p,q)$ arise as coordinates in a Hamiltonian system, and $(P,Q)$ are alternative coordinates derived from a canonical transformation). I'm looking at an example and the final step is: \begin{equation} \dots=\frac{\partial{Q}}{\partial{p}}\Bigg{|}_q\frac{\partial{p}}{\partial{Q}}\Bigg{|}_q = 1 \end{equation} Now I have looked at various stackexchange posts and spent quite a while trying understand why you can treat normal derivatives like fractions, and why you can't do the same for partial derivatives, which perhaps seems contradictory to the last step above. I have tried to justify it in the following way, can someone please tell me if this is correct (I've taken out the vertical restriction bars to save time). We have: \begin{equation} dQ=\frac{\partial{Q}}{\partial{q}}dq + \frac{\partial{Q}}{\partial{p}}dp~~~~~,~~~~~dp=\frac{\partial{p}}{\partial{q}}dq + \frac{\partial{p}}{\partial{Q}}dQ \end{equation} Now, since $q$ is held constant, $dq=0$. Hence if we divide through by $dp$ in the left equation and $dQ$ in the right we get $\frac{\partial{Q}}{\partial{p}}=\frac{dQ}{dp}$ and $\frac{\partial{p}}{\partial{Q}}=\frac{dp}{dQ}$, and using the fact that we can cancel total derivatives like fractions (for reasons I'm not wanting to discuss here, unless they are important to the dicussion), we have: \begin{equation} \frac{\partial{Q}}{\partial{p}}\frac{\partial{p}}{\partial{Q}} = \frac{dQ}{dp}\frac{dp}{dQ}=1 \end{equation} Is this correct, and if so, am I overcomplicating things at all? Is there perhaps a more intuitive reason why in this case , we can cancel the partial derivatives like fractions? Thanks",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
57,Bivariate infinite series: explicit sum?,Bivariate infinite series: explicit sum?,,"Let $S_n(x,y)=\sum_{k=0}^n\binom{n}{k}\frac{x^k}{k!}y^{n-k}$, and consider the series $S(x,y)=\sum_{n=0}^\infty S_n(x,y)$, where $x,y\in \mathbb{R}$.  My question is: does this series have an explicit sum (i.e. closed-form expression)? The main reason why such a sum may exist is that if the binomial coefficient $\binom{n}{k}$ were set equal to unity in $S_n$, then the series becomes $S(x,y)=\frac{\exp x}{1-y}$ by the Cauchy product.  It is possible that we have a ""simple"" modification of this function that produces the binomial coefficients in $S_n$.  This seems plausible, since it can be shown that $S$ is absolutely convergent for all $x$ and for $y\in(-1,1)$, which mimics the properties of $\frac{\exp x}{1-y}$.","Let $S_n(x,y)=\sum_{k=0}^n\binom{n}{k}\frac{x^k}{k!}y^{n-k}$, and consider the series $S(x,y)=\sum_{n=0}^\infty S_n(x,y)$, where $x,y\in \mathbb{R}$.  My question is: does this series have an explicit sum (i.e. closed-form expression)? The main reason why such a sum may exist is that if the binomial coefficient $\binom{n}{k}$ were set equal to unity in $S_n$, then the series becomes $S(x,y)=\frac{\exp x}{1-y}$ by the Cauchy product.  It is possible that we have a ""simple"" modification of this function that produces the binomial coefficients in $S_n$.  This seems plausible, since it can be shown that $S$ is absolutely convergent for all $x$ and for $y\in(-1,1)$, which mimics the properties of $\frac{\exp x}{1-y}$.",,"['real-analysis', 'sequences-and-series', 'multivariable-calculus']"
58,"Determine if first and second partial derivatives are positive, negative or zero based on level curves","Determine if first and second partial derivatives are positive, negative or zero based on level curves",,"Assuming I have a point on a level curves graph for function f(x,y), how would I determine whether the first and second partial derivatives are positive, negative, or zero? I understand that for a regular graph, the slope and concavity would be the indicator, but how would it work with level curves? (partial derivatives with respect to x, y, xx, yy, and xy)","Assuming I have a point on a level curves graph for function f(x,y), how would I determine whether the first and second partial derivatives are positive, negative, or zero? I understand that for a regular graph, the slope and concavity would be the indicator, but how would it work with level curves? (partial derivatives with respect to x, y, xx, yy, and xy)",,['multivariable-calculus']
59,find extreme values of $\cos(x)+\cos(y)+\cos(z)$ when $x+y+z=\pi$,find extreme values of  when,\cos(x)+\cos(y)+\cos(z) x+y+z=\pi,"How can I find the maximum and minimum of $\cos(x)+\cos(y)+\cos(z)$ if $x,y,z\geq0$ such that they are vertices of a triangle with $x+y+z=\pi$. I don't know how to start, but I feel like the Lagrange multipliers are a good place to start.","How can I find the maximum and minimum of $\cos(x)+\cos(y)+\cos(z)$ if $x,y,z\geq0$ such that they are vertices of a triangle with $x+y+z=\pi$. I don't know how to start, but I feel like the Lagrange multipliers are a good place to start.",,"['multivariable-calculus', 'lagrange-multiplier']"
60,What are higher derivatives?,What are higher derivatives?,,"From Wikipedia : Higher derivatives can also be defined for functions of several variables, studied in multivariable calculus. In this case, instead of repeatedly applying the derivative, one repeatedly applies partial derivatives with respect to different variables. For example, the second order partial derivatives of a scalar function of n variables can be organized into an n by n matrix, the Hessian matrix. One of the subtle points is that the higher derivatives are not intrinsically defined, and depend on the choice of the coordinates in a complicated fashion (in particular, the Hessian matrix of a function is not a tensor). Nevertheless, higher derivatives have important applications to analysis of local extrema of a function at its critical points. For an advanced application of this analysis to topology of manifolds, see Morse theory. In multivariable calculus, I was told that higher derivatives were tensors and that was the reason we never went beyond Hessians (none of us had studied tensors before).  If higher derivatives aren't tensors, then what are they?  Where can I learn more about them?","From Wikipedia : Higher derivatives can also be defined for functions of several variables, studied in multivariable calculus. In this case, instead of repeatedly applying the derivative, one repeatedly applies partial derivatives with respect to different variables. For example, the second order partial derivatives of a scalar function of n variables can be organized into an n by n matrix, the Hessian matrix. One of the subtle points is that the higher derivatives are not intrinsically defined, and depend on the choice of the coordinates in a complicated fashion (in particular, the Hessian matrix of a function is not a tensor). Nevertheless, higher derivatives have important applications to analysis of local extrema of a function at its critical points. For an advanced application of this analysis to topology of manifolds, see Morse theory. In multivariable calculus, I was told that higher derivatives were tensors and that was the reason we never went beyond Hessians (none of us had studied tensors before).  If higher derivatives aren't tensors, then what are they?  Where can I learn more about them?",,"['multivariable-calculus', 'derivatives', 'tensors']"
61,What is the unit normal vector of the curve $y + x^2 = 1$,What is the unit normal vector of the curve,y + x^2 = 1,"What is the unit normal vector of the curve $y + x^2 = 1$, $-1 \leq x \leq 1$? I need this to calculate the flux integral of a vector field over that curve.","What is the unit normal vector of the curve $y + x^2 = 1$, $-1 \leq x \leq 1$? I need this to calculate the flux integral of a vector field over that curve.",,"['multivariable-calculus', 'vectors']"
62,Linearity of Multilinear Maps,Linearity of Multilinear Maps,,"If $f: \mathbb{R^n} \rightarrow \mathbb{R^m}$, with $n>1$, is a multilinear map, is $f$ linear? I think $f$ is only linear for the special case that the range of $f$ consists of a single element, $\vec 0$. Is this correct?","If $f: \mathbb{R^n} \rightarrow \mathbb{R^m}$, with $n>1$, is a multilinear map, is $f$ linear? I think $f$ is only linear for the special case that the range of $f$ consists of a single element, $\vec 0$. Is this correct?",,"['real-analysis', 'multivariable-calculus', 'vector-analysis']"
63,Double Integral related to Gaussian Integral.,Double Integral related to Gaussian Integral.,,"We know that $\int_{-\infty}^{\infty} e^{-x^2}dx=\sqrt{\pi}.$ Using this , how can you evaluate $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-(x^2+y^2+xy)}dxdy= ?$ Are there any standard tricks for integrals which are related to the gaussian integral ?","We know that $\int_{-\infty}^{\infty} e^{-x^2}dx=\sqrt{\pi}.$ Using this , how can you evaluate $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-(x^2+y^2+xy)}dxdy= ?$ Are there any standard tricks for integrals which are related to the gaussian integral ?",,"['integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals', 'gaussian-integral']"
64,"Finding an outward pointing normal on the unit sphere,","Finding an outward pointing normal on the unit sphere,",,"I am trying to apply the divergence theorem, but I need to find an outward pointing normal vector on the unit sphere.  The answer gives $\hat n= (x_1,x_2,x_3)$. Is the person who wrote up the solution just saying that any ordered triple on the sphere is an outward pointing normal - and this is all we need for the nds part of the integral? Thanks,","I am trying to apply the divergence theorem, but I need to find an outward pointing normal vector on the unit sphere.  The answer gives $\hat n= (x_1,x_2,x_3)$. Is the person who wrote up the solution just saying that any ordered triple on the sphere is an outward pointing normal - and this is all we need for the nds part of the integral? Thanks,",,"['integration', 'multivariable-calculus', 'vector-analysis']"
65,Laplace-de Rham operator,Laplace-de Rham operator,,"Consider an operator $\partial = (-1)^k \star^{-1}\,d\star: \Omega^k(\mathbb{R}^n) \to \Omega^{k-1}(\mathbb{R}^n)$. Note that we equivalently can write $\partial = (-1)^{nk + n + 1} \star\,d\star$. Denote $\triangle := (\partial + d)^2$ (this is Laplace-de Rham operator ). a) Verify that $\triangle$ is an operator $\Omega^k(\mathbb{R}^n) \to \Omega^k(\mathbb{R}^n)$. b) Compute $\triangle(f\alpha)$ for a function $f: \mathbb{R}^n \to \mathbb{R}$ and a differential $1$-form $\alpha$. c) Find explicit formulas for $\triangle\alpha$ for the cases when $n = 3$ and $k = 0, 1, 2, 3$. d) For $n=2$ find explicit formulas for $\triangle \alpha$ in polar coordinates for $k = 0, 1, 2$. Progress so far: I know how to do a). We have $\nabla = \partial^2 + \partial d + d\partial + d^2$. Now$$\partial^2 = \star^{-1}d\star\star^{-1}\,d\star = \star^{-1}d\,d\star = 0$$so $\triangle = \partial d + d\partial$ and this is a map $\Omega^k(\mathbb{R}^n) \to \Omega^k(\mathbb{R}^n)$. But I am stuck on the other ones. Any help would be appreciated.","Consider an operator $\partial = (-1)^k \star^{-1}\,d\star: \Omega^k(\mathbb{R}^n) \to \Omega^{k-1}(\mathbb{R}^n)$. Note that we equivalently can write $\partial = (-1)^{nk + n + 1} \star\,d\star$. Denote $\triangle := (\partial + d)^2$ (this is Laplace-de Rham operator ). a) Verify that $\triangle$ is an operator $\Omega^k(\mathbb{R}^n) \to \Omega^k(\mathbb{R}^n)$. b) Compute $\triangle(f\alpha)$ for a function $f: \mathbb{R}^n \to \mathbb{R}$ and a differential $1$-form $\alpha$. c) Find explicit formulas for $\triangle\alpha$ for the cases when $n = 3$ and $k = 0, 1, 2, 3$. d) For $n=2$ find explicit formulas for $\triangle \alpha$ in polar coordinates for $k = 0, 1, 2$. Progress so far: I know how to do a). We have $\nabla = \partial^2 + \partial d + d\partial + d^2$. Now$$\partial^2 = \star^{-1}d\star\star^{-1}\,d\star = \star^{-1}d\,d\star = 0$$so $\triangle = \partial d + d\partial$ and this is a map $\Omega^k(\mathbb{R}^n) \to \Omega^k(\mathbb{R}^n)$. But I am stuck on the other ones. Any help would be appreciated.",,"['multivariable-calculus', 'differential-geometry']"
66,How to draw the picture of vector field,How to draw the picture of vector field,,"The given function is $$f(x,y,z) = \frac{1}{\sqrt{x^2+y^2+z^2}}$$ and I need to find gradient and draw the picture of this vector field. Gradient that I calculate is: grad $f(x,y,z) =  \left(\frac{-x}{(x^2+y^2+z^2)^{\frac{3}{2}}} , \frac{-y}{(x^2+y^2+z^2)^{\frac{3}{2}}} , \frac{-z}{(x^2+y^2+z^2)^{\frac{3}{2}}}\right)$ Could anyone help me to draw the vector field?. Thanks :)","The given function is $$f(x,y,z) = \frac{1}{\sqrt{x^2+y^2+z^2}}$$ and I need to find gradient and draw the picture of this vector field. Gradient that I calculate is: grad $f(x,y,z) =  \left(\frac{-x}{(x^2+y^2+z^2)^{\frac{3}{2}}} , \frac{-y}{(x^2+y^2+z^2)^{\frac{3}{2}}} , \frac{-z}{(x^2+y^2+z^2)^{\frac{3}{2}}}\right)$ Could anyone help me to draw the vector field?. Thanks :)",,['multivariable-calculus']
67,Intuition behind divergence?,Intuition behind divergence?,,"$\overrightarrow f = 3x\overrightarrow i - 3y\overrightarrow j$ $\overrightarrow g = 3x\overrightarrow i + 3y\overrightarrow j$ If I calculate the divergence of $f$ I get $0$. If I calculate the divergence of $g$ I get $6$. This seems to be saying that if the 'fluid' is flowing down it cancels out the positive horizontal flow. And if the fluid is flowing up it combines with the horizontal flow. This doesn't seem right, we have the same magnitudes in both directions for $f$ and $g$, $g$ simply goes in the opposite direction for the $\overrightarrow j$ component. Intuitively it seems to me that for $f$ and $g$ and for any point $(x, y)$ we should have a divergence of $6$...ie. at any point, the fluid is flowing out from it as opposed to towards it?","$\overrightarrow f = 3x\overrightarrow i - 3y\overrightarrow j$ $\overrightarrow g = 3x\overrightarrow i + 3y\overrightarrow j$ If I calculate the divergence of $f$ I get $0$. If I calculate the divergence of $g$ I get $6$. This seems to be saying that if the 'fluid' is flowing down it cancels out the positive horizontal flow. And if the fluid is flowing up it combines with the horizontal flow. This doesn't seem right, we have the same magnitudes in both directions for $f$ and $g$, $g$ simply goes in the opposite direction for the $\overrightarrow j$ component. Intuitively it seems to me that for $f$ and $g$ and for any point $(x, y)$ we should have a divergence of $6$...ie. at any point, the fluid is flowing out from it as opposed to towards it?",,"['multivariable-calculus', 'vector-analysis', 'fluid-dynamics']"
68,To show unique solution for the Laplace equation,To show unique solution for the Laplace equation,,"The problem is in the top while its weak form at the end, source $\hskip 1in$ I know that the solution is unique because the boundary condition is Dirichlet.  But I want to show this. How can you show the solution here is unique?","The problem is in the top while its weak form at the end, source $\hskip 1in$ I know that the solution is unique because the boundary condition is Dirichlet.  But I want to show this. How can you show the solution here is unique?",,"['multivariable-calculus', 'partial-differential-equations', 'boundary-value-problem', 'harmonic-functions']"
69,"What is the potential function of the field $\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right)$",What is the potential function of the field,"\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right)","The vector field is obviously conservative on every closed domain that doesn't encompass the point $(0,0)$, so there must be a potential function. I've got $\arctan(\frac{x}{y})$ for $x$ unequal to zero and $\arctan(\frac{y}{x})$ for $y$ unequal to zero. However, when I try to find the line integral of the given field from point $(1,0)$ to point $(0,1)$ I get $\frac{\pi}{2}$, but when I try to find the result by using the potential function I get $0$. What am I doing wrong? Thanks in advance.","The vector field is obviously conservative on every closed domain that doesn't encompass the point $(0,0)$, so there must be a potential function. I've got $\arctan(\frac{x}{y})$ for $x$ unequal to zero and $\arctan(\frac{y}{x})$ for $y$ unequal to zero. However, when I try to find the line integral of the given field from point $(1,0)$ to point $(0,1)$ I get $\frac{\pi}{2}$, but when I try to find the result by using the potential function I get $0$. What am I doing wrong? Thanks in advance.",,"['calculus', 'multivariable-calculus', 'field-theory', 'vector-fields']"
70,Integrating $\int_1^2 \int_0^ \sqrt{2x-x^2} \frac{1}{((x^2+y^2)^2} dydx $ in polar coordinates,Integrating  in polar coordinates,\int_1^2 \int_0^ \sqrt{2x-x^2} \frac{1}{((x^2+y^2)^2} dydx ,"I'm having a problem converting $\int\limits_1^2  \int\limits_0^ \sqrt{2x-x^2}   \frac{1}{(x^2+y^2)^2}  dy dx $ to polar coordinates. I drew the graph using my calculator, which looked like half a circle on the x axis.  I know that $\frac{1}{(x^2+y^2)^2}  dydx$ turns to $ \frac{r}{(r^2)^2}drd\theta$, which would be $ \frac{1}{r^3}$ The region of integration in  $\theta $ is I guess $0 \leq  \theta  \leq  \frac{1}{4}  \pi $, but I'm stuck for r. The lower boundary is $x=1=rcos\theta \rightarrow r=1/cos\theta=sec\theta$, but what is the upper boundary?? Is it  $\sqrt{2rcos\theta-r^2cos^2\theta}$? I tried this but I couldn't integrate my equation... Could someone please help me out? Also, how can I draw the graph of $y= \sqrt{2x-x^2} $? I could only know how it looked like by using my calculator...","I'm having a problem converting $\int\limits_1^2  \int\limits_0^ \sqrt{2x-x^2}   \frac{1}{(x^2+y^2)^2}  dy dx $ to polar coordinates. I drew the graph using my calculator, which looked like half a circle on the x axis.  I know that $\frac{1}{(x^2+y^2)^2}  dydx$ turns to $ \frac{r}{(r^2)^2}drd\theta$, which would be $ \frac{1}{r^3}$ The region of integration in  $\theta $ is I guess $0 \leq  \theta  \leq  \frac{1}{4}  \pi $, but I'm stuck for r. The lower boundary is $x=1=rcos\theta \rightarrow r=1/cos\theta=sec\theta$, but what is the upper boundary?? Is it  $\sqrt{2rcos\theta-r^2cos^2\theta}$? I tried this but I couldn't integrate my equation... Could someone please help me out? Also, how can I draw the graph of $y= \sqrt{2x-x^2} $? I could only know how it looked like by using my calculator...",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
71,"Interesting dilemma, answer not matching with stewart, My work is Included","Interesting dilemma, answer not matching with stewart, My work is Included",,"Question : Compute flux through the upper hemisphere of $x^2+y^2+z^2 = 1$ . Where $$\textbf{F} =  \left( z^2x\right)\textbf{ i }+\left[\dfrac{1}{3}y^3+ \tan z\right]\textbf{ j } + \left(x^2z+y^2 \right)\textbf{ k }$$ ANSWER GIVEN AT BACK OF STEWART : $\dfrac{13\pi}{20}$ MY WORK $\textbf{Divergence theorem }$ $$\textbf{Flux} = \int\int_S \textbf{F}\cdot d\textbf{S} = \int\int\int_E \text{div }\textbf{F} \hspace{2mm} dV$$ Where $S$ is a closed surface. And $E$ is the region inside that surface. In this problem, instead of computing the surface is not closed But we want to use divergence theorem, because divergence of the given vector field is cute. We will over come this problem by attaching a disk at the bottom of the hemisphere, we call this closed this closed surface $S_2$ and the disk as $S_1$ We can use divergence theorem for $S_2$ We will then find flux through $S_1$. Then the  flux through $S$ =  Flux through $S_2$  $-S_1$ $$\begin{align} & \text{div }\textbf{F} = \dfrac{\partial \left( z^2x\right)}{\partial x}+\dfrac{\partial }{\partial y}\left[\dfrac{1}{3}y^3+ \tan z\right] + \dfrac{\partial \left(x^2z+y^2 \right)}{\partial z} \\ & \text{div }\textbf{F} =  z^2+y^2 +x^2 \end{align}$$ $$\textbf{Compute Flux through $S_2$ using divergence theorem }$$ $$\int\int\int_E \text{div }\textbf{F} \hspace{2mm} dV= \int\int\int_E x^2+y^2+z^2 \hspace{2mm} dV$$ $ $ We can define $E$ in spherical as follows : $$\begin{align} & \left(\rho, \theta, \phi \right)\in E \hspace{1mm} |  \hspace{2mm} 0< \rho < 1, \hspace{2mm}  0< \phi < \dfrac{\pi}{2}, \hspace{2mm} 0< \theta < 2\pi \\ & \text{Therefore,} \quad   \int_0^{\pi/2}\int_0^{2\pi}\int_0^{1}  \rho^2  \quad (\rho) d\rho d\theta d \phi  \\  & =\int_0^{\pi/2}\int_0^{2\pi}\int_0^{1}  \rho^3  \hspace{2mm} d\rho d\theta d\phi \\ & =\int_0^{\pi/2}\int_0^{2\pi}\left[ \dfrac{\rho^4}{4}  \right]_0^{1}  \hspace{2mm}  d\theta d\phi \\  & =\dfrac{1}{4}\int_0^{\pi/2}\int_0^{2\pi}  d\theta d\phi  = \dfrac{1}{4}\times \dfrac{\pi}{2}\times 2\pi = \dfrac{\pi^2}{4}  \end{align} $$ $$\textbf{Compute Flux through $S_1$ }$$ $ $ Note that $$\int\int_{S_1} \textbf{F}\cdot d\textbf{S} = \int\int_{S_1} \textbf{F}\cdot \textbf{n}\hspace{1mm}dS$$ Note that $S_1$ is part of the plane $z=0$ $$ = \int\int_{D} \textbf{F}\cdot \textbf{n}\sqrt{\left( \dfrac{\partial z}{\partial x}\right)^2+\left( \dfrac{\partial z}{\partial y}\right)^2+1}\hspace{1mm}dA $$ Where $\textbf{n} = \textbf{k}$ [because $\textbf{n}$ is  normal unit vector to $S_1$ ] And $D$ is the region inside the circle $x^2+y^2=1$ [ Because $D$ is projection of $S_1$ on $xy$ plane ] $$ \begin{align} & = \int\int_{D} \textbf{F}\cdot \textbf{k}\sqrt{\left( 0\right)^2+\left( 0\right)^2+1}\hspace{1mm}dA \\ & = \int\int_{D} x^2z+y^2 \hspace{1mm}dA \end{align} $$ Substitute $z=0$, since $S_1$ is part of the plane $z=0$ $$ = \int\int_{D} y^2 \hspace{1mm}dA $$ In polar coordinates the Integral becomes $$ \begin{align} & = \int_0^{2\pi}\int_{0}^1 r^2\sin^2\theta \hspace{1mm}(r)drd\theta \\ &  =\left( \int_0^{2\pi}\sin^2\theta \hspace{1mm}d\theta \right) \left(\int_{0}^1 r^3 \hspace{1mm}dr \right) \\ & =\dfrac{1}{2}\left[  \theta-\dfrac{\sin2\theta}{2}\right]_0^{2\pi} \left[ \dfrac{r^4}{4} \right]_{0}^1 \\ & =\dfrac{1}{2}\times 2\pi \times \dfrac{1}{4} = \dfrac{\pi}{4} \\ \end{align}$$ Therefore, answer should be $$\dfrac{\pi^2-\pi}{4}$$","Question : Compute flux through the upper hemisphere of $x^2+y^2+z^2 = 1$ . Where $$\textbf{F} =  \left( z^2x\right)\textbf{ i }+\left[\dfrac{1}{3}y^3+ \tan z\right]\textbf{ j } + \left(x^2z+y^2 \right)\textbf{ k }$$ ANSWER GIVEN AT BACK OF STEWART : $\dfrac{13\pi}{20}$ MY WORK $\textbf{Divergence theorem }$ $$\textbf{Flux} = \int\int_S \textbf{F}\cdot d\textbf{S} = \int\int\int_E \text{div }\textbf{F} \hspace{2mm} dV$$ Where $S$ is a closed surface. And $E$ is the region inside that surface. In this problem, instead of computing the surface is not closed But we want to use divergence theorem, because divergence of the given vector field is cute. We will over come this problem by attaching a disk at the bottom of the hemisphere, we call this closed this closed surface $S_2$ and the disk as $S_1$ We can use divergence theorem for $S_2$ We will then find flux through $S_1$. Then the  flux through $S$ =  Flux through $S_2$  $-S_1$ $$\begin{align} & \text{div }\textbf{F} = \dfrac{\partial \left( z^2x\right)}{\partial x}+\dfrac{\partial }{\partial y}\left[\dfrac{1}{3}y^3+ \tan z\right] + \dfrac{\partial \left(x^2z+y^2 \right)}{\partial z} \\ & \text{div }\textbf{F} =  z^2+y^2 +x^2 \end{align}$$ $$\textbf{Compute Flux through $S_2$ using divergence theorem }$$ $$\int\int\int_E \text{div }\textbf{F} \hspace{2mm} dV= \int\int\int_E x^2+y^2+z^2 \hspace{2mm} dV$$ $ $ We can define $E$ in spherical as follows : $$\begin{align} & \left(\rho, \theta, \phi \right)\in E \hspace{1mm} |  \hspace{2mm} 0< \rho < 1, \hspace{2mm}  0< \phi < \dfrac{\pi}{2}, \hspace{2mm} 0< \theta < 2\pi \\ & \text{Therefore,} \quad   \int_0^{\pi/2}\int_0^{2\pi}\int_0^{1}  \rho^2  \quad (\rho) d\rho d\theta d \phi  \\  & =\int_0^{\pi/2}\int_0^{2\pi}\int_0^{1}  \rho^3  \hspace{2mm} d\rho d\theta d\phi \\ & =\int_0^{\pi/2}\int_0^{2\pi}\left[ \dfrac{\rho^4}{4}  \right]_0^{1}  \hspace{2mm}  d\theta d\phi \\  & =\dfrac{1}{4}\int_0^{\pi/2}\int_0^{2\pi}  d\theta d\phi  = \dfrac{1}{4}\times \dfrac{\pi}{2}\times 2\pi = \dfrac{\pi^2}{4}  \end{align} $$ $$\textbf{Compute Flux through $S_1$ }$$ $ $ Note that $$\int\int_{S_1} \textbf{F}\cdot d\textbf{S} = \int\int_{S_1} \textbf{F}\cdot \textbf{n}\hspace{1mm}dS$$ Note that $S_1$ is part of the plane $z=0$ $$ = \int\int_{D} \textbf{F}\cdot \textbf{n}\sqrt{\left( \dfrac{\partial z}{\partial x}\right)^2+\left( \dfrac{\partial z}{\partial y}\right)^2+1}\hspace{1mm}dA $$ Where $\textbf{n} = \textbf{k}$ [because $\textbf{n}$ is  normal unit vector to $S_1$ ] And $D$ is the region inside the circle $x^2+y^2=1$ [ Because $D$ is projection of $S_1$ on $xy$ plane ] $$ \begin{align} & = \int\int_{D} \textbf{F}\cdot \textbf{k}\sqrt{\left( 0\right)^2+\left( 0\right)^2+1}\hspace{1mm}dA \\ & = \int\int_{D} x^2z+y^2 \hspace{1mm}dA \end{align} $$ Substitute $z=0$, since $S_1$ is part of the plane $z=0$ $$ = \int\int_{D} y^2 \hspace{1mm}dA $$ In polar coordinates the Integral becomes $$ \begin{align} & = \int_0^{2\pi}\int_{0}^1 r^2\sin^2\theta \hspace{1mm}(r)drd\theta \\ &  =\left( \int_0^{2\pi}\sin^2\theta \hspace{1mm}d\theta \right) \left(\int_{0}^1 r^3 \hspace{1mm}dr \right) \\ & =\dfrac{1}{2}\left[  \theta-\dfrac{\sin2\theta}{2}\right]_0^{2\pi} \left[ \dfrac{r^4}{4} \right]_{0}^1 \\ & =\dfrac{1}{2}\times 2\pi \times \dfrac{1}{4} = \dfrac{\pi}{4} \\ \end{align}$$ Therefore, answer should be $$\dfrac{\pi^2-\pi}{4}$$",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'vector-analysis']"
72,Why is $\operatorname{Div}\big(\operatorname{Curl} F\big) = 0$? Intuition?,Why is ? Intuition?,\operatorname{Div}\big(\operatorname{Curl} F\big) = 0,"Why is $\operatorname{Div}\big(\operatorname{Curl} F\big) = 0$? Is there an intuitive explanation to what this means as well as an algebraic proof? Also I understand that $\operatorname{Curl} \operatorname{Grad}F =0$, I interpet this as the gradient not having any rotation since its the direction of steepest ascent and if this formed some sort of loop this would not make sense? Is this somewhat correct? Any explanations would be appreciated.","Why is $\operatorname{Div}\big(\operatorname{Curl} F\big) = 0$? Is there an intuitive explanation to what this means as well as an algebraic proof? Also I understand that $\operatorname{Curl} \operatorname{Grad}F =0$, I interpet this as the gradient not having any rotation since its the direction of steepest ascent and if this formed some sort of loop this would not make sense? Is this somewhat correct? Any explanations would be appreciated.",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
73,Separate the variables of the function $\frac{x^2}{\sqrt{x^2+y^2}}$,Separate the variables of the function,\frac{x^2}{\sqrt{x^2+y^2}},"Is there a way to express the function $\frac{x^2}{\sqrt{x^2+y^2}}$ as the product of two functions: $f(x)\cdot g(y)$, i.e. one in each variable? This is becasue I want to apply a convolution whose kernel is define that way, and I am willing to separate that kernel. For that I need to be able to express it as a product of two functions that are on each variable separatedly.","Is there a way to express the function $\frac{x^2}{\sqrt{x^2+y^2}}$ as the product of two functions: $f(x)\cdot g(y)$, i.e. one in each variable? This is becasue I want to apply a convolution whose kernel is define that way, and I am willing to separate that kernel. For that I need to be able to express it as a product of two functions that are on each variable separatedly.",,"['multivariable-calculus', 'functions', 'convolution']"
74,Find the angle of intersection of circles $x^2+y^2-6x+4=0 \ \&\ x^2+y^2-2x-2y-8=0$,Find the angle of intersection of circles,x^2+y^2-6x+4=0 \ \&\ x^2+y^2-2x-2y-8=0,Find the angle of  intersection of circles $$x^2+y^2-6x+4=0 \\  x^2+y^2-2x-2y-8=0$$ my answer is : 41.14 degrees. but i'm not sure if it's right. please help me.,Find the angle of  intersection of circles $$x^2+y^2-6x+4=0 \\  x^2+y^2-2x-2y-8=0$$ my answer is : 41.14 degrees. but i'm not sure if it's right. please help me.,,"['calculus', 'multivariable-calculus']"
75,Using Lagrange multipliers to solve for minimum,Using Lagrange multipliers to solve for minimum,,"I am having troubles with one part of this homework problem.  Hopefully somebody can help me out: Find the minimum and maximum values of the function $f(x,y)=x^2+y^2$  subject to the given constraint $x^4+y^4=18$. Using Lagrange multipliers, I can easily solve for the maximum: $f_x(x,y)=2x$ and $f_y(x,y)=2y$. If we call the second equation $g$, then: $g_x=4x^3$ and $g_y=4y^3$. Then we apply the Lagrange multiplier: $2x=\lambda 4x^3$ and $2y = \lambda 4y^3$ By solving for $x$ and $y$ and plugging in to $g$, we get $\lambda=1/\sqrt{36}=\pm1/6$. To find the maximum, I will use the positive $1/6$, and solve for $x^2=3$ and $y^2=3$, resulting in a maximum of $6$, which the system spits out as correct. For a minimum, I originally thought $0$ because $x^2$ and $y^2$ must be positive numbers, but that is not correct.  Then I noticed that since $x^2=1/(2\lambda)$, and $y$ also, then when $\lambda$ is negative that would be the minimum, resulting in $-6$, which is also not correct. So long story short, how do I find the minimum value in this case? Thanks!","I am having troubles with one part of this homework problem.  Hopefully somebody can help me out: Find the minimum and maximum values of the function $f(x,y)=x^2+y^2$  subject to the given constraint $x^4+y^4=18$. Using Lagrange multipliers, I can easily solve for the maximum: $f_x(x,y)=2x$ and $f_y(x,y)=2y$. If we call the second equation $g$, then: $g_x=4x^3$ and $g_y=4y^3$. Then we apply the Lagrange multiplier: $2x=\lambda 4x^3$ and $2y = \lambda 4y^3$ By solving for $x$ and $y$ and plugging in to $g$, we get $\lambda=1/\sqrt{36}=\pm1/6$. To find the maximum, I will use the positive $1/6$, and solve for $x^2=3$ and $y^2=3$, resulting in a maximum of $6$, which the system spits out as correct. For a minimum, I originally thought $0$ because $x^2$ and $y^2$ must be positive numbers, but that is not correct.  Then I noticed that since $x^2=1/(2\lambda)$, and $y$ also, then when $\lambda$ is negative that would be the minimum, resulting in $-6$, which is also not correct. So long story short, how do I find the minimum value in this case? Thanks!",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
76,Inequality for Integral of Vector Function,Inequality for Integral of Vector Function,,"I am trying to prove $\| \int^{b}_{a} \vec{r}(t) dt\| \leq \int^{b}_{a} \| \vec{r}(t) \|dt$. I am fairly certain that this can be derived from the Cauchy-Schwarz inequality, but I can't quite remember how to make it come out.  I tried squaring as such: $\| \int^{b}_{a} \vec{r}(t) dt\| ^2 = (\int^{b}_{a} \vec{r}(t)dt) \cdot (\int^{b}_{a} \vec{r}(t)dt) $ I can't quite recall the rest of the proof, but there should be a fairly straightforward proof of this.  Any help would be appreciated!","I am trying to prove $\| \int^{b}_{a} \vec{r}(t) dt\| \leq \int^{b}_{a} \| \vec{r}(t) \|dt$. I am fairly certain that this can be derived from the Cauchy-Schwarz inequality, but I can't quite remember how to make it come out.  I tried squaring as such: $\| \int^{b}_{a} \vec{r}(t) dt\| ^2 = (\int^{b}_{a} \vec{r}(t)dt) \cdot (\int^{b}_{a} \vec{r}(t)dt) $ I can't quite recall the rest of the proof, but there should be a fairly straightforward proof of this.  Any help would be appreciated!",,"['calculus', 'multivariable-calculus']"
77,Problems with limits of functions of two variables,Problems with limits of functions of two variables,,"I have the following function: $$f(x,y):=\begin{cases}\frac{x^3y}{x^6+y^2}&,\;\;(x,y)\neq (0,0)\\{}\\0&,\;\;(x,y)=(0,0)\end{cases}$$ I'm asked about continuity at the origin and the limit of function there. Now, the limit doesn't exist since $$\begin{align*}y=x^3&\implies f(x,x^3)=\frac{x^6}{x^6+x^6}=\frac12\xrightarrow [x\to 0]{}\frac12\;,\;\;\text{whereas}\\y=x&\implies f(x,x)=\frac{x^4}{x^6+x^2}=\frac{x^2}{x^4+1}\xrightarrow[x\to 0]{}0\end{align*}$$ My problem is: if I try to apply what's been shown in several questions in this site, namely polar coordinates, I get $$\begin{cases}x=r\cos t\\y=r\sin t\end{cases}\implies f(r,t)=\frac{r^2\cos^3t\sin t}{r^4\cos^6t+\sin^2t}$$ and now I argue: if $\;\sin t=0\;$ then $\;x=0\;$ and clearly $\;f(0,y)=0\;$ , otherwise $$\lim_{r\to 0}\frac{r^2\cos^3t\sin t}{r^4\cos^6t+\sin^2t}=\frac 0{0+\sin t}=0$$ and thus the limit is zero...where am I going wrong?! Thanks.","I have the following function: $$f(x,y):=\begin{cases}\frac{x^3y}{x^6+y^2}&,\;\;(x,y)\neq (0,0)\\{}\\0&,\;\;(x,y)=(0,0)\end{cases}$$ I'm asked about continuity at the origin and the limit of function there. Now, the limit doesn't exist since $$\begin{align*}y=x^3&\implies f(x,x^3)=\frac{x^6}{x^6+x^6}=\frac12\xrightarrow [x\to 0]{}\frac12\;,\;\;\text{whereas}\\y=x&\implies f(x,x)=\frac{x^4}{x^6+x^2}=\frac{x^2}{x^4+1}\xrightarrow[x\to 0]{}0\end{align*}$$ My problem is: if I try to apply what's been shown in several questions in this site, namely polar coordinates, I get $$\begin{cases}x=r\cos t\\y=r\sin t\end{cases}\implies f(r,t)=\frac{r^2\cos^3t\sin t}{r^4\cos^6t+\sin^2t}$$ and now I argue: if $\;\sin t=0\;$ then $\;x=0\;$ and clearly $\;f(0,y)=0\;$ , otherwise $$\lim_{r\to 0}\frac{r^2\cos^3t\sin t}{r^4\cos^6t+\sin^2t}=\frac 0{0+\sin t}=0$$ and thus the limit is zero...where am I going wrong?! Thanks.",,['multivariable-calculus']
78,"Find the critical points for $F(x,y,z)=-x^{3}-y^{2}+2xy+x+2z$",Find the critical points for,"F(x,y,z)=-x^{3}-y^{2}+2xy+x+2z",I started by taking the first order partial derivatives: $F_{x}=-3x^{2}+2y + 1$ $F_{y}=-2y+2x $ $F_{z}=2 $ Now I would try to solve it for $F_{x}=F_{y}=F_{z}=0$ but $F_{z}=2$. How can I proceed or this means that $F$ doesn't have critical points?,I started by taking the first order partial derivatives: $F_{x}=-3x^{2}+2y + 1$ $F_{y}=-2y+2x $ $F_{z}=2 $ Now I would try to solve it for $F_{x}=F_{y}=F_{z}=0$ but $F_{z}=2$. How can I proceed or this means that $F$ doesn't have critical points?,,"['multivariable-calculus', 'partial-derivative']"
79,"Only one critical point, local minimum but not global","Only one critical point, local minimum but not global",,"Given a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ which has only one critical point and it's a local minimum, for what $n$ is it a global minimum? For a convex function with one variable a local minimum is always global. For functions with two variables, it's not true. There are many counterexamples: $f(x,y) = e^{3x} + y^3-3ye^x$. Here the only solution of  $f_x=3e^{3x}-3ye^x=0$, and $f_y=3y^2-3e^x=0$ is $(0,1)$ which is a local minimum by the second derivative test. But $f(0,-3)=-17<f(0,1)=-1$ $f(x,y)=x^2+y^2(1+x)^3$ has the same property. What about higher dimensions? Could you help me determine the condition on $n$ for which the only local minimum is global? Thank you.","Given a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ which has only one critical point and it's a local minimum, for what $n$ is it a global minimum? For a convex function with one variable a local minimum is always global. For functions with two variables, it's not true. There are many counterexamples: $f(x,y) = e^{3x} + y^3-3ye^x$. Here the only solution of  $f_x=3e^{3x}-3ye^x=0$, and $f_y=3y^2-3e^x=0$ is $(0,1)$ which is a local minimum by the second derivative test. But $f(0,-3)=-17<f(0,1)=-1$ $f(x,y)=x^2+y^2(1+x)^3$ has the same property. What about higher dimensions? Could you help me determine the condition on $n$ for which the only local minimum is global? Thank you.",,"['real-analysis', 'multivariable-calculus']"
80,Does the orientation you evaluate line integrals matter?,Does the orientation you evaluate line integrals matter?,,"If instead of evaluating the above line integral in counter-clockwise direction, I evaluate it via the clockwise direction, would that change the answer? What if I evaluate $C_1$ and $C_3$ in the counter-clockwise direction, but I evaluate $C_2$ in the clockwise direction? If the direction does matter, in which direction would I evaluate the below line integral?","If instead of evaluating the above line integral in counter-clockwise direction, I evaluate it via the clockwise direction, would that change the answer? What if I evaluate $C_1$ and $C_3$ in the counter-clockwise direction, but I evaluate $C_2$ in the clockwise direction? If the direction does matter, in which direction would I evaluate the below line integral?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
81,Wave equation: show eventually $\int_{\mathbb{R}}u_x^2 = \int_{\mathbb{R}}u_t^2$,Wave equation: show eventually,\int_{\mathbb{R}}u_x^2 = \int_{\mathbb{R}}u_t^2,"Suppose $u$ solves the wave equation in $\mathbb{R}$ and has compactly supported initial data $f(x) = u(x,0)$ and $g(x)=u_t(x,0)$. Show that the ""kinetic energy"" $\int_{\mathbb{R}}u_t^2$ eventually equals the ""potential energy"" $\int_{\mathbb{R}}u_x^2$. My attempt so far: When I expand $\int_{\mathbb{R}}u_t^2 - u_x^2$ using d'Alembert's formula,  I get $$\int_{\mathbb{R}}f'(x+t)f'(x-t)-g(x+t)g(x-t)\\ + f'(x+t)g(x+t) - f'(x-t)g(x-t)dx.$$ The first two terms will eventually be zero, because at least one of the two factors of each will be zero (since the initial data has compact support). I need to find a way to make the second two terms zero. I'm trying to do it by integration by parts, noting that the last two terms can be written as $$\left.f'g\right]_{x-t}^{x+t}$$ or $$\int_{x-t}^{x+t} f''g+g'f',$$ but this isn't getting me anywhere.","Suppose $u$ solves the wave equation in $\mathbb{R}$ and has compactly supported initial data $f(x) = u(x,0)$ and $g(x)=u_t(x,0)$. Show that the ""kinetic energy"" $\int_{\mathbb{R}}u_t^2$ eventually equals the ""potential energy"" $\int_{\mathbb{R}}u_x^2$. My attempt so far: When I expand $\int_{\mathbb{R}}u_t^2 - u_x^2$ using d'Alembert's formula,  I get $$\int_{\mathbb{R}}f'(x+t)f'(x-t)-g(x+t)g(x-t)\\ + f'(x+t)g(x+t) - f'(x-t)g(x-t)dx.$$ The first two terms will eventually be zero, because at least one of the two factors of each will be zero (since the initial data has compact support). I need to find a way to make the second two terms zero. I'm trying to do it by integration by parts, noting that the last two terms can be written as $$\left.f'g\right]_{x-t}^{x+t}$$ or $$\int_{x-t}^{x+t} f''g+g'f',$$ but this isn't getting me anywhere.",,"['multivariable-calculus', 'partial-differential-equations', 'wave-equation']"
82,Fourier transform on $\mathbb R^n$ of Gaussian function,Fourier transform on  of Gaussian function,\mathbb R^n,"Let $\displaystyle{K(x)= e^{- \pi |x|^2} \quad ,x \in \mathbb R^n}$ be   the Gaussian kernel on $\mathbb R^n$. Prove that its Fourier transform   is  $$ \hat{K} (\xi) = e^{- \pi |\xi|^2} $$ I can prove this on $\mathbb R$ using the fact $\displaystyle{ \int_{- \infty}^{\infty} e^{ - \pi x^2} =1}$, but I do not know how to prove it on $\mathbb R^n$ I did a search, but all the things I found was for the $n=1$ case. Any help?","Let $\displaystyle{K(x)= e^{- \pi |x|^2} \quad ,x \in \mathbb R^n}$ be   the Gaussian kernel on $\mathbb R^n$. Prove that its Fourier transform   is  $$ \hat{K} (\xi) = e^{- \pi |\xi|^2} $$ I can prove this on $\mathbb R$ using the fact $\displaystyle{ \int_{- \infty}^{\infty} e^{ - \pi x^2} =1}$, but I do not know how to prove it on $\mathbb R^n$ I did a search, but all the things I found was for the $n=1$ case. Any help?",,"['integration', 'multivariable-calculus', 'fourier-analysis']"
83,Laplace's equation is solved when the functional $E[u] = \int_{\Omega}|\nabla u|^2 $ is minimized,Laplace's equation is solved when the functional  is minimized,E[u] = \int_{\Omega}|\nabla u|^2 ,"My professor mentioned something like ""Laplace's equation is solved when the functional $E[u] = \int_{\Omega}|\nabla u|^2 $ is minimized."" I've been trying to understand this statement. If I say that $E[u+tv]$ has a minimum at $t=0$, I get the condition $$\int_{\partial \Omega}v \frac{\partial u}{\partial \nu} = \int_\Omega v \Delta u.$$ But what does this show? Can someone help me to flesh this out? Must $\Omega$ be bounded?","My professor mentioned something like ""Laplace's equation is solved when the functional $E[u] = \int_{\Omega}|\nabla u|^2 $ is minimized."" I've been trying to understand this statement. If I say that $E[u+tv]$ has a minimum at $t=0$, I get the condition $$\int_{\partial \Omega}v \frac{\partial u}{\partial \nu} = \int_\Omega v \Delta u.$$ But what does this show? Can someone help me to flesh this out? Must $\Omega$ be bounded?",,"['multivariable-calculus', 'partial-differential-equations', 'harmonic-functions']"
84,Why is the directional derivative $D_v(F)$ equal to $\nabla F \cdot v$?,Why is the directional derivative  equal to ?,D_v(F) \nabla F \cdot v,"Why is the directional derivative $D_{\bf v}(F)$ equal to $\nabla F \cdot {\bf v}$?  It doesn't seem obvious from the definition of the directional derivative, $$\lim_{h \to 0} \frac{f({\bf x} + h{\bf v}) - f({\bf x})}{h}$$","Why is the directional derivative $D_{\bf v}(F)$ equal to $\nabla F \cdot {\bf v}$?  It doesn't seem obvious from the definition of the directional derivative, $$\lim_{h \to 0} \frac{f({\bf x} + h{\bf v}) - f({\bf x})}{h}$$",,"['calculus', 'multivariable-calculus']"
85,Integral vector calculus (by parts),Integral vector calculus (by parts),,"This is taken from the bottom of page 37 of Griffith's Introduction to Electrodynamics Using integration by parts, $\nabla \cdot (f {\bf A})$ = $ f (\nabla \cdot {\bf A}) + {\bf A} \cdot (\nabla f)$, and therefore, $\int \nabla \cdot (f {\bf A})$ = $ \int f (\nabla \cdot {\bf A}) + \int {\bf A} \cdot (\nabla f)$ The divergence theorem states that $\int_V (\nabla \cdot {\bf A}) d \tau $ = $ \oint_S {\bf A} \cdot dA$. In the textbook, by invoking the divergence theorem, the obtained result was that $\int \nabla \cdot (f {\bf A}) d \tau $ = $\int f(\nabla \cdot {\bf A}) d \tau $ + $\int {\bf A}\cdot (\nabla f) d \tau $ = $\oint f {\bf A} \cdot d{\bf a}$. My confusion begins when substituting the divergence theorem into the equation in the 2nd line. I think that $\int \nabla \cdot (f {\bf A})$ = $ \oint_S {\bf A} \cdot dA$ + $\int {\bf A} \cdot (\nabla f)$, so I am not sure what happens to the $\int {\bf A} \cdot (\nabla f)$  in the textbook's result. Also, any clarification on when exactly to use the $v, s, c$ and all the other integral subscripts are to be used would be appreciated. It's easy to know when the questions asks for a specific volume integral or surface integral etc, but is it possible to tell just from the equations given? With the 2 questions that follow after the example in the book, I am having trouble understanding their solutions. The question is given with its solution here where I am to use product rules and integral theorems to prove equality of the integrals. However, like my problem in the example, I cannot seem to be able to follow when trying to substitute. Thanks.","This is taken from the bottom of page 37 of Griffith's Introduction to Electrodynamics Using integration by parts, $\nabla \cdot (f {\bf A})$ = $ f (\nabla \cdot {\bf A}) + {\bf A} \cdot (\nabla f)$, and therefore, $\int \nabla \cdot (f {\bf A})$ = $ \int f (\nabla \cdot {\bf A}) + \int {\bf A} \cdot (\nabla f)$ The divergence theorem states that $\int_V (\nabla \cdot {\bf A}) d \tau $ = $ \oint_S {\bf A} \cdot dA$. In the textbook, by invoking the divergence theorem, the obtained result was that $\int \nabla \cdot (f {\bf A}) d \tau $ = $\int f(\nabla \cdot {\bf A}) d \tau $ + $\int {\bf A}\cdot (\nabla f) d \tau $ = $\oint f {\bf A} \cdot d{\bf a}$. My confusion begins when substituting the divergence theorem into the equation in the 2nd line. I think that $\int \nabla \cdot (f {\bf A})$ = $ \oint_S {\bf A} \cdot dA$ + $\int {\bf A} \cdot (\nabla f)$, so I am not sure what happens to the $\int {\bf A} \cdot (\nabla f)$  in the textbook's result. Also, any clarification on when exactly to use the $v, s, c$ and all the other integral subscripts are to be used would be appreciated. It's easy to know when the questions asks for a specific volume integral or surface integral etc, but is it possible to tell just from the equations given? With the 2 questions that follow after the example in the book, I am having trouble understanding their solutions. The question is given with its solution here where I am to use product rules and integral theorems to prove equality of the integrals. However, like my problem in the example, I cannot seem to be able to follow when trying to substitute. Thanks.",,"['multivariable-calculus', 'vector-analysis']"
86,surface area of a sphere above a cylinder,surface area of a sphere above a cylinder,,"I need to find the surface area of the sphere $x^2+y^2+z^2=4$ above the cone $z = \sqrt{x^2+y^2}$, but I'm not sure how. I know that the surface area of a surface can be calculated with the equation $A=\int{\int_D{\sqrt{f_x^2+f_y^2+1}}}dA$, but I'm not sure how to take into account the constraint that it must lie above the cone. How is this done?","I need to find the surface area of the sphere $x^2+y^2+z^2=4$ above the cone $z = \sqrt{x^2+y^2}$, but I'm not sure how. I know that the surface area of a surface can be calculated with the equation $A=\int{\int_D{\sqrt{f_x^2+f_y^2+1}}}dA$, but I'm not sure how to take into account the constraint that it must lie above the cone. How is this done?",,"['multivariable-calculus', 'integration', 'analytic-geometry']"
87,Problem about differential of a linear map,Problem about differential of a linear map,,Please can you tell how to solve this problem clearly? Please solve this explanatorily. Thank you,Please can you tell how to solve this problem clearly? Please solve this explanatorily. Thank you,,"['linear-algebra', 'abstract-algebra', 'multivariable-calculus', 'manifolds']"
88,"Let $f(x,y)=\frac{x^3-y^3}{x^2+y^2}$. Is f differentiable in $(0,0)$?",Let . Is f differentiable in ?,"f(x,y)=\frac{x^3-y^3}{x^2+y^2} (0,0)","Let  $$f(x,y)=\frac{x^3-y^3}{x^2+y^2}$$ My solution manual says that this function is not diffb. in $(0,0)$ because it is not linear. Well my problem is that I don't see why this function is linear, and I also don't see why that would imply that $f$ is not differentiable.","Let  $$f(x,y)=\frac{x^3-y^3}{x^2+y^2}$$ My solution manual says that this function is not diffb. in $(0,0)$ because it is not linear. Well my problem is that I don't see why this function is linear, and I also don't see why that would imply that $f$ is not differentiable.",,['multivariable-calculus']
89,Pure significance of line integrals of vector fields,Pure significance of line integrals of vector fields,,"I can understand how the line integral of a scalar function has pure-mathematical significance, representing an area beneath a curve. But is there a pure significance to the line integral of a vector field? The only understanding I have of this is the physical example of work done on a moving particle. Thanks for any insight.","I can understand how the line integral of a scalar function has pure-mathematical significance, representing an area beneath a curve. But is there a pure significance to the line integral of a vector field? The only understanding I have of this is the physical example of work done on a moving particle. Thanks for any insight.",,['multivariable-calculus']
90,$\iiint e^{-x^2-2y^2-3z^2}dV$,,\iiint e^{-x^2-2y^2-3z^2}dV,"I was given this question in class but I don't understand how to do it. Evaluate the triple integral in $\mathbb{R}^3$ of $\iiint e^{-x^2-2y^2-3z^2}dV$. The hint was to use the idea that $\int e^{-x^2}dx = \sqrt \pi$, which I understand, but I don't get how to apply it here... If anyone can help I would appreciate it.","I was given this question in class but I don't understand how to do it. Evaluate the triple integral in $\mathbb{R}^3$ of $\iiint e^{-x^2-2y^2-3z^2}dV$. The hint was to use the idea that $\int e^{-x^2}dx = \sqrt \pi$, which I understand, but I don't get how to apply it here... If anyone can help I would appreciate it.",,"['multivariable-calculus', 'integration', 'exponential-function']"
91,How to find the greatest value of the function?,How to find the greatest value of the function?,,"$$ f(x, y)=x^2+y^2,\quad x,\;y \ge 0,\quad 3x+2y \le 6$$ The max value is in the point $(0, 3)$, but how do I prove it? I may be able to prove that the function decreases on the curve $x(t)=2t,\; y(t)=3-3t$, but I don't know how.","$$ f(x, y)=x^2+y^2,\quad x,\;y \ge 0,\quad 3x+2y \le 6$$ The max value is in the point $(0, 3)$, but how do I prove it? I may be able to prove that the function decreases on the curve $x(t)=2t,\; y(t)=3-3t$, but I don't know how.",,['multivariable-calculus']
92,Spherical Coordinates Triple Integral,Spherical Coordinates Triple Integral,,"Write a triple integral in spherical coordinates that expresses the volume of the solid formed when a sphere with radius $a$ tangent to the $xy$ plane at the origin intersects at the plane z = a. (Equation of the sphere is $x^2 + y^2 + (z-a)^2 = a^2$) I recognize that this is just half the volume of the sphere, but I need to just write the integral expressing this volume. I'm having some trouble because it is translate from the origin... Not sure how that affects the values for phi and rho. Some clarity would be appreciated.","Write a triple integral in spherical coordinates that expresses the volume of the solid formed when a sphere with radius $a$ tangent to the $xy$ plane at the origin intersects at the plane z = a. (Equation of the sphere is $x^2 + y^2 + (z-a)^2 = a^2$) I recognize that this is just half the volume of the sphere, but I need to just write the integral expressing this volume. I'm having some trouble because it is translate from the origin... Not sure how that affects the values for phi and rho. Some clarity would be appreciated.",,['multivariable-calculus']
93,How can I prove Stokes theorem using Green's formula?,How can I prove Stokes theorem using Green's formula?,,"$$ \int_{\partial \Omega} (u ~dx + v ~dy) = \iint_{\Omega} \left( \frac{\partial v}{\partial x} - \frac{\partial u}{\partial y} \right) ~dx ~dy $$ Then I want to prove that$$ \int_{\partial \Omega} w = \iint_{\Omega} ~dw, \;(w = u ~dx + v ~dy) $$ Would you give me an elementary proof for this?","$$ \int_{\partial \Omega} (u ~dx + v ~dy) = \iint_{\Omega} \left( \frac{\partial v}{\partial x} - \frac{\partial u}{\partial y} \right) ~dx ~dy $$ Then I want to prove that$$ \int_{\partial \Omega} w = \iint_{\Omega} ~dw, \;(w = u ~dx + v ~dy) $$ Would you give me an elementary proof for this?",,['multivariable-calculus']
94,"Spivak's ""Calculus on Manifolds"" :: A good relearning of MV Calculus?","Spivak's ""Calculus on Manifolds"" :: A good relearning of MV Calculus?",,"A friend of mine gifted me his copy of Spivak's Calculus on Manifolds. I was looking out for a good book to relearn MV Calculus to the extent of : Multivariable Limits, Continuity and Differentiation Differential Calculus of Vector and Scalar Fields Multiple/Surface Integrals My intention was to go through a nice rigorous text to prepare me for my research in Numerical Optimization.","A friend of mine gifted me his copy of Spivak's Calculus on Manifolds. I was looking out for a good book to relearn MV Calculus to the extent of : Multivariable Limits, Continuity and Differentiation Differential Calculus of Vector and Scalar Fields Multiple/Surface Integrals My intention was to go through a nice rigorous text to prepare me for my research in Numerical Optimization.",,"['reference-request', 'multivariable-calculus']"
95,Notation in Munkres' $\textit{Analysis on Manifolds}$,Notation in Munkres',\textit{Analysis on Manifolds},"I am trying to understand Theorem 9.1 of 1991 copy of Munkres' Analysis on Manifolds. I have stated what I don't understand below; there is a heading in bold. This theorem is a precursor to the implicit function theorem and on my copy of the book is on page 73. Now on page 72 he states the following definition: Let $A$ be open in $\Bbb{R}^m$; let $f : A \rightarrow \Bbb{R}^n$ be differentiable. Let $f_1,\ldots,f_n$ be the component functions of $f$. We sometimes use the notation      $$Df = \frac{\partial(f_1,\ldots,f_n)}{\partial(x_1,\ldots,x_m)}$$     for the derivative of $f$. On occasion we shorten this to the notation $Df = \partial f /\partial \Bbb{x}$. This is all good, so now on to theorem 9.1 (which is where my confusion lies). Theorem 9.1: Let $A$ be open in $\Bbb{R}^{k+n}$; let $f : A \rightarrow \Bbb{R}^n $ be differentiable. Write $f$ in the form $f(\Bbb{x},\Bbb{y})$, for $\Bbb{x} \in \Bbb{R}^k$ and $\Bbb{y} \in \Bbb{R}^n$; then $Df$ has the form     $$Df = \Big[ \partial f/\partial \Bbb{x} \hspace{5mm} \partial f / \partial \Bbb{y}\Big].$$     Suppose there is a differentiable function $g : B \rightarrow \Bbb{R}^n$ defined on an open set $B$ in $\Bbb{R}^k$, such that      $$f(\Bbb{x},g(\Bbb{x})) = 0$$     for all $\Bbb{x} \in B$. Then for $\Bbb{x} \in B$,      $$ \frac{\partial f}{\partial \Bbb{x}}(\Bbb{x},g(\Bbb{x})) + \frac{\partial f}{\partial \Bbb{y}}(\Bbb{x},g(\Bbb{x}))\cdot Dg(\Bbb{x}) = 0.$$ The dot just before $Dg(\Bbb{x})$ means matrix multiplication. Now the proof of this goes as follows, given $g$, we can define $h : B \rightarrow \Bbb{R}^{k+n}$ by the equation $$h(\Bbb{x}) = (\Bbb{x},g(\Bbb{x})).$$ The hypotheses of the theorem then imply that the composite function $f(h(\Bbb{X})) = f(\Bbb{x},g(\Bbb{x}))$ is defined and equals zero for all $\Bbb{x} \in B$. The chain rule then implies that $$\begin{eqnarray*} 0 &=& Df(h(\Bbb{x}))\cdot Dh(\Bbb{x})\\ &=& \Big[\frac{\partial f}{\partial \Bbb{x}}(h(\Bbb{x})) \hspace{4mm}  \frac{\partial f}{\partial \Bbb{y}}(h(\Bbb{x}))    \Big] \cdot \left[\begin{array}{c} I_k \\ Dg(\Bbb{x}) \end{array}\right] \end{eqnarray*}.$$ What I don't understand: In the last row above, I get the second matrix on the right hand side, the one involving the identity matrix. However for the first matrix, I can see the notation means that it is formed by concatenating two matrices together, one from $\frac{\partial f}{\partial \Bbb{x}}(h(\Bbb{x}))$ and the other from $\frac{\partial f}{\partial \Bbb{y}}(h(\Bbb{x}))$. My problem now is I don't even no what these matrices look like. I have tried several ways to interpret them, but keep getting tied up. Also, for the second matrix on the right it is of dimensions $$(n + k) \times k$$ yes? But if this were so, then how can $Df(h(\Bbb{x}))$ be a map from $\Bbb{R}^{k+n}$ to $\Bbb{R}^n$? Thanks.","I am trying to understand Theorem 9.1 of 1991 copy of Munkres' Analysis on Manifolds. I have stated what I don't understand below; there is a heading in bold. This theorem is a precursor to the implicit function theorem and on my copy of the book is on page 73. Now on page 72 he states the following definition: Let $A$ be open in $\Bbb{R}^m$; let $f : A \rightarrow \Bbb{R}^n$ be differentiable. Let $f_1,\ldots,f_n$ be the component functions of $f$. We sometimes use the notation      $$Df = \frac{\partial(f_1,\ldots,f_n)}{\partial(x_1,\ldots,x_m)}$$     for the derivative of $f$. On occasion we shorten this to the notation $Df = \partial f /\partial \Bbb{x}$. This is all good, so now on to theorem 9.1 (which is where my confusion lies). Theorem 9.1: Let $A$ be open in $\Bbb{R}^{k+n}$; let $f : A \rightarrow \Bbb{R}^n $ be differentiable. Write $f$ in the form $f(\Bbb{x},\Bbb{y})$, for $\Bbb{x} \in \Bbb{R}^k$ and $\Bbb{y} \in \Bbb{R}^n$; then $Df$ has the form     $$Df = \Big[ \partial f/\partial \Bbb{x} \hspace{5mm} \partial f / \partial \Bbb{y}\Big].$$     Suppose there is a differentiable function $g : B \rightarrow \Bbb{R}^n$ defined on an open set $B$ in $\Bbb{R}^k$, such that      $$f(\Bbb{x},g(\Bbb{x})) = 0$$     for all $\Bbb{x} \in B$. Then for $\Bbb{x} \in B$,      $$ \frac{\partial f}{\partial \Bbb{x}}(\Bbb{x},g(\Bbb{x})) + \frac{\partial f}{\partial \Bbb{y}}(\Bbb{x},g(\Bbb{x}))\cdot Dg(\Bbb{x}) = 0.$$ The dot just before $Dg(\Bbb{x})$ means matrix multiplication. Now the proof of this goes as follows, given $g$, we can define $h : B \rightarrow \Bbb{R}^{k+n}$ by the equation $$h(\Bbb{x}) = (\Bbb{x},g(\Bbb{x})).$$ The hypotheses of the theorem then imply that the composite function $f(h(\Bbb{X})) = f(\Bbb{x},g(\Bbb{x}))$ is defined and equals zero for all $\Bbb{x} \in B$. The chain rule then implies that $$\begin{eqnarray*} 0 &=& Df(h(\Bbb{x}))\cdot Dh(\Bbb{x})\\ &=& \Big[\frac{\partial f}{\partial \Bbb{x}}(h(\Bbb{x})) \hspace{4mm}  \frac{\partial f}{\partial \Bbb{y}}(h(\Bbb{x}))    \Big] \cdot \left[\begin{array}{c} I_k \\ Dg(\Bbb{x}) \end{array}\right] \end{eqnarray*}.$$ What I don't understand: In the last row above, I get the second matrix on the right hand side, the one involving the identity matrix. However for the first matrix, I can see the notation means that it is formed by concatenating two matrices together, one from $\frac{\partial f}{\partial \Bbb{x}}(h(\Bbb{x}))$ and the other from $\frac{\partial f}{\partial \Bbb{y}}(h(\Bbb{x}))$. My problem now is I don't even no what these matrices look like. I have tried several ways to interpret them, but keep getting tied up. Also, for the second matrix on the right it is of dimensions $$(n + k) \times k$$ yes? But if this were so, then how can $Df(h(\Bbb{x}))$ be a map from $\Bbb{R}^{k+n}$ to $\Bbb{R}^n$? Thanks.",,['multivariable-calculus']
96,Steepest Descent Algorithm for Solving Linear Systems,Steepest Descent Algorithm for Solving Linear Systems,,"I am trying to implement the steepest descent algorithm for linear systems. The equation is below: $\begin{align*} Ax &= b\\ x_0 &= [0]*[m,n]\\ x_k &= x_{k-1} + \frac{|d_{k-1}|^2}{d_{k-1} * Ad_{k-1}}d_{k-1}\\ d_{k-1} &= -(Ax_{k-1} - b) \end{align*}$ My problem is that I think $d_{k-1}$ should be an $m\times1$ matrix and when you try and dot the denominator of $x_k$ it does not work because you end up trying to dot an $m\times1$ by an $m\times1$ matrix. What am I doing wrong?","I am trying to implement the steepest descent algorithm for linear systems. The equation is below: $\begin{align*} Ax &= b\\ x_0 &= [0]*[m,n]\\ x_k &= x_{k-1} + \frac{|d_{k-1}|^2}{d_{k-1} * Ad_{k-1}}d_{k-1}\\ d_{k-1} &= -(Ax_{k-1} - b) \end{align*}$ My problem is that I think $d_{k-1}$ should be an $m\times1$ matrix and when you try and dot the denominator of $x_k$ it does not work because you end up trying to dot an $m\times1$ by an $m\times1$ matrix. What am I doing wrong?",,"['linear-algebra', 'multivariable-calculus', 'numerical-methods']"
97,Technical question regarding using the coarea formula to calculate the relation between the $n$-ball's volume and the $(n-1)$-sphere volume,Technical question regarding using the coarea formula to calculate the relation between the -ball's volume and the -sphere volume,n (n-1),"We were shown in class this next calculation: (Here, $V_n(RB^n)$ is the volume of an $n$ dimensional ball of radius $R$, likewise $S_{n-1}$ is the surface area of the $n$ dimensional sphere in $\mathbb{R}^n$. $rS^{n-1}$ denotes the $n$ dimensional sphere of radius $r$ and integrating $d\textbf{S}$ means a surface integral.) $$V_n(RB^n)=\int_{RB^n}1dx=\int_0^R\int_{rS^{n-1}}1d\textbf{S}dr=\int_0^R\int_{S^{n-1}}r^{n-1}d\textbf{S}dr=$$$$=\int_0^Rr^{n-1}\int_{S^{n-1}}1d\textbf{S}dr=\int_0^Rr^{n-1}S_{n-1}dr=\frac{R^n}{n}S_{n-1}$$ and finally $V_n=\frac{1}{n}S_{n-1}$ since $V_n(RB^n)=R^nV_n$. My problem is with the 3rd equality. The first is obvious and the second is the coarea formula. I assume the third equality is a result of a change of variables, but since this is taking place in $\mathbb{R}^n$ I'd expect the change of variables to be $x\mapsto rx$ which gives the Jacobian of $r^n$ - not the $r^{n-1}$ we see after the third equality. It'd be easier for me to assume the teacher had a mistake here, had she not used this result later on in her lectures... So my question is, was she wrong in the change of variables there or am I missing something about surface integrals?","We were shown in class this next calculation: (Here, $V_n(RB^n)$ is the volume of an $n$ dimensional ball of radius $R$, likewise $S_{n-1}$ is the surface area of the $n$ dimensional sphere in $\mathbb{R}^n$. $rS^{n-1}$ denotes the $n$ dimensional sphere of radius $r$ and integrating $d\textbf{S}$ means a surface integral.) $$V_n(RB^n)=\int_{RB^n}1dx=\int_0^R\int_{rS^{n-1}}1d\textbf{S}dr=\int_0^R\int_{S^{n-1}}r^{n-1}d\textbf{S}dr=$$$$=\int_0^Rr^{n-1}\int_{S^{n-1}}1d\textbf{S}dr=\int_0^Rr^{n-1}S_{n-1}dr=\frac{R^n}{n}S_{n-1}$$ and finally $V_n=\frac{1}{n}S_{n-1}$ since $V_n(RB^n)=R^nV_n$. My problem is with the 3rd equality. The first is obvious and the second is the coarea formula. I assume the third equality is a result of a change of variables, but since this is taking place in $\mathbb{R}^n$ I'd expect the change of variables to be $x\mapsto rx$ which gives the Jacobian of $r^n$ - not the $r^{n-1}$ we see after the third equality. It'd be easier for me to assume the teacher had a mistake here, had she not used this result later on in her lectures... So my question is, was she wrong in the change of variables there or am I missing something about surface integrals?",,"['integration', 'multivariable-calculus']"
98,Notations about multiple integrals,Notations about multiple integrals,,"I have the following sum: $$ S = \int f_1(x_1) dx_1 + \int f_2(x_2) dx_2 + \int f_3(x_3) dx_3 $$ Letting $x = (x_1,x_2, x_3)$ and $f = (f_1(x_1), f_2(x_2), f_3(x_3))$ can I rewrite $$ S = \int f(x)\cdot dx $$ where $\cdot$ is the dot product ?","I have the following sum: $$ S = \int f_1(x_1) dx_1 + \int f_2(x_2) dx_2 + \int f_3(x_3) dx_3 $$ Letting $x = (x_1,x_2, x_3)$ and $f = (f_1(x_1), f_2(x_2), f_3(x_3))$ can I rewrite $$ S = \int f(x)\cdot dx $$ where $\cdot$ is the dot product ?",,"['notation', 'multivariable-calculus']"
99,Trouble with partial derivatives,Trouble with partial derivatives,,"I've no clue how to get started .I'am unable to even understand what the hint is saying.I need your help please. Given $$u = f(ax^2 + 2hxy + by^2), \qquad v = \phi (ax^2 + 2hxy + by^2),$$ then prove that $$\frac{\partial }{\partial y} \left ( u\frac{\partial u }{\partial x} \right ) = \frac{\partial }{\partial x}\left ( u \frac{\partial v}{\partial y} \right ).$$ Hint. Given $$u = f(z),v = \phi(z), \text{where} z = ax^2 + 2hxy + by^2$$","I've no clue how to get started .I'am unable to even understand what the hint is saying.I need your help please. Given $$u = f(ax^2 + 2hxy + by^2), \qquad v = \phi (ax^2 + 2hxy + by^2),$$ then prove that $$\frac{\partial }{\partial y} \left ( u\frac{\partial u }{\partial x} \right ) = \frac{\partial }{\partial x}\left ( u \frac{\partial v}{\partial y} \right ).$$ Hint. Given $$u = f(z),v = \phi(z), \text{where} z = ax^2 + 2hxy + by^2$$",,['multivariable-calculus']
