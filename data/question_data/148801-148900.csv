,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Normal planes and spherical curves,Normal planes and spherical curves,,"I am interested in the following result: ""If all the normal planes of a curve pass through a particular point, then the curve is contained in a sphere"". My approach: Let $\alpha: I \to \mathbb{R}^3$ be the curve (assume it is arc length parametrized). Let $P$ be the common point of all the normal planes. It is a well-known fact that, if $\tau(s) \ne 0 \quad \forall s$ and $k'(s) \ne 0 \quad \forall s$ , then $\alpha$ lies in a sphere if and only if $\dfrac{\tau(s)}{k(s)} = \left(\dfrac{k'(s)}{\tau(s)k^2(s)}\right)'$ . Consider a point $\alpha(s)$ . Since $P$ belongs to the normal plane of $\alpha$ at $\alpha(s)$ , then $$ \alpha(s)-P = x(s)\overrightarrow{n}(s) + y(s)\overrightarrow{b}(s) $$ Deriving and applying Frenet formulas, we get $$ 0 = -\bigl(1+x(s)k(s)\bigl)\overrightarrow{t} + \bigl(x'(s)-y(s)\tau(s)\bigl)\overrightarrow{n} + \bigl(x(s)\tau(s)+y'(s)\bigl)\overrightarrow{b} $$ so $$ \left\{ \begin{array}{l} 1+x(s)k(s) = 0 \\ x'(s) -y(s)\tau(s) = 0\\ x(s)\tau(s) + y'(s) = 0 \end{array} \right. $$ Playing with these equations and assuming $\tau(s) \ne 0$ , I get $\dfrac{\tau(s)}{k(s)} = \left(\dfrac{k'(s)}{\tau(s)k^2(s)}\right)'$ as desired. However, I don't see how I could prove that $\tau(s) \ne 0$ and $k'(s) \ne 0$ . I am aware that, as Shifrin notes suggests, using the fact that $\lVert f(t) \rVert = \text{constant} \iff f \cdot f' = 0$ helps deriving the result without using the characterization of spherical curves in terms of their curvature, torsion and their derivatives. However, I'd like to complete (if possible) the proof I have been working on.","I am interested in the following result: ""If all the normal planes of a curve pass through a particular point, then the curve is contained in a sphere"". My approach: Let be the curve (assume it is arc length parametrized). Let be the common point of all the normal planes. It is a well-known fact that, if and , then lies in a sphere if and only if . Consider a point . Since belongs to the normal plane of at , then Deriving and applying Frenet formulas, we get so Playing with these equations and assuming , I get as desired. However, I don't see how I could prove that and . I am aware that, as Shifrin notes suggests, using the fact that helps deriving the result without using the characterization of spherical curves in terms of their curvature, torsion and their derivatives. However, I'd like to complete (if possible) the proof I have been working on.","\alpha: I \to \mathbb{R}^3 P \tau(s) \ne 0 \quad \forall s k'(s) \ne 0 \quad \forall s \alpha \dfrac{\tau(s)}{k(s)} = \left(\dfrac{k'(s)}{\tau(s)k^2(s)}\right)' \alpha(s) P \alpha \alpha(s) 
\alpha(s)-P = x(s)\overrightarrow{n}(s) + y(s)\overrightarrow{b}(s)
 
0 = -\bigl(1+x(s)k(s)\bigl)\overrightarrow{t} + \bigl(x'(s)-y(s)\tau(s)\bigl)\overrightarrow{n} + \bigl(x(s)\tau(s)+y'(s)\bigl)\overrightarrow{b}
  \left\{
\begin{array}{l}
1+x(s)k(s) = 0 \\
x'(s) -y(s)\tau(s) = 0\\
x(s)\tau(s) + y'(s) = 0
\end{array} \right.
 \tau(s) \ne 0 \dfrac{\tau(s)}{k(s)} = \left(\dfrac{k'(s)}{\tau(s)k^2(s)}\right)' \tau(s) \ne 0 k'(s) \ne 0 \lVert f(t) \rVert = \text{constant} \iff f \cdot f' = 0","['analysis', 'differential-geometry', 'curves', 'frenet-frame']"
1,Show that the positive solution of $a = \tanh(ax)$ satisfies $\frac{1}{1-a^2} > x$,Show that the positive solution of  satisfies,a = \tanh(ax) \frac{1}{1-a^2} > x,"Let $x>1$ and $a = \tanh(ax)$ be positive (out of the three roots). How can I conclude that $$ \frac{1}{1-a^2} > x? $$ Context of how this popped up: I was solving the Ising model in the mean field theory for zero magnetic field case. But all this can be ignored if one is not familiar. The following is the relevant math. I had to minimize $F(M)$ , and it turned out that \begin{align} F'(M) &= -(NqJ)M + \frac{N}{2\beta}\ln\left( \frac{1+M}{1-M} \right)\quad\text{and,}\\ F''(M) &= \frac{N}{\beta}\left( -\beta qJ + \frac{1}{1-M^2} \right), \end{align} where $\beta$ , $N$ , $q$ , $J$ are constants of the theory. $M$ is the variable. It is easily seen that $F'(M) = 0$ corresponds to the transcendental equation $M = \tanh(\beta qJM)$ , which splits into two cases: $\beta qJ \le 1$ . Then we have the only solution to be $M = 0$ , which also happens to be a minimum. $\beta qJ> 1$ . Then we have three solutions to the transcendental equation, say $-M_s, 0, M_s$ (with $M_s > 0$ ). Then $M = 0$ is not a minimum. To show that $M_s$ (and hence $-M_s$ ) is a local minimum, I needed the inequality to hold.","Let and be positive (out of the three roots). How can I conclude that Context of how this popped up: I was solving the Ising model in the mean field theory for zero magnetic field case. But all this can be ignored if one is not familiar. The following is the relevant math. I had to minimize , and it turned out that where , , , are constants of the theory. is the variable. It is easily seen that corresponds to the transcendental equation , which splits into two cases: . Then we have the only solution to be , which also happens to be a minimum. . Then we have three solutions to the transcendental equation, say (with ). Then is not a minimum. To show that (and hence ) is a local minimum, I needed the inequality to hold.","x>1 a = \tanh(ax) 
\frac{1}{1-a^2} > x?
 F(M) \begin{align}
F'(M) &= -(NqJ)M + \frac{N}{2\beta}\ln\left( \frac{1+M}{1-M} \right)\quad\text{and,}\\
F''(M) &= \frac{N}{\beta}\left( -\beta qJ + \frac{1}{1-M^2} \right),
\end{align} \beta N q J M F'(M) = 0 M = \tanh(\beta qJM) \beta qJ \le 1 M = 0 \beta qJ> 1 -M_s, 0, M_s M_s > 0 M = 0 M_s -M_s","['real-analysis', 'analysis', 'inequality']"
2,Prove that a set E is not complete by exhibiting a sequence of points in E that is Cauchy but does not converge to an element of E,Prove that a set E is not complete by exhibiting a sequence of points in E that is Cauchy but does not converge to an element of E,,"Prove that the set $E=\{(x_1, . . . , x_d) :x_1, . . . , x_d>0\}$ in $\mathbb{R}^d$ is not complete by exhibiting a sequence of points $(x_n)_{n \in \mathbb{N}}$ in $E$ that is Cauchy but does not converge to an element of $E$ . I know that $E$ is complete if for every sequence $(x_n)_{n \in \mathbb{N}}$ of points in $E$ that is Cauchy, there exists an $x \in E$ such that $x_n \rightarrow x$ . So incompleteness would be that there does not an exist such an $x$ . I am not sure how to prove this.","Prove that the set in is not complete by exhibiting a sequence of points in that is Cauchy but does not converge to an element of . I know that is complete if for every sequence of points in that is Cauchy, there exists an such that . So incompleteness would be that there does not an exist such an . I am not sure how to prove this.","E=\{(x_1, . . . , x_d) :x_1, . . . , x_d>0\} \mathbb{R}^d (x_n)_{n \in \mathbb{N}} E E E (x_n)_{n \in \mathbb{N}} E x \in E x_n \rightarrow x x","['analysis', 'proof-writing', 'cauchy-sequences', 'complete-spaces']"
3,Differentiable function and the limit as x goes to infinity,Differentiable function and the limit as x goes to infinity,,"Let $$f: \mathbb{R} \to \mathbb{R}, \lim_{x \to \infty} f'(x) = 0.$$ Prove: $$\lim_{x \to \infty} (f(x + 2) - f(x)) = 0$$ I know I need use the MVT somewhere in here. I know $f$ is differentiable on $(x, x+2)$ so $$f(x+2) - f(x) = 2f'(c)$$ for some $c \in (x,x+2).$ So if we keep looking at the interval as $x \to \infty$ then it seems that's all we need, but I'm not sure how to make it more rigorous. Edit: Am I on the right track here: $$f(x+2) - f(x) = 2f'(c)$$ implies $$\lim_{x \to \infty} f(x+2) - f(x) = \lim_{c \to \infty} 2f'(c) = 0$$","Let Prove: I know I need use the MVT somewhere in here. I know is differentiable on so for some So if we keep looking at the interval as then it seems that's all we need, but I'm not sure how to make it more rigorous. Edit: Am I on the right track here: implies","f: \mathbb{R} \to \mathbb{R}, \lim_{x \to \infty} f'(x) = 0. \lim_{x \to \infty} (f(x + 2) - f(x)) = 0 f (x, x+2) f(x+2) - f(x) = 2f'(c) c \in (x,x+2). x \to \infty f(x+2) - f(x) = 2f'(c) \lim_{x \to \infty} f(x+2) - f(x) = \lim_{c \to \infty} 2f'(c) = 0","['analysis', 'derivatives']"
4,Evaluate $\lim_{r \to \infty} \iint_{D_r} \frac{1}{1+x^2+y^2} dx dy$,Evaluate,\lim_{r \to \infty} \iint_{D_r} \frac{1}{1+x^2+y^2} dx dy,"Evaluate $$\lim_{r \to \infty} \iint_{D_r} \frac{1}{1+x^2+y^2} dx dy$$ where $D_r=\{(x,y) \in \mathbb{R}^2 \ \text{s.t.} \ 0 \leq y \leq 1, \ x \geq 0, \ x^2+y^2 \leq r^2 \}$ . My try is the following: I use a change of variable in polar coordinates with pole in the origin, hence $$D_r \mapsto E_r=\{(\rho, \theta) \in \mathbb{R}^2 \ \text{s.t.} \ 0 \leq \rho \sin \theta \leq 1, \rho \cos \theta \geq 0, \rho^2 \leq r^2, \rho \geq 0, 0 \leq \theta <2\pi\}$$ $$\iff \left\{(\rho, \theta) \in \mathbb{R}^2 \ \text{s.t.} \ 0 \leq \rho \leq \frac{1}{\sin \theta}, 0 \leq \theta \leq \frac{\pi}{2}, 0 \leq \rho \leq r\right\}$$ And $$\lim_{r \to \infty} \iint_{D_r} \frac{1}{1+x^2+y^2} dx dy=\lim_{r \to \infty}\iint_{E_r} \frac{\rho}{1+\rho^2} d \rho d \theta$$ Since $\rho$ has two upper bounds I discuss $0 \leq \rho \leq \min\left(r, \frac{1}{\sin \theta}\right)$ . Now, being interested in what happens when $r \to \infty$ , I assume that $r>\frac{1}{\sin \theta}$ and so I get that $\min\left(r, \frac{1}{\sin \theta}\right)=\frac{1}{\sin \theta}$ ; hence $$\lim_{r \to \infty}\iint_{E_r} \frac{\rho}{1+\rho^2} d \rho d \theta=\lim_{r \to \infty} \int_0^{\frac{\pi}{2}} \left(\int_0^{\min\left(r,\frac{1}{\sin\theta}\right)} \frac{\rho}{1+\rho^2}d\rho\right)d\theta$$ $$=\lim_{r \to \infty} \int_0^{\frac{\pi}{2}} \left(\int_0^{\frac{1}{\sin\theta}} \frac{\rho}{1+\rho^2}d\rho\right)d\theta$$ At this point both the set is independent of $r$ and so $$\lim_{r \to \infty} \int_0^{\frac{\pi}{2}} \left(\int_0^{\frac{1}{\sin\theta}} \frac{\rho}{1+\rho^2}d\rho\right)d\theta=\int_0^{\frac{\pi}{2}} \left(\int_0^{\frac{1}{\sin\theta}} \frac{\rho}{1+\rho^2}d\rho\right)d\theta=\int_0^{\frac{\pi}{2}} \frac{1}{2}\log\left(1+\frac{1}{\sin^2 \theta}\right)d\theta$$ $$=\frac{\pi}{2} \log(1+\sqrt{2})$$ The result is correct, however I'm not sure when I declare that since $r \to \infty$ I can assume $r>\frac{1}{\sin \theta}$ ; the doubt comes from the fact that $\theta \in \left[0,\frac{\pi}{2}\right]$ and so when $\theta$ gets really close to $0$ the function $\frac{1}{\sin \theta}$ is unbounded. So I feel like there is some kind of ""battle"" between $r$ and $\frac{1}{\sin \theta}$ going to infinity that makes impossible to understand which one is actually bigger for discussing the minimum. Am I missing something and there is no problem for $\frac{1}{\sin \theta}$ when $\theta \to 0^+$ (and if there isn't, why it isn't? Why can I ignore the fact that it tends to infinity for small values of $\theta$ in the domain of integration?) or I was just lucky and my approach was wrong because I can't make that assumption? Thanks. Edit: Added a term $\frac{1}{2}$ that I forgot.","Evaluate where . My try is the following: I use a change of variable in polar coordinates with pole in the origin, hence And Since has two upper bounds I discuss . Now, being interested in what happens when , I assume that and so I get that ; hence At this point both the set is independent of and so The result is correct, however I'm not sure when I declare that since I can assume ; the doubt comes from the fact that and so when gets really close to the function is unbounded. So I feel like there is some kind of ""battle"" between and going to infinity that makes impossible to understand which one is actually bigger for discussing the minimum. Am I missing something and there is no problem for when (and if there isn't, why it isn't? Why can I ignore the fact that it tends to infinity for small values of in the domain of integration?) or I was just lucky and my approach was wrong because I can't make that assumption? Thanks. Edit: Added a term that I forgot.","\lim_{r \to \infty} \iint_{D_r} \frac{1}{1+x^2+y^2} dx dy D_r=\{(x,y) \in \mathbb{R}^2 \ \text{s.t.} \ 0 \leq y \leq 1, \ x \geq 0, \ x^2+y^2 \leq r^2 \} D_r \mapsto E_r=\{(\rho, \theta) \in \mathbb{R}^2 \ \text{s.t.} \ 0 \leq \rho \sin \theta \leq 1, \rho \cos \theta \geq 0, \rho^2 \leq r^2, \rho \geq 0, 0 \leq \theta <2\pi\} \iff \left\{(\rho, \theta) \in \mathbb{R}^2 \ \text{s.t.} \ 0 \leq \rho \leq \frac{1}{\sin \theta}, 0 \leq \theta \leq \frac{\pi}{2}, 0 \leq \rho \leq r\right\} \lim_{r \to \infty} \iint_{D_r} \frac{1}{1+x^2+y^2} dx dy=\lim_{r \to \infty}\iint_{E_r} \frac{\rho}{1+\rho^2} d \rho d \theta \rho 0 \leq \rho \leq \min\left(r, \frac{1}{\sin \theta}\right) r \to \infty r>\frac{1}{\sin \theta} \min\left(r, \frac{1}{\sin \theta}\right)=\frac{1}{\sin \theta} \lim_{r \to \infty}\iint_{E_r} \frac{\rho}{1+\rho^2} d \rho d \theta=\lim_{r \to \infty} \int_0^{\frac{\pi}{2}} \left(\int_0^{\min\left(r,\frac{1}{\sin\theta}\right)} \frac{\rho}{1+\rho^2}d\rho\right)d\theta =\lim_{r \to \infty} \int_0^{\frac{\pi}{2}} \left(\int_0^{\frac{1}{\sin\theta}} \frac{\rho}{1+\rho^2}d\rho\right)d\theta r \lim_{r \to \infty} \int_0^{\frac{\pi}{2}} \left(\int_0^{\frac{1}{\sin\theta}} \frac{\rho}{1+\rho^2}d\rho\right)d\theta=\int_0^{\frac{\pi}{2}} \left(\int_0^{\frac{1}{\sin\theta}} \frac{\rho}{1+\rho^2}d\rho\right)d\theta=\int_0^{\frac{\pi}{2}} \frac{1}{2}\log\left(1+\frac{1}{\sin^2 \theta}\right)d\theta =\frac{\pi}{2} \log(1+\sqrt{2}) r \to \infty r>\frac{1}{\sin \theta} \theta \in \left[0,\frac{\pi}{2}\right] \theta 0 \frac{1}{\sin \theta} r \frac{1}{\sin \theta} \frac{1}{\sin \theta} \theta \to 0^+ \theta \frac{1}{2}","['integration', 'analysis', 'multivariable-calculus', 'definite-integrals']"
5,Contiguous hypergeometric function question,Contiguous hypergeometric function question,,"I have come upon the hypergeometric function $$_2F_1\left(k+\frac{1}{2},k+\frac{1}{2};\frac{3}{2},z\right)$$ where $k \geq 1$ is an integer, and I believe that this is equal to $$\frac{p(z)}{(1-z)^{(4k-1)/2}}$$ where $p$ is a polynomial of degree $k-1$ (Wolframalpha confirms the first few values). I understand that this must follow from some relationship involving contiguous hypergeometric functions, but I don't know how, and don't have a good reference (library at my uni is closed for COVID-19). I actually don't care about the coefficients in the polynomial, because I'm just trying to show an integral is finite. Is anyone able to put me on the right track? Many thanks, Greg","I have come upon the hypergeometric function where is an integer, and I believe that this is equal to where is a polynomial of degree (Wolframalpha confirms the first few values). I understand that this must follow from some relationship involving contiguous hypergeometric functions, but I don't know how, and don't have a good reference (library at my uni is closed for COVID-19). I actually don't care about the coefficients in the polynomial, because I'm just trying to show an integral is finite. Is anyone able to put me on the right track? Many thanks, Greg","_2F_1\left(k+\frac{1}{2},k+\frac{1}{2};\frac{3}{2},z\right) k \geq 1 \frac{p(z)}{(1-z)^{(4k-1)/2}} p k-1","['analysis', 'special-functions', 'hypergeometric-function']"
6,"Given positive real numbers $a$, $b$, $c$, $d$, $e$ with $\sum_{\text{cyc}}\,\frac{1}{4+a}=1$, prove that $\sum_{\text{cyc}}\,\frac{a}{4+a^2}\le1$.","Given positive real numbers , , , ,  with , prove that .","a b c d e \sum_{\text{cyc}}\,\frac{1}{4+a}=1 \sum_{\text{cyc}}\,\frac{a}{4+a^2}\le1","Let $a, b, c, d, e$ be positive real numbers such that $$\dfrac{1}{4+a} + \dfrac{1}{4+b} +\dfrac{1}{4+c} +\dfrac{1}{4+d} +\dfrac{1}{4+e}  = 1.$$ Prove that $$\dfrac{a}{4+a^{2}} + \dfrac{b}{4+b^{2}} +\dfrac{c}{4+c^{2}} +\dfrac{d}{4+d^{2}} +\dfrac{e}{4+e^{2}}  \leq 1.$$ My question is how to prove this inequality by using AM-GM inequality? My solution (using the Chebyshev inequality). Since $\dfrac{1}{4+a}+\dfrac{1}{4+b}+\dfrac{1}{4+c}+\dfrac{1}{4+d}+\dfrac{1}{4+e} =1,$ we have $$1 = \dfrac{1}{4+a}+\dfrac{1}{4+b}+\dfrac{1}{4+c}+\dfrac{1}{4+d}+\dfrac{1}{4+e} \geq \dfrac{a}{4+a^2}+\dfrac{b}{4+b^2}+\dfrac{c}{4+c^2}+\dfrac{d}{4+d^2}+\dfrac{e}{4+e^2}$$ $$\Leftrightarrow \dfrac{1-a}{(4+a)(4+a^2)}+\dfrac{1-b}{(4+b)(4+b^2)}+\dfrac{1-c}{(4+c)(4+c^2)}+\dfrac{1-d}{(4+d)(4+d^2)}+\dfrac{1-e}{(4+e)(4+e^2)} \geq 0.$$ Suppose that $a \geq b \geq c \geq d \geq e$ . Then, we get $$\dfrac{1-a}{4+a} \leq \dfrac{1-b}{4+b} \leq \dfrac{1-c}{4+c} \leq \dfrac{1-d}{4+d} \leq \dfrac{1-e}{4+e}.$$ and $$\dfrac{1}{4+a^2} \leq \dfrac{1}{4+b^2} \leq \dfrac{1}{4+c^2} \leq \dfrac{1}{4+d^2} \leq \dfrac{1}{4+e^2}.$$ Applying the Chebyshev inequality, one gets $$ \sum_{cyc}\dfrac{1-a}{(4+a)(4+a^2)} \geq \dfrac{1}{5} \sum_{cyc}\dfrac{1-a}{4+a}. \sum_{cyc}\dfrac{1}{4+a^2} = \dfrac{1}{5}\sum_{cyc}\dfrac{1}{4+a^2} \sum_{cyc} \left( \dfrac{5}{4+a}-1 \right)=0.$$","Let be positive real numbers such that Prove that My question is how to prove this inequality by using AM-GM inequality? My solution (using the Chebyshev inequality). Since we have Suppose that . Then, we get and Applying the Chebyshev inequality, one gets","a, b, c, d, e \dfrac{1}{4+a} + \dfrac{1}{4+b} +\dfrac{1}{4+c} +\dfrac{1}{4+d} +\dfrac{1}{4+e}  = 1. \dfrac{a}{4+a^{2}} + \dfrac{b}{4+b^{2}} +\dfrac{c}{4+c^{2}} +\dfrac{d}{4+d^{2}} +\dfrac{e}{4+e^{2}}  \leq 1. \dfrac{1}{4+a}+\dfrac{1}{4+b}+\dfrac{1}{4+c}+\dfrac{1}{4+d}+\dfrac{1}{4+e} =1, 1 = \dfrac{1}{4+a}+\dfrac{1}{4+b}+\dfrac{1}{4+c}+\dfrac{1}{4+d}+\dfrac{1}{4+e} \geq \dfrac{a}{4+a^2}+\dfrac{b}{4+b^2}+\dfrac{c}{4+c^2}+\dfrac{d}{4+d^2}+\dfrac{e}{4+e^2} \Leftrightarrow \dfrac{1-a}{(4+a)(4+a^2)}+\dfrac{1-b}{(4+b)(4+b^2)}+\dfrac{1-c}{(4+c)(4+c^2)}+\dfrac{1-d}{(4+d)(4+d^2)}+\dfrac{1-e}{(4+e)(4+e^2)} \geq 0. a \geq b \geq c \geq d \geq e \dfrac{1-a}{4+a} \leq \dfrac{1-b}{4+b} \leq \dfrac{1-c}{4+c} \leq \dfrac{1-d}{4+d} \leq \dfrac{1-e}{4+e}. \dfrac{1}{4+a^2} \leq \dfrac{1}{4+b^2} \leq \dfrac{1}{4+c^2} \leq \dfrac{1}{4+d^2} \leq \dfrac{1}{4+e^2}.  \sum_{cyc}\dfrac{1-a}{(4+a)(4+a^2)} \geq \dfrac{1}{5} \sum_{cyc}\dfrac{1-a}{4+a}. \sum_{cyc}\dfrac{1}{4+a^2} = \dfrac{1}{5}\sum_{cyc}\dfrac{1}{4+a^2} \sum_{cyc} \left( \dfrac{5}{4+a}-1 \right)=0.","['real-analysis', 'analysis', 'inequality', 'a.m.-g.m.-inequality', 'tangent-line-method']"
7,How to Motivate Open-Cover Formulation of Compactness in a Metric Space?,How to Motivate Open-Cover Formulation of Compactness in a Metric Space?,,"The open cover formulation of compactness always seemed to come out of nowhere for me. I've consulted many Analysis textbooks, but all of them have been like - 'Here's the open cover formulation, now we prove this and the sequential formulation are equivalent.' None of them actually go on to explain where this open cover formulation comes from. So, my question is this - suppose I was a researcher trying to come up with an open set formulation of compactness for the first time. All I know is Real Analysis, and I have defined a compact set as one in which a sequence has a convergent subsequence. How would I go about doing so?","The open cover formulation of compactness always seemed to come out of nowhere for me. I've consulted many Analysis textbooks, but all of them have been like - 'Here's the open cover formulation, now we prove this and the sequential formulation are equivalent.' None of them actually go on to explain where this open cover formulation comes from. So, my question is this - suppose I was a researcher trying to come up with an open set formulation of compactness for the first time. All I know is Real Analysis, and I have defined a compact set as one in which a sequence has a convergent subsequence. How would I go about doing so?",,"['real-analysis', 'analysis', 'metric-spaces', 'compactness']"
8,Bound for the Fourier transform,Bound for the Fourier transform,,"Can someone come up with a proof for this bound of the Fourier transformation? Let $f$ be $s$ times continuously differentiable, with compact support, then there exists a real constant $c$ such that: $$\vert F(k)\vert\leq \frac{c}{(1+\vert k\vert)^s},$$ where $F$ is the Fourier transform of $f$ . Since there is a compact support, the maximum of the function exists and the Fourier transformation is bounded, but where does the factor $(1 +\vert k\vert)^{-s}$ come from? Thanks","Can someone come up with a proof for this bound of the Fourier transformation? Let be times continuously differentiable, with compact support, then there exists a real constant such that: where is the Fourier transform of . Since there is a compact support, the maximum of the function exists and the Fourier transformation is bounded, but where does the factor come from? Thanks","f s c \vert F(k)\vert\leq \frac{c}{(1+\vert k\vert)^s}, F f (1 +\vert k\vert)^{-s}","['analysis', 'mathematical-physics', 'fourier-transform']"
9,Stokes Theorem: manifolds vs. chains,Stokes Theorem: manifolds vs. chains,,"So, reading both Baby Rudin (Principles of Mathematical Analysis) and Munkres (Analysis on Manifolds), I start to wonder the difference between this two approaches. In Rudin's, he defines the Stokes Theorem in chains: Let $\Psi$ a $k$ -chain of class $C''$ in an open set $V \subset \mathbb{R}^n$ and $\omega$ a $(k - 1)$ -form of class $C'$ in $V$ . Then: $$\int_{\Psi} d\omega = \int_{\partial \Psi} \omega.$$ While in Munkres, the definition is on a oriented manifold: Let $k>1$ and let $M$ be a compact oriented $k$ -manifold in $\mathbb{R}^n$ ; give $\partial M$ the induced orientation if $\partial M \neq \emptyset$ . Let $\omega$ be a $(k - 1)$ -form defined in an open set of $\mathbb{R}^n$ containing $M$ . Then, if $\partial M = \emptyset$ , $$\int_{\partial M} \omega = 0.$$ If $\partial M\neq\emptyset$ , $$\int_M d\omega = \int_{\partial M} \omega.$$ I don't have the mathematical maturity yet to understand both statements completely (I have a litle knowledge of what a manifold and a differential form is, but I don't know very well what a chain happens to be), but, in case of future studies, what is the difference between these? Which one is more general? Searching about integration on chains, I came across some definitions of algebraic topology, such as homology and simplexes. What is the relation between the Stokes theorem and these concepts? Sorry about the long post and such ""disjoint"" questions, but this different approaches poked up my curiosity. Thanks in advance!","So, reading both Baby Rudin (Principles of Mathematical Analysis) and Munkres (Analysis on Manifolds), I start to wonder the difference between this two approaches. In Rudin's, he defines the Stokes Theorem in chains: Let a -chain of class in an open set and a -form of class in . Then: While in Munkres, the definition is on a oriented manifold: Let and let be a compact oriented -manifold in ; give the induced orientation if . Let be a -form defined in an open set of containing . Then, if , If , I don't have the mathematical maturity yet to understand both statements completely (I have a litle knowledge of what a manifold and a differential form is, but I don't know very well what a chain happens to be), but, in case of future studies, what is the difference between these? Which one is more general? Searching about integration on chains, I came across some definitions of algebraic topology, such as homology and simplexes. What is the relation between the Stokes theorem and these concepts? Sorry about the long post and such ""disjoint"" questions, but this different approaches poked up my curiosity. Thanks in advance!",\Psi k C'' V \subset \mathbb{R}^n \omega (k - 1) C' V \int_{\Psi} d\omega = \int_{\partial \Psi} \omega. k>1 M k \mathbb{R}^n \partial M \partial M \neq \emptyset \omega (k - 1) \mathbb{R}^n M \partial M = \emptyset \int_{\partial M} \omega = 0. \partial M\neq\emptyset \int_M d\omega = \int_{\partial M} \omega.,"['analysis', 'differential-geometry', 'manifolds', 'differential-forms', 'stokes-theorem']"
10,Understanding from where a recurrence of polynomials comes from,Understanding from where a recurrence of polynomials comes from,,"In chapter $7$ of Principles of Mathematical Analysis ( $3$ rd edition) by Walter Rudin, the exercise $23$ says: Put $P_0=0$ , and define, for $n=0,1,2,\dots,$ $$P_{n+1}(x)=P_{n}(x)+\frac{x^2-P_{n}^2(x)}{2}.$$ Prove that $$\lim_{n\to \infty}P_{n}(x)=|x|,$$ uniformly on $[-1,1]$ . I am interested here in the origin of the polynomials (i.e. how Rudin came up with them) rather than a solution to the exercise.  My intuition is that the polynomials come as some sort of approximation of $\sqrt{x^2}$ , but I am unsure how to use an approximation of the square root function to derive the recurrence $P_0(x) = 0$ and $$P_{n+1}(x) = P_n(x) + \frac {x^2 - P_n^2(x)} 2$$ as an approximation of $\sqrt{x^2}$ .","In chapter of Principles of Mathematical Analysis ( rd edition) by Walter Rudin, the exercise says: Put , and define, for Prove that uniformly on . I am interested here in the origin of the polynomials (i.e. how Rudin came up with them) rather than a solution to the exercise.  My intuition is that the polynomials come as some sort of approximation of , but I am unsure how to use an approximation of the square root function to derive the recurrence and as an approximation of .","7 3 23 P_0=0 n=0,1,2,\dots, P_{n+1}(x)=P_{n}(x)+\frac{x^2-P_{n}^2(x)}{2}. \lim_{n\to \infty}P_{n}(x)=|x|, [-1,1] \sqrt{x^2} P_0(x) = 0 P_{n+1}(x) = P_n(x) + \frac {x^2 - P_n^2(x)} 2 \sqrt{x^2}","['real-analysis', 'analysis', 'polynomials', 'radicals', 'interpolation']"
11,"Counterexamples to: If $f:X \to Y$ is continuous, $Y$ is compact, then $f^{-1}$ is continuous.","Counterexamples to: If  is continuous,  is compact, then  is continuous.",f:X \to Y Y f^{-1},"It is a theorem that if $f:X \to Y$ is a continuous bijection, $X$ is compact, then $g = f^{-1}$ is continuous. My professor asked us to find a counterexample to If $f:X \to Y$ is continuous, $Y$ is compact, then $g = f^{-1}$ is continuous. I do not like my counterexample so much because it uses the discrete metric. Are there other counterexamples? My Counterexample: Let $X = [0, 1]$ with the Discrete metric, $Y = [0, 1]$ with the Euclidean metric, and let $f$ be the identity function.","It is a theorem that if is a continuous bijection, is compact, then is continuous. My professor asked us to find a counterexample to If is continuous, is compact, then is continuous. I do not like my counterexample so much because it uses the discrete metric. Are there other counterexamples? My Counterexample: Let with the Discrete metric, with the Euclidean metric, and let be the identity function.","f:X \to Y X g = f^{-1} f:X \to Y Y g = f^{-1} X = [0, 1] Y = [0, 1] f","['real-analysis', 'analysis', 'metric-spaces', 'compactness']"
12,Analysis Textbook like Zorich,Analysis Textbook like Zorich,,"I'm an undergraduate student currently studying mathematical analysis. Our professor uses Zorich's Mathematical Analysis, but I found the text too difficult to understand. After exploring some textbooks, I found that Abbott was easier to follow, so I studied Abbott until I realized that there's a significant amount of content in Zorich that Abbott doesn't cover. So I was wondering if there's a book out there that covers as much content as Zorich but is more readable? Thank you for any help.","I'm an undergraduate student currently studying mathematical analysis. Our professor uses Zorich's Mathematical Analysis, but I found the text too difficult to understand. After exploring some textbooks, I found that Abbott was easier to follow, so I studied Abbott until I realized that there's a significant amount of content in Zorich that Abbott doesn't cover. So I was wondering if there's a book out there that covers as much content as Zorich but is more readable? Thank you for any help.",,"['analysis', 'book-recommendation']"
13,Difficulty in learning maths,Difficulty in learning maths,,"I am an undergraduate student in Mathematics. I am writing here because I think I have a problem in the way I study Math. My grades are pretty good, but I think that I am not able to effectively remember what I studied. I will try to explain myself better: I passed Calculus and Analysis (I am from Italy, we usually learn those subjects together) with a very good grade, but I struggle to remember some basic stuff to series series or some techniques to solve differential equations. I am able to remember effectively things in short or medium periods of time, but I do not have a way to remember things in longer periods of time (years). I realized this today, during a lesson of Analytic Number Theory for undergraduates: we needed to use the classic series for $\log 2$ , the harmonic alternating one, and I was not able to recall it. I have realized that this is a general problem for me in Math, but it is  very clear in Calculus/Analysis. Do you have any suggestions on how to overcome this problem? How can I study in order to remember better thing in Math (both theorems, their proofs and how to do exercises)? Thanks","I am an undergraduate student in Mathematics. I am writing here because I think I have a problem in the way I study Math. My grades are pretty good, but I think that I am not able to effectively remember what I studied. I will try to explain myself better: I passed Calculus and Analysis (I am from Italy, we usually learn those subjects together) with a very good grade, but I struggle to remember some basic stuff to series series or some techniques to solve differential equations. I am able to remember effectively things in short or medium periods of time, but I do not have a way to remember things in longer periods of time (years). I realized this today, during a lesson of Analytic Number Theory for undergraduates: we needed to use the classic series for , the harmonic alternating one, and I was not able to recall it. I have realized that this is a general problem for me in Math, but it is  very clear in Calculus/Analysis. Do you have any suggestions on how to overcome this problem? How can I study in order to remember better thing in Math (both theorems, their proofs and how to do exercises)? Thanks",\log 2,"['analysis', 'education', 'learning']"
14,Monotonicity of function averages,Monotonicity of function averages,,"Please let me know if you know an answer to this problem. May be you could provide a reference to some publication on this topic? Let $f(x)$ be a real-valued  strictly convex function on $[0, 1]$ . For any integer $k$ between $0$ and some positive integer $n$ let $x_k =k/n$ . Consider the average value function $g(n)= \sum_{k=0}^{n}f(x_k)/(n+1) $ . Is it true that $g(n)$ is decreasing? Thank you!",Please let me know if you know an answer to this problem. May be you could provide a reference to some publication on this topic? Let be a real-valued  strictly convex function on . For any integer between and some positive integer let . Consider the average value function . Is it true that is decreasing? Thank you!,"f(x) [0, 1] k 0 n x_k =k/n g(n)= \sum_{k=0}^{n}f(x_k)/(n+1)  g(n)","['real-analysis', 'analysis', 'functions', 'inequality', 'average']"
15,"Define $P_{n+1}(x) = P_{n}(x)+\frac{x^{2} - P_{n}(x)^{2}}{2}$. Show $P_{n}$ converges uniformly on $[-1, 1]$",Define . Show  converges uniformly on,"P_{n+1}(x) = P_{n}(x)+\frac{x^{2} - P_{n}(x)^{2}}{2} P_{n} [-1, 1]","Let $P_{0} = 0$ and for $n = 0,1,2,...,$ define $P_{n+1}(x) = P_{n}(x)+\frac{x^{2} - P_{n}(x)^{2}}{2}$ . Show that $P_{n}$ converges uniformly on $[-1, 1]$ and find its limit. For proving that two claim we have to prove Claim 1: $0 \leq P_{n}(x) \leq P_{n + 1}(x) \leq \vert x\vert \leq 1$ for $x\in [-1, 1]$ , proved easily. Claim2: $0 \leq \vert x \vert - P_{n}(x) < \frac{2}{n + 1}$ . This is difficult for me. I get a help form web note where they first prove $0 \leq \vert x \vert - P_{n}(x) \leq \vert x \vert \left(1 - \frac{\vert x \vert}{2}\right)^{n}$ and I understand the rest of the proof. My question is where they get intention to prove $0 \leq \vert x \vert - P_{n}(x) \leq \vert x \vert \left(1 - \frac{\vert x \vert}{2}\right)^{n}$ ?","Let and for define . Show that converges uniformly on and find its limit. For proving that two claim we have to prove Claim 1: for , proved easily. Claim2: . This is difficult for me. I get a help form web note where they first prove and I understand the rest of the proof. My question is where they get intention to prove ?","P_{0} = 0 n = 0,1,2,..., P_{n+1}(x) = P_{n}(x)+\frac{x^{2} - P_{n}(x)^{2}}{2} P_{n} [-1, 1] 0 \leq P_{n}(x) \leq P_{n + 1}(x) \leq \vert x\vert \leq 1 x\in [-1, 1] 0 \leq \vert x \vert - P_{n}(x) < \frac{2}{n + 1} 0 \leq \vert x \vert - P_{n}(x) \leq \vert x \vert \left(1 - \frac{\vert x \vert}{2}\right)^{n} 0 \leq \vert x \vert - P_{n}(x) \leq \vert x \vert \left(1 - \frac{\vert x \vert}{2}\right)^{n}","['real-analysis', 'analysis']"
16,Proving an estimate for a function that is null on the boundary of the unit ball,Proving an estimate for a function that is null on the boundary of the unit ball,,Let $B_1(0)$ be the unit ball in $\mathbb{R}^3$ and let $f \in C^1(\overline{B_1(0)})$ such that $f$ is identically zero on $\partial B_1(0)$ . Prove that $$|f(0)| \leq\frac{1}{4\pi}\int_{B_1(0)}|\nabla f(x)||x|^{-2}dx$$ I have absolutely no idea of how to do this! Any hint would be highly appreciated.,Let be the unit ball in and let such that is identically zero on . Prove that I have absolutely no idea of how to do this! Any hint would be highly appreciated.,B_1(0) \mathbb{R}^3 f \in C^1(\overline{B_1(0)}) f \partial B_1(0) |f(0)| \leq\frac{1}{4\pi}\int_{B_1(0)}|\nabla f(x)||x|^{-2}dx,"['real-analysis', 'calculus', 'analysis']"
17,"If f(x_n) converges whenever (x_n), then f is continuous.","If f(x_n) converges whenever (x_n), then f is continuous.",,"This is the original question: Let $f: X\to Y$ be a map between metric spaces.  Prove that if $(f(x_n))_{n=0}^\infty$ converges in $Y$ whenever $(x_n)_{n=0}^\infty$ converges in $X$ then $f$ is continuous.   [Note: it is not given that $f(x_n) → f(x)$ whenever $x_n → x$ .] I have spent quite a bit of time on this problem. I finally looked it up and have been reading the solution in the link below: for every convergent sequence $x_n$, $f(x_n)$ also converges. Does this imply continuity of f? I am confused by the solution given. I understand that they proved that $f(x_n)$ does not converge to $f(x)$ , but this doesn't prove that $f(x_n)$ does not converge at all. If we want to use the contrapositive, shouldn't be have to show that if $f$ is discontinuous, then there is some sequence $(x_n)$ that converges  in $X$ but $f(x_n)$ does not converge in $Y$ . That is, we have to show that for some convergent sequence, there is some $\varepsilon$ such that for all $n$ greater than some $N$ , $f(x_n)>\varepsilon$ . Any clarification or hints would be greatly appreciated.","This is the original question: Let be a map between metric spaces.  Prove that if converges in whenever converges in then is continuous.   [Note: it is not given that whenever .] I have spent quite a bit of time on this problem. I finally looked it up and have been reading the solution in the link below: for every convergent sequence $x_n$, $f(x_n)$ also converges. Does this imply continuity of f? I am confused by the solution given. I understand that they proved that does not converge to , but this doesn't prove that does not converge at all. If we want to use the contrapositive, shouldn't be have to show that if is discontinuous, then there is some sequence that converges  in but does not converge in . That is, we have to show that for some convergent sequence, there is some such that for all greater than some , . Any clarification or hints would be greatly appreciated.",f: X\to Y (f(x_n))_{n=0}^\infty Y (x_n)_{n=0}^\infty X f f(x_n) → f(x) x_n → x f(x_n) f(x) f(x_n) f (x_n) X f(x_n) Y \varepsilon n N f(x_n)>\varepsilon,"['analysis', 'convergence-divergence', 'continuity']"
18,How to evaluate this integral $\int\frac{\arcsin{\sqrt{x}}-\arccos{\sqrt{x}}}{\arcsin{\sqrt{x}}+\arccos{\sqrt{x}}}\cdot dx$?,How to evaluate this integral ?,\int\frac{\arcsin{\sqrt{x}}-\arccos{\sqrt{x}}}{\arcsin{\sqrt{x}}+\arccos{\sqrt{x}}}\cdot dx,The integral is $$\int\frac{\arcsin{\sqrt{x}}-\arccos{\sqrt{x}}}{\arcsin{\sqrt{x}}+\arccos{\sqrt{x}}}\cdot dx$$ What I did was to sum up the denominator to equal $\frac\pi2$ and then applied integration by parts on the remaining $\arccos{\sqrt{x}}$ and $\arcsin{\sqrt{x}}$ integrals but this process was very lengthy. Can someone suggest a shorter way?,The integral is What I did was to sum up the denominator to equal and then applied integration by parts on the remaining and integrals but this process was very lengthy. Can someone suggest a shorter way?,\int\frac{\arcsin{\sqrt{x}}-\arccos{\sqrt{x}}}{\arcsin{\sqrt{x}}+\arccos{\sqrt{x}}}\cdot dx \frac\pi2 \arccos{\sqrt{x}} \arcsin{\sqrt{x}},"['integration', 'analysis', 'indefinite-integrals', 'inverse-function']"
19,How to prove that is a Banach space,How to prove that is a Banach space,,"Let $E=\{f\in C^1([0,\infty[, R), \lim_{t\to\infty}\frac{f(t)}{1+t}=\lim_{t\to\infty}f'(t)=0\}$ with the norm $$||f||=\max\left(\sup\limits_{t\geq 0}\dfrac{|f(t)|}{1+t}, \sup\limits_{t\geq 0}|f'(t)|\right). $$ Prove that $E$ is a Banach space. I started by let $(u_n)$ a Cauchy sequence that is $$\forall \varepsilon>0, \exists n_0\in \mathbb{N}, \forall p,q\in \mathbb{N}; p>q\geq n_0\Rightarrow ||u_p-u_q||<\varepsilon $$ that is $$\forall \varepsilon>0, \exists n_0\in \mathbb{N}, \forall p,q\in \mathbb{N}; p>q\geq n_0\Rightarrow \dfrac{|u_p(t)-u_q(t)|}{1+t}<\varepsilon \, \text{and}\, |u'_p(t)-u'_q(t)|<\varepsilon $$ that is $u_n'(t)$ and $\frac{u_n(t)}{1+t}$ are  a Cauchy sequences in the complete $(\Bbb R,|\cdot|)$ so $u_n'(t)$ converge to $v(t)$ .  and $\frac{u_n(t)}{1+t}$ converge to $w(t)$ How to prove that $(1+t)w(t)$ is derivable?? Please",Let with the norm Prove that is a Banach space. I started by let a Cauchy sequence that is that is that is and are  a Cauchy sequences in the complete so converge to .  and converge to How to prove that is derivable?? Please,"E=\{f\in C^1([0,\infty[, R), \lim_{t\to\infty}\frac{f(t)}{1+t}=\lim_{t\to\infty}f'(t)=0\} ||f||=\max\left(\sup\limits_{t\geq 0}\dfrac{|f(t)|}{1+t}, \sup\limits_{t\geq 0}|f'(t)|\right).  E (u_n) \forall \varepsilon>0, \exists n_0\in \mathbb{N}, \forall p,q\in \mathbb{N}; p>q\geq n_0\Rightarrow ||u_p-u_q||<\varepsilon  \forall \varepsilon>0, \exists n_0\in \mathbb{N}, \forall p,q\in \mathbb{N}; p>q\geq n_0\Rightarrow \dfrac{|u_p(t)-u_q(t)|}{1+t}<\varepsilon \, \text{and}\, |u'_p(t)-u'_q(t)|<\varepsilon  u_n'(t) \frac{u_n(t)}{1+t} (\Bbb R,|\cdot|) u_n'(t) v(t) \frac{u_n(t)}{1+t} w(t) (1+t)w(t)",['analysis']
20,Purpose of the Einstein Tensor,Purpose of the Einstein Tensor,,"I was reading ""Tensor Analysis and Elementary Differential Geometry for Physicists and Engineers"" by Hung Nguyen-Schäfer and Jan-Philip Schmidt and in the chapter 2 named ""Tensor Analysis"" they introduce the Einstein Tensor: $$ G_j^i \equiv R_j^i - \frac 1 2 \delta_j^i R$$ With $R_j^i$ the second-kind Ricci tensor, $\delta_j^i$ the usual and friendly Kronecker tensor and $R$ the Ricci curvature. Then, few properties of this tensor were shown on this subsection dedicated to this tensor: Symmetry of the Einstein tensor Divergence of the Einstein tensor is always 0 The chapter ends with the second property with: This result is very important and has been often used in the general relativity theories and other relativity fields. This leads me into thinking that the property of 0-divergence was the goal of this construction. My questions are: Is the purpose of the Einstein tensor to have a linear combination of Ricci Tensor and it being a symmetric tensor so that it has the divergence equal to zero? Or is it for another purpose and the null divergence is just a handy property that has nothing to do with the construction of this tensor? To a greater extent, what were the objectives of the Einstein tensor? I am unsure if this question is more physics or mathematics, but why is it "" used in the general relativity theories and other relativity fields"" to have a tensor with a divergence of zero? For a vector field, it would mean that the ""quantity"" that goes inside the infinitesimal domain equals the ""quantity"" that exits the same domain. However, I am not sure what it means for a tensor or if it's useful for certain results in mathematics and physics. Please apologize me if this belongs more in physics SE.","I was reading ""Tensor Analysis and Elementary Differential Geometry for Physicists and Engineers"" by Hung Nguyen-Schäfer and Jan-Philip Schmidt and in the chapter 2 named ""Tensor Analysis"" they introduce the Einstein Tensor: With the second-kind Ricci tensor, the usual and friendly Kronecker tensor and the Ricci curvature. Then, few properties of this tensor were shown on this subsection dedicated to this tensor: Symmetry of the Einstein tensor Divergence of the Einstein tensor is always 0 The chapter ends with the second property with: This result is very important and has been often used in the general relativity theories and other relativity fields. This leads me into thinking that the property of 0-divergence was the goal of this construction. My questions are: Is the purpose of the Einstein tensor to have a linear combination of Ricci Tensor and it being a symmetric tensor so that it has the divergence equal to zero? Or is it for another purpose and the null divergence is just a handy property that has nothing to do with the construction of this tensor? To a greater extent, what were the objectives of the Einstein tensor? I am unsure if this question is more physics or mathematics, but why is it "" used in the general relativity theories and other relativity fields"" to have a tensor with a divergence of zero? For a vector field, it would mean that the ""quantity"" that goes inside the infinitesimal domain equals the ""quantity"" that exits the same domain. However, I am not sure what it means for a tensor or if it's useful for certain results in mathematics and physics. Please apologize me if this belongs more in physics SE.", G_j^i \equiv R_j^i - \frac 1 2 \delta_j^i R R_j^i \delta_j^i R,"['analysis', 'tensors', 'divergence-operator', 'general-relativity']"
21,"About $\log (1 + x)$ on p.427 ""Calculus 4th Edtion"" by Michael Spivak","About  on p.427 ""Calculus 4th Edtion"" by Michael Spivak",\log (1 + x),"I am reading ""Calculus 4th Edtion"" by Michael Spivak. On p.427 he wrote as follows: From the calculations on page 413, we see that for $x \geq 0$ we have $$\log(1+x)=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots+\frac{(-1)^{n-1}x^n}{n}+\frac{(-1)^n}{n+1}t^{n+1}$$ where $$\left \vert {\frac{(-1)^n}{n+1} t^{n+1}}\right\vert \leq \frac{x^{n+1}}{n+1}$$ and there is a slightly more complicated estimate when $-1 < x < 0$ (Problem 16). I guess $t \in (0, x)$ if $x > 0$ and $t = 0$ if $x = 0$ . Does the above equality really hold? I guess Spivak applied Taylor's theorem(p.424, Lagrange form of the remainder) incorrectly to $\log(1 + x)$ .","I am reading ""Calculus 4th Edtion"" by Michael Spivak. On p.427 he wrote as follows: From the calculations on page 413, we see that for we have where and there is a slightly more complicated estimate when (Problem 16). I guess if and if . Does the above equality really hold? I guess Spivak applied Taylor's theorem(p.424, Lagrange form of the remainder) incorrectly to .","x \geq 0 \log(1+x)=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots+\frac{(-1)^{n-1}x^n}{n}+\frac{(-1)^n}{n+1}t^{n+1} \left \vert {\frac{(-1)^n}{n+1} t^{n+1}}\right\vert \leq \frac{x^{n+1}}{n+1} -1 < x < 0 t \in (0, x) x > 0 t = 0 x = 0 \log(1 + x)","['calculus', 'analysis', 'logarithms', 'taylor-expansion', 'proof-explanation']"
22,Spherical mean property,Spherical mean property,,Let $ u(x)$ a continuos function over a domain $\Omega$ . Let $N\omega_n r^{N-1}$ the area of sphere in $R^N$ . I don't understand the reason of this limit: $$ \dfrac{1}{N \omega_n \epsilon^{N-1}} \int_{\partial B_{\epsilon}(y)}u(x) d\sigma  \rightarrow u(y)$$ for $\epsilon \rightarrow 0$ . P.S. $u(x)$ is not an harmonic function.,Let a continuos function over a domain . Let the area of sphere in . I don't understand the reason of this limit: for . P.S. is not an harmonic function., u(x) \Omega N\omega_n r^{N-1} R^N  \dfrac{1}{N \omega_n \epsilon^{N-1}} \int_{\partial B_{\epsilon}(y)}u(x) d\sigma  \rightarrow u(y) \epsilon \rightarrow 0 u(x),['analysis']
23,Morrey's Inequality in 1D,Morrey's Inequality in 1D,,"Morrey's Inequality in 1D for $p=2$ : There exists a constant $C$ such that $\|u\|_{C^{0,1/2}(\mathbb{R})} \leqslant C \|u\|_{H^{1}(\mathbb{R})}$ for all $u \in C^{1}(\mathbb{R})$ . Of course, for any $u \in C^{1}$ we have by the fundamental theorem of calculus and Holder's inequality: $|u(x)-u(y)| \leqslant \int_{y}^{x} |u'(t)|\;dt \leqslant \|u'\|_{L^{2}(\mathbb{R})} |x-y|^{1/2}$ . This implies $[u]_{C^{0,1/2}(\mathbb{R})} \leqslant \|u\|_{H^{1}(\mathbb{R})}$ . To conclude, we need to show that $|u(x)| \leqslant C\|u\|_{H^{1}(\mathbb{R})}$ for all $x \in \mathbb{R}$ . How can I go about proving this? I know Evans has a proof for $\mathbb{R}^{n}$ but I would like to know if there is a simpler approach in 1D.","Morrey's Inequality in 1D for : There exists a constant such that for all . Of course, for any we have by the fundamental theorem of calculus and Holder's inequality: . This implies . To conclude, we need to show that for all . How can I go about proving this? I know Evans has a proof for but I would like to know if there is a simpler approach in 1D.","p=2 C \|u\|_{C^{0,1/2}(\mathbb{R})} \leqslant C \|u\|_{H^{1}(\mathbb{R})} u \in C^{1}(\mathbb{R}) u \in C^{1} |u(x)-u(y)| \leqslant \int_{y}^{x} |u'(t)|\;dt \leqslant \|u'\|_{L^{2}(\mathbb{R})} |x-y|^{1/2} [u]_{C^{0,1/2}(\mathbb{R})} \leqslant \|u\|_{H^{1}(\mathbb{R})} |u(x)| \leqslant C\|u\|_{H^{1}(\mathbb{R})} x \in \mathbb{R} \mathbb{R}^{n}","['analysis', 'sobolev-spaces']"
24,Log (uniform) continuous functions,Log (uniform) continuous functions,,"I am interested in functions $f: \mathbb R_+\to \mathbb R_+$ such that $\log \circ f \circ \exp$ is uniformly continuous. In other words \begin{align}    \forall_{c}\, \exists_{c'}, \forall_{ x,x'\in[x/c',xc']}\, f(x')\in[f(x)/c, f(x)c]. \end{align} This definition includes functions such as $x\mapsto x^2$ and $x\mapsto x^{-1}$ which are not uniformly continuous. I'm wondering what the right term for such functions is? I have also looked for ""multiplicatively uniformly continuous"", also with no luck. If the range of $f$ is compact, can we say that $f^{-1}$ is also log uniformly continuous? Can we say that if $f$ and $g$ are Log uniform continuous, then so is $fg$ ?","I am interested in functions such that is uniformly continuous. In other words This definition includes functions such as and which are not uniformly continuous. I'm wondering what the right term for such functions is? I have also looked for ""multiplicatively uniformly continuous"", also with no luck. If the range of is compact, can we say that is also log uniformly continuous? Can we say that if and are Log uniform continuous, then so is ?","f: \mathbb R_+\to \mathbb R_+ \log \circ f \circ \exp \begin{align}
   \forall_{c}\, \exists_{c'}, \forall_{ x,x'\in[x/c',xc']}\, f(x')\in[f(x)/c, f(x)c].
\end{align} x\mapsto x^2 x\mapsto x^{-1} f f^{-1} f g fg","['analysis', 'reference-request', 'continuity', 'uniform-continuity']"
25,Show that $\int_{-\infty}^{\infty} \frac{\cos (\theta) e^{\theta y}}{2 \cosh(\pi y/2)} y dy=\tan (\theta)$.,Show that .,\int_{-\infty}^{\infty} \frac{\cos (\theta) e^{\theta y}}{2 \cosh(\pi y/2)} y dy=\tan (\theta),"I'm not a hardcore mathematician. I have exhausted all integration trick I learned in calculus including change of variables, integration by parts. That cosh on denominator is just very much in the way. How one should approach an integral like this.","I'm not a hardcore mathematician. I have exhausted all integration trick I learned in calculus including change of variables, integration by parts. That cosh on denominator is just very much in the way. How one should approach an integral like this.",,"['calculus', 'integration', 'analysis', 'definite-integrals']"
26,$\{ x \in X : |f(x)| \geq \epsilon \}$ is compact then $f$ is uniformly continuous on $X$.,is compact then  is uniformly continuous on .,\{ x \in X : |f(x)| \geq \epsilon \} f X,"Past year paper question. Let $(X, d)$ be a metric space and let $f: X \to \mathbb{R}$ be a continuous function, where $\mathbb{R}$ is given the standard metric. Assume that for any $\epsilon > 0$ , the set $\{ x \in X : |f(x)| \geq \epsilon \}$ is a compact metric subspace of $X$ . Show that $f$ is uniformly continuous on $X$ . Attempt: Let $\epsilon >0$ be given. Let $K := \{ x \in X : |f(x)| \geq \frac{\epsilon}{2} \}$ . $f$ is continuous on $K$ $\implies f$ is uniform continuous on $K$ . Then there exists $\delta$ such that $d(x,y)<\delta \implies |f(x)-f(y)| < \frac{\epsilon}{2}$ . Take any $2$ points $x,y \in X$ , such that $d(x,y)<\delta$ . If $x,y \in K$ , then $|f(x)-f(y)| < \frac{\epsilon}{2} < \epsilon$ . If $x,y \notin K$ , then $|f(x)-f(y)| \leq |f(x)|+|f(y)| \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} \leq \epsilon$ . But I am stuck here, because what if $x \in K$ and $y \notin K$ .","Past year paper question. Let be a metric space and let be a continuous function, where is given the standard metric. Assume that for any , the set is a compact metric subspace of . Show that is uniformly continuous on . Attempt: Let be given. Let . is continuous on is uniform continuous on . Then there exists such that . Take any points , such that . If , then . If , then . But I am stuck here, because what if and .","(X, d) f: X \to \mathbb{R} \mathbb{R} \epsilon > 0 \{ x \in X : |f(x)| \geq \epsilon \} X f X \epsilon >0 K := \{ x \in X : |f(x)| \geq \frac{\epsilon}{2} \} f K \implies f K \delta d(x,y)<\delta \implies |f(x)-f(y)| < \frac{\epsilon}{2} 2 x,y \in X d(x,y)<\delta x,y \in K |f(x)-f(y)| < \frac{\epsilon}{2} < \epsilon x,y \notin K |f(x)-f(y)| \leq |f(x)|+|f(y)| \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} \leq \epsilon x \in K y \notin K","['analysis', 'metric-spaces']"
27,"If $ f > 0 $, $g\ge0$, and $ \int_a^b g > 0 $, then $ \int_a^b fg > 0 $?","If , , and , then ?", f > 0  g\ge0  \int_a^b g > 0   \int_a^b fg > 0 ,"Let $ f:[a,b] \rightarrow \mathbb{R} $ be continuous on $ [a,b] $ and $ f > 0 $ on $ (a,b) $ , and let $ g:[a,b] \rightarrow \mathbb{R} $ be non-negative and integrable on $ [a,b] $ .  If $ f > 0 $ , and $$ \int_a^b g > 0 \,, $$ then is it true that $$ \int_a^b fg > 0 \,? $$ The integrals are Riemann integrals.","Let be continuous on and on , and let be non-negative and integrable on .  If , and then is it true that The integrals are Riemann integrals."," f:[a,b] \rightarrow \mathbb{R}   [a,b]   f > 0   (a,b)   g:[a,b] \rightarrow \mathbb{R}   [a,b]   f > 0   \int_a^b g > 0 \,,   \int_a^b fg > 0 \,? ","['real-analysis', 'integration', 'analysis']"
28,"Prob. 4, Exercises 8.14, in Apostol's CALCULUS Vol II: Computing the gradient vector and a directional derivative for a scalar field","Prob. 4, Exercises 8.14, in Apostol's CALCULUS Vol II: Computing the gradient vector and a directional derivative for a scalar field",,"Here is Prob. 4, Exercises 8.14, in the book Calculus Vol II by Tom M. Apostol, 2nd edition: A differentiable scalar field $f$ has, at the point $(1, 2)$, directional derivative $+2$ in the direction toward $(2, 2)$ and $-2$ in the direction toward $(1, 1)$. Determine the gradient vector at $(1, 2)$ and compute the directional derivative in the direction toward $(4, 6)$. My Attempt: Let $(a, b)$ be the gradient vector $\nabla f(1, 2)$ of the scalar field $f$ at the point $(1, 2)$. The vector $\mathbf{u}$ from $(1, 2)$ to $(2, 2)$ is given by    $$ \mathbf{u} = (2, 2) - (1, 2) = (1, 0). $$   Now as $f$ is differentiable at $(1, 2)$, so the directional derivative of $f$ in the direction of $\mathbf{u}$ is    $$ \nabla f(1, 2) \cdot \mathbf{u} = (a, b) \cdot (1, 0) = a. $$   Therefore we obtain $a = 2$. The vector $\mathbf{v}$ from $(1, 2)$ to $(1, 1)$ is given by    $$ \mathbf{v} = (1, 1) - (1, 2) = (0, -1). $$   Once again as $f$ is differentiable at $(1, 2)$, so the directional derivative of $f$ in the direction of $\mathbf{v}$ is    $$ \nabla f(1, 2) \cdot \mathbf{v} = (a, b) \cdot (0, -1) = -b. $$   Therefore we obtain $b = 2$. Thus the gradient vector of $f$ at $(1, 2)$ is given by    $$ \nabla f(1, 2) = (2, 2). $$ Now the vector $\mathbf{w}$ from $(1, 2)$ toward $(4, 6)$ is given by    $$ \mathbf{w} = (4, 6)- (1, 2) = (3, 4). $$   So the directional derivative of $f$ at $(1, 2)$ in the direction toward $\mathbf{w}$ is    $$ \nabla f(1, 2) \cdot \mathbf{w} = (2, 2) \cdot (3, 4) = 14. $$ Is there any error --- either in logic or calculation --- in this solution?","Here is Prob. 4, Exercises 8.14, in the book Calculus Vol II by Tom M. Apostol, 2nd edition: A differentiable scalar field $f$ has, at the point $(1, 2)$, directional derivative $+2$ in the direction toward $(2, 2)$ and $-2$ in the direction toward $(1, 1)$. Determine the gradient vector at $(1, 2)$ and compute the directional derivative in the direction toward $(4, 6)$. My Attempt: Let $(a, b)$ be the gradient vector $\nabla f(1, 2)$ of the scalar field $f$ at the point $(1, 2)$. The vector $\mathbf{u}$ from $(1, 2)$ to $(2, 2)$ is given by    $$ \mathbf{u} = (2, 2) - (1, 2) = (1, 0). $$   Now as $f$ is differentiable at $(1, 2)$, so the directional derivative of $f$ in the direction of $\mathbf{u}$ is    $$ \nabla f(1, 2) \cdot \mathbf{u} = (a, b) \cdot (1, 0) = a. $$   Therefore we obtain $a = 2$. The vector $\mathbf{v}$ from $(1, 2)$ to $(1, 1)$ is given by    $$ \mathbf{v} = (1, 1) - (1, 2) = (0, -1). $$   Once again as $f$ is differentiable at $(1, 2)$, so the directional derivative of $f$ in the direction of $\mathbf{v}$ is    $$ \nabla f(1, 2) \cdot \mathbf{v} = (a, b) \cdot (0, -1) = -b. $$   Therefore we obtain $b = 2$. Thus the gradient vector of $f$ at $(1, 2)$ is given by    $$ \nabla f(1, 2) = (2, 2). $$ Now the vector $\mathbf{w}$ from $(1, 2)$ toward $(4, 6)$ is given by    $$ \mathbf{w} = (4, 6)- (1, 2) = (3, 4). $$   So the directional derivative of $f$ at $(1, 2)$ in the direction toward $\mathbf{w}$ is    $$ \nabla f(1, 2) \cdot \mathbf{w} = (2, 2) \cdot (3, 4) = 14. $$ Is there any error --- either in logic or calculation --- in this solution?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
29,Are the partial derivatives of a mollifier bounded?,Are the partial derivatives of a mollifier bounded?,,"More precisely, I am wondering whether for any mollifier $ \varphi : \mathbb{R}^n \to \mathbb{R} $ there exists a constant $ M $ such that $ \frac{\partial \varphi}{\partial x^i} (x) \leq M $ for all $ x \in \mathbb{R}^n $. Recall, a mollifier $ \varphi $ is a function which satisfies the following conditions: $ \varphi \in C^{\infty}_{0}(\mathbb{R}^n) $, with $ \mathrm{supp} = \{ x \in  \mathbb{R}^n : \lvert x\rvert \leq 1 \} $; $ \varphi > 0 $; $ \int_{\mathbb{R}^n} \varphi = 1 $. – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – This question occurred to me whilst trying to prove the following result: Let $ \varphi : \mathbb{R}^n \to \mathbb{R} $ be a mollifier and define $ \varphi_{\epsilon} := \epsilon^{-n} \varphi(x/\epsilon) $, so that the defining properties of a mollifier listed above automatically imply that $ \varphi_{\epsilon} \in C^{\infty}_{0}(\mathbb{R}^n) $, $ \mathrm{supp} = B(0,\epsilon) $ and $ \int_{\mathbb{R}^n} \varphi_{\epsilon} = 1 $. Furthermore let $ u: \mathbb{R}^n \to \mathbb{C} $ be a locally integrable function and define $ u_{\epsilon} $ to be the convolution $ u_{\epsilon} := u \ast \varphi_{\epsilon} $. Then the function $ u_\epsilon : \mathbb{R}^n \to \mathbb{C}$ is $ C^{\infty} $. My attempt of proving this result begins as follows: Let $ e_i $ be the $i$'th unit vector of the standard basis of $ \mathbb{R}^n $, that is, $ e_i = (e_i^1, ..., e_i^n) $ with $ e_i^j = \delta_{ij} $, where the latter is the Kronecker delta. Then, for a fixed $ x \in \mathbb{R}^n $, we have \begin{align*} \frac{\partial u_{\epsilon}}{\partial x^i} (x) &= \frac{\partial}{\partial x^i} (u \ast \varphi_{\epsilon}) (x) \\ &= \lim_{h \to 0} \frac{1}{h} \left\{ (u \ast \varphi_{\epsilon})(x + h e_i) - (u \ast \varphi_{\epsilon})(x) \right\} \\ &= \lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \mathrm{d}y . \end{align*} Now the most elegant way to reach the desired conclusion would probably be to use the mean value theorem , by which there must exist a constant $ 0 < c(h) < h $ such that \begin{align*} \lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \mathrm{d}y &= \lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \: \frac{\partial \varphi_{\epsilon}}{\partial x^i} (x-y+c(h)) \: \mathrm{d}y \\ &= \lim_{h \to 0} \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x+c(h)) . \end{align*} It is not hard to show that the convolution of a locally integrable function with a continuous function of compact support is continuous and thus we obtain \begin{align*} \frac{\partial u_{\epsilon}}{\partial x^i} (x) &= \lim_{h \to 0} \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x+c(h)) \\ &= \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x) . \end{align*} I began wondering, however, if there is an alternative to the application of the mean-value theorem. If one could show, for instance, that the integrand $ \left\{ u(y) \left( \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right) \right\} $ satisfies the conditions of Lebesgue's dominated convergence theorem , then one could interchange the limit and integral in the first equation above to immediately obtain that $ \frac{\partial u_{\epsilon}}{\partial x^i} (x) = \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x) $. But the difficulty lies in showing that the integrand is dominated by an $ L^1 $ function. However, if we knew that the partial derivatives of $ \varphi $ (and thus of $ \varphi_\epsilon $) were bounded, then the continuity of these partial derivatives would imply (via the mean value theorem I believe...) that for all $ h \leq \delta $, where $ \delta > 0 $ is some constant, there exists a constant $ c > 0 $ such that for all $ y $ \begin{align} \left \lvert \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} - \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) \right \rvert \leq c , \end{align} (remember $ x $ was fixed) and so \begin{align} \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) - c \leq \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \leq \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) + c . \end{align} Now just let $ M := \max \left\lvert \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) - c \pm \right\rvert $. Then \begin{align} \left\lvert u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \right\rvert \leq M \: \Bigr\lvert \: u\bigr\vert_{supp\left( \varphi_{\epsilon}(x+\delta e_i - \: \cdot) \right) \cap supp\left( \varphi_{\epsilon}(x- \: \cdot) \right)}(y) \Bigr\rvert . \end{align} Hence the integrand is bounded by an integrable function. This is how my question originated.","More precisely, I am wondering whether for any mollifier $ \varphi : \mathbb{R}^n \to \mathbb{R} $ there exists a constant $ M $ such that $ \frac{\partial \varphi}{\partial x^i} (x) \leq M $ for all $ x \in \mathbb{R}^n $. Recall, a mollifier $ \varphi $ is a function which satisfies the following conditions: $ \varphi \in C^{\infty}_{0}(\mathbb{R}^n) $, with $ \mathrm{supp} = \{ x \in  \mathbb{R}^n : \lvert x\rvert \leq 1 \} $; $ \varphi > 0 $; $ \int_{\mathbb{R}^n} \varphi = 1 $. – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – This question occurred to me whilst trying to prove the following result: Let $ \varphi : \mathbb{R}^n \to \mathbb{R} $ be a mollifier and define $ \varphi_{\epsilon} := \epsilon^{-n} \varphi(x/\epsilon) $, so that the defining properties of a mollifier listed above automatically imply that $ \varphi_{\epsilon} \in C^{\infty}_{0}(\mathbb{R}^n) $, $ \mathrm{supp} = B(0,\epsilon) $ and $ \int_{\mathbb{R}^n} \varphi_{\epsilon} = 1 $. Furthermore let $ u: \mathbb{R}^n \to \mathbb{C} $ be a locally integrable function and define $ u_{\epsilon} $ to be the convolution $ u_{\epsilon} := u \ast \varphi_{\epsilon} $. Then the function $ u_\epsilon : \mathbb{R}^n \to \mathbb{C}$ is $ C^{\infty} $. My attempt of proving this result begins as follows: Let $ e_i $ be the $i$'th unit vector of the standard basis of $ \mathbb{R}^n $, that is, $ e_i = (e_i^1, ..., e_i^n) $ with $ e_i^j = \delta_{ij} $, where the latter is the Kronecker delta. Then, for a fixed $ x \in \mathbb{R}^n $, we have \begin{align*} \frac{\partial u_{\epsilon}}{\partial x^i} (x) &= \frac{\partial}{\partial x^i} (u \ast \varphi_{\epsilon}) (x) \\ &= \lim_{h \to 0} \frac{1}{h} \left\{ (u \ast \varphi_{\epsilon})(x + h e_i) - (u \ast \varphi_{\epsilon})(x) \right\} \\ &= \lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \mathrm{d}y . \end{align*} Now the most elegant way to reach the desired conclusion would probably be to use the mean value theorem , by which there must exist a constant $ 0 < c(h) < h $ such that \begin{align*} \lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \mathrm{d}y &= \lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \: \frac{\partial \varphi_{\epsilon}}{\partial x^i} (x-y+c(h)) \: \mathrm{d}y \\ &= \lim_{h \to 0} \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x+c(h)) . \end{align*} It is not hard to show that the convolution of a locally integrable function with a continuous function of compact support is continuous and thus we obtain \begin{align*} \frac{\partial u_{\epsilon}}{\partial x^i} (x) &= \lim_{h \to 0} \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x+c(h)) \\ &= \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x) . \end{align*} I began wondering, however, if there is an alternative to the application of the mean-value theorem. If one could show, for instance, that the integrand $ \left\{ u(y) \left( \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right) \right\} $ satisfies the conditions of Lebesgue's dominated convergence theorem , then one could interchange the limit and integral in the first equation above to immediately obtain that $ \frac{\partial u_{\epsilon}}{\partial x^i} (x) = \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x) $. But the difficulty lies in showing that the integrand is dominated by an $ L^1 $ function. However, if we knew that the partial derivatives of $ \varphi $ (and thus of $ \varphi_\epsilon $) were bounded, then the continuity of these partial derivatives would imply (via the mean value theorem I believe...) that for all $ h \leq \delta $, where $ \delta > 0 $ is some constant, there exists a constant $ c > 0 $ such that for all $ y $ \begin{align} \left \lvert \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} - \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) \right \rvert \leq c , \end{align} (remember $ x $ was fixed) and so \begin{align} \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) - c \leq \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \leq \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) + c . \end{align} Now just let $ M := \max \left\lvert \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) - c \pm \right\rvert $. Then \begin{align} \left\lvert u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \right\rvert \leq M \: \Bigr\lvert \: u\bigr\vert_{supp\left( \varphi_{\epsilon}(x+\delta e_i - \: \cdot) \right) \cap supp\left( \varphi_{\epsilon}(x- \: \cdot) \right)}(y) \Bigr\rvert . \end{align} Hence the integrand is bounded by an integrable function. This is how my question originated.",,"['real-analysis', 'integration', 'analysis', 'partial-derivative', 'convolution']"
30,High iteration Newton's Method,High iteration Newton's Method,,"Suppose newton's iteration method is applied to $f(x)=1/x$ with $x_{0}=1$. Find $x_{50}$. So for me it seems like that newton's method is trying to approximate the root of the function. From Newton's Method $x_{i+1}=x_{i}-\frac{f(x_{i})}{f'(x_{i})}$ So \begin{align*} x_{i+1}=x_{i}-\frac{\frac{1}{x_{i}}}{-\frac{1}{x^2_{i}}}=x_{i}+x_{i} \end{align*} From the above relation, we can compute the first values \begin{align*} x_{1}&=x_{0}+x_{0}=2\\ x_{2}&=x_{1}+x_{1}=4\\ x_{3}&=x_{2}+x_{2}=8\\ x_{4}&=x_{3}+x_{3}=16\\ x_{5}&=x_{4}+x_{4}=32 \end{align*} So \begin{align*} x_{0}&=1\\ x_{1}&=x_{0} \cdot 2\\ x_{2}&=x_{0} \cdot 2^2\\ x_{3}&=x_{0} \cdot 2^3\\ x_{n}&=x_{0} \cdot 2^n \end{align*} Therefore $x_{50}=2^{50}$ I just think my result looks strange, does it just mean what we can see intuitively that the function has no roots?","Suppose newton's iteration method is applied to $f(x)=1/x$ with $x_{0}=1$. Find $x_{50}$. So for me it seems like that newton's method is trying to approximate the root of the function. From Newton's Method $x_{i+1}=x_{i}-\frac{f(x_{i})}{f'(x_{i})}$ So \begin{align*} x_{i+1}=x_{i}-\frac{\frac{1}{x_{i}}}{-\frac{1}{x^2_{i}}}=x_{i}+x_{i} \end{align*} From the above relation, we can compute the first values \begin{align*} x_{1}&=x_{0}+x_{0}=2\\ x_{2}&=x_{1}+x_{1}=4\\ x_{3}&=x_{2}+x_{2}=8\\ x_{4}&=x_{3}+x_{3}=16\\ x_{5}&=x_{4}+x_{4}=32 \end{align*} So \begin{align*} x_{0}&=1\\ x_{1}&=x_{0} \cdot 2\\ x_{2}&=x_{0} \cdot 2^2\\ x_{3}&=x_{0} \cdot 2^3\\ x_{n}&=x_{0} \cdot 2^n \end{align*} Therefore $x_{50}=2^{50}$ I just think my result looks strange, does it just mean what we can see intuitively that the function has no roots?",,['analysis']
31,Is it possible to obtain a constant jacobian on change of coordinates?,Is it possible to obtain a constant jacobian on change of coordinates?,,"Let M to be a compact orientable $m$-dimensional manifold. I am interested in the following comment given by a user of forum:  suppose that ""you have an atlas $\{(U_{\alpha},\mathbf{x}_{\alpha})\}_{\alpha \in I}$ such that for all $\alpha , \beta \in I$ with $U_{\alpha} \cap U_{\beta} \neq \varnothing$ the jacobian matrix of $\mathbf{x}_{\beta} \circ \mathbf{x}_{\alpha}^{-1}$ is constant (i.e., the determinant of change of coordinates matrix is constant)."" (See 1 ). We know that the affin manifolds satisfy this property. We would like to know if given a compact orientable $m$-dimensional manifold there exists an atlas of manifold that  satisfies this property. (It is look false, but the question is more general, and I do not know how to solve it). Att","Let M to be a compact orientable $m$-dimensional manifold. I am interested in the following comment given by a user of forum:  suppose that ""you have an atlas $\{(U_{\alpha},\mathbf{x}_{\alpha})\}_{\alpha \in I}$ such that for all $\alpha , \beta \in I$ with $U_{\alpha} \cap U_{\beta} \neq \varnothing$ the jacobian matrix of $\mathbf{x}_{\beta} \circ \mathbf{x}_{\alpha}^{-1}$ is constant (i.e., the determinant of change of coordinates matrix is constant)."" (See 1 ). We know that the affin manifolds satisfy this property. We would like to know if given a compact orientable $m$-dimensional manifold there exists an atlas of manifold that  satisfies this property. (It is look false, but the question is more general, and I do not know how to solve it). Att",,"['analysis', 'differential-geometry']"
32,Understanding theorem 9.12 in Rudin's PMA,Understanding theorem 9.12 in Rudin's PMA,,"$9.11$ Definition Suppose $E$ is an open set in $R^n,$ f maps $E$ into $R^m,$ and x $\in E.$ If there exists a linear transformation $\mathbf{A}$ of $R^n$ into $R^m$ such that $$\lim_{h\to 0}\frac{\left|\mathbf{f(x +h)-f(x)-Ah}\right|}{|\mathbf{h}|}=0,\tag{14}$$ then we say $\mathbf{f}$ is differentiable at $\mathbf{x},$ and we write $\mathbf{f'(x)=A}$ $9.12$ Theorem Suppose $E$ and f are as in Definition $9.11,$ x $\in E,$ and $(14)$ holds with $\mathbf{A=A_1}$ and with $\mathbf{A=A_2}.$ Then $\mathbf{A_1=A_2}.$ So the idea is to consider $\mathbf{B= A_1-A_2}$ and show that $\left|\mathbf{Bh}\right|\le \epsilon$ for every $\epsilon >0.$ But I failed to understand Rudin's argument : If $\mathbf{B=A_1-A_2},$ the inequality $$\left|\mathbf{Bh}\right|\le\left|\mathbf{f(x+h)-f(x)-A_1h}\right|+\left|\mathbf{f(x+h)-f(x)-A_2h}\right|$$ shows that $\frac{|\mathbf{Bh}|}{|\mathbf{h}|}\to 0$ as $\mathbf{h}\to 0.$ For fixed $\mathbf{h\ne 0},$ it follows that $$\frac{|\mathbf{B(th)}|}{|\mathbf{th}|}\to 0 \text{ as } t\to 0.\tag{16}$$ The linearity of $\mathbf{B}$ shows that the left side of $(16)$ is independent of $t.$ Thus $\mathbf{Bh}=0$ for every $\mathbf{h}\in R^n.$ Hence $\mathbf{B}=0.$ I understood each and every step but I am not able to see the link showing $\left|\mathbf{Bh}\right|\le \epsilon$ for every $\epsilon >0.$ Can someone help me with this?",Definition Suppose is an open set in f maps into and x If there exists a linear transformation of into such that then we say is differentiable at and we write Theorem Suppose and f are as in Definition x and holds with and with Then So the idea is to consider and show that for every But I failed to understand Rudin's argument : If the inequality shows that as For fixed it follows that The linearity of shows that the left side of is independent of Thus for every Hence I understood each and every step but I am not able to see the link showing for every Can someone help me with this?,"9.11 E R^n, E R^m, \in E. \mathbf{A} R^n R^m \lim_{h\to 0}\frac{\left|\mathbf{f(x +h)-f(x)-Ah}\right|}{|\mathbf{h}|}=0,\tag{14} \mathbf{f} \mathbf{x}, \mathbf{f'(x)=A} 9.12 E 9.11, \in E, (14) \mathbf{A=A_1} \mathbf{A=A_2}. \mathbf{A_1=A_2}. \mathbf{B= A_1-A_2} \left|\mathbf{Bh}\right|\le \epsilon \epsilon >0. \mathbf{B=A_1-A_2}, \left|\mathbf{Bh}\right|\le\left|\mathbf{f(x+h)-f(x)-A_1h}\right|+\left|\mathbf{f(x+h)-f(x)-A_2h}\right| \frac{|\mathbf{Bh}|}{|\mathbf{h}|}\to 0 \mathbf{h}\to 0. \mathbf{h\ne 0}, \frac{|\mathbf{B(th)}|}{|\mathbf{th}|}\to 0 \text{ as } t\to 0.\tag{16} \mathbf{B} (16) t. \mathbf{Bh}=0 \mathbf{h}\in R^n. \mathbf{B}=0. \left|\mathbf{Bh}\right|\le \epsilon \epsilon >0.","['real-analysis', 'analysis']"
33,Is there an algorithm to determine if a power series is periodic?,Is there an algorithm to determine if a power series is periodic?,,"We know that the sine function is periodic by its geometric definition. The Taylor/MacLaurin series expansion about 0 which is the basis of actual mechanisms for computing it is: $$\sin(x) = \sum_{n=0}^\infty  \frac{(-1)^n}{(2n+1)!}x^{2n+1}$$ This series manages to be periodic with period $2\pi$ because it has an alternating sign. Is there a way to tell if an arbitrary power series is periodic? More informally, if someone gave us the above summation for $\sin$ without telling us it was a trigonometric function, is there a procedure for discovering that it is periodic and finding the period? Amendment: As pointed out in the answers, there is clearly no algorithm if the coefficients are allowed to be arbitrary, thus containing an unbounded amount of information. I should have asked ""Under what limitations to a function defining the coefficients of a power series does there an exist algorithm for determining if the power series is periodic?"" In particular, if $f(n)$ is limited to a rational expression that would be accepted as a ""closed-form"" expression, as it is in the case of $\sin(x)$, does such an algorithm exist? If $f(n)$ is limited to being a simple arithmetic computation from $n$, can we determine if the function is periodic?","We know that the sine function is periodic by its geometric definition. The Taylor/MacLaurin series expansion about 0 which is the basis of actual mechanisms for computing it is: $$\sin(x) = \sum_{n=0}^\infty  \frac{(-1)^n}{(2n+1)!}x^{2n+1}$$ This series manages to be periodic with period $2\pi$ because it has an alternating sign. Is there a way to tell if an arbitrary power series is periodic? More informally, if someone gave us the above summation for $\sin$ without telling us it was a trigonometric function, is there a procedure for discovering that it is periodic and finding the period? Amendment: As pointed out in the answers, there is clearly no algorithm if the coefficients are allowed to be arbitrary, thus containing an unbounded amount of information. I should have asked ""Under what limitations to a function defining the coefficients of a power series does there an exist algorithm for determining if the power series is periodic?"" In particular, if $f(n)$ is limited to a rational expression that would be accepted as a ""closed-form"" expression, as it is in the case of $\sin(x)$, does such an algorithm exist? If $f(n)$ is limited to being a simple arithmetic computation from $n$, can we determine if the function is periodic?",,"['analysis', 'algorithms', 'power-series', 'taylor-expansion', 'turing-machines']"
34,Generated $\sigma$ - algebra example,Generated  - algebra example,\sigma,"Let $\Omega = \mathbb{Z}$. Consider $E_1 := \big\{\{2n|n \in \mathbb{Z}\}\big\}$ and $E_2 := \big\{\{2n\} | n \in \mathbb{Z}\big\}$. Find the generated $\sigma$-algebra $\sigma$($E_1$) and $\sigma$($E_2$). So this solution is from my teacher: $$\sigma(E_1) = \{ \emptyset, E_1, E_1^c, \Omega \} = \big\{ \emptyset, \{2n|n \in \mathbb{Z}\}, \{2n+1|n \in \mathbb{Z}\}, \mathbb{Z} \big\}.$$ $$\sigma(E_2) = \big\{ \mathcal{P}(E_2),  \{2n+1|n \in \mathbb{Z}\}, ( \{2n+1|n \in \mathbb{Z}\} \cup C ) | C \in \mathcal{P}(E_2) \}$$ Remark: $\mathcal{P}$ is the power set. So is this really correct? To me $\sigma(E_1)$ is only correct if we would have $E_1 = \{2n|n \in \mathbb{Z}\}$ not $\big\{\{2n|n \in \mathbb{Z}\}\big\}$. And for $\sigma(E_2)$ I would say that we have $\big\{ C, \{2n+1|n \in \mathbb{Z}\}, ( \{2n+1|n \in \mathbb{Z}\} \cup C ) | C \in \mathcal{P}(E_2)\}$.","Let $\Omega = \mathbb{Z}$. Consider $E_1 := \big\{\{2n|n \in \mathbb{Z}\}\big\}$ and $E_2 := \big\{\{2n\} | n \in \mathbb{Z}\big\}$. Find the generated $\sigma$-algebra $\sigma$($E_1$) and $\sigma$($E_2$). So this solution is from my teacher: $$\sigma(E_1) = \{ \emptyset, E_1, E_1^c, \Omega \} = \big\{ \emptyset, \{2n|n \in \mathbb{Z}\}, \{2n+1|n \in \mathbb{Z}\}, \mathbb{Z} \big\}.$$ $$\sigma(E_2) = \big\{ \mathcal{P}(E_2),  \{2n+1|n \in \mathbb{Z}\}, ( \{2n+1|n \in \mathbb{Z}\} \cup C ) | C \in \mathcal{P}(E_2) \}$$ Remark: $\mathcal{P}$ is the power set. So is this really correct? To me $\sigma(E_1)$ is only correct if we would have $E_1 = \{2n|n \in \mathbb{Z}\}$ not $\big\{\{2n|n \in \mathbb{Z}\}\big\}$. And for $\sigma(E_2)$ I would say that we have $\big\{ C, \{2n+1|n \in \mathbb{Z}\}, ( \{2n+1|n \in \mathbb{Z}\} \cup C ) | C \in \mathcal{P}(E_2)\}$.",,"['analysis', 'measure-theory']"
35,Integration by Parts for multivariable functions,Integration by Parts for multivariable functions,,"Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$. Let also $f$ be twice continuously differentiable, $f \in C^2(\mathbb{R}^2)$, and the function $\frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} f(x_1,x_2)$ be absolutely integrable, $\frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} f(x_1,x_2)\in L_1(\mathbb{R}^2)$. I am wondering what are sufficient conditions for the following identity to hold \begin{align*} 	 &\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} \frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} f(x_1,x_2)  dx_1dx_2\\ 	 &\qquad\qquad= 	 \left(f(x_1,x_2)\Big\vert_{x_1=-\infty}^{x_1=+\infty} \right)\Big\vert_{x_2=-\infty}^{x_2=+\infty}\\ 	  &\qquad\qquad=  	 f(+\infty,+\infty) - f(+\infty,-\infty) -  f(-\infty,+\infty) + f(-\infty,-\infty)? \end{align*} I would appreciate any ideas, suggestions, counterexamples. Thanks!","Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$. Let also $f$ be twice continuously differentiable, $f \in C^2(\mathbb{R}^2)$, and the function $\frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} f(x_1,x_2)$ be absolutely integrable, $\frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} f(x_1,x_2)\in L_1(\mathbb{R}^2)$. I am wondering what are sufficient conditions for the following identity to hold \begin{align*} 	 &\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} \frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} f(x_1,x_2)  dx_1dx_2\\ 	 &\qquad\qquad= 	 \left(f(x_1,x_2)\Big\vert_{x_1=-\infty}^{x_1=+\infty} \right)\Big\vert_{x_2=-\infty}^{x_2=+\infty}\\ 	  &\qquad\qquad=  	 f(+\infty,+\infty) - f(+\infty,-\infty) -  f(-\infty,+\infty) + f(-\infty,-\infty)? \end{align*} I would appreciate any ideas, suggestions, counterexamples. Thanks!",,"['calculus', 'integration', 'analysis']"
36,Asymptotic of a sum: $\sum_{k=n}^{\infty}{f(k)} = \int_{n}^{\infty}{f(t)dt} +\frac{f(n)}{2}+\mathcal{O}(f'(n))$,Asymptotic of a sum:,\sum_{k=n}^{\infty}{f(k)} = \int_{n}^{\infty}{f(t)dt} +\frac{f(n)}{2}+\mathcal{O}(f'(n)),"Someone told me that the following formula holds for $f$ differentiable and decreasing, with $\lim_{x\rightarrow +\infty}{f(x)}=0$. $$\sum_{k=n}^{\infty}{f(k)} = \int_{n}^{\infty}{f(t)dt} +\frac{f(n)}{2}+\mathcal{O}(f'(n))$$ But I managed to prove only if the function is convex, with the help of the formula $$f(x+h)=f(x)+hf'(x)+\int_{0}^{1}{h\left[ f'(x+ht)-f'(x) \right]dt}$$ Which give us integrating it $$\int_{0}^{1}{f(x+\theta)d\theta}=f(x)+\frac{f'(x)}{2}+\int_{0}^{1}{\int_{0}^{1}{\theta\left[ f'(x+\theta t)-f'(x) \right]dt}d\theta}$$ And then $$\begin{align} \int_{0}^{1}{f(x+\theta)d\theta} &=\frac{f(x+1)+f(x)}{2}\\&\qquad+\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta(f'(x+\theta t)-f'(x))dt}\right) -\frac{(f'(x+\theta)-f'(x))}{2}\right]d\theta}\\ &=\frac{f(x+1)+f(x)}{2}+\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta f'(x+\theta t)dt}\right)-\frac{f'(x+\theta)}{2}\right]d\theta} \end{align}$$ If we assume that the function is convex, then the term inside the integral is positive, because $f'(x+\theta t)\geq f'(x+\theta)$, and also holds that, as $f'(x+\theta t)\leq f'(x)$ and $f'(x+\theta)\geq f'(x+1)$ $$\begin{align}\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta f'(x+\theta t)dt}\right)-\frac{f'(x+\theta)}{2}\right]d\theta} &\leq \frac{1}{2}\int_{0}^{1}{(f'(x)-f'(x+1))d \theta}\\&=\frac{f'(x)-f'(x+1)}{2} \end{align}$$ Summing the expression and using these last inequalities, the result follows. Can someone help me to prove this for only differentiable functions, not necessarily convex?","Someone told me that the following formula holds for $f$ differentiable and decreasing, with $\lim_{x\rightarrow +\infty}{f(x)}=0$. $$\sum_{k=n}^{\infty}{f(k)} = \int_{n}^{\infty}{f(t)dt} +\frac{f(n)}{2}+\mathcal{O}(f'(n))$$ But I managed to prove only if the function is convex, with the help of the formula $$f(x+h)=f(x)+hf'(x)+\int_{0}^{1}{h\left[ f'(x+ht)-f'(x) \right]dt}$$ Which give us integrating it $$\int_{0}^{1}{f(x+\theta)d\theta}=f(x)+\frac{f'(x)}{2}+\int_{0}^{1}{\int_{0}^{1}{\theta\left[ f'(x+\theta t)-f'(x) \right]dt}d\theta}$$ And then $$\begin{align} \int_{0}^{1}{f(x+\theta)d\theta} &=\frac{f(x+1)+f(x)}{2}\\&\qquad+\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta(f'(x+\theta t)-f'(x))dt}\right) -\frac{(f'(x+\theta)-f'(x))}{2}\right]d\theta}\\ &=\frac{f(x+1)+f(x)}{2}+\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta f'(x+\theta t)dt}\right)-\frac{f'(x+\theta)}{2}\right]d\theta} \end{align}$$ If we assume that the function is convex, then the term inside the integral is positive, because $f'(x+\theta t)\geq f'(x+\theta)$, and also holds that, as $f'(x+\theta t)\leq f'(x)$ and $f'(x+\theta)\geq f'(x+1)$ $$\begin{align}\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta f'(x+\theta t)dt}\right)-\frac{f'(x+\theta)}{2}\right]d\theta} &\leq \frac{1}{2}\int_{0}^{1}{(f'(x)-f'(x+1))d \theta}\\&=\frac{f'(x)-f'(x+1)}{2} \end{align}$$ Summing the expression and using these last inequalities, the result follows. Can someone help me to prove this for only differentiable functions, not necessarily convex?",,"['calculus', 'integration', 'analysis', 'summation', 'asymptotics']"
37,Show that if $\lambda>\|A\|$ then $(\lambda I-A)^{-1}=\int_0^\infty e^{-t(\lambda I-A)}\mathrm dt$,Show that if  then,\lambda>\|A\| (\lambda I-A)^{-1}=\int_0^\infty e^{-t(\lambda I-A)}\mathrm dt,"Let $A\in\mathcal L(E)$. Show that if $\lambda>\|A\|$ then $(\lambda I-A)^{-1}=\int_0^\infty e^{-t(\lambda I-A)}\mathrm dt$. Here $E$ is a Banach space. I have proved the case for $E$ finite-dimensional using matrices but I dont know how to solve the case for $E$ being infinite dimensional. My first attempt was to restrict $A$ to arbitrary finite subspaces $H\subset E$, but then in general $A|_H\notin\mathcal L(H)$, that is, $H$ is not necessarily invariant under the action of $A$. At most I can assume that $A|_H\in\mathcal L(H,A(H))$, but then I cant use the same strategy that used before for the case of $E$ being finite-dimensional, what used the eigenvalues of $A$. Some help will be appreciated, thank you. I had an idea (I dont know if it would work): if I prove that $e^{t(\lambda-A)}\to \infty$ seems then easy to show that $e^{-t(\lambda-A)}\to 0$.","Let $A\in\mathcal L(E)$. Show that if $\lambda>\|A\|$ then $(\lambda I-A)^{-1}=\int_0^\infty e^{-t(\lambda I-A)}\mathrm dt$. Here $E$ is a Banach space. I have proved the case for $E$ finite-dimensional using matrices but I dont know how to solve the case for $E$ being infinite dimensional. My first attempt was to restrict $A$ to arbitrary finite subspaces $H\subset E$, but then in general $A|_H\notin\mathcal L(H)$, that is, $H$ is not necessarily invariant under the action of $A$. At most I can assume that $A|_H\in\mathcal L(H,A(H))$, but then I cant use the same strategy that used before for the case of $E$ being finite-dimensional, what used the eigenvalues of $A$. Some help will be appreciated, thank you. I had an idea (I dont know if it would work): if I prove that $e^{t(\lambda-A)}\to \infty$ seems then easy to show that $e^{-t(\lambda-A)}\to 0$.",,"['linear-algebra', 'analysis']"
38,Showing that the evaluation map is not continuous?,Showing that the evaluation map is not continuous?,,"Earlier today I was reading over some analysis notes, and I noticed something interesting and unintuitive. The metric $d_1: \mathcal{C}([0,1],\mathbb{C}) \times \mathcal{C}([0,1],\mathbb{C}) \to \mathbb{R}$ was defined via $d_1(f,g) = \int_0^1|f(x)-g(x)|\mathrm{d}x$, where $\mathcal{C}([0,1],\mathbb{C})$ is the space of continuous functions from $[0,1]$ to $\mathbb{C}$. There were some other simpler theorems proven, but right at the end it said: Interestingly, one may notice that with $x$ fixed in $[0,1]$, the evaluation map $f \mapsto f(x)$ from $\mathcal{C}([0,1],\mathbb{C})$ to $\mathbb{C}$ is not continuous with respect to $d_1$. There was no explanation for this. I was trying to think of how I could show this, but nothing I think of seems like it would work. Is there a simple explanation for this?","Earlier today I was reading over some analysis notes, and I noticed something interesting and unintuitive. The metric $d_1: \mathcal{C}([0,1],\mathbb{C}) \times \mathcal{C}([0,1],\mathbb{C}) \to \mathbb{R}$ was defined via $d_1(f,g) = \int_0^1|f(x)-g(x)|\mathrm{d}x$, where $\mathcal{C}([0,1],\mathbb{C})$ is the space of continuous functions from $[0,1]$ to $\mathbb{C}$. There were some other simpler theorems proven, but right at the end it said: Interestingly, one may notice that with $x$ fixed in $[0,1]$, the evaluation map $f \mapsto f(x)$ from $\mathcal{C}([0,1],\mathbb{C})$ to $\mathbb{C}$ is not continuous with respect to $d_1$. There was no explanation for this. I was trying to think of how I could show this, but nothing I think of seems like it would work. Is there a simple explanation for this?",,['analysis']
39,Inhomogeneous Laplace equation,Inhomogeneous Laplace equation,,"It is well known that the Laplace equation $$\Delta f=0$$ has many solutions in $\mathbb{R}^2$, but what about the inhomogeneous Laplace equation $$\Delta f=g$$ Can anyone give me a reference which discuss this equation? I want to know the condition on $g$ which makes this equation solvable, in particular, I want to know if the equation is solvable for any smooth $g$.","It is well known that the Laplace equation $$\Delta f=0$$ has many solutions in $\mathbb{R}^2$, but what about the inhomogeneous Laplace equation $$\Delta f=g$$ Can anyone give me a reference which discuss this equation? I want to know the condition on $g$ which makes this equation solvable, in particular, I want to know if the equation is solvable for any smooth $g$.",,"['analysis', 'partial-differential-equations', 'laplacian']"
40,Generalizing Jensen's inequality to several variables.,Generalizing Jensen's inequality to several variables.,,"Let $f\colon D\to\mathbb{R}$ be a continuous function, where   $D\subseteq\mathbb{R}^n$ is a convex open set. Find a sufficient   condition on the derivative of $f$ such that for any $x_i\in D$,   $1\leq i\leq n$, we have $$\frac{\sum_{i=1}^n f(x_i)}{n}\geq f\bigg(\frac{\sum_{i=1}^n x_i}{n}\bigg).$$ I thought up this question myself. I was wondering whether there is a simple test (for example, $\frac{\partial f}{\partial x}\geq0$ and $\frac{\partial f}{\partial y}\geq0$) so that I can use the inequality above.","Let $f\colon D\to\mathbb{R}$ be a continuous function, where   $D\subseteq\mathbb{R}^n$ is a convex open set. Find a sufficient   condition on the derivative of $f$ such that for any $x_i\in D$,   $1\leq i\leq n$, we have $$\frac{\sum_{i=1}^n f(x_i)}{n}\geq f\bigg(\frac{\sum_{i=1}^n x_i}{n}\bigg).$$ I thought up this question myself. I was wondering whether there is a simple test (for example, $\frac{\partial f}{\partial x}\geq0$ and $\frac{\partial f}{\partial y}\geq0$) so that I can use the inequality above.",,"['real-analysis', 'analysis', 'inequality']"
41,Show that if the sum of components of one vector adds up to 1 then the sum of the squares of the same vector is at least 1/n,Show that if the sum of components of one vector adds up to 1 then the sum of the squares of the same vector is at least 1/n,,"(NOTE: Already posted here , but closed without an answer) Hi, I've been trying to complete the following question: Suppose we have two vectors of $n$ real numbers, $[x_1,x_2,⋯,x_n]$ and $[y_1,y_2 ⋯,y_n]$ and the following inequality holds: $$(x_1y_1+x_2y_2+⋯+x_ny_n)^2≤(x_1^2+x_2^2+⋯+x_n^2)(y_1^2+y_2^2+⋯+y_n^2)$$ Show that if the sum of components of one vector adds up to 1 then the sum of the squares of the same vector is at least $\frac 1n$. I've tried a few avenues, and have come up a proof that I am not confident is right. Proof by induction: Base case is $n=1$, which is trivial, since $x_1^2 = 1^2 = 1$ and so $1 \ge \frac 1 1$. Therefore base case is true. Assume it is true for n. $$x_1^2+...+x_n^2+x_{n+1}^2 \ge \frac 1 {n+1}$$ Since $x_1^2+...+x_n^2 \ge \frac 1 n$ by our assumption, $$\frac 1 n + x_{n+1}^2 \ge \frac 1 {n+1}$$ It is this step that I think is incorrect, as the $x_1^2+...+x_n^2$ must get smaller in order to accomodate the new value of $x_{n+1}$, and still remain equal to 1. Therefore I don't think I can do this step? $$x_{n+1}^2 \ge \frac 1 {n+1} - \frac 1 n$$ The left hand side must always be $\ge 0$ and the right hand side must always be negative for values of $n \ge 1$. Therefore true for $n+1$, so must be true for all $n$. QED. Can you confirm it is wrong? If it is wrong, could you please explain how to prove it in your answer. Thanks.","(NOTE: Already posted here , but closed without an answer) Hi, I've been trying to complete the following question: Suppose we have two vectors of $n$ real numbers, $[x_1,x_2,⋯,x_n]$ and $[y_1,y_2 ⋯,y_n]$ and the following inequality holds: $$(x_1y_1+x_2y_2+⋯+x_ny_n)^2≤(x_1^2+x_2^2+⋯+x_n^2)(y_1^2+y_2^2+⋯+y_n^2)$$ Show that if the sum of components of one vector adds up to 1 then the sum of the squares of the same vector is at least $\frac 1n$. I've tried a few avenues, and have come up a proof that I am not confident is right. Proof by induction: Base case is $n=1$, which is trivial, since $x_1^2 = 1^2 = 1$ and so $1 \ge \frac 1 1$. Therefore base case is true. Assume it is true for n. $$x_1^2+...+x_n^2+x_{n+1}^2 \ge \frac 1 {n+1}$$ Since $x_1^2+...+x_n^2 \ge \frac 1 n$ by our assumption, $$\frac 1 n + x_{n+1}^2 \ge \frac 1 {n+1}$$ It is this step that I think is incorrect, as the $x_1^2+...+x_n^2$ must get smaller in order to accomodate the new value of $x_{n+1}$, and still remain equal to 1. Therefore I don't think I can do this step? $$x_{n+1}^2 \ge \frac 1 {n+1} - \frac 1 n$$ The left hand side must always be $\ge 0$ and the right hand side must always be negative for values of $n \ge 1$. Therefore true for $n+1$, so must be true for all $n$. QED. Can you confirm it is wrong? If it is wrong, could you please explain how to prove it in your answer. Thanks.",,"['real-analysis', 'analysis', 'proof-verification', 'induction']"
42,Does $\sum \frac{a_n}{1+n a_n}$ converge if $a_n = \frac{1}{\sqrt{n}}$ if $n$ is a perfect square and $a_n = \frac{1}{n^2}$ otherwise?,Does  converge if  if  is a perfect square and  otherwise?,\sum \frac{a_n}{1+n a_n} a_n = \frac{1}{\sqrt{n}} n a_n = \frac{1}{n^2},"Given the sequence $\left\{ a_n \right\}$, where  $$a_n = \begin{cases} \frac{1}{\sqrt{n}} \ \mbox{ if } n \mbox{ is a perfect square} \\ \frac{1}{n^2} \ \mbox{ otherwise}, \end{cases}$$  does the series $$\sum \frac{a_n}{1 + n a_n}$$ converge or diverge? My effort: If $n = m^2$, then we have  $$\frac{a_n}{1+na_n} = \frac{ \frac{1}{m} }{ 1 + m^2 \frac{1}{m} } = \frac{1}{m(m+1)} = \frac{1}{\sqrt{n} (1 + \sqrt{n})}, $$  and otherwise  $$ \frac{a_n}{1+na_n} = \frac{ \frac{1}{n^2}}{1+ n \frac{1}{n^2}} = \frac{1}{n^2 + n}. $$  Thus we have  $$ \frac{a_n}{1+na_n} = \begin{cases} \frac{1}{\sqrt{n} (1 + \sqrt{n})} \ \mbox{ if } n \mbox{ is a perfect square} \\ \frac{1}{n( 1 + n)} \ \mbox{ otherwise}. \end{cases} $$ What next? How to proceed from here? An afterthought: If $n$ is a perfect square, then we note that  $$\frac{a_n}{1+na_n} = \frac{1}{\sqrt{n}(1+ \sqrt{n})} \geq \frac{1}{2n},$$","Given the sequence $\left\{ a_n \right\}$, where  $$a_n = \begin{cases} \frac{1}{\sqrt{n}} \ \mbox{ if } n \mbox{ is a perfect square} \\ \frac{1}{n^2} \ \mbox{ otherwise}, \end{cases}$$  does the series $$\sum \frac{a_n}{1 + n a_n}$$ converge or diverge? My effort: If $n = m^2$, then we have  $$\frac{a_n}{1+na_n} = \frac{ \frac{1}{m} }{ 1 + m^2 \frac{1}{m} } = \frac{1}{m(m+1)} = \frac{1}{\sqrt{n} (1 + \sqrt{n})}, $$  and otherwise  $$ \frac{a_n}{1+na_n} = \frac{ \frac{1}{n^2}}{1+ n \frac{1}{n^2}} = \frac{1}{n^2 + n}. $$  Thus we have  $$ \frac{a_n}{1+na_n} = \begin{cases} \frac{1}{\sqrt{n} (1 + \sqrt{n})} \ \mbox{ if } n \mbox{ is a perfect square} \\ \frac{1}{n( 1 + n)} \ \mbox{ otherwise}. \end{cases} $$ What next? How to proceed from here? An afterthought: If $n$ is a perfect square, then we note that  $$\frac{a_n}{1+na_n} = \frac{1}{\sqrt{n}(1+ \sqrt{n})} \geq \frac{1}{2n},$$",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
43,Vector field and differential equation,Vector field and differential equation,,"We consider from $\mathbb{R}^2$ \ $\left \{ 0 \right \}$ the vector field $$X(x,y)=\left ( \dfrac{x}{x^2+y^2},\dfrac{y}{x^2+y^2} \right )$$ How to show that the differential equation $\dot{\gamma }(t)=X(\gamma (t))$ to any initial condition $p \in \mathbb{R}^2$ \ $\left \{ 0 \right \}$ is a curve solution to $\gamma: [0,+\infty) \to \mathbb{R}^2 $ \ $\left \{ 0 \right \}$ with $\gamma(0)=p$. And is this also true if we replace X by -X?","We consider from $\mathbb{R}^2$ \ $\left \{ 0 \right \}$ the vector field $$X(x,y)=\left ( \dfrac{x}{x^2+y^2},\dfrac{y}{x^2+y^2} \right )$$ How to show that the differential equation $\dot{\gamma }(t)=X(\gamma (t))$ to any initial condition $p \in \mathbb{R}^2$ \ $\left \{ 0 \right \}$ is a curve solution to $\gamma: [0,+\infty) \to \mathbb{R}^2 $ \ $\left \{ 0 \right \}$ with $\gamma(0)=p$. And is this also true if we replace X by -X?",,['analysis']
44,Alternate proof of Baby Rudin Theorem 6.9,Alternate proof of Baby Rudin Theorem 6.9,,"Theorem 6.9 of Baby Rudin states:  if $f$ is monotonic on $[a,b]$ and if $α$ is continuous then $f \in \mathscr R(\alpha)$ ($α$ is assumed monotonic). Here is a try at an alternate proof of above theorem in case f is monotonic increasing or decreasing: Let $f$ be monotonic increasing. Let $P$ be a partition of $[a,b]$ with $a=x_0<x_1<x_2<\cdots<x_n=b\\U(P,f,\alpha)-L(P,f,\alpha)=\sum_{i=1}^n (f_i^M-f_i^m)(\alpha(x_{i+1})-a(x_i))\\=\sum_{i=1}^n(f(x_{i+1})-f(x_i))(a(x_{i+1})-a(x_i))$ The last step follows from $f$ being monotonic increasing. So $U(P,f,\alpha)-L(P,f,\alpha)=U(P,\alpha,f)-L(P,\alpha,f)$ Now from Theorem 6.8 of baby rudin $\alpha \in \mathscr R(f)$, so that a for any arbitrary $\epsilon > 0$ partition $P$ can always be found so that $U(P,\alpha,f)-L(P,\alpha,f)<\epsilon$. The same partition will do for $f$ with respect to $\alpha$. Any comments appreciated.","Theorem 6.9 of Baby Rudin states:  if $f$ is monotonic on $[a,b]$ and if $α$ is continuous then $f \in \mathscr R(\alpha)$ ($α$ is assumed monotonic). Here is a try at an alternate proof of above theorem in case f is monotonic increasing or decreasing: Let $f$ be monotonic increasing. Let $P$ be a partition of $[a,b]$ with $a=x_0<x_1<x_2<\cdots<x_n=b\\U(P,f,\alpha)-L(P,f,\alpha)=\sum_{i=1}^n (f_i^M-f_i^m)(\alpha(x_{i+1})-a(x_i))\\=\sum_{i=1}^n(f(x_{i+1})-f(x_i))(a(x_{i+1})-a(x_i))$ The last step follows from $f$ being monotonic increasing. So $U(P,f,\alpha)-L(P,f,\alpha)=U(P,\alpha,f)-L(P,\alpha,f)$ Now from Theorem 6.8 of baby rudin $\alpha \in \mathscr R(f)$, so that a for any arbitrary $\epsilon > 0$ partition $P$ can always be found so that $U(P,\alpha,f)-L(P,\alpha,f)<\epsilon$. The same partition will do for $f$ with respect to $\alpha$. Any comments appreciated.",,"['real-analysis', 'analysis']"
45,"Real Analysis, Folland Problem 6.1.16 $L^p$ spaces","Real Analysis, Folland Problem 6.1.16  spaces",L^p,"Problem 6.1.16 - If $0 < p < 1$, the formula $\rho(f,g) = \int |f-g|^p$ defines a metric on $L^p$ that makes $L^p$ into a complete topological vector space. Attempted proof - Suppose $a,b > 0$ and $0 < p < 1$. For $t > 0$ we have $t^{p-1} > (a + t)^{p-1}$, and by integrating from $0$ to $b$ we obtain $a^p + b^p > (a+b)^p$. Let $E$ and $F$ be disjoint sets of positive finite measure in $X$ and set $a = \mu(E)^{1/p}$ and $b = \mu(F)^{1/p}$ we see that $$\|1_{E} + 1_{F}\|_{p} = (a^p + b^p) > a + b = \|1_{E}\|_{p} + \|1_{F}\|_{p}$$ I am not sure where to go from here, any suggestions is greatly appreciated.","Problem 6.1.16 - If $0 < p < 1$, the formula $\rho(f,g) = \int |f-g|^p$ defines a metric on $L^p$ that makes $L^p$ into a complete topological vector space. Attempted proof - Suppose $a,b > 0$ and $0 < p < 1$. For $t > 0$ we have $t^{p-1} > (a + t)^{p-1}$, and by integrating from $0$ to $b$ we obtain $a^p + b^p > (a+b)^p$. Let $E$ and $F$ be disjoint sets of positive finite measure in $X$ and set $a = \mu(E)^{1/p}$ and $b = \mu(F)^{1/p}$ we see that $$\|1_{E} + 1_{F}\|_{p} = (a^p + b^p) > a + b = \|1_{E}\|_{p} + \|1_{F}\|_{p}$$ I am not sure where to go from here, any suggestions is greatly appreciated.",,"['analysis', 'lp-spaces']"
46,Lower bound on function given lower bound on integral,Lower bound on function given lower bound on integral,,"If we are given a continuous function $f : [0,1]\to[0,1] $ and $\int_0^1 f dx > \varepsilon$ then can we put a lower bound on the function on some finite union of disjoint intervals such that the sum of their lengths and the bound on the function depend solely on $\varepsilon $. That is, can we find $\delta $ and $a $ as functions of $\varepsilon $ such that there exists a set of $n$ intervals, $\sum_1^n b_i-a_i > \delta $ and $f>a $ at every point of these intervals.","If we are given a continuous function $f : [0,1]\to[0,1] $ and $\int_0^1 f dx > \varepsilon$ then can we put a lower bound on the function on some finite union of disjoint intervals such that the sum of their lengths and the bound on the function depend solely on $\varepsilon $. That is, can we find $\delta $ and $a $ as functions of $\varepsilon $ such that there exists a set of $n$ intervals, $\sum_1^n b_i-a_i > \delta $ and $f>a $ at every point of these intervals.",,"['real-analysis', 'analysis']"
47,"Limit of $n a_n$ for positive, decreasing $(a_n)$ and convergent $\sum a_n$","Limit of  for positive, decreasing  and convergent",n a_n (a_n) \sum a_n,"I have the following question, and a hint but I am not sure how to go with it. I have an intuition that tells me it's right but I don't know how to prove it. Is it somehow related to Cauchy's condensation test ?","I have the following question, and a hint but I am not sure how to go with it. I have an intuition that tells me it's right but I don't know how to prove it. Is it somehow related to Cauchy's condensation test ?",,"['real-analysis', 'sequences-and-series', 'analysis']"
48,There exists strictly increasing $\{x_n\}$ that converges to $\sup E$,There exists strictly increasing  that converges to,\{x_n\} \sup E,"I need to prove that, if $E \subseteq \mathbb{R}$ is a non-empty bounded set and $\sup E \not\in E$ then there exists a strictly increasing sequence $\{x_n\}$ that converges to $\sup E$ such that $x_n \in E$ for all $n \in \mathbb{N}$. I've been trying to find a clue in the textbook, but couldn't. I don't even know how to start the proof. Could someone please give a clue?","I need to prove that, if $E \subseteq \mathbb{R}$ is a non-empty bounded set and $\sup E \not\in E$ then there exists a strictly increasing sequence $\{x_n\}$ that converges to $\sup E$ such that $x_n \in E$ for all $n \in \mathbb{N}$. I've been trying to find a clue in the textbook, but couldn't. I don't even know how to start the proof. Could someone please give a clue?",,"['real-analysis', 'analysis']"
49,Equi-continuous sequence of functions bounded at $0$ is uniformly bounded,Equi-continuous sequence of functions bounded at  is uniformly bounded,0,"Question: Suppose $f_n : [0,1] \rightarrow \mathbb{R}$ is an equi-continuous sequence of functions and suppose $\lvert f_n(0) \lvert \leq 1$ for all $n$. Then, $\{f_n\}$ is uniformly bounded. Work so far: Given $\varepsilon>0$, there exists a $\delta > 0$ such that for all $\lvert x \lvert <\delta$, we have: $$\lvert f_n(x) \lvert = \lvert f_n(x) - f_n(0) + f_n(0) \lvert \leq \lvert f_n(x) - f_n(0) \lvert + \lvert f_n(0) \lvert \leq \varepsilon + 1$$ However, this seems like it could potentially bound $\lvert f_n(x) \lvert$ for only a small portion of $[0,1]$, i.e. $x \in (-\delta,\delta)$. Is there some way to extend this to a cover of open intervals on which $\lvert f_n(x) \lvert$ is bounded, so that we can take advantage of the compactness of $[0,1]$? At this point I am a bit lost. Any direction would be much appreciated. Edit: The family of functions is uniformly equi-continuous on $[0,1]$.","Question: Suppose $f_n : [0,1] \rightarrow \mathbb{R}$ is an equi-continuous sequence of functions and suppose $\lvert f_n(0) \lvert \leq 1$ for all $n$. Then, $\{f_n\}$ is uniformly bounded. Work so far: Given $\varepsilon>0$, there exists a $\delta > 0$ such that for all $\lvert x \lvert <\delta$, we have: $$\lvert f_n(x) \lvert = \lvert f_n(x) - f_n(0) + f_n(0) \lvert \leq \lvert f_n(x) - f_n(0) \lvert + \lvert f_n(0) \lvert \leq \varepsilon + 1$$ However, this seems like it could potentially bound $\lvert f_n(x) \lvert$ for only a small portion of $[0,1]$, i.e. $x \in (-\delta,\delta)$. Is there some way to extend this to a cover of open intervals on which $\lvert f_n(x) \lvert$ is bounded, so that we can take advantage of the compactness of $[0,1]$? At this point I am a bit lost. Any direction would be much appreciated. Edit: The family of functions is uniformly equi-continuous on $[0,1]$.",,"['real-analysis', 'analysis']"
50,A Bound for the Error of the Numerical Approximation of a the Integral of a Continuous Function,A Bound for the Error of the Numerical Approximation of a the Integral of a Continuous Function,,"How to numerically integrate a nasty function? Suppose $f$ is only continuos; which method can you employ to approximate $$\int_0^t f(s)ds$$ Since $f$ is continuos the integral exists, but all the numerical approximation methods I studied bound the error term with the hypothesis that $f$ is at least $C^2$ or something. I also know of the left rectangle method that only requires $f$ to be holder-continuos for some $\alpha$, but suppose this $f$ is not even Holder continuos. Can we find a meaningful error bound?","How to numerically integrate a nasty function? Suppose $f$ is only continuos; which method can you employ to approximate $$\int_0^t f(s)ds$$ Since $f$ is continuos the integral exists, but all the numerical approximation methods I studied bound the error term with the hypothesis that $f$ is at least $C^2$ or something. I also know of the left rectangle method that only requires $f$ to be holder-continuos for some $\alpha$, but suppose this $f$ is not even Holder continuos. Can we find a meaningful error bound?",,"['real-analysis', 'integration', 'analysis', 'definite-integrals', 'numerical-methods']"
51,Let B ⊂ R. Let L be the set of all limit points of B. Prove that B ∪ L contains all its limit points.,Let B ⊂ R. Let L be the set of all limit points of B. Prove that B ∪ L contains all its limit points.,,"Let B ⊂ R. Let L be the set of all limit points of B. Prove that B ∪ L contains all its limit points. I have a question about this one. Here is my proof Suppose x is a limit point of B ∪ L and x does not belong to B ∪ L If x is a limit point of B ∪ L, then x is either limit point of B or limit point of L (or both) By assuming that x does not belong to B ∪ L x is not in B and x is not in L Here is where I am stuck and not sure whether this is right approach Can anyone give me some ideas?","Let B ⊂ R. Let L be the set of all limit points of B. Prove that B ∪ L contains all its limit points. I have a question about this one. Here is my proof Suppose x is a limit point of B ∪ L and x does not belong to B ∪ L If x is a limit point of B ∪ L, then x is either limit point of B or limit point of L (or both) By assuming that x does not belong to B ∪ L x is not in B and x is not in L Here is where I am stuck and not sure whether this is right approach Can anyone give me some ideas?",,"['real-analysis', 'analysis', 'elementary-set-theory']"
52,Set approximation by the balls,Set approximation by the balls,,"Let's consider a non-empty open set $A \subset \mathbb{R}^{n}$. How to prove that there exist a finite or a countable family of open pairwise disjoint balls $$\bigcup_{n \in I} {A_{n}}$$ $$ \forall n \in I,  A_{n} \subset A$$ so that $$m(A/ \bigcup{A_{n}}) = 0$$ where $m$ is  Lebesgue measure. Despite the fact that the by the light of the nature this fact seems to be correct, i failed in proceeding a rigorous proof. How to obtain it? The fact is extremely useful, for example, while proving that the Lebesgue outer measure is invariant under the rotations, since we can approximate the instant set not only by the  rectanges, but also by open balls, which are clearly invariant under the transformation. Any sort of help would be much appreciated.","Let's consider a non-empty open set $A \subset \mathbb{R}^{n}$. How to prove that there exist a finite or a countable family of open pairwise disjoint balls $$\bigcup_{n \in I} {A_{n}}$$ $$ \forall n \in I,  A_{n} \subset A$$ so that $$m(A/ \bigcup{A_{n}}) = 0$$ where $m$ is  Lebesgue measure. Despite the fact that the by the light of the nature this fact seems to be correct, i failed in proceeding a rigorous proof. How to obtain it? The fact is extremely useful, for example, while proving that the Lebesgue outer measure is invariant under the rotations, since we can approximate the instant set not only by the  rectanges, but also by open balls, which are clearly invariant under the transformation. Any sort of help would be much appreciated.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
53,Analysis: Showing a set can be bounded below in the reals,Analysis: Showing a set can be bounded below in the reals,,"How do I go about showing the set $A=\left\{x^2+6x+6 : x\in\mathbb{R}\right\}$ is bounded below in the reals. Further,how may I go about finding the greatest lower bound for $A$? Thank you in advance","How do I go about showing the set $A=\left\{x^2+6x+6 : x\in\mathbb{R}\right\}$ is bounded below in the reals. Further,how may I go about finding the greatest lower bound for $A$? Thank you in advance",,['analysis']
54,"Real Analysis, Cauchy but not null.","Real Analysis, Cauchy but not null.",,"I came across this question in a book on p-adic numbers and thought it looked interesting. However, I am having trouble getting started with it. Any hints/suggestions is much welcomed Let $(a_n)$ be Cauchy but not null. Show there exists a $c>0$ and $N>0$ such that $||a_n||>c$ when $n>N$.","I came across this question in a book on p-adic numbers and thought it looked interesting. However, I am having trouble getting started with it. Any hints/suggestions is much welcomed Let $(a_n)$ be Cauchy but not null. Show there exists a $c>0$ and $N>0$ such that $||a_n||>c$ when $n>N$.",,"['sequences-and-series', 'analysis']"
55,"If $f:\mathbb R^n\to\mathbb R$ is twice continuously differentiable, then $\nabla f$ is Lipschitz continuous","If  is twice continuously differentiable, then  is Lipschitz continuous",f:\mathbb R^n\to\mathbb R \nabla f,"Let $f\in C^2(\mathbb R^n)$. I've read that since $f$ is twice continuously differentiable, $\nabla f$ is Lipschitz continuous. Is that really true? By the mean-value theorem, $$\left\|\nabla f(x)-\nabla f(y)\right\|\le\left\|\int_0^1\nabla^2f(y+t(x-y))\;dt\right\|\left\|x-y\right\|\le\underbrace{\sup_{t\in[0,1]}\left\|\nabla^2f(y+t(x-y))\right\|}_{=:L(x,y)}\left\|x-y\right\|\tag 1$$ for any operator norm $\left\|\;\cdot\;\right\|$ and $x,y\in\mathbb R^n$. However, $L$ doesn't seem to be uniformly bounded. So, I assume that we cannot prove Lipschitz continuity without making further assumptions. In the book that I read, they further assume that there is a $x^*\in\mathbb R^n$ with $\nabla f(x^*)=0$ such that $\nabla^2f(x^*)$ is invertible. This implies that there is a $\varepsilon>0$ such that $\nabla^2f(x)$ is invertible, too, and $$\left\|\nabla^2f(x)^{-1}\right\|\le C\;,$$ for some $C>0$, for all $x$ in the (open) ball $B_\varepsilon(x^*)$. Is this somehow helpful?","Let $f\in C^2(\mathbb R^n)$. I've read that since $f$ is twice continuously differentiable, $\nabla f$ is Lipschitz continuous. Is that really true? By the mean-value theorem, $$\left\|\nabla f(x)-\nabla f(y)\right\|\le\left\|\int_0^1\nabla^2f(y+t(x-y))\;dt\right\|\left\|x-y\right\|\le\underbrace{\sup_{t\in[0,1]}\left\|\nabla^2f(y+t(x-y))\right\|}_{=:L(x,y)}\left\|x-y\right\|\tag 1$$ for any operator norm $\left\|\;\cdot\;\right\|$ and $x,y\in\mathbb R^n$. However, $L$ doesn't seem to be uniformly bounded. So, I assume that we cannot prove Lipschitz continuity without making further assumptions. In the book that I read, they further assume that there is a $x^*\in\mathbb R^n$ with $\nabla f(x^*)=0$ such that $\nabla^2f(x^*)$ is invertible. This implies that there is a $\varepsilon>0$ such that $\nabla^2f(x)$ is invertible, too, and $$\left\|\nabla^2f(x)^{-1}\right\|\le C\;,$$ for some $C>0$, for all $x$ in the (open) ball $B_\varepsilon(x^*)$. Is this somehow helpful?",,"['calculus', 'analysis', 'multivariable-calculus', 'continuity', 'lipschitz-functions']"
56,"If $\langle f'(x) \cdot v , v \rangle > 0$ then $f$ is injective",If  then  is injective,"\langle f'(x) \cdot v , v \rangle > 0 f","Question: Let $f: U \to \mathbb R^m$ differentiable at the convex set $U \subseteq \mathbb R^m$. If $$\langle f'(x) \cdot v , v \rangle > 0 , \,\,\, \forall\,\, x \in U, v \neq 0 \in \mathbb R^m $$   then $f$ is injective. If $f \in C^1$ then $f$ is a diffeomorphism of $U$ over a subset of $\mathbb R^m$. Give an example such that $U = \mathbb R^m$, but $f$ is not surjective. Attempt: The idea is to show $$|f(x+v) - f(x)| > 0$$ As $U$ is convex and $f$ id differentiable in $U$ then $[x,x+v] \subseteq U$ for any $x, x + v \in U$, by the Mean Value Theorem there exists $\theta \in (0,1)$ such that $$f(x + v) - f(x) = \frac{\partial f}{\partial v}(x + \theta v) = f'(x + \theta v) \cdot v$$ Then $$\langle f(x + v) - f(x) , v\rangle  = \langle f'(x + \theta v) \cdot v , v\rangle > 0 $$ and I couldn't conclude anything. The second part is o.k. Any thoughts? Edit: I  can't use the Mean Value Theorem here.","Question: Let $f: U \to \mathbb R^m$ differentiable at the convex set $U \subseteq \mathbb R^m$. If $$\langle f'(x) \cdot v , v \rangle > 0 , \,\,\, \forall\,\, x \in U, v \neq 0 \in \mathbb R^m $$   then $f$ is injective. If $f \in C^1$ then $f$ is a diffeomorphism of $U$ over a subset of $\mathbb R^m$. Give an example such that $U = \mathbb R^m$, but $f$ is not surjective. Attempt: The idea is to show $$|f(x+v) - f(x)| > 0$$ As $U$ is convex and $f$ id differentiable in $U$ then $[x,x+v] \subseteq U$ for any $x, x + v \in U$, by the Mean Value Theorem there exists $\theta \in (0,1)$ such that $$f(x + v) - f(x) = \frac{\partial f}{\partial v}(x + \theta v) = f'(x + \theta v) \cdot v$$ Then $$\langle f(x + v) - f(x) , v\rangle  = \langle f'(x + \theta v) \cdot v , v\rangle > 0 $$ and I couldn't conclude anything. The second part is o.k. Any thoughts? Edit: I  can't use the Mean Value Theorem here.",,"['analysis', 'multivariable-calculus']"
57,Show that if $\int_0^x f(y)dy \sim Ax^\alpha$ then $f(x)\sim \alpha Ax^{\alpha -1}$,Show that if  then,\int_0^x f(y)dy \sim Ax^\alpha f(x)\sim \alpha Ax^{\alpha -1},"Let $f$ be a real, continuous function defined on $[0,\infty)$ such that $xf(x)$ is increasing for all sufficiently large values of $x$. Show that if $$\int_0^x f(y)\,dy \sim Ax^\alpha \quad \left(\,x\to \infty\right)$$ for some positive constants $A$ and $\alpha$, then $$f(x)\sim \alpha Ax^{\alpha -1} \quad \left(\,x\to \infty\right).$$ Clearly, I have to use differentiation somewhere, but I don't know how to manipulate $\lim_{x\to \infty}\frac{\int_0^x f(y)\,dy}{Ax^\alpha}=1$ to get the desired result. From suggestions given, I know that by L'hospital's rule, it's enough to show that the limit $\lim \frac{f(x)}{\alpha Ax^{\alpha -1}}$ exists, and this limit equals $ \lim \frac{xf(x)}{\alpha Ax^{\alpha}}$. From here, I'll need to use the given assumption that $xf(x)$ is eventually increasing. But how can I show the existence of the limit based on these facts? I would greatly appreciate any solutions, hints or suggestions.","Let $f$ be a real, continuous function defined on $[0,\infty)$ such that $xf(x)$ is increasing for all sufficiently large values of $x$. Show that if $$\int_0^x f(y)\,dy \sim Ax^\alpha \quad \left(\,x\to \infty\right)$$ for some positive constants $A$ and $\alpha$, then $$f(x)\sim \alpha Ax^{\alpha -1} \quad \left(\,x\to \infty\right).$$ Clearly, I have to use differentiation somewhere, but I don't know how to manipulate $\lim_{x\to \infty}\frac{\int_0^x f(y)\,dy}{Ax^\alpha}=1$ to get the desired result. From suggestions given, I know that by L'hospital's rule, it's enough to show that the limit $\lim \frac{f(x)}{\alpha Ax^{\alpha -1}}$ exists, and this limit equals $ \lim \frac{xf(x)}{\alpha Ax^{\alpha}}$. From here, I'll need to use the given assumption that $xf(x)$ is eventually increasing. But how can I show the existence of the limit based on these facts? I would greatly appreciate any solutions, hints or suggestions.",,"['real-analysis', 'analysis', 'asymptotics']"
58,Equality with dilogarithms,Equality with dilogarithms,,"During some calculations with definite integrals I happened to get the equality  \begin{eqnarray} 2\, \textrm{Li}_2(-\frac{1}{2}) - 2 \, \textrm{Li}_2(\frac{1}{4})+ 2\, \textrm{Li}_2(\frac{2}{3})=  3 \log^2 2 - \log^2 3 \end{eqnarray} Does this follow from some well known equalities for dilogarithms ?","During some calculations with definite integrals I happened to get the equality  \begin{eqnarray} 2\, \textrm{Li}_2(-\frac{1}{2}) - 2 \, \textrm{Li}_2(\frac{1}{4})+ 2\, \textrm{Li}_2(\frac{2}{3})=  3 \log^2 2 - \log^2 3 \end{eqnarray} Does this follow from some well known equalities for dilogarithms ?",,"['analysis', 'special-functions', 'polylogarithm']"
59,Hessian-Matrix positive definite $\iff$ $a$ local minimum?,Hessian-Matrix positive definite   local minimum?,\iff a,"It is commonly known that if $f$ is twice differentiable, $\nabla f(a) = 0$ and  $H_f(a)$ positive definite, $a$ is a local minimum. So, in short: $H_f(a)$ positive definite $ \implies $ $a$ local minimum But what about "" $\impliedby $ "" ? What can I say about the Hessian-Matrix, when I know $a$ is a local minimum? Do cases exist, where $a$ is a local minimum and the Hessian-Matrix is not positive definite ?","It is commonly known that if $f$ is twice differentiable, $\nabla f(a) = 0$ and  $H_f(a)$ positive definite, $a$ is a local minimum. So, in short: $H_f(a)$ positive definite $ \implies $ $a$ local minimum But what about "" $\impliedby $ "" ? What can I say about the Hessian-Matrix, when I know $a$ is a local minimum? Do cases exist, where $a$ is a local minimum and the Hessian-Matrix is not positive definite ?",,"['real-analysis', 'analysis', 'optimization']"
60,On a condition when bounded sets in $\mathbb R^n$ is convex ?,On a condition when bounded sets in  is convex ?,\mathbb R^n,"Is it true that a bounded set in $\mathbb R^n$ , $n>1$ , is convex iff every straight line  through an arbitrary interior point of the set intersects the boundary of the set in exactly two points ? I can view it geometrically , I think it is true , but am not able to write down a formal proof . Please help .","Is it true that a bounded set in $\mathbb R^n$ , $n>1$ , is convex iff every straight line  through an arbitrary interior point of the set intersects the boundary of the set in exactly two points ? I can view it geometrically , I think it is true , but am not able to write down a formal proof . Please help .",,['analysis']
61,A continuously differentiable bijection implies its inverse is Lipschitz continuous,A continuously differentiable bijection implies its inverse is Lipschitz continuous,,"Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a continuously differentiable bijection. Does this imply that $f^{-1}$ is Lipschitz continuous? (of course, not globally, take for instance $f(x)=x^3$) If not, what if $f\in C^2$ in addition? I tried stupid things like... Fix an interval $[a,b]$ and pick $x,y\in [a,b]$. Then $$|x-y|= |f(f^{-1}(x)) - f(f^{-1}(y))|\leq \|f'\|_{\infty} |f^{-1}(x)-f^{-1}(y)|$$ where $\|f'\|_{\infty} := \sup_{x\in [a,b]} |f'(x)|$. But I don't see how to conclude from this. Thanks!","Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a continuously differentiable bijection. Does this imply that $f^{-1}$ is Lipschitz continuous? (of course, not globally, take for instance $f(x)=x^3$) If not, what if $f\in C^2$ in addition? I tried stupid things like... Fix an interval $[a,b]$ and pick $x,y\in [a,b]$. Then $$|x-y|= |f(f^{-1}(x)) - f(f^{-1}(y))|\leq \|f'\|_{\infty} |f^{-1}(x)-f^{-1}(y)|$$ where $\|f'\|_{\infty} := \sup_{x\in [a,b]} |f'(x)|$. But I don't see how to conclude from this. Thanks!",,"['calculus', 'real-analysis', 'analysis', 'continuity', 'lipschitz-functions']"
62,Does the polynomial have n distinct real roots,Does the polynomial have n distinct real roots,,"Let $a_1,a_2,a_3...a_n$ be real numbers such that the polynomial $p(x)=x^n+a_1x^{n-1}+...+ a_{n-1}x+a_n$ has n distinct real roots. Does there exist $\epsilon$ >0 such that for all $b_1,b_2...b_n \in R$ with the property that $|a_j-b_j|<\epsilon$, for all j=1,2,...n the polynomial $q(x) = x^n+b_1x^{n-1}+...+b_{n-1}x+b_n$ has n distinct real roots? I am momentarily studying integration in $R^n$ but I couldn't make any relation to this one.Maybe it is off topic related to integration. But how do I prove it?","Let $a_1,a_2,a_3...a_n$ be real numbers such that the polynomial $p(x)=x^n+a_1x^{n-1}+...+ a_{n-1}x+a_n$ has n distinct real roots. Does there exist $\epsilon$ >0 such that for all $b_1,b_2...b_n \in R$ with the property that $|a_j-b_j|<\epsilon$, for all j=1,2,...n the polynomial $q(x) = x^n+b_1x^{n-1}+...+b_{n-1}x+b_n$ has n distinct real roots? I am momentarily studying integration in $R^n$ but I couldn't make any relation to this one.Maybe it is off topic related to integration. But how do I prove it?",,['analysis']
63,Integration with 2-forms,Integration with 2-forms,,"Wikipedia says: Let $$ \omega=f_{z}\, \mathrm dx \wedge \mathrm dy + f_{x}\, \mathrm dy \wedge \mathrm dz + f_{y}\, \mathrm dz  \wedge \mathrm dx $$ be a 2-form on a surface with parametrization  $$\mathbf{x} (s,t)=( x(s,t), y(s,t), z(s,t))\!$$ defined on some domain $D.$ Then, the surface integral of the two-form on the surface $S$ is given by $$  \int_{S} \omega = \int_D \left[ f_{z} ( \mathbf{x} (s,t)) \frac{\partial(x,y)}{\partial(s,t)} + f_{x} ( \mathbf{x} (s,t))\frac{\partial(y,z)}{\partial(s,t)} + f_{y} ( \mathbf{x} (s,t))\frac{\partial(z,x)}{\partial(s,t)} \right]\, \mathrm ds\, \mathrm dt$$ Now my question is: Is this the same as $$\int_{D} \omega(\mathbf{x}(s,t))(\frac{\partial \mathbf{x}}{\partial s}(s,t),\frac{\partial \mathbf{x}}{\partial t}(s,t)) ds dt ?$$ If anything is unclear, please let me know.","Wikipedia says: Let $$ \omega=f_{z}\, \mathrm dx \wedge \mathrm dy + f_{x}\, \mathrm dy \wedge \mathrm dz + f_{y}\, \mathrm dz  \wedge \mathrm dx $$ be a 2-form on a surface with parametrization  $$\mathbf{x} (s,t)=( x(s,t), y(s,t), z(s,t))\!$$ defined on some domain $D.$ Then, the surface integral of the two-form on the surface $S$ is given by $$  \int_{S} \omega = \int_D \left[ f_{z} ( \mathbf{x} (s,t)) \frac{\partial(x,y)}{\partial(s,t)} + f_{x} ( \mathbf{x} (s,t))\frac{\partial(y,z)}{\partial(s,t)} + f_{y} ( \mathbf{x} (s,t))\frac{\partial(z,x)}{\partial(s,t)} \right]\, \mathrm ds\, \mathrm dt$$ Now my question is: Is this the same as $$\int_{D} \omega(\mathbf{x}(s,t))(\frac{\partial \mathbf{x}}{\partial s}(s,t),\frac{\partial \mathbf{x}}{\partial t}(s,t)) ds dt ?$$ If anything is unclear, please let me know.",,"['real-analysis', 'analysis', 'manifolds', 'lebesgue-integral', 'differential-forms']"
64,"Why sum of two little ""o"" notation is equal little ""o"" notation from sum?","Why sum of two little ""o"" notation is equal little ""o"" notation from sum?",,"Why sum of two little ""o"" notation is equal little ""o"" notation from sum? $o( f(n) ) + o( g(n) ) = o( f(n) + g(n) ) ?$ For example: $f(n) = n^3$ $g(n) = 1/n$ so $o(f(n)) = n^2$ $o(g(n)) = 1/n^2$ and $o( f(n) ) + o( g(n) ) = n^2 + 1/n^2$ $o( f(n) + g(n) ) = n^2$ Of course, I could write it like $o( f(n) ) + o( g(n) ) = n^2 + o( g(n) )$ $o( f(n) + g(n) ) = n^2 + o( g(n) )$ My question is why? I don't understand it, because in first we always get two parameters.","Why sum of two little ""o"" notation is equal little ""o"" notation from sum? $o( f(n) ) + o( g(n) ) = o( f(n) + g(n) ) ?$ For example: $f(n) = n^3$ $g(n) = 1/n$ so $o(f(n)) = n^2$ $o(g(n)) = 1/n^2$ and $o( f(n) ) + o( g(n) ) = n^2 + 1/n^2$ $o( f(n) + g(n) ) = n^2$ Of course, I could write it like $o( f(n) ) + o( g(n) ) = n^2 + o( g(n) )$ $o( f(n) + g(n) ) = n^2 + o( g(n) )$ My question is why? I don't understand it, because in first we always get two parameters.",,"['analysis', 'notation']"
65,Trying to prove Tietze extension theorem,Trying to prove Tietze extension theorem,,"I am trying to prove Tietze extension theorem for metric spaces that is "" If $X$ is a metric space , $F$ is a closed set in $X$ and $f:F \to [0,1]$ is a continuous function , then there is a continuous function $g:X \to \mathbb R$ such that $g(x)=f(x) , \forall x \in F$ "" . I have seen the proof that uses uniformly convergent sequence of functions $\{g_n\}$ and define the extension as the limit functions , but I quite don't like this proof . I saw the proof which defines the extension as $g(x)=f(x) , \forall x\in F$ and $g(x)=\inf\{f(a)+\dfrac{d(x,a)}{dist (x,F)} -1:a \in  F\} , \forall x \in X \setminus F$ , but I ma unable to prove that this $g$ is continuous on $X$ . Please help in this proof . Also is there any other proof of the extension theorem ? Thanks in advance","I am trying to prove Tietze extension theorem for metric spaces that is "" If $X$ is a metric space , $F$ is a closed set in $X$ and $f:F \to [0,1]$ is a continuous function , then there is a continuous function $g:X \to \mathbb R$ such that $g(x)=f(x) , \forall x \in F$ "" . I have seen the proof that uses uniformly convergent sequence of functions $\{g_n\}$ and define the extension as the limit functions , but I quite don't like this proof . I saw the proof which defines the extension as $g(x)=f(x) , \forall x\in F$ and $g(x)=\inf\{f(a)+\dfrac{d(x,a)}{dist (x,F)} -1:a \in  F\} , \forall x \in X \setminus F$ , but I ma unable to prove that this $g$ is continuous on $X$ . Please help in this proof . Also is there any other proof of the extension theorem ? Thanks in advance",,['analysis']
66,For what values does $x^{x^{x^{x^{.^{.^{.}}}}}}$ make sense [duplicate],For what values does  make sense [duplicate],x^{x^{x^{x^{.^{.^{.}}}}}},"This question already has answers here : Infinite tetration, convergence radius (3 answers) Closed 9 years ago . For which values of $x\ge 1$ does the expression $x^{x^{x^{x^{.^{.^{.}}}}}}$ make sense? To tackle this, define  $f_1(x)=x$ and $f_{n+1}(x)=x^{f_n(x)}$ for $x \ge 1$ and $n\ge1$. a) Show that $f_{n+1}(x) \ge f_n(x)$ for all $n\ge1$. b) When $L(x) = \lim_{n\to\infty} f_n(x)$ exists, find an equation for $L(x)$. Use it to find an upper bound for $x$. c) For these values of $x$, show by induction that $f_n(x)$ is bounded above by $e$ for all $n\ge1$. What can you conclude? d) What happens for larger $x$? I'm having trouble showing it is increasing for a). I solved b) with the fact that $\lim_{n\to\infty}f_n(x)=L(x)=x^{L(x)}\implies x=L(x)^{\frac{1}{L(x)}}$ Maximizing on $L>0$: $$\frac{d}{dL}L^{1/L}=\frac{d}{dL}e^\frac{\ln(L)}{L}=\frac{d}{dL}\left(\frac{\ln(L)}{L}\right)e^\frac{\ln(L)}{L}$$ $$=\frac{1-\ln(L)}{L^2}L^{\frac{1}{L}}=L^{\frac{1}{L}-2}(1-\ln(L))$$ $$=-L^{\frac{1}{L}-2}(\ln(L)-1)=0\iff\ln(L)-1=0 \text{ so } L=e$$ Therefore $\max\{L^{1/L}\}$ happens when $L=e$ so $x\le e^{1/e}$ and I now have bounds for $x$ It then follows for c): Since $f_n$ is increasing on $[1,e^{1/e}], \max\{f_n(x)\}=f_n(e^{1/e})$ Base case: $f_1(x)=x\le e^{1/e}<e$. Assume $f_n(x)<e$, consider $x^{f_n(x)}\le x^e$ $\implies f_{n+1}(x)\le f_{n+1}(e^{1/e})<x^e<(e^{1/e})^e=e$ therefore by principle of mathematical induction, $f_n(x)<e$ for $x\in[1,e^{1/e}]$","This question already has answers here : Infinite tetration, convergence radius (3 answers) Closed 9 years ago . For which values of $x\ge 1$ does the expression $x^{x^{x^{x^{.^{.^{.}}}}}}$ make sense? To tackle this, define  $f_1(x)=x$ and $f_{n+1}(x)=x^{f_n(x)}$ for $x \ge 1$ and $n\ge1$. a) Show that $f_{n+1}(x) \ge f_n(x)$ for all $n\ge1$. b) When $L(x) = \lim_{n\to\infty} f_n(x)$ exists, find an equation for $L(x)$. Use it to find an upper bound for $x$. c) For these values of $x$, show by induction that $f_n(x)$ is bounded above by $e$ for all $n\ge1$. What can you conclude? d) What happens for larger $x$? I'm having trouble showing it is increasing for a). I solved b) with the fact that $\lim_{n\to\infty}f_n(x)=L(x)=x^{L(x)}\implies x=L(x)^{\frac{1}{L(x)}}$ Maximizing on $L>0$: $$\frac{d}{dL}L^{1/L}=\frac{d}{dL}e^\frac{\ln(L)}{L}=\frac{d}{dL}\left(\frac{\ln(L)}{L}\right)e^\frac{\ln(L)}{L}$$ $$=\frac{1-\ln(L)}{L^2}L^{\frac{1}{L}}=L^{\frac{1}{L}-2}(1-\ln(L))$$ $$=-L^{\frac{1}{L}-2}(\ln(L)-1)=0\iff\ln(L)-1=0 \text{ so } L=e$$ Therefore $\max\{L^{1/L}\}$ happens when $L=e$ so $x\le e^{1/e}$ and I now have bounds for $x$ It then follows for c): Since $f_n$ is increasing on $[1,e^{1/e}], \max\{f_n(x)\}=f_n(e^{1/e})$ Base case: $f_1(x)=x\le e^{1/e}<e$. Assume $f_n(x)<e$, consider $x^{f_n(x)}\le x^e$ $\implies f_{n+1}(x)\le f_{n+1}(e^{1/e})<x^e<(e^{1/e})^e=e$ therefore by principle of mathematical induction, $f_n(x)<e$ for $x\in[1,e^{1/e}]$",,"['sequences-and-series', 'analysis', 'tetration']"
67,Getting a specific formula for a sequence.,Getting a specific formula for a sequence.,,"If: $$a_0 = \frac{5}{2}, a_k = a_{k-1}^{2} - 2$$ for $k \ge 1$. How do I get a general formula for $a_k$? With induction proof. I even tried calculating $a_1, a_2 ...$: $$a_0 = \frac{5}{2}$$ $$a_1 = \frac{17}{4}$$ $$a_2 = \frac{273}{16}$$ $$a_3 = \frac{74017}{256}$$ I will treat the numerator and denominator seperately. I see that for the denominator. $$d = 2^{2^k}$$ Now to the numerator: I cant get it. I tried, $$2^{2^{k}} + 1$$ but it doesnt work for $k=2$. But it is shifted $1$, meaning for $n=2$, I got the value of $n=1$.","If: $$a_0 = \frac{5}{2}, a_k = a_{k-1}^{2} - 2$$ for $k \ge 1$. How do I get a general formula for $a_k$? With induction proof. I even tried calculating $a_1, a_2 ...$: $$a_0 = \frac{5}{2}$$ $$a_1 = \frac{17}{4}$$ $$a_2 = \frac{273}{16}$$ $$a_3 = \frac{74017}{256}$$ I will treat the numerator and denominator seperately. I see that for the denominator. $$d = 2^{2^k}$$ Now to the numerator: I cant get it. I tried, $$2^{2^{k}} + 1$$ but it doesnt work for $k=2$. But it is shifted $1$, meaning for $n=2$, I got the value of $n=1$.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'contest-math']"
68,Continuous function and dense set,Continuous function and dense set,,"Let $f: X \rightarrow Y $ and $g:X \rightarrow Y$ continuous functions and $(X,d_X),(Y,d_Y)$ metric space. Let $E$ be a dense subset in $X$ . Prove that $f(E)$ is a dense subset in $f(X)$ . If $f(p)=g(p)$ for all $p\in E$ , prove that $f(x)=g(x)$ for all $x\in X$ . My attempt: since $E \subset X \Rightarrow f(E)\subset f(X)$ and from the dense property we have $f(X) \subset f(\overline{E})$ and since $f$ is continuous, $f(X) \subset f(\overline{E}) \subset \overline{f(E)}$ . Then, $f(E)$ is dense in $f(X)$ . Suppose that $f(p)=g(p), \;  \forall p \in E$ . Hence, we have $$d_{Y}(f(x),g(x)) \leq d_{Y}(f(x),f(p)) + d_{Y}(f(p),g(x)) = d_{Y}(f(x),f(p)) + d_{Y}(g(p),g(x)).$$ By the fact that $f,g$ are continuous, there exist a $\delta_1$ and $\delta_2$ such that $$d_X(x,p) < \delta_1 \qquad d_X(x,p) < \delta_2$$ implies $$d_{Y}(f(x),f(p))<\epsilon /2 \qquad d_{Y}(g(p),g(x)) < \epsilon /2.$$ for all $\epsilon >0.$ Is this correct?","Let and continuous functions and metric space. Let be a dense subset in . Prove that is a dense subset in . If for all , prove that for all . My attempt: since and from the dense property we have and since is continuous, . Then, is dense in . Suppose that . Hence, we have By the fact that are continuous, there exist a and such that implies for all Is this correct?","f: X \rightarrow Y  g:X \rightarrow Y (X,d_X),(Y,d_Y) E X f(E) f(X) f(p)=g(p) p\in E f(x)=g(x) x\in X E \subset X \Rightarrow f(E)\subset f(X) f(X) \subset f(\overline{E}) f f(X) \subset f(\overline{E}) \subset \overline{f(E)} f(E) f(X) f(p)=g(p), \;  \forall p \in E d_{Y}(f(x),g(x)) \leq d_{Y}(f(x),f(p)) + d_{Y}(f(p),g(x)) = d_{Y}(f(x),f(p)) + d_{Y}(g(p),g(x)). f,g \delta_1 \delta_2 d_X(x,p) < \delta_1 \qquad d_X(x,p) < \delta_2 d_{Y}(f(x),f(p))<\epsilon /2 \qquad d_{Y}(g(p),g(x)) < \epsilon /2. \epsilon >0.","['analysis', 'metric-spaces']"
69,Find this sum $S$ using Real-analysis methods only,Find this sum  using Real-analysis methods only,S,"$$S = \sum_{k=1}^{\infty}\frac{2H_k}{(k+1)(k+2)^3}$$ I have tried a lot and failed, any help is appreciated. $H_k$ is the harmonic number. Thanks (real method only please)","$$S = \sum_{k=1}^{\infty}\frac{2H_k}{(k+1)(k+2)^3}$$ I have tried a lot and failed, any help is appreciated. $H_k$ is the harmonic number. Thanks (real method only please)",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'analysis']"
70,An inequality relating to a continuous and twice differentiable function,An inequality relating to a continuous and twice differentiable function,,"Suppose that $f$ is a continuous and twice differentiable function in $[0,1]$. Please show that $$ \int_0^1 \vert f'(x) \vert dx \leq 9\int_0^1 \vert f(x) \vert dx + \int_0^1 \vert f''(x) \vert dx  $$ Any idea? I just couldn't find the way.","Suppose that $f$ is a continuous and twice differentiable function in $[0,1]$. Please show that $$ \int_0^1 \vert f'(x) \vert dx \leq 9\int_0^1 \vert f(x) \vert dx + \int_0^1 \vert f''(x) \vert dx  $$ Any idea? I just couldn't find the way.",,"['calculus', 'real-analysis', 'analysis', 'inequality', 'definite-integrals']"
71,integral of a function,integral of a function,,"I wanted to find the integral of the function $f(x)$ from zero to one: $$f(x)=\begin{cases}2x\sin(1/x)-\cos(1/x) & : x\in(0,1]\\ 0 & :x=0\end{cases}$$ but I think whether its integral is not defined or is $\sin 1$. please help me with this question. Thanks.","I wanted to find the integral of the function $f(x)$ from zero to one: $$f(x)=\begin{cases}2x\sin(1/x)-\cos(1/x) & : x\in(0,1]\\ 0 & :x=0\end{cases}$$ but I think whether its integral is not defined or is $\sin 1$. please help me with this question. Thanks.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'definite-integrals']"
72,Continuous function on $\mathbb{R}^{n}$ preserving compactness - some clarification,Continuous function on  preserving compactness - some clarification,\mathbb{R}^{n},"My professor went over a proof of the following in class: Suppose $A \in \mathbb{R}^{n}$ is compact and $f:A \rightarrow \mathbb{R}^{n}$ is continuous. Then $f(A)$ is compact. The proof (presented by my professor in class) is as follows: Since $f$ is continuous, the set $ \{f^{-1}(U_{i})|i \in I \}$ is an open cover for $A$. Since $A$ is compact, each open cover has a finite sub cover, hence $ \{ f^{-1}(U_{j}) | j \in I \}$ is a finite sub cover. Therefore, $ \{ U_{j} | j \in I \}$ is a finite sub cover for $f(A)$ and $f(A)$ is compact. There are a few areas of this proof which are not quite clear to me, addressed in the four questions below (the last two are purely notational questions): Does this hold for any metric space or is this just a special property of $\mathbb{R}^{n}$? A part of me is skeptical that this proof is sufficient. It seems that we've only shown that one open cover (i.e. $ \{U_{i} | i \in I \} $ )  of $f(A)$ has a finite sub cover. Does it matter that we've given no explicit bound for the finite sub cover? That is, shouldn't the finite sub cover be written: $ \{ f^{-1}(U_{i})| i \in I$ and $i \leq j \}$ Why does each $i$ have to be in an index set $I$? Can't I just set $i \in \mathbb{N}$?","My professor went over a proof of the following in class: Suppose $A \in \mathbb{R}^{n}$ is compact and $f:A \rightarrow \mathbb{R}^{n}$ is continuous. Then $f(A)$ is compact. The proof (presented by my professor in class) is as follows: Since $f$ is continuous, the set $ \{f^{-1}(U_{i})|i \in I \}$ is an open cover for $A$. Since $A$ is compact, each open cover has a finite sub cover, hence $ \{ f^{-1}(U_{j}) | j \in I \}$ is a finite sub cover. Therefore, $ \{ U_{j} | j \in I \}$ is a finite sub cover for $f(A)$ and $f(A)$ is compact. There are a few areas of this proof which are not quite clear to me, addressed in the four questions below (the last two are purely notational questions): Does this hold for any metric space or is this just a special property of $\mathbb{R}^{n}$? A part of me is skeptical that this proof is sufficient. It seems that we've only shown that one open cover (i.e. $ \{U_{i} | i \in I \} $ )  of $f(A)$ has a finite sub cover. Does it matter that we've given no explicit bound for the finite sub cover? That is, shouldn't the finite sub cover be written: $ \{ f^{-1}(U_{i})| i \in I$ and $i \leq j \}$ Why does each $i$ have to be in an index set $I$? Can't I just set $i \in \mathbb{N}$?",,"['analysis', 'continuity', 'compactness']"
73,calculation of normal derivative,calculation of normal derivative,,"Suppose $\Omega$ is a bounded region in the plane $\mathbb{R}^2$ with smooth boundary $\partial\Omega$. Suppose $u$ is a smooth function in $\Omega$. I want to calculate $$\frac{\partial}{\partial\nu}|\nabla u|^2\mbox{ on }\partial\Omega,$$ the normal derivative with respect to the outward unit normal $\nu$. Here is my calculation:  $$\tag{1}\frac{\partial}{\partial\nu}|\nabla u|^2=\frac{\partial}{\partial\nu} \left\langle\nabla u,\nabla u\right\rangle=2\left\langle\nabla \frac{\partial u}{\partial\nu},\nabla u\right\rangle.$$ But I am sure this is wrong, because I can obtain an absurd conclusion by assuming that $u$ is the first Steklov eigenfunction. So I would like to ask where my mistake is in $(1)$. My guess is that I may have to add a term related to geodesic curvature in $(1)$.","Suppose $\Omega$ is a bounded region in the plane $\mathbb{R}^2$ with smooth boundary $\partial\Omega$. Suppose $u$ is a smooth function in $\Omega$. I want to calculate $$\frac{\partial}{\partial\nu}|\nabla u|^2\mbox{ on }\partial\Omega,$$ the normal derivative with respect to the outward unit normal $\nu$. Here is my calculation:  $$\tag{1}\frac{\partial}{\partial\nu}|\nabla u|^2=\frac{\partial}{\partial\nu} \left\langle\nabla u,\nabla u\right\rangle=2\left\langle\nabla \frac{\partial u}{\partial\nu},\nabla u\right\rangle.$$ But I am sure this is wrong, because I can obtain an absurd conclusion by assuming that $u$ is the first Steklov eigenfunction. So I would like to ask where my mistake is in $(1)$. My guess is that I may have to add a term related to geodesic curvature in $(1)$.",,"['calculus', 'analysis', 'multivariable-calculus', 'differential-geometry', 'derivatives']"
74,"Show that for any $g \in L_{p'}(E)$, where $p'$ is the conjugate of $p$, $\lim_{k \rightarrow \infty}\int_Ef_k(x)g(x)dx = \int_Ef(x)g(x)dx$","Show that for any , where  is the conjugate of ,",g \in L_{p'}(E) p' p \lim_{k \rightarrow \infty}\int_Ef_k(x)g(x)dx = \int_Ef(x)g(x)dx,"Let $1 < p < \infty, f_k \in L_p(E), k = 1, 2, ..., $ and $\lim_{k \rightarrow \infty}f_k(x) = f(x)$ a.e., $\sup_{1 \leq k<\infty}||f_k||_p \leq M$.  Show that for any $g \in L_{p'}(E)$, where $p'$ is the conjugate of $p$, $\lim_{k \rightarrow \infty}\int_Ef_k(x)g(x)dx = \int_Ef(x)g(x)dx$ This is from a past qual.  Not really sure what to do.  Thought about using dominated convergence theorem.  I did show that is $E$ is finite, $f_k \rightarrow f$ in $L_1(E)$ by vitali convergence theorem.  I think I was able to extend this to $m(E) = \infty$.  But, this doesn't show $f_k \rightarrow f$ in $L_p$ which is more useful.  Any suggestions?","Let $1 < p < \infty, f_k \in L_p(E), k = 1, 2, ..., $ and $\lim_{k \rightarrow \infty}f_k(x) = f(x)$ a.e., $\sup_{1 \leq k<\infty}||f_k||_p \leq M$.  Show that for any $g \in L_{p'}(E)$, where $p'$ is the conjugate of $p$, $\lim_{k \rightarrow \infty}\int_Ef_k(x)g(x)dx = \int_Ef(x)g(x)dx$ This is from a past qual.  Not really sure what to do.  Thought about using dominated convergence theorem.  I did show that is $E$ is finite, $f_k \rightarrow f$ in $L_1(E)$ by vitali convergence theorem.  I think I was able to extend this to $m(E) = \infty$.  But, this doesn't show $f_k \rightarrow f$ in $L_p$ which is more useful.  Any suggestions?",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
75,Discretization of an integral,Discretization of an integral,,"Given $f: [a,b] \to R $ and $K: [a,b]$ x $[a,b]$ $\to R$, we want to find a solution $\varphi:[a,b] \to R $ to the Fredholm integral equation: $$\varphi(x) = f(x)+\int _{ a }^{ b }{ K( x,t)\varphi (t)dt } $$ For a given integer n, we will consider a set of n+1 equispaced points $x_0$ ,..., $x_n$ with $x_0 = a$ and $x_n = b$. We let $h = (b−a)/n$. We want to find values $\varphi_0$, ..., $\varphi_n$ such that $\varphi(x_i) \approx \varphi_i$ for $i = 1...n$. Note that we have for each i: $$\varphi(x_i) = f(x_i)+\int _{ a }^{ b }{ K({ x }_{ i },t)\varphi (t)dt } $$ How would we discretize the above integral using points $x_0,...,x_n$ and values $\varphi_0,...,\varphi_n$ using an integration scheme? How would we find the system of linear equations that determines the values $\varphi_0,...,\varphi_n$? Any ideas?","Given $f: [a,b] \to R $ and $K: [a,b]$ x $[a,b]$ $\to R$, we want to find a solution $\varphi:[a,b] \to R $ to the Fredholm integral equation: $$\varphi(x) = f(x)+\int _{ a }^{ b }{ K( x,t)\varphi (t)dt } $$ For a given integer n, we will consider a set of n+1 equispaced points $x_0$ ,..., $x_n$ with $x_0 = a$ and $x_n = b$. We let $h = (b−a)/n$. We want to find values $\varphi_0$, ..., $\varphi_n$ such that $\varphi(x_i) \approx \varphi_i$ for $i = 1...n$. Note that we have for each i: $$\varphi(x_i) = f(x_i)+\int _{ a }^{ b }{ K({ x }_{ i },t)\varphi (t)dt } $$ How would we discretize the above integral using points $x_0,...,x_n$ and values $\varphi_0,...,\varphi_n$ using an integration scheme? How would we find the system of linear equations that determines the values $\varphi_0,...,\varphi_n$? Any ideas?",,"['integration', 'analysis', 'discrete-mathematics', 'numerical-methods', 'discrete-calculus']"
76,"An ""obvious"" statement about a nonincreasing supremum","An ""obvious"" statement about a nonincreasing supremum",,"Consider a nonnegative function $f(t,x): [0,\infty) \times [0,1] \rightarrow [0, \infty)$.  Suppose we have the following property: $$ \mbox{ If } ~~~~~~~~~~f(t,y) > \frac{1}{2} \sup_{x \in [0,1]} f(t,x) ~~~~~~~\mbox{ then}~~~~~~~ \frac{d}{dt} f(t,y) < 0 $$ In words, if any $y$ has $f(t,y)$ at least half as big as the largest $f(t,\cdot)$, then $f(t,y)$ is decreasing. It seems very natural to guess that $$\sup_{x \in [0,1]} f(t,x) \mbox{ is nonincreasing in } t$$ This seems obvious - the ``top half'' of $f(t, \cdot)$ is always decreasing, so how can the supremum increase? But I don't see how to show this mathematically. Thus my question is whether this is true. Some comments: The above implicitly assumes that for each $x$, $f(t,x)$ is differentiable with respect to $t$. To make sure nothing weird happens at $t=0$, let us also assume that the function $f(t,x)$ is a continuous function of $t$ for each $x$. If it helps, we can further assume that its derivative $f_t(t,x)$ is a continuous function of $t$. Note that there is no assumption of differentiability or even continuity with respect to $x$. Furthermore, there is no assumption that the suprema in question must remain finite.","Consider a nonnegative function $f(t,x): [0,\infty) \times [0,1] \rightarrow [0, \infty)$.  Suppose we have the following property: $$ \mbox{ If } ~~~~~~~~~~f(t,y) > \frac{1}{2} \sup_{x \in [0,1]} f(t,x) ~~~~~~~\mbox{ then}~~~~~~~ \frac{d}{dt} f(t,y) < 0 $$ In words, if any $y$ has $f(t,y)$ at least half as big as the largest $f(t,\cdot)$, then $f(t,y)$ is decreasing. It seems very natural to guess that $$\sup_{x \in [0,1]} f(t,x) \mbox{ is nonincreasing in } t$$ This seems obvious - the ``top half'' of $f(t, \cdot)$ is always decreasing, so how can the supremum increase? But I don't see how to show this mathematically. Thus my question is whether this is true. Some comments: The above implicitly assumes that for each $x$, $f(t,x)$ is differentiable with respect to $t$. To make sure nothing weird happens at $t=0$, let us also assume that the function $f(t,x)$ is a continuous function of $t$ for each $x$. If it helps, we can further assume that its derivative $f_t(t,x)$ is a continuous function of $t$. Note that there is no assumption of differentiability or even continuity with respect to $x$. Furthermore, there is no assumption that the suprema in question must remain finite.",,"['real-analysis', 'analysis']"
77,Is this another version of the gamma function?,Is this another version of the gamma function?,,"I know that $\Gamma  \left( x \right) $ is the unique function on $x \in (0, \infty)$ such that $f \left( 1 \right) =1$ $f(x+1)=xf(x)$ ${\frac {d^{2}}{d{x}^{2}}}ln(f \left( x \right))>0$ However define the set of functions $g(x)=\Gamma(x){e^{2\pi inx}}\:for\:n\in \Bbb Z$ and $i=\sqrt{-1}$, then $g(1)=\Gamma(1)e^{2\pi in}=1$ $g(x+1)=\Gamma(x+1){e^{2\pi in(x+1)}}=x\Gamma(x){e^{2\pi inx}}{e^{2\pi in}}=x\Gamma(x){e^{2\pi inx}}=xg(x)$ ${\frac {d^{2}}{d{x}^{2}}}ln(g \left( x \right))={\frac {d^{2}}{d{x}^{2}}}(ln(\Gamma(x))+2\pi i n x)={\frac {d^2}{dx^2}}ln(\Gamma(x))>0$ so therefore $g(x)=\Gamma(x)$ for $x\in(0,\infty)$. However, this is obviously not true, but it satisfies all the conditions of the Bohr-Mollerup theorem. Why is this? Is this some other version of the Gamma function? If anyone's curious, I got this idea from this post","I know that $\Gamma  \left( x \right) $ is the unique function on $x \in (0, \infty)$ such that $f \left( 1 \right) =1$ $f(x+1)=xf(x)$ ${\frac {d^{2}}{d{x}^{2}}}ln(f \left( x \right))>0$ However define the set of functions $g(x)=\Gamma(x){e^{2\pi inx}}\:for\:n\in \Bbb Z$ and $i=\sqrt{-1}$, then $g(1)=\Gamma(1)e^{2\pi in}=1$ $g(x+1)=\Gamma(x+1){e^{2\pi in(x+1)}}=x\Gamma(x){e^{2\pi inx}}{e^{2\pi in}}=x\Gamma(x){e^{2\pi inx}}=xg(x)$ ${\frac {d^{2}}{d{x}^{2}}}ln(g \left( x \right))={\frac {d^{2}}{d{x}^{2}}}(ln(\Gamma(x))+2\pi i n x)={\frac {d^2}{dx^2}}ln(\Gamma(x))>0$ so therefore $g(x)=\Gamma(x)$ for $x\in(0,\infty)$. However, this is obviously not true, but it satisfies all the conditions of the Bohr-Mollerup theorem. Why is this? Is this some other version of the Gamma function? If anyone's curious, I got this idea from this post",,"['analysis', 'gamma-function']"
78,Convergence in a metric space,Convergence in a metric space,,"Is it possible to define a metric on $\mathbb R$ such that $(1,0,1,0,...)$ converges on $(\mathbb R, d)$? I believe it is impossible. But how to show analytically? Any hint would be appreciated.","Is it possible to define a metric on $\mathbb R$ such that $(1,0,1,0,...)$ converges on $(\mathbb R, d)$? I believe it is impossible. But how to show analytically? Any hint would be appreciated.",,"['analysis', 'metric-spaces']"
79,Borel measure supported on $\mathbb{Q}$,Borel measure supported on,\mathbb{Q},Let $\mu$ be a Borel measure supported on $\mathbb{Q} \subset \mathbb{R}$. Must $\mu$ be a sum of Dirac measures?,Let $\mu$ be a Borel measure supported on $\mathbb{Q} \subset \mathbb{R}$. Must $\mu$ be a sum of Dirac measures?,,"['real-analysis', 'analysis', 'measure-theory']"
80,About p-adic numbers,About p-adic numbers,,"I'm studying the dual group of the diadic rationals, $\widehat{\mathbb{Z}[1/2]}$, where the dual is the dual of Pontryagin of $\mathbb{Z}[1/2]$. In some papers says that $\widehat{\mathbb{Z}[1/2]}$, is naturally a compact metric space. The problem is, that I don't know which metric I have to use for this. Can you help me? Greetings !","I'm studying the dual group of the diadic rationals, $\widehat{\mathbb{Z}[1/2]}$, where the dual is the dual of Pontryagin of $\mathbb{Z}[1/2]$. In some papers says that $\widehat{\mathbb{Z}[1/2]}$, is naturally a compact metric space. The problem is, that I don't know which metric I have to use for this. Can you help me? Greetings !",,"['analysis', 'harmonic-analysis', 'p-adic-number-theory']"
81,"$f:\mathbb{R^2}\setminus\{(0,0)\}\ \rightarrow \mathbb{R}$ of class $C^2$ for which $f_x(x,y)=\frac{y}{x^2+y^2}$ and $f_y(x,y)=\frac{-x}{x^2+y^2}$",of class  for which  and,"f:\mathbb{R^2}\setminus\{(0,0)\}\ \rightarrow \mathbb{R} C^2 f_x(x,y)=\frac{y}{x^2+y^2} f_y(x,y)=\frac{-x}{x^2+y^2}","Is there exists $f:\mathbb{R^2}\setminus\{(0,0)\}\ \rightarrow \mathbb{R}$ of class $C^2$ for which $f_x(x,y)=\frac{y}{x^2+y^2}$ and $f_y(x,y)=\frac{-x}{x^2+y^2}$ for all $(x,y)\in\mathbb{R^2}\setminus\{(0,0)\}$? I thought $\arctan(x/y)$ would be ok, but the answer to that question is negative. Why?","Is there exists $f:\mathbb{R^2}\setminus\{(0,0)\}\ \rightarrow \mathbb{R}$ of class $C^2$ for which $f_x(x,y)=\frac{y}{x^2+y^2}$ and $f_y(x,y)=\frac{-x}{x^2+y^2}$ for all $(x,y)\in\mathbb{R^2}\setminus\{(0,0)\}$? I thought $\arctan(x/y)$ would be ok, but the answer to that question is negative. Why?",,"['calculus', 'analysis', 'multivariable-calculus']"
82,Prove that any unbounded sequence has a subsequence that diverges to $∞$.,Prove that any unbounded sequence has a subsequence that diverges to .,∞,"To prove that any unbounded sequence has a subsequence that diverges to ∞, is it enough to say that you can take a subsequence $(a_{m(k)})$ where $m(k)=k$, as you know that this diverges to infinity, you are done?","To prove that any unbounded sequence has a subsequence that diverges to ∞, is it enough to say that you can take a subsequence $(a_{m(k)})$ where $m(k)=k$, as you know that this diverges to infinity, you are done?",,"['real-analysis', 'sequences-and-series', 'analysis', 'proof-verification']"
83,Differentiation Formula for Moving Regions.,Differentiation Formula for Moving Regions.,,"I've run into a few calculations in a series of textbooks/papers that require differentiating an integral with a changing region. In particular, I'd like to know if $f(x,t):\mathbb{R}^d\times \mathbb{R}\to \mathbb{R}$ is smooth (say), is $\frac{d}{dt}\int_{\{x: |{x}|\le t\}} f(x,t)\ dx=\int_{\{|x|=t\}}f(x,t)\cdot \frac{x}{|x|}dS(x)+\int_{\{|x|\le t\}}\frac{d}{dt}f(t,x)\ dx$? Where $dS$ is surface measure of the ball. I realize this might be a particular instance of the Reynold transport formula, but I have yet to find a reference that gives an example of how to compute the velocity of the moving region. (Cf. Evans PDE Page 713)","I've run into a few calculations in a series of textbooks/papers that require differentiating an integral with a changing region. In particular, I'd like to know if $f(x,t):\mathbb{R}^d\times \mathbb{R}\to \mathbb{R}$ is smooth (say), is $\frac{d}{dt}\int_{\{x: |{x}|\le t\}} f(x,t)\ dx=\int_{\{|x|=t\}}f(x,t)\cdot \frac{x}{|x|}dS(x)+\int_{\{|x|\le t\}}\frac{d}{dt}f(t,x)\ dx$? Where $dS$ is surface measure of the ball. I realize this might be a particular instance of the Reynold transport formula, but I have yet to find a reference that gives an example of how to compute the velocity of the moving region. (Cf. Evans PDE Page 713)",,"['analysis', 'multivariable-calculus', 'partial-differential-equations']"
84,existence of a special function,existence of a special function,,"Whether there exists a function $f(x,y)$ defined on $[0,1]\times(0,1]$ satisfies the following conditions: for any $x\in(0,1]$, $f(x,y)$ is decreasing with respect to $y$ and $\lim_{y\rightarrow0}f(x,y)=\log x$.","Whether there exists a function $f(x,y)$ defined on $[0,1]\times(0,1]$ satisfies the following conditions: for any $x\in(0,1]$, $f(x,y)$ is decreasing with respect to $y$ and $\lim_{y\rightarrow0}f(x,y)=\log x$.",,['analysis']
85,Function compositions that are in $L^p$,Function compositions that are in,L^p,"We have $f \in L^p$. The goal is to show that $\exists \psi \in C(\mathbb{R^+}, \mathbb{R^+})$ such that $$ \lim_{s \to +\infty} \frac{\phi(s)}{s}=+ \infty \text{ and } \phi(|f|) \in L^p$$ I neeed some pointer on how to begin. Edit: I went to my professor for a hint and he said that there exists a continous function $\psi$ such that $\lim_{s \to \infty}\psi(s)=+\infty$ with $\sum \psi(n)a_n < \infty$ for a convergent series of poistive terms. Use $\psi$ to construct $\phi$.","We have $f \in L^p$. The goal is to show that $\exists \psi \in C(\mathbb{R^+}, \mathbb{R^+})$ such that $$ \lim_{s \to +\infty} \frac{\phi(s)}{s}=+ \infty \text{ and } \phi(|f|) \in L^p$$ I neeed some pointer on how to begin. Edit: I went to my professor for a hint and he said that there exists a continous function $\psi$ such that $\lim_{s \to \infty}\psi(s)=+\infty$ with $\sum \psi(n)a_n < \infty$ for a convergent series of poistive terms. Use $\psi$ to construct $\phi$.",,"['real-analysis', 'integration', 'analysis', 'lp-spaces']"
86,$\left\{x\in H: 2\leq \|x\|\leq 5\right\}$ is compact?,is compact?,\left\{x\in H: 2\leq \|x\|\leq 5\right\},"In a Hilbert space $H$ of dimention infinite, $A=\left\{x\in H:2\leq \|x\|\leq 5\right\}$ is compact? (totally bounded and complete) Thanks in advance.","In a Hilbert space $H$ of dimention infinite, $A=\left\{x\in H:2\leq \|x\|\leq 5\right\}$ is compact? (totally bounded and complete) Thanks in advance.",,"['analysis', 'hilbert-spaces', 'compactness', 'normed-spaces']"
87,"Discontinuous function in $W^{1, 1}(\mathbb{R}^{2})$",Discontinuous function in,"W^{1, 1}(\mathbb{R}^{2})","What's an example of a bounded function in $W^{1, 1}(\mathbb{R}^{2})$ which is discontinuous? Can this function be discontinuous on a set of positive measure?","What's an example of a bounded function in $W^{1, 1}(\mathbb{R}^{2})$ which is discontinuous? Can this function be discontinuous on a set of positive measure?",,"['analysis', 'sobolev-spaces']"
88,Differentiability implies continuity -- possibly pedantic question about the common proof,Differentiability implies continuity -- possibly pedantic question about the common proof,,"The common proof that differentiability implies continuity arrives at this limit: $$\lim_{x\to a} [f(x) - f(a)] = 0$$ I'm failing to see the simple justification for moving to the next step, which seems to be essentially this: $$\lim_{x\to a} f(x) - \lim_{x\to a} f(a) = 0$$ Intuitively, it makes sense, and I'm sure an epsilon-delta proof can be furnished. But as a matter of simple limit laws (the subtraction law in this case), the above assumes $\lim_{x\to a}f(x)$ exists, no? Curiously, the authors who use this proof consider it important in other contexts to beat home the fact that $\lim_{x\to a}f(x)$ exists, which they do by using the continuity of f . In this case, they can't use the continuity of f , for the continuity of f is precisely what's under question. So what are they using? By the way, I understand the final steps of the proof: $$\lim_{x\to a} f(x) = \lim_{x\to a} f(a)$$ $$\lim_{x\to a} f(x) = f(a)$$","The common proof that differentiability implies continuity arrives at this limit: $$\lim_{x\to a} [f(x) - f(a)] = 0$$ I'm failing to see the simple justification for moving to the next step, which seems to be essentially this: $$\lim_{x\to a} f(x) - \lim_{x\to a} f(a) = 0$$ Intuitively, it makes sense, and I'm sure an epsilon-delta proof can be furnished. But as a matter of simple limit laws (the subtraction law in this case), the above assumes $\lim_{x\to a}f(x)$ exists, no? Curiously, the authors who use this proof consider it important in other contexts to beat home the fact that $\lim_{x\to a}f(x)$ exists, which they do by using the continuity of f . In this case, they can't use the continuity of f , for the continuity of f is precisely what's under question. So what are they using? By the way, I understand the final steps of the proof: $$\lim_{x\to a} f(x) = \lim_{x\to a} f(a)$$ $$\lim_{x\to a} f(x) = f(a)$$",,"['calculus', 'analysis', 'derivatives', 'continuity']"
89,Equivalent definitions of differentiable,Equivalent definitions of differentiable,,"I am trying to show: The two statements are equivalent: (i) $f$ is diﬀerentiable at $a$, (ii) $f(a + h) = f(a) + ch + o(h)$, where c is some constant (depending on $a$) and $o(h)$ denotes some function of $h$ (also depending on $a$), with the property that $$\lim_{h\to 0} \frac{o(h)}{|h|} = 0$$ (That is $o(h) = h\alpha(h)$; where $\lim_{h\to0} \alpha(h) = 0$) What is the relation between $c$ and $f′(a)$? For (i) I have given the standard definition of differentiable in terms of limit. I see this is not too different from the statement in (ii) but I cannot make them equivalent. Any help would be much appreciated.","I am trying to show: The two statements are equivalent: (i) $f$ is diﬀerentiable at $a$, (ii) $f(a + h) = f(a) + ch + o(h)$, where c is some constant (depending on $a$) and $o(h)$ denotes some function of $h$ (also depending on $a$), with the property that $$\lim_{h\to 0} \frac{o(h)}{|h|} = 0$$ (That is $o(h) = h\alpha(h)$; where $\lim_{h\to0} \alpha(h) = 0$) What is the relation between $c$ and $f′(a)$? For (i) I have given the standard definition of differentiable in terms of limit. I see this is not too different from the statement in (ii) but I cannot make them equivalent. Any help would be much appreciated.",,"['analysis', 'functions', 'derivatives']"
90,Tangent Cone is a cone?,Tangent Cone is a cone?,,"I first give two definitions. Def1 : A set $S$ is a cone if $x \in S, \lambda \geq 0 \implies \lambda x \in S$ . Def2 : Let $S$ be any set (we may assume $\mathbb{R}^n$ with the usual Euclidean norm) and $\bar{x} \in S$ . The tangent cone to $S$ at $\bar{x}$ is defined as $$T_S(\bar{x}) = \overline{\{ h :  \bar{x} + \lambda h \in S, \text{ for some } \lambda >0\}}$$ Prove that the tangent cone is indeed a cone.",I first give two definitions. Def1 : A set is a cone if . Def2 : Let be any set (we may assume with the usual Euclidean norm) and . The tangent cone to at is defined as Prove that the tangent cone is indeed a cone.,"S x \in S, \lambda \geq 0 \implies \lambda x \in S S \mathbb{R}^n \bar{x} \in S S \bar{x} T_S(\bar{x}) = \overline{\{ h :  \bar{x} + \lambda h \in S, \text{ for some } \lambda >0\}}","['analysis', 'proof-writing', 'convex-analysis', 'convex-optimization']"
91,Unbounded Sequence with Bounded Partial Sums,Unbounded Sequence with Bounded Partial Sums,,"In my Analysis class the other day we were discussing Sequences that have bounded partial sums yet their infinite series does not converge, with the typical example of $\{ a_n \}=(-1)^n$. In discussion we started to wonder if there exists an unbounded sequence that had bounded partial sums. I have been thinking about this for a few days and I think that $$b_n = (-1)^nln(n) $$ is such a sequence. In Mathematica I tested the first 2,000,000 sums and they are all within $\pm10 $. I would like to be able to prove that the partial sums are bounded yet I really do not know even where to start. If you can come up with any other examples or know where to start proving $$ \sum_1^n (-1)^nln(n) $$ is bounded, I would love to hear what you have to say.","In my Analysis class the other day we were discussing Sequences that have bounded partial sums yet their infinite series does not converge, with the typical example of $\{ a_n \}=(-1)^n$. In discussion we started to wonder if there exists an unbounded sequence that had bounded partial sums. I have been thinking about this for a few days and I think that $$b_n = (-1)^nln(n) $$ is such a sequence. In Mathematica I tested the first 2,000,000 sums and they are all within $\pm10 $. I would like to be able to prove that the partial sums are bounded yet I really do not know even where to start. If you can come up with any other examples or know where to start proving $$ \sum_1^n (-1)^nln(n) $$ is bounded, I would love to hear what you have to say.",,"['sequences-and-series', 'analysis']"
92,Differentiabilty only at a single point implies the jacobian is singular?,Differentiabilty only at a single point implies the jacobian is singular?,,let $f:\mathbb{R}^n \to \mathbb{R}^n$ be a function differentiable at a single point $p \in \mathbb{R}^n$ yet not differentiable at any other point in $\mathbb{R}^n$. The inverse function theorem tells us that if the jacobian of $f$ is non singular at $p$ then $f$ differentiable on some open set that contains $p$. So this means the jacobian of $f$ in our case must be singular. Is that right?,let $f:\mathbb{R}^n \to \mathbb{R}^n$ be a function differentiable at a single point $p \in \mathbb{R}^n$ yet not differentiable at any other point in $\mathbb{R}^n$. The inverse function theorem tells us that if the jacobian of $f$ is non singular at $p$ then $f$ differentiable on some open set that contains $p$. So this means the jacobian of $f$ in our case must be singular. Is that right?,,[]
93,Lebesgue measure/Measurable sets,Lebesgue measure/Measurable sets,,"Question : Let $f,g$ be measurable real valued functions on $\mathbb{R}$ such that  : $$\int_{-\infty}^{\infty} (f(x)^2+g(x)^2)dx=2\int_{-\infty}^{\infty} f(x)g(x)dx$$ Let $E=\{x\in \mathbb{R} : f(x)\neq g(x)\}$ . Which of the followng statements are necessarily true? $E$ is empty set $E$ is measurable $E$ has lebesgue measure $0$ For almost all $x\in \mathbb{R}$ we have $f(x)=0$ and $g(x)=0$ Explanation: What all I could see is that second bullet and third bullet are probably correct. Because : $$\int_{-\infty}^{\infty} (f(x)^2+g(x)^2)dx=2\int_{-\infty}^{\infty} f(x)g(x)dx$$ i.e., $$\int_{-\infty}^{\infty} (f(x)^2+g(x)^2)dx-2\int_{-\infty}^{\infty} f(x)g(x)dx=0$$ i.e., $$\int_{-\infty}^{\infty}(f(x)-g(x))^2dx=0$$ Though I have negative limits my function $(f(x)-g(x))^2$ is positive So, I would see that $E=\{x\in \mathbb{R} : f(x)\neq g(x)\}$ is measurable and has measure $0$ Please tell me if what I have done is sufficient/clear.","Question : Let be measurable real valued functions on such that  : Let . Which of the followng statements are necessarily true? is empty set is measurable has lebesgue measure For almost all we have and Explanation: What all I could see is that second bullet and third bullet are probably correct. Because : i.e., i.e., Though I have negative limits my function is positive So, I would see that is measurable and has measure Please tell me if what I have done is sufficient/clear.","f,g \mathbb{R} \int_{-\infty}^{\infty} (f(x)^2+g(x)^2)dx=2\int_{-\infty}^{\infty} f(x)g(x)dx E=\{x\in \mathbb{R} : f(x)\neq g(x)\} E E E 0 x\in \mathbb{R} f(x)=0 g(x)=0 \int_{-\infty}^{\infty} (f(x)^2+g(x)^2)dx=2\int_{-\infty}^{\infty} f(x)g(x)dx \int_{-\infty}^{\infty} (f(x)^2+g(x)^2)dx-2\int_{-\infty}^{\infty} f(x)g(x)dx=0 \int_{-\infty}^{\infty}(f(x)-g(x))^2dx=0 (f(x)-g(x))^2 E=\{x\in \mathbb{R} : f(x)\neq g(x)\} 0","['real-analysis', 'analysis']"
94,The unsolved extension problem on manifolds.,The unsolved extension problem on manifolds.,,"I have been struggeling for quite a while with this problem: Let $M \subset \mathbb{R}^n$ be a compact $C^k-$ submanifold and $\phi_i: B_i(0) \rightarrow M$ be the associated set of charts $(\phi_i)_{i \in \{1,...,n\}}$, where $B_i(0)$ is an open ball around $0$, such that $\phi_i(B_i(0)) \subset M$ is relatively open. Now we call a function $f: M \rightarrow \mathbb{R}$ k-times continuously differentiable if for every chart $f \circ \phi_i$ is k-times continuously differentiable. Now I am supposed to show that there is a map $F: \mathbb{R}^n \rightarrow \mathbb{R}$ that is k-times continuously differentiable and $F|_M = f$ such that $\text{supp(F)} \subset N$, where N is an epsilon-surrounding of M. What have I tried so far: My first idea was to use a partition of unity, as we only have a continuously differentiable function $f$ via $f \circ \phi_i$, but this did not really work. Further, my problem was how to make the transition from $M$, where it is pretty clear, what $F$ does to $N$, which is outside from $M$ and hence completely undefined. I appreciate any kind of help and suggestion, also if you cannot offer a complete solution. Thank you very much in advance.","I have been struggeling for quite a while with this problem: Let $M \subset \mathbb{R}^n$ be a compact $C^k-$ submanifold and $\phi_i: B_i(0) \rightarrow M$ be the associated set of charts $(\phi_i)_{i \in \{1,...,n\}}$, where $B_i(0)$ is an open ball around $0$, such that $\phi_i(B_i(0)) \subset M$ is relatively open. Now we call a function $f: M \rightarrow \mathbb{R}$ k-times continuously differentiable if for every chart $f \circ \phi_i$ is k-times continuously differentiable. Now I am supposed to show that there is a map $F: \mathbb{R}^n \rightarrow \mathbb{R}$ that is k-times continuously differentiable and $F|_M = f$ such that $\text{supp(F)} \subset N$, where N is an epsilon-surrounding of M. What have I tried so far: My first idea was to use a partition of unity, as we only have a continuously differentiable function $f$ via $f \circ \phi_i$, but this did not really work. Further, my problem was how to make the transition from $M$, where it is pretty clear, what $F$ does to $N$, which is outside from $M$ and hence completely undefined. I appreciate any kind of help and suggestion, also if you cannot offer a complete solution. Thank you very much in advance.",,"['calculus', 'real-analysis']"
95,Problem with notation: Laplacian on a manifold,Problem with notation: Laplacian on a manifold,,"In the Aubin's book ""Nonlinear analysis on manifolds"" the Laplacian operator on functions on some smooth manifold is defined by the formula $$      \Delta = -\nabla^\gamma\nabla_\gamma, $$ where $\nabla_\gamma$ is the covariant derivative. The author didn't write what does mean $\nabla^\gamma$. I've found in the internet that the notation $\nabla^\gamma$ is sometimes used for the so-called contravariant derivative that is given by $\nabla^\gamma = g^{\gamma i} \nabla_i$, where $g_{ij}$ is a metric tensor. If we use this definition then for a smooth function $f$ on a manifold we will have: $$    \Delta f = - g^{\gamma i} \nabla_i \nabla_\gamma f = g^{\gamma i}\nabla_i \left( \frac{\partial f}{\partial x^\gamma}\right) = g^{\gamma i} \frac{\partial^2 f}{\partial x^i \partial x^\gamma}, $$ but this formula doesn't coincide with the formula for the Laplace-Beltrami operator. On the other hand I have found that $\Delta = -\nabla_\gamma^* \nabla_\gamma$, where $\nabla_\gamma^*$ is the formal conjugate to the covariant derivative with respect to scalar product $(f,g) = \int f g \, \Omega$, but in the book the operator $\nabla^\gamma$ appears before the definition of scalar product, so it can't be just a notation for the formal conjugate operator to $\nabla_\gamma$.","In the Aubin's book ""Nonlinear analysis on manifolds"" the Laplacian operator on functions on some smooth manifold is defined by the formula $$      \Delta = -\nabla^\gamma\nabla_\gamma, $$ where $\nabla_\gamma$ is the covariant derivative. The author didn't write what does mean $\nabla^\gamma$. I've found in the internet that the notation $\nabla^\gamma$ is sometimes used for the so-called contravariant derivative that is given by $\nabla^\gamma = g^{\gamma i} \nabla_i$, where $g_{ij}$ is a metric tensor. If we use this definition then for a smooth function $f$ on a manifold we will have: $$    \Delta f = - g^{\gamma i} \nabla_i \nabla_\gamma f = g^{\gamma i}\nabla_i \left( \frac{\partial f}{\partial x^\gamma}\right) = g^{\gamma i} \frac{\partial^2 f}{\partial x^i \partial x^\gamma}, $$ but this formula doesn't coincide with the formula for the Laplace-Beltrami operator. On the other hand I have found that $\Delta = -\nabla_\gamma^* \nabla_\gamma$, where $\nabla_\gamma^*$ is the formal conjugate to the covariant derivative with respect to scalar product $(f,g) = \int f g \, \Omega$, but in the book the operator $\nabla^\gamma$ appears before the definition of scalar product, so it can't be just a notation for the formal conjugate operator to $\nabla_\gamma$.",,"['analysis', 'differential-geometry', 'partial-differential-equations', 'notation', 'riemannian-geometry']"
96,strong maximum principle - harmonic function,strong maximum principle - harmonic function,,"Consider the following the theorem in the classical PDE book of Evans (chapter 2): (Part of the strong maximum principle) Let $U$ a open set in $R^n$ and $u \in C^2 (U) \cap C(\overline{U})$, with $\Delta u = 0$ in $U$. If $U$ is connected and there exists a point $x_0 \in U$ such that   $$ u(x_0) = \displaystyle\max_{\overline{U}} u$$   then $u$ is constant within $U$. Part of the proof: Suppose there exists a point $x_0 \in U$ with $u(x_0) = M = \displaystyle\max_{\overline{U}} u . $  Then for $0 < r < \mbox{dist} (x_0 , \partial U)$, the mean value property asserts    $$ M = u(x_0)  = \displaystyle\frac{\displaystyle1}{|B(x_0, r)|}\int_{B(x_0,r) } u  \ dy \leq  M.$$ Then   $$u = M\quad\text{in}\quad B(x_0 , r)\tag{$*$}.$$ I dont understand the equality in $(*)$. If I be non rigorous, for me is clear to see the equality in $(*)$. But I dont know how to prove the equality... Someone can give me a hint about how to prove the equality in $(*)$? Thanks in advance!","Consider the following the theorem in the classical PDE book of Evans (chapter 2): (Part of the strong maximum principle) Let $U$ a open set in $R^n$ and $u \in C^2 (U) \cap C(\overline{U})$, with $\Delta u = 0$ in $U$. If $U$ is connected and there exists a point $x_0 \in U$ such that   $$ u(x_0) = \displaystyle\max_{\overline{U}} u$$   then $u$ is constant within $U$. Part of the proof: Suppose there exists a point $x_0 \in U$ with $u(x_0) = M = \displaystyle\max_{\overline{U}} u . $  Then for $0 < r < \mbox{dist} (x_0 , \partial U)$, the mean value property asserts    $$ M = u(x_0)  = \displaystyle\frac{\displaystyle1}{|B(x_0, r)|}\int_{B(x_0,r) } u  \ dy \leq  M.$$ Then   $$u = M\quad\text{in}\quad B(x_0 , r)\tag{$*$}.$$ I dont understand the equality in $(*)$. If I be non rigorous, for me is clear to see the equality in $(*)$. But I dont know how to prove the equality... Someone can give me a hint about how to prove the equality in $(*)$? Thanks in advance!",,"['analysis', 'partial-differential-equations', 'harmonic-analysis']"
97,Constructing a vector field,Constructing a vector field,,"I'm doing a list of exercises, but there is one problem I can't solve. Let $$\Omega=\Big\{(x,y,z)\in\mathbb{R}^3 \ | \ \frac{x^2}{4}+\frac{y^2}{9}+\frac{z^2}{16}<1\Big\}.$$ Suppose that $\rho:\Omega\rightarrow\mathbb{R}$ is a given continuous function, and that $i:\Omega\rightarrow\mathbb{R}^3$ is $C^1$ and is also given. Also supose that $\nabla\cdot i=0$ in $\Omega$. Show that exists $$F:\Omega\rightarrow\mathbb{R}^3$$ such that $\nabla\cdot F=\rho$ and $\nabla\wedge F=i$. PS: $\nabla\cdot F$ is the divergence and $\nabla\wedge F$ is the curl. I know I'm supposed to use this information to construct the vector field $F$...I'm a little lost here cause I never did this before. Thanks.","I'm doing a list of exercises, but there is one problem I can't solve. Let $$\Omega=\Big\{(x,y,z)\in\mathbb{R}^3 \ | \ \frac{x^2}{4}+\frac{y^2}{9}+\frac{z^2}{16}<1\Big\}.$$ Suppose that $\rho:\Omega\rightarrow\mathbb{R}$ is a given continuous function, and that $i:\Omega\rightarrow\mathbb{R}^3$ is $C^1$ and is also given. Also supose that $\nabla\cdot i=0$ in $\Omega$. Show that exists $$F:\Omega\rightarrow\mathbb{R}^3$$ such that $\nabla\cdot F=\rho$ and $\nabla\wedge F=i$. PS: $\nabla\cdot F$ is the divergence and $\nabla\wedge F$ is the curl. I know I'm supposed to use this information to construct the vector field $F$...I'm a little lost here cause I never did this before. Thanks.",,"['analysis', 'vector-analysis']"
98,"Find the weak derivative of $u(x)=\begin{cases}|x|, & \text{if}\,\,\, x<1 \\ 1-x^2, & \text{if}\,\,\, x\ge1 \end{cases}$",Find the weak derivative of,"u(x)=\begin{cases}|x|, & \text{if}\,\,\, x<1 \\ 1-x^2, & \text{if}\,\,\, x\ge1 \end{cases}","Find a weak derivative of the function $u:(-1;2)\rightarrow \mathbb{R}$ defined as follows: $$u(x)=\begin{cases}|x|, & \text{if}\,\,\, x<1 \\ 1-x^2, & \text{if}\,\,\, x\ge1 \end{cases}$$ I know that a weak derivative of $|x|$ is equal to $2\chi_{(0,1)}(x)-1$ and a weak derivative of $1-x^2$ is equal to $-2x$. Should I simply combine them? Can a weak derivative be not continous? Thanks in advance.","Find a weak derivative of the function $u:(-1;2)\rightarrow \mathbb{R}$ defined as follows: $$u(x)=\begin{cases}|x|, & \text{if}\,\,\, x<1 \\ 1-x^2, & \text{if}\,\,\, x\ge1 \end{cases}$$ I know that a weak derivative of $|x|$ is equal to $2\chi_{(0,1)}(x)-1$ and a weak derivative of $1-x^2$ is equal to $-2x$. Should I simply combine them? Can a weak derivative be not continous? Thanks in advance.",,"['analysis', 'sobolev-spaces', 'weak-derivatives']"
99,Tangent map $F_{*}: T_{p}(M) \to T_{F(p)}(N)$ are linear tranformations.,Tangent map  are linear tranformations.,F_{*}: T_{p}(M) \to T_{F(p)}(N),"How to show that tangent map $F_{*}: T_{p}(M) \to T_{F(p)}(N)$ are linear tranformations? I know that it suffices to show that $$F_{*}(ax_{u}+bx_{v})=aF_{*}(x_{u})+bF_{*}(x_{v})$$ where $x$ is a patch in $M$. By definition, $$F_{*}(ax_{u}+bx_{v})=\frac{d}{dt}F(\alpha(t))_{t=0}$$ where $\alpha'(0)=ax_{u}+bx_{v}$. I also know that if $y=F(\alpha(t))$, then $y'=F_{*}(\alpha'(t))$. We can express $\alpha(t) =x(a_{1}(t),a_{2}(t))$, then $$\alpha'(t) =a_{1}'(t)[x_{u}(a_{1}(t),a_{2}(t))]+a_{2}'[x_{v}(a_{1}(t),a_{2}(t))]$$ For $t=0$, $\alpha'(0) =a_{1}'(0)[x_{u}(a_{1}(0),a_{2}(0))]+a_{2}'[x_{v}(a_{1}(0),a_{2}(0))]=ax_{u}+bx_{v}$. Thus, I get $a_{1}(0)=u$, $a_{2}(0)=v$, $a_{1}'(0)=a$, $a_{2}'(0)=b$. Now, we can say that $$F_{*}(ax_{u}+bx_{y})=\frac{d}{dt}F(x(a_{1}(t),x(a_{2}(t)))_{t=0}$$ $$=a_{1}'(0)\frac{\partial}{\partial{u}}F(x(a_{1}(t),x(a_{2}(t)))+a_{2}'(0)\frac{\partial}{\partial{v}}F(x(a_{1}(t),x(a_{2}(t))$$ If $\frac{\partial}{\partial{u}}F(x)=F(x_{u})$ , done. but is it right? Thanks in advance.","How to show that tangent map $F_{*}: T_{p}(M) \to T_{F(p)}(N)$ are linear tranformations? I know that it suffices to show that $$F_{*}(ax_{u}+bx_{v})=aF_{*}(x_{u})+bF_{*}(x_{v})$$ where $x$ is a patch in $M$. By definition, $$F_{*}(ax_{u}+bx_{v})=\frac{d}{dt}F(\alpha(t))_{t=0}$$ where $\alpha'(0)=ax_{u}+bx_{v}$. I also know that if $y=F(\alpha(t))$, then $y'=F_{*}(\alpha'(t))$. We can express $\alpha(t) =x(a_{1}(t),a_{2}(t))$, then $$\alpha'(t) =a_{1}'(t)[x_{u}(a_{1}(t),a_{2}(t))]+a_{2}'[x_{v}(a_{1}(t),a_{2}(t))]$$ For $t=0$, $\alpha'(0) =a_{1}'(0)[x_{u}(a_{1}(0),a_{2}(0))]+a_{2}'[x_{v}(a_{1}(0),a_{2}(0))]=ax_{u}+bx_{v}$. Thus, I get $a_{1}(0)=u$, $a_{2}(0)=v$, $a_{1}'(0)=a$, $a_{2}'(0)=b$. Now, we can say that $$F_{*}(ax_{u}+bx_{y})=\frac{d}{dt}F(x(a_{1}(t),x(a_{2}(t)))_{t=0}$$ $$=a_{1}'(0)\frac{\partial}{\partial{u}}F(x(a_{1}(t),x(a_{2}(t)))+a_{2}'(0)\frac{\partial}{\partial{v}}F(x(a_{1}(t),x(a_{2}(t))$$ If $\frac{\partial}{\partial{u}}F(x)=F(x_{u})$ , done. but is it right? Thanks in advance.",,"['analysis', 'differential-geometry']"
