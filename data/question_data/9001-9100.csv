,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Derivation of 2D Trapezoid Rule,Derivation of 2D Trapezoid Rule,,"According to this reference , the trapezoid rule in two dimensions in a domain $x\in[a,b]$ , $y\in[c,d]$ is: \begin{align} \iint\limits_A f(x, y)\ dA &= \int\limits_a^b\int\limits_c^d f(x, y)\ dy\ dx\\                            &={ \Delta x \Delta y \over 4}\bigg[f(a, c) + f(b, c) + f(a, d) + f(b, d) \\ &+ 2\sum_i f(x_i, c) + 2\sum_i f(x_i, d) + 2\sum_j f(a, y_j) + 2\sum_j f(b, y_j)\\ &+ 4\sum_j \big(\sum_i f(x_i, y_j)\big)\bigg] \end{align} Considering that the trapezoid rule in 1D is $\int f(x) dx = \sum_i {f(x_i) + f(x_{i+1})\over 2}\Delta x_i$ I tried to derive the 2D trapezoid rule as follows: \begin{align} \iint\limits_A f(x,y)\ dA &= {\Delta x \over 2}\int dy \sum_i f(x_i,y) + f(x_{i+1}, y)\\ &={\Delta x \over 2} \sum_i \int dy\ (f(x_i, y) + f(x_{i+1}, y))\\ &={\Delta x \over 2} \sum_i \bigg[\int dy \ f(x_i, y) + \int dy\ f(x_{i+1}, y)\bigg]\\ &= {\Delta x \Delta y \over 4}\sum_i\bigg[\sum_j f(x_i, y_j) + f(x_i, y_{j+1}) + \sum_j f(x_{i+1}, y_j) + f(x_{i+1}, y_{j+1})\bigg] \end{align} but this is significantly different from the result shown above so I'm looking for help to derive the 2D trapezoid rule. Thanks.","According to this reference , the trapezoid rule in two dimensions in a domain , is: Considering that the trapezoid rule in 1D is I tried to derive the 2D trapezoid rule as follows: but this is significantly different from the result shown above so I'm looking for help to derive the 2D trapezoid rule. Thanks.","x\in[a,b] y\in[c,d] \begin{align}
\iint\limits_A f(x, y)\ dA &= \int\limits_a^b\int\limits_c^d f(x, y)\ dy\ dx\\
                           &={ \Delta x \Delta y \over 4}\bigg[f(a, c) + f(b, c) + f(a, d) + f(b, d) \\
&+ 2\sum_i f(x_i, c) + 2\sum_i f(x_i, d) + 2\sum_j f(a, y_j) + 2\sum_j f(b, y_j)\\ &+ 4\sum_j \big(\sum_i f(x_i, y_j)\big)\bigg]
\end{align} \int f(x) dx = \sum_i {f(x_i) + f(x_{i+1})\over 2}\Delta x_i \begin{align}
\iint\limits_A f(x,y)\ dA &= {\Delta x \over 2}\int dy \sum_i f(x_i,y) + f(x_{i+1}, y)\\ &={\Delta x \over 2} \sum_i \int dy\ (f(x_i, y) + f(x_{i+1}, y))\\ &={\Delta x \over 2} \sum_i \bigg[\int dy \ f(x_i, y) + \int dy\ f(x_{i+1}, y)\bigg]\\ &= {\Delta x \Delta y \over 4}\sum_i\bigg[\sum_j f(x_i, y_j) + f(x_i, y_{j+1}) + \sum_j f(x_{i+1}, y_j) + f(x_{i+1}, y_{j+1})\bigg]
\end{align}","['calculus', 'real-analysis', 'integration', 'analysis']"
1,"How to integrate $\int\frac{1}{\sqrt{x(x-9)(x-5)}}\,dx$?",How to integrate ?,"\int\frac{1}{\sqrt{x(x-9)(x-5)}}\,dx","Integrate: $$\int\frac{1}{\sqrt{x(x-9)(x-5)}}\,dx$$ I did some substitutions, but it seems not to be the right path to follow. Some hints? Noticing that $x(x-9)(x-5) =x((x-7)^2-4)$ we have: $$\int\frac{1}{\sqrt{x(x-9)(x-5)}}\,dx\,\,= \int\frac{1}{\sqrt{(y+7)(y^2-4)}}\,dy\,\,\,=\int\frac{1}{\sqrt{7+2\cosh(t)}}\,dt\,\,= \int\frac{2}{\sqrt{z^4+7z^2+1}}\,dz\,\,= \int\frac{2}{\sqrt{\left(z^2+\frac{7}{2}\right)^2- \frac{45}{4}}}\,dz\,\,=\,\,??? $$ Substitutions are: $y=x-7\,\,\,;y=2\cosh(t)\,\,\,;z=e^\frac{t}{2}$","Integrate: $$\int\frac{1}{\sqrt{x(x-9)(x-5)}}\,dx$$ I did some substitutions, but it seems not to be the right path to follow. Some hints? Noticing that $x(x-9)(x-5) =x((x-7)^2-4)$ we have: $$\int\frac{1}{\sqrt{x(x-9)(x-5)}}\,dx\,\,= \int\frac{1}{\sqrt{(y+7)(y^2-4)}}\,dy\,\,\,=\int\frac{1}{\sqrt{7+2\cosh(t)}}\,dt\,\,= \int\frac{2}{\sqrt{z^4+7z^2+1}}\,dz\,\,= \int\frac{2}{\sqrt{\left(z^2+\frac{7}{2}\right)^2- \frac{45}{4}}}\,dz\,\,=\,\,??? $$ Substitutions are: $y=x-7\,\,\,;y=2\cosh(t)\,\,\,;z=e^\frac{t}{2}$",,"['real-analysis', 'integration', 'indefinite-integrals', 'elliptic-integrals']"
2,"Does "" All continuous functions are bounded "" or "" All continuous functions attain a maximum "" or together imply the domain is compact?","Does "" All continuous functions are bounded "" or "" All continuous functions attain a maximum "" or together imply the domain is compact?",,"Let $(X,d)$ be an infinite metric space satisfying H1 or H2 or both. H1: (All continuous functions on X to $\mathbb{R}$ are bounded.) If $f: X\to\mathbb{R}$ is continuous on $X$, then $f(x)$ is bounded. H2: (All continuous functions on $X$ to $\mathbb{R}$ attain a maximum.) If $f: X\to\mathbb{R}$ is continuous on $X$, then there exists at least one point $p \in X $ such that $f(p) \geq f(x)$ for every $x \in X.  $ Question Does H1 only or does H2 only imply $X$ is compact? Do H1 and H2 together imply $X$ is compact? Note: I know "" If $X$ is compact then all continuous functions on $X$ to $\mathbb{R}$ are bounded "" and "" If $X$ is compact then all continuous functions on $X$ to $\mathbb{R}$ attain a maximum "". I am just curious about whether or not the converse still hold.","Let $(X,d)$ be an infinite metric space satisfying H1 or H2 or both. H1: (All continuous functions on X to $\mathbb{R}$ are bounded.) If $f: X\to\mathbb{R}$ is continuous on $X$, then $f(x)$ is bounded. H2: (All continuous functions on $X$ to $\mathbb{R}$ attain a maximum.) If $f: X\to\mathbb{R}$ is continuous on $X$, then there exists at least one point $p \in X $ such that $f(p) \geq f(x)$ for every $x \in X.  $ Question Does H1 only or does H2 only imply $X$ is compact? Do H1 and H2 together imply $X$ is compact? Note: I know "" If $X$ is compact then all continuous functions on $X$ to $\mathbb{R}$ are bounded "" and "" If $X$ is compact then all continuous functions on $X$ to $\mathbb{R}$ attain a maximum "". I am just curious about whether or not the converse still hold.",,"['real-analysis', 'general-topology']"
3,"Example of a noncompact operator on $L^2([0,1])$",Example of a noncompact operator on,"L^2([0,1])","For $f \in L^2([0,1])$, define operator $Tf: x \mapsto \frac{1}{x}\int_0^x f(y)dy$. Show that $T$ is not a compact operator on $L^2([0,1])$ and that $T$ is bounded. For the second part, I can show $T$ is bounded by looking at $\|Tf\|_2$ and rewriting it by integration by parts and then apply the Cauchy-Schwartz inequality. However, I was not able to find a bounded sequence of $L^2$ functions so that its image under $T$ is not precompact in $L^2$. Any help is tremendously appreciated.","For $f \in L^2([0,1])$, define operator $Tf: x \mapsto \frac{1}{x}\int_0^x f(y)dy$. Show that $T$ is not a compact operator on $L^2([0,1])$ and that $T$ is bounded. For the second part, I can show $T$ is bounded by looking at $\|Tf\|_2$ and rewriting it by integration by parts and then apply the Cauchy-Schwartz inequality. However, I was not able to find a bounded sequence of $L^2$ functions so that its image under $T$ is not precompact in $L^2$. Any help is tremendously appreciated.",,"['real-analysis', 'functional-analysis', 'compact-operators']"
4,Prove this integral related to the Ising model,Prove this integral related to the Ising model,,"I came across this integral when learning the Ising model. Without external field Onsager's solution of a 2D square lattice with $J_2=0$ should equal the solution of a 1D Ising model, which leads to this $$ \int_0^{2\pi}\ln \left(\cosh 2K-\sinh 2K\cos\theta\right)\,\mathrm{d}\theta=4\pi\ln\cosh K $$ where $K\in\mathbb{R}$. I tested this integral numerically using Mathematica and it holds. But I want to prove it. I tried splitting the LHS to $$ 2\pi\ln\cosh2K+\int_0^{2\pi}\ln(1-\tanh 2K\cos\theta)\,\mathrm{d}\theta $$ and expanding the log function using Taylor series, but then the integration result became a complicated series $$ -\sum_{n=1}^\infty \frac{1}{2n}\frac{2\pi}{4^n}\binom{2n}{n}\tanh^{2n}2K $$ which does not easily relate to the RHS. Anyone has better idea?  Ising model is famous so this may have been done long ago.","I came across this integral when learning the Ising model. Without external field Onsager's solution of a 2D square lattice with $J_2=0$ should equal the solution of a 1D Ising model, which leads to this $$ \int_0^{2\pi}\ln \left(\cosh 2K-\sinh 2K\cos\theta\right)\,\mathrm{d}\theta=4\pi\ln\cosh K $$ where $K\in\mathbb{R}$. I tested this integral numerically using Mathematica and it holds. But I want to prove it. I tried splitting the LHS to $$ 2\pi\ln\cosh2K+\int_0^{2\pi}\ln(1-\tanh 2K\cos\theta)\,\mathrm{d}\theta $$ and expanding the log function using Taylor series, but then the integration result became a complicated series $$ -\sum_{n=1}^\infty \frac{1}{2n}\frac{2\pi}{4^n}\binom{2n}{n}\tanh^{2n}2K $$ which does not easily relate to the RHS. Anyone has better idea?  Ising model is famous so this may have been done long ago.",,"['calculus', 'real-analysis', 'definite-integrals', 'hyperbolic-functions', 'trigonometric-integrals']"
5,"If $fg_n$ converges in $L^p$ for all $f\in L^p$, then $g_n$ converges in $L^\infty$.","If  converges in  for all , then  converges in .",fg_n L^p f\in L^p g_n L^\infty,"Let $(X,\mathcal{A},\mu)$ be a measure space with $\mu(X)<\infty$. I know that if $g:X\rightarrow\mathbb{R}$ satisfies that $fg\in L^p$ for all $f\in L^p$, then $g\in L^\infty$. My question is whether there is a ""sequential version"" of this result: if $fg_n$ converges in $L^p$ for all $f\in L^p$, then $g_n$ converges in $L^\infty$. I would like a reference of this result (if it is true).","Let $(X,\mathcal{A},\mu)$ be a measure space with $\mu(X)<\infty$. I know that if $g:X\rightarrow\mathbb{R}$ satisfies that $fg\in L^p$ for all $f\in L^p$, then $g\in L^\infty$. My question is whether there is a ""sequential version"" of this result: if $fg_n$ converges in $L^p$ for all $f\in L^p$, then $g_n$ converges in $L^\infty$. I would like a reference of this result (if it is true).",,"['real-analysis', 'functional-analysis', 'reference-request', 'lebesgue-integral', 'lebesgue-measure']"
6,calculating the limit $\lim _{n\rightarrow \infty}((4^n+3)^{1/n}-(3^n+4)^{1/n})^{n3^n}$,calculating the limit,\lim _{n\rightarrow \infty}((4^n+3)^{1/n}-(3^n+4)^{1/n})^{n3^n},"We need to calculating the limit  $$ \lim _{n\rightarrow \infty}((4^n+3)^{1/n}-(3^n+4)^{1/n})^{n3^n} $$ I have tried taking the logarithm, but the limit doesnt seem to arrive at any familiar form.","We need to calculating the limit  $$ \lim _{n\rightarrow \infty}((4^n+3)^{1/n}-(3^n+4)^{1/n})^{n3^n} $$ I have tried taking the logarithm, but the limit doesnt seem to arrive at any familiar form.",,"['calculus', 'real-analysis', 'limits']"
7,Limit of $L^p$ Norms and Essential Supremum,Limit of  Norms and Essential Supremum,L^p,"Assume $m(E) < \infty$. For $f \in L^\infty(E)$, show that $\lim_{n \to \infty}||f||_n= ||f||_\infty$ This problem comes from Royden and Fitzpatrick's Real analysis. I think the statement is false, but hopefully someone will correct me if I am mistaken. Let $E=[0,1/2]$, and consider $f(x)=x \in L^\infty(E)$. Since $f$ is continuous, $||f||_\infty = ||f|||_{max} = 1/2$. However, $$||f||_n \le [m(E)]^{1/n} ||f||_\infty = (1/2)^{1/n}||f||_\infty,$$ which implies $$\lim_{n \to \infty} ||f||_n \le ||f||_\infty \lim_{n \to \infty} (1/2)^n = 0,$$ and therefore $\lim_{n \to \infty} ||f||_n = 0 \neq 1/2$. What am I missing? EDIT: Made a stupid error: the ""$n$"" should be replaced with ""$1/n$"". I've been working on this problem for quite some time now; I could use a gentle prod in the right direction.","Assume $m(E) < \infty$. For $f \in L^\infty(E)$, show that $\lim_{n \to \infty}||f||_n= ||f||_\infty$ This problem comes from Royden and Fitzpatrick's Real analysis. I think the statement is false, but hopefully someone will correct me if I am mistaken. Let $E=[0,1/2]$, and consider $f(x)=x \in L^\infty(E)$. Since $f$ is continuous, $||f||_\infty = ||f|||_{max} = 1/2$. However, $$||f||_n \le [m(E)]^{1/n} ||f||_\infty = (1/2)^{1/n}||f||_\infty,$$ which implies $$\lim_{n \to \infty} ||f||_n \le ||f||_\infty \lim_{n \to \infty} (1/2)^n = 0,$$ and therefore $\lim_{n \to \infty} ||f||_n = 0 \neq 1/2$. What am I missing? EDIT: Made a stupid error: the ""$n$"" should be replaced with ""$1/n$"". I've been working on this problem for quite some time now; I could use a gentle prod in the right direction.",,"['real-analysis', 'measure-theory']"
8,Upper Derivative and Increasing Function on a Compact Interval,Upper Derivative and Increasing Function on a Compact Interval,,"Definition. For a real valued function $f$ and an interior point $x$ of its domain, the upper derivative of $f$ at $x$ denoted by $\overline{D}f(x)$ is defined as follows: $$\overline{D}f(x)=\lim_{h\rightarrow0}\left[ \sup \left \{\frac{f(x+t)-f(x)}{t}: 0<|t|\leq h \right \} \right]$$ I am working through Royden and Fitzpatrick's proof of the following lemma: Lemma. Let $f$ be an increasing function on the closed, bounded interval $[a,b]$ . Then for each $\alpha>0$ , $$m^*\{x\in (a,b) : \overline{D}f(x) \geq  \alpha \} \leq \frac{1}{\alpha}[f(b)-f(a)].$$ Here is the relevant part of the proof giving me trouble. Let $\alpha>0$ . Define $E_{\alpha}:=\{x\in (a,b): \overline{D}f(x)\geq\alpha \}$ . Choose $\alpha' \in (0,\alpha)$ . Let $\mathscr{F}$ be the collection of closed, bounded intervals $[c,d]$ contained in $(a,b)$ for which $f(d)-f(c)\geq \alpha ' (d-c)$ . Since $\overline{D}f\geq \alpha$ on $E_{\alpha}$ , $\mathscr{F}$ is a Vitali covering for $E_{\alpha}$ . I realize that the questions am I about to pose have been asked here (indeed, I copied-&-pasted the relevant parts from that post), but астон вілла олоф мэллбэрг's answer is much too brief for my liking and the long string of comments, which appear to contain part of the answer to Kurome's question, are way to jumbled to get anything out of them. I have the same question as the OP in the link: why is $\scr{F} \neq \emptyset$ and why is $\scr{F}$ a Vitali covering of $E_\alpha$ . Specifically, I don't understand the implication $$t<\delta \implies\frac{f(x+t)-f(x)}{t}\geq\alpha'$$ holds (I tried unpacking the definition of the upper derivative, but I couldn't see it); nor do I understand how астон вілла олоф мэллбэрг is able to choose $d \in (a,b)$ such that $d-x < \delta$ . And why does does $[x,d]$ having a length less than $\delta$ imply $\scr{F}$ is a Vitali covering of $E_\alpha$ ? That $\delta$ wasn't arbitrary. For ease of references, here is астон вілла олоф мэллбэрг's answer: [астон вілла олоф мэллбэрг's answer]: Take any $x \in E_\alpha$ . Now, since $\overline{D}f(x)\geq\alpha$ , it follows that for some small $\delta$ , $t<\delta \implies\frac{f(x+t)-f(x)}{t}\geq\alpha'$ . (The definition for the upper derivative above is slightly wrong, I will edit it) Putting $t=d-x$ , this means that $t<\delta \implies f(d)-f(x) \geq \alpha'(d-x)$ . The interval $[d,x]$ is in $\mathscr{F}$ for every $d$ close enough to $x$ ,for arbitrary $x$ in $E_\alpha$ . This makes $\mathscr{F}$ a Vitali covering for $E_\alpha$ .","Definition. For a real valued function and an interior point of its domain, the upper derivative of at denoted by is defined as follows: I am working through Royden and Fitzpatrick's proof of the following lemma: Lemma. Let be an increasing function on the closed, bounded interval . Then for each , Here is the relevant part of the proof giving me trouble. Let . Define . Choose . Let be the collection of closed, bounded intervals contained in for which . Since on , is a Vitali covering for . I realize that the questions am I about to pose have been asked here (indeed, I copied-&-pasted the relevant parts from that post), but астон вілла олоф мэллбэрг's answer is much too brief for my liking and the long string of comments, which appear to contain part of the answer to Kurome's question, are way to jumbled to get anything out of them. I have the same question as the OP in the link: why is and why is a Vitali covering of . Specifically, I don't understand the implication holds (I tried unpacking the definition of the upper derivative, but I couldn't see it); nor do I understand how астон вілла олоф мэллбэрг is able to choose such that . And why does does having a length less than imply is a Vitali covering of ? That wasn't arbitrary. For ease of references, here is астон вілла олоф мэллбэрг's answer: [астон вілла олоф мэллбэрг's answer]: Take any . Now, since , it follows that for some small , . (The definition for the upper derivative above is slightly wrong, I will edit it) Putting , this means that . The interval is in for every close enough to ,for arbitrary in . This makes a Vitali covering for .","f x f x \overline{D}f(x) \overline{D}f(x)=\lim_{h\rightarrow0}\left[ \sup \left \{\frac{f(x+t)-f(x)}{t}: 0<|t|\leq h \right \} \right] f [a,b] \alpha>0 m^*\{x\in (a,b) : \overline{D}f(x) \geq 
\alpha \} \leq \frac{1}{\alpha}[f(b)-f(a)]. \alpha>0 E_{\alpha}:=\{x\in (a,b): \overline{D}f(x)\geq\alpha \} \alpha' \in (0,\alpha) \mathscr{F} [c,d] (a,b) f(d)-f(c)\geq \alpha ' (d-c) \overline{D}f\geq \alpha E_{\alpha} \mathscr{F} E_{\alpha} \scr{F} \neq \emptyset \scr{F} E_\alpha t<\delta \implies\frac{f(x+t)-f(x)}{t}\geq\alpha' d \in (a,b) d-x < \delta [x,d] \delta \scr{F} E_\alpha \delta x \in E_\alpha \overline{D}f(x)\geq\alpha \delta t<\delta \implies\frac{f(x+t)-f(x)}{t}\geq\alpha' t=d-x t<\delta \implies f(d)-f(x) \geq \alpha'(d-x) [d,x] \mathscr{F} d x x E_\alpha \mathscr{F} E_\alpha","['real-analysis', 'measure-theory', 'proof-explanation']"
9,Smooth function vanishing on axis,Smooth function vanishing on axis,,"suppose you have a smooth function $f : \mathbb{R}^{2} \rightarrow \mathbb{R}$ such that $f(x,y) = 0$ if either of the co-ordinates $x$ and $y$ are $0$ (i.e. it vanishes on both  of the co-ordinate axes). Does  there exist constants $k>0$ and $\epsilon>0$ so that $|f(x,y)| \leq k|x||y|$, when $|(x,y)| \leq\epsilon$?","suppose you have a smooth function $f : \mathbb{R}^{2} \rightarrow \mathbb{R}$ such that $f(x,y) = 0$ if either of the co-ordinates $x$ and $y$ are $0$ (i.e. it vanishes on both  of the co-ordinate axes). Does  there exist constants $k>0$ and $\epsilon>0$ so that $|f(x,y)| \leq k|x||y|$, when $|(x,y)| \leq\epsilon$?",,"['real-analysis', 'multivariable-calculus']"
10,Showing that g is integrable and $\int^b_a{f}$ = $\int^b_a{g}$,Showing that g is integrable and  =,\int^b_a{f} \int^b_a{g},"Let $f$ be integrable on $[a,b]$, and suppose g is a function on $[a,b]$ such that $g$($x$) = $f$($x$) except for finitely many $x$ in $[a,b]$. Show $g$ is integrable and $\int^b_a{f}$ = $\int^b_a{g}$. I started out solving this problem by letting a function $h = f - g$ such that $h(x) = 0$ expect at one point in [a,b]. Since $f$ is integrable, then there exists a partition P such that $U(f,P)-L(f,P)$ < $\epsilon$. I'm not really sure how to proceed from here. Any hints and help is much appreciated.","Let $f$ be integrable on $[a,b]$, and suppose g is a function on $[a,b]$ such that $g$($x$) = $f$($x$) except for finitely many $x$ in $[a,b]$. Show $g$ is integrable and $\int^b_a{f}$ = $\int^b_a{g}$. I started out solving this problem by letting a function $h = f - g$ such that $h(x) = 0$ expect at one point in [a,b]. Since $f$ is integrable, then there exists a partition P such that $U(f,P)-L(f,P)$ < $\epsilon$. I'm not really sure how to proceed from here. Any hints and help is much appreciated.",,"['real-analysis', 'partitions-for-integration']"
11,"Calculate this integral $\int_a^b (\int_a^b \frac{f(t) \overline{f(s)}}{1-ts} \,ds) \, dt$",Calculate this integral,"\int_a^b (\int_a^b \frac{f(t) \overline{f(s)}}{1-ts} \,ds) \, dt","How to calculate this kind of integrals? $$\int_a^b \left(\int_a^b \frac{f(t) \overline{f(s)}}{1-ts} \, ds\right) dt$$ $a=0$, $0<b<1$, $t,s \in [a,b]$ are real, and $f$ ""lives"" in $C([a,b], \mathbb{C})$ I have to find that it's equal to $\sum_{n=0}^{+\infty} \left|\int_a^b f(t) t^n \, dt\right|^2.$ I just know that $\sum\limits_n (st)^n = \dfrac{1}{1-ts} \dots$ Could someone help me?","How to calculate this kind of integrals? $$\int_a^b \left(\int_a^b \frac{f(t) \overline{f(s)}}{1-ts} \, ds\right) dt$$ $a=0$, $0<b<1$, $t,s \in [a,b]$ are real, and $f$ ""lives"" in $C([a,b], \mathbb{C})$ I have to find that it's equal to $\sum_{n=0}^{+\infty} \left|\int_a^b f(t) t^n \, dt\right|^2.$ I just know that $\sum\limits_n (st)^n = \dfrac{1}{1-ts} \dots$ Could someone help me?",,"['real-analysis', 'integration']"
12,"For what values of $(a,b,c)$ is $M$ a manifold?",For what values of  is  a manifold?,"(a,b,c) M","Let $M$ denote the subset of $\mathbb{R}^3$ defined by the equations: $x^2 +y^2 + z^2 = 1$ $ax^2 + by^2 + cz^2 = 0$ Where $a, b,$ and $c$ are constants. For what values of $(a,b,c)$ is $M$ a manifold? Is $M$ compact? Here's my attempt at a solution: Define $(u,v)$ = $F(x,y,z)$ = $(x^2 +y^2 + z^2 - 1, ax^2 + by^2 + cz^2)$ The Jacobian is computed to be: $\begin{bmatrix}2x & 2y & 2z\\2ax & 2by & 2cz\end{bmatrix}$ Solving for when the $2 \times 2$ minor matrices are zero we get: $4bxy - 4axy = 0$ $4cxz - 4axz = 0$ $4cyz - 4byz = 0$ Since the point $(x,y,z) = (0,0,0)$ is not in the solution set to the original given equations, we can divide by $xy, xz, yz$ respectively. This results in $a = b =c$. So my answer is, so long as $a \neq b \neq c$, $M$ is a manifold. Is this reasoning correct? And how can I show that $M$ is compact? Obviously the unit sphere is compact, but what about it's intersection with $ax^2 + by^2 + cz^2 = 0$ ? Can somebody please help? Thank you kindly.","Let $M$ denote the subset of $\mathbb{R}^3$ defined by the equations: $x^2 +y^2 + z^2 = 1$ $ax^2 + by^2 + cz^2 = 0$ Where $a, b,$ and $c$ are constants. For what values of $(a,b,c)$ is $M$ a manifold? Is $M$ compact? Here's my attempt at a solution: Define $(u,v)$ = $F(x,y,z)$ = $(x^2 +y^2 + z^2 - 1, ax^2 + by^2 + cz^2)$ The Jacobian is computed to be: $\begin{bmatrix}2x & 2y & 2z\\2ax & 2by & 2cz\end{bmatrix}$ Solving for when the $2 \times 2$ minor matrices are zero we get: $4bxy - 4axy = 0$ $4cxz - 4axz = 0$ $4cyz - 4byz = 0$ Since the point $(x,y,z) = (0,0,0)$ is not in the solution set to the original given equations, we can divide by $xy, xz, yz$ respectively. This results in $a = b =c$. So my answer is, so long as $a \neq b \neq c$, $M$ is a manifold. Is this reasoning correct? And how can I show that $M$ is compact? Obviously the unit sphere is compact, but what about it's intersection with $ax^2 + by^2 + cz^2 = 0$ ? Can somebody please help? Thank you kindly.",,"['real-analysis', 'general-topology', 'multivariable-calculus', 'manifolds']"
13,On real roots of a polynomial equation,On real roots of a polynomial equation,,"Let $f(x)=x^3 + ax^2 + bx + c$ where $a,b,c \in \mathbb R$ such that if $f(r)=0$ , then $ f ' (r) \ne 0$ i.e. $f$ has no double-root in $\mathbb C$ i.e. $f$ has three distinct roots with at least one real root. Let $g(x) = 2 f''(x) f(x) - (f'(x))^2$. Then how to show that $g$ has exactly two real roots ? If $r< s$ are the two real roots of $g(x)$, then how to show that $f(r) <0$ and $f(s) >0$ ? Since $g'(x)=12 f(x)$, so $g$ has degree $4$ and since $f$ has no double root , so all the 4 roots of $g$ are also distinct. I am unable to say anything else. Please help.","Let $f(x)=x^3 + ax^2 + bx + c$ where $a,b,c \in \mathbb R$ such that if $f(r)=0$ , then $ f ' (r) \ne 0$ i.e. $f$ has no double-root in $\mathbb C$ i.e. $f$ has three distinct roots with at least one real root. Let $g(x) = 2 f''(x) f(x) - (f'(x))^2$. Then how to show that $g$ has exactly two real roots ? If $r< s$ are the two real roots of $g(x)$, then how to show that $f(r) <0$ and $f(s) >0$ ? Since $g'(x)=12 f(x)$, so $g$ has degree $4$ and since $f$ has no double root , so all the 4 roots of $g$ are also distinct. I am unable to say anything else. Please help.",,"['real-analysis', 'complex-analysis']"
14,"Prove that $\int_E \log fd\mu \leqslant \mu(E) \, \log \left[\frac{1}{\mu(E)} \right]$ for strictly positive measure $\mu$",Prove that  for strictly positive measure,"\int_E \log fd\mu \leqslant \mu(E) \, \log \left[\frac{1}{\mu(E)} \right] \mu","Let $(X, \Sigma, \mu)$ be a measurable space, with strictly positive measure $\mu$. Let $f: X \rightarrow (0, \infty)$ be a function such that $\int_X fd\mu=1$. Show that for every $\mu$-measurable set $E \in \Sigma$ with $0 < \mu(E)<\infty$ the following inequality holds: $$\int_E \log fd\mu \leqslant \mu(E) \, \log \left[\frac{1}{\mu(E)} \right]$$ I'm dealing with this quiestion quite a bit and can't seems to find a solution. My approach: We define new measure $\nu = f \cdot \mu$. Since $f>0$ and $\mu$ strictly positive, then $\nu$ is strictly positive. Moreover, $\nu(X) \equiv \int_Xdv = \int_Xd(f\cdot\mu) =\int_Xfd\mu = 1$. Therefore, by the Jansen inequality (concaved version): For every integrable function $g$ (according to $\mu$ ) such that $g:X \rightarrow (0,\infty)$ and concave function $F:(0, \infty) \rightarrow$ R (R for the real numbers): $$\int_X F\circ g d\nu \leq F \biggl(\int_X g d\nu \biggl)$$ I thought about taking $F(x) = \log(x)$ but I can't seem to find the right $g$ and how to conclude it to the integral overall given set $E$. Does anyone have a solution for this? Thanks. Edit: I got it. Take $g=f$ and for every set E above define $\nu_E = \frac{1}{\mu(E)} \cdot \mu$. Clearly, $\nu(E)=1$ there fore by the Jensen inequality we get: $$\int_E \frac{1}{\mu(E)} \cdot \log f d\mu  \leq \log \biggl(\int_E f \frac{1}{\mu(E)} \cdot d\mu \biggl)$$ Now we play with the last inequality and we get an equivalent one: $$\int_E \log fd\mu \leqslant \mu(E) \, \log \left[\frac{1}{\mu(E)} \right]$$","Let $(X, \Sigma, \mu)$ be a measurable space, with strictly positive measure $\mu$. Let $f: X \rightarrow (0, \infty)$ be a function such that $\int_X fd\mu=1$. Show that for every $\mu$-measurable set $E \in \Sigma$ with $0 < \mu(E)<\infty$ the following inequality holds: $$\int_E \log fd\mu \leqslant \mu(E) \, \log \left[\frac{1}{\mu(E)} \right]$$ I'm dealing with this quiestion quite a bit and can't seems to find a solution. My approach: We define new measure $\nu = f \cdot \mu$. Since $f>0$ and $\mu$ strictly positive, then $\nu$ is strictly positive. Moreover, $\nu(X) \equiv \int_Xdv = \int_Xd(f\cdot\mu) =\int_Xfd\mu = 1$. Therefore, by the Jansen inequality (concaved version): For every integrable function $g$ (according to $\mu$ ) such that $g:X \rightarrow (0,\infty)$ and concave function $F:(0, \infty) \rightarrow$ R (R for the real numbers): $$\int_X F\circ g d\nu \leq F \biggl(\int_X g d\nu \biggl)$$ I thought about taking $F(x) = \log(x)$ but I can't seem to find the right $g$ and how to conclude it to the integral overall given set $E$. Does anyone have a solution for this? Thanks. Edit: I got it. Take $g=f$ and for every set E above define $\nu_E = \frac{1}{\mu(E)} \cdot \mu$. Clearly, $\nu(E)=1$ there fore by the Jensen inequality we get: $$\int_E \frac{1}{\mu(E)} \cdot \log f d\mu  \leq \log \biggl(\int_E f \frac{1}{\mu(E)} \cdot d\mu \biggl)$$ Now we play with the last inequality and we get an equivalent one: $$\int_E \log fd\mu \leqslant \mu(E) \, \log \left[\frac{1}{\mu(E)} \right]$$",,"['real-analysis', 'integration', 'measure-theory', 'inequality', 'jensen-inequality']"
15,Series unchanged by rearrangement implies absolute convergence?,Series unchanged by rearrangement implies absolute convergence?,,"If a series converges absolutely, then it is known that the value of the series is independent of rearrangements. More precisely, if $\sum |a_n| < \infty$ and $\sigma:\Bbb N\to \Bbb N$ is a bijection then $\sum a_{\sigma(n)}$ converges and its value is independent of $\sigma$ . Now what about the converse? That is,  if $\{a_n\}_{n=1}^\infty$ is an arbitrary real or complex sequence such that for every bijection $\sigma$ of $\Bbb N$ , we have $\sum a_{\sigma(n)}$ converges to the same value,  is it true that $\sum a_n$ coverges absolutely? I am interested in this in the context of signed measures on a measurable space à la Stein's and Shakarchi's definition of signed measures in their third volume on real analysis.","If a series converges absolutely, then it is known that the value of the series is independent of rearrangements. More precisely, if and is a bijection then converges and its value is independent of . Now what about the converse? That is,  if is an arbitrary real or complex sequence such that for every bijection of , we have converges to the same value,  is it true that coverges absolutely? I am interested in this in the context of signed measures on a measurable space à la Stein's and Shakarchi's definition of signed measures in their third volume on real analysis.",\sum |a_n| < \infty \sigma:\Bbb N\to \Bbb N \sum a_{\sigma(n)} \sigma \{a_n\}_{n=1}^\infty \sigma \Bbb N \sum a_{\sigma(n)} \sum a_n,['real-analysis']
16,Sequential characterization of continuity - Is proof by contradiction unavoidable?,Sequential characterization of continuity - Is proof by contradiction unavoidable?,,"I have a question that's been bothering me lately. Theorem. Let $f:X\rightarrow Y$ be a map between metric spaces. Then $f$ is continuous at $x_{0}$ if and only if $x_{n}\rightarrow x_{0}$ always implies $f(x_{n})\rightarrow f(x_{0})$ . Here is the standard proof that I'm aware of. Proof. $(\implies).$ Let $(x_{n})$ be a sequence in $X$ that converges to $x_{0}$ . Let $\epsilon > 0$ . By continuity of $f$ , there exists $\delta > 0$ such that $$ d(x, x_{0}) < \delta \quad \implies\quad d(f(x), f(x_{0})) < \epsilon.  $$ By convergence of our sequence, choose $N\in\mathbb{N}$ such that $n\ge N$ implies $d(x_{n}, x_{0}) < \delta$ .   Then the above implies $$ n\ge N \quad \implies \quad d(f(x_{n}), f(x_{0})) < \epsilon, $$ and so $f(x_{n})\rightarrow f(x_{0})$ . $(\impliedby).$ For this, we will prove the contrapositive. Assume $f$ is not continuous at $x_{0}$ . Then there exists $\epsilon > 0$ such that for all $\delta > 0$ , there exists some $d(\tilde{x}, x_{0}) < \delta$ that has $d(f(\tilde{x}), f(x_{0}))\ge \epsilon$ . We use this to construct the following sequence. For $\delta = \tfrac{1}{n}$ we choose $x_{n}$ to be the element such that $d(x_{n}, x_{0}) < \tfrac{1}{n}$ but $d(f(x_{n}), f(x_{0}))\ge\epsilon$ .   Then we have $x_{n}\rightarrow x_{0}$ but $f(x_{n})\not\rightarrow f(x_{0})$ . $\;\square$ For the backwards direction, we used the contrapositive, which boils down to proof by contradiction, as far as I'm aware. Usually I'm okay with these kinds of proofs, but I find it strange that we need to use proof by contradiction. I can't seem to find a proof that doesn't rely on contradiction here. My Attempt Suppose $x_{n}\rightarrow x_{0}$ always implies $f(x_{n})\rightarrow f(x_{0})$ . Let $\epsilon > 0$ . I want to show that there exists $\delta > 0$ such that $d(x, x_{0})<\delta$ implies $d(f(x), f(x_{0}))<\epsilon$ . Perhaps I can consider the set of all possible sequences $S = (x_{n})$ for which $d(x_{n}, x_{0}) < \tfrac{1}{n}$ . By hypothesis, each sequence $S$ has an index $N_{S}$ for which $n\ge N_{S} \implies d(f(x_{n}),f(x_{0}))<\epsilon$ . By the well-ordering principle, we may take $N_{S}$ to be the least such natural number for $S$ . Let's say I take $\delta = \inf \tfrac{1}{N_{S}}$ (where the infimum is over all those $S$ ). What exactly prevents us from having $\delta = 0$ ? I see that every sequence $S$ has $N_{S}$ , but if there are uncountably many possible $S$ , it feels as though you can choose some contrived sequence that gets closer and closer to $x_{0}$ but has nonetheless an arbitrarily large $N_{S}$ . My Questions My main question is, is the use of the law of non-contradiction unavoidable? Is there a convincing argument one way or the other?","I have a question that's been bothering me lately. Theorem. Let be a map between metric spaces. Then is continuous at if and only if always implies . Here is the standard proof that I'm aware of. Proof. Let be a sequence in that converges to . Let . By continuity of , there exists such that By convergence of our sequence, choose such that implies .   Then the above implies and so . For this, we will prove the contrapositive. Assume is not continuous at . Then there exists such that for all , there exists some that has . We use this to construct the following sequence. For we choose to be the element such that but .   Then we have but . For the backwards direction, we used the contrapositive, which boils down to proof by contradiction, as far as I'm aware. Usually I'm okay with these kinds of proofs, but I find it strange that we need to use proof by contradiction. I can't seem to find a proof that doesn't rely on contradiction here. My Attempt Suppose always implies . Let . I want to show that there exists such that implies . Perhaps I can consider the set of all possible sequences for which . By hypothesis, each sequence has an index for which . By the well-ordering principle, we may take to be the least such natural number for . Let's say I take (where the infimum is over all those ). What exactly prevents us from having ? I see that every sequence has , but if there are uncountably many possible , it feels as though you can choose some contrived sequence that gets closer and closer to but has nonetheless an arbitrarily large . My Questions My main question is, is the use of the law of non-contradiction unavoidable? Is there a convincing argument one way or the other?","f:X\rightarrow Y f x_{0} x_{n}\rightarrow x_{0} f(x_{n})\rightarrow f(x_{0}) (\implies). (x_{n}) X x_{0} \epsilon > 0 f \delta > 0  d(x, x_{0}) < \delta \quad \implies\quad d(f(x), f(x_{0})) < \epsilon.   N\in\mathbb{N} n\ge N d(x_{n}, x_{0}) < \delta  n\ge N \quad \implies \quad d(f(x_{n}), f(x_{0})) < \epsilon,  f(x_{n})\rightarrow f(x_{0}) (\impliedby). f x_{0} \epsilon > 0 \delta > 0 d(\tilde{x}, x_{0}) < \delta d(f(\tilde{x}), f(x_{0}))\ge \epsilon \delta = \tfrac{1}{n} x_{n} d(x_{n}, x_{0}) < \tfrac{1}{n} d(f(x_{n}), f(x_{0}))\ge\epsilon x_{n}\rightarrow x_{0} f(x_{n})\not\rightarrow f(x_{0}) \;\square x_{n}\rightarrow x_{0} f(x_{n})\rightarrow f(x_{0}) \epsilon > 0 \delta > 0 d(x, x_{0})<\delta d(f(x), f(x_{0}))<\epsilon S = (x_{n}) d(x_{n}, x_{0}) < \tfrac{1}{n} S N_{S} n\ge N_{S} \implies d(f(x_{n}),f(x_{0}))<\epsilon N_{S} S \delta = \inf \tfrac{1}{N_{S}} S \delta = 0 S N_{S} S x_{0} N_{S}","['real-analysis', 'sequences-and-series', 'continuity', 'constructive-mathematics']"
17,"Wedge products, dual basis and exterior algebra in my real analysis book","Wedge products, dual basis and exterior algebra in my real analysis book",,"I'm reading a book on Real Analysis and there's a chapter about surface integrals. It starts by defining the wedge product, and then it starts defining differential forms of some degree. It starts like this: Let's use the notation $\{dx_1, \ldots, dx_m\}$ for the canonical   basis of $(\mathbb{R}^m)^*$, dual of the basis $\{e_1, \ldots, e_m\}\subset\mathbb{R}^m$, where $e_1 = (1,0,\ldots, 0)$ etc. For each   set $I = \{i_1< \cdots < i_r\}\subset \{1,2,\ldots, m\}$, let's write $$dx_I = dx_{i_1}\wedge \cdots \wedge dx_{i_r}$$ the $r$-linear alternating forms $d_{x_I}$ make the canonical basis of   the vector space $A_r(\mathbb{R}^m)$ (space of the $r$ linear maps   from $\mathbb{R}^m$ to $\mathbb{R}$). Given a list of $r$ vectors $v_1,\ldots, v_r\in\mathbb{R}^m$, we   obtain a matrix $a = (a_{ij})$, with $m$ lines and $r$ columns, at   which the $j$-th column is the vector $v_j = (a_{1j}, \ldots,  a_{mk})$. In this case: $$d_{x_I}(v_1, \ldots, v_r) = \det(a_I)$$ where $a_I$ is the matrix $r\times r$ obtained selecting the lines   such that its indexes belong to the set $I$ I know that here, $dx_1, \ldots$ are just names for the dual basis elements, but it has something to do with $dx$ of integrals. What is this relation? Why are we even using dual basis vectors, for what they're useful? I've been googling things and found that this is related to something called exterior algebra, which uses a lot of wedge products, but I'm completely lost here, I don't get anything about the usefulness of wedge products and its relation to dual vectors. Could somebody help me?","I'm reading a book on Real Analysis and there's a chapter about surface integrals. It starts by defining the wedge product, and then it starts defining differential forms of some degree. It starts like this: Let's use the notation $\{dx_1, \ldots, dx_m\}$ for the canonical   basis of $(\mathbb{R}^m)^*$, dual of the basis $\{e_1, \ldots, e_m\}\subset\mathbb{R}^m$, where $e_1 = (1,0,\ldots, 0)$ etc. For each   set $I = \{i_1< \cdots < i_r\}\subset \{1,2,\ldots, m\}$, let's write $$dx_I = dx_{i_1}\wedge \cdots \wedge dx_{i_r}$$ the $r$-linear alternating forms $d_{x_I}$ make the canonical basis of   the vector space $A_r(\mathbb{R}^m)$ (space of the $r$ linear maps   from $\mathbb{R}^m$ to $\mathbb{R}$). Given a list of $r$ vectors $v_1,\ldots, v_r\in\mathbb{R}^m$, we   obtain a matrix $a = (a_{ij})$, with $m$ lines and $r$ columns, at   which the $j$-th column is the vector $v_j = (a_{1j}, \ldots,  a_{mk})$. In this case: $$d_{x_I}(v_1, \ldots, v_r) = \det(a_I)$$ where $a_I$ is the matrix $r\times r$ obtained selecting the lines   such that its indexes belong to the set $I$ I know that here, $dx_1, \ldots$ are just names for the dual basis elements, but it has something to do with $dx$ of integrals. What is this relation? Why are we even using dual basis vectors, for what they're useful? I've been googling things and found that this is related to something called exterior algebra, which uses a lot of wedge products, but I'm completely lost here, I don't get anything about the usefulness of wedge products and its relation to dual vectors. Could somebody help me?",,"['real-analysis', 'linear-algebra', 'integration', 'exterior-algebra']"
18,"Show that the space $(X,d)$ is not complete and prove its completion",Show that the space  is not complete and prove its completion,"(X,d)","Let $X = \left\{(x_n):\sum\limits_{n=1}^\infty n|x_n|<\infty\right\}$, $d(x,y) = \sup|x_n - y_n|$. Show that the space $(X,d)$ is not complete. Prove that the space $c_0$ is its completion. For the solution, I know that a metric space $(X,d)$ is complete if every Cauchy sequence $(x_n)$ in $X$ converges to a point in $X$ so for the first part I need to find a Cauchy sequence that does not converges to a point in $X$. For $(x_n) = (1/n^3)_{n=1}^\infty$ and $(y_n) = (1/n^4)_{n=1}^\infty,$  $d(x,y)$ not converges. Thus $X$ is not complete. Is my counter example right? For the second part, proving $c_o$ is its completion I am having trouble about what kind of approach I should follow.","Let $X = \left\{(x_n):\sum\limits_{n=1}^\infty n|x_n|<\infty\right\}$, $d(x,y) = \sup|x_n - y_n|$. Show that the space $(X,d)$ is not complete. Prove that the space $c_0$ is its completion. For the solution, I know that a metric space $(X,d)$ is complete if every Cauchy sequence $(x_n)$ in $X$ converges to a point in $X$ so for the first part I need to find a Cauchy sequence that does not converges to a point in $X$. For $(x_n) = (1/n^3)_{n=1}^\infty$ and $(y_n) = (1/n^4)_{n=1}^\infty,$  $d(x,y)$ not converges. Thus $X$ is not complete. Is my counter example right? For the second part, proving $c_o$ is its completion I am having trouble about what kind of approach I should follow.",,"['real-analysis', 'convergence-divergence']"
19,Splitting the representation of $\mathbb{R}$ on $L^2(\mathbb{R})$ into irreducibles.,Splitting the representation of  on  into irreducibles.,\mathbb{R} L^2(\mathbb{R}),"On p2 of these notes , the author writes that the translation action of $\mathbb{R}$ on $L^2(\mathbb{R})$ is a reducible representation which does not contain any irreducible subrepresentations. Presumably by $L^2(\mathbb{R})$ he means the square integrable functions $f : \mathbb{R}\rightarrow\mathbb{C}$. ""Indeed, for any measurable set $S\subset\mathbb{R}$, the space of functions $f$ for which the Fourier transform $\hat{f}$ has support in $S$ is a proper closed invariant subspace."" This space is certainly invariant - how would one see that it is closed? ""By Schur's lemma, any irreducible subrepresentation $W\subset L^2(\mathbb{R})$ would have dimension 1, ie $W = \langle F\rangle$ where $F: \mathbb{R}\rightarrow\mathbb{C}$ is a function such that $F(x+r)$ is proportional to $F(x)$. Thus, $F(x)$ is proportional to $e^{\lambda x}$ for some $\lambda\in\mathbb{C}$, but no such function lies in $L^2$"" No issues with this. ""However, a representation ""decomposes"" into a (possibly uncountable) number of irreducibles. This is the theory of ""unitary disintegration"""" This is my main question - what does he mean by this? Googling ""unitary disintegration"" did not yield any relevant results. Furthermore, how can a representation decompose into irreducibles, but have no irreducible subrepresentations? References would be appreciated.","On p2 of these notes , the author writes that the translation action of $\mathbb{R}$ on $L^2(\mathbb{R})$ is a reducible representation which does not contain any irreducible subrepresentations. Presumably by $L^2(\mathbb{R})$ he means the square integrable functions $f : \mathbb{R}\rightarrow\mathbb{C}$. ""Indeed, for any measurable set $S\subset\mathbb{R}$, the space of functions $f$ for which the Fourier transform $\hat{f}$ has support in $S$ is a proper closed invariant subspace."" This space is certainly invariant - how would one see that it is closed? ""By Schur's lemma, any irreducible subrepresentation $W\subset L^2(\mathbb{R})$ would have dimension 1, ie $W = \langle F\rangle$ where $F: \mathbb{R}\rightarrow\mathbb{C}$ is a function such that $F(x+r)$ is proportional to $F(x)$. Thus, $F(x)$ is proportional to $e^{\lambda x}$ for some $\lambda\in\mathbb{C}$, but no such function lies in $L^2$"" No issues with this. ""However, a representation ""decomposes"" into a (possibly uncountable) number of irreducibles. This is the theory of ""unitary disintegration"""" This is my main question - what does he mean by this? Googling ""unitary disintegration"" did not yield any relevant results. Furthermore, how can a representation decompose into irreducibles, but have no irreducible subrepresentations? References would be appreciated.",,"['real-analysis', 'general-topology']"
20,Why is this a contradiction? Egoroff's Thm when $m(E) = \infty$,Why is this a contradiction? Egoroff's Thm when,m(E) = \infty,"Problem 27 : Show that the conclusion of Egoroff’s Theorem can fail if we drop the assumption that the domain has finite measure. Solution : Consider sequence $f_n(x) = χ_{[n,\infty)}(x)$. Clearly $f_n \to 0$ pointwise on $\mathbb{R}$. Suppose that there existed a set $F$ such that $m(\mathbb{R} \setminus F) < \epsilon$ and $f_n \to 0$ uniformly on $F$. Since $f_n$ is an indicator function, this means we can find an $N$ such that $f_N = 0$ on $F$. This implies $F \subset (-\infty, N)$ and so $[N, \infty) \subset \mathbb{R} \setminus F$, giving $m([N, \infty)) \le m(\mathbb{R} \setminus F) < \epsilon$, which is a contradiction. I don't understand the last line.  What is being contradicted?  Does $f$ not converge uniformly?  Why?","Problem 27 : Show that the conclusion of Egoroff’s Theorem can fail if we drop the assumption that the domain has finite measure. Solution : Consider sequence $f_n(x) = χ_{[n,\infty)}(x)$. Clearly $f_n \to 0$ pointwise on $\mathbb{R}$. Suppose that there existed a set $F$ such that $m(\mathbb{R} \setminus F) < \epsilon$ and $f_n \to 0$ uniformly on $F$. Since $f_n$ is an indicator function, this means we can find an $N$ such that $f_N = 0$ on $F$. This implies $F \subset (-\infty, N)$ and so $[N, \infty) \subset \mathbb{R} \setminus F$, giving $m([N, \infty)) \le m(\mathbb{R} \setminus F) < \epsilon$, which is a contradiction. I don't understand the last line.  What is being contradicted?  Does $f$ not converge uniformly?  Why?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
21,Intuition of dual space action in dual of $\ell_p$,Intuition of dual space action in dual of,\ell_p,"For $p>1,$ the sequence space $\ell_p$ is defined as $$\ell_p=\left\{ x=(x_i)_{i\in\mathbb{N}} : \sum_{i=1}^\infty |x_i|^p < \infty \right\}.$$ A classical duality theorem of $\ell_p$ space is that $\ell_p^* \cong \ell_q$ where $1/p + 1/q = 1$ and $\cong$ means isometrically isomorphic and $\ell_p^*$ means dual space of $\ell_p.$ One way to show the isometrically isomorphism is the following: For any $\eta=(\eta_i)_{i\in\mathbb{N}} \in \ell_q,$ define a map $\phi_{\eta}:\ell_q\to\mathbb{R}$ given by the dual space action, that is, $$\phi_\eta(\xi) = \sum_{i=1}^\infty \eta_i \, \xi_i$$ where $\xi=(\xi_i)_{i\in\mathbb{N}} \in \ell_q.$ Then one proceeds to show that $\|\eta\|_q = \| \phi_\eta \|$ to obtain into isometry from $\ell_q$ into $\ell_p*.$ Question : What is the intuition behind the dual space action? When I try to show that $\ell_q \cong \ell_p^*$ myself, I could not think of the action. So I am wondering what triggers one to think of the dual space action.","For the sequence space is defined as A classical duality theorem of space is that where and means isometrically isomorphic and means dual space of One way to show the isometrically isomorphism is the following: For any define a map given by the dual space action, that is, where Then one proceeds to show that to obtain into isometry from into Question : What is the intuition behind the dual space action? When I try to show that myself, I could not think of the action. So I am wondering what triggers one to think of the dual space action.","p>1, \ell_p \ell_p=\left\{ x=(x_i)_{i\in\mathbb{N}} : \sum_{i=1}^\infty |x_i|^p < \infty \right\}. \ell_p \ell_p^* \cong \ell_q 1/p + 1/q = 1 \cong \ell_p^* \ell_p. \eta=(\eta_i)_{i\in\mathbb{N}} \in \ell_q, \phi_{\eta}:\ell_q\to\mathbb{R} \phi_\eta(\xi) = \sum_{i=1}^\infty \eta_i \, \xi_i \xi=(\xi_i)_{i\in\mathbb{N}} \in \ell_q. \|\eta\|_q = \| \phi_\eta \| \ell_q \ell_p*. \ell_q \cong \ell_p^*","['real-analysis', 'functional-analysis', 'lp-spaces', 'dual-spaces']"
22,"Prob. 7 (a), Chap. 6, in Baby Rudin: If $f$ is integrable on $[c, 1]$ for every $c>0$, then $\int_0^1 f(x) \ \mathrm{d}x = $ . . .","Prob. 7 (a), Chap. 6, in Baby Rudin: If  is integrable on  for every , then  . . .","f [c, 1] c>0 \int_0^1 f(x) \ \mathrm{d}x = ","Here is Prob. 7, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real function on $(0, 1]$ and $f \in \mathscr{R}$ on $[c, 1]$ for every $c > 0$. Define    $$ \int_0^1 f(x) \ \mathrm{d} x = \lim_{c \to 0} \int_c^1 f(x) \ \mathrm{d} x $$    if this limit exists (and is finite). (a) If $f \in \mathscr{R}$ on $[0, 1]$, show that this definition of the integral agrees with the old one. (b) Construct a function $f$ such that the above limit exists, although it fails to exist with $\lvert f \rvert$ in place of $f$. Here I'll only attempt Part (a): My Attempt: Here is the link to a post of mine here on Math SE where I've copied the definition of the Riemann and Riemann-Stieltjes integral that Rudin uses (i.e. Definitions 6.1 and 6.2 in Baby Rudin, 3rd edition): Theorem 6.10 in Baby Rudin: If $f$ is bounded on $[a, b]$ with only finitely many points of discontinuity at which $\alpha$ is continuous, then As $f \in \mathscr{R}$ on $[0, 1]$, so $\int_0^1 f(x) \ \mathrm{d} x$ exists in $\mathbb{R}$. According to the statement of the problem, we only need to show that    $$ \lim_{c \to 0+} \int_c^1 f(x) \ \mathrm{d} x = \int_0^1 f(x) \ \mathrm{d} x. \tag{0}$$ Let $\varepsilon > 0$ be given. We need to find a real number $\delta> 0$ such that    $$ \left\lvert \int_c^1 f(x) \ \mathrm{d} x \ - \ \int_0^1 f(x) \ \mathrm{d} x \right\rvert < \varepsilon \tag{1} $$   for any real number $c$ such that $0 < c < \delta$. Now let's choose a real number $\delta_0 \in (0, 1)$, and let us choose $c$ such that $0 < c < \delta_0$. Then as $f \in \mathscr{R}$ on $[0, 1]$ and as $c \in (0, 1)$, so by Theorem 6.12 (c) in Baby Rudin $f \in \mathscr{R}$ on $[0, c]$ and on $[c, 1]$, and    $$ \int_0^c f(x) \ \mathrm{d} x \ + \ \int_c^1 f(x) \ \mathrm{d} x = \int_0^1 f(x) \ \mathrm{d} x. \tag{2} $$ Here is the link to my Math SE post on Theorem 6.12 (c) in Baby Rudin, 3rd edition: Theorem 6.12 (c) in Baby Rudin: If $f\in\mathscr{R}(\alpha)$ on $[a, b]$ and $a<c<b$, then $f\in\mathscr{R}(\alpha)$ on $[a, c]$ and $[c, b]$ In the light of (1) and (2), we can conclude that we now only need to show that there exists a real number $\delta > 0$ such that    $$ \left\lvert \int_0^c f(x) \ \mathrm{d} x \right\rvert < \varepsilon \tag{3} $$    for any real number $c$ such that $0 < c < \delta$, and we now also know that    $ 0 < c < \delta_0 < 1$. As $f \in \mathscr{R}$ on $[0, 1]$, so $f$ is also bounded on $[0, 1]$ and hence also on $[0, c]$. Let $M \colon= \sup \{ \ f(x) \ \colon \ 0 \leq x \leq c \ \}$. Then by Theorem 6.12 (d) in Baby Rudin, we have    $$ \left\lvert \int_0^c f(x) \ \mathrm{d} x \right\rvert \leq M c. \tag{4} $$ Here is the link to my Math SE post on Theorem 6.12 (d) in Baby Rudin, 3rd edition: Theorem 6.12 (d) in Baby Rudin: If $\lvert f(x) \rvert \leq M$ on $[a, b]$, then $\lvert \int_a^b f d\alpha \rvert \leq \ldots$ So if we choose our $\delta$ such that $$0 < \delta < \min \left\{ \ \delta_0, \frac{\varepsilon}{M+1} \ \right\}, $$    then, for any real number $c$ such that $0 < c < \delta$, we have $0 < c < \delta_0$ so that $c \in (0, 1)$ and from (4) we also have    $$ \left\lvert \int_0^c f(x) \ \mathrm{d} x \right\rvert \leq M c \leq  \frac{M \varepsilon}{M+1} < \varepsilon, $$    which by virtue of (3) implies that (1) holds. Since $\varepsilon > 0$ was arbitrary, therefore (0) holds as well, as required. Is this proof correct and rigorous enough for Rudin? If not, then where is it lacking? Is this proof the same as the proof asked for by Rudin?","Here is Prob. 7, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real function on $(0, 1]$ and $f \in \mathscr{R}$ on $[c, 1]$ for every $c > 0$. Define    $$ \int_0^1 f(x) \ \mathrm{d} x = \lim_{c \to 0} \int_c^1 f(x) \ \mathrm{d} x $$    if this limit exists (and is finite). (a) If $f \in \mathscr{R}$ on $[0, 1]$, show that this definition of the integral agrees with the old one. (b) Construct a function $f$ such that the above limit exists, although it fails to exist with $\lvert f \rvert$ in place of $f$. Here I'll only attempt Part (a): My Attempt: Here is the link to a post of mine here on Math SE where I've copied the definition of the Riemann and Riemann-Stieltjes integral that Rudin uses (i.e. Definitions 6.1 and 6.2 in Baby Rudin, 3rd edition): Theorem 6.10 in Baby Rudin: If $f$ is bounded on $[a, b]$ with only finitely many points of discontinuity at which $\alpha$ is continuous, then As $f \in \mathscr{R}$ on $[0, 1]$, so $\int_0^1 f(x) \ \mathrm{d} x$ exists in $\mathbb{R}$. According to the statement of the problem, we only need to show that    $$ \lim_{c \to 0+} \int_c^1 f(x) \ \mathrm{d} x = \int_0^1 f(x) \ \mathrm{d} x. \tag{0}$$ Let $\varepsilon > 0$ be given. We need to find a real number $\delta> 0$ such that    $$ \left\lvert \int_c^1 f(x) \ \mathrm{d} x \ - \ \int_0^1 f(x) \ \mathrm{d} x \right\rvert < \varepsilon \tag{1} $$   for any real number $c$ such that $0 < c < \delta$. Now let's choose a real number $\delta_0 \in (0, 1)$, and let us choose $c$ such that $0 < c < \delta_0$. Then as $f \in \mathscr{R}$ on $[0, 1]$ and as $c \in (0, 1)$, so by Theorem 6.12 (c) in Baby Rudin $f \in \mathscr{R}$ on $[0, c]$ and on $[c, 1]$, and    $$ \int_0^c f(x) \ \mathrm{d} x \ + \ \int_c^1 f(x) \ \mathrm{d} x = \int_0^1 f(x) \ \mathrm{d} x. \tag{2} $$ Here is the link to my Math SE post on Theorem 6.12 (c) in Baby Rudin, 3rd edition: Theorem 6.12 (c) in Baby Rudin: If $f\in\mathscr{R}(\alpha)$ on $[a, b]$ and $a<c<b$, then $f\in\mathscr{R}(\alpha)$ on $[a, c]$ and $[c, b]$ In the light of (1) and (2), we can conclude that we now only need to show that there exists a real number $\delta > 0$ such that    $$ \left\lvert \int_0^c f(x) \ \mathrm{d} x \right\rvert < \varepsilon \tag{3} $$    for any real number $c$ such that $0 < c < \delta$, and we now also know that    $ 0 < c < \delta_0 < 1$. As $f \in \mathscr{R}$ on $[0, 1]$, so $f$ is also bounded on $[0, 1]$ and hence also on $[0, c]$. Let $M \colon= \sup \{ \ f(x) \ \colon \ 0 \leq x \leq c \ \}$. Then by Theorem 6.12 (d) in Baby Rudin, we have    $$ \left\lvert \int_0^c f(x) \ \mathrm{d} x \right\rvert \leq M c. \tag{4} $$ Here is the link to my Math SE post on Theorem 6.12 (d) in Baby Rudin, 3rd edition: Theorem 6.12 (d) in Baby Rudin: If $\lvert f(x) \rvert \leq M$ on $[a, b]$, then $\lvert \int_a^b f d\alpha \rvert \leq \ldots$ So if we choose our $\delta$ such that $$0 < \delta < \min \left\{ \ \delta_0, \frac{\varepsilon}{M+1} \ \right\}, $$    then, for any real number $c$ such that $0 < c < \delta$, we have $0 < c < \delta_0$ so that $c \in (0, 1)$ and from (4) we also have    $$ \left\lvert \int_0^c f(x) \ \mathrm{d} x \right\rvert \leq M c \leq  \frac{M \varepsilon}{M+1} < \varepsilon, $$    which by virtue of (3) implies that (1) holds. Since $\varepsilon > 0$ was arbitrary, therefore (0) holds as well, as required. Is this proof correct and rigorous enough for Rudin? If not, then where is it lacking? Is this proof the same as the proof asked for by Rudin?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'definite-integrals']"
23,"Is there a function which is ""midpoint linear"" but not linear?","Is there a function which is ""midpoint linear"" but not linear?",,"Let a function $f:\mathbb{R}\rightarrow\mathbb{R}$ be midpoint linear if for all $x,y\in\mathbb{R}$, $f(\frac{x+y}{2})=\frac{f(x)+f(y)}{2}$. If a midpoint linear function $f$ is continuous, then I believe you can show through a density argument that $f$ is also linear. However, if we drop the assumption of continuity, I wonder if we can construct (or at least describe) a counterexample that is midpoint linear function but is not in fact linear. Because midpoint convexity does not imply convexity without the additional assumption of continuity, I believe that such a counterexample should exist, and that it will likely be through a Vitali set sort of argument that involves taking a basis for $\mathbb{R}$ over $\mathbb{Q}$. I am not sure of the details though.","Let a function $f:\mathbb{R}\rightarrow\mathbb{R}$ be midpoint linear if for all $x,y\in\mathbb{R}$, $f(\frac{x+y}{2})=\frac{f(x)+f(y)}{2}$. If a midpoint linear function $f$ is continuous, then I believe you can show through a density argument that $f$ is also linear. However, if we drop the assumption of continuity, I wonder if we can construct (or at least describe) a counterexample that is midpoint linear function but is not in fact linear. Because midpoint convexity does not imply convexity without the additional assumption of continuity, I believe that such a counterexample should exist, and that it will likely be through a Vitali set sort of argument that involves taking a basis for $\mathbb{R}$ over $\mathbb{Q}$. I am not sure of the details though.",,"['real-analysis', 'continuity', 'linear-transformations', 'examples-counterexamples']"
24,Prove that $\alpha_{m}=\sum_{n=1}^{\infty}\frac{1}{(n!)^{m}}$ is irrational for all $m\geq 1$.,Prove that  is irrational for all .,\alpha_{m}=\sum_{n=1}^{\infty}\frac{1}{(n!)^{m}} m\geq 1,"I came across this question and went through the proof that $e$ is irrational with a few minor tweaks. I hope someone can have a look through it and hopefully check it or tidy it up. Thanks. Proof : To begin, note that for $m\geq 1$: $$\alpha_{m} \,=\, 1 + \sum_{n=2}^{\infty}\frac{1}{(n!)^{m}} \,>\, 1 $$ and  $$\alpha_{m} \,=\, -1 + \sum_{n=0}^{\infty}\frac{1}{(n!)^{m}} \,<\, -1 + \sum_{n=0}^{\infty}\frac{1}{n!} \,=\, -1 + e \,<\, 2 .$$ Thus $1<\alpha_{m}<2$ for all $m\geq 1$ and so $\alpha_{m}\notin\mathbb{Z}$. Now assume (for contradiction) that $\alpha_{m}\in\mathbb{Q}$: $$\exists\, p,q\in\mathbb{N} \quad\text{with}\quad q\,>\,1\quad : \quad \alpha_{m}=\frac{p}{q}.$$ Since $p,q\in\mathbb{N}$ and $m\geq 1$:  $$(q!)^{m}\alpha_{m} \;=\; (q!)^{m}\cdot \frac{p}{q} \;=\; q!\,(q!)^{m-1}\cdot \frac{p}{q} \;=\;  (q-1)!(q!)^{m-1}p$$ and hence $(q!)^{m}\alpha_{m}\in\mathbb{Z}$. Now $$\begin{align} (q!)^{m}\alpha_{m} &\;=\; (q!)^{m}\sum_{n=1}^{\infty}\frac{1}{(n!)^{m}} \;=\;  (q!)^{m}\left(\sum_{n=1}^{q}\frac{1}{(n!)^{m}} + \sum_{n=q+1}^{\infty}\frac{1}{(n!)^{m}}  \right)  \\[0.2cm] &\;=\; \sum_{n=1}^{q}\left(\frac{q!}{(n!)}\right)^{m} + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m} \end{align}$$ for some $N\in\mathbb{Z}$, since $n!|q!$ for each $n\leq q$. In particular, since the series in the last term is positive, we have the bound: $$N \;<\; (q!)^{m}\alpha_{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m}.$$ Considering the sum: $$\begin{align} \sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} &\;=\; \left(\frac{q!}{(q+1)!}\right)^{m} + \left(\frac{q!}{(q+2)!}\right)^{m} + \left(\frac{q!}{(q+3)!}\right)^{m} + \cdots \\[0.2cm] &\;=\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+2)^{m}} + \frac{1}{(q+1)^{m}(q+2)^{m}(q+3)^{m}}+ \cdots \\[0.2cm] &\;<\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+1)^{m}(q+1)^{m}} + \cdots \\[0.2cm] &\;=\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{2m}} + \frac{1}{(q+1)^{3m}} + \cdots. \end{align}$$ This is a geometric series with ratio $0<\frac{1}{(q+1)^{m}}<1$ for all $m\geq 1$. Hence $$\sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} \;=\; \frac{\frac{1}{(q+1)^{m}}}{1-\frac{1}{(q+1)^{m}}} \;=\; \frac{1}{(q+1)^{m}-1} \;<\; 1$$ for all $m\geq 1$ since $(q+1)^{m}>1$. Thus we have arrived at: $$N \;<\; (q!)^{m}\alpha_{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} \;<\; N + 1.$$ But no integer exists in the interval $(N,N+1)$ and hence we have a contradiction; our assumption must be false and $\alpha_{m}\notin\mathbb{Q} \; \blacksquare$.","I came across this question and went through the proof that $e$ is irrational with a few minor tweaks. I hope someone can have a look through it and hopefully check it or tidy it up. Thanks. Proof : To begin, note that for $m\geq 1$: $$\alpha_{m} \,=\, 1 + \sum_{n=2}^{\infty}\frac{1}{(n!)^{m}} \,>\, 1 $$ and  $$\alpha_{m} \,=\, -1 + \sum_{n=0}^{\infty}\frac{1}{(n!)^{m}} \,<\, -1 + \sum_{n=0}^{\infty}\frac{1}{n!} \,=\, -1 + e \,<\, 2 .$$ Thus $1<\alpha_{m}<2$ for all $m\geq 1$ and so $\alpha_{m}\notin\mathbb{Z}$. Now assume (for contradiction) that $\alpha_{m}\in\mathbb{Q}$: $$\exists\, p,q\in\mathbb{N} \quad\text{with}\quad q\,>\,1\quad : \quad \alpha_{m}=\frac{p}{q}.$$ Since $p,q\in\mathbb{N}$ and $m\geq 1$:  $$(q!)^{m}\alpha_{m} \;=\; (q!)^{m}\cdot \frac{p}{q} \;=\; q!\,(q!)^{m-1}\cdot \frac{p}{q} \;=\;  (q-1)!(q!)^{m-1}p$$ and hence $(q!)^{m}\alpha_{m}\in\mathbb{Z}$. Now $$\begin{align} (q!)^{m}\alpha_{m} &\;=\; (q!)^{m}\sum_{n=1}^{\infty}\frac{1}{(n!)^{m}} \;=\;  (q!)^{m}\left(\sum_{n=1}^{q}\frac{1}{(n!)^{m}} + \sum_{n=q+1}^{\infty}\frac{1}{(n!)^{m}}  \right)  \\[0.2cm] &\;=\; \sum_{n=1}^{q}\left(\frac{q!}{(n!)}\right)^{m} + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m} \end{align}$$ for some $N\in\mathbb{Z}$, since $n!|q!$ for each $n\leq q$. In particular, since the series in the last term is positive, we have the bound: $$N \;<\; (q!)^{m}\alpha_{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m}.$$ Considering the sum: $$\begin{align} \sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} &\;=\; \left(\frac{q!}{(q+1)!}\right)^{m} + \left(\frac{q!}{(q+2)!}\right)^{m} + \left(\frac{q!}{(q+3)!}\right)^{m} + \cdots \\[0.2cm] &\;=\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+2)^{m}} + \frac{1}{(q+1)^{m}(q+2)^{m}(q+3)^{m}}+ \cdots \\[0.2cm] &\;<\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+1)^{m}(q+1)^{m}} + \cdots \\[0.2cm] &\;=\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{2m}} + \frac{1}{(q+1)^{3m}} + \cdots. \end{align}$$ This is a geometric series with ratio $0<\frac{1}{(q+1)^{m}}<1$ for all $m\geq 1$. Hence $$\sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} \;=\; \frac{\frac{1}{(q+1)^{m}}}{1-\frac{1}{(q+1)^{m}}} \;=\; \frac{1}{(q+1)^{m}-1} \;<\; 1$$ for all $m\geq 1$ since $(q+1)^{m}>1$. Thus we have arrived at: $$N \;<\; (q!)^{m}\alpha_{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} \;<\; N + 1.$$ But no integer exists in the interval $(N,N+1)$ and hence we have a contradiction; our assumption must be false and $\alpha_{m}\notin\mathbb{Q} \; \blacksquare$.",,"['real-analysis', 'sequences-and-series', 'proof-verification']"
25,consequence of supremum of a series,consequence of supremum of a series,,"Given $f: \mathbb N \to \mathbb R$. Asume that  $$A:=\sup_{k\geqslant 0}\Big| \sum_{n=0}^\infty \frac{t^n}{n!}f(k+n)\Big|<\infty,$$ for some $t\in (0,1)$, can we conclude that $f$ is bounded on $\mathbb N$? My proof as follow: If $f\geqslant 0$, then $\infty > A \geqslant \sup_{k\geqslant 0}|f(k)|$ (consider at $n=0$). Otherwise, we write $f=f^+-f^-$ and can we apply the above to get the conclusion? This is the point that I'm not sure.","Given $f: \mathbb N \to \mathbb R$. Asume that  $$A:=\sup_{k\geqslant 0}\Big| \sum_{n=0}^\infty \frac{t^n}{n!}f(k+n)\Big|<\infty,$$ for some $t\in (0,1)$, can we conclude that $f$ is bounded on $\mathbb N$? My proof as follow: If $f\geqslant 0$, then $\infty > A \geqslant \sup_{k\geqslant 0}|f(k)|$ (consider at $n=0$). Otherwise, we write $f=f^+-f^-$ and can we apply the above to get the conclusion? This is the point that I'm not sure.",,"['real-analysis', 'sequences-and-series']"
26,"Convergence of $ \int_0^{1/2}\frac{1}{\sin x\ln x}\, dx $",Convergence of," \int_0^{1/2}\frac{1}{\sin x\ln x}\, dx ","Does the following integral converge or diverge?    $$ \int_0^{1/2}\frac{1}{\sin x\ln x}\, dx $$ When $x \in [0,1/2] , \sin x>0$ and $\ln x<0$. Therefore $-1/(\sin x\ln x)>0$, which means I can use the comparision test. $$ \int_0^{1/2} \frac{1}{\sin x\ln x}dx  <  \int_0^{1/2} \frac{1}{\ln x}dx $$ I don't know what to do next in order to prove convergence (which I think is the answer). Hints will be appreciated.","Does the following integral converge or diverge?    $$ \int_0^{1/2}\frac{1}{\sin x\ln x}\, dx $$ When $x \in [0,1/2] , \sin x>0$ and $\ln x<0$. Therefore $-1/(\sin x\ln x)>0$, which means I can use the comparision test. $$ \int_0^{1/2} \frac{1}{\sin x\ln x}dx  <  \int_0^{1/2} \frac{1}{\ln x}dx $$ I don't know what to do next in order to prove convergence (which I think is the answer). Hints will be appreciated.",,"['real-analysis', 'convergence-divergence', 'improper-integrals']"
27,Abel's Test for convergence proof,Abel's Test for convergence proof,,"I am working from ""Understanding Analysis"" by Abbot and the following is an exercise that works through the proof of Abel's Test. I reproduce the question and the solution. I am confused at a section of the proof towards the end. Any clarifications would be great. Abel's Test for convergence states that if the series   $\sum_{n=1}^{\infty}x_n$ converges, and if $(y_n)$ is a sequence   satisfying $y_1 \ge y_2 \ge \cdots \ge 0$, then the series   $\sum_{n=1}^{\infty}x_ny_n$ converges. (a) Assume that $\sum_{n=1}^{\infty}a_n$ has partial sums that are   bounded by a constant $A>0$ and assume $b_1 \ge b_2 \ge \cdots \ge 0$.   Use summation by parts to show that $|\sum_{j=1}^{n} a_jb_j | \le  2Ab_1$. Let $A>0$ be an upper bound for the partial sums $s_n$ of $\sum_{n=1}^{\infty}a_n$, hence \begin{align} |\sum_{j=1}^{n} a_jb_j | &= |s_n b_{n+1} - s_m b_{m+1} + \sum_{j=m+1}^{n} s_j(b_j-b_{j+1})|\le \\ & \le Ab_{n+1} + Ab_{m+1} + A(b_{m+1}-b_{n+1})= \\ & =2Ab_{m+1} \le 2Ab_1 \end{align} (b) Prove Abel's Test by setting $a_n = x_{m+n}$ and $b_n = y_{m+n}$. To show that $\sum_{n=1}^{\infty}x_ny_n$ converges, we use the Cauchy Criterion. Let $\epsilon > 0$, we need to show that there exists an $N$ such that if $n > m \ge N$, it follows that $|\sum_{j=m+1}^{n}x_jy_j|<\epsilon$. Let $a_n = x_{m+n}$ and $b_n = y_{m+n}$, then from part (a), we have \begin{align} |\sum_{j=m+1}^{n}x_jy_j| = |\sum_{j=1}^{n-m}a_jb_j| \le 2Ab_1, \end{align} where $A$ is an upper bound on the partial sums of $\sum_{n=1}^{\infty}a_n=\sum_{j=m+1}^{\infty}x_j$. Since $\sum_{n=1}^{\infty}x_n$ converges, then by the Cauchy Criterion, we can pick $N$ such that $n > m \ge N$ implies $|\sum_{j=m+1}^{n}x_j| < \frac{\epsilon}{2y_1}$. Up until this point, I clearly understand every step of the proof, but the following is when I get confused: Looking again at what the constant $A$ represents, it follows that if $n > m \ge N$, then  \begin{align} A \le |\sum_{j=m+1}^{n}x_j| < \frac{\epsilon}{2y_1}. \end{align} How can the above be true? $A$ is the upper bound of the partial sum of the series $\sum_{j=m+1}^{\infty}x_j$, so by definition, we should have $|\sum_{j=m+1}^{n}x_j| \le A$ for all $n > m$. So, why is the above inequality $A \le |\sum_{j=m+1}^{n}x_j|$?","I am working from ""Understanding Analysis"" by Abbot and the following is an exercise that works through the proof of Abel's Test. I reproduce the question and the solution. I am confused at a section of the proof towards the end. Any clarifications would be great. Abel's Test for convergence states that if the series   $\sum_{n=1}^{\infty}x_n$ converges, and if $(y_n)$ is a sequence   satisfying $y_1 \ge y_2 \ge \cdots \ge 0$, then the series   $\sum_{n=1}^{\infty}x_ny_n$ converges. (a) Assume that $\sum_{n=1}^{\infty}a_n$ has partial sums that are   bounded by a constant $A>0$ and assume $b_1 \ge b_2 \ge \cdots \ge 0$.   Use summation by parts to show that $|\sum_{j=1}^{n} a_jb_j | \le  2Ab_1$. Let $A>0$ be an upper bound for the partial sums $s_n$ of $\sum_{n=1}^{\infty}a_n$, hence \begin{align} |\sum_{j=1}^{n} a_jb_j | &= |s_n b_{n+1} - s_m b_{m+1} + \sum_{j=m+1}^{n} s_j(b_j-b_{j+1})|\le \\ & \le Ab_{n+1} + Ab_{m+1} + A(b_{m+1}-b_{n+1})= \\ & =2Ab_{m+1} \le 2Ab_1 \end{align} (b) Prove Abel's Test by setting $a_n = x_{m+n}$ and $b_n = y_{m+n}$. To show that $\sum_{n=1}^{\infty}x_ny_n$ converges, we use the Cauchy Criterion. Let $\epsilon > 0$, we need to show that there exists an $N$ such that if $n > m \ge N$, it follows that $|\sum_{j=m+1}^{n}x_jy_j|<\epsilon$. Let $a_n = x_{m+n}$ and $b_n = y_{m+n}$, then from part (a), we have \begin{align} |\sum_{j=m+1}^{n}x_jy_j| = |\sum_{j=1}^{n-m}a_jb_j| \le 2Ab_1, \end{align} where $A$ is an upper bound on the partial sums of $\sum_{n=1}^{\infty}a_n=\sum_{j=m+1}^{\infty}x_j$. Since $\sum_{n=1}^{\infty}x_n$ converges, then by the Cauchy Criterion, we can pick $N$ such that $n > m \ge N$ implies $|\sum_{j=m+1}^{n}x_j| < \frac{\epsilon}{2y_1}$. Up until this point, I clearly understand every step of the proof, but the following is when I get confused: Looking again at what the constant $A$ represents, it follows that if $n > m \ge N$, then  \begin{align} A \le |\sum_{j=m+1}^{n}x_j| < \frac{\epsilon}{2y_1}. \end{align} How can the above be true? $A$ is the upper bound of the partial sum of the series $\sum_{j=m+1}^{\infty}x_j$, so by definition, we should have $|\sum_{j=m+1}^{n}x_j| \le A$ for all $n > m$. So, why is the above inequality $A \le |\sum_{j=m+1}^{n}x_j|$?",,['real-analysis']
28,Subsets of a vector space that are convex and have convex complement.,Subsets of a vector space that are convex and have convex complement.,,"Let $V$ be a vector space over some field $F$. Is there a characterization of the subsets $A$ that are convex and also have convex complement? Maybe when the dimension is finite it is possible? I think that in $\mathbb R^n$ we can define it recursively, by using nested semihyperplanes.","Let $V$ be a vector space over some field $F$. Is there a characterization of the subsets $A$ that are convex and also have convex complement? Maybe when the dimension is finite it is possible? I think that in $\mathbb R^n$ we can define it recursively, by using nested semihyperplanes.",,"['real-analysis', 'general-topology', 'geometry', 'convex-geometry']"
29,Is inversion of bounded homeomorphisms continuous with the uniform metric?,Is inversion of bounded homeomorphisms continuous with the uniform metric?,,"Consider the metric space $H$ of uniformly continuous homeomorphisms (with uniformly continuous inverses) of some open bounded subset $G$ of $\mathbb C$, with the uniform metric $$d(f, g) = \sup_{z \in G} \left|f(z) - g(z)\right|.$$ It is clear that $(f, g) \mapsto f \circ g$ is a continuous map $H \times H \to H$ (and in fact this is due to the uniform metric, not the uniform continuity of any particular homeomorphism), but is inversion $f \mapsto f^{-1}$ a continuous map $H \to H$? In other words, is it the case that whenever $(f_n) \to f$ uniformly on $G$, where $f$ and each $f_n$ is a uniformly continuous homeomorphism of $G$ with a uniformly continuous inverse, then $(f_n^{-1}) \to f^{-1}$ uniformly on $G$?","Consider the metric space $H$ of uniformly continuous homeomorphisms (with uniformly continuous inverses) of some open bounded subset $G$ of $\mathbb C$, with the uniform metric $$d(f, g) = \sup_{z \in G} \left|f(z) - g(z)\right|.$$ It is clear that $(f, g) \mapsto f \circ g$ is a continuous map $H \times H \to H$ (and in fact this is due to the uniform metric, not the uniform continuity of any particular homeomorphism), but is inversion $f \mapsto f^{-1}$ a continuous map $H \to H$? In other words, is it the case that whenever $(f_n) \to f$ uniformly on $G$, where $f$ and each $f_n$ is a uniformly continuous homeomorphism of $G$ with a uniformly continuous inverse, then $(f_n^{-1}) \to f^{-1}$ uniformly on $G$?",,"['real-analysis', 'general-topology', 'metric-spaces']"
30,"Derivative and Antiderivative: Suppose $F: [a,b]\rightarrow \mathbb{R}$ is differentiable on $[a,b]$.",Derivative and Antiderivative: Suppose  is differentiable on .,"F: [a,b]\rightarrow \mathbb{R} [a,b]","Suppose $F: [a,b]\rightarrow \mathbb{R}$ is differentiable on $[a,b]$. Does it follow that $\int_{a}^{b} F' = F(b)-F(a)$? This is an interesting question. Don't we have to assume that $F'(x)$ is always equal to $f(x)$ for this to work? In other words, does $F(x)$ being differentiable always guarantee that $F'(x)$ is integrable? Thanks!","Suppose $F: [a,b]\rightarrow \mathbb{R}$ is differentiable on $[a,b]$. Does it follow that $\int_{a}^{b} F' = F(b)-F(a)$? This is an interesting question. Don't we have to assume that $F'(x)$ is always equal to $f(x)$ for this to work? In other words, does $F(x)$ being differentiable always guarantee that $F'(x)$ is integrable? Thanks!",,"['real-analysis', 'integration']"
31,"Detail of a proof ""Sobolev inequality $\Rightarrow$ Isoperimetric inequality"".","Detail of a proof ""Sobolev inequality  Isoperimetric inequality"".",\Rightarrow,"From: Sobolev inequality: For all $u\in C_c^{\infty}(\mathbb{R}^n)$ $$\|u\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}\leq C \|\nabla u\|_{L^1(\mathbb{R}^n)}.$$ I want to prove: Isoperimetric inequality: For every bounded open set $E\subseteq\mathbb{R}^n$ with boundary $\partial E$ of class $C^1$, $$|E|^{\frac{n-1}{n}}\leq C|\partial E|.$$ The idea is to put $u=\chi_E$ in the Sobolev inequality, because in such a case $\|u\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}=|E|^{\frac{n-1}{n}}$ is the left hand-side of the isoperimetric inequality. The problem is that $\chi_E$ is not in the Sobolev space $W^{1,1}(\mathbb{R}^n)$. Thus, we regularize via convolutions: define $u_\epsilon=\chi_E\ast \rho_\epsilon$, where $\rho_\epsilon(x)=(1/\epsilon^n)\rho(x/\epsilon)$, $\rho\in C_c^\infty(\mathbb{R}^n)$, $\text{support}(\rho)\subseteq B(0,1)$, $\rho\geq0$ and $\int \rho=1$. We have: $$C\|\nabla u_\epsilon\|_{L^1(\mathbb{R}^n)}\geq \|u_\epsilon\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}\stackrel{\epsilon\rightarrow0^+}{\longrightarrow} \|\chi_E\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}=|E|^{\frac{n-1}{n}}.$$ We want to prove that $\|\nabla u_\epsilon\|_{L^1(\mathbb{R}^n)}\leq |\partial E|$ for all $\epsilon>0$. We use the following result: For all $v\in C_c^{\infty}(\mathbb{R}^n)$ $$\int_{\mathbb{R}^n}|\nabla v|\,dx=\sup\left\{-\int_{\mathbb{R}^n}\nabla v\cdot X\,dx:\,X\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\,|X(x)|=\left(\sum_{i=1}^n (X^i(x))^2\right)^{\frac12}\leq 1\right\}.$$ It suffices then $$-\int_{\mathbb{R}^n}\nabla u_\epsilon\cdot X\,dx\leq |\partial E|$$ for all $X\in C_c^1(\mathbb{R}^n,\mathbb{R}^n)$ and $|X|=(\sum_{i=1}^n (X^i)^2)^{\frac12}\leq 1$. We have, using integration by parts, $$-\int_{\mathbb{R}^n}\nabla u_\epsilon\cdot X\,dx=\int_{\mathbb{R}^n}u_\epsilon\,\text{Div}X\,dx=\int_E \text{Div}X_\epsilon\,dx=\int_{\partial E}X_\epsilon\cdot\nu\,d\sigma,$$ where $(X_\epsilon)^i=\rho_\epsilon\ast X^i$. It suffices to see that $|X_\epsilon(x)|\leq 1$ using $|X(x)|\leq 1$. I was not able to prove this. Any idea?","From: Sobolev inequality: For all $u\in C_c^{\infty}(\mathbb{R}^n)$ $$\|u\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}\leq C \|\nabla u\|_{L^1(\mathbb{R}^n)}.$$ I want to prove: Isoperimetric inequality: For every bounded open set $E\subseteq\mathbb{R}^n$ with boundary $\partial E$ of class $C^1$, $$|E|^{\frac{n-1}{n}}\leq C|\partial E|.$$ The idea is to put $u=\chi_E$ in the Sobolev inequality, because in such a case $\|u\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}=|E|^{\frac{n-1}{n}}$ is the left hand-side of the isoperimetric inequality. The problem is that $\chi_E$ is not in the Sobolev space $W^{1,1}(\mathbb{R}^n)$. Thus, we regularize via convolutions: define $u_\epsilon=\chi_E\ast \rho_\epsilon$, where $\rho_\epsilon(x)=(1/\epsilon^n)\rho(x/\epsilon)$, $\rho\in C_c^\infty(\mathbb{R}^n)$, $\text{support}(\rho)\subseteq B(0,1)$, $\rho\geq0$ and $\int \rho=1$. We have: $$C\|\nabla u_\epsilon\|_{L^1(\mathbb{R}^n)}\geq \|u_\epsilon\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}\stackrel{\epsilon\rightarrow0^+}{\longrightarrow} \|\chi_E\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}=|E|^{\frac{n-1}{n}}.$$ We want to prove that $\|\nabla u_\epsilon\|_{L^1(\mathbb{R}^n)}\leq |\partial E|$ for all $\epsilon>0$. We use the following result: For all $v\in C_c^{\infty}(\mathbb{R}^n)$ $$\int_{\mathbb{R}^n}|\nabla v|\,dx=\sup\left\{-\int_{\mathbb{R}^n}\nabla v\cdot X\,dx:\,X\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\,|X(x)|=\left(\sum_{i=1}^n (X^i(x))^2\right)^{\frac12}\leq 1\right\}.$$ It suffices then $$-\int_{\mathbb{R}^n}\nabla u_\epsilon\cdot X\,dx\leq |\partial E|$$ for all $X\in C_c^1(\mathbb{R}^n,\mathbb{R}^n)$ and $|X|=(\sum_{i=1}^n (X^i)^2)^{\frac12}\leq 1$. We have, using integration by parts, $$-\int_{\mathbb{R}^n}\nabla u_\epsilon\cdot X\,dx=\int_{\mathbb{R}^n}u_\epsilon\,\text{Div}X\,dx=\int_E \text{Div}X_\epsilon\,dx=\int_{\partial E}X_\epsilon\cdot\nu\,d\sigma,$$ where $(X_\epsilon)^i=\rho_\epsilon\ast X^i$. It suffices to see that $|X_\epsilon(x)|\leq 1$ using $|X(x)|\leq 1$. I was not able to prove this. Any idea?",,"['real-analysis', 'vector-analysis', 'sobolev-spaces', 'surface-integrals']"
32,"If $f(a) =f(b) =0,f(x)>0 \forall x\in(a, b) $ and $f(x) +f''(x) >0\forall x\in[a, b] $ then $b-a\geq\pi$.",If  and  then .,"f(a) =f(b) =0,f(x)>0 \forall x\in(a, b)  f(x) +f''(x) >0\forall x\in[a, b]  b-a\geq\pi","I need help in the following problem : Assume that the function $f:[a,b]\to \mathbb{R}$ satisfies $f(a)=f(b)=0, \forall x\in (a,b): f(x)>0$, for all $x\in [a,b]$ and $f+f''>0$. Prove that $b-a\geq \pi$ . Here is when I am stuck:  let $h=f+f''$ , $h$ is a positive function in $[a,b]$ and $f$ is a solution of the differential equation $y+y""=h$ By standard method of resolution we can prove that there existe two constante $u,v\in \mathbb{R}$ such that  $f(x)=u\cos(x)+v\sin(x)+\int_{a}^{x} h(t)\sin(x-t)dt$ We can notice that  $f(x)+f(x+\pi)=\int_{0}^{\pi} h(u+x)\sin(u)du \geq 0 $ , but I don't see how to conclude that $b-a \geq \pi $ , any idea ? thank you .","I need help in the following problem : Assume that the function $f:[a,b]\to \mathbb{R}$ satisfies $f(a)=f(b)=0, \forall x\in (a,b): f(x)>0$, for all $x\in [a,b]$ and $f+f''>0$. Prove that $b-a\geq \pi$ . Here is when I am stuck:  let $h=f+f''$ , $h$ is a positive function in $[a,b]$ and $f$ is a solution of the differential equation $y+y""=h$ By standard method of resolution we can prove that there existe two constante $u,v\in \mathbb{R}$ such that  $f(x)=u\cos(x)+v\sin(x)+\int_{a}^{x} h(t)\sin(x-t)dt$ We can notice that  $f(x)+f(x+\pi)=\int_{0}^{\pi} h(u+x)\sin(u)du \geq 0 $ , but I don't see how to conclude that $b-a \geq \pi $ , any idea ? thank you .",,"['calculus', 'real-analysis']"
33,Expectation of sequences of random variables that converge to 0 in probability,Expectation of sequences of random variables that converge to 0 in probability,,"Let $X_n, n \geq 1$ be a sequence of random variables that converges to zero in probability, that is, $\forall \varepsilon >0$, $$\lim_{n \to \infty} P(|X_n| < \varepsilon) = 1$$ Moreover, let $X_n=o_p(n^{-1})$, that is, $\forall \varepsilon >0$, $$\lim_{n \to \infty} P\left(\left|\frac{X_n}{n^{-1}}\right| < \varepsilon\right) = 1,$$ or equivalently, $\forall \varepsilon, \eta >0$, there exists $n_0$ such that for $n\geq n_0$, $$P\left(\left|\frac{X_n}{n^{-1}}\right| < \varepsilon\right) \geq 1-\eta,$$ My question is, what can we say about $E(X_n)$ when $n \to \infty$? For instance, is it true that $E(X_n)=o(n^{-1})$? Or more generally, is it true that $E(o_p(n^{-1}))=o(n^{-1})$? How can I prove so?","Let $X_n, n \geq 1$ be a sequence of random variables that converges to zero in probability, that is, $\forall \varepsilon >0$, $$\lim_{n \to \infty} P(|X_n| < \varepsilon) = 1$$ Moreover, let $X_n=o_p(n^{-1})$, that is, $\forall \varepsilon >0$, $$\lim_{n \to \infty} P\left(\left|\frac{X_n}{n^{-1}}\right| < \varepsilon\right) = 1,$$ or equivalently, $\forall \varepsilon, \eta >0$, there exists $n_0$ such that for $n\geq n_0$, $$P\left(\left|\frac{X_n}{n^{-1}}\right| < \varepsilon\right) \geq 1-\eta,$$ My question is, what can we say about $E(X_n)$ when $n \to \infty$? For instance, is it true that $E(X_n)=o(n^{-1})$? Or more generally, is it true that $E(o_p(n^{-1}))=o(n^{-1})$? How can I prove so?",,"['calculus', 'real-analysis', 'probability', 'probability-theory', 'asymptotics']"
34,Show the sequence $\{a_{n}\}_{n=1}^{\infty}$ is unbounded,Show the sequence  is unbounded,\{a_{n}\}_{n=1}^{\infty},"Let $a_{n}>0 ,(n=1,2,\cdots)$ and   $$\lim_{n\rightarrow\infty}\frac{a_{_{n}}}{a_{_{n+1}}+a_{_{n+2}}}=0.$$Show the sequence $\{a_{n}\}_{n=1}^{\infty}$ is unbounded. The following is my thoughts.But I am not sure my answer is right,I need someone to check it.Even though it's correct, this answer is complicated and cumbersome.Do you have some concise ways by using reduction to absurdity ? Suppose the sequence $\{a_{n}\}$ is bounded,then exists $M(>0)$ such that $0<a_{n}\leq M,(n=1,2,\cdots).$ (1). $$0<\frac{a_{n}}{2M}\leq\frac{a_{_{n}}}{a_{_{n+1}}+a_{_{n+2}}},(n=1,2,\cdots)\Rightarrow\lim_{n\rightarrow\infty}a_{n}=0 $$ (2). $$\frac{a_{n}+a_{n+1}}{a_{n+1}+a_{n+2}}=A(n)+B(n),A(n)=\frac{a_{n}}{a_{n+1}+a_{n+2}}, B(n)=\frac{a_{n+1}}{a_{n+1}+a_{n+2}}(B(n)\in (0,1],n=1,2,\cdots)\Rightarrow$$ $$0\leq\varlimsup_{n\rightarrow\infty} \frac{a_{n}}{a_{n+2}+a_{n+3}}=\varlimsup_{n\rightarrow\infty} \frac{a_{n}}{a_{n+1}+a_{n+2}}\cdot \frac{a_{n+1}+a_{n+2}}{a_{n+2}+a_{n+3}}\leq\varlimsup_{n\rightarrow\infty} \frac{a_{n}}{a_{n+1}+a_{n+2}} \cdot\varlimsup_{n\rightarrow\infty} \frac{a_{n}+a_{n+1}}{a_{n+1}+a_{n+2}}=0\Rightarrow \lim_{n\rightarrow\infty}\frac{a_{n}}{a_{n+2}+a_{n+3}}=0$$ (3). Futher,we have  $\forall p\in\mathbb{N},$ $$\lim_{n\rightarrow\infty}\frac{a_{_{n}}}{a_{_{n+p}}+a_{_{n+p+1}}}=0$$ (4). From (3) ,Let $$p_{1}=1, \exists N_{1}\in\mathbb {N},\text{such that}\frac{a_{_{n}}}{a_{_{n+1}}+a_{_{n+2}}}<\frac{1}{2}\text{ whenever} \quad n>N_{1};$$ $$p_{2}=2, \exists N_{2}\in\mathbb {N}(N_{2}>N_{1}),\text{such that}\frac{a_{_{n}}}{a_{_{n+2}}+a_{_{n+3}}}<\frac{1}{2}\text{ whenever} \quad n>N_{2};$$ $$\cdots\cdots\cdots\cdots$$$$p_{k}=k, \exists N_{k}\in\mathbb {N}(N_{k}>N_{k-1}>\cdots>N_{1}),\text{such that}\frac{a_{_{n}}}{a_{_{n+k}}+a_{_{n+k+1}}}<\frac{1}{2}\text{ whenever} \quad n>N_{k};$$$$\cdots\cdots\cdots\cdots$$ (5). From (4) ,there must exists an index $k_{0}\in \mathbb{N}$ and $n_{0}>N_{k_{0}}$ such that $a_{n_{0}}>a_{n_{0}+k_{0}}$ and $a_{n_{0}}>a_{n_{0}+k_{0}+1}.$ In fact, If for all $k\in\mathbb{N}$ and every $n>N_{k},$ we have either $a_{n}\leq a_{n+k}$ or $a_{n}\leq a_{n+k+1},$ then for every $n>N_{k},a_{n}=0$.This is contradicting  $a_{n}>0 (n=1,2,\cdots)!$ (6). From (5), $$\frac{1}{2}<\frac{a_{n_{0}}}{a_{n_{0}+k_{0}}+a_{n_{0}+k_{0}+1}}<\frac{1}{2}. \quad \text{It's impossible!}$$ From above all,we can say the sequence $\{a_{n}\}_{n=1}^{\infty}$ is unbounded.","Let $a_{n}>0 ,(n=1,2,\cdots)$ and   $$\lim_{n\rightarrow\infty}\frac{a_{_{n}}}{a_{_{n+1}}+a_{_{n+2}}}=0.$$Show the sequence $\{a_{n}\}_{n=1}^{\infty}$ is unbounded. The following is my thoughts.But I am not sure my answer is right,I need someone to check it.Even though it's correct, this answer is complicated and cumbersome.Do you have some concise ways by using reduction to absurdity ? Suppose the sequence $\{a_{n}\}$ is bounded,then exists $M(>0)$ such that $0<a_{n}\leq M,(n=1,2,\cdots).$ (1). $$0<\frac{a_{n}}{2M}\leq\frac{a_{_{n}}}{a_{_{n+1}}+a_{_{n+2}}},(n=1,2,\cdots)\Rightarrow\lim_{n\rightarrow\infty}a_{n}=0 $$ (2). $$\frac{a_{n}+a_{n+1}}{a_{n+1}+a_{n+2}}=A(n)+B(n),A(n)=\frac{a_{n}}{a_{n+1}+a_{n+2}}, B(n)=\frac{a_{n+1}}{a_{n+1}+a_{n+2}}(B(n)\in (0,1],n=1,2,\cdots)\Rightarrow$$ $$0\leq\varlimsup_{n\rightarrow\infty} \frac{a_{n}}{a_{n+2}+a_{n+3}}=\varlimsup_{n\rightarrow\infty} \frac{a_{n}}{a_{n+1}+a_{n+2}}\cdot \frac{a_{n+1}+a_{n+2}}{a_{n+2}+a_{n+3}}\leq\varlimsup_{n\rightarrow\infty} \frac{a_{n}}{a_{n+1}+a_{n+2}} \cdot\varlimsup_{n\rightarrow\infty} \frac{a_{n}+a_{n+1}}{a_{n+1}+a_{n+2}}=0\Rightarrow \lim_{n\rightarrow\infty}\frac{a_{n}}{a_{n+2}+a_{n+3}}=0$$ (3). Futher,we have  $\forall p\in\mathbb{N},$ $$\lim_{n\rightarrow\infty}\frac{a_{_{n}}}{a_{_{n+p}}+a_{_{n+p+1}}}=0$$ (4). From (3) ,Let $$p_{1}=1, \exists N_{1}\in\mathbb {N},\text{such that}\frac{a_{_{n}}}{a_{_{n+1}}+a_{_{n+2}}}<\frac{1}{2}\text{ whenever} \quad n>N_{1};$$ $$p_{2}=2, \exists N_{2}\in\mathbb {N}(N_{2}>N_{1}),\text{such that}\frac{a_{_{n}}}{a_{_{n+2}}+a_{_{n+3}}}<\frac{1}{2}\text{ whenever} \quad n>N_{2};$$ $$\cdots\cdots\cdots\cdots$$$$p_{k}=k, \exists N_{k}\in\mathbb {N}(N_{k}>N_{k-1}>\cdots>N_{1}),\text{such that}\frac{a_{_{n}}}{a_{_{n+k}}+a_{_{n+k+1}}}<\frac{1}{2}\text{ whenever} \quad n>N_{k};$$$$\cdots\cdots\cdots\cdots$$ (5). From (4) ,there must exists an index $k_{0}\in \mathbb{N}$ and $n_{0}>N_{k_{0}}$ such that $a_{n_{0}}>a_{n_{0}+k_{0}}$ and $a_{n_{0}}>a_{n_{0}+k_{0}+1}.$ In fact, If for all $k\in\mathbb{N}$ and every $n>N_{k},$ we have either $a_{n}\leq a_{n+k}$ or $a_{n}\leq a_{n+k+1},$ then for every $n>N_{k},a_{n}=0$.This is contradicting  $a_{n}>0 (n=1,2,\cdots)!$ (6). From (5), $$\frac{1}{2}<\frac{a_{n_{0}}}{a_{n_{0}+k_{0}}+a_{n_{0}+k_{0}+1}}<\frac{1}{2}. \quad \text{It's impossible!}$$ From above all,we can say the sequence $\{a_{n}\}_{n=1}^{\infty}$ is unbounded.",,"['real-analysis', 'sequences-and-series', 'limits']"
35,"By Cauchy's criterion of limit, show that $\lim_{x\to 0}(\sin{\frac{1}{x}}+x\cos{\frac{1}{x}})$ does not exist.","By Cauchy's criterion of limit, show that  does not exist.",\lim_{x\to 0}(\sin{\frac{1}{x}}+x\cos{\frac{1}{x}}),"By Cauchy's criterion of limit (not sequencial criterion), show that $$\lim_{x\to 0}(\sin{\frac{1}{x}}+x\cos{\frac{1}{x}})$$ does not exist. Cauchy's criterion of limit $\lim_{x\to c}f(x)=l$ iff for every $\epsilon>0$, there exists $\delta$ such that $$|f(x_2)-f(x_1)|<\epsilon$$ for $0<|x_1-c|<\delta$ and $0<|x_2-c|<\delta$. Please suggest $x_2, x_1$ and help me to solve the problem.","By Cauchy's criterion of limit (not sequencial criterion), show that $$\lim_{x\to 0}(\sin{\frac{1}{x}}+x\cos{\frac{1}{x}})$$ does not exist. Cauchy's criterion of limit $\lim_{x\to c}f(x)=l$ iff for every $\epsilon>0$, there exists $\delta$ such that $$|f(x_2)-f(x_1)|<\epsilon$$ for $0<|x_1-c|<\delta$ and $0<|x_2-c|<\delta$. Please suggest $x_2, x_1$ and help me to solve the problem.",,"['calculus', 'real-analysis', 'limits']"
36,"If a convex set $S \subseteq R^n$ contains no ray, can you show that it's bounded?","If a convex set  contains no ray, can you show that it's bounded?",S \subseteq R^n,"I read about a slightly different problem: Show that a closed convex set $S \subseteq R^n$ is bounded, if and only if it contains no ray, as answered here: To show a closed convex set $S \subseteq R^n$ is bounded if and only if $S$ contains no rays. I'm thinking whether the closedness condition is necessary? It's used in the original proof, but I wasn't able to construct a counterexample with the closedness condition dropped.","I read about a slightly different problem: Show that a closed convex set $S \subseteq R^n$ is bounded, if and only if it contains no ray, as answered here: To show a closed convex set $S \subseteq R^n$ is bounded if and only if $S$ contains no rays. I'm thinking whether the closedness condition is necessary? It's used in the original proof, but I wasn't able to construct a counterexample with the closedness condition dropped.",,"['real-analysis', 'general-topology', 'convex-analysis']"
37,If $a+b=2$ so $a^{a^{\frac{b}{2}}}b^{b^{\frac{a}{2}}}\geq1$,If  so,a+b=2 a^{a^{\frac{b}{2}}}b^{b^{\frac{a}{2}}}\geq1,"Let $a$ and $b$ be positive numbers such that $a+b=2$. Prove that:   $$a^{a^{\frac{b}{2}}}b^{b^{\frac{a}{2}}}\geq1$$ We can prove an easier inequality $a^{a^{b}}b^{b^{a}}\geq1$ with the same conditions by the following reasoning. Let $a\geq1\geq b>0$. Hence, $a^{b-1}\geq b^{a-1}$, which gives $a^b\geq\frac{a}{b}\cdot b^a$ and $$a^{a^{b}}b^{b^{a}}\geq a^{\frac{a}{b}\cdot b^a}b^{b^{a}}.$$ Thus, it remains to prove that $$a^{\frac{a}{b}\cdot b^a}b^{b^{a}}\geq1$$ or $$a^ab^b\geq1,$$ which follows from $$a^ab^b\geq\left(\frac{a+b}{2}\right)^{a+b},$$ which is Jensen for $f(x)=x\ln{x}$.","Let $a$ and $b$ be positive numbers such that $a+b=2$. Prove that:   $$a^{a^{\frac{b}{2}}}b^{b^{\frac{a}{2}}}\geq1$$ We can prove an easier inequality $a^{a^{b}}b^{b^{a}}\geq1$ with the same conditions by the following reasoning. Let $a\geq1\geq b>0$. Hence, $a^{b-1}\geq b^{a-1}$, which gives $a^b\geq\frac{a}{b}\cdot b^a$ and $$a^{a^{b}}b^{b^{a}}\geq a^{\frac{a}{b}\cdot b^a}b^{b^{a}}.$$ Thus, it remains to prove that $$a^{\frac{a}{b}\cdot b^a}b^{b^{a}}\geq1$$ or $$a^ab^b\geq1,$$ which follows from $$a^ab^b\geq\left(\frac{a+b}{2}\right)^{a+b},$$ which is Jensen for $f(x)=x\ln{x}$.",,"['calculus', 'real-analysis', 'inequality', 'contest-math']"
38,What happens to a Banach space of functions when the domain is not compact?,What happens to a Banach space of functions when the domain is not compact?,,"Let $X$ be a noncompact topological space. Let $V$ be a normed vector space. Consider the set of bounded continuous functions $C_B(X,V)$ from $X$ to $V$. Define a norm on $C_B(X,V)$ by $\|f\|_{\sup} = \sup_{x\in X} \| f(x) \|$ If $X$ is compact then this space is complete (and thus Banach). That is every cauchy sequence converges to a point in $C_B(X,V)$. I suspect that if $X$ is not compact then the space is no longer complete but I am not sure how to show it. I cannot think of an example of a cauchy sequence which does not converge because $X$ is not compact.","Let $X$ be a noncompact topological space. Let $V$ be a normed vector space. Consider the set of bounded continuous functions $C_B(X,V)$ from $X$ to $V$. Define a norm on $C_B(X,V)$ by $\|f\|_{\sup} = \sup_{x\in X} \| f(x) \|$ If $X$ is compact then this space is complete (and thus Banach). That is every cauchy sequence converges to a point in $C_B(X,V)$. I suspect that if $X$ is not compact then the space is no longer complete but I am not sure how to show it. I cannot think of an example of a cauchy sequence which does not converge because $X$ is not compact.",,"['real-analysis', 'general-topology', 'functional-analysis', 'banach-spaces', 'topological-vector-spaces']"
39,Academic reference concerning Minkowski's question mark function,Academic reference concerning Minkowski's question mark function,,"The Wikipedia article about Minkowski's question mark function says that ""the derivative vanishes on the rational numbers"", i.e. that $?(x)=0$ for all $x\in\mathbb Q$. However, I have not been able to find an academic paper or book which contains this result. (I have only found it stated that the derivative is zero almost everywhere, but not that the ""almost everywhere"" includes all the rationals.) Since I need to refer to this fact in an academic paper of my own, Wikipedia won't do. Can anyone help me with a source?","The Wikipedia article about Minkowski's question mark function says that ""the derivative vanishes on the rational numbers"", i.e. that $?(x)=0$ for all $x\in\mathbb Q$. However, I have not been able to find an academic paper or book which contains this result. (I have only found it stated that the derivative is zero almost everywhere, but not that the ""almost everywhere"" includes all the rationals.) Since I need to refer to this fact in an academic paper of my own, Wikipedia won't do. Can anyone help me with a source?",,"['real-analysis', 'derivatives', 'reference-request']"
40,"Prob. 2, Sec. 1.4 in Kreyszig's Functional Analysis Book: Any Cauchy sequence with a convergent subsequence converges","Prob. 2, Sec. 1.4 in Kreyszig's Functional Analysis Book: Any Cauchy sequence with a convergent subsequence converges",,"Here is Prob. 2, Sec. 1.4 in the book Introductory Functional Analysis With Applications by Erwine Kreyszig. If $\left( x_n \right)$ is Cauchy and has a convergent subsequence, say, $x_{n_k} \to x$, show that $\left( x_n \right)$ is convergent with the limit $x$. My effort: Let $(X, d)$ be the metric space in which $\left( x_n \right)$ is a Cauchy sequence, and let $\varphi \colon \mathbb{N} \to \mathbb{N}$ be a strictly increasing function such that the sequence    $\left( x_{\varphi(n)} \right)$ converges to a point $x$ of $X$. And, we can put $$n_k = \varphi(k) \ \mbox{ for all } k \in \mathbb{N}.$$ Then, given a real number $\varepsilon > 0$, we can find a natural number $N$ such that $$ (1) \ \ \ \  d \left( x_m, x_n \right) < \frac{\varepsilon}{2} \ \mbox{ for any pair } (m, n) \mbox{ of natural numbers such that } m > N \mbox{ and } n > N.$$ Now for each $k \in \mathbb{N}$, we know that $n_k = \varphi(k) \in \mathbb{N}$; moreover, $$ n_k = \varphi(k) < \varphi(r) = n_r \ \mbox{ if $k \in \mathbb{N}$, $r \in \mathbb{N}$, and $k < r$},$$ because $\varphi$ is a strictly increasing function. Thus, $n_1 = \varphi(1) \in \mathbb{N}$ and so  $n_1 \geq 1$. Let $k$ be any natural number, and suppose that $$n_k = \varphi(k) \geq k.$$ Then since $\varphi$ is a strictly increasing function and since $k+1 \in \mathbb{N}$, therefore we can conclude that    $$n_{k+1} = \varphi(k+1) > \varphi(k) \geq k.$$   But $\varphi(k+1) \in \mathbb{N}$. So we can conclude that $\varphi(k+1) \geq k+1$. Hence by induction it follows that $$n_k = \varphi(k) \geq k \mbox{ for all } k \in \mathbb{N}. $$ Thus we know that, for each $k \in \mathbb{N}$, (i) $\varphi(k) \in \mathbb{N}$ and (ii) $\varphi(k) \geq k$. Now since $$\lim_{k \to \infty} x_{\varphi(k)} = \lim_{k \to \infty} x_{n_k} = x, $$   therefore we can find a natural number $K$ such that $$ (2) \ \ \ \ d\left( x_{n_k}, x \right) = d\left( x_{\varphi(k)}, x \right) < \frac{\varepsilon}{2} \ \mbox{ for any natural number } k \mbox{ such that } k > K.$$ Now let $M$ be any natural number such that $M > \max \left( K, N \right)$. Such an $M$ exists since the set of natural numbers is not bounded above (or by the Archimedian property of $\mathbb{R}$). Let $n \in \mathbb{N}$ such that $n > M$. Then we note that    $$n_{M+1} = \varphi(M+1) \geq M+1 > M > K$$ so that (2) can be used, and we also note that $$n_{M+1} \in \mathbb{N}, \ \ \ n \in \mathbb{N},  \ \ \ n_{M+1} > M > N, \ \ \ \mbox{ and } \ n > N$$   so that (1) can be used. Then, from (1) and (2) above, we see that, $$ d\left( x_n, x\right) \leq d \left( x_n, x_{n_{M+1}} \right) + d \left( x_{n_{M+1}}, x \right) < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon,$$   for any natural number $n > M$. Thus we have shown that, corresponding to  every real number $\varepsilon > 0$, we can find a natural number $M$ such that $$ d \left( x_n, x \right) < \varepsilon \ \mbox{ for any natural number } n > N.$$   Hence the sequence  $\left(x_n \right)$ converges in the metric space $(X, d)$ to the point $x \in X$. Is the above proof correct? If so, then is the presentation good enough? If not, then where lies the flaw?","Here is Prob. 2, Sec. 1.4 in the book Introductory Functional Analysis With Applications by Erwine Kreyszig. If $\left( x_n \right)$ is Cauchy and has a convergent subsequence, say, $x_{n_k} \to x$, show that $\left( x_n \right)$ is convergent with the limit $x$. My effort: Let $(X, d)$ be the metric space in which $\left( x_n \right)$ is a Cauchy sequence, and let $\varphi \colon \mathbb{N} \to \mathbb{N}$ be a strictly increasing function such that the sequence    $\left( x_{\varphi(n)} \right)$ converges to a point $x$ of $X$. And, we can put $$n_k = \varphi(k) \ \mbox{ for all } k \in \mathbb{N}.$$ Then, given a real number $\varepsilon > 0$, we can find a natural number $N$ such that $$ (1) \ \ \ \  d \left( x_m, x_n \right) < \frac{\varepsilon}{2} \ \mbox{ for any pair } (m, n) \mbox{ of natural numbers such that } m > N \mbox{ and } n > N.$$ Now for each $k \in \mathbb{N}$, we know that $n_k = \varphi(k) \in \mathbb{N}$; moreover, $$ n_k = \varphi(k) < \varphi(r) = n_r \ \mbox{ if $k \in \mathbb{N}$, $r \in \mathbb{N}$, and $k < r$},$$ because $\varphi$ is a strictly increasing function. Thus, $n_1 = \varphi(1) \in \mathbb{N}$ and so  $n_1 \geq 1$. Let $k$ be any natural number, and suppose that $$n_k = \varphi(k) \geq k.$$ Then since $\varphi$ is a strictly increasing function and since $k+1 \in \mathbb{N}$, therefore we can conclude that    $$n_{k+1} = \varphi(k+1) > \varphi(k) \geq k.$$   But $\varphi(k+1) \in \mathbb{N}$. So we can conclude that $\varphi(k+1) \geq k+1$. Hence by induction it follows that $$n_k = \varphi(k) \geq k \mbox{ for all } k \in \mathbb{N}. $$ Thus we know that, for each $k \in \mathbb{N}$, (i) $\varphi(k) \in \mathbb{N}$ and (ii) $\varphi(k) \geq k$. Now since $$\lim_{k \to \infty} x_{\varphi(k)} = \lim_{k \to \infty} x_{n_k} = x, $$   therefore we can find a natural number $K$ such that $$ (2) \ \ \ \ d\left( x_{n_k}, x \right) = d\left( x_{\varphi(k)}, x \right) < \frac{\varepsilon}{2} \ \mbox{ for any natural number } k \mbox{ such that } k > K.$$ Now let $M$ be any natural number such that $M > \max \left( K, N \right)$. Such an $M$ exists since the set of natural numbers is not bounded above (or by the Archimedian property of $\mathbb{R}$). Let $n \in \mathbb{N}$ such that $n > M$. Then we note that    $$n_{M+1} = \varphi(M+1) \geq M+1 > M > K$$ so that (2) can be used, and we also note that $$n_{M+1} \in \mathbb{N}, \ \ \ n \in \mathbb{N},  \ \ \ n_{M+1} > M > N, \ \ \ \mbox{ and } \ n > N$$   so that (1) can be used. Then, from (1) and (2) above, we see that, $$ d\left( x_n, x\right) \leq d \left( x_n, x_{n_{M+1}} \right) + d \left( x_{n_{M+1}}, x \right) < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon,$$   for any natural number $n > M$. Thus we have shown that, corresponding to  every real number $\varepsilon > 0$, we can find a natural number $M$ such that $$ d \left( x_n, x \right) < \varepsilon \ \mbox{ for any natural number } n > N.$$   Hence the sequence  $\left(x_n \right)$ converges in the metric space $(X, d)$ to the point $x \in X$. Is the above proof correct? If so, then is the presentation good enough? If not, then where lies the flaw?",,"['real-analysis', 'sequences-and-series', 'analysis', 'metric-spaces', 'cauchy-sequences']"
41,"prove that $f(c)=\frac{1}{2}(c-a)(c-b)f''(\xi)$ for $\xi \in (a,b)$",prove that  for,"f(c)=\frac{1}{2}(c-a)(c-b)f''(\xi) \xi \in (a,b)","A function $f(x)$ is continuous on $[a,b]$ and $f''(x)$ exists for all $x\in (a,b)$. IF $c\in (a,b)$ $f(a)=f(b)=0$ prove that $f(c)=\frac{1}{2}(c-a)(c-b)f''(\xi)$ for $\xi \in (a,b)$. I have no idea to solve it. I know that I have to use Lagrange MVT. Please help.","A function $f(x)$ is continuous on $[a,b]$ and $f''(x)$ exists for all $x\in (a,b)$. IF $c\in (a,b)$ $f(a)=f(b)=0$ prove that $f(c)=\frac{1}{2}(c-a)(c-b)f''(\xi)$ for $\xi \in (a,b)$. I have no idea to solve it. I know that I have to use Lagrange MVT. Please help.",,"['calculus', 'real-analysis']"
42,Completeness of $\ell^2$ done right,Completeness of  done right,\ell^2,"One of our problem sets this quarter in analysis asked us to show that $\ell^2$, the set of all square-summable sequences, is complete. At the time, I struggled to prove this, but I have an idea after reading on in Rudin's Principles . My idea is to use uniform convergence of a sequence of continuous functions and change the order of limits. Please let me know what I need to make the following go through; I have seen many questions here on StackExchange, similar to this, where the answerer skirts the question of this exchange of limits, and I would like if someone could just answer with as many epsilons as needed to get the thing settled once and for all. We have shown that $\ell^2$ is an inner-product space, and it has a norm $\lVert \mathbf x\rVert^2 = \sum_{n\ge 1} x_n^2$. One step of the process is to come up with a candidate limit for a Cauchy sequence, and we can do this fairly easily. Let $(\mathbf x_n)$ be a Cauchy sequence in $\ell^2$ with $\mathbf x_n = x_{n,1},x_{n,2},x_{n,3},\dotsc$. We can arrange the sequences $\mathbf x_n$ into an infinite square matrix $(x_{i,j})$, and show without much trouble that for a fixed $j\in \mathbf N$ the sequence $(x_{i,j})_{i\in\mathbf N}$ is Cauchy (the columns of the infinite array are themselves Cauchy). For each fixed $j\in \mathbf N$, and with $m,n$ sufficiently large, $|x_{n,j}-x_{m,j}|\le\lVert \mathbf x_n-\mathbf x_m\rVert < \varepsilon$, since $(\mathbf x_n)$ is Cauchy. Since $\mathbf R$ is complete, put $y_j = \lim_{n\to\infty}x_{n,j}$. We claim that $\mathbf y = y_1,y_2,y_3,\dotsc$ is in $\ell^2$ and $\mathbf x_n\to \mathbf y$. By the reverse-triangle inequality, for sufficiently large $m,n\in\mathbf N$, $|\lVert \mathbf x_n\rVert-\lVert\mathbf x_m\rVert| \le \lVert \mathbf x_n-\mathbf x_m\rVert < 1$ by the Cauchy criterion, so that since $\lVert \mathbf x_m\rVert = C < \infty$, we have $\lim_{n\to\infty}\lVert \mathbf x_n\rVert \le C + 1 < \infty$. We want to show that $\sum_{j=1}^\infty y_j^2 < \infty$, and here is how I want to do that. For each $n=1,2,\dotsc$, put $f_n:\mathbf N\to \mathbf R$ to be the function $f_n(k) = \sum_{j=1}^kx_{n,j}^2$. Every function from $\mathbf N$ to $\mathbf R$ is continuous, so $(f_n)$ is a sequence of continuous functions. Now, $\sum_{j=1}^\infty y_j^2 = \lim_{k\to\infty}\lim_{n\to\infty}\sum_{j=1}^kx_{n,j}^2 = \lim_{k\to\infty}\lim_{n\to\infty}f_n(k)$. If only I could interchange these limits, then I would have $\sum_{j=1}^\infty y_j^2 = \lim_{n\to\infty}\sum_{j=1}^\infty x_{n,j}^2 = \lim_{n\to\infty}\lVert \mathbf x_n\rVert^2 < \infty$, as we already saw. My thoughts are that, as $k\to\infty$, $f_n(k)\to \lVert \mathbf x_n\rVert^2$, and as $n\to\infty, f_n(k)\to\sum_{j=1}^ky_j^2$, and that this latter convergence is uniform (we are dealing with a finite sum, so the uniform convergence shouldn't be hard to demonstrate is my gut reaction). Thus, if we put $f(k) = \sum_{j=1}^k y_j^2$, then $f_n\to f$ uniformly on $\mathbf N$, so we can exchange the order of the limits, just like in Rudin's theorem 7.11. I am not sure if this is correct—particularly the part where I am asserting $f_n\to f$ uniformly on $\mathbf N$. If anyone could help straighten this out, I'd be much obliged. Edit I am aware that this isn't all that has to be said to justify completeness. My main question is the rigorous justification of switching limits.","One of our problem sets this quarter in analysis asked us to show that $\ell^2$, the set of all square-summable sequences, is complete. At the time, I struggled to prove this, but I have an idea after reading on in Rudin's Principles . My idea is to use uniform convergence of a sequence of continuous functions and change the order of limits. Please let me know what I need to make the following go through; I have seen many questions here on StackExchange, similar to this, where the answerer skirts the question of this exchange of limits, and I would like if someone could just answer with as many epsilons as needed to get the thing settled once and for all. We have shown that $\ell^2$ is an inner-product space, and it has a norm $\lVert \mathbf x\rVert^2 = \sum_{n\ge 1} x_n^2$. One step of the process is to come up with a candidate limit for a Cauchy sequence, and we can do this fairly easily. Let $(\mathbf x_n)$ be a Cauchy sequence in $\ell^2$ with $\mathbf x_n = x_{n,1},x_{n,2},x_{n,3},\dotsc$. We can arrange the sequences $\mathbf x_n$ into an infinite square matrix $(x_{i,j})$, and show without much trouble that for a fixed $j\in \mathbf N$ the sequence $(x_{i,j})_{i\in\mathbf N}$ is Cauchy (the columns of the infinite array are themselves Cauchy). For each fixed $j\in \mathbf N$, and with $m,n$ sufficiently large, $|x_{n,j}-x_{m,j}|\le\lVert \mathbf x_n-\mathbf x_m\rVert < \varepsilon$, since $(\mathbf x_n)$ is Cauchy. Since $\mathbf R$ is complete, put $y_j = \lim_{n\to\infty}x_{n,j}$. We claim that $\mathbf y = y_1,y_2,y_3,\dotsc$ is in $\ell^2$ and $\mathbf x_n\to \mathbf y$. By the reverse-triangle inequality, for sufficiently large $m,n\in\mathbf N$, $|\lVert \mathbf x_n\rVert-\lVert\mathbf x_m\rVert| \le \lVert \mathbf x_n-\mathbf x_m\rVert < 1$ by the Cauchy criterion, so that since $\lVert \mathbf x_m\rVert = C < \infty$, we have $\lim_{n\to\infty}\lVert \mathbf x_n\rVert \le C + 1 < \infty$. We want to show that $\sum_{j=1}^\infty y_j^2 < \infty$, and here is how I want to do that. For each $n=1,2,\dotsc$, put $f_n:\mathbf N\to \mathbf R$ to be the function $f_n(k) = \sum_{j=1}^kx_{n,j}^2$. Every function from $\mathbf N$ to $\mathbf R$ is continuous, so $(f_n)$ is a sequence of continuous functions. Now, $\sum_{j=1}^\infty y_j^2 = \lim_{k\to\infty}\lim_{n\to\infty}\sum_{j=1}^kx_{n,j}^2 = \lim_{k\to\infty}\lim_{n\to\infty}f_n(k)$. If only I could interchange these limits, then I would have $\sum_{j=1}^\infty y_j^2 = \lim_{n\to\infty}\sum_{j=1}^\infty x_{n,j}^2 = \lim_{n\to\infty}\lVert \mathbf x_n\rVert^2 < \infty$, as we already saw. My thoughts are that, as $k\to\infty$, $f_n(k)\to \lVert \mathbf x_n\rVert^2$, and as $n\to\infty, f_n(k)\to\sum_{j=1}^ky_j^2$, and that this latter convergence is uniform (we are dealing with a finite sum, so the uniform convergence shouldn't be hard to demonstrate is my gut reaction). Thus, if we put $f(k) = \sum_{j=1}^k y_j^2$, then $f_n\to f$ uniformly on $\mathbf N$, so we can exchange the order of the limits, just like in Rudin's theorem 7.11. I am not sure if this is correct—particularly the part where I am asserting $f_n\to f$ uniformly on $\mathbf N$. If anyone could help straighten this out, I'd be much obliged. Edit I am aware that this isn't all that has to be said to justify completeness. My main question is the rigorous justification of switching limits.",,"['real-analysis', 'proof-verification']"
43,Evaluate $\lim\limits_{x \to \infty} \sin(\frac{1}{x})^x$,Evaluate,\lim\limits_{x \to \infty} \sin(\frac{1}{x})^x,This in an exercise in my Analysis book in a section on L'Hopital's rules. $$\lim\limits_{x \to \infty} \left[\sin\left(\frac{1}{x}\right)\right]^x$$ Now it's an indeterminate of the form $0^\infty$ however I don't know how to solve this. I have tried the following: $$y=\lim\limits_{x \to \infty} \left[\sin\left(\frac{1}{x}\right)\right]^x$$ $$\ln y=\lim\limits_{x \to \infty}\ln \left[\sin\left(\frac{1}{x}\right)\right]^x$$ $$\ln{y}=\lim\limits_{x \to \infty} x\ln{\sin\frac{1}{x}}$$ Now this is an indeterminate limit of form $\infty\cdot\infty$ which approaches $\infty$. However I may not write now that therefore $y=e^\infty=\infty$. How do I write this out correctly?,This in an exercise in my Analysis book in a section on L'Hopital's rules. $$\lim\limits_{x \to \infty} \left[\sin\left(\frac{1}{x}\right)\right]^x$$ Now it's an indeterminate of the form $0^\infty$ however I don't know how to solve this. I have tried the following: $$y=\lim\limits_{x \to \infty} \left[\sin\left(\frac{1}{x}\right)\right]^x$$ $$\ln y=\lim\limits_{x \to \infty}\ln \left[\sin\left(\frac{1}{x}\right)\right]^x$$ $$\ln{y}=\lim\limits_{x \to \infty} x\ln{\sin\frac{1}{x}}$$ Now this is an indeterminate limit of form $\infty\cdot\infty$ which approaches $\infty$. However I may not write now that therefore $y=e^\infty=\infty$. How do I write this out correctly?,,"['real-analysis', 'limits']"
44,Could convergence in measure imply converge almost everywhere for the entire sequence and not just sub-sequence here?,Could convergence in measure imply converge almost everywhere for the entire sequence and not just sub-sequence here?,,"This doesn't seem to have been asked on this site before. I've been self-studying measure theory and came across this problem. Given a sequence of measurable functions $\{f_n\}_{n \in \mathbb{N}}$ such that for all $\epsilon \gt 0$ $$\sum_{n=1}^\infty \mu (\{x: |f_n (x)| \gt \epsilon \})  \lt \infty $$ Prove that $f_n \to 0$ a.e. I've given the problem a try and clearly the sum being finite implies the summand converges to 0 which satisfies the definition of convergence in measure. Also, I know that there exists a subsequence $f_{n_j}$ converging a.e to 0. In fact, were this a finite measure space, I could prove the problem statement. However, in this more general setting, I'm unable to prove the statement. Any help would be appreciated. This is my first time posting here so I hope I've met all conventions.","This doesn't seem to have been asked on this site before. I've been self-studying measure theory and came across this problem. Given a sequence of measurable functions $\{f_n\}_{n \in \mathbb{N}}$ such that for all $\epsilon \gt 0$ $$\sum_{n=1}^\infty \mu (\{x: |f_n (x)| \gt \epsilon \})  \lt \infty $$ Prove that $f_n \to 0$ a.e. I've given the problem a try and clearly the sum being finite implies the summand converges to 0 which satisfies the definition of convergence in measure. Also, I know that there exists a subsequence $f_{n_j}$ converging a.e to 0. In fact, were this a finite measure space, I could prove the problem statement. However, in this more general setting, I'm unable to prove the statement. Any help would be appreciated. This is my first time posting here so I hope I've met all conventions.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
45,"Suppose that $f'(x)g(x)=f(x)g'(x)$ and $g(x)\ne 0$ on (a,b). How are $f$ and $g$ related?","Suppose that  and  on (a,b). How are  and  related?",f'(x)g(x)=f(x)g'(x) g(x)\ne 0 f g,"Suppose that $f'(x)g(x)=f(x)g'(x)$ and $g(x)\ne 0$ on (a,b). How are $f$ and $g$ related? I posted this earlier and accidentally deleted it. But thus far, I have: Let $$h(x)=\frac{f(x)}{g(x)}$$ Then $$h'(x)=\frac{f'(x)g(x)-f(x)g'(x)}{(g(x))^2}$$ Since $f'(x)g(x)=f(x)g'(x)$, I propose we replace $f(x)g'(x)$ in $h'(x)$ and we get $$h'(x)=\frac{f'(x)g(x)-f'(x)g(x)}{(g(x))^2}$$ $$=\frac{0}{(g(x))^2}$$ Is it enough, then, to say that $f$ and $g$ are constants on $(a,b)$?","Suppose that $f'(x)g(x)=f(x)g'(x)$ and $g(x)\ne 0$ on (a,b). How are $f$ and $g$ related? I posted this earlier and accidentally deleted it. But thus far, I have: Let $$h(x)=\frac{f(x)}{g(x)}$$ Then $$h'(x)=\frac{f'(x)g(x)-f(x)g'(x)}{(g(x))^2}$$ Since $f'(x)g(x)=f(x)g'(x)$, I propose we replace $f(x)g'(x)$ in $h'(x)$ and we get $$h'(x)=\frac{f'(x)g(x)-f'(x)g(x)}{(g(x))^2}$$ $$=\frac{0}{(g(x))^2}$$ Is it enough, then, to say that $f$ and $g$ are constants on $(a,b)$?",,"['real-analysis', 'derivatives']"
46,How to show that a Cauchy sequence of sequences converges,How to show that a Cauchy sequence of sequences converges,,"Let $c_0$ be the space of real-valued sequences $\{x_n\}$ which converge to zero, equipped with the metric $d(\{x_n\}, \{y_n\}) = sup_n |x_n − y_n|$. I want to show that the metric space $(c_0, d)$ is complete, in other words every Cauchy sequence converges. I need to show that every Cauchy sequence has a limit, and that that limit actually belongs to $c_0$. Right now I am having trouble with the first part. If we assume that $\{a_n\}$ in $c_0$ is Cauchy, I know by the definition of the metric $d$ that the real valued sequence along the $k^\text{th}$ term of each term (which itself is a sequence of reals) of $\{a_n\}$ is also Cauchy. In other words, the sequence $\{a_n\}$ converges term-wise (since Cauchy sequences in the reals converge), but how do I use that to show the entire sequence converges?","Let $c_0$ be the space of real-valued sequences $\{x_n\}$ which converge to zero, equipped with the metric $d(\{x_n\}, \{y_n\}) = sup_n |x_n − y_n|$. I want to show that the metric space $(c_0, d)$ is complete, in other words every Cauchy sequence converges. I need to show that every Cauchy sequence has a limit, and that that limit actually belongs to $c_0$. Right now I am having trouble with the first part. If we assume that $\{a_n\}$ in $c_0$ is Cauchy, I know by the definition of the metric $d$ that the real valued sequence along the $k^\text{th}$ term of each term (which itself is a sequence of reals) of $\{a_n\}$ is also Cauchy. In other words, the sequence $\{a_n\}$ converges term-wise (since Cauchy sequences in the reals converge), but how do I use that to show the entire sequence converges?",,"['real-analysis', 'convergence-divergence', 'cauchy-sequences']"
47,Dominated Convergence Theorem Exercise,Dominated Convergence Theorem Exercise,,"I am asked to find $$\lim_{n \to \infty} \int_0^\infty n^2e^{-nx} \tan^{-1} x \, dx.$$ Here is my attempt. Write $$\int_0^\infty n^2e^{-nx}\tan^{-1}x \, dx=\int_0^1 n^2e^{-nx} \tan^{-1} x \,dx + \int_1^\infty n^2e^{-nx}\tan^{-1} x \, dx$$ $$=\int_0^{n^2} e^{-\frac x n} \tan^{-1}\left(\frac x {n^2}\right) \, dx+\int_1^\infty n^2 e^{-nx} \tan^{-1}x \, dx.$$ Then note that $$\left| 1_{(0,n^2)}(x)e^{-x/n}\tan^{-1} \left(\frac x {n^2}\right) \right| \le \frac \pi 2$$ for all $x>0$ and all $n\ge 1$ and $$|n^2e^{-nx}\tan^{-1}x| \le \frac{\pi}{2}\frac 2 {x^2}$$ for all $x\in [1,\infty)$ and all $n\ge 1$. Thus the dominated convergence gives $${\lim_{n\to\infty} \int_0^\infty 1_{(0,n^2)}(x)e^{-x/n}\tan^{-1} \left(\frac x {n^2}\right) \, dx = 0}$$ and $$\lim_{n\to\infty} \int_1^\infty n^2e^{-nx} \tan^{-1}x\,dx=0,$$ and hence $$\lim_{n \to \infty}\int_0^\infty n^2e^{-nx}\tan^{-1}x\,dx=0.$$ Is this correct? EDIT: Unfortunately the above is not correct (see Dr. MV's comment). The correct justification is shown below (given by Sangchul Lee). $$\int_0^\infty n^2e^{-nx} \tan^{-1} xdx=\int_0^\infty ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx.$$ Since $$|ne^{-x}\tan^{-1} (\frac{x}{n})|\le xe^{-x}$$ for all $x>0$ and all $n\ge1$ we deduce that  $$\lim_{n \to \infty} \int_0^\infty n^2e^{-nx} \tan^{-1} x \, dx=\lim_{n\to\infty}\int_0^\infty ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx=\int_0^\infty \lim_{n \to\infty}ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx=\int_0^\infty xe^{-x} \, dx=1.$$ The point is that $\tan^{-1}x\le x$ for all $x\ge0$, an inequality I had forgotten!","I am asked to find $$\lim_{n \to \infty} \int_0^\infty n^2e^{-nx} \tan^{-1} x \, dx.$$ Here is my attempt. Write $$\int_0^\infty n^2e^{-nx}\tan^{-1}x \, dx=\int_0^1 n^2e^{-nx} \tan^{-1} x \,dx + \int_1^\infty n^2e^{-nx}\tan^{-1} x \, dx$$ $$=\int_0^{n^2} e^{-\frac x n} \tan^{-1}\left(\frac x {n^2}\right) \, dx+\int_1^\infty n^2 e^{-nx} \tan^{-1}x \, dx.$$ Then note that $$\left| 1_{(0,n^2)}(x)e^{-x/n}\tan^{-1} \left(\frac x {n^2}\right) \right| \le \frac \pi 2$$ for all $x>0$ and all $n\ge 1$ and $$|n^2e^{-nx}\tan^{-1}x| \le \frac{\pi}{2}\frac 2 {x^2}$$ for all $x\in [1,\infty)$ and all $n\ge 1$. Thus the dominated convergence gives $${\lim_{n\to\infty} \int_0^\infty 1_{(0,n^2)}(x)e^{-x/n}\tan^{-1} \left(\frac x {n^2}\right) \, dx = 0}$$ and $$\lim_{n\to\infty} \int_1^\infty n^2e^{-nx} \tan^{-1}x\,dx=0,$$ and hence $$\lim_{n \to \infty}\int_0^\infty n^2e^{-nx}\tan^{-1}x\,dx=0.$$ Is this correct? EDIT: Unfortunately the above is not correct (see Dr. MV's comment). The correct justification is shown below (given by Sangchul Lee). $$\int_0^\infty n^2e^{-nx} \tan^{-1} xdx=\int_0^\infty ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx.$$ Since $$|ne^{-x}\tan^{-1} (\frac{x}{n})|\le xe^{-x}$$ for all $x>0$ and all $n\ge1$ we deduce that  $$\lim_{n \to \infty} \int_0^\infty n^2e^{-nx} \tan^{-1} x \, dx=\lim_{n\to\infty}\int_0^\infty ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx=\int_0^\infty \lim_{n \to\infty}ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx=\int_0^\infty xe^{-x} \, dx=1.$$ The point is that $\tan^{-1}x\le x$ for all $x\ge0$, an inequality I had forgotten!",,"['real-analysis', 'measure-theory']"
48,"prove $f\in L^2([0,1])$",prove,"f\in L^2([0,1])","Assume $f:[0,1]\to R$ is a measurable function such that $fg\in L^1([0,1])$ for all $g\in L^2([0,1])$.prove that $f\in L^2([0,1])$. My opinion: if I can find a function g such that f+g is in $L^2([0,1])$, then use $$\int(f+g)^2-\int2fg+g^2$$to get $\int f^2$, then we can prove f is in $L^2([0,1])$. Is it right?","Assume $f:[0,1]\to R$ is a measurable function such that $fg\in L^1([0,1])$ for all $g\in L^2([0,1])$.prove that $f\in L^2([0,1])$. My opinion: if I can find a function g such that f+g is in $L^2([0,1])$, then use $$\int(f+g)^2-\int2fg+g^2$$to get $\int f^2$, then we can prove f is in $L^2([0,1])$. Is it right?",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'analysis']"
49,Intuition for the epsilon-delta definition of continuity,Intuition for the epsilon-delta definition of continuity,,"This is my first question so I hope this sort of thing is okay to ask. I'm working my way through Rudin's Principles of Mathematical Analysis, and I'm up to chapter 4, which is on continuity in the context of functions between metric spaces. It introduces what I understand to be the standard epsilon-delta definition used in calculus, but I'm struggling to gain an intuitive understanding of what it means. I came up with what I think is an English version of the gist of it: A function f is continuous at some point p in its domain iff sufficiently small deviations from p result in arbitrarily small variations in f(p). Does this show the general idea of continuity? If not, how should it be changed to fix it? Thanks in advance for any answers :)","This is my first question so I hope this sort of thing is okay to ask. I'm working my way through Rudin's Principles of Mathematical Analysis, and I'm up to chapter 4, which is on continuity in the context of functions between metric spaces. It introduces what I understand to be the standard epsilon-delta definition used in calculus, but I'm struggling to gain an intuitive understanding of what it means. I came up with what I think is an English version of the gist of it: A function f is continuous at some point p in its domain iff sufficiently small deviations from p result in arbitrarily small variations in f(p). Does this show the general idea of continuity? If not, how should it be changed to fix it? Thanks in advance for any answers :)",,"['real-analysis', 'continuity', 'definition', 'intuition', 'epsilon-delta']"
50,An inequality involving hyperbolic sine functions,An inequality involving hyperbolic sine functions,,"The following problem arised in my research work and has been challenging me for several days: Prove that for all $r\in (0,1)$ and for all $x>0$, we have $$\frac{\sinh(r(2-r)x)}{r(2-r)x} \bigg[ \frac{\sinh((1-r)x)}{(1-r)x} \bigg]^2 > \frac{\sinh(x)}{x} \frac{\sinh((1-r)^2x)}{(1-r)^2x}.$$ Below are my thoughts. The inequality is obviously true when $x$ is large enough, thanks to the equivalence $\sinh(y)\sim \frac{1}{2}\exp(y)$, $y\rightarrow +\infty$, and to the fact that $$r(2-r)+2(1-r) > 1 + (1-r)^2$$ for $r\in(0,1)$. The inequality also holds true when $x$ is small enough, since it can be readily shown that the Taylor expansion of the difference between the LHS and the RHS has $$\frac{1}{45}r^2(1-r)^2(2-r)^2 x^4$$ as the leading term. However, the difficulty lies in establishing the claim for all $x>0$, which is ""testified"" by various graphic display softwares (Maple, Gnuplot, WolframAlpha). A first natural transformation is to take the logarithm of both sides and to invoke the convexity of the function $x\mapsto\ln(\sinh(x)/x)$. But this leads nowhere: Jensen's inequality alone cannot help us deriving $$f(r(2-r)x)+2f((1-r)x) > f(x)+f((1-r)^2x)$$ for any convex function $f$ (consider a linear function and its opposite). As a second attack, we can multiply both sides by $4r(2-r)(1-r)^2x^3$ and use the linearization formulae $$2\sinh(a)\sinh(b) = \cosh(a+b)-\cosh(a-b)$$ $$2\sinh(a)\cosh(b) = \sinh(a+b) + \sinh(a-b)$$ in order to obtain the equivalent inequality $$\sinh((2-r^2)x) -\sinh((r^2-4r+2)x) - 2\sinh(r(2-r)x) > 2r(2-r)x \big( \cosh((r^2-2r+2)x)- \cosh(r(2-r)x)\big).$$ The direct study of the difference between the new LHS and the new RHS appears to be unwieldy. Nevertheless, it is now straightforward to compute the Taylor series of this new difference. This series (whose radius of convergence is infinite) is equal to $$\sum_{n=0}^{\infty} \frac{a_n(r)}{(2n+1)!} x^{2n+1},$$ with $$a_n(r)= (2-r^2)^{2n+1} - (r^2-4r+2)^{2n+1} + 4n (r(2-r))^{2n+1} - (4n+2) r(2-r)(r^2-2r+2)^{2n}.$$ The first three coefficients are easily shown to vanish, i.e., $a_0(r)=a_1(r) = a_2(r) = 0$. Pushing further the calculations yields $$a_3(r)=448 r^3(1-r)^4(2-r)^3$$ $$a_4(r)=768 r^3(1-r)^4(2-r)^3 [3(1-r)^4 + 2(1-r)^2 + 3]$$ $$a_5(r)= 1408 r^3(1-r)^4(2-r)^3 [5(1-r)^8 + 12(1-r)^6 + 6(1-r)^4 + 12(1-r)^2 + 5].$$ These suggest that $a_n(r)$ is the product of $r^3(1-r)^4(2-r)^3$ with some even and symmetric polynomials in $1-r$ whose coefficients are all non-negative. The conjecture $a_n(r)> 0$ for $r\in(0,1)$ implies the desired result, but again I cannot prove it for $n\geq 6$. Is there a general technique to prove that the power series coefficients of a given function are all non-negative? According to what I could find in the litterature, this seems to be a delicate issue. Is there any fresh approach to the initial question? Many thanks for any help.","The following problem arised in my research work and has been challenging me for several days: Prove that for all $r\in (0,1)$ and for all $x>0$, we have $$\frac{\sinh(r(2-r)x)}{r(2-r)x} \bigg[ \frac{\sinh((1-r)x)}{(1-r)x} \bigg]^2 > \frac{\sinh(x)}{x} \frac{\sinh((1-r)^2x)}{(1-r)^2x}.$$ Below are my thoughts. The inequality is obviously true when $x$ is large enough, thanks to the equivalence $\sinh(y)\sim \frac{1}{2}\exp(y)$, $y\rightarrow +\infty$, and to the fact that $$r(2-r)+2(1-r) > 1 + (1-r)^2$$ for $r\in(0,1)$. The inequality also holds true when $x$ is small enough, since it can be readily shown that the Taylor expansion of the difference between the LHS and the RHS has $$\frac{1}{45}r^2(1-r)^2(2-r)^2 x^4$$ as the leading term. However, the difficulty lies in establishing the claim for all $x>0$, which is ""testified"" by various graphic display softwares (Maple, Gnuplot, WolframAlpha). A first natural transformation is to take the logarithm of both sides and to invoke the convexity of the function $x\mapsto\ln(\sinh(x)/x)$. But this leads nowhere: Jensen's inequality alone cannot help us deriving $$f(r(2-r)x)+2f((1-r)x) > f(x)+f((1-r)^2x)$$ for any convex function $f$ (consider a linear function and its opposite). As a second attack, we can multiply both sides by $4r(2-r)(1-r)^2x^3$ and use the linearization formulae $$2\sinh(a)\sinh(b) = \cosh(a+b)-\cosh(a-b)$$ $$2\sinh(a)\cosh(b) = \sinh(a+b) + \sinh(a-b)$$ in order to obtain the equivalent inequality $$\sinh((2-r^2)x) -\sinh((r^2-4r+2)x) - 2\sinh(r(2-r)x) > 2r(2-r)x \big( \cosh((r^2-2r+2)x)- \cosh(r(2-r)x)\big).$$ The direct study of the difference between the new LHS and the new RHS appears to be unwieldy. Nevertheless, it is now straightforward to compute the Taylor series of this new difference. This series (whose radius of convergence is infinite) is equal to $$\sum_{n=0}^{\infty} \frac{a_n(r)}{(2n+1)!} x^{2n+1},$$ with $$a_n(r)= (2-r^2)^{2n+1} - (r^2-4r+2)^{2n+1} + 4n (r(2-r))^{2n+1} - (4n+2) r(2-r)(r^2-2r+2)^{2n}.$$ The first three coefficients are easily shown to vanish, i.e., $a_0(r)=a_1(r) = a_2(r) = 0$. Pushing further the calculations yields $$a_3(r)=448 r^3(1-r)^4(2-r)^3$$ $$a_4(r)=768 r^3(1-r)^4(2-r)^3 [3(1-r)^4 + 2(1-r)^2 + 3]$$ $$a_5(r)= 1408 r^3(1-r)^4(2-r)^3 [5(1-r)^8 + 12(1-r)^6 + 6(1-r)^4 + 12(1-r)^2 + 5].$$ These suggest that $a_n(r)$ is the product of $r^3(1-r)^4(2-r)^3$ with some even and symmetric polynomials in $1-r$ whose coefficients are all non-negative. The conjecture $a_n(r)> 0$ for $r\in(0,1)$ implies the desired result, but again I cannot prove it for $n\geq 6$. Is there a general technique to prove that the power series coefficients of a given function are all non-negative? According to what I could find in the litterature, this seems to be a delicate issue. Is there any fresh approach to the initial question? Many thanks for any help.",,"['real-analysis', 'inequality', 'hyperbolic-functions']"
51,Dual of the Banach space of $k$-times continuously differentiable functions.,Dual of the Banach space of -times continuously differentiable functions.,k,"Let $C^k([0,1])$ denote the Banach space of $k$-times continuously differentiable functions $f:[0,1]\to \mathbb R$ with norm $$\|f\|_{C^k}:=\max_{i=0,\dots,k}\sup_{x\in [0,1]}|f^{i}(x)|.$$ I'm trying to understand its (continuous) dual space -- is there a nice characterisation of it and perhaps a reference where I can read about such things? I'm also interested in the dual space of $C^r$ for $r>0$ non-integer ($\lfloor r\rfloor$-times continuously differentiable functions with $\lfloor r\rfloor^{\text{th}}$ derivative being $(r-\lfloor r\rfloor)$-Holder continuous if anyone could shed some light on this. Many thanks!","Let $C^k([0,1])$ denote the Banach space of $k$-times continuously differentiable functions $f:[0,1]\to \mathbb R$ with norm $$\|f\|_{C^k}:=\max_{i=0,\dots,k}\sup_{x\in [0,1]}|f^{i}(x)|.$$ I'm trying to understand its (continuous) dual space -- is there a nice characterisation of it and perhaps a reference where I can read about such things? I'm also interested in the dual space of $C^r$ for $r>0$ non-integer ($\lfloor r\rfloor$-times continuously differentiable functions with $\lfloor r\rfloor^{\text{th}}$ derivative being $(r-\lfloor r\rfloor)$-Holder continuous if anyone could shed some light on this. Many thanks!",,"['real-analysis', 'functional-analysis', 'analysis']"
52,"Real Analysis, Folland Theorem 3.18 Differentiation on Euclidean Space","Real Analysis, Folland Theorem 3.18 Differentiation on Euclidean Space",,"Background Information: A measurable function $f:\mathbb{R}^n\rightarrow \mathbb{C}$ is called locally integrable (w.r.t Lebesgue measure) if $\int_K |f(x)|dx < \infty$ for every bounded measurable $K\subset \mathbb{R}^n$. We denote the space of locally integrable functions by $L^1_{loc}$. If $f\in L^1_{loc}$, $x\in \mathbb{R}^n$, and $r > 0$, we define $A_r f(x)$ to be the average value of $f$ on $B(r,x)$ (ball of radius $r$ around $x$): $$A_r f(x) = \frac{1}{m(B(r,x))}\int_{B(r,x)} f(y) dy$$ Maximal Theorem  - There is a constant $C > 0$ such that for all $f\in L^1$ and all $\alpha > 0$, $$m(\{x:Hf(x) > \alpha\}) \leq \frac{C}{\alpha}\int |f(x)|dx$$ Question: Theorem 3.18 - If $f\in L^1_{loc}$ then $\lim_{r\rightarrow 0} A_r f(x) = f(x)$ for a.e. $x\in\mathbb{R}^n$ I am trying to understand Folland's proof but I am having some trouble. He first begins by saying that it suffices to show that for $N\in\mathbb{N}$, $A_r f(x)\rightarrow f(x)$ for a.e. with $|x| < N$. Why does $|x| \leq N$? Then he states that but for $|x|\leq N$ an $r\leq 1$ the values $A_r f(x)$ depend on $f(y)$ for $|y|\leq N + 1$. Again why is $|y|\leq N+1$? Then he says by Theorem 2.41 we can find a continuous integrable function $g$ such that $\int |g(y) - f(y)|dy < \epsilon$. Then the rest is not so bad to follow. If there is another way of proving this please let me know otherwise I just need to understand the beginning points and I think I should be able to understand the proof. Second question: As mentioned Folland uses Theorem 2.41 to find a continuous integrable function $g$ such that $\int |g(y) - f(y)|dy < \epsilon$. By contunity of $g$ we have that for $x\in\mathbb{R}^n$ and $\delta > 0$ there exists an $r > 0$ such that $|g(y) - g(x)| < \delta$ whenever $|y - x| < r$, and hence $$|A_r g(x) - g(x)| = \frac{1}{m(B(r,x)}\left|int_{B(r,x)} [g(y) - g(x)] dy \right| < \delta$$ therefore $A_r g(x)\rightarrow g(x)$ as $r\rightarrow 0$ for all $x$, so  \begin{align*} \lim_{r\rightarrow 0}\sup|A_rf(x) - f(x)| &= \lim_{r\rightarrow 0}\sup |A_r(f-g)(x) + (A_r g - g)(x) + (g - f)(x)|\\ &\leq H (f-g)(x) + 0 + |f-g|(x) \end{align*} Let, $$E_{\alpha} = \{x:\lim_{r\rightarrow 0}\sup |A_r f(x) - f(x)| > \alpha\} \ \ \ F_{\alpha} = \{x: |f - g|(x) > \alpha\}$$ This is where I get confused again. Folland says note that $$E_{\alpha} \subset F_{\alpha/2}\cup \{x: H(f-g)(x) > \alpha/2\}$$ Why does he have $F_{\alpha/2}$ I understand why $E_{\alpha}$ is a subset of these but I don't understand the intuition. Then he says but $$(\alpha/2)m(F_{\alpha/2}) \leq \int_{F_{\alpha/2}} |f(x) - g(x)| dx < \epsilon$$ Is he using the Maximal Theorem there?","Background Information: A measurable function $f:\mathbb{R}^n\rightarrow \mathbb{C}$ is called locally integrable (w.r.t Lebesgue measure) if $\int_K |f(x)|dx < \infty$ for every bounded measurable $K\subset \mathbb{R}^n$. We denote the space of locally integrable functions by $L^1_{loc}$. If $f\in L^1_{loc}$, $x\in \mathbb{R}^n$, and $r > 0$, we define $A_r f(x)$ to be the average value of $f$ on $B(r,x)$ (ball of radius $r$ around $x$): $$A_r f(x) = \frac{1}{m(B(r,x))}\int_{B(r,x)} f(y) dy$$ Maximal Theorem  - There is a constant $C > 0$ such that for all $f\in L^1$ and all $\alpha > 0$, $$m(\{x:Hf(x) > \alpha\}) \leq \frac{C}{\alpha}\int |f(x)|dx$$ Question: Theorem 3.18 - If $f\in L^1_{loc}$ then $\lim_{r\rightarrow 0} A_r f(x) = f(x)$ for a.e. $x\in\mathbb{R}^n$ I am trying to understand Folland's proof but I am having some trouble. He first begins by saying that it suffices to show that for $N\in\mathbb{N}$, $A_r f(x)\rightarrow f(x)$ for a.e. with $|x| < N$. Why does $|x| \leq N$? Then he states that but for $|x|\leq N$ an $r\leq 1$ the values $A_r f(x)$ depend on $f(y)$ for $|y|\leq N + 1$. Again why is $|y|\leq N+1$? Then he says by Theorem 2.41 we can find a continuous integrable function $g$ such that $\int |g(y) - f(y)|dy < \epsilon$. Then the rest is not so bad to follow. If there is another way of proving this please let me know otherwise I just need to understand the beginning points and I think I should be able to understand the proof. Second question: As mentioned Folland uses Theorem 2.41 to find a continuous integrable function $g$ such that $\int |g(y) - f(y)|dy < \epsilon$. By contunity of $g$ we have that for $x\in\mathbb{R}^n$ and $\delta > 0$ there exists an $r > 0$ such that $|g(y) - g(x)| < \delta$ whenever $|y - x| < r$, and hence $$|A_r g(x) - g(x)| = \frac{1}{m(B(r,x)}\left|int_{B(r,x)} [g(y) - g(x)] dy \right| < \delta$$ therefore $A_r g(x)\rightarrow g(x)$ as $r\rightarrow 0$ for all $x$, so  \begin{align*} \lim_{r\rightarrow 0}\sup|A_rf(x) - f(x)| &= \lim_{r\rightarrow 0}\sup |A_r(f-g)(x) + (A_r g - g)(x) + (g - f)(x)|\\ &\leq H (f-g)(x) + 0 + |f-g|(x) \end{align*} Let, $$E_{\alpha} = \{x:\lim_{r\rightarrow 0}\sup |A_r f(x) - f(x)| > \alpha\} \ \ \ F_{\alpha} = \{x: |f - g|(x) > \alpha\}$$ This is where I get confused again. Folland says note that $$E_{\alpha} \subset F_{\alpha/2}\cup \{x: H(f-g)(x) > \alpha/2\}$$ Why does he have $F_{\alpha/2}$ I understand why $E_{\alpha}$ is a subset of these but I don't understand the intuition. Then he says but $$(\alpha/2)m(F_{\alpha/2}) \leq \int_{F_{\alpha/2}} |f(x) - g(x)| dx < \epsilon$$ Is he using the Maximal Theorem there?",,"['real-analysis', 'measure-theory']"
53,Generalization of Jensen's inequality to multivariate functions,Generalization of Jensen's inequality to multivariate functions,,"Is there a generalization of Jensen's inequality for convex multivariate functions? By convex, let's say $f$ is a multivariate function defined on the convex set $A$, and for all $x,y \in A$ and $\lambda \in [0,1]$,  $$f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y).$$ Then, letting $x_1,\ldots,x_n$ denote points in $A$, the result would be something to the effect of saying that for any $n$ points in $A$, $$\frac{\sum_{i=1}^n f(x_i)}{n} \geq f \left(\frac{\sum_{i=1}^n{x_i}}{n} \right). $$ I do see a few articles that may be related: Perlman, Michael D. ""Jensen's inequality for a convex vector-valued function on an infinite-dimensional space."" Journal of Multivariate Analysis 4.1 (1974): 52-65. Merkle, Milan. ""Jensen's inequality for multivariate medians."" Journal of Mathematical Analysis and Applications 370.1 (2010): 258-269. Aras-Gazic, G., et al. ""GENERALIZATION OF JENSEN’S INEQUALITY BY HERMITE POLYNOMIALS AND RELATED RESULTS."" Mathematical reports 17.2 (2015): 201-223. Agnew, Robert A. ""Multivariate version of a Jensen-type inequality."" J. Inequal. in Pure and Appl. Math 6.4 (2005). I do not think the first is particularly related if I'm interested in finite dimensional spaces, and my function is not vector-valued in any case. The second may be more related, but it seems to be generalizing in slightly different directions. The third is beyond my comprehension and the fourth, again, seems to be working on a slightly different generalization. Are there no less technical generalizations of Jensen's to multivariate functions out there?","Is there a generalization of Jensen's inequality for convex multivariate functions? By convex, let's say $f$ is a multivariate function defined on the convex set $A$, and for all $x,y \in A$ and $\lambda \in [0,1]$,  $$f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y).$$ Then, letting $x_1,\ldots,x_n$ denote points in $A$, the result would be something to the effect of saying that for any $n$ points in $A$, $$\frac{\sum_{i=1}^n f(x_i)}{n} \geq f \left(\frac{\sum_{i=1}^n{x_i}}{n} \right). $$ I do see a few articles that may be related: Perlman, Michael D. ""Jensen's inequality for a convex vector-valued function on an infinite-dimensional space."" Journal of Multivariate Analysis 4.1 (1974): 52-65. Merkle, Milan. ""Jensen's inequality for multivariate medians."" Journal of Mathematical Analysis and Applications 370.1 (2010): 258-269. Aras-Gazic, G., et al. ""GENERALIZATION OF JENSEN’S INEQUALITY BY HERMITE POLYNOMIALS AND RELATED RESULTS."" Mathematical reports 17.2 (2015): 201-223. Agnew, Robert A. ""Multivariate version of a Jensen-type inequality."" J. Inequal. in Pure and Appl. Math 6.4 (2005). I do not think the first is particularly related if I'm interested in finite dimensional spaces, and my function is not vector-valued in any case. The second may be more related, but it seems to be generalizing in slightly different directions. The third is beyond my comprehension and the fourth, again, seems to be working on a slightly different generalization. Are there no less technical generalizations of Jensen's to multivariate functions out there?",,"['real-analysis', 'inequality']"
54,Theorem 2.22 from RCA Rudin,Theorem 2.22 from RCA Rudin,,"I read this interesting result from Rudin's book but I would like to clarify some confusing moments. As I understood $(\mathbb{R}^1, +)$ is group and $(\mathbb{Q}, +)$ is subgroup. He considers cosets $E_r=r+\mathbb{Q}$ for $r\in \mathbb{R}$. Since $E_r\neq \varnothing$ then by axiom of choice we can costruct set $E$ which contains only one element from each $E_r$. Question 1: I am not sure that $(E+r)\cap (E+s)=\varnothing$ for $r,s\in \mathbb{Q}$, $r\neq s$. Let's consider cosets $\sqrt{2}+\mathbb{Q}$ and $1+\sqrt{2}+\mathbb{Q}$ and let $E$ contains $\sqrt{2}$ and $\sqrt{2}+1$ from the first and second cosets, respectively. But $E+1$ also contains $\sqrt{2}+1$. So we see that $E\cap (E+1)\neq \varnothing$. Question 2: Suppose that I have mistake in my previous question. Let $y$ and $z$ lie in the same coset of $Q$? Where's the contradiction? Question 3: Why such $y$ exists? I am about point $(b)$. Would be very thankful for help!","I read this interesting result from Rudin's book but I would like to clarify some confusing moments. As I understood $(\mathbb{R}^1, +)$ is group and $(\mathbb{Q}, +)$ is subgroup. He considers cosets $E_r=r+\mathbb{Q}$ for $r\in \mathbb{R}$. Since $E_r\neq \varnothing$ then by axiom of choice we can costruct set $E$ which contains only one element from each $E_r$. Question 1: I am not sure that $(E+r)\cap (E+s)=\varnothing$ for $r,s\in \mathbb{Q}$, $r\neq s$. Let's consider cosets $\sqrt{2}+\mathbb{Q}$ and $1+\sqrt{2}+\mathbb{Q}$ and let $E$ contains $\sqrt{2}$ and $\sqrt{2}+1$ from the first and second cosets, respectively. But $E+1$ also contains $\sqrt{2}+1$. So we see that $E\cap (E+1)\neq \varnothing$. Question 2: Suppose that I have mistake in my previous question. Let $y$ and $z$ lie in the same coset of $Q$? Where's the contradiction? Question 3: Why such $y$ exists? I am about point $(b)$. Would be very thankful for help!",,"['real-analysis', 'measure-theory']"
55,Example of a set and monotone class where monotone class is not a $\sigma$-algebra?,Example of a set and monotone class where monotone class is not a -algebra?,\sigma,"What is an example of a set $X$ and a monotone class $\mathcal{M}$ consisting of subsets of $X$ such that $\emptyset \in \mathcal{M}$, $X \in \mathcal{M}$, but $\mathcal{M}$ is not a $\sigma$-algebra?","What is an example of a set $X$ and a monotone class $\mathcal{M}$ consisting of subsets of $X$ such that $\emptyset \in \mathcal{M}$, $X \in \mathcal{M}$, but $\mathcal{M}$ is not a $\sigma$-algebra?",,"['real-analysis', 'probability']"
56,What is the difference between hyperreal numbers and dual numbers,What is the difference between hyperreal numbers and dual numbers,,Wikipedia has two different but unconnected pages for Hyperreal and Dual numbers . https://en.wikipedia.org/wiki/Hyperreal_number and https://en.wikipedia.org/wiki/Dual_number I cannot stop seeing them very related to each other. In one the product is not explicitly defined (it is said that it is the result of a series of cuts) in the other it is stressed that $\epsilon^2=0$ is the defining property. Both are related to derivatives when evaluated in functions (for example of polynomials or Taylor series) although in one the st symbol is used and in the other $\epsilon$ is used. Is there a simple relation between these two mathematical constructs? are both the same? is one just a specialization (for a certain operation) case of the other? Is one a field and the other just a ring for example? Is the difference the partial vs. total order?,Wikipedia has two different but unconnected pages for Hyperreal and Dual numbers . https://en.wikipedia.org/wiki/Hyperreal_number and https://en.wikipedia.org/wiki/Dual_number I cannot stop seeing them very related to each other. In one the product is not explicitly defined (it is said that it is the result of a series of cuts) in the other it is stressed that $\epsilon^2=0$ is the defining property. Both are related to derivatives when evaluated in functions (for example of polynomials or Taylor series) although in one the st symbol is used and in the other $\epsilon$ is used. Is there a simple relation between these two mathematical constructs? are both the same? is one just a specialization (for a certain operation) case of the other? Is one a field and the other just a ring for example? Is the difference the partial vs. total order?,,"['real-analysis', 'definition', 'nonstandard-analysis']"
57,Bijection from $\mathbb {Z}^3$ to $\mathbb {Z}$,Bijection from  to,\mathbb {Z}^3 \mathbb {Z},"I am not a mathematician. Let $\mathbb {Z}$ be a positive integer set. I need to know whether there exist a bijection from $\mathbb {Z}^3$ to $\mathbb {Z}$, what might be a possible mapping? I know that bijection exists from $\mathbb {R}^3$ to $\mathbb {R}$.","I am not a mathematician. Let $\mathbb {Z}$ be a positive integer set. I need to know whether there exist a bijection from $\mathbb {Z}^3$ to $\mathbb {Z}$, what might be a possible mapping? I know that bijection exists from $\mathbb {R}^3$ to $\mathbb {R}$.",,"['real-analysis', 'functions', 'cardinals']"
58,"For differentiable functions $f,g$, $\nabla f(x)=g(x)x$. Then $f$ is constant on S.","For differentiable functions , . Then  is constant on S.","f,g \nabla f(x)=g(x)x f","Problem saying that : $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$   is differentiable. Assume   that there is a differentiable function   $g:\mathbb{R}^{n}\rightarrow\mathbb{R}$   such that $\nabla  f(x)=g(x)x$  . Show that $f$   is constant on   $S=\{x\in\mathbb{R}^{n}:||x||=r\}$   where $r$   is positive constant. For $x=(x_1,\dots ,x_n)$ and $\nabla f=(\frac{\partial f}{\partial x_{1}},\dots,\frac{\partial f}{\partial x_{n}})$, problem says $\frac{\partial f}{\partial x_{i}}=g(x)x_{i}$. It seems to me that to solve this problem, knowing the relation between norm of gradient and its value is crucial. How can I do? Notification : This question is edited since it's about same problem and the former one is about just notation.","Problem saying that : $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$   is differentiable. Assume   that there is a differentiable function   $g:\mathbb{R}^{n}\rightarrow\mathbb{R}$   such that $\nabla  f(x)=g(x)x$  . Show that $f$   is constant on   $S=\{x\in\mathbb{R}^{n}:||x||=r\}$   where $r$   is positive constant. For $x=(x_1,\dots ,x_n)$ and $\nabla f=(\frac{\partial f}{\partial x_{1}},\dots,\frac{\partial f}{\partial x_{n}})$, problem says $\frac{\partial f}{\partial x_{i}}=g(x)x_{i}$. It seems to me that to solve this problem, knowing the relation between norm of gradient and its value is crucial. How can I do? Notification : This question is edited since it's about same problem and the former one is about just notation.",,['real-analysis']
59,"Problem about uniform continunity on $[0,\infty)$",Problem about uniform continunity on,"[0,\infty)","The question is the following: $f(x)$ is uniformly continuous on $[0, \infty)$ and for any $x > 0$, $\lim\limits_{n\to \infty}f(x+n) = 0$, where $n \in \mathbb{Z}_{>0}$. Prove that $\lim\limits_{x\to \infty} f(x) = 0$. Hint: Divide $[0, 1]$ into small equal-length intervals I do not understand what this question means. What exactly it is asking to be proved? Wouldn't it be obvious that $\lim\limits_{x\to \infty} f(x) = 0$ since $\lim\limits_{n\to \infty}f(x+n) = 0$. Also, what is the meaning or help from the hint? Thanks for your help! I am so confused...","The question is the following: $f(x)$ is uniformly continuous on $[0, \infty)$ and for any $x > 0$, $\lim\limits_{n\to \infty}f(x+n) = 0$, where $n \in \mathbb{Z}_{>0}$. Prove that $\lim\limits_{x\to \infty} f(x) = 0$. Hint: Divide $[0, 1]$ into small equal-length intervals I do not understand what this question means. What exactly it is asking to be proved? Wouldn't it be obvious that $\lim\limits_{x\to \infty} f(x) = 0$ since $\lim\limits_{n\to \infty}f(x+n) = 0$. Also, what is the meaning or help from the hint? Thanks for your help! I am so confused...",,"['real-analysis', 'uniform-continuity']"
60,Maximum of a upper semicontinuous function,Maximum of a upper semicontinuous function,,"If $f:\mathbb{R}^N\rightarrow\mathbb{R}$ is a continuous function and $\lim_{|x|\rightarrow \infty}f(x)=-\infty$, so for definition for all $N>0$ exists a $M>0$ such that $|x|>M$ implies $f(x)<-N$. Since I want search a global maximum, I can search it in $A=\{x\in\mathbb{R}^N : |x|\leq M\}$. It is a compact and so I can say that $f$ attains its maximum in some point $x_{0}$. How can I extend this for a upper semicontinuous function? Thank you.","If $f:\mathbb{R}^N\rightarrow\mathbb{R}$ is a continuous function and $\lim_{|x|\rightarrow \infty}f(x)=-\infty$, so for definition for all $N>0$ exists a $M>0$ such that $|x|>M$ implies $f(x)<-N$. Since I want search a global maximum, I can search it in $A=\{x\in\mathbb{R}^N : |x|\leq M\}$. It is a compact and so I can say that $f$ attains its maximum in some point $x_{0}$. How can I extend this for a upper semicontinuous function? Thank you.",,"['real-analysis', 'analysis', 'semicontinuous-functions']"
61,Is it good to use mean value theorem in $\epsilon-\delta$ continuity proofs?,Is it good to use mean value theorem in  continuity proofs?,\epsilon-\delta,"I wanted to prove $f(x) = \cos(x)$ is continuous using $\epsilon-\delta$ proof Couple of posts on MSE appealed to MVT to resolve this problem. Namely: $\exists c \in [x,x_o]$ s.t. $|\cos(x)-\cos(x_o)| = |\sin(c)||x-x_o|$ Tada! Problem here is that we are appealing to the fact $\sin(x)$ is the derivative of $\cos(x)$ ...which necessarily implies that $\cos(x)$ is continuous. Is it ""good"" to use MVT in proving a function is continuous?","I wanted to prove $f(x) = \cos(x)$ is continuous using $\epsilon-\delta$ proof Couple of posts on MSE appealed to MVT to resolve this problem. Namely: $\exists c \in [x,x_o]$ s.t. $|\cos(x)-\cos(x_o)| = |\sin(c)||x-x_o|$ Tada! Problem here is that we are appealing to the fact $\sin(x)$ is the derivative of $\cos(x)$ ...which necessarily implies that $\cos(x)$ is continuous. Is it ""good"" to use MVT in proving a function is continuous?",,"['calculus', 'real-analysis', 'soft-question', 'continuity', 'proof-writing']"
62,Show $\sum_\limits{k=1}^{\infty}1/k$ does not converge. [duplicate],Show  does not converge. [duplicate],\sum_\limits{k=1}^{\infty}1/k,"This question already has answers here : Why does the series $\sum_{n=1}^\infty\frac1n$ not converge? (26 answers) Closed 8 years ago . Show $$\sum_\limits{k=1}^\infty \frac 1 k$$ does not converge. Attempt: Let $s_n=\sum_\limits{k=1}^{n}1/k$, and let $\epsilon=1/2$. For all $N\in\mathbb{N}$, we have $$\left|s_{2n}-s_n\right|=\left|\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n}\right|\geq1/2,\qquad\text{for all $n\geq N$}$$ Hence, $\{s_n\}_{n=1}^{\infty}$ is not a Cauchy sequence. Since $\{s_n\}_{k=1}^{\infty}$ is not a Cauchy sequence which implies $\{s_n\}$ doesn't converge, we have $$\lim\limits_{n\rightarrow\infty}s_n=\lim\limits_{n\rightarrow\infty}\sum_{k=1}^{n}\frac{1}{k}=\infty$$ Therefore, the infinite series $\sum_\limits{k=1}^{\infty}\frac{1}{k}$ does not converge. I am not sure this is valid or not because I use contradiction to do these kind problem.","This question already has answers here : Why does the series $\sum_{n=1}^\infty\frac1n$ not converge? (26 answers) Closed 8 years ago . Show $$\sum_\limits{k=1}^\infty \frac 1 k$$ does not converge. Attempt: Let $s_n=\sum_\limits{k=1}^{n}1/k$, and let $\epsilon=1/2$. For all $N\in\mathbb{N}$, we have $$\left|s_{2n}-s_n\right|=\left|\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n}\right|\geq1/2,\qquad\text{for all $n\geq N$}$$ Hence, $\{s_n\}_{n=1}^{\infty}$ is not a Cauchy sequence. Since $\{s_n\}_{k=1}^{\infty}$ is not a Cauchy sequence which implies $\{s_n\}$ doesn't converge, we have $$\lim\limits_{n\rightarrow\infty}s_n=\lim\limits_{n\rightarrow\infty}\sum_{k=1}^{n}\frac{1}{k}=\infty$$ Therefore, the infinite series $\sum_\limits{k=1}^{\infty}\frac{1}{k}$ does not converge. I am not sure this is valid or not because I use contradiction to do these kind problem.",,"['real-analysis', 'sequences-and-series', 'proof-verification', 'harmonic-numbers']"
63,Deducing the series expansion of $\arctan(x^2)$ via the series expansion of $\arctan(x)$ at $x=0$,Deducing the series expansion of  via the series expansion of  at,\arctan(x^2) \arctan(x) x=0,Comparing the series expansion of $\arctan(x^2)$ and $\arctan(x)$ at $x=0$ it looks like one can take the result from $\arctan(x)$ and replace each $x$ with $x^2$ to deduce the series expansion of $\arctan(x^2)$. Is this just true in this specific case or is this approach generally valid? Do you have any other examples or counter-examples for this observation?,Comparing the series expansion of $\arctan(x^2)$ and $\arctan(x)$ at $x=0$ it looks like one can take the result from $\arctan(x)$ and replace each $x$ with $x^2$ to deduce the series expansion of $\arctan(x^2)$. Is this just true in this specific case or is this approach generally valid? Do you have any other examples or counter-examples for this observation?,,"['calculus', 'real-analysis', 'power-series', 'taylor-expansion']"
64,Proof $\int \limits_{0}^{\infty}\left(\frac{\sin x}{x}\right)^2dx=\frac{\pi}{2}$ by definition,Proof  by definition,\int \limits_{0}^{\infty}\left(\frac{\sin x}{x}\right)^2dx=\frac{\pi}{2},"Let $f(x)=\mathbb{I}_{[-t,t]}$ where $t\in (0,\pi)$. Using Parseval's theorem to this function we get: $$\sum \limits_{n=1}^{\infty}\dfrac{\sin ^2(nt)}{n^2t}=\dfrac{\pi-t}{2}.$$ Prove that $$\int \limits_{0}^{\infty}\left(\frac{\sin x}{x}\right)^2dx=\lim \limits_{t\to 0}\sum \limits_{n=1}^{\infty}\dfrac{\sin ^2(nt)}{n^2t}=\frac{\pi}{2}.$$ How to prove the first equality strictly using $\varepsilon-\delta$? Unfortunately I have not any ideas. I know that this sum is a Riemann-integral sum but I can't prove it rigorously. I would be very grateful to anyone who'll post full solution because I can't find it's proof.","Let $f(x)=\mathbb{I}_{[-t,t]}$ where $t\in (0,\pi)$. Using Parseval's theorem to this function we get: $$\sum \limits_{n=1}^{\infty}\dfrac{\sin ^2(nt)}{n^2t}=\dfrac{\pi-t}{2}.$$ Prove that $$\int \limits_{0}^{\infty}\left(\frac{\sin x}{x}\right)^2dx=\lim \limits_{t\to 0}\sum \limits_{n=1}^{\infty}\dfrac{\sin ^2(nt)}{n^2t}=\frac{\pi}{2}.$$ How to prove the first equality strictly using $\varepsilon-\delta$? Unfortunately I have not any ideas. I know that this sum is a Riemann-integral sum but I can't prove it rigorously. I would be very grateful to anyone who'll post full solution because I can't find it's proof.",,['real-analysis']
65,difference between the dual space of $H^1(\Omega)$ and the dual of $H^1_0(\Omega)$,difference between the dual space of  and the dual of,H^1(\Omega) H^1_0(\Omega),"In the Partial Differential Equations by Evans (2nd edition p299), $H^{-1}(\Omega)$ denotes the dual space to $H^1_0(\Omega)$ where $\Omega$ is an open subset of $\mathbb{R}^n$ and $H^1(\Omega)=W^{1,2}(\Omega)$, $H^1_0(\Omega)=W^{1,2}_0(\Omega)$: $$ W^{1,2}_0(\Omega)=\overline{C_c^\infty(\Omega)}^{\|\cdot\|_{W^{1,2}(\Omega)}} $$ While in the Navier Stokes Equations by Constantin and Foias (p7), $H^{-1}(\Omega)$ denotes the dual space of $H^1(\Omega)$. Let $X$ be the (continuous) dual of $H^1(\Omega)$ and $Y$ the dual of  $H^1_0(\Omega)$. One has that $X\subset Y$. Here is my question : Could somebody describe the difference between $X$ and $Y$?","In the Partial Differential Equations by Evans (2nd edition p299), $H^{-1}(\Omega)$ denotes the dual space to $H^1_0(\Omega)$ where $\Omega$ is an open subset of $\mathbb{R}^n$ and $H^1(\Omega)=W^{1,2}(\Omega)$, $H^1_0(\Omega)=W^{1,2}_0(\Omega)$: $$ W^{1,2}_0(\Omega)=\overline{C_c^\infty(\Omega)}^{\|\cdot\|_{W^{1,2}(\Omega)}} $$ While in the Navier Stokes Equations by Constantin and Foias (p7), $H^{-1}(\Omega)$ denotes the dual space of $H^1(\Omega)$. Let $X$ be the (continuous) dual of $H^1(\Omega)$ and $Y$ the dual of  $H^1_0(\Omega)$. One has that $X\subset Y$. Here is my question : Could somebody describe the difference between $X$ and $Y$?",,"['real-analysis', 'functional-analysis']"
66,Weak Limit of Measures Mutually Singular wrt Lebesgue Measure,Weak Limit of Measures Mutually Singular wrt Lebesgue Measure,,"I'm stuck on the following qual problem: Let $\{h_{n}\}$ be a sequence of positive continuous functions on the unit cube $Q$ in $\mathbb{R}^{d}$ satisfying the following conditions: $\lim_{n\rightarrow\infty}h_{n}(x)=0$ $m$-a.e. ($m$ denotes the Lebesgue measure on $Q$) $\int_{Q}h_{n}dx=1$ $\forall n$ $\lim_{n\rightarrow\infty}\int_{Q}fh_{n}dx=\int_{Q}fd\mu$ for every continuous function $f$ on $Q$. Prove that $\mu\perp m$ or give a counterexample. My intuition suggests to me that $\mu\perp m$ since $h_{n}\rightarrow 0$ a.e. and therefore must become very large on sets of small Lebesgue measure; however, I'm struggling to prove my guess. My thought was to write the $\int_{Q}fd\mu=\int_{Q}fh dx+\int_{Q}fd\nu$, where $hdx+\nu=\mu$ is the Lebesgue decomposition of $\mu$ and then show that $h=0$ $m$-a.e, or equivalently $\int_{Q}fhdx=0$ for every continuous $f$. But I have been unable to do this. Any suggestions?","I'm stuck on the following qual problem: Let $\{h_{n}\}$ be a sequence of positive continuous functions on the unit cube $Q$ in $\mathbb{R}^{d}$ satisfying the following conditions: $\lim_{n\rightarrow\infty}h_{n}(x)=0$ $m$-a.e. ($m$ denotes the Lebesgue measure on $Q$) $\int_{Q}h_{n}dx=1$ $\forall n$ $\lim_{n\rightarrow\infty}\int_{Q}fh_{n}dx=\int_{Q}fd\mu$ for every continuous function $f$ on $Q$. Prove that $\mu\perp m$ or give a counterexample. My intuition suggests to me that $\mu\perp m$ since $h_{n}\rightarrow 0$ a.e. and therefore must become very large on sets of small Lebesgue measure; however, I'm struggling to prove my guess. My thought was to write the $\int_{Q}fd\mu=\int_{Q}fh dx+\int_{Q}fd\nu$, where $hdx+\nu=\mu$ is the Lebesgue decomposition of $\mu$ and then show that $h=0$ $m$-a.e, or equivalently $\int_{Q}fhdx=0$ for every continuous $f$. But I have been unable to do this. Any suggestions?",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
67,Prove that the sequence converges absolutely,Prove that the sequence converges absolutely,,Let $\{a_n\}$ and $\{r_n\}$ be two sequences of real numbers such that $\sum_{n=1}^\infty |a_n|< \infty.$ Prove that $$ \displaystyle \sum_{n=1}^{\infty} \frac{a_n}{\sqrt{|x-r_n|}} $$ converges absolutely for almost every $x \in \mathbb{R}$. Can anyone provide a useful hint to solve the problem ? I am unable to figure out how does almost every $x$ come into picture. Should I use some lebesgue integral ?,Let $\{a_n\}$ and $\{r_n\}$ be two sequences of real numbers such that $\sum_{n=1}^\infty |a_n|< \infty.$ Prove that $$ \displaystyle \sum_{n=1}^{\infty} \frac{a_n}{\sqrt{|x-r_n|}} $$ converges absolutely for almost every $x \in \mathbb{R}$. Can anyone provide a useful hint to solve the problem ? I am unable to figure out how does almost every $x$ come into picture. Should I use some lebesgue integral ?,,"['real-analysis', 'measure-theory']"
68,"For $f \in L^1(\mathbb{R})$ and $y > 0$, we have ${1\over{\sqrt{y}}} \int_\mathbb{R} f(x - t)e^{{-\pi t^2}\over{y}}dt \in L^1(\mathbb{R})$?","For  and , we have ?",f \in L^1(\mathbb{R}) y > 0 {1\over{\sqrt{y}}} \int_\mathbb{R} f(x - t)e^{{-\pi t^2}\over{y}}dt \in L^1(\mathbb{R}),"For $f \in L^1(\mathbb{R})$ and $y > 0$, let$$f_y(x) := {1\over{\sqrt{y}}} \int_\mathbb{R} f(x - t)e^{{-\pi t^2}\over{y}}dt.$$ Do we have $f_y \in L^1(\mathbb{R})$ for every $ y > 0$? Do we have $\lim_{y \to 0} \int_\mathbb{R} |f(x) - f_y(x)|\,dx = 0$? Does there exist $C > 0$ such that, for every $f \in L^1(\mathbb{R})$,$$\left\{x \in \mathbb{R} : \sup_{y > 0} \left|f_y(x)\right| > \lambda\right\} \le {C\over\lambda} \int_\mathbb{R} \left|f(t)\right|\,dt?$$ If $f \in L^1(\mathbb{R}$, then do we have that for a.e. $x \in \mathbb{R}$, $\lim_{y \to 0} f_y(x) = f(x)$?","For $f \in L^1(\mathbb{R})$ and $y > 0$, let$$f_y(x) := {1\over{\sqrt{y}}} \int_\mathbb{R} f(x - t)e^{{-\pi t^2}\over{y}}dt.$$ Do we have $f_y \in L^1(\mathbb{R})$ for every $ y > 0$? Do we have $\lim_{y \to 0} \int_\mathbb{R} |f(x) - f_y(x)|\,dx = 0$? Does there exist $C > 0$ such that, for every $f \in L^1(\mathbb{R})$,$$\left\{x \in \mathbb{R} : \sup_{y > 0} \left|f_y(x)\right| > \lambda\right\} \le {C\over\lambda} \int_\mathbb{R} \left|f(t)\right|\,dt?$$ If $f \in L^1(\mathbb{R}$, then do we have that for a.e. $x \in \mathbb{R}$, $\lim_{y \to 0} f_y(x) = f(x)$?",,"['calculus', 'real-analysis']"
69,"Continuous and increasing on $[0,1]$ and absolutely continuous on $[\varepsilon,1] \forall \varepsilon >0 =>$ absolutely continuous on $[0,1]$?",Continuous and increasing on  and absolutely continuous on  absolutely continuous on ?,"[0,1] [\varepsilon,1] \forall \varepsilon >0 => [0,1]","Does continuous and increasing on $[0,1]$ and absolutely continuous on $[\varepsilon,1]$ imply absolutely continuous on $[0,1]$? EDIT: I am trying to do this via the definition, but I am doing something wrong: $\forall \epsilon >0, \exists \delta$ such that $\forall$ sequences $(a_n,b_n)$ if $\sum_{k=1}^{n}|b_n-a_n|<\delta =>\sum_{k=1}^{n}|f(b_n)-f(a_n)|<\epsilon$ Now, from continuity I have $|b_n-a_n|<\delta => |f(b_n)-f(a_n)|<\epsilon_1$ for some $\epsilon_1>0$. and I can write the sum  $\sum_{k=1}^{n}|f(b_n)-f(a_n)|<\epsilon_1n$ But this has to be wrong since continuity does not imply absolute continuity, I should be using monotonicity at some point in here. But I can't see where.","Does continuous and increasing on $[0,1]$ and absolutely continuous on $[\varepsilon,1]$ imply absolutely continuous on $[0,1]$? EDIT: I am trying to do this via the definition, but I am doing something wrong: $\forall \epsilon >0, \exists \delta$ such that $\forall$ sequences $(a_n,b_n)$ if $\sum_{k=1}^{n}|b_n-a_n|<\delta =>\sum_{k=1}^{n}|f(b_n)-f(a_n)|<\epsilon$ Now, from continuity I have $|b_n-a_n|<\delta => |f(b_n)-f(a_n)|<\epsilon_1$ for some $\epsilon_1>0$. and I can write the sum  $\sum_{k=1}^{n}|f(b_n)-f(a_n)|<\epsilon_1n$ But this has to be wrong since continuity does not imply absolute continuity, I should be using monotonicity at some point in here. But I can't see where.",,"['real-analysis', 'analysis', 'continuity', 'lipschitz-functions']"
70,Prove $\lim_{n\rightarrow \infty} 2^n \sqrt{2-x_n}=\pi$ using the half angle identities.,Prove  using the half angle identities.,\lim_{n\rightarrow \infty} 2^n \sqrt{2-x_n}=\pi,"Given is the sequence $x_1=0,\; x_{n+1}=\sqrt{2+x_n}$. Prove: $$\lim_{n\rightarrow \infty} 2^n \sqrt{2-x_n}=\pi$$ Hint: Use the following formulas: $$\cos\left(\frac{x}{2}\right)=\sqrt{\frac{1+\cos x}{2}}$$ $$\sin\left(\frac{x}{2}\right)=\sqrt{\frac{1-\cos x}{2}}$$ Any idea how to solve this problem?","Given is the sequence $x_1=0,\; x_{n+1}=\sqrt{2+x_n}$. Prove: $$\lim_{n\rightarrow \infty} 2^n \sqrt{2-x_n}=\pi$$ Hint: Use the following formulas: $$\cos\left(\frac{x}{2}\right)=\sqrt{\frac{1+\cos x}{2}}$$ $$\sin\left(\frac{x}{2}\right)=\sqrt{\frac{1-\cos x}{2}}$$ Any idea how to solve this problem?",,"['real-analysis', 'limits', 'trigonometry']"
71,Dense Subspace of $L_{0}^{1}(\mathbb{R}^{n})$,Dense Subspace of,L_{0}^{1}(\mathbb{R}^{n}),"Let $L_{0}^{1}(\mathbb{R}^{n})$ denote the the closed subspace of $L^{1}$ functions whose Fourier transform vanishes at the origin (equivalently, $\int f=0$). At the top of pg. 231 in E.M. Stein, Singular Integrals and Differentiability Properties of Functions , the author claims that a dense subspace of $L_{0}^{1}(\mathbb{R}^{n})$ is the subspace $$\left\{f\in L^{1}(\mathbb{R}^{n}) : \text{supp}(\widehat{f}) \text{ compact }, \text{supp}(\widehat{f})\subset\mathbb{R}^{n}\setminus\left\{0\right\}\right\}$$ and that density ""...can be proved directly by elementary computation, or one can appeal to Wiener's theorem characterizing the maximal ideals of $L^{1}(\mathbb{R}^{n})$."" I am not familiar with Wiener's theorem, so I tried the direct route. I know that $L^{1}$ functions with compactly supported Fourier transforms are dense in $L^{1}$, but I don't see how to extend this result where the support is disjoint from the origin in frequency space. I tried taking a cutoff function $\varphi$ which is $\equiv 1$ on the unit ball $B_{1}(0)$ and supported in $B_{2}(0)$ and then considering the sequence of functions defined by $$f_{\delta}(x):=(\widehat{f}[1-\varphi(\cdot/\delta)])^{\vee}(x)$$ But it's not clear to me at the moment how to estimate the $L^{1}$-norm of $f-f_{\delta}$. Any suggestions?","Let $L_{0}^{1}(\mathbb{R}^{n})$ denote the the closed subspace of $L^{1}$ functions whose Fourier transform vanishes at the origin (equivalently, $\int f=0$). At the top of pg. 231 in E.M. Stein, Singular Integrals and Differentiability Properties of Functions , the author claims that a dense subspace of $L_{0}^{1}(\mathbb{R}^{n})$ is the subspace $$\left\{f\in L^{1}(\mathbb{R}^{n}) : \text{supp}(\widehat{f}) \text{ compact }, \text{supp}(\widehat{f})\subset\mathbb{R}^{n}\setminus\left\{0\right\}\right\}$$ and that density ""...can be proved directly by elementary computation, or one can appeal to Wiener's theorem characterizing the maximal ideals of $L^{1}(\mathbb{R}^{n})$."" I am not familiar with Wiener's theorem, so I tried the direct route. I know that $L^{1}$ functions with compactly supported Fourier transforms are dense in $L^{1}$, but I don't see how to extend this result where the support is disjoint from the origin in frequency space. I tried taking a cutoff function $\varphi$ which is $\equiv 1$ on the unit ball $B_{1}(0)$ and supported in $B_{2}(0)$ and then considering the sequence of functions defined by $$f_{\delta}(x):=(\widehat{f}[1-\varphi(\cdot/\delta)])^{\vee}(x)$$ But it's not clear to me at the moment how to estimate the $L^{1}$-norm of $f-f_{\delta}$. Any suggestions?",,"['real-analysis', 'fourier-analysis', 'harmonic-analysis']"
72,A collection of sequences that cannot all be made to converge,A collection of sequences that cannot all be made to converge,,"I am trying, mostly out of curiosity, to exhibit an infinite countable set $X$ of real, null sequences, such that given a sequence of signs $(s_n)\in\{-1,1\}^{\mathbb{N}},$ at least one of $(x_n)\in X$ will lead $\sum s_nx_n$ to diverge. Every construction I have tried has failed, yet I am convinced such an $X$ should exist. Would appreciate any clever ideas, or a proof of the contrary. Source: my question is inspired by this one , which treats the case $X$ finite: in that particular case there is no such set because it is possible to construct $(s_n)$ so that every $(x_n)\in X$ leads to $\sum s_nx_n$ convergent. The case where $X$ has cardinality $|\mathbb{R}|$ is almost trivial, so I am wondering what happens when $X$ is merely countable.","I am trying, mostly out of curiosity, to exhibit an infinite countable set $X$ of real, null sequences, such that given a sequence of signs $(s_n)\in\{-1,1\}^{\mathbb{N}},$ at least one of $(x_n)\in X$ will lead $\sum s_nx_n$ to diverge. Every construction I have tried has failed, yet I am convinced such an $X$ should exist. Would appreciate any clever ideas, or a proof of the contrary. Source: my question is inspired by this one , which treats the case $X$ finite: in that particular case there is no such set because it is possible to construct $(s_n)$ so that every $(x_n)\in X$ leads to $\sum s_nx_n$ convergent. The case where $X$ has cardinality $|\mathbb{R}|$ is almost trivial, so I am wondering what happens when $X$ is merely countable.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
73,"Prove that there is a smallest point in the interval $[0,1]$ at which the function $f$ attains the value $0$",Prove that there is a smallest point in the interval  at which the function  attains the value,"[0,1] f 0","Suppose that the function $f:[0,1]\rightarrow \mathbb{R}$ is continuous, $f(0)>0$, and $f(1)=0$. Prove that there is a number $x_0 \in (0,1]$ such that $f(x_0)=0$ and $f(x)>0$ for $0\le x < x_0$; that is, there is a smallest point in the interval $[0,1]$ at which the function $f$ attains the value $0$. Let $R=\{x\in(0,1]|f(x)=0\}$, $R$ is bounded below and is not empty since $f(1)=0$ where $1\in R$. So there exists an element $x_0$ in $(0,1]$ be the infimum of $R$. To yield a contradiction, assume that $x_0$ is not a element of $R$, then for all $n$, there exists a sequence $x_n$ in $R$ such that $x_0<x_n<x_0+1/n$. Thus $x_n$ converges to $x_0$ and gives us $f(x_n)=f(x_0)$ since $f(x)$ is continuous. But since for all n $x_n$ in $R$, so we have $f(x_n)=0=f(x_0)$ which contradicts with $x_0$ is not an element of $R$. Therefore, $x_0$ is the minimum element in $R$. To show $f(x)>0$ for $0\leq x<x_0$, we will apply contradiction. To yield a contradiction, we assume that there exists a number $x_1\in[0,x_0)$ such that $f(x_1)<0$. Since $f(0)>0$, now apply the Intermediate Value Theorem, there exists a number $x_2$ in $[0,x_0)$ such that $f(x_2)=0$, but this contradicts with $x_0$ is the minimum number. Therefore, there exists a number $x_0\in(0,1]$ such that $f(x_0)=0$ and $f(x)>0$ for $0\leq x<x_0$. $\square$ Can someone give me a hit or suggestion to write a proof without using the Intermediate Value Theorem and write a direct proof ? Thanks","Suppose that the function $f:[0,1]\rightarrow \mathbb{R}$ is continuous, $f(0)>0$, and $f(1)=0$. Prove that there is a number $x_0 \in (0,1]$ such that $f(x_0)=0$ and $f(x)>0$ for $0\le x < x_0$; that is, there is a smallest point in the interval $[0,1]$ at which the function $f$ attains the value $0$. Let $R=\{x\in(0,1]|f(x)=0\}$, $R$ is bounded below and is not empty since $f(1)=0$ where $1\in R$. So there exists an element $x_0$ in $(0,1]$ be the infimum of $R$. To yield a contradiction, assume that $x_0$ is not a element of $R$, then for all $n$, there exists a sequence $x_n$ in $R$ such that $x_0<x_n<x_0+1/n$. Thus $x_n$ converges to $x_0$ and gives us $f(x_n)=f(x_0)$ since $f(x)$ is continuous. But since for all n $x_n$ in $R$, so we have $f(x_n)=0=f(x_0)$ which contradicts with $x_0$ is not an element of $R$. Therefore, $x_0$ is the minimum element in $R$. To show $f(x)>0$ for $0\leq x<x_0$, we will apply contradiction. To yield a contradiction, we assume that there exists a number $x_1\in[0,x_0)$ such that $f(x_1)<0$. Since $f(0)>0$, now apply the Intermediate Value Theorem, there exists a number $x_2$ in $[0,x_0)$ such that $f(x_2)=0$, but this contradicts with $x_0$ is the minimum number. Therefore, there exists a number $x_0\in(0,1]$ such that $f(x_0)=0$ and $f(x)>0$ for $0\leq x<x_0$. $\square$ Can someone give me a hit or suggestion to write a proof without using the Intermediate Value Theorem and write a direct proof ? Thanks",,['real-analysis']
74,Why doesn't exist a Cousin's lemma for left-tagged partitions?,Why doesn't exist a Cousin's lemma for left-tagged partitions?,,"I am thinking of the possible validity of a  statement like this: given any positive mapping $\delta$ on $[a,b]$, there exists a partition $\, a=a_0<a_1<\cdots<a_n=b \,$ of $ \,[a,b] \,$ so that $$a_r - a_{r-1} < \delta (a_{r-1})\qquad(r=1,\ldots,n)$$ Jean Mawhin says it is not true at the end of his contribution in Fulvia Skof (Ed.) Giuseppe Peano between Mathematics and Logic (2011). I cannot understand if the reason is banal or not, so I need your help to know why it is not true. Thank you in advance. (the question is tied to the need of the so-called straddle lemma in the introduction of the Henstock-Kurzweil integral)","I am thinking of the possible validity of a  statement like this: given any positive mapping $\delta$ on $[a,b]$, there exists a partition $\, a=a_0<a_1<\cdots<a_n=b \,$ of $ \,[a,b] \,$ so that $$a_r - a_{r-1} < \delta (a_{r-1})\qquad(r=1,\ldots,n)$$ Jean Mawhin says it is not true at the end of his contribution in Fulvia Skof (Ed.) Giuseppe Peano between Mathematics and Logic (2011). I cannot understand if the reason is banal or not, so I need your help to know why it is not true. Thank you in advance. (the question is tied to the need of the so-called straddle lemma in the introduction of the Henstock-Kurzweil integral)",,"['real-analysis', 'integration']"
75,$L^2$-norm of a solution of the heat equation,-norm of a solution of the heat equation,L^2,"Let $u\in\mathcal{C}^{2,1}(\mathbb{R}^n\times (0,\infty))$ be a solution of the heat equation $$\left[\begin{array}{ll}u_t-\Delta u=0& \mathrm{in}\ \mathbb{R}^n\times(0,\infty)\\ u(x,0)=u_0(x),&x\in\mathbb{R}^n\end{array}\right.$$ where $u_0\in\mathcal{C}^0_c(\mathbb{R}^n)$. The boundary conditions above are meant in the following sense: $\lim_{t\to 0, x\to x_0}u(x,t)=u_0(x_0)$ for all $x_0\in\mathbb{R}^n$. If we assume in advance that $|u|\to 0$ as $|x|\to\infty$ then the following relation holds for all $t>0$: $$\|u(\cdot,t)\|_{L^2(\mathbb{R}^n)}\leq\|u_0\|_{L^2(\mathbb{R}^n)}.$$ How to prove this result? I am required to use an energy method whence I started with $E(t):=\int_{\mathbb{R}^n}u(x,t)^2dx$ and tried to prove that this function is differentiable for $t>0$ with non-positive derivative. Therefore I introduced $\Omega_n:=B_n(0)$ and considered $E_n(t):=\int_{\Omega_n}u(x,t)^2dx$ (although it is not clear to me why the derivative of $E_n$ converges to the one of $E$). Differentiating $E_n$ and using the PDE as well as integratio by parts I arrive at: $$E_n'(t)=\int_{\Omega_n}2uu_tdx=2\int_{\Omega_n}2u\Delta udx=-2\int_{\Omega_n}|\nabla u|^2dx+2\oint_{\partial\Omega_n}u\frac{\partial u}{\partial\nu}dS.$$ How to proceed? Although I know that the modulus of $u$ converges to 0, I do not know how to conclude something similar for the normal derivative. Is this ansatz reasonable after all? Thank you very in advance!","Let $u\in\mathcal{C}^{2,1}(\mathbb{R}^n\times (0,\infty))$ be a solution of the heat equation $$\left[\begin{array}{ll}u_t-\Delta u=0& \mathrm{in}\ \mathbb{R}^n\times(0,\infty)\\ u(x,0)=u_0(x),&x\in\mathbb{R}^n\end{array}\right.$$ where $u_0\in\mathcal{C}^0_c(\mathbb{R}^n)$. The boundary conditions above are meant in the following sense: $\lim_{t\to 0, x\to x_0}u(x,t)=u_0(x_0)$ for all $x_0\in\mathbb{R}^n$. If we assume in advance that $|u|\to 0$ as $|x|\to\infty$ then the following relation holds for all $t>0$: $$\|u(\cdot,t)\|_{L^2(\mathbb{R}^n)}\leq\|u_0\|_{L^2(\mathbb{R}^n)}.$$ How to prove this result? I am required to use an energy method whence I started with $E(t):=\int_{\mathbb{R}^n}u(x,t)^2dx$ and tried to prove that this function is differentiable for $t>0$ with non-positive derivative. Therefore I introduced $\Omega_n:=B_n(0)$ and considered $E_n(t):=\int_{\Omega_n}u(x,t)^2dx$ (although it is not clear to me why the derivative of $E_n$ converges to the one of $E$). Differentiating $E_n$ and using the PDE as well as integratio by parts I arrive at: $$E_n'(t)=\int_{\Omega_n}2uu_tdx=2\int_{\Omega_n}2u\Delta udx=-2\int_{\Omega_n}|\nabla u|^2dx+2\oint_{\partial\Omega_n}u\frac{\partial u}{\partial\nu}dS.$$ How to proceed? Although I know that the modulus of $u$ converges to 0, I do not know how to conclude something similar for the normal derivative. Is this ansatz reasonable after all? Thank you very in advance!",,"['real-analysis', 'partial-differential-equations', 'heat-equation']"
76,How discontinuous can the limit function be?,How discontinuous can the limit function be?,,"While I was reading an article on Wikipedia which deals with pointwise convergence of a sequence of functions I asked myself how bad can the limit function be? When I say bad I mean how discontinuous it can be? So I have these two questions: 1) Does there exist a sequence of continuous functions $f_n$ defined on the closed (or, if you like you can take open) interval $[a,b]$ (which has finite length) which converges pointwise to the limit function $f$ such that the limit function $f$ has infinite number of discontinuities? 2) Does there exist a sequence of continuous functions $f_n$ defined on the closed (or, if you like you can take open) interval $[a,b]$ (which has finite length) which converges pointwise to the limit function $f$ such that the limit function $f$ has infinite number of discontinuities and for every two points $c\in [a,b]$, $d\in [a,b]$ in which $f$ is continuous there exist point $e\in [c,d]$ in which $f$ is discontinuous? I stumbled upon Egorov´s theorem which says, roughly, that pointwise convergence on some set implies uniform convergence on some smaller set and I know that uniform convergence on some set implies continuity of the limit function on that set but I do not know can these two questions be resolved only with Egorov´s theorem or with some of its modifications, so if someone can help me or point me in the right direction that would be nice.","While I was reading an article on Wikipedia which deals with pointwise convergence of a sequence of functions I asked myself how bad can the limit function be? When I say bad I mean how discontinuous it can be? So I have these two questions: 1) Does there exist a sequence of continuous functions $f_n$ defined on the closed (or, if you like you can take open) interval $[a,b]$ (which has finite length) which converges pointwise to the limit function $f$ such that the limit function $f$ has infinite number of discontinuities? 2) Does there exist a sequence of continuous functions $f_n$ defined on the closed (or, if you like you can take open) interval $[a,b]$ (which has finite length) which converges pointwise to the limit function $f$ such that the limit function $f$ has infinite number of discontinuities and for every two points $c\in [a,b]$, $d\in [a,b]$ in which $f$ is continuous there exist point $e\in [c,d]$ in which $f$ is discontinuous? I stumbled upon Egorov´s theorem which says, roughly, that pointwise convergence on some set implies uniform convergence on some smaller set and I know that uniform convergence on some set implies continuity of the limit function on that set but I do not know can these two questions be resolved only with Egorov´s theorem or with some of its modifications, so if someone can help me or point me in the right direction that would be nice.",,"['real-analysis', 'limits', 'continuity']"
77,Prove that a bounded sequence has two convergent subsequences.,Prove that a bounded sequence has two convergent subsequences.,,"Let $a_1,a_2\dots$ be a bounded sequence in $\mathbb{R}$ that does not converge. Prove that the sequence has two subsequences that converge to different limits Here is my proof: Let $s_n$ be a bounded sequence in $\mathbb{R}$ that does not converge. Then by the Bolzano Weierstrass theorem there exists a subsequence, $s_{n_k}$, that converges. Let $a$ be the limit of $s_{n_k}$. By definition $s_n$ does not converge so it can not converge to $a$. That is, there exists $\epsilon >0$ such that for all $N\in\mathbb{R}$, there exists $n>N$ such that $|s_n-a|\ge\epsilon$. So there exists a sequence $s_m$ that is a subsequence of $s_n$ with $m>N$. By its definition $s_m$ does not have a subsequence that converges to $a$. However, $s_m$ is bounded because $s_n$ is bounded. So by the Bolzano Weierstrass theorem there exists a subsequence, $s_{m_j}$, that converges. Clearly $s_{m_j}$ does not converge to $a$. $\square$ Am I correct in saying ""By its definition $s_m$ does not have a subsequence that converges to $a$."" This is what my proof relies on, but I'm not sure I have justified it correctly. Thanks in advance for any input!","Let $a_1,a_2\dots$ be a bounded sequence in $\mathbb{R}$ that does not converge. Prove that the sequence has two subsequences that converge to different limits Here is my proof: Let $s_n$ be a bounded sequence in $\mathbb{R}$ that does not converge. Then by the Bolzano Weierstrass theorem there exists a subsequence, $s_{n_k}$, that converges. Let $a$ be the limit of $s_{n_k}$. By definition $s_n$ does not converge so it can not converge to $a$. That is, there exists $\epsilon >0$ such that for all $N\in\mathbb{R}$, there exists $n>N$ such that $|s_n-a|\ge\epsilon$. So there exists a sequence $s_m$ that is a subsequence of $s_n$ with $m>N$. By its definition $s_m$ does not have a subsequence that converges to $a$. However, $s_m$ is bounded because $s_n$ is bounded. So by the Bolzano Weierstrass theorem there exists a subsequence, $s_{m_j}$, that converges. Clearly $s_{m_j}$ does not converge to $a$. $\square$ Am I correct in saying ""By its definition $s_m$ does not have a subsequence that converges to $a$."" This is what my proof relies on, but I'm not sure I have justified it correctly. Thanks in advance for any input!",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'proof-verification']"
78,How to show $\sum_{n=2}^N g\left(\frac{1}{n}\right)\frac{n}{N} \rightarrow g'(0) $?,How to show ?,\sum_{n=2}^N g\left(\frac{1}{n}\right)\frac{n}{N} \rightarrow g'(0) ,"For any smooth function $g$ that satisfies $g(0)=0$, the following weirdly seems to hold: $$\sum_{n=2}^N g\left(\frac{1}{n}\right)\frac{n}{N} \rightarrow g'(0) $$ as $N \rightarrow \infty$ Can anyone explain or prove?","For any smooth function $g$ that satisfies $g(0)=0$, the following weirdly seems to hold: $$\sum_{n=2}^N g\left(\frac{1}{n}\right)\frac{n}{N} \rightarrow g'(0) $$ as $N \rightarrow \infty$ Can anyone explain or prove?",,"['real-analysis', 'limits', 'summation']"
79,Every open set in the real line is the countable union of disjoint intervals,Every open set in the real line is the countable union of disjoint intervals,,"Prove that every open set in $\mathbb{R}^1$ is the union of an at most countable collection of disjoint intervals. Proof: Let $\mathbb{R}^1\supset G$ is an open set. Then $\forall$ $x\in G$ $\exists \varepsilon_x>0:$ $I_x=(x-\varepsilon_x, x+\varepsilon_x)\subset G$. Then $$G=\cup_{x\in G}I_x.$$ But $\mathbb{R}^1$ is separable then it has a countable base then any open cover has a countable subcover. Then $$G=\cup_{i\geqslant}I_i.$$ How to turn these intervals to disjoint intervals?","Prove that every open set in $\mathbb{R}^1$ is the union of an at most countable collection of disjoint intervals. Proof: Let $\mathbb{R}^1\supset G$ is an open set. Then $\forall$ $x\in G$ $\exists \varepsilon_x>0:$ $I_x=(x-\varepsilon_x, x+\varepsilon_x)\subset G$. Then $$G=\cup_{x\in G}I_x.$$ But $\mathbb{R}^1$ is separable then it has a countable base then any open cover has a countable subcover. Then $$G=\cup_{i\geqslant}I_i.$$ How to turn these intervals to disjoint intervals?",,"['real-analysis', 'general-topology']"
80,Parallelogram law in $L_1$ space,Parallelogram law in  space,L_1,"Exercise 5.5 from Capinski's and Kopp's book ""Measure, Integral and Probability"" asks to show that it is impossible to define an inner product on the space $L^1([0,1])$. In order to get this result we need to show that parallelogram law $$\Vert f+g \Vert^2 + \Vert f-g \Vert^2 = 2( \Vert f\Vert^2 + \Vert g \Vert^2 )  $$ does not hold for $\Vert \cdot \Vert_1$ where $$\Vert f \Vert_1=\int_0^1|f(x)|\mathsf dx.$$ The hint in the book suggests to consider two following functions: \begin{align} f(x)&=\frac12-x\\ g(x)&=x-\frac12. \end{align} However, $g(x)=-f(x)$ and thus $\Vert g \Vert_1 = \Vert f \Vert_1 $ . Moreover, $\Vert f+g \Vert_1 = 0$ and $$\Vert f-g \Vert_1 = \Vert 2f \Vert_1 = 2\Vert f \Vert_1.$$ Finally I get: \begin{align} \Vert f+g \Vert_1^2 + \Vert f-g \Vert_1^2 &= 4\Vert f \Vert_1^2\\ &= 2(2\Vert f \Vert_1^2)\\&= 2( \Vert f\Vert_1^2 + \Vert g \Vert_1^2 ) \end{align} which in turn shows that actually Parallelogram law holds. What did I miss? May it be related to the fact that $g$ is a scaled version of $f$?","Exercise 5.5 from Capinski's and Kopp's book ""Measure, Integral and Probability"" asks to show that it is impossible to define an inner product on the space $L^1([0,1])$. In order to get this result we need to show that parallelogram law $$\Vert f+g \Vert^2 + \Vert f-g \Vert^2 = 2( \Vert f\Vert^2 + \Vert g \Vert^2 )  $$ does not hold for $\Vert \cdot \Vert_1$ where $$\Vert f \Vert_1=\int_0^1|f(x)|\mathsf dx.$$ The hint in the book suggests to consider two following functions: \begin{align} f(x)&=\frac12-x\\ g(x)&=x-\frac12. \end{align} However, $g(x)=-f(x)$ and thus $\Vert g \Vert_1 = \Vert f \Vert_1 $ . Moreover, $\Vert f+g \Vert_1 = 0$ and $$\Vert f-g \Vert_1 = \Vert 2f \Vert_1 = 2\Vert f \Vert_1.$$ Finally I get: \begin{align} \Vert f+g \Vert_1^2 + \Vert f-g \Vert_1^2 &= 4\Vert f \Vert_1^2\\ &= 2(2\Vert f \Vert_1^2)\\&= 2( \Vert f\Vert_1^2 + \Vert g \Vert_1^2 ) \end{align} which in turn shows that actually Parallelogram law holds. What did I miss? May it be related to the fact that $g$ is a scaled version of $f$?",,"['real-analysis', 'functional-analysis', 'vector-spaces', 'normed-spaces', 'inner-products']"
81,How to prove $f_n$ converge uniformly?,How to prove  converge uniformly?,f_n,"For each $n \in \mathbb{N}$ consider the function $f_n : [0,+\infty) \to \mathbb{R}$ given by $$f_n(x) := \sin\left(\sqrt{4\pi^2n^2+x}\right), \ \ \ \ \forall x \ge 0.$$ Prove that $f_n$ converges uniformly on each interval $[0,a]$ with $a > 0$; $f_n$ does not converge uniformly on $[0,+\infty)$. Original image at https://i.sstatic.net/WNTyz.png My attempt: I showed how to prove (2), just let $x_n=\pi^2(1/4+2n)$ then for any $n$, $f_n(x_n)=1$, so $f_n(x_n)$ cannot converge uniformly on $[0,\infty)$. I have question about (1) , If I use Arzela-Ascoli theorem, I can only show there is a subsequence converge uniformly on $[0,a]$. How to conclude that $f_n$ actually converge uniformly? Could someone kindly help? Thanks!","For each $n \in \mathbb{N}$ consider the function $f_n : [0,+\infty) \to \mathbb{R}$ given by $$f_n(x) := \sin\left(\sqrt{4\pi^2n^2+x}\right), \ \ \ \ \forall x \ge 0.$$ Prove that $f_n$ converges uniformly on each interval $[0,a]$ with $a > 0$; $f_n$ does not converge uniformly on $[0,+\infty)$. Original image at https://i.sstatic.net/WNTyz.png My attempt: I showed how to prove (2), just let $x_n=\pi^2(1/4+2n)$ then for any $n$, $f_n(x_n)=1$, so $f_n(x_n)$ cannot converge uniformly on $[0,\infty)$. I have question about (1) , If I use Arzela-Ascoli theorem, I can only show there is a subsequence converge uniformly on $[0,a]$. How to conclude that $f_n$ actually converge uniformly? Could someone kindly help? Thanks!",,"['real-analysis', 'convergence-divergence']"
82,Compact set of real numbers with countably many limit points.,Compact set of real numbers with countably many limit points.,,"Construct a compact set of real numbers whose limit points form a countable set. My example: Let $E_1=\{1\}\cup \{1+1/n: n\in \mathbb{N}\},$ $E_2=\{1/2\}\cup \{1/2+1/n: n>2\},$ $E_3=\{1/3\}\cup \{1/3+1/n: n>6\},\dots,$ $E_k=\{1/k\}\cup \{1/k+1/n: n>k(k-1)\},\dots$ Let $F=\cup_{k=1}^{\infty}E_k$. It's easy to see that $F$ is bounded and $F$ is closed because $F'=\{1/j\}_{j\geqslant 1}$. So $F$ is compact set. Also $F'\sim \mathbb{N}$. Is my example true?","Construct a compact set of real numbers whose limit points form a countable set. My example: Let $E_1=\{1\}\cup \{1+1/n: n\in \mathbb{N}\},$ $E_2=\{1/2\}\cup \{1/2+1/n: n>2\},$ $E_3=\{1/3\}\cup \{1/3+1/n: n>6\},\dots,$ $E_k=\{1/k\}\cup \{1/k+1/n: n>k(k-1)\},\dots$ Let $F=\cup_{k=1}^{\infty}E_k$. It's easy to see that $F$ is bounded and $F$ is closed because $F'=\{1/j\}_{j\geqslant 1}$. So $F$ is compact set. Also $F'\sim \mathbb{N}$. Is my example true?",,"['real-analysis', 'general-topology', 'compactness']"
83,Students and Real Analysis [closed],Students and Real Analysis [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 months ago . Improve this question I am currently working on a project investigating why students tend to struggle when they first encounter Real Analysis and what can be done to improve the situation. I would be very grateful if any of you could offer your perspectives on this matter and/or point me in the direction of any literature/sources that focus on the teaching of Real Analysis (or Maths in general). Many Thanks.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 months ago . Improve this question I am currently working on a project investigating why students tend to struggle when they first encounter Real Analysis and what can be done to improve the situation. I would be very grateful if any of you could offer your perspectives on this matter and/or point me in the direction of any literature/sources that focus on the teaching of Real Analysis (or Maths in general). Many Thanks.",,"['real-analysis', 'education']"
84,How to prove this sequence tends to zero [duplicate],How to prove this sequence tends to zero [duplicate],,"This question already has answers here : Prove $\lim\limits_{n\to\infty}na_n\ln(n)=0$ [duplicate] (3 answers) Closed 8 years ago . Suppose $a_n>0$,$\sum a_n$ converges, $\{na_n\}_{\Bbb N}$ is monotonic, prove   $$\lim na_n\ln n=0$$ My attempt so far has shown that $\{na_n\}$ is decresing: otherwise, $na_n\ge a_1\implies a_n\ge\frac{a_1}{n}\implies \sum a_n=+\infty$ which contradicts the convergence of $\sum a_n$. My friend has got a bit further: $(n+1)a_{n+1}< na_n\implies \frac{a_{n+1}}{a_n}<\frac{n}{n+1}$. He then let $$b_n:=\frac{a_n}{\frac{1}{n\ln n}}$$ and therefore $$\frac{b_{n+1}}{b_n}=\frac{a_{n+1}}{a_n}\cdot\frac{(n+1)\ln(n+1)}{n\ln n}<\frac{\ln(n+1)}{\ln n}$$ However he can't prove the monotony of $b_n$ because the RHS of the inequality is too weak. In fact it only shows $\limsup \frac{b_{n+1}}{b_n}\le 1$, which is not effective here. And I can't come up with a stronger version, either. Can you help me? Any direct help or hint will be appreciated. Thanks!","This question already has answers here : Prove $\lim\limits_{n\to\infty}na_n\ln(n)=0$ [duplicate] (3 answers) Closed 8 years ago . Suppose $a_n>0$,$\sum a_n$ converges, $\{na_n\}_{\Bbb N}$ is monotonic, prove   $$\lim na_n\ln n=0$$ My attempt so far has shown that $\{na_n\}$ is decresing: otherwise, $na_n\ge a_1\implies a_n\ge\frac{a_1}{n}\implies \sum a_n=+\infty$ which contradicts the convergence of $\sum a_n$. My friend has got a bit further: $(n+1)a_{n+1}< na_n\implies \frac{a_{n+1}}{a_n}<\frac{n}{n+1}$. He then let $$b_n:=\frac{a_n}{\frac{1}{n\ln n}}$$ and therefore $$\frac{b_{n+1}}{b_n}=\frac{a_{n+1}}{a_n}\cdot\frac{(n+1)\ln(n+1)}{n\ln n}<\frac{\ln(n+1)}{\ln n}$$ However he can't prove the monotony of $b_n$ because the RHS of the inequality is too weak. In fact it only shows $\limsup \frac{b_{n+1}}{b_n}\le 1$, which is not effective here. And I can't come up with a stronger version, either. Can you help me? Any direct help or hint will be appreciated. Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
85,"How to show that if $\sum_na_n=\infty$ and $a_n\downarrow 0$ then $\sum\limits_n\min(a_{n},\frac{1}{n})=\infty$? [duplicate]",How to show that if  and  then ? [duplicate],"\sum_na_n=\infty a_n\downarrow 0 \sum\limits_n\min(a_{n},\frac{1}{n})=\infty","This question already has an answer here : If $\sum_na_n=\infty$ and $a_n\downarrow 0$ then $\sum\limits_n\min(a_{n},\frac{1}{n})=\infty$ [duplicate] (1 answer) Closed 6 years ago . Suppose $a_n\downarrow 0, \sum\limits_{n=1}^{\infty}a_n=+\infty,   b_n=\min\{a_n,1/n\}$. Prove that $\sum b_n $ diverges. In fact, I have known that two positive divergent series $\sum a_n ~\sum b_n$,  $c_n=\min\{a_n,b_n\}, \sum c_n$ is not always divergent. But I do not know why this above series is surely divergent. Sincerely thanks.","This question already has an answer here : If $\sum_na_n=\infty$ and $a_n\downarrow 0$ then $\sum\limits_n\min(a_{n},\frac{1}{n})=\infty$ [duplicate] (1 answer) Closed 6 years ago . Suppose $a_n\downarrow 0, \sum\limits_{n=1}^{\infty}a_n=+\infty,   b_n=\min\{a_n,1/n\}$. Prove that $\sum b_n $ diverges. In fact, I have known that two positive divergent series $\sum a_n ~\sum b_n$,  $c_n=\min\{a_n,b_n\}, \sum c_n$ is not always divergent. But I do not know why this above series is surely divergent. Sincerely thanks.",,"['real-analysis', 'sequences-and-series']"
86,A question about a polynomial,A question about a polynomial,,"Suppose that $p$ is a real polynomial of degree $n$. Prove that for $|x|<1$,   $$\sum\limits_{m=0}^\infty{p(m)x^m}=h((1-x)^{-1})$$   for some  real polynomial $h$ of degree $n+1$ without the constant term. I have started trying by taking a specific expression of the polynomial $p$, but I have completely lost. I understand that this result generalizes the following identity $$(1-x)^{-1}=\sum\limits_{m=0}^\infty{x^m}$$ for $|x|<1$ but couldn't anymore rather thinking about it. Help is highly appreciated.","Suppose that $p$ is a real polynomial of degree $n$. Prove that for $|x|<1$,   $$\sum\limits_{m=0}^\infty{p(m)x^m}=h((1-x)^{-1})$$   for some  real polynomial $h$ of degree $n+1$ without the constant term. I have started trying by taking a specific expression of the polynomial $p$, but I have completely lost. I understand that this result generalizes the following identity $$(1-x)^{-1}=\sum\limits_{m=0}^\infty{x^m}$$ for $|x|<1$ but couldn't anymore rather thinking about it. Help is highly appreciated.",,"['real-analysis', 'polynomials', 'power-series']"
87,Determining a measure through a class of measure preserving functions,Determining a measure through a class of measure preserving functions,,"Let $\mu$ and $\mu^\prime$ be probability measures over the sigma algebra $\Sigma$ consisting of the Lebesgue measurable subsets of $[0,1]$. Suppose also that $\mu$ and $\mu^\prime$ assign measure $0$ to all and only null sets. (Notation: if $X$ is measurable, let $\mu_X$ denote the renormalized measure over measurable subsets of $X$: $\mu_X(Y)=\mu(Y)/\mu(X)$ (and similarly for $\mu^\prime$).) Now suppose that we have a collection of functions: $$\{f_X:X\rightarrow \overline{X} \mid X\in \Sigma, 0<\lambda(X)<1\}$$ such that for every way of partitioning $[0,1]$ into two sets, $X$ and $\overline{X}$, $f_X$ preserves the measure both between $\mu_X$ and $\mu_{\overline{X}}$ and between $\mu^\prime_X$ and $\mu^\prime_{\overline{X}}$. (In other words $\mu_X(f^{-1}(Y)) = \mu_{\overline{X}}(Y)$, and similarly for $\mu^\prime$.) Does it follow that $\mu=\mu^\prime$?","Let $\mu$ and $\mu^\prime$ be probability measures over the sigma algebra $\Sigma$ consisting of the Lebesgue measurable subsets of $[0,1]$. Suppose also that $\mu$ and $\mu^\prime$ assign measure $0$ to all and only null sets. (Notation: if $X$ is measurable, let $\mu_X$ denote the renormalized measure over measurable subsets of $X$: $\mu_X(Y)=\mu(Y)/\mu(X)$ (and similarly for $\mu^\prime$).) Now suppose that we have a collection of functions: $$\{f_X:X\rightarrow \overline{X} \mid X\in \Sigma, 0<\lambda(X)<1\}$$ such that for every way of partitioning $[0,1]$ into two sets, $X$ and $\overline{X}$, $f_X$ preserves the measure both between $\mu_X$ and $\mu_{\overline{X}}$ and between $\mu^\prime_X$ and $\mu^\prime_{\overline{X}}$. (In other words $\mu_X(f^{-1}(Y)) = \mu_{\overline{X}}(Y)$, and similarly for $\mu^\prime$.) Does it follow that $\mu=\mu^\prime$?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
88,Prove if $f'(x)\geq 1$ then $\exists c$ such that $f(c)=0$.,Prove if  then  such that .,f'(x)\geq 1 \exists c f(c)=0,"Let $f:\mathbb{R}\to\mathbb{R}$ be differentiable on $\mathbb{R}$ If $f'(x)\geq 1$ for all $x\in \mathbb{R}$, then there exists a $c\in \mathbb{R}$ such that $f(c)=0$. I realised that since $f'(x)\geq 1$, $f$ is monotonously increasing, ie: $x\leq y\Rightarrow f(x)\leq f(y)$, and I also realised if $f(x)>0$ for all $x$ or $f(x)<0$ for all $x$ then by IVT there wouldn't be $c$ such that $f(c)=0$. But I dunno where to go from there, nor do i know how to put it into a reasonable proof. Help?","Let $f:\mathbb{R}\to\mathbb{R}$ be differentiable on $\mathbb{R}$ If $f'(x)\geq 1$ for all $x\in \mathbb{R}$, then there exists a $c\in \mathbb{R}$ such that $f(c)=0$. I realised that since $f'(x)\geq 1$, $f$ is monotonously increasing, ie: $x\leq y\Rightarrow f(x)\leq f(y)$, and I also realised if $f(x)>0$ for all $x$ or $f(x)<0$ for all $x$ then by IVT there wouldn't be $c$ such that $f(c)=0$. But I dunno where to go from there, nor do i know how to put it into a reasonable proof. Help?",,"['real-analysis', 'derivatives']"
89,Proof on Riemann's Theorem that any conditionally convergent series can be rearranged to yield a series which converges to any given sum.,Proof on Riemann's Theorem that any conditionally convergent series can be rearranged to yield a series which converges to any given sum.,,"I am looking at the proof of the following theorem from Apostol's Mathematical Analysis. I am having trouble showing the last part that the author left to the reader. I'm trying to show that $y$ is the limit superior of this rearrangement. To do so, I need to show two things based on the definition of limit superior from the text. First, for every $\epsilon \gt 0$ there is an integer $N$ such that $n \gt N$ implies $h_n \lt y + \epsilon$. Second, given $\epsilon \gt 0$ and $m \gt 0$, there is an integer $n \gt m$ such that $h_n \gt y-\epsilon$. Where I put $h_n$ as the rearrangement of the original series. The second condition is immediately satisfied since for every $y_n$ there is a rearrangement greater than it by construction. However, I'm having trouble showing the first part. How can I guarantee that for any $\epsilon$, all but finitely many $h_n$ is less than $y+ \epsilon$. I don't know how to show this part since our construction only guarantees that we have some rearrangement greater than every $y_n$. Finally, how does this theorem lead to the conclusion that any conditionally convergent series of real terms can be rearranged to yield a series which converges to any prescribed sum? I'd greatly appreciate it if anyone could rigorously establish the above facts for me.","I am looking at the proof of the following theorem from Apostol's Mathematical Analysis. I am having trouble showing the last part that the author left to the reader. I'm trying to show that $y$ is the limit superior of this rearrangement. To do so, I need to show two things based on the definition of limit superior from the text. First, for every $\epsilon \gt 0$ there is an integer $N$ such that $n \gt N$ implies $h_n \lt y + \epsilon$. Second, given $\epsilon \gt 0$ and $m \gt 0$, there is an integer $n \gt m$ such that $h_n \gt y-\epsilon$. Where I put $h_n$ as the rearrangement of the original series. The second condition is immediately satisfied since for every $y_n$ there is a rearrangement greater than it by construction. However, I'm having trouble showing the first part. How can I guarantee that for any $\epsilon$, all but finitely many $h_n$ is less than $y+ \epsilon$. I don't know how to show this part since our construction only guarantees that we have some rearrangement greater than every $y_n$. Finally, how does this theorem lead to the conclusion that any conditionally convergent series of real terms can be rearranged to yield a series which converges to any prescribed sum? I'd greatly appreciate it if anyone could rigorously establish the above facts for me.",,"['real-analysis', 'sequences-and-series', 'analysis']"
90,Questions about the definition of convergence,Questions about the definition of convergence,,"I am having difficulty understanding the definition of convergence. I've been rereading and looking at examples during this past week and I haven't made any progress. Definition: We say that {${a_{n}}$} converges to a point $a \in \mathbb{R}$ if for any $\epsilon$, there exists a positive integer $N$ such that for any $n \in \mathbb{N}$ with $n\geq N$, one has $|a_{n}-a|< \epsilon$. My questions: (1) What role does $\epsilon$ play? Is that the actual limit? I thought that $a$ is what we are ""assuming"" is the limit? (2) Why does $n \geq N$? I'm asking this because we were just given the definition of what it means to be convergent (in Real Analysis, not the Calculus sequence), no formal proof. (3) In various examples they are trying to set $N$ to be less than or equal to $\epsilon$. Why? My main issue is that I don't understand how the components of this definition work. I can follow the examples, but I'd rather understand why it works then just take it on blind faith. Thank you for any input/suggestions.","I am having difficulty understanding the definition of convergence. I've been rereading and looking at examples during this past week and I haven't made any progress. Definition: We say that {${a_{n}}$} converges to a point $a \in \mathbb{R}$ if for any $\epsilon$, there exists a positive integer $N$ such that for any $n \in \mathbb{N}$ with $n\geq N$, one has $|a_{n}-a|< \epsilon$. My questions: (1) What role does $\epsilon$ play? Is that the actual limit? I thought that $a$ is what we are ""assuming"" is the limit? (2) Why does $n \geq N$? I'm asking this because we were just given the definition of what it means to be convergent (in Real Analysis, not the Calculus sequence), no formal proof. (3) In various examples they are trying to set $N$ to be less than or equal to $\epsilon$. Why? My main issue is that I don't understand how the components of this definition work. I can follow the examples, but I'd rather understand why it works then just take it on blind faith. Thank you for any input/suggestions.",,"['real-analysis', 'definition']"
91,how to prove $\int{f}d\mu=\sum_{x\in\Omega}f(x)$,how to prove,\int{f}d\mu=\sum_{x\in\Omega}f(x),"Prove that $\int{f}d\mu=\sum_{x\in\Omega}f(x)$ when $f$ is absolutely summable, where $\mu$ is a counting measure on the measure space $(\Omega,\mathscr{F})$. Can someone give me hints?","Prove that $\int{f}d\mu=\sum_{x\in\Omega}f(x)$ when $f$ is absolutely summable, where $\mu$ is a counting measure on the measure space $(\Omega,\mathscr{F})$. Can someone give me hints?",,"['real-analysis', 'lebesgue-integral']"
92,Clarification of proof of Theorem 4.14 in Baby Rudin,Clarification of proof of Theorem 4.14 in Baby Rudin,,"We are proving that $f : X \to Y$, $X$ compact $\Rightarrow f(X)$ is compact. We reach a step saying $$X \subset f^{-1}(V_{\alpha_1}) \cup \dots \cup f^{-1}(V_{\alpha_n})$$ Rudin says that, since $f(f^{-1} (E)) \subset E$, we have $$f(X) \subset V_{\alpha_1} \cup \dots \cup V_{\alpha_n}$$ Here, is it true that he is implicitly using the following: First, $A \subset B \to f(A) \subset f(B)$ to say $X \subset f^{-1}(V_{\alpha_1}) \cup \dots \cup f^{-1}(V_{\alpha_n}) \to f(X) \subset f(f^{-1}(V_{\alpha_1}) \cup \dots \cup f^{-1}(V_{\alpha_n}))$ And then saying that $f^{-1}(A) \cup f^{-1}(B) = f^{-1} (A \cup B)$ to say that $f(f^{-1}(V_{\alpha_1}) \cup \dots \cup f^{-1}(V_{\alpha_n})) = f(f^{-1} (V_{\alpha_1} \cup \dots V_{\alpha_n}))$ and then finally using $f(f^{-1} (E)) \subset E$? I'm not sure if there is something less subtle at play here.","We are proving that $f : X \to Y$, $X$ compact $\Rightarrow f(X)$ is compact. We reach a step saying $$X \subset f^{-1}(V_{\alpha_1}) \cup \dots \cup f^{-1}(V_{\alpha_n})$$ Rudin says that, since $f(f^{-1} (E)) \subset E$, we have $$f(X) \subset V_{\alpha_1} \cup \dots \cup V_{\alpha_n}$$ Here, is it true that he is implicitly using the following: First, $A \subset B \to f(A) \subset f(B)$ to say $X \subset f^{-1}(V_{\alpha_1}) \cup \dots \cup f^{-1}(V_{\alpha_n}) \to f(X) \subset f(f^{-1}(V_{\alpha_1}) \cup \dots \cup f^{-1}(V_{\alpha_n}))$ And then saying that $f^{-1}(A) \cup f^{-1}(B) = f^{-1} (A \cup B)$ to say that $f(f^{-1}(V_{\alpha_1}) \cup \dots \cup f^{-1}(V_{\alpha_n})) = f(f^{-1} (V_{\alpha_1} \cup \dots V_{\alpha_n}))$ and then finally using $f(f^{-1} (E)) \subset E$? I'm not sure if there is something less subtle at play here.",,"['real-analysis', 'proof-verification']"
93,Is there a notion of indefinite Lebesgue integral?,Is there a notion of indefinite Lebesgue integral?,,"When I started studying integration rigorously via the Riemann and Lebesgue integrals, one thing that struck me is that we loose completely the concept of indefinite integrals . Integrals of functions are only defined over a particular set. I was wondering if there is still a way of defining an indefinite Lebesgue integral.","When I started studying integration rigorously via the Riemann and Lebesgue integrals, one thing that struck me is that we loose completely the concept of indefinite integrals . Integrals of functions are only defined over a particular set. I was wondering if there is still a way of defining an indefinite Lebesgue integral.",,"['real-analysis', 'integration', 'lebesgue-integral', 'indefinite-integrals']"
94,$f(\alpha x) = f(x)^{\beta}$ under different constraints,under different constraints,f(\alpha x) = f(x)^{\beta},"With $\alpha > 0,\, \beta \in \Bbb R^*,\, \alpha, \beta \neq 1$ and $f : \Bbb R \to \Bbb R_+^*$, let's consider the functional equation  $$ f(\alpha x) = f(x)^{\beta} \tag{$\Xi$}$$  or equivalently $g(\alpha x) = \beta g(x)$ for $g = \ln f$. The case where $\alpha = \sqrt2$, $\beta = 2$ and $f \in \mathcal C^2$ has already been solved here : Solving $(f(x))^2 = f(\sqrt{2}x)$ (the answer is $\exists \lambda\mid f(x) = e^{\lambda x^2}$). What if we relax/change some of the constraints, for instance: Keeping $f$ regular (say $\mathcal C^{\infty}$) but setting $\alpha, \beta$ generic $f \in \mathcal C^0$ $f \in L^1$ (other ideas?)","With $\alpha > 0,\, \beta \in \Bbb R^*,\, \alpha, \beta \neq 1$ and $f : \Bbb R \to \Bbb R_+^*$, let's consider the functional equation  $$ f(\alpha x) = f(x)^{\beta} \tag{$\Xi$}$$  or equivalently $g(\alpha x) = \beta g(x)$ for $g = \ln f$. The case where $\alpha = \sqrt2$, $\beta = 2$ and $f \in \mathcal C^2$ has already been solved here : Solving $(f(x))^2 = f(\sqrt{2}x)$ (the answer is $\exists \lambda\mid f(x) = e^{\lambda x^2}$). What if we relax/change some of the constraints, for instance: Keeping $f$ regular (say $\mathcal C^{\infty}$) but setting $\alpha, \beta$ generic $f \in \mathcal C^0$ $f \in L^1$ (other ideas?)",,"['real-analysis', 'functional-equations']"
95,"If $\{a_n\}$ and $\{b_n\}$ are Cauchy, then $\{a_n + b_n\}$ is Cauchy.","If  and  are Cauchy, then  is Cauchy.",\{a_n\} \{b_n\} \{a_n + b_n\},"If $\{a_n\}$ and $\{b_n\}$ are Cauchy, then $\{a_n + b_n\}$ is Cauchy. Proof: $|a_{m_1}-a_{n_1}|\lt \epsilon_1$ and $|b_{m_2} - b_{n_2}|\lt \epsilon_2$ Then take $m_3=\max(m_1,m_2),n_3=\max(n_1,n_2)$ Then $|a_{m_3}+b_{m_3} - a_{n_3}-b_{n_3}|\leq |a_{m_3}-a_{n_3}|+|b_{m_3}-b_{n_3}|\lt2\epsilon$ Now I am unsure how to progress. It would work if my original cauchy sequences were less than $\frac{\epsilon}{2}$, but I don't understand how I would obtain this. Thanks","If $\{a_n\}$ and $\{b_n\}$ are Cauchy, then $\{a_n + b_n\}$ is Cauchy. Proof: $|a_{m_1}-a_{n_1}|\lt \epsilon_1$ and $|b_{m_2} - b_{n_2}|\lt \epsilon_2$ Then take $m_3=\max(m_1,m_2),n_3=\max(n_1,n_2)$ Then $|a_{m_3}+b_{m_3} - a_{n_3}-b_{n_3}|\leq |a_{m_3}-a_{n_3}|+|b_{m_3}-b_{n_3}|\lt2\epsilon$ Now I am unsure how to progress. It would work if my original cauchy sequences were less than $\frac{\epsilon}{2}$, but I don't understand how I would obtain this. Thanks",,['real-analysis']
96,Fact in proof of Lebesgue's Differentiation Theorem,Fact in proof of Lebesgue's Differentiation Theorem,,"I'm reading a proof of Lebesgue's Differentiation Theorem, where there is a fact that is not further specified. Let $f \in L^1(\mathbb R^n)$. For $r > 0$ we set  $$f_r(x) := \frac{1}{\lambda^n(\mathbb B(x, r))} \int_{\mathbb B(x, r)} f(y) \, \mathrm dy \; .$$ Now the author states, that  $$ \Vert f_r - f \Vert_{L^1(\mathbb R^n)} \to 0 \quad \text{for } r \to 0 \; .$$ Why does this hold? Is it obvious?","I'm reading a proof of Lebesgue's Differentiation Theorem, where there is a fact that is not further specified. Let $f \in L^1(\mathbb R^n)$. For $r > 0$ we set  $$f_r(x) := \frac{1}{\lambda^n(\mathbb B(x, r))} \int_{\mathbb B(x, r)} f(y) \, \mathrm dy \; .$$ Now the author states, that  $$ \Vert f_r - f \Vert_{L^1(\mathbb R^n)} \to 0 \quad \text{for } r \to 0 \; .$$ Why does this hold? Is it obvious?",,['real-analysis']
97,Lebesgue Measure: show that a subset of R is equal to R,Lebesgue Measure: show that a subset of R is equal to R,,If B is a subset of R such that I) B' has Lebesgue measure zero II) B is closed under addition Show that B = R this is my first course in measure theory.  I only know that nonempty close and open subset of R is equal to R. What do I have from i and ii?  And what property of R should I use?,If B is a subset of R such that I) B' has Lebesgue measure zero II) B is closed under addition Show that B = R this is my first course in measure theory.  I only know that nonempty close and open subset of R is equal to R. What do I have from i and ii?  And what property of R should I use?,,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
98,$\mu$ test for convergence of improper integral of first kind,test for convergence of improper integral of first kind,\mu,"While going through an Indian text on Analysis I found a test for convergence of improper integral. It was stated without proof. I tried to prove it...then some doubts pop up... Statement is this: Let $f(x)$ be bounded and integrable in every closed subinterval of $ (a,\infty)$ , where $a >0$ . Let $\mu$ be a positive number  such that $\lim_{x \rightarrow \infty} x^{\mu}f(x)$ exists. If $\mu > 1 $ , then $\int_{a}^{\infty} f(x)dx$ converges. If $ \mu \leq 1 $ , then $\int_{a}^{\infty} f(x)dx$ diverges. My proof: $\lim_{x \rightarrow \infty} x^{\mu}f(x) =L $ then for suitable $\epsilon >0$ we will get $x_{0} $ such that $(-L+\epsilon)< |x|^{\mu}|f(x)| < (L- \epsilon)$ for $x>x_{0}$ . This will lead to to $|f(x)| < (L- \epsilon)|x|^{-\mu}$ then using comparison test we will get as $\mu$ >1 $\int_{a}^{\infty} f(x)dx$ converges absolutely. But this proof cannot be used for discussing divergence. Even though we use left inequality $|x|^{-\mu}(-L+\epsilon)< |f(x)|$ . There are two problems I felt. One is comparison test is applicable for positive functions. $|x|^{-\mu}(-L+\epsilon)$ need not be positive. Can we solve this problem  by taking out $(-L+\epsilon)$ ? Second problem is this: Even though we got for $\mu \leq 1$ this integral diverge  by comparison test we get $\int_{a}^{\infty} |f(x)|dx$ diverges. It does not lead to the divergence of improper integral as stated by theorem.","While going through an Indian text on Analysis I found a test for convergence of improper integral. It was stated without proof. I tried to prove it...then some doubts pop up... Statement is this: Let be bounded and integrable in every closed subinterval of , where . Let be a positive number  such that exists. If , then converges. If , then diverges. My proof: then for suitable we will get such that for . This will lead to to then using comparison test we will get as >1 converges absolutely. But this proof cannot be used for discussing divergence. Even though we use left inequality . There are two problems I felt. One is comparison test is applicable for positive functions. need not be positive. Can we solve this problem  by taking out ? Second problem is this: Even though we got for this integral diverge  by comparison test we get diverges. It does not lead to the divergence of improper integral as stated by theorem.","f(x)  (a,\infty) a >0 \mu \lim_{x \rightarrow \infty} x^{\mu}f(x) \mu > 1  \int_{a}^{\infty} f(x)dx  \mu \leq 1  \int_{a}^{\infty} f(x)dx \lim_{x \rightarrow \infty} x^{\mu}f(x) =L  \epsilon >0 x_{0}  (-L+\epsilon)< |x|^{\mu}|f(x)| < (L- \epsilon) x>x_{0} |f(x)| < (L- \epsilon)|x|^{-\mu} \mu \int_{a}^{\infty} f(x)dx |x|^{-\mu}(-L+\epsilon)< |f(x)| |x|^{-\mu}(-L+\epsilon) (-L+\epsilon) \mu \leq 1 \int_{a}^{\infty} |f(x)|dx","['real-analysis', 'improper-integrals']"
99,Does convergence in probability imply a.s. convergence in a countable space?,Does convergence in probability imply a.s. convergence in a countable space?,,"Let $(\Omega, \mathcal F,\mathbb P)$ be such that $\Omega$ is countable. I'm trying to find a simple example of random variables $X_n$ which converge to $0$ in probability but not a.s. If $\mathcal F = 2^{\Omega}$ (i.e., $\{\omega\} \in \mathcal F$ for each $\omega$ since $\Omega$ is countable), it is shown here that such random variables do not exist. But now we only assume (of course) that the $X_n$ are measurable. This question is essentially the same (but the measure there need not be finite).","Let $(\Omega, \mathcal F,\mathbb P)$ be such that $\Omega$ is countable. I'm trying to find a simple example of random variables $X_n$ which converge to $0$ in probability but not a.s. If $\mathcal F = 2^{\Omega}$ (i.e., $\{\omega\} \in \mathcal F$ for each $\omega$ since $\Omega$ is countable), it is shown here that such random variables do not exist. But now we only assume (of course) that the $X_n$ are measurable. This question is essentially the same (but the measure there need not be finite).",,"['real-analysis', 'measure-theory', 'probability-theory', 'convergence-divergence']"
