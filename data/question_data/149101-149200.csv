,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Prove that $(B, \|-\|_{\infty})$ complete. B the set of bounded real valued functions on [0,1] which are pointwise limit of continuous functions.","Prove that  complete. B the set of bounded real valued functions on [0,1] which are pointwise limit of continuous functions.","(B, \|-\|_{\infty})","Question: Prove that $(B, \|-\|_{\infty})$ is complete. B the set of bounded real valued functions on [0,1] which are pointwise limit of continuous functions on [0,1]. Context: Old exam problem I'm using to study. Real Analysis by Carothers. I've attempted to avoid a direct proof from the definition using Cauchy sequences by appealing to the fact that ($B_{\infty}, \|-\|_{\infty}$) is a normed space where $B_{\infty}$ is the set of bounded real valued functions on $\mathbb{R}$ that are pointwise limits of continuous functions. So $B \in B_{\infty}$ and by a Theorem a normed spacve is complete if and only if every absolutely summable series in $B$ is summable. I'm having trouble making a solid case for proving the condition in the last part and also how to say that $B$ is indeed a normed space. Thank you in advance.","Question: Prove that $(B, \|-\|_{\infty})$ is complete. B the set of bounded real valued functions on [0,1] which are pointwise limit of continuous functions on [0,1]. Context: Old exam problem I'm using to study. Real Analysis by Carothers. I've attempted to avoid a direct proof from the definition using Cauchy sequences by appealing to the fact that ($B_{\infty}, \|-\|_{\infty}$) is a normed space where $B_{\infty}$ is the set of bounded real valued functions on $\mathbb{R}$ that are pointwise limits of continuous functions. So $B \in B_{\infty}$ and by a Theorem a normed spacve is complete if and only if every absolutely summable series in $B$ is summable. I'm having trouble making a solid case for proving the condition in the last part and also how to say that $B$ is indeed a normed space. Thank you in advance.",,"['real-analysis', 'analysis', 'banach-spaces', 'normed-spaces']"
1,Problem with uniform convergence,Problem with uniform convergence,,"Let $f_n$ ($n=1,2,\dots$) be a sequence of functions $f_n\colon \mathbb R\to \mathbb R$ of class $C^1$ such that $f_n \rightrightarrows 0 $, $f_n' \rightrightarrows 0 $. Assume moreover that functions $f_n(\sqrt{x})$ ($n=1,2,...$) are also of class $C^1[0, \infty)$. Is it then $[f_n(\sqrt{x})]' \rightrightarrows 0$ ?","Let $f_n$ ($n=1,2,\dots$) be a sequence of functions $f_n\colon \mathbb R\to \mathbb R$ of class $C^1$ such that $f_n \rightrightarrows 0 $, $f_n' \rightrightarrows 0 $. Assume moreover that functions $f_n(\sqrt{x})$ ($n=1,2,...$) are also of class $C^1[0, \infty)$. Is it then $[f_n(\sqrt{x})]' \rightrightarrows 0$ ?",,"['real-analysis', 'analysis', 'functions', 'convergence-divergence']"
2,Convergent or divergent series examples,Convergent or divergent series examples,,Suppose $\sum a_n$  is convergent. Is $\sum {{a_n} \over {1+|a_n|}}$ convergent or divergent?,Suppose $\sum a_n$  is convergent. Is $\sum {{a_n} \over {1+|a_n|}}$ convergent or divergent?,,"['calculus', 'sequences-and-series', 'analysis']"
3,A type of local minimum,A type of local minimum,,"Data: $\Omega \subset \mathbb{R}^{n}$ is an open connected (may be unbounded) set, and locally $\partial \Omega$ is aLipschitz graph. $S \subset \partial \Omega$ is measurabel and $H^{n-1}(S)>0.$ The Dirichlet data on $S$ are given by non-negative function $u^0 \in ^{1}_{Loc}(\Omega)$ with $\nabla u^0 \in L^{2}(\Omega)$. The given force function $Q$ is non-negative and measurable. Consider the convex set  \begin{equation} K:=\{ v \in L^{1}_{Loc}(\Omega): \nabla v \in L^{2}(\Omega) \quad \mbox{and} \quad v=u^0 \quad \mbox{on} S\}. \end{equation} We are looking for an absolute minimum of the functional \begin{equation} J(v):= \int_{\Omega}(|\nabla v|^{2} + \chi(\{v>0\})Q^2) \end{equation} in the class $K$. Definition: We call $u \in K$ a local minimum if for some smal $\varepsilon>0$ we have $J(u)\le J(v)$ for every $v \in K$ with   \begin{equation} \|\nabla (u-v)\|_{L^{2}(\Omega)} + \| \chi(\{v>0\}) -\chi(\{u>0\})\|_{L^{1}(\Omega)} \le \varepsilon. \end{equation} Lemma: If $u$ is a minimum local, then $u$ is subharmonic, hence we can assume that    \begin{equation} u(x) = \lim_{r\downarrow 0} \oint_{B_r(x)}u \quad \mbox{for} \quad x \in \Omega, \end{equation}   where $\oint $ denotes the mean value. Proof: For non-negative functions $\xi \in C^{\infty}_{0}(\Omega)$ we have \begin{equation} 0 \le \limsup_{\varepsilon\downarrow 0} \dfrac{1}{2\varepsilon} (J(u- \varepsilon \xi) - J(u)) \le - \int_{\Omega} \nabla \xi \nabla u, \end{equation} that is, $u$ is subharmonic. Then the limit in the assertuion exists for every $x \in \Omega$, and coincides with $u(x)$ for almost all $x$. Lemma: If $u$ is a minimum local, then $0\le u\le\sup_{\Omega}u^0$. Proof:For $|\varepsilon|\le 1$ use $u_\varepsilon:=u-\varepsilon \min (u,0)$ and $u_\varepsilon:=u+\varepsilon \min (\sup_{\Omega}u^0-u,0)$ as a first variation. The lemma suggests that $u$ is subharmonic if for all non-negative $\xi \in C^{\infty}_{0}(\Omega)$ we have  \begin{equation} \int_{\Omega} \nabla \xi \nabla u \le 0 \end{equation} I'd like to know what the relation between this definition of subharmoninic and others. For example, by trudinger, if $u$ is subharmonic we have \begin{equation} u(x) \le \lim_{r\downarrow 0} \oint_{B_r(x)}u \quad \mbox{for} \quad x \in \Omega. \end{equation} Please, correct me if I am wrong. In the last lemma, What is a first variariation? 3.\begin{eqnarray} \limsup_{\varepsilon \downarrow 0}\dfrac{1}{2\varepsilon} (J(u- \varepsilon \xi) - J(u))&=& \limsup_{\varepsilon \downarrow 0}\left  \{\dfrac{1}{2\varepsilon}( \int_{\Omega} -2 \varepsilon \nabla \xi \nabla u + \varepsilon^2 \nabla \xi + \chi(\{u-\varepsilon \xi> 0\}) - \chi(\{u>0\}))\right \} \\ &\le & \limsup_{\varepsilon \downarrow 0}\left  \{\dfrac{1}{2\varepsilon}( \int_{\Omega} -2 \varepsilon \nabla \xi \nabla u + \varepsilon^2 \nabla \xi \right \}\\ & & + \limsup_{\varepsilon \downarrow 0}\left  \{ \chi(\{u-\varepsilon \xi>0\}) - \chi(\{u>0\}))\right \} \\ &\le& \int_{\Omega} -\nabla \xi \nabla u + \limsup_{\varepsilon \downarrow 0}\dfrac{1}{2\varepsilon}\left  \{ \chi(\{u-\varepsilon \xi>0\}) - \chi(\{u>0\}))\right \} \end{eqnarray} Am I right here? Why $\limsup_{\varepsilon \downarrow 0}\dfrac{1}{2\varepsilon}\left  \{ \chi(\{u-\varepsilon \xi>0\}) - \chi(\{u>0\}))\right \} \le 0$? If you want the details can be found in the article Alt, H. M. and Caffarelli, L. A. Existence and regularity for a minimum problem with free boundary. J. Reine Angew. Math., 325, (1981), 105–144. . I thank any hint.","Data: $\Omega \subset \mathbb{R}^{n}$ is an open connected (may be unbounded) set, and locally $\partial \Omega$ is aLipschitz graph. $S \subset \partial \Omega$ is measurabel and $H^{n-1}(S)>0.$ The Dirichlet data on $S$ are given by non-negative function $u^0 \in ^{1}_{Loc}(\Omega)$ with $\nabla u^0 \in L^{2}(\Omega)$. The given force function $Q$ is non-negative and measurable. Consider the convex set  \begin{equation} K:=\{ v \in L^{1}_{Loc}(\Omega): \nabla v \in L^{2}(\Omega) \quad \mbox{and} \quad v=u^0 \quad \mbox{on} S\}. \end{equation} We are looking for an absolute minimum of the functional \begin{equation} J(v):= \int_{\Omega}(|\nabla v|^{2} + \chi(\{v>0\})Q^2) \end{equation} in the class $K$. Definition: We call $u \in K$ a local minimum if for some smal $\varepsilon>0$ we have $J(u)\le J(v)$ for every $v \in K$ with   \begin{equation} \|\nabla (u-v)\|_{L^{2}(\Omega)} + \| \chi(\{v>0\}) -\chi(\{u>0\})\|_{L^{1}(\Omega)} \le \varepsilon. \end{equation} Lemma: If $u$ is a minimum local, then $u$ is subharmonic, hence we can assume that    \begin{equation} u(x) = \lim_{r\downarrow 0} \oint_{B_r(x)}u \quad \mbox{for} \quad x \in \Omega, \end{equation}   where $\oint $ denotes the mean value. Proof: For non-negative functions $\xi \in C^{\infty}_{0}(\Omega)$ we have \begin{equation} 0 \le \limsup_{\varepsilon\downarrow 0} \dfrac{1}{2\varepsilon} (J(u- \varepsilon \xi) - J(u)) \le - \int_{\Omega} \nabla \xi \nabla u, \end{equation} that is, $u$ is subharmonic. Then the limit in the assertuion exists for every $x \in \Omega$, and coincides with $u(x)$ for almost all $x$. Lemma: If $u$ is a minimum local, then $0\le u\le\sup_{\Omega}u^0$. Proof:For $|\varepsilon|\le 1$ use $u_\varepsilon:=u-\varepsilon \min (u,0)$ and $u_\varepsilon:=u+\varepsilon \min (\sup_{\Omega}u^0-u,0)$ as a first variation. The lemma suggests that $u$ is subharmonic if for all non-negative $\xi \in C^{\infty}_{0}(\Omega)$ we have  \begin{equation} \int_{\Omega} \nabla \xi \nabla u \le 0 \end{equation} I'd like to know what the relation between this definition of subharmoninic and others. For example, by trudinger, if $u$ is subharmonic we have \begin{equation} u(x) \le \lim_{r\downarrow 0} \oint_{B_r(x)}u \quad \mbox{for} \quad x \in \Omega. \end{equation} Please, correct me if I am wrong. In the last lemma, What is a first variariation? 3.\begin{eqnarray} \limsup_{\varepsilon \downarrow 0}\dfrac{1}{2\varepsilon} (J(u- \varepsilon \xi) - J(u))&=& \limsup_{\varepsilon \downarrow 0}\left  \{\dfrac{1}{2\varepsilon}( \int_{\Omega} -2 \varepsilon \nabla \xi \nabla u + \varepsilon^2 \nabla \xi + \chi(\{u-\varepsilon \xi> 0\}) - \chi(\{u>0\}))\right \} \\ &\le & \limsup_{\varepsilon \downarrow 0}\left  \{\dfrac{1}{2\varepsilon}( \int_{\Omega} -2 \varepsilon \nabla \xi \nabla u + \varepsilon^2 \nabla \xi \right \}\\ & & + \limsup_{\varepsilon \downarrow 0}\left  \{ \chi(\{u-\varepsilon \xi>0\}) - \chi(\{u>0\}))\right \} \\ &\le& \int_{\Omega} -\nabla \xi \nabla u + \limsup_{\varepsilon \downarrow 0}\dfrac{1}{2\varepsilon}\left  \{ \chi(\{u-\varepsilon \xi>0\}) - \chi(\{u>0\}))\right \} \end{eqnarray} Am I right here? Why $\limsup_{\varepsilon \downarrow 0}\dfrac{1}{2\varepsilon}\left  \{ \chi(\{u-\varepsilon \xi>0\}) - \chi(\{u>0\}))\right \} \le 0$? If you want the details can be found in the article Alt, H. M. and Caffarelli, L. A. Existence and regularity for a minimum problem with free boundary. J. Reine Angew. Math., 325, (1981), 105–144. . I thank any hint.",,"['analysis', 'measure-theory', 'partial-differential-equations']"
4,Inequality between volume and its projections,Inequality between volume and its projections,,"Let $A \subset \mathbb{R}^3$ be connected and let's define $A_1, A_2, A_3 \subset \mathbb{R}^2$ as projections of $A$ onto three perpendicular (to each other) planes. Show that: $$|A| \le \sqrt{|A_1| |A_2| |A_3|}\;,$$ where $|\cdot|$ is volume when applied to $A$ and area whenas $A_{1,2,3}$.","Let $A \subset \mathbb{R}^3$ be connected and let's define $A_1, A_2, A_3 \subset \mathbb{R}^2$ as projections of $A$ onto three perpendicular (to each other) planes. Show that: $$|A| \le \sqrt{|A_1| |A_2| |A_3|}\;,$$ where $|\cdot|$ is volume when applied to $A$ and area whenas $A_{1,2,3}$.",,"['analysis', 'inequality']"
5,"For a $1$-form $h$, why does $\int_\Gamma \varphi^*h=\int_{\varphi\circ\Gamma}h$?","For a -form , why does ?",1 h \int_\Gamma \varphi^*h=\int_{\varphi\circ\Gamma}h,"I'm trying to understand why for a differentiable arc $\Gamma:[a,b]\to\Omega$ and a $1$-form $h=fdx+gdy$, then $$ \int_\Gamma\varphi^*h=\int_{\varphi\circ\Gamma}h? $$ For background, $\Omega$ is an open set in $\mathbb{C}$, and $\varphi:\Omega\to\mathbb{C}$ a smooth map. For a function $f$, I have the definition $\varphi^*f=f\circ\phi$, (when this makes sense for $f$ of course). I also have the definitions  $$ \varphi^*\,dx=\frac{\partial u}{\partial x}\,dx+\frac{\partial u}{\partial y}\,dy, \qquad \varphi^*dy=\frac{\partial v}{\partial x}\,dx+\frac{\partial v}{\partial y}\,dy, $$ where $u$ is the $x$ component of $\varphi$ and $v$ is the $y$ component. For a $1$-form $h=f\,dx+g\,dy$,  $$ \varphi^*h=(\varphi^*f)\varphi^*\,dx+(\varphi^*g)\varphi^*\,dy. $$ I calculate  \begin{align*} \int_\Gamma \varphi^*h &= \int_\Gamma(\varphi^*f)\varphi^*dx+\int_\Gamma (\varphi^*g)\varphi^*dy\\ &= \int_\Gamma(f\circ\varphi)\frac{\partial u}{\partial x}dx+ \int_\Gamma(f\circ\varphi)\frac{\partial u}{\partial y}dy+ \int_\Gamma(g\circ\varphi)\frac{\partial v}{\partial x}dx+ \int_\Gamma(f\circ\varphi)\frac{\partial v}{\partial y}dy\\ &= \int_\Gamma\left((f\circ\varphi)\frac{\partial u}{\partial x}+(g\circ\varphi)\frac{\partial v}{\partial x}\right)dx+\int_\Gamma\left((f\circ\varphi)\frac{\partial u}{\partial y}+(g\circ\varphi)\frac{\partial v}{\partial y}\right)dy \end{align*} but I don't see if this fits into the form $\int_{\varphi\circ\Gamma}h=\int_{\varphi\circ\Gamma}fdx+\int_{\varphi\circ\Gamma}gdy$? Can it be made to fit? Thanks.","I'm trying to understand why for a differentiable arc $\Gamma:[a,b]\to\Omega$ and a $1$-form $h=fdx+gdy$, then $$ \int_\Gamma\varphi^*h=\int_{\varphi\circ\Gamma}h? $$ For background, $\Omega$ is an open set in $\mathbb{C}$, and $\varphi:\Omega\to\mathbb{C}$ a smooth map. For a function $f$, I have the definition $\varphi^*f=f\circ\phi$, (when this makes sense for $f$ of course). I also have the definitions  $$ \varphi^*\,dx=\frac{\partial u}{\partial x}\,dx+\frac{\partial u}{\partial y}\,dy, \qquad \varphi^*dy=\frac{\partial v}{\partial x}\,dx+\frac{\partial v}{\partial y}\,dy, $$ where $u$ is the $x$ component of $\varphi$ and $v$ is the $y$ component. For a $1$-form $h=f\,dx+g\,dy$,  $$ \varphi^*h=(\varphi^*f)\varphi^*\,dx+(\varphi^*g)\varphi^*\,dy. $$ I calculate  \begin{align*} \int_\Gamma \varphi^*h &= \int_\Gamma(\varphi^*f)\varphi^*dx+\int_\Gamma (\varphi^*g)\varphi^*dy\\ &= \int_\Gamma(f\circ\varphi)\frac{\partial u}{\partial x}dx+ \int_\Gamma(f\circ\varphi)\frac{\partial u}{\partial y}dy+ \int_\Gamma(g\circ\varphi)\frac{\partial v}{\partial x}dx+ \int_\Gamma(f\circ\varphi)\frac{\partial v}{\partial y}dy\\ &= \int_\Gamma\left((f\circ\varphi)\frac{\partial u}{\partial x}+(g\circ\varphi)\frac{\partial v}{\partial x}\right)dx+\int_\Gamma\left((f\circ\varphi)\frac{\partial u}{\partial y}+(g\circ\varphi)\frac{\partial v}{\partial y}\right)dy \end{align*} but I don't see if this fits into the form $\int_{\varphi\circ\Gamma}h=\int_{\varphi\circ\Gamma}fdx+\int_{\varphi\circ\Gamma}gdy$? Can it be made to fit? Thanks.",,"['analysis', 'differential-forms']"
6,A simple application of Hölders inequality (I think),A simple application of Hölders inequality (I think),,"I'm reading a paper where the following inequality appears. $$ \| \widehat{f} \|^2_{L^2(d\mu)} \leq \| f \ast \widehat{\mu} \|_p \| f \|_{p^\prime} $$ where $f$ is a real-valued measurable function on $\mathbb{R}^n$, $\mu$ is a positive measure on $\mathbb{R}^n$, and $\frac{1}{p} + \frac{1}{p^{\prime}} = 1$.  I think $\| \cdot \|_p$ and $\| \cdot \|_{p^{\prime}}$ are with respect to Lebesgue measure. $$ \widehat{\mu}(\xi) = \int e^{-2 \pi i x \xi} d\mu(x) $$ I feel like this should be a consequence of Hölder's inequality and some identities relating convolution and the Fourier transform, but I can't figure it out. Can someone please help?","I'm reading a paper where the following inequality appears. $$ \| \widehat{f} \|^2_{L^2(d\mu)} \leq \| f \ast \widehat{\mu} \|_p \| f \|_{p^\prime} $$ where $f$ is a real-valued measurable function on $\mathbb{R}^n$, $\mu$ is a positive measure on $\mathbb{R}^n$, and $\frac{1}{p} + \frac{1}{p^{\prime}} = 1$.  I think $\| \cdot \|_p$ and $\| \cdot \|_{p^{\prime}}$ are with respect to Lebesgue measure. $$ \widehat{\mu}(\xi) = \int e^{-2 \pi i x \xi} d\mu(x) $$ I feel like this should be a consequence of Hölder's inequality and some identities relating convolution and the Fourier transform, but I can't figure it out. Can someone please help?",,"['real-analysis', 'analysis', 'fourier-analysis']"
7,Cauchy sequence,Cauchy sequence,,"Show that if $(x_{n})_{n}$ is a Cauchy sequence in X and $\lambda \in \mathbb{R}$, then the sequence $(\lambda x_{n})_{n}$, is also Cauchy in X. We know that for $(x_{n})_{n}$, we have $$\forall \epsilon >0:\exists N\in \mathbb{N} : n,m\ge N\implies ||x_{n}-x_{m}||\le \epsilon$$ We can also assume that $$||\lambda (x_{n}-x_{m})||\le \epsilon$$ So to prove this, we can say that: $$||\lambda (x_{n}-x_{m})||\le |\lambda |\cdot||x_{n}-x_{m}|| \le |\lambda|\epsilon$$ But I can't help but feel dubious about having the $\lambda$ at the end. Any tips?","Show that if $(x_{n})_{n}$ is a Cauchy sequence in X and $\lambda \in \mathbb{R}$, then the sequence $(\lambda x_{n})_{n}$, is also Cauchy in X. We know that for $(x_{n})_{n}$, we have $$\forall \epsilon >0:\exists N\in \mathbb{N} : n,m\ge N\implies ||x_{n}-x_{m}||\le \epsilon$$ We can also assume that $$||\lambda (x_{n}-x_{m})||\le \epsilon$$ So to prove this, we can say that: $$||\lambda (x_{n}-x_{m})||\le |\lambda |\cdot||x_{n}-x_{m}|| \le |\lambda|\epsilon$$ But I can't help but feel dubious about having the $\lambda$ at the end. Any tips?",,['analysis']
8,Integration over a bounded set,Integration over a bounded set,,"Let $f,g: S\to \mathbb{R}$. If we assume $f$ and $g$ are integrable over $S$, then I'm trying to show: If $f$ and $g$ agree except on a set of measure zero, then $\int_S f=\int_S g$. Also, how can we show: If $f(x) \le g(x)$ for $x \in S$ and $\int_S f=\int_S g$, then $f$ and $g$ agree except on a set of measure zero.","Let $f,g: S\to \mathbb{R}$. If we assume $f$ and $g$ are integrable over $S$, then I'm trying to show: If $f$ and $g$ agree except on a set of measure zero, then $\int_S f=\int_S g$. Also, how can we show: If $f(x) \le g(x)$ for $x \in S$ and $\int_S f=\int_S g$, then $f$ and $g$ agree except on a set of measure zero.",,['analysis']
9,Minimize distance between 2 functions,Minimize distance between 2 functions,,"Just so you know, this is a homework question, and I basically need help with the steps to solve this problem. I understand what it's asking; however, my attempts haven't worked out, and I'm probably over-complicating it. Anyway, here it is: I have a space $C([-\pi/2,\pi/2])$ with the supremum norm: $||f||_\infty = \sup\{|f(x)| : x \in [-\pi/2,\pi/2]\}$ and I need to find the closest linear function $g(x) = cx$ closest to the function $f(x) = \sin(x)$ w.r.t. this norm. In other words, it appears that I need to find $c$ that minimizes the distance between these two curves, or that generates the smallest least upper bound for the function $h(c) = |\sin(x)-cx|$ for $x \in [-\pi/2,\pi/2]$. I thought this would be the same as minimizing the area between these two curves as a function of $c$, but it's not optimal to find the points of intersection of these functions as it would be dependent on $c$. Any ideas?","Just so you know, this is a homework question, and I basically need help with the steps to solve this problem. I understand what it's asking; however, my attempts haven't worked out, and I'm probably over-complicating it. Anyway, here it is: I have a space $C([-\pi/2,\pi/2])$ with the supremum norm: $||f||_\infty = \sup\{|f(x)| : x \in [-\pi/2,\pi/2]\}$ and I need to find the closest linear function $g(x) = cx$ closest to the function $f(x) = \sin(x)$ w.r.t. this norm. In other words, it appears that I need to find $c$ that minimizes the distance between these two curves, or that generates the smallest least upper bound for the function $h(c) = |\sin(x)-cx|$ for $x \in [-\pi/2,\pi/2]$. I thought this would be the same as minimizing the area between these two curves as a function of $c$, but it's not optimal to find the points of intersection of these functions as it would be dependent on $c$. Any ideas?",,"['real-analysis', 'analysis', 'optimization', 'normed-spaces']"
10,Is this set closed?,Is this set closed?,,"$X=(C[0,1],\rho_\infty)$ where $\rho_\infty$ is the uniform norm. $M\in(0,\infty)$, define $A=\{f\in X:f(0)=0, f\;\mathrm{differentiable\;on}\;(0,1)\;,|f^\prime(x)|\leq M\;\;\forall x\in(0,1)\}$. I was trying to prove that $A$ is compact, I wanted to use Ascoli-Arzelà, so I proved that $A$ is equicontinuous and bounded, but I don't know how to prove that it's closed, what is giving me problems is that if $(f_n)\subset A$ and $f_n\rightarrow f$ then $f$ is differentiable, I'm not even sure that it's true. I mean, in general it's not true, but I don't know if the other conditions of $A$ force this to be true, could you help me? EDIT: following the hint of yoyo I think that we should take the sequence $f_n(x)=1/2-|x-1/2|$ if $|x|\geq 1/2+1/n$, and $f_n(x)=1/2-n/2(x-1/2)^2-1/(2n)$ if $|x|\leq 1/2+1/n$, but I'm having problems to prove that this sequence converges uniformly to $1/2-|x-1/2|$, any hints? EDIT EDIT: following the hint of Jonas I think I solve the problem: modify the function as Jonas said, then $f(x)-f_n(x)=-|x-1/2|+n/2(x-1/2)^2+1/(2n)$ if $|x-1/2|\leq 1/n$ and zero elsewhere. But $-1/(2n)=-1/n+0+1/(2n)\leq-|x-1/2|+n/2(x-1/2)^2+1/(2n)\leq0+n/2\cdot1/n^2+1/(2n)=1/n$ and so $|f(x)-f_n(x)|\leq 1/(2n)$ and the convergence is uniform. Am I right?","$X=(C[0,1],\rho_\infty)$ where $\rho_\infty$ is the uniform norm. $M\in(0,\infty)$, define $A=\{f\in X:f(0)=0, f\;\mathrm{differentiable\;on}\;(0,1)\;,|f^\prime(x)|\leq M\;\;\forall x\in(0,1)\}$. I was trying to prove that $A$ is compact, I wanted to use Ascoli-Arzelà, so I proved that $A$ is equicontinuous and bounded, but I don't know how to prove that it's closed, what is giving me problems is that if $(f_n)\subset A$ and $f_n\rightarrow f$ then $f$ is differentiable, I'm not even sure that it's true. I mean, in general it's not true, but I don't know if the other conditions of $A$ force this to be true, could you help me? EDIT: following the hint of yoyo I think that we should take the sequence $f_n(x)=1/2-|x-1/2|$ if $|x|\geq 1/2+1/n$, and $f_n(x)=1/2-n/2(x-1/2)^2-1/(2n)$ if $|x|\leq 1/2+1/n$, but I'm having problems to prove that this sequence converges uniformly to $1/2-|x-1/2|$, any hints? EDIT EDIT: following the hint of Jonas I think I solve the problem: modify the function as Jonas said, then $f(x)-f_n(x)=-|x-1/2|+n/2(x-1/2)^2+1/(2n)$ if $|x-1/2|\leq 1/n$ and zero elsewhere. But $-1/(2n)=-1/n+0+1/(2n)\leq-|x-1/2|+n/2(x-1/2)^2+1/(2n)\leq0+n/2\cdot1/n^2+1/(2n)=1/n$ and so $|f(x)-f_n(x)|\leq 1/(2n)$ and the convergence is uniform. Am I right?",,"['real-analysis', 'analysis']"
11,Deriving an asymptotic formula,Deriving an asymptotic formula,,"I'm doing some exercises in a book on asymptotic analysis.  While I think I found a solution to this problem, I'm not entirely sure if it's correct, and I want to make sure that I know what's going on. The exercise is to show that $\int_1^x (1+t^{-1})^t dt = ex - \frac{1}{2}e \log{x} + O(1)$, for $x > 1$, with the hint to first show that $e^{-1}(1+t^{-1})^t = 1 -\frac{1}{2}t^{-1} + O(t^{-2})$, $(t > 1)$.  Getting the first statement from the second is simple.  Using the fact that $\log(1+t^{-1}) = t^{-1} - \frac{1/2}t^{-2} + O(t^{-3})$ for $t>1$, we have that $$e^{-1}(1+t^{-1})^t = \exp(t\log(1+t^{-1})-1)=\exp(-\frac{1}{2}t^{-1}+O(t^{-2})))$$ At this point, I have to use what feels like a really handwavey argument and argue that, since $-\frac{1}{2}t^{-1}+O(t^{-2})) = 1 -\frac{1}{2}t^{-1} + O(t^{-2})$, evaluating a decreasing function (which $\exp$ is on the LHS, for the given values of $t$) on the left-hand side will preserve the relation. Thus, my question is whether my derivation is correct, especially the last part.  In addition, I'm curious why the last part is necessary- it feels like throwing away a lot of information (although I might just not yet be used to how much information to throw away in asymptotic proofs), and my only guess as to why is to get something that's easy to integrate.  Is there some other reason?","I'm doing some exercises in a book on asymptotic analysis.  While I think I found a solution to this problem, I'm not entirely sure if it's correct, and I want to make sure that I know what's going on. The exercise is to show that $\int_1^x (1+t^{-1})^t dt = ex - \frac{1}{2}e \log{x} + O(1)$, for $x > 1$, with the hint to first show that $e^{-1}(1+t^{-1})^t = 1 -\frac{1}{2}t^{-1} + O(t^{-2})$, $(t > 1)$.  Getting the first statement from the second is simple.  Using the fact that $\log(1+t^{-1}) = t^{-1} - \frac{1/2}t^{-2} + O(t^{-3})$ for $t>1$, we have that $$e^{-1}(1+t^{-1})^t = \exp(t\log(1+t^{-1})-1)=\exp(-\frac{1}{2}t^{-1}+O(t^{-2})))$$ At this point, I have to use what feels like a really handwavey argument and argue that, since $-\frac{1}{2}t^{-1}+O(t^{-2})) = 1 -\frac{1}{2}t^{-1} + O(t^{-2})$, evaluating a decreasing function (which $\exp$ is on the LHS, for the given values of $t$) on the left-hand side will preserve the relation. Thus, my question is whether my derivation is correct, especially the last part.  In addition, I'm curious why the last part is necessary- it feels like throwing away a lot of information (although I might just not yet be used to how much information to throw away in asymptotic proofs), and my only guess as to why is to get something that's easy to integrate.  Is there some other reason?",,"['analysis', 'asymptotics']"
12,Constructing a Measure from a Function,Constructing a Measure from a Function,,"A well-known result is that we can always construct a countably additive function $\mu$ from a nondecreasing and right-continuous function $G$. More specifically, we define on the semiring $\mathcal{C}$ of all intervals $(a,b]$, $$\mu((a,b])=G(b)-G(a),$$ where $\mu$ is the Lebesgue measure when $G$ is the identity mapping. I'm curious if the following function gets this property as well: Fix a countable set $C=\{c_n:n\in\mathbb{N}\}\subset\mathbb{R}$ where each $c_n$ is distinct, and a countable set $\{a_n:n\in\mathbb{N}\}\subset\mathbb{R}$ where each $a_n$ is non-negative and $\sum_na_n<\infty$. Define $G:\mathbb{R}\rightarrow\mathbb{R}$ such that $$x\longmapsto\sum\{a_n:c_n\leq x\}.$$ That is, is this function right-continuous and non-decreasing? Furthermore, does the measure $\mu$ constructed from this function satisfy $\mu(\{c_n\})=a_n$ and $\mu(\mathbb{R}\backslash C)=0?$ EDIT: $G$ being right-continuous and non-decreasing seems pretty easy to see. Non-decreasing is rather obvious from the property of the $a_n$'s and I basically argued right-continuity to myself in the comment box below. However for the second part, I'm not sure how to approach either property. How do you interpret $\mu(\{c_n\})$ to get only $a_n$ remaining in the subtraction? I appreciate any help!","A well-known result is that we can always construct a countably additive function $\mu$ from a nondecreasing and right-continuous function $G$. More specifically, we define on the semiring $\mathcal{C}$ of all intervals $(a,b]$, $$\mu((a,b])=G(b)-G(a),$$ where $\mu$ is the Lebesgue measure when $G$ is the identity mapping. I'm curious if the following function gets this property as well: Fix a countable set $C=\{c_n:n\in\mathbb{N}\}\subset\mathbb{R}$ where each $c_n$ is distinct, and a countable set $\{a_n:n\in\mathbb{N}\}\subset\mathbb{R}$ where each $a_n$ is non-negative and $\sum_na_n<\infty$. Define $G:\mathbb{R}\rightarrow\mathbb{R}$ such that $$x\longmapsto\sum\{a_n:c_n\leq x\}.$$ That is, is this function right-continuous and non-decreasing? Furthermore, does the measure $\mu$ constructed from this function satisfy $\mu(\{c_n\})=a_n$ and $\mu(\mathbb{R}\backslash C)=0?$ EDIT: $G$ being right-continuous and non-decreasing seems pretty easy to see. Non-decreasing is rather obvious from the property of the $a_n$'s and I basically argued right-continuity to myself in the comment box below. However for the second part, I'm not sure how to approach either property. How do you interpret $\mu(\{c_n\})$ to get only $a_n$ remaining in the subtraction? I appreciate any help!",,"['real-analysis', 'analysis', 'measure-theory']"
13,Singular functions,Singular functions,,"I will start with a definition. A monotone function $f$ on $[a,b]$ is called singular if $f'=0$ almost everywhere. Let $f$ be a nondecreasing function on $[a,b]$ such that given $\epsilon~,~\delta\gt 0$, $\exists$ a finite collection $\{[y_k,x_k]\}$ of nonoverlapping intervals such that $$\sum |x_k-y_k|\lt \delta~~~~~\text{and}~~~~\sum\left(f(x_k)-f(y_k)\right)\gt f(b)-f(a)-\epsilon.$$ I would like to show that $f$ is singular. Attempt: From a previous exercise, I showed that a monotone function $f$ can be written as the sum of an absolutely continuous function, $g$ and a singular function, $h$ .  Thus $$ f = g + h ,~~~\text{where}~~g=\int_a^x f' .$$ My goal is to show that $g=0$ almost everywhere. Let $I=\bigcup (y_k,x_k).$  Then  $$ \int_I f' = \sum \int_{(x_k,y_k)}~f' = \sum\left(f(x_k)-f(y_k)\right)\lt \epsilon.$$ Now, I know that since $f'$ is integrable, there is an $\epsilon$ such that $$\int_{[a,b]\setminus I}~f'\lt \epsilon. $$ But $$\begin{align*} 0\leq \int_a^b f' & = \int_I f'+\int_{[a,b]\setminus I}~f'\\  & \lt \epsilon + \epsilon\\  & = 2\epsilon.  \end{align*}$$ Since $\epsilon$ is arbitrary, we can let $\epsilon \rightarrow 0$ and thus $$ \int_a^b f'=0 ~~\text{and }~~g = 0.$$ Is what I've done right? Is there another way of approaching the problem? Thanks.","I will start with a definition. A monotone function $f$ on $[a,b]$ is called singular if $f'=0$ almost everywhere. Let $f$ be a nondecreasing function on $[a,b]$ such that given $\epsilon~,~\delta\gt 0$, $\exists$ a finite collection $\{[y_k,x_k]\}$ of nonoverlapping intervals such that $$\sum |x_k-y_k|\lt \delta~~~~~\text{and}~~~~\sum\left(f(x_k)-f(y_k)\right)\gt f(b)-f(a)-\epsilon.$$ I would like to show that $f$ is singular. Attempt: From a previous exercise, I showed that a monotone function $f$ can be written as the sum of an absolutely continuous function, $g$ and a singular function, $h$ .  Thus $$ f = g + h ,~~~\text{where}~~g=\int_a^x f' .$$ My goal is to show that $g=0$ almost everywhere. Let $I=\bigcup (y_k,x_k).$  Then  $$ \int_I f' = \sum \int_{(x_k,y_k)}~f' = \sum\left(f(x_k)-f(y_k)\right)\lt \epsilon.$$ Now, I know that since $f'$ is integrable, there is an $\epsilon$ such that $$\int_{[a,b]\setminus I}~f'\lt \epsilon. $$ But $$\begin{align*} 0\leq \int_a^b f' & = \int_I f'+\int_{[a,b]\setminus I}~f'\\  & \lt \epsilon + \epsilon\\  & = 2\epsilon.  \end{align*}$$ Since $\epsilon$ is arbitrary, we can let $\epsilon \rightarrow 0$ and thus $$ \int_a^b f'=0 ~~\text{and }~~g = 0.$$ Is what I've done right? Is there another way of approaching the problem? Thanks.",,['analysis']
14,"Strange condition on a problem in Kaczor, Nowak","Strange condition on a problem in Kaczor, Nowak",,"Following is problem 1.1.9(b) in Problems in Mathematical Analysis II. Show that if in a deleted neighborhood of zero the inequalities $f(x)\ge|x|^{\alpha}$, $\frac12<\alpha<1$, and $f(x)f(2x)\le|x|$ hold, then $\lim_{x\to0}f(x)=0$. The given solution used $|x|^{\alpha}\le f(x)\le\frac{|x|}{|2x|^{\alpha}}$ and concludes that $\lim_{x\to0}f(x)=0$ (probably by the Squeeze Theorem). My question is, why do we need $\frac12<\alpha<1$? Is $0<\alpha<1$ not enough?","Following is problem 1.1.9(b) in Problems in Mathematical Analysis II. Show that if in a deleted neighborhood of zero the inequalities $f(x)\ge|x|^{\alpha}$, $\frac12<\alpha<1$, and $f(x)f(2x)\le|x|$ hold, then $\lim_{x\to0}f(x)=0$. The given solution used $|x|^{\alpha}\le f(x)\le\frac{|x|}{|2x|^{\alpha}}$ and concludes that $\lim_{x\to0}f(x)=0$ (probably by the Squeeze Theorem). My question is, why do we need $\frac12<\alpha<1$? Is $0<\alpha<1$ not enough?",,['analysis']
15,Haar Measure and Automorphic Forms,Haar Measure and Automorphic Forms,,"Let $G = SL_{2}(\mathbb{R})$ and $\Gamma = \Gamma_{0}(N)$. Every element $g =\begin{pmatrix}a & b\\ c& d\end{pmatrix}\in G$ can be written as $$\begin{pmatrix} y^{1/2} & xy^{-1/2} \\ 0 & y^{-1/2}\end{pmatrix}\begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$$ for some $x, y, \theta$. Therefore we can associate each $g \in G$ with $(x, y, \theta)$ with $x \in \mathbb{R}$, $y > 0$, and $\theta \in [0, 2\pi]$. With $g$ as defined above, $z = x + iy = g(i)$ and $\theta = \arg(ci + d)$. For each $f \in S_{k}(\Gamma)$, define $\phi_{f}(g)$ on $G$ by $\phi_{f}(g) = f(g(i))j(g, i)^{-k}$ where $j(g, i) = (ci + d)(\det g)^{-1/2}$. We consider the Haar measure on $\Gamma$ and $\Gamma\backslash G$. My question is: Why can we normalize the Haar measure on $G$ through the formula $$\int_{G}\phi_{f}(g)\, dg = \frac{1}{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty}\int_{-\infty}^{\infty} \phi_{f}(x, y, \theta)\, \frac{dxdy}{y^{2}}\, d\theta,$$ what is the reasoning behind this formula? Also why does this imply that $$\int_{\Gamma\backslash G} |\phi_{f}(g)|^{2}\, dg = \iint_{F} |f(z)|^{2} y^{k}\frac{dxdy}{y^{2}}$$ where $F$ is the fundamental domain for $\Gamma$ in the upper half plane.","Let $G = SL_{2}(\mathbb{R})$ and $\Gamma = \Gamma_{0}(N)$. Every element $g =\begin{pmatrix}a & b\\ c& d\end{pmatrix}\in G$ can be written as $$\begin{pmatrix} y^{1/2} & xy^{-1/2} \\ 0 & y^{-1/2}\end{pmatrix}\begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$$ for some $x, y, \theta$. Therefore we can associate each $g \in G$ with $(x, y, \theta)$ with $x \in \mathbb{R}$, $y > 0$, and $\theta \in [0, 2\pi]$. With $g$ as defined above, $z = x + iy = g(i)$ and $\theta = \arg(ci + d)$. For each $f \in S_{k}(\Gamma)$, define $\phi_{f}(g)$ on $G$ by $\phi_{f}(g) = f(g(i))j(g, i)^{-k}$ where $j(g, i) = (ci + d)(\det g)^{-1/2}$. We consider the Haar measure on $\Gamma$ and $\Gamma\backslash G$. My question is: Why can we normalize the Haar measure on $G$ through the formula $$\int_{G}\phi_{f}(g)\, dg = \frac{1}{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty}\int_{-\infty}^{\infty} \phi_{f}(x, y, \theta)\, \frac{dxdy}{y^{2}}\, d\theta,$$ what is the reasoning behind this formula? Also why does this imply that $$\int_{\Gamma\backslash G} |\phi_{f}(g)|^{2}\, dg = \iint_{F} |f(z)|^{2} y^{k}\frac{dxdy}{y^{2}}$$ where $F$ is the fundamental domain for $\Gamma$ in the upper half plane.",,"['analysis', 'measure-theory', 'automorphic-forms']"
16,Bound on the Convolution of a Smooth Function with $1/|x|$,Bound on the Convolution of a Smooth Function with,1/|x|,"Let $f:\mathbb{R}^{3} \rightarrow \mathbb{R}$ be compactly supported and $C^{\infty}$.  Let $g:\mathbb{R}^{3} \rightarrow \mathbb{R}$ be defined by $g(x) = \frac{1}{|x|}$ (with $g(0)$ defined arbitarily).  Define $h(x)$ by  $$ h(x) = (f \ast g)(x) = \int_{\mathbb{R}^{3}} f(y)g(x-y) dy = \int_{\mathbb{R}^{3}} f(x-y)g(y) dy $$ I want to prove that if $\int_{\mathbb{R}^{3}} f = 0$ there is a constant $C > 0$ such that $h(x) \leq C/|x|^2$ for all large $|x|$. I have been able to prove that $h(x) \leq C/|x|$ for all large $|x|$, but I want to do better.  My proof doesn't use that $\int_{\mathbb{R}^{3}} f = 0$ or the full smoothness of $f$.  It runs as follows.  Since $f$ is compactly supported there is an $R > 0$ such that $f(y)=0$ for all $|y| \geq R$.  So  $ h(x) = \int_{|y|<R} f(y)g(x-y) dy. $ Since $g(x-y)=1/|x-y| \geq 0$ and since $f(y) \leq f^{+}(y) := \max\{f(y),0\}$ everywhere, we have $$ h(x) \leq \int_{|y|<R} \frac{f^{+}(y)}{|x-y|} dy \leq \sup_{|y|<R} \frac{1}{|x-y|} \int_{|y|<R} f^{+}(y) dy \leq \frac{1}{|x|} \sup_{|y|<R} \frac{|x|}{|x|-|y|}  \int_{|y|<R} f^{+}(y) dy $$ Since  $$ \frac{|x|}{|x|-|y|} \leq c \quad \text{ if and only if } \quad |x| \geq \frac{c}{c-1}|y|, $$ if $|x| \geq \frac{c}{c-1}R$ and $R > |y|$, we have $|x|/(|x|-|y|) \leq c$.  Hence $$ h(x) \leq \frac{c}{|x|} \int_{|y|<R} f^{+}(y) dy = \frac{C}{|x|} $$ for $|x| \geq \frac{c}{c-1}R$. I haven't been able to figure out a way to adjust my method to prove the result I want.  Can someone please help me out?","Let $f:\mathbb{R}^{3} \rightarrow \mathbb{R}$ be compactly supported and $C^{\infty}$.  Let $g:\mathbb{R}^{3} \rightarrow \mathbb{R}$ be defined by $g(x) = \frac{1}{|x|}$ (with $g(0)$ defined arbitarily).  Define $h(x)$ by  $$ h(x) = (f \ast g)(x) = \int_{\mathbb{R}^{3}} f(y)g(x-y) dy = \int_{\mathbb{R}^{3}} f(x-y)g(y) dy $$ I want to prove that if $\int_{\mathbb{R}^{3}} f = 0$ there is a constant $C > 0$ such that $h(x) \leq C/|x|^2$ for all large $|x|$. I have been able to prove that $h(x) \leq C/|x|$ for all large $|x|$, but I want to do better.  My proof doesn't use that $\int_{\mathbb{R}^{3}} f = 0$ or the full smoothness of $f$.  It runs as follows.  Since $f$ is compactly supported there is an $R > 0$ such that $f(y)=0$ for all $|y| \geq R$.  So  $ h(x) = \int_{|y|<R} f(y)g(x-y) dy. $ Since $g(x-y)=1/|x-y| \geq 0$ and since $f(y) \leq f^{+}(y) := \max\{f(y),0\}$ everywhere, we have $$ h(x) \leq \int_{|y|<R} \frac{f^{+}(y)}{|x-y|} dy \leq \sup_{|y|<R} \frac{1}{|x-y|} \int_{|y|<R} f^{+}(y) dy \leq \frac{1}{|x|} \sup_{|y|<R} \frac{|x|}{|x|-|y|}  \int_{|y|<R} f^{+}(y) dy $$ Since  $$ \frac{|x|}{|x|-|y|} \leq c \quad \text{ if and only if } \quad |x| \geq \frac{c}{c-1}|y|, $$ if $|x| \geq \frac{c}{c-1}R$ and $R > |y|$, we have $|x|/(|x|-|y|) \leq c$.  Hence $$ h(x) \leq \frac{c}{|x|} \int_{|y|<R} f^{+}(y) dy = \frac{C}{|x|} $$ for $|x| \geq \frac{c}{c-1}R$. I haven't been able to figure out a way to adjust my method to prove the result I want.  Can someone please help me out?",,"['real-analysis', 'analysis']"
17,Nature of zeros of $4\cdot2^{2p}\cdot3^p-4\cdot3^{2p}-7\cdot2^{3p}+8\cdot2^{2p}+8\cdot3^p-4=0$,Nature of zeros of,4\cdot2^{2p}\cdot3^p-4\cdot3^{2p}-7\cdot2^{3p}+8\cdot2^{2p}+8\cdot3^p-4=0,"Recently while coming up with an example for a paper I'm writing I find myself wanting something to say about how 'awful' the first positive root of the equation $$ 4\cdot2^{2p}\cdot3^p-4\cdot3^{2p}-7\cdot2^{3p}+8\cdot2^{2p}+8\cdot3^p-4=0 $$ is. Numerically I know it's about 1.576, but I suspect that it's no only irrational but cannot be solved for using elementary functions, e.g. $\log_2(3+\sqrt{5})$. I've not much / any background in number theory of things like this and am kind of stumped. Are there any simple arguments in this direction? Note: This is the simplest of the awful equations I could come up with. I'm also aware that it has a root at $p=2$","Recently while coming up with an example for a paper I'm writing I find myself wanting something to say about how 'awful' the first positive root of the equation $$ 4\cdot2^{2p}\cdot3^p-4\cdot3^{2p}-7\cdot2^{3p}+8\cdot2^{2p}+8\cdot3^p-4=0 $$ is. Numerically I know it's about 1.576, but I suspect that it's no only irrational but cannot be solved for using elementary functions, e.g. $\log_2(3+\sqrt{5})$. I've not much / any background in number theory of things like this and am kind of stumped. Are there any simple arguments in this direction? Note: This is the simplest of the awful equations I could come up with. I'm also aware that it has a root at $p=2$",,"['number-theory', 'analysis']"
18,Sharp upper bounds for sums of the form $\sum_{p \mid k} \frac{1}{p+1}$,Sharp upper bounds for sums of the form,\sum_{p \mid k} \frac{1}{p+1},"Are there known sharp upper bounds for sums of the form $\sum_{p \mid k} \frac{1}{p+1}$ for $k > 1$ subject to the constraint $\sum_{p \mid k} \frac{1}{p+1} < 1$? (The factor of +1 in the denominator and the constraint of the sum of inverse ""shifted prime divisors"" is bounded by 1 are both necessary.) A related question: Suppose $(p_i)$ is a set of $n$ consecutive primes which minimizes $1 - \sum_{i = 1}^{n} \frac{1}{p_i+1} > 0$ for a given $n > 1$. Are there known bounds for $1 - \sum_{i = 1}^{n} \frac{1}{p_i + 1}$ from below in terms of $n$, e.g., $n^{-\delta n}$ for some fixed $\delta > 0$? Thanks!","Are there known sharp upper bounds for sums of the form $\sum_{p \mid k} \frac{1}{p+1}$ for $k > 1$ subject to the constraint $\sum_{p \mid k} \frac{1}{p+1} < 1$? (The factor of +1 in the denominator and the constraint of the sum of inverse ""shifted prime divisors"" is bounded by 1 are both necessary.) A related question: Suppose $(p_i)$ is a set of $n$ consecutive primes which minimizes $1 - \sum_{i = 1}^{n} \frac{1}{p_i+1} > 0$ for a given $n > 1$. Are there known bounds for $1 - \sum_{i = 1}^{n} \frac{1}{p_i + 1}$ from below in terms of $n$, e.g., $n^{-\delta n}$ for some fixed $\delta > 0$? Thanks!",,"['number-theory', 'real-analysis', 'analysis', 'asymptotics']"
19,Continuity on the torus,Continuity on the torus,,"Let $\mathbb{T}^d$ be the $d$ -dimensional torus, for all $r >0,\eta_r(x):=e^{-r|x|^2},x \in \mathbb{R}^d,$ we denote by $\mathscr{F}^{-1}\eta_r$ the inverse Fourier transform defined by $\mathscr{F}^{-1}\eta_r(x):=\sum_{k \in \mathbb{Z}^d}\eta_r(k)e^{2\pi\mathrm{i}\langle x,k\rangle},x \in \mathbb{T}^d.$ Let $v_0 \in C^{\infty}(\mathbb{T}^d),$ Is it true that $$\forall x \in \mathbb{T}^d,\lim_{r \to 0}\mathscr{F}^{-1}\eta_r*v_0(x)=v_0(x)?$$ I know how to prove this when we are dealing with $\mathbb{R}^d:$ $$\forall x \in \mathbb{R}^d,\int_{\mathbb{R}^d}v_0(x-y)\mathscr{F}^{-1}\eta_r(y)dy=\int_{\mathbb{R}^d}\mathscr{F}^{-1}\eta_1(y)v_0(x-ry)dy$$ where we used a simple linear change of variable, we conclude using the dominated convergence theorem, that the limit is $v_0(x).$ How to treat the problem when we are on the torus? Unfortunately, the above change of variable won't work.","Let be the -dimensional torus, for all we denote by the inverse Fourier transform defined by Let Is it true that I know how to prove this when we are dealing with where we used a simple linear change of variable, we conclude using the dominated convergence theorem, that the limit is How to treat the problem when we are on the torus? Unfortunately, the above change of variable won't work.","\mathbb{T}^d d r >0,\eta_r(x):=e^{-r|x|^2},x \in \mathbb{R}^d, \mathscr{F}^{-1}\eta_r \mathscr{F}^{-1}\eta_r(x):=\sum_{k \in \mathbb{Z}^d}\eta_r(k)e^{2\pi\mathrm{i}\langle x,k\rangle},x \in \mathbb{T}^d. v_0 \in C^{\infty}(\mathbb{T}^d), \forall x \in \mathbb{T}^d,\lim_{r \to 0}\mathscr{F}^{-1}\eta_r*v_0(x)=v_0(x)? \mathbb{R}^d: \forall x \in \mathbb{R}^d,\int_{\mathbb{R}^d}v_0(x-y)\mathscr{F}^{-1}\eta_r(y)dy=\int_{\mathbb{R}^d}\mathscr{F}^{-1}\eta_1(y)v_0(x-ry)dy v_0(x).","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
20,Question About Half-Open Cubes on $\mathbb{R}^d$,Question About Half-Open Cubes on,\mathbb{R}^d,"I am self-studying real analysis. I encountered the following assertion without a proof: For each positive integer $k$ , let $\mathcal{C}_k$ be the collection of all cubes of the form \begin{align*} \{(x_1,\dots,x_d):j_i2^{-k} \leq x_i < (j_i+1)2^{-k}\ \text{for}\ i = 1,\dots,d\}, \end{align*} where $j_1,\dots,j_d$ are arbitrary integers. Then (1) each $\mathcal{C}_k$ is a countable partition of $\mathbb{R}^d$ , and (2) if $k_1<k_2$ , then each cube in $\mathcal{C}_{k_2}$ is included in some cube in $\mathcal{C}_{k_1}$ . I tried a couple of examples, for instance, I tried $k=1$ and $j_i=i$ , $k=1$ and $j_1=0$ , $j_2=-5$ , $j_3=10$ , and so on. It does show that the claim is correct. However, I am really having a hard time proving it rigorously. I really appreciate it if someone could help me out! I do not have the definition of a partition of $\mathbb{R}^d$ . But I think it means a collection of nonempty disjoint sets whose union is $\mathbb{R}^d$ . Reference: Lemma 1.4.2 from Measure Theory by Donald Cohn","I am self-studying real analysis. I encountered the following assertion without a proof: For each positive integer , let be the collection of all cubes of the form where are arbitrary integers. Then (1) each is a countable partition of , and (2) if , then each cube in is included in some cube in . I tried a couple of examples, for instance, I tried and , and , , , and so on. It does show that the claim is correct. However, I am really having a hard time proving it rigorously. I really appreciate it if someone could help me out! I do not have the definition of a partition of . But I think it means a collection of nonempty disjoint sets whose union is . Reference: Lemma 1.4.2 from Measure Theory by Donald Cohn","k \mathcal{C}_k \begin{align*}
\{(x_1,\dots,x_d):j_i2^{-k} \leq x_i < (j_i+1)2^{-k}\ \text{for}\ i = 1,\dots,d\},
\end{align*} j_1,\dots,j_d \mathcal{C}_k \mathbb{R}^d k_1<k_2 \mathcal{C}_{k_2} \mathcal{C}_{k_1} k=1 j_i=i k=1 j_1=0 j_2=-5 j_3=10 \mathbb{R}^d \mathbb{R}^d","['real-analysis', 'analysis', 'measure-theory', 'elementary-set-theory', 'problem-solving']"
21,Can Darboux Theorem be stronger by making the derivative at c continuous?,Can Darboux Theorem be stronger by making the derivative at c continuous?,,"Darboux Theorem: If $f$ is differentiable on $[a,b]$ and $\eta$ between $f'(a)$ and $f'(b)$ , then exists $c\in[a,b]$ such that $f'(c)=\eta$ . The common counterexample about $f'$ at $c$ not necessarily continuous is $f(x)=x^2\sin\frac{1}{x}(\mathbb{R}-\{0\}); 0(0)$ , $\eta=0$ and $f'(0)=0$ where $f'$ is not continuous. Yet we have a lot of zeroes around the point $0$ and all of $f'$ there are continuous. It's easy to prove there are these discontinuities of $f'$ are convergence points (Edit: This proof is wrong.), but the points around could also be discontinuities. I want to state that we can find this $c$ such that $f'$ is continuous at $c$ , which is a stronger proposition. Is this statement true? If so, how to prove it, and can we extend it to ""continuous on its neighborhood""? If not, do we have a counterexample?","Darboux Theorem: If is differentiable on and between and , then exists such that . The common counterexample about at not necessarily continuous is , and where is not continuous. Yet we have a lot of zeroes around the point and all of there are continuous. It's easy to prove there are these discontinuities of are convergence points (Edit: This proof is wrong.), but the points around could also be discontinuities. I want to state that we can find this such that is continuous at , which is a stronger proposition. Is this statement true? If so, how to prove it, and can we extend it to ""continuous on its neighborhood""? If not, do we have a counterexample?","f [a,b] \eta f'(a) f'(b) c\in[a,b] f'(c)=\eta f' c f(x)=x^2\sin\frac{1}{x}(\mathbb{R}-\{0\}); 0(0) \eta=0 f'(0)=0 f' 0 f' f' c f' c","['real-analysis', 'analysis', 'derivatives', 'continuity']"
22,Integral related to $\psi(x)$,Integral related to,\psi(x),"I'm trying to solve this integral: $$I(x)=\int_{0}^{\infty}\frac{\ln(t)\sin(2\pi x)\sinh(t)}{\left(\cosh(t)-\cos(2\pi x)\right)^2}dt,$$ where $0<x<1$ . But I can't find intelligent ways to approach it (with elementary substitutions it gets much more complicated). The only thing I know is the solution to a very similar integral: $$\int_{0}^{\infty}\frac{\ln(t)\left(1-\cos(2\pi x)\cosh(t)\right)}{\left(\cosh(t)-\cos(2\pi x)\right)^2}dt=\ln(2\pi)+\frac{\psi(x)+\psi(1-x)}{2},$$ where $\psi(x)$ is the digamma function. Graphing the first integral $I(x)$ saw that $I(1/2)=0$ , so I imagine that the solution is something like $f(x)-f(1-x)$ . Since $2\cosh(x)=e^x+e^{-x}$ and $2 \sinh(x)=e^x-e^{-x}$ structurally I think it might make sense, but I'm not sure. Motivation of the question I was trying to find a formula similar to what is asked in this question :  I used the formula the OP posted on his IG page","I'm trying to solve this integral: where . But I can't find intelligent ways to approach it (with elementary substitutions it gets much more complicated). The only thing I know is the solution to a very similar integral: where is the digamma function. Graphing the first integral saw that , so I imagine that the solution is something like . Since and structurally I think it might make sense, but I'm not sure. Motivation of the question I was trying to find a formula similar to what is asked in this question :  I used the formula the OP posted on his IG page","I(x)=\int_{0}^{\infty}\frac{\ln(t)\sin(2\pi x)\sinh(t)}{\left(\cosh(t)-\cos(2\pi x)\right)^2}dt, 0<x<1 \int_{0}^{\infty}\frac{\ln(t)\left(1-\cos(2\pi x)\cosh(t)\right)}{\left(\cosh(t)-\cos(2\pi x)\right)^2}dt=\ln(2\pi)+\frac{\psi(x)+\psi(1-x)}{2}, \psi(x) I(x) I(1/2)=0 f(x)-f(1-x) 2\cosh(x)=e^x+e^{-x} 2 \sinh(x)=e^x-e^{-x}","['calculus', 'integration', 'analysis']"
23,"Prove $f(x)=\begin{cases} x, & x\in[0,1]\setminus\mathbb Q\\ p\sin(1/q), & x=p/q\in[0,1]\cap\mathbb Q\end{cases}$ is continuous at irrational points.",Prove  is continuous at irrational points.,"f(x)=\begin{cases} x, & x\in[0,1]\setminus\mathbb Q\\ p\sin(1/q), & x=p/q\in[0,1]\cap\mathbb Q\end{cases}","The function is defined on $[0,1]$ by $f(x) = x$ when $x \in \mathbb{R} \setminus \mathbb{Q}$ and $f(x) = p\sin(1/q)$ when $x=p/q \in \mathbb{Q}$ , where $p < q, p \in \mathbb{Z}, q \in \mathbb{Z} \setminus \{ 0 \}.$ My observations so far: Let $x_{0} \in \mathbb{R}\setminus\mathbb{Q}.$ Then $|f(x) - f(x_{0})| = |x-x_{0}|$ for $x \in \mathbb{R} \setminus \mathbb{Q}.$ So we choose $\delta = \epsilon.$ Then $|x - x_{0}| < \delta \implies |f(x) - f(x_{0})| < \epsilon.$ Now I consider the case when $x = \frac{p}{q} \in \mathbb{Q}.$ In this case, we have that $|f(x) - f(x_{0})| = |p\sin(1/q) - x_{0}| \leq |p\cdot 1/q - x_{0}| < |1 - x_{0}|,$ since $p < q.$ Now I'm struggling a little to find a suitable $\delta > 0$ .. Any HINTS are appreciated! Thank you.","The function is defined on by when and when , where My observations so far: Let Then for So we choose Then Now I consider the case when In this case, we have that since Now I'm struggling a little to find a suitable .. Any HINTS are appreciated! Thank you.","[0,1] f(x) = x x \in \mathbb{R} \setminus \mathbb{Q} f(x) = p\sin(1/q) x=p/q \in \mathbb{Q} p < q, p \in \mathbb{Z}, q \in \mathbb{Z} \setminus \{ 0 \}. x_{0} \in \mathbb{R}\setminus\mathbb{Q}. |f(x) - f(x_{0})| = |x-x_{0}| x \in \mathbb{R} \setminus \mathbb{Q}. \delta = \epsilon. |x - x_{0}| < \delta \implies |f(x) - f(x_{0})| < \epsilon. x = \frac{p}{q} \in \mathbb{Q}. |f(x) - f(x_{0})| = |p\sin(1/q) - x_{0}| \leq |p\cdot 1/q - x_{0}| < |1 - x_{0}|, p < q. \delta > 0","['real-analysis', 'calculus']"
24,How could we define circulation over a surface or a tangential surface integral?,How could we define circulation over a surface or a tangential surface integral?,,"The definition of surface integral of a vector function is about the projection of a function on the normal vector, so it is impossible for tangential vector fields to be intergrated over a surface, for the integral would be always zero. I would like to extend this thought as follows: I found this definition of $curl$ of a vector field: $$(\nabla \times \mathbf F)(p):=\lim_{V\to 0} \frac {1}{|V|} \oint_S \mathbf n \times \mathbf F dS, $$ where $p$ is a point inside a volume $V$ with boundary the surface $S$ . The vector $\mathbf n \times \mathbf F$ is in fact tangential to the surface, and it sounds nice to integrate over the surface to find the given total rate. But I cannot understand the exact meaning of this ""integral"". I would like to ask: How could we present the circulation over $a$ $surface$ -and not over a curve?","The definition of surface integral of a vector function is about the projection of a function on the normal vector, so it is impossible for tangential vector fields to be intergrated over a surface, for the integral would be always zero. I would like to extend this thought as follows: I found this definition of of a vector field: where is a point inside a volume with boundary the surface . The vector is in fact tangential to the surface, and it sounds nice to integrate over the surface to find the given total rate. But I cannot understand the exact meaning of this ""integral"". I would like to ask: How could we present the circulation over -and not over a curve?","curl (\nabla \times \mathbf F)(p):=\lim_{V\to 0} \frac {1}{|V|} \oint_S \mathbf n \times \mathbf F dS,  p V S \mathbf n \times \mathbf F a surface","['real-analysis', 'integration', 'analysis']"
25,"Show $f(x,y)=\left(\frac{\cos x+\cos y-1}{2}, \cos x-\cos y\right)$ has a fixed point using the mean value inequality.",Show  has a fixed point using the mean value inequality.,"f(x,y)=\left(\frac{\cos x+\cos y-1}{2}, \cos x-\cos y\right)","The problem: I am attempting to show $$f(x,y)=\left(\frac{\cos x+\cos y-1}{2}, \cos x-\cos y\right)$$ has a fixed point using the mean value inequality, which states that for any $a \in \mathbb{R}^n$ and $f: B_r(a) \to \mathbb{R}^m$ differentiable on $B_r(a)$ with $\|Df(x)\| \leq M$ for all $x \in B_r(a)$ , we have $|f(b_1) - f(b_2)\| \leq M\|b_1 - b_2\|$ for any $b_1, b_2 \in B_r(a)$ . If I can find such an $M$ on some ball, and show $M < 1$ , then I am done by the contraction mapping theorem. Here's what I've done: Note that $|\frac{\cos x+\cos y-1}{2}| \leq \frac{3}{2}$ and $|\cos x-\cos y| \leq 2$ by applying the triangle inequality, so the fixed point should be on $B_{r}(a)$ with $r=2$ & $a=(0,0)$ . We can calculate that $$Df(x,y)=\pmatrix{ -\frac{\sin x}{2} & -\sin x \\\ -\frac{\sin y}{2} & \sin y }$$ Now, I'm a bit stuck on the interpretation of the mean value inequality, specifically how to find and get $\|Df(x)\|$ . If I'm interpreting things correctly, I believe this should be using the operator norm, which I'm not confident in using. If so, I know that for $A \in L(\mathbb{R}^n, \mathbb{R}^m)$ $$|A|=\sup_{0 \neq x' \in \mathbb{R}^n} \frac{||Ax'||}{||x'||}$$ I've tried letting $x' = (u,v) \in B_{2}(0)$ be a vector on the ball and calculate $$\sup_{0 \neq x' \in B_{2}(0)} \frac{\sqrt{((\sin^2 x + \sin^2 y) (\frac{1}{2}u+v)^2)}}{\sqrt{u^2+v^2}}$$ (the numerator is $|Df(x,y)x'|$ ) as an upper bound for $M$ . But I can't seem to get this bound lower than $1$ in order to use the contraction mapping theorem, I'm getting things above $1$ . I'm not sure where the problem is, perhaps I'm misunderstanding the theory. Is this calculation wrong? I'm thinking, perhaps, $x'$ should be $(x,y)$ , i.e. the same variables used in $f$ , but I'm not sure if this is correct.","The problem: I am attempting to show has a fixed point using the mean value inequality, which states that for any and differentiable on with for all , we have for any . If I can find such an on some ball, and show , then I am done by the contraction mapping theorem. Here's what I've done: Note that and by applying the triangle inequality, so the fixed point should be on with & . We can calculate that Now, I'm a bit stuck on the interpretation of the mean value inequality, specifically how to find and get . If I'm interpreting things correctly, I believe this should be using the operator norm, which I'm not confident in using. If so, I know that for I've tried letting be a vector on the ball and calculate (the numerator is ) as an upper bound for . But I can't seem to get this bound lower than in order to use the contraction mapping theorem, I'm getting things above . I'm not sure where the problem is, perhaps I'm misunderstanding the theory. Is this calculation wrong? I'm thinking, perhaps, should be , i.e. the same variables used in , but I'm not sure if this is correct.","f(x,y)=\left(\frac{\cos x+\cos y-1}{2}, \cos x-\cos y\right) a \in \mathbb{R}^n f: B_r(a) \to \mathbb{R}^m B_r(a) \|Df(x)\| \leq M x \in B_r(a) |f(b_1) - f(b_2)\| \leq M\|b_1 - b_2\| b_1, b_2 \in B_r(a) M M < 1 |\frac{\cos x+\cos y-1}{2}| \leq \frac{3}{2} |\cos x-\cos y| \leq 2 B_{r}(a) r=2 a=(0,0) Df(x,y)=\pmatrix{
-\frac{\sin x}{2} & -\sin x \\\
-\frac{\sin y}{2} & \sin y
} \|Df(x)\| A \in L(\mathbb{R}^n, \mathbb{R}^m) |A|=\sup_{0 \neq x' \in \mathbb{R}^n} \frac{||Ax'||}{||x'||} x' = (u,v) \in B_{2}(0) \sup_{0 \neq x' \in B_{2}(0)} \frac{\sqrt{((\sin^2 x + \sin^2 y) (\frac{1}{2}u+v)^2)}}{\sqrt{u^2+v^2}} |Df(x,y)x'| M 1 1 x' (x,y) f","['real-analysis', 'analysis', 'trigonometry', 'fixed-point-theorems']"
26,Question in the proof of Lusin's theorem.,Question in the proof of Lusin's theorem.,,"I want to prove Lusin's theorem. Let $E\subset\mathbb R$ be a measurable set and $f:E\to\mathbb R$ be a measurable function. Then, for each $\epsilon$ , there exists $F\subset E$ s.t. $F$ is closed about relative topology from $E$ , $m^\ast(E\setminus F)<\epsilon$ , $f|_F$ is continuous. ( $m^\ast$ is Lebesgue outer measure.) I've already understand the case $m^\ast(E)<\infty$ , and I'm trying general case. Hint is given : Consider $E_k:=E\cap[-k,k]$ . Let $E_k:=E\cap[-k,k]$ . For each $k\in\mathbb N$ , $m^\ast(E_k)<\infty$ and $f|_{E_k}$ is measurable thus there is $F_k\subset E_k$ s.t. $F_k$ is closed about relative topology from $E_k$ , $m^\ast(E_k\setminus F_k)<\epsilon/2^k$ , $f|_{F_k}$ is continuous. $F:=\cup_{k=1}^\infty F_k$ , $E:=\cup_{k=1}^\infty E_k$ . Then $F\subset E$ . Since $E\setminus F\subset\cup_{k=1}^\infty (E_k\setminus F_k)$ , I get $m^\ast(E\setminus F)\leqq\sum_k m^\ast(E_k\setminus F_k)\leqq\epsilon.$ Maybe what I did so far is correct, but this is not sufficient yet : I have to check (i) $F$ is closed about relative topology from $E$ . (ii) $f|_F$ is continuous. For (i), each $F_k$ is closed in $E_k$ but I don't know why $F=\cup_k F_k$ is closed in $E$ . $F$ has to be written as $F=E\cap C$ , where $C$ in closed in $\mathbb R$ . I haven't found such $C$ . For (ii), each $f|_{F_k}$ is continuous but I don't think this guarantees the continuity of $f|_F$ . I think that ""each $f|_{A_k}$ is continuous $\Rightarrow$ $f|_{\cup_{k=1}^\infty}A_k$ is continuous"" doesn't generally hold. (Maybe, I have to show $x_j\to x$ $\Rightarrow$ $f|_F(x_j)\to f|_F(x)$ , but I don't know how.) Postscript Hint says $E_k:=E\cap[-k,k]$ but perhaps $E_k$ has to be defined s.t. $\{E_k\}$ is disjoint : e.g., $E_k=E\cap(k,k+1]$ or, $E_1=E\cap[-1,1]$ , $E_k=E\cap([-k,k]\setminus[1-k,k-1])$ for $k\geqq 2.$","I want to prove Lusin's theorem. Let be a measurable set and be a measurable function. Then, for each , there exists s.t. is closed about relative topology from , , is continuous. ( is Lebesgue outer measure.) I've already understand the case , and I'm trying general case. Hint is given : Consider . Let . For each , and is measurable thus there is s.t. is closed about relative topology from , , is continuous. , . Then . Since , I get Maybe what I did so far is correct, but this is not sufficient yet : I have to check (i) is closed about relative topology from . (ii) is continuous. For (i), each is closed in but I don't know why is closed in . has to be written as , where in closed in . I haven't found such . For (ii), each is continuous but I don't think this guarantees the continuity of . I think that ""each is continuous is continuous"" doesn't generally hold. (Maybe, I have to show , but I don't know how.) Postscript Hint says but perhaps has to be defined s.t. is disjoint : e.g., or, , for","E\subset\mathbb R f:E\to\mathbb R \epsilon F\subset E F E m^\ast(E\setminus F)<\epsilon f|_F m^\ast m^\ast(E)<\infty E_k:=E\cap[-k,k] E_k:=E\cap[-k,k] k\in\mathbb N m^\ast(E_k)<\infty f|_{E_k} F_k\subset E_k F_k E_k m^\ast(E_k\setminus F_k)<\epsilon/2^k f|_{F_k} F:=\cup_{k=1}^\infty F_k E:=\cup_{k=1}^\infty E_k F\subset E E\setminus F\subset\cup_{k=1}^\infty (E_k\setminus F_k) m^\ast(E\setminus F)\leqq\sum_k m^\ast(E_k\setminus F_k)\leqq\epsilon. F E f|_F F_k E_k F=\cup_k F_k E F F=E\cap C C \mathbb R C f|_{F_k} f|_F f|_{A_k} \Rightarrow f|_{\cup_{k=1}^\infty}A_k x_j\to x \Rightarrow f|_F(x_j)\to f|_F(x) E_k:=E\cap[-k,k] E_k \{E_k\} E_k=E\cap(k,k+1] E_1=E\cap[-1,1] E_k=E\cap([-k,k]\setminus[1-k,k-1]) k\geqq 2.","['analysis', 'lebesgue-integral']"
27,Find all monotonic functions $f : \mathbb{R}\to\mathbb{R}$ that satisfy $f''(x) + xf'(x) - (\delta+\gamma f(x))(f'(x))^2>0$,Find all monotonic functions  that satisfy,f : \mathbb{R}\to\mathbb{R} f''(x) + xf'(x) - (\delta+\gamma f(x))(f'(x))^2>0,"I would like to find a set of functions that satisfy the following: Find all monotonic functions $f : \mathbb{R}\to\mathbb{R}$ that satisfy $f''(x) + xf'(x) - (\delta+\gamma f(x))(f'(x))^2>0$ where $\gamma$ and $\delta$ are finite, non-zero constants. (A previous version of the question had $\gamma = \delta \equiv 1$ , but this need not necessarily be the case). The equation in the inequality is the derivative of a generalised log-odds function, the condition implying the final logistic curve is strictly increasing. Ideally, bounding the inequality by taking limits as $x\to\pm\infty$ should lead to an expression dependent solely on the parameters $\gamma$ and $\delta$ ${}^{1}$ . Properties that would naturally seem to be needed are $f'(x)\to 0$ and $f''(x)\to 0$ as $x \pm\infty$ . Two good candidates I have found that would satisfy this are: $f(x) = \text{arcsinh}(x)$ , and $f(x) = \tanh{(x)}$ , yet I'm not sure how to exploit their limiting behaviour to reduce the inequality to a simple form for $\gamma$ and $\delta$ (i.e., no explicit dependence on $x$ ). How can I do this in a rigorous manner? Are there any other suitable monotonic functions $f : \mathbb{R} \to \mathbb{R}$ ? ${}^{1}$ This is for optimisation purposes guaranteeing the monotonicity of the logistic curve for some functional form of $f(x)$ . In reality $x$ will never reach $\pm\infty$ ; it will exist in the interval $\left(\Phi^{-1}(\varepsilon),\Phi^{-1}(1-\varepsilon)\right)$ where $\mathcal{O}(\varepsilon) \sim 10^{-6}$ . Therefore, for practical purposes, if bounding the inequality via the asymptotic limiting behaviour of $f(x)$ is too troublesome, we can also bound it by assessing its value as $x\to \pm 5$ .","I would like to find a set of functions that satisfy the following: Find all monotonic functions that satisfy where and are finite, non-zero constants. (A previous version of the question had , but this need not necessarily be the case). The equation in the inequality is the derivative of a generalised log-odds function, the condition implying the final logistic curve is strictly increasing. Ideally, bounding the inequality by taking limits as should lead to an expression dependent solely on the parameters and . Properties that would naturally seem to be needed are and as . Two good candidates I have found that would satisfy this are: , and , yet I'm not sure how to exploit their limiting behaviour to reduce the inequality to a simple form for and (i.e., no explicit dependence on ). How can I do this in a rigorous manner? Are there any other suitable monotonic functions ? This is for optimisation purposes guaranteeing the monotonicity of the logistic curve for some functional form of . In reality will never reach ; it will exist in the interval where . Therefore, for practical purposes, if bounding the inequality via the asymptotic limiting behaviour of is too troublesome, we can also bound it by assessing its value as .","f : \mathbb{R}\to\mathbb{R} f''(x) + xf'(x) - (\delta+\gamma f(x))(f'(x))^2>0 \gamma \delta \gamma = \delta \equiv 1 x\to\pm\infty \gamma \delta {}^{1} f'(x)\to 0 f''(x)\to 0 x \pm\infty f(x) = \text{arcsinh}(x) f(x) = \tanh{(x)} \gamma \delta x f : \mathbb{R} \to \mathbb{R} {}^{1} f(x) x \pm\infty \left(\Phi^{-1}(\varepsilon),\Phi^{-1}(1-\varepsilon)\right) \mathcal{O}(\varepsilon) \sim 10^{-6} f(x) x\to \pm 5","['real-analysis', 'analysis', 'functions', 'convex-analysis', 'nonlinear-analysis']"
28,How to switch a sum and an integral with a summation variable in its upper bound?,How to switch a sum and an integral with a summation variable in its upper bound?,,"Recently, I looked at some problems in which the order of summation and integration were reversed to arrive at a solution. I believe the process could be summed up (excuse the pun) in this way: $$\left.\sum_{n=a}^{b} \int_c^d  f(x,n) \ dx= \int_c^d \sum_{i=a}^{b} f(x,n) \ dx \qquad \right| \ a,b \in \mathbb{N}, c,d \in \mathbb{R}\ \forall\ a<b, c<d $$ where $f(x, n)$ is a function with $x$ and $n$ inputs and satisfies the respective theorems needed to permit the exchange in order of discrete and continuous sums, to begin with (dominated convergence theorem, monotone convergence theorem, etc.). However, this led me to ponder on how the order of a sum and an integral with its upper bound as this sum's summation variable may be exchanged (now that I have typed it - it probably sounds a bit like nonsense) Perhaps my question should be as follows: $$\left.\sum_{n=a}^{b} \int_c^n f(x,n)\ dx = \int_?^? \sum_{n=?}^{?} f(x,n) \ dx \qquad \right| \ c \in \mathbb{R}, a,b \in \mathbb{N}, c ≤ a $$ (although c = a may introduce redundancy on the first term of the first expression) If it is possible to express the first expression in such a form, please could you tell me? It would be much appreciated. Thank you :)","Recently, I looked at some problems in which the order of summation and integration were reversed to arrive at a solution. I believe the process could be summed up (excuse the pun) in this way: where is a function with and inputs and satisfies the respective theorems needed to permit the exchange in order of discrete and continuous sums, to begin with (dominated convergence theorem, monotone convergence theorem, etc.). However, this led me to ponder on how the order of a sum and an integral with its upper bound as this sum's summation variable may be exchanged (now that I have typed it - it probably sounds a bit like nonsense) Perhaps my question should be as follows: (although c = a may introduce redundancy on the first term of the first expression) If it is possible to express the first expression in such a form, please could you tell me? It would be much appreciated. Thank you :)","\left.\sum_{n=a}^{b} \int_c^d  f(x,n) \ dx= \int_c^d \sum_{i=a}^{b} f(x,n) \ dx \qquad \right| \ a,b \in \mathbb{N}, c,d \in \mathbb{R}\ \forall\ a<b, c<d  f(x, n) x n \left.\sum_{n=a}^{b} \int_c^n f(x,n)\ dx = \int_?^? \sum_{n=?}^{?} f(x,n) \ dx \qquad \right| \ c \in \mathbb{R}, a,b \in \mathbb{N}, c ≤ a ","['calculus', 'integration', 'analysis']"
29,Question about the Inverse Fourier Transform of a particular function.,Question about the Inverse Fourier Transform of a particular function.,,"I'm trying to find the IFT of the following function: $\hat{f}(\omega) = \frac{i\omega}{1+\omega^2}$ . I know about the Laplace Transform, so right away I thought about looking for an inverse of $\hat{f}$ involving $\cosh$ and then do a change of variables. I managed to arrive at $f(x) = \sqrt{2\pi} \cosh(x) \theta(x)$ , where $\theta$ is the Heaviside Function. Wolfram Alpha agrees that the Fourier Transform of that $f$ is $\frac{i\omega}{1+\omega^2}$ . However, when I use Wolfram Alpha to calculate $\hat{f}^{-1}$ , I have in return $g(x) = -\sqrt{\frac{\pi}{2}}e^{-x}[e^{2x}\theta(-x) - \theta(x)]$ . Both functions, $f$ and $g$ , are obviously not the same. Did I do something wrong? Is it normal for a function to have more than one inverse? In that case, if I was asked on an exam to find the IFT of a function, would both $f$ and $g$ be correct?","I'm trying to find the IFT of the following function: . I know about the Laplace Transform, so right away I thought about looking for an inverse of involving and then do a change of variables. I managed to arrive at , where is the Heaviside Function. Wolfram Alpha agrees that the Fourier Transform of that is . However, when I use Wolfram Alpha to calculate , I have in return . Both functions, and , are obviously not the same. Did I do something wrong? Is it normal for a function to have more than one inverse? In that case, if I was asked on an exam to find the IFT of a function, would both and be correct?",\hat{f}(\omega) = \frac{i\omega}{1+\omega^2} \hat{f} \cosh f(x) = \sqrt{2\pi} \cosh(x) \theta(x) \theta f \frac{i\omega}{1+\omega^2} \hat{f}^{-1} g(x) = -\sqrt{\frac{\pi}{2}}e^{-x}[e^{2x}\theta(-x) - \theta(x)] f g f g,"['analysis', 'partial-differential-equations']"
30,"For arbitrary billiard tables with elastic boundary reflections, is the ""Lebesgue measure"" an invariant of the flow maps?","For arbitrary billiard tables with elastic boundary reflections, is the ""Lebesgue measure"" an invariant of the flow maps?",,"This question pertains to billiard dynamics and their invariant measures. Specifically, it concerns the oft-quoted 'fact' that billiard flow maps (built using specular reflection boundary conditions) admit the Liouville measure (also known as the restriction of the Lebesgue measure to the tangent bundle) as an invariant measure for all times. To be precise, if the billiard table $\mathcal{P}\subset\mathbb{R}^{N}$ is a 'reasonably-nice' subset of Euclidean space (for instance, it admits the structure of a manifold with corners, an example of which would be the unit square if $N=2$ ), I'd like to prove that billiard flow maps $T^{t}:T\mathcal{P}\rightarrow T\mathcal{P}$ admit the property that $$T^{t}\#\mathscr{L}_{2N}\mathsf{L}T\mathcal{P}=\mathscr{L}_{2N}\mathsf{L}T\mathcal{P}$$ for all $t\in\mathbb{R}$ , where: $T\mathcal{P}$ is the tangent bundle of the table $\mathcal{P}$ , considered as a subset of Euclidean space $\mathbb{R}^{2N}$ ; $T^{t}$ is the Lebesgue almost everywhere-defined billiard map built using 'elastic reflection boundary conditions' on $\partial\mathcal{P}$ , whose associated trajectories $t\mapsto T^{t}((x_{0}, v_{0}))$ are piecewise linear and continuous in the spatial variable, but lower semi-continuous in the velocity variable for any initial point $(x_{0}, v_{0})\in T\mathcal{P}$ ; $\mathscr{L}_{2N}$ denotes the Lebesgue measure on $\mathbb{R}^{2N}$ ; $\#$ denotes the pushforward operation, and $\mathsf{L}$ denotes the restriction measure operation, whence $\mathscr{L}_{2N}\mathsf{L}T\mathcal{P}$ is the restriction of the Lebesgue measure on $\mathbb{R}^{2N}$ to the set $T\mathcal{P}$ . One of the main issues in proving this statement is that the dynamics is not smooth, and one cannot appeal to the classical Liouville theorem of symplectic geometry to prove it. Does anyone know of a reference in which the above statement is proved, at least for some class of tables $\mathcal{P}$ ? I have spent quite some time going through the literature, but I have so far come up empty handed. I was trained as a mathematical analyst, so I'm looking for a proof that someone in that community would consider as complete.","This question pertains to billiard dynamics and their invariant measures. Specifically, it concerns the oft-quoted 'fact' that billiard flow maps (built using specular reflection boundary conditions) admit the Liouville measure (also known as the restriction of the Lebesgue measure to the tangent bundle) as an invariant measure for all times. To be precise, if the billiard table is a 'reasonably-nice' subset of Euclidean space (for instance, it admits the structure of a manifold with corners, an example of which would be the unit square if ), I'd like to prove that billiard flow maps admit the property that for all , where: is the tangent bundle of the table , considered as a subset of Euclidean space ; is the Lebesgue almost everywhere-defined billiard map built using 'elastic reflection boundary conditions' on , whose associated trajectories are piecewise linear and continuous in the spatial variable, but lower semi-continuous in the velocity variable for any initial point ; denotes the Lebesgue measure on ; denotes the pushforward operation, and denotes the restriction measure operation, whence is the restriction of the Lebesgue measure on to the set . One of the main issues in proving this statement is that the dynamics is not smooth, and one cannot appeal to the classical Liouville theorem of symplectic geometry to prove it. Does anyone know of a reference in which the above statement is proved, at least for some class of tables ? I have spent quite some time going through the literature, but I have so far come up empty handed. I was trained as a mathematical analyst, so I'm looking for a proof that someone in that community would consider as complete.","\mathcal{P}\subset\mathbb{R}^{N} N=2 T^{t}:T\mathcal{P}\rightarrow T\mathcal{P} T^{t}\#\mathscr{L}_{2N}\mathsf{L}T\mathcal{P}=\mathscr{L}_{2N}\mathsf{L}T\mathcal{P} t\in\mathbb{R} T\mathcal{P} \mathcal{P} \mathbb{R}^{2N} T^{t} \partial\mathcal{P} t\mapsto T^{t}((x_{0}, v_{0})) (x_{0}, v_{0})\in T\mathcal{P} \mathscr{L}_{2N} \mathbb{R}^{2N} \# \mathsf{L} \mathscr{L}_{2N}\mathsf{L}T\mathcal{P} \mathbb{R}^{2N} T\mathcal{P} \mathcal{P}","['analysis', 'measure-theory', 'mathematical-physics', 'statistical-mechanics', 'billiards']"
31,Fix $x$ and $y$ in $\Bbb R$. Suppose that $x < y + e$ for all $e>0$. Prove that $x\leqslant y$.,Fix  and  in . Suppose that  for all . Prove that .,x y \Bbb R x < y + e e>0 x\leqslant y,"I just started learning proofs in analysis class and this is my first time in this page. I learned about the Completeness Axiom and tried to solve some problems, but none of them look easy to me This is my proof: Assume $x>y$ is true, then there exists a number $n$ in natural numbers such that $n>e$ thereby making $x+n>y+e$ by the Archimedean Property $x+n>y+e$ and $x<y+e$ contradiction Therefore, $x\leqslant y$ (I think my proof is seriously messy) If it is possible can you please tell me how can I approach proof problems? Should I first look at the answers and memorize them all or just keep trying? I have no idea where to start And can you also teach me where can I learn how to write a decent coded math equation so that I can clearly deliver my questions? Thank you so much for your help","I just started learning proofs in analysis class and this is my first time in this page. I learned about the Completeness Axiom and tried to solve some problems, but none of them look easy to me This is my proof: Assume is true, then there exists a number in natural numbers such that thereby making by the Archimedean Property and contradiction Therefore, (I think my proof is seriously messy) If it is possible can you please tell me how can I approach proof problems? Should I first look at the answers and memorize them all or just keep trying? I have no idea where to start And can you also teach me where can I learn how to write a decent coded math equation so that I can clearly deliver my questions? Thank you so much for your help",x>y n n>e x+n>y+e x+n>y+e x<y+e x\leqslant y,"['analysis', 'solution-verification']"
32,How to show that controllability and observability are not affected by replacing $A$ with $(A+αI)$?,How to show that controllability and observability are not affected by replacing  with ?,A (A+αI),"I found a thread with the same question. The answer there: ""Controllability: there does not exist a left-eigenvector for $A$ that is orthogonal for $B$ . For your problem, for any $x$ such that $xA=λx, \  xB≠0$ . Since the eigenvectors of A are also the eigenvecors of $A+αI$ , i.e., $x(A+Iα)=x(λ+α)$ , you have $xB≠0$ ."" I don't understand what is the connection here. Why is it obvious that the eigenvecors of $A$ are also the eigenvecors of $A+αI$ ? Even if that is true, why does it mean that $x(A+Iα)=x(λ+α)$ ? I have read that replacing $A$ with $(A+αI $ ) can ruin stabilizability. But why can it ruin that and not controllability and observability?","I found a thread with the same question. The answer there: ""Controllability: there does not exist a left-eigenvector for that is orthogonal for . For your problem, for any such that . Since the eigenvectors of A are also the eigenvecors of , i.e., , you have ."" I don't understand what is the connection here. Why is it obvious that the eigenvecors of are also the eigenvecors of ? Even if that is true, why does it mean that ? I have read that replacing with ) can ruin stabilizability. But why can it ruin that and not controllability and observability?","A B x xA=λx, \  xB≠0 A+αI x(A+Iα)=x(λ+α) xB≠0 A A+αI x(A+Iα)=x(λ+α) A (A+αI ","['analysis', 'eigenvalues-eigenvectors', 'matrix-rank', 'control-theory', 'linear-control']"
33,Analyzing the proof of Kirszbraun's theorem,Analyzing the proof of Kirszbraun's theorem,,"Kirszbraun's theorem 1934 Let $A \subset \mathbb{R}^n$ . If $f \colon A \rightarrow \mathbb{R}^m$ is a $L$ -Lipschitz function, then there exists an extension $F \colon \mathbb{R}^n \rightarrow \mathbb{R} ^m$ of $f$ which is also $L$ -Lipschitz. I'm studying that important Lipschitz function extension theorem, better known as Kirszbraun's theorem. The following questions arise through the proof, which I have not been able to solve yet. If anyone has ideas to clearly answer these questions, I would greatly appreciate it. Why are the sets in the equation $(1)$ compact and why is $K_{\gamma} = \bigcap_{i=1}^{n}{B(y_i , \gamma r_i)} \neq \emptyset$ for any sufficiently large value of $\gamma$ ? Why is it enough to show that $\gamma_{0} \leq L$ ? If $0 \notin \text{conv}\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ , why should there be an $m-1$ dimensional plane separating the origin from $\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ and why would we have $B(0 , \varepsilon)$ on the opposite side of the plane from $\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ for all $\varepsilon$ sufficiently small, and why does that contradict the definition of $\gamma_0$ ? Here is the proof of the theorem: Proof If $m = 1$ , the result follows directly from McShane's theorem. Suppose $m>1$ and consider the family $\mathscr{F}$ of $L$ -Lipschitz extensions of $f$ to some set $T$ with $A \subset T$ . This collection is non-empty because it contains at least the original function $f$ . We define a partial ordering on $\scr{F}$ as follows: suppose that $g_1 \colon T_1 \rightarrow \mathbb{R}^m$ and $g_2 \colon T_2 \rightarrow \mathbb{R}^m$ are both elements of $ \scr{F}$ , then $g_1 \preceq g_2$ if and only if $g_2$ is an extension of $g_1$ , that is, $A \subset T_1 \subset T_2$ and $g_{1}(x) = g_{2}(x)$ for all $x \in T_1$ . (The same partial ordering is defined if we recall that a function from a subset of $\mathbb{R}^n$ to $\mathbb{R}^m$ is a set of elements of $\mathbb{R} ^n \times \mathbb{R}^m$ , and we partially order $\scr{F}$ by inclusion). By Hausdorff's maximal principle, $\scr{F}$ has a maximal totally ordered subfamily $\tilde{\scr{F}}$ . Let $\tilde{A}$ be the union of the domains of the functions in $\tilde{\scr{F}}$ . Let us define the function $F \colon \tilde{A} \rightarrow \mathbb{R}^m$ by $F(x) = g(x)$ where $g \in \tilde{\scr{F}}$ and $x \in \text{dom}(g)$ . Clearly $F$ is Lipschitz and $\text{Lip}(F) = L$ . We claim that $\tilde{A} = \mathbb{R}^n$ . If not, then we can fix $x_0 \in \mathbb{R}^n \setminus \tilde{A}$ . A contradiction would be reached if we show that there is $y_0 \in \mathbb{R}^m$ such that $\|y-y_0\| \leq L\|x-x_0\|$ whenever $y=F(x)$ and $x \in \tilde{A}$ , that is, if we show that \begin{equation}\tag{1} \bigcap_{x \in \tilde{A}}{B(F(x),L\|x-x_0\|)} \neq \emptyset \end{equation} Since $(1)$ involves an intersection of compact sets, it suffices to show that any such finite intersection is non-empty. Accordingly, let $x_1 , x_2, \cdots , x_n \in \tilde{A}$ be fixed. Let $y_i = F(x_i)$ , $r_i = \|x_i -x_0\|$ , for $i = 1,2, \cdots , n$ , and $r^{*} = \sup\{r_1 , r_2 , \cdots , r_n\}$ . We know that for any sufficiently large value of $\gamma$ $$K_{\gamma} = \bigcap_{i=1}^{n}{B(y_i , \gamma r_i)} \neq \emptyset$$ Let $$\gamma_0 = \inf\{\gamma \colon K_{\gamma} \neq \emptyset\}$$ Since \begin{equation}\tag{2} K_{\gamma_{0}} = \bigcap_{\gamma > \gamma_{0}}{K_{\gamma}} \end{equation} and the intersection of any finitely many of the sets on the right-hand side of $(2)$ is non-empty, we see that $K_{\gamma_0} \neq \emptyset$ . It will suffice to show that $\gamma_{0} \leq L$ . We many, of course, assume $\gamma_{0} >0$ . Note that $K_{\gamma_{0}}$ must contain exactly one point, because if $k_1 ,k_2 \in K_{\gamma_{0}}$ , then \begin{align*} \left\|\dfrac{k_1 + k_2}{2} - y_i\right\|^2 &= \dfrac{\|k_1 +k_2\|^2}{4} + \|y_i\|^2 - ( k_1 - k_2) \cdot y_1 \\ &=\frac{\|k_1\|^2}{2} + \frac{\|k_2\|^2}{2} - \frac{\|k_1 - k_2\|^2}{4} + \ |y_i\|^2 - k_{1} \cdot y_i - k_{2} \cdot y_i \\ &= \dfrac{\|k_1 - y_i\|^2 + \|k_2 -y_i\|^2}{2} - \dfrac{\|k_1 - k_2\|^2}{4} \\ &\leq \gamma_{0}^2 r_{i}^2 - \dfrac{\|k_1 - k_2\|^2}{4(r^{*})^2}r_{i}^2 \end{align*} holds for $i=1,2, \cdots , n$ , and $\dfrac{k_1 + k_2}{2} \in K_{\gamma}$ with $\gamma = \sqrt{\gamma_{0}^ 2 - \dfrac{\|k_1 - k_2\|^2}{4(r^{*})^2}} < \gamma_{0}$ contradicting the definition of $\gamma_{0}$ . By translating the coordinate system if necessary, we can assume that $K_{\gamma_{0}} = \{0\}$ . Consequently, we have $\|y_i\| \leq \gamma_{0} r_i$ , for $i = 1,2, \cdots, n$ . We now claim that $0 \in \text{conv}\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ . Were that not the case, there would exist an $m-1$ dimensional plane separating the origin from $\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ , but then for all sufficiently small $\varepsilon >0$ we would have $B(0 , \varepsilon)$ on the opposite side of the plane from $\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ , again contradicting the definition of $\gamma_{0}$ . Thus we can choose non-negative scalars $\lambda_1 , \lambda_2 , \cdots, \lambda_n$ such that $$\|y_i\| < \gamma_{0} r_i \implies \lambda_i = 0$$ and $$0 = \sum_{i=1}^n{\lambda_i y_i} \hspace{0.5cm} \text{con} \hspace{0.4cm} \sum_{i=1}^n{\lambda_i} = 1$$ It follows that \begin{align*}    0 &= 2 \left\| \sum_{i=i}^n {\lambda_i y_i}\right\|^2 \\      &= 2 \sum_{i,j=1}^n {\lambda_i \lambda_j y_i \cdot y_j} \\      &= \sum_{i,j =1}^n {\lambda_i \lambda_j \left[\|y_i\|^2 + \|y_j\|^2 - \|y_i -y_j\|^2\right]}\\      &\geq \sum_{i,j=1}^n {\lambda_i \lambda_j \left[\gamma_{0}^2 r_{i}^2 + \gamma_{0}^2 r_{j}^2 - L^2 \|x_i - x_j\|^2\right]}\\      &= \sum_{i,j=1}^n {\lambda_i \lambda_j \left[2(x_i -x_0)\cdot \gamma_{0}^2(x_j - x_0) + (\gamma_{0}^2 - L^2)\|x_i - x_j\|^2\right]}\\      &=2 \left\|\gamma_{0} \sum_{i=1}^n{\lambda_i (x_i - x_0)}\right\|^2 + (\gamma_{0}^2 - L^2)\sum_{i,j=1}^n {\lambda_i \lambda_j \|x_i - x_j\|^2} \tag{3} \end{align*} If there were but one non-zero $\lambda_i$ , then the second term in $(3)$ would vanish and the first would be positive, a contradiction. Thus there are at least two non-zero $\lambda_i$ 's and the second term in $(3)$ must be non-positive, forcing $\gamma_{0} \leq L$ , as desired. $\square$","Kirszbraun's theorem 1934 Let . If is a -Lipschitz function, then there exists an extension of which is also -Lipschitz. I'm studying that important Lipschitz function extension theorem, better known as Kirszbraun's theorem. The following questions arise through the proof, which I have not been able to solve yet. If anyone has ideas to clearly answer these questions, I would greatly appreciate it. Why are the sets in the equation compact and why is for any sufficiently large value of ? Why is it enough to show that ? If , why should there be an dimensional plane separating the origin from and why would we have on the opposite side of the plane from for all sufficiently small, and why does that contradict the definition of ? Here is the proof of the theorem: Proof If , the result follows directly from McShane's theorem. Suppose and consider the family of -Lipschitz extensions of to some set with . This collection is non-empty because it contains at least the original function . We define a partial ordering on as follows: suppose that and are both elements of , then if and only if is an extension of , that is, and for all . (The same partial ordering is defined if we recall that a function from a subset of to is a set of elements of , and we partially order by inclusion). By Hausdorff's maximal principle, has a maximal totally ordered subfamily . Let be the union of the domains of the functions in . Let us define the function by where and . Clearly is Lipschitz and . We claim that . If not, then we can fix . A contradiction would be reached if we show that there is such that whenever and , that is, if we show that Since involves an intersection of compact sets, it suffices to show that any such finite intersection is non-empty. Accordingly, let be fixed. Let , , for , and . We know that for any sufficiently large value of Let Since and the intersection of any finitely many of the sets on the right-hand side of is non-empty, we see that . It will suffice to show that . We many, of course, assume . Note that must contain exactly one point, because if , then holds for , and with contradicting the definition of . By translating the coordinate system if necessary, we can assume that . Consequently, we have , for . We now claim that . Were that not the case, there would exist an dimensional plane separating the origin from , but then for all sufficiently small we would have on the opposite side of the plane from , again contradicting the definition of . Thus we can choose non-negative scalars such that and It follows that If there were but one non-zero , then the second term in would vanish and the first would be positive, a contradiction. Thus there are at least two non-zero 's and the second term in must be non-positive, forcing , as desired.","A \subset \mathbb{R}^n f \colon A \rightarrow \mathbb{R}^m L F \colon \mathbb{R}^n \rightarrow \mathbb{R} ^m f L (1) K_{\gamma} = \bigcap_{i=1}^{n}{B(y_i , \gamma r_i)} \neq \emptyset \gamma \gamma_{0} \leq L 0 \notin \text{conv}\{y_i \colon \|y_i\| = \gamma_{0} r_i\} m-1 \{y_i \colon \|y_i\| = \gamma_{0} r_i\} B(0 , \varepsilon) \{y_i \colon \|y_i\| = \gamma_{0} r_i\} \varepsilon \gamma_0 m = 1 m>1 \mathscr{F} L f T A \subset T f \scr{F} g_1 \colon T_1 \rightarrow \mathbb{R}^m g_2 \colon T_2 \rightarrow \mathbb{R}^m  \scr{F} g_1 \preceq g_2 g_2 g_1 A \subset T_1 \subset T_2 g_{1}(x) = g_{2}(x) x \in T_1 \mathbb{R}^n \mathbb{R}^m \mathbb{R} ^n \times \mathbb{R}^m \scr{F} \scr{F} \tilde{\scr{F}} \tilde{A} \tilde{\scr{F}} F \colon \tilde{A} \rightarrow \mathbb{R}^m F(x) = g(x) g \in \tilde{\scr{F}} x \in \text{dom}(g) F \text{Lip}(F) = L \tilde{A} = \mathbb{R}^n x_0 \in \mathbb{R}^n \setminus \tilde{A} y_0 \in \mathbb{R}^m \|y-y_0\| \leq L\|x-x_0\| y=F(x) x \in \tilde{A} \begin{equation}\tag{1}
\bigcap_{x \in \tilde{A}}{B(F(x),L\|x-x_0\|)} \neq \emptyset
\end{equation} (1) x_1 , x_2, \cdots , x_n \in \tilde{A} y_i = F(x_i) r_i = \|x_i -x_0\| i = 1,2, \cdots , n r^{*} = \sup\{r_1 , r_2 , \cdots , r_n\} \gamma K_{\gamma} = \bigcap_{i=1}^{n}{B(y_i , \gamma r_i)} \neq \emptyset \gamma_0 = \inf\{\gamma \colon K_{\gamma} \neq \emptyset\} \begin{equation}\tag{2}
K_{\gamma_{0}} = \bigcap_{\gamma > \gamma_{0}}{K_{\gamma}}
\end{equation} (2) K_{\gamma_0} \neq \emptyset \gamma_{0} \leq L \gamma_{0} >0 K_{\gamma_{0}} k_1 ,k_2 \in K_{\gamma_{0}} \begin{align*}
\left\|\dfrac{k_1 + k_2}{2} - y_i\right\|^2 &= \dfrac{\|k_1 +k_2\|^2}{4} + \|y_i\|^2 - ( k_1 - k_2) \cdot y_1 \\
&=\frac{\|k_1\|^2}{2} + \frac{\|k_2\|^2}{2} - \frac{\|k_1 - k_2\|^2}{4} + \ |y_i\|^2 - k_{1} \cdot y_i - k_{2} \cdot y_i \\
&= \dfrac{\|k_1 - y_i\|^2 + \|k_2 -y_i\|^2}{2} - \dfrac{\|k_1 - k_2\|^2}{4} \\
&\leq \gamma_{0}^2 r_{i}^2 - \dfrac{\|k_1 - k_2\|^2}{4(r^{*})^2}r_{i}^2
\end{align*} i=1,2, \cdots , n \dfrac{k_1 + k_2}{2} \in K_{\gamma} \gamma = \sqrt{\gamma_{0}^ 2 - \dfrac{\|k_1 - k_2\|^2}{4(r^{*})^2}} < \gamma_{0} \gamma_{0} K_{\gamma_{0}} = \{0\} \|y_i\| \leq \gamma_{0} r_i i = 1,2, \cdots, n 0 \in \text{conv}\{y_i \colon \|y_i\| = \gamma_{0} r_i\} m-1 \{y_i \colon \|y_i\| = \gamma_{0} r_i\} \varepsilon >0 B(0 , \varepsilon) \{y_i \colon \|y_i\| = \gamma_{0} r_i\} \gamma_{0} \lambda_1 , \lambda_2 , \cdots, \lambda_n \|y_i\| < \gamma_{0} r_i \implies \lambda_i = 0 0 = \sum_{i=1}^n{\lambda_i y_i} \hspace{0.5cm} \text{con} \hspace{0.4cm} \sum_{i=1}^n{\lambda_i} = 1 \begin{align*}
   0 &= 2 \left\| \sum_{i=i}^n {\lambda_i y_i}\right\|^2 \\
     &= 2 \sum_{i,j=1}^n {\lambda_i \lambda_j y_i \cdot y_j} \\
     &= \sum_{i,j =1}^n {\lambda_i \lambda_j \left[\|y_i\|^2 + \|y_j\|^2 - \|y_i -y_j\|^2\right]}\\
     &\geq \sum_{i,j=1}^n {\lambda_i \lambda_j \left[\gamma_{0}^2 r_{i}^2 + \gamma_{0}^2 r_{j}^2 - L^2 \|x_i - x_j\|^2\right]}\\
     &= \sum_{i,j=1}^n {\lambda_i \lambda_j \left[2(x_i -x_0)\cdot \gamma_{0}^2(x_j - x_0) + (\gamma_{0}^2 - L^2)\|x_i - x_j\|^2\right]}\\
     &=2 \left\|\gamma_{0} \sum_{i=1}^n{\lambda_i (x_i - x_0)}\right\|^2 + (\gamma_{0}^2 - L^2)\sum_{i,j=1}^n {\lambda_i \lambda_j \|x_i - x_j\|^2} \tag{3}
\end{align*} \lambda_i (3) \lambda_i (3) \gamma_{0} \leq L \square","['analysis', 'proof-explanation', 'compactness', 'lipschitz-functions', 'well-orders']"
34,How to prove the following inequality between $u$ and integral of its gradient,How to prove the following inequality between  and integral of its gradient,u,"I'm reading a paper and it states that the following result is well known. I can't prove it myself but would like to see a proof before I continue the reading, can anybody help? $\text{Let}\ u\in C_0^{\infty}(B_R(x))\ \text{where}\ B_R(x)\ \text{is a ball of radius}\ R\ \text{centered at}\ x\ \text{then}$ $$|u(x)| \leq c \int_{B_{R}(x)} \frac{|\nabla u(y)|}{|x-y|^{n-1}} d y$$","I'm reading a paper and it states that the following result is well known. I can't prove it myself but would like to see a proof before I continue the reading, can anybody help?",\text{Let}\ u\in C_0^{\infty}(B_R(x))\ \text{where}\ B_R(x)\ \text{is a ball of radius}\ R\ \text{centered at}\ x\ \text{then} |u(x)| \leq c \int_{B_{R}(x)} \frac{|\nabla u(y)|}{|x-y|^{n-1}} d y,"['analysis', 'multivariable-calculus']"
35,Proving that implicit midpoint method for Hamiltonian systems is symplectic using a criterion,Proving that implicit midpoint method for Hamiltonian systems is symplectic using a criterion,,"The implicit midpoint rule is defined as $$y_{n+1}=y_n+hJ^{-1}\nabla H\left(\frac{y_{n+1}+y_n}{2}\right).$$ where $y=(p,q)$ . I know how to prove that this method is symplectic by hand, using the definition of symplecticity and very lengthy computation. But in this note , it says that we only have to use the following critierion ( Theorem 5 on page 11 ): Let $(p,q)\rightarrow (P,Q)$ be a smooth mapping, close to the identity. It is symplectic if and only if one of the following conditions holds locally: $$(Q-q)^T d(P+p)-(P-p)^Td(Q+q)=2dS \mbox{ for some function } S((P+p)/2,(Q+q)/2).$$ I tried to prove the symplecticity of the method using this criterion but I was not able to, even for the most simple case where $p$ and $q$ has only one dimension. This is what I tried: $$Q=q+h\frac{\partial H}{\partial p}\left(\frac{q+Q}{2},\frac{p+P}{2}\right)$$ $$P=p-h\frac{\partial H}{\partial q}\left(\frac{q+Q}{2},\frac{p+P}{2}\right)$$ So that \begin{align*} &\quad(Q-q)^T d(P+p)-(P-p)^Td(Q+q)\\ &=h\frac{\partial H}{\partial p}\left(\frac{q+Q}{2},\frac{p+P}{2}\right) d\left(2p-h\frac{\partial H}{\partial q}\left(\frac{q+Q}{2},\frac{p+P}{2}\right)\right)\\ &\quad h\frac{\partial H}{\partial q}\left(\frac{q+Q}{2},\frac{p+P}{2}\right) d\left(2q+h\frac{\partial H}{\partial p}\left(\frac{q+Q}{2},\frac{p+P}{2}\right)\right) \end{align*} I continued to expand the terms, but I don't see how to find the function $S((P+p)/2,(Q+q)/2)$ so that the above is equal to $2dS$ . But in the notes, it sounds like that it should be easy. Could you help? Thanks in advance!","The implicit midpoint rule is defined as where . I know how to prove that this method is symplectic by hand, using the definition of symplecticity and very lengthy computation. But in this note , it says that we only have to use the following critierion ( Theorem 5 on page 11 ): Let be a smooth mapping, close to the identity. It is symplectic if and only if one of the following conditions holds locally: I tried to prove the symplecticity of the method using this criterion but I was not able to, even for the most simple case where and has only one dimension. This is what I tried: So that I continued to expand the terms, but I don't see how to find the function so that the above is equal to . But in the notes, it sounds like that it should be easy. Could you help? Thanks in advance!","y_{n+1}=y_n+hJ^{-1}\nabla H\left(\frac{y_{n+1}+y_n}{2}\right). y=(p,q) (p,q)\rightarrow (P,Q) (Q-q)^T d(P+p)-(P-p)^Td(Q+q)=2dS \mbox{ for some function } S((P+p)/2,(Q+q)/2). p q Q=q+h\frac{\partial H}{\partial p}\left(\frac{q+Q}{2},\frac{p+P}{2}\right) P=p-h\frac{\partial H}{\partial q}\left(\frac{q+Q}{2},\frac{p+P}{2}\right) \begin{align*}
&\quad(Q-q)^T d(P+p)-(P-p)^Td(Q+q)\\
&=h\frac{\partial H}{\partial p}\left(\frac{q+Q}{2},\frac{p+P}{2}\right) d\left(2p-h\frac{\partial H}{\partial q}\left(\frac{q+Q}{2},\frac{p+P}{2}\right)\right)\\
&\quad h\frac{\partial H}{\partial q}\left(\frac{q+Q}{2},\frac{p+P}{2}\right) d\left(2q+h\frac{\partial H}{\partial p}\left(\frac{q+Q}{2},\frac{p+P}{2}\right)\right)
\end{align*} S((P+p)/2,(Q+q)/2) 2dS","['analysis', 'numerical-methods', 'symplectic-geometry', 'hamilton-equations']"
36,Proof of a closed-form of $\prod_{n=1}^{\infty} \frac{1}{e} \left(1+\frac{1}{3n}\right)^{3n+1/2}$,Proof of a closed-form of,\prod_{n=1}^{\infty} \frac{1}{e} \left(1+\frac{1}{3n}\right)^{3n+1/2},"I'm looking for a proof of the following equality: $$\prod_{n=1}^{\infty} \frac{1}{e} \left(1+\frac{1}{3n}\right)^{3n+1/2}=\sqrt{\frac{\Gamma\left(\frac{1}{3}\right)}{2\pi}}\frac{3^{13/24} \exp \left[1+\frac{2\pi^2-3\psi_1 \left(\frac{1}{3}\right)}{12\pi \sqrt{3}} \right]}{A^4}$$ Where $\psi_1$ is the trigamma function, and $A$ is the Glaisher-Kinkelin constant. This product is the last entry of this Wolfram Mathworld page. The product is stated to have been found by Gosper, however, his proof is not listed in the references. I have checked the OEIS entry of its decimal expansion also, however, the closed-form is not referenced there either. Assuming, Gosper refers to Bill Gosper, nothing about this product is stated on his Wikipedia page either.","I'm looking for a proof of the following equality: Where is the trigamma function, and is the Glaisher-Kinkelin constant. This product is the last entry of this Wolfram Mathworld page. The product is stated to have been found by Gosper, however, his proof is not listed in the references. I have checked the OEIS entry of its decimal expansion also, however, the closed-form is not referenced there either. Assuming, Gosper refers to Bill Gosper, nothing about this product is stated on his Wikipedia page either.",\prod_{n=1}^{\infty} \frac{1}{e} \left(1+\frac{1}{3n}\right)^{3n+1/2}=\sqrt{\frac{\Gamma\left(\frac{1}{3}\right)}{2\pi}}\frac{3^{13/24} \exp \left[1+\frac{2\pi^2-3\psi_1 \left(\frac{1}{3}\right)}{12\pi \sqrt{3}} \right]}{A^4} \psi_1 A,"['calculus', 'integration', 'sequences-and-series', 'analysis', 'infinite-product']"
37,Prove that $f(x)=x^2$ is integrable using superior and inferior sums,Prove that  is integrable using superior and inferior sums,f(x)=x^2,"I need to prove that $f:[-1,2]\rightarrow\mathbb{R}$ given by $f(x)=x^2$ is integrable using this theorem: Let $f:[a,b]\rightarrow\mathbb{R}$ be limited. The following afirmations are equivalent: (1) $f$ is integrable; (2) for all $\varepsilon>0$ , exists partitions $P$ and $Q$ from $[a,b]$ such that $S(f;Q)-s(f;P)<\varepsilon$ ; (3) for all $\varepsilon>0$ , exists a partition $P=\{t_0,\dots,t_n\}$ from $[a,b]$ such that $$S(f;P)-s(f;P)=\sum_{i=1}^{n}\omega_i(t_i-t_{i-1})<\varepsilon.$$ $$s(f;P)=\sum_{i=1}^{n}m_i(t_i-t_{i-1}); S(f;P)=\sum_{i=1}^{n}M_i(t_i-t_{i-1});$$ $$m_i=\inf\{f(x);x\in[t_{i-1},t_i]\};M_i=\sup\{f(x);x\in[t_{i-1},t_i]\};\omega_i=M_i-m_i.$$ I got stuck in a part of the demonstration I tried: Given $\varepsilon>0$ , exists $n\in\mathbb{N}$ such that $$\dfrac{1}{n}<\varepsilon\quad\mbox{ and }\quad\dfrac{2}{n}<2\varepsilon.$$ Let's take partitions $P_1$ and $P_2$ that refine $P_0=\{-1,0,2\}$ such that $$P_1=\left\{t_0=-1,t_1=-1+\dfrac{1}{n},t_2=-1+\dfrac{2}{n},\dots,t_n=-1+\dfrac{n}{n}=0\right\}\mbox{and}$$ $$P_2=\left\{t_0=0,t_1=0+\dfrac{2}{n},t_2=0+\dfrac{2(2)}{n},\dots,t_n=0+\dfrac{n(2)}{n}\right\}.$$ Note that for each interval $[t_{i-1},t_i]$ from $P_1$ we have that $$m_i=f(t_i)=(t_i)^2\quad\mbox{and}\quad M_i=f(t_{i-1})=(t_{i-1})^2;$$ because $f$ strictly decreasing in $[-1,0]$ . Note, also, that for each interval $[t_{i-1},t_i]$ from $P_2$ we have that $$m_i=f(t_{i-1})=(t_{i-1})^2\quad\mbox{and}\quad M_i=f(t_{i})=(t_{i})^2;$$ because $f$ is strictly increasing in $[0,2]$ . In this way, $$S(f;P_2)-s(f;P_1)=$$ $$\left[\left(\dfrac{2}{n}\right)^2\left(\dfrac{2}{n}-0\right)+\left(\dfrac{4}{n}\right)^2\left(\dfrac{4}{n}-\dfrac{2}{n}\right)+\cdots+\left(2\right)^2\left(2-\dfrac{(n-1)(2)}{n}\right)\right]$$ $$-\left[\left(-1\right)^2\left(-1+\dfrac{1}{n}-(-1)\right)+\left(-1+\dfrac{1}{n}\right)^2\left(-1+\dfrac{2}{n}-\left(-1+\dfrac{1}{n}\right)\right)+\cdots+\left(-1+\dfrac{n-1}{n}\right)^2\left(0-\left(-1+\dfrac{n-1}{n}\right)\right)\right]$$ and that's it... I got stuck. What I was trying to do is simplify all of this to get that $$S(f;P_2)-s(f;P_1)=\left(\dfrac{2}{n}\right)-\left(\dfrac{1}{n}\right)<2\varepsilon-\varepsilon=\varepsilon.$$ But I couldn't do it... anyone can help me?","I need to prove that given by is integrable using this theorem: Let be limited. The following afirmations are equivalent: (1) is integrable; (2) for all , exists partitions and from such that ; (3) for all , exists a partition from such that I got stuck in a part of the demonstration I tried: Given , exists such that Let's take partitions and that refine such that Note that for each interval from we have that because strictly decreasing in . Note, also, that for each interval from we have that because is strictly increasing in . In this way, and that's it... I got stuck. What I was trying to do is simplify all of this to get that But I couldn't do it... anyone can help me?","f:[-1,2]\rightarrow\mathbb{R} f(x)=x^2 f:[a,b]\rightarrow\mathbb{R} f \varepsilon>0 P Q [a,b] S(f;Q)-s(f;P)<\varepsilon \varepsilon>0 P=\{t_0,\dots,t_n\} [a,b] S(f;P)-s(f;P)=\sum_{i=1}^{n}\omega_i(t_i-t_{i-1})<\varepsilon. s(f;P)=\sum_{i=1}^{n}m_i(t_i-t_{i-1}); S(f;P)=\sum_{i=1}^{n}M_i(t_i-t_{i-1}); m_i=\inf\{f(x);x\in[t_{i-1},t_i]\};M_i=\sup\{f(x);x\in[t_{i-1},t_i]\};\omega_i=M_i-m_i. \varepsilon>0 n\in\mathbb{N} \dfrac{1}{n}<\varepsilon\quad\mbox{ and }\quad\dfrac{2}{n}<2\varepsilon. P_1 P_2 P_0=\{-1,0,2\} P_1=\left\{t_0=-1,t_1=-1+\dfrac{1}{n},t_2=-1+\dfrac{2}{n},\dots,t_n=-1+\dfrac{n}{n}=0\right\}\mbox{and} P_2=\left\{t_0=0,t_1=0+\dfrac{2}{n},t_2=0+\dfrac{2(2)}{n},\dots,t_n=0+\dfrac{n(2)}{n}\right\}. [t_{i-1},t_i] P_1 m_i=f(t_i)=(t_i)^2\quad\mbox{and}\quad M_i=f(t_{i-1})=(t_{i-1})^2; f [-1,0] [t_{i-1},t_i] P_2 m_i=f(t_{i-1})=(t_{i-1})^2\quad\mbox{and}\quad M_i=f(t_{i})=(t_{i})^2; f [0,2] S(f;P_2)-s(f;P_1)= \left[\left(\dfrac{2}{n}\right)^2\left(\dfrac{2}{n}-0\right)+\left(\dfrac{4}{n}\right)^2\left(\dfrac{4}{n}-\dfrac{2}{n}\right)+\cdots+\left(2\right)^2\left(2-\dfrac{(n-1)(2)}{n}\right)\right] -\left[\left(-1\right)^2\left(-1+\dfrac{1}{n}-(-1)\right)+\left(-1+\dfrac{1}{n}\right)^2\left(-1+\dfrac{2}{n}-\left(-1+\dfrac{1}{n}\right)\right)+\cdots+\left(-1+\dfrac{n-1}{n}\right)^2\left(0-\left(-1+\dfrac{n-1}{n}\right)\right)\right] S(f;P_2)-s(f;P_1)=\left(\dfrac{2}{n}\right)-\left(\dfrac{1}{n}\right)<2\varepsilon-\varepsilon=\varepsilon.","['real-analysis', 'analysis', 'riemann-sum']"
38,"Orthonormal sequence in L$^2(0,1)$",Orthonormal sequence in L,"^2(0,1)","Let $\{\phi_n\}_{n=1}^{\infty}$ be an orthonormal sequence in L $^2(0,1)$ . Prove that $\{\phi_n\}_{n=1}^{\infty}$ is an orthonormal basis iff $\forall a\in [0,1]$ , $a=\sum_{n=1}^{\infty}|\int_0^a\phi_ndx|^2$ . For the first direction, this is just using Parseval's identity with $\chi_{(0,a)}$ , because: $$a=\|\chi_{(0,a)}\|^2=\sum_{n=1}^{\infty}|\int_0^1\phi_n\chi_{(0,a)}dx|^2=\sum_{n=1}^{\infty}|\int_0^a\phi_ndx|^2$$ . For the second direction, I tried using the fact that $\{\phi_n\}$ is orthonormal basis iff $\{\{\phi_n\}_{n=1}^{\infty}\})^{\bot}=\{\vec 0\}$ , but this didn't work. Also tried to show that Parsavel's equality must hold but also got stuck there. Any hint would be appreciated.","Let be an orthonormal sequence in L . Prove that is an orthonormal basis iff , . For the first direction, this is just using Parseval's identity with , because: . For the second direction, I tried using the fact that is orthonormal basis iff , but this didn't work. Also tried to show that Parsavel's equality must hold but also got stuck there. Any hint would be appreciated.","\{\phi_n\}_{n=1}^{\infty} ^2(0,1) \{\phi_n\}_{n=1}^{\infty} \forall a\in [0,1] a=\sum_{n=1}^{\infty}|\int_0^a\phi_ndx|^2 \chi_{(0,a)} a=\|\chi_{(0,a)}\|^2=\sum_{n=1}^{\infty}|\int_0^1\phi_n\chi_{(0,a)}dx|^2=\sum_{n=1}^{\infty}|\int_0^a\phi_ndx|^2 \{\phi_n\} \{\{\phi_n\}_{n=1}^{\infty}\})^{\bot}=\{\vec 0\}","['real-analysis', 'analysis', 'hilbert-spaces', 'banach-spaces', 'orthogonality']"
39,How to prove the existence of such a map and such a function,How to prove the existence of such a map and such a function,,"This question was in my manifolds notes which I borrowed from a senior and I was unable to  prove the result. Let S be an k-dimensional submanifold of an n-manifold M with k<n. Show that , for every point $p\in S$ , there exists an open set $U\subset M$ containing p, and a smooth function $f: U \to \mathbb{R}^{n-k}$ such that $f^{-1}(0) =U \cap S$ . For every $p\in S$ there will always exists an open set $U \in M$ containing p but I have to find a specific U that also satisfies the condition on the smooth function. A subset S of M is called a k-dimensional submanifold of M if the following holds: If $p\in S$ there exists a chart $(U,\phi)$ containing P st $\phi(U\cap S)$ is an open subset of $\mathbb{R}^k$ . But How to use it to find such a function. I am really sorry but I am unable any way to prove it. I couldn't add much to my attempt and thoughts because I am struck. Kindly don't close this question. Please shed some light on this!","This question was in my manifolds notes which I borrowed from a senior and I was unable to  prove the result. Let S be an k-dimensional submanifold of an n-manifold M with k<n. Show that , for every point , there exists an open set containing p, and a smooth function such that . For every there will always exists an open set containing p but I have to find a specific U that also satisfies the condition on the smooth function. A subset S of M is called a k-dimensional submanifold of M if the following holds: If there exists a chart containing P st is an open subset of . But How to use it to find such a function. I am really sorry but I am unable any way to prove it. I couldn't add much to my attempt and thoughts because I am struck. Kindly don't close this question. Please shed some light on this!","p\in S U\subset M f: U \to \mathbb{R}^{n-k} f^{-1}(0) =U \cap S p\in S U \in M p\in S (U,\phi) \phi(U\cap S) \mathbb{R}^k",['analysis']
40,Maximizing expression under assumption that similar expression is maximized.,Maximizing expression under assumption that similar expression is maximized.,,"I'm trying to maximize the following expression: $$ \left(\frac{a_{2}}{4^{k_{1}}}-4 a_{2}^{3}\right) \cdot 1+\sum_{j=2}^{m}\left(\frac{a_{j+1}}{4^{k_{j}}}-4 a_{j+1}^{3}\right) \prod_{i=2}^{j}\left(\frac{4^{k_{i}}}{4^{k_{i-1}}}-1\right) $$ where $k_i$ is a positive integer such that $a_i \in (2^{-k_i-1}, 2^{-k_i}]$ for $i = 1,...,m$ . We can assume that $a_i \in [0,1]$ and that the values are decreasing i.e if $a_i \in (2^{-k_i-1}, 2^{-k_i}]$ then $a_{i+1} \leq 2^{-k_i-1}$ . Based on some experimentation I have a strong suspicion that the expression is maximized for $a_{i+1}=1 / 2^{k_{i}+2}$ . I figured that induction and local optimality will probably do the trick: Assume the result for limited $i$ , apply the assumption, then optimize the terms with $i+1$ under the assumption that the smaller terms must also be chosen optimally wrt each other. However, I can't really get anywhere with this. It is stated in this [Page 63] that a similar expression, namely $$ \left(\frac{a_{2}}{2^{k_{1}}}-2 a_{2}^{2}\right) \cdot 1+\sum_{j=2}^{m}\left(\frac{a_{j+1}}{2^{k_{j}}}-2 a_{j+1}^{2}\right) \prod_{i=2}^{j}\left(\frac{2^{k_{i}}}{2^{k_{i-1}}}-1\right) $$ where $a_i$ is subject to the same constraints is maximized if indeed we choose $a_{i+1}=1 / 2^{k_{i}+2}$ , for $i=1, \ldots, m$ , i.e., $k_{i}=k_{1}+2(i-1)$ . So. Any help with a solution from the scratch would be greatly appreciated or alternatively, a solution using the result of the second expression would be more than sufficient. Edit: The $k_i$ 's are determined by the $a_i$ 's (as it should show in the second line). Providing some more context $2^{-k_i}$ is the smallest number greater than or equal to $a_i$ or alternatively $a_i$ is the side length of a square and we round $a_i$ up to the closest power of 1/2.","I'm trying to maximize the following expression: where is a positive integer such that for . We can assume that and that the values are decreasing i.e if then . Based on some experimentation I have a strong suspicion that the expression is maximized for . I figured that induction and local optimality will probably do the trick: Assume the result for limited , apply the assumption, then optimize the terms with under the assumption that the smaller terms must also be chosen optimally wrt each other. However, I can't really get anywhere with this. It is stated in this [Page 63] that a similar expression, namely where is subject to the same constraints is maximized if indeed we choose , for , i.e., . So. Any help with a solution from the scratch would be greatly appreciated or alternatively, a solution using the result of the second expression would be more than sufficient. Edit: The 's are determined by the 's (as it should show in the second line). Providing some more context is the smallest number greater than or equal to or alternatively is the side length of a square and we round up to the closest power of 1/2.","
\left(\frac{a_{2}}{4^{k_{1}}}-4 a_{2}^{3}\right) \cdot 1+\sum_{j=2}^{m}\left(\frac{a_{j+1}}{4^{k_{j}}}-4 a_{j+1}^{3}\right) \prod_{i=2}^{j}\left(\frac{4^{k_{i}}}{4^{k_{i-1}}}-1\right)
 k_i a_i \in (2^{-k_i-1}, 2^{-k_i}] i = 1,...,m a_i \in [0,1] a_i \in (2^{-k_i-1}, 2^{-k_i}] a_{i+1} \leq 2^{-k_i-1} a_{i+1}=1 / 2^{k_{i}+2} i i+1 
\left(\frac{a_{2}}{2^{k_{1}}}-2 a_{2}^{2}\right) \cdot 1+\sum_{j=2}^{m}\left(\frac{a_{j+1}}{2^{k_{j}}}-2 a_{j+1}^{2}\right) \prod_{i=2}^{j}\left(\frac{2^{k_{i}}}{2^{k_{i-1}}}-1\right)
 a_i a_{i+1}=1 / 2^{k_{i}+2} i=1, \ldots, m k_{i}=k_{1}+2(i-1) k_i a_i 2^{-k_i} a_i a_i a_i","['analysis', 'optimization', 'induction']"
41,Would it be possible to skip undergraduate college with self study/online courses [closed],Would it be possible to skip undergraduate college with self study/online courses [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 2 years ago . Improve this question Hi I was wondering if it were possible to skip undergraduate and apply to a graduate school. I have been taking some math courses through CTY a somewhat accredited program offered by Johns Hopkins university. They offer up to real analysis 1 and complex analysis 1. If I take up to there, would it be possible to skip undergraduate having self studied the higher level courses i.e topology/further analysis courses.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 2 years ago . Improve this question Hi I was wondering if it were possible to skip undergraduate and apply to a graduate school. I have been taking some math courses through CTY a somewhat accredited program offered by Johns Hopkins university. They offer up to real analysis 1 and complex analysis 1. If I take up to there, would it be possible to skip undergraduate having self studied the higher level courses i.e topology/further analysis courses.",,['analysis']
42,Proof of Mergesort $N$ elements with $N \log N + O(N)$ comparisons,Proof of Mergesort  elements with  comparisons,N N \log N + O(N),"In the book ""An introduction to the Analysis of Algorithms"", written by Robert Sedgewick and Philippe Flajolet during the proof of the Theorem 1.1: (Mergesort) To sort an array of N elements, Mergesort uses $N \log N + O(N)$ comparions. It contains the following part: To get an indication for the nature of the solution to this recurrence, we consider the case when N is a power of 2: $C_{2^n} = 2C_{2^n-1} + 2^n$ for $n \geq 1$ with $C_1 = 0$ . Dividing both sides of this equation by $2^n$ , we find that $$ \frac{C_{2^n}}{2^n} = \frac{C_{2^n-1}}{2^{n-1}} + 1 = \frac{C_{2^n-2}}{2^{n-2}} + 2 = \frac{C_{2^n-3}}{2^{n-3}} + 3 = ... = \frac{C_{2^0}}{2^0} + n = n. $$ I can't understand the trick behind incrementing the right side of the term (+1, then +2, then +3). Why does the change of the left side lead to the increment on the right side? Any help is kindly appreciated.","In the book ""An introduction to the Analysis of Algorithms"", written by Robert Sedgewick and Philippe Flajolet during the proof of the Theorem 1.1: (Mergesort) To sort an array of N elements, Mergesort uses comparions. It contains the following part: To get an indication for the nature of the solution to this recurrence, we consider the case when N is a power of 2: for with . Dividing both sides of this equation by , we find that I can't understand the trick behind incrementing the right side of the term (+1, then +2, then +3). Why does the change of the left side lead to the increment on the right side? Any help is kindly appreciated.","N \log N + O(N) C_{2^n} = 2C_{2^n-1} + 2^n n \geq 1 C_1 = 0 2^n 
\frac{C_{2^n}}{2^n} = \frac{C_{2^n-1}}{2^{n-1}} + 1 = \frac{C_{2^n-2}}{2^{n-2}} + 2 = \frac{C_{2^n-3}}{2^{n-3}} + 3 = ... = \frac{C_{2^0}}{2^0} + n = n.
","['analysis', 'algorithms', 'sorting']"
43,Problem with normal derivative,Problem with normal derivative,,"I have a problem with normal derivative of given function $h(x)=e^{-ar^{2}}-e^{-ar^{2}_{0}}$ and showing that $L(h)>0$ , where $L$ is elliptic operator. $L=\sum_{i,k}a_{ik}\frac{\partial^2u}{\partial x_i \partial x_k}+\sum_{i}b_{i}(x) \frac{\partial u}{\partial x_i} \ (a_{ik},  b_i - \text{continuous})$ I found in the book that the normal derivative of $h$ is greater than $0$ , but I don't know how to show that. I did compute a gradient which is equal $-2ae^{-ar^{2}}\vec{x}$ , so the normal derivative is $-2ae^{ar^{2}}\vec{x} \cdot \vec{n}$ and as I see $\vec{x} \cdot \vec{n}$ should be negative but I don't know why. $L(h)=(\sum_{i,k}4a^2a_{ik}x_ix_k-\sum_{i}2ab_{i} x_i)e^{-ar^{2}}$ $L(h)$ should be greater than $0$ , but why?","I have a problem with normal derivative of given function and showing that , where is elliptic operator. I found in the book that the normal derivative of is greater than , but I don't know how to show that. I did compute a gradient which is equal , so the normal derivative is and as I see should be negative but I don't know why. should be greater than , but why?","h(x)=e^{-ar^{2}}-e^{-ar^{2}_{0}} L(h)>0 L L=\sum_{i,k}a_{ik}\frac{\partial^2u}{\partial x_i \partial x_k}+\sum_{i}b_{i}(x) \frac{\partial u}{\partial x_i} \ (a_{ik},  b_i - \text{continuous}) h 0 -2ae^{-ar^{2}}\vec{x} -2ae^{ar^{2}}\vec{x} \cdot \vec{n} \vec{x} \cdot \vec{n} L(h)=(\sum_{i,k}4a^2a_{ik}x_ix_k-\sum_{i}2ab_{i} x_i)e^{-ar^{2}} L(h) 0","['analysis', 'partial-differential-equations']"
44,Why is it enough to consider limits as $𝛿\to 0$ through $\{𝛿_k\}$ such that $𝛿_{k+1} \ge c𝛿_k$ for some $0 < c < 1$ to find $\dim_B F$?,Why is it enough to consider limits as  through  such that  for some  to find ?,𝛿\to 0 \{𝛿_k\} 𝛿_{k+1} \ge c𝛿_k 0 < c < 1 \dim_B F,"To set the stage, let me recall the definition of the box-counting dimension of a set $F \subset \mathbb R^n$ . The lower and upper box-counting dimensions of a subset $F$ of $ℝ^n$ are given by $$\underline{\dim}_B F = \underline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ $$\overline{\dim}_B F = \overline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ and the box-counting dimension of $F$ by $$\dim_B F = \lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ (if this limit exists), where $N_\delta(F)$ is the smallest number of sets of diameter at most $𝛿$ that cover $F$ . Now, the author says that to find $\dim_B F$ , it is enough to consider limits as $𝛿\to 0$ through $\{𝛿_k\}$ such that $𝛿_{k+1} \ge c𝛿_k$ for some $0 < c < 1$ , in particular $\delta_k = c^k$ . I have trouble seeing why this is true. Following is the reasoning provided by the author: Note that if $\delta_{k+1} \le \delta < \delta_k$ , then with $N_𝛿(F)$ the least number of sets in a $𝛿$ -cover of F, $$\frac{\log N_\delta(F)}{-\log\delta} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_k} = \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log{(\delta_{k+1}/\delta_k)}} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log c}$$ and so $$\overline{\lim_{\delta\to 0}} \frac{\log N_\delta(F)}{-\log\delta} \le \overline{\lim_{k\to\infty}} \frac{\log N_{\delta_k}(F)}{-\log\delta_k}$$ The opposite inequality is trivial; the case of lower limits may be dealt with in the same way. The author says $0 < c < 1$ , so that $\log c < 0$ . I do not understand where the following implication comes from: $$\frac{\log N_\delta(F)}{-\log\delta} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log c} \implies \overline{\lim_{\delta\to 0}} \frac{\log N_\delta(F)}{-\log\delta} \le \overline{\lim_{k\to\infty}} \frac{\log N_{\delta_k}(F)}{-\log\delta_k} $$ For the opposite direction (assuming the implication in (1) is correct), I would do $$\frac{\log N_\delta(F)}{-\log\delta} \ge \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k+1}} = \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k} + \log(\delta_k/\delta_{k+1})} \ge \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k} - \log c}$$ Taking limits (this needs more justification) , $$\overline\lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log\delta} \ge \overline\lim_{k\to\infty} \frac{\log N_{\delta_k}(F)}{-\log\delta_k}$$ which is the required inequality. Combining this with the previous inequality, stated in the question, we get $$\overline\lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log\delta} = \overline\lim_{k\to\infty} \frac{\log N_{\delta_k}(F)}{-\log\delta_k}$$ A similar procedure could be adopted for the case of $\underline\lim_{\delta\to 0}$ and $\underline\lim_{k\to \infty}$ (just take lower limits in place of upper ones, everything else unchanged). Is this right? References. Fractal Geometry by Kenneth Falconer.","To set the stage, let me recall the definition of the box-counting dimension of a set . The lower and upper box-counting dimensions of a subset of are given by and the box-counting dimension of by (if this limit exists), where is the smallest number of sets of diameter at most that cover . Now, the author says that to find , it is enough to consider limits as through such that for some , in particular . I have trouble seeing why this is true. Following is the reasoning provided by the author: Note that if , then with the least number of sets in a -cover of F, and so The opposite inequality is trivial; the case of lower limits may be dealt with in the same way. The author says , so that . I do not understand where the following implication comes from: For the opposite direction (assuming the implication in (1) is correct), I would do Taking limits (this needs more justification) , which is the required inequality. Combining this with the previous inequality, stated in the question, we get A similar procedure could be adopted for the case of and (just take lower limits in place of upper ones, everything else unchanged). Is this right? References. Fractal Geometry by Kenneth Falconer.",F \subset \mathbb R^n F ℝ^n \underline{\dim}_B F = \underline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta} \overline{\dim}_B F = \overline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta} F \dim_B F = \lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta} N_\delta(F) 𝛿 F \dim_B F 𝛿\to 0 \{𝛿_k\} 𝛿_{k+1} \ge c𝛿_k 0 < c < 1 \delta_k = c^k \delta_{k+1} \le \delta < \delta_k N_𝛿(F) 𝛿 \frac{\log N_\delta(F)}{-\log\delta} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_k} = \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log{(\delta_{k+1}/\delta_k)}} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log c} \overline{\lim_{\delta\to 0}} \frac{\log N_\delta(F)}{-\log\delta} \le \overline{\lim_{k\to\infty}} \frac{\log N_{\delta_k}(F)}{-\log\delta_k} 0 < c < 1 \log c < 0 \frac{\log N_\delta(F)}{-\log\delta} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log c} \implies \overline{\lim_{\delta\to 0}} \frac{\log N_\delta(F)}{-\log\delta} \le \overline{\lim_{k\to\infty}} \frac{\log N_{\delta_k}(F)}{-\log\delta_k}  \frac{\log N_\delta(F)}{-\log\delta} \ge \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k+1}} = \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k} + \log(\delta_k/\delta_{k+1})} \ge \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k} - \log c} \overline\lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log\delta} \ge \overline\lim_{k\to\infty} \frac{\log N_{\delta_k}(F)}{-\log\delta_k} \overline\lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log\delta} = \overline\lim_{k\to\infty} \frac{\log N_{\delta_k}(F)}{-\log\delta_k} \underline\lim_{\delta\to 0} \underline\lim_{k\to \infty},"['analysis', 'measure-theory', 'proof-explanation', 'fractals', 'fractal-analysis']"
45,Asymptotic series for the inverse Riemann ζ-function,Asymptotic series for the inverse Riemann ζ-function,,"Consider the positive branch of the Riemann $\zeta$ -function: $\zeta:(1,\infty)\to(1,\infty)$ and its inverse $\zeta^{\small(-1)}:(1,\infty)\to(1,\infty)$ satisfying $\zeta\left(\zeta^{\small(-1)}(x)\right) = x$ for $x>1$ . Here are a few initial terms of an asymptotic expansion of $\zeta^{\small(-1)}$ in powers of $x$ (except for the initial term that is logarithmic): $$\begin{align}\zeta^{\scriptscriptstyle(-1)}(1+x)=-\log_2x+{\small\frac1{\ln2}}\Big(&x^{\log_2\left(\frac32\right)}+x+{\small\log_2\left(\tfrac{\sqrt2}3\right)}\cdot x^{2\log _2\left(\frac32\right)}\\+\,&x^{\log _2\left(\frac52\right)}-{\small\log_2 3}\cdot x^{\log_23}\Big)+\mathcal O\Big(x^{3\log_2\left(\frac32\right)}\Big)\color{gray}{,\,x\to0^+,}\end{align} $$ or, equivalently, $$\begin{align}\zeta^{\scriptscriptstyle(-1)}\left(1+2^{-z}\right)=z+{\small\frac1{\ln2}}\Big(&\left(\tfrac32\right)^{-z}+{\small2^{-z}}+{\small\log_2\left(\tfrac{\sqrt2}3\right)}\cdot \left(\tfrac32\right)^{-2z}\\+\,&\left(\tfrac52\right)^{-z}-{\small\log_2 3}\cdot {\small3^{-z}}\Big)+\mathcal O\Big(\left(\tfrac32\right)^{-3z}\Big)\color{gray}{,\,\,z\to\infty.}\end{align} $$ Can we compute more terms of this series or find a general formula for its terms? Does this series converge?","Consider the positive branch of the Riemann -function: and its inverse satisfying for . Here are a few initial terms of an asymptotic expansion of in powers of (except for the initial term that is logarithmic): or, equivalently, Can we compute more terms of this series or find a general formula for its terms? Does this series converge?","\zeta \zeta:(1,\infty)\to(1,\infty) \zeta^{\small(-1)}:(1,\infty)\to(1,\infty) \zeta\left(\zeta^{\small(-1)}(x)\right) = x x>1 \zeta^{\small(-1)} x \begin{align}\zeta^{\scriptscriptstyle(-1)}(1+x)=-\log_2x+{\small\frac1{\ln2}}\Big(&x^{\log_2\left(\frac32\right)}+x+{\small\log_2\left(\tfrac{\sqrt2}3\right)}\cdot x^{2\log _2\left(\frac32\right)}\\+\,&x^{\log _2\left(\frac52\right)}-{\small\log_2 3}\cdot x^{\log_23}\Big)+\mathcal O\Big(x^{3\log_2\left(\frac32\right)}\Big)\color{gray}{,\,x\to0^+,}\end{align}
 \begin{align}\zeta^{\scriptscriptstyle(-1)}\left(1+2^{-z}\right)=z+{\small\frac1{\ln2}}\Big(&\left(\tfrac32\right)^{-z}+{\small2^{-z}}+{\small\log_2\left(\tfrac{\sqrt2}3\right)}\cdot \left(\tfrac32\right)^{-2z}\\+\,&\left(\tfrac52\right)^{-z}-{\small\log_2 3}\cdot {\small3^{-z}}\Big)+\mathcal O\Big(\left(\tfrac32\right)^{-3z}\Big)\color{gray}{,\,\,z\to\infty.}\end{align}
","['analysis', 'asymptotics', 'riemann-zeta', 'inverse-function', 'conjectures']"
46,"Let $\phi:C^1([a,b])\to \Bbb{R}$ defined by $\phi(f)=\int\limits_a^b L(t,f(t),f'(t))\ dt$ where $L\in C^1(\Bbb{R}^3)$. Find $D\phi(f)(h)$.",Let  defined by  where . Find .,"\phi:C^1([a,b])\to \Bbb{R} \phi(f)=\int\limits_a^b L(t,f(t),f'(t))\ dt L\in C^1(\Bbb{R}^3) D\phi(f)(h)","Prove that $E=C^1([a,b];\Bbb{R})$ is a Banach space with respect to the norm $\lVert f\rVert=\text{sup}|f|+\text{sup}|f'|$ . Let $L\in C^1(\Bbb{R}^3;\Bbb{R})$ be fixed, define $\phi:E\to \Bbb{R}$ by $$\phi(f)=\int\limits_a^b L(t,f(t),f'(t))\ dt$$ Prove that, $$D\phi( f)(h)=\int\limits_a^b (D_2L(t, f(t),f'(t))h(t)+D_3L(t, f(t), f'(t))h'(t))\ dt$$ I have proved the first part of the problem. Let $\{f_n\}$ be a cauchy sequence in $E$ . Let, $\epsilon>0$ then $\exists N\in\Bbb{N}$ such that $\lVert f_n-f_m\rVert <\epsilon/2\ \forall n,m\ge N$ . This implies $\text{sup}|f_n-f_m|<\epsilon/2$ and $\text{sup}|f_n'-f_m'|<\epsilon/2\ \forall n,m\ge N$ . This shows that $\{f_n\}$ converges uniformly to a function $f$ on $[a,b]$ . Again, $\{f_n'\}$ also converges uniformly to a function $g$ on $[a,b]$ and it is continuous as $f_n'$ is continuous for all $n$ . From the property of derivative and uniform convergence we have $f$ is differentiable and $f'=g$ . Hence, $f\in E$ . Now we have $|f_n(x)-f_m(x)|<\epsilon/2$ and $|f_n'(x)-f_m'(x)|<\epsilon/2\ \forall x\in [a,b]\ \forall n,m\ge N$ . Taking $m\to \infty$ we get $|f_n(x)-f(x)|\le\epsilon/2$ and $|f_n'(x)-f'(x)|\le\epsilon/2\ \forall x\in [a,b]\ \forall n\ge N\implies \text{sup}|f_n-f|+\text{sup}|f_n'-f'|\le\epsilon/2\ \forall n\ge N\implies \lVert f_n-f\rVert\le \epsilon\ \forall n\ge N$ . So, $f_n\to f$ in the NLS $(E,\lVert\cdot\rVert)$ . Therefore, $E$ is a banach space. But I have no idea how to differentiate this $\phi$ . Can anyone help me in this regard? Thanks for your help in advance.","Prove that is a Banach space with respect to the norm . Let be fixed, define by Prove that, I have proved the first part of the problem. Let be a cauchy sequence in . Let, then such that . This implies and . This shows that converges uniformly to a function on . Again, also converges uniformly to a function on and it is continuous as is continuous for all . From the property of derivative and uniform convergence we have is differentiable and . Hence, . Now we have and . Taking we get and . So, in the NLS . Therefore, is a banach space. But I have no idea how to differentiate this . Can anyone help me in this regard? Thanks for your help in advance.","E=C^1([a,b];\Bbb{R}) \lVert f\rVert=\text{sup}|f|+\text{sup}|f'| L\in C^1(\Bbb{R}^3;\Bbb{R}) \phi:E\to \Bbb{R} \phi(f)=\int\limits_a^b L(t,f(t),f'(t))\ dt D\phi(
f)(h)=\int\limits_a^b (D_2L(t, f(t),f'(t))h(t)+D_3L(t, f(t), f'(t))h'(t))\ dt \{f_n\} E \epsilon>0 \exists N\in\Bbb{N} \lVert f_n-f_m\rVert <\epsilon/2\ \forall n,m\ge N \text{sup}|f_n-f_m|<\epsilon/2 \text{sup}|f_n'-f_m'|<\epsilon/2\ \forall n,m\ge N \{f_n\} f [a,b] \{f_n'\} g [a,b] f_n' n f f'=g f\in E |f_n(x)-f_m(x)|<\epsilon/2 |f_n'(x)-f_m'(x)|<\epsilon/2\ \forall x\in [a,b]\ \forall n,m\ge N m\to \infty |f_n(x)-f(x)|\le\epsilon/2 |f_n'(x)-f'(x)|\le\epsilon/2\ \forall x\in [a,b]\ \forall n\ge N\implies \text{sup}|f_n-f|+\text{sup}|f_n'-f'|\le\epsilon/2\ \forall n\ge N\implies \lVert f_n-f\rVert\le \epsilon\ \forall n\ge N f_n\to f (E,\lVert\cdot\rVert) E \phi","['calculus', 'integration', 'analysis', 'multivariable-calculus', 'banach-spaces']"
47,$\lim\limits_{n\to \infty}\frac{3^n+6^n-1}{6^n-1}$,,\lim\limits_{n\to \infty}\frac{3^n+6^n-1}{6^n-1},$\lim\limits_{n\to \infty}\dfrac{3^n+6^n-1}{6^n-1} = 1$ I proved the statement above but now the exercise says that I have to find $n_0$ so that $\left\vert x_n-1\right\vert < \frac{1}{50}$ with $(x_n)$ as the given seqeunce above. I wrote the sequence like this: $(x_n)=\frac{3^n}{6^n-1}+1$ and then I looked at $\left\vert \frac{3^n}{6^n-1}+1-1\right\vert < \frac{1}{50}$ which is equivalent to $\frac{\frac{1}{2^n}}{1-\frac{1}{6^n}} < \frac{1}{50}$ . But now I don't know how to go on... Can someone help me?,I proved the statement above but now the exercise says that I have to find so that with as the given seqeunce above. I wrote the sequence like this: and then I looked at which is equivalent to . But now I don't know how to go on... Can someone help me?,\lim\limits_{n\to \infty}\dfrac{3^n+6^n-1}{6^n-1} = 1 n_0 \left\vert x_n-1\right\vert < \frac{1}{50} (x_n) (x_n)=\frac{3^n}{6^n-1}+1 \left\vert \frac{3^n}{6^n-1}+1-1\right\vert < \frac{1}{50} \frac{\frac{1}{2^n}}{1-\frac{1}{6^n}} < \frac{1}{50},"['sequences-and-series', 'analysis']"
48,Prove $\sum_{n=1}^{+\infty}{\left| C_n \right|}<+\infty $,Prove,\sum_{n=1}^{+\infty}{\left| C_n \right|}<+\infty ,"Assume there's a real function $f\in C^{1}[0,\pi]$ . Let $$ C_n=\int_0^{\pi}{f\left( x \right) \cos \left( nx \right) \text{d}x},\quad n=1,2,3,\cdots $$ Prove that, $$\sum_{n=1}^{+\infty}{\left| C_n \right|}<+\infty $$ I don't have any ideas after a long time of thinking. Can anyone help?","Assume there's a real function . Let Prove that, I don't have any ideas after a long time of thinking. Can anyone help?","f\in C^{1}[0,\pi] 
C_n=\int_0^{\pi}{f\left( x \right) \cos \left( nx \right) \text{d}x},\quad n=1,2,3,\cdots
 \sum_{n=1}^{+\infty}{\left| C_n \right|}<+\infty ","['sequences-and-series', 'analysis', 'fourier-analysis']"
49,"$f$ is integrable $\iff$ for every sub-block $B$ we have that the function $f|_{B}$ is integrable, i.e. $\int_{A}f=\sum_{B}\int_{B}f|_{B}$","is integrable  for every sub-block  we have that the function  is integrable, i.e.",f \iff B f|_{B} \int_{A}f=\sum_{B}\int_{B}f|_{B},"QUESTION: Let $f:A \rightarrow \mathbb{R}$ be a limited function and  let $P$ be a partition of the block $A$ ( $A$ is a block in $\mathbb{R}^m$ ). Then $f$ is integrable $\iff$ for every sub-block $B$ we have that the function $f|_{B}$ is integrable and in this case, $$\int_{A}f=\sum_{B}\int_{B}f|_{B}$$ . REMARK: The professor allowed us to use the following concepts: Proposition: Let $P_0$ be an arbitrary partition of the block $A$ . In order to consider the upper and lower integrals of the limited function $f:A \rightarrow \mathbb{R}$ , we just need to consider partition refinements of $P_0$ . That is, we have $$\underline\int_{A} f(x) dx= \underset{P\supset P_0}{sup} s(f; P)$$ and $$\overline\int_{A} f(x) dx= \underset{P\supset P_0}{inf} S(f; P)$$ Theorem: The limited function $f: A \rightarrow \mathbb{R}$ is integrable $\iff$ for every $\epsilon>0$ it is possible to find a partition $P$ of the block $A$ such that $$\displaystyle\sum_{B\in P} \omega_{B}\cdot vol B<\epsilon$$ Where $\omega_{B}$ is the set of the oscillations, i. e., $$\omega_{B}:= sup\{|f(x)-f(y)|; x, y \in B\}$$ MY ATTEMPTY: $(\Longrightarrow)$ Let $f: A \rightarrow \mathbb{R}$ be a limited function and let $P$ be a partition of the block $A$ . Suppose that $f$ is integrable then $\forall \epsilon >0$ it is possible to obtain an partition $P=P_1 \times \cdots \times P_n$ of $A$ such that $\displaystyle\sum_{B\in P} \omega_B \cdot \text{vol}B <\epsilon$ , where $B$ are blocks in $P$ . Once $B$ are sub-blocks of $A$ , let $P_0$ be an partition of $B$ . Therefore for every limited function $f|_{B}$ we just need to consider the refinement partitions of $P_0$ . Indeed, let $B=\displaystyle\Pi_{i=1}^{n}[b_i, c_i] \subset A$ then for every $i=1, \cdots, n$ lets define $Q_i= P_i\cap[b_i, c_i]$ from this we have a new partition $Q = Q_1 \times \cdots \times Q_n$ of $A$ that is a refinement of $P$ and, furthermore, the blocks of $Q$ are contained in $B$ makes a partition $P_0$ of $B$ . Thus $$\underbrace{\displaystyle\sum_{B'\in P_0}\omega_{B'}\cdot \text{vol} B'}_{(I)}\leq\displaystyle\underbrace{\sum_{B\in P}\omega_{B}\cdot \text{vol} B<\epsilon}_{(II)}$$ $(I) \subset (II)$ therefore $f|_{B}$ is intagrable. $(\Longleftarrow)$ We just need to consider $P=P_1 \times \cdots \times P_n$ as a partition of the block $A$ and we also need to consider that this partition is a composition of the block $A$ in sub-blocks like $B=I_1 \times \cdots \times I_n$ where every $I_j$ is an interval of the partition $P_j$ , where every sub-block $B$ is the block of partition $P$ , i.e., $B\in P$ . So, writting $A=\displaystyle\bigcup_{i=1}^{n}B_i$ and remembering that every $f|_{B}$ is integrable. Note that if $P_i$ is a partition of $B_i$ we can consider $Q=\displaystyle\sum_{i=1}^{n}P_i$ as an refinement partition of $P$ thus $f:A \rightarrow\mathbb{R}$ is integrable. Now we just need to show that: $$\int_{A} f \leq \displaystyle\sum_{B \in P} \int_{B} f|_{B}$$ . In $f:A \rightarrow \mathbb{R}$ considering the partition $P$ of the block $A$ we just need to consider refinement partitions of $P$ , let $Q$ be an arbitrary partition of the block $A$ we can consider, for instance $P_0= P+Q$ . It follows from upper integration definition that $$s(f, P)=\displaystyle\sum_{B \in P} m_B(f)\cdot \textbf{vol}B= \displaystyle\sum_{B \in P} m_{B}(f|_{B}) \cdot \textbf{vol} B$$ . Then, for every $B$ we consider $B' \subset B$ , the sub-blocks of $B$ resultants of the refinement of $P$ , and $B=\bigcup B'$ . Therefore, \begin{align*}     \int_{A} f = \displaystyle sup_{P_0\supset P} s(f, P_0)& = sup \left(\displaystyle\sum_{B\in P}m_{B}(f|_{B}) \cdot \textbf{vol} B\right)\\     & = sup \left(\displaystyle\sum_{B\in P}m_{B}(f|_{B}) \displaystyle\sum_{B'\subset B} \textbf{vol} B'\right)\\     & = sup \left(\displaystyle\sum_{B\in P}\displaystyle\sum_{B'\subset B}m_{B}(f|_{B})  \textbf{vol} B'\right)\\     & \leq sup \left(\displaystyle\sum_{B\in P}\displaystyle\sum_{B'\subset B}m_{B'}(f|_{B})  \textbf{vol} B'\right)\\     & = \displaystyle\sum_{B\in P} sup \left(\displaystyle\sum_{B'\subset B}m_{B'}(f|_{B})  \textbf{vol} B'\right)\\     & = \displaystyle\sum_{B\in P} \underline{\int_{B}} f|_{B}\\     & = \displaystyle\sum_{B\in P} \int_{B} f|_{B} \end{align*} Thus, $$\int_{A} f \leq \displaystyle\sum_{B\in P} \int_{B} f|_{B}$$ Similarly, we can show for upper sum, and obtain $$\int_{A} f \geq \displaystyle\sum_{B\in P} \int_{B} f|_{B}$$ And finally, conclude $$\int_{A} f = \displaystyle\sum_{B\in P} \int_{B} f|_{B}$$ MY DOUBT: Would you help me to improve my answer? Specialy in this $(\Longleftarrow)$ way.","QUESTION: Let be a limited function and  let be a partition of the block ( is a block in ). Then is integrable for every sub-block we have that the function is integrable and in this case, . REMARK: The professor allowed us to use the following concepts: Proposition: Let be an arbitrary partition of the block . In order to consider the upper and lower integrals of the limited function , we just need to consider partition refinements of . That is, we have and Theorem: The limited function is integrable for every it is possible to find a partition of the block such that Where is the set of the oscillations, i. e., MY ATTEMPTY: Let be a limited function and let be a partition of the block . Suppose that is integrable then it is possible to obtain an partition of such that , where are blocks in . Once are sub-blocks of , let be an partition of . Therefore for every limited function we just need to consider the refinement partitions of . Indeed, let then for every lets define from this we have a new partition of that is a refinement of and, furthermore, the blocks of are contained in makes a partition of . Thus therefore is intagrable. We just need to consider as a partition of the block and we also need to consider that this partition is a composition of the block in sub-blocks like where every is an interval of the partition , where every sub-block is the block of partition , i.e., . So, writting and remembering that every is integrable. Note that if is a partition of we can consider as an refinement partition of thus is integrable. Now we just need to show that: . In considering the partition of the block we just need to consider refinement partitions of , let be an arbitrary partition of the block we can consider, for instance . It follows from upper integration definition that . Then, for every we consider , the sub-blocks of resultants of the refinement of , and . Therefore, Thus, Similarly, we can show for upper sum, and obtain And finally, conclude MY DOUBT: Would you help me to improve my answer? Specialy in this way.","f:A \rightarrow \mathbb{R} P A A \mathbb{R}^m f \iff B f|_{B} \int_{A}f=\sum_{B}\int_{B}f|_{B} P_0 A f:A \rightarrow \mathbb{R} P_0 \underline\int_{A} f(x) dx= \underset{P\supset P_0}{sup} s(f; P) \overline\int_{A} f(x) dx= \underset{P\supset P_0}{inf} S(f; P) f: A \rightarrow \mathbb{R} \iff \epsilon>0 P A \displaystyle\sum_{B\in P} \omega_{B}\cdot vol B<\epsilon \omega_{B} \omega_{B}:= sup\{|f(x)-f(y)|; x, y \in B\} (\Longrightarrow) f: A \rightarrow \mathbb{R} P A f \forall \epsilon >0 P=P_1 \times \cdots \times P_n A \displaystyle\sum_{B\in P} \omega_B \cdot \text{vol}B <\epsilon B P B A P_0 B f|_{B} P_0 B=\displaystyle\Pi_{i=1}^{n}[b_i, c_i] \subset A i=1, \cdots, n Q_i= P_i\cap[b_i, c_i] Q = Q_1 \times \cdots \times Q_n A P Q B P_0 B \underbrace{\displaystyle\sum_{B'\in P_0}\omega_{B'}\cdot \text{vol} B'}_{(I)}\leq\displaystyle\underbrace{\sum_{B\in P}\omega_{B}\cdot \text{vol} B<\epsilon}_{(II)} (I) \subset (II) f|_{B} (\Longleftarrow) P=P_1 \times \cdots \times P_n A A B=I_1 \times \cdots \times I_n I_j P_j B P B\in P A=\displaystyle\bigcup_{i=1}^{n}B_i f|_{B} P_i B_i Q=\displaystyle\sum_{i=1}^{n}P_i P f:A \rightarrow\mathbb{R} \int_{A} f \leq \displaystyle\sum_{B \in P} \int_{B} f|_{B} f:A \rightarrow \mathbb{R} P A P Q A P_0= P+Q s(f, P)=\displaystyle\sum_{B \in P} m_B(f)\cdot \textbf{vol}B= \displaystyle\sum_{B \in P} m_{B}(f|_{B}) \cdot \textbf{vol} B B B' \subset B B P B=\bigcup B' \begin{align*}
    \int_{A} f = \displaystyle sup_{P_0\supset P} s(f, P_0)& = sup \left(\displaystyle\sum_{B\in P}m_{B}(f|_{B}) \cdot \textbf{vol} B\right)\\
    & = sup \left(\displaystyle\sum_{B\in P}m_{B}(f|_{B}) \displaystyle\sum_{B'\subset B} \textbf{vol} B'\right)\\
    & = sup \left(\displaystyle\sum_{B\in P}\displaystyle\sum_{B'\subset B}m_{B}(f|_{B})  \textbf{vol} B'\right)\\
    & \leq sup \left(\displaystyle\sum_{B\in P}\displaystyle\sum_{B'\subset B}m_{B'}(f|_{B})  \textbf{vol} B'\right)\\
    & = \displaystyle\sum_{B\in P} sup \left(\displaystyle\sum_{B'\subset B}m_{B'}(f|_{B})  \textbf{vol} B'\right)\\
    & = \displaystyle\sum_{B\in P} \underline{\int_{B}} f|_{B}\\
    & = \displaystyle\sum_{B\in P} \int_{B} f|_{B}
\end{align*} \int_{A} f \leq \displaystyle\sum_{B\in P} \int_{B} f|_{B} \int_{A} f \geq \displaystyle\sum_{B\in P} \int_{B} f|_{B} \int_{A} f = \displaystyle\sum_{B\in P} \int_{B} f|_{B} (\Longleftarrow)","['integration', 'analysis', 'definite-integrals', 'riemann-integration']"
50,Strong differentiability of the homeomorphism inverse. Inverse Function Theorem.,Strong differentiability of the homeomorphism inverse. Inverse Function Theorem.,,"I am trying to prove the Inverse Function Theorem in a version different of the classical, for strongly differentiable functions, whose definition is: Definition: A function $f:U \rightarrow \mathbb{R}^n$ , $U$ is a open of $\mathbb{R^m}$ , is said strongly differentiable in $a \in U$ if there exists a linear transformation $T: \mathbb{R}^m \rightarrow \mathbb{R}^n$ such that $$f(x) - f(y) = T \cdot (x-y) + r_a(x,y)|x-y|,$$ for all $x,y \in U$ and such that $\displaystyle \lim_{(x,y) \rightarrow (a,a)} r_a(x,y) = 0$ . The classic version of Theorem proves the differentiability of the  homeomorphism inverse. In this context I need to prove the strong differentiability of the  homeomorphism inverse, ie, the next lemma: Lemma: Let $f:U \rightarrow V$ it is a homeomorphism, where $U$ and $V$ are open of the $\mathbb{R}^m$ . If $f$ is strongly differentiable in $a \in U$ and $f'(a): \mathbb{R}^m \rightarrow \mathbb{R}^m$ is a isomorphism, then $f^{-1}$ is strongly differentiable in $b = f(a)$ . In my reference, the author presents a lemma to prove this result, but I think it is more direct, like the classic version, but I am unable to prove it. What does this concept of ""strongly differentiable""? Was it as if he were going to prove the theorem on a point? Thank you for your help.","I am trying to prove the Inverse Function Theorem in a version different of the classical, for strongly differentiable functions, whose definition is: Definition: A function , is a open of , is said strongly differentiable in if there exists a linear transformation such that for all and such that . The classic version of Theorem proves the differentiability of the  homeomorphism inverse. In this context I need to prove the strong differentiability of the  homeomorphism inverse, ie, the next lemma: Lemma: Let it is a homeomorphism, where and are open of the . If is strongly differentiable in and is a isomorphism, then is strongly differentiable in . In my reference, the author presents a lemma to prove this result, but I think it is more direct, like the classic version, but I am unable to prove it. What does this concept of ""strongly differentiable""? Was it as if he were going to prove the theorem on a point? Thank you for your help.","f:U \rightarrow \mathbb{R}^n U \mathbb{R^m} a \in U T: \mathbb{R}^m \rightarrow \mathbb{R}^n f(x) - f(y) = T \cdot (x-y) + r_a(x,y)|x-y|, x,y \in U \displaystyle \lim_{(x,y) \rightarrow (a,a)} r_a(x,y) = 0 f:U \rightarrow V U V \mathbb{R}^m f a \in U f'(a): \mathbb{R}^m \rightarrow \mathbb{R}^m f^{-1} b = f(a)","['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives', 'inverse-function-theorem']"
51,Prove that these $3$ functions are constant given a relation is satisfied by divisors of them,Prove that these  functions are constant given a relation is satisfied by divisors of them,3,"This particular question was asked in masters exam for which I am preparing and I am not able to solve it . Question : suppose $f,g,h$ are functions from the set of positive real numbers into itself  satisfying $f(x)g(y)=h((x^2 +y^2)^{1/2})$ for all $x,y \in(0,\infty)$ . Then show that the three functions $\frac{f(x)}{g(x)} ,\frac{g(x)}{h(x)}$ and $\frac{h(x)}{f(x)}$ are all constant . Attempt : I first tried by putting $y=0$ only to realize that $0$ is not in domain of $g(y)$ . I am unable to think about any other approach . I am sorry but cant give any other thing in attempt . Kindly tell how should I approach this question .",This particular question was asked in masters exam for which I am preparing and I am not able to solve it . Question : suppose are functions from the set of positive real numbers into itself  satisfying for all . Then show that the three functions and are all constant . Attempt : I first tried by putting only to realize that is not in domain of . I am unable to think about any other approach . I am sorry but cant give any other thing in attempt . Kindly tell how should I approach this question .,"f,g,h f(x)g(y)=h((x^2 +y^2)^{1/2}) x,y \in(0,\infty) \frac{f(x)}{g(x)} ,\frac{g(x)}{h(x)} \frac{h(x)}{f(x)} y=0 0 g(y)",['real-analysis']
52,Show that function distances are preserved,Show that function distances are preserved,,"If for all $x,y \in \mathbb{R}^n$ that satisfies $|x - y| = t$ also satisfies $|f(x) - f(y)|=t$ (for some constant $t \in \mathbb{R}^{+}$ ), show that $|f(x) - f(y)| = |x-y|$ for all values of $x,y \in \mathbb{R}^n$ . This seems very much intuitive to me and that $f(x) = x$ or that the range set is a copy of $\mathbb{R}^n$ . My initial goal was to show if $r \in l(x,y)$ , then $f(r) \in l(f(x),f(y))$ such that $|f(r)-f(z)| = |r - z|$ where $z \in \{x,y\}$ . Showing this is enough to solve the problem. The most intuitive idea, however that seems to work is creating equilateral triangles in some way and make it work. $\color{green}{\text{Edit :}}$ In $\mathbb{R}$ , by taking $f$ as the greatest integer function with a period of $t$ , we get a clear contradiction to the initial statement. The idea doesn't seem to directly work for higher dimensions, albeit the first answer claims so. Can this (or the claim in the original question) be proven for $n > 1$ ?","If for all that satisfies also satisfies (for some constant ), show that for all values of . This seems very much intuitive to me and that or that the range set is a copy of . My initial goal was to show if , then such that where . Showing this is enough to solve the problem. The most intuitive idea, however that seems to work is creating equilateral triangles in some way and make it work. In , by taking as the greatest integer function with a period of , we get a clear contradiction to the initial statement. The idea doesn't seem to directly work for higher dimensions, albeit the first answer claims so. Can this (or the claim in the original question) be proven for ?","x,y \in \mathbb{R}^n |x - y| = t |f(x) - f(y)|=t t \in \mathbb{R}^{+} |f(x) - f(y)| = |x-y| x,y \in \mathbb{R}^n f(x) = x \mathbb{R}^n r \in l(x,y) f(r) \in l(f(x),f(y)) |f(r)-f(z)| = |r - z| z \in \{x,y\} \color{green}{\text{Edit :}} \mathbb{R} f t n > 1","['analysis', 'combinatorial-geometry']"
53,Study of an infinite product,Study of an infinite product,,"During some research, I obtained the following convergent product $$ P_a(x) := \prod_{j = 1}^{\infty} \, \cos\left(\frac{x}{j^{a}}\right) \quad (x \in \mathbb{R}, a > 1).$$ Considering how I got it, I know it's convergent and continuous at $0$ (for any fixed $a$ ), but if I look at $P_a$ now, it doesn't seem so obvious for me. Try : I showed that $P_a \in L^1(\mathbb{R})$ , i.e. it is absolutely integrable on $\mathbb{R}$ . Indeed, by using the linearization of the cosine function and the inequalities $\ln(1-y)\leqslant -y$ (for any $y<1$ ) and $1-\cos\,z \geqslant z^2/2$ (for any real $z$ ), we obtain \begin{eqnarray*} P_a(x)^2  &=& \prod_{j = 1}^{\infty} \left(1-\frac{1-\cos(x\,j^{-a})}{2}\right) \leqslant \prod_{j > |x|^{1/a}} \left(1-\frac{1-\cos(x\,j^{-a})}{2}\right) \leqslant  \exp\left(-C |x|^{1/a} \right), \end{eqnarray*} for some absolute constant $C>0$ . However, I have not been able to use this upper bound to prove continuity at $0$ (via uniform convergence for example, if we can). Question : I wanted to know how to study $P_a$ (e.g. its convergence and continuity at $0$ ), if you think it has an other form ""without product"" (or other nice properties) and finally if anyone has already seen this type of product (in some references/articles), please. Thank you in advance.","During some research, I obtained the following convergent product Considering how I got it, I know it's convergent and continuous at (for any fixed ), but if I look at now, it doesn't seem so obvious for me. Try : I showed that , i.e. it is absolutely integrable on . Indeed, by using the linearization of the cosine function and the inequalities (for any ) and (for any real ), we obtain for some absolute constant . However, I have not been able to use this upper bound to prove continuity at (via uniform convergence for example, if we can). Question : I wanted to know how to study (e.g. its convergence and continuity at ), if you think it has an other form ""without product"" (or other nice properties) and finally if anyone has already seen this type of product (in some references/articles), please. Thank you in advance."," P_a(x) := \prod_{j = 1}^{\infty} \, \cos\left(\frac{x}{j^{a}}\right) \quad (x \in \mathbb{R}, a > 1). 0 a P_a P_a \in L^1(\mathbb{R}) \mathbb{R} \ln(1-y)\leqslant -y y<1 1-\cos\,z \geqslant z^2/2 z \begin{eqnarray*}
P_a(x)^2  &=& \prod_{j = 1}^{\infty} \left(1-\frac{1-\cos(x\,j^{-a})}{2}\right) \leqslant \prod_{j > |x|^{1/a}} \left(1-\frac{1-\cos(x\,j^{-a})}{2}\right) \leqslant  \exp\left(-C |x|^{1/a} \right),
\end{eqnarray*} C>0 0 P_a 0","['real-analysis', 'analysis', 'reference-request', 'continuity', 'infinite-product']"
54,Exercise about the fixed point,Exercise about the fixed point,,I'm studying the fixed point and I found this exercise on my text book.  I've tried to study the function: $f(x)=x^2+1/4$ and find the possible fixed attracting or repelling point. Now I find that the fixed point $x=1/2$ but now to prove if it is a repelling or an attracting fixed point it's a little bit difficult. I've two general definition of this two type of points but I can't find a way to apply it correctly.,I'm studying the fixed point and I found this exercise on my text book.  I've tried to study the function: and find the possible fixed attracting or repelling point. Now I find that the fixed point but now to prove if it is a repelling or an attracting fixed point it's a little bit difficult. I've two general definition of this two type of points but I can't find a way to apply it correctly.,f(x)=x^2+1/4 x=1/2,"['real-analysis', 'analysis']"
55,Integrability of composite of Riemann integrable functions given condition,Integrability of composite of Riemann integrable functions given condition,,"This is a question from a past qualifying exam, which I am studying for. The question has been asked before here , and has an answer, but the answer uses Lebesgue's criterion for Riemann integrability, which is disallowed on the exam. Is there a more elementary way to solve this question? Let $f: [0,1] \to \mathbb{R}$ and $g: [0,1] \to [0,1]$ be two Riemann integrable functions. Assume that $|g(x) - g(y)| \geq \alpha |x-y|$ for any $x,y \in [0,1]$ and some fixed $\alpha \in (0,1)$ . Show that $f \circ g$ is Riemann integrable. Some thoughts have been bounding the intervals in which $f$ has a large oscillation by its integrability, and trying use the condition on $g$ to control the growth of these interval lengths. However, I am unsure how to apply the Riemann integrability of $g$ .","This is a question from a past qualifying exam, which I am studying for. The question has been asked before here , and has an answer, but the answer uses Lebesgue's criterion for Riemann integrability, which is disallowed on the exam. Is there a more elementary way to solve this question? Let and be two Riemann integrable functions. Assume that for any and some fixed . Show that is Riemann integrable. Some thoughts have been bounding the intervals in which has a large oscillation by its integrability, and trying use the condition on to control the growth of these interval lengths. However, I am unsure how to apply the Riemann integrability of .","f: [0,1] \to \mathbb{R} g: [0,1] \to [0,1] |g(x) - g(y)| \geq \alpha |x-y| x,y \in [0,1] \alpha \in (0,1) f \circ g f g g","['real-analysis', 'analysis', 'riemann-integration']"
56,Differentiating an integral depending on a parameter,Differentiating an integral depending on a parameter,,"Consider the following double integral depending on a parameter $x \in \mathbb{R}$ : $$ I(x) := \int_{x}^{\infty} f ( z) e^{-(z-x)} \int_{0}^{\infty} \frac{e^{-(z-x)t}}{10+\ln^2{t}} \ dt \ dz, $$ where $f(z)$ is continuous and vanishes at infinity, meaning that $$ \lim_{ | z | \rightarrow \infty} f ( z ) = 0 $$ It is known that under this condition $I(x)$ is also continuous and vanishes at infinity . I am trying to find out whether $I(x)$ is necessarily differentiable   in $x$ without any additional assumptions on $f$ . If we denote by $$ J(x,z) := \int_{0}^{\infty} \frac{e^{-(z-x)t}}{10+\ln^2{t}} \ dt $$ then a naive ""differentiation"" of $J(x,z)$ w.r.t. $x$ may lead to $$ \frac{d}{dx} J(x,z)  \overset{?}{=} \int_{ 0 }^{ \infty} \frac{te^{-(z-x)t}}{10 + \ln^2 t} \ dt, $$ which does not converge if we set $z=x$ . Can this imply that $I(x)$ is not necessarily differentiable? One could also try a change of variables through $y :=z-x$ and get $$ I( x ) := \int_{0}^{\infty} f( y + x ) e^{-y} \int_{0}^{\infty} \frac{e^{-yt}}{10+\ln^2{t}} \ dt \ dy, $$ and it seems that this would hardly lead anywhere, since we know nothing about the differentiability of $f$ w.r.t. $x$ . Another thing, which may or may not be helpful, is that $$ \int_{0}^{\infty} \int_{0}^{\infty} \frac{e^{-yt}}{10+\ln^2{t}} \ \color\red{dy} \ dt = \frac{ \pi }{ \sqrt{10}}, $$ as computed here by wolframalpha. In summary, knowing that $I(x)$ is continuous and vanishing at infinity, are there any criteria which can be used to show that $I(x)$ is necessarily differentiable without additional assumptions on $f$ , and, if so, how does the derivative look like? Alternatively, is there some $f$ which is continuous and vanishing at infinity, but such that $I(x)$ is not differentiable?","Consider the following double integral depending on a parameter : where is continuous and vanishes at infinity, meaning that It is known that under this condition is also continuous and vanishes at infinity . I am trying to find out whether is necessarily differentiable   in without any additional assumptions on . If we denote by then a naive ""differentiation"" of w.r.t. may lead to which does not converge if we set . Can this imply that is not necessarily differentiable? One could also try a change of variables through and get and it seems that this would hardly lead anywhere, since we know nothing about the differentiability of w.r.t. . Another thing, which may or may not be helpful, is that as computed here by wolframalpha. In summary, knowing that is continuous and vanishing at infinity, are there any criteria which can be used to show that is necessarily differentiable without additional assumptions on , and, if so, how does the derivative look like? Alternatively, is there some which is continuous and vanishing at infinity, but such that is not differentiable?","x \in \mathbb{R} 
I(x) := \int_{x}^{\infty} f ( z) e^{-(z-x)} \int_{0}^{\infty} \frac{e^{-(z-x)t}}{10+\ln^2{t}} \ dt \ dz,
 f(z) 
\lim_{ | z | \rightarrow \infty} f ( z ) = 0
 I(x) I(x) x f 
J(x,z) := \int_{0}^{\infty} \frac{e^{-(z-x)t}}{10+\ln^2{t}} \ dt
 J(x,z) x 
\frac{d}{dx} J(x,z)  \overset{?}{=} \int_{ 0 }^{ \infty} \frac{te^{-(z-x)t}}{10 + \ln^2 t} \ dt,
 z=x I(x) y :=z-x 
I( x ) := \int_{0}^{\infty} f( y + x ) e^{-y} \int_{0}^{\infty} \frac{e^{-yt}}{10+\ln^2{t}} \ dt \ dy,
 f x 
\int_{0}^{\infty} \int_{0}^{\infty} \frac{e^{-yt}}{10+\ln^2{t}} \ \color\red{dy} \ dt = \frac{ \pi }{ \sqrt{10}},
 I(x) I(x) f f I(x)","['real-analysis', 'calculus', 'integration', 'analysis', 'improper-integrals']"
57,Proving the coefficients of the power series are the Taylor coefficients,Proving the coefficients of the power series are the Taylor coefficients,,"Supose we have $f:I \to \mathbb{R}$ a $C^{\infty}$ function and let $I=(a- \delta, a+\delta)$ $\forall x \in I$ we can write the function as the power series $f(x) = \sum_{n = 0}^{\infty} a_n (x - x_0)^n$ I need to show that the coefficients are exactly the Taylor coefficients: $a_n = \frac{f^{(n)} (x_0)}{n!}$ I know the standard approach would be to argue we can derive each term but since we don't know if the series of the derivatives converge uniformly, we would have to show that every power series converge uniformly in a compact inside its convergence interval and also show the series of derivatives has the same convergence radius of the original series. I know I am being overly cautious here, but I would like to know if there is another way. For example, using induction to show what the formula of the n-th derivative of the series would be... Any comment would help!","Supose we have a function and let we can write the function as the power series I need to show that the coefficients are exactly the Taylor coefficients: I know the standard approach would be to argue we can derive each term but since we don't know if the series of the derivatives converge uniformly, we would have to show that every power series converge uniformly in a compact inside its convergence interval and also show the series of derivatives has the same convergence radius of the original series. I know I am being overly cautious here, but I would like to know if there is another way. For example, using induction to show what the formula of the n-th derivative of the series would be... Any comment would help!","f:I \to \mathbb{R} C^{\infty} I=(a- \delta, a+\delta) \forall x \in I f(x) = \sum_{n = 0}^{\infty} a_n (x - x_0)^n a_n = \frac{f^{(n)} (x_0)}{n!}","['real-analysis', 'analysis', 'power-series', 'taylor-expansion']"
58,"Baby Rudin Chapter 6, Problem 15 : Strict inequality","Baby Rudin Chapter 6, Problem 15 : Strict inequality",,"Problem 15 in Chapter 6 of Principles of Mathematical Analysis by Walter Rudin: Suppose $f$ is real, continuously differentiable on $[a,b]$ , $f(a)=f(b)=0$ , and $\int_a^b f^2(x)dx = 1$ . Prove that $\int_a^b xf(x)f'(x)dx = -1/2$ $\int_a^b [f'(x)]^2dx \cdot \int_a^b x^2f^2(x)dx > 1/4$ I am able to prove the first part using integration by parts and the second using Cauchy-Schwarz. To get a strict inequality, I assumed that there was equality, in which case $xf(x) = \lambda f'(x)$ for all $x \in [a,b]$ for some constant $\lambda$ . I am unable to get a contradiction with this (note that Rudin has not introduced the exponential and logarithmic functions and so I don't want to integrate both sides directly).","Problem 15 in Chapter 6 of Principles of Mathematical Analysis by Walter Rudin: Suppose is real, continuously differentiable on , , and . Prove that I am able to prove the first part using integration by parts and the second using Cauchy-Schwarz. To get a strict inequality, I assumed that there was equality, in which case for all for some constant . I am unable to get a contradiction with this (note that Rudin has not introduced the exponential and logarithmic functions and so I don't want to integrate both sides directly).","f [a,b] f(a)=f(b)=0 \int_a^b f^2(x)dx = 1 \int_a^b xf(x)f'(x)dx = -1/2 \int_a^b [f'(x)]^2dx \cdot \int_a^b x^2f^2(x)dx > 1/4 xf(x) = \lambda f'(x) x \in [a,b] \lambda","['real-analysis', 'integration']"
59,"f is a differentiable function with $f(0)>0$. f has only one fixed point on [0,1] Prove that $f′(x_0)<1$","f is a differentiable function with . f has only one fixed point on [0,1] Prove that",f(0)>0 f′(x_0)<1,"Question: Let $f:[0,1]\rightarrow [0,1]$ be a differentiable function with $f(0) > 0$ . Asuming $x_0\in(0,1)$ is the only fixed point of $f$ , prove that $f'(x_0) < 1$ My Attempt: Let $g(x) := f(x)-x$ for $x\in[0,1]$ By given condition $g(0) = f(0)-0 > 0$ As $f$ has unique fixed point on [0,1] so $g$ will have only one solution on [0,1]. So if $g(0)>0$ then $\forall x<x_0,\; g(x)>0\; and \; \forall x>x_0,\; g(x)<0$ Now $g'(x)=f'(x)-1$ $f$ is differentiable on [0,1] so is $g$ . Consider Right-hand derivative of $g$ at $x_0$ $A=\lim_{h \rightarrow 0}\frac{g(x_0+h)-g(x_0)}{h}$ for $h>0$ Using fact that $g(x_0)=0\;and\;\forall x>x_0,\; g(x)<0$ we have $A<0$ So we can say that $g'(x_0)<0$ Therefore $f'(x_0)<1$ I want to know if my proof is correct or not. Is there any simple proof for this question.","Question: Let be a differentiable function with . Asuming is the only fixed point of , prove that My Attempt: Let for By given condition As has unique fixed point on [0,1] so will have only one solution on [0,1]. So if then Now is differentiable on [0,1] so is . Consider Right-hand derivative of at for Using fact that we have So we can say that Therefore I want to know if my proof is correct or not. Is there any simple proof for this question.","f:[0,1]\rightarrow [0,1] f(0) > 0 x_0\in(0,1) f f'(x_0) < 1 g(x) := f(x)-x x\in[0,1] g(0) = f(0)-0 > 0 f g g(0)>0 \forall x<x_0,\; g(x)>0\; and \; \forall x>x_0,\; g(x)<0 g'(x)=f'(x)-1 f g g x_0 A=\lim_{h \rightarrow 0}\frac{g(x_0+h)-g(x_0)}{h} h>0 g(x_0)=0\;and\;\forall x>x_0,\; g(x)<0 A<0 g'(x_0)<0 f'(x_0)<1","['real-analysis', 'analysis', 'proof-verification', 'derivatives']"
60,A Problem from Past Entrance Exams into Math Master's Program in Taiwan,A Problem from Past Entrance Exams into Math Master's Program in Taiwan,,"Let $f$ and $g$ be $\mathbb{R}$ -valued $C^{\infty}$ functions on $\mathbb{R}^2$ and let $S=\{(x,y)\in\mathbb{R}^2|f(x,y)=0\}$ . Suppose that at some point $p=(a,b)\in S$ we have $\frac{\partial f}{\partial x}(p)=-1$ , $\frac{\partial f}{\partial y}(p)=2$ , $\frac{\partial g}{\partial x}(p)=3$ , $\frac{\partial g}{\partial y}(p)=-6$ , $\begin{bmatrix} \frac{\partial^2 f}{\partial x^2}(p) & \frac{\partial^2 f}{\partial x\partial y}(p) \\ \frac{\partial^2 f}{\partial y\partial x}(p) & \frac{\partial^2 f}{\partial y^2}(p) \end{bmatrix} = \begin{bmatrix} 1 & 3 \\ 3 & 0 \end{bmatrix}$ , and $\begin{bmatrix} \frac{\partial^2 g}{\partial x^2}(p) & \frac{\partial^2 g}{\partial x\partial y}(p) \\ \frac{\partial^2 g}{\partial y\partial x}(p) & \frac{\partial^2 g}{\partial y^2}(p) \end{bmatrix} = \begin{bmatrix} 3 & -1 \\ -1 & 2 \end{bmatrix}$ . Show that there exists $R>0$ such that $g(p)<g(q)$ for $q\in S\cap\{(x,y)\in\mathbb{R}^2|(x-a)^2+(y-b)^2<R^2\}$ . I am very very sorry that I keep haven't figured out it is because what concepts/topics/applications/skills that I am unfamiliar with so that I fail to have an idea how to solve this problem from entrance exams into math Master's program in Taiwan. I really need someone can guess what I lack of and point out the direction for me to solve this problem. Thanks eveyone in advance for every possible help!!! P.S. This problem already had appeared in the past exams in year 2017. It is already a past/old problem now. An past/old exam file can be downloaded from: http://www.math.ntu.edu.tw/sites/default/files/imce/documents/exams/M_aca_107.pdf","Let and be -valued functions on and let . Suppose that at some point we have , , , , , and . Show that there exists such that for . I am very very sorry that I keep haven't figured out it is because what concepts/topics/applications/skills that I am unfamiliar with so that I fail to have an idea how to solve this problem from entrance exams into math Master's program in Taiwan. I really need someone can guess what I lack of and point out the direction for me to solve this problem. Thanks eveyone in advance for every possible help!!! P.S. This problem already had appeared in the past exams in year 2017. It is already a past/old problem now. An past/old exam file can be downloaded from: http://www.math.ntu.edu.tw/sites/default/files/imce/documents/exams/M_aca_107.pdf","f g \mathbb{R} C^{\infty} \mathbb{R}^2 S=\{(x,y)\in\mathbb{R}^2|f(x,y)=0\} p=(a,b)\in S \frac{\partial f}{\partial x}(p)=-1 \frac{\partial f}{\partial y}(p)=2 \frac{\partial g}{\partial x}(p)=3 \frac{\partial g}{\partial y}(p)=-6 \begin{bmatrix} \frac{\partial^2 f}{\partial x^2}(p) & \frac{\partial^2 f}{\partial x\partial y}(p) \\ \frac{\partial^2 f}{\partial y\partial x}(p) & \frac{\partial^2 f}{\partial y^2}(p)
\end{bmatrix} = \begin{bmatrix} 1 & 3 \\ 3 & 0 \end{bmatrix} \begin{bmatrix} \frac{\partial^2 g}{\partial x^2}(p) & \frac{\partial^2 g}{\partial x\partial y}(p) \\ \frac{\partial^2 g}{\partial y\partial x}(p) & \frac{\partial^2 g}{\partial y^2}(p)
\end{bmatrix} = \begin{bmatrix} 3 & -1 \\ -1 & 2 \end{bmatrix} R>0 g(p)<g(q) q\in S\cap\{(x,y)\in\mathbb{R}^2|(x-a)^2+(y-b)^2<R^2\}","['calculus', 'analysis']"
61,Zeros of the $n$-th derivative,Zeros of the -th derivative,n,"Suppose $f \in C^{\infty}(-1, 1)$ and $\sup_{(-1, 1)}|f(x)| \leq 1$ . I need to prove that for every $n \in \mathbb{N}$ there is some $\alpha_n \in \mathbb{R}$ such that if $|f'(0)| \geq \alpha_n$ then there is at least $n-1$ solutions to $f^{(n)}(x) = 0$ on $(-1, 1)$ . Now this obviously needs some kind of induction. It is easy to prove this for $n=2$ (just pick some $A > 1$ and suppose $|f'(0)| \geq A$ . Then if $|f'(x)|$ is $\geq A$ on all the $(-1, 1)$ , then $|f(-1 + \epsilon) - f(1 - \epsilon)| > 2$ for some $\epsilon$ by the mean value theorem, which contradicts the supremum assumption, so there is points left and right to 0, where $|f'(0)|$ is less than $A$ , which gives at least two points of different signs for $f''(x)$ , and that, since $f''(x)$ is continuous, gives a zero somewhere inbetween). Now if by induction we have $n-2$ zeros for $f^{n-1}(x)$ , it is easy to prove the existence of at least $n-3$ zeros for $f^{n}(x)$ by the same mean value theorem. But how do I find 2 more by simply increasing $|f'(0)|$ ?","Suppose and . I need to prove that for every there is some such that if then there is at least solutions to on . Now this obviously needs some kind of induction. It is easy to prove this for (just pick some and suppose . Then if is on all the , then for some by the mean value theorem, which contradicts the supremum assumption, so there is points left and right to 0, where is less than , which gives at least two points of different signs for , and that, since is continuous, gives a zero somewhere inbetween). Now if by induction we have zeros for , it is easy to prove the existence of at least zeros for by the same mean value theorem. But how do I find 2 more by simply increasing ?","f \in C^{\infty}(-1, 1) \sup_{(-1, 1)}|f(x)| \leq 1 n \in \mathbb{N} \alpha_n \in \mathbb{R} |f'(0)| \geq \alpha_n n-1 f^{(n)}(x) = 0 (-1, 1) n=2 A > 1 |f'(0)| \geq A |f'(x)| \geq A (-1, 1) |f(-1 + \epsilon) - f(1 - \epsilon)| > 2 \epsilon |f'(0)| A f''(x) f''(x) n-2 f^{n-1}(x) n-3 f^{n}(x) |f'(0)|","['analysis', 'derivatives']"
62,Existence of a particular inverse transformation,Existence of a particular inverse transformation,,"Let $h : \mathbb{R}^D \rightarrow \mathbb{R}^d$ , where $d < D$ , be a differentiable function. I would like to find minimal conditions under which there exists a differentiable function $g : \mathbb{R}^{D} \rightarrow \mathbb{R}^{D-d}$ such that the function $f : \mathbb{R}^D \rightarrow \mathbb{R}^D$ defined by $f(x)=(h(x)^\top, g(x)^\top)^\top$ is invertible. If possible, I would also like to obtain a construction of this $g$ function. I hypothesize that the following conditions might be enough, but I am not sure: - $h$ is surjective. - $h$ cannot have the same value on a set with non-zero measure, that is, for every $y \in \mathbb{R}^{d}$ , the set $h^{-1}(\{y\})=\{x\in\mathbb{R}^D : h(x)=y\}$ has Lebesgue measure 0. The first condition is clearly necessary, and the reason why I believe that the second condition might be enough is the following: In order for $f$ to be invertible, $f^{-1}(\{z\})$ has to be a singleton for every $z \in \mathbb{R}^D$ . If $g$ was such that $g(x_1)\neq g(x_2)$ for every $x_1$ and $x_2$ such that $h(x_1)=h(x_2)$ , that would ensure that $f^{-1}(\{z\})$ is indeed a singleton for every $z \in \mathbb{R}^D$ . My intuition is that the second condition might ensure that such a $g$ function actually exists. Furthermore, if such a $g$ exists that also makes $f$ surjective, the result would follow. Any help would be very appreciated, either in proving my above conjecture, disproving it, or providing non trivial assumptions about $h$ that would result in the existence of $g$ . Thank you very much!","Let , where , be a differentiable function. I would like to find minimal conditions under which there exists a differentiable function such that the function defined by is invertible. If possible, I would also like to obtain a construction of this function. I hypothesize that the following conditions might be enough, but I am not sure: - is surjective. - cannot have the same value on a set with non-zero measure, that is, for every , the set has Lebesgue measure 0. The first condition is clearly necessary, and the reason why I believe that the second condition might be enough is the following: In order for to be invertible, has to be a singleton for every . If was such that for every and such that , that would ensure that is indeed a singleton for every . My intuition is that the second condition might ensure that such a function actually exists. Furthermore, if such a exists that also makes surjective, the result would follow. Any help would be very appreciated, either in proving my above conjecture, disproving it, or providing non trivial assumptions about that would result in the existence of . Thank you very much!","h : \mathbb{R}^D \rightarrow \mathbb{R}^d d < D g : \mathbb{R}^{D} \rightarrow \mathbb{R}^{D-d} f : \mathbb{R}^D \rightarrow \mathbb{R}^D f(x)=(h(x)^\top, g(x)^\top)^\top g h h y \in \mathbb{R}^{d} h^{-1}(\{y\})=\{x\in\mathbb{R}^D : h(x)=y\} f f^{-1}(\{z\}) z \in \mathbb{R}^D g g(x_1)\neq g(x_2) x_1 x_2 h(x_1)=h(x_2) f^{-1}(\{z\}) z \in \mathbb{R}^D g g f h g","['analysis', 'differential-topology', 'lebesgue-measure', 'inverse-function']"
63,Equivalence condition of Absolute Continuity,Equivalence condition of Absolute Continuity,,"Let me restate my point. The intuition behind this construction is straightforward. Let $f$ be a continuous increasing function. Take out pieces of the graph of $f$ which is correspond to a collection of finite disjoint subintervals, then glue them together to make a new function $g$ . If we claim every $g$ must be continuous, then by the definition of continuity $\forall \delta\exists\epsilon$ such that “total length of the intervals $<\epsilon$ ” implies “total variation of $f$ on these intervals< $\delta$ ” which means $f$ is absolute continuous. This is not so sensible, so I hypotheze that $g$ is not necessarily continuous even if $f$ is. $f:I\to\mathbb R$ is continuous. Take finite number of subintervals $[x_1,y_1]\cup [x_2,y_2],...,\cup[x_n,y_n]=\mathcal K\subseteq I$ , where $\sum_k\mu([x_k,y_k])=K$ , i.e. $|\mathcal K|=K$ . Let's consider the the part of graph of $f$ constrained in $\mathcal K$ . Rigorous definition follows. Define function $g:[x_1,x_1+K]\to\mathbb R$ such that, if $x\in[x_1,y_1],$ then $g(x)=f(x)$ , if $x\in[y_1,y_1+y_2-x_2]$ , then $g(x)=f(x-y_1+x_2)+c_1$ , where $c_2=f(y_1)-f(x_2)$ . We thus define $x_1'=x_1$ , $y_1'=y_1$ , $x_2'=y_1$ , $y_2'=x_2'+y_2-x_2$ , and, recursively, $x_k'=y_{k-1}', y_k'=x_k'+y_k-x_k$ . If $(x+y_k')\in[y_k',y_k'+y_{k+1}-x_{k+1}]$ , then $g(x+y_k')=f(x+y_k)+c_k$ , and $c_k=g(y_k')-f(y_k)$ . We call $g$ a clipping of $f$ at $\mathcal K$ : $g=clip(f,\mathcal K)$ . Intuitively, $g(x)$ is exactly equivalent to $f(x)$ on each interval up to the equivalent class of affine transformation. Consider the following condition: Condition 1: $\forall\mathcal K\subseteq I$ , $g=clip(f,\mathcal K)$ is continuous. Is  this condition a necessary and sufficient condition for the absolute continuity of $f$ on $I$ ?  Can it be extended to multidimensional functions? Is the morphism ""clipping"" well studied? This is showing that $g$ may not continuous even if $f$ is continuous. $g$ is not continuous at $x_1$ means: $\exists\delta\forall\epsilon\exists y$ such that $|x_1-y|<\epsilon$ but $|f(x_1)-f(y)|=\delta$ Let $f$ the Cantor function. Let $n$ be a natural number. At the $n$ -th stage of the construction of the Cantor set, a disjoint collection ${[x_k, y_k]}_{1<k<2^n} $ of $2^n$ subintervals of $I=[0, 1]$ have been constructed that cover the Cantor set, each of which has length $(1/3)^n$ . The Cantor-Lebesgue function is constant on each of the intervals that comprise the complement in $[0, 1]$ of this collection of intervals. We have $\sum_{k\leq 2^n}(y_k-x_k)=K<(2/3)^n$ while $\sum_{k\leq 2^n}(f(y_k)-f(x_k))=1$ $\forall \epsilon>0$ $\exists n>0$ such that $K<\epsilon$ . By definition, $K=y_k'-x_k$ , and $\sum_{k\leq 2^n}(f(y_k)-f(x_k))=g(y'_k)-g(x_k)$ . That is : $\exists\delta=1\forall\epsilon\exists y=y_k'$ such that $|x_1-y'_k|<\epsilon$ but $|g(x_1)-g(y_k')|=\delta$ . $g$ is not continuous. It is well known that, in the definition of absolute continuity, the word ""finite"" can be replaced by ""countably infinite"": A function $f: I \to \mathbb{R}$ is absolutely continuous on an interval $I$ if for every $\epsilon > 0$ there is a $\delta > 0$ such that whenever a countable sequence of pairwise disjoint sub-intervals $(x_k, y_k)$ of $E$ satisfies $$ \sum_{k} |y_{k} - x_{k}| < \delta$$ then $$\sum_{k} |f(y_{k}) - f(x_{k})| < \epsilon$$ We know that the square wave can be written as the infinite sum of forms of sine functions: $\lim_{n\to\infty}\sum_{i=1}^n h_i(x)$ . Each $h_i$ is continuous but the limit is not. Let $H_n=\sum_{i=1}^n h_i(x)$ . $H_n$ should be continuous by induction. If $H_n$ is indeed continuous at $x=0$ then $\forall \delta\exists\epsilon\forall y\in\mathbb R$ such that $|y-x|<\epsilon$ implies $|f(y)-f(x)|<\delta$ . However, $\forall \epsilon>0$ $\exists n>0$ such that $|y-x|<\epsilon$ and $f(y)-f(0)=1$ , contradition! To Ramiro: Let assume $f$ is increasing. Your answer seems imply that, if $f$ is continuous, then $g$ is continuous, then $\forall \delta\exists\epsilon\forall y_n'$ such that $|y_n'-x_1|<\epsilon$ implies $|g(y_n')-g(x_1)|<\delta$ ; we know $K=\sum_{i\leq n} |y_i-x_i|,$ and $ \sum_{i\leq n} |f(y_i)-f(x_i)|=|g(y_n')-g(x_1)|$ , SO: Take any one collection of disjoint interval $\mathcal K$ such that $\sum_{i\leq n} |y_i-x_i|<\epsilon$ , this implies $K<\epsilon$ , which implies $|g_\mathcal K(y_n')-g_\mathcal K(x_1)|<\delta$ , which implies $\sum_{i\leq n} |f(y_i)-f(x_i)|<\delta$ . Then $f$ is absolute continuous! I am not sure that a single $g$ is required.","Let me restate my point. The intuition behind this construction is straightforward. Let be a continuous increasing function. Take out pieces of the graph of which is correspond to a collection of finite disjoint subintervals, then glue them together to make a new function . If we claim every must be continuous, then by the definition of continuity such that “total length of the intervals ” implies “total variation of on these intervals< ” which means is absolute continuous. This is not so sensible, so I hypotheze that is not necessarily continuous even if is. is continuous. Take finite number of subintervals , where , i.e. . Let's consider the the part of graph of constrained in . Rigorous definition follows. Define function such that, if then , if , then , where . We thus define , , , , and, recursively, . If , then , and . We call a clipping of at : . Intuitively, is exactly equivalent to on each interval up to the equivalent class of affine transformation. Consider the following condition: Condition 1: , is continuous. Is  this condition a necessary and sufficient condition for the absolute continuity of on ?  Can it be extended to multidimensional functions? Is the morphism ""clipping"" well studied? This is showing that may not continuous even if is continuous. is not continuous at means: such that but Let the Cantor function. Let be a natural number. At the -th stage of the construction of the Cantor set, a disjoint collection of subintervals of have been constructed that cover the Cantor set, each of which has length . The Cantor-Lebesgue function is constant on each of the intervals that comprise the complement in of this collection of intervals. We have while such that . By definition, , and . That is : such that but . is not continuous. It is well known that, in the definition of absolute continuity, the word ""finite"" can be replaced by ""countably infinite"": A function is absolutely continuous on an interval if for every there is a such that whenever a countable sequence of pairwise disjoint sub-intervals of satisfies then We know that the square wave can be written as the infinite sum of forms of sine functions: . Each is continuous but the limit is not. Let . should be continuous by induction. If is indeed continuous at then such that implies . However, such that and , contradition! To Ramiro: Let assume is increasing. Your answer seems imply that, if is continuous, then is continuous, then such that implies ; we know and , SO: Take any one collection of disjoint interval such that , this implies , which implies , which implies . Then is absolute continuous! I am not sure that a single is required.","f f g g \forall \delta\exists\epsilon <\epsilon f \delta f g f f:I\to\mathbb R [x_1,y_1]\cup [x_2,y_2],...,\cup[x_n,y_n]=\mathcal K\subseteq I \sum_k\mu([x_k,y_k])=K |\mathcal K|=K f \mathcal K g:[x_1,x_1+K]\to\mathbb R x\in[x_1,y_1], g(x)=f(x) x\in[y_1,y_1+y_2-x_2] g(x)=f(x-y_1+x_2)+c_1 c_2=f(y_1)-f(x_2) x_1'=x_1 y_1'=y_1 x_2'=y_1 y_2'=x_2'+y_2-x_2 x_k'=y_{k-1}', y_k'=x_k'+y_k-x_k (x+y_k')\in[y_k',y_k'+y_{k+1}-x_{k+1}] g(x+y_k')=f(x+y_k)+c_k c_k=g(y_k')-f(y_k) g f \mathcal K g=clip(f,\mathcal K) g(x) f(x) \forall\mathcal K\subseteq I g=clip(f,\mathcal K) f I g f g x_1 \exists\delta\forall\epsilon\exists y |x_1-y|<\epsilon |f(x_1)-f(y)|=\delta f n n {[x_k, y_k]}_{1<k<2^n}  2^n I=[0, 1] (1/3)^n [0, 1] \sum_{k\leq 2^n}(y_k-x_k)=K<(2/3)^n \sum_{k\leq 2^n}(f(y_k)-f(x_k))=1 \forall \epsilon>0 \exists n>0 K<\epsilon K=y_k'-x_k \sum_{k\leq 2^n}(f(y_k)-f(x_k))=g(y'_k)-g(x_k) \exists\delta=1\forall\epsilon\exists y=y_k' |x_1-y'_k|<\epsilon |g(x_1)-g(y_k')|=\delta g f: I \to \mathbb{R} I \epsilon > 0 \delta > 0 (x_k, y_k) E  \sum_{k} |y_{k} - x_{k}| < \delta \sum_{k} |f(y_{k}) - f(x_{k})| < \epsilon \lim_{n\to\infty}\sum_{i=1}^n h_i(x) h_i H_n=\sum_{i=1}^n h_i(x) H_n H_n x=0 \forall \delta\exists\epsilon\forall y\in\mathbb R |y-x|<\epsilon |f(y)-f(x)|<\delta \forall \epsilon>0 \exists n>0 |y-x|<\epsilon f(y)-f(0)=1 f f g \forall \delta\exists\epsilon\forall y_n' |y_n'-x_1|<\epsilon |g(y_n')-g(x_1)|<\delta K=\sum_{i\leq n} |y_i-x_i|,  \sum_{i\leq n} |f(y_i)-f(x_i)|=|g(y_n')-g(x_1)| \mathcal K \sum_{i\leq n} |y_i-x_i|<\epsilon K<\epsilon |g_\mathcal K(y_n')-g_\mathcal K(x_1)|<\delta \sum_{i\leq n} |f(y_i)-f(x_i)|<\delta f g","['real-analysis', 'calculus', 'analysis', 'continuity', 'absolute-continuity']"
64,increasing sequence of functions modulo fixed finite error,increasing sequence of functions modulo fixed finite error,,"Here we consider only functions from $\mathbb N$ to itself.  Say $f <_n g$ if $\{ i : f(i) \geq g(i) \}$ has size at most $n$ . Is it possible to have a fixed integer $k$ , a $<_k$ -increasing (linearly ordered) sequence $\{ f_n : n \in \mathbb N \}$ and a function $g$ such that $g >_k f_n$ for all $n$ ?","Here we consider only functions from to itself.  Say if has size at most . Is it possible to have a fixed integer , a -increasing (linearly ordered) sequence and a function such that for all ?",\mathbb N f <_n g \{ i : f(i) \geq g(i) \} n k <_k \{ f_n : n \in \mathbb N \} g g >_k f_n n,"['analysis', 'functions', 'set-theory']"
65,Question about Proof of Merten's theorem (Cauchy-Product formula),Question about Proof of Merten's theorem (Cauchy-Product formula),,"I have a question about the proof on the german wikepedia page: The proof is stated as follow: Let $A= \sum_{k=0}^{\infty}a_k$ and $B=\sum_{k=0}^{\infty}b_k$ , if at least one of them is absolutely convergent, then their Cauchy-Product converges to $AB$ . Definition of the Cauchy-Product: $C=\sum_{k=0}^{\infty}c_k,c_k=\sum_{j=0}^{k}a_jb_{k-j}$ Without loss of generality let A be the absolutely convergent series and $S_n=\sum_{k=0}^{n}c_k$ 1: $AB=(A-A_n)B+\sum_{k=0}^{n}a_kB$ 2: $S_n=\sum_{k=0}^{n}a_kB_ {n-k}$ 1-2= $AB-S_n=(A-A_n)B+\sum_{k=0}^{n}a_k(B-B_{n-k})$ $(A-A_n)B \rightarrow 0$ and with $N:=[\frac{n}{2}]$ the other series can be splitted into two parts with: $\sum_{k=0}^{N}a_k(B-B_{n-k})+\sum_{k=N+1}^{n}a_k(B-B_{n-k})$ Then $|\sum_{k=0}^{N}a_k(B-B_{n-k})|\leq \sum_{k=0}^{N}|a_k(B-B_{n-k})|=\sum_{k=0}^{N}|a_k||(B-B_{n-k})|\leq\max\limits_{N \leq k \leq n}|B-B_k|\sum_{k=0}^{N}|a_k|\rightarrow 0$ Because the last expression of the above inequalities is a product with a zero-convergent sequence with a bounded sequence. Because the zero-convergent sequence $(B-B_k)$ is bounded there is a $C > 0$ with $|B-B_k|<C\forall k \in \mathbb{N}$ Hence $|\sum_{k=N+1}{n}a_k(B-B_{n-k})|\leq \sum_{k=N+1}{n}|a_k||(B-B_{n-k})|\leq C\sum_{k=N+1}{n}|a_k|\rightarrow 0 \square$ I don't understand why the sum is splitted in two parts, can also somebody tell me what's with the $max$ estimate. Thank you for your time, I would appreciate your help very much.","I have a question about the proof on the german wikepedia page: The proof is stated as follow: Let and , if at least one of them is absolutely convergent, then their Cauchy-Product converges to . Definition of the Cauchy-Product: Without loss of generality let A be the absolutely convergent series and 1: 2: 1-2= and with the other series can be splitted into two parts with: Then Because the last expression of the above inequalities is a product with a zero-convergent sequence with a bounded sequence. Because the zero-convergent sequence is bounded there is a with Hence I don't understand why the sum is splitted in two parts, can also somebody tell me what's with the estimate. Thank you for your time, I would appreciate your help very much.","A= \sum_{k=0}^{\infty}a_k B=\sum_{k=0}^{\infty}b_k AB C=\sum_{k=0}^{\infty}c_k,c_k=\sum_{j=0}^{k}a_jb_{k-j} S_n=\sum_{k=0}^{n}c_k AB=(A-A_n)B+\sum_{k=0}^{n}a_kB S_n=\sum_{k=0}^{n}a_kB_ {n-k} AB-S_n=(A-A_n)B+\sum_{k=0}^{n}a_k(B-B_{n-k}) (A-A_n)B \rightarrow 0 N:=[\frac{n}{2}] \sum_{k=0}^{N}a_k(B-B_{n-k})+\sum_{k=N+1}^{n}a_k(B-B_{n-k}) |\sum_{k=0}^{N}a_k(B-B_{n-k})|\leq \sum_{k=0}^{N}|a_k(B-B_{n-k})|=\sum_{k=0}^{N}|a_k||(B-B_{n-k})|\leq\max\limits_{N \leq k \leq n}|B-B_k|\sum_{k=0}^{N}|a_k|\rightarrow 0 (B-B_k) C > 0 |B-B_k|<C\forall k \in \mathbb{N} |\sum_{k=N+1}{n}a_k(B-B_{n-k})|\leq \sum_{k=N+1}{n}|a_k||(B-B_{n-k})|\leq C\sum_{k=N+1}{n}|a_k|\rightarrow 0 \square max","['sequences-and-series', 'analysis']"
66,How to show $a$ and $b$ are different??,How to show  and  are different??,a b,"We have a continuously differenciable function $f:[0,1]\rightarrow[0,1]$ such that $f(0)=0, f(1)=1$ . We need to show that there exists different real numbers $a,b\in(0,1)$ such that $f'(a)f'(b)=1.$ I used the mean value theorem as follows: There exists $\alpha\in(0,1)$ such that $(f\circ f)'(\alpha)=1$ , that is to say, $f'(f(\alpha))f'(\alpha)=1$ . Let $a=f(\alpha)$ and $b=\alpha$ . Then, $f'(a)f'(b)=1$ . But I didn't use the continuity of the derivative, and a don't know how to show that $a\neq b$ . Thanks, any help will be appreciated.","We have a continuously differenciable function such that . We need to show that there exists different real numbers such that I used the mean value theorem as follows: There exists such that , that is to say, . Let and . Then, . But I didn't use the continuity of the derivative, and a don't know how to show that . Thanks, any help will be appreciated.","f:[0,1]\rightarrow[0,1] f(0)=0, f(1)=1 a,b\in(0,1) f'(a)f'(b)=1. \alpha\in(0,1) (f\circ f)'(\alpha)=1 f'(f(\alpha))f'(\alpha)=1 a=f(\alpha) b=\alpha f'(a)f'(b)=1 a\neq b","['calculus', 'analysis']"
67,"In a 7x7 grid, what is the number in the bottom right corner?","In a 7x7 grid, what is the number in the bottom right corner?",,"My math teacher have presented me with a problem, that in my opinion is quite hard. It goes. ""You have a 7x7 grid. Some of the spaces in the grid are already filled with numbers. You have to insert numbers in the remaining spaces, so that the sum in every 3x3 grid inside the 7x7, is 2019. What number should be in the bottom right corner?"" Here is the grid. I know the answer is 7, but I have no idea of how you arrive at that answer.","My math teacher have presented me with a problem, that in my opinion is quite hard. It goes. ""You have a 7x7 grid. Some of the spaces in the grid are already filled with numbers. You have to insert numbers in the remaining spaces, so that the sum in every 3x3 grid inside the 7x7, is 2019. What number should be in the bottom right corner?"" Here is the grid. I know the answer is 7, but I have no idea of how you arrive at that answer.",,"['analysis', 'pattern-recognition']"
68,How do geometric properties of sine and cosine follow from their power series definition?,How do geometric properties of sine and cosine follow from their power series definition?,,"If you define $\cos$ and $\sin$ using their power series, or as the real and imaginary part of the power series of $e^{ix}$ , how can you prove that they are periodic? Also, how do you prove that period is $\pi$ ? And how do you prove that the points $(\cos(x), \ \sin(x))$ for $x \in [0, 2\pi]$ form a circle? I believe the last question can be proven if you use the continuity of $\cos$ and $\sin$ , which follows from their power series definition, and from the fact that $\cos^2(x) + \sin^2(x) = 1$ , but using only these two properties is not enough for proving they form a full circle I believe. I think you also need to find their derivatives on the intervals $[0, \ \pi/2]$ , $[\pi/2, \ \pi]$ , $[\pi, \ 3\pi/2]$ and $[3\pi/2, \ 2\pi]$ , is this correct? If so, how can this be done?","If you define and using their power series, or as the real and imaginary part of the power series of , how can you prove that they are periodic? Also, how do you prove that period is ? And how do you prove that the points for form a circle? I believe the last question can be proven if you use the continuity of and , which follows from their power series definition, and from the fact that , but using only these two properties is not enough for proving they form a full circle I believe. I think you also need to find their derivatives on the intervals , , and , is this correct? If so, how can this be done?","\cos \sin e^{ix} \pi (\cos(x), \ \sin(x)) x \in [0, 2\pi] \cos \sin \cos^2(x) + \sin^2(x) = 1 [0, \ \pi/2] [\pi/2, \ \pi] [\pi, \ 3\pi/2] [3\pi/2, \ 2\pi]","['calculus', 'analysis', 'trigonometry', 'power-series']"
69,Convergence of decreasing decrements of sequences,Convergence of decreasing decrements of sequences,,"Let $\beta_m\searrow 0$ such that $\alpha_m:=\beta_m-\beta_{m+1}\searrow 0$. Define $b_n:=\inf\{m:\alpha_m<2^{-n}\}$. Is it true that $$ \sum_{n=1}^\infty \frac{b_n}{2^n}<\infty? $$ For example, if $\beta_m=\frac 1 m$, then $b_n\sim 2^{n/2}$, so that the above series converges. A critical case is when $\beta_m=1/\log m$, whence $\alpha_m\sim 1/m(\log m)^2$, and $b_n\sim 2^n/n^2$, so the series converges. Edited: I am sorry I had a typo: I meant $\beta_m:=1/\log m$, not $\alpha_m:=1/\log m$. In the latter case, this is a simple question. However, it is not in the former case.","Let $\beta_m\searrow 0$ such that $\alpha_m:=\beta_m-\beta_{m+1}\searrow 0$. Define $b_n:=\inf\{m:\alpha_m<2^{-n}\}$. Is it true that $$ \sum_{n=1}^\infty \frac{b_n}{2^n}<\infty? $$ For example, if $\beta_m=\frac 1 m$, then $b_n\sim 2^{n/2}$, so that the above series converges. A critical case is when $\beta_m=1/\log m$, whence $\alpha_m\sim 1/m(\log m)^2$, and $b_n\sim 2^n/n^2$, so the series converges. Edited: I am sorry I had a typo: I meant $\beta_m:=1/\log m$, not $\alpha_m:=1/\log m$. In the latter case, this is a simple question. However, it is not in the former case.",,"['sequences-and-series', 'analysis', 'convergence-divergence']"
70,How to evaluate this integral using this definition?,How to evaluate this integral using this definition?,,"Let $a$ and $b$ be any two real numbers such that $a < b$, and let $f$ be the real-valued function defined on $[a, b]$ by the formula  $$ f(x) = \mathrm{e}^x \ \mbox{ for all } \ x \in [a, b]. $$ Then how to evaluate  $$ \int_a^b f(x) \ \mathrm{d} x, $$ using the definition of the integral given here ? My Attempt: Since $f$ is strictly increasing on $[a, b]$ [Can we show this fact using the machinery developed in the first seven chapters of Baby Rudin?], so it is Riemann-integrable on $[a, b]$; that is, the integral $\int_a^b f(x) \ \mathrm{d} x$ exists as a real number. Thus the upper integral $\overline{\int}_a^b f $ and the lower integral $\underline{\int}_a^b f $ are equal, where    $$ \overline{\int}_a^b f \colon= \inf \left\{ \ U(P, f) \ \colon P \mbox{ is a partition of the interval } [a, b] \ \right\}, $$   and    $$ \underline{\int}_a^b f \colon= \sup \left\{ \ L(P, f) \ \colon P \mbox{ is a partition of the interval } [a, b] \ \right\}, $$ Thus, for every partition $P$ of $[a, b]$, we have    $$ L(P, f) \leq \int_a^b f \leq U(P, f). $$ Again as $f$ is Riemann-integrable on $[a, b]$, so, corresponding to every real number $\varepsilon > 0$, we can find a partition $P_\varepsilon$ of $[a, b]$ such that    $$ U \left( P_\varepsilon, f \right) - L \left( P_\varepsilon, f \right) < \varepsilon. $$ Now let $P \colon= \left\{ \ x_0, x_1, \ldots, x_{n-1}, x_n \ \right\}$ be any partition of the interval $[a, b]$, where    $$ a = x_0 < x_1 < \cdots < x_{n-1} < x_n = b. $$   As $f$ is a monotonically increasing function on $[a, b]$, so for each $i = 1, \ldots, n$, we see that    $$ m_i \colon= \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = f \left( x_{i-1} \right) = \mathrm{e}^{x_{i-1}}, $$   and    $$ M_i \colon= \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = f \left( x_{i} \right) = \mathrm{e}^{x_{i}}; $$   therefore we have    $$ L(P, f) \colon= \sum_{i=1}^n m_i \left( x_i - x_{i-1} \right) = \sum_{i=1}^n \mathrm{e}^{x_{i-1}} \left( x_i - x_{i-1} \right), $$   and    $$ U(P, f) \colon= \sum_{i=1}^n M_i \left( x_i - x_{i-1} \right) = \sum_{i=1}^n \mathrm{e}^{x_{i}} \left( x_i - x_{i-1} \right). $$ Now for each positive integer $n$, let $P_n$ be the partition of $[a, b]$ given by    $$ P_n \colon= \left\{ \ a, a + \frac{b-a}{n}, a +  \frac{2(b-a)}{n}, \ldots, a + \frac{ (n-1) ( b-a ) }{n}, b \ \right\}; $$   that is, $P_n$ partitions the interval $[a, b]$ into $n$ equal subintervals. Thus    $$ P_n = \left\{ x_0, x_1, \ldots, x_n \ \right\}, $$   where $$ x_i \colon= a + \frac{ i(b-a)}{n} $$   for each $i = 1, \ldots, n$.    Then we have    $$  \begin{align}   L \left( P_n, f \right) &= \sum_{i=1}^n \mathrm{e}^{a + \frac{ (i-1)(b-a)}{n} } \frac{b-a}{n} \\ &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \mathrm{e}^{ \frac{ (i-1)(b-a)}{n} } \\  &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i-1} \\ &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=0}^{n-1} \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i} \\ &= \frac{b-a}{n} \mathrm{e}^a  \frac{ \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{n}  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &= \frac{b-a}{n} \mathrm{e}^a  \frac{ \mathrm{e}^{ \frac{ n (b-a) }{n} }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &=  \frac{b-a}{n} \mathrm{e}^a  \frac{ \mathrm{e}^{b-a }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 }  \\ &=  \frac{b-a}{n}   \frac{ \mathrm{e}^{b }  -  \mathrm{e}^a }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &= \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right),  \end{align} $$   and    $$  \begin{align}   U \left( P_n, f \right) &= \sum_{i=1}^n \mathrm{e}^{a + \frac{ i(b-a)}{n} } \frac{b-a}{n} \\ &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \mathrm{e}^{ \frac{ i(b-a)}{n} } \\  &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i} \\ &= \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} } \frac{ \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{n}  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &= \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} }  \frac{ \mathrm{e}^{ \frac{ n (b-a) }{n} }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &=  \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} } \frac{ \mathrm{e}^{b-a }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 }  \\ &=  \frac{b-a}{n} \mathrm{e}^{ \frac{ b-a}{n} }  \frac{ \mathrm{e}^{b }  -  \mathrm{e}^a }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &= \frac{b-a}{n}  \frac{ \mathrm{e}^{ \frac{ b-a}{n} } }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \\ &= \frac{b-a}{n} \left\{ 1 + \frac{1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \\ &= \left\{ \frac{b-a}{n}  + \frac{ \frac{b-a}{n}  }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) .  \end{align} $$ Is what I have done so far correct? If so, then what next? How to proceed from here, preferably using only the machinery developed by Walter Rudin as far as Chapter 7 in the book Principles of Mathematical Analysis , third edition? P.S.: We note that, for every partition $P$ of $[a, b]$, we have    $$ L(P, f) \leq \underline{\int}_a^b f  \leq \overline{\int}_a^b f \leq U(P, f). \tag{A} $$ Now from (A), we can conclude that, for each $n \in \mathbb{N}$, by using the partitions $P_n$, we have    $$ L \left( P_n, f \right) \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq U \left( P_n, f \right); $$   that is,    $$ \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq \left\{ \frac{b-a}{n}  + \frac{ \frac{b-a}{n}  }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \tag{B} $$   for each $n \in \mathbb{N}$. Now we see that    $$ \begin{align}  \lim_{r \to 0} \frac{ r }{ \mathrm{e}^r - 1 } &= \lim_{r \to 0} \frac{1}{\mathrm{e}^r - 0} \qquad \mbox{ [ using the L'Hosptial's rule ] } \\  &= \frac{1}{1} \\ &= 1. \tag{1} \end{align} $$ Now as $n \to \infty$, $(b-a)/n \to 0$, and so from (1) we can conclude that    $$ \lim_{n \to \infty} \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{b-a}{n} } - 1 } = 1. \tag{2} $$ Finally letting $n \to \infty$ in (B) and using (2), we find that    $$ \mathrm{e}^b - \mathrm{e}^a \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq \mathrm{e}^{b }  -  \mathrm{e}^a.  $$   Therefore    $$ \underline{\int}_a^b f  = \mathrm{e}^b - \mathrm{e}^a = \overline{\int}_a^b f. $$   That is,    $$ \int_a^b f =  \mathrm{e}^b - \mathrm{e}^a. $$ I hope I've completed the proof satisfacorily enough in the P.S., haven't I?","Let $a$ and $b$ be any two real numbers such that $a < b$, and let $f$ be the real-valued function defined on $[a, b]$ by the formula  $$ f(x) = \mathrm{e}^x \ \mbox{ for all } \ x \in [a, b]. $$ Then how to evaluate  $$ \int_a^b f(x) \ \mathrm{d} x, $$ using the definition of the integral given here ? My Attempt: Since $f$ is strictly increasing on $[a, b]$ [Can we show this fact using the machinery developed in the first seven chapters of Baby Rudin?], so it is Riemann-integrable on $[a, b]$; that is, the integral $\int_a^b f(x) \ \mathrm{d} x$ exists as a real number. Thus the upper integral $\overline{\int}_a^b f $ and the lower integral $\underline{\int}_a^b f $ are equal, where    $$ \overline{\int}_a^b f \colon= \inf \left\{ \ U(P, f) \ \colon P \mbox{ is a partition of the interval } [a, b] \ \right\}, $$   and    $$ \underline{\int}_a^b f \colon= \sup \left\{ \ L(P, f) \ \colon P \mbox{ is a partition of the interval } [a, b] \ \right\}, $$ Thus, for every partition $P$ of $[a, b]$, we have    $$ L(P, f) \leq \int_a^b f \leq U(P, f). $$ Again as $f$ is Riemann-integrable on $[a, b]$, so, corresponding to every real number $\varepsilon > 0$, we can find a partition $P_\varepsilon$ of $[a, b]$ such that    $$ U \left( P_\varepsilon, f \right) - L \left( P_\varepsilon, f \right) < \varepsilon. $$ Now let $P \colon= \left\{ \ x_0, x_1, \ldots, x_{n-1}, x_n \ \right\}$ be any partition of the interval $[a, b]$, where    $$ a = x_0 < x_1 < \cdots < x_{n-1} < x_n = b. $$   As $f$ is a monotonically increasing function on $[a, b]$, so for each $i = 1, \ldots, n$, we see that    $$ m_i \colon= \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = f \left( x_{i-1} \right) = \mathrm{e}^{x_{i-1}}, $$   and    $$ M_i \colon= \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = f \left( x_{i} \right) = \mathrm{e}^{x_{i}}; $$   therefore we have    $$ L(P, f) \colon= \sum_{i=1}^n m_i \left( x_i - x_{i-1} \right) = \sum_{i=1}^n \mathrm{e}^{x_{i-1}} \left( x_i - x_{i-1} \right), $$   and    $$ U(P, f) \colon= \sum_{i=1}^n M_i \left( x_i - x_{i-1} \right) = \sum_{i=1}^n \mathrm{e}^{x_{i}} \left( x_i - x_{i-1} \right). $$ Now for each positive integer $n$, let $P_n$ be the partition of $[a, b]$ given by    $$ P_n \colon= \left\{ \ a, a + \frac{b-a}{n}, a +  \frac{2(b-a)}{n}, \ldots, a + \frac{ (n-1) ( b-a ) }{n}, b \ \right\}; $$   that is, $P_n$ partitions the interval $[a, b]$ into $n$ equal subintervals. Thus    $$ P_n = \left\{ x_0, x_1, \ldots, x_n \ \right\}, $$   where $$ x_i \colon= a + \frac{ i(b-a)}{n} $$   for each $i = 1, \ldots, n$.    Then we have    $$  \begin{align}   L \left( P_n, f \right) &= \sum_{i=1}^n \mathrm{e}^{a + \frac{ (i-1)(b-a)}{n} } \frac{b-a}{n} \\ &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \mathrm{e}^{ \frac{ (i-1)(b-a)}{n} } \\  &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i-1} \\ &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=0}^{n-1} \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i} \\ &= \frac{b-a}{n} \mathrm{e}^a  \frac{ \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{n}  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &= \frac{b-a}{n} \mathrm{e}^a  \frac{ \mathrm{e}^{ \frac{ n (b-a) }{n} }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &=  \frac{b-a}{n} \mathrm{e}^a  \frac{ \mathrm{e}^{b-a }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 }  \\ &=  \frac{b-a}{n}   \frac{ \mathrm{e}^{b }  -  \mathrm{e}^a }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &= \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right),  \end{align} $$   and    $$  \begin{align}   U \left( P_n, f \right) &= \sum_{i=1}^n \mathrm{e}^{a + \frac{ i(b-a)}{n} } \frac{b-a}{n} \\ &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \mathrm{e}^{ \frac{ i(b-a)}{n} } \\  &= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i} \\ &= \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} } \frac{ \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{n}  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &= \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} }  \frac{ \mathrm{e}^{ \frac{ n (b-a) }{n} }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &=  \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} } \frac{ \mathrm{e}^{b-a }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 }  \\ &=  \frac{b-a}{n} \mathrm{e}^{ \frac{ b-a}{n} }  \frac{ \mathrm{e}^{b }  -  \mathrm{e}^a }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\ &= \frac{b-a}{n}  \frac{ \mathrm{e}^{ \frac{ b-a}{n} } }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \\ &= \frac{b-a}{n} \left\{ 1 + \frac{1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \\ &= \left\{ \frac{b-a}{n}  + \frac{ \frac{b-a}{n}  }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) .  \end{align} $$ Is what I have done so far correct? If so, then what next? How to proceed from here, preferably using only the machinery developed by Walter Rudin as far as Chapter 7 in the book Principles of Mathematical Analysis , third edition? P.S.: We note that, for every partition $P$ of $[a, b]$, we have    $$ L(P, f) \leq \underline{\int}_a^b f  \leq \overline{\int}_a^b f \leq U(P, f). \tag{A} $$ Now from (A), we can conclude that, for each $n \in \mathbb{N}$, by using the partitions $P_n$, we have    $$ L \left( P_n, f \right) \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq U \left( P_n, f \right); $$   that is,    $$ \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq \left\{ \frac{b-a}{n}  + \frac{ \frac{b-a}{n}  }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \tag{B} $$   for each $n \in \mathbb{N}$. Now we see that    $$ \begin{align}  \lim_{r \to 0} \frac{ r }{ \mathrm{e}^r - 1 } &= \lim_{r \to 0} \frac{1}{\mathrm{e}^r - 0} \qquad \mbox{ [ using the L'Hosptial's rule ] } \\  &= \frac{1}{1} \\ &= 1. \tag{1} \end{align} $$ Now as $n \to \infty$, $(b-a)/n \to 0$, and so from (1) we can conclude that    $$ \lim_{n \to \infty} \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{b-a}{n} } - 1 } = 1. \tag{2} $$ Finally letting $n \to \infty$ in (B) and using (2), we find that    $$ \mathrm{e}^b - \mathrm{e}^a \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq \mathrm{e}^{b }  -  \mathrm{e}^a.  $$   Therefore    $$ \underline{\int}_a^b f  = \mathrm{e}^b - \mathrm{e}^a = \overline{\int}_a^b f. $$   That is,    $$ \int_a^b f =  \mathrm{e}^b - \mathrm{e}^a. $$ I hope I've completed the proof satisfacorily enough in the P.S., haven't I?",,"['real-analysis', 'integration', 'analysis', 'definite-integrals', 'riemann-integration']"
71,"At what values of $\beta$ does $\sum a_n$ converge, where $a_n=\sqrt{1+\frac{(-1)^n}{n^\beta}}-1\,$?","At what values of  does  converge, where ?","\beta \sum a_n a_n=\sqrt{1+\frac{(-1)^n}{n^\beta}}-1\,","I'm trying to figure out the values of $\beta$ for which $\sum a_n$ converges, where $a_n=\sqrt{1+\dfrac{(-1)^n}{n^\beta}}-1\,$. Here is what I have done: I tried to re-write $a_n$ as the following, so as to get the monotonic pattern first but I couldn't even forge ahead. $$a_{2n}=\sqrt{1+\dfrac{1}{(2n)^\beta}}-1\,$$ and $$a_{2n-1}=\sqrt{1+\dfrac{1}{(2n-1)^\beta}}-1\,.$$ Please, can anyone show a way-out?","I'm trying to figure out the values of $\beta$ for which $\sum a_n$ converges, where $a_n=\sqrt{1+\dfrac{(-1)^n}{n^\beta}}-1\,$. Here is what I have done: I tried to re-write $a_n$ as the following, so as to get the monotonic pattern first but I couldn't even forge ahead. $$a_{2n}=\sqrt{1+\dfrac{1}{(2n)^\beta}}-1\,$$ and $$a_{2n-1}=\sqrt{1+\dfrac{1}{(2n-1)^\beta}}-1\,.$$ Please, can anyone show a way-out?",,"['real-analysis', 'sequences-and-series', 'analysis']"
72,Approximate numbers by certains rationals,Approximate numbers by certains rationals,,"Let $n \in \mathbb N$ and $k_n \in \left\{0,..,n \right\}$ then we define the numbers $$x_{n,k_n} = \frac{k_n+n^2}{n^3+n^2}.$$ It is easy to see that these numbers satisfy $$x_{n,0} = \frac{1}{n+1} \le x_{n,k_n} \le x_{n,n} =\frac{1}{n}.$$ I would like to know whether there exist three constants $C_1,C_2,C_3>0$ and an integer $i \in \mathbb N$ such that we can find for every $x_{n,k_n}$ a $reduced$ fraction $$\frac{p_{n,k_n}}{q_{n,k_n}}$$ such that two conditions hold: 1.) The denominator can be controlled nicely: $$ \frac{C_1}{n^i} \le \frac{1}{q_{n,k_n}} \le  \frac{C_2}{n^3}$$ and 2.) The approximation is sufficiently good: $$\left\vert x_{n,k_n}-\frac{p_{n,k_n}}{q_{n,k_n}} \right\vert \le \frac{C_3}{n^3}.$$ So to summarize: I am wondering whether one can approximate the $x_{n,k_n}$ by reduced fractions up to an error of order $1/n^3$ and whether those fractions can have a denominator that is always between two different powers of $1/n^k.$",Let and then we define the numbers It is easy to see that these numbers satisfy I would like to know whether there exist three constants and an integer such that we can find for every a fraction such that two conditions hold: 1.) The denominator can be controlled nicely: and 2.) The approximation is sufficiently good: So to summarize: I am wondering whether one can approximate the by reduced fractions up to an error of order and whether those fractions can have a denominator that is always between two different powers of,"n \in \mathbb N k_n \in \left\{0,..,n \right\} x_{n,k_n} = \frac{k_n+n^2}{n^3+n^2}. x_{n,0} = \frac{1}{n+1} \le x_{n,k_n} \le x_{n,n} =\frac{1}{n}. C_1,C_2,C_3>0 i \in \mathbb N x_{n,k_n} reduced \frac{p_{n,k_n}}{q_{n,k_n}}  \frac{C_1}{n^i} \le \frac{1}{q_{n,k_n}} \le  \frac{C_2}{n^3} \left\vert x_{n,k_n}-\frac{p_{n,k_n}}{q_{n,k_n}} \right\vert \le \frac{C_3}{n^3}. x_{n,k_n} 1/n^3 1/n^k.","['real-analysis', 'calculus']"
73,How to prove escalation to infinity (+ or -) for a few numbers with cases n/3 and 4n+1.,How to prove escalation to infinity (+ or -) for a few numbers with cases n/3 and 4n+1.,,"How do I prove that following numbers 45, 61, 101 and -59 are going up to infinity, but -155 falls into a loop. f(n)= \begin{cases} n/3, & \text{if $n≡ 0$ $(mod$ $3)$}  \\ 4n+1, & \text{if $n≢ 0$ $(mod$ $3)$} \end{cases} Example: 45 - 15 - 5 - 21 - 7 - 29 - 117 - 39 - 13 - 53 - 213 - 71 - 285 - 95 - 381 - 127 - 509 - 2037 - 679 - 2717 - 10869 - 3623 - 14493 - 4831 - 77301 - 25767 - etc.","How do I prove that following numbers 45, 61, 101 and -59 are going up to infinity, but -155 falls into a loop. f(n)= \begin{cases} n/3, & \text{if $n≡ 0$ $(mod$ $3)$}  \\ 4n+1, & \text{if $n≢ 0$ $(mod$ $3)$} \end{cases} Example: 45 - 15 - 5 - 21 - 7 - 29 - 117 - 39 - 13 - 53 - 213 - 71 - 285 - 95 - 381 - 127 - 509 - 2037 - 679 - 2717 - 10869 - 3623 - 14493 - 4831 - 77301 - 25767 - etc.",,"['sequences-and-series', 'analysis', 'elementary-number-theory']"
74,Solving the continuity equation $u_t + \nabla \cdot (uv) = 0$,Solving the continuity equation,u_t + \nabla \cdot (uv) = 0,"Consider the continuity equation $u_t + \nabla \cdot (u v) = 0$ where $u : \mathbb{R}^3 \times (0,\infty) \to \mathbb{R},$ with smooth $v(x) : \mathbb{R}^3 \to \mathbb{R}^3$ and initial data $u_0.$ Suppose $-1 < \nabla \cdot v$ and $$u_0(x) = \begin{cases} 1 & -1 \leq |x| \leq 1 \\ 0 & \text{else} \end{cases}.$$   Show that $\Omega := \{x : u(x,1) > 0\}$ has volume greater than $\frac{4}{3}.$ Any ideas on how one might go about showing this, any suggestions?  I've tried to solve it, but I'm stumped on how the given assumption on the divergence of $v$ would help even if I did solve it.  Thanks!","Consider the continuity equation $u_t + \nabla \cdot (u v) = 0$ where $u : \mathbb{R}^3 \times (0,\infty) \to \mathbb{R},$ with smooth $v(x) : \mathbb{R}^3 \to \mathbb{R}^3$ and initial data $u_0.$ Suppose $-1 < \nabla \cdot v$ and $$u_0(x) = \begin{cases} 1 & -1 \leq |x| \leq 1 \\ 0 & \text{else} \end{cases}.$$   Show that $\Omega := \{x : u(x,1) > 0\}$ has volume greater than $\frac{4}{3}.$ Any ideas on how one might go about showing this, any suggestions?  I've tried to solve it, but I'm stumped on how the given assumption on the divergence of $v$ would help even if I did solve it.  Thanks!",,"['analysis', 'partial-differential-equations', 'hyperbolic-equations']"
75,How to rescale the length of a price series?,How to rescale the length of a price series?,,"The question is about an equation coming from a scientific paper which i find it hard to reproduce. But the problem is as follows: A range of window lengths varying from twenty days to sixty days are considered. Let l denote the window length, L denote the price series length and i denote an index in price series. For each window length between i and i + l days, where index i ranges from the beginning of price series to L - l - 1 (inclusive range). The price list observed between days [i, i + l] is scaled to a new price list with 20 elements. The price series length scaling is performed using the following equation. Rescaling is analogous to resizing the price series length to 20 days. Price for day j in actual price series becomes the price on day dj in rescaled price series where dj is given by the following equation. Price for days between dj and dj+1 are interpolated. Given the following price list , how would this formula be applied in for example Excel? I tried to reproduce the result but i get some really weird numbers, so i wonder if someone can help apply this formula for the given price table list +-----+--------+ | Day | Price  | +-----+--------+ |   1 |   86.5 | |   2 |  84.76 | |   3 |   78.7 | |   4 |  71.21 | |   5 |  79.84 | |   6 |  72.83 | |   7 |  73.01 | |   8 |  67.42 | |   9 |  75.79 | |  10 |  73.52 | |  11 |  77.61 | |  12 |  71.43 | |  13 |  66.81 | |  14 |  66.54 | |  15 |  69.44 | |  16 |  63.33 | |  17 |  64.73 | |  18 |  61.14 | |  19 |  67.52 | |  20 |  71.12 | |  21 |  63.93 | |  22 |  66.59 | |  23 |  63.78 | |  24 |  66.65 | |  25 |  63.34 | |  26 |  71.13 | |  27 |  74.63 | |  28 |  67.08 | |  29 |  60.44 | |  30 |  60.87 | |  31 |  66.35 | |  32 |  67.53 | |  33 |  69.62 | |  34 |  73.98 | |  35 |  79.52 | |  36 |  86.74 | |  37 |   87.9 | |  38 |  82.19 | |  39 |  84.98 | |  40 |  80.11 | |  41 |  80.69 | |  42 |  81.28 | |  43 |  85.92 | |  44 |  95.83 | |  45 |  98.66 | |  46 |  96.84 | |  47 |  100.3 | |  48 | 101.91 | |  49 | 108.97 | |  50 | 116.54 | |  51 | 119.52 | |  52 | 124.37 | |  53 | 130.74 | |  54 | 136.83 | |  55 | 140.88 | |  56 | 140.68 | |  57 | 142.73 | |  58 | 135.58 | |  59 | 144.65 | |  60 |  150.3 | +-----+--------+ The result should be 40 windows with rescaled prices to 20 elements each. The equation is a linear interpolator that rescales the series over interval L elements to one over 20 elements. So basically taking all the prices after the 20th element in the price series and rescale that new list to a 20 elements list.  Here is the same table with the windows it should rescale. I created a simple excel file here with the following table: +-----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+ |           | windows length (l) |  21   |  22   |  23   |  24   |  25   |  26   |  27   |  28   |  29   |  30   |  31   |  32   |  33   |  34   |  35   |  36   |  37   |  38   |  39   |  40   |  41   |  42   |  43   |  44   |  45   |  46   |  47   |   48   |   49   |   50   |   51   |   52   |   53   |   54   |   55   |   56   |   57   |   58   |   59   |   60   | +-----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+ | Index (i) | Price              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        |        |        |        |        | | 1         | 86.5               |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 | | 2         | 84.76              | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 | | 3         | 78.7               |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 | | 4         | 71.21              | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 | | 5         | 79.84              | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 | | 6         | 72.83              | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 | | 7         | 73.01              | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 | | 8         | 67.42              | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 | | 9         | 75.79              | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 | | 10        | 73.52              | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 | | 11        | 77.61              | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 | | 12        | 71.43              | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 | | 13        | 66.81              | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 | | 14        | 66.54              | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 | | 15        | 69.44              | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 | | 16        | 63.33              | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 | | 17        | 64.73              | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 | | 18        | 61.14              | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 | | 19        | 67.52              | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 | | 20        | 71.12              | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 | | 21        | 63.93              | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 | | 22        | 66.59              |       | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 | | 23        | 63.78              |       |       | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 | | 24        | 66.65              |       |       |       | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 | | 25        | 63.34              |       |       |       |       | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 | | 26        | 71.13              |       |       |       |       |       | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 | | 27        | 74.63              |       |       |       |       |       |       | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 | | 28        | 67.08              |       |       |       |       |       |       |       | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 | | 29        | 60.44              |       |       |       |       |       |       |       |       | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 | | 30        | 60.87              |       |       |       |       |       |       |       |       |       | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 | | 31        | 66.35              |       |       |       |       |       |       |       |       |       |       | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 | | 32        | 67.53              |       |       |       |       |       |       |       |       |       |       |       | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 | | 33        | 69.62              |       |       |       |       |       |       |       |       |       |       |       |       | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 | | 34        | 73.98              |       |       |       |       |       |       |       |       |       |       |       |       |       | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 | | 35        | 79.52              |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 | | 36        | 86.74              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 | | 37        | 87.9               |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 | | 38        | 82.19              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 | | 39        | 84.98              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 | | 40        | 80.11              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 80.11 | 80.11 | 80.11 | 80.11 | 80.11 | 80.11 | 80.11 | 80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 | | 41        | 80.69              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 80.69 | 80.69 | 80.69 | 80.69 | 80.69 | 80.69 | 80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 | | 42        | 81.28              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 81.28 | 81.28 | 81.28 | 81.28 | 81.28 | 81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 | | 43        | 85.92              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 85.92 | 85.92 | 85.92 | 85.92 | 85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 | | 44        | 95.83              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 95.83 | 95.83 | 95.83 | 95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 | | 45        | 98.66              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 98.66 | 98.66 | 98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 | | 46        | 96.84              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 96.84 | 96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 | | 47        | 100.3              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 | | 48        | 101.91             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | | 49        | 108.97             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | | 50        | 116.54             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | | 51        | 119.52             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | | 52        | 124.37             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | | 53        | 130.74             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        | 130.74 | 130.74 | 130.74 | 130.74 | 130.74 | 130.74 | 130.74 | 130.74 | | 54        | 136.83             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        | 136.83 | 136.83 | 136.83 | 136.83 | 136.83 | 136.83 | 136.83 | | 55        | 140.88             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        | 140.88 | 140.88 | 140.88 | 140.88 | 140.88 | 140.88 | | 56        | 140.68             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        | 140.68 | 140.68 | 140.68 | 140.68 | 140.68 | | 57        | 142.73             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        | 142.73 | 142.73 | 142.73 | 142.73 | | 58        | 135.58             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        |        | 135.58 | 135.58 | 135.58 | | 59        | 144.65             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        |        |        | 144.65 | 144.65 | | 60        | 150.3              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        |        |        |        |  150.3 | +-----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+","The question is about an equation coming from a scientific paper which i find it hard to reproduce. But the problem is as follows: A range of window lengths varying from twenty days to sixty days are considered. Let l denote the window length, L denote the price series length and i denote an index in price series. For each window length between i and i + l days, where index i ranges from the beginning of price series to L - l - 1 (inclusive range). The price list observed between days [i, i + l] is scaled to a new price list with 20 elements. The price series length scaling is performed using the following equation. Rescaling is analogous to resizing the price series length to 20 days. Price for day j in actual price series becomes the price on day dj in rescaled price series where dj is given by the following equation. Price for days between dj and dj+1 are interpolated. Given the following price list , how would this formula be applied in for example Excel? I tried to reproduce the result but i get some really weird numbers, so i wonder if someone can help apply this formula for the given price table list +-----+--------+ | Day | Price  | +-----+--------+ |   1 |   86.5 | |   2 |  84.76 | |   3 |   78.7 | |   4 |  71.21 | |   5 |  79.84 | |   6 |  72.83 | |   7 |  73.01 | |   8 |  67.42 | |   9 |  75.79 | |  10 |  73.52 | |  11 |  77.61 | |  12 |  71.43 | |  13 |  66.81 | |  14 |  66.54 | |  15 |  69.44 | |  16 |  63.33 | |  17 |  64.73 | |  18 |  61.14 | |  19 |  67.52 | |  20 |  71.12 | |  21 |  63.93 | |  22 |  66.59 | |  23 |  63.78 | |  24 |  66.65 | |  25 |  63.34 | |  26 |  71.13 | |  27 |  74.63 | |  28 |  67.08 | |  29 |  60.44 | |  30 |  60.87 | |  31 |  66.35 | |  32 |  67.53 | |  33 |  69.62 | |  34 |  73.98 | |  35 |  79.52 | |  36 |  86.74 | |  37 |   87.9 | |  38 |  82.19 | |  39 |  84.98 | |  40 |  80.11 | |  41 |  80.69 | |  42 |  81.28 | |  43 |  85.92 | |  44 |  95.83 | |  45 |  98.66 | |  46 |  96.84 | |  47 |  100.3 | |  48 | 101.91 | |  49 | 108.97 | |  50 | 116.54 | |  51 | 119.52 | |  52 | 124.37 | |  53 | 130.74 | |  54 | 136.83 | |  55 | 140.88 | |  56 | 140.68 | |  57 | 142.73 | |  58 | 135.58 | |  59 | 144.65 | |  60 |  150.3 | +-----+--------+ The result should be 40 windows with rescaled prices to 20 elements each. The equation is a linear interpolator that rescales the series over interval L elements to one over 20 elements. So basically taking all the prices after the 20th element in the price series and rescale that new list to a 20 elements list.  Here is the same table with the windows it should rescale. I created a simple excel file here with the following table: +-----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+ |           | windows length (l) |  21   |  22   |  23   |  24   |  25   |  26   |  27   |  28   |  29   |  30   |  31   |  32   |  33   |  34   |  35   |  36   |  37   |  38   |  39   |  40   |  41   |  42   |  43   |  44   |  45   |  46   |  47   |   48   |   49   |   50   |   51   |   52   |   53   |   54   |   55   |   56   |   57   |   58   |   59   |   60   | +-----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+ | Index (i) | Price              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        |        |        |        |        | | 1         | 86.5               |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |  86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 |   86.5 | | 2         | 84.76              | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 | 84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 |  84.76 | | 3         | 78.7               |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |  78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 |   78.7 | | 4         | 71.21              | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 | 71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 |  71.21 | | 5         | 79.84              | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 | 79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 |  79.84 | | 6         | 72.83              | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 | 72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 |  72.83 | | 7         | 73.01              | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 | 73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 |  73.01 | | 8         | 67.42              | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 | 67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 |  67.42 | | 9         | 75.79              | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 | 75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 |  75.79 | | 10        | 73.52              | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 | 73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 |  73.52 | | 11        | 77.61              | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 | 77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 |  77.61 | | 12        | 71.43              | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 | 71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 |  71.43 | | 13        | 66.81              | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 | 66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 |  66.81 | | 14        | 66.54              | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 | 66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 |  66.54 | | 15        | 69.44              | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 | 69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 |  69.44 | | 16        | 63.33              | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 | 63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 |  63.33 | | 17        | 64.73              | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 | 64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 |  64.73 | | 18        | 61.14              | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 | 61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 |  61.14 | | 19        | 67.52              | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 | 67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 |  67.52 | | 20        | 71.12              | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 | 71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 |  71.12 | | 21        | 63.93              | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 | 63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 |  63.93 | | 22        | 66.59              |       | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 | 66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 |  66.59 | | 23        | 63.78              |       |       | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 | 63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 |  63.78 | | 24        | 66.65              |       |       |       | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 | 66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 |  66.65 | | 25        | 63.34              |       |       |       |       | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 | 63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 |  63.34 | | 26        | 71.13              |       |       |       |       |       | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 | 71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 |  71.13 | | 27        | 74.63              |       |       |       |       |       |       | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 | 74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 |  74.63 | | 28        | 67.08              |       |       |       |       |       |       |       | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 | 67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 |  67.08 | | 29        | 60.44              |       |       |       |       |       |       |       |       | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 | 60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 |  60.44 | | 30        | 60.87              |       |       |       |       |       |       |       |       |       | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 | 60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 |  60.87 | | 31        | 66.35              |       |       |       |       |       |       |       |       |       |       | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 | 66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 |  66.35 | | 32        | 67.53              |       |       |       |       |       |       |       |       |       |       |       | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 | 67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 |  67.53 | | 33        | 69.62              |       |       |       |       |       |       |       |       |       |       |       |       | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 | 69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 |  69.62 | | 34        | 73.98              |       |       |       |       |       |       |       |       |       |       |       |       |       | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 | 73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 |  73.98 | | 35        | 79.52              |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 | 79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 |  79.52 | | 36        | 86.74              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 | 86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 |  86.74 | | 37        | 87.9               |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |  87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 |   87.9 | | 38        | 82.19              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 | 82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 |  82.19 | | 39        | 84.98              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 | 84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 |  84.98 | | 40        | 80.11              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 80.11 | 80.11 | 80.11 | 80.11 | 80.11 | 80.11 | 80.11 | 80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 |  80.11 | | 41        | 80.69              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 80.69 | 80.69 | 80.69 | 80.69 | 80.69 | 80.69 | 80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 |  80.69 | | 42        | 81.28              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 81.28 | 81.28 | 81.28 | 81.28 | 81.28 | 81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 |  81.28 | | 43        | 85.92              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 85.92 | 85.92 | 85.92 | 85.92 | 85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 |  85.92 | | 44        | 95.83              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 95.83 | 95.83 | 95.83 | 95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 |  95.83 | | 45        | 98.66              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 98.66 | 98.66 | 98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 |  98.66 | | 46        | 96.84              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 96.84 | 96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 |  96.84 | | 47        | 100.3              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 |  100.3 | | 48        | 101.91             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | 101.91 | | 49        | 108.97             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | 108.97 | | 50        | 116.54             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | 116.54 | | 51        | 119.52             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | 119.52 | | 52        | 124.37             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | 124.37 | | 53        | 130.74             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        | 130.74 | 130.74 | 130.74 | 130.74 | 130.74 | 130.74 | 130.74 | 130.74 | | 54        | 136.83             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        | 136.83 | 136.83 | 136.83 | 136.83 | 136.83 | 136.83 | 136.83 | | 55        | 140.88             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        | 140.88 | 140.88 | 140.88 | 140.88 | 140.88 | 140.88 | | 56        | 140.68             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        | 140.68 | 140.68 | 140.68 | 140.68 | 140.68 | | 57        | 142.73             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        | 142.73 | 142.73 | 142.73 | 142.73 | | 58        | 135.58             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        |        | 135.58 | 135.58 | 135.58 | | 59        | 144.65             |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        |        |        | 144.65 | 144.65 | | 60        | 150.3              |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |        |        |        |        |        |        |        |        |        |        |        |        |  150.3 | +-----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+",,"['calculus', 'sequences-and-series', 'analysis', 'interpolation']"
76,Continuous integral function $F$,Continuous integral function,F,"If $f$ is continuous on rectangle $[a,b]\times[c,d]$, and if $g\in R$ on $[a,b]$, then the function $F$ defined by equation $$F(y)=\int_a^bg(x)f(x,y)dx$$ is continuous on $[c,d]$. Proof: If $G(x)=\int_a^xg(t)dt$, then by Theorem 1, $F(y)=\int_a^bf(x,y)dG(x)$. Now by Theorem 2 , $F(y)=\int_a^bg(x)f(x,y)dx$ is continuous on $[c,d]$. Theorem 1: Suppose $f\in R(\alpha)$ and $g\in R(\alpha)$ on $[a,b]$ where $\alpha$ is increasing on $[a,b]$. We define $F(x)=\int_a^xf(t)d\alpha (t)$ and $G(x)=\int_a^xg(t)d\alpha (t)$ if $x\in [a,b]$. Then $f\in R(G),g\in R(F), fg\in R(\alpha )$ on $[a,b]$ and $\int_a^bf(x)g(x)d\alpha (x)=\int_a^bf(x)dG(x)=\int_a^bg(x)dF(x)$. Theorem 2: Let $f$ be a continuous function in each point $(x,y)$ of rectangle $[a,b]\times[c,d]$. Suppose $\alpha$ is bounded variation on $[a,b]$ and let $F$ be the function defined on $[c,d]$ by $F(y)=\int_a^bf(x,y)d\alpha (x)$. Then $F$ is continuous on $[c,d]$. How the proof apply theorem 1 ? I don't see it. When using theorem 2, how do we know that g is bounded variation?","If $f$ is continuous on rectangle $[a,b]\times[c,d]$, and if $g\in R$ on $[a,b]$, then the function $F$ defined by equation $$F(y)=\int_a^bg(x)f(x,y)dx$$ is continuous on $[c,d]$. Proof: If $G(x)=\int_a^xg(t)dt$, then by Theorem 1, $F(y)=\int_a^bf(x,y)dG(x)$. Now by Theorem 2 , $F(y)=\int_a^bg(x)f(x,y)dx$ is continuous on $[c,d]$. Theorem 1: Suppose $f\in R(\alpha)$ and $g\in R(\alpha)$ on $[a,b]$ where $\alpha$ is increasing on $[a,b]$. We define $F(x)=\int_a^xf(t)d\alpha (t)$ and $G(x)=\int_a^xg(t)d\alpha (t)$ if $x\in [a,b]$. Then $f\in R(G),g\in R(F), fg\in R(\alpha )$ on $[a,b]$ and $\int_a^bf(x)g(x)d\alpha (x)=\int_a^bf(x)dG(x)=\int_a^bg(x)dF(x)$. Theorem 2: Let $f$ be a continuous function in each point $(x,y)$ of rectangle $[a,b]\times[c,d]$. Suppose $\alpha$ is bounded variation on $[a,b]$ and let $F$ be the function defined on $[c,d]$ by $F(y)=\int_a^bf(x,y)d\alpha (x)$. Then $F$ is continuous on $[c,d]$. How the proof apply theorem 1 ? I don't see it. When using theorem 2, how do we know that g is bounded variation?",,"['integration', 'analysis', 'continuity', 'proof-explanation', 'riemann-integration']"
77,Extending continuous functions to the completion (metric spaces).,Extending continuous functions to the completion (metric spaces).,,"I'm am trying to fill in an omitted detail from my lecture notes on Functional Analysis. Let $f:X\to Y$, where $X$ and $Y$ are metric spaces, with $Y$ complete. Suppose $f$ maps Cauchy sequences to Cauchy sequences (so is continuous in particular). Denote by $\mathrm{Comp}(X)$ the completion of $X$. Since $X$ densely embeds into $\mathrm{comp}(X)$ (and metric spaces are Hausdorff), there is at most one continuous extension of $f$ to $\mathrm{comp}(X)$, which we denote also by $f$. Existence is what's bothering me. Presumably for $[(a_n)]\in\mathrm{comp}(X)$, one defines $$f([a_n]):=\lim_{n\to \infty}f(a_n).$$ Is it obvious that (1) this extension is well-defined and (2) it's a continuous function? One defines the metric $d$ on the completion by $d([a_n], [b_n]):=\lim_n d(a_n, b_n)$. Many thanks!","I'm am trying to fill in an omitted detail from my lecture notes on Functional Analysis. Let $f:X\to Y$, where $X$ and $Y$ are metric spaces, with $Y$ complete. Suppose $f$ maps Cauchy sequences to Cauchy sequences (so is continuous in particular). Denote by $\mathrm{Comp}(X)$ the completion of $X$. Since $X$ densely embeds into $\mathrm{comp}(X)$ (and metric spaces are Hausdorff), there is at most one continuous extension of $f$ to $\mathrm{comp}(X)$, which we denote also by $f$. Existence is what's bothering me. Presumably for $[(a_n)]\in\mathrm{comp}(X)$, one defines $$f([a_n]):=\lim_{n\to \infty}f(a_n).$$ Is it obvious that (1) this extension is well-defined and (2) it's a continuous function? One defines the metric $d$ on the completion by $d([a_n], [b_n]):=\lim_n d(a_n, b_n)$. Many thanks!",,"['analysis', 'metric-spaces']"
78,Theorem 5.5 in Baby Rudin: Do we need the continuity of $f$ on the entire interval?,Theorem 5.5 in Baby Rudin: Do we need the continuity of  on the entire interval?,f,"Here is Theorem 5.5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is continuous on $[a, b]$, $f^\prime(x)$ exists at some point $x \in [a, b]$, $g$ is defined on an interval $I$ which contains the range of $f$, and $g$ is differentiable at the point $f(x)$. If $$h(t) = g \left( f(t) \right) \ \ \ (a \leq t \leq b), $$ then $h$ is differentiable at $x$, and    $$\tag{3} h^\prime(x) = g^\prime \left( f(x) \right) f^\prime(x).$$ And, here is Rudin's proof. Let $y = f(x)$. By the definition of the derivative, we have    $$ \tag{4} f(t) - f(x) = (t-x) \left[ f^\prime(x) + u(t) \right], $$   $$ \tag{5} g(s) - g(y) = (s-y) \left[ g^\prime(y) + v(s) \right], $$   where $t \in [a, b]$, $s \in I$, and $u(t) \to 0$ as $t \to x$, $v(s) \to 0$ as $s \to y$. Let $s = f(t)$. Using first (5) and then (4), we obtain    $$  \begin{align} h(t) - h(x) &= g\left( f(t) \right) - g\left( f(x) \right) \\  &= \left[ f(t) - f(x) \right] \cdot \left[ g^\prime(y) + v(s) \right] \\  &= (t-x) \cdot \left[ f^\prime(x) + u(t) \right] \cdot \left[ g^\prime(y) + v(s) \right], \end{align} $$   or, if $t \neq x$,    $$\tag{6} \frac{h(t) - h(x) }{t-x} = \left[ f^\prime(x) + u(t) \right] \cdot \left[ g^\prime(y) + v(s) \right]. $$   Letting $t \to x$, we see that $s \to y$, by the continuity of $f$, so that the right side of (6) tends to $g^\prime(y) f^\prime(x)$, which gives (3). Now my question is, is the continuity of $f$ on the entire interval $[a, b]$ essential in this theorem as Rudin has stated and proved it? Or, is it sufficient to just assume the continuity of $f$ at the point $x \in [a, b]$?","Here is Theorem 5.5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is continuous on $[a, b]$, $f^\prime(x)$ exists at some point $x \in [a, b]$, $g$ is defined on an interval $I$ which contains the range of $f$, and $g$ is differentiable at the point $f(x)$. If $$h(t) = g \left( f(t) \right) \ \ \ (a \leq t \leq b), $$ then $h$ is differentiable at $x$, and    $$\tag{3} h^\prime(x) = g^\prime \left( f(x) \right) f^\prime(x).$$ And, here is Rudin's proof. Let $y = f(x)$. By the definition of the derivative, we have    $$ \tag{4} f(t) - f(x) = (t-x) \left[ f^\prime(x) + u(t) \right], $$   $$ \tag{5} g(s) - g(y) = (s-y) \left[ g^\prime(y) + v(s) \right], $$   where $t \in [a, b]$, $s \in I$, and $u(t) \to 0$ as $t \to x$, $v(s) \to 0$ as $s \to y$. Let $s = f(t)$. Using first (5) and then (4), we obtain    $$  \begin{align} h(t) - h(x) &= g\left( f(t) \right) - g\left( f(x) \right) \\  &= \left[ f(t) - f(x) \right] \cdot \left[ g^\prime(y) + v(s) \right] \\  &= (t-x) \cdot \left[ f^\prime(x) + u(t) \right] \cdot \left[ g^\prime(y) + v(s) \right], \end{align} $$   or, if $t \neq x$,    $$\tag{6} \frac{h(t) - h(x) }{t-x} = \left[ f^\prime(x) + u(t) \right] \cdot \left[ g^\prime(y) + v(s) \right]. $$   Letting $t \to x$, we see that $s \to y$, by the continuity of $f$, so that the right side of (6) tends to $g^\prime(y) f^\prime(x)$, which gives (3). Now my question is, is the continuity of $f$ on the entire interval $[a, b]$ essential in this theorem as Rudin has stated and proved it? Or, is it sufficient to just assume the continuity of $f$ at the point $x \in [a, b]$?",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'chain-rule']"
79,"If $f$ is continuous, and $f(x) \rightarrow 2$ as $x \rightarrow \infty$ then $f$ is uniformly continuous","If  is continuous, and  as  then  is uniformly continuous",f f(x) \rightarrow 2 x \rightarrow \infty f,"I recently asked for some direction with how to connect a convergent function to proving it is uniformly continuous and I was directed to an awesome post that explained a lot for me. So now I tried to prove this problem which pretty much asks a specific case of that and I was wondering if some people could look it over for errors. Suppose $f$ is continuous on $[0, \infty)$ and $f(x) \rightarrow 2$ as $x \rightarrow \infty$. Prove $f$ is uniformly continuous on $[0, \infty)$. Since $f(x) \rightarrow 2$ as $x \rightarrow \infty$, then for $\epsilon>0, \exists k $ s.t. $ \forall x \geq k$ we have $|f(x) - 2| < \frac{\epsilon}{2}$. Also, note that since $f$ is continuous on $[0, \infty)$ it is uniformly continuous on $[0, k+2]$ and so for $\epsilon >0$, $\exists \delta_1 > 0$ s.t. if $x,y \in [0, k+2]$ and $|x-y| < \delta_1$ then $|f(x) - f(y)| < \epsilon$. Now given $\epsilon >0$, take $\delta = \min(\delta_1, \frac{1}{2}) > 0$. Suppose $x,y \in [0, \infty)$ and $|x-y| < \delta$. Then if $x > k + 1$, since $|x-y| < \frac{1}{2}$ we have $k+1 - y < x - y < \frac{1}{2}$ and so $y > k$. Together, we have $x, y > k$ and $|x-y| < \delta$, so $|f(x) - f(y)| = |(f(x) - 2) + (-f(y) + 2)| \leq |f(x) -2| + |f(y) - 2| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$. If $x \leq k + 1$, since $|x - y| < \frac{1}{2}$, then we have $y < k + \frac{3}{2} < k+2$. Together then, we have $x, y \in [0,k+2]$ and $|x-y| < \delta \leq \delta_1$ and so $|f(x) - f(y)| < \epsilon$. Thus we have shown for $\epsilon >0$, there is a $\delta > 0$ s.t. if $x,y \in [0, \infty)$ and $|x-y| < 0$, then $|f(x) - f(y)| < \epsilon$.","I recently asked for some direction with how to connect a convergent function to proving it is uniformly continuous and I was directed to an awesome post that explained a lot for me. So now I tried to prove this problem which pretty much asks a specific case of that and I was wondering if some people could look it over for errors. Suppose $f$ is continuous on $[0, \infty)$ and $f(x) \rightarrow 2$ as $x \rightarrow \infty$. Prove $f$ is uniformly continuous on $[0, \infty)$. Since $f(x) \rightarrow 2$ as $x \rightarrow \infty$, then for $\epsilon>0, \exists k $ s.t. $ \forall x \geq k$ we have $|f(x) - 2| < \frac{\epsilon}{2}$. Also, note that since $f$ is continuous on $[0, \infty)$ it is uniformly continuous on $[0, k+2]$ and so for $\epsilon >0$, $\exists \delta_1 > 0$ s.t. if $x,y \in [0, k+2]$ and $|x-y| < \delta_1$ then $|f(x) - f(y)| < \epsilon$. Now given $\epsilon >0$, take $\delta = \min(\delta_1, \frac{1}{2}) > 0$. Suppose $x,y \in [0, \infty)$ and $|x-y| < \delta$. Then if $x > k + 1$, since $|x-y| < \frac{1}{2}$ we have $k+1 - y < x - y < \frac{1}{2}$ and so $y > k$. Together, we have $x, y > k$ and $|x-y| < \delta$, so $|f(x) - f(y)| = |(f(x) - 2) + (-f(y) + 2)| \leq |f(x) -2| + |f(y) - 2| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$. If $x \leq k + 1$, since $|x - y| < \frac{1}{2}$, then we have $y < k + \frac{3}{2} < k+2$. Together then, we have $x, y \in [0,k+2]$ and $|x-y| < \delta \leq \delta_1$ and so $|f(x) - f(y)| < \epsilon$. Thus we have shown for $\epsilon >0$, there is a $\delta > 0$ s.t. if $x,y \in [0, \infty)$ and $|x-y| < 0$, then $|f(x) - f(y)| < \epsilon$.",,"['real-analysis', 'analysis', 'proof-verification', 'uniform-continuity']"
80,"If$f_n$ a sequence of integrable functions on a set of finite measure which converge to f uniformly, then the limit and integral can be swapped. [duplicate]","If a sequence of integrable functions on a set of finite measure which converge to f uniformly, then the limit and integral can be swapped. [duplicate]",f_n,"This question already has answers here : Lebesgue integral uniform convergence (2 answers) Closed 7 years ago . Hello I came across this problem and tried to prove it, I would just like to confirm that my proof is correct please as I think there is something wrong with it. The problem says the following: Suppose $(f_n)$ is a sequence of integrable functions on a set A of finite measure. Show that if the sequence is uniformly convergent on A, then $\lim_{n\to \infty} \int_{A} f_n dm= \int_{A} \lim_{n\to \infty} f_n dm$. My attenpt: Let $\epsilon>0$ be fixed but arbitrary. Then by uniform convergence,there exists an N s.t. $|f_n-f|<\epsilon$ for all n>N. Then $0\leq \int_A |f_n-f| \leq \epsilon m(A)$ for all n>N. Now since $\epsilon>0$ arbitrary and $m(A)<\infty$ then $\int_A |f_n-f| \to 0$ which implies $0<|\int_{A} (f_n-f)| \leq \int_{A} |f_n-f|$ and now taking the limit gives the desired result. Could someone verify my proof? I am not sure when steps like since epsilon is arbitrary work.Also where did I use integrability of the $f_n$ ? Thanks.","This question already has answers here : Lebesgue integral uniform convergence (2 answers) Closed 7 years ago . Hello I came across this problem and tried to prove it, I would just like to confirm that my proof is correct please as I think there is something wrong with it. The problem says the following: Suppose $(f_n)$ is a sequence of integrable functions on a set A of finite measure. Show that if the sequence is uniformly convergent on A, then $\lim_{n\to \infty} \int_{A} f_n dm= \int_{A} \lim_{n\to \infty} f_n dm$. My attenpt: Let $\epsilon>0$ be fixed but arbitrary. Then by uniform convergence,there exists an N s.t. $|f_n-f|<\epsilon$ for all n>N. Then $0\leq \int_A |f_n-f| \leq \epsilon m(A)$ for all n>N. Now since $\epsilon>0$ arbitrary and $m(A)<\infty$ then $\int_A |f_n-f| \to 0$ which implies $0<|\int_{A} (f_n-f)| \leq \int_{A} |f_n-f|$ and now taking the limit gives the desired result. Could someone verify my proof? I am not sure when steps like since epsilon is arbitrary work.Also where did I use integrability of the $f_n$ ? Thanks.",,['analysis']
81,Good PDE Book That Explains Why,Good PDE Book That Explains Why,,"I am interested in studying the basics of PDEs. I cannot find a good book on PDEs. I want a book that is more on the abstract side and explains why (so not Evan's). For example, if the topic is second order linear PDEs, I want the book to explain why they are classified into elliptic, hyperbolic and parabolic, and then explain how the time coordinate can be separated in the hyperbolic and parabolic cases (preferably before trying to solve them). I have a fairly strong background in introductory mathematics and prefer that the book takes advantage of functional analysis (but with motivation) and (sometimes) considers general Banach spaces. I want specific PDEs (such as Laplace's equation) to only be examples.","I am interested in studying the basics of PDEs. I cannot find a good book on PDEs. I want a book that is more on the abstract side and explains why (so not Evan's). For example, if the topic is second order linear PDEs, I want the book to explain why they are classified into elliptic, hyperbolic and parabolic, and then explain how the time coordinate can be separated in the hyperbolic and parabolic cases (preferably before trying to solve them). I have a fairly strong background in introductory mathematics and prefer that the book takes advantage of functional analysis (but with motivation) and (sometimes) considers general Banach spaces. I want specific PDEs (such as Laplace's equation) to only be examples.",,"['analysis', 'reference-request', 'partial-differential-equations']"
82,A question about properties of Newton's Divided Difference,A question about properties of Newton's Divided Difference,,"Recall that Newton's Divided Difference: $$f[x_0,x_1]=\frac{f(x_1)-f(x_0)}{x_1-x_0},$$ and $$f[x_0,x_1,\ldots,x_n]=\frac{f[x_1,\ldots,x_n]-f[x_0,\ldots,x_{n-1}]}{x_n-x_0},$$ where $x_0,x_1,\ldots,x_n$ are distinct. Now I have a question about properties of Newton's Divided Difference. Let $f(x)$ be a function and $x_0,x_1,\ldots,x_n$ are distinct. Define  $$g(x)=f[x_0,x_1,\ldots,x_n,x].$$ How can I show that $g'(x)=f[x_0,x_1,\ldots,x_n,x,x]$?","Recall that Newton's Divided Difference: $$f[x_0,x_1]=\frac{f(x_1)-f(x_0)}{x_1-x_0},$$ and $$f[x_0,x_1,\ldots,x_n]=\frac{f[x_1,\ldots,x_n]-f[x_0,\ldots,x_{n-1}]}{x_n-x_0},$$ where $x_0,x_1,\ldots,x_n$ are distinct. Now I have a question about properties of Newton's Divided Difference. Let $f(x)$ be a function and $x_0,x_1,\ldots,x_n$ are distinct. Define  $$g(x)=f[x_0,x_1,\ldots,x_n,x].$$ How can I show that $g'(x)=f[x_0,x_1,\ldots,x_n,x,x]$?",,"['analysis', 'numerical-methods']"
83,Spivak Calculus - Chapter 1 Question 4.6,Spivak Calculus - Chapter 1 Question 4.6,,"In Spivak's Calculus, Chapter 1 Question 4.6: Find all the numbers $x$ for which $x^2+x+1>2$ The chapter focuses on using the following properties of numbers to prove solutions are correct: Based on those properties, I am able to perform the following algebra: $ \begin{align} x^2 + x + 1 &> 2 & \text{Given}\\ x (x + 1) + 1 &> 2 & \text{P9}\\ x (x+1) &> 1 & \text{P3 P2 and Addition} \end{align} $ And from there, I can note that: $ \begin{align} x &\neq (x+1)^{-1}\\  x^{-1} &\neq (x+1)\\  \end{align} $ By P6, because $x (x+1) > 1$ and $x (x+1) \neq 1$. However, in his book Spivak is able to find the following: $ \begin{align} x &> \frac{-1+\sqrt{5}}{2} \text{ or}\\ x &< \frac{-1-\sqrt{5}}{2} \end{align} $ How does he come to that conclusion using only the properties listed above?","In Spivak's Calculus, Chapter 1 Question 4.6: Find all the numbers $x$ for which $x^2+x+1>2$ The chapter focuses on using the following properties of numbers to prove solutions are correct: Based on those properties, I am able to perform the following algebra: $ \begin{align} x^2 + x + 1 &> 2 & \text{Given}\\ x (x + 1) + 1 &> 2 & \text{P9}\\ x (x+1) &> 1 & \text{P3 P2 and Addition} \end{align} $ And from there, I can note that: $ \begin{align} x &\neq (x+1)^{-1}\\  x^{-1} &\neq (x+1)\\  \end{align} $ By P6, because $x (x+1) > 1$ and $x (x+1) \neq 1$. However, in his book Spivak is able to find the following: $ \begin{align} x &> \frac{-1+\sqrt{5}}{2} \text{ or}\\ x &< \frac{-1-\sqrt{5}}{2} \end{align} $ How does he come to that conclusion using only the properties listed above?",,"['calculus', 'analysis', 'proof-explanation']"
84,Is my proof of $\lim_{x\rightarrow c}x^2=c^2$ correct?,Is my proof of  correct?,\lim_{x\rightarrow c}x^2=c^2,"I know the most common proof of $\lim_{x\rightarrow c}x^2=c^2$. But I wonder if my alternative proof is valid and correct. Here's my proof. Let $\varepsilon>0$, want to find a $\delta>0$ such that $\forall x\in\mathbb{R},0<|x-c|<\delta\Rightarrow |x^2-c^2|<\varepsilon$ For the convenience for observation, suppose that $0<|x-c|<\square$, we want to find out which $\square$ is ok, and then know what $\delta$ to pick. Since $|x^2-c^2|<\varepsilon\Leftrightarrow |x+c||x-c|<\varepsilon$ and \begin{alignat*}{3} &0<|x-c|<\square\\ \Longleftrightarrow &c-\square<x<c+\square&(except\ x=c)\\ \Longleftrightarrow &2c-\square<x+c<2c+\square\qquad&(except\ x=c)\\ \Longrightarrow &|x+c|< |2c|+\square \end{alignat*} So we see that if we want  $|x+c||x-c|<(|2c|+\square)\square<\varepsilon$ to be true, we need to solve a positive solution of the quadratic inequality $\square^2+|2c|\square-\varepsilon<0$, by the relationship between roots and coefficient, we know $\square^2+|2c|\square-\varepsilon=0$ has exactly one positive and one negative root. Hence the solution of the prior inequality is $(-c-\sqrt{c^2+\varepsilon},-c+\sqrt{c^2+\varepsilon})$, where the right endpoint is positive. Thus, we pick $\delta=-c+\sqrt{c^2+\varepsilon}$ to complete the proof.","I know the most common proof of $\lim_{x\rightarrow c}x^2=c^2$. But I wonder if my alternative proof is valid and correct. Here's my proof. Let $\varepsilon>0$, want to find a $\delta>0$ such that $\forall x\in\mathbb{R},0<|x-c|<\delta\Rightarrow |x^2-c^2|<\varepsilon$ For the convenience for observation, suppose that $0<|x-c|<\square$, we want to find out which $\square$ is ok, and then know what $\delta$ to pick. Since $|x^2-c^2|<\varepsilon\Leftrightarrow |x+c||x-c|<\varepsilon$ and \begin{alignat*}{3} &0<|x-c|<\square\\ \Longleftrightarrow &c-\square<x<c+\square&(except\ x=c)\\ \Longleftrightarrow &2c-\square<x+c<2c+\square\qquad&(except\ x=c)\\ \Longrightarrow &|x+c|< |2c|+\square \end{alignat*} So we see that if we want  $|x+c||x-c|<(|2c|+\square)\square<\varepsilon$ to be true, we need to solve a positive solution of the quadratic inequality $\square^2+|2c|\square-\varepsilon<0$, by the relationship between roots and coefficient, we know $\square^2+|2c|\square-\varepsilon=0$ has exactly one positive and one negative root. Hence the solution of the prior inequality is $(-c-\sqrt{c^2+\varepsilon},-c+\sqrt{c^2+\varepsilon})$, where the right endpoint is positive. Thus, we pick $\delta=-c+\sqrt{c^2+\varepsilon}$ to complete the proof.",,['analysis']
85,Fourier Series for $f(x)=\sin(x)+\cos(2x)$,Fourier Series for,f(x)=\sin(x)+\cos(2x),Find the Fourier Series for $$f(x)=\sin(x)+\cos(2x)$$ I got $a_0=0$ which seems correct but I'm struggling with $a_n$ and $b_n$. Here are my attempts: $$\begin{align} a_n&=\frac{1}{2\pi} \overbrace{\int\limits_0^{2\pi} \cos(nx)\sin(x)dx}^{\text{odd function}}+\int\limits_0^{2\pi} \cos(nx)\cos(2x)dx\\  &= \frac{1}{2\pi} \left[\frac{\sin(nx)}{n}\cos(2x)\right]_0^{2\pi}-2\cdot\int\limits_0^{2\pi}\frac{\sin(nx)}{n}\sin(2x)\\ \end{align}$$ If I integrate the second term again I get an even function again and again. I need a fitting addtion theorem or something else. Any hints?,Find the Fourier Series for $$f(x)=\sin(x)+\cos(2x)$$ I got $a_0=0$ which seems correct but I'm struggling with $a_n$ and $b_n$. Here are my attempts: $$\begin{align} a_n&=\frac{1}{2\pi} \overbrace{\int\limits_0^{2\pi} \cos(nx)\sin(x)dx}^{\text{odd function}}+\int\limits_0^{2\pi} \cos(nx)\cos(2x)dx\\  &= \frac{1}{2\pi} \left[\frac{\sin(nx)}{n}\cos(2x)\right]_0^{2\pi}-2\cdot\int\limits_0^{2\pi}\frac{\sin(nx)}{n}\sin(2x)\\ \end{align}$$ If I integrate the second term again I get an even function again and again. I need a fitting addtion theorem or something else. Any hints?,,"['calculus', 'integration', 'analysis', 'fourier-series', 'trigonometric-integrals']"
86,Integral formula for the differential of matrix exponential,Integral formula for the differential of matrix exponential,,"This is a problem from Jacques Faraut's Analysis on Lie Groups . Given $A,X\in M(n,\mathbb{R})$, put $F(t)=\exp(t(A+X))$. In the first part of the problem we showed that $F$ is a solution to the integral equation $$F(t)-\int_0^t\exp((t-s)A)F(s)\,ds = \exp(tA).$$ This was straightforward. The next few parts are giving me trouble though. We define a sequence of maps $W_k(t)$ by $W_0(t) = \exp(tA)$ and $$W_k(t) = \int_0^t \exp((t-s)A)XW_{k-1}(s)\,ds.\tag{1}$$ The problem is to show that the series $\sum_{k=0}^\infty W_k(t)$ converges for each $t$, and that it converges to $F(t)$. We are then asked to prove the formula $$(D\exp)_AX = W_1(1)=\int_0^1\exp((1-s)A)X\exp(sA)\,ds.\tag{2}$$ I believe I have shown $\sum_{k=0}^\infty W_k(t)$ converges by taking the norm of (1) and applying induction, which let me compare the sum to a power series convergent for fixed $t$. I haven't the faintest clue how to show the series converges to $F(t)$ though. I tried converting $\exp$ to a formal sum and showing that $\sum_{k=0}^\infty W_k(t) = \sum_{j=0}^\infty t^j(X+A)^j/j!$, but the fact that $W_k$ is defined via recursive integration makes this difficult. I'm also unsure of how to prove formula (2), since it was unclear to me how to apply the previous parts. I've looked into other ways of proving the result (2), and most of them seem to rely on manipulating a two-parameter family $\Gamma(s,t) = \exp(-sX(t))\,\partial_t\!\exp(tX(t))$. My instincts are telling me that these two methods are roughly equivalent, but I can't tease out their relationship. Any hints/help/advice is greatly appreciated.","This is a problem from Jacques Faraut's Analysis on Lie Groups . Given $A,X\in M(n,\mathbb{R})$, put $F(t)=\exp(t(A+X))$. In the first part of the problem we showed that $F$ is a solution to the integral equation $$F(t)-\int_0^t\exp((t-s)A)F(s)\,ds = \exp(tA).$$ This was straightforward. The next few parts are giving me trouble though. We define a sequence of maps $W_k(t)$ by $W_0(t) = \exp(tA)$ and $$W_k(t) = \int_0^t \exp((t-s)A)XW_{k-1}(s)\,ds.\tag{1}$$ The problem is to show that the series $\sum_{k=0}^\infty W_k(t)$ converges for each $t$, and that it converges to $F(t)$. We are then asked to prove the formula $$(D\exp)_AX = W_1(1)=\int_0^1\exp((1-s)A)X\exp(sA)\,ds.\tag{2}$$ I believe I have shown $\sum_{k=0}^\infty W_k(t)$ converges by taking the norm of (1) and applying induction, which let me compare the sum to a power series convergent for fixed $t$. I haven't the faintest clue how to show the series converges to $F(t)$ though. I tried converting $\exp$ to a formal sum and showing that $\sum_{k=0}^\infty W_k(t) = \sum_{j=0}^\infty t^j(X+A)^j/j!$, but the fact that $W_k$ is defined via recursive integration makes this difficult. I'm also unsure of how to prove formula (2), since it was unclear to me how to apply the previous parts. I've looked into other ways of proving the result (2), and most of them seem to rely on manipulating a two-parameter family $\Gamma(s,t) = \exp(-sX(t))\,\partial_t\!\exp(tX(t))$. My instincts are telling me that these two methods are roughly equivalent, but I can't tease out their relationship. Any hints/help/advice is greatly appreciated.",,"['real-analysis', 'analysis', 'lie-groups', 'matrix-calculus']"
87,Integrability question with a function on a box in $\mathbb{R}^2$ (bounty added),Integrability question with a function on a box in  (bounty added),\mathbb{R}^2,"Let $f:\mathbb{R} \to\mathbb{R} \ be \ bounded,$ $\phi: \mathbb{R}^2 \to\mathbb{R}^2 $ be defined as $\phi(x,y)=(x,y+f(x))$ Prove that if for every bounded box $B\subset \mathbb{R}^2, \phi(B)$   is admissible (that means, $\mathbb{1}_{\phi(B)}$ is   Riemann-integrable), f is continuous almost everywhere. My work: I tried to assume f is not continuous almost everywhere. That means that there is a segment in $\mathbb{R}$ $[x1,x2]$ where f is not riemann-integrable. so we look at that segment and take the box defined by $(x1,y1),(x2,y2),(x2,y1),(x1,y2)$. We activate $\phi$ on that box. Now here is the part I'm not sure of.. Because $f$ is bounded (and that f is not riemann integrable in this segment) we can take a $y_c$ so that $y_c$ is between $y1+f(x)$ and $y2+f(x)$ in a dense set of the segment $[x1,x2]$ and is not between those values in a different dense set in that segment. Now we see that the Upper darboux sum in that segment of $\mathbb{1}_{\phi(B)}(x,y_c)=1*(x2-x1)$ and the Lower darboux sum is 0. That would mean we found a box B where $\phi(B)$ is not admissible. And that proves it. This is what I answered in my test and I lost 30 out of 40 points on   it, so I would appreciate it a lot if you tell me where I was wrong   and why. Thanks a lot.","Let $f:\mathbb{R} \to\mathbb{R} \ be \ bounded,$ $\phi: \mathbb{R}^2 \to\mathbb{R}^2 $ be defined as $\phi(x,y)=(x,y+f(x))$ Prove that if for every bounded box $B\subset \mathbb{R}^2, \phi(B)$   is admissible (that means, $\mathbb{1}_{\phi(B)}$ is   Riemann-integrable), f is continuous almost everywhere. My work: I tried to assume f is not continuous almost everywhere. That means that there is a segment in $\mathbb{R}$ $[x1,x2]$ where f is not riemann-integrable. so we look at that segment and take the box defined by $(x1,y1),(x2,y2),(x2,y1),(x1,y2)$. We activate $\phi$ on that box. Now here is the part I'm not sure of.. Because $f$ is bounded (and that f is not riemann integrable in this segment) we can take a $y_c$ so that $y_c$ is between $y1+f(x)$ and $y2+f(x)$ in a dense set of the segment $[x1,x2]$ and is not between those values in a different dense set in that segment. Now we see that the Upper darboux sum in that segment of $\mathbb{1}_{\phi(B)}(x,y_c)=1*(x2-x1)$ and the Lower darboux sum is 0. That would mean we found a box B where $\phi(B)$ is not admissible. And that proves it. This is what I answered in my test and I lost 30 out of 40 points on   it, so I would appreciate it a lot if you tell me where I was wrong   and why. Thanks a lot.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'riemann-integration']"
88,Estimate the value of f at a given point,Estimate the value of f at a given point,,"Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be a differentiable everywhere. Assume $f(-\sqrt2,-\sqrt2)=0$, and also that  $|\dfrac{\partial f}{\partial x}(x,y)|\le |\sin(x^2+y^2)|$ and $|\dfrac{\partial f}{\partial y}(x,y)|\le |\cos(x^2+y^2)|$ for each $(x,y) \in \mathbb{R}^2\setminus \{(0,0)\} $. Prove that $|f(\sqrt2,\sqrt2)|\le 4$. I tried to use the Taylor expansion at the point $(-\sqrt2,-\sqrt2)$ as follows, $f(x,y)=f(-\sqrt2,-\sqrt2)+ \dfrac{\partial f}{\partial x}(-\sqrt2,-\sqrt2)(x+\sqrt2)+\dfrac{\partial f}{\partial y}(x,y)(y+\sqrt2)+ \epsilon(||(x+\sqrt2,y+\sqrt2)||) $ $=\dfrac{\partial f}{\partial x}(-\sqrt2,-\sqrt2)(x+\sqrt2)+\dfrac{\partial f}{\partial y}(x,y)(y+\sqrt2)+ \epsilon(||(x+\sqrt2,y+\sqrt2)||).$ Using Cauchy-Schwartz inequality, we obtain the upper bound of $ \dfrac{\partial f}{\partial x}(-\sqrt2,-\sqrt2)(x+\sqrt2)+\dfrac{\partial f}{\partial y}(x,y)(y+\sqrt2)$ at the point $(\sqrt 2, \sqrt2)$ is 4. However, I don't know how to deal with the $\epsilon(||(x+\sqrt2,y+\sqrt2)||) $. I would appreciate it if someone could help me with this.","Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be a differentiable everywhere. Assume $f(-\sqrt2,-\sqrt2)=0$, and also that  $|\dfrac{\partial f}{\partial x}(x,y)|\le |\sin(x^2+y^2)|$ and $|\dfrac{\partial f}{\partial y}(x,y)|\le |\cos(x^2+y^2)|$ for each $(x,y) \in \mathbb{R}^2\setminus \{(0,0)\} $. Prove that $|f(\sqrt2,\sqrt2)|\le 4$. I tried to use the Taylor expansion at the point $(-\sqrt2,-\sqrt2)$ as follows, $f(x,y)=f(-\sqrt2,-\sqrt2)+ \dfrac{\partial f}{\partial x}(-\sqrt2,-\sqrt2)(x+\sqrt2)+\dfrac{\partial f}{\partial y}(x,y)(y+\sqrt2)+ \epsilon(||(x+\sqrt2,y+\sqrt2)||) $ $=\dfrac{\partial f}{\partial x}(-\sqrt2,-\sqrt2)(x+\sqrt2)+\dfrac{\partial f}{\partial y}(x,y)(y+\sqrt2)+ \epsilon(||(x+\sqrt2,y+\sqrt2)||).$ Using Cauchy-Schwartz inequality, we obtain the upper bound of $ \dfrac{\partial f}{\partial x}(-\sqrt2,-\sqrt2)(x+\sqrt2)+\dfrac{\partial f}{\partial y}(x,y)(y+\sqrt2)$ at the point $(\sqrt 2, \sqrt2)$ is 4. However, I don't know how to deal with the $\epsilon(||(x+\sqrt2,y+\sqrt2)||) $. I would appreciate it if someone could help me with this.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
89,"$f:\mathbb R^2 \to \mathbb R$ be a function , $|f(x)-f(y)|\ge 3\|x-y\| , \forall x,y \in \mathbb R^2$ ; is $f(\mathbb R^2)$ open in $\mathbb R$?","be a function ,  ; is  open in ?","f:\mathbb R^2 \to \mathbb R |f(x)-f(y)|\ge 3\|x-y\| , \forall x,y \in \mathbb R^2 f(\mathbb R^2) \mathbb R","Let $f:\mathbb R^2 \to \mathbb R$ be a function such that $|f(x)-f(y)|\ge 3\|x-y\| , \forall x,y \in \mathbb R^2$ , then is it true that $f$ maps open sets of $\mathbb R^2$ to open sets of $\mathbb R$ ? I can show that $f$ maps open sets to open sets in $f(\mathbb R^2)$ , so to prove the claim we only need to show that $f(\mathbb R^2)$ is open in $\mathbb R$ . Please help . Thanks in advance","Let $f:\mathbb R^2 \to \mathbb R$ be a function such that $|f(x)-f(y)|\ge 3\|x-y\| , \forall x,y \in \mathbb R^2$ , then is it true that $f$ maps open sets of $\mathbb R^2$ to open sets of $\mathbb R$ ? I can show that $f$ maps open sets to open sets in $f(\mathbb R^2)$ , so to prove the claim we only need to show that $f(\mathbb R^2)$ is open in $\mathbb R$ . Please help . Thanks in advance",,"['analysis', 'multivariable-calculus']"
90,Explicit functions evaluated,Explicit functions evaluated,,"(a) Defined $f$ by $f(y):=\int_0^\infty\frac{xy}{(x^4+y^4)^{3/4}}dx$. Prove $f(y)$ is defined (i.e integral exists) for every $y\in\mathbb{R}$. (b)Prove that actually $f(y)=c\operatorname{sign} y$ for some positive number $c$. *So in particular $f$ is not continuous at $0$, therefore a substitution may help and no need to evaluate $c$ explicitly. (c)Prove that $g(y):=\int_0^\infty\frac{xy}{(x^4+y^4+x^2)^{3/4}}dx$ DOES define a function that is continuous for all $y\in \mathbb{R}$. These are a few examples I ran across in the midst of a self study. Please, any help is appreciated.","(a) Defined $f$ by $f(y):=\int_0^\infty\frac{xy}{(x^4+y^4)^{3/4}}dx$. Prove $f(y)$ is defined (i.e integral exists) for every $y\in\mathbb{R}$. (b)Prove that actually $f(y)=c\operatorname{sign} y$ for some positive number $c$. *So in particular $f$ is not continuous at $0$, therefore a substitution may help and no need to evaluate $c$ explicitly. (c)Prove that $g(y):=\int_0^\infty\frac{xy}{(x^4+y^4+x^2)^{3/4}}dx$ DOES define a function that is continuous for all $y\in \mathbb{R}$. These are a few examples I ran across in the midst of a self study. Please, any help is appreciated.",,"['real-analysis', 'analysis', 'definite-integrals', 'continuity']"
91,Vector Calculus: solution to Poisson equation,Vector Calculus: solution to Poisson equation,,This is problem 8.4.17. from Marsden Vector Calculus book. Let $\rho$ be a continuous function which vanishes outside a 3D region $W$ . Define $\phi(\textbf{p})=\displaystyle\iiint_W\frac{\rho(\textbf{q})}{4\pi\lVert\textbf{p}-\textbf{q}\rVert}dV(\textbf{q})$ . How to show that $\displaystyle\iint_{\partial W}\nabla\phi\cdot\textbf{dS}=-\iiint_W\rho\; dV$ without the knowledege that $\phi$ is the solution to the Poisson equation $\nabla^2\phi=-\rho$ (which will be a consequence of this)?,This is problem 8.4.17. from Marsden Vector Calculus book. Let be a continuous function which vanishes outside a 3D region . Define . How to show that without the knowledege that is the solution to the Poisson equation (which will be a consequence of this)?,\rho W \phi(\textbf{p})=\displaystyle\iiint_W\frac{\rho(\textbf{q})}{4\pi\lVert\textbf{p}-\textbf{q}\rVert}dV(\textbf{q}) \displaystyle\iint_{\partial W}\nabla\phi\cdot\textbf{dS}=-\iiint_W\rho\; dV \phi \nabla^2\phi=-\rho,"['linear-algebra', 'analysis', 'multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
92,Landau Notation - Practical explanations,Landau Notation - Practical explanations,,"Someone told me this week that the Landau notation is very pratical in general in analysis. Definition : Let the function $\phi$ defined on an open set containing $x_0$.We want to compare $f$ à $\phi$: we want to know if   $\displaystyle{\left\vert\frac{f}{\phi}\right\vert}$ is bound or if   the limit is nul at this point; but, we can write the rapport only if   $ \phi$ doesn't vanish. We say that  $ f\in o(\phi)$ in the neighborhood of $ x_0$ if and only   if for all  $ \varepsilon >0$ there exists  $ \eta>0$ such that $ \displaystyle{\forall x\in ]x_0-\eta,x_0+\eta[, \vert  f(x)\vert<\varepsilon \vert\phi(x)\vert}$ If $ \phi$ doesn't vanish, we have : $ \displaystyle{\forall x\in  ]x_0-\eta,x_0+\eta[,  \left\vert\frac{f(x)}{\phi(x)}\right\vert<\varepsilon }$ We say in   this case that $ f$ is negligible in front of $ \phi$ in a    neighborhood of  $ x_0$. I would like to know why it is important to embrace this notation and master the concept. Does someone could explain to me the importance of this notation (With examples, comparative, etc.)? The question may be silly, but sometimes I wonder about certain topic without being myself able to answer this question. I'm a little young (13 years old), and still have a lot to learn in mathematics. Thanks!","Someone told me this week that the Landau notation is very pratical in general in analysis. Definition : Let the function $\phi$ defined on an open set containing $x_0$.We want to compare $f$ à $\phi$: we want to know if   $\displaystyle{\left\vert\frac{f}{\phi}\right\vert}$ is bound or if   the limit is nul at this point; but, we can write the rapport only if   $ \phi$ doesn't vanish. We say that  $ f\in o(\phi)$ in the neighborhood of $ x_0$ if and only   if for all  $ \varepsilon >0$ there exists  $ \eta>0$ such that $ \displaystyle{\forall x\in ]x_0-\eta,x_0+\eta[, \vert  f(x)\vert<\varepsilon \vert\phi(x)\vert}$ If $ \phi$ doesn't vanish, we have : $ \displaystyle{\forall x\in  ]x_0-\eta,x_0+\eta[,  \left\vert\frac{f(x)}{\phi(x)}\right\vert<\varepsilon }$ We say in   this case that $ f$ is negligible in front of $ \phi$ in a    neighborhood of  $ x_0$. I would like to know why it is important to embrace this notation and master the concept. Does someone could explain to me the importance of this notation (With examples, comparative, etc.)? The question may be silly, but sometimes I wonder about certain topic without being myself able to answer this question. I'm a little young (13 years old), and still have a lot to learn in mathematics. Thanks!",,['analysis']
93,Proving stokes theorem on chains (spivak),Proving stokes theorem on chains (spivak),,"I am having troubles with the formula after ""note that"": I don't see how the formula is $0$ if $j \not= i$. If I calculate $(I^k_{(j,\alpha)})^{\star} (fdx^1\wedge ... \wedge \hat{dx^i} \wedge ... \wedge dx^k)$   I will get $$f(I^k_{(j,\alpha)}(x^1,...,x^{k-1}))(dx^1\wedge ... \wedge \hat{dx^i} \wedge ... \wedge dx^k)((I^k_{(j,\alpha)}(x^1,...,x^{k-1}))(DI^k_{(j,\alpha)}(...))$$ but the matrix representing $DI^k_{(j,\alpha)}$ will have diagonal 1's everywhere except the jth row will be a 0 row. I can't see how this could possibly lead to the result stated any help please","I am having troubles with the formula after ""note that"": I don't see how the formula is $0$ if $j \not= i$. If I calculate $(I^k_{(j,\alpha)})^{\star} (fdx^1\wedge ... \wedge \hat{dx^i} \wedge ... \wedge dx^k)$   I will get $$f(I^k_{(j,\alpha)}(x^1,...,x^{k-1}))(dx^1\wedge ... \wedge \hat{dx^i} \wedge ... \wedge dx^k)((I^k_{(j,\alpha)}(x^1,...,x^{k-1}))(DI^k_{(j,\alpha)}(...))$$ but the matrix representing $DI^k_{(j,\alpha)}$ will have diagonal 1's everywhere except the jth row will be a 0 row. I can't see how this could possibly lead to the result stated any help please",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'differential-geometry']"
94,$u_{xx}$ and $u_{yy}$ continuous $\Rightarrow u$ is continuous,and  continuous  is continuous,u_{xx} u_{yy} \Rightarrow u,"I found an exciting question in a scriptum: Let $U\subset\mathbb{R}^2$ open and convex, $u:U\to\mathbb{R}$. Assume the existence of the continuous partial derivatives $u_{yy}$ and $u_{xx}$ satisfying $u_{yy}=u_{xx}$. Note that the existence of the mixed partial derivatives is not explicitly assumed. Can I conclude that the function $u$ is continuous? Otherwise should I need more assumptions?","I found an exciting question in a scriptum: Let $U\subset\mathbb{R}^2$ open and convex, $u:U\to\mathbb{R}$. Assume the existence of the continuous partial derivatives $u_{yy}$ and $u_{xx}$ satisfying $u_{yy}=u_{xx}$. Note that the existence of the mixed partial derivatives is not explicitly assumed. Can I conclude that the function $u$ is continuous? Otherwise should I need more assumptions?",,"['real-analysis', 'analysis']"
95,Question about Sets with a Certain Property,Question about Sets with a Certain Property,,"I've been working on answering this question on and off for a while (months). I can't seem to solve it, and I've presented it to a few people who also cannot solve it. I will present a special case of the problem here, as I intuitively suspect that if a counterexample can be found, it will be found in $ \mathbb{R}$. $S \subseteq \mathbb{R}$ is said to have ""Property 1"" if and only if: $\forall m \in \mathbb{R}$, $\exists$ $k \in \mathbb{R}$ such that $m+k \in S$ and $m-k \in S$. Prove or disprove the following: If $S$ has ""Property 1"", then $\exists$ $S' \subseteq S$ such that $S'$ has ""Property 1"" and $\forall$ $C \subseteq S'$, $C \neq \emptyset$, $S'-C$ does not have ""Property 1."" I attempted a proof using Zorn's Lemma and quickly discovered that in order to even satisfy the hypothesis of said lemma, I would pretty much have to assume what we're trying to prove here (unless there's some clever way around this I'm not seeing). I didn't want to post this anywhere, I really wanted to prove it myself, but I'm having quite a bit of trouble. Any insight would be appreciated.","I've been working on answering this question on and off for a while (months). I can't seem to solve it, and I've presented it to a few people who also cannot solve it. I will present a special case of the problem here, as I intuitively suspect that if a counterexample can be found, it will be found in $ \mathbb{R}$. $S \subseteq \mathbb{R}$ is said to have ""Property 1"" if and only if: $\forall m \in \mathbb{R}$, $\exists$ $k \in \mathbb{R}$ such that $m+k \in S$ and $m-k \in S$. Prove or disprove the following: If $S$ has ""Property 1"", then $\exists$ $S' \subseteq S$ such that $S'$ has ""Property 1"" and $\forall$ $C \subseteq S'$, $C \neq \emptyset$, $S'-C$ does not have ""Property 1."" I attempted a proof using Zorn's Lemma and quickly discovered that in order to even satisfy the hypothesis of said lemma, I would pretty much have to assume what we're trying to prove here (unless there's some clever way around this I'm not seeing). I didn't want to post this anywhere, I really wanted to prove it myself, but I'm having quite a bit of trouble. Any insight would be appreciated.",,['analysis']
96,Proving a theorem related to convex cone,Proving a theorem related to convex cone,,"Let $V$ be a finite dimensional real Euclidean space. Let $C$ be a non-empty convex cone in $V$. I need to prove that the following are equivalent. int($C$) is non-empty. $C$ contains a basis of $V$. $C-C=V$. I have proved 1 implies 3 and 2 implies 3. I tried proving 3 implies 2 as follows. Let $C-C=V$. I need to prove that $C$ contains a basis of $V$. Suppose C doesn't contain a basis of V. Let $\{v_{1},...v_{r}\}$ be a maximal linearly independent set in $C$ where $r<n$=dim$V$. I know that $C-C$ is the smallest subspace of $V$ containing $C$. So if I can prove that $C \subset  span\{v_{1},...v_{r}\}$ which is a proper subspace of $V=C-C$, I will get a contradiction. But I'm not able to prove that $C \subset  span\{v_{1},...v_{r}\}$. Is my approach correct? Can someone help to prove it?! Also I need help to prove either 2 implies 1 or 3 implies 1.","Let $V$ be a finite dimensional real Euclidean space. Let $C$ be a non-empty convex cone in $V$. I need to prove that the following are equivalent. int($C$) is non-empty. $C$ contains a basis of $V$. $C-C=V$. I have proved 1 implies 3 and 2 implies 3. I tried proving 3 implies 2 as follows. Let $C-C=V$. I need to prove that $C$ contains a basis of $V$. Suppose C doesn't contain a basis of V. Let $\{v_{1},...v_{r}\}$ be a maximal linearly independent set in $C$ where $r<n$=dim$V$. I know that $C-C$ is the smallest subspace of $V$ containing $C$. So if I can prove that $C \subset  span\{v_{1},...v_{r}\}$ which is a proper subspace of $V=C-C$, I will get a contradiction. But I'm not able to prove that $C \subset  span\{v_{1},...v_{r}\}$. Is my approach correct? Can someone help to prove it?! Also I need help to prove either 2 implies 1 or 3 implies 1.",,"['linear-algebra', 'analysis']"
97,Prove Piecewise Function Integrable,Prove Piecewise Function Integrable,,"$$ f(x) = \begin{cases} -2, & \text{if }x < 0 \\ 1, & \text{if }x > 0\\ 0, & \text{if }x = 0 \end{cases} $$ Hey guys I need some help showing that this function is integrable on the closed interval $[-1,2]$. So far my idea has been to show $$U(f,P)-L(f,P) < \epsilon$$ for some $\epsilon>0$. The only problem is the point $(0,0)$ on the function. I don't understand how to handle that. Can I just say that $U(f,P)$ for some partition will equal to $3$ and then find a partition $P$ for which $$3-L(f,P)<\epsilon?$$","$$ f(x) = \begin{cases} -2, & \text{if }x < 0 \\ 1, & \text{if }x > 0\\ 0, & \text{if }x = 0 \end{cases} $$ Hey guys I need some help showing that this function is integrable on the closed interval $[-1,2]$. So far my idea has been to show $$U(f,P)-L(f,P) < \epsilon$$ for some $\epsilon>0$. The only problem is the point $(0,0)$ on the function. I don't understand how to handle that. Can I just say that $U(f,P)$ for some partition will equal to $3$ and then find a partition $P$ for which $$3-L(f,P)<\epsilon?$$",,"['real-analysis', 'integration', 'analysis', 'continuity', 'riemann-integration']"
98,Find $\lim (a_{n+1}^\alpha-a_n^\alpha)$,Find,\lim (a_{n+1}^\alpha-a_n^\alpha),"Assume $\alpha \in (0,1)$, and $\{a_n\}$ is a strictly monotone increasing positive series. and $\{a_{n+1}-a_n\}$ is bounded. Find $$\lim_{n \rightarrow \infty}(a_{n+1}^\alpha - a_{n}^\alpha)$$. My idea is first proving for rational numbers , then use a rational sequences to approximate real numbers. But I can only prove for rational numbers. If $\alpha \in \Bbb{Q} \cap (0,1)$: $$a_{n+1}^\alpha - a_{n}^\alpha = a_{n+1}^{\frac{q}{p}} - a_{n}^{\frac{q}{p}}=\frac{\left((a_{n+1}^q)^{\frac{1}{p}}-(a_{n}^q)^{\frac{1}{p}}\right)\left((a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}\right)}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}}=\frac{a_{n+1}^q-a_n^q}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}}=(a_{n+1}-a_n)\frac{(a_{n+1}^{q-1}+\cdots+a_n^{q-1})}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}}$$ For $\{a_n\}$ is a strictly monotone increasing positive series. then $\lim a_n = a$ or $\lim a_n = +\infty$ If $\lim a_n = a$. then $\exists M>0$. such that $$\left| \frac{(a_{n+1}^{q-1}+\cdots+a_n^{q-1})}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}} \right|<M$$. So $0\leq\lim (a_{n+1}^\alpha-a_n^\alpha) \leq \lim (a_{n+1}-a_n) M=0$ If $\lim a_n = +\infty$, then $$\frac{(a_{n+1}^{q-1}+\cdots+a_n^{q-1})}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}} \rightarrow 0$$ So for rational numbers, $\lim_{n \rightarrow \infty} (a_{n+1}^\alpha - a_n^\alpha) = 0$ For $\alpha \in (0,1)$, I choose $\alpha_k \in \Bbb{Q}\cap (0,1)$, and $\lim \alpha_k = \alpha$.  But I can't construct the following relationship $$\lim_{n\rightarrow \infty}\lim_{k\rightarrow \infty}(a_{n+1}^{\alpha_k}-a_{n}^{\alpha_k}) = \lim_{k\rightarrow \infty}\lim_{n\rightarrow \infty}(a_{n+1}^{\alpha_k}-a_{n}^{\alpha_k})$$ I want to follow this way to extend the index from rational numbers to real numbers. But I failed. Can we solve this problem using this method? Is there any other way to solve this problem?","Assume $\alpha \in (0,1)$, and $\{a_n\}$ is a strictly monotone increasing positive series. and $\{a_{n+1}-a_n\}$ is bounded. Find $$\lim_{n \rightarrow \infty}(a_{n+1}^\alpha - a_{n}^\alpha)$$. My idea is first proving for rational numbers , then use a rational sequences to approximate real numbers. But I can only prove for rational numbers. If $\alpha \in \Bbb{Q} \cap (0,1)$: $$a_{n+1}^\alpha - a_{n}^\alpha = a_{n+1}^{\frac{q}{p}} - a_{n}^{\frac{q}{p}}=\frac{\left((a_{n+1}^q)^{\frac{1}{p}}-(a_{n}^q)^{\frac{1}{p}}\right)\left((a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}\right)}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}}=\frac{a_{n+1}^q-a_n^q}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}}=(a_{n+1}-a_n)\frac{(a_{n+1}^{q-1}+\cdots+a_n^{q-1})}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}}$$ For $\{a_n\}$ is a strictly monotone increasing positive series. then $\lim a_n = a$ or $\lim a_n = +\infty$ If $\lim a_n = a$. then $\exists M>0$. such that $$\left| \frac{(a_{n+1}^{q-1}+\cdots+a_n^{q-1})}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}} \right|<M$$. So $0\leq\lim (a_{n+1}^\alpha-a_n^\alpha) \leq \lim (a_{n+1}-a_n) M=0$ If $\lim a_n = +\infty$, then $$\frac{(a_{n+1}^{q-1}+\cdots+a_n^{q-1})}{(a_{n+1}^q)^{\frac{p-1}{p}}+(a_{n+1}^q)^{\frac{p-2}{p}}(a_{n}^q)^{\frac{1}{p}}+\cdots+(a_{n}^q)^{\frac{p-1}{p}}} \rightarrow 0$$ So for rational numbers, $\lim_{n \rightarrow \infty} (a_{n+1}^\alpha - a_n^\alpha) = 0$ For $\alpha \in (0,1)$, I choose $\alpha_k \in \Bbb{Q}\cap (0,1)$, and $\lim \alpha_k = \alpha$.  But I can't construct the following relationship $$\lim_{n\rightarrow \infty}\lim_{k\rightarrow \infty}(a_{n+1}^{\alpha_k}-a_{n}^{\alpha_k}) = \lim_{k\rightarrow \infty}\lim_{n\rightarrow \infty}(a_{n+1}^{\alpha_k}-a_{n}^{\alpha_k})$$ I want to follow this way to extend the index from rational numbers to real numbers. But I failed. Can we solve this problem using this method? Is there any other way to solve this problem?",,"['sequences-and-series', 'analysis']"
99,"Suppose f is uniformly continuous on I and J. Prove that if I intersecting J are non-empty, then f is uniformly continuous on I union J.","Suppose f is uniformly continuous on I and J. Prove that if I intersecting J are non-empty, then f is uniformly continuous on I union J.",,"Problem: Let I and J be two non-degenerate intervals and suppose f is uniformly continuous on I and J. Prove that if I intersecting J are non-empty, then f is uniformly continuous on I union J. I understand what non-degenerate means. But my thought was since f is uniformly continuous on I and J, then it is clearly uniformly continuous on I union J. This is incorrect though...Help!","Problem: Let I and J be two non-degenerate intervals and suppose f is uniformly continuous on I and J. Prove that if I intersecting J are non-empty, then f is uniformly continuous on I union J. I understand what non-degenerate means. But my thought was since f is uniformly continuous on I and J, then it is clearly uniformly continuous on I union J. This is incorrect though...Help!",,"['real-analysis', 'analysis']"
