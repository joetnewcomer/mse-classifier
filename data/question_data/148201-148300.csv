,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is there an intuitive explanation for the maximal ergodic theorem?,Is there an intuitive explanation for the maximal ergodic theorem?,,The maximal ergodic theorem states that for an $L^1$ integrable function the set $$E(f)=\{x : \max_n\left(\sum_{i=0}^{n-1}f(T^i(x))\right)>0\}$$ has the property that $\int_{E(f)} f d \mu \geq 0$ .  The only time I've seen it used is to prove Birkhoff's ergodic theorem by showing that the set of points where the $\limsup$ and $\liminf$ of the average of the interations of $T$ is not equal has measure zero.  Does the maximal ergodic theorem say anything interesting or is it just a clever tool?,The maximal ergodic theorem states that for an integrable function the set has the property that .  The only time I've seen it used is to prove Birkhoff's ergodic theorem by showing that the set of points where the and of the average of the interations of is not equal has measure zero.  Does the maximal ergodic theorem say anything interesting or is it just a clever tool?,L^1 E(f)=\{x : \max_n\left(\sum_{i=0}^{n-1}f(T^i(x))\right)>0\} \int_{E(f)} f d \mu \geq 0 \limsup \liminf T,"['real-analysis', 'analysis', 'measure-theory', 'dynamical-systems', 'ergodic-theory']"
1,Gettings bounds for seminorms from bound of absolute value,Gettings bounds for seminorms from bound of absolute value,,"On a compact domain $\Omega \subseteq \mathbb{R}^d$ , we have a function $u(x) \in C^{\infty}(\Omega)$ with an approximation $u_h(x)$ with the following properties: $$ |u(x) - u_h (x)| \leq C h^{m+1} |u(x)|_{C^{m+1}(\Omega)}, $$ where the seminorm on the right is defined as $ |u(x)|_{C^{m+1}(\Omega)} = \text{max}_{|\alpha|=m+1} \|D^{\alpha} u\|_{L^{\infty}} $ , with the derivative being defined using the multi-index notation: $$ D^{\alpha} f = \frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1} \cdots\partial x_n^{\alpha_n}}, \quad \alpha = (\alpha_1, \cdots, \alpha_n), $$ for $|\alpha| = \alpha_1 + \alpha_2 + \cdots \alpha_n| \leq k$ , $x \in \mathbb{R}^d$ , $f \in C^k (\Omega)$ . I want to use that approximation to derive similar bound for the seminorm $|v|^2_{k, \Omega}$ defined as the sum of all the squares of the $L^2$ norms of all the derivatives of order $k$ . So in 2D, this seminorm would look like: $$ \|v\|^2_{0, \Omega} = |v|^2_{0, \Omega} = \int_{\Omega} v^2 d\Omega \\ |v|^2_{1, \Omega} = \left| \frac{\partial v}{\partial x_1} \right|^2_{0, \Omega} + \left| \frac{\partial v}{\partial x_2} \right|^2_{0, \Omega}  \\ |v|^2_{2, \Omega} = \left| \frac{\partial^2 v}{\partial x_1^2} \right|^2_{0, \Omega} + \left| \frac{\partial^2 v}{\partial x_1 \partial x_2} \right|^2_{0, \Omega} + \left| \frac{\partial^2 v}{\partial^2 x_2} \right|^2_{0, \Omega} $$ To be more precise, I'm trying to arrive at some bound for $|u(x) - u_h(x)|_{s, \Omega}$ , with $0 \leq s < m$ . Any suggestions how I could achieve that?","On a compact domain , we have a function with an approximation with the following properties: where the seminorm on the right is defined as , with the derivative being defined using the multi-index notation: for , , . I want to use that approximation to derive similar bound for the seminorm defined as the sum of all the squares of the norms of all the derivatives of order . So in 2D, this seminorm would look like: To be more precise, I'm trying to arrive at some bound for , with . Any suggestions how I could achieve that?","\Omega \subseteq \mathbb{R}^d u(x) \in C^{\infty}(\Omega) u_h(x) 
|u(x) - u_h (x)| \leq C h^{m+1} |u(x)|_{C^{m+1}(\Omega)},
 
|u(x)|_{C^{m+1}(\Omega)} = \text{max}_{|\alpha|=m+1} \|D^{\alpha} u\|_{L^{\infty}}
 
D^{\alpha} f = \frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1} \cdots\partial x_n^{\alpha_n}}, \quad \alpha = (\alpha_1, \cdots, \alpha_n),
 |\alpha| = \alpha_1 + \alpha_2 + \cdots \alpha_n| \leq k x \in \mathbb{R}^d f \in C^k (\Omega) |v|^2_{k, \Omega} L^2 k 
\|v\|^2_{0, \Omega} = |v|^2_{0, \Omega} = \int_{\Omega} v^2 d\Omega \\
|v|^2_{1, \Omega} = \left| \frac{\partial v}{\partial x_1} \right|^2_{0, \Omega} + \left| \frac{\partial v}{\partial x_2} \right|^2_{0, \Omega}  \\
|v|^2_{2, \Omega} = \left| \frac{\partial^2 v}{\partial x_1^2} \right|^2_{0, \Omega} + \left| \frac{\partial^2 v}{\partial x_1 \partial x_2} \right|^2_{0, \Omega} + \left| \frac{\partial^2 v}{\partial^2 x_2} \right|^2_{0, \Omega}
 |u(x) - u_h(x)|_{s, \Omega} 0 \leq s < m","['analysis', 'normed-spaces', 'upper-lower-bounds']"
2,Bound of the Wave Equation in 3 dimensions,Bound of the Wave Equation in 3 dimensions,,"For n=3, $u(x,t)\in C^2$ and $\Box u=0$ , $x\in\mathbb{R}^3,\ t\geq0$ . Assume U(t)= $\sum_{|a|\leq2}\int_{\mathbb{R}^3} |D^au(x,t)|dx<\infty$ for $t=0$ . a) Show that there exists a constant K independent of u such that: $|u(x,t)|\leq \frac{K}{t}U(0)$ for $t>0$ Hint: Write the integrand in Kirchoff's formula as: $\sum_i[\frac{1}{ct}(th(y)+g(y))(y_i-x_i)+ctg_{y_i}(y)]\xi_i $ where $\xi_i=\frac{y_i-x_i}{ct}$ are the directions cosines of the exterior surface normal. Convert the integral to one extended over the solid sphere $|y-x|<ct$ . b)Show that $\lim_{t\rightarrow\infty}\frac{U(t)}{t}=0$ Implies that u vanishes identically. Hint: Apply (a) to the function $v(x,t,T)=u(x,T-t)$ for large T. Exercise 4, page 133 of Partial Differential Equations, John F. My attempt was: $u(x,t)=\frac{1}{4\pi c^2 t^2}\sum\int_{\partial B}\frac{1}{2ct}(th(y)+g(y))D_{y_i}\lvert y-x\rvert^2 \xi_i+ctD_{y_i}g(y)\xi_i dS(y)$ . Then using Green's identities to transform it into: $u(x,t)= \frac{1}{4\pi c^2 t^2}\sum\int_{B}\frac{1}{2ct}(th(y)+ g(y))\Delta_{y_i}\lvert y-x\rvert^2+ \frac{1}{2ct}D_{y_i}(th(y)+g(y))D_{y_i}\lvert y-x\rvert^2+ct \Delta_{y_i}g(y)\ dy$ $u(x,t)=\frac{1}{4\pi c^2 t^2}\sum\int_{B}\frac{1}{ct}(th(y)+g(y))+ \frac{1}{ct}D_{y_i}(th(y)+g(y))( y_i-x_i)+ct\Delta_{y_i}g(y)\ dy$ . $u(x,t)\leq\frac{1}{4\pi c^2 t}\int_R |\Delta g| + \frac{1}{t}|Dg| + \frac{3}{c^2t^2}|g| + |Dh| +\frac{3}{ct}|h|dy$ . For $ct>1$ there is a constant K such as $|u(x,t)|\leq\frac{K}{t}U(0)$ . Based on page 17 $U(0)=\int_R|g|+|h|+|Dg|+|Dh|+|D^2g|dy$ . But how is it computed? b) $v(x,t,T)=u(x,T-t)\implies v(x,T-t,T)=u(x,t)$ $|u(x,t)|=v(x,T-t,T)\leq\frac{K}{T-t}U(T)$ , therefore for $T\rightarrow\infty\implies |u(x,t)=0|$","For n=3, and , . Assume U(t)= for . a) Show that there exists a constant K independent of u such that: for Hint: Write the integrand in Kirchoff's formula as: where are the directions cosines of the exterior surface normal. Convert the integral to one extended over the solid sphere . b)Show that Implies that u vanishes identically. Hint: Apply (a) to the function for large T. Exercise 4, page 133 of Partial Differential Equations, John F. My attempt was: . Then using Green's identities to transform it into: . . For there is a constant K such as . Based on page 17 . But how is it computed? b) , therefore for","u(x,t)\in C^2 \Box u=0 x\in\mathbb{R}^3,\ t\geq0 \sum_{|a|\leq2}\int_{\mathbb{R}^3} |D^au(x,t)|dx<\infty t=0 |u(x,t)|\leq \frac{K}{t}U(0) t>0 \sum_i[\frac{1}{ct}(th(y)+g(y))(y_i-x_i)+ctg_{y_i}(y)]\xi_i  \xi_i=\frac{y_i-x_i}{ct} |y-x|<ct \lim_{t\rightarrow\infty}\frac{U(t)}{t}=0 v(x,t,T)=u(x,T-t) u(x,t)=\frac{1}{4\pi c^2 t^2}\sum\int_{\partial B}\frac{1}{2ct}(th(y)+g(y))D_{y_i}\lvert y-x\rvert^2 \xi_i+ctD_{y_i}g(y)\xi_i dS(y) u(x,t)= \frac{1}{4\pi c^2 t^2}\sum\int_{B}\frac{1}{2ct}(th(y)+
g(y))\Delta_{y_i}\lvert y-x\rvert^2+ \frac{1}{2ct}D_{y_i}(th(y)+g(y))D_{y_i}\lvert y-x\rvert^2+ct \Delta_{y_i}g(y)\ dy u(x,t)=\frac{1}{4\pi c^2 t^2}\sum\int_{B}\frac{1}{ct}(th(y)+g(y))+ \frac{1}{ct}D_{y_i}(th(y)+g(y))( y_i-x_i)+ct\Delta_{y_i}g(y)\ dy u(x,t)\leq\frac{1}{4\pi c^2 t}\int_R |\Delta g| + \frac{1}{t}|Dg| + \frac{3}{c^2t^2}|g| + |Dh| +\frac{3}{ct}|h|dy ct>1 |u(x,t)|\leq\frac{K}{t}U(0) U(0)=\int_R|g|+|h|+|Dg|+|Dh|+|D^2g|dy v(x,t,T)=u(x,T-t)\implies v(x,T-t,T)=u(x,t) |u(x,t)|=v(x,T-t,T)\leq\frac{K}{T-t}U(T) T\rightarrow\infty\implies |u(x,t)=0|","['analysis', 'multivariable-calculus', 'inequality', 'partial-differential-equations']"
3,Prove that the following statement involving Buchstab function is true,Prove that the following statement involving Buchstab function is true,,"I came across this problem in this book called Cambridge studies in advanced mathematics 97, page 217-218. I wad reading this line where the author starts the inductive steps and apply the inductive hypothesis and yields $$\Phi(x,x^{\frac{1}{u}}) = \frac{U\omega(U)x}{\ln(x)}+O\bigg(\frac{x}{\ln^2(x)}\bigg)+\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \bigg(\frac{u_p\omega(u_p)x}{p\ln(\frac{x}{p})}+O\bigg(\frac{p}{\ln(p)}\bigg)+O\bigg(\frac{x}{p\ln^2(x)}\bigg)\bigg)$$ which indeed is $$\Phi(x,x^{\frac{1}{u}}) = \frac{U\omega(U)x}{\ln(x)}+O\bigg(\frac{x}{\ln^2(x)}\bigg)+O\bigg(\frac{x^{\frac{2}{U}}}{\ln^2(x)}\bigg)+O\bigg(\frac{x}{\ln^2(x)}\bigg)+\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{u_p\omega(u_p)x}{p\ln(\frac{x}{p})}$$ since the author wrote that the sum over $p$ of the first error term is $\ll x^{\frac{2}{U}}/\ln^2(x)$ , and the sum over $p$ of the second is $\ll x/\ln^2(x)$ . The author then estimate the contribution of the main term in the sum by writing the Prime Number Theorem in the form $\pi(t)=\mathrm{li}(t)+R(t)$ , then apply Riemann-Stieltjes integration and yields $$\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^2(t)}\,dt+f(t)R(t)\bigg|_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}}-\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} R(t)\,df(t)$$ where $$f(t)=\frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln(t)}$$ which in fact it is $$\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^2(t)}\,dt+\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln(t)}\,dR(t) $$ before applying Riemann-Stieltjes integration. I noticed that for large $t$ , we have $$dR(t)=\bigg(\frac{\ln(t)-1}{\ln^2(t)}-\frac{1}{\ln(t)}\bigg)\,dt=-\frac{dt}{\ln^2(t)}$$ I am also reading this paper on Dickman function and noticed the similar situation $$\sum_{x^d<p\leq x^{u_1}} \rho\bigg(\frac{\ln(p)}{\ln(x)-\ln(p)}\bigg)\frac{x}{p}=x\int_{x^d}^{x^{u_1}} \rho\bigg(\frac{\ln(t)}{\ln(x)-\ln(t)}\bigg)\,dF(t)$$ where $F(t)=\sum_{p\leq t} \frac{1}{p}$ , therefore I guess that $\frac{1}{p\ln(p)}$ could also be expressed as such and my conjecture would be $$\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{(\ln(t)-1)\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^3(t)}\,dt$$ The difficulty that I am facing is to prove the conjecture I made above, but currently I have no idea how this can be achieved. Is this conjecture correct and if it is correct, then how do I show it? Therefore I hope someone could explain the things that I am missing.","I came across this problem in this book called Cambridge studies in advanced mathematics 97, page 217-218. I wad reading this line where the author starts the inductive steps and apply the inductive hypothesis and yields which indeed is since the author wrote that the sum over of the first error term is , and the sum over of the second is . The author then estimate the contribution of the main term in the sum by writing the Prime Number Theorem in the form , then apply Riemann-Stieltjes integration and yields where which in fact it is before applying Riemann-Stieltjes integration. I noticed that for large , we have I am also reading this paper on Dickman function and noticed the similar situation where , therefore I guess that could also be expressed as such and my conjecture would be The difficulty that I am facing is to prove the conjecture I made above, but currently I have no idea how this can be achieved. Is this conjecture correct and if it is correct, then how do I show it? Therefore I hope someone could explain the things that I am missing.","\Phi(x,x^{\frac{1}{u}}) = \frac{U\omega(U)x}{\ln(x)}+O\bigg(\frac{x}{\ln^2(x)}\bigg)+\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \bigg(\frac{u_p\omega(u_p)x}{p\ln(\frac{x}{p})}+O\bigg(\frac{p}{\ln(p)}\bigg)+O\bigg(\frac{x}{p\ln^2(x)}\bigg)\bigg) \Phi(x,x^{\frac{1}{u}}) = \frac{U\omega(U)x}{\ln(x)}+O\bigg(\frac{x}{\ln^2(x)}\bigg)+O\bigg(\frac{x^{\frac{2}{U}}}{\ln^2(x)}\bigg)+O\bigg(\frac{x}{\ln^2(x)}\bigg)+\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{u_p\omega(u_p)x}{p\ln(\frac{x}{p})} p \ll x^{\frac{2}{U}}/\ln^2(x) p \ll x/\ln^2(x) \pi(t)=\mathrm{li}(t)+R(t) \sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^2(t)}\,dt+f(t)R(t)\bigg|_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}}-\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} R(t)\,df(t) f(t)=\frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln(t)} \sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^2(t)}\,dt+\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln(t)}\,dR(t)  t dR(t)=\bigg(\frac{\ln(t)-1}{\ln^2(t)}-\frac{1}{\ln(t)}\bigg)\,dt=-\frac{dt}{\ln^2(t)} \sum_{x^d<p\leq x^{u_1}} \rho\bigg(\frac{\ln(p)}{\ln(x)-\ln(p)}\bigg)\frac{x}{p}=x\int_{x^d}^{x^{u_1}} \rho\bigg(\frac{\ln(t)}{\ln(x)-\ln(t)}\bigg)\,dF(t) F(t)=\sum_{p\leq t} \frac{1}{p} \frac{1}{p\ln(p)} \sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{(\ln(t)-1)\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^3(t)}\,dt","['real-analysis', 'number-theory', 'analysis', 'analytic-number-theory']"
4,definition of the Fréchet derivative: why assume it is a bounded linear map?,definition of the Fréchet derivative: why assume it is a bounded linear map?,,"Definition (in our lecture): Let $X,Y$ be normed vector spaces and $f:U \to Y$ with $U \subset X$ open. Then $f$ is called Fréchet-differentiable in $x_0 \in U$ if there exists a linear map $A:X \to Y$ such that $$\lim_{x \to x_0} \frac{||f(x)-f(x_0)-A(x-x_0)||_Y}{||x-x_0||_X}=0.$$ In all books I looked into there is additionally required that $A$ is continuous. Is this assumption necessary?",Definition (in our lecture): Let be normed vector spaces and with open. Then is called Fréchet-differentiable in if there exists a linear map such that In all books I looked into there is additionally required that is continuous. Is this assumption necessary?,"X,Y f:U \to Y U \subset X f x_0 \in U A:X \to Y \lim_{x \to x_0} \frac{||f(x)-f(x_0)-A(x-x_0)||_Y}{||x-x_0||_X}=0. A",['analysis']
5,How far have we got towards proving Catalan's constant is irrational?,How far have we got towards proving Catalan's constant is irrational?,,"While it is not know if Catalan's constant is irrational,what progress has been made on this problem?For example is there a reformulation of the problem ? (this maybe a criteria, using continued fractions, that can be used to prove if Catalan's constant is irrational-or a different reformulation not using continued fractions).","While it is not know if Catalan's constant is irrational,what progress has been made on this problem?For example is there a reformulation of the problem ? (this maybe a criteria, using continued fractions, that can be used to prove if Catalan's constant is irrational-or a different reformulation not using continued fractions).",,"['analysis', 'irrational-numbers', 'catalans-constant']"
6,Leading Symbol of Pseudo-Differential Operators,Leading Symbol of Pseudo-Differential Operators,,"I have seen various definitions of symbols used to define pseudo-differential operators. The class of symbols I am working with is $S^d(U)=\{p(x,\xi)\in C^{\infty}\left (U\times \mathbb R^m ,M_{k\times l}(\mathbb C) \right): p$ has compact $x$ -support in $U , \ \forall \ \alpha,\beta $ multi-indices $ \exists \ C_{\alpha,\beta}>0$ such that $\left | D^{\alpha}_xD^{\beta}_{\xi}p(x,\xi)\right |\leq C_{\alpha,\beta}(1+|\xi|)^{d-|\beta|}  \}$ This defines a $\Psi DO$ of order $d$ $P: C^{\infty}_c (U)\rightarrow C^{\infty}_c(U) \\ f\longmapsto Pf$ defined by $Pf(x)=\int e^{ix\cdot\xi}p(x,\xi)\hat f(\xi) d\xi $ Let $\Psi^d(U)$ be the class of all pseudo-differential operators defined as above. The leading symbol map is then given by $\sigma_L :\Psi^d(U)\rightarrow \frac{S^d(U)}{S^{d-1}(U)} \\ P\mapsto [p(x,\xi)]$ Is there a canonical choice of representative for the leading symbol like in the case of differential operators of degree $d$ , one can canonically assign a homogeneous polynomial of degree $d$ . https://mathoverflow.net/questions/75976/symbol-of-pseudodiff-operator/77437#77437 In the answer to this question there is a suggestion of defining the leading symbol as $\sigma_LP(x,\xi) = \lim_{\lambda \rightarrow \infty } \lambda^{-d}e^{-i\lambda \phi}P(e^{i\lambda \phi}) $ \ but I can't really see why this limit should exist.","I have seen various definitions of symbols used to define pseudo-differential operators. The class of symbols I am working with is has compact -support in multi-indices such that This defines a of order defined by Let be the class of all pseudo-differential operators defined as above. The leading symbol map is then given by Is there a canonical choice of representative for the leading symbol like in the case of differential operators of degree , one can canonically assign a homogeneous polynomial of degree . https://mathoverflow.net/questions/75976/symbol-of-pseudodiff-operator/77437#77437 In the answer to this question there is a suggestion of defining the leading symbol as \ but I can't really see why this limit should exist.","S^d(U)=\{p(x,\xi)\in C^{\infty}\left (U\times \mathbb R^m ,M_{k\times l}(\mathbb C) \right): p x U , \ \forall \ \alpha,\beta   \exists \ C_{\alpha,\beta}>0 \left | D^{\alpha}_xD^{\beta}_{\xi}p(x,\xi)\right |\leq C_{\alpha,\beta}(1+|\xi|)^{d-|\beta|}  \} \Psi DO d P: C^{\infty}_c (U)\rightarrow C^{\infty}_c(U) \\
f\longmapsto Pf Pf(x)=\int e^{ix\cdot\xi}p(x,\xi)\hat f(\xi) d\xi  \Psi^d(U) \sigma_L :\Psi^d(U)\rightarrow \frac{S^d(U)}{S^{d-1}(U)} \\
P\mapsto [p(x,\xi)] d d \sigma_LP(x,\xi) = \lim_{\lambda \rightarrow \infty } \lambda^{-d}e^{-i\lambda \phi}P(e^{i\lambda \phi}) ","['analysis', 'differential-geometry', 'pseudo-differential-operators']"
7,What is the most surprising / interesting application of the inverse function theorem you have ever seen?,What is the most surprising / interesting application of the inverse function theorem you have ever seen?,,"The inverse function theorem states that: Suppose that $f: U \subset \mathbb{R}^m \rightarrow \mathbb{R}^m$ is a $C^k$ -function  and that there exists $a \in U$ such that $f'(a): \mathbb{R}^m \rightarrow \mathbb{R}^m $ is an isomorphism. Then, there exist $\delta > 0$ and an open ball $B_{\delta} : = B(a, \delta) \subset U$ such that $f \mid_{B_{\delta}} : B_{\delta} \rightarrow V \ni f(a)$ is a diffeomorphism, with $V$ being an open set. There are two remarkable applications of this theorem: 1- Existence of matrices $X$ such that $X^ k = Y$ where $Y$ is a  matrix sufficiently close to the identity; 2- Differentiable perturbation of the identity: Let $U \subset \mathbb{R}^m$ a convex and open set. If $ \varphi : U \rightarrow \mathbb{R}^m$ is $C^k$ , with $|\varphi'(x)| \leq \lambda < 1$ for all $x \in U$ , then $f : U \rightarrow \mathbb{R}^m$ given by $f(x) = x + \varphi(x)$ is a diffeomorphism on $U$ onto its image $f(U)$ . My goal with this question is to broaden the knowledge about the application/importance of this theorem in other contexts in the areas of Analysis, Geometry, Differential Topology, etc...","The inverse function theorem states that: Suppose that is a -function  and that there exists such that is an isomorphism. Then, there exist and an open ball such that is a diffeomorphism, with being an open set. There are two remarkable applications of this theorem: 1- Existence of matrices such that where is a  matrix sufficiently close to the identity; 2- Differentiable perturbation of the identity: Let a convex and open set. If is , with for all , then given by is a diffeomorphism on onto its image . My goal with this question is to broaden the knowledge about the application/importance of this theorem in other contexts in the areas of Analysis, Geometry, Differential Topology, etc...","f: U \subset \mathbb{R}^m \rightarrow \mathbb{R}^m C^k a \in U f'(a): \mathbb{R}^m \rightarrow \mathbb{R}^m  \delta > 0 B_{\delta} : = B(a, \delta) \subset U f \mid_{B_{\delta}} : B_{\delta} \rightarrow V \ni f(a) V X X^ k = Y Y U \subset \mathbb{R}^m  \varphi : U \rightarrow \mathbb{R}^m C^k |\varphi'(x)| \leq \lambda < 1 x \in U f : U \rightarrow \mathbb{R}^m f(x) = x + \varphi(x) U f(U)","['analysis', 'differential-geometry', 'partial-differential-equations', 'differential-topology', 'morse-theory']"
8,Tyrtyshnikov's proof that polynomial roots depend continuously on the coefficients,Tyrtyshnikov's proof that polynomial roots depend continuously on the coefficients,,"I'm having trouble understanding the following proof, which is taken from A Brief Introduction to Numerical Analysis by Eugene E. Tyrtyshnikov . (Note: The polynomials in the following are complex) Theorem 3.9.1 Consider a parametrized batch of polynomials $$p(x,t)=x^n+a_1(t)x^{n-1}+...+a_n(t),$$ where $a_1(t),...,a_n(t)\in C[\alpha,\beta]$ . Then there exist functions $$x_1(t),...,x_n(t)\in C[\alpha,\beta]$$ such that $$p(x_i(t),t)=0\;\;\;\textit{for}\;\;\;\alpha\le t\le\beta,\qquad i=1,...,n.$$ [The author argues why establishing the existence of one such function is sufficient. I will comment on this part later.] [The author states and proves the Arzelà–Ascoli Theorem, which will be used in the following. Note that he uses ""uniformly continuous"" to mean  equicontinuous.] Proof of Theorem 3.9.1. Build up on $[\alpha,\beta]$ a sequence of uniform grids $$\alpha=t_{0m}<t_{1m}<...<t_{mm}=\beta;\qquad t_{i+1,m}-t_{im}=\frac{\beta-\alpha}{m}.$$ Let $y_m(t)$ be a piecewise linear function with breaks at $t_{0m},t_{1m},...,t_{mm}$ . Define the values at the nodes as follows. Take a root $z_0$ of the polynomial $p(x,\alpha)$ , and, for all $m$ , set $$y_m(t_{0m})=z_{0m}\equiv z_0.$$ Further, let $z_{1m}$ be any of those roots of the polynomial $p(x,t_{1m})$ nearest to $z_{0m}$ , and, by induction, let $z_{i+1,m}$ be any of the roots of the polynomial $p(x,t_{i+1,m})$ nearest to $z_{im}$ . Set $$y_m(t_{im})=z_{im},\qquad i=1,...,m.$$ The uniform boundedness of the piecewise linear functions $y_m(t)$ is evident. I devised the following reasoning with some help: For $i=1,...,n$ , there are constants $A_i$ , such that $|a_i(t)|\le A_i$ for all $t\in[\alpha,\beta]$ , by the Extreme Value Theorem. Let $R\colon=1+\sum_{i=1}^nA_i$ . Then for any $x\in\mathbb{C}$ , such that $|x|\ge R$ , we have \begin{align} |p(x,t)|\ge&|x|^n-\sum_{i=1}^n|a_i(t)||x|^{n-i}\\ \ge&|x|^n-|x|^{n-1}\sum_{i=1}^nA_i\\ \ge&R^{n-1}\left(|x|-\sum_{i=1}^nA_i\right)\\ \ge&R^{n-1}>0. \end{align} Thus, the roots of $p(x,t)$ are contained in the disk of radius $R$ centered at the origin. For $t\in[0,1],z_1,z_2\in\mathbb{C}$ , $$|tz_1+(1-t)z_2|\le t|z_1|+(1-t)|z_2|\le\max\{|z_1|,|z_2|\}.$$ This shows that each $y_m(t)$ attains it's maximum absolute value at one of the nodes. The values at the nodes are roots of $p(x,t)$ , hence $R$ uniformly bounds the $y_m(t)$ . The uniform continuity emanates from the inequality $$|z_{i+1,m}-z_{im}|\le|p(z_{im},t_{i+1,m})|^{\frac{1}{n}}=...$$ Why does this hold? This is the most enigmatic part of the proof for me. I assume this part uses that $|z_{i+1,m}-z_{im}|$ was minimized in the construction, but I don't see how it comes together at all. $$...=|p(z_{im},t_{i+1,m})-p(z_{im},t_{im})|^{\frac{1}{n}}\le R\left(\max_{\substack{\alpha\le t_1,t_2\le\beta\\|t_1-t_2|\le\frac{\beta-\alpha}{m}}}\sum_{j=1}^n|a_j(t_1)-a_j(t_2)|\right)^{\frac{1}{n}},$$ where $R\ge1$ is the radius (not necessarily minimal) of a circle encompassing all the roots of all the polynomials $p(x,t)$ for $\alpha\le t\le\beta$ . The existence of such an $R$ has already been established above. The equality is obvious as $p(z_{im},t_{im})=0$ by construction. For any $z_{im}$ , we have either $|z_{im}|<1$ , in which case $|z_{im}|^k\le|z_{im}|<1\le R\le R^n$ for $k=1,...,n-1$ , or $|z_{im}|\ge1$ , in which case $|z_{im}|^k\le R^k\le R^n$ for $k=1,...,n-1$ . Then, \begin{align} |p(z_{im},t_{i+1,m})-p(z_{im},t_{im})|^{1/n}&=\left|z_{im}^n+\sum_{j=1}^na_j(t_{i+1,m})z_{im}^{n-j}-z_{im}^n-\sum_{j=1}^na_j(t_{im})z_{im}^{n-j}\right|^{1/n}\\ &=\left|\sum_{j=1}^nz_{im}^{n-j}(a_j(t_{i+1,m})-a_j(t_{im}))\right|^{1/n}\\ &\le\left(\sum_{j=1}^n|z_{im}|^{n-j}|a_j(t_{i+1,m})-a_j(t_{im})|\right)^{1/n}\\ &\le R\left(\sum_{j=1}^n|a_j(t_{i+1,m})-a_j(t_{im})|\right)^{1/n}\\ &\le R\left(\max_{\substack{\alpha\le t_1,t_2\le\beta\\|t_1-t_2|\le\frac{\beta-\alpha}{m}}}\sum_{j=1}^n|a_j(t_1)-a_j(t_2)|\right)^{1/n}. \end{align} The last inequality follows since $\alpha\le t_{i+1,m},t_{im}\le\beta$ and $|t_{i+1,m}-t_{im}|=\frac{\beta-\alpha}{m}$ . It remains to see why that maximum actually exists: Let $D\colon=\left\{(t_1,t_2)\in[\alpha,\beta]^2\mid|t_1-t_2|\le\frac{\beta-\alpha}{m}\right\}$ . This set is clearly bounded. Now if $((t_{1n},t_{2n}))_n$ is a sequence in $D$ such that $(t_{1n},t_{2n})\rightarrow(t_1,t_2)$ , then taking limits in $|t_{1n}-t_{2n}|\le\frac{\beta-\alpha}{m}$ yields $|t_1-t_2|\le\frac{\beta-\alpha}{m}$ , so $(t_1,t_2)\in D$ . Thus, $D$ is closed and, by Heine-Borel, compact. The function $f\colon D\rightarrow\mathbb{R}$ with $f(t_1,t_2)=\sum_{j=1}^n|a_j(t_1)-a_j(t_2)|$ is continuous since $a_j(t)$ is continuous for $j=1,...,n$ . Thus, by the Extreme Value Theorem, $f$ attains a maximum on $D$ . Even then, however, I do not see how the above inequality implies equicontinuity. Demonstrating equicontinuity would require bounding $|y_m(z_1)-y_m(z_2)|$ for all $m\in\mathbb{N}$ in terms of $|z_1-z_2|$ . Now, if $t_{im}\le z_1,z_2\le t_{i+1,m}$ , then $|y_m(z_1)-y_m(z_2)|\le|z_{im}-z_{i+1,m}|$ . However, one cannot impose such conditions on $z_1,z_2$ , especially since the nodes get arbitrarily close for arbitrarily large $m$ . So how does one use the above inequality to prove equicontinuity? Using the Arzela-Ascoli theorem we find a uniformly convergent subsequence. Take into account that the limit of a uniformly convergent sequence of continuous functions on $[\alpha,\beta]$ must be a continuous function. The only thing left to check is that the limit function $y(t)$ satisfies $p(y(t),t)=0$ for all $\alpha\le t\le\beta$ . That will do the proof. $\quad\square$ Let $(y_{m_k})_k$ be the uniformly convergent subsequence. Let $t\in[\alpha,\beta]$ be arbitrary and $t_{i_km_k}$ be the node closest to $t$ for each $k\in\mathbb{N}$ . Then, by construction, we have $|t-t_{i_km_k}|\le\frac{\beta-\alpha}{m_k}\rightarrow0$ , hence $t_{i_km_k}\rightarrow t$ . Now, since $y_{m_k}\rightarrow y$ uniformly , $y_{m_k}(t_{i_km_k})\rightarrow y(t)$ . Finally, from the continuity of the $a_i(t)$ and the standard results on sums and products of limits, we have $$0=p(z_{i_km_k},t_{i_km_k})=p(y_{m_k}(t_{i_km_k}),t_{i_km_k})\rightarrow p(y(t),t),\qquad\text{hence}\;\;p(y(t),t)=0.$$ Now the initial paragraph I alluded to earlier: To begin the proof, note that it is sufficient to establish the existence of any single continuous function $x_n(t)$ such that $p(x_n(t),t)=0$ for $\alpha\le t\le\beta$ . Should this be done, we write $$p(x,t)=(x-x_n(t))q(x,t),$$ where $q(x,t)=x^{n-1}+b_1(t)x^{n-2}+...+b_{n-1}(t)$ . On the strength of the familiar algorithm for diving polynomials, $b_1(t),...,b_{n-1}(t)\in C[\alpha,\beta]$ . So we may prove it by induction. The application of polynomial division is clear since $x_n(t)$ is a root of $p(x,t)$ for any $t\in[\alpha,\beta]$ . Comparing coefficients also yields that the coefficient of $x^{n-1}$ in $q(x,t)$ must be $1$ . $p(x,t)$ is continuous in $t$ due to the continuity of the $a_i(t)$ , so if $t_k\rightarrow t$ , then $$(x-x_n(t_k))q(x,t_k)=p(x,t_k)\rightarrow p(x,t)=(x-x_n(t))q(x,t).$$ However, we already know $(x-x_n(t_k))\rightarrow(x-x_n(t))$ by the continuity of $x_n(t)$ . Is this sufficient to conclude $q(x,t_k)\rightarrow q(x,t)$ ? (I believe it should be so, but the multiple variables and attempts to avoid an accidental division by zero make me wary.) Furthermore, how may we conclude the continuity of the individual coefficients $b_i(t)$ for $i=1,...,n$ from the continuity of $q(x,t)$ in $t$ as a whole? Can this be done by using the linear independence of the monomials or something of the sort? Finally, the statement of the Theorem only says there exist functions $x_1(t),...,x_n(t)$ which are roots of $p(x,t)$ for each $t\in[\alpha,\beta]$ . However, this would trivially be satisfied by already choosing them all to be the same function. Does the inductive step not actually yield the stronger claim that $$p(x,t)=(x-x_1(t))\cdot...\cdot(x-x_n(t))\text{ for }t\in[\alpha,\beta],$$ which entails that, for each $t\in[\alpha,\beta]$ , $x_1(t),...,x_n(t)$ are precisely all the $n$ roots of $p(x,t)$ counted with multiplicity? Any answer to the specific questions I asked in the above, as well as any correction of potential mistakes I committed in my additions or any supplementation that the argument may still need will be appreciated. Thanks in advance.","I'm having trouble understanding the following proof, which is taken from A Brief Introduction to Numerical Analysis by Eugene E. Tyrtyshnikov . (Note: The polynomials in the following are complex) Theorem 3.9.1 Consider a parametrized batch of polynomials where . Then there exist functions such that [The author argues why establishing the existence of one such function is sufficient. I will comment on this part later.] [The author states and proves the Arzelà–Ascoli Theorem, which will be used in the following. Note that he uses ""uniformly continuous"" to mean  equicontinuous.] Proof of Theorem 3.9.1. Build up on a sequence of uniform grids Let be a piecewise linear function with breaks at . Define the values at the nodes as follows. Take a root of the polynomial , and, for all , set Further, let be any of those roots of the polynomial nearest to , and, by induction, let be any of the roots of the polynomial nearest to . Set The uniform boundedness of the piecewise linear functions is evident. I devised the following reasoning with some help: For , there are constants , such that for all , by the Extreme Value Theorem. Let . Then for any , such that , we have Thus, the roots of are contained in the disk of radius centered at the origin. For , This shows that each attains it's maximum absolute value at one of the nodes. The values at the nodes are roots of , hence uniformly bounds the . The uniform continuity emanates from the inequality Why does this hold? This is the most enigmatic part of the proof for me. I assume this part uses that was minimized in the construction, but I don't see how it comes together at all. where is the radius (not necessarily minimal) of a circle encompassing all the roots of all the polynomials for . The existence of such an has already been established above. The equality is obvious as by construction. For any , we have either , in which case for , or , in which case for . Then, The last inequality follows since and . It remains to see why that maximum actually exists: Let . This set is clearly bounded. Now if is a sequence in such that , then taking limits in yields , so . Thus, is closed and, by Heine-Borel, compact. The function with is continuous since is continuous for . Thus, by the Extreme Value Theorem, attains a maximum on . Even then, however, I do not see how the above inequality implies equicontinuity. Demonstrating equicontinuity would require bounding for all in terms of . Now, if , then . However, one cannot impose such conditions on , especially since the nodes get arbitrarily close for arbitrarily large . So how does one use the above inequality to prove equicontinuity? Using the Arzela-Ascoli theorem we find a uniformly convergent subsequence. Take into account that the limit of a uniformly convergent sequence of continuous functions on must be a continuous function. The only thing left to check is that the limit function satisfies for all . That will do the proof. Let be the uniformly convergent subsequence. Let be arbitrary and be the node closest to for each . Then, by construction, we have , hence . Now, since uniformly , . Finally, from the continuity of the and the standard results on sums and products of limits, we have Now the initial paragraph I alluded to earlier: To begin the proof, note that it is sufficient to establish the existence of any single continuous function such that for . Should this be done, we write where . On the strength of the familiar algorithm for diving polynomials, . So we may prove it by induction. The application of polynomial division is clear since is a root of for any . Comparing coefficients also yields that the coefficient of in must be . is continuous in due to the continuity of the , so if , then However, we already know by the continuity of . Is this sufficient to conclude ? (I believe it should be so, but the multiple variables and attempts to avoid an accidental division by zero make me wary.) Furthermore, how may we conclude the continuity of the individual coefficients for from the continuity of in as a whole? Can this be done by using the linear independence of the monomials or something of the sort? Finally, the statement of the Theorem only says there exist functions which are roots of for each . However, this would trivially be satisfied by already choosing them all to be the same function. Does the inductive step not actually yield the stronger claim that which entails that, for each , are precisely all the roots of counted with multiplicity? Any answer to the specific questions I asked in the above, as well as any correction of potential mistakes I committed in my additions or any supplementation that the argument may still need will be appreciated. Thanks in advance.","p(x,t)=x^n+a_1(t)x^{n-1}+...+a_n(t), a_1(t),...,a_n(t)\in C[\alpha,\beta] x_1(t),...,x_n(t)\in C[\alpha,\beta] p(x_i(t),t)=0\;\;\;\textit{for}\;\;\;\alpha\le t\le\beta,\qquad i=1,...,n. [\alpha,\beta] \alpha=t_{0m}<t_{1m}<...<t_{mm}=\beta;\qquad t_{i+1,m}-t_{im}=\frac{\beta-\alpha}{m}. y_m(t) t_{0m},t_{1m},...,t_{mm} z_0 p(x,\alpha) m y_m(t_{0m})=z_{0m}\equiv z_0. z_{1m} p(x,t_{1m}) z_{0m} z_{i+1,m} p(x,t_{i+1,m}) z_{im} y_m(t_{im})=z_{im},\qquad i=1,...,m. y_m(t) i=1,...,n A_i |a_i(t)|\le A_i t\in[\alpha,\beta] R\colon=1+\sum_{i=1}^nA_i x\in\mathbb{C} |x|\ge R \begin{align}
|p(x,t)|\ge&|x|^n-\sum_{i=1}^n|a_i(t)||x|^{n-i}\\
\ge&|x|^n-|x|^{n-1}\sum_{i=1}^nA_i\\
\ge&R^{n-1}\left(|x|-\sum_{i=1}^nA_i\right)\\
\ge&R^{n-1}>0.
\end{align} p(x,t) R t\in[0,1],z_1,z_2\in\mathbb{C} |tz_1+(1-t)z_2|\le t|z_1|+(1-t)|z_2|\le\max\{|z_1|,|z_2|\}. y_m(t) p(x,t) R y_m(t) |z_{i+1,m}-z_{im}|\le|p(z_{im},t_{i+1,m})|^{\frac{1}{n}}=... |z_{i+1,m}-z_{im}| ...=|p(z_{im},t_{i+1,m})-p(z_{im},t_{im})|^{\frac{1}{n}}\le R\left(\max_{\substack{\alpha\le t_1,t_2\le\beta\\|t_1-t_2|\le\frac{\beta-\alpha}{m}}}\sum_{j=1}^n|a_j(t_1)-a_j(t_2)|\right)^{\frac{1}{n}}, R\ge1 p(x,t) \alpha\le t\le\beta R p(z_{im},t_{im})=0 z_{im} |z_{im}|<1 |z_{im}|^k\le|z_{im}|<1\le R\le R^n k=1,...,n-1 |z_{im}|\ge1 |z_{im}|^k\le R^k\le R^n k=1,...,n-1 \begin{align}
|p(z_{im},t_{i+1,m})-p(z_{im},t_{im})|^{1/n}&=\left|z_{im}^n+\sum_{j=1}^na_j(t_{i+1,m})z_{im}^{n-j}-z_{im}^n-\sum_{j=1}^na_j(t_{im})z_{im}^{n-j}\right|^{1/n}\\
&=\left|\sum_{j=1}^nz_{im}^{n-j}(a_j(t_{i+1,m})-a_j(t_{im}))\right|^{1/n}\\
&\le\left(\sum_{j=1}^n|z_{im}|^{n-j}|a_j(t_{i+1,m})-a_j(t_{im})|\right)^{1/n}\\
&\le R\left(\sum_{j=1}^n|a_j(t_{i+1,m})-a_j(t_{im})|\right)^{1/n}\\
&\le R\left(\max_{\substack{\alpha\le t_1,t_2\le\beta\\|t_1-t_2|\le\frac{\beta-\alpha}{m}}}\sum_{j=1}^n|a_j(t_1)-a_j(t_2)|\right)^{1/n}.
\end{align} \alpha\le t_{i+1,m},t_{im}\le\beta |t_{i+1,m}-t_{im}|=\frac{\beta-\alpha}{m} D\colon=\left\{(t_1,t_2)\in[\alpha,\beta]^2\mid|t_1-t_2|\le\frac{\beta-\alpha}{m}\right\} ((t_{1n},t_{2n}))_n D (t_{1n},t_{2n})\rightarrow(t_1,t_2) |t_{1n}-t_{2n}|\le\frac{\beta-\alpha}{m} |t_1-t_2|\le\frac{\beta-\alpha}{m} (t_1,t_2)\in D D f\colon D\rightarrow\mathbb{R} f(t_1,t_2)=\sum_{j=1}^n|a_j(t_1)-a_j(t_2)| a_j(t) j=1,...,n f D |y_m(z_1)-y_m(z_2)| m\in\mathbb{N} |z_1-z_2| t_{im}\le z_1,z_2\le t_{i+1,m} |y_m(z_1)-y_m(z_2)|\le|z_{im}-z_{i+1,m}| z_1,z_2 m [\alpha,\beta] y(t) p(y(t),t)=0 \alpha\le t\le\beta \quad\square (y_{m_k})_k t\in[\alpha,\beta] t_{i_km_k} t k\in\mathbb{N} |t-t_{i_km_k}|\le\frac{\beta-\alpha}{m_k}\rightarrow0 t_{i_km_k}\rightarrow t y_{m_k}\rightarrow y y_{m_k}(t_{i_km_k})\rightarrow y(t) a_i(t) 0=p(z_{i_km_k},t_{i_km_k})=p(y_{m_k}(t_{i_km_k}),t_{i_km_k})\rightarrow p(y(t),t),\qquad\text{hence}\;\;p(y(t),t)=0. x_n(t) p(x_n(t),t)=0 \alpha\le t\le\beta p(x,t)=(x-x_n(t))q(x,t), q(x,t)=x^{n-1}+b_1(t)x^{n-2}+...+b_{n-1}(t) b_1(t),...,b_{n-1}(t)\in C[\alpha,\beta] x_n(t) p(x,t) t\in[\alpha,\beta] x^{n-1} q(x,t) 1 p(x,t) t a_i(t) t_k\rightarrow t (x-x_n(t_k))q(x,t_k)=p(x,t_k)\rightarrow p(x,t)=(x-x_n(t))q(x,t). (x-x_n(t_k))\rightarrow(x-x_n(t)) x_n(t) q(x,t_k)\rightarrow q(x,t) b_i(t) i=1,...,n q(x,t) t x_1(t),...,x_n(t) p(x,t) t\in[\alpha,\beta] p(x,t)=(x-x_1(t))\cdot...\cdot(x-x_n(t))\text{ for }t\in[\alpha,\beta], t\in[\alpha,\beta] x_1(t),...,x_n(t) n p(x,t)","['analysis', 'polynomials', 'proof-explanation', 'equicontinuity']"
9,Communication on the boundary of a $C^1$ domain,Communication on the boundary of a  domain,C^1,"Assume $\Omega$ is a $C^1$ bounded domain of $\mathbb{R}^d$ , $d \geq 2$ . For $(x,y) \in (\partial \Omega)^2$ , we say $x \sim y$ if $n_x \cdot (y-x) > 0$ , $n_y \cdot (x-y) > 0$ , and $(tx+(1-t)y) \in \Omega$ for all $t \in (0,1)$ . Is it true that for any $(x,y) \in (\partial \Omega)^2$ such that $x \sim y$ , one can find $\epsilon_1 > 0, \epsilon_2 > 0$ such that $(B(x,\epsilon_1) \cap \partial \Omega) \sim (B(y,\epsilon_2) \cap \partial \Omega)$ (in the sense that for any $a \in (B(x,\epsilon_1) \cap \partial \Omega)$ , $b \in B(y,\epsilon_2) \cap \partial \Omega)$ , $a \sim b$ )? From the $C^1$ property of the domain it seems clear that for all $(x,y) \in \partial \Omega^2$ with $x \sim y$ , one can find $\epsilon > 0$ such that $x \sim B(y,\epsilon) \cap \partial \Omega$ (and I have a proof for it). However the stronger result that I ask, although I don't see any reason why this should not hold, puzzles me a lot more. Any idea whether the statement is true ? And how to show it ?","Assume is a bounded domain of , . For , we say if , , and for all . Is it true that for any such that , one can find such that (in the sense that for any , , )? From the property of the domain it seems clear that for all with , one can find such that (and I have a proof for it). However the stronger result that I ask, although I don't see any reason why this should not hold, puzzles me a lot more. Any idea whether the statement is true ? And how to show it ?","\Omega C^1 \mathbb{R}^d d \geq 2 (x,y) \in (\partial \Omega)^2 x \sim y n_x \cdot (y-x) > 0 n_y \cdot (x-y) > 0 (tx+(1-t)y) \in \Omega t \in (0,1) (x,y) \in (\partial \Omega)^2 x \sim y \epsilon_1 > 0, \epsilon_2 > 0 (B(x,\epsilon_1) \cap \partial \Omega) \sim (B(y,\epsilon_2) \cap \partial \Omega) a \in (B(x,\epsilon_1) \cap \partial \Omega) b \in B(y,\epsilon_2) \cap \partial \Omega) a \sim b C^1 (x,y) \in \partial \Omega^2 x \sim y \epsilon > 0 x \sim B(y,\epsilon) \cap \partial \Omega","['analysis', 'differential-geometry', 'analytic-geometry']"
10,Is the sum of series $\sum_{n=0}^{\infty} \lfloor n\pi \rfloor x^n$ a rational function?,Is the sum of series  a rational function?,\sum_{n=0}^{\infty} \lfloor n\pi \rfloor x^n,I was reading this paper by L.J. Mordell (original paper by Morris Newman). I am trying to apply first theorem from the paper with $f(x)=x$ and $k=\pi$ . So the series $\displaystyle \sum_{n=0}^{\infty} \lfloor n\pi \rfloor x^n$ must not converge to a rational function for $|x|<1$ . But it seems that the series converges to a rational function as the series can be written as $3x+6x^2+9x^3+...=\frac{3x}{(x-1)^2}$ after we evaluate $\lfloor n\pi \rfloor$ for each $n$ ? Am I doing something wrong with the evaluation of $\lfloor n\pi \rfloor$ ? EDIT: David found my mistake. Now the question is can we find the sum of this series or show that the sum is not a rational function without Mordell's result?,I was reading this paper by L.J. Mordell (original paper by Morris Newman). I am trying to apply first theorem from the paper with and . So the series must not converge to a rational function for . But it seems that the series converges to a rational function as the series can be written as after we evaluate for each ? Am I doing something wrong with the evaluation of ? EDIT: David found my mistake. Now the question is can we find the sum of this series or show that the sum is not a rational function without Mordell's result?,f(x)=x k=\pi \displaystyle \sum_{n=0}^{\infty} \lfloor n\pi \rfloor x^n |x|<1 3x+6x^2+9x^3+...=\frac{3x}{(x-1)^2} \lfloor n\pi \rfloor n \lfloor n\pi \rfloor,"['real-analysis', 'analysis', 'power-series', 'rational-functions']"
11,Direct Proof that Continuous Functions Satisfy Epsilon-Delta Condition,Direct Proof that Continuous Functions Satisfy Epsilon-Delta Condition,,"If we define a continuous real function $f: M \to R$ as one that preserves sequential convergence: $$  (x_n)\to p \Rightarrow f(x_n) \to f(p), \tag{1} $$ then to show that this definition is equivalent to the epsilon-delta condition, $$ \forall p \forall \epsilon \exists \delta: x \in V_{\delta}(p) \Rightarrow f(x) \in V_{\epsilon}(f(p)), \tag{2} $$ my textbooks use use a proof by contradiction in which they derive a convergent sequence that does not converge under the mapping. Question: Is it possible to have a direct proof of $(1) \to (2)$ ? Thought: I think to directly show that $(1) \to (2)$ , we will have to show that every element of M is an element of at least a sequence in $M$ , i.e., the union of the elements of all the sequences in $M$ is $M$ .","If we define a continuous real function as one that preserves sequential convergence: then to show that this definition is equivalent to the epsilon-delta condition, my textbooks use use a proof by contradiction in which they derive a convergent sequence that does not converge under the mapping. Question: Is it possible to have a direct proof of ? Thought: I think to directly show that , we will have to show that every element of M is an element of at least a sequence in , i.e., the union of the elements of all the sequences in is .","f: M \to R 
 (x_n)\to p \Rightarrow f(x_n) \to f(p), \tag{1}
 
\forall p \forall \epsilon \exists \delta: x \in V_{\delta}(p) \Rightarrow f(x) \in V_{\epsilon}(f(p)), \tag{2}
 (1) \to (2) (1) \to (2) M M M","['real-analysis', 'analysis']"
12,"Is this set dense in $\mathcal{C}^{\infty} (\mathbb{S}^3 , \mathbb{R}^3 ) $?",Is this set dense in ?,"\mathcal{C}^{\infty} (\mathbb{S}^3 , \mathbb{R}^3 ) ","Consider $\mathcal{F} = {\{ f:\mathbb{S}^3 \to \mathbb{R}^3; f\mbox{ is smooth} \}}$ and the metric $d: \mathcal{F}\times  \mathcal{F} \to \mathbb{R} $ $$d(f,g) = |f-g| = \sup_{p\in \mathbb{S}^3} \|f(p) - g(p)\|, $$ it is easy to see that $(\mathcal{F}, d)$ is a metric space. Let $T^2 \subset \mathbb{S}^3$ be the set $$T^2 = \frac{1}{\sqrt{2}}\cdot \mathbb{T}^2 = \left\{(x_1,x_2,x_3,x_4)\in \mathbb{S}^3 ; \ x_1^2 + x_2^2 = x_3^2 + x_4^2 = \frac{1}{2} \right\}. $$ Question: Is the set $\mathcal{A} = \{f\in \mathcal{F};\ f(p)\neq 0 \ \  \forall\ p\in T^2 \}$ dense in $\mathcal{F}$ ? This seems true but I'm a little confused about how I supposed to approach this problem. Can anyone help me?",Consider and the metric it is easy to see that is a metric space. Let be the set Question: Is the set dense in ? This seems true but I'm a little confused about how I supposed to approach this problem. Can anyone help me?,"\mathcal{F} = {\{ f:\mathbb{S}^3 \to \mathbb{R}^3; f\mbox{ is smooth} \}} d: \mathcal{F}\times  \mathcal{F} \to \mathbb{R}  d(f,g) = |f-g| = \sup_{p\in \mathbb{S}^3} \|f(p) - g(p)\|,  (\mathcal{F}, d) T^2 \subset \mathbb{S}^3 T^2 = \frac{1}{\sqrt{2}}\cdot \mathbb{T}^2 = \left\{(x_1,x_2,x_3,x_4)\in \mathbb{S}^3 ; \ x_1^2 + x_2^2 = x_3^2 + x_4^2 = \frac{1}{2} \right\}.  \mathcal{A} = \{f\in \mathcal{F};\ f(p)\neq 0 \ \  \forall\ p\in T^2 \} \mathcal{F}","['analysis', 'metric-spaces', 'differential-topology']"
13,Making sense of a Lagrangian on a manifold,Making sense of a Lagrangian on a manifold,,"Going through Villani's book on Optimal Transport, I came across the following setup: a Riemannian manifold $M$ and a Lagrangian $L : TM \times [0, 1] \to \mathbb{R}$ (So $L$ takes $(x, v, t)$ values as inputs). I'm wondering if I have the right notions of derivatives here. If $D_x L$ represents the spatial gradient, then we should have $$ D_x L(x_0, v_0, t_0)(w) = \dot\gamma(0),  $$ where $\tilde{\gamma}(0) = x_0, \dot{\tilde{\gamma}}(0) = w$ , and $\gamma(t) = L(\gamma(t), v_0, t_0)$ . Is this correct? I'm a little more confused about the notion of $D_vL$ , where this represents the velocity gradient. I have just defined as follows: $$ D_v L(x_0, v_0, t_0)(w) = \dot\gamma(0), $$ where $\gamma(t) = L(x_0, v_0 + tw, t_0)$ , because this seems to make sense to me. I was wondering if anyone could 1) let me know if these are correct, and (perhaps more importantly) 2) whether or not there is a better way of thinking about these objects. Much appreciation for any help.","Going through Villani's book on Optimal Transport, I came across the following setup: a Riemannian manifold and a Lagrangian (So takes values as inputs). I'm wondering if I have the right notions of derivatives here. If represents the spatial gradient, then we should have where , and . Is this correct? I'm a little more confused about the notion of , where this represents the velocity gradient. I have just defined as follows: where , because this seems to make sense to me. I was wondering if anyone could 1) let me know if these are correct, and (perhaps more importantly) 2) whether or not there is a better way of thinking about these objects. Much appreciation for any help.","M L : TM \times [0, 1] \to \mathbb{R} L (x, v, t) D_x L 
D_x L(x_0, v_0, t_0)(w) = \dot\gamma(0), 
 \tilde{\gamma}(0) = x_0, \dot{\tilde{\gamma}}(0) = w \gamma(t) = L(\gamma(t), v_0, t_0) D_vL 
D_v L(x_0, v_0, t_0)(w) = \dot\gamma(0),
 \gamma(t) = L(x_0, v_0 + tw, t_0)","['analysis', 'manifolds', 'riemannian-geometry']"
14,Proving the determinant of the derivative of a function can not be zero on any open set.,Proving the determinant of the derivative of a function can not be zero on any open set.,,"I'm working on the following problem for my multivariable calculus course: Let $f: R^2 \to R^2 $ be a $C^1$ function such that for each $y \in R^2$ , the set $f^{-1}(y)$ is finite. Show that $\det Df(x)$ cannot vanish identically on any open subset of $R^2$ I'm assuming I need to use the inverse function theorem somewhere, but I cannot find a place to apply it.","I'm working on the following problem for my multivariable calculus course: Let be a function such that for each , the set is finite. Show that cannot vanish identically on any open subset of I'm assuming I need to use the inverse function theorem somewhere, but I cannot find a place to apply it.",f: R^2 \to R^2  C^1 y \in R^2 f^{-1}(y) \det Df(x) R^2,"['analysis', 'multivariable-calculus']"
15,Fatou's Lemma proof from Royden 4th,Fatou's Lemma proof from Royden 4th,,"I'm having a hard time understanding the proof presented by Royden, Fitzpatrick in Real Analysis 4th edition. The proof is described as follows... Fatou's Lemma Let $\{f_n\}$ be a sequence of nonnegative measurable functions on $E$ . \begin{equation} \textit{If}~\{f_n\}\rightarrow f~\textit{pointwise a.e. on}~E,~\textit{then}~\int_E{f}\leq \liminf\int_E{f_n}. \end{equation} Proof In view of addititivity over domains of integration , by possibly excising from $E$ a set of measure zero, we assume the pointwise convergence is on all of $E$ . The function $f$ is nonnegative and measurable since it is the pointwise limit of a sequence of such functions. To verify the inequality in Fatou's Lemma it is necessary and sufficient to show that if $h$ is any bounded measurable function of finite support for which $0\leq h \leq f$ on $E$ , then \begin{equation} \int_E h \leq \liminf \int_E f_n. \end{equation} Let $h$ be such a function. Choose $M\geq 0$ for which $\lvert h \rvert\leq M$ on $E$ . Define $E_0 = \left\{x \in E~\middle|~h(x)\neq 0 \right\}$ . Then $m(E_0) < \infty$ . Let $n$ be a natural number. Define a function $h_n$ on $E$ by \begin{equation} h_n = \min\left\{h,f_n\right\}~\text{on}~E. \end{equation} Observe that the function $h_n$ is measurable, that \begin{equation} 0\leq h_n \leq M~\text{on}~E_0~\text{and}~h_n \equiv 0~\text{on}~E\sim E_0. \end{equation} Furthermore, for each $x$ in $E$ , since $h(x) \leq f(x)$ and $\left\{f_n(x)\right\} \rightarrow f(x)$ , $\left\{h_n(x)\right\} \rightarrow h(x)$ . We infer from the Bounded Convergence Theorem applied to the uniformly bounded sequence of restrictions to $h_n$ to the set of finite measure $E_0$ , and the vanishing of each $h_n$ on $E \sim E_0$ , that \begin{equation} \lim_{n\rightarrow \infty}\int_E{h_n} = \lim_{n\rightarrow \infty}\int_{E_0}{h_n} = \int_{E_0}{h} = \int_{E}{h}. \end{equation} However, for each $n$ , $h_n \leq f_n$ on $E$ and therefore, by the definition of the integral of $f_n$ over $E$ , $\int_E{h_n} \leq \int_E{f_n}$ . Thus, \begin{equation} \int_E{h} = \lim_{n\rightarrow \infty} \int_{E}{h_n} \leq \liminf \int_E f_n. \end{equation} So I understand everything up until the final conclusion. It seems that the conclusion I reach from the inequality $\int_E h_n \leq \int_E f_n$ is that \begin{equation}     \int_E{h} = \lim_{n\rightarrow \infty} \int_{E}{h_n} \leq \lim_{n\rightarrow \infty} \int_E f_n. \end{equation} Where does the $\liminf$ come from?","I'm having a hard time understanding the proof presented by Royden, Fitzpatrick in Real Analysis 4th edition. The proof is described as follows... Fatou's Lemma Let be a sequence of nonnegative measurable functions on . Proof In view of addititivity over domains of integration , by possibly excising from a set of measure zero, we assume the pointwise convergence is on all of . The function is nonnegative and measurable since it is the pointwise limit of a sequence of such functions. To verify the inequality in Fatou's Lemma it is necessary and sufficient to show that if is any bounded measurable function of finite support for which on , then Let be such a function. Choose for which on . Define . Then . Let be a natural number. Define a function on by Observe that the function is measurable, that Furthermore, for each in , since and , . We infer from the Bounded Convergence Theorem applied to the uniformly bounded sequence of restrictions to to the set of finite measure , and the vanishing of each on , that However, for each , on and therefore, by the definition of the integral of over , . Thus, So I understand everything up until the final conclusion. It seems that the conclusion I reach from the inequality is that Where does the come from?","\{f_n\} E \begin{equation}
\textit{If}~\{f_n\}\rightarrow f~\textit{pointwise a.e. on}~E,~\textit{then}~\int_E{f}\leq \liminf\int_E{f_n}.
\end{equation} E E f h 0\leq h \leq f E \begin{equation}
\int_E h \leq \liminf \int_E f_n.
\end{equation} h M\geq 0 \lvert h \rvert\leq M E E_0 = \left\{x \in E~\middle|~h(x)\neq 0 \right\} m(E_0) < \infty n h_n E \begin{equation}
h_n = \min\left\{h,f_n\right\}~\text{on}~E.
\end{equation} h_n \begin{equation}
0\leq h_n \leq M~\text{on}~E_0~\text{and}~h_n \equiv 0~\text{on}~E\sim E_0.
\end{equation} x E h(x) \leq f(x) \left\{f_n(x)\right\} \rightarrow f(x) \left\{h_n(x)\right\} \rightarrow h(x) h_n E_0 h_n E \sim E_0 \begin{equation}
\lim_{n\rightarrow \infty}\int_E{h_n} = \lim_{n\rightarrow \infty}\int_{E_0}{h_n} = \int_{E_0}{h} = \int_{E}{h}.
\end{equation} n h_n \leq f_n E f_n E \int_E{h_n} \leq \int_E{f_n} \begin{equation}
\int_E{h} = \lim_{n\rightarrow \infty} \int_{E}{h_n} \leq \liminf \int_E f_n.
\end{equation} \int_E h_n \leq \int_E f_n \begin{equation}
    \int_E{h} = \lim_{n\rightarrow \infty} \int_{E}{h_n} \leq \lim_{n\rightarrow \infty} \int_E f_n.
\end{equation} \liminf","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
16,Differences between derivatives and strong derivatives,Differences between derivatives and strong derivatives,,"Definition: Let $f$ be a real valued function. We say $f$ is $\mathbf{strongly}$ $\mathbf{differentiable}$ at $x = a$ if the following limits exists and is finite: $$ \lim_{x \to a, y \to a, x \neq y} \frac{ f(x)-f(y)}{x-y} = f^*(a) $$ and we can $f^*(a)$ the strong derivative of $f$ at $a$. Why is this definition of derivative different than the usual one? What is the main crucial point to understand here that makes it different?","Definition: Let $f$ be a real valued function. We say $f$ is $\mathbf{strongly}$ $\mathbf{differentiable}$ at $x = a$ if the following limits exists and is finite: $$ \lim_{x \to a, y \to a, x \neq y} \frac{ f(x)-f(y)}{x-y} = f^*(a) $$ and we can $f^*(a)$ the strong derivative of $f$ at $a$. Why is this definition of derivative different than the usual one? What is the main crucial point to understand here that makes it different?",,['calculus']
17,Under which circumstances is the space of Hölder continuous functions a Banach space?,Under which circumstances is the space of Hölder continuous functions a Banach space?,,"Let $(M,d)$ be a metric space; $\Lambda\subseteq M$ ; $E$ be a $\mathbb R$ -Banach space. Let $$\left\|f\right\|_{B(\Lambda,\:E)}:=\sup_{x\in\Lambda}\left\|f(x)\right\|_E\;\;\;\text{for }f:\Lambda\to E.$$ It's easy to show that $$B(\Lambda,E):=\left\{f:\Lambda\to E\mid\left\|f\right\|_{B(\Lambda,\:E)}<\infty\right\},$$ equipped with $\left\|\;\cdot\;\right\|_{B(\Lambda,\:E)}$ is a $\mathbb R$ -Banach space. Now, let $$[f]_{C^{0+\alpha}(\Lambda,\:E)}:=\sup_{\substack{x,\:y\:\in\:\Lambda\\x\:\ne\:y}}\frac{\left\|f(x)-f(y)\right\|_E}{d(x,y)^\alpha}\;\;\;\text{for }f:\Lambda\to E$$ for $\alpha\in(0,1]$ . Moreover, let $$C^{0+\alpha}(\Lambda,E):=\left\{f\in B(\Lambda,E):[f]_{C^{0+\alpha}(\Lambda,\:E)}<\infty\right\}$$ and $$\left\|f\right\|_{C^{0+\alpha}(\Lambda,\:E)}:=\left\|f\right\|_{B(\Lambda,\:E)}+[f]_{C^{0+\alpha}(\Lambda,\:E)}\;\;\;\text{for }f:\Lambda\to E.$$ How can we show that $C^{0+\alpha}(\Lambda,E)$ equipped with $\left\|\;\cdot\;\right\|_{C^{0+\alpha}(\Lambda,\:E)}$ is a $\mathbb R$ -Banach space? Let $(f_n)_{n\in\mathbb N}\subseteq C^{0+\alpha}(\Lambda, E)$ be Cauchy. There is a unique $f\in B(\Lambda,E)$ with $$\left\|f-f_n\right\|_{B(\Lambda,\:E)}\xrightarrow{n\to\infty}0.\tag1$$ Now, $$\frac{\left\|f(x)-f(y)\right\|_E}{d(x,y)^\alpha}=\lim_{n\to\infty}\frac{\left\|f_n(x)-f_n(y)\right\|_E}{d(x,y)^\alpha}\le\lim_{n\to\infty}\left\|f_n\right\|_{C^{0+\alpha}(\Lambda,\:E)}<\infty$$ for all $x,y\in\Lambda$ with $x\ne y$ and hence $f\in C^{0+\alpha}(\Lambda,E)$ . Moreover, $$\frac{\left\|(f-f_n)(x)-(f-f_n)(y)\right\|_E}{d(x,y)^\alpha}=\lim_{m\to\infty}\frac{\left\|(f_m-f_n)(x)-(f_m-f_n)(y)\right\|_E}{d(x,y)^\alpha}\le\limsup_{m\to\infty}\left\|f_m-f_n\right\|_{C^{0+\alpha}(\Lambda,\:E)}\xrightarrow{n\to\infty}0$$ for all $x,y\in\Lambda$ with $x\ne y$ and hence $f\in C^{0+\alpha}(\Lambda,E)$ with $$[f-f_n]_{C^{0+\alpha}(\Lambda,\:E)}\xrightarrow{n\to\infty}0.$$ We should be done. However, I've seen other people proving the claim in a different way. First of all, they assume that $\Lambda$ is open. Why should this be necessary? Moreover, they claim that the $f$ in my proof above can be chosen from $B(\overline\Lambda,E)$ and that $(1)$ holds with respect to $\left\|\;\cdot\;\right\|_{B(\overline\Lambda,\:E)}$ . It's clear to me that (without the openness assumption), each $g\in C^{0+\alpha}(\Lambda,E)$ admits a unique extension $\tilde g\in C^{0+\alpha}(\overline\Lambda,E)$ with $$[\tilde g]_{C^{0+\alpha}(\overline\Lambda,\:E)}=[g]_{C^{0+\alpha}(\Lambda,\:E)}.$$ But I don't see why (if $f$ is chosen to be its unique extension $\tilde f$ ) the claimed convergence should hold (since we should only know that $\left\|f-f_n\right\|_{B(\Lambda,\:E)}\le\left\|f-f_n\right\|_{B(\overline\Lambda,\:E)}$ and we only know that the left-hand side converges as $n\to\infty$ ). However, as I said before, I don't see why this should be of any use at all. So, am I missing something?","Let be a metric space; ; be a -Banach space. Let It's easy to show that equipped with is a -Banach space. Now, let for . Moreover, let and How can we show that equipped with is a -Banach space? Let be Cauchy. There is a unique with Now, for all with and hence . Moreover, for all with and hence with We should be done. However, I've seen other people proving the claim in a different way. First of all, they assume that is open. Why should this be necessary? Moreover, they claim that the in my proof above can be chosen from and that holds with respect to . It's clear to me that (without the openness assumption), each admits a unique extension with But I don't see why (if is chosen to be its unique extension ) the claimed convergence should hold (since we should only know that and we only know that the left-hand side converges as ). However, as I said before, I don't see why this should be of any use at all. So, am I missing something?","(M,d) \Lambda\subseteq M E \mathbb R \left\|f\right\|_{B(\Lambda,\:E)}:=\sup_{x\in\Lambda}\left\|f(x)\right\|_E\;\;\;\text{for }f:\Lambda\to E. B(\Lambda,E):=\left\{f:\Lambda\to E\mid\left\|f\right\|_{B(\Lambda,\:E)}<\infty\right\}, \left\|\;\cdot\;\right\|_{B(\Lambda,\:E)} \mathbb R [f]_{C^{0+\alpha}(\Lambda,\:E)}:=\sup_{\substack{x,\:y\:\in\:\Lambda\\x\:\ne\:y}}\frac{\left\|f(x)-f(y)\right\|_E}{d(x,y)^\alpha}\;\;\;\text{for }f:\Lambda\to E \alpha\in(0,1] C^{0+\alpha}(\Lambda,E):=\left\{f\in B(\Lambda,E):[f]_{C^{0+\alpha}(\Lambda,\:E)}<\infty\right\} \left\|f\right\|_{C^{0+\alpha}(\Lambda,\:E)}:=\left\|f\right\|_{B(\Lambda,\:E)}+[f]_{C^{0+\alpha}(\Lambda,\:E)}\;\;\;\text{for }f:\Lambda\to E. C^{0+\alpha}(\Lambda,E) \left\|\;\cdot\;\right\|_{C^{0+\alpha}(\Lambda,\:E)} \mathbb R (f_n)_{n\in\mathbb N}\subseteq C^{0+\alpha}(\Lambda, E) f\in B(\Lambda,E) \left\|f-f_n\right\|_{B(\Lambda,\:E)}\xrightarrow{n\to\infty}0.\tag1 \frac{\left\|f(x)-f(y)\right\|_E}{d(x,y)^\alpha}=\lim_{n\to\infty}\frac{\left\|f_n(x)-f_n(y)\right\|_E}{d(x,y)^\alpha}\le\lim_{n\to\infty}\left\|f_n\right\|_{C^{0+\alpha}(\Lambda,\:E)}<\infty x,y\in\Lambda x\ne y f\in C^{0+\alpha}(\Lambda,E) \frac{\left\|(f-f_n)(x)-(f-f_n)(y)\right\|_E}{d(x,y)^\alpha}=\lim_{m\to\infty}\frac{\left\|(f_m-f_n)(x)-(f_m-f_n)(y)\right\|_E}{d(x,y)^\alpha}\le\limsup_{m\to\infty}\left\|f_m-f_n\right\|_{C^{0+\alpha}(\Lambda,\:E)}\xrightarrow{n\to\infty}0 x,y\in\Lambda x\ne y f\in C^{0+\alpha}(\Lambda,E) [f-f_n]_{C^{0+\alpha}(\Lambda,\:E)}\xrightarrow{n\to\infty}0. \Lambda f B(\overline\Lambda,E) (1) \left\|\;\cdot\;\right\|_{B(\overline\Lambda,\:E)} g\in C^{0+\alpha}(\Lambda,E) \tilde g\in C^{0+\alpha}(\overline\Lambda,E) [\tilde g]_{C^{0+\alpha}(\overline\Lambda,\:E)}=[g]_{C^{0+\alpha}(\Lambda,\:E)}. f \tilde f \left\|f-f_n\right\|_{B(\Lambda,\:E)}\le\left\|f-f_n\right\|_{B(\overline\Lambda,\:E)} n\to\infty","['analysis', 'continuity', 'holder-spaces']"
18,Functions $f: \mathbb{R}^n \to \mathbb{R}$ such that $|f(x) -f(y)| \le C \prod_{i=1}^n |x_i - y_i|^{\alpha_i}$,Functions  such that,f: \mathbb{R}^n \to \mathbb{R} |f(x) -f(y)| \le C \prod_{i=1}^n |x_i - y_i|^{\alpha_i},"The standard definition of a Holder continuous function between metric spaces $X,Y$ is a function $f: X \to Y$ such that there exist $C>0$ and $0 < \alpha \le 1$ such that $$ d_Y(f(x),f(y)) \le C (d_X(x,y))^\alpha \text{ for all }x,y \in X. $$ When specialized to $X= \mathbb{R}^n$ and $Y = \mathbb{R}$ or $\mathbb{C}$ this reads $$ |f(x)-f(y)| \le C |x-y|^\alpha \text{ for all }x,y \in \mathbb{R}^n. $$ It is a simple matter to find examples of such functions, with the easiest case being when $\alpha =1$, as in this case any differentiable function with bounded derivative will obey the $\alpha =1$ (Lipschitz) estimate. I was recently looking at Classical Fourier Analysis by Grafakos.  In the study of pointwise convergence of Fourier series he proves a result (Corollary 3.3.8) that says that if we have a function on the $n-$torus, $f: \mathbb{T}^n \to \mathbb{C}$, and a point $y \in \mathbb{T}^n$ such that $$ |f(x) - f(y)| \le C \prod_{i=1}^n |x_i - y_i|^{\alpha_j} \text{ for all }x \in \mathbb{T}^m $$ then the Fourier partial sums converge at $y$.  This suggests that the collection of functions satisfying this condition is of some interest. It's not hard to build a function that satisfies this condition at a point.  To make things simpler let's swap to $\mathbb{R}^n$ from the torus and restrict to real-valued functions.  Then we can define $f(x) = \prod_{i=1}^n |x_i|^{\alpha_i}$ to get a trivial example satisfying the condition at $y=0$.  I then started thinking about this condition as a general one, satisfied for all pairs of points $x,y$, i.e. I want a function $f: \mathbb{R}^n \to \mathbb{R}$ such that there exist $C>0$ and $0 < \alpha_i \le 1$ such that  $$ |f(x) - f(y)| \le C \prod_{j=1}^n |x_i - y_i|^{\alpha_i} \text{ for all }x,y \in \mathbb{R}^n. $$ This is a sort of strange variant of the standard Holder continuity condition.  Note that to avoid triviality we should assume that $\sum_i \alpha_i \le 1$ as otherwise  $$ |f(x)-f(y)| \le C |x-y|^{1+\epsilon}  $$ for $\epsilon = \sum_{i} \alpha_i -1 >0$, and so $f$ is constant. My problem is that I can't seem to come up with any example that satisfies this condition for all $x,y$.  Initially I guessed functions of the form $f(x) = \prod_{i=1}^n f_i(x_i)$ would work, but this doesn't fly.  For example, with $n=2$ we get $$ f_1(x_1)f_2(x_2) - f_1(y_1) f_2(y_2) = (f_1(x_1) - f_1(y_1))f_2(x_2) + f_1(y_1)(f_2(x_2) - f_2(y_2)) $$ and so if $f_1,f_2$ are bounded and Holder continuous (with exponents $\alpha_1,\alpha_2$) we get  $$ |f(x) - f(y)| \le C |x_1 - y_1|^{\alpha_1} + C |x_2 - y_2|^{\alpha_2}. $$ This is not what we want. Do such functions exist?  If so, is there a sufficient condition for generating them (something like the bounded derivative condition above)?  Any ideas would be greatly appreciated! EDIT: @Dap made the observation I had overlooked.  If $x_i = y_i$ then $f(x)=f(y)$ and so $f$ must be constant on every hypersurface parallel to the coordinate axes.  This shows that $f$ is constant.","The standard definition of a Holder continuous function between metric spaces $X,Y$ is a function $f: X \to Y$ such that there exist $C>0$ and $0 < \alpha \le 1$ such that $$ d_Y(f(x),f(y)) \le C (d_X(x,y))^\alpha \text{ for all }x,y \in X. $$ When specialized to $X= \mathbb{R}^n$ and $Y = \mathbb{R}$ or $\mathbb{C}$ this reads $$ |f(x)-f(y)| \le C |x-y|^\alpha \text{ for all }x,y \in \mathbb{R}^n. $$ It is a simple matter to find examples of such functions, with the easiest case being when $\alpha =1$, as in this case any differentiable function with bounded derivative will obey the $\alpha =1$ (Lipschitz) estimate. I was recently looking at Classical Fourier Analysis by Grafakos.  In the study of pointwise convergence of Fourier series he proves a result (Corollary 3.3.8) that says that if we have a function on the $n-$torus, $f: \mathbb{T}^n \to \mathbb{C}$, and a point $y \in \mathbb{T}^n$ such that $$ |f(x) - f(y)| \le C \prod_{i=1}^n |x_i - y_i|^{\alpha_j} \text{ for all }x \in \mathbb{T}^m $$ then the Fourier partial sums converge at $y$.  This suggests that the collection of functions satisfying this condition is of some interest. It's not hard to build a function that satisfies this condition at a point.  To make things simpler let's swap to $\mathbb{R}^n$ from the torus and restrict to real-valued functions.  Then we can define $f(x) = \prod_{i=1}^n |x_i|^{\alpha_i}$ to get a trivial example satisfying the condition at $y=0$.  I then started thinking about this condition as a general one, satisfied for all pairs of points $x,y$, i.e. I want a function $f: \mathbb{R}^n \to \mathbb{R}$ such that there exist $C>0$ and $0 < \alpha_i \le 1$ such that  $$ |f(x) - f(y)| \le C \prod_{j=1}^n |x_i - y_i|^{\alpha_i} \text{ for all }x,y \in \mathbb{R}^n. $$ This is a sort of strange variant of the standard Holder continuity condition.  Note that to avoid triviality we should assume that $\sum_i \alpha_i \le 1$ as otherwise  $$ |f(x)-f(y)| \le C |x-y|^{1+\epsilon}  $$ for $\epsilon = \sum_{i} \alpha_i -1 >0$, and so $f$ is constant. My problem is that I can't seem to come up with any example that satisfies this condition for all $x,y$.  Initially I guessed functions of the form $f(x) = \prod_{i=1}^n f_i(x_i)$ would work, but this doesn't fly.  For example, with $n=2$ we get $$ f_1(x_1)f_2(x_2) - f_1(y_1) f_2(y_2) = (f_1(x_1) - f_1(y_1))f_2(x_2) + f_1(y_1)(f_2(x_2) - f_2(y_2)) $$ and so if $f_1,f_2$ are bounded and Holder continuous (with exponents $\alpha_1,\alpha_2$) we get  $$ |f(x) - f(y)| \le C |x_1 - y_1|^{\alpha_1} + C |x_2 - y_2|^{\alpha_2}. $$ This is not what we want. Do such functions exist?  If so, is there a sufficient condition for generating them (something like the bounded derivative condition above)?  Any ideas would be greatly appreciated! EDIT: @Dap made the observation I had overlooked.  If $x_i = y_i$ then $f(x)=f(y)$ and so $f$ must be constant on every hypersurface parallel to the coordinate axes.  This shows that $f$ is constant.",,"['real-analysis', 'analysis', 'metric-spaces', 'continuity', 'uniform-continuity']"
19,Lagrange multiplier for the Stokes equations,Lagrange multiplier for the Stokes equations,,"I'm trying to understand the following: Let $\Omega \subset \mathbb{R}^2,\ V$ = space of (vectorvalued) continuous piecewise linear functions with zero boundary, $W = $ space of continuous piecewise linear (scalar) function. We're looking for solutions $(u,p) \in V \times W$ of the following discrete formulation of the Stokes equations: $\begin{eqnarray} a(u,v) - (p,\text{div}v) &=& (f,v) \ \ \ \forall v \in V  \\ (q, \text{div} u) &=& 0\ \ \ \forall q \in W \end{eqnarray}$ with $a(u,v):= \int_\Omega \sum\limits_i Du^iDv^idx$ and the $L^2$-scalar product $(\cdot,\cdot).$ Now the notes say, these equations are a necessary condition for the minimization problem $E(v) := \frac{1}{2}a(v,v) - (f,v) \rightarrow \text{Min}$ in $V_\text{div}$ with $\ \ \ V_\text{div}:= \{ v \in V : (q, \text{div}v) = 0 \ \ \ \forall q \in W  \}$, since the Lagrangian of the problem is given by $L(v,q) = \frac{1}{2}a(v,v) - (f,v) - (q,\text{div} v)$ and the above equations are a necessary condition for a stationary point in this function. Now my questions are: 1.) Why is this the Lagrangian for the Minimization problem? I only know the Lagrange multipliers for functions of real numbers and I'm not sure how to apply this to such a functional. The first two terms are obvious, but I can't see how exactly we obtain the term $-(q, \text{div}u)$ from the above constraint. Is $q$ the Lagrange mulitplier? 2.) Why are the equations for $u$ and $p$ the conditions for a stationary point? Here I have no idea, in what way do we take the derivatives of $L$? 3.) The lecture notes say further, that this is the reason, why this discretization does not work, because $V_\text{div}$ won't contain a good approximation for the classical solution $u$ anymore. This seems logic, but i don't see why we need the minimization problem to see this, since we already have the same restriction $(q,\text{div}v) = 0 \ \ \ \forall q \in W$ in the discrete formulation above? I would be very thankful for any hint to these questions!","I'm trying to understand the following: Let $\Omega \subset \mathbb{R}^2,\ V$ = space of (vectorvalued) continuous piecewise linear functions with zero boundary, $W = $ space of continuous piecewise linear (scalar) function. We're looking for solutions $(u,p) \in V \times W$ of the following discrete formulation of the Stokes equations: $\begin{eqnarray} a(u,v) - (p,\text{div}v) &=& (f,v) \ \ \ \forall v \in V  \\ (q, \text{div} u) &=& 0\ \ \ \forall q \in W \end{eqnarray}$ with $a(u,v):= \int_\Omega \sum\limits_i Du^iDv^idx$ and the $L^2$-scalar product $(\cdot,\cdot).$ Now the notes say, these equations are a necessary condition for the minimization problem $E(v) := \frac{1}{2}a(v,v) - (f,v) \rightarrow \text{Min}$ in $V_\text{div}$ with $\ \ \ V_\text{div}:= \{ v \in V : (q, \text{div}v) = 0 \ \ \ \forall q \in W  \}$, since the Lagrangian of the problem is given by $L(v,q) = \frac{1}{2}a(v,v) - (f,v) - (q,\text{div} v)$ and the above equations are a necessary condition for a stationary point in this function. Now my questions are: 1.) Why is this the Lagrangian for the Minimization problem? I only know the Lagrange multipliers for functions of real numbers and I'm not sure how to apply this to such a functional. The first two terms are obvious, but I can't see how exactly we obtain the term $-(q, \text{div}u)$ from the above constraint. Is $q$ the Lagrange mulitplier? 2.) Why are the equations for $u$ and $p$ the conditions for a stationary point? Here I have no idea, in what way do we take the derivatives of $L$? 3.) The lecture notes say further, that this is the reason, why this discretization does not work, because $V_\text{div}$ won't contain a good approximation for the classical solution $u$ anymore. This seems logic, but i don't see why we need the minimization problem to see this, since we already have the same restriction $(q,\text{div}v) = 0 \ \ \ \forall q \in W$ in the discrete formulation above? I would be very thankful for any hint to these questions!",,"['analysis', 'partial-differential-equations', 'numerical-methods', 'lagrange-multiplier', 'finite-element-method']"
20,$\alpha$ is an irrational number. Is $\liminf_{n\rightarrow\infty}n\{ n\alpha\}$ always positive?,is an irrational number. Is  always positive?,\alpha \liminf_{n\rightarrow\infty}n\{ n\alpha\},"We know that $$\liminf_{n\rightarrow\infty}\ n\{ n\alpha\}=L\left(\alpha\right)$$ exists and $L\left(\alpha\right)\geq 0$. Then, is the limit always greater than  $0,\ \forall\alpha\in\mathbb{R}\setminus\mathbb{Q}$? $\{n\alpha\}$ is the fractional part of $n\alpha$, i.e. $n\alpha-\left[n\alpha\right]$. I know that there are infinitely many rational numbers $\frac{p}{q}$ satisfies $\left|\alpha-\frac{p}{q}\right|<\frac{1}{q^2}$, but if $\frac{1}{q^2}$ is modified as $\frac{C}{q^k}$($C$ is a given constant), does there still exist infinitely many such rational numbers? Moreover, what about the special cases $\alpha=\sqrt{2}$ or $\alpha=2^\frac{1}{4}$?","We know that $$\liminf_{n\rightarrow\infty}\ n\{ n\alpha\}=L\left(\alpha\right)$$ exists and $L\left(\alpha\right)\geq 0$. Then, is the limit always greater than  $0,\ \forall\alpha\in\mathbb{R}\setminus\mathbb{Q}$? $\{n\alpha\}$ is the fractional part of $n\alpha$, i.e. $n\alpha-\left[n\alpha\right]$. I know that there are infinitely many rational numbers $\frac{p}{q}$ satisfies $\left|\alpha-\frac{p}{q}\right|<\frac{1}{q^2}$, but if $\frac{1}{q^2}$ is modified as $\frac{C}{q^k}$($C$ is a given constant), does there still exist infinitely many such rational numbers? Moreover, what about the special cases $\alpha=\sqrt{2}$ or $\alpha=2^\frac{1}{4}$?",,"['analysis', 'irrational-numbers', 'limsup-and-liminf']"
21,Existence of a continuous surjective map [duplicate],Existence of a continuous surjective map [duplicate],,"This question already has an answer here : Does there exist a continuous surjection from $\Bbb R^3-S^2$ to $\Bbb R^2-\{(0,0)\}$? (1 answer) Closed 5 years ago . I got this question in an exam recently. I haven't been able to solve it. The question goes like this : Does there exist a continuous surjective map from $\mathbb{R}^3\setminus \mathbb{S}^2$ to $\mathbb{R }^2\setminus \{0\}$? I was proceeding in kind of a naïve fashion by trying to construct a  suitable map from $\mathbb{R}^3$ to $\mathbb{R }^2$ which vanishes exactly on the unit sphere. But I doubt of that will help. Even though I am not quite sure I think the claim is false and will require some nontrivial use of tricky techniques on topology. Can someone help me with some hints?","This question already has an answer here : Does there exist a continuous surjection from $\Bbb R^3-S^2$ to $\Bbb R^2-\{(0,0)\}$? (1 answer) Closed 5 years ago . I got this question in an exam recently. I haven't been able to solve it. The question goes like this : Does there exist a continuous surjective map from $\mathbb{R}^3\setminus \mathbb{S}^2$ to $\mathbb{R }^2\setminus \{0\}$? I was proceeding in kind of a naïve fashion by trying to construct a  suitable map from $\mathbb{R}^3$ to $\mathbb{R }^2$ which vanishes exactly on the unit sphere. But I doubt of that will help. Even though I am not quite sure I think the claim is false and will require some nontrivial use of tricky techniques on topology. Can someone help me with some hints?",,"['analysis', 'algebraic-topology']"
22,"If $f^3$ measurable, show $f$ is measurable (Check my solution please)","If  measurable, show  is measurable (Check my solution please)",f^3 f,"I am new on this site. I am wondering if my solution is correct. I know we can do this using the fact that composition of continuous functions with a measurable one is measurable, but I want to see if my reasoning is correct for this specific example.  Let X be a set, with sigma algebra $A$. Suppose $f: X\to \mathbb{R}$, and suppose $f^3$ is measurable. Then show that $f$ is measurable. My attempt: Let $a \in \mathbb{R}$, we will show $\{x: f(x)<a\}$ is measurable. Now $\{x: f(x)<a\}=\{x: f^3(x)<a^3\}$. But $\{x: f^3(x)<a^3\}$ is measurable as $f^3$ is measurable. Hence since $a$ is arbitrary and the intervals $(-\infty,b)$ generate the borel sigma algebra, $f$ is measurable. Thanks in advance. And if its not correct, please tell me why. Edit: The sigma algebra on R is assumed to be the Borel sigma algebra.","I am new on this site. I am wondering if my solution is correct. I know we can do this using the fact that composition of continuous functions with a measurable one is measurable, but I want to see if my reasoning is correct for this specific example.  Let X be a set, with sigma algebra $A$. Suppose $f: X\to \mathbb{R}$, and suppose $f^3$ is measurable. Then show that $f$ is measurable. My attempt: Let $a \in \mathbb{R}$, we will show $\{x: f(x)<a\}$ is measurable. Now $\{x: f(x)<a\}=\{x: f^3(x)<a^3\}$. But $\{x: f^3(x)<a^3\}$ is measurable as $f^3$ is measurable. Hence since $a$ is arbitrary and the intervals $(-\infty,b)$ generate the borel sigma algebra, $f$ is measurable. Thanks in advance. And if its not correct, please tell me why. Edit: The sigma algebra on R is assumed to be the Borel sigma algebra.",,"['real-analysis', 'analysis', 'measure-theory', 'borel-sets']"
23,"Show that $f+\epsilon g\in{\rm Diff^1}(\Bbb R^m)$ for all $\epsilon\in(-\epsilon_0,\epsilon_0)$",Show that  for all,"f+\epsilon g\in{\rm Diff^1}(\Bbb R^m) \epsilon\in(-\epsilon_0,\epsilon_0)","This is an exercise on page 220 of Analysis II of Amann and Escher Here $\rm Diff^k$ means the set of diffeomorphisms where the $k$-th derivative is also an homeomorphism. My work below. The exercise have a hint that Ive not used because I dont see the relation to the exercise. The hint says ""consider the function ${\rm id_{\Bbb R^m}}+f^{-1}\circ(\epsilon g)$ and apply exercise 7 "". We want to show for each case that exists some $\epsilon_0>0$ such that $f+\epsilon g\in{\rm Diff^1}(\Bbb R^m)$ for all $\epsilon\in(-\epsilon_0,\epsilon_0)$. (a) We have that $\exists \alpha>0: |f(x)-f(y)|\ge \alpha |x-y|,\, x,y\in\Bbb R^m$. $\exists\beta>0:|g(x)-g(y)|\le\beta|x-y|,\, x,y\in\Bbb R^m$. If $f+\epsilon g$ is not injective then exists $x,y\in\Bbb R^m$ such that $f(x)-f(y)=-\epsilon(g(x)-g(y))$. Clearly $f+\epsilon g\in C^1(\Bbb R^m)$ for any $\epsilon\in\Bbb R$. From the third statement we have that $|f(x)-f(y)|=|\epsilon|\,|g(x)-g(y)|$, and from the first two statements we have that $$ \alpha|x-y|\le|f(x)-f(y)|=|\epsilon|\,|g(x)-g(y)|\le\beta|\epsilon|\,|x-y|\tag1 $$ Hence if we take $\epsilon_0=\alpha/\beta$ then we get a contradiction in $(1)$ so the function is injective. And $$ |f(x)-f(y)+\epsilon(g(x)-g(y))|\ge\big||f(x)-f(y)|-|\epsilon||g(x)-g(y)|\big|\\\ge\big|\alpha|x-y|-|\epsilon|\beta|x-y|\big|=\big|\alpha-|\epsilon|\beta\big||x-y|\tag2 $$ so $(f+\epsilon g)^{-1}​$ is Lipschitz for each $\epsilon\in(-\alpha/\beta,\alpha/\beta)​$, so it is continuous. From it continuity we find that $f+\epsilon g​$ is surjective because the image of it inverse is $\Bbb R^m​$. It remains to show that $\partial(f+\epsilon g)(x)\in\mathcal L{\rm is}(\Bbb R^m)$ for each $x\in\Bbb R^m$. From $(2)$ we have that $$ |(f+\epsilon g)(x)-(f+\epsilon g)(y)|\ge K |x-y|,\quad\forall x,y\in\Bbb R^m,\, K:=|\alpha-\beta|\epsilon||>0 $$ Thus, from the definition of directional derivative, it follow that $\partial (f+\epsilon g)(x)h\neq 0$ for all $x,h\in\Bbb R^m\setminus\{0\}$. (b) Without lose of generality suppose that this bounded set, namely $X$, is open and convex. Then $\overline X$ is compact, so its easy to see that $g$ is bounded. Also $\partial g$ is zero outside of $\overline X$ so $\partial g$ is also bounded and, by the MVT, $g$ is Lipschitz. By the same reasons $f^{-1}|_{f(X)}$ is also Lipschitz so we knows that $f+\epsilon g\in{\rm Diff^1}(X,Y)$ for some $Y\subset\Bbb R^m$. It remains to see that $f+\epsilon g$ is bijective in $\Bbb R^m$. To show injectivity we need to show that $f(x)-f(y)\neq\epsilon g(y)$ for all $x\in\Bbb R^m\setminus X$ and all $y\in X$, what is equivalent to show that $f(X)=(f+\epsilon g)(X)$. My questions: Is the part a) correctly done? Im stuck in the part b), Im unable to prove the injectivity of $f+\epsilon g$ in $\Bbb R^m$ knowing that this function is injective, separately, in the domains $\Bbb R^m\setminus X$ and $X$. I need some help here. UPDATE: I think I finally solved it. Observe that  $$ (f+\epsilon g)(x)=({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(f(x))\tag3 $$  I set $h:=g\circ f^{-1}$. Then by the mean value theorem we knows that $$ |h(y)-h(x)|\le\sup_{t\in[0,1]}|\partial h(xt+y(1-t))||x-y|\tag4 $$ and because $\partial h(f(x))=\partial g(x)[\partial f(x)]^{-1}$ is continuous and vanishes if $x\notin X$ then we can conclude that $\partial h$ is bounded, so $h$ is Lipschitz with some Lipschitz constant $K>0$. Thus using $(3)$ we can see that $$ \frac{|x'+\epsilon h(x')-y-\epsilon h(y')|}{|x'-y'|}\ge\left|1-|\epsilon|\frac{|h(x')-h(y')|}{|x'-y'|}\right|\ge |1-|\epsilon|K|\tag5 $$ whenever $|\epsilon| K<1$, and where $x':=f(x)$ and $y':=f(y)$. This shows that for $\epsilon\in(-1/K,1/K)$ we have that $$ |({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(x')-({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(y')|\ge |1-|\epsilon|K||x'-y'|\tag6 $$ From a previous result I know that a function $r\in C^1(\Bbb R^m,\Bbb R^m)$ such that $|r(x)-r(y)|\ge\alpha|x-y|$ for all $x,y\in\Bbb R^m$ and some $\alpha>0$ is a diffeomorphism in the whole $\Bbb R^m$, then ${\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1}$ is a diffeomorphism, and by $(3)$ we find that $f+\epsilon g$ it is also.$\Box$ Can someone confirm if the above is correct? Thank you.","This is an exercise on page 220 of Analysis II of Amann and Escher Here $\rm Diff^k$ means the set of diffeomorphisms where the $k$-th derivative is also an homeomorphism. My work below. The exercise have a hint that Ive not used because I dont see the relation to the exercise. The hint says ""consider the function ${\rm id_{\Bbb R^m}}+f^{-1}\circ(\epsilon g)$ and apply exercise 7 "". We want to show for each case that exists some $\epsilon_0>0$ such that $f+\epsilon g\in{\rm Diff^1}(\Bbb R^m)$ for all $\epsilon\in(-\epsilon_0,\epsilon_0)$. (a) We have that $\exists \alpha>0: |f(x)-f(y)|\ge \alpha |x-y|,\, x,y\in\Bbb R^m$. $\exists\beta>0:|g(x)-g(y)|\le\beta|x-y|,\, x,y\in\Bbb R^m$. If $f+\epsilon g$ is not injective then exists $x,y\in\Bbb R^m$ such that $f(x)-f(y)=-\epsilon(g(x)-g(y))$. Clearly $f+\epsilon g\in C^1(\Bbb R^m)$ for any $\epsilon\in\Bbb R$. From the third statement we have that $|f(x)-f(y)|=|\epsilon|\,|g(x)-g(y)|$, and from the first two statements we have that $$ \alpha|x-y|\le|f(x)-f(y)|=|\epsilon|\,|g(x)-g(y)|\le\beta|\epsilon|\,|x-y|\tag1 $$ Hence if we take $\epsilon_0=\alpha/\beta$ then we get a contradiction in $(1)$ so the function is injective. And $$ |f(x)-f(y)+\epsilon(g(x)-g(y))|\ge\big||f(x)-f(y)|-|\epsilon||g(x)-g(y)|\big|\\\ge\big|\alpha|x-y|-|\epsilon|\beta|x-y|\big|=\big|\alpha-|\epsilon|\beta\big||x-y|\tag2 $$ so $(f+\epsilon g)^{-1}​$ is Lipschitz for each $\epsilon\in(-\alpha/\beta,\alpha/\beta)​$, so it is continuous. From it continuity we find that $f+\epsilon g​$ is surjective because the image of it inverse is $\Bbb R^m​$. It remains to show that $\partial(f+\epsilon g)(x)\in\mathcal L{\rm is}(\Bbb R^m)$ for each $x\in\Bbb R^m$. From $(2)$ we have that $$ |(f+\epsilon g)(x)-(f+\epsilon g)(y)|\ge K |x-y|,\quad\forall x,y\in\Bbb R^m,\, K:=|\alpha-\beta|\epsilon||>0 $$ Thus, from the definition of directional derivative, it follow that $\partial (f+\epsilon g)(x)h\neq 0$ for all $x,h\in\Bbb R^m\setminus\{0\}$. (b) Without lose of generality suppose that this bounded set, namely $X$, is open and convex. Then $\overline X$ is compact, so its easy to see that $g$ is bounded. Also $\partial g$ is zero outside of $\overline X$ so $\partial g$ is also bounded and, by the MVT, $g$ is Lipschitz. By the same reasons $f^{-1}|_{f(X)}$ is also Lipschitz so we knows that $f+\epsilon g\in{\rm Diff^1}(X,Y)$ for some $Y\subset\Bbb R^m$. It remains to see that $f+\epsilon g$ is bijective in $\Bbb R^m$. To show injectivity we need to show that $f(x)-f(y)\neq\epsilon g(y)$ for all $x\in\Bbb R^m\setminus X$ and all $y\in X$, what is equivalent to show that $f(X)=(f+\epsilon g)(X)$. My questions: Is the part a) correctly done? Im stuck in the part b), Im unable to prove the injectivity of $f+\epsilon g$ in $\Bbb R^m$ knowing that this function is injective, separately, in the domains $\Bbb R^m\setminus X$ and $X$. I need some help here. UPDATE: I think I finally solved it. Observe that  $$ (f+\epsilon g)(x)=({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(f(x))\tag3 $$  I set $h:=g\circ f^{-1}$. Then by the mean value theorem we knows that $$ |h(y)-h(x)|\le\sup_{t\in[0,1]}|\partial h(xt+y(1-t))||x-y|\tag4 $$ and because $\partial h(f(x))=\partial g(x)[\partial f(x)]^{-1}$ is continuous and vanishes if $x\notin X$ then we can conclude that $\partial h$ is bounded, so $h$ is Lipschitz with some Lipschitz constant $K>0$. Thus using $(3)$ we can see that $$ \frac{|x'+\epsilon h(x')-y-\epsilon h(y')|}{|x'-y'|}\ge\left|1-|\epsilon|\frac{|h(x')-h(y')|}{|x'-y'|}\right|\ge |1-|\epsilon|K|\tag5 $$ whenever $|\epsilon| K<1$, and where $x':=f(x)$ and $y':=f(y)$. This shows that for $\epsilon\in(-1/K,1/K)$ we have that $$ |({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(x')-({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(y')|\ge |1-|\epsilon|K||x'-y'|\tag6 $$ From a previous result I know that a function $r\in C^1(\Bbb R^m,\Bbb R^m)$ such that $|r(x)-r(y)|\ge\alpha|x-y|$ for all $x,y\in\Bbb R^m$ and some $\alpha>0$ is a diffeomorphism in the whole $\Bbb R^m$, then ${\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1}$ is a diffeomorphism, and by $(3)$ we find that $f+\epsilon g$ it is also.$\Box$ Can someone confirm if the above is correct? Thank you.",,"['analysis', 'multivariable-calculus', 'proof-verification', 'inverse-function']"
24,Is an essentially bounded continuous function bounded?,Is an essentially bounded continuous function bounded?,,"I've just started working with $L^p$ spaces, and I've learned that a function $u:\mathbb{R}^n \rightarrow \mathbb{R}$ is essentially bounded if there exists a constant M such that $\{x \in \mathbb{R}^n : |u(x)| > M\}$ is of measure zero. We also write this as $u \in L^{\infty}(\mathbb{R}^n)$. If we assume that some $u \in L^{\infty}(\mathbb{R}^n)$ is also continuous, does it imply that $u$ is bounded, not just essentially bounded? I think it would be possible to prove this by contradiction; if we assume $u$ is unbounded on a set of measure zero, by continuity we can find a small (but of nonzero measure) region around a point with arbitrarily large $u(x)$. However, I haven't really done any measure theory so I'm not sure how to go about the details.","I've just started working with $L^p$ spaces, and I've learned that a function $u:\mathbb{R}^n \rightarrow \mathbb{R}$ is essentially bounded if there exists a constant M such that $\{x \in \mathbb{R}^n : |u(x)| > M\}$ is of measure zero. We also write this as $u \in L^{\infty}(\mathbb{R}^n)$. If we assume that some $u \in L^{\infty}(\mathbb{R}^n)$ is also continuous, does it imply that $u$ is bounded, not just essentially bounded? I think it would be possible to prove this by contradiction; if we assume $u$ is unbounded on a set of measure zero, by continuity we can find a small (but of nonzero measure) region around a point with arbitrarily large $u(x)$. However, I haven't really done any measure theory so I'm not sure how to go about the details.",,"['analysis', 'measure-theory', 'lp-spaces']"
25,When do we have $\sup_{x}\int fdy=\int \sup_{x}fdy$?,When do we have ?,\sup_{x}\int fdy=\int \sup_{x}fdy,"When do we have $$\sup_{x}\int fdy=\int (\sup_{x}f)dy$$? Here, $f$ is a function of $x$ and $y$. Some say: Use The monotone convergence theorem, but I really don't understand that hint.","When do we have $$\sup_{x}\int fdy=\int (\sup_{x}f)dy$$? Here, $f$ is a function of $x$ and $y$. Some say: Use The monotone convergence theorem, but I really don't understand that hint.",,"['analysis', 'measure-theory']"
26,"""Anti-curl"" operator in a topologically non-trivial domain","""Anti-curl"" operator in a topologically non-trivial domain",,"$\DeclareMathOperator{\div}{div}\DeclareMathOperator{\rot}{rot}$Let $D$ be an open bounded connected domain in $\mathbb R^3$ and let $X$ be a smooth vector field in $D$ such that $$    \int_S X \cdot dS = 0 \tag{1} $$ for any closed surface $S$ in $D$. In particular, $\mathop{\mathrm{div}} X = 0$ in $D$ since $$    \mathop{\mathrm{div}} X(x) = \lim_{\varepsilon \to +0} \tfrac{3}{4\pi \varepsilon^3} \int_{|x-y| = \varepsilon} X(y) \cdot dS_y = 0 $$ by the Ostrogradski-Gauss formula and the mean-value formula. But condition (1) is stronger than just $\mathop{\mathrm{div}} X = 0$ in $D$, it also means that $X \cdot dS$ integrates to $0$ for any closed surface $S$ enclosing a ""hole"" in $D$. I think that (1) implies that $X = \mathop{\mathrm{rot}} B$ for some smooth vector field $B$ in $D$, and I think that I can show it using differential forms. But is it possible to prove this and to find an explicit expression for such $B$ using ""elementary"" tools (without de Rham's theorem)? Here are some thoughts. As $\div X = 0$, there exists a small covering of $D$ by $U_\alpha$'s such that $X = \rot Y_\alpha$ in $U_\alpha$. Then $$    Y_\alpha - Y_\beta = \nabla \varphi_{\alpha\beta} \quad \text{in $U_\alpha \cap U_\beta$}, $$ for some $\varphi_{\alpha\beta} \in C^\infty(U_\alpha \cap U_\beta)$ such that $\varphi_{\alpha\beta}+\varphi_{\beta\alpha}=0$. Next, $$    \varphi_{\alpha \beta}+\varphi_{\beta\gamma}+\varphi_{\gamma\alpha} = c_{\alpha\beta\gamma} \in \mathbb R \quad \text{in $U_\alpha \cap U_\beta \cap U_\gamma$} $$ where $c_{\alpha\beta\gamma}$ satisfy the $2$-cocycle conditions. Condition (1) must somehow imply that $c_{\alpha\beta\gamma}$ is a $2$-coboundary but I don't see why. I tried to rewrite (1) as $$   \int_S X \cdot dS = \sum_\alpha \int_{S\cap U_\alpha} \rot Y_\alpha \cdot dS - \sum_{\alpha\beta}\int_{S \cap U_\alpha \cap U_\beta} \rot Y_\alpha\cdot dS + \sum_{\alpha\beta\gamma}\int_{S \cap U_\alpha \cap U_\beta \cap U_\gamma} \rot Y_\alpha \cdot dS \\  = \sum_\alpha \int_{S \cap \partial U_\alpha} Y_\alpha \cdot dl+ \cdots = 0 $$ but I don't see how to deduce from here some equations on $c_{\alpha\beta\gamma}$.","$\DeclareMathOperator{\div}{div}\DeclareMathOperator{\rot}{rot}$Let $D$ be an open bounded connected domain in $\mathbb R^3$ and let $X$ be a smooth vector field in $D$ such that $$    \int_S X \cdot dS = 0 \tag{1} $$ for any closed surface $S$ in $D$. In particular, $\mathop{\mathrm{div}} X = 0$ in $D$ since $$    \mathop{\mathrm{div}} X(x) = \lim_{\varepsilon \to +0} \tfrac{3}{4\pi \varepsilon^3} \int_{|x-y| = \varepsilon} X(y) \cdot dS_y = 0 $$ by the Ostrogradski-Gauss formula and the mean-value formula. But condition (1) is stronger than just $\mathop{\mathrm{div}} X = 0$ in $D$, it also means that $X \cdot dS$ integrates to $0$ for any closed surface $S$ enclosing a ""hole"" in $D$. I think that (1) implies that $X = \mathop{\mathrm{rot}} B$ for some smooth vector field $B$ in $D$, and I think that I can show it using differential forms. But is it possible to prove this and to find an explicit expression for such $B$ using ""elementary"" tools (without de Rham's theorem)? Here are some thoughts. As $\div X = 0$, there exists a small covering of $D$ by $U_\alpha$'s such that $X = \rot Y_\alpha$ in $U_\alpha$. Then $$    Y_\alpha - Y_\beta = \nabla \varphi_{\alpha\beta} \quad \text{in $U_\alpha \cap U_\beta$}, $$ for some $\varphi_{\alpha\beta} \in C^\infty(U_\alpha \cap U_\beta)$ such that $\varphi_{\alpha\beta}+\varphi_{\beta\alpha}=0$. Next, $$    \varphi_{\alpha \beta}+\varphi_{\beta\gamma}+\varphi_{\gamma\alpha} = c_{\alpha\beta\gamma} \in \mathbb R \quad \text{in $U_\alpha \cap U_\beta \cap U_\gamma$} $$ where $c_{\alpha\beta\gamma}$ satisfy the $2$-cocycle conditions. Condition (1) must somehow imply that $c_{\alpha\beta\gamma}$ is a $2$-coboundary but I don't see why. I tried to rewrite (1) as $$   \int_S X \cdot dS = \sum_\alpha \int_{S\cap U_\alpha} \rot Y_\alpha \cdot dS - \sum_{\alpha\beta}\int_{S \cap U_\alpha \cap U_\beta} \rot Y_\alpha\cdot dS + \sum_{\alpha\beta\gamma}\int_{S \cap U_\alpha \cap U_\beta \cap U_\gamma} \rot Y_\alpha \cdot dS \\  = \sum_\alpha \int_{S \cap \partial U_\alpha} Y_\alpha \cdot dl+ \cdots = 0 $$ but I don't see how to deduce from here some equations on $c_{\alpha\beta\gamma}$.",,"['analysis', 'multivariable-calculus']"
27,Monotone convergence theorem on $\ln(2)$ sum?,Monotone convergence theorem on  sum?,\ln(2),"Given $\frac{1}{1+x}=\sum^{\infty}_{k=0}(1-x)x^{2k}$ for $x\in[0,1)$ I shall apply the 'monotone convergence theorem' on $[0,1)$ for calculating $\sum^{\infty}_{k=1}\frac{(-1)^{k+1}}{k}$. In Wikipedia it's said that $\sum^{\infty}_{k=0}\frac{1}{n^k}\binom{n}{k}=(1+\frac{1}{n})^n$ and I don't know how they apply this theorem on it :/ Maybe someone can explain me it with the example from Wikipedia :) I know that $\int_0^1\frac{1}{1+x}dx=[ln(x)]_0^1$, but don't know how to put this in relation with the second sum ? Thanks in advance.","Given $\frac{1}{1+x}=\sum^{\infty}_{k=0}(1-x)x^{2k}$ for $x\in[0,1)$ I shall apply the 'monotone convergence theorem' on $[0,1)$ for calculating $\sum^{\infty}_{k=1}\frac{(-1)^{k+1}}{k}$. In Wikipedia it's said that $\sum^{\infty}_{k=0}\frac{1}{n^k}\binom{n}{k}=(1+\frac{1}{n})^n$ and I don't know how they apply this theorem on it :/ Maybe someone can explain me it with the example from Wikipedia :) I know that $\int_0^1\frac{1}{1+x}dx=[ln(x)]_0^1$, but don't know how to put this in relation with the second sum ? Thanks in advance.",,['analysis']
28,Laplace-Beltrami Operator of p-forms,Laplace-Beltrami Operator of p-forms,,"I'm trying to compute the Laplace-Beltrami Operator in local coordinates of some Riemannian manifold $M$ . By definition, Laplace-Beltrami Operator $$\Delta=d\delta+\delta d$$ acts in p-form on $M$ , where is defined by $$\delta=(-1)^{n(p+1)+1}\star d\star$$ and $\star$ is the Hodge operator. So, I started with the $M=\mathbb{R}^n$ and by direct calculation is simple to verifie that, in coordinates $(x_1,\dots,x_n)$ if $\omega=fdx_{i_1}\wedge\dots\wedge dx_{i_p}$ is a p-form on $\mathbb{R}^n$ then $$\Delta\omega=-\sum_{s=1}^n \frac{\partial^2f}{\partial x_s ^2}dx_{i_1}\wedge\dots\wedge dx_{i_p}.$$ The question is if I have a local coordinates of $\mathbb{S}^n$ , for example, $F:\mathbb{S}^n \setminus{(0,\dots,0,1)}\subset\mathbb{R}^{n+1}\to \mathbb{R}^n$ defined by $$F(x_1,\dots,x_{n+1})=\frac{1}{x_{n+1}-1}(x_1,\dots,x_n)$$ how can I write the Laplace-Beltrami operator of the p-form $\omega=fdx_{i_1}\wedge\dots\wedge dx_{i_p}$ ? Any idea? Thanks so much.","I'm trying to compute the Laplace-Beltrami Operator in local coordinates of some Riemannian manifold . By definition, Laplace-Beltrami Operator acts in p-form on , where is defined by and is the Hodge operator. So, I started with the and by direct calculation is simple to verifie that, in coordinates if is a p-form on then The question is if I have a local coordinates of , for example, defined by how can I write the Laplace-Beltrami operator of the p-form ? Any idea? Thanks so much.","M \Delta=d\delta+\delta d M \delta=(-1)^{n(p+1)+1}\star d\star \star M=\mathbb{R}^n (x_1,\dots,x_n) \omega=fdx_{i_1}\wedge\dots\wedge dx_{i_p} \mathbb{R}^n \Delta\omega=-\sum_{s=1}^n \frac{\partial^2f}{\partial x_s ^2}dx_{i_1}\wedge\dots\wedge dx_{i_p}. \mathbb{S}^n F:\mathbb{S}^n \setminus{(0,\dots,0,1)}\subset\mathbb{R}^{n+1}\to \mathbb{R}^n F(x_1,\dots,x_{n+1})=\frac{1}{x_{n+1}-1}(x_1,\dots,x_n) \omega=fdx_{i_1}\wedge\dots\wedge dx_{i_p}","['analysis', 'differential-geometry', 'riemannian-geometry']"
29,Is this a bad proof for Cauchy mean value theorem or is it valid?,Is this a bad proof for Cauchy mean value theorem or is it valid?,,"By MVT, $$\frac{f(b)-f(a)}{b-a}=f'(c)$$ $$\frac{g(b)-g(a)}{b-a}=g'(c)$$ so, $$\frac{f(b)-f(a)}{g(b)-g(a)} = \frac{\frac{f(b)-f(a)}{b-a} }{\frac{g(b)-g(a)}{b-a}}=\frac{f'(c)}{g'(c)}$$ I think it is not a valid proof but I am having a discussion with a classmate and I am not sure anymore...","By MVT, $$\frac{f(b)-f(a)}{b-a}=f'(c)$$ $$\frac{g(b)-g(a)}{b-a}=g'(c)$$ so, $$\frac{f(b)-f(a)}{g(b)-g(a)} = \frac{\frac{f(b)-f(a)}{b-a} }{\frac{g(b)-g(a)}{b-a}}=\frac{f'(c)}{g'(c)}$$ I think it is not a valid proof but I am having a discussion with a classmate and I am not sure anymore...",,"['calculus', 'analysis']"
30,Does the tangent space functor commute with fibered products?,Does the tangent space functor commute with fibered products?,,"Let $(M,p)\xrightarrow{\ \ F\ \ }(S,s)\xleftarrow{\ \ G\ \ }(N,q)$ be smooth maps of pointed differentiable manifolds. If $F$ and $G$ are transversal then the fibered product $(M,p)\times_{(S,s)}(N,q)$ exists, the underlying pointed topological space is given by the fibered product $(M\times_SN,(p,q))$ and the tangent space is given by the fibered product $$T_{(p,q)}( (M,p)\times_{(S,s)}(N,q))=T_p(M)\times_{T_s(S)}T_q(N)\,.$$In other words, fibered products of transversal morphisms of pointed differentiable manifolds exists and are preserved both by the forgetful functor to the category of pointed topological spaces and by the tangent space functor to the category of $\mathbb{K}$-vector spaces. A reference for this is Theorem 5.47 in Manifolds, Sheaves and Cohomology by Torsten Wedhorn. I know that not all fibered products exist in the category of (pointed) manifolds, see for example here . However, my question is the following: Do the forgetful functor $\mathbf{Man}_\ast\longrightarrow\mathbf{Top}_\ast$ and the tangent space functor $\mathbf{Man}_\ast\longrightarrow\mathbf{Vect}_\mathbb{K}$ commute with the formation of fibered products in the categorical sense? I.e. are universal cones sent to universal cones whenever they exist ? Is there anything unclear about the question or has nobody ever thought about it?","Let $(M,p)\xrightarrow{\ \ F\ \ }(S,s)\xleftarrow{\ \ G\ \ }(N,q)$ be smooth maps of pointed differentiable manifolds. If $F$ and $G$ are transversal then the fibered product $(M,p)\times_{(S,s)}(N,q)$ exists, the underlying pointed topological space is given by the fibered product $(M\times_SN,(p,q))$ and the tangent space is given by the fibered product $$T_{(p,q)}( (M,p)\times_{(S,s)}(N,q))=T_p(M)\times_{T_s(S)}T_q(N)\,.$$In other words, fibered products of transversal morphisms of pointed differentiable manifolds exists and are preserved both by the forgetful functor to the category of pointed topological spaces and by the tangent space functor to the category of $\mathbb{K}$-vector spaces. A reference for this is Theorem 5.47 in Manifolds, Sheaves and Cohomology by Torsten Wedhorn. I know that not all fibered products exist in the category of (pointed) manifolds, see for example here . However, my question is the following: Do the forgetful functor $\mathbf{Man}_\ast\longrightarrow\mathbf{Top}_\ast$ and the tangent space functor $\mathbf{Man}_\ast\longrightarrow\mathbf{Vect}_\mathbb{K}$ commute with the formation of fibered products in the categorical sense? I.e. are universal cones sent to universal cones whenever they exist ? Is there anything unclear about the question or has nobody ever thought about it?",,"['analysis', 'differential-geometry']"
31,Baby Rudin Exercise 4.13 Alternate Proof Verification,Baby Rudin Exercise 4.13 Alternate Proof Verification,,"I would like to know if my proof of ex 4.13 is correct.  Thanks! Exercise 4.13 in Rudin asks: Let $E$ be a dense subset of a metric space $X$, and let $f$ be a uniformly continuous real function defined on $E$. Prove that $f$ has a continuous extension from $E$ to $X$. My proof: Suppose $p \in X \setminus E$.  Then there is a sequence $\{p_n\} \to p$ in $E$, because $p$ must be a limit point of $E$. Since $f$ is uniformly continuous on $E$, we have for each $\epsilon > 0$ the existence of a $\delta > 0$ such that $$d(p_n, p_m) < \delta \implies d(f(p_n), f(p_m))<\epsilon$$  However, since $\{p_n\}$ converges, it is also Cauchy so that there is some $N$ such that $$n, m \geq N \implies d(p_n, p_m) < \delta \implies d(f(p_n), f(p_m))<\epsilon$$ This statement implies that $\{f(p_n)\}$ is Cauchy.  Now since $f$ is real-valued, cauchy sequences converge and thus $\{f(p_n)\}$ converges.  Now just define $$g(p) = f(p) \text{ if } p \in E$$ $$g(p) = \lim_{n\to\infty} f(p_n) \text{ for } p \notin E, \{p_n\} \to p \text{ in } E$$ This is the desired continuous extension.($p_n$ is an arbitrary sequence in $E$ that tends to $p$)","I would like to know if my proof of ex 4.13 is correct.  Thanks! Exercise 4.13 in Rudin asks: Let $E$ be a dense subset of a metric space $X$, and let $f$ be a uniformly continuous real function defined on $E$. Prove that $f$ has a continuous extension from $E$ to $X$. My proof: Suppose $p \in X \setminus E$.  Then there is a sequence $\{p_n\} \to p$ in $E$, because $p$ must be a limit point of $E$. Since $f$ is uniformly continuous on $E$, we have for each $\epsilon > 0$ the existence of a $\delta > 0$ such that $$d(p_n, p_m) < \delta \implies d(f(p_n), f(p_m))<\epsilon$$  However, since $\{p_n\}$ converges, it is also Cauchy so that there is some $N$ such that $$n, m \geq N \implies d(p_n, p_m) < \delta \implies d(f(p_n), f(p_m))<\epsilon$$ This statement implies that $\{f(p_n)\}$ is Cauchy.  Now since $f$ is real-valued, cauchy sequences converge and thus $\{f(p_n)\}$ converges.  Now just define $$g(p) = f(p) \text{ if } p \in E$$ $$g(p) = \lim_{n\to\infty} f(p_n) \text{ for } p \notin E, \{p_n\} \to p \text{ in } E$$ This is the desired continuous extension.($p_n$ is an arbitrary sequence in $E$ that tends to $p$)",,"['real-analysis', 'analysis', 'continuity', 'uniform-continuity', 'cauchy-sequences']"
32,Most general space on which we can do calculus,Most general space on which we can do calculus,,"I have two somewhat related questions: Question 1: What is the most general space (set of objects) on which we can do calculus? Is it a normed space, or can we relax the conditions a bit further? Subquestion 1a: Does it matter how we define ""do calculus""? I would think that the most basic idea of being able to do calculus would be being able to take limits.  But does that necessarily imply that we can do something like differentiation and/ or integration? Question 2: What is the least amount of structure we need to add to an affine space before we can do calculus? A general affine space has no inner product or even a metric.  Though one can compare straight line distances, I don't imagine that'd be enough to take limits with.  But we could define some set of vectors in the associated vector space to be the ""unit sphere"" such that we'd be able to measure distances.  Would that be enough, or would we need to add a inner product?  In fact, maybe that wouldn't even be enough for practical purposes, maybe we'd need an inner product and for our space to be complete.  Does anyone have an idea of the minimum amount of structure we'd need?  Would there even be a way to show that less structure necessarily means we wouldn't be able to do calculus?","I have two somewhat related questions: Question 1: What is the most general space (set of objects) on which we can do calculus? Is it a normed space, or can we relax the conditions a bit further? Subquestion 1a: Does it matter how we define ""do calculus""? I would think that the most basic idea of being able to do calculus would be being able to take limits.  But does that necessarily imply that we can do something like differentiation and/ or integration? Question 2: What is the least amount of structure we need to add to an affine space before we can do calculus? A general affine space has no inner product or even a metric.  Though one can compare straight line distances, I don't imagine that'd be enough to take limits with.  But we could define some set of vectors in the associated vector space to be the ""unit sphere"" such that we'd be able to measure distances.  Would that be enough, or would we need to add a inner product?  In fact, maybe that wouldn't even be enough for practical purposes, maybe we'd need an inner product and for our space to be complete.  Does anyone have an idea of the minimum amount of structure we'd need?  Would there even be a way to show that less structure necessarily means we wouldn't be able to do calculus?",,"['calculus', 'analysis', 'affine-geometry']"
33,Relation between Karamata's and Hardy-Littlewood's inequalities,Relation between Karamata's and Hardy-Littlewood's inequalities,,"In the field of (elementary) classical inequalities one of the most famous tools is the majorization inequality due to Karamata [1] (also known as Hardy-Littlewood-Polya ). In its integral version, it states that: If $\psi_1, \psi_2 : [a,b] \rightarrow \mathbb{R_{>0}}$ are non-negative measurable functions and, for example, $\psi_1$ ""majorates"" over $\psi_2$ (usually donated via $\psi_2 \succeq \psi_2$): $$\int_{a}^{s} \psi_1 \, dx \geq \int_{a}^{s} \psi_2 \, dx, \, \forall s \in [a,b) \> \text{ and } \> \int_{a}^{b} \psi_1 \, dx = \int_{a}^{b} \psi_2 \, dx$$   then for any increasing $\phi : [a,b] \rightarrow \mathbb{R}$ holds:   $$\int_{a}^{b} \psi_1 \phi \, dx \geq \int_{a}^{b} \psi_2 \phi \, dx .$$ From here one may derive [2] the majority of the classical inequalities: $AM \geq GM$, Muirhead, Schur, Cauchy-Schwartz, Hölder, Minkowski, Jensen, Chebyshev, etc. Besides this one, in the literature there is another famous inequality [3] , named after Hardy and Littlewood - the generalized rearrangement inequality: Let $\phi^*$ donates the symmetric (decreasing) rearrangement of a measurable, non-negative, asymptotically vanishing function $\phi$ defined over $\mathbb{R}^n$ - roughly speaking, $\phi^* (x)$ is the ""height"" at which the ""size"" of the level set $\{ t : \phi(t) > \phi^* (x) \}$ is equal to $x$ (please, excuse me for not going into great depth with the definitions). Then for any two functions $f, g : \mathbb{R}^n \rightarrow \mathbb{R}_{>0}$ of the above type:   $$ \int f g \, dx \leq \int f^* g^* \, dx .$$ My question is whether there is any relation between the above inequalities and, more generally, between the concepts of majorization and symmetric rearrangement? I'm sorry if the answer is obvious - I haven't given it much thought...","In the field of (elementary) classical inequalities one of the most famous tools is the majorization inequality due to Karamata [1] (also known as Hardy-Littlewood-Polya ). In its integral version, it states that: If $\psi_1, \psi_2 : [a,b] \rightarrow \mathbb{R_{>0}}$ are non-negative measurable functions and, for example, $\psi_1$ ""majorates"" over $\psi_2$ (usually donated via $\psi_2 \succeq \psi_2$): $$\int_{a}^{s} \psi_1 \, dx \geq \int_{a}^{s} \psi_2 \, dx, \, \forall s \in [a,b) \> \text{ and } \> \int_{a}^{b} \psi_1 \, dx = \int_{a}^{b} \psi_2 \, dx$$   then for any increasing $\phi : [a,b] \rightarrow \mathbb{R}$ holds:   $$\int_{a}^{b} \psi_1 \phi \, dx \geq \int_{a}^{b} \psi_2 \phi \, dx .$$ From here one may derive [2] the majority of the classical inequalities: $AM \geq GM$, Muirhead, Schur, Cauchy-Schwartz, Hölder, Minkowski, Jensen, Chebyshev, etc. Besides this one, in the literature there is another famous inequality [3] , named after Hardy and Littlewood - the generalized rearrangement inequality: Let $\phi^*$ donates the symmetric (decreasing) rearrangement of a measurable, non-negative, asymptotically vanishing function $\phi$ defined over $\mathbb{R}^n$ - roughly speaking, $\phi^* (x)$ is the ""height"" at which the ""size"" of the level set $\{ t : \phi(t) > \phi^* (x) \}$ is equal to $x$ (please, excuse me for not going into great depth with the definitions). Then for any two functions $f, g : \mathbb{R}^n \rightarrow \mathbb{R}_{>0}$ of the above type:   $$ \int f g \, dx \leq \int f^* g^* \, dx .$$ My question is whether there is any relation between the above inequalities and, more generally, between the concepts of majorization and symmetric rearrangement? I'm sorry if the answer is obvious - I haven't given it much thought...",,"['analysis', 'measure-theory', 'integral-inequality']"
34,Possible correction to Exercise $5.15$ in Rudin's Principles,Possible correction to Exercise  in Rudin's Principles,5.15,"Here's Exercise $5.15$ in Rudin's Principles of Mathematical Analysis (Page $115$): Suppose $a \in \mathbb{R}^1$, $f$ is a twice-differentiable real function on  $(a,\infty)$, and $M_0$, $M_1$, $M_2$ are the least upper bounds of $|f(x)|$, $|f'(x)|$, $|f''(x)|$, respectively, on $(a,\infty)$. Prove that \begin{align*} M_1^2 \leq 4 M_0 M_2.  \end{align*} Setting $a = 0$ and $f(x) = x$, we have $f'(x) = 1$, and $f''(x) = 0$, so \begin{align*} M_0 &= \sup_{x \in (0, \infty)} \{ x \} = \infty, \\ M_1 &= \sup_{x \in (0, \infty)} \{ 1 \} =  1, \\ M_2 &= \sup_{x \in (0, \infty)} \{ 0 \} = 0. \end{align*} In this case the inequality reads $1 \leq 4 \cdot \infty \cdot 0$, yet as Rudin comments after Theorem $4.34$ on page $98$, $\infty \cdot 0$ is not defined in the extended real numbers. To fix this, should we assume that $M_i$ is finite for $i = 0 , 1 , 2$?","Here's Exercise $5.15$ in Rudin's Principles of Mathematical Analysis (Page $115$): Suppose $a \in \mathbb{R}^1$, $f$ is a twice-differentiable real function on  $(a,\infty)$, and $M_0$, $M_1$, $M_2$ are the least upper bounds of $|f(x)|$, $|f'(x)|$, $|f''(x)|$, respectively, on $(a,\infty)$. Prove that \begin{align*} M_1^2 \leq 4 M_0 M_2.  \end{align*} Setting $a = 0$ and $f(x) = x$, we have $f'(x) = 1$, and $f''(x) = 0$, so \begin{align*} M_0 &= \sup_{x \in (0, \infty)} \{ x \} = \infty, \\ M_1 &= \sup_{x \in (0, \infty)} \{ 1 \} =  1, \\ M_2 &= \sup_{x \in (0, \infty)} \{ 0 \} = 0. \end{align*} In this case the inequality reads $1 \leq 4 \cdot \infty \cdot 0$, yet as Rudin comments after Theorem $4.34$ on page $98$, $\infty \cdot 0$ is not defined in the extended real numbers. To fix this, should we assume that $M_i$ is finite for $i = 0 , 1 , 2$?",,"['calculus', 'analysis', 'derivatives']"
35,On the form of injective function $f:\mathbb R^+ \to \mathbb R^+$ such that $\|u\|:=f^{-1}\Big(\sum_{i=1}^nf(|u_i|)\Big)$ is a norm,On the form of injective function  such that  is a norm,f:\mathbb R^+ \to \mathbb R^+ \|u\|:=f^{-1}\Big(\sum_{i=1}^nf(|u_i|)\Big),"Let $f:\mathbb R^+ \to \mathbb R^+$ be injective such that on $\mathbb R^n , n>1)$ , $\|u\|:=f^{-1}\Big(\sum_{i=1}^nf(|u_i|)\Big)$ , where $u=(u_1,u_,\ldots,u_n)$ , defines a norm ; then is it true that for some $p\ge1$ , $f(x)=cx^p , \forall x \in \mathbb R^+$ ?","Let $f:\mathbb R^+ \to \mathbb R^+$ be injective such that on $\mathbb R^n , n>1)$ , $\|u\|:=f^{-1}\Big(\sum_{i=1}^nf(|u_i|)\Big)$ , where $u=(u_1,u_,\ldots,u_n)$ , defines a norm ; then is it true that for some $p\ge1$ , $f(x)=cx^p , \forall x \in \mathbb R^+$ ?",,['analysis']
36,Calderon-Zygmund Operator Associated to Zero Kernel,Calderon-Zygmund Operator Associated to Zero Kernel,,"We say that a Calderon-Zygmund operator (CZO) $T:L^{2}(\mathbb{R}^{n})\rightarrow L^{2}(\mathbb{R}^{n})$ (i.e. a bounded linear operator) is associated to a CZ kernel $K:\mathbb{R}^{n}\times\mathbb{R}^{n}\setminus\Delta$ ($\Delta$ denotes the diagonal subset) if the following holds: for $f\in L_{c}^{2}(\mathbb{R}^{n})$ fixed, for almost everywhere (a.e.) $x\notin\text{supp}(f)$,     $$Tf(x)=\int_{\mathbb{R}^{n}}K(x,y)f(y)dy$$ Suppose that a CZO $T:L^{2}(\mathbb{R}^{n})\rightarrow L^{2}(\mathbb{R}^{n})$ is associated the kernel $K(x,y)=0$. In the hint for Question 2 in the Exercises section of Tao's Lecture Notes 4 , he suggests that we can show $T$ is given by pointwise multiplication by a bounded function (i.e. $Tf=bf$, where $b\in L^{\infty}$) by observing that the set function     $$E\mapsto \langle{T(\chi_{E}),\chi_{E}}\rangle$$ is an absolutely continuous measure and then applying the Radon-Nikodym theorem. I see where the Radon-Nikodym theorem gives us a measurable function $b$ such that     $$\langle{Tf,f}\rangle=\int_{\mathbb{R}^{n}}b\left|f\right|^{2},\quad\forall f\in L^{2}(\mathbb{R}^{n})$$ by the boundedness of $T$, the density of simple functions in $L^{2}$, and the linearity of the integral. But I am failing to see (perhaps very foolishly) why we can readily deduce that $Tf=bf$ for all $f\in L^{2}$. I believe I can obtain the desired result using a different argument, as shown below; but I would still like to see how to make Tao's suggestion work. Does anyone have a suggestion? Let $g\in L^{2}(\mathbb{R}^{n})$ have compact support. For any cube $Q\subset\mathbb{R}^{n}$,     $$T(\chi_{Q}g)(x)=\int_{\mathbb{R}^{n}}K(x,y)\chi_{Q}(y)g(y)dy=0 \quad \text{ a.e. } x\notin\overline{Q}$$ and     $$T(\chi_{Q^{c}}g)(x)=\int_{\mathbb{R}^{n}}K(x,y)\chi_{Q^{c}}(y)g(y)dy=0 \quad \text{ a.e. } x\in Q^{o}$$ Since $T(\chi_{Q}g)=T(g)-T(\chi_{Q^{c}}g) \text{ a.e. }$, we conclude that     $$T(\chi_{Q}g)(x)=\chi_{Q}(x)(Tg)(x) \quad \text{ a.e. }$$ Since the collection of all cubes with rational vertices is countable and finite linear combinations of the characteristic functions of such cubes are dense in $L^{2}$, the boundedness of $T$ yields     $$T(fg)=f(Tg) \quad\text{ a.e. },\qquad\forall f\in L^{2}(\mathbb{R}^{n})$$ To see that $T$ is given by pointwise multiplication by a bounded function $b$, let $B_{j}$ be the ball centered at the origin of radius $j$. Observe that if $j'\geq j$, then $\chi_{B_{j}}\chi_{B_{j'}}=\chi_{B_{j}}$ and therefore     $$T(\chi_{B_{j}})=T(\chi_{B_{j}}\chi_{B_{j'}})=\chi_{B_{j}}T(B_{j'}) \quad \text{ a.e. }$$ by our previous result. Thus, we can define almost everywhere a measurable function $b$ by     $$b(x)=T(\chi_{B_{j}})(x), \quad x\in B_{j}$$ For an $f\in L^{2}(\mathbb{R}^{n})$, let $f_{j}=f\chi_{B_{j}}$.Then by the continuity of $T$ and a.e. convergence $f_{j}\rightarrow f$     $$T(f)=\lim_{j\rightarrow\infty}T(f_{j})=\lim_{j\rightarrow\infty}bf_{j}=bf \quad \text{ a.e. }$$ To see that $b$ is bounded, observe that     $$\left\|T\right\|_{L^{2}\rightarrow L^{2}}=\sup_{f\in L^{2}}\left\|Tf\right\|_{L^{2}}=\sup_{f\in L_{c}^{2}}\left\|bf\right\|_{L^{2}}=\left\|b\right\|_{L^{\infty}}$$","We say that a Calderon-Zygmund operator (CZO) $T:L^{2}(\mathbb{R}^{n})\rightarrow L^{2}(\mathbb{R}^{n})$ (i.e. a bounded linear operator) is associated to a CZ kernel $K:\mathbb{R}^{n}\times\mathbb{R}^{n}\setminus\Delta$ ($\Delta$ denotes the diagonal subset) if the following holds: for $f\in L_{c}^{2}(\mathbb{R}^{n})$ fixed, for almost everywhere (a.e.) $x\notin\text{supp}(f)$,     $$Tf(x)=\int_{\mathbb{R}^{n}}K(x,y)f(y)dy$$ Suppose that a CZO $T:L^{2}(\mathbb{R}^{n})\rightarrow L^{2}(\mathbb{R}^{n})$ is associated the kernel $K(x,y)=0$. In the hint for Question 2 in the Exercises section of Tao's Lecture Notes 4 , he suggests that we can show $T$ is given by pointwise multiplication by a bounded function (i.e. $Tf=bf$, where $b\in L^{\infty}$) by observing that the set function     $$E\mapsto \langle{T(\chi_{E}),\chi_{E}}\rangle$$ is an absolutely continuous measure and then applying the Radon-Nikodym theorem. I see where the Radon-Nikodym theorem gives us a measurable function $b$ such that     $$\langle{Tf,f}\rangle=\int_{\mathbb{R}^{n}}b\left|f\right|^{2},\quad\forall f\in L^{2}(\mathbb{R}^{n})$$ by the boundedness of $T$, the density of simple functions in $L^{2}$, and the linearity of the integral. But I am failing to see (perhaps very foolishly) why we can readily deduce that $Tf=bf$ for all $f\in L^{2}$. I believe I can obtain the desired result using a different argument, as shown below; but I would still like to see how to make Tao's suggestion work. Does anyone have a suggestion? Let $g\in L^{2}(\mathbb{R}^{n})$ have compact support. For any cube $Q\subset\mathbb{R}^{n}$,     $$T(\chi_{Q}g)(x)=\int_{\mathbb{R}^{n}}K(x,y)\chi_{Q}(y)g(y)dy=0 \quad \text{ a.e. } x\notin\overline{Q}$$ and     $$T(\chi_{Q^{c}}g)(x)=\int_{\mathbb{R}^{n}}K(x,y)\chi_{Q^{c}}(y)g(y)dy=0 \quad \text{ a.e. } x\in Q^{o}$$ Since $T(\chi_{Q}g)=T(g)-T(\chi_{Q^{c}}g) \text{ a.e. }$, we conclude that     $$T(\chi_{Q}g)(x)=\chi_{Q}(x)(Tg)(x) \quad \text{ a.e. }$$ Since the collection of all cubes with rational vertices is countable and finite linear combinations of the characteristic functions of such cubes are dense in $L^{2}$, the boundedness of $T$ yields     $$T(fg)=f(Tg) \quad\text{ a.e. },\qquad\forall f\in L^{2}(\mathbb{R}^{n})$$ To see that $T$ is given by pointwise multiplication by a bounded function $b$, let $B_{j}$ be the ball centered at the origin of radius $j$. Observe that if $j'\geq j$, then $\chi_{B_{j}}\chi_{B_{j'}}=\chi_{B_{j}}$ and therefore     $$T(\chi_{B_{j}})=T(\chi_{B_{j}}\chi_{B_{j'}})=\chi_{B_{j}}T(B_{j'}) \quad \text{ a.e. }$$ by our previous result. Thus, we can define almost everywhere a measurable function $b$ by     $$b(x)=T(\chi_{B_{j}})(x), \quad x\in B_{j}$$ For an $f\in L^{2}(\mathbb{R}^{n})$, let $f_{j}=f\chi_{B_{j}}$.Then by the continuity of $T$ and a.e. convergence $f_{j}\rightarrow f$     $$T(f)=\lim_{j\rightarrow\infty}T(f_{j})=\lim_{j\rightarrow\infty}bf_{j}=bf \quad \text{ a.e. }$$ To see that $b$ is bounded, observe that     $$\left\|T\right\|_{L^{2}\rightarrow L^{2}}=\sup_{f\in L^{2}}\left\|Tf\right\|_{L^{2}}=\sup_{f\in L_{c}^{2}}\left\|bf\right\|_{L^{2}}=\left\|b\right\|_{L^{\infty}}$$",,"['analysis', 'fourier-analysis', 'harmonic-analysis', 'singular-integrals']"
37,The existence of a measurable set with measure between rationals and the reals [duplicate],The existence of a measurable set with measure between rationals and the reals [duplicate],,"This question already has answers here : Construction of a Borel set with positive but not full measure in each interval (7 answers) Closed 8 years ago . Is there a measurable subset E ⊆ R such that whenever a < b are real numbers we have both $m(E ∩ (a, b)) > 0$ and $m((a, b) -E) > 0$ ?  This is an extra question on my real analysis class, but it is not graded, just for fun. I was thinking about the ""fat Cantor set""( https://en.wikipedia.org/wiki/Smith%E2%80%93Volterra%E2%80%93Cantor_set ), and play with it a little bit, but somehow I can still find ""gaps"" between real numbers. Can anyone give me some hint so I can carry on ?  Thanks !!!","This question already has answers here : Construction of a Borel set with positive but not full measure in each interval (7 answers) Closed 8 years ago . Is there a measurable subset E ⊆ R such that whenever a < b are real numbers we have both $m(E ∩ (a, b)) > 0$ and $m((a, b) -E) > 0$ ?  This is an extra question on my real analysis class, but it is not graded, just for fun. I was thinking about the ""fat Cantor set""( https://en.wikipedia.org/wiki/Smith%E2%80%93Volterra%E2%80%93Cantor_set ), and play with it a little bit, but somehow I can still find ""gaps"" between real numbers. Can anyone give me some hint so I can carry on ?  Thanks !!!",,"['real-analysis', 'analysis', 'measure-theory', 'real-numbers', 'cantor-set']"
38,How to understand $G_{k_0}\bigcap R_n\subset D_n$?,How to understand ?,G_{k_0}\bigcap R_n\subset D_n,"$G\subset R^N$ is a measurable set, $mG< +\infty$ ,and $\varphi_n(x)\rightarrow \varphi(x)$ in measure. $\sigma\in R,k_0\in Z^+$,$f(x,u)$ meets the Caratheodory condition (for almost all of  $x\in G$,$f(x,u)$ is continue about $u$,and for every $u\in R$ ,$f(x,u)$ is a measurable function about $x$ on $G$). $$ G_{k_0}=\{x\in G:\forall u\in R,\text{ if } |u-\varphi(x)|<\frac{1}{k_0}, \text{ then } |f(x,u)-f(x,\varphi(x))|<\sigma \} $$ $$ R_n=\{x\in G:|\varphi_n(x)-\varphi(x)|<\frac{1}{k_0}\} $$ $$ D_n=\{x\in G:|f(x,\varphi_n(x))-f(x,\varphi(x))|<\sigma \} $$ show that :  $G_{k_0}\cap R_n\subset D_n$","$G\subset R^N$ is a measurable set, $mG< +\infty$ ,and $\varphi_n(x)\rightarrow \varphi(x)$ in measure. $\sigma\in R,k_0\in Z^+$,$f(x,u)$ meets the Caratheodory condition (for almost all of  $x\in G$,$f(x,u)$ is continue about $u$,and for every $u\in R$ ,$f(x,u)$ is a measurable function about $x$ on $G$). $$ G_{k_0}=\{x\in G:\forall u\in R,\text{ if } |u-\varphi(x)|<\frac{1}{k_0}, \text{ then } |f(x,u)-f(x,\varphi(x))|<\sigma \} $$ $$ R_n=\{x\in G:|\varphi_n(x)-\varphi(x)|<\frac{1}{k_0}\} $$ $$ D_n=\{x\in G:|f(x,\varphi_n(x))-f(x,\varphi(x))|<\sigma \} $$ show that :  $G_{k_0}\cap R_n\subset D_n$",,"['analysis', 'measure-theory']"
39,Covariant and partial derivative commute?,Covariant and partial derivative commute?,,"I know that we have for a function $\Gamma: (-\varepsilon,\varepsilon)^2 \rightarrow M$ we have (at least I think I know that this is true) $$\nabla_{\frac{\partial \Gamma}{\partial s}} \frac{\partial \Gamma(s,t)}{\partial t} =  \nabla_{\frac{\partial \Gamma}{\partial t}} \frac{\partial \Gamma(s,t)}{\partial s}.$$ I am not looking for a proof of this one! Now I found a proof on page 2 of this reference (first line) click me where I was wondering what exactly happens between the second and third term. I mean it looks pretty much like this, but I was wondering at which points you have to evaluate the function and in what way they commute with each other. So there it says $$  \nabla_{\frac{\partial \Gamma}{\partial t}} \frac{\partial}{\partial s}|_{s=0} \Gamma = \nabla_{\frac{\partial \Gamma}{\partial s}} \frac{\partial}{\partial t}|_{s=0} \Gamma .$$ My problem is that the $t$ evaluation is completely missing here and I don't know whether I have to commute $t$ and $s$ evaluation, too. Thus, I don't know where the covariant derivative should be evaluated. So if anybody could make this step more precise, this would totally answer my question.","I know that we have for a function we have (at least I think I know that this is true) I am not looking for a proof of this one! Now I found a proof on page 2 of this reference (first line) click me where I was wondering what exactly happens between the second and third term. I mean it looks pretty much like this, but I was wondering at which points you have to evaluate the function and in what way they commute with each other. So there it says My problem is that the evaluation is completely missing here and I don't know whether I have to commute and evaluation, too. Thus, I don't know where the covariant derivative should be evaluated. So if anybody could make this step more precise, this would totally answer my question.","\Gamma: (-\varepsilon,\varepsilon)^2 \rightarrow M \nabla_{\frac{\partial \Gamma}{\partial s}} \frac{\partial \Gamma(s,t)}{\partial t} =  \nabla_{\frac{\partial \Gamma}{\partial t}} \frac{\partial \Gamma(s,t)}{\partial s}.   \nabla_{\frac{\partial \Gamma}{\partial t}} \frac{\partial}{\partial s}|_{s=0} \Gamma = \nabla_{\frac{\partial \Gamma}{\partial s}} \frac{\partial}{\partial t}|_{s=0} \Gamma . t t s","['real-analysis', 'analysis', 'differential-geometry', 'differential-topology', 'riemannian-geometry']"
40,$L^2$ convergence of this sequence,convergence of this sequence,L^2,"I am given the following sequence of functions $(f_m)_{m \in \mathbb{N}}$. They are defined by $$ f_m(x):=\left( \frac{e^{-ix}-1}{-ix} \right)^m \left( \sum_{l \in \mathbb{Z}} \frac{\left|e^{-ix}-1 \right|^{2m}}{|x+ 2 \pi l |^{2m}} \right)^{-\frac{1}{2}}$$ for $x \neq 2 \pi l$ with $l \in \mathbb{Z}$ and $f_m(2 \pi l)=1$. Now, I want to show that this sequence converges to the characteristic function $\chi_{[-\pi,\pi]}$ in the $L^2$ sense? So $\|f_m- \chi_{[-\pi,\pi]} \|_2 \rightarrow 0.$ The idea could be to use the dominated convergence theorem. For this we need to find a uniform upper bound and be sure that pointwise convergence is satisfied. Regarding the upper bound it may be useful to see that $$|f_m(x)|^2 = \left( \sum_{l \in \mathbb{Z}} \frac{\left|x \right|^{2m}}{|x+ 2 \pi l |^{2m}} \right)^{-1}.$$","I am given the following sequence of functions $(f_m)_{m \in \mathbb{N}}$. They are defined by $$ f_m(x):=\left( \frac{e^{-ix}-1}{-ix} \right)^m \left( \sum_{l \in \mathbb{Z}} \frac{\left|e^{-ix}-1 \right|^{2m}}{|x+ 2 \pi l |^{2m}} \right)^{-\frac{1}{2}}$$ for $x \neq 2 \pi l$ with $l \in \mathbb{Z}$ and $f_m(2 \pi l)=1$. Now, I want to show that this sequence converges to the characteristic function $\chi_{[-\pi,\pi]}$ in the $L^2$ sense? So $\|f_m- \chi_{[-\pi,\pi]} \|_2 \rightarrow 0.$ The idea could be to use the dominated convergence theorem. For this we need to find a uniform upper bound and be sure that pointwise convergence is satisfied. Regarding the upper bound it may be useful to see that $$|f_m(x)|^2 = \left( \sum_{l \in \mathbb{Z}} \frac{\left|x \right|^{2m}}{|x+ 2 \pi l |^{2m}} \right)^{-1}.$$",,"['calculus', 'real-analysis', 'analysis', 'lebesgue-integral']"
41,Interpreting the statement,Interpreting the statement,,"Just started quantifiers, and I'm having some trouble with interpreting this. Here's what I understand: For every Epsilon $> 0$, there exists a Delta $> 0$ for all $x$ in $\mathbb{R}$. The antecedent of the implication is false, because there is no Delta that is always greater than $x$. The precedent of the implication is also false, because not every Epsilon greater than $0$ is also greater than $x^2 -1$ for any $x$. So the statement is false. Is my logic sound? I'm not sure if I'm reading this right.","Just started quantifiers, and I'm having some trouble with interpreting this. Here's what I understand: For every Epsilon $> 0$, there exists a Delta $> 0$ for all $x$ in $\mathbb{R}$. The antecedent of the implication is false, because there is no Delta that is always greater than $x$. The precedent of the implication is also false, because not every Epsilon greater than $0$ is also greater than $x^2 -1$ for any $x$. So the statement is false. Is my logic sound? I'm not sure if I'm reading this right.",,"['real-analysis', 'analysis', 'logic', 'proof-verification', 'quantifiers']"
42,"Proving the function $ f $ is continuous on $ [0,1] $",Proving the function  is continuous on," f   [0,1] ","I'm trying to prove that the following function $ f $ is continuous on $ [0,1] $. The function $ f:[0,1]\rightarrow [0,1] $ is defined as follows. Let $ x\in [0,1] $. Then $ x= \sum\limits_{n = 1}^\infty  \frac{a_n}{3^n} $ ; where $ a_{n}\in \{0,1,2\}$ for each $ n\in \mathbb{N} $. If $ x\in C $ (the Cantor set) then $ x $ can be written as of the form $ \sum\limits_{n = 1}^\infty  \frac{a_n}{3^n} $ ; where $a_{n}\in \{0,2\}$  for each $ n\in \mathbb{N} $ and then we define $ f(x)=\sum\limits_{n = 1}^\infty  \frac{a_n}{2^{n+1}} $;(in this case we use the ternary expansion of $x$ which does not contain 1s to define the function $f$ ). If $ x\notin C $ then there exists $ n_{0}\in \mathbb{N} $ such that $ a_{n_{0}}=1 $. Put $ N=\min \{n_{0}\in \mathbb{N}:a_{n_{0}}=1\} $. If $ N=1 $ then define $ f(x)=\dfrac{1}{2} $ and otherwise we define  $ f(x)=f(\sum\limits_{n = 1}^\infty  \frac{a_n}{3^n})=\sum\limits_{n = 1}^{N-1}  \frac{a_n}{2^{n+1}}+\dfrac{1}{2^{N}} $. My attempt: Let $ x\in [0,1] $ be arbitrary. Then either $ x\in C $ or $ x\notin C $. If $ x\notin C $ then $x$ belongs to some open middle third (say $I$) which has removed in the construction of the Cantor set C. Since $f$ is constant on the open interval $I$, $f$ is obviously continuous at $x$. But I'm stuck on the Proving that $f$ is continuous at $x$ if $ x\in C $. Could anyone give me some help ? Any hints/ideas are much appreciated. Thanks in advance for any replies.","I'm trying to prove that the following function $ f $ is continuous on $ [0,1] $. The function $ f:[0,1]\rightarrow [0,1] $ is defined as follows. Let $ x\in [0,1] $. Then $ x= \sum\limits_{n = 1}^\infty  \frac{a_n}{3^n} $ ; where $ a_{n}\in \{0,1,2\}$ for each $ n\in \mathbb{N} $. If $ x\in C $ (the Cantor set) then $ x $ can be written as of the form $ \sum\limits_{n = 1}^\infty  \frac{a_n}{3^n} $ ; where $a_{n}\in \{0,2\}$  for each $ n\in \mathbb{N} $ and then we define $ f(x)=\sum\limits_{n = 1}^\infty  \frac{a_n}{2^{n+1}} $;(in this case we use the ternary expansion of $x$ which does not contain 1s to define the function $f$ ). If $ x\notin C $ then there exists $ n_{0}\in \mathbb{N} $ such that $ a_{n_{0}}=1 $. Put $ N=\min \{n_{0}\in \mathbb{N}:a_{n_{0}}=1\} $. If $ N=1 $ then define $ f(x)=\dfrac{1}{2} $ and otherwise we define  $ f(x)=f(\sum\limits_{n = 1}^\infty  \frac{a_n}{3^n})=\sum\limits_{n = 1}^{N-1}  \frac{a_n}{2^{n+1}}+\dfrac{1}{2^{N}} $. My attempt: Let $ x\in [0,1] $ be arbitrary. Then either $ x\in C $ or $ x\notin C $. If $ x\notin C $ then $x$ belongs to some open middle third (say $I$) which has removed in the construction of the Cantor set C. Since $f$ is constant on the open interval $I$, $f$ is obviously continuous at $x$. But I'm stuck on the Proving that $f$ is continuous at $x$ if $ x\in C $. Could anyone give me some help ? Any hints/ideas are much appreciated. Thanks in advance for any replies.",,"['real-analysis', 'analysis', 'continuity', 'cantor-set']"
43,Function in Lipschitz space,Function in Lipschitz space,,"I'm looking for a function that is in $W^{1,1}(0,1)$ but only in the Lipschitz space $\mathrm{Lip} (\alpha, L_2(0,1))$ for $0<\alpha < 1$. $\mathrm{Lip}(\alpha, L_2(0,1))$ is defined as the set of all functions $f\in L_2(0,1)$ for which $|| f(\cdot + h) - f(\cdot)||_{L_2(0,1-h)} \le C h^{\alpha}$  for every $1 > h > 0$. I thought about $f(x) = x^{\alpha}$ which is in $L(\alpha, L_{\infty}(0,1))$ but not in higher spaces. But some numerical experiments show that this function is possibly in $\mathrm{Lip}(1,L_2)$ and not only in $\mathrm{Lip}(\alpha, L_2(0,1))$. Can someone help me?","I'm looking for a function that is in $W^{1,1}(0,1)$ but only in the Lipschitz space $\mathrm{Lip} (\alpha, L_2(0,1))$ for $0<\alpha < 1$. $\mathrm{Lip}(\alpha, L_2(0,1))$ is defined as the set of all functions $f\in L_2(0,1)$ for which $|| f(\cdot + h) - f(\cdot)||_{L_2(0,1-h)} \le C h^{\alpha}$  for every $1 > h > 0$. I thought about $f(x) = x^{\alpha}$ which is in $L(\alpha, L_{\infty}(0,1))$ but not in higher spaces. But some numerical experiments show that this function is possibly in $\mathrm{Lip}(1,L_2)$ and not only in $\mathrm{Lip}(\alpha, L_2(0,1))$. Can someone help me?",,"['analysis', 'functions', 'special-functions', 'examples-counterexamples', 'lipschitz-functions']"
44,How prove there exist $\xi$ such $3af'(a)=3f(a)+a^3f'''(\xi)$,How prove there exist  such,\xi 3af'(a)=3f(a)+a^3f'''(\xi),"let $f(x)$ have three  derivatives  on $[0,a]$ ,and such $f(0)=f''(0)=0$ , show that: there exsit $\xi\in[0,a]$ such $$3af'(a)=3f(a)+a^3f'''(\xi)$$ I think we can use Tarlor to solve it, $$f(0)=f(a)+f'(a)(-a)+f''(a)/2\cdot a^2+\dfrac{f'''(\eta)}{6}(-a)^3$$ $$f''(0)=f''(a)+f'''(\eta_{2})(-a)$$ But this maybe not usefull,and I have try some old methods also can't prove this problem,can you help me，Thank you","let have three  derivatives  on ,and such , show that: there exsit such I think we can use Tarlor to solve it, But this maybe not usefull,and I have try some old methods also can't prove this problem,can you help me，Thank you","f(x) [0,a] f(0)=f''(0)=0 \xi\in[0,a] 3af'(a)=3f(a)+a^3f'''(\xi) f(0)=f(a)+f'(a)(-a)+f''(a)/2\cdot a^2+\dfrac{f'''(\eta)}{6}(-a)^3 f''(0)=f''(a)+f'''(\eta_{2})(-a)",['analysis']
45,Please explain the Sequential Characterization of Limits more clearly.,Please explain the Sequential Characterization of Limits more clearly.,,"Sequential Characterization of Limits (SCL) Let $a \in \mathbb{R}$, let $I$ be an open interval which contains $a$, and let $f$ be a real function defined everywhere on $I$ except possibly at $a$. Then $$ L = \lim\limits_{x\to a} f(x) $$ exists if and only if $f(x_n) \to L$ as $n \to \infty$ for every sequence $x_n \in I \backslash \{a\}$ which converges to $a$ as $n \to \infty$. What I know: I know from learning calculus that a limit doesn't have to exist at a point, but haven't found a proof for that statement. The formal definition of Limit of a sequence (barely).  I understand the concept, but still have trouble with properly writing the proofs. I understand the first part of the statement of SCL; however, the last part confuses me. The domain of $f(x_n)$ is the sequence $x_n$, and $f(x_n) \to L$ means (?) that every point in the domain of $f(x)$ approaches L.  Is SCL only for functions that have a single point in it's range?","Sequential Characterization of Limits (SCL) Let $a \in \mathbb{R}$, let $I$ be an open interval which contains $a$, and let $f$ be a real function defined everywhere on $I$ except possibly at $a$. Then $$ L = \lim\limits_{x\to a} f(x) $$ exists if and only if $f(x_n) \to L$ as $n \to \infty$ for every sequence $x_n \in I \backslash \{a\}$ which converges to $a$ as $n \to \infty$. What I know: I know from learning calculus that a limit doesn't have to exist at a point, but haven't found a proof for that statement. The formal definition of Limit of a sequence (barely).  I understand the concept, but still have trouble with properly writing the proofs. I understand the first part of the statement of SCL; however, the last part confuses me. The domain of $f(x_n)$ is the sequence $x_n$, and $f(x_n) \to L$ means (?) that every point in the domain of $f(x)$ approaches L.  Is SCL only for functions that have a single point in it's range?",,"['real-analysis', 'analysis']"
46,Fubini's theorem applied to heaviside step functions,Fubini's theorem applied to heaviside step functions,,"First of all, I should probably mention that I am a physicist, not a mathematician so I sincerely apologize for any lack of rigour in my explanation of my problem. Recently, I have been trying to calculate (simple) 2d Fourier transforms of time-ordered Matsubara Green's functions, but I have been having some problems with it. I can't seem to determine whether or not the order of integration matters for a double integral like the following $$ \int_{0}^{\beta} \int_{0}^{\beta} e^{i \omega_1 \tau_1}e^{i \omega_2 \tau_2} \theta(\tau_1-\tau_2) e^{g (\tau_1-\tau_2)} d\tau_1 d\tau_2 \quad (1) $$ By my calculation (and double checking with mathematica), evaluating the above double integral iteratively as: $$ \int_{0}^{\beta} \left(\int_{0}^{\beta} e^{i \omega_1 \tau_1}e^{i \omega_2 \tau_2} \theta(\tau_1-\tau_2) e^{g (\tau_1-\tau_2)} d\tau_1\right) d\tau_2 \quad (2) $$ gives $$ \frac{e^{i \beta  \omega _1} \left(e^{\beta  g}-e^{i \beta  \omega _2}\right)}{\left(g+i \omega _1\right) \left(g-i \omega    _2\right)}+\frac{-1+e^{i \beta  \left(\omega _1+\omega _2\right)}}{\left(\omega _1+\omega _2\right) \left(\omega _1-i g\right)} \quad (3) $$ whereas evaluating iteratively instead as $$ \int_{0}^{\beta} \left(\int_{0}^{\beta} e^{i \omega_1 \tau_1}e^{i \omega_2 \tau_2} \theta(\tau_1-\tau_2) e^{g (\tau_1-\tau_2)} d\tau_2\right) d\tau_1 \quad (4) $$ gives $$ \frac{-1+e^{\beta  (g+i \omega_1)}}{(g+i \omega_1) (g-i \omega_2)}-\frac{-1+e^{i \beta  (\omega_1+\omega_2)}}{(\omega_2+i g) (\omega_1+\omega_2)} \quad (5) $$ Such a discrepancy between results does not seem to occur when the limits of each integral is $(-\infty,\infty)$ (such a problem has been addressed in previous posts such as this one ). Furthermore, I even tried the usual variable substitution approach where I defined the following new set of variables $$ \tau = \tau_1-\tau_2, \qquad \tilde{\tau} = \tau_2 \\ \tilde{\tau} \epsilon [0,\beta], \qquad \tau \epsilon [-\tilde{\tau}, \beta-\tilde{\tau}] $$ And this gives me the same result as (3). So I'm not sure whether I am making a mistake or that Fubini's theorem does not apply when dealing with Heaviside theta functions. But if the order of integration does matter, then which is the correct way of calculating these integrals? Surely this is crucial when calculating Matusbara Green's functions in frequency space. Any advice would be greatly appreciated!","First of all, I should probably mention that I am a physicist, not a mathematician so I sincerely apologize for any lack of rigour in my explanation of my problem. Recently, I have been trying to calculate (simple) 2d Fourier transforms of time-ordered Matsubara Green's functions, but I have been having some problems with it. I can't seem to determine whether or not the order of integration matters for a double integral like the following $$ \int_{0}^{\beta} \int_{0}^{\beta} e^{i \omega_1 \tau_1}e^{i \omega_2 \tau_2} \theta(\tau_1-\tau_2) e^{g (\tau_1-\tau_2)} d\tau_1 d\tau_2 \quad (1) $$ By my calculation (and double checking with mathematica), evaluating the above double integral iteratively as: $$ \int_{0}^{\beta} \left(\int_{0}^{\beta} e^{i \omega_1 \tau_1}e^{i \omega_2 \tau_2} \theta(\tau_1-\tau_2) e^{g (\tau_1-\tau_2)} d\tau_1\right) d\tau_2 \quad (2) $$ gives $$ \frac{e^{i \beta  \omega _1} \left(e^{\beta  g}-e^{i \beta  \omega _2}\right)}{\left(g+i \omega _1\right) \left(g-i \omega    _2\right)}+\frac{-1+e^{i \beta  \left(\omega _1+\omega _2\right)}}{\left(\omega _1+\omega _2\right) \left(\omega _1-i g\right)} \quad (3) $$ whereas evaluating iteratively instead as $$ \int_{0}^{\beta} \left(\int_{0}^{\beta} e^{i \omega_1 \tau_1}e^{i \omega_2 \tau_2} \theta(\tau_1-\tau_2) e^{g (\tau_1-\tau_2)} d\tau_2\right) d\tau_1 \quad (4) $$ gives $$ \frac{-1+e^{\beta  (g+i \omega_1)}}{(g+i \omega_1) (g-i \omega_2)}-\frac{-1+e^{i \beta  (\omega_1+\omega_2)}}{(\omega_2+i g) (\omega_1+\omega_2)} \quad (5) $$ Such a discrepancy between results does not seem to occur when the limits of each integral is $(-\infty,\infty)$ (such a problem has been addressed in previous posts such as this one ). Furthermore, I even tried the usual variable substitution approach where I defined the following new set of variables $$ \tau = \tau_1-\tau_2, \qquad \tilde{\tau} = \tau_2 \\ \tilde{\tau} \epsilon [0,\beta], \qquad \tau \epsilon [-\tilde{\tau}, \beta-\tilde{\tau}] $$ And this gives me the same result as (3). So I'm not sure whether I am making a mistake or that Fubini's theorem does not apply when dealing with Heaviside theta functions. But if the order of integration does matter, then which is the correct way of calculating these integrals? Surely this is crucial when calculating Matusbara Green's functions in frequency space. Any advice would be greatly appreciated!",,"['calculus', 'real-analysis', 'analysis']"
47,Why is the derivative of the translates of a measure measurable?,Why is the derivative of the translates of a measure measurable?,,"Let G be a topological group and X a measure space. Let $G \times X \rightarrow X$ be a measurable group action, $\mu$ a $\sigma$-finite measure on $X$, and $g\mu$ (for any $g \in G$) the measure $g\mu(A)= \mu(g^{-1}A)$. Assume that $g\mu$ is always equivalent to $\mu$, that is, $\mu(A)=0$ iff $g\mu(A)=0$ for every measurable $A \subset X$. Define $$\phi : G \times X \rightarrow \mathbb{R}, (g,x) \mapsto \frac{dg\mu}{d\mu}(x)$$ where $\frac{dg\mu}{d\mu}$ is the Radon-Nikodym derivative. Why is $\phi$ measurable?","Let G be a topological group and X a measure space. Let $G \times X \rightarrow X$ be a measurable group action, $\mu$ a $\sigma$-finite measure on $X$, and $g\mu$ (for any $g \in G$) the measure $g\mu(A)= \mu(g^{-1}A)$. Assume that $g\mu$ is always equivalent to $\mu$, that is, $\mu(A)=0$ iff $g\mu(A)=0$ for every measurable $A \subset X$. Define $$\phi : G \times X \rightarrow \mathbb{R}, (g,x) \mapsto \frac{dg\mu}{d\mu}(x)$$ where $\frac{dg\mu}{d\mu}$ is the Radon-Nikodym derivative. Why is $\phi$ measurable?",,"['analysis', 'measure-theory', 'harmonic-analysis']"
48,Prove that $\lim\limits_{t \rightarrow 0} \int\limits_{0}^{\infty} e^{-tx} f(x) dx = 1$ for $t>0$.,Prove that  for .,\lim\limits_{t \rightarrow 0} \int\limits_{0}^{\infty} e^{-tx} f(x) dx = 1 t>0,"Suppose $f \in \mathcal{R}$ on $[0,A]$ for all $A < \infty$, and $f(x) \rightarrow 1$ as $x \rightarrow + \infty$. Prove that $\lim\limits_{t \rightarrow 0} \int\limits_{0}^{\infty} e^{-tx} f(x) dx = 1 \qquad (t>0)$. Proof: Let $\epsilon > 0$. Choose $a>0$ such that $|f(x) - 1| < \frac{1}{2} \epsilon$ for $x \ge a$. Since $f$ is Riemann integrable on $[0,a]$, it is bounded there. So we choose $M$ such that $|f(x)| \le M$ for all $x \in [0,a]$. Now choose $\delta$ so small that $\delta(M+1) < \frac{1}{2} \epsilon$. Then $0 < t < \delta$ implies $t \int\limits_{0}^{\infty} e^{-tx} \cdot 1 dx = 1$. and then $t \int\limits_{0}^{\infty} e^{-tx} |f(x) - 1| dx \le \delta \int\limits_{0}^{a} e^{-tx} (M+1) dx + t \int\limits_{a}^{\infty} e^{-tx} \frac{1}{2} \epsilon dx \le \delta(M+1) + \frac{1}{2} \epsilon e^{-at} < \epsilon$ How does this look? Do I need to add in any extra details? Thanks in advance!","Suppose $f \in \mathcal{R}$ on $[0,A]$ for all $A < \infty$, and $f(x) \rightarrow 1$ as $x \rightarrow + \infty$. Prove that $\lim\limits_{t \rightarrow 0} \int\limits_{0}^{\infty} e^{-tx} f(x) dx = 1 \qquad (t>0)$. Proof: Let $\epsilon > 0$. Choose $a>0$ such that $|f(x) - 1| < \frac{1}{2} \epsilon$ for $x \ge a$. Since $f$ is Riemann integrable on $[0,a]$, it is bounded there. So we choose $M$ such that $|f(x)| \le M$ for all $x \in [0,a]$. Now choose $\delta$ so small that $\delta(M+1) < \frac{1}{2} \epsilon$. Then $0 < t < \delta$ implies $t \int\limits_{0}^{\infty} e^{-tx} \cdot 1 dx = 1$. and then $t \int\limits_{0}^{\infty} e^{-tx} |f(x) - 1| dx \le \delta \int\limits_{0}^{a} e^{-tx} (M+1) dx + t \int\limits_{a}^{\infty} e^{-tx} \frac{1}{2} \epsilon dx \le \delta(M+1) + \frac{1}{2} \epsilon e^{-at} < \epsilon$ How does this look? Do I need to add in any extra details? Thanks in advance!",,"['real-analysis', 'analysis', 'proof-verification']"
49,Showing the exponential and logarithmic functions are unique in satisfying their properties,Showing the exponential and logarithmic functions are unique in satisfying their properties,,"The question asks to prove that there exists a unique function defined on $\Bbb R$ and satisfying the following conditions: 1) $f(1) = a$ $(a>0, a \neq 0)$ 2) $f(x_1) \cdot f(x_2) = f(x_1 + x_2)$ 3) $f(x) \rightarrow f(x_0)$ as $x \rightarrow x_0 $ Since the text goes into detail constructing the exponential function and proving these properties, I assume I only have to show uniqueness. i.e. showing that if functions $f$ and $g$ satisfy these properties, then $f=g$. $f(1) = g(1)$ by property (1). Then for $n\in \Bbb N$, $(f-g)(n)$ $=$$f(n) - g(n)$ $= f(1)^n -g(1)^n $ by property (2) and induction. But $f(1) = g(1) = a > 0$  , which implies $f(1)^n -g(1)^n = 0$ and hence $f(n) =g(n)$ We have $f(1)-g(1)$ $=$  $f(\frac1n \cdot n) -g(\frac1n \cdot n)$ $=$$f(\frac1n)^n-g(\frac1n)^n   = 0$. Now $f(x) = f(\frac x2)^2 \ge 0$, so $f(\frac1n) =g(\frac1n)$. $f(\frac mn ) -g (\frac mn ) $  $=$ $f(\frac1n)^m - g(\frac1n)^m$ which implies that $f(\frac mn) - g(\frac mn)$ $=0$. For $x \in \Bbb R$, $\lim \limits_{\Bbb Q \in r \to x}$ $(f-g)(r) =0$: for any $\epsilon > 0$, we can choose a $\delta > 0$ such that $|r - x| < \delta $ and $|f(r) - g(r)| = 0 < \epsilon $. By property 3 we also have $\lim \limits_{\Bbb Q \in r \to x}(f-g)(r) =\lim \limits_{\Bbb Q \in r \to x} f(r) - \lim \limits_{\Bbb Q \in r \to x} g(r) = f(x) - g(x)$. Thus $f(x) = g(x)$ $\forall x \in \Bbb R$. Is this approach correct? A similar question is asked for the logarithmic function.","The question asks to prove that there exists a unique function defined on $\Bbb R$ and satisfying the following conditions: 1) $f(1) = a$ $(a>0, a \neq 0)$ 2) $f(x_1) \cdot f(x_2) = f(x_1 + x_2)$ 3) $f(x) \rightarrow f(x_0)$ as $x \rightarrow x_0 $ Since the text goes into detail constructing the exponential function and proving these properties, I assume I only have to show uniqueness. i.e. showing that if functions $f$ and $g$ satisfy these properties, then $f=g$. $f(1) = g(1)$ by property (1). Then for $n\in \Bbb N$, $(f-g)(n)$ $=$$f(n) - g(n)$ $= f(1)^n -g(1)^n $ by property (2) and induction. But $f(1) = g(1) = a > 0$  , which implies $f(1)^n -g(1)^n = 0$ and hence $f(n) =g(n)$ We have $f(1)-g(1)$ $=$  $f(\frac1n \cdot n) -g(\frac1n \cdot n)$ $=$$f(\frac1n)^n-g(\frac1n)^n   = 0$. Now $f(x) = f(\frac x2)^2 \ge 0$, so $f(\frac1n) =g(\frac1n)$. $f(\frac mn ) -g (\frac mn ) $  $=$ $f(\frac1n)^m - g(\frac1n)^m$ which implies that $f(\frac mn) - g(\frac mn)$ $=0$. For $x \in \Bbb R$, $\lim \limits_{\Bbb Q \in r \to x}$ $(f-g)(r) =0$: for any $\epsilon > 0$, we can choose a $\delta > 0$ such that $|r - x| < \delta $ and $|f(r) - g(r)| = 0 < \epsilon $. By property 3 we also have $\lim \limits_{\Bbb Q \in r \to x}(f-g)(r) =\lim \limits_{\Bbb Q \in r \to x} f(r) - \lim \limits_{\Bbb Q \in r \to x} g(r) = f(x) - g(x)$. Thus $f(x) = g(x)$ $\forall x \in \Bbb R$. Is this approach correct? A similar question is asked for the logarithmic function.",,"['analysis', 'exponential-function']"
50,$f$ continuous nowhere implies $f$ not Riemann integrable,continuous nowhere implies  not Riemann integrable,f f,"I am trying to prove the statement in the title, without using the theorem which says that the set of discontinuities of a Riemann integrable function must have measure $0$.  I am aware that similar questions have been asked before, but I am asking for a proof-check rather than an answer. Attempt: Consider $f:\;[a,b]\to\mathbb{R}$ where $f$ is continuous nowhere on $[a,b]$. We want to show $\mathcal{U}_{f,\mathcal{D}}-\mathcal{L}_{f,\mathcal{D}}\geq (b-a)\eta$ for some $\eta>0$, where $\mathcal{U}_{f,\mathcal{D}},\mathcal{L}_{f,\mathcal{D}}$ are the upper & lower Darboux sums for any partition $\mathcal{D}=\{x_1,\cdots,x_n\}:\;a=x_1<x_2<\cdots<x_n=b$ of $[a,b]$. To do this, suppose $f$ is bounded $[$if not we are done$]$, it is sufficient $(?)$ to show that: $$\limsup_{x\to w} f>\liminf_{x\to w} f,\quad \forall w\in (a,b)$$ This is immediate since $\limsup_{x\to w}f=\liminf_{x\to w}f\Rightarrow f$ has at worst a removable discontinuity. $$\eta=\inf\Big\{\big(\limsup_{x\to w} f-\liminf_{x\to w}f\big):\;w\in (a,b)\Big\}$$ $$\Rightarrow\quad\mathcal{U}_{f,\mathcal{D}}-\mathcal{L}_{f,\mathcal{D}}\geq \sum_i (x_{i+1}-x_i)\eta=(b-a)\eta$$ First, is this anywhere near a correct approach? Second, there is an issue which I cannot seem to correct: the argument allows $\eta =0$. Can something be fixed to rule this out? Thank you for any help.","I am trying to prove the statement in the title, without using the theorem which says that the set of discontinuities of a Riemann integrable function must have measure $0$.  I am aware that similar questions have been asked before, but I am asking for a proof-check rather than an answer. Attempt: Consider $f:\;[a,b]\to\mathbb{R}$ where $f$ is continuous nowhere on $[a,b]$. We want to show $\mathcal{U}_{f,\mathcal{D}}-\mathcal{L}_{f,\mathcal{D}}\geq (b-a)\eta$ for some $\eta>0$, where $\mathcal{U}_{f,\mathcal{D}},\mathcal{L}_{f,\mathcal{D}}$ are the upper & lower Darboux sums for any partition $\mathcal{D}=\{x_1,\cdots,x_n\}:\;a=x_1<x_2<\cdots<x_n=b$ of $[a,b]$. To do this, suppose $f$ is bounded $[$if not we are done$]$, it is sufficient $(?)$ to show that: $$\limsup_{x\to w} f>\liminf_{x\to w} f,\quad \forall w\in (a,b)$$ This is immediate since $\limsup_{x\to w}f=\liminf_{x\to w}f\Rightarrow f$ has at worst a removable discontinuity. $$\eta=\inf\Big\{\big(\limsup_{x\to w} f-\liminf_{x\to w}f\big):\;w\in (a,b)\Big\}$$ $$\Rightarrow\quad\mathcal{U}_{f,\mathcal{D}}-\mathcal{L}_{f,\mathcal{D}}\geq \sum_i (x_{i+1}-x_i)\eta=(b-a)\eta$$ First, is this anywhere near a correct approach? Second, there is an issue which I cannot seem to correct: the argument allows $\eta =0$. Can something be fixed to rule this out? Thank you for any help.",,"['real-analysis', 'analysis', 'proof-verification']"
51,"Homework: Second derivative of $\langle Ax, x \rangle$",Homework: Second derivative of,"\langle Ax, x \rangle","So let $A \in M_{n}$ and define $f: \mathbb{R}^n \to \mathbb{R}$ by $f(x) = \langle Ax, x \rangle $. Find f' and f''. After some work, I found the first derivative to be $f'(x)(v) = \langle Ax, v \rangle + \langle Av, x \rangle = \langle (A^T + A)x, v \rangle$. That is not what I'm having trouble with. I don't think it is necessary to compute the second derivative the ""long way"" through a complicated limit. I believe that since $f'$ is itself a linear function, then we should have $f''(x) = f'$. That $f''(x)(v,w) = f'(v,w) = \langle (A^T + A) v, w \rangle$. However I don't know how to justify this. I feel that there are some basic mechanics that I am missing. I am more interested in the mechanics than in a direct answer.","So let $A \in M_{n}$ and define $f: \mathbb{R}^n \to \mathbb{R}$ by $f(x) = \langle Ax, x \rangle $. Find f' and f''. After some work, I found the first derivative to be $f'(x)(v) = \langle Ax, v \rangle + \langle Av, x \rangle = \langle (A^T + A)x, v \rangle$. That is not what I'm having trouble with. I don't think it is necessary to compute the second derivative the ""long way"" through a complicated limit. I believe that since $f'$ is itself a linear function, then we should have $f''(x) = f'$. That $f''(x)(v,w) = f'(v,w) = \langle (A^T + A) v, w \rangle$. However I don't know how to justify this. I feel that there are some basic mechanics that I am missing. I am more interested in the mechanics than in a direct answer.",,"['analysis', 'multivariable-calculus', 'inner-products']"
52,Can the rational numbers be specified as an ordered field with <order property>?,Can the rational numbers be specified as an ordered field with <order property>?,,"In other words, (the opposite of my question is) does there exist an ordered field which is isomorphic as (as an ordered SET) to $\mathbb{Q}$? If not, does there exist an order property which specifies $\mathbb{Q}$ among ordered fields? For instance, the integers are the unique ordered ring whose positive elements are well-ordered. (You can replace this with 'Any set bounded below is well-ordered', to remove any reference to the ring structure). The reals are the unique ordered field with the least upper bound property. My first thought for a counterexample would be finite real algebraic extensions of $\mathbb{Q}$ which inherit an ordering as subsets of $\mathbb{R}$. These are at least countable, but I can't think of how I would construct an isomorphism.","In other words, (the opposite of my question is) does there exist an ordered field which is isomorphic as (as an ordered SET) to $\mathbb{Q}$? If not, does there exist an order property which specifies $\mathbb{Q}$ among ordered fields? For instance, the integers are the unique ordered ring whose positive elements are well-ordered. (You can replace this with 'Any set bounded below is well-ordered', to remove any reference to the ring structure). The reals are the unique ordered field with the least upper bound property. My first thought for a counterexample would be finite real algebraic extensions of $\mathbb{Q}$ which inherit an ordering as subsets of $\mathbb{R}$. These are at least countable, but I can't think of how I would construct an isomorphism.",,"['real-analysis', 'analysis', 'rational-numbers']"
53,How can I show it?,How can I show it?,,"Let $f$ be an absolute continuous function in $(0, 1)$ and satisfies  $$ |f(x+h)+f(x-h)-2f(x)|\leq \text{const}\frac{|h|}{(\log\frac{1}{|h|})^{\gamma}} , $$  where $\gamma \in (0, 1)$ and $|h|$- is sufficiently small.  Q: Is following  $$ \int_{|h|}^{1-|h|}|f'(x+|h|)-f'(x)|dx\leq \text{const}\frac{1}{(\log\frac{1}{|h|})^{\gamma}}   $$   inequality true? If it is how can I show?","Let $f$ be an absolute continuous function in $(0, 1)$ and satisfies  $$ |f(x+h)+f(x-h)-2f(x)|\leq \text{const}\frac{|h|}{(\log\frac{1}{|h|})^{\gamma}} , $$  where $\gamma \in (0, 1)$ and $|h|$- is sufficiently small.  Q: Is following  $$ \int_{|h|}^{1-|h|}|f'(x+|h|)-f'(x)|dx\leq \text{const}\frac{1}{(\log\frac{1}{|h|})^{\gamma}}   $$   inequality true? If it is how can I show?",,"['real-analysis', 'analysis']"
54,Show that $\mathfrak{Z}$ is a semi ring,Show that  is a semi ring,\mathfrak{Z},"Consider measurable spaces $(\Omega_t,\mathcal{A}_t), t\in T$ ($T$ is any index set). With $\mathcal{E}(T)$ we the set of all finite, not-empty subsets of $T$. Show that      $$ \mathfrak{Z}:=\mathfrak{Z}(\mathcal{A}_t: t\in T):=\bigcup_{S\in\mathcal{E}(T)}\underbrace{\left\{\times_{t\in T}A_t: A_t\in\mathcal{A}_t, t\in S; A_t=\Omega_t, t\in T\setminus S\right\}}_{=:\mathfrak{K}_S} $$     is a semi ring. The first thing I have to show is that $\emptyset\in\mathfrak{Z}$. To show this just take any $S\in\mathcal{E}(T)$ and choose $A_t=\emptyset$ for a $t\in S$. The second thing I have to show is, that for $x,y\in\mathfrak{Z}$ it is $x\cap y\in\mathfrak{Z}$. I think, that's easy, too: Assume that $x,y\in\mathfrak{Z}$. Then there is a $S_1\in\mathcal{E}(T)$ so that $x\in \mathfrak{K}_{S_1}$ and a $S_2\in\mathfrak{E}(T)$ so that $y\in \mathfrak{K}_{S_2}$. Then $S_1\cup S_2\in\mathcal{E}(T)$ and $x\cap y\in\mathfrak{K}_{S_1\cup S_2}\in\mathfrak{Z}$. The last thing to show is the most difficult one for me. It is to show that for $x,y\in\mathfrak{Z}, x\subset y$ there are finite many disjoint sets $c_1,\ldots c_n$ so that $$ y\setminus x=\biguplus_{k=1}^{n} c_k. $$ Do not really know how to show that. My first idea is: If $x,y\in\mathfrak{Z},x\subset y$, then there is a $S\in\mathcal{E}(T): x,y\in\mathfrak{K}_S$. Let $y$ be $y=\times_{t\in T}A_t$ with $A_t\in\mathcal{A}_t$ for $t\in S$ and $A_t=\Omega_t$ for $t\in T\setminus S$ and $x$ be $x=\times_{t\in T}B_t$ with $B_t\in\mathcal{A}_t$ if $t\in S$ and $B_t=\Omega_t$ if $t\in T\setminus S$. Then to my understanding it is $$ y\setminus x=\times_{t\in T}C_t, C_t=B_t\setminus A_t, t\in S; C_t=\Omega_t, t\in T\setminus S. $$ But now I do not know how to write this as a disjoint union of sets $c_1,\ldots c_n$. Would be great to here if my recent results are allright and then to get some help to end the third necessary thing. With greetings","Consider measurable spaces $(\Omega_t,\mathcal{A}_t), t\in T$ ($T$ is any index set). With $\mathcal{E}(T)$ we the set of all finite, not-empty subsets of $T$. Show that      $$ \mathfrak{Z}:=\mathfrak{Z}(\mathcal{A}_t: t\in T):=\bigcup_{S\in\mathcal{E}(T)}\underbrace{\left\{\times_{t\in T}A_t: A_t\in\mathcal{A}_t, t\in S; A_t=\Omega_t, t\in T\setminus S\right\}}_{=:\mathfrak{K}_S} $$     is a semi ring. The first thing I have to show is that $\emptyset\in\mathfrak{Z}$. To show this just take any $S\in\mathcal{E}(T)$ and choose $A_t=\emptyset$ for a $t\in S$. The second thing I have to show is, that for $x,y\in\mathfrak{Z}$ it is $x\cap y\in\mathfrak{Z}$. I think, that's easy, too: Assume that $x,y\in\mathfrak{Z}$. Then there is a $S_1\in\mathcal{E}(T)$ so that $x\in \mathfrak{K}_{S_1}$ and a $S_2\in\mathfrak{E}(T)$ so that $y\in \mathfrak{K}_{S_2}$. Then $S_1\cup S_2\in\mathcal{E}(T)$ and $x\cap y\in\mathfrak{K}_{S_1\cup S_2}\in\mathfrak{Z}$. The last thing to show is the most difficult one for me. It is to show that for $x,y\in\mathfrak{Z}, x\subset y$ there are finite many disjoint sets $c_1,\ldots c_n$ so that $$ y\setminus x=\biguplus_{k=1}^{n} c_k. $$ Do not really know how to show that. My first idea is: If $x,y\in\mathfrak{Z},x\subset y$, then there is a $S\in\mathcal{E}(T): x,y\in\mathfrak{K}_S$. Let $y$ be $y=\times_{t\in T}A_t$ with $A_t\in\mathcal{A}_t$ for $t\in S$ and $A_t=\Omega_t$ for $t\in T\setminus S$ and $x$ be $x=\times_{t\in T}B_t$ with $B_t\in\mathcal{A}_t$ if $t\in S$ and $B_t=\Omega_t$ if $t\in T\setminus S$. Then to my understanding it is $$ y\setminus x=\times_{t\in T}C_t, C_t=B_t\setminus A_t, t\in S; C_t=\Omega_t, t\in T\setminus S. $$ But now I do not know how to write this as a disjoint union of sets $c_1,\ldots c_n$. Would be great to here if my recent results are allright and then to get some help to end the third necessary thing. With greetings",,"['analysis', 'measure-theory']"
55,sum of polynoms of given property,sum of polynoms of given property,,"I have $P(x)$ a polynomial with degree $n$ ,$P(x) \ge 0$ for all $x \in$ real. I have to prove that: $f(x)=P(x)+P'(x)+P""(x)+......+P^{n}(x) \ge 0$ for all $x$. I tried different methods to solve it but I got stuck.Any suggestion or advice is welcomed.","I have $P(x)$ a polynomial with degree $n$ ,$P(x) \ge 0$ for all $x \in$ real. I have to prove that: $f(x)=P(x)+P'(x)+P""(x)+......+P^{n}(x) \ge 0$ for all $x$. I tried different methods to solve it but I got stuck.Any suggestion or advice is welcomed.",,"['polynomials', 'derivatives']"
56,On the existence of $\sqrt{2}$ (guided),On the existence of  (guided),\sqrt{2},"Introduction : This is a homework assignment of mine, first I want to mention that I am aware of that there are many proofs all over the internet (including this site) about the existence of $\sqrt{2}$. However, in my assignment I am somewhat 'guided' through it with some vague steps and I would enjoy to understand them, or at least have an idea about what they are trying to tell me. Problem : Show the existence of $\sqrt{2}$, which means that: $\exists b \in \mathbb{R}, b \geq 0, b^2=2$ by using the following: (a) Define $Y=\lbrace a \in \mathbb{R} : a \geq 0 , a^2 \geq 2 \rbrace$ (b) Show that $b:= \inf Y$ exists (c) Show that $b \geq 0$ and $ b^2=2$ hint : Consider $a=\frac{1}{2}\left(1-\frac{2}{b^2}\right), \ a'=\frac{1}{2}\left(1-\frac{b^2}{2}\right)$ and the values $b(1-a)$ and $\frac{b}{1-a'}$ My approach : The given set $Y= \lbrace a \in \mathbb{R}: a \geq 0, a^2 \geq 2 \rbrace$ clearly is nonempty, for instance $5 \in Y$, thus $Y \neq \emptyset$. For (b) I simply said that i.e. $1$ is a lower bound for $Y$, hence $Y$ is closed from below and there exists a greatest lower bound $b:= \inf Y$. From all the resources I have read I have an idea how to proceed, I must assume that $b^2 > 2$ and lead this to a contradiction and do the same with $b^2 <2$. If both $b^2 >2$ and $b^2 < 2$ are false then $b^2=2$. In the papers I have read they do it as follows (usually with the Supremum). They subtract a positive number from the supremum such that no member of the Set is larger than it, this would lead to a contradiction (since the Supremum is the least upper bound) In my case I would have to increase the Infimum by a positive number and show that no member of the set $Y$ is smaller than it. My approach (cont.) with the given hints : I considered the first case where $b^2>2$ then I came up with the following inequalitites: \begin{align}b^2>2 \implies 1> \frac{2}{b^2} \implies 1>1-\frac{2}{b^2}>0 \\\text{s.t. }\frac{1}{2}> \frac{1}{2}\left(1-\frac{2}{b^2}\right)>0  \end{align} So I could substitute $a=\frac{1}{2}\left(1-\frac{2}{b^2}\right)$ and say $a \in \left]0, \frac{1}{2}\right[$ Unfortunately I am stuck here, it looks to me like $a$ is another lower bound of $Y$ but it doesn't seem to me to be strictly greater than the Infimum of $Y$. I don't see how I can bring this information up to be a contradiction. I would appreciate some hints/comments about how I could continue from here, and whether or not the given values $b(1-a)$ and $\frac{b}{1-a'}$ can be helpful. I apologize that this problem seems so vague, but I'd much rather continue with the given informations than copying a already existent proof from the internet.","Introduction : This is a homework assignment of mine, first I want to mention that I am aware of that there are many proofs all over the internet (including this site) about the existence of $\sqrt{2}$. However, in my assignment I am somewhat 'guided' through it with some vague steps and I would enjoy to understand them, or at least have an idea about what they are trying to tell me. Problem : Show the existence of $\sqrt{2}$, which means that: $\exists b \in \mathbb{R}, b \geq 0, b^2=2$ by using the following: (a) Define $Y=\lbrace a \in \mathbb{R} : a \geq 0 , a^2 \geq 2 \rbrace$ (b) Show that $b:= \inf Y$ exists (c) Show that $b \geq 0$ and $ b^2=2$ hint : Consider $a=\frac{1}{2}\left(1-\frac{2}{b^2}\right), \ a'=\frac{1}{2}\left(1-\frac{b^2}{2}\right)$ and the values $b(1-a)$ and $\frac{b}{1-a'}$ My approach : The given set $Y= \lbrace a \in \mathbb{R}: a \geq 0, a^2 \geq 2 \rbrace$ clearly is nonempty, for instance $5 \in Y$, thus $Y \neq \emptyset$. For (b) I simply said that i.e. $1$ is a lower bound for $Y$, hence $Y$ is closed from below and there exists a greatest lower bound $b:= \inf Y$. From all the resources I have read I have an idea how to proceed, I must assume that $b^2 > 2$ and lead this to a contradiction and do the same with $b^2 <2$. If both $b^2 >2$ and $b^2 < 2$ are false then $b^2=2$. In the papers I have read they do it as follows (usually with the Supremum). They subtract a positive number from the supremum such that no member of the Set is larger than it, this would lead to a contradiction (since the Supremum is the least upper bound) In my case I would have to increase the Infimum by a positive number and show that no member of the set $Y$ is smaller than it. My approach (cont.) with the given hints : I considered the first case where $b^2>2$ then I came up with the following inequalitites: \begin{align}b^2>2 \implies 1> \frac{2}{b^2} \implies 1>1-\frac{2}{b^2}>0 \\\text{s.t. }\frac{1}{2}> \frac{1}{2}\left(1-\frac{2}{b^2}\right)>0  \end{align} So I could substitute $a=\frac{1}{2}\left(1-\frac{2}{b^2}\right)$ and say $a \in \left]0, \frac{1}{2}\right[$ Unfortunately I am stuck here, it looks to me like $a$ is another lower bound of $Y$ but it doesn't seem to me to be strictly greater than the Infimum of $Y$. I don't see how I can bring this information up to be a contradiction. I would appreciate some hints/comments about how I could continue from here, and whether or not the given values $b(1-a)$ and $\frac{b}{1-a'}$ can be helpful. I apologize that this problem seems so vague, but I'd much rather continue with the given informations than copying a already existent proof from the internet.",,['analysis']
57,"How prove this Mathematical Analysis by Zorich, from the chapter on continuous functions.","How prove this Mathematical Analysis by Zorich, from the chapter on continuous functions.",,"Let $P_n$ be a polynomial of degree $n$. For a function $f:[a,b]\to\mathbb{R}$, Let $\Delta(P_n) = \sup_{x\in[a,b]} |f(x)-P_n(x)|$. and $E_n(f) = \inf_{P_n} \Delta(P_n)$. A polynomial $P_n$ is the best approximation of degree $n$ of $f$ is $\Delta(P_n) = E_n(f)$. If there exists a polynomial of best approximation of degree $n$, there also exists a polynomial of best approximation of degree $n+1$. I Know this is old problem, But this problem Now I can't see any  solution,can you help me,Thank you : and there post this problem,but can't solution: There exist a degree $n+1$ polynomial of best approximation if there exist a degree $n$ polynomial of best approximation","Let $P_n$ be a polynomial of degree $n$. For a function $f:[a,b]\to\mathbb{R}$, Let $\Delta(P_n) = \sup_{x\in[a,b]} |f(x)-P_n(x)|$. and $E_n(f) = \inf_{P_n} \Delta(P_n)$. A polynomial $P_n$ is the best approximation of degree $n$ of $f$ is $\Delta(P_n) = E_n(f)$. If there exists a polynomial of best approximation of degree $n$, there also exists a polynomial of best approximation of degree $n+1$. I Know this is old problem, But this problem Now I can't see any  solution,can you help me,Thank you : and there post this problem,but can't solution: There exist a degree $n+1$ polynomial of best approximation if there exist a degree $n$ polynomial of best approximation",,['analysis']
58,Meaning of fractional Fourier transform with imaginary iteration count?,Meaning of fractional Fourier transform with imaginary iteration count?,,"As one may know, the Fourier Transform $$F[f](\nu) = \int_{-\infty}^{\infty} f(t) e^{-2\pi i \nu t} dt$$ can be iterated, and this iteration generalized to fractional iteration count via $$F^h[f](\nu) = \sqrt{1 - i \cot\left(\frac{\pi h}{2}\right)} e^{i\pi \cot\left(\frac{\pi h}{2}\right) \nu^2} \int_{-\infty}^{\infty} e^{-i 2\pi \left( \csc\left(\frac{\pi h}{2}\right) \nu t - \frac{\cot\left(\frac{\pi h}{2}\right)}{2} t^2\right)} f(t) dt$$. This can be interpreted, in a sense, as ""rotating"" $f$ between the time and frequency domain. In particular, $h = 1/2$ corresponds to a half-way rotation -- half-way between time and frequency. $h = -1$ is the inverse Fourier transform. $h = 2$ (i.e. $F \circ F$) is the ""mirror-image"" transform, that is, the time reversal of $f$, or $f(-t)$. The above formula may be written without the $\frac{\pi}{2}$ factors, which parameterizes it more directly in terms of the angle as an actual radian angle, with $\frac{\pi}{2}$ radians, i.e. a right angle, being the angle at which the frequency domain lies to the time one. But what is the interpretation and meaning of the above when $h = i$, that is, a single imaginary iteration of the transform? What does the ""spectrum"" given by $F^i[f]$ mean? More generally, what happens with $h = 2i$, $h = 3i$, etc.? EDIT: I note that for such positive-imaginary $h$, the kernel in the integrand grows quickly because the exponent becomes real (note that $\csc(ix) = -i\ \mathrm{csch}(x)$ and $\cot(ix) = -i \coth(x)$ and these get multiplied by $-i 2\pi$, turning them real), suggesting a strong decay of $f$ is required to get convergence, unless perhaps we can abuse analytic continuation from the real iteration counts in other cases somehow, though I think that might not yield a unique result. However it does seem to work better for negative -imaginary $h$, i.e. $h = -i$, $h = -2i$, etc. . Perhaps exploring those would be better? EDIT 2: Perhaps an example would be in order. Let $$f(t) = \begin{cases} 1, \mbox{if } |t| \le 1 \\ 0, \mbox{otherwise} \end{cases}$$, i.e. the ""boxcar"" function, or a fixed-width pulse in the time domain. Since the support is compact, the integral will converge. If we take the usual Fourier transform, $F^1[f] = F[f]$, we get a sinc function as the result. We can interpret this as a frequency spectrum of the original $f$, and it contains mostly low-frequency content, with little ripples of higher frequencies. But, suppose we take the imaginary Fourier transform -- one imaginary iteration of $F$, or $F^i$. What we get cannot be expressed in elementary terms, however we can use the ""imaginary error function"" to get a ""closed form"" of sorts: $$F^i[f](\nu) = \frac{\sqrt{1 – \coth\left(\frac{\pi}{2}\right)}}{2 \sqrt{\coth\left(\frac{\pi}{2}\right)}} e^{\pi \left(\coth\left(\frac{\pi}{2}\right) – \mathrm{csch}\left(\frac{\pi}{2}\right) \mathrm{sech}\left(\frac{\pi}{2}\right)\right) \nu^2} \\ \left(\mathrm{erfi}\left(\frac{2\pi \coth\left(\frac{\pi}{2}\right) – 2\pi \mathrm{csch}\left(\frac{\pi}{2}\right) \nu}{2 \sqrt{\pi \coth\left(\frac{\pi}{2}\right)}}\right) – \mathrm{erfi}\left(\frac{-2\pi \coth\left(\frac{\pi}{2}\right) – 2\pi \mathrm{csch}\left(\frac{\pi}{2}\right) \nu}{2 \sqrt{\pi \coth\left(\frac{\pi}{2}\right)}}\right)\right)$$ Now the $1 - \coth\left(\frac{\pi}{2}\right)$ under the one square root sign is negative, so $F^i[f]$ is purely imaginary at each $\nu$. At $0$, it is about $3.338i$, and grows very quickly as we move toward the positive and negative $\nu$ directions. What kind of ""spectrum"" is this ? What is it telling us about the boxcar function?","As one may know, the Fourier Transform $$F[f](\nu) = \int_{-\infty}^{\infty} f(t) e^{-2\pi i \nu t} dt$$ can be iterated, and this iteration generalized to fractional iteration count via $$F^h[f](\nu) = \sqrt{1 - i \cot\left(\frac{\pi h}{2}\right)} e^{i\pi \cot\left(\frac{\pi h}{2}\right) \nu^2} \int_{-\infty}^{\infty} e^{-i 2\pi \left( \csc\left(\frac{\pi h}{2}\right) \nu t - \frac{\cot\left(\frac{\pi h}{2}\right)}{2} t^2\right)} f(t) dt$$. This can be interpreted, in a sense, as ""rotating"" $f$ between the time and frequency domain. In particular, $h = 1/2$ corresponds to a half-way rotation -- half-way between time and frequency. $h = -1$ is the inverse Fourier transform. $h = 2$ (i.e. $F \circ F$) is the ""mirror-image"" transform, that is, the time reversal of $f$, or $f(-t)$. The above formula may be written without the $\frac{\pi}{2}$ factors, which parameterizes it more directly in terms of the angle as an actual radian angle, with $\frac{\pi}{2}$ radians, i.e. a right angle, being the angle at which the frequency domain lies to the time one. But what is the interpretation and meaning of the above when $h = i$, that is, a single imaginary iteration of the transform? What does the ""spectrum"" given by $F^i[f]$ mean? More generally, what happens with $h = 2i$, $h = 3i$, etc.? EDIT: I note that for such positive-imaginary $h$, the kernel in the integrand grows quickly because the exponent becomes real (note that $\csc(ix) = -i\ \mathrm{csch}(x)$ and $\cot(ix) = -i \coth(x)$ and these get multiplied by $-i 2\pi$, turning them real), suggesting a strong decay of $f$ is required to get convergence, unless perhaps we can abuse analytic continuation from the real iteration counts in other cases somehow, though I think that might not yield a unique result. However it does seem to work better for negative -imaginary $h$, i.e. $h = -i$, $h = -2i$, etc. . Perhaps exploring those would be better? EDIT 2: Perhaps an example would be in order. Let $$f(t) = \begin{cases} 1, \mbox{if } |t| \le 1 \\ 0, \mbox{otherwise} \end{cases}$$, i.e. the ""boxcar"" function, or a fixed-width pulse in the time domain. Since the support is compact, the integral will converge. If we take the usual Fourier transform, $F^1[f] = F[f]$, we get a sinc function as the result. We can interpret this as a frequency spectrum of the original $f$, and it contains mostly low-frequency content, with little ripples of higher frequencies. But, suppose we take the imaginary Fourier transform -- one imaginary iteration of $F$, or $F^i$. What we get cannot be expressed in elementary terms, however we can use the ""imaginary error function"" to get a ""closed form"" of sorts: $$F^i[f](\nu) = \frac{\sqrt{1 – \coth\left(\frac{\pi}{2}\right)}}{2 \sqrt{\coth\left(\frac{\pi}{2}\right)}} e^{\pi \left(\coth\left(\frac{\pi}{2}\right) – \mathrm{csch}\left(\frac{\pi}{2}\right) \mathrm{sech}\left(\frac{\pi}{2}\right)\right) \nu^2} \\ \left(\mathrm{erfi}\left(\frac{2\pi \coth\left(\frac{\pi}{2}\right) – 2\pi \mathrm{csch}\left(\frac{\pi}{2}\right) \nu}{2 \sqrt{\pi \coth\left(\frac{\pi}{2}\right)}}\right) – \mathrm{erfi}\left(\frac{-2\pi \coth\left(\frac{\pi}{2}\right) – 2\pi \mathrm{csch}\left(\frac{\pi}{2}\right) \nu}{2 \sqrt{\pi \coth\left(\frac{\pi}{2}\right)}}\right)\right)$$ Now the $1 - \coth\left(\frac{\pi}{2}\right)$ under the one square root sign is negative, so $F^i[f]$ is purely imaginary at each $\nu$. At $0$, it is about $3.338i$, and grows very quickly as we move toward the positive and negative $\nu$ directions. What kind of ""spectrum"" is this ? What is it telling us about the boxcar function?",,"['analysis', 'fourier-analysis']"
59,Green's function for the Laplace operator $\Delta$ in a rectangle (or square)?,Green's function for the Laplace operator  in a rectangle (or square)?,\Delta,"What I'm looking for is an explicit form of the Green's function $G$ for the Laplace operator $\Delta$ in a rectangular (or square) domain $D\subset\mathbb R^2$, e.g. $D=[0,1]\times[0,1]$. Namely, I'm looking for the solution $G:D\times D\rightarrow \mathbb R$ to the following PDE: $$ \begin{cases}   \Delta_x G(x,z) = -\delta_z(x) & \text{for }x\in D \\   G(x,z) = 0 & \text{for }x\in\partial D \end{cases} $$ where $\Delta_x$ is the laplacian with respect to the first variable $x\in\mathbb R^2$ and $\delta_z(x)$ is the Dirac delta ($\delta_z(x)=1$ if $x=z$ and $0$ otherwise). I need it in order to provide a (hopefully simple) example to a problem I am studying, which involves the Green's function. I don't know where to start and I am not very familiar with second order PDEs; any solution or hint is highly appreciated, thanks in advance!","What I'm looking for is an explicit form of the Green's function $G$ for the Laplace operator $\Delta$ in a rectangular (or square) domain $D\subset\mathbb R^2$, e.g. $D=[0,1]\times[0,1]$. Namely, I'm looking for the solution $G:D\times D\rightarrow \mathbb R$ to the following PDE: $$ \begin{cases}   \Delta_x G(x,z) = -\delta_z(x) & \text{for }x\in D \\   G(x,z) = 0 & \text{for }x\in\partial D \end{cases} $$ where $\Delta_x$ is the laplacian with respect to the first variable $x\in\mathbb R^2$ and $\delta_z(x)$ is the Dirac delta ($\delta_z(x)=1$ if $x=z$ and $0$ otherwise). I need it in order to provide a (hopefully simple) example to a problem I am studying, which involves the Green's function. I don't know where to start and I am not very familiar with second order PDEs; any solution or hint is highly appreciated, thanks in advance!",,"['analysis', 'partial-differential-equations', 'harmonic-functions']"
60,Can we do some scaling argument in the presence of inhomogeneous norms?,Can we do some scaling argument in the presence of inhomogeneous norms?,,"Notation : $B^n_R$ stands for the ball of radius $R$ in $\mathbb{R}^n$. $\hat{f}$ stands for the Fourier transform of $f$. Question . The following inequality holds true for all $f\in H^s(\mathbb{R}^n)$ and all measurable $t=t(x)$ provided that $s>1/2$ ($^1$):   $$\tag{1}\left\lVert \int_{\mathbb{R}^n}\, \hat{f}(\xi)e^{i(x\cdot\xi-t(x)\lvert\xi\rvert^2)}\, d\xi\right\rVert_{L^2_x(B^n_R)}\le C_s R^{\frac{1}{2}}\lVert f \rVert_{H^s(\mathbb{R}^n)}.$$   Can we use a scaling argument (or a variation of it) to show that the exponent of $R$ in the right hand side of (1) cannot be different from $1/2$? It is mostly the presence of the inhomogeneous norm $$\lVert f\rVert_{H^s}^2=\int_{\mathbb{R}^n}\lvert \hat{f}(\xi)\rvert^2\left( 1+ \lvert \xi\rvert^2\right)^s\, d\xi$$ which confuses me. However, I think that there should be a scaling explanation for that term $R^{1/2}$. Any hint is welcome. ($^1$) Cfr. Rogers-Vargas-Vega, Pointwise convergence of solutions of the non-elliptic Schrödinger equation , Theorem 3.","Notation : $B^n_R$ stands for the ball of radius $R$ in $\mathbb{R}^n$. $\hat{f}$ stands for the Fourier transform of $f$. Question . The following inequality holds true for all $f\in H^s(\mathbb{R}^n)$ and all measurable $t=t(x)$ provided that $s>1/2$ ($^1$):   $$\tag{1}\left\lVert \int_{\mathbb{R}^n}\, \hat{f}(\xi)e^{i(x\cdot\xi-t(x)\lvert\xi\rvert^2)}\, d\xi\right\rVert_{L^2_x(B^n_R)}\le C_s R^{\frac{1}{2}}\lVert f \rVert_{H^s(\mathbb{R}^n)}.$$   Can we use a scaling argument (or a variation of it) to show that the exponent of $R$ in the right hand side of (1) cannot be different from $1/2$? It is mostly the presence of the inhomogeneous norm $$\lVert f\rVert_{H^s}^2=\int_{\mathbb{R}^n}\lvert \hat{f}(\xi)\rvert^2\left( 1+ \lvert \xi\rvert^2\right)^s\, d\xi$$ which confuses me. However, I think that there should be a scaling explanation for that term $R^{1/2}$. Any hint is welcome. ($^1$) Cfr. Rogers-Vargas-Vega, Pointwise convergence of solutions of the non-elliptic Schrödinger equation , Theorem 3.",,"['analysis', 'partial-differential-equations', 'sobolev-spaces', 'harmonic-analysis']"
61,Infinite product of measurable spaces,Infinite product of measurable spaces,,"Suppose there is a family (can be infinite) of measurable spaces. What are the usual ways to define a sigma algebra on their Cartesian product? There is one way in the context of defining product measure on planetmath . Let $(E_i, B_i)$ be measurable spaces, where $i \in I$ is an index set, possibly infinite. We define their product as follows: let $E= \prod_{i \in I} E_i$ , the    Cartesian product of $E_i$ , let $B=\sigma((B_i)i \in I)$ , the    smallest sigma algebra containing    subsets of $E$ of the form $\prod_{i    \in I}B_i$ where $B_i=E_i$ for all    but a finite number of $i \in I$ . I was wondering why it is required that "" $B_i=E_i$ for all but a finite number of $i \in I$ ""? Thanks and regards! ADDED: I was wondering if the product sigma algebra defined in 2 is the smallest sigma algebra such that any tuple composed of one measurable set from each individual sigma algebra is measurable?","Suppose there is a family (can be infinite) of measurable spaces. What are the usual ways to define a sigma algebra on their Cartesian product? There is one way in the context of defining product measure on planetmath . Let be measurable spaces, where is an index set, possibly infinite. We define their product as follows: let , the    Cartesian product of , let , the    smallest sigma algebra containing    subsets of of the form where for all    but a finite number of . I was wondering why it is required that "" for all but a finite number of ""? Thanks and regards! ADDED: I was wondering if the product sigma algebra defined in 2 is the smallest sigma algebra such that any tuple composed of one measurable set from each individual sigma algebra is measurable?","(E_i, B_i) i \in I E= \prod_{i \in I} E_i E_i B=\sigma((B_i)i \in I) E \prod_{i    \in I}B_i B_i=E_i i \in I B_i=E_i i \in I","['measure-theory', 'elementary-set-theory', 'product-space']"
62,Reduction to particular cases of a lemma,Reduction to particular cases of a lemma,,"Lemma:Let $\phi(s)$ be a non-negative and non-decreasing function. Suposse that    \begin{equation} \phi(r) \le C_1 \left[\Bigl(\dfrac{r}{R}\Bigr)^\alpha + \mu \right]\phi(R) + C_2 R^\beta] \end{equation}   for all $r\le R \le R_0$, with $C_1,\alpha,\beta$ positive constants. Then, for any $\sigma<\min\{\alpha,\beta\}$ there exists a constant $\mu_0 = \mu_0(C_1,\alpha,\beta,\sigma)$ such that if $\mu<\mu_0$, then for for all $r\le R\le R_0$ we have   \begin{equation} \phi(r)\le C_3\Bigl(\dfrac{r}{R}\Bigr)^\sigma \left[\phi(R) + C_2R^\sigma \right] \end{equation}   where $C_3=C_3(C_1,\sigma-\min\{\alpha,\beta\})$ is a positive constant. In turn,    \begin{equation} \phi(r)\le C_4r^\sigma, \end{equation}   where $C_4=C_4(C_2,C_3,R_0,\phi,\sigma)$ is a positive constant. proof: We can assume $\beta < \alpha$ and, in this case, it suffices to show the estimate for $\sigma = \beta. \cdots$ Ask I'd like to prove the  assertion in the first line of the proof above. The lemma can be found here on page 9 and a similar lemma can be found in article,book in the end on page 10.","Lemma:Let $\phi(s)$ be a non-negative and non-decreasing function. Suposse that    \begin{equation} \phi(r) \le C_1 \left[\Bigl(\dfrac{r}{R}\Bigr)^\alpha + \mu \right]\phi(R) + C_2 R^\beta] \end{equation}   for all $r\le R \le R_0$, with $C_1,\alpha,\beta$ positive constants. Then, for any $\sigma<\min\{\alpha,\beta\}$ there exists a constant $\mu_0 = \mu_0(C_1,\alpha,\beta,\sigma)$ such that if $\mu<\mu_0$, then for for all $r\le R\le R_0$ we have   \begin{equation} \phi(r)\le C_3\Bigl(\dfrac{r}{R}\Bigr)^\sigma \left[\phi(R) + C_2R^\sigma \right] \end{equation}   where $C_3=C_3(C_1,\sigma-\min\{\alpha,\beta\})$ is a positive constant. In turn,    \begin{equation} \phi(r)\le C_4r^\sigma, \end{equation}   where $C_4=C_4(C_2,C_3,R_0,\phi,\sigma)$ is a positive constant. proof: We can assume $\beta < \alpha$ and, in this case, it suffices to show the estimate for $\sigma = \beta. \cdots$ Ask I'd like to prove the  assertion in the first line of the proof above. The lemma can be found here on page 9 and a similar lemma can be found in article,book in the end on page 10.",,['analysis']
63,Slowing down divergence 2,Slowing down divergence 2,,"Let $f(x)$ and $g(x)$ be positive nondecreasing functions such that $ \sum_{n>1} \frac1{f(n)} \text{ and } \sum_{n>1} \frac1{g(n)} $ diverges. (Why) must the series $$\sum_{n>1} \frac1{g(n)+f(n)}$$ diverge?","Let $f(x)$ and $g(x)$ be positive nondecreasing functions such that $ \sum_{n>1} \frac1{f(n)} \text{ and } \sum_{n>1} \frac1{g(n)} $ diverges. (Why) must the series $$\sum_{n>1} \frac1{g(n)+f(n)}$$ diverge?",,"['sequences-and-series', 'convergence-divergence']"
64,surface area of torus of revolution,surface area of torus of revolution,,"Here's a question from one of my exercises, Exercise 14. Let $C$ be a curve in $\mathbb{R}^2$ given by parametric equations $x=f(t)$ , $y=g(t)$ . Let $S$ be the surface of revolution of the curve $C$ about the $y$ -axis (something like the one shown in Figure 1). (a) Parametrize the surface $S$ . Parameters are $t$ and $\theta$ , where $0\leq\theta\leq2\pi$ . (b) Apply part (a) to parametrize the torus of revolution (see Figure 2) obtained by rotating the circle of radius $b$ centered at $(a,0)$ about the $y$ -axis. (Assume $a>b$ here.) I've got part (a) and (b), but how should I go about finding the surface area of this torus? Here's my solution to first 2 parts: (a) Applying cylindrical coordinates, since rotation is about $y$ -axis, let $x= r \cos \theta$ , $y = y$ and $z = r \sin \theta$ , at $\theta=0$ we get $f(t) = r$ , hence the parameterization is: $$x = f(t) \cos \theta\quad y = g(t) \quad z = f(t) \sin \theta$$ for $a \leq t \leq b$ and $0 \leq\theta\leq 2\pi$ . (b) In the plane $x,y$ the circle is $x = a + b \cos \psi$ , $y = b \sin \psi$ . From part (a), we get the following parametrization for the torus: $$x = (a + b \cos \psi)\cos \theta, \quad y = b \sin \psi, \quad z = (a + b \cos \psi)\sin \theta$$ where $0\leq\psi,\theta\leq 2\pi$ . What is the surface area of this torus?","Here's a question from one of my exercises, Exercise 14. Let be a curve in given by parametric equations , . Let be the surface of revolution of the curve about the -axis (something like the one shown in Figure 1). (a) Parametrize the surface . Parameters are and , where . (b) Apply part (a) to parametrize the torus of revolution (see Figure 2) obtained by rotating the circle of radius centered at about the -axis. (Assume here.) I've got part (a) and (b), but how should I go about finding the surface area of this torus? Here's my solution to first 2 parts: (a) Applying cylindrical coordinates, since rotation is about -axis, let , and , at we get , hence the parameterization is: for and . (b) In the plane the circle is , . From part (a), we get the following parametrization for the torus: where . What is the surface area of this torus?","C \mathbb{R}^2 x=f(t) y=g(t) S C y S t \theta 0\leq\theta\leq2\pi b (a,0) y a>b y x= r \cos \theta y = y z = r \sin \theta \theta=0 f(t) = r x = f(t) \cos \theta\quad y = g(t) \quad z = f(t) \sin \theta a \leq t \leq b 0 \leq\theta\leq 2\pi x,y x = a + b \cos \psi y = b \sin \psi x = (a + b \cos \psi)\cos \theta, \quad y = b \sin \psi, \quad z = (a + b \cos \psi)\sin \theta 0\leq\psi,\theta\leq 2\pi",['multivariable-calculus']
65,Is this closed interval measurable?,Is this closed interval measurable?,,"A common example of a semiring of sets is the family of half open interals $(a,b]\subseteq\mathbb{R}$. Also, the premeasure $\rho((a,b])=b-a$ is well known to extend to a measure on a $\sigma$-algebra. With a little tinkering, I believe the ""mirror images"" across the origin of these intervals also form a semiring. That is, the sets of form $[-b,-a)\cup(a,b]$ for $0<a<b$ are also a semiring. Say $I_{a,b}=[-b,-a)\cup(a,b]$. If I put nearly the same premeasure $\rho(I_{a,b})=b-a$ on this semiring, then I think that $\rho$ can be extended to a measure on a $\sigma$-algebra by taking the measure which sends a set $A$ to $\mu(A)/2$ for the usual Lebesgue measure $\mu$ on $\mathbb{R}$. (I hope this is correct?) Is there a way to tell if a closed interval $[a,b]$ is $\rho^*$ measurable? I'm interested in seeing maybe an example first to figure this out. Take an interval $[1,2]$ for example. I know that $[1,2]$ is $\rho^*$ measurable if for any $I\subseteq\mathbb{R}$, then $$\rho^*(I)=\rho^*(I\cap[1,2])+\rho^*(I\setminus[1,2]).$$ My feeling is that $[1,2]$ is $\rho^*$ measurable just by testing it with a few subsets $I$ of the real line. Is there a way to prove or disprove whether this is true? Thank you.","A common example of a semiring of sets is the family of half open interals $(a,b]\subseteq\mathbb{R}$. Also, the premeasure $\rho((a,b])=b-a$ is well known to extend to a measure on a $\sigma$-algebra. With a little tinkering, I believe the ""mirror images"" across the origin of these intervals also form a semiring. That is, the sets of form $[-b,-a)\cup(a,b]$ for $0<a<b$ are also a semiring. Say $I_{a,b}=[-b,-a)\cup(a,b]$. If I put nearly the same premeasure $\rho(I_{a,b})=b-a$ on this semiring, then I think that $\rho$ can be extended to a measure on a $\sigma$-algebra by taking the measure which sends a set $A$ to $\mu(A)/2$ for the usual Lebesgue measure $\mu$ on $\mathbb{R}$. (I hope this is correct?) Is there a way to tell if a closed interval $[a,b]$ is $\rho^*$ measurable? I'm interested in seeing maybe an example first to figure this out. Take an interval $[1,2]$ for example. I know that $[1,2]$ is $\rho^*$ measurable if for any $I\subseteq\mathbb{R}$, then $$\rho^*(I)=\rho^*(I\cap[1,2])+\rho^*(I\setminus[1,2]).$$ My feeling is that $[1,2]$ is $\rho^*$ measurable just by testing it with a few subsets $I$ of the real line. Is there a way to prove or disprove whether this is true? Thank you.",,"['analysis', 'measure-theory']"
66,Convergence check (No steps/solutions/proofs please),Convergence check (No steps/solutions/proofs please),,"I just wish to check that I have got these right. Please just indicate whether the ans are right -- please don't show steps (I wish to figure those out myself). Given $\phi_n (x)=n^k(1-x)x^n$ where $k\in\mathbb R, \,\,\,\,\,\,x\in[0,1]$ Then $\phi_n'(x)$ converge pointwise to $0$ for $k<0$ only $\phi_n'(x)=n^{k+1}x^{n-1}[1-(1-{1\over n})x]$ Clearly $\phi_n'(x)$ converge pointwise to $0$  $\forall x\in[0,1)$ For $x=1$,   $\phi_n'(x)=-n^k$ so we need $k<0$ $\phi_n'(x)$ converge uniformly to $0$ for $k<0$ only $c_n:=\sup_{x\in[0,1]} |\phi_n(x)-0|=\sup_{x\in[0,1]} n^kx^{n-1}[n-(n+1)x]$ Then $c_n$ is continuous on a closed bounded interval hence attains its sup $\phi_n'(x_{stationary point})=n^k({n-1\over n+1})^{n-1}$ obtained by differentiation then substitution. Noting that this is $\phi_n'(0)=0$ and $\phi_n'(1)=-n^k$ we have that $c_n=n^k({n-1\over n+1})^{n-1}$. It then follows that we need $k<0$. $\int_0^1\phi_n(x)\,\,dx$ converge to $0$ for $k<2$ only $\int_0^1\phi_n(x)\,\,dx={n^k\over(n+1)(n+2)}\to{n^k\over n^2}$ Hence we need $k<2$ Thank you.","I just wish to check that I have got these right. Please just indicate whether the ans are right -- please don't show steps (I wish to figure those out myself). Given $\phi_n (x)=n^k(1-x)x^n$ where $k\in\mathbb R, \,\,\,\,\,\,x\in[0,1]$ Then $\phi_n'(x)$ converge pointwise to $0$ for $k<0$ only $\phi_n'(x)=n^{k+1}x^{n-1}[1-(1-{1\over n})x]$ Clearly $\phi_n'(x)$ converge pointwise to $0$  $\forall x\in[0,1)$ For $x=1$,   $\phi_n'(x)=-n^k$ so we need $k<0$ $\phi_n'(x)$ converge uniformly to $0$ for $k<0$ only $c_n:=\sup_{x\in[0,1]} |\phi_n(x)-0|=\sup_{x\in[0,1]} n^kx^{n-1}[n-(n+1)x]$ Then $c_n$ is continuous on a closed bounded interval hence attains its sup $\phi_n'(x_{stationary point})=n^k({n-1\over n+1})^{n-1}$ obtained by differentiation then substitution. Noting that this is $\phi_n'(0)=0$ and $\phi_n'(1)=-n^k$ we have that $c_n=n^k({n-1\over n+1})^{n-1}$. It then follows that we need $k<0$. $\int_0^1\phi_n(x)\,\,dx$ converge to $0$ for $k<2$ only $\int_0^1\phi_n(x)\,\,dx={n^k\over(n+1)(n+2)}\to{n^k\over n^2}$ Hence we need $k<2$ Thank you.",,"['real-analysis', 'analysis']"
67,Intuitive test of convergence,Intuitive test of convergence,,"Are there any intuitive tests that might help one decide whether a sequence of functions converges / converges uniformly? For example, an intuitive test I have recently realized for uniform continuity is that the graph of the function must not tend to having infinite slopes, and that helped. Any suggestions will be very much appreciated. Thanks.","Are there any intuitive tests that might help one decide whether a sequence of functions converges / converges uniformly? For example, an intuitive test I have recently realized for uniform continuity is that the graph of the function must not tend to having infinite slopes, and that helped. Any suggestions will be very much appreciated. Thanks.",,"['real-analysis', 'analysis', 'intuition']"
68,Upper bound for the quality of an $abc$-triple,Upper bound for the quality of an -triple,abc,"A triple of positive integers $(a,b,c)$ is an $abc$ -triple if $a$ and $b$ are coprime and $c = a + b$ . Define the quality or power of an $abc$ -triple as $P(a,b,c) = \frac{\log c}{\log \text{rad}(abc)}$ , where $\text{rad}(k)$ denotes the product of distinct prime divisors of $k$ . One version of the $abc$ -Conjecture is that for each $\varepsilon > 0$ , there are finitely many $abc$ -triples such that $P(a,b,c) > 1 + \varepsilon$ . There are finitely many known triples satisfying $P > 1.4$ , the so called good triples ,  and the largest (quality) is $P(2,3^{10} \cdot 109, 23^{5}) = 1.629911684 \dots$ (discovered by E. Reyssat). This leads me to some questions: Question one: Is there an upper bound for $P(a,b,c)$ simply in terms of $c$ and absolute constants (sharper than $\log_{2} c$ )? Question two: Is there an absolute upper bound for $P(a,b,c)$ so that no triple has higher quality? Best Answer: If there were such a bound, then asymptotic FLT would be in hand. (Thanks Ace of Base on MathOverflow). The full answer and restatement of the question can be found here .","A triple of positive integers is an -triple if and are coprime and . Define the quality or power of an -triple as , where denotes the product of distinct prime divisors of . One version of the -Conjecture is that for each , there are finitely many -triples such that . There are finitely many known triples satisfying , the so called good triples ,  and the largest (quality) is (discovered by E. Reyssat). This leads me to some questions: Question one: Is there an upper bound for simply in terms of and absolute constants (sharper than )? Question two: Is there an absolute upper bound for so that no triple has higher quality? Best Answer: If there were such a bound, then asymptotic FLT would be in hand. (Thanks Ace of Base on MathOverflow). The full answer and restatement of the question can be found here .","(a,b,c) abc a b c = a + b abc P(a,b,c) = \frac{\log c}{\log \text{rad}(abc)} \text{rad}(k) k abc \varepsilon > 0 abc P(a,b,c) > 1 + \varepsilon P > 1.4 P(2,3^{10} \cdot 109, 23^{5}) = 1.629911684 \dots P(a,b,c) c \log_{2} c P(a,b,c)","['number-theory', 'real-analysis', 'analysis', 'diophantine-equations', 'asymptotics']"
69,"$(f(1 - f(x)) = 1 - x^9$, $f(1) = 0$ and $f'(1) < 0$, then where is the real number $r$ such that $f(r) = r^{99}$?",",  and , then where is the real number  such that ?",(f(1 - f(x)) = 1 - x^9 f(1) = 0 f'(1) < 0 r f(r) = r^{99},"If $f(1 - f(x)) = 1 - x^9$ , $f$ : R $\to$ R is differentiable, $f(1) = 0$ and $f'(1) < 0$ , how to show there is a real number $r$ such that $$f(r) = r^{99}?$$ Edit: Taylor Theorem makes no use. I try to take $a = 1 - {1 \over n}$ , where n is also a real no. Then as $f'(1) < 0$ , $f$ is decreasing at $x = 1$ and $f(a) > f(1) = 0$ . By mean-value theorem, $$f(a) - f(1) = f'(a_0)(a - 1), a < a_0 < 1$$ Let $g(x) = f(x) - x^{99}$ , then clearly $g(1) < 0$ and $g(r) = 0$ . How about showing $g(a) > 0$ ? Or to take $a$ as something else?","If , : R R is differentiable, and , how to show there is a real number such that Edit: Taylor Theorem makes no use. I try to take , where n is also a real no. Then as , is decreasing at and . By mean-value theorem, Let , then clearly and . How about showing ? Or to take as something else?","f(1 - f(x)) = 1 - x^9 f \to f(1) = 0 f'(1) < 0 r f(r) = r^{99}? a = 1 - {1 \over n} f'(1) < 0 f x = 1 f(a) > f(1) = 0 f(a) - f(1) = f'(a_0)(a - 1), a < a_0 < 1 g(x) = f(x) - x^{99} g(1) < 0 g(r) = 0 g(a) > 0 a",['analysis']
70,Can anyone clarify the meaning of zero content?,Can anyone clarify the meaning of zero content?,,"I am having hard time understanding the definition of zero content. The following are the definitions of zero content in $\mathbb{R}$ and $\mathbb{R}^2.$ A set $Z \subset \mathbb{R}$ is said to have zero content if $\forall \epsilon > 0$ there is a collection of intervals $I_1, \ldots, I_L$ such that (i) $Z \subset \bigcup_1^L I_l,$ and (ii) the sum of the lengths of the $I_l$ is less than $\epsilon.$ A set $Z \subset \mathbb{R}^2$ is said to have zero content if $\forall \epsilon > 0$ there is a finite collection of rectangles $R_i$ such that (i) $Z \subset \bigcup_1^M R_i$ and (ii) the sum of areas of the $R_i$ is less than $\epsilon.$ Thanks!","I am having hard time understanding the definition of zero content. The following are the definitions of zero content in $\mathbb{R}$ and $\mathbb{R}^2.$ A set $Z \subset \mathbb{R}$ is said to have zero content if $\forall \epsilon > 0$ there is a collection of intervals $I_1, \ldots, I_L$ such that (i) $Z \subset \bigcup_1^L I_l,$ and (ii) the sum of the lengths of the $I_l$ is less than $\epsilon.$ A set $Z \subset \mathbb{R}^2$ is said to have zero content if $\forall \epsilon > 0$ there is a finite collection of rectangles $R_i$ such that (i) $Z \subset \bigcup_1^M R_i$ and (ii) the sum of areas of the $R_i$ is less than $\epsilon.$ Thanks!",,['analysis']
71,Infinite closed subset of $S^1$ such that the squaring map is a bijection?,Infinite closed subset of  such that the squaring map is a bijection?,S^1,Is there an infinite closed subset $X$ of the unit circle in $\mathbb C$ such that the squaring map induces a bijection from $X$ to itself?,Is there an infinite closed subset $X$ of the unit circle in $\mathbb C$ such that the squaring map induces a bijection from $X$ to itself?,,['analysis']
72,Characterization of Asymptotic Stability via KL-class functions,Characterization of Asymptotic Stability via KL-class functions,,"Let us adopt the following definition of stability and asymptotic stability of a dynamical system of the form: $$ \dot{x}=f(x) $$ The trajectory of this system starting from the initial point $x_0$ at $t_0=0$ will be hereinafter denoted by $\varphi(t,x_0)$. Definition 1 (Stability) . Let $x^\star$ be an equilibrium point for $\dot{x}=f(x)$, i.e. $f(x^\star)=0$. This point is said to be stable if for every $\varepsilon>0$ there is a $\delta=\delta(\varepsilon)>0$ such that $$ \|x-x^\star\|<\delta \implies \|\varphi(t,x_0)-x^\star\|<\varepsilon $$ for all $t\geq 0$. Definition 2 (Asymptotic Stability) . Let $x^\star$ be an equilibrium point for $\dot{x}=f(x)$, i.e. $f(x^\star)=0$. This point is said to be asymptotically stable if it is stable and additionally there is an $\alpha>0$ such that $$\lim_{t\to\infty}\|\varphi(t,x_0)-x^\star\|=0$$ for all $x$ with $\|x-x^\star\|<\alpha$. Definition 3 (Classes of functions) . We define the following four classes of functions: A function $\alpha:\mathbb{R}^+\to\mathbb{R}^+$ is said to be $\mathcal{K}$-class and we denote $\alpha\in\mathcal{K}$ if it is continuous, strictly increasing and $\alpha(0)=0$. A function $\beta:\mathbb{R}^+\to\mathbb{R}^+$ is said to be $\mathcal{K}_\infty$-class and we denote $\beta\in\mathcal{K}_\infty$ if $\beta\in\mathcal{K}$ and it is unbounded. A $\mu:\mathbb{R}^+\to\mathbb{R}^+$ is said to be $\mathcal{L}$-class and we denote $\mu\in\mathcal{L}$ if it is continuous, strictly decreasing and $\lim_{\tau\to\infty}\mu(\tau)=0$ A $\gamma:\mathbb{R}^+\times\mathbb{R}^+\to\mathbb{R}^+$ is said to be $\mathcal{KL}$-class and we denote $\gamma\in\mathcal{KL}$ if $\gamma(\cdot,t)\in\mathcal{K}$ for all $t\in\mathbb{R}^+$ and $\gamma(r,\cdot)\in\mathcal{L}$ for all $r\in\mathbb{R}^+$ Proposition 1. An equilibrium point $x^\star$ is asymptotically stable if and only if there is a $\gamma\in\mathcal{KL}$ and a constant $\kappa>0$ so that: $$  \|\varphi(t;x_0)-x^\star\|\leq \gamma(\|x_0-x_\star\|,t) $$ for all $x_0$ such that $\|x_0-x^\star\|<\kappa$ and for all $t\geq 0$. I have some problem with the proof! Proof: 1. [The first part looks easy] Let us assume that $x^\star$ satisfies the inequality mentioned above. Let us choose an $\varepsilon>0$; $\gamma(\cdot,t)$ is $\mathcal{K}$-class, so we may choose a $\delta$ such that $0<\delta<\kappa$ and $\gamma(\delta,0)\leq \varepsilon$. Now for $\|x_0-x^\star\|<\delta$ we have: $$  \|\varphi(t;x_0)-x^\star\|\leq\gamma(\|x_0-x_\star\|,t)\leq\gamma(\|x_0-x_\star\|,0)\leq\gamma(\delta,0)\leq\varepsilon $$ So if the inequality state above holds then $x^\star$ is stable and more: $$  0\leq\lim_{t\to\infty}\|\varphi(t;x_0)-x^\star\|\leq \lim_{t\to\infty}\gamma(\|x_0-x^\star\|,t)=0 $$ Therefore $$  \lim_{t\to\infty}\|\varphi(t;x_0)-x^\star\|=0\Leftrightarrow\lim_{t\to\infty}\varphi(t;x_0)=x^\star $$ What about the converse? How can one prove it? We need to prove that if $x^\star$ is a (locally) asymptotically stable equilibrium point, then there is a $\gamma\in\mathcal{KL}$ and a constant $\kappa>0$ so that: $$  \|\varphi(t;x_0)-x^\star\|\leq \gamma(\|x_0-x^\star\|,t) $$ for all $x_0$ such that $\|x_0-x^\star\|<\kappa$ and for all $t\geq 0$. I would appreciate a proof or at least a reference to some book or article where I can find it clearly explained.","Let us adopt the following definition of stability and asymptotic stability of a dynamical system of the form: $$ \dot{x}=f(x) $$ The trajectory of this system starting from the initial point $x_0$ at $t_0=0$ will be hereinafter denoted by $\varphi(t,x_0)$. Definition 1 (Stability) . Let $x^\star$ be an equilibrium point for $\dot{x}=f(x)$, i.e. $f(x^\star)=0$. This point is said to be stable if for every $\varepsilon>0$ there is a $\delta=\delta(\varepsilon)>0$ such that $$ \|x-x^\star\|<\delta \implies \|\varphi(t,x_0)-x^\star\|<\varepsilon $$ for all $t\geq 0$. Definition 2 (Asymptotic Stability) . Let $x^\star$ be an equilibrium point for $\dot{x}=f(x)$, i.e. $f(x^\star)=0$. This point is said to be asymptotically stable if it is stable and additionally there is an $\alpha>0$ such that $$\lim_{t\to\infty}\|\varphi(t,x_0)-x^\star\|=0$$ for all $x$ with $\|x-x^\star\|<\alpha$. Definition 3 (Classes of functions) . We define the following four classes of functions: A function $\alpha:\mathbb{R}^+\to\mathbb{R}^+$ is said to be $\mathcal{K}$-class and we denote $\alpha\in\mathcal{K}$ if it is continuous, strictly increasing and $\alpha(0)=0$. A function $\beta:\mathbb{R}^+\to\mathbb{R}^+$ is said to be $\mathcal{K}_\infty$-class and we denote $\beta\in\mathcal{K}_\infty$ if $\beta\in\mathcal{K}$ and it is unbounded. A $\mu:\mathbb{R}^+\to\mathbb{R}^+$ is said to be $\mathcal{L}$-class and we denote $\mu\in\mathcal{L}$ if it is continuous, strictly decreasing and $\lim_{\tau\to\infty}\mu(\tau)=0$ A $\gamma:\mathbb{R}^+\times\mathbb{R}^+\to\mathbb{R}^+$ is said to be $\mathcal{KL}$-class and we denote $\gamma\in\mathcal{KL}$ if $\gamma(\cdot,t)\in\mathcal{K}$ for all $t\in\mathbb{R}^+$ and $\gamma(r,\cdot)\in\mathcal{L}$ for all $r\in\mathbb{R}^+$ Proposition 1. An equilibrium point $x^\star$ is asymptotically stable if and only if there is a $\gamma\in\mathcal{KL}$ and a constant $\kappa>0$ so that: $$  \|\varphi(t;x_0)-x^\star\|\leq \gamma(\|x_0-x_\star\|,t) $$ for all $x_0$ such that $\|x_0-x^\star\|<\kappa$ and for all $t\geq 0$. I have some problem with the proof! Proof: 1. [The first part looks easy] Let us assume that $x^\star$ satisfies the inequality mentioned above. Let us choose an $\varepsilon>0$; $\gamma(\cdot,t)$ is $\mathcal{K}$-class, so we may choose a $\delta$ such that $0<\delta<\kappa$ and $\gamma(\delta,0)\leq \varepsilon$. Now for $\|x_0-x^\star\|<\delta$ we have: $$  \|\varphi(t;x_0)-x^\star\|\leq\gamma(\|x_0-x_\star\|,t)\leq\gamma(\|x_0-x_\star\|,0)\leq\gamma(\delta,0)\leq\varepsilon $$ So if the inequality state above holds then $x^\star$ is stable and more: $$  0\leq\lim_{t\to\infty}\|\varphi(t;x_0)-x^\star\|\leq \lim_{t\to\infty}\gamma(\|x_0-x^\star\|,t)=0 $$ Therefore $$  \lim_{t\to\infty}\|\varphi(t;x_0)-x^\star\|=0\Leftrightarrow\lim_{t\to\infty}\varphi(t;x_0)=x^\star $$ What about the converse? How can one prove it? We need to prove that if $x^\star$ is a (locally) asymptotically stable equilibrium point, then there is a $\gamma\in\mathcal{KL}$ and a constant $\kappa>0$ so that: $$  \|\varphi(t;x_0)-x^\star\|\leq \gamma(\|x_0-x^\star\|,t) $$ for all $x_0$ such that $\|x_0-x^\star\|<\kappa$ and for all $t\geq 0$. I would appreciate a proof or at least a reference to some book or article where I can find it clearly explained.",,"['analysis', 'dynamical-systems']"
73,"Homeomorphism from $(0,1)$ to $\mathbb{R}$",Homeomorphism from  to,"(0,1) \mathbb{R}","I want to show that $(0,1)$  is homeomorphic to $\mathbb{R}$ by finding a homeomorphism between the two. I think the function will be related to $tan(x)$ but I'm stuck on how to modify it to fit the domain $(0,1)$. Any help would be appreciated!","I want to show that $(0,1)$  is homeomorphic to $\mathbb{R}$ by finding a homeomorphism between the two. I think the function will be related to $tan(x)$ but I'm stuck on how to modify it to fit the domain $(0,1)$. Any help would be appreciated!",,"['analysis', 'metric-spaces']"
74,number with finite binary representation and infinite decimal representation,number with finite binary representation and infinite decimal representation,,"One can easily find numbers with finite decimal representation with infinite binary representation. (Like $0.3$ and $0.01010101..$) I assume there is an opposite case, meaning a number with finite binary representation but infinite decimal representation, does any of you know such number? if the existence is impossible then why?","One can easily find numbers with finite decimal representation with infinite binary representation. (Like $0.3$ and $0.01010101..$) I assume there is an opposite case, meaning a number with finite binary representation but infinite decimal representation, does any of you know such number? if the existence is impossible then why?",,"['analysis', 'numerical-methods']"
75,Are monotonic and bijective functions the same?,Are monotonic and bijective functions the same?,,"The question is simple :  consider the family of monotonic functions ; $m(x) : \mathbb{R} \rightarrow \mathbb{R}$, and the familly of bijective functions ; $b(x) : \mathbb{R} \rightarrow \mathbb{R}$.  Are they actually the same?  If not, I would like to see some simple counter-examples.","The question is simple :  consider the family of monotonic functions ; $m(x) : \mathbb{R} \rightarrow \mathbb{R}$, and the familly of bijective functions ; $b(x) : \mathbb{R} \rightarrow \mathbb{R}$.  Are they actually the same?  If not, I would like to see some simple counter-examples.",,"['real-analysis', 'analysis', 'functions', 'continuity']"
76,What functions satisfy such equation?,What functions satisfy such equation?,,"Let $f:(-a,a)\rightarrow \mathbb R$ be a  continuous function such that $$ f(0)=\frac{f(-x)+f(x)}{2} \textrm{ for } |x|<a. $$ What about $f$? Is it necessarilly an odd function?","Let $f:(-a,a)\rightarrow \mathbb R$ be a  continuous function such that $$ f(0)=\frac{f(-x)+f(x)}{2} \textrm{ for } |x|<a. $$ What about $f$? Is it necessarilly an odd function?",,['analysis']
77,Prove $x^{4}-x+1=0$ has no solution,Prove  has no solution,x^{4}-x+1=0,I would like to prove that the following equation has no solution in $\mathbb{R}$    $$x^{4}-x+1=0$$ my question : could we use  Intermediate Value Theorem to prove it otherways I'm interested in more ways of prove that has no solution in $\mathbb{R}$. without : i know that we can prove it by that way : $x=x^4+1\geq 1$ and $x^4-x+1=x^2(x^2-1)+x^2-x+1>0$,I would like to prove that the following equation has no solution in $\mathbb{R}$    $$x^{4}-x+1=0$$ my question : could we use  Intermediate Value Theorem to prove it otherways I'm interested in more ways of prove that has no solution in $\mathbb{R}$. without : i know that we can prove it by that way : $x=x^4+1\geq 1$ and $x^4-x+1=x^2(x^2-1)+x^2-x+1>0$,,"['calculus', 'real-analysis', 'analysis']"
78,About the domain of the derivative,About the domain of the derivative,,"Is the domain of the derivative $f'(x)$ of a function $f(x)$ conditioned by the function $f(x)$ itself? I mean, take for example $f(x) = \ln(x)$ whose domain is $x > 0$ . We have $f'(x) = \frac{1}{x}$ , whose natural domain is $\mathbb{R}\backslash\{0\}$ . Yet I don't ""care"" about the branch $x < 0$ , for $f(x)$ is not defined over here. So my question is: is $f'(x)$ a new function, or when dealing with its domain I have to take into account the informaton about the domain of $f(x)$ ?","Is the domain of the derivative of a function conditioned by the function itself? I mean, take for example whose domain is . We have , whose natural domain is . Yet I don't ""care"" about the branch , for is not defined over here. So my question is: is a new function, or when dealing with its domain I have to take into account the informaton about the domain of ?",f'(x) f(x) f(x) f(x) = \ln(x) x > 0 f'(x) = \frac{1}{x} \mathbb{R}\backslash\{0\} x < 0 f(x) f'(x) f(x),"['calculus', 'analysis', 'derivatives']"
79,Approximation of exponential function by power series,Approximation of exponential function by power series,,"Let $x \in (-\frac{1}{2},\frac{1}{2}), n \in \mathbb{N}$ How can I choose a $n$ that the the inequality is valid? $$\left|e^x-\sum_{k=0}^n \frac{x^k}{k!}\right| \leq \frac{|e^x|}{10^{16}}$$ My ideas: Try some values for $n$ and verify the inequality for value greater than $-1/2$ and less than $1/2$ because of the monotony of the exponential function... But I could not find a $n$.","Let $x \in (-\frac{1}{2},\frac{1}{2}), n \in \mathbb{N}$ How can I choose a $n$ that the the inequality is valid? $$\left|e^x-\sum_{k=0}^n \frac{x^k}{k!}\right| \leq \frac{|e^x|}{10^{16}}$$ My ideas: Try some values for $n$ and verify the inequality for value greater than $-1/2$ and less than $1/2$ because of the monotony of the exponential function... But I could not find a $n$.",,['analysis']
80,Generalizing integration,Generalizing integration,,"Is it possible to generalize the notion of integrals to other sets than the reals? In particular, would it be possible to integrate over a subset of the reals such as the rationals or the irrationals? What about more exotic sets?","Is it possible to generalize the notion of integrals to other sets than the reals? In particular, would it be possible to integrate over a subset of the reals such as the rationals or the irrationals? What about more exotic sets?",,['analysis']
81,Typo in Hatcher? $\mathbb{R}^n - \{x \} \cong S^{n-1} \times \mathbb{R}$?,Typo in Hatcher? ?,\mathbb{R}^n - \{x \} \cong S^{n-1} \times \mathbb{R},On page 35 Hatcher writes that  $\mathbb{R}^n - \{x \}$  is homeomorphic to  $ S^{n-1} \times \mathbb{R}$. I know e.g. $\mathbb{R}^2 - \{x\}$ is homotopy equivalent to $S^1$ and also to $S^1 \times \mathbb{R}$. I don't see though how they are homeomorphic. Is this a typo? Thanks for your help!,On page 35 Hatcher writes that  $\mathbb{R}^n - \{x \}$  is homeomorphic to  $ S^{n-1} \times \mathbb{R}$. I know e.g. $\mathbb{R}^2 - \{x\}$ is homotopy equivalent to $S^1$ and also to $S^1 \times \mathbb{R}$. I don't see though how they are homeomorphic. Is this a typo? Thanks for your help!,,"['analysis', 'algebraic-topology']"
82,"Prove that $\forall x > 0, x - 1 \ge \ln(x)$",Prove that,"\forall x > 0, x - 1 \ge \ln(x)","Prove that $\forall x > 0, x - 1 \ge \ln(x)$ . Here is my proof: We prove the inequality on two intervals, $(0,1]$ and $[1,+\infty)$. First the easier one, $[1,+\infty)$. Notice that at $x=1$, the inequality holds true. Notice also that $\forall x\in [1,+\infty)$, $\frac{1}{x} \le 1 \le x$. But this is the same as: $$\forall x\in [1,+\infty), \frac{d[ \ln{x}]}{dx} \le 1 \le \frac{d[x-1]}{dx}$$ But we know that if $f,g$ are two functions such that, for some $x_0$, $f(x_0) \ge g(x_0)$ and $\forall x\ge x_0, f'(x)\ge g'(x),$ then $\forall x \ge x_0 , f(x) \ge g(x). $ Therefore our inequality holds on the interval $[1,+\infty)$. Now for the second interval. Consider the functions $f(x) = -x -1$ and $g(x) = \ln(-x)$. These are essentially the reflections of our two previous functions in the y-axis. We will consider these functions only on the interval $[-1, 0)$. Notice that at $x=-1$, we have $f(x) = g(x)$. Furthermore, on our interval $\frac{1}{x} \le -1 \le x$. But this is exactly the same as:  $$\forall x \in [-1, 0), g'(x) \le f'(x)$$ By the same theorem as before, we have: $$\forall x \in [-1, 0), \ln(-x) \le -x-1$$ Now let $y=-x$. Then $y \in (0,1]$ and the previous statement is equivalent to:  $$\Leftrightarrow \forall y\in (0,1], \ln(y) \le y-1$$ Hence we have shown that the equality holds on $(0,+\infty].$ Is the proof complete? I am especially curious about the validity of the last step, where we let $-x = y$. Is that a valid step? Is there any way to shorten the proof?","Prove that $\forall x > 0, x - 1 \ge \ln(x)$ . Here is my proof: We prove the inequality on two intervals, $(0,1]$ and $[1,+\infty)$. First the easier one, $[1,+\infty)$. Notice that at $x=1$, the inequality holds true. Notice also that $\forall x\in [1,+\infty)$, $\frac{1}{x} \le 1 \le x$. But this is the same as: $$\forall x\in [1,+\infty), \frac{d[ \ln{x}]}{dx} \le 1 \le \frac{d[x-1]}{dx}$$ But we know that if $f,g$ are two functions such that, for some $x_0$, $f(x_0) \ge g(x_0)$ and $\forall x\ge x_0, f'(x)\ge g'(x),$ then $\forall x \ge x_0 , f(x) \ge g(x). $ Therefore our inequality holds on the interval $[1,+\infty)$. Now for the second interval. Consider the functions $f(x) = -x -1$ and $g(x) = \ln(-x)$. These are essentially the reflections of our two previous functions in the y-axis. We will consider these functions only on the interval $[-1, 0)$. Notice that at $x=-1$, we have $f(x) = g(x)$. Furthermore, on our interval $\frac{1}{x} \le -1 \le x$. But this is exactly the same as:  $$\forall x \in [-1, 0), g'(x) \le f'(x)$$ By the same theorem as before, we have: $$\forall x \in [-1, 0), \ln(-x) \le -x-1$$ Now let $y=-x$. Then $y \in (0,1]$ and the previous statement is equivalent to:  $$\Leftrightarrow \forall y\in (0,1], \ln(y) \le y-1$$ Hence we have shown that the equality holds on $(0,+\infty].$ Is the proof complete? I am especially curious about the validity of the last step, where we let $-x = y$. Is that a valid step? Is there any way to shorten the proof?",,"['analysis', 'functions', 'inequality', 'proof-verification', 'logarithms']"
83,"Is $\{\frac{m}{10^n}\mid m,n\in\mathbb Z,\ n\geq 0\}$ dense in $\mathbb R$?",Is  dense in ?,"\{\frac{m}{10^n}\mid m,n\in\mathbb Z,\ n\geq 0\} \mathbb R","The set $S$ of real numbers of the form $m/10^n$, $m,n$ integers and $n$ greater than or equal to $0$, is a dense subset of $\mathbb R$ or not?? I know dense means closure of $S$ in $\mathbb R$ is $\mathbb R$ then it will be dense. How to prove or disprove I have no idea.","The set $S$ of real numbers of the form $m/10^n$, $m,n$ integers and $n$ greater than or equal to $0$, is a dense subset of $\mathbb R$ or not?? I know dense means closure of $S$ in $\mathbb R$ is $\mathbb R$ then it will be dense. How to prove or disprove I have no idea.",,"['real-analysis', 'analysis']"
84,"Proving a polynomial has a solution in the interval (0,1) [duplicate]","Proving a polynomial has a solution in the interval (0,1) [duplicate]",,"This question already has answers here : Prove that $\sum_{k=0}^n a_k x^k = 0$ has at least $1$ real root if $\sum_{k=0}^n \frac{a_k}{k+1} = 0$ (2 answers) Closed 5 years ago . I have no idea how to start this problem. I am assuming that the Mean Value Theorem is needed in the proof but I am not exactly sure how to apply it to the given polynomial. Any hints/help would be appreciated. If $ \frac{a_n}{n+1} +  \frac{a_{n-1}}{n} + \cdot \cdot \cdot + \frac{a_1}{2} + a_0 = 0 $, prove that the polynomial equation $ a_n x^n + a_{n-1}x^{n-1} + \cdot \cdot \cdot + a_1x + a_0 = 0$ has a solution in $(0,1)$.","This question already has answers here : Prove that $\sum_{k=0}^n a_k x^k = 0$ has at least $1$ real root if $\sum_{k=0}^n \frac{a_k}{k+1} = 0$ (2 answers) Closed 5 years ago . I have no idea how to start this problem. I am assuming that the Mean Value Theorem is needed in the proof but I am not exactly sure how to apply it to the given polynomial. Any hints/help would be appreciated. If $ \frac{a_n}{n+1} +  \frac{a_{n-1}}{n} + \cdot \cdot \cdot + \frac{a_1}{2} + a_0 = 0 $, prove that the polynomial equation $ a_n x^n + a_{n-1}x^{n-1} + \cdot \cdot \cdot + a_1x + a_0 = 0$ has a solution in $(0,1)$.",,"['analysis', 'polynomials']"
85,Request for example of a value of limit of a sequence where sequence does not converge,Request for example of a value of limit of a sequence where sequence does not converge,,"In Limit of the nested radical $\sqrt{7+\sqrt{7+\sqrt{7+\cdots}}}$ Timothy Wagner gave a correct answer that was questioned for not having shown that the limit exists in the first place. My question is, are there any examples were a value of limit can be derived although the limit does not exist? Please note I am not questioning that limit must exist before it is exhibited but that, how a value for limit can be exhibited if the limit doesn't exist? Is that not a contradiction? On one hand we have a value for the limit and on the other hand the proof that it can't exist! Please give an example were a more general form of convergence does not account for the calculated value. Otherwise it seems as if the value was hinting that the notion of convergence required adjustment and not the calculated value that required justification.","In Limit of the nested radical $\sqrt{7+\sqrt{7+\sqrt{7+\cdots}}}$ Timothy Wagner gave a correct answer that was questioned for not having shown that the limit exists in the first place. My question is, are there any examples were a value of limit can be derived although the limit does not exist? Please note I am not questioning that limit must exist before it is exhibited but that, how a value for limit can be exhibited if the limit doesn't exist? Is that not a contradiction? On one hand we have a value for the limit and on the other hand the proof that it can't exist! Please give an example were a more general form of convergence does not account for the calculated value. Otherwise it seems as if the value was hinting that the notion of convergence required adjustment and not the calculated value that required justification.",,"['analysis', 'logic']"
86,"Disprove the statement: If a function is twice differentiable at a local maximum point, then its second derivative is negative at that point","Disprove the statement: If a function is twice differentiable at a local maximum point, then its second derivative is negative at that point",,"I have to develop a counter example that disproves this statement but I am not to sure on how to go about this. Is it something simple that I am forgetting about? If a function is twice differentiable at a local maximum point, then its second derivative is negative at that point","I have to develop a counter example that disproves this statement but I am not to sure on how to go about this. Is it something simple that I am forgetting about? If a function is twice differentiable at a local maximum point, then its second derivative is negative at that point",,"['analysis', 'functions']"
87,Why for dirac function $\int_{-\infty}^{\infty}\delta(x)\ dx=1$,Why for dirac function,\int_{-\infty}^{\infty}\delta(x)\ dx=1,"The dirac function is defined as $\delta(x)=\infty$ when $x=0$, $\delta(x)=0$ otherwise. I am wondering why we can derive $\int_{-\infty}^{\infty}\delta(x)\ dx=1$, or this is just a definition","The dirac function is defined as $\delta(x)=\infty$ when $x=0$, $\delta(x)=0$ otherwise. I am wondering why we can derive $\int_{-\infty}^{\infty}\delta(x)\ dx=1$, or this is just a definition",,['analysis']
88,Help with Spivak's Calculus: Chapter 1 problem 21,Help with Spivak's Calculus: Chapter 1 problem 21,,"I've been stuck on this problem for over a day, and the answerbook simply says ""see chapter 5"" for problems 20,21, and 22. But I want to complete the problem without using knowledge given later in the book, so I've been banging my head against the wall trying all sorts of things, but nothing I do seems to lead me anywhere. The problem is as follows: Prove that if $|x - x_0| < min (\frac{\varepsilon}{2(|y_0| + 1)}, 1)$ and $|y - y_0| < min (\frac{\varepsilon}{2(|x_0| + 1)}, 1)$ then $|xy - x_0 y_0| < \varepsilon$. Here are some of the things I've been thinking about, I don't know which of these are useful (if any), but they somewhat outline the logic behind my various attempts. Since at most $|x - x_0| < 1$ and $|y - y_0| < 1$ then it follows that $(|x-x_0|)(y-y_0|) < |x-x_0|$ and $(|x-x_0|)(y-y_0|) < |y-y_0|$ Also, $(|x - x_0|)(|y_0| + 1) < \frac{\varepsilon}{2}$ and $(|y - y_0|)(|x_0| + 1) < \frac{\varepsilon}{2}$ so $(|x - x_0|)(|y_0| + 1) + (|y - y_0|)(|x_0| + 1) < \varepsilon$. and since $|a + b| \leq |a| + |b| < \varepsilon$ I've tried multiplying things out, and then adding them together to see if anything cancels, but I can't make anything meaningful come out of it. Also since $|a - b| \leq |a| + |b|$ I've also tried subtracting one side from the other, but to no avail. I was also thinking that since $(|x-x_0|)(y-y_0|) < |x-x_0|$, then I could try something along the lines of $(|x-x_0|)(y-y_0|)(|x_0| + 1) + (|x-x_0|)(y-y_0|)(|y_0| + 1)< \varepsilon$ and various combinations as such, but I just can't seem to get anything meaningful to come out of any of these attempts. I have a sneaking suspicion that the road to the solution is simpler than I'm making it out to be, but I just can't see it.","I've been stuck on this problem for over a day, and the answerbook simply says ""see chapter 5"" for problems 20,21, and 22. But I want to complete the problem without using knowledge given later in the book, so I've been banging my head against the wall trying all sorts of things, but nothing I do seems to lead me anywhere. The problem is as follows: Prove that if $|x - x_0| < min (\frac{\varepsilon}{2(|y_0| + 1)}, 1)$ and $|y - y_0| < min (\frac{\varepsilon}{2(|x_0| + 1)}, 1)$ then $|xy - x_0 y_0| < \varepsilon$. Here are some of the things I've been thinking about, I don't know which of these are useful (if any), but they somewhat outline the logic behind my various attempts. Since at most $|x - x_0| < 1$ and $|y - y_0| < 1$ then it follows that $(|x-x_0|)(y-y_0|) < |x-x_0|$ and $(|x-x_0|)(y-y_0|) < |y-y_0|$ Also, $(|x - x_0|)(|y_0| + 1) < \frac{\varepsilon}{2}$ and $(|y - y_0|)(|x_0| + 1) < \frac{\varepsilon}{2}$ so $(|x - x_0|)(|y_0| + 1) + (|y - y_0|)(|x_0| + 1) < \varepsilon$. and since $|a + b| \leq |a| + |b| < \varepsilon$ I've tried multiplying things out, and then adding them together to see if anything cancels, but I can't make anything meaningful come out of it. Also since $|a - b| \leq |a| + |b|$ I've also tried subtracting one side from the other, but to no avail. I was also thinking that since $(|x-x_0|)(y-y_0|) < |x-x_0|$, then I could try something along the lines of $(|x-x_0|)(y-y_0|)(|x_0| + 1) + (|x-x_0|)(y-y_0|)(|y_0| + 1)< \varepsilon$ and various combinations as such, but I just can't seem to get anything meaningful to come out of any of these attempts. I have a sneaking suspicion that the road to the solution is simpler than I'm making it out to be, but I just can't see it.",,"['calculus', 'analysis']"
89,Is there a bijection from a bounded open interval of $\mathbb{Q}$ onto $\mathbb{Q}$?,Is there a bijection from a bounded open interval of  onto ?,\mathbb{Q} \mathbb{Q},"It is easy to create a bijection between two bounded open intervals of $\mathbb{R}$, such as: $$ \begin{align}  f : (a,b) &\to (\alpha,\beta) \\  x &\mapsto \alpha+(x-a)(\beta-\alpha). \end{align} $$ It is also possible to biject a bounded open interval of $\mathbb{R}$ onto the whole of $\mathbb{R}$, e.g.: $$ \begin{align}  f : (a,b) &\to \mathbb{R} \\  x &\mapsto \tanh^{-1} x. \end{align} $$ Consider now the set of rational numbers $\mathbb{Q}$. The bijection between two bounded open intervals still holds, but is it possible to biject: $$ f : (p,q) \to \mathbb{Q} $$ where $p,q\in\mathbb{Q}$ and $(p,q) = \{x\in \mathbb{Q} : p < x < q\}$?","It is easy to create a bijection between two bounded open intervals of $\mathbb{R}$, such as: $$ \begin{align}  f : (a,b) &\to (\alpha,\beta) \\  x &\mapsto \alpha+(x-a)(\beta-\alpha). \end{align} $$ It is also possible to biject a bounded open interval of $\mathbb{R}$ onto the whole of $\mathbb{R}$, e.g.: $$ \begin{align}  f : (a,b) &\to \mathbb{R} \\  x &\mapsto \tanh^{-1} x. \end{align} $$ Consider now the set of rational numbers $\mathbb{Q}$. The bijection between two bounded open intervals still holds, but is it possible to biject: $$ f : (p,q) \to \mathbb{Q} $$ where $p,q\in\mathbb{Q}$ and $(p,q) = \{x\in \mathbb{Q} : p < x < q\}$?",,"['real-analysis', 'analysis', 'elementary-set-theory', 'rational-numbers']"
90,"Find a bijection from $[0,1]$ to $[0,1]$ that is not strictly monotone. is this possible?",Find a bijection from  to  that is not strictly monotone. is this possible?,"[0,1] [0,1]","I'm not convinced this is possible, as soon as you have $2$ distinct elements mapping to the same number, the function is no longer $1$-$1$ and therefore not a bijection.","I'm not convinced this is possible, as soon as you have $2$ distinct elements mapping to the same number, the function is no longer $1$-$1$ and therefore not a bijection.",,[]
91,Monotonic function; limits from the right and from the left,Monotonic function; limits from the right and from the left,,"Can someone explain this to me? It seems quite easy, but somehow I can't manage to prove this on my own ... or in other words: when exactly does a function $f$ not have  limits from the right and from the left ? (Picture is a screenshot from wikipedia)","Can someone explain this to me? It seems quite easy, but somehow I can't manage to prove this on my own ... or in other words: when exactly does a function $f$ not have  limits from the right and from the left ? (Picture is a screenshot from wikipedia)",,"['real-analysis', 'analysis', 'epsilon-delta']"
92,Equivalent statements not giving equivalent negations,Equivalent statements not giving equivalent negations,,"Consider the statement $\forall \epsilon >0 \exists y\ldots.$ Negating it gives $\exists \epsilon>0 \forall y\ldots.$ I understand this. $\forall \epsilon:\epsilon>0 \exists y\ldots.$ is equivalent to the first statement, but negating it gives $\exists\epsilon:\epsilon\leq 0 \forall y\ldots.$ What is my error? It makes sense why first one is right, so the first statement has to be different from last statement, but why is it different isn't $\epsilon>0=\epsilon:\epsilon>0 ?$ Any answer or reference to a website is appreciated.","Consider the statement Negating it gives I understand this. is equivalent to the first statement, but negating it gives What is my error? It makes sense why first one is right, so the first statement has to be different from last statement, but why is it different isn't Any answer or reference to a website is appreciated.",\forall \epsilon >0 \exists y\ldots. \exists \epsilon>0 \forall y\ldots. \forall \epsilon:\epsilon>0 \exists y\ldots. \exists\epsilon:\epsilon\leq 0 \forall y\ldots. \epsilon>0=\epsilon:\epsilon>0 ?,"['analysis', 'logic', 'predicate-logic', 'quantifiers']"
93,Showing that Lebesgue Dominated convergence theorem is false in case of Riemann integration.,Showing that Lebesgue Dominated convergence theorem is false in case of Riemann integration.,,"I was reading Tom Apostol book called ""Mathematical Analysis"" and I read this statement: the Lebesgue Dominated convergence theorem is false in case of Riemann integration. Here is the statement of LDCT: My question is: Could someone give me an example that shows that LDCT is false in the case of Riemann integration, please?","I was reading Tom Apostol book called ""Mathematical Analysis"" and I read this statement: the Lebesgue Dominated convergence theorem is false in case of Riemann integration. Here is the statement of LDCT: My question is: Could someone give me an example that shows that LDCT is false in the case of Riemann integration, please?",,"['real-analysis', 'analysis']"
94,If $A=\int_{0}^{1} x^n(1-x)^ndx$ then $A^{-1} \in \mathbb{N}$.,If  then .,A=\int_{0}^{1} x^n(1-x)^ndx A^{-1} \in \mathbb{N},Consider the integral $$A=\int_{0}^{1} x^n(1-x)^ndx$$ Prove that $A^{-1}$ is a natural number. I have no idea how to compute the integral. But by putting $n=1$ I got $A^{-1}=6$ a natural number. Is there any simple method to show that $A^{-1}$ is a natural number?,Consider the integral Prove that is a natural number. I have no idea how to compute the integral. But by putting I got a natural number. Is there any simple method to show that is a natural number?,A=\int_{0}^{1} x^n(1-x)^ndx A^{-1} n=1 A^{-1}=6 A^{-1},"['analysis', 'definite-integrals']"
95,"Let $f$ be continuous on $[0,1]$ such that $\int_0^x f = \int_x^1 f$, what is $f$?","Let  be continuous on  such that , what is ?","f [0,1] \int_0^x f = \int_x^1 f f","Let $f$ be continuous on $[0,1]$ , and suppose that for all $x$ , $0<x<1$ , $\int_0^x f = \int_x^1 f$ Can you determine $f$ ? I've argued the following: Since $f$ is continuous on $[0,1]$ , it follows that $f$ has an antiderivative $F$ on $I=[0,1]$ . Evaluating both sides of the equation we get $F(x)-F(0) = F(1)-F(x)$ = $2F(x) = F(1) + F(0)$ $F(x) = \frac{F(1) + F(0)}{2}$ Taking the derivative of both sides we get $F'(x) = f(x) = \frac{F'(1) + F'(0)}{2} = 0$ since $F(1) + F(0)$ is a constant. I'm not sure if I'm going about this problem correctly.","Let be continuous on , and suppose that for all , , Can you determine ? I've argued the following: Since is continuous on , it follows that has an antiderivative on . Evaluating both sides of the equation we get = Taking the derivative of both sides we get since is a constant. I'm not sure if I'm going about this problem correctly.","f [0,1] x 0<x<1 \int_0^x f = \int_x^1 f f f [0,1] f F I=[0,1] F(x)-F(0) = F(1)-F(x) 2F(x) = F(1) + F(0) F(x) = \frac{F(1) + F(0)}{2} F'(x) = f(x) = \frac{F'(1) + F'(0)}{2} = 0 F(1) + F(0)","['calculus', 'analysis']"
96,How to prove $\lim\limits_{x\to 0}x^m (\ln x)^n = 0$,How to prove,\lim\limits_{x\to 0}x^m (\ln x)^n = 0,"$$\lim\limits_{x\to 0^+}x^m (\ln x)^n = 0\quad \,\text{for} \quad m,n \in \mathbb N$$ Question : How can I prove this? Is there a better way than saying, well, if the factor $\lim\limits_{x\to 0}x = 0$ the whole equation is $0$ ? Note : I have found a similiar post but however I need to solve the task without integrals. I appreciate every hint.","Question : How can I prove this? Is there a better way than saying, well, if the factor the whole equation is ? Note : I have found a similiar post but however I need to solve the task without integrals. I appreciate every hint.","\lim\limits_{x\to 0^+}x^m (\ln x)^n = 0\quad \,\text{for} \quad m,n \in \mathbb N \lim\limits_{x\to 0}x = 0 0","['analysis', 'logarithms']"
97,"I have to think of a metric that makes (0,1) an unbounded interval","I have to think of a metric that makes (0,1) an unbounded interval",,I don't even know how to begin to answer this question?,I don't even know how to begin to answer this question?,,['analysis']
98,Prove that any continuous integer-valued function of a real variable is constant.,Prove that any continuous integer-valued function of a real variable is constant.,,"I'm stuck on a question which asks to prove that any continuous integer-valued function of a real variable is constant. In my lecture notes we are told that a map is continuous if the preimage of any open set is open. How do I use this to answer the question? A full answer would be greatly appreciated as I can't seem to figure out how to solve it in my  notes, in analysis books or anywhere online.","I'm stuck on a question which asks to prove that any continuous integer-valued function of a real variable is constant. In my lecture notes we are told that a map is continuous if the preimage of any open set is open. How do I use this to answer the question? A full answer would be greatly appreciated as I can't seem to figure out how to solve it in my  notes, in analysis books or anywhere online.",,['analysis']
99,$ \begin{vmatrix} f(a) & g(a) & h(a) \\ f(b) & g(b) & h(b) \\ f'(c) & g'(c) & h'(c) \\ \end{vmatrix} =0 $,, \begin{vmatrix} f(a) & g(a) & h(a) \\ f(b) & g(b) & h(b) \\ f'(c) & g'(c) & h'(c) \\ \end{vmatrix} =0 ,"If $f(x),g(x),h(x)$ have derivatives in $[a,b]$ , show that there exists a value $c$ of $x$ in $(a,b)$ such that $$     \begin{vmatrix}     f(a) & g(a) & h(a) \\     f(b) & g(b) & h(b) \\     f'(c) & g'(c) & h'(c) \\     \end{vmatrix} =0 $$ I am getting an idea of using generalized mean value theorem, but not able to proceed. Need help!","If have derivatives in , show that there exists a value of in such that I am getting an idea of using generalized mean value theorem, but not able to proceed. Need help!","f(x),g(x),h(x) [a,b] c x (a,b) 
    \begin{vmatrix}
    f(a) & g(a) & h(a) \\
    f(b) & g(b) & h(b) \\
    f'(c) & g'(c) & h'(c) \\
    \end{vmatrix} =0
","['real-analysis', 'analysis', 'derivatives']"
