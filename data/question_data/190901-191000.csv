,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$f$ not increasing in neighborhood of $0$,not increasing in neighborhood of,f 0,"Let $f: \mathbb R \to \mathbb R$, \begin{eqnarray} f(x)= \begin{cases} x-x^2&\quad\text {if } x \in \mathbb Q \cr  x+x^2&\quad\text{if } x \notin \mathbb Q \cr \end{cases}  \end{eqnarray} Show that $f'(0)=1$ but $f$ isnt increasing in the neighborhood of $0$. I derivated both of $f(x)$ and both result in $f'(0)=1$ (not sure if both are needed, or just the one where $x \in \mathbb Q$). Can anyone help if I've misunderstood anything, is it about being continuous/differentiable? Since $x+x^2$ is over $x-x^2$, after $0$ it isnt increasing, but how do I prove it","Let $f: \mathbb R \to \mathbb R$, \begin{eqnarray} f(x)= \begin{cases} x-x^2&\quad\text {if } x \in \mathbb Q \cr  x+x^2&\quad\text{if } x \notin \mathbb Q \cr \end{cases}  \end{eqnarray} Show that $f'(0)=1$ but $f$ isnt increasing in the neighborhood of $0$. I derivated both of $f(x)$ and both result in $f'(0)=1$ (not sure if both are needed, or just the one where $x \in \mathbb Q$). Can anyone help if I've misunderstood anything, is it about being continuous/differentiable? Since $x+x^2$ is over $x-x^2$, after $0$ it isnt increasing, but how do I prove it",,"['real-analysis', 'derivatives']"
1,Formula for smallest distance between two parabolas,Formula for smallest distance between two parabolas,,"I have been struggling with this problem I came across: Create a general formula for finding the closest points between two   parabolas. Given that the parabolas have opposing concavity and are not interesecting. I want to answer this problem in the simplest way possible so I can plug in any a, b, and c and get out a value that will give me the points  that are closest together in pair of parabolas. I have tried different approaches but I end up having to solve for x when x has exponents up to the power of three and I haven't been able to solve them correctly or efficiently. My closest attempt is the following (sorry to be wordy): The two parabolas $$f_1(x)=ax^2+bx+c$$$$f_2(x)=gx^2+hx+j$$ have the same slope at the closest distance between them.  So I realized I needed a function that returned an x value based on a given slope (because the derivative of these parabolas are ax + b.) The functions are$$x_1(m)=\frac{m-b}{2a}$$ $$x_2(m)=\frac{m-h}{2g}$$ If I know that those are my x's and I know that at some m, the closest distance exists, then I can try to create a formula for finding the m that gives the closest distance. To do that we plug the 'x' functions into the distance formula. $$d(m)=\sqrt{[x_1(m)-x_2(m)]^2+[f_1(x_1(m))-f_2(x_2(m))]^2}$$ and because this is a type of optimization problem, we can remove the square root (square both sides) before finding the derivative. We need to find the roots of the derivative to find our relative extrema of d(m). (for readability x 1 (m) -> x 1 and f 2 (m) -> f 2 etc.) $$d'(m)=2(x_1-x_2)(x_1'-x_2')+2\Big[f_1(x_1)-f_2(x_2)\Big]\Big[f_1'(x_1)x_1'-f_2'(x_2)x_2'\Big]$$ divide both sides by two (remember, optimization) and expand the functions $$d'(m)=(\frac{m-b}{2a}-\frac{m-h}{2g})(\frac{1}{2a}-\frac{1}{2g})+\Big[a(\frac{m-b}{2a})^2+b(\frac{m-b}{2a})+c-g(\frac{m-h}{2g})^2-h(\frac{m-h}{2g})-j\Big]\Big[a(\frac{m-b}{2a})(\frac{1}{2a})+b(\frac{1}{2a})-g(\frac{m-h}{2g})(\frac{1}{2g})-h(\frac{1}{2g})\Big]$$ But now here I am stuck. I don't know how to solve for m but I can see where m's roots are on a graphing calculator. TL;DR used distance formula on two parabolas but ended with cubic functions that I don't know how to solve for. Help me find a way to complete this problem. P.S. there are other questions covering this topic but they do not discuss a general formula for this.","I have been struggling with this problem I came across: Create a general formula for finding the closest points between two   parabolas. Given that the parabolas have opposing concavity and are not interesecting. I want to answer this problem in the simplest way possible so I can plug in any a, b, and c and get out a value that will give me the points  that are closest together in pair of parabolas. I have tried different approaches but I end up having to solve for x when x has exponents up to the power of three and I haven't been able to solve them correctly or efficiently. My closest attempt is the following (sorry to be wordy): The two parabolas $$f_1(x)=ax^2+bx+c$$$$f_2(x)=gx^2+hx+j$$ have the same slope at the closest distance between them.  So I realized I needed a function that returned an x value based on a given slope (because the derivative of these parabolas are ax + b.) The functions are$$x_1(m)=\frac{m-b}{2a}$$ $$x_2(m)=\frac{m-h}{2g}$$ If I know that those are my x's and I know that at some m, the closest distance exists, then I can try to create a formula for finding the m that gives the closest distance. To do that we plug the 'x' functions into the distance formula. $$d(m)=\sqrt{[x_1(m)-x_2(m)]^2+[f_1(x_1(m))-f_2(x_2(m))]^2}$$ and because this is a type of optimization problem, we can remove the square root (square both sides) before finding the derivative. We need to find the roots of the derivative to find our relative extrema of d(m). (for readability x 1 (m) -> x 1 and f 2 (m) -> f 2 etc.) $$d'(m)=2(x_1-x_2)(x_1'-x_2')+2\Big[f_1(x_1)-f_2(x_2)\Big]\Big[f_1'(x_1)x_1'-f_2'(x_2)x_2'\Big]$$ divide both sides by two (remember, optimization) and expand the functions $$d'(m)=(\frac{m-b}{2a}-\frac{m-h}{2g})(\frac{1}{2a}-\frac{1}{2g})+\Big[a(\frac{m-b}{2a})^2+b(\frac{m-b}{2a})+c-g(\frac{m-h}{2g})^2-h(\frac{m-h}{2g})-j\Big]\Big[a(\frac{m-b}{2a})(\frac{1}{2a})+b(\frac{1}{2a})-g(\frac{m-h}{2g})(\frac{1}{2g})-h(\frac{1}{2g})\Big]$$ But now here I am stuck. I don't know how to solve for m but I can see where m's roots are on a graphing calculator. TL;DR used distance formula on two parabolas but ended with cubic functions that I don't know how to solve for. Help me find a way to complete this problem. P.S. there are other questions covering this topic but they do not discuss a general formula for this.",,"['calculus', 'derivatives', 'optimization', 'conic-sections', 'cubics']"
2,Using chain rule take the derivative of $x^{\sin(x)}$,Using chain rule take the derivative of,x^{\sin(x)},How do I use specifically the chain rule for $x^{\sin(x)}$?,How do I use specifically the chain rule for $x^{\sin(x)}$?,,"['calculus', 'derivatives', 'chain-rule']"
3,Does the matrix square root have directional derivatives at semipositive points?,Does the matrix square root have directional derivatives at semipositive points?,,"$\newcommand{\psym}{\operatorname{P}_{\ge 0}}$ Let $\psym$ be the set of symmetric positive semidefinite (real) matrices. Let $\sqrt \cdot :\psym  \to \psym $ be the unique positive semidefinite square root. Let $A \in \psym$ be a matrix on the boundary , i.e $\det A=0$. Let $B$ be a symmetric matrix such that $A+tB$ is positive semidefinite for $t>0$ small enough**. Quesion: Characterize all such pairs $A,B$ where the one sided directional derivative $$  (d\sqrt{\cdot})_A(B)=\left. \frac{d}{dt}\right|_{t=0}  \sqrt{A+tB}:=\lim_{t \to 0^+} \frac{ \sqrt{A+tB}-\sqrt{A}}{t} \, \, \text{ exist.}$$ Partial results: (1) For $A=0$ the limit exists only for $B=0$. (2) For every $A=B$ the limit exists (and equals $\frac{1}{2}\sqrt{A}$). (3) For every $A$, there are a ""lot"" of matrices $B$ for which the limit does not exist (see the answer of loup blanc): By orthogonal diagonalization, after noting that $(d\sqrt{\cdot})_A(B)$ exists $\iff (d\sqrt{\cdot})_{V^TAV}(V^TBV)$ exists for every $V \in O_n$, we reduce to the case $A=diag(0,a_2,\cdots,a_n)$ where $a_i\geq 0$:  For every matrix $B$ of the form $B=diag(\lambda,C)$ where $\lambda > 0,C\in M_{n-1}$ is symmetric s.t. $diag(a_2,\cdots,a_n)+C\geq 0$,  $(d\sqrt{\cdot})_A(B)$ does not exist. In this question , it is shown that $\sqrt{\cdot} $ is not differentiable in the standard sense. In fact, this answer shows $\psym$ is not a manifold (with boundary) at all, so ""standard differentiability"" does not really make sense here. ** Actually, as noted by loup blanc, if $B$ is a symmetric matrix such that $A+B \in \psym$, then $A+tB \in \psym$ for small enough $t$. Indeed, $A,A+B \in \psym$ and $\psym$ is convex, hence $(1-t)A+t(A+B)=A+tB \in \psym$. Note that the reverse implication is false in general; It can happen that $A+tB \in \psym$, but $A+B \notin \psym$, see here .","$\newcommand{\psym}{\operatorname{P}_{\ge 0}}$ Let $\psym$ be the set of symmetric positive semidefinite (real) matrices. Let $\sqrt \cdot :\psym  \to \psym $ be the unique positive semidefinite square root. Let $A \in \psym$ be a matrix on the boundary , i.e $\det A=0$. Let $B$ be a symmetric matrix such that $A+tB$ is positive semidefinite for $t>0$ small enough**. Quesion: Characterize all such pairs $A,B$ where the one sided directional derivative $$  (d\sqrt{\cdot})_A(B)=\left. \frac{d}{dt}\right|_{t=0}  \sqrt{A+tB}:=\lim_{t \to 0^+} \frac{ \sqrt{A+tB}-\sqrt{A}}{t} \, \, \text{ exist.}$$ Partial results: (1) For $A=0$ the limit exists only for $B=0$. (2) For every $A=B$ the limit exists (and equals $\frac{1}{2}\sqrt{A}$). (3) For every $A$, there are a ""lot"" of matrices $B$ for which the limit does not exist (see the answer of loup blanc): By orthogonal diagonalization, after noting that $(d\sqrt{\cdot})_A(B)$ exists $\iff (d\sqrt{\cdot})_{V^TAV}(V^TBV)$ exists for every $V \in O_n$, we reduce to the case $A=diag(0,a_2,\cdots,a_n)$ where $a_i\geq 0$:  For every matrix $B$ of the form $B=diag(\lambda,C)$ where $\lambda > 0,C\in M_{n-1}$ is symmetric s.t. $diag(a_2,\cdots,a_n)+C\geq 0$,  $(d\sqrt{\cdot})_A(B)$ does not exist. In this question , it is shown that $\sqrt{\cdot} $ is not differentiable in the standard sense. In fact, this answer shows $\psym$ is not a manifold (with boundary) at all, so ""standard differentiability"" does not really make sense here. ** Actually, as noted by loup blanc, if $B$ is a symmetric matrix such that $A+B \in \psym$, then $A+tB \in \psym$ for small enough $t$. Indeed, $A,A+B \in \psym$ and $\psym$ is convex, hence $(1-t)A+t(A+B)=A+tB \in \psym$. Note that the reverse implication is false in general; It can happen that $A+tB \in \psym$, but $A+B \notin \psym$, see here .",,"['real-analysis', 'linear-algebra', 'derivatives', 'matrix-calculus', 'positive-definite']"
4,Is there a closed form solution for $\frac {d^n}{dx^n} \frac {1}{1-ae^x}$?,Is there a closed form solution for ?,\frac {d^n}{dx^n} \frac {1}{1-ae^x},"I am trying to find a closed form solution for $$\frac {d^n}{dx^n} \frac {1}{1-ae^x}$$ I have tried recognizing a pattern by computing specific cases but so far no luck. I do know that the function is a composition of $x^{-1}$ and $1-ae^x$. The $k$th derivative of the latter is the same for all $k\ge 1$ and there is a closed form of the $k$th derivative of the former. Yet I am still not finding any answer. However, when computing the derivatives manually I can see that in the numerator there is always a monic polynomial in $ae^x$ with constant term equal to one, all multiplied by $-ae^x$ and the denominator is always raised to the $n+1$ for the $n$th derivative. I tried arranging the coefficients of the polynomial in the numerator in a triangle like that of Pascal's but I only recognized a pattern for the coefficients of the first and last two terms. I tried Faa di Bruno's Formula but frankly it is over my head. Is there a clever solution or even a solution at all?","I am trying to find a closed form solution for $$\frac {d^n}{dx^n} \frac {1}{1-ae^x}$$ I have tried recognizing a pattern by computing specific cases but so far no luck. I do know that the function is a composition of $x^{-1}$ and $1-ae^x$. The $k$th derivative of the latter is the same for all $k\ge 1$ and there is a closed form of the $k$th derivative of the former. Yet I am still not finding any answer. However, when computing the derivatives manually I can see that in the numerator there is always a monic polynomial in $ae^x$ with constant term equal to one, all multiplied by $-ae^x$ and the denominator is always raised to the $n+1$ for the $n$th derivative. I tried arranging the coefficients of the polynomial in the numerator in a triangle like that of Pascal's but I only recognized a pattern for the coefficients of the first and last two terms. I tried Faa di Bruno's Formula but frankly it is over my head. Is there a clever solution or even a solution at all?",,"['derivatives', 'closed-form']"
5,Find the derivative of $f(x) = \frac{e^{x^{2}} (\arcsin{x})^{2}x\sqrt{\cos{x}}}{(\ln{x})^{6} \sin^{2}x}$,Find the derivative of,f(x) = \frac{e^{x^{2}} (\arcsin{x})^{2}x\sqrt{\cos{x}}}{(\ln{x})^{6} \sin^{2}x},"Question: Find the derivative of: $$f(x) = \frac{e^{x^{2}} (\arcsin{x})^{2}x\sqrt{\cos{x}}}{(\ln{x})^{6} \sin^{2}x}$$ Attempted Solution The most productive approach seems to be logarithmic differentiation: $$\ln |f(x)| = \ln \left|\frac{e^{x^{2}} (\arcsin{x})^{2}x\sqrt{\cos{x}}}{(\ln{x})^{6} \sin^{2}x}\right|$$ Distributing the natural logarithm function gives addition instead of multiplication and subtraction instead of division: $$\ln \left|\frac{e^{x^{2}} (\arcsin{x})^{2}x\sqrt{\cos{x}}}{(\ln{x})^{6} \sin^{2}x}\right| = \ln |e^{x^2}| + \ln |(\arcsin x)^2| + \ln |x|$$ $$+ \ln |\sqrt{\cos x}| - \ln |(\ln x)^6| - \ln |\sin^2 x|$$ Taking the derivative of both sides gives us: $$f'(x) = f(x) \left( \frac{2 e^{x^2}}{e^{x^2}} + \frac{2 \arcsin x}{(\arcsin x)^2} \frac{1}{\sqrt{1-x^2}} + \frac{1}{x} + \frac{-\sin x}{2 \cos x} + \frac{6 (\ln x)^5}{(\ln x)^6} \frac{1}{x} - \frac{2 \sin x \cos x}{\sin x}\right) $$ Simplifying gives: $$f'(x) = f(x) \left( 2 + \frac{2}{\arcsin x \sqrt{1-x^2}} + \frac{1}{x} -\frac{1}{2} \tan x + \frac{6}{x \ln x} + 2\cos x \right)$$ However, this is not the correct answer. In particular, the first and last terms are wrong. Where and how did it go wrong?","Question: Find the derivative of: $$f(x) = \frac{e^{x^{2}} (\arcsin{x})^{2}x\sqrt{\cos{x}}}{(\ln{x})^{6} \sin^{2}x}$$ Attempted Solution The most productive approach seems to be logarithmic differentiation: $$\ln |f(x)| = \ln \left|\frac{e^{x^{2}} (\arcsin{x})^{2}x\sqrt{\cos{x}}}{(\ln{x})^{6} \sin^{2}x}\right|$$ Distributing the natural logarithm function gives addition instead of multiplication and subtraction instead of division: $$\ln \left|\frac{e^{x^{2}} (\arcsin{x})^{2}x\sqrt{\cos{x}}}{(\ln{x})^{6} \sin^{2}x}\right| = \ln |e^{x^2}| + \ln |(\arcsin x)^2| + \ln |x|$$ $$+ \ln |\sqrt{\cos x}| - \ln |(\ln x)^6| - \ln |\sin^2 x|$$ Taking the derivative of both sides gives us: $$f'(x) = f(x) \left( \frac{2 e^{x^2}}{e^{x^2}} + \frac{2 \arcsin x}{(\arcsin x)^2} \frac{1}{\sqrt{1-x^2}} + \frac{1}{x} + \frac{-\sin x}{2 \cos x} + \frac{6 (\ln x)^5}{(\ln x)^6} \frac{1}{x} - \frac{2 \sin x \cos x}{\sin x}\right) $$ Simplifying gives: $$f'(x) = f(x) \left( 2 + \frac{2}{\arcsin x \sqrt{1-x^2}} + \frac{1}{x} -\frac{1}{2} \tan x + \frac{6}{x \ln x} + 2\cos x \right)$$ However, this is not the correct answer. In particular, the first and last terms are wrong. Where and how did it go wrong?",,"['calculus', 'derivatives']"
6,How do I calculate the gradient of a discrete function?,How do I calculate the gradient of a discrete function?,,"In the continuous case, I have $$\lim_{x\to x_0} \frac{f(x) - f(x_0)}{x - x_0} = \lim_{h\to 0} \frac{f(x_0 +h) - f(x_0)}{h}$$ But what is the gradient of the function $f: \mathbb{N} \rightarrow \mathbb{N}$, for example $f(n) = n^2$? For example, at $n=4$ I would expect it to be either $$5^2 - 4^2 = 25 - 16 = 9$$ or $$(3^2 - 4^2)/(-1) = (9 - 16)/(-1) = 7$$ Which of both is the correct solution? Or is it something different like the mean of both?","In the continuous case, I have $$\lim_{x\to x_0} \frac{f(x) - f(x_0)}{x - x_0} = \lim_{h\to 0} \frac{f(x_0 +h) - f(x_0)}{h}$$ But what is the gradient of the function $f: \mathbb{N} \rightarrow \mathbb{N}$, for example $f(n) = n^2$? For example, at $n=4$ I would expect it to be either $$5^2 - 4^2 = 25 - 16 = 9$$ or $$(3^2 - 4^2)/(-1) = (9 - 16)/(-1) = 7$$ Which of both is the correct solution? Or is it something different like the mean of both?",,"['discrete-mathematics', 'derivatives']"
7,Derivative of Heaviside Function and Equivalence,Derivative of Heaviside Function and Equivalence,,"The derivative of the Heaviside function $\theta(x - a)$ is normally taken to be the delta function $\delta(x - a)$ . This question has two parts, the first is whether a constant coefficient is introduced when differentiating the Heaviside function. According to Wolframalpha (I know it is not the best reference site but unable to find information elsewhere), $$\frac{\partial}{\partial x}\theta(x/a - 1) = \frac{1}{a}\delta(x/a - 1),$$ whereas, $$\frac{\partial}{\partial x}\theta(x-a) = \delta(x - a),$$ although these two Heaviside functions are essentially the same. Wolframalpha: first equation second equation . The second part is whether is is correct to say that, $$\theta(x - a) = \theta(x/a - 1),$$ which the above seems to imply is not true but I would normally freely switch between the two. I guess a similar question would be if, $$\delta(x - a) = \delta(x/a - 1).$$ This could be a mistake by me in either formulating the problem or not really a proper mathematical question (I am a physicist and so skirt the boundaries of mathematical rigour). Many thanks","The derivative of the Heaviside function is normally taken to be the delta function . This question has two parts, the first is whether a constant coefficient is introduced when differentiating the Heaviside function. According to Wolframalpha (I know it is not the best reference site but unable to find information elsewhere), whereas, although these two Heaviside functions are essentially the same. Wolframalpha: first equation second equation . The second part is whether is is correct to say that, which the above seems to imply is not true but I would normally freely switch between the two. I guess a similar question would be if, This could be a mistake by me in either formulating the problem or not really a proper mathematical question (I am a physicist and so skirt the boundaries of mathematical rigour). Many thanks","\theta(x - a) \delta(x - a) \frac{\partial}{\partial x}\theta(x/a - 1) = \frac{1}{a}\delta(x/a - 1), \frac{\partial}{\partial x}\theta(x-a) = \delta(x - a), \theta(x - a) = \theta(x/a - 1), \delta(x - a) = \delta(x/a - 1).","['derivatives', 'dirac-delta']"
8,Notation for higher degree derivatives,Notation for higher degree derivatives,,"Lebniz's notation for ordinary derivatives as quotients of differentials is a convenient abuse of notation, since it lets you express things like the chain rule and the derivative of the inverse function in a suggestive form: $$\frac{dz}{dx} = \frac{dy}{dx} \cdot \frac{dz}{dy}$$ $$\frac{dx}{dy} = \frac{1}{\frac{dy}{dx}}$$ where $x,y,z$ are interdependent variables. This approach completely breaks down with second and higher order derivatives, since for example the second derivative of the inverse is $$\frac{d^2x}{dy^2} =-\frac{d^2y}{dx^2} \left(\frac{dy}{dx} \right)^{-3} \neq \frac{1}{\frac{dy^2}{d^2x}}$$ where the left hand side isn't even defined. I know that the concept of differential can be formalized, for example as infinitesimal variables in nonstandard analysis, and that this, in a sense, explains why these formal manipulations work. I know the concept of second degree differential exists, that's why I suspect that the reason they don't work in the case of higher degree derivatives is because the notation must be ""wrong"".  My question is: Is it possible to modify Leibniz's notation for second and higher order derivatives, so that the corresponding ""differentiation rules"" can be obtained by formal algebraic manipulation of the differentials $dx$, $d^2x$, etc. involved?","Lebniz's notation for ordinary derivatives as quotients of differentials is a convenient abuse of notation, since it lets you express things like the chain rule and the derivative of the inverse function in a suggestive form: $$\frac{dz}{dx} = \frac{dy}{dx} \cdot \frac{dz}{dy}$$ $$\frac{dx}{dy} = \frac{1}{\frac{dy}{dx}}$$ where $x,y,z$ are interdependent variables. This approach completely breaks down with second and higher order derivatives, since for example the second derivative of the inverse is $$\frac{d^2x}{dy^2} =-\frac{d^2y}{dx^2} \left(\frac{dy}{dx} \right)^{-3} \neq \frac{1}{\frac{dy^2}{d^2x}}$$ where the left hand side isn't even defined. I know that the concept of differential can be formalized, for example as infinitesimal variables in nonstandard analysis, and that this, in a sense, explains why these formal manipulations work. I know the concept of second degree differential exists, that's why I suspect that the reason they don't work in the case of higher degree derivatives is because the notation must be ""wrong"".  My question is: Is it possible to modify Leibniz's notation for second and higher order derivatives, so that the corresponding ""differentiation rules"" can be obtained by formal algebraic manipulation of the differentials $dx$, $d^2x$, etc. involved?",,"['calculus', 'derivatives', 'notation']"
9,"Prove that there exists $\delta>0$ such that for all $(x,y)\in S$, if $\|(x,y)\|<\delta$, then $f(x,y)\le f(0,0)$.","Prove that there exists  such that for all , if , then .","\delta>0 (x,y)\in S \|(x,y)\|<\delta f(x,y)\le f(0,0)","I need help with this question: Let $f,g:\mathbb{R}^2\rightarrow\mathbb{R}$ be two $C^2$ functions, and let $S=$ { $(x,y)\in\mathbb{R}^2:g(x,y)=0$ }. Suppose that $g(0,0)=\frac{\partial g}{\partial x}(0,0)=0$ and $\frac{\partial g}{\partial y}(0,0)\neq0$ . Also assume that there exists $\lambda \in \mathbb{R}$ such that $\nabla f(0,0)=\lambda\nabla g(0,0)$ and $\frac{\partial^2f}{\partial x^2}(0,0)<\lambda\frac{\partial^2g}{\partial x^2}(0,0)$ . Prove that there exists $\delta>0$ such that for all $(x,y)\in S$ , if $\|(x,y)\|<\delta$ , then $f(x,y)\le f(0,0)$ . Please, I can't do it.","I need help with this question: Let be two functions, and let { }. Suppose that and . Also assume that there exists such that and . Prove that there exists such that for all , if , then . Please, I can't do it.","f,g:\mathbb{R}^2\rightarrow\mathbb{R} C^2 S= (x,y)\in\mathbb{R}^2:g(x,y)=0 g(0,0)=\frac{\partial g}{\partial x}(0,0)=0 \frac{\partial g}{\partial y}(0,0)\neq0 \lambda \in \mathbb{R} \nabla f(0,0)=\lambda\nabla g(0,0) \frac{\partial^2f}{\partial x^2}(0,0)<\lambda\frac{\partial^2g}{\partial x^2}(0,0) \delta>0 (x,y)\in S \|(x,y)\|<\delta f(x,y)\le f(0,0)","['calculus', 'real-analysis', 'derivatives', 'partial-derivative']"
10,Prove that $f′(x)=f′(0)f(x)$ derivatives,Prove that  derivatives,f′(x)=f′(0)f(x),"Let $f:I \to R$ be differrentiable on an open interval $I \subseteq R$ with $$f(a + b) = f(a)f(b) \quad \forall a, b \in R$$ Suppose that $f(0) = 1$ and that $f'(0)$ exists. Show that: $$f'(x) = f'(0)f(x) \quad \forall x \in R$$ So far, this is what I got: $$  f(x+0)= f(x)f(0)$$  $$f(x) = f(x)f(0)$$ For all $x$ , $f(0)$ has to be $1$. $$0=f(x)f(0) - f(x)$$ $$0 = f(x)(f(0) -1)$$ $f(x)=0$ for all $x$ or $f(0) = 1$. I'm stuck here. Could you please give a hint?","Let $f:I \to R$ be differrentiable on an open interval $I \subseteq R$ with $$f(a + b) = f(a)f(b) \quad \forall a, b \in R$$ Suppose that $f(0) = 1$ and that $f'(0)$ exists. Show that: $$f'(x) = f'(0)f(x) \quad \forall x \in R$$ So far, this is what I got: $$  f(x+0)= f(x)f(0)$$  $$f(x) = f(x)f(0)$$ For all $x$ , $f(0)$ has to be $1$. $$0=f(x)f(0) - f(x)$$ $$0 = f(x)(f(0) -1)$$ $f(x)=0$ for all $x$ or $f(0) = 1$. I'm stuck here. Could you please give a hint?",,"['calculus', 'derivatives', 'functional-equations']"
11,What's the $n$-th derivative of $\ln(\sin(x))$?,What's the -th derivative of ?,n \ln(\sin(x)),"I want to find the $n$-th derivative of $\ln(\sin x)$, i.e. $$ \frac{d^n\ln(\sin x)}{dx^n} $$ where $x\in (0,\pi/2)$ such that $\sin x>0$. To make the problem definitely, $x=\pi/4$ is assumed. In wolframalpha , we know that for $n=1,2,\cdots$, we have $1,-2,4,-16,80,-512,\cdots$. So, what's the general behavior of the derivative w.r.t. $n$?","I want to find the $n$-th derivative of $\ln(\sin x)$, i.e. $$ \frac{d^n\ln(\sin x)}{dx^n} $$ where $x\in (0,\pi/2)$ such that $\sin x>0$. To make the problem definitely, $x=\pi/4$ is assumed. In wolframalpha , we know that for $n=1,2,\cdots$, we have $1,-2,4,-16,80,-512,\cdots$. So, what's the general behavior of the derivative w.r.t. $n$?",,"['calculus', 'derivatives', 'closed-form']"
12,"""Mean value like"" problem.","""Mean value like"" problem.",,"Let $f:\mathbb{R} \longrightarrow \mathbb{R}$ be differentiable, take $a<a'<b<b'$. Prove that there exists $c<c'$ such that $$\frac{f(b)-f(a)}{b-a}=f'(c) \quad and \quad \frac{f(b')-f(a')}{b'-a'}=f'(c').$$ My first tries were connected with mean value because we can find such $c,c'$ but we don't konw if they satisfie required relation. We know though that they are in $(a',b)$. I ask for some hints .","Let $f:\mathbb{R} \longrightarrow \mathbb{R}$ be differentiable, take $a<a'<b<b'$. Prove that there exists $c<c'$ such that $$\frac{f(b)-f(a)}{b-a}=f'(c) \quad and \quad \frac{f(b')-f(a')}{b'-a'}=f'(c').$$ My first tries were connected with mean value because we can find such $c,c'$ but we don't konw if they satisfie required relation. We know though that they are in $(a',b)$. I ask for some hints .",,"['calculus', 'real-analysis', 'derivatives']"
13,Differentiate expression involving reciprocal of square roots.,Differentiate expression involving reciprocal of square roots.,,I need to differentiate $$5\over 2+\sqrt{1+3x}$$ I can get the answer from Wolfram Alpha but I'm trying to understand the working. Do I use the chain rule? My calculus is at the basic level.,I need to differentiate $$5\over 2+\sqrt{1+3x}$$ I can get the answer from Wolfram Alpha but I'm trying to understand the working. Do I use the chain rule? My calculus is at the basic level.,,"['calculus', 'derivatives']"
14,What is the formal name for the conformal laplacian?,What is the formal name for the conformal laplacian?,,"\begin{align} L=R-4\dfrac{n-1}{n-2}\nabla^k\nabla_k \end{align} What is the formal name for $L$? I have seen it referred to as the conformal laplacian, however I thought I once read $L$ with a formal name in another article, possibly named after some else?","\begin{align} L=R-4\dfrac{n-1}{n-2}\nabla^k\nabla_k \end{align} What is the formal name for $L$? I have seen it referred to as the conformal laplacian, however I thought I once read $L$ with a formal name in another article, possibly named after some else?",,"['differential-geometry', 'derivatives', 'riemannian-geometry', 'conformal-geometry', 'laplacian']"
15,How to solve the derivative of $b^x$ using the definition,How to solve the derivative of  using the definition,b^x,"I know that the derivative of $b^x$ is just $b^x \log{(b)}$, and I've seen it being derived using chain rule and such (not that I understand how it's done, I just learned about $e$ today so using the chain rule to derive $b^x$ is outside my scope as of now). In any case I was trying to find out how you could derive $b^x$ using the definition of the derivative, $[f(x+h) - f(x)]/h$. I couldn't find it on the internet so I was wondering if someone here could show me. Thanks in advance!","I know that the derivative of $b^x$ is just $b^x \log{(b)}$, and I've seen it being derived using chain rule and such (not that I understand how it's done, I just learned about $e$ today so using the chain rule to derive $b^x$ is outside my scope as of now). In any case I was trying to find out how you could derive $b^x$ using the definition of the derivative, $[f(x+h) - f(x)]/h$. I couldn't find it on the internet so I was wondering if someone here could show me. Thanks in advance!",,"['calculus', 'derivatives', 'logarithms', 'euler-mascheroni-constant']"
16,How to prove $f$ is Lipschitz continuous,How to prove  is Lipschitz continuous,f,"Let $U\subset \mathbb R^N$ be open and convex, and the function $f: U\to \mathbb R$ is differentiable in $U$ . I've got to show that: $f$ is Lipschitz continuous iff $\exists\: M>0$ such that $\|\nabla f(x)\|\leq M$ for all $x\in U$ . I understand that a function is Lipschitz continuous on a subset $E$ of $\mathbb R^N$ if for all $x, y\in E$ , $$|f(x)-f(y)|\le L|x-y|$$ for some $L>0$ . But how do I work with this definition to get to my result? I sense that I would have to use the fact the norm is a convex function and show that $$\|\nabla f(x)\| < \frac{|f(x)-f(y)|}{|x-y|}$$ but I'm not sure if this is the right approach.","Let be open and convex, and the function is differentiable in . I've got to show that: is Lipschitz continuous iff such that for all . I understand that a function is Lipschitz continuous on a subset of if for all , for some . But how do I work with this definition to get to my result? I sense that I would have to use the fact the norm is a convex function and show that but I'm not sure if this is the right approach.","U\subset \mathbb R^N f: U\to \mathbb R U f \exists\: M>0 \|\nabla f(x)\|\leq M x\in U E \mathbb R^N x, y\in E |f(x)-f(y)|\le L|x-y| L>0 \|\nabla f(x)\| < \frac{|f(x)-f(y)|}{|x-y|}","['real-analysis', 'derivatives', 'lipschitz-functions']"
17,Rolle's Theorem with roots,Rolle's Theorem with roots,,"Let $f : [a, b] \to \Bbb R$ be $n$ times differentiable and have $n+1$ distinct roots (i.e. solutions of $f(x) = 0$ ) in $[a,b]$ . Show that there is an $x \in [a, b]$ s. t. the $n^{\text{th}}$ derivative of $f$ has a root in $[a,b]$ . I know that you need to use Rolle's Theorem for this problem. Do I prove by induction? So for the base case $(n=1)$ then $f$ has $2$ distinct roots, then by Rolle's Theorem $f'(x)$ must have a root? How would I do the inductive case? Thanks","Let be times differentiable and have distinct roots (i.e. solutions of ) in . Show that there is an s. t. the derivative of has a root in . I know that you need to use Rolle's Theorem for this problem. Do I prove by induction? So for the base case then has distinct roots, then by Rolle's Theorem must have a root? How would I do the inductive case? Thanks","f : [a, b] \to \Bbb R n n+1 f(x) = 0 [a,b] x \in [a, b] n^{\text{th}} f [a,b] (n=1) f 2 f'(x)",['derivatives']
18,Proof that derivative of a function at a point is the slope of the tangent at the point,Proof that derivative of a function at a point is the slope of the tangent at the point,,Why is the derivative for a function at point A considered the slope of the tangent of the function at this point?,Why is the derivative for a function at point A considered the slope of the tangent of the function at this point?,,"['calculus', 'derivatives']"
19,Derivative of logistic loss function,Derivative of logistic loss function,,"I am using logistic in classification task. The task equivalents with find $\omega, b$ to minimize loss function: That means we will take derivative of L with respect to $\omega$ and $b$ (assume y and X are known). Could you help me develop that derivation . Thank you so much",I am using logistic in classification task. The task equivalents with find to minimize loss function: That means we will take derivative of L with respect to and (assume y and X are known). Could you help me develop that derivation . Thank you so much,"\omega, b \omega b","['linear-algebra', 'discrete-mathematics', 'derivatives', 'regression']"
20,"Prove that $\int_a^c f(t)dt - \int_c^b f(t)dt = f(c)(a+b-2c) $, for some $c\in(a,b)$","Prove that , for some","\int_a^c f(t)dt - \int_c^b f(t)dt = f(c)(a+b-2c)  c\in(a,b)","Let $f$ be a continuous on $[a,b]$ then prove that there exist some $c$ that lies in $(a,b)$ such that $$\int_a^cf(t)\,dt  - \int_c^b  f(t)\,dt  = f(c)(a+b-2c) $$ and hence prove that  $\int_a^c f(t)\,dt  - \int_c^b  f(t)\,dt  = n  f(c)(a+b-2c) $ where $n \ge0$ I have been able to prove the first part. but not for general $n$. I used rolles theorem to prove the first part. I dont know in which tag to put this question into.","Let $f$ be a continuous on $[a,b]$ then prove that there exist some $c$ that lies in $(a,b)$ such that $$\int_a^cf(t)\,dt  - \int_c^b  f(t)\,dt  = f(c)(a+b-2c) $$ and hence prove that  $\int_a^c f(t)\,dt  - \int_c^b  f(t)\,dt  = n  f(c)(a+b-2c) $ where $n \ge0$ I have been able to prove the first part. but not for general $n$. I used rolles theorem to prove the first part. I dont know in which tag to put this question into.",,"['calculus', 'integration', 'derivatives']"
21,About equivalent norms,About equivalent norms,,"Consider $E$ the space of the functions $f: [0,1] \to \mathbb{R}$ such that $f(0) = 0$ and $f$ satisfies a Lipschitz condition. We define two norms: $$\|f\| = \sup_{x \in [0,1]} |f(x)|$$ and $$\langle \langle f \rangle \rangle = \sup_{x \in ]0,1]} \frac{|f(x)|}{x}$$ The first part asks to prove that $\langle \langle \cdot \rangle \rangle $ is strictly finer than $\| \cdot \|$. This I managed to do. But I found the second part difficult. Let $\varphi \in E$, of class $C^1$, such that $\varphi '(0) = 0$. Let $F \subset E$ the set of functions $f$ such that $|f(x)| \leq |\varphi (x)|  \quad \forall x \in [0, 1]$ We have to prove that the induced metrics on $F$ are equivalent. My attempt : It should suffice to prove that $\langle \langle \cdot \rangle \rangle$ and $\| \cdot \|$ are equivalent. Because of the first part, we only have to find a constant $k$ such that $\langle \langle f \rangle \rangle \leq k \|f\| \quad \forall f \in F$. By the definition of $\varphi '(0)$, I get $\lim_{x \to 0} \frac{\varphi (x)}{x} = 0$ Joining this with the condition $|f(x)| \leq |\varphi (x)|$, I got $\lim_{x \to 0} \left|\frac{f(x)}{x}\right| = 0 $ Since $f$ and $\varphi$ satisfy Lipschitz conditions, exists $k_1, k_2 \in \mathbb{R}$ such that $|f(x)| \leq k_1 |x|$ and $|\varphi (x)| \leq k_2 |x|$, for every $x \in [0,1]$ But I can't join all this information. My intuition says that the constant we're looking for is related to the Lipschitz constant for $\varphi$, since it is fixed. Thanks in advance!","Consider $E$ the space of the functions $f: [0,1] \to \mathbb{R}$ such that $f(0) = 0$ and $f$ satisfies a Lipschitz condition. We define two norms: $$\|f\| = \sup_{x \in [0,1]} |f(x)|$$ and $$\langle \langle f \rangle \rangle = \sup_{x \in ]0,1]} \frac{|f(x)|}{x}$$ The first part asks to prove that $\langle \langle \cdot \rangle \rangle $ is strictly finer than $\| \cdot \|$. This I managed to do. But I found the second part difficult. Let $\varphi \in E$, of class $C^1$, such that $\varphi '(0) = 0$. Let $F \subset E$ the set of functions $f$ such that $|f(x)| \leq |\varphi (x)|  \quad \forall x \in [0, 1]$ We have to prove that the induced metrics on $F$ are equivalent. My attempt : It should suffice to prove that $\langle \langle \cdot \rangle \rangle$ and $\| \cdot \|$ are equivalent. Because of the first part, we only have to find a constant $k$ such that $\langle \langle f \rangle \rangle \leq k \|f\| \quad \forall f \in F$. By the definition of $\varphi '(0)$, I get $\lim_{x \to 0} \frac{\varphi (x)}{x} = 0$ Joining this with the condition $|f(x)| \leq |\varphi (x)|$, I got $\lim_{x \to 0} \left|\frac{f(x)}{x}\right| = 0 $ Since $f$ and $\varphi$ satisfy Lipschitz conditions, exists $k_1, k_2 \in \mathbb{R}$ such that $|f(x)| \leq k_1 |x|$ and $|\varphi (x)| \leq k_2 |x|$, for every $x \in [0,1]$ But I can't join all this information. My intuition says that the constant we're looking for is related to the Lipschitz constant for $\varphi$, since it is fixed. Thanks in advance!",,"['real-analysis', 'derivatives', 'metric-spaces']"
22,Convergence in $L^2$ of difference quotients to derivative of function in $H^1$,Convergence in  of difference quotients to derivative of function in,L^2 H^1,"Is it true that if $u\in H^1({\mathbb R})$, then $(u(x+h)-u(x))/h$ converges to $u'(x)$ in $L^2({\mathbb R})$, as $h\to 0$?  It's hard for me to get a handle on this, since $u'$ doesn't have to be continuous (so there's no uniform convergence, even on compacts, since $u$ does have to be continuous), but I can't seem to construct a counterexample either.","Is it true that if $u\in H^1({\mathbb R})$, then $(u(x+h)-u(x))/h$ converges to $u'(x)$ in $L^2({\mathbb R})$, as $h\to 0$?  It's hard for me to get a handle on this, since $u'$ doesn't have to be continuous (so there's no uniform convergence, even on compacts, since $u$ does have to be continuous), but I can't seem to construct a counterexample either.",,"['derivatives', 'sobolev-spaces']"
23,Proof of an inequality involving $e^x$.,Proof of an inequality involving .,e^x,"Prove that $e^{x-1} \geq x, $ for every $x$. I'm not allowed to use MVT or integrals, but IVT and derivatives are allowed. I tried to define a function $f(x)=e^{x-1}-x$ and then $\,f'(x)=e^{x-1}-1$ so the function has minimum in $x=1$ where $y=0$, thus the inequality holds. Is that good? Is there another way which does not involve derivatives? Thanks!","Prove that $e^{x-1} \geq x, $ for every $x$. I'm not allowed to use MVT or integrals, but IVT and derivatives are allowed. I tried to define a function $f(x)=e^{x-1}-x$ and then $\,f'(x)=e^{x-1}-1$ so the function has minimum in $x=1$ where $y=0$, thus the inequality holds. Is that good? Is there another way which does not involve derivatives? Thanks!",,"['calculus', 'inequality', 'derivatives']"
24,Differentiating under integral with bounded derivatives,Differentiating under integral with bounded derivatives,,"I remember there is a trick, where for certain conditions of a function $f:\mathbb{R}\rightarrow\mathbb{R}$, we can say that $$\frac{d}{dx}\int_X f(x)d\mu(x) = \int_X\frac{d}{dx}f(x)d\mu(x).$$ The condition has something to do with the derivative of $f$ being bounded, and the proof of the statement goes by dominated convergence theorem. What is the exact statement and proof?","I remember there is a trick, where for certain conditions of a function $f:\mathbb{R}\rightarrow\mathbb{R}$, we can say that $$\frac{d}{dx}\int_X f(x)d\mu(x) = \int_X\frac{d}{dx}f(x)d\mu(x).$$ The condition has something to do with the derivative of $f$ being bounded, and the proof of the statement goes by dominated convergence theorem. What is the exact statement and proof?",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
25,Differentiating under integral for convolution,Differentiating under integral for convolution,,"I have a function $f\in L^1(\mathbb{R}) $ and $g(x)=\dfrac{1}{2\sqrt{\pi t}}e^{-\frac{(at+x)^2}{4t}}$, where $a,t\in\mathbb{R}$, $t>0$. I want to show that $$\dfrac{d}{dx}\int_{-\infty}^\infty f(y)g(x-y)dy=\int_{-\infty}^\infty f(y)\dfrac{d}{dx}g(x-y)$$ Leibniz doesn't work since there's no continuity assumption on $f$. I'm thinking about using the dominated convergence theorem, but how would the proof go?","I have a function $f\in L^1(\mathbb{R}) $ and $g(x)=\dfrac{1}{2\sqrt{\pi t}}e^{-\frac{(at+x)^2}{4t}}$, where $a,t\in\mathbb{R}$, $t>0$. I want to show that $$\dfrac{d}{dx}\int_{-\infty}^\infty f(y)g(x-y)dy=\int_{-\infty}^\infty f(y)\dfrac{d}{dx}g(x-y)$$ Leibniz doesn't work since there's no continuity assumption on $f$. I'm thinking about using the dominated convergence theorem, but how would the proof go?",,"['integration', 'derivatives']"
26,"Find the equation of the tangent line to the curve at the given point. $y = 1+2x-x^3$ at $(1,2)$",Find the equation of the tangent line to the curve at the given point.  at,"y = 1+2x-x^3 (1,2)","I have the equation $y = 1+2x-x^3$ and the point $(1,2)$. When I work it out I come up with the derivative of $2-3x^2$. When I apply the point I come up with a slope of $-1$ and a tangent line of $y=4-x$. Can someone work it out and confirm my answer or show me where I am going wrong?","I have the equation $y = 1+2x-x^3$ and the point $(1,2)$. When I work it out I come up with the derivative of $2-3x^2$. When I apply the point I come up with a slope of $-1$ and a tangent line of $y=4-x$. Can someone work it out and confirm my answer or show me where I am going wrong?",,['derivatives']
27,Problem with differentiation as a concept.,Problem with differentiation as a concept.,,"I don't understand quiet good something here, for example if we want to find the derivative of the function $\displaystyle f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(h)}{h} $ and if we compute it from the function: $ f(x) = 12 + 7x $ We get that the derivative of $f(x)$ is equal to $$\lim_{h \to 0} \frac{7h}{h}$$ But I thought that we can't divide by zero (here we cancel 0 over 0), I'm I wrong or $\displaystyle \frac{0}{0}$ equals 1?","I don't understand quiet good something here, for example if we want to find the derivative of the function $\displaystyle f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(h)}{h} $ and if we compute it from the function: $ f(x) = 12 + 7x $ We get that the derivative of $f(x)$ is equal to $$\lim_{h \to 0} \frac{7h}{h}$$ But I thought that we can't divide by zero (here we cancel 0 over 0), I'm I wrong or $\displaystyle \frac{0}{0}$ equals 1?",,"['calculus', 'derivatives']"
28,Real variable lemma,Real variable lemma,,"Someone can help with the following lemma? Lemma: Let $\theta'(r)\geq0,\theta(r)>0$ and $\dfrac{\theta'(r)}{\theta(r)}$ decreasing for $r>0$. Then    $$\frac{\displaystyle\int^r_0\theta'(s)\,ds}{\displaystyle\int^r_0\theta(s)\,ds}$$    is decreasing. Thanks!","Someone can help with the following lemma? Lemma: Let $\theta'(r)\geq0,\theta(r)>0$ and $\dfrac{\theta'(r)}{\theta(r)}$ decreasing for $r>0$. Then    $$\frac{\displaystyle\int^r_0\theta'(s)\,ds}{\displaystyle\int^r_0\theta(s)\,ds}$$    is decreasing. Thanks!",,"['calculus', 'integration', 'derivatives']"
29,Proof identity of differential equation [closed],Proof identity of differential equation [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Q: $f''(x)$ continuous in $\mathbb{R}$ show that  $$ \lim_{h\to 0}\frac{f(x+h)+f(x-h)-2f(x)}{h^2}=f''(x)$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Q: $f''(x)$ continuous in $\mathbb{R}$ show that  $$ \lim_{h\to 0}\frac{f(x+h)+f(x-h)-2f(x)}{h^2}=f''(x)$$",,"['calculus', 'real-analysis', 'derivatives']"
30,Differentiation under the double integral sign,Differentiation under the double integral sign,,"Working on three-body dispersion forces I got the following quantity: $$\frac{\partial } {{\partial \lambda }}\int\limits_\lambda ^{\pi  - \lambda } {d\theta } \int\limits_\lambda ^{\pi  - \lambda } {d\phi f\left( {\theta ,\phi } \right)}  $$ where f is independent of $\lambda$. My question is: how can i take the derivative under the double integral sign? In one-dimensional case we have: $$\frac{d} {{dx}}\int\limits_{a\left( x \right)}^{b\left( x \right)} {f\left( {x,y} \right)} dy = f\left( {x,b\left( x \right)} \right)b'\left( x \right) - f\left( {x,a\left( x \right)} \right)a'\left( x \right) + \int\limits_{a\left( x \right)}^{b\left( x \right)} {f_x } \left( {x,y} \right)dy $$ Is there an analogous formula for the two-dimensional case?","Working on three-body dispersion forces I got the following quantity: $$\frac{\partial } {{\partial \lambda }}\int\limits_\lambda ^{\pi  - \lambda } {d\theta } \int\limits_\lambda ^{\pi  - \lambda } {d\phi f\left( {\theta ,\phi } \right)}  $$ where f is independent of $\lambda$. My question is: how can i take the derivative under the double integral sign? In one-dimensional case we have: $$\frac{d} {{dx}}\int\limits_{a\left( x \right)}^{b\left( x \right)} {f\left( {x,y} \right)} dy = f\left( {x,b\left( x \right)} \right)b'\left( x \right) - f\left( {x,a\left( x \right)} \right)a'\left( x \right) + \int\limits_{a\left( x \right)}^{b\left( x \right)} {f_x } \left( {x,y} \right)dy $$ Is there an analogous formula for the two-dimensional case?",,"['calculus', 'integration', 'derivatives']"
31,Find the stationary point of $y=(x-a)^p(x-b)^q$,Find the stationary point of,y=(x-a)^p(x-b)^q,"If a, b, p, and q are positive with $a<b$ find the x-coordinate of the stationary point of the curve $y=(x-a)^p(x-b)^q$ in the domain $a<x<b$. This is what I tried using the product rule: $\dfrac{dy}{dx}=(x-a)^pq(x-b)^{q-1}+(x-b)^qp(x-a)^{p-1}$ I am stuck here and I don't know if I even differentiated correctly. The answer is: $\dfrac{qa+pb}{p+q}$, but I have no idea how to get here. I know a and b will be stationary points but they are not in the domain.","If a, b, p, and q are positive with $a<b$ find the x-coordinate of the stationary point of the curve $y=(x-a)^p(x-b)^q$ in the domain $a<x<b$. This is what I tried using the product rule: $\dfrac{dy}{dx}=(x-a)^pq(x-b)^{q-1}+(x-b)^qp(x-a)^{p-1}$ I am stuck here and I don't know if I even differentiated correctly. The answer is: $\dfrac{qa+pb}{p+q}$, but I have no idea how to get here. I know a and b will be stationary points but they are not in the domain.",,"['calculus', 'derivatives']"
32,Pathological function needed,Pathological function needed,,"Give a differentiable function that has a positive derivative at $0$,   yet is not increasing on any open neighbourhood of $0$. I believe that the required function needs to have a derived function that is discontinuous at $0$ (ie. the required function needs to be not continuously differentiable at $0$). Any suggestion would be appreciated.","Give a differentiable function that has a positive derivative at $0$,   yet is not increasing on any open neighbourhood of $0$. I believe that the required function needs to have a derived function that is discontinuous at $0$ (ie. the required function needs to be not continuously differentiable at $0$). Any suggestion would be appreciated.",,"['calculus', 'real-analysis', 'derivatives']"
33,Finding an example which is Gâteaux-differentiable in one point but not continuous in this point,Finding an example which is Gâteaux-differentiable in one point but not continuous in this point,,"Find an example for a $f\colon X\to Y$ which is Gâteaux-differentiable in a point $x_0$ but not continuous in this point $x_0$. I am not good in finding examples but I thought of $$ f\colon\mathbb{R}\to\mathbb{R}, f(x):=\begin{cases}0, & x\leq 1\\1, & x>1\end{cases} $$ I guess on the one hand this function is not continuous in $x_0=1$ but the Gâteaux derivative $$ \eta=Df(1)[h]=\lim\limits_{t\to 0}\frac{f(1+th)-f(1)}{t}=0 $$ exists to my opinion. Is this an appropriate example? Edit Isn't the function $$ F\colon\mathbb{R}\to\mathbb{R}, F(x):=\begin{cases}1, & x\neq z\\0, & x=z\end{cases} $$ Gâteaux-differentiable in $z$ and not continuous in $z$?","Find an example for a $f\colon X\to Y$ which is Gâteaux-differentiable in a point $x_0$ but not continuous in this point $x_0$. I am not good in finding examples but I thought of $$ f\colon\mathbb{R}\to\mathbb{R}, f(x):=\begin{cases}0, & x\leq 1\\1, & x>1\end{cases} $$ I guess on the one hand this function is not continuous in $x_0=1$ but the Gâteaux derivative $$ \eta=Df(1)[h]=\lim\limits_{t\to 0}\frac{f(1+th)-f(1)}{t}=0 $$ exists to my opinion. Is this an appropriate example? Edit Isn't the function $$ F\colon\mathbb{R}\to\mathbb{R}, F(x):=\begin{cases}1, & x\neq z\\0, & x=z\end{cases} $$ Gâteaux-differentiable in $z$ and not continuous in $z$?",,[]
34,How to find the following derivative?,How to find the following derivative?,,"Here is the complete problem but (c) is the part that I am having problems with, I have already solved (a) and (b): (a) If $t=\tan\left(\frac{x}{2}\right)$,$-\pi<x<\pi$, sketch a right triangle or use trigonometric identities to show that $$\cos\left(\frac{x}{2}\right)=\frac{1}{\sqrt{1+t^2}}\qquad\sin\left(\frac{x}{2}\right)=\frac{t}{\sqrt{1+t^2}}$$ (b) Show that $$\cos x=\frac{1-t^2}{1+t^2}\qquad\sin x=\frac{2t}{1+t^2}$$ (c) Show that $$dx = \frac{2}{1+t^2}dt$$ I am aware that it is relatively simple to obtain the correct result by $x = 2\arctan t$ and if $y = \arctan x$ then  $\frac{dy}{dx} = \frac{1}{1+x^2}$ so we obtain the result above. My problem is that I attempted to do it by $x = \arcsin \frac {2t}{1+t^2}$ and knowing that if $y = \arcsin x$ then $\frac{dy}{dx} = \frac{1}{\sqrt{1 - x^2}}$ I obtained the following result $$ dx = -\frac{2}{1+t^2}dt$$ I have reviewed my solution several times and I cannot find an algebraic mistake. In the case that the result is algebraically correct, I am speculating that both results are equivalent because of something that has to do with the restrictions imposed when defining inverse trigonometric functions but I am lost and cannot figure out the connection. EDIT I understand the mistake now, the restriction of $x \in (-\pi/2,\pi/2)$ needs to be made as dictated by the definition of $\arcsin$ and then $t=\tan(x/2)\in[\tan(-\pi/4),\tan(\pi/4)]=[-1,1]$. Now, my question is the following: when attempting to find $dx$ by $x = 2\arctan t$ we impose the restriction of $x \in (-\pi,\pi)$  because $\arctan t \in (-\pi/2,\pi/2)$ but, doesn't this contradict the restrictions we imposed on $x$ when finding $dx$ by $x = \arcsin \frac {2t}{1+t^2}$?","Here is the complete problem but (c) is the part that I am having problems with, I have already solved (a) and (b): (a) If $t=\tan\left(\frac{x}{2}\right)$,$-\pi<x<\pi$, sketch a right triangle or use trigonometric identities to show that $$\cos\left(\frac{x}{2}\right)=\frac{1}{\sqrt{1+t^2}}\qquad\sin\left(\frac{x}{2}\right)=\frac{t}{\sqrt{1+t^2}}$$ (b) Show that $$\cos x=\frac{1-t^2}{1+t^2}\qquad\sin x=\frac{2t}{1+t^2}$$ (c) Show that $$dx = \frac{2}{1+t^2}dt$$ I am aware that it is relatively simple to obtain the correct result by $x = 2\arctan t$ and if $y = \arctan x$ then  $\frac{dy}{dx} = \frac{1}{1+x^2}$ so we obtain the result above. My problem is that I attempted to do it by $x = \arcsin \frac {2t}{1+t^2}$ and knowing that if $y = \arcsin x$ then $\frac{dy}{dx} = \frac{1}{\sqrt{1 - x^2}}$ I obtained the following result $$ dx = -\frac{2}{1+t^2}dt$$ I have reviewed my solution several times and I cannot find an algebraic mistake. In the case that the result is algebraically correct, I am speculating that both results are equivalent because of something that has to do with the restrictions imposed when defining inverse trigonometric functions but I am lost and cannot figure out the connection. EDIT I understand the mistake now, the restriction of $x \in (-\pi/2,\pi/2)$ needs to be made as dictated by the definition of $\arcsin$ and then $t=\tan(x/2)\in[\tan(-\pi/4),\tan(\pi/4)]=[-1,1]$. Now, my question is the following: when attempting to find $dx$ by $x = 2\arctan t$ we impose the restriction of $x \in (-\pi,\pi)$  because $\arctan t \in (-\pi/2,\pi/2)$ but, doesn't this contradict the restrictions we imposed on $x$ when finding $dx$ by $x = \arcsin \frac {2t}{1+t^2}$?",,"['calculus', 'derivatives']"
35,Slope of a nonlinear curve at a single point,Slope of a nonlinear curve at a single point,,"This part of my microeconomics lesson plan has me baffled. Consider for example the nonlinear continuous and differentiable function Y = f(X) = X 2 + 4. Suppose we want to know its slope at the point (X, Y) = (3, 13). The derivative of this function is f ’(X) = 2X , which takes on the value 6 when X = 3. Hence, the slope of this function is 6 at the point (3, 13). pdf of Source I've bold-faced the parts which I believe are disrupting my understanding, specifically. For one thing, I've always understood slope to be between two points . I can conceptualize the slope at a single point on a curve (a stick against a ball would accomplish the same - only one solution for each point) but not what it's purpose would be. The other thing confusing me is where the derivative comes from. Each equation I've found (I did internet research on this) seems to pull a derivative from thin air, unrelated to the equation itself. I've gotten as far as trigonometry in my schooling and each internet result I have found leads me into the realm of calculus. But that class wasn't a prerequisite for microeconomics! Can anyone shed some light on this for me? I know I'm supposed to be finding the tangent line on that point, but is there a more math-work way of accomplishing this than the visual method of plotting the graph and using a ruler?","This part of my microeconomics lesson plan has me baffled. Consider for example the nonlinear continuous and differentiable function Y = f(X) = X 2 + 4. Suppose we want to know its slope at the point (X, Y) = (3, 13). The derivative of this function is f ’(X) = 2X , which takes on the value 6 when X = 3. Hence, the slope of this function is 6 at the point (3, 13). pdf of Source I've bold-faced the parts which I believe are disrupting my understanding, specifically. For one thing, I've always understood slope to be between two points . I can conceptualize the slope at a single point on a curve (a stick against a ball would accomplish the same - only one solution for each point) but not what it's purpose would be. The other thing confusing me is where the derivative comes from. Each equation I've found (I did internet research on this) seems to pull a derivative from thin air, unrelated to the equation itself. I've gotten as far as trigonometry in my schooling and each internet result I have found leads me into the realm of calculus. But that class wasn't a prerequisite for microeconomics! Can anyone shed some light on this for me? I know I'm supposed to be finding the tangent line on that point, but is there a more math-work way of accomplishing this than the visual method of plotting the graph and using a ruler?",,"['calculus', 'terminology', 'derivatives', 'graphing-functions', 'economics']"
36,Derivative of Neural Network function,Derivative of Neural Network function,,"I would like to code this neural net activation function, using the C language: $$f(x) = 1.7159 \tanh( \frac{2}{3} x )$$ and I will also need to code its derivative. I've read that the derivative of $\tanh(x)$ is $\operatorname{sech}^2(x)$, but since C doesn't have a hyperbolic secant function I will need to use $\cosh$, i.e. the derivative of $\tanh(x)$ is $1\over \cosh^2(x)$, I think. Since my knowledge of calculus is very rusty, my best attempt for the derivative of the above function is: $$1 \over \cosh^2(\frac{2}{3}x)$$ Is this correct?","I would like to code this neural net activation function, using the C language: $$f(x) = 1.7159 \tanh( \frac{2}{3} x )$$ and I will also need to code its derivative. I've read that the derivative of $\tanh(x)$ is $\operatorname{sech}^2(x)$, but since C doesn't have a hyperbolic secant function I will need to use $\cosh$, i.e. the derivative of $\tanh(x)$ is $1\over \cosh^2(x)$, I think. Since my knowledge of calculus is very rusty, my best attempt for the derivative of the above function is: $$1 \over \cosh^2(\frac{2}{3}x)$$ Is this correct?",,"['calculus', 'derivatives', 'neural-networks']"
37,"Given a cubic function, and its quadratic derivative- can I recover the cubic from quadratic?","Given a cubic function, and its quadratic derivative- can I recover the cubic from quadratic?",,"Background: I'm trying to learn how to work with cubic and quadratic bezier splines for various drawing libraries, and working through how to approximate a cubic spline with a quadratic spline. It's occured to me that it should be possible to approximate any continuous parametric function with an arbitrary series of interconnected quadratic splines, as long as you have a suitable fitness function for each section. It has further occured to me that the derivative of any cubic spline is expressible as a quadratic spline. So now I am curious. if I have an abitrary function f(t); its derivative f'(t); and I can approximate f'(t) with a series of quadratic splines... can I have some way of assuming those quadratics as derivatives of cubics, and directly compute a series of cubics that then approximates f(t) from the quadratics? is there something wrong with my reasoning here? (edited for brevity) (clarification) I mentioned I was working through how to approximate a cubic spline with a quadratic spline. Let us assume for the purposes of this question that I have already solved this problem, and I have a procedure P which takes any arbitrary function f(t) (which could be a cubic spline, or anything else) with the constraint that we will only approximate over intervals where f'(t) is continuous. The question therefore is not how do I approximate a cubic with a quadric but, given that I have procedure P, if I apply procedure P to f'(t) to compute a series of quadratic splines, can I use that information to recover a cubic spline approximation of f(t). From the answers, if I am understanding corrrectly,  I think that I can so long as I can still use f(t) to recover this ""integral constant"". This is an acceptable solution since what I want is something like a procedure P2 which can take a f(t) and give me a sequence of cubic splines which approximate it. edit: further, it seems that if we have f(t) and its f'(t) and its f''(t) we could even do a straight forward linear approximation of f''(t) and recover a cubic spline approximation of f(t) by solving for the integral constants in f'(t) and f(t). neat.","Background: I'm trying to learn how to work with cubic and quadratic bezier splines for various drawing libraries, and working through how to approximate a cubic spline with a quadratic spline. It's occured to me that it should be possible to approximate any continuous parametric function with an arbitrary series of interconnected quadratic splines, as long as you have a suitable fitness function for each section. It has further occured to me that the derivative of any cubic spline is expressible as a quadratic spline. So now I am curious. if I have an abitrary function f(t); its derivative f'(t); and I can approximate f'(t) with a series of quadratic splines... can I have some way of assuming those quadratics as derivatives of cubics, and directly compute a series of cubics that then approximates f(t) from the quadratics? is there something wrong with my reasoning here? (edited for brevity) (clarification) I mentioned I was working through how to approximate a cubic spline with a quadratic spline. Let us assume for the purposes of this question that I have already solved this problem, and I have a procedure P which takes any arbitrary function f(t) (which could be a cubic spline, or anything else) with the constraint that we will only approximate over intervals where f'(t) is continuous. The question therefore is not how do I approximate a cubic with a quadric but, given that I have procedure P, if I apply procedure P to f'(t) to compute a series of quadratic splines, can I use that information to recover a cubic spline approximation of f(t). From the answers, if I am understanding corrrectly,  I think that I can so long as I can still use f(t) to recover this ""integral constant"". This is an acceptable solution since what I want is something like a procedure P2 which can take a f(t) and give me a sequence of cubic splines which approximate it. edit: further, it seems that if we have f(t) and its f'(t) and its f''(t) we could even do a straight forward linear approximation of f''(t) and recover a cubic spline approximation of f(t) by solving for the integral constants in f'(t) and f(t). neat.",,"['numerical-methods', 'derivatives', 'quadratics', 'spline']"
38,Derivative of exponential of sum of non-commuting matrices,Derivative of exponential of sum of non-commuting matrices,,"Given two square real $m \times m$ matrices $A$ and $B$ , which will generally not commute, and given the function $$f(x)=e^{A+Bx}$$ Where $x$ is a real variable, is there an expression for the derivative $f'(x)$ ? Due to the fact that $A$ and $B$ may not commute, I can't write $e^{A+Bx}=e^A e^{Bx}$ , therefore it seems to me that the usual way to prove the expression of the derivative of the exponential for real functions does not extend to this case.","Given two square real matrices and , which will generally not commute, and given the function Where is a real variable, is there an expression for the derivative ? Due to the fact that and may not commute, I can't write , therefore it seems to me that the usual way to prove the expression of the derivative of the exponential for real functions does not extend to this case.",m \times m A B f(x)=e^{A+Bx} x f'(x) A B e^{A+Bx}=e^A e^{Bx},"['linear-algebra', 'derivatives', 'operator-theory']"
39,Evaluate $\int_0^1 \cos^{-1} x\ dx$ by first finding $\frac{d}{dx}(x\cos^{-1} x)$,Evaluate  by first finding,\int_0^1 \cos^{-1} x\ dx \frac{d}{dx}(x\cos^{-1} x),"Question Evaluate $$\int_0^1 \cos^{-1} x\ dx$$ by first finding the value of $$\frac{d}{dx}(x\cos^{-1} x).$$ My Working As the question said to evaluate $$\frac{d}{dx}(x\cos^{-1} x),$$ I used the product rule to differentiate. We first have to let $u=x$ and $v=\cos^{-1} x$ , so \begin{align} & \quad\frac{d}{dx}(x\cos^{-1} x)\\ &=u^\prime v+v^\prime u\\ &=1\cdot \cos^{-1}x + x\cdot\frac{-1}{\sqrt{1-x^2}}\\ &=\cos^{-1}x-\frac{x}{\sqrt{1-x^2}} \end{align} Unfortunately, after that, I have no idea about how to proceed, but I think that we should go somewhere from $$\int\left[\frac{d}{dx}(x\cos^{-1}x)\right]\ dx+\int\frac{x}{\sqrt{1-x^2}}\ dx=\int \cos^{-1}x???$$ Thank you for your help!","Question Evaluate by first finding the value of My Working As the question said to evaluate I used the product rule to differentiate. We first have to let and , so Unfortunately, after that, I have no idea about how to proceed, but I think that we should go somewhere from Thank you for your help!","\int_0^1 \cos^{-1} x\ dx \frac{d}{dx}(x\cos^{-1} x). \frac{d}{dx}(x\cos^{-1} x), u=x v=\cos^{-1} x \begin{align}
& \quad\frac{d}{dx}(x\cos^{-1} x)\\
&=u^\prime v+v^\prime u\\
&=1\cdot \cos^{-1}x + x\cdot\frac{-1}{\sqrt{1-x^2}}\\
&=\cos^{-1}x-\frac{x}{\sqrt{1-x^2}}
\end{align} \int\left[\frac{d}{dx}(x\cos^{-1}x)\right]\ dx+\int\frac{x}{\sqrt{1-x^2}}\ dx=\int \cos^{-1}x???","['integration', 'derivatives', 'trigonometric-integrals']"
40,"Spivak, Ch. 20, Problem 9d: Understanding Solution Manual Proof","Spivak, Ch. 20, Problem 9d: Understanding Solution Manual Proof",,"The following problem is from Chapter 20 of Spivak's Calculus , ""Approximation by Polynomial Functions"". My question is about item $(d)$ , and I have previously asked a question about the comment at the end of item $(c)$ . (a) Problem $7(i)$ amounts to the equation $$P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g}$$ Give a more direct proof by writing $$f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1}$$ $$g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2}$$ and using the obvious fact about $R_{n,a,f}+R_{n,a,g}$ . (b) Similarly, Problem $7(ii)$ could be used to show that $$P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n$$ where $[P]_n$ denotes the truncation of $P$ to degree $n$ , the sum of all terms of $P$ of degree $\leq n$ [with $P$ written as a polynomial in $x-a$ ]. Again, give a more direct proof, using the obvious facts about products involving terms of the form $R_n$ . (c) Prove that if $p$ and $q$ are polynomials in $x-a$ and $\lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0$ then $$p(q(x)+R(x))=p(q(x))+\bar{R}(x)$$ where $$\lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0$$ Also note that if $p$ is a polynomial in $x-a$ having only terms of degree $>n$ , and $q$ is a polynomial in $x-a$ whose constant term is $0$ , then all terms of $p(q(x-a))$ are of degree $>n$ . (d) If $a=0$ and $b=g(a)=0$ , then $$P_{n,a,f\circ g}=[P_{n,b,f}\circ P_{n,a,g}]_n$$ Here is what the solution manual says Writing $$f(x)=P_{n,0,f}(x)+R_{n,0,f}(x)$$ $$g(x)=P_{n,0,g}(x)+R_{n,0,g}(x)$$ we have $$(f\circ g)(x)=P_{n,0,f}(P_{n,0,g}(x)+R_{n,0,g}(x))+R_{n,0,f}(g(x))$$ $$=A+B$$ Part $(c)$ shows that $$A=P_{n,0,f}(P_{n,0,g}(x))+\bar{R}(x)$$ where $$\lim\limits_{x\to a} \frac{\bar{R}(x)}{(x-a)^n}=0\tag{3}$$ and the remark added at the end of $(c)$ shows that $$\lim\limits_{x\to a}  \frac{B}{(x-a)^n}=0\tag{4}$$ Then, applying $(c)$ once again, we have $$(f\circ g)(x)=(P_{n,0,f}\circ P_{n,0,g})(x)+\bar{\bar{R}}(x)\tag{5}$$ where $\lim\limits_{x\to a} \frac{\bar{\bar{R}}(x)}{(x-a)^n}=0$ . It follows, just as in part $(b)$ , that $P_{n,0,f\circ g}=[P_{n,0,f}\circ  P_{n,0,g}]_n$ Everything up to $(3)$ is fine. How do we obtain $(4)$ ? My attempt at understanding it is: Since $R_{n,0,f}$ is polynomial in $x-a=x$ , composed of a single term of degree $n+1$ , and $g(x-a)=g(x)$ is such that the constant term in its Taylor polynomial is zero ( $g(a)=0$ by assumption), then as per the comment at the end of $(c)$ all of the terms in $R_{n,0,f}(g(x))$ are of degree $>n$ . Hence $$\lim\limits_{x\to a} \frac{R_{n,0,f}(g(x))}{(x-a)^n}=0$$ My other question is: how exactly is part $(c)$ applied again to reach $(5)$ ? Ie, in the context of part $(c)$ , when are the polynomials $p$ and $q$ here in part $(d)$ ? EDIT: Clarifications required by the bounty The solution manual sets $a=b=g(a)=0$ . I'd like to see a proof of $(d)$ that does not require $a=0$ or $b=0$ , only $b=g(a)$ . I will write an answer specifying my current understanding of the problem and solution.","The following problem is from Chapter 20 of Spivak's Calculus , ""Approximation by Polynomial Functions"". My question is about item , and I have previously asked a question about the comment at the end of item . (a) Problem amounts to the equation Give a more direct proof by writing and using the obvious fact about . (b) Similarly, Problem could be used to show that where denotes the truncation of to degree , the sum of all terms of of degree [with written as a polynomial in ]. Again, give a more direct proof, using the obvious facts about products involving terms of the form . (c) Prove that if and are polynomials in and then where Also note that if is a polynomial in having only terms of degree , and is a polynomial in whose constant term is , then all terms of are of degree . (d) If and , then Here is what the solution manual says Writing we have Part shows that where and the remark added at the end of shows that Then, applying once again, we have where . It follows, just as in part , that Everything up to is fine. How do we obtain ? My attempt at understanding it is: Since is polynomial in , composed of a single term of degree , and is such that the constant term in its Taylor polynomial is zero ( by assumption), then as per the comment at the end of all of the terms in are of degree . Hence My other question is: how exactly is part applied again to reach ? Ie, in the context of part , when are the polynomials and here in part ? EDIT: Clarifications required by the bounty The solution manual sets . I'd like to see a proof of that does not require or , only . I will write an answer specifying my current understanding of the problem and solution.","(d) (c) 7(i) P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g} f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1} g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2} R_{n,a,f}+R_{n,a,g} 7(ii) P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n [P]_n P n P \leq n P x-a R_n p q x-a \lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0 p(q(x)+R(x))=p(q(x))+\bar{R}(x) \lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0 p x-a >n q x-a 0 p(q(x-a)) >n a=0 b=g(a)=0 P_{n,a,f\circ g}=[P_{n,b,f}\circ P_{n,a,g}]_n f(x)=P_{n,0,f}(x)+R_{n,0,f}(x) g(x)=P_{n,0,g}(x)+R_{n,0,g}(x) (f\circ g)(x)=P_{n,0,f}(P_{n,0,g}(x)+R_{n,0,g}(x))+R_{n,0,f}(g(x)) =A+B (c) A=P_{n,0,f}(P_{n,0,g}(x))+\bar{R}(x) \lim\limits_{x\to a} \frac{\bar{R}(x)}{(x-a)^n}=0\tag{3} (c) \lim\limits_{x\to a}
 \frac{B}{(x-a)^n}=0\tag{4} (c) (f\circ g)(x)=(P_{n,0,f}\circ P_{n,0,g})(x)+\bar{\bar{R}}(x)\tag{5} \lim\limits_{x\to a} \frac{\bar{\bar{R}}(x)}{(x-a)^n}=0 (b) P_{n,0,f\circ g}=[P_{n,0,f}\circ
 P_{n,0,g}]_n (3) (4) R_{n,0,f} x-a=x n+1 g(x-a)=g(x) g(a)=0 (c) R_{n,0,f}(g(x)) >n \lim\limits_{x\to a} \frac{R_{n,0,f}(g(x))}{(x-a)^n}=0 (c) (5) (c) p q (d) a=b=g(a)=0 (d) a=0 b=0 b=g(a)","['calculus', 'integration', 'derivatives', 'proof-explanation', 'taylor-expansion']"
41,Have I evaluated this question correctly$?$,Have I evaluated this question correctly,?,"If $y=f(e^{-x})$ and $f'(x)=\ln x$ then $\frac{dy}{dx}$ at $x=\ln2$ is My work: I calculated $f(x)=x\ln x-x+C$ Now, $y=f(e^{-x})=e^{-x}\cdot\ln(e^{-x})-e^{-x}+C=-e^{-x}\cdot x-e^{-x}+C$ Then $$\frac{dy}{dx}=e^{-x}\cdot x$$ Now put $x=\ln2$ $$\frac{dy}{dx}|_{x=\ln2}=e^{-\ln2}\cdot\ln2$$ Now let $e^{-\ln2}=a$$\implies$ $$\ln(e^{-\ln2})=\ln a$$ or $$\ln2^{-1}=\ln a$$ or $$a=e^{-\ln2}=\frac12$$ Now doing $e^{-x}\cdot x=\frac{\ln2}{2}$ But the correct answer given is $\ln\sqrt{2}$ Where did I go wrong $?$ Any help is greatly appreciated.","If and then at is My work: I calculated Now, Then Now put Now let or or Now doing But the correct answer given is Where did I go wrong Any help is greatly appreciated.",y=f(e^{-x}) f'(x)=\ln x \frac{dy}{dx} x=\ln2 f(x)=x\ln x-x+C y=f(e^{-x})=e^{-x}\cdot\ln(e^{-x})-e^{-x}+C=-e^{-x}\cdot x-e^{-x}+C \frac{dy}{dx}=e^{-x}\cdot x x=\ln2 \frac{dy}{dx}|_{x=\ln2}=e^{-\ln2}\cdot\ln2 e^{-\ln2}=a\implies \ln(e^{-\ln2})=\ln a \ln2^{-1}=\ln a a=e^{-\ln2}=\frac12 e^{-x}\cdot x=\frac{\ln2}{2} \ln\sqrt{2} ?,"['real-analysis', 'calculus', 'integration', 'derivatives', 'solution-verification']"
42,Partial derivative of a recursive function,Partial derivative of a recursive function,,"There are two functions: $$f(x,y):\mathbb{R}^2\to \mathbb{R}$$ and $$g(z):\mathbb{R}\to \mathbb{R}.$$ Both are differentiable. I have a function $G(x)=g(f(x,G(x)))$ . I want to take the derivative with respect to $x$ . Is this possible? If so, how should I go about this? Thank you!","There are two functions: and Both are differentiable. I have a function . I want to take the derivative with respect to . Is this possible? If so, how should I go about this? Thank you!","f(x,y):\mathbb{R}^2\to \mathbb{R} g(z):\mathbb{R}\to \mathbb{R}. G(x)=g(f(x,G(x))) x","['calculus', 'derivatives', 'partial-derivative']"
43,Why is differentiation of $x^2$ equal to $2x$ and not $2x + 1$? [closed],Why is differentiation of  equal to  and not ? [closed],x^2 2x 2x + 1,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Tbh, I know very little about differentiation and so this should most probably be a dumb question, but I was just trying out a few examples and saw that $d/dx$ of $x^2$ actually appears to be $2x + 1$ and that of $x^3$ appears to be $3x^2 + 3x + 3$ (for an increase of $1$ ). Then why do we use only $2x$ and $3x^2$ instead? Is this for simplification? Thanks. Examples: $2^2 \rightarrow  3^2$ is actually an increase of $7 (2 * 2 + 1)$ . $3^3 \rightarrow 4^3$ is actually an increase of $37 (3 * 3^2 + 3 * 3 + 1)$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Tbh, I know very little about differentiation and so this should most probably be a dumb question, but I was just trying out a few examples and saw that of actually appears to be and that of appears to be (for an increase of ). Then why do we use only and instead? Is this for simplification? Thanks. Examples: is actually an increase of . is actually an increase of",d/dx x^2 2x + 1 x^3 3x^2 + 3x + 3 1 2x 3x^2 2^2 \rightarrow  3^2 7 (2 * 2 + 1) 3^3 \rightarrow 4^3 37 (3 * 3^2 + 3 * 3 + 1),"['calculus', 'derivatives']"
44,Alternatives For Numerical Differentiation,Alternatives For Numerical Differentiation,,"For a long time, the following point always confused me: In High School Calculus, we are told that almost all functions we encounter are are analytically differentiable  (i.e. have derivatives) but not all functions are analytically integrable (i.e. have integrals) - if this is true, then why do we need numerical differentiation? Then, I found out that numerical differentiation is used in instances when we do not have the actual equation of the ""function"", but instead only have observed data points from this ""function"" instead: Thus, it seems as Numerical Differentiation is providing us with a way to ""interpolate derivatives"" at arbitrary points over an observed range. This leads me to my next point: it seems that there are many instances in statistics where we are faced with a very similar problem : for example, we are often given a set of data points and are sometimes interested in interpolation, or fitting a smoothed function through these points: As far as I know, this can be done in many different ways, such as: Polynomial Regression (Smoothed) Splines (Smoothed) Kernel Density Estimation By fitting a smoothed function to the data (as opposed to a piecewise function), this allows us to obtain an ""analytical and closed form"" function that passes through the data (we can also measure the quality of how well this function fits the data) - and then we should be able to differentiate (i.e. evaluate derivatives) this function at any point that we desire. For instance, provided if we have sufficient reasons to believe that the regression models below fit the data well - could we not just evaluate derivatives (e.g. first derivative, second derivative) of these regression models and use them as surrogates for numerical differentiation? My Question: Is fitting a statistical model a mathematically valid alternative to numerical differentiation? The only downside to this I can think of is that this approach requires you to first fit a statistical model to the observed data (a source of error and uncertainty) and then differentiate this fitted statistical model (another source of error and uncertainty) - effectively compounding the uncertainties through error propagation. I am not sure, but perhaps standard numerical differentiation techniques (e.g. Backwards Differencing, Forward Differencing ) have less of a chance of invoking such error prorogation. Can someone please comment on this? Thanks!","For a long time, the following point always confused me: In High School Calculus, we are told that almost all functions we encounter are are analytically differentiable  (i.e. have derivatives) but not all functions are analytically integrable (i.e. have integrals) - if this is true, then why do we need numerical differentiation? Then, I found out that numerical differentiation is used in instances when we do not have the actual equation of the ""function"", but instead only have observed data points from this ""function"" instead: Thus, it seems as Numerical Differentiation is providing us with a way to ""interpolate derivatives"" at arbitrary points over an observed range. This leads me to my next point: it seems that there are many instances in statistics where we are faced with a very similar problem : for example, we are often given a set of data points and are sometimes interested in interpolation, or fitting a smoothed function through these points: As far as I know, this can be done in many different ways, such as: Polynomial Regression (Smoothed) Splines (Smoothed) Kernel Density Estimation By fitting a smoothed function to the data (as opposed to a piecewise function), this allows us to obtain an ""analytical and closed form"" function that passes through the data (we can also measure the quality of how well this function fits the data) - and then we should be able to differentiate (i.e. evaluate derivatives) this function at any point that we desire. For instance, provided if we have sufficient reasons to believe that the regression models below fit the data well - could we not just evaluate derivatives (e.g. first derivative, second derivative) of these regression models and use them as surrogates for numerical differentiation? My Question: Is fitting a statistical model a mathematically valid alternative to numerical differentiation? The only downside to this I can think of is that this approach requires you to first fit a statistical model to the observed data (a source of error and uncertainty) and then differentiate this fitted statistical model (another source of error and uncertainty) - effectively compounding the uncertainties through error propagation. I am not sure, but perhaps standard numerical differentiation techniques (e.g. Backwards Differencing, Forward Differencing ) have less of a chance of invoking such error prorogation. Can someone please comment on this? Thanks!",,"['calculus', 'derivatives', 'numerical-methods']"
45,How to calculate a definite integral with a variable as an endpoint and get its derivative,How to calculate a definite integral with a variable as an endpoint and get its derivative,,$$F(x) = \int_{{3}}^{x^2} \frac{t^{{5}} + \sin(t)}{t^{{5}}} dt$$ $$Find\:F'(x)$$ I've been trying to think about how to do this question for hours and trying many different things. I would imagine there is a way to do it without taking the antiderivative as it seems a really complex task. If someone could point me in the right direction on this that would be great.,I've been trying to think about how to do this question for hours and trying many different things. I would imagine there is a way to do it without taking the antiderivative as it seems a really complex task. If someone could point me in the right direction on this that would be great.,F(x) = \int_{{3}}^{x^2} \frac{t^{{5}} + \sin(t)}{t^{{5}}} dt Find\:F'(x),"['calculus', 'integration', 'derivatives', 'definite-integrals']"
46,A twist to the definition of a derivative,A twist to the definition of a derivative,,"Two questions: Functions are connections between numbers. Derivatives are connections between functions. So is there something like connections between derivatives? What function does the following limit yield: $$\lim_{x \rightarrow a}  \frac{ f' (x)- f' (a)}{f(x)-f(a)}$$ (for example if we plug in $\sin(x),$ then we get $- \tan(x),$ and if we plug in $x^{2},$ then we get $\frac{1}{x} $ )",Two questions: Functions are connections between numbers. Derivatives are connections between functions. So is there something like connections between derivatives? What function does the following limit yield: (for example if we plug in then we get and if we plug in then we get ),"\lim_{x \rightarrow a}  \frac{ f' (x)- f' (a)}{f(x)-f(a)} \sin(x), - \tan(x), x^{2}, \frac{1}{x} ","['derivatives', 'definition']"
47,"Showing that $x^n-ax^{n-1}-bx-1$, for non-negative integers $n\geq3$, $a\geq1$, $b$, has no double roots","Showing that , for non-negative integers , , , has no double roots",x^n-ax^{n-1}-bx-1 n\geq3 a\geq1 b,"Let $f(x)=x^n-ax^{n-1}-bx-1$ be a polynomial, where $n, a, b$ are non-negative integers, with $n\geq 3$ and $a\geq 1$ . I would like to prove that this polynomial does not have any double root. (This is supported with a Mathematica routine for $(n,a,b)\in [3,10]\times [1,50]\times [0,50]$ ). In fact, I was able to show that the only possible double root must be a real negative number $x_0$ (actually a quadratic irrational number) which can happen only for the case $n$ odd. Of course, I tried to study the system $f(x)=f'(x)=0$ which lead to the quadratic equation $$ b(n-1)x^2+(ab-a(n-1)+n)x-a(n-1)=0. $$ (Indeed from $f(x)=0$ and $f'(x)=0$ , we obtain $x^{n-1}(x-a)=bx+1$ and $x^{n-2}(nx-(n-1)a)=b$ . It is easy to see that $f(x)$ does not have rational roots - rational root theorem - so we can combine $$ x^{n-1}=\frac{bx+1}{x-a}\ \mbox{and}\ x^{n-2}=\frac{b}{nx-(n-1)a} $$ to obtain the previous quadratic relation). Also it is possible to prove that such a double root must satisfy a relation like $$ ax^{n-1}+b(n-1)x+n=0. $$ So, I tried to use some arithmetic behaviour of power of quadratic irrationals of the form $x_0=(r+s\sqrt{\xi})$ , where $r,s\in \mathbb{Q}$ and $\xi$ is a non-square positive integer. However, without success. Of course, it is possible to impose conditions on $a$ and $b$ for which we do not have double roots (as for example, $b>a+2$ forces $f(-1)>0$ and so we have exactly two negative roots, counted with multiplicity. Here we used Descartes' sign rule). Thus, I would like to ask you guys for some suggestion. Thanks!","Let be a polynomial, where are non-negative integers, with and . I would like to prove that this polynomial does not have any double root. (This is supported with a Mathematica routine for ). In fact, I was able to show that the only possible double root must be a real negative number (actually a quadratic irrational number) which can happen only for the case odd. Of course, I tried to study the system which lead to the quadratic equation (Indeed from and , we obtain and . It is easy to see that does not have rational roots - rational root theorem - so we can combine to obtain the previous quadratic relation). Also it is possible to prove that such a double root must satisfy a relation like So, I tried to use some arithmetic behaviour of power of quadratic irrationals of the form , where and is a non-square positive integer. However, without success. Of course, it is possible to impose conditions on and for which we do not have double roots (as for example, forces and so we have exactly two negative roots, counted with multiplicity. Here we used Descartes' sign rule). Thus, I would like to ask you guys for some suggestion. Thanks!","f(x)=x^n-ax^{n-1}-bx-1 n, a, b n\geq 3 a\geq 1 (n,a,b)\in [3,10]\times [1,50]\times [0,50] x_0 n f(x)=f'(x)=0 
b(n-1)x^2+(ab-a(n-1)+n)x-a(n-1)=0.
 f(x)=0 f'(x)=0 x^{n-1}(x-a)=bx+1 x^{n-2}(nx-(n-1)a)=b f(x) 
x^{n-1}=\frac{bx+1}{x-a}\ \mbox{and}\ x^{n-2}=\frac{b}{nx-(n-1)a}
 
ax^{n-1}+b(n-1)x+n=0.
 x_0=(r+s\sqrt{\xi}) r,s\in \mathbb{Q} \xi a b b>a+2 f(-1)>0","['calculus', 'derivatives', 'polynomials', 'roots', 'quadratics']"
48,"Total differentiation, is this true: $D(Df(a))(a) = f$?","Total differentiation, is this true: ?",D(Df(a))(a) = f,"Let $f: \mathbb{R}^p \rightarrow \mathbb{R}^q$ be linear. We have proven that for $T$ linear it is: $$ D T(a) = T. $$ So it should imply that: $$ D(Df(a))(a) = f. $$ Right? This seems to be trivial, but I had a long discussion with a fellow student who keeps saying that it is $0$ and also, that considering such examples is absolutely irrelevant in mathematical praxis. So who is wrong...?","Let be linear. We have proven that for linear it is: So it should imply that: Right? This seems to be trivial, but I had a long discussion with a fellow student who keeps saying that it is and also, that considering such examples is absolutely irrelevant in mathematical praxis. So who is wrong...?","f: \mathbb{R}^p \rightarrow \mathbb{R}^q T 
D T(a) = T.
 
D(Df(a))(a) = f.
 0","['real-analysis', 'derivatives']"
49,Property for Indefinite Integral,Property for Indefinite Integral,,"Let $f$ be a continuous function on $[a, b]$ . Assume that there occurs a positive constant $T$ for which $$ |f(y)| \leq T \int_{a}^{y}|f(t)| d t $$ for every value $y$ in $[a, b]$ . Prove that the equality $f(y)=0$ holds for any $y$ in $[a, b]$ . What could be the major strategies and original intuition behind finding a contradiction if one assumes the existence of some non-zero value? I tried to incorporate the continuity of function and indefinite integral, but it seems no success in mathematical rigor.","Let be a continuous function on . Assume that there occurs a positive constant for which for every value in . Prove that the equality holds for any in . What could be the major strategies and original intuition behind finding a contradiction if one assumes the existence of some non-zero value? I tried to incorporate the continuity of function and indefinite integral, but it seems no success in mathematical rigor.","f [a, b] T 
|f(y)| \leq T \int_{a}^{y}|f(t)| d t
 y [a, b] f(y)=0 y [a, b]","['integration', 'derivatives', 'indefinite-integrals']"
50,Every Vertical Strip Has A Point With Positive Slope,Every Vertical Strip Has A Point With Positive Slope,,"Consider a differentiable function $f$ on $[x_0, x_1]$ . Suppose $f(x_0) < f(x_1)$ . It seems intuitively obvious that every vertical strip between $f(x_0)$ and $f(x_1)$ contains a point with positive slope. i.e. a point $x$ with $f'(x) > 0$ and $a < f(x) < b$ Let the lower bound of this vertical strip be $a$ . Let the upper bound of this vertical strip be $b$ . This seems like it should be a very common question, I believe it can be proved formally using the intermediate + mean value theorem. Does anyone have any idea how to prove this?","Consider a differentiable function on . Suppose . It seems intuitively obvious that every vertical strip between and contains a point with positive slope. i.e. a point with and Let the lower bound of this vertical strip be . Let the upper bound of this vertical strip be . This seems like it should be a very common question, I believe it can be proved formally using the intermediate + mean value theorem. Does anyone have any idea how to prove this?","f [x_0, x_1] f(x_0) < f(x_1) f(x_0) f(x_1) x f'(x) > 0 a < f(x) < b a b","['real-analysis', 'calculus', 'derivatives', 'continuity']"
51,Trouble with $I(\alpha) = \int_0^{\infty} \frac{\cos (\alpha x)}{x^2 + 1} dx$,Trouble with,I(\alpha) = \int_0^{\infty} \frac{\cos (\alpha x)}{x^2 + 1} dx,I'm ultimately trying to solve $$I(\alpha) = \int_0^{\infty} \dfrac{\cos (\alpha x)}{x^2 + 1} dx$$ by using differentiation under the integral. I realize that this is most easily done using residues but I'm intending this problem to introduce my advanced calculus 2/differential equations students to some interesting techniques before they take real analysis. Differentiating under the integral a first time leads to $$I'(\alpha) = \int_0^{\infty} \dfrac{-x \sin (\alpha x)}{x^2 + 1} dx = - \dfrac{\pi}{2} + \int_0^{\infty} \dfrac{\sin (\alpha x)}{x(x^2 + 1)}dx$$ by making use of the Dirichlet integral and again to $$I''(\alpha) = \int_0^{\infty} \dfrac{\cos (\alpha x)}{x^2 + 1} = I(\alpha)$$ To solve this second-order ODE we'll need two initial conditions. The integral for $I'(\alpha)$ leads to the incorrect result $I'(0) = 0$ but the rewritten version leads to the correct result of $I'(0) = -\dfrac{\pi}{2}$ . I'm having trouble justifying this. Any help or guidance is appreciated. I'll also settle for simpler arguments as to why $I'(0) \neq 0$ .,I'm ultimately trying to solve by using differentiation under the integral. I realize that this is most easily done using residues but I'm intending this problem to introduce my advanced calculus 2/differential equations students to some interesting techniques before they take real analysis. Differentiating under the integral a first time leads to by making use of the Dirichlet integral and again to To solve this second-order ODE we'll need two initial conditions. The integral for leads to the incorrect result but the rewritten version leads to the correct result of . I'm having trouble justifying this. Any help or guidance is appreciated. I'll also settle for simpler arguments as to why .,I(\alpha) = \int_0^{\infty} \dfrac{\cos (\alpha x)}{x^2 + 1} dx I'(\alpha) = \int_0^{\infty} \dfrac{-x \sin (\alpha x)}{x^2 + 1} dx = - \dfrac{\pi}{2} + \int_0^{\infty} \dfrac{\sin (\alpha x)}{x(x^2 + 1)}dx I''(\alpha) = \int_0^{\infty} \dfrac{\cos (\alpha x)}{x^2 + 1} = I(\alpha) I'(\alpha) I'(0) = 0 I'(0) = -\dfrac{\pi}{2} I'(0) \neq 0,"['derivatives', 'leibniz-integral-rule']"
52,Question regarding second derivatives and maximums. Justifying a specific part,Question regarding second derivatives and maximums. Justifying a specific part,,"Note: My question is regarding the bolded parts. Read those and my explanation below to see what I am asking. Let $f$ be a real-valued function on an open subset $U$ of $R$ that is twice differentiable at $x_0 \in U$ . Show that if $f'(x_0) = 0$ and $f''(x_0) < 0$ , then the restriction of $f$ to some open ball of center $x_0$ attains a maximum at $x_0$ . Here is my proof To show there is a maximum, we want to show that $f(x) < f(x_0) + f'(x_0)(x - x_0) \ \forall x \neq x_0, x_0 \in U$ Case 1.) Assume $x < x_0$ . Applying the Mean Value Theorem on the interval $[x,x_0], \exists c \ where \ x < c < x_0 \ s.t.$ $f(x_0) - f(x) = f'(c)(x_0 - x)$ $f(x) = f(x_0) - f'(c)(x_0 - x)$ $f(x) = f(x_0) + f'(c)(x - x_0)$ Since $f''(x) < 0 \forall x \in U, f'(x)$ must be decreasing. Since we know from the MVT that $c < x_0$ , we can put these results together to have. $f'(x_0) < f'(c)$ Multiplying this by $x - x_0$ (which is negative, so flip the inequality), $f'(c)(x - x_0) < f'(x_0)(x - x_0)$ Adding $f(x_0)$ to both sides, $f(x_0) + f'(c)(x - x_0) < f(x_0) + f'(x)(x - x_0)$ The left side is $f(x)$ $f(x) < f(x_0) + f'(x)(x - x_0)$ as desired. Case 2.) Assume $x_0 < x$ . Using the MVT on the interval $[x_0,x], \exists d \ where \ x_0 < d < x_0 \ s.t.$ $f(x) - f(x_0) = f'(c)(x - x_0)$ $f(x) = f(x_0) + f'(c)(x - x_0)$ Similar to before, $f''(x) < 0$ means $f'(x)$ is decreasing. We know from this application of MVT that $x_0 < c$ Therefore, $f'(c) < f'(x_0)$ Multiplying by $x - x_0$ (positive this time) $f'(c)(x - x_0)) < f'(x_0)(x - x_0)$ Adding $f(x_0)$ , $f(x_0) + f'(c)(x - x_0) < f(x_0) + f'(x_0)(x - x_0)$ The left side is $f(x)$ $f(x) < f(x_0) + f'(x_0)(x - x_0)$ still holds. So we have shown that $\forall x \in U \ s.t. \ x \neq x_0$ , the function will be concave down. Therefore, it has a maximum at $x_0$ I bolded the parts that were unclear. My professor told me that this is not necessarily true. The function will admit a negative second derivative at $x_0$ , but not in the entire neighborhood. He said in this case, what I did is correct, but he said it is not obvious as to why. He said I have to say something about why I can make that claim. Can someone help me figure out what I can say to make that part clear? I'm not seeing it right now. Edit: I am thinking it maybe due to the fact that I'm claiming this happens on all of R. So like, the neighborhood wouldn't really matter? But then again, I feel like that would be trivial, so I don't think he would ask the question. I think I just defined it that way. I'm really not sure. Please, any help is appreciated! Thanks in advance!","Note: My question is regarding the bolded parts. Read those and my explanation below to see what I am asking. Let be a real-valued function on an open subset of that is twice differentiable at . Show that if and , then the restriction of to some open ball of center attains a maximum at . Here is my proof To show there is a maximum, we want to show that Case 1.) Assume . Applying the Mean Value Theorem on the interval Since must be decreasing. Since we know from the MVT that , we can put these results together to have. Multiplying this by (which is negative, so flip the inequality), Adding to both sides, The left side is as desired. Case 2.) Assume . Using the MVT on the interval Similar to before, means is decreasing. We know from this application of MVT that Therefore, Multiplying by (positive this time) Adding , The left side is still holds. So we have shown that , the function will be concave down. Therefore, it has a maximum at I bolded the parts that were unclear. My professor told me that this is not necessarily true. The function will admit a negative second derivative at , but not in the entire neighborhood. He said in this case, what I did is correct, but he said it is not obvious as to why. He said I have to say something about why I can make that claim. Can someone help me figure out what I can say to make that part clear? I'm not seeing it right now. Edit: I am thinking it maybe due to the fact that I'm claiming this happens on all of R. So like, the neighborhood wouldn't really matter? But then again, I feel like that would be trivial, so I don't think he would ask the question. I think I just defined it that way. I'm really not sure. Please, any help is appreciated! Thanks in advance!","f U R x_0 \in U f'(x_0) = 0 f''(x_0) < 0 f x_0 x_0 f(x) < f(x_0) + f'(x_0)(x - x_0) \ \forall x \neq x_0, x_0 \in U x < x_0 [x,x_0], \exists c \ where \ x < c < x_0 \ s.t. f(x_0) - f(x) = f'(c)(x_0 - x) f(x) = f(x_0) - f'(c)(x_0 - x) f(x) = f(x_0) + f'(c)(x - x_0) f''(x) < 0 \forall x \in U, f'(x) c < x_0 f'(x_0) < f'(c) x - x_0 f'(c)(x - x_0) < f'(x_0)(x - x_0) f(x_0) f(x_0) + f'(c)(x - x_0) < f(x_0) + f'(x)(x - x_0) f(x) f(x) < f(x_0) + f'(x)(x - x_0) x_0 < x [x_0,x], \exists d \ where \ x_0 < d < x_0 \ s.t. f(x) - f(x_0) = f'(c)(x - x_0) f(x) = f(x_0) + f'(c)(x - x_0) f''(x) < 0 f'(x) x_0 < c f'(c) < f'(x_0) x - x_0 f'(c)(x - x_0)) < f'(x_0)(x - x_0) f(x_0) f(x_0) + f'(c)(x - x_0) < f(x_0) + f'(x_0)(x - x_0) f(x) f(x) < f(x_0) + f'(x_0)(x - x_0) \forall x \in U \ s.t. \ x \neq x_0 x_0 x_0","['real-analysis', 'calculus', 'derivatives', 'maxima-minima']"
53,"Show that $\max_{x \in [a, b]} |f'(x)| \leq \frac{(b-a)^2}{2} \max_{x \in [a, b]}|f''(x)|$.",Show that .,"\max_{x \in [a, b]} |f'(x)| \leq \frac{(b-a)^2}{2} \max_{x \in [a, b]}|f''(x)|","Suppose $f(x) \in C^2([a, b])$ with $f(a) = f(b) = 0$ . Show that $$\max_{x \in [a, b]} |f'(x)| \leq \frac{(b-a)^2}{2} \max_{x \in [a, b]}|f''(x)|.$$ My work: Suppose that $f'(x_0) = \max_{x \in [a, b]} |f'(x)|$ . One can expand $f(x)$ at $x_0$ as follows: $$f(a) - f(x_0) = f'(x_0) (a - x_0) + \frac{1}{2}f''(\xi_1) (a-x_0)^2,~\xi_1\in[a, x_0]$$ $$f(b) - f(x_0) = f'(x_0) (b - x_0) + \frac{1}{2}f''(\xi_2) (b-x_0)^2,~\xi_2\in[x_0, b]$$ The difference of the two equations leads to $$f'(x_0) (b-a) = \frac{1}{2}\Big[f''(\xi_1)(a-x_0)^2 - f''(\xi_2)(b-x_0)^2\Big].$$ I have no idea how to reach the term $\max |f''(x)|$ .",Suppose with . Show that My work: Suppose that . One can expand at as follows: The difference of the two equations leads to I have no idea how to reach the term .,"f(x) \in C^2([a, b]) f(a) = f(b) = 0 \max_{x \in [a, b]} |f'(x)| \leq \frac{(b-a)^2}{2} \max_{x \in [a, b]}|f''(x)|. f'(x_0) = \max_{x \in [a, b]} |f'(x)| f(x) x_0 f(a) - f(x_0) = f'(x_0) (a - x_0) + \frac{1}{2}f''(\xi_1) (a-x_0)^2,~\xi_1\in[a, x_0] f(b) - f(x_0) = f'(x_0) (b - x_0) + \frac{1}{2}f''(\xi_2) (b-x_0)^2,~\xi_2\in[x_0, b] f'(x_0) (b-a) = \frac{1}{2}\Big[f''(\xi_1)(a-x_0)^2 - f''(\xi_2)(b-x_0)^2\Big]. \max |f''(x)|","['real-analysis', 'derivatives']"
54,Continuity of the derivative function. [closed],Continuity of the derivative function. [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose we have a function f(x) that is differentiable for all values of x. Is it necessary for the derivative function to be continuous for all x ?.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose we have a function f(x) that is differentiable for all values of x. Is it necessary for the derivative function to be continuous for all x ?.",,"['calculus', 'derivatives', 'continuity']"
55,Problem working out a second derivative,Problem working out a second derivative,,"I am studying maths as a hobby and feel I am having problems working out second derivatives. The problem is as follows: Find the maximum or minimum values of the function $y = (2x - 5)^4$ Here is my working: $$dy/dx = 4(2x - 5)^3\cdot 2 = 8(2x -5)^3 ,$$ Which is zero when $x = 2\frac {1}{2}$ . To find whether this is a maximum or a minimum I find the second derivative: $$d^2y/dx^2 = 24(2x - 5)^2.2 = 48(2x - 5)^2.$$ But this is where I feel I must have gone wrong because this is zero when $x = 2\frac {1}{2}$ .",I am studying maths as a hobby and feel I am having problems working out second derivatives. The problem is as follows: Find the maximum or minimum values of the function Here is my working: Which is zero when . To find whether this is a maximum or a minimum I find the second derivative: But this is where I feel I must have gone wrong because this is zero when .,"y = (2x - 5)^4 dy/dx = 4(2x - 5)^3\cdot 2 = 8(2x -5)^3 , x = 2\frac {1}{2} d^2y/dx^2 = 24(2x - 5)^2.2 = 48(2x - 5)^2. x = 2\frac {1}{2}","['calculus', 'derivatives', 'maxima-minima']"
56,"$f$ is continuous on $[0,\infty)$, differentiable $(0,\infty)$, $f(0)\geq0$ and $f'(x)-f(x)\geq0$. Prove that $f(x)\geq0$","is continuous on , differentiable ,  and . Prove that","f [0,\infty) (0,\infty) f(0)\geq0 f'(x)-f(x)\geq0 f(x)\geq0","This question was asked in my real analysis quiz and I was unable to solve it. It has 2 parts and unfortunately, I couldn't solve both. Question:(a) Suppose $f$ is continuous on $[0,\infty) $ , differentiable on $(0,\infty)$ and $f(0)\geq0$ . Suppose $f'(x) \geq f(x)$ for all $x \in (0,\infty)$ . Show that $f(x)\geq 0$ for all $x \in (0,\infty)$ . (b) Let $f$ and $g$ are continuous functions on $[0,1]$ satisfying $f(x)\geq g(x)$ for every $0\leq x \leq 1$ , and if $\int_0^{1} f(x) dx =\int_0^{1} g(x) dx $ then show that $f=g$ . Attempt: In (a) I thought of integrating $f'(x)\geq f(x)$ from $0$ to $x$ with variable t to get $f(x) \geq \int_{0}^{x} f(t) dt  +f(0)$ . But I dont know how to proceed from here. (b) Continuous functions are integrable so $\int_0^{1} f(t) dt \geq \int_0^{1} g(t) dt  > I$ tried by thinking of teaking sets $A =\{x : f(x)>g(x)\}$ and $B =\{ x: f(x)<g(x) \}$ but could not  proceed . Kindly Help !!","This question was asked in my real analysis quiz and I was unable to solve it. It has 2 parts and unfortunately, I couldn't solve both. Question:(a) Suppose is continuous on , differentiable on and . Suppose for all . Show that for all . (b) Let and are continuous functions on satisfying for every , and if then show that . Attempt: In (a) I thought of integrating from to with variable t to get . But I dont know how to proceed from here. (b) Continuous functions are integrable so tried by thinking of teaking sets and but could not  proceed . Kindly Help !!","f [0,\infty)  (0,\infty) f(0)\geq0 f'(x) \geq f(x) x \in (0,\infty) f(x)\geq 0 x \in (0,\infty) f g [0,1] f(x)\geq g(x) 0\leq x \leq 1 \int_0^{1} f(x) dx =\int_0^{1} g(x) dx  f=g f'(x)\geq f(x) 0 x f(x) \geq \int_{0}^{x} f(t) dt  +f(0) \int_0^{1} f(t) dt \geq \int_0^{1} g(t) dt  > I A =\{x : f(x)>g(x)\} B =\{ x: f(x)<g(x) \}",['real-analysis']
57,Find the derivative of $f(x)$,Find the derivative of,f(x),"$f(x)=\begin{cases}1 & x\in\Bbb{Q}\\ \sin x& x\notin\Bbb{Q}\end{cases}$ I know when $x=2\pi(n-1)+\frac{\pi}{2},\space f(x)=1$ when $x=2\pi(n-1)-\frac{\pi}{2},\space f(x)=-1$ when $x=n\pi,\space f(x)=0$ Is $f(x)$ then differentiable? I assume it is only differentiable when $x\in\Bbb{Q} $ and $x=2\pi(n-1)+\frac{\pi}{2}$ . Am I correct?",I know when when when Is then differentiable? I assume it is only differentiable when and . Am I correct?,"f(x)=\begin{cases}1 & x\in\Bbb{Q}\\ \sin x& x\notin\Bbb{Q}\end{cases} x=2\pi(n-1)+\frac{\pi}{2},\space f(x)=1 x=2\pi(n-1)-\frac{\pi}{2},\space f(x)=-1 x=n\pi,\space f(x)=0 f(x) x\in\Bbb{Q}  x=2\pi(n-1)+\frac{\pi}{2}",['derivatives']
58,Uniform bound for integral in terms of $\left\lVert f' \right\rVert_4^4$,Uniform bound for integral in terms of,\left\lVert f' \right\rVert_4^4,"Show that there is a constant $C>0$ such that for any compactly supported $C^1$ function $f: \mathbb{R} \to \mathbb{R}$ , we have $$\int_{\mathbb{R}} \left(\frac{f(x)-f(y)}{x-y}\right)^4dy \le C \left\lVert f' \right\rVert_4^4\qquad\text{for all }x \in \mathbb{R}.$$ This is an old quals problem that I don't know how to do. One hint is that I may use integration by parts, but I don't know how to apply the hint either. Any approach would be much appreciated.","Show that there is a constant such that for any compactly supported function , we have This is an old quals problem that I don't know how to do. One hint is that I may use integration by parts, but I don't know how to apply the hint either. Any approach would be much appreciated.",C>0 C^1 f: \mathbb{R} \to \mathbb{R} \int_{\mathbb{R}} \left(\frac{f(x)-f(y)}{x-y}\right)^4dy \le C \left\lVert f' \right\rVert_4^4\qquad\text{for all }x \in \mathbb{R}.,"['real-analysis', 'integration', 'derivatives', 'lp-spaces']"
59,Derivative of Softmax loss function (with temperature T),Derivative of Softmax loss function (with temperature T),,"I am try to calculate the derivative of cross-entropy, when the softmax layer has the temperature T. That is: \begin{equation} p_j = \frac{e^{o_j/T}}{\sum_k e^{o_k/T}} \end{equation} This question here was answered at T=1: Derivative of Softmax loss function Now what would be the final derivative in terms of $p_i$ , $q_i$ , and T? Please see the linked question for the notations. Edit: Thanks to Alex for pointing out a typo","I am try to calculate the derivative of cross-entropy, when the softmax layer has the temperature T. That is: This question here was answered at T=1: Derivative of Softmax loss function Now what would be the final derivative in terms of , , and T? Please see the linked question for the notations. Edit: Thanks to Alex for pointing out a typo","\begin{equation}
p_j = \frac{e^{o_j/T}}{\sum_k e^{o_k/T}}
\end{equation} p_i q_i","['derivatives', 'machine-learning']"
60,Struggling on how to prove an inequality using the Mean Value Theorem,Struggling on how to prove an inequality using the Mean Value Theorem,,"Prove the following using the mean value theorem: $ln(1+x)$ $\lt$ $x$ , for all $x \gt 0$ . So far, I have tried the following: I have created a function $$f(x) = ln(1+x) - x , $$ $x \gt 0$ . I have differentiated this $f(x)$ to give : $$f'(x) = \frac{1}{1+x} -1 = \frac{-x}{1+x}$$ which is clearly less than $0$ for all $x \gt 0$ . But I am struggling to apply the Mean Value Theorem here. I have noticed that $log(1) = 0$ and so: $$log(1+x) - x = f(x) - f(0) = x (f'(c))$$ for some $c \in (0,x)$ but I don't know where to go from here.","Prove the following using the mean value theorem: , for all . So far, I have tried the following: I have created a function . I have differentiated this to give : which is clearly less than for all . But I am struggling to apply the Mean Value Theorem here. I have noticed that and so: for some but I don't know where to go from here.","ln(1+x) \lt x x \gt 0 f(x) = ln(1+x) - x ,  x \gt 0 f(x) f'(x) = \frac{1}{1+x} -1 = \frac{-x}{1+x} 0 x \gt 0 log(1) = 0 log(1+x) - x = f(x) - f(0) = x (f'(c)) c \in (0,x)","['real-analysis', 'calculus', 'derivatives', 'inequality']"
61,Is $f(x) = 0 \implies f'(x) > 0$ a sufficient condition for uniqueness of roots for a smooth (non constant) function $f$?,Is  a sufficient condition for uniqueness of roots for a smooth (non constant) function ?,f(x) = 0 \implies f'(x) > 0 f,"Let $f : \mathbb R \rightarrow \mathbb R : x \mapsto f(x)$ a smooth non constant function such that $f(x) = 0 \implies f'(x) > 0.$ Does $f$ have a unique root ? I think this is true but I can't prove it. Here what I've done so far. Case 1. Let $x_1 < x_2$ be two roots with no other root in $(x_1,x_2)$ . Since $f(x_i) = 0$ we have $f'(x_i) > 0$ so by continuity of $f'$ we have that $f' > 0$ on a small interval $(x_i - \delta,x_i + \delta)$ around $x_i$ . Therefore $f$ is strictly increasing on a neighbourhood of each root. In particular $f>0$ on $(x_i, x_i + \delta)$ and $f< 0$ on $(x_i-\delta,x_i).$ Using the intermediate value theorem we can find another root $c$ somewhere between $x_1$ and $x_2$ , a contradiction. In particular $f(x) = 0$ as infinitely many solutions. Here's where I'm not so sure : Case 2. If there are roots between any given given roots $x_1 < x_2$ then we can apply the above reasoning to $x_1,c$ and $c,x_2$ to find new roots between $x_1$ and $x_2$ so we can find infinitely many roots between any given roots $x_1$ and $x_2$ . In don't know how to proceed. It feels like $f$ should be equal to $0$ on some interval which would then contradict $f(x) = 0 \implies f'(x)>0$ Can someone find a counter example or finish the proof ?","Let a smooth non constant function such that Does have a unique root ? I think this is true but I can't prove it. Here what I've done so far. Case 1. Let be two roots with no other root in . Since we have so by continuity of we have that on a small interval around . Therefore is strictly increasing on a neighbourhood of each root. In particular on and on Using the intermediate value theorem we can find another root somewhere between and , a contradiction. In particular as infinitely many solutions. Here's where I'm not so sure : Case 2. If there are roots between any given given roots then we can apply the above reasoning to and to find new roots between and so we can find infinitely many roots between any given roots and . In don't know how to proceed. It feels like should be equal to on some interval which would then contradict Can someone find a counter example or finish the proof ?","f : \mathbb R \rightarrow \mathbb R : x \mapsto f(x) f(x) = 0 \implies f'(x) > 0. f x_1 < x_2 (x_1,x_2) f(x_i) = 0 f'(x_i) > 0 f' f' > 0 (x_i - \delta,x_i + \delta) x_i f f>0 (x_i, x_i + \delta) f< 0 (x_i-\delta,x_i). c x_1 x_2 f(x) = 0 x_1 < x_2 x_1,c c,x_2 x_1 x_2 x_1 x_2 f 0 f(x) = 0 \implies f'(x)>0","['real-analysis', 'derivatives', 'roots']"
62,An approach to the derivative of $e^x$ in Calculus 1,An approach to the derivative of  in Calculus 1,e^x,"While I know there are plenty of topics on here concerning the derivative of $e^x$ , I was wondering if what I have below is ""okay"" for the proof of it, or if there are holes in logic. Proof: $\displaystyle\frac{d}{dx}e^x = \lim_{h \to 0}{\frac{e^{x+h}-e^x}{h}}=\lim_{h\to 0}\frac{e^xe^h-e^x}{h}=\lim_{h \to 0} \frac{e^x(e^h-1)}{h}$ Now, $\displaystyle e=\lim_{x \to \infty}(1+\frac{1}{x})^x$ Letting $\displaystyle x=\frac{1}{h} \Rightarrow e=\lim_{h \to 0}(1+h)^{1/h}$ because $h \to 0$ as $x\to \infty$ . If you raise both sides to the $h$ power you would then get $\displaystyle e^h=\lim_{h\to 0}(1+h)$ . You could then make a substitution for $e^h$ back into line one, though I am a little concerned. Namely, couldn't I simplify $\displaystyle \lim_{h\to 0}(1+h)=1$ , but then substituting would not give the correct answer then. NOTE: I wanted to find a proof that a Calculus 1 student could follow, but I also wanted to go about it as if we don't know the result of $\displaystyle\frac{d}{dx}\ln(x)$ yet nor have learned Taylor Series (as that would be Calculus 2).","While I know there are plenty of topics on here concerning the derivative of , I was wondering if what I have below is ""okay"" for the proof of it, or if there are holes in logic. Proof: Now, Letting because as . If you raise both sides to the power you would then get . You could then make a substitution for back into line one, though I am a little concerned. Namely, couldn't I simplify , but then substituting would not give the correct answer then. NOTE: I wanted to find a proof that a Calculus 1 student could follow, but I also wanted to go about it as if we don't know the result of yet nor have learned Taylor Series (as that would be Calculus 2).",e^x \displaystyle\frac{d}{dx}e^x = \lim_{h \to 0}{\frac{e^{x+h}-e^x}{h}}=\lim_{h\to 0}\frac{e^xe^h-e^x}{h}=\lim_{h \to 0} \frac{e^x(e^h-1)}{h} \displaystyle e=\lim_{x \to \infty}(1+\frac{1}{x})^x \displaystyle x=\frac{1}{h} \Rightarrow e=\lim_{h \to 0}(1+h)^{1/h} h \to 0 x\to \infty h \displaystyle e^h=\lim_{h\to 0}(1+h) e^h \displaystyle \lim_{h\to 0}(1+h)=1 \displaystyle\frac{d}{dx}\ln(x),"['calculus', 'proof-verification', 'derivatives', 'proof-writing']"
63,"$\forall x\in \mathbb{R} \exists b\in (x, x+a) \frac{f'(x) } {f(x) }=e^{a{f'(b) }/{f(b) } }$",,"\forall x\in \mathbb{R} \exists b\in (x, x+a) \frac{f'(x) } {f(x) }=e^{a{f'(b) }/{f(b) } }","Let $a\ge 0$ and $f:\mathbb{R}\rightarrow  \mathbb{R}$ be a differentiable positive function such that $f'(x) =f(x+a) \forall x\in \mathbb{R}$ . How can I prove that $\forall x\in \mathbb{R} \exists b\in (x, a+x) \frac{f'(x) } {f(x) }=e^{a\frac{f'(b) }{f(b) } }$ . I tried the intermediate value Theorem but couldn't prove it. Thank you in advance for your help.",Let and be a differentiable positive function such that . How can I prove that . I tried the intermediate value Theorem but couldn't prove it. Thank you in advance for your help.,"a\ge 0 f:\mathbb{R}\rightarrow  \mathbb{R} f'(x) =f(x+a) \forall x\in \mathbb{R} \forall x\in \mathbb{R} \exists b\in (x, a+x) \frac{f'(x) } {f(x) }=e^{a\frac{f'(b) }{f(b) } }","['real-analysis', 'derivatives', 'continuity']"
64,"Characterization of real-valued $C^1$ functions on $[0,1]$",Characterization of real-valued  functions on,"C^1 [0,1]","I have encountered the following example, where I found things I don't understand: Let $\Bbb{Q}^+$ be the set of positive rational numbers, $C([0,1])$ be the space of all continuous real-valued functions on $[0,1]$ and denote by $C^1$ the class of continuously differentiable functions in $C([0,1])$ .   (At the endpoints we take one-sided derivatives.)   Then for $f\in C([0,1])$ , $f\in C^1$ iff for all $\epsilon\in\Bbb{Q}^+$ there exists rational open intervals $I_0,\dots,I_{n-1}$ covering $[0,1]$ s.t. for all $j<n$ : $$\forall a,b,c,d\in I_j\cap [0,1], a\ne b,c\ne d,\lvert \frac{f(a)-f(b)}{a-b}-\frac{f(c)-f(d)}{c-d}\rvert\le \epsilon.$$ So for an open interval $J$ and $\epsilon>0$ , we put $$A_{J,\epsilon}=\{f\in C([0,1])\mid \forall a,b,c,d\in J\cap [0,1], a\ne b,c\ne d,\lvert \frac{f(a)-f(b)}{a-b}-\frac{f(c)-f(d)}{c-d}\rvert\le \epsilon\},$$ we have that $A_{J,\epsilon}$ is closed in $C([0,1])$ . As I understand it, functions in $C^1$ are characterized in terms of the uniform continuity of the quotient difference (right?). Question: How can I prove that? Attempt: By definition, $$f\in C^1 \iff \forall x\in (0,1),\exists \lim_{y\to x}\frac{f(y)-f(x)}{y-x}=f'(x), f'(x)=\lim_{y\to x}f'(y)$$ (at endpoints a similar condition holds with right/left limits). It seems clear to me that the above implies continuity of the quotient difference at every $x\in [0,1]$ , but does the converse hold? If not, how can I rigorously prove the above characterization? Thank you in advance for your help.","I have encountered the following example, where I found things I don't understand: Let be the set of positive rational numbers, be the space of all continuous real-valued functions on and denote by the class of continuously differentiable functions in .   (At the endpoints we take one-sided derivatives.)   Then for , iff for all there exists rational open intervals covering s.t. for all : So for an open interval and , we put we have that is closed in . As I understand it, functions in are characterized in terms of the uniform continuity of the quotient difference (right?). Question: How can I prove that? Attempt: By definition, (at endpoints a similar condition holds with right/left limits). It seems clear to me that the above implies continuity of the quotient difference at every , but does the converse hold? If not, how can I rigorously prove the above characterization? Thank you in advance for your help.","\Bbb{Q}^+ C([0,1]) [0,1] C^1 C([0,1]) f\in C([0,1]) f\in C^1 \epsilon\in\Bbb{Q}^+ I_0,\dots,I_{n-1} [0,1] j<n \forall a,b,c,d\in I_j\cap [0,1], a\ne b,c\ne d,\lvert \frac{f(a)-f(b)}{a-b}-\frac{f(c)-f(d)}{c-d}\rvert\le \epsilon. J \epsilon>0 A_{J,\epsilon}=\{f\in C([0,1])\mid \forall a,b,c,d\in J\cap [0,1], a\ne b,c\ne d,\lvert \frac{f(a)-f(b)}{a-b}-\frac{f(c)-f(d)}{c-d}\rvert\le \epsilon\}, A_{J,\epsilon} C([0,1]) C^1 f\in C^1 \iff \forall x\in (0,1),\exists \lim_{y\to x}\frac{f(y)-f(x)}{y-x}=f'(x), f'(x)=\lim_{y\to x}f'(y) x\in [0,1]","['real-analysis', 'derivatives', 'continuity', 'uniform-continuity']"
65,Is the sum of this series a differentiable function?,Is the sum of this series a differentiable function?,,Let $$f(x) = \sum_{n=1}^{\infty} \frac{1}{nx} \left( 1 - \frac{1}{e^{ \frac{x}{n}}}  \right) \wedge x>0$$ Is the sum of this series a differentiable function? my idea For examining differentiation let: $$ g_n (x) :=  \frac{1}{nx} \left(1 - \frac{1}{e^{ \frac{x}{n}}} \right) $$ then $$ g_n'(x) = -\frac{\left(n e^{x/n}-n-x\right)}{e^{\frac{x}{n}}n^2 x^2} $$ but $g_n'(x)$ seems to be asymptotic similar  to $\frac{1}{n}$ so I can't use theorem which can help me to eventually proof that $f$ is differentiable. What should I do in such situation?,Let Is the sum of this series a differentiable function? my idea For examining differentiation let: then but seems to be asymptotic similar  to so I can't use theorem which can help me to eventually proof that is differentiable. What should I do in such situation?,f(x) = \sum_{n=1}^{\infty} \frac{1}{nx} \left( 1 - \frac{1}{e^{ \frac{x}{n}}}  \right) \wedge x>0  g_n (x) :=  \frac{1}{nx} \left(1 - \frac{1}{e^{ \frac{x}{n}}} \right)   g_n'(x) = -\frac{\left(n e^{x/n}-n-x\right)}{e^{\frac{x}{n}}n^2 x^2}  g_n'(x) \frac{1}{n} f,"['real-analysis', 'derivatives']"
66,Differentiability of the Schatten $p$-norm on positive definite matrices,Differentiability of the Schatten -norm on positive definite matrices,p,"Let $V$ be the vector space of symmetric matrices in $\Bbb R^{n\times n}$ . For $p\in (1,\infty)$ , the Schatten $p$ -norm of $M\in V$ is defined as $\|M\|_p =(\sum_{i=1}^n \sigma_i(M)^p)^{1/p}$ where $\sigma_1(M),\ldots,\sigma_n(M)$ are the singular values of $M$ . Now, let $C\subset V$ be the cone of positive semi-definite matrices. It follows from this old post that $$\nabla \|M\|_p = \|M\|_p^{1-p}M^{p-1}\qquad \forall M\in C.$$ Where is a reference for the above statement?","Let be the vector space of symmetric matrices in . For , the Schatten -norm of is defined as where are the singular values of . Now, let be the cone of positive semi-definite matrices. It follows from this old post that Where is a reference for the above statement?","V \Bbb R^{n\times n} p\in (1,\infty) p M\in V \|M\|_p =(\sum_{i=1}^n \sigma_i(M)^p)^{1/p} \sigma_1(M),\ldots,\sigma_n(M) M C\subset V \nabla \|M\|_p = \|M\|_p^{1-p}M^{p-1}\qquad \forall M\in C.","['linear-algebra', 'derivatives', 'reference-request', 'positive-semidefinite']"
67,Derivative of the Cayley Transform.,Derivative of the Cayley Transform.,,"Given $V$ a finite dimensional vector space, let $R=\{f \in  \operatorname{End}(V) : I+f \text{ is invertible}\}.$ We define the Cayley Transform $T$ such that $T: R \to  \operatorname{End}(V)$ and $T(f) = (1-f)(1+f)^{-1}$ . I am asked to show that the Cayley Transform is differentiable and to find its derivative. I've already showed that the Cayley Transform is an involution but I don't know how to use this to show that its differentiable. I am trying to use the definition of differentiable but all I get is that $T(f+h) = (I-(f+h))(I+(f+h))^{-1}$ and I don't know what to do next","Given a finite dimensional vector space, let We define the Cayley Transform such that and . I am asked to show that the Cayley Transform is differentiable and to find its derivative. I've already showed that the Cayley Transform is an involution but I don't know how to use this to show that its differentiable. I am trying to use the definition of differentiable but all I get is that and I don't know what to do next",V R=\{f \in  \operatorname{End}(V) : I+f \text{ is invertible}\}. T T: R \to  \operatorname{End}(V) T(f) = (1-f)(1+f)^{-1} T(f+h) = (I-(f+h))(I+(f+h))^{-1},"['derivatives', 'differential-geometry']"
68,Differentiability through paths,Differentiability through paths,,"Let $f:\mathbb{R}^{2} \to \mathbb{R}$ defined by $$f(x) = \begin{cases}\displaystyle\frac{x^{3}}{x^{2}+y^{2}},& (x,y)\neq (0,0)\\ 0,& (x,y) = (0,0)\end{cases}.$$ Show that $f$ is not differentiable in $(0,0)$ , however, show that for every differentiable path $\lambda: (0,1) \to \mathbb{R}^{2}$ , passing through origin, $f \circ \lambda$ is differentiable. My problem is in the second part. If $\lambda: \mathbb{R} \to \mathbb{R}^{2}$ is a line pararallel to $e_{i}$ passing through $0$ , $\lambda(t) = (0,0) + te_{i}$ , I can show that $(f\circ \lambda)'(0) = \frac{\partial}{\partial x_{i}}f(0,0)$ . I've tried to generalize this to an arbitrary path with domain $(0,1)$ , but I couldnt. Can someone help me?","Let defined by Show that is not differentiable in , however, show that for every differentiable path , passing through origin, is differentiable. My problem is in the second part. If is a line pararallel to passing through , , I can show that . I've tried to generalize this to an arbitrary path with domain , but I couldnt. Can someone help me?","f:\mathbb{R}^{2} \to \mathbb{R} f(x) = \begin{cases}\displaystyle\frac{x^{3}}{x^{2}+y^{2}},& (x,y)\neq (0,0)\\ 0,& (x,y) = (0,0)\end{cases}. f (0,0) \lambda: (0,1) \to \mathbb{R}^{2} f \circ \lambda \lambda: \mathbb{R} \to \mathbb{R}^{2} e_{i} 0 \lambda(t) = (0,0) + te_{i} (f\circ \lambda)'(0) = \frac{\partial}{\partial x_{i}}f(0,0) (0,1)","['real-analysis', 'derivatives']"
69,Maximizing the area of a cyclic trapezoid whose long base is the circumdiameter. Non-trigonometric solution?,Maximizing the area of a cyclic trapezoid whose long base is the circumdiameter. Non-trigonometric solution?,,"A half circle with a radius of R encompasses an isosceles trapezoid such that the large base of the trapezoid is the diameter of the circle encompassing it. In terms of R, what is the length of the smaller base of all the possible trapezoids as described, whose area is maximal? After some attempts at the problem, I managed to solve it using unit circle trigonometry, but I am curious if there are purely geometric solutions for this (which is what I was trying to find when I first attempted the problem). Here is my solution: Let $x$ be $\measuredangle AOD$ Let $h$ be the height of the trapezoid Assume $0 < x < 90^\circ$ $$h = AO\sin x = R\sin x$$ $$AB = 2AO\cos x = 2R\cos x$$ Trapezoid area formula: $$\frac{AB + DC}{2} \cdot h $$ $$\downarrow$$ $$S_{(x)} = \frac{2R\cos x  + 2R}{2} \cdot R\sin x$$ From here, we find our function's derivative, get the $x$ for which there is a maxima, and plug it into our definition of AB to get it in terms of R, which would be AB = R. Is there an alternative?","A half circle with a radius of R encompasses an isosceles trapezoid such that the large base of the trapezoid is the diameter of the circle encompassing it. In terms of R, what is the length of the smaller base of all the possible trapezoids as described, whose area is maximal? After some attempts at the problem, I managed to solve it using unit circle trigonometry, but I am curious if there are purely geometric solutions for this (which is what I was trying to find when I first attempted the problem). Here is my solution: Let be Let be the height of the trapezoid Assume Trapezoid area formula: From here, we find our function's derivative, get the for which there is a maxima, and plug it into our definition of AB to get it in terms of R, which would be AB = R. Is there an alternative?",x \measuredangle AOD h 0 < x < 90^\circ h = AO\sin x = R\sin x AB = 2AO\cos x = 2R\cos x \frac{AB + DC}{2} \cdot h  \downarrow S_{(x)} = \frac{2R\cos x  + 2R}{2} \cdot R\sin x x,"['derivatives', 'trigonometry', 'euclidean-geometry']"
70,Optimization Problem. Find Smallest Perimeter of a Rectangle Given Area.,Optimization Problem. Find Smallest Perimeter of a Rectangle Given Area.,,QUESTION Find the dimensions of a rectangle with area $1000$ m $^2$ whose perimeter is as small as possible. MY WORK I think we are solving for $\frac{dy}{dx}$ : \begin{align*} P &= (2x+2y)  \\ A &= (x\cdot y) \\ \frac{d}{dx}1000&=\frac{d}{dx}(x \cdot y) \\ 0 &=\frac{d}{dx}((x)\prime(y)+(y)\prime(x)) \\ 0 &=y+x\frac{dy}{dx}\\ \frac{dy}{dx} &=\frac{-y}{x} \end{align*} I didn't use the perimeter formula. I am not sure where I messed up. I think probably somewhere in my setup for the equation. If someone could take a look at my work and point me in the right direction it would be greatly appreciated!,QUESTION Find the dimensions of a rectangle with area m whose perimeter is as small as possible. MY WORK I think we are solving for : I didn't use the perimeter formula. I am not sure where I messed up. I think probably somewhere in my setup for the equation. If someone could take a look at my work and point me in the right direction it would be greatly appreciated!,"1000 ^2 \frac{dy}{dx} \begin{align*}
P &= (2x+2y)  \\
A &= (x\cdot y) \\
\frac{d}{dx}1000&=\frac{d}{dx}(x \cdot y) \\
0 &=\frac{d}{dx}((x)\prime(y)+(y)\prime(x)) \\
0 &=y+x\frac{dy}{dx}\\
\frac{dy}{dx} &=\frac{-y}{x}
\end{align*}","['calculus', 'derivatives', 'optimization', 'implicit-differentiation']"
71,Numerical differentiation with Binomial Theorem,Numerical differentiation with Binomial Theorem,,"In George Shilov's Elementary Real and Complex Analysis, there is a problem which asks us prove If $f$ is twice differentiable on some open interval and the second derivative is continuous at $x$ , then prove that $$f''(x)=\lim_{h\rightarrow 0}\frac{f(x)-2f(x+h)+f(x+2h)}{h^2}\,.$$ This is a common fact in numerical differentiation to approximate derivatives at the left-hand point and is fairly immediate from two applications of Taylor's Theorem with Lagrange Remainder. However, this was not the end of Shilov's problem. He also states Find a similar expression for $f^{(n)}(x)$ (with appropriate hypotheses). In the back of his book, he asserts that $$f^{(n)}(x)=\lim_{h\rightarrow 0}\frac{1}{h^n}\sum_{k=0}^n (-1)^k\binom{n}{k}f(x+kh)$$ which I found interesting enough to at least remember, if not attempt. However, I recently came upon an application where this formula would be useful and attempted to prove it. However, it seems there was an error in Shilov's claim. He must have meant $$(-1)^nf^{(n)}(x)=\lim_{h\rightarrow 0}\frac{1}{h^n}\sum_{k=0}^n (-1)^k\binom{n}{k}f(x+kh)$$ because working out $n=3$ and applying Lagrange's Remainder three times results in $$\frac{f(x)-3f(x+h)+3f(x+2h)-f(x+3h)}{h^3}=\frac{1}{3!}\left(-3f'''(\xi_1)+24f'''(\xi_2)-27f'''(x_3)\right)$$ which gives the corrected limit (with continuity of $f^{(3)}$ at $x$ assumed). Is there an easy way to go about proving this result in general? We can attack this fairly directly, without induction. But this becomes equivalent to proving several interesting binomial identities: $$\sum_{k=0}^n(-1)^k\binom{n}{k} k^m=\begin{cases} (-1)^n n!&\text{ if }m=n\\0&\text{ if }0\leq m<n\end{cases}$$ The first of which was tackled here while the others seem to have gone largely unasked. The case $m=1$ is tackled here and here , and I can see that I could continue the approaches taken in these answers by differentiating several times. The book-keeping isn't too awful because all these identities are just sums of $0$ s. Thus $0\leq m<n$ isn't too bad, if we can do $m=1$ . However, proving the cases $m=1$ and $m=n$ aren't entirely trivial. Shilov seems to have hidden an interesting exercise in a terse sentence without any hint that it would be interesting. This makes me wonder if there's an easier way to go about proving this result.","In George Shilov's Elementary Real and Complex Analysis, there is a problem which asks us prove If is twice differentiable on some open interval and the second derivative is continuous at , then prove that This is a common fact in numerical differentiation to approximate derivatives at the left-hand point and is fairly immediate from two applications of Taylor's Theorem with Lagrange Remainder. However, this was not the end of Shilov's problem. He also states Find a similar expression for (with appropriate hypotheses). In the back of his book, he asserts that which I found interesting enough to at least remember, if not attempt. However, I recently came upon an application where this formula would be useful and attempted to prove it. However, it seems there was an error in Shilov's claim. He must have meant because working out and applying Lagrange's Remainder three times results in which gives the corrected limit (with continuity of at assumed). Is there an easy way to go about proving this result in general? We can attack this fairly directly, without induction. But this becomes equivalent to proving several interesting binomial identities: The first of which was tackled here while the others seem to have gone largely unasked. The case is tackled here and here , and I can see that I could continue the approaches taken in these answers by differentiating several times. The book-keeping isn't too awful because all these identities are just sums of s. Thus isn't too bad, if we can do . However, proving the cases and aren't entirely trivial. Shilov seems to have hidden an interesting exercise in a terse sentence without any hint that it would be interesting. This makes me wonder if there's an easier way to go about proving this result.","f x f''(x)=\lim_{h\rightarrow 0}\frac{f(x)-2f(x+h)+f(x+2h)}{h^2}\,. f^{(n)}(x) f^{(n)}(x)=\lim_{h\rightarrow 0}\frac{1}{h^n}\sum_{k=0}^n (-1)^k\binom{n}{k}f(x+kh) (-1)^nf^{(n)}(x)=\lim_{h\rightarrow 0}\frac{1}{h^n}\sum_{k=0}^n (-1)^k\binom{n}{k}f(x+kh) n=3 \frac{f(x)-3f(x+h)+3f(x+2h)-f(x+3h)}{h^3}=\frac{1}{3!}\left(-3f'''(\xi_1)+24f'''(\xi_2)-27f'''(x_3)\right) f^{(3)} x \sum_{k=0}^n(-1)^k\binom{n}{k} k^m=\begin{cases} (-1)^n n!&\text{ if }m=n\\0&\text{ if }0\leq m<n\end{cases} m=1 0 0\leq m<n m=1 m=1 m=n","['calculus', 'real-analysis']"
72,Calculus u-substitution: Choosing between $\frac{dx}{du}$ and $\frac{du}{dx}$,Calculus u-substitution: Choosing between  and,\frac{dx}{du} \frac{du}{dx},"I searched and couldn't find any answers to this one. It's probably a dumb question but one that has been troubling me. Let's say I have an integration: $$\int \frac{1}{3x+2}$$ It seems the correct way to solve it is by choosing $u=3x+2$ . So $$\begin{align}u &= 3x + 2 \\ \frac{du}{dx} &= 3 \\ dx &= \frac{1}{3}du \end{align}$$ and so on the answer comes down to $\frac{1}{3}\ln(3x+2)$ My question is, why do I need to choose it specifically this way: $ \frac{du}{dx} = 3$ ? If I choose $ \frac{dx}{du} = 3$ , I get $dx=3du$ , but it gives me an answer different from the above. How do I choose which one I use as a denominator and nominator for my u substitution? Thank you.","I searched and couldn't find any answers to this one. It's probably a dumb question but one that has been troubling me. Let's say I have an integration: It seems the correct way to solve it is by choosing . So and so on the answer comes down to My question is, why do I need to choose it specifically this way: ? If I choose , I get , but it gives me an answer different from the above. How do I choose which one I use as a denominator and nominator for my u substitution? Thank you.","\int \frac{1}{3x+2} u=3x+2 \begin{align}u &= 3x + 2 \\
\frac{du}{dx} &= 3 \\
dx &= \frac{1}{3}du \end{align} \frac{1}{3}\ln(3x+2)  \frac{du}{dx} = 3  \frac{dx}{du} = 3 dx=3du","['calculus', 'integration', 'derivatives', 'substitution']"
73,How to find the value of the $20^\text{th}$ derivative of the function in concrete point?,How to find the value of the  derivative of the function in concrete point?,20^\text{th},"We have function $\arcsin(x)$. How to find it's  $20^\text{th}$ derivative in $x = 0$? Actually i don't have any idea except to get that derivatives manually one by one. Also, i've tried to get it with computer help, the function i get is something terrible. Also it can be solved with Leibniz formula, but it is too hard (to my mind).","We have function $\arcsin(x)$. How to find it's  $20^\text{th}$ derivative in $x = 0$? Actually i don't have any idea except to get that derivatives manually one by one. Also, i've tried to get it with computer help, the function i get is something terrible. Also it can be solved with Leibniz formula, but it is too hard (to my mind).",,['derivatives']
74,Gradient of the Rayleigh Quotient,Gradient of the Rayleigh Quotient,,"In the introduction part of the paper The Fast Convergence of Incremental PCA , the authors mention that the gradient of the Rayleigh quotient is equal to: $$ \triangledown G(v) = \frac{2}{\|v\|^2}(A - \frac{v^{T}Av}{v^{T}v} I_d)v $$ when the Rayleigh quotient is: $$G(v) = \frac{v^{T}Av}{v^{T}v}$$ ($v \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$) What are the steps to derive the given value for $\triangledown G(v)$ ? EDIT: As suggested by @Alex R. in a comment, I tried to proceed using the identity for a derivative of a quotient. I don't know/remember the matrix calculus identities to proceed. Here's what I tried: Let $N = v^{T}Av$ and $D = v^{T}v$, Then, $G'(v) = \frac{N'D-ND'}{D^{2}} \tag{1}\label{eq1}$ $D^2$ can be written as $(v^{T}v)^2 = (\|v\|^{2})^2$. Using $\frac{dx^{T}Ax}{dx} = x^{T}(A + A^{T})$, $N'$ can be written as $v^{T}(A+A^{T})$. $D' = 2v^{T}$. Plugging these values to $\eqref{eq1}$ yields: $$ G'(v) = \frac{v^{T}(A+A^{T})v^{T}v - v^{T}Av(2v^{T})}{\|v\|^4} $$ I can't figure out how to simplfy this. Any pointers to resources I should look are also appreciated.","In the introduction part of the paper The Fast Convergence of Incremental PCA , the authors mention that the gradient of the Rayleigh quotient is equal to: $$ \triangledown G(v) = \frac{2}{\|v\|^2}(A - \frac{v^{T}Av}{v^{T}v} I_d)v $$ when the Rayleigh quotient is: $$G(v) = \frac{v^{T}Av}{v^{T}v}$$ ($v \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$) What are the steps to derive the given value for $\triangledown G(v)$ ? EDIT: As suggested by @Alex R. in a comment, I tried to proceed using the identity for a derivative of a quotient. I don't know/remember the matrix calculus identities to proceed. Here's what I tried: Let $N = v^{T}Av$ and $D = v^{T}v$, Then, $G'(v) = \frac{N'D-ND'}{D^{2}} \tag{1}\label{eq1}$ $D^2$ can be written as $(v^{T}v)^2 = (\|v\|^{2})^2$. Using $\frac{dx^{T}Ax}{dx} = x^{T}(A + A^{T})$, $N'$ can be written as $v^{T}(A+A^{T})$. $D' = 2v^{T}$. Plugging these values to $\eqref{eq1}$ yields: $$ G'(v) = \frac{v^{T}(A+A^{T})v^{T}v - v^{T}Av(2v^{T})}{\|v\|^4} $$ I can't figure out how to simplfy this. Any pointers to resources I should look are also appreciated.",,"['derivatives', 'eigenvalues-eigenvectors', 'matrix-calculus']"
75,What is the derivative of this function: $\frac {d}{dx}x^{\lfloor{x}\rfloor}?$,What is the derivative of this function:,\frac {d}{dx}x^{\lfloor{x}\rfloor}?,"What is the derivative of the following function? $$\frac {d}{dx}x^{\lfloor{x}\rfloor}$$   Here, $\lfloor x \rfloor$ is the floor function. I tried: $$\frac {d}{dx} x^x=\frac {d}{dx} e^{x \ln x}=x^x (\ln x +1)+C$$ But, here $\lfloor{x}\rfloor$ is problematic for me.","What is the derivative of the following function? $$\frac {d}{dx}x^{\lfloor{x}\rfloor}$$   Here, $\lfloor x \rfloor$ is the floor function. I tried: $$\frac {d}{dx} x^x=\frac {d}{dx} e^{x \ln x}=x^x (\ln x +1)+C$$ But, here $\lfloor{x}\rfloor$ is problematic for me.",,"['calculus', 'derivatives']"
76,Derivatives of Infinite Product that Diverges to 0,Derivatives of Infinite Product that Diverges to 0,,"Consider the function given by $$f_n(x) = \prod\limits_{k=2}^n \left( 1-\frac{x}{k} \right).$$ Then for all $x \in (0,2)$, we have that $$\lim_{n\to\infty}\; f_n(x) = \prod\limits_{k=2}^\infty \left( 1-\frac{x}{k} \right) =0. $$ This follows from the fact that if $q_k \in [0,1)$ for all $k$, then $\prod\limits_{k=1}^{\infty} (1-q_k) = 0$ if and only if $\sum\limits_{k=1}^\infty q_k$ diverges (See for example here and here ). I am interested in the derivatives of this function $f_n$. For instance, let $f_n^{(j)}$ denote the $j$-th derivative of $f_n$. Is it true that $\lim_{n\to\infty} f_n^{(j)}(x) = 0$ for all $x\in(0,2)$? Considering the first derivative, we have that from the product rule: $$ f_n'(x) =  \sum\limits_{k=2}^n \frac{-1}{k} \prod\limits_{i \neq k} \left(1-\frac{x}{i} \right)\tag1 \label 1 .$$ Now, we have that  \begin{align}\prod\limits_{i \neq k} \left(1-\frac{x}{i} \right) &\leq \prod\limits_{i =2}^{n-1} \left(1-\frac{x}{i} \right)\\ &= \exp\left\{ \sum_{i=2}^{n-1} \log\left(1-\frac{x}{i} \right)  \right\} \\ &\leq \exp\left\{ \sum_{i=2}^{n-1} -\frac{x}{i} \right\} \tag2  \\ &\leq \exp\left\{-x\log(n)+x \right\}\tag3\\ &= {\left(\frac{e}{n}\right)}^x.\end{align} Where (2) above follows from the fact that $\log(1-y)\leq -y$ for all $y<1$, and (3) follows from $\sum\limits_{i=2}^{n-1}\frac{1}{i} \geq \log(n)-1$. Now, by noticing all the terms in the sum in $\ref1$ are nonpositive, we have the following \begin{align*}  f_n'(x) \geq {\left(\frac{e}{n}\right)}^x\sum\limits_{k=2}^n \frac{-1}{k} \geq -{\left(\frac{e}{n}\right)}^x \log(n) \end{align*} Finally, because $f_n'(x)\leq 0$ and $\lim_{n \to \infty}  {\left(\frac{e}{n}\right)}^x \log(n) = 0$, we are able to conclude that $\lim \; f_n'(x) = 0$ for all $x \in (0,2)$. Now, is there a way to show this for the $j$-th derivative $f_n^{(j)}$? I am unable to even currently show that the second derivative is zero (in my simulations it seems to approach zero, but at a rate slower than the first derivative). An idea that I have briefly considered using is considering the differentiation of the infinite product (see here and similarly here ). However in my case my infinite product diverges to zero, so I am not sure how useful this will be. Also, I am interested in the higher order derivatives of this infinite product, not just the first derivative. I appreciate any input on this problem!","Consider the function given by $$f_n(x) = \prod\limits_{k=2}^n \left( 1-\frac{x}{k} \right).$$ Then for all $x \in (0,2)$, we have that $$\lim_{n\to\infty}\; f_n(x) = \prod\limits_{k=2}^\infty \left( 1-\frac{x}{k} \right) =0. $$ This follows from the fact that if $q_k \in [0,1)$ for all $k$, then $\prod\limits_{k=1}^{\infty} (1-q_k) = 0$ if and only if $\sum\limits_{k=1}^\infty q_k$ diverges (See for example here and here ). I am interested in the derivatives of this function $f_n$. For instance, let $f_n^{(j)}$ denote the $j$-th derivative of $f_n$. Is it true that $\lim_{n\to\infty} f_n^{(j)}(x) = 0$ for all $x\in(0,2)$? Considering the first derivative, we have that from the product rule: $$ f_n'(x) =  \sum\limits_{k=2}^n \frac{-1}{k} \prod\limits_{i \neq k} \left(1-\frac{x}{i} \right)\tag1 \label 1 .$$ Now, we have that  \begin{align}\prod\limits_{i \neq k} \left(1-\frac{x}{i} \right) &\leq \prod\limits_{i =2}^{n-1} \left(1-\frac{x}{i} \right)\\ &= \exp\left\{ \sum_{i=2}^{n-1} \log\left(1-\frac{x}{i} \right)  \right\} \\ &\leq \exp\left\{ \sum_{i=2}^{n-1} -\frac{x}{i} \right\} \tag2  \\ &\leq \exp\left\{-x\log(n)+x \right\}\tag3\\ &= {\left(\frac{e}{n}\right)}^x.\end{align} Where (2) above follows from the fact that $\log(1-y)\leq -y$ for all $y<1$, and (3) follows from $\sum\limits_{i=2}^{n-1}\frac{1}{i} \geq \log(n)-1$. Now, by noticing all the terms in the sum in $\ref1$ are nonpositive, we have the following \begin{align*}  f_n'(x) \geq {\left(\frac{e}{n}\right)}^x\sum\limits_{k=2}^n \frac{-1}{k} \geq -{\left(\frac{e}{n}\right)}^x \log(n) \end{align*} Finally, because $f_n'(x)\leq 0$ and $\lim_{n \to \infty}  {\left(\frac{e}{n}\right)}^x \log(n) = 0$, we are able to conclude that $\lim \; f_n'(x) = 0$ for all $x \in (0,2)$. Now, is there a way to show this for the $j$-th derivative $f_n^{(j)}$? I am unable to even currently show that the second derivative is zero (in my simulations it seems to approach zero, but at a rate slower than the first derivative). An idea that I have briefly considered using is considering the differentiation of the infinite product (see here and similarly here ). However in my case my infinite product diverges to zero, so I am not sure how useful this will be. Also, I am interested in the higher order derivatives of this infinite product, not just the first derivative. I appreciate any input on this problem!",,"['derivatives', 'infinite-product']"
77,Length of the arc of the curve from the origin to the point.,Length of the arc of the curve from the origin to the point.,,"We have $\left \{ X(t),Y(t) \right \}, 1\leq t\leq \pi$ defined by the following two definite integral: $X(t)=\int_{1}^{t}\frac{\cos z}{z^2}dz, Y(t)=\int_{1}^{t}\frac{\sin z}{z^2}dz$ $L$ be the length of the arc of the curve from the origin to the point $P$ on the curve at which the tangent is perpendicular to the x-axis. Then, the $L$ will be equal to? I took the derivative of both $X(t),Y(t)$ using the Leibniz rule and then found the $\frac{dy}{dx}$ as follows: $\frac{dy}{dx}=\tan (t)$ I am thinking of using the following formula for the length of the curve: $\int_{0}^{a}\sqrt{1+f'(x)^2} dx$ But I am unable to find what will be the upper limit of the above integral. Any help to find the upper limit will be appreciated. Thanks in advance","We have $\left \{ X(t),Y(t) \right \}, 1\leq t\leq \pi$ defined by the following two definite integral: $X(t)=\int_{1}^{t}\frac{\cos z}{z^2}dz, Y(t)=\int_{1}^{t}\frac{\sin z}{z^2}dz$ $L$ be the length of the arc of the curve from the origin to the point $P$ on the curve at which the tangent is perpendicular to the x-axis. Then, the $L$ will be equal to? I took the derivative of both $X(t),Y(t)$ using the Leibniz rule and then found the $\frac{dy}{dx}$ as follows: $\frac{dy}{dx}=\tan (t)$ I am thinking of using the following formula for the length of the curve: $\int_{0}^{a}\sqrt{1+f'(x)^2} dx$ But I am unable to find what will be the upper limit of the above integral. Any help to find the upper limit will be appreciated. Thanks in advance",,"['derivatives', 'arc-length']"
78,Derivative visual meaning,Derivative visual meaning,,"Edit: I figured out on my own but here is question: Question: Find the visual meaning  of the numerator of:  $\lim\limits_{h \to 0} \frac{f(x+h)-2f(x)+f(x-h)}{h^2}$ So, I found this: Is this how the graph looks like?","Edit: I figured out on my own but here is question: Question: Find the visual meaning  of the numerator of:  $\lim\limits_{h \to 0} \frac{f(x+h)-2f(x)+f(x-h)}{h^2}$ So, I found this: Is this how the graph looks like?",,"['derivatives', 'graphing-functions']"
79,A good substitution to prove $\frac{f(b)-f(a)-(b-a)f'(a)}{g(b)-g(a)-(b-a)g'(a)}=\frac{f''(c)}{g''(c)}$. [duplicate],A good substitution to prove . [duplicate],\frac{f(b)-f(a)-(b-a)f'(a)}{g(b)-g(a)-(b-a)g'(a)}=\frac{f''(c)}{g''(c)},"This question already has an answer here : Prove that $a<c<b$, $\frac{f(b)-f(a)-(b-a)f'(a)}{g(b)-g(a)-(b-a)g'(a)}=\frac{f""(c)}{g""(c)}$ (1 answer) Closed 6 years ago . Let $f'(x)$ and $g'(x)$ satisfy the hypothesis of mean value theorem, then prove that $$\frac{f(b)-f(a)-(b-a)f'(a)}{g(b)-g(a)-(b-a)g'(a)}=\frac{f''(c)}{g''(c)}$$ where $g''(c)\neq 0$. I have tried various things like Cauchy MVT with $f'(x)$ and $g'(x)$, substituting $h(x)=\dfrac{1}{g'(x)}$ ,$h(x)=f'(x)+\dfrac{a}{g'(x)}$ where $a\in\mathbb{R}$, but it didn't lead anywhere. Any help will be appreciated. Please note that I am allowed to use only Cauchy's MVT , LMVT and Rolle 's theorem.","This question already has an answer here : Prove that $a<c<b$, $\frac{f(b)-f(a)-(b-a)f'(a)}{g(b)-g(a)-(b-a)g'(a)}=\frac{f""(c)}{g""(c)}$ (1 answer) Closed 6 years ago . Let $f'(x)$ and $g'(x)$ satisfy the hypothesis of mean value theorem, then prove that $$\frac{f(b)-f(a)-(b-a)f'(a)}{g(b)-g(a)-(b-a)g'(a)}=\frac{f''(c)}{g''(c)}$$ where $g''(c)\neq 0$. I have tried various things like Cauchy MVT with $f'(x)$ and $g'(x)$, substituting $h(x)=\dfrac{1}{g'(x)}$ ,$h(x)=f'(x)+\dfrac{a}{g'(x)}$ where $a\in\mathbb{R}$, but it didn't lead anywhere. Any help will be appreciated. Please note that I am allowed to use only Cauchy's MVT , LMVT and Rolle 's theorem.",,"['calculus', 'derivatives']"
80,Proving every derivative of $\sqrt\cos x$ is unbounded?,Proving every derivative of  is unbounded?,\sqrt\cos x,"This question is related to one I posted earlier today. I am $99$% sure the claim is true, and I can of course prove it for the first two or three derivatives, but I don't know how to jump to the infinite case. Is induction of some kind in order? I'm not sure how to proceed.","This question is related to one I posted earlier today. I am $99$% sure the claim is true, and I can of course prove it for the first two or three derivatives, but I don't know how to jump to the infinite case. Is induction of some kind in order? I'm not sure how to proceed.",,"['calculus', 'real-analysis', 'derivatives']"
81,"Is a Gaussian ""steepest"" at 1-sigma?","Is a Gaussian ""steepest"" at 1-sigma?",,"If I take the 2nd derivative of a Gaussian $e^{-\frac{x^2}{2\sigma^2}}$ and set it equal to $0$, the inflection point is $x=\pm \sigma$. Is $1$-$\sigma$ the point at which the Gaussian curve is ""steepest""?","If I take the 2nd derivative of a Gaussian $e^{-\frac{x^2}{2\sigma^2}}$ and set it equal to $0$, the inflection point is $x=\pm \sigma$. Is $1$-$\sigma$ the point at which the Gaussian curve is ""steepest""?",,"['calculus', 'derivatives']"
82,Differential Operator Issue,Differential Operator Issue,,"Let us consider the differential operator $H:= x \frac{d}{dx}$ and let us define \begin{equation} \hat{\mathcal{O}_n}= \frac{1}{n!}(H+n)(H+n-1)\cdots(H+2)(H+1). \end{equation} I proved - by induction - that  \begin{equation} \hat{\mathcal{O}_n}\left ( \frac{\log x}{x} \right )=\frac{1}{nx} \end{equation} I notice that something similar happens with the following: \begin{gather*} \frac{\log(x)}{x^2}; \\ \frac{\log(x)}{x^3}. \end{gather*} In particular: \begin{equation} \hat{\mathcal{O}_n}\left ( \frac{\log x}{x^2} \right )=-\frac{1}{n(n-1)x^2} \end{equation} for $n \ge 2$, and \begin{equation} \hat{\mathcal{O}_n}\left ( \frac{\log x}{x^3} \right )=\frac{2}{n(n-1)(n-2)x^3} \end{equation} So I think that, generally, the following statement is TRUE. Statement (Conjecture). Let us consider the differential operator $\hat{\mathcal{O}_n}$, then \begin{equation} \hat{\mathcal{O}_n}\left ( \frac{\log x}{x^m} \right )=(-1)^{m+1}\frac{(m-1)!}{n(n-1)(n-2) \cdots (n-m+1)x^m} \end{equation} with $n \in \mathbb{N}$, $n \ge m$. Anyone have any idea how it can be proved?","Let us consider the differential operator $H:= x \frac{d}{dx}$ and let us define \begin{equation} \hat{\mathcal{O}_n}= \frac{1}{n!}(H+n)(H+n-1)\cdots(H+2)(H+1). \end{equation} I proved - by induction - that  \begin{equation} \hat{\mathcal{O}_n}\left ( \frac{\log x}{x} \right )=\frac{1}{nx} \end{equation} I notice that something similar happens with the following: \begin{gather*} \frac{\log(x)}{x^2}; \\ \frac{\log(x)}{x^3}. \end{gather*} In particular: \begin{equation} \hat{\mathcal{O}_n}\left ( \frac{\log x}{x^2} \right )=-\frac{1}{n(n-1)x^2} \end{equation} for $n \ge 2$, and \begin{equation} \hat{\mathcal{O}_n}\left ( \frac{\log x}{x^3} \right )=\frac{2}{n(n-1)(n-2)x^3} \end{equation} So I think that, generally, the following statement is TRUE. Statement (Conjecture). Let us consider the differential operator $\hat{\mathcal{O}_n}$, then \begin{equation} \hat{\mathcal{O}_n}\left ( \frac{\log x}{x^m} \right )=(-1)^{m+1}\frac{(m-1)!}{n(n-1)(n-2) \cdots (n-m+1)x^m} \end{equation} with $n \in \mathbb{N}$, $n \ge m$. Anyone have any idea how it can be proved?",,"['derivatives', 'proof-verification', 'induction', 'differential-operators']"
83,Derivative of unit step function?,Derivative of unit step function?,,How would I find the derivative of a unit step function? I understand that the unit impulse function will be used but I'm not sure how to use it. I am trying to find the derivative of this: $v(t) = u(t+1) - 2u(t) + u(t-1)$ $u(t) = 0$  when $t < 0$ $u(t) = 1$ when $t > 0$ The relationship between unit step function and impulse function: δ(n) = u(n) - u(n-1) $ δ(t)=du(t)/dt $,How would I find the derivative of a unit step function? I understand that the unit impulse function will be used but I'm not sure how to use it. I am trying to find the derivative of this: $v(t) = u(t+1) - 2u(t) + u(t-1)$ $u(t) = 0$  when $t < 0$ $u(t) = 1$ when $t > 0$ The relationship between unit step function and impulse function: δ(n) = u(n) - u(n-1) $ δ(t)=du(t)/dt $,,"['calculus', 'derivatives']"
84,How do I calculate the derivative of a derivative w.r.t. another derivative?,How do I calculate the derivative of a derivative w.r.t. another derivative?,,"The question is a bit of a mouthful. In Classical Mechanics by Goldstein I have seen the use of the following: $$ \frac{d\dot{F}}{d\dot{q}_i} = \frac{dF}{dq_i} $$ where $F = F(q_1,q_2,...,q_n;t)$, $ \dot{F} = \frac{dF}{dt} $ and $ \dot{q_i} = \frac{dq_i}{dt} $ . How would I show this? The use of this result is used on page 17 of the PDF solutions guide (Page 10 within the derivations chapter) by showing that the transformation $L'(q,\dot{q},t) = L(q,\dot{q},t) + \frac{dF(q,t)}{dt}$ keeps the form of the Euler-Lagrange equation invariant: http://www.slideshare.net/venuatsrr/solution-manual-classical-mechanics-goldstein","The question is a bit of a mouthful. In Classical Mechanics by Goldstein I have seen the use of the following: $$ \frac{d\dot{F}}{d\dot{q}_i} = \frac{dF}{dq_i} $$ where $F = F(q_1,q_2,...,q_n;t)$, $ \dot{F} = \frac{dF}{dt} $ and $ \dot{q_i} = \frac{dq_i}{dt} $ . How would I show this? The use of this result is used on page 17 of the PDF solutions guide (Page 10 within the derivations chapter) by showing that the transformation $L'(q,\dot{q},t) = L(q,\dot{q},t) + \frac{dF(q,t)}{dt}$ keeps the form of the Euler-Lagrange equation invariant: http://www.slideshare.net/venuatsrr/solution-manual-classical-mechanics-goldstein",,"['derivatives', 'euler-lagrange-equation']"
85,"Has $(\max(0, x))^2$ continuous first derivative?",Has  continuous first derivative?,"(\max(0, x))^2","At first glance $(\max(0, x))^2$ looks smooth. If I'm not mistaken: if $x \le 0$ the derivative equals $2 \times\max(0, x) \times0 = 0$ if $x \ge 0$ the derivative equals $2 \times \max(0, x) \times 1$ and it equal zero if $x = 0$ Looks like derivative of $(\max(0, x))^2$ is continuous. But when I tried to check with Wolfram Alpha it shows me that derivative of $(\max(0, x))^2$ is indeterminate at $x = 0$. Link So can anyone tell me where I made a mistake?","At first glance $(\max(0, x))^2$ looks smooth. If I'm not mistaken: if $x \le 0$ the derivative equals $2 \times\max(0, x) \times0 = 0$ if $x \ge 0$ the derivative equals $2 \times \max(0, x) \times 1$ and it equal zero if $x = 0$ Looks like derivative of $(\max(0, x))^2$ is continuous. But when I tried to check with Wolfram Alpha it shows me that derivative of $(\max(0, x))^2$ is indeterminate at $x = 0$. Link So can anyone tell me where I made a mistake?",,"['calculus', 'derivatives']"
86,A fact on C1 functions,A fact on C1 functions,,"Consider $f:[0,1]\rightarrow \mathbb{R}$ and define  \begin{equation} \sigma_n(f):=\frac{1}{n}\sum_{k=1}^{n}f \left(\frac{k}{n}\right) \end{equation} for $n=1,2,\dots$ I'm trying to prove that if $f\in C^1([0,1])$, then there exists a constant (depending on the function $f$) $a(f)$ such that \begin{equation} \sigma_n(f)=a+\frac{\sigma_n(f')}{2n}+o\left(\frac{1}{n}\right). \end{equation} I have tried to use Taylor' series about $\frac{k-1}{n}$ for $k=1,\dots,n$ but I can't figure it out...it seems to be not working...","Consider $f:[0,1]\rightarrow \mathbb{R}$ and define  \begin{equation} \sigma_n(f):=\frac{1}{n}\sum_{k=1}^{n}f \left(\frac{k}{n}\right) \end{equation} for $n=1,2,\dots$ I'm trying to prove that if $f\in C^1([0,1])$, then there exists a constant (depending on the function $f$) $a(f)$ such that \begin{equation} \sigma_n(f)=a+\frac{\sigma_n(f')}{2n}+o\left(\frac{1}{n}\right). \end{equation} I have tried to use Taylor' series about $\frac{k-1}{n}$ for $k=1,\dots,n$ but I can't figure it out...it seems to be not working...",,"['derivatives', 'taylor-expansion']"
87,$2$ points on a curve have a common tangent,points on a curve have a common tangent,2,"Let $2$ points $(x_1,y_1)$ and $(x_2,y_2)$ on the curve $y=x^4-2x^2-x$ have a common tangent line. Find the value of $|x_1|+|x_2|+|y_1|+|y_2|$. It seems to me that I a missing a link and hence the problem appears incomplete to me. Is there something which I might be missing? Thanks.","Let $2$ points $(x_1,y_1)$ and $(x_2,y_2)$ on the curve $y=x^4-2x^2-x$ have a common tangent line. Find the value of $|x_1|+|x_2|+|y_1|+|y_2|$. It seems to me that I a missing a link and hence the problem appears incomplete to me. Is there something which I might be missing? Thanks.",,['derivatives']
88,Problem in understanding the concept of differentiation.,Problem in understanding the concept of differentiation.,,"I have just completed the chapter differentiation. But I still have confusion in understanding the concept of it.I observe a fact that if we can draw a tangent to a curve at any point of it or in other words if it is possible to make the best linear approximation to a curve at any point then it is said that the curve or equivalently the function is differentiable at that point.But I fail to relate this concept to various other facts of differentiability like velocity or acceleration.Why do we consider the best linear approximation of the displacement curve, traversed by a particle, at any time t as the velocity of the particle at time t. I have similar confusion regarding acceleration. Please help me in understanding these important facts reminding that I am a new to this subject.Thank you in advance.","I have just completed the chapter differentiation. But I still have confusion in understanding the concept of it.I observe a fact that if we can draw a tangent to a curve at any point of it or in other words if it is possible to make the best linear approximation to a curve at any point then it is said that the curve or equivalently the function is differentiable at that point.But I fail to relate this concept to various other facts of differentiability like velocity or acceleration.Why do we consider the best linear approximation of the displacement curve, traversed by a particle, at any time t as the velocity of the particle at time t. I have similar confusion regarding acceleration. Please help me in understanding these important facts reminding that I am a new to this subject.Thank you in advance.",,[]
89,100th derivative of $(1-2x)^{2/3}$ at point $x=0$,100th derivative of  at point,(1-2x)^{2/3} x=0,$$\frac{\mathrm d^{100}}{\mathrm dx^{100}} (1-2x)^{2/3}$$ Without Taylor. I relay don't have any idea how to use General Leibniz rule in this case.,$$\frac{\mathrm d^{100}}{\mathrm dx^{100}} (1-2x)^{2/3}$$ Without Taylor. I relay don't have any idea how to use General Leibniz rule in this case.,,"['calculus', 'real-analysis', 'derivatives']"
90,First derivative meaning in this case,First derivative meaning in this case,,"If we have a function: $$f(x)=\frac{x}{2}+\arcsin{\frac{2x}{1+x^2}}$$ And it's first derivative is calculated as: $$f'(x)=\frac{1}{2}+\frac{1}{\sqrt{1-\big(\frac{2x}{1+x^2}\big)^2}}\frac{2+2x^2-4x^2}{(1+x^2)^2}=$$ $$\frac{1}{2}+\frac{2(1-x^2)}{\sqrt{\frac{(1-x^2)^2}{(1+x^2)^2}}\cdot(1+x^2)^2}=$$ $$\frac{1}{2}+\frac{2(1-x^2)}{|1-x^2|\cdot(1+x^2)}=$$ $$\frac{1}{2}+\frac{2\cdot sgn(1-x^2)}{1+x^2}$$ Why did my teacher say the critical points are at $x=\pm1$? Then she wrote: $$f_+'(1)=-\frac{1}{2}$$ $$f_-'(1)=\frac{3}{2}$$ and I'm not sure what she meant by that eather, nor by the following: $$f'(x)= \begin{cases} \frac{x^2-3}{2(x^2+1)}, & |x|>1 \\ \frac{x^2+5}{2(x^2+1)}, & |x|<1 \end{cases}$$ Thank you for your time.","If we have a function: $$f(x)=\frac{x}{2}+\arcsin{\frac{2x}{1+x^2}}$$ And it's first derivative is calculated as: $$f'(x)=\frac{1}{2}+\frac{1}{\sqrt{1-\big(\frac{2x}{1+x^2}\big)^2}}\frac{2+2x^2-4x^2}{(1+x^2)^2}=$$ $$\frac{1}{2}+\frac{2(1-x^2)}{\sqrt{\frac{(1-x^2)^2}{(1+x^2)^2}}\cdot(1+x^2)^2}=$$ $$\frac{1}{2}+\frac{2(1-x^2)}{|1-x^2|\cdot(1+x^2)}=$$ $$\frac{1}{2}+\frac{2\cdot sgn(1-x^2)}{1+x^2}$$ Why did my teacher say the critical points are at $x=\pm1$? Then she wrote: $$f_+'(1)=-\frac{1}{2}$$ $$f_-'(1)=\frac{3}{2}$$ and I'm not sure what she meant by that eather, nor by the following: $$f'(x)= \begin{cases} \frac{x^2-3}{2(x^2+1)}, & |x|>1 \\ \frac{x^2+5}{2(x^2+1)}, & |x|<1 \end{cases}$$ Thank you for your time.",,"['calculus', 'derivatives']"
91,What does a power of $|\nabla|$ mean in the context of PDE?,What does a power of  mean in the context of PDE?,|\nabla|,"I've seen the notation $|\nabla|^\alpha f$ used in a PDE setting, where $f$ is some function on $\mathbb R^n$. Could someone tell me what that means? For example in this discussion on Math Overflow the first answer uses this notation https://mathoverflow.net/questions/48292/applications-of-hardys-inequality","I've seen the notation $|\nabla|^\alpha f$ used in a PDE setting, where $f$ is some function on $\mathbb R^n$. Could someone tell me what that means? For example in this discussion on Math Overflow the first answer uses this notation https://mathoverflow.net/questions/48292/applications-of-hardys-inequality",,"['derivatives', 'partial-differential-equations', 'notation']"
92,What is the 90th derivative of $\cos(x^5)$ where x = 0?,What is the 90th derivative of  where x = 0?,\cos(x^5),"Trying to figure out how to calculate the 90th derivative of $\cos(x^5)$ evaluated at 0.  This is what I tried, but I guess I must have done something wrong or am not understanding something fundamental: $\cos(x) = \displaystyle\sum_{n=0}^\infty \dfrac{(-1)^n}{(2n)!} {(x)}^{2n}$ $\cos(x^5) = \displaystyle\sum_{n=0}^\infty \dfrac{(-1)^n}{(2n)!} {(x^5)}^{2n}=$ $1-\dfrac{{x^{5\cdot2}}}{2!}+\dfrac{{x^{5\cdot4}}}{4!}-\dfrac{{x^{5\cdot6}}}{6!}+...-\dfrac{{x^{5\cdot18}}}{18!}+...-\dfrac{{x^{5\cdot90}}}{90!}+...+\dfrac{{x^{5\cdot190}}}{180!}+...$ $f(x) = \displaystyle\sum_{n=0}^\infty \dfrac{f^{(n)}(0)}{n!} \cdot {x^{n}} $ $\dfrac{f^{90}(0)}{90!}\cdot {{x^{90}}} = -\dfrac{{x^{5\cdot18}}}{18!}$ ${f^{90}(0)} = -\dfrac{90!}{18!}$ Wolfram has it at some really large negative number.","Trying to figure out how to calculate the 90th derivative of $\cos(x^5)$ evaluated at 0.  This is what I tried, but I guess I must have done something wrong or am not understanding something fundamental: $\cos(x) = \displaystyle\sum_{n=0}^\infty \dfrac{(-1)^n}{(2n)!} {(x)}^{2n}$ $\cos(x^5) = \displaystyle\sum_{n=0}^\infty \dfrac{(-1)^n}{(2n)!} {(x^5)}^{2n}=$ $1-\dfrac{{x^{5\cdot2}}}{2!}+\dfrac{{x^{5\cdot4}}}{4!}-\dfrac{{x^{5\cdot6}}}{6!}+...-\dfrac{{x^{5\cdot18}}}{18!}+...-\dfrac{{x^{5\cdot90}}}{90!}+...+\dfrac{{x^{5\cdot190}}}{180!}+...$ $f(x) = \displaystyle\sum_{n=0}^\infty \dfrac{f^{(n)}(0)}{n!} \cdot {x^{n}} $ $\dfrac{f^{90}(0)}{90!}\cdot {{x^{90}}} = -\dfrac{{x^{5\cdot18}}}{18!}$ ${f^{90}(0)} = -\dfrac{90!}{18!}$ Wolfram has it at some really large negative number.",,"['calculus', 'trigonometry', 'derivatives', 'taylor-expansion']"
93,Intution: second derivative corresponds to curvature?,Intution: second derivative corresponds to curvature?,,"What's the intuition for understanding that the second derivative does correspond to the curvature of the function? For first derivatives one can think of the tangent lines, but what to think for second derivatives?","What's the intuition for understanding that the second derivative does correspond to the curvature of the function? For first derivatives one can think of the tangent lines, but what to think for second derivatives?",,"['derivatives', 'curvature']"
94,Formalizing differation of a function $f$ by function $g$,Formalizing differation of a function  by function,f g,"Sometimes (usually in physics) i encounter expressions such as $\dfrac{d}{dg(x)} f(x)$ This expression is quite intuitive: what is the change in $f(x)$ with respect to change in $g(x)$? How can i formalize this in terms of limits? Is it the same as: $$\dfrac{d}{dg(x)} f(x)\big|_{x=x_0} = \lim_{g(x)\rightarrow g(x_0)}\dfrac{f(g(x))-f(g(x_0))}{g(x)-g(x_0)} $$ If so, this looks like the ""usual"" differation of composite function by $x$, but i cant really figure out the expression $g(x)\rightarrow g(x_0)$. -How does $g(x)$ tend to $g(x_0)$? -Does $g(x)$ need to be 1-1 map? For example, if we look at $r:\mathbb R\rightarrow \mathbb R $ defined by: $r(t) = \sqrt{x^2(t)+y^2(t)+z^2(t)}$ What is $\dfrac{dr(t)}{d\dot r(t)}$?","Sometimes (usually in physics) i encounter expressions such as $\dfrac{d}{dg(x)} f(x)$ This expression is quite intuitive: what is the change in $f(x)$ with respect to change in $g(x)$? How can i formalize this in terms of limits? Is it the same as: $$\dfrac{d}{dg(x)} f(x)\big|_{x=x_0} = \lim_{g(x)\rightarrow g(x_0)}\dfrac{f(g(x))-f(g(x_0))}{g(x)-g(x_0)} $$ If so, this looks like the ""usual"" differation of composite function by $x$, but i cant really figure out the expression $g(x)\rightarrow g(x_0)$. -How does $g(x)$ tend to $g(x_0)$? -Does $g(x)$ need to be 1-1 map? For example, if we look at $r:\mathbb R\rightarrow \mathbb R $ defined by: $r(t) = \sqrt{x^2(t)+y^2(t)+z^2(t)}$ What is $\dfrac{dr(t)}{d\dot r(t)}$?",,"['real-analysis', 'derivatives', 'coordinate-systems']"
95,Signal differentiation by wavelet transform,Signal differentiation by wavelet transform,,"I have a noisy signal x(t) that I want to differentiate in time, in order to obtain x'(t). Though numerical finite difference approximantion does not work well on noisy signal, I would like to differentiate its continuous wavelet transform (cwt) and to reconstruct the derivative from its wavelet transform. Is it possible to do so? Given the wavelet transform of the original signal, how do I differentiate it in order to get the wavelet transform of its derivative? I am using complex Morlet wavelet. Please bear in mind that I no matematician, so please be as human as you can in your answer.","I have a noisy signal x(t) that I want to differentiate in time, in order to obtain x'(t). Though numerical finite difference approximantion does not work well on noisy signal, I would like to differentiate its continuous wavelet transform (cwt) and to reconstruct the derivative from its wavelet transform. Is it possible to do so? Given the wavelet transform of the original signal, how do I differentiate it in order to get the wavelet transform of its derivative? I am using complex Morlet wavelet. Please bear in mind that I no matematician, so please be as human as you can in your answer.",,"['derivatives', 'wavelets']"
96,Function that is continuous and its differential is continuous,Function that is continuous and its differential is continuous,,"Let $ f: \mathbb{R}  \rightarrow \mathbb{R}$ . Show that $f$ is continuously differentiable if and only if,  for every $x \in \mathbb{R}$ there exists a $l \in \mathbb{R}$ with the property that for all $\epsilon >0$ there exists a $\delta>0$ , such that for all $h,t, \in B(0,\delta)$ the following holds: $$ |f(x+h) -f(x+t) - l(h-t)| \leq \epsilon |h-t| $$ I've been staring at this exercise for way too long and still have no clue on how to proof this. The statement looks very much like the definition of the differential but I don't see how to get both a $h$ and a $t$ in my expression. Furthermore I don't see how the continuously differentiable should be used to proof this property. I tried to use the mean value theorem but this makes it all to confusing for me as I suppose I would need the limit definition but then I don't see how to get rid of it.","Let . Show that is continuously differentiable if and only if,  for every there exists a with the property that for all there exists a , such that for all the following holds: I've been staring at this exercise for way too long and still have no clue on how to proof this. The statement looks very much like the definition of the differential but I don't see how to get both a and a in my expression. Furthermore I don't see how the continuously differentiable should be used to proof this property. I tried to use the mean value theorem but this makes it all to confusing for me as I suppose I would need the limit definition but then I don't see how to get rid of it."," f: \mathbb{R}  \rightarrow \mathbb{R} f x \in \mathbb{R} l \in \mathbb{R} \epsilon >0 \delta>0 h,t, \in B(0,\delta)  |f(x+h) -f(x+t) - l(h-t)| \leq \epsilon |h-t|  h t","['real-analysis', 'derivatives']"
97,Prove $f$ is derivable and find $f'(0)$,Prove  is derivable and find,f f'(0),"I'm stuck solving the following problem: Problem: Prove there exists an unique function $f : \mathbb{R} \to \mathbb{R}$ such that: $$2(f(x))^3 - 3(f(x))^2 + 6f(x) = x \quad \forall x \in \mathbb{R}$$ Also prove $f$ is derivable in $\mathbb{R}$ and calculate $f'(0)$. I have been able to prove the existence and uniqueness (see below) but I don't know how to prove $f$ is derivable or calculate $f'(0)$. How can I prove it? Proof of existence and uniqueness: Let $a \in \mathbb{R}$. The equation $$2x^3 - 3x^2 + 6x = a \qquad (\ast)$$ has at least one solution since $p(x) = 2x^3 - 3x^2 + 6x$ is an odd-degree polynomial. Suppose there were two different solutions $x_1, x_2$ for $(\ast)$. Then, by Rolle's theorem there exists $c \in (x_1, x_2)$ such that: $$p'(c) = 6x^2 - 6x + 6 = 0$$ But $\Delta_{p'} = 6^2 - 4 \cdot 6^2 < 0$, so $p'(c) = 0$ has no real solutions, which is a contradiction. Hence, there exists an unique solution for $(\ast)$. Finally, we can define $f(a) = x$, where $x$ is the solution to $(\ast)$.","I'm stuck solving the following problem: Problem: Prove there exists an unique function $f : \mathbb{R} \to \mathbb{R}$ such that: $$2(f(x))^3 - 3(f(x))^2 + 6f(x) = x \quad \forall x \in \mathbb{R}$$ Also prove $f$ is derivable in $\mathbb{R}$ and calculate $f'(0)$. I have been able to prove the existence and uniqueness (see below) but I don't know how to prove $f$ is derivable or calculate $f'(0)$. How can I prove it? Proof of existence and uniqueness: Let $a \in \mathbb{R}$. The equation $$2x^3 - 3x^2 + 6x = a \qquad (\ast)$$ has at least one solution since $p(x) = 2x^3 - 3x^2 + 6x$ is an odd-degree polynomial. Suppose there were two different solutions $x_1, x_2$ for $(\ast)$. Then, by Rolle's theorem there exists $c \in (x_1, x_2)$ such that: $$p'(c) = 6x^2 - 6x + 6 = 0$$ But $\Delta_{p'} = 6^2 - 4 \cdot 6^2 < 0$, so $p'(c) = 0$ has no real solutions, which is a contradiction. Hence, there exists an unique solution for $(\ast)$. Finally, we can define $f(a) = x$, where $x$ is the solution to $(\ast)$.",,"['calculus', 'derivatives']"
98,"$f_x(x,y)=f_y(x,y)$ for all $(x,y)\in\mathbb{R}^2 \iff f(x,y)=f(0,x+y)$",for all,"f_x(x,y)=f_y(x,y) (x,y)\in\mathbb{R}^2 \iff f(x,y)=f(0,x+y)","Let $f:\mathbb{R}^2\to \mathbb{R}$ be continuously differentiable. I want to prove: $f_x(x,y)=f_y(x,y)$ for all $(x,y)\in\mathbb{R}^2 \iff f(x,y)=f(0,x+y)$ for all $(x,y)\in\mathbb{R}^2$. For this <= direction I have to derivate f and calculate $f_x(x,y)$ and $f_y(x,y)$, but I'm not sure how to calculate $f_x(0,x+y)$ for example. What is $f_x(0,x+y)$?. How to prove this => direction? Maybe I have to integrate, but I don't know how to do it exactly here:(","Let $f:\mathbb{R}^2\to \mathbb{R}$ be continuously differentiable. I want to prove: $f_x(x,y)=f_y(x,y)$ for all $(x,y)\in\mathbb{R}^2 \iff f(x,y)=f(0,x+y)$ for all $(x,y)\in\mathbb{R}^2$. For this <= direction I have to derivate f and calculate $f_x(x,y)$ and $f_y(x,y)$, but I'm not sure how to calculate $f_x(0,x+y)$ for example. What is $f_x(0,x+y)$?. How to prove this => direction? Maybe I have to integrate, but I don't know how to do it exactly here:(",,"['real-analysis', 'derivatives']"
99,Asymmetric second difference quotient?,Asymmetric second difference quotient?,,"I need to find (approximate) the second derivative of a discrete function. Usually I would approximate the second derivative with $$f''(x)\approx\frac{f(x+h)-2f(x)+f(x-h)}{h^2}\tag{1}$$ In my case, however, the $x$ values at which $f$ is known are distributed non-uniformly. So the ""forward"" $h$ and ""backward"" $h$ can be different $h_1\neq h_2$. Intuitively I would go with something like $$f''(x)\approx\frac{f(x+h_1)-2f(x)+f(x-h_2)}{h_1h_2}\tag{2}$$ but when I start from the forward and backward difference I end up with $$f''(x)\approx\frac{\frac{f(x+h_1)-f(x)}{h_1}-\frac{f(x)-f(x-h_2)}{h_2}}{?}$$ and I don't know what to put in the quotient. I guess the arithmetic mean $\frac{h_1+h_2}{2}$ would be a good choice but that leads to $$f''(x)\approx\frac{\frac{2h_2}{h_1+h_2}f(x+h_1)-2f(x)+\frac{2h_1}{h_1+h_2}f(x-h_2)}{h_1h_2}\tag{3}$$ which for $h_1=h_2$ simplifies to $(1)$ but is not the same as $(2)$. Is $(2)$ or $(3)$ the right way to do this or is there another way?","I need to find (approximate) the second derivative of a discrete function. Usually I would approximate the second derivative with $$f''(x)\approx\frac{f(x+h)-2f(x)+f(x-h)}{h^2}\tag{1}$$ In my case, however, the $x$ values at which $f$ is known are distributed non-uniformly. So the ""forward"" $h$ and ""backward"" $h$ can be different $h_1\neq h_2$. Intuitively I would go with something like $$f''(x)\approx\frac{f(x+h_1)-2f(x)+f(x-h_2)}{h_1h_2}\tag{2}$$ but when I start from the forward and backward difference I end up with $$f''(x)\approx\frac{\frac{f(x+h_1)-f(x)}{h_1}-\frac{f(x)-f(x-h_2)}{h_2}}{?}$$ and I don't know what to put in the quotient. I guess the arithmetic mean $\frac{h_1+h_2}{2}$ would be a good choice but that leads to $$f''(x)\approx\frac{\frac{2h_2}{h_1+h_2}f(x+h_1)-2f(x)+\frac{2h_1}{h_1+h_2}f(x-h_2)}{h_1h_2}\tag{3}$$ which for $h_1=h_2$ simplifies to $(1)$ but is not the same as $(2)$. Is $(2)$ or $(3)$ the right way to do this or is there another way?",,"['derivatives', 'approximation']"
