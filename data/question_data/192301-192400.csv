,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,differential calculus in $\mathbb{R}^\mathbb{N}$?,differential calculus in ?,\mathbb{R}^\mathbb{N},"is it possible to define the derivative of a function of countable variables? I found differential calculus of function with a finite number of variables, or differential calculus in Banach spaces (uncountable number of dimensions) . but what can be done in countable dimensions? functions of variables in  $\mathbb{R}^\mathbb{N}$?","is it possible to define the derivative of a function of countable variables? I found differential calculus of function with a finite number of variables, or differential calculus in Banach spaces (uncountable number of dimensions) . but what can be done in countable dimensions? functions of variables in  $\mathbb{R}^\mathbb{N}$?",,"['differential-geometry', 'derivatives']"
1,Prove the Fundamental Theorem of Calculus,Prove the Fundamental Theorem of Calculus,,"Prove the Fundamental Theorem of Calculus with this hypothesis: If $f$ is integrable over $[a,b]$, if $g:[a,b]\rightarrow\Bbb R$ given by $g(x)=\int_{a}^{x}f(t)dt$ and $f$ is continuous in $x_0 \in [a,b] \implies g'(x_0)=f(x_0)\Rightarrow g'(x_0)-f(x_0) = 0 $ How can this be proven?","Prove the Fundamental Theorem of Calculus with this hypothesis: If $f$ is integrable over $[a,b]$, if $g:[a,b]\rightarrow\Bbb R$ given by $g(x)=\int_{a}^{x}f(t)dt$ and $f$ is continuous in $x_0 \in [a,b] \implies g'(x_0)=f(x_0)\Rightarrow g'(x_0)-f(x_0) = 0 $ How can this be proven?",,"['calculus', 'integration', 'derivatives', 'proof-verification']"
2,Finding the tangent line to a curve,Finding the tangent line to a curve,,"Find an equation for the tangent line to the curve $$x\sin(xy-y^2)=x^2-1$$ through the point $(1,1)$.","Find an equation for the tangent line to the curve $$x\sin(xy-y^2)=x^2-1$$ through the point $(1,1)$.",,['derivatives']
3,Taylor expansion,Taylor expansion,,"Is there an easier way to do a Taylor expansion of $e^{u^2+u}$ than do derivatives or substitute and then use Newton's binomial? For example, expanding until the $4$th term: $$e^{u^2+u}=1+u^2+u+ \frac{(u^2+u)^2}{2!} + \frac{(u^2+u)^3}{3!}+\frac{(u^2+u)^4}{4!}$$ $$e^{u^2+u}=1+u+\frac{3}{2}u^2+\frac{7}{6}u^3+\frac{25}{24}u^4+H.O.T.$$ I thought of using the $n$th derivative formula on $e^{u^2}\cdot e^u$ $$(f(x)\cdot g(x))^{(n)}=\sum_{n=0}^n{{n\choose k}f^{k}g^{n-k}}.$$","Is there an easier way to do a Taylor expansion of $e^{u^2+u}$ than do derivatives or substitute and then use Newton's binomial? For example, expanding until the $4$th term: $$e^{u^2+u}=1+u^2+u+ \frac{(u^2+u)^2}{2!} + \frac{(u^2+u)^3}{3!}+\frac{(u^2+u)^4}{4!}$$ $$e^{u^2+u}=1+u+\frac{3}{2}u^2+\frac{7}{6}u^3+\frac{25}{24}u^4+H.O.T.$$ I thought of using the $n$th derivative formula on $e^{u^2}\cdot e^u$ $$(f(x)\cdot g(x))^{(n)}=\sum_{n=0}^n{{n\choose k}f^{k}g^{n-k}}.$$",,"['calculus', 'derivatives', 'taylor-expansion']"
4,"Differentiation, an issue with an exercise","Differentiation, an issue with an exercise",,I'm currently working on an exercise that involves quite a few fractional exponents. This is it: $$y = \frac {(x^4 + a)^\frac {1}{3}} {(x^3 + a)^ \frac {1}{2}} $$ I take the multiplication route by doing: $$(x^4 + a)^ \frac{1}{3}(x^3 + a) ^ \frac {-1}{2} $$ I eventually get $$(x^4 + a)^\frac {1}{3} \frac{-3}{2}x^2(x^3 + a)^ \frac{-3}{2} + (x^3 + a)^ \frac{-1}{2}\frac{4}{3}x^3(x^4 + a)^\frac{-2}{3}$$ This is what I can't get past at the moment. I'm not sure of the next step I should take to ensure I get the correct answer. Any help on this is much appreciated as always! There may be some formatting issues with the negative fractional exponents as they do look a little odd.,I'm currently working on an exercise that involves quite a few fractional exponents. This is it: $$y = \frac {(x^4 + a)^\frac {1}{3}} {(x^3 + a)^ \frac {1}{2}} $$ I take the multiplication route by doing: $$(x^4 + a)^ \frac{1}{3}(x^3 + a) ^ \frac {-1}{2} $$ I eventually get $$(x^4 + a)^\frac {1}{3} \frac{-3}{2}x^2(x^3 + a)^ \frac{-3}{2} + (x^3 + a)^ \frac{-1}{2}\frac{4}{3}x^3(x^4 + a)^\frac{-2}{3}$$ This is what I can't get past at the moment. I'm not sure of the next step I should take to ensure I get the correct answer. Any help on this is much appreciated as always! There may be some formatting issues with the negative fractional exponents as they do look a little odd.,,"['calculus', 'derivatives']"
5,How to find a directional derivative of an implicit function?,How to find a directional derivative of an implicit function?,,"it's not my homework, I just want to find out how to find a directional derivative of an implicit function. I know what is a directional derivative and how to find it when I have a function in normal form (I mean like like z=x^2+y....). $$ xz + yz^2 = 3xy + 3 $$ the point is: $$ P(1,−1) $$ and the direction (vector): $$ u = [1, 1] $$ Could you give me a formula for this? I know how to compute the derivatives of an implicit function as well.","it's not my homework, I just want to find out how to find a directional derivative of an implicit function. I know what is a directional derivative and how to find it when I have a function in normal form (I mean like like z=x^2+y....). $$ xz + yz^2 = 3xy + 3 $$ the point is: $$ P(1,−1) $$ and the direction (vector): $$ u = [1, 1] $$ Could you give me a formula for this? I know how to compute the derivatives of an implicit function as well.",,"['calculus', 'derivatives', 'partial-derivative', 'implicit-differentiation']"
6,What's the Derivative?,What's the Derivative?,,"What is the derivative (gradient) of $$\operatorname{vec}(W)^\top \left[\sum_i (ue_i^\top)\otimes(e_ie_i^\top) + \sum_j(e_je_j^\top)\otimes(ue_j^\top) - 2I\otimes I\right] \operatorname{vec}(FF^\top),\tag{3} $$ where $e_i$ denotes the $i$-th vector in the canonical basis and $u=\sum_ie_i=(1,1,\ldots,1)^\top$ where ""$\otimes$"" denotes the Kronecker product (tensor product) with respect to the rectangular matrix $F$? and $W$ is a constant matrix. The above term can also be simplified as $\sum_{ijl}(F_{il}-F_{jl})^2W_{ij}$ which is a much simpler expression. What would the derivative be? All the above entries are real while $F$ is a rectangular matrix. Also please refer to : Represent in a matrix form: $\sum_{ijl}(F_{il}-F_{jl})^2W_{ij}$ for the details on simplification/algebra.","What is the derivative (gradient) of $$\operatorname{vec}(W)^\top \left[\sum_i (ue_i^\top)\otimes(e_ie_i^\top) + \sum_j(e_je_j^\top)\otimes(ue_j^\top) - 2I\otimes I\right] \operatorname{vec}(FF^\top),\tag{3} $$ where $e_i$ denotes the $i$-th vector in the canonical basis and $u=\sum_ie_i=(1,1,\ldots,1)^\top$ where ""$\otimes$"" denotes the Kronecker product (tensor product) with respect to the rectangular matrix $F$? and $W$ is a constant matrix. The above term can also be simplified as $\sum_{ijl}(F_{il}-F_{jl})^2W_{ij}$ which is a much simpler expression. What would the derivative be? All the above entries are real while $F$ is a rectangular matrix. Also please refer to : Represent in a matrix form: $\sum_{ijl}(F_{il}-F_{jl})^2W_{ij}$ for the details on simplification/algebra.",,"['calculus', 'real-analysis', 'derivatives', 'tensor-products', 'partial-derivative']"
7,What's the value of $ y^{(n)}$when $ y=\frac{x^n}{(x+1)^2(x+2)^2}$,What's the value of when, y^{(n)}  y=\frac{x^n}{(x+1)^2(x+2)^2},"What's the value of $\displaystyle  y^{(n)}$when $\displaystyle  y=\frac{x^n}{(x+1)^2(x+2)^2}$? My Try:Let $\displaystyle y_n=\frac{x^n}{(x+1)^2(x+2)^2}$,so $\displaystyle y_n=xy_{n-1}$.According to Leibniz's formula,$$y_n^{(n)}=ny_{n-1}^{(n-1)}+xy_{n-1}^{(n)}$$.But I don't konw how to achieve it.","What's the value of $\displaystyle  y^{(n)}$when $\displaystyle  y=\frac{x^n}{(x+1)^2(x+2)^2}$? My Try:Let $\displaystyle y_n=\frac{x^n}{(x+1)^2(x+2)^2}$,so $\displaystyle y_n=xy_{n-1}$.According to Leibniz's formula,$$y_n^{(n)}=ny_{n-1}^{(n-1)}+xy_{n-1}^{(n)}$$.But I don't konw how to achieve it.",,"['calculus', 'derivatives']"
8,"Find the shortest distance from the point $P(0,1)$ to a point on the curve $x² - y² = 1$ , and find the point on the curve closest to $P$.","Find the shortest distance from the point  to a point on the curve  , and find the point on the curve closest to .","P(0,1) x² - y² = 1 P","Find the shortest distance from the point $P(0,1)$ to a point on the curve $x² - y² = 1$, and find the point on the curve closest to $P$. What I did so far is : plot the y var from $x^2-y^2=1 \implies y=\sqrt{1-x^2}$ Create a distance equation : $d = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ $ d = \sqrt{x^2+(1-y)^2}$ $ d = \sqrt{2-2\sqrt{1-x^2}}$ $\displaystyle \frac d{dx} (d) =\frac x{\sqrt{1-x^2}\sqrt{2-2\sqrt{1-x^2}}}$ I need to find the max or min? Any suggestions? thanks!","Find the shortest distance from the point $P(0,1)$ to a point on the curve $x² - y² = 1$, and find the point on the curve closest to $P$. What I did so far is : plot the y var from $x^2-y^2=1 \implies y=\sqrt{1-x^2}$ Create a distance equation : $d = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ $ d = \sqrt{x^2+(1-y)^2}$ $ d = \sqrt{2-2\sqrt{1-x^2}}$ $\displaystyle \frac d{dx} (d) =\frac x{\sqrt{1-x^2}\sqrt{2-2\sqrt{1-x^2}}}$ I need to find the max or min? Any suggestions? thanks!",,"['derivatives', 'education']"
9,Reasoning about the gamma function using the digamma function,Reasoning about the gamma function using the digamma function,,"I am working on evaluating the following equation: $\log\Gamma(\frac{1}{2}x) - \log\Gamma(\frac{1}{3}x)$ If I'm understanding correctly, the above is an increasing function which can be demonstrated by the following argument using the digamma function $\frac{\Gamma'}{\Gamma}(x) = \int_0^\infty(\frac{e^{-t}}{t} - \frac{e^{-xt}}{1-e^{-t}})$: $\frac{\Gamma'}{\Gamma}(\frac{1}{2}x) - \frac{\Gamma'}{\Gamma'}(\frac{1}{3}x) = \int_0^\infty\frac{1}{1-e^{-t}}(e^{-\frac{1}{3}xt} - e^{-\frac{1}{2}xt})dt > 0 (x > 1)$ Please let me know if this reasoning is incorrect or if you have any corrections. Thanks very much! -Larry","I am working on evaluating the following equation: $\log\Gamma(\frac{1}{2}x) - \log\Gamma(\frac{1}{3}x)$ If I'm understanding correctly, the above is an increasing function which can be demonstrated by the following argument using the digamma function $\frac{\Gamma'}{\Gamma}(x) = \int_0^\infty(\frac{e^{-t}}{t} - \frac{e^{-xt}}{1-e^{-t}})$: $\frac{\Gamma'}{\Gamma}(\frac{1}{2}x) - \frac{\Gamma'}{\Gamma'}(\frac{1}{3}x) = \int_0^\infty\frac{1}{1-e^{-t}}(e^{-\frac{1}{3}xt} - e^{-\frac{1}{2}xt})dt > 0 (x > 1)$ Please let me know if this reasoning is incorrect or if you have any corrections. Thanks very much! -Larry",,"['derivatives', 'special-functions', 'gamma-function']"
10,Implicit differentiation 2 questions?,Implicit differentiation 2 questions?,,"Hello everyone I have two questions on implicit differentiation. My first one is express $\frac{dy}{dx}$ in terms of $x$ and $y$ if $x^2-4xy^3+8x^2y=20$ what I did is this $2x-4x(3y^2)(\frac{dy}{dx})+4y^3+8x^2\frac{dy}{dx}+16xy=0$ $-4x(3y^2\frac{dy}{dx})+8x^2\frac{dy}{dx}=-2x-4y^3-16$ Finally I got $\frac{dy}{dx}=\frac{2x-4y^3-16}{-4(x)(3y^2)+8x^2}$ My second question is Find an equation for the tangent line to the graph of $2x^2-5y^2+xy=5$ at the point $(2,1)$ Anyway I simplified the equation to $\frac{dy}{dx}=\frac{-4x-1}{-10y+x}$ plugging in x and y I got $\frac{9}{8}$ so my line is $y-1=\frac{9}{8}(x-2)$ but I am unsure if if I did this correctly.","Hello everyone I have two questions on implicit differentiation. My first one is express $\frac{dy}{dx}$ in terms of $x$ and $y$ if $x^2-4xy^3+8x^2y=20$ what I did is this $2x-4x(3y^2)(\frac{dy}{dx})+4y^3+8x^2\frac{dy}{dx}+16xy=0$ $-4x(3y^2\frac{dy}{dx})+8x^2\frac{dy}{dx}=-2x-4y^3-16$ Finally I got $\frac{dy}{dx}=\frac{2x-4y^3-16}{-4(x)(3y^2)+8x^2}$ My second question is Find an equation for the tangent line to the graph of $2x^2-5y^2+xy=5$ at the point $(2,1)$ Anyway I simplified the equation to $\frac{dy}{dx}=\frac{-4x-1}{-10y+x}$ plugging in x and y I got $\frac{9}{8}$ so my line is $y-1=\frac{9}{8}(x-2)$ but I am unsure if if I did this correctly.",,"['calculus', 'derivatives']"
11,prove the following equations,prove the following equations,,"(i) Using the fact that $\sin x \lt x ~\forall~ x \gt 0$, prove that for all $x \gt 0$, $\cos x > 1- \dfrac{x^2}{2}$ (ii) Using (i) or otherwise, prove that for all $x \gt 0$, $\sin x > x - \dfrac{x^3}{6}$ For part (i) I did the following, Let $f(x)= \cos x, g(x)= 1- \dfrac{x^2}{2}$ After differentiation, I've got $\sin x < x$ Subbing $x=1$, I've got $0.84 < 1$ (proven) Am I doing it right for part (i), if so, How should I proceed for part (ii)? Thanks","(i) Using the fact that $\sin x \lt x ~\forall~ x \gt 0$, prove that for all $x \gt 0$, $\cos x > 1- \dfrac{x^2}{2}$ (ii) Using (i) or otherwise, prove that for all $x \gt 0$, $\sin x > x - \dfrac{x^3}{6}$ For part (i) I did the following, Let $f(x)= \cos x, g(x)= 1- \dfrac{x^2}{2}$ After differentiation, I've got $\sin x < x$ Subbing $x=1$, I've got $0.84 < 1$ (proven) Am I doing it right for part (i), if so, How should I proceed for part (ii)? Thanks",,"['calculus', 'derivatives']"
12,Derivative question?,Derivative question?,,Did I do the following derivatives correctly? $f(x)=\frac{3x^4+2x}{x-5}$ I used the quotient rule and got $\frac{(x-5)(12x^3+2)-(1)(3x^4+2x)}{(x-5)^2}$ apparently I do not have to simplify my second question find the derivative of $f(x)=(2x^5-3x^2)(\sqrt{x}-4x)$ I used product rule and got $f(x)'=(2x^5-3x^2)(\frac{1}{2\sqrt{x}}-4)+(10x^4-6x)(\sqrt{x}-4x)$,Did I do the following derivatives correctly? $f(x)=\frac{3x^4+2x}{x-5}$ I used the quotient rule and got $\frac{(x-5)(12x^3+2)-(1)(3x^4+2x)}{(x-5)^2}$ apparently I do not have to simplify my second question find the derivative of $f(x)=(2x^5-3x^2)(\sqrt{x}-4x)$ I used product rule and got $f(x)'=(2x^5-3x^2)(\frac{1}{2\sqrt{x}}-4)+(10x^4-6x)(\sqrt{x}-4x)$,,"['calculus', 'derivatives']"
13,Computing a derivative using logarithmic differentiation,Computing a derivative using logarithmic differentiation,,"Using the Logarithmic Differentiation find the derivative of $y=\sqrt{x(x-1)/(x-2)}$...so I tried,but the result is not correct..can you show me a hint? so $\ln y= 0.5\ln[x(x-1)/(x-2)]$ $$\ln y=0.5[\ln x+\ln(x-1)-\ln(x-2)]$$ $$\ln y=0.5\ln x+0.5\ln(x-1)+0.5\ln(x-2)$$ $$y'=0.5\ln x+0.5\ln(x-1)-0.5\ln(x-2)[\sqrt{x(x-1)/(x-2)}]$$","Using the Logarithmic Differentiation find the derivative of $y=\sqrt{x(x-1)/(x-2)}$...so I tried,but the result is not correct..can you show me a hint? so $\ln y= 0.5\ln[x(x-1)/(x-2)]$ $$\ln y=0.5[\ln x+\ln(x-1)-\ln(x-2)]$$ $$\ln y=0.5\ln x+0.5\ln(x-1)+0.5\ln(x-2)$$ $$y'=0.5\ln x+0.5\ln(x-1)-0.5\ln(x-2)[\sqrt{x(x-1)/(x-2)}]$$",,"['calculus', 'derivatives']"
14,Partial derivative of a summation.,Partial derivative of a summation.,,"I am trying to confirm a stated result on my lecture slide. Question : Given that $A:= \sum_i^n \frac{a_i}{(1+b)^{t_i}}$, where $a_i,b \in \mathbb{R}_+$ and $t_i \in \{t_1,...,t_n\}$ where $0 < t_1 < ... < t_n < \infty$. Demonstrate that: $-\frac1A \frac{\partial A}{\partial b} = - \sum_i^n t_i \frac{a_i}{(1+b)^{-t_i}}$ Current progress : $\frac{\partial A}{\partial b} = - \sum_i^n t_i \frac{a_i}{(1+b)^{t_i + 1}}$ PROBLEM Just from this first step (which could be incorrect), it seems that I can't arrive at what the lecture slide claims. Note that this is not from a mathematics lecturer so it could be wrong. Any assistance welcome and appreciated :-)","I am trying to confirm a stated result on my lecture slide. Question : Given that $A:= \sum_i^n \frac{a_i}{(1+b)^{t_i}}$, where $a_i,b \in \mathbb{R}_+$ and $t_i \in \{t_1,...,t_n\}$ where $0 < t_1 < ... < t_n < \infty$. Demonstrate that: $-\frac1A \frac{\partial A}{\partial b} = - \sum_i^n t_i \frac{a_i}{(1+b)^{-t_i}}$ Current progress : $\frac{\partial A}{\partial b} = - \sum_i^n t_i \frac{a_i}{(1+b)^{t_i + 1}}$ PROBLEM Just from this first step (which could be incorrect), it seems that I can't arrive at what the lecture slide claims. Note that this is not from a mathematics lecturer so it could be wrong. Any assistance welcome and appreciated :-)",,"['calculus', 'derivatives', 'summation']"
15,error bound of a integration,error bound of a integration,,"I have a function, which is $f(x) = e^{1/x}$. I want to calculate the error bound for the Trapezoid Rule , which formula is: $$|E|\leq K\frac{(a-b)^3}{12\cdot n^2}$$ where $|f''(x)|\leq K$. What's the value of $K$ for the above function? If I calculated $f''(x)$ correctly, it should be: $$f""(x) = \frac{e^{1/x}}{x^3}\left(\frac 1x + 2\right).$$ Please correct me if I'm wrong. The boundaries are $[1, 2]$.","I have a function, which is $f(x) = e^{1/x}$. I want to calculate the error bound for the Trapezoid Rule , which formula is: $$|E|\leq K\frac{(a-b)^3}{12\cdot n^2}$$ where $|f''(x)|\leq K$. What's the value of $K$ for the above function? If I calculated $f''(x)$ correctly, it should be: $$f""(x) = \frac{e^{1/x}}{x^3}\left(\frac 1x + 2\right).$$ Please correct me if I'm wrong. The boundaries are $[1, 2]$.",,"['integration', 'derivatives']"
16,"Leibniz's Rule. Where did this ""t"" come from?","Leibniz's Rule. Where did this ""t"" come from?",,"I know that they probably treated $\displaystyle f(s,t) = e^{-st} f(t)$ so the integration/differentiation thing doesn't matter, but what confuses me is when they got rid of the derivative and how the ""$t$"" pop out? It's taking the derivative with respect to $s$ not $t$. Proof : Consider the identity $$\frac{dF(s)}{ds} = \frac{d}{ds} \int_0^{\infty} e^{-st} f(t) dt.$$ Because of the assumptions on $f(t)$, we can apply a theorem from advanced calculus (sometimes called Leibniz's rule ) to interchange the order of integration and differentiation:   $$ \begin{align} \frac{dF(s)}{ds} & = \frac{d}{ds} \int_0^{\infty} e^{-st} f(t) dt\\ & = \int_0^{\infty} \frac{d \left(e^{-st} \right)}{ds} f(t) dt\\ & = - \frac{d}{ds} \int_0^{\infty} t e^{-st} f(t) dt\\ & = - \mathcal{L} \{ tf(t) \}(s). \end{align} $$   Thus,   $$\mathcal{L} \{ tf(t) \}(s) = (-1) \frac{dF(s)}{ds}$$","I know that they probably treated $\displaystyle f(s,t) = e^{-st} f(t)$ so the integration/differentiation thing doesn't matter, but what confuses me is when they got rid of the derivative and how the ""$t$"" pop out? It's taking the derivative with respect to $s$ not $t$. Proof : Consider the identity $$\frac{dF(s)}{ds} = \frac{d}{ds} \int_0^{\infty} e^{-st} f(t) dt.$$ Because of the assumptions on $f(t)$, we can apply a theorem from advanced calculus (sometimes called Leibniz's rule ) to interchange the order of integration and differentiation:   $$ \begin{align} \frac{dF(s)}{ds} & = \frac{d}{ds} \int_0^{\infty} e^{-st} f(t) dt\\ & = \int_0^{\infty} \frac{d \left(e^{-st} \right)}{ds} f(t) dt\\ & = - \frac{d}{ds} \int_0^{\infty} t e^{-st} f(t) dt\\ & = - \mathcal{L} \{ tf(t) \}(s). \end{align} $$   Thus,   $$\mathcal{L} \{ tf(t) \}(s) = (-1) \frac{dF(s)}{ds}$$",,"['calculus', 'derivatives', 'laplace-transform']"
17,Proving equivalent form of the mean-value theorem for derivatives,Proving equivalent form of the mean-value theorem for derivatives,,"I'm trying to prove the following statement (from Apostol's Calculus I, p. 186, Exercise 6) Show that the mean-value formula can be expressed in the form   $$f(x+h) = f(x) + hf'(x + \theta h) \qquad \text{where} \, 0 < \theta < 1. $$ As I'm not certain how standard the presentation of the mean-value formula is, Apostol gives it as $$ f(b) - f(a) = f'(c)(b-a) $$ For $f$ continuous on $[a,b]$ and having a derivative at each point of the open interval $(a,b)$ where $c \in (a,b)$. This really does not strike me as a problem that should be too difficult, but for some reason I can't seem to make the right connections. First, I notice that $f(x+h) = f(x) + hf'(x + \theta h) \implies \frac{f(x+h) - f(x)}{h} = f'(x + \theta h)$ looks like we are getting something reminiscent of the derivative of $f$ at $x$ on the left.  It's not clear to me what I can or should do with that. The other thing that it occurs to me to do, is to write $x = \theta x + (1-\theta)x$ for $0 < \theta < 1$.  I'm not sure what to do with that precisely, but it seems like the sort of thing that one should do in this situation. I wish I had more of my own work to show, but I can't seem to make any legitimate progress.","I'm trying to prove the following statement (from Apostol's Calculus I, p. 186, Exercise 6) Show that the mean-value formula can be expressed in the form   $$f(x+h) = f(x) + hf'(x + \theta h) \qquad \text{where} \, 0 < \theta < 1. $$ As I'm not certain how standard the presentation of the mean-value formula is, Apostol gives it as $$ f(b) - f(a) = f'(c)(b-a) $$ For $f$ continuous on $[a,b]$ and having a derivative at each point of the open interval $(a,b)$ where $c \in (a,b)$. This really does not strike me as a problem that should be too difficult, but for some reason I can't seem to make the right connections. First, I notice that $f(x+h) = f(x) + hf'(x + \theta h) \implies \frac{f(x+h) - f(x)}{h} = f'(x + \theta h)$ looks like we are getting something reminiscent of the derivative of $f$ at $x$ on the left.  It's not clear to me what I can or should do with that. The other thing that it occurs to me to do, is to write $x = \theta x + (1-\theta)x$ for $0 < \theta < 1$.  I'm not sure what to do with that precisely, but it seems like the sort of thing that one should do in this situation. I wish I had more of my own work to show, but I can't seem to make any legitimate progress.",,['calculus']
18,Give X a differentiable structure such that the inclusion map is smooth,Give X a differentiable structure such that the inclusion map is smooth,,"Let $X = \{(x,y,z) \in R^3: z^4 = x^2+y^2, z \geq 0\}$ . First I'm trying to prove that this is not a smooth submanifold of $R^3$ . Clearly the problematic point is $(0,0,0)$ however I haven't been able to prove formally that this point fails. For reference I'm trying to do an elementary proof of this, so this should basically be a result of the definition of a submanifold. I tried to asume the existence of a chart $\varphi$ of a neighborhood of $(0,0,0)$ to $R^2$ and with this try to consider the projection $\pi$ from $X$ to $R^2$ and consider the map $\pi \circ \varphi^{-1}$ and use inverse function theorem, but the derivative of this function is not invertible, so I didn't know how to proceed. Second for the differential structure I took the atlas generated by the chart $\phi(x,y,z) = (x^{1/3},y^{1/3})$ . Now with this atlas the inclusion map is differentiable but not smooth, so something like this should work, but I don't know exactly what chart to take. Any help would be greatly appreciated.","Let . First I'm trying to prove that this is not a smooth submanifold of . Clearly the problematic point is however I haven't been able to prove formally that this point fails. For reference I'm trying to do an elementary proof of this, so this should basically be a result of the definition of a submanifold. I tried to asume the existence of a chart of a neighborhood of to and with this try to consider the projection from to and consider the map and use inverse function theorem, but the derivative of this function is not invertible, so I didn't know how to proceed. Second for the differential structure I took the atlas generated by the chart . Now with this atlas the inclusion map is differentiable but not smooth, so something like this should work, but I don't know exactly what chart to take. Any help would be greatly appreciated.","X = \{(x,y,z) \in R^3: z^4 = x^2+y^2, z \geq 0\} R^3 (0,0,0) \varphi (0,0,0) R^2 \pi X R^2 \pi \circ \varphi^{-1} \phi(x,y,z) = (x^{1/3},y^{1/3})","['derivatives', 'differential-geometry', 'differential-topology', 'smooth-manifolds', 'smooth-functions']"
19,Differentiating Dirac delta with product rule,Differentiating Dirac delta with product rule,,"I have here an equation. $$ h'(t_2) \delta(t_1 - t_2) = [h(t_2) - h(t_1)] \delta'(t_1 - t_2) $$ I checked the equality by integrating both sides with a test function. $$ \int d t_1 \phi(t_1) \ldots \to h'(t_2) \phi(t_2) \\ \int d t_2 \phi(t_2) \ldots \to h'(t_1) \phi(t_1)  $$ Is this equation mathematically correct? I see a very similar derivation in this answer . If correct, can this kind of equation be derived using some sort of product rule? Checking using test functions may not be very practical for more complicated expressions. E.g. $$ h'(t_1) \delta(t_1 - t_3) = \int d t_2 h(t_2) [\delta'(t_1 - t_2) \delta(t_2 - t_3) - \delta(t_1 - t_2) \delta'(t_2 - t_3)] $$","I have here an equation. I checked the equality by integrating both sides with a test function. Is this equation mathematically correct? I see a very similar derivation in this answer . If correct, can this kind of equation be derived using some sort of product rule? Checking using test functions may not be very practical for more complicated expressions. E.g.","
h'(t_2) \delta(t_1 - t_2) = [h(t_2) - h(t_1)] \delta'(t_1 - t_2)
 
\int d t_1 \phi(t_1) \ldots \to h'(t_2) \phi(t_2) \\
\int d t_2 \phi(t_2) \ldots \to h'(t_1) \phi(t_1) 
 
h'(t_1) \delta(t_1 - t_3) = \int d t_2 h(t_2) [\delta'(t_1 - t_2) \delta(t_2 - t_3) - \delta(t_1 - t_2) \delta'(t_2 - t_3)]
","['derivatives', 'distribution-theory', 'dirac-delta']"
20,Difficulty interpreting generalised derivatives.,Difficulty interpreting generalised derivatives.,,"The Fréchet derivative is a generalised definition of the derivative for normed vector spaces: Let $(X, \lVert.\rVert_X)$ , $(Y,\lVert.\rVert_Y)$ be normed vector spaces. A map $F:X\to Y$ is called Fréchet differentiable with derivative $\mathrm{D}F(x_0)$ at $x_0\in X$ if there exists a bounded operator $\mathrm{D}F(x_0):X\to Y$ such that for all $h\in X$ $$ \lim_{\lVert h\rVert_X\to 0}\frac{\lVert F(x_0+h)-F(x_0)-\mathrm{D}F(x_0)(h) \rVert_Y}{\lVert h\rVert_X} = 0. $$ I can see the intuition behind this definition as follows: the usual limit definition in one-variable calculus is $$ \lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0} = f'(x_0) \equiv \mathrm{D}f(x_0). $$ Move terms and add $|\cdot|$ to get $$ \lim_{x\to x_0}\frac{\left|f(x)-f(x_0)-\mathrm{D}f(x_0)(x-x_0)\right|}{|x-x_0|} = 0,$$ which has the same form as the Fréchet derivative. However, I find it hard to conceptualise the derivative for more abstract spaces, such as in the space of matrices or integral of functions. An example I came across another day is the derivative in $\mathrm{GL}_n(\mathbb{R})$ . Consider the inverse map $$ F:\mathrm{GL}_n(\mathbb{R})\to\mathrm{GL}_n(\mathbb{R})\equiv A \mapsto A^{-1}, $$ where $$\mathrm{GL}_n(\mathbb{R}) = \{ A \;|\;\mathrm{det}(A) \neq 0 \}.$$ Following the above definition, we can differentiate $F$ (with respect to what?!) and find that for all $A\in\mathrm{GL}_n(\mathbb{R})$ , $H\in M_n(\mathbb{R})$ $$ \mathrm{D}F(A)(H) = -A^{-1}HA^{-1}.$$ There have been several posts about this ( 1 , 2 ), using various methods to arrive at the result, but none of them seem to give an intuitive interpretation. What does it even mean to differentiate a matrix-valued function or a functional that takes in functions as its input?","The Fréchet derivative is a generalised definition of the derivative for normed vector spaces: Let , be normed vector spaces. A map is called Fréchet differentiable with derivative at if there exists a bounded operator such that for all I can see the intuition behind this definition as follows: the usual limit definition in one-variable calculus is Move terms and add to get which has the same form as the Fréchet derivative. However, I find it hard to conceptualise the derivative for more abstract spaces, such as in the space of matrices or integral of functions. An example I came across another day is the derivative in . Consider the inverse map where Following the above definition, we can differentiate (with respect to what?!) and find that for all , There have been several posts about this ( 1 , 2 ), using various methods to arrive at the result, but none of them seem to give an intuitive interpretation. What does it even mean to differentiate a matrix-valued function or a functional that takes in functions as its input?","(X, \lVert.\rVert_X) (Y,\lVert.\rVert_Y) F:X\to Y \mathrm{D}F(x_0) x_0\in X \mathrm{D}F(x_0):X\to Y h\in X  \lim_{\lVert h\rVert_X\to 0}\frac{\lVert F(x_0+h)-F(x_0)-\mathrm{D}F(x_0)(h) \rVert_Y}{\lVert h\rVert_X} = 0.   \lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0} = f'(x_0) \equiv \mathrm{D}f(x_0).  |\cdot|  \lim_{x\to x_0}\frac{\left|f(x)-f(x_0)-\mathrm{D}f(x_0)(x-x_0)\right|}{|x-x_0|} = 0, \mathrm{GL}_n(\mathbb{R})  F:\mathrm{GL}_n(\mathbb{R})\to\mathrm{GL}_n(\mathbb{R})\equiv A \mapsto A^{-1},  \mathrm{GL}_n(\mathbb{R}) = \{ A \;|\;\mathrm{det}(A) \neq 0 \}. F A\in\mathrm{GL}_n(\mathbb{R}) H\in M_n(\mathbb{R})  \mathrm{D}F(A)(H) = -A^{-1}HA^{-1}.","['real-analysis', 'linear-algebra', 'derivatives']"
21,Newton root finding and preserving automatic differentiation.,Newton root finding and preserving automatic differentiation.,,"I am applying Newton's root solving algorithm. Suppose the example problem below, solving for $g$ with fixed parameter, $s$ : $$ f(g;s) = g^2 - s = 0, \qquad g_{i+1}=g_i - \frac{f(g_i;s)}{\frac{df}{dg}(g_i;s)} $$ Clearly this has the solution: $$ g= \sqrt{s}, \qquad \frac{dg}{ds} = \frac{1}{2\sqrt{s}}, \qquad \frac{d^2g}{ds^2} = \frac{-1}{4s^{3/2}}  $$ When you search for ""Newton Algorithm"" and ""Automatic Differentiation"", the majority of the results/papers reference applying automatic differentiation to determine the derivative, $\frac{df}{dg}(g_i;s)$ at each step in the interation. I am not interested in this. What I am interested in is a formal proof that the following process can capture the accurate resultant sensitivities (i.e. $\frac{dg}{ds}$ and $\frac{d^2g}{dx^2}$ : First converge to solution using floating point arithmetic. Perform one more iteration using the current float iterate for $g_i$ and adding the AD variable sensitivity to $s$ . This will capture 1st deriavtive sensitivity accurately. To capture 2nd order sensitivity perform one further iteration . Empirically in all my tests this has not failed and is exactly accurate, but I would be more comfortable with a proof. For those interested in some abstract code which I have working according to the above definitions is... def f_and_df(g, s):     f = g ** 2 - s     df = 2*g     return f, df  result = newton_root(f_and_df, g0=1.0, args=(2.0,)) # result = 1.4142135.....  result = newton_root(f_and_df, g0=1.0, args=(Dual(2.0, vars=[""s""]),)) # result = <Dual: 1.4142135, (""s""), [0.35355..]>  result = newton_root(f_and_df, g0=1.0, args=(Dual2(2.0, vars=[""s""]),)) # result = <Dual2: 1.4142135, (""s""), [0.35355..], [-0.088388..]> # Internal newton_root code # # After float convergence # # Final iteration method to preserve AD f0, f1 = f(g1, *(*args, *final_args)) if isinstance(f0, (Dual, Dual2)) or isinstance(f1, (Dual, Dual2)):     # Perform 1 extra iteration because of AD variable     g1 = g1 - f0 / f1 if isinstance(f0, Dual2) or isinstance(f1, Dual2):     # Perform a 2nd iteration because of order = 2     f0, f1 = f(g1, *(*args, *final_args))     g1 = g1 - f0 / f1 return g1","I am applying Newton's root solving algorithm. Suppose the example problem below, solving for with fixed parameter, : Clearly this has the solution: When you search for ""Newton Algorithm"" and ""Automatic Differentiation"", the majority of the results/papers reference applying automatic differentiation to determine the derivative, at each step in the interation. I am not interested in this. What I am interested in is a formal proof that the following process can capture the accurate resultant sensitivities (i.e. and : First converge to solution using floating point arithmetic. Perform one more iteration using the current float iterate for and adding the AD variable sensitivity to . This will capture 1st deriavtive sensitivity accurately. To capture 2nd order sensitivity perform one further iteration . Empirically in all my tests this has not failed and is exactly accurate, but I would be more comfortable with a proof. For those interested in some abstract code which I have working according to the above definitions is... def f_and_df(g, s):     f = g ** 2 - s     df = 2*g     return f, df  result = newton_root(f_and_df, g0=1.0, args=(2.0,)) # result = 1.4142135.....  result = newton_root(f_and_df, g0=1.0, args=(Dual(2.0, vars=[""s""]),)) # result = <Dual: 1.4142135, (""s""), [0.35355..]>  result = newton_root(f_and_df, g0=1.0, args=(Dual2(2.0, vars=[""s""]),)) # result = <Dual2: 1.4142135, (""s""), [0.35355..], [-0.088388..]> # Internal newton_root code # # After float convergence # # Final iteration method to preserve AD f0, f1 = f(g1, *(*args, *final_args)) if isinstance(f0, (Dual, Dual2)) or isinstance(f1, (Dual, Dual2)):     # Perform 1 extra iteration because of AD variable     g1 = g1 - f0 / f1 if isinstance(f0, Dual2) or isinstance(f1, Dual2):     # Perform a 2nd iteration because of order = 2     f0, f1 = f(g1, *(*args, *final_args))     g1 = g1 - f0 / f1 return g1","g s  f(g;s) = g^2 - s = 0, \qquad g_{i+1}=g_i - \frac{f(g_i;s)}{\frac{df}{dg}(g_i;s)}   g= \sqrt{s}, \qquad \frac{dg}{ds} = \frac{1}{2\sqrt{s}}, \qquad \frac{d^2g}{ds^2} = \frac{-1}{4s^{3/2}}   \frac{df}{dg}(g_i;s) \frac{dg}{ds} \frac{d^2g}{dx^2} g_i s","['derivatives', 'algorithms', 'newton-raphson']"
22,Least square derivatives,Least square derivatives,,"Let $X_1, \ldots, X_N \in \mathbb{R}^p$ and $Y_1, \ldots, Y_N \in \mathbb{R}$ . Define $$ X=\left[\begin{array}{c} X_1^{\top} \\ \vdots \\ X_N^{\top} \end{array}\right] \in \mathbb{R}^{N \times p}, \quad Y=\left[\begin{array}{c} Y_1 \\ \vdots \\ Y_N \end{array}\right] \in \mathbb{R}^N $$ Let $$ \ell_i(\theta)=\frac{1}{2}\left(X_i^{\top} \theta-Y_i\right)^2 \quad \text { for } i=1, \ldots, N, \quad \mathcal{L}(\theta)=\frac{1}{2}\|X \theta-Y\|^2 . $$ Show (a) $\nabla_\theta \ell_i(\theta)=\left(X_i^{\top} \theta-Y_i\right) X_i$ and (b) $\nabla_\theta \mathcal{L}(\theta)=X^{\top}(X \theta-Y)$ . Hint. For part (a), start by computing $\frac{\partial}{\partial \theta_j} \ell_i(\theta)$ . For part (b), use the fact that $$ M v=\sum_{i=1}^N M_{:, i} v_i \in \mathbb{R}^p $$ for any $M \in \mathbb{R}^{p \times N}, v \in \mathbb{R}^N$ , where $M_{:, i}$ is the $i$ th column of $M$ for $i=1, \ldots, N$ . My Preliminary Progress: For part (a): I started with the definition of $\ell_i(\theta)$ and applied the chain rule to compute the gradient, arriving at: $$ \nabla_\theta \ell_i(\theta)=\left(X_i^{\top} \theta-Y_i\right) X_i $$ For part (b): Using the overall loss $\mathcal{L}(\theta)$ , I leveraged matrix differentiation rules and the hint provided regarding matrix-vector multiplication to deduce that: $$ \nabla_\theta \mathcal{L}(\theta)=X^{\top}(X \theta-Y) $$ My Questions: Rigorous Derivation: Could someone provide a step-by-step, rigorous derivation of both (a) and (b), possibly highlighting any subtleties in matrix calculus that I might have glossed over? Matrix Calculus Techniques: Are there specific matrix calculus techniques or identities that are particularly useful in simplifying these derivations? Generalization: How might these results generalize to other loss functions or optimization problems within machine learning? I believe that understanding these derivations in depth will greatly enhance my comprehension of optimization in statistical learning. Thank you in advance for your time and help!","Let and . Define Let Show (a) and (b) . Hint. For part (a), start by computing . For part (b), use the fact that for any , where is the th column of for . My Preliminary Progress: For part (a): I started with the definition of and applied the chain rule to compute the gradient, arriving at: For part (b): Using the overall loss , I leveraged matrix differentiation rules and the hint provided regarding matrix-vector multiplication to deduce that: My Questions: Rigorous Derivation: Could someone provide a step-by-step, rigorous derivation of both (a) and (b), possibly highlighting any subtleties in matrix calculus that I might have glossed over? Matrix Calculus Techniques: Are there specific matrix calculus techniques or identities that are particularly useful in simplifying these derivations? Generalization: How might these results generalize to other loss functions or optimization problems within machine learning? I believe that understanding these derivations in depth will greatly enhance my comprehension of optimization in statistical learning. Thank you in advance for your time and help!","X_1, \ldots, X_N \in \mathbb{R}^p Y_1, \ldots, Y_N \in \mathbb{R} 
X=\left[\begin{array}{c}
X_1^{\top} \\
\vdots \\
X_N^{\top}
\end{array}\right] \in \mathbb{R}^{N \times p}, \quad Y=\left[\begin{array}{c}
Y_1 \\
\vdots \\
Y_N
\end{array}\right] \in \mathbb{R}^N
 
\ell_i(\theta)=\frac{1}{2}\left(X_i^{\top} \theta-Y_i\right)^2 \quad \text { for } i=1, \ldots, N, \quad \mathcal{L}(\theta)=\frac{1}{2}\|X \theta-Y\|^2 .
 \nabla_\theta \ell_i(\theta)=\left(X_i^{\top} \theta-Y_i\right) X_i \nabla_\theta \mathcal{L}(\theta)=X^{\top}(X \theta-Y) \frac{\partial}{\partial \theta_j} \ell_i(\theta) 
M v=\sum_{i=1}^N M_{:, i} v_i \in \mathbb{R}^p
 M \in \mathbb{R}^{p \times N}, v \in \mathbb{R}^N M_{:, i} i M i=1, \ldots, N \ell_i(\theta) 
\nabla_\theta \ell_i(\theta)=\left(X_i^{\top} \theta-Y_i\right) X_i
 \mathcal{L}(\theta) 
\nabla_\theta \mathcal{L}(\theta)=X^{\top}(X \theta-Y)
","['linear-algebra', 'derivatives']"
23,Inconsistent Function Monotonicity from hand and Mathematica image,Inconsistent Function Monotonicity from hand and Mathematica image,,"$g(x)=\frac{\phi(x)}{1-\Phi(x)}$ , where $\phi(x)$ and $\Phi(x)$ are p.d.f and c.d.f of standard normal distribution respectively. $g'(x)=\frac{\phi'(x)(1-\Phi(x))+\phi^2(x)}{(1-\Phi(x))^2}=\frac{\phi(x)}{(1-\Phi(x))^2} \left( \phi(x)-x(1-\Phi(x)) \right)$ since $\phi'(x)=-x\phi(x)$ . Let $h(x)=\phi(x)-x(1-\Phi(x))$ , $h'(x)$ is thus $\phi'(x)-(1-\Phi(x))+x\phi(x)=\Phi(x)-1 \leq 0$ $h(x)$ is non-increasing, and $\lim_{x\to \infty} h(x)=0 $ , thus $h(x)\geq0$ and since $\frac{\phi(x)}{(1-\Phi(x))^2}\geq 0$ then $g'(x)\geq0$ . $g(x)$ must be non-decreasing. But in Mathematica, I have this like a wavy line. Mathematica Image What's the problem please!",", where and are p.d.f and c.d.f of standard normal distribution respectively. since . Let , is thus is non-increasing, and , thus and since then . must be non-decreasing. But in Mathematica, I have this like a wavy line. Mathematica Image What's the problem please!",g(x)=\frac{\phi(x)}{1-\Phi(x)} \phi(x) \Phi(x) g'(x)=\frac{\phi'(x)(1-\Phi(x))+\phi^2(x)}{(1-\Phi(x))^2}=\frac{\phi(x)}{(1-\Phi(x))^2} \left( \phi(x)-x(1-\Phi(x)) \right) \phi'(x)=-x\phi(x) h(x)=\phi(x)-x(1-\Phi(x)) h'(x) \phi'(x)-(1-\Phi(x))+x\phi(x)=\Phi(x)-1 \leq 0 h(x) \lim_{x\to \infty} h(x)=0  h(x)\geq0 \frac{\phi(x)}{(1-\Phi(x))^2}\geq 0 g'(x)\geq0 g(x),"['real-analysis', 'calculus', 'derivatives', 'normal-distribution']"
24,Derivative of a function with respect to a function,Derivative of a function with respect to a function,,"This is Definition 10 from Semi-Riemannian Geometry With Applications to Relativity by Barrett O'Neill, page 7: On both sides of Definition 10, he is differentiating with respect to functions , not independent variables. What does that mean? He defines both x i and u i to be functions, not coordinates. He does so on the first two pages of the book: Here's a picture of what the functions do:","This is Definition 10 from Semi-Riemannian Geometry With Applications to Relativity by Barrett O'Neill, page 7: On both sides of Definition 10, he is differentiating with respect to functions , not independent variables. What does that mean? He defines both x i and u i to be functions, not coordinates. He does so on the first two pages of the book: Here's a picture of what the functions do:",,"['derivatives', 'differential-geometry', 'manifolds', 'semi-riemannian-geometry']"
25,Solving logistic regression equation for slope,Solving logistic regression equation for slope,,"I've calculated a logistic regression model involving two variables $X_1$ and $X_2$ and their interaction $X_1 \times X_2$ and obtained regression coefficients for each. The equation takes following linear form: $$ \text{logit }p(X_1,X_2) = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_{\text{interaction}} \cdot X_1 \cdot X_2 $$ If got the mathematics correct this should correspond to the non-linear form: $$p(X_1,X_2) = \frac{1}{1 + \exp\left(-\left(\beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_{\text{interaction}} \cdot X_1 \cdot X_2\right)\right)}$$ The regression coeffiecients I obtained are the following: $\beta_0$ = 0.31, $\beta_1$ = -5.63, $\beta_2$ = -0.02, $\beta_\text{interaction}$ = 0.45; Based on the parameters I can plot the probability against a range of value for $X_1$ (in my case this is named DeltaT ) and across values of $X_2$ (in the plot as single panels and called Tamb ) which looks like: From the plot I can see that there is an point of inflection at appr. $X_2$ = 12.5, i.e. where the slope of the line changes from negative to positive (or vice versa). Now, I'd like the get the exact point when this happens. In other words at which value of $X_2$ does the slope change (at $X_1$ = 0). From manual simulations and numerically obtaining the point where the slope changes, it seems this is at $\frac{-\beta_1}{\beta_\text{interaction}} = \frac{-1*(-5.63)}{0.45} = 12.51 $ . [As a note why I am confused: I'd have set $X_1$ in the first linear equation to 0 and solved for $X_2$ which would then have been $X_2 = \frac{-\beta_0}{\beta_2}$ ?, but that's not how it works.] I guess obtaining the slope (and where it changes) can be done by calculating the first derivative of the equation with respect to $X_2$ . Setting the derivative equal to zero should obtain the point where the slope becomes zero? However, I don't get the mathematical steps behind that, as in the end this seems to be solved by just two regression coefficients(?).","I've calculated a logistic regression model involving two variables and and their interaction and obtained regression coefficients for each. The equation takes following linear form: If got the mathematics correct this should correspond to the non-linear form: The regression coeffiecients I obtained are the following: = 0.31, = -5.63, = -0.02, = 0.45; Based on the parameters I can plot the probability against a range of value for (in my case this is named DeltaT ) and across values of (in the plot as single panels and called Tamb ) which looks like: From the plot I can see that there is an point of inflection at appr. = 12.5, i.e. where the slope of the line changes from negative to positive (or vice versa). Now, I'd like the get the exact point when this happens. In other words at which value of does the slope change (at = 0). From manual simulations and numerically obtaining the point where the slope changes, it seems this is at . [As a note why I am confused: I'd have set in the first linear equation to 0 and solved for which would then have been ?, but that's not how it works.] I guess obtaining the slope (and where it changes) can be done by calculating the first derivative of the equation with respect to . Setting the derivative equal to zero should obtain the point where the slope becomes zero? However, I don't get the mathematical steps behind that, as in the end this seems to be solved by just two regression coefficients(?).","X_1 X_2 X_1 \times X_2 
\text{logit }p(X_1,X_2) = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_{\text{interaction}} \cdot X_1 \cdot X_2
 p(X_1,X_2) = \frac{1}{1 + \exp\left(-\left(\beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_{\text{interaction}} \cdot X_1 \cdot X_2\right)\right)} \beta_0 \beta_1 \beta_2 \beta_\text{interaction} X_1 X_2 X_2 X_2 X_1 \frac{-\beta_1}{\beta_\text{interaction}} = \frac{-1*(-5.63)}{0.45} = 12.51  X_1 X_2 X_2 = \frac{-\beta_0}{\beta_2} X_2","['derivatives', 'slope', 'logistic-regression']"
26,Basic Understanding of derivatives as a rate of change,Basic Understanding of derivatives as a rate of change,,"I am teaching myself calculus and trying to understand derivatives from a physics point of view. I am faced with this question: the edge of a square is being enlarged at $  2 \frac{cm}{sec}$ . What is the rate of change of the area when the edge is 6 cm long. The solution in the book (Serge Lang, A first course in Calculus, 5th ed) is as follows: $A = x^2 \\ \frac{dA}{dx} = 2x$ Using chain rule: $ \frac{dA} {dt} =\frac{dA} {dx} * \frac{dx} {dt} \\ = 2x * 2 = 4x $ When the edge is 6: $\frac{dA}{dt} = 4*6 = 24$ . My main issue is that intuitively I had assume I can do the following: At time $t_1, A_1 = x^2$ $\\ $ At time $t_1+1, A_2 = (x+2)(x+2) = x^2 + 4x + 4$ Then $\frac {dA}{dt}= \frac{A_2 - A_1} {t_1 +1 - t_1} = 4x+4$ Which does not match the answer, also, calculating difference of area when edge goes from 4 to 6 is 36 - 16 = 20 and not 24. I would like to understand what was wrong in my logic. Also I considered this might be due to the fact that I am taking a long interval ( 1 sec. and derivatives evaluate the rate of change at exactly the point of change). But again the rate which we are using is defined as 2 $\frac {cm}{sec}$ ,we don't have any information about how it grows in shorter intervals, so I don't know if it make sense to think of shorter intervals than 1 second. When I googled about derivates from definition are different from formulas I came against something called backward and forward differences and centered differences, it seems to be related to discrete mathematics which I have no experience with yet. But using the idea to evaluate the derivate at the center yields: $(x+1)(x+1) - (x-1)(x-1) = 4x$ This also is correct as difference between 49 and 25. Maybe someone can shade light on this? (I guess same result can be reached if we take two differences, one forward and one backward and average them on the time interval of 2 second). Also one final remark that confused me, the fact I can evaluate the area when $x = 5$ and $x = 6$ is based on the initial condition of the length of the edge. That is, is not there a hidden assumption that the increase is uniform everywhere during this one second? What I mean to point out is that if the edge was being increased in pulses and not uniformly then I can not assume that $x-1$ and $x+1$ (despite having a distance of 2 separating them) are actually legal to evaluate, because the only information I was given that $x_{next } = x_{old} + 2$ for every second that passes Thanks.","I am teaching myself calculus and trying to understand derivatives from a physics point of view. I am faced with this question: the edge of a square is being enlarged at . What is the rate of change of the area when the edge is 6 cm long. The solution in the book (Serge Lang, A first course in Calculus, 5th ed) is as follows: Using chain rule: When the edge is 6: . My main issue is that intuitively I had assume I can do the following: At time At time Then Which does not match the answer, also, calculating difference of area when edge goes from 4 to 6 is 36 - 16 = 20 and not 24. I would like to understand what was wrong in my logic. Also I considered this might be due to the fact that I am taking a long interval ( 1 sec. and derivatives evaluate the rate of change at exactly the point of change). But again the rate which we are using is defined as 2 ,we don't have any information about how it grows in shorter intervals, so I don't know if it make sense to think of shorter intervals than 1 second. When I googled about derivates from definition are different from formulas I came against something called backward and forward differences and centered differences, it seems to be related to discrete mathematics which I have no experience with yet. But using the idea to evaluate the derivate at the center yields: This also is correct as difference between 49 and 25. Maybe someone can shade light on this? (I guess same result can be reached if we take two differences, one forward and one backward and average them on the time interval of 2 second). Also one final remark that confused me, the fact I can evaluate the area when and is based on the initial condition of the length of the edge. That is, is not there a hidden assumption that the increase is uniform everywhere during this one second? What I mean to point out is that if the edge was being increased in pulses and not uniformly then I can not assume that and (despite having a distance of 2 separating them) are actually legal to evaluate, because the only information I was given that for every second that passes Thanks."," 
2 \frac{cm}{sec} A = x^2 \\
\frac{dA}{dx} = 2x  \frac{dA} {dt} =\frac{dA} {dx} * \frac{dx} {dt} \\
= 2x * 2 = 4x  \frac{dA}{dt} = 4*6 = 24 t_1, A_1 = x^2 \\  t_1+1, A_2 = (x+2)(x+2) =
x^2 + 4x + 4 \frac {dA}{dt}= \frac{A_2 - A_1} {t_1 +1 - t_1} = 4x+4 \frac {cm}{sec} (x+1)(x+1) - (x-1)(x-1) = 4x x = 5 x = 6 x-1 x+1 x_{next } = x_{old} + 2","['calculus', 'derivatives', 'area']"
27,Values of $m$ such that $|x^2-2x+m| + 2x + 1$ has $3$ extrema,Values of  such that  has  extrema,m |x^2-2x+m| + 2x + 1 3,I was given the following question: Find the values of $m$ for which the curve $y=|x^2-2x+m| + 2x + 1$ has $3$ extrema. My teacher suggested that we should use the quadratic formula $(b^2-4ac)$ and check every case of it. I am sure that there are three main cases of the inside of absolute value being $<0 ; =0 ; >0$ . The solution is $m<0$ but I do not know how to dig down into each case.,I was given the following question: Find the values of for which the curve has extrema. My teacher suggested that we should use the quadratic formula and check every case of it. I am sure that there are three main cases of the inside of absolute value being . The solution is but I do not know how to dig down into each case.,m y=|x^2-2x+m| + 2x + 1 3 (b^2-4ac) <0 ; =0 ; >0 m<0,"['calculus', 'derivatives', 'quadratics', 'absolute-value']"
28,"Under which condition $d(y)=\min_x f(x,y)$ is smooth ($C^1$) (similar to Danskin's theorem))?",Under which condition  is smooth () (similar to Danskin's theorem))?,"d(y)=\min_x f(x,y) C^1","Under which condition $d(y)=\min_x f(x,y)$ is smooth (differentiable, $C^1$ )? To determine the smoothness of $d(y)$ , I think the following conditions are required: Differentiability: The function $f(x, y)$ is smooth with respect to both $x$ and $y$ in the feasible set. This ensures that the partial derivatives of $f(x, y)$ exist and can be used to find the minimum. Then, is this condition enough to guarantee that $d$ is smooth? For example, let $x\in\mathbb{R}_+$ , $y\in\mathbb{R}^n$ and $$ f(x,y)=x + \frac{\|y\|_2^2}{x}.$$ Then $d(y) = 2\|y\|_2$ is not smooth at zero point. Which conditions should we add next? What if we add the Lispchitz continuous? A similar result for the convex case can be found here . I would rather consider a special case $$ d(y) = \min_x f(x,y) + g(x),$$ where $g$ is a strongly convex function, Which conditions should we add? If the solution to $$\min_x f(x,y)+ g(x)$$ is unique, can we use implicit function theorem to obtain the smoothness of $d$ ?","Under which condition is smooth (differentiable, )? To determine the smoothness of , I think the following conditions are required: Differentiability: The function is smooth with respect to both and in the feasible set. This ensures that the partial derivatives of exist and can be used to find the minimum. Then, is this condition enough to guarantee that is smooth? For example, let , and Then is not smooth at zero point. Which conditions should we add next? What if we add the Lispchitz continuous? A similar result for the convex case can be found here . I would rather consider a special case where is a strongly convex function, Which conditions should we add? If the solution to is unique, can we use implicit function theorem to obtain the smoothness of ?","d(y)=\min_x f(x,y) C^1 d(y) f(x, y) x y f(x, y) d x\in\mathbb{R}_+ y\in\mathbb{R}^n  f(x,y)=x + \frac{\|y\|_2^2}{x}. d(y) = 2\|y\|_2  d(y) = \min_x f(x,y) + g(x), g \min_x f(x,y)+ g(x) d","['calculus', 'derivatives', 'optimization', 'convex-optimization', 'smooth-functions']"
29,Derivative of a periodic function with respect to a parameter,Derivative of a periodic function with respect to a parameter,,"Assume we have a real periodic function $f(x)$ with some fundamental period $P$ . Let us introduce a coordinate transformation $x=ay$ , where $a \in \mathbb{R}$ and $a \neq 0$ . Then, on the one hand, $$ \frac{\partial f}{\partial a} = \frac{\partial f(ay)}{\partial (ay)} \frac{\partial(ay)}{\partial a} = \frac{\partial f(x)}{\partial x} y = a^{-1} \frac{\partial f(x)}{\partial x} x $$ which is clearly not periodic (due to the $x$ term). On the other hand, I expect $\partial_a f$ to also be periodic in $x$ (see this question for example). Perhaps the latter statement is invalid in case of multivariable functions? What kind of requirement should one impose on coordinate transformations to make $\partial_a f$ periodic? Is there a general statement that is valid for any kind of coordinate transformation that depends on a parameter (i.e. $y=g_a (x)$ )?","Assume we have a real periodic function with some fundamental period . Let us introduce a coordinate transformation , where and . Then, on the one hand, which is clearly not periodic (due to the term). On the other hand, I expect to also be periodic in (see this question for example). Perhaps the latter statement is invalid in case of multivariable functions? What kind of requirement should one impose on coordinate transformations to make periodic? Is there a general statement that is valid for any kind of coordinate transformation that depends on a parameter (i.e. )?","f(x) P x=ay a \in \mathbb{R} a \neq 0 
\frac{\partial f}{\partial a} = \frac{\partial f(ay)}{\partial (ay)} \frac{\partial(ay)}{\partial a} = \frac{\partial f(x)}{\partial x} y = a^{-1} \frac{\partial f(x)}{\partial x} x
 x \partial_a f x \partial_a f y=g_a (x)","['calculus', 'derivatives', 'periodic-functions']"
30,Find value of $x$ from $f'(x) + Cf(x) = 0$,Find value of  from,x f'(x) + Cf(x) = 0,"I have the following equation: $$ \sum_{i=1}^N \ln (d_i) d_i^{-x} = C \sum_{i=1}^N d_i^{-x} $$ where $C$ and $d_i$ are constants (known variables). For $N=2$ the equation takes the form $$ \ln (d_1) d_1^{-x} + \ln (d_2) d_2^{-x} = C (d_1^{-x} + d_2^{-x}) $$ and it is possible to solve it analytically for $x$ . My question is whether it is possible to solve the first equation analytically for $N>2$ . In general, this type of equation can be written as: $$ \sum_{i=1}^N g(x_i) f(x_i) = C \sum_{i=1}^N f(x_i) $$ Edit : from @ancient mathematician we can rewrite $d_i$ to $e^{\lambda_i}$ . Thus, the equation now is defined as $$ \sum_{i=1}^N \lambda_i  e^{-\lambda_i x} = C \sum_{i=1}^N   e^{-\lambda_i x} $$ We can notice that the left part is the derivative of the right part, such as $$ -\frac{d}{dx} \sum_{i=1}^N  e^{-\lambda_i x} = C \sum_{i=1}^N   e^{-\lambda_i x} $$ with $f(x) =  \sum_{i=1}^N  e^{-\lambda_i x}$ we have $$ f'(x) + Cf(x) = 0 $$ The task is to find the $x$ from this equation.","I have the following equation: where and are constants (known variables). For the equation takes the form and it is possible to solve it analytically for . My question is whether it is possible to solve the first equation analytically for . In general, this type of equation can be written as: Edit : from @ancient mathematician we can rewrite to . Thus, the equation now is defined as We can notice that the left part is the derivative of the right part, such as with we have The task is to find the from this equation.","
\sum_{i=1}^N \ln (d_i) d_i^{-x} = C \sum_{i=1}^N d_i^{-x}
 C d_i N=2 
\ln (d_1) d_1^{-x} + \ln (d_2) d_2^{-x} = C (d_1^{-x} + d_2^{-x})
 x N>2 
\sum_{i=1}^N g(x_i) f(x_i) = C \sum_{i=1}^N f(x_i)
 d_i e^{\lambda_i} 
\sum_{i=1}^N \lambda_i  e^{-\lambda_i x} = C \sum_{i=1}^N   e^{-\lambda_i x}
 
-\frac{d}{dx} \sum_{i=1}^N  e^{-\lambda_i x} = C \sum_{i=1}^N   e^{-\lambda_i x}
 f(x) =  \sum_{i=1}^N  e^{-\lambda_i x} 
f'(x) + Cf(x) = 0
 x","['derivatives', 'closed-form']"
31,Checking the stability of equilibrium point,Checking the stability of equilibrium point,,"Is equilibrium point $x = 0$ stable for $\ddot{x}+\dot{x}=-x\sin x$ ? It is a case of a bigger problem, where we need to check the stability of critical points $x \in [-\frac \pi 2, \frac \pi 2]$ for $\ddot{x}+\dot{x} = x(a-1)(\sin x-a)$ . I've done with all other situations, but can not do anything with this case. Obviously, we need to say $\dot{x}=y$ , $\dot{y} =-y-x\sin x$ . Linear approximation doesn't help here. So, it seems we need to find Lyapunov function $V$ in order to use Lyapunov theorem or Chetaev theorem. I've tried some, but I didn't manage to find any. I think that we shoud in some way modify $V = \frac{y^2}2 - x\cos x + \sin x$ , because $\dot{V} = -y^2$ .","Is equilibrium point stable for ? It is a case of a bigger problem, where we need to check the stability of critical points for . I've done with all other situations, but can not do anything with this case. Obviously, we need to say , . Linear approximation doesn't help here. So, it seems we need to find Lyapunov function in order to use Lyapunov theorem or Chetaev theorem. I've tried some, but I didn't manage to find any. I think that we shoud in some way modify , because .","x = 0 \ddot{x}+\dot{x}=-x\sin x x \in [-\frac \pi 2, \frac \pi 2] \ddot{x}+\dot{x} = x(a-1)(\sin x-a) \dot{x}=y \dot{y} =-y-x\sin x V V = \frac{y^2}2 - x\cos x + \sin x \dot{V} = -y^2","['derivatives', 'dynamical-systems', 'nonlinear-system', 'differential', 'nonlinear-dynamics']"
32,Asking for clarification of a Wikipedia article on uniform continuity,Asking for clarification of a Wikipedia article on uniform continuity,,"A portion of Wikipedia article on uniform continuity Functions that have slopes that become unbounded on an infinite domain cannot be uniformly continuous. The exponential function $ x\mapsto e^{x}$ is continuous everywhere on the real line but is not uniformly continuous on the line, since its derivative tends to infinity as $x\to \infty$ We know that  a differentiable function $f:\Bbb{R}\to\Bbb{R}$ is Lipschitz iff $\|f'\|<\infty$ . $f$ Lipschitz implies $f$ is uniformly continuous. $f$ uniformly continuous but $f$ may not be differentiable $( x\mapsto{|x|})$ . Even when $f'$ exists, $f'$ need not be bounded $(x\mapsto{\sqrt{x}} $ on $(0, \infty) ) $ Please elaborate the quoted portion.","A portion of Wikipedia article on uniform continuity Functions that have slopes that become unbounded on an infinite domain cannot be uniformly continuous. The exponential function is continuous everywhere on the real line but is not uniformly continuous on the line, since its derivative tends to infinity as We know that  a differentiable function is Lipschitz iff . Lipschitz implies is uniformly continuous. uniformly continuous but may not be differentiable . Even when exists, need not be bounded on Please elaborate the quoted portion."," x\mapsto e^{x} x\to \infty f:\Bbb{R}\to\Bbb{R} \|f'\|<\infty f f f f ( x\mapsto{|x|}) f' f' (x\mapsto{\sqrt{x}}  (0, \infty) ) ","['real-analysis', 'derivatives', 'soft-question', 'uniform-continuity']"
33,Why does taking the derivative of a parabola give the axis of symmetry? [closed],Why does taking the derivative of a parabola give the axis of symmetry? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question So, I was doing this exercise and it says that to find the axis of symmetry of a general parabola $$ax^2+bxy+cy^2+dx+ey+f=0$$ I can just take the partial derivative of this conic. As I didn't understand why this works, I tried doing the derivative on a normal parabola (the one that has the axis parallel to the x or y axis $$ ax^2+bx+c=0 $$ ) and it works as well. Can someone explain why this works?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question So, I was doing this exercise and it says that to find the axis of symmetry of a general parabola I can just take the partial derivative of this conic. As I didn't understand why this works, I tried doing the derivative on a normal parabola (the one that has the axis parallel to the x or y axis ) and it works as well. Can someone explain why this works?",ax^2+bxy+cy^2+dx+ey+f=0  ax^2+bx+c=0 ,"['linear-algebra', 'geometry', 'derivatives']"
34,First Fundamental Theorem of Calculus Domain,First Fundamental Theorem of Calculus Domain,,"Suppose $F:[-1,4]\to\Bbb R$ , $F(x):=\int_{-3}^{2x^4+x^2+1}{e^{-t}}$ , $x \in [-1,4]$ . I want to find a formula for the derivative without the integral symbol. So I know how to do this by splitting the integral up to give: $F(x):=\int_{0}^{2x^4+x^2+1}{e^{-t}} =\int_{0}^{2x^4+x^2+1}{e^{-t}} +  \int_{-3}^{0}{e^{-t}}$ . Then the second integral has derivative $0$ and then you define a composite function for the first integral with upper limit $x$ and a function mapping $x$ to $2x^4+x^2+1$ . My question is that I know I have a problem because I need to prove the functions are continuous on $[-1,4]$ but the lower limit $-3$ is outside the domain. So what do I do ? Do I say the integral can't be evaluated since $-3$ is outside the domain, or do I split the $\int_{-3}^{0}{e^{-t}} = \int_{-1}^{0}{e^{-t}}+\int_{-3}^{-1}{e^{-t}}$ and ignore the $\int_{-3}^{-1}{e^{-t}}$ ? Cheers.","Suppose , , . I want to find a formula for the derivative without the integral symbol. So I know how to do this by splitting the integral up to give: . Then the second integral has derivative and then you define a composite function for the first integral with upper limit and a function mapping to . My question is that I know I have a problem because I need to prove the functions are continuous on but the lower limit is outside the domain. So what do I do ? Do I say the integral can't be evaluated since is outside the domain, or do I split the and ignore the ? Cheers.","F:[-1,4]\to\Bbb R F(x):=\int_{-3}^{2x^4+x^2+1}{e^{-t}} x \in [-1,4] F(x):=\int_{0}^{2x^4+x^2+1}{e^{-t}} =\int_{0}^{2x^4+x^2+1}{e^{-t}} +  \int_{-3}^{0}{e^{-t}} 0 x x 2x^4+x^2+1 [-1,4] -3 -3 \int_{-3}^{0}{e^{-t}} = \int_{-1}^{0}{e^{-t}}+\int_{-3}^{-1}{e^{-t}} \int_{-3}^{-1}{e^{-t}}","['calculus', 'integration']"
35,Alternate multinomial theorem for $\frac{d^n}{dx^n}\prod\limits_{k=1}^m f_k(x)$ without $\sum\limits_{k_1+\dots+k_m=n}$ nor Kronecker delta.,Alternate multinomial theorem for  without  nor Kronecker delta.,\frac{d^n}{dx^n}\prod\limits_{k=1}^m f_k(x) \sum\limits_{k_1+\dots+k_m=n},"The generalized product rule complicates putting series coefficients into closed or hypergeometric form. There are 2 forms with Lagrange $n$ th derivative notation and the multinomial $\binom n{n_1,\dots,n_j}$ ; the $\displaystyle\sum_{\sum\limits_jn_j=n}$ version : $$\left(\prod_{k=1}^m f_k\right)^{(n)}=\sum_{\sum\limits_jn_j=n} \binom n{n_1,\dots,n_m} \prod_{k=1}^m f_k^{(n_k)}$$ and the the $\displaystyle\sum_{n_1,\dots,n_j=0}^n$ version with Kronecker delta $\delta_{a,b}$ : $$\left(\prod_{k=1}^m f_k\right)^{(n)}=\sum_{n_1=0}^n\dots\sum_{n_m=0}^n\delta_{n,\sum\limits_{j=1}^mn_j} \binom n{n_1,\dots,n_m} \prod_{k=1}^m f_k^{(n_k)}.$$ But evaluating these sum by sum is complicated for given $f_k(x)$ , so here is an alternate multinomial theorem without a single sum over a multivariable restriction nor a tensor function via repeated generalized Leibniz rule : $$(fg)^{(n)}=\sum_{k=0}^n\binom nkf^{(k)}g^{(n-k)}$$ Trying with $5$ functions, one gets: $$\begin{align}(f_1f_2f_3f_4f_5)^{(n)}=\sum_{n_1=0}^n\binom n{n_1}f_1^{(n-n_1)}\sum_{n_2=0}^{n_1}\binom{n_1}{n_2}f_2^{(n_1-n_2)}(f_3f_4f_5)^{(n_2)}= \sum_{n_1=0}^n\binom n{n_1}f_1^{(n-n_1)}\sum_{n_2=0}^{n_1}\binom{n_1}{n_2}f_2^{(n_1-n_2)}\sum_{n_3=0}^{n_2}\binom{n_2}{n_3}f_3^{(n_2-n_3)}\sum_{n_4=0}^{n_3}\binom{n_3}{n_4}f_4^{(n_3-n_4)}f_5^{(n_4)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\sum_{n_3=0}^{n_2}\sum_{n_4=0}^{n_3}\frac{n!f_1^{(n-n_1)} f_2^{(n_1-n_2)} f_3^{(n_2-n_3)} f_4^{(n_3-n_4)}f_5^{(n_4)}}{(n-n_1)!(n_1-n_2)!(n_2-n_3)!(n_3-n_4)!n_4!}\end{align}$$ Therefore: $$\boxed{\left(\prod_{k=1}^mf_k\right)^{(n)}\mathop=^?\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\dots\sum_{n_{m-1}=0}^{n_{m-2}}\binom n{n_1}f_1^{(n-n_1)}f_m^{(n_{m-1})}\prod_{k=2}^{m-1}\binom{n_{k-1}}{n_k}f_k^{(n_{k-1}-n_k)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\dots\sum_{n_{m-1}=0}^{n_{m-2}}\frac{n!f_1^{(n-n_1)}f_m^{(n_{m-1})}}{(n-n_1)!n_{m-1}!}\prod_{k=2}^{m-1}\frac{f_k^{(n_{k-1}-n_k)}}{(n_{k-1}-n_k)!}}\tag1$$ Similarly, but without $(n_{k-1}-n_k)$ in any but $1$ $n$ th derivative and factorial: $$(f_1f_2f_3f_4f_5)^{(n)}=\sum_{n_1=0}^n\binom n{n_1}f_1^{(n_1)}(f_2f_3f_4f_5)^{(n-n_1)}= \sum_{n_1=0}^n\binom n{n_1}f_1^{(n_1)}\sum_{n_2=0}^{n-n_1}\binom{n-n_1}{n_2}f_2^{(n_2)}\sum_{n_3=0}^{n-n_1-n_2}\binom{n-n_1-n_2}{n_3}f_3^{(n_3)}\sum_{n_4=0}^{n-n_1-n_2-n_3}\binom{n-n_1-n_2-n_3}{n_4} f_4^{(4)} f_5^{(n-n_1-n_2-n_3-n_4)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\sum_{n_3=0}^{n-n_1-n_2}\sum_{n_4=0}^{n-n_1-n_2-n_3}\frac{n!f_1^{(n_1)}f_2^{(n_2)}f_3^{(n_3)}f_4^{(n_4)}f_5^{(n-n_1-n_2-n_3-n_4)}}{n_1!n_2!n_3!n_4!(n-n_1-n_2-n_3-n_4)!}$$ Therefore: $$\boxed{\left(\prod_{k=1}^mf_k\right)^{(n)}\mathop=^?\sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\dots\sum_{n_{m-1}=0}^{n-\sum\limits_{k=1}^{m-2}n_k}\frac{n!f_m^{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)}}{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)!}\prod_{k=1}^{m-1}\frac{f_k^{(n_k)}}{n_k!}= \sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\dots\sum_{n_{m-1}=0}^{n-\sum\limits_{k=1}^{m-2}n_k}\binom n{n_1,\dots,n_{m-1}}\frac{f_m^{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)}}{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)!}\prod_{k=1}^{m-1}f_k^{(n_k)}}\tag2$$ Are $(1),(2)$ correct and fully simplified?","The generalized product rule complicates putting series coefficients into closed or hypergeometric form. There are 2 forms with Lagrange th derivative notation and the multinomial ; the version : and the the version with Kronecker delta : But evaluating these sum by sum is complicated for given , so here is an alternate multinomial theorem without a single sum over a multivariable restriction nor a tensor function via repeated generalized Leibniz rule : Trying with functions, one gets: Therefore: Similarly, but without in any but th derivative and factorial: Therefore: Are correct and fully simplified?","n \binom n{n_1,\dots,n_j} \displaystyle\sum_{\sum\limits_jn_j=n} \left(\prod_{k=1}^m f_k\right)^{(n)}=\sum_{\sum\limits_jn_j=n} \binom n{n_1,\dots,n_m} \prod_{k=1}^m f_k^{(n_k)} \displaystyle\sum_{n_1,\dots,n_j=0}^n \delta_{a,b} \left(\prod_{k=1}^m f_k\right)^{(n)}=\sum_{n_1=0}^n\dots\sum_{n_m=0}^n\delta_{n,\sum\limits_{j=1}^mn_j} \binom n{n_1,\dots,n_m} \prod_{k=1}^m f_k^{(n_k)}. f_k(x) (fg)^{(n)}=\sum_{k=0}^n\binom nkf^{(k)}g^{(n-k)} 5 \begin{align}(f_1f_2f_3f_4f_5)^{(n)}=\sum_{n_1=0}^n\binom n{n_1}f_1^{(n-n_1)}\sum_{n_2=0}^{n_1}\binom{n_1}{n_2}f_2^{(n_1-n_2)}(f_3f_4f_5)^{(n_2)}= \sum_{n_1=0}^n\binom n{n_1}f_1^{(n-n_1)}\sum_{n_2=0}^{n_1}\binom{n_1}{n_2}f_2^{(n_1-n_2)}\sum_{n_3=0}^{n_2}\binom{n_2}{n_3}f_3^{(n_2-n_3)}\sum_{n_4=0}^{n_3}\binom{n_3}{n_4}f_4^{(n_3-n_4)}f_5^{(n_4)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\sum_{n_3=0}^{n_2}\sum_{n_4=0}^{n_3}\frac{n!f_1^{(n-n_1)} f_2^{(n_1-n_2)} f_3^{(n_2-n_3)} f_4^{(n_3-n_4)}f_5^{(n_4)}}{(n-n_1)!(n_1-n_2)!(n_2-n_3)!(n_3-n_4)!n_4!}\end{align} \boxed{\left(\prod_{k=1}^mf_k\right)^{(n)}\mathop=^?\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\dots\sum_{n_{m-1}=0}^{n_{m-2}}\binom n{n_1}f_1^{(n-n_1)}f_m^{(n_{m-1})}\prod_{k=2}^{m-1}\binom{n_{k-1}}{n_k}f_k^{(n_{k-1}-n_k)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n_1}\dots\sum_{n_{m-1}=0}^{n_{m-2}}\frac{n!f_1^{(n-n_1)}f_m^{(n_{m-1})}}{(n-n_1)!n_{m-1}!}\prod_{k=2}^{m-1}\frac{f_k^{(n_{k-1}-n_k)}}{(n_{k-1}-n_k)!}}\tag1 (n_{k-1}-n_k) 1 n (f_1f_2f_3f_4f_5)^{(n)}=\sum_{n_1=0}^n\binom n{n_1}f_1^{(n_1)}(f_2f_3f_4f_5)^{(n-n_1)}= \sum_{n_1=0}^n\binom n{n_1}f_1^{(n_1)}\sum_{n_2=0}^{n-n_1}\binom{n-n_1}{n_2}f_2^{(n_2)}\sum_{n_3=0}^{n-n_1-n_2}\binom{n-n_1-n_2}{n_3}f_3^{(n_3)}\sum_{n_4=0}^{n-n_1-n_2-n_3}\binom{n-n_1-n_2-n_3}{n_4} f_4^{(4)} f_5^{(n-n_1-n_2-n_3-n_4)}=\sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\sum_{n_3=0}^{n-n_1-n_2}\sum_{n_4=0}^{n-n_1-n_2-n_3}\frac{n!f_1^{(n_1)}f_2^{(n_2)}f_3^{(n_3)}f_4^{(n_4)}f_5^{(n-n_1-n_2-n_3-n_4)}}{n_1!n_2!n_3!n_4!(n-n_1-n_2-n_3-n_4)!} \boxed{\left(\prod_{k=1}^mf_k\right)^{(n)}\mathop=^?\sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\dots\sum_{n_{m-1}=0}^{n-\sum\limits_{k=1}^{m-2}n_k}\frac{n!f_m^{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)}}{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)!}\prod_{k=1}^{m-1}\frac{f_k^{(n_k)}}{n_k!}= \sum_{n_1=0}^n\sum_{n_2=0}^{n-n_1}\dots\sum_{n_{m-1}=0}^{n-\sum\limits_{k=1}^{m-2}n_k}\binom n{n_1,\dots,n_{m-1}}\frac{f_m^{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)}}{\left(n-\sum\limits_{k=1}^{m-1}n_k\right)!}\prod_{k=1}^{m-1}f_k^{(n_k)}}\tag2 (1),(2)","['derivatives', 'multinomial-coefficients', 'index-notation', 'multinomial-theorem', 'kronecker-delta']"
36,$\frac{dy}{dx}$ Vs. $\dfrac{1}{\frac{dx}{dy}}$,Vs.,\frac{dy}{dx} \dfrac{1}{\frac{dx}{dy}},"Today, my friend gave me a question which is stated below: On the curve $x^3 = 12y$ , the abscissa changes at a faster rate than the ordinate. Then find the interval in which $x$ belongs to. I did it as follows: Differentiating $x^3 = 12y$ wrt $y$ , $$3x^2\frac{dx}{dy} = 12$$ $$\implies \frac{dx}{dy} = \frac{12}{3x^2}\quad, x \not = 0$$ $$\implies \frac{dx}{dy} = \frac{4}{x^2}\quad, x \not = 0$$ Now $\frac{dx}{dy}$ must be greater than $1$ so that the rate of change of abscissa is greater than rate of change of ordinate. So, $$\dfrac{4}{x^2} > 1\quad, x \not =  0\implies \boxed{x \in (-2, 2) - \{0\}}$$ which is correct according to our book. Whereas, my friend solved it as follows: Differentiating $x^3 = 12y$ wrt $x$ , $$\implies 3x^2 = 12 \dfrac{dy}{dx}$$ $$\implies \dfrac{3 x^2}{12} = \dfrac{dy}{dx}$$ $$\implies \dfrac{x^2}{4} = \dfrac{dy}{dx}$$ Now, $\frac{dy}{dx}$ must be less than $1$ so that the abscissa changes at a faster rate than the ordinate. So, $$\frac{x^2}{4} < 1 \implies\boxed{ x \in (-2, 2)}$$ What's the mistake? Whose answer is correct?","Today, my friend gave me a question which is stated below: On the curve , the abscissa changes at a faster rate than the ordinate. Then find the interval in which belongs to. I did it as follows: Differentiating wrt , Now must be greater than so that the rate of change of abscissa is greater than rate of change of ordinate. So, which is correct according to our book. Whereas, my friend solved it as follows: Differentiating wrt , Now, must be less than so that the abscissa changes at a faster rate than the ordinate. So, What's the mistake? Whose answer is correct?","x^3 = 12y x x^3 = 12y y 3x^2\frac{dx}{dy} = 12 \implies \frac{dx}{dy} = \frac{12}{3x^2}\quad, x \not = 0 \implies \frac{dx}{dy} = \frac{4}{x^2}\quad, x \not = 0 \frac{dx}{dy} 1 \dfrac{4}{x^2} > 1\quad, x \not =  0\implies \boxed{x \in (-2, 2) - \{0\}} x^3 = 12y x \implies 3x^2 = 12 \dfrac{dy}{dx} \implies \dfrac{3 x^2}{12} = \dfrac{dy}{dx} \implies \dfrac{x^2}{4} = \dfrac{dy}{dx} \frac{dy}{dx} 1 \frac{x^2}{4} < 1 \implies\boxed{ x \in (-2, 2)}",['calculus']
37,Understanding Differential Operator?,Understanding Differential Operator?,,"$\frac{d}{dx}$ is an operator used often in calculus, but I don't understand this use of notation, for example I saw recently in a problem an expression's derivative given as $\frac{d}{dx}(f(x)$ , I'm confused about this, as $f(x)$ refers to a number, and represented as an expression, we cannot have $\frac{d}{dx}$ for example is $\frac{d}{dx}$ acting on the number, or the expression, I understand it mapping between a function $f$ and a function $f'$ but $f(x)$ is not a function but a value. We even see something where we use $\frac{d}{dx}$ without even specifying the function, just as $\frac{d}{dx}(e^x)$ . Do we see it as 'the operator on the function defined by the expression'? If we cannot substitute into $x$ when an expression is enclosed by the differential operator, we get $\frac{d}{dx}$ (a) which makes no sense, so is $x$ bound in this expression and how does this work considering the expression $\frac{d}{dx}f(x)$ depends on the value $x$ which is not how bound variables work.","is an operator used often in calculus, but I don't understand this use of notation, for example I saw recently in a problem an expression's derivative given as , I'm confused about this, as refers to a number, and represented as an expression, we cannot have for example is acting on the number, or the expression, I understand it mapping between a function and a function but is not a function but a value. We even see something where we use without even specifying the function, just as . Do we see it as 'the operator on the function defined by the expression'? If we cannot substitute into when an expression is enclosed by the differential operator, we get (a) which makes no sense, so is bound in this expression and how does this work considering the expression depends on the value which is not how bound variables work.",\frac{d}{dx} \frac{d}{dx}(f(x) f(x) \frac{d}{dx} \frac{d}{dx} f f' f(x) \frac{d}{dx} \frac{d}{dx}(e^x) x \frac{d}{dx} x \frac{d}{dx}f(x) x,"['calculus', 'derivatives', 'notation']"
38,"Find all $f:\mathbb{R}\to \mathbb{R}, f\in C^1$ so that $q, f(q)$ has the same denominator as $q$",Find all  so that  has the same denominator as,"f:\mathbb{R}\to \mathbb{R}, f\in C^1 q, f(q) q","Find all continuously differentiable functions $f:\mathbb{R}\to \mathbb{R}$ so that for every rational number $q, f(q)$ is rational and has the same denominator as $q$ in lowest terms (to simplify, say $f(q)$ has the same denominator as $q$ ). Clearly $-x + n$ and $x+n$ work for any integer $n$ . But I'm not sure how to prove they're the only ones.  I found the solution below, but I don't understand why $f((an+1)/(bn)) = f(a/b)$ for sufficiently large n has denominator $b$ rather than $bn$ . Also why does that yield a contradiction? Similarly, why can't $|c|$ be greater than 1? I don't understand their explanation. Specifically, I believe it's not true that for $n$ sufficiently large $\dfrac{an+1}{bn}$ has denominator bn.","Find all continuously differentiable functions so that for every rational number is rational and has the same denominator as in lowest terms (to simplify, say has the same denominator as ). Clearly and work for any integer . But I'm not sure how to prove they're the only ones.  I found the solution below, but I don't understand why for sufficiently large n has denominator rather than . Also why does that yield a contradiction? Similarly, why can't be greater than 1? I don't understand their explanation. Specifically, I believe it's not true that for sufficiently large has denominator bn.","f:\mathbb{R}\to \mathbb{R} q, f(q) q f(q) q -x + n x+n n f((an+1)/(bn)) = f(a/b) b bn |c| n \dfrac{an+1}{bn}","['calculus', 'elementary-number-theory', 'derivatives', 'divisibility']"
39,Velocity with respect to position [closed],Velocity with respect to position [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question We are given an expression for an object's position with respect to time. To find its velocity at $t=0,$ can we put $t=0$ into the position then divide by zero? If not, why?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question We are given an expression for an object's position with respect to time. To find its velocity at can we put into the position then divide by zero? If not, why?","t=0, t=0","['derivatives', 'kinematics']"
40,How do we formally and precisely specify a directed angle?,How do we formally and precisely specify a directed angle?,,"I haven't been able to find a precise explanation of what a directed angle is. I encountered the concept in Chapter 15 of Spivak's Calculus , where he simply says In elementary geometry an angle is simply the union of two half-lines with a common initial point. More useful for trigonometry are ""directed angles"", which may be regarded as pairs $(l_1,l_2)$ of half-lines with the same initial point. I googled and found this document , which sort of sheds a bit of light. We specify a directed angle as an ordered pair $(\vec{OA}, \vec{OB})$ plus a direction. However, they specify the direction with a picture, as in Also, apparently it is by convention that if the direction is counterclockwise, then the angle is positive, and if clockwise then it is negative. When I think about it, in common usage, it seems we say $50^{\circ}$ or $-70^{\circ}$ , and this is a measure of magnitude of an angle. We associate the magnitude with a particular angle. We know that if the magnitude is negative, for example, then the angle is being measured counterclockwise. Is this in fact the notation to specify a directed angle, ie the magnitude? I found the following definition of a directed angle here Definition: Given any two non-parallel lines $l$ and $m$ , we defined the directed angle $\angle(l,m)$ to be the measure of the angle starting from $l$ and ending at $m$ , measured counterclockwise. Then Notice that $$\angle(l,m)+\angle(m,l)=180^{\circ}\tag{1}$$ holds universally. This is kind of nice, but it's a bit annoying to have that $180^{\circ}$ lying around there, and so we will also the all angle measures modulo $180^{\circ}$ . That means that $-70^{\circ}=110^{\circ}=290^{\circ}=...$ Once we take mod $180^{\circ}$ , $(1)$ becomes the following very important result Proposition: For any lines $l$ and $m$ , $$\angle(l,m)=-\angle(m,l)\tag{2}$$ (In other words, measuring the angle clockwise instead of counterclockwise corresponds to negation). I'm not sure I follow the calculations. I'm not too familiar with modular arithmetic. We have $$180 \mod 180=0$$ Thus $$[\angle(l,m)+\angle(m,l)] \mod 180=0$$ But as far as I can tell $$[\angle(l,m)+\angle(m,l)] \mod 180=\left [[\angle(l,m)\mod 180]+[\angle(m,l)\mod 180]\right ] \mod 180=0$$ How do we obtain $(2)$ ? Also, are these calculations general in the sense that they explain why when we measure angles in a clockwise direction they are negative? I would guess not. Is the reason rather just convention? The convention being that when we specify a directed angle as an ordered pair $(l,m)$ , by default we mean counterclockwise?","I haven't been able to find a precise explanation of what a directed angle is. I encountered the concept in Chapter 15 of Spivak's Calculus , where he simply says In elementary geometry an angle is simply the union of two half-lines with a common initial point. More useful for trigonometry are ""directed angles"", which may be regarded as pairs of half-lines with the same initial point. I googled and found this document , which sort of sheds a bit of light. We specify a directed angle as an ordered pair plus a direction. However, they specify the direction with a picture, as in Also, apparently it is by convention that if the direction is counterclockwise, then the angle is positive, and if clockwise then it is negative. When I think about it, in common usage, it seems we say or , and this is a measure of magnitude of an angle. We associate the magnitude with a particular angle. We know that if the magnitude is negative, for example, then the angle is being measured counterclockwise. Is this in fact the notation to specify a directed angle, ie the magnitude? I found the following definition of a directed angle here Definition: Given any two non-parallel lines and , we defined the directed angle to be the measure of the angle starting from and ending at , measured counterclockwise. Then Notice that holds universally. This is kind of nice, but it's a bit annoying to have that lying around there, and so we will also the all angle measures modulo . That means that Once we take mod , becomes the following very important result Proposition: For any lines and , (In other words, measuring the angle clockwise instead of counterclockwise corresponds to negation). I'm not sure I follow the calculations. I'm not too familiar with modular arithmetic. We have Thus But as far as I can tell How do we obtain ? Also, are these calculations general in the sense that they explain why when we measure angles in a clockwise direction they are negative? I would guess not. Is the reason rather just convention? The convention being that when we specify a directed angle as an ordered pair , by default we mean counterclockwise?","(l_1,l_2) (\vec{OA}, \vec{OB}) 50^{\circ} -70^{\circ} l m \angle(l,m) l m \angle(l,m)+\angle(m,l)=180^{\circ}\tag{1} 180^{\circ} 180^{\circ} -70^{\circ}=110^{\circ}=290^{\circ}=... 180^{\circ} (1) l m \angle(l,m)=-\angle(m,l)\tag{2} 180 \mod 180=0 [\angle(l,m)+\angle(m,l)] \mod 180=0 [\angle(l,m)+\angle(m,l)] \mod 180=\left [[\angle(l,m)\mod 180]+[\angle(m,l)\mod 180]\right ] \mod 180=0 (2) (l,m)","['calculus', 'geometry', 'derivatives', 'trigonometry', 'angle']"
41,Convergence of derivatives near the boundary of an open interval,Convergence of derivatives near the boundary of an open interval,,"Suppose there are two continuously differentiable functions $a(x_1)$ and $b(x_2)$ , where $x_1, x_2\in \mathbb{R}_+$ . I assume that $a'(x_1) >0$ if $x_1<x^o$ and $a'(x_1)<0$ if $x_1>x^o$ . It follows that $a'(x^o)=0$ and $x^o$ is the point that gives the maximum of $a(x_1)$ . Similarly, I assume $b'(x_2) >0$ if $x_2<x^p$ and $b'(x_2)<0$ if $x_2>x^p$ . It follows that $b'(x^p)=0$ and $x^p$ is the point that gives the maximum of $b(x_2)$ . I assume $x^p > x^o$ and take the open interval $(x^o, x^p)$ . Suppose $\exists x^q \in (x^o, x^p): |a'(x_1)|>b'(x_2) \forall x_1 \in (x^o, x^p) $ and $\forall x_2 \in (x^q, x^p)$ . I would like to then say that $|a'(x^c)| = b'(x^q)$ ,  where $x^c$ is a point infinitesmally close to $x^o$ such that $|a'(x^c)| >0$ . The problem is there is always a point closer to $x^o$ than $x^c$ . How would I say that $|a'(x_1)|$ and $b'(x_2)$ converge at $x_1 = x^c$ and $x_2 = x^q$ , without negating all the values of $x_1$ that exist between $x^o$ and $x^c$ ?","Suppose there are two continuously differentiable functions and , where . I assume that if and if . It follows that and is the point that gives the maximum of . Similarly, I assume if and if . It follows that and is the point that gives the maximum of . I assume and take the open interval . Suppose and . I would like to then say that ,  where is a point infinitesmally close to such that . The problem is there is always a point closer to than . How would I say that and converge at and , without negating all the values of that exist between and ?","a(x_1) b(x_2) x_1, x_2\in \mathbb{R}_+ a'(x_1) >0 x_1<x^o a'(x_1)<0 x_1>x^o a'(x^o)=0 x^o a(x_1) b'(x_2) >0 x_2<x^p b'(x_2)<0 x_2>x^p b'(x^p)=0 x^p b(x_2) x^p > x^o (x^o, x^p) \exists x^q \in (x^o, x^p): |a'(x_1)|>b'(x_2) \forall x_1 \in (x^o, x^p)  \forall x_2 \in (x^q, x^p) |a'(x^c)| = b'(x^q) x^c x^o |a'(x^c)| >0 x^o x^c |a'(x_1)| b'(x_2) x_1 = x^c x_2 = x^q x_1 x^o x^c","['derivatives', 'convergence-divergence', 'optimization', 'infinitesimals']"
42,"How to interpret FTC1 that assumes integrability on $[a,b]$ and continuity at $c \in [a,b]$, as opposed to continuity on $[a,b]$?","How to interpret FTC1 that assumes integrability on  and continuity at , as opposed to continuity on ?","[a,b] c \in [a,b] [a,b]","Consider the statement of the first fundamental theorem of calculus in Chapter 14 of Spivak's Calculus Let $f$ be integrable on $[a,b]$ , and define $F$ on $[a,b]$ by $$F(x)=\int\limits_a^x f\tag{1}$$ If $f$ is continuous at $c$ in $[a,b]$ , then $F$ is differentiable at $c$ , and $F'(c)=f(c)$ (If $c=a$ or $b$ , then $F'(c)$ is understood to mean the right- or left-hand derivative of $F$ ) In Chapter 13, entitled ""Integrals"", there is a theorem which says $f$ continuous on $[a,b]$ $\implies$ $f$ integrable on $[a,b]$ Why didn't the statement of the fundamental theorem simply say Define $F$ on $[a,b]$ by $$F(x)=\int\limits_a^x f$$ If $f$ is continuous at $c$ in $[a,b]$ , then $F$ is differentiable at $c$ , and $F'(c)=f(c)$ . Here is my attempt at explaining this Consider the function $$f(x)=\begin{cases} x, \text{ if } x \text{ rational } \\ 0, \text{  if } x \text{ irrational }\end{cases}$$ and consider the point $x=0$ . $f$ is continuous at $0$ (and nowhere else). Let $[a,b]$ be any interval containing $0$ , and let $P$ be a partition on $[a,b]$ . Then $$L(f,P)=\sum\limits_{i=1}^n m_i \Delta t_i = 0$$ $$U(f,P)=\sum\limits_{i=1}^n M_i \Delta t_i>0$$ Hence $L(f,P) \neq U(f,P)$ for all partitions, and hence $f$ is not integrable on $[a,b]$ . Thus the right-hand side of $(1)$ isn't even defined. So is the reason we can't use the second proposed version of the theorem because of cases such this? It seems that most of the statements of the first fundamental theorem of calculus use continuity of $f$ on an interval $[a,b]$ . This rules out the example I gave above. The first version above seems to be more general, because it accommodates continuity on an interval. If $f$ is continuous on $[a,b]$ then it is integrable on $[a,b]$ , and so we can apply the first theorem to each point in this interval to conclude that for all $x \in [a,b]$ , $F'(x)=f(x)$ . Is this analysis correct?","Consider the statement of the first fundamental theorem of calculus in Chapter 14 of Spivak's Calculus Let be integrable on , and define on by If is continuous at in , then is differentiable at , and (If or , then is understood to mean the right- or left-hand derivative of ) In Chapter 13, entitled ""Integrals"", there is a theorem which says continuous on integrable on Why didn't the statement of the fundamental theorem simply say Define on by If is continuous at in , then is differentiable at , and . Here is my attempt at explaining this Consider the function and consider the point . is continuous at (and nowhere else). Let be any interval containing , and let be a partition on . Then Hence for all partitions, and hence is not integrable on . Thus the right-hand side of isn't even defined. So is the reason we can't use the second proposed version of the theorem because of cases such this? It seems that most of the statements of the first fundamental theorem of calculus use continuity of on an interval . This rules out the example I gave above. The first version above seems to be more general, because it accommodates continuity on an interval. If is continuous on then it is integrable on , and so we can apply the first theorem to each point in this interval to conclude that for all , . Is this analysis correct?","f [a,b] F [a,b] F(x)=\int\limits_a^x f\tag{1} f c [a,b] F c F'(c)=f(c) c=a b F'(c) F f [a,b] \implies f [a,b] F [a,b] F(x)=\int\limits_a^x f f c [a,b] F c F'(c)=f(c) f(x)=\begin{cases} x, \text{ if } x \text{ rational } \\ 0, \text{
 if } x \text{ irrational }\end{cases} x=0 f 0 [a,b] 0 P [a,b] L(f,P)=\sum\limits_{i=1}^n m_i \Delta t_i = 0 U(f,P)=\sum\limits_{i=1}^n M_i \Delta t_i>0 L(f,P) \neq U(f,P) f [a,b] (1) f [a,b] f [a,b] [a,b] x \in [a,b] F'(x)=f(x)","['calculus', 'integration', 'derivatives']"
43,Help to show that f is not differentiable.,Help to show that f is not differentiable.,,"Let $ 0 < \alpha < 1 $ . If $\vert f(x)\vert \geq \vert x\vert^\alpha$ for all $x$ and $f(0) =0$ , then $f$ is not differentiable at $x=0$ . I would appreciate some help to prove this. Edit : I supposed that $f$ is differentiable at 0 and used the definition of derivative as a limit. Since $f(0)=0$ , I get $ \lim_{h \to 0} \frac{f(h)}{h} \geq  \lim_{h \to 0} \frac{1}{h^{1- \alpha}} $ by hypothesis and the fact that limit preserves inequality. Since the second limit goes to infinity, then the first limit goes to infinity too, and this is a contradiction with the fact that $f’(0)$ exist. Its that right?","Let . If for all and , then is not differentiable at . I would appreciate some help to prove this. Edit : I supposed that is differentiable at 0 and used the definition of derivative as a limit. Since , I get by hypothesis and the fact that limit preserves inequality. Since the second limit goes to infinity, then the first limit goes to infinity too, and this is a contradiction with the fact that exist. Its that right?", 0 < \alpha < 1  \vert f(x)\vert \geq \vert x\vert^\alpha x f(0) =0 f x=0 f f(0)=0  \lim_{h \to 0} \frac{f(h)}{h} \geq  \lim_{h \to 0} \frac{1}{h^{1- \alpha}}  f’(0),"['calculus', 'derivatives']"
44,Unexpected result when calculating shortest distance between point and curve,Unexpected result when calculating shortest distance between point and curve,,"My math book has following problem: ""Find distance between point $(2,0)$ and curve $y=\sqrt{x}$ ."" I assume this asks for the shortest distance because I cannot think of any other distance that would be meaningful. I tried solving it this way: Define distance as function of $x$ $$ D(x) = \sqrt{(x-2)^2+(\sqrt{x}-0)^2} $$ Find derivate of $D(x)$ $$ D'(x) = \frac{2x-3}{2\sqrt{x^2-3x+4}}$$ Shortest distance is where the derivate equals zero $$ 0 = \frac{2x-3}{2\sqrt{x^2-3x+4}} $$ $$ x = \frac{3}{2} $$ Get distance using the value of $x$ $$ D\left(\frac{3}{2}\right) = \sqrt{\left(\frac{3}{2}-2\right)^2+\left(\sqrt{\frac{3}{2}}-0\right)^2} = \frac{\sqrt7}{2} \approx 1.3$$ But my book states that result is $1.56$ . Is there error in my calculation? Or could there be some other meaningful distance I should be calculating instead?","My math book has following problem: ""Find distance between point and curve ."" I assume this asks for the shortest distance because I cannot think of any other distance that would be meaningful. I tried solving it this way: Define distance as function of Find derivate of Shortest distance is where the derivate equals zero Get distance using the value of But my book states that result is . Is there error in my calculation? Or could there be some other meaningful distance I should be calculating instead?","(2,0) y=\sqrt{x} x  D(x) = \sqrt{(x-2)^2+(\sqrt{x}-0)^2}  D(x)  D'(x) = \frac{2x-3}{2\sqrt{x^2-3x+4}}  0 = \frac{2x-3}{2\sqrt{x^2-3x+4}}   x = \frac{3}{2}  x  D\left(\frac{3}{2}\right) = \sqrt{\left(\frac{3}{2}-2\right)^2+\left(\sqrt{\frac{3}{2}}-0\right)^2} = \frac{\sqrt7}{2} \approx 1.3 1.56","['calculus', 'derivatives']"
45,Calculating the gradient of Log-Euclidean distance between SPD matrices on Riemannian manifold,Calculating the gradient of Log-Euclidean distance between SPD matrices on Riemannian manifold,,"In the paper Log-Euclidean metrics for fast and simple calculus on diffusion tensors , the geodesic distance between SPD matrices $A,B$ is defined as $$d(A,B)=||\log A- \log B||_F,$$ where $F$ is the Frobenius norm. I did not find more information on the other aspects of the Riemannian structure. I am interested in the gradient (on Riemannian manifold) of the distance function $$ f(X) = d(X,A_0). $$ I am not familiar with geometry other than some basics, $\langle grad f,X\rangle = Xf $ for all smooth vector field $X$ . How can I derive $grad f$ ?","In the paper Log-Euclidean metrics for fast and simple calculus on diffusion tensors , the geodesic distance between SPD matrices is defined as where is the Frobenius norm. I did not find more information on the other aspects of the Riemannian structure. I am interested in the gradient (on Riemannian manifold) of the distance function I am not familiar with geometry other than some basics, for all smooth vector field . How can I derive ?","A,B d(A,B)=||\log A- \log B||_F, F 
f(X) = d(X,A_0).
 \langle grad f,X\rangle = Xf  X grad f","['linear-algebra', 'derivatives', 'differential-geometry', 'riemannian-geometry', 'matrix-norms']"
46,"Spivak, Chapter 10, ""Differentiation"", problem 9: Rate of change of distance between moving particles. Is the solution manual solution incorrect?","Spivak, Chapter 10, ""Differentiation"", problem 9: Rate of change of distance between moving particles. Is the solution manual solution incorrect?",,"Spivak, Chapter 10, ""Differentiation"", problem 9. Particle A moves along the positive horizontal axis, and particle B along the graph of $f(x)=-\sqrt{3}x, x \leq 0$ . At a certain time, A is at the point (5,0) and moving with speed 3 units/sec; and B is at a distance of 3 units from the origin and moving with speed 4 units/sec. At what rate is the distance between A and B changing? The solution I came up with is similar to the solution manual solution, but it seems we disagree on certain signs which leads to different results. My result is that the distance is changing at a rate of $-\frac{5}{14}$ units/sec (ie it is decreasing), and the solution manual says the distance is increasing at $\frac{83}{14}$ units/sec. Given that the vertical distance is certainly decreasing, and that since horizontally both particles are moving to the right the relative speed must be smaller than 3 units/second, it seems to me that the result $\frac{83}{14} \approx 5.92$ units/sec is incorrect. Here is my solution Everything in this problem is analyzed at a single point in time. The variables below are all functions of time, though I will omit the time parameter. Initial Position of Particle B $$D_{BO}^2=x_B^2+y_B^2$$ $$y_B=-\sqrt{3}x_B$$ $$D_{BO}^2=x_B^2+3x_B^2=4x_B^2\tag{1}$$ $$9=4x_B^2$$ $$x_B=\frac{3}{2}$$ $$\implies y_B=-\frac{3\sqrt{3}}{2}$$ Rates of Change of B's Coordinates $$y_B(t)=-\sqrt{3}x_B(t)$$ $$y_B'=-\sqrt{3}x_B'$$ Here is a tricky part regarding signs. If we take the square root of $(1)$ then $$D_{BO}=2x_B\tag{2}$$ Note that $D_{BO}$ is a distance, so it must be positive. $x_B$ on the other hand is actually negative. But $(2)$ is actually incorrect, it should be $$D_{BO}=2|x_B|$$ $$=\begin{cases} -2x_B, x_B<0 \\ 2x_B, x_B \geq 0 \end{cases}$$ But in our single point in time we are in a situation where $x_B<0$ . In this scenario, we have $$D_{BO}'=-2x_B'$$ Note that $D_{BO}'=-4$ because particle B is moving towards the origin. $$-4=-2x_B'$$ $$x_B'=2$$ $$\implies y_B=-2\sqrt{3}$$ These derivatives make sense: $x_B$ is increasing from a negative value towards $0$ , and $y_B$ is decreasing from a positive value. Rate of Change of Distance Between the Two Particles $$D = \sqrt{y_B^2+(x_A-x_B)^2}$$ Note that since $x_B$ is negative, I am subtracting from $x_A$ to get difference in x-coordinate between the particles. $$D'=\frac{y_By_B'+(x_A-x_B)(x_A'-x_B')}{\sqrt{y_B^2+(x_A-x_B)^2}}$$ We have the values of all these variables at the time we are considering. If we plug them in we get $$D'=-\frac{5}{14}$$ Thus the distance between the particles is decreasing at a rate of $\frac{5}{14}$ units/sec. Is this correct (ie is the solution manual wrong?)","Spivak, Chapter 10, ""Differentiation"", problem 9. Particle A moves along the positive horizontal axis, and particle B along the graph of . At a certain time, A is at the point (5,0) and moving with speed 3 units/sec; and B is at a distance of 3 units from the origin and moving with speed 4 units/sec. At what rate is the distance between A and B changing? The solution I came up with is similar to the solution manual solution, but it seems we disagree on certain signs which leads to different results. My result is that the distance is changing at a rate of units/sec (ie it is decreasing), and the solution manual says the distance is increasing at units/sec. Given that the vertical distance is certainly decreasing, and that since horizontally both particles are moving to the right the relative speed must be smaller than 3 units/second, it seems to me that the result units/sec is incorrect. Here is my solution Everything in this problem is analyzed at a single point in time. The variables below are all functions of time, though I will omit the time parameter. Initial Position of Particle B Rates of Change of B's Coordinates Here is a tricky part regarding signs. If we take the square root of then Note that is a distance, so it must be positive. on the other hand is actually negative. But is actually incorrect, it should be But in our single point in time we are in a situation where . In this scenario, we have Note that because particle B is moving towards the origin. These derivatives make sense: is increasing from a negative value towards , and is decreasing from a positive value. Rate of Change of Distance Between the Two Particles Note that since is negative, I am subtracting from to get difference in x-coordinate between the particles. We have the values of all these variables at the time we are considering. If we plug them in we get Thus the distance between the particles is decreasing at a rate of units/sec. Is this correct (ie is the solution manual wrong?)","f(x)=-\sqrt{3}x, x \leq 0 -\frac{5}{14} \frac{83}{14} \frac{83}{14} \approx 5.92 D_{BO}^2=x_B^2+y_B^2 y_B=-\sqrt{3}x_B D_{BO}^2=x_B^2+3x_B^2=4x_B^2\tag{1} 9=4x_B^2 x_B=\frac{3}{2} \implies y_B=-\frac{3\sqrt{3}}{2} y_B(t)=-\sqrt{3}x_B(t) y_B'=-\sqrt{3}x_B' (1) D_{BO}=2x_B\tag{2} D_{BO} x_B (2) D_{BO}=2|x_B| =\begin{cases} -2x_B, x_B<0 \\ 2x_B, x_B \geq 0 \end{cases} x_B<0 D_{BO}'=-2x_B' D_{BO}'=-4 -4=-2x_B' x_B'=2 \implies y_B=-2\sqrt{3} x_B 0 y_B D = \sqrt{y_B^2+(x_A-x_B)^2} x_B x_A D'=\frac{y_By_B'+(x_A-x_B)(x_A'-x_B')}{\sqrt{y_B^2+(x_A-x_B)^2}} D'=-\frac{5}{14} \frac{5}{14}","['calculus', 'derivatives', 'solution-verification']"
47,What do you call third order derivative matrix and what does it geometrically signify?,What do you call third order derivative matrix and what does it geometrically signify?,,"The first-order derivatives matrix is known as Jacobian, gives the gradient of the graph. Similarly, the second-order derivatives matrix is Hessian, which gives the curvature of the plot. What next? i.e., is there a third-order derivative matrix? If yes, what does it denote?","The first-order derivatives matrix is known as Jacobian, gives the gradient of the graph. Similarly, the second-order derivatives matrix is Hessian, which gives the curvature of the plot. What next? i.e., is there a third-order derivative matrix? If yes, what does it denote?",,"['matrices', 'derivatives', 'jacobian', 'hessian-matrix', 'gradient-flows']"
48,"How can I solve $\int (f^2 - x f_x f ) \, dx$?",How can I solve ?,"\int (f^2 - x f_x f ) \, dx","I am trying to solve the following integral $$\int (f^2 - x f_x f ) \, dx$$ where $f:=f(x)$ is a differentiable function and $f_x = \frac{df}{dx}$ . By substituting $u = \frac{f^2}{x^2}$ , we obtain $du= \frac{2}{x^3} (x f_x f - f^2) \, dx$ and then $$\int (f^2 - x f_x f ) \, d x = -\int \frac{x^3}{2} \, du.$$ Now, using integration by parts, we get $$\int (f^2 - x f_x f ) \, d x = - \frac{x}{2}f^2 + \frac{3}{2} \int f^2 \, dx.$$ I'm unable to proceed further from this point. How can I solve this integral? Thanks in advance.","I am trying to solve the following integral where is a differentiable function and . By substituting , we obtain and then Now, using integration by parts, we get I'm unable to proceed further from this point. How can I solve this integral? Thanks in advance.","\int (f^2 - x f_x f ) \, dx f:=f(x) f_x = \frac{df}{dx} u = \frac{f^2}{x^2} du= \frac{2}{x^3} (x f_x f - f^2) \, dx \int (f^2 - x f_x f ) \, d x = -\int \frac{x^3}{2} \, du. \int (f^2 - x f_x f ) \, d x = - \frac{x}{2}f^2 + \frac{3}{2} \int f^2 \, dx.","['calculus', 'derivatives', 'indefinite-integrals']"
49,"Baby Rudin, Exr. 24 Ch. 5, explanation","Baby Rudin, Exr. 24 Ch. 5, explanation",,"In Rudin's Principles of Mathematical Analysis (3e), the following exercise is given in the chapter about differentiation: Exr.24 (p.118): The process described in part (c) of Exercise 22 can of course also be applied to functions that map $(0, \infty)$ to $(0, \infty)$ . Fix some $\alpha > 1$ , and put $$ f(x) = \frac{1}{2} \left( x + \frac{\alpha}{x} \right), \qquad g(x) = \frac{\alpha+x}{1+x}. $$ Both $f$ and $g$ have $\sqrt{\alpha}$ as their only fixed point in $(0, \infty)$ . Try to explain, on the basis of properties of $f$ and $g$ , why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare $f^\prime$ and $g^\prime$ , draw the zig-zags suggested in Exercise 22.) Do the same when $0 < \alpha < 1$ . The exercise referred to above reads: Exr.22 (p.117): Suppose $f$ is a real function on $(-\infty, \infty)$ . Call $x$ a fixed point of $f$ if $f(x)=x$ . (c) If there is a constant $A < 1$ such that $\left| f^\prime(t) \right| \leq A$ for all real $t$ , prove that a fixed point $x$ of $f$ exists, and that $x = \lim x_n$ , where $x_1$ is an arbitrary real number and $$ x_{n+1} = f \left( x_n \right) $$ for $n = 1, 2, 3, \ldots$ . (d) Show that the process described in (c) can be visualized by the zig-zag path $$ \left( x_1, x_2 \right) \rightarrow \left( x_2, x_2 \right) \rightarrow \left( x_2, x_3 \right) \rightarrow \left( x_3, x_3 \right) \rightarrow \left( x_3, x_4 \right) \rightarrow \cdots.$$ (Credit to @Saaqib Mahmood for having written these exercises before). I know that this question was asked before (see Prob. 24, Chap. 5 in Baby Rudin: For $\alpha>1$, let $f(x) = (x+\alpha/x)/2$, $g(x) = (\alpha+x)/(1+x)$ have $\sqrt{\alpha}$ as their only fixed point ), however it didn't receive a satisfying explanation: I do not understand what is Rudin asking in Exr. 24. I tried to compare the derivatives of the functions, using also the Mean Value Theorem in order to link them to the approximation we make computing $\sqrt{\alpha}$ by means of $x_n$ , however, I only obtain a gross estimate of this error, which is too coarse to actually give any information on the rate of convergence (it is more precise to directly deal with the two functions). I also tried to compare the "" zig-zag "" (as Rudin calls it) of the two functions and I have only noticed that $f$ creates a staircase whereas $g$ creates a spiral, but I can not draw any other conclusion from that. Any help is highly appreciated as always!","In Rudin's Principles of Mathematical Analysis (3e), the following exercise is given in the chapter about differentiation: Exr.24 (p.118): The process described in part (c) of Exercise 22 can of course also be applied to functions that map to . Fix some , and put Both and have as their only fixed point in . Try to explain, on the basis of properties of and , why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare and , draw the zig-zags suggested in Exercise 22.) Do the same when . The exercise referred to above reads: Exr.22 (p.117): Suppose is a real function on . Call a fixed point of if . (c) If there is a constant such that for all real , prove that a fixed point of exists, and that , where is an arbitrary real number and for . (d) Show that the process described in (c) can be visualized by the zig-zag path (Credit to @Saaqib Mahmood for having written these exercises before). I know that this question was asked before (see Prob. 24, Chap. 5 in Baby Rudin: For $\alpha>1$, let $f(x) = (x+\alpha/x)/2$, $g(x) = (\alpha+x)/(1+x)$ have $\sqrt{\alpha}$ as their only fixed point ), however it didn't receive a satisfying explanation: I do not understand what is Rudin asking in Exr. 24. I tried to compare the derivatives of the functions, using also the Mean Value Theorem in order to link them to the approximation we make computing by means of , however, I only obtain a gross estimate of this error, which is too coarse to actually give any information on the rate of convergence (it is more precise to directly deal with the two functions). I also tried to compare the "" zig-zag "" (as Rudin calls it) of the two functions and I have only noticed that creates a staircase whereas creates a spiral, but I can not draw any other conclusion from that. Any help is highly appreciated as always!","(0, \infty) (0, \infty) \alpha > 1  f(x) = \frac{1}{2} \left( x + \frac{\alpha}{x} \right), \qquad g(x) = \frac{\alpha+x}{1+x}.  f g \sqrt{\alpha} (0, \infty) f g f^\prime g^\prime 0 < \alpha < 1 f (-\infty, \infty) x f f(x)=x A < 1 \left| f^\prime(t) \right| \leq A t x f x = \lim x_n x_1  x_{n+1} = f \left( x_n \right)  n = 1, 2, 3, \ldots  \left( x_1, x_2 \right) \rightarrow \left( x_2, x_2 \right) \rightarrow \left( x_2, x_3 \right) \rightarrow \left( x_3, x_3 \right) \rightarrow \left( x_3, x_4 \right) \rightarrow \cdots. \sqrt{\alpha} x_n f g","['real-analysis', 'derivatives', 'algorithms', 'dynamical-systems', 'cobweb-diagram']"
50,Question about a solution of $y'(t) \le Ky(t)$ for $t \ge 0$ implies $y(t) \le y(0)e^{Kt}$,Question about a solution of  for  implies,y'(t) \le Ky(t) t \ge 0 y(t) \le y(0)e^{Kt},"There is a step in the following solution that I am not sure it is correct, but maybe I am missing something. The problem is: Let $K$ be a real constant. Suppose that $y(t)$ is a positive differentiable function satisfying $y'(t) \le Ky(t)$ for $t \ge 0$ . Prove that $y(t) \le e^{Kt}y(0)$ for $t \ge 0$ . And the solution is: From the given inequality we get $0 \ge e^{-Kt}y'(t)-Ke^{-Kt}y(t)=\frac{d}{dt}\left(e^{-Kt}y(t)\right)$ for $t \ge 0$ ; integrating from $0$ to $t$ we find that, for $t \ge 0$ , $e^{-Kt}y(t)-y(0) \le 0$ from which the desired inequality follows. I am not sure about the step where the author integrates from $0$ to $t$ : in the hypotheses we have, we know that $y$ is differentiable and, since $e^{-Kt}$ is indefinitely differentiable, so $y(t)e^{-Kt}$ is at most differentiable in this context. So $\frac{d}{dt}y(t)e^{-Kt}$ is not  continuous, and $\frac{d}{dt}y(t)e^{-Kt}$ is not monotonic as far as we know, hence there is no theorem (as far as I know) that guarantees the integrability of $\frac{d}{dt}y(t)e^{-Kt}$ in the interval $[0,t]$ . Moreover, where the hypothesis that $y$ is positive is used? My solution is similar, but after getting to $\frac{d}{dt}\left(e^{-Kt}y(t)\right) \le 0$ I proceed by saying that so $y(t)e^{-Kt}$ is decreasing for $t \ge 0$ and so $y(t)e^{-Kt} \le y(0)e^{-K \cdot 0}=y(0)$ , so $y(t) \le e^{Kt}y(0)$ . Is my solution correct? I am not using that $y$ is positive as well.","There is a step in the following solution that I am not sure it is correct, but maybe I am missing something. The problem is: Let be a real constant. Suppose that is a positive differentiable function satisfying for . Prove that for . And the solution is: From the given inequality we get for ; integrating from to we find that, for , from which the desired inequality follows. I am not sure about the step where the author integrates from to : in the hypotheses we have, we know that is differentiable and, since is indefinitely differentiable, so is at most differentiable in this context. So is not  continuous, and is not monotonic as far as we know, hence there is no theorem (as far as I know) that guarantees the integrability of in the interval . Moreover, where the hypothesis that is positive is used? My solution is similar, but after getting to I proceed by saying that so is decreasing for and so , so . Is my solution correct? I am not using that is positive as well.","K y(t) y'(t) \le Ky(t) t \ge 0 y(t) \le e^{Kt}y(0) t \ge 0 0 \ge e^{-Kt}y'(t)-Ke^{-Kt}y(t)=\frac{d}{dt}\left(e^{-Kt}y(t)\right) t \ge 0 0 t t \ge 0 e^{-Kt}y(t)-y(0) \le 0 0 t y e^{-Kt} y(t)e^{-Kt} \frac{d}{dt}y(t)e^{-Kt} \frac{d}{dt}y(t)e^{-Kt} \frac{d}{dt}y(t)e^{-Kt} [0,t] y \frac{d}{dt}\left(e^{-Kt}y(t)\right) \le 0 y(t)e^{-Kt} t \ge 0 y(t)e^{-Kt} \le y(0)e^{-K \cdot 0}=y(0) y(t) \le e^{Kt}y(0) y","['real-analysis', 'derivatives', 'inequality']"
51,Second derivative of convex decreasing functions larger than square of first derivative?,Second derivative of convex decreasing functions larger than square of first derivative?,,"Given a twice differentiable, convex, decreasing function $f\in C^2$ with $\lim_{x\to\infty} f(x)=k$ , I am trying to identify some sufficient conditions for the second derivative to be larger than the square of the first derivative above some level $\bar x$ , or $$ f''(x) > f'(x)^2 \qquad x>\bar x.$$ I see that this is true of the negative exponential for $x>0$ and of the negative power function $1/x^n$ for $x^n>n/(n+1)$ .","Given a twice differentiable, convex, decreasing function with , I am trying to identify some sufficient conditions for the second derivative to be larger than the square of the first derivative above some level , or I see that this is true of the negative exponential for and of the negative power function for .",f\in C^2 \lim_{x\to\infty} f(x)=k \bar x  f''(x) > f'(x)^2 \qquad x>\bar x. x>0 1/x^n x^n>n/(n+1),"['derivatives', 'convex-analysis']"
52,Why is Leibniz Notation written this way for the second derivative? [duplicate],Why is Leibniz Notation written this way for the second derivative? [duplicate],,"This question already has answers here : Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$? (6 answers) Explaining intuitively the notation for derivatives [duplicate] (2 answers) Closed 2 years ago . Let $y = f(x)$ . $$f'' = \frac{d^2y}{dx^2}$$ The explanation for this being that $$ \Bigl(\dfrac{d}{dx}\Bigr)^2 y = \dfrac{d^2}{dx^2}\,y = \dfrac{d^2 y}{dx^2};$$ Since there are two $d$ 's in the bottom of the fraction, why is it not written $$\frac{d^2y}{d^2x^2}$$ Maybe it's because $dx$ needs to be thought of as a single thing.  But notice that $d$ is used by itself and squared in the numerator.. Does my point here make sense, is it just a convenience to avoid the extra exponent, or is there a logical reason it's written in the form it is?","This question already has answers here : Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$? (6 answers) Explaining intuitively the notation for derivatives [duplicate] (2 answers) Closed 2 years ago . Let . The explanation for this being that Since there are two 's in the bottom of the fraction, why is it not written Maybe it's because needs to be thought of as a single thing.  But notice that is used by itself and squared in the numerator.. Does my point here make sense, is it just a convenience to avoid the extra exponent, or is there a logical reason it's written in the form it is?","y = f(x) f'' = \frac{d^2y}{dx^2}  \Bigl(\dfrac{d}{dx}\Bigr)^2 y = \dfrac{d^2}{dx^2}\,y = \dfrac{d^2 y}{dx^2}; d \frac{d^2y}{d^2x^2} dx d","['calculus', 'derivatives', 'notation', 'differential']"
53,Deriving the tangential directional derivative of a scalar function on a parametric surface,Deriving the tangential directional derivative of a scalar function on a parametric surface,,"Given are a parametric surface $S(u,v) = \big( x(u,v), y(u,v), z(u,v) \big)$ and a scalar function $f(u,v)$ , both defined on the same domain $\Omega \subset \mathbb{R}^2$ . As such, we can associate every point on the surface $S(u,v)$ with a scalar value $f(u,v)$ . Throughout, it is assumed that the surface is regular and the function $f$ differentiable. ​ Out of curiosity, I'm interested in deriving an expression for the tangential directional derivative of the function $f$ as seen from the surface $S(u,v)$ , which I'll denote $\tilde{f}$ . That is, considering a point $p = S(u,v)$ and a nearby point $q = S(u+ha, v+hb)$ with $h$ variable and for some (domain) direction $w = \begin{pmatrix}a\\b\end{pmatrix}$ , what is the result of $$\lim_{h \to 0} \frac{\tilde{f}(q) - \tilde{f}(p)}{\|q-p\|}.$$ Below I've written down my approach — the question is whether the result is correct (I do have some doubts about it, as explained below). As the numerator is the difference of two function values, it is equivalent to $f(u+ha, v+hb) - f(u,v)$ . Substituting the expressions for $p$ and $q$ , the denominator is $\| S(u+ha, v+hb) - S(u,v) \|$ . From the Taylor expansion , we know that $S(u+ha, v+hb) \approx S(u,v) + ha \frac{\partial S}{\partial u}(u,v) + hb \frac{\partial S}{\partial v}(u,v)$ for small $h$ , which in the limit becomes an equivalence. As such, in the limit the denominator represents the length of a tangent vector I'll denote $h \tilde{w}$ . Introducing the $3 \times 2$ Jacobian matrix $J = \big(\frac{\partial S}{\partial u}, \frac{\partial S}{\partial v}\big)$ , we can express this tangent vector as $h \tilde{w} = h J w$ . We can now re-write the above as $$\lim_{h \to 0} \frac{\tilde{f}(q) - \tilde{f}(p)}{\| q - p \|} = \lim_{h \to 0} \frac{f(u+ha, v+hb) - f(u,v)}{h} \frac{h}{\| q - p \|} = D_w f(u,v) \frac{1}{\| \tilde{w} \|} = \frac{\nabla f \cdot w}{\| \tilde{w} \|},$$ where $D_w f(u,v)$ is the directional derivative of $f(u,v)$ in the direction of $w$ . Next, with $\tilde {w} = J w$ , we have $w = J^+ \tilde{w}$ , with $J^+$ the (left) Moore-Penrose inverse . That is, $J^+ = (J^TJ)^{-1} J^T$ . Introducing the first fundamental form (i.e. the metric tensor ) $g = J^TJ$ , we can write $J^+ = g^{-1} J^T$ . Therefore, we have $$\frac{\nabla f \cdot w}{\| \tilde{w} \|} = \frac{ \left(\nabla f\right)^T g^{-1} J^T \tilde{w}}{\| \tilde{w} \|} = \left( J g^{-1} \nabla f \right) \cdot \frac{\tilde{w}}{\| \tilde{w} \|},$$ which follows from transposing (recall that $g$ is symmetric and therefore $g^{-1}$ is as well). On the one hand, this looks promising, as $J g^{-1} \nabla f$ matches what I've seen described as the tangential gradient (also referred to as the surface gradient ) in the literature. Therefore, like the ordinary directional derivative, the tangential directional derivative appears to be the dot product of a gradient and a direction vector. On the other hand, I'm a bit sceptical about this vector appearing normalised . Is this a result of using this specific approach, or did I make a mistake/wrong assumption somewhere along the way?","Given are a parametric surface and a scalar function , both defined on the same domain . As such, we can associate every point on the surface with a scalar value . Throughout, it is assumed that the surface is regular and the function differentiable. ​ Out of curiosity, I'm interested in deriving an expression for the tangential directional derivative of the function as seen from the surface , which I'll denote . That is, considering a point and a nearby point with variable and for some (domain) direction , what is the result of Below I've written down my approach — the question is whether the result is correct (I do have some doubts about it, as explained below). As the numerator is the difference of two function values, it is equivalent to . Substituting the expressions for and , the denominator is . From the Taylor expansion , we know that for small , which in the limit becomes an equivalence. As such, in the limit the denominator represents the length of a tangent vector I'll denote . Introducing the Jacobian matrix , we can express this tangent vector as . We can now re-write the above as where is the directional derivative of in the direction of . Next, with , we have , with the (left) Moore-Penrose inverse . That is, . Introducing the first fundamental form (i.e. the metric tensor ) , we can write . Therefore, we have which follows from transposing (recall that is symmetric and therefore is as well). On the one hand, this looks promising, as matches what I've seen described as the tangential gradient (also referred to as the surface gradient ) in the literature. Therefore, like the ordinary directional derivative, the tangential directional derivative appears to be the dot product of a gradient and a direction vector. On the other hand, I'm a bit sceptical about this vector appearing normalised . Is this a result of using this specific approach, or did I make a mistake/wrong assumption somewhere along the way?","S(u,v) = \big( x(u,v), y(u,v), z(u,v) \big) f(u,v) \Omega \subset \mathbb{R}^2 S(u,v) f(u,v) f f S(u,v) \tilde{f} p = S(u,v) q = S(u+ha, v+hb) h w = \begin{pmatrix}a\\b\end{pmatrix} \lim_{h \to 0} \frac{\tilde{f}(q) - \tilde{f}(p)}{\|q-p\|}. f(u+ha, v+hb) - f(u,v) p q \| S(u+ha, v+hb) - S(u,v) \| S(u+ha, v+hb) \approx S(u,v) + ha \frac{\partial S}{\partial u}(u,v) + hb \frac{\partial S}{\partial v}(u,v) h h \tilde{w} 3 \times 2 J = \big(\frac{\partial S}{\partial u}, \frac{\partial S}{\partial v}\big) h \tilde{w} = h J w \lim_{h \to 0} \frac{\tilde{f}(q) - \tilde{f}(p)}{\| q - p \|} = \lim_{h \to 0} \frac{f(u+ha, v+hb) - f(u,v)}{h} \frac{h}{\| q - p \|} = D_w f(u,v) \frac{1}{\| \tilde{w} \|} = \frac{\nabla f \cdot w}{\| \tilde{w} \|}, D_w f(u,v) f(u,v) w \tilde {w} = J w w = J^+ \tilde{w} J^+ J^+ = (J^TJ)^{-1} J^T g = J^TJ J^+ = g^{-1} J^T \frac{\nabla f \cdot w}{\| \tilde{w} \|} = \frac{ \left(\nabla f\right)^T g^{-1} J^T \tilde{w}}{\| \tilde{w} \|} = \left( J g^{-1} \nabla f \right) \cdot \frac{\tilde{w}}{\| \tilde{w} \|}, g g^{-1} J g^{-1} \nabla f","['derivatives', 'differential-geometry']"
54,Differential formula for polynomial,Differential formula for polynomial,,Let $g(x)$ be a polynomial whose degree  is $m$ . I want to show that $$\lim_{h\rightarrow 0}\frac{\sum_{j=0}^m\binom{m}{j}(-1)^{m-j}g(x+jh)}{h^m}=\frac{d^m}{dx}g(x).$$ My idea is to proceed by induction on $m$ . The formula holds trivially for $m=1$ . Assume we have the equation for $m$ . Let $g(x)$ be a polynomial with degree $m+1$ . We have to show $$\lim_{h\rightarrow 0}\frac{\sum_{j=0}^{m+1}\binom{m+1}{j}(-1)^{m+1-j}g(x+jh)}{h^{m+1}}=\frac{d^{m+1}}{dx}g(x).$$ I tried to expand the numerator using the identity $\binom{m+1}{j}=\binom{m}{j}+\binom{m}{j-1}$ but that got me nowhere. It's not obvious to me how I can utilize the inductive hypothesis. Maybe a proof by induction is not the right approach? Any hints/suggestions?,Let be a polynomial whose degree  is . I want to show that My idea is to proceed by induction on . The formula holds trivially for . Assume we have the equation for . Let be a polynomial with degree . We have to show I tried to expand the numerator using the identity but that got me nowhere. It's not obvious to me how I can utilize the inductive hypothesis. Maybe a proof by induction is not the right approach? Any hints/suggestions?,g(x) m \lim_{h\rightarrow 0}\frac{\sum_{j=0}^m\binom{m}{j}(-1)^{m-j}g(x+jh)}{h^m}=\frac{d^m}{dx}g(x). m m=1 m g(x) m+1 \lim_{h\rightarrow 0}\frac{\sum_{j=0}^{m+1}\binom{m+1}{j}(-1)^{m+1-j}g(x+jh)}{h^{m+1}}=\frac{d^{m+1}}{dx}g(x). \binom{m+1}{j}=\binom{m}{j}+\binom{m}{j-1},"['real-analysis', 'derivatives', 'polynomials']"
55,Finding the $n$th derivative of $f(x) = e^{\sin(x)}$,Finding the th derivative of,n f(x) = e^{\sin(x)},"I have a function $$f(x) = e^{\sin(x)}$$ I want to expand it in a infinte series using Maclaurin's theorem and for that I need to know if the remainder term $$R_n = \frac{x^n}{n!}f^{(n)}(\theta x),\quad (0<\theta<1)\quad[\text{Lagrange's from}] $$ converges as $n \to \infty$ but for that I need to know the $f^{(n)}(x) = e^{\sin(x)}$ the $n$ th derivative of exponential trigonometric function. I know how to find $n$ -th derivative of functions like $\sin(x)$ or $ e^{ax}$ where you can  see the pattern but I cant see anything in this function. $$\begin{align} f'(x) &= \phantom{-}e^{\sin(x)}\cos(x) \\ f''(x) &= -e^{\sin(x)}(\sin(x)-\cos^2(x)) \\ f'''(x) &= -e^{\sin(x)}\cos(x)(3\sin(x)-\cos(2x)+1) \\ f^{(4)}(x) &= \phantom{-}e^{\sin(x)}(3\sin^2(x)+(1-6\cos^2(x))\sin(x)+\cos^4(x)+4\cos^2(x)) \end{align}$$ How am I supposed to find the $n$ th derivative of function like these?",I have a function I want to expand it in a infinte series using Maclaurin's theorem and for that I need to know if the remainder term converges as but for that I need to know the the th derivative of exponential trigonometric function. I know how to find -th derivative of functions like or where you can  see the pattern but I cant see anything in this function. How am I supposed to find the th derivative of function like these?,"f(x) = e^{\sin(x)} R_n = \frac{x^n}{n!}f^{(n)}(\theta x),\quad (0<\theta<1)\quad[\text{Lagrange's from}]  n \to \infty f^{(n)}(x) = e^{\sin(x)} n n \sin(x)  e^{ax} \begin{align}
f'(x) &= \phantom{-}e^{\sin(x)}\cos(x) \\
f''(x) &= -e^{\sin(x)}(\sin(x)-\cos^2(x)) \\
f'''(x) &= -e^{\sin(x)}\cos(x)(3\sin(x)-\cos(2x)+1) \\
f^{(4)}(x) &= \phantom{-}e^{\sin(x)}(3\sin^2(x)+(1-6\cos^2(x))\sin(x)+\cos^4(x)+4\cos^2(x))
\end{align} n","['calculus', 'derivatives', 'trigonometry', 'exponential-function']"
56,maximum of $\frac{1}{(1+(x- \frac{1}{2})^2 )^{\frac{3}{2}}}+\frac{1}{(1+(x+ \frac{1}{2})^2 )^{\frac{3}{2}}}$,maximum of,\frac{1}{(1+(x- \frac{1}{2})^2 )^{\frac{3}{2}}}+\frac{1}{(1+(x+ \frac{1}{2})^2 )^{\frac{3}{2}}},"How to find this function's maximum? $$\frac{1}{(1+(x- \frac{1}{2})^2 )^{\frac{3}{2}}}+\frac{1}{(1+(x+ \frac{1}{2})^2 )^{\frac{3}{2}}}$$ I think it has a maximum value at x=0. A search on Wolfram Alpha revealed that this is correct. However, the result in wolframalpha was to solve the 12th-order equation by differentiating and then performing general differentiation, I wonder if there is an easier way.","How to find this function's maximum? I think it has a maximum value at x=0. A search on Wolfram Alpha revealed that this is correct. However, the result in wolframalpha was to solve the 12th-order equation by differentiating and then performing general differentiation, I wonder if there is an easier way.",\frac{1}{(1+(x- \frac{1}{2})^2 )^{\frac{3}{2}}}+\frac{1}{(1+(x+ \frac{1}{2})^2 )^{\frac{3}{2}}},"['real-analysis', 'derivatives', 'maxima-minima']"
57,Is this method of finding the derivative of $x^x$ acceptable in math?,Is this method of finding the derivative of  acceptable in math?,x^x,"I wanted to find the derivative of the $x^x$ , and I came up with the following, and I want to know if it's acceptable to do so: Let's put $x^x = y$ $x^x = y $ $\ln(x^x) = \ln(y)$ $x\ln(x) = \ln(y)$ $d[x\ln(x)]/dx = [\ln(y)']/dx$ $\ln(x) + 1 = y'/y$ $y(\ln(x) + 1) = y'$ $x^x(\ln(x) + 1) = y'$ When I graph it, it comes as the correct derivative. Is it acceptable or not to do this?","I wanted to find the derivative of the , and I came up with the following, and I want to know if it's acceptable to do so: Let's put When I graph it, it comes as the correct derivative. Is it acceptable or not to do this?",x^x x^x = y x^x = y  \ln(x^x) = \ln(y) x\ln(x) = \ln(y) d[x\ln(x)]/dx = [\ln(y)']/dx \ln(x) + 1 = y'/y y(\ln(x) + 1) = y' x^x(\ln(x) + 1) = y',['derivatives']
58,Is logarithmic differentiation method missing the cases that $f(x)=0$?,Is logarithmic differentiation method missing the cases that ?,f(x)=0,"I am learning logarithmic differentiation. It goes like this: First we define a function $$L_0(x)=\log|x|=\int_1^{|x|}\frac{1}{t}dt$$ After studying the positive and negative ranges, we know $$L_0'(x)=\frac{1}{x}$$ for all real $x\ne 0$ . Apply the above to a function $f(x)$ , we have $$g'(x)=(L_0(f(x)))'=L_0'(f(x))f'(x)=\frac{f'(x)}{f(x)}$$ So $f'(x)=g'(x)f(x)$ . However we made an assumption the moment we introduced $L_0(f(x))$ : $f(x)\ne 0$ . Is the logarithmic derivative method missing the cases when $f(x)=0$ ? Or to put it in another way, why do we trust the result that it will work for roots of $f(x)$ ?","I am learning logarithmic differentiation. It goes like this: First we define a function After studying the positive and negative ranges, we know for all real . Apply the above to a function , we have So . However we made an assumption the moment we introduced : . Is the logarithmic derivative method missing the cases when ? Or to put it in another way, why do we trust the result that it will work for roots of ?",L_0(x)=\log|x|=\int_1^{|x|}\frac{1}{t}dt L_0'(x)=\frac{1}{x} x\ne 0 f(x) g'(x)=(L_0(f(x)))'=L_0'(f(x))f'(x)=\frac{f'(x)}{f(x)} f'(x)=g'(x)f(x) L_0(f(x)) f(x)\ne 0 f(x)=0 f(x),"['derivatives', 'logarithms']"
59,On a sequence of continuous functions,On a sequence of continuous functions,,"I am working on the problem: Let $\{ f_n \}$ be a sequence of real-valued functions on $[0,1]$ defined by $f_0 = f \in C[0,1]$ and $f_n$ is an anti-derivative of $f_{n-1}.$ Suppose that for each $x \in [0, 1]$ there is $n \in \mathbb{N}$ for which $f_n(x) = 0.$ Prove that $f = 0.$ Here is my attempt: We first note that if $f_n$ is an anti-derivative of $f_{n-1}$ , this implies that $f_{n-1}$ is a derivative   of $f_n$ . i.e. \begin{equation}     f_{n-1} = \frac{d}{dx}(f_n). \end{equation} Case 1: If $n = 1$ such that $f_n = 0$ for all $x \in [0,1]$ . Here if $n = 1$ then $f_1(x) = 0$ for all $x \in [0,1]$ . Now, put $n = 1$ in equation (1). We get that $$f_{1-1} = \frac{d}{dx}(f_1(x)) \implies f_0 = 0 = f   \forall x \in [0,1].$$ Case 2: If $n>1$ such that $f_n(x) = 0$ for all $x \in [0,1]$ . Here if $n > 1$ , from equation (1) we   get that $$f_{n-1} = \frac{d}{dx}(f_n) \implies f_{n-1} = 0.$$ Now take the derivative of $f_{n-1}$ . $$\frac{d}{dx}(f_{n-1}) = f_{n-2} \implies \frac{d^2}{dx^2}(f_n) = \frac{d}{dx}(f_{n-1}) = f_{n-2}.$$ This then implies $$\frac{d^n}{dx^n}(f_n) = f_{n-n}   \forall x \in [0,1].$$ Hence $\frac{d^n}{dx^n}(0) = f_0 = f$ for all $x \in [0,1].$ Thus $f = 0$ for all $x \in [0,1].$ $\square$ Please let me know if I am on the right track or assist me in proving this correctly","I am working on the problem: Let be a sequence of real-valued functions on defined by and is an anti-derivative of Suppose that for each there is for which Prove that Here is my attempt: We first note that if is an anti-derivative of , this implies that is a derivative   of . i.e. Case 1: If such that for all . Here if then for all . Now, put in equation (1). We get that Case 2: If such that for all . Here if , from equation (1) we   get that Now take the derivative of . This then implies Hence for all Thus for all Please let me know if I am on the right track or assist me in proving this correctly","\{ f_n \} [0,1] f_0 = f \in C[0,1] f_n f_{n-1}. x \in [0, 1] n \in \mathbb{N} f_n(x) = 0. f = 0. f_n f_{n-1} f_{n-1} f_n \begin{equation}
    f_{n-1} = \frac{d}{dx}(f_n).
\end{equation} n = 1 f_n = 0 x \in [0,1] n = 1 f_1(x) = 0 x \in [0,1] n = 1 f_{1-1} = \frac{d}{dx}(f_1(x)) \implies f_0 = 0 = f   \forall x \in [0,1]. n>1 f_n(x) = 0 x \in [0,1] n > 1 f_{n-1} = \frac{d}{dx}(f_n) \implies f_{n-1} = 0. f_{n-1} \frac{d}{dx}(f_{n-1}) = f_{n-2} \implies \frac{d^2}{dx^2}(f_n) = \frac{d}{dx}(f_{n-1}) = f_{n-2}. \frac{d^n}{dx^n}(f_n) = f_{n-n}   \forall x \in [0,1]. \frac{d^n}{dx^n}(0) = f_0 = f x \in [0,1]. f = 0 x \in [0,1]. \square","['real-analysis', 'derivatives']"
60,Prove that $e^{2x-\frac{x^2}{2}}+e^{x-\frac{x^2}{2}}-2e^x-x \ge0$，where $x\leq0$,Prove that ，where,e^{2x-\frac{x^2}{2}}+e^{x-\frac{x^2}{2}}-2e^x-x \ge0 x\leq0,"Prove that $$e^{2x-\frac{x^2}{2}}+e^{x-\frac{x^2}{2}}-2e^x-x \ge0$$ where $x\leq0$ . First I tried to find the derivative , but it is hard to find the sign of it. I also found that when $x$ is roughly smaller than $-2$ , $-2e^x-x \ge 0$ is easy to prove. So the question is how to prove it when $x$ is near 0, I tried $e^x \ge x+1$ , failed totally. Wish someone could help and thanks in advance.","Prove that where . First I tried to find the derivative , but it is hard to find the sign of it. I also found that when is roughly smaller than , is easy to prove. So the question is how to prove it when is near 0, I tried , failed totally. Wish someone could help and thanks in advance.",e^{2x-\frac{x^2}{2}}+e^{x-\frac{x^2}{2}}-2e^x-x \ge0 x\leq0 x -2 -2e^x-x \ge 0 x e^x \ge x+1,"['calculus', 'derivatives', 'inequality']"
61,Theorems of the type $D\lim_n f_n = \lim_n Df_n$ for non-Archimedean fields,Theorems of the type  for non-Archimedean fields,D\lim_n f_n = \lim_n Df_n,"I'm kind of interested in seeing if there is a base-field agnostic setting for calculus, but I don't have much experience with the non-Archimedian case. Let $X,Y$ be Banach spaces over some non-discrete locally compact field $k$ , $U\subseteq X$ open. Consider a statement of the form: Theorem (template) Let $f_n: U\to Y$ be a sequence of differentiable functions that satisfies some property $A$ , then there is a (at minimum pointwise) limit $f:U\to Y$ of $f_n$ with $f$ differentiable and $Df=\lim_n Df_n$ . If $k$ is the real or complex numbers an example of such a condition $A$ is: Example (property $A$ ) $Df_n$ converges locally uniformly to some function $F: U\to L(X,Y)$ and for every connected component $U_i$ of $U$ there is some $x_i\in U_i$ with $f_n(x_i)$ convergent. The only proof I know uses the real line-integral, which is not available in the non-Archimedean case. I don't know if the statement itself remains true for other fields and haven't been able to find any similar kind of statement for this case. Question Is there a useful property for which the theorem is true for any locally compact field?","I'm kind of interested in seeing if there is a base-field agnostic setting for calculus, but I don't have much experience with the non-Archimedian case. Let be Banach spaces over some non-discrete locally compact field , open. Consider a statement of the form: Theorem (template) Let be a sequence of differentiable functions that satisfies some property , then there is a (at minimum pointwise) limit of with differentiable and . If is the real or complex numbers an example of such a condition is: Example (property ) converges locally uniformly to some function and for every connected component of there is some with convergent. The only proof I know uses the real line-integral, which is not available in the non-Archimedean case. I don't know if the statement itself remains true for other fields and haven't been able to find any similar kind of statement for this case. Question Is there a useful property for which the theorem is true for any locally compact field?","X,Y k U\subseteq X f_n: U\to Y A f:U\to Y f_n f Df=\lim_n Df_n k A A Df_n F: U\to L(X,Y) U_i U x_i\in U_i f_n(x_i)","['real-analysis', 'functional-analysis', 'derivatives', 'p-adic-number-theory', 'nonarchimedian-analysis']"
62,"Use continuous limit theorem to prove if $f$ is uniformly differentiable, then $f'$ is uniformly continuous","Use continuous limit theorem to prove if  is uniformly differentiable, then  is uniformly continuous",f f',"This is my first approaching to answer my own question in math stack exchange. Before that, I would define several terms according to Steven Abbott's Understand Analysis, which, by the way, the origin of this question. Derivative Let $g:\mathbb{A}\rightarrow\mathbb{R}$ be a function defined on an interval $\mathbb{A}$ . Given $c\in\mathbb{A}$ , the derivative of $g$ at $c$ is defined by $g'(c)=\lim_{x\to c}\frac{g(x)-g(c)}{x-c}$ . Continuous limit theorem Let $(f_n)$ be a sequence of functions defined on $\mathbb{A}\subseteq\mathbb{R}$ that converges uniformly on $\mathbb{A}$ to a function $f$ . If each $f_N$ is continuous at $c\in\mathbb{A}$ , then $f$ , is continuous at $c$ . Uniformly differentiable Given a differentiable function $f:\mathbb{A}\rightarrow\mathbb{R}$ , let's say that $f$ is uniformly differentiable on $\mathbb{A}$ if, given $\epsilon>0\exists\delta>0$ such that if $0<|x-y|<\delta$ , then $|\frac{f(x)-f(y)}{x-y}-f'(y)|<\epsilon$ .","This is my first approaching to answer my own question in math stack exchange. Before that, I would define several terms according to Steven Abbott's Understand Analysis, which, by the way, the origin of this question. Derivative Let be a function defined on an interval . Given , the derivative of at is defined by . Continuous limit theorem Let be a sequence of functions defined on that converges uniformly on to a function . If each is continuous at , then , is continuous at . Uniformly differentiable Given a differentiable function , let's say that is uniformly differentiable on if, given such that if , then .",g:\mathbb{A}\rightarrow\mathbb{R} \mathbb{A} c\in\mathbb{A} g c g'(c)=\lim_{x\to c}\frac{g(x)-g(c)}{x-c} (f_n) \mathbb{A}\subseteq\mathbb{R} \mathbb{A} f f_N c\in\mathbb{A} f c f:\mathbb{A}\rightarrow\mathbb{R} f \mathbb{A} \epsilon>0\exists\delta>0 0<|x-y|<\delta |\frac{f(x)-f(y)}{x-y}-f'(y)|<\epsilon,"['real-analysis', 'derivatives', 'continuity', 'uniform-convergence']"
63,"Don't need help solving this problem, just need some info. A push in the right direction would be nice.","Don't need help solving this problem, just need some info. A push in the right direction would be nice.",,Find $\displaystyle\frac{dy}{dx}$ given that $$\sqrt{3x^7+y^2}=\sin^2y+100xy.$$ Should I start off by squaring both sides to get rid of the radical on the left? And then start the derivative process? Thank you. This is what I have so far: Differentiating both sides with respect to $x$ : $$\begin{align}\frac{21x^6+2y\displaystyle\frac{dy}{dx}}{2(3x^7+y^2)^{1/2}} &= 2\sin(y) \cos(y) \cdot \displaystyle\frac{dy}{dx} +100\left(y+x\displaystyle\frac{dy}{dx}\right)\end{align}$$ I think I had made the left side $1/2$ to get rid of the root and forgot to apply it to the right side of the equation. – I'm at $$3x^7+y^2=(\sin^2y+100xy)^2$$ then I think I would start from left to right until they are all in their derivative form?,Find given that Should I start off by squaring both sides to get rid of the radical on the left? And then start the derivative process? Thank you. This is what I have so far: Differentiating both sides with respect to : I think I had made the left side to get rid of the root and forgot to apply it to the right side of the equation. – I'm at then I think I would start from left to right until they are all in their derivative form?,\displaystyle\frac{dy}{dx} \sqrt{3x^7+y^2}=\sin^2y+100xy. x \begin{align}\frac{21x^6+2y\displaystyle\frac{dy}{dx}}{2(3x^7+y^2)^{1/2}} &= 2\sin(y) \cos(y) \cdot \displaystyle\frac{dy}{dx} +100\left(y+x\displaystyle\frac{dy}{dx}\right)\end{align} 1/2 3x^7+y^2=(\sin^2y+100xy)^2,"['calculus', 'derivatives', 'implicit-differentiation']"
64,Very Basic Ito's Formula Problem,Very Basic Ito's Formula Problem,,"Let $(X_t)_{t≥0}$ be an Ito process of the form $dX_t = µ(t)dt + σ(t)dW_t$ for some $µ ∈ \mathbb{L}^1(0, T)$ and $σ ∈ \mathbb{L}^2(0, T)$ . I have been asked to apply Ito’s formula to $Y_t = g(t, X_t)$ for $g(t, x) = e^x + t \,sin(x)$ to write $Y_t$ as an Ito process. So far, I have calculated the partial derivatives: $$g_t = sin(x)$$ $$g_x = e^x + t\,cos(x)$$ $$g_{xx}=e^x - t\, sin(x)$$ And I have plugged these into Ito's formula to give: $$dY_t = sin(X_t)\, dt\, + \, (e^{X_t} + t \, cos(X_t)) \, dX_t \, + \, \frac{1}{2}\sigma^2(e^{X_t} \, - \, t\, sin(X_t))\, dt$$ However I am told this is incorrect, and it also needs further steps. Can anyone help guide me in the right direction and inform me where I have went wrong? EDIT: I am told that the final solution is as follows: $$dg(t, X_t) = (sin(X_t) + (e^{X_t} + t\,cos(X_t))\mu(t) + \frac{1}{2}(e^{X_t} - t\,sin(X_t)\sigma^2(t)))\, dt \, + (e^{X_t} + t\,cos(X_t))\sigma(t)\,dW_t$$","Let be an Ito process of the form for some and . I have been asked to apply Ito’s formula to for to write as an Ito process. So far, I have calculated the partial derivatives: And I have plugged these into Ito's formula to give: However I am told this is incorrect, and it also needs further steps. Can anyone help guide me in the right direction and inform me where I have went wrong? EDIT: I am told that the final solution is as follows:","(X_t)_{t≥0} dX_t = µ(t)dt + σ(t)dW_t µ ∈ \mathbb{L}^1(0, T) σ ∈ \mathbb{L}^2(0, T) Y_t = g(t, X_t) g(t, x) = e^x + t \,sin(x) Y_t g_t = sin(x) g_x = e^x + t\,cos(x) g_{xx}=e^x - t\, sin(x) dY_t = sin(X_t)\, dt\, + \, (e^{X_t} + t \, cos(X_t)) \, dX_t \, + \, \frac{1}{2}\sigma^2(e^{X_t} \, - \, t\, sin(X_t))\, dt dg(t, X_t) = (sin(X_t) + (e^{X_t} + t\,cos(X_t))\mu(t) + \frac{1}{2}(e^{X_t} - t\,sin(X_t)\sigma^2(t)))\, dt \, + (e^{X_t} + t\,cos(X_t))\sigma(t)\,dW_t","['calculus', 'derivatives', 'stochastic-processes', 'brownian-motion']"
65,"Is there some ""types"" of ""discontinuous derivative""? [duplicate]","Is there some ""types"" of ""discontinuous derivative""? [duplicate]",,"This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 3 years ago . I'm just started learning standard Calculus class in the university (It treats James Stewart's ""Essential Calculus: Early transcendentals"" for 1 semester, just letting you know) and I got some questions about the relationship between 'differentiability of a function' (i.e.the existence of $f'(a)$ regarding $x = a$ ) and 'continuity of a derivative of function'. I made a derivative of a function on my own, which is $$f'(x)=\begin{cases}-x + 2 & x<0, \\ 1 & x=0, \\ -x & x>0. \end{cases}$$ I said the integral constant $C = 0$ arbitrarily for convenience. Because $f'$ is defined at every $R$ , then $f$ is continuous at every $R$ by theorem (don't remember the number of it...) By integrating $f'$ at every interval, then we get $$f(x)=\begin{cases} -x^2/2 + 2x & x<0, \\ 0 & x=0, \\ -x^2/2 & x>0. \end{cases}$$ But by using the definition of $f'(0)$ and given $f(x)$ , we have to say that $f'(0)$ doesn't exist! So this was a contradiction. I asked about this in my country's internet community, but the answers were the following. ""If $f'(a)$ exists, and $f'$ is not continuous at a, then there are 2 types. Both left and right limit of $f'$ at $x = a$ exists but $f'$ is not continuous at $a$ Either left or right limit of $f'$ at $x = a$ doesn't exist, so $f'$ is not continuous at $a$ . $f'$ is only available at type 2. I just accepted with no excuse, but after few days, I just wanted to know some prove about this. I read about this but as I said before, I just started Calculus, so it was too hard for me to understand some comments at there. I am not American or British, so I'm not very good at English and sorry about that. Question is that could somebody prove that ""if $f'(a)$ exists and $f'$ is discontinuous at $x = a$ , then left-hand or right-hand limit of $f'$ at $x = a$ doesn't exist."" Thx for reading this awful long writing.","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 3 years ago . I'm just started learning standard Calculus class in the university (It treats James Stewart's ""Essential Calculus: Early transcendentals"" for 1 semester, just letting you know) and I got some questions about the relationship between 'differentiability of a function' (i.e.the existence of regarding ) and 'continuity of a derivative of function'. I made a derivative of a function on my own, which is I said the integral constant arbitrarily for convenience. Because is defined at every , then is continuous at every by theorem (don't remember the number of it...) By integrating at every interval, then we get But by using the definition of and given , we have to say that doesn't exist! So this was a contradiction. I asked about this in my country's internet community, but the answers were the following. ""If exists, and is not continuous at a, then there are 2 types. Both left and right limit of at exists but is not continuous at Either left or right limit of at doesn't exist, so is not continuous at . is only available at type 2. I just accepted with no excuse, but after few days, I just wanted to know some prove about this. I read about this but as I said before, I just started Calculus, so it was too hard for me to understand some comments at there. I am not American or British, so I'm not very good at English and sorry about that. Question is that could somebody prove that ""if exists and is discontinuous at , then left-hand or right-hand limit of at doesn't exist."" Thx for reading this awful long writing.","f'(a) x = a f'(x)=\begin{cases}-x + 2 & x<0, \\ 1 & x=0, \\ -x & x>0. \end{cases} C = 0 f' R f R f' f(x)=\begin{cases} -x^2/2 + 2x & x<0, \\ 0 & x=0, \\ -x^2/2 & x>0. \end{cases} f'(0) f(x) f'(0) f'(a) f' f' x = a f' a f' x = a f' a f' f'(a) f' x = a f' x = a","['calculus', 'derivatives', 'continuity']"
66,if $f$ is in $C^{2}$ then why does $\int_{0}^{1}\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}du$ necessarily exist,if  is in  then why does  necessarily exist,f C^{2} \int_{0}^{1}\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}du,"if $f$ is in $C^{2}$ then why does $\int_{0}^{1}\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}du$ necessarily exist? Clearly $u\to\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}$ converges to $f^{\prime\prime}(x)$ as $u \to 0$ , how does this help me prove the above? Attempt: Let $\delta, \; \varepsilon > 0$ , then it follows that $\int_{\delta}^{1}\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}du$ exists, as we can bound the integrand by $4\frac{\lvert f\rvert}{\delta ^{2}}$ Now consider that for all $u$ smaller than some $\delta > 0$ we have $\lvert f^{\prime\prime}(x)-\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}\rvert \leq \varepsilon$ , then we obtain: \begin{align}&\int_{0}^{\delta}\lvert \frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}\lvert du \\ \leq &\int_{0}^{\delta}\lvert \frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}-f^{\prime\prime}(x)\lvert du+\delta\lvert f^{\prime\prime}(x)\rvert\\ \leq &\varepsilon \delta+\delta\lvert f^{\prime\prime}(x)\rvert\end{align}","if is in then why does necessarily exist? Clearly converges to as , how does this help me prove the above? Attempt: Let , then it follows that exists, as we can bound the integrand by Now consider that for all smaller than some we have , then we obtain:","f C^{2} \int_{0}^{1}\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}du u\to\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}} f^{\prime\prime}(x) u \to 0 \delta, \; \varepsilon > 0 \int_{\delta}^{1}\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}du 4\frac{\lvert f\rvert}{\delta ^{2}} u \delta > 0 \lvert f^{\prime\prime}(x)-\frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}\rvert \leq \varepsilon \begin{align}&\int_{0}^{\delta}\lvert \frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}\lvert du \\ \leq &\int_{0}^{\delta}\lvert \frac{f(x+u)+f(x-u)-2f(x)}{u^{2}}-f^{\prime\prime}(x)\lvert du+\delta\lvert f^{\prime\prime}(x)\rvert\\ \leq &\varepsilon \delta+\delta\lvert f^{\prime\prime}(x)\rvert\end{align}","['real-analysis', 'integration', 'derivatives', 'convergence-divergence', 'continuity']"
67,Differentiating a column with respect to a matrix,Differentiating a column with respect to a matrix,,"Let $\mathbf{X} = [\mathbf{x}_1 | ... | \mathbf{x}_n]$ be a $m \times n$ matrix. I would like to differentiate $\mathbf{x}_i = \mathbf{X} \mathbf{e}_i$ (where $\mathbf{e}_i \in \mathbb{R}^{n \times 1}$ is the unit vectors with $1$ on the $i$ th place and $0$ 's in the rest) with respect to $\mathbf{X}$ . Then $$ d\mathbf{x}_i = d(\mathbf{X}\mathbf{e}_i) = (\mathbf{X} + d\mathbf{X})\mathbf{e}_i - \mathbf{X}\mathbf{e}_i = (d\mathbf{X})\mathbf{e}_i $$ and therefore $$ \frac{d\mathbf{x}_i}{d\mathbf{X}} = \mathbf{e}_i \in \mathbb{R}^{n \times 1} $$ However, I suspect that is not consistent dimension-wise. For example: $f(\mathbf{X}) = \mathbf{a} \mathbf{x}_i$ where $\mathbf{a} \in \mathbb{R}^{1 \times m}$ then simply using the result above $$ \frac{d f(\mathbf{X})}{d\mathbf{X}} = \frac{d(\mathbf{a}\mathbf{x}_i)}{d\mathbf{X}} = \mathbf{a} \mathbf{e}_i \implies \mbox{Dimensions mismatch!} $$ since $\mathbf{a} \in \mathbb{R}^{1 \times m}$ and $\mathbf{e}_i \in \mathbb{R}^{n \times 1}$ . How to fix this issue? An idea is to put a pseudo identity matrix $$ \frac{d\mathbf{x}_i}{d\mathbf{X}} = \mathbf{I}_{m \times n} \mathbf{e}_i \in \mathbb{R}^{n \times 1} $$ such that $\mathbf{X} = \mathbf{X} \circ \mathbf{I}_{m \times n}$ with Hadamard product. But is this the right way to go?","Let be a matrix. I would like to differentiate (where is the unit vectors with on the th place and 's in the rest) with respect to . Then and therefore However, I suspect that is not consistent dimension-wise. For example: where then simply using the result above since and . How to fix this issue? An idea is to put a pseudo identity matrix such that with Hadamard product. But is this the right way to go?","\mathbf{X} = [\mathbf{x}_1 | ... | \mathbf{x}_n] m \times n \mathbf{x}_i = \mathbf{X} \mathbf{e}_i \mathbf{e}_i \in \mathbb{R}^{n \times 1} 1 i 0 \mathbf{X} 
d\mathbf{x}_i = d(\mathbf{X}\mathbf{e}_i) = (\mathbf{X} + d\mathbf{X})\mathbf{e}_i - \mathbf{X}\mathbf{e}_i = (d\mathbf{X})\mathbf{e}_i
 
\frac{d\mathbf{x}_i}{d\mathbf{X}} = \mathbf{e}_i \in \mathbb{R}^{n \times 1}
 f(\mathbf{X}) = \mathbf{a} \mathbf{x}_i \mathbf{a} \in \mathbb{R}^{1 \times m} 
\frac{d f(\mathbf{X})}{d\mathbf{X}} = \frac{d(\mathbf{a}\mathbf{x}_i)}{d\mathbf{X}} = \mathbf{a} \mathbf{e}_i \implies \mbox{Dimensions mismatch!}
 \mathbf{a} \in \mathbb{R}^{1 \times m} \mathbf{e}_i \in \mathbb{R}^{n \times 1} 
\frac{d\mathbf{x}_i}{d\mathbf{X}} = \mathbf{I}_{m \times n} \mathbf{e}_i \in \mathbb{R}^{n \times 1}
 \mathbf{X} = \mathbf{X} \circ \mathbf{I}_{m \times n}","['matrices', 'derivatives', 'matrix-calculus', 'tensors']"
68,How to find the maximum distance between two paths over a time interval (diff eqn) AP Calc Question,How to find the maximum distance between two paths over a time interval (diff eqn) AP Calc Question,,"Alice and Bob go for a jog in the same direction along a straight path. For $0\leq t \leq20$ , Alice’s velocity at time t is given by $A(t)=\frac{6010}{t^2-3t+50.5}$ meters per minute, and Bob’s velocity at time t is given by $B(t)=8.5t^3e^{-0.45t}$ meters per minute. Both of these velocities are always positive. Alice is 12 meters ahead of Bob at time $t=0$ , and she remains ahead of Bob for $0\leq t\leq 20$ . What is the maximum distance between Alice and Bob over the time interval $0\leq t \leq 20$ ? The answer is $d=1413.23$ when $t=4.58$ . I'm confused on how to get this answer... I did $\int_0^{20} \frac{6010}{t^2-3t+50.5} - \int_0^{20} 8.5t^3e^{-0.45t}$ $\int_0^{20}\frac{6010}{t^2-3t+50.5} =1232.323$ $\int_0^{20} 8.5t^3e^{-0.45t}=1217.313$ and got $1232.323-1217.313=15.01=d$ I thought that the distance between Alice and Bob is $\int A(t)-\int(B(t)$ so I'm confused where I went wrong. How can I solve this?","Alice and Bob go for a jog in the same direction along a straight path. For , Alice’s velocity at time t is given by meters per minute, and Bob’s velocity at time t is given by meters per minute. Both of these velocities are always positive. Alice is 12 meters ahead of Bob at time , and she remains ahead of Bob for . What is the maximum distance between Alice and Bob over the time interval ? The answer is when . I'm confused on how to get this answer... I did and got I thought that the distance between Alice and Bob is so I'm confused where I went wrong. How can I solve this?",0\leq t \leq20 A(t)=\frac{6010}{t^2-3t+50.5} B(t)=8.5t^3e^{-0.45t} t=0 0\leq t\leq 20 0\leq t \leq 20 d=1413.23 t=4.58 \int_0^{20} \frac{6010}{t^2-3t+50.5} - \int_0^{20} 8.5t^3e^{-0.45t} \int_0^{20}\frac{6010}{t^2-3t+50.5} =1232.323 \int_0^{20} 8.5t^3e^{-0.45t}=1217.313 1232.323-1217.313=15.01=d \int A(t)-\int(B(t),['derivatives']
69,Derivative of matrix multiplication's norm for linear regression,Derivative of matrix multiplication's norm for linear regression,,"I am trying to solve the derivative for the following function $f(θ)=0.5∥Xθ−y∥_2^2$ where X is a big(1000x2) matrix, θ is a 2x1 vector and y is a 1000x1 vector. I have so far realised that f can be reduced to 0.5(Xθ−y)(Xθ−y) and I tried applying both the chain rule and the product rule to come to the same result both times. The result being $f'(θ)= (Xθ−y)*\dfrac{df}{dθ}(Xθ−y)$ I've tried to do a lot of research on how to continue past this point and frankly from what I've seen I might have done everything wrong from the start. Could anyone give me any pointers?","I am trying to solve the derivative for the following function where X is a big(1000x2) matrix, θ is a 2x1 vector and y is a 1000x1 vector. I have so far realised that f can be reduced to 0.5(Xθ−y)(Xθ−y) and I tried applying both the chain rule and the product rule to come to the same result both times. The result being I've tried to do a lot of research on how to continue past this point and frankly from what I've seen I might have done everything wrong from the start. Could anyone give me any pointers?",f(θ)=0.5∥Xθ−y∥_2^2 f'(θ)= (Xθ−y)*\dfrac{df}{dθ}(Xθ−y),"['linear-algebra', 'derivatives', 'machine-learning', 'linear-regression']"
70,"Show there is a real number $c$, such that for any continuous $f$, $f'(x) =cx$ for some $x$ in $(0,1)$","Show there is a real number , such that for any continuous ,  for some  in","c f f'(x) =cx x (0,1)","Here is the full problem statement. Show that there is a unique real number $c$ such that for any function $f$ that is differentiable on $[0, 1]$ and that takes the values $f (0) = 0$ and $f (1) = 1$ there is some point $x \in (0,1)$ (depending on the function $f\;$ ) such that $f'(x) = cx$ . I think the correct value of $c$ is $2$ , by considering $f(x) = x^2$ , and can also use that function to prove uniqueness of $c$ . I’m having a hard time proving that any $f$ must have such a point. I think it will be an application of MVT, but would appreciate a hint. Cheers!","Here is the full problem statement. Show that there is a unique real number such that for any function that is differentiable on and that takes the values and there is some point (depending on the function ) such that . I think the correct value of is , by considering , and can also use that function to prove uniqueness of . I’m having a hard time proving that any must have such a point. I think it will be an application of MVT, but would appreciate a hint. Cheers!","c f [0, 1] f (0) = 0 f (1) = 1 x \in (0,1) f\; f'(x) = cx c 2 f(x) = x^2 c f","['real-analysis', 'derivatives']"
71,"If $f(x) = 2x^4-15x^3+ax^2+bx+c$, then if $(x-5)^2$ is a factor of $f$, then $f^{'}(5)=0$.","If , then if  is a factor of , then .",f(x) = 2x^4-15x^3+ax^2+bx+c (x-5)^2 f f^{'}(5)=0,"Let $f(x) = 2x^4-15x^3+ax^2+bx+c$ , then if $(x-5)^2$ is a factor of $f$ , then $f^{'}(5)=0$ . It is not immediately obvious why this is so, since this question is only awarded 1 mark. I actually have to let $f(x) = (x-5)^2(x-k_1)(x-k_2)$ and differentiate to find the answer. The product rule guarantees that the derivative will contain $x-5$ , hence the answer. but given it is 1 mark in an IB exam, is there any easier way?","Let , then if is a factor of , then . It is not immediately obvious why this is so, since this question is only awarded 1 mark. I actually have to let and differentiate to find the answer. The product rule guarantees that the derivative will contain , hence the answer. but given it is 1 mark in an IB exam, is there any easier way?",f(x) = 2x^4-15x^3+ax^2+bx+c (x-5)^2 f f^{'}(5)=0 f(x) = (x-5)^2(x-k_1)(x-k_2) x-5,"['derivatives', 'roots']"
72,Confusion on notation of multivariable differentiation in n-dimensional space for matrices (gradients),Confusion on notation of multivariable differentiation in n-dimensional space for matrices (gradients),,"I am learning gradients on my own. One of the rules given in the textbook I am reading states this (without proof): $$∇_\mathbf{x}\mathbf{A}\mathbf{x} = \mathbf{A}^⊤$$ and then proceeds onwards as if it's trivial to see. I haven't seen this before so I need to go prove that it is correct. I do not know if the above equation is equal to $$∇f(\mathbf{x}) = \mathbf{A}^⊤$$ where $$f(\mathbf{x}) = \mathbf{A}\mathbf{x}$$ I decided to go ahead with this anyways and see where it takes me. I figure doing this with a simple $\mathbb{R}^{2 \times 2}$ like $\mathbf{A} = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ and seeing where it takes me when $\mathbf{x} = \begin{bmatrix}i\\j\end{bmatrix}$ . This means $\mathbf{A}\mathbf{x}$ is: $$\begin{bmatrix}ai + bj\\ci + dj\end{bmatrix}$$ The gradient would be (for $\mathbf{Ax}$ as I'm passing that in for the parameter): $$∇f(\mathbf{Ax}) = \begin{bmatrix} \frac{\partial f(\mathbf{Ax})}{\partial i} \frac{\partial f(\mathbf{Ax})}{\partial j}\end{bmatrix}^⊤$$ Which means we want to evaluate $∇_\mathbf{x} \begin{bmatrix}ai + bj\\ci + dj\end{bmatrix}$ . By the definition of a gradient, then we should get $∇f(\mathbf{Ax}) = \begin{bmatrix} \begin{bmatrix}a\\c\end{bmatrix} \\ \begin{bmatrix}b\\d\end{bmatrix} \end{bmatrix}$ This however does not look right, or is it? Is that equal to $\begin{bmatrix}a&c\\b&d\end{bmatrix}$ ? I wrote it out that way because I suspect that when we ask for $a_{21}$ we are saying ""get the 2nd row in the first column, and in a programming fashion, A[2][1] (or A[1][0] for most languages) would get $b$ from the result above. This would make sense because I'd get the transpose I was looking for in the primitive example. My linear algebra class only covered vectors, then jumped to matrices and showed us matrix multiplication without covering matrices as if they were a vector of vectors, so if there is a fundamental gap in my knowledge here and what I've described above is correct... then we've found an issue. Moving to an n-dimensional proof after figuring out what is wrong (if anything) seems like it'd be straight forward with more rigorous variable and index naming.","I am learning gradients on my own. One of the rules given in the textbook I am reading states this (without proof): and then proceeds onwards as if it's trivial to see. I haven't seen this before so I need to go prove that it is correct. I do not know if the above equation is equal to where I decided to go ahead with this anyways and see where it takes me. I figure doing this with a simple like and seeing where it takes me when . This means is: The gradient would be (for as I'm passing that in for the parameter): Which means we want to evaluate . By the definition of a gradient, then we should get This however does not look right, or is it? Is that equal to ? I wrote it out that way because I suspect that when we ask for we are saying ""get the 2nd row in the first column, and in a programming fashion, A[2][1] (or A[1][0] for most languages) would get from the result above. This would make sense because I'd get the transpose I was looking for in the primitive example. My linear algebra class only covered vectors, then jumped to matrices and showed us matrix multiplication without covering matrices as if they were a vector of vectors, so if there is a fundamental gap in my knowledge here and what I've described above is correct... then we've found an issue. Moving to an n-dimensional proof after figuring out what is wrong (if anything) seems like it'd be straight forward with more rigorous variable and index naming.",∇_\mathbf{x}\mathbf{A}\mathbf{x} = \mathbf{A}^⊤ ∇f(\mathbf{x}) = \mathbf{A}^⊤ f(\mathbf{x}) = \mathbf{A}\mathbf{x} \mathbb{R}^{2 \times 2} \mathbf{A} = \begin{bmatrix}a&b\\c&d\end{bmatrix} \mathbf{x} = \begin{bmatrix}i\\j\end{bmatrix} \mathbf{A}\mathbf{x} \begin{bmatrix}ai + bj\\ci + dj\end{bmatrix} \mathbf{Ax} ∇f(\mathbf{Ax}) = \begin{bmatrix} \frac{\partial f(\mathbf{Ax})}{\partial i} \frac{\partial f(\mathbf{Ax})}{\partial j}\end{bmatrix}^⊤ ∇_\mathbf{x} \begin{bmatrix}ai + bj\\ci + dj\end{bmatrix} ∇f(\mathbf{Ax}) = \begin{bmatrix} \begin{bmatrix}a\\c\end{bmatrix} \\ \begin{bmatrix}b\\d\end{bmatrix} \end{bmatrix} \begin{bmatrix}a&c\\b&d\end{bmatrix} a_{21} b,"['linear-algebra', 'derivatives', 'partial-derivative']"
73,$n$th derivative of a function that depends both explicitly and implicitly on a variable,th derivative of a function that depends both explicitly and implicitly on a variable,n,"I would like to have a closed-formula for the $n$ th derivative of a function of the type $f(x,g(x))$ , i.e. a function that depends both explicitly and implicitly on a (scalar) variable: $$ \frac{d^n f}{d x^n} = ? $$ It can be assumed that all partial derivatives up to order $n$ included exist. I can calculate the first (or second, or third...) derivatives by hand by using the chain rule. For example: $$ \frac{d f}{d x} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x} = f^{(1,0)} + g'f^{(0,1)}. $$ or $$ \frac{d^2 f}{d x^2} =  f^{(2, 0)} +  2 g' f^{(1, 1)} + (g')^2 f^{(0, 2)} + g'' f^{(0, 1)} $$ I however cannot see a pattern for a general formula. I am aware of the Faà di Bruno formula for calculating high order derivatives of a purely implicit function, but I am failing in seeing how to use it in combination with an explicit dependence of the function on the variable.","I would like to have a closed-formula for the th derivative of a function of the type , i.e. a function that depends both explicitly and implicitly on a (scalar) variable: It can be assumed that all partial derivatives up to order included exist. I can calculate the first (or second, or third...) derivatives by hand by using the chain rule. For example: or I however cannot see a pattern for a general formula. I am aware of the Faà di Bruno formula for calculating high order derivatives of a purely implicit function, but I am failing in seeing how to use it in combination with an explicit dependence of the function on the variable.","n f(x,g(x)) 
\frac{d^n f}{d x^n} = ?
 n 
\frac{d f}{d x} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x} = f^{(1,0)} + g'f^{(0,1)}.
 
\frac{d^2 f}{d x^2} =  f^{(2, 0)} +  2 g' f^{(1, 1)} + (g')^2 f^{(0, 2)} + g'' f^{(0, 1)}
","['derivatives', 'implicit-differentiation', 'chain-rule']"
74,Extension of Definition of Concavity,Extension of Definition of Concavity,,"Question Suppose that the graph of a function $f$ is concave up on an open interval $I$ . Show that, for any $a, b \in I$ , where $a < b$ and $0 < \lambda < 1$ , $$(1 - \lambda)f(a) + \lambda f(b) > f((1 - \lambda)a + \lambda b).$$ My working From the definition of a function being concave up, $$f(b) - f(a) > (b - a)f'(a).$$ When $b = (1 - \lambda)a + \lambda b$ , $$f((1 - \lambda)a + \lambda b) - f(a) > ((1 - \lambda)a + \lambda b - a)f'(a)$$ $$\implies f((1 - \lambda)a + \lambda b) - f(a) > \lambda (b - a)f'(a)$$ This is where I am currently stuck at. I have a hunch that I am supposed to use the definition once more by substituting another set of values for $b$ and $a$ in order to get rid of the $f'(a)$ , but I cannot see what they are. Any intuitions will be greatly appreciated!","Question Suppose that the graph of a function is concave up on an open interval . Show that, for any , where and , My working From the definition of a function being concave up, When , This is where I am currently stuck at. I have a hunch that I am supposed to use the definition once more by substituting another set of values for and in order to get rid of the , but I cannot see what they are. Any intuitions will be greatly appreciated!","f I a, b \in I a < b 0 < \lambda < 1 (1 - \lambda)f(a) + \lambda f(b) > f((1 - \lambda)a + \lambda b). f(b) - f(a) > (b - a)f'(a). b = (1 - \lambda)a + \lambda b f((1 - \lambda)a + \lambda b) - f(a) > ((1 - \lambda)a + \lambda b - a)f'(a) \implies f((1 - \lambda)a + \lambda b) - f(a) > \lambda (b - a)f'(a) b a f'(a)","['real-analysis', 'calculus', 'derivatives']"
75,Proving stationary points of inflection,Proving stationary points of inflection,,"Edit For the purposes of proving the statement below, a stationary point of inflection of a curve shall be defined as a point on the curve where the curve changes concavity. Problem Suppose $f(x)$ is $k$ times differentiable with $k \mod 2 \equiv 1$ and $k \geq 3$ . Then, if $f^{(n)}(c) = 0$ for $n = 1, ..., k - 1$ and $f^{(k)}(c) \neq 0$ , prove that $c$ is a stationary point of inflection of $f$ . I have successfully proven the cases where $k = 3$ and $k = 5$ (or so I think) and I am currently trying to devise a proof for the general case above. I am trying to use the ideas from my two proofs (they are largely based on the second derivative test) and am thinking along the lines of induction, but I am not sure if that is wise. Any suggestions/hints/help will be greatly appreciated! As I am not so well-versed in mathematical proof-writing as I would like to be, I am also providing my proofs for the $k = 3$ and $k = 5$ , so that the community may critique them for me! Proof for $k = 3$ Suppose $f^{(3)}(c) > 0$ $\because f^{(3)}(c) = \lim \limits_{x \to c} \frac {f^{(2)}(x) - f^{(2)}(c)} {x - c} = \lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c}$ $\therefore \lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c} > 0$ When $x \rightarrow c^+$ , $x > c$ For $\lim \limits_{x \to c^+} \frac {f^{(2)}(x)} {x - c} > 0$ , $f^{(2)}(x) > 0$ When $x \rightarrow c^-$ , $x < c$ For $\lim \limits_{x \to c^-} \frac {f^{(2)}(x)} {x - c} > 0$ , $f^{(2)}(x) < 0$ $\because f^{(2)}(x)$ changes sign at $c$ $\therefore f$ changes concavity at $c$ $\implies$ By definition, $c$ is a stationary point of inflection of $f(x)$ Similarly, if $f^{(3)}(x) < 0$ , then $\lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c} < 0$ When $x \rightarrow c^+$ For $\lim \limits_{x \to c^+} \frac {f^{(2)}(x)} {x - c} < 0$ , $f^{(2)}(x) < 0$ When $x \rightarrow c^-$ For $\lim \limits_{x \to c^-} \frac {f^{(2)}(x)} {x - c} < 0$ , $f^{(2)}(x) > 0$ $\because f^{(2)}(x)$ changes sign at $c$ $\therefore f$ changes concavity at $c$ $\implies$ By definition, $c$ is a stationary point of inflection of $f(x)$ To conclude, suppose $f(x)$ is $3$ times differentiable. If $f^{(n)}(c) = 0$ for $n = 1, 2$ and $f^{(3)}(c) \neq 0$ , then $c$ is a stationary point of inflection of $f$ . Proof for $k = 5$ Suppose $f^{(5)}(c) > 0$ Let $g(x) = f^{(3)}(x)$ $\because g^{(1)}(c) = 0$ and $g^{(2)}(c) > 0$ $\therefore g(x)$ has a minimum at $c$ $\because g(c) = 0$ $\therefore$ for all $x$ near $c$ , $g(x) > 0$ $\implies f^{(2)}(x)$ is an increasing function near $c$ In particular, when $x \rightarrow c^-$ , $f^{(2)}(x) < f^{(2)}(c)$ and when $x \rightarrow c^+$ , $f^{(2)}(x) > f^{(2)}(c)$ $\because f^{(2)}(c) = 0$ $\therefore f^{(2)}(x)$ changes sign at $c$ $\implies f(x)$ changes concavity at $c$ $\therefore$ By definition, $c$ is a stationary point of inflection of $f(x)$ Similarly, if $f^{(5)}(c) < 0$ , then $g(x)$ has a maximum at $c$ $\because g(c) = 0$ $\therefore$ for all $x$ near $c$ , $g(x) < 0$ $\implies f^{(2)}(x)$ is a decreasing function near $c$ In particular, when $x \rightarrow c^-$ , $f^{(2)}(x) > f^{(2)}(c)$ and when $x \rightarrow c^+$ , $f^{(2)}(x) < f^{(2)}(c)$ $\because f^{(2)}(c) = 0$ $\therefore f^{(2)}(x)$ changes sign at $c$ $\implies f(x)$ changes concavity at $c$ $\therefore$ By definition, $c$ is a stationary point of inflection of $f(x)$ To conclude, suppose $f(x)$ is $5$ times differentiable. If $f^{(n)}(c) = 0$ for $n = 1, ..., 4$ and $f^{(5)}(c) \neq 0$ , then $c$ is a stationary point of inflection of $f$ . Having been able to come up with these two proofs largely by myself, with some help from my professor, I am actually quite excited on trying a proof for the general case where I am leaning towards induction (actually, it is the only form I can think of), but as my ideas for $k = 3$ and $k = 5$ are not exactly identical, I am not sure if induction is the way to go. I am also trying to stick to second derivative tests (or something of similar difficulty) as I am currently only taking an introductory calculus module at university, so I do not have such ""high-powered"" tools at my disposal, such as Taylor's Series/Theorem and the likes of it. Also, apologies for the lengthy post! Edit 2 Proof for the general case (Many thanks to John Hughes for the guidance) Let $g(x) = f(x + c) - f(c)$ $\implies g(0) = 0$ and $g^{(k)}(0) = f^{(k)}(c)$ Then, it suffices to prove that, if $0$ is a stationary point of inflection of $g$ , $c$ will be a stationary point of inflection of $f$ . Suppose $g^{(3)}(0) > 0$ $\because g^{(3)}(c) = \lim \limits_{x \to 0} \frac {g^{(2)}(x) - g^{(2)}(0)} {x - 0} = \lim \limits_{x \to 0} \frac {g^{(2)}(x)} {x}$ $\therefore \lim \limits_{x \to 0} \frac {g^{(2)}(0)} {x} > 0$ When $x \rightarrow 0^+$ , $x > 0$ For $\lim \limits_{x \to 0^+} \frac {g^{(2)}(x)} {x} > 0$ , $g^{(2)}(x) > 0$ $\because g^{(2)}(x) > 0$ for some $x \in (0, b)$ and $f^{(2)}(x) = g^{(2)}(x - c)$ , $\therefore f^{(2)}(x) > 0$ for some $x \in (c, b + c)$ When $x \rightarrow 0^-$ , $x < 0$ For $\lim \limits_{x \to 0^-} \frac {g^{(2)}(x)} {x} > 0$ , $g^{(2)}(x) < 0$ $\because g^{(2)}(x) < 0$ for some $x \in (-b, 0)$ and $f^{(2)}(x) = g^{(2)}(x - c)$ , $\therefore f^{(2)}(x) < 0$ for some $x \in (-b + c, c)$ $\implies f^{(2)}$ changes sign near $c$ $\implies f$ changes concavity at $c$ $\therefore c$ is a stationary point of inflection of $f$ Similarly, if $g^{(3)}(0) < 0$ , then $\lim \limits_{x \to 0} \frac {g^{(2)}(x)} {x} < 0$ When $x \rightarrow 0^+$ For $\lim \limits_{x \to 0^+} \frac {g^{(2)}(x)} {x} < 0$ , $g^{(2)}(x) < 0$ $\because g^{(2)}(x) < 0$ for some $x \in (0, b)$ and $f^{(2)}(x) = g^{(2)}(x - c)$ , $\therefore f^{(2)}(x) < 0$ for some $x \in (c, b + c)$ When $x \rightarrow 0^-$ For $\lim \limits_{x \to 0^-} \frac {g^{(2)}(x)} {x} < 0$ , $g^{(2)}(x) > 0$ $\because g^{(2)}(x) > 0$ for some $x \in (-b, 0)$ and $f^{(2)}(x) = g^{(2)}(x - c)$ , $\therefore f^{(2)}(x) > 0$ for some $x \in (-b + c, c)$ $\implies f^{(2)}$ changes sign near $c$ $\implies f$ changes concavity at $c$ $\therefore c$ is a stationary point of inflection of $f$ To conclude, suppose $f(x)$ is $k$ times differentiable with $k \mod 2 \equiv 1$ and $k \geq 3$ . If $f^{(n)}(c) = 0$ for $n = 1, ..., k - 1$ and $f^{(k)}(c) \neq 0$ , then $c$ is a stationary point of inflection of $f$ .","Edit For the purposes of proving the statement below, a stationary point of inflection of a curve shall be defined as a point on the curve where the curve changes concavity. Problem Suppose is times differentiable with and . Then, if for and , prove that is a stationary point of inflection of . I have successfully proven the cases where and (or so I think) and I am currently trying to devise a proof for the general case above. I am trying to use the ideas from my two proofs (they are largely based on the second derivative test) and am thinking along the lines of induction, but I am not sure if that is wise. Any suggestions/hints/help will be greatly appreciated! As I am not so well-versed in mathematical proof-writing as I would like to be, I am also providing my proofs for the and , so that the community may critique them for me! Proof for Suppose When , For , When , For , changes sign at changes concavity at By definition, is a stationary point of inflection of Similarly, if , then When For , When For , changes sign at changes concavity at By definition, is a stationary point of inflection of To conclude, suppose is times differentiable. If for and , then is a stationary point of inflection of . Proof for Suppose Let and has a minimum at for all near , is an increasing function near In particular, when , and when , changes sign at changes concavity at By definition, is a stationary point of inflection of Similarly, if , then has a maximum at for all near , is a decreasing function near In particular, when , and when , changes sign at changes concavity at By definition, is a stationary point of inflection of To conclude, suppose is times differentiable. If for and , then is a stationary point of inflection of . Having been able to come up with these two proofs largely by myself, with some help from my professor, I am actually quite excited on trying a proof for the general case where I am leaning towards induction (actually, it is the only form I can think of), but as my ideas for and are not exactly identical, I am not sure if induction is the way to go. I am also trying to stick to second derivative tests (or something of similar difficulty) as I am currently only taking an introductory calculus module at university, so I do not have such ""high-powered"" tools at my disposal, such as Taylor's Series/Theorem and the likes of it. Also, apologies for the lengthy post! Edit 2 Proof for the general case (Many thanks to John Hughes for the guidance) Let and Then, it suffices to prove that, if is a stationary point of inflection of , will be a stationary point of inflection of . Suppose When , For , for some and , for some When , For , for some and , for some changes sign near changes concavity at is a stationary point of inflection of Similarly, if , then When For , for some and , for some When For , for some and , for some changes sign near changes concavity at is a stationary point of inflection of To conclude, suppose is times differentiable with and . If for and , then is a stationary point of inflection of .","f(x) k k \mod 2 \equiv 1 k \geq 3 f^{(n)}(c) = 0 n = 1, ..., k - 1 f^{(k)}(c) \neq 0 c f k = 3 k = 5 k = 3 k = 5 k = 3 f^{(3)}(c) > 0 \because f^{(3)}(c) = \lim \limits_{x \to c} \frac {f^{(2)}(x) - f^{(2)}(c)} {x - c} = \lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c} \therefore \lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c} > 0 x \rightarrow c^+ x > c \lim \limits_{x \to c^+} \frac {f^{(2)}(x)} {x - c} > 0 f^{(2)}(x) > 0 x \rightarrow c^- x < c \lim \limits_{x \to c^-} \frac {f^{(2)}(x)} {x - c} > 0 f^{(2)}(x) < 0 \because f^{(2)}(x) c \therefore f c \implies c f(x) f^{(3)}(x) < 0 \lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c} < 0 x \rightarrow c^+ \lim \limits_{x \to c^+} \frac {f^{(2)}(x)} {x - c} < 0 f^{(2)}(x) < 0 x \rightarrow c^- \lim \limits_{x \to c^-} \frac {f^{(2)}(x)} {x - c} < 0 f^{(2)}(x) > 0 \because f^{(2)}(x) c \therefore f c \implies c f(x) f(x) 3 f^{(n)}(c) = 0 n = 1, 2 f^{(3)}(c) \neq 0 c f k = 5 f^{(5)}(c) > 0 g(x) = f^{(3)}(x) \because g^{(1)}(c) = 0 g^{(2)}(c) > 0 \therefore g(x) c \because g(c) = 0 \therefore x c g(x) > 0 \implies f^{(2)}(x) c x \rightarrow c^- f^{(2)}(x) < f^{(2)}(c) x \rightarrow c^+ f^{(2)}(x) > f^{(2)}(c) \because f^{(2)}(c) = 0 \therefore f^{(2)}(x) c \implies f(x) c \therefore c f(x) f^{(5)}(c) < 0 g(x) c \because g(c) = 0 \therefore x c g(x) < 0 \implies f^{(2)}(x) c x \rightarrow c^- f^{(2)}(x) > f^{(2)}(c) x \rightarrow c^+ f^{(2)}(x) < f^{(2)}(c) \because f^{(2)}(c) = 0 \therefore f^{(2)}(x) c \implies f(x) c \therefore c f(x) f(x) 5 f^{(n)}(c) = 0 n = 1, ..., 4 f^{(5)}(c) \neq 0 c f k = 3 k = 5 g(x) = f(x + c) - f(c) \implies g(0) = 0 g^{(k)}(0) = f^{(k)}(c) 0 g c f g^{(3)}(0) > 0 \because g^{(3)}(c) = \lim \limits_{x \to 0} \frac {g^{(2)}(x) - g^{(2)}(0)} {x - 0} = \lim \limits_{x \to 0} \frac {g^{(2)}(x)} {x} \therefore \lim \limits_{x \to 0} \frac {g^{(2)}(0)} {x} > 0 x \rightarrow 0^+ x > 0 \lim \limits_{x \to 0^+} \frac {g^{(2)}(x)} {x} > 0 g^{(2)}(x) > 0 \because g^{(2)}(x) > 0 x \in (0, b) f^{(2)}(x) = g^{(2)}(x - c) \therefore f^{(2)}(x) > 0 x \in (c, b + c) x \rightarrow 0^- x < 0 \lim \limits_{x \to 0^-} \frac {g^{(2)}(x)} {x} > 0 g^{(2)}(x) < 0 \because g^{(2)}(x) < 0 x \in (-b, 0) f^{(2)}(x) = g^{(2)}(x - c) \therefore f^{(2)}(x) < 0 x \in (-b + c, c) \implies f^{(2)} c \implies f c \therefore c f g^{(3)}(0) < 0 \lim \limits_{x \to 0} \frac {g^{(2)}(x)} {x} < 0 x \rightarrow 0^+ \lim \limits_{x \to 0^+} \frac {g^{(2)}(x)} {x} < 0 g^{(2)}(x) < 0 \because g^{(2)}(x) < 0 x \in (0, b) f^{(2)}(x) = g^{(2)}(x - c) \therefore f^{(2)}(x) < 0 x \in (c, b + c) x \rightarrow 0^- \lim \limits_{x \to 0^-} \frac {g^{(2)}(x)} {x} < 0 g^{(2)}(x) > 0 \because g^{(2)}(x) > 0 x \in (-b, 0) f^{(2)}(x) = g^{(2)}(x - c) \therefore f^{(2)}(x) > 0 x \in (-b + c, c) \implies f^{(2)} c \implies f c \therefore c f f(x) k k \mod 2 \equiv 1 k \geq 3 f^{(n)}(c) = 0 n = 1, ..., k - 1 f^{(k)}(c) \neq 0 c f","['real-analysis', 'calculus', 'derivatives', 'stationary-point']"
76,"A differentiable function on $(a,b)$ with nonzero derivative over $(a,b)$ where $f'(c)>0$ for some $c\in(a,b)$ means $f'(x)>0$ for all $x\in(a,b)$.",A differentiable function on  with nonzero derivative over  where  for some  means  for all .,"(a,b) (a,b) f'(c)>0 c\in(a,b) f'(x)>0 x\in(a,b)","It certainly seems correct that if you have at least one point $c\in(a,b)$ such that $f'(c)>0$ , where $f'(x)\ne0$ for all $x\in(a,b)$ , then $f'(x)>0$ for all $x\in(a,b)$ . At least from a calculus standpoint, if $f$ is well-defined (not necessarily continuous), it should be the case that this is true. But how does one show this formally? The exact statement I am attempting to prove is this: Suppose $f:(a,b)\to\mathbb{R}$ is a differentiable function such that $f'(x)\ne0$ for all $x\in(a,b)$ . Suppose there exists a point $c\in(a,b)$ with $f'(c)>0$ , then $f'(x)>0$ for all $x\in(a,b)$ . I thought it might be easiest to prove by contradiction, and then use the intermediate value property to derive a contradiction. Any pointers would be helpful on this one! EDIT: My original direct proof went something like this: Let $f:(a,b)\to\mathbb{R}$ be a differentiable function such that $f'(x)\ne0$ for all $x\in(a,b)$ . Take a point $c\in(a,b)$ such that $f'(c)>0$ . Since $f'(x)\ne0$ for any $x\in(a,b)$ there can be no extreme values on $(a,b)$ . Then take some $y\in\mathbb{R}$ such that $f'(c)=y$ and by the intermediate value property, we are guaranteed that $f'(a)<y<f'(b)$ , that is $f$ is strictly increasing on $(a,b)$ . Hence $f'(x)>0$ for all $x\in(a,b)$ . I'm a little worried about some of the logic in this. It feels incomplete to me.","It certainly seems correct that if you have at least one point such that , where for all , then for all . At least from a calculus standpoint, if is well-defined (not necessarily continuous), it should be the case that this is true. But how does one show this formally? The exact statement I am attempting to prove is this: Suppose is a differentiable function such that for all . Suppose there exists a point with , then for all . I thought it might be easiest to prove by contradiction, and then use the intermediate value property to derive a contradiction. Any pointers would be helpful on this one! EDIT: My original direct proof went something like this: Let be a differentiable function such that for all . Take a point such that . Since for any there can be no extreme values on . Then take some such that and by the intermediate value property, we are guaranteed that , that is is strictly increasing on . Hence for all . I'm a little worried about some of the logic in this. It feels incomplete to me.","c\in(a,b) f'(c)>0 f'(x)\ne0 x\in(a,b) f'(x)>0 x\in(a,b) f f:(a,b)\to\mathbb{R} f'(x)\ne0 x\in(a,b) c\in(a,b) f'(c)>0 f'(x)>0 x\in(a,b) f:(a,b)\to\mathbb{R} f'(x)\ne0 x\in(a,b) c\in(a,b) f'(c)>0 f'(x)\ne0 x\in(a,b) (a,b) y\in\mathbb{R} f'(c)=y f'(a)<y<f'(b) f (a,b) f'(x)>0 x\in(a,b)","['real-analysis', 'derivatives']"
77,Test for a continuous function,Test for a continuous function,,"Let $f$ be a function defined in $[0, 6]$ , continuous in $[0, 6]$ and it is provided of a third derivative in $]0, 6[.$ Which of the following assertions is false ? $$\fbox{A}\quad f \text{ has no asymptotes; }$$ $$\fbox{B}\quad f \text{ may have no critical points; }$$ $$\fbox{C}\quad f \text{ has a relative maximum or has a minimum relative; }$$ $$\fbox{D}\quad f'' \text{ is continuous in } ]0; 6[;$$ $$\fbox{E}\quad \text{If } f'(5) = f''(5) = 0 \text{ and } f'''(5) = 7, \text{then } f \text{ has an inflection point with  a horizontal tangent at } x = 5$$ Below there is the original question in Italian Language. Above there is the translation. My attempt of resolution for to find the correct answer. The $\fbox{A}$ is true being $f$ is continuous in $[0,6]$ .  The $\fbox{B}$ is true for the Weierstrass' theorem: remark that $[0,6]$ is closed set. If I think to the polynomial $\deg(p(x))=6$ and $\fbox{C}$ for me it is true. For the $\fbox{D}$ I have thought that if $f$ and it is provided of a third derivative in $]0,6[$ , almost for $f''$ is continuous in $]0,6[$ . I'd say the $\fbox{E}$ is false , but I can't justify it. I ask if my reasoning is correct or there are incongruities.","Let be a function defined in , continuous in and it is provided of a third derivative in Which of the following assertions is false ? Below there is the original question in Italian Language. Above there is the translation. My attempt of resolution for to find the correct answer. The is true being is continuous in .  The is true for the Weierstrass' theorem: remark that is closed set. If I think to the polynomial and for me it is true. For the I have thought that if and it is provided of a third derivative in , almost for is continuous in . I'd say the is false , but I can't justify it. I ask if my reasoning is correct or there are incongruities.","f [0, 6] [0, 6] ]0, 6[. \fbox{A}\quad f \text{ has no asymptotes; } \fbox{B}\quad f \text{ may have no critical points; } \fbox{C}\quad f \text{ has a relative maximum or has a minimum
relative; } \fbox{D}\quad f'' \text{ is continuous in } ]0; 6[; \fbox{E}\quad \text{If } f'(5) = f''(5) = 0 \text{ and } f'''(5) = 7, \text{then } f \text{ has an inflection point with 
a horizontal tangent at } x = 5 \fbox{A} f [0,6] \fbox{B} [0,6] \deg(p(x))=6 \fbox{C} \fbox{D} f ]0,6[ f'' ]0,6[ \fbox{E}","['real-analysis', 'calculus', 'derivatives']"
78,Prove that $ f(1)\leq f(x)<f(0)$ and another conjecture .,Prove that  and another conjecture ., f(1)\leq f(x)<f(0),"It's a problem found with the help of WA . Let $0<x$ a real number and $n\geq 1$ a natural number then we have : $$ f(1)\leq f(x)=(1+x)^{\frac{-1}{(nx)}}+\Big(1+\frac{1}{x}\Big)^{-\frac{x}{n}}<f(0)$$ I have also conjectured that : Let $1\leq x$ a real number and $n\geq 1$ a natural number then we have : $$f\Big(\frac{x+\frac{1}{x}}{2}\Big)\leq f(x)$$ This conjecture I think is useful because of this fact : Let $g(x)=\frac{x+\frac{1}{x}}{2}$ and $x\geq 1$ a real number then we have : $$f(1) \leq f(g_n(x))\leq f(g_{n-1}(x))\leq \cdots \leq f(g(x))\leq f(x)\leq f(g^{-1}(x))\leq \cdots\leq f(g^{-1}_n(x))<f(0)$$ Where we speak about the inverse of the function $g(x)$ and the iteration ( $n$ to $n$ times) of the function $g(x)$ with itself . So the idea is to prove more generaly that $x=1$ is a minimum and $x=0$ is an infimum . Well I have tried the same method as here by user Robin Aldabanx (first answer with bounty) for the first case or $n=1$ . I have tried power series as well without success this time . I have been also inspired by the Polya's proof of Am-Gm but no good issues . Update Case $n=1$ One can prove that the function : $$h(x)=(1+x)^{\frac{-1}{x}}$$ is concave on $(0,\infty)$ . So we can apply Karamata's inequality and a majorization to get something of the kind : $$h(x)+h\Big(\frac{1}{x}\Big)\geq h(y)+h\Big(\frac{1}{y}\Big)\quad (1)$$ The inequality $(1)$ gives information on $f(x)$ in the case where $n=1$ .Via the majorization we can say as it's increasing or decreasing . Moreover I think we can apply this  method to the general case $n\geq 1$ . I don't know if it's really relevant but the inverse function of $h(x)$ is : $$h^{-1}(x)=\frac{\operatorname{W}(x\log(x))-\log(x)}{\log(x)}$$ With the Lambert's function . If you have a nice way to solve it . Thanks you very much .",It's a problem found with the help of WA . Let a real number and a natural number then we have : I have also conjectured that : Let a real number and a natural number then we have : This conjecture I think is useful because of this fact : Let and a real number then we have : Where we speak about the inverse of the function and the iteration ( to times) of the function with itself . So the idea is to prove more generaly that is a minimum and is an infimum . Well I have tried the same method as here by user Robin Aldabanx (first answer with bounty) for the first case or . I have tried power series as well without success this time . I have been also inspired by the Polya's proof of Am-Gm but no good issues . Update Case One can prove that the function : is concave on . So we can apply Karamata's inequality and a majorization to get something of the kind : The inequality gives information on in the case where .Via the majorization we can say as it's increasing or decreasing . Moreover I think we can apply this  method to the general case . I don't know if it's really relevant but the inverse function of is : With the Lambert's function . If you have a nice way to solve it . Thanks you very much .,"0<x n\geq 1  f(1)\leq f(x)=(1+x)^{\frac{-1}{(nx)}}+\Big(1+\frac{1}{x}\Big)^{-\frac{x}{n}}<f(0) 1\leq x n\geq 1 f\Big(\frac{x+\frac{1}{x}}{2}\Big)\leq f(x) g(x)=\frac{x+\frac{1}{x}}{2} x\geq 1 f(1) \leq f(g_n(x))\leq f(g_{n-1}(x))\leq \cdots \leq f(g(x))\leq f(x)\leq f(g^{-1}(x))\leq \cdots\leq f(g^{-1}_n(x))<f(0) g(x) n n g(x) x=1 x=0 n=1 n=1 h(x)=(1+x)^{\frac{-1}{x}} (0,\infty) h(x)+h\Big(\frac{1}{x}\Big)\geq h(y)+h\Big(\frac{1}{y}\Big)\quad (1) (1) f(x) n=1 n\geq 1 h(x) h^{-1}(x)=\frac{\operatorname{W}(x\log(x))-\log(x)}{\log(x)}","['derivatives', 'inequality', 'exponentiation']"
79,Compute $f^{(2001)}(0)$ where $f(x) = e^{-x}\sin(x)$ [duplicate],Compute  where  [duplicate],f^{(2001)}(0) f(x) = e^{-x}\sin(x),"This question already has answers here : $100$-th derivative of the function $f(x)=e^{x}\cos(x)$ (7 answers) Closed 3 years ago . The question: Let $f(x) = e^{-x}\sin(x)$ . Calculate $f^{(2001)}(0)$ . Note that in the question, $f^{(n)}(x)$ . means the $n$ -th derivative of $f$ . I calculated the first six derivatives on zero and got: $f^{(0)}(0) = 0$ . $f^{(1)}(0) = 1$ . $f^{(2)}(0) = -2$ . $f^{(3)}(0) = 2$ . $f^{(4)}(0) = 0$ . $f^{(5)}(0) = -4$ . $f^{(6)}(0) = 8$ . So, the pattern isn't pretty clear. Also, I thought that if $n$ is even, then: $f^{(n)}(x) = -e^{-x}\cos(x) - f(x) + f^{(1)}(x) - f^{(2)}(x) + ... + f^{(n-3)}(x) - f^{(n-2)}(x) - f^{(n-1)}(x)$ . and $f^{(n - 1)}(x) = e^{-x}\cos(x) + f(x) - f^{(1)}(x) + f^{(2)}(x) + ... + f^{(n-4)}(x) - f^{(n-3)}(x) - f^{(n-2)}(x)$ . So by substituting $f^{(n - 1)}(x)$ in the first equation, we get: $f^{(n)}(x) = -2(e^{-x}\cos(x) + f(x) - f^{(1)}(x) + f^{(2)}(x) + ... + f^{(n-4)}(x) - f^{(n-3)}(x))$ I tried to the same thing with $n$ being odd and tried to perform an induction to get a formula for the value of $f^{(n)}(x)$ but couldn't find a way to do so. Any ideas?","This question already has answers here : $100$-th derivative of the function $f(x)=e^{x}\cos(x)$ (7 answers) Closed 3 years ago . The question: Let . Calculate . Note that in the question, . means the -th derivative of . I calculated the first six derivatives on zero and got: . . . . . . . So, the pattern isn't pretty clear. Also, I thought that if is even, then: . and . So by substituting in the first equation, we get: I tried to the same thing with being odd and tried to perform an induction to get a formula for the value of but couldn't find a way to do so. Any ideas?",f(x) = e^{-x}\sin(x) f^{(2001)}(0) f^{(n)}(x) n f f^{(0)}(0) = 0 f^{(1)}(0) = 1 f^{(2)}(0) = -2 f^{(3)}(0) = 2 f^{(4)}(0) = 0 f^{(5)}(0) = -4 f^{(6)}(0) = 8 n f^{(n)}(x) = -e^{-x}\cos(x) - f(x) + f^{(1)}(x) - f^{(2)}(x) + ... + f^{(n-3)}(x) - f^{(n-2)}(x) - f^{(n-1)}(x) f^{(n - 1)}(x) = e^{-x}\cos(x) + f(x) - f^{(1)}(x) + f^{(2)}(x) + ... + f^{(n-4)}(x) - f^{(n-3)}(x) - f^{(n-2)}(x) f^{(n - 1)}(x) f^{(n)}(x) = -2(e^{-x}\cos(x) + f(x) - f^{(1)}(x) + f^{(2)}(x) + ... + f^{(n-4)}(x) - f^{(n-3)}(x)) n f^{(n)}(x),"['calculus', 'derivatives']"
80,"How do you take the derivative $\frac{d}{dx} \int_a^x f(x,t) dt$?",How do you take the derivative ?,"\frac{d}{dx} \int_a^x f(x,t) dt","How does one take the derivative for the function $g(x) = \int_a^xf(x,t)\ dt$ ? $$ \frac{d}{dx}g(x) = \frac{d}{dx}\int_a^xf(x,t)\ dt $$ For example, how would one find $\frac{d}{dx}\int_0^x x + t \ dt$ ? If $f=f(t)$ , then I know I can just use the fundamental theorem, but here $f=f(x,t)$ .","How does one take the derivative for the function ? For example, how would one find ? If , then I know I can just use the fundamental theorem, but here .","g(x) = \int_a^xf(x,t)\ dt  \frac{d}{dx}g(x) = \frac{d}{dx}\int_a^xf(x,t)\ dt  \frac{d}{dx}\int_0^x x + t \ dt f=f(t) f=f(x,t)","['calculus', 'integration', 'derivatives']"
81,How can one point determine a unique straight line in differentiation?,How can one point determine a unique straight line in differentiation?,,"I found a similar question and a beautiful answer here . However I'm not able to fully understand the answer and have a question on the selected answer at: Consider all the lines going through point $(x_0,f(x_0))$ . For every line, the relative error should approach $0$ as $x$ approaches $0$ because all these lines go through point $(x_0,f(x_0))$ , the linear approximation equals the function at $x=x_0$ . What is special about the tangent line in relation to the relative error ? Why does Arturo say only the tangent line makes the relative error zero ?","I found a similar question and a beautiful answer here . However I'm not able to fully understand the answer and have a question on the selected answer at: Consider all the lines going through point . For every line, the relative error should approach as approaches because all these lines go through point , the linear approximation equals the function at . What is special about the tangent line in relation to the relative error ? Why does Arturo say only the tangent line makes the relative error zero ?","(x_0,f(x_0)) 0 x 0 (x_0,f(x_0)) x=x_0","['calculus', 'derivatives', 'tangent-line', 'slope']"
82,"Is it allowed to take the total derivative of an infinitesimal, and is it equal to zero?","Is it allowed to take the total derivative of an infinitesimal, and is it equal to zero?",,"For instance, I start with this relation: $$ s^2=x^2+y^2 $$ Taking the total derivative on each side, I get: $$ 2sds=2xdx+2ydy $$ Can I take the total derivative a second like this: $$ d[sds]=d[xdx]+d[ydy]\\ dsds+sd[ds]=dxdx+xd[dx]+dydy+yd[dy]\\ (ds)^2=(dx)^2+(dy)^2 $$ where $d[d[s]]=0$ .","For instance, I start with this relation: Taking the total derivative on each side, I get: Can I take the total derivative a second like this: where .","
s^2=x^2+y^2
 
2sds=2xdx+2ydy
 
d[sds]=d[xdx]+d[ydy]\\
dsds+sd[ds]=dxdx+xd[dx]+dydy+yd[dy]\\
(ds)^2=(dx)^2+(dy)^2
 d[d[s]]=0","['calculus', 'derivatives', 'implicit-differentiation']"
83,"Find the values of $\theta$ for which the tangent line to the given curve is parallel to $x$ ,$y$ axis","Find the values of  for which the tangent line to the given curve is parallel to  , axis",\theta x y,"Given the curve $$r(\theta):=\sec\left(\theta\right)+a\cos\left(\theta\right) \tag{$a \in \mathbb R$}$$ Find the values of $\theta$ for which the tangent line to the curve is parallel to the $x$ and $y$ axis. The points for which the tangent line to the curve is parallel to the $y$ axis is given by : $$\frac{dx}{d\theta}=0$$ $$\left(\sec\left(\theta\right)\tan\left(\theta\right)-a\sin\left(\theta\right)\right)\cos\left(\theta\right)-\sin\left(\theta\right)\left(\sec\left(\theta\right)+a\cos\left(\theta\right)\right)=0$$ $$\tan\left(\theta\right)-a\sin\left(\theta\right)\cos\left(\theta\right)-\tan\left(\theta\right)-a\sin\left(\theta\right)\cos\left(\theta\right)=0$$ Assuming $a\ne0$ : $$\sin\left(\theta\right)\cos\left(\theta\right)=0$$ $$\theta=\frac{k\pi}{2}\tag{$k \in \mathbb Z$}$$ On the other hand duo the existence of $\sec$ function we see that the acceptable $\theta$ 's are : $$\theta=\frac{2k\pi}{2}=k\pi\tag{$k \in \mathbb Z$}$$ Implies the points $\left(x,y\right)=\left(r\cos\left(\theta\right),r\sin\left(\theta\right)\right)$ are all in the form: $$\left(\color{red}{\left(\sec\left(k\pi\right)+a\cos\left(k\pi\right)\right)\cos\left(k\pi\right)},\color{blue}{\left(\sec\left(k\pi\right)+a\cos\left(k\pi\right)\right)\sin\left(k\pi\right)}\right)$$ We see that the curves with $a\ne 0$ do have such tangent lines parallel to the $y$ axis.(Moreover for $a=0$ we have the line $x=1$ and the tangent line to the line (curve $r=\sec(\theta)$ ) parallel to the $y$ axis is the line itself.) The points for which the tangent line to the curve is parallel to the $x$ axis is given by : $$\frac{dy}{d\theta}=0$$ $$\left(\sec\left(\theta\right)\tan\left(\theta\right)-a\sin\left(\theta\right)\right)\sin\left(\theta\right)+\cos\left(\theta\right)\left(\sec\left(\theta\right)+a\cos\left(\theta\right)\right)=0$$ $$\frac{1}{\cos^{2}\left(\theta\right)}+2a\cos^{2}\left(\theta\right)-a=0$$ $$2a\cos^{4}\left(\theta\right)-a\cos^{2}\left(\theta\right)+1=0$$ $$\cos^{2}\left(\theta\right)=\frac{a\pm\sqrt{a^{2}-8a}}{4a}$$ Which is true whenever $$0\le\frac{a\pm\sqrt{a^{2}-8a}}{4a}\le1$$ Since $a^{2}-8a \ge 0$ ,we see that the curves with $0<a<8$ does not have such tangent lines parallel to the $x$ axis,moreover $\frac{a\pm\sqrt{a^{2}-8a}}{4a}$ is never between $0$ and $1$ and the inequality is not even sharp,so based on this information,such tangents lines parallel to the $x$ axis don't exist,but this is not true. So where was I wrong?","Given the curve Find the values of for which the tangent line to the curve is parallel to the and axis. The points for which the tangent line to the curve is parallel to the axis is given by : Assuming : On the other hand duo the existence of function we see that the acceptable 's are : Implies the points are all in the form: We see that the curves with do have such tangent lines parallel to the axis.(Moreover for we have the line and the tangent line to the line (curve ) parallel to the axis is the line itself.) The points for which the tangent line to the curve is parallel to the axis is given by : Which is true whenever Since ,we see that the curves with does not have such tangent lines parallel to the axis,moreover is never between and and the inequality is not even sharp,so based on this information,such tangents lines parallel to the axis don't exist,but this is not true. So where was I wrong?","r(\theta):=\sec\left(\theta\right)+a\cos\left(\theta\right) \tag{a \in \mathbb R} \theta x y y \frac{dx}{d\theta}=0 \left(\sec\left(\theta\right)\tan\left(\theta\right)-a\sin\left(\theta\right)\right)\cos\left(\theta\right)-\sin\left(\theta\right)\left(\sec\left(\theta\right)+a\cos\left(\theta\right)\right)=0 \tan\left(\theta\right)-a\sin\left(\theta\right)\cos\left(\theta\right)-\tan\left(\theta\right)-a\sin\left(\theta\right)\cos\left(\theta\right)=0 a\ne0 \sin\left(\theta\right)\cos\left(\theta\right)=0 \theta=\frac{k\pi}{2}\tag{k \in \mathbb Z} \sec \theta \theta=\frac{2k\pi}{2}=k\pi\tag{k \in \mathbb Z} \left(x,y\right)=\left(r\cos\left(\theta\right),r\sin\left(\theta\right)\right) \left(\color{red}{\left(\sec\left(k\pi\right)+a\cos\left(k\pi\right)\right)\cos\left(k\pi\right)},\color{blue}{\left(\sec\left(k\pi\right)+a\cos\left(k\pi\right)\right)\sin\left(k\pi\right)}\right) a\ne 0 y a=0 x=1 r=\sec(\theta) y x \frac{dy}{d\theta}=0 \left(\sec\left(\theta\right)\tan\left(\theta\right)-a\sin\left(\theta\right)\right)\sin\left(\theta\right)+\cos\left(\theta\right)\left(\sec\left(\theta\right)+a\cos\left(\theta\right)\right)=0 \frac{1}{\cos^{2}\left(\theta\right)}+2a\cos^{2}\left(\theta\right)-a=0 2a\cos^{4}\left(\theta\right)-a\cos^{2}\left(\theta\right)+1=0 \cos^{2}\left(\theta\right)=\frac{a\pm\sqrt{a^{2}-8a}}{4a} 0\le\frac{a\pm\sqrt{a^{2}-8a}}{4a}\le1 a^{2}-8a \ge 0 0<a<8 x \frac{a\pm\sqrt{a^{2}-8a}}{4a} 0 1 x",['derivatives']
84,Computing derivatives? [closed],Computing derivatives? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $u: \Bbb R^2\to\Bbb R$ and define $f(t)=u(\sin(t),\cos(t))$ How may I calculate $f'(t)$ ? Note: I am expecting the answer to include symbols like ${\partial u}/ {\partial x}$ Is there a law for such kind of derivatives?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let and define How may I calculate ? Note: I am expecting the answer to include symbols like Is there a law for such kind of derivatives?","u: \Bbb R^2\to\Bbb R f(t)=u(\sin(t),\cos(t)) f'(t) {\partial u}/ {\partial x}","['calculus', 'derivatives', 'partial-derivative']"
85,From derivatives to gradients in backpropagation,From derivatives to gradients in backpropagation,,"Consider the following explanation of backpropagation from Wikipedia: Given an input–output pair {\displaystyle (x,y)}(x,y), the loss is: $ C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{1}(W^{1}x)\cdots )))$ The derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a total derivative, evaluated at the value of the network (at each node) on the input $x$ : ${\frac {dC}{da^{L}}}\cdot {\frac {da^{L}}{dz^{L}}}\cdot {\frac {dz^{L}}{da^{L-1}}}\cdot {\frac {da^{L-1}}{dz^{L-1}}}\cdot {\frac {dz^{L-1}}{da^{L-2}}}\cdots {\frac {da^{1}}{dz^{1}}}\cdot {\frac {\partial z^{1}}{\partial x}}$ These terms are: the derivative of the loss function; the derivatives of the activation functions; and the matrices of weights: $\displaystyle {\frac {dC}{da^{L}}}\cdot (f^{L})'\cdot W^{L}\cdot (f^{L-1})'\cdot W^{L-1}\cdots  (f^{1})'\cdot W^{1}.$ So far so good. Then they say: The gradient $\nabla$ is the transpose of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed , but the entries are the same: $ \nabla _{x}C=(W^{1})^{T}\cdot (f^{1})'\cdots \cdot (W^{L-1})^{T}\cdot (f^{L-1})'\cdot (W^{L})^{T}\cdot (f^{L})'\cdot \nabla _{a^{L}}C.$ Why ? i.e. why does converting this to ""gradient form"" (not sure if that's what they actually did above) requires ""transposing matrices and reversing the ordering of multiplication""? In case it helps, I thought this article here (layout conventions in matrix calculus) may help, but I'm not able to cross-reference hat convention was used above, and why vectors / matrices are transposed, and why they chose to re-arrange the ordering of the terms","Consider the following explanation of backpropagation from Wikipedia: Given an input–output pair {\displaystyle (x,y)}(x,y), the loss is: The derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a total derivative, evaluated at the value of the network (at each node) on the input : These terms are: the derivative of the loss function; the derivatives of the activation functions; and the matrices of weights: So far so good. Then they say: The gradient is the transpose of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed , but the entries are the same: Why ? i.e. why does converting this to ""gradient form"" (not sure if that's what they actually did above) requires ""transposing matrices and reversing the ordering of multiplication""? In case it helps, I thought this article here (layout conventions in matrix calculus) may help, but I'm not able to cross-reference hat convention was used above, and why vectors / matrices are transposed, and why they chose to re-arrange the ordering of the terms"," C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{1}(W^{1}x)\cdots ))) x {\frac {dC}{da^{L}}}\cdot {\frac {da^{L}}{dz^{L}}}\cdot {\frac {dz^{L}}{da^{L-1}}}\cdot {\frac {da^{L-1}}{dz^{L-1}}}\cdot {\frac {dz^{L-1}}{da^{L-2}}}\cdots {\frac {da^{1}}{dz^{1}}}\cdot {\frac {\partial z^{1}}{\partial x}} \displaystyle {\frac {dC}{da^{L}}}\cdot (f^{L})'\cdot W^{L}\cdot (f^{L-1})'\cdot W^{L-1}\cdots 
(f^{1})'\cdot W^{1}. \nabla  \nabla _{x}C=(W^{1})^{T}\cdot (f^{1})'\cdots \cdot (W^{L-1})^{T}\cdot (f^{L-1})'\cdot (W^{L})^{T}\cdot (f^{L})'\cdot \nabla _{a^{L}}C.","['derivatives', 'matrix-calculus', 'chain-rule', 'neural-networks']"
86,Showing that $f(x) = \dfrac{x}{(2\ln x)^2}$ is an increasing function for $x \ge 8$,Showing that  is an increasing function for,f(x) = \dfrac{x}{(2\ln x)^2} x \ge 8,"I apologize for the repetition.  I asked a similar question here before. I was trying to generalize the result.  Does the following reasoning also work to show that $f(x) = \dfrac{x}{(2\ln x)^2}$ is an increasing function for $x \ge 8$ Please let me know if any of these steps are wrong: (1)  Using the quotient rule with $g(x) = x$ and $h(x) = (2\ln x)^2$ : $$f'(x) = \frac{g'(x)h(x) - g(x)h'(x)}{(h(x))^2}$$ (2) Using the exponent rule for derivatives with $s(x) = 2\ln x$ : $$h'(x) = (s(x)^2)' = s(x)^2\left(s'(x)\dfrac{2}{s(x)}\right) = 2s(x)s'(x)$$ (3) $s'(x) = \dfrac{2}{x}$ so that: $$h'(x) = \dfrac{8\ln(x)}{x}$$ (4) With $g'(x) = 1$ , it follows that: $$f'(x) = \dfrac{(2\ln x)^2 - \frac{8x\ln(x)}{x}}{(2\ln x)^4} = \dfrac{(2\ln x) - 4}{(2\ln x)^3}$$ (5)  It is increasing at $x\ge 8$ since: $$\dfrac{(2\ln 8) - 4}{(2\ln(8))^3} > 0.0022 > 0$$ Are these steps correct? Edit:  I changed step(5) to $x\ge 8$ since that is my goal. It looks like my result may be correct for $x=8$ but insufficient for $x \ge 8$ . Edit 2:  Made a fix based on John Omielan's comment.","I apologize for the repetition.  I asked a similar question here before. I was trying to generalize the result.  Does the following reasoning also work to show that is an increasing function for Please let me know if any of these steps are wrong: (1)  Using the quotient rule with and : (2) Using the exponent rule for derivatives with : (3) so that: (4) With , it follows that: (5)  It is increasing at since: Are these steps correct? Edit:  I changed step(5) to since that is my goal. It looks like my result may be correct for but insufficient for . Edit 2:  Made a fix based on John Omielan's comment.",f(x) = \dfrac{x}{(2\ln x)^2} x \ge 8 g(x) = x h(x) = (2\ln x)^2 f'(x) = \frac{g'(x)h(x) - g(x)h'(x)}{(h(x))^2} s(x) = 2\ln x h'(x) = (s(x)^2)' = s(x)^2\left(s'(x)\dfrac{2}{s(x)}\right) = 2s(x)s'(x) s'(x) = \dfrac{2}{x} h'(x) = \dfrac{8\ln(x)}{x} g'(x) = 1 f'(x) = \dfrac{(2\ln x)^2 - \frac{8x\ln(x)}{x}}{(2\ln x)^4} = \dfrac{(2\ln x) - 4}{(2\ln x)^3} x\ge 8 \dfrac{(2\ln 8) - 4}{(2\ln(8))^3} > 0.0022 > 0 x\ge 8 x=8 x \ge 8,"['derivatives', 'inequality', 'solution-verification']"
87,Show that $\frac{n!}{m!(n-m)!}p^m q^{n-m}$ is maximized at $p = m/n$.,Show that  is maximized at .,\frac{n!}{m!(n-m)!}p^m q^{n-m} p = m/n,"(Feller volume 1 p 184) (3.11) $a_0 = \frac{n!}{m!(n-m)!}p^m q^{n-m} \sim \frac1{\sqrt{2\pi npq}}$ . A straightforward differentiation shows that the middle term in (3.11) assumes its maximum when $p = m /n$ . Differentiating $\frac{n!}{m!(n-m)!}p^m q^{n-m}$ with respect to $p$ , I have that $-\frac{n!}{m!(n-m)!}m(n-m)p^{m-1} q^{n-m-1} = 0$ . But, I am not sure how to proceed from here. I would appreciate if you give some help.","(Feller volume 1 p 184) (3.11) . A straightforward differentiation shows that the middle term in (3.11) assumes its maximum when . Differentiating with respect to , I have that . But, I am not sure how to proceed from here. I would appreciate if you give some help.",a_0 = \frac{n!}{m!(n-m)!}p^m q^{n-m} \sim \frac1{\sqrt{2\pi npq}} p = m /n \frac{n!}{m!(n-m)!}p^m q^{n-m} p -\frac{n!}{m!(n-m)!}m(n-m)p^{m-1} q^{n-m-1} = 0,"['probability', 'derivatives', 'optimization']"
88,Exercise on the implicit function theorem.,Exercise on the implicit function theorem.,,"Prove that the equation $x^2+y^2+\sin y=0$ defines a unique function $y=f(x)$ in a neighbourhood of $(0,0)$ . Prove also that in $x=0$ there's a maxima for $f$ . I tried to do this exercise in two different ways and I'm asking you to tell me if I did something wrong, if the procedure is correct and if I could have done something better. To prove that the equation defines a unique function $y=f(x)$ I want to use the implicit function theorem with the function $F(x,y)=x^2+y^2+\sin y$ . We have that $F(0,0)=0$ and $F$ is clearly $C^{\infty}$ so we don't have regularity problems. It's easy to verify that $F_y(x,y)=2y+\cos y$ and so $F_y(0,0)=1 \neq 0$ . For the implicit function theorem we have that there exists a unique function $f \colon \mathbb{R} \to \mathbb{R}$ at least defined in a neighbourhood of $0$ such that $y=f(x)$ . We also remind that as $F$ is $C^{\infty}$ even $f$ is $C^{\infty}$ . Now we have the two different solutions for the second question, proving that in $x=0$ there is a maxima. We know from the implicit function theorem that $$f'(x)=-\frac{F_x(x,f(x))}{F_y(x,f(x))}$$ and so $$f'(x)=0$$ which tells us that $x=0$ is a stationary point for $f$ . We can now use the chain rule to say that $$f''(x)=-\frac{[F_{xx}(x,f(x))+F_{xy}(x,f(x))f'(x)]F_y(x,f(x))-F_x(x,f(x))[F_{yx}(x,f(x))+F_{yy}(x,f(x))}{F_y(x,f(x))^2}$$ from which $f''(0)<0$ and so $f$ has a maxima in $0$ . We know that $f$ is $C^{\infty}$ so we have that $$f(x)=f(0)+f'(0)x+\frac{f''(0)}{2}x^2+o(x^2)$$ and for what we observed in the first part we have $f(0)=f'(0)=0$ . We know also that $F(x,f(x))=0$ and so, by plugging in the last equation the expression we found for $f(x)$ we have that $$x^2+\frac{f''(0)}{2}x^2+o(x^2)=0 \implies f''(0)=-2$$ and so the thesis. Is it alright? Suggestions?","Prove that the equation defines a unique function in a neighbourhood of . Prove also that in there's a maxima for . I tried to do this exercise in two different ways and I'm asking you to tell me if I did something wrong, if the procedure is correct and if I could have done something better. To prove that the equation defines a unique function I want to use the implicit function theorem with the function . We have that and is clearly so we don't have regularity problems. It's easy to verify that and so . For the implicit function theorem we have that there exists a unique function at least defined in a neighbourhood of such that . We also remind that as is even is . Now we have the two different solutions for the second question, proving that in there is a maxima. We know from the implicit function theorem that and so which tells us that is a stationary point for . We can now use the chain rule to say that from which and so has a maxima in . We know that is so we have that and for what we observed in the first part we have . We know also that and so, by plugging in the last equation the expression we found for we have that and so the thesis. Is it alright? Suggestions?","x^2+y^2+\sin y=0 y=f(x) (0,0) x=0 f y=f(x) F(x,y)=x^2+y^2+\sin y F(0,0)=0 F C^{\infty} F_y(x,y)=2y+\cos y F_y(0,0)=1 \neq 0 f \colon \mathbb{R} \to \mathbb{R} 0 y=f(x) F C^{\infty} f C^{\infty} x=0 f'(x)=-\frac{F_x(x,f(x))}{F_y(x,f(x))} f'(x)=0 x=0 f f''(x)=-\frac{[F_{xx}(x,f(x))+F_{xy}(x,f(x))f'(x)]F_y(x,f(x))-F_x(x,f(x))[F_{yx}(x,f(x))+F_{yy}(x,f(x))}{F_y(x,f(x))^2} f''(0)<0 f 0 f C^{\infty} f(x)=f(0)+f'(0)x+\frac{f''(0)}{2}x^2+o(x^2) f(0)=f'(0)=0 F(x,f(x))=0 f(x) x^2+\frac{f''(0)}{2}x^2+o(x^2)=0 \implies f''(0)=-2","['real-analysis', 'calculus', 'derivatives', 'inverse', 'implicit-function-theorem']"
89,Integral of function over its derivative,Integral of function over its derivative,,"I am looking for a solution to the following integral: For a function $f(x)$ , $$ \int \frac{f}{f'}dx, $$ where $f'$ is the derivative of $f$ with respect to $x$ . It is clear that $\int \frac{f'}{f} dx = \log(f)$ , but I have no idea how to solve the above one. Any help would be greatly appreciated!! Cheers, Marc","I am looking for a solution to the following integral: For a function , where is the derivative of with respect to . It is clear that , but I have no idea how to solve the above one. Any help would be greatly appreciated!! Cheers, Marc","f(x)  \int \frac{f}{f'}dx,  f' f x \int \frac{f'}{f} dx = \log(f)","['integration', 'derivatives', 'fractions']"
90,Find the price that will maximize the profit,Find the price that will maximize the profit,,"Given the functions for price (p) and cost (C): $p=75-.1\sqrt{x}$ $C=30x+400$ Find the price that will maximize the profit. Solution: I will denote profit by $P$ . Then $P=R-C$ where $R$ is the revenue, and $R=xp$ , the price function multiplied by $x$ . We need to figure out what the function for profit is, find the value of $x$ that maximizes it, and then plug that value of $x$ into our price function. This will give us the price that maximizes the profit. So lets find the value of $x$ that maximizes $P$ by finding the critical points to $P$ : $P=xp-C=x(75-.1\sqrt{x})-(30x+400)$ $=75x-\frac{x^{\frac{3}{2}}}{10}-30x-400$ $=45x-\frac{x^{\frac{3}{2}}}{10}-400$ $\rightarrow P'=45-\frac{3}{2}\frac{x^{\frac{1}{2}}}{10}-0$ $=45-\frac{3x^{\frac{1}{2}}}{20}$ We are finding the critical points, so we have to set this to zero and solve for $x$ : $45-\frac{3x^{\frac{1}{2}}}{20}=0$ $\rightarrow 45=\frac{3x^{\frac{1}{2}}}{20}$ $\rightarrow \frac{900}{3}=x^{\frac{1}{2}}$ $\rightarrow 300^2=x$ Lets plug this value of $x$ into our price function $p$ : $p(300^2)=75-.1\sqrt{300^2}$ $=75-.1(300)$ $=75-30=45$ So the price that will maximizes the profit is $p=45$","Given the functions for price (p) and cost (C): Find the price that will maximize the profit. Solution: I will denote profit by . Then where is the revenue, and , the price function multiplied by . We need to figure out what the function for profit is, find the value of that maximizes it, and then plug that value of into our price function. This will give us the price that maximizes the profit. So lets find the value of that maximizes by finding the critical points to : We are finding the critical points, so we have to set this to zero and solve for : Lets plug this value of into our price function : So the price that will maximizes the profit is",p=75-.1\sqrt{x} C=30x+400 P P=R-C R R=xp x x x x P P P=xp-C=x(75-.1\sqrt{x})-(30x+400) =75x-\frac{x^{\frac{3}{2}}}{10}-30x-400 =45x-\frac{x^{\frac{3}{2}}}{10}-400 \rightarrow P'=45-\frac{3}{2}\frac{x^{\frac{1}{2}}}{10}-0 =45-\frac{3x^{\frac{1}{2}}}{20} x 45-\frac{3x^{\frac{1}{2}}}{20}=0 \rightarrow 45=\frac{3x^{\frac{1}{2}}}{20} \rightarrow \frac{900}{3}=x^{\frac{1}{2}} \rightarrow 300^2=x x p p(300^2)=75-.1\sqrt{300^2} =75-.1(300) =75-30=45 p=45,['calculus']
91,Conditions that ensure a convex function is log-concave?,Conditions that ensure a convex function is log-concave?,,"Suppose we have a convex function $g(x)$ (i'm particularly interested in $g(x)$ decreasing but that's not a requirement for an answer) Are there necessary or sufficient condition(s) that we can look at to examine whether $\log g(x)$ is concave? Looking at some examples: an affine function is convex and log-concave $x^2, x^4$ , etc are convex and log-concave $e^x$ is log concave $e^{x^2}$ is not log concave So a crude guess might be that a convex function is log-concave if it does not increase ""too fast"". But I don't know if this is correct, and even if it is, I don't know what ""too fast"" is (is $e^x$ the limit?)","Suppose we have a convex function (i'm particularly interested in decreasing but that's not a requirement for an answer) Are there necessary or sufficient condition(s) that we can look at to examine whether is concave? Looking at some examples: an affine function is convex and log-concave , etc are convex and log-concave is log concave is not log concave So a crude guess might be that a convex function is log-concave if it does not increase ""too fast"". But I don't know if this is correct, and even if it is, I don't know what ""too fast"" is (is the limit?)","g(x) g(x) \log g(x) x^2, x^4 e^x e^{x^2} e^x","['derivatives', 'convex-analysis', 'convex-optimization']"
92,Gradient of exponential scalar function of a matrix variable,Gradient of exponential scalar function of a matrix variable,,"Write the explicit formula of the gradient of $$ E(u)= \sum_{i=2}^{n-1} \sum_{j=2}^{m-1} \exp \left( - \frac{(u_{i+1,j} - u_{i,j-1})^2 + (u_{i,j+1} - u_{i-1,j})^2}{2 {\cal E} ^ 2}  \right) $$ with respect to $n \times m$ matrix $u$ , and where ${\cal E} > 0$ is a given   constant. Show all the steps of your calculations. Define fourth-order tensors $\Omega_1$ and $\Omega_2$ with components $$\eqalign{ \Omega_{1ijkl} &= \begin{cases} +1  &{\rm if}\quad(k = i+1)\;{\rm and}\;(l = j) \\ -1  &{\rm if}\quad(k = i)\;{\rm and}\;(l = j-1) \\ \;\;\,0  &{\rm otherwise}\ \ \end{cases} \\ }$$ and $$\eqalign{ \Omega_{2ijkl} &= \begin{cases} +1  &{\rm if}\quad(k = i)\;{\rm and}\;(l = j+1) \\ -1  &{\rm if}\quad(k = i-1)\;{\rm and}\;(l = j) \\ \;\;\,0  &{\rm otherwise}\ \ \end{cases} \\ }$$ and use it to define the following matrices $$\eqalign{ A_1 &= \Omega_1:U\quad&\implies A_{1ij}  = \sum_{k=1}^n\sum_{l=1}^m\Omega_{1ijkl} U_{kl} \\ A_2 &= \Omega_2:U\quad&\implies A_{2ij}  = \sum_{k=1}^n\sum_{l=1}^m\Omega_{2ijkl} U_{kl} \\ X_1 &= A_1\odot A_1\quad&\implies X_{1ij} = A_{1ij}A_{1ij} \\ X_2 &= A_2\odot A_2\quad&\implies X_{2ij} = A_{2ij}A_{2ij} \\ X_3 &= \frac{-1}{2 {\cal E} ^ 2} (X_1 + X_2) \quad&\implies X_{3ij} = \frac{-1}{2 {\cal E} ^ 2} ( X_{1ij} + X_{2ij}) \\ C &= \exp(X_3)\quad&\implies C_{ij} = \exp(X_{3ij}) \\ \quad&\implies dC = C\odot dX_3 \\ B &= (\frac{-1}{ {\cal E} ^ 2} ((A_1 : \Omega_1 ) + (A_2 : \Omega_2))) \\ }$$ where $(:)$ denotes the double-dot product, $(\odot)$ denotes the element-wise product, and all functions are applied element-wise on their arguments. $$\eqalign{ m &= {\tt1}:C \\ dm  &= {\tt1}:dC \\  &= {\tt1}:(C\odot dX_3) \\  &= C:dX_3 \\  &= C:(\frac{-1}{2 {\cal E} ^ 2} (2A_1 \odot dA_1 + 2A_2 \odot dA_2 ) ) \\  &= C:(\frac{-1}{ {\cal E} ^ 2} (A_1 \odot dA_1 + A_2 \odot dA_2 ) ) \\  &= C:(\frac{-1}{ {\cal E} ^ 2} (A_1 \odot \Omega_1:dU + A_2 \odot \Omega_2:dU ) ) \\  &= C:(\frac{-1}{ {\cal E} ^ 2} ((A_1 : \Omega_1 ) + (A_2 : \Omega_2))) \odot dU \\  &= C:B \odot dU \\  &= C \odot B : dU \\ \frac{\partial{\cal m}}{\partial U} &= C \odot B \\ }$$ Is it correct? If it is not correct why? Can anybody help me? Tnx","Write the explicit formula of the gradient of with respect to matrix , and where is a given   constant. Show all the steps of your calculations. Define fourth-order tensors and with components and and use it to define the following matrices where denotes the double-dot product, denotes the element-wise product, and all functions are applied element-wise on their arguments. Is it correct? If it is not correct why? Can anybody help me? Tnx"," E(u)= \sum_{i=2}^{n-1} \sum_{j=2}^{m-1} \exp \left( - \frac{(u_{i+1,j} - u_{i,j-1})^2 + (u_{i,j+1} - u_{i-1,j})^2}{2 {\cal E} ^ 2}  \right)  n \times m u {\cal E} > 0 \Omega_1 \Omega_2 \eqalign{
\Omega_{1ijkl} &= \begin{cases}
+1  &{\rm if}\quad(k = i+1)\;{\rm and}\;(l = j) \\
-1  &{\rm if}\quad(k = i)\;{\rm and}\;(l = j-1) \\
\;\;\,0  &{\rm otherwise}\ \
\end{cases} \\
} \eqalign{
\Omega_{2ijkl} &= \begin{cases}
+1  &{\rm if}\quad(k = i)\;{\rm and}\;(l = j+1) \\
-1  &{\rm if}\quad(k = i-1)\;{\rm and}\;(l = j) \\
\;\;\,0  &{\rm otherwise}\ \
\end{cases} \\
} \eqalign{
A_1 &= \Omega_1:U\quad&\implies A_{1ij}
 = \sum_{k=1}^n\sum_{l=1}^m\Omega_{1ijkl} U_{kl} \\
A_2 &= \Omega_2:U\quad&\implies A_{2ij}
 = \sum_{k=1}^n\sum_{l=1}^m\Omega_{2ijkl} U_{kl} \\
X_1 &= A_1\odot A_1\quad&\implies X_{1ij} = A_{1ij}A_{1ij} \\
X_2 &= A_2\odot A_2\quad&\implies X_{2ij} = A_{2ij}A_{2ij} \\
X_3 &= \frac{-1}{2 {\cal E} ^ 2} (X_1 + X_2) \quad&\implies X_{3ij} = \frac{-1}{2 {\cal E} ^ 2} ( X_{1ij} + X_{2ij}) \\
C &= \exp(X_3)\quad&\implies C_{ij} = \exp(X_{3ij}) \\
\quad&\implies dC = C\odot dX_3 \\
B &= (\frac{-1}{ {\cal E} ^ 2} ((A_1 : \Omega_1 ) + (A_2 : \Omega_2))) \\
} (:) (\odot) \eqalign{
m &= {\tt1}:C \\
dm
 &= {\tt1}:dC \\
 &= {\tt1}:(C\odot dX_3) \\
 &= C:dX_3 \\
 &= C:(\frac{-1}{2 {\cal E} ^ 2} (2A_1 \odot dA_1 + 2A_2 \odot dA_2 ) ) \\
 &= C:(\frac{-1}{ {\cal E} ^ 2} (A_1 \odot dA_1 + A_2 \odot dA_2 ) ) \\
 &= C:(\frac{-1}{ {\cal E} ^ 2} (A_1 \odot \Omega_1:dU + A_2 \odot \Omega_2:dU ) ) \\
 &= C:(\frac{-1}{ {\cal E} ^ 2} ((A_1 : \Omega_1 ) + (A_2 : \Omega_2))) \odot dU \\
 &= C:B \odot dU \\
 &= C \odot B : dU \\
\frac{\partial{\cal m}}{\partial U} &= C \odot B \\
}","['matrices', 'derivatives', 'matrix-calculus']"
93,Infinitely differentiable function with compact support on $\mathbb{R}^n$ with given properties,Infinitely differentiable function with compact support on  with given properties,\mathbb{R}^n,"For the following parts: $f(t)=\begin{cases}e^{-1/t^2}&t\neq0\\0&t=0\end{cases}$ $\quad(a)$ Show that $f\in C^\infty(\mathbb R)$ ; that is, $f$ is differentiable to all orders on $\mathbb R$ . $\quad(b)$ Use $f$ to define a function $g\in C^\infty(\mathbb R)$ whose support is $[a,b]$ , where $a<b$ . $\quad(c)$ Show how $g$ can be used to define a function $h\in C^\infty(\mathbb R^n)$ such that $$h(x)\begin{cases}=1&||x||\leq1\\\in[0,1]&1<||x||\leq2\\=0&2<||x||.\end{cases}$$ The part I am struggling with is part c.  I'm struggling to find a function which fits the given criteria for function values while also being infinitely differentiable with compact support.  I used the bump function as my answer to part b.  Any help would be greatly appreciated.","For the following parts: Show that ; that is, is differentiable to all orders on . Use to define a function whose support is , where . Show how can be used to define a function such that The part I am struggling with is part c.  I'm struggling to find a function which fits the given criteria for function values while also being infinitely differentiable with compact support.  I used the bump function as my answer to part b.  Any help would be greatly appreciated.","f(t)=\begin{cases}e^{-1/t^2}&t\neq0\\0&t=0\end{cases} \quad(a) f\in C^\infty(\mathbb R) f \mathbb R \quad(b) f g\in C^\infty(\mathbb R) [a,b] a<b \quad(c) g h\in C^\infty(\mathbb R^n) h(x)\begin{cases}=1&||x||\leq1\\\in[0,1]&1<||x||\leq2\\=0&2<||x||.\end{cases}","['real-analysis', 'derivatives']"
94,"Suppose $f(x) > 0, f'(x) > 0,$ and $f''(x) >0.$ Then $\lim_{x \to \infty} f(x) = \infty.$",Suppose  and  Then,"f(x) > 0, f'(x) > 0, f''(x) >0. \lim_{x \to \infty} f(x) = \infty.","Intuitively I see why this is true, in terms of a graph, but I can't seem to get a handle on a proof. I know that I need to show for every $M > 0$ there's a $d > 0$ such that whenever $x > d$ , $f(x) > M.$ I attempted proceeding by contradiction, but wasn't able to make much progress. I think I'm missing something.","Intuitively I see why this is true, in terms of a graph, but I can't seem to get a handle on a proof. I know that I need to show for every there's a such that whenever , I attempted proceeding by contradiction, but wasn't able to make much progress. I think I'm missing something.",M > 0 d > 0 x > d f(x) > M.,"['real-analysis', 'derivatives']"
95,Find the values of x where curve is concave,Find the values of x where curve is concave,,"This is a question found in an A-Level math book. Given a curve $$f(x) = \frac{\cos2x}{e^x}, 0\le x\le\pi$$ Determine the interval where $f(x)$ is concave. According to the math book: The function $f(x)$ is concave on a given interval if and only if $f''(x) \le 0$ for every $x$ in that interval. Also The point at whitch a curve changes from being concave to convex is called a point of inflection . A point of inflection is a point at which $f''(x)$ changes sign. Hence, to calculate the interval, I first calculates the second derivative of $f(x)$ : $$f''(x)=\frac{4\sin2x-3\cos2x}{e^x}$$ From this point and onwards, I can either use the definition of concavity or the definition of the point of inflection to find the interval. Using the definition of concavity Set $f''(x) \le 0$ : $$\therefore\frac{4\sin2x-3\cos2x}{e^x} \le 0 $$ $$\because e^x > 0$$ $$\therefore4\sin2x-3\cos2x \le 0$$ $$\therefore\tan2x \le 0.75$$ Hence, the answer is \begin{align} 0\le &x \le-0.322\\ \pi/4\le &x \le1.892\\ 0.75\pi\le &x\le\pi\\ \end{align} Using the definiton of point of inflection The point of inflection is at $x=0.322$ and $x=1.892$ . And from the first derivative, one can find a local minimal at $x=1.34$ meaning the curve between the two point of inflection is convex. Hence, the answer is \begin{align} 0\le &x \le-0.322\\ 1.892\le &x\le\pi\\ \end{align} Which one is correct? And why is the other incorrect? Update I think both approaches are valid, but a mistake is produced when I divide $sin$ by $cos$ in the first answer. Is it because $\cos2x < 0$ when $0.25\pi< x <0.75\pi$ , so $\le$ needs to be changed to $\ge$ ? i.e. \begin{cases} \tan2x \le 0.75, &0\le x<0.25\pi \text{    and    } 0.75< x<\pi\\ \tan2x \ge 0.75, &0.25\pi <x<0.75\pi\\ \end{cases} Solving this gives the second answer.","This is a question found in an A-Level math book. Given a curve Determine the interval where is concave. According to the math book: The function is concave on a given interval if and only if for every in that interval. Also The point at whitch a curve changes from being concave to convex is called a point of inflection . A point of inflection is a point at which changes sign. Hence, to calculate the interval, I first calculates the second derivative of : From this point and onwards, I can either use the definition of concavity or the definition of the point of inflection to find the interval. Using the definition of concavity Set : Hence, the answer is Using the definiton of point of inflection The point of inflection is at and . And from the first derivative, one can find a local minimal at meaning the curve between the two point of inflection is convex. Hence, the answer is Which one is correct? And why is the other incorrect? Update I think both approaches are valid, but a mistake is produced when I divide by in the first answer. Is it because when , so needs to be changed to ? i.e. Solving this gives the second answer.","f(x) = \frac{\cos2x}{e^x}, 0\le x\le\pi f(x) f(x) f''(x) \le 0 x f''(x) f(x) f''(x)=\frac{4\sin2x-3\cos2x}{e^x} f''(x) \le 0 \therefore\frac{4\sin2x-3\cos2x}{e^x} \le 0  \because e^x > 0 \therefore4\sin2x-3\cos2x \le 0 \therefore\tan2x \le 0.75 \begin{align}
0\le &x \le-0.322\\
\pi/4\le &x \le1.892\\
0.75\pi\le &x\le\pi\\
\end{align} x=0.322 x=1.892 x=1.34 \begin{align}
0\le &x \le-0.322\\
1.892\le &x\le\pi\\
\end{align} sin cos \cos2x < 0 0.25\pi< x <0.75\pi \le \ge \begin{cases}
\tan2x \le 0.75, &0\le x<0.25\pi \text{    and    } 0.75< x<\pi\\
\tan2x \ge 0.75, &0.25\pi <x<0.75\pi\\
\end{cases}","['derivatives', 'curves']"
96,"$f\in C^{3}[0,+\infty),f,f',f''>0,f'''\leq0.$How to prove $\limsup_{x\to\infty}\frac{ff''}{f'^{2}}\leq\frac{1}{2}.$",How to prove,"f\in C^{3}[0,+\infty),f,f',f''>0,f'''\leq0. \limsup_{x\to\infty}\frac{ff''}{f'^{2}}\leq\frac{1}{2}.","Suppose $f\in C^{3}[0,+\infty)$ and $f,f',f''>0,f'''\leq0.$ I want to prove $$\limsup_{x\to\infty}\frac{ff''}{f'^{2}}\leq\frac{1}{2}.$$ I have proved that $$\frac{f'}{f''}\geq\frac{x}{2},\frac{f}{f'}\geq\frac{x}{4}.$$ But this two inequalities seem no help to this problem (the second is opposite.) I think some other functions should be constructed,but I cannot construct them.Any help will be thanked.","Suppose and I want to prove I have proved that But this two inequalities seem no help to this problem (the second is opposite.) I think some other functions should be constructed,but I cannot construct them.Any help will be thanked.","f\in C^{3}[0,+\infty) f,f',f''>0,f'''\leq0. \limsup_{x\to\infty}\frac{ff''}{f'^{2}}\leq\frac{1}{2}. \frac{f'}{f''}\geq\frac{x}{2},\frac{f}{f'}\geq\frac{x}{4}.","['real-analysis', 'calculus', 'derivatives']"
97,Directional derivatives and unit vector,Directional derivatives and unit vector,,"I have an exercise in my textbook saying the following: Let $f : R^2 → R$ be defined by $z(x, y) = x^3 − 2x^2y + xy^2 + 1$ . Find the directional derivative at $(1, 2)$ along the direction towards $(4, 6)$ . While I understand the process and the evaluation process, I don't understand my teachers approach to finding the unit vector. According to the solution it is this: We have $x = (1, 2), y = (3, 4)$ because the direction is given by the difference of the two points. Yet, using the normal approach, I would get $u=\frac{(4,6)}{\sqrt{4^2+6^2}} = (\frac{2}{\sqrt{13}},\frac{3}{\sqrt{13}})$ Meaning his solution is $5$ , while mine is $\frac{4\sqrt{13}}{13}$ Who is right and if he is, why is my approach wrong and how does his' work?","I have an exercise in my textbook saying the following: Let be defined by . Find the directional derivative at along the direction towards . While I understand the process and the evaluation process, I don't understand my teachers approach to finding the unit vector. According to the solution it is this: We have because the direction is given by the difference of the two points. Yet, using the normal approach, I would get Meaning his solution is , while mine is Who is right and if he is, why is my approach wrong and how does his' work?","f : R^2 → R z(x, y) = x^3 − 2x^2y + xy^2 + 1 (1, 2) (4, 6) x = (1, 2), y = (3, 4) u=\frac{(4,6)}{\sqrt{4^2+6^2}} = (\frac{2}{\sqrt{13}},\frac{3}{\sqrt{13}}) 5 \frac{4\sqrt{13}}{13}","['derivatives', 'vectors']"
98,$n$-th derivative of $x^\alpha$ where $\alpha = m + 1/2$,-th derivative of  where,n x^\alpha \alpha = m + 1/2,"It is well-known that, for any real $\alpha$ and nonnegative integer $n$ $$ \frac{d^n x^\alpha}{dx^n} = \alpha(\alpha-1)\cdots(\alpha - n + 1) x^{\alpha - n}  $$ I just found out that the coefficient is known as a falling factorial $$ \alpha(\alpha-1)\cdots(\alpha - n + 1) =: (\alpha)_n $$ where $(\alpha)_n$ is Pochhammer's symbol. It seems that in the case $\alpha = 1/2$ and more generally $\alpha = m + 1/2$ where $m$ is a nonnegative integer, it is possible to express $(\alpha)_n$ with powers of 2 and standard (integer) factorials.  For example: $$ (1/2)_4 = \frac12\bigg(-\frac12\bigg)\bigg(-\frac32\bigg)\bigg(-\frac52\bigg) =- \frac{1\times3\times5}{2^4} = -\frac{5!}{2^4\times(2\times4)} =- \frac{5!}{2^4\times2^2\times 2!} $$ Would anyone know a good reference to such formulas?  I am guessing the cases $n>m$ and $n<m$ must be distinguished. Thanks! p.","It is well-known that, for any real and nonnegative integer I just found out that the coefficient is known as a falling factorial where is Pochhammer's symbol. It seems that in the case and more generally where is a nonnegative integer, it is possible to express with powers of 2 and standard (integer) factorials.  For example: Would anyone know a good reference to such formulas?  I am guessing the cases and must be distinguished. Thanks! p.",\alpha n  \frac{d^n x^\alpha}{dx^n} = \alpha(\alpha-1)\cdots(\alpha - n + 1) x^{\alpha - n}    \alpha(\alpha-1)\cdots(\alpha - n + 1) =: (\alpha)_n  (\alpha)_n \alpha = 1/2 \alpha = m + 1/2 m (\alpha)_n  (1/2)_4 = \frac12\bigg(-\frac12\bigg)\bigg(-\frac32\bigg)\bigg(-\frac52\bigg) =- \frac{1\times3\times5}{2^4} = -\frac{5!}{2^4\times(2\times4)} =- \frac{5!}{2^4\times2^2\times 2!}  n>m n<m,"['derivatives', 'radicals', 'factorial', 'pochhammer-symbol']"
99,I need help with this rate of change problem,I need help with this rate of change problem,,"The solid shown in the figure below consists of a cylinder of the radius (r) and height (h) and a hemispherical void of the radius (r). The dimensions at a given instant (t) are: $$h(t)=3t^2+2;r(t)=8-\frac{t^2}{4}$$ Find the rate of change of the volume (V) and surface area (S) of the solid at $t = 4$ seconds. State whether (V) and (S) are increasing, decreasing or neither. Note: The volume of Sphere = $\frac{4}{3}πr^3$ $(Ans: \frac{dV}{dt} = - 1105.84 (Deceasing))$ I tried this: $$\frac{dV}{dt}=\frac{\partial{V}}{\partial{r}}*\frac{dr}{dt}$$ and said: $$t=4$$ $$r(4)=4$$ $$\frac{\partial{V}}{\partial{r}}=4πr^2;\frac{dr}{dt}=\frac{-t}{2}$$ so: $$\frac{dV}{dt}=(4πr^2)*(\frac{-t}{2})$$ and the final result I get: $$\frac{dV}{dt}=128π$$ What am I doing wrong?","The solid shown in the figure below consists of a cylinder of the radius (r) and height (h) and a hemispherical void of the radius (r). The dimensions at a given instant (t) are: Find the rate of change of the volume (V) and surface area (S) of the solid at seconds. State whether (V) and (S) are increasing, decreasing or neither. Note: The volume of Sphere = I tried this: and said: so: and the final result I get: What am I doing wrong?",h(t)=3t^2+2;r(t)=8-\frac{t^2}{4} t = 4 \frac{4}{3}πr^3 (Ans: \frac{dV}{dt} = - 1105.84 (Deceasing)) \frac{dV}{dt}=\frac{\partial{V}}{\partial{r}}*\frac{dr}{dt} t=4 r(4)=4 \frac{\partial{V}}{\partial{r}}=4πr^2;\frac{dr}{dt}=\frac{-t}{2} \frac{dV}{dt}=(4πr^2)*(\frac{-t}{2}) \frac{dV}{dt}=128π,"['calculus', 'linear-algebra', 'derivatives', 'partial-derivative', 'volume']"
