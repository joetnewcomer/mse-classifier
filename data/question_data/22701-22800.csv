,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find all Equilateral Triangles with vertices on 3 straight lines,Find all Equilateral Triangles with vertices on 3 straight lines,,"I have 3 straight lines of equations: $$r:\begin{cases}x+y-z=0\\2x-y=0\end{cases}$$ $$s:\begin{cases}2x-z=1\\x-y=0\end{cases}$$ $$t:\begin{cases}x+z=0\\x+y-z=\frac{1}{2}\end{cases}$$ Find all equilater triangles with vertices on $r,s,t$. My idea was to equal the distance of the 3 straight line with generic points, but this way seems to be little complicated. It's like calculate: $$\lVert \overline{RS}\rVert=\lVert \overline{ST}\rVert=\lVert \overline{TR}\rVert$$ with $R\in r$, $S\in s$, $T\in t$, generic points on respective straight lines. Is there a simple way to do this?","I have 3 straight lines of equations: $$r:\begin{cases}x+y-z=0\\2x-y=0\end{cases}$$ $$s:\begin{cases}2x-z=1\\x-y=0\end{cases}$$ $$t:\begin{cases}x+z=0\\x+y-z=\frac{1}{2}\end{cases}$$ Find all equilater triangles with vertices on $r,s,t$. My idea was to equal the distance of the 3 straight line with generic points, but this way seems to be little complicated. It's like calculate: $$\lVert \overline{RS}\rVert=\lVert \overline{ST}\rVert=\lVert \overline{TR}\rVert$$ with $R\in r$, $S\in s$, $T\in t$, generic points on respective straight lines. Is there a simple way to do this?",,"['linear-algebra', 'geometry']"
1,Explain eigenvalues of a distance/cost matrix,Explain eigenvalues of a distance/cost matrix,,"Assume there are N countries. The cost of making a phone call from country $i$ to country $j$ is $C_{ij}$ . We know that all costs are non-negative. (Q1) Can you think of a verbal interpretation of eigenvalues of the matrix $C_{ij}$ ? (Q2) Does anything change, if we allow weights to be negative? I am aware that an eigendecomposition of a transformation $T$ is given by $T = R^{-1}DR$ , which means that, if a matrix were to be used as a transformation, it could be interpreted as rotation, scaling, and rotation back to the original basis. However, I'm not necessarily using my matrix to transform anything, so my intuition does not quite help","Assume there are N countries. The cost of making a phone call from country to country is . We know that all costs are non-negative. (Q1) Can you think of a verbal interpretation of eigenvalues of the matrix ? (Q2) Does anything change, if we allow weights to be negative? I am aware that an eigendecomposition of a transformation is given by , which means that, if a matrix were to be used as a transformation, it could be interpreted as rotation, scaling, and rotation back to the original basis. However, I'm not necessarily using my matrix to transform anything, so my intuition does not quite help",i j C_{ij} C_{ij} T T = R^{-1}DR,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
2,Matrix Representation of linear operators,Matrix Representation of linear operators,,"Let $S:\mathbb R^n \to \mathbb R^n$ be given by $S(v) = \alpha (v)$, for a fixed $\alpha \in \mathbb R, \alpha \neq 0$. Let $T:\mathbb R^n \to \mathbb R^n$ be a linear operator such that $B = (v_1, v_2, ..., v_n)$ is a set of linearly independent eigen vectors of $T$. Then prove or disprove that the matrix of $T-S$ with respect to $B$ is diagonal. Clearly, $B$ forms a basis for $\mathbb R^n$. This implies that $T$ is diagonalizable and hence the matrix representation of $T$ with respect to $B$ (i.e. $[T]_B$) is diagonal, with corresponding eigen values of $T$ (say $\lambda_1, \lambda_2, ..., \lambda_n$) as the diagonal entries. Also, $S$ is another linear operator on $\mathbb R^n$, by its definition in the problem. With respect to $B$, the matrix representation of $S$ (i.e. $[S]_B$) is also diagonal, with $\alpha$ as the diagonal entries. Therefore, the matrix representation of $T-S$ with respect to $B$ is again diagonal, with diagonal entries $\lambda_1-\alpha, \lambda_2-\alpha, ..., \lambda_n-\alpha$ because $(T-S)(v_i) = T(v_i) - S(v_i) = (\lambda_i - \alpha)(v_i)$, $v_i \in B, 1 \leq i \leq n$. Is my conclusion correct?","Let $S:\mathbb R^n \to \mathbb R^n$ be given by $S(v) = \alpha (v)$, for a fixed $\alpha \in \mathbb R, \alpha \neq 0$. Let $T:\mathbb R^n \to \mathbb R^n$ be a linear operator such that $B = (v_1, v_2, ..., v_n)$ is a set of linearly independent eigen vectors of $T$. Then prove or disprove that the matrix of $T-S$ with respect to $B$ is diagonal. Clearly, $B$ forms a basis for $\mathbb R^n$. This implies that $T$ is diagonalizable and hence the matrix representation of $T$ with respect to $B$ (i.e. $[T]_B$) is diagonal, with corresponding eigen values of $T$ (say $\lambda_1, \lambda_2, ..., \lambda_n$) as the diagonal entries. Also, $S$ is another linear operator on $\mathbb R^n$, by its definition in the problem. With respect to $B$, the matrix representation of $S$ (i.e. $[S]_B$) is also diagonal, with $\alpha$ as the diagonal entries. Therefore, the matrix representation of $T-S$ with respect to $B$ is again diagonal, with diagonal entries $\lambda_1-\alpha, \lambda_2-\alpha, ..., \lambda_n-\alpha$ because $(T-S)(v_i) = T(v_i) - S(v_i) = (\lambda_i - \alpha)(v_i)$, $v_i \in B, 1 \leq i \leq n$. Is my conclusion correct?",,"['linear-algebra', 'proof-verification', 'diagonalization']"
3,A linear map from matrices to matrices,A linear map from matrices to matrices,,"Let $M_n(k)$ denote the set of matrices over the field $k$, which could be viewed as a linear space of dimension $n^2$. Suppose $\varphi: M_n(k) \rightarrow M_n(k)$ is a nonzero linear map such that $$ \forall A, B \in M_n(k):~\varphi(AB) = \varphi(A) \varphi(B).$$  How to prove that there exists a nonsingular matrix $C \in GL_n(k)$ with $$\forall A \in M_n(k):~\varphi(A) = CAC^{-1}?$$","Let $M_n(k)$ denote the set of matrices over the field $k$, which could be viewed as a linear space of dimension $n^2$. Suppose $\varphi: M_n(k) \rightarrow M_n(k)$ is a nonzero linear map such that $$ \forall A, B \in M_n(k):~\varphi(AB) = \varphi(A) \varphi(B).$$  How to prove that there exists a nonsingular matrix $C \in GL_n(k)$ with $$\forall A \in M_n(k):~\varphi(A) = CAC^{-1}?$$",,"['linear-algebra', 'matrices']"
4,2 complex representations have the same character iff they are equivalent,2 complex representations have the same character iff they are equivalent,,"I have to prove the following statement: Let $\rho: G \to GL(V),  \tau: G \to GL(W)$ be representations of a   finite group $G$. Then $\rho$ and $\tau$ are equivalent if and only if   $\chi_\rho = \chi_\tau$ I can use the following theorems, and nothing else: If $(.|.): \mathbb{C}^G\times \mathbb{C}^G \to \mathbb{C}^G:  (\phi|\psi) \mapsto \frac{1}{|G|}\sum_{g \in G}  \phi(g)\overline{\psi(g)}$, then we have: If $\chi$ is a complex, irreducible character of a finite group $G$,   then $(\chi|\chi) = 1$ If $\chi_\rho, \chi_\tau$ are irreducible, complex characters and   $\rho$ not equivalent to $\tau$, then $(\chi_\rho|\chi_\tau) = 0$ If $\rho =  \bigoplus_{i=1}^n \rho_i $, and $\chi_\rho'$ is an   irreducible character of a certain representation $\tau$, then the   amount of irreducible components in the direct sum decomposition (as   written above) equivalent with $\rho'$, is given by   $(\chi_\rho|\chi_\rho')$ My attempt: $\boxed{\Rightarrow}$ Let $\rho$ and $\tau$ be equivalent representations, then for $g \in G$ and for a certain isomorphism $f: V \to W$ we have $$\rho_g = f \circ \tau_g \circ f^{-1}$$ and $Tr(\rho_g) = Tr([f \circ \tau_g \circ f^{-1}]) = Tr(\tau_g)$ by the cyclic property of trace. Hence, $\chi_\rho = \chi_\tau$ $\boxed{\Leftarrow}$ Here I'm stuck. Can someone hint me in the right direction here? I tried to use the second theorem I listed, but could not find a correct argument.","I have to prove the following statement: Let $\rho: G \to GL(V),  \tau: G \to GL(W)$ be representations of a   finite group $G$. Then $\rho$ and $\tau$ are equivalent if and only if   $\chi_\rho = \chi_\tau$ I can use the following theorems, and nothing else: If $(.|.): \mathbb{C}^G\times \mathbb{C}^G \to \mathbb{C}^G:  (\phi|\psi) \mapsto \frac{1}{|G|}\sum_{g \in G}  \phi(g)\overline{\psi(g)}$, then we have: If $\chi$ is a complex, irreducible character of a finite group $G$,   then $(\chi|\chi) = 1$ If $\chi_\rho, \chi_\tau$ are irreducible, complex characters and   $\rho$ not equivalent to $\tau$, then $(\chi_\rho|\chi_\tau) = 0$ If $\rho =  \bigoplus_{i=1}^n \rho_i $, and $\chi_\rho'$ is an   irreducible character of a certain representation $\tau$, then the   amount of irreducible components in the direct sum decomposition (as   written above) equivalent with $\rho'$, is given by   $(\chi_\rho|\chi_\rho')$ My attempt: $\boxed{\Rightarrow}$ Let $\rho$ and $\tau$ be equivalent representations, then for $g \in G$ and for a certain isomorphism $f: V \to W$ we have $$\rho_g = f \circ \tau_g \circ f^{-1}$$ and $Tr(\rho_g) = Tr([f \circ \tau_g \circ f^{-1}]) = Tr(\tau_g)$ by the cyclic property of trace. Hence, $\chi_\rho = \chi_\tau$ $\boxed{\Leftarrow}$ Here I'm stuck. Can someone hint me in the right direction here? I tried to use the second theorem I listed, but could not find a correct argument.",,"['linear-algebra', 'abstract-algebra']"
5,Solving $c A + I = (\det A) S$,Solving,c A + I = (\det A) S,"I have a simple matrix equation $$c A + I = (\det A) S$$ which seems linear (or perhaps quadratic) and involves the determinant of the matrix being solved for. Here, $c$ is a constant, and $S$ is a matrix. How could I solve for $A$, a symmetric matrix in $\mathbb{R}^{k\times k}$? An extremely efficient way of computing $A$ would also be fine. If it helps solve the problem, then know that $S$ corresponds to a (full-rank) covariance matrix $\frac{2}{n}X X^{\intercal}$, and $c$ corresponds to the norm of a mean-vector $\left\Vert \frac{1}{n}\overline{x}\right\Vert_2 = \frac{1}{n^2}\overline{x}^{\intercal}\overline{x}$. If there is no simple solution, a solution for the $k=2$ case is all I really need.","I have a simple matrix equation $$c A + I = (\det A) S$$ which seems linear (or perhaps quadratic) and involves the determinant of the matrix being solved for. Here, $c$ is a constant, and $S$ is a matrix. How could I solve for $A$, a symmetric matrix in $\mathbb{R}^{k\times k}$? An extremely efficient way of computing $A$ would also be fine. If it helps solve the problem, then know that $S$ corresponds to a (full-rank) covariance matrix $\frac{2}{n}X X^{\intercal}$, and $c$ corresponds to the norm of a mean-vector $\left\Vert \frac{1}{n}\overline{x}\right\Vert_2 = \frac{1}{n^2}\overline{x}^{\intercal}\overline{x}$. If there is no simple solution, a solution for the $k=2$ case is all I really need.",,"['linear-algebra', 'matrices', 'statistics', 'matrix-equations', 'matrix-calculus']"
6,Working out a concrete example of tensor product,Working out a concrete example of tensor product,,"From this entry in Wikipedia : The tensor product of two vector spaces $V$ and $W$ over a field $K$ is another vector space over $K$. It is denoted $V\otimes_K W$, or $V\otimes W$ when the underlying field $K$ is understood. If $V$ has a basis $e_1,\cdots,e_m$ and $W$ has a basis $f_1,\cdots,f_n$, then the tensor product $V\otimes W$ can be taken to be a vector space spanned by a basis consisting of all pairs $(e_i,f_j)$; each such basis element of $V\otimes W$ is denoted $e_i\otimes f_j$.  For any vectors $v=\sum_i v_ie_i\in V$ and $w=\sum_j w_j f_j\in W$ there is a corresponding product vector $v\otimes w$ in $V\otimes W$ given by $\sum_{ij}v_iw_j(e_i\otimes f_j)\in V\otimes W.$  This product operation $\otimes:V\times W \rightarrow V\otimes W$ is quickly verified to be bilinear. As an example, letting $V=W=\mathbb R^3$ (considered as a vector space over the field of real numbers) and considering the standard basis set $\{\hat x, \hat y,\hat z\}$ for each, the tensor product $V\otimes W$ is spanned by the nine basis vectors $\{\hat x \otimes \hat x,\hat x \otimes \hat y,\hat x \otimes \hat z,\hat y \otimes \hat x,\hat y \otimes \hat y, \hat y \otimes \hat z ,\hat z\otimes \hat x,,\hat z \otimes \hat y, \hat z \otimes \hat z  \}$ and is isomorphic to $\mathbb R^9$. For vectors $v=(1,2,3),w=(1,0,0)\in \mathbb R^3$ the tensor product $$\bbox[10px, border:2px solid red]{v\otimes w=  \hat x\otimes \hat x + 2\hat y\otimes \hat x+3\hat z\otimes \hat x}$$ The above definition relies on a choice of basis, which can not be done canonically for a generic vector space. However, any two choices of basis lead to isomorphic tensor product spaces (c.f. the universal property described below).  Alternatively, the tensor product may be defined in an expressly basis-independent manner as a quotient space of a free vector space over $V\times W$.  This approach is described below. QUESTION: If we decide on the standard Euclidean orthonormal basis, what is the final expression of the $v\otimes w$ product in the red boxed expression? Do we eventually get rid of the vector expressions with hats (as well as the $\otimes$ symbols) to get a number as per the (approximate) idea of a tensor as a map from $V\times W\rightarrow \mathbb R?$ What if we change the bases from orthonormal to $\large\begin{bmatrix}\tilde x\\\tilde y\\\tilde z\end{bmatrix}=\begin{bmatrix}3&4&-1\\0&3&7\\1&3&0.5\end{bmatrix}\begin{bmatrix}\hat x\\\hat y\\\hat z\end{bmatrix}?$","From this entry in Wikipedia : The tensor product of two vector spaces $V$ and $W$ over a field $K$ is another vector space over $K$. It is denoted $V\otimes_K W$, or $V\otimes W$ when the underlying field $K$ is understood. If $V$ has a basis $e_1,\cdots,e_m$ and $W$ has a basis $f_1,\cdots,f_n$, then the tensor product $V\otimes W$ can be taken to be a vector space spanned by a basis consisting of all pairs $(e_i,f_j)$; each such basis element of $V\otimes W$ is denoted $e_i\otimes f_j$.  For any vectors $v=\sum_i v_ie_i\in V$ and $w=\sum_j w_j f_j\in W$ there is a corresponding product vector $v\otimes w$ in $V\otimes W$ given by $\sum_{ij}v_iw_j(e_i\otimes f_j)\in V\otimes W.$  This product operation $\otimes:V\times W \rightarrow V\otimes W$ is quickly verified to be bilinear. As an example, letting $V=W=\mathbb R^3$ (considered as a vector space over the field of real numbers) and considering the standard basis set $\{\hat x, \hat y,\hat z\}$ for each, the tensor product $V\otimes W$ is spanned by the nine basis vectors $\{\hat x \otimes \hat x,\hat x \otimes \hat y,\hat x \otimes \hat z,\hat y \otimes \hat x,\hat y \otimes \hat y, \hat y \otimes \hat z ,\hat z\otimes \hat x,,\hat z \otimes \hat y, \hat z \otimes \hat z  \}$ and is isomorphic to $\mathbb R^9$. For vectors $v=(1,2,3),w=(1,0,0)\in \mathbb R^3$ the tensor product $$\bbox[10px, border:2px solid red]{v\otimes w=  \hat x\otimes \hat x + 2\hat y\otimes \hat x+3\hat z\otimes \hat x}$$ The above definition relies on a choice of basis, which can not be done canonically for a generic vector space. However, any two choices of basis lead to isomorphic tensor product spaces (c.f. the universal property described below).  Alternatively, the tensor product may be defined in an expressly basis-independent manner as a quotient space of a free vector space over $V\times W$.  This approach is described below. QUESTION: If we decide on the standard Euclidean orthonormal basis, what is the final expression of the $v\otimes w$ product in the red boxed expression? Do we eventually get rid of the vector expressions with hats (as well as the $\otimes$ symbols) to get a number as per the (approximate) idea of a tensor as a map from $V\times W\rightarrow \mathbb R?$ What if we change the bases from orthonormal to $\large\begin{bmatrix}\tilde x\\\tilde y\\\tilde z\end{bmatrix}=\begin{bmatrix}3&4&-1\\0&3&7\\1&3&0.5\end{bmatrix}\begin{bmatrix}\hat x\\\hat y\\\hat z\end{bmatrix}?$",,"['linear-algebra', 'abstract-algebra', 'tensor-products', 'tensors']"
7,Commuting analogue of the cross product,Commuting analogue of the cross product,,"Let $V$ be a finite-dimensional real inner product space. I am fairly sure that the following operation cannot exist, and would like a nice conceptual explanaition for its impossibility. Question: Can there exist a product $(x,y) \mapsto xy : V \times V \to V$ satisfying Bilinearity Commutativity Orthogonality meaning $xy$ is perpindicular to $x$ (and $y$ too, by commutativity) for all $x,y \in V$? Added: As was pointed out in the comments, I want to exclude the zero product. My thoughts: I didn't think of anything too useful, just that the orthogonality gives us that for fixed $x$, the map $y \mapsto xy$ is an antisymmetric transformation. So we have the identity $(xy,z) = -(y,xz)$. This lets us play around a bit, e.g. $$(xy,z) = (yx,z) = -(x,yz) = -(x,zy) = \ldots$$ but I didn't get anywhere this way.","Let $V$ be a finite-dimensional real inner product space. I am fairly sure that the following operation cannot exist, and would like a nice conceptual explanaition for its impossibility. Question: Can there exist a product $(x,y) \mapsto xy : V \times V \to V$ satisfying Bilinearity Commutativity Orthogonality meaning $xy$ is perpindicular to $x$ (and $y$ too, by commutativity) for all $x,y \in V$? Added: As was pointed out in the comments, I want to exclude the zero product. My thoughts: I didn't think of anything too useful, just that the orthogonality gives us that for fixed $x$, the map $y \mapsto xy$ is an antisymmetric transformation. So we have the identity $(xy,z) = -(y,xz)$. This lets us play around a bit, e.g. $$(xy,z) = (yx,z) = -(x,yz) = -(x,zy) = \ldots$$ but I didn't get anywhere this way.",,"['linear-algebra', 'multilinear-algebra']"
8,looking for motivation in teaching algebra,looking for motivation in teaching algebra,,"I am going to teach some grade 9 students about solving linear and quadratic equations. I am looking for a question from every day life (of a teenager) or a puzzle which is hard to solve without using algebra. There are of course loads out there in textbooks and the internet, but I haven't yet found one which is really intriguing and which could arouse the interest even of a student who has other things on his/her mind and who has a general dislike for school-mathematics. For example questions concerning the respective speeds, distances and time-periods of two vehicles with respect to each other are classic examples motivating linear equations, but they don't really seem to be relevant for real life (from the perspective of a teenager) nor are they particularly fascinating (at least for someone who isn't interested in mathematics anyway). I'd like to begin the subject with such a question and let the students work on it together for maybe half an hour or so (i.e. the question shouldn't be too easy to solve); hopefully this way they will see themselves how useful it can be to introduce variables. Any ideas?","I am going to teach some grade 9 students about solving linear and quadratic equations. I am looking for a question from every day life (of a teenager) or a puzzle which is hard to solve without using algebra. There are of course loads out there in textbooks and the internet, but I haven't yet found one which is really intriguing and which could arouse the interest even of a student who has other things on his/her mind and who has a general dislike for school-mathematics. For example questions concerning the respective speeds, distances and time-periods of two vehicles with respect to each other are classic examples motivating linear equations, but they don't really seem to be relevant for real life (from the perspective of a teenager) nor are they particularly fascinating (at least for someone who isn't interested in mathematics anyway). I'd like to begin the subject with such a question and let the students work on it together for maybe half an hour or so (i.e. the question shouldn't be too easy to solve); hopefully this way they will see themselves how useful it can be to introduce variables. Any ideas?",,"['linear-algebra', 'soft-question', 'education']"
9,Constructive proof that countably infinite-dimensional normed vector space is incomplete,Constructive proof that countably infinite-dimensional normed vector space is incomplete,,"I'm familiar with the standard proof that there exists no $\mathbb{N}$-dimensional Banach space based on Baire: Let $\{ v_{K} : k \in \mathbb{N} \}$ be a normalized basis for $V$, and let $W_{\ell} = \mathbb{R} v_{1} + \cdots + \mathbb{R} v_{\ell}$. Then $V = \cup_{\ell \in \mathbb{N}} W_{\ell}$. Moreover, $W_{\ell}$ is closed, so $X_{\ell} = V \setminus W_{\ell}$ is open. Also, $X_{\ell}$ is dense. To show it's dense, let $v = \sum _{k = 1}^{m} r_{k} v_{k} \in V$, where $r_{m} \neq 0$. If $m > \ell$, then $v \in V \setminus W_{\ell}$. If not, let $\epsilon > 0$; then $v + \frac{\epsilon}{2} v_{\ell + 1} \in X_{\ell} \cap N(v, \epsilon)$. Baire tells us that $\cap_{\ell \in \mathbb{N}} X_{\ell}$ is dense, but it is in fact empty, a contradiction. My question is: Can the argument be made constructively? Do we need BCT to show there exists no $\mathbb{N}$-dimensional Banach space? Can we instead make a diagonalization argument, i.e. construct a Cauchy sequence that doesn't converge in $\operatorname{span} \{ v_{k} : k \in \mathbb{N} \}$? Perhaps further, is the result dependent on BCT? That is, does the result imply BCT? Thanks in advance.","I'm familiar with the standard proof that there exists no $\mathbb{N}$-dimensional Banach space based on Baire: Let $\{ v_{K} : k \in \mathbb{N} \}$ be a normalized basis for $V$, and let $W_{\ell} = \mathbb{R} v_{1} + \cdots + \mathbb{R} v_{\ell}$. Then $V = \cup_{\ell \in \mathbb{N}} W_{\ell}$. Moreover, $W_{\ell}$ is closed, so $X_{\ell} = V \setminus W_{\ell}$ is open. Also, $X_{\ell}$ is dense. To show it's dense, let $v = \sum _{k = 1}^{m} r_{k} v_{k} \in V$, where $r_{m} \neq 0$. If $m > \ell$, then $v \in V \setminus W_{\ell}$. If not, let $\epsilon > 0$; then $v + \frac{\epsilon}{2} v_{\ell + 1} \in X_{\ell} \cap N(v, \epsilon)$. Baire tells us that $\cap_{\ell \in \mathbb{N}} X_{\ell}$ is dense, but it is in fact empty, a contradiction. My question is: Can the argument be made constructively? Do we need BCT to show there exists no $\mathbb{N}$-dimensional Banach space? Can we instead make a diagonalization argument, i.e. construct a Cauchy sequence that doesn't converge in $\operatorname{span} \{ v_{k} : k \in \mathbb{N} \}$? Perhaps further, is the result dependent on BCT? That is, does the result imply BCT? Thanks in advance.",,"['linear-algebra', 'banach-spaces', 'constructive-mathematics']"
10,List of matrix properties which are preserved after a change of basis,List of matrix properties which are preserved after a change of basis,,"Lately I encountered such a problem. Which of the properties of matrices are preserved after a change of basis ? ( orthogonal basis and square matrix are preferred in the first place ) Maybe it is a reasonable to make such a comprehensive list? Wikipedia doesn't provide even a short list in Change of basis though it gives some answers in Matrix_similarity . If someone knows however about such a list please give a pointer towards it. I need the list, not a bibliography. I would like additionally to divide properties into general and specific ones, where general properties are like symmetry, skew-symmetry, etc ( they can be or not associated with a given matrix - binary decision )  and  specific properties are like rank, determinant, trace etc.. ( they can be always characterized by a single number or a set of them ). So I will start. Also important is to list what is not preserved. What is preserved ? General properties: (if happens) Symmetry. Yes. Skew-Symmetry. Yes Orthogonality . Yes Diagonality (non-zero entries only on diagonal) No Positivness (all entries are positive) No . Specific properties: Trace. Yes . Rank. Yes . Determinant. Yes . What else can be added? ....","Lately I encountered such a problem. Which of the properties of matrices are preserved after a change of basis ? ( orthogonal basis and square matrix are preferred in the first place ) Maybe it is a reasonable to make such a comprehensive list? Wikipedia doesn't provide even a short list in Change of basis though it gives some answers in Matrix_similarity . If someone knows however about such a list please give a pointer towards it. I need the list, not a bibliography. I would like additionally to divide properties into general and specific ones, where general properties are like symmetry, skew-symmetry, etc ( they can be or not associated with a given matrix - binary decision )  and  specific properties are like rank, determinant, trace etc.. ( they can be always characterized by a single number or a set of them ). So I will start. Also important is to list what is not preserved. What is preserved ? General properties: (if happens) Symmetry. Yes. Skew-Symmetry. Yes Orthogonality . Yes Diagonality (non-zero entries only on diagonal) No Positivness (all entries are positive) No . Specific properties: Trace. Yes . Rank. Yes . Determinant. Yes . What else can be added? ....",,"['linear-algebra', 'big-list']"
11,Generalize Oddtown,Generalize Oddtown,,"Suppose we have a city with $n$ people. In this city there are $m$ clubs. The number of members in each club is not divisible by $s$ (for a given $s$), but the number of people in the intersection of each two clubs is. Show that $m<cn$, where $c$ is the number of (distinct) primes that divide $s$. My ideas so far: I can solve this when $s=p^k$ for $p$ prime. In this case, consider the n-coordinate vectors $v_1,v_2..v_m$ such that the $j$ entry of $v_i$ is $1$ if the person number $j$ belongs to the club number $i$ and else it's $0$. If these vectors are LD over the rationals, they are also LD over the integers, ie, we can find integers $r_i$ such that $\sum_i r_i v_i=0$. We can assume that there is a $j$ such that $r_j$ is not a multiple of $p$, otherwise we divide everyone for $p$ until that happens. Taking inner product with $v_j$ we have $\sum_i r_i v_iv_j=0$. We have $p^k \mid r_iv_iv_j$ for all $i$ different from $j$, therefore $p^k\mid r_jv_jv_j$ which implies $p \mid r_j$, contradiction. So, the vectors are LI and $m\leq n$. I think the general case will follow in a similar fashion, but for that we will need $cn$ coordinate vectors, and it's not obvious how to construct that.","Suppose we have a city with $n$ people. In this city there are $m$ clubs. The number of members in each club is not divisible by $s$ (for a given $s$), but the number of people in the intersection of each two clubs is. Show that $m<cn$, where $c$ is the number of (distinct) primes that divide $s$. My ideas so far: I can solve this when $s=p^k$ for $p$ prime. In this case, consider the n-coordinate vectors $v_1,v_2..v_m$ such that the $j$ entry of $v_i$ is $1$ if the person number $j$ belongs to the club number $i$ and else it's $0$. If these vectors are LD over the rationals, they are also LD over the integers, ie, we can find integers $r_i$ such that $\sum_i r_i v_i=0$. We can assume that there is a $j$ such that $r_j$ is not a multiple of $p$, otherwise we divide everyone for $p$ until that happens. Taking inner product with $v_j$ we have $\sum_i r_i v_iv_j=0$. We have $p^k \mid r_iv_iv_j$ for all $i$ different from $j$, therefore $p^k\mid r_jv_jv_j$ which implies $p \mid r_j$, contradiction. So, the vectors are LI and $m\leq n$. I think the general case will follow in a similar fashion, but for that we will need $cn$ coordinate vectors, and it's not obvious how to construct that.",,"['linear-algebra', 'combinatorics']"
12,Proof for $(U+W)^\perp=U^\perp \cap W^\perp$ if $U$ and $W$ are both subspaces.,Proof for  if  and  are both subspaces.,(U+W)^\perp=U^\perp \cap W^\perp U W,"In this case $(U+W)^\perp$ means the orthogonal complement of the union of $U$ and $W$ (regardless of repetition, is it correct?). The vectors inside the orthogonal complement has to be orthogonal to every vector in $(U+W)$ and so it must be composed of the orthogonal complement for each $U$ and $W$ . Digitally, $(U+W)^\perp=$ { $x$ in $ℝ^n|x \cdot f_u =0$ $\iff x \cdot f_w =0$ }=U $^\perp \cap W^\perp$ . But I felt the proof is either somewhat incorrect or incomplete, could anyone give some opinions?","In this case means the orthogonal complement of the union of and (regardless of repetition, is it correct?). The vectors inside the orthogonal complement has to be orthogonal to every vector in and so it must be composed of the orthogonal complement for each and . Digitally, { in }=U . But I felt the proof is either somewhat incorrect or incomplete, could anyone give some opinions?",(U+W)^\perp U W (U+W) U W (U+W)^\perp= x ℝ^n|x \cdot f_u =0 \iff x \cdot f_w =0 ^\perp \cap W^\perp,"['linear-algebra', 'inner-products']"
13,Show that there exists a diagonal matrix $B$ the diagonal entries of which are $±1$ such that $A + B$ is nonsingular.,Show that there exists a diagonal matrix  the diagonal entries of which are  such that  is nonsingular.,B ±1 A + B,Let $n$ be an odd positive integer let $A ∈ M_{n×n}(\mathbb{R})$. Show that there exists a diagonal matrix $B$ the diagonal entries of which are $±1$ such that $A + B$ is nonsingular. Any solutions/hints are greatly appreciated. I'm not sure how to do this.,Let $n$ be an odd positive integer let $A ∈ M_{n×n}(\mathbb{R})$. Show that there exists a diagonal matrix $B$ the diagonal entries of which are $±1$ such that $A + B$ is nonsingular. Any solutions/hints are greatly appreciated. I'm not sure how to do this.,,"['linear-algebra', 'matrices']"
14,(Homework) Prove that there is no T-invariant subspace $W_2$ such that $R^2$ = $W_1 \oplus W_2$,(Homework) Prove that there is no T-invariant subspace  such that  =,W_2 R^2 W_1 \oplus W_2,"Let T be a linear operator on $R^2$ defined by T(x,y) = (2x+y, 2y) and $W_1 = span{(1,0)}$ Prove that there is no T-invariant subspace $W_2$ such that $R^2 = W_1 \oplus W_2$ At first I showed that $W_1$ is an eigenspace and T is not diagonalizable so $R^2$ cannot be decomposed into the direct sum of the eigenspaces of T.  But this is not enough right since eigenspace is not the only T-invariant subspace. So I tried this way instead Suppose $R^2 = W_1 \oplus W_2$ .  Then dim$W_2$ = 1 that is $W_2$ is spanned by a single vector (a,b) and b is nonzero.  But T[(a,b)] = (2a+b, 2b) which is not in $W_2$.  Therefore $W_2$ is not T-invariant. Is my argument correct. We're using Friedberg in class and we've just covered eigenspace, invariant subspace and minimal polynomial. Thanks","Let T be a linear operator on $R^2$ defined by T(x,y) = (2x+y, 2y) and $W_1 = span{(1,0)}$ Prove that there is no T-invariant subspace $W_2$ such that $R^2 = W_1 \oplus W_2$ At first I showed that $W_1$ is an eigenspace and T is not diagonalizable so $R^2$ cannot be decomposed into the direct sum of the eigenspaces of T.  But this is not enough right since eigenspace is not the only T-invariant subspace. So I tried this way instead Suppose $R^2 = W_1 \oplus W_2$ .  Then dim$W_2$ = 1 that is $W_2$ is spanned by a single vector (a,b) and b is nonzero.  But T[(a,b)] = (2a+b, 2b) which is not in $W_2$.  Therefore $W_2$ is not T-invariant. Is my argument correct. We're using Friedberg in class and we've just covered eigenspace, invariant subspace and minimal polynomial. Thanks",,['linear-algebra']
15,"The subgroup of $GL_{n}(\mathbb{R})$ inside $B(1, 1)$ is $\lbrace I \rbrace$",The subgroup of  inside  is,"GL_{n}(\mathbb{R}) B(1, 1) \lbrace I \rbrace","Let $G \subset GL_{n}(\mathbb{R})$ be a subgroup such that $G \subset B$ where  $$B = \lbrace M \in \mathcal{M}_{n}(\mathbb{R}), ||M-I|| < 1\rbrace $$ Let $g \in G$. I'm asked to show: The only complex eigenvalue of $g$ is $1$ $g = I+N$ where $N$ is nilpotent and finally that $g=I$. I have shown that if $\lambda \in \mathbb{C}$ is an eigenvalue, then $|\lambda -1| < 1$. But I don't really know if it helps.. (note sure for the tags btw)","Let $G \subset GL_{n}(\mathbb{R})$ be a subgroup such that $G \subset B$ where  $$B = \lbrace M \in \mathcal{M}_{n}(\mathbb{R}), ||M-I|| < 1\rbrace $$ Let $g \in G$. I'm asked to show: The only complex eigenvalue of $g$ is $1$ $g = I+N$ where $N$ is nilpotent and finally that $g=I$. I have shown that if $\lambda \in \mathbb{C}$ is an eigenvalue, then $|\lambda -1| < 1$. But I don't really know if it helps.. (note sure for the tags btw)",,"['linear-algebra', 'operator-theory', 'spectral-theory']"
16,Is there a mathematical way to fold a $20 dollar bill for compactness?,Is there a mathematical way to fold a $20 dollar bill for compactness?,,"I had a strange thought. I used to carry a pill fob on my keys with an emergency $20 bill in it, before the whole thing got stolen. I always had some trouble fitting the bill inside the fob and barely managed it each time.  I would fold it a couple times then roll it really tight and put it into the tube before it expanded. Imagine that I don't have the restriction of needing a tube. I know to minimize volume a sphere is best, but you can't fold a paper into a sphere, something about Gaussian Curvature. Is there a math formula or technique where a bill can be folded maximally? Specifically to narrow it down I found this formula online: $$   L  = \frac{\pi t}{6}(2^n + 4)(2^n - 1) $$ Gallivan's formula gives the maximum number of times you can fold a bill going in the same direction.  There is also another formula she came up with for an accordion fold. If I have a finite number of folds that can be done and I know what that number is.  Then is there a way to figure out what the most compact method of folding the bill is without having to try all of them?","I had a strange thought. I used to carry a pill fob on my keys with an emergency $20 bill in it, before the whole thing got stolen. I always had some trouble fitting the bill inside the fob and barely managed it each time.  I would fold it a couple times then roll it really tight and put it into the tube before it expanded. Imagine that I don't have the restriction of needing a tube. I know to minimize volume a sphere is best, but you can't fold a paper into a sphere, something about Gaussian Curvature. Is there a math formula or technique where a bill can be folded maximally? Specifically to narrow it down I found this formula online: $$   L  = \frac{\pi t}{6}(2^n + 4)(2^n - 1) $$ Gallivan's formula gives the maximum number of times you can fold a bill going in the same direction.  There is also another formula she came up with for an accordion fold. If I have a finite number of folds that can be done and I know what that number is.  Then is there a way to figure out what the most compact method of folding the bill is without having to try all of them?",,"['linear-algebra', 'origami']"
17,Eigenvalues of checkerboard matrix,Eigenvalues of checkerboard matrix,,"I am trying to find the eigenvalues and eigenvectors of the following 4x4 ""checkerboard"" matrix: $$ \mathbf C = \begin{pmatrix} 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ \end{pmatrix}$$ Here is my approach: $$ \det(\mathbf C - \lambda \mathbf I) =  \begin{vmatrix} -\lambda & 1 & 0 & 1 \\ 1 & -\lambda  & 1 & 0 \\ 0 & 1 & -\lambda & 1 \\ 1 & 0 & 1 & -\lambda \end{vmatrix} = 0$$ From this I get a characteristic polynomial of: $$\lambda^4 - 4\lambda^2 = \lambda^2(\lambda - 2)(\lambda + 2)$$ So that would yield eigenvalues of 2, -2, 0, 0. For eigenvalue $\lambda_1 = 2$, I get eigenvector of: $$v_1 = \begin{pmatrix}1\\1\\1\\1\end{pmatrix}$$ For eigenvalue $\lambda_2 = -2$, I get eigenvector of: $$v_2 = \begin{pmatrix}-1\\1\\-1\\1\end{pmatrix}$$ This seems to check out. If I multiply $C v_1$, I get $\lambda_1 v_1 = 2 v_1$. If I multiply $C v_2$, I get $\lambda_2 v_2 = -2 v_2$. My professor disagrees. He states that the eigenvalues should be 2, 2, 0, 0, and that the two corresponding eigenvectors associated with eigenvalue of 2 are: $$v_1' = \begin{pmatrix}0\\1\\0\\1\end{pmatrix} \qquad v_2' = \begin{pmatrix}1\\0\\1\\0\end{pmatrix}$$ My professor further states: Try multiplying the vectors $v'_1$ and $v'_2$ by $\mathbf C$ and you’ll see that, indeed, they are both eigenvectors with eigenvalue 2. That still seems wrong to me. I did as he suggested and $$ C v'_1 = \begin{pmatrix}2\\0\\2\\0\end{pmatrix} \neq 2 v'_1 \qquad C v'_2 = \begin{pmatrix}0\\2\\0\\2\end{pmatrix}$$ I believe there is a subtle mistake here. $C v'_1 = 2 v'_2$ and $C v'_2 = 2 v'_1$ but that is irrelevant, correct? That does not make the eigenvalue of $\lambda = 2$ have a multiplicity of 2, does it?","I am trying to find the eigenvalues and eigenvectors of the following 4x4 ""checkerboard"" matrix: $$ \mathbf C = \begin{pmatrix} 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ \end{pmatrix}$$ Here is my approach: $$ \det(\mathbf C - \lambda \mathbf I) =  \begin{vmatrix} -\lambda & 1 & 0 & 1 \\ 1 & -\lambda  & 1 & 0 \\ 0 & 1 & -\lambda & 1 \\ 1 & 0 & 1 & -\lambda \end{vmatrix} = 0$$ From this I get a characteristic polynomial of: $$\lambda^4 - 4\lambda^2 = \lambda^2(\lambda - 2)(\lambda + 2)$$ So that would yield eigenvalues of 2, -2, 0, 0. For eigenvalue $\lambda_1 = 2$, I get eigenvector of: $$v_1 = \begin{pmatrix}1\\1\\1\\1\end{pmatrix}$$ For eigenvalue $\lambda_2 = -2$, I get eigenvector of: $$v_2 = \begin{pmatrix}-1\\1\\-1\\1\end{pmatrix}$$ This seems to check out. If I multiply $C v_1$, I get $\lambda_1 v_1 = 2 v_1$. If I multiply $C v_2$, I get $\lambda_2 v_2 = -2 v_2$. My professor disagrees. He states that the eigenvalues should be 2, 2, 0, 0, and that the two corresponding eigenvectors associated with eigenvalue of 2 are: $$v_1' = \begin{pmatrix}0\\1\\0\\1\end{pmatrix} \qquad v_2' = \begin{pmatrix}1\\0\\1\\0\end{pmatrix}$$ My professor further states: Try multiplying the vectors $v'_1$ and $v'_2$ by $\mathbf C$ and you’ll see that, indeed, they are both eigenvectors with eigenvalue 2. That still seems wrong to me. I did as he suggested and $$ C v'_1 = \begin{pmatrix}2\\0\\2\\0\end{pmatrix} \neq 2 v'_1 \qquad C v'_2 = \begin{pmatrix}0\\2\\0\\2\end{pmatrix}$$ I believe there is a subtle mistake here. $C v'_1 = 2 v'_2$ and $C v'_2 = 2 v'_1$ but that is irrelevant, correct? That does not make the eigenvalue of $\lambda = 2$ have a multiplicity of 2, does it?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
18,Extending a Chebyshev-polynomial determinant identity,Extending a Chebyshev-polynomial determinant identity,,"The following $n\times n$ determinant identity appears as eq. 19 on Mathworld's entry for the Chebyshev polynomials of the second kind : $$U_n(x)=\det{A_n(x)}\equiv \begin{vmatrix}2 x& 1 &  0 &\cdots &0\\ 1 & 2x &1  &\cdots &0 \\ 0 & 1 & 2x &\cdots &0\\0 & 0 & 1 & \ddots & \vdots \\ \vdots & \ddots & \ddots &\ddots & 1\\ 0 & 0 & \cdots & 1 & 2x\end{vmatrix}$$ as can be proven (for example) by expanding by minors to get the recurrence relation for $U_n(x)$. While working on a spectral problem for my research, I noticed that this result can be extended. Suppose we consider the determinant of $A_n(x)+t\,\mathbf{e}_k \mathbf{e}^T_k$ where $t$ is some free parameter and $k$ is some index. Then the $k$-th column vector may be expressed as $$\mathbf{e}_{k-1}+(2x+t)\mathbf{e}_k+\mathbf{e}_{k+1}= (\mathbf{e}_{k-1}+2x\,\mathbf{e}_k+\mathbf{e}_{k+1})+t\,\mathbf{e}_k.$$ Since the determinant is an linear function of its $k$-th column vector, we can expand in two terms: The first is just $\det{A_n(x)}=U_n(x)$, and for the second we can expand by minors to get a block diagonal matrix $\text{diag}(A_{k-1}(x),A_{n-k}(x))$ with determinant simplying to $U_{k-1}(x)U_{n-k}$. Putting these together gives the result $$\det{A_n(x)+t\,\mathbf{e}_k\mathbf{e}^T_k}=U_n(x)+t \, U_{k-1}(x)U_{n-k}(x).$$ One can similarly introduce a second parameter at a different row vector and compute the resulting determinant. Hence there should be a well-defined answer to the following question: Given a set of $n$ parameters $\{t_k\}$, express $\det{(A_n(x)+\text{diag}(\{t_k\}))}$ in terms of $\{U_n(x)\}.$","The following $n\times n$ determinant identity appears as eq. 19 on Mathworld's entry for the Chebyshev polynomials of the second kind : $$U_n(x)=\det{A_n(x)}\equiv \begin{vmatrix}2 x& 1 &  0 &\cdots &0\\ 1 & 2x &1  &\cdots &0 \\ 0 & 1 & 2x &\cdots &0\\0 & 0 & 1 & \ddots & \vdots \\ \vdots & \ddots & \ddots &\ddots & 1\\ 0 & 0 & \cdots & 1 & 2x\end{vmatrix}$$ as can be proven (for example) by expanding by minors to get the recurrence relation for $U_n(x)$. While working on a spectral problem for my research, I noticed that this result can be extended. Suppose we consider the determinant of $A_n(x)+t\,\mathbf{e}_k \mathbf{e}^T_k$ where $t$ is some free parameter and $k$ is some index. Then the $k$-th column vector may be expressed as $$\mathbf{e}_{k-1}+(2x+t)\mathbf{e}_k+\mathbf{e}_{k+1}= (\mathbf{e}_{k-1}+2x\,\mathbf{e}_k+\mathbf{e}_{k+1})+t\,\mathbf{e}_k.$$ Since the determinant is an linear function of its $k$-th column vector, we can expand in two terms: The first is just $\det{A_n(x)}=U_n(x)$, and for the second we can expand by minors to get a block diagonal matrix $\text{diag}(A_{k-1}(x),A_{n-k}(x))$ with determinant simplying to $U_{k-1}(x)U_{n-k}$. Putting these together gives the result $$\det{A_n(x)+t\,\mathbf{e}_k\mathbf{e}^T_k}=U_n(x)+t \, U_{k-1}(x)U_{n-k}(x).$$ One can similarly introduce a second parameter at a different row vector and compute the resulting determinant. Hence there should be a well-defined answer to the following question: Given a set of $n$ parameters $\{t_k\}$, express $\det{(A_n(x)+\text{diag}(\{t_k\}))}$ in terms of $\{U_n(x)\}.$",,"['linear-algebra', 'matrices', 'determinant', 'chebyshev-polynomials']"
19,"Given an irreducible representation, is there a *unique* unitary representation that it is equivalent to?","Given an irreducible representation, is there a *unique* unitary representation that it is equivalent to?",,"I might need help here in understanding my own question in places and please don't hesitate in asking for edits and clarifications. Background : A representation $\rho$ of a finite group $G$ is a group homomorphism from $G$ into $GL(V)$ for some vector space $V$. If $W$ is a subspace of $V$ invariant under $\rho(G)$, then $\rho_{\left.\right|W}$ is called a subrepresentation. In the usual way we can show that every representation is a direct sum of irreducible representations. If we endow $V$ with an inner product $\langle \cdot,\cdot\rangle$, then we can show that $$\langle u,v\rangle_\rho=\sum_{t\in G}\langle \rho(t)u,\rho(t)v\rangle$$ is another and furthermore, with respect this inner product, the operators $\rho(s)$ are unitary. Where $d_i$ is the dimension of the vector space $V_i$, where $\rho_i:G\rightarrow GL(V_i)$ it can be shown that the regular representation, which acts on the vector space $V_r=\mathbb{C}^{|G|}$, can be decomposed as $$V_r=d_1V_1\oplus d_2V_2\oplus\cdots d_nV_n,$$ where $\{\rho_i\}_{i\in[n]}$ are the unitary irreducible representations  $\rho_i:G\rightarrow GL(V_i)$ and so $$r:G\rightarrow GL\left(\bigoplus_{i\in[n]}d_iV_i\right),$$ and we write $$r=\bigoplus_{i\in[n]}d_i\rho_i.$$ Using the fact that the irreducible representations are equivalent to unitary ones allows us to show that the matrix elements of the unitary irreducible representations are orthogonal as elements of $F(G)$ with respect to the inner product $$\langle f,g\rangle=\frac{1}{|G|}\sum_{t\in G}\overline{f(t)}\cdot g(t),$$ and as there are $|G|$ of them, the matrix elements form an orthogonal basis of $F(G)$. Questions With respect to the canonical basis of $F(G)$, in what sense can I talk about the matrix elements of the unitary irreducible representations? Is there any natural way that the matrix elements of unitary irreducible representations form a basis? Example: I think I have for $G=\mathbb{Z}_3$, that there are unitary irreducible representations $\{\tau,\rho_1,\rho_2\}$ with matrix elements $$a_0:=\left(\begin{array}{c}1 \\ 1 \\ 1\end{array}\right),\,a_1:=\left(\begin{array}{c}1 \\ \omega \\ \omega_2\end{array}\right) \text{ and }\,a_2:=\left(\begin{array}{c}1 \\ \omega^2 \\ \omega\end{array}\right)\in F(\mathbb{Z_3}).$$ These are written with respect to the canonical basis of $F(\mathbb{Z_3})$ (although everything here is easier as $\mathbb{Z}_3$ is abelian). Consider, with respect to the canonical basis of $F(G)$,  $$f=\left(\begin{array}{c}1 \\ 2 \\ 3\end{array}\right).$$ With respect to the basis $\{a_0,a_1,a_2\}$ basis, I think $$f=\left(\begin{array}{c}2 \\ \frac{1}{\sqrt{3}}e^{5\pi i/6} \\ \frac{1}{\sqrt{3}}e^{7\pi i/6}\end{array}\right).$$","I might need help here in understanding my own question in places and please don't hesitate in asking for edits and clarifications. Background : A representation $\rho$ of a finite group $G$ is a group homomorphism from $G$ into $GL(V)$ for some vector space $V$. If $W$ is a subspace of $V$ invariant under $\rho(G)$, then $\rho_{\left.\right|W}$ is called a subrepresentation. In the usual way we can show that every representation is a direct sum of irreducible representations. If we endow $V$ with an inner product $\langle \cdot,\cdot\rangle$, then we can show that $$\langle u,v\rangle_\rho=\sum_{t\in G}\langle \rho(t)u,\rho(t)v\rangle$$ is another and furthermore, with respect this inner product, the operators $\rho(s)$ are unitary. Where $d_i$ is the dimension of the vector space $V_i$, where $\rho_i:G\rightarrow GL(V_i)$ it can be shown that the regular representation, which acts on the vector space $V_r=\mathbb{C}^{|G|}$, can be decomposed as $$V_r=d_1V_1\oplus d_2V_2\oplus\cdots d_nV_n,$$ where $\{\rho_i\}_{i\in[n]}$ are the unitary irreducible representations  $\rho_i:G\rightarrow GL(V_i)$ and so $$r:G\rightarrow GL\left(\bigoplus_{i\in[n]}d_iV_i\right),$$ and we write $$r=\bigoplus_{i\in[n]}d_i\rho_i.$$ Using the fact that the irreducible representations are equivalent to unitary ones allows us to show that the matrix elements of the unitary irreducible representations are orthogonal as elements of $F(G)$ with respect to the inner product $$\langle f,g\rangle=\frac{1}{|G|}\sum_{t\in G}\overline{f(t)}\cdot g(t),$$ and as there are $|G|$ of them, the matrix elements form an orthogonal basis of $F(G)$. Questions With respect to the canonical basis of $F(G)$, in what sense can I talk about the matrix elements of the unitary irreducible representations? Is there any natural way that the matrix elements of unitary irreducible representations form a basis? Example: I think I have for $G=\mathbb{Z}_3$, that there are unitary irreducible representations $\{\tau,\rho_1,\rho_2\}$ with matrix elements $$a_0:=\left(\begin{array}{c}1 \\ 1 \\ 1\end{array}\right),\,a_1:=\left(\begin{array}{c}1 \\ \omega \\ \omega_2\end{array}\right) \text{ and }\,a_2:=\left(\begin{array}{c}1 \\ \omega^2 \\ \omega\end{array}\right)\in F(\mathbb{Z_3}).$$ These are written with respect to the canonical basis of $F(\mathbb{Z_3})$ (although everything here is easier as $\mathbb{Z}_3$ is abelian). Consider, with respect to the canonical basis of $F(G)$,  $$f=\left(\begin{array}{c}1 \\ 2 \\ 3\end{array}\right).$$ With respect to the basis $\{a_0,a_1,a_2\}$ basis, I think $$f=\left(\begin{array}{c}2 \\ \frac{1}{\sqrt{3}}e^{5\pi i/6} \\ \frac{1}{\sqrt{3}}e^{7\pi i/6}\end{array}\right).$$",,"['linear-algebra', 'finite-groups', 'representation-theory']"
20,The set of differentiable functions such that $f'(2)=b$ is a linear subspace if and only if $b=0$??,The set of differentiable functions such that  is a linear subspace if and only if ??,f'(2)=b b=0,"Questions are in bold. The set of differentiable real-valued functions on (0,3) such that $f'(2)=b$ is a subspace of $(0,3)\to \mathbb R$ if and only if $b=0$ ($(0,3)\to \mathbb R$ denotes the set of functions from $(0,3)$ to $\mathbb{R}$) So that it forms a subspace we must have the $0$ element $0(x):x\mapsto0$, and this function have a zero derivative, hence $f'(2)=0$, thus $b=0$. Also why $f'(2)$ and not $f'(x)$? Right since we have a ""if and only if"" so that $f'(2)=0$ is not a necessary condition for that $f'(x)=0$ for $x\in(0,3)$? Also how can we regard a function as a vector? I understand now that a function from a nonempty interval to $\mathbb{R}$ is a vector in $\mathbb{R}^\infty$ since we can write $$f=\{(x_1,f(x_1)),(x_2,f(x_2)),...\}$$ but do we write $(\mathbb{R}^2)^\infty$ or just $\mathbb{R}^\infty$? Or do we have another notation? ($f$ is real valued)","Questions are in bold. The set of differentiable real-valued functions on (0,3) such that $f'(2)=b$ is a subspace of $(0,3)\to \mathbb R$ if and only if $b=0$ ($(0,3)\to \mathbb R$ denotes the set of functions from $(0,3)$ to $\mathbb{R}$) So that it forms a subspace we must have the $0$ element $0(x):x\mapsto0$, and this function have a zero derivative, hence $f'(2)=0$, thus $b=0$. Also why $f'(2)$ and not $f'(x)$? Right since we have a ""if and only if"" so that $f'(2)=0$ is not a necessary condition for that $f'(x)=0$ for $x\in(0,3)$? Also how can we regard a function as a vector? I understand now that a function from a nonempty interval to $\mathbb{R}$ is a vector in $\mathbb{R}^\infty$ since we can write $$f=\{(x_1,f(x_1)),(x_2,f(x_2)),...\}$$ but do we write $(\mathbb{R}^2)^\infty$ or just $\mathbb{R}^\infty$? Or do we have another notation? ($f$ is real valued)",,"['linear-algebra', 'vector-spaces']"
21,Invariance of determinant of metric tensor,Invariance of determinant of metric tensor,,"Given any 2-tensor on a Riemannian manifold $M$ equipped with metric $g,$ we have a coordinate-free definition of its trace: $$\operatorname{trace}(T)=g^{ij}T_{ij}= T_i^i.$$ In particular, we have $$\operatorname{trace}(g)= g^{ij}g_{ij}= \dim(M).$$ On the other hand, the determinant as I have seen it defined is simply the usual determinant of the matrix $(g)_{ij}$ written in some coordinate system. In this definition, the determinant depends on coordinates. (Example: the riemannian volume form is $\mu= \sqrt{\det(g_{ij})} \,dx^1 \wedge \dots \wedge dx^n$)). My (possibly naive) aquestion: is there a coordinate-invariant notion of a determinant, which makes the determinant of a metric $g$ a smooth function on the manifold?","Given any 2-tensor on a Riemannian manifold $M$ equipped with metric $g,$ we have a coordinate-free definition of its trace: $$\operatorname{trace}(T)=g^{ij}T_{ij}= T_i^i.$$ In particular, we have $$\operatorname{trace}(g)= g^{ij}g_{ij}= \dim(M).$$ On the other hand, the determinant as I have seen it defined is simply the usual determinant of the matrix $(g)_{ij}$ written in some coordinate system. In this definition, the determinant depends on coordinates. (Example: the riemannian volume form is $\mu= \sqrt{\det(g_{ij})} \,dx^1 \wedge \dots \wedge dx^n$)). My (possibly naive) aquestion: is there a coordinate-invariant notion of a determinant, which makes the determinant of a metric $g$ a smooth function on the manifold?",,"['linear-algebra', 'differential-geometry', 'riemannian-geometry']"
22,When is a pseudoinverse of a matrix non-negative?,When is a pseudoinverse of a matrix non-negative?,,"Consider a matrix $A \in \mathbb{R}^{n \times m}, n > m$ with independent columns and non-negative entries. Consider the oblique pseudo-inverse of $A$, i.e. the matrix $A^\dagger_B = (B ^\top A)^{-1} B^\top$ for some $B \in \mathbb{R}^{n \times m}$ such that the inverse $(B^\top A)^{-1}$ exists. Characterize the class of $B$s for which the matrix $A^\dagger_B$ has non-negative entries. Edit: There was a typo (now corrected) in the previous version of this post in the definition of $A^\dagger_B$. $A^\dagger_B$ fulfills the following Moore-Penrose conditions for any choice of $B$ such that the inverse $(B^\top A)^{-1}$ exists. (1) $A A^\dagger_B A = A$ (2) $A^\dagger_B A A^\dagger_B = A^\dagger_B$ (4) $A^\dagger_B A = I$ is symmetric The condition (3) $AA^\dagger_B$ is symmetric is only true for $B=A$, when $A^\dagger_B$ becomes the standard Moore-Penrose pseudoinverse. I am aware of the following result for the special case of $B = A$, i.e. the Moore-Penrose pseudoinverse: given a non-negative $A$, $A^\dagger$ is non-negative if and only if rows of $A$ are (up to a permutation) composed of rank-1 blocks orthogonal to each other (""Nonnegative Matrices in the Mathematical Sciences"", A. Berman, R. J. Plemmons, Theorem 5.2). I am wondering whether the class of matrices having the desired property is richer in the oblique case.","Consider a matrix $A \in \mathbb{R}^{n \times m}, n > m$ with independent columns and non-negative entries. Consider the oblique pseudo-inverse of $A$, i.e. the matrix $A^\dagger_B = (B ^\top A)^{-1} B^\top$ for some $B \in \mathbb{R}^{n \times m}$ such that the inverse $(B^\top A)^{-1}$ exists. Characterize the class of $B$s for which the matrix $A^\dagger_B$ has non-negative entries. Edit: There was a typo (now corrected) in the previous version of this post in the definition of $A^\dagger_B$. $A^\dagger_B$ fulfills the following Moore-Penrose conditions for any choice of $B$ such that the inverse $(B^\top A)^{-1}$ exists. (1) $A A^\dagger_B A = A$ (2) $A^\dagger_B A A^\dagger_B = A^\dagger_B$ (4) $A^\dagger_B A = I$ is symmetric The condition (3) $AA^\dagger_B$ is symmetric is only true for $B=A$, when $A^\dagger_B$ becomes the standard Moore-Penrose pseudoinverse. I am aware of the following result for the special case of $B = A$, i.e. the Moore-Penrose pseudoinverse: given a non-negative $A$, $A^\dagger$ is non-negative if and only if rows of $A$ are (up to a permutation) composed of rank-1 blocks orthogonal to each other (""Nonnegative Matrices in the Mathematical Sciences"", A. Berman, R. J. Plemmons, Theorem 5.2). I am wondering whether the class of matrices having the desired property is richer in the oblique case.",,"['linear-algebra', 'matrices', 'convex-analysis']"
23,Why is the nuclear norm called so?,Why is the nuclear norm called so?,,"A simple question. Why is the sum of the singular values of a matrix called its nuclear norm ? What is the origin of, and motivation for, this term? Apparently the term nucleus is sometimes used to refer to the kernel of a linear transformation, but that doesn't seem to have anything to do with singular values. To save you the effort, neither nucleus nor nuclear have entries in Earliest Known Uses of Some of the Words of Mathematics .","A simple question. Why is the sum of the singular values of a matrix called its nuclear norm ? What is the origin of, and motivation for, this term? Apparently the term nucleus is sometimes used to refer to the kernel of a linear transformation, but that doesn't seem to have anything to do with singular values. To save you the effort, neither nucleus nor nuclear have entries in Earliest Known Uses of Some of the Words of Mathematics .",,"['linear-algebra', 'matrices']"
24,General Steinitz exchange lemma,General Steinitz exchange lemma,,"Where can I find a proof of the following general Steinitz exchange lemma: Let $B$ be a basis of a vector space $V$, and $L\subset V$ be linearly independent. Then there is an injection $j:L\rightarrow B$ such that $L\cup(B\setminus j(L))$ is a disjoint union and a basis of the vector space $V$. Or can someone suggest a proof of this result? Probably using Zorn's lemma. Thank you!","Where can I find a proof of the following general Steinitz exchange lemma: Let $B$ be a basis of a vector space $V$, and $L\subset V$ be linearly independent. Then there is an injection $j:L\rightarrow B$ such that $L\cup(B\setminus j(L))$ is a disjoint union and a basis of the vector space $V$. Or can someone suggest a proof of this result? Probably using Zorn's lemma. Thank you!",,"['linear-algebra', 'abstract-algebra', 'reference-request', 'vector-spaces']"
25,Inverse of a diagonal matrix plus a constant,Inverse of a diagonal matrix plus a constant,,"I am looking for an efficient solution for inverting a matrix of the following form: $$D+aP$$ where $D$ is a (full-rank) diagonal matrix, $a$ is a constant, and $P$ is an all-ones matrix. Inverse of constant matrix plus diagonal matrix gives a solution to the special case where all diagonal entries of $D$ are the same. The Sherman-Morrison formula is also capable of providing a solution to this problem by setting appropriate $u$ and $v$, but it loses efficiency ($O(n^3)$ time complexity to compute) at high dimension. I am hoping to get a result in the same form so the space and time complexity are both $O(n)$.","I am looking for an efficient solution for inverting a matrix of the following form: $$D+aP$$ where $D$ is a (full-rank) diagonal matrix, $a$ is a constant, and $P$ is an all-ones matrix. Inverse of constant matrix plus diagonal matrix gives a solution to the special case where all diagonal entries of $D$ are the same. The Sherman-Morrison formula is also capable of providing a solution to this problem by setting appropriate $u$ and $v$, but it loses efficiency ($O(n^3)$ time complexity to compute) at high dimension. I am hoping to get a result in the same form so the space and time complexity are both $O(n)$.",,"['linear-algebra', 'inverse', 'numerical-linear-algebra']"
26,Eigenvalues of a $4\times 4$ parameters matrix,Eigenvalues of a  parameters matrix,4\times 4,"Let $a,b,c,d\in\Bbb{C}$ and $B =\begin{bmatrix}     a & b & c & d\\     d & a & b & c\\     c & d & a & b\\     b & c & d & a\\   \end{bmatrix}$ I know that $t=a+b+c+d$ is an eigenvalue because every row's sum is $t$ (and every column) and I also know that the sum of the eigenvalues is $4a$ (because it equals the trace of $B$). However I don't know how to continue.. and calculating the chracteristic polynomial of $B$ seems not very pleasant. Any ideas? thanks","Let $a,b,c,d\in\Bbb{C}$ and $B =\begin{bmatrix}     a & b & c & d\\     d & a & b & c\\     c & d & a & b\\     b & c & d & a\\   \end{bmatrix}$ I know that $t=a+b+c+d$ is an eigenvalue because every row's sum is $t$ (and every column) and I also know that the sum of the eigenvalues is $4a$ (because it equals the trace of $B$). However I don't know how to continue.. and calculating the chracteristic polynomial of $B$ seems not very pleasant. Any ideas? thanks",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
27,Analogy between linear basis and prime factoring,Analogy between linear basis and prime factoring,,"I recall learning that we can define linear systems such that any vector in the system can be represented as a weighted sum of basis vectors, as long as we have 'suitable' definitions for addition and multiplication operators (i.e. fulfilling certain properties.)  This turned out to be extremely useful, as we could prove general things about linear operations over vector spaces and apply them to a surprisingly wide array of systems with linear properties.  One of the interesting things about this was that you could define a series of unique real number coordinates for a given series of independent basis vectors in the system. It struck me the other day that there is an interesting, albeit slightly different pattern in the natural numbers.  Any natural number can be written as the product of natural powers of primes.  In a sense, it seems like the primes form a kind of 'basis' for the natural numbers, with the series of powers being a kind of 'coordinate'. Is there is a name for this pattern? If so, are there are other kinds of sets that can be decomposed as products of powers in this way, with similar generic results we can deduce for how these 'products of independent factors' behave? I apologize for the lack of clarity here, but my unfamiliarity with the terminology makes it difficult to describe.","I recall learning that we can define linear systems such that any vector in the system can be represented as a weighted sum of basis vectors, as long as we have 'suitable' definitions for addition and multiplication operators (i.e. fulfilling certain properties.)  This turned out to be extremely useful, as we could prove general things about linear operations over vector spaces and apply them to a surprisingly wide array of systems with linear properties.  One of the interesting things about this was that you could define a series of unique real number coordinates for a given series of independent basis vectors in the system. It struck me the other day that there is an interesting, albeit slightly different pattern in the natural numbers.  Any natural number can be written as the product of natural powers of primes.  In a sense, it seems like the primes form a kind of 'basis' for the natural numbers, with the series of powers being a kind of 'coordinate'. Is there is a name for this pattern? If so, are there are other kinds of sets that can be decomposed as products of powers in this way, with similar generic results we can deduce for how these 'products of independent factors' behave? I apologize for the lack of clarity here, but my unfamiliarity with the terminology makes it difficult to describe.",,"['linear-algebra', 'soft-question', 'prime-factorization']"
28,Steepest descent with quadratic form converges in 1 iteration,Steepest descent with quadratic form converges in 1 iteration,,"Well I'm stuck on an exercise given: The steepest descent method is applied to the quadratic form $$Q(\mathbf{x}) = \tfrac{1}{2}\mathbf{x}^TA\mathbf{x} - \mathbf{b}^T\mathbf{x} + c$$ where $A$ , $\mathbf{b}$ and $c$ , are matrix, vector and scalar constants. Under what condition on the matrix $A$ does the steepest descent method converge to the exact minimum in 1 iteration, from any initial condition $x_0$ ? [Hint: If the initial search line $\mathbf{x}_0 + \alpha \mathbf{d}_0$ includes the exact minimum of $Q(\mathbf{x})$ , then the method will converge in 1 iteration.] More specifically, some conditions about the matrix $A$ are asked, seeing this is a multiple choice question, the options: $A$ is a multiple of the identity matrix $A$ is diagonal $A$ is symmetric $A$ is positive definite $A$ has only positive eigenvalues $A$ is equal to $\mathbf{b}\mathbf{b}^T$ It never converges in 1 iteration It always converges in 1 iteration Now following the ""hint"", the exact solution is given by solving $Q'(\mathbf{x}) = 0$ : $$A\mathbf{\tilde{x}} - \mathbf{b} = 0$$ $$\mathbf{\tilde{x}} = A^{-1} \mathbf{b}$$ From the definition of the steepest descent method ( $x_{n+1} = x_n - \alpha f'(x_n)$ ): $$\mathbf{x}_1 = \mathbf{x}_0 + \alpha (A\mathbf{x}_0-\mathbf{b})$$ Now $\mathbf{x}_1 = \mathbf{\tilde{x}}$ has to be solved, or written out: $$\mathbf{x}_0 + \alpha (A\mathbf{x}_0-\mathbf{b}) = A^{-1} \mathbf{b} \qquad \forall\ \mathbf{x}_0,\alpha \in \mathbb{R}$$ Now I'm stuck, what can be said about $A$ ? Geometrically it feels like solution should be easy to be found, an example is a ""cone like surface"".","Well I'm stuck on an exercise given: The steepest descent method is applied to the quadratic form where , and , are matrix, vector and scalar constants. Under what condition on the matrix does the steepest descent method converge to the exact minimum in 1 iteration, from any initial condition ? [Hint: If the initial search line includes the exact minimum of , then the method will converge in 1 iteration.] More specifically, some conditions about the matrix are asked, seeing this is a multiple choice question, the options: is a multiple of the identity matrix is diagonal is symmetric is positive definite has only positive eigenvalues is equal to It never converges in 1 iteration It always converges in 1 iteration Now following the ""hint"", the exact solution is given by solving : From the definition of the steepest descent method ( ): Now has to be solved, or written out: Now I'm stuck, what can be said about ? Geometrically it feels like solution should be easy to be found, an example is a ""cone like surface"".","Q(\mathbf{x}) = \tfrac{1}{2}\mathbf{x}^TA\mathbf{x} - \mathbf{b}^T\mathbf{x} + c A \mathbf{b} c A x_0 \mathbf{x}_0 + \alpha \mathbf{d}_0 Q(\mathbf{x}) A A A A A A A \mathbf{b}\mathbf{b}^T Q'(\mathbf{x}) = 0 A\mathbf{\tilde{x}} - \mathbf{b} = 0 \mathbf{\tilde{x}} = A^{-1} \mathbf{b} x_{n+1} = x_n - \alpha f'(x_n) \mathbf{x}_1 = \mathbf{x}_0 + \alpha (A\mathbf{x}_0-\mathbf{b}) \mathbf{x}_1 = \mathbf{\tilde{x}} \mathbf{x}_0 + \alpha (A\mathbf{x}_0-\mathbf{b}) = A^{-1} \mathbf{b} \qquad \forall\ \mathbf{x}_0,\alpha \in \mathbb{R} A","['linear-algebra', 'optimization', 'numerical-methods', 'quadratic-forms']"
29,Artin exercise on free modules homomorphism,Artin exercise on free modules homomorphism,,"2.3 Let $A$ be the matrix of a homomorphism $\varphi:\mathbb Z^n\to\mathbb Z^m$ of free $\mathbb Z$ -modules. (a) Prove that $\varphi$ is injective if and only if the rank of $A$ , as a real matrix, is $n$ . (b) Prove that $\varphi$ is surjective if and only if the greatest common divisor of the determinants of the $m\times m$ minors of $A$ is $1$ . This is from Chapter 12 (1st Edition) of Artin. I believe it is Ch 14 in the 2nd Edition. I saw this question here earlier, but it appears to have been deleted. After coming across it in Artin, I tried it myself and am stuck, particularly on (b). I thought to extend $\varphi$ to a $\mathbb Q$ -homomorphism $\mathbb Q^n\to\mathbb Q^m$ and use Smith normal form, but I am having trouble making this work and not sure if it will. Does anyone have a direction that may be helpful? Also, is there a generalization of this exercise? For instance, a homomorphism $\varphi:R^n\to R^m$ with $R$ a Euclidean domain?","2.3 Let be the matrix of a homomorphism of free -modules. (a) Prove that is injective if and only if the rank of , as a real matrix, is . (b) Prove that is surjective if and only if the greatest common divisor of the determinants of the minors of is . This is from Chapter 12 (1st Edition) of Artin. I believe it is Ch 14 in the 2nd Edition. I saw this question here earlier, but it appears to have been deleted. After coming across it in Artin, I tried it myself and am stuck, particularly on (b). I thought to extend to a -homomorphism and use Smith normal form, but I am having trouble making this work and not sure if it will. Does anyone have a direction that may be helpful? Also, is there a generalization of this exercise? For instance, a homomorphism with a Euclidean domain?",A \varphi:\mathbb Z^n\to\mathbb Z^m \mathbb Z \varphi A n \varphi m\times m A 1 \varphi \mathbb Q \mathbb Q^n\to\mathbb Q^m \varphi:R^n\to R^m R,['linear-algebra']
30,How to prove this $A$ is an invertible matrix,How to prove this  is an invertible matrix,A,"let Symmetric matrix $A=(a_{ij})_{n\times n},n\ge 2$,and  $$\begin{cases} a_{jk}=j+k\cdot i&j< k\\ a_{jj}=2j\cdot(i+1) \end{cases}$$ where $i^2=-1$ show that :$A$ is Invertible matrix My idea: I want to  find this value $\det(A)=?$, or maybe  don't have closed form? when $n=2$,then  $$A=\begin{bmatrix} 2(i+1)&1+2i\\ 1+2i&4(i+1) \end{bmatrix}$$ so $$det(A)=8(i+1)^2-(1+2i)^2=16i-(4i^2+4i+1)=3+12i?$$ and  this problem is from china linear algebra problem book ,and this book most of problem is very hard.and this problem is last at this book. Thank you for you help me","let Symmetric matrix $A=(a_{ij})_{n\times n},n\ge 2$,and  $$\begin{cases} a_{jk}=j+k\cdot i&j< k\\ a_{jj}=2j\cdot(i+1) \end{cases}$$ where $i^2=-1$ show that :$A$ is Invertible matrix My idea: I want to  find this value $\det(A)=?$, or maybe  don't have closed form? when $n=2$,then  $$A=\begin{bmatrix} 2(i+1)&1+2i\\ 1+2i&4(i+1) \end{bmatrix}$$ so $$det(A)=8(i+1)^2-(1+2i)^2=16i-(4i^2+4i+1)=3+12i?$$ and  this problem is from china linear algebra problem book ,and this book most of problem is very hard.and this problem is last at this book. Thank you for you help me",,"['linear-algebra', 'matrices', 'determinant']"
31,"Smallest non-zero eigenvalue of a (0,1) matrix","Smallest non-zero eigenvalue of a (0,1) matrix",,What's the smallest absolute value possible of a non-zero eigenvalue of an $n$ by $n$ square matrix whose entries are either $0$ or $1$ (all operations are over $\mathbb{R}$)?,What's the smallest absolute value possible of a non-zero eigenvalue of an $n$ by $n$ square matrix whose entries are either $0$ or $1$ (all operations are over $\mathbb{R}$)?,,"['linear-algebra', 'matrices']"
32,How to prove that there exists a $B$ such that $A=B^2$,How to prove that there exists a  such that,B A=B^2,"If matrix $A$ is such that $A+A^T$ is a positive definite matrix, show that there exists a $B$ such that $A=B^2$, where $B+B^T$ is a positive definite matrix. My try: since $A+A^T$ is positive matrix, then exists $Q$ such $$Q^{-1}(A+A^T)Q=diag(a_{1},a_{2},\cdots,a_{n})$$ where $a_{i}>0$, $i=1,2,\cdots,n$ then I can't，Thank you","If matrix $A$ is such that $A+A^T$ is a positive definite matrix, show that there exists a $B$ such that $A=B^2$, where $B+B^T$ is a positive definite matrix. My try: since $A+A^T$ is positive matrix, then exists $Q$ such $$Q^{-1}(A+A^T)Q=diag(a_{1},a_{2},\cdots,a_{n})$$ where $a_{i}>0$, $i=1,2,\cdots,n$ then I can't，Thank you",,['linear-algebra']
33,Prove the operators $T+U$ and $U$ have the same eigenvalues where $T$ is nilpotent,Prove the operators  and  have the same eigenvalues where  is nilpotent,T+U U T,"Let $V$ be an $n$-dimensional vector space on $\mathbb{C}$, and $T$ a nilpotent operator on $V$.  Let $U$ be in $L(V)$ s.t. $UT = TU$. Prove that the operators $T+U$ and $U$ have the same eigenvalues. So far I know that the characteristic polynomial of $T$ is $x^n$, but I'm having a difficult time moving forward from there. Any help is appreciated.","Let $V$ be an $n$-dimensional vector space on $\mathbb{C}$, and $T$ a nilpotent operator on $V$.  Let $U$ be in $L(V)$ s.t. $UT = TU$. Prove that the operators $T+U$ and $U$ have the same eigenvalues. So far I know that the characteristic polynomial of $T$ is $x^n$, but I'm having a difficult time moving forward from there. Any help is appreciated.",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
34,"Obtain a basis of invertible matrices for $M_n(D)$, where $D$ is an integral domain","Obtain a basis of invertible matrices for , where  is an integral domain",M_n(D) D,"Let $D$ be an integral domain. Prove that $M_n(D)$ has a basis consisting of $n^2$ invertible matrices. Consider $M_n(D)$ as a $D$-module. Define invertible elements $V_{nn}=I_n$, $V_{n-1,n-1} = V_{nn} - E_{nn} + E_{n-1,n} + E_{n,n-1}$, $V_{ii}$ considered as permutation $(n,n-1,\dots,i+2,i+1)$ for $i=1,2,\dots,n-2$, and $V_{ij}=I_n+E_{ij}$ for $i\ne j$. I don't know how to prove they are linearly independent.","Let $D$ be an integral domain. Prove that $M_n(D)$ has a basis consisting of $n^2$ invertible matrices. Consider $M_n(D)$ as a $D$-module. Define invertible elements $V_{nn}=I_n$, $V_{n-1,n-1} = V_{nn} - E_{nn} + E_{n-1,n} + E_{n,n-1}$, $V_{ii}$ considered as permutation $(n,n-1,\dots,i+2,i+1)$ for $i=1,2,\dots,n-2$, and $V_{ij}=I_n+E_{ij}$ for $i\ne j$. I don't know how to prove they are linearly independent.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'modules']"
35,Finding the maximum number of subspaces of a vector space over finite field that satisfy these relations,Finding the maximum number of subspaces of a vector space over finite field that satisfy these relations,,"I have a question and I am stuck. I was wondering if anyone has a thought, before I start a brute-force search. For $q$ a prime number and $n =6$, let $\mathbb {F}_{q}^{n}$ be an $n$-dimensional vector space over $\mathbb{F}_{q}$.   Furthermore, let $ U_1, \dots, U_m$ be a family of $2$-dimensional subspaces of $\mathbb{F}_{q}^n$ such that $U_i \cap U_j = \{0\}$ and $\langle U_i, U_j \rangle \cap U_k = \{0\}$, for all $i, j, k \in \{1,\dots, n\}$, $i\neq j \neq k$. What is the biggest possible $m$? Thanks in advance.","I have a question and I am stuck. I was wondering if anyone has a thought, before I start a brute-force search. For $q$ a prime number and $n =6$, let $\mathbb {F}_{q}^{n}$ be an $n$-dimensional vector space over $\mathbb{F}_{q}$.   Furthermore, let $ U_1, \dots, U_m$ be a family of $2$-dimensional subspaces of $\mathbb{F}_{q}^n$ such that $U_i \cap U_j = \{0\}$ and $\langle U_i, U_j \rangle \cap U_k = \{0\}$, for all $i, j, k \in \{1,\dots, n\}$, $i\neq j \neq k$. What is the biggest possible $m$? Thanks in advance.",,"['linear-algebra', 'combinatorics', 'vector-spaces', 'finite-groups', 'finite-fields']"
36,Is Householder orthogonalization/QR practicable for non-Euclidean inner products?,Is Householder orthogonalization/QR practicable for non-Euclidean inner products?,,"The question Is there a variant of the Householder QR algorithm to orthonormalize a set of vectors with respect to an inner product if no orthonormal basis is known a priori? Background Let's assume that $A\in\mathbb{C}^{n,k}$ is a matrix with $\operatorname{rank}(A)=k$ . We can then compute the QR factorization $A=QR$ with $Q=[q_1,\ldots,q_k]\in\mathbb{C}^{n,k}$ and $R\in\mathbb{C}^{k,k}$ to obtain an orthonormal basis (with respect to the Euclidean inner product) $q_1,\ldots,q_k$ of $\operatorname{range}(A)$ , i.e. $q_i^*q_j = \delta_{ij}$ . If orthonormality is critical in the presence of round-off errors, the Householder QR algorithm is the method of choice (cf. Golub, Van Loan. Matrix Computations. ). Compared to the (modified) Gram-Schmidt algorithm the Householder QR algorithm benefits from the favorable round-off properties of Householder transformations. I am aware of the paper Trefethen, Householder triangularization of a quasimatrix, 2009 where the $L^2$ -inner product is used for a Householder QR algorithm. However, an orthonormal basis (the Legendre polynomials) is known in the paper by Trefethen. In my application only the Hermitian and positive definite matrix $B\in\mathbb{C}^{n,n}$ defining the inner product $\langle x,y \rangle_B = x^* B y$ is known. Because $n\gg k$ it's also infeasible to compute an eigen-decomposition of B. It seems to me that knowing one orthonormal basis (i.e. the standard basis) is crucial in the construction of the Householder QR algorithm but perhaps I'm wrong and there is a trick to let Householder QR work with arbitrary inner products?","The question Is there a variant of the Householder QR algorithm to orthonormalize a set of vectors with respect to an inner product if no orthonormal basis is known a priori? Background Let's assume that is a matrix with . We can then compute the QR factorization with and to obtain an orthonormal basis (with respect to the Euclidean inner product) of , i.e. . If orthonormality is critical in the presence of round-off errors, the Householder QR algorithm is the method of choice (cf. Golub, Van Loan. Matrix Computations. ). Compared to the (modified) Gram-Schmidt algorithm the Householder QR algorithm benefits from the favorable round-off properties of Householder transformations. I am aware of the paper Trefethen, Householder triangularization of a quasimatrix, 2009 where the -inner product is used for a Householder QR algorithm. However, an orthonormal basis (the Legendre polynomials) is known in the paper by Trefethen. In my application only the Hermitian and positive definite matrix defining the inner product is known. Because it's also infeasible to compute an eigen-decomposition of B. It seems to me that knowing one orthonormal basis (i.e. the standard basis) is crucial in the construction of the Householder QR algorithm but perhaps I'm wrong and there is a trick to let Householder QR work with arbitrary inner products?","A\in\mathbb{C}^{n,k} \operatorname{rank}(A)=k A=QR Q=[q_1,\ldots,q_k]\in\mathbb{C}^{n,k} R\in\mathbb{C}^{k,k} q_1,\ldots,q_k \operatorname{range}(A) q_i^*q_j = \delta_{ij} L^2 B\in\mathbb{C}^{n,n} \langle x,y \rangle_B = x^* B y n\gg k","['linear-algebra', 'matrices', 'functional-analysis', 'numerical-methods', 'numerical-linear-algebra']"
37,Complement of all-one vector in binary vector space,Complement of all-one vector in binary vector space,,"Let $V$ be a k-dimensional subspace of $(\mathbb{F}_2)^n$, such that vector $\vec{j}=(1,1,...,1) \in V$. Standard linear algebra shows that it is possible to find a $(k-1)$-dimensional space $W$ such that $\langle \vec{j},W\rangle=V$. However, this choice is not unique. Is there any ""canonical"" choice for $W$, i.e. one that does not depend on making certain positions special, like there is the orthogonal complement in the $\mathbb{R}$-case? In case it matters, my parameters are $n=2058$, $k=52$ and all vectors in $V$ have even weight (so orthogonal complement is pointless). Edit: clarification of what I mean with ""without making certain positions special"". Consider the action of $S_n$ as a permutation group on the positions of the vector space, and let $G\le S_n$ be the stabilizer of $V$ in that action. Then $G$ should also stabilize $W$, as in the orthogonal complement case. (Note that $G$ automatically stabilizes $\vec{j}$.) I'm unsure if such spaces exist in general, so a feasible algorithm to find one is also appreciated.","Let $V$ be a k-dimensional subspace of $(\mathbb{F}_2)^n$, such that vector $\vec{j}=(1,1,...,1) \in V$. Standard linear algebra shows that it is possible to find a $(k-1)$-dimensional space $W$ such that $\langle \vec{j},W\rangle=V$. However, this choice is not unique. Is there any ""canonical"" choice for $W$, i.e. one that does not depend on making certain positions special, like there is the orthogonal complement in the $\mathbb{R}$-case? In case it matters, my parameters are $n=2058$, $k=52$ and all vectors in $V$ have even weight (so orthogonal complement is pointless). Edit: clarification of what I mean with ""without making certain positions special"". Consider the action of $S_n$ as a permutation group on the positions of the vector space, and let $G\le S_n$ be the stabilizer of $V$ in that action. Then $G$ should also stabilize $W$, as in the orthogonal complement case. (Note that $G$ automatically stabilizes $\vec{j}$.) I'm unsure if such spaces exist in general, so a feasible algorithm to find one is also appreciated.",,"['linear-algebra', 'vector-spaces', 'binary']"
38,Fredholm Equations,Fredholm Equations,,"I have the following problem to solve $$\phi (x)=\lambda \int_{o}^{\pi }\cos(2x+y)\phi (y) dy+ \sin x$$ following the instructions from the following link early to conclude that: $$\phi (x)=\lambda \int_{o}^{\pi }\cos(2x+y)\phi (y) dy+\sin x$$ $$=\lambda \int_{o}^{\pi }\cos^2(x)\cos(y)-\sin^2(x)\cos(y)-2\sin(x)\cos(x)\sin(y)\phi (y) dy+ \sin x$$ then start to calculate $a_{11}, a_{12},a_{13},a_{21},a_{22},a_{23},a_{31},a_{32},a_{33}$, such that (these will be the elements of the matrix A): $$a_{1}=\cos^2(x);a_{2}=-\sin^2(x) ;a _{3}=-2\sin(x)\cos(x)$$ $$b_{1}=\cos(y);b_{2}=\cos(y) ;b_{3}=\sin(y)$$ thus calculated: $$a_{11}=\int_{o}^{\pi }b_{1}(y)a_{1}(y)dy$$ $$a_{12}=\int_{o}^{\pi }b_{1}(y)a_{2}(y)dy$$ $$a_{13}=\int_{o}^{\pi }b_{1}(y)a_{3}(y)dy$$ calculating and so on until all $a_{nm}$ with $n =1,2,3$ and $m=1,2,3$. After calculations we obtain the matrix $$A=\begin{bmatrix} a_{11} &a_{12}  & a_{13}\\  a _{21}&a_{22}  & a_{23}\\  a_{31} &a_{32}  & a_{33}  \end{bmatrix}$$ making: $\;(I-\lambda A)$, and calculating the determinant originated by $\;(I-\lambda A)$, get all values ​​of $\lambda$. The solutions they have there is only one $\lambda=3/2\sqrt{2}$ and $\lambda=-3/2\sqrt{2}$ But never get to that result. I appreciate any and all help. I do not even know if I was wrong in whole or in calculating the determinant of the matrix.","I have the following problem to solve $$\phi (x)=\lambda \int_{o}^{\pi }\cos(2x+y)\phi (y) dy+ \sin x$$ following the instructions from the following link early to conclude that: $$\phi (x)=\lambda \int_{o}^{\pi }\cos(2x+y)\phi (y) dy+\sin x$$ $$=\lambda \int_{o}^{\pi }\cos^2(x)\cos(y)-\sin^2(x)\cos(y)-2\sin(x)\cos(x)\sin(y)\phi (y) dy+ \sin x$$ then start to calculate $a_{11}, a_{12},a_{13},a_{21},a_{22},a_{23},a_{31},a_{32},a_{33}$, such that (these will be the elements of the matrix A): $$a_{1}=\cos^2(x);a_{2}=-\sin^2(x) ;a _{3}=-2\sin(x)\cos(x)$$ $$b_{1}=\cos(y);b_{2}=\cos(y) ;b_{3}=\sin(y)$$ thus calculated: $$a_{11}=\int_{o}^{\pi }b_{1}(y)a_{1}(y)dy$$ $$a_{12}=\int_{o}^{\pi }b_{1}(y)a_{2}(y)dy$$ $$a_{13}=\int_{o}^{\pi }b_{1}(y)a_{3}(y)dy$$ calculating and so on until all $a_{nm}$ with $n =1,2,3$ and $m=1,2,3$. After calculations we obtain the matrix $$A=\begin{bmatrix} a_{11} &a_{12}  & a_{13}\\  a _{21}&a_{22}  & a_{23}\\  a_{31} &a_{32}  & a_{33}  \end{bmatrix}$$ making: $\;(I-\lambda A)$, and calculating the determinant originated by $\;(I-\lambda A)$, get all values ​​of $\lambda$. The solutions they have there is only one $\lambda=3/2\sqrt{2}$ and $\lambda=-3/2\sqrt{2}$ But never get to that result. I appreciate any and all help. I do not even know if I was wrong in whole or in calculating the determinant of the matrix.",,"['linear-algebra', 'functional-analysis', 'integral-equations']"
39,Find eigenspaces using ruler and compasses,Find eigenspaces using ruler and compasses,,"I think this is an interesting question: In the 2-dimensional real vector space, we are given a linear transformation $f$. Suppose we already know the images of the standard bases, say $f(e_1),f(e_2)$. Of course, in some cases the eigenspace might not exist. But assuming the eigenspaces exist, how can we use ruler and compasses to determine them ?","I think this is an interesting question: In the 2-dimensional real vector space, we are given a linear transformation $f$. Suppose we already know the images of the standard bases, say $f(e_1),f(e_2)$. Of course, in some cases the eigenspace might not exist. But assuming the eigenspaces exist, how can we use ruler and compasses to determine them ?",,"['linear-algebra', 'trigonometry', 'euclidean-geometry']"
40,Bounding the number of nonzero coefficients in a conic combination,Bounding the number of nonzero coefficients in a conic combination,,"I'm looking for a proof for the following statement in order to understand a proof about integer programming I'm reading. Given vectors $x_1, \ldots, x_s \in \mathbb R^n$, nonnegative coefficients $\lambda_1, \ldots, \lambda_s \in \mathbb R_{\geq 0}$ and a vector $v = \sum_{i=1}^s \lambda_i x_i$, then there exist coefficients $\lambda_1', \ldots, \lambda_s' \in \mathbb R_{\geq 0}$ of which at most $n$ are nonzero with $v = \sum_{i=1}^s \lambda_i' x_i$. In other words: given a finite set $B$ of real vectors, every vector in the polyhedral cone generated by $B$ can be written as a conical combination of at most $n$ vectors of $B$.","I'm looking for a proof for the following statement in order to understand a proof about integer programming I'm reading. Given vectors $x_1, \ldots, x_s \in \mathbb R^n$, nonnegative coefficients $\lambda_1, \ldots, \lambda_s \in \mathbb R_{\geq 0}$ and a vector $v = \sum_{i=1}^s \lambda_i x_i$, then there exist coefficients $\lambda_1', \ldots, \lambda_s' \in \mathbb R_{\geq 0}$ of which at most $n$ are nonzero with $v = \sum_{i=1}^s \lambda_i' x_i$. In other words: given a finite set $B$ of real vectors, every vector in the polyhedral cone generated by $B$ can be written as a conical combination of at most $n$ vectors of $B$.",,"['linear-algebra', 'linear-programming']"
41,Basis (linear algebra),Basis (linear algebra),,"Let's have vectors $v_1,v_2,v_3,v_4\in \mathbb R^4$. Prove that these vectors form basis (1), determine if they are orthogonal and/or orthonormal (2). Then make a transition matrix $T_{\epsilon\alpha}$  from basis $\alpha$ to standard basis and use it to get coordinates of vector $w=(2,3,5,1)_\alpha$ in standard basis (3). $$v_1=(1,1-1,1),v_2=(1,-1-1,-1),v_3=(0,1,0,-1),v_4=(1,0,1,0) $$ (1) Prove that these vectors form a basis. I did following:   $$\left( \begin{array}{cccc}  1 & 1 & -1 &1 \\ 1 & -1 & -1 &-1 \\ 0 & 1 & 0 &-1 \\ 1 & 0 & 1 & 0   \end{array} \right)=A$$   $$\left| \begin{array}{cccc}  1 & 1 & -1 &1 \\ 1 & -1 & -1 &-1 \\ 0 & 1 & 0 &-1 \\ 1 & 0 & 1 & 0   \end{array} \right|=-8 \ $$   $|A| \neq 0 \implies$ Vectors $v_1,v_2,v_3,v_4$ are linearly independent and form basis. (2) Determine if they are orthogonal and/or orthonormal. I did following:   $$v_1\cdot v_2=(1-1+1-1)=0$$   $$v_1\cdot v_3=(0+1+0-1)=0$$   $$v_1\cdot v_4=(1+0-1+0)=0$$   $$v_2\cdot v_3=(0-1+0+1)=0$$   $$v_3\cdot v_4=(0+0+0+0)=0$$   $\implies$ Basis is orthogonal.   $$|v_1|=\sqrt {1+1+1+1}=\sqrt4=2$$   $$|v_2|=\sqrt {1+1+1+1}=\sqrt4=2$$   $$|v_3|=\sqrt {0+1+0+1}=\sqrt2$$   $$|v_4|=\sqrt {1+0+1+0}=\sqrt2$$   $\implies$ Basis is not orthonormal. (3) Make a transition matrix $T_{\epsilon\alpha}$  from basis $\alpha$ to standard basis and use it to get coordinates of vector $w=(2,3,5,1)_\alpha$ in standard basis. $$T_{\epsilon\alpha}=A^{-1}=\left( \begin{array}{cccc} \frac14 & \frac14 & 0 & \frac12 \\ \frac14 & -\frac14 & \frac12 &0 \\ -\frac14 & -\frac14 & 0 & \frac12 \\ \frac14 & -\frac14 & -\frac12 & 0   \end{array} \right)$$ $$u=A^{-1} w$$ Where $u$ is in standard basis? Is (1) and (2) correct and can you help me out with (3) ?","Let's have vectors $v_1,v_2,v_3,v_4\in \mathbb R^4$. Prove that these vectors form basis (1), determine if they are orthogonal and/or orthonormal (2). Then make a transition matrix $T_{\epsilon\alpha}$  from basis $\alpha$ to standard basis and use it to get coordinates of vector $w=(2,3,5,1)_\alpha$ in standard basis (3). $$v_1=(1,1-1,1),v_2=(1,-1-1,-1),v_3=(0,1,0,-1),v_4=(1,0,1,0) $$ (1) Prove that these vectors form a basis. I did following:   $$\left( \begin{array}{cccc}  1 & 1 & -1 &1 \\ 1 & -1 & -1 &-1 \\ 0 & 1 & 0 &-1 \\ 1 & 0 & 1 & 0   \end{array} \right)=A$$   $$\left| \begin{array}{cccc}  1 & 1 & -1 &1 \\ 1 & -1 & -1 &-1 \\ 0 & 1 & 0 &-1 \\ 1 & 0 & 1 & 0   \end{array} \right|=-8 \ $$   $|A| \neq 0 \implies$ Vectors $v_1,v_2,v_3,v_4$ are linearly independent and form basis. (2) Determine if they are orthogonal and/or orthonormal. I did following:   $$v_1\cdot v_2=(1-1+1-1)=0$$   $$v_1\cdot v_3=(0+1+0-1)=0$$   $$v_1\cdot v_4=(1+0-1+0)=0$$   $$v_2\cdot v_3=(0-1+0+1)=0$$   $$v_3\cdot v_4=(0+0+0+0)=0$$   $\implies$ Basis is orthogonal.   $$|v_1|=\sqrt {1+1+1+1}=\sqrt4=2$$   $$|v_2|=\sqrt {1+1+1+1}=\sqrt4=2$$   $$|v_3|=\sqrt {0+1+0+1}=\sqrt2$$   $$|v_4|=\sqrt {1+0+1+0}=\sqrt2$$   $\implies$ Basis is not orthonormal. (3) Make a transition matrix $T_{\epsilon\alpha}$  from basis $\alpha$ to standard basis and use it to get coordinates of vector $w=(2,3,5,1)_\alpha$ in standard basis. $$T_{\epsilon\alpha}=A^{-1}=\left( \begin{array}{cccc} \frac14 & \frac14 & 0 & \frac12 \\ \frac14 & -\frac14 & \frac12 &0 \\ -\frac14 & -\frac14 & 0 & \frac12 \\ \frac14 & -\frac14 & -\frac12 & 0   \end{array} \right)$$ $$u=A^{-1} w$$ Where $u$ is in standard basis? Is (1) and (2) correct and can you help me out with (3) ?",,"['linear-algebra', 'vector-spaces']"
42,Determinant of matrices along a line between two given matrices,Determinant of matrices along a line between two given matrices,,"The question, with no simplifications or motivation: Let $A$ and $B$ be square matrices of the same size (with real or complex coefficients). What is the most reasonable formula one can find for the determinant $$\det((1-t)A + tB)$$ as a function of $t \in [0,1]$? If no reasonable formula exists, what can we say about these determinants? So we're taking a line between two matrices $A$ and $B$, and computing the determinant along this line. When $A$ and $B$ are diagonal, say $$A = \operatorname{diag}(a_1,\ldots,a_n), B = \operatorname{diag}(b_1,\ldots,b_n),$$ then we can compute this directly: $$\begin{aligned} \det((1-t)A + tB) &= \det \operatorname{diag}((1-t)a_1 + tb_1, \ldots, (1-t)a_n + tb_n) \\ &= \prod_{j=1}^n ((1-t)a_j + tb_j). \end{aligned}$$ I'm not sure if this can be further simplified, but I'm sure someone can push things at least a tiny bit further than I have. I'm most curious about the case where $A = I$ and each $(1-t)A + tB$ is assumed to be invertible. Here's what I know in this case: writing $$D(t) = \det((1-t)I - tB),$$ we can compute that $$ \dot{D}(t) = D(t) c(t)$$ where $$c(t) := \operatorname{trace}(((1-t)I + tB)^{-1}(B-I))$$ (a warning: I am not 100% sure this formula holds). Thus we can write $$D(t) = \exp\left(\int_0^t c(\tau) \; d\tau\right)$$ since $D(0) = 1$. I have no idea how to deal with the function $c(\tau)$ though. Any tips?","The question, with no simplifications or motivation: Let $A$ and $B$ be square matrices of the same size (with real or complex coefficients). What is the most reasonable formula one can find for the determinant $$\det((1-t)A + tB)$$ as a function of $t \in [0,1]$? If no reasonable formula exists, what can we say about these determinants? So we're taking a line between two matrices $A$ and $B$, and computing the determinant along this line. When $A$ and $B$ are diagonal, say $$A = \operatorname{diag}(a_1,\ldots,a_n), B = \operatorname{diag}(b_1,\ldots,b_n),$$ then we can compute this directly: $$\begin{aligned} \det((1-t)A + tB) &= \det \operatorname{diag}((1-t)a_1 + tb_1, \ldots, (1-t)a_n + tb_n) \\ &= \prod_{j=1}^n ((1-t)a_j + tb_j). \end{aligned}$$ I'm not sure if this can be further simplified, but I'm sure someone can push things at least a tiny bit further than I have. I'm most curious about the case where $A = I$ and each $(1-t)A + tB$ is assumed to be invertible. Here's what I know in this case: writing $$D(t) = \det((1-t)I - tB),$$ we can compute that $$ \dot{D}(t) = D(t) c(t)$$ where $$c(t) := \operatorname{trace}(((1-t)I + tB)^{-1}(B-I))$$ (a warning: I am not 100% sure this formula holds). Thus we can write $$D(t) = \exp\left(\int_0^t c(\tau) \; d\tau\right)$$ since $D(0) = 1$. I have no idea how to deal with the function $c(\tau)$ though. Any tips?",,['linear-algebra']
43,Conditions for the equivalence of $\mathbf A^T \mathbf A \mathbf x = \mathbf A^T \mathbf b$ and $\mathbf A \mathbf x = \mathbf b$,Conditions for the equivalence of  and,\mathbf A^T \mathbf A \mathbf x = \mathbf A^T \mathbf b \mathbf A \mathbf x = \mathbf b,"I have an application where I have to minimize a cost function of the form: $J(\mathbf x) = \| \mathbf A \mathbf x - \mathbf b \|^2 \quad (1)$ By calculating the gradient, I derived that I have to solve the system of equations: $\mathbf A^T \mathbf A \mathbf x = \mathbf A^T \mathbf b \quad (2)$ Now my question is, when can I solve the following system instead? $\mathbf A \mathbf x = \mathbf b \quad (3)$ From my point of view, this depends on $\mathbf A$ to be invertible. In my application $\mathbf A$ is a square matrix of the form $\mathbf A = \mathbf I - \mathbf W$ where $\mathbf I$ is the identity matrix and $\mathbf W$ is a square matrix with zeros on the main diagonal and small values on a few secondary diagonals. The values of $\mathbf W$ are arbitrary but normalized, so that each row of $\mathbf W$ sums to 1. However, some lines of $\mathbf W$ can also be zero. For example $\mathbf A$ may look like this: $\mathbf A = \pmatrix{ 1 & -0.2 & 0 & 0 & -0.3 & -0.2 & -0.3 & 0 & 0 \cr -0.1 & 1 & -0.2 & 0 & 0 & -0.4 & -0.2 & -0.1 & 0 \cr 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \cr 0 & 0 & -0.2 & 1 & -0.1 & 0 & 0 & -0.6 & -0.1 \cr -0.2 & 0 & 0 & -0.2 & 1 & -0.5 & 0 & 0 & -0.1 \cr -0.4 & -0.3 & 0 & 0 & -0.1 & 1 & -0.2 & 0 & 0 \cr -0.1 & -0.1 & -0.1 & 0 & 0 & -0.6 & 1 & -0.1 & 0 \cr 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \cr 0 & 0 & -0.2 & -0.4 & -0.1 & 0 & 0 & -0.3 & 1 \cr }$ Have I argued correctly? Then how can I show that $\mathbf A$ is invertible? Or is there any other argument for solving (3) instead of (2)? I tried to solve both systems (2) and (3) with MATLAB and the Intel MKL, but surprisingly only (3) gave me the expected result. I would expect that it also works with (2). Maybe a numerical problem?","I have an application where I have to minimize a cost function of the form: $J(\mathbf x) = \| \mathbf A \mathbf x - \mathbf b \|^2 \quad (1)$ By calculating the gradient, I derived that I have to solve the system of equations: $\mathbf A^T \mathbf A \mathbf x = \mathbf A^T \mathbf b \quad (2)$ Now my question is, when can I solve the following system instead? $\mathbf A \mathbf x = \mathbf b \quad (3)$ From my point of view, this depends on $\mathbf A$ to be invertible. In my application $\mathbf A$ is a square matrix of the form $\mathbf A = \mathbf I - \mathbf W$ where $\mathbf I$ is the identity matrix and $\mathbf W$ is a square matrix with zeros on the main diagonal and small values on a few secondary diagonals. The values of $\mathbf W$ are arbitrary but normalized, so that each row of $\mathbf W$ sums to 1. However, some lines of $\mathbf W$ can also be zero. For example $\mathbf A$ may look like this: $\mathbf A = \pmatrix{ 1 & -0.2 & 0 & 0 & -0.3 & -0.2 & -0.3 & 0 & 0 \cr -0.1 & 1 & -0.2 & 0 & 0 & -0.4 & -0.2 & -0.1 & 0 \cr 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \cr 0 & 0 & -0.2 & 1 & -0.1 & 0 & 0 & -0.6 & -0.1 \cr -0.2 & 0 & 0 & -0.2 & 1 & -0.5 & 0 & 0 & -0.1 \cr -0.4 & -0.3 & 0 & 0 & -0.1 & 1 & -0.2 & 0 & 0 \cr -0.1 & -0.1 & -0.1 & 0 & 0 & -0.6 & 1 & -0.1 & 0 \cr 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \cr 0 & 0 & -0.2 & -0.4 & -0.1 & 0 & 0 & -0.3 & 1 \cr }$ Have I argued correctly? Then how can I show that $\mathbf A$ is invertible? Or is there any other argument for solving (3) instead of (2)? I tried to solve both systems (2) and (3) with MATLAB and the Intel MKL, but surprisingly only (3) gave me the expected result. I would expect that it also works with (2). Maybe a numerical problem?",,"['linear-algebra', 'numerical-linear-algebra']"
44,Formula for cylinder,Formula for cylinder,,"In an exercise I was asked to find a formula of the form $F(x,y,z)=C$ for a cylinder though the axis $(t,t,t)$ and radius $R$. The formula I got seemed a bit suspicious so I wanted to ask if I have it right. Basically I used the vector formula for the distance between a line and a point found here: http://mathworld.wolfram.com/Point-LineDistance2-Dimensional.html . I chose $x_1=(0,0,0), x_2=(1,1,1)$ to determine the line. Marking $r=(x,y,z)$ for a point on the cylinder, after some simplification and moving things around in the equation, I got that each point on the cylinder needs to fulfill the formula: $$(y-z)^2+(z-x)^2+(x-y)^2=3R^2$$ Have I correctly derived the formula? Thanks a bunch!","In an exercise I was asked to find a formula of the form $F(x,y,z)=C$ for a cylinder though the axis $(t,t,t)$ and radius $R$. The formula I got seemed a bit suspicious so I wanted to ask if I have it right. Basically I used the vector formula for the distance between a line and a point found here: http://mathworld.wolfram.com/Point-LineDistance2-Dimensional.html . I chose $x_1=(0,0,0), x_2=(1,1,1)$ to determine the line. Marking $r=(x,y,z)$ for a point on the cylinder, after some simplification and moving things around in the equation, I got that each point on the cylinder needs to fulfill the formula: $$(y-z)^2+(z-x)^2+(x-y)^2=3R^2$$ Have I correctly derived the formula? Thanks a bunch!",,"['linear-algebra', 'geometry']"
45,References on the History of Linear Algebra,References on the History of Linear Algebra,,"I have an aggregated understanding of the history of linear algebra compiled from friends, teachers, and coworkers. It may have several errors. It goes something like this: Even ancient cultures like the Chinese used the idea of row reduction to solve systems of linear equations, even if the format looked somewhat different. In medieval times, Arabic cultures kept this idea alive. In the 18th century, European mathematicians developed matrix notation to represent a linear system. There was even now the idea of inverting that matrix. By the mid 19th century, matrices were fully understood as derivatives of multivariable functions, and hence understood as transformations between finite dimensional vector spaces. Still at this point there was limited applicability for practical problem solving. If a matrix was even moderately large, it was a nice theoretical tool, but practically impossible to compute with. So the most recent chapter is In the 1940s and 1950s, with the advent of modern computing technology these limitations were overcome, and for example, Leontiff could solve a system of 500 equations. I suppose my first question is: is any part of this significantly inaccurate? But my main question is: is anyone aware of a good historical reference emphasizing the last chapter, where computers sparked renewed interest in linear algebra? I can't seem to find any good histories that focus on this era. Most of what I find focuses on the theory developed up to the mid 19th century.","I have an aggregated understanding of the history of linear algebra compiled from friends, teachers, and coworkers. It may have several errors. It goes something like this: Even ancient cultures like the Chinese used the idea of row reduction to solve systems of linear equations, even if the format looked somewhat different. In medieval times, Arabic cultures kept this idea alive. In the 18th century, European mathematicians developed matrix notation to represent a linear system. There was even now the idea of inverting that matrix. By the mid 19th century, matrices were fully understood as derivatives of multivariable functions, and hence understood as transformations between finite dimensional vector spaces. Still at this point there was limited applicability for practical problem solving. If a matrix was even moderately large, it was a nice theoretical tool, but practically impossible to compute with. So the most recent chapter is In the 1940s and 1950s, with the advent of modern computing technology these limitations were overcome, and for example, Leontiff could solve a system of 500 equations. I suppose my first question is: is any part of this significantly inaccurate? But my main question is: is anyone aware of a good historical reference emphasizing the last chapter, where computers sparked renewed interest in linear algebra? I can't seem to find any good histories that focus on this era. Most of what I find focuses on the theory developed up to the mid 19th century.",,"['linear-algebra', 'reference-request', 'math-history']"
46,Relation between linear recurrence and linear equations that produce the same number sequence,Relation between linear recurrence and linear equations that produce the same number sequence,,"The linear recurrence is $$\begin{align} v_1^{(k+1)}&:=-\frac{1}{2}v_1^{(k)}-\frac{1}{6}v_2^{(k)}\cdots-\frac{1}{n(n-1)}v_n^{(k)}\\ v_2^{(k+1)}&:=+\frac{1}{1}v_1^{(k)}\\ v_3^{(k+1)}&:=+\frac{1}{2}v_2^{(k)}\\ \vdots \\v_{n}^{(k+1)}&:=+\frac{1}{n-1}v_{n-1}^{(k)}\\ \end{align}$$ Consider the following sequence $$\begin{array} &&v_1^{(0)}=1,&v_2^{(0)}=0,&v_3^{(0)}=0,&v_4^{(0)}=0,&v_5^{(0)}=0,&\cdots,&v_n^{(0)}=0\\ &v_1^{(1)}=-\frac{1}{2},&v_2^{(1)}=1,&v_3^{(1)}=0,&v_4^{(1)}=0,&v_5^{(1)}=0,&\cdots,&v_n^{(1)}=0\\ &v_1^{(2)}=\frac{1}{12},&v_2^{(2)}=-\frac{1}{2},&v_3^{(2)}=\frac{1}{2},&v_4^{(2)}=0,&v_5^{(2)}=0,&\cdots,&v_n^{(2)}=0\\ &v_1^{(3)}=0,&v_2^{(3)}=\frac{1}{12},&v_3^{(3)}=-\frac{1}{4},&v_4^{(3)}=\frac{1}{6},&v_5^{(3)}=0,&\cdots,&v_n^{(3)}=0\\ &v_1^{(4)}=-\frac{1}{720},&v_2^{(4)}=0,&v_3^{(4)}=\frac{1}{24},&v_4^{(4)}=-\frac{1}{12},&v_5^{(4)}=\frac{1}{24},&\cdots,&v_n^{(4)}=0\\ \end{array}$$ We see that $k!v_i^{(k)}$ are the coefficients of the bernoulli polynomials . These coefficients are realated to the Bernoulli Numbers by $$B_n(x) = \sum_{k=0}^n{n\choose k}B_kx^{n-k}$$ see wikipedia . The system of linear equations is $$\begin{array}~ {2\choose 0}B_1&+&{2\choose 1}B_2&&&&&&&=0\\ {3\choose 0}B_1&+&{3\choose 1}B_2&+&{3\choose 2}B_3&&&&&=0\\ {4\choose 0}B_1&+&{4\choose 1}B_2&+&{4\choose 2}B_3&+&{4\choose 3}B_4&&&=0\\ {5\choose 0}B_1&+&{5\choose 1}B_2&+&{5\choose 2}B_3&+&{5\choose 3}B_4&+&{5\choose 4}B_5&=0\\ \end{array}$$ If $B_1:=1,B_2:=-\frac{1}{2},B_3:=\frac{1}{6},B_4:=0$ and $B_5:=-\frac{1}{30}$ (notice eq. $(34)$ on this page ) the system is solved. Is there some specific realation between the recurrence and the linear system, other than that they both produce this number sequence?","The linear recurrence is $$\begin{align} v_1^{(k+1)}&:=-\frac{1}{2}v_1^{(k)}-\frac{1}{6}v_2^{(k)}\cdots-\frac{1}{n(n-1)}v_n^{(k)}\\ v_2^{(k+1)}&:=+\frac{1}{1}v_1^{(k)}\\ v_3^{(k+1)}&:=+\frac{1}{2}v_2^{(k)}\\ \vdots \\v_{n}^{(k+1)}&:=+\frac{1}{n-1}v_{n-1}^{(k)}\\ \end{align}$$ Consider the following sequence $$\begin{array} &&v_1^{(0)}=1,&v_2^{(0)}=0,&v_3^{(0)}=0,&v_4^{(0)}=0,&v_5^{(0)}=0,&\cdots,&v_n^{(0)}=0\\ &v_1^{(1)}=-\frac{1}{2},&v_2^{(1)}=1,&v_3^{(1)}=0,&v_4^{(1)}=0,&v_5^{(1)}=0,&\cdots,&v_n^{(1)}=0\\ &v_1^{(2)}=\frac{1}{12},&v_2^{(2)}=-\frac{1}{2},&v_3^{(2)}=\frac{1}{2},&v_4^{(2)}=0,&v_5^{(2)}=0,&\cdots,&v_n^{(2)}=0\\ &v_1^{(3)}=0,&v_2^{(3)}=\frac{1}{12},&v_3^{(3)}=-\frac{1}{4},&v_4^{(3)}=\frac{1}{6},&v_5^{(3)}=0,&\cdots,&v_n^{(3)}=0\\ &v_1^{(4)}=-\frac{1}{720},&v_2^{(4)}=0,&v_3^{(4)}=\frac{1}{24},&v_4^{(4)}=-\frac{1}{12},&v_5^{(4)}=\frac{1}{24},&\cdots,&v_n^{(4)}=0\\ \end{array}$$ We see that $k!v_i^{(k)}$ are the coefficients of the bernoulli polynomials . These coefficients are realated to the Bernoulli Numbers by $$B_n(x) = \sum_{k=0}^n{n\choose k}B_kx^{n-k}$$ see wikipedia . The system of linear equations is $$\begin{array}~ {2\choose 0}B_1&+&{2\choose 1}B_2&&&&&&&=0\\ {3\choose 0}B_1&+&{3\choose 1}B_2&+&{3\choose 2}B_3&&&&&=0\\ {4\choose 0}B_1&+&{4\choose 1}B_2&+&{4\choose 2}B_3&+&{4\choose 3}B_4&&&=0\\ {5\choose 0}B_1&+&{5\choose 1}B_2&+&{5\choose 2}B_3&+&{5\choose 3}B_4&+&{5\choose 4}B_5&=0\\ \end{array}$$ If $B_1:=1,B_2:=-\frac{1}{2},B_3:=\frac{1}{6},B_4:=0$ and $B_5:=-\frac{1}{30}$ (notice eq. $(34)$ on this page ) the system is solved. Is there some specific realation between the recurrence and the linear system, other than that they both produce this number sequence?",,"['linear-algebra', 'elementary-number-theory']"
47,Extending a positive linear functional in finite dimensions,Extending a positive linear functional in finite dimensions,,"Let $V$ be a vector subspace of $R^N$, and $l:V \to R$ a linear mapping such that $l(V\bigcap R_{+}^N)\subseteq R_{+}$ (i.e., $l$ is positive). I have heard that there exists a separating hyperplane sort of argument that allows us to show that $l$ extends to a positive linear functional on $R^N$. I have my own proof, but it is complicated and uses the Bauer-Namioka condition for extension of positive linear functionals on ordered vector spaces of any dimension. Question : What is this simple separating hyperplane argument that shows that $l$ extends to a positive linear functional on $R^N$? (I believe it should be quite straightforward, but I was not able to construct the right problem to invoke a separation argument...) A reference would be okay as well. Thanks.","Let $V$ be a vector subspace of $R^N$, and $l:V \to R$ a linear mapping such that $l(V\bigcap R_{+}^N)\subseteq R_{+}$ (i.e., $l$ is positive). I have heard that there exists a separating hyperplane sort of argument that allows us to show that $l$ extends to a positive linear functional on $R^N$. I have my own proof, but it is complicated and uses the Bauer-Namioka condition for extension of positive linear functionals on ordered vector spaces of any dimension. Question : What is this simple separating hyperplane argument that shows that $l$ extends to a positive linear functional on $R^N$? (I believe it should be quite straightforward, but I was not able to construct the right problem to invoke a separation argument...) A reference would be okay as well. Thanks.",,['linear-algebra']
48,Fitting a parameter dependent matrix to its eigenvalues,Fitting a parameter dependent matrix to its eigenvalues,,"The essence of my question is, if I have a Hermitian matrix that is linearly dependent on a set of parameters and I have an estimate of its eigenvalues, is there a ""simple"" way to determine the values of the parameters?  Ideally, I would also like to have some measure of the goodness of the fit and the degree of variation within the parameters. As a materials physicist, I often have to create a simple quantum mechanical model from either experimental data or a more complex calculation.  For the smaller problems (8x8, with 10 params), the parameters can be found by painstakingly working through the various relationships among the parameters, due to symmetry, etc.  But, this method is specific to each problem and does not scale well to larger problems.  For instance, one system I'm looking at would require a 20x20 matrix with 21 parameters, and that is without including spin!  Alternatively, there is the brute force method of simulated annealing, which involves taking a random walk through the parameter space, and slowly decreasing the step size in the hopes that the calculation will get stuck in the global minimum.  Neither of these methods is particularly appealing, so I'd like some ideas on how to approach this in a consistent manner.","The essence of my question is, if I have a Hermitian matrix that is linearly dependent on a set of parameters and I have an estimate of its eigenvalues, is there a ""simple"" way to determine the values of the parameters?  Ideally, I would also like to have some measure of the goodness of the fit and the degree of variation within the parameters. As a materials physicist, I often have to create a simple quantum mechanical model from either experimental data or a more complex calculation.  For the smaller problems (8x8, with 10 params), the parameters can be found by painstakingly working through the various relationships among the parameters, due to symmetry, etc.  But, this method is specific to each problem and does not scale well to larger problems.  For instance, one system I'm looking at would require a 20x20 matrix with 21 parameters, and that is without including spin!  Alternatively, there is the brute force method of simulated annealing, which involves taking a random walk through the parameter space, and slowly decreasing the step size in the hopes that the calculation will get stuck in the global minimum.  Neither of these methods is particularly appealing, so I'd like some ideas on how to approach this in a consistent manner.",,"['linear-algebra', 'matrices', 'physics']"
49,Frobenius norm of a product of Gaussian matrices,Frobenius norm of a product of Gaussian matrices,,"Suppose $$C_n=X_1 X_2\cdots X_n$$ where $X_i$ is $d\times d$ matrix with IID entries sampled with normal centered at 0 and variance $1/d$ . The following appears to be true for large $d$ , why? $$\|C_n C_n^T\|_F^2\approx d(n+1)$$ Here are some numbers from a 4 samples with $d=1000$ $$ \begin{array}{c|ccccc}   & \text{n} & \text{sample1} & \text{sample2} & \text{sample3} & \text{sample4} \\ \hline   & 1 & 2003.99 & 1998.66 & 1999.51 & 1998.14 \\   & 2 & 3029.97 & 2990.12 & 3008.21 & 2999.13 \\   & 3 & 3967.81 & 3995.46 & 4022.33 & 4005.2 \\   & 4 & 5027.41 & 5075.39 & 4941.94 & 5057.4 \\   & 5 & 6143.21 & 5964.35 & 5844.76 & 6015.08 \\ \end{array} $$ Notebook It also appears to hold if I use the same matrix for all $X_i$ , ie, $$\|X^n (X^T)^n\|^2_F \approx d(n+1)$$ Dividing entries of table by $d$ below we get near perfect agreement in table below with $d=4000$ using either $X^n$ (fixed) or $C_n$ (resampled) ( crossposted on stats.SE)","Suppose where is matrix with IID entries sampled with normal centered at 0 and variance . The following appears to be true for large , why? Here are some numbers from a 4 samples with Notebook It also appears to hold if I use the same matrix for all , ie, Dividing entries of table by below we get near perfect agreement in table below with using either (fixed) or (resampled) ( crossposted on stats.SE)","C_n=X_1 X_2\cdots X_n X_i d\times d 1/d d \|C_n C_n^T\|_F^2\approx d(n+1) d=1000 
\begin{array}{c|ccccc}
  & \text{n} & \text{sample1} & \text{sample2} & \text{sample3} & \text{sample4} \\
\hline
  & 1 & 2003.99 & 1998.66 & 1999.51 & 1998.14 \\
  & 2 & 3029.97 & 2990.12 & 3008.21 & 2999.13 \\
  & 3 & 3967.81 & 3995.46 & 4022.33 & 4005.2 \\
  & 4 & 5027.41 & 5075.39 & 4941.94 & 5057.4 \\
  & 5 & 6143.21 & 5964.35 & 5844.76 & 6015.08 \\
\end{array}
 X_i \|X^n (X^T)^n\|^2_F \approx d(n+1) d d=4000 X^n C_n","['linear-algebra', 'probability', 'probability-theory', 'statistics', 'random-matrices']"
50,If $f((\text{deg}(f) + 1$) consecutive $\mathbb{Z}\text{s}) \subset \mathbb{Z}$ then $f(\mathbb{Z}) \subseteq \mathbb{Z}$,If ) consecutive  then,f((\text{deg}(f) + 1 \mathbb{Z}\text{s}) \subset \mathbb{Z} f(\mathbb{Z}) \subseteq \mathbb{Z},"Question: Let $f:\mathbb{R}\to\mathbb{R}$ be a polynomial of degree $n$ . Suppose there exists an integer $m\in\mathbb{Z}$ such that $f(m)\in\mathbb{Z}$ , $f(m+1)\in\mathbb{Z}$ , ..., $f(m+n)\in\mathbb{Z}$ (i.e. there exists $n+1$ consecutive integers all satisfying $f(x)\in\mathbb{Z}$ ). Prove that $\forall x\in\mathbb{Z}, f(x)\in\mathbb{Z}$ . Context: This is a problem I thought of on my own when I was preparing for a contest next month. This problem at first glance ""felt"" like one of those classic and old problem with lots of solutions and thoughts around it. Surprisingly, I could find neither solutions nor questions regarding the same problem. After spending a while on the problem, I came up with a finite difference solution. The solution is as follows: My solution: Let $P(n)$ denote the statement we want to prove for a function of degree $n$ . We shall prove that $P(n)$ is true for all $n$ by induction. Base case: When $n=0$ , $f(x)=c$ where $c$ is a constant. Since $f(m)\in\mathbb{Z}$ , we have $c\in\mathbb{Z}$ , which means that $\forall x\in\mathbb{Z}, f(x)=c\in\mathbb{Z}$ . Hence, $P(0)$ is true. Induction hypothesis: $P(k)$ is true where $k\in\mathbb{Z}^+$ . Inductive step: We claim that $P(k+1)$ is true. We have the condition that there exists an integer $m\in\mathbb{Z}$ such that $f(m)\in\mathbb{Z}$ , $f(m+1)\in\mathbb{Z}$ , ..., $f(m+k+1)\in\mathbb{Z}$ . Consider a function $g(x)=f(x)-f(x-1)$ (i.e. first finite difference). Since $f(x)$ has degree $k+1$ , we have that $g(x)$ has degree $k$ . From the conditions given we also know that $$g(m+1)=f(m+1)-f(m)\in\mathbb{Z} \\ g(m+2)=f(m+2)-f(m+1)\in\mathbb{Z} \\ \dots \\ g(m+k+1)=f(m+k+1)-f(m+k)\in\mathbb{Z}$$ Hence, we have $k+1$ consecutive integers all satisfying $g(x)\in\mathbb{Z}$ . By the induction hypothesis, this means that $\forall x\in\mathbb{Z}, g(x)\in\mathbb{Z}$ . Then, we have $$f(m+k+2)=f(m+k+1)+g(m+k+2)\in\mathbb{Z} \\ f(m+k+3)=f(m+k+2)+g(m+k+3)\in\mathbb{Z} \\ \dots$$ and $$f(m-1)=f(m)-g(m)\in\mathbb{Z} \\ f(m-2)=f(m-1)-g(m-1)\in\mathbb{Z} \\ \dots$$ It can now be easily proven by induction that $\forall x\in\mathbb{Z}, f(x)\in\mathbb{Z}$ . This completes the induction, $Q.E.D.$ First I'd like to ask you to verify if my proof is correct (and the spaces for improvements). Second, I'd also like to ask for more insightful and/or elegant proofs (or any generalisation from this). Any help is appreciated!","Question: Let be a polynomial of degree . Suppose there exists an integer such that , , ..., (i.e. there exists consecutive integers all satisfying ). Prove that . Context: This is a problem I thought of on my own when I was preparing for a contest next month. This problem at first glance ""felt"" like one of those classic and old problem with lots of solutions and thoughts around it. Surprisingly, I could find neither solutions nor questions regarding the same problem. After spending a while on the problem, I came up with a finite difference solution. The solution is as follows: My solution: Let denote the statement we want to prove for a function of degree . We shall prove that is true for all by induction. Base case: When , where is a constant. Since , we have , which means that . Hence, is true. Induction hypothesis: is true where . Inductive step: We claim that is true. We have the condition that there exists an integer such that , , ..., . Consider a function (i.e. first finite difference). Since has degree , we have that has degree . From the conditions given we also know that Hence, we have consecutive integers all satisfying . By the induction hypothesis, this means that . Then, we have and It can now be easily proven by induction that . This completes the induction, First I'd like to ask you to verify if my proof is correct (and the spaces for improvements). Second, I'd also like to ask for more insightful and/or elegant proofs (or any generalisation from this). Any help is appreciated!","f:\mathbb{R}\to\mathbb{R} n m\in\mathbb{Z} f(m)\in\mathbb{Z} f(m+1)\in\mathbb{Z} f(m+n)\in\mathbb{Z} n+1 f(x)\in\mathbb{Z} \forall x\in\mathbb{Z}, f(x)\in\mathbb{Z} P(n) n P(n) n n=0 f(x)=c c f(m)\in\mathbb{Z} c\in\mathbb{Z} \forall x\in\mathbb{Z}, f(x)=c\in\mathbb{Z} P(0) P(k) k\in\mathbb{Z}^+ P(k+1) m\in\mathbb{Z} f(m)\in\mathbb{Z} f(m+1)\in\mathbb{Z} f(m+k+1)\in\mathbb{Z} g(x)=f(x)-f(x-1) f(x) k+1 g(x) k g(m+1)=f(m+1)-f(m)\in\mathbb{Z} \\ g(m+2)=f(m+2)-f(m+1)\in\mathbb{Z} \\ \dots \\ g(m+k+1)=f(m+k+1)-f(m+k)\in\mathbb{Z} k+1 g(x)\in\mathbb{Z} \forall x\in\mathbb{Z}, g(x)\in\mathbb{Z} f(m+k+2)=f(m+k+1)+g(m+k+2)\in\mathbb{Z} \\ f(m+k+3)=f(m+k+2)+g(m+k+3)\in\mathbb{Z} \\ \dots f(m-1)=f(m)-g(m)\in\mathbb{Z} \\ f(m-2)=f(m-1)-g(m-1)\in\mathbb{Z} \\ \dots \forall x\in\mathbb{Z}, f(x)\in\mathbb{Z} Q.E.D.","['linear-algebra', 'polynomials', 'contest-math', 'finite-differences']"
51,Is the ideal generated by coefficients of characteristic polynomial of a matrix prime?,Is the ideal generated by coefficients of characteristic polynomial of a matrix prime?,,"Consider the ring $R=\mathbb C[a_{ij}]$ which is the free polynomial ring of $n^2$ variables $a_{ij}$ over the field of complex numbers $\mathbb C$ . Set matrix $A=[a_{ij}]_{n\times n}$ and $f(\lambda):=\det(\lambda I-A)=\lambda^n+\sum_{k=0}^{n-1}f_k\lambda^k$ be its characteristic polynomial. Let the ideal $J=(f_0,\cdots,f_{n-1})\subset R$ . Is $J$ a prime ideal? What I have done is the following: Take a look at the algebraic set $Z=V_{\mathbb C^{n\times n}}\ (J)\subset M_n(\mathbb C)$ , the set of all nilpotent matrices, one can show that $Z$ is irreducible. Consider the Jordan block of size $n$ and eigenvalue $0$ , $B=J_n(0)$ . Check the map $\mathrm{GL}_n(\mathbb C)\to M_n(\mathbb C)$ given by $P\mapsto PBP^{-1}$ . The image of this map is nilpotent matrices with Jordan canonical form $B$ , since the set of these matrices is Zariski dense in $Z$ , so the closure of the image is $Z$ . By the fact that $\mathrm{GL}_n(\mathbb C)$ is irreducible, we know the image is irreducible, so $Z$ is irreducible too. It remains to show that $J$ is a radical ideal: by $I(Z)=\sqrt J$ if $J=\sqrt J$ then it is prime. For $n=2$ , this can be done by hand. When $n=3$ , By a SageMath check: sage: R.<a11, a12, a13, a21, a22, a23, a31, a32, a33> = PolynomialRing(QQ,9) ....: I = R * [a13*a22*a31 - a12*a23*a31- a13*a21*a32 + a11*a23*a32       + a12*a21*a33 - a11*a22*a33, - a12*a21 + a11*a22 - a13*a31 -        a23*a32 + a11*a33 + a22*a33, -a11 - a22 - a33] ....: I.is_prime() True","Consider the ring which is the free polynomial ring of variables over the field of complex numbers . Set matrix and be its characteristic polynomial. Let the ideal . Is a prime ideal? What I have done is the following: Take a look at the algebraic set , the set of all nilpotent matrices, one can show that is irreducible. Consider the Jordan block of size and eigenvalue , . Check the map given by . The image of this map is nilpotent matrices with Jordan canonical form , since the set of these matrices is Zariski dense in , so the closure of the image is . By the fact that is irreducible, we know the image is irreducible, so is irreducible too. It remains to show that is a radical ideal: by if then it is prime. For , this can be done by hand. When , By a SageMath check: sage: R.<a11, a12, a13, a21, a22, a23, a31, a32, a33> = PolynomialRing(QQ,9) ....: I = R * [a13*a22*a31 - a12*a23*a31- a13*a21*a32 + a11*a23*a32       + a12*a21*a33 - a11*a22*a33, - a12*a21 + a11*a22 - a13*a31 -        a23*a32 + a11*a33 + a22*a33, -a11 - a22 - a33] ....: I.is_prime() True","R=\mathbb C[a_{ij}] n^2 a_{ij} \mathbb C A=[a_{ij}]_{n\times n} f(\lambda):=\det(\lambda I-A)=\lambda^n+\sum_{k=0}^{n-1}f_k\lambda^k J=(f_0,\cdots,f_{n-1})\subset R J Z=V_{\mathbb C^{n\times n}}\ (J)\subset M_n(\mathbb C) Z n 0 B=J_n(0) \mathrm{GL}_n(\mathbb C)\to M_n(\mathbb C) P\mapsto PBP^{-1} B Z Z \mathrm{GL}_n(\mathbb C) Z J I(Z)=\sqrt J J=\sqrt J n=2 n=3","['linear-algebra', 'matrices', 'commutative-algebra', 'maximal-and-prime-ideals', 'characteristic-polynomial']"
52,Prove $\frac{d}{dt} \Big|_{t=0}\mbox{tr}(e^{X+tY})=\mbox{tr}(e^XY)$,Prove,\frac{d}{dt} \Big|_{t=0}\mbox{tr}(e^{X+tY})=\mbox{tr}(e^XY),"I’m asked to prove $\frac{d}{dt}\Big|_{t=0}\mbox{tr}(e^{X+tY})=\mbox{tr}(e^XY)$ for any $X,Y$ in $M_n(\mathbb{C})$ . My attempt is to assume both $X$ and $Y$ are diagonalizable, and since the set of all diagonalizable matrices is dense in $M_n(\mathbb{C})$ , if we can show this is true for diagonalizable matrices, then we are done. I expected this will somehow simplify the proof, but seems it does not work well unless I further assume $X,Y$ can be diagonalizable at the same time. Any suggestions on this?","I’m asked to prove for any in . My attempt is to assume both and are diagonalizable, and since the set of all diagonalizable matrices is dense in , if we can show this is true for diagonalizable matrices, then we are done. I expected this will somehow simplify the proof, but seems it does not work well unless I further assume can be diagonalizable at the same time. Any suggestions on this?","\frac{d}{dt}\Big|_{t=0}\mbox{tr}(e^{X+tY})=\mbox{tr}(e^XY) X,Y M_n(\mathbb{C}) X Y M_n(\mathbb{C}) X,Y","['linear-algebra', 'matrices', 'complex-analysis', 'matrix-calculus', 'trace']"
53,Need help in understanding (a part of) the proof of John's Theorem,Need help in understanding (a part of) the proof of John's Theorem,,"This is the theorem statement: John's Theorem: Each convex body $K$ contains a unique ellipsoid of maximal volume. This ellipsoid is $B^n_2$ (Euclidean ball of unit radius) iff: $B^n_2 \subset K$ and (for some $m$ ), there are Euclidean unit vectors $(u_i)_1^m$ on the boundary of $K$ and positive numbers $(c_i)_1^m$ satisfying \begin{equation}     \sum_{i=1}^m c_i u_i = 0 \end{equation} and \begin{equation}     \sum_{i=1}^m c_i \langle x,u_i\rangle^2 = \|x\|^2 \text{ for each }x\in\mathbb{R}^n \end{equation} Note that we're only working with centrally symmetric convex bodies for the rest of this post, in which case the second condition implies the first, i.e. the latter is redundant. What part I'm specifically concerned with: Suppose $B_2^n$ is an ellipsoid of largest volume in $K$ . We want to show that there is a sequence of contact points $(u_i)$ and positive weights $(c_i)$ with $$\frac{1}{n}I_n = \frac{1}{n}\sum c_i \ u_i\otimes u_i$$ The proof begins: Equating traces on both sides of the equation, we know that if this is possible , then $$\sum \frac{c_i}{n} = 1$$ So our aim is to show that the matrix $I_n/n$ can be written as a convex combination of (a finite number of) matrices of the form $u \otimes u$ , where each $u$ is a contact point. Since the space of matrices is finite-dimensional, the problem is simply to show that $I_n /n$ belongs to the convex hull of the set of all such rank-one matrices, $$T = \{u \otimes u : u \text{ is a contact point}\}$$ How does the space of matrices being finite-dimensional help? The definition of $T$ seems slightly off. Perhaps the author meant $T = \text{conv}\{u \otimes u : u \text{ is a contact point}\}$ , i.e. the convex hull of all matrices of the form $uu^T$ ? Being a contact point between $B^n_2$ and $K$ , $u\in\partial K$ and $\|u\| = 1$ . We shall aim to get a contradiction by showing that if $I_n/n$ is not in T, we can perturb the unit ball slightly to get a new ellipsoid in $K$ of larger volume than the unit ball. Suppose that $I_n/n$ is not in $T$ . Apply the separation theorem in the space of matrices to get a linear functional $φ$ (on this space) with the property that $$φ\left(\frac{I_n}{n}\right) < φ(u\otimes u)$$ for each contact point $u$ . Observe that $φ$ can be represented by an $n \times n$ matrix $H = (h_{jk})$ , so that, for any matrix $A = (a_ {jk})$ , $$φ(A) = \sum_{jk}h_{jk}a_{jk}$$ How did we come up with this linear functional? It makes intuitive sense, but I want to know exactly how we used the separation theorem as stated here. I saw this coming - since we are working in the space of matrices, we had to define an inner product similar to the one for $\mathbb{R}^n$ - hence we just chose the element-wise product and applied the separating hyperplane theorem? We didn't have any other option for the inner product though, right? Since all the matrices $u \otimes u$ and $I_n /n$ are symmetric, we may assume the same for $H$ . Moreover, since these matrices all have the same trace, namely $1$ , the inequality $φ(I_n /n) < φ(u \otimes u)$ will remain unchanged if we add a constant to each diagonal entry of $H$ . So we may assume that the trace of $H$ is $0$ : but this says precisely that $φ(I_n) = 0$ . Hence, unless the identity has the representation we want , we have found a symmetric matrix $H$ with zero trace for which $$\sum_{jk}h_{jk}(u\otimes u)_{jk} > 0$$ for every contact point $u$ . We shall use this $H$ to build a bigger ellipsoid inside $K$ . Now, for each vector $u$ , $$\sum_{jk}h_{jk}(u\otimes u)_{jk} = u^THu$$ What does the author mean by ""the representation we want""? For sufficiently small $δ > 0$ , the set $$E_δ = \{x ∈ \mathbb{R}^n : x^T (I_n + δH)x ≤ 1\}$$ is an ellipsoid and as $δ$ tends to $0$ these ellipsoids approach $B_2^n$ . If $u$ is one of the original contact points, then $$u^T (I_n + δH)u = 1 + δu^T Hu > 1$$ so $u$ does not belong to $E_δ$ . Since the boundary of $K$ is compact (and the function $x \mapsto x^T Hx$ is continuous) $E_δ$ will not contain any other point of $∂K$ as long as $δ$ is sufficiently small. Thus, for such $δ$ , the ellipsoid $E_δ$ is strictly inside $K$ and some slightly expanded ellipsoid is inside $K$ . It remains to check that each $E_δ$ has volume at least that of $B_2^n$ . If we denote by $(μ_j)$ the eigenvalues of the symmetric matrix $I_n + δH$ , the volume of $E_δ$ is $v_n/\prod\mu_j$ so the problem is to show that, for each $δ$ , we have $\prod μ_j ≤ 1$ . What we know is that $\sum μ_j$ is the trace of $I_n + δH$ , which is $n$ , since the trace of $H$ is $0$ . So the AM/GM inequality again gives $$\prod \mu_j^{1/n} \le \frac{1}{n}\sum \mu_j \le 1$$ as required. $v_n$ denotes the volume of $B^n_2$ - how did we write the volume of the ellipsoid $E_\delta$ in terms of the eigenvalues of $I_n + \delta H$ ? We assumed $I_n/n\notin T$ , in order to get a contradiction. Where is the contradiction ? I know this is a long post! Thanks a lot for reading this far. Since this is a long one, and understanding the proof requires several clarifications, I have decided to award a bounty to an answer that helps with all (or most) of my questions. Thanks again - I'd appreciate any help!","This is the theorem statement: John's Theorem: Each convex body contains a unique ellipsoid of maximal volume. This ellipsoid is (Euclidean ball of unit radius) iff: and (for some ), there are Euclidean unit vectors on the boundary of and positive numbers satisfying and Note that we're only working with centrally symmetric convex bodies for the rest of this post, in which case the second condition implies the first, i.e. the latter is redundant. What part I'm specifically concerned with: Suppose is an ellipsoid of largest volume in . We want to show that there is a sequence of contact points and positive weights with The proof begins: Equating traces on both sides of the equation, we know that if this is possible , then So our aim is to show that the matrix can be written as a convex combination of (a finite number of) matrices of the form , where each is a contact point. Since the space of matrices is finite-dimensional, the problem is simply to show that belongs to the convex hull of the set of all such rank-one matrices, How does the space of matrices being finite-dimensional help? The definition of seems slightly off. Perhaps the author meant , i.e. the convex hull of all matrices of the form ? Being a contact point between and , and . We shall aim to get a contradiction by showing that if is not in T, we can perturb the unit ball slightly to get a new ellipsoid in of larger volume than the unit ball. Suppose that is not in . Apply the separation theorem in the space of matrices to get a linear functional (on this space) with the property that for each contact point . Observe that can be represented by an matrix , so that, for any matrix , How did we come up with this linear functional? It makes intuitive sense, but I want to know exactly how we used the separation theorem as stated here. I saw this coming - since we are working in the space of matrices, we had to define an inner product similar to the one for - hence we just chose the element-wise product and applied the separating hyperplane theorem? We didn't have any other option for the inner product though, right? Since all the matrices and are symmetric, we may assume the same for . Moreover, since these matrices all have the same trace, namely , the inequality will remain unchanged if we add a constant to each diagonal entry of . So we may assume that the trace of is : but this says precisely that . Hence, unless the identity has the representation we want , we have found a symmetric matrix with zero trace for which for every contact point . We shall use this to build a bigger ellipsoid inside . Now, for each vector , What does the author mean by ""the representation we want""? For sufficiently small , the set is an ellipsoid and as tends to these ellipsoids approach . If is one of the original contact points, then so does not belong to . Since the boundary of is compact (and the function is continuous) will not contain any other point of as long as is sufficiently small. Thus, for such , the ellipsoid is strictly inside and some slightly expanded ellipsoid is inside . It remains to check that each has volume at least that of . If we denote by the eigenvalues of the symmetric matrix , the volume of is so the problem is to show that, for each , we have . What we know is that is the trace of , which is , since the trace of is . So the AM/GM inequality again gives as required. denotes the volume of - how did we write the volume of the ellipsoid in terms of the eigenvalues of ? We assumed , in order to get a contradiction. Where is the contradiction ? I know this is a long post! Thanks a lot for reading this far. Since this is a long one, and understanding the proof requires several clarifications, I have decided to award a bounty to an answer that helps with all (or most) of my questions. Thanks again - I'd appreciate any help!","K B^n_2 B^n_2 \subset K m (u_i)_1^m K (c_i)_1^m \begin{equation}
    \sum_{i=1}^m c_i u_i = 0
\end{equation} \begin{equation}
    \sum_{i=1}^m c_i \langle x,u_i\rangle^2 = \|x\|^2 \text{ for each }x\in\mathbb{R}^n
\end{equation} B_2^n K (u_i) (c_i) \frac{1}{n}I_n = \frac{1}{n}\sum c_i \ u_i\otimes u_i \sum \frac{c_i}{n} = 1 I_n/n u \otimes u u I_n /n T = \{u \otimes u : u \text{ is a contact point}\} T T = \text{conv}\{u \otimes u : u \text{ is a contact point}\} uu^T B^n_2 K u\in\partial K \|u\| = 1 I_n/n K I_n/n T φ φ\left(\frac{I_n}{n}\right) < φ(u\otimes u) u φ n \times n H = (h_{jk}) A = (a_ {jk}) φ(A) = \sum_{jk}h_{jk}a_{jk} \mathbb{R}^n u \otimes u I_n /n H 1 φ(I_n /n) < φ(u \otimes u) H H 0 φ(I_n) = 0 H \sum_{jk}h_{jk}(u\otimes u)_{jk} > 0 u H K u \sum_{jk}h_{jk}(u\otimes u)_{jk} = u^THu δ > 0 E_δ = \{x ∈ \mathbb{R}^n : x^T (I_n + δH)x ≤ 1\} δ 0 B_2^n u u^T (I_n + δH)u = 1 + δu^T Hu > 1 u E_δ K x \mapsto x^T Hx E_δ ∂K δ δ E_δ K K E_δ B_2^n (μ_j) I_n + δH E_δ v_n/\prod\mu_j δ \prod μ_j ≤ 1 \sum μ_j I_n + δH n H 0 \prod \mu_j^{1/n} \le \frac{1}{n}\sum \mu_j \le 1 v_n B^n_2 E_\delta I_n + \delta H I_n/n\notin T","['linear-algebra', 'geometry', 'proof-explanation', 'convex-analysis', 'convex-geometry']"
54,Arrangements of Solutions to $n$ Linear Equations with $n$ Unknowns,Arrangements of Solutions to  Linear Equations with  Unknowns,n n,"In a system of two linear equations with two unknowns, there are three ""arrangements"" that we can see when we graph the two lines in the $xy$ plane: The two lines intersect at a single point (one solution to the system). The two lines are parallel and never intersect (no solution to the system). The two equations describe the same line (infinitely many solutions to the system). When we move up to a system of three linear equations with three unknowns, now we have three planes in space, and there are eight distinct arrangements of the three planes: If we consider one linear equation with one unknown, I suppose it makes sense to say that there is one arrangement, which is a single point on the real number line. So, for one, two and three unknowns, we have the start of a sequence: $1, 3, 8, ...$ I am interested in how this sequence continues. I have searched in vain on the OEIS. Alas, there are many sequences that have $1, 3, 8,...$ and I'm not sure which, if any, are the right one. This one: https://oeis.org/A001792 looks like it might be it, because the comments say that sequence is related to matrices in certain ways. Also I expect a formula for this sequence should involve powers of 2. But, I wouldn't bet any money on it. Is there a sequence with a simple formula to work out the number of arrangements?","In a system of two linear equations with two unknowns, there are three ""arrangements"" that we can see when we graph the two lines in the plane: The two lines intersect at a single point (one solution to the system). The two lines are parallel and never intersect (no solution to the system). The two equations describe the same line (infinitely many solutions to the system). When we move up to a system of three linear equations with three unknowns, now we have three planes in space, and there are eight distinct arrangements of the three planes: If we consider one linear equation with one unknown, I suppose it makes sense to say that there is one arrangement, which is a single point on the real number line. So, for one, two and three unknowns, we have the start of a sequence: I am interested in how this sequence continues. I have searched in vain on the OEIS. Alas, there are many sequences that have and I'm not sure which, if any, are the right one. This one: https://oeis.org/A001792 looks like it might be it, because the comments say that sequence is related to matrices in certain ways. Also I expect a formula for this sequence should involve powers of 2. But, I wouldn't bet any money on it. Is there a sequence with a simple formula to work out the number of arrangements?","xy 1, 3, 8, ... 1, 3, 8,...","['linear-algebra', 'algebra-precalculus']"
55,Deeper Algebraic Structure to Random Vectors?,Deeper Algebraic Structure to Random Vectors?,,"Given a probability space $\Omega,$ the space of square-integrable measurable functions $\Omega \to \mathbb{R}^n$ (""random vectors"") can be made a vector space over $\mathbb{R}$ in a natural way. Call this space $V.$ In probability theory, we proceed to define several operators on this space, like the expectation operator $E : V \to \mathbb{R}^n$ given by $(X_1,X_2...,X_n) \mapsto (E(X_1),E(X_2)...,E(X_n))$ . However, going just a bit deeper into the theory, we start to see some properties of $E$ nicer than linearity over $\mathbb{R}$ would alone suggest. For example, for any $k \times n$ matrix $A$ , we find that $E(AX) = AE(X).$ Similar occurrences occur with the bilinear covariance operator $\mathrm{Cov} : V \to \mathbb{R}^{n \times n}$ . For example, for any $k \times n$ matrices $A$ and $B,$ we find $\mathrm{Cov}(AX,BY) = A\mathrm{Cov}(X,Y)B^T,$ where $B^T$ denotes the transpose of $B.$ On one level, one can just view this as matrix algebra (and this may be all there is to it). But I've always been inclined to look for deeper algebraic structure than just matrix algebra when I see matrices, so I'm wondering if there's a deeper algebraic reason to this. For example, we could have viewed $V$ as a module over $n \times n$ matrices, but this approach doesn't seem to explain the transposes and the generalization to $k \times n$ matrices with $k \neq n.$ So, I'm wondering if there's some algebraic structure to $V$ in which the ""matrix linearity"" of the form seen in $E$ and $\mathrm{Cov}$ become natural (and hence easy to remember!).","Given a probability space the space of square-integrable measurable functions (""random vectors"") can be made a vector space over in a natural way. Call this space In probability theory, we proceed to define several operators on this space, like the expectation operator given by . However, going just a bit deeper into the theory, we start to see some properties of nicer than linearity over would alone suggest. For example, for any matrix , we find that Similar occurrences occur with the bilinear covariance operator . For example, for any matrices and we find where denotes the transpose of On one level, one can just view this as matrix algebra (and this may be all there is to it). But I've always been inclined to look for deeper algebraic structure than just matrix algebra when I see matrices, so I'm wondering if there's a deeper algebraic reason to this. For example, we could have viewed as a module over matrices, but this approach doesn't seem to explain the transposes and the generalization to matrices with So, I'm wondering if there's some algebraic structure to in which the ""matrix linearity"" of the form seen in and become natural (and hence easy to remember!).","\Omega, \Omega \to \mathbb{R}^n \mathbb{R} V. E : V \to \mathbb{R}^n (X_1,X_2...,X_n) \mapsto (E(X_1),E(X_2)...,E(X_n)) E \mathbb{R} k \times n A E(AX) = AE(X). \mathrm{Cov} : V \to \mathbb{R}^{n \times n} k \times n A B, \mathrm{Cov}(AX,BY) = A\mathrm{Cov}(X,Y)B^T, B^T B. V n \times n k \times n k \neq n. V E \mathrm{Cov}","['linear-algebra', 'probability-theory', 'statistics', 'modules']"
56,Log of product of 3 matrix exponentials,Log of product of 3 matrix exponentials,,"I'm working on a certain problem that involves the following question: Let $A,B$ be two self-adjoint operators, and define $C=e^{-A}e^{-B}e^{-A}$ . Is there a ""convenient"" way to express $log(C)$ ? I'm not entirely sure what ""convenient"" is, looking for anything that could be useful. I tried two things - the first was using the Campbell-Baker-Hausdorff formula, but I got very complicated expressions that I do not know how to deal with (since the two operators do not necessarily commute). The second was assuming that one of the operators is diagonal (I simply calculate everything up to a conjugation), but still, I couldn't find a fairly simple way to express what I'm looking for. Does anyone know of some useful identities/methods that could be useful for this? In the problem I'm working on, $A,B$ depend on two parameters which vary in a certain region, and I want to find a ""simple"" expression for $log(C)$ , so that I can change the parameters and immediately see what happens. Thanks in advance.","I'm working on a certain problem that involves the following question: Let be two self-adjoint operators, and define . Is there a ""convenient"" way to express ? I'm not entirely sure what ""convenient"" is, looking for anything that could be useful. I tried two things - the first was using the Campbell-Baker-Hausdorff formula, but I got very complicated expressions that I do not know how to deal with (since the two operators do not necessarily commute). The second was assuming that one of the operators is diagonal (I simply calculate everything up to a conjugation), but still, I couldn't find a fairly simple way to express what I'm looking for. Does anyone know of some useful identities/methods that could be useful for this? In the problem I'm working on, depend on two parameters which vary in a certain region, and I want to find a ""simple"" expression for , so that I can change the parameters and immediately see what happens. Thanks in advance.","A,B C=e^{-A}e^{-B}e^{-A} log(C) A,B log(C)","['linear-algebra', 'matrices', 'functional-analysis', 'operator-theory']"
57,Is it possible to determine the given matrix is positive semidefinite under these conditions?,Is it possible to determine the given matrix is positive semidefinite under these conditions?,,"Suppose I have a $2^n$ by $2^n$ symmetric matrix M. I know the following facts are true about $M$ . The diagonal of M is $n+1$ , which is strictly larger than any other non-diagonal entry. The sum of each row of the matrix M is exactly $2^n$ The value of non-diagonal entry cannot be smaller than $-n+1$ Each row contains the same elements (but the order is different so that $M$ is symmetric) I really hope to conclude $M$ is positive semidefinite, but I have to admit this may not be true. I know the fact that if each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row, $M$ would be positive definite. However, we cannot use here because of fact 2. On the other hand, fact 2 implies $M$ has eigenvector 1 with eigenvalue $2^n$ and there cannot be too many negative entries in $M$ . I wonder if these conditions can be sufficient for me to draw such a conclusion. Courant fischer theorem seems helpful here since we can express the smallest eigenvalue as $\min_{v \perp 1} \frac{v^{T}Mv}{v^{T}v}$","Suppose I have a by symmetric matrix M. I know the following facts are true about . The diagonal of M is , which is strictly larger than any other non-diagonal entry. The sum of each row of the matrix M is exactly The value of non-diagonal entry cannot be smaller than Each row contains the same elements (but the order is different so that is symmetric) I really hope to conclude is positive semidefinite, but I have to admit this may not be true. I know the fact that if each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row, would be positive definite. However, we cannot use here because of fact 2. On the other hand, fact 2 implies has eigenvector 1 with eigenvalue and there cannot be too many negative entries in . I wonder if these conditions can be sufficient for me to draw such a conclusion. Courant fischer theorem seems helpful here since we can express the smallest eigenvalue as",2^n 2^n M n+1 2^n -n+1 M M M M 2^n M \min_{v \perp 1} \frac{v^{T}Mv}{v^{T}v},"['linear-algebra', 'positive-semidefinite']"
58,How wide is the Birkhoff Polytope?,How wide is the Birkhoff Polytope?,,"Now also posted on Math Overflow. Define the width of a polytope $P \subset \mathbb R^d$ as the minimum length of the interval $\{v \cdot p:p \in P\}$ for $v$ in the unit sphere. In other words the width is the smallest number $W$ such that you can sandwich $P$ between two hyperplanes distance $W$ apart. Here's a picture: Suppose the polytope $P \subset \mathbb R^d$ is contained in the affine subspace $A + x$ for $A \subset \mathbb R^d$ a hyerplane. Define the relative width as the smallest length of $\{v \cdot p:p \in P\}$ as $v$ ranges over the unit sphere in $A$ . In other words translate the affine subspace to contain the origin and then ignore the perpendicular directions. The Birkhoff polytope $\mathcal B$ is defined as the convex hull of the $n!$ permutation matrices. That means the $n \times n$ matrices with all zeros except for exactly one $1$ in each row and column. Equivalently $\mathcal B$ is the set of nonnegative matrices with all row and column sums equal to $1$ . In this case the affine subspace is defined as $$\left \{x \in \mathbb R^d: \sum_j x^i_j =1, \sum_i x^i_j =1\right \}.$$ This just says the row and column sums equal $1$ . Within that subspace the polytope is defined as the intersection with the first quadrant. I am having trouble computing or estimating the height of $\mathcal B$ . I would imagine the $v$ that minimises the projection is something like $$    v =   \left( {\begin{array}{cccc}    1/4 & -1/4 & 1/4& -1/4\\    -1/4 & 1/4 & -1/4 & 1/4\\    1/4 & -1/4 & 1/4 & -1/4\\   - 1/4 & 1/4 & - 1/4 & 1/4\\   \end{array} } \right) $$ or in general make half the diagonals equal to $1/n$ and the other equal to $-1/n$ . Then choosing the correct permutation matrices for the endpoints of the interval, we can force the interval to have length $2$ . The only reason I have to believe this is there are many choices of permutation matrices, and we want to minimise the interval length among all pairs. So $v$ should be symmetric in some sense. Does anyone have ideas?","Now also posted on Math Overflow. Define the width of a polytope as the minimum length of the interval for in the unit sphere. In other words the width is the smallest number such that you can sandwich between two hyperplanes distance apart. Here's a picture: Suppose the polytope is contained in the affine subspace for a hyerplane. Define the relative width as the smallest length of as ranges over the unit sphere in . In other words translate the affine subspace to contain the origin and then ignore the perpendicular directions. The Birkhoff polytope is defined as the convex hull of the permutation matrices. That means the matrices with all zeros except for exactly one in each row and column. Equivalently is the set of nonnegative matrices with all row and column sums equal to . In this case the affine subspace is defined as This just says the row and column sums equal . Within that subspace the polytope is defined as the intersection with the first quadrant. I am having trouble computing or estimating the height of . I would imagine the that minimises the projection is something like or in general make half the diagonals equal to and the other equal to . Then choosing the correct permutation matrices for the endpoints of the interval, we can force the interval to have length . The only reason I have to believe this is there are many choices of permutation matrices, and we want to minimise the interval length among all pairs. So should be symmetric in some sense. Does anyone have ideas?","P \subset \mathbb R^d \{v \cdot p:p \in P\} v W P W P \subset \mathbb R^d A + x A \subset \mathbb R^d \{v \cdot p:p \in P\} v A \mathcal B n! n \times n 1 \mathcal B 1 \left \{x \in \mathbb R^d: \sum_j x^i_j =1, \sum_i x^i_j =1\right \}. 1 \mathcal B v 
   v =
  \left( {\begin{array}{cccc}
   1/4 & -1/4 & 1/4& -1/4\\
   -1/4 & 1/4 & -1/4 & 1/4\\
   1/4 & -1/4 & 1/4 & -1/4\\
  - 1/4 & 1/4 & - 1/4 & 1/4\\
  \end{array} } \right)
 1/n -1/n 2 v","['linear-algebra', 'permutations', 'polytopes', 'discrete-geometry', 'birkhoff-polytopes']"
59,How many N by N adjacency matrices exist with maximum degree of m?,How many N by N adjacency matrices exist with maximum degree of m?,,"I want to calculate the number of adjacency matrices corresponding to graphs with $N$ nodes satisfying all of the following properties: There are no isolated nodes (each row of the matrix must have at least one 1). The maximum degree for each node is at most $m \leq N$ (For example, if $m=3$ , then each row of the matrix can have at most three 1s). The adjacency matrix represents an undirected graph. My attempts thus far : I know that the number of adjacency matrices for an undirected graph with $N$ nodes is $2^{N(N-1)/2}$ , but I have not been able to incorporate the remaining assumptions.","I want to calculate the number of adjacency matrices corresponding to graphs with nodes satisfying all of the following properties: There are no isolated nodes (each row of the matrix must have at least one 1). The maximum degree for each node is at most (For example, if , then each row of the matrix can have at most three 1s). The adjacency matrix represents an undirected graph. My attempts thus far : I know that the number of adjacency matrices for an undirected graph with nodes is , but I have not been able to incorporate the remaining assumptions.",N m \leq N m=3 N 2^{N(N-1)/2},"['linear-algebra', 'combinatorics', 'discrete-mathematics', 'graph-theory']"
60,Is every norm for real matrices extensible to a norm for complex matrices?,Is every norm for real matrices extensible to a norm for complex matrices?,,"I was browsing this website haphardly and hit upon this question: Do we have for all $M \in SL_n(\Bbb K)$, $\lVert M \rVert \geq 1$ when $\lVert \cdot \rVert$ is a matrix norm? . It suddenly dawned on me that all submultiplicative matrix norms I have ever seen are actually defined for complex matrices. None of them is defined specifically for real matrices. Is it because every submultiplicative norm for real matrices is extensible to the complex case? More specifically: Suppose $\|\cdot\|_r$ is a submultiplicative matrix norm on $M_n(\mathbb R)$ . (The subscript $r$ is just a notation; it is not a number and it doesn't signify any specific matrix norm.) Does there always exist a submultiplicative norm $\|\cdot\|_c$ on $M_n(\mathbb C)$ such that $\|A\|_r=\|A\|_c$ for every $A\in M_n(\mathbb R)$ ? If homogenity and submultiplicativity are not required, we can pick a vector norm on $\mathbb R^2$ and define $\|X+iY\|_c=\|(\|X\|_r,\|Y\|_r)\|$ . However, I don't see any way to implement the homogenity condition $\|(a+ib)(X+iY)\|_c=|a+ib|\|X+iY\|_c$ , not to mention submultiplicativity. I have also considered the case where there exists a submultiplicative norm $\|\cdot\|_R$ on $M_{2n}(\mathbb R)$ such that $$\left\|\pmatrix{A&0\\ 0&A}\right\|_R=\|A\|_r$$ for every $A\in M_n(\mathbb R)$ . My first thought was to define $$\|X+iY\|_c=\left\|\pmatrix{X&-Y\\ Y&X}\right\|_R.$$ If such a norm $\|\cdot\|_R$ does exist, then $\|\cdot\|_c$ is automatically submultiplicative. However, I don't see any reason why $\|\cdot\|_R$ should exist and why the homogenity condition $$\left\|\pmatrix{aI&-bI\\ bI&aI}\pmatrix{X&-Y\\ Y&X}\right\|_R=|a+ib|\left\|\pmatrix{X&-Y\\ Y&X}\right\|_R.$$ is satisfied.","I was browsing this website haphardly and hit upon this question: Do we have for all $M \in SL_n(\Bbb K)$, $\lVert M \rVert \geq 1$ when $\lVert \cdot \rVert$ is a matrix norm? . It suddenly dawned on me that all submultiplicative matrix norms I have ever seen are actually defined for complex matrices. None of them is defined specifically for real matrices. Is it because every submultiplicative norm for real matrices is extensible to the complex case? More specifically: Suppose is a submultiplicative matrix norm on . (The subscript is just a notation; it is not a number and it doesn't signify any specific matrix norm.) Does there always exist a submultiplicative norm on such that for every ? If homogenity and submultiplicativity are not required, we can pick a vector norm on and define . However, I don't see any way to implement the homogenity condition , not to mention submultiplicativity. I have also considered the case where there exists a submultiplicative norm on such that for every . My first thought was to define If such a norm does exist, then is automatically submultiplicative. However, I don't see any reason why should exist and why the homogenity condition is satisfied.","\|\cdot\|_r M_n(\mathbb R) r \|\cdot\|_c M_n(\mathbb C) \|A\|_r=\|A\|_c A\in M_n(\mathbb R) \mathbb R^2 \|X+iY\|_c=\|(\|X\|_r,\|Y\|_r)\| \|(a+ib)(X+iY)\|_c=|a+ib|\|X+iY\|_c \|\cdot\|_R M_{2n}(\mathbb R) \left\|\pmatrix{A&0\\ 0&A}\right\|_R=\|A\|_r A\in M_n(\mathbb R) \|X+iY\|_c=\left\|\pmatrix{X&-Y\\ Y&X}\right\|_R. \|\cdot\|_R \|\cdot\|_c \|\cdot\|_R \left\|\pmatrix{aI&-bI\\ bI&aI}\pmatrix{X&-Y\\ Y&X}\right\|_R=|a+ib|\left\|\pmatrix{X&-Y\\ Y&X}\right\|_R.","['linear-algebra', 'matrices', 'analysis', 'normed-spaces']"
61,"Geometric intuition on $\langle x, A^\top y\rangle = \langle y, Ax\rangle$",Geometric intuition on,"\langle x, A^\top y\rangle = \langle y, Ax\rangle","I am looking for a geometric intuition on $\langle x, A^\top y\rangle = \langle y, Ax\rangle$ . This can be proven algebraically by disassembling the expression into basic sums and product and reordering a few terms. But that does not offer any insight. Semantically this equality states that the scaled projection (dot product) of $x$ with a linear combination of the rows of $A$ weighted by $y$ is equal to the scaled projection of $y$ onto linear combination of the columns of $A$ weighted by $x$ . But I fail to understand this in an intuitive geometric way. Do you know of any insightful interpretation? If this equality has a name, that would also be useful. Right now I cannot really research it without a name. I came across the equation here: Eigenvectors of real symmetric matrices are orthogonal","I am looking for a geometric intuition on . This can be proven algebraically by disassembling the expression into basic sums and product and reordering a few terms. But that does not offer any insight. Semantically this equality states that the scaled projection (dot product) of with a linear combination of the rows of weighted by is equal to the scaled projection of onto linear combination of the columns of weighted by . But I fail to understand this in an intuitive geometric way. Do you know of any insightful interpretation? If this equality has a name, that would also be useful. Right now I cannot really research it without a name. I came across the equation here: Eigenvectors of real symmetric matrices are orthogonal","\langle x, A^\top y\rangle = \langle y, Ax\rangle x A y y A x","['linear-algebra', 'geometry', 'linear-transformations', 'bilinear-form']"
62,Kronecker Product Interpretation,Kronecker Product Interpretation,,The algebraic expression for a Kronecker product is simple enough.  Is there some way to understand what this product is? The expression for matrix-vector multiplication is easy enough to understand.  But realizing that the multiplication yields a linear combination of the columns of the matrix is a useful insight. Is there some analogous insight for the Kronecker product?,The algebraic expression for a Kronecker product is simple enough.  Is there some way to understand what this product is? The expression for matrix-vector multiplication is easy enough to understand.  But realizing that the multiplication yields a linear combination of the columns of the matrix is a useful insight. Is there some analogous insight for the Kronecker product?,,"['linear-algebra', 'tensor-products', 'kronecker-product']"
63,Challenging linear algebra problem - n people are standing on a circle,Challenging linear algebra problem - n people are standing on a circle,,"We have $n$ people standing on  a circle, each one has written on his head a real number, and the sum of the numbers on the people's heads is $M$ . We are also given that $n$ is odd. Each second, each of the people deletes the number on his head and replaces it with the average of the numbers of his neighbors. Define the number on the head of the i'th guy  after $k$ seconds as $a_i(k)$ . Show that $lim_{k \to \infty} a_i(k) = M/n$ . Hint: Look at $T:\mathbb{C}^n \rightarrow \mathbb{C}^n$ defined by $Te_i = e_{i+1}, Te_n=e_1$ . Let $\lambda_n$ be a primitive $n$ 'th root of unity. Then $(w_1,...,w_n)$ is eigenbasis where $(w_i)_j = \lambda_n^{-(i-1)(j-1)}$ with eigenvalues $1,\lambda_n,\lambda_n^2,...,\lambda_n^{n-1}$ . It is also an eigenbasis for $T^{-1}$ with eigenvalues $1,\lambda_n^{-1},,,,\lambda_n^{-(n-1)}$ Use that to find eigenbasis for $R = \frac{1}{2} (T+T^{-1})$ . Use that to solve the problem. I did everthing in the hint and I got that $w_1,...,w_n$ is eigenbasis for $R$ with eigenvalues $1,\cos(\theta),\cos(2\theta),...,\cos((n-1)\theta)$ where $\theta = arg(\lambda_n)$ , I also realized how $R$ is related to the problem: $R^k$ just gives us the vector with the $a_i(k)$ 's. Now, take the vector $v$ of the $a_i(0)$ 's, and write it as $v = \sum_{j=1}^{n}a_iw_i$ . Then we have $R^k(v)= \sum_{j=1}^{n}a_i\cos^k((j-1)\theta)w_i$ . Now I don't know how to continue.","We have people standing on  a circle, each one has written on his head a real number, and the sum of the numbers on the people's heads is . We are also given that is odd. Each second, each of the people deletes the number on his head and replaces it with the average of the numbers of his neighbors. Define the number on the head of the i'th guy  after seconds as . Show that . Hint: Look at defined by . Let be a primitive 'th root of unity. Then is eigenbasis where with eigenvalues . It is also an eigenbasis for with eigenvalues Use that to find eigenbasis for . Use that to solve the problem. I did everthing in the hint and I got that is eigenbasis for with eigenvalues where , I also realized how is related to the problem: just gives us the vector with the 's. Now, take the vector of the 's, and write it as . Then we have . Now I don't know how to continue.","n M n k a_i(k) lim_{k \to \infty} a_i(k) = M/n T:\mathbb{C}^n \rightarrow \mathbb{C}^n Te_i = e_{i+1}, Te_n=e_1 \lambda_n n (w_1,...,w_n) (w_i)_j = \lambda_n^{-(i-1)(j-1)} 1,\lambda_n,\lambda_n^2,...,\lambda_n^{n-1} T^{-1} 1,\lambda_n^{-1},,,,\lambda_n^{-(n-1)} R = \frac{1}{2} (T+T^{-1}) w_1,...,w_n R 1,\cos(\theta),\cos(2\theta),...,\cos((n-1)\theta) \theta = arg(\lambda_n) R R^k a_i(k) v a_i(0) v = \sum_{j=1}^{n}a_iw_i R^k(v)= \sum_{j=1}^{n}a_i\cos^k((j-1)\theta)w_i","['linear-algebra', 'eigenvalues-eigenvectors']"
64,Machine learning book with robust linear algebra approach,Machine learning book with robust linear algebra approach,,"I am looking for machine learning book - neural network, deep learning etc etc - that use linear algebra in a robust manner. I found satisfactory the old book of Simon Haykin : Neural Networks : A Comprehensive Foundation -1998. Do you know if exist a text book recently released in the same trace?  Many thanks","I am looking for machine learning book - neural network, deep learning etc etc - that use linear algebra in a robust manner. I found satisfactory the old book of Simon Haykin : Neural Networks : A Comprehensive Foundation -1998. Do you know if exist a text book recently released in the same trace?  Many thanks",,"['linear-algebra', 'reference-request', 'book-recommendation', 'machine-learning', 'neural-networks']"
65,"Let $V$ be a subspace of $\mathbb{R}^4$, spanned by $v$ and $u$. Find a linear transformation whose kernel is $V$.","Let  be a subspace of , spanned by  and . Find a linear transformation whose kernel is .",V \mathbb{R}^4 v u V,"And the vectors given are $v = (1,0,3,-2)$ and $u = (0,1,4,1)$ . It asks me to find the linear transformation from $\mathbb{R}^4$ to $\mathbb{R}^2$ , where the kernel of that transformation is $V$ . So what I know is that: the transformation I'm trying to find, applied to every vector in the span of $(1,0,3,-2)$ and $(0,1,4,1)$ , will give the zero vector. Please let me know if that interpretation is incorrect. I've really no idea how to get started on this question. I have the equation $Av = 0$ where $A$ is the matrix of the transformation in question, and v is any vector of the subspace V, but...I don't think that gets me anywhere. Any help is greatly appreciated.","And the vectors given are and . It asks me to find the linear transformation from to , where the kernel of that transformation is . So what I know is that: the transformation I'm trying to find, applied to every vector in the span of and , will give the zero vector. Please let me know if that interpretation is incorrect. I've really no idea how to get started on this question. I have the equation where is the matrix of the transformation in question, and v is any vector of the subspace V, but...I don't think that gets me anywhere. Any help is greatly appreciated.","v = (1,0,3,-2) u = (0,1,4,1) \mathbb{R}^4 \mathbb{R}^2 V (1,0,3,-2) (0,1,4,1) Av = 0 A","['linear-algebra', 'linear-transformations']"
66,On Proposition 2.6 Gualtieri Thesis Generalized complex geometry,On Proposition 2.6 Gualtieri Thesis Generalized complex geometry,,"I'm working with Gualtieri's thesis about Generalized complex Geometry and I don't understand the proof of the Proposition 2.6 (p. 7). It says Every maximal isotropic subspace (maximal totally null subspaces) of $V\oplus V^*$ can be express as $L(E,\alpha)$ for some appropriate $E\subseteq V$ and 2-form $\alpha\in\Lambda^2(E)$ . (Recall $$ L(E,\alpha)=\{X+\xi\in E\oplus V^* : \xi|_E = i_\alpha X \}.  $$ In the proof, he defines $E=\pi_V (L)$ and \begin{array}{rcl} \alpha: E & \longrightarrow & E^* \\ X & \longmapsto & \Psi(\pi_{V^*}(\pi_V^{-1}(X)\cap L)) , \end{array} where $\pi$ are the canonical projections onto $V$ and $V^*$ and $\Psi:E^*\rightarrow V^*/\operatorname{Ann} E$ is the isomorphism he mentions (he doesn't use the isomorphism explicitly even they says it is necessary). I don't understand what the map does so I can't prove it is skew. Can you help me? I wish to understand what the map does, but my actual goal is to prove it is skew $$ \alpha(X)(Y)+\alpha(Y)(X)=0 \qquad \forall X,Y\in E $$ (I've picked some tags but the real ones don't exist. We have neither Generalize geometry and Dirac structures) EDIT: I haven't done any progress but I can show some example I have thougt about to show you I'm working in it. Let $V$ be a 3-dimensional vector space spanned by $\{E_i\}$ and let $\{\epsilon_j\}$ be its dual basis ($\epsilon_j(E_i)=\delta_{ij}$). The set $\{E_i,\epsilon_j\}$ is a basis for $V\oplus V^*$. Here we consider the indefinite product $$\langle X+\xi, Y+\eta \rangle = \eta(X)+ \xi (Y) . $$ The subspaces $$  L_1 = \operatorname{span} \{E_1+\epsilon_2, E_1+\epsilon_3, \epsilon_3 \}$$ and $$  L_2 = \operatorname{span} \{E_1+\epsilon_3, E_2+\epsilon_3, \epsilon_3 \}$$ are both maximal isotropic subspaces. The problem in both cases is $L$ can be descomposed into $W\oplus \operatorname{Ann}W$, $W=\pi_V(L)$. So it is easy to see $\alpha=0$. This kind of examples are very easy and I can't think of anythinc more avdanced. Some ideas? SOLUTION? I think I can give an alternative proof which constructs explicitly such a form. Ii would be like this: Let $L$ be a maximal isotropic subspaces spanned by the vectors $\{E_1+\xi_1,E_2+\xi_2,\dots ,E_n+\xi_n\}$. As above, define $W=\pi_V(L)$. This subspaces will be spanned by some of the $E_i$'s. Call $B$ sucha set. For the shake of simplicity also call $$ I=\{i: E_i\in B\} $$ Let $\{\theta_i\}$ be a dual basis for $B$: $$\theta_i(E_j)= \delta_{ij} \qquad  \forall i,j\in I $$ Now, define $\alpha\in\Lambda^2(W)$ as follows: For $E_1$: if $E_1\in B$, let $\alpha_1= \iota^*(\epsilon_1)\wedge\theta^1$, where $\iota^*:V^*\rightarrow W^*$ is the dual of the inclusion. Otherwise $\alpha_1=0$. Next, if $E_2\in B$ let $\alpha_2=\alpha_1+ \iota^*(\epsilon_2)\wedge \theta_2 $. If moreover $E_2=E_1$, set $\alpha_2 = \frac{1}{2}\alpha_1 + \frac{1}{2}\iota^*(\epsilon_2)\wedge \theta_2. $ At the end we should have a $2$-form $$\alpha= \sum_{i\in I} w_i \iota^*(\epsilon_i)\wedge \theta_i, $$ where $w_i$ is 1 over the times each $E_i$ appears in the span of $L$. I claim this $\alpha$ is what I'm looking for, but I don't know how to prove it. My problem is I need to show that, if $E_1+\epsilon_1$ and $E_1+\epsilon_2$ span $L$, then $\iota^*(\epsilon_1)=\iota^*(\epsilon_2)$. Clearly, for each $i\in B$, $\epsilon_i=i_{E_i}\alpha$.","I'm working with Gualtieri's thesis about Generalized complex Geometry and I don't understand the proof of the Proposition 2.6 (p. 7). It says Every maximal isotropic subspace (maximal totally null subspaces) of $V\oplus V^*$ can be express as $L(E,\alpha)$ for some appropriate $E\subseteq V$ and 2-form $\alpha\in\Lambda^2(E)$ . (Recall $$ L(E,\alpha)=\{X+\xi\in E\oplus V^* : \xi|_E = i_\alpha X \}.  $$ In the proof, he defines $E=\pi_V (L)$ and \begin{array}{rcl} \alpha: E & \longrightarrow & E^* \\ X & \longmapsto & \Psi(\pi_{V^*}(\pi_V^{-1}(X)\cap L)) , \end{array} where $\pi$ are the canonical projections onto $V$ and $V^*$ and $\Psi:E^*\rightarrow V^*/\operatorname{Ann} E$ is the isomorphism he mentions (he doesn't use the isomorphism explicitly even they says it is necessary). I don't understand what the map does so I can't prove it is skew. Can you help me? I wish to understand what the map does, but my actual goal is to prove it is skew $$ \alpha(X)(Y)+\alpha(Y)(X)=0 \qquad \forall X,Y\in E $$ (I've picked some tags but the real ones don't exist. We have neither Generalize geometry and Dirac structures) EDIT: I haven't done any progress but I can show some example I have thougt about to show you I'm working in it. Let $V$ be a 3-dimensional vector space spanned by $\{E_i\}$ and let $\{\epsilon_j\}$ be its dual basis ($\epsilon_j(E_i)=\delta_{ij}$). The set $\{E_i,\epsilon_j\}$ is a basis for $V\oplus V^*$. Here we consider the indefinite product $$\langle X+\xi, Y+\eta \rangle = \eta(X)+ \xi (Y) . $$ The subspaces $$  L_1 = \operatorname{span} \{E_1+\epsilon_2, E_1+\epsilon_3, \epsilon_3 \}$$ and $$  L_2 = \operatorname{span} \{E_1+\epsilon_3, E_2+\epsilon_3, \epsilon_3 \}$$ are both maximal isotropic subspaces. The problem in both cases is $L$ can be descomposed into $W\oplus \operatorname{Ann}W$, $W=\pi_V(L)$. So it is easy to see $\alpha=0$. This kind of examples are very easy and I can't think of anythinc more avdanced. Some ideas? SOLUTION? I think I can give an alternative proof which constructs explicitly such a form. Ii would be like this: Let $L$ be a maximal isotropic subspaces spanned by the vectors $\{E_1+\xi_1,E_2+\xi_2,\dots ,E_n+\xi_n\}$. As above, define $W=\pi_V(L)$. This subspaces will be spanned by some of the $E_i$'s. Call $B$ sucha set. For the shake of simplicity also call $$ I=\{i: E_i\in B\} $$ Let $\{\theta_i\}$ be a dual basis for $B$: $$\theta_i(E_j)= \delta_{ij} \qquad  \forall i,j\in I $$ Now, define $\alpha\in\Lambda^2(W)$ as follows: For $E_1$: if $E_1\in B$, let $\alpha_1= \iota^*(\epsilon_1)\wedge\theta^1$, where $\iota^*:V^*\rightarrow W^*$ is the dual of the inclusion. Otherwise $\alpha_1=0$. Next, if $E_2\in B$ let $\alpha_2=\alpha_1+ \iota^*(\epsilon_2)\wedge \theta_2 $. If moreover $E_2=E_1$, set $\alpha_2 = \frac{1}{2}\alpha_1 + \frac{1}{2}\iota^*(\epsilon_2)\wedge \theta_2. $ At the end we should have a $2$-form $$\alpha= \sum_{i\in I} w_i \iota^*(\epsilon_i)\wedge \theta_i, $$ where $w_i$ is 1 over the times each $E_i$ appears in the span of $L$. I claim this $\alpha$ is what I'm looking for, but I don't know how to prove it. My problem is I need to show that, if $E_1+\epsilon_1$ and $E_1+\epsilon_2$ span $L$, then $\iota^*(\epsilon_1)=\iota^*(\epsilon_2)$. Clearly, for each $i\in B$, $\epsilon_i=i_{E_i}\alpha$.",,"['linear-algebra', 'differential-geometry', 'quadratic-forms', 'spin-geometry']"
67,Linear independence of indicator functions,Linear independence of indicator functions,,"Question Let $X$ be a finite set, let $\mathbf{F}$ be a field. Let $\mathcal{A}$ be a subset of the power set of $X$, i.e. $\mathcal{A}\subseteq\mathcal{P}(X)$. Let $1_A:X\to F$ denote the indicator function of $A\subseteq X$. When is $\{1_A:A\in\mathcal{A}\}$ linearly independent in the vector space $\mathbf{F}^X$? Potential difficulty Unfortunately, this seems to depend on the characteristic of $\mathbf{F}$. For example, if $X=\{a,b,c\}$ and $\mathcal{A}$ consists of all two-element subsets of $X$, then $\{1_A:A\in\mathcal{A}\}$ is dependent when $\mathrm{char}(\mathbf{F})=2$, but independent otherwise. Partial answer All I can say so far is that $\mathcal{A}$ cannot contain an inclusion-exclusion family by which I mean a collection of the form $$\{A,B,A\cap B, A\cup B\}\text{ or}$$ $$\{A,B,C,A\cap B, A\cap C, B\cap C, A\cap B\cap C, A\cup B \cup C\} \text{ or so on}.$$","Question Let $X$ be a finite set, let $\mathbf{F}$ be a field. Let $\mathcal{A}$ be a subset of the power set of $X$, i.e. $\mathcal{A}\subseteq\mathcal{P}(X)$. Let $1_A:X\to F$ denote the indicator function of $A\subseteq X$. When is $\{1_A:A\in\mathcal{A}\}$ linearly independent in the vector space $\mathbf{F}^X$? Potential difficulty Unfortunately, this seems to depend on the characteristic of $\mathbf{F}$. For example, if $X=\{a,b,c\}$ and $\mathcal{A}$ consists of all two-element subsets of $X$, then $\{1_A:A\in\mathcal{A}\}$ is dependent when $\mathrm{char}(\mathbf{F})=2$, but independent otherwise. Partial answer All I can say so far is that $\mathcal{A}$ cannot contain an inclusion-exclusion family by which I mean a collection of the form $$\{A,B,A\cap B, A\cup B\}\text{ or}$$ $$\{A,B,C,A\cap B, A\cap C, B\cap C, A\cap B\cap C, A\cup B \cup C\} \text{ or so on}.$$",,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'inclusion-exclusion']"
68,Prove that KKT matrix has at least $n-m$ positive and $m$ negative eigenvalues.,Prove that KKT matrix has at least  positive and  negative eigenvalues.,n-m m,"I would like to prove that KTT matrix defined below has at least $n-m$ positive and at least $m$ negative eigenvalues. $$ K:= \begin{bmatrix} H & A^{T} \\ A & 0 \end{bmatrix} $$ where $H \in \Bbb R^{n \times n}$ is symmetric, $A \in \Bbb R^{m \times n}$ full rank, $\operatorname{rank} (A) = m$ . Assume $H$ is positive definite , $x^{T} H x > 0$ on the nullspace of $A$ , $A x = 0$ where $x \neq 0$ . I also wonder if we can say that null( $H$ ) spans the same vectors as null( $A$ ) since it is on the null( $A$ )? What is the intuitive meaning of being on the nullspace of a matrix? Any hints would be appreciated! Thanks!","I would like to prove that KTT matrix defined below has at least positive and at least negative eigenvalues. where is symmetric, full rank, . Assume is positive definite , on the nullspace of , where . I also wonder if we can say that null( ) spans the same vectors as null( ) since it is on the null( )? What is the intuitive meaning of being on the nullspace of a matrix? Any hints would be appreciated! Thanks!",n-m m  K:= \begin{bmatrix} H & A^{T} \\ A & 0 \end{bmatrix}  H \in \Bbb R^{n \times n} A \in \Bbb R^{m \times n} \operatorname{rank} (A) = m H x^{T} H x > 0 A A x = 0 x \neq 0 H A A,"['linear-algebra', 'matrices', 'nonlinear-optimization', 'block-matrices']"
69,Maximization of sum of squared Frobenius norms,Maximization of sum of squared Frobenius norms,,"Let $\{X_1,\dots,X_K\}$ is a set of random matrices, where $X_k\in\mathbb{R}^{M\times N}, k=1,\dots,K$, and $U\in\mathbb{R}^{M\times r}$ and $V\in\mathbb{R}^{N\times r}$ are two matrices containing orthogonal columns (i.e.,  $U^\top U =I, V^\top V =I$). I was wondering, if the following question has a analytical solution: $$\displaystyle\max_{U,V} \sum_{k=1}^K \|U^\top X_k V\|_F^2$$ If not, how should I solve it? Alternating optimization? (At first, I thought it may be related to the SVD of the sum of the matrices $\{X_k\}$, but so far I have no hint to prove it.)","Let $\{X_1,\dots,X_K\}$ is a set of random matrices, where $X_k\in\mathbb{R}^{M\times N}, k=1,\dots,K$, and $U\in\mathbb{R}^{M\times r}$ and $V\in\mathbb{R}^{N\times r}$ are two matrices containing orthogonal columns (i.e.,  $U^\top U =I, V^\top V =I$). I was wondering, if the following question has a analytical solution: $$\displaystyle\max_{U,V} \sum_{k=1}^K \|U^\top X_k V\|_F^2$$ If not, how should I solve it? Alternating optimization? (At first, I thought it may be related to the SVD of the sum of the matrices $\{X_k\}$, but so far I have no hint to prove it.)",,"['optimization', 'matrices', 'linear-algebra', 'matrix-calculus']"
70,Coefficient of characteristic polynomial as sum of principal minors,Coefficient of characteristic polynomial as sum of principal minors,,"Horn and Johnson in ""Matrix Analysis"" leave as a exercise this proof: Let A be a real matrix with characteristic polynomial $p(\lambda)$ where    $$p(\lambda) = \lambda^n + c_{1}\lambda^{n-1} + c_2\lambda^{n-2}+\cdots+c_n.$$   Let $E_k = \sum C_{kk}$ where $C_{kk}$ means a $k$-by-$k$ principal minor of $A$, and the summation is over all $k$-by-$k$ principal minors. Then,   $$p(\lambda) = \lambda^n + E_1\lambda^{n-1}+E_2\lambda^{n-2}+\cdots+E_n.$$ The authors say that this can be proved by mathematical induction, using the Laplace expansion. I have written out the base case and the induction hypothesis. My assumption is that the induction is on the dimension of the matrix, although now I am not sure at this point as I don't know what to do from here. Could anyone give me a hint or help as to what I should do next?","Horn and Johnson in ""Matrix Analysis"" leave as a exercise this proof: Let A be a real matrix with characteristic polynomial $p(\lambda)$ where    $$p(\lambda) = \lambda^n + c_{1}\lambda^{n-1} + c_2\lambda^{n-2}+\cdots+c_n.$$   Let $E_k = \sum C_{kk}$ where $C_{kk}$ means a $k$-by-$k$ principal minor of $A$, and the summation is over all $k$-by-$k$ principal minors. Then,   $$p(\lambda) = \lambda^n + E_1\lambda^{n-1}+E_2\lambda^{n-2}+\cdots+E_n.$$ The authors say that this can be proved by mathematical induction, using the Laplace expansion. I have written out the base case and the induction hypothesis. My assumption is that the induction is on the dimension of the matrix, although now I am not sure at this point as I don't know what to do from here. Could anyone give me a hint or help as to what I should do next?",,"['linear-algebra', 'matrices', 'characteristic-polynomial']"
71,Eigenvectors and eigenvalues in iterative methods,Eigenvectors and eigenvalues in iterative methods,,"I've been studying many iterative methods, like Jacobi, fixed-point, Newton's and the conjugate gradient methods. Currently, I'm studying the CG method, but it's not the first time where the eigenvectors (and eigenvalues) of the usual matrix involved are important in determining how the iterative method behaves, like what's the convergence of the method. In particular, I was reading the book ""A first course in numerical methods"" (by Greif, Ascher) and at a certain point (pp. 186-7) there's: If the eigenvalues of A are located in only a few narrow clusters, then the CG method requires only a few iterations to converge. Convergence is slower, though, if the eigenvalues are widely spread. which makes me wonder why, why if the eigenvalues of $A$ are located in an only a few narrow clusters the CG converges fast? This is not the first time that I see similar observations where the eigenvalues and eigenvectors of the matrix involved in the iterative method somehow determine the behavior or performance of the iterative method. My questions, apart from the specific case above, are: What are the relations between the eigenvectors and eigenvalues of the matrices involved in an iterative method and the behaviour of the iterative method (if this can be generalized)? What are the general behaviors of iterative methods that we can predict given the eigenvectors or eigenvalues? If this can't be generalized, either I could ask specific questions, or you could like list a few examples where eigenvectors and eigenvalues influence different iterative methods. Of course, an exhaustive list would be nice for everyone. Note: I know what are eigenvectors and eigenvalues, even though when I think about them I can't really make sense of why they are useful. You could argue that they are useful like in the example above, but I can't see the relation very well and clearly.","I've been studying many iterative methods, like Jacobi, fixed-point, Newton's and the conjugate gradient methods. Currently, I'm studying the CG method, but it's not the first time where the eigenvectors (and eigenvalues) of the usual matrix involved are important in determining how the iterative method behaves, like what's the convergence of the method. In particular, I was reading the book ""A first course in numerical methods"" (by Greif, Ascher) and at a certain point (pp. 186-7) there's: If the eigenvalues of A are located in only a few narrow clusters, then the CG method requires only a few iterations to converge. Convergence is slower, though, if the eigenvalues are widely spread. which makes me wonder why, why if the eigenvalues of $A$ are located in an only a few narrow clusters the CG converges fast? This is not the first time that I see similar observations where the eigenvalues and eigenvectors of the matrix involved in the iterative method somehow determine the behavior or performance of the iterative method. My questions, apart from the specific case above, are: What are the relations between the eigenvectors and eigenvalues of the matrices involved in an iterative method and the behaviour of the iterative method (if this can be generalized)? What are the general behaviors of iterative methods that we can predict given the eigenvectors or eigenvalues? If this can't be generalized, either I could ask specific questions, or you could like list a few examples where eigenvectors and eigenvalues influence different iterative methods. Of course, an exhaustive list would be nice for everyone. Note: I know what are eigenvectors and eigenvalues, even though when I think about them I can't really make sense of why they are useful. You could argue that they are useful like in the example above, but I can't see the relation very well and clearly.",,['linear-algebra']
72,Are there linear transformations from vector spaces over different fields?,Are there linear transformations from vector spaces over different fields?,,"I'm in a matrix theory class, and today we started talking about linear transformations. My professor noted that the range and domain of a linear transformation must be vector spaces over the same field. This made sense to me at first because of the fields are different, then something like $T(cx)=cT(x)$ doesn't make any sense when c is only in one of the vector spaces. But then I thought that maybe some fields might be compatible, like for example, $\mathbb{Z}_5$ and $\mathbb{Z}_7$ Are there any linear transformations for two spaces over different fields? or Are there functions that have all the same properties as linear transformations except that the spaces involved use different fields? If the answer is no, can you prove it?","I'm in a matrix theory class, and today we started talking about linear transformations. My professor noted that the range and domain of a linear transformation must be vector spaces over the same field. This made sense to me at first because of the fields are different, then something like $T(cx)=cT(x)$ doesn't make any sense when c is only in one of the vector spaces. But then I thought that maybe some fields might be compatible, like for example, $\mathbb{Z}_5$ and $\mathbb{Z}_7$ Are there any linear transformations for two spaces over different fields? or Are there functions that have all the same properties as linear transformations except that the spaces involved use different fields? If the answer is no, can you prove it?",,"['linear-algebra', 'abstract-algebra', 'linear-transformations']"
73,Number of $m$-dimensional subspaces of $V$ is same as number of $(n-m)$-dimensional subspaces.,Number of -dimensional subspaces of  is same as number of -dimensional subspaces.,m V (n-m),"Let $V$ be an $n$-dimensional vector space over a finite field $F$. For $ 0\le m \le n$, the number of $m$-dimensional subspaces of $V$ is same as the number of $(n-m)$-dimensional subspaces. I tried using the following argument: Corresponding to every $m$ dimensional subspace, we can find a complementary subspace of dimension $(n-m)$. So, $$ \text{number of subspaces of dimension }m\le \text{number of subspaces of dimension }(n-m) $$ Reversing the argument, we get they are equal. Is this correct?","Let $V$ be an $n$-dimensional vector space over a finite field $F$. For $ 0\le m \le n$, the number of $m$-dimensional subspaces of $V$ is same as the number of $(n-m)$-dimensional subspaces. I tried using the following argument: Corresponding to every $m$ dimensional subspace, we can find a complementary subspace of dimension $(n-m)$. So, $$ \text{number of subspaces of dimension }m\le \text{number of subspaces of dimension }(n-m) $$ Reversing the argument, we get they are equal. Is this correct?",,"['linear-algebra', 'vector-spaces', 'finite-fields']"
74,"What are some applications of modules, but not of vector spaces? [closed]","What are some applications of modules, but not of vector spaces? [closed]",,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question Some background: I'm taking Matrix Analysis, and my professor seems to think vector spaces are foundational to all of mathematics. What are some interesting applications from other areas of mathematics (or the outside world) that arise from modules, but are not applications of vector spaces?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question Some background: I'm taking Matrix Analysis, and my professor seems to think vector spaces are foundational to all of mathematics. What are some interesting applications from other areas of mathematics (or the outside world) that arise from modules, but are not applications of vector spaces?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'modules']"
75,"Prove that $\sum_{i,j} \langle v_i, v_j \rangle \langle w_i, w_j \rangle \geq 0$",Prove that,"\sum_{i,j} \langle v_i, v_j \rangle \langle w_i, w_j \rangle \geq 0","Let $v_1 \dots v_n, w_1 \dots w_n \in H$ an inner product space. I am trying (unsuccesfully) to show that $$ \sum_{i,j=1}^n \langle v_i, v_j \rangle \langle w_i, w_j \rangle \geq 0 .$$ Any hints?","Let $v_1 \dots v_n, w_1 \dots w_n \in H$ an inner product space. I am trying (unsuccesfully) to show that $$ \sum_{i,j=1}^n \langle v_i, v_j \rangle \langle w_i, w_j \rangle \geq 0 .$$ Any hints?",,"['linear-algebra', 'inequality', 'inner-products']"
76,Linear combination of a vector and its negative,Linear combination of a vector and its negative,,I'm having trouble understanding the question's answer from #19. How does the combination of a vector v and its negative fill a half space? Doesn't it only fill a line?,I'm having trouble understanding the question's answer from #19. How does the combination of a vector v and its negative fill a half space? Doesn't it only fill a line?,,['linear-algebra']
77,Largest eigenvalue of a Hermitian Toeplitz matrix,Largest eigenvalue of a Hermitian Toeplitz matrix,,"I have two Toeplitz positive semi-definite Hermitian matrices $\mathbf{R}_1, \mathbf{R}_2 \in \mathbb{C}^{M \times M}$ . They are in fact covariance matrices satisfing the following conditions: ${\mathop{\rm diag}\nolimits}\{\mathbf{R}_1\} = d_1 \mathbf{I}_M$ and ${\mathop{\rm diag}\nolimits}\{\mathbf{R}_2\} = d_2 \mathbf{I}_M$ , where $d_1$ and $d_2$ are real numbers. The off-diagonal entries of the covariance matrix are complex with absolute value no larger than the diagonal entires. In other words, the ij-th element of $\mathbf{R}$ , namely $r_{ij}, \forall i\neq j$ , satisfies $|r_{ij}| \leq d$ , where $d$ is the diagonal element(s) of $\mathbf{R}$ . I'm interested in the largest eigenvalue (or spectral norm) of the following matrix: $$\mathbf{R}_1 (\mathbf{R}_1 + \mathbf{R}_2 +\mathbf{I}_M)^{-2} \mathbf{R}_1,$$ where $\mathbf{I}_M$ is the identity matrix. I tried with Matlab and observed that the largest eigenvalue is always smaller than $1$ . However I couldn't prove it. Is there anyone who can show me the way?","I have two Toeplitz positive semi-definite Hermitian matrices . They are in fact covariance matrices satisfing the following conditions: and , where and are real numbers. The off-diagonal entries of the covariance matrix are complex with absolute value no larger than the diagonal entires. In other words, the ij-th element of , namely , satisfies , where is the diagonal element(s) of . I'm interested in the largest eigenvalue (or spectral norm) of the following matrix: where is the identity matrix. I tried with Matlab and observed that the largest eigenvalue is always smaller than . However I couldn't prove it. Is there anyone who can show me the way?","\mathbf{R}_1, \mathbf{R}_2 \in \mathbb{C}^{M \times M} {\mathop{\rm diag}\nolimits}\{\mathbf{R}_1\} = d_1 \mathbf{I}_M {\mathop{\rm diag}\nolimits}\{\mathbf{R}_2\} = d_2 \mathbf{I}_M d_1 d_2 \mathbf{R} r_{ij}, \forall i\neq j |r_{ij}| \leq d d \mathbf{R} \mathbf{R}_1 (\mathbf{R}_1 + \mathbf{R}_2 +\mathbf{I}_M)^{-2} \mathbf{R}_1, \mathbf{I}_M 1","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'covariance', 'toeplitz-matrices']"
78,"Non-computational proof that $\det(A)$ is a unit in $R$ implies $A$ is a unit, for $A \in M_n(R)$.","Non-computational proof that  is a unit in  implies  is a unit, for .",\det(A) R A A \in M_n(R),"Quoting from Waterhouse's Introduction to Affine Group Schemes: ""...suppose we have a representable functor $G$ [from $k$-algebras to groups], and a map $\Delta: A \to A \otimes A$ giving a composition law on the $G(R)$... the units and inverses are uniquely determined and give natural maps, so by the Yoneda lemma there are uniquely determined (counit) $\epsilon$ and (coinverse) $S$ making $A$ a hopf algebra. Consider for example $n \times n$ matrices with invertible determinant, represented by $k[X_{11}, \ldots, X_{nn}, 1 /\det]$. We might use a non-computational proof to show that such matrices are invertible and thus form a group. But then we have a group scheme, and hence $S$ exists. That is, we would know a priori that something like Cramer's rule must be true - there are polynomials in the $X_{ij}$ and $1/\det$ giving the entries of the inverse matrix."" Well - cool! But what non-computational proof? I only know the one that constructs the inverse using adjugate matrics. Does anyone know a truly non-computational argument that proves $\det(A)$ invertible implies $A$ invertible?","Quoting from Waterhouse's Introduction to Affine Group Schemes: ""...suppose we have a representable functor $G$ [from $k$-algebras to groups], and a map $\Delta: A \to A \otimes A$ giving a composition law on the $G(R)$... the units and inverses are uniquely determined and give natural maps, so by the Yoneda lemma there are uniquely determined (counit) $\epsilon$ and (coinverse) $S$ making $A$ a hopf algebra. Consider for example $n \times n$ matrices with invertible determinant, represented by $k[X_{11}, \ldots, X_{nn}, 1 /\det]$. We might use a non-computational proof to show that such matrices are invertible and thus form a group. But then we have a group scheme, and hence $S$ exists. That is, we would know a priori that something like Cramer's rule must be true - there are polynomials in the $X_{ij}$ and $1/\det$ giving the entries of the inverse matrix."" Well - cool! But what non-computational proof? I only know the one that constructs the inverse using adjugate matrics. Does anyone know a truly non-computational argument that proves $\det(A)$ invertible implies $A$ invertible?",,"['linear-algebra', 'hopf-algebras', 'affine-schemes', 'group-schemes']"
79,"A question on ""Linear Algebra"" by Kenneth Hoffman","A question on ""Linear Algebra"" by Kenneth Hoffman",,"I'm reading ""Linear Algebra"" by Kenneth Hoffman and Ray Kunze. I'm now lost at $\S$6.4 Theorem 6: the proof looks OK, but when I pick an example, somehow it does not tally. Please find below the theorem and proof, and my example in $\color{blue}{\texttt{blue}}$, and question in $\color{red}{\texttt{red}}$ in step 8. Please kindly help me: where did I mistake? Theorem: Let $V$ be a finite-dimensional vector space over the field $F$ and let $T$ be a linear operator on $V$. Then $T$ is diagonalizable if and only if the minimal polynomial for $T$ has the form $p = (x - c_1) \dots (x - c_k)$ where $c_1, \dots , c_k$ are distinct elements of $F$. $$\color{blue}{\texttt{Example: Choose } V=\mathbb R^3, F=\mathbb R, T=\begin{bmatrix}        1 & 0 & 0\\       0 & 2 & 1\\       0 & 0 & 1   \end{bmatrix}, p=(x-1)(x-2), c_1=1, c_2=2 }$$ The proof is: (the (1)(2).. numbers are added by me) Proof (1) We have noted earlier that, if $T$ is diagonalizable, its minimal polynomial is a product of distinct linear factors (see the discussion prior to Example 4). (2)To prove the converse, let $W$ be the subspace spanned by all of the characteristic vectors of $T$, and suppose $W \ne V$. $$\color{blue}{\texttt{characteristic vectors:} v_1=\begin{bmatrix}        1 \\       0 \\       0    \end{bmatrix} \texttt{for } c_1=1,  v_2=\begin{bmatrix}        0 \\       1 \\       0    \end{bmatrix} \texttt{for } c_2=2.\\ W=\langle v1, v2 \rangle, W\ne V=\mathbb R^3. }$$  (3)By the lemma used in the proof of Theorem 5, there is a vector $\alpha$ not in $W$ and a characteristic value $c_j$ of $T$ such that the vector $\beta= (T - c_jI)\alpha$ lies in W.  $$\color{blue}{\texttt{Choose: } \alpha=\begin{bmatrix}        0 \\       0.5 \\       0.5    \end{bmatrix}, c_j=c_1=1, \\ \beta=\begin{bmatrix}        0&0&0 \\       0&1&1 \\       0&0&0        \end{bmatrix}    \begin{bmatrix}        0 \\       0.5 \\       0.5    \end{bmatrix}       =\begin{bmatrix}        0 \\       1 \\       0    \end{bmatrix}, \beta=v_2 \in \langle v_1, v_2 \rangle = W.  }$$  (4)Since $\beta$ is in $W$, $\beta = \beta_1+\dots\beta_k$ where $T\beta_i = c_i\beta_i$, $1\le i\le k$, and therefore the vector $h(T)\beta = h(c_1)\beta_1+\dots+h(c_k)\beta_k$ is in $W$, for every polynomial $h$. (5)Now $p = (x-c_j)q$, for some polynomial $q$.  $$\color{blue}{ p=(x-1)(x-2)=(x-1)q \Rightarrow q=(x-2)  }$$  (6)Also $q- q(c_j) = (x - c_j)h$. $$\color{blue}{ q-q(c_1)=q-q(1)=(x-2)-(1-2)=(x-1)=(x-1)h \Rightarrow h=1 }$$  (7)We have $q(T)\alpha - q(c_j)\alpha = h(T)(T - c_jI)\alpha = h(T)\beta$. $$\color{blue}{ q(T)=(T-2I)=\begin{bmatrix}        -1&0&0 \\       0&0&1 \\       0&0&-1        \end{bmatrix}, \\  q(T)\alpha=\begin{bmatrix}        -1&0&0 \\       0&0&1 \\       0&0&-1        \end{bmatrix}        \begin{bmatrix}        0 \\       0.5 \\       0.5        \end{bmatrix} =\begin{bmatrix}        0 \\       0.5 \\       -0.5        \end{bmatrix}\\ q(c_j)\alpha=q(c_1)\alpha=q(1)\alpha=(1-2)\alpha=-\alpha=\begin{bmatrix}        0 \\      - 0.5 \\      - 0.5    \end{bmatrix}, \\   q(T)\alpha-q(c_j)\alpha=\begin{bmatrix}        0 \\      1 \\      0   \end{bmatrix} = h(T)\beta = 1\beta\\ }$$  (8)But $h(T)\beta$ is in $W$ and, since $0 = p(T)\alpha = (T - c_jI)q(T)\alpha$, $$\color{blue}{ p(T)=0\\ q(T)\alpha =     \begin{bmatrix}        -1&0&0 \\       0&0&1 \\       0&0&-1        \end{bmatrix}   \begin{bmatrix}        0 \\      0.5  \\      0.5   \end{bmatrix} = \begin{bmatrix}        0 \\      0.5  \\      -0.5   \end{bmatrix} \\ 0=p(T)\alpha =  (T - c_jI)q(T)\alpha = \begin{bmatrix}        0 & 0&0\\      0& 1&1 \\      0&0&0   \end{bmatrix}      \begin{bmatrix}        0 \\      0.5  \\      -0.5   \end{bmatrix}  }$$  the vector $q(T)\alpha$ is in $W$.  $$\color{red}{ q(T)\alpha   = \begin{bmatrix}        0 \\      0.5  \\      -0.5   \end{bmatrix}      \notin     W=   \langle  v_1, v_2 \rangle = \left \langle   \begin{bmatrix}        1\\      0   \\      0   \end{bmatrix} ,     \begin{bmatrix}        0 \\      1 \\      0   \end{bmatrix}  \right \rangle\\ \texttt{--What happen? why this step does not tally?} }$$  (9)Therefore, $q(c_j)\alpha$ is in $W$. (10)Since $\alpha$ is not in $W$, we have $q(c_j) = 0$. (11)That contradicts the fact that $p$ has distinct roots. QED.","I'm reading ""Linear Algebra"" by Kenneth Hoffman and Ray Kunze. I'm now lost at $\S$6.4 Theorem 6: the proof looks OK, but when I pick an example, somehow it does not tally. Please find below the theorem and proof, and my example in $\color{blue}{\texttt{blue}}$, and question in $\color{red}{\texttt{red}}$ in step 8. Please kindly help me: where did I mistake? Theorem: Let $V$ be a finite-dimensional vector space over the field $F$ and let $T$ be a linear operator on $V$. Then $T$ is diagonalizable if and only if the minimal polynomial for $T$ has the form $p = (x - c_1) \dots (x - c_k)$ where $c_1, \dots , c_k$ are distinct elements of $F$. $$\color{blue}{\texttt{Example: Choose } V=\mathbb R^3, F=\mathbb R, T=\begin{bmatrix}        1 & 0 & 0\\       0 & 2 & 1\\       0 & 0 & 1   \end{bmatrix}, p=(x-1)(x-2), c_1=1, c_2=2 }$$ The proof is: (the (1)(2).. numbers are added by me) Proof (1) We have noted earlier that, if $T$ is diagonalizable, its minimal polynomial is a product of distinct linear factors (see the discussion prior to Example 4). (2)To prove the converse, let $W$ be the subspace spanned by all of the characteristic vectors of $T$, and suppose $W \ne V$. $$\color{blue}{\texttt{characteristic vectors:} v_1=\begin{bmatrix}        1 \\       0 \\       0    \end{bmatrix} \texttt{for } c_1=1,  v_2=\begin{bmatrix}        0 \\       1 \\       0    \end{bmatrix} \texttt{for } c_2=2.\\ W=\langle v1, v2 \rangle, W\ne V=\mathbb R^3. }$$  (3)By the lemma used in the proof of Theorem 5, there is a vector $\alpha$ not in $W$ and a characteristic value $c_j$ of $T$ such that the vector $\beta= (T - c_jI)\alpha$ lies in W.  $$\color{blue}{\texttt{Choose: } \alpha=\begin{bmatrix}        0 \\       0.5 \\       0.5    \end{bmatrix}, c_j=c_1=1, \\ \beta=\begin{bmatrix}        0&0&0 \\       0&1&1 \\       0&0&0        \end{bmatrix}    \begin{bmatrix}        0 \\       0.5 \\       0.5    \end{bmatrix}       =\begin{bmatrix}        0 \\       1 \\       0    \end{bmatrix}, \beta=v_2 \in \langle v_1, v_2 \rangle = W.  }$$  (4)Since $\beta$ is in $W$, $\beta = \beta_1+\dots\beta_k$ where $T\beta_i = c_i\beta_i$, $1\le i\le k$, and therefore the vector $h(T)\beta = h(c_1)\beta_1+\dots+h(c_k)\beta_k$ is in $W$, for every polynomial $h$. (5)Now $p = (x-c_j)q$, for some polynomial $q$.  $$\color{blue}{ p=(x-1)(x-2)=(x-1)q \Rightarrow q=(x-2)  }$$  (6)Also $q- q(c_j) = (x - c_j)h$. $$\color{blue}{ q-q(c_1)=q-q(1)=(x-2)-(1-2)=(x-1)=(x-1)h \Rightarrow h=1 }$$  (7)We have $q(T)\alpha - q(c_j)\alpha = h(T)(T - c_jI)\alpha = h(T)\beta$. $$\color{blue}{ q(T)=(T-2I)=\begin{bmatrix}        -1&0&0 \\       0&0&1 \\       0&0&-1        \end{bmatrix}, \\  q(T)\alpha=\begin{bmatrix}        -1&0&0 \\       0&0&1 \\       0&0&-1        \end{bmatrix}        \begin{bmatrix}        0 \\       0.5 \\       0.5        \end{bmatrix} =\begin{bmatrix}        0 \\       0.5 \\       -0.5        \end{bmatrix}\\ q(c_j)\alpha=q(c_1)\alpha=q(1)\alpha=(1-2)\alpha=-\alpha=\begin{bmatrix}        0 \\      - 0.5 \\      - 0.5    \end{bmatrix}, \\   q(T)\alpha-q(c_j)\alpha=\begin{bmatrix}        0 \\      1 \\      0   \end{bmatrix} = h(T)\beta = 1\beta\\ }$$  (8)But $h(T)\beta$ is in $W$ and, since $0 = p(T)\alpha = (T - c_jI)q(T)\alpha$, $$\color{blue}{ p(T)=0\\ q(T)\alpha =     \begin{bmatrix}        -1&0&0 \\       0&0&1 \\       0&0&-1        \end{bmatrix}   \begin{bmatrix}        0 \\      0.5  \\      0.5   \end{bmatrix} = \begin{bmatrix}        0 \\      0.5  \\      -0.5   \end{bmatrix} \\ 0=p(T)\alpha =  (T - c_jI)q(T)\alpha = \begin{bmatrix}        0 & 0&0\\      0& 1&1 \\      0&0&0   \end{bmatrix}      \begin{bmatrix}        0 \\      0.5  \\      -0.5   \end{bmatrix}  }$$  the vector $q(T)\alpha$ is in $W$.  $$\color{red}{ q(T)\alpha   = \begin{bmatrix}        0 \\      0.5  \\      -0.5   \end{bmatrix}      \notin     W=   \langle  v_1, v_2 \rangle = \left \langle   \begin{bmatrix}        1\\      0   \\      0   \end{bmatrix} ,     \begin{bmatrix}        0 \\      1 \\      0   \end{bmatrix}  \right \rangle\\ \texttt{--What happen? why this step does not tally?} }$$  (9)Therefore, $q(c_j)\alpha$ is in $W$. (10)Since $\alpha$ is not in $W$, we have $q(c_j) = 0$. (11)That contradicts the fact that $p$ has distinct roots. QED.",,['linear-algebra']
80,Amount of solutions to the Diophantine equation of Frobenius,Amount of solutions to the Diophantine equation of Frobenius,,"The Diophantine equation of Frobenius is any equation of the form: $$\sum_{i=1}^k a_i x_i = n$$ where the $a_i$'s are given and so are $k$ and $n$. I'm looking for an algorithm to compute the number of solutions in non-negative integers to such equation. Given as input an array of the $a_i$'s and $n$, I need the algorithm to produce me the AMOUNT of solutions to the equation, regardless of what the solutions are. I've searched all over the internet (literally), including this site and https://stackoverflow.com/ . I have found many articles about it and similiar questions, and all of which I've read, but I have not found particularly what I'm looking for. The most ""useful"" article I've found is : http://sertoz.bilkent.edu.tr/papers/froben.pdf which provides an explicit formula at the very end. However, with the lack of mathematical knowledge and recognition of the notations, due to me not being a student of mathematics/combinatorics, I cannot compute the algorithm to the formula, nor be sure it's really ""explicit"" and simple. I found a book with which i've gotten this far (read over 100 pages), which provides explicit formulas for such solutions but for very simple diophantine equations.  Whether or not formulas exist for larger $k$'s (up to 20) and $n$'s (up to 5000), I know FOR SURE these equations can be solved algorithmically and efficiently, as I know of (very very few) people who have done so. I've also tried to create such efficient algorithm myself but to no avail. So.. I guess that's it. If you can provide me with some insights/formulas/algorithms (if in Java then even better) then I would be.. Unspeakably happy and grateful for this. Thanks a lot in advance, Matan.","The Diophantine equation of Frobenius is any equation of the form: $$\sum_{i=1}^k a_i x_i = n$$ where the $a_i$'s are given and so are $k$ and $n$. I'm looking for an algorithm to compute the number of solutions in non-negative integers to such equation. Given as input an array of the $a_i$'s and $n$, I need the algorithm to produce me the AMOUNT of solutions to the equation, regardless of what the solutions are. I've searched all over the internet (literally), including this site and https://stackoverflow.com/ . I have found many articles about it and similiar questions, and all of which I've read, but I have not found particularly what I'm looking for. The most ""useful"" article I've found is : http://sertoz.bilkent.edu.tr/papers/froben.pdf which provides an explicit formula at the very end. However, with the lack of mathematical knowledge and recognition of the notations, due to me not being a student of mathematics/combinatorics, I cannot compute the algorithm to the formula, nor be sure it's really ""explicit"" and simple. I found a book with which i've gotten this far (read over 100 pages), which provides explicit formulas for such solutions but for very simple diophantine equations.  Whether or not formulas exist for larger $k$'s (up to 20) and $n$'s (up to 5000), I know FOR SURE these equations can be solved algorithmically and efficiently, as I know of (very very few) people who have done so. I've also tried to create such efficient algorithm myself but to no avail. So.. I guess that's it. If you can provide me with some insights/formulas/algorithms (if in Java then even better) then I would be.. Unspeakably happy and grateful for this. Thanks a lot in advance, Matan.",,"['linear-algebra', 'combinatorics', 'elementary-number-theory', 'diophantine-equations', 'systems-of-equations']"
81,Convergence of Conjugate Gradient Method for Positive Semi-Definite Matrix,Convergence of Conjugate Gradient Method for Positive Semi-Definite Matrix,,"Let $A\in\mathbb{R}^{N\times N}$   be a positive semi-definite matrix, given $b\in\mbox{Col}\left(A\right)$   we want to solve the equation system $Ax=b$  . To add some notation, we define $\left<u,v\right>_{A}:=u^{\top}Av$   and given a starting guess $x^{0}$   we define $d^{0}=r^{0}=b-Ax^{0}$  . The iterative conjugate gradient method now follows these update equations: $$\alpha^{n}:=\frac{\left(d^{n}\right)^{\top}r^{n}}{\left\Vert d^{n}\right\Vert _{A}}$$ $$x^{n+1}=x^{n}+\alpha^{n}d^{n}$$ $$r^{n+1}:=b-Ax^{n+1}$$ $$d^{n+1}=r^{n+1}-\frac{\left<d^{n},r^{n+1}\right>_{A}}{\left\Vert d^{n}\right\Vert _{A}}d^{n}  $$  Now, I have shown that given $A$   is positive semi-definite then $\left\Vert d^{n}\right\Vert _{A}=0$   iff $d^{n}=0$   and I have shown that $d^{n}\in\mbox{Col}\left(A\right)$   for all $n$  , combining these two facts shows that $\alpha^{n}$   is well defined and the method can be applied to positive semi-definite matrices. Now I want to show that given the error term $e^{n}:=\hat{x}-x^{n}$   (where $\hat{x}$   is a solution for $Ax=b$  ) and given that $e^{n}\notin\ker\left(A\right)$ we have $\left\Vert e^{n}\right\Vert _{A}\to0$ when $n\to\infty$. It would also suffice to show that $\frac{\left\Vert e^{n+1}\right\Vert _{A}}{\left\Vert e^{n}\right\Vert _{A}}<1$. I was advised to look at representation of the remainder, error and current solution in an eigenbasis of $A$   but I haven't managed to get anywhere, help would be most appreciated. Proof of monotonic error decrease: First observe that $$e_{n+1} = x_{n+1} - \hat{x} = x_{n} + \alpha_{n}d_{n} - \hat{x} = e_{n} + \alpha_{n} d_{n}$$  Thus: $$\left\Vert e_{n+1}\right\Vert _{A}^{2}=\left(e_{n}+\alpha_{n}d_{n}\right)^{\top}A\left(e_{n}+\alpha_{n}d_{n}\right)$$ $$=e_{n}^{\top}Ae_{n}+\alpha_{n}e_{n}^{\top}Ad_{n}+\alpha_{n}\overbrace{d_{n}^{\top}Ae_{n}}^{=e_{n}^{\top}Ad_{n}}+\alpha_{n}^{2}d_{n}^{\top}Ad_{n}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}+2\left(\frac{r_{n}^{\top}d_{n}}{d_{n}^{\top}Ad_{n}}\right)e_{n}^{\top}Ad_{n}+\left(\frac{r_{n}^{\top}d_{n}}{d_{n}^{\top}Ad_{n}}\right)^{2}d_{n}^{\top}Ad_{n}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}+2\left(\frac{r_{n}^{\top}d_{n}}{d_{n}^{\top}Ad_{n}}\right)e_{n}^{\top}Ad_{n}+\frac{\left(r_{n}^{\top}d_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}+2\left(\frac{\left(-Ae_{n}\right)^{\top}d_{n}}{d_{n}^{\top}Ad_{n}}\right)e_{n}^{\top}Ad_{n}+\frac{\left(\left(-Ae_{n}\right)^{\top}d_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}-2\left(\frac{e_{n}^{\top}Ad_{n}}{d_{n}^{\top}Ad_{n}}\right)\overbrace{d_{n}^{\top}Ae_{n}}^{=e_{n}^{\top}Ad_{n}}+\frac{\left(e_{n}^{\top}Ad_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}-2\frac{\left(e_{n}^{\top}Ad_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}+\frac{\left(e_{n}^{\top}Ad_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}=\left\Vert e_{n}\right\Vert _{A}^{2}-\frac{\left(e_{n}^{\top}Ad_{n}\right)^{2}}{\left\Vert d_{n}\right\Vert _{A}^{2}}<\left\Vert e_{n}\right\Vert _{A}^{2}$$ I used the easily shown equality $r_{n}=-Ae_{n}$ and he last inequality is true as long as $e_{n}\neq0$.","Let $A\in\mathbb{R}^{N\times N}$   be a positive semi-definite matrix, given $b\in\mbox{Col}\left(A\right)$   we want to solve the equation system $Ax=b$  . To add some notation, we define $\left<u,v\right>_{A}:=u^{\top}Av$   and given a starting guess $x^{0}$   we define $d^{0}=r^{0}=b-Ax^{0}$  . The iterative conjugate gradient method now follows these update equations: $$\alpha^{n}:=\frac{\left(d^{n}\right)^{\top}r^{n}}{\left\Vert d^{n}\right\Vert _{A}}$$ $$x^{n+1}=x^{n}+\alpha^{n}d^{n}$$ $$r^{n+1}:=b-Ax^{n+1}$$ $$d^{n+1}=r^{n+1}-\frac{\left<d^{n},r^{n+1}\right>_{A}}{\left\Vert d^{n}\right\Vert _{A}}d^{n}  $$  Now, I have shown that given $A$   is positive semi-definite then $\left\Vert d^{n}\right\Vert _{A}=0$   iff $d^{n}=0$   and I have shown that $d^{n}\in\mbox{Col}\left(A\right)$   for all $n$  , combining these two facts shows that $\alpha^{n}$   is well defined and the method can be applied to positive semi-definite matrices. Now I want to show that given the error term $e^{n}:=\hat{x}-x^{n}$   (where $\hat{x}$   is a solution for $Ax=b$  ) and given that $e^{n}\notin\ker\left(A\right)$ we have $\left\Vert e^{n}\right\Vert _{A}\to0$ when $n\to\infty$. It would also suffice to show that $\frac{\left\Vert e^{n+1}\right\Vert _{A}}{\left\Vert e^{n}\right\Vert _{A}}<1$. I was advised to look at representation of the remainder, error and current solution in an eigenbasis of $A$   but I haven't managed to get anywhere, help would be most appreciated. Proof of monotonic error decrease: First observe that $$e_{n+1} = x_{n+1} - \hat{x} = x_{n} + \alpha_{n}d_{n} - \hat{x} = e_{n} + \alpha_{n} d_{n}$$  Thus: $$\left\Vert e_{n+1}\right\Vert _{A}^{2}=\left(e_{n}+\alpha_{n}d_{n}\right)^{\top}A\left(e_{n}+\alpha_{n}d_{n}\right)$$ $$=e_{n}^{\top}Ae_{n}+\alpha_{n}e_{n}^{\top}Ad_{n}+\alpha_{n}\overbrace{d_{n}^{\top}Ae_{n}}^{=e_{n}^{\top}Ad_{n}}+\alpha_{n}^{2}d_{n}^{\top}Ad_{n}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}+2\left(\frac{r_{n}^{\top}d_{n}}{d_{n}^{\top}Ad_{n}}\right)e_{n}^{\top}Ad_{n}+\left(\frac{r_{n}^{\top}d_{n}}{d_{n}^{\top}Ad_{n}}\right)^{2}d_{n}^{\top}Ad_{n}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}+2\left(\frac{r_{n}^{\top}d_{n}}{d_{n}^{\top}Ad_{n}}\right)e_{n}^{\top}Ad_{n}+\frac{\left(r_{n}^{\top}d_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}+2\left(\frac{\left(-Ae_{n}\right)^{\top}d_{n}}{d_{n}^{\top}Ad_{n}}\right)e_{n}^{\top}Ad_{n}+\frac{\left(\left(-Ae_{n}\right)^{\top}d_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}-2\left(\frac{e_{n}^{\top}Ad_{n}}{d_{n}^{\top}Ad_{n}}\right)\overbrace{d_{n}^{\top}Ae_{n}}^{=e_{n}^{\top}Ad_{n}}+\frac{\left(e_{n}^{\top}Ad_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}$$ $$=\left\Vert e_{n}\right\Vert _{A}^{2}-2\frac{\left(e_{n}^{\top}Ad_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}+\frac{\left(e_{n}^{\top}Ad_{n}\right)^{2}}{d_{n}^{\top}Ad_{n}}=\left\Vert e_{n}\right\Vert _{A}^{2}-\frac{\left(e_{n}^{\top}Ad_{n}\right)^{2}}{\left\Vert d_{n}\right\Vert _{A}^{2}}<\left\Vert e_{n}\right\Vert _{A}^{2}$$ I used the easily shown equality $r_{n}=-Ae_{n}$ and he last inequality is true as long as $e_{n}\neq0$.",,"['linear-algebra', 'numerical-methods', 'systems-of-equations', 'numerical-linear-algebra']"
82,Is the limit of the dual spaces the dual space of the limit?,Is the limit of the dual spaces the dual space of the limit?,,"Note: Whenever I say ""dual space"" in this question, I mean the algebraic dual space. Consider an infinite-dimensional vector space $V$. Since it is infinite-dimensional, its double-dual $V^{**}$ is strictly larger than $V$. On the other hand, there's a natural injection $\iota:V\to V^{**}$, so by identifying $v$ with $\iota(v)$, we get $V\subsetneq V^{**}$. Of course $V^{**}$ again is an infinite dimensional vector space, so taking its double-dual gives again a larger vector space. So if we define $$V_0=V, V_{n+1}=V_n^{**}$$ and do the identification as above, we get an infinite sequence of vector spaces $$V_0\subsetneq V_1\subsetneq V_2\subsetneq\dots$$ We can now define the limit space as $$V_\infty = \bigcup_{n=0}^\infty V_n$$ It is not hard to verify that this is also a vector space: For every finite set of vectors in $V_\infty$ you find a $V_n$ which contains all of them, and thus also their linear combination, which therefore also is in $V_\infty$. Now obviously also for the dual spaces we have that $V_{n+1}^*$ is the double dual of $V_n^*$, therefore we can also define the limit space of the duals: $$W_\infty = \bigcup_{n=0}^\infty V_n^*$$ Now my question is: Is $W_\infty=V_\infty^*$? It seems intuitive that it should be, but on the other hand, each $V_n$ (except for $V_0$) is also the dual of $V_{n-1}^*$, and thus the same intuition gives that $V_\infty=W_\infty^*$. But both together cannot hold, since otherwise the double-dual of $V_\infty$ would be again $V_\infty$, which cannot be since $V_\infty$ is infinite-dimensional. So clearly the intuition doesn't help here. Indeed, that argument seems to indicate that both are not the same; however I can't see how to make that rigorous. Now clearly $W_\infty\subseteq V_\infty^*$, so assuming they are not equal, is it possible to explicitly construct an element of $V_\infty^*\setminus W_\infty$?","Note: Whenever I say ""dual space"" in this question, I mean the algebraic dual space. Consider an infinite-dimensional vector space $V$. Since it is infinite-dimensional, its double-dual $V^{**}$ is strictly larger than $V$. On the other hand, there's a natural injection $\iota:V\to V^{**}$, so by identifying $v$ with $\iota(v)$, we get $V\subsetneq V^{**}$. Of course $V^{**}$ again is an infinite dimensional vector space, so taking its double-dual gives again a larger vector space. So if we define $$V_0=V, V_{n+1}=V_n^{**}$$ and do the identification as above, we get an infinite sequence of vector spaces $$V_0\subsetneq V_1\subsetneq V_2\subsetneq\dots$$ We can now define the limit space as $$V_\infty = \bigcup_{n=0}^\infty V_n$$ It is not hard to verify that this is also a vector space: For every finite set of vectors in $V_\infty$ you find a $V_n$ which contains all of them, and thus also their linear combination, which therefore also is in $V_\infty$. Now obviously also for the dual spaces we have that $V_{n+1}^*$ is the double dual of $V_n^*$, therefore we can also define the limit space of the duals: $$W_\infty = \bigcup_{n=0}^\infty V_n^*$$ Now my question is: Is $W_\infty=V_\infty^*$? It seems intuitive that it should be, but on the other hand, each $V_n$ (except for $V_0$) is also the dual of $V_{n-1}^*$, and thus the same intuition gives that $V_\infty=W_\infty^*$. But both together cannot hold, since otherwise the double-dual of $V_\infty$ would be again $V_\infty$, which cannot be since $V_\infty$ is infinite-dimensional. So clearly the intuition doesn't help here. Indeed, that argument seems to indicate that both are not the same; however I can't see how to make that rigorous. Now clearly $W_\infty\subseteq V_\infty^*$, so assuming they are not equal, is it possible to explicitly construct an element of $V_\infty^*\setminus W_\infty$?",,['linear-algebra']
83,Why Trace and the main diagonal of a matrix are distinguished,Why Trace and the main diagonal of a matrix are distinguished,,"Let $A$ be a square $n \times n$ over a field (say $\mathbb{R}$ or $\mathbb{C}$). As we know, the main diagonal $(a_{1,1},...,a_{n,n})$ is important in linear algebra while the off-diagonal is far less important. The deep question is why? We try to test it using the trace, which lies heavily on the main diagonal of a matrix. Recall that the trace is defined as $$ \operatorname{Tr}(A) := \sum_{i=1}^n a_{i,i} \ .$$ We know that it is a good operator, in the sense it is invariant under matrix conjugation (matrix similarity: $B = M A M^{-1} \Rightarrow \operatorname{Tr}(A) = \operatorname{Tr}(B)$). Let us look on the symmetry group $S_n$ of $n!$ permutations. Let us define a more general trace: let $\sigma \in S_n$ then $$ \operatorname{Tr}_{\sigma}(A) := \sum_{i=1}^n a_{i,\sigma(i)} \ . $$ Note that $\operatorname{Tr}(A) = \operatorname{Tr}_{\operatorname{id}}(A)$ I checked on few examples that $\operatorname{Tr}_{\sigma}(A)$ is not generally invariant under matrix conjugation. The question is why? My thought is that it is due to the fact $\operatorname{Tr}(A) = \operatorname{Tr}_{\operatorname{id}}(A)$ and that $\operatorname{id} \in S_n$ is the unit element of the group $S_n$ and hence a distinguished element. However, I did not managed to show how this fact implies that the trace is invariant under matrix conjugation while $\sigma$-trace is generally not. What I did managed to show is that $$ \operatorname{Tr}_{\sigma}(AB) \ne \operatorname{Tr}_{\sigma}(BA) $$ unless $\sigma = \operatorname{id}$. I think this implies that the $\sigma$-trace isn't invariant under conjugation (as $\operatorname{Tr}(MAM^{-1}) = \operatorname{Tr}(AM^{-1}M) = \operatorname{Tr}(A)$). I want to connect between the deep fact the the identity is the unit element of $S_n$ to the fact the only the $\sigma$-trace $\operatorname{Tr}_{\operatorname{id}}(A)$ is invariant under matrix conjugation.","Let $A$ be a square $n \times n$ over a field (say $\mathbb{R}$ or $\mathbb{C}$). As we know, the main diagonal $(a_{1,1},...,a_{n,n})$ is important in linear algebra while the off-diagonal is far less important. The deep question is why? We try to test it using the trace, which lies heavily on the main diagonal of a matrix. Recall that the trace is defined as $$ \operatorname{Tr}(A) := \sum_{i=1}^n a_{i,i} \ .$$ We know that it is a good operator, in the sense it is invariant under matrix conjugation (matrix similarity: $B = M A M^{-1} \Rightarrow \operatorname{Tr}(A) = \operatorname{Tr}(B)$). Let us look on the symmetry group $S_n$ of $n!$ permutations. Let us define a more general trace: let $\sigma \in S_n$ then $$ \operatorname{Tr}_{\sigma}(A) := \sum_{i=1}^n a_{i,\sigma(i)} \ . $$ Note that $\operatorname{Tr}(A) = \operatorname{Tr}_{\operatorname{id}}(A)$ I checked on few examples that $\operatorname{Tr}_{\sigma}(A)$ is not generally invariant under matrix conjugation. The question is why? My thought is that it is due to the fact $\operatorname{Tr}(A) = \operatorname{Tr}_{\operatorname{id}}(A)$ and that $\operatorname{id} \in S_n$ is the unit element of the group $S_n$ and hence a distinguished element. However, I did not managed to show how this fact implies that the trace is invariant under matrix conjugation while $\sigma$-trace is generally not. What I did managed to show is that $$ \operatorname{Tr}_{\sigma}(AB) \ne \operatorname{Tr}_{\sigma}(BA) $$ unless $\sigma = \operatorname{id}$. I think this implies that the $\sigma$-trace isn't invariant under conjugation (as $\operatorname{Tr}(MAM^{-1}) = \operatorname{Tr}(AM^{-1}M) = \operatorname{Tr}(A)$). I want to connect between the deep fact the the identity is the unit element of $S_n$ to the fact the only the $\sigma$-trace $\operatorname{Tr}_{\operatorname{id}}(A)$ is invariant under matrix conjugation.",,['linear-algebra']
84,Angle between two polynomials,Angle between two polynomials,,"Given the inner product of two polynomials $p(X), q(X) \in P(d)$, where $P(d)$ is the vector space of all polynomials of degree less than or equal to d, with real coefficients, and using the inner product $$\langle p(x),q(X) \rangle = \int_{-1}^{1} p(X)q(X)dX$$ How can the angle $$\cos(\alpha)=\frac{\langle p(x),q(X) \rangle}{\|p(X)\|\|q(X)\|}$$ be interpreted geometrically?","Given the inner product of two polynomials $p(X), q(X) \in P(d)$, where $P(d)$ is the vector space of all polynomials of degree less than or equal to d, with real coefficients, and using the inner product $$\langle p(x),q(X) \rangle = \int_{-1}^{1} p(X)q(X)dX$$ How can the angle $$\cos(\alpha)=\frac{\langle p(x),q(X) \rangle}{\|p(X)\|\|q(X)\|}$$ be interpreted geometrically?",,"['linear-algebra', 'polynomials', 'inner-products']"
85,What is the Moore-Penrose pseudoinverse for scaled linear regression?,What is the Moore-Penrose pseudoinverse for scaled linear regression?,,"The matrix equation for linear regression is: $$ \vec{y} = X\vec{\beta}+\vec{\epsilon} $$ The Least Square Error solution of this forms the normal equations: $$ ({\bf{X}}^T \bf{X}) \vec{\beta}= {\bf{X}}^T \vec{y} $$ Using the Moore-Penrose pseudoinverse: $$ X^+ = ({\bf{X}}^T \bf{X})^{-1}{\bf{X}}^T $$ this can be written as: $$  \vec{\beta}= A^+ \vec{y} $$ Decomposing using SVD: $$ X=U\Sigma V^T$$ alledgely leads to:  $$ X^+=V \Sigma^+U^T $$ where the pseudoinverse of $ \Sigma $ is just found by taking the inverse of all nonzero diagonal elements. The point of doing things this way is that I will be doing linear regression n >> 1 times and $\vec{y}$ will vary but X (and therefore $X^+$ also) will be constant. In particular I am interested in doing ""scaled"" linear regression:  $$ \vec{y} = WX\vec{\beta}+\vec{\epsilon} $$  where W is a diagonal matrix (and in my case the diagonal elements are integer values) which change over n. The reason for this is that sometimes I have multiple measurements of $y_i$ for the same X. I understand that I could solve this using the pseudoinverse of WX but this pseudoinverse would then have to be found for each n and that defeats the point. In the case of scaled linear regression the normal equations are: $$ ({\bf{X}}^T \bf{W}^2 \bf{X}) \vec{\beta}= ({\bf{X}}^T \bf{W}^T) \vec{y} $$ If I substitute SVD into this equation what is then the equation for $ \vec{\beta} $? I am hoping for some cheap operation so that: $$  \vec{\beta}= operation(W,X^+,\vec{y}) $$ As an example something like $$  \vec{\beta}= (WX^+)\vec{y} $$ would be ideal (this example is probable not correct though), since I would just find the pseudoinverse of X once (costly) and then multiply it with a diagonal matrix and a vector (cheap) for each n.","The matrix equation for linear regression is: $$ \vec{y} = X\vec{\beta}+\vec{\epsilon} $$ The Least Square Error solution of this forms the normal equations: $$ ({\bf{X}}^T \bf{X}) \vec{\beta}= {\bf{X}}^T \vec{y} $$ Using the Moore-Penrose pseudoinverse: $$ X^+ = ({\bf{X}}^T \bf{X})^{-1}{\bf{X}}^T $$ this can be written as: $$  \vec{\beta}= A^+ \vec{y} $$ Decomposing using SVD: $$ X=U\Sigma V^T$$ alledgely leads to:  $$ X^+=V \Sigma^+U^T $$ where the pseudoinverse of $ \Sigma $ is just found by taking the inverse of all nonzero diagonal elements. The point of doing things this way is that I will be doing linear regression n >> 1 times and $\vec{y}$ will vary but X (and therefore $X^+$ also) will be constant. In particular I am interested in doing ""scaled"" linear regression:  $$ \vec{y} = WX\vec{\beta}+\vec{\epsilon} $$  where W is a diagonal matrix (and in my case the diagonal elements are integer values) which change over n. The reason for this is that sometimes I have multiple measurements of $y_i$ for the same X. I understand that I could solve this using the pseudoinverse of WX but this pseudoinverse would then have to be found for each n and that defeats the point. In the case of scaled linear regression the normal equations are: $$ ({\bf{X}}^T \bf{W}^2 \bf{X}) \vec{\beta}= ({\bf{X}}^T \bf{W}^T) \vec{y} $$ If I substitute SVD into this equation what is then the equation for $ \vec{\beta} $? I am hoping for some cheap operation so that: $$  \vec{\beta}= operation(W,X^+,\vec{y}) $$ As an example something like $$  \vec{\beta}= (WX^+)\vec{y} $$ would be ideal (this example is probable not correct though), since I would just find the pseudoinverse of X once (costly) and then multiply it with a diagonal matrix and a vector (cheap) for each n.",,"['linear-algebra', 'numerical-linear-algebra', 'regression', 'pseudoinverse']"
86,"What is the limit $\lim\limits_{(x,y)\to(1,1),\ (x,y)\in S}(1-x^py^q)(1-x^ry^s)\sum_{p/q\le m/n\le r/s}x^my^n$?",What is the limit ?,"\lim\limits_{(x,y)\to(1,1),\ (x,y)\in S}(1-x^py^q)(1-x^ry^s)\sum_{p/q\le m/n\le r/s}x^my^n","Let $S=[0,1)^2$ and $m,n$ are positive integers and $p/q,r/s$ are positive rationals with $p/q<r/s$. What is the limit $$\lim\limits_{(x,y)\to(1,1),\ (x,y)\in S}(1-x^py^q)(1-x^ry^s)\sum_{p/q\le m/n\le r/s}x^my^n?$$ The answer is $qr-ps$. Interesting this is the determinant of matrix $\begin{pmatrix} r & s \\ p & q \end{pmatrix}$. The sum $\sum_{p/q\le m/n\le r/s}x^my^n$ also seems to be of the form $-1+(1+(x^ry^s)+(x^ry^s)^2+\cdots)(1+(x^py^q)+(x^py^q)^2+\cdots)P(x,y),$ where $P(x,y)$ is a polynomial with $qr-ps$ terms, and each term $x^ay^b$ correspond to the lattice point $(a,b)$ inside the paralleogram spanned by $(p,q)$ and $(r,s)$. In other words, $(a,b)=u(p,q)+v(r,s)$ where $0\le u,v<1$, $u,v\in\mathbb{Q}$. But I don't know how to prove the number of lattice points is the determinant.","Let $S=[0,1)^2$ and $m,n$ are positive integers and $p/q,r/s$ are positive rationals with $p/q<r/s$. What is the limit $$\lim\limits_{(x,y)\to(1,1),\ (x,y)\in S}(1-x^py^q)(1-x^ry^s)\sum_{p/q\le m/n\le r/s}x^my^n?$$ The answer is $qr-ps$. Interesting this is the determinant of matrix $\begin{pmatrix} r & s \\ p & q \end{pmatrix}$. The sum $\sum_{p/q\le m/n\le r/s}x^my^n$ also seems to be of the form $-1+(1+(x^ry^s)+(x^ry^s)^2+\cdots)(1+(x^py^q)+(x^py^q)^2+\cdots)P(x,y),$ where $P(x,y)$ is a polynomial with $qr-ps$ terms, and each term $x^ay^b$ correspond to the lattice point $(a,b)$ inside the paralleogram spanned by $(p,q)$ and $(r,s)$. In other words, $(a,b)=u(p,q)+v(r,s)$ where $0\le u,v<1$, $u,v\in\mathbb{Q}$. But I don't know how to prove the number of lattice points is the determinant.",,"['linear-algebra', 'determinant', 'integer-lattices']"
87,"How can I tell if a matrix can be LU decomposed without actually finding the L, U?","How can I tell if a matrix can be LU decomposed without actually finding the L, U?",,"I've seen quite a few problems like that. For example, suppose we have the following A matrix: \begin{pmatrix}  5 & 1 & 1 & 1 & 0 &1\\   2 & 6 & -1 & 0 & -1 &1\\   {1} & {3} & {-9} & {2} & {-1} &1\\  {2} & {3} & {4} & {12} & {-1} &0\\   {1} & {1} & {1} & {2} & {9} &8\\   {0} & {0} & {0} & {0} & {-3} &0  \end{pmatrix} and the problem given is to find out without actually doing the calculations to find the L & U, that the matrix can be decomposed to a LU product (A=LU) without doing any row exchanges (which means there's no P matrix used (or P=I if you will)). The only hint given is that one should use the last row of the matrix to find out that the determinant of the matrix isn't 0. Now, as far as I understand: a) If the determinant of a A matrix is not 0, then the A matrix is invertible. b) If a A matrix is invertible and the determinants of the A[1...k, 1...k] matrices are not 0 (which makes those invertible as well), then the A matrix can be decomposed to LU. So, if per the suggestion, I use the last row to find the determinant of the matrix, due to the zeroes, I get detA=-3*detA$_{65}$ (where detA$_{65}$ is the determinant of the matrix A if we remove row 6 and column 5) right? Now that determinant still requires a bit of work (which troubles me I am not solving this right..), but anyway, if I continue the calculations, its value is non 0. But still that only proved that A is invertible. Now I have to find the determinants of 6 matrices: 1) \begin{pmatrix}5 \end{pmatrix} 2) \begin{pmatrix}  5 & 1 \\   2 & 6  \end{pmatrix} 3) \begin{pmatrix}  5 & 1 & 1\\   2 & 6 & -1\\  2 & 6 & -9\\  \end{pmatrix} ....... and if none of those are 0, then and only then I can answer that the A matrix can be decomposed to LU, right? Now, that's quite a lot of work, which makes me think I am not solving correctly the problem. Any ideas? PS Excuse me for the crappy typesetting at times, this is my first time here and I did my best to learn and use LaTeX but I still have a lot to learn.","I've seen quite a few problems like that. For example, suppose we have the following A matrix: \begin{pmatrix}  5 & 1 & 1 & 1 & 0 &1\\   2 & 6 & -1 & 0 & -1 &1\\   {1} & {3} & {-9} & {2} & {-1} &1\\  {2} & {3} & {4} & {12} & {-1} &0\\   {1} & {1} & {1} & {2} & {9} &8\\   {0} & {0} & {0} & {0} & {-3} &0  \end{pmatrix} and the problem given is to find out without actually doing the calculations to find the L & U, that the matrix can be decomposed to a LU product (A=LU) without doing any row exchanges (which means there's no P matrix used (or P=I if you will)). The only hint given is that one should use the last row of the matrix to find out that the determinant of the matrix isn't 0. Now, as far as I understand: a) If the determinant of a A matrix is not 0, then the A matrix is invertible. b) If a A matrix is invertible and the determinants of the A[1...k, 1...k] matrices are not 0 (which makes those invertible as well), then the A matrix can be decomposed to LU. So, if per the suggestion, I use the last row to find the determinant of the matrix, due to the zeroes, I get detA=-3*detA$_{65}$ (where detA$_{65}$ is the determinant of the matrix A if we remove row 6 and column 5) right? Now that determinant still requires a bit of work (which troubles me I am not solving this right..), but anyway, if I continue the calculations, its value is non 0. But still that only proved that A is invertible. Now I have to find the determinants of 6 matrices: 1) \begin{pmatrix}5 \end{pmatrix} 2) \begin{pmatrix}  5 & 1 \\   2 & 6  \end{pmatrix} 3) \begin{pmatrix}  5 & 1 & 1\\   2 & 6 & -1\\  2 & 6 & -9\\  \end{pmatrix} ....... and if none of those are 0, then and only then I can answer that the A matrix can be decomposed to LU, right? Now, that's quite a lot of work, which makes me think I am not solving correctly the problem. Any ideas? PS Excuse me for the crappy typesetting at times, this is my first time here and I did my best to learn and use LaTeX but I still have a lot to learn.",,"['linear-algebra', 'matrices']"
88,Largest eigenvalue of a symmetric positive definite matrix with rank-one updates,Largest eigenvalue of a symmetric positive definite matrix with rank-one updates,,"I have a $n \times n$ symmetric positive definite matrix $A$ which I will repeatedly update using two consecutive rank-one updates of the form $A' = A + e_j u^T +u e_j^T$ where $\{e_i: 1 \leq i \leq n\}$ is the standard basis. I also compute the updates to $A^{-1}$ using Sherman-Morrison. Due to the nature of the updates, the matrix $A'$ is guaranteed to be non-singular and positive definite. I would like to keep track of the largest and smallest eigenvalue of the matrix. Since I have the inverse, a method for calculating the largest (or smallest) eigenvalue would suffice. I know I can calculate the eigendecomposition of $A$ and update it in $O(n^2)$ but I was wondering if there was a more efficient method seeing as I only care about one particular eigenvalue (and not at all about the eigenvectors). A lower bound on the eigenvalue, might also be helpful, but it would have to be tight. Gershgorin discs seem too loose. Finally, if I do have to go via the eigendecomposition route, any pointers to what algorithms are used in practice for computational efficiency and numerical stability?","I have a $n \times n$ symmetric positive definite matrix $A$ which I will repeatedly update using two consecutive rank-one updates of the form $A' = A + e_j u^T +u e_j^T$ where $\{e_i: 1 \leq i \leq n\}$ is the standard basis. I also compute the updates to $A^{-1}$ using Sherman-Morrison. Due to the nature of the updates, the matrix $A'$ is guaranteed to be non-singular and positive definite. I would like to keep track of the largest and smallest eigenvalue of the matrix. Since I have the inverse, a method for calculating the largest (or smallest) eigenvalue would suffice. I know I can calculate the eigendecomposition of $A$ and update it in $O(n^2)$ but I was wondering if there was a more efficient method seeing as I only care about one particular eigenvalue (and not at all about the eigenvectors). A lower bound on the eigenvalue, might also be helpful, but it would have to be tight. Gershgorin discs seem too loose. Finally, if I do have to go via the eigendecomposition route, any pointers to what algorithms are used in practice for computational efficiency and numerical stability?",,"['linear-algebra', 'matrices']"
89,Finiteness of groups preserving a symmetric positive definite bilinear form,Finiteness of groups preserving a symmetric positive definite bilinear form,,"This question arises from reading the note Hodge cycles on abelian varieties by P. Deligne (notes by J.S. Milne).  Suppose we are given a group $G$ (for example,  either a fundamental group $\pi_1(S, s_0)$ or a Galois group $Gal(k/k_0)$) acting on a finite dimensional $\mathbb{Q}$-vectors space.   It seems to me that the author concludes that the action factors through a finite subgroup once it is known there is a symmetric  positive definite $\mathbb{Q}$-valued bilinear form on $V$.  Of course, this is note true in general (Take $V=\mathbb{Q}^2$ with the standard inner product, for example.)  So I am wondering what is the missing link in my understanding of the proofs.  The proofs in question are Theorem 2.15 and (iii) of Propositin 2.9(b).   Why does the action factors through finite subgroups in those proofs?","This question arises from reading the note Hodge cycles on abelian varieties by P. Deligne (notes by J.S. Milne).  Suppose we are given a group $G$ (for example,  either a fundamental group $\pi_1(S, s_0)$ or a Galois group $Gal(k/k_0)$) acting on a finite dimensional $\mathbb{Q}$-vectors space.   It seems to me that the author concludes that the action factors through a finite subgroup once it is known there is a symmetric  positive definite $\mathbb{Q}$-valued bilinear form on $V$.  Of course, this is note true in general (Take $V=\mathbb{Q}^2$ with the standard inner product, for example.)  So I am wondering what is the missing link in my understanding of the proofs.  The proofs in question are Theorem 2.15 and (iii) of Propositin 2.9(b).   Why does the action factors through finite subgroups in those proofs?",,"['linear-algebra', 'group-theory', 'algebraic-geometry']"
90,How to solve system of equations with multiple constraints?,How to solve system of equations with multiple constraints?,,"I have a system of equations that looks like this: $$\begin{array}{rl} a_1 b_1 c_1+a_2 b_2 c_2+a_3 b_3 c_3&=1000\\ a_1+a_2+a_3&=1\\ a_2&=0.6 \,a_1\\ b_1+b_2+b_3&=500 \end{array}$$ and $$a_1 a_2 a_3 b_1 b_2 b_3 c_1 c_2 c_3 > 0$$ $c_1,c_2,c_3$ are free. I have no experience with linear programming, some in linear algebra. How do I go about finding the optimal solution so that $a_2b_2c_2$ is maximized?","I have a system of equations that looks like this: $$\begin{array}{rl} a_1 b_1 c_1+a_2 b_2 c_2+a_3 b_3 c_3&=1000\\ a_1+a_2+a_3&=1\\ a_2&=0.6 \,a_1\\ b_1+b_2+b_3&=500 \end{array}$$ and $$a_1 a_2 a_3 b_1 b_2 b_3 c_1 c_2 c_3 > 0$$ $c_1,c_2,c_3$ are free. I have no experience with linear programming, some in linear algebra. How do I go about finding the optimal solution so that $a_2b_2c_2$ is maximized?",,"['linear-algebra', 'matrices', 'linear-programming']"
91,Learning linear algebra and probability by a challenge each day,Learning linear algebra and probability by a challenge each day,,"For a few weeks, I spend some time to solve a question from Project Euler each day. I liked it and noticed that it makes me motivated. I wonder if there exists such a site with challenging questions on especially topics such as linear algebra and probability which I would like to be good at. Do you know any challenge site with varying levels of difficulties? P.S: Math.SE is even a good challenging resource where you may get badges and reputation but I'm more interested in a site like Project Euler or Python Challenge . Besides, it is difficult for a learner to answer questions in Math.SE faster than the experts on the topic, so Math.SE is not a good fit for motivation I intend to mean.","For a few weeks, I spend some time to solve a question from Project Euler each day. I liked it and noticed that it makes me motivated. I wonder if there exists such a site with challenging questions on especially topics such as linear algebra and probability which I would like to be good at. Do you know any challenge site with varying levels of difficulties? P.S: Math.SE is even a good challenging resource where you may get badges and reputation but I'm more interested in a site like Project Euler or Python Challenge . Besides, it is difficult for a learner to answer questions in Math.SE faster than the experts on the topic, so Math.SE is not a good fit for motivation I intend to mean.",,"['linear-algebra', 'probability', 'online-resources']"
92,"If $K=K^2$ then every automorphism of $\mbox{Aut}_K V$, where $\dim V< \infty$, is the square of some endomorphism.","If  then every automorphism of , where , is the square of some endomorphism.",K=K^2 \mbox{Aut}_K V \dim V< \infty,"I have to show the following: Let $K$ be a field such that $\mbox{char } K \neq 2$ and each element of $K$ is a square (i.e. $K^2=K$) and let $V$ be a finite-dimensional vector spaces over $K$. Then, for every automorphism $\tau \in \mbox{Aut}_K V$ there exists an endomorphism $\rho \in \mbox{End}_K V$ such that $\tau = \rho^2$. I have proved (according to the hint given in the problem) that if $\sigma$ is a nilpotent endomorphism, then there exists an endomorphism $\rho$ such that $\rho^2=1_V+\sigma$. So, I guess (although I am not sure) that under our assumptions one could show the automorphism $\tau$ can be represented as $\tau=1_V+\sigma$, where $\sigma$ is nilpotent. I'll be grateful for your help.","I have to show the following: Let $K$ be a field such that $\mbox{char } K \neq 2$ and each element of $K$ is a square (i.e. $K^2=K$) and let $V$ be a finite-dimensional vector spaces over $K$. Then, for every automorphism $\tau \in \mbox{Aut}_K V$ there exists an endomorphism $\rho \in \mbox{End}_K V$ such that $\tau = \rho^2$. I have proved (according to the hint given in the problem) that if $\sigma$ is a nilpotent endomorphism, then there exists an endomorphism $\rho$ such that $\rho^2=1_V+\sigma$. So, I guess (although I am not sure) that under our assumptions one could show the automorphism $\tau$ can be represented as $\tau=1_V+\sigma$, where $\sigma$ is nilpotent. I'll be grateful for your help.",,"['linear-algebra', 'field-theory']"
93,"General properties of eigenvalues of a Jacobian matrix when premultiplied by a symmetric, positive definite matrix?","General properties of eigenvalues of a Jacobian matrix when premultiplied by a symmetric, positive definite matrix?",,"For a particular engineering problem that I'm working on, I have computed a Jacobian matrix $J$ and there is another matrix $M$ associated with the problem. $M$ is known to be symmetric, real-valued, and positive definite. At a particular step in the process, I need to compute the eigenvalues of $MJ$. I am wondering if there are any known results about how the eigenvalues of $MJ$ relate (via shifting, scaling, or other transformations) to the eigenvalues of $J$, if at all. Statements that require $M$ to have certain extra properties would be valuable too (i.e. I realize there are dumb corner cases such as when $M=J^{-1}$, for example, so answers that meaningfully exclude cases like that but which leave open interesting results are welcome, if they exist). Note that I don't mean the classical problem of simultaneous diagonalization. This is really a computational problem at root. I'm trying to avoid needing to do a more complicated numerical solution for the eigenvalues of $MJ$ if possible. In my program, I will already have pre-computed the eigenvalues of $J$ and it would lose efficiency if, after getting the associated matrix $M$, I had to then solve the classical problem of computing the spectrum of $MJ$. The goal is make the overall numerical method faster by exploiting any knowledge that $J$ gives us about the spectrum of $MJ$. In trying to think about this, we can assume that $J$ yields an eigenbasis of $\{\lambda_{k},e_{k}\}$, so that $MJx = \sigma{x}$ can be rewritten $\sum_{k}\lambda_{k}Me_{k} = \sum_{k}\sigma\lambda_{k}e_{k}$ for any $x$ that happens to be an eigenvector of $MJ$. What kinds of situations then allow us to make statements about $\sigma$ in terms of the $\lambda_{k}$, especially for somewhat large classes of matrices $M$? The references that I have already looked through are ""Matrix Analysis"" by Horn and Johnson, Gil Strang's Linear Algebra book, and ""Matrix Computations"" by Golub and Van Loan, none of which gives any kind of usable answer. References to research papers or books that shed any light would be appreciated if (as I suspect) this turns out to be a question that's not really answerable in general and statements can only be made for narrow classes of matrices $M$.","For a particular engineering problem that I'm working on, I have computed a Jacobian matrix $J$ and there is another matrix $M$ associated with the problem. $M$ is known to be symmetric, real-valued, and positive definite. At a particular step in the process, I need to compute the eigenvalues of $MJ$. I am wondering if there are any known results about how the eigenvalues of $MJ$ relate (via shifting, scaling, or other transformations) to the eigenvalues of $J$, if at all. Statements that require $M$ to have certain extra properties would be valuable too (i.e. I realize there are dumb corner cases such as when $M=J^{-1}$, for example, so answers that meaningfully exclude cases like that but which leave open interesting results are welcome, if they exist). Note that I don't mean the classical problem of simultaneous diagonalization. This is really a computational problem at root. I'm trying to avoid needing to do a more complicated numerical solution for the eigenvalues of $MJ$ if possible. In my program, I will already have pre-computed the eigenvalues of $J$ and it would lose efficiency if, after getting the associated matrix $M$, I had to then solve the classical problem of computing the spectrum of $MJ$. The goal is make the overall numerical method faster by exploiting any knowledge that $J$ gives us about the spectrum of $MJ$. In trying to think about this, we can assume that $J$ yields an eigenbasis of $\{\lambda_{k},e_{k}\}$, so that $MJx = \sigma{x}$ can be rewritten $\sum_{k}\lambda_{k}Me_{k} = \sum_{k}\sigma\lambda_{k}e_{k}$ for any $x$ that happens to be an eigenvector of $MJ$. What kinds of situations then allow us to make statements about $\sigma$ in terms of the $\lambda_{k}$, especially for somewhat large classes of matrices $M$? The references that I have already looked through are ""Matrix Analysis"" by Horn and Johnson, Gil Strang's Linear Algebra book, and ""Matrix Computations"" by Golub and Van Loan, none of which gives any kind of usable answer. References to research papers or books that shed any light would be appreciated if (as I suspect) this turns out to be a question that's not really answerable in general and statements can only be made for narrow classes of matrices $M$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
94,Hilbert Schmidt Norm-Rank-inequality,Hilbert Schmidt Norm-Rank-inequality,,"Problem: Let $A_{n.n}$ be square complex matrix. Prove the following: $$\left \| A \right \|=\left \| A \right \|_{HS}\Leftrightarrow  rank(A)\leqslant 1$$. Where $\left \| . \right \|_{HS} $ is the Hilbert Schmidt Norm. Please read my solution and tell me whether it is correct. If not, let me know where the mistake is. Proof of the implication $\Leftarrow $: If $rank(A)=0$, then in this case $A=0$. It follows that $\left \| A \right \|=\left \| A \right \|_{HS}=0$ If $rank(A)=1$, than: $ A=\begin{pmatrix} A_{1}\\  \alpha _{2}A_{1}\\  ...\\  \alpha _{n}A_{1} \end{pmatrix}$ where $A_{1}$ is the first row of $A$ and in this case: $\left \| A \right \|=\left \| A \right \|_{HS}=\left \| A_{1} \right \|\sqrt{1+\alpha _{1}^{2}+...+\alpha _{n}^{2}}$ Proof of the implication $\Rightarrow $ We know that $\left \| A \right \|=max_{\left \| x \right \|=1}\left \| Ax \right \|$ On the other hand: $ A=\begin{pmatrix} A_{1}\\  A_{2}\\  ... \\ A_{n}  \end{pmatrix}$. So, $$\left \| Ax \right \|=\left \| \begin{pmatrix} \left \langle A_{1},x \right \rangle\\  \left \langle A_{2},x \right \rangle\\  ...\\   \left \langle A_{n},x \right \rangle\end{pmatrix} \right \|=\sqrt{\left \langle A_{1},x \right \rangle^{2}+\left \langle A_{2},x \right \rangle^{2}+...+\left \langle A_{n},x \right \rangle^{2}} $$ Where $ \left \langle A_{i},x \right \rangle$ is the inner product of $A_{i}$ and $x$ Using the Cauchy-Schwarz inequality: $\left \langle A_{i},x \right \rangle^{2}\leq \left \| A_{i} \right \|^{2}\left \| x \right \|^{2}$, we get: $\left \| A \right \|=max_{\left \| x \right \|=1}\left \| Ax \right \|\leq max_{\left \| x \right \|=1}\left \| x \right \|\left \| A \right \|_{HS}=\left \| A \right \|_{HS}$. In order to have the equality: $\left \| A \right \|=\left \| A \right \|_{HS}$, we should have: $\left \langle A_{i},x \right \rangle^{2}=\left \| A_{i} \right \|^{2}\left \| x \right \|^{2} $ which occurs only if $A_{i}=\lambda _{i} x$. So, the rows of A are dependent, which implies that $rank(A)=1$. Note that the case $rank(A)=0$ happens when $A=0$ Please let me know if my solution makes sense.","Problem: Let $A_{n.n}$ be square complex matrix. Prove the following: $$\left \| A \right \|=\left \| A \right \|_{HS}\Leftrightarrow  rank(A)\leqslant 1$$. Where $\left \| . \right \|_{HS} $ is the Hilbert Schmidt Norm. Please read my solution and tell me whether it is correct. If not, let me know where the mistake is. Proof of the implication $\Leftarrow $: If $rank(A)=0$, then in this case $A=0$. It follows that $\left \| A \right \|=\left \| A \right \|_{HS}=0$ If $rank(A)=1$, than: $ A=\begin{pmatrix} A_{1}\\  \alpha _{2}A_{1}\\  ...\\  \alpha _{n}A_{1} \end{pmatrix}$ where $A_{1}$ is the first row of $A$ and in this case: $\left \| A \right \|=\left \| A \right \|_{HS}=\left \| A_{1} \right \|\sqrt{1+\alpha _{1}^{2}+...+\alpha _{n}^{2}}$ Proof of the implication $\Rightarrow $ We know that $\left \| A \right \|=max_{\left \| x \right \|=1}\left \| Ax \right \|$ On the other hand: $ A=\begin{pmatrix} A_{1}\\  A_{2}\\  ... \\ A_{n}  \end{pmatrix}$. So, $$\left \| Ax \right \|=\left \| \begin{pmatrix} \left \langle A_{1},x \right \rangle\\  \left \langle A_{2},x \right \rangle\\  ...\\   \left \langle A_{n},x \right \rangle\end{pmatrix} \right \|=\sqrt{\left \langle A_{1},x \right \rangle^{2}+\left \langle A_{2},x \right \rangle^{2}+...+\left \langle A_{n},x \right \rangle^{2}} $$ Where $ \left \langle A_{i},x \right \rangle$ is the inner product of $A_{i}$ and $x$ Using the Cauchy-Schwarz inequality: $\left \langle A_{i},x \right \rangle^{2}\leq \left \| A_{i} \right \|^{2}\left \| x \right \|^{2}$, we get: $\left \| A \right \|=max_{\left \| x \right \|=1}\left \| Ax \right \|\leq max_{\left \| x \right \|=1}\left \| x \right \|\left \| A \right \|_{HS}=\left \| A \right \|_{HS}$. In order to have the equality: $\left \| A \right \|=\left \| A \right \|_{HS}$, we should have: $\left \langle A_{i},x \right \rangle^{2}=\left \| A_{i} \right \|^{2}\left \| x \right \|^{2} $ which occurs only if $A_{i}=\lambda _{i} x$. So, the rows of A are dependent, which implies that $rank(A)=1$. Note that the case $rank(A)=0$ happens when $A=0$ Please let me know if my solution makes sense.",,"['linear-algebra', 'matrices', 'normed-spaces']"
95,Least-squares left-inverse having smallest Frobenius norm,Least-squares left-inverse having smallest Frobenius norm,,"While trying to prove that the left-inverse of $A$ provided by the least-squares solution to $y=Ax$ has the smallest Frobenius norm, I am stuck at a point which I describe below: Let $B$ be any left-inverse of a full-rank tall matrix $A$, i.e., $BA=I$. Let the QR-decomposition: $A=QR$. In this case, $R$ is invertible since A is full-rank and $Q$ has orthonormal columns as always. I want to show that $\|B\|_F \ge \|BQ\|_F$. Any ideas? The rest of the proof to show that the least-squares left-inverse has the smallest Frobenius norm is in place and I will be done if I can show this.","While trying to prove that the left-inverse of $A$ provided by the least-squares solution to $y=Ax$ has the smallest Frobenius norm, I am stuck at a point which I describe below: Let $B$ be any left-inverse of a full-rank tall matrix $A$, i.e., $BA=I$. Let the QR-decomposition: $A=QR$. In this case, $R$ is invertible since A is full-rank and $Q$ has orthonormal columns as always. I want to show that $\|B\|_F \ge \|BQ\|_F$. Any ideas? The rest of the proof to show that the least-squares left-inverse has the smallest Frobenius norm is in place and I will be done if I can show this.",,"['linear-algebra', 'matrices', 'inequality', 'optimization']"
96,"If $A$ and $B$ are row-stochastic matrices, what can we say about $AB$?","If  and  are row-stochastic matrices, what can we say about ?",A B AB,"Let $A \in \mathbb{R}^{n \times n}$ be a symmetric row-stochastic matrix (that is, for each row, the sum over the columns is $1$) with all elements positive. Let $B \in \mathbb{R}^{n \times m}$ be a matrix with all elements positive and the sum over columns is $1$. What can we say about the product $A \times B$? What if we continuously apply $B_{t+1} \leftarrow A \times B_t$ where $B_1 = B$?","Let $A \in \mathbb{R}^{n \times n}$ be a symmetric row-stochastic matrix (that is, for each row, the sum over the columns is $1$) with all elements positive. Let $B \in \mathbb{R}^{n \times m}$ be a matrix with all elements positive and the sum over columns is $1$. What can we say about the product $A \times B$? What if we continuously apply $B_{t+1} \leftarrow A \times B_t$ where $B_1 = B$?",,"['linear-algebra', 'stochastic-processes']"
97,A direct proof of the properties of the matrix of minors,A direct proof of the properties of the matrix of minors,,"Let $A$ be an invertible $n\times n$ matrix.  Define the matrix of minors $\Delta(A)$ of $A$ to be the matrix whose $(i,j)$ entry is the determinant of the minor of $A$ with the $i$ th row and $j$ column omitted. Certainly, the most important property of $\Delta(A)$ is that it is instrumental in computing the inverse of $A$ : $$ A^{-1}= \frac{1}{\operatorname{det}(A)}\sigma\cdot \Delta(A)^T\cdot \sigma$$ where here, $\sigma$ is the diagonal matrix whose diagonal entries alternate $1$ and $-1$ . From this fact and elementary properties of the inverse, it is easy to prove the following Taking the matrix of minors is an involution up to scaling; that is, $\Delta(\Delta(A))=\operatorname{det}(A)^{n-2}\cdot A$ Taking the matrix of minors is an group homomorphism; that is, $\Delta(AB)=\Delta(A)\Delta(B)$ . If you actually write out either of these identities in terms of minors, you get a series of non-trivial-looking identities on the minors of an invertible matrix. Is this the easiest way to obtain these identities on minors?  It would bother me if it was, for a couple of reasons.  First, it seems extremely likely that the second identity above holds for non-invertible matrices.  Second, if $A$ is totally positive or non-negative (that is, every minor of every size is positive or non-negative), then so is $\Delta(A)$ .  However, the inverse of a totally positive matrix will almost never be totally positive, and so the above proof of the identities will stray off the totally positive path. Therefore, Can the identities on minors implied by the above relations on $\Delta$ be proven in a more direct way? Ideally, this proof would be 'subtraction-free', to make it more natural in the totally-positive setting. Edit: I should be a little bit more clear about what sort of thing I am looking for.  I am interested in spaces of totally-positive matrices and the corresponding algebras generated by minors on them (specifically, they are cluster algebras).  An important involution of these spaces and algebras is the matrix of minors defined above.  Note that taking inverses or cofactors does not preserve totally positivity, and so it doesn't act on this space. Mainly, I asked this question because I was annoyed at using a map whose properties couldn't be proven without passing to a different, bigger space (not that this doesn't happen all the time in math).  I was curious if there was a proof along algebraic lines, following only from the simplest relations on minors (the 3-term Plucker relations).","Let be an invertible matrix.  Define the matrix of minors of to be the matrix whose entry is the determinant of the minor of with the th row and column omitted. Certainly, the most important property of is that it is instrumental in computing the inverse of : where here, is the diagonal matrix whose diagonal entries alternate and . From this fact and elementary properties of the inverse, it is easy to prove the following Taking the matrix of minors is an involution up to scaling; that is, Taking the matrix of minors is an group homomorphism; that is, . If you actually write out either of these identities in terms of minors, you get a series of non-trivial-looking identities on the minors of an invertible matrix. Is this the easiest way to obtain these identities on minors?  It would bother me if it was, for a couple of reasons.  First, it seems extremely likely that the second identity above holds for non-invertible matrices.  Second, if is totally positive or non-negative (that is, every minor of every size is positive or non-negative), then so is .  However, the inverse of a totally positive matrix will almost never be totally positive, and so the above proof of the identities will stray off the totally positive path. Therefore, Can the identities on minors implied by the above relations on be proven in a more direct way? Ideally, this proof would be 'subtraction-free', to make it more natural in the totally-positive setting. Edit: I should be a little bit more clear about what sort of thing I am looking for.  I am interested in spaces of totally-positive matrices and the corresponding algebras generated by minors on them (specifically, they are cluster algebras).  An important involution of these spaces and algebras is the matrix of minors defined above.  Note that taking inverses or cofactors does not preserve totally positivity, and so it doesn't act on this space. Mainly, I asked this question because I was annoyed at using a map whose properties couldn't be proven without passing to a different, bigger space (not that this doesn't happen all the time in math).  I was curious if there was a proof along algebraic lines, following only from the simplest relations on minors (the 3-term Plucker relations).","A n\times n \Delta(A) A (i,j) A i j \Delta(A) A  A^{-1}= \frac{1}{\operatorname{det}(A)}\sigma\cdot \Delta(A)^T\cdot \sigma \sigma 1 -1 \Delta(\Delta(A))=\operatorname{det}(A)^{n-2}\cdot A \Delta(AB)=\Delta(A)\Delta(B) A \Delta(A) \Delta","['linear-algebra', 'matrices', 'determinant']"
98,How to compute the change in the angle between two unit norm vectors as the $\ell_1$ norm of one vector changes?,How to compute the change in the angle between two unit norm vectors as the  norm of one vector changes?,\ell_1,"Motivation Suppose that $u \in \mathbb{R}^d$ is a unit-norm vector, $\|u\| = 1$, $a, b, c$ are some positive constants and $\xi \in [0,1]$ is another constant (usually chosen close to 1). I am interested in solving the following problem $$ \sup_{v \in \mathbb{R}^d}\ (\xi + (1-\xi)\|v\|_1^2)\left(\sqrt{ \frac{a \langle u, v \rangle^2 + b}{\xi + (1-\xi)\|v\|_1^2}} - c \right) $$ subject to $\|v\| = 1$. Question While any suggestions on how to find an optimal $v$ are welcome, I am specifically interested in the following question. How to find a vector $v$ that maximizes $\langle v, u \rangle$ and satisfies $\|v\|_2 = 1$ and $\|v\|_1 = x$? A related question (that may be an easier one): given a vector $v_1$ that maximizes $\langle v_1, u \rangle$ and satisfies $\|v_1\|_2 = 1$ and $\|v_1\|_1 = x$ and another vector $v_2$ that maximizes $\langle v_2, u \rangle$ and satisfies $\|v_2\|_2 = 1$ and $\|v_2\|_1 = x + \delta$, how to find the change $\langle v_1, u \rangle - \langle v_2, u \rangle$ as a function of $\delta$?","Motivation Suppose that $u \in \mathbb{R}^d$ is a unit-norm vector, $\|u\| = 1$, $a, b, c$ are some positive constants and $\xi \in [0,1]$ is another constant (usually chosen close to 1). I am interested in solving the following problem $$ \sup_{v \in \mathbb{R}^d}\ (\xi + (1-\xi)\|v\|_1^2)\left(\sqrt{ \frac{a \langle u, v \rangle^2 + b}{\xi + (1-\xi)\|v\|_1^2}} - c \right) $$ subject to $\|v\| = 1$. Question While any suggestions on how to find an optimal $v$ are welcome, I am specifically interested in the following question. How to find a vector $v$ that maximizes $\langle v, u \rangle$ and satisfies $\|v\|_2 = 1$ and $\|v\|_1 = x$? A related question (that may be an easier one): given a vector $v_1$ that maximizes $\langle v_1, u \rangle$ and satisfies $\|v_1\|_2 = 1$ and $\|v_1\|_1 = x$ and another vector $v_2$ that maximizes $\langle v_2, u \rangle$ and satisfies $\|v_2\|_2 = 1$ and $\|v_2\|_1 = x + \delta$, how to find the change $\langle v_1, u \rangle - \langle v_2, u \rangle$ as a function of $\delta$?",,"['geometry', 'linear-algebra', 'optimization', 'euclidean-geometry']"
99,Is this a block Toeplitz matrix?,Is this a block Toeplitz matrix?,,"What is the name of the following matrix? $$\begin{pmatrix} a & b & 0 \\  c & d & 0 \\  0 & a & b\\  0& c& d& \\  b & 0 & a \\ d & 0 & c\end{pmatrix}$$ It looks like a Block Toeplitz matrix, but usually one defines those by full shifts by (in this case) $2 \times 2$ matrices. In particular, I'm interested in solving linear equations of this form. Any reference would be appreciated.","What is the name of the following matrix? It looks like a Block Toeplitz matrix, but usually one defines those by full shifts by (in this case) matrices. In particular, I'm interested in solving linear equations of this form. Any reference would be appreciated.","\begin{pmatrix} a & b & 0 \\ 
c & d & 0 \\ 
0 & a & b\\ 
0& c& d& \\ 
b & 0 & a \\
d & 0 & c\end{pmatrix} 2 \times 2","['linear-algebra', 'matrices']"
