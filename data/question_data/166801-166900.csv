,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"prove that for an idempotent matrix, trace=rank [duplicate]","prove that for an idempotent matrix, trace=rank [duplicate]",,"This question already has answers here : Proving: ""The trace of an idempotent matrix equals the rank of the matrix"" (5 answers) Closed 4 years ago . prove that for an idempotent matrix, trace=rank","This question already has answers here : Proving: ""The trace of an idempotent matrix equals the rank of the matrix"" (5 answers) Closed 4 years ago . prove that for an idempotent matrix, trace=rank",,"['linear-algebra', 'statistics']"
1,What is the distribution of an average of independent Poisson random variables?,What is the distribution of an average of independent Poisson random variables?,,"If I have random variables $X_1,X_2,\ldots,X_n$ that are Poisson distributed with parameters $λ_1,λ_2,\ldots,λ_n$, what is the distribution of $Y=1/n \sum X_i$? Who knows this can give an answer because I don't know how to do, please.","If I have random variables $X_1,X_2,\ldots,X_n$ that are Poisson distributed with parameters $λ_1,λ_2,\ldots,λ_n$, what is the distribution of $Y=1/n \sum X_i$? Who knows this can give an answer because I don't know how to do, please.",,['statistics']
2,Partition-based entropy of a sequence,Partition-based entropy of a sequence,,"The entropy $H$ of a discrete random variable $X$ is defined by $$H(X)=E[I(X)]=\sum_xP(x)I(x)=\sum_xP(x)\log P(x)^{-1}$$ where $x$ are the possible values of $X$, $P(x)$ is the probability of $x$, $E$ is the expected value operator, and $I$ is the self-information . In other words, $H(X)$ is the expected information content of $X$. Suppose we have a sequence $$s=\langle s_1,s_2,s_3,s_4,\ldots,s_n\rangle$$ Let $N(x)$ be the number of occurrences of $x$ in $s$. Assuming a uniform distribution, we can express $P(x)$ as $N(x)/n$, where $n$ is the length of the sequence. Hence we can express the entropy as $$\sum_x\frac{N(x)}{n}\log\frac{n}{N(x)}=\log n-\frac{1}{n}\sum N(x)\log N(x)$$ This allows us to calculate the entropy of the sequence based on the number of occurrences of individual symbols. For example, $$H(\langle a,b,a,b,a,b\rangle)=1$$ according to this measure. Switching the last two symbols yields the same entropy $$H(\langle a,b,a,b,b,a\rangle)=1$$ since the number of occurrences of each symbol does not change. However, it seems intuitively true that the second sequence is more ""random"" than the first sequence and hence should, in some sense, have higher entropy. Indeed, I found that taking the entropy of adjacent pairs of symbols in $s$, we obtain $$H(\langle ab,ba,ab,ba,ab\rangle)\approx 0.970951$$ $$H(\langle ab,ba,ab,bb,ba\rangle)\approx 1.52193$$ By this measure, the entropy of the second sequence is higher than that of the first sequence. We can do the same for triplets of symbols as well: $$H(\langle aba,bab,aba,bab\rangle)=1$$ $$H(\langle aba,bab,abb,bba\rangle)=2$$ and so on. Thus measuring the entropy not only for individual symbols but subsequences of symbols in the sequence appears to yield a better estimate of the ""complexity"" of the sequence. The same concept can be applied to 2D partitions (see here ). My questions are as follows What is the name of this approach in the literature? I'm thinking it might be something along the lines of partition-based entropy or substring-based entropy. How can I rigorously combine the partition-based entropy estimates for varying lengths to obtain a single, overall estimate of the entropy of the sequence? Is this approach related to Lempel-Ziv compression ? Can this approach be extended to continuous functions , perhaps by partitioning through open subsets of the domain of the function? Edit: On further thought, it seems a semi-rigorous combined result might be achieved by using the minimum length description principle and establishing a prior for the hypothesis ""the alphabet of the source is sequences of symbols of length $n$"" as follows: $$P(\text{source produces words of length }n)=2^{-n}\text{ or }\frac{1}{n(n+1)}$$ which satisfies unitarity: $$\sum_{n=1}^\infty P(x)=1$$ thus the $n$-partition entropy estimate for each $n$ is weighed by a function of $n$. Edit: This appears to correspond to the serial test here and the overlapping permutations test here .","The entropy $H$ of a discrete random variable $X$ is defined by $$H(X)=E[I(X)]=\sum_xP(x)I(x)=\sum_xP(x)\log P(x)^{-1}$$ where $x$ are the possible values of $X$, $P(x)$ is the probability of $x$, $E$ is the expected value operator, and $I$ is the self-information . In other words, $H(X)$ is the expected information content of $X$. Suppose we have a sequence $$s=\langle s_1,s_2,s_3,s_4,\ldots,s_n\rangle$$ Let $N(x)$ be the number of occurrences of $x$ in $s$. Assuming a uniform distribution, we can express $P(x)$ as $N(x)/n$, where $n$ is the length of the sequence. Hence we can express the entropy as $$\sum_x\frac{N(x)}{n}\log\frac{n}{N(x)}=\log n-\frac{1}{n}\sum N(x)\log N(x)$$ This allows us to calculate the entropy of the sequence based on the number of occurrences of individual symbols. For example, $$H(\langle a,b,a,b,a,b\rangle)=1$$ according to this measure. Switching the last two symbols yields the same entropy $$H(\langle a,b,a,b,b,a\rangle)=1$$ since the number of occurrences of each symbol does not change. However, it seems intuitively true that the second sequence is more ""random"" than the first sequence and hence should, in some sense, have higher entropy. Indeed, I found that taking the entropy of adjacent pairs of symbols in $s$, we obtain $$H(\langle ab,ba,ab,ba,ab\rangle)\approx 0.970951$$ $$H(\langle ab,ba,ab,bb,ba\rangle)\approx 1.52193$$ By this measure, the entropy of the second sequence is higher than that of the first sequence. We can do the same for triplets of symbols as well: $$H(\langle aba,bab,aba,bab\rangle)=1$$ $$H(\langle aba,bab,abb,bba\rangle)=2$$ and so on. Thus measuring the entropy not only for individual symbols but subsequences of symbols in the sequence appears to yield a better estimate of the ""complexity"" of the sequence. The same concept can be applied to 2D partitions (see here ). My questions are as follows What is the name of this approach in the literature? I'm thinking it might be something along the lines of partition-based entropy or substring-based entropy. How can I rigorously combine the partition-based entropy estimates for varying lengths to obtain a single, overall estimate of the entropy of the sequence? Is this approach related to Lempel-Ziv compression ? Can this approach be extended to continuous functions , perhaps by partitioning through open subsets of the domain of the function? Edit: On further thought, it seems a semi-rigorous combined result might be achieved by using the minimum length description principle and establishing a prior for the hypothesis ""the alphabet of the source is sequences of symbols of length $n$"" as follows: $$P(\text{source produces words of length }n)=2^{-n}\text{ or }\frac{1}{n(n+1)}$$ which satisfies unitarity: $$\sum_{n=1}^\infty P(x)=1$$ thus the $n$-partition entropy estimate for each $n$ is weighed by a function of $n$. Edit: This appears to correspond to the serial test here and the overlapping permutations test here .",,"['statistics', 'random-variables', 'random', 'information-theory', 'entropy']"
3,"Sampling distribution of $Y = \frac{\ln U_1}{\ln U_1 + \ln (1 - U_2)}$, where $U_i \sim U(0,1), \forall i$","Sampling distribution of , where","Y = \frac{\ln U_1}{\ln U_1 + \ln (1 - U_2)} U_i \sim U(0,1), \forall i","For this problem I have used the fact, $-2 \ln U \sim \chi^2_{(2)}$. But I have doubt on the independence of numerator and the denominator which are $\ln U_1$ and $\ln U_1 + \ln (1 - U_2)$. If they are independent, then resultant statistic boils down to $F_{2,4}$ statistic. Please help.","For this problem I have used the fact, $-2 \ln U \sim \chi^2_{(2)}$. But I have doubt on the independence of numerator and the denominator which are $\ln U_1$ and $\ln U_1 + \ln (1 - U_2)$. If they are independent, then resultant statistic boils down to $F_{2,4}$ statistic. Please help.",,"['statistics', 'probability-distributions', 'sampling']"
4,When do I use a z-score vs a t-score for confidence intervals?,When do I use a z-score vs a t-score for confidence intervals?,,"I have a set of 1000 data points.  I would like to estimate their mean using a confidence interval.  I read somewhere that if the sample size, $n$, is bigger than 30 you should use a t-score, and else use a z-score. Is that true?","I have a set of 1000 data points.  I would like to estimate their mean using a confidence interval.  I read somewhere that if the sample size, $n$, is bigger than 30 you should use a t-score, and else use a z-score. Is that true?",,['statistics']
5,Difficult to understand difference between the estimates on E(X) and V(X) and the estimates on variance and std.dev. on lambda-hat,Difficult to understand difference between the estimates on E(X) and V(X) and the estimates on variance and std.dev. on lambda-hat,,"I'm having a very hard time to separate estimates on population values versus estimates on sample values. I'm struggling with this exercise (not homework, self-study for my exam in introductionary statistic course): a) Calculate the maximum likelihood estimate on $\lambda$ and name this $\hat{\lambda}$ I did this. First I calculated the maximum likelihood estimator for $\lambda$, which is $\hat{\lambda}=\frac1n \sum_{i=0}^n k_i$ And the maximum likelihood estimate was $\hat{\lambda_e}=\frac{15}6=2.5$ Hopefully this is right. But now I'm asked to calculate the ""estimates on $E(X)$ and $E(Y)$ and afterwards I'm asked to calculate the estimate on both the variance and standarddeviation on the estimate, $\hat{\lambda}$? I'm really struggling to separate the two from each other, so any hint or guidance would be much appreciated.","I'm having a very hard time to separate estimates on population values versus estimates on sample values. I'm struggling with this exercise (not homework, self-study for my exam in introductionary statistic course): a) Calculate the maximum likelihood estimate on $\lambda$ and name this $\hat{\lambda}$ I did this. First I calculated the maximum likelihood estimator for $\lambda$, which is $\hat{\lambda}=\frac1n \sum_{i=0}^n k_i$ And the maximum likelihood estimate was $\hat{\lambda_e}=\frac{15}6=2.5$ Hopefully this is right. But now I'm asked to calculate the ""estimates on $E(X)$ and $E(Y)$ and afterwards I'm asked to calculate the estimate on both the variance and standarddeviation on the estimate, $\hat{\lambda}$? I'm really struggling to separate the two from each other, so any hint or guidance would be much appreciated.",,"['statistics', 'estimation', 'parameter-estimation', 'poisson-distribution']"
6,Two different formulas for standard error of difference between two means,Two different formulas for standard error of difference between two means,,"I mostly see this formula when searching for a formula for the estimate of the standard error in difference between two means, and it is also used in this video . $$\Delta=\sqrt{s_1^2/N_1+s_2^2/N_2}$$ But I've also seen this one (and this is the one my book uses): $$\Delta'=\sqrt{\dfrac{\left(N_1-1\right)s_1^2+\left(N_2-1\right)s_2^2}{N_1+N_2-2}\left(\dfrac{1}{N_1}+\dfrac{1}{N_2}\right)}$$ As these are two very different formulas, how come they are used seemingly interchangeably?","I mostly see this formula when searching for a formula for the estimate of the standard error in difference between two means, and it is also used in this video . But I've also seen this one (and this is the one my book uses): As these are two very different formulas, how come they are used seemingly interchangeably?",\Delta=\sqrt{s_1^2/N_1+s_2^2/N_2} \Delta'=\sqrt{\dfrac{\left(N_1-1\right)s_1^2+\left(N_2-1\right)s_2^2}{N_1+N_2-2}\left(\dfrac{1}{N_1}+\dfrac{1}{N_2}\right)},"['statistics', 'means']"
7,Finding the CDF of a piecewise PDF,Finding the CDF of a piecewise PDF,,"So for my statistics class I am taking this semester we've been working on continuous random variables and we have one question that the teacher did not cover at all nor his notes, and it has to deal with piecewise functions. I'm given the piecewise PDF f(x) = x + 1 for -1 < x < 0 and -x + 1 for 0 < x < 1. I do know that to get from a PDF to a CDF you need to integrate the function which I did for both of these giving me x^2/2 + x and -x^2/2 + x. This question given in the book has the answer given in the back of the book and it has a + 1/2 on the end of both CDF functions. My question is so I understand what is going here, where does that 1/2 come from in this question because the teacher never went over problems involving piecewise functions just single functions, and also the book doesn't ever mention piecewise functions either.","So for my statistics class I am taking this semester we've been working on continuous random variables and we have one question that the teacher did not cover at all nor his notes, and it has to deal with piecewise functions. I'm given the piecewise PDF f(x) = x + 1 for -1 < x < 0 and -x + 1 for 0 < x < 1. I do know that to get from a PDF to a CDF you need to integrate the function which I did for both of these giving me x^2/2 + x and -x^2/2 + x. This question given in the book has the answer given in the back of the book and it has a + 1/2 on the end of both CDF functions. My question is so I understand what is going here, where does that 1/2 come from in this question because the teacher never went over problems involving piecewise functions just single functions, and also the book doesn't ever mention piecewise functions either.",,"['integration', 'statistics']"
8,Calculating the best match between two sets,Calculating the best match between two sets,,"I’m a PHP developer and I have a problem calculating the perfect match between two different data sets. I have data sets from companies, where each company defines the requirements for a specific job. I also have data sets from users, where each user can define a data set describing their skills. Both of this datasets could hold values between $1$ and $12$. Here’s an example of two data sets: $\begin{align*} \text{Company} & \to [\phantom{1}4, \phantom{1}8, 12, \phantom{1}4, 10] \\ \text{User} & \to [\phantom{1}8, 10, \phantom{1}5, \phantom{1}5, \phantom{0}1] \end{align*}$ Question: What is the best way to calculate the best matching job from a company? There were two thoughts that crossed my mind, but I don’t know which would be better, of if indeed there’s another completely different approach. Calculate the sum of all absolute diffs. $\newcommand{\abs}{\operatorname{abs}}$ For example: $$\text{score} = \abs(4-8) + \abs(8-10) + \abs(12-5) + \abs(4-5) + \abs(10-1) = 23$$ Calculate the absolute diff between the sum of both data sets. For example: $$\text{score} = \abs\left[(4+8+12+4+10)-(8+10+5+5+1)\right] = 9$$","I’m a PHP developer and I have a problem calculating the perfect match between two different data sets. I have data sets from companies, where each company defines the requirements for a specific job. I also have data sets from users, where each user can define a data set describing their skills. Both of this datasets could hold values between $1$ and $12$. Here’s an example of two data sets: $\begin{align*} \text{Company} & \to [\phantom{1}4, \phantom{1}8, 12, \phantom{1}4, 10] \\ \text{User} & \to [\phantom{1}8, 10, \phantom{1}5, \phantom{1}5, \phantom{0}1] \end{align*}$ Question: What is the best way to calculate the best matching job from a company? There were two thoughts that crossed my mind, but I don’t know which would be better, of if indeed there’s another completely different approach. Calculate the sum of all absolute diffs. $\newcommand{\abs}{\operatorname{abs}}$ For example: $$\text{score} = \abs(4-8) + \abs(8-10) + \abs(12-5) + \abs(4-5) + \abs(10-1) = 23$$ Calculate the absolute diff between the sum of both data sets. For example: $$\text{score} = \abs\left[(4+8+12+4+10)-(8+10+5+5+1)\right] = 9$$",,"['statistics', 'absolute-value']"
9,Question about English sentences in statistics?,Question about English sentences in statistics?,,"Can somebody help me interpreting the red circled sentences in planer English? I understand ""We view $y_i$ as a realization of a random variable $Y_i$ that can take the values of one and zero"" but the next following words, ""with probabilities $\pi_i$ and $1-\pi_i$"" , make me confused in interpreting the whole sentence.","Can somebody help me interpreting the red circled sentences in planer English? I understand ""We view $y_i$ as a realization of a random variable $Y_i$ that can take the values of one and zero"" but the next following words, ""with probabilities $\pi_i$ and $1-\pi_i$"" , make me confused in interpreting the whole sentence.",,"['statistics', 'statistical-inference', 'order-statistics', 'descriptive-statistics']"
10,Estimate length of confidence interval,Estimate length of confidence interval,,"From Hogg & Tanis, 8th ed., p. 291: Let $X_1, X_1, \dots X_n$ by a random sample of size $n$ from the normal distribution $N(\mu, \sigma^2)$. Calculate the expected length of a 95% confidence interval for $\mu$, assuming that $n = 5$ and variance is (a) known, (b) unknown. For (a), I let $L = 2z_{\alpha / 2}(\sigma / \sqrt{n})$.  Since everything is constant, $E[L] = L$; just plug in all the numbers and out comes an expression in terms of $\sigma$. For (b), though, I'm not as sure.  I let $L = 2t_{\alpha / 2}(n - 1)\cdot(S / \sqrt{n}) = 2t_{0.025}(4)\cdot(S / \sqrt{5})$. Then $E[L] = \frac{5.552}{\sqrt{5}}E[S]$. By an earlier result (to which a hint for this question refers), $$E[S] = \frac{\sigma}{\sqrt{n-1}}\cdot\frac{\sqrt{2}\Gamma(n/2)}{\Gamma((n-1)/2)}$$ Plugging in all the relevant values, I get $$E[L] = \frac{4.164\sqrt{\pi}}{\sqrt{10}}\sigma$$ which looks a bit like the previous result.  The result for (a) makes sense to me because $\sigma$ is known; plug in $\sigma$ and out comes the length of the interval.  For (b), though, $\sigma$ is unknown.  So — if this is correct — how would that form of $E[L]$ be at all informative?  Can anyone offer any intuition on what's going on here?","From Hogg & Tanis, 8th ed., p. 291: Let $X_1, X_1, \dots X_n$ by a random sample of size $n$ from the normal distribution $N(\mu, \sigma^2)$. Calculate the expected length of a 95% confidence interval for $\mu$, assuming that $n = 5$ and variance is (a) known, (b) unknown. For (a), I let $L = 2z_{\alpha / 2}(\sigma / \sqrt{n})$.  Since everything is constant, $E[L] = L$; just plug in all the numbers and out comes an expression in terms of $\sigma$. For (b), though, I'm not as sure.  I let $L = 2t_{\alpha / 2}(n - 1)\cdot(S / \sqrt{n}) = 2t_{0.025}(4)\cdot(S / \sqrt{5})$. Then $E[L] = \frac{5.552}{\sqrt{5}}E[S]$. By an earlier result (to which a hint for this question refers), $$E[S] = \frac{\sigma}{\sqrt{n-1}}\cdot\frac{\sqrt{2}\Gamma(n/2)}{\Gamma((n-1)/2)}$$ Plugging in all the relevant values, I get $$E[L] = \frac{4.164\sqrt{\pi}}{\sqrt{10}}\sigma$$ which looks a bit like the previous result.  The result for (a) makes sense to me because $\sigma$ is known; plug in $\sigma$ and out comes the length of the interval.  For (b), though, $\sigma$ is unknown.  So — if this is correct — how would that form of $E[L]$ be at all informative?  Can anyone offer any intuition on what's going on here?",,"['statistics', 'intuition']"
11,Clayton copula and Kendall's tau,Clayton copula and Kendall's tau,,"I'm currently preparing for an exam in Risk Management (mathematics) by doing exercises from old exams. One of these exercises proved to be too difficult because of the following: Given Kendall's tau $ \tau = 1/3 $ and the Clayton copula $$ C(u,v) = (u^{-\theta} + v^{-\theta} - 1)^{-1/\theta} $$ we can calculate the parameter $ \theta $ by $$ \tau = \theta / (\theta + 2), $$ which we see is $ \theta = 1 $. My problem is that I can't see why this is so, how does this work? After some more reading I have arrived at this: $$ \begin{align} \tau & = 4 \cdot \mathbf{E} [C(U,V)] - 1 \\       & = 4 \int_{0}^{1} \int_{0}^{1} (u^{-\theta} + v^{-\theta} - 1)^{(-1/\theta)} dudv - 1, \end{align} $$ but I'm not able to solve this double integral. I've checked on the internet, tried Wolfram Alpha integral calculator as well as using a mathematics handbook. Any help as to how I should proceed in order to solve this integral would be much appreciated!","I'm currently preparing for an exam in Risk Management (mathematics) by doing exercises from old exams. One of these exercises proved to be too difficult because of the following: Given Kendall's tau $ \tau = 1/3 $ and the Clayton copula $$ C(u,v) = (u^{-\theta} + v^{-\theta} - 1)^{-1/\theta} $$ we can calculate the parameter $ \theta $ by $$ \tau = \theta / (\theta + 2), $$ which we see is $ \theta = 1 $. My problem is that I can't see why this is so, how does this work? After some more reading I have arrived at this: $$ \begin{align} \tau & = 4 \cdot \mathbf{E} [C(U,V)] - 1 \\       & = 4 \int_{0}^{1} \int_{0}^{1} (u^{-\theta} + v^{-\theta} - 1)^{(-1/\theta)} dudv - 1, \end{align} $$ but I'm not able to solve this double integral. I've checked on the internet, tried Wolfram Alpha integral calculator as well as using a mathematics handbook. Any help as to how I should proceed in order to solve this integral would be much appreciated!",,"['integration', 'statistics']"
12,Multiple regression degrees of freedom $f$-test.,Multiple regression degrees of freedom -test.,f,"I'm finding conflicting information from college textbooks on calculating the degrees of freedom for a a global $F$-test on a multiple regression. To be absolutely clear, assume there are 50 observations and 3 independent variables. Can you please tell me the df for the numerator and denominator?  I have found 2 sets of numbers in college texts. One indicating the numerator is equal to $P$, in this case 3, and alternatively $P-1$.  For the denominator I am finding $n-p$,which in this case would be 47, and alternatively, $n-p-1$.  Perhaps I am misunderstanding the material and there are circumstances when one vs. the other formula applies. I've not done any regression analysis in more than 25 years and now find I'm stuck on a Christmas vacation project I wanted to do with my son. So any help that would explain, in a gentle way, (I can't get through the quadratic explanation, or something that will bury me in calculus) how to determine the df would be appreciated. Concrete examples would be very beneficial. Also, if there is a good practical walk through of multiple regression/Anova that will show some examples and explain concepts (but please do not recommend Regression for Dummies) I'd appreciate a referral to that as well.  Thanks for your help.","I'm finding conflicting information from college textbooks on calculating the degrees of freedom for a a global $F$-test on a multiple regression. To be absolutely clear, assume there are 50 observations and 3 independent variables. Can you please tell me the df for the numerator and denominator?  I have found 2 sets of numbers in college texts. One indicating the numerator is equal to $P$, in this case 3, and alternatively $P-1$.  For the denominator I am finding $n-p$,which in this case would be 47, and alternatively, $n-p-1$.  Perhaps I am misunderstanding the material and there are circumstances when one vs. the other formula applies. I've not done any regression analysis in more than 25 years and now find I'm stuck on a Christmas vacation project I wanted to do with my son. So any help that would explain, in a gentle way, (I can't get through the quadratic explanation, or something that will bury me in calculus) how to determine the df would be appreciated. Concrete examples would be very beneficial. Also, if there is a good practical walk through of multiple regression/Anova that will show some examples and explain concepts (but please do not recommend Regression for Dummies) I'd appreciate a referral to that as well.  Thanks for your help.",,['statistics']
13,Detecting periodicity in point processes,Detecting periodicity in point processes,,"I have  data from a periodic one dimensional point process representing the times of events which is also mixed in with lots of data that I regard as noise.  The total number of points is of order one million. I would like to detect (approximate) periodicity in the data to reconstruct the periodic point precess.   This question is a little vague but what is a good method for doing this? As an example, if my points are $1,10,25,30,31,50$ I would like to detect $10, 30, 50$ as equally spaced points.  As my values are reals I need this to tolerate small errors in the data.","I have  data from a periodic one dimensional point process representing the times of events which is also mixed in with lots of data that I regard as noise.  The total number of points is of order one million. I would like to detect (approximate) periodicity in the data to reconstruct the periodic point precess.   This question is a little vague but what is a good method for doing this? As an example, if my points are $1,10,25,30,31,50$ I would like to detect $10, 30, 50$ as equally spaced points.  As my values are reals I need this to tolerate small errors in the data.",,"['statistics', 'signal-processing']"
14,Why does the standard deviation change from confidence intervals to hypothesis tests?,Why does the standard deviation change from confidence intervals to hypothesis tests?,,"When considering two-sample data that involves a difference of proportions, both a confidence interval and a hypothesis test can be done. The standard deviation used for a difference of proportions in creating a confidence interval is $\sqrt{\frac{p_1(1 - p_1)}{n_1} + \frac{p_2(1 - p_2)}{n_2}}$. However, the standard deviation used for confidence intervals is $\sqrt{\frac{p(1 - p)}{n_1} + \frac{p(1 - p)}{n_2}}$, where $p = \frac{x_1 + x_2}{n_1 + n_2}$, $x_1 = p_1n_1$, and $x_2 = p_2n_2$. What I don't understand is why these are different. They're both the standard deviation of the same proportion, so why should they differ?","When considering two-sample data that involves a difference of proportions, both a confidence interval and a hypothesis test can be done. The standard deviation used for a difference of proportions in creating a confidence interval is $\sqrt{\frac{p_1(1 - p_1)}{n_1} + \frac{p_2(1 - p_2)}{n_2}}$. However, the standard deviation used for confidence intervals is $\sqrt{\frac{p(1 - p)}{n_1} + \frac{p(1 - p)}{n_2}}$, where $p = \frac{x_1 + x_2}{n_1 + n_2}$, $x_1 = p_1n_1$, and $x_2 = p_2n_2$. What I don't understand is why these are different. They're both the standard deviation of the same proportion, so why should they differ?",,"['statistics', 'statistical-inference']"
15,Discrete Pareto Distribution,Discrete Pareto Distribution,,"What would be the equation to describe a set of 10,000 bugs if you already know they have a pareto distribution. In other words 2,000 of the bugs would equal 80% of all your problems. I'm struggling to make the pareto and/or Zipf equation fit this model... It should be relatively simple. I would like to make a simple ""progress bar"" type graph that will use this equation.","What would be the equation to describe a set of 10,000 bugs if you already know they have a pareto distribution. In other words 2,000 of the bugs would equal 80% of all your problems. I'm struggling to make the pareto and/or Zipf equation fit this model... It should be relatively simple. I would like to make a simple ""progress bar"" type graph that will use this equation.",,"['statistics', 'probability-distributions']"
16,Statistics: Finding posterior distribution given prior distribution & R.Vs distribution,Statistics: Finding posterior distribution given prior distribution & R.Vs distribution,,"I'm now learning Bayesian inference.This is one of the questions I'm doing. Suppose we have R.V.s $X_1,X_2,\ldots,X_n$ each have an Exponential distribution with parameter $\theta$. and prior for $\theta$ is an Exponential distribution with parameter $\lambda$. So what would you do to find posterior? Attempts: Prior should be PDF of exponential with parameter $\lambda$. Likelihood should be product of PDF of exponential of each $X_i$, with parameter $\theta$. Then what would you do next? Many thanks.","I'm now learning Bayesian inference.This is one of the questions I'm doing. Suppose we have R.V.s $X_1,X_2,\ldots,X_n$ each have an Exponential distribution with parameter $\theta$. and prior for $\theta$ is an Exponential distribution with parameter $\lambda$. So what would you do to find posterior? Attempts: Prior should be PDF of exponential with parameter $\lambda$. Likelihood should be product of PDF of exponential of each $X_i$, with parameter $\theta$. Then what would you do next? Many thanks.",,"['statistics', 'bayesian']"
17,Whats the difference between a one way anova and two way anova?,Whats the difference between a one way anova and two way anova?,,"I know that in one way anova you compare the difference between two or more  means and the same in two way, but I'm unclear as to how the use of categorical variables differs between them. Any help would be appreciated","I know that in one way anova you compare the difference between two or more  means and the same in two way, but I'm unclear as to how the use of categorical variables differs between them. Any help would be appreciated",,['statistics']
18,Zipfs law and LogNormal distributions,Zipfs law and LogNormal distributions,,"If a particular dataset has a lognormal distribution, will it also follow Zipf's law when the items are ranked? That is, say I have a set of populations of a random sampling of cities (assumed to be a lognormal distribution).  If I ordered them by rank, would their values be predicted by some power curve? If so, given a mean and standard deviation of a lognormal distribution, how can I derive the power curve that Zipf's law describes? If not, what type of distribution has the quality where when it's items are ranked, they follow Zipf's law?  And also what type of curve best approximates a ranked list of items from a lognormal distribution? Thanks.","If a particular dataset has a lognormal distribution, will it also follow Zipf's law when the items are ranked? That is, say I have a set of populations of a random sampling of cities (assumed to be a lognormal distribution).  If I ordered them by rank, would their values be predicted by some power curve? If so, given a mean and standard deviation of a lognormal distribution, how can I derive the power curve that Zipf's law describes? If not, what type of distribution has the quality where when it's items are ranked, they follow Zipf's law?  And also what type of curve best approximates a ranked list of items from a lognormal distribution? Thanks.",,"['statistics', 'probability-distributions']"
19,Expectation Maximization algorithm,Expectation Maximization algorithm,,I am having problems finding a well thought out complete explanation of expectation maximization. Does anyone have a best source for someone completely new to this stuff?,I am having problems finding a well thought out complete explanation of expectation maximization. Does anyone have a best source for someone completely new to this stuff?,,"['reference-request', 'statistics']"
20,Verifying Exponential Family,Verifying Exponential Family,,"Why is the following $$f(x|\theta) = \theta^{-1} \exp(1-(x/\theta)), \ \ 0 < \theta < x <\infty$$ not an exponential family? We know that $$f(x| \theta) = h(x)c(\theta) \exp(w(\theta)t(x))$$ where $h(x) = e, \  c(\theta) = \theta^{-1}, w(\theta) = \theta^{-1}$ and $t(x) = -x$.","Why is the following $$f(x|\theta) = \theta^{-1} \exp(1-(x/\theta)), \ \ 0 < \theta < x <\infty$$ not an exponential family? We know that $$f(x| \theta) = h(x)c(\theta) \exp(w(\theta)t(x))$$ where $h(x) = e, \  c(\theta) = \theta^{-1}, w(\theta) = \theta^{-1}$ and $t(x) = -x$.",,"['statistics', 'probability-distributions']"
21,Calculating mean from the probability mass function,Calculating mean from the probability mass function,,"Question. The number of flaws $X$ on an electroplated car grill is known to the have the following probability mass function: $$ \begin{matrix} x    & : &  0   &  1  &   2  & 3  \\ p(x) & : & 0.8 & 0.1 & 0.05 & 0.05  \end{matrix} $$ Calculate the mean of $X$. My working. $$ \text{Mean} = E(X) = (0 \times 0.8) + (1 \times 0.1) + (2 \times 0.05) + (3 \times 0.05) = 0.35 .$$ But the answer is $0.25$ (which is also $\frac{0.8+0.1+0.05+0.05}{4}$). What am I doing wrong?","Question. The number of flaws $X$ on an electroplated car grill is known to the have the following probability mass function: $$ \begin{matrix} x    & : &  0   &  1  &   2  & 3  \\ p(x) & : & 0.8 & 0.1 & 0.05 & 0.05  \end{matrix} $$ Calculate the mean of $X$. My working. $$ \text{Mean} = E(X) = (0 \times 0.8) + (1 \times 0.1) + (2 \times 0.05) + (3 \times 0.05) = 0.35 .$$ But the answer is $0.25$ (which is also $\frac{0.8+0.1+0.05+0.05}{4}$). What am I doing wrong?",,"['statistics', 'probability-distributions']"
22,Is it wrong to use Binary Vector data in Cosine Similarity?,Is it wrong to use Binary Vector data in Cosine Similarity?,,"I am doing Information Retrieval using Cosine Similarity. My data is a binary vector. Since most of the references I read were using non-binary vector (non-binary matrix) data, I am wondering if it is wrong to use binary vector data in the cosine similarity function.","I am doing Information Retrieval using Cosine Similarity. My data is a binary vector. Since most of the references I read were using non-binary vector (non-binary matrix) data, I am wondering if it is wrong to use binary vector data in the cosine similarity function.",,"['statistics', 'vector-spaces', 'information-theory']"
23,The Metropolis Algorithm,The Metropolis Algorithm,,"I Know how to apply the Metropolis Algorithm, but I'd be grateful if someone could explain to me the reasoning behind the steps in the algorithm. I've tried in vain looking for the original paper. Thanks.","I Know how to apply the Metropolis Algorithm, but I'd be grateful if someone could explain to me the reasoning behind the steps in the algorithm. I've tried in vain looking for the original paper. Thanks.",,"['statistics', 'algorithms']"
24,Optimal sample size?,Optimal sample size?,,"What's the simplest formula to calculate the optimal sample size from a population size? I've been reading a little about it and they ask me stuff like confidence level or confidence interval which I have no idea about. I need to test some HTML elements in web pages to get a better idea about general properties of HTML elements, and I have no idea about he results I'll get. So I just need the most neutral possible version of the sample size calculating formula when all I know is the population size.","What's the simplest formula to calculate the optimal sample size from a population size? I've been reading a little about it and they ask me stuff like confidence level or confidence interval which I have no idea about. I need to test some HTML elements in web pages to get a better idea about general properties of HTML elements, and I have no idea about he results I'll get. So I just need the most neutral possible version of the sample size calculating formula when all I know is the population size.",,['statistics']
25,Detecting subtle signals embedded in random patterns?,Detecting subtle signals embedded in random patterns?,,"Suppose one million random 10 digit binary numbers have been generated: $$1011010101$$ $$1110010100$$ $$0110001010$$ $$...$$ A ""bug"" in the generator code is such that the $3^{rd}$ and $7^{th}$ digit of the sequence have an ""extra"" conditional algorithm tucked in: If the parity of all digits to the left of the $3^{rd}$ digit is odd, and the parity of all digits to the right of the $7^{th}$ digit is even, then there is a 10% chance of the $7^{th}$ digit morphing to match the $3^{rd}$ digit value. $$01\color{red}0010\color{red}1110$$ Suppose this bug is unknown to us. Is there a systematic statistical way to discover the inherent non-randomness in these set of 1 million numbers, and if so, is there a way to discover which digits are problematic and the exact bug algorithm that produces them?","Suppose one million random 10 digit binary numbers have been generated: A ""bug"" in the generator code is such that the and digit of the sequence have an ""extra"" conditional algorithm tucked in: If the parity of all digits to the left of the digit is odd, and the parity of all digits to the right of the digit is even, then there is a 10% chance of the digit morphing to match the digit value. Suppose this bug is unknown to us. Is there a systematic statistical way to discover the inherent non-randomness in these set of 1 million numbers, and if so, is there a way to discover which digits are problematic and the exact bug algorithm that produces them?",1011010101 1110010100 0110001010 ... 3^{rd} 7^{th} 3^{rd} 7^{th} 7^{th} 3^{rd} 01\color{red}0010\color{red}1110,"['statistics', 'random-variables']"
26,How to derive test statistic under composite null hypothesis?,How to derive test statistic under composite null hypothesis?,,"Suppose I have a multiple linear regression of the form: $$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +\epsilon$$ Then suppose I am interested in testing this set of hypotheses: $$H_0: \beta_1 = 0 \text{ OR } \beta_2 = 0$$ $$H_A: \beta_1 \neq 0 \text{ AND } \beta_2 \neq 0$$ I am interested in deriving a test statistic for this hypothesis.  I understand that I need to derive this under the null hypothesis.  However, it is not as straightforward as just plugging in $\beta_1 = 0 \text{ and } \beta_2 = 0$ , since our null is that either/or is zero, not both. Any input would be much appreciated, thank you!","Suppose I have a multiple linear regression of the form: Then suppose I am interested in testing this set of hypotheses: I am interested in deriving a test statistic for this hypothesis.  I understand that I need to derive this under the null hypothesis.  However, it is not as straightforward as just plugging in , since our null is that either/or is zero, not both. Any input would be much appreciated, thank you!",y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +\epsilon H_0: \beta_1 = 0 \text{ OR } \beta_2 = 0 H_A: \beta_1 \neq 0 \text{ AND } \beta_2 \neq 0 \beta_1 = 0 \text{ and } \beta_2 = 0,"['statistics', 'statistical-inference', 'hypothesis-testing', 'parameter-estimation']"
27,How to set boundaries when approximating a discrete RV with a normally distributed RV?,How to set boundaries when approximating a discrete RV with a normally distributed RV?,,"I am approximating a random variable $S_n \sim \text{Bin}(n,0.02)$ with $Z_n=\frac{S_n-\mu n}{\sqrt{np(1-p)}} \sim \mathcal{N}(0,1)$ for high enough $n$ (by central limit theorem). My problem is how to set interval boundaries when calculating probabilities with that approach. For example when calculating the chance of $2$ or more (edited the value, but not important) successes in $100$ trials, what is the interval I would like $S_n$ to be in? Is it $[2,100]$ (meaning $S_n\geq2$ ) or $(1, 100]$ (meaning $S_n>1$ ), or something inbetween like $[1.5, 100]$ ? Calculation for $S_n \in (1,100]$ : $$P(S_n \in (1,100])=1-P(S_n \notin (1,100])=1-P\Big[\frac{S_{100}-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\leq \frac{1-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\Big]=1-\Phi(-0.71)=\Phi(0.71)\approx0,76$$ Calculation for $S_{100} \in [2,100]$ : $$P(S_{100} \in [2,100])=1-P(S_{100} \notin [2,100])=1-P\Big[\frac{S_{100}-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\leq \frac{2-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\Big]=1-\Phi(0)=0.5$$ Both results are not exactly good approximations of the exact result $0.6$ calculated with the binominal distribution. I strongly suspect it is due to bad boundary setting.","I am approximating a random variable with for high enough (by central limit theorem). My problem is how to set interval boundaries when calculating probabilities with that approach. For example when calculating the chance of or more (edited the value, but not important) successes in trials, what is the interval I would like to be in? Is it (meaning ) or (meaning ), or something inbetween like ? Calculation for : Calculation for : Both results are not exactly good approximations of the exact result calculated with the binominal distribution. I strongly suspect it is due to bad boundary setting.","S_n \sim \text{Bin}(n,0.02) Z_n=\frac{S_n-\mu n}{\sqrt{np(1-p)}} \sim \mathcal{N}(0,1) n 2 100 S_n [2,100] S_n\geq2 (1, 100] S_n>1 [1.5, 100] S_n \in (1,100] P(S_n \in (1,100])=1-P(S_n \notin (1,100])=1-P\Big[\frac{S_{100}-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\leq \frac{1-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\Big]=1-\Phi(-0.71)=\Phi(0.71)\approx0,76 S_{100} \in [2,100] P(S_{100} \in [2,100])=1-P(S_{100} \notin [2,100])=1-P\Big[\frac{S_{100}-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\leq \frac{2-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\Big]=1-\Phi(0)=0.5 0.6","['statistics', 'normal-distribution', 'central-limit-theorem']"
28,"Completeness, UMVUE, MLE in uniform $(-\theta,2\theta)$ distribution","Completeness, UMVUE, MLE in uniform  distribution","(-\theta,2\theta)","Let $\theta >0$ be a parameter and let $X_1,X_2,\ldots,X_n$ be a random sample with pdf $f(x\mid\theta)=\frac{1}{3\theta}$ if $-\theta \leq x\leq 2\theta$ and $0$ otherwise. a) Find the MLE of $\theta$ b) Is the MLE a sufficient statistic for $\theta$ ? c) Is the MLE a complete statistic for $\theta$ ? d) Is $\frac{n+1}{n}\cdot MLE$ the UMVUE of $\theta$ ? I've been able to solve a). The MLE of $\theta$ is $\max(-X_{(1)},\frac{X_{(n)}}{2}).$ Also, you can show that it is sufficient using the Factorization Theorem. However, I cannot solve the next questions I think because of the $\max$ in the MLE. Is there another way to express $\max(-X_{(1)},\frac{X_{(n)}}{2})$ ? Can I express the MLE as $\frac{|X|_{(n)}}{2}?$","Let be a parameter and let be a random sample with pdf if and otherwise. a) Find the MLE of b) Is the MLE a sufficient statistic for ? c) Is the MLE a complete statistic for ? d) Is the UMVUE of ? I've been able to solve a). The MLE of is Also, you can show that it is sufficient using the Factorization Theorem. However, I cannot solve the next questions I think because of the in the MLE. Is there another way to express ? Can I express the MLE as","\theta >0 X_1,X_2,\ldots,X_n f(x\mid\theta)=\frac{1}{3\theta} -\theta \leq x\leq 2\theta 0 \theta \theta \theta \frac{n+1}{n}\cdot MLE \theta \theta \max(-X_{(1)},\frac{X_{(n)}}{2}). \max \max(-X_{(1)},\frac{X_{(n)}}{2}) \frac{|X|_{(n)}}{2}?","['statistics', 'probability-distributions', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
29,find the region for CDF,find the region for CDF,,"Let $X$ and $Y$ be two random variables with the following density function: $$f(x)= \begin{cases} 6x(1-x),  & \text{if   } 0\le x\le 1 \\ 0, & \text{otherwise}  \\ \end{cases} \\ g(y)=\begin{cases} 3y^2,  & \text{if   } 0\le y\le 1 \\ 0, & \text{otherwise}  \\ \end{cases}$$ If $X$ and $Y$ are independent find the pdf of random variables $Z=\frac{X}{Y}\text{ and }U=XY$ . As $X$ and $Y$ are independent then the joint pdf is, $$f_{X,Y}(x,y)=\begin{cases} 18xy^2(1-x),  & \text{if   } 0\le x\le 1,0\le y\le 1 \\ 0, & \text{otherwise}  \\ \end{cases}$$ $(1)$ Let $F_Z(z)$ is the CDF of Z, $$F_Z(z)=P(Z\le z)=P\left(\frac{x}{y}\le z\right)\stackrel{(a)}{=}P(x\le zy)$$ As $0\le y\le 1$ , our inequality didn't change in $(a)$ . Now it's time to draw the region, $$F_Z(z)=\int_0^1\int_0^{zy}f_{XY}(x,y)\:dx\:dy$$ I can do the rest. Can anyone tell me Am I correctly done everything until here $?$ $(2)$ Let $F_U(u)$ is the CDF of U, $$F_U(u)=P(U\le u)=P(xy\le u)$$ From here I can't image the region. Can Anyone help me to figure it out. Any solution or hint will be appreciated. Thanks in advance.","Let and be two random variables with the following density function: If and are independent find the pdf of random variables . As and are independent then the joint pdf is, Let is the CDF of Z, As , our inequality didn't change in . Now it's time to draw the region, I can do the rest. Can anyone tell me Am I correctly done everything until here Let is the CDF of U, From here I can't image the region. Can Anyone help me to figure it out. Any solution or hint will be appreciated. Thanks in advance.","X Y f(x)=
\begin{cases}
6x(1-x),  & \text{if   } 0\le x\le 1 \\
0, & \text{otherwise}  \\
\end{cases} \\ g(y)=\begin{cases}
3y^2,  & \text{if   } 0\le y\le 1 \\
0, & \text{otherwise}  \\
\end{cases} X Y Z=\frac{X}{Y}\text{ and }U=XY X Y f_{X,Y}(x,y)=\begin{cases}
18xy^2(1-x),  & \text{if   } 0\le x\le 1,0\le y\le 1 \\
0, & \text{otherwise}  \\
\end{cases} (1) F_Z(z) F_Z(z)=P(Z\le z)=P\left(\frac{x}{y}\le z\right)\stackrel{(a)}{=}P(x\le zy) 0\le y\le 1 (a) F_Z(z)=\int_0^1\int_0^{zy}f_{XY}(x,y)\:dx\:dy ? (2) F_U(u) F_U(u)=P(U\le u)=P(xy\le u)","['statistics', 'probability-distributions']"
30,Asymptotic distribution of a Maximum Likelihood Estimator using the Central Limit Theorem,Asymptotic distribution of a Maximum Likelihood Estimator using the Central Limit Theorem,,"Let $(X_1,\dots,X_n)$ be a random sample from a population $X$ having probability density function $$f(x;\vartheta)=\vartheta\,x^{\vartheta -1}\,I_{(0,1)}(x)$$ $$\vartheta>0\qquad\qquad I_{(0,1)}(x)= \begin{cases} 1\qquad\text{if }x\in(0,1)\\ 0\qquad\text{otherwise} \end{cases}$$ Find: $\hat{\vartheta}_n$ , the MLE (Maximum Likelihood Estimator) of the parameter $\vartheta$ the approximate distribution of $\hat{\vartheta}_n$ for $n$ big While I'm pretty confident on how to solve the first point, I'd like some advice on the second one. 1. To find the MLE $\hat{\vartheta}_n$ , the likelihood function is calculated: $$\mathscr{L}(\underline{x};\vartheta) = \vartheta^n\,\left( x_1 \times\dots\times x_n \right)^{\vartheta -1} = \vartheta^n\,\left( \prod_{i=1}^n x_i \right)^{\vartheta -1} \qquad\qquad(0<x_i<1,\, \forall\,i=1,2,\dots,n)$$ The first derivative with respect to $\vartheta$ is $$\frac{\partial\mathscr{L}(\underline{x};\vartheta)}{\partial\vartheta} = \vartheta^{n-1}\,\left( \prod_{i=1}^n x_i \right)^{\vartheta -1}\left[ n+\vartheta\,\log{\left(\prod_{i=1}^n x_i\right)}\right]$$ and $\hat{\vartheta}_n$ is obtained solving for $$\frac{\partial\mathscr{L}(\underline{x};\vartheta)}{\partial\vartheta} = 0\quad \implies \quad \hat{\vartheta}_n = -\frac{n}{\log{\left(\prod_{i=1}^n x_i\right)}}$$ To be precise, it should now be checked that $$\left. \frac{\partial^2\mathscr{L}(\underline{x};\vartheta)}{\partial\vartheta^2}\right|_{\vartheta=\hat{\vartheta}_n} < 0$$ In order to do so, the second derivative with respect to $\vartheta$ is calculated: $$\frac{\partial^2\mathscr{L}(\underline{x};\vartheta)}{\partial\vartheta^2} = \underbrace{\vartheta^{n-2}\,\left( \prod_{i=1}^n x_i \right)^{\vartheta -1}}_{\Gamma}\underbrace{\left[ n(n-1)+2n\vartheta\log{\left(\prod_{i=1}^n x_i\right)}+\vartheta^2\log^2{\left(\prod_{i=1}^n x_i\right)}\right]}_{\Delta}$$ $\Gamma$ is always positive, hence we only need to evaluate $$\left.\Delta\right|_{\vartheta=\hat{\vartheta}_n} = -n <0$$ 2. To obtain the approximate distribution of $\hat{\vartheta}_n$ for $n$ big (a.k.a. the asymptotic distribution of $\hat{\vartheta}_n$ for $n\to\infty$ ), I thought of applying Cramér's Theorem for the asymptotic normality of the MLE. This assures that $$\sqrt{n}\left(\hat{\vartheta}_n-\vartheta\right)\xrightarrow{d}\mathscr{N}(0,1/I(\vartheta))$$ where $I(\vartheta)$ is Fisher information , calculated after simple but tedious algebra: $$I(\vartheta) = \mathbb{E}\left[\left(\frac{\partial}{\partial\vartheta}\log{f(x;\vartheta)} \right)^2\right] = \frac{1}{\vartheta^2}$$ MY QUESTION While the procedure presented above to obtain the asymptotic distribution is completely general and should be correct (please tell me if it's not), I was wondering if there is any way to straightforwardly apply the Central Limit Theorem (CLT) to the variable $$\hat{\vartheta}_n = -\frac{n}{\log{\left(\prod_{i=1}^n x_i\right)}}$$ in order to obtain the same result, since $\hat{\vartheta}_n$ can be almost re-written as the sum of (the $\log$ of) iid variables: $$\hat{\vartheta}_n = -\frac{n}{\sum_{i=1}^n \log{x_i}}$$ Any idea/suggestion would be greatly appreciated!","Let be a random sample from a population having probability density function Find: , the MLE (Maximum Likelihood Estimator) of the parameter the approximate distribution of for big While I'm pretty confident on how to solve the first point, I'd like some advice on the second one. 1. To find the MLE , the likelihood function is calculated: The first derivative with respect to is and is obtained solving for To be precise, it should now be checked that In order to do so, the second derivative with respect to is calculated: is always positive, hence we only need to evaluate 2. To obtain the approximate distribution of for big (a.k.a. the asymptotic distribution of for ), I thought of applying Cramér's Theorem for the asymptotic normality of the MLE. This assures that where is Fisher information , calculated after simple but tedious algebra: MY QUESTION While the procedure presented above to obtain the asymptotic distribution is completely general and should be correct (please tell me if it's not), I was wondering if there is any way to straightforwardly apply the Central Limit Theorem (CLT) to the variable in order to obtain the same result, since can be almost re-written as the sum of (the of) iid variables: Any idea/suggestion would be greatly appreciated!","(X_1,\dots,X_n) X f(x;\vartheta)=\vartheta\,x^{\vartheta -1}\,I_{(0,1)}(x) \vartheta>0\qquad\qquad I_{(0,1)}(x)=
\begin{cases}
1\qquad\text{if }x\in(0,1)\\
0\qquad\text{otherwise}
\end{cases} \hat{\vartheta}_n \vartheta \hat{\vartheta}_n n \hat{\vartheta}_n \mathscr{L}(\underline{x};\vartheta) = \vartheta^n\,\left( x_1 \times\dots\times x_n \right)^{\vartheta -1} = \vartheta^n\,\left( \prod_{i=1}^n x_i \right)^{\vartheta -1} \qquad\qquad(0<x_i<1,\, \forall\,i=1,2,\dots,n) \vartheta \frac{\partial\mathscr{L}(\underline{x};\vartheta)}{\partial\vartheta} = \vartheta^{n-1}\,\left( \prod_{i=1}^n x_i \right)^{\vartheta -1}\left[ n+\vartheta\,\log{\left(\prod_{i=1}^n x_i\right)}\right] \hat{\vartheta}_n \frac{\partial\mathscr{L}(\underline{x};\vartheta)}{\partial\vartheta} = 0\quad \implies \quad \hat{\vartheta}_n = -\frac{n}{\log{\left(\prod_{i=1}^n x_i\right)}} \left. \frac{\partial^2\mathscr{L}(\underline{x};\vartheta)}{\partial\vartheta^2}\right|_{\vartheta=\hat{\vartheta}_n} < 0 \vartheta \frac{\partial^2\mathscr{L}(\underline{x};\vartheta)}{\partial\vartheta^2} = \underbrace{\vartheta^{n-2}\,\left( \prod_{i=1}^n x_i \right)^{\vartheta -1}}_{\Gamma}\underbrace{\left[ n(n-1)+2n\vartheta\log{\left(\prod_{i=1}^n x_i\right)}+\vartheta^2\log^2{\left(\prod_{i=1}^n x_i\right)}\right]}_{\Delta} \Gamma \left.\Delta\right|_{\vartheta=\hat{\vartheta}_n} = -n <0 \hat{\vartheta}_n n \hat{\vartheta}_n n\to\infty \sqrt{n}\left(\hat{\vartheta}_n-\vartheta\right)\xrightarrow{d}\mathscr{N}(0,1/I(\vartheta)) I(\vartheta) I(\vartheta) = \mathbb{E}\left[\left(\frac{\partial}{\partial\vartheta}\log{f(x;\vartheta)} \right)^2\right] = \frac{1}{\vartheta^2} \hat{\vartheta}_n = -\frac{n}{\log{\left(\prod_{i=1}^n x_i\right)}} \hat{\vartheta}_n \log \hat{\vartheta}_n = -\frac{n}{\sum_{i=1}^n \log{x_i}}","['statistics', 'statistical-inference', 'central-limit-theorem', 'maximum-likelihood', 'parameter-estimation']"
31,"How to prove $\int_0^\infty e^{-a q} \Gamma (0,q)^2 \, dq = -\frac{1}{a}\left(2 \operatorname{Li}_2(-a-1)+\frac{\pi ^2}{6}\right)$",How to prove,"\int_0^\infty e^{-a q} \Gamma (0,q)^2 \, dq = -\frac{1}{a}\left(2 \operatorname{Li}_2(-a-1)+\frac{\pi ^2}{6}\right)","In my study ( https://math.stackexchange.com/a/3392284/198592 ) of pdfs for the harmonic mean of $n$ independent random variables $x_{i} \sim U(0,1)$ I discovered the lemma $$f_{2}(a) = \int_0^{\infty } e^{-a q} \Gamma (0,q)^2 \, dq = -\frac{1}{a}\left(2 \operatorname{Li}_2(-a-1)+\frac{\pi ^2}{6}\right)$$ Here $\Gamma(b,q) = \int_{q}^{\infty} t^{b-1}e^{-t}\,dt$ is the incomplete Gamma function and $\operatorname{Li}_n(x)=\sum _{k=1}^{\infty } \frac{x^k}{k^n}$ is the polylog function. I have a nice proof of it, and I'd like to share with you the joy of finding it or preferrably another one.","In my study ( https://math.stackexchange.com/a/3392284/198592 ) of pdfs for the harmonic mean of independent random variables I discovered the lemma Here is the incomplete Gamma function and is the polylog function. I have a nice proof of it, and I'd like to share with you the joy of finding it or preferrably another one.","n x_{i} \sim U(0,1) f_{2}(a) = \int_0^{\infty } e^{-a q} \Gamma (0,q)^2 \, dq = -\frac{1}{a}\left(2 \operatorname{Li}_2(-a-1)+\frac{\pi ^2}{6}\right) \Gamma(b,q) = \int_{q}^{\infty} t^{b-1}e^{-t}\,dt \operatorname{Li}_n(x)=\sum _{k=1}^{\infty } \frac{x^k}{k^n}","['statistics', 'probability-distributions', 'special-functions', 'riemann-zeta', 'polylogarithm']"
32,Does equivariance of the MLE require the function be invertible?,Does equivariance of the MLE require the function be invertible?,,"My statistics text states this theorem as if it works for any function $g$ : Let $\tau = g(\theta)$ be a function of $\theta$ . Let $\hat{\theta}_n$ be the MLE (Maximum Likelihood Estimator) of $\theta$ . Then $\hat{\tau}_n = g(\hat{\theta}_n)$ is the MLE of $g(\theta)$ . And offers this proof that seems to assume $g$ has an inverse: Proof. Let $h = g^{-1}$ denote the inverse of $g$ . Then $\hat{\theta}_n = h(\hat{\tau}_n)$ . For any $\tau$ , $\mathcal{L}(\tau)  = \prod_i f(x_i; h(\tau)) = \prod_i f(x_i;\theta) = \mathcal{L}(\theta)$ where $\theta = h(\tau)$ . Hence, for any $\tau$ , $\mathcal{L}_n(\tau) = \mathcal{L}(\theta) \leq  \mathcal{L}(\hat{\theta}) = \mathcal{L}_n(\hat{\tau})$ . Is an inverse required? Maybe the author is assuming one for a simpler proof? Also I'm not sure where the inequality is coming from? I tried reading the Wikipedia article on equivariant maps (my statistics text is my first exposure to the term) but it uses too much material I haven't learned yet.","My statistics text states this theorem as if it works for any function : Let be a function of . Let be the MLE (Maximum Likelihood Estimator) of . Then is the MLE of . And offers this proof that seems to assume has an inverse: Proof. Let denote the inverse of . Then . For any , where . Hence, for any , . Is an inverse required? Maybe the author is assuming one for a simpler proof? Also I'm not sure where the inequality is coming from? I tried reading the Wikipedia article on equivariant maps (my statistics text is my first exposure to the term) but it uses too much material I haven't learned yet.","g \tau = g(\theta) \theta \hat{\theta}_n \theta \hat{\tau}_n = g(\hat{\theta}_n) g(\theta) g h = g^{-1} g \hat{\theta}_n = h(\hat{\tau}_n) \tau \mathcal{L}(\tau)
 = \prod_i f(x_i; h(\tau)) = \prod_i f(x_i;\theta) = \mathcal{L}(\theta) \theta = h(\tau) \tau \mathcal{L}_n(\tau) = \mathcal{L}(\theta) \leq
 \mathcal{L}(\hat{\theta}) = \mathcal{L}_n(\hat{\tau})","['statistics', 'maximum-likelihood', 'equivariant-maps']"
33,Spherical Gaussian MLE,Spherical Gaussian MLE,,"I am having trouble doing a derivation. I want to find the MLE estimate of $\sigma^2$ in a spherical gaussian, i.e when we have set $\Sigma = \sigma^2I$ . I have already seen https://stats.stackexchange.com/questions/238199/mle-of-multivariate-normal-distribution-with-diagonal-covariance-matrix but wanted to do the derivation more thoroughly. Say that we have $m$ points, and that our data consists of $p$ -features. We then know that the log likelihood of a multivariate gaussian is (from https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian/351550 ) $$l(\mu, \Sigma )  = - \frac{mp}{2} \log (2 \pi) - \frac{m}{2} \log |\Sigma|  - \frac{1}{2}  \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu) }  $$ Now, inserting the fact that $\Sigma = \sigma^2I$ we have that $|\Sigma| = |\sigma^2 I| = (\sigma^2)^p$ , we get that $$l(\mu, \sigma^2 )  = - \frac{mp}{2} \log (2 \pi) - \frac{mp}{2} \log \sigma^2  - \frac{1}{2 \sigma^2}  \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) }$$ Deriving based on $\sigma^2$ we get that $$\frac{\partial l}{\partial \sigma^2} = -\frac{mp}{2}\frac{1}{\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) }$$ Setting the derivative to 0 we get that $$\hat \sigma^2 = \frac{1}{mp} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) } $$ But this is not the same as other results I have found online (for example https://stats.stackexchange.com/questions/238199/mle-of-multivariate-normal-distribution-with-diagonal-covariance-matrix which I think argue that $\hat \sigma^2 = \frac{1}{m} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) }$ and http://cs229.stanford.edu/section/gaussians.pdf which states that it should equal the MLE from the univariate case). What is going on here? I have for a while suspected that I calculate the determinant incorrectly, thus giving extra $p$ in the denominator, but I think that is not the case. Can you help me find out what is wrong in my reasoning?","I am having trouble doing a derivation. I want to find the MLE estimate of in a spherical gaussian, i.e when we have set . I have already seen https://stats.stackexchange.com/questions/238199/mle-of-multivariate-normal-distribution-with-diagonal-covariance-matrix but wanted to do the derivation more thoroughly. Say that we have points, and that our data consists of -features. We then know that the log likelihood of a multivariate gaussian is (from https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian/351550 ) Now, inserting the fact that we have that , we get that Deriving based on we get that Setting the derivative to 0 we get that But this is not the same as other results I have found online (for example https://stats.stackexchange.com/questions/238199/mle-of-multivariate-normal-distribution-with-diagonal-covariance-matrix which I think argue that and http://cs229.stanford.edu/section/gaussians.pdf which states that it should equal the MLE from the univariate case). What is going on here? I have for a while suspected that I calculate the determinant incorrectly, thus giving extra in the denominator, but I think that is not the case. Can you help me find out what is wrong in my reasoning?","\sigma^2 \Sigma = \sigma^2I m p l(\mu, \Sigma )  = - \frac{mp}{2} \log (2 \pi) - \frac{m}{2} \log |\Sigma|  - \frac{1}{2}  \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu) }   \Sigma = \sigma^2I |\Sigma| = |\sigma^2 I| = (\sigma^2)^p l(\mu, \sigma^2 )  = - \frac{mp}{2} \log (2 \pi) - \frac{mp}{2} \log \sigma^2  - \frac{1}{2 \sigma^2}  \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) } \sigma^2 \frac{\partial l}{\partial \sigma^2} = -\frac{mp}{2}\frac{1}{\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) } \hat \sigma^2 = \frac{1}{mp} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) }  \hat \sigma^2 = \frac{1}{m} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) } p","['statistics', 'probability-distributions', 'normal-distribution', 'maximum-likelihood', 'parameter-estimation']"
34,Finding the maximum likelihood estimate of N for a hypergeometric distribution,Finding the maximum likelihood estimate of N for a hypergeometric distribution,,"A census in the United States is an attempt to count everyone in the country. It is inevitable that many people are not counted. The U. S. Census Bureau   proposed a way to estimate the number of people who were not counted by   the latest census. Their proposal was as follows: In a given locality, let N   denote the actual number of people who live there. Assume that the census   counted $n_1$ people living in this area. Now, another census was taken in the   locality, and $n_2$ people were counted. In addition, $ n_{12}$ people were counted   both times. b ) Now assume that $X = n_{12}$ . Find the value of N which maximises the   expression in part (a). Hint: Consider the ratio of the expressions for   successive values of N. Given (from part a) $h(N,n_1,n_2,n_{12}) = \frac{\binom{n_1}{n_{12}}\binom{N-n_1}{n_2-n_{12}}}{\binom{N}{n_2}}$ I found the ratio of $\frac{h(N+1,n_1,n_{12})}{h(N,n_1,n_{12})}$ and belive that N is maximum would be the first or smallest number in which that fraction becomes less than 1: $\frac{\binom{N+1-n_1}{n_2-n_{12}}\binom{N}{n_2}}{\binom{N+1}{n_2}\binom{N-n_1}{n_2-n_{12}}} \leq 1$ $\frac{(N+1-n_1)(N+1-n_2)}{(N+1)(N+1-n_1-n_2+n_{12})} \leq 1$ I found from simplifying this expression that $N \geq \frac{n_1n_2-n{12}}{n{12}}$ so setting that inequality to a equality should give the result. However, the solution here has $N = \frac{n_1n_2}{n_{12}}$ I'm not  sure where I went wrong.","A census in the United States is an attempt to count everyone in the country. It is inevitable that many people are not counted. The U. S. Census Bureau   proposed a way to estimate the number of people who were not counted by   the latest census. Their proposal was as follows: In a given locality, let N   denote the actual number of people who live there. Assume that the census   counted people living in this area. Now, another census was taken in the   locality, and people were counted. In addition, people were counted   both times. b ) Now assume that . Find the value of N which maximises the   expression in part (a). Hint: Consider the ratio of the expressions for   successive values of N. Given (from part a) I found the ratio of and belive that N is maximum would be the first or smallest number in which that fraction becomes less than 1: I found from simplifying this expression that so setting that inequality to a equality should give the result. However, the solution here has I'm not  sure where I went wrong.","n_1 n_2  n_{12} X = n_{12} h(N,n_1,n_2,n_{12}) = \frac{\binom{n_1}{n_{12}}\binom{N-n_1}{n_2-n_{12}}}{\binom{N}{n_2}} \frac{h(N+1,n_1,n_{12})}{h(N,n_1,n_{12})} \frac{\binom{N+1-n_1}{n_2-n_{12}}\binom{N}{n_2}}{\binom{N+1}{n_2}\binom{N-n_1}{n_2-n_{12}}} \leq 1 \frac{(N+1-n_1)(N+1-n_2)}{(N+1)(N+1-n_1-n_2+n_{12})} \leq 1 N \geq \frac{n_1n_2-n{12}}{n{12}} N = \frac{n_1n_2}{n_{12}}","['statistics', 'probability-distributions', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
35,Real Examples of Misleading Statistics,Real Examples of Misleading Statistics,,"I need to give a presentation to a group of students on Tuesday about why one needs to be careful when examining statistics or mathematical results in the media or online. In his book How Not To Be Wrong , Jordan Ellenberg provides a few examples that I planned on using as case studies to present to the students Wisconsin governor in 2011 claims, since there was a net 18,000 jobs added in 2011 and 9,000 were in Wisconsin, that implies that Wisconsin is doing something right. Failed to mention that net jobs added included states that lost jobs, which reduces the net rate. Mathematicians find proof that the Torah sends messages to the future by looking at sequences of characters that correspond to rabbi names and said rabbi birth/death dates. However, these results only held in the event of very specific names and dates; using any other accepted names or dates for each rabbi resulted in failed tests. If anyone knew of any other good real world examples of misleading statistics or mathematics. I know of a many examples due to variability in sample size, but the more intricate and (potentially) nefarious, the better. Thank you!","I need to give a presentation to a group of students on Tuesday about why one needs to be careful when examining statistics or mathematical results in the media or online. In his book How Not To Be Wrong , Jordan Ellenberg provides a few examples that I planned on using as case studies to present to the students Wisconsin governor in 2011 claims, since there was a net 18,000 jobs added in 2011 and 9,000 were in Wisconsin, that implies that Wisconsin is doing something right. Failed to mention that net jobs added included states that lost jobs, which reduces the net rate. Mathematicians find proof that the Torah sends messages to the future by looking at sequences of characters that correspond to rabbi names and said rabbi birth/death dates. However, these results only held in the event of very specific names and dates; using any other accepted names or dates for each rabbi resulted in failed tests. If anyone knew of any other good real world examples of misleading statistics or mathematics. I know of a many examples due to variability in sample size, but the more intricate and (potentially) nefarious, the better. Thank you!",,"['statistics', 'soft-question', 'statistical-inference', 'big-list', 'descriptive-statistics']"
36,Why does $\sqrt{n} (\bar X - \mu)/S$ have approximately a $t$-distribution?,Why does  have approximately a -distribution?,\sqrt{n} (\bar X - \mu)/S t,"Suppose that the random variables $X_1,\ldots, X_n$ are independent and identically distributed with mean $\mu$ and variance $\sigma^2$ . Let $\bar X = (X_1 + \cdots + X_n)/n$ be the sample mean and $S = ((X_1 - \bar X)^2 + \cdots + (X_n - \bar X)^2)/(n-1)$ be the sample variance. If the random variables $X_i$ are normally distributed, then $\sqrt{n}(\bar X - \mu)/\sigma$ is normally distributed and $\sqrt{n}(\bar X - \mu)/S$ has a $t$ -distribution. Moreover, even if the random variables $X_i$ are not normally distributed, then $\sqrt{n}(\bar X - \mu)/\sigma$ is approximately normally distributed, according to the central limit theorem. Question: In this case (where the random variables $X_i$ are not normally distributed), does the central limit theorem also imply that $\sqrt{n}(\bar X - \mu)/S$ has approximately a $t$ -distribution? This question arises in the context of constructing confidence intervals for the mean of a random variable, in the case where the variance is unknown. The following passage appears in Ross's book Introduction to Probability and Statistics for Engineers and Scientists: Our derivations of the $100(1-\alpha)$ percent confidence intervals   for the population mean $\mu$ have assumed that the population   distribution is normal. However, even when this is not the case, if   the sample size is reasonably large then the intervals obtained will   still be approximate $100(1-\alpha)$ percent confidence intervals for $\mu$ . This is true because, by the central limit theorem, $\sqrt{n}(\bar X - \mu)/\sigma$ will have approximately a normal   distribution, and $\sqrt{n}(\bar X - \mu)/S$ will have approximately a $t$ -distribution. I'm trying to understand the final statement about $\sqrt{n}(\bar X - \mu)/S$ having approximately a $t$ -distribution.","Suppose that the random variables are independent and identically distributed with mean and variance . Let be the sample mean and be the sample variance. If the random variables are normally distributed, then is normally distributed and has a -distribution. Moreover, even if the random variables are not normally distributed, then is approximately normally distributed, according to the central limit theorem. Question: In this case (where the random variables are not normally distributed), does the central limit theorem also imply that has approximately a -distribution? This question arises in the context of constructing confidence intervals for the mean of a random variable, in the case where the variance is unknown. The following passage appears in Ross's book Introduction to Probability and Statistics for Engineers and Scientists: Our derivations of the percent confidence intervals   for the population mean have assumed that the population   distribution is normal. However, even when this is not the case, if   the sample size is reasonably large then the intervals obtained will   still be approximate percent confidence intervals for . This is true because, by the central limit theorem, will have approximately a normal   distribution, and will have approximately a -distribution. I'm trying to understand the final statement about having approximately a -distribution.","X_1,\ldots, X_n \mu \sigma^2 \bar X = (X_1 + \cdots + X_n)/n S = ((X_1 - \bar X)^2 + \cdots + (X_n - \bar X)^2)/(n-1) X_i \sqrt{n}(\bar X - \mu)/\sigma \sqrt{n}(\bar X - \mu)/S t X_i \sqrt{n}(\bar X - \mu)/\sigma X_i \sqrt{n}(\bar X - \mu)/S t 100(1-\alpha) \mu 100(1-\alpha) \mu \sqrt{n}(\bar X - \mu)/\sigma \sqrt{n}(\bar X - \mu)/S t \sqrt{n}(\bar X - \mu)/S t","['statistics', 'confidence-interval']"
37,"Random variables and co-variance, Statistics 318","Random variables and co-variance, Statistics 318",,"For the given example in the book John E. Freund's Mathematical Statistics with Applications, 8th edition, by Miller and Miller. ISBN: 9780321807090 I've highlighted using colors what numbers corespond with what is given (Blue, Green, and Yellow). What I don't understand is where the highlighted red numbers come from. The example states it uses ""Theorem 15"" I'm just confused on how. ""The following is another important theorem about linear combinations of random variables; it concerns the covariance of two linear combinations of n random variables.""","For the given example in the book John E. Freund's Mathematical Statistics with Applications, 8th edition, by Miller and Miller. ISBN: 9780321807090 I've highlighted using colors what numbers corespond with what is given (Blue, Green, and Yellow). What I don't understand is where the highlighted red numbers come from. The example states it uses ""Theorem 15"" I'm just confused on how. ""The following is another important theorem about linear combinations of random variables; it concerns the covariance of two linear combinations of n random variables.""",,"['statistics', 'discrete-mathematics', 'random-variables', 'covariance', 'variance']"
38,What does interquartile range represent (real world scenario),What does interquartile range represent (real world scenario),,"I wish to understand when will i use the concept of interquartile range in real world scenario. I can understand how to calculate interquartile range but i do not understand or make sense of the final answer. For example. Below are the list of scores. 81,82,83,83,84,84,84,1000 The interquartile range is 1.5 Now what does 1.5 actually represent in above ages ? Like if i change the 1000 score to 85 ; the ineterquartile range will still be the same because the way its calculated. So whats the application of interquartile range ?","I wish to understand when will i use the concept of interquartile range in real world scenario. I can understand how to calculate interquartile range but i do not understand or make sense of the final answer. For example. Below are the list of scores. 81,82,83,83,84,84,84,1000 The interquartile range is 1.5 Now what does 1.5 actually represent in above ages ? Like if i change the 1000 score to 85 ; the ineterquartile range will still be the same because the way its calculated. So whats the application of interquartile range ?",,['statistics']
39,Find the correlation coefficient,Find the correlation coefficient,,"In studying the relation between the two variables $x$ and $y$ , if the equation of the regression line of $y$ on $x$ was  $$y=0.421x+0.67$$ and the equation of the regression line of $x$ on $y$ was  $$x=1.58y+3.9$$ \Find\ \ The linear correlation coefficient between $x$ and $y$  My solution is  $$r= \pm\sqrt{0.421\times 1.58}= \pm0.8155$$ Does my solution correct or i would not take the negative value into account ?","In studying the relation between the two variables $x$ and $y$ , if the equation of the regression line of $y$ on $x$ was  $$y=0.421x+0.67$$ and the equation of the regression line of $x$ on $y$ was  $$x=1.58y+3.9$$ \Find\ \ The linear correlation coefficient between $x$ and $y$  My solution is  $$r= \pm\sqrt{0.421\times 1.58}= \pm0.8155$$ Does my solution correct or i would not take the negative value into account ?",,['statistics']
40,What is a good statistical test to use to compare 2 models,What is a good statistical test to use to compare 2 models,,"I have already posted this question on Stats stackexchange so I am unsure whether I am allowed to post it here to. I am asking what would be a good/suitable investigation/test to use when trying to compare 2 models. So for my model in question, I am looking at the Lac Operon model and I add in substace $Y$ into the model and it produces enzyme $X$. The objective of adding $Y$ into the system is the produce as much $X$ as possible. I want to investigate if it is better to add all my $Y$ into the system in one big go (let's say I add 10,000 in all together), or to add say 2,000 $Y$ in at 5 even time intervals, or to add 1,000 of $Y$ in in 10 even time intervals. I am wondering what sort of statistical analysis should i be using here, or what sort of tests should I be performing.  I know I could just look at the amount of $X$ produced for each variation of my model, but I am looking for more intricate tests to perform. Now my models aren't linear models so I am unable to do anova using R , they are actually stochastic models. They are a series of chemical equations but I am basically just interested in the value of $X$, so i doubt the specifics of the model are needed to be known. Edit: May I propose another question to you. So to follow on from my earlier point, basically I have a system of chemical equations, and the rate constants from these equations follow a gamma distribution. Should I be testing rate constants against each other, or should I be testing the data against each other? Edit: Maybe I could test the comparison of the rate constants for my 2 models by using Bayesian Inference? I was wondering what would be a suitable test to compare to posterior/prior distributions rather than just using Bayes Factors??","I have already posted this question on Stats stackexchange so I am unsure whether I am allowed to post it here to. I am asking what would be a good/suitable investigation/test to use when trying to compare 2 models. So for my model in question, I am looking at the Lac Operon model and I add in substace $Y$ into the model and it produces enzyme $X$. The objective of adding $Y$ into the system is the produce as much $X$ as possible. I want to investigate if it is better to add all my $Y$ into the system in one big go (let's say I add 10,000 in all together), or to add say 2,000 $Y$ in at 5 even time intervals, or to add 1,000 of $Y$ in in 10 even time intervals. I am wondering what sort of statistical analysis should i be using here, or what sort of tests should I be performing.  I know I could just look at the amount of $X$ produced for each variation of my model, but I am looking for more intricate tests to perform. Now my models aren't linear models so I am unable to do anova using R , they are actually stochastic models. They are a series of chemical equations but I am basically just interested in the value of $X$, so i doubt the specifics of the model are needed to be known. Edit: May I propose another question to you. So to follow on from my earlier point, basically I have a system of chemical equations, and the rate constants from these equations follow a gamma distribution. Should I be testing rate constants against each other, or should I be testing the data against each other? Edit: Maybe I could test the comparison of the rate constants for my 2 models by using Bayesian Inference? I was wondering what would be a suitable test to compare to posterior/prior distributions rather than just using Bayes Factors??",,"['statistics', 'stochastic-processes', 'statistical-inference', 'bayesian', 'hypothesis-testing']"
41,Kernel density estimation -Effect of bandwidth,Kernel density estimation -Effect of bandwidth,,"I am trying to learn Kernel density estimation, I need help to understand how the bandwidth $h$ affects the Kernel density estimator. Consider a Gaussian Kernel $k(x)~=~\frac{1}{\sqrt{2 \pi}} e^{-x^2}$. The Kernel density estimator is given by ${\hat{f}}_h (x) ~=~ \frac{1}{h} \sum_{i=1}^{n} K_h(x-X_i)$. Clearly, $k(x)$ is independent of $h$, where does $h$ come in? What would be ${\hat{f}}_h (x)$? How does $h$ affect the Kernel? Thank you!","I am trying to learn Kernel density estimation, I need help to understand how the bandwidth $h$ affects the Kernel density estimator. Consider a Gaussian Kernel $k(x)~=~\frac{1}{\sqrt{2 \pi}} e^{-x^2}$. The Kernel density estimator is given by ${\hat{f}}_h (x) ~=~ \frac{1}{h} \sum_{i=1}^{n} K_h(x-X_i)$. Clearly, $k(x)$ is independent of $h$, where does $h$ come in? What would be ${\hat{f}}_h (x)$? How does $h$ affect the Kernel? Thank you!",,"['statistics', 'probability-distributions']"
42,Ambiguity in rule for determining an outlier,Ambiguity in rule for determining an outlier,,"I have the following set of numbers, in which I need to determine if there are any outliers: 11, 14, 21, 26, 29, 33, 61. I graphed them in Desmos (see above), and visually, it appears to me that 61 is an outlier. I searched online for a mathematical rule to determine whether a number is an outlier, and I found multiple references to the rule that a number is an outlier if it is greater than Q3, or less than Q1, by more than 1.5*(interquartile range). However, I found two different methods for determining the interquartile range of a data set with an odd number of members: method 1: Remove the median of data set as a whole.  Determine the medians of the larger and smaller groups.  Subtract the smaller median from the larger to get the interquartile range. method 2: Consider the median of the dataset as a whole as belonging to both the larger and smaller groups.  Determine the medians of the larger and smaller groups.  Subtract the smaller median from the larger to get the interquartile range. Method 1 gives the groups (11, 14, 21) and (29, 33, 61), which has medians 14 and 33, for an interquartile range of 19. Method 2 gives the groups (11, 14, 21, 26) and (26, 29, 33, 61), which has medians 17.5 and 31, for an interquartile range of 13.5. (Incidentally, the TI-83 calculator uses method 1, and Microsoft Excel uses method 2.) Getting back to the 'determining outliers' question: Using method 1 gives 33 + 1.5*19 = 61.5.  Since this is greater than 61, by this method, there are no outliers in the data set. Using method 2 gives 31 + 1.5*13.5 = 51.25.  Since this is less than 61, by this method, 61 is an outlier. So my question is: Is one of these methods considered more correct than the other?  Or is the whole 'point' of outliers more intuitive, and less bound by actual formulas, so the right method would be just looking at my graph instead?  (If possible, please include links to any authoritative source or reference that you may know of.  Thank you.) EDIT: After reading some of the responses, I think I need to add a bit more context info here.  I am reviewing an Algebra textbook meant for middle to high school students, and this is part of a question which appears in that book, specifically in the area of the book where outliers are being taught about.  It looks to me like the numbers in this question were specifically chosen to have one be as far away as possible from the rest, while having it not be considered an outlier by the 1.5IQR formula. I think that this will give students the perspective that outliers are always determined by a rigid formula, without any sort of intuitive 'that point looks far away from the rest' outlook.  I am looking for input on whether this a correct / proper outlook to be teaching.","I have the following set of numbers, in which I need to determine if there are any outliers: 11, 14, 21, 26, 29, 33, 61. I graphed them in Desmos (see above), and visually, it appears to me that 61 is an outlier. I searched online for a mathematical rule to determine whether a number is an outlier, and I found multiple references to the rule that a number is an outlier if it is greater than Q3, or less than Q1, by more than 1.5*(interquartile range). However, I found two different methods for determining the interquartile range of a data set with an odd number of members: method 1: Remove the median of data set as a whole.  Determine the medians of the larger and smaller groups.  Subtract the smaller median from the larger to get the interquartile range. method 2: Consider the median of the dataset as a whole as belonging to both the larger and smaller groups.  Determine the medians of the larger and smaller groups.  Subtract the smaller median from the larger to get the interquartile range. Method 1 gives the groups (11, 14, 21) and (29, 33, 61), which has medians 14 and 33, for an interquartile range of 19. Method 2 gives the groups (11, 14, 21, 26) and (26, 29, 33, 61), which has medians 17.5 and 31, for an interquartile range of 13.5. (Incidentally, the TI-83 calculator uses method 1, and Microsoft Excel uses method 2.) Getting back to the 'determining outliers' question: Using method 1 gives 33 + 1.5*19 = 61.5.  Since this is greater than 61, by this method, there are no outliers in the data set. Using method 2 gives 31 + 1.5*13.5 = 51.25.  Since this is less than 61, by this method, 61 is an outlier. So my question is: Is one of these methods considered more correct than the other?  Or is the whole 'point' of outliers more intuitive, and less bound by actual formulas, so the right method would be just looking at my graph instead?  (If possible, please include links to any authoritative source or reference that you may know of.  Thank you.) EDIT: After reading some of the responses, I think I need to add a bit more context info here.  I am reviewing an Algebra textbook meant for middle to high school students, and this is part of a question which appears in that book, specifically in the area of the book where outliers are being taught about.  It looks to me like the numbers in this question were specifically chosen to have one be as far away as possible from the rest, while having it not be considered an outlier by the 1.5IQR formula. I think that this will give students the perspective that outliers are always determined by a rigid formula, without any sort of intuitive 'that point looks far away from the rest' outlook.  I am looking for input on whether this a correct / proper outlook to be teaching.",,['statistics']
43,Find the MLE of a GLM,Find the MLE of a GLM,,"(Note this is not an assignment, but revision for a topic from Cambridge past exam papers) I have been trying to attempt the below question, and I am struggling with part (b). For (a) it is obvious that the pmf is the same as the Bernoulli and so $$ f(y;p) = \exp\left(y\log\left(\frac{p}{1-p}\right)+\log(1-p) \right).$$ Then for (b) , the log-likelihood is given by  $$\ell(\beta,y) = \sum_{i=1}^n \left(y_i\log\left(\frac{p_i}{1-p_i}\right)+\log(1-p_i) \right)$$ Now, $\log(\frac{p}{1-p}) = \beta^Tx \Rightarrow \log(1-p) = \log(p) - \beta^Tx$ And so, \begin{align} \ell(\beta,y) &= \sum_{i=1}^n y_i\beta^Tx_i + \sum_{i=1}^n \log(p) - \sum_{i=1}^n \beta^Tx_i\\ &= \sum_{i=1}^n y_i\beta^Tx_i + \sum_{i=1}^n \log\left(\frac{e^{\beta^Tx}}{1+e^{\beta^Tx}}\right) - \sum_{i=1}^n \beta^Tx_i \end{align} And i am unsure how to deal with this middle term, particularly when differentiating, as i cannot get the final answer.","(Note this is not an assignment, but revision for a topic from Cambridge past exam papers) I have been trying to attempt the below question, and I am struggling with part (b). For (a) it is obvious that the pmf is the same as the Bernoulli and so $$ f(y;p) = \exp\left(y\log\left(\frac{p}{1-p}\right)+\log(1-p) \right).$$ Then for (b) , the log-likelihood is given by  $$\ell(\beta,y) = \sum_{i=1}^n \left(y_i\log\left(\frac{p_i}{1-p_i}\right)+\log(1-p_i) \right)$$ Now, $\log(\frac{p}{1-p}) = \beta^Tx \Rightarrow \log(1-p) = \log(p) - \beta^Tx$ And so, \begin{align} \ell(\beta,y) &= \sum_{i=1}^n y_i\beta^Tx_i + \sum_{i=1}^n \log(p) - \sum_{i=1}^n \beta^Tx_i\\ &= \sum_{i=1}^n y_i\beta^Tx_i + \sum_{i=1}^n \log\left(\frac{e^{\beta^Tx}}{1+e^{\beta^Tx}}\right) - \sum_{i=1}^n \beta^Tx_i \end{align} And i am unsure how to deal with this middle term, particularly when differentiating, as i cannot get the final answer.",,"['statistics', 'summation', 'maximum-likelihood', 'log-likelihood', 'logistic-regression']"
44,"If $X$ is uniform on $(-1,1)$, find $g(x)$, so that $Y = g(X)$ has pdf $f_Y (y) = 2e^{-2y}$","If  is uniform on , find , so that  has pdf","X (-1,1) g(x) Y = g(X) f_Y (y) = 2e^{-2y}","If $X$ is uniformly distributed in $(-1, 1)$, find $g(x)$, so that the random variable $Y = g(X)$ may   have the density function $f_Y (y) = 2e^{-2y}, \  y > 0.$ Suppose $g:(-1,1) \rightarrow \mathbb{R}_{>0}$ be a monotonically increasing function such that $Y=g(X)$. Let $a \in (-1,1)$ and $b=g(a)$. Therefore we have $$P(Y \leq b)=P(X \leq a) $$ $P(Y \leq b) =\int_0^b 2e^{-2y} dy = 1-e^{-2b} $ and $P(X \leq a) = \frac{a+1}{2}$ (X is uniform). Therefore $1-e^{-2b}=\frac{a+1}{2}$. Hence $b = -\frac{1}{2} \log (\frac{1-a}{2})$. Therefore $$ g(x) = -\frac{1}{2} \log (\frac{1-x}{2}). $$ Is this correct? Help!","If $X$ is uniformly distributed in $(-1, 1)$, find $g(x)$, so that the random variable $Y = g(X)$ may   have the density function $f_Y (y) = 2e^{-2y}, \  y > 0.$ Suppose $g:(-1,1) \rightarrow \mathbb{R}_{>0}$ be a monotonically increasing function such that $Y=g(X)$. Let $a \in (-1,1)$ and $b=g(a)$. Therefore we have $$P(Y \leq b)=P(X \leq a) $$ $P(Y \leq b) =\int_0^b 2e^{-2y} dy = 1-e^{-2b} $ and $P(X \leq a) = \frac{a+1}{2}$ (X is uniform). Therefore $1-e^{-2b}=\frac{a+1}{2}$. Hence $b = -\frac{1}{2} \log (\frac{1-a}{2})$. Therefore $$ g(x) = -\frac{1}{2} \log (\frac{1-x}{2}). $$ Is this correct? Help!",,"['statistics', 'probability-distributions', 'random-variables']"
45,Normality test vs. Fitting a Gaussian curve,Normality test vs. Fitting a Gaussian curve,,"I have a set of one-dimensional data, and I suspect that the data is normally distributed. Before embarking in a normality test, I decided to fit a Gaussian curve to the histogram of relative frequencies, and see how well it fits. The result can be seen in the following image: In my opinion it fits quite well, but I am a novice in Statistics (and just about everything else). So my questions would be: Based on this graphical evidence, would you venture to hypothesize that the data is normally distributed? If so, do we still need to proceed with a chi-square normality test? More generally, would it be possible to replace a normality test by, say, a least-squares Gaussian fit? In that case, $R^2$ (the sum of squared differences) would be some kind of measure of the normality of the data. By the way, I obtained this fit with MATLAB's cftool. I don't know exactly which method it uses. I suppose it uses least squares, but I'm not sure. If anybody can confirm that, I'd appreciate it.","I have a set of one-dimensional data, and I suspect that the data is normally distributed. Before embarking in a normality test, I decided to fit a Gaussian curve to the histogram of relative frequencies, and see how well it fits. The result can be seen in the following image: In my opinion it fits quite well, but I am a novice in Statistics (and just about everything else). So my questions would be: Based on this graphical evidence, would you venture to hypothesize that the data is normally distributed? If so, do we still need to proceed with a chi-square normality test? More generally, would it be possible to replace a normality test by, say, a least-squares Gaussian fit? In that case, $R^2$ (the sum of squared differences) would be some kind of measure of the normality of the data. By the way, I obtained this fit with MATLAB's cftool. I don't know exactly which method it uses. I suppose it uses least squares, but I'm not sure. If anybody can confirm that, I'd appreciate it.",,"['statistics', 'normal-distribution', 'matlab', 'least-squares', 'data-analysis']"
46,Is there any significance to the derivative of the average?,Is there any significance to the derivative of the average?,,"I have a set of time vs. concentration data that I'm trying to turn into an average derivative value for use in an equation (I'm worried a simple average of the first and last points ignores too much data). The most obvious way to do it would be to simply do $\frac{C - C_0}{t - t_0}$ for each point and take the average, but I'm wondering, is there any significance to the value $\frac{C_{avg}}{t_{avg}}$? Since I have uniform time intervals I think this is equivalent to $$\frac{1}{N} \sum_{i}{\frac{C_i}{\Delta}}$$ where $\Delta$ is the time interval length and $N$ is the number of samples. If possible, avoiding numerical differentiation would be great since I'm getting some weird values from that approach. Any advice would be greatly appreciated. Thanks!","I have a set of time vs. concentration data that I'm trying to turn into an average derivative value for use in an equation (I'm worried a simple average of the first and last points ignores too much data). The most obvious way to do it would be to simply do $\frac{C - C_0}{t - t_0}$ for each point and take the average, but I'm wondering, is there any significance to the value $\frac{C_{avg}}{t_{avg}}$? Since I have uniform time intervals I think this is equivalent to $$\frac{1}{N} \sum_{i}{\frac{C_i}{\Delta}}$$ where $\Delta$ is the time interval length and $N$ is the number of samples. If possible, avoiding numerical differentiation would be great since I'm getting some weird values from that approach. Any advice would be greatly appreciated. Thanks!",,"['statistics', 'derivatives', 'numerical-methods']"
47,"What is the difference between ""fliers"" and ""whiskers"" in a boxplot?","What is the difference between ""fliers"" and ""whiskers"" in a boxplot?",,"Here an example in Python: In [87]: from pylab import *  In [88]: data = [1,2,3,4,5,6,7,100,101,102]  In [89]: r = boxplot(data)  In [90]: r Out[90]: {'boxes': [<matplotlib.lines.Line2D at 0x11b032908>],  'caps': [<matplotlib.lines.Line2D at 0x11b015390>,   <matplotlib.lines.Line2D at 0x11b024940>],  'fliers': [<matplotlib.lines.Line2D at 0x11b062b70>],  'means': [],  'medians': [<matplotlib.lines.Line2D at 0x11b062518>],  'whiskers': [<matplotlib.lines.Line2D at 0x11b0274a8>,   <matplotlib.lines.Line2D at 0x11b0327b8>]} I know that the (lower/upper) whiskers are given by Q1 - IQR*1.5 and Q3 + IQR*1.5 (Q1/Q3: 1st/3rd quartile , IQR: interquartile range ), and that they are used to identify outliers. But I don't understand what ""fliers"" are... According to this SO post , the flier is the line separating outliers, but is that correct?","Here an example in Python: In [87]: from pylab import *  In [88]: data = [1,2,3,4,5,6,7,100,101,102]  In [89]: r = boxplot(data)  In [90]: r Out[90]: {'boxes': [<matplotlib.lines.Line2D at 0x11b032908>],  'caps': [<matplotlib.lines.Line2D at 0x11b015390>,   <matplotlib.lines.Line2D at 0x11b024940>],  'fliers': [<matplotlib.lines.Line2D at 0x11b062b70>],  'means': [],  'medians': [<matplotlib.lines.Line2D at 0x11b062518>],  'whiskers': [<matplotlib.lines.Line2D at 0x11b0274a8>,   <matplotlib.lines.Line2D at 0x11b0327b8>]} I know that the (lower/upper) whiskers are given by Q1 - IQR*1.5 and Q3 + IQR*1.5 (Q1/Q3: 1st/3rd quartile , IQR: interquartile range ), and that they are used to identify outliers. But I don't understand what ""fliers"" are... According to this SO post , the flier is the line separating outliers, but is that correct?",,"['statistics', 'python']"
48,How to explain to non-mathematician what statistical significance is?,How to explain to non-mathematician what statistical significance is?,,"Recently I faced with the problem, and I can't explain it to my colleagues. The thing is, that they want to conduct a surveys with one question and the question may have only two answers, and they ask me how many surveys conducted is enough for the statistical significance. I tried to explain, that for the statistical significance you need the hypothesis about the approximate distribution of answers, that you need to accept or decline, but they don't understand, and I cannot explain it with easy examples and non-mathematician language. Help me with the explanation, or explain it to me if I'm wrong Thanks","Recently I faced with the problem, and I can't explain it to my colleagues. The thing is, that they want to conduct a surveys with one question and the question may have only two answers, and they ask me how many surveys conducted is enough for the statistical significance. I tried to explain, that for the statistical significance you need the hypothesis about the approximate distribution of answers, that you need to accept or decline, but they don't understand, and I cannot explain it with easy examples and non-mathematician language. Help me with the explanation, or explain it to me if I'm wrong Thanks",,['statistics']
49,Distribution of two random variable generating schemes,Distribution of two random variable generating schemes,,"Goal: Generate five numbers from 0 to 1 with sum 1. Method 1: Generate four numbers in range $(0,1)$ (by uniform distribution) to be the cuts of the interval, i.e. say the four random numbers generated is $a_1<a_2<a_3<a_4$, then the five random numbers are $a_1, a_2-a_1, a_3-a_2, a_4-a_3,1-a_4$ Method 2: Generate five numbers in range $(0, n)$ (by uniform distribution), where $n$ is arbitrary number bigger than zero. And then normalise the sum to be 1, i.e. divide all the numbers by their sum. My question is, are the distribution of the two methods equivalent? Is any of the two correspondents to some well-known distributions?","Goal: Generate five numbers from 0 to 1 with sum 1. Method 1: Generate four numbers in range $(0,1)$ (by uniform distribution) to be the cuts of the interval, i.e. say the four random numbers generated is $a_1<a_2<a_3<a_4$, then the five random numbers are $a_1, a_2-a_1, a_3-a_2, a_4-a_3,1-a_4$ Method 2: Generate five numbers in range $(0, n)$ (by uniform distribution), where $n$ is arbitrary number bigger than zero. And then normalise the sum to be 1, i.e. divide all the numbers by their sum. My question is, are the distribution of the two methods equivalent? Is any of the two correspondents to some well-known distributions?",,"['statistics', 'probability-distributions', 'random-variables']"
50,Estimating the total cost of purchasing every item in a grocery store,Estimating the total cost of purchasing every item in a grocery store,,"My friend and I were arguing for way too long the other night about how much it would cost you to buy every single thing in a grocery store. Our first go at it went something like this: Assume there are $N_{\text{items}}$ items per row in the grocery store, and let $p_{\text{avg}}$ be the average price for each item. Then say that there are $N_{\text{rows}}$ rows. Multiplying this out we get a total price $P_{\text{total}}$ as $$ P_{\text{total}} = N_{\text{items}}p_{\text{avg}}N_{\text{rows}}$$ The only issue is, there is a vast range of difference prices for items, and vast ranges of items per row, depending on what row you're in. For instance, if you go down the aisle with all the spices, there's a ton of items at very low cost, but the coffee aisle has a lot of items at very high cost; the meat aisle has relatively average number of items at a much higher cost, as well as the kitchen-utensils/kitchenware aisle etc. This got me thinking that there must be a better way to do an accurate estimation for a problem like this. Perhaps come up with some sort of intelligent distribution for prices (I was thinking maybe a log-normal distribution with a maximum around some arbitrary ""most-probable"" price, based on observation). And possibly do the same thing with the number of items per row? Estimating $N_{\text{rows}}$ is relatively straight forward since most grocery stores have somewhere between ten and twenty rows, so letting be $N_{\text{rows}}$ Gaussian centered at ten should take care of that, if we even want to get that fancy with that variable. Anyway, I'm not that savy with probability/statistics in the first place, so I thought I would ask you brilliant people: how would you most intelligently try to take a stab at this estimation?","My friend and I were arguing for way too long the other night about how much it would cost you to buy every single thing in a grocery store. Our first go at it went something like this: Assume there are $N_{\text{items}}$ items per row in the grocery store, and let $p_{\text{avg}}$ be the average price for each item. Then say that there are $N_{\text{rows}}$ rows. Multiplying this out we get a total price $P_{\text{total}}$ as $$ P_{\text{total}} = N_{\text{items}}p_{\text{avg}}N_{\text{rows}}$$ The only issue is, there is a vast range of difference prices for items, and vast ranges of items per row, depending on what row you're in. For instance, if you go down the aisle with all the spices, there's a ton of items at very low cost, but the coffee aisle has a lot of items at very high cost; the meat aisle has relatively average number of items at a much higher cost, as well as the kitchen-utensils/kitchenware aisle etc. This got me thinking that there must be a better way to do an accurate estimation for a problem like this. Perhaps come up with some sort of intelligent distribution for prices (I was thinking maybe a log-normal distribution with a maximum around some arbitrary ""most-probable"" price, based on observation). And possibly do the same thing with the number of items per row? Estimating $N_{\text{rows}}$ is relatively straight forward since most grocery stores have somewhere between ten and twenty rows, so letting be $N_{\text{rows}}$ Gaussian centered at ten should take care of that, if we even want to get that fancy with that variable. Anyway, I'm not that savy with probability/statistics in the first place, so I thought I would ask you brilliant people: how would you most intelligently try to take a stab at this estimation?",,"['statistics', 'recreational-mathematics', 'estimation', 'fermi-problems']"
51,How to calculate standard deviation of entire set from means and populations of all subsets?,How to calculate standard deviation of entire set from means and populations of all subsets?,,"If I have a list $X$ of size $N$, where $N$ is known, but the exact values of $X$ aren't, how can I calculate the standard deviation of $X$ using only the means and sizes of $n$ separate sublists (no overlap between sublists, all objects in $N$ are contained in exactly one sublists). Example) $X=\{4, 9, 5, 4, 6, 10, 3, 9, 3, 2, 6, 8\}$ $A=\{4, 9, 5, 4\}$ $B=\{6, 10, 3, 9\}$ $C=\{3, 2, 6, 8\}$ Without knowing the values in any of the lists, can I determine the standard deviation of $X$ given that every object in $X$ is in one of the three sublists, and that this information is known: $x̅_A=5.5$, $x̅_B=7$, $x̅_C=4.75$, $x̅_X=5.75$ $N_A=4$, $N_B=4$, $N_C=4$, $N_X=12$ Although my example uses sublists of equal size, my actual data does not, so I am looking for answers which aren't specific to sublists of equal size.","If I have a list $X$ of size $N$, where $N$ is known, but the exact values of $X$ aren't, how can I calculate the standard deviation of $X$ using only the means and sizes of $n$ separate sublists (no overlap between sublists, all objects in $N$ are contained in exactly one sublists). Example) $X=\{4, 9, 5, 4, 6, 10, 3, 9, 3, 2, 6, 8\}$ $A=\{4, 9, 5, 4\}$ $B=\{6, 10, 3, 9\}$ $C=\{3, 2, 6, 8\}$ Without knowing the values in any of the lists, can I determine the standard deviation of $X$ given that every object in $X$ is in one of the three sublists, and that this information is known: $x̅_A=5.5$, $x̅_B=7$, $x̅_C=4.75$, $x̅_X=5.75$ $N_A=4$, $N_B=4$, $N_C=4$, $N_X=12$ Although my example uses sublists of equal size, my actual data does not, so I am looking for answers which aren't specific to sublists of equal size.",,"['statistics', 'standard-deviation']"
52,Relationship between subset medians and the median,Relationship between subset medians and the median,,"Suppose we have a set of data $A = \{a_1, a_2, \dots, a_n\}$ and $B = \{b_1, b_2, \dots, b_n \}$. So there are $2n$ elements in total. Further suppose the median of $A$ and $B$ is $a$ and $b$, respectively. I'm wondering if whether there is any relationship between the median $m$ of $A \cup B$ and $a$, $b$? Now suppose we divide the data sets into $A_L$, $A_R$, $B_L$, and $B_R$ so that $A_L$ contains all the elements of $A$ that are less than or equal to the median $a$, $A_R$ contains all the elements that are greater than or equal to $a$, and with a similar definition for $B$. Can we narrow down the search for the actual median by knowing $a$ and $b$? For example, can we decide if $m$ lies in any one or more of $A_L$, $A_R$, $B_L$, or $B_R$ (but not all)? I'm not sure if this question makes sense since I thought of it randomly but I am extremely curious to know if there is a real answer or to understand why there isn't.","Suppose we have a set of data $A = \{a_1, a_2, \dots, a_n\}$ and $B = \{b_1, b_2, \dots, b_n \}$. So there are $2n$ elements in total. Further suppose the median of $A$ and $B$ is $a$ and $b$, respectively. I'm wondering if whether there is any relationship between the median $m$ of $A \cup B$ and $a$, $b$? Now suppose we divide the data sets into $A_L$, $A_R$, $B_L$, and $B_R$ so that $A_L$ contains all the elements of $A$ that are less than or equal to the median $a$, $A_R$ contains all the elements that are greater than or equal to $a$, and with a similar definition for $B$. Can we narrow down the search for the actual median by knowing $a$ and $b$? For example, can we decide if $m$ lies in any one or more of $A_L$, $A_R$, $B_L$, or $B_R$ (but not all)? I'm not sure if this question makes sense since I thought of it randomly but I am extremely curious to know if there is a real answer or to understand why there isn't.",,"['statistics', 'data-analysis', 'median']"
53,98% confidence interval,98% confidence interval,,"For this question from a large amount of data I have calculated that the mean is 44.22, the sample size is 100 and the standard deviation is 22.0773. From this I am asked to , make the 98% confidence intervals for the (1) true mean µ of the module mark (2) true variance of the module mark And for each: (a) Determine what quantity to look at, and which distribution table to use, justifying your choice.  (b) Determine the number of degrees of freedom, justifying your answer. (c) Calculate the actual intervals. So far for 1, I have used the z table to  look for $99\%$ as I need $1\%$ to the right of $2.33$ and $1\%$ to the left of $-2.33$, so $98\%$ is between $\pm2.33$. Giving me $$ \bar x \pm 2.33\frac{\sigma}{\sqrt{n}} $$ Which provides me with a 39.08 to 49.36 confidence interval, is this correct? And how would I determine degrees of freedom and go about answering part 2?","For this question from a large amount of data I have calculated that the mean is 44.22, the sample size is 100 and the standard deviation is 22.0773. From this I am asked to , make the 98% confidence intervals for the (1) true mean µ of the module mark (2) true variance of the module mark And for each: (a) Determine what quantity to look at, and which distribution table to use, justifying your choice.  (b) Determine the number of degrees of freedom, justifying your answer. (c) Calculate the actual intervals. So far for 1, I have used the z table to  look for $99\%$ as I need $1\%$ to the right of $2.33$ and $1\%$ to the left of $-2.33$, so $98\%$ is between $\pm2.33$. Giving me $$ \bar x \pm 2.33\frac{\sigma}{\sqrt{n}} $$ Which provides me with a 39.08 to 49.36 confidence interval, is this correct? And how would I determine degrees of freedom and go about answering part 2?",,['statistics']
54,Explaining the standard deviation formula,Explaining the standard deviation formula,,"I'm revisiting standard deviation for the first time years, and I can't for the life of me recall the difference between two formulas. In particular, I'm also looking for how we arrived at these forumulas. Firstly we have for the sample standard deviation $$ \sqrt{\dfrac{ \sum_{i=1}^{n}(X-\bar{X})^2}{n-1}}$$ Also we have the population standard deviation $$ \sqrt{\dfrac{ \sum_{i=1}^{n}(X-\mu)^2}{n}}$$ From what I understand, we sqaure the difference to remove negative values. After that I'm lost. Is the square root to go back to the difference but without the negatives? Also, why do we divide by $n-1$ on sample, and by $n$ on the population? Why is there a difference and can anyone give a real example?","I'm revisiting standard deviation for the first time years, and I can't for the life of me recall the difference between two formulas. In particular, I'm also looking for how we arrived at these forumulas. Firstly we have for the sample standard deviation Also we have the population standard deviation From what I understand, we sqaure the difference to remove negative values. After that I'm lost. Is the square root to go back to the difference but without the negatives? Also, why do we divide by on sample, and by on the population? Why is there a difference and can anyone give a real example?", \sqrt{\dfrac{ \sum_{i=1}^{n}(X-\bar{X})^2}{n-1}}  \sqrt{\dfrac{ \sum_{i=1}^{n}(X-\mu)^2}{n}} n-1 n,"['statistics', 'probability-distributions', 'normal-distribution', 'standard-deviation', 'robust-statistics']"
55,Show that the moment generating function of $ W$ is $M_W(t) = (qe^t+p)^n$,Show that the moment generating function of  is, W M_W(t) = (qe^t+p)^n,"If $Y$ is a random variable with moment-generating function $M_Y(t)$ and if $W$ is given by $W=aY+b$, then the moment generating function of $W$ is $e^{tb}M_Y(at)$ Suppose that $Y$ is a binomial random variable based on $n$ trials with success probability of $p$ and let $W = n - Y$. Show that the moment generating function of $W$ is $M_W(t) = (qe^t+p)^n$ using the fact that $M_W(t) =e^{tb}M_Y(at)$. If $W$ is a binomial random variable, then the moment generating function in the open form is given by $$ M_W(t) = e^{tb}M_Y(at) = e^{tb}E(e^{atY})$$ $$ M_W(t) = e^{tb}\sum\limits_{y=0}^ne^{aty}\binom{n}{y}p^yq^{n-y}.$$ We are provided with the information that $W = n -Y$. $$ M_W(t) = e^{tb}\sum\limits_{n-w=0}^ne^{at(n-w)}\binom{n}{n-w}p^{n-w}q^{w}.$$ I know that we seek to manipulate this summation in such a way that $$ M_W(t) = \sum\limits_{n-w=0}^n\binom{n}{n-w}p^{n-w}(e^tq)^{w}.$$ I have now idea where to begin.","If $Y$ is a random variable with moment-generating function $M_Y(t)$ and if $W$ is given by $W=aY+b$, then the moment generating function of $W$ is $e^{tb}M_Y(at)$ Suppose that $Y$ is a binomial random variable based on $n$ trials with success probability of $p$ and let $W = n - Y$. Show that the moment generating function of $W$ is $M_W(t) = (qe^t+p)^n$ using the fact that $M_W(t) =e^{tb}M_Y(at)$. If $W$ is a binomial random variable, then the moment generating function in the open form is given by $$ M_W(t) = e^{tb}M_Y(at) = e^{tb}E(e^{atY})$$ $$ M_W(t) = e^{tb}\sum\limits_{y=0}^ne^{aty}\binom{n}{y}p^yq^{n-y}.$$ We are provided with the information that $W = n -Y$. $$ M_W(t) = e^{tb}\sum\limits_{n-w=0}^ne^{at(n-w)}\binom{n}{n-w}p^{n-w}q^{w}.$$ I know that we seek to manipulate this summation in such a way that $$ M_W(t) = \sum\limits_{n-w=0}^n\binom{n}{n-w}p^{n-w}(e^tq)^{w}.$$ I have now idea where to begin.",,"['statistics', 'summation', 'random-variables', 'moment-generating-functions', 'binomial-distribution']"
56,"""Nested"" Binomial Distribution","""Nested"" Binomial Distribution",,"I am currently working with a statistical distribution, and I'm wondering if any exploration has been done on this. The distribution is denoted $\xi$. To construct $\xi$ we use auxillary random variables $X_1,X_2, \ldots, X_k$ For indexing purposes we let the random variable $X_1 = 1$ $X_2 \sim \mathrm{Bin}(4, \frac{1}{2})$ $X_3 \sim \mathrm{Bin}(4X_2, \frac{1}{2})$ and in general $X_k \sim \mathrm{Bin}(4X_{k-1}, \frac{1}{2})$ So that the number of trials of the binomial distribution is itself a random variable. One can show that the sum of $P(X_k = 0)$ over all $k$ is about $0.08737$. The distribution $\xi$ is given by $P(\xi=k) := \dfrac{P(X_{k+1} = 0) - P(X_k = 0)}{0.08737}$ (we divide because probabilities have to add up to $1$).","I am currently working with a statistical distribution, and I'm wondering if any exploration has been done on this. The distribution is denoted $\xi$. To construct $\xi$ we use auxillary random variables $X_1,X_2, \ldots, X_k$ For indexing purposes we let the random variable $X_1 = 1$ $X_2 \sim \mathrm{Bin}(4, \frac{1}{2})$ $X_3 \sim \mathrm{Bin}(4X_2, \frac{1}{2})$ and in general $X_k \sim \mathrm{Bin}(4X_{k-1}, \frac{1}{2})$ So that the number of trials of the binomial distribution is itself a random variable. One can show that the sum of $P(X_k = 0)$ over all $k$ is about $0.08737$. The distribution $\xi$ is given by $P(\xi=k) := \dfrac{P(X_{k+1} = 0) - P(X_k = 0)}{0.08737}$ (we divide because probabilities have to add up to $1$).",,['statistics']
57,MLE for the PDF $f_\theta(x)=\theta x$ on $0\leq x\leq\sqrt{2/\theta}$: where is the mistake?,MLE for the PDF  on : where is the mistake?,f_\theta(x)=\theta x 0\leq x\leq\sqrt{2/\theta},"Consider $f_X(x;\theta)=\theta\cdot x$, $x\leq\sqrt{\frac{2}{\theta}}$.   Find the maximum likelihood for the estimator $\hat{\theta}$ of $\theta$. By definition, the MLE of $f(x_1\ldots,x_n;\hat{\theta})=\max.f(x_1,\ldots,x_n;\theta)$ $$L(\theta)=\prod_{i=1}^n f(x_i;\theta)  \implies \ln (L(\theta))=\sum_{i=1}^n \ln(\theta \cdot x_i)=n\cdot \ln(\theta)+\sum_{i=1}^n\ln(x_i)$$ $$\implies \frac{d}{d\theta}L'(\theta)=\frac{d}{d\theta}n\cdot \ln(\theta)=\frac{n}{\theta}=0 \iff n=0$$ This makes of course no sense, so could anyone give me a hint where I made a mistake?","Consider $f_X(x;\theta)=\theta\cdot x$, $x\leq\sqrt{\frac{2}{\theta}}$.   Find the maximum likelihood for the estimator $\hat{\theta}$ of $\theta$. By definition, the MLE of $f(x_1\ldots,x_n;\hat{\theta})=\max.f(x_1,\ldots,x_n;\theta)$ $$L(\theta)=\prod_{i=1}^n f(x_i;\theta)  \implies \ln (L(\theta))=\sum_{i=1}^n \ln(\theta \cdot x_i)=n\cdot \ln(\theta)+\sum_{i=1}^n\ln(x_i)$$ $$\implies \frac{d}{d\theta}L'(\theta)=\frac{d}{d\theta}n\cdot \ln(\theta)=\frac{n}{\theta}=0 \iff n=0$$ This makes of course no sense, so could anyone give me a hint where I made a mistake?",,"['statistics', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
58,Finding new standard deviation and mean after adding an element,Finding new standard deviation and mean after adding an element,,"Say I have a mean and standard deviation for a dataset of 5 elements. I now add a sixth element. Is there a way to calculate the new mean and standard deviation using the information we had prior (i.e. not just recalculating the whole thing from scratch)? For the mean, I see that I can just multiply the old one by $5$, add the new element, and divide by $6$. I'm not sure if there's something I can do with the standard deviation, however. $$\sigma_{old} = \sqrt{\sum_i (X_i - \mu_{old})^2}$$ $$\sigma_{new} = \sqrt{\sum_i (X_i - \mu_{new})^2 + (X_{new} - \mu_{new})^2}$$ $$\mu_{new} = \frac{\mu_{old}*N + X_{new}}{N+1}$$ $$\sigma^2_{new} = \sigma^2_{old} + \sum_i \left( (X_i - \mu_{new})^2 - (X_i - \mu_{old})^2 \right) + (X_{new} - \mu_{new})^2$$ After putting it in terms of the old stats, this becomes (I think) $$\sigma^2_{new} = \sigma^2_{old} + \sum_i \left(2 X_i + \frac{(2N+1) \mu_{old} + X_{new}}{N+1} \right) \left(\frac{X_{new} - \mu_{old}}{N+1}\right) + (X_{new} - \frac{\mu_{old}*N + X_{new}}{N+1})^2$$ Is there anything better than this monstrosity?","Say I have a mean and standard deviation for a dataset of 5 elements. I now add a sixth element. Is there a way to calculate the new mean and standard deviation using the information we had prior (i.e. not just recalculating the whole thing from scratch)? For the mean, I see that I can just multiply the old one by $5$, add the new element, and divide by $6$. I'm not sure if there's something I can do with the standard deviation, however. $$\sigma_{old} = \sqrt{\sum_i (X_i - \mu_{old})^2}$$ $$\sigma_{new} = \sqrt{\sum_i (X_i - \mu_{new})^2 + (X_{new} - \mu_{new})^2}$$ $$\mu_{new} = \frac{\mu_{old}*N + X_{new}}{N+1}$$ $$\sigma^2_{new} = \sigma^2_{old} + \sum_i \left( (X_i - \mu_{new})^2 - (X_i - \mu_{old})^2 \right) + (X_{new} - \mu_{new})^2$$ After putting it in terms of the old stats, this becomes (I think) $$\sigma^2_{new} = \sigma^2_{old} + \sum_i \left(2 X_i + \frac{(2N+1) \mu_{old} + X_{new}}{N+1} \right) \left(\frac{X_{new} - \mu_{old}}{N+1}\right) + (X_{new} - \frac{\mu_{old}*N + X_{new}}{N+1})^2$$ Is there anything better than this monstrosity?",,['statistics']
59,Distribution of minimum absolute value,Distribution of minimum absolute value,,"Consider $K$ independent Laplace variables $X_k, k=1,\ldots,K$, with mean 0 and scale $\lambda$ (so that their PDF is $f(x)=\frac{1}{2\lambda}e^{-\frac{|x|}{\lambda}}$. Let $Y$ be the variable taking the value of the Laplace variable whose absolute value is the minimum among all $X_k$. That is, $$Y=X_{k^*}$$ $$k^*=arg\min_k{|X_k|}$$ I would like to know what the CDF of $Y$ is. Does it also follow a Laplace distribution? How might I prove or disprove that? Many thanks!","Consider $K$ independent Laplace variables $X_k, k=1,\ldots,K$, with mean 0 and scale $\lambda$ (so that their PDF is $f(x)=\frac{1}{2\lambda}e^{-\frac{|x|}{\lambda}}$. Let $Y$ be the variable taking the value of the Laplace variable whose absolute value is the minimum among all $X_k$. That is, $$Y=X_{k^*}$$ $$k^*=arg\min_k{|X_k|}$$ I would like to know what the CDF of $Y$ is. Does it also follow a Laplace distribution? How might I prove or disprove that? Many thanks!",,"['statistics', 'probability-distributions', 'absolute-value']"
60,Impact of random numbers on the eigen-values,Impact of random numbers on the eigen-values,,"How do the eigen-values of the following tridiagonal matrix ($A$) change when adding random numbers $R_i$ (with a normal distribution with the mean 0 and variance $m$) to its diagonal.  A is a square matrix defined as follows: For i = 1 to N $$ A(i,i) = -2 + R_i $$ $$ A(i,i+1) = 1 $$ $$ A(i,i-1) = 1 $$ where m and N are constants. Notice that the eigen-values are known for the case $R_i=0$: for j=1 to N; $$ Eig(j) = -2 -2 cos(j\pi/N) $$ Numerical solution reveals that the eigen values around the maximum eigenvalue have the largest deviation from the case without random numbers ($R_i=0$). For example, the following figure shows eigen values with and without having the random numbers for m=0.1. But how we can solve the problem analytically? (Maybe by using central limit theorem)","How do the eigen-values of the following tridiagonal matrix ($A$) change when adding random numbers $R_i$ (with a normal distribution with the mean 0 and variance $m$) to its diagonal.  A is a square matrix defined as follows: For i = 1 to N $$ A(i,i) = -2 + R_i $$ $$ A(i,i+1) = 1 $$ $$ A(i,i-1) = 1 $$ where m and N are constants. Notice that the eigen-values are known for the case $R_i=0$: for j=1 to N; $$ Eig(j) = -2 -2 cos(j\pi/N) $$ Numerical solution reveals that the eigen values around the maximum eigenvalue have the largest deviation from the case without random numbers ($R_i=0$). For example, the following figure shows eigen values with and without having the random numbers for m=0.1. But how we can solve the problem analytically? (Maybe by using central limit theorem)",,"['statistics', 'eigenvalues-eigenvectors', 'random-variables', 'random-matrices']"
61,Bootstrap method failing where blocking works,Bootstrap method failing where blocking works,,"I'm computing an average of individual samples that are not entirely independent and need an estimate for the true standard deviation. According to Newman and Barkema's book the most reliable method will be Bootstrap sampling (see section 3.4.3), where you don't have to worry about the samples being independent and which should give an estimate of the standard deviation of the mean $\sigma_m\approx\sigma\ /\sqrt{n}$ where $n$ is the number of samples. However I proceed to compute the average a number of times so that I get a brute force estimate of the actual $\sigma_m$, and it turns out that the bootstrap is consistently underestimating this. In itself that is maybe not so strange; the bootstrap being an estimate. But the weird thing is that if I use the blocking (or binning) method (see 3.4.2) I get a much better estimate - while according to Newman and Barkema this should be a much more primitive method. In fact the bootstrap consistently gives an estimate very close to the naive $\sigma_m\approx\sqrt{\big(\ \overline{x^2}-\overline{x}^2\ \big)\ /\ n}$. Any idea what's going on?","I'm computing an average of individual samples that are not entirely independent and need an estimate for the true standard deviation. According to Newman and Barkema's book the most reliable method will be Bootstrap sampling (see section 3.4.3), where you don't have to worry about the samples being independent and which should give an estimate of the standard deviation of the mean $\sigma_m\approx\sigma\ /\sqrt{n}$ where $n$ is the number of samples. However I proceed to compute the average a number of times so that I get a brute force estimate of the actual $\sigma_m$, and it turns out that the bootstrap is consistently underestimating this. In itself that is maybe not so strange; the bootstrap being an estimate. But the weird thing is that if I use the blocking (or binning) method (see 3.4.2) I get a much better estimate - while according to Newman and Barkema this should be a much more primitive method. In fact the bootstrap consistently gives an estimate very close to the naive $\sigma_m\approx\sqrt{\big(\ \overline{x^2}-\overline{x}^2\ \big)\ /\ n}$. Any idea what's going on?",,"['statistics', 'standard-deviation', 'monte-carlo', 'means']"
62,Finding Percentiles - How am I wrong?,Finding Percentiles - How am I wrong?,,So the problem goes: There are 40 students and their test scores are: 30 35 43 44 47 48 54 55 56 57    59 62 63 65 66 68 69 69 71 72   72 73 74 76 77 77 78 79 80 81   81 82 83 85 89 92 93 94 97 98 Find P85. What I did: Using the formula L=(K/100)n... I did L=(85/100) => .85(40) = 34. The 34th value is 85. But the answer is actually 87?? (Odd.. there isn't even 87 in the values :)),So the problem goes: There are 40 students and their test scores are: 30 35 43 44 47 48 54 55 56 57    59 62 63 65 66 68 69 69 71 72   72 73 74 76 77 77 78 79 80 81   81 82 83 85 89 92 93 94 97 98 Find P85. What I did: Using the formula L=(K/100)n... I did L=(85/100) => .85(40) = 34. The 34th value is 85. But the answer is actually 87?? (Odd.. there isn't even 87 in the values :)),,"['statistics', 'percentile']"
63,Central sample moments are asymptotically unbiased,Central sample moments are asymptotically unbiased,,"Let $\newcommand\top{\overset p\to}\newcommand\isd{\overset d=}\newcommand\P{\mathcal P}\DeclareMathOperator\var{Var}$$X\isd X_1\isd x_2\isd\ldots\isd X_n$ be independent stochastic variables with same distribution for which the central moments $\mu_k'=E((X-E(X))^k)$ exist. (And suppose $E(X)$ exists.) An exercise in our course notes asks to prove that the central sample moments $m_k'=\frac 1n((X_1-\bar X_n)^k+\cdots+(X_n-\bar X_n)^k)$ are consistent and asymptotically unbiased estimators for the $\mu_k'$. ($\bar X_n$ denotes the sample mean $\frac1n(X_1+\cdots+X_n)$.) Recall ""$T_n$ is asymptotically unbiased for $t$"" means $E(T_n)\to t$, and ""$T_n$ is consistent for $t$"" means $\P(|T_n-t|>\varepsilon)\to0$ for all $\varepsilon>0$ (that is, $T_n$ converges in probability to $t$, denoted $T_n\top t$). I was able to do the consistency part, based on the Weak Law of Large Numbers (WLLN) which says $\bar X_n\top E(X)$ for i.i.d. $X_l$'s: $$\frac1n((X_1-\bar X_n)^k+\cdots+(X_n-\bar X_n)^k)=\sum_{j=1}^k\binom kj(-\bar X_n)^{k-j}\frac1n\sum_{l=1}^nX_l^j.$$ By WLLN, each $\frac1n\sum_{l=1}^nX_l^j\top E(X^j)$ and $\bar X_n\top E(X)$ and hence the whole thing $$\top\sum_{j=1}^k\binom kj (-E(X))^{k-j}E(X^j)=E((X-E(X))^k).$$ (Recall $A_n\top A$ and $B_n\top B$ implies $A_n+B_n\top A+B$ and $A_nB_n\top AB$.) Proving the asymptotic unbiasedness seems much harder: $$E\left(\frac {(X_1-\bar X_n)^k+\cdots+(X_n-\bar X_n)^k}n\right)\overset?\to E((X-E(X))^k).$$ I have some ideas but can't make a proof from it: I know that $E(T_n)\to t$ implies $T_n\top t$ (provided $\var(T_n)\to0$). I don't know if the reverse is true, at least I can't invert the proof of that implication. We have $E(\frac1n(X_1^k+\cdots+X_n^k))=\frac1n\cdot n\cdot E(X^k)=E(X^k)$. We can't however do the same because each $k$th power $(X_l-\bar X_n)^k$ involves every other $X_l$ too. Perhaps we can use the same strategy with Newton's binomial expansion as above, but I don't think there's a general rule which says that $E(A_n)\to a$ and $E(B_n)\to b$ implies $E(A_nB_n)\to ab$. Since at this point in the notes the concept of convergence in distribution isn't introduced yet, I don't expect to need it. The Strong Law of Large Numbers (for i.i.d. $X_l$: $\bar X_n\to E(X)$ almost everywhere provided $E(X)$ exists) has been mentioned without proof but can be used, I suppose. I couldn't find a proof on the internet. How can I prove it?","Let $\newcommand\top{\overset p\to}\newcommand\isd{\overset d=}\newcommand\P{\mathcal P}\DeclareMathOperator\var{Var}$$X\isd X_1\isd x_2\isd\ldots\isd X_n$ be independent stochastic variables with same distribution for which the central moments $\mu_k'=E((X-E(X))^k)$ exist. (And suppose $E(X)$ exists.) An exercise in our course notes asks to prove that the central sample moments $m_k'=\frac 1n((X_1-\bar X_n)^k+\cdots+(X_n-\bar X_n)^k)$ are consistent and asymptotically unbiased estimators for the $\mu_k'$. ($\bar X_n$ denotes the sample mean $\frac1n(X_1+\cdots+X_n)$.) Recall ""$T_n$ is asymptotically unbiased for $t$"" means $E(T_n)\to t$, and ""$T_n$ is consistent for $t$"" means $\P(|T_n-t|>\varepsilon)\to0$ for all $\varepsilon>0$ (that is, $T_n$ converges in probability to $t$, denoted $T_n\top t$). I was able to do the consistency part, based on the Weak Law of Large Numbers (WLLN) which says $\bar X_n\top E(X)$ for i.i.d. $X_l$'s: $$\frac1n((X_1-\bar X_n)^k+\cdots+(X_n-\bar X_n)^k)=\sum_{j=1}^k\binom kj(-\bar X_n)^{k-j}\frac1n\sum_{l=1}^nX_l^j.$$ By WLLN, each $\frac1n\sum_{l=1}^nX_l^j\top E(X^j)$ and $\bar X_n\top E(X)$ and hence the whole thing $$\top\sum_{j=1}^k\binom kj (-E(X))^{k-j}E(X^j)=E((X-E(X))^k).$$ (Recall $A_n\top A$ and $B_n\top B$ implies $A_n+B_n\top A+B$ and $A_nB_n\top AB$.) Proving the asymptotic unbiasedness seems much harder: $$E\left(\frac {(X_1-\bar X_n)^k+\cdots+(X_n-\bar X_n)^k}n\right)\overset?\to E((X-E(X))^k).$$ I have some ideas but can't make a proof from it: I know that $E(T_n)\to t$ implies $T_n\top t$ (provided $\var(T_n)\to0$). I don't know if the reverse is true, at least I can't invert the proof of that implication. We have $E(\frac1n(X_1^k+\cdots+X_n^k))=\frac1n\cdot n\cdot E(X^k)=E(X^k)$. We can't however do the same because each $k$th power $(X_l-\bar X_n)^k$ involves every other $X_l$ too. Perhaps we can use the same strategy with Newton's binomial expansion as above, but I don't think there's a general rule which says that $E(A_n)\to a$ and $E(B_n)\to b$ implies $E(A_nB_n)\to ab$. Since at this point in the notes the concept of convergence in distribution isn't introduced yet, I don't expect to need it. The Strong Law of Large Numbers (for i.i.d. $X_l$: $\bar X_n\to E(X)$ almost everywhere provided $E(X)$ exists) has been mentioned without proof but can be used, I suppose. I couldn't find a proof on the internet. How can I prove it?",,"['statistics', 'convergence-divergence', 'expectation']"
64,Portfolio VaR with Copula?,Portfolio VaR with Copula?,,"Let the portfolio be given by: $$X=X_1+X_2$$ $(X_1,X_2)$ are dependent through a Copula function $C(u_1,u_2)$, such that the joint distribution is given by: $$F(x_1,x_2)=C(F(x_1),F(x_2))$$ What is the VaR of this portfolio? Usually VaR is the inverse quantile: $VaR_\alpha=F^{-1}(\alpha)$. I am not sure how to determine it in this multivariate case?","Let the portfolio be given by: $$X=X_1+X_2$$ $(X_1,X_2)$ are dependent through a Copula function $C(u_1,u_2)$, such that the joint distribution is given by: $$F(x_1,x_2)=C(F(x_1),F(x_2))$$ What is the VaR of this portfolio? Usually VaR is the inverse quantile: $VaR_\alpha=F^{-1}(\alpha)$. I am not sure how to determine it in this multivariate case?",,"['statistics', 'finance']"
65,How to prove it has a $\chi^{2}$ distribution,How to prove it has a  distribution,\chi^{2},"I tried to make $T$ close to $$T_1=\left(\frac{(x_1 - \mu_1)^2}{\sigma_1^2}+\frac{(x_2 - \mu_2)^2}{\sigma_2^2}-2 \frac{\rho}{\sigma_1 \sigma_2} \frac{x_1 - \mu_1}{\sigma_1}\frac{x_2 - \mu_2}{\sigma_2}\right)\frac{1}{1-(\frac{\rho}{\sigma_1\sigma_2})^2}$$ to solve the following problem. Problem: let $x=(X_1,X_2)'$ follow $N_2(\mu,\Sigma)$, where $\mu=(\mu_1,\mu_2)$, and $\Sigma=\begin{bmatrix} \sigma_1^2 & \rho\\ \rho & \sigma_2^2 \end{bmatrix}$. Show that $$T=\frac{1}{2(1- \rho^2)}\left(\frac{(x_1 - \mu_1)^2}{\sigma_1^2}+\frac{(x_2 - \mu_2)^2}{\sigma_2^2}-2 \rho \frac{x_1 - \mu_1}{\sigma_1}\frac{x_2 - \mu_2}{\sigma_2}\right)$$ has a $\chi^2$ distribution. And give the its parameters? Since $T_1$ follows as $\chi^2$, I tried to make $T$ close to $T_1$, but I failed since $$2 \frac{\rho}{\sigma_1 \sigma_2} \frac{x_1 - \mu_1}{\sigma_1}\frac{x_2 - \mu_2}{\sigma_2}\text{ and }2 \rho \frac{x_1 - \mu_1}{\sigma_1}\frac{x_2 - \mu_2}{\sigma_2}$$ are different.","I tried to make $T$ close to $$T_1=\left(\frac{(x_1 - \mu_1)^2}{\sigma_1^2}+\frac{(x_2 - \mu_2)^2}{\sigma_2^2}-2 \frac{\rho}{\sigma_1 \sigma_2} \frac{x_1 - \mu_1}{\sigma_1}\frac{x_2 - \mu_2}{\sigma_2}\right)\frac{1}{1-(\frac{\rho}{\sigma_1\sigma_2})^2}$$ to solve the following problem. Problem: let $x=(X_1,X_2)'$ follow $N_2(\mu,\Sigma)$, where $\mu=(\mu_1,\mu_2)$, and $\Sigma=\begin{bmatrix} \sigma_1^2 & \rho\\ \rho & \sigma_2^2 \end{bmatrix}$. Show that $$T=\frac{1}{2(1- \rho^2)}\left(\frac{(x_1 - \mu_1)^2}{\sigma_1^2}+\frac{(x_2 - \mu_2)^2}{\sigma_2^2}-2 \rho \frac{x_1 - \mu_1}{\sigma_1}\frac{x_2 - \mu_2}{\sigma_2}\right)$$ has a $\chi^2$ distribution. And give the its parameters? Since $T_1$ follows as $\chi^2$, I tried to make $T$ close to $T_1$, but I failed since $$2 \frac{\rho}{\sigma_1 \sigma_2} \frac{x_1 - \mu_1}{\sigma_1}\frac{x_2 - \mu_2}{\sigma_2}\text{ and }2 \rho \frac{x_1 - \mu_1}{\sigma_1}\frac{x_2 - \mu_2}{\sigma_2}$$ are different.",,"['statistics', 'multivariable-calculus', 'probability-distributions']"
66,How are critical values derived for the Kolmogorov-Smirnov Test?,How are critical values derived for the Kolmogorov-Smirnov Test?,,"One appealing feature of the K-S test is that it is distribution-free. So this leads to my question - how are the critical values for the K-S derived, then? Is there a way to express the critical values as an integral, like for percentiles of the standard normal distribution? Sources that have such information would be very helpful (i.e., a textbook). See, for example, the table below (from http://people.cs.pitt.edu/~lipschultz/cs1538/prob-table_KS.pdf ).","One appealing feature of the K-S test is that it is distribution-free. So this leads to my question - how are the critical values for the K-S derived, then? Is there a way to express the critical values as an integral, like for percentiles of the standard normal distribution? Sources that have such information would be very helpful (i.e., a textbook). See, for example, the table below (from http://people.cs.pitt.edu/~lipschultz/cs1538/prob-table_KS.pdf ).",,['statistics']
67,Determine sample size according to some unknown distribution with given error rate and confidence,Determine sample size according to some unknown distribution with given error rate and confidence,,"Assume $x\in\mathbb{N}$ obey some unknown distribution, and I can sequentially  and independently acquire infinite samples of $x$.  Now, given an error rate $\epsilon$ and confidence $1-\delta$, can I find a sufficient big number $m$ that if I draw $m$ samples, the probability of drawing a sample beyond the maximum value $M$ observed is less than $\epsilon$ with confidence $1-\delta$? (That is, if I already got a sample $S_m = \{x_1, \ldots, x_m\}$, then $Pr[Pr[x_{m+1}>\text{max}\{x_i | x_i\in S_m\}]\geq\epsilon]\leq\delta$)","Assume $x\in\mathbb{N}$ obey some unknown distribution, and I can sequentially  and independently acquire infinite samples of $x$.  Now, given an error rate $\epsilon$ and confidence $1-\delta$, can I find a sufficient big number $m$ that if I draw $m$ samples, the probability of drawing a sample beyond the maximum value $M$ observed is less than $\epsilon$ with confidence $1-\delta$? (That is, if I already got a sample $S_m = \{x_1, \ldots, x_m\}$, then $Pr[Pr[x_{m+1}>\text{max}\{x_i | x_i\in S_m\}]\geq\epsilon]\leq\delta$)",,"['statistics', 'statistical-inference', 'sampling']"
68,"Finding an efficient estimator for $\theta$ in $U[0, \theta]$ in terms of the sample maximum",Finding an efficient estimator for  in  in terms of the sample maximum,"\theta U[0, \theta]","This question appeared in a past exam paper, in the form: Let $X = (X_1\dotsc X_n)\in\mathbb{R}^n$ be an i.i.d. sample from $U[0, \theta], \theta>0$ Apply Rao-Blackwell's theorem to the unbiased estimator $2X_1$ using the statistic $X_{(n)} = \max\{X_i\}$ to compute the efficient estimator for $\theta$. (In other parts of the problem we showed that $X_{(n)}$ is a complete sufficient statistic) My working looks like this: \begin{align} F_{X_{(n)}}(y)     &= P(X_{(n)}\leq y)= F_{X_1}(y)^n \\     &= \left(\frac{y}{\theta}\right)^n\mathbf{1}(0\leq y\leq \theta)          + \mathbf{1}(y>\theta)\\ f_{X_{(n)}}(y)     &= \frac{ny^{n-1}}{\theta^n}\mathbf{1}(0\leq y\leq \theta)\\ F_{(X_{1},X_{(n)})}(x, y)     &= P(X_1\leq x, X_{(n)}\leq y)\\      &= \left\{         \begin{array}{cl}             \theta^{-n}xy^{n-1} & :0\leq x\leq y\leq \theta \\             \theta^{-n}y^n &: 0\leq y\leq x\leq \theta         \end{array}     \right.\\ f{(X_{1},X_{(n)})}(x, y)      &= \theta^{-n}(n-1)y^{n-2}\mathbf{1}(0\leq x\leq y\leq \theta)\\ f_{X_{1}|X_{(n)}}(x| y)     &=\frac{\theta^{-n}(n-1)y^{n-2}}{\theta^{-n}ny^{n-1}}\mathbf{1}(0\leq x\leq y)\\     &= \frac{n-1}{n}y^{-1}\mathbf{1}(0\leq x\leq y)\\ \mathbf{E}(X_1|X_{(n)}=y)     &=\frac{n-1}{n}\int_0^y\frac{x}{y}dy \\     &=\frac{n-1}{n}\frac{y}{2} \\ \mathbf{E}(2X_1|X_{(n)})     &= \frac{n-1}{n} X_{(n)} \end{align} According to Rao-Blackwell's theorem , this should yield an unbiased estimator for $\theta$. Unfortunately, it is not unbiased and also yields an impossible value of $\theta$, since $X_{(n)}<\theta$. Playing around with the result, I found that $\frac{n+1}{n} X_{(n)}$ does the job and makes sense, but I can't figure out where I went wrong in my calculations and would be very grateful if someone could point my error out.","This question appeared in a past exam paper, in the form: Let $X = (X_1\dotsc X_n)\in\mathbb{R}^n$ be an i.i.d. sample from $U[0, \theta], \theta>0$ Apply Rao-Blackwell's theorem to the unbiased estimator $2X_1$ using the statistic $X_{(n)} = \max\{X_i\}$ to compute the efficient estimator for $\theta$. (In other parts of the problem we showed that $X_{(n)}$ is a complete sufficient statistic) My working looks like this: \begin{align} F_{X_{(n)}}(y)     &= P(X_{(n)}\leq y)= F_{X_1}(y)^n \\     &= \left(\frac{y}{\theta}\right)^n\mathbf{1}(0\leq y\leq \theta)          + \mathbf{1}(y>\theta)\\ f_{X_{(n)}}(y)     &= \frac{ny^{n-1}}{\theta^n}\mathbf{1}(0\leq y\leq \theta)\\ F_{(X_{1},X_{(n)})}(x, y)     &= P(X_1\leq x, X_{(n)}\leq y)\\      &= \left\{         \begin{array}{cl}             \theta^{-n}xy^{n-1} & :0\leq x\leq y\leq \theta \\             \theta^{-n}y^n &: 0\leq y\leq x\leq \theta         \end{array}     \right.\\ f{(X_{1},X_{(n)})}(x, y)      &= \theta^{-n}(n-1)y^{n-2}\mathbf{1}(0\leq x\leq y\leq \theta)\\ f_{X_{1}|X_{(n)}}(x| y)     &=\frac{\theta^{-n}(n-1)y^{n-2}}{\theta^{-n}ny^{n-1}}\mathbf{1}(0\leq x\leq y)\\     &= \frac{n-1}{n}y^{-1}\mathbf{1}(0\leq x\leq y)\\ \mathbf{E}(X_1|X_{(n)}=y)     &=\frac{n-1}{n}\int_0^y\frac{x}{y}dy \\     &=\frac{n-1}{n}\frac{y}{2} \\ \mathbf{E}(2X_1|X_{(n)})     &= \frac{n-1}{n} X_{(n)} \end{align} According to Rao-Blackwell's theorem , this should yield an unbiased estimator for $\theta$. Unfortunately, it is not unbiased and also yields an impossible value of $\theta$, since $X_{(n)}<\theta$. Playing around with the result, I found that $\frac{n+1}{n} X_{(n)}$ does the job and makes sense, but I can't figure out where I went wrong in my calculations and would be very grateful if someone could point my error out.",,"['statistics', 'conditional-probability', 'parameter-estimation']"
69,Why choosing Linear Regression?,Why choosing Linear Regression?,,"First I would like to say that I am not a statistician nor am I good in the field. I have been collecting data for over a period of e.g 100 days and each day has a varying amount of data that I can collect. So for example yesterday I collected 10 and tomorrow I might collect 20 or 2 etc. I collected all this data and made a graph in excel which basically shows the flow. Now, I wanted to see how based on all the data I have up till now, what can I expect tomorrow? Excel provides some (click and produce) trend options like exponential, linear regression, etc. So, although I don't fully understand how they work, I used the linear regression trend. In my report draft, my teacher insist on knowing why linear regression but I don't know a right defense. Was using linear regression right and if so, how may I defend the usage in my scenario ? Please use the image below for reference.","First I would like to say that I am not a statistician nor am I good in the field. I have been collecting data for over a period of e.g 100 days and each day has a varying amount of data that I can collect. So for example yesterday I collected 10 and tomorrow I might collect 20 or 2 etc. I collected all this data and made a graph in excel which basically shows the flow. Now, I wanted to see how based on all the data I have up till now, what can I expect tomorrow? Excel provides some (click and produce) trend options like exponential, linear regression, etc. So, although I don't fully understand how they work, I used the linear regression trend. In my report draft, my teacher insist on knowing why linear regression but I don't know a right defense. Was using linear regression right and if so, how may I defend the usage in my scenario ? Please use the image below for reference.",,"['statistics', 'regression']"
70,"Bayesian statistics, bivariate prior distribution","Bayesian statistics, bivariate prior distribution",,"I've got a simple question buy I'm not sure how to solve it. It's a bit long. Suppose you've got $n$ iid random variables $X_1$, $\dots$, $X_n$ from the normal distribution with unknown mean $M$ and unknown precision (inverse variance) $H$. Then we've got the likelihood function for data $X_1=x_1$, $\dots$, $X_n=x_n$ $$L_n(\mu,h)\propto h^{n/2}\exp\left(-\frac{1}{2}h\left(n(\bar{x}-\mu)^2+S\right)\right), $$ where $\bar{x}$ is the mean of $x_1$, $\dots$, $x_n$ and $S=\sum_i(x_i-\bar{x})^2$. Now, a bivariate prior distribution for $(M,H)$ is specified, in terms of hyperparameters $(\alpha_0,\beta_0,m_0,\lambda_0)$, as follows. The marginal distribution of $H$ is $\Gamma(\alpha_0,\beta_0)$ with density $$\pi(h)\propto  h^{\alpha_0-1}e^{-\beta_0h}$$ for $h>0$, and the conditional distribution of $M$ given $H=h$ is normal with mean $m_0$ and precision $\lambda_0h$. Now I should find the posterior joint distribution of $(M,h)$ given data $X_1=x_1$, $\dots$, $X_n=x_n$ and give the updated hyperparameters $(\alpha_n,\beta_n,m_n,\lambda_n)$ in terms of the prior hyperparameters and the data. Could somebody please tell me how to do this? Thank you very much.","I've got a simple question buy I'm not sure how to solve it. It's a bit long. Suppose you've got $n$ iid random variables $X_1$, $\dots$, $X_n$ from the normal distribution with unknown mean $M$ and unknown precision (inverse variance) $H$. Then we've got the likelihood function for data $X_1=x_1$, $\dots$, $X_n=x_n$ $$L_n(\mu,h)\propto h^{n/2}\exp\left(-\frac{1}{2}h\left(n(\bar{x}-\mu)^2+S\right)\right), $$ where $\bar{x}$ is the mean of $x_1$, $\dots$, $x_n$ and $S=\sum_i(x_i-\bar{x})^2$. Now, a bivariate prior distribution for $(M,H)$ is specified, in terms of hyperparameters $(\alpha_0,\beta_0,m_0,\lambda_0)$, as follows. The marginal distribution of $H$ is $\Gamma(\alpha_0,\beta_0)$ with density $$\pi(h)\propto  h^{\alpha_0-1}e^{-\beta_0h}$$ for $h>0$, and the conditional distribution of $M$ given $H=h$ is normal with mean $m_0$ and precision $\lambda_0h$. Now I should find the posterior joint distribution of $(M,h)$ given data $X_1=x_1$, $\dots$, $X_n=x_n$ and give the updated hyperparameters $(\alpha_n,\beta_n,m_n,\lambda_n)$ in terms of the prior hyperparameters and the data. Could somebody please tell me how to do this? Thank you very much.",,"['statistics', 'bayesian']"
71,Is Calculus a requirement to become better at Probability and Satistics?,Is Calculus a requirement to become better at Probability and Satistics?,,"Is Calculus really required to be better at Statistics and Probability and to be a good Data Scientist? Arthur Benjamin says in his TED video : ""Very few people actually use calculus in a conscious, meaningful way in their day-to-day lives.  On the other hand, statistics–that’s a subject that you could, and should, use on a daily basis.” “If it’s taught properly, it can be a lot of FUN. I mean, probability and statistics–it’s the mathematics of games and gambling, it’s…it’s analyzing trends, it’s predicting the future.”","Is Calculus really required to be better at Statistics and Probability and to be a good Data Scientist? Arthur Benjamin says in his TED video : ""Very few people actually use calculus in a conscious, meaningful way in their day-to-day lives.  On the other hand, statistics–that’s a subject that you could, and should, use on a daily basis.” “If it’s taught properly, it can be a lot of FUN. I mean, probability and statistics–it’s the mathematics of games and gambling, it’s…it’s analyzing trends, it’s predicting the future.”",,"['calculus', 'statistics']"
72,Finding MLE of $f(x;\theta) =1$ if $\theta-1/2<x< \theta+1/2$,Finding MLE of  if,f(x;\theta) =1 \theta-1/2<x< \theta+1/2,"Let $X_1,...,X_n$ have density: $$f(x;\theta) = \begin{cases} 1 & \text{if } \theta-1/2<x< \theta+1/2 \\  0 & \text{otherwise} \end{cases}$$ Let $Y_1=\min \lbrace X_1,\ldots,X_n \rbrace$ and $Y_n=\max \lbrace X_1,\ldots,X_n\rbrace$ Show that any statistic $u(X_1,\ldots,X_n)$ that satisfies $Y_n-1/2<u<Y_1+1/2$ is a maximum likelihood estimate of $\theta$. My attempt: First rewrite the density: $$f(x;\theta) = \begin{cases} 1 &\text{if } -1/2<x- \theta<1/2 \\  0 & \text{otherwise} \end{cases}$$ Okay, so we obviously need to use the fact that $Y_n-1/2<u<Y_1+1/2$, I started by using the regular MLE finding method: $$L(\theta)=\prod_i(x-\theta)$$ $$\log L(\theta)=\log\prod_i(x_i-\theta)=\sum_i \log(x_i- \theta)$$ $$(\log L(\theta))'=\sum_i \frac 1 {x_i- \theta} =0$$ And I'm stuck here..I think that I need to use the given here but I'm not sure how to proceed. Any help would really be appreciated:) Thanks!","Let $X_1,...,X_n$ have density: $$f(x;\theta) = \begin{cases} 1 & \text{if } \theta-1/2<x< \theta+1/2 \\  0 & \text{otherwise} \end{cases}$$ Let $Y_1=\min \lbrace X_1,\ldots,X_n \rbrace$ and $Y_n=\max \lbrace X_1,\ldots,X_n\rbrace$ Show that any statistic $u(X_1,\ldots,X_n)$ that satisfies $Y_n-1/2<u<Y_1+1/2$ is a maximum likelihood estimate of $\theta$. My attempt: First rewrite the density: $$f(x;\theta) = \begin{cases} 1 &\text{if } -1/2<x- \theta<1/2 \\  0 & \text{otherwise} \end{cases}$$ Okay, so we obviously need to use the fact that $Y_n-1/2<u<Y_1+1/2$, I started by using the regular MLE finding method: $$L(\theta)=\prod_i(x-\theta)$$ $$\log L(\theta)=\log\prod_i(x_i-\theta)=\sum_i \log(x_i- \theta)$$ $$(\log L(\theta))'=\sum_i \frac 1 {x_i- \theta} =0$$ And I'm stuck here..I think that I need to use the given here but I'm not sure how to proceed. Any help would really be appreciated:) Thanks!",,"['statistics', 'statistical-inference', 'maximum-likelihood']"
73,CDF on Standard uniform gives the same distribution,CDF on Standard uniform gives the same distribution,,Assume that $X$ has a continuous and strictly increasing CDF $F_X$. Define $Y = F_X^{-1}(U)$ where $U$ is standard Uniform. How dow I show that $X$ and $Y$ have the same distribution?,Assume that $X$ has a continuous and strictly increasing CDF $F_X$. Define $Y = F_X^{-1}(U)$ where $U$ is standard Uniform. How dow I show that $X$ and $Y$ have the same distribution?,,"['statistics', 'probability-distributions', 'uniform-distribution']"
74,"find a function such that it is symmetric across $y=1-x$ passing through $(0,0)$ and $(1,1)$, (not homework)","find a function such that it is symmetric across  passing through  and , (not homework)","y=1-x (0,0) (1,1)","I am attempting to characterize a specific type of function. The function would be such that it is symmetric across the line $y=1-x$ This function would be a mapping $f:[0,1]\rightarrow [0,1]$ such that $f(0)=0$, $f(1)=1$. This is being used for a robust measure of copula dependence and for detection of concordance unusual events in the bivariate case with possible future generalization to multivariate case. A function that I came up with was  $$f(x; a,b) = \frac{b^{ax}-1}{b^a-1}$$ I don not know if there exist a base $b$ such that this function will ever be symmetric across the line $y=1-x$ A plot is given below of $f$ and $f^{-1}$ from 0 to 1 for base being the golden ratio and the parameter $a$ being 10. My goal is to have a be a parameter such that, in some limit the function and its inverse will engulf the entire square. https://i.sstatic.net/y78vk.jpg edit I think it boils down to finding a function such that $f^{-1}(x) = 1-f(x)$","I am attempting to characterize a specific type of function. The function would be such that it is symmetric across the line $y=1-x$ This function would be a mapping $f:[0,1]\rightarrow [0,1]$ such that $f(0)=0$, $f(1)=1$. This is being used for a robust measure of copula dependence and for detection of concordance unusual events in the bivariate case with possible future generalization to multivariate case. A function that I came up with was  $$f(x; a,b) = \frac{b^{ax}-1}{b^a-1}$$ I don not know if there exist a base $b$ such that this function will ever be symmetric across the line $y=1-x$ A plot is given below of $f$ and $f^{-1}$ from 0 to 1 for base being the golden ratio and the parameter $a$ being 10. My goal is to have a be a parameter such that, in some limit the function and its inverse will engulf the entire square. https://i.sstatic.net/y78vk.jpg edit I think it boils down to finding a function such that $f^{-1}(x) = 1-f(x)$",,"['statistics', 'graphing-functions']"
75,Product of 2 Binomial distributions,Product of 2 Binomial distributions,,"I was wondering what the distribution is of the product of two binomial distributed random variables, X and Y; So suppose X ~ Bin(n,p1) and Y ~ Bin(n,p2) (so the number of experiments n is the same), what can we say about the distribution of XY? I have to calculate E(XY) and I don't see another method then finding the distribution of XY, which seems quite difficult to me.. Thanks.","I was wondering what the distribution is of the product of two binomial distributed random variables, X and Y; So suppose X ~ Bin(n,p1) and Y ~ Bin(n,p2) (so the number of experiments n is the same), what can we say about the distribution of XY? I have to calculate E(XY) and I don't see another method then finding the distribution of XY, which seems quite difficult to me.. Thanks.",,['statistics']
76,Find the method of moments estimator of theta,Find the method of moments estimator of theta,,"Let $Y_1, Y_2, \dots, Y_n$ be i.i.d RVs from the following distribution: $$f(y) = \theta y ^{\theta -1} \qquad  0 < y < 1, \quad\theta > 0$$ Show the method of moment estimator of theta is: $\bar Y/(1-\bar Y)$ I must only be seeing solutions that skip steps, because I cannot come to this answer. I can get up to this part (which I believe is in the right track): $$E(y) = \int_0^1 y \theta y^{\theta -1}\,dy,$$ which leads to $E(y) = \theta/(\theta + 1)$ But I do not know how to proceed from there....","Let $Y_1, Y_2, \dots, Y_n$ be i.i.d RVs from the following distribution: $$f(y) = \theta y ^{\theta -1} \qquad  0 < y < 1, \quad\theta > 0$$ Show the method of moment estimator of theta is: $\bar Y/(1-\bar Y)$ I must only be seeing solutions that skip steps, because I cannot come to this answer. I can get up to this part (which I believe is in the right track): $$E(y) = \int_0^1 y \theta y^{\theta -1}\,dy,$$ which leads to $E(y) = \theta/(\theta + 1)$ But I do not know how to proceed from there....",,['statistics']
77,CRLB/UMVUE estimation of $\theta$,CRLB/UMVUE estimation of,\theta,"We have a random sample $X_1,X_2,\ldots,X_n$ from a probabilitiy distribution with density $f(x;\theta) = \theta x^{-\theta-1} $ given that $x > 1$, and $0$ else. where $\theta >1 $ is an unknown estimator derive the CRLB for unbiased estimators of $\theta$. Is there an unbiased estimator of $\theta$ that reaches the CRLB? show that $S = \sum^n_{i=1} \ln(X_i)$ is a complete sufficient statistic and use this result to derive thje UMVUE for $\frac{1}{\theta}$ for 1. i did the following: $$ \begin{align} f_\bar{x}(x_1,...,x_n) &= \prod_{i=1}^n \theta x_i^{-(\theta+1)} \\  l(\theta) &= n\ln(\theta) - (\theta+1) \sum_{i=1}^n \ln(x_i) \\ \frac{\delta}{\delta \theta} l(\theta) &= -\frac{\theta+1}{\sum_{i=1}^n x_i} \end{align} $$ however setting this to 0 really doesnt help me. I have been told that looking at the $l(\theta)$ function and seeing how it behaves should help, but I really am not getting any wiser. for 2. I would just fill in the CRLB equation, should i use $\tau(\theta) = \hat{\theta}$ ? for 3. what is the difference between a complete sufficient statistic and a sufficient statistic?","We have a random sample $X_1,X_2,\ldots,X_n$ from a probabilitiy distribution with density $f(x;\theta) = \theta x^{-\theta-1} $ given that $x > 1$, and $0$ else. where $\theta >1 $ is an unknown estimator derive the CRLB for unbiased estimators of $\theta$. Is there an unbiased estimator of $\theta$ that reaches the CRLB? show that $S = \sum^n_{i=1} \ln(X_i)$ is a complete sufficient statistic and use this result to derive thje UMVUE for $\frac{1}{\theta}$ for 1. i did the following: $$ \begin{align} f_\bar{x}(x_1,...,x_n) &= \prod_{i=1}^n \theta x_i^{-(\theta+1)} \\  l(\theta) &= n\ln(\theta) - (\theta+1) \sum_{i=1}^n \ln(x_i) \\ \frac{\delta}{\delta \theta} l(\theta) &= -\frac{\theta+1}{\sum_{i=1}^n x_i} \end{align} $$ however setting this to 0 really doesnt help me. I have been told that looking at the $l(\theta)$ function and seeing how it behaves should help, but I really am not getting any wiser. for 2. I would just fill in the CRLB equation, should i use $\tau(\theta) = \hat{\theta}$ ? for 3. what is the difference between a complete sufficient statistic and a sufficient statistic?",,"['statistics', 'parameter-estimation']"
78,Multivariate extension of the Glivenko-Cantelli Theorem,Multivariate extension of the Glivenko-Cantelli Theorem,,"Is there a multivariate extension of the Glivenko-Cantelli theorem for empiric process. If not, are there some weaker statements or other results about the probability limits of empirical distributions of vector of real (or integer) random variables?","Is there a multivariate extension of the Glivenko-Cantelli theorem for empiric process. If not, are there some weaker statements or other results about the probability limits of empirical distributions of vector of real (or integer) random variables?",,"['statistics', 'stochastic-processes']"
79,"Is there a formal explanation of the concept of ""improper prior"" in Bayesian statistics?","Is there a formal explanation of the concept of ""improper prior"" in Bayesian statistics?",,"The Bayesian concept of ""improper prior"" seems to be surrounded with magic. Even formal, Bayesian-oriented books, such as Schervish's ""Theory of Statistics"", treat it with the heuristic hand waving ambiguity usual in less rigorous textbooks. Is there a book or article that deals with this concept/technique rigorously? Schervish mentions a couple formal attempts at tackling this concept, but also notes that they are radical in that they go beyond the standard axiomatization of probability theory, and hence open a whole ""can of worms"" (in his words). However, Schervish's book was published almost 20 years ago. Perhaps some advances in the field have been achieved in the meanwhile?","The Bayesian concept of ""improper prior"" seems to be surrounded with magic. Even formal, Bayesian-oriented books, such as Schervish's ""Theory of Statistics"", treat it with the heuristic hand waving ambiguity usual in less rigorous textbooks. Is there a book or article that deals with this concept/technique rigorously? Schervish mentions a couple formal attempts at tackling this concept, but also notes that they are radical in that they go beyond the standard axiomatization of probability theory, and hence open a whole ""can of worms"" (in his words). However, Schervish's book was published almost 20 years ago. Perhaps some advances in the field have been achieved in the meanwhile?",,"['statistics', 'reference-request', 'bayesian']"
80,What gives rise to the normal distribution?,What gives rise to the normal distribution?,,"I'd like to know if anyone has a generally friendly explanation of why the normal distribution is an attractor of so many observed behaviors in their eventuality. I have a degree in math if you want to get technical, but I'd like to be able to explain to my grandma as well","I'd like to know if anyone has a generally friendly explanation of why the normal distribution is an attractor of so many observed behaviors in their eventuality. I have a degree in math if you want to get technical, but I'd like to be able to explain to my grandma as well",,['statistics']
81,Rounding of complementary percentages [duplicate],Rounding of complementary percentages [duplicate],,"This question already has an answer here : Rounding Percentages (1 answer) Closed 8 years ago . This problem arose while tallying votes in a poll and displaying the results as a whole number percentage. So if ""Option A"" received $300$ votes and ""Option B"" received $100$, the display should read ""$75\%$, $25\%$"". There is a requirement that the integer percentages must add up to $100\%$. Which means that rounding up one percentage will cause the other to round down. How should the following percentages be rounded? $20.5\%$ & $79.5\%$ (which one rounds up?) $49.5\%$ & $50.5\%$ (equal or not?) $0.5\%$ & $99.5\%$ (is $0.5\%$ statistically significant?) I tend to feel that the small numbers should be given more priority (so $0.5\%$ rounds up to $1$, while the $1\%$ difference in the $49.5$/$50.5$ split gets increased to $2\%$ giving $49$/$51$). Is there any ""one rule to round them all""?","This question already has an answer here : Rounding Percentages (1 answer) Closed 8 years ago . This problem arose while tallying votes in a poll and displaying the results as a whole number percentage. So if ""Option A"" received $300$ votes and ""Option B"" received $100$, the display should read ""$75\%$, $25\%$"". There is a requirement that the integer percentages must add up to $100\%$. Which means that rounding up one percentage will cause the other to round down. How should the following percentages be rounded? $20.5\%$ & $79.5\%$ (which one rounds up?) $49.5\%$ & $50.5\%$ (equal or not?) $0.5\%$ & $99.5\%$ (is $0.5\%$ statistically significant?) I tend to feel that the small numbers should be given more priority (so $0.5\%$ rounds up to $1$, while the $1\%$ difference in the $49.5$/$50.5$ split gets increased to $2\%$ giving $49$/$51$). Is there any ""one rule to round them all""?",,['statistics']
82,Multivariate Moment Generating Function,Multivariate Moment Generating Function,,"Let $X$ and $Y$ be two independent random variables both have Laplace distribution. What is the moment generating function of $U=X+Y$ and $V=X-Y$? Initially, I want to work out the $f_{U,V}(u,v)$, and then work out the $M_{U,V}(s,t)=E_{f_{U,V}}(e^{sU+tV})$. But they are hard to compute. So I try another way: $$M_{U,V}(s,t)=E_{f_{U,V}}(e^{sU+tV})=E_{f_{U,V}}(e^{s(X+Y)+t(X-Y)})=E_{f_{U,V}}(e^{(s+t)X+(s-t)Y)})$$ But at this point I am not so sure about whether I can make it become $E_{f_{X}}(e^{(s+t)X})E_{f_{Y}}(e^{(s-t)Y})$. Can I do this? Why? Thanks in advance.","Let $X$ and $Y$ be two independent random variables both have Laplace distribution. What is the moment generating function of $U=X+Y$ and $V=X-Y$? Initially, I want to work out the $f_{U,V}(u,v)$, and then work out the $M_{U,V}(s,t)=E_{f_{U,V}}(e^{sU+tV})$. But they are hard to compute. So I try another way: $$M_{U,V}(s,t)=E_{f_{U,V}}(e^{sU+tV})=E_{f_{U,V}}(e^{s(X+Y)+t(X-Y)})=E_{f_{U,V}}(e^{(s+t)X+(s-t)Y)})$$ But at this point I am not so sure about whether I can make it become $E_{f_{X}}(e^{(s+t)X})E_{f_{Y}}(e^{(s-t)Y})$. Can I do this? Why? Thanks in advance.",,['statistics']
83,learning maths for statistics,learning maths for statistics,,"Apologies if I have posted this in the wrong place first off. My work has taken me into a unexpectantly large amount of statistics. In order to really understand what I am doing I need to understand these statistical approaches. However it seems almost impossible to do that unless you speak math (I don't as I'm sure you can tell). So my question is are there any free online resources for learning/ getting familiar with terminology, symbols, concepts commonly used in maths/statistics that are aimed at people with basic math skills (basic algebra is okay, can barely remember how to differentiate). Thanks in advance, Cheers, Davy Edit to say: I should mention that I am somewhat (not very) familiar with basic concepts like variance, hypothesis testing, and regression, but only on a practical level. I want understand these on the mathematical level on which they were built. Method development is something I am interested in, but I'll never be able to do, If I don't start learning more mathematics. MY problem thus far seem that every book on statistics assume you don't want to see any maths at all, or your totally familiar with giant equations, which are therefore, not explained well to someone who has no idea what (probably) commonplace symbols mean. I guess what I am really looking for is an induction to the domain specific language of maths, with a view towards stats.","Apologies if I have posted this in the wrong place first off. My work has taken me into a unexpectantly large amount of statistics. In order to really understand what I am doing I need to understand these statistical approaches. However it seems almost impossible to do that unless you speak math (I don't as I'm sure you can tell). So my question is are there any free online resources for learning/ getting familiar with terminology, symbols, concepts commonly used in maths/statistics that are aimed at people with basic math skills (basic algebra is okay, can barely remember how to differentiate). Thanks in advance, Cheers, Davy Edit to say: I should mention that I am somewhat (not very) familiar with basic concepts like variance, hypothesis testing, and regression, but only on a practical level. I want understand these on the mathematical level on which they were built. Method development is something I am interested in, but I'll never be able to do, If I don't start learning more mathematics. MY problem thus far seem that every book on statistics assume you don't want to see any maths at all, or your totally familiar with giant equations, which are therefore, not explained well to someone who has no idea what (probably) commonplace symbols mean. I guess what I am really looking for is an induction to the domain specific language of maths, with a view towards stats.",,"['statistics', 'education', 'learning']"
84,Does the ratio $\mathrm{Var}(x)/E(x)$ have any statistical meaning?,Does the ratio  have any statistical meaning?,\mathrm{Var}(x)/E(x),I am wondering how to compare the volatility of two sets of samples. Can I consider the ratio $\mathrm{Var}(x)/E(x)$ as a normalized variance?,I am wondering how to compare the volatility of two sets of samples. Can I consider the ratio $\mathrm{Var}(x)/E(x)$ as a normalized variance?,,['statistics']
85,What is the standard error of the mean of an exponential distribution of the form $Ae^{Bx}$ with N measurements?,What is the standard error of the mean of an exponential distribution of the form  with N measurements?,Ae^{Bx},What is the standard error of the mean of an exponential distribution of the form $Ae^{Bx}$ with $N$ measurements?,What is the standard error of the mean of an exponential distribution of the form $Ae^{Bx}$ with $N$ measurements?,,"['statistics', 'average']"
86,UMVU estimator of $\lambda^2$ via Rao-Blackwell,UMVU estimator of  via Rao-Blackwell,\lambda^2,"I have been working on a problem, which goes as follows: Given the statistical model $(\mathcal{X},\mathcal{B},\mathcal{P})$ , where $\mathcal{P}=\{P_{\lambda}^{\otimes}:P_{\lambda}=Pos(\lambda), \lambda>0\}$ , use the Rao-Blackwell theorem, to find an UMVU estimator (UMVUE) for $\lambda^2$ via $\hat{\gamma}: \mathbb{N}^n_0 \longrightarrow \mathbb{N}_0 $ , $\hat{\gamma}(x)=x_1x_2$ , with respect to the statistic $T:\mathbb{N}^n_0 \longrightarrow \mathbb{N}_0, T(x)=\sum_\limits{{i=1}}^n x_i$ . In other words we have to calculate the following expected value: $$\hat{\gamma}^*(t):=\mathbb{E}_{\lambda}(\hat{\gamma}(X)|T(X)=t), \quad t \in \mathbb{N}_0$$ where $X=(X_1, \dots , X_n)$ , ( $X_1, \dots , X_n \stackrel{iid}{\sim}Pos(\lambda))$ is the random variable inducing the statistical model. We need $T$ to be a complete and sufficient statistic and $E_{\lambda}(\hat{\gamma})<\infty$ , for $\hat{\gamma}^*$ being UMVUE. I already solved this by applying Lehmann-Scheffe, to another  unbiased estimator for $\lambda^2$ and the same statistic $T$ , then concluded that $\hat{\gamma}^*$ has to be equal to my UMVUE, since UMVUE are unique. I then tried to calculate the above expected value, but I am stuck evaluating it, here is what I have done so far : For $t \in \mathbb{N_0}$ : $\mathbb{E}_{\lambda}(\hat{\gamma}(X)|T(X)=t)=\sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\mathbb{P}_{\lambda}(X=x|T(X)=t)=\sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n|\sum_\limits{{i=1}}^n X_i=t)$ I have concluded that: $$\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n|\sum_\limits{{i=1}}^n X_i=t)= \frac{\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n, \sum_\limits{{i=1}}^n X_i=t)}{\mathbb{P}_{\lambda}(\sum_\limits{{i=1}}^n X_i=t)}= \frac{\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n)\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}}{e^{n\lambda}\frac{(n\lambda)^{t}}{t!}}=\frac{e^{n\lambda}\frac{\lambda^{t}}{x_1!\dots x_n!}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}}{e^{n\lambda}\frac{(n\lambda)^{t}}{t!}}= \frac{t!}{x_1!\dots x_n!n^t}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}$$ I then tried to proceed with the evaluation : $$\sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\frac{t!}{x_1!\dots x_n!n^t}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}=\sum_\limits{x \in \{\sum_\limits{{i=1}}^n x_i=t\}}x_1x_2\frac{t!}{x_1!\dots x_n!n^t}=$$ $$\sum_\limits{x_1,x_2 \in \{x_1,x_2>0, x_1+x_2 \leq t\}}\frac{x_1x_2}{x_1!x_2!}\sum_\limits{x \in \{\sum_\limits{{i=3}}^n x_i=t-x_1-x_2\}}\frac{(t-x_1-x_2)!}{x_3!\dots x_n!n^{t-x_1-x_2}}=\sum_\limits{x_1,x_2 \in \{x_1,x_2>0, x_1+x_2 \leq t\}}\frac{x_1x_2}{x_1!x_2!}\stackrel{?}{=}\frac{t(t-1)}{n^2}$$ My Question : Im not quite sure, about the last two equations, if the second last is correct, then the second sum would be equal to one, since we are summing over the support of a density function, but I doubt that the very last ""equation"" would follow, but this is what it should be equal to. Any help is appreciated!","I have been working on a problem, which goes as follows: Given the statistical model , where , use the Rao-Blackwell theorem, to find an UMVU estimator (UMVUE) for via , , with respect to the statistic . In other words we have to calculate the following expected value: where , ( is the random variable inducing the statistical model. We need to be a complete and sufficient statistic and , for being UMVUE. I already solved this by applying Lehmann-Scheffe, to another  unbiased estimator for and the same statistic , then concluded that has to be equal to my UMVUE, since UMVUE are unique. I then tried to calculate the above expected value, but I am stuck evaluating it, here is what I have done so far : For : I have concluded that: I then tried to proceed with the evaluation : My Question : Im not quite sure, about the last two equations, if the second last is correct, then the second sum would be equal to one, since we are summing over the support of a density function, but I doubt that the very last ""equation"" would follow, but this is what it should be equal to. Any help is appreciated!","(\mathcal{X},\mathcal{B},\mathcal{P}) \mathcal{P}=\{P_{\lambda}^{\otimes}:P_{\lambda}=Pos(\lambda), \lambda>0\} \lambda^2 \hat{\gamma}: \mathbb{N}^n_0 \longrightarrow \mathbb{N}_0  \hat{\gamma}(x)=x_1x_2 T:\mathbb{N}^n_0 \longrightarrow \mathbb{N}_0, T(x)=\sum_\limits{{i=1}}^n x_i \hat{\gamma}^*(t):=\mathbb{E}_{\lambda}(\hat{\gamma}(X)|T(X)=t), \quad t \in \mathbb{N}_0 X=(X_1, \dots , X_n) X_1, \dots , X_n \stackrel{iid}{\sim}Pos(\lambda)) T E_{\lambda}(\hat{\gamma})<\infty \hat{\gamma}^* \lambda^2 T \hat{\gamma}^* t \in \mathbb{N_0} \mathbb{E}_{\lambda}(\hat{\gamma}(X)|T(X)=t)=\sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\mathbb{P}_{\lambda}(X=x|T(X)=t)=\sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n|\sum_\limits{{i=1}}^n X_i=t) \mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n|\sum_\limits{{i=1}}^n X_i=t)=
\frac{\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n, \sum_\limits{{i=1}}^n X_i=t)}{\mathbb{P}_{\lambda}(\sum_\limits{{i=1}}^n X_i=t)}=
\frac{\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n)\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}}{e^{n\lambda}\frac{(n\lambda)^{t}}{t!}}=\frac{e^{n\lambda}\frac{\lambda^{t}}{x_1!\dots x_n!}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}}{e^{n\lambda}\frac{(n\lambda)^{t}}{t!}}=
\frac{t!}{x_1!\dots x_n!n^t}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}} \sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\frac{t!}{x_1!\dots x_n!n^t}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}=\sum_\limits{x \in \{\sum_\limits{{i=1}}^n x_i=t\}}x_1x_2\frac{t!}{x_1!\dots x_n!n^t}= \sum_\limits{x_1,x_2 \in \{x_1,x_2>0, x_1+x_2 \leq t\}}\frac{x_1x_2}{x_1!x_2!}\sum_\limits{x \in \{\sum_\limits{{i=3}}^n x_i=t-x_1-x_2\}}\frac{(t-x_1-x_2)!}{x_3!\dots x_n!n^{t-x_1-x_2}}=\sum_\limits{x_1,x_2 \in \{x_1,x_2>0, x_1+x_2 \leq t\}}\frac{x_1x_2}{x_1!x_2!}\stackrel{?}{=}\frac{t(t-1)}{n^2}","['statistics', 'poisson-distribution', 'parameter-estimation', 'umvue']"
87,"Evaluating $\lim_{n \to \infty} \frac{1}{n}\sum_{t = 1}^{n}e^{-k\cos^2(\omega t)}$, where $k>0$ and $0<\omega<\pi$","Evaluating , where  and",\lim_{n \to \infty} \frac{1}{n}\sum_{t = 1}^{n}e^{-k\cos^2(\omega t)} k>0 0<\omega<\pi,"Need to evalute a closed form expression of the following limit: $$\lim_{n \to \infty} \frac{1}{n}\sum_{t = 1}^{n}e^{-k\cos^2(\omega t)}$$ where $k>0$ and $0<\omega<\pi$ . Empirically, I have observed that the limit exists and it does not depend on $\omega$ , it depends only on $k$ . Also, the value of the limit decreases as $k$ increases, which means the limit is a decreasing function in $k$ . I need to prove that the limit is independent of $\omega$ and is a decreasing function in $k$ .  Please help me in finding the limit.","Need to evalute a closed form expression of the following limit: where and . Empirically, I have observed that the limit exists and it does not depend on , it depends only on . Also, the value of the limit decreases as increases, which means the limit is a decreasing function in . I need to prove that the limit is independent of and is a decreasing function in .  Please help me in finding the limit.",\lim_{n \to \infty} \frac{1}{n}\sum_{t = 1}^{n}e^{-k\cos^2(\omega t)} k>0 0<\omega<\pi \omega k k k \omega k,"['real-analysis', 'statistics', 'computational-mathematics']"
88,Doubt regarding Continuous Random Variables,Doubt regarding Continuous Random Variables,,"I do know that the probability of random variables (say $X$ and $Y$ ) taking specific values is considered to be zero (i.e. $P(X=x,Y=y) = 0$ ). My doubt, however, is how do I determine the probability of events wherein ( $a<X<b,Y=y$ )(i.e. $P(a<X<b,Y=y)$ . I am aware that the probability of events such as ( $a<X<b\mid Y=y$ ) can be evaluated by finding the conditional probability density function of $X$ and integrating the function over the interval $(a, b)$ . But what about the case above? Is it also equal to zero? If so, could you please provide a detailed example to clarify the doubt? I have been struggling with continuous random variables for some time now. Also, are the theorems valid for continuous random variables same/analogous to those of discrete random variables? A detailed explanation would be much appreciated. Thanks in advance :)","I do know that the probability of random variables (say and ) taking specific values is considered to be zero (i.e. ). My doubt, however, is how do I determine the probability of events wherein ( )(i.e. . I am aware that the probability of events such as ( ) can be evaluated by finding the conditional probability density function of and integrating the function over the interval . But what about the case above? Is it also equal to zero? If so, could you please provide a detailed example to clarify the doubt? I have been struggling with continuous random variables for some time now. Also, are the theorems valid for continuous random variables same/analogous to those of discrete random variables? A detailed explanation would be much appreciated. Thanks in advance :)","X Y P(X=x,Y=y) = 0 a<X<b,Y=y P(a<X<b,Y=y) a<X<b\mid Y=y X (a, b)","['statistics', 'probability-distributions', 'random-variables']"
89,Decomposes a unit vector into the kronecker product of three unit vectors,Decomposes a unit vector into the kronecker product of three unit vectors,,"I want to get the optimal solution to the following equation $$\mathop{\rm{min}}_ {\| \boldsymbol{v}_i \| = 1     \atop     i= 1,2,3}  \| \boldsymbol{w} - \boldsymbol{v}_3 \otimes \boldsymbol{v}_2 \otimes \boldsymbol{v}_1 \|^2$$ where $\boldsymbol{v}_i \in \mathbb{R}^{ p_i\times 1}$ , $\boldsymbol{w} \in \mathbb{R}^{ p_1 p_2 p_3\times 1}$ is given and $\| \boldsymbol{w}\| = 1$ . I want to get the best estimate of $\boldsymbol{v}_i.$ I can get $p_{i+1}p_{i+2}$ $v_1^i/v_j^i$ 's , $j = 2,\ldots,p_i$ , where $v_j^i$ denotes the $j$ -th entries of $\boldsymbol{v}_i$ and $i+1$ , $i+2$ are computed modulo 3. Let $$\hat{\dfrac{v_1^i}{v_j^i}} = \dfrac{1}{p_{i+1}p_{i+2}} \sum v_1^i/v_j^i , j = 2,\ldots,p_i$$ , then we can get the estimator $\hat{\boldsymbol{v}_i}$ under $\|\boldsymbol{v}_i\|=1.$ But I dont know whether this solution is optimal or appropriate.","I want to get the optimal solution to the following equation where , is given and . I want to get the best estimate of I can get 's , , where denotes the -th entries of and , are computed modulo 3. Let , then we can get the estimator under But I dont know whether this solution is optimal or appropriate.","\mathop{\rm{min}}_ {\| \boldsymbol{v}_i \| = 1
    \atop
    i= 1,2,3}  \| \boldsymbol{w} - \boldsymbol{v}_3 \otimes \boldsymbol{v}_2 \otimes \boldsymbol{v}_1 \|^2 \boldsymbol{v}_i \in \mathbb{R}^{ p_i\times 1} \boldsymbol{w} \in \mathbb{R}^{ p_1 p_2 p_3\times 1} \| \boldsymbol{w}\| = 1 \boldsymbol{v}_i. p_{i+1}p_{i+2} v_1^i/v_j^i j = 2,\ldots,p_i v_j^i j \boldsymbol{v}_i i+1 i+2 \hat{\dfrac{v_1^i}{v_j^i}} = \dfrac{1}{p_{i+1}p_{i+2}} \sum v_1^i/v_j^i , j = 2,\ldots,p_i \hat{\boldsymbol{v}_i} \|\boldsymbol{v}_i\|=1.","['statistics', 'optimization', 'computational-mathematics', 'kronecker-product']"
90,Need help understanding a paper's statement.,Need help understanding a paper's statement.,,"Given $$ \boldsymbol{E}_\ell= \alpha\left(\boldsymbol{I}+\alpha \boldsymbol{Z}_{\ell} \boldsymbol{Z}_{\ell}^{*}\right)^{-1} $$ where $\boldsymbol{Z}_\ell \in \mathbb{R}^{n\times m}$ and $\boldsymbol{E}_\ell \in \mathbb{R}^{n\times n}$ . A paper claims that this is related to Ridge autoregression residual, and it states: $$ \tag{1} \boldsymbol{E}_{\ell} \boldsymbol{z}_{\ell}=\alpha\left(\boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell}\left[\boldsymbol{q}_{\ell}\right]_{\star}\right) $$ where $\left[\boldsymbol{q}_{\ell}\right]_{\star} = \underset{\boldsymbol{q}_{\ell}}{\operatorname{argmin}} \alpha\left\|\boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell} \boldsymbol{q}_{\boldsymbol{\ell}}\right\|_{2}^{2}+\left\|\boldsymbol{q}_{\ell}\right\|_{2}^{2}$ . I know the formulation of $[\boldsymbol{q}_{\ell}]_{\star}$ ends up with the solution of Ridge regression . And I know $\boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell}\left[\boldsymbol{q}_{\ell}\right]_{\star}$ will then be the regression residual. However, I cannot see the reason for equation (1) to hold. Any idea?","Given where and . A paper claims that this is related to Ridge autoregression residual, and it states: where . I know the formulation of ends up with the solution of Ridge regression . And I know will then be the regression residual. However, I cannot see the reason for equation (1) to hold. Any idea?","
\boldsymbol{E}_\ell= \alpha\left(\boldsymbol{I}+\alpha \boldsymbol{Z}_{\ell} \boldsymbol{Z}_{\ell}^{*}\right)^{-1}
 \boldsymbol{Z}_\ell \in \mathbb{R}^{n\times m} \boldsymbol{E}_\ell \in \mathbb{R}^{n\times n} 
\tag{1}
\boldsymbol{E}_{\ell} \boldsymbol{z}_{\ell}=\alpha\left(\boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell}\left[\boldsymbol{q}_{\ell}\right]_{\star}\right)
 \left[\boldsymbol{q}_{\ell}\right]_{\star} = \underset{\boldsymbol{q}_{\ell}}{\operatorname{argmin}} \alpha\left\|\boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell} \boldsymbol{q}_{\boldsymbol{\ell}}\right\|_{2}^{2}+\left\|\boldsymbol{q}_{\ell}\right\|_{2}^{2} [\boldsymbol{q}_{\ell}]_{\star} \boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell}\left[\boldsymbol{q}_{\ell}\right]_{\star}","['linear-algebra', 'statistics', 'regression']"
91,Is the cosine angle between two R.V. an (approximation) not equality to the correlation coefficient?,Is the cosine angle between two R.V. an (approximation) not equality to the correlation coefficient?,,"I have seen in websites that given two R.V. $X,Y$ , if $$ \cos(\theta)=\frac{X\cdot Y}{\|X\|_2\|Y\|_2} $$ and $$ \rho=\frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}} $$ then $$ \cos(\theta)=\rho $$ This identity implies $\text{Cov}(X,Y)=X\cdot Y$ . Isn't $X\cdot Y$ the Maximum Likelihood Estimate for the covariance missing some factors? If true then the equation above is not equality but rather $≈$ as the samples become bigger. Next is the denominator which implies  that $$ \text{Var}(X)= \| X\|_2 ^2$$ . Again, isn't the right side not an identity but rather an estimator (MLE) to the variance of $X$ ? Isn't $$ \rho ≈ \cos(\theta)$$ I have also seen the dot product (without the denominator in the first equation I've given but being more general using inner products) being used to measure correlation in some papers like Least Angle Regression. I am confused about the relationship between dot products and correlation. This leads me to a general question: Is $$ \langle X,X\rangle = \text{Var}(X) $$ in Euclidean space.","I have seen in websites that given two R.V. , if and then This identity implies . Isn't the Maximum Likelihood Estimate for the covariance missing some factors? If true then the equation above is not equality but rather as the samples become bigger. Next is the denominator which implies  that . Again, isn't the right side not an identity but rather an estimator (MLE) to the variance of ? Isn't I have also seen the dot product (without the denominator in the first equation I've given but being more general using inner products) being used to measure correlation in some papers like Least Angle Regression. I am confused about the relationship between dot products and correlation. This leads me to a general question: Is in Euclidean space.","X,Y 
\cos(\theta)=\frac{X\cdot Y}{\|X\|_2\|Y\|_2}
 
\rho=\frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}
 
\cos(\theta)=\rho
 \text{Cov}(X,Y)=X\cdot Y X\cdot Y ≈  \text{Var}(X)= \| X\|_2 ^2 X  \rho ≈ \cos(\theta) 
\langle X,X\rangle = \text{Var}(X)
","['statistics', 'inner-products', 'variance', 'covariance', 'correlation']"
92,Is there a way to find errors in a Gaussian fit?,Is there a way to find errors in a Gaussian fit?,,"If I have a set of data that $(x_i,y_i)$ could be visualized on a scatter plot, and I want to apply the least-square method to fit them using a Gaussian function: $$ G(x) = B+A\exp\left[-\left(\frac{x-\mu}{\sigma}\right)^2\right] $$ I think this would give us an estimation of the expected value $\mu$ . However, is there a way I can find the error of those 4 parameters $A,B,\mu,\sigma$ using this method?","If I have a set of data that could be visualized on a scatter plot, and I want to apply the least-square method to fit them using a Gaussian function: I think this would give us an estimation of the expected value . However, is there a way I can find the error of those 4 parameters using this method?","(x_i,y_i) 
G(x) = B+A\exp\left[-\left(\frac{x-\mu}{\sigma}\right)^2\right]
 \mu A,B,\mu,\sigma","['statistics', 'least-squares', 'gaussian']"
93,Generate random points on perimeter of ellipse,Generate random points on perimeter of ellipse,,"Sampling only from the uniform distribution $U(0,1)$ , I am hoping to use transformations to create random values distributed uniformly around the perimeter of an ellipse. Eventually, I'd like to do the same on the surfaces of ellipsoids and other problematic objects. My first idea was as follows. We can easily get $\Theta \sim U(0,2\pi)$ . Then, from the parametric form of the ellipse, $$X \equiv a \cos \Theta \\  Y \equiv b \sin \Theta $$ is a random point on the ellipse's perimeter. Similarly, if we independently sample another angle $\Phi \sim U(0,\pi)$ , we could use $$X \equiv a \sin \Theta \cos\Phi \\  Y \equiv b \sin \Theta \sin\Phi\\ Z \equiv c \cos \Theta $$ The problem with these approaches is that they are uniformly distributed with respect to theta, not along the surface. They are equivalent to taking a uniform distribution on a circle and then projecting about the radius to the perimeter of the ellipse, so the density of points is higher near the major axis, as you can see here: (This is itself counterintuitive to me: One would expect the points to be denser about the minor axis since they are being ""sprayed"" over a more concentrated region, right?) How can I generate points distributed uniformly about the perimeter of the ellipse? Here is an example of what I am trying to do but using a circle instead. The transformation used there doesn't work for the ellipse because it creates the same bunching behavior.","Sampling only from the uniform distribution , I am hoping to use transformations to create random values distributed uniformly around the perimeter of an ellipse. Eventually, I'd like to do the same on the surfaces of ellipsoids and other problematic objects. My first idea was as follows. We can easily get . Then, from the parametric form of the ellipse, is a random point on the ellipse's perimeter. Similarly, if we independently sample another angle , we could use The problem with these approaches is that they are uniformly distributed with respect to theta, not along the surface. They are equivalent to taking a uniform distribution on a circle and then projecting about the radius to the perimeter of the ellipse, so the density of points is higher near the major axis, as you can see here: (This is itself counterintuitive to me: One would expect the points to be denser about the minor axis since they are being ""sprayed"" over a more concentrated region, right?) How can I generate points distributed uniformly about the perimeter of the ellipse? Here is an example of what I am trying to do but using a circle instead. The transformation used there doesn't work for the ellipse because it creates the same bunching behavior.","U(0,1) \Theta \sim U(0,2\pi) X \equiv a \cos \Theta \\
 Y \equiv b \sin \Theta  \Phi \sim U(0,\pi) X \equiv a \sin \Theta \cos\Phi \\
 Y \equiv b \sin \Theta \sin\Phi\\
Z \equiv c \cos \Theta ","['statistics', 'random-variables', 'transformation']"
94,Can a bimodal distribution have a gap?,Can a bimodal distribution have a gap?,,"The question asks to describe the distribution of aspen tree diameters from the sample. I said that the distribution was bimodal with one peak around 5.2 and the other peak around 9.2. However the correct answer is that the distribution is skewed to the right and has a gap between 7 and 8 inches. I tried looking online for some answers but I can't find any. Why is this a skewed unimodal distribution instead of a bimodal distribution? Is it because in a bimodal distribution there are two peaks but there is no gap, instead it is just a very low frequency between the two peaks?","The question asks to describe the distribution of aspen tree diameters from the sample. I said that the distribution was bimodal with one peak around 5.2 and the other peak around 9.2. However the correct answer is that the distribution is skewed to the right and has a gap between 7 and 8 inches. I tried looking online for some answers but I can't find any. Why is this a skewed unimodal distribution instead of a bimodal distribution? Is it because in a bimodal distribution there are two peaks but there is no gap, instead it is just a very low frequency between the two peaks?",,['statistics']
95,Understanding principal component analysis,Understanding principal component analysis,,"Let $X$ be $m\times n$ sample matrix where each row is a sample point. We want to find matrix $P$ of dimension $n \times r$ such that $XP$ is the dimension reduced matrix of samples after applying the principal component technique. We find $P$ by maximizing the trace of the covariance matrix $C_Y^{'}=\frac{1}{m}(XP)^T(XP)=P^T(\frac{1}{m}X^TX)P$ . Because we want the variance of each variable to be maximized. We let $C=\frac{1}{m}X^TX$ and we want to maximize $tr(P^TCP)$ subject to $P^TP=I$ . They said that we can use lagrange method to find partial of $f(P)=tr(P^TCP)+\lambda(P^TP-I)$ . I don't understand this, please explain. Also, they used $\frac{\partial tr(AB)}{\partial A}=B^T$ , and $\frac{\partial X^TX}{\partial X}=X$ . I need help understanding that as well. They did $\frac{\partial f}{\partial p}= \frac{\partial tr(P^TCP)}{\partial P}+\lambda \frac{\partial (P^TP)}{\partial P} =\frac{\partial tr(PP^TC)}{\partial P}+\lambda P=(P^TC)^T+\lambda P=C^TP+\lambda P=CP+\lambda P$ , and when set to $0$ , we get $CP=(-\lambda)P$ . And that shows why we need to calculate eigenvalues. I need clarification on that as well, for example, how to choose size of $P$ ?","Let be sample matrix where each row is a sample point. We want to find matrix of dimension such that is the dimension reduced matrix of samples after applying the principal component technique. We find by maximizing the trace of the covariance matrix . Because we want the variance of each variable to be maximized. We let and we want to maximize subject to . They said that we can use lagrange method to find partial of . I don't understand this, please explain. Also, they used , and . I need help understanding that as well. They did , and when set to , we get . And that shows why we need to calculate eigenvalues. I need clarification on that as well, for example, how to choose size of ?",X m\times n P n \times r XP P C_Y^{'}=\frac{1}{m}(XP)^T(XP)=P^T(\frac{1}{m}X^TX)P C=\frac{1}{m}X^TX tr(P^TCP) P^TP=I f(P)=tr(P^TCP)+\lambda(P^TP-I) \frac{\partial tr(AB)}{\partial A}=B^T \frac{\partial X^TX}{\partial X}=X \frac{\partial f}{\partial p}= \frac{\partial tr(P^TCP)}{\partial P}+\lambda \frac{\partial (P^TP)}{\partial P} =\frac{\partial tr(PP^TC)}{\partial P}+\lambda P=(P^TC)^T+\lambda P=C^TP+\lambda P=CP+\lambda P 0 CP=(-\lambda)P P,"['linear-algebra', 'statistics', 'multivariable-calculus']"
96,When are geometric and harmonic means used?,When are geometric and harmonic means used?,,"From what little statistics I know, the only 'mean' commonly used is the arithmetic mean, and the rest are irrelevant. Any reading I've done has pretty much said something along the lines of ""acceleration or something"". So, under what situations are geometric, harmonic, and the other types of means are genuinely useful, and why are they used for those situations?","From what little statistics I know, the only 'mean' commonly used is the arithmetic mean, and the rest are irrelevant. Any reading I've done has pretty much said something along the lines of ""acceleration or something"". So, under what situations are geometric, harmonic, and the other types of means are genuinely useful, and why are they used for those situations?",,"['statistics', 'statistical-inference', 'average', 'means']"
97,Estimating the parameter lambda in exponential distribution,Estimating the parameter lambda in exponential distribution,,"I'm reading the following from page 60 of Information theory and Machine Learning. My questions are the following Under the assumption that $\lambda \ll 20$ why is $\bar{x}-1$ a good estimator, could someone add the detail explaining it? What kind of ad hoc binning techniques would work for $\lambda\gg20$ ? I know the author merely introduces his thought process so that the supervisor eventually leads him to a bayesian way of thinking but i'm interested in why his logic for these particular cases works even if the solution is not a unifying one. Thanks!","I'm reading the following from page 60 of Information theory and Machine Learning. My questions are the following Under the assumption that $\lambda \ll 20$ why is $\bar{x}-1$ a good estimator, could someone add the detail explaining it? What kind of ad hoc binning techniques would work for $\lambda\gg20$ ? I know the author merely introduces his thought process so that the supervisor eventually leads him to a bayesian way of thinking but i'm interested in why his logic for these particular cases works even if the solution is not a unifying one. Thanks!",,"['statistics', 'statistical-inference', 'machine-learning', 'information-theory', 'bayesian']"
98,When can the collapsed Gibbs sampling be applied?,When can the collapsed Gibbs sampling be applied?,,"I understand Gibbs sampling is a means of statistics inference, and it seems that sometimes certain variables can be integrated out in the sampling process, known as collapsed Gibbs sampling. I really want to know in what circumstances the collapsed Gibbs sampling can be applied, and which variables can be integrated out? I did some search on Google, and it appears there are no detailed explanation on it. Though there are some papers on applying collapsed Gibbs sampling on Latent Dirichlet Allocation (LDA), I am no expert of MCMC and have no idea what LDA is, so it may be hard for me to read those papers. Can someone answer this question and it would be better, provide some examples? Much appreciated!","I understand Gibbs sampling is a means of statistics inference, and it seems that sometimes certain variables can be integrated out in the sampling process, known as collapsed Gibbs sampling. I really want to know in what circumstances the collapsed Gibbs sampling can be applied, and which variables can be integrated out? I did some search on Google, and it appears there are no detailed explanation on it. Though there are some papers on applying collapsed Gibbs sampling on Latent Dirichlet Allocation (LDA), I am no expert of MCMC and have no idea what LDA is, so it may be hard for me to read those papers. Can someone answer this question and it would be better, provide some examples? Much appreciated!",,"['statistics', 'statistical-inference', 'bayesian', 'sampling', 'monte-carlo']"
99,Math for statistics,Math for statistics,,I suddenly got interested in machine learning so i began looking for where to start. After some research i found a book about statistical learning which has to be a good place to begin. I'm about to take my second year in civil engineering and i don't know any matrix algebra. I found out that matrix algebra is a part of linear algebra. I searched for some videos and books on linear algebra and found quite a few of them. I don't know what source to pick so I would appreciate if someone would point me to a good source to learn linear algebra which would help me with mentioned book about statistical learning.,I suddenly got interested in machine learning so i began looking for where to start. After some research i found a book about statistical learning which has to be a good place to begin. I'm about to take my second year in civil engineering and i don't know any matrix algebra. I found out that matrix algebra is a part of linear algebra. I searched for some videos and books on linear algebra and found quite a few of them. I don't know what source to pick so I would appreciate if someone would point me to a good source to learn linear algebra which would help me with mentioned book about statistical learning.,,"['linear-algebra', 'statistics']"
