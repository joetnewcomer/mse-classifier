,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If $A_1 \subset A_2 \subset \mathbb R$ and $m^*(A_1) = m^*(A_2)$, will $m^*(A_1 \cap T) = m^*(A_2 \cap T), \forall T \subset \mathbb R$?","If  and , will ?","A_1 \subset A_2 \subset \mathbb R m^*(A_1) = m^*(A_2) m^*(A_1 \cap T) = m^*(A_2 \cap T), \forall T \subset \mathbb R","Definition of Lebesgue Outer Measure : Given a set $E$ of $\mathbb R$ , we define the Lebesgue Outer Measure of $E$ by, $$m^*(E) = \inf \left\{\sum_{n=1}^{+\infty} l(I_n): E \subset \bigcup_{n=1}^{+\infty}I_n \right\}$$ where $l(I_n)$ denotes the length of interval (bounded and nonempty interval). Definition of measurable set : A set $E$ measurable if $$m^*(T) = m^*(T \cap E) + m^*(T \cap E^c)$$ for every subset of $T$ of $\mathbb R$ . If $A_1 \subset A_2 \subset \mathbb R$ and $m^*(A_1) = m^*(A_2)$ , will $$m^*(A_1 \cap T) = m^*(A_2 \cap T)$$ for all $T \subset \mathbb R$ ? Why? How about the result if $A_1$ being a measurable set added? Update: Sincerely, I appreciate user140776's answer and he gives the counterexample that equality doesn't hold for $m^*(A_1) = m^*(A_2) = +\infty$ . I still have a question: if $m^*(A_1) = m^*(A_2) < +\infty$ , does the equality hold? Or it doesn't hold until adding the condition that $A_1$ is measurable? Besides, if I remove the restriction of $A_1$ being a subset of $A_2$ that is $A_1, A_2 \subset \mathbb R$ and $m^*(A_1) = m^*(A_2) < +\infty$ , will that equality $$m^*(A_1 \cap T) = m^*(A_2 \cap T)$$ for all $T \subset \mathbb R$ still hold?","Definition of Lebesgue Outer Measure : Given a set of , we define the Lebesgue Outer Measure of by, where denotes the length of interval (bounded and nonempty interval). Definition of measurable set : A set measurable if for every subset of of . If and , will for all ? Why? How about the result if being a measurable set added? Update: Sincerely, I appreciate user140776's answer and he gives the counterexample that equality doesn't hold for . I still have a question: if , does the equality hold? Or it doesn't hold until adding the condition that is measurable? Besides, if I remove the restriction of being a subset of that is and , will that equality for all still hold?","E \mathbb R E m^*(E) = \inf \left\{\sum_{n=1}^{+\infty} l(I_n): E \subset \bigcup_{n=1}^{+\infty}I_n \right\} l(I_n) E m^*(T) = m^*(T \cap E) + m^*(T \cap E^c) T \mathbb R A_1 \subset A_2 \subset \mathbb R m^*(A_1) = m^*(A_2) m^*(A_1 \cap T) = m^*(A_2 \cap T) T \subset \mathbb R A_1 m^*(A_1) = m^*(A_2) = +\infty m^*(A_1) = m^*(A_2) < +\infty A_1 A_1 A_2 A_1, A_2 \subset \mathbb R m^*(A_1) = m^*(A_2) < +\infty m^*(A_1 \cap T) = m^*(A_2 \cap T) T \subset \mathbb R","['real-analysis', 'measure-theory', 'lebesgue-measure']"
1,Let ${f_n}$ be a sequence of integrable functions on $\mathbb{R}$ such that $f_n\rightarrow f$ almost everywhere.,Let  be a sequence of integrable functions on  such that  almost everywhere.,{f_n} \mathbb{R} f_n\rightarrow f,"Let ${f_n}$ be a sequence of integrable functions on $\mathbb{R}$ such that $f_n\rightarrow f$ almost everywhere. We also have $f\in L^1(\mathbb{R})$ and $\int_{\mathbb{R}}f_n\rightarrow \int_{\mathbb{R}}f$. I want to prove: For all $\epsilon>0$, there exist a measurable set $A\subset\mathbb{R}$ with finite measure, an integrable function $g\geq 0$, and an integer$N\in \mathbb{N}$ such that for all $n\geq N$, $|\int_{\mathbb{R}\backslash A}f_n|<\epsilon$ and $|f_n|\leq g$ on $A$. This question appears in one of the past papers of the real analysis course I am taking this semester. But I have no idea how to do it.","Let ${f_n}$ be a sequence of integrable functions on $\mathbb{R}$ such that $f_n\rightarrow f$ almost everywhere. We also have $f\in L^1(\mathbb{R})$ and $\int_{\mathbb{R}}f_n\rightarrow \int_{\mathbb{R}}f$. I want to prove: For all $\epsilon>0$, there exist a measurable set $A\subset\mathbb{R}$ with finite measure, an integrable function $g\geq 0$, and an integer$N\in \mathbb{N}$ such that for all $n\geq N$, $|\int_{\mathbb{R}\backslash A}f_n|<\epsilon$ and $|f_n|\leq g$ on $A$. This question appears in one of the past papers of the real analysis course I am taking this semester. But I have no idea how to do it.",,"['real-analysis', 'measure-theory']"
2,Sine and Cosine Expansion Problem,Sine and Cosine Expansion Problem,,"We know: $$\sin{x} = x - \frac{x^3}{3!} + \frac{x^3}{5!}-\dotsb$$  and so on. Also, $$\cos{x} = 1 - \frac{x^2}{2!} +\frac{x^4}{4!}-\dotsb$$ and so on. With the help of these expansions we need to prove that $\sin^2 x+\cos^2 x=1$ . I tried generalizing $\sin{x}$ as $$\sum (-1)^n\frac{x^{2n+1}}{(2n+1)!}$$ and $\cos{x}$ as  $$\sum (-1)^n\frac{x^{2n}}{(2n)!},$$  then squaring and adding. But it didn't get through. Please help!","We know: $$\sin{x} = x - \frac{x^3}{3!} + \frac{x^3}{5!}-\dotsb$$  and so on. Also, $$\cos{x} = 1 - \frac{x^2}{2!} +\frac{x^4}{4!}-\dotsb$$ and so on. With the help of these expansions we need to prove that $\sin^2 x+\cos^2 x=1$ . I tried generalizing $\sin{x}$ as $$\sum (-1)^n\frac{x^{2n+1}}{(2n+1)!}$$ and $\cos{x}$ as  $$\sum (-1)^n\frac{x^{2n}}{(2n)!},$$  then squaring and adding. But it didn't get through. Please help!",,['real-analysis']
3,Show that $\lim_{n\to\infty}(6n)^{\frac16}a_n=1$ with $(a_n)$ such that $\lim_{n\to\infty}a_n\sum_{j=1}^na_j^5=1$,Show that  with  such that,\lim_{n\to\infty}(6n)^{\frac16}a_n=1 (a_n) \lim_{n\to\infty}a_n\sum_{j=1}^na_j^5=1,"Show that $$ \lim_{n\to\infty}(6n)^{\frac16}a_n=1, $$ where $(a_n)$ is a sequence of nonnegative real numbers such that $\lim_{n\to\infty}a_n\sum_{j=1}^na_j^5=1.$ I recently got stuck on this problem. I can't seem to make an approach because this is the first I've seen this sort of problem.","Show that $$ \lim_{n\to\infty}(6n)^{\frac16}a_n=1, $$ where $(a_n)$ is a sequence of nonnegative real numbers such that $\lim_{n\to\infty}a_n\sum_{j=1}^na_j^5=1.$ I recently got stuck on this problem. I can't seem to make an approach because this is the first I've seen this sort of problem.",,"['calculus', 'real-analysis', 'limits', 'summation']"
4,Define the Riemann integral via trapezoids instead of rectangles,Define the Riemann integral via trapezoids instead of rectangles,,"Let $I$ be an interval and $f\colon I \to \mathbb{R}$. Recall that $f$ is called Riemann-integrable with integral $s$ if the following is true: For all $\epsilon > 0$, there exists $\delta > 0$ such that for any tagged partition $x_0,\ldots,x_n$ of $I$ and $t_0,\ldots,t_{n-1}$ whose mesh is less than $\delta$, we have $$\left|\sum_{i=0}^{n-1} f(t_i) (x_{i+1}-x_i) - s\right| < \epsilon$$ The intuitive idea which leads to the Riemann integral is that you approximate the ""area under the curve"" by rectangles. However one could also start with the idea to approximate it via trapezoids. So one could try to define the ""trapezoid integral"" via: For all $\epsilon > 0$, there exists $\delta > 0$ such that for any  partition $x_0,\ldots,x_n$ of $I$ whose mesh is less than $\delta$, we have $$\left |\frac{1}{2} \sum_{k=0}^{n-1} \left( x_{k+1} - x_{k} \right) \left( f(x_{k+1}) + f(x_{k})\right) -s \right | < \epsilon$$ Would this ""trapezoid integral"" be equivalent to the Riemann integral in the sense that a function is trapezoid integrable iff it is Riemann integrable and the integral s are equal in this case? If not, is one more general than the other? If not: Is it possible to make a slightly different definition of the integral starting from the trapezoid idea such that one can state such a theorem? Is it also possible to generalize the idea to a Newton-Cotes approach and also get a clear connection to the Riemann integral Is this type of ""trapezoid"" integral (or a generalization) known in the literature? If so, do you have a reference which states and proves theorems about the relation to the Riemann integral? Note that I know the trapezoid Rule for approximating the Riemann integral but this is only for numerical approximations.","Let $I$ be an interval and $f\colon I \to \mathbb{R}$. Recall that $f$ is called Riemann-integrable with integral $s$ if the following is true: For all $\epsilon > 0$, there exists $\delta > 0$ such that for any tagged partition $x_0,\ldots,x_n$ of $I$ and $t_0,\ldots,t_{n-1}$ whose mesh is less than $\delta$, we have $$\left|\sum_{i=0}^{n-1} f(t_i) (x_{i+1}-x_i) - s\right| < \epsilon$$ The intuitive idea which leads to the Riemann integral is that you approximate the ""area under the curve"" by rectangles. However one could also start with the idea to approximate it via trapezoids. So one could try to define the ""trapezoid integral"" via: For all $\epsilon > 0$, there exists $\delta > 0$ such that for any  partition $x_0,\ldots,x_n$ of $I$ whose mesh is less than $\delta$, we have $$\left |\frac{1}{2} \sum_{k=0}^{n-1} \left( x_{k+1} - x_{k} \right) \left( f(x_{k+1}) + f(x_{k})\right) -s \right | < \epsilon$$ Would this ""trapezoid integral"" be equivalent to the Riemann integral in the sense that a function is trapezoid integrable iff it is Riemann integrable and the integral s are equal in this case? If not, is one more general than the other? If not: Is it possible to make a slightly different definition of the integral starting from the trapezoid idea such that one can state such a theorem? Is it also possible to generalize the idea to a Newton-Cotes approach and also get a clear connection to the Riemann integral Is this type of ""trapezoid"" integral (or a generalization) known in the literature? If so, do you have a reference which states and proves theorems about the relation to the Riemann integral? Note that I know the trapezoid Rule for approximating the Riemann integral but this is only for numerical approximations.",,"['real-analysis', 'integration', 'analysis', 'reference-request', 'riemann-sum']"
5,Find lim sup lim inf,Find lim sup lim inf,,How do I find $\lim \sup\text{ or } \lim \inf$ of $ \sin (\frac{n\pi}{5})$ ? I know the $\sin$ function normally oscillates between $-1$ and $1$ but that obviously is not the answer for $\lim  \inf$ and $\lim \sup$.,How do I find $\lim \sup\text{ or } \lim \inf$ of $ \sin (\frac{n\pi}{5})$ ? I know the $\sin$ function normally oscillates between $-1$ and $1$ but that obviously is not the answer for $\lim  \inf$ and $\lim \sup$.,,['real-analysis']
6,Absolute continuity implies Lipschitz iff $\sup(f'(x))<\infty$,Absolute continuity implies Lipschitz iff,\sup(f'(x))<\infty,"If $f(x)$ is absolutely continuous on $[0,1]$, prove that $f(x)$ is Lipschitz if and only if $\sup\left|f'(x)\right|<\infty$. Does the conclusion follow if $f(x)$ is of bounded variation, but not absolutely continuous? This question represents the last of my ""not entirely confident"" answers I gave on my recent qualifying exam. Generally, I'm concerned about whether I've applied all the concepts correctly in my proof. ATTEMPT: Given that $f(x)$ is absolutely continuous, we have that $f'(x)$ exists a.e. and that $\int_x^y f'(t)dt=f(y)-f(x)$ for all $y, x\in[0,1]$, $y>x$. Suppose that $L=\sup\left|f'(x)\right|<\infty$ where the supremum is taken over all $x\in[0,1]$ such that $f'(x)$ exists. Without loss of generality, let $y,x\in [0,1]$ with $y>x.$ Then we have that $\int_x^y f'(t)dt = f(y)-f(x) \leq \int_x^y L dt = L(y-x)$. Hence $f(x)$ is Lipschitz. Suppose $f(x)$ is Lipschitz; hence there is an $L<\infty$ such that $\left|f(y)-f(x)\right|\leq L\left|y-x\right|$ for $y,x \in [0,1]$. Then if $f$ is differentiable at $x\in[0,1]$, we have that, for $y\not = x$, $$\left|f'(x)\right|= \lim_{y \to x} \left|\frac{f(y)-f(x)}{y-x}\right|\leq L\left|\frac{y-x}{y-x}\right|=L<\infty$$ hence $\sup{\left|f'(x)\right|} \leq L<\infty$. This concludes the proof. For the follow-up question, the conclusion does not follow, as a function that is not absolutely continuous cannot be Lipschitz (I provided a proof that all Lipschitz functions are absolutely continuous, but will not reproduce it here). To show that this set is nonempty, let $f$ be the Cantor function on $[0,1]$. Then $f$ is a bounded, monotone increasing function, hence of bounded variation (with total variation 1) with $f'(x)=0$ a.e. But $\int_0^1 f'(x)dx=0<f(1)-f(0)=1-0=1$ shows that $f$ is not absolutely continuous on $[0,1]$.","If $f(x)$ is absolutely continuous on $[0,1]$, prove that $f(x)$ is Lipschitz if and only if $\sup\left|f'(x)\right|<\infty$. Does the conclusion follow if $f(x)$ is of bounded variation, but not absolutely continuous? This question represents the last of my ""not entirely confident"" answers I gave on my recent qualifying exam. Generally, I'm concerned about whether I've applied all the concepts correctly in my proof. ATTEMPT: Given that $f(x)$ is absolutely continuous, we have that $f'(x)$ exists a.e. and that $\int_x^y f'(t)dt=f(y)-f(x)$ for all $y, x\in[0,1]$, $y>x$. Suppose that $L=\sup\left|f'(x)\right|<\infty$ where the supremum is taken over all $x\in[0,1]$ such that $f'(x)$ exists. Without loss of generality, let $y,x\in [0,1]$ with $y>x.$ Then we have that $\int_x^y f'(t)dt = f(y)-f(x) \leq \int_x^y L dt = L(y-x)$. Hence $f(x)$ is Lipschitz. Suppose $f(x)$ is Lipschitz; hence there is an $L<\infty$ such that $\left|f(y)-f(x)\right|\leq L\left|y-x\right|$ for $y,x \in [0,1]$. Then if $f$ is differentiable at $x\in[0,1]$, we have that, for $y\not = x$, $$\left|f'(x)\right|= \lim_{y \to x} \left|\frac{f(y)-f(x)}{y-x}\right|\leq L\left|\frac{y-x}{y-x}\right|=L<\infty$$ hence $\sup{\left|f'(x)\right|} \leq L<\infty$. This concludes the proof. For the follow-up question, the conclusion does not follow, as a function that is not absolutely continuous cannot be Lipschitz (I provided a proof that all Lipschitz functions are absolutely continuous, but will not reproduce it here). To show that this set is nonempty, let $f$ be the Cantor function on $[0,1]$. Then $f$ is a bounded, monotone increasing function, hence of bounded variation (with total variation 1) with $f'(x)=0$ a.e. But $\int_0^1 f'(x)dx=0<f(1)-f(0)=1-0=1$ shows that $f$ is not absolutely continuous on $[0,1]$.",,"['real-analysis', 'proof-verification']"
7,"(complex measures) $d\nu=d\lambda +f\,dm \Rightarrow d|\nu|=d|\lambda| +|f|\,dm$ and $f\in L^1(\nu)\Rightarrow f\in L^1(|\nu|)$",(complex measures)  and,"d\nu=d\lambda +f\,dm \Rightarrow d|\nu|=d|\lambda| +|f|\,dm f\in L^1(\nu)\Rightarrow f\in L^1(|\nu|)","Two simple exercises on complex measures I don't know how to solve, from Folland's Real Analysis . I find it difficult to manipulate the definitions in computations. For context, if $\nu$ is a complex measure on $(X,\mathcal{M})$ then $|\nu|$ is defined as $d|\nu|=|g|\,d\mu$ where positive measure $\mu$ and $g\in L^1(\mu)$ are such that $\nu \ll \mu$ and $\nu = g\,d\mu$ (and in the textbook it's shown that this definition does not depend on the choice of $\mu$). 1) Let $\nu$ be a reguar complex Borel measure on $\mathbb{R}^n$, $m$ denote   Lebesgue measure, and   let $d\nu = d\lambda + f\,dm$ be its Lebesgue-Radon-Nikodym   representation. Then (""it is easily verified"")   $d|\nu|=d|\lambda|+|f|\,dm$. 2) If $f\in L^1(\nu)$ then $f\in L^1(|\nu|)$. I was able to prove the   reverse implication, but not this one. The definition of $|\nu|$ in the case of real signed measures (as $|\nu|=\nu^+ + \nu^-$) makes sense to me but I can't see intuitively into the definion of $|\nu|$ for complex measures as given in the textbook. Do you have any advice on this? Thank you!","Two simple exercises on complex measures I don't know how to solve, from Folland's Real Analysis . I find it difficult to manipulate the definitions in computations. For context, if $\nu$ is a complex measure on $(X,\mathcal{M})$ then $|\nu|$ is defined as $d|\nu|=|g|\,d\mu$ where positive measure $\mu$ and $g\in L^1(\mu)$ are such that $\nu \ll \mu$ and $\nu = g\,d\mu$ (and in the textbook it's shown that this definition does not depend on the choice of $\mu$). 1) Let $\nu$ be a reguar complex Borel measure on $\mathbb{R}^n$, $m$ denote   Lebesgue measure, and   let $d\nu = d\lambda + f\,dm$ be its Lebesgue-Radon-Nikodym   representation. Then (""it is easily verified"")   $d|\nu|=d|\lambda|+|f|\,dm$. 2) If $f\in L^1(\nu)$ then $f\in L^1(|\nu|)$. I was able to prove the   reverse implication, but not this one. The definition of $|\nu|$ in the case of real signed measures (as $|\nu|=\nu^+ + \nu^-$) makes sense to me but I can't see intuitively into the definion of $|\nu|$ for complex measures as given in the textbook. Do you have any advice on this? Thank you!",,"['real-analysis', 'integration', 'measure-theory', 'derivatives']"
8,How to prove that $f(x)=x^{x^x}$ is strictly increasing without calculating the derivative?,How to prove that  is strictly increasing without calculating the derivative?,f(x)=x^{x^x},"How to prove that $f:(0,\infty )\to\mathbb R$ defined by $f(x)=x^{(x^x)}$ is strictly increasing without calculating the derivative?","How to prove that $f:(0,\infty )\to\mathbb R$ defined by $f(x)=x^{(x^x)}$ is strictly increasing without calculating the derivative?",,['real-analysis']
9,Any function $f:\mathbb{R} \to \mathbb{R}$ is sum of two Darboux functions,Any function  is sum of two Darboux functions,f:\mathbb{R} \to \mathbb{R},"From Wikipedia: Darboux functions are a quite general class of functions. It turns out that any real-valued function f on the real line can be written as the sum of two Darboux functions. This implies in particular that the class of Darboux functions is not closed under addition. (Darboux functions are simply those that satisfy the intermediate value property). Proof? I'm looking for this out of interest, and couldn't find one - a hint, nudge, reference or link are also sufficient. If you're feeling brave: can we extend this, say to $f:\mathbb{C} \to \mathbb{R}$? Proof or counter-example, of course. EDIT: Further question: If a function $f:[a,b] \to \mathbb{R}$ is differentiable on $[a,b]$ then its derivative $f'$ is Darboux on $[a,b]$. Can any real function be written as $f'+g'$ for some $f,g$? (no)","From Wikipedia: Darboux functions are a quite general class of functions. It turns out that any real-valued function f on the real line can be written as the sum of two Darboux functions. This implies in particular that the class of Darboux functions is not closed under addition. (Darboux functions are simply those that satisfy the intermediate value property). Proof? I'm looking for this out of interest, and couldn't find one - a hint, nudge, reference or link are also sufficient. If you're feeling brave: can we extend this, say to $f:\mathbb{C} \to \mathbb{R}$? Proof or counter-example, of course. EDIT: Further question: If a function $f:[a,b] \to \mathbb{R}$ is differentiable on $[a,b]$ then its derivative $f'$ is Darboux on $[a,b]$. Can any real function be written as $f'+g'$ for some $f,g$? (no)",,['real-analysis']
10,The existence of conditional expectation with respect to a sub-$\sigma$-algebra,The existence of conditional expectation with respect to a sub--algebra,\sigma,"I was trying to solve the exercise 3.17 from the book of real analysis by Folland and I've found a problem. The first part of the exercise is the following: Let $(X, M, \mu) $ be a $\sigma$-finite measure space, $ N $ a sub-$\sigma $-algebra of M and  $\nu$ the restriction of $\mu $ to $N $. If $ f \in L^1 (\mu) $, there exist $ g \in L^1 (\nu) $ such that $\int_E f \,d\mu = \int_E g\, d\nu $ for all $ E \in N $. Let $ X $ the set of natural numbers, $\mu $ the counting measure, $ N $ containing just $ X $ and the empty set and $ f $ the characteristic function of the singlepoint $1 $ which is a $ L^1 $ function. Then, we have $$ 1 = \int_X g\, d\nu ,$$ for some $ g \in L^1 (\nu) $. But if $ h $ is positive and $\nu $-measurable function, we can take an increasing sequence of simple and measurable functions which converges to $ h $. By definition of $ N $, a simple measurable function is a linear combination of the characeristic of $ X $ and the empty set. So, the integral of $ h $ must be 0 or infinity. Then $ L^1 (\nu) $ is trivial, and we have an absurd. Is there something wrong in this argument? I can't see. And since he doesn't define sub-$\sigma $-algebra before (I'm assuming that is just a $\sigma $-algebra contained in the first), perhaps we need to assume more things. If I could conclude that $ N $ is also $ \sigma$- finite, then we can apply the Lebesgue Radon Nikodym to solve the problem, but the same example above shows that we don't have this in general.","I was trying to solve the exercise 3.17 from the book of real analysis by Folland and I've found a problem. The first part of the exercise is the following: Let $(X, M, \mu) $ be a $\sigma$-finite measure space, $ N $ a sub-$\sigma $-algebra of M and  $\nu$ the restriction of $\mu $ to $N $. If $ f \in L^1 (\mu) $, there exist $ g \in L^1 (\nu) $ such that $\int_E f \,d\mu = \int_E g\, d\nu $ for all $ E \in N $. Let $ X $ the set of natural numbers, $\mu $ the counting measure, $ N $ containing just $ X $ and the empty set and $ f $ the characteristic function of the singlepoint $1 $ which is a $ L^1 $ function. Then, we have $$ 1 = \int_X g\, d\nu ,$$ for some $ g \in L^1 (\nu) $. But if $ h $ is positive and $\nu $-measurable function, we can take an increasing sequence of simple and measurable functions which converges to $ h $. By definition of $ N $, a simple measurable function is a linear combination of the characeristic of $ X $ and the empty set. So, the integral of $ h $ must be 0 or infinity. Then $ L^1 (\nu) $ is trivial, and we have an absurd. Is there something wrong in this argument? I can't see. And since he doesn't define sub-$\sigma $-algebra before (I'm assuming that is just a $\sigma $-algebra contained in the first), perhaps we need to assume more things. If I could conclude that $ N $ is also $ \sigma$- finite, then we can apply the Lebesgue Radon Nikodym to solve the problem, but the same example above shows that we don't have this in general.",,"['real-analysis', 'measure-theory', 'lp-spaces', 'conditional-expectation']"
11,Integral of products of cosines,Integral of products of cosines,,"Given $m+1$ integers $\alpha_0,\ldots,\alpha_m\geq 1$, I was trying to get a nice closed formula for the integral $$ \int_0^\pi\cos(\alpha_1\theta)\cdots\cos(\alpha_m\theta)d\theta. $$ More precisely, I was trying to count the number of $m$-uplets $(\alpha_1\ldots,\alpha_m)$ with $1\leq \alpha_1\leq\ldots\leq\alpha_m\leq n$ such that the later integral is non-zero. When $m=1$, this is easy because of the orthogonality relation $$ \frac{2}{\pi}\int_0^\pi\cos(\alpha_0\theta)\cos(\alpha_1\theta)d\theta=\;\boldsymbol 1_{\alpha_0=\alpha_1}, $$  and thus the number of such solutions is $\boldsymbol 1_{\alpha_0\leq n}$, that is the number of ways to partition $\alpha_0$ into one positive integer smaller than $n$. If one looks instead at the integral $$ \frac{1}{2\pi}\int_0^{2\pi}e^{i\alpha_0\theta}e^{-i\alpha_1\theta}\cdots e^{-i\alpha_m\theta} d\theta $$ then the number of solutions is the number of ways to partition $\alpha_0$ into $m$ integers smaller than $n$. I was somehow hopping that a similar type of solution would appear for the cosine integral, but maybe that is too optimistic ... Any idea for the general case with the cosines ? Or a reference ? I have tried to use inductively the trigonometric formula $cos(a)cos(b)=\frac{1}{2}(cos(a+b)+\cos(a-b))$, but I don't see any structure appearing.","Given $m+1$ integers $\alpha_0,\ldots,\alpha_m\geq 1$, I was trying to get a nice closed formula for the integral $$ \int_0^\pi\cos(\alpha_1\theta)\cdots\cos(\alpha_m\theta)d\theta. $$ More precisely, I was trying to count the number of $m$-uplets $(\alpha_1\ldots,\alpha_m)$ with $1\leq \alpha_1\leq\ldots\leq\alpha_m\leq n$ such that the later integral is non-zero. When $m=1$, this is easy because of the orthogonality relation $$ \frac{2}{\pi}\int_0^\pi\cos(\alpha_0\theta)\cos(\alpha_1\theta)d\theta=\;\boldsymbol 1_{\alpha_0=\alpha_1}, $$  and thus the number of such solutions is $\boldsymbol 1_{\alpha_0\leq n}$, that is the number of ways to partition $\alpha_0$ into one positive integer smaller than $n$. If one looks instead at the integral $$ \frac{1}{2\pi}\int_0^{2\pi}e^{i\alpha_0\theta}e^{-i\alpha_1\theta}\cdots e^{-i\alpha_m\theta} d\theta $$ then the number of solutions is the number of ways to partition $\alpha_0$ into $m$ integers smaller than $n$. I was somehow hopping that a similar type of solution would appear for the cosine integral, but maybe that is too optimistic ... Any idea for the general case with the cosines ? Or a reference ? I have tried to use inductively the trigonometric formula $cos(a)cos(b)=\frac{1}{2}(cos(a+b)+\cos(a-b))$, but I don't see any structure appearing.",,"['real-analysis', 'integration', 'reference-request', 'integer-partitions']"
12,Sequence Limit Problem: If $0 \leq x_{m+n} \leq x_n + x_m$ then limit of $x_n/n$ exists,Sequence Limit Problem: If  then limit of  exists,0 \leq x_{m+n} \leq x_n + x_m x_n/n,"If the sequence $\{x_n\}$ satisfies the property that $0 \leq x_{m+n} \leq x_n + x_m$ for all $n$, $m \in \mathbb{N}$ , show that the limit of the sequence $\left\{\frac{x_n}{n}\right\}_n$ exists. Provide an example of such a sequence. I can show that it is bounded, but I don't know how to show that it is monotone increasing (I think it is monotone increasing, not sure).","If the sequence $\{x_n\}$ satisfies the property that $0 \leq x_{m+n} \leq x_n + x_m$ for all $n$, $m \in \mathbb{N}$ , show that the limit of the sequence $\left\{\frac{x_n}{n}\right\}_n$ exists. Provide an example of such a sequence. I can show that it is bounded, but I don't know how to show that it is monotone increasing (I think it is monotone increasing, not sure).",,"['real-analysis', 'sequences-and-series']"
13,Integral $ \int_0^\infty \frac{x^n\ln x}{(x^2+\alpha^2)^2(e^x-1)}dx$,Integral, \int_0^\infty \frac{x^n\ln x}{(x^2+\alpha^2)^2(e^x-1)}dx,"Hey I am trying to integrate $$ \int_0^\infty \frac{x^n\ln x}{(x^2+\alpha^2)^2(e^x-1)}dx,\quad \alpha,n \in \mathbb{R}^{0+}. $$ This integral is old.  I am also looking for literature on these integrals as I have seen many for small values of n, and variations of this.  Thanks.  Maybe we can use residues however the log function in the denominator is what is concerning me, without that I can see what to do","Hey I am trying to integrate $$ \int_0^\infty \frac{x^n\ln x}{(x^2+\alpha^2)^2(e^x-1)}dx,\quad \alpha,n \in \mathbb{R}^{0+}. $$ This integral is old.  I am also looking for literature on these integrals as I have seen many for small values of n, and variations of this.  Thanks.  Maybe we can use residues however the log function in the denominator is what is concerning me, without that I can see what to do",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'contour-integration']"
14,Does this really converge to 1/e? (Massaging a sum),Does this really converge to 1/e? (Massaging a sum),,"Short version: can we prove that $$\sum_{k=0}^n (-1)^k \binom{n}{k}^2 \frac{k!}{n^{2k}} \to \frac1e$$ as $n \to \infty$? Long version: First, consider $$a_n = \sum_{k=0}^n \frac{(-1)^k}{k!}$$ It is well-known that $a_n \to \dfrac1e$ as $n \to \infty$; indeed $a_n = \dfrac{!n}{n!}$ is the truncation to the first $n$ terms of the power series for $e^x$, evaluated at $x = -1$ (where $!n$ denotes the subfactorial; it is also equal to the number of derangements on $n$ elements). There is also a simple expression for the generating function $A(z) = \sum_{n=0}^{\infty} a_n z^n$, which is $A(z) = \dfrac{e^{-z}}{1-z}$. (See Exponential Generating Functions For Derangements .) Next, consider $$b_n = \sum_{k=0}^n \frac{n^{\underline k}}{n^k} \frac{(-1)^k}{k!}$$ where $n^{\underline k} = \binom{n}{k}k!$ denotes a falling factorial, so the extra factor $\frac{n^{\underline k}}{n^k}$ is $\frac{n(n-1)\cdots(n-k+1)}{n(n)\cdots(n)} = 1\cdot\left(1 - \frac1n\right)\cdot\left(1 - \frac2n\right)\cdots\left(1 - \frac{k-1}n\right)$ which for large $n$ (and fixed $k$) is close to $1$.  I don't know if $b_n$ has a simple form for its generating function too, but it is easy to see that $b_n \to \dfrac1e$ as well; indeed by the binomial theorem we have $b_n = \sum_{k=0}^n \binom{n}{k} (\frac{-1}{n})^k = \left(1 - \frac1n \right)^n$ which is well-known to converge to $\frac1e$ (indeed such a limit is sometimes taken to be the definition of $e^x$). Finally, consider $$c_n = \sum_{k=0}^n \frac{n^{\underline k}}{n^k} \frac{n^{\underline k}}{n^k} \frac{(-1)^k}{k!}$$ This is the same transformation going from $b_n$ to $c_n$ as from $a_n$ to $b_n$. But can we prove that $c_n \to \frac1e$ too? (And can we write down its generating function compactly, perchance?) More generally, what techniques exist that help in proving something about $\sum t_n s_n$, given $\sum s_n$? This question arose from an attempt to answer this question , where I arrived at the expression $c_n$ above (there I called it $P_{n, n, 0}$; next I'll try to understand $P_{m, w, k}$). [Note: I'm tagging this special-functions too, as I understand $c_n$ has something to do with hypergeometric functions / Bessel functions / something like that.]","Short version: can we prove that $$\sum_{k=0}^n (-1)^k \binom{n}{k}^2 \frac{k!}{n^{2k}} \to \frac1e$$ as $n \to \infty$? Long version: First, consider $$a_n = \sum_{k=0}^n \frac{(-1)^k}{k!}$$ It is well-known that $a_n \to \dfrac1e$ as $n \to \infty$; indeed $a_n = \dfrac{!n}{n!}$ is the truncation to the first $n$ terms of the power series for $e^x$, evaluated at $x = -1$ (where $!n$ denotes the subfactorial; it is also equal to the number of derangements on $n$ elements). There is also a simple expression for the generating function $A(z) = \sum_{n=0}^{\infty} a_n z^n$, which is $A(z) = \dfrac{e^{-z}}{1-z}$. (See Exponential Generating Functions For Derangements .) Next, consider $$b_n = \sum_{k=0}^n \frac{n^{\underline k}}{n^k} \frac{(-1)^k}{k!}$$ where $n^{\underline k} = \binom{n}{k}k!$ denotes a falling factorial, so the extra factor $\frac{n^{\underline k}}{n^k}$ is $\frac{n(n-1)\cdots(n-k+1)}{n(n)\cdots(n)} = 1\cdot\left(1 - \frac1n\right)\cdot\left(1 - \frac2n\right)\cdots\left(1 - \frac{k-1}n\right)$ which for large $n$ (and fixed $k$) is close to $1$.  I don't know if $b_n$ has a simple form for its generating function too, but it is easy to see that $b_n \to \dfrac1e$ as well; indeed by the binomial theorem we have $b_n = \sum_{k=0}^n \binom{n}{k} (\frac{-1}{n})^k = \left(1 - \frac1n \right)^n$ which is well-known to converge to $\frac1e$ (indeed such a limit is sometimes taken to be the definition of $e^x$). Finally, consider $$c_n = \sum_{k=0}^n \frac{n^{\underline k}}{n^k} \frac{n^{\underline k}}{n^k} \frac{(-1)^k}{k!}$$ This is the same transformation going from $b_n$ to $c_n$ as from $a_n$ to $b_n$. But can we prove that $c_n \to \frac1e$ too? (And can we write down its generating function compactly, perchance?) More generally, what techniques exist that help in proving something about $\sum t_n s_n$, given $\sum s_n$? This question arose from an attempt to answer this question , where I arrived at the expression $c_n$ above (there I called it $P_{n, n, 0}$; next I'll try to understand $P_{m, w, k}$). [Note: I'm tagging this special-functions too, as I understand $c_n$ has something to do with hypergeometric functions / Bessel functions / something like that.]",,"['real-analysis', 'sequences-and-series', 'special-functions', 'binomial-coefficients']"
15,"Two sequences are subsequences of one another, one converges. Are they the same?","Two sequences are subsequences of one another, one converges. Are they the same?",,"Let $a_n$ converge to a. Let $b_n$ be a subsequence of $a_n$ and $a_n$ be a subsequence of $b_n$. Are they the same? I've tried showing they are by contradiction, the Bolzano-Weierstrass theorem seems relevant but I can't see where. I can't see any counter example but there seems to be some pattern to why examples fail but I can't see it. Removing the restriction of $a_n$ converging seems to allow for counter examples which all seem to alternate, if I could prove that they all alternate then it would be done as alternating series don't converge. Hints would be most welcome!","Let $a_n$ converge to a. Let $b_n$ be a subsequence of $a_n$ and $a_n$ be a subsequence of $b_n$. Are they the same? I've tried showing they are by contradiction, the Bolzano-Weierstrass theorem seems relevant but I can't see where. I can't see any counter example but there seems to be some pattern to why examples fail but I can't see it. Removing the restriction of $a_n$ converging seems to allow for counter examples which all seem to alternate, if I could prove that they all alternate then it would be done as alternating series don't converge. Hints would be most welcome!",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
16,Supremum of a product measurable function...,Supremum of a product measurable function...,,"This is an interesting question that has stumped the entirety of my measure theory class, including the professor: Prove or disprove: Let $(X,\mathcal A)$, $(Y,\mathcal B)$ be measure spaces. Let $f$ be an $\mathcal A$ $\times$ $\mathcal B$ measurable nonnegative function, and let $g(x)$ $=$ $sup_{y \in Y}f(x,y)$, with $g(x)<\infty$ for all $x$. Is $g(x)$ necessarily an $\mathcal A$-measurable function? We all feel the answer is no, given that slices are measurable, and sups of measurable functions are only guaranteed to be measurable over a countable index. We think the correct answer is to start with a nonmeasurable set $S$ in $X$, and to try to build a set $T$ in $Y$ that makes $S \times T$ measurable in $\mathcal A \times \mathcal B$, but we suspect this is quite difficult with no further guidance. Any ideas?","This is an interesting question that has stumped the entirety of my measure theory class, including the professor: Prove or disprove: Let $(X,\mathcal A)$, $(Y,\mathcal B)$ be measure spaces. Let $f$ be an $\mathcal A$ $\times$ $\mathcal B$ measurable nonnegative function, and let $g(x)$ $=$ $sup_{y \in Y}f(x,y)$, with $g(x)<\infty$ for all $x$. Is $g(x)$ necessarily an $\mathcal A$-measurable function? We all feel the answer is no, given that slices are measurable, and sups of measurable functions are only guaranteed to be measurable over a countable index. We think the correct answer is to start with a nonmeasurable set $S$ in $X$, and to try to build a set $T$ in $Y$ that makes $S \times T$ measurable in $\mathcal A \times \mathcal B$, but we suspect this is quite difficult with no further guidance. Any ideas?",,"['real-analysis', 'measure-theory']"
17,Tangent space for product of submanifolds,Tangent space for product of submanifolds,,"Suppose that $X_1$ is an $n_1$-dimensional submanifold of $\mathbb{R}^{N_1}$, and $X_2$ is an $n_2$-dimensional submanifold of $\mathbb{R}^{N_2}$, and let $X=X_1\times X_2$. Let $p_1\in X_1$ and $p_2\in X_2$. Describe the tangent space $T_pX$ in terms of the tangent spaces $T_{p_1}X_1$ and $T_{p_2}X_2$, where $p=(p_1,p_2)$. By this question we have that $X$ is an $(n_1+n_2)$-dimensional submanifold of $\mathbb{R}^{N_1+N_2}$. By definition of submanifold, for every $p\in X$, there exists an open set $U\in\mathbb{R}^{n_1+n_2}$ and a neighborhood $V_p$ of $p$ in $\mathbb{R}^{N_1+N_2}$ and a diffeomorphism $\phi: U\rightarrow X\cap V_p$. Let $q=\phi^{-1}(p)$. The derivative at $q$ is a map $d\phi_q:T_q\mathbb{R}^n\rightarrow T_p\mathbb{R}^N$. Basically, this takes a point $(q,v)$ to $(p,D\phi(q)v)$. and the tangent space $T_pX$ to $X$ at $p$ is the image of this linear map. So this image consists of all $(p,w)$ where $w=D\phi(q)v$ for some $v\in\mathbb{R}^n$. I am trying to prove that $T_pX=T_{p_1}X_1\oplus T_{p_2}X_2$, but I don't quite see how that will go.","Suppose that $X_1$ is an $n_1$-dimensional submanifold of $\mathbb{R}^{N_1}$, and $X_2$ is an $n_2$-dimensional submanifold of $\mathbb{R}^{N_2}$, and let $X=X_1\times X_2$. Let $p_1\in X_1$ and $p_2\in X_2$. Describe the tangent space $T_pX$ in terms of the tangent spaces $T_{p_1}X_1$ and $T_{p_2}X_2$, where $p=(p_1,p_2)$. By this question we have that $X$ is an $(n_1+n_2)$-dimensional submanifold of $\mathbb{R}^{N_1+N_2}$. By definition of submanifold, for every $p\in X$, there exists an open set $U\in\mathbb{R}^{n_1+n_2}$ and a neighborhood $V_p$ of $p$ in $\mathbb{R}^{N_1+N_2}$ and a diffeomorphism $\phi: U\rightarrow X\cap V_p$. Let $q=\phi^{-1}(p)$. The derivative at $q$ is a map $d\phi_q:T_q\mathbb{R}^n\rightarrow T_p\mathbb{R}^N$. Basically, this takes a point $(q,v)$ to $(p,D\phi(q)v)$. and the tangent space $T_pX$ to $X$ at $p$ is the image of this linear map. So this image consists of all $(p,w)$ where $w=D\phi(q)v$ for some $v\in\mathbb{R}^n$. I am trying to prove that $T_pX=T_{p_1}X_1\oplus T_{p_2}X_2$, but I don't quite see how that will go.",,"['real-analysis', 'manifolds']"
18,If $\|\left(f'(x)\right)^{-1}\|\le 1 \Longrightarrow$ $f$ is an diffeomorphism,If   is an diffeomorphism,\|\left(f'(x)\right)^{-1}\|\le 1 \Longrightarrow f,"Let $f:\mathbb{R}^n \longrightarrow \mathbb{R}^n,f\in C^1(\mathbb{R}^n)$ such that $\forall x \in \mathbb{R}^n\;,\;f'(x)$ is an isomorphism and: $$ \|\left(f'(x)\right)^{-1}\|\le 1\;,\forall x \in \mathbb{R}^n$$ How can we prove that  $f$ is a diffeomorphism ? Author's suggestion: $\forall [f(a),\color{red}b]\subset \mathbb{R}^n,\exists \lambda:[0,1]\longrightarrow \mathbb{R}^n$ such that $\lambda(0)=a$ and $f(\lambda(t))=(1-t)f(a)+t\color{red}b$ therefore $f$ is surjective. Note that  $f$ is a local diffeomorphism, the strong part is to show that $f$ is injective. There is a proof without using homotopy ? Any hints would be appreciated.","Let $f:\mathbb{R}^n \longrightarrow \mathbb{R}^n,f\in C^1(\mathbb{R}^n)$ such that $\forall x \in \mathbb{R}^n\;,\;f'(x)$ is an isomorphism and: $$ \|\left(f'(x)\right)^{-1}\|\le 1\;,\forall x \in \mathbb{R}^n$$ How can we prove that  $f$ is a diffeomorphism ? Author's suggestion: $\forall [f(a),\color{red}b]\subset \mathbb{R}^n,\exists \lambda:[0,1]\longrightarrow \mathbb{R}^n$ such that $\lambda(0)=a$ and $f(\lambda(t))=(1-t)f(a)+t\color{red}b$ therefore $f$ is surjective. Note that  $f$ is a local diffeomorphism, the strong part is to show that $f$ is injective. There is a proof without using homotopy ? Any hints would be appreciated.",,"['real-analysis', 'algebraic-topology', 'covering-spaces']"
19,Two variable limits via paths - are there pathalogical examples?,Two variable limits via paths - are there pathalogical examples?,,"In the first year calculus course at my university, we do not introduce the $\varepsilon$-$\delta$ definition of a limit. When considering the limit of a function of two variables, we resort to paths. That is, for the real-valued function $f$, defined in a deleted open neighbourhood of ${\bf a} \in \mathbb{R}^2$, we write $\displaystyle\lim_{{\bf x} \to {\bf a}}f({\bf x}) = L$ if the limit along every path to ${\bf a}$ is $L$. Though we don't go into this detail, what is meant by the limit along a path is to pick a parameterisation $\gamma : (r, s) \to \mathbb{R}^2$ with $\displaystyle\lim_{t \to s^-}\gamma(t) = {\bf a}$, and then check that $\displaystyle\lim_{t \to s^-}f(\gamma(t)) = L$. Of course, without the $\varepsilon$-$\delta$ definition, this one-dimensional limit can only be deduced from limit laws and given fundamental limits. Without loss of generality, suppose ${\bf a = 0}$. When asked to determine whether or not the limit $\displaystyle\lim_{{\bf x} \to {\bf 0}}f({\bf x})$ exists, there are only a couple of things the students can do: employ continuity if possible (limit laws may be needed), use polar coordinates together with the squeeze theorem, find a path along which the limit does not exist, and find two paths with have different limits. The first two options can be used to show the limit exists, while the last two options can be used to show the limit does not exist. An efficient way to test limits along different paths is to try a whole family of paths simulateously, i.e. we could consider the family of quadratic paths given by $\gamma(t) = (t, kt^2)$ where $k \in \mathbb{R}$. If the limit $\displaystyle\lim_{t \to 0-}f(\gamma(t))$ exists and is independent of $k$, then along all of these paths, the limit is the same. This does not prove that $\displaystyle\lim_{{\bf x} \to {\bf 0}}f({\bf x})$ exists as there are still plenty of paths that have yet to be considered. Questions: Is there a reference which takes this approach to limits in higher dimensions (i.e. greater than one) in a rigorous way without using the $\varepsilon$-$\delta$ definition? I wouldn't be surprised if the answer is no. However, I am somewhat more interested in the following question: How pathological can two-dimensional limits be? That is, along how many paths can the function have the same limit whilst still failing to have the overall limit existing? Is it possible to have all but one? I imagine there would be some sort of density argument that would prevent this.","In the first year calculus course at my university, we do not introduce the $\varepsilon$-$\delta$ definition of a limit. When considering the limit of a function of two variables, we resort to paths. That is, for the real-valued function $f$, defined in a deleted open neighbourhood of ${\bf a} \in \mathbb{R}^2$, we write $\displaystyle\lim_{{\bf x} \to {\bf a}}f({\bf x}) = L$ if the limit along every path to ${\bf a}$ is $L$. Though we don't go into this detail, what is meant by the limit along a path is to pick a parameterisation $\gamma : (r, s) \to \mathbb{R}^2$ with $\displaystyle\lim_{t \to s^-}\gamma(t) = {\bf a}$, and then check that $\displaystyle\lim_{t \to s^-}f(\gamma(t)) = L$. Of course, without the $\varepsilon$-$\delta$ definition, this one-dimensional limit can only be deduced from limit laws and given fundamental limits. Without loss of generality, suppose ${\bf a = 0}$. When asked to determine whether or not the limit $\displaystyle\lim_{{\bf x} \to {\bf 0}}f({\bf x})$ exists, there are only a couple of things the students can do: employ continuity if possible (limit laws may be needed), use polar coordinates together with the squeeze theorem, find a path along which the limit does not exist, and find two paths with have different limits. The first two options can be used to show the limit exists, while the last two options can be used to show the limit does not exist. An efficient way to test limits along different paths is to try a whole family of paths simulateously, i.e. we could consider the family of quadratic paths given by $\gamma(t) = (t, kt^2)$ where $k \in \mathbb{R}$. If the limit $\displaystyle\lim_{t \to 0-}f(\gamma(t))$ exists and is independent of $k$, then along all of these paths, the limit is the same. This does not prove that $\displaystyle\lim_{{\bf x} \to {\bf 0}}f({\bf x})$ exists as there are still plenty of paths that have yet to be considered. Questions: Is there a reference which takes this approach to limits in higher dimensions (i.e. greater than one) in a rigorous way without using the $\varepsilon$-$\delta$ definition? I wouldn't be surprised if the answer is no. However, I am somewhat more interested in the following question: How pathological can two-dimensional limits be? That is, along how many paths can the function have the same limit whilst still failing to have the overall limit existing? Is it possible to have all but one? I imagine there would be some sort of density argument that would prevent this.",,"['calculus', 'real-analysis', 'reference-request', 'limits', 'examples-counterexamples']"
20,What conditions on $f$ make $f\circ |\cdot |$ a smooth function?,What conditions on  make  a smooth function?,f f\circ |\cdot |,"Let $|\cdot |$ be the Euclidean norm on $\mathbb{R}^n$. What conditions (preferably necessary) on $f:[0,\infty)\mapsto\mathbb{R}$ ensure that $f\circ |\cdot |\in C^{\infty}(\mathbb{R}^n)$? For $n=1$, I believe that $f\in C^{\infty}([0,\infty))$ along with $f^{(k)}(0)=0$ for odd $k$ suffice.","Let $|\cdot |$ be the Euclidean norm on $\mathbb{R}^n$. What conditions (preferably necessary) on $f:[0,\infty)\mapsto\mathbb{R}$ ensure that $f\circ |\cdot |\in C^{\infty}(\mathbb{R}^n)$? For $n=1$, I believe that $f\in C^{\infty}([0,\infty))$ along with $f^{(k)}(0)=0$ for odd $k$ suffice.",,"['real-analysis', 'multivariable-calculus']"
21,Derivative map of the diagonal inclusion map on manifolds,Derivative map of the diagonal inclusion map on manifolds,,"I was trying to work through a problem(#10 of $\S$1.2) in Guillemin and Pollack's book $\textit{differential topology. }$ The problem is given as follows. Let $f: X\longrightarrow  X\times X$ be the mapping $f(x)=(x,x)$.   Check that $df_x(v)=(v,v)$. Here $X\subset \mathbf R^m$ is a manifold. My attempt so far has been: First we parametrise an open neighbourhood of $x\in X$ and $(x,x) \in X\times X$ locally by $\phi$ and $\phi \times \phi$ into open subsets $U\subset\mathbf R^m$ and $U\times U$ (we use $\phi(0)=x$ for simplicity). This gives the commuting diagram as follows: $$ \begin{array}[c]{ccc} X\;\;&\stackrel{f}{\longrightarrow}&X\times X\\ \downarrow\scriptstyle{\phi}&&\downarrow\scriptstyle{\phi \times \phi}\\ U\;\;&\stackrel{h}{\longrightarrow}&U\times U \end{array} $$ $$ \begin{array}[c]{ccc} T_x(X)&\stackrel{df_x}{\longrightarrow}&T_{(x,x)}(X\times X)\\ \downarrow\scriptstyle{d\phi_0}&&\downarrow\scriptstyle{d\phi_0 \times d\phi_0}\\ \mathbf R^m\;\;&\stackrel{dh_0}{\longrightarrow}&\mathbf R^m \times \mathbf R^m \end{array} $$ According to the definition (or the commuting diagram above), $df_x=(d\phi_0 \times d\phi_0) \circ dh_0 \circ d\phi_0$. However I have no idea how to proceed after that. If I want to calculate $df_x$, I have to know what $d\phi_0$ is first... But since we let $\phi(0)=x$, what would the derivative map of that be (since $\phi(0)=x$ just means we send a specific point $0$ to a specific point $x$, it doesn't tell us anything about the expression of this parameterisation)? Thanks everyone for the help!","I was trying to work through a problem(#10 of $\S$1.2) in Guillemin and Pollack's book $\textit{differential topology. }$ The problem is given as follows. Let $f: X\longrightarrow  X\times X$ be the mapping $f(x)=(x,x)$.   Check that $df_x(v)=(v,v)$. Here $X\subset \mathbf R^m$ is a manifold. My attempt so far has been: First we parametrise an open neighbourhood of $x\in X$ and $(x,x) \in X\times X$ locally by $\phi$ and $\phi \times \phi$ into open subsets $U\subset\mathbf R^m$ and $U\times U$ (we use $\phi(0)=x$ for simplicity). This gives the commuting diagram as follows: $$ \begin{array}[c]{ccc} X\;\;&\stackrel{f}{\longrightarrow}&X\times X\\ \downarrow\scriptstyle{\phi}&&\downarrow\scriptstyle{\phi \times \phi}\\ U\;\;&\stackrel{h}{\longrightarrow}&U\times U \end{array} $$ $$ \begin{array}[c]{ccc} T_x(X)&\stackrel{df_x}{\longrightarrow}&T_{(x,x)}(X\times X)\\ \downarrow\scriptstyle{d\phi_0}&&\downarrow\scriptstyle{d\phi_0 \times d\phi_0}\\ \mathbf R^m\;\;&\stackrel{dh_0}{\longrightarrow}&\mathbf R^m \times \mathbf R^m \end{array} $$ According to the definition (or the commuting diagram above), $df_x=(d\phi_0 \times d\phi_0) \circ dh_0 \circ d\phi_0$. However I have no idea how to proceed after that. If I want to calculate $df_x$, I have to know what $d\phi_0$ is first... But since we let $\phi(0)=x$, what would the derivative map of that be (since $\phi(0)=x$ just means we send a specific point $0$ to a specific point $x$, it doesn't tell us anything about the expression of this parameterisation)? Thanks everyone for the help!",,"['real-analysis', 'general-topology', 'differential-geometry', 'manifolds', 'differential-topology']"
22,$a_{n+f(n)}-a_{n}\rightarrow0$ implies convergence?,implies convergence?,a_{n+f(n)}-a_{n}\rightarrow0,$(a_{n})$ is a sequence of reals. Say $a_{n+f(n)}-a_{n}$ tends to 0 as n tends to infinity for every function f from the positive integers to the positive integers. Does this imply that $a_{n}$ is convergent?,$(a_{n})$ is a sequence of reals. Say $a_{n+f(n)}-a_{n}$ tends to 0 as n tends to infinity for every function f from the positive integers to the positive integers. Does this imply that $a_{n}$ is convergent?,,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
23,"A classic example in measure theory, hold the measure theory","A classic example in measure theory, hold the measure theory",,"I answered a question recently drawing on the following construction. Let $\{r_n\}$ be a enumeration of the rationals and set $$E:=\bigcup_{n=1}^\infty B(r_n,2^{-n}).$$ Then  $$m(E) \leq \sum_{n=1}^\infty m\big(B(r_n,2^{-n})\big)=2$$ and in particular $\mathbb R \setminus E$ is uncountable. I realized when answering this question that I didn't know how to prove that $\mathbb R \setminus E$ is uncountable without measure theory. It seems to me a nice example in point set topology, I recall a period of time when I thought that surely any dense open subset of $\mathbb R$ was only missing a countable number of points (I hadn't read about the Cantor set either at that time). So I was wondering if there was a way to approach this that a student in a typical first class in analysis or point-set topology would understand.","I answered a question recently drawing on the following construction. Let $\{r_n\}$ be a enumeration of the rationals and set $$E:=\bigcup_{n=1}^\infty B(r_n,2^{-n}).$$ Then  $$m(E) \leq \sum_{n=1}^\infty m\big(B(r_n,2^{-n})\big)=2$$ and in particular $\mathbb R \setminus E$ is uncountable. I realized when answering this question that I didn't know how to prove that $\mathbb R \setminus E$ is uncountable without measure theory. It seems to me a nice example in point set topology, I recall a period of time when I thought that surely any dense open subset of $\mathbb R$ was only missing a countable number of points (I hadn't read about the Cantor set either at that time). So I was wondering if there was a way to approach this that a student in a typical first class in analysis or point-set topology would understand.",,"['real-analysis', 'general-topology', 'measure-theory']"
24,"Prove that $x^\alpha \cdot\sin(1/x)$ is absolutely continuous on $(0,1)$",Prove that  is absolutely continuous on,"x^\alpha \cdot\sin(1/x) (0,1)","I would appreciate any help with my HW exercise: Prove that $f(x) = x^\alpha \cdot \sin(1/x)$ is absolutely continuous on $(0,1)$, when $\alpha>1$. It's easy to find the derivative of $f$: $$f'(x) = \alpha x^{\alpha-1} \sin(1/x) - x^{\alpha-2} \cos(1/x).$$ So, when $1<\alpha<2$, the function is not Lipschitz, and that is the main problem. I searched for similar questions and found this: Examples of absolutely continuous functions that are not Lipschitz. and this: http://mathdl.maa.org/images/cms_upload/0002989049585.di021349.02p00072.pdf But I couldn't understand the PDF file, which merely handles the case $\alpha=3/2$. Thanks!","I would appreciate any help with my HW exercise: Prove that $f(x) = x^\alpha \cdot \sin(1/x)$ is absolutely continuous on $(0,1)$, when $\alpha>1$. It's easy to find the derivative of $f$: $$f'(x) = \alpha x^{\alpha-1} \sin(1/x) - x^{\alpha-2} \cos(1/x).$$ So, when $1<\alpha<2$, the function is not Lipschitz, and that is the main problem. I searched for similar questions and found this: Examples of absolutely continuous functions that are not Lipschitz. and this: http://mathdl.maa.org/images/cms_upload/0002989049585.di021349.02p00072.pdf But I couldn't understand the PDF file, which merely handles the case $\alpha=3/2$. Thanks!",,"['real-analysis', 'measure-theory']"
25,A problem about convergence in finite measure space,A problem about convergence in finite measure space,,"Suppose we have a measure space $(X,M,\mu)$, where $\mu(X)<+\infty$. Let ${f_n}$ be a sequence of non-negative functions, $f_n\in L_1,\ \forall n$ and $f_n$ converges to some $f$ pointwise. $f$ is not necessarily in $L_1$. Suppose now $\underset{X}{\int} f_nd\mu$ also converges to $\underset{X}{\int}fd\mu$, as $n\rightarrow \infty$. Is it true that $\underset{E}{\int} f_nd\mu\rightarrow\underset{E}{\int}fd\mu,\ \forall E\in M$? If not, can you give a counter-example? I've encountered a similar problem, where I have the condition $f\in L_1(X,M,\mu)$ but don't have the condition $\mu(X)<+\infty$. In that case it is pretty easy to prove this problem. But here I don't have this condition, and $\underset{X}{\int}fd\mu = \infty$ can indeed happen, so how can I prove the problem now? I tried using Egoroff's theorem, but I failed to get it to work. Any suggestions? Thanks!","Suppose we have a measure space $(X,M,\mu)$, where $\mu(X)<+\infty$. Let ${f_n}$ be a sequence of non-negative functions, $f_n\in L_1,\ \forall n$ and $f_n$ converges to some $f$ pointwise. $f$ is not necessarily in $L_1$. Suppose now $\underset{X}{\int} f_nd\mu$ also converges to $\underset{X}{\int}fd\mu$, as $n\rightarrow \infty$. Is it true that $\underset{E}{\int} f_nd\mu\rightarrow\underset{E}{\int}fd\mu,\ \forall E\in M$? If not, can you give a counter-example? I've encountered a similar problem, where I have the condition $f\in L_1(X,M,\mu)$ but don't have the condition $\mu(X)<+\infty$. In that case it is pretty easy to prove this problem. But here I don't have this condition, and $\underset{X}{\int}fd\mu = \infty$ can indeed happen, so how can I prove the problem now? I tried using Egoroff's theorem, but I failed to get it to work. Any suggestions? Thanks!",,['real-analysis']
26,Continuous partials at a point but not differentiable there?,Continuous partials at a point but not differentiable there?,,"In Question on differentiability at a point , it is mentioned (and in Equivalent condition for differentiability on partial derivatives it is cited from Apostol) that for $f:\mathbb{R}^2\to\mathbb{R}$ to be differentiable at a point, one sufficient condition is that both partials exist at the point, and one of the partials is continuous throughout a neighborhood of the point. I have seen counterexamples (e.g. the one given on Wikipedia ) of functions which have partials defined everywhere, but fail to be differentiable at some point. But the examples I've seen aren't satisfying because both partials aren't continuous at the point in question (in fact, the partials fail to have a limit at that point at all), let alone in a neighborhood of the point. Can someone direct me to a function that shows why the ""throughout a neighborhood"" condition is required for the sufficient condition mentioned in Apostol? I'm having trouble imagining a function where a partial is continuous at the point, but not throughout a neighborhood of the point. One thought I had was to use something whose cross section looked like Volterra's function (centered at a point not in the fat Cantor set), but I'm not certain that this would work, and even if it does, I wonder if there's a nicer example.","In Question on differentiability at a point , it is mentioned (and in Equivalent condition for differentiability on partial derivatives it is cited from Apostol) that for $f:\mathbb{R}^2\to\mathbb{R}$ to be differentiable at a point, one sufficient condition is that both partials exist at the point, and one of the partials is continuous throughout a neighborhood of the point. I have seen counterexamples (e.g. the one given on Wikipedia ) of functions which have partials defined everywhere, but fail to be differentiable at some point. But the examples I've seen aren't satisfying because both partials aren't continuous at the point in question (in fact, the partials fail to have a limit at that point at all), let alone in a neighborhood of the point. Can someone direct me to a function that shows why the ""throughout a neighborhood"" condition is required for the sufficient condition mentioned in Apostol? I'm having trouble imagining a function where a partial is continuous at the point, but not throughout a neighborhood of the point. One thought I had was to use something whose cross section looked like Volterra's function (centered at a point not in the fat Cantor set), but I'm not certain that this would work, and even if it does, I wonder if there's a nicer example.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'examples-counterexamples']"
27,Lemma about the integral of a function with compact support,Lemma about the integral of a function with compact support,,"Lemma 16.4 (p. 140) of Munkres' Analysis on Manifolds says: Let $A$ be open in $\mathbb{R}^n$; let $f: A \rightarrow \mathbb{R}$ be continuous. If $f$ vanishes outside a compact subset $C$ of $A$, then the integrals $\int_A f$ and $\int_C f$ exist and are equal. The first step in his proof is saying that the integral $\int_C f$ exists because $C$ is bounded and $f$ is continuous and bounded on all of $\mathbb{R}^n$. But don't you need $C$ to be rectifiable (i.e. bounded and boundary has measure $0$) for integrability? The fat cantor set is compact but not rectifiable, so the integral over it won't exist.","Lemma 16.4 (p. 140) of Munkres' Analysis on Manifolds says: Let $A$ be open in $\mathbb{R}^n$; let $f: A \rightarrow \mathbb{R}$ be continuous. If $f$ vanishes outside a compact subset $C$ of $A$, then the integrals $\int_A f$ and $\int_C f$ exist and are equal. The first step in his proof is saying that the integral $\int_C f$ exists because $C$ is bounded and $f$ is continuous and bounded on all of $\mathbb{R}^n$. But don't you need $C$ to be rectifiable (i.e. bounded and boundary has measure $0$) for integrability? The fat cantor set is compact but not rectifiable, so the integral over it won't exist.",,"['real-analysis', 'integration']"
28,"If $p$ is an element of $\overline E$ but not a limit point of $E$, then why is there a $p' \in E$ such that $d(p, p') < \varepsilon$?","If  is an element of  but not a limit point of , then why is there a  such that ?","p \overline E E p' \in E d(p, p') < \varepsilon","I don't understand one of the steps of the proof of Theorem 3.10(a) in Baby Rudin. Here's the theorem and the proof up to where I'm stuck: Relevant Definitions The closure of the subset $E$ of some metric space is the union of $E$ with the set of all its limit points. The diameter of the subset $E$ of some metric space is the supremum of the set of all pairwise distances between its elements. For the points $x$ and $y$ in some metric space, $d(x, y)$ denotes the distance between $x$ and $y$. Theorem 3.10(a) If $\overline{E}$ is the closure of a set $E$ in a metric space $X$, then $$ \text{diam} \ \overline{E} = \text{diam} \ E. $$ Proof. Since $E \subseteq \overline{E}$, we have $$\begin{equation*} \text{diam} \ E \leq \text{diam} \ \overline{E}. \end{equation*}$$ Let $\epsilon > 0$, and pick $p, q \in \overline{E}$. By the definition of $\overline{E}$, there are points $p', q' \in E$ such that $d(p,p') < \epsilon$ and $d(q, q') < \epsilon$... I see that this works if $p$ and $q$ are limit points of $E$. But how does this work if, say, $p$ isn't a limit point of $E$? What if $E$ is some region in $\mathbb{R}^2$ along with the point $p$ by itself way off somewhere?","I don't understand one of the steps of the proof of Theorem 3.10(a) in Baby Rudin. Here's the theorem and the proof up to where I'm stuck: Relevant Definitions The closure of the subset $E$ of some metric space is the union of $E$ with the set of all its limit points. The diameter of the subset $E$ of some metric space is the supremum of the set of all pairwise distances between its elements. For the points $x$ and $y$ in some metric space, $d(x, y)$ denotes the distance between $x$ and $y$. Theorem 3.10(a) If $\overline{E}$ is the closure of a set $E$ in a metric space $X$, then $$ \text{diam} \ \overline{E} = \text{diam} \ E. $$ Proof. Since $E \subseteq \overline{E}$, we have $$\begin{equation*} \text{diam} \ E \leq \text{diam} \ \overline{E}. \end{equation*}$$ Let $\epsilon > 0$, and pick $p, q \in \overline{E}$. By the definition of $\overline{E}$, there are points $p', q' \in E$ such that $d(p,p') < \epsilon$ and $d(q, q') < \epsilon$... I see that this works if $p$ and $q$ are limit points of $E$. But how does this work if, say, $p$ isn't a limit point of $E$? What if $E$ is some region in $\mathbb{R}^2$ along with the point $p$ by itself way off somewhere?",,"['real-analysis', 'general-topology']"
29,Sufficient and necessary condition for continuously differentiable,Sufficient and necessary condition for continuously differentiable,,"I was trying to prove the following statement: Let $U\subset\mathbb{R}^m$ be open and $f:U\to\mathbb{R}^n$. Show that $f$ is continuously differentiable if, and only if, for each $x\in U$, there exists a linear operator $A(x):\mathbb{R}^m\to\mathbb{R}^n$ such that   $$ \lim\frac{f(x+h)-f(x+k)-A(x)(h-k)}{\|h-k\|}=0 $$   whenever $(h,k)\to(0,0)$, with $h\ne k$. If $f$ is continuously differentiable, then, for each $x\in U$, defining $g_x:=f-f'(x)$ (then $g_x'(y)=f'(y)-f'(x)$) and applying the Mean Value Theorem, using the continuity for $f'$, and for adequate closed ball $B=\bar B(x;\delta)\subset U$ such that $\|f'(a)-f'(b)\|<\epsilon/2$ if $a,b\in B(x;2\delta)\subset U$, and for any $h,k\in B,h\ne k$, we have $$ \frac{\|f(x+k+h-k)-f(x+k)-f'(x)(h-k)\|}{\|h-k\|}=\frac{\|g_x(x+h)-g_x(x+k)\|}{\|h-k\|}\\\le\sup\limits_{h,k\in B}\|f'(x+k)-f'(x+h)\|<\epsilon $$ so, the equality holds, where $A(x):=f'(x)$, for each $x\in U$. Reciprocally, fixing $k=0$, we see that $f$ is differentiable in $U$ and $f'(x)=A(x),\forall x\in U$ (and hence continuous). But I couldn't prove that $f'$ is continuous... Any hint? Thanks!","I was trying to prove the following statement: Let $U\subset\mathbb{R}^m$ be open and $f:U\to\mathbb{R}^n$. Show that $f$ is continuously differentiable if, and only if, for each $x\in U$, there exists a linear operator $A(x):\mathbb{R}^m\to\mathbb{R}^n$ such that   $$ \lim\frac{f(x+h)-f(x+k)-A(x)(h-k)}{\|h-k\|}=0 $$   whenever $(h,k)\to(0,0)$, with $h\ne k$. If $f$ is continuously differentiable, then, for each $x\in U$, defining $g_x:=f-f'(x)$ (then $g_x'(y)=f'(y)-f'(x)$) and applying the Mean Value Theorem, using the continuity for $f'$, and for adequate closed ball $B=\bar B(x;\delta)\subset U$ such that $\|f'(a)-f'(b)\|<\epsilon/2$ if $a,b\in B(x;2\delta)\subset U$, and for any $h,k\in B,h\ne k$, we have $$ \frac{\|f(x+k+h-k)-f(x+k)-f'(x)(h-k)\|}{\|h-k\|}=\frac{\|g_x(x+h)-g_x(x+k)\|}{\|h-k\|}\\\le\sup\limits_{h,k\in B}\|f'(x+k)-f'(x+h)\|<\epsilon $$ so, the equality holds, where $A(x):=f'(x)$, for each $x\in U$. Reciprocally, fixing $k=0$, we see that $f$ is differentiable in $U$ and $f'(x)=A(x),\forall x\in U$ (and hence continuous). But I couldn't prove that $f'$ is continuous... Any hint? Thanks!",,['real-analysis']
30,Properties of subharmonic functions,Properties of subharmonic functions,,"A function $f$ is called subharmonic if $f:U\rightarrow\mathbb R$ (with $U\subset\mathbb R^n$) is upper semi-continuous and $$\forall\space \mathbb B_r(x)\subset U:f(x)\le\frac{1}{n\alpha(n)r^{n-1}}\int\limits_{\partial\mathbb B_r(x)}f(y)\mathrm \space dS(y)$$ Now I read some stuff about its porperties and I found these ones: $$\begin{align*} (i)\space& f \space\space\text{subharmonic}\\ (ii)\space&\Longleftrightarrow\forall\space\text{open}\space D\subset U,\text{harmonic}\space u\in C^0(\overline D)\cap C^2(D)\space\text{with}\space f\leq u \space\text{on}\space\partial D: f\leq u\space\text{in}\space D\\ (iii)\space&\Longleftrightarrow\forall\phi\in C_c^\infty(U):\int_U f\cdot\Delta \phi\geq0\\ (iv)\space&\Longleftrightarrow\Delta f\geq0 \end{align*}$$ Now I tried to prove these equivalences but I only have the first and fourth licked and have no idea how to prove the second and third one. Why are they equivalent? Thanks a lot! Edit: added a numeration","A function $f$ is called subharmonic if $f:U\rightarrow\mathbb R$ (with $U\subset\mathbb R^n$) is upper semi-continuous and $$\forall\space \mathbb B_r(x)\subset U:f(x)\le\frac{1}{n\alpha(n)r^{n-1}}\int\limits_{\partial\mathbb B_r(x)}f(y)\mathrm \space dS(y)$$ Now I read some stuff about its porperties and I found these ones: $$\begin{align*} (i)\space& f \space\space\text{subharmonic}\\ (ii)\space&\Longleftrightarrow\forall\space\text{open}\space D\subset U,\text{harmonic}\space u\in C^0(\overline D)\cap C^2(D)\space\text{with}\space f\leq u \space\text{on}\space\partial D: f\leq u\space\text{in}\space D\\ (iii)\space&\Longleftrightarrow\forall\phi\in C_c^\infty(U):\int_U f\cdot\Delta \phi\geq0\\ (iv)\space&\Longleftrightarrow\Delta f\geq0 \end{align*}$$ Now I tried to prove these equivalences but I only have the first and fourth licked and have no idea how to prove the second and third one. Why are they equivalent? Thanks a lot! Edit: added a numeration",,"['real-analysis', 'analysis', 'partial-differential-equations', 'harmonic-analysis']"
31,Maps that preserve measure zero property,Maps that preserve measure zero property,,"Consider a map $f: \mathbb{R}^n \to \mathbb{R}^m$ that is differentiable (usually even smooth). If $B \subset \mathbb{R}^m$ has measure zero (Lebesgue measure), then what types of maps $f$ satisfy $A = f^{-1}(B)$ also has measure zero? To provide some context: I have a property $\mathcal{P}$ that holds almost everywhere in $\mathbb{R}^m$; now I want to characterize the class of maps $f$ such that $\mathcal{P}(f(\cdot))$ holds almost everywhere in $\mathbb{R}^n$","Consider a map $f: \mathbb{R}^n \to \mathbb{R}^m$ that is differentiable (usually even smooth). If $B \subset \mathbb{R}^m$ has measure zero (Lebesgue measure), then what types of maps $f$ satisfy $A = f^{-1}(B)$ also has measure zero? To provide some context: I have a property $\mathcal{P}$ that holds almost everywhere in $\mathbb{R}^m$; now I want to characterize the class of maps $f$ such that $\mathcal{P}(f(\cdot))$ holds almost everywhere in $\mathbb{R}^n$",,"['real-analysis', 'measure-theory']"
32,Proving the convergence of a series,Proving the convergence of a series,,"I'm trying to solve this exercise: Let $(a_n)_{n\in\mathbb{N}}\subseteq\mathbb{R}$ and $p,q\in\mathbb{R}$ such that $p<q$. Prove that if the series $\sum \frac{a_n}{n^p}$ converges, then the series $\sum \frac{a_n}{n^q}$ also does. The comparison test sounds useful, but since we don't know that $a_n$ is positive it doesn't apply. Any pointers?","I'm trying to solve this exercise: Let $(a_n)_{n\in\mathbb{N}}\subseteq\mathbb{R}$ and $p,q\in\mathbb{R}$ such that $p<q$. Prove that if the series $\sum \frac{a_n}{n^p}$ converges, then the series $\sum \frac{a_n}{n^q}$ also does. The comparison test sounds useful, but since we don't know that $a_n$ is positive it doesn't apply. Any pointers?",,"['real-analysis', 'sequences-and-series']"
33,Arbitrary Union and Intersection of Closed and Open Sets,Arbitrary Union and Intersection of Closed and Open Sets,,"I have four quick questions and have listed them below. I am seeking for corroboration of the first three and a bit of insight on the fourth, as I have hit a solid brick wall. Definition. A set $A\subseteq\mathbb{R}$ is called an $F_\sigma$ set if it can be written as the countable union of closed sets. A set $B\subseteq\mathbb{R}$ is called a $G_\delta$ set if it can be written as the countable intersection of open sets. 1. Argue that a set $A$ is a $G_\delta$ set if and only if its complement is an $F_\sigma$ set. Proof. Let $A$ be a $G_\delta$ set. Then$$A=\bigcap_{n=1}K_n,$$where each $K_n$ is an open set. By De Morgan's law, it follows that$$A^c=\left(\bigcap_{n=1}K_n\right)^c=\bigcup_{n=1}K_n^c,$$which is an $F_\sigma$ set, since it is a countable union of closed sets. The converse statement can be proven in a similar fashion. $\square$ 2. Show that a closed interval $[a,b]$ is a $G_\delta$ set. Proof. Take$$\bigcap_{n=1}^\infty\left(a-\frac{1}{n},b+\frac{1}{n}\right)=[a,b].$$Therefore, $[a,b]$ is a $G_\delta$ set. $\square$ 3. Show that the half-open interval $(a,b]$ is both a $G_\delta$ and an $F_\sigma$ set. Proof. Take$$\bigcap_{n=1}^\infty\left(a,b+\frac{1}{n}\right)=(a,b],$$and$$\bigcup_{n=4}^\infty\left[a-\frac{a-b}{n},b\right]=(a,b].$$Therefore, $(a,b]$ is both a $G_\delta$ and an $F_\sigma$ set. $\square$ 4. Show that $\mathbb{Q}$ is an $F_\sigma$ set, and the set of irrationals $\mathbb{I}$ forms a $G_\delta$ set. I am oblivious as to how to tackle this problem. I know that I need to show that $\mathbb{Q}$ can be written as the countable union of closed sets, and that upon doing this, I can apply De Morgan's law to it to prove the second statement. However, I do not know how to begin. Do you guys have any ideas? Thanks in advance! Edit 1: I followed Robert's advice, and this is what I managed to weave: Proof. Since $\mathbb{Q}$ is countable, we can write it as $\mathbb{Q}=\{r_1,r_2,r_3,\cdots\}$, where each $r_n$ is a unique rational number. It follows that we can express $\mathbb{Q}$ as the union of singletons $[r_n]$, which are closed sets, as follows:$$\mathbb{Q}=\bigcup_{n=1}^\infty[r_n].$$Therefore, $\mathbb{Q}$ is an $F_\sigma$ set, since it is a countable union of closed sets. Furthermore, it naturally follows that$$\mathbb{I}=\mathbb{Q}^c=\left(\bigcup_{n=1}^\infty[r_n]\right)^c=\bigcap_{n=1}^\infty[r_n]^c=\bigcap_{n=1}^\infty(-\infty,r_n)\cup(r_n,\infty)$$is a $G_\delta$ set, since it is a countable intersection of open sets. $\square$ Is this sound?","I have four quick questions and have listed them below. I am seeking for corroboration of the first three and a bit of insight on the fourth, as I have hit a solid brick wall. Definition. A set $A\subseteq\mathbb{R}$ is called an $F_\sigma$ set if it can be written as the countable union of closed sets. A set $B\subseteq\mathbb{R}$ is called a $G_\delta$ set if it can be written as the countable intersection of open sets. 1. Argue that a set $A$ is a $G_\delta$ set if and only if its complement is an $F_\sigma$ set. Proof. Let $A$ be a $G_\delta$ set. Then$$A=\bigcap_{n=1}K_n,$$where each $K_n$ is an open set. By De Morgan's law, it follows that$$A^c=\left(\bigcap_{n=1}K_n\right)^c=\bigcup_{n=1}K_n^c,$$which is an $F_\sigma$ set, since it is a countable union of closed sets. The converse statement can be proven in a similar fashion. $\square$ 2. Show that a closed interval $[a,b]$ is a $G_\delta$ set. Proof. Take$$\bigcap_{n=1}^\infty\left(a-\frac{1}{n},b+\frac{1}{n}\right)=[a,b].$$Therefore, $[a,b]$ is a $G_\delta$ set. $\square$ 3. Show that the half-open interval $(a,b]$ is both a $G_\delta$ and an $F_\sigma$ set. Proof. Take$$\bigcap_{n=1}^\infty\left(a,b+\frac{1}{n}\right)=(a,b],$$and$$\bigcup_{n=4}^\infty\left[a-\frac{a-b}{n},b\right]=(a,b].$$Therefore, $(a,b]$ is both a $G_\delta$ and an $F_\sigma$ set. $\square$ 4. Show that $\mathbb{Q}$ is an $F_\sigma$ set, and the set of irrationals $\mathbb{I}$ forms a $G_\delta$ set. I am oblivious as to how to tackle this problem. I know that I need to show that $\mathbb{Q}$ can be written as the countable union of closed sets, and that upon doing this, I can apply De Morgan's law to it to prove the second statement. However, I do not know how to begin. Do you guys have any ideas? Thanks in advance! Edit 1: I followed Robert's advice, and this is what I managed to weave: Proof. Since $\mathbb{Q}$ is countable, we can write it as $\mathbb{Q}=\{r_1,r_2,r_3,\cdots\}$, where each $r_n$ is a unique rational number. It follows that we can express $\mathbb{Q}$ as the union of singletons $[r_n]$, which are closed sets, as follows:$$\mathbb{Q}=\bigcup_{n=1}^\infty[r_n].$$Therefore, $\mathbb{Q}$ is an $F_\sigma$ set, since it is a countable union of closed sets. Furthermore, it naturally follows that$$\mathbb{I}=\mathbb{Q}^c=\left(\bigcup_{n=1}^\infty[r_n]\right)^c=\bigcap_{n=1}^\infty[r_n]^c=\bigcap_{n=1}^\infty(-\infty,r_n)\cup(r_n,\infty)$$is a $G_\delta$ set, since it is a countable intersection of open sets. $\square$ Is this sound?",,['real-analysis']
34,"If $f: \mathbb R^n \to \mathbb R^n$ is a contraction, then $x-f(x)$ is a homeomorphism","If  is a contraction, then  is a homeomorphism",f: \mathbb R^n \to \mathbb R^n x-f(x),"I am stuck in following homework question. Let $f : \mathbb R^n \to \mathbb R^n$ be a uniform contraction and $g(x) = x - f(x)$. Investigate whether $g : \mathbb R^n \to \mathbb R^n$ is a homeomorphism or not. The definition of uniform contraction is as follows: $(X,d)$ is metric space. $f: X \to X$ is uniform contraction if there exists $0 <\alpha <1$ such that $d(f(x),f(y)) \leq \alpha d(x,y)$. I proved that $f$ and $g$ are continuous. But I do not have any idea about inverse of $g$. By invariance of domain, one could conclude that a bijective continuous function $h: \mathbb R^n \to \mathbb R^n$ is homeomorphism. But I do not know how I can see $g$ is bijective or not. I do not have any other idea to prove homeomorphism. Maybe it is not a homeomorphism but I can't think of any counterexample. Thank you in advance.","I am stuck in following homework question. Let $f : \mathbb R^n \to \mathbb R^n$ be a uniform contraction and $g(x) = x - f(x)$. Investigate whether $g : \mathbb R^n \to \mathbb R^n$ is a homeomorphism or not. The definition of uniform contraction is as follows: $(X,d)$ is metric space. $f: X \to X$ is uniform contraction if there exists $0 <\alpha <1$ such that $d(f(x),f(y)) \leq \alpha d(x,y)$. I proved that $f$ and $g$ are continuous. But I do not have any idea about inverse of $g$. By invariance of domain, one could conclude that a bijective continuous function $h: \mathbb R^n \to \mathbb R^n$ is homeomorphism. But I do not know how I can see $g$ is bijective or not. I do not have any other idea to prove homeomorphism. Maybe it is not a homeomorphism but I can't think of any counterexample. Thank you in advance.",,"['real-analysis', 'fixed-point-theorems']"
35,Separation of two points with null-sets,Separation of two points with null-sets,,"I am asked the following question: (I write $\{a,b\}$ for points in $\mathbb{R}^2$) Let $\{0,0\},\{1,1\} \notin K \subset [0,1]^2$ such that the projections of $K$ onto the $x$-axis and the $y$-axis are (1 dimensional) lebesgue null-sets. Is there a curve $\gamma : [0,1] \longrightarrow [0,1]^2\backslash K$ such that $\gamma(0)=\{0,0\}, \gamma(1)=\{1,1\}$ and $\ell(\gamma)\leq2$? My idea was to consider a family of disjunct curves to do a dimension argument, but I stumbled across the cantor-set and therefore saw that $K$ doesn't have to be countable, might there even be a counter example?","I am asked the following question: (I write $\{a,b\}$ for points in $\mathbb{R}^2$) Let $\{0,0\},\{1,1\} \notin K \subset [0,1]^2$ such that the projections of $K$ onto the $x$-axis and the $y$-axis are (1 dimensional) lebesgue null-sets. Is there a curve $\gamma : [0,1] \longrightarrow [0,1]^2\backslash K$ such that $\gamma(0)=\{0,0\}, \gamma(1)=\{1,1\}$ and $\ell(\gamma)\leq2$? My idea was to consider a family of disjunct curves to do a dimension argument, but I stumbled across the cantor-set and therefore saw that $K$ doesn't have to be countable, might there even be a counter example?",,"['real-analysis', 'measure-theory']"
36,Approximating $\log x$ by a sum of power functions $a x^b$,Approximating  by a sum of power functions,\log x a x^b,"Let's approximate $\log x$ on the interval $(0,1)$ by a power function $a x^b$ to minimize the integral of the squared difference $$\delta_0(a,b)=\int_0^1\left(\log x-a x^b\right)^2dx.\tag1$$ It's easy to verify that the minimum is attained at $a_0=-\frac34,\,b_0=-\frac13$ that gives the approximation $$\log x=-\tfrac34x^{-1/3}+\mathcal R(x),\tag2$$ where $\mathcal R(x)$ is the error term. Now, let's again approximate $\mathcal R(x)$ by a power function $a x^b$ to minimize $$\delta_1(a,b)=\int_0^1\left(\mathcal R(x)-a x^b\right)^2dx=\int_0^1\left(\log x-\left(-\tfrac34x^{-1/3}+a x^b\right)\right)^2dx.\tag3$$ The minimum is attained at $$\begin{align}a_1&=\frac{17}4-\sqrt{58} \sin \left[\frac13 \arctan \left(\frac{433}{33\sqrt7}\right)\right]\approx0.88760008404...,\\b_1&=\frac{1}{3}+\frac{4}{3} \sqrt{2} \cos \left[\frac{1}{3} \arctan\left(\frac{\sqrt{7}}{11}\right)\right]\approx2.21311796239...,\end{align}\tag4$$ which are algebraic numbers of degree $3^\dagger$ . If we repeat this process once again, we will get the next term $a_2x^{b_2}$ , where $$a_2\approx-0.1406322691...,\, b_2\approx-0.2430593194...\tag5$$ are algebraic numbers of degree $15^\ddagger$ , for which I don't know any closed form. The following steps will similarly produce pairs of algebraic numbers of higher degrees, resulting in an approximation of $\log x$ on the interval $(0,1)$ by a generalized power series $$\log x\approx-\tfrac34x^{-1/3}+a_1x^{b_1}+a_2x^{b_2}+\dots,\tag6$$ where each next term causes the integral of the squared error term to progressively decrease. The powers $b_n$ and coefficients $a_n$ are not monotone and do not exhibit any clear pattern (although the coefficients generally tend to decrease in absolute value, with some sporadic spikes). Question: What does the series $(6)$ converge to? Does it converge to $\log x$ on any interval? If it does converge to $\log x$ , then empirically the convergence appears to be quite slow and erratic. ${^\dagger}$ The corresponding minimal polynomials are $\small64 z^3-816 z^2+684 z-9$ and $\small9 z^3-9 z^2-21 z-7.$ ${^\ddagger}$ The corresponding minimal polynomials are $\small5035261952 z^{15}+180729937920 z^{14}+19190513664 z^{13}-60948402536448    z^{12}-383744783499264 z^{11}+6281308897579008 z^{10}+50474690060451840    z^9-155303784466089984 z^8-1906255797863421024 z^7+805421030545306296    z^6+670389754270702752 z^5+127003127714790264 z^4+8514399973766202    z^3+130643635592430 z^2-127629387774 z-79827687$ and $\small118098 z^{15}-1299078 z^{14}-15628302 z^{13}-52936335 z^{12}-55068660    z^{11}+119832291 z^{10}+512627130 z^9+898647291 z^8+984822786 z^7+742152591    z^6+396538632 z^5+150470676 z^4+39697272 z^3+6920496 z^2+715716 z+33172.$ Although the polynomials look scary, they are quite nice in some sense, e.g. $\small5035261952=2^{21}\cdot7^4$ and $\small79827687=3^8\cdot23^3,$ and they also can be factored into quintics over some $\mathbb Q[q]$ with $q$ expressible in radicals.","Let's approximate on the interval by a power function to minimize the integral of the squared difference It's easy to verify that the minimum is attained at that gives the approximation where is the error term. Now, let's again approximate by a power function to minimize The minimum is attained at which are algebraic numbers of degree . If we repeat this process once again, we will get the next term , where are algebraic numbers of degree , for which I don't know any closed form. The following steps will similarly produce pairs of algebraic numbers of higher degrees, resulting in an approximation of on the interval by a generalized power series where each next term causes the integral of the squared error term to progressively decrease. The powers and coefficients are not monotone and do not exhibit any clear pattern (although the coefficients generally tend to decrease in absolute value, with some sporadic spikes). Question: What does the series converge to? Does it converge to on any interval? If it does converge to , then empirically the convergence appears to be quite slow and erratic. The corresponding minimal polynomials are and The corresponding minimal polynomials are and Although the polynomials look scary, they are quite nice in some sense, e.g. and and they also can be factored into quintics over some with expressible in radicals.","\log x (0,1) a x^b \delta_0(a,b)=\int_0^1\left(\log x-a x^b\right)^2dx.\tag1 a_0=-\frac34,\,b_0=-\frac13 \log x=-\tfrac34x^{-1/3}+\mathcal R(x),\tag2 \mathcal R(x) \mathcal R(x) a x^b \delta_1(a,b)=\int_0^1\left(\mathcal R(x)-a x^b\right)^2dx=\int_0^1\left(\log x-\left(-\tfrac34x^{-1/3}+a x^b\right)\right)^2dx.\tag3 \begin{align}a_1&=\frac{17}4-\sqrt{58} \sin \left[\frac13 \arctan \left(\frac{433}{33\sqrt7}\right)\right]\approx0.88760008404...,\\b_1&=\frac{1}{3}+\frac{4}{3} \sqrt{2} \cos \left[\frac{1}{3} \arctan\left(\frac{\sqrt{7}}{11}\right)\right]\approx2.21311796239...,\end{align}\tag4 3^\dagger a_2x^{b_2} a_2\approx-0.1406322691...,\, b_2\approx-0.2430593194...\tag5 15^\ddagger \log x (0,1) \log x\approx-\tfrac34x^{-1/3}+a_1x^{b_1}+a_2x^{b_2}+\dots,\tag6 b_n a_n (6) \log x \log x {^\dagger} \small64 z^3-816 z^2+684 z-9 \small9 z^3-9 z^2-21 z-7. {^\ddagger} \small5035261952 z^{15}+180729937920 z^{14}+19190513664 z^{13}-60948402536448
   z^{12}-383744783499264 z^{11}+6281308897579008 z^{10}+50474690060451840
   z^9-155303784466089984 z^8-1906255797863421024 z^7+805421030545306296
   z^6+670389754270702752 z^5+127003127714790264 z^4+8514399973766202
   z^3+130643635592430 z^2-127629387774 z-79827687 \small118098 z^{15}-1299078 z^{14}-15628302 z^{13}-52936335 z^{12}-55068660
   z^{11}+119832291 z^{10}+512627130 z^9+898647291 z^8+984822786 z^7+742152591
   z^6+396538632 z^5+150470676 z^4+39697272 z^3+6920496 z^2+715716 z+33172. \small5035261952=2^{21}\cdot7^4 \small79827687=3^8\cdot23^3, \mathbb Q[q] q","['real-analysis', 'calculus', 'sequences-and-series', 'logarithms', 'approximation']"
37,Calculating Mehta integral using induction.,Calculating Mehta integral using induction.,,"I was wondering if there is a way to compute the Mehta integral for real numbers by using induction or some nice reverse engineering with Laplace transform, or if this is vain. Here $\gamma$ is real $$ \frac{1}{(2 \pi)^{n / 2}} \int\limits_{-\infty}^{+\infty} \cdots \int\limits_{-\infty}^{+\infty} \prod_{i=1}^n e^{-t_i^2 / 2} \prod_{1 \leq i<j \leq n}\left|t_i-t_j\right|^{2 \gamma} d t_1 \cdots d t_n=\prod_{j=1}^n \frac{\Gamma(1+j \gamma)}{\Gamma(1+\gamma)} $$ Update : I want to inform that instead of requiring $\gamma$ to be real, now I want $\gamma$ to be a positive integer. Now any idea? Thank you.","I was wondering if there is a way to compute the Mehta integral for real numbers by using induction or some nice reverse engineering with Laplace transform, or if this is vain. Here is real Update : I want to inform that instead of requiring to be real, now I want to be a positive integer. Now any idea? Thank you.","\gamma 
\frac{1}{(2 \pi)^{n / 2}} \int\limits_{-\infty}^{+\infty} \cdots \int\limits_{-\infty}^{+\infty} \prod_{i=1}^n e^{-t_i^2 / 2} \prod_{1 \leq i<j \leq n}\left|t_i-t_j\right|^{2 \gamma} d t_1 \cdots d t_n=\prod_{j=1}^n \frac{\Gamma(1+j \gamma)}{\Gamma(1+\gamma)}
 \gamma \gamma","['real-analysis', 'complex-analysis', 'definite-integrals']"
38,What makes Cousin's theorem remarkable?,What makes Cousin's theorem remarkable?,,"I've stumbled upon a mention of Cousin's theorem in the context of HenstockKurzweil integral and got confused. I do not understand why this fact is called a theorem and what makes it any remarkable, when it seems to be a more or less trivial consequence of compactness (in the sens that every open cover has a finite subcover). Here is the statement taken from Wikipedia (slightly simplified): Let $\mathcal{C}$ be a full cover of $[a, b]$ , that is, a collection of closed subintervals of $[a, b]$ with the property that for every $x\in[a, b]$ , there exists a $\delta > 0$ so that $\mathcal{C}$ contain all subintervals of $[a, b]$ which contain $x$ and are of length smaller than $\delta$ . Then there exists a partition $a = x_0 < x_1 <\dotsb < x_n = b$ of $[a, b]$ such that $[x_{i-1},x_{i}]\in{\mathcal{C}}$ for all $i$ . Is there some historical context that makes this obvious consequence of compactness deserve to be called a theorem? If so, why is this theorem still so often mentioned in the context of HenstockKurzweil integral nowadays instead of just referring to the compactness of the interval? Am I missing something? Clearly, the interval $[a,b]$ is covered by the interiors (the interiors relative to $[a,b]$ ) of closed intervals in $\mathcal{C}$ such that all their closed subintervals containing their midpoint are also in $\mathcal{C}$ , so there is a finite set $\mathcal{D}$ of such closed intervals in $\mathcal{C}$ that covers $[a,b]$ . Is the hard part supposed to be to prove that if $[a,b]$ is covered by a finite set $\mathcal{D}$ of closed intervals, then there is a partition $a = x_0 < x_1 <\dotsb < x_n = b$ of $[a, b]$ such that each $[x_{i-1},x_{i}]$ is contained in an element of $\mathcal{D}$ and contains that element's midpoint?","I've stumbled upon a mention of Cousin's theorem in the context of HenstockKurzweil integral and got confused. I do not understand why this fact is called a theorem and what makes it any remarkable, when it seems to be a more or less trivial consequence of compactness (in the sens that every open cover has a finite subcover). Here is the statement taken from Wikipedia (slightly simplified): Let be a full cover of , that is, a collection of closed subintervals of with the property that for every , there exists a so that contain all subintervals of which contain and are of length smaller than . Then there exists a partition of such that for all . Is there some historical context that makes this obvious consequence of compactness deserve to be called a theorem? If so, why is this theorem still so often mentioned in the context of HenstockKurzweil integral nowadays instead of just referring to the compactness of the interval? Am I missing something? Clearly, the interval is covered by the interiors (the interiors relative to ) of closed intervals in such that all their closed subintervals containing their midpoint are also in , so there is a finite set of such closed intervals in that covers . Is the hard part supposed to be to prove that if is covered by a finite set of closed intervals, then there is a partition of such that each is contained in an element of and contains that element's midpoint?","\mathcal{C} [a, b] [a, b] x\in[a, b] \delta > 0 \mathcal{C} [a, b] x \delta a = x_0 < x_1 <\dotsb < x_n = b [a, b] [x_{i-1},x_{i}]\in{\mathcal{C}} i [a,b] [a,b] \mathcal{C} \mathcal{C} \mathcal{D} \mathcal{C} [a,b] [a,b] \mathcal{D} a = x_0 < x_1 <\dotsb < x_n = b [a, b] [x_{i-1},x_{i}] \mathcal{D}","['real-analysis', 'integration', 'soft-question', 'math-history', 'gauge-integral']"
39,"Let $2, 1+\frac{1}{2}, 3, 1+\frac{1}{3}, 4, 1+\frac {1}{4},\dots$ be a sequence. Does $a_n$ converge/diverge? Is there a $\sup$ or $\inf$?",Let  be a sequence. Does  converge/diverge? Is there a  or ?,"2, 1+\frac{1}{2}, 3, 1+\frac{1}{3}, 4, 1+\frac {1}{4},\dots a_n \sup \inf","Let $2,1+\frac{1}{2},3,1+\frac{1}{3},4,1+\frac {1}{4},...$ be a sequence then which of the statements is true? $a_n$ coverges to a finitie limit or diverges to infinity. $\limsup \limits_{n \to \infty} (a_n) = \sup \{a_n| n \in \Bbb N\}$ $\liminf \limits_{n \to \infty} (a_n) = \inf\{a_n| n \in \Bbb N\}$ The sequence $a_n$ has at least $3$ subsequential limits. None of the above. For the first statement I could not actually find a formula, but it is obvious that if we look at the subsequence in the even indexes we get $\lim \limits_{n \to \infty}a_{2n} =1 $ and in odd $\lim \limits_{n \to \infty}a_{2n-1} = \infty $ so the limit does not exist therefore the statement is not correct. For the second statement I assumed it was bounded from above, therefore there exists an $M>1$ such that for all $n$ we get $a_n \leq M$ , let $L$ be a sub sequential limit of $a_n$ so there exists a subsequence $a_{n_k}$ of $a_n$ such that $\lim \limits_{n \to \infty}a_{{n_k}} =L$ for all $n$ we have $a_n \leq M$ $\implies$ for all $k$ we have $a_{n_k} \leq M$ so we get $L = \lim \limits_{n \to \infty}a_{n_k} \leq M$ meaning all of the sub sequential limits of $a_n$ are less or equal to $M$ because one subsequential limit is $1$ and the other is infinity so it is not bounded from above and there is not $\sup$ . I thought the third statement is correct because the sequence is bounded from below by $1$ so it is the minimum, but according to the answers in the book it is not the correct answer and I could not figure out why. For the fourth statement, as we found in the first part there are two different limits that one is even and the other is odd indexes that cover the sequence so there are only two limits so it is not true. The right answer according to the book is the fifth statement but why isn't the third statement correct? Thank you for the amazing help and tips!","Let be a sequence then which of the statements is true? coverges to a finitie limit or diverges to infinity. The sequence has at least subsequential limits. None of the above. For the first statement I could not actually find a formula, but it is obvious that if we look at the subsequence in the even indexes we get and in odd so the limit does not exist therefore the statement is not correct. For the second statement I assumed it was bounded from above, therefore there exists an such that for all we get , let be a sub sequential limit of so there exists a subsequence of such that for all we have for all we have so we get meaning all of the sub sequential limits of are less or equal to because one subsequential limit is and the other is infinity so it is not bounded from above and there is not . I thought the third statement is correct because the sequence is bounded from below by so it is the minimum, but according to the answers in the book it is not the correct answer and I could not figure out why. For the fourth statement, as we found in the first part there are two different limits that one is even and the other is odd indexes that cover the sequence so there are only two limits so it is not true. The right answer according to the book is the fifth statement but why isn't the third statement correct? Thank you for the amazing help and tips!","2,1+\frac{1}{2},3,1+\frac{1}{3},4,1+\frac {1}{4},... a_n \limsup \limits_{n \to \infty} (a_n) = \sup \{a_n| n \in \Bbb N\} \liminf \limits_{n \to \infty} (a_n) = \inf\{a_n| n \in \Bbb N\} a_n 3 \lim \limits_{n \to \infty}a_{2n} =1  \lim \limits_{n \to \infty}a_{2n-1} = \infty  M>1 n a_n \leq M L a_n a_{n_k} a_n \lim \limits_{n \to \infty}a_{{n_k}} =L n a_n \leq M \implies k a_{n_k} \leq M L = \lim \limits_{n \to \infty}a_{n_k} \leq M a_n M 1 \sup 1","['real-analysis', 'sequences-and-series', 'supremum-and-infimum']"
40,Discontinuous function with continuous restriction to any smooth curve,Discontinuous function with continuous restriction to any smooth curve,,"The famous function $$f(x,y) = \cases {0,&if $(x,y)=(0,0)$\\ \frac{xy^2}{x^2+y^4} &otherwise}$$ is continuous when restricted to any smooth curve of the form $(x, g(x))$ , but is discontinuous at $(0, 0)$ over the curve $(y^3, y)$ . But if a function $f: \mathbb R^2 \rightarrow \mathbb R$ is continuous when restricted to any smooth curve in $\mathbb R^2$ , does it need to be continuous? I would bet on no, but I can not come up with a sound construction at the moment. What about restrictions only to analytic curves?","The famous function is continuous when restricted to any smooth curve of the form , but is discontinuous at over the curve . But if a function is continuous when restricted to any smooth curve in , does it need to be continuous? I would bet on no, but I can not come up with a sound construction at the moment. What about restrictions only to analytic curves?","f(x,y) = \cases {0,&if (x,y)=(0,0)\\
\frac{xy^2}{x^2+y^4} &otherwise} (x, g(x)) (0, 0) (y^3, y) f: \mathbb R^2 \rightarrow \mathbb R \mathbb R^2","['real-analysis', 'multivariable-calculus', 'continuity']"
41,Property of a closed path,Property of a closed path,,"I have the feeling this property holds true but I cannot find a way to prove it: Let $\gamma: [0,1] \to \mathbb{R}^n$ be a differentiable closed path, i.e. $\gamma(0) = \gamma(1)$ . Then there exist two values $\lambda_1 \neq \lambda_2 \in [0,1[$ such that $$ \left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right> \geq 0, $$ and $$ \left<\gamma'(\lambda_2),  \gamma(\lambda_1) - \gamma(\lambda_2) \right> \geq 0. $$ Graphically this means that I can find two points on my path such that the projection of the tangent vector at these points on the line linking these two points face each other. I would be happy to discuss how this problem could be approached :) EDIT: I did not want to influence the reflexion in a way that would lead to a dead-end but here is the way I tried to approach the problem. Let define the function $f: [0,1] \to \mathbb{R}$ as follows: $$f(\lambda) = \left<\gamma'(\lambda),  \gamma(\lambda^{+}) - \gamma(\lambda) \right> + \left<\gamma'(\lambda^{+}),  \gamma(\lambda^{+}) - \gamma(\lambda) \right>, $$ with $\lambda^{+}(\lambda) = mod(\lambda + 0.5, 1)$ . Hence, for instance $\lambda^{+}(0.25) = 0.75$ and $\lambda^{+}(0.9) = 0.4$ . Obviously $\lambda^{+}(\lambda^{+}(\lambda)) = \lambda$ . We have that $f$ is continuous and $f(\lambda^{+}) = - f(\lambda)$ . Therefore, one can find with the Intermediate Value Theorem a value $\lambda_1$ such that $f(\lambda_1) = 0$ . If I define and $\lambda_2 = \lambda^+(\lambda_1)$ , this implies that $$ \left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right> + \left<\gamma'(\lambda_2),  \gamma(\lambda_2) - \gamma(\lambda_1) \right> = 0  $$ $$ \Leftrightarrow \left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right> = \left<\gamma'(\lambda_2),  \gamma(\lambda_1) - \gamma(\lambda_2) \right>. $$ This means that I can find two points on my curve $\gamma(\lambda_1) $ and $\gamma(\lambda_2)$ such that the projection of the tangent vector at these points on the line between these points either point towards each other, either away from each other. I am however stuck on how to prove that the former should hold (I believe it should based on intuition and a lot of examples I tried).","I have the feeling this property holds true but I cannot find a way to prove it: Let be a differentiable closed path, i.e. . Then there exist two values such that and Graphically this means that I can find two points on my path such that the projection of the tangent vector at these points on the line linking these two points face each other. I would be happy to discuss how this problem could be approached :) EDIT: I did not want to influence the reflexion in a way that would lead to a dead-end but here is the way I tried to approach the problem. Let define the function as follows: with . Hence, for instance and . Obviously . We have that is continuous and . Therefore, one can find with the Intermediate Value Theorem a value such that . If I define and , this implies that This means that I can find two points on my curve and such that the projection of the tangent vector at these points on the line between these points either point towards each other, either away from each other. I am however stuck on how to prove that the former should hold (I believe it should based on intuition and a lot of examples I tried).","\gamma: [0,1] \to \mathbb{R}^n \gamma(0) = \gamma(1) \lambda_1 \neq \lambda_2 \in [0,1[  \left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right> \geq 0,   \left<\gamma'(\lambda_2),  \gamma(\lambda_1) - \gamma(\lambda_2) \right> \geq 0.  f: [0,1] \to \mathbb{R} f(\lambda)
=
\left<\gamma'(\lambda),  \gamma(\lambda^{+}) - \gamma(\lambda) \right>
+
\left<\gamma'(\lambda^{+}),  \gamma(\lambda^{+}) - \gamma(\lambda) \right>,
 \lambda^{+}(\lambda) = mod(\lambda + 0.5, 1) \lambda^{+}(0.25) = 0.75 \lambda^{+}(0.9) = 0.4 \lambda^{+}(\lambda^{+}(\lambda)) = \lambda f f(\lambda^{+}) = - f(\lambda) \lambda_1 f(\lambda_1) = 0 \lambda_2 = \lambda^+(\lambda_1) 
\left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right>
+
\left<\gamma'(\lambda_2),  \gamma(\lambda_2) - \gamma(\lambda_1) \right>
= 0 
 
\Leftrightarrow
\left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right>
=
\left<\gamma'(\lambda_2),  \gamma(\lambda_1) - \gamma(\lambda_2) \right>.
 \gamma(\lambda_1)  \gamma(\lambda_2)","['real-analysis', 'vector-analysis']"
42,"Given $z^{z^x}=x$, estimate $\frac{dx}{dz}$","Given , estimate",z^{z^x}=x \frac{dx}{dz},"I'm reading Knoebel's Exponentials Reiterated , page 242 and have very hard time understanding the following: If $z^{z^x}=x$ , then $\frac{dx}{dz}\gt 0$ on the open line $$L=\{<z,x>|0\lt z\lt e^{-e}\,\text{and}\, x=e^{-1}\}.$$ This doesn't make sense to me. Because if $z^{z^x}=x$ , then $x=e^{-1}$ implies $z=e^{-e}$ , which is not in the open interval $\left(0,e^{-e}\right)$ . I know that, if $j(z,x)=z^{z^x}-x$ , then $$dj(z,x)=z^{z^x}z^{x-1}(x\ln z+1)\,dz+\left(z^{z^x}z^x\ln ^2z-1\right)\,dx,$$ and there is a unique trajectory satisfying $j(z,x)=0$ when it is not true that $z=e^{-e}$ and $x=e^{-1}$ simultaneously by the Implicit function theorem , but how could that be used to prove $\frac{dx}{dz}\gt 0$ on the line?","I'm reading Knoebel's Exponentials Reiterated , page 242 and have very hard time understanding the following: If , then on the open line This doesn't make sense to me. Because if , then implies , which is not in the open interval . I know that, if , then and there is a unique trajectory satisfying when it is not true that and simultaneously by the Implicit function theorem , but how could that be used to prove on the line?","z^{z^x}=x \frac{dx}{dz}\gt 0 L=\{<z,x>|0\lt z\lt e^{-e}\,\text{and}\, x=e^{-1}\}. z^{z^x}=x x=e^{-1} z=e^{-e} \left(0,e^{-e}\right) j(z,x)=z^{z^x}-x dj(z,x)=z^{z^x}z^{x-1}(x\ln z+1)\,dz+\left(z^{z^x}z^x\ln ^2z-1\right)\,dx, j(z,x)=0 z=e^{-e} x=e^{-1} \frac{dx}{dz}\gt 0","['real-analysis', 'calculus', 'derivatives']"
43,About possible pathological solutions to the DE $y=y'+y''+y'''+\ldots$,About possible pathological solutions to the DE,y=y'+y''+y'''+\ldots,"Is it possible to construct (or indirectly show the existence of) a function $y(x)\in\mathcal{C}^{\infty}(\mathbb{R})$ such that series $$ S(x) = \sum_{n=1}^{+\infty}\frac{d^n y}{dx^n} $$ is pointwise convergent for any $x\in\mathbb{R}$ , but $S(x)$ is not a differentiable function? A more-or-less equivalent and strictly related question is the following one: is it possible for the operator $$ T:\varphi(x) \mapsto e^x \int_{0}^{x} e^{-t}\varphi(t)\,dt $$ to produce $T(\varphi)\in \mathcal{C}^\infty$ without $\varphi$ being differentiable? Probably no, because... Assuming that the integral appearing above is the Riemann integral, $T$ acts on the space of almost-everywhere continuous functions, so we are free to assume that $\varphi$ is a.e. $\mathcal{C}^0$ , as well as $$\Phi(x)=\frac{d}{dx}\int_{0}^{x}e^{-t}\varphi(t)\,dt. $$ By the remark in the comments, $\int_{0}^{x}\left(\Phi(t)-e^{-t}\varphi(t)\right)\,dt$ equals zero almost everywhere, so the fundamental theorem of Calculus ""almost applies"". $T(\varphi)$ is smooth by assumption, so it is $T(\varphi)e^{-x}=\int_{0}^{x}e^{-t}\varphi(t)\,dt$ and its derivative $\Phi(x)$ . On the other hand $\Phi(x)=e^{-x}\varphi(x)$ almost everywhere, so $\varphi(x)$ is $\mathcal{C}^\infty$ almost everywhere.","Is it possible to construct (or indirectly show the existence of) a function such that series is pointwise convergent for any , but is not a differentiable function? A more-or-less equivalent and strictly related question is the following one: is it possible for the operator to produce without being differentiable? Probably no, because... Assuming that the integral appearing above is the Riemann integral, acts on the space of almost-everywhere continuous functions, so we are free to assume that is a.e. , as well as By the remark in the comments, equals zero almost everywhere, so the fundamental theorem of Calculus ""almost applies"". is smooth by assumption, so it is and its derivative . On the other hand almost everywhere, so is almost everywhere.","y(x)\in\mathcal{C}^{\infty}(\mathbb{R})  S(x) = \sum_{n=1}^{+\infty}\frac{d^n y}{dx^n}  x\in\mathbb{R} S(x)  T:\varphi(x) \mapsto e^x \int_{0}^{x} e^{-t}\varphi(t)\,dt  T(\varphi)\in \mathcal{C}^\infty \varphi T \varphi \mathcal{C}^0 \Phi(x)=\frac{d}{dx}\int_{0}^{x}e^{-t}\varphi(t)\,dt.  \int_{0}^{x}\left(\Phi(t)-e^{-t}\varphi(t)\right)\,dt T(\varphi) T(\varphi)e^{-x}=\int_{0}^{x}e^{-t}\varphi(t)\,dt \Phi(x) \Phi(x)=e^{-x}\varphi(x) \varphi(x) \mathcal{C}^\infty","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives']"
44,Convergence of recursively defined sequence,Convergence of recursively defined sequence,,"I have the following problem: Suppose $a$ and $k$ are positive reals and $ a^2 > 2k $ . Set $x_{0} = a$ and define $x_{n}=x_{n-1} + \frac{k}{x_{n-1}}$ for $n\geq1$ . Prove that $\lim_{n \to \infty}\frac{x_{n}}{\sqrt{n}}$ exists and determine its value. For reference this is a problem in Hardy and William's The Green Book of Mathematical Problems . I believe I have a proof of this and would appreciate if anyone could check for its correctness. I'd also appreciate if someone could give a simpler argument. Here is my proof: We show that $$\limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}=\liminf_{n \to \infty}\frac{x_{n}}{\sqrt{n}}=\sqrt{2k}$$ This will show the sequence converges to $\sqrt{2k}$ . From squaring the defining relation, we get $$x_{n}^2=x_{n-1}^2+2k+\frac{k^2}{x_{n-1}^2} > x_{n-1}^2 +2k$$ Applying this same estimate $n$ times gives us the lower bound $ x_{n}^2 > a^2+2nk$ , so that $x_{n} > \sqrt{2nk+a^2}>\sqrt{2k(n+1)}$ (*) We then get $$\liminf_{n \to \infty}\frac{x_{n}}{\sqrt{n}} \geq \liminf_{n \to \infty}\sqrt{2k\frac{n+1}{n}}=\sqrt{2k}$$ We'll now show $$\limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}\leq\sqrt{2k}$$ We first derive an upper bound on $ x_{n}-x_{n-1} $ . Applying the estimate (*), to $x_{n-1}$ gives us $$x_{n}= x_{n-1} + \frac{k}{x_{n-1}} < x_{n-1} + \frac{k}{\sqrt{2k(n-1)+a^2}} < x_{n-1} +\sqrt{\frac{k}{2n}}$$ where $a^2 > 2k$ was used. Thus, for $n\geq1$ , we get $$x_{n}-x_{n-1} < \sqrt{\frac{k}{2n}}$$ (**) Observe that by telescoping, we have $$x_{n}=a+\sum_{j=1}^{n}x_{j}-x_{j-1}$$ Applying the estimate (**) to each term in the summand gives $$\frac{x_{n}-a}{\sqrt{n}}=\frac{1}{\sqrt{n}}\sum_{j=1}^{n}x_{j}-x_{j-1}<\sqrt{\frac{k}{2n}}\sum_{j=1}^{n}\frac{1}{\sqrt{j}}$$ Estimating this last term with an integral gives: $$\sqrt{\frac{k}{2n}}\sum_{j=1}^{n}\frac{1}{\sqrt{j}} < \sqrt{\frac{k}{2n}}(1+\int_{1}^{n}\frac{dx}{\sqrt{x}})=\sqrt{\frac{k}{2n}}(2\sqrt{n}-1)$$ Thus, we get $$\frac{x_{n}}{\sqrt{n}}<\frac{a}{\sqrt{n}}-\sqrt{\frac{k}{2n}}+\sqrt{2k}$$ Taking the limsup of both sides gives $$\limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}\leq\sqrt{2k}$$ which completes the proof. Feedback and or corrections are much appreciated!","I have the following problem: Suppose and are positive reals and . Set and define for . Prove that exists and determine its value. For reference this is a problem in Hardy and William's The Green Book of Mathematical Problems . I believe I have a proof of this and would appreciate if anyone could check for its correctness. I'd also appreciate if someone could give a simpler argument. Here is my proof: We show that This will show the sequence converges to . From squaring the defining relation, we get Applying this same estimate times gives us the lower bound , so that (*) We then get We'll now show We first derive an upper bound on . Applying the estimate (*), to gives us where was used. Thus, for , we get (**) Observe that by telescoping, we have Applying the estimate (**) to each term in the summand gives Estimating this last term with an integral gives: Thus, we get Taking the limsup of both sides gives which completes the proof. Feedback and or corrections are much appreciated!",a k  a^2 > 2k  x_{0} = a x_{n}=x_{n-1} + \frac{k}{x_{n-1}} n\geq1 \lim_{n \to \infty}\frac{x_{n}}{\sqrt{n}} \limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}=\liminf_{n \to \infty}\frac{x_{n}}{\sqrt{n}}=\sqrt{2k} \sqrt{2k} x_{n}^2=x_{n-1}^2+2k+\frac{k^2}{x_{n-1}^2} > x_{n-1}^2 +2k n  x_{n}^2 > a^2+2nk x_{n} > \sqrt{2nk+a^2}>\sqrt{2k(n+1)} \liminf_{n \to \infty}\frac{x_{n}}{\sqrt{n}} \geq \liminf_{n \to \infty}\sqrt{2k\frac{n+1}{n}}=\sqrt{2k} \limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}\leq\sqrt{2k}  x_{n}-x_{n-1}  x_{n-1} x_{n}= x_{n-1} + \frac{k}{x_{n-1}} < x_{n-1} + \frac{k}{\sqrt{2k(n-1)+a^2}} < x_{n-1} +\sqrt{\frac{k}{2n}} a^2 > 2k n\geq1 x_{n}-x_{n-1} < \sqrt{\frac{k}{2n}} x_{n}=a+\sum_{j=1}^{n}x_{j}-x_{j-1} \frac{x_{n}-a}{\sqrt{n}}=\frac{1}{\sqrt{n}}\sum_{j=1}^{n}x_{j}-x_{j-1}<\sqrt{\frac{k}{2n}}\sum_{j=1}^{n}\frac{1}{\sqrt{j}} \sqrt{\frac{k}{2n}}\sum_{j=1}^{n}\frac{1}{\sqrt{j}} < \sqrt{\frac{k}{2n}}(1+\int_{1}^{n}\frac{dx}{\sqrt{x}})=\sqrt{\frac{k}{2n}}(2\sqrt{n}-1) \frac{x_{n}}{\sqrt{n}}<\frac{a}{\sqrt{n}}-\sqrt{\frac{k}{2n}}+\sqrt{2k} \limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}\leq\sqrt{2k},"['real-analysis', 'calculus', 'sequences-and-series']"
45,For which $\alpha>0$ does $x\le|W(-cx^2)|^{-\alpha}$,For which  does,\alpha>0 x\le|W(-cx^2)|^{-\alpha},"Let $x\in(0,1)$ . I want to know for which $\alpha>0$ it's true that $$ x\le|W(-cx^2)|^{-\alpha},\label{1}\tag{$\ast$} $$ where $W$ is the Lambert W-function and $c>0$ is some constant. In my numerical tests, the value of $c$ didn't really seem to matter, but \eqref{1} seemed to hold for very small $\alpha$ , for example $\alpha\approx 0.001$ . It seems difficult to prove analytically because of the non-elementary nature of the Lambert W function. For negative $y<0$ , it seems to be true that $W(-y)<0$ . So we can rewrite \eqref{1} as $$ x(-W(-cx^2))^{-\alpha}\le 1. $$ We can define a function $f(x)=x(-W(-cx^2))^{-\alpha}$ . Then $f(0)=0$ , $f>0$ on $(0,1)$ and $f\in C^1$ since $W$ is differentiable on $(0,1)$ as it does not include the points $\{0,\frac{1}{e}\}$ . So the maximum of $f$ reached at $x_0$ should satisfy $$f'(x_0)=0\label{2}\tag{$\ast\ast$}$$ where $$ f'(x)=\left(-W(-cx^2)\right)^\alpha\left(1-\frac{2 c\alpha x^2 W'(-cx^2)}{W(-cx^2)} \right), $$ So \eqref{2} is \begin{align*} &\left(-W(-cx_0^2) \right)^\alpha-2c\alpha x_0^2\left(-W(cx_0^2) \right)^{\alpha-1}W'(-cx_0^2)=0 \\ \iff& \alpha=-c'\frac{W(-cx_0^2)}{x_0^2 W'(-cx_0^2)} \\ \iff& x^2_0\frac{\mathrm{d}}{\mathrm{d}x_0}\log\left( W(-cx_0^2)\right)=-c''\frac{1}{\alpha}, \end{align*} But I don't see how to go from here, i.e., how to invert the function $$ \psi(x)=x^2\frac{\mathrm{d}}{\mathrm{d}x}\log\left(W(-cx^2)\right) $$ to recover $x_0$ as $$ x_0=\psi^{-1}\left(-c''\frac{1}{\alpha}\right), $$ and plug that back into \eqref{1}. But in Mathematica, it gives $$ c'''\psi^{-1}\left(-c''\frac{1}{\alpha}\right)=\pm c'''\left(\alpha W\left(\mp c'' i\frac{1}{\sqrt{\alpha}}\right)\right)^{-\frac{1}{2}}, $$ which isn't very helpful!","Let . I want to know for which it's true that where is the Lambert W-function and is some constant. In my numerical tests, the value of didn't really seem to matter, but \eqref{1} seemed to hold for very small , for example . It seems difficult to prove analytically because of the non-elementary nature of the Lambert W function. For negative , it seems to be true that . So we can rewrite \eqref{1} as We can define a function . Then , on and since is differentiable on as it does not include the points . So the maximum of reached at should satisfy where So \eqref{2} is But I don't see how to go from here, i.e., how to invert the function to recover as and plug that back into \eqref{1}. But in Mathematica, it gives which isn't very helpful!","x\in(0,1) \alpha>0 
x\le|W(-cx^2)|^{-\alpha},\label{1}\tag{\ast}
 W c>0 c \alpha \alpha\approx 0.001 y<0 W(-y)<0 
x(-W(-cx^2))^{-\alpha}\le 1.
 f(x)=x(-W(-cx^2))^{-\alpha} f(0)=0 f>0 (0,1) f\in C^1 W (0,1) \{0,\frac{1}{e}\} f x_0 f'(x_0)=0\label{2}\tag{\ast\ast} 
f'(x)=\left(-W(-cx^2)\right)^\alpha\left(1-\frac{2 c\alpha x^2 W'(-cx^2)}{W(-cx^2)} \right),
 \begin{align*}
&\left(-W(-cx_0^2) \right)^\alpha-2c\alpha x_0^2\left(-W(cx_0^2) \right)^{\alpha-1}W'(-cx_0^2)=0
\\
\iff& \alpha=-c'\frac{W(-cx_0^2)}{x_0^2 W'(-cx_0^2)}
\\
\iff& x^2_0\frac{\mathrm{d}}{\mathrm{d}x_0}\log\left( W(-cx_0^2)\right)=-c''\frac{1}{\alpha},
\end{align*} 
\psi(x)=x^2\frac{\mathrm{d}}{\mathrm{d}x}\log\left(W(-cx^2)\right)
 x_0 
x_0=\psi^{-1}\left(-c''\frac{1}{\alpha}\right),
 
c'''\psi^{-1}\left(-c''\frac{1}{\alpha}\right)=\pm c'''\left(\alpha W\left(\mp c'' i\frac{1}{\sqrt{\alpha}}\right)\right)^{-\frac{1}{2}},
","['real-analysis', 'inequality', 'logarithms', 'exponential-function', 'lambert-w']"
46,"How do I solve $ f(y) \int_0^1 \tfrac{\exp(-\frac{(x-y)^2}{2})}{\int_0^1 \exp(-\frac{(x-z)^2}{2}) f(z) dz}\,dx =1$?",How do I solve ?," f(y) \int_0^1 \tfrac{\exp(-\frac{(x-y)^2}{2})}{\int_0^1 \exp(-\frac{(x-z)^2}{2}) f(z) dz}\,dx =1","I am trying to solve this integral equation for $f$ (can be assumed to be positive) $$ f(y) \int_0^1 \frac{e^{-\frac{(x-y)^2}{2}}}{\int_0^1 e^{-\frac{(x-z)^2}{2}} f(z) dz} dx =1, \quad y \in [0,1]$$ Any insight is welcome, I have no idea how this kind of equations is called and how to solve them. I thought about differentiating both side with regard to $y$ , we get $$f(y)\int_0^1 \frac{(x-y)e^{-\frac{(x-y)^2}{2}}}{\int_0^1 e^{-\frac{(x-z)^2}{2}} f(z) dz} dx + f'(y)\int_0^1 \frac{e^{-\frac{(x-y)^2}{2}}}{\int_0^1 e^{-\frac{(x-z)^2}{2}} f(z) dz} dx  =0$$ Plugging in the first equation, we get $$ f(y)\int_0^1 \frac{xe^{-\frac{(x-y)^2}{2}}}{\int_0^1 e^{-\frac{(x-z)^2}{2}} f(z) dz} dx -y + \frac{f'(y)}{f(y)} = 0$$ But it only seems to make the problem harder. EDIT: As this question seems to interest some people, here is some more info. We can rewrite the problem as a system of two integral equations: $$ f(y) \int_0^1 e^{-\frac{(x-y)^2}{2}} \hat f(x) dx =1, \quad y \in [0,1]$$ $$ \hat f(x) \int_0^1 e^{-\frac{(x-y)^2}{2}} f(y) dy =1, \quad x \in [0,1]$$ The existence of solutions $f,\hat f$ is a deep result in stochastic processes and probability theory. The trained eye recognized the heat kernel (Brownian transition density). The product $f(y)\hat f(x)$ is actually the density (not w.r.t Lebesgue, it's complicated) of a certain coupling of 2 given random variables. So $f,\hat f$ contain all the information about the dependence between those 2 random variables. They are unique up to multiplicative constant (when you multiply one by $c$ , you divide the other one by $c$ )","I am trying to solve this integral equation for (can be assumed to be positive) Any insight is welcome, I have no idea how this kind of equations is called and how to solve them. I thought about differentiating both side with regard to , we get Plugging in the first equation, we get But it only seems to make the problem harder. EDIT: As this question seems to interest some people, here is some more info. We can rewrite the problem as a system of two integral equations: The existence of solutions is a deep result in stochastic processes and probability theory. The trained eye recognized the heat kernel (Brownian transition density). The product is actually the density (not w.r.t Lebesgue, it's complicated) of a certain coupling of 2 given random variables. So contain all the information about the dependence between those 2 random variables. They are unique up to multiplicative constant (when you multiply one by , you divide the other one by )","f  f(y) \int_0^1 \frac{e^{-\frac{(x-y)^2}{2}}}{\int_0^1 e^{-\frac{(x-z)^2}{2}} f(z) dz} dx =1, \quad y \in [0,1] y f(y)\int_0^1 \frac{(x-y)e^{-\frac{(x-y)^2}{2}}}{\int_0^1 e^{-\frac{(x-z)^2}{2}} f(z) dz} dx + f'(y)\int_0^1 \frac{e^{-\frac{(x-y)^2}{2}}}{\int_0^1 e^{-\frac{(x-z)^2}{2}} f(z) dz} dx  =0  f(y)\int_0^1 \frac{xe^{-\frac{(x-y)^2}{2}}}{\int_0^1 e^{-\frac{(x-z)^2}{2}} f(z) dz} dx -y + \frac{f'(y)}{f(y)} = 0  f(y) \int_0^1 e^{-\frac{(x-y)^2}{2}} \hat f(x) dx =1, \quad y \in [0,1]  \hat f(x) \int_0^1 e^{-\frac{(x-y)^2}{2}} f(y) dy =1, \quad x \in [0,1] f,\hat f f(y)\hat f(x) f,\hat f c c","['real-analysis', 'integration', 'functional-analysis', 'ordinary-differential-equations', 'analysis']"
47,The volume of the image of a map with vanishing Jacobian is zero,The volume of the image of a map with vanishing Jacobian is zero,,"Let $\Omega \subseteq \mathbb{R}^n$ be a nice domain with smooth boundary (say a ball), and let $f:\Omega \to \mathbb{R}^n$ be smooth. Set $\Omega_0=\{ x \in \Omega \, | \, \det df_x =0 \} $ Is there an elementary way to prove that $m(f(\Omega_0))=0$ ? ( $m$ is the Lebesgue measure). I know that this follows from the (co)area formula or Sard's theorem*, but is there a way that avoids them? Edit: In this answer , zhw proves that if $x \in \Omega_0$ , and $B(r)$ is an Euclidean ball of radius $r$ centered at $x$ , then $\frac{m(f(B(r))}{m(B(r))} \to 0$ . Since $\Omega_0$ is compact, we can proceed as follows: Cover $\Omega_0$ by finitely many balls $B_i$ with radius $r$ , centered around points that belong $\Omega_0$ . ( The centers depend on $r$ )Then, $$ m(f(\Omega_0)) \le \sum_i m(f(B_i))=\sum_i \frac{m(f(B_i))}{m(B_i)}m(B_i).$$ If we could prove that $\frac{m(f(B_i))}{m(B_i)} \to 0$ when $r \to 0$ uniformly in $i$ , then we could get $$ m(f(\Omega_0)) \le o(1) \sum_i m(B_i)=o(1) \sum_i m(B(r)). $$ Since we can cover $\Omega_0$ by $N(r)$ balls of radius $r$ with centers in $\Omega_0$ , where $N(r) \le c \frac{1}{m(B(r))}$ , we could conclude that $m(f(\Omega_0)) \le o(1)$ , so it must be zero. The problem is that I am not sure if $\frac{m(f(B_{x_i(r)}(r)))}{m(B_{x_i(r)}(r))} $ converges to zero independently of $i$ . I asked about this separately here . *If I am not mistaken, then Sard's theorem implies that almost every $y \in \mathbb R^n$ is a regular value of $f$ - so it doesn't have a preimage in $\Omega_0$ . **I don't know an elementary proof even in the case where $\Omega_0=\Omega$ . That is, even if we assume the the domain where the Jacobian vanishes is ""open and nice"" I don't know if its trivial. In general, $\Omega_0$ is an arbitrary closed set, which might be pretty complicated.","Let be a nice domain with smooth boundary (say a ball), and let be smooth. Set Is there an elementary way to prove that ? ( is the Lebesgue measure). I know that this follows from the (co)area formula or Sard's theorem*, but is there a way that avoids them? Edit: In this answer , zhw proves that if , and is an Euclidean ball of radius centered at , then . Since is compact, we can proceed as follows: Cover by finitely many balls with radius , centered around points that belong . ( The centers depend on )Then, If we could prove that when uniformly in , then we could get Since we can cover by balls of radius with centers in , where , we could conclude that , so it must be zero. The problem is that I am not sure if converges to zero independently of . I asked about this separately here . *If I am not mistaken, then Sard's theorem implies that almost every is a regular value of - so it doesn't have a preimage in . **I don't know an elementary proof even in the case where . That is, even if we assume the the domain where the Jacobian vanishes is ""open and nice"" I don't know if its trivial. In general, is an arbitrary closed set, which might be pretty complicated.","\Omega \subseteq \mathbb{R}^n f:\Omega \to \mathbb{R}^n \Omega_0=\{ x \in \Omega \, | \, \det df_x =0 \}  m(f(\Omega_0))=0 m x \in \Omega_0 B(r) r x \frac{m(f(B(r))}{m(B(r))} \to 0 \Omega_0 \Omega_0 B_i r \Omega_0 r  m(f(\Omega_0)) \le \sum_i m(f(B_i))=\sum_i \frac{m(f(B_i))}{m(B_i)}m(B_i). \frac{m(f(B_i))}{m(B_i)} \to 0 r \to 0 i 
m(f(\Omega_0)) \le o(1) \sum_i m(B_i)=o(1) \sum_i m(B(r)).
 \Omega_0 N(r) r \Omega_0 N(r) \le c \frac{1}{m(B(r))} m(f(\Omega_0)) \le o(1) \frac{m(f(B_{x_i(r)}(r)))}{m(B_{x_i(r)}(r))}  i y \in \mathbb R^n f \Omega_0 \Omega_0=\Omega \Omega_0","['real-analysis', 'measure-theory', 'multivariable-calculus', 'geometric-measure-theory', 'change-of-variable']"
48,"$f,g$ analytic on $I\subset \mathbb{R}$. If exist $a\in I$ such that $f=g$ and $f^{(n)}=g^{(n)}$then we have $f(x)=g(x)$ for every $x \in I$.",analytic on . If exist  such that  and then we have  for every .,"f,g I\subset \mathbb{R} a\in I f=g f^{(n)}=g^{(n)} f(x)=g(x) x \in I","Problem Let $f,g$ analytic on an open interval $I\subset \mathbb{R}$ . If exist some $a\in I$ such that $f(a)=g(a)$ and $f^{(n)}(a)=g^{(n)}(a)$ for all $n \in \mathbb{N}$ then we have $f(x)=g(x)$ for every $x \in I$ . Show this is no longer true if you just suppose $f,g$ class $C^\infty > $ Well, my idea is just write these series and show that if all terms are the same, these functions well be the same as well. But my main problem is understanding this fundamental difference between analytic or $ C^\infty $ . The core of this problem is being able to write the series of these functions?","Problem Let analytic on an open interval . If exist some such that and for all then we have for every . Show this is no longer true if you just suppose class Well, my idea is just write these series and show that if all terms are the same, these functions well be the same as well. But my main problem is understanding this fundamental difference between analytic or . The core of this problem is being able to write the series of these functions?","f,g I\subset \mathbb{R} a\in I f(a)=g(a) f^{(n)}(a)=g^{(n)}(a) n \in \mathbb{N} f(x)=g(x) x \in I f,g C^\infty
>   C^\infty ","['real-analysis', 'analyticity', 'analytic-functions']"
49,"Construct a compact set with countably infinite limit points using $1/n + 1/m$ (Rudin, Cooke) [duplicate]","Construct a compact set with countably infinite limit points using  (Rudin, Cooke) [duplicate]",1/n + 1/m,"This question already has answers here : Find the limit points of the set $\{ \frac{1}{n} +\frac{1}{m} \mid n , m = 1,2,3,\dots \}$ (5 answers) Closed 4 years ago . So I am having trouble proving that if $$ K= \{0\} \cup \left\{\frac1n: n = 1,2,\ldots\right\} \cup \left\{\frac1n + \frac1m : n=m,m+1,\ldots; m=1,2,\ldots \right\} $$ then 0 and the points $\frac1m$ are the only limit points of $K$ . Cooke proves this by saying that: Since $x\geq0$ for all $x \in K$ and for any positive number $\epsilon$ there is only a finite set of number s in $K$ larger than $1+\epsilon$ , it is clear that no negative number and no number larger than 1 can be a limit point of K. Hence we need only consider positive numbers $x$ satisfying $0\lt x \lt 1 $ . If $x$ is such a number and $x$ is not one of the points $\frac1m$ , let $p$ be such that $\frac1{1+p} \lt x \lt \frac1\epsilon$ , and let $\epsilon=\frac12\min(x-\frac1{p+1},\frac1p - x)$ . The intersection of the set $K$ with the interval $(x-\epsilon,x+\epsilon)$ is contained in the set of points $$ \left\{\frac1{p+1}+\frac1k:p+1\le k \lt \frac1\epsilon\right\} \cup \left\{\frac1m+\frac1n:m\le n \lt \frac1{p+1} - \frac1{p+2}; m=p+2, \ldots,2p+2\right\}, $$ which is a finite set. Therefore $x$ cannot be a limit point of $K$ . I understand why you take $\epsilon$ to be half of $\min$ , and the using that every neighborhood around a limit point contains infinitely many points, but I don't understand why the intersection of $K$ with the interval $(x-\epsilon,x+\epsilon)$ is contained in the set mentioned above, and why it is finite. Is it a typo? $$ \left\{\frac1m+\frac1n:m\le n \lt \frac1{p+1} - \frac1{p+2}; m=p+2, \ldots,2p+2\right\}, $$ here I think n is meant to be an integer in order for the finite argument to work, but how can be $$ n \lt \frac1{p+1} - \frac1{p+2}\:\:? $$ Here is how I fixed the typo to work: if $$ \left\{\frac1m+\frac1n:m\le n \lt \frac1{p+1} - \frac1{p+2}; m=p+2, \ldots,2p+2\right\} $$ is changed to $$ \left\{\frac1m+\frac1n: m\le n, \frac1n \ge \frac1{p+1} - \frac1{p+2}; m=p+2, \ldots,2p+2, n \in N\right\}, $$ then $m\le n \le (p+1)(p+2)$ which would be finite. Am  I understanding this argument correctly? I think there is a typo or error in Cooke's solution, as the original version can't seem to work. I know there are other constructions that can be understood easily, but I would like to understand Cooke's proof of $\frac1n + \frac1m$ and see if there are are any typos or errors in the original solution. Thanks a lot.","This question already has answers here : Find the limit points of the set $\{ \frac{1}{n} +\frac{1}{m} \mid n , m = 1,2,3,\dots \}$ (5 answers) Closed 4 years ago . So I am having trouble proving that if then 0 and the points are the only limit points of . Cooke proves this by saying that: Since for all and for any positive number there is only a finite set of number s in larger than , it is clear that no negative number and no number larger than 1 can be a limit point of K. Hence we need only consider positive numbers satisfying . If is such a number and is not one of the points , let be such that , and let . The intersection of the set with the interval is contained in the set of points which is a finite set. Therefore cannot be a limit point of . I understand why you take to be half of , and the using that every neighborhood around a limit point contains infinitely many points, but I don't understand why the intersection of with the interval is contained in the set mentioned above, and why it is finite. Is it a typo? here I think n is meant to be an integer in order for the finite argument to work, but how can be Here is how I fixed the typo to work: if is changed to then which would be finite. Am  I understanding this argument correctly? I think there is a typo or error in Cooke's solution, as the original version can't seem to work. I know there are other constructions that can be understood easily, but I would like to understand Cooke's proof of and see if there are are any typos or errors in the original solution. Thanks a lot.","
K= \{0\} \cup \left\{\frac1n: n = 1,2,\ldots\right\} \cup \left\{\frac1n + \frac1m : n=m,m+1,\ldots; m=1,2,\ldots \right\}
 \frac1m K x\geq0 x \in K \epsilon K 1+\epsilon x 0\lt x \lt 1  x x \frac1m p \frac1{1+p} \lt x \lt \frac1\epsilon \epsilon=\frac12\min(x-\frac1{p+1},\frac1p - x) K (x-\epsilon,x+\epsilon) 
\left\{\frac1{p+1}+\frac1k:p+1\le k \lt \frac1\epsilon\right\} \cup \left\{\frac1m+\frac1n:m\le n \lt \frac1{p+1} - \frac1{p+2}; m=p+2, \ldots,2p+2\right\},
 x K \epsilon \min K (x-\epsilon,x+\epsilon) 
\left\{\frac1m+\frac1n:m\le n \lt \frac1{p+1} - \frac1{p+2}; m=p+2, \ldots,2p+2\right\},
 
n \lt \frac1{p+1} - \frac1{p+2}\:\:?
 
\left\{\frac1m+\frac1n:m\le n \lt \frac1{p+1} - \frac1{p+2}; m=p+2, \ldots,2p+2\right\}
 
\left\{\frac1m+\frac1n: m\le n, \frac1n \ge \frac1{p+1} - \frac1{p+2}; m=p+2, \ldots,2p+2, n \in N\right\},
 m\le n \le (p+1)(p+2) \frac1n + \frac1m","['real-analysis', 'limits', 'proof-explanation']"
50,If the cardinality of $f^{-1}$ is at most $f(x)^2$ then $f$ is differentiable almost everywhere.,If the cardinality of  is at most  then  is differentiable almost everywhere.,f^{-1} f(x)^2 f,"I came across the following problem in a prelim question paper. The question as stated seems meaningless to me, I am adding the picture so as to avoid any error from my end. My case with the above question is that $f$ is given to be defined on $(0,1)$ so $f(x)$ makes sense only if $x\in (0,1)$ . Although, it doesnt tell s anything but if we assume therefore that the range is contained only in $(0,1)$ . But then it would mean that a point $x$ is taken only $0$ times which is again a nonsense. I am not sure if I am missing something or the question is really wrong. If it is wrong, what can be the nearest correct version of it? Meaning if we take the function to be defined from $\mathbb{R}$ to $(0,\infty)$ ?","I came across the following problem in a prelim question paper. The question as stated seems meaningless to me, I am adding the picture so as to avoid any error from my end. My case with the above question is that is given to be defined on so makes sense only if . Although, it doesnt tell s anything but if we assume therefore that the range is contained only in . But then it would mean that a point is taken only times which is again a nonsense. I am not sure if I am missing something or the question is really wrong. If it is wrong, what can be the nearest correct version of it? Meaning if we take the function to be defined from to ?","f (0,1) f(x) x\in (0,1) (0,1) x 0 \mathbb{R} (0,\infty)","['real-analysis', 'functional-analysis', 'bounded-variation']"
51,Can the Leibniz integral rule be used on integrals with singular endpoints?,Can the Leibniz integral rule be used on integrals with singular endpoints?,,"I'd like to know if the Leibniz integral rule has an extension or generalization that can handle convergent integrals whose endpoints are singular. This post attempts to ask a similar question but doesn't give a good example. Here's my example: $$\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx=\frac{\pi}{2a}.$$ Suppose we take $d/da$ of both sides. Clearly we have: $$\frac{d}{da}\left(\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx\right)=\frac{-\pi}{2a^2}.$$ But the Liebniz integral rule seems to give: $$\frac{d}{da}\left(\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx\right)=\int_a^\infty \frac{a}{x(x^2-a^2)^{3/2}}\,dx-\frac{1}{a\sqrt{a^2-a^2}}.$$ Of course, the rightmost term is a problem, which results from the fact that the endpoint of the original integral was singular. Suppose the original integral weren't so easily solvable and we actually needed the Leibniz integral rule to make progressis there a valid way to apply it? One idea I have is to change the original integral to $$\lim_{\epsilon\to 0^+}\int_{a+\epsilon}^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx$$ and then take the derivative using the Leibniz rule, which should result in a difference of two expressions that individually diverge as $\epsilon\to 0$ , but whose difference converges. But this would require justifying bringing the derivative $d/da$ inside the $\lim_{\epsilon\to 0}$ operation, which would require a uniform convergence proof. Is there a shortcut that gets around this difficulty, or a guarantee that this maneuver is always allowed?","I'd like to know if the Leibniz integral rule has an extension or generalization that can handle convergent integrals whose endpoints are singular. This post attempts to ask a similar question but doesn't give a good example. Here's my example: Suppose we take of both sides. Clearly we have: But the Liebniz integral rule seems to give: Of course, the rightmost term is a problem, which results from the fact that the endpoint of the original integral was singular. Suppose the original integral weren't so easily solvable and we actually needed the Leibniz integral rule to make progressis there a valid way to apply it? One idea I have is to change the original integral to and then take the derivative using the Leibniz rule, which should result in a difference of two expressions that individually diverge as , but whose difference converges. But this would require justifying bringing the derivative inside the operation, which would require a uniform convergence proof. Is there a shortcut that gets around this difficulty, or a guarantee that this maneuver is always allowed?","\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx=\frac{\pi}{2a}. d/da \frac{d}{da}\left(\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx\right)=\frac{-\pi}{2a^2}. \frac{d}{da}\left(\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx\right)=\int_a^\infty \frac{a}{x(x^2-a^2)^{3/2}}\,dx-\frac{1}{a\sqrt{a^2-a^2}}. \lim_{\epsilon\to 0^+}\int_{a+\epsilon}^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx \epsilon\to 0 d/da \lim_{\epsilon\to 0}","['real-analysis', 'integration', 'improper-integrals']"
52,Classifying Solutions to $f^n(x) = x$,Classifying Solutions to,f^n(x) = x,"Let $f: \mathbb{R} \to \mathbb{R}$ be continuous everywhere in $\mathbb{R}$ except on some finite set. Suppose we also have $f^n(x) = x$ for all $x$ where $f$ is defined. Note that by $f^n(x)$ I mean the $n$ th composition of $f$ with itself. Given a fixed $n$ , how do you classify all solutions to this? For $n=1,2$ this is easy enough. But when $n=3$ we have for example, $f(x) = \frac{1}{1-x}$ . It's not obvious how to find all the solutions to $f^3(x) = x$ .","Let be continuous everywhere in except on some finite set. Suppose we also have for all where is defined. Note that by I mean the th composition of with itself. Given a fixed , how do you classify all solutions to this? For this is easy enough. But when we have for example, . It's not obvious how to find all the solutions to .","f: \mathbb{R} \to \mathbb{R} \mathbb{R} f^n(x) = x x f f^n(x) n f n n=1,2 n=3 f(x) = \frac{1}{1-x} f^3(x) = x","['real-analysis', 'functional-equations']"
53,"If $f : \mathbb{R}^n \to \Bbb R$ is monotone over all lines, can it be written as $h \circ l$ where $h$ is monotone and $l$ is a linear form?","If  is monotone over all lines, can it be written as  where  is monotone and  is a linear form?",f : \mathbb{R}^n \to \Bbb R h \circ l h l,"Let be $f : \Bbb R^n \to \Bbb R$ monotone over all lines (not affine ones, but if there is an answer over affine lines, I'm interested.) Is it possible to find $h : \mathbb{R} \to \Bbb R$ monotone and $l : \Bbb R^n \to \Bbb R$ linear so that $f = h \circ l$ ? I tried to look by supposing I have such a factorization, and as $\ker l$ is a hyperplane, I have $n - 1$ lines where $f$ is constant. I tried to use the monotonicity condition by trying to compare $f(0)$ and over lines, but it didn't work.","Let be monotone over all lines (not affine ones, but if there is an answer over affine lines, I'm interested.) Is it possible to find monotone and linear so that ? I tried to look by supposing I have such a factorization, and as is a hyperplane, I have lines where is constant. I tried to use the monotonicity condition by trying to compare and over lines, but it didn't work.",f : \Bbb R^n \to \Bbb R h : \mathbb{R} \to \Bbb R l : \Bbb R^n \to \Bbb R f = h \circ l \ker l n - 1 f f(0),"['real-analysis', 'linear-algebra', 'monotone-functions']"
54,Difference in my and wolfram's integration.,Difference in my and wolfram's integration.,,"Calculate $$\int \frac{\sin ^3(x)+1}{\cos (x)+1} \, dx$$ Let $$u = \tan(x/2)$$ $\int \frac{\sin ^3(x)+1}{\cos (x)+1} \, dx = \int \frac{2\left(\frac{8u^3}{(u^2+1)^3}+1 \right)}{(u^2+1)\left( \frac{1-u^2}{u^2+1} + 1 \right)} \, du = \int \frac{8u^3}{(u^2+1)^3 } + 1 \, du$ $$ s := u^2 \wedge ds = 2u \, du $$ $ u + \int \frac{8su}{(s+1)^3 } (2u)^{-1} \, ds = u + 4\int \frac{s}{(s+1)^3 }  \, ds = $ $ u + 4\int \frac{1}{(s+1)^2 } - \frac{1}{(s+1)^3 }  \, ds =  u + 4\left(\frac{-1}{s+1} +\frac{1}{2(s+1)^2} \right) + C =$ $ u + 4\left(\frac{-1}{u^2+1} +\frac{1}{2(u^2+1)^2} \right)  + C = $ $ \tan(x/2) + 4\left(\frac{-1}{(\tan(x/2))^2+1} +\frac{1}{2((\tan(x/2))^2+1)^2} \right)  + C := F(x)$ $$F(\pi/2) - F(-\pi/2) = 2 $$ Wolfram tells that result ( $=2$ ) is okay, but my integral is different from wolfram result: $$ \frac{1}{8} \sec \left(\frac{x}{2}\right) \left(8 \sin \left(\frac{x}{2}\right)-4 \cos \left(\frac{x}{2}\right)-3 \cos \left(\frac{3 x}{2}\right)+\cos \left(\frac{5 x}{2}\right)\right) $$ Where I failed?","Calculate Let Wolfram tells that result ( ) is okay, but my integral is different from wolfram result: Where I failed?","\int \frac{\sin ^3(x)+1}{\cos (x)+1} \, dx u = \tan(x/2) \int \frac{\sin ^3(x)+1}{\cos (x)+1} \, dx = \int \frac{2\left(\frac{8u^3}{(u^2+1)^3}+1 \right)}{(u^2+1)\left( \frac{1-u^2}{u^2+1} + 1 \right)} \, du = \int \frac{8u^3}{(u^2+1)^3 } + 1 \, du  s := u^2 \wedge ds = 2u \, du   u + \int \frac{8su}{(s+1)^3 } (2u)^{-1} \, ds = u + 4\int \frac{s}{(s+1)^3 }  \, ds =   u + 4\int \frac{1}{(s+1)^2 } - \frac{1}{(s+1)^3 }  \, ds =  u + 4\left(\frac{-1}{s+1} +\frac{1}{2(s+1)^2} \right) + C =  u + 4\left(\frac{-1}{u^2+1} +\frac{1}{2(u^2+1)^2} \right)  + C =   \tan(x/2) + 4\left(\frac{-1}{(\tan(x/2))^2+1} +\frac{1}{2((\tan(x/2))^2+1)^2} \right)  + C := F(x) F(\pi/2) - F(-\pi/2) = 2  =2  \frac{1}{8} \sec \left(\frac{x}{2}\right) \left(8 \sin \left(\frac{x}{2}\right)-4 \cos \left(\frac{x}{2}\right)-3 \cos \left(\frac{3 x}{2}\right)+\cos \left(\frac{5 x}{2}\right)\right) ","['real-analysis', 'integration', 'indefinite-integrals', 'substitution']"
55,"Nonnegative derivative bounded by function, show that $f$ is zero","Nonnegative derivative bounded by function, show that  is zero",f,"I'm struggling to prove the following statement, which makes intuitive sense: Let $f: \mathbb{R} \to \mathbb{R}$ be differentiable. Suppose $\forall x \in \mathbb{R} \quad 0 \le f'(x) \le f(x)\,$ . If $f$ vanishes at some point, show that $f$ is identically zero. This is exercise 6 in section 6.7, in Elementary Classical Analysis by Mardsen & Hofmann. I've managed to do the following: Let $\omega \in \mathbb{R}$ be the point such that $f(\omega) = 0$ (which exists by hypothesis). Let $x < \omega$ . By Cauchy's Mean Value Theorem, $$ \exists \xi \in (x, \omega)\colon\quad 0 \le f(x) =f(x) - f(\omega) = f'(\xi)(x - \omega)\,. $$ By hypothesis, we have that $f'(\xi) \ge 0$ . On the other hand, $x - \omega < 0$ . Thus, $f'(\xi)(x - \omega) \le 0$ . By the above proposition, though, $ f'(\xi)(x - \omega) \ge 0\,$ . Therefore, $f'(\xi) = 0$ which implies $f(x) = 0$ . With this we have proven that $\forall x \in \mathbb{R}\quad x \le \omega \rightarrow f(x) = 0\,$ . But for $x > \omega$ , I haven't been able to prove much. I've got so far: Let $x > \omega$ . By Cauchy's Mean Value Theorem, $$ \exists \xi_1 \in (\omega, x)\colon\quad 0 \le f(x) = f(x) - f(\omega) = f'(\xi_1)(x - \omega) \le f(\xi_1)(x - \omega)\,. $$ If we apply this reasoning recursively on $f(\xi_1)$ , we get a succession $(\xi_i)_{i\in\mathbb{N}} \subseteq (\omega, x)$ such that $\forall i \in \mathbb{N}\quad \xi_{i+1} \in (\omega, \xi_i)$ and $$ \forall n \in \mathbb{N}\quad f(x) \le f(\xi_n) \prod_{i = 0}^{n - 1} (\xi_i - \omega) $$ where $\xi_0 = x$ . Is there a hint?","I'm struggling to prove the following statement, which makes intuitive sense: Let be differentiable. Suppose . If vanishes at some point, show that is identically zero. This is exercise 6 in section 6.7, in Elementary Classical Analysis by Mardsen & Hofmann. I've managed to do the following: Let be the point such that (which exists by hypothesis). Let . By Cauchy's Mean Value Theorem, By hypothesis, we have that . On the other hand, . Thus, . By the above proposition, though, . Therefore, which implies . With this we have proven that . But for , I haven't been able to prove much. I've got so far: Let . By Cauchy's Mean Value Theorem, If we apply this reasoning recursively on , we get a succession such that and where . Is there a hint?","f: \mathbb{R} \to \mathbb{R} \forall x \in \mathbb{R} \quad 0 \le f'(x) \le f(x)\, f f \omega \in \mathbb{R} f(\omega) = 0 x < \omega 
\exists \xi \in (x, \omega)\colon\quad 0 \le f(x) =f(x) - f(\omega) = f'(\xi)(x - \omega)\,.
 f'(\xi) \ge 0 x - \omega < 0 f'(\xi)(x - \omega) \le 0  f'(\xi)(x - \omega) \ge 0\, f'(\xi) = 0 f(x) = 0 \forall x \in \mathbb{R}\quad x \le \omega \rightarrow f(x) = 0\, x > \omega x > \omega 
\exists \xi_1 \in (\omega, x)\colon\quad 0 \le f(x) = f(x) - f(\omega) = f'(\xi_1)(x - \omega) \le f(\xi_1)(x - \omega)\,.
 f(\xi_1) (\xi_i)_{i\in\mathbb{N}} \subseteq (\omega, x) \forall i \in \mathbb{N}\quad \xi_{i+1} \in (\omega, \xi_i) 
\forall n \in \mathbb{N}\quad f(x) \le f(\xi_n) \prod_{i = 0}^{n - 1} (\xi_i - \omega)
 \xi_0 = x","['real-analysis', 'calculus', 'derivatives']"
56,Existence of $n$ distinct roots of a set of polynomials,Existence of  distinct roots of a set of polynomials,n,"The question is Let $n$ be a positive integer. Show that there are positive real numbers $a_0,a_1,\dots, a_n$ such that for each choice of signs the polynomial $$\pm a_{n} x^{n} \pm a_{n-1} x^{n-1}\pm a_{n-2} x^{n-2 }+\cdots \pm a_{1} x^{1}\pm a_{0} x^{0}$$ has $n$ distinct real roots. I cannot determine a condition to guarantee $n$ distinct real roots for a polynomial. One way for getting multiple roots is to use intermediate value theorem to produce at least one root in $(0,1),(1,2),(2,3), \dots, (n-1,n)$ , but we are changing signs of coefficient for the rest of polynomials so I am not able to proceed from here.","The question is Let be a positive integer. Show that there are positive real numbers such that for each choice of signs the polynomial has distinct real roots. I cannot determine a condition to guarantee distinct real roots for a polynomial. One way for getting multiple roots is to use intermediate value theorem to produce at least one root in , but we are changing signs of coefficient for the rest of polynomials so I am not able to proceed from here.","n a_0,a_1,\dots, a_n \pm a_{n} x^{n} \pm a_{n-1} x^{n-1}\pm a_{n-2} x^{n-2 }+\cdots \pm a_{1} x^{1}\pm a_{0} x^{0} n n (0,1),(1,2),(2,3), \dots, (n-1,n)","['real-analysis', 'abstract-algebra', 'algebra-precalculus', 'polynomials', 'galois-theory']"
57,Prove that there is a $m \in \mathbb{R}$ so that $f(f(m)) = m$,Prove that there is a  so that,m \in \mathbb{R} f(f(m)) = m,"The function $f : \mathbb{R} \to \mathbb{R}$ is continuous . Also $0\lt x_1 \lt x_2 \lt x_3 \lt x_4 \ $ and $f(x_1) = x_2 \ , \ f(x_2) = x_3 \ , \ f(x_3) = x_4 \ , \ f(x_4) = x_1 $ . Prove that there is a $m \in \mathbb{R}$ so that $f(f(m)) = m$ . My Try : Let $g(x) = f(x) - x $ and it is continuous over $\mathbb{R} $ . Furthermore , we have $g(x_1) \ , \ g(x_2) \ , \ g(x_3) \gt 0 $ and $g(x_4) \lt 0$ . The result is $f(c) = c$ for some $c$ between $x_3$ and $x_4$ . Applying $f$ to both sides yields to $f(f(c)) = f(c) = c$ . Is my solution correct ? Also write other solutions . Thanks in advance !","The function is continuous . Also and . Prove that there is a so that . My Try : Let and it is continuous over . Furthermore , we have and . The result is for some between and . Applying to both sides yields to . Is my solution correct ? Also write other solutions . Thanks in advance !","f : \mathbb{R} \to \mathbb{R} 0\lt x_1 \lt x_2 \lt x_3 \lt x_4 \  f(x_1) = x_2 \ , \ f(x_2) = x_3 \ , \ f(x_3) = x_4 \ , \ f(x_4) = x_1  m \in \mathbb{R} f(f(m)) = m g(x) = f(x) - x  \mathbb{R}  g(x_1) \ , \ g(x_2) \ , \ g(x_3) \gt 0  g(x_4) \lt 0 f(c) = c c x_3 x_4 f f(f(c)) = f(c) = c","['real-analysis', 'calculus', 'continuity', 'solution-verification']"
58,"Prove that $x^2$ and $x^3$ are topological conjugated on $[0, \infty)$.",Prove that  and  are topological conjugated on .,"x^2 x^3 [0, \infty)","As in the topic. I have to prove that the functions $x ^ 2$ and $x ^ 3$ are topologically conjugated. I tried to write it out by definition: $f (x ^ 2) = f (x) ^ 3$ and choose $f (x) = x ^ a$, but unfortunately it doesn't work. It's my beginnings in this field, so I do not have much experience yet. Do you have any hints?","As in the topic. I have to prove that the functions $x ^ 2$ and $x ^ 3$ are topologically conjugated. I tried to write it out by definition: $f (x ^ 2) = f (x) ^ 3$ and choose $f (x) = x ^ a$, but unfortunately it doesn't work. It's my beginnings in this field, so I do not have much experience yet. Do you have any hints?",,"['real-analysis', 'dynamical-systems']"
59,"$\forall x\in [0,1]$, if $f(t)f(t-x)$ is integrable, then $f(t)$ is integrable?",", if  is integrable, then  is integrable?","\forall x\in [0,1] f(t)f(t-x) f(t)","The original problem (from here problem 8) is: Let $\alpha>\frac{1}{2}$ be a real number. Prove that it is impossible to find a real function $f$ such that $$f(x)=1+\alpha\int_x^1 f(t)f(t-x) \, dt$$ holds for all $0\leq x\leq1$. My solution: Integrate both sides for $x$ from $0$ to $1$ and suppose $s=\int_0^1f(x) \, dx$. Then the equality becomes $s=1+\frac{1}{2}\alpha s^2$. Apply AM-GM to finish the proof. However, my solution works only when $f(x)$ is integrable (on $[0,1]$). My question is: if the equality in the problem holds, then $\forall x\in [0,1]$, $f(t)f(t-x)$ is integrable, so is it true that $f(x)$ is integrable? If it is not true, is there an explicit counterexample? For all ""integrable"" here, I think I mean ""Riemann integrable"". I have not studied the Lebesgue theory yet.","The original problem (from here problem 8) is: Let $\alpha>\frac{1}{2}$ be a real number. Prove that it is impossible to find a real function $f$ such that $$f(x)=1+\alpha\int_x^1 f(t)f(t-x) \, dt$$ holds for all $0\leq x\leq1$. My solution: Integrate both sides for $x$ from $0$ to $1$ and suppose $s=\int_0^1f(x) \, dx$. Then the equality becomes $s=1+\frac{1}{2}\alpha s^2$. Apply AM-GM to finish the proof. However, my solution works only when $f(x)$ is integrable (on $[0,1]$). My question is: if the equality in the problem holds, then $\forall x\in [0,1]$, $f(t)f(t-x)$ is integrable, so is it true that $f(x)$ is integrable? If it is not true, is there an explicit counterexample? For all ""integrable"" here, I think I mean ""Riemann integrable"". I have not studied the Lebesgue theory yet.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'riemann-integration']"
60,"Using different characterizations of compactness, continuity, etc.","Using different characterizations of compactness, continuity, etc.",,"I've been looking for simple problems online to improve my grasp on the basics of elementary analysis. I'm not sure how much context I should include to make this question understood, so I'll just include it all; my question will be at the very bottom of the post if the details aren't needed and you want to skip to it. I found the following problem, and thought of two ways to solve it: $\textbf{Problem}$: A real-valued function $f$ on a closed, bounded interval $[a,b]$ is said to be upper semicontinuous provided that for every $\epsilon>0$ and every $p\in[a,b]$, there is a $\delta=\delta(\epsilon,p)>0$ such if $x\in[a,b]$ and $|x-p|<\delta$ then $f(x)<f(p)+\epsilon$. Prove that an upper semicontinuous function is bounded above on $[a,b]$. $\underline{\text{First Proof}}$: The argument is by contradiction. Suppose that $f$ is not bounded above. Then, for each $n\in\mathbb{N}$, there exists $x_n$ such that $f(x_n)>n$. Since the sequence $(x_n)$ is bounded, it contains a convergent subsequence, $(x_{n_k})$, converging to some $x^*\in[a,b]$. Given $\epsilon>0$, there exists $\delta>0$ such that for all $x$ satisfying $|x-x^*|<\delta$, we have $f(x)<f(x^*)+\epsilon$. There exists $K$ such that for all $k\geq K$ we have $|x_{n_k}-x^*|<\delta$, so that $f(x_{n_k})<f(x^*)+\epsilon$ for all $k\geq K$. This is a contradiction, since we must have $f(x_{n_k})>n_k\geq k$ for all $k$. $\underline{\text{Second Proof}}$: Given an $\epsilon >0$, the set of open intervals $$\mathcal{O}=\big\{(p-\delta_{\epsilon,p}, p+\delta_{\epsilon,p}): p\in [a,b]\big\}$$ forms an open cover of $[a,b]$. Heine-Borel guarantees the existence of a finite subcover: $$[a,b] \subset \bigcup^{n}_{i=1}(p_i-\delta_{\epsilon,p_i}, p_i+\delta_{\epsilon, p_i}).$$ Let $f(p^*)=\max \{f(p_1),f(p_2),...,f(p_n)\}$. Then $f(x)<f(p^*)+\epsilon$ for all $x\in [a,b]$, so that $f(p^*)+\epsilon$ serves as an upper bound for $f$ on $[a,b]$. After doing the second proof and rereading the first proof, I realized (assuming the proofs are correct): Both rely on the compactness of the interval $[a,b]$. The first proof uses the sequential characterization of compactness; the second uses the finite subcover definition of compactness. I wondered: Can I use the sequential characterization to get a direct proof? I don't see how to do this, and doubt it can be done. So, here's my question: If a direct proof really $\textit{isn't}$ possible with the sequential characterization, is there some deeper reason for this? Does this suggest some underlying difference between the sequential characterization and the finite open subcover definition? More generally, it's impossible not to notice that there are often several equivalent characterizations of a single concept in analysis (e.g., the $\epsilon$-$\delta$ vs. the sequential vs. the open set definitions of continuity), and sometimes one is more appropriate than another. If it happens that for a given problem or a given theorem, one characterization of a concept only allows for indirect proof whereas another characterization allows for a direct proof, is there always some reason for this? Does this tell us anything significant about the differences between the characterizations, or about what assumptions underlie the different characterizations? I hope this question make sense. Please change the tag if another is more appropriate.","I've been looking for simple problems online to improve my grasp on the basics of elementary analysis. I'm not sure how much context I should include to make this question understood, so I'll just include it all; my question will be at the very bottom of the post if the details aren't needed and you want to skip to it. I found the following problem, and thought of two ways to solve it: $\textbf{Problem}$: A real-valued function $f$ on a closed, bounded interval $[a,b]$ is said to be upper semicontinuous provided that for every $\epsilon>0$ and every $p\in[a,b]$, there is a $\delta=\delta(\epsilon,p)>0$ such if $x\in[a,b]$ and $|x-p|<\delta$ then $f(x)<f(p)+\epsilon$. Prove that an upper semicontinuous function is bounded above on $[a,b]$. $\underline{\text{First Proof}}$: The argument is by contradiction. Suppose that $f$ is not bounded above. Then, for each $n\in\mathbb{N}$, there exists $x_n$ such that $f(x_n)>n$. Since the sequence $(x_n)$ is bounded, it contains a convergent subsequence, $(x_{n_k})$, converging to some $x^*\in[a,b]$. Given $\epsilon>0$, there exists $\delta>0$ such that for all $x$ satisfying $|x-x^*|<\delta$, we have $f(x)<f(x^*)+\epsilon$. There exists $K$ such that for all $k\geq K$ we have $|x_{n_k}-x^*|<\delta$, so that $f(x_{n_k})<f(x^*)+\epsilon$ for all $k\geq K$. This is a contradiction, since we must have $f(x_{n_k})>n_k\geq k$ for all $k$. $\underline{\text{Second Proof}}$: Given an $\epsilon >0$, the set of open intervals $$\mathcal{O}=\big\{(p-\delta_{\epsilon,p}, p+\delta_{\epsilon,p}): p\in [a,b]\big\}$$ forms an open cover of $[a,b]$. Heine-Borel guarantees the existence of a finite subcover: $$[a,b] \subset \bigcup^{n}_{i=1}(p_i-\delta_{\epsilon,p_i}, p_i+\delta_{\epsilon, p_i}).$$ Let $f(p^*)=\max \{f(p_1),f(p_2),...,f(p_n)\}$. Then $f(x)<f(p^*)+\epsilon$ for all $x\in [a,b]$, so that $f(p^*)+\epsilon$ serves as an upper bound for $f$ on $[a,b]$. After doing the second proof and rereading the first proof, I realized (assuming the proofs are correct): Both rely on the compactness of the interval $[a,b]$. The first proof uses the sequential characterization of compactness; the second uses the finite subcover definition of compactness. I wondered: Can I use the sequential characterization to get a direct proof? I don't see how to do this, and doubt it can be done. So, here's my question: If a direct proof really $\textit{isn't}$ possible with the sequential characterization, is there some deeper reason for this? Does this suggest some underlying difference between the sequential characterization and the finite open subcover definition? More generally, it's impossible not to notice that there are often several equivalent characterizations of a single concept in analysis (e.g., the $\epsilon$-$\delta$ vs. the sequential vs. the open set definitions of continuity), and sometimes one is more appropriate than another. If it happens that for a given problem or a given theorem, one characterization of a concept only allows for indirect proof whereas another characterization allows for a direct proof, is there always some reason for this? Does this tell us anything significant about the differences between the characterizations, or about what assumptions underlie the different characterizations? I hope this question make sense. Please change the tag if another is more appropriate.",,['real-analysis']
61,Why can I make the substitution $x = \sin \theta$?,Why can I make the substitution ?,x = \sin \theta,"For a dummy variable in an integral like $x$ in $$\int_a^b \frac{x^2}{1-x^2} \, dx$$ Why is it acceptable to make a substitution: let $ x = \sin \theta$  to get something like $$\int_a^b \frac{\sin^2\theta}{\cos^2\theta} \cos \theta \, d\theta$$ I am not interested in solving this problem but I am curious as to why the substitution is legal since $x$, depending on the boundaries can take on any value,  whereas $\left|\sin \theta \right| < 1$. So, why is it allowed, and are there restrictions on substitution (besides: continuous on that interval, defined on that interval)?","For a dummy variable in an integral like $x$ in $$\int_a^b \frac{x^2}{1-x^2} \, dx$$ Why is it acceptable to make a substitution: let $ x = \sin \theta$  to get something like $$\int_a^b \frac{\sin^2\theta}{\cos^2\theta} \cos \theta \, d\theta$$ I am not interested in solving this problem but I am curious as to why the substitution is legal since $x$, depending on the boundaries can take on any value,  whereas $\left|\sin \theta \right| < 1$. So, why is it allowed, and are there restrictions on substitution (besides: continuous on that interval, defined on that interval)?",,"['calculus', 'real-analysis']"
62,Showing that $\sum_{j=0}^{2n-1}{\cos^n(\frac{j\pi}{2n})(2\cos(\frac{2j\pi}n)+1)\cos(\frac{j\pi}2-\frac{2j\pi}n)}$ is never an integer for $n>10$,Showing that  is never an integer for,\sum_{j=0}^{2n-1}{\cos^n(\frac{j\pi}{2n})(2\cos(\frac{2j\pi}n)+1)\cos(\frac{j\pi}2-\frac{2j\pi}n)} n>10,"I want to show that $$f(n) = \sum_{j=0}^{2n-1}{\cos^n\left(  \frac{j \pi}{2n}\right) \left( 2\cos \left( \frac{2 j \pi}{n} \right) + 1\right) \cos \left( \frac{j \pi}{2} - \frac{2 j \pi}{n} \right)}$$ is an integer only for $n \in \{1,2,3,4,5, 10\}$ . What I have tried so far is: Calculate $f(n)$ for each $n$ up to $16$ . The values above are the only values I have found that are integers. At $n = 16$ , the value is $\frac{1941}{2048}$ , just under $1$ . By computing $f(n)$ for $16$ and up, I can see that it is always decreasing and therefore is $\lt 1$ . If I can show $\forall n \in \mathbb{N}, n\ge 16, f(n) \lt 1$ , then it follows that it can't be an integer. How do I show $f(n)$ is always decreasing (once $n$ gets big enough)?","I want to show that is an integer only for . What I have tried so far is: Calculate for each up to . The values above are the only values I have found that are integers. At , the value is , just under . By computing for and up, I can see that it is always decreasing and therefore is . If I can show , then it follows that it can't be an integer. How do I show is always decreasing (once gets big enough)?","f(n) = \sum_{j=0}^{2n-1}{\cos^n\left(  \frac{j \pi}{2n}\right) \left( 2\cos \left( \frac{2 j \pi}{n} \right) + 1\right) \cos \left( \frac{j \pi}{2} - \frac{2 j \pi}{n} \right)} n \in \{1,2,3,4,5, 10\} f(n) n 16 n = 16 \frac{1941}{2048} 1 f(n) 16 \lt 1 \forall n \in \mathbb{N}, n\ge 16, f(n) \lt 1 f(n) n","['real-analysis', 'trigonometry', 'summation', 'closed-form']"
63,Showing that the integral with respect to an abstract measure is infinite [duplicate],Showing that the integral with respect to an abstract measure is infinite [duplicate],,"This question already has answers here : A set with a finite integral of measure zero? (2 answers) Closed 6 years ago . Suppose $\mu$ is a measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. Then show that, for $\mu$-almost every $x$ $$  \int_\mathbb{R} \frac{1}{(x-t)^2}\,\mathrm{d}\mu(t) = \infty $$ I am not sure where to start. Any hints are welcome.","This question already has answers here : A set with a finite integral of measure zero? (2 answers) Closed 6 years ago . Suppose $\mu$ is a measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. Then show that, for $\mu$-almost every $x$ $$  \int_\mathbb{R} \frac{1}{(x-t)^2}\,\mathrm{d}\mu(t) = \infty $$ I am not sure where to start. Any hints are welcome.",,"['real-analysis', 'integration', 'analysis', 'measure-theory']"
64,How can we find $\lim_{n \rightarrow +\infty}\int_{1}^{n} \frac{\cos(nx)(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx $?,How can we find ?,\lim_{n \rightarrow +\infty}\int_{1}^{n} \frac{\cos(nx)(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx ,"I have a question about improper integrals: How can we find $\lim_{n \rightarrow +\infty} \int_{1}^{n} \frac{\cos(nx)(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx$? $\textbf{Some effort:}$ We know that $-1 \leq \cos(nx) \leq 1$, so we will try to sandwich it: $$-\lim_{n \rightarrow +\infty} \int_{1}^{n} \frac{(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx \leq \lim_{n \rightarrow +\infty} \int_{1}^{n} \frac{\cos(nx)(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx \leq \mbox{$\displaystyle\lim_{n \rightarrow +\infty} \int_{1}^{n} \frac{(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx$} $$ The next step is to get rid of the natural logarithm, which will be by using the change of variables and letting $\ln(x)=u$. But this makes the situation worse. I am interested of using again sandwich rule for natural logarithm as well. But I do not know that $\ln(x) < ?$ and $\ln(x) > ?$ for $x \in [0,+\infty]$? Can you please give me an idea? Thanks!","I have a question about improper integrals: How can we find $\lim_{n \rightarrow +\infty} \int_{1}^{n} \frac{\cos(nx)(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx$? $\textbf{Some effort:}$ We know that $-1 \leq \cos(nx) \leq 1$, so we will try to sandwich it: $$-\lim_{n \rightarrow +\infty} \int_{1}^{n} \frac{(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx \leq \lim_{n \rightarrow +\infty} \int_{1}^{n} \frac{\cos(nx)(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx \leq \mbox{$\displaystyle\lim_{n \rightarrow +\infty} \int_{1}^{n} \frac{(x-1)}{\sqrt{\ln^3(x)(1+x^4)}}dx$} $$ The next step is to get rid of the natural logarithm, which will be by using the change of variables and letting $\ln(x)=u$. But this makes the situation worse. I am interested of using again sandwich rule for natural logarithm as well. But I do not know that $\ln(x) < ?$ and $\ln(x) > ?$ for $x \in [0,+\infty]$? Can you please give me an idea? Thanks!",,"['real-analysis', 'integration', 'improper-integrals']"
65,If a function has an infinite amount of automorphisms would that imply that it is periodic?,If a function has an infinite amount of automorphisms would that imply that it is periodic?,,"Let $\phi(x)$ be continuous and differentiable everywhere and its automorphisms be bijections $\gamma_{n}$ such that $\phi(\gamma_{n}(x))=\phi(x)$. If the order of $Aut(\phi)$ is infinite, would that mean that $\phi(x)$ is periodic? I can think of an example of the top of my head: $\phi(x)=sin(x)$ with automorphisms of the form $\gamma_{n}(x)=x+2\pi n$. Could this be applied to any general function $\phi$?","Let $\phi(x)$ be continuous and differentiable everywhere and its automorphisms be bijections $\gamma_{n}$ such that $\phi(\gamma_{n}(x))=\phi(x)$. If the order of $Aut(\phi)$ is infinite, would that mean that $\phi(x)$ is periodic? I can think of an example of the top of my head: $\phi(x)=sin(x)$ with automorphisms of the form $\gamma_{n}(x)=x+2\pi n$. Could this be applied to any general function $\phi$?",,"['real-analysis', 'group-theory', 'periodic-functions', 'automorphism-group']"
66,"Calculate $\lim_{(x,y)\to(0,0)}\tan(x)\sin(\frac1{|x|+|y|})$",Calculate,"\lim_{(x,y)\to(0,0)}\tan(x)\sin(\frac1{|x|+|y|})","Calculate $\lim_\limits{(x,y)\to(0,0)}\tan(x)\sin\left(\dfrac1{|x|+|y|}\right)$ We know that $$-1\leq \sin\left(\dfrac{1}{|x|+|y|}\right)\leq 1$$ $$-\tan(x)\leq \tan(x)\sin\left(\dfrac{1}{|x|+|y|}\right)\leq \tan(x)$$ Taking the limit on both sides gives us $0$ , therefore the limit is $0$ . Does this work? I am doubtful of the fact that $\sin\left(\dfrac{1}{|x|+|y|}\right)$ is undefined, so I am not sure.","Calculate We know that Taking the limit on both sides gives us , therefore the limit is . Does this work? I am doubtful of the fact that is undefined, so I am not sure.","\lim_\limits{(x,y)\to(0,0)}\tan(x)\sin\left(\dfrac1{|x|+|y|}\right) -1\leq \sin\left(\dfrac{1}{|x|+|y|}\right)\leq 1 -\tan(x)\leq \tan(x)\sin\left(\dfrac{1}{|x|+|y|}\right)\leq \tan(x) 0 0 \sin\left(\dfrac{1}{|x|+|y|}\right)","['real-analysis', 'limits', 'multivariable-calculus', 'trigonometry']"
67,"Proof that $x^y + y^x > 1 \ \forall x,y > 0 $ [duplicate]",Proof that  [duplicate],"x^y + y^x > 1 \ \forall x,y > 0 ","This question already has answers here : A classic exponential inequality: $x^y+y^x>1$ [duplicate] (1 answer) Exponential teaser [closed] (1 answer) Closed 6 years ago . The problem is trivial if at least one of $x$ or $y$ is greater than $1$.  So all we need is to proof that $x^y+y^x > 1 \ \forall x,y \in (0,1)$.","This question already has answers here : A classic exponential inequality: $x^y+y^x>1$ [duplicate] (1 answer) Exponential teaser [closed] (1 answer) Closed 6 years ago . The problem is trivial if at least one of $x$ or $y$ is greater than $1$.  So all we need is to proof that $x^y+y^x > 1 \ \forall x,y \in (0,1)$.",,"['real-analysis', 'algebra-precalculus', 'inequality', 'exponential-function']"
68,"Completion of the space of piecewise-constant functions on $[0,1]$",Completion of the space of piecewise-constant functions on,"[0,1]","Let $L$ be the space of piecewise-constant functions on $[0,1]\subset \mathbb{R}$ equipped with the supremum norm (i.e. step functions). What is the completion of this space? We discussed in my class that all metric spaces have (unique) completion, but the proof of existence using equivalence classes doesn't give much mechanism to actually compute the completion. Intuitively, the space should contain all continuous functions, but it should be strictly larger as we can have a sequence $f_n \in L$ which is just the constant sequence of a function which is discontinuous. My guess would be $C[0,1] \cup \{f |  f $  is piecewise constant$ \}$, and I could probably show that $L$ is dense in this space, but I'm not sure how I'd go about showing that it is complete. Notably these functions are all integrable, so perhaps that's the completion - but we haven't discussed integrals yet in our course which makes me a bit hesitant.","Let $L$ be the space of piecewise-constant functions on $[0,1]\subset \mathbb{R}$ equipped with the supremum norm (i.e. step functions). What is the completion of this space? We discussed in my class that all metric spaces have (unique) completion, but the proof of existence using equivalence classes doesn't give much mechanism to actually compute the completion. Intuitively, the space should contain all continuous functions, but it should be strictly larger as we can have a sequence $f_n \in L$ which is just the constant sequence of a function which is discontinuous. My guess would be $C[0,1] \cup \{f |  f $  is piecewise constant$ \}$, and I could probably show that $L$ is dense in this space, but I'm not sure how I'd go about showing that it is complete. Notably these functions are all integrable, so perhaps that's the completion - but we haven't discussed integrals yet in our course which makes me a bit hesitant.",,[]
69,Finding the global minimum of $x^{n}+x^{n-1}+...+1$ for even $n.$,Finding the global minimum of  for even,x^{n}+x^{n-1}+...+1 n.,"Inspired by this question, I am curious if there is an asymptotic of the global minimum of the function: $$f_n(x) = x^{2n}+x^{2n-1}+...+x^2+x+1.$$ In the referred question, I showed that $$f_n(x) = x^{2n}+x^{2n-1}+...+x^2+x+1 =x^{2n-2}(x+\dfrac{1}{2})^2+\dfrac{3}{4}x^{2n-4}(x+\dfrac{2}{3})^2+\dfrac{4}{6}x^{2n-6}(x+\dfrac{3}{4})^2+\dots+ \dfrac{n+1}{2n}(x+\dfrac{n}{n+1})^2+\dfrac{n+2}{2n+2}> \dfrac{n+2}{2n+2}.$$   Since the method was completely elementary, I figured this was a loose bound. However, it turned out to be surprisingly close to the actual values for when $n=2,3$ and sharp $n=1.$ When $n=2$, the minimum is $0.673753\approx\dfrac{2}{3}=0.66$ and  when $n=3$, the minimum is $0.635\approx \dfrac{5}{8} = 0.625.$ Thus, is it possible to obtain an asymptotic for $\min_{x\in\mathbb{R}}f_n(x)$, as $n\to\infty?$  A closed form solution would be even better, but that just seem hopeless.","Inspired by this question, I am curious if there is an asymptotic of the global minimum of the function: $$f_n(x) = x^{2n}+x^{2n-1}+...+x^2+x+1.$$ In the referred question, I showed that $$f_n(x) = x^{2n}+x^{2n-1}+...+x^2+x+1 =x^{2n-2}(x+\dfrac{1}{2})^2+\dfrac{3}{4}x^{2n-4}(x+\dfrac{2}{3})^2+\dfrac{4}{6}x^{2n-6}(x+\dfrac{3}{4})^2+\dots+ \dfrac{n+1}{2n}(x+\dfrac{n}{n+1})^2+\dfrac{n+2}{2n+2}> \dfrac{n+2}{2n+2}.$$   Since the method was completely elementary, I figured this was a loose bound. However, it turned out to be surprisingly close to the actual values for when $n=2,3$ and sharp $n=1.$ When $n=2$, the minimum is $0.673753\approx\dfrac{2}{3}=0.66$ and  when $n=3$, the minimum is $0.635\approx \dfrac{5}{8} = 0.625.$ Thus, is it possible to obtain an asymptotic for $\min_{x\in\mathbb{R}}f_n(x)$, as $n\to\infty?$  A closed form solution would be even better, but that just seem hopeless.",,"['calculus', 'real-analysis', 'polynomials', 'optimization']"
70,Separating convex sets in Vector spaces,Separating convex sets in Vector spaces,,"This question just popped on my mind. Let $A, B$ two disjoint, nonempty convex sets in the vector space $X$ , can they be separated via a nonzero linear function in $X' = \{ f : X \to R  ~ | \quad \text{f is linear}  \} ?$ i.e., does there exist $f \in X' \setminus \{ 0\}$ such that $$          f(a) \leq   f(b)   \quad \forall a\in A, ~ \forall b \in B            $$ If not under what minimal condition one can separate them. My Thought  :  Since $A \cap  B = \emptyset  $ using Zorn Lemma we can find two disjoint maximal convex sets, say $U, ~ V$ such that $  A \subseteq U, ~ B \subseteq V $ and through maximality of $U, V$ we can deduce that $U \cup V = X$ in other words $U,~ V$ make a convex partition of the space. Now from this,  can we say that $U, ~V$ are two sides of a hyperplane ?  i.e., $$ U \subseteq \{ x \in  X ~ | \quad  f(x) \leq \alpha  \} , ~ V \subseteq \{ x \in  X ~ | \quad  f(x) \geq \alpha \} $$ for some $f \in X'$ and $\alpha \in \Bbb R$ Question #2: What if we assume $A, B$ are pointed cones with $A \cap B = \{0\}$ EDIT: I realized the answer of question # 1 is No generally see below link Can any two disjoint nonempty convex sets in a vector space be separated by a hyperplane? But Still any answer regarding minimal conditions that guarantees separation is my main interest, and an answer for question #2. Thank for your help.","This question just popped on my mind. Let two disjoint, nonempty convex sets in the vector space , can they be separated via a nonzero linear function in i.e., does there exist such that If not under what minimal condition one can separate them. My Thought  :  Since using Zorn Lemma we can find two disjoint maximal convex sets, say such that and through maximality of we can deduce that in other words make a convex partition of the space. Now from this,  can we say that are two sides of a hyperplane ?  i.e., for some and Question #2: What if we assume are pointed cones with EDIT: I realized the answer of question # 1 is No generally see below link Can any two disjoint nonempty convex sets in a vector space be separated by a hyperplane? But Still any answer regarding minimal conditions that guarantees separation is my main interest, and an answer for question #2. Thank for your help.","A, B X X' = \{ f : X \to R  ~ | \quad \text{f is linear}  \} ? f \in X' \setminus \{ 0\}           f(a) \leq   f(b)   \quad \forall a\in A, ~ \forall b \in B             A \cap  B = \emptyset   U, ~ V   A \subseteq U, ~ B \subseteq V  U, V U \cup V = X U,~ V U, ~V  U \subseteq \{ x \in  X ~ | \quad  f(x) \leq \alpha  \} , ~ V \subseteq \{ x \in  X ~ | \quad  f(x) \geq \alpha \}  f \in X' \alpha \in \Bbb R A, B A \cap B = \{0\}","['real-analysis', 'functional-analysis', 'convex-analysis']"
71,"Fractional integrals problem: $g(x)_{|_{[0,t]}}=\frac{(-1)^{\alpha-1}}{\Gamma(1-\alpha)}\int_x^t(y-x)^{-\alpha}h(y)\,dy$",Fractional integrals problem:,"g(x)_{|_{[0,t]}}=\frac{(-1)^{\alpha-1}}{\Gamma(1-\alpha)}\int_x^t(y-x)^{-\alpha}h(y)\,dy","Let's consider $0<\alpha<1/2$ and denote by $W_T^{1-\alpha,\infty}(0,T)$ the space of measurable functions $g:[0,T]\to\Bbb R$ such that $$ ||g||_{1-\alpha,\infty,T}:=\sup_{0<s<t<T}\left[\frac{|g(t)-g(s)|}{(t-s)^{1-\alpha}}+\int_s^t\frac{|g(y)-g(s)|}{(y-s)^{2-\alpha}}\,dy\right]<+\infty\;\;\;. $$ Moreover, we define the right sided Riemann-Liouville integral of order $1-\alpha$ of a function $f\in L^p(0,t)$, with $1\le p\le\infty$, as $$ I_{t-}^{1-\alpha}f(x):=\frac{(-1)^{\alpha-1}}{\Gamma(1-\alpha)}\int_x^t(y-x)^{-\alpha}f(y)\,dy\;\;\; $$ for a.a. $x\in[0,t]$. My problem is the following: in a paper by Nulart and Rascanu it is stated that, if $g\in W_T^{1-\alpha,\infty}(0,T)$ then its restriction to $[0,t]$ stays in $I_{t-}^{1-\alpha}(L^{\infty}(0,t))$ for all $0<t<T$; so in other words, given such $g$, there exists $h\in L^{\infty}(0,t)$ such that $$ g(x)|_{[0,t]}=I_{t-}^{1-\alpha}h(x)=\frac{(-1)^{\alpha-1}}{\Gamma(1-\alpha)}\int_x^t(y-x)^{-\alpha}h(y)\,dy\;\;\;.$$ It seems to me, that I DON'T HAVE to search explicitly the $h$ depending on the $g$ but rather I should use a theoretical argument, which proves the existence of such $h$; but I don't know how to do it. I'm quite lost, can someone shade a light please? EDIT: Obviously $\Gamma$ is the Euler Gamma function and $(-1)^{\alpha-1}=e^{i\pi(\alpha-1)}$, but these terms are constant, thus this doesn't play any relevant role here. SECOND EDIT: We can state the ""symmetric"" claim; the underlying duality could help. We denote by $W_0^{\alpha,1}(0,T)$ the space of measurable functions $f:[0,T]\to\Bbb R$ such that $$ ||f||_{\alpha,1}:=\int_0^T\frac{|f(s)|}{s^{\alpha}}\,ds+\int_0^T\int_0^s\frac{|f(s)-f(y)|}{(s-y)^{\alpha+1}}\,dyds<+\infty $$ As before we define the left sided Riemann-Liouville integral of order $\alpha$ of a function $f\in L^p(0,t)$, with $1\le p\le\infty$, as $$ I_{0+}^{\alpha}f(x):=\frac{1}{\Gamma(\alpha)}\int_0^x(x-y)^{\alpha-1}f(y)\,dy\;\;\; $$ for a.a. $x\in[0,t]$. Then if $g\in W_0^{\alpha,1}(0,T)$ then its restriction to $[0,t]$ stays in $I_{0+}^{\alpha}(L^1(0,t))$ for all $0<t<T$.","Let's consider $0<\alpha<1/2$ and denote by $W_T^{1-\alpha,\infty}(0,T)$ the space of measurable functions $g:[0,T]\to\Bbb R$ such that $$ ||g||_{1-\alpha,\infty,T}:=\sup_{0<s<t<T}\left[\frac{|g(t)-g(s)|}{(t-s)^{1-\alpha}}+\int_s^t\frac{|g(y)-g(s)|}{(y-s)^{2-\alpha}}\,dy\right]<+\infty\;\;\;. $$ Moreover, we define the right sided Riemann-Liouville integral of order $1-\alpha$ of a function $f\in L^p(0,t)$, with $1\le p\le\infty$, as $$ I_{t-}^{1-\alpha}f(x):=\frac{(-1)^{\alpha-1}}{\Gamma(1-\alpha)}\int_x^t(y-x)^{-\alpha}f(y)\,dy\;\;\; $$ for a.a. $x\in[0,t]$. My problem is the following: in a paper by Nulart and Rascanu it is stated that, if $g\in W_T^{1-\alpha,\infty}(0,T)$ then its restriction to $[0,t]$ stays in $I_{t-}^{1-\alpha}(L^{\infty}(0,t))$ for all $0<t<T$; so in other words, given such $g$, there exists $h\in L^{\infty}(0,t)$ such that $$ g(x)|_{[0,t]}=I_{t-}^{1-\alpha}h(x)=\frac{(-1)^{\alpha-1}}{\Gamma(1-\alpha)}\int_x^t(y-x)^{-\alpha}h(y)\,dy\;\;\;.$$ It seems to me, that I DON'T HAVE to search explicitly the $h$ depending on the $g$ but rather I should use a theoretical argument, which proves the existence of such $h$; but I don't know how to do it. I'm quite lost, can someone shade a light please? EDIT: Obviously $\Gamma$ is the Euler Gamma function and $(-1)^{\alpha-1}=e^{i\pi(\alpha-1)}$, but these terms are constant, thus this doesn't play any relevant role here. SECOND EDIT: We can state the ""symmetric"" claim; the underlying duality could help. We denote by $W_0^{\alpha,1}(0,T)$ the space of measurable functions $f:[0,T]\to\Bbb R$ such that $$ ||f||_{\alpha,1}:=\int_0^T\frac{|f(s)|}{s^{\alpha}}\,ds+\int_0^T\int_0^s\frac{|f(s)-f(y)|}{(s-y)^{\alpha+1}}\,dyds<+\infty $$ As before we define the left sided Riemann-Liouville integral of order $\alpha$ of a function $f\in L^p(0,t)$, with $1\le p\le\infty$, as $$ I_{0+}^{\alpha}f(x):=\frac{1}{\Gamma(\alpha)}\int_0^x(x-y)^{\alpha-1}f(y)\,dy\;\;\; $$ for a.a. $x\in[0,t]$. Then if $g\in W_0^{\alpha,1}(0,T)$ then its restriction to $[0,t]$ stays in $I_{0+}^{\alpha}(L^1(0,t))$ for all $0<t<T$.",,"['real-analysis', 'fractional-calculus']"
72,A Vite-like infinite product for $\sin (\pi/7)$,A Vite-like infinite product for,\sin (\pi/7),"In the article ""The unruly $\sin (\pi/7)$ of Samuel Moreno, the following infinite product is given: Also, the same article shows that $\sin (\pi/7)$ is equal to the infinite nested radical $ \sin (\pi/7) = \frac{1}{2}\sqrt{2 - \sqrt{2 - \sqrt{2 - \sqrt{2 + \sqrt{2 - \sqrt{2 - \sqrt{2 + \sqrt{2 - \sqrt{2 - \sqrt{2+...}}}}}}}}}}$ The question is: are these two expressions fundamentally different for $\sin(\pi/7)$, or is it possible to rewritte the infinite nested radical into that infinite product by simple algebraic manipulations?","In the article ""The unruly $\sin (\pi/7)$ of Samuel Moreno, the following infinite product is given: Also, the same article shows that $\sin (\pi/7)$ is equal to the infinite nested radical $ \sin (\pi/7) = \frac{1}{2}\sqrt{2 - \sqrt{2 - \sqrt{2 - \sqrt{2 + \sqrt{2 - \sqrt{2 - \sqrt{2 + \sqrt{2 - \sqrt{2 - \sqrt{2+...}}}}}}}}}}$ The question is: are these two expressions fundamentally different for $\sin(\pi/7)$, or is it possible to rewritte the infinite nested radical into that infinite product by simple algebraic manipulations?",,"['real-analysis', 'functions', 'trigonometry', 'infinite-product', 'nested-radicals']"
73,"Does $X_n+Y_n\rightarrow0$ almost surely (where $X_n,Y_n$ are i.i.d.) imply $X_n\rightarrow0$ almost surely",Does  almost surely (where  are i.i.d.) imply  almost surely,"X_n+Y_n\rightarrow0 X_n,Y_n X_n\rightarrow0","Let $\{X_n,Y_n:n\in\mathbb N\}$ be a sequence of independent random variables. Suppose that $X_n$ and $Y_n$ have the same distribution for every $n\in\mathbb N$. Prove that if $X_n+Y_n\rightarrow0$ almost surely, then $X_n\rightarrow0$ almost surely. I've tried  to use Borel-Cantelli Lemma to reduce this to prove that  $\sum_nP(|X_n+Y_n|>\epsilon)<\infty$ for any $\epsilon>0$ implies $\sum_nP(|X_n|>\epsilon)<\infty$ for any $\epsilon>0$. I observed that $P(|X_n|>\epsilon)=P(|X_n|>\epsilon,|X_n-Y_n|<\epsilon)+P(|X_n|>\epsilon,|X_n-Y_n|\geqslant\epsilon)$, and that the first term $P(|X_n|>\epsilon,|X_n-Y_n|<\epsilon)\leqslant P(|X_n+Y_n|>\epsilon)$. But I don't konw how to control the second term. Please feel free to discuss it. I am not sure I am heading a right way: I can't find a proper method to use the condition $X_n\stackrel{d}{=}Y_n$.","Let $\{X_n,Y_n:n\in\mathbb N\}$ be a sequence of independent random variables. Suppose that $X_n$ and $Y_n$ have the same distribution for every $n\in\mathbb N$. Prove that if $X_n+Y_n\rightarrow0$ almost surely, then $X_n\rightarrow0$ almost surely. I've tried  to use Borel-Cantelli Lemma to reduce this to prove that  $\sum_nP(|X_n+Y_n|>\epsilon)<\infty$ for any $\epsilon>0$ implies $\sum_nP(|X_n|>\epsilon)<\infty$ for any $\epsilon>0$. I observed that $P(|X_n|>\epsilon)=P(|X_n|>\epsilon,|X_n-Y_n|<\epsilon)+P(|X_n|>\epsilon,|X_n-Y_n|\geqslant\epsilon)$, and that the first term $P(|X_n|>\epsilon,|X_n-Y_n|<\epsilon)\leqslant P(|X_n+Y_n|>\epsilon)$. But I don't konw how to control the second term. Please feel free to discuss it. I am not sure I am heading a right way: I can't find a proper method to use the condition $X_n\stackrel{d}{=}Y_n$.",,"['real-analysis', 'probability', 'sequences-and-series', 'probability-theory', 'convergence-divergence']"
74,Find $\lim \limits_{n \to \infty}{1*4*7*\dots(3n+1) \over 2*5*8* \dots (3n+2)}$,Find,\lim \limits_{n \to \infty}{1*4*7*\dots(3n+1) \over 2*5*8* \dots (3n+2)},"I am trying to find $$\lim \limits_{n \to \infty}{1*4*7*\dots(3n+1) \over 2*5*8* \dots (3n+2)}$$ My first guess is to look at the reciprocal and isolate factors: $${2 \over 1}{5 \over 4}{8 \over 7} \dots {3n+2 \over 3n+1}= {\left(1+1\right)}\left(1+{1 \over 4}\right)\left(1+{1 \over 7}\right) \dots \left(1+{1 \over 3n+1}\right)$$ Now we take the natural log: $$\ln{ + \ln\left(1+1\right)} + \ln\left(1+{1 \over 4}\right) +\ln\left(1+{1 \over 7}\right) \dots +\ln\left(1+{1 \over 3n+1}\right)$$ Using $\ln(1+x) \le x$, we get: $$\ln{ + \ln\left(1+1\right)} + \ln\left(1+{1 \over 4}\right) +\ln\left(1+{1 \over 7}\right) \dots +\ln\left(1+{1 \over 3n+1}\right) \le 1 + {1 \over 4} + {1 \over 7} + \dots + {1 \over 3n+1}$$ Now, I'm stuck. I suppose I might use the fact that the RHS is similar to the harmonic series and show that it converges to some log, but I'm not sure how to do that. Do you have any clues?","I am trying to find $$\lim \limits_{n \to \infty}{1*4*7*\dots(3n+1) \over 2*5*8* \dots (3n+2)}$$ My first guess is to look at the reciprocal and isolate factors: $${2 \over 1}{5 \over 4}{8 \over 7} \dots {3n+2 \over 3n+1}= {\left(1+1\right)}\left(1+{1 \over 4}\right)\left(1+{1 \over 7}\right) \dots \left(1+{1 \over 3n+1}\right)$$ Now we take the natural log: $$\ln{ + \ln\left(1+1\right)} + \ln\left(1+{1 \over 4}\right) +\ln\left(1+{1 \over 7}\right) \dots +\ln\left(1+{1 \over 3n+1}\right)$$ Using $\ln(1+x) \le x$, we get: $$\ln{ + \ln\left(1+1\right)} + \ln\left(1+{1 \over 4}\right) +\ln\left(1+{1 \over 7}\right) \dots +\ln\left(1+{1 \over 3n+1}\right) \le 1 + {1 \over 4} + {1 \over 7} + \dots + {1 \over 3n+1}$$ Now, I'm stuck. I suppose I might use the fact that the RHS is similar to the harmonic series and show that it converges to some log, but I'm not sure how to do that. Do you have any clues?",,"['real-analysis', 'sequences-and-series', 'limits', 'infinite-product']"
75,Could Euclid have proven Dedekind's definition of real number multiplication?,Could Euclid have proven Dedekind's definition of real number multiplication?,,"In Euclid's day, the modern notion of real number did not exist; Euclid did not believe that the length of a line segment was a quantity measurable by number. But he did think it made sense to talk about the ratio of two lengths. In fact, he devotes Book V of his Elements to the study of such ratios, using the so-called Eudoxian theory of proportions. Here's how it works. Let $w$ and $x$ be two magnitudes of the same kind (for instance two lengths), and let $y$ and $z$ be two magnitudes of the same kind (for instance two areas). Then [according to Euclid,][2] the ratio of $w$ to $x$ is said to be equal to the ratio of $y$ to $z$ if for all positive integers $m$ and $n$, if $nw$ is greater, equal, or less than $mx$, then $ny$ is greater, equal, or less than $mz$, respectively.  Or to put it in modern language, $w/x = y/z$ if the same rational numbers $m/n$ are less than both, the same rational numbers are equal to both, and the same rational numbers are greater than both. In other words, a ratio is defined by the classes of rational numbers which are less than, equal to, and greater than it. If you've studied real analysis; this should look familiar to you: it is how the real number system is constructed using Dedekind cuts! In fact, Dedekind took the Eudoxian theory of proportions in Euclid's Book V as the inspiration for his Dedekind cut construction. So to sum up, while Euclid wouldn't have thought of them as numbers, his notion of ""ratios"" basically corresponds to our notion of ""positive real numbers"". Now with that background, my question is about the multiplication of real numbers.  Here is how Euclid defines the product of ratios: we say that the product of $w/x$ and $y/z$ is equal to $u/v$ if there exist magnitudes $r,s,$ and $t$ such that $w/x = r/s$, $y/z = s/t$, and $u/v = r/t$.  (This is well-defined by Euclid's proposition V.22 )  But this is not the standard way that multiplication is defined in the Dedekund cut construction of the real numbers, where you form a new cut by taking the products of the rational numbers in the two cuts that you're multiplying. So my question is, how can we prove than Euclid's definition of real number multiplication is equal to the Dedekind cut definition?  If I'm not mistaken, the problem basically reduces to proving the following: For all positive integers $l$, $m$, and $n$ and all magnitudes $x$, $y$, and $z$: If $l/m < x/y$ and $m/n < y/z$ then $l / n < x/ z$ If $l/m = x/y$ and $m/n = y/z$ then $l / n = x/ z$ If $l/m > x/y$ and $m/n > y/z$ then $l / n > x/ z$ And that in turn is equivalent to the following: For all positive integers $l$, $m$, and $n$ and all magnitudes $x$, $y$, and $z$: If $ly < mx$ and $mz < ny$ then $lz < nx$ If $ly = mx$ and $mz = ny$ then $lz = nx$ If $ly > mx$ and $mz > ny$ then $lz > nx$ So does anyone have any idea how to go about proving that?  For reference, addition of magnitudes is associative and commutative, and magnitudes also obey the following properties : V.1. Multiplication by numbers distributes over addition of magnitudes.   $m(x_1 + x_2 + ... + x_n) = m x_1 + m x_2 + ... + m x_n$ V.2. Multiplication by magnitudes distributes over addition of numbers.   $(m + n)x = mx + nx$ V.3. An associativity of multiplication.   $m(nx) = (mn)x$ V.5. Multiplication by numbers distributes over subtraction of magnitudes.   $m(x  y) = mx  my$ V.6. Uses multiplication by magnitudes distributes over subtraction of numbers.   $(m  n)x = mx  nx$ EDIT: Out of the three statements I wanted to prove, I just realized that Euclid proved statement 2 in his proposition V.22 . So now I just need to prove statements 1 and 3.","In Euclid's day, the modern notion of real number did not exist; Euclid did not believe that the length of a line segment was a quantity measurable by number. But he did think it made sense to talk about the ratio of two lengths. In fact, he devotes Book V of his Elements to the study of such ratios, using the so-called Eudoxian theory of proportions. Here's how it works. Let $w$ and $x$ be two magnitudes of the same kind (for instance two lengths), and let $y$ and $z$ be two magnitudes of the same kind (for instance two areas). Then [according to Euclid,][2] the ratio of $w$ to $x$ is said to be equal to the ratio of $y$ to $z$ if for all positive integers $m$ and $n$, if $nw$ is greater, equal, or less than $mx$, then $ny$ is greater, equal, or less than $mz$, respectively.  Or to put it in modern language, $w/x = y/z$ if the same rational numbers $m/n$ are less than both, the same rational numbers are equal to both, and the same rational numbers are greater than both. In other words, a ratio is defined by the classes of rational numbers which are less than, equal to, and greater than it. If you've studied real analysis; this should look familiar to you: it is how the real number system is constructed using Dedekind cuts! In fact, Dedekind took the Eudoxian theory of proportions in Euclid's Book V as the inspiration for his Dedekind cut construction. So to sum up, while Euclid wouldn't have thought of them as numbers, his notion of ""ratios"" basically corresponds to our notion of ""positive real numbers"". Now with that background, my question is about the multiplication of real numbers.  Here is how Euclid defines the product of ratios: we say that the product of $w/x$ and $y/z$ is equal to $u/v$ if there exist magnitudes $r,s,$ and $t$ such that $w/x = r/s$, $y/z = s/t$, and $u/v = r/t$.  (This is well-defined by Euclid's proposition V.22 )  But this is not the standard way that multiplication is defined in the Dedekund cut construction of the real numbers, where you form a new cut by taking the products of the rational numbers in the two cuts that you're multiplying. So my question is, how can we prove than Euclid's definition of real number multiplication is equal to the Dedekind cut definition?  If I'm not mistaken, the problem basically reduces to proving the following: For all positive integers $l$, $m$, and $n$ and all magnitudes $x$, $y$, and $z$: If $l/m < x/y$ and $m/n < y/z$ then $l / n < x/ z$ If $l/m = x/y$ and $m/n = y/z$ then $l / n = x/ z$ If $l/m > x/y$ and $m/n > y/z$ then $l / n > x/ z$ And that in turn is equivalent to the following: For all positive integers $l$, $m$, and $n$ and all magnitudes $x$, $y$, and $z$: If $ly < mx$ and $mz < ny$ then $lz < nx$ If $ly = mx$ and $mz = ny$ then $lz = nx$ If $ly > mx$ and $mz > ny$ then $lz > nx$ So does anyone have any idea how to go about proving that?  For reference, addition of magnitudes is associative and commutative, and magnitudes also obey the following properties : V.1. Multiplication by numbers distributes over addition of magnitudes.   $m(x_1 + x_2 + ... + x_n) = m x_1 + m x_2 + ... + m x_n$ V.2. Multiplication by magnitudes distributes over addition of numbers.   $(m + n)x = mx + nx$ V.3. An associativity of multiplication.   $m(nx) = (mn)x$ V.5. Multiplication by numbers distributes over subtraction of magnitudes.   $m(x  y) = mx  my$ V.6. Uses multiplication by magnitudes distributes over subtraction of numbers.   $(m  n)x = mx  nx$ EDIT: Out of the three statements I wanted to prove, I just realized that Euclid proved statement 2 in his proposition V.22 . So now I just need to prove statements 1 and 3.",,"['real-analysis', 'geometry', 'euclidean-geometry', 'math-history', 'real-numbers']"
76,Function measurable iff the components are?,Function measurable iff the components are?,,"If $\boldsymbol{f}:X\to\mathbb{R}^n$ is a $\mu$-measurable function, I think it is quite easy to see that its components $f_i$ also are. In fact, the projection $\pi_i:\mathbb{R}^n\to\mathbb{R}$, $\boldsymbol{x}\mapsto x_i$ is continuous and therefore Borel measurable, and $f_i=\pi_i\circ\boldsymbol{f} $. Therefore the counterimage through $\pi_i$ of a Borel subset of $\mathbb{R}$ is a Borel subset of $\mathbb{R}^n$, whose counterimage through $\boldsymbol{f}$ is a $\mu$-measurable subset of $X$. I cannot prove the converse, i.e. that, if $f_1,\ldots,f_n$ are $\mu$-measurable functions, then $\boldsymbol{f}=(f_1,\ldots,f_n)$ is, but I would not be amazed if it were true. Is it and, if it is, how can it be proved? I thank any answerer very much.","If $\boldsymbol{f}:X\to\mathbb{R}^n$ is a $\mu$-measurable function, I think it is quite easy to see that its components $f_i$ also are. In fact, the projection $\pi_i:\mathbb{R}^n\to\mathbb{R}$, $\boldsymbol{x}\mapsto x_i$ is continuous and therefore Borel measurable, and $f_i=\pi_i\circ\boldsymbol{f} $. Therefore the counterimage through $\pi_i$ of a Borel subset of $\mathbb{R}$ is a Borel subset of $\mathbb{R}^n$, whose counterimage through $\boldsymbol{f}$ is a $\mu$-measurable subset of $X$. I cannot prove the converse, i.e. that, if $f_1,\ldots,f_n$ are $\mu$-measurable functions, then $\boldsymbol{f}=(f_1,\ldots,f_n)$ is, but I would not be amazed if it were true. Is it and, if it is, how can it be proved? I thank any answerer very much.",,"['real-analysis', 'measure-theory']"
77,"Inf-convolution, two basic questions?","Inf-convolution, two basic questions?",,"Let $E$ be a normed vector space. Given two functions $\varphi$, $\psi : E \to (-\infty, +\infty]$, one defines the inf-convolution of $\varphi$ and $\psi$ as follows: for every $x \in E$, let$$(\varphi \Box \psi)(x) = \inf_{x \in E} \{\varphi(x - y) + \psi(y)\}.$$I have two quetsions. Let $\varphi$, $\psi: E \to (-\infty, +\infty]$ be functions such that $D(\varphi^*) \cap D(\psi^*) \neq \emptyset$. How do I see that$$\text{epist}(\varphi \Box \psi) = (\text{epist}\,\varphi) + (\text{epist}\,\psi)?$$ If $\varphi$, $\psi: E \to (-\infty, +\infty]$ are convex functions such that $D(\varphi^*) \cap D(\psi^*) \neq \emptyset$, does it follow that $(\varphi \Box \psi)$ is a convex function? Notation. We denote by $D(\varphi)$ the domain of $\varphi$, that is,$$D(\varphi) = \{x \in E: \varphi(x) < +\infty\}.$$Given a function $\varphi: E \to (-\infty, +\infty]$, set$$\text{epist}\,\varphi = \{(x, \lambda) \in E \times \mathbb{R}: \varphi(x) < \lambda\}.$$","Let $E$ be a normed vector space. Given two functions $\varphi$, $\psi : E \to (-\infty, +\infty]$, one defines the inf-convolution of $\varphi$ and $\psi$ as follows: for every $x \in E$, let$$(\varphi \Box \psi)(x) = \inf_{x \in E} \{\varphi(x - y) + \psi(y)\}.$$I have two quetsions. Let $\varphi$, $\psi: E \to (-\infty, +\infty]$ be functions such that $D(\varphi^*) \cap D(\psi^*) \neq \emptyset$. How do I see that$$\text{epist}(\varphi \Box \psi) = (\text{epist}\,\varphi) + (\text{epist}\,\psi)?$$ If $\varphi$, $\psi: E \to (-\infty, +\infty]$ are convex functions such that $D(\varphi^*) \cap D(\psi^*) \neq \emptyset$, does it follow that $(\varphi \Box \psi)$ is a convex function? Notation. We denote by $D(\varphi)$ the domain of $\varphi$, that is,$$D(\varphi) = \{x \in E: \varphi(x) < +\infty\}.$$Given a function $\varphi: E \to (-\infty, +\infty]$, set$$\text{epist}\,\varphi = \{(x, \lambda) \in E \times \mathbb{R}: \varphi(x) < \lambda\}.$$",,"['calculus', 'real-analysis', 'functional-analysis', 'convex-analysis', 'convolution']"
78,"Prove that the Dirichlet function does not have a limit at $0$, with epsilon and delta","Prove that the Dirichlet function does not have a limit at , with epsilon and delta",0,"I am doing a question (q-39 from ch 2.4- Stewart's Calculus) that asks me to prove that for the function $$f(x)=\begin{cases} 0, & \text{if } x \text{ is rational} \\  1, & \text{if } x \text{ is irrational}  \end{cases}$$ the $\lim_{x\to 0}f(x)$  does not exist. I am stuck because I have not dealt with the limits of conditional functions using delta epsilon. I understand that the start of the proof would be to assume $|f(x)-L|<\epsilon$. However the solution set suggests that it follows that $\epsilon=1/2$. How do I get $\epsilon=1/2$ from $|f(x)-L|<\epsilon$?","I am doing a question (q-39 from ch 2.4- Stewart's Calculus) that asks me to prove that for the function $$f(x)=\begin{cases} 0, & \text{if } x \text{ is rational} \\  1, & \text{if } x \text{ is irrational}  \end{cases}$$ the $\lim_{x\to 0}f(x)$  does not exist. I am stuck because I have not dealt with the limits of conditional functions using delta epsilon. I understand that the start of the proof would be to assume $|f(x)-L|<\epsilon$. However the solution set suggests that it follows that $\epsilon=1/2$. How do I get $\epsilon=1/2$ from $|f(x)-L|<\epsilon$?",,"['real-analysis', 'limits', 'epsilon-delta']"
79,"Relationship among the function spaces $C_c^\infty(\Omega)$, $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$","Relationship among the function spaces ,  and",C_c^\infty(\Omega) C_c^\infty(\overline{\Omega}) C_c^\infty(\Bbb{R}^d),"I have seen the spaces $C_c^\infty(\Omega)$, $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ a lot in theorems regarding PDE where $\Omega$ denotes some  open subset of $\Bbb{R}^d$. There is no doubt about the definitions of $C_c^\infty(\Omega)$ and $C_c^\infty(\Bbb{R}^d)$. But I'm not very clear about the relationships among these three spaces. Here are my questions : What is the definition for $C_c^\infty(\overline{\Omega})$? If one says it consists of functions $f:\overline{\Omega}\to\Bbb{R}$ such that $f$ is smooth (infinitely differentiable) and with compact support, my question concerns the value of $f$ on $\partial\Omega\setminus\Omega$. (I would really appreciate if one could also come up with a reference for definiton of this space. ) Could one come up with an example illustrating the difference between $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Omega)$? Are $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ ""essentially"" the same? If so, how? (I've seen these two spaces in different books regarding a same theorem.) [Added:] In the book Navier-Stokes Equations--- Theory and Numerical Analysis by Temam, the author defines (page 3) $\mathcal{D}(\Omega)$ (or $\mathcal{D}(\overline{\Omega})$) be the space of $C^\infty$ functions with compact support contained in $\Omega$ (or $\overline{\Omega}$). I have replaced the symbol $\mathcal{D}$ with $C_c^\infty$ here.","I have seen the spaces $C_c^\infty(\Omega)$, $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ a lot in theorems regarding PDE where $\Omega$ denotes some  open subset of $\Bbb{R}^d$. There is no doubt about the definitions of $C_c^\infty(\Omega)$ and $C_c^\infty(\Bbb{R}^d)$. But I'm not very clear about the relationships among these three spaces. Here are my questions : What is the definition for $C_c^\infty(\overline{\Omega})$? If one says it consists of functions $f:\overline{\Omega}\to\Bbb{R}$ such that $f$ is smooth (infinitely differentiable) and with compact support, my question concerns the value of $f$ on $\partial\Omega\setminus\Omega$. (I would really appreciate if one could also come up with a reference for definiton of this space. ) Could one come up with an example illustrating the difference between $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Omega)$? Are $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ ""essentially"" the same? If so, how? (I've seen these two spaces in different books regarding a same theorem.) [Added:] In the book Navier-Stokes Equations--- Theory and Numerical Analysis by Temam, the author defines (page 3) $\mathcal{D}(\Omega)$ (or $\mathcal{D}(\overline{\Omega})$) be the space of $C^\infty$ functions with compact support contained in $\Omega$ (or $\overline{\Omega}$). I have replaced the symbol $\mathcal{D}$ with $C_c^\infty$ here.",,"['real-analysis', 'functional-analysis']"
80,How to prove the matrix fractional function is convex by definition,How to prove the matrix fractional function is convex by definition,,"It is well known that the matrix fractional function $f(\mathbf{w},\boldsymbol{\Omega})=\mathbf{w}^T\boldsymbol{\Omega}^{-1}\mathbf{w}$ is jointly convex with respect to $\mathbf{w}$ and $\boldsymbol{\Omega}$ for $\mathbf{w}\in\mathbb{R}^n$ and $\boldsymbol{\Omega}\in\mathbb{S}_+^{n\times n}$, where $\mathbb{S}_+^{n\times n}$ denotes the set of all $n\times n$ positive definite matrix. Now I want to prove it based on the definition of convex functions. That is, I need to prove that the following inequality holds for any $\mathbf{w}_1,\mathbf{w}_2\in\mathbb{R}^n$, $\boldsymbol{\Omega}_1,\boldsymbol{\Omega}_2\in\mathbb{S}^{n\times n}_+$, and $\alpha\in[0,1]$: $$\alpha\mathbf{w}_1^T\boldsymbol{\Omega}_1^{-1}\mathbf{w}_1+\beta\mathbf{w}_2^T\boldsymbol{\Omega}_2^{-1}\mathbf{w}_2-(\alpha\mathbf{w}_1+\beta\mathbf{w}_2)^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}(\alpha\mathbf{w}_1+\beta\mathbf{w}_2)\ge 0,$$ where $\beta=1-\alpha$. In order to prove this inequality, I first simplify the left-hand side as \begin{align*} &\alpha\mathbf{w}_1^T\boldsymbol{\Omega}_1^{-1}\mathbf{w}_1+\beta\mathbf{w}_2^T\boldsymbol{\Omega}_2^{-1}\mathbf{w}_2-(\alpha\mathbf{w}_1+\beta\mathbf{w}_2)^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}(\alpha\mathbf{w}_1+\beta\mathbf{w}_2)\\ =&(\alpha\mathbf{w}_1^T\boldsymbol{\Omega}_1^{-1}\mathbf{w}_1-\alpha^2\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_1)+(\beta\mathbf{w}_2^T\boldsymbol{\Omega}_2^{-1}\mathbf{w}_2-\beta^2\mathbf{w}_2^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2)\\ &-2\alpha\beta\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2\\ =&\alpha\beta\mathbf{w}_1^T\boldsymbol{\Omega}_1^{-1}\boldsymbol{\Omega}_2(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_1+\alpha\beta\mathbf{w}_2^T\boldsymbol{\Omega}_2^{-1}\boldsymbol{\Omega}_1(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2\\ &-2\alpha\beta\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2\\ =&\alpha\beta\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1\boldsymbol{\Omega}_2^{-1}\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_1)^{-1}\mathbf{w}_1+\alpha\beta\mathbf{w}_2^T(\alpha\boldsymbol{\Omega}_2+\beta\boldsymbol{\Omega}_2\boldsymbol{\Omega}_1^{-1}\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2\\ &-2\alpha\beta\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2 \end{align*} I cannot prove the inequality based on this simplification. Is there any way to prove the inequality? Thanks.","It is well known that the matrix fractional function $f(\mathbf{w},\boldsymbol{\Omega})=\mathbf{w}^T\boldsymbol{\Omega}^{-1}\mathbf{w}$ is jointly convex with respect to $\mathbf{w}$ and $\boldsymbol{\Omega}$ for $\mathbf{w}\in\mathbb{R}^n$ and $\boldsymbol{\Omega}\in\mathbb{S}_+^{n\times n}$, where $\mathbb{S}_+^{n\times n}$ denotes the set of all $n\times n$ positive definite matrix. Now I want to prove it based on the definition of convex functions. That is, I need to prove that the following inequality holds for any $\mathbf{w}_1,\mathbf{w}_2\in\mathbb{R}^n$, $\boldsymbol{\Omega}_1,\boldsymbol{\Omega}_2\in\mathbb{S}^{n\times n}_+$, and $\alpha\in[0,1]$: $$\alpha\mathbf{w}_1^T\boldsymbol{\Omega}_1^{-1}\mathbf{w}_1+\beta\mathbf{w}_2^T\boldsymbol{\Omega}_2^{-1}\mathbf{w}_2-(\alpha\mathbf{w}_1+\beta\mathbf{w}_2)^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}(\alpha\mathbf{w}_1+\beta\mathbf{w}_2)\ge 0,$$ where $\beta=1-\alpha$. In order to prove this inequality, I first simplify the left-hand side as \begin{align*} &\alpha\mathbf{w}_1^T\boldsymbol{\Omega}_1^{-1}\mathbf{w}_1+\beta\mathbf{w}_2^T\boldsymbol{\Omega}_2^{-1}\mathbf{w}_2-(\alpha\mathbf{w}_1+\beta\mathbf{w}_2)^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}(\alpha\mathbf{w}_1+\beta\mathbf{w}_2)\\ =&(\alpha\mathbf{w}_1^T\boldsymbol{\Omega}_1^{-1}\mathbf{w}_1-\alpha^2\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_1)+(\beta\mathbf{w}_2^T\boldsymbol{\Omega}_2^{-1}\mathbf{w}_2-\beta^2\mathbf{w}_2^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2)\\ &-2\alpha\beta\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2\\ =&\alpha\beta\mathbf{w}_1^T\boldsymbol{\Omega}_1^{-1}\boldsymbol{\Omega}_2(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_1+\alpha\beta\mathbf{w}_2^T\boldsymbol{\Omega}_2^{-1}\boldsymbol{\Omega}_1(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2\\ &-2\alpha\beta\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2\\ =&\alpha\beta\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1\boldsymbol{\Omega}_2^{-1}\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_1)^{-1}\mathbf{w}_1+\alpha\beta\mathbf{w}_2^T(\alpha\boldsymbol{\Omega}_2+\beta\boldsymbol{\Omega}_2\boldsymbol{\Omega}_1^{-1}\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2\\ &-2\alpha\beta\mathbf{w}_1^T(\alpha\boldsymbol{\Omega}_1+\beta\boldsymbol{\Omega}_2)^{-1}\mathbf{w}_2 \end{align*} I cannot prove the inequality based on this simplification. Is there any way to prove the inequality? Thanks.",,"['real-analysis', 'convex-analysis', 'convex-optimization']"
81,prove the sequence is increasing,prove the sequence is increasing,,"Im asked to show the sequence $a_{n+1}=\sqrt{3+2a_{n}}$ where $a_{1}=0, a_{2}=1$ is increasing and bounded and therefore convergent. I don't even know how to start the proof. Im sure it increases because its essentially adding a really small number each time and the number gets very small since its being square rooted a bunch of times but i'm having a real issue showing that it actually is increasing for all n. $$ a_{2}=\sqrt{3} \\ a_{3}=\sqrt{3+2\sqrt{3}} \\ a_{4}=\sqrt{3+2\sqrt{3+2\sqrt{3}}} $$ Everything I try to do leads nowhere and makes no sense. I've seen some proofs that go about by squaring both sides then solving the resulting quadratic equation assuming the sequence has a limit but im not sure how that shows that its increasing. If anyone has any tips that might push my in the right direction it would be greatly appreciated.","Im asked to show the sequence $a_{n+1}=\sqrt{3+2a_{n}}$ where $a_{1}=0, a_{2}=1$ is increasing and bounded and therefore convergent. I don't even know how to start the proof. Im sure it increases because its essentially adding a really small number each time and the number gets very small since its being square rooted a bunch of times but i'm having a real issue showing that it actually is increasing for all n. $$ a_{2}=\sqrt{3} \\ a_{3}=\sqrt{3+2\sqrt{3}} \\ a_{4}=\sqrt{3+2\sqrt{3+2\sqrt{3}}} $$ Everything I try to do leads nowhere and makes no sense. I've seen some proofs that go about by squaring both sides then solving the resulting quadratic equation assuming the sequence has a limit but im not sure how that shows that its increasing. If anyone has any tips that might push my in the right direction it would be greatly appreciated.",,"['real-analysis', 'sequences-and-series', 'recurrence-relations']"
82,Prove that the set of orthogonal matrices is compact,Prove that the set of orthogonal matrices is compact,,Let the set of all $n \times n$ matrices (denoted by $M_n(\mathbb R)$ ) be a metric space. Show that set of all orthogonal matrices is compact. My attempt: well i am beginner in real analysis. compact means every open cover has a finite subcover. so here i have to find finite subcover for set of orthogonal matrices.showing closed and bounded may help i guess. but is there a easy way to do it?,Let the set of all $n \times n$ matrices (denoted by $M_n(\mathbb R)$ ) be a metric space. Show that set of all orthogonal matrices is compact. My attempt: well i am beginner in real analysis. compact means every open cover has a finite subcover. so here i have to find finite subcover for set of orthogonal matrices.showing closed and bounded may help i guess. but is there a easy way to do it?,,"['real-analysis', 'general-topology', 'matrices', 'compactness', 'orthogonal-matrices']"
83,Radon-Nikodym derivative of sum of two measures,Radon-Nikodym derivative of sum of two measures,,"Problem Statement: Suppose that $\mu$ and $\nu$ are two finite measures such that $\nu \ll \mu$, let $\rho = \mu + \nu$, and note that since $\mu(A) \le \rho(A)$, and $\nu(A) \le \rho(A)$, we have $\mu \ll \rho$ and $\nu \ll \rho$. Prove that if $f = d\mu/d\rho$ and $g = d\nu/d\rho$, then $f +g = 1$ and $d\nu = (g/f) \, d\mu$. My attempt at a solution: For the first part, I said $$\mu(A) = \int_A f \, d\rho \ \ \ \text{and} \ \ \ \nu(A) = \int_A g \, d\rho,$$ therefore, $\mu(A) + \nu(A) = \int_A (f+g) \, d\rho$. Now, $\mu(A) + \nu(A) = (\mu+\nu)(A)$, and $\mu + \nu$ is another finite measure, so we have that $f+g$ is the Radon-Nikodym derivative of $\mu + \nu$ w.r.t $\rho$, written as $f + g = \frac{d(\mu+\nu)}{d\rho}$. Now, we consider $\int_A 1 \, d\rho = \int \chi_A \, d\rho = \rho(A) = \mu(A) + \nu(A) = (\mu+\nu)(A)$, and by the uniqueness of the Radon-Nikodym derivative, $f + g = 1$ a.e. However, I am not sure how to prove that $f+ g = 1$ everywhere - am I going about this proof completely wrong, or is there something else I need to do from here to show this? For this next part, I am not entirely sure what the question is asking. We know that there is some non-negative, integrable w.r.t. $\mu$ function $h$ such that $\nu(A) = \int_A h \, d\mu$. Are we being asked to prove that $h = g/f$? If so, it would be easy to write something like $\int_A (g/f) \, d\mu = \int_A\frac{d\nu \, d\rho}{d\rho \, d\mu} \, d\mu = \int_A \, d\nu = \nu(A)$, thus showing that $g/f = h$ a.e.; however, I don't know that we can manipulate Radon-Nikodym derivatives in the way, yet, so there must be something else going on here, and I don't know how to go about proving it!","Problem Statement: Suppose that $\mu$ and $\nu$ are two finite measures such that $\nu \ll \mu$, let $\rho = \mu + \nu$, and note that since $\mu(A) \le \rho(A)$, and $\nu(A) \le \rho(A)$, we have $\mu \ll \rho$ and $\nu \ll \rho$. Prove that if $f = d\mu/d\rho$ and $g = d\nu/d\rho$, then $f +g = 1$ and $d\nu = (g/f) \, d\mu$. My attempt at a solution: For the first part, I said $$\mu(A) = \int_A f \, d\rho \ \ \ \text{and} \ \ \ \nu(A) = \int_A g \, d\rho,$$ therefore, $\mu(A) + \nu(A) = \int_A (f+g) \, d\rho$. Now, $\mu(A) + \nu(A) = (\mu+\nu)(A)$, and $\mu + \nu$ is another finite measure, so we have that $f+g$ is the Radon-Nikodym derivative of $\mu + \nu$ w.r.t $\rho$, written as $f + g = \frac{d(\mu+\nu)}{d\rho}$. Now, we consider $\int_A 1 \, d\rho = \int \chi_A \, d\rho = \rho(A) = \mu(A) + \nu(A) = (\mu+\nu)(A)$, and by the uniqueness of the Radon-Nikodym derivative, $f + g = 1$ a.e. However, I am not sure how to prove that $f+ g = 1$ everywhere - am I going about this proof completely wrong, or is there something else I need to do from here to show this? For this next part, I am not entirely sure what the question is asking. We know that there is some non-negative, integrable w.r.t. $\mu$ function $h$ such that $\nu(A) = \int_A h \, d\mu$. Are we being asked to prove that $h = g/f$? If so, it would be easy to write something like $\int_A (g/f) \, d\mu = \int_A\frac{d\nu \, d\rho}{d\rho \, d\mu} \, d\mu = \int_A \, d\nu = \nu(A)$, thus showing that $g/f = h$ a.e.; however, I don't know that we can manipulate Radon-Nikodym derivatives in the way, yet, so there must be something else going on here, and I don't know how to go about proving it!",,"['real-analysis', 'analysis', 'probability-theory', 'measure-theory']"
84,Series convergence proof review,Series convergence proof review,,"Problem: If $\sum a_n$ converges, and if $\{b_n\}$ is monotonic and bounded, prove that $\sum a_n b_n$ converges. My attempt: The partial sums of $\sum a_n$ form a bounded sequence (if not $\sum a_n$ wouldn't converge). Also, $\{b_n\}$ converges to some limit $b$. Suppose $\{b_n\}$ is monotonic decreasing (proof is similar in other case). Then $\forall n$, $b_n - b \geq 0$. Set $c_n = b_n - b$. Then $\{c_n\}$ is monotonic decreasing and $\lim_{n\to\infty}c_n = 0$. Theorem 3.42 in Baby Rudin:   Suppose (a) the partial sums of $\sum a_n$ form a bounded sequence; (b) $b_0 \geq b_1 \geq b_2 \geq \cdots$; (c) $\lim_{n\to\infty}b_n = 0.$ Then $\sum a_n b_n$ converges. So $\sum a_n c_n$ converges. $\sum a_n c_n = \sum a_n (b_n - b) = \sum a_n b_n - \sum a_n b$ so $$\sum a_n b_n = \sum a_n c_n + b\sum a_n.$$ As the sum of convergent series, $\sum a_n b_n$ converges. Please point out errors. Thanks in advance :)","Problem: If $\sum a_n$ converges, and if $\{b_n\}$ is monotonic and bounded, prove that $\sum a_n b_n$ converges. My attempt: The partial sums of $\sum a_n$ form a bounded sequence (if not $\sum a_n$ wouldn't converge). Also, $\{b_n\}$ converges to some limit $b$. Suppose $\{b_n\}$ is monotonic decreasing (proof is similar in other case). Then $\forall n$, $b_n - b \geq 0$. Set $c_n = b_n - b$. Then $\{c_n\}$ is monotonic decreasing and $\lim_{n\to\infty}c_n = 0$. Theorem 3.42 in Baby Rudin:   Suppose (a) the partial sums of $\sum a_n$ form a bounded sequence; (b) $b_0 \geq b_1 \geq b_2 \geq \cdots$; (c) $\lim_{n\to\infty}b_n = 0.$ Then $\sum a_n b_n$ converges. So $\sum a_n c_n$ converges. $\sum a_n c_n = \sum a_n (b_n - b) = \sum a_n b_n - \sum a_n b$ so $$\sum a_n b_n = \sum a_n c_n + b\sum a_n.$$ As the sum of convergent series, $\sum a_n b_n$ converges. Please point out errors. Thanks in advance :)",,['real-analysis']
85,Finding a better upper bound for an integral of a product of $n$ terms,Finding a better upper bound for an integral of a product of  terms,n,"So I'm trying to find and upper bound for the integral $$ \int\limits_{a}^b \! (x-x_1)^2 \cdots (x-x_n)^2\, \mathrm{d}x, $$ where $x_i \in [a,b], \enspace \forall i=1,\dots ,n.$ I've tried evaluating a single facotr $(x-x_i)^2$ but all I've gotten was the obvious one: $$ (x-x_i)^2 \leq (b-a)^2, \enspace \forall i=1,\dots,n, $$ which as a result gives me: $$ \int\limits_{a}^b \! (x-x_1)^2 \cdots (x-x_n)^2\, \mathrm{d}x \leq (b-a)^{2n+1}. $$ That bound is not that good for what I need. Any suggestions on how to make it better?","So I'm trying to find and upper bound for the integral $$ \int\limits_{a}^b \! (x-x_1)^2 \cdots (x-x_n)^2\, \mathrm{d}x, $$ where $x_i \in [a,b], \enspace \forall i=1,\dots ,n.$ I've tried evaluating a single facotr $(x-x_i)^2$ but all I've gotten was the obvious one: $$ (x-x_i)^2 \leq (b-a)^2, \enspace \forall i=1,\dots,n, $$ which as a result gives me: $$ \int\limits_{a}^b \! (x-x_1)^2 \cdots (x-x_n)^2\, \mathrm{d}x \leq (b-a)^{2n+1}. $$ That bound is not that good for what I need. Any suggestions on how to make it better?",,"['real-analysis', 'integration', 'functional-analysis', 'definite-integrals', 'estimation']"
86,"If a function is left- and right-differentiable everywhere, how much can the one-sided derivatives disagree?","If a function is left- and right-differentiable everywhere, how much can the one-sided derivatives disagree?",,"Just for fun, I was proving some results about convex functions the other day. I was able to show that for a convex set $E\subseteq\Bbb R,$ if $f:E\to\Bbb R$ is convex, then $f$ is left- and right-differentiable (and so continuous) on the interior of $E$ (though it needn't even be continuous at any boundary points contained in $E$). Moreover, if $x_0$ is an interior point of $E,$ then the left derivative of $f$ at $x_0$ is no greater than the right derivative of $f$ at $x_0.$ I am aware (though I haven't had a chance to try to prove) that the left and right derivative of such a function $f$ disagree at no more than countably-many points, but that got me wondering about a more general result. Given an open convex subset $E\subseteq\Bbb R$ and a (not necessarily convex) function $f:E\to\Bbb R$ such that $f$ is left- and right- differentiable at every point of $E,$ can we conclude that $f$ fails to be differentiable at no more than countably-many points of $E,$ or do we need more information about $f$ to get there? What if we know that the right derivative dominates the left derivative everywhere? Is it possible that the right derivative strictly dominates the left derivative everywhere?","Just for fun, I was proving some results about convex functions the other day. I was able to show that for a convex set $E\subseteq\Bbb R,$ if $f:E\to\Bbb R$ is convex, then $f$ is left- and right-differentiable (and so continuous) on the interior of $E$ (though it needn't even be continuous at any boundary points contained in $E$). Moreover, if $x_0$ is an interior point of $E,$ then the left derivative of $f$ at $x_0$ is no greater than the right derivative of $f$ at $x_0.$ I am aware (though I haven't had a chance to try to prove) that the left and right derivative of such a function $f$ disagree at no more than countably-many points, but that got me wondering about a more general result. Given an open convex subset $E\subseteq\Bbb R$ and a (not necessarily convex) function $f:E\to\Bbb R$ such that $f$ is left- and right- differentiable at every point of $E,$ can we conclude that $f$ fails to be differentiable at no more than countably-many points of $E,$ or do we need more information about $f$ to get there? What if we know that the right derivative dominates the left derivative everywhere? Is it possible that the right derivative strictly dominates the left derivative everywhere?",,"['calculus', 'real-analysis', 'derivatives']"
87,If $f:I\to \mathbb{R}$ is differentiable and $f'$ is monotone nondecreasing then $f$ is convex.,If  is differentiable and  is monotone nondecreasing then  is convex.,f:I\to \mathbb{R} f' f,Let $I \subseteq \Bbb{R}$ be an open interval. If $f:I\to \mathbb{R}$ is differentiable and $f'$ is monotone nondecreasing then $f$ is convex. I would like a hint to solve this exercise. I applied the mean value theorem but I failed to solve it. Thanks.,Let $I \subseteq \Bbb{R}$ be an open interval. If $f:I\to \mathbb{R}$ is differentiable and $f'$ is monotone nondecreasing then $f$ is convex. I would like a hint to solve this exercise. I applied the mean value theorem but I failed to solve it. Thanks.,,['real-analysis']
88,Convergence in $L^1_{loc}$ implies convergence almost everywhere,Convergence in  implies convergence almost everywhere,L^1_{loc},"Let $f_n\in L^1_{loc}(\mathbb{R})$ be a sequence of a locally integrable functions such that for all $a<b$  $$\int_a^b|f_n(x)|dx\to 0,$$ when $n\to\infty$. We know that for each interval $[a,b]$ there exists a subsequence $(f_{p_n})$ which converges pointwise a.e. on $[a,b]$ to $0$. But can we construct a subsequence which converges a.e. on $\mathbb{R}$. My problem is that the subsequence $(f_{p_n})$ depends on the interval $[a,b]$. I don't even know if it is possible to do this or not.","Let $f_n\in L^1_{loc}(\mathbb{R})$ be a sequence of a locally integrable functions such that for all $a<b$  $$\int_a^b|f_n(x)|dx\to 0,$$ when $n\to\infty$. We know that for each interval $[a,b]$ there exists a subsequence $(f_{p_n})$ which converges pointwise a.e. on $[a,b]$ to $0$. But can we construct a subsequence which converges a.e. on $\mathbb{R}$. My problem is that the subsequence $(f_{p_n})$ depends on the interval $[a,b]$. I don't even know if it is possible to do this or not.",,"['real-analysis', 'functional-analysis', 'convergence-divergence', 'lebesgue-integral', 'lp-spaces']"
89,Let $f$ be a function such that every point of discontinuity is a removable discontinuity. Prove that $g(x)= \lim_{y\to x}f(y)$ is continuous.,Let  be a function such that every point of discontinuity is a removable discontinuity. Prove that  is continuous.,f g(x)= \lim_{y\to x}f(y),"Let $f$ be a function with the property that every point of discontinuity is a removable discontinuity, i.e., $\lim_{y \to x}f(y)$ exists for all $x$, but $f$ may be discontinuous at some (even infinitely many) numbers $x$. Define $g(x)= \lim_{y\to x}f(y)$. Prove that $g$ is continuous. Since $g(a)= \lim_{y\to a}f(y)$, by definition, it follows that for any $\epsilon \gt 0$ there is a $\delta \gt 0$ such that $|f(y)-g(a)|\lt \epsilon$ for $0\lt |y-a| \lt \delta$. This means that $$g(a)-\epsilon \lt f(y) \lt g(a)+\epsilon$$. for $0\lt |y-a| \lt \delta$. What I need to show is that if $|x-a| \lt \delta$, we have $$g(a)-\epsilon \lt \lim_{y\to x}f(y) \lt g(a)+\epsilon$$. I'm having trouble reaching this conclusion with the information I have. I'd appreciate any solutions or suggestions.","Let $f$ be a function with the property that every point of discontinuity is a removable discontinuity, i.e., $\lim_{y \to x}f(y)$ exists for all $x$, but $f$ may be discontinuous at some (even infinitely many) numbers $x$. Define $g(x)= \lim_{y\to x}f(y)$. Prove that $g$ is continuous. Since $g(a)= \lim_{y\to a}f(y)$, by definition, it follows that for any $\epsilon \gt 0$ there is a $\delta \gt 0$ such that $|f(y)-g(a)|\lt \epsilon$ for $0\lt |y-a| \lt \delta$. This means that $$g(a)-\epsilon \lt f(y) \lt g(a)+\epsilon$$. for $0\lt |y-a| \lt \delta$. What I need to show is that if $|x-a| \lt \delta$, we have $$g(a)-\epsilon \lt \lim_{y\to x}f(y) \lt g(a)+\epsilon$$. I'm having trouble reaching this conclusion with the information I have. I'd appreciate any solutions or suggestions.",,"['calculus', 'real-analysis', 'analysis', 'continuity']"
90,Show that any solution of second order differential equation has atmost a countable number of zeroes $?$,Show that any solution of second order differential equation has atmost a countable number of zeroes,?,"Question : Consider the second order differential equation $y''(t) + a(t) y'(t) + b(t) y(t) = 0$. Then any solution of the second order differential equation  has atmost a countable number of zeroes on $[a , b]$ . What I have tried: Let $S$ be the set of zeroes of $y(t)$. If $S$ is finite then there is nothing to prove. Let $S$ be infinite . I can easily prove that every zero of $y(t)$ is isolated. Since $[a ,b]$ is closed and bounded and $S$ is an ordered set,  So we can find a minimum element of $S$ say $t_1$ so that for  $t_1 \in S $, there exists a $\delta_{t_1}$ such that in $( t_1 - \delta_{t_1} , t_1 + \delta_{t_1})$, $y(t)$ has no zeroes other than $t_1$ Let $ t_2$  be the minimum of $S-\{t_1\}$ of $y(t)$ . So   for  $t_2 \in S $, there exists a $\delta_{t_2}$ such that in $( t_2 - \delta_{t_2} , t_2 + \delta_{t_2})$, $y(t)$ has no zeroes other than $t_2$ and $( t_2 - \delta_{t_2} , t_2 + \delta_{t_2}) \cap( t_2 - \delta_{t_2} , t_2 + \delta_{t_2}) = \phi$. Similarly we can proceed. Thus we can correspond for every $t \in S$ a rational number $ q_t $ which belongs to $( t - \delta , t + \delta)$ and since rational numbers are countable, so $S$ is countable. Please check my soution, i f you think any correction is required, please tell me. Thank you.","Question : Consider the second order differential equation $y''(t) + a(t) y'(t) + b(t) y(t) = 0$. Then any solution of the second order differential equation  has atmost a countable number of zeroes on $[a , b]$ . What I have tried: Let $S$ be the set of zeroes of $y(t)$. If $S$ is finite then there is nothing to prove. Let $S$ be infinite . I can easily prove that every zero of $y(t)$ is isolated. Since $[a ,b]$ is closed and bounded and $S$ is an ordered set,  So we can find a minimum element of $S$ say $t_1$ so that for  $t_1 \in S $, there exists a $\delta_{t_1}$ such that in $( t_1 - \delta_{t_1} , t_1 + \delta_{t_1})$, $y(t)$ has no zeroes other than $t_1$ Let $ t_2$  be the minimum of $S-\{t_1\}$ of $y(t)$ . So   for  $t_2 \in S $, there exists a $\delta_{t_2}$ such that in $( t_2 - \delta_{t_2} , t_2 + \delta_{t_2})$, $y(t)$ has no zeroes other than $t_2$ and $( t_2 - \delta_{t_2} , t_2 + \delta_{t_2}) \cap( t_2 - \delta_{t_2} , t_2 + \delta_{t_2}) = \phi$. Similarly we can proceed. Thus we can correspond for every $t \in S$ a rational number $ q_t $ which belongs to $( t - \delta , t + \delta)$ and since rational numbers are countable, so $S$ is countable. Please check my soution, i f you think any correction is required, please tell me. Thank you.",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
91,"if $f(x)$ is even and can be infinitely differentiable, how about $f(\sqrt{x})$","if  is even and can be infinitely differentiable, how about",f(x) f(\sqrt{x}),"I have a question $f(x)$ is even and can be infinitely differentiable, how about $f(\sqrt{x})$ in [0,$\infty$)? can we say that the  $f(\sqrt{x})$ also can be infinitely differentiable in $[0,\infty)$. My thoughts: let $g(x)=f(\sqrt{x})$, and then I have proved $g(x)$ can be differentiable and continuous in $[0,\infty)$, and I supposed this conclusion still holds for $n=k$($k$ is an integer),which means $g(x)$ can be $k$-times differentiable and continous in $[0,\infty)$, so I want to use mathematical induction to prove it. But I am stuck here, since I have no idea how to prove g can be $(k+1)$ times differentiable using the assumption that $g$ can be $k$-times differentiable and continuous. Can someone tell me whether it is true or not?","I have a question $f(x)$ is even and can be infinitely differentiable, how about $f(\sqrt{x})$ in [0,$\infty$)? can we say that the  $f(\sqrt{x})$ also can be infinitely differentiable in $[0,\infty)$. My thoughts: let $g(x)=f(\sqrt{x})$, and then I have proved $g(x)$ can be differentiable and continuous in $[0,\infty)$, and I supposed this conclusion still holds for $n=k$($k$ is an integer),which means $g(x)$ can be $k$-times differentiable and continous in $[0,\infty)$, so I want to use mathematical induction to prove it. But I am stuck here, since I have no idea how to prove g can be $(k+1)$ times differentiable using the assumption that $g$ can be $k$-times differentiable and continuous. Can someone tell me whether it is true or not?",,"['calculus', 'real-analysis', 'analysis']"
92,A function that crosses each horizontal line only finitely many times,A function that crosses each horizontal line only finitely many times,,"Consider a real function $f(x)$ (not necessarily continuous) defined on a finite interval. Given a constant $C$, divide the interval to sub-intervals such that, in every sub-interval, either $f(x)<C$ or $f(x)>C$ (where the points $f(x)=C$ are ignored). Let $N(f,C)$ be the smallest number of sub-intervals in such a division. Informally, $N(f,C)$ is approximately the number of times that the function $y=f(x)$ ""crosses"" the horiznotal line $y=C$, where ""crosses"" means that it goes from being below the line to being above the line or vice versa. For example: If $f(x)=\sin(x)$ defined on the interval $[-\pi,\pi]$, then $N(f,0)=2$, since $f(x)$ is negative on $(\pi,0)$ and positive on $(0,\pi)$. If $f(x)=x\cdot \sin(1/x)$ on the same interval then then $N(f,0)=\infty$, since this function crosses the line y=0 infinitely many times . What term describes the functions for which $N(f,C)$ is finite for every $C$ and on any interval?","Consider a real function $f(x)$ (not necessarily continuous) defined on a finite interval. Given a constant $C$, divide the interval to sub-intervals such that, in every sub-interval, either $f(x)<C$ or $f(x)>C$ (where the points $f(x)=C$ are ignored). Let $N(f,C)$ be the smallest number of sub-intervals in such a division. Informally, $N(f,C)$ is approximately the number of times that the function $y=f(x)$ ""crosses"" the horiznotal line $y=C$, where ""crosses"" means that it goes from being below the line to being above the line or vice versa. For example: If $f(x)=\sin(x)$ defined on the interval $[-\pi,\pi]$, then $N(f,0)=2$, since $f(x)$ is negative on $(\pi,0)$ and positive on $(0,\pi)$. If $f(x)=x\cdot \sin(1/x)$ on the same interval then then $N(f,0)=\infty$, since this function crosses the line y=0 infinitely many times . What term describes the functions for which $N(f,C)$ is finite for every $C$ and on any interval?",,"['real-analysis', 'terminology']"
93,Marginal of Dirichlet distribution is Beta (integral),Marginal of Dirichlet distribution is Beta (integral),,"Just for the sake of simplicity, take $K=3$ then a random vector $(X_1,X_2,X_3)$ has a Dirichlet distribution, i.e. $(X_1,X_2,X_3)\sim Dirichlet(\alpha_1,\alpha_2,\alpha_3)$ if the density takes the form: $$p(x_1, x_2) \propto x_1^{\alpha_1-1} x_2^{\alpha_2-1} (1-x_1-x_2)^{\alpha_3-1}.$$ The support is the $2$-dimensional simplex: $0<x_1,x_2,x_3<1$, $x_1+x_2+x_3=1$. The dependence is just on two variables because of the restriction $x_1+x_2+x_3=1$. So one can ask about the distribution of $X_1$ and $X_2$. My question is: How to prove (easily) that the marginals are Beta distributed. For instance, $X_1\sim Beta ( \tilde{\alpha}_1, \tilde{\alpha}_2)$ and what are the parameters? It should be: $$p(x_1) = \int_0^1 p(x_1,x_2)dx_2 \propto x_1^{\alpha_1-1}\int_0^1 x_2^{\alpha_2-1} (1-x_1-x_2)^{\alpha_3-1} dx_2.$$ It is the latter integral I struggle with: I tried expanding the power using Newton's binomial but I get weird things. Does anyone see the mistake/way to proceed or have an easy proof? Thanks a lot!","Just for the sake of simplicity, take $K=3$ then a random vector $(X_1,X_2,X_3)$ has a Dirichlet distribution, i.e. $(X_1,X_2,X_3)\sim Dirichlet(\alpha_1,\alpha_2,\alpha_3)$ if the density takes the form: $$p(x_1, x_2) \propto x_1^{\alpha_1-1} x_2^{\alpha_2-1} (1-x_1-x_2)^{\alpha_3-1}.$$ The support is the $2$-dimensional simplex: $0<x_1,x_2,x_3<1$, $x_1+x_2+x_3=1$. The dependence is just on two variables because of the restriction $x_1+x_2+x_3=1$. So one can ask about the distribution of $X_1$ and $X_2$. My question is: How to prove (easily) that the marginals are Beta distributed. For instance, $X_1\sim Beta ( \tilde{\alpha}_1, \tilde{\alpha}_2)$ and what are the parameters? It should be: $$p(x_1) = \int_0^1 p(x_1,x_2)dx_2 \propto x_1^{\alpha_1-1}\int_0^1 x_2^{\alpha_2-1} (1-x_1-x_2)^{\alpha_3-1} dx_2.$$ It is the latter integral I struggle with: I tried expanding the power using Newton's binomial but I get weird things. Does anyone see the mistake/way to proceed or have an easy proof? Thanks a lot!",,"['calculus', 'real-analysis', 'probability', 'integration', 'probability-distributions']"
94,Approximating $L^p$ functions using Schwartz functions with compact support on the Fourier side,Approximating  functions using Schwartz functions with compact support on the Fourier side,L^p,"For $1\leq p<\infty$, how would you show for any $f\in L^p(\mathbb{R})$ and given $\epsilon>0$, there exists $L<\infty$ and $g\in \mathcal{S}(\mathbb{R})$ such that $\|f-g\|_p<\epsilon$ and $\operatorname{supp}(\widehat{g})\subset [-L, L]$ We know that $C^\infty_c$ is dense subset of $S$ and $L^p$ but then we cannot make sure the Fourier transform of $g$ is compactly supported.","For $1\leq p<\infty$, how would you show for any $f\in L^p(\mathbb{R})$ and given $\epsilon>0$, there exists $L<\infty$ and $g\in \mathcal{S}(\mathbb{R})$ such that $\|f-g\|_p<\epsilon$ and $\operatorname{supp}(\widehat{g})\subset [-L, L]$ We know that $C^\infty_c$ is dense subset of $S$ and $L^p$ but then we cannot make sure the Fourier transform of $g$ is compactly supported.",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'measure-theory', 'harmonic-analysis']"
95,"Prove that for any $n$, we have $\sum_{i=1}^n \frac{a_i}i \ge a_n $.","Prove that for any , we have .",n \sum_{i=1}^n \frac{a_i}i \ge a_n ,"Consider the sequence $(a_n)_{n \ge1}$ of real nos such that $$a_{m+n}\le a_m+a_n,\ \forall m,n \ge 1$$ Prove that for any $n$, we have $$\sum_{i=1}^n \frac{a_i}i \ge a_n .$$","Consider the sequence $(a_n)_{n \ge1}$ of real nos such that $$a_{m+n}\le a_m+a_n,\ \forall m,n \ge 1$$ Prove that for any $n$, we have $$\sum_{i=1}^n \frac{a_i}i \ge a_n .$$",,"['real-analysis', 'sequences-and-series']"
96,A question of rationality,A question of rationality,,This problem was asked to me by a friend and I simply have no idea about it. So I have not progressed a single bit. The problem is this:  If $f :\mathbb{R}\to \mathbb{R}$ is an infinitely differentiable function and $f(x)\in\mathbb{Q} \;\forall x\in\mathbb{Q}$ then must $f'(x)$ be rational for all rational $x$ ?,This problem was asked to me by a friend and I simply have no idea about it. So I have not progressed a single bit. The problem is this:  If $f :\mathbb{R}\to \mathbb{R}$ is an infinitely differentiable function and $f(x)\in\mathbb{Q} \;\forall x\in\mathbb{Q}$ then must $f'(x)$ be rational for all rational $x$ ?,,"['calculus', 'real-analysis', 'functions']"
97,Definition of completion of a measure space,Definition of completion of a measure space,,"On a measure space $(\Omega,\mathcal{A},\mu)$ a completion of a measure is defined as: $\{A: A_1\subset A\subset A_2$ with $A_1,A_2\in\mathcal{A}$ and $\mu(A_2\backslash A_1)=0\}$ I'm trying to show that this set is equivalent to: $\{A\cup N:A\in\mathcal{A}$, and $N\subset B\in\mathcal{A}$ for some $B$ with $\mu(B)=0\}$ and $\{A\triangle N:A\in\mathcal{A}$, and $N\subset B\in\mathcal{A}$ for some $B$ with $\mu(B)=0\}$ where $\triangle$ is the symmetric difference. To show that the first definition is equivalent to the second, I showed that some $A_a$ in the first set is in the second set (which was pretty easy since $A_a\in\mathcal{A}$ implies that it is in $\mathcal{A}\cup N$. However, I am having trouble going the other direction. For the third set, I'm having a hard time seeing the picture. Thanks for your help.","On a measure space $(\Omega,\mathcal{A},\mu)$ a completion of a measure is defined as: $\{A: A_1\subset A\subset A_2$ with $A_1,A_2\in\mathcal{A}$ and $\mu(A_2\backslash A_1)=0\}$ I'm trying to show that this set is equivalent to: $\{A\cup N:A\in\mathcal{A}$, and $N\subset B\in\mathcal{A}$ for some $B$ with $\mu(B)=0\}$ and $\{A\triangle N:A\in\mathcal{A}$, and $N\subset B\in\mathcal{A}$ for some $B$ with $\mu(B)=0\}$ where $\triangle$ is the symmetric difference. To show that the first definition is equivalent to the second, I showed that some $A_a$ in the first set is in the second set (which was pretty easy since $A_a\in\mathcal{A}$ implies that it is in $\mathcal{A}\cup N$. However, I am having trouble going the other direction. For the third set, I'm having a hard time seeing the picture. Thanks for your help.",,"['real-analysis', 'measure-theory']"
98,What exactly is a modulus of continuity?,What exactly is a modulus of continuity?,,"This is my first post on here, so forgive me if I am ignorant of certain customs. I am currently reading Courant & John's Introduction to Calculus and Analysis Volume I . Unfortunately, I have stumbled upon an ambiguous notation on page 41. It begins: Our definition of continuity of the function $f(x)$ at $x_0$ requires that for every degree of precision $\epsilon > 0$ there exists quantities $\delta \gt 0$ (so-called moduli of continuity) such that $|f(x)-f(x_0)| < \epsilon$ for all $x$ in the domain of $f$ for which $|x-x_0| < \delta$ . He then goes on to say that (for ""regular continuity""): In general, as our example show, this $\delta = \delta(\epsilon)$ depends not only on $\epsilon$ but also on the value of $x_0$ . What exactly is meant by a modulus of continuity and how is it related to $\delta(\epsilon)$ ? Edit - Upon further inspection I have noted that the ""modulus of continuity"" depends on both $x_0$ and $\epsilon$ for ""regular continuity"", however only depends on $\epsilon$ for uniform continuity. I believe that some of the confusion has been cleared by the notation of $\delta(\epsilon)$ . However, he states that moduli of continuity are simply quantities $\delta > 0$ . Is this ""it"", or is there further discussion to be had?","This is my first post on here, so forgive me if I am ignorant of certain customs. I am currently reading Courant & John's Introduction to Calculus and Analysis Volume I . Unfortunately, I have stumbled upon an ambiguous notation on page 41. It begins: Our definition of continuity of the function at requires that for every degree of precision there exists quantities (so-called moduli of continuity) such that for all in the domain of for which . He then goes on to say that (for ""regular continuity""): In general, as our example show, this depends not only on but also on the value of . What exactly is meant by a modulus of continuity and how is it related to ? Edit - Upon further inspection I have noted that the ""modulus of continuity"" depends on both and for ""regular continuity"", however only depends on for uniform continuity. I believe that some of the confusion has been cleared by the notation of . However, he states that moduli of continuity are simply quantities . Is this ""it"", or is there further discussion to be had?",f(x) x_0 \epsilon > 0 \delta \gt 0 |f(x)-f(x_0)| < \epsilon x f |x-x_0| < \delta \delta = \delta(\epsilon) \epsilon x_0 \delta(\epsilon) x_0 \epsilon \epsilon \delta(\epsilon) \delta > 0,"['real-analysis', 'continuity']"
99,Measure zero and compact then content zero,Measure zero and compact then content zero,,"I'm trying to show that a compact set $E$ with measure zero has content zero. It seems simple because for every $\varepsilon$ I take a subcover of the rectangles, but the issue I'm facing here is that I can't take a subcover because the rectangles are closed. I tried using the interior but what guaranties that for every $\varepsilon > 0$ I'll have a set of rectangles such that $E \subset\bigcup_{k \in \Bbb{N}} R_k^\circ $ and $\sum_{k=1}^\infty \mu (R_k) < \varepsilon$. Definition measure zero for every $\varepsilon >0$ we have a set of closed rectangles such that $E \subset\bigcup_{k \in \Bbb{N}} R_k$ and $\sum_{k=1}^\infty \mu (R_k) < \varepsilon$. Content zero is the same but with finitely many rectangles. Note I don't know any measure theoretic concepts besides those.","I'm trying to show that a compact set $E$ with measure zero has content zero. It seems simple because for every $\varepsilon$ I take a subcover of the rectangles, but the issue I'm facing here is that I can't take a subcover because the rectangles are closed. I tried using the interior but what guaranties that for every $\varepsilon > 0$ I'll have a set of rectangles such that $E \subset\bigcup_{k \in \Bbb{N}} R_k^\circ $ and $\sum_{k=1}^\infty \mu (R_k) < \varepsilon$. Definition measure zero for every $\varepsilon >0$ we have a set of closed rectangles such that $E \subset\bigcup_{k \in \Bbb{N}} R_k$ and $\sum_{k=1}^\infty \mu (R_k) < \varepsilon$. Content zero is the same but with finitely many rectangles. Note I don't know any measure theoretic concepts besides those.",,['real-analysis']
