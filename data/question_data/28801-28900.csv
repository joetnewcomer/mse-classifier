,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Expected value of game involving 100-sided die,Expected value of game involving 100-sided die,,"The following question is from a Jane Street interview. You are given a 100-sided die. After you roll once, you can choose to either get paid the dollar amount of that roll OR pay one dollar for one more roll. What is the expected value of the game? (There is no limit on the number of rolls.) P.S. I think the question assumes that we are rational.","The following question is from a Jane Street interview. You are given a 100-sided die. After you roll once, you can choose to either get paid the dollar amount of that roll OR pay one dollar for one more roll. What is the expected value of the game? (There is no limit on the number of rolls.) P.S. I think the question assumes that we are rational.",,['probability']
1,"Conditional expectation of $\max(X,Y)$ and $\min(X,Y)$ when $X,Y$ are iid and exponentially distributed",Conditional expectation of  and  when  are iid and exponentially distributed,"\max(X,Y) \min(X,Y) X,Y","I am trying to compute the conditional expectation $$E[\max(X,Y) | \min(X,Y)]$$ where $X$ and $Y$ are two iid random variables with $X,Y \sim \exp(1)$. I already calculated the densities of $\min(X,Y)$ and $\max(X,Y)$, but I failed in calculating the joint density. Is this the right way? How can I compute the joint density then? Or do I have to take another ansatz?","I am trying to compute the conditional expectation $$E[\max(X,Y) | \min(X,Y)]$$ where $X$ and $Y$ are two iid random variables with $X,Y \sim \exp(1)$. I already calculated the densities of $\min(X,Y)$ and $\max(X,Y)$, but I failed in calculating the joint density. Is this the right way? How can I compute the joint density then? Or do I have to take another ansatz?",,"['probability', 'probability-theory', 'probability-distributions']"
2,Probability question about married couples,Probability question about married couples,,"If four married couples are arranged in a row, what is the probability    that no husband sits next to his wife? Would it be $1- \frac{2(4!)}{8!}$?","If four married couples are arranged in a row, what is the probability    that no husband sits next to his wife? Would it be $1- \frac{2(4!)}{8!}$?",,"['probability', 'combinatorics']"
3,"When calculating averages, why can we treat exploding die as if they're independent?","When calculating averages, why can we treat exploding die as if they're independent?",,"I'm certain that I'm forgetting something basic, but here goes. The exploding dice is a (house) rule for some games that says when you roll the maximum result on a given die (e.g. 6 on a six-sided die) you roll that die again and add the result. If you roll the maximum value again, you roll again and add that. This will carry on until you stop rolling the maximum value. From here, a natural question arises: ""what's the average of an exploding die?"". With the example of a six-sided die, the following answer comes naturally: $3.5*+3.5*\frac{1}{6}+3.5*\frac{1}{6^2}\dots=4.2$ This does indeed seem to be correct, and holds to any empirical test that I can think of, but why does this work? I want to use some excuse to the effect of ""expected value is linear and we've got identical distributions"", but I find that unsatisfactory. In particular, I don't understand why we can use the average values of 3.5 when every term to the right of that 3.5 assumes that we've beat the average. I have no doubt that this is why we need the $6^{-n}$ terms, but my intuition insists that this is insufficient. Note: What I really want here is to see the rigor. An ideal answer will attack this from the ground up, possibly even axiomatically. I'd hope that we don't have to go as deep as using probability measures on sets, but at the very least I want some answer that focuses on what property of averages allows us to factor the dice like this.","I'm certain that I'm forgetting something basic, but here goes. The exploding dice is a (house) rule for some games that says when you roll the maximum result on a given die (e.g. 6 on a six-sided die) you roll that die again and add the result. If you roll the maximum value again, you roll again and add that. This will carry on until you stop rolling the maximum value. From here, a natural question arises: ""what's the average of an exploding die?"". With the example of a six-sided die, the following answer comes naturally: This does indeed seem to be correct, and holds to any empirical test that I can think of, but why does this work? I want to use some excuse to the effect of ""expected value is linear and we've got identical distributions"", but I find that unsatisfactory. In particular, I don't understand why we can use the average values of 3.5 when every term to the right of that 3.5 assumes that we've beat the average. I have no doubt that this is why we need the terms, but my intuition insists that this is insufficient. Note: What I really want here is to see the rigor. An ideal answer will attack this from the ground up, possibly even axiomatically. I'd hope that we don't have to go as deep as using probability measures on sets, but at the very least I want some answer that focuses on what property of averages allows us to factor the dice like this.",3.5*+3.5*\frac{1}{6}+3.5*\frac{1}{6^2}\dots=4.2 6^{-n},"['probability', 'dice', 'average']"
4,Expected value of the max of $2$ dice [duplicate],Expected value of the max of  dice [duplicate],2,This question already has answers here : Expectation of Double Dice Throw (4 answers) What is the average of rolling two dice and only taking the value of the higher dice roll? (5 answers) Closed 6 years ago . What is the expected value of the max of two dice? I just wonder if there's a better way to get the answer to this question than listing out all possible outcomes and determining the expected value from there as this is actually an interview question? Thanks,This question already has answers here : Expectation of Double Dice Throw (4 answers) What is the average of rolling two dice and only taking the value of the higher dice roll? (5 answers) Closed 6 years ago . What is the expected value of the max of two dice? I just wonder if there's a better way to get the answer to this question than listing out all possible outcomes and determining the expected value from there as this is actually an interview question? Thanks,,['probability']
5,Toss a fair coin a number of times. [closed],Toss a fair coin a number of times. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Toss a fair coin, number of times. You always bet on heads and gain one dollar if it is a head and lose one dollar if it is a tail. You stop whenever you gain 3 dollars or you lose 2 dollars. Then which of the following is the correct one? you end up winning with $2\over 3$ probability you end up winning with $1\over 3$ probability you end up winning with $3\over 4$ probability you end up winning with $3\over 5$ probability None of the above is correct. If we don't know the number of trials, how can we do it?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Toss a fair coin, number of times. You always bet on heads and gain one dollar if it is a head and lose one dollar if it is a tail. You stop whenever you gain 3 dollars or you lose 2 dollars. Then which of the following is the correct one? you end up winning with $2\over 3$ probability you end up winning with $1\over 3$ probability you end up winning with $3\over 4$ probability you end up winning with $3\over 5$ probability None of the above is correct. If we don't know the number of trials, how can we do it?",,['probability']
6,Negative binomial distribution - sum of two random variables,Negative binomial distribution - sum of two random variables,,"Suppose $X, Y$ are independent random variables with $X\sim NB(r,p)$ and $Y\sim NB(s,p)$. Then $$X + Y \sim NB(r+s,p)$$ How do I go about proving this? I'm not sure where to begin, I'd be glad for any hint.","Suppose $X, Y$ are independent random variables with $X\sim NB(r,p)$ and $Y\sim NB(s,p)$. Then $$X + Y \sim NB(r+s,p)$$ How do I go about proving this? I'm not sure where to begin, I'd be glad for any hint.",,"['probability', 'statistics', 'probability-distributions']"
7,Probability - Length of an arc which contains a fixed point,Probability - Length of an arc which contains a fixed point,,"On the circumference $x^2+y^2=1$ one randomly chooses (uniformly and independently) $3$ points. These points split the circumference, forming $3$ arcs. What's the expected value of the length of the arc which contains the point $(1,0)$? Is there any quick way to solve this? I kind of already used some symmetries but it seems too complex to me. (It's supposed to be an easy exercise, so maybe there's some clever thing to consider.)","On the circumference $x^2+y^2=1$ one randomly chooses (uniformly and independently) $3$ points. These points split the circumference, forming $3$ arcs. What's the expected value of the length of the arc which contains the point $(1,0)$? Is there any quick way to solve this? I kind of already used some symmetries but it seems too complex to me. (It's supposed to be an easy exercise, so maybe there's some clever thing to consider.)",,"['probability', 'probability-distributions', 'geometric-probability']"
8,Bob took a quiz,Bob took a quiz,,"Bob took a quiz consisting of 500 questions with two options: Yes and No. Bob did not prepare for the quiz. Flustered, he hastily reached for a fair coin in his wallet and started to toss this coin for answers. Whenever the coin landed heads, he shaded Yes; whenever the coin landed tails, he shaded No. It turned out that 40% of the questions have Yes as the correct answer, and the rest have No as the correct answer. Among those questions that Bob shaded Yes, roughly how many percent did he get correct? A) 30% B) 40% C) 50% D)60% E) 70% My thought is that of the questions he put as Yes, the chance of getting the right answers is equal to 40%. But this is just a guess and I'm not sure of the explanation or if this is the right answer.","Bob took a quiz consisting of 500 questions with two options: Yes and No. Bob did not prepare for the quiz. Flustered, he hastily reached for a fair coin in his wallet and started to toss this coin for answers. Whenever the coin landed heads, he shaded Yes; whenever the coin landed tails, he shaded No. It turned out that 40% of the questions have Yes as the correct answer, and the rest have No as the correct answer. Among those questions that Bob shaded Yes, roughly how many percent did he get correct? A) 30% B) 40% C) 50% D)60% E) 70% My thought is that of the questions he put as Yes, the chance of getting the right answers is equal to 40%. But this is just a guess and I'm not sure of the explanation or if this is the right answer.",,"['probability', 'statistics', 'recreational-mathematics', 'sampling']"
9,Probability of $n$ successes in a row at the $k$-th Bernoulli trial... geometric?,Probability of  successes in a row at the -th Bernoulli trial... geometric?,n k,"If one has Bernoulli trials with success probability $p$, then it makes sense that the probability of the first success observed to be at trial number $k$ be given by $$(1-p)^{k-1} p.$$ But how would we approach the same problem if the probability we are after is the probability of getting the first occurrence of two successes in a row at the $k$-th trial? In other words, for trial $k$ to be a success, with trial $k-1$ being a success and with no other occurrence of two successes in a row before that? In general what if we want the first $n$ successes in a row? I can do this with some counting if $p = 1/2$ but my counting argument (a requirement on the sequence of first $k-3$ trials, plus enforcing failure on the $k-2$ and then enforcing two successes in a row) breaks down if $p \neq 1/2$.  This shouldn't be too hard as it seems very similar in nature to the geometric distribution. Thanks.","If one has Bernoulli trials with success probability $p$, then it makes sense that the probability of the first success observed to be at trial number $k$ be given by $$(1-p)^{k-1} p.$$ But how would we approach the same problem if the probability we are after is the probability of getting the first occurrence of two successes in a row at the $k$-th trial? In other words, for trial $k$ to be a success, with trial $k-1$ being a success and with no other occurrence of two successes in a row before that? In general what if we want the first $n$ successes in a row? I can do this with some counting if $p = 1/2$ but my counting argument (a requirement on the sequence of first $k-3$ trials, plus enforcing failure on the $k-2$ and then enforcing two successes in a row) breaks down if $p \neq 1/2$.  This shouldn't be too hard as it seems very similar in nature to the geometric distribution. Thanks.",,"['probability', 'combinatorics', 'probability-distributions']"
10,Random solving of a Rubik cube .,Random solving of a Rubik cube .,,"After playing a little with a Rubik cube I thought of the following problem : Suppose we start with a solved Rubik cube (a general one , with $n^3$ cubes) . Then we choose one of the moves , each having a probability of $\frac{1}{6n}$ of being chosen , and apply it on our cube . We continue doing so ( choosing a new move randomly and then applying it and so on...) , until we reach the solved position again . What is the expected number of moves needed to solve the cube in this way ? I thought about it and I think the answer should be $\infty$ (at least for $n \geq 3$) because there are a lot of random sequences of moves that will take a long time to solve the cube but I don't have a rigorous way to prove it. For $n=2$ I'm not that sure if the answer should be finite or infinite (but I still tend to think it's infinite). Thank you for your time to help me!","After playing a little with a Rubik cube I thought of the following problem : Suppose we start with a solved Rubik cube (a general one , with $n^3$ cubes) . Then we choose one of the moves , each having a probability of $\frac{1}{6n}$ of being chosen , and apply it on our cube . We continue doing so ( choosing a new move randomly and then applying it and so on...) , until we reach the solved position again . What is the expected number of moves needed to solve the cube in this way ? I thought about it and I think the answer should be $\infty$ (at least for $n \geq 3$) because there are a lot of random sequences of moves that will take a long time to solve the cube but I don't have a rigorous way to prove it. For $n=2$ I'm not that sure if the answer should be finite or infinite (but I still tend to think it's infinite). Thank you for your time to help me!",,"['probability', 'combinatorics']"
11,Does 0% chance mean impossible? [duplicate],Does 0% chance mean impossible? [duplicate],,"This question already has answers here : Zero probability and impossibility (8 answers) Closed 9 years ago . Suppose we pick a random real number between 0 and 1 and call it $x$. There are $2^{\aleph_0}$ possible values, so the chance of picking any specific number (such as $x$) in that range is 0. But in the end, we did manage to pick $x$, despite its probability of 0. Does this mean that a 0% chance is actually possible, or is there some flaw in this logic?","This question already has answers here : Zero probability and impossibility (8 answers) Closed 9 years ago . Suppose we pick a random real number between 0 and 1 and call it $x$. There are $2^{\aleph_0}$ possible values, so the chance of picking any specific number (such as $x$) in that range is 0. But in the end, we did manage to pick $x$, despite its probability of 0. Does this mean that a 0% chance is actually possible, or is there some flaw in this logic?",,"['probability', 'infinity']"
12,confusion about permutation,confusion about permutation,,"$7$ white identical balls and $3$ black identical balls are randomly placed in a row. The probability that no two black balls are together is ? I am getting it as $ \frac{1}{3}$ while the answer in my book is $\frac{7}{15}$. Total ways are $\frac{10!}{7!.3!}=120$ now i considered three consecutive balls as one so $\frac{(1+7)!}{7!}=8$,then two balls as consecutive which is $\frac{(1+8)!}{7!}=72$ so probability is $\frac{120-8-72}{120}=1/3$ What am I missing on? Any help.","$7$ white identical balls and $3$ black identical balls are randomly placed in a row. The probability that no two black balls are together is ? I am getting it as $ \frac{1}{3}$ while the answer in my book is $\frac{7}{15}$. Total ways are $\frac{10!}{7!.3!}=120$ now i considered three consecutive balls as one so $\frac{(1+7)!}{7!}=8$,then two balls as consecutive which is $\frac{(1+8)!}{7!}=72$ so probability is $\frac{120-8-72}{120}=1/3$ What am I missing on? Any help.",,"['probability', 'combinatorics', 'permutations']"
13,Is this always true: $P(A|B) = 1-P(A^c|B)$?,Is this always true: ?,P(A|B) = 1-P(A^c|B),"Does this identity hold for all events? $$ P(A|B) = 1-P(A'|B) $$ Logically speaking, if the probability of $A$ given $B$ occurred is $X$, shouldn't the probability that $A$ does not occur, $A'$, given $B$, be similarly $1-X$? There is a related question here . This is the closest that I could get to proving (or disproving) it: $P(A\cap B)=P(A)-P(A' \cap B)$ $P(A|B)P(B)=P(A)-P(A'|B)P(B)$ $\therefore P(A'|B)= \frac {P(A)} {P(B)}-P(A|B)$ Are there are certain formulae which can be used to prove this? Or does the identity only hold under certain situations, and if so, what kind of situations? Thanks.","Does this identity hold for all events? $$ P(A|B) = 1-P(A'|B) $$ Logically speaking, if the probability of $A$ given $B$ occurred is $X$, shouldn't the probability that $A$ does not occur, $A'$, given $B$, be similarly $1-X$? There is a related question here . This is the closest that I could get to proving (or disproving) it: $P(A\cap B)=P(A)-P(A' \cap B)$ $P(A|B)P(B)=P(A)-P(A'|B)P(B)$ $\therefore P(A'|B)= \frac {P(A)} {P(B)}-P(A|B)$ Are there are certain formulae which can be used to prove this? Or does the identity only hold under certain situations, and if so, what kind of situations? Thanks.",,['probability']
14,"If three dice are rolled, what is the probability that all three are the same number?","If three dice are rolled, what is the probability that all three are the same number?",,The dice are fair. You have a $1\over6$ chance of getting the first number. A $1\over6$ chance of the second and so on.  Is it just $({1\over6})^3$ (1/216) or is that not accounting for the second and third roll properly?,The dice are fair. You have a $1\over6$ chance of getting the first number. A $1\over6$ chance of the second and so on.  Is it just $({1\over6})^3$ (1/216) or is that not accounting for the second and third roll properly?,,['probability']
15,Probability of winning a prize in a raffle,Probability of winning a prize in a raffle,,"My work is having it's annual Christmas raffle today. 1600 tickets have been sold, and there are 40 prizes to win. I have bought ten tickets. What are the odds I will win a prize? While an initial estimate of 1/160 is probably within a close enough range to suggest I have little chance of winning, I am curious as to what the precise odds would be. Tickets are not put back in once they have been drawn.","My work is having it's annual Christmas raffle today. 1600 tickets have been sold, and there are 40 prizes to win. I have bought ten tickets. What are the odds I will win a prize? While an initial estimate of 1/160 is probably within a close enough range to suggest I have little chance of winning, I am curious as to what the precise odds would be. Tickets are not put back in once they have been drawn.",,"['probability', 'combinatorics', 'binomial-coefficients', 'recreational-mathematics', 'problem-solving']"
16,Seating of $n$ people with tickets into $n+k$ chairs with 1st person taking a random seat,Seating of  people with tickets into  chairs with 1st person taking a random seat,n n+k,"""$n$ music lovers have reserved seats in a theater containing a total of $n+k$ seats ($k$ seats are unassigned). The first person who enters the theater, however, lost his seat assignment and chooses a seat at random. Subsequently, people enter the theater one at a time and sit in their assigned seat unless it is already occupied. If it is, they choose a seat at random from the remaining empty seats. What is the probability that person $n$, the last person to enter the theater, finds their seat already occupied?"" I could do this problem for small specific values of $n$ and $k$ but as they grow the expressions seem to get really messy really quick with no discernible pattern. How would one solve this problem?","""$n$ music lovers have reserved seats in a theater containing a total of $n+k$ seats ($k$ seats are unassigned). The first person who enters the theater, however, lost his seat assignment and chooses a seat at random. Subsequently, people enter the theater one at a time and sit in their assigned seat unless it is already occupied. If it is, they choose a seat at random from the remaining empty seats. What is the probability that person $n$, the last person to enter the theater, finds their seat already occupied?"" I could do this problem for small specific values of $n$ and $k$ but as they grow the expressions seem to get really messy really quick with no discernible pattern. How would one solve this problem?",,['probability']
17,Successful approaches to the modelization of ''randomness'',Successful approaches to the modelization of ''randomness'',,"If you pick a number $x$ randomly from $[0,100]$, we would naturally say that the probability of $x>50$ is $1/2$, right? This is because we assumed that randomly meant that the experiment was to pick a point from $[0,100]$ (with numbers equally distributed). But, since $f(r)=r^2$ is a bijection  $[0,10] \rightarrow [0,100]$, we could also pick a number $r$ from $[0,10]$ and then do $x=r^2 \in [0,100]$ and let that be our random experiment. This time $x>5$ only for $r> \sqrt{50} \sim 7.07$. In this case we would agree that the first way of choosing $x$ looks a lot more natural . So we would equally agree that is a successful way of modeling the experiment ''pick a random number from [0,100]''. There are sometimes when we can't even agree on that! For example, on Bertrand's Paradox we are asked to pick a random chord from a circumference and calculate the probability that it is longer than the side of the inscribed equilateral triangle. The point is there are several (a priori) natural ways of choosing the chords (three of them are nicely described here ) which, of course, produce different probabilities. How and when can we consider something is truly random ? Does it even make any sense saying something is truly random or is it more a matter of agreement? Is there any convention in the mathematical community about this issues? Could we say the common notion of randomness relates to the notion of uniform distribution? Are there any successful approaches on models about randomness? (That let us decide if a certain distribution represents randomness in the sense of being an uniform distribution) For example, on the comments it is said: ""One can show [using Kolmogorov Complexity ] that a number in [0,1] is random with probability 1 under the uniform distribution, so it coheres well with other notions.''","If you pick a number $x$ randomly from $[0,100]$, we would naturally say that the probability of $x>50$ is $1/2$, right? This is because we assumed that randomly meant that the experiment was to pick a point from $[0,100]$ (with numbers equally distributed). But, since $f(r)=r^2$ is a bijection  $[0,10] \rightarrow [0,100]$, we could also pick a number $r$ from $[0,10]$ and then do $x=r^2 \in [0,100]$ and let that be our random experiment. This time $x>5$ only for $r> \sqrt{50} \sim 7.07$. In this case we would agree that the first way of choosing $x$ looks a lot more natural . So we would equally agree that is a successful way of modeling the experiment ''pick a random number from [0,100]''. There are sometimes when we can't even agree on that! For example, on Bertrand's Paradox we are asked to pick a random chord from a circumference and calculate the probability that it is longer than the side of the inscribed equilateral triangle. The point is there are several (a priori) natural ways of choosing the chords (three of them are nicely described here ) which, of course, produce different probabilities. How and when can we consider something is truly random ? Does it even make any sense saying something is truly random or is it more a matter of agreement? Is there any convention in the mathematical community about this issues? Could we say the common notion of randomness relates to the notion of uniform distribution? Are there any successful approaches on models about randomness? (That let us decide if a certain distribution represents randomness in the sense of being an uniform distribution) For example, on the comments it is said: ""One can show [using Kolmogorov Complexity ] that a number in [0,1] is random with probability 1 under the uniform distribution, so it coheres well with other notions.''",,"['probability', 'probability-theory', 'probability-distributions', 'philosophy']"
18,Better than random,Better than random,,"I have been trying to solve this question , but in vain. Please help. You are given two boxes with a number inside each box. The two numbers   are different but you have no idea what they are. You pick one box to   open; read the number inside; and then guess if the number in the   other box is larger or smaller. You win if you guess correctly, and   lose otherwise. Is there anyway that you can win the game with more   than 50% chances no matter what the two numbers are?","I have been trying to solve this question , but in vain. Please help. You are given two boxes with a number inside each box. The two numbers   are different but you have no idea what they are. You pick one box to   open; read the number inside; and then guess if the number in the   other box is larger or smaller. You win if you guess correctly, and   lose otherwise. Is there anyway that you can win the game with more   than 50% chances no matter what the two numbers are?",,"['puzzle', 'probability']"
19,"What does ""maximum value"" of a set of random variables mean?","What does ""maximum value"" of a set of random variables mean?",,"In our statistical mechanics lecture the professor said something along the lines of: If we have some independent random variables $x_1,x_2,x_3,...,x_n$ having identical distributions: Suppose $M_{n}=\text{max}(x_1,x_2,x_3,...)$, then we say that   probability that $M_{n}<x$ is $\text{$\text{Pr}(M_{n}<x$)}$ (say). In such a case   $\text{$\text{Pr}(M_n<x$)}=\text{$\text{Pr}(x_1<x,x_2<x,x_3<x,...$)}=(\text{Pr}(x))^{n}$ Now, first of all I don't understand what he meant by $M_{n}=\text{max}(x_1,x_2,x_3,...)$. What does maximum of a set of random variables even mean? Does it refer to the random variable which can take the highest value? Secondly I don't understand the step $\text{$\text{Pr}(M_n<x$)}=\text{$\text{Pr}(x_1<x,x_2<x,x_3<x,...$)}=(\text{Pr}(x))^{n}$","In our statistical mechanics lecture the professor said something along the lines of: If we have some independent random variables $x_1,x_2,x_3,...,x_n$ having identical distributions: Suppose $M_{n}=\text{max}(x_1,x_2,x_3,...)$, then we say that   probability that $M_{n}<x$ is $\text{$\text{Pr}(M_{n}<x$)}$ (say). In such a case   $\text{$\text{Pr}(M_n<x$)}=\text{$\text{Pr}(x_1<x,x_2<x,x_3<x,...$)}=(\text{Pr}(x))^{n}$ Now, first of all I don't understand what he meant by $M_{n}=\text{max}(x_1,x_2,x_3,...)$. What does maximum of a set of random variables even mean? Does it refer to the random variable which can take the highest value? Secondly I don't understand the step $\text{$\text{Pr}(M_n<x$)}=\text{$\text{Pr}(x_1<x,x_2<x,x_3<x,...$)}=(\text{Pr}(x))^{n}$",,['probability']
20,Intuition about Blumenthal's 0-1 law,Intuition about Blumenthal's 0-1 law,,"I'm studying Brownian motion from Durrett. I'm trying to understand what  Blumenthal's 0-1 law really says about what Durrett calls the germ field , $\mathcal{F}_0^+$ . Let $\mathcal{F}_t^+ = \cap_{s > t} \mathcal{F}_s^0$ and $\mathcal{F}_t^0 = \sigma(B_s,s\le t)$ where $B_t$ is Brownian motion. Also let $P_x$ denote the usual measure on $\mathcal{C}[0,\infty)$ making $B_t(\omega) = \omega(t)$ a Brownian motion starting at $x$ . Blumenthal's 0-1 law: If $A \in \mathcal{F}_0^+$ then for all $x \in \mathbb{R}$ , $P_x(A) \in \{0,1\}$ . I have no trouble understanding this proof, but I'm not sure I understand why this is surprising/significant. Obviously, $\mathcal{F}_0^0$ is trivial. Is there any intuitive way to explain why we might not expect $\mathcal{F}_0^+$ to be trivial as well? Even though $\mathcal{F}_0^+$ seems to involve a tiny bit of information into the future past $t=0$ , since Brownian motion is continuous it would seem that we ought to expect no difference between the two fields.","I'm studying Brownian motion from Durrett. I'm trying to understand what  Blumenthal's 0-1 law really says about what Durrett calls the germ field , . Let and where is Brownian motion. Also let denote the usual measure on making a Brownian motion starting at . Blumenthal's 0-1 law: If then for all , . I have no trouble understanding this proof, but I'm not sure I understand why this is surprising/significant. Obviously, is trivial. Is there any intuitive way to explain why we might not expect to be trivial as well? Even though seems to involve a tiny bit of information into the future past , since Brownian motion is continuous it would seem that we ought to expect no difference between the two fields.","\mathcal{F}_0^+ \mathcal{F}_t^+ = \cap_{s > t} \mathcal{F}_s^0 \mathcal{F}_t^0 = \sigma(B_s,s\le t) B_t P_x \mathcal{C}[0,\infty) B_t(\omega) = \omega(t) x A \in \mathcal{F}_0^+ x \in \mathbb{R} P_x(A) \in \{0,1\} \mathcal{F}_0^0 \mathcal{F}_0^+ \mathcal{F}_0^+ t=0","['probability', 'soft-question', 'stochastic-processes', 'brownian-motion']"
21,Product of two random variables that has exponential distribution,Product of two random variables that has exponential distribution,,I am trying to define the probability distribution of $Z$ such as $Z = X_1\cdot X_2$ where $X_1$ and $X_2$ are two independent and identically exponentially distributed variables. $$P(X_1=x) = \lambda e^{-\lambda x}$$ $$P(X_2=x) = \lambda e^{-\lambda x}$$ I tried something like that…. $$P(Z=x) = P(X_1 = a) \cdot P(X_2 = x/a)$$ Is this first equation correct? $$P(Z=x) = \lambda e^{-\lambda a} \cdot \lambda e^{-\lambda x/a} = \lambda^2 e^{-\lambda (a+x/a)}$$ I doesn't feel like I found the solution as the dummy variable $a$ remains in the final result. Can you please help me solving this problem?,I am trying to define the probability distribution of $Z$ such as $Z = X_1\cdot X_2$ where $X_1$ and $X_2$ are two independent and identically exponentially distributed variables. $$P(X_1=x) = \lambda e^{-\lambda x}$$ $$P(X_2=x) = \lambda e^{-\lambda x}$$ I tried something like that…. $$P(Z=x) = P(X_1 = a) \cdot P(X_2 = x/a)$$ Is this first equation correct? $$P(Z=x) = \lambda e^{-\lambda a} \cdot \lambda e^{-\lambda x/a} = \lambda^2 e^{-\lambda (a+x/a)}$$ I doesn't feel like I found the solution as the dummy variable $a$ remains in the final result. Can you please help me solving this problem?,,"['probability', 'probability-theory', 'probability-distributions']"
22,"""Converse"" of optional stopping theorem","""Converse"" of optional stopping theorem",,"Assume we are considering the discrete case. If $\{X_n\}_{n\in\mathbb{N}}$ is a martingale adapted to $F_n$ and $X_n\in L^1$ , then for any bounded stopping time $\tau$ , according to the optional stopping theorem (or optional sampling theorem), we will have that $E(X_\tau)=E(X_0)$ . I've seen some discussions on the condition such as the boundedness of the stopping time. However, now I am curious about that if we don't have the assumption that $X_n$ is a martingale does $E(X_\tau)=E(X_0)$ still holds for any bounded stopping time? In another word, does there exist any integrable random process $X_n$ adapted to $F_n$ which is not a martingale such that for any bounded stopping time $\tau$ it holds $E(X_\tau)=E(X_0)$ ? Or we can rephrase this as ""converse"" of OST: if integrable random process $X_n$ adapted to $F_n$ and for any bounded stopping time $\tau$ it holds $E(X_\tau)=E(X_0)$ , then $X_n$ is a martingale. Is this converse true or not?","Assume we are considering the discrete case. If is a martingale adapted to and , then for any bounded stopping time , according to the optional stopping theorem (or optional sampling theorem), we will have that . I've seen some discussions on the condition such as the boundedness of the stopping time. However, now I am curious about that if we don't have the assumption that is a martingale does still holds for any bounded stopping time? In another word, does there exist any integrable random process adapted to which is not a martingale such that for any bounded stopping time it holds ? Or we can rephrase this as ""converse"" of OST: if integrable random process adapted to and for any bounded stopping time it holds , then is a martingale. Is this converse true or not?",\{X_n\}_{n\in\mathbb{N}} F_n X_n\in L^1 \tau E(X_\tau)=E(X_0) X_n E(X_\tau)=E(X_0) X_n F_n \tau E(X_\tau)=E(X_0) X_n F_n \tau E(X_\tau)=E(X_0) X_n,"['probability', 'probability-theory', 'stochastic-processes', 'martingales', 'stopping-times']"
23,Uncorrelated but not independent random variables,Uncorrelated but not independent random variables,,"Is it possible to construct two random variables $X, Y$ both of them assuming exactly two non-zero values which are uncorrelated, i. e. $\mathbf{E}[X \, Y] = \mathbf{E}[X]\,\mathbf{E}[Y]$, but not independent? If that is not possible, what is the simplest example of non-zero discrete random variables which are uncorrelated but not independent? Thanks a lot!","Is it possible to construct two random variables $X, Y$ both of them assuming exactly two non-zero values which are uncorrelated, i. e. $\mathbf{E}[X \, Y] = \mathbf{E}[X]\,\mathbf{E}[Y]$, but not independent? If that is not possible, what is the simplest example of non-zero discrete random variables which are uncorrelated but not independent? Thanks a lot!",,"['probability', 'random-variables', 'correlation']"
24,"Expectation of CDF of continuous random variable $X$, evaluated at $X$","Expectation of CDF of continuous random variable , evaluated at",X X,"Given the continuous random variable $X$ with cumulative distribution function $F_{X}$, find $E[F_{X}(X)]$. Attempt at solution: I understand that the expected value, $E[X]$, of a random variable, $X$, is $\int^{+\infty}_{-\infty} x f_{X}(x)\operatorname{d}x$, where $f_{X}$ is the probability density function. However, I'm a little thrown off by the wording of the question. Is $E[F_{X}(X)]$ the same thing as $E[X]$?","Given the continuous random variable $X$ with cumulative distribution function $F_{X}$, find $E[F_{X}(X)]$. Attempt at solution: I understand that the expected value, $E[X]$, of a random variable, $X$, is $\int^{+\infty}_{-\infty} x f_{X}(x)\operatorname{d}x$, where $f_{X}$ is the probability density function. However, I'm a little thrown off by the wording of the question. Is $E[F_{X}(X)]$ the same thing as $E[X]$?",,['probability']
25,Probability that two people see each other at the coffee shop,Probability that two people see each other at the coffee shop,,"Two mathematicians each come into a coffee shop at a random time between 8:00 a.m. and 9:00 a.m. each day. Each orders a cup of coffee then sits at a table, reading a newspaper for 20 minutes before leaving to go to work. On any day, what is the probability that both mathematicians are at the coffee shop at the same time (that is, their arrival times are within 20 minutes of each other)?","Two mathematicians each come into a coffee shop at a random time between 8:00 a.m. and 9:00 a.m. each day. Each orders a cup of coffee then sits at a table, reading a newspaper for 20 minutes before leaving to go to work. On any day, what is the probability that both mathematicians are at the coffee shop at the same time (that is, their arrival times are within 20 minutes of each other)?",,['probability']
26,Conditional Probability: Bridge Hand given north and south has 8 spades,Conditional Probability: Bridge Hand given north and south has 8 spades,,"I apologise as I'm pretty certain I could find the actual answer myself on these boards (as in the solution to the problem) but if possible I'd like it if someone can confirm whether my thinking is correct. In the card game bridge, the entire 52 cards are dealt out equally to 4 players, E/W/N/S if N and S have a total of 8 spades among them, what is the probability E has 3 of the remaining 5 spades? This is example 2c on page 60 in ""A First Course in probability 8th edition"" and gives the solution as $$\frac{{{5}\choose{3}}{{21}\choose{10}}}{{26}\choose{13}}\approx .339$$ Later in the book, it asks to recalculate this using conditional probability (question 3.3 page 102): Compute the conditional probability that E has 3 spades given North and South have a combined total of 8 spades. My answer is then as follows: Let E be the event that E has 3 spades. and F the event that N and S have 8 spades. Then $P(E)=\frac{{{5}\choose{3}}{{21}\choose{10}}}{{26}\choose{13}}$ as it is given that 26 cards have already been dealt to N and S, of which 8 of the 13 spades have also been dealt. This means for E to occur we have to choose 3 of the remaining 5 spades and any combination of 10 cards remaining from the 21 leftovers (that is 26 remaining cards minus the 5 spades). All are divided by the reduced sample space. (This is one of the parts in which I want to be certain) As we only care about E we can consider N and S to be one person which will be dealt 26 cards, 8 of which must be spades, giving $P(F)=\frac{{{13}\choose{8}}{{39}\choose{19}}}{{52}\choose{26}}$ this reasoning seems right to me as the two of them should receive any combination of 26 cards so long as 8 of them are spades. (We don't care if N has all 8 or if S has all 8.) Then our conditional Probability is $$P(E|F) = \frac{P(E\cap F)}{P(F)}.$$ My major issue is I'm not sure how I would go about calculating $P(E\cap F)$ in this instance. Logically I argue that the two events P and F are independent of each other, meaning $$P(E\cap F)=P(E)P(F)$$ and so making our final conditional probability $P(E|F)=P(E)$ which yes does give me the right answer, but unless the question specifically states that these are independent of each other, I would then have to prove that $P(E\cap F)=P(E)P(F)$ holds surely? Any clarification would be greatly appreciated. thank you.","I apologise as I'm pretty certain I could find the actual answer myself on these boards (as in the solution to the problem) but if possible I'd like it if someone can confirm whether my thinking is correct. In the card game bridge, the entire 52 cards are dealt out equally to 4 players, E/W/N/S if N and S have a total of 8 spades among them, what is the probability E has 3 of the remaining 5 spades? This is example 2c on page 60 in ""A First Course in probability 8th edition"" and gives the solution as Later in the book, it asks to recalculate this using conditional probability (question 3.3 page 102): Compute the conditional probability that E has 3 spades given North and South have a combined total of 8 spades. My answer is then as follows: Let E be the event that E has 3 spades. and F the event that N and S have 8 spades. Then as it is given that 26 cards have already been dealt to N and S, of which 8 of the 13 spades have also been dealt. This means for E to occur we have to choose 3 of the remaining 5 spades and any combination of 10 cards remaining from the 21 leftovers (that is 26 remaining cards minus the 5 spades). All are divided by the reduced sample space. (This is one of the parts in which I want to be certain) As we only care about E we can consider N and S to be one person which will be dealt 26 cards, 8 of which must be spades, giving this reasoning seems right to me as the two of them should receive any combination of 26 cards so long as 8 of them are spades. (We don't care if N has all 8 or if S has all 8.) Then our conditional Probability is My major issue is I'm not sure how I would go about calculating in this instance. Logically I argue that the two events P and F are independent of each other, meaning and so making our final conditional probability which yes does give me the right answer, but unless the question specifically states that these are independent of each other, I would then have to prove that holds surely? Any clarification would be greatly appreciated. thank you.",\frac{{{5}\choose{3}}{{21}\choose{10}}}{{26}\choose{13}}\approx .339 P(E)=\frac{{{5}\choose{3}}{{21}\choose{10}}}{{26}\choose{13}} P(F)=\frac{{{13}\choose{8}}{{39}\choose{19}}}{{52}\choose{26}} P(E|F) = \frac{P(E\cap F)}{P(F)}. P(E\cap F) P(E\cap F)=P(E)P(F) P(E|F)=P(E) P(E\cap F)=P(E)P(F),"['probability', 'combinatorics', 'card-games']"
27,Conditional Expectations Given Sum of I.I.D.,Conditional Expectations Given Sum of I.I.D.,,"Given that $X_1,...Xn$ are all identical independent random variables. $\mathbb{E}(X_1|\sum_{k=1}^{n}X_k)$ = ? I am unsure how to proceed on this one. I know the default relation: $\mathbb{E}(X|Y)$ = $\mathbb{E}(X*I_{[Y=y]})\over\mathbb{P}(Y=y))$, where I is an indicator function. Intuitively, I believe the answer should be the sum of the random variables divided by how many random variables or the average of the sum.","Given that $X_1,...Xn$ are all identical independent random variables. $\mathbb{E}(X_1|\sum_{k=1}^{n}X_k)$ = ? I am unsure how to proceed on this one. I know the default relation: $\mathbb{E}(X|Y)$ = $\mathbb{E}(X*I_{[Y=y]})\over\mathbb{P}(Y=y))$, where I is an indicator function. Intuitively, I believe the answer should be the sum of the random variables divided by how many random variables or the average of the sum.",,"['probability', 'statistics']"
28,Where am I going wrong with this proof of expected value of a geometric random variable?,Where am I going wrong with this proof of expected value of a geometric random variable?,,I know that the expected value of a geometrically distributed random variable is $\frac1p$ but how do we get there. This is what I got so far: $$\sum_{x=1}^\infty xP(X=x)$$ where X is the number of failures until first success. Since it's geometric we have:$$\begin{align} \sum_{x=1}^\infty xp(1-p)^{x-1}\\ \frac{p}{1-p} \sum_{x=1}^\infty x(1-p)^x\\ ....  \end{align}$$ How do we sum that?,I know that the expected value of a geometrically distributed random variable is $\frac1p$ but how do we get there. This is what I got so far: $$\sum_{x=1}^\infty xP(X=x)$$ where X is the number of failures until first success. Since it's geometric we have:$$\begin{align} \sum_{x=1}^\infty xp(1-p)^{x-1}\\ \frac{p}{1-p} \sum_{x=1}^\infty x(1-p)^x\\ ....  \end{align}$$ How do we sum that?,,"['probability', 'probability-theory']"
29,Is it better to play $\$1$ on $10$ lottery draws or $\$10$ on one lottery draw?,Is it better to play 110\ on one lottery draw?,\  on   lottery draws or  10,"If I had 10 dollars to spend on a 1 dollar lottery draw, would I have more chance of winning if I spent all 10 dollars in one draw or bought 1 dollar tickets for 10 separate draws? Edit: in terms of lottery definition, you pick 6 numbers from a pool of 49 numbers (1-49), that is classed as one lottery ticket. So each 1 dollar represents a selection of 6 numbers. Across multiple tickets you can pick the same numbers as appear on your previous tickets. If you are familiar with EuroMillions or UK Lotto, it's that kind of lottery. http://www.national-lottery.co.uk/player/p/lotterydrawgames/lotto.ftl Edit 2: Let me re-phrase the question. The probability of winning the jackpot in the lottery is 1 in 13,983,816. Would buying 10 tickets for one draw change those odds to 10 in 13,983,816 ? and if so is that better than playing in 10 different draws at 1 in 13,983,816 odds each?","If I had 10 dollars to spend on a 1 dollar lottery draw, would I have more chance of winning if I spent all 10 dollars in one draw or bought 1 dollar tickets for 10 separate draws? Edit: in terms of lottery definition, you pick 6 numbers from a pool of 49 numbers (1-49), that is classed as one lottery ticket. So each 1 dollar represents a selection of 6 numbers. Across multiple tickets you can pick the same numbers as appear on your previous tickets. If you are familiar with EuroMillions or UK Lotto, it's that kind of lottery. http://www.national-lottery.co.uk/player/p/lotterydrawgames/lotto.ftl Edit 2: Let me re-phrase the question. The probability of winning the jackpot in the lottery is 1 in 13,983,816. Would buying 10 tickets for one draw change those odds to 10 in 13,983,816 ? and if so is that better than playing in 10 different draws at 1 in 13,983,816 odds each?",,"['probability', 'lotteries']"
30,probability mass function of sum of two independent geometric random variables,probability mass function of sum of two independent geometric random variables,,"How could it be proved that the probability mass function of X + Y, where X and Y are independent random variables each geometrically distributed with parameter p; i.e. $p_X(n)=p_Y(n)=\left\{\begin{matrix} p(1-p)^{n-1} & n=1,2,...\\  0 & otherwise \end{matrix}\right.$ equals to $\mathbf{P_{X+Y}(n)= \color{Red}{(n-1)}\ p^2(1-p)^{n-2}}$ Using convolution I get $\mathbf{P(X+Y=n)=\sum_{n}^{k=0} Pr(X=k)*Pr(Y=n-k) =\sum_{k=1}^n p_X (1-p_x)^{k-1} p_Y(1-p_Y)^{n-k-1}}$ as $p=p_X=p_Y$  it reduces to $\mathbf{P(X+Y=n)=\sum_{k=1}^n p^2(1-p)^{n-2}}$ is this a correct way? I am stuck here, I don't know how to get the final formula.I miss some transition in order to get the (n-1).","How could it be proved that the probability mass function of X + Y, where X and Y are independent random variables each geometrically distributed with parameter p; i.e. $p_X(n)=p_Y(n)=\left\{\begin{matrix} p(1-p)^{n-1} & n=1,2,...\\  0 & otherwise \end{matrix}\right.$ equals to $\mathbf{P_{X+Y}(n)= \color{Red}{(n-1)}\ p^2(1-p)^{n-2}}$ Using convolution I get $\mathbf{P(X+Y=n)=\sum_{n}^{k=0} Pr(X=k)*Pr(Y=n-k) =\sum_{k=1}^n p_X (1-p_x)^{k-1} p_Y(1-p_Y)^{n-k-1}}$ as $p=p_X=p_Y$  it reduces to $\mathbf{P(X+Y=n)=\sum_{k=1}^n p^2(1-p)^{n-2}}$ is this a correct way? I am stuck here, I don't know how to get the final formula.I miss some transition in order to get the (n-1).",,"['probability', 'random-variables', 'convolution']"
31,Expected number of card draws to get all 4 suits,Expected number of card draws to get all 4 suits,,"You have a standard 52 card deck, with 13 cards of each of the 4 suits (Hearts, Diamonds, Spades, Clubs). What is the expected number of cards you have to draw from the deck until you have all 4 suits represented in your hand? I couldn't think of how to get the negative binomial to work, since this is sampling without replacement and has 4 suits instead of just 2. I imagine a distribution that could solve this might be called the Negative Hypergeometric Multivariate. Anyone have any ideas? Thanks very much.","You have a standard 52 card deck, with 13 cards of each of the 4 suits (Hearts, Diamonds, Spades, Clubs). What is the expected number of cards you have to draw from the deck until you have all 4 suits represented in your hand? I couldn't think of how to get the negative binomial to work, since this is sampling without replacement and has 4 suits instead of just 2. I imagine a distribution that could solve this might be called the Negative Hypergeometric Multivariate. Anyone have any ideas? Thanks very much.",,"['probability', 'statistics']"
32,Is this a known algebraic identity?,Is this a known algebraic identity?,,"In the course of analyzing a certain Markov chain, I once had to prove the following algebraic identity. Is there a slick or known proof? For $n$-tuples $(x_1,x_2,\dots, x_n)$ of positive real numbers define $$\mu(x_1,x_2,\dots, x_n)=\prod_{j=1}^n {x_j\over x_j+x_{j+1}+\cdots+x_n}.$$ Then if $x^\ast$ is another positive real, and $1\leq k\leq n+1$, then  define $x^*_k$ to be the $(n+1)$-tuple $(x_1,x_2,\dots, x^*,\dots, x_n)$ where  $x^*$ is in the $k$th place. The identity is $$\sum_{k=1}^{n+1}\ \mu(x^\ast_k)=\mu(x_1,x_2,\dots, x_n).$$ For example, $$ {xyz\over(x+y+z)(y+z)z}  + {yxz\over(y+x+z)(x+z)z}  + {yzx\over(y+z+x)(z+x)x}={yz\over(y+z)z}.$$","In the course of analyzing a certain Markov chain, I once had to prove the following algebraic identity. Is there a slick or known proof? For $n$-tuples $(x_1,x_2,\dots, x_n)$ of positive real numbers define $$\mu(x_1,x_2,\dots, x_n)=\prod_{j=1}^n {x_j\over x_j+x_{j+1}+\cdots+x_n}.$$ Then if $x^\ast$ is another positive real, and $1\leq k\leq n+1$, then  define $x^*_k$ to be the $(n+1)$-tuple $(x_1,x_2,\dots, x^*,\dots, x_n)$ where  $x^*$ is in the $k$th place. The identity is $$\sum_{k=1}^{n+1}\ \mu(x^\ast_k)=\mu(x_1,x_2,\dots, x_n).$$ For example, $$ {xyz\over(x+y+z)(y+z)z}  + {yxz\over(y+x+z)(x+z)z}  + {yzx\over(y+z+x)(z+x)x}={yz\over(y+z)z}.$$",,['algebra-precalculus']
33,Compute the expectation of steps making $n$ different balls the same,Compute the expectation of steps making  different balls the same,n,"Suppose that we are given $n$ balls of different colors. Each time we pick up two balls randomly and change the color of the second ball to that of the first, then we put them back. The question is what is the expectation of steps when these balls become the same color. I solved the problem for $n=2, 3, 4$. The naive solution is to solve a system of recursive equations. But there are too many states if $n$ is lager than $4$. As requested by the comment, I now present my solution for $n=3$; similar method works for $n=4$. Let $E(1, 2, 3)$ be the desired expectation, we have  $$ E(1, 2, 3)=1+\frac{1}{6}(E(1, 1, 2)+E(1, 2, 2)+etc.)=1+E(1,1, 2) $$ and  $$ E(1, 1, 2)=1+\frac{1}{3}(E(1,1,2)+E(1,1,1)+E(1, 2, 2))=1+\frac{2}{3}E(1,1,2). $$ It not hard to see that $E(1,1,2)$ is finite, hence $E(1,1,2)=3$ and $E(1,2,3)=4$. P.S. I was asked this question by a phone interviewer. If I or he didn't get the question wrong, then I hope there might be a 'clever and quick' solution for general cases.","Suppose that we are given $n$ balls of different colors. Each time we pick up two balls randomly and change the color of the second ball to that of the first, then we put them back. The question is what is the expectation of steps when these balls become the same color. I solved the problem for $n=2, 3, 4$. The naive solution is to solve a system of recursive equations. But there are too many states if $n$ is lager than $4$. As requested by the comment, I now present my solution for $n=3$; similar method works for $n=4$. Let $E(1, 2, 3)$ be the desired expectation, we have  $$ E(1, 2, 3)=1+\frac{1}{6}(E(1, 1, 2)+E(1, 2, 2)+etc.)=1+E(1,1, 2) $$ and  $$ E(1, 1, 2)=1+\frac{1}{3}(E(1,1,2)+E(1,1,1)+E(1, 2, 2))=1+\frac{2}{3}E(1,1,2). $$ It not hard to see that $E(1,1,2)$ is finite, hence $E(1,1,2)=3$ and $E(1,2,3)=4$. P.S. I was asked this question by a phone interviewer. If I or he didn't get the question wrong, then I hope there might be a 'clever and quick' solution for general cases.",,['probability']
34,Variance of exit time for simple symmetric random walk,Variance of exit time for simple symmetric random walk,,"For a simple symmetric random walk starting at 0 (that is, a Markov chain on the integers starting at 0 with equal probabilities of going to the left and right at each step), I want to compute the variance of the exit time $\tau$ from the interval $(a,b)$ where $a < 0$ and $b > 0$. I know the expectation of this exit time is $-ab$, as can be determined by a difference equation or by defining a clever martingale, namely $X_n^2-n$, and applying the optional stopping theorem. But neither of these methods generalize to compute $E(\tau^2)$. The difference equation can no longer be written down, because the change in $\tau^2$ from one step to the next depends on what $\tau$ already is. The martingale becomes a submartingale and yields the trivial fact that $Var(\tau) \geq 0$. Can anyone help with this problem?","For a simple symmetric random walk starting at 0 (that is, a Markov chain on the integers starting at 0 with equal probabilities of going to the left and right at each step), I want to compute the variance of the exit time $\tau$ from the interval $(a,b)$ where $a < 0$ and $b > 0$. I know the expectation of this exit time is $-ab$, as can be determined by a difference equation or by defining a clever martingale, namely $X_n^2-n$, and applying the optional stopping theorem. But neither of these methods generalize to compute $E(\tau^2)$. The difference equation can no longer be written down, because the change in $\tau^2$ from one step to the next depends on what $\tau$ already is. The martingale becomes a submartingale and yields the trivial fact that $Var(\tau) \geq 0$. Can anyone help with this problem?",,"['probability', 'stochastic-processes']"
35,Finding mode in Binomial distribution,Finding mode in Binomial distribution,,"Suppose that $X$ has the Binomial distribution with parameters $n,p$ . How can I show that if $(n+1)p$ is integer then $X$ has two mode that is $(n+1)p$ or $(n+1)p-1?$","Suppose that $X$ has the Binomial distribution with parameters $n,p$ . How can I show that if $(n+1)p$ is integer then $X$ has two mode that is $(n+1)p$ or $(n+1)p-1?$",,"['probability', 'statistics', 'binomial-distribution']"
36,What values makes this Markov chain aperiodic?,What values makes this Markov chain aperiodic?,,"Let the following transition matrix represent a $4$ state Markov chain $$\begin{pmatrix}   0 & a & 0 & b \\ \frac{1}{2} & 0 & \frac{1}{3}+c & d \\   0 & a & 0 & b  \\   e & 0 & f & 0  \end{pmatrix}$$ Let all the constants be positive real numbers, what values of these would make the chain aperiodic? Any help here is greatly appreciated, because I can't see how to do this other than finding an expression for the $n$th power of the matrix at the  diagonal values","Let the following transition matrix represent a $4$ state Markov chain $$\begin{pmatrix}   0 & a & 0 & b \\ \frac{1}{2} & 0 & \frac{1}{3}+c & d \\   0 & a & 0 & b  \\   e & 0 & f & 0  \end{pmatrix}$$ Let all the constants be positive real numbers, what values of these would make the chain aperiodic? Any help here is greatly appreciated, because I can't see how to do this other than finding an expression for the $n$th power of the matrix at the  diagonal values",,"['probability', 'markov-chains']"
37,Can you demystify the Power Law?,Can you demystify the Power Law?,,"How would you describe the Power Law in simple words? The Wikipedia entry is too long and verbose. I would like to understand the concept of the power law and how and why it shows up everywhere. For example, a recent Economist article Cry havoc! And let slip the maths of war , shows that terrorist attacks follow an inverted power law curve.","How would you describe the Power Law in simple words? The Wikipedia entry is too long and verbose. I would like to understand the concept of the power law and how and why it shows up everywhere. For example, a recent Economist article Cry havoc! And let slip the maths of war , shows that terrorist attacks follow an inverted power law curve.",,"['probability', 'statistics', 'probability-distributions']"
38,Probability that $2^a+3^b+5^c$ is divisible by 4,Probability that  is divisible by 4,2^a+3^b+5^c,"If $a,b,c\in{1,2,3,4,5}$, find the probability that $2^a+3^b+5^c$ is divisible by 4. For a number to be divisible by $4$, the last two digits have to be divisible by $4$ $5^c= \_~\_25$ if $c>1$ $3^1=3,~3^2=9,~3^3=27,~3^4=81,~ 3^5=243$ $2^1=2,~2^2=4,~2^3=8,~2^4=16,~2^5=32$ Should I add all possibilities? Is there a simpler method?","If $a,b,c\in{1,2,3,4,5}$, find the probability that $2^a+3^b+5^c$ is divisible by 4. For a number to be divisible by $4$, the last two digits have to be divisible by $4$ $5^c= \_~\_25$ if $c>1$ $3^1=3,~3^2=9,~3^3=27,~3^4=81,~ 3^5=243$ $2^1=2,~2^2=4,~2^3=8,~2^4=16,~2^5=32$ Should I add all possibilities? Is there a simpler method?",,"['probability', 'combinatorics', 'divisibility']"
39,Random variables defined on the same probability space with different distributions,Random variables defined on the same probability space with different distributions,,"Consider the real-valued random variable $X$ and suppose it is defined on the probability space $(\Omega, \mathcal{A}, \mathbb{P})$. Assume that $X \sim N(\mu, \sigma^2)$. This means that  $$ (1)\text{ } \mathbb{P}(X\in [a,b])=\mathbb{P}(\{w \in \Omega \text{ s.t } X(\omega)\in [a,b]\})=\frac{1}{2}\left(1+\frac{1}{\sqrt{\pi}}\int_{-(\frac{x-\mu}{\sigma \sqrt{2}})}^{(\frac{x-\mu}{\sigma \sqrt{2}})}e^{-t^2}dt\right) $$ In several books I found that we can also say that $X$ is distributed according to $\mathbb{P}$. Now suppose that we add another random variable $Y$ on the same probability space and assume $Y \sim U([0,1])$. This means that, for $0\leq a\leq b \leq 1$ $$ (2)\text{ } \mathbb{P}(Y \in [a,b])=\mathbb{P}(\{w \in \Omega \text{ s.t } Y(\omega)\in [a,b]\})=b-a $$ Question: the fact that $X$ and $Y$ are defined on the same probability space but have different probability distribution is a contradiction? What is the relation between $\mathbb{P}$, the normal cdf and the uniform cdf? Can we say that both $X$ and $Y$ are distributed according to $\mathbb{P}$ even if they have different distributions?","Consider the real-valued random variable $X$ and suppose it is defined on the probability space $(\Omega, \mathcal{A}, \mathbb{P})$. Assume that $X \sim N(\mu, \sigma^2)$. This means that  $$ (1)\text{ } \mathbb{P}(X\in [a,b])=\mathbb{P}(\{w \in \Omega \text{ s.t } X(\omega)\in [a,b]\})=\frac{1}{2}\left(1+\frac{1}{\sqrt{\pi}}\int_{-(\frac{x-\mu}{\sigma \sqrt{2}})}^{(\frac{x-\mu}{\sigma \sqrt{2}})}e^{-t^2}dt\right) $$ In several books I found that we can also say that $X$ is distributed according to $\mathbb{P}$. Now suppose that we add another random variable $Y$ on the same probability space and assume $Y \sim U([0,1])$. This means that, for $0\leq a\leq b \leq 1$ $$ (2)\text{ } \mathbb{P}(Y \in [a,b])=\mathbb{P}(\{w \in \Omega \text{ s.t } Y(\omega)\in [a,b]\})=b-a $$ Question: the fact that $X$ and $Y$ are defined on the same probability space but have different probability distribution is a contradiction? What is the relation between $\mathbb{P}$, the normal cdf and the uniform cdf? Can we say that both $X$ and $Y$ are distributed according to $\mathbb{P}$ even if they have different distributions?",,"['probability', 'measure-theory', 'probability-distributions']"
40,covariance of normal distribution,covariance of normal distribution,,"Suppose we have two normal distribution $\mathcal{N}(0,s)$, $\mathcal{N}(0,t)$. how can I find the covariance if I don't have the distribution function $f(x,y)$ or anything, not given the correlation (but they are correlated); I only know the distributions.","Suppose we have two normal distribution $\mathcal{N}(0,s)$, $\mathcal{N}(0,t)$. how can I find the covariance if I don't have the distribution function $f(x,y)$ or anything, not given the correlation (but they are correlated); I only know the distributions.",,"['probability', 'probability-distributions']"
41,"Is there a mathematical basis for the idea that this interpretation of confidence intervals is incorrect, or is it just frequentist philosophy?","Is there a mathematical basis for the idea that this interpretation of confidence intervals is incorrect, or is it just frequentist philosophy?",,"Suppose the mean time it takes all workers in a particular city to get to work is estimated as $21$. A $95\%$ confident interval is calculated to be $(18.3, 23.7).$ According to this website, the following statement is incorrect: There is a $95\%$ chance that the mean time it takes all workers in this city to get to work is between $18.3$ and $23.7$ minutes. Indeed, a lot websites echo a similar sentiment. This one , for example, says: It is not quite correct to ask about the probability that the interval contains the population mean. It either does or it doesn't. The meta-concept at work seems to be the idea that population parameters cannot be random, only the data we obtain about them can be random ( related ). This doesn't sit right with me, because I tend to think of probability as being fundamentally about our certainty that the world is a certain way. Also, if I understand correctly, there's really no mathematical basis for the notion that probabilities only apply to data and not parameters; in particular, this seems to be a manifestation of the frequentist/bayesianism debate. Question. If the above comments are correct, then it would seem that the kinds of statements made on the aforementioned websites shouldn't be taken too seriously. To make a stronger claim, I'm under the impression that if an exam grader were to mark a student down for the aforementioned ""incorrect"" interpretation of confidence intervals, my impression is that this would be inappropriate (this hasn't happened to me; it's a hypothetical). In any event, based on the underlying mathematics, are these fair comments I'm making, or is there something I'm missing?","Suppose the mean time it takes all workers in a particular city to get to work is estimated as $21$. A $95\%$ confident interval is calculated to be $(18.3, 23.7).$ According to this website, the following statement is incorrect: There is a $95\%$ chance that the mean time it takes all workers in this city to get to work is between $18.3$ and $23.7$ minutes. Indeed, a lot websites echo a similar sentiment. This one , for example, says: It is not quite correct to ask about the probability that the interval contains the population mean. It either does or it doesn't. The meta-concept at work seems to be the idea that population parameters cannot be random, only the data we obtain about them can be random ( related ). This doesn't sit right with me, because I tend to think of probability as being fundamentally about our certainty that the world is a certain way. Also, if I understand correctly, there's really no mathematical basis for the notion that probabilities only apply to data and not parameters; in particular, this seems to be a manifestation of the frequentist/bayesianism debate. Question. If the above comments are correct, then it would seem that the kinds of statements made on the aforementioned websites shouldn't be taken too seriously. To make a stronger claim, I'm under the impression that if an exam grader were to mark a student down for the aforementioned ""incorrect"" interpretation of confidence intervals, my impression is that this would be inappropriate (this hasn't happened to me; it's a hypothetical). In any event, based on the underlying mathematics, are these fair comments I'm making, or is there something I'm missing?",,"['probability', 'probability-theory', 'statistics', 'statistical-inference', 'philosophy']"
42,Probability of landing on the nth stair.,Probability of landing on the nth stair.,,"Initial Question : We begin climbing a staircase beginning at stair zero.  We  choose to take either 1,2, or 3 steps at a time, where each number of steps have an equal chance of being chosen.  What is the probability of hitting the fourth stair? How I proceeded : In lieu of recognizing what type of problem I was looking at, I formed a tree, and found that there are seven ways to hit the fourth stair.  Taking into consideration the weights of each of these outcomes I found the probability of hitting the fourth stair to be 37/81. Deeper Question : While I found that finding the number of ways to hit the $n$th step was not too difficult (I think it is the sum of the ways of hitting the three previous steps, $k(n-3)+k(n-2)+k(n-1)$), I was not able to use this successfully to find general rule for the probability of hitting the $n$th step. I am looking for some advice classifying this type of probability problem, or perhaps a link to a similar problem.  Could this be viewed as some sort of random walk?  Is there any hope for a closed form (explicit) solution?","Initial Question : We begin climbing a staircase beginning at stair zero.  We  choose to take either 1,2, or 3 steps at a time, where each number of steps have an equal chance of being chosen.  What is the probability of hitting the fourth stair? How I proceeded : In lieu of recognizing what type of problem I was looking at, I formed a tree, and found that there are seven ways to hit the fourth stair.  Taking into consideration the weights of each of these outcomes I found the probability of hitting the fourth stair to be 37/81. Deeper Question : While I found that finding the number of ways to hit the $n$th step was not too difficult (I think it is the sum of the ways of hitting the three previous steps, $k(n-3)+k(n-2)+k(n-1)$), I was not able to use this successfully to find general rule for the probability of hitting the $n$th step. I am looking for some advice classifying this type of probability problem, or perhaps a link to a similar problem.  Could this be viewed as some sort of random walk?  Is there any hope for a closed form (explicit) solution?",,"['probability', 'combinatorics', 'random-walk']"
43,Why is the sum of the rolls of two dices a Binomial Distribution? What is defined as a success in this experiment?,Why is the sum of the rolls of two dices a Binomial Distribution? What is defined as a success in this experiment?,,"I know that a Binomial Distribution, with parameters n and p, is the discrete probability distribution of the number of successes in a sequence of n independent yes/no experiments, each of which yields success with probability p. I read that the sum of the roll of two dice is a binomial distribution. Is this right? I know that the sum looks like a binomial: But if it is a binomial : What event do I regard as a success? What is its probability? How many times do I repeat the experiment? I mean a Binomial Distribution measures the probability of observing an event (which has probability of success p), k times in specific amount of repetitions n. Here I see 12 different events with different probabilities and I am confused.","I know that a Binomial Distribution, with parameters n and p, is the discrete probability distribution of the number of successes in a sequence of n independent yes/no experiments, each of which yields success with probability p. I read that the sum of the roll of two dice is a binomial distribution. Is this right? I know that the sum looks like a binomial: But if it is a binomial : What event do I regard as a success? What is its probability? How many times do I repeat the experiment? I mean a Binomial Distribution measures the probability of observing an event (which has probability of success p), k times in specific amount of repetitions n. Here I see 12 different events with different probabilities and I am confused.",,"['probability', 'dice', 'binomial-distribution']"
44,Proof of the inclusion-exclusion formula in probability,Proof of the inclusion-exclusion formula in probability,,"Let $(\Omega,\mathcal F, P)$ be a probability space and let $A_1,A_2,...,A_n$ be events in $\mathcal F$. Prove the following inclusion-exclusion formula $P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^n$ $\sum_{\mathcal J \subset \{1,...,n\}; |\mathcal J|=k} (-1)^{k+1}P(\bigcap_{i \in \mathcal J} A_i)$ I am trying to prove this formula by induction; for $n=2$, let $A, B$ be two events in $\mathcal F$. We can write $A=(A \setminus B) \cup (A\cap B)$, $B=(B \setminus A) \cup (A\cap B)$, since these are disjoint unions, then $P(A)=P(A \setminus B)+P(A\cap B)$ and $P(B)=P(B \setminus A)+P(A\cap B)$. Now, $A \cup B$ can be expressed as $A \cup B= (A\setminus B) \cup (A \cap B) \cup (B \setminus A)$, which it's also union of disjoint sets, so $P(A \cup B)=P(A \setminus B)+P(A\cap B)+P(B \setminus A)=P(A)+P(B)-P(A \cap B)$. This proves that for $n=2$, the formula is correct. Now, I want to show that the formula is true for $n \implies$ the formula is satisfied for $n+1$, I got stuck at this point, maybe I could express $P(\cup_{i=1}^{n+1} A_i)=P((\cup_{i=1}^n A_i \setminus A_{n+1}) \cup A_{n+1})=P(\cup_{i=1}^n A_i \setminus A_{n+1})+P(A_{n+1})$ (1) I can use the inductive hypothesis on the term $P(\cup_{i=1}^n A_i \setminus A_{n+1})$, so $P(\cup_{i=1}^n (A_i \setminus A_{n+1}))=\sum_{k=1}^n$ $\sum_{\mathcal J \subset \{1,...,n\}; |\mathcal J|=k} (-1)^{k+1}P(\bigcap_{i \in \mathcal J} (A_i\setminus A_{n+1}))$ Expression (1) becomes $\sum_{\mathcal J \subset \{1,...,n\}; |\mathcal J|=k} (-1)^{k+1}P(\bigcap_{i \in \mathcal J} (A_i\setminus A_{n+1})) +P(A_{n+1})$ I don't know to to take this expression to the one I want, which is $\sum_{k=1}^{n+1}$ $\sum_{\mathcal J \subset \{1,...,n+1\}; |\mathcal J|=k} (-1)^{k+1}P(\bigcap_{i \in \mathcal J} A_i)$ I would like to know how to continue this part of the exercise.","Let $(\Omega,\mathcal F, P)$ be a probability space and let $A_1,A_2,...,A_n$ be events in $\mathcal F$. Prove the following inclusion-exclusion formula $P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^n$ $\sum_{\mathcal J \subset \{1,...,n\}; |\mathcal J|=k} (-1)^{k+1}P(\bigcap_{i \in \mathcal J} A_i)$ I am trying to prove this formula by induction; for $n=2$, let $A, B$ be two events in $\mathcal F$. We can write $A=(A \setminus B) \cup (A\cap B)$, $B=(B \setminus A) \cup (A\cap B)$, since these are disjoint unions, then $P(A)=P(A \setminus B)+P(A\cap B)$ and $P(B)=P(B \setminus A)+P(A\cap B)$. Now, $A \cup B$ can be expressed as $A \cup B= (A\setminus B) \cup (A \cap B) \cup (B \setminus A)$, which it's also union of disjoint sets, so $P(A \cup B)=P(A \setminus B)+P(A\cap B)+P(B \setminus A)=P(A)+P(B)-P(A \cap B)$. This proves that for $n=2$, the formula is correct. Now, I want to show that the formula is true for $n \implies$ the formula is satisfied for $n+1$, I got stuck at this point, maybe I could express $P(\cup_{i=1}^{n+1} A_i)=P((\cup_{i=1}^n A_i \setminus A_{n+1}) \cup A_{n+1})=P(\cup_{i=1}^n A_i \setminus A_{n+1})+P(A_{n+1})$ (1) I can use the inductive hypothesis on the term $P(\cup_{i=1}^n A_i \setminus A_{n+1})$, so $P(\cup_{i=1}^n (A_i \setminus A_{n+1}))=\sum_{k=1}^n$ $\sum_{\mathcal J \subset \{1,...,n\}; |\mathcal J|=k} (-1)^{k+1}P(\bigcap_{i \in \mathcal J} (A_i\setminus A_{n+1}))$ Expression (1) becomes $\sum_{\mathcal J \subset \{1,...,n\}; |\mathcal J|=k} (-1)^{k+1}P(\bigcap_{i \in \mathcal J} (A_i\setminus A_{n+1})) +P(A_{n+1})$ I don't know to to take this expression to the one I want, which is $\sum_{k=1}^{n+1}$ $\sum_{\mathcal J \subset \{1,...,n+1\}; |\mathcal J|=k} (-1)^{k+1}P(\bigcap_{i \in \mathcal J} A_i)$ I would like to know how to continue this part of the exercise.",,['probability']
45,"$\frac{1}{e}=$""Probability that every chocolate goes into a wrong spot"".","""Probability that every chocolate goes into a wrong spot"".",\frac{1}{e}=,"While watching a video by Po Shen Loh I found something strange. In the video, he said that: Suppose I have a box of chocolates having $100$ chocolates, and I drop them all on the ground, and then I try to put them all back in. What is the probability that every chocolate went back in a wrong spot? According to him, the probability is $\frac{1}{e}$ . Now the question is that how can we get that? To me, It is as interesting as Buffon's Needle problem, that is why I am eager to know the method to reach at $\frac{1}{e}$ . I shall be thankful if you guys can provide me idea about what is happening. Thanks","While watching a video by Po Shen Loh I found something strange. In the video, he said that: Suppose I have a box of chocolates having chocolates, and I drop them all on the ground, and then I try to put them all back in. What is the probability that every chocolate went back in a wrong spot? According to him, the probability is . Now the question is that how can we get that? To me, It is as interesting as Buffon's Needle problem, that is why I am eager to know the method to reach at . I shall be thankful if you guys can provide me idea about what is happening. Thanks",100 \frac{1}{e} \frac{1}{e},"['probability', 'derangements']"
46,Conditional Expectation of X given X^2,Conditional Expectation of X given X^2,,What can we say about $E[X|X^2]$ in general? And if $X$ has density $f$ respect the Lebesgue measure?,What can we say about $E[X|X^2]$ in general? And if $X$ has density $f$ respect the Lebesgue measure?,,"['probability', 'random-variables', 'conditional-expectation']"
47,St. Petersburg Paradox,St. Petersburg Paradox,,"A fair coin will be tossed until a heads results. You will then be paid $2^{n-1}$ dollars where $n$ equals the number of flips. Now why is the expected pay out infinite?  $$ \sum_{n \geq 1} (\frac{1}{2^n})2^{n-1} = \sum_{n \geq 1} \frac{2^{n-1}}{2^n}  $$ Why does this give the payout? The payout shouldn't be infinite, it should be a potential infinity.   For example, if I flip the coin once and it is a tails, then I will receive two dollars for certain. On the second flip there is a $25$ percent chance that I will receive a minimum of $4$ dollars. but a $75$ percent chance that I will receive only two dollars. And so on as the chances of receiving more money decreases and eventually approaches $0$. Why then is it suggested that an investor pay any amount of money for such an opportunity? Source : A Brief History of the Paradox, Philosophy and Labyrinths of the Mind Obviously this appears false by intuition, but I'd like to know the argument for the paradox; why one would pay a trillion dollars to enter such a game. The argument just seems far too fallacious to me. How many times would one have to play? After almost twenty thousand plays, we are still making less than 8 dollars. IF every twenty thousand plays we increased our winnings by 8 dollars, we would have to play hundreds of billions of times just to start to have an average earnings equal to one trillion... we'd run out of time in the universe it seems... Edit I am curious,  however,  if the overall slope of the simulation data would remain consistent,  if so why? If not, why? What accounts for the vertical jumps we see occurring less and less often? Why do they occur at all?","A fair coin will be tossed until a heads results. You will then be paid $2^{n-1}$ dollars where $n$ equals the number of flips. Now why is the expected pay out infinite?  $$ \sum_{n \geq 1} (\frac{1}{2^n})2^{n-1} = \sum_{n \geq 1} \frac{2^{n-1}}{2^n}  $$ Why does this give the payout? The payout shouldn't be infinite, it should be a potential infinity.   For example, if I flip the coin once and it is a tails, then I will receive two dollars for certain. On the second flip there is a $25$ percent chance that I will receive a minimum of $4$ dollars. but a $75$ percent chance that I will receive only two dollars. And so on as the chances of receiving more money decreases and eventually approaches $0$. Why then is it suggested that an investor pay any amount of money for such an opportunity? Source : A Brief History of the Paradox, Philosophy and Labyrinths of the Mind Obviously this appears false by intuition, but I'd like to know the argument for the paradox; why one would pay a trillion dollars to enter such a game. The argument just seems far too fallacious to me. How many times would one have to play? After almost twenty thousand plays, we are still making less than 8 dollars. IF every twenty thousand plays we increased our winnings by 8 dollars, we would have to play hundreds of billions of times just to start to have an average earnings equal to one trillion... we'd run out of time in the universe it seems... Edit I am curious,  however,  if the overall slope of the simulation data would remain consistent,  if so why? If not, why? What accounts for the vertical jumps we see occurring less and less often? Why do they occur at all?",,"['probability', 'intuition', 'paradoxes']"
48,Average Distance Between Random Points in a Rectangle,Average Distance Between Random Points in a Rectangle,,"My question is similar to this one but for rectangles instead of lines. Suppose I have a rectangle with sides of length $L_w$ and $L_h$. What is the average distance between two uniformly-distributed random points inside the rectangle, and why?","My question is similar to this one but for rectangles instead of lines. Suppose I have a rectangle with sides of length $L_w$ and $L_h$. What is the average distance between two uniformly-distributed random points inside the rectangle, and why?",,"['probability', 'geometry', 'euclidean-geometry', 'average', 'uniform-distribution']"
49,Combining two probability distributions,Combining two probability distributions,,"I have a variable $X$. In a measurement $A$, $X$ follows the normal distribution $N_1$ with mean $m_1$ and standard deviation $\sigma_1$. In a similar measurement $B$, $X$ follows another normal distribution $N_2$ with mean $m_2$ and standard deviation $\sigma_2$.  In this case, what will be the combined or joint probability distribution of $X$? Will it be $N_1+N_2$  or $N_1 N_2$ ? (Addition) Let's assume $A$ and $B$ are independent measurements. We can think about a situation when a measurer $A$ comes and measure the distribution of $X$ and then next a person $B$ comes and measures the distribution again. Both measurers measure independently.  The question is what will be the true probability distribution of $X$ in this case? We assume that the measurement of $A$ and $B$ are equally reliable. (Paraphrasing a comment from r.e.s.) If person C receives reports from equally-reliable observers A and B, stating their respective independent judgements about X (in the form of the stated normal distributions), then how does C combine these reports to form a fair and unbiased judgement about X?","I have a variable $X$. In a measurement $A$, $X$ follows the normal distribution $N_1$ with mean $m_1$ and standard deviation $\sigma_1$. In a similar measurement $B$, $X$ follows another normal distribution $N_2$ with mean $m_2$ and standard deviation $\sigma_2$.  In this case, what will be the combined or joint probability distribution of $X$? Will it be $N_1+N_2$  or $N_1 N_2$ ? (Addition) Let's assume $A$ and $B$ are independent measurements. We can think about a situation when a measurer $A$ comes and measure the distribution of $X$ and then next a person $B$ comes and measures the distribution again. Both measurers measure independently.  The question is what will be the true probability distribution of $X$ in this case? We assume that the measurement of $A$ and $B$ are equally reliable. (Paraphrasing a comment from r.e.s.) If person C receives reports from equally-reliable observers A and B, stating their respective independent judgements about X (in the form of the stated normal distributions), then how does C combine these reports to form a fair and unbiased judgement about X?",,"['probability', 'probability-distributions', 'normal-distribution']"
50,expected area of a triangle determined by randomly placed points [duplicate],expected area of a triangle determined by randomly placed points [duplicate],,This question already has an answer here : The expected area of a triangle formed by three points randomly chosen from the unit square (1 answer) Closed 4 years ago . Three points are placed at independently and at random in a unit square. What is the expected value of the area of the triangle formed by the three points?,This question already has an answer here : The expected area of a triangle formed by three points randomly chosen from the unit square (1 answer) Closed 4 years ago . Three points are placed at independently and at random in a unit square. What is the expected value of the area of the triangle formed by the three points?,,['probability']
51,Equilibrium distributions of Markov Chains,Equilibrium distributions of Markov Chains,,"I often get confused about when a Markov chain has an equilibrium distribution; when this equilibrium distribution is unique; which starting states converge to the equilibrium distribution; and how finite and countably infinite Markov chains different with respect to the above. (Google isn't quite clearing up my confusion.) Is the following correct/am I missing anything? An irreducible Markov chain (finite or countably infinite) has a unique equilibrium distribution if and only if all states are positive recurrent. (What about reducible Markov chains? A reducible Markov chain has a non-unique equilibrium distribution iff all states are positive recurrent?) However, not all starting states necessarily converge to the unique equilibrium, unless the Markov chain is also aperiodic; that is, an irreducible Markov chain converges to its unique equilibrium regardless of initial state, if and only if all states are positive recurrent and aperiodic.","I often get confused about when a Markov chain has an equilibrium distribution; when this equilibrium distribution is unique; which starting states converge to the equilibrium distribution; and how finite and countably infinite Markov chains different with respect to the above. (Google isn't quite clearing up my confusion.) Is the following correct/am I missing anything? An irreducible Markov chain (finite or countably infinite) has a unique equilibrium distribution if and only if all states are positive recurrent. (What about reducible Markov chains? A reducible Markov chain has a non-unique equilibrium distribution iff all states are positive recurrent?) However, not all starting states necessarily converge to the unique equilibrium, unless the Markov chain is also aperiodic; that is, an irreducible Markov chain converges to its unique equilibrium regardless of initial state, if and only if all states are positive recurrent and aperiodic.",,"['probability', 'stochastic-processes', 'markov-chains']"
52,Probability of rolling a dice 8 times before all numbers are shown.,Probability of rolling a dice 8 times before all numbers are shown.,,"What is the probability of having to roll a (six sided) dice at least 8 times before you get to see all of the numbers at least once? I don't really have a clue how to work this out. Edit: If we are trying to find the number of situations where all of the numbers are shown, for seven rolls, a favorable outcome would be one in which there are two numbers the same, for example: 1123456. These numbers can be arranged in 7!/2!5! ways, and there are 6 different numbers that could be repeated. So the probability of getting all 6 numbers with 7 throws is (6*7!/2!5!)/6^7 = 126/279936.  Is that right?  Then 1 minus this is the probability? Edit: prob. of not getting all six numbers with seven rolls = 1-prob of getting all six numbers with seven rolls Six numbers same with seven rolls means one number repeated twice 7!/2! ways of doing this for each repeated number  6*(7!/2!) is number of ways of getting all six numbers with seven rolls. Total number of outcomes 6^7. Prob of getting all six numbers with seven rolls = (6*(7!/2!))/6^7 = 0.054 Prob of not getting all six numbers with seven rolls (=prob of needing at least 8 rolls to get all numbers) = 1-0.054 = 0.946","What is the probability of having to roll a (six sided) dice at least 8 times before you get to see all of the numbers at least once? I don't really have a clue how to work this out. Edit: If we are trying to find the number of situations where all of the numbers are shown, for seven rolls, a favorable outcome would be one in which there are two numbers the same, for example: 1123456. These numbers can be arranged in 7!/2!5! ways, and there are 6 different numbers that could be repeated. So the probability of getting all 6 numbers with 7 throws is (6*7!/2!5!)/6^7 = 126/279936.  Is that right?  Then 1 minus this is the probability? Edit: prob. of not getting all six numbers with seven rolls = 1-prob of getting all six numbers with seven rolls Six numbers same with seven rolls means one number repeated twice 7!/2! ways of doing this for each repeated number  6*(7!/2!) is number of ways of getting all six numbers with seven rolls. Total number of outcomes 6^7. Prob of getting all six numbers with seven rolls = (6*(7!/2!))/6^7 = 0.054 Prob of not getting all six numbers with seven rolls (=prob of needing at least 8 rolls to get all numbers) = 1-0.054 = 0.946",,"['probability', 'combinatorics']"
53,Estimating population standard deviation with sample standard deviation,Estimating population standard deviation with sample standard deviation,,"At 4:30 of this video the author decided to estimate the standard deviation of the population with sample standard deviation (sample size was $100$). In the next video, the author mentioned that it was reasonable because the sample size greater than $30$. Well, what tells us that we could estimate standard deviation in this way? Why is $30$ that magical boundary? Does it have anything to do with Central Limit Theorem? (I guess not, because we don't calculating the standard deviation of the mean, so it's not related in any way).","At 4:30 of this video the author decided to estimate the standard deviation of the population with sample standard deviation (sample size was $100$). In the next video, the author mentioned that it was reasonable because the sample size greater than $30$. Well, what tells us that we could estimate standard deviation in this way? Why is $30$ that magical boundary? Does it have anything to do with Central Limit Theorem? (I guess not, because we don't calculating the standard deviation of the mean, so it's not related in any way).",,"['probability', 'statistics', 'standard-deviation']"
54,How to explain why the probability of a continuous random variable at a specific value is 0?,How to explain why the probability of a continuous random variable at a specific value is 0?,,"Consider X as a continuous random variable which can assume any value in [0, 1]. It is known that P(X=x)=0 where P is the probability density function. I want to understand this intuitively. The math insight article helps me somewhat: In other words, the probability that the random number X is any   particular number x∈[0,1] (confused?) should be some constant value;   let's use c to denote this probability of any single number. But, now   we run into trouble due to the fact that there are an infinite number   of possibilities. If each possibility has the same probability c and   the probabilities must add up to 1 and there are an infinite number of   possibilities, what could the individual probability c possibly be? If   c were any finite number greater than zero, once we add up an infinite   number of the c's, we must get to infinity, which is definitely larger   than the required sum of 1. In order to prevent the sum from blowing   up to infinity, we must have c be infinitesimally small, i.e., we must   insist that c=0. But, what if I choose, c=1/N (where N is that large number) for the uniform case? The sum will still be 1 as far as I can understand. For the non-uniform case, I can pick some 0's and others non-zeros and still be theoretically able to get a sum of 1 for all the possible values. Anything that helps me understand this clearly will be of immense help. The answer here helps but I still don't get it.","Consider X as a continuous random variable which can assume any value in [0, 1]. It is known that P(X=x)=0 where P is the probability density function. I want to understand this intuitively. The math insight article helps me somewhat: In other words, the probability that the random number X is any   particular number x∈[0,1] (confused?) should be some constant value;   let's use c to denote this probability of any single number. But, now   we run into trouble due to the fact that there are an infinite number   of possibilities. If each possibility has the same probability c and   the probabilities must add up to 1 and there are an infinite number of   possibilities, what could the individual probability c possibly be? If   c were any finite number greater than zero, once we add up an infinite   number of the c's, we must get to infinity, which is definitely larger   than the required sum of 1. In order to prevent the sum from blowing   up to infinity, we must have c be infinitesimally small, i.e., we must   insist that c=0. But, what if I choose, c=1/N (where N is that large number) for the uniform case? The sum will still be 1 as far as I can understand. For the non-uniform case, I can pick some 0's and others non-zeros and still be theoretically able to get a sum of 1 for all the possible values. Anything that helps me understand this clearly will be of immense help. The answer here helps but I still don't get it.",,"['probability', 'random-variables']"
55,What is the complement of conditional probabilities?,What is the complement of conditional probabilities?,,"I am working with a problem that uses Bayes Theorem and conditional probabilities. I have the conditional probability that a plane has an emergency locator $(E)$ given that it was discovered $(D)$ which is $P(E\mid D)=0.60$. Now I am given that $P(E'\mid D')=0.90$, where a plane does not have a emergency locator given that it was not discovered. I wanted to know what the complement of $P(E'\mid D')$ would be. Is it $P(E\mid D)$ or $P(E\mid D')$? I am not sure whether or not to flip the $D$ in the conditional.","I am working with a problem that uses Bayes Theorem and conditional probabilities. I have the conditional probability that a plane has an emergency locator $(E)$ given that it was discovered $(D)$ which is $P(E\mid D)=0.60$. Now I am given that $P(E'\mid D')=0.90$, where a plane does not have a emergency locator given that it was not discovered. I wanted to know what the complement of $P(E'\mid D')$ would be. Is it $P(E\mid D)$ or $P(E\mid D')$? I am not sure whether or not to flip the $D$ in the conditional.",,"['probability', 'elementary-set-theory', 'bayes-theorem']"
56,Expectation of random variables ratio,Expectation of random variables ratio,,"Let $X_1, X_2, \dots, X_n$ be $n$ positive iid random variables. Then show that $$E\left(\frac{\sum_{j=1}^k X_j}{\sum_{i=1}^{n} X_i}\right) = \frac{k}{n}.$$ Because of the linearlity of the expectation I known that $E\left(\frac{\sum_{j=1}^k X_j}{\sum_{i=1}^{n} X_i}\right) = \sum_{j=1}^k E\left(\frac{X_j}{\sum_{i=1}^{n} X_i}\right)$, so it's enougth to show $E\left(\frac{X_1}{\sum_{i=1}^{n} X_i}\right) = \frac{1}{n}$. But I'm unable to deal with the $X_i$ in the denominator.","Let $X_1, X_2, \dots, X_n$ be $n$ positive iid random variables. Then show that $$E\left(\frac{\sum_{j=1}^k X_j}{\sum_{i=1}^{n} X_i}\right) = \frac{k}{n}.$$ Because of the linearlity of the expectation I known that $E\left(\frac{\sum_{j=1}^k X_j}{\sum_{i=1}^{n} X_i}\right) = \sum_{j=1}^k E\left(\frac{X_j}{\sum_{i=1}^{n} X_i}\right)$, so it's enougth to show $E\left(\frac{X_1}{\sum_{i=1}^{n} X_i}\right) = \frac{1}{n}$. But I'm unable to deal with the $X_i$ in the denominator.",,"['probability', 'random-variables', 'expectation']"
57,How can I solve bins-and-balls problems?,How can I solve bins-and-balls problems?,,"Below is the problem that I wanted to solve When there are $m$ balls and $n$ bins, balls are thrown into  bins where each ball is thrown into a bin uniformly at random. What is the expected number of bins that contain strictly more than 1 ball? What I am understanding so far is that, each ball toss will be independent, and expected number of balls in each basket will be $m/n$. But this does not seem to help me solving the problem. I heard this is very similar to the birthday problem, but with different number of bins and arbitrary number of balls. How should I approach solving this problem?","Below is the problem that I wanted to solve When there are $m$ balls and $n$ bins, balls are thrown into  bins where each ball is thrown into a bin uniformly at random. What is the expected number of bins that contain strictly more than 1 ball? What I am understanding so far is that, each ball toss will be independent, and expected number of balls in each basket will be $m/n$. But this does not seem to help me solving the problem. I heard this is very similar to the birthday problem, but with different number of bins and arbitrary number of balls. How should I approach solving this problem?",,"['probability', 'balls-in-bins', 'birthday']"
58,$P(X<Y)$ where X and Y are exponential with means $2$ and $1$,where X and Y are exponential with means  and,P(X<Y) 2 1,"Given that $X$ and $Y$ are independent variables and that $X$ is exponential with a mean of $2$ and $Y$ is exponential with a mean of $1$. Find $P(X<Y)$? From the information it can be concluded that $X \sim \operatorname{Exp}(\frac{1}{2})$ and $Y \sim \operatorname{Exp}(1)$ Hence, $f(x,y) = \frac{1}{2}e^{-\frac{1}{2}x} e^{-y} $ Therefore, $P(X<Y) = \int_{0}^{\infty} \int_{0}^{y} \frac{1}{2}e^{-\frac{1}{2}x} e^{-y} \,\,dx dy = \frac{1}{3}$ Is this the correct way to solve this problem?","Given that $X$ and $Y$ are independent variables and that $X$ is exponential with a mean of $2$ and $Y$ is exponential with a mean of $1$. Find $P(X<Y)$? From the information it can be concluded that $X \sim \operatorname{Exp}(\frac{1}{2})$ and $Y \sim \operatorname{Exp}(1)$ Hence, $f(x,y) = \frac{1}{2}e^{-\frac{1}{2}x} e^{-y} $ Therefore, $P(X<Y) = \int_{0}^{\infty} \int_{0}^{y} \frac{1}{2}e^{-\frac{1}{2}x} e^{-y} \,\,dx dy = \frac{1}{3}$ Is this the correct way to solve this problem?",,"['probability', 'statistics']"
59,Monty Hall Problem Intuition,Monty Hall Problem Intuition,,"I was thinking about the Monty Hall problem and I thought of a possible intuitive explanation: You choose a door. Monty gives you the option of sticking with your original choice or instead choosing both of the other two doors. If you decided to switch (which now becomes an obvious choice), Monty first opens the door with the goat behind it (say, to add to the excitement), and then opens the other door. My question then is, is this reasoning flawed? Is this even the same problem as before? Because now, choosing to switch from one door to two doors becomes quite obvious, and so does the $2/3^{rd}$ chance of winning the car on switching.","I was thinking about the Monty Hall problem and I thought of a possible intuitive explanation: You choose a door. Monty gives you the option of sticking with your original choice or instead choosing both of the other two doors. If you decided to switch (which now becomes an obvious choice), Monty first opens the door with the goat behind it (say, to add to the excitement), and then opens the other door. My question then is, is this reasoning flawed? Is this even the same problem as before? Because now, choosing to switch from one door to two doors becomes quite obvious, and so does the $2/3^{rd}$ chance of winning the car on switching.",,"['probability', 'probability-theory', 'monty-hall']"
60,From correlation coefficient to conditional probability,From correlation coefficient to conditional probability,,"In the best-selling book Thinking Fast and Slow (p. 205), Daniel Kahneman (a Nobel Prize winner in Economics) makes the following claim: 'Suppose you consider many pairs of firms. The two firms in each pair are generally similar, but the CEO of one of them is better than the other. How often will you find that the firm with the stronger CEO is the more successful of the two? . . . A correlation of .30 [between stronger CEO and more successful firm] implies that you would find the stronger CEO leading the stronger firm in about 60% of the pairs.' How is the 'about 60%' computed under plausible assumptions?","In the best-selling book Thinking Fast and Slow (p. 205), Daniel Kahneman (a Nobel Prize winner in Economics) makes the following claim: 'Suppose you consider many pairs of firms. The two firms in each pair are generally similar, but the CEO of one of them is better than the other. How often will you find that the firm with the stronger CEO is the more successful of the two? . . . A correlation of .30 [between stronger CEO and more successful firm] implies that you would find the stronger CEO leading the stronger firm in about 60% of the pairs.' How is the 'about 60%' computed under plausible assumptions?",,"['probability', 'statistics']"
61,"Why do we refer to the denominator of Bayes' theorem as ""marginal probability""?","Why do we refer to the denominator of Bayes' theorem as ""marginal probability""?",,"Consider the following characterization of the Bayes' theorem: Bayes' Theorem Given some observed data $x$ , the posterior probability that the paramater $\Theta$ has the value $\theta$ is $p(\theta \mid x) = p(x \mid \theta) p (\theta) / p(x)$ , where $p(x \mid \theta)$ is the likelihood, $p(\theta)$ is the prior probability of the value $\theta$ , and $p(x)$ is the marginal probability of the value $x$ . Is there any special reason why we call $p(x)$ the ""marginal probability""? What is ""marginal"" about it?","Consider the following characterization of the Bayes' theorem: Bayes' Theorem Given some observed data , the posterior probability that the paramater has the value is , where is the likelihood, is the prior probability of the value , and is the marginal probability of the value . Is there any special reason why we call the ""marginal probability""? What is ""marginal"" about it?",x \Theta \theta p(\theta \mid x) = p(x \mid \theta) p (\theta) / p(x) p(x \mid \theta) p(\theta) \theta p(x) x p(x),"['probability', 'probability-theory', 'bayes-theorem']"
62,"Proof of ""continuity from above"" and ""continuity from below"" from the axioms of probability","Proof of ""continuity from above"" and ""continuity from below"" from the axioms of probability",,"One of the consequences of the axioms of probability ($\sigma$ field and probability axiom) is the ""infinite subset"" and ""infinite union"" property, I can't figure out how it follows from them. if $A_1 \subseteq A_2 \subseteq \cdots$ then $$P\left(\bigcup_{i=1}^{\infty}A_i \right) = \lim_{k \to \infty} P(A_k)  $$ If $A_1 \supseteq A_2 \supseteq \cdots$ then $$P\left(\bigcap_{i=1}^{\infty}A_i \right) = \lim_{k \to \infty} P(A_k)  $$","One of the consequences of the axioms of probability ($\sigma$ field and probability axiom) is the ""infinite subset"" and ""infinite union"" property, I can't figure out how it follows from them. if $A_1 \subseteq A_2 \subseteq \cdots$ then $$P\left(\bigcup_{i=1}^{\infty}A_i \right) = \lim_{k \to \infty} P(A_k)  $$ If $A_1 \supseteq A_2 \supseteq \cdots$ then $$P\left(\bigcap_{i=1}^{\infty}A_i \right) = \lim_{k \to \infty} P(A_k)  $$",,"['probability', 'probability-theory']"
63,Question about connection between Poisson and Gamma distributions,Question about connection between Poisson and Gamma distributions,,"Assuming $X\sim\mathcal{P}(\lambda)$ and $Y\sim\Gamma(w,1)$ prove that $P(X\ge w)=P(Y\le \lambda)$. How this fact is lead from the connection between the poisson and exponential distributions? I don't know from where to start. poisson is defined only for discrete situations but the exponential is only for continious situation. How can I prove the fact ? EDIT : for gamma distribution I wrote that $f_Y(y)=\frac{y^{w-1}e^{-y}}{\Gamma(w)}$, but I have problem with integrating it. About Poisson: its function is $\displaystyle \sum _{w_i=0}^w P(X=w_i)$ which I don't know how to sum into a final expression. How can I continue?","Assuming $X\sim\mathcal{P}(\lambda)$ and $Y\sim\Gamma(w,1)$ prove that $P(X\ge w)=P(Y\le \lambda)$. How this fact is lead from the connection between the poisson and exponential distributions? I don't know from where to start. poisson is defined only for discrete situations but the exponential is only for continious situation. How can I prove the fact ? EDIT : for gamma distribution I wrote that $f_Y(y)=\frac{y^{w-1}e^{-y}}{\Gamma(w)}$, but I have problem with integrating it. About Poisson: its function is $\displaystyle \sum _{w_i=0}^w P(X=w_i)$ which I don't know how to sum into a final expression. How can I continue?",,['probability']
64,Average number of days to see all possible cards,Average number of days to see all possible cards,,"My father and I go to the restaurant everyday, and each one of us needs to grab a card, which has a number from 1 to 600. I thought about registering every new card we see in a list, and a question arose: ""How many days, on average, would we need to see every possible card?"" Each card has an equal probability of being chosen The two cards my father and I get are different So far, I reasoned that the minimum number is 300, if it were the case that everyday we got different cards. In addition, it gets ever more harder to reach the final number of cards. When the list of seen cards is almost complete, it is more likely that we are going to draw a card that has already been seen before, and not a new one. I coded 1000 simulations in python and this is the graph of the number of days expected to reach $n$ cards on average: which is compatible with my reasoning, and results in an average of 2088 days for us to see each one of the 600 cards. But I would like to see a non-brute force way to derive such value.","My father and I go to the restaurant everyday, and each one of us needs to grab a card, which has a number from 1 to 600. I thought about registering every new card we see in a list, and a question arose: ""How many days, on average, would we need to see every possible card?"" Each card has an equal probability of being chosen The two cards my father and I get are different So far, I reasoned that the minimum number is 300, if it were the case that everyday we got different cards. In addition, it gets ever more harder to reach the final number of cards. When the list of seen cards is almost complete, it is more likely that we are going to draw a card that has already been seen before, and not a new one. I coded 1000 simulations in python and this is the graph of the number of days expected to reach cards on average: which is compatible with my reasoning, and results in an average of 2088 days for us to see each one of the 600 cards. But I would like to see a non-brute force way to derive such value.",n,"['probability', 'expected-value', 'coupon-collector']"
65,Is there something like a normal distribution model for discrete probability?,Is there something like a normal distribution model for discrete probability?,,"If one wants to find the probability that a continuous random variable will fall within a range of $a \leq X \leq b$, based on a mean value $\mu$, and a deviation of $\sigma$, he would integrate the normal distribution function: $$\int^b_a \frac{e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}dx$$ Since this is for continuous probability, is there an alternative to normal distribution for discrete probability? Suppose $\mu = 100$, and $\sigma = 50$. For discrete probability, I would try to use bounds close together to achieve a similar, but still not the completely desired outcome. However, the probability is very low: $$\int^{99.95}_{100.05}p(x)dx = 0.0008$$ The probability seems too low for this to be true, which suggests that a different model for discrete probability should exist.","If one wants to find the probability that a continuous random variable will fall within a range of $a \leq X \leq b$, based on a mean value $\mu$, and a deviation of $\sigma$, he would integrate the normal distribution function: $$\int^b_a \frac{e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}dx$$ Since this is for continuous probability, is there an alternative to normal distribution for discrete probability? Suppose $\mu = 100$, and $\sigma = 50$. For discrete probability, I would try to use bounds close together to achieve a similar, but still not the completely desired outcome. However, the probability is very low: $$\int^{99.95}_{100.05}p(x)dx = 0.0008$$ The probability seems too low for this to be true, which suggests that a different model for discrete probability should exist.",,"['probability', 'normal-distribution']"
66,A very challenging probability question,A very challenging probability question,,"In a certain 2-player game, the winner is determined by rolling a single 6-sided die in turn, until a 6 is shown, at which point the game ends immediately. Now, suppose that k dice are now rolled simultaneously by each player on his turn, and the ﬁrst player to obtain a total of k (or more) 6’s, accumulated over all his throws, wins the game. (For example, if k = 3, then player 1 will throw 3 dice, and keep track of any 6’s that show up. If player 1 did not get all 6’s then player 2 will do the same. Assuming that player 1 gets another turn, he will again throw 3 dice, and any 6’s that show up will be added to his previous total). Compute the expected number of turns that will be needed to complete the game. I've analysed this problem as follows: The problem can be modeled by a negative binomial distribution with probability $p=\frac{1}{6}$. Now, X is a random variable representing the  number of dice being thrown. I need to find the cdf $Pr[X\geq k]$, and then find the expectation as follows $E[X] = \int_0^\infty kPr[X\geq k]$. The problem here is that the cdf of a negative binomal distribution is a regularized beta function and this is quite messy to deal with. I'm wondering  is there another way to approach this problem that wouldn't involve that?","In a certain 2-player game, the winner is determined by rolling a single 6-sided die in turn, until a 6 is shown, at which point the game ends immediately. Now, suppose that k dice are now rolled simultaneously by each player on his turn, and the ﬁrst player to obtain a total of k (or more) 6’s, accumulated over all his throws, wins the game. (For example, if k = 3, then player 1 will throw 3 dice, and keep track of any 6’s that show up. If player 1 did not get all 6’s then player 2 will do the same. Assuming that player 1 gets another turn, he will again throw 3 dice, and any 6’s that show up will be added to his previous total). Compute the expected number of turns that will be needed to complete the game. I've analysed this problem as follows: The problem can be modeled by a negative binomial distribution with probability $p=\frac{1}{6}$. Now, X is a random variable representing the  number of dice being thrown. I need to find the cdf $Pr[X\geq k]$, and then find the expectation as follows $E[X] = \int_0^\infty kPr[X\geq k]$. The problem here is that the cdf of a negative binomal distribution is a regularized beta function and this is quite messy to deal with. I'm wondering  is there another way to approach this problem that wouldn't involve that?",,"['probability', 'combinatorics', 'probability-distributions']"
67,What is the probability that $XYZ$ is divisible by $5$?,What is the probability that  is divisible by ?,XYZ 5,"A solution of $X + Y + Z = 20$ in non-negative integers is chosen at random. What is the probability that $XYZ$ is divisible by $5$? Edit: This happens to be an exam question. So I can't use calculators or computers and have to get the answer in less than 20 minutes while showing systematic workings. I appreciate the answers below, but can someone instruct me on solving the question given the mentioned constraints?","A solution of $X + Y + Z = 20$ in non-negative integers is chosen at random. What is the probability that $XYZ$ is divisible by $5$? Edit: This happens to be an exam question. So I can't use calculators or computers and have to get the answer in less than 20 minutes while showing systematic workings. I appreciate the answers below, but can someone instruct me on solving the question given the mentioned constraints?",,['probability']
68,You see a route 14 bus on the moon. What is the most likely number of bus routes on the moon?,You see a route 14 bus on the moon. What is the most likely number of bus routes on the moon?,,"This question was asked on a forum and while many argued that the answer is 14 (since the probability of you seeing bus 14 is maximum in this case), I argued against it that they were working backwards. My claim is that this question is invalid as there is no method to determine the probability of number of bus routes. I'm looking for clarification as to the right answer (with proof obviously)","This question was asked on a forum and while many argued that the answer is 14 (since the probability of you seeing bus 14 is maximum in this case), I argued against it that they were working backwards. My claim is that this question is invalid as there is no method to determine the probability of number of bus routes. I'm looking for clarification as to the right answer (with proof obviously)",,"['probability', 'statistics', 'parameter-estimation']"
69,martingale and filtration,martingale and filtration,,"As I understand, martingale is a stochastic process (i.e., a sequence of random variables) such that the conditional expected value of an observation at some time $t$, given all the observations up to some earlier time $s$, is equal to the observation at that earlier time $s$. A sequence $Y_1, Y_2, Y_3 ...$ is said to be a martingale with respect to another sequence $X_1, X_2, X_3 ...$ if for all $n$: $E(Y_{n+1}|X_1,...,X_n) = Y_n$ Now I don't understand how it is defined in terms of filtration. Does filtration discretize the time space of a stochastic process so that we can analyze the process as a martingale? A simple explanation or an example on what is filtration and how it relates to martingale theory would be very helpful. I can then read more detailed content.","As I understand, martingale is a stochastic process (i.e., a sequence of random variables) such that the conditional expected value of an observation at some time $t$, given all the observations up to some earlier time $s$, is equal to the observation at that earlier time $s$. A sequence $Y_1, Y_2, Y_3 ...$ is said to be a martingale with respect to another sequence $X_1, X_2, X_3 ...$ if for all $n$: $E(Y_{n+1}|X_1,...,X_n) = Y_n$ Now I don't understand how it is defined in terms of filtration. Does filtration discretize the time space of a stochastic process so that we can analyze the process as a martingale? A simple explanation or an example on what is filtration and how it relates to martingale theory would be very helpful. I can then read more detailed content.",,"['probability', 'measure-theory', 'probability-theory', 'martingales']"
70,Probability of rain after an amount of time in Minecraft game,Probability of rain after an amount of time in Minecraft game,,"Minecraft time is measured in ticks. When a world is loaded, the game waits anywhere from 12,000 to 180,000 ticks to start raining. After it starts raining, the rain lasts anywhere from 12,000 to 24,000 ticks. When the rain ends, clear weather lasts for another 12,000 to 180,000 ticks, etc. This cycle repeats forever. I want to know if there is a function that can determine the chance that it's raining on tick t . From ticks 0 to 12,000 the chance is 0%. From ticks 12,000 to 24,000 the chance is (t - 12,000) / 168,000 (I believe) After this, I got confused. What would the probabilities be past this point? I created a simulation to brute force it and this is what the graph looked like (x is ticks in thousands, y is the percentage chance of rain). But I'm not actually sure what the function for this would be. Thank you.","Minecraft time is measured in ticks. When a world is loaded, the game waits anywhere from 12,000 to 180,000 ticks to start raining. After it starts raining, the rain lasts anywhere from 12,000 to 24,000 ticks. When the rain ends, clear weather lasts for another 12,000 to 180,000 ticks, etc. This cycle repeats forever. I want to know if there is a function that can determine the chance that it's raining on tick t . From ticks 0 to 12,000 the chance is 0%. From ticks 12,000 to 24,000 the chance is (t - 12,000) / 168,000 (I believe) After this, I got confused. What would the probabilities be past this point? I created a simulation to brute force it and this is what the graph looked like (x is ticks in thousands, y is the percentage chance of rain). But I'm not actually sure what the function for this would be. Thank you.",,['probability']
71,Transformation of Random Variable $Y = X^2$,Transformation of Random Variable,Y = X^2,"I'm learning probability, specifically transformations of random variables, and need help to understand the solution to the following exercise: Consider the continuous random variable $X$ with probability density function $$f(x) = \begin{cases} \frac{1}{3}x^2 \quad  -1 \leq x \leq 2, \\ 0 \quad \quad \text{elsewhere}. \end{cases}$$ Find the cumulative distribution function of the random variable $Y = X^2$. The author gives the following solution: For $0 \leq y \leq 1: F_Y(y) = P(Y \leq y) = P(X^2 \leq y) \stackrel{?}{=} P(-\sqrt y \leq X \leq \sqrt y) = \int_{-\sqrt y}^{\sqrt y}\frac{1}{3}x^2\, dx = \frac{2}{9}y\sqrt y.$ For $1 \leq y \leq 4: F_Y(y) = P(Y \leq y) = P(X^2 \leq y) \stackrel{?}{=} P(-1 \leq X \leq \sqrt y) = \int_{-1}^{\sqrt y}\frac{1}{3}x^2\, dx = \frac{1}{9} + \frac{1}{9}y\sqrt y.$ For $y > 4: F_{Y}(y) = 1.$ Previous to this exercise, I've managed to follow the solutions of two similar (obviously simpler) problems for a strictly increasing and strictly decreasing function of $X$, respectively. However in this problem, I don't understand the computations being done, specifically: How does the three intervals $0 \leq y \leq 1$, $1 \leq y \leq 4$ and $y > 4$ are determined? In the two previous problems I've encountered, we only considered one interval which was identical to the interval where $f(x)$ was non-zero. In the case where $0 \leq y \leq 1$, why does $P(X^2 \leq y) = P(-\sqrt y \leq X \leq \sqrt y)$ and not $P(X \leq \sqrt y)$? I have put question marks above the equalities that I don't understand. I think I have not understand the theory well enough. I'm looking for an answer that will make me understand the solution to this problem and possibly make the theory clearer.","I'm learning probability, specifically transformations of random variables, and need help to understand the solution to the following exercise: Consider the continuous random variable $X$ with probability density function $$f(x) = \begin{cases} \frac{1}{3}x^2 \quad  -1 \leq x \leq 2, \\ 0 \quad \quad \text{elsewhere}. \end{cases}$$ Find the cumulative distribution function of the random variable $Y = X^2$. The author gives the following solution: For $0 \leq y \leq 1: F_Y(y) = P(Y \leq y) = P(X^2 \leq y) \stackrel{?}{=} P(-\sqrt y \leq X \leq \sqrt y) = \int_{-\sqrt y}^{\sqrt y}\frac{1}{3}x^2\, dx = \frac{2}{9}y\sqrt y.$ For $1 \leq y \leq 4: F_Y(y) = P(Y \leq y) = P(X^2 \leq y) \stackrel{?}{=} P(-1 \leq X \leq \sqrt y) = \int_{-1}^{\sqrt y}\frac{1}{3}x^2\, dx = \frac{1}{9} + \frac{1}{9}y\sqrt y.$ For $y > 4: F_{Y}(y) = 1.$ Previous to this exercise, I've managed to follow the solutions of two similar (obviously simpler) problems for a strictly increasing and strictly decreasing function of $X$, respectively. However in this problem, I don't understand the computations being done, specifically: How does the three intervals $0 \leq y \leq 1$, $1 \leq y \leq 4$ and $y > 4$ are determined? In the two previous problems I've encountered, we only considered one interval which was identical to the interval where $f(x)$ was non-zero. In the case where $0 \leq y \leq 1$, why does $P(X^2 \leq y) = P(-\sqrt y \leq X \leq \sqrt y)$ and not $P(X \leq \sqrt y)$? I have put question marks above the equalities that I don't understand. I think I have not understand the theory well enough. I'm looking for an answer that will make me understand the solution to this problem and possibly make the theory clearer.",,['probability']
72,Expected value of size of subset,Expected value of size of subset,,"Given a set $S$ such that $|S|=n$, A random item is chosen randomly from $S$, and being appended to a new set $T$. This process is being repeated $n$ times (with repetition), what is the expected value of $|T|$ ?","Given a set $S$ such that $|S|=n$, A random item is chosen randomly from $S$, and being appended to a new set $T$. This process is being repeated $n$ times (with repetition), what is the expected value of $|T|$ ?",,"['probability', 'expectation', 'balls-in-bins']"
73,Conditional expectation to de maximum $E(X_1\mid X_{(n)})$,Conditional expectation to de maximum,E(X_1\mid X_{(n)}),"Let $X_1, \ldots, X_n$ a random sample of a Uniform(0,1): Which is $E(X_1\mid X_{(n)})$ ? where $X_{(n)}=\max\{X_1,\ldots,X_n\}$","Let $X_1, \ldots, X_n$ a random sample of a Uniform(0,1): Which is $E(X_1\mid X_{(n)})$ ? where $X_{(n)}=\max\{X_1,\ldots,X_n\}$",,"['probability', 'order-statistics', 'conditional-expectation']"
74,Exchanging max and expectation,Exchanging max and expectation,,"If $X$ is a random variable and $\rho$ is a parameter, and $L$ is a concave function of $(\rho,X)$, under what conditions is the following statement true? $$\mathbb{E}\max_{\rho} L(\rho,X) =\max_{\rho}\mathbb{E}( L(\rho,X)).$$ I can show that  $$\mathbb{E}\max_{\rho} L(\rho,X) \geq \max_{\rho}\mathbb{E}( L(\rho,X)),$$ for all $L$ and $X$ (using a proof very similar to that of the min-max theorem). However, I am not able to derive conditions under which the other inequality holds.","If $X$ is a random variable and $\rho$ is a parameter, and $L$ is a concave function of $(\rho,X)$, under what conditions is the following statement true? $$\mathbb{E}\max_{\rho} L(\rho,X) =\max_{\rho}\mathbb{E}( L(\rho,X)).$$ I can show that  $$\mathbb{E}\max_{\rho} L(\rho,X) \geq \max_{\rho}\mathbb{E}( L(\rho,X)),$$ for all $L$ and $X$ (using a proof very similar to that of the min-max theorem). However, I am not able to derive conditions under which the other inequality holds.",,"['probability', 'optimization', 'expectation']"
75,Exchange integral and conditional expectation,Exchange integral and conditional expectation,,I know that if we have $E[\int_0^1 |X_t|dt] < \infty$ we may apply Fubini's theorem and compute $E[\int_0^1 X_tdt] = \int_0^1 E[X_t]dt$. Is there a similar version that allows the exchange of integral and conditional expectation? I want to say $E[\int_0^1 X_tdt|F_s] = \int_0^1 E[X_t|F_s]dt$ but I don't know why it should be true.,I know that if we have $E[\int_0^1 |X_t|dt] < \infty$ we may apply Fubini's theorem and compute $E[\int_0^1 X_tdt] = \int_0^1 E[X_t]dt$. Is there a similar version that allows the exchange of integral and conditional expectation? I want to say $E[\int_0^1 X_tdt|F_s] = \int_0^1 E[X_t|F_s]dt$ but I don't know why it should be true.,,"['probability', 'measure-theory', 'stochastic-calculus']"
76,For which probability distributions does $\lim_{n\to\infty} \mathbb{E}\left[\left(\sum_1^n X_i\right)^2 / \left(\sum_1^n {X_i}^2\right)\right]$ exist?,For which probability distributions does  exist?,\lim_{n\to\infty} \mathbb{E}\left[\left(\sum_1^n X_i\right)^2 / \left(\sum_1^n {X_i}^2\right)\right],"Say, we have $n$ positive i.i.d. random variables, $X_1, X_2, \dots, X_n$ which are distributed according to some probability distribution, $f$ . So, only distributions on either $[0, 1]$ or $[0, +\infty]$ . The expected value of the ratio between the square of the sum and the sum of squares is then a function of $n$ and the distribution $f$ , $$ G(n; f) = \mathbb{E}\left[\frac{\left(\sum_{i=1}^{n} X_i\right)^2}{\sum_{i=1}^{n} X_i^2 } \right] $$ In general, I am interested in the behavior of this function, especially in the large- $n$ limit, or as $n \to \infty$ . Specifically, for which distributions is $G(n; f)$ asymptotically constant? It's easy to see that $G(n)$ is invariant to scaling such as $X_i \to a X_i$ . So, there is no loss of generality when limiting ourselves to either distributions on $[0, 1]$ or $[0, +\infty]$ . However, I suspect that it is not possible to get constant $G$ at large- $n$ for distributions on bounded intervals, anyway. Here's how far I've gotten myself: Firstly, we know that $1 \le G(n) \le n$ by the Cauchy-Schwarz inequality. $G(n) = n$ is only true for the Dirac delta distribution ( $X_i = 1$ ). It seems for some common distributions I tried out numerically, $G(n; f) \propto n$ in the large- $n$ limit. For example, with the uniform distribution, as $n \to \infty$ $$ G(n; U(0, 1)) \sim \frac{3}{4} n$$ I used some very non-rigorous manipulation (""physicist math""), assuming that the expected value of the ratio is the ratio of expected values in the large- $n$ limit, to derive $$ G(n; f) \sim \frac{\mu^2}{\mu^2 + \sigma^2} n$$ where $f$ is a distribution with mean $\mu$ and variance $\sigma^2$ . Numerically, this seems to agree with the uniform and log-normal distributions. My guess is that it applies to any distribution with finite mean and variance. Numerically, I also found that for power-law distributions with heavy tails (such as Pareto distribution with $\alpha < 1$ ), $G(n)$ does appear to converge to a constant. But I can't seem to show this mathematically or find an expression for $G(n)$ in terms of $\alpha$ . Edit: An interesting, perhaps useful result from Albrecher & Teugels (2007) , is that if $f$ is a Pareto-type distribution with $0 < \alpha < 1$ , then $$\lim_{n\to\infty} \mathbb{E}\left[\frac{\sum_{i=1}^{n} X_i^2 }{\left(\sum_{i=1}^{n} X_i\right)^2}\right] = {1 - \alpha} $$","Say, we have positive i.i.d. random variables, which are distributed according to some probability distribution, . So, only distributions on either or . The expected value of the ratio between the square of the sum and the sum of squares is then a function of and the distribution , In general, I am interested in the behavior of this function, especially in the large- limit, or as . Specifically, for which distributions is asymptotically constant? It's easy to see that is invariant to scaling such as . So, there is no loss of generality when limiting ourselves to either distributions on or . However, I suspect that it is not possible to get constant at large- for distributions on bounded intervals, anyway. Here's how far I've gotten myself: Firstly, we know that by the Cauchy-Schwarz inequality. is only true for the Dirac delta distribution ( ). It seems for some common distributions I tried out numerically, in the large- limit. For example, with the uniform distribution, as I used some very non-rigorous manipulation (""physicist math""), assuming that the expected value of the ratio is the ratio of expected values in the large- limit, to derive where is a distribution with mean and variance . Numerically, this seems to agree with the uniform and log-normal distributions. My guess is that it applies to any distribution with finite mean and variance. Numerically, I also found that for power-law distributions with heavy tails (such as Pareto distribution with ), does appear to converge to a constant. But I can't seem to show this mathematically or find an expression for in terms of . Edit: An interesting, perhaps useful result from Albrecher & Teugels (2007) , is that if is a Pareto-type distribution with , then","n X_1, X_2, \dots, X_n f [0, 1] [0, +\infty] n f  G(n; f) = \mathbb{E}\left[\frac{\left(\sum_{i=1}^{n} X_i\right)^2}{\sum_{i=1}^{n} X_i^2 } \right]  n n \to \infty G(n; f) G(n) X_i \to a X_i [0, 1] [0, +\infty] G n 1 \le G(n) \le n G(n) = n X_i = 1 G(n; f) \propto n n n \to \infty  G(n; U(0, 1)) \sim \frac{3}{4} n n  G(n; f) \sim \frac{\mu^2}{\mu^2 + \sigma^2} n f \mu \sigma^2 \alpha < 1 G(n) G(n) \alpha f 0 < \alpha < 1 \lim_{n\to\infty} \mathbb{E}\left[\frac{\sum_{i=1}^{n} X_i^2 }{\left(\sum_{i=1}^{n} X_i\right)^2}\right] = {1 - \alpha} ","['probability', 'statistics', 'probability-distributions', 'random-variables']"
77,Probability: Escaping Prisoner Question,Probability: Escaping Prisoner Question,,"Question : A prisoner in a dark dungeon discovers three tunnels leading from his cell. Unbeknownst to him, the first tunnel reaches a dead end after 50 feet and the second tunnel reaches a dead end after 20 feet, but the third tunnel leads to freedom after 100 feet. Each day, the prisoner picks a tunnel at random and crawls along it. If he reaches a dead end he has to crawl back to his cell. (The darkness is so complete that he might try the same tunnel on successive days). Find the expected distance that he crawls to reach freedom. My Guess : I know that each tunnel has a probability of $\frac{1}{3}$ of being chosen. $$ \text{If Distance to freedom} = 100(X) + 40(Y) + (100)(Z)$$ Where $$  X = \text{number of times he tries tunnel 1}$$ $$ Y = \text{number of times he tries tunnel 2} $$ $$Z = \text{number of times he tries tunnel 3}$$ $$ \text{ (=1 since it leads him to freedom)} $$ Then $$ E(D) = E(100X + 40Y + 100) = 100E(X) + 40E(Y) + 100$$ I'm not sure if this logic is correct, and I'm a little confused on how to find E(X) and E(Y). Thanks!","Question : A prisoner in a dark dungeon discovers three tunnels leading from his cell. Unbeknownst to him, the first tunnel reaches a dead end after 50 feet and the second tunnel reaches a dead end after 20 feet, but the third tunnel leads to freedom after 100 feet. Each day, the prisoner picks a tunnel at random and crawls along it. If he reaches a dead end he has to crawl back to his cell. (The darkness is so complete that he might try the same tunnel on successive days). Find the expected distance that he crawls to reach freedom. My Guess : I know that each tunnel has a probability of of being chosen. Where Then I'm not sure if this logic is correct, and I'm a little confused on how to find E(X) and E(Y). Thanks!",\frac{1}{3}  \text{If Distance to freedom} = 100(X) + 40(Y) + (100)(Z)   X = \text{number of times he tries tunnel 1}  Y = \text{number of times he tries tunnel 2}  Z = \text{number of times he tries tunnel 3}  \text{ (=1 since it leads him to freedom)}   E(D) = E(100X + 40Y + 100) = 100E(X) + 40E(Y) + 100,"['probability', 'probability-theory', 'statistics', 'expected-value']"
78,Expectation of gradient in stochastic gradient descent algorithm,Expectation of gradient in stochastic gradient descent algorithm,,I'm studying stochastic gradient descent algorithm for optimization. It looks like this: $$ \begin{aligned} L(w) &= \frac{1}{N} \sum_{n=1}^{N} L_n(w) \\ w^{(t+1)} &= w^{(t)} - \gamma \nabla L_n(w^{(t)}) \end{aligned} $$ I assume that $n$ is chosen randomly each time the algorithm iterates. The problem comes when my notes state that $\Bbb E[\nabla L_n(w)] = \nabla L(w)$ . Where does this come from?,I'm studying stochastic gradient descent algorithm for optimization. It looks like this: I assume that is chosen randomly each time the algorithm iterates. The problem comes when my notes state that . Where does this come from?, \begin{aligned} L(w) &= \frac{1}{N} \sum_{n=1}^{N} L_n(w) \\ w^{(t+1)} &= w^{(t)} - \gamma \nabla L_n(w^{(t)}) \end{aligned}  n \Bbb E[\nabla L_n(w)] = \nabla L(w),"['probability', 'statistics', 'optimization', 'numerical-optimization', 'gradient-descent']"
79,Probability that two random integers have only one prime factor in common,Probability that two random integers have only one prime factor in common,,"The probability that two integers picked at random are relatively prime is known to be $1/\zeta{(2)}=6/\pi^2\approx0.607927...$. Generalizing, the probability that $n$ random integers have $\gcd=1$ is $1/\zeta{(n)}$. What is the probability that two random integers have one (and only one) prime factor in common? I did some calculations and obtained the formula $\displaystyle\frac{P(2)}{\zeta(2)}=0.274933...$, where $P(n)$ is the prime zeta function. In general, I suppose that the probability that $n$ random integers have only one prime factor in common is $\displaystyle\frac{P(n)}{\zeta(n)}$. I would be interested in having confirmation of these results, and in getting a formal proof. I would also like to obtain a generalization expressing the probability that $n$ integers picked at random have exactly $k$ prime numbers in common.","The probability that two integers picked at random are relatively prime is known to be $1/\zeta{(2)}=6/\pi^2\approx0.607927...$. Generalizing, the probability that $n$ random integers have $\gcd=1$ is $1/\zeta{(n)}$. What is the probability that two random integers have one (and only one) prime factor in common? I did some calculations and obtained the formula $\displaystyle\frac{P(2)}{\zeta(2)}=0.274933...$, where $P(n)$ is the prime zeta function. In general, I suppose that the probability that $n$ random integers have only one prime factor in common is $\displaystyle\frac{P(n)}{\zeta(n)}$. I would be interested in having confirmation of these results, and in getting a formal proof. I would also like to obtain a generalization expressing the probability that $n$ integers picked at random have exactly $k$ prime numbers in common.",,"['probability', 'number-theory', 'prime-numbers', 'divisibility', 'riemann-zeta']"
80,The continuity of the expectation of a continuous stochastic procees,The continuity of the expectation of a continuous stochastic procees,,"Let $X_t$ be a continuous stochastic process on a filtered space $(\Omega, \mathcal F, \mathcal F_t, \mathbb P)$. Is $\mathbb E[X_t]$ necessarily a continuous function? My first answer would be no. For example if $X_t$ admits densities $f(t,x)$, the first  equality in: $$\lim_{t \rightarrow t_0} \int_{\mathbb R} x f(t, x) dx=\int_{\mathbb R} \lim_{t \rightarrow t_0} x f(t,x)=\int_{\mathbb R} x f(t_0,x) $$ requires $f$ to be continuous in $t$ uniformly in $x$ to hold. Examples where $\mathbb E[X_t]$ is indeed continuous are abundant. Counterexamples where it is not? Thanks.","Let $X_t$ be a continuous stochastic process on a filtered space $(\Omega, \mathcal F, \mathcal F_t, \mathbb P)$. Is $\mathbb E[X_t]$ necessarily a continuous function? My first answer would be no. For example if $X_t$ admits densities $f(t,x)$, the first  equality in: $$\lim_{t \rightarrow t_0} \int_{\mathbb R} x f(t, x) dx=\int_{\mathbb R} \lim_{t \rightarrow t_0} x f(t,x)=\int_{\mathbb R} x f(t_0,x) $$ requires $f$ to be continuous in $t$ uniformly in $x$ to hold. Examples where $\mathbb E[X_t]$ is indeed continuous are abundant. Counterexamples where it is not? Thanks.",,"['probability', 'stochastic-processes', 'continuity', 'expectation']"
81,Strategy for Black&White game,Strategy for Black&White game,,"Consider the following game. Let $n$ be a positive integer. There are two players, $\newcommand\A{\mathrm{A}}\A$ and $\newcommand\B{\mathrm{B}}\B$ , and a referee. $\A$ and $\B$ first agree on a strategy. After this step, $\A$ and $\B$ cannot communicate. Initially, the referee chooses a sequence $\{c_i\}$ of length $n$ , where each entry is either “black” or “white”. $\A$ is made aware of this entire sequence, but $\B$ is not. There are a series of $n$ rounds, where $\A$ and $\B$ simultaneously guess either “black” or “white”. During the $k^\text{th}$ round, the team wins a point if $\A$ ’s guess and $\B$ ’s guess are both equal to the $k^\text{th}$ entry of the referee’s sequence. After each round, $\B$ is told what $\A$ ’s guess was, and what the referee’s pick was, for that round. This means $\B$ makes each decision while aware of all information from previous rounds. Find a strategy for $\A$ and $\B$ so that they can guarantee winning $g(n)$ times, where $\lim\limits_{n\to+\infty}\frac{g(n)}n=\frac34$ . I have a strategy for which $\lim\limits_{n\to+\infty}\frac{g(n)}n=\frac35$ . We will prove that $\A$ and $\B$ can win $3$ times in $5$ games. Here is the strategy. In the first game, $\A$ will guess the color that appears more in the subsequence $(c_2,c_3,c_4)$ , and $\B$ will guess this color in game $2$ to $4$ . If $c_2=c_3=c_4$ , then $\A$ and $\B$ can win games $2$ to $4$ (so $3$ games have been won). Otherwise, $\A$ and $\B$ will lose exactly one game among $2$ to $4$ . But $\A$ knows which game that will be, so they can guess $c_5$ in that game so that $\A$ and $\B$ win the fifth game. I have no strategy for which $\lim\limits_{n\to+\infty}\frac{g(n)}n=\frac34$ though.","Consider the following game. Let be a positive integer. There are two players, and , and a referee. and first agree on a strategy. After this step, and cannot communicate. Initially, the referee chooses a sequence of length , where each entry is either “black” or “white”. is made aware of this entire sequence, but is not. There are a series of rounds, where and simultaneously guess either “black” or “white”. During the round, the team wins a point if ’s guess and ’s guess are both equal to the entry of the referee’s sequence. After each round, is told what ’s guess was, and what the referee’s pick was, for that round. This means makes each decision while aware of all information from previous rounds. Find a strategy for and so that they can guarantee winning times, where . I have a strategy for which . We will prove that and can win times in games. Here is the strategy. In the first game, will guess the color that appears more in the subsequence , and will guess this color in game to . If , then and can win games to (so games have been won). Otherwise, and will lose exactly one game among to . But knows which game that will be, so they can guess in that game so that and win the fifth game. I have no strategy for which though.","n \newcommand\A{\mathrm{A}}\A \newcommand\B{\mathrm{B}}\B \A \B \A \B \{c_i\} n \A \B n \A \B k^\text{th} \A \B k^\text{th} \B \A \B \A \B g(n) \lim\limits_{n\to+\infty}\frac{g(n)}n=\frac34 \lim\limits_{n\to+\infty}\frac{g(n)}n=\frac35 \A \B 3 5 \A (c_2,c_3,c_4) \B 2 4 c_2=c_3=c_4 \A \B 2 4 3 \A \B 2 4 \A c_5 \A \B \lim\limits_{n\to+\infty}\frac{g(n)}n=\frac34","['probability', 'combinatorics', 'information-theory']"
82,Average value of $\frac{x'A^2x}{x'A^3x}$ over surface of $n$-dimensional sphere,Average value of  over surface of -dimensional sphere,\frac{x'A^2x}{x'A^3x} n,"Suppose $A$ is a diagonal matrix with eigenvalues $1,\frac{1}{2},\frac{1}{3},\ldots,\frac{1}{n}$ and $x$ is drawn from standard Gaussian in $n$ dimensions. In numerical simulations, the following quantity seems to converge to $2$ as $n\rightarrow \infty$ $$z_n=E_{x\sim \mathcal{N}\left(0, I_n\right)}\left[\frac{x^T A^2 x}{x^T A^3 x}\right]$$ Can this be proven or disproven? $z_n$ can also be written as the following sum $$z_n=\sum_{i=1}^n i E_{y\sim \mathcal{N}\left(0,A^3\right)}\left[\frac{y_i^2}{\|y\|^2}\right]$$ This quantity can be viewed as the average ratio of quadratic forms $A^2$ and $A^3$ on the surface of $n$ -dimensional sphere. Here's what the distribution looks like for a few values of $n$ , means are tending towards $2$","Suppose is a diagonal matrix with eigenvalues and is drawn from standard Gaussian in dimensions. In numerical simulations, the following quantity seems to converge to as Can this be proven or disproven? can also be written as the following sum This quantity can be viewed as the average ratio of quadratic forms and on the surface of -dimensional sphere. Here's what the distribution looks like for a few values of , means are tending towards","A 1,\frac{1}{2},\frac{1}{3},\ldots,\frac{1}{n} x n 2 n\rightarrow \infty z_n=E_{x\sim \mathcal{N}\left(0, I_n\right)}\left[\frac{x^T A^2 x}{x^T A^3 x}\right] z_n z_n=\sum_{i=1}^n i E_{y\sim \mathcal{N}\left(0,A^3\right)}\left[\frac{y_i^2}{\|y\|^2}\right] A^2 A^3 n n 2","['probability', 'sequences-and-series', 'statistics', 'definite-integrals', 'normal-distribution']"
83,A Measure Theoretic formulation of Bayes' Theorem,A Measure Theoretic formulation of Bayes' Theorem,,"I am trying to find a measure theoretic formulation of Bayes' theorem, when used in statistical inference, Bayes' theorem is usually defined as: $$p\left(\theta|x\right) = \frac{p\left(x|\theta\right) \cdot p\left(\theta\right)}{p\left(x\right)}$$ where: $p\left(\theta|x\right)$ : the posterior density of the parameter. $p\left(x|\theta\right)$ : the statistical model (or likelihood ). $p\left(\theta\right)$ : the prior density of the parameter. $p\left(x\right)$ : the evidence . Now how would we define Bayes' theorem in a measure theoretic way? So, I started by defining a probability space: $$\left(\Theta, \mathcal{F}_\Theta, \mathbb{P}_\Theta\right)$$ such that $\theta \in \Theta$ . I then defined another probability space: $$\left(X, \mathcal{F}_X, \mathbb{P}_X\right)$$ such that $x \in X$ . From here now on I don't know what to do, the joint probability space would be: $$\left(\Theta \times X, \mathcal{F}_\Theta \otimes \mathcal{F}_X, ?\right)$$ but I don't know what the measure should be. Bayes' theorem should be written as follow: $$? = \frac{? \cdot \mathbb{P}_\Theta}{\mathbb{P}_X}$$ where: $$\mathbb{P}_X = \int_{\theta \in \Theta} ? \space \mathrm{d}\mathbb{P}_\Theta$$ but as you can see I don't know the other measures and in which probability space they reside. I stumbled upon this thread but it was of little help and I don't know how was the following measure-theoretic generalization of Bayes' rule reached: $${P_{\Theta |y}}(A) = \int\limits_{x \in A} {\frac{{\mathrm d{P_{\Omega |x}}}}{{\mathrm d{P_\Omega }}}(y)\mathrm d{P_\Theta }}$$ I'm self-learning measure theoretic probability and lack guidance so excuse my ignorance.","I am trying to find a measure theoretic formulation of Bayes' theorem, when used in statistical inference, Bayes' theorem is usually defined as: where: : the posterior density of the parameter. : the statistical model (or likelihood ). : the prior density of the parameter. : the evidence . Now how would we define Bayes' theorem in a measure theoretic way? So, I started by defining a probability space: such that . I then defined another probability space: such that . From here now on I don't know what to do, the joint probability space would be: but I don't know what the measure should be. Bayes' theorem should be written as follow: where: but as you can see I don't know the other measures and in which probability space they reside. I stumbled upon this thread but it was of little help and I don't know how was the following measure-theoretic generalization of Bayes' rule reached: I'm self-learning measure theoretic probability and lack guidance so excuse my ignorance.","p\left(\theta|x\right) = \frac{p\left(x|\theta\right) \cdot p\left(\theta\right)}{p\left(x\right)} p\left(\theta|x\right) p\left(x|\theta\right) p\left(\theta\right) p\left(x\right) \left(\Theta, \mathcal{F}_\Theta, \mathbb{P}_\Theta\right) \theta \in \Theta \left(X, \mathcal{F}_X, \mathbb{P}_X\right) x \in X \left(\Theta \times X, \mathcal{F}_\Theta \otimes \mathcal{F}_X, ?\right) ? = \frac{? \cdot \mathbb{P}_\Theta}{\mathbb{P}_X} \mathbb{P}_X = \int_{\theta \in \Theta} ? \space \mathrm{d}\mathbb{P}_\Theta {P_{\Theta |y}}(A) = \int\limits_{x \in A} {\frac{{\mathrm d{P_{\Omega |x}}}}{{\mathrm d{P_\Omega }}}(y)\mathrm d{P_\Theta }}","['probability', 'probability-theory', 'measure-theory']"
84,Convergence in distribution (weak convergence) of sum of real-valued random variables,Convergence in distribution (weak convergence) of sum of real-valued random variables,,"Suppose that $\{(X_n,Y_n)\}^\infty_{n=1}$ is a sequence of pairs of real-valued random variables that converge in distribution to $(X,Y)$. Show that $X_n + Y_n$ converges in distribution to X+Y. Attempt at a solution:  If $h(x,y) = x+y$, then $h(x,y)$ is continuous, and so is any $f(h(x,y))$ where $f$ is a continuous function. I know that the class of bounded, continuous functions is measure-determining, which I think would be useful, but I just can't wrap my head around how to apply it.","Suppose that $\{(X_n,Y_n)\}^\infty_{n=1}$ is a sequence of pairs of real-valued random variables that converge in distribution to $(X,Y)$. Show that $X_n + Y_n$ converges in distribution to X+Y. Attempt at a solution:  If $h(x,y) = x+y$, then $h(x,y)$ is continuous, and so is any $f(h(x,y))$ where $f$ is a continuous function. I know that the class of bounded, continuous functions is measure-determining, which I think would be useful, but I just can't wrap my head around how to apply it.",,"['probability', 'probability-theory', 'probability-distributions', 'weak-convergence']"
85,The Price is Right optimal play,The Price is Right optimal play,,"The following situation happened on the Price is Right and I was curious about the optimal response. The rules are: A contestant rolls a wheel with 5 cent increments from 5 - 100 (20 numbers total). A contestant can choose to spin the wheel once and accept the number the wheel landed on (stay) or spin again and have this new number added to the previous number. If the contestant has a number that is over 100 cents they automatically lose. The object of the game is to roll the highest number out of a set of contestants. GirlA rolled once and rolled 60 cents. GirlA knows GirlB will play afterwards. Should GirlA roll again? At what number is the expected value neutral / what range of numbers should GirlA stay on? If there is more than one player behind, what range of numbers is best to stay on?","The following situation happened on the Price is Right and I was curious about the optimal response. The rules are: A contestant rolls a wheel with 5 cent increments from 5 - 100 (20 numbers total). A contestant can choose to spin the wheel once and accept the number the wheel landed on (stay) or spin again and have this new number added to the previous number. If the contestant has a number that is over 100 cents they automatically lose. The object of the game is to roll the highest number out of a set of contestants. GirlA rolled once and rolled 60 cents. GirlA knows GirlB will play afterwards. Should GirlA roll again? At what number is the expected value neutral / what range of numbers should GirlA stay on? If there is more than one player behind, what range of numbers is best to stay on?",,"['probability', 'game-theory', 'expectation']"
86,Rigorous Book on Stochastic Calculus,Rigorous Book on Stochastic Calculus,,"I have already taken a couse in Stochastic Calculus. Due to time constraints on many ocassions we had to skip some formalities among the proofs. I'm trying now to fill the gaps left, and I have been searching for a book to do so. My problem is that I haven't found many good references. I'm intersted in a book (or books) with rigorous treatment of: Brownian Motion (Wiener Process, Wiener Measure and construction) Martingale Theory (Discrete and Continuous, but specially the transition from Discrete to Continuous Time) Stochastic Calculus (Ito Integration) SDE I have already explored some books such as Karatsas but have found them very dry and almost encyclopedia like, which is something I don't like from books. Any references (online notes or books) are appreciated. I'm kind of trying to overcome the thought that this subject (Stochastic Calculus) is filled with dry formalities. I'm trying to find a treatment which balances intuition and formality but without feeling dry and devoid of motivation. By the way I have a good base on measure theory so no problem with it as a prequisite. Thanks in advance","I have already taken a couse in Stochastic Calculus. Due to time constraints on many ocassions we had to skip some formalities among the proofs. I'm trying now to fill the gaps left, and I have been searching for a book to do so. My problem is that I haven't found many good references. I'm intersted in a book (or books) with rigorous treatment of: Brownian Motion (Wiener Process, Wiener Measure and construction) Martingale Theory (Discrete and Continuous, but specially the transition from Discrete to Continuous Time) Stochastic Calculus (Ito Integration) SDE I have already explored some books such as Karatsas but have found them very dry and almost encyclopedia like, which is something I don't like from books. Any references (online notes or books) are appreciated. I'm kind of trying to overcome the thought that this subject (Stochastic Calculus) is filled with dry formalities. I'm trying to find a treatment which balances intuition and formality but without feeling dry and devoid of motivation. By the way I have a good base on measure theory so no problem with it as a prequisite. Thanks in advance",,"['probability', 'probability-theory', 'stochastic-calculus', 'martingales', 'stochastic-analysis']"
87,Expected number of steps for reaching $K$ in a random walk,Expected number of steps for reaching  in a random walk,K,"Assuming steps are $+1/-1$ with a $50/50$ probability. What is the expected step count for reaching $10, 100$ or $K$?","Assuming steps are $+1/-1$ with a $50/50$ probability. What is the expected step count for reaching $10, 100$ or $K$?",,"['probability', 'random-walk']"
88,What is the probability on rolling $2n$ dice that the sum of the first $n$ equals the sum of the last $n$?,What is the probability on rolling  dice that the sum of the first  equals the sum of the last ?,2n n n,"The Question What is the probability, rolling $n$ six-sided dice twice, that their sum each time totals to the same amount? For example, if $n = 4$, and we roll $1,3,4,6$ and $2,2,5,5$, adding them gives $$ 1+3+4+6 = 14 = 2+2+5+5 $$ What is the probability this happens as a function of $n$? Early Investigation This problem is not too hard for $n = 1$ or $n = 2$ via brute force... For $n = 2$: Tie at a total of $2$: $$ \frac{1}{36} * \frac{1}{36} = \frac{1}{1296} $$ Tie at a total of $3$: $$ \frac{2}{36} * \frac{2}{36} = \frac{4}{1296} $$ etc. so the answer is  $$ \frac{1^2 + 2^2 + 3^2 + ... + 6^6 + 5^2 + ... + 1^2}{1296} = \frac{\frac{(6)(7)(13)}{6} + \frac{(5)(6)(11)}{6}}{1296}  = \frac{146}{1296} $$ Note that I use the formula: $\sum_{k=1}^{n}k^2=\frac{(n)(n+1)(2n+1)}{6}$. Is there a way to do this in general for $n$ dice? Or at least a process for coming up with a reasonably fast brute force formula? The Difficulty The problem arises that the sum of squares is not so simple when we get to three dice. Using a spreadsheet, I figured out we need to sum these squares for 3 dice: $$ 1, 3, 6, 10, 15, 21, 25, 27, 27, 25, 21, 15, 10, 6, 3, 1 $$ For a brute force answer of $\frac{4332}{46656}$. Note how we can no longer use the sum of squares formula, as the squares we need to sum are no longer linear. Some Thoughts I am no closer to figuring out an answer for $n$ dice, and obviously the question becomes increasingly more difficult for more dice. One thing I noticed: I see a resemblance to Pascal's Triangle here, except we start with the first row being six $1$, not one $1$. Se we have: 1 1 1 1 1 1           1 2 3 4 5 6 5 4 3 2 1  1 3 6 10 15 21 25 27 27 25 21 15 10 6 3 1 1 4 9 16 25 36 46 52 54 52 46 36 25 16 9 4 1 ... but that's still a process, not a formula. And still not practical for $n = 200$. I know how to prove the formula for any cell in Pascal's Triangle to be $C(n,r) = \frac{n!}{r!(n-r)!}$... using induction; that doesn't really give me any hints to deterministically figuring out a similar formula for my modified triangle. Also there is no immediately obvious sum for a row of this triangle like there is (powers of 2) in Pascal's Triangle. Any insight would be appreciated. Thanks in advance!","The Question What is the probability, rolling $n$ six-sided dice twice, that their sum each time totals to the same amount? For example, if $n = 4$, and we roll $1,3,4,6$ and $2,2,5,5$, adding them gives $$ 1+3+4+6 = 14 = 2+2+5+5 $$ What is the probability this happens as a function of $n$? Early Investigation This problem is not too hard for $n = 1$ or $n = 2$ via brute force... For $n = 2$: Tie at a total of $2$: $$ \frac{1}{36} * \frac{1}{36} = \frac{1}{1296} $$ Tie at a total of $3$: $$ \frac{2}{36} * \frac{2}{36} = \frac{4}{1296} $$ etc. so the answer is  $$ \frac{1^2 + 2^2 + 3^2 + ... + 6^6 + 5^2 + ... + 1^2}{1296} = \frac{\frac{(6)(7)(13)}{6} + \frac{(5)(6)(11)}{6}}{1296}  = \frac{146}{1296} $$ Note that I use the formula: $\sum_{k=1}^{n}k^2=\frac{(n)(n+1)(2n+1)}{6}$. Is there a way to do this in general for $n$ dice? Or at least a process for coming up with a reasonably fast brute force formula? The Difficulty The problem arises that the sum of squares is not so simple when we get to three dice. Using a spreadsheet, I figured out we need to sum these squares for 3 dice: $$ 1, 3, 6, 10, 15, 21, 25, 27, 27, 25, 21, 15, 10, 6, 3, 1 $$ For a brute force answer of $\frac{4332}{46656}$. Note how we can no longer use the sum of squares formula, as the squares we need to sum are no longer linear. Some Thoughts I am no closer to figuring out an answer for $n$ dice, and obviously the question becomes increasingly more difficult for more dice. One thing I noticed: I see a resemblance to Pascal's Triangle here, except we start with the first row being six $1$, not one $1$. Se we have: 1 1 1 1 1 1           1 2 3 4 5 6 5 4 3 2 1  1 3 6 10 15 21 25 27 27 25 21 15 10 6 3 1 1 4 9 16 25 36 46 52 54 52 46 36 25 16 9 4 1 ... but that's still a process, not a formula. And still not practical for $n = 200$. I know how to prove the formula for any cell in Pascal's Triangle to be $C(n,r) = \frac{n!}{r!(n-r)!}$... using induction; that doesn't really give me any hints to deterministically figuring out a similar formula for my modified triangle. Also there is no immediately obvious sum for a row of this triangle like there is (powers of 2) in Pascal's Triangle. Any insight would be appreciated. Thanks in advance!",,"['probability', 'dice', 'summation']"
89,Joint moments of Brownian motion,Joint moments of Brownian motion,,"My approach to this SE question uses the following joint moments of Brownian motion. For $n=1,2$ they are obvious and well-known, the others  are not terribly hard to work out. Is there a reference where these  formulas are given, or/and is there a  pattern to the coefficients? Fix $t_1\leq t_2\leq t_3\leq\cdots \leq t_n$. For odd values of $n$ we have $\mathbb{E}[W(t_1)\ W(t_2) \cdots W(t_n)]=0$ while for even values of $n$ we get \begin{eqnarray*} \mathbb{E}[W (t_1)\ W(t_2)]&=& t_1 \cr \mathbb{E}[W (t_1)\ W(t_2)\ W(t_3)\ W(t_4)]&=& 2t_1 t_2+t_1t_3 \cr \mathbb{E}[W (t_1)\ W(t_2)\ W(t_3)\ W(t_4)\ W(t_5)\ W(t_6)]&=& 2t_1t_2t_5+t_1  t_3  t_5 +4 t_1  t_2  t_4 +2 t_1  t_3  t_4 +6 t_1  t_2  t_3  \end{eqnarray*} I suppose everything about Brownian motion has been worked out, but I can't find this in any of my books.  It's not very important, but I'm just curious!","My approach to this SE question uses the following joint moments of Brownian motion. For $n=1,2$ they are obvious and well-known, the others  are not terribly hard to work out. Is there a reference where these  formulas are given, or/and is there a  pattern to the coefficients? Fix $t_1\leq t_2\leq t_3\leq\cdots \leq t_n$. For odd values of $n$ we have $\mathbb{E}[W(t_1)\ W(t_2) \cdots W(t_n)]=0$ while for even values of $n$ we get \begin{eqnarray*} \mathbb{E}[W (t_1)\ W(t_2)]&=& t_1 \cr \mathbb{E}[W (t_1)\ W(t_2)\ W(t_3)\ W(t_4)]&=& 2t_1 t_2+t_1t_3 \cr \mathbb{E}[W (t_1)\ W(t_2)\ W(t_3)\ W(t_4)\ W(t_5)\ W(t_6)]&=& 2t_1t_2t_5+t_1  t_3  t_5 +4 t_1  t_2  t_4 +2 t_1  t_3  t_4 +6 t_1  t_2  t_3  \end{eqnarray*} I suppose everything about Brownian motion has been worked out, but I can't find this in any of my books.  It's not very important, but I'm just curious!",,"['probability', 'reference-request']"
90,What is the probability that $\pi(x) + x$ is injective?,What is the probability that  is injective?,\pi(x) + x,"Let $S$ be a finite group with operator + and $\pi$ be a permutation on $S$.  Then what is the probability that $\pi(x) + x$ is injective over choices of $\pi$? The concrete instantiation I'm interested in is $S=$GF$(2^n)$ for fixed $n > 0$.  (Computer Scientists call this ""xor on $n$-bit strings."") For $n=1$ we have two permutations, neither of which produce an injection. For $n=2$ we have 24 permutations, 8 of which induce an injection so the probability is 1/3. Here is an example $\pi$ for $n=2$: $$\pi(0)=0,\ \pi(1) = z,\ \pi(z)=z+1,\ \pi(z+1)=1.$$ Here notice that $\pi(x) + x$ produces $0$, $z+1$, $1$, and $z$, respectively, which is an injection.","Let $S$ be a finite group with operator + and $\pi$ be a permutation on $S$.  Then what is the probability that $\pi(x) + x$ is injective over choices of $\pi$? The concrete instantiation I'm interested in is $S=$GF$(2^n)$ for fixed $n > 0$.  (Computer Scientists call this ""xor on $n$-bit strings."") For $n=1$ we have two permutations, neither of which produce an injection. For $n=2$ we have 24 permutations, 8 of which induce an injection so the probability is 1/3. Here is an example $\pi$ for $n=2$: $$\pi(0)=0,\ \pi(1) = z,\ \pi(z)=z+1,\ \pi(z+1)=1.$$ Here notice that $\pi(x) + x$ produces $0$, $z+1$, $1$, and $z$, respectively, which is an injection.",,"['probability', 'discrete-mathematics', 'combinatorics']"
91,What is the probability of getting yahtzee?,What is the probability of getting yahtzee?,,"What is the probability of getting a yahtzee using $N$ dice with $X$ sides in $Y$ throws in a single round? Which side of the dice appears on the yahtzee with doesn't matter (i.e. it doesn't matter if I throw ones, or twos, etc.). I also assume perfect strategy is used; that is, after each throw, one saves the number at which the most dice landed (in case of a tie, one just picks at random).","What is the probability of getting a yahtzee using dice with sides in throws in a single round? Which side of the dice appears on the yahtzee with doesn't matter (i.e. it doesn't matter if I throw ones, or twos, etc.). I also assume perfect strategy is used; that is, after each throw, one saves the number at which the most dice landed (in case of a tie, one just picks at random).",N X Y,['probability']
92,What is the motivation for using cross-entropy to compare two probability vectors?,What is the motivation for using cross-entropy to compare two probability vectors?,,"Define a ""probability vector"" to be a vector $p = (p_1,\ldots, p_K) \in \mathbb R^K$ whose components are nonnegative and which satisfies $\sum_{k=1}^K p_k = 1$ . We can think of a probability vector as specifying a probability mass function (PMF) for a random variable with $K$ distinct possible values. A straightforward and intuitive way to compare two vectors $p$ and $q$ in $\mathbb R^K$ is to compute the quantity $$ d(p,q) = \frac12 \| p - q \|_2^2, $$ which is small when $p$ is close to $q$ . However, if $p$ and $q$ are probability vectors, I think it is somehow more natural to compare them using the ""cross-entropy loss function"" $\ell$ defined by $$ \ell(p,q) = -\sum_{k=1}^K q_k \log(p_k). $$ (This function is only defined when all components of $p$ are nonzero.) Question: What is the motivation for using the cross-entropy loss function when comparing probability vectors? Is there a viewpoint that makes it directly obvious that this is the ""correct"" thing to do? Some additional background information: This method of comparing probability vectors is fundamental in machine learning, because we have the following ""recipe"" for a classification algorithm which classifies objects into one of $K$ distinct classes. Suppose that we are given a list of training examples $x_i \in \mathbb R^n$ and corresponding one-hot encoded label vectors $y_i \in \mathbb R^K$ . (So if the $i$ th training example belongs to class $k$ , then the $k$ th component of the vector $y_i$ is $1$ and the other components are $0$ .) Let $S: \mathbb R^K \to \mathbb R^K$ be the softmax function defined by $$ S(u) = \begin{bmatrix} \frac{e^{u_1}}{\sum_k e^{u_k}} \\ \vdots \\ \frac{e^{u_K}}{\sum_k e^{u_k}} \end{bmatrix}. $$ The softmax function is useful because it converts a vector in $\mathbb R^K$ into a probability vector. To develop a classification algorithm, we attempt to find a function $f: \mathbb R^n \to \mathbb R^K$ such that for each training example $x_i$ the probability vector $p_i = S(f(x_i))$ is close to $y_i$ in the sense that $\ell(p_i, y_i)$ is small. For example, $f$ might be a neural network with a particular architecture, and the parameter vector $\theta$ which contains the weights of the neural network is chosen to minimize $$ \sum_{i = 1}^N \ell(p_i, y_i), $$ where $N$ is the number of training examples. (Multiclass logistic regression is the especially simple case where $f$ is assumed to be affine: $f(x_i) = A x_i + b$ .) One way to discover the cross-entropy loss function is to go through the steps of using maximum likelihood estimation to estimate the parameter vector $\theta$ which specifies $f$ (assuming that $f$ is restricted to be a member of a certain parameterized family of functions, such as affine functions or neural networks with a particular architecture). The cross-entropy loss function just pops out of the MLE procedure. This is the approach that currently seems the most clear to me. There is also an information theory viewpoint. Is there any simple way to recognize that the cross-entropy loss function is a ""natural"" way to compare probability vectors?","Define a ""probability vector"" to be a vector whose components are nonnegative and which satisfies . We can think of a probability vector as specifying a probability mass function (PMF) for a random variable with distinct possible values. A straightforward and intuitive way to compare two vectors and in is to compute the quantity which is small when is close to . However, if and are probability vectors, I think it is somehow more natural to compare them using the ""cross-entropy loss function"" defined by (This function is only defined when all components of are nonzero.) Question: What is the motivation for using the cross-entropy loss function when comparing probability vectors? Is there a viewpoint that makes it directly obvious that this is the ""correct"" thing to do? Some additional background information: This method of comparing probability vectors is fundamental in machine learning, because we have the following ""recipe"" for a classification algorithm which classifies objects into one of distinct classes. Suppose that we are given a list of training examples and corresponding one-hot encoded label vectors . (So if the th training example belongs to class , then the th component of the vector is and the other components are .) Let be the softmax function defined by The softmax function is useful because it converts a vector in into a probability vector. To develop a classification algorithm, we attempt to find a function such that for each training example the probability vector is close to in the sense that is small. For example, might be a neural network with a particular architecture, and the parameter vector which contains the weights of the neural network is chosen to minimize where is the number of training examples. (Multiclass logistic regression is the especially simple case where is assumed to be affine: .) One way to discover the cross-entropy loss function is to go through the steps of using maximum likelihood estimation to estimate the parameter vector which specifies (assuming that is restricted to be a member of a certain parameterized family of functions, such as affine functions or neural networks with a particular architecture). The cross-entropy loss function just pops out of the MLE procedure. This is the approach that currently seems the most clear to me. There is also an information theory viewpoint. Is there any simple way to recognize that the cross-entropy loss function is a ""natural"" way to compare probability vectors?","p = (p_1,\ldots, p_K) \in \mathbb R^K \sum_{k=1}^K p_k = 1 K p q \mathbb R^K 
d(p,q) = \frac12 \| p - q \|_2^2,
 p q p q \ell 
\ell(p,q) = -\sum_{k=1}^K q_k \log(p_k).
 p K x_i \in \mathbb R^n y_i \in \mathbb R^K i k k y_i 1 0 S: \mathbb R^K \to \mathbb R^K 
S(u) = \begin{bmatrix} \frac{e^{u_1}}{\sum_k e^{u_k}} \\ \vdots \\ \frac{e^{u_K}}{\sum_k e^{u_k}} \end{bmatrix}.
 \mathbb R^K f: \mathbb R^n \to \mathbb R^K x_i p_i = S(f(x_i)) y_i \ell(p_i, y_i) f \theta 
\sum_{i = 1}^N \ell(p_i, y_i),
 N f f(x_i) = A x_i + b \theta f f","['probability', 'statistical-inference', 'machine-learning']"
93,Distribution of weighted sum of Bernoulli RVs,Distribution of weighted sum of Bernoulli RVs,,"Let $x_1,...,x_m$ be drawn from independent Bernoulli distributions with parameters $p_1,...,p_m$. I'm interested in distribution of $t=\sum_i a_ix_i,~a_i\in \mathbb{R}$ $m$ is not large so I can not use central limit theorems. I have the following questions: 1- What is the distribution of $s=\sum_i x_i$? 2- What is the distribution of $t=\sum_i a_ix_i$ or $t=\sum_i a_ix_i-\sum_i a_i$ (to ensure non-negative support) for known $a_i$'s? can I approximate its distribution with a Gamma distribution? If yes, what would be the parameters (as a function of $p_i$'s and $a_i$'s)? 3- Is there a truncated Gamma distribution (or any other distribution (except normal)) that can approximately fits my problem? However, $m$ is not very large, but it is still very large such that I can not calculate the distribution by convolution. Thanks a bunch!","Let $x_1,...,x_m$ be drawn from independent Bernoulli distributions with parameters $p_1,...,p_m$. I'm interested in distribution of $t=\sum_i a_ix_i,~a_i\in \mathbb{R}$ $m$ is not large so I can not use central limit theorems. I have the following questions: 1- What is the distribution of $s=\sum_i x_i$? 2- What is the distribution of $t=\sum_i a_ix_i$ or $t=\sum_i a_ix_i-\sum_i a_i$ (to ensure non-negative support) for known $a_i$'s? can I approximate its distribution with a Gamma distribution? If yes, what would be the parameters (as a function of $p_i$'s and $a_i$'s)? 3- Is there a truncated Gamma distribution (or any other distribution (except normal)) that can approximately fits my problem? However, $m$ is not very large, but it is still very large such that I can not calculate the distribution by convolution. Thanks a bunch!",,"['probability', 'statistics', 'probability-distributions', 'binomial-distribution', 'gamma-distribution']"
94,Picking balls from urns.,Picking balls from urns.,,"We have 2 urns, 5 red balls and 5 green balls. We will split the 10 balls in the 2 urns in any way we like (provided that each urn has exactly 5 balls) and then will choose one urn at random, of which we will pick one ball.   If the ball is red, we will be asked to draw a second one from the other urn. If the first ball is green, we will keep it out of the urn and draw a second one from the same urn. We will win 100,000 rupees if we pick 2 balls of the same color. What is the optimum way to split the balls in the urns? (obviously the urns are identical, they are not transparent and we can’t cheat! Also, once we split the balls, the urns are taken away and brought back to us, so that we don't know which is which.) Obviously we can't put 5 red in one urn and 5 green in the other, because if we pick the urn with the red balls, we will never pick 2 of the same color. So both jars must contain both colors but I can't figure out the optimum combination. I was told it is something about a Bayes theorem, but I am not familiar with this (although I looked it up in wiki). Any help? Many thanks!","We have 2 urns, 5 red balls and 5 green balls. We will split the 10 balls in the 2 urns in any way we like (provided that each urn has exactly 5 balls) and then will choose one urn at random, of which we will pick one ball.   If the ball is red, we will be asked to draw a second one from the other urn. If the first ball is green, we will keep it out of the urn and draw a second one from the same urn. We will win 100,000 rupees if we pick 2 balls of the same color. What is the optimum way to split the balls in the urns? (obviously the urns are identical, they are not transparent and we can’t cheat! Also, once we split the balls, the urns are taken away and brought back to us, so that we don't know which is which.) Obviously we can't put 5 red in one urn and 5 green in the other, because if we pick the urn with the red balls, we will never pick 2 of the same color. So both jars must contain both colors but I can't figure out the optimum combination. I was told it is something about a Bayes theorem, but I am not familiar with this (although I looked it up in wiki). Any help? Many thanks!",,"['probability', 'combinatorics', 'discrete-mathematics']"
95,Why do many textbooks on Bayes' Theorem include the frequency of the disease in examples on the reliability of medical tests?,Why do many textbooks on Bayes' Theorem include the frequency of the disease in examples on the reliability of medical tests?,,"A ""standard"" example of Bayes Theorem goes something like the following: In any given year, 1% of the population will get disease X . A particular test will detect the disease in 90% of individuals who have the disease but has a 5% false positive rate. If you have a family history of X , your chances of getting the disease are 10% higher than they would have been otherwise. Virtually all explanations I've seen of Bayes' Theorem will include all of those facts in their formulation of the probability. It makes perfect sense to me to account for patient-specific factors like family history, and it also makes perfect sense to me to include information on the overall reliability of the test. I'm struggling to understand the relevance of the fact that 1% of the population will get disease X , though. In particular, that fact is presumably true for all patients who receive the test; that being the case, wouldn't Bayes' Theorem imply that the actual probability of a false positive is much higher than 5% (and that one of the numbers is therefore wrong)? Alternatively, why doesn't the 5% figure already account for that fact? Given that the 5% figure was presumably calculated directly from the data, wouldn't Bayes' Theorem effectively be contradicting the data in this case?","A ""standard"" example of Bayes Theorem goes something like the following: In any given year, 1% of the population will get disease X . A particular test will detect the disease in 90% of individuals who have the disease but has a 5% false positive rate. If you have a family history of X , your chances of getting the disease are 10% higher than they would have been otherwise. Virtually all explanations I've seen of Bayes' Theorem will include all of those facts in their formulation of the probability. It makes perfect sense to me to account for patient-specific factors like family history, and it also makes perfect sense to me to include information on the overall reliability of the test. I'm struggling to understand the relevance of the fact that 1% of the population will get disease X , though. In particular, that fact is presumably true for all patients who receive the test; that being the case, wouldn't Bayes' Theorem imply that the actual probability of a false positive is much higher than 5% (and that one of the numbers is therefore wrong)? Alternatively, why doesn't the 5% figure already account for that fact? Given that the 5% figure was presumably calculated directly from the data, wouldn't Bayes' Theorem effectively be contradicting the data in this case?",,"['probability', 'statistics', 'bayesian', 'conditional-probability', 'bayes-theorem']"
96,convolution of $n$ exponential distributions,convolution of  exponential distributions,n,"Let $exp(k)$ be the exponential distribution, $k>0$. Then it has density $$  f(x)= \begin{cases} ke^{-kx} & \text{ if } 0\leq x < \infty\\ 0  &\text{otherwise} \end{cases}   $$ I want to find the convolution of $n$ exponential distributions. For $n=2$ I have $$ \int_{\mathbb{R}} f(x-t)f(t) dt =\int_0^x (k e^{-k(x-t)}ke^{-kt}) dt=\int_0^x k^2e^{-kx} dt=k^2e^{-kx} \int_0^x dt= k^2xe^{-kx}. $$ For $n \geq 3$ I would like to take convolutions inductively, but I am not even sure what my inductive hypothesis would be. Some help?","Let $exp(k)$ be the exponential distribution, $k>0$. Then it has density $$  f(x)= \begin{cases} ke^{-kx} & \text{ if } 0\leq x < \infty\\ 0  &\text{otherwise} \end{cases}   $$ I want to find the convolution of $n$ exponential distributions. For $n=2$ I have $$ \int_{\mathbb{R}} f(x-t)f(t) dt =\int_0^x (k e^{-k(x-t)}ke^{-kt}) dt=\int_0^x k^2e^{-kx} dt=k^2e^{-kx} \int_0^x dt= k^2xe^{-kx}. $$ For $n \geq 3$ I would like to take convolutions inductively, but I am not even sure what my inductive hypothesis would be. Some help?",,"['probability', 'probability-theory', 'probability-distributions', 'convolution', 'density-function']"
97,Maximum entropy principle for Poisson distribution,Maximum entropy principle for Poisson distribution,,"I know that certain probability distributions may be derived from the requirement that entropy be maximal along with a constraint such as fixed variance. In the case of fixed variance, for example, one finds the normal distribution . In particular, the maximisation is over the set of all (!) continuous PDFs with that fixed variance. Now my question is, is there a similarly general derivation of the Poisson distribution as a maximum entropy distribution? E.g. fixing that mean and variance are equal and maximising entropy? I have found a couple of articles but they always seem to prove maximality on a restricted set of discrete PDFs. Is it because there is no more general maximum entropy principle for the Poisson distribution? If so, is it because the discrete case is simply more complex than the continuous one?","I know that certain probability distributions may be derived from the requirement that entropy be maximal along with a constraint such as fixed variance. In the case of fixed variance, for example, one finds the normal distribution . In particular, the maximisation is over the set of all (!) continuous PDFs with that fixed variance. Now my question is, is there a similarly general derivation of the Poisson distribution as a maximum entropy distribution? E.g. fixing that mean and variance are equal and maximising entropy? I have found a couple of articles but they always seem to prove maximality on a restricted set of discrete PDFs. Is it because there is no more general maximum entropy principle for the Poisson distribution? If so, is it because the discrete case is simply more complex than the continuous one?",,"['probability', 'entropy']"
98,Projection of Gaussian distribution along a vector.,Projection of Gaussian distribution along a vector.,,"Can anyone help me understand how to compute the projection of a 2D gaussian distribution along a vector. I intuitively realize that the projection will result in a 1D Gaussian, but I want to be sure. Can someone help me understand/show a proof/direct me to a proof where a 2D gaussian projected along a vector gives a line. Eg. Consider a Gaussian $\mathbf{X} \sim N (\mu,\Sigma)$ where $\mu = [3,2]^T$ and $\Sigma = \begin{bmatrix} 4 & 0 \\ 0 & 7 \end{bmatrix}$, what is the projection along the vector $v = 2i + 4j$ ? Any help would be much appreciated!! Thanks","Can anyone help me understand how to compute the projection of a 2D gaussian distribution along a vector. I intuitively realize that the projection will result in a 1D Gaussian, but I want to be sure. Can someone help me understand/show a proof/direct me to a proof where a 2D gaussian projected along a vector gives a line. Eg. Consider a Gaussian $\mathbf{X} \sim N (\mu,\Sigma)$ where $\mu = [3,2]^T$ and $\Sigma = \begin{bmatrix} 4 & 0 \\ 0 & 7 \end{bmatrix}$, what is the projection along the vector $v = 2i + 4j$ ? Any help would be much appreciated!! Thanks",,"['probability', 'normal-distribution']"
99,Characteristic Function and Random Variable Transformation,Characteristic Function and Random Variable Transformation,,"Let $X$ be a random variable, and let $\phi_X(t)$ be its characteristic function. Let $Y = f(X)$ be a transformation of the random variable $X$ where $f$ is increasing and one-to-one. Is there a direct functional relationship between the characteristic function of the transformation $f(X)$ and the characteristic function $\phi_X(x)$? Meaning if $\phi_Y(t)$ is the characteristic function of the random variable $f(X)$, can we write $\phi_Y(t)$ in terms of $\phi_X(t)$?","Let $X$ be a random variable, and let $\phi_X(t)$ be its characteristic function. Let $Y = f(X)$ be a transformation of the random variable $X$ where $f$ is increasing and one-to-one. Is there a direct functional relationship between the characteristic function of the transformation $f(X)$ and the characteristic function $\phi_X(x)$? Meaning if $\phi_Y(t)$ is the characteristic function of the random variable $f(X)$, can we write $\phi_Y(t)$ in terms of $\phi_X(t)$?",,"['probability', 'probability-theory', 'probability-distributions']"
