,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Applying mean value theorem to function of two variables,Applying mean value theorem to function of two variables,,"If I am given a function $f$ of two variables, where partial derivatives exist on all of $\mathbb{R}^2$ . Can I apply the mean value theorem on an interval $[x_1,x_2]$ to $f(x,y)$ by fixing a $y_0$ and then apply the mean value theorem for functions of one variable to choose a $c \in (x_1,x_2)$ with $f(x_2,y_0)-f(x_1,y_0)=f'(c)(x_2-x_1)$ ?","If I am given a function of two variables, where partial derivatives exist on all of . Can I apply the mean value theorem on an interval to by fixing a and then apply the mean value theorem for functions of one variable to choose a with ?","f \mathbb{R}^2 [x_1,x_2] f(x,y) y_0 c \in (x_1,x_2) f(x_2,y_0)-f(x_1,y_0)=f'(c)(x_2-x_1)",['multivariable-calculus']
1,"$\lim_{(x,y)\to(0,0)} \frac{xy}{x-y}$",,"\lim_{(x,y)\to(0,0)} \frac{xy}{x-y}","I was trying to calculate the following limit: $$\lim_{(x,y)\to(0,0)} \frac{xy}{x-y}$$ I used polar coordinates: $x=r\cos\theta$ and $y=r\sin\theta$ . But this gives me: $$\lim_{r\to 0} \frac{r\cos\theta\cdot r\sin\theta}{r\cos\theta-r\sin\theta}=\lim_{r \to 0} r\frac{\cos\theta\sin\theta}{\cos\theta-\sin\theta}=0$$ But Wolfram Alpha says that this limit does not exist. What is the problem in what I did? Is it because of the fact that if $\theta = \frac{\pi}{4}$ I am dividing by $0$ ? (My guess is that it is in fact the problem and by applying L'Hôpital's rule I will find out the limit does not exist.)",I was trying to calculate the following limit: I used polar coordinates: and . But this gives me: But Wolfram Alpha says that this limit does not exist. What is the problem in what I did? Is it because of the fact that if I am dividing by ? (My guess is that it is in fact the problem and by applying L'Hôpital's rule I will find out the limit does not exist.),"\lim_{(x,y)\to(0,0)} \frac{xy}{x-y} x=r\cos\theta y=r\sin\theta \lim_{r\to 0} \frac{r\cos\theta\cdot r\sin\theta}{r\cos\theta-r\sin\theta}=\lim_{r \to 0} r\frac{\cos\theta\sin\theta}{\cos\theta-\sin\theta}=0 \theta = \frac{\pi}{4} 0","['real-analysis', 'limits', 'multivariable-calculus', 'fake-proofs']"
2,Does there exist a differentiable function from $\mathbb R^3$ to $\mathbb R$ whose zeros are a line?,Does there exist a differentiable function from  to  whose zeros are a line?,\mathbb R^3 \mathbb R,"Suppose we are given a line in $\mathbb R^3$ .  Is there a differentiable function, $f$ , from $\mathbb R^3$ to $\mathbb R$ such that the solutions to $f(\vec{x})=0$ are precisely the points on the line? I would like to be able to describe a line using the zeros of a single function rather than using a parametrization. EDIT: I deleted a false claim about not being able to use a polynomial.","Suppose we are given a line in .  Is there a differentiable function, , from to such that the solutions to are precisely the points on the line? I would like to be able to describe a line using the zeros of a single function rather than using a parametrization. EDIT: I deleted a false claim about not being able to use a polynomial.",\mathbb R^3 f \mathbb R^3 \mathbb R f(\vec{x})=0,['geometry']
3,line integral of a gradient vector field,line integral of a gradient vector field,,"I'm having trouble finding the line integral of this problem. So they give me the gradient vector field $$\nabla f(x,y,z) = 2xyze^{x^2}i + ze^{x^2}j+ye^{x^2}k$$ they say that $f(0,0,0) = 5$ and i want to know what is the value of line integral $f(1,1,2)$ i know that the line integral of a gradient vector field is given by $$\int_{a}^{b} \nabla f(x,y,z)ds = f(c(b)) -f(c(a))$$ But i'm stuck by how to find $f$ , can someone explain to me","I'm having trouble finding the line integral of this problem. So they give me the gradient vector field they say that and i want to know what is the value of line integral i know that the line integral of a gradient vector field is given by But i'm stuck by how to find , can someone explain to me","\nabla f(x,y,z) = 2xyze^{x^2}i + ze^{x^2}j+ye^{x^2}k f(0,0,0) = 5 f(1,1,2) \int_{a}^{b} \nabla f(x,y,z)ds = f(c(b)) -f(c(a)) f","['calculus', 'multivariable-calculus', 'vector-analysis']"
4,"Evaluate the triple integral $\iiint\limits_E\frac{yz\,dx\,dy\,dz}{x^2+y^2+z^2}$ using spherical coordinates",Evaluate the triple integral  using spherical coordinates,"\iiint\limits_E\frac{yz\,dx\,dy\,dz}{x^2+y^2+z^2}","How to evaluate triple integral $$\iiint\limits_E\frac{yz\,dx\,dy\,dz}{x^2+y^2+z^2}$$ when $E$ is bounded by $x^2+y^2+z^2-x=0$ ? I know that spherical coordinates mean that $$x=r\sin\theta\cos\varphi,\quad y=r\sin\theta\sin\varphi,\quad z=r\cos\theta$$ and this function in spherical coordinates is \begin{align*} &\iiint\limits_E\frac{yzdxdydz}{x^2+y^2+z^2} = \iiint\limits_E\frac{r^2\sin\theta}{r^2\sin^2\theta\cos^2\varphi + r^2\sin^2\theta\sin^2\varphi + r^2\cos^2\theta}drd\theta d\varphi = \\ &\iiint\limits_E\frac{r^2\sin\theta}{r^2(\sin^2\theta\cos^2\varphi + \sin^2\theta\sin^2\varphi + \cos^2\theta)}drd\theta d\varphi = \iiint\limits_E\frac{\sin\theta}{\sin^2\theta\cos^2\varphi + \sin^2\theta\sin^2\varphi + \cos^2\theta}drd\theta d\varphi \end{align*} but I don't know how to write $E$ as set and convert it to spherical coordinates, and also what happens with this function after conversion. Triple integrals is now topic for me and I have never used spherical coordinates before, so I would be grateful if anyone can help me with this.","How to evaluate triple integral when is bounded by ? I know that spherical coordinates mean that and this function in spherical coordinates is but I don't know how to write as set and convert it to spherical coordinates, and also what happens with this function after conversion. Triple integrals is now topic for me and I have never used spherical coordinates before, so I would be grateful if anyone can help me with this.","\iiint\limits_E\frac{yz\,dx\,dy\,dz}{x^2+y^2+z^2} E x^2+y^2+z^2-x=0 x=r\sin\theta\cos\varphi,\quad y=r\sin\theta\sin\varphi,\quad z=r\cos\theta \begin{align*}
&\iiint\limits_E\frac{yzdxdydz}{x^2+y^2+z^2} = \iiint\limits_E\frac{r^2\sin\theta}{r^2\sin^2\theta\cos^2\varphi + r^2\sin^2\theta\sin^2\varphi + r^2\cos^2\theta}drd\theta d\varphi = \\ &\iiint\limits_E\frac{r^2\sin\theta}{r^2(\sin^2\theta\cos^2\varphi + \sin^2\theta\sin^2\varphi + \cos^2\theta)}drd\theta d\varphi = \iiint\limits_E\frac{\sin\theta}{\sin^2\theta\cos^2\varphi + \sin^2\theta\sin^2\varphi + \cos^2\theta}drd\theta d\varphi
\end{align*} E","['integration', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
5,"How can I show that $F(x,y)$ is not continuous?",How can I show that  is not continuous?,"F(x,y)","I want to show $F(x,y)=\frac{xy}{x^2+y^2}$ is not continuous at the origin. Here is my attempt: Choose a path $y=mx$ . Then, $F(x,mx)=\dfrac{xmx}{x^2+m^2x^2}=\dfrac{mx^2}{x^2(1+m^2)}=\dfrac{m^2}{1+m^2}$ if $(x,mx)\neq0$ . Thus, $\lim_\limits{(x,y)\to(0,0)}F(x,y)$ does not exist, so $F$ is not continuous.","I want to show is not continuous at the origin. Here is my attempt: Choose a path . Then, if . Thus, does not exist, so is not continuous.","F(x,y)=\frac{xy}{x^2+y^2} y=mx F(x,mx)=\dfrac{xmx}{x^2+m^2x^2}=\dfrac{mx^2}{x^2(1+m^2)}=\dfrac{m^2}{1+m^2} (x,mx)\neq0 \lim_\limits{(x,y)\to(0,0)}F(x,y) F","['calculus', 'multivariable-calculus', 'continuity']"
6,"Closed form $\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}}\sin(xyz)\,dx\,dy\,dz$",Closed form,"\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}}\sin(xyz)\,dx\,dy\,dz","This is part of an assignment from a multivariable calculus course. We've only just defined triple integrals and we're not using special functions or anything like that, so I'm pretty sure this was a mistake. Still, I'm interested to see if there exists some nice closed form. The only progress I've managed to get is $$\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}}\sin(xyz)\,dx\,dy\,dz=\int_{0}^{\frac{\pi}{2}}\dfrac{1}{z}\left(\ln\left(\dfrac{\pi^2}{4}z\right)-\text{Ci}\left(\dfrac{\pi^2}{4}z\right)+\gamma\right)dz\,,$$ where $\text{Ci}(x)$ is the cosine integral, but nothing else. Any ideas?","This is part of an assignment from a multivariable calculus course. We've only just defined triple integrals and we're not using special functions or anything like that, so I'm pretty sure this was a mistake. Still, I'm interested to see if there exists some nice closed form. The only progress I've managed to get is where is the cosine integral, but nothing else. Any ideas?","\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}}\sin(xyz)\,dx\,dy\,dz=\int_{0}^{\frac{\pi}{2}}\dfrac{1}{z}\left(\ln\left(\dfrac{\pi^2}{4}z\right)-\text{Ci}\left(\dfrac{\pi^2}{4}z\right)+\gamma\right)dz\,, \text{Ci}(x)","['multivariable-calculus', 'definite-integrals', 'closed-form', 'multiple-integral']"
7,Derivative of inverse of matrix wrt itself in Einstein notation,Derivative of inverse of matrix wrt itself in Einstein notation,,"I'm struggling to work with Einstein notation to derive $\frac{dA^{-1}}{dA}$ . I.e. I understand given $A^{-1}A=I$ I can differentiate both sides to  get $\frac{d}{dA}A^{-1}A=0$ and apply product rule then rearrange to get $\frac{dA^{-1}}{dA} = -A^{-2}$ . The problem is deriving this in Einstein notation.  My indices end up all over the place. I.e. $\frac{d}{dA_{ij}}(A^{-1}_{kl}A_{lm}) = (\frac{d}{dA_{ij}}A^{-1}_{kl})A_{lm} + A^{-1}_{kl}(\frac{d}{dA_{ij}}A_{lm})= (\frac{d}{dA_{ij}}A^{-1}_{kl})A_{lm} + A^{-1}_{kl}(\delta_{il}\delta_{jm})$ $\iff \frac{d}{dA_{ij}}A^{-1}_{kl} = -A^{-1}_{ki}\delta_{jm}A^{-1}_{lm} = -A^{-1}_{ki}A^{-1}_{lj}$ ... a fourth-order tensor?!  Obviously I'm doing something wrong, any hints much appreciated.","I'm struggling to work with Einstein notation to derive . I.e. I understand given I can differentiate both sides to  get and apply product rule then rearrange to get . The problem is deriving this in Einstein notation.  My indices end up all over the place. I.e. ... a fourth-order tensor?!  Obviously I'm doing something wrong, any hints much appreciated.",\frac{dA^{-1}}{dA} A^{-1}A=I \frac{d}{dA}A^{-1}A=0 \frac{dA^{-1}}{dA} = -A^{-2} \frac{d}{dA_{ij}}(A^{-1}_{kl}A_{lm}) = (\frac{d}{dA_{ij}}A^{-1}_{kl})A_{lm} + A^{-1}_{kl}(\frac{d}{dA_{ij}}A_{lm})= (\frac{d}{dA_{ij}}A^{-1}_{kl})A_{lm} + A^{-1}_{kl}(\delta_{il}\delta_{jm}) \iff \frac{d}{dA_{ij}}A^{-1}_{kl} = -A^{-1}_{ki}\delta_{jm}A^{-1}_{lm} = -A^{-1}_{ki}A^{-1}_{lj},"['calculus', 'matrices', 'multivariable-calculus', 'tensors', 'index-notation']"
8,Lots of doubts abot the surface area of a cylinder.,Lots of doubts abot the surface area of a cylinder.,,"I recently started to study parametric surfaces, and I come across this exercise that I try to solve but I have a lot of doubts reganding  the correctness of my resolution, and also I don't find similar examples on the internet. I need to find the surface area of the cylinder $$x^{2} + y^{2} = 4x$$ bounded by z=0 and z+ x =4. The cylinder is centered at (2,0) with radius 2. I made the parametrization $$<2+rcos(t) , rsen(t), 2-rcos(t)>$$ with r between 0 and 2, and t between 0 and 2π (First doubt : Is the parametrization right?) Then, if everything is ok, I would proceed to do the formula of a surface area (I would not write the whole formula because I am very bad at MathJax). But you know, the double integral of the norm of the vector (""u"") being ""u"" the cross product of the partial derivatives of the parametrization. The vector u in this case is (r,0,r) and the norm is $$\sqrt{2}r $$ Then, if everything is right, the area of the surface is the double integral $$\int_0^{2π}\int_0^2\sqrt{2}*r^2 dθdr  $$ Is this resolution right? If not, can you help me? Thanks. PS : I know that the cylinder bounded by the plane is half of the full cylinder. This is the main reason that I think this resolution is wrong.","I recently started to study parametric surfaces, and I come across this exercise that I try to solve but I have a lot of doubts reganding  the correctness of my resolution, and also I don't find similar examples on the internet. I need to find the surface area of the cylinder bounded by z=0 and z+ x =4. The cylinder is centered at (2,0) with radius 2. I made the parametrization with r between 0 and 2, and t between 0 and 2π (First doubt : Is the parametrization right?) Then, if everything is ok, I would proceed to do the formula of a surface area (I would not write the whole formula because I am very bad at MathJax). But you know, the double integral of the norm of the vector (""u"") being ""u"" the cross product of the partial derivatives of the parametrization. The vector u in this case is (r,0,r) and the norm is Then, if everything is right, the area of the surface is the double integral Is this resolution right? If not, can you help me? Thanks. PS : I know that the cylinder bounded by the plane is half of the full cylinder. This is the main reason that I think this resolution is wrong.","x^{2} + y^{2} = 4x <2+rcos(t) , rsen(t), 2-rcos(t)> \sqrt{2}r  \int_0^{2π}\int_0^2\sqrt{2}*r^2 dθdr  ",['multivariable-calculus']
9,"How to solve $\int_0^1dx\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}\,dy$",How to solve,"\int_0^1dx\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}\,dy","The original question is: Prove that: $$\begin{aligned}\\ \int_0^1dx\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}\,dy\neq\int_0^1dy&\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}\,dx\\ \end{aligned}\\$$ But I can't evaluate the integral $$\int_0^1dx\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}\,dy$$ At first, I assumed $x^2+y^2=z^2$ . But, it is so complicated. Then, I assumed $x=r\cos\theta$ and $y=r\sin\theta$ . But, I can't calculate the limits. Solving the equations I got three values of $\theta$ i.e. $\theta=0$ , $\theta=\frac{\pi}{4}$ and $\theta=\frac{\pi}{2}$ . I am just confused. Please help.","The original question is: Prove that: But I can't evaluate the integral At first, I assumed . But, it is so complicated. Then, I assumed and . But, I can't calculate the limits. Solving the equations I got three values of i.e. , and . I am just confused. Please help.","\begin{aligned}\\
\int_0^1dx\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}\,dy\neq\int_0^1dy&\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}\,dx\\
\end{aligned}\\ \int_0^1dx\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}\,dy x^2+y^2=z^2 x=r\cos\theta y=r\sin\theta \theta \theta=0 \theta=\frac{\pi}{4} \theta=\frac{\pi}{2}","['calculus', 'multivariable-calculus', 'definite-integrals']"
10,$\iint_{\mathbb{R}^2} \frac{1}{\sqrt{1+x^4+y^4}}$ converges or diverges?,converges or diverges?,\iint_{\mathbb{R}^2} \frac{1}{\sqrt{1+x^4+y^4}},$$\iint_{\mathbb{R}^2} \frac{1}{\sqrt{1+x^4+y^4}}$$ converges or diverges? I've tried to change to polar coordinates but i got stuck really quick $$\int_{0}^{2\pi}\int_{0}^{\infty}\frac{r}{\sqrt{1+r^4(1-2\sin^2(t)\cos^2(t))}}drdt$$ any hint please?,converges or diverges? I've tried to change to polar coordinates but i got stuck really quick any hint please?,\iint_{\mathbb{R}^2} \frac{1}{\sqrt{1+x^4+y^4}} \int_{0}^{2\pi}\int_{0}^{\infty}\frac{r}{\sqrt{1+r^4(1-2\sin^2(t)\cos^2(t))}}drdt,"['multivariable-calculus', 'convergence-divergence', 'polar-coordinates', 'multiple-integral', 'trigonometric-integrals']"
11,Equation of curves defined by following vector function in terms of variable $t$,Equation of curves defined by following vector function in terms of variable,t,"Write an equation for a surface which  contain the curves defined by following vector function $r(t)=\bigg<3t,e^{t},1-t^2\bigg>$ What i try: Campare with $r(t)=\bigg<x,y,z\bigg>$ We get $x=3t,y=e^{t},z=1-t^2$ $$9z=9-(3t)^2=9-x^2\Longrightarrow 9z=1-x^2$$ I did not understand what is the use of $y$ coordinate Here. Help me please . Thanks",Write an equation for a surface which  contain the curves defined by following vector function What i try: Campare with We get I did not understand what is the use of coordinate Here. Help me please . Thanks,"r(t)=\bigg<3t,e^{t},1-t^2\bigg> r(t)=\bigg<x,y,z\bigg> x=3t,y=e^{t},z=1-t^2 9z=9-(3t)^2=9-x^2\Longrightarrow 9z=1-x^2 y",['multivariable-calculus']
12,$f=g \implies \nabla f = \nabla g$?,?,f=g \implies \nabla f = \nabla g,"I want to disprove the following statement: $$f=g \text{ on } S \implies \nabla f = \nabla g \text{ on } S$$ where $S$ is some smooth closed surface and $f,g$ are smooth. I don't understand why this shouldn't be the case, assuming $S$ is more than a single point. In the simplest case, where $f,g:\mathbb{R}\to\mathbb{R}$ and S is some closed interval this is obviously true ( $f'=g'$ , right?). I've tried looking for a relatively simple counterexample (something like $f:\mathbb{R}^2\to\mathbb{R}$ ) i can plot on geogebra and really see whats going on geometrically, but don't seem to be getting anywhere.","I want to disprove the following statement: where is some smooth closed surface and are smooth. I don't understand why this shouldn't be the case, assuming is more than a single point. In the simplest case, where and S is some closed interval this is obviously true ( , right?). I've tried looking for a relatively simple counterexample (something like ) i can plot on geogebra and really see whats going on geometrically, but don't seem to be getting anywhere.","f=g \text{ on } S \implies \nabla f = \nabla g \text{ on } S S f,g S f,g:\mathbb{R}\to\mathbb{R} f'=g' f:\mathbb{R}^2\to\mathbb{R}","['multivariable-calculus', 'differential-geometry']"
13,Find the parametric equation of a line of intersection,Find the parametric equation of a line of intersection,,"Find the curve of intersection for the following surfaces: $z= x + \frac{y}{2} + \frac{1}{2}$ and $z^2= -x^2 + y$ I keep trying and trying to set them equal to each other and just end up with a mess. I wonder if it's possible to set them up with sin and cos, but I honestly have no idea. This is the first step to a problem I have, and then I have to calculate the length, which I can do, but I just can't find the parametric equations.","Find the curve of intersection for the following surfaces: and I keep trying and trying to set them equal to each other and just end up with a mess. I wonder if it's possible to set them up with sin and cos, but I honestly have no idea. This is the first step to a problem I have, and then I have to calculate the length, which I can do, but I just can't find the parametric equations.",z= x + \frac{y}{2} + \frac{1}{2} z^2= -x^2 + y,['multivariable-calculus']
14,Tricky multivariable limit [closed],Tricky multivariable limit [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Calculate the limit without using L'Hopital's rule: $\lim_{(x,y) \rightarrow (0,0)} \sqrt{(x^2+y^2)} ~\log|y|$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Calculate the limit without using L'Hopital's rule:","\lim_{(x,y) \rightarrow (0,0)} \sqrt{(x^2+y^2)} ~\log|y|","['limits', 'multivariable-calculus', 'limits-without-lhopital']"
15,"When classifying critical points of $f(x,y)$, what does a ""degenerate point"" (when $f_{xx}f_{yy}=f_{xy}^2$) mean, exactly?","When classifying critical points of , what does a ""degenerate point"" (when ) mean, exactly?","f(x,y) f_{xx}f_{yy}=f_{xy}^2","In my textbook, I have the following way of classifying critical points: \begin{array} {|c|c|c|c|}\hline f_x=f_y=0 & f_{xx} < 0 & W > 0 & \text{local maximum} \\ f_x=f_y=0 & f_{xx} > 0 & W > 0 & \text{local minimum} \\ f_x=f_y=0 & f_{xx} \text{ anything} & W < 0 & \text{saddle point} \\ f_x=f_y=0 & f_{xx} \text{ anything} & W = 0 & \text{degenerate} \\ \hline  \end{array} Notation: $f_x$ and $f_y$ are partial derivatives with respect to $x$ and $y$ . And $W=f_{xx}f_{yy}-f^2_{xy}$ is the determinant of the Jacobian of second derivatives. I understand  what minimum/maximum points are and what saddle points are. But what does a degenerate point mean, exactly?","In my textbook, I have the following way of classifying critical points: Notation: and are partial derivatives with respect to and . And is the determinant of the Jacobian of second derivatives. I understand  what minimum/maximum points are and what saddle points are. But what does a degenerate point mean, exactly?",\begin{array} {|c|c|c|c|}\hline f_x=f_y=0 & f_{xx} < 0 & W > 0 & \text{local maximum} \\ f_x=f_y=0 & f_{xx} > 0 & W > 0 & \text{local minimum} \\ f_x=f_y=0 & f_{xx} \text{ anything} & W < 0 & \text{saddle point} \\ f_x=f_y=0 & f_{xx} \text{ anything} & W = 0 & \text{degenerate} \\ \hline  \end{array} f_x f_y x y W=f_{xx}f_{yy}-f^2_{xy},"['calculus', 'multivariable-calculus']"
16,"Factorizing $ f(\lambda, R) = -6+4\lambda+6R-2R^2+2\lambda R^2-5\lambda R $ for $(\lambda, R)\in \mathbb{R}^2$",Factorizing  for," f(\lambda, R) = -6+4\lambda+6R-2R^2+2\lambda R^2-5\lambda R  (\lambda, R)\in \mathbb{R}^2","Consider the following bivariate polynomial $$ f(\lambda, R) = -6+4\lambda+6R-2R^2+2\lambda R^2-5\lambda R \, . $$ It can readily be seen that $f(2,1) = 0$ . But, would it be possible to write $f(\lambda, R)$ as a product of two or more multivariate polynomials? Thank you","Consider the following bivariate polynomial It can readily be seen that . But, would it be possible to write as a product of two or more multivariate polynomials? Thank you","
f(\lambda, R) = -6+4\lambda+6R-2R^2+2\lambda R^2-5\lambda R \, .
 f(2,1) = 0 f(\lambda, R)","['real-analysis', 'calculus', 'multivariable-calculus', 'polynomials', 'factoring']"
17,Find a $p $ such that $q \mapsto q^Tg + 2 p^T S q + q^T S q$ is the constant zero map.,Find a  such that  is the constant zero map.,p  q \mapsto q^Tg + 2 p^T S q + q^T S q,"I have the following problem: given the map $$p \mapsto p^T g + p^T S p$$ where the operation in the first term of the sum is the dot product, and in the second matrix multiplication, I would like to find its critical points. [The context for the interested is that $g$ and $S $ represents the gradient and the Hessian, respectively, of a twice differentiable map from $\mathbb R^n$ to $\mathbb R$ .] My approach is to differentiate the map w.r.t. $p $ . Then, assuming that $S $ is symmetric, I get that for each $p$ the derivative is the map $$q \mapsto q^Tg + 2 p^T S q + q^T S q$$ Now how can I find a $p $ such that this represents the constant zero function? Thanks in advance! EDIT Here is how I tried to calculate the derivative of the function at a given point $p $ To begin with, my definition of a derivative of a map $m$ from $\mathbb R^n$ to $\mathbb R$ at a point $p$ is a linear map $A$ from $\mathbb R^n $ to $\mathbb R$ such that $$\lim_{h \to 0 } \frac{|m(p+h) - m(p) - Ah|}{|h|}=0$$ My approach is to differentiate the two maps $p \mapsto p^Tg $ and $p  \mapsto p^TS p$ individually and then add those two functions.  And we differentiate them by finding a function that ""fits"" the definition given above. Since the map $p \mapsto p ^T g$ is linear it is immediate that $$\lim _{h \to 0 } \frac {|((p+h)^T g - p^T g - h^T g | } {|h | } =  \lim_{h \to 0}\frac{0}{|h|} $$ and thus that the map is its own derivative. For the second term I assume that $S$ is symmetric [a similar derivation would work otherwise]. Then since $$(p+h)^T S (p+h) = p^T S p + 2 p^T S h + h^T S h$$ we have simarily as above that $$\lim_{h \to 0 } \frac{|(p+h)^T S (p+h) - p^TS p - (2 p^T S h + h^T S  h)| }{|h|}= \lim_{h \to 0 } \frac{0}{|h|}=0$$ and so the mat $q \mapsto p^T S q + q^T S q$ would be a derivative at the point $p$ of the map $q \mapsto q^T S q$ . Then combining those two derivatives we get the map $$q \mapsto q^Tg + 2 p^T S q + q^T S q$$ is the derivative of $$q \mapsto q^T g + q^T S q$$ at the point $p$ . SECOND EDIT I found the error I made above. Since the map $$q \mapsto q^T S q $$ isn't linear we cannot have it in the derivative of the function $$q \mapsto q^T S q $$ as this would make the map I stated as the derivative at the point $p $ , namely $q \mapsto p^T S q + q^T S q$ , nonlinear. Insted we simply use that $\lim _{h \to 0 } h^T S h = 0$ (see here) and thus we have $$\lim_{h \to 0 } \frac{|(p+h)^T S (p+h) - p^TS p - 2 p^T S h | }{|h|}= \lim_{h \to 0 } \frac{|h^T S h|}{|h|}=0$$","I have the following problem: given the map where the operation in the first term of the sum is the dot product, and in the second matrix multiplication, I would like to find its critical points. [The context for the interested is that and represents the gradient and the Hessian, respectively, of a twice differentiable map from to .] My approach is to differentiate the map w.r.t. . Then, assuming that is symmetric, I get that for each the derivative is the map Now how can I find a such that this represents the constant zero function? Thanks in advance! EDIT Here is how I tried to calculate the derivative of the function at a given point To begin with, my definition of a derivative of a map from to at a point is a linear map from to such that My approach is to differentiate the two maps and individually and then add those two functions.  And we differentiate them by finding a function that ""fits"" the definition given above. Since the map is linear it is immediate that and thus that the map is its own derivative. For the second term I assume that is symmetric [a similar derivation would work otherwise]. Then since we have simarily as above that and so the mat would be a derivative at the point of the map . Then combining those two derivatives we get the map is the derivative of at the point . SECOND EDIT I found the error I made above. Since the map isn't linear we cannot have it in the derivative of the function as this would make the map I stated as the derivative at the point , namely , nonlinear. Insted we simply use that (see here) and thus we have","p \mapsto p^T g + p^T S p g S  \mathbb R^n \mathbb R p  S  p q \mapsto q^Tg + 2 p^T S q + q^T S q p  p  m \mathbb R^n \mathbb R p A \mathbb R^n  \mathbb R \lim_{h \to 0 } \frac{|m(p+h) - m(p) - Ah|}{|h|}=0 p \mapsto p^Tg  p
 \mapsto p^TS p p \mapsto p ^T g \lim _{h \to 0 } \frac {|((p+h)^T g - p^T g - h^T g | } {|h | } =
 \lim_{h \to 0}\frac{0}{|h|}  S (p+h)^T S (p+h) = p^T S p + 2 p^T S h + h^T S h \lim_{h \to 0 } \frac{|(p+h)^T S (p+h) - p^TS p - (2 p^T S h + h^T S
 h)| }{|h|}= \lim_{h \to 0 } \frac{0}{|h|}=0 q \mapsto p^T S q + q^T S q p q \mapsto q^T S q q \mapsto q^Tg + 2 p^T S q + q^T S q q \mapsto q^T g + q^T S q p q \mapsto q^T S q  q \mapsto q^T S q  p  q \mapsto p^T S q + q^T S q \lim _{h \to 0 } h^T S h = 0 \lim_{h \to 0 } \frac{|(p+h)^T S (p+h) - p^TS p - 2 p^T S h | }{|h|}= \lim_{h \to 0 } \frac{|h^T S h|}{|h|}=0","['multivariable-calculus', 'derivatives']"
18,"Does a symmetric expression in $x,y$ take optimum value only if $x=y$?",Does a symmetric expression in  take optimum value only if ?,"x,y x=y","Suppose $f(x,y)$ is a symmetric expression in $x,y$ .Suppose $f$ has a maximum or minimum.Then does it occur at a point where $x=y$ ?Can it happen that $f$ is optimum although $x\neq y$ ? I have used this result many times this thing but I want to know rigorously if the result is always true.Can someone help me?I think due to symmetry this should always happen.",Suppose is a symmetric expression in .Suppose has a maximum or minimum.Then does it occur at a point where ?Can it happen that is optimum although ? I have used this result many times this thing but I want to know rigorously if the result is always true.Can someone help me?I think due to symmetry this should always happen.,"f(x,y) x,y f x=y f x\neq y","['real-analysis', 'multivariable-calculus', 'functions', 'optimization']"
19,"$f:[0,1] \rightarrow \mathbb{R}$ is continuous. What is the value of $\int_{0}^{1} \int_{x}^{1-x} f(y) d y d x ?$ Use Fubini's theorem",is continuous. What is the value of  Use Fubini's theorem,"f:[0,1] \rightarrow \mathbb{R} \int_{0}^{1} \int_{x}^{1-x} f(y) d y d x ?","Suppose $f:[0,1] \rightarrow \mathbb{R}$ is continuous. What is the value of $\int_{0}^{1} \int_{x}^{1-x} f(y) d y d x ?$ Again, do not forget to justify any use of Fubini's Theorem. My attempt. I evaluated by the calculator that $\int_{0}^{1} \int_{x}^{1-x} f(y) d y d x=0.$ When we make the variable change $u=1-x, x=0, u=1, x=1, u=0, d u=-d x \int_{0}^{1} \int_{x}^{1-x} f(y) d y d x$ $=\int_{1}^{0} \int_{1-u}^{u} f(y) d y(-d u)=\int_{1}^{0} \int_{u}^{1-u} f(y) d y d u=-\int_{0}^{1} \int_{u}^{1-u} f(y) d y d u$ Then, I couln't continue, can you help? Thanks...","Suppose is continuous. What is the value of Again, do not forget to justify any use of Fubini's Theorem. My attempt. I evaluated by the calculator that When we make the variable change Then, I couln't continue, can you help? Thanks...","f:[0,1] \rightarrow \mathbb{R} \int_{0}^{1} \int_{x}^{1-x} f(y) d y d x ? \int_{0}^{1} \int_{x}^{1-x} f(y) d y d x=0. u=1-x, x=0, u=1, x=1, u=0, d u=-d x \int_{0}^{1} \int_{x}^{1-x} f(y) d y d x =\int_{1}^{0} \int_{1-u}^{u} f(y) d y(-d u)=\int_{1}^{0} \int_{u}^{1-u} f(y) d y d u=-\int_{0}^{1} \int_{u}^{1-u} f(y) d y d u","['real-analysis', 'calculus']"
20,prove smoothness of multivariable function,prove smoothness of multivariable function,,"I am trying to do Exercise 2.66 in Jeffrey Lee's book on differential manifolds. There is one step that escapes me, and I think it reduces to the following problem. Let $g:U\subset\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ be a continuous function, where $U$ is an open and connected neighbourhood of $\mathbb{R}^{n}$ . Assume that $$x\rightarrow\sum_{i=1}^{n}g_{i}(x)\frac{\partial{}f}{\partial{}x_{i}}(x)$$ is smooth (infinitely differentiable) for all smooth $f:U\subset\mathbb{R}^{n}\rightarrow\mathbb{R}$ . Show that $g$ is smooth.","I am trying to do Exercise 2.66 in Jeffrey Lee's book on differential manifolds. There is one step that escapes me, and I think it reduces to the following problem. Let be a continuous function, where is an open and connected neighbourhood of . Assume that is smooth (infinitely differentiable) for all smooth . Show that is smooth.",g:U\subset\mathbb{R}^{n}\rightarrow\mathbb{R}^{n} U \mathbb{R}^{n} x\rightarrow\sum_{i=1}^{n}g_{i}(x)\frac{\partial{}f}{\partial{}x_{i}}(x) f:U\subset\mathbb{R}^{n}\rightarrow\mathbb{R} g,"['multivariable-calculus', 'differential-geometry']"
21,"$f:[a,b]\subseteq \Bbb R \to \Bbb{R}^2$ s. t. $f(a) = f(b)$. Then $f'(t)$ takes any possible direction",s. t. . Then  takes any possible direction,"f:[a,b]\subseteq \Bbb R \to \Bbb{R}^2 f(a) = f(b) f'(t)","I'm asked to prove that for a function $f:[a,b]\subseteq \Bbb R \to \Bbb{R}^2$ such that $f(a)=f(b)$ is derivable for all $t$ on the interval then $f'(t)$ takes all possible directions. I am not sure my logic is correct, but I Was thinking about a function $f$ where $f(t)=v$ . Then we would still have $f(a)=f(b)$ but we would not have f' taking any possible direction.","I'm asked to prove that for a function such that is derivable for all on the interval then takes all possible directions. I am not sure my logic is correct, but I Was thinking about a function where . Then we would still have but we would not have f' taking any possible direction.","f:[a,b]\subseteq \Bbb R \to \Bbb{R}^2 f(a)=f(b) t f'(t) f f(t)=v f(a)=f(b)",['multivariable-calculus']
22,The flux of a vector field through a cylinder.,The flux of a vector field through a cylinder.,,"The question is by using Gauss’ Theorem calculate the flux of the   vector field $\overrightarrow{F} = x \hat{i} + y \hat{j}+ z \hat{k}$ through the surface of a cylinder of radius A and height H, which has   its axis along the z-axis and the base of the cylinder is on the   xy-plane. So, first of all I converted the vector field into cylindrical coordinates $\overrightarrow{F}= \rho \cos^2 \phi \hat{e}_\rho + \rho \sin^2 \phi \hat{e}_\rho + z \hat{e}_z $ which can be further reduced to- $\overrightarrow{F}= \rho \hat{e}_\rho + z \hat{e}_z$ The  surface of the cylinder has three parts, $ \ S_1 $ , $ \ S_2 $ , and $ \ S_3 $ . $ \ S_1 $ and $ \ S_2 $ are the top and bottom of surface of the cylinder and $ \ S_3 $ is the curved surface. We can write the surface integral over the surface of the cylinder as $\unicode{x222F}_S \overrightarrow{F} . d\overrightarrow{S}=\iint_{S_1} \overrightarrow{F} . d\overrightarrow{S_1} +\iint_{S_2} \overrightarrow{F} . d\overrightarrow{S_2} + \iint_{S_3} \overrightarrow{F} . d\overrightarrow{S_3}  $ As the area element is in $\rho \phi$ plane (for a constant value of z) has the value $\rho d \rho d \phi$ . So an area element on $ \ S_1 $ and $ \ S_2 $ will have magnitude $\rho d \rho d \phi$ , and the outward unit normals to $ \ S_1 $ and $ \ S_2 $ are then $ \hat{e}_z$ and $- \hat{e}_z$ , respectively $\therefore d\overrightarrow{S_1}= \rho d \rho d \phi \hat{e}_z$ and $d\overrightarrow{S_2}= -\rho d \rho d \phi \hat{e}_z$ And the area element for the $d\overrightarrow{S_3}= \rho dz d \phi \hat{e}_ \rho $ Now, keeping the conditions in mind- $0 \le \rho \le A$ ; $0 \le \phi \le 2 \pi$ ; $0 \le z \le H$ $\unicode{x222F}_S \overrightarrow{F} . d\overrightarrow{S}=\iint_{S_1} [\rho \hat{e}_\rho + z \hat{e}_z].[\rho d \rho d \phi \hat{e}_z]+ \iint_{S_2} [\rho \hat{e}_\rho + z \hat{e}_z].[-\rho d \rho d \phi \hat{e}_z]+ \iint_{S_3} [\rho \hat{e}_\rho + z \hat{e}_z].[\rho dz d \phi \hat{e}_ \rho]$ The flux of $d\overrightarrow{S_1}$ and $ d\overrightarrow{S_2}$ will cancel  out each other. Now, integrating $\iint_{S_3} \overrightarrow{F} . d\overrightarrow{S_3}  $ as double integral- $\int _{\phi =0}^{2\pi }\:\int _{z=0}^H\:\rho^2 dz d \phi$ $= 2 \pi A^2 H$ where $\rho = A$ So, the total flux is $= 2 \pi A^2 H$ which I think is wrong, as the flux should be the curved surface area of the cylinder,i.e., $= 2 \pi A H$ I am still learning this topic, so please mention any mistake that I've done while solving it","The question is by using Gauss’ Theorem calculate the flux of the   vector field through the surface of a cylinder of radius A and height H, which has   its axis along the z-axis and the base of the cylinder is on the   xy-plane. So, first of all I converted the vector field into cylindrical coordinates which can be further reduced to- The  surface of the cylinder has three parts, , , and . and are the top and bottom of surface of the cylinder and is the curved surface. We can write the surface integral over the surface of the cylinder as As the area element is in plane (for a constant value of z) has the value . So an area element on and will have magnitude , and the outward unit normals to and are then and , respectively and And the area element for the Now, keeping the conditions in mind- ; ; The flux of and will cancel  out each other. Now, integrating as double integral- where So, the total flux is which I think is wrong, as the flux should be the curved surface area of the cylinder,i.e., I am still learning this topic, so please mention any mistake that I've done while solving it",\overrightarrow{F} = x \hat{i} + y \hat{j}+ z \hat{k} \overrightarrow{F}= \rho \cos^2 \phi \hat{e}_\rho + \rho \sin^2 \phi \hat{e}_\rho + z \hat{e}_z  \overrightarrow{F}= \rho \hat{e}_\rho + z \hat{e}_z  \ S_1   \ S_2   \ S_3   \ S_1   \ S_2   \ S_3  \unicode{x222F}_S \overrightarrow{F} . d\overrightarrow{S}=\iint_{S_1} \overrightarrow{F} . d\overrightarrow{S_1} +\iint_{S_2} \overrightarrow{F} . d\overrightarrow{S_2} + \iint_{S_3} \overrightarrow{F} . d\overrightarrow{S_3}   \rho \phi \rho d \rho d \phi  \ S_1   \ S_2  \rho d \rho d \phi  \ S_1   \ S_2   \hat{e}_z - \hat{e}_z \therefore d\overrightarrow{S_1}= \rho d \rho d \phi \hat{e}_z d\overrightarrow{S_2}= -\rho d \rho d \phi \hat{e}_z d\overrightarrow{S_3}= \rho dz d \phi \hat{e}_ \rho  0 \le \rho \le A 0 \le \phi \le 2 \pi 0 \le z \le H \unicode{x222F}_S \overrightarrow{F} . d\overrightarrow{S}=\iint_{S_1} [\rho \hat{e}_\rho + z \hat{e}_z].[\rho d \rho d \phi \hat{e}_z]+ \iint_{S_2} [\rho \hat{e}_\rho + z \hat{e}_z].[-\rho d \rho d \phi \hat{e}_z]+ \iint_{S_3} [\rho \hat{e}_\rho + z \hat{e}_z].[\rho dz d \phi \hat{e}_ \rho] d\overrightarrow{S_1}  d\overrightarrow{S_2} \iint_{S_3} \overrightarrow{F} . d\overrightarrow{S_3}   \int _{\phi =0}^{2\pi }\:\int _{z=0}^H\:\rho^2 dz d \phi = 2 \pi A^2 H \rho = A = 2 \pi A^2 H = 2 \pi A H,"['integration', 'multivariable-calculus']"
23,Particle traveling on a Helix curve and work against gravity,Particle traveling on a Helix curve and work against gravity,,"A particle moves around Helix curve $x = a\cos t, y=b\sin t, z = ct$ when $0 \le t \le 2\pi $ How do I calculate the work what the particle does against gravity $\mathbf F(x,y,z)=-mg\,\mathbf k$ I know that the work is $-\int_C \mathbf F\cdot \mathbf {dr}$",A particle moves around Helix curve when How do I calculate the work what the particle does against gravity I know that the work is,"x = a\cos t, y=b\sin t, z = ct 0 \le t \le 2\pi  \mathbf F(x,y,z)=-mg\,\mathbf k -\int_C \mathbf F\cdot \mathbf {dr}","['calculus', 'integration', 'multivariable-calculus', 'elliptic-curves', 'multiple-integral']"
24,"Multiple Variable Limit.x, y tending to 1","Multiple Variable Limit.x, y tending to 1",,"I have a multi-variable limit issue that I have no idea how to do. $$ \lim_{(x,y)\to(1,1)} \frac{(x-1)^{4/3}-(y-1)^{4/3}}{(x-1)^{2/3}+(y-1)^{2/3}} $$",I have a multi-variable limit issue that I have no idea how to do.," \lim_{(x,y)\to(1,1)} \frac{(x-1)^{4/3}-(y-1)^{4/3}}{(x-1)^{2/3}+(y-1)^{2/3}} ","['calculus', 'limits', 'multivariable-calculus']"
25,Continuous choice of elements in the kernel of linear maps,Continuous choice of elements in the kernel of linear maps,,"Let $\Omega\subseteq\mathbb R^l$ be open and $A:\Omega\to \mathbb R^{n\times n}$ be a matrix valued $C^k$ -function, $k\in\mathbb N_0\cup\{\infty\} $ such that for all $p\in\Omega$ $A_p$ is not injective or equivalently $\det (A_p)=0$ . Then for all $p$ we can find $x_p\in  \mathbb R^n\setminus\{0\}$ with $A_p(x_p)=0$ . The question now is: Can the $x_p$ be choosen such that $p\mapsto x_p$ is also $C^k$ ? $\textbf{Edit:}$ As Aloizio Macedo's  answer shows this is not true in general. I am still interested in the case $\Omega=\mathbb R$ though since  this is what I  originally had in mind.","Let be open and be a matrix valued -function, such that for all is not injective or equivalently . Then for all we can find with . The question now is: Can the be choosen such that is also ? As Aloizio Macedo's  answer shows this is not true in general. I am still interested in the case though since  this is what I  originally had in mind.",\Omega\subseteq\mathbb R^l A:\Omega\to \mathbb R^{n\times n} C^k k\in\mathbb N_0\cup\{\infty\}  p\in\Omega A_p \det (A_p)=0 p x_p\in  \mathbb R^n\setminus\{0\} A_p(x_p)=0 x_p p\mapsto x_p C^k \textbf{Edit:} \Omega=\mathbb R,"['linear-algebra', 'analysis', 'multivariable-calculus', 'differential-geometry', 'continuity']"
26,"$ \lim_{(x,y) \rightarrow (0,0)} (x^2+y^2)( \sin( \frac{1}{x^2+y^2} ))$",," \lim_{(x,y) \rightarrow (0,0)} (x^2+y^2)( \sin( \frac{1}{x^2+y^2} ))","I was doing exercises on Howard Anton Calculus and I came across a problem which asks to find: $$ \lim_{(x,y) \rightarrow (0,0)} (x^2+y^2)\sin\left( \frac{1}{x^2+y^2} \right).$$ . Intuitively we know the answer: $0$ , but is there a step by step procedure that can be proposed as an argument ?","I was doing exercises on Howard Anton Calculus and I came across a problem which asks to find: . Intuitively we know the answer: , but is there a step by step procedure that can be proposed as an argument ?"," \lim_{(x,y) \rightarrow (0,0)} (x^2+y^2)\sin\left( \frac{1}{x^2+y^2} \right). 0","['limits', 'multivariable-calculus', 'polar-coordinates']"
27,Calculating the volume with double integral,Calculating the volume with double integral,,"Hello i am trying to calculate the volume for a double integral but i am having problem with define the integral because it is not given in a pure form. I have $z = xy$ , $x+y+z=1$ $z=0$ my approach is to set the function for a integral to be $$\int_Dxy$$ and to find the $limits for$ $dy$ i set $z$ to be zero it is also given by definition and i get $y = 1-x$ after that i set both $z$ and $y$ to zero and i get $x = 1$ so i have the following limits $$\int_0^1 \int_0^{x-1}xy$$ but i am not getting the right answer after evaluating the integral. What confusing me here is that the integral is not given by default here also the other thing that confuses me is i have the same problem but to be solved with triple integral. I am thinking maybe for the volume i just need $dydx$ without a function but i am not sure. Thank you for any help in advance.","Hello i am trying to calculate the volume for a double integral but i am having problem with define the integral because it is not given in a pure form. I have , my approach is to set the function for a integral to be and to find the i set to be zero it is also given by definition and i get after that i set both and to zero and i get so i have the following limits but i am not getting the right answer after evaluating the integral. What confusing me here is that the integral is not given by default here also the other thing that confuses me is i have the same problem but to be solved with triple integral. I am thinking maybe for the volume i just need without a function but i am not sure. Thank you for any help in advance.",z = xy x+y+z=1 z=0 \int_Dxy limits for dy z y = 1-x z y x = 1 \int_0^1 \int_0^{x-1}xy dydx,"['calculus', 'integration', 'multivariable-calculus', 'volume']"
28,"If $ y= x^{n-1}\log(x)$ , then prove that $D^n y $ is $ \frac{(N-1)!}{x}$","If  , then prove that  is", y= x^{n-1}\log(x) D^n y   \frac{(N-1)!}{x},"$ y= x^{n-1} \log(x)$ , then prove that $D^n y $ is $  \frac{(N-1)!}{x}$ ""> I don't understand this notation of "" $D^n$ "".I searched it on internet and didn't got any useful information ( What is the operator ""capital D"" and how can the chain rule be used in this way ) Please tell me what this notation stands for so that I can solve this question.","$ y= x^{n-1} \log(x)$ , then prove that is ""> I don't understand this notation of "" "".I searched it on internet and didn't got any useful information ( What is the operator ""capital D"" and how can the chain rule be used in this way ) Please tell me what this notation stands for so that I can solve this question.",D^n y    \frac{(N-1)!}{x} D^n,"['calculus', 'multivariable-calculus']"
29,Green theorem intuition,Green theorem intuition,,What I have a hard time understanding is the connection between line integrals of vector fields and Greens theorem. It was explained that taking lines integrals of parametrized curves is to be interpreted as the work done over the curve. However what Greens theorem provides is that we could write this in-terms of a double integral of the area. Bounded by a closed curve. What are we then calculating? Is it still the work over a closed curve?,What I have a hard time understanding is the connection between line integrals of vector fields and Greens theorem. It was explained that taking lines integrals of parametrized curves is to be interpreted as the work done over the curve. However what Greens theorem provides is that we could write this in-terms of a double integral of the area. Bounded by a closed curve. What are we then calculating? Is it still the work over a closed curve?,,"['multivariable-calculus', 'greens-theorem']"
30,"Partial derivative of $ f(f(x,-x) , f(x,x)) $.",Partial derivative of .," f(f(x,-x) , f(x,x)) ","$ f: \mathbb{R^2} \to \mathbb{R} $ is a function of $C^1$ . and $g: \mathbb{R} \to \mathbb{R}$ defined as : $$ g(x) = f(f(x,-x) , f(x,x))$$ Compute $g'(x)$ . Suppose $f(0,0) = 0$ . Determine $g'(0)$ . Prove that if $f(0,0) = 0$ and $(0,0)$ is not a critical point of $f$ then, $g$ is stricly increasing near the point $(0,0)$ . I don't know how to answer question $3$ , as I am not sure of my answers of question $1$ and $2$ . Here is my attempt: Using the chain rule, I got: $ \begin{align} g'(x) & = \frac{d}{dx} f(f(x,-x) , (x,x))\\ & =   \frac{df}{df}. \frac{df}{dx} (x,-x) + \frac{df}{df}. \frac{df}{dx} (x,x) \\ & = \frac{df}{dx} (x,-x) + \frac{df}{dx} (x,x) \end{align}$ $g'(0) = 2.\frac{df}{dx} (0,0)$","is a function of . and defined as : Compute . Suppose . Determine . Prove that if and is not a critical point of then, is stricly increasing near the point . I don't know how to answer question , as I am not sure of my answers of question and . Here is my attempt: Using the chain rule, I got:"," f: \mathbb{R^2} \to \mathbb{R}  C^1 g: \mathbb{R} \to \mathbb{R}  g(x) = f(f(x,-x) , f(x,x)) g'(x) f(0,0) = 0 g'(0) f(0,0) = 0 (0,0) f g (0,0) 3 1 2 
\begin{align} g'(x) & = \frac{d}{dx} f(f(x,-x) , (x,x))\\
& =   \frac{df}{df}. \frac{df}{dx} (x,-x) + \frac{df}{df}. \frac{df}{dx} (x,x) \\
& = \frac{df}{dx} (x,-x) + \frac{df}{dx} (x,x)
\end{align} g'(0) = 2.\frac{df}{dx} (0,0)","['real-analysis', 'multivariable-calculus', 'partial-derivative']"
31,Conservative field defined over a not simply-connected region,Conservative field defined over a not simply-connected region,,"I am wondering if a field can be conservative if the region where it is defined is not simply-connected. By definition $F$ is conservative if there exists a differentiable function which satisfies $F=grad(u)$ If I find such function which is defined in the not simply connected region, am I OK?","I am wondering if a field can be conservative if the region where it is defined is not simply-connected. By definition is conservative if there exists a differentiable function which satisfies If I find such function which is defined in the not simply connected region, am I OK?",F F=grad(u),"['calculus', 'multivariable-calculus', 'vector-fields']"
32,Liouville's theorem for harmonic functions,Liouville's theorem for harmonic functions,,"I was reading the proof of Liouville's theorem for harmonic functions (in $\mathbb{R}^n$ ) in Wikipedia, but I could not understand where do they use in that proof the assumption that $f$ is bounded. The proof - Taken from - https://en.m.wikipedia.org/wiki/Harmonic_function","I was reading the proof of Liouville's theorem for harmonic functions (in ) in Wikipedia, but I could not understand where do they use in that proof the assumption that is bounded. The proof - Taken from - https://en.m.wikipedia.org/wiki/Harmonic_function",\mathbb{R}^n f,"['calculus', 'integration', 'multivariable-calculus', 'harmonic-functions']"
33,Is $|\frac{y}{x^2}|\exp(- |\frac{y}{x^2}|)$ continuous?,Is  continuous?,|\frac{y}{x^2}|\exp(- |\frac{y}{x^2}|),"Let $q: \mathbb{R}^2 \to\mathbb{R}$ such that $$ q(x,y):=\begin{cases}\left|\frac{y}{x^2}\right|\exp\left(-\left|\frac{y}{x^2}\right|\right),&\text{ for }x\neq 0\\0,&\text{ for }x=0\end{cases} $$ Can I say that for all $x$ and $y$ and for all $\epsilon>0$ , there exists $\delta>0$ such that if $|q(x,y)−c|<δ$ , then $|q(x,y)−q(c)|<ϵ$ . So since $\exp$ is defined everywhere on $\Bbb R$ , let $c \in \Bbb R$ and let $\epsilon>0$ . But how do I find a corresponding $\delta>0$ ? Or is that already totally wrong?","Let such that Can I say that for all and and for all , there exists such that if , then . So since is defined everywhere on , let and let . But how do I find a corresponding ? Or is that already totally wrong?","q: \mathbb{R}^2 \to\mathbb{R} 
q(x,y):=\begin{cases}\left|\frac{y}{x^2}\right|\exp\left(-\left|\frac{y}{x^2}\right|\right),&\text{ for }x\neq 0\\0,&\text{ for }x=0\end{cases}
 x y \epsilon>0 \delta>0 |q(x,y)−c|<δ |q(x,y)−q(c)|<ϵ \exp \Bbb R c \in \Bbb R \epsilon>0 \delta>0","['multivariable-calculus', 'functions']"
34,Does integrating by parts in two variables works the same as one variable?,Does integrating by parts in two variables works the same as one variable?,,I will give a example that I tried to integrate by parts in $x$ : \begin{align} \int_0^1 \int_0^1 ye^x \frac{d^n}{dx^n} x^n\:\: dxdy \end{align} Using the rule of integration by parts: $f= ye^x \implies f^{(n)} = ye^x$ $g'= \frac{d^n}{dx^n}x^n \implies g = x^n$ I found the $n$ th derivative and integrated $n$ times. \begin{align} &\int_0^1 \int_0^1 ye^x \frac{d^n}{dx^n} x^n\:\: dxdy = \\ &fg\vert_0^1 - \int_0^1 \int_0^1 f'g \: \:dxdy=\\ & ye^x   x \vert_0^1 - \int_0^1 \int_0^1 ye^x x^n \:\:dxdy=\\ & y e  -  \int_0^1 \int_0^1 ye^x x^n \:\:dxdy\\ \end{align} The left side is a function of $y$ and the right side is a function of $n$ . Since $n$ a natural number we can evaluate the right side but why is the left side a function of $y$ ? Shouldn't be  the integral a function of $n$ ? Is this how we integrate by parts in two variables ?,I will give a example that I tried to integrate by parts in : Using the rule of integration by parts: I found the th derivative and integrated times. The left side is a function of and the right side is a function of . Since a natural number we can evaluate the right side but why is the left side a function of ? Shouldn't be  the integral a function of ? Is this how we integrate by parts in two variables ?,"x \begin{align}
\int_0^1 \int_0^1 ye^x \frac{d^n}{dx^n} x^n\:\: dxdy
\end{align} f= ye^x \implies f^{(n)} = ye^x g'= \frac{d^n}{dx^n}x^n \implies g = x^n n n \begin{align}
&\int_0^1 \int_0^1 ye^x \frac{d^n}{dx^n} x^n\:\: dxdy = \\
&fg\vert_0^1 - \int_0^1 \int_0^1 f'g \: \:dxdy=\\
& ye^x   x \vert_0^1 - \int_0^1 \int_0^1 ye^x x^n \:\:dxdy=\\
& y e  -  \int_0^1 \int_0^1 ye^x x^n \:\:dxdy\\
\end{align} y n n y n","['integration', 'multivariable-calculus']"
35,Notation - Functions in $\mathbb{R}^n$,Notation - Functions in,\mathbb{R}^n,"In an example I am given that following function $F: \mathbb{R}^N \rightarrow \mathbb{R}^N$ is given by \begin{equation} F(x_1, x_2, ... , x_N)= \bigg( \sum\limits_{i=1}^{N} x_i^2 \bigg)(x_1,x_2,...,x_N) \end{equation} I am a bit confused with the notation. If I expand the function is it given by the following: $F(x_1, x_2, ... , x_N)= x_1^2+x_2^2+...+x_N^2$ ? I am given that it is a function from $\mathbb{R}^N$ to $\mathbb{R}^N$ so I think that is wrong but don't understand how the term $(x_1,x_2,...,x_N)$ at the end works. Could it be that: \begin{equation} F(x_1, x_2, ... , x_N)= \begin{pmatrix}   x_1^2+x_2^2+...+x_N^2 \\   x_1^2+x_2^2+...+x_N^2 \\   \vdots \\   x_1^2+x_2^2+...+x_N^2 \\ \end{pmatrix} \end{equation}",In an example I am given that following function is given by I am a bit confused with the notation. If I expand the function is it given by the following: ? I am given that it is a function from to so I think that is wrong but don't understand how the term at the end works. Could it be that:,"F: \mathbb{R}^N \rightarrow \mathbb{R}^N \begin{equation}
F(x_1, x_2, ... , x_N)= \bigg( \sum\limits_{i=1}^{N} x_i^2 \bigg)(x_1,x_2,...,x_N)
\end{equation} F(x_1, x_2, ... , x_N)= x_1^2+x_2^2+...+x_N^2 \mathbb{R}^N \mathbb{R}^N (x_1,x_2,...,x_N) \begin{equation}
F(x_1, x_2, ... , x_N)=
\begin{pmatrix}
  x_1^2+x_2^2+...+x_N^2 \\
  x_1^2+x_2^2+...+x_N^2 \\
  \vdots \\
  x_1^2+x_2^2+...+x_N^2 \\
\end{pmatrix}
\end{equation}","['multivariable-calculus', 'notation']"
36,Linear function ortogonal at each point is a cross product,Linear function ortogonal at each point is a cross product,,"Let $f\colon \mathbb{R}^3 \to \mathbb{R}^3$ be a linear function satisfying $$\langle x, f(x) \rangle = 0$$ for all $x \in \mathbb{R}^3$ . I want to prove (and hopefully not disprove) that there exists a vector $F \in \mathbb{R}^3$ s.t. $$f(v) = F \times v$$ The problem is equivalent to $F = v\times f(v)$ being constant. I've tried showing it's derivative is zero but I haven't being lucky in this approach. I've always been mathematically skeptic as to why cross products appear anywhere at all in nature, and an affirmative answer to this question would quench my thirst for a mathematical explanation. It would be even more satisfying if it's a ""coordinate free"" proof, without going down to messy calculations with the partial derivatives of $f$ and so on..","Let be a linear function satisfying for all . I want to prove (and hopefully not disprove) that there exists a vector s.t. The problem is equivalent to being constant. I've tried showing it's derivative is zero but I haven't being lucky in this approach. I've always been mathematically skeptic as to why cross products appear anywhere at all in nature, and an affirmative answer to this question would quench my thirst for a mathematical explanation. It would be even more satisfying if it's a ""coordinate free"" proof, without going down to messy calculations with the partial derivatives of and so on..","f\colon \mathbb{R}^3 \to \mathbb{R}^3 \langle x, f(x) \rangle = 0 x \in \mathbb{R}^3 F \in \mathbb{R}^3 f(v) = F \times v F = v\times f(v) f","['calculus', 'linear-algebra', 'multivariable-calculus', 'derivatives']"
37,Calculating the value of the limit,Calculating the value of the limit,,"$$\lim_{(x,y)\to(1,2)}(\sin(y)-\sin(x))$$ My try: I got as $$\sin(2)-\sin(1)$$ But I cannot calculate the exact value of the given limit. Can anyone please explain this.",My try: I got as But I cannot calculate the exact value of the given limit. Can anyone please explain this.,"\lim_{(x,y)\to(1,2)}(\sin(y)-\sin(x)) \sin(2)-\sin(1)",['limits']
38,Prove that $(x−2y+z)^2 \geq 4xz−8y$,Prove that,(x−2y+z)^2 \geq 4xz−8y,"Let $x,y,z$ be nonnegative real numbers such that $x+z\leq2$ Prove that, and determine when equality holds. $(x−2y+z)^2 \geq 4xz−8y$ Please correct me if my methods are incorrect or would lead nowhere. I tried expanding the LHS of the inequality getting $x^2+4y^2+z^2-4xy-4yz+2xz \geq 4xz-8y$ And got lost as to how I should manipulate the inequality to find something true through rough work. After I tried manipulating $x+z\leq2$   subtract 2 $x+z-2\leq0$ since $y\ge 0$ $x+z-2\le y$ subtract 2y and add 2 to both sides $x-2y+z\le 2-y$ And again lost sight of how I could manipulate the inequalities.","Let $x,y,z$ be nonnegative real numbers such that $x+z\leq2$ Prove that, and determine when equality holds. $(x−2y+z)^2 \geq 4xz−8y$ Please correct me if my methods are incorrect or would lead nowhere. I tried expanding the LHS of the inequality getting $x^2+4y^2+z^2-4xy-4yz+2xz \geq 4xz-8y$ And got lost as to how I should manipulate the inequality to find something true through rough work. After I tried manipulating $x+z\leq2$   subtract 2 $x+z-2\leq0$ since $y\ge 0$ $x+z-2\le y$ subtract 2y and add 2 to both sides $x-2y+z\le 2-y$ And again lost sight of how I could manipulate the inequalities.",,"['multivariable-calculus', 'proof-verification', 'inequality', 'polynomials', 'sum-of-squares-method']"
39,"Let $R$ be the region bounded by $x+y=1, x=0, y=0$. Show $\iint \cos\frac{x-y}{x+y}\, dx\,dy=\frac{\sin1}{2}$ over the region $R.$",Let  be the region bounded by . Show  over the region,"R x+y=1, x=0, y=0 \iint \cos\frac{x-y}{x+y}\, dx\,dy=\frac{\sin1}{2} R.","Let $R$ be the region bounded by $x+y=1, x=0, y=0.$ Show  $$\iint_R \cos\frac{x-y}{x+y}\, dx\,dy=\frac{\sin1}{2}.$$ So I've let $u=x-y$, $x+y=v$. Graphically, the domain is a triangle with the slope as the $y=1-x$ line with vertices $(0,1)$ and $(1,0)$. It seems pretty obvious that the domain for $v$ is $0$ to $1$. For $u$, I've imagined a bunch of $y=x-u$ lines whereby $u$ is the variable. So in my mind as $u$ varies we get a stack of lines with gradient $1.$ The domain for $u$ will be when these lines intersect $(0,1)$ and $(1,0)$. Graphically, it seems like $u$ will vary from $1$ to $-1$. Calculating the inverse Jacobian: $$J=     \begin{vmatrix}     1 & -1\\     1 & 1     \end{vmatrix}=2 $$ $$\frac{1}{J}=\frac{1}{2}.$$ But when I start plugging in the substitution the problem starts: $$\iint_R \cos\frac{x-y}{x+y}\, dx\,dy$$ $$\frac{1}{2}\int^1_0\int^1_{-1}\cos\frac{u}{v}\,du\,dv$$ $$\frac{1}{2}\int^1_0 2v\sin\frac{1}{v}\,dv.$$ After this I'm stuck. Am I on the right track?","Let $R$ be the region bounded by $x+y=1, x=0, y=0.$ Show  $$\iint_R \cos\frac{x-y}{x+y}\, dx\,dy=\frac{\sin1}{2}.$$ So I've let $u=x-y$, $x+y=v$. Graphically, the domain is a triangle with the slope as the $y=1-x$ line with vertices $(0,1)$ and $(1,0)$. It seems pretty obvious that the domain for $v$ is $0$ to $1$. For $u$, I've imagined a bunch of $y=x-u$ lines whereby $u$ is the variable. So in my mind as $u$ varies we get a stack of lines with gradient $1.$ The domain for $u$ will be when these lines intersect $(0,1)$ and $(1,0)$. Graphically, it seems like $u$ will vary from $1$ to $-1$. Calculating the inverse Jacobian: $$J=     \begin{vmatrix}     1 & -1\\     1 & 1     \end{vmatrix}=2 $$ $$\frac{1}{J}=\frac{1}{2}.$$ But when I start plugging in the substitution the problem starts: $$\iint_R \cos\frac{x-y}{x+y}\, dx\,dy$$ $$\frac{1}{2}\int^1_0\int^1_{-1}\cos\frac{u}{v}\,du\,dv$$ $$\frac{1}{2}\int^1_0 2v\sin\frac{1}{v}\,dv.$$ After this I'm stuck. Am I on the right track?",,"['calculus', 'integration', 'multivariable-calculus']"
40,Show that a function is not integrable but an iterated integral exist,Show that a function is not integrable but an iterated integral exist,,"So, I have the funcion $f(x,y) =     \begin{cases}       0 & \text{if }x \text{ irrational} \\       2y & \text{if } x \text{ rational}\     \end{cases} $ Defined in $R=[0,1]\times[0,1]$ I know that $f$ is not Riemann-integrable since the value of the lower/upper Darboux' sums depends on the choice of the sample points. I just don't understand why should the iterated integral exist: $\int_{0}^{1}[\int_{0}^{1}f(x,y)dy]dx$ But it does, and I don't know what its value should be.","So, I have the funcion $f(x,y) =     \begin{cases}       0 & \text{if }x \text{ irrational} \\       2y & \text{if } x \text{ rational}\     \end{cases} $ Defined in $R=[0,1]\times[0,1]$ I know that $f$ is not Riemann-integrable since the value of the lower/upper Darboux' sums depends on the choice of the sample points. I just don't understand why should the iterated integral exist: $\int_{0}^{1}[\int_{0}^{1}f(x,y)dy]dx$ But it does, and I don't know what its value should be.",,"['real-analysis', 'integration', 'multivariable-calculus', 'riemann-sum']"
41,Is anisotropic diffusion equation a gradient flow of a generalized Dirichlet Energy?,Is anisotropic diffusion equation a gradient flow of a generalized Dirichlet Energy?,,I know that the gradient descent flow of the Dirichlet energy $$\min_u E(u) = \int_{\Omega}|\nabla u|^2 dA$$  Is the diffusion/heat equation: $$u_t = \Delta u$$ Is there a change in the Dirichlet energy such that it gives an anisotropic diffusion flow?,I know that the gradient descent flow of the Dirichlet energy $$\min_u E(u) = \int_{\Omega}|\nabla u|^2 dA$$  Is the diffusion/heat equation: $$u_t = \Delta u$$ Is there a change in the Dirichlet energy such that it gives an anisotropic diffusion flow?,,"['calculus', 'multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'vector-analysis']"
42,"Help with the notation $(x,t)\in \mathbb R^n \times (0,\infty)$",Help with the notation,"(x,t)\in \mathbb R^n \times (0,\infty)","What is the meaning of $$(x,t)\in \mathbb R^n \times (0,\infty)\quad ?\tag 1\label1$$ I guess $x$ is a $n$-vector and $t$ is just a scalar, i.e.  \begin{align} x&=(x_1, x_2, \dots, x_n)\in \mathbb R^n \tag 2\\ t&\in (0,\infty) \tag 3 \end{align} Attempt 1: Does \eqref{1} mean I have , i.e. \begin{align} (x_1, t),  (x_2,t), \dots, (x_n,t) \tag 4 \end{align} I.e. $n$ number of points in $\mathbb R^2$ (I guess?). Attempt 2: Or does \eqref{1} mean  \begin{align} (x_1, x_2, \dots, x_n,t) \tag 5 \end{align} I.e. just one point. But how many dimensions?","What is the meaning of $$(x,t)\in \mathbb R^n \times (0,\infty)\quad ?\tag 1\label1$$ I guess $x$ is a $n$-vector and $t$ is just a scalar, i.e.  \begin{align} x&=(x_1, x_2, \dots, x_n)\in \mathbb R^n \tag 2\\ t&\in (0,\infty) \tag 3 \end{align} Attempt 1: Does \eqref{1} mean I have , i.e. \begin{align} (x_1, t),  (x_2,t), \dots, (x_n,t) \tag 4 \end{align} I.e. $n$ number of points in $\mathbb R^2$ (I guess?). Attempt 2: Or does \eqref{1} mean  \begin{align} (x_1, x_2, \dots, x_n,t) \tag 5 \end{align} I.e. just one point. But how many dimensions?",,"['multivariable-calculus', 'elementary-set-theory', 'notation', 'vectors']"
43,What exactly is $\frac{\partial}{\partial x^i}\bigg|_p$?,What exactly is ?,\frac{\partial}{\partial x^i}\bigg|_p,"Let me give the reason I ask this question. We know that for a point $p = (x^1, \dots, x^n) \in \mathbb{R}^n$, the tangent space at $p$ denoted by $T_p(\mathbb{R}^n)$ has as basis $$\left\{\frac{\partial}{\partial x^1}\bigg|_p, \dots, \frac{\partial}{\partial x^n}\bigg|_p\right\}$$ where $$\frac{\partial}{\partial x^i}\bigg|_p \text{ is defined by } \left(\frac{\partial}{\partial x^i}\bigg|_p\right)(f) = \frac{\partial f}{\partial x^1}(p)$$ Now my question is what exactly are these: $$\frac{\partial}{\partial x^i}\bigg|_p$$  precisely? They are usually just called derivations , and not much further is explained in most books, but to me they seem like they are functions taking as inputs functions and returning real numbers. If so what is their domain, is it the set of all functions on $\mathbb{R}^n$? What I'm basically looking for is a way to make the construction of the basis for the tangent space of $\mathbb{R}^n$ at a point more rigorous, because at the moment it seems very symbolic based on the definition above.","Let me give the reason I ask this question. We know that for a point $p = (x^1, \dots, x^n) \in \mathbb{R}^n$, the tangent space at $p$ denoted by $T_p(\mathbb{R}^n)$ has as basis $$\left\{\frac{\partial}{\partial x^1}\bigg|_p, \dots, \frac{\partial}{\partial x^n}\bigg|_p\right\}$$ where $$\frac{\partial}{\partial x^i}\bigg|_p \text{ is defined by } \left(\frac{\partial}{\partial x^i}\bigg|_p\right)(f) = \frac{\partial f}{\partial x^1}(p)$$ Now my question is what exactly are these: $$\frac{\partial}{\partial x^i}\bigg|_p$$  precisely? They are usually just called derivations , and not much further is explained in most books, but to me they seem like they are functions taking as inputs functions and returning real numbers. If so what is their domain, is it the set of all functions on $\mathbb{R}^n$? What I'm basically looking for is a way to make the construction of the basis for the tangent space of $\mathbb{R}^n$ at a point more rigorous, because at the moment it seems very symbolic based on the definition above.",,"['calculus', 'multivariable-calculus', 'differential-geometry', 'differential-topology', 'smooth-manifolds']"
44,"Proving the derivative of the multivariable function $f(x,y)=\frac{x^2y}{x^2+y^2}$ exists.",Proving the derivative of the multivariable function  exists.,"f(x,y)=\frac{x^2y}{x^2+y^2}","Given is the function $f:\mathbb{R}^2\rightarrow\mathbb{R}$ with $f(0,0)=0$ and $f(x,y)=\dfrac{x^2y}{x^2+y^2}$ for $(x,y)\neq(0,0)$ . I understand the function is continuous at $(0,0)$ and its partial derivatives exist at $(0,0)$ . I need to prove the function is differentiable at $(0,0)$ . The problem is that I'm unsure of what ""derivative"" means in this context. Is it the total derivative? So far I thought of $$\frac{f(x,y)-f(0,0)}{\sqrt{x^2+y^2}}=\frac{\frac{x^2y}{x^2+y^2}}{\sqrt{x^2+y^ 2}}\leq\frac{x^2y}{(x^2+y^2)y}=\frac{x^2}{x^2+y^2}\leq\frac{x^2}{x^2}=1.$$ Therefore $\lim_{(x,y)\rightarrow(0,0)}\dfrac{f(x,y)-f(0,0)}{\sqrt{x^2+y^2}}\leq1$ . Is this enough to prove the derivative exists in $(0,0)$ ?","Given is the function with and for . I understand the function is continuous at and its partial derivatives exist at . I need to prove the function is differentiable at . The problem is that I'm unsure of what ""derivative"" means in this context. Is it the total derivative? So far I thought of Therefore . Is this enough to prove the derivative exists in ?","f:\mathbb{R}^2\rightarrow\mathbb{R} f(0,0)=0 f(x,y)=\dfrac{x^2y}{x^2+y^2} (x,y)\neq(0,0) (0,0) (0,0) (0,0) \frac{f(x,y)-f(0,0)}{\sqrt{x^2+y^2}}=\frac{\frac{x^2y}{x^2+y^2}}{\sqrt{x^2+y^ 2}}\leq\frac{x^2y}{(x^2+y^2)y}=\frac{x^2}{x^2+y^2}\leq\frac{x^2}{x^2}=1. \lim_{(x,y)\rightarrow(0,0)}\dfrac{f(x,y)-f(0,0)}{\sqrt{x^2+y^2}}\leq1 (0,0)","['multivariable-calculus', 'derivatives']"
45,"Double integration over a ""triangular"" semialgebraic set","Double integration over a ""triangular"" semialgebraic set",,"I must compute the double integral $$\iint_D x^6y^6 dx dy$$ where $$D = \left\{ (x,y) : x^{2}\le y\le x^{1/8} \right\}$$ Functions $x^2=x^{1/8}$ are going to be equal for $0$ and $1$. The region looks as follows. So, I have $$\int_0^1 \left[ \cfrac{x^6y^7}{7} \right]_{x^2}^{x^{1/8}}$$ But that gives me $$\left[ \cfrac{x^{55/8}-x^{20}}{7} \right]_{0}^{1} = 0$$ I am missing a big chunk of the theory. But, what?","I must compute the double integral $$\iint_D x^6y^6 dx dy$$ where $$D = \left\{ (x,y) : x^{2}\le y\le x^{1/8} \right\}$$ Functions $x^2=x^{1/8}$ are going to be equal for $0$ and $1$. The region looks as follows. So, I have $$\int_0^1 \left[ \cfrac{x^6y^7}{7} \right]_{x^2}^{x^{1/8}}$$ But that gives me $$\left[ \cfrac{x^{55/8}-x^{20}}{7} \right]_{0}^{1} = 0$$ I am missing a big chunk of the theory. But, what?",,"['calculus', 'multivariable-calculus', 'definite-integrals']"
46,Is any square norm a $\mathcal{C}^\infty$ function?,Is any square norm a  function?,\mathcal{C}^\infty,"Is it true that if $\|\cdot\|:\mathbb{R}^n \to \mathbb{R}$ is an arbitrary norm   then $\|\cdot\|^2$ is $\mathcal{C}^\infty$ function ? If the norm comes from an inner product $\langle\cdot,\cdot\rangle$, i.e. $$\|x\|^2 = \langle x , x \rangle; $$ Then it is easy to see that  $\| \cdot\|^2$ is a $\mathcal{C}^\infty$ function, because, if $\{v_1,...,v_n\}$ is an orthonomal basis with respect to the inner product $\langle\cdot, \cdot\rangle$, and $A$ $\in$ $M_n(\mathbb{R})$ is the change basis matrix of $\{e_1,...,e_n\}$ to $\{v_1, ..., v_n\}$ then $$\|x\|^2=\langle x, x\rangle = (A\cdot x)^{\text{T}}G (A\cdot x)$$ where $G$ is a matrix such that $[G]_{ij} = [\langle v_i, v_j\rangle]$. Using the above formula is easy to conclude that $\|\cdot \|^2$ is a $\mathcal{C}^\infty$ function. But when $\|\cdot \|$ is an arbitrary norm I just was able to conclude that $\| \cdot \|^2$ is a continuous function. Can anyone help me?","Is it true that if $\|\cdot\|:\mathbb{R}^n \to \mathbb{R}$ is an arbitrary norm   then $\|\cdot\|^2$ is $\mathcal{C}^\infty$ function ? If the norm comes from an inner product $\langle\cdot,\cdot\rangle$, i.e. $$\|x\|^2 = \langle x , x \rangle; $$ Then it is easy to see that  $\| \cdot\|^2$ is a $\mathcal{C}^\infty$ function, because, if $\{v_1,...,v_n\}$ is an orthonomal basis with respect to the inner product $\langle\cdot, \cdot\rangle$, and $A$ $\in$ $M_n(\mathbb{R})$ is the change basis matrix of $\{e_1,...,e_n\}$ to $\{v_1, ..., v_n\}$ then $$\|x\|^2=\langle x, x\rangle = (A\cdot x)^{\text{T}}G (A\cdot x)$$ where $G$ is a matrix such that $[G]_{ij} = [\langle v_i, v_j\rangle]$. Using the above formula is easy to conclude that $\|\cdot \|^2$ is a $\mathcal{C}^\infty$ function. But when $\|\cdot \|$ is an arbitrary norm I just was able to conclude that $\| \cdot \|^2$ is a continuous function. Can anyone help me?",,"['analysis', 'multivariable-calculus', 'normed-spaces']"
47,"Limit of $\lim_{(x,y)\rightarrow (0,0)}\ xy\log(\lvert x\rvert+\lvert y\rvert)$",Limit of,"\lim_{(x,y)\rightarrow (0,0)}\ xy\log(\lvert x\rvert+\lvert y\rvert)","I need help to understand how we compute this kind of limit: $\lim_{(x,y)\rightarrow (0,0)}\ xy\log(\lvert x\rvert+\lvert y\rvert)$ I think we can use the squeeze theorem but I don't know how to bound the function, so I can use the theorem. If I suppose $0 \lt \sqrt{x^2+y^2} \lt 1$ then, but I'm struggle.. $ 0 \le \lvert f(x,y)\rvert = \lvert xy\log(\lvert x\rvert+\lvert y\rvert)\rvert$ Thanks in advance for the help.","I need help to understand how we compute this kind of limit: $\lim_{(x,y)\rightarrow (0,0)}\ xy\log(\lvert x\rvert+\lvert y\rvert)$ I think we can use the squeeze theorem but I don't know how to bound the function, so I can use the theorem. If I suppose $0 \lt \sqrt{x^2+y^2} \lt 1$ then, but I'm struggle.. $ 0 \le \lvert f(x,y)\rvert = \lvert xy\log(\lvert x\rvert+\lvert y\rvert)\rvert$ Thanks in advance for the help.",,"['calculus', 'limits', 'multivariable-calculus']"
48,"Does the following definition of ""approach independent"" derivative of a vector valued function $f$ implies the continuity of $f$?","Does the following definition of ""approach independent"" derivative of a vector valued function  implies the continuity of ?",f f,"Before going to my question, let me give two prelimilary definitions Definition 1. Let $S\subseteq\mathbb{R}^n$ be a non=empty open set in $\mathbb{R}^n$ under the usual topology on $\mathbb{R}^n$ and $f:S\to \mathbb{R}^m$. Let $\mathbf{c}\in S$ and $g:U(\subseteq \mathbb{R})\to S$ is such that, $U$ is open in $\mathbb{R}$ under the usual topology on $\mathbb{R}$ $g(0)=\mathbf{c}$ $g$ is continuous at $0$ Then $f$ will be said to have derivative along the cruve $g$ at the point $\mathbf{c}$ if, $$\displaystyle\lim_{h\to 0}\dfrac{(f\circ g)(h)-(f\circ g)(0)}{h}$$ exists. Definition 2. Let $S\subseteq\mathbb{R}^n$ be a non=empty open set in $\mathbb{R}^n$ under the usual topology on $\mathbb{R}^n$ and $f:S\to \mathbb{R}^m$. Let $\mathbf{c}\in S$. Then $f$ will be said to have approach independent derivative at $\mathbf{c}$ if, $$\displaystyle\lim_{h\to 0}\dfrac{(f\circ g)(h)-(f\circ g)(0)}{h}$$ exists for all $g$ satisfying the properties listed in the previous definition. Question If $f$ has approach independent derivative at $\bf{c}$ then is it continuous at $\mathbf{c}$? I was trying to find a counter example of such a function $f$ but till now I have not been able to find such an example. Any help will be appreciated.","Before going to my question, let me give two prelimilary definitions Definition 1. Let $S\subseteq\mathbb{R}^n$ be a non=empty open set in $\mathbb{R}^n$ under the usual topology on $\mathbb{R}^n$ and $f:S\to \mathbb{R}^m$. Let $\mathbf{c}\in S$ and $g:U(\subseteq \mathbb{R})\to S$ is such that, $U$ is open in $\mathbb{R}$ under the usual topology on $\mathbb{R}$ $g(0)=\mathbf{c}$ $g$ is continuous at $0$ Then $f$ will be said to have derivative along the cruve $g$ at the point $\mathbf{c}$ if, $$\displaystyle\lim_{h\to 0}\dfrac{(f\circ g)(h)-(f\circ g)(0)}{h}$$ exists. Definition 2. Let $S\subseteq\mathbb{R}^n$ be a non=empty open set in $\mathbb{R}^n$ under the usual topology on $\mathbb{R}^n$ and $f:S\to \mathbb{R}^m$. Let $\mathbf{c}\in S$. Then $f$ will be said to have approach independent derivative at $\mathbf{c}$ if, $$\displaystyle\lim_{h\to 0}\dfrac{(f\circ g)(h)-(f\circ g)(0)}{h}$$ exists for all $g$ satisfying the properties listed in the previous definition. Question If $f$ has approach independent derivative at $\bf{c}$ then is it continuous at $\mathbf{c}$? I was trying to find a counter example of such a function $f$ but till now I have not been able to find such an example. Any help will be appreciated.",,"['real-analysis', 'multivariable-calculus']"
49,"Is f(x,y) continuous at (0,0) $ f(x,y) = (x+y)^2\cos\left(\frac{1}{\sqrt{x^2+y^2}}\right)$","Is f(x,y) continuous at (0,0)"," f(x,y) = (x+y)^2\cos\left(\frac{1}{\sqrt{x^2+y^2}}\right)","Show if the following function of two variables has a limit in (0,0). $$ f(x,y) = (x+y)^2\cos\left(\frac{1}{\sqrt{x^2+y^2}}\right);(x,y)\ne(0,0);f(0,0)=0 $$ I tried to find a limit of the function by substituting y = x: $$ f(x,x) = 4x^2\cos(\frac{1}{x\sqrt{2}}) $$ Then easily by squeeze theorem: $$ \lim_{(x,y)\to(0,0)}f(x,x) = 0 $$ On the other hand, after a little fiddling with it, I've come to this: $$ f(x,y) = \frac{\cos\left(\frac{1}{\sqrt{x^2+y^2}}\right)}{(\frac{1}{\sqrt{x^2+y^2}})^2} + \cos\left(\frac{1}{x^2+y^2}\right)2xy $$ EDIT: Here I thought the left side should go to $\infty$, which is obviously wrong. When f(x,y) approaches (0,0), $\frac{1}{f(x,y)}$ approaches infinity. Thus everything is in order. $$ \lim_{(x,y)\to(0,0)} f(x,y) = 0 $$ EDIT: Also, a very useful and stunningly easy solution (from answer below) is using squeeze theorem with cos. $$ -(x+y)^2 \le (x+y)^2\cos(g(x)) \le (x+y)^2 $$ From that it's easily seen that function is continuous.","Show if the following function of two variables has a limit in (0,0). $$ f(x,y) = (x+y)^2\cos\left(\frac{1}{\sqrt{x^2+y^2}}\right);(x,y)\ne(0,0);f(0,0)=0 $$ I tried to find a limit of the function by substituting y = x: $$ f(x,x) = 4x^2\cos(\frac{1}{x\sqrt{2}}) $$ Then easily by squeeze theorem: $$ \lim_{(x,y)\to(0,0)}f(x,x) = 0 $$ On the other hand, after a little fiddling with it, I've come to this: $$ f(x,y) = \frac{\cos\left(\frac{1}{\sqrt{x^2+y^2}}\right)}{(\frac{1}{\sqrt{x^2+y^2}})^2} + \cos\left(\frac{1}{x^2+y^2}\right)2xy $$ EDIT: Here I thought the left side should go to $\infty$, which is obviously wrong. When f(x,y) approaches (0,0), $\frac{1}{f(x,y)}$ approaches infinity. Thus everything is in order. $$ \lim_{(x,y)\to(0,0)} f(x,y) = 0 $$ EDIT: Also, a very useful and stunningly easy solution (from answer below) is using squeeze theorem with cos. $$ -(x+y)^2 \le (x+y)^2\cos(g(x)) \le (x+y)^2 $$ From that it's easily seen that function is continuous.",,"['limits', 'multivariable-calculus', 'continuity']"
50,Extrema of implicit functions - one point - two values?!,Extrema of implicit functions - one point - two values?!,,"I am given this expression: $$x^2+y^2+z^2-2x-2y-2z+2 = 0$$ And I need to find the extrema of the function  $$z = z(x,y)$$ I did it in the following way: Differentiate the expression with respect to $x$ and solve for $\frac{dz}{dx}$ Differentiate with respect to $y$ and solve for $\frac{dz}{dy}$ Find all tuples $(x, y ,z)$ such that $\frac{dz}{dx} = \frac{dz}{dy} = 0$ given that both partial derivatives are continuous and exist. Now, I got two points: $(1, 1, 0) $ and $(1, 1, 2)$ both of them turn out to be an extremum, but - to be honest - I do not know what is happening - why does this function have two outputs for one input? Could you explain this to me in as simple terms as possible?","I am given this expression: $$x^2+y^2+z^2-2x-2y-2z+2 = 0$$ And I need to find the extrema of the function  $$z = z(x,y)$$ I did it in the following way: Differentiate the expression with respect to $x$ and solve for $\frac{dz}{dx}$ Differentiate with respect to $y$ and solve for $\frac{dz}{dy}$ Find all tuples $(x, y ,z)$ such that $\frac{dz}{dx} = \frac{dz}{dy} = 0$ given that both partial derivatives are continuous and exist. Now, I got two points: $(1, 1, 0) $ and $(1, 1, 2)$ both of them turn out to be an extremum, but - to be honest - I do not know what is happening - why does this function have two outputs for one input? Could you explain this to me in as simple terms as possible?",,"['calculus', 'multivariable-calculus']"
51,What kind of object is it in $\mathbb{R}^4$,What kind of object is it in,\mathbb{R}^4,"Given an object in $\mathbb{R}^4$ : $\{(x_1,x_2,x_3, x_4)\in\mathbb{R}^4:x_1^2+x_2^2 =\frac{1}{2} , x_3^2+x_4^2 =\frac{1}{2}\}$ What kind of object could it be? How could I determine its parametric equation $(x_1,x_2,x_3, x_4)=(f_1(t),f_2(t),f_3(t),f_4(t))$?","Given an object in $\mathbb{R}^4$ : $\{(x_1,x_2,x_3, x_4)\in\mathbb{R}^4:x_1^2+x_2^2 =\frac{1}{2} , x_3^2+x_4^2 =\frac{1}{2}\}$ What kind of object could it be? How could I determine its parametric equation $(x_1,x_2,x_3, x_4)=(f_1(t),f_2(t),f_3(t),f_4(t))$?",,"['geometry', 'multivariable-calculus', 'differential-geometry']"
52,How to find the gradient of $f(x)=-\sum_{i=1}^n \log x_i$?,How to find the gradient of ?,f(x)=-\sum_{i=1}^n \log x_i,"I am trying to find the gradient $\nabla f(x)\,\,$ of $\,f: \mathbb{R}^n\rightarrow\mathbb{R}$ of $$f(x)=-\sum_{i=1}^n \log x_i,$$ but I am getting stuck when it comes time to deal with the log. I know there are several options for taking the derivative: delta method, which involves basically applying the extreme value theorem.  Add a small perturbation, and then isolating the parts of the function which involve an inner product of the perturbation with a function of the variable $$ f(x+h) = f(x) + \langle \nabla f, h \rangle + o(\|h\|) $$ where $o(\|h\|)$ is a function such that the limit as $h \to 0$ is zero (comes straight from the definition of differentiability); vector calculus using chain rule. Since the equation $f(x)$ is not expressed in a vector here, I figured it would be easier to do 1) via perturbation, so here's what I tried: I try to add $h$ which is my perturbation to the vector $x$ : $$f(x+h)= - \sum_{i=1}^n \log (x_i+h_i)$$ But now here, there are not many options for taking the logarithm of the sum of two numbers.  So, I'm wondering if maybe I can split this out into $\log (x_i) + \log(h_i)$ and just call my perturbation $log(h_i)$ instead of $h_i$ ? Are there any other hints to finding the gradient of this function $f$ ?","I am trying to find the gradient of of but I am getting stuck when it comes time to deal with the log. I know there are several options for taking the derivative: delta method, which involves basically applying the extreme value theorem.  Add a small perturbation, and then isolating the parts of the function which involve an inner product of the perturbation with a function of the variable where is a function such that the limit as is zero (comes straight from the definition of differentiability); vector calculus using chain rule. Since the equation is not expressed in a vector here, I figured it would be easier to do 1) via perturbation, so here's what I tried: I try to add which is my perturbation to the vector : But now here, there are not many options for taking the logarithm of the sum of two numbers.  So, I'm wondering if maybe I can split this out into and just call my perturbation instead of ? Are there any other hints to finding the gradient of this function ?","\nabla f(x)\,\, \,f: \mathbb{R}^n\rightarrow\mathbb{R} f(x)=-\sum_{i=1}^n \log x_i,  f(x+h) = f(x) + \langle \nabla f, h \rangle + o(\|h\|)  o(\|h\|) h \to 0 f(x) h x f(x+h)= - \sum_{i=1}^n \log (x_i+h_i) \log (x_i) + \log(h_i) log(h_i) h_i f","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'scalar-fields']"
53,Linear program with $10$ variables,Linear program with  variables,10,"$$\begin{array}{ll} \text{maximize} & \frac{a}{100} + \frac{4b}{100} + \frac{9c}{100} + \frac{16d}{100} + \frac{25e}{100} + \frac{36f}{100} + \frac{49g}{100} + \frac{64h}{100} + \frac{81i}{100} + \frac{100j}{100}\\ \text{subject to} & a+b+c+d+e+f+g+h+i+j = 100\end{array}$$ When I use the Lagrange multiplier and I take all my partial derivatives, I do not get any of the variables in terms of $\lambda$. For example, when I take the derivative with respect to $a$, I get $$\frac{1}{100} = \lambda$$ and with respect to $b$, I get $$\frac{4}{100} = \lambda$$ Is there any way I can find the values of $a$, $b$, $c$, $d$, $e$, $f$, $g$, $h$, $i$ and $j$ which maximises my function?","$$\begin{array}{ll} \text{maximize} & \frac{a}{100} + \frac{4b}{100} + \frac{9c}{100} + \frac{16d}{100} + \frac{25e}{100} + \frac{36f}{100} + \frac{49g}{100} + \frac{64h}{100} + \frac{81i}{100} + \frac{100j}{100}\\ \text{subject to} & a+b+c+d+e+f+g+h+i+j = 100\end{array}$$ When I use the Lagrange multiplier and I take all my partial derivatives, I do not get any of the variables in terms of $\lambda$. For example, when I take the derivative with respect to $a$, I get $$\frac{1}{100} = \lambda$$ and with respect to $b$, I get $$\frac{4}{100} = \lambda$$ Is there any way I can find the values of $a$, $b$, $c$, $d$, $e$, $f$, $g$, $h$, $i$ and $j$ which maximises my function?",,"['multivariable-calculus', 'optimization', 'convex-optimization', 'linear-programming', 'lagrange-multiplier']"
54,"Let $f: \mathbb{R}^2\to \mathbb{R}^2$ given by $f(x, y) = (e^x \cos y, e^x \sin y)$.",Let  given by .,"f: \mathbb{R}^2\to \mathbb{R}^2 f(x, y) = (e^x \cos y, e^x \sin y)","Let $f: \mathbb{R}^2\to \mathbb{R}^2$ given by $f(x, y) = (e^x \cos y, e^x \sin y)$. Take $S$ to be the set $S = [0, 1]\times [0, \pi]$. (a) Calculate $Df$ and $\det Df$. (b) Sketch the image under $f$ of the set $S$. We remark that if one identifies $\mathbb{C}$ with $\mathbb{R}^2$ as usual, then $f$ is the function $f(z) = e^z$. For (a), $Df(x,y)=\begin{bmatrix}e^x \cos y & -e^x \sin y\\e^x \sin y  & e^x \cos y\end{bmatrix}$ and $\det \begin{bmatrix}e^x \cos y & -e^x \sin y\\e^x \sin y  & e^x \cos y\end{bmatrix}=e^{2x}\cos^2 y+e^{2x}\sin^2 y=e^{2x}$ I do not understand what I have to do in (b), could someone help me please? Thank you","Let $f: \mathbb{R}^2\to \mathbb{R}^2$ given by $f(x, y) = (e^x \cos y, e^x \sin y)$. Take $S$ to be the set $S = [0, 1]\times [0, \pi]$. (a) Calculate $Df$ and $\det Df$. (b) Sketch the image under $f$ of the set $S$. We remark that if one identifies $\mathbb{C}$ with $\mathbb{R}^2$ as usual, then $f$ is the function $f(z) = e^z$. For (a), $Df(x,y)=\begin{bmatrix}e^x \cos y & -e^x \sin y\\e^x \sin y  & e^x \cos y\end{bmatrix}$ and $\det \begin{bmatrix}e^x \cos y & -e^x \sin y\\e^x \sin y  & e^x \cos y\end{bmatrix}=e^{2x}\cos^2 y+e^{2x}\sin^2 y=e^{2x}$ I do not understand what I have to do in (b), could someone help me please? Thank you",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
55,"What characteristics do functions have, where $f(x,y) = -f(y,x)$?","What characteristics do functions have, where ?","f(x,y) = -f(y,x)",Edited the question. One commenter said these functions are antisymmetric. Does that mean they're not symmetric? Symmetric to what exactly? What are some general characteristics.,Edited the question. One commenter said these functions are antisymmetric. Does that mean they're not symmetric? Symmetric to what exactly? What are some general characteristics.,,"['calculus', 'algebra-precalculus', 'multivariable-calculus']"
56,"Show $f(x,y) = y^2 - x^2$ at $(0,0)$ has a critical point, but is not a max/min value","Show  at  has a critical point, but is not a max/min value","f(x,y) = y^2 - x^2 (0,0)","So as always... I found the partial derivative with respect to $x$ and $y$ of $f(x,y)$ which gave me: $f_x=-2x$ $f_y=2y$ So I wasn't too sure what to do next, but I set $f_x = 0$: $0 = -2x$ $x=0$ And I got stuck again. How do I continue AND prove that $f(x,y)$ at $(0,0)$ has a critical point but is not max/min value? Thanks!","So as always... I found the partial derivative with respect to $x$ and $y$ of $f(x,y)$ which gave me: $f_x=-2x$ $f_y=2y$ So I wasn't too sure what to do next, but I set $f_x = 0$: $0 = -2x$ $x=0$ And I got stuck again. How do I continue AND prove that $f(x,y)$ at $(0,0)$ has a critical point but is not max/min value? Thanks!",,"['multivariable-calculus', 'extreme-value-theorem']"
57,"Are bounded level curves of a continuous function $f(x,y)$ closed?",Are bounded level curves of a continuous function  closed?,"f(x,y)","Let's say I consider all the points $(x,y)$ such that $f(x,y) = c$ for some $c$, given $f$ continuous, and let's assume the set of this points is bounded. Now consider any of the connected components of this set. I get the impression that the only way for a connected component not to be a closed curve (closed in the sense of curves, not in the sense of sets) is for the values $f(x,y)$ to be local extrema. Is this right? If not, can anyone come up with an example of a continuous function that has a bounded non closed level curve, which is not comprised of local extrema? If yes, does the answer extend to higher dimensions? (e.g., closed surfaces for a continuous function $f(x,y,z)$)","Let's say I consider all the points $(x,y)$ such that $f(x,y) = c$ for some $c$, given $f$ continuous, and let's assume the set of this points is bounded. Now consider any of the connected components of this set. I get the impression that the only way for a connected component not to be a closed curve (closed in the sense of curves, not in the sense of sets) is for the values $f(x,y)$ to be local extrema. Is this right? If not, can anyone come up with an example of a continuous function that has a bounded non closed level curve, which is not comprised of local extrema? If yes, does the answer extend to higher dimensions? (e.g., closed surfaces for a continuous function $f(x,y,z)$)",,"['real-analysis', 'functions', 'multivariable-calculus']"
58,Range of a vector function,Range of a vector function,,"May someone help me. Suppose that there exist two real functions $f_1$ and $f_2$ defined on $A\subset\Bbb{R}^n$. The images of them are denoted by $f_1(A)$ and $f_2(A)$, respectively. Let us define a vector function by $f=(f_1,f_2): A \to \Bbb{R}^2$. Am I right or not if I write $f(A)=f_1(A) \times f_2(A)$?. Is there a notion of the range for a vector function? If yes, the set $f(A)$ can be said the range of $f$ or not?","May someone help me. Suppose that there exist two real functions $f_1$ and $f_2$ defined on $A\subset\Bbb{R}^n$. The images of them are denoted by $f_1(A)$ and $f_2(A)$, respectively. Let us define a vector function by $f=(f_1,f_2): A \to \Bbb{R}^2$. Am I right or not if I write $f(A)=f_1(A) \times f_2(A)$?. Is there a notion of the range for a vector function? If yes, the set $f(A)$ can be said the range of $f$ or not?",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
59,Determining all points where this function is holomorphic,Determining all points where this function is holomorphic,,"Given $z=x+iy$ and $f(z)=x^3+3xy^2+i(y^3+3x^2y)$ I have to determine all points where f is holomorphic. Since the function is a polynomial, does it not mean that it would be holomorphic everywhere?","Given $z=x+iy$ and $f(z)=x^3+3xy^2+i(y^3+3x^2y)$ I have to determine all points where f is holomorphic. Since the function is a polynomial, does it not mean that it would be holomorphic everywhere?",,"['complex-analysis', 'multivariable-calculus', 'complex-numbers']"
60,Laplacian in Polar Coordinates: $\nabla^2f(r)=f''(r)+\frac{2}{r}f'(r)$,Laplacian in Polar Coordinates:,\nabla^2f(r)=f''(r)+\frac{2}{r}f'(r),"$\vec{R}=x \hat{i} + y\hat{j} + z\hat{k}$ and   $r=|\vec{R}|=\sqrt{x^2+y^2+z^2}$ Prove that $\nabla^2f(r)=f''(r)+\frac{2}{r}f'(r)$ So we need to basically show that $$\frac{\partial^2}{\partial x^2}f(r)+\frac{\partial^2}{\partial y^2}f(r)+\frac{\partial^2}{\partial z^2}f(r)=\frac{d^2f}{dr^2}+\frac{2}{r}\frac{df}{dr}$$ $f(r)$ is a scalar function of $r$. I'm not sure how to go about proving this. How can we possibly express $\frac{d^2f}{dr^2}$ and $\frac{df}{dr}$ in terms of $\frac{\partial^2}{\partial x^2}f(r)$, $\frac{\partial^2}{\partial y^2}f(r)$ and $\frac{\partial^2}{\partial z^2}f(r)$ ?","$\vec{R}=x \hat{i} + y\hat{j} + z\hat{k}$ and   $r=|\vec{R}|=\sqrt{x^2+y^2+z^2}$ Prove that $\nabla^2f(r)=f''(r)+\frac{2}{r}f'(r)$ So we need to basically show that $$\frac{\partial^2}{\partial x^2}f(r)+\frac{\partial^2}{\partial y^2}f(r)+\frac{\partial^2}{\partial z^2}f(r)=\frac{d^2f}{dr^2}+\frac{2}{r}\frac{df}{dr}$$ $f(r)$ is a scalar function of $r$. I'm not sure how to go about proving this. How can we possibly express $\frac{d^2f}{dr^2}$ and $\frac{df}{dr}$ in terms of $\frac{\partial^2}{\partial x^2}f(r)$, $\frac{\partial^2}{\partial y^2}f(r)$ and $\frac{\partial^2}{\partial z^2}f(r)$ ?",,"['calculus', 'real-analysis']"
61,What does the notation $\big(\frac{\partial p}{\partial T}\big)_V$ mean?,What does the notation  mean?,\big(\frac{\partial p}{\partial T}\big)_V,"In a book about thermodynamics, I came across the notation shown in the title. A more complete example would be $$dU = \left(\frac{\partial U}{\partial T}\right)_V\; dT + \left(\frac{\partial U}{\partial V}\right)_T\; dV$$ But I have no idea what the subscript means in this notation, would love some help. I ran in to trouble with an exercise that said Assume that gases behave according to a law given by $pV = f(T)$, where $f(T)$ is a function of temperature. Show that this implies $$\left(\frac{\partial p}{\partial T}\right)_V = \frac1V\frac{df}{dT}$$","In a book about thermodynamics, I came across the notation shown in the title. A more complete example would be $$dU = \left(\frac{\partial U}{\partial T}\right)_V\; dT + \left(\frac{\partial U}{\partial V}\right)_T\; dV$$ But I have no idea what the subscript means in this notation, would love some help. I ran in to trouble with an exercise that said Assume that gases behave according to a law given by $pV = f(T)$, where $f(T)$ is a function of temperature. Show that this implies $$\left(\frac{\partial p}{\partial T}\right)_V = \frac1V\frac{df}{dT}$$",,"['multivariable-calculus', 'notation', 'physics']"
62,Show $\nabla x^TAx = (A + A^T)x$ using property of $\nabla$,Show  using property of,\nabla x^TAx = (A + A^T)x \nabla,"This question is similar to How to take the gradient of the quadratic form? But I do not like the answer to that question. The reason is because the answerer is mixing $\nabla$ with $\dfrac{\partial}{\partial x}$. What I have learned is that $\nabla f$ is a column vector, it is the gradient of $f$, and$\dfrac{\partial}{\partial x} f$ is the Jacobian of $f$, which evaluates to a row vector when $f$ is a scalar. Why is it even appropriate to mix these notations...? ... In any case, is there a straight forward way of using properties of $\nabla$ to show $$\nabla x^TAx = (A + A^T)x, A \in \mathbb{R}^{n \times n}, x \in \mathbb{R}^n$$ I like the approach used in the linked question. Let $y(x) = Ax$, then $$\nabla x^TAx = \nabla x^Ty(x)$$ Is there a way to use a chain rule for $\nabla$ at this stage? I'm thinking of something like $$\nabla x^TAx = \nabla x^Ty(x) = y(x)^T \nabla x + x^T\nabla y(x)$$ But I have no idea if the above holds.","This question is similar to How to take the gradient of the quadratic form? But I do not like the answer to that question. The reason is because the answerer is mixing $\nabla$ with $\dfrac{\partial}{\partial x}$. What I have learned is that $\nabla f$ is a column vector, it is the gradient of $f$, and$\dfrac{\partial}{\partial x} f$ is the Jacobian of $f$, which evaluates to a row vector when $f$ is a scalar. Why is it even appropriate to mix these notations...? ... In any case, is there a straight forward way of using properties of $\nabla$ to show $$\nabla x^TAx = (A + A^T)x, A \in \mathbb{R}^{n \times n}, x \in \mathbb{R}^n$$ I like the approach used in the linked question. Let $y(x) = Ax$, then $$\nabla x^TAx = \nabla x^Ty(x)$$ Is there a way to use a chain rule for $\nabla$ at this stage? I'm thinking of something like $$\nabla x^TAx = \nabla x^Ty(x) = y(x)^T \nabla x + x^T\nabla y(x)$$ But I have no idea if the above holds.",,"['linear-algebra', 'multivariable-calculus', 'derivatives', 'vector-analysis', 'quadratic-forms']"
63,Does the gradient of a gradient exist? [closed],Does the gradient of a gradient exist? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Does the gradient of a gradient exist and if yes, what is it and does it have a intuitive representation ? Thanks for any answers!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Does the gradient of a gradient exist and if yes, what is it and does it have a intuitive representation ? Thanks for any answers!",,"['multivariable-calculus', 'vector-analysis']"
64,Average distance from a point in a hollow sphere to the surface of the sphere?,Average distance from a point in a hollow sphere to the surface of the sphere?,,"I know that the average value of a function $f(x,y,z)$ over a volume $D$ is given by $$ \bar{f} = \frac{1}{V} \int\int\int_D f(x,y,z) dV $$ I'm trying to set up the integral of the average distance from a point some distance a from the origin of a hollow sphere to the surface of the sphere ( image from here ): I used this equation to find $d$: $R^2=a^2+d^2+2adcos(\theta)$ $$ \Rightarrow d = \sqrt{R^2-a^2sin^2(\theta)}-acos(\theta) $$ My problem is I am now having difficulty setting up the integral. I know I'm supposed to get: $$ \bar{r} = \frac{3R}{2} \int_0^1 \int_0^{\pi} \sqrt{1-k^2sin^2(\theta)}k^2sin(\theta)dkd\theta $$ About the closest I can get to that is: $$ \bar{d} = \frac{3}{4\pi R^3} \int_0^1 \int_0^{\pi} \int_0^{2\pi} \bigg(\sqrt{R^2-a^2sin^2(\theta)}-acos(\theta)\bigg)k^2sin(\theta)dkd\phi d\theta $$ I'm clearly conceptually missing something, so any advice for what I'm misunderstanding or hints on how to set up this integral would be greatly appreciated.","I know that the average value of a function $f(x,y,z)$ over a volume $D$ is given by $$ \bar{f} = \frac{1}{V} \int\int\int_D f(x,y,z) dV $$ I'm trying to set up the integral of the average distance from a point some distance a from the origin of a hollow sphere to the surface of the sphere ( image from here ): I used this equation to find $d$: $R^2=a^2+d^2+2adcos(\theta)$ $$ \Rightarrow d = \sqrt{R^2-a^2sin^2(\theta)}-acos(\theta) $$ My problem is I am now having difficulty setting up the integral. I know I'm supposed to get: $$ \bar{r} = \frac{3R}{2} \int_0^1 \int_0^{\pi} \sqrt{1-k^2sin^2(\theta)}k^2sin(\theta)dkd\theta $$ About the closest I can get to that is: $$ \bar{d} = \frac{3}{4\pi R^3} \int_0^1 \int_0^{\pi} \int_0^{2\pi} \bigg(\sqrt{R^2-a^2sin^2(\theta)}-acos(\theta)\bigg)k^2sin(\theta)dkd\phi d\theta $$ I'm clearly conceptually missing something, so any advice for what I'm misunderstanding or hints on how to set up this integral would be greatly appreciated.",,"['integration', 'multivariable-calculus', 'definite-integrals', 'volume']"
65,how to parametrize a curve in $\mathbb R^3$?,how to parametrize a curve in ?,\mathbb R^3,"How to parametrize $x^2+y^2+z^2=4$ and $x+z=2$ $x$, $y$ and $z$ should be function of $t$ I have tried to eliminate $z$ but it doesn't work","How to parametrize $x^2+y^2+z^2=4$ and $x+z=2$ $x$, $y$ and $z$ should be function of $t$ I have tried to eliminate $z$ but it doesn't work",,['multivariable-calculus']
66,Finding the matrix derivative of $X^{-1}$ with respect to $X$ [duplicate],Finding the matrix derivative of  with respect to  [duplicate],X^{-1} X,"This question already has answers here : How to evaluate the derivatives of matrix inverse? (3 answers) Closed 6 years ago . Assume $X \in \mathbb{R^{n \times n}}$. I could not found particular formula to calculate the Derivative of $X^{-1}$ with respect to $X$, but I found a formula related to inverse of matrix as follows: (1)$\frac{\partial}{\partial X} (a^TX^{-1}b) = -X^{-T}ab^TX^{-T} \quad a, b \in \mathbb{R}^n$ Can anyone give an insight on how derive a formula for derivative of $X^{-1}$ or formula (1) please? Thank you in advance. ====== This post shared and discussed the same topic and I was asked if the current post is redundant. I believe the way the problem stated and discussed in these two posts is different. Specifically, I was trying to learn a simple approach for finding the derivative of a matrix expression that contains inverse of a matrix. I believe the detailed answer and discussion in the post is helpful to other learners like me(with an elementary calculus and matrix understanding).","This question already has answers here : How to evaluate the derivatives of matrix inverse? (3 answers) Closed 6 years ago . Assume $X \in \mathbb{R^{n \times n}}$. I could not found particular formula to calculate the Derivative of $X^{-1}$ with respect to $X$, but I found a formula related to inverse of matrix as follows: (1)$\frac{\partial}{\partial X} (a^TX^{-1}b) = -X^{-T}ab^TX^{-T} \quad a, b \in \mathbb{R}^n$ Can anyone give an insight on how derive a formula for derivative of $X^{-1}$ or formula (1) please? Thank you in advance. ====== This post shared and discussed the same topic and I was asked if the current post is redundant. I believe the way the problem stated and discussed in these two posts is different. Specifically, I was trying to learn a simple approach for finding the derivative of a matrix expression that contains inverse of a matrix. I believe the detailed answer and discussion in the post is helpful to other learners like me(with an elementary calculus and matrix understanding).",,"['matrices', 'multivariable-calculus', 'derivatives', 'conic-sections', 'matrix-calculus']"
67,"Does the curve $c(t)=\langle \sqrt{1-t^2}\cos t,\sqrt{1-t^2}\sin t,t\rangle$ lie on unit sphere?",Does the curve  lie on unit sphere?,"c(t)=\langle \sqrt{1-t^2}\cos t,\sqrt{1-t^2}\sin t,t\rangle","Given curve $c(t)=\langle \sqrt{1-t^2}\cos t,\sqrt{1-t^2}\sin t,t\rangle$ and $|t|\le 1$ does the curve lie on a sphere which has radius of $1$ and is centered at $(0,0,0)$? I thought that: $$ x=\sqrt{1-t^2}\cos t\implies\cos t=\frac{x}{\sqrt{1-t^2}}\\ y=\sqrt{1-t^2}\sin t\implies\sin t=\frac{y}{\sqrt{1-t^2}} $$ but I have no idea what to do with $z$ coordinate.","Given curve $c(t)=\langle \sqrt{1-t^2}\cos t,\sqrt{1-t^2}\sin t,t\rangle$ and $|t|\le 1$ does the curve lie on a sphere which has radius of $1$ and is centered at $(0,0,0)$? I thought that: $$ x=\sqrt{1-t^2}\cos t\implies\cos t=\frac{x}{\sqrt{1-t^2}}\\ y=\sqrt{1-t^2}\sin t\implies\sin t=\frac{y}{\sqrt{1-t^2}} $$ but I have no idea what to do with $z$ coordinate.",,"['multivariable-calculus', 'parametric']"
68,limit of integration after change of variables,limit of integration after change of variables,,"Evaluate $$\int_{0}^{\infty}\int_{0}^{\infty}e^{-(5x^2-6xy+5y^2)}dxdy$$ after applying the change of variables as $$x=u+v~~,y=u-v$$ i got the integral as $$\int\int e^{-4u^2-16v^2}{2}dudv$$ But how do i find the limits of integration? After seeing the cooments i got the integral set up as $$\int_{-\infty}^{\infty}\int_{0}^{\infty} e^{-4u^2-16v^2}{2}dudv=4\int_{0}^{\infty}\int_{0}^{\infty} e^{-4u^2-16v^2}dudv=4\int_{0}^{\infty}e^{-4u^2}du\int_{0}^{\infty}e^{-16v^2}dv=\dfrac{\pi}{8}$$,  But when i solve this integral from wolfram it gives $\dfrac{1}{16}\left(\pi+2\tan^{-1}\dfrac{3}{4}\right)$, i can't find the error in my calculation, can somebody help","Evaluate $$\int_{0}^{\infty}\int_{0}^{\infty}e^{-(5x^2-6xy+5y^2)}dxdy$$ after applying the change of variables as $$x=u+v~~,y=u-v$$ i got the integral as $$\int\int e^{-4u^2-16v^2}{2}dudv$$ But how do i find the limits of integration? After seeing the cooments i got the integral set up as $$\int_{-\infty}^{\infty}\int_{0}^{\infty} e^{-4u^2-16v^2}{2}dudv=4\int_{0}^{\infty}\int_{0}^{\infty} e^{-4u^2-16v^2}dudv=4\int_{0}^{\infty}e^{-4u^2}du\int_{0}^{\infty}e^{-16v^2}dv=\dfrac{\pi}{8}$$,  But when i solve this integral from wolfram it gives $\dfrac{1}{16}\left(\pi+2\tan^{-1}\dfrac{3}{4}\right)$, i can't find the error in my calculation, can somebody help",,"['multivariable-calculus', 'definite-integrals', 'gamma-function', 'change-of-variable']"
69,"Double integral $ \iint\ln(\sin(u-7v)) \,du\,dv$",Double integral," \iint\ln(\sin(u-7v)) \,du\,dv","Can you please help me solve this double integral? $$ \iint_D\ln(\sin(u-7v))\,du\,dv $$ where $$D:=\left\{(u,v)\;:\; 0\le u \le \pi \;,\; 0\le v \le \frac u7\right\} .$$ I know it's not possible to solve $\int \ln(\sin(x))$ but definite integral is?","Can you please help me solve this double integral? $$ \iint_D\ln(\sin(u-7v))\,du\,dv $$ where $$D:=\left\{(u,v)\;:\; 0\le u \le \pi \;,\; 0\le v \le \frac u7\right\} .$$ I know it's not possible to solve $\int \ln(\sin(x))$ but definite integral is?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
70,"Find extremas of $f(x,y) = xy \ln(x^2+y^2), x>0, y>0$",Find extremas of,"f(x,y) = xy \ln(x^2+y^2), x>0, y>0","As the title says I need to find extreme values(maximum and minimum) of $$f(x,y) = xy \ln(x^2+y^2), x>0, y>0$$ I don't understand how to find critical points of this problem. I start with finding partial derivative and set derivatives equal to zero. And that is where I am stuck currently. So any help would be appreciated. So: $ \frac{\partial f}{\partial x} = \frac{\partial}{\partial x} (xy* ln(x^2+y^2)) = y*ln(x^2+y^2) + \frac{2xy^2}{x^2+y^2} $ $ \frac{\partial f}{\partial y} = \frac{\partial}{\partial y} (xy* ln(x^2+y^2)) = x*ln(x^2+y^2) + \frac{2x^2y}{x^2+y^2} $ So we have now: $ \nabla f(x,y) = (0,0)  $ $  y*ln(x^2+y^2) + \frac{2xy^2}{(x^2+y^2)^2} = 0 $ And $  x*ln(x^2+y^2) + \frac{2x^2y}{(x^2+y^2)^2} = 0 $ After trying to solve these equations I get that $x=y$ Is that correct?. So I don't understand what are then critical points as It can't be (0,0)?","As the title says I need to find extreme values(maximum and minimum) of $$f(x,y) = xy \ln(x^2+y^2), x>0, y>0$$ I don't understand how to find critical points of this problem. I start with finding partial derivative and set derivatives equal to zero. And that is where I am stuck currently. So any help would be appreciated. So: $ \frac{\partial f}{\partial x} = \frac{\partial}{\partial x} (xy* ln(x^2+y^2)) = y*ln(x^2+y^2) + \frac{2xy^2}{x^2+y^2} $ $ \frac{\partial f}{\partial y} = \frac{\partial}{\partial y} (xy* ln(x^2+y^2)) = x*ln(x^2+y^2) + \frac{2x^2y}{x^2+y^2} $ So we have now: $ \nabla f(x,y) = (0,0)  $ $  y*ln(x^2+y^2) + \frac{2xy^2}{(x^2+y^2)^2} = 0 $ And $  x*ln(x^2+y^2) + \frac{2x^2y}{(x^2+y^2)^2} = 0 $ After trying to solve these equations I get that $x=y$ Is that correct?. So I don't understand what are then critical points as It can't be (0,0)?",,['multivariable-calculus']
71,Change integration order,Change integration order,,"I am confused due to graphics $$\int_0^2\mathrm{d}x\int_{x}^{2x}f(x,y)\,\mathrm{d}y$$ well, for reverse order we have to find $x=y$ and $x=\frac{y}2{}$ as a functional limits for $dx$ but I do not know how determine number limits for $dy$ Plot does not make things clear: How should I handle?","I am confused due to graphics $$\int_0^2\mathrm{d}x\int_{x}^{2x}f(x,y)\,\mathrm{d}y$$ well, for reverse order we have to find $x=y$ and $x=\frac{y}2{}$ as a functional limits for $dx$ but I do not know how determine number limits for $dy$ Plot does not make things clear: How should I handle?",,"['multivariable-calculus', 'multiple-integral']"
72,$\frac1{2\pi\rho^n}\int_{-\pi}^{\pi}\exp\left(\frac{2+\rho\cos\theta}{4+4\rho\cos\theta+\rho^2}\right)\cos\beta_nd\theta$ does not depend on $\rho$.,does not depend on .,\frac1{2\pi\rho^n}\int_{-\pi}^{\pi}\exp\left(\frac{2+\rho\cos\theta}{4+4\rho\cos\theta+\rho^2}\right)\cos\beta_nd\theta \rho,Let $$\beta_n=\frac{\rho\sin\theta}{4+4\rho\cos\theta+\rho^2}+n\theta$$ where $0<\rho<2$. Could anyone give me some hints to prove analytically that $$\frac1{2\pi\rho^n}\int_{-\pi}^{\pi}\exp\left(\frac{2+\rho\cos\theta}{4+4\rho\cos\theta+\rho^2}\right)\cos\beta_nd\theta$$ does not depend on $\rho$?,Let $$\beta_n=\frac{\rho\sin\theta}{4+4\rho\cos\theta+\rho^2}+n\theta$$ where $0<\rho<2$. Could anyone give me some hints to prove analytically that $$\frac1{2\pi\rho^n}\int_{-\pi}^{\pi}\exp\left(\frac{2+\rho\cos\theta}{4+4\rho\cos\theta+\rho^2}\right)\cos\beta_nd\theta$$ does not depend on $\rho$?,,"['calculus', 'real-analysis']"
73,Confusion with Edwards's Proof of Inverse Mapping Theorem: The convergent sequence.,Confusion with Edwards's Proof of Inverse Mapping Theorem: The convergent sequence.,,"This question arose while working through the proof of the inverse mapping theorem (Theorem III 3.3) in C.H. Edwards, Jr.'s Advanced Calculus of Several Variables. Edit to add definitions: $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ is a $\mathscr{C}^{1}$ mapping in a neighborhood of the point $a$. $T=df_{a}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ is the differential of $f$ at $a$. The follow are established in the text $g_{0}(y)=a$ $\tau_{a}(x)=x+a$ $T\circ\tau_{a}^{-1}\circ g_{k+1}(y)=T\circ\tau_{a}^{-1}\circ g_{k}(y)-[f(g_{k}(y))-y]$. Edwards instructs the student to apply $\tau_{a}\circ T^{-1}$ to both sides of this expression. What I get is $g_{k+1}(y)=g_{k}(y)-\tau_{a}\circ T^{-1}[f(g_{k}(y))-y]$ $g_{k+1}(y)=g_{k}(y)-T^{-1}[f(g_{k}(y))-y]-a$. Edwards gives the generic induction step as $g_{k+1}(y)=g_{k}(y)-T^{-1}[f(g_{k}(y))-y]$. Now, according to my result the first induction step is $g_{1}(y)=g_{0}(y)-T^{-1}[f(g_{0}(y))-y]-a$ $=a-T^{-1}[f(a)-y]-a$ $=-T^{-1}[f(a)-y]$ So the $-a$ is canceled. But I don't see how that would be inherited in subsequent steps. For example, $g_{2}(y)=-T^{-1}[f(-T^{-1}[f(a)-y])-y]-a$ seems to be the obvious next term. Where is the error?","This question arose while working through the proof of the inverse mapping theorem (Theorem III 3.3) in C.H. Edwards, Jr.'s Advanced Calculus of Several Variables. Edit to add definitions: $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ is a $\mathscr{C}^{1}$ mapping in a neighborhood of the point $a$. $T=df_{a}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ is the differential of $f$ at $a$. The follow are established in the text $g_{0}(y)=a$ $\tau_{a}(x)=x+a$ $T\circ\tau_{a}^{-1}\circ g_{k+1}(y)=T\circ\tau_{a}^{-1}\circ g_{k}(y)-[f(g_{k}(y))-y]$. Edwards instructs the student to apply $\tau_{a}\circ T^{-1}$ to both sides of this expression. What I get is $g_{k+1}(y)=g_{k}(y)-\tau_{a}\circ T^{-1}[f(g_{k}(y))-y]$ $g_{k+1}(y)=g_{k}(y)-T^{-1}[f(g_{k}(y))-y]-a$. Edwards gives the generic induction step as $g_{k+1}(y)=g_{k}(y)-T^{-1}[f(g_{k}(y))-y]$. Now, according to my result the first induction step is $g_{1}(y)=g_{0}(y)-T^{-1}[f(g_{0}(y))-y]-a$ $=a-T^{-1}[f(a)-y]-a$ $=-T^{-1}[f(a)-y]$ So the $-a$ is canceled. But I don't see how that would be inherited in subsequent steps. For example, $g_{2}(y)=-T^{-1}[f(-T^{-1}[f(a)-y])-y]-a$ seems to be the obvious next term. Where is the error?",,"['sequences-and-series', 'multivariable-calculus', 'induction']"
74,Why doesn't the limit of $(1 + x - y) / (x^2+ y^2) $ exist?,Why doesn't the limit of  exist?,(1 + x - y) / (x^2+ y^2) ,"How to compute the following limit: $$\lim_{(x,y)\to (0,0)}\frac{1+x-y}{x^2+y^2}$$ ? The teacher's answers is ""the limit doesn't exist"". But when replace the variables, the value is  $$\frac{1 + 0 - 0}{0^2+0^2}=\frac{1}{0}=\infty.$$ Which one is correct?","How to compute the following limit: $$\lim_{(x,y)\to (0,0)}\frac{1+x-y}{x^2+y^2}$$ ? The teacher's answers is ""the limit doesn't exist"". But when replace the variables, the value is  $$\frac{1 + 0 - 0}{0^2+0^2}=\frac{1}{0}=\infty.$$ Which one is correct?",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus']"
75,What is$\displaystyle \lim_{\substack{x \rightarrow 0^{+} \\ y \rightarrow 1^{-}}} \frac{x+y-1}{\sqrt{x}-\sqrt{1-y}}$?,What is?,\displaystyle \lim_{\substack{x \rightarrow 0^{+} \\ y \rightarrow 1^{-}}} \frac{x+y-1}{\sqrt{x}-\sqrt{1-y}},"Will be the value in the form of $\frac{""0""}{""0""}$? Do I have to use the L'Hopital rule? Or can I say, that the limit doesn't exist?","Will be the value in the form of $\frac{""0""}{""0""}$? Do I have to use the L'Hopital rule? Or can I say, that the limit doesn't exist?",,"['limits', 'multivariable-calculus']"
76,How to calculate gradient of $f(x)=x^TAx+b^Tx$? [duplicate],How to calculate gradient of ? [duplicate],f(x)=x^TAx+b^Tx,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . How to calculate the gradient of $f(x)=x^TAx+b^Tx$  when $A$ is symmetric and when $A$ is not symmetric? I will have confirmation if the computation of the gradient of $f$ when $A$ is a square matrix of size $n \times n$ non-symmetric and when $A$ is symmetric. I begin my proof $f:R^n \to R$ 1) A is no symmetric: It is already noted that : $f(x)=a^TAx=x^TA^Tx$ because $a^TAx$ is a scalar$ So about calculating gradient of $$ and he does that using the concept of exterior derivative. $f(x+a)=(x+a)^T(x+h)+b^T(x+h)$ $x^TAx+a^Ax+x^Aa+a^TAa+B^Tx+B^Th$ $f(x)+x^T(A+A^T)a+a^T+B^Tx+B^Th$ $∇f(x)a=(A+A^T+B^T)x+B^Th$ 2) A is symmetric so $A^T=A$ $∇f(x)a=$2Ax It would be great if someone could help me solve the problems I will be very thankful Also ,  I will also have another question in both cases the staging for a decadent gradient and quasi newton algorithm, thank you Paul-henri","This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . How to calculate the gradient of $f(x)=x^TAx+b^Tx$  when $A$ is symmetric and when $A$ is not symmetric? I will have confirmation if the computation of the gradient of $f$ when $A$ is a square matrix of size $n \times n$ non-symmetric and when $A$ is symmetric. I begin my proof $f:R^n \to R$ 1) A is no symmetric: It is already noted that : $f(x)=a^TAx=x^TA^Tx$ because $a^TAx$ is a scalar$ So about calculating gradient of $$ and he does that using the concept of exterior derivative. $f(x+a)=(x+a)^T(x+h)+b^T(x+h)$ $x^TAx+a^Ax+x^Aa+a^TAa+B^Tx+B^Th$ $f(x)+x^T(A+A^T)a+a^T+B^Tx+B^Th$ $∇f(x)a=(A+A^T+B^T)x+B^Th$ 2) A is symmetric so $A^T=A$ $∇f(x)a=$2Ax It would be great if someone could help me solve the problems I will be very thankful Also ,  I will also have another question in both cases the staging for a decadent gradient and quasi newton algorithm, thank you Paul-henri",,"['multivariable-calculus', 'derivatives', 'matrix-calculus']"
77,Expressing double integrals in polar coordinates,Expressing double integrals in polar coordinates,,I have the double integrals $\int_{x}^\infty \int_0^\infty e^{-\Lambda^a(x^a+y^a)}dxdy$ where $a$ and $\Lambda$ are real positive constants. I would like to express it in polar coordinates. What are my $r$ and $\theta$? Thanks.,I have the double integrals $\int_{x}^\infty \int_0^\infty e^{-\Lambda^a(x^a+y^a)}dxdy$ where $a$ and $\Lambda$ are real positive constants. I would like to express it in polar coordinates. What are my $r$ and $\theta$? Thanks.,,"['calculus', 'integration', 'multivariable-calculus', 'special-functions', 'polar-coordinates']"
78,Find the maximum of the multivariable function,Find the maximum of the multivariable function,,"Find the maximum of the following function:   $$h(x, y)  =  \ln(x^{40} y^{60}) $$   given the constraints:   $$2x^2 + 3y^2  =  10, \quad \quad x > 0,   y > 0. $$ I know that for the first step I the $h$'s should $= 40/x , 60/y$ and the $g$'s should be $4x , 6y$. But i'm not should I should do with this information to get to the final answer. What is the final answer? It should be rounded to 5 decimal points.","Find the maximum of the following function:   $$h(x, y)  =  \ln(x^{40} y^{60}) $$   given the constraints:   $$2x^2 + 3y^2  =  10, \quad \quad x > 0,   y > 0. $$ I know that for the first step I the $h$'s should $= 40/x , 60/y$ and the $g$'s should be $4x , 6y$. But i'm not should I should do with this information to get to the final answer. What is the final answer? It should be rounded to 5 decimal points.",,"['calculus', 'multivariable-calculus']"
79,Parametric integral over a circle [closed],Parametric integral over a circle [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let us live in $\mathbb{R^2}$ , $\vec{e_1} = \vec{(1,0)}, \vec{e_2} = \vec{(0,1)}$ are two standard vectors. Evaluate $\Large{\int_{C_1} \frac{y\vec{e_1}-x\vec{e_2}}{x^2+y^2} d\vec{r}}$ where $C_1$ is a circle of radius $3$ centered at $(0,0)$ I am completely stuck.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let us live in , are two standard vectors. Evaluate where is a circle of radius centered at I am completely stuck.","\mathbb{R^2} \vec{e_1} = \vec{(1,0)}, \vec{e_2} = \vec{(0,1)} \Large{\int_{C_1} \frac{y\vec{e_1}-x\vec{e_2}}{x^2+y^2} d\vec{r}} C_1 3 (0,0)","['multivariable-calculus', 'line-integrals']"
80,Calculate volume of area enclosed by the surface $\sqrt[3]{x^2}+\sqrt[3]{y^2}+\sqrt[3]{z^2}=\sqrt[3]{a^2}$.,Calculate volume of area enclosed by the surface .,\sqrt[3]{x^2}+\sqrt[3]{y^2}+\sqrt[3]{z^2}=\sqrt[3]{a^2},Calculate the volume of the area enclosed by the surface   \begin{equation} x^{(2/3)}+y^{(2/3)}+z^{(2/3)} = a^{(2/3)} \end{equation}   where $a > 0$ is a constant. However I'm not sure where to begin.,Calculate the volume of the area enclosed by the surface   \begin{equation} x^{(2/3)}+y^{(2/3)}+z^{(2/3)} = a^{(2/3)} \end{equation}   where $a > 0$ is a constant. However I'm not sure where to begin.,,[]
81,"Why doesn't $\lim\limits_{(x,y) \to (0,0)} \frac{3}{x^2+2y^2}$ exist?",Why doesn't  exist?,"\lim\limits_{(x,y) \to (0,0)} \frac{3}{x^2+2y^2}","So, my book has this homework problem that has me pulling my hair out over. It asks us to evaluate the limit below - or rather, by trying along paths of the x and y axes, to show that it doesn't exist. $\lim\limits_{(x,y) \to (0,0)} \frac{3}{x^2+2y^2}$ When I evaluate along $y=0$ I get $\lim\limits_{(x,0) \to (0,0)} \frac{3}{x^2+2(0)^2}$ Which to me looks like it should just evaluate to +∞, just as along $x=0$ I get $\lim\limits_{(0,y) \to (0,0)} \frac{3}{0^2+2y^2}$ which also looks like it should evaluate to +∞. The flimsiest thing I can get about it not existing is that the Y term increases faster than the X term, but the book's answers section tells me that the limit doesn't exist along $x=0$. I'm probably just forgetting something really basic like a moron, but can someone beat me over the head, please?","So, my book has this homework problem that has me pulling my hair out over. It asks us to evaluate the limit below - or rather, by trying along paths of the x and y axes, to show that it doesn't exist. $\lim\limits_{(x,y) \to (0,0)} \frac{3}{x^2+2y^2}$ When I evaluate along $y=0$ I get $\lim\limits_{(x,0) \to (0,0)} \frac{3}{x^2+2(0)^2}$ Which to me looks like it should just evaluate to +∞, just as along $x=0$ I get $\lim\limits_{(0,y) \to (0,0)} \frac{3}{0^2+2y^2}$ which also looks like it should evaluate to +∞. The flimsiest thing I can get about it not existing is that the Y term increases faster than the X term, but the book's answers section tells me that the limit doesn't exist along $x=0$. I'm probably just forgetting something really basic like a moron, but can someone beat me over the head, please?",,"['limits', 'multivariable-calculus']"
82,Show limit to zero for $\frac{x^3-xy^2}{\vert x \vert + y^2}$ does not exist,Show limit to zero for  does not exist,\frac{x^3-xy^2}{\vert x \vert + y^2},"Given is the following function $f(x,y)= \frac{x^3-xy^2}{\vert x \vert + y^2}$. How to proof that the limit $\lim_{(x,y)\to(0,0)}f(x,y)$ does not exist? I have yet tried approaching zero in various directions, yet I have not been able to find two with different outcomes.","Given is the following function $f(x,y)= \frac{x^3-xy^2}{\vert x \vert + y^2}$. How to proof that the limit $\lim_{(x,y)\to(0,0)}f(x,y)$ does not exist? I have yet tried approaching zero in various directions, yet I have not been able to find two with different outcomes.",,['limits']
83,"Multivariable second derivative test: what if $f_{xx}(a, b) = 0$?",Multivariable second derivative test: what if ?,"f_{xx}(a, b) = 0","My textbook introduces the second derivative test for multivariable functions to determine local extrema. However, it does not discuss the case when $f_{xx} = 0$, so I'm at a loss as to what conclusion I can form. Here's the problem: Determine the local extrema for $f(x, y) = x^3 - 3x + 3xy^2$ My solution: $f_x = 3x^2 - 3 + 3y^2 = 0$ $f_y = 6xy = 0$ [this is true if $x = 0$ or $y = 0$] Using the conclusion above: $x = 0: 3y^2 - 3 = 0$, so $y = \pm1$ $y = 0: 3x^2 - 3 = 0$, so $x = \pm1$ Therefore, the critical points are $(0, -1)$, $(0, 1)$, $(-1, 0)$, and $(1, 0)$ Second partial derivatives: $f_{xx} = 6x$ $f_{yy} = 6x$ $f_{xy} = 6y$ Therefore: $D(x, y) = 36x^2 - 36y^2$ But at point (0, -1), I get that $D > 0$ and $f_{xx} = 0$. The textbook only specifies conclusions for $f_{xx} > 0$ or $f_{xx} < 0$.","My textbook introduces the second derivative test for multivariable functions to determine local extrema. However, it does not discuss the case when $f_{xx} = 0$, so I'm at a loss as to what conclusion I can form. Here's the problem: Determine the local extrema for $f(x, y) = x^3 - 3x + 3xy^2$ My solution: $f_x = 3x^2 - 3 + 3y^2 = 0$ $f_y = 6xy = 0$ [this is true if $x = 0$ or $y = 0$] Using the conclusion above: $x = 0: 3y^2 - 3 = 0$, so $y = \pm1$ $y = 0: 3x^2 - 3 = 0$, so $x = \pm1$ Therefore, the critical points are $(0, -1)$, $(0, 1)$, $(-1, 0)$, and $(1, 0)$ Second partial derivatives: $f_{xx} = 6x$ $f_{yy} = 6x$ $f_{xy} = 6y$ Therefore: $D(x, y) = 36x^2 - 36y^2$ But at point (0, -1), I get that $D > 0$ and $f_{xx} = 0$. The textbook only specifies conclusions for $f_{xx} > 0$ or $f_{xx} < 0$.",,"['multivariable-calculus', 'optimization']"
84,Interesting double integral,Interesting double integral,,"I just had a test at calculus and it seems that nobody computed this integral. Wolfram gives me the value, so nothing, symbolab has steps currently unavailable  (like always). I know the test has passed and I will never do calculus again, but still, I want to know how to compute this. $$ \iint_A\frac{2xy\sin(y)}{(1+x^4)(1+\cos^2(y))}dxdy \qquad\qquad A = [0, 1]\times[0,\pi] $$ In my thinking the first integral from left to right is the integral with respect to $y$, so the bounds will pe $0, \pi$, the second integral is with respect to $x$, in this case the bounds will be $0, 1$. Clarification: $A = [0, 1]\cup[0,\pi]$ and let $f:A\to \Bbb R$. Compute:  $$\iint_Af(x, y) dx dy $$ $A$ is just the domain of the function.","I just had a test at calculus and it seems that nobody computed this integral. Wolfram gives me the value, so nothing, symbolab has steps currently unavailable  (like always). I know the test has passed and I will never do calculus again, but still, I want to know how to compute this. $$ \iint_A\frac{2xy\sin(y)}{(1+x^4)(1+\cos^2(y))}dxdy \qquad\qquad A = [0, 1]\times[0,\pi] $$ In my thinking the first integral from left to right is the integral with respect to $y$, so the bounds will pe $0, \pi$, the second integral is with respect to $x$, in this case the bounds will be $0, 1$. Clarification: $A = [0, 1]\cup[0,\pi]$ and let $f:A\to \Bbb R$. Compute:  $$\iint_Af(x, y) dx dy $$ $A$ is just the domain of the function.",,"['real-analysis', 'integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
85,"Differentiability in $(0,0)$ of $f(x,y)=\frac{x^4-y^3}{x^2+y^2}$",Differentiability in  of,"(0,0) f(x,y)=\frac{x^4-y^3}{x^2+y^2}","Let the function $f$ be defined as $$f(x,y)=\frac{x^4-y^3}{x^2+y^2}$$ If $(x,y)=(0,0)$ then $f$ is equal to zero. My problem is to prove that this function isn't differentiable at the point $(0,0)$. My solution: First idea is maybe to show that $f$ is not continuous at $(0,0)$. That will show that $f$ isn't differentiable at $(0,0)$. But it is easy to see that $f$ is continuous at $(0,0)$, so we need to check the partial derivatives at $(0,0)$. Partial derivative in $x$-direction is $$\lim_{h\to 0}\frac{f(h,0)-0}{h}=0$$ Partial derivative in $y$-direction is $$\lim_{h\to 0}\frac{f(0,h)-0}{h}=-1$$ We now know how our Jacobian matrix at the point $(0,0)$ looks. We now use the definition of derivatives: $$\lim_{(x,y)\rightarrow(0,0)}\frac{\frac{x^4-y^3}{x^2+y^2}+y}{\sqrt{(x^2+y^2)}}=\frac{x^4+x^2y}{(x^2+y^2)^{\frac{3}{2}}}$$ But this looks like having a limit in $(0,0)$ and it's equal to $0$?","Let the function $f$ be defined as $$f(x,y)=\frac{x^4-y^3}{x^2+y^2}$$ If $(x,y)=(0,0)$ then $f$ is equal to zero. My problem is to prove that this function isn't differentiable at the point $(0,0)$. My solution: First idea is maybe to show that $f$ is not continuous at $(0,0)$. That will show that $f$ isn't differentiable at $(0,0)$. But it is easy to see that $f$ is continuous at $(0,0)$, so we need to check the partial derivatives at $(0,0)$. Partial derivative in $x$-direction is $$\lim_{h\to 0}\frac{f(h,0)-0}{h}=0$$ Partial derivative in $y$-direction is $$\lim_{h\to 0}\frac{f(0,h)-0}{h}=-1$$ We now know how our Jacobian matrix at the point $(0,0)$ looks. We now use the definition of derivatives: $$\lim_{(x,y)\rightarrow(0,0)}\frac{\frac{x^4-y^3}{x^2+y^2}+y}{\sqrt{(x^2+y^2)}}=\frac{x^4+x^2y}{(x^2+y^2)^{\frac{3}{2}}}$$ But this looks like having a limit in $(0,0)$ and it's equal to $0$?",,"['limits', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
86,Infinitely differentiable implicit function,Infinitely differentiable implicit function,,"I have an infinitely differentiable function $F(x,y) : [0,1]^2 \rightarrow \mathbb{R}$. I know that for every $x \in [0,1]$ there is a unique $y$ for which $F(x,y)=0$. Can I claim that the function $y(x)$ given by $F(x,y) = 0$ is infinitely differentiable and differentiate $F(x,y) = 0$ w.r.t $x$?","I have an infinitely differentiable function $F(x,y) : [0,1]^2 \rightarrow \mathbb{R}$. I know that for every $x \in [0,1]$ there is a unique $y$ for which $F(x,y)=0$. Can I claim that the function $y(x)$ given by $F(x,y) = 0$ is infinitely differentiable and differentiate $F(x,y) = 0$ w.r.t $x$?",,"['multivariable-calculus', 'implicit-differentiation']"
87,Why is a gradient not a linear functional?,Why is a gradient not a linear functional?,,"I'm reading this book and on page 104 they define: Afterwards they said that if $m=1$, the function $f$ is real-valued and $T$ is the gradient which has to be a linear functional according to the definition. The problem is this is not true in general, see for example: $f:\mathbb R^2\to \mathbb R$ defined as $f(x,y)=\sin x$. The gradient is $\nabla f=(\cos x,0)$ which is not a linear functional.","I'm reading this book and on page 104 they define: Afterwards they said that if $m=1$, the function $f$ is real-valued and $T$ is the gradient which has to be a linear functional according to the definition. The problem is this is not true in general, see for example: $f:\mathbb R^2\to \mathbb R$ defined as $f(x,y)=\sin x$. The gradient is $\nabla f=(\cos x,0)$ which is not a linear functional.",,"['analysis', 'multivariable-calculus', 'definition']"
88,Understanding second derivatives,Understanding second derivatives,,"I am having a hard time understanding how to determine the second derivative of a matrix. I have researched Hessian matrices and do not see how i would apply it to vector funciton. problem statement: compute the derivative of the following $$ f(x) =\begin{bmatrix} x_1+x_1x_2^2\\ -x_2+x_2^2+x_1^2\\ \end{bmatrix}$$ I have found $$ DF =\begin{bmatrix} 1+x_2^2&2x_1\\ 2x_1&-1+x_2\\ \end{bmatrix} $$ I now need to compute $D^2f(x_0)(x,y)$. I have $$ D^2f(x_0)(x,y) = \sum_{j_1,j_2=1}^n \frac{\partial^2f(x_0)}{\partial x_{j_1} \partial x_{j_2}} x_{j_1}y_{j_2} $$ But don't quite understand it. I have also searched Hessian Matrix, however the majority of times when it is used is when there is only one equation. Could someone please walk me through the $$D^2f(x_0)(x,y)$$ equation and also explain if a Hessian matrix is applicable here. I will provide $x_0$ if needed","I am having a hard time understanding how to determine the second derivative of a matrix. I have researched Hessian matrices and do not see how i would apply it to vector funciton. problem statement: compute the derivative of the following $$ f(x) =\begin{bmatrix} x_1+x_1x_2^2\\ -x_2+x_2^2+x_1^2\\ \end{bmatrix}$$ I have found $$ DF =\begin{bmatrix} 1+x_2^2&2x_1\\ 2x_1&-1+x_2\\ \end{bmatrix} $$ I now need to compute $D^2f(x_0)(x,y)$. I have $$ D^2f(x_0)(x,y) = \sum_{j_1,j_2=1}^n \frac{\partial^2f(x_0)}{\partial x_{j_1} \partial x_{j_2}} x_{j_1}y_{j_2} $$ But don't quite understand it. I have also searched Hessian Matrix, however the majority of times when it is used is when there is only one equation. Could someone please walk me through the $$D^2f(x_0)(x,y)$$ equation and also explain if a Hessian matrix is applicable here. I will provide $x_0$ if needed",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'tensors', 'hessian-matrix']"
89,"Find the absolute minimum of the function $ f(x,y) = x^2+y^2 $ subject to the constraint $x^2+ 2y^2 = 1 $.",Find the absolute minimum of the function  subject to the constraint .," f(x,y) = x^2+y^2  x^2+ 2y^2 = 1 ","I tried this question on a site and found that none of the answers were the ones I got. I think my calculations were right but my understanding of applying the correct theorems may have been off. I found the derivatives with respect to x and y and got: $f_x = 2x \quad$   $f_{xx} = 2 \quad $ $ f_{y} = 2y \quad $ $ f_{yy}=2 \quad $ $ f_{xy} = 0 $ The critical values are when $ 2x = 2y $ which is the line x = y. the fancy formula: $ D = f_{xx}*f_{yy} - (f_{xy})^2 $ gives us  $D = 4$ always. So long as the point is a critical point, its a relative min. Since it is subject to the constraint, we need the values which satisfy   $x^2+ 2y^2 = 1 $. Those are ones where $ x=y$ so we look at $x^2+ 2x^2 = 1 \implies 3x^2= 1 \implies x = \pm1/\sqrt{3}$. Either works so when I plug it in to the original function, since $ x=y$, I get $ 2/3 $. Thus my absolute minimum. Is my reasoning correct? or am I missing tools or using the tools incorrectly from multivariable Calculus?","I tried this question on a site and found that none of the answers were the ones I got. I think my calculations were right but my understanding of applying the correct theorems may have been off. I found the derivatives with respect to x and y and got: $f_x = 2x \quad$   $f_{xx} = 2 \quad $ $ f_{y} = 2y \quad $ $ f_{yy}=2 \quad $ $ f_{xy} = 0 $ The critical values are when $ 2x = 2y $ which is the line x = y. the fancy formula: $ D = f_{xx}*f_{yy} - (f_{xy})^2 $ gives us  $D = 4$ always. So long as the point is a critical point, its a relative min. Since it is subject to the constraint, we need the values which satisfy   $x^2+ 2y^2 = 1 $. Those are ones where $ x=y$ so we look at $x^2+ 2x^2 = 1 \implies 3x^2= 1 \implies x = \pm1/\sqrt{3}$. Either works so when I plug it in to the original function, since $ x=y$, I get $ 2/3 $. Thus my absolute minimum. Is my reasoning correct? or am I missing tools or using the tools incorrectly from multivariable Calculus?",,"['multivariable-calculus', 'maxima-minima']"
90,How to change the limits of a double integral to polar coordinates limits?,How to change the limits of a double integral to polar coordinates limits?,,"Problem: Use polar coordinates to evaluate the following integral: $$\int_{0}^{2}\int_{0}^{\sqrt{2x-x^{2}}}xdydy$$ Solution: First, this is the graph I manually plotted to define the new limits: So I set up the new integral with these new limits in polar coordinates: $$1\leqslant r\leqslant 2$$ $$0\leqslant \theta\leqslant\pi$$ But the integral gives me $0$ as a result. This is the integral to evaluate: $$\int_{0}^{\pi}\int_{1}^{2}r\cos(\theta)rdrd\theta$$","Problem: Use polar coordinates to evaluate the following integral: $$\int_{0}^{2}\int_{0}^{\sqrt{2x-x^{2}}}xdydy$$ Solution: First, this is the graph I manually plotted to define the new limits: So I set up the new integral with these new limits in polar coordinates: $$1\leqslant r\leqslant 2$$ $$0\leqslant \theta\leqslant\pi$$ But the integral gives me $0$ as a result. This is the integral to evaluate: $$\int_{0}^{\pi}\int_{1}^{2}r\cos(\theta)rdrd\theta$$",,['multivariable-calculus']
91,"Why $f(x,y)=y^2$ instead of $f(y)=y^2$?",Why  instead of ?,"f(x,y)=y^2 f(y)=y^2","If I have a function $f(x,y)=y^2$, is it equivalent with $f(y)=y^2$? Are there any differences? Is $f(x,y)=y^2$ a multivariable function despite no $x$?","If I have a function $f(x,y)=y^2$, is it equivalent with $f(y)=y^2$? Are there any differences? Is $f(x,y)=y^2$ a multivariable function despite no $x$?",,"['calculus', 'functions', 'multivariable-calculus']"
92,Partial Derivatives Tend to Zero implies Limit to Infinity Exist?,Partial Derivatives Tend to Zero implies Limit to Infinity Exist?,,"Let $g:\mathbb{R}^2\to\mathbb{R}$ be a function such that $\frac{\partial g}{\partial x}$ and $\frac{\partial g}{\partial y}$ exist and are continuous on $\mathbb{R}^2$. Suppose that $$|\frac{\partial g}{\partial x}(x,y)|+|\frac{\partial g}{\partial y}(x,y)|\leq\frac{1}{x^2+y^2}$$ if $(x,y)\neq (0,0)$. Prove that $\lim_{(x,y)\to\infty}g(x,y)$ exists. I am not very sure how to go about proving this. I know that the first condition (partial derivatives exist and continuous) implies that $g$ is differentiable on $\mathbb{R}^2$. Then, I am thinking of using Taylor's Theorem somehow. One problem I face is I don't know what is the candidate limit of $\lim_{(x,y)\to\infty}g(x,y)$ which makes it hard to prove it using the Epsilon-Delta method. Another line of thinking could be the fundamental theorem of line integrals (as suggested by a commenter below). Again, I face problems coming up with the full proof. Thanks for any help! Updates (I am working on this question in the meantime): If we are proving using the $\epsilon-\delta$ definition, we need to show that there exists $c\in\mathbb{R}$ such that for all $\epsilon>0$ there exists $R>0$ such that whenever $\sqrt{x^2+y^2}>R$ we have $|g(x,y)-c|<\epsilon$. Finding a candidate for $c$ seems very hard to me, perhaps the way is to prove by contradiction. I am following AlexM.'s solution, which looks promising, except for some details in the inequality. I am thinking if that can be fixed using application of Mean Value Theorem for Definite Integrals.","Let $g:\mathbb{R}^2\to\mathbb{R}$ be a function such that $\frac{\partial g}{\partial x}$ and $\frac{\partial g}{\partial y}$ exist and are continuous on $\mathbb{R}^2$. Suppose that $$|\frac{\partial g}{\partial x}(x,y)|+|\frac{\partial g}{\partial y}(x,y)|\leq\frac{1}{x^2+y^2}$$ if $(x,y)\neq (0,0)$. Prove that $\lim_{(x,y)\to\infty}g(x,y)$ exists. I am not very sure how to go about proving this. I know that the first condition (partial derivatives exist and continuous) implies that $g$ is differentiable on $\mathbb{R}^2$. Then, I am thinking of using Taylor's Theorem somehow. One problem I face is I don't know what is the candidate limit of $\lim_{(x,y)\to\infty}g(x,y)$ which makes it hard to prove it using the Epsilon-Delta method. Another line of thinking could be the fundamental theorem of line integrals (as suggested by a commenter below). Again, I face problems coming up with the full proof. Thanks for any help! Updates (I am working on this question in the meantime): If we are proving using the $\epsilon-\delta$ definition, we need to show that there exists $c\in\mathbb{R}$ such that for all $\epsilon>0$ there exists $R>0$ such that whenever $\sqrt{x^2+y^2}>R$ we have $|g(x,y)-c|<\epsilon$. Finding a candidate for $c$ seems very hard to me, perhaps the way is to prove by contradiction. I am following AlexM.'s solution, which looks promising, except for some details in the inequality. I am thinking if that can be fixed using application of Mean Value Theorem for Definite Integrals.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
93,Calculus of Variations. Understand Epsilon's Need?,Calculus of Variations. Understand Epsilon's Need?,,"What purpose is Epsilon in Calculus of Variations? My question being what is the need for Epsilon in the equation  $y(x)+\epsilon n(x)$ when we assume this as the neighborhood curve of $y(x)$? When we try to find out the Euler Lagrange condition, we take $y(x)$ to be the curve and $y(x)+\epsilon n(x)$ to be the neighboring curve.","What purpose is Epsilon in Calculus of Variations? My question being what is the need for Epsilon in the equation  $y(x)+\epsilon n(x)$ when we assume this as the neighborhood curve of $y(x)$? When we try to find out the Euler Lagrange condition, we take $y(x)$ to be the curve and $y(x)+\epsilon n(x)$ to be the neighboring curve.",,['multivariable-calculus']
94,Why isn't the gradient vector of a parametric curve parallel to the tangent vector?,Why isn't the gradient vector of a parametric curve parallel to the tangent vector?,,"Consider a parametric curve defined by the equation: $$\mathbf{r}(t) = X(t)\mathbf{\hat{i}} + Y(t)\mathbf{\hat{j}} + Z(t)\mathbf{\hat{k}}$$ Paul's online math notes indicate that the unit tangent vector to such a curve is defined by: $$\mathbf{T}(t) = \frac{\mathbf{r}'(t)}{|\mathbf{r}'(t)|}$$ More importantly, $\mathbf{r}'(t)$ is a tangent vector. Now let's consider a scenario wherein a thin, insulated wire follows the path $\mathbf{r}(t)$ between two arbitrary end points with some boundary conditions applied. The wire will attain some temperature distribution $T=T(x,y,z)=T(t)$. My intuition tells me (and I hope everyone will agree) that the temperature gradient at any point along the wire should be a vector tangent to the wire, i.e.: $$\mathbf{\nabla}T \propto \mathbf{r}'(t)$$ Now I want to confirm my intuition. I'll start with the definition of the gradient vector in Cartesian co-oridinates: $$ \mathbf{\nabla} = \frac{\partial}{\partial x}\mathbf{\hat{i}} + \frac{\partial}{\partial y}\mathbf{\hat{j}} + \frac{\partial}{\partial z}\mathbf{\hat{k}} $$ Using the chain rule I can re-write each of the derivative operators in terms of $t$: \begin{align} \mathbf{\nabla} &= \frac{\partial t}{\partial x}\frac{\partial}{\partial t}\mathbf{\hat{i}} + \frac{\partial t}{\partial y}\frac{\partial}{\partial t}\mathbf{\hat{j}} + \frac{\partial t}{\partial z}\frac{\partial}{\partial t}\mathbf{\hat{k}} \\ \mathbf{\nabla} &= \left(\frac{\partial t}{\partial x}\mathbf{\hat{i}} + \frac{\partial t}{\partial y}\mathbf{\hat{j}} + \frac{\partial t}{\partial z}\mathbf{\hat{k}}\right) \frac{\partial}{\partial t} \\ \mathbf{\nabla} &= \left(\frac{1}{X'(t)}\mathbf{\hat{i}} + \frac{1}{Y'(t)}\mathbf{\hat{j}} + \frac{1}{Z'(t)}\mathbf{\hat{k}}\right) \frac{\partial}{\partial t} \end{align} The vector in the above equation should be parallel to $\mathbf{r}'(t)$. I can test this by checking the cross product of these two vectors (should be $\mathbf{0}$ if they are parallel): \begin{align} \left(\frac{1}{X'(t)}\mathbf{\hat{i}} + \frac{1}{Y'(t)}\mathbf{\hat{j}} + \frac{1}{Z'(t)}\mathbf{\hat{k}}\right) \times \left( X(t)\mathbf{\hat{i}} + Y(t)\mathbf{\hat{j}} + Z(t)\mathbf{\hat{k}} \right) \\ = \left( \frac{Z'}{Y'} - \frac{Y'}{Z'} \right)\mathbf{\hat{i}} +  \left( \frac{X'}{Z'} - \frac{Z'}{X'} \right)\mathbf{\hat{j}} + \left( \frac{Y'}{X'} - \frac{X'}{Y'} \right)\mathbf{\hat{k}} \end{align} So it appears these vectors are parallel only when $X'^2 = Y'^2 = Z'^2$, which seems like a really unlikely restriction. What's wrong here: my intuition, my math, or both? Also please keep in mind what I'm really interested in is the correct transformation of the gradient operator. This question is just the most concise way to explain what I'm struggling with. I realize that similar questions on this topic have been asked previously: characteristic curves tangent and gradient Gradient vector of parametric curve This may seem like a duplicate of one of those, but from what I understand about those questions neither of them address the case where the shape of the domain is confined to a parametric curve , which is the focus here.","Consider a parametric curve defined by the equation: $$\mathbf{r}(t) = X(t)\mathbf{\hat{i}} + Y(t)\mathbf{\hat{j}} + Z(t)\mathbf{\hat{k}}$$ Paul's online math notes indicate that the unit tangent vector to such a curve is defined by: $$\mathbf{T}(t) = \frac{\mathbf{r}'(t)}{|\mathbf{r}'(t)|}$$ More importantly, $\mathbf{r}'(t)$ is a tangent vector. Now let's consider a scenario wherein a thin, insulated wire follows the path $\mathbf{r}(t)$ between two arbitrary end points with some boundary conditions applied. The wire will attain some temperature distribution $T=T(x,y,z)=T(t)$. My intuition tells me (and I hope everyone will agree) that the temperature gradient at any point along the wire should be a vector tangent to the wire, i.e.: $$\mathbf{\nabla}T \propto \mathbf{r}'(t)$$ Now I want to confirm my intuition. I'll start with the definition of the gradient vector in Cartesian co-oridinates: $$ \mathbf{\nabla} = \frac{\partial}{\partial x}\mathbf{\hat{i}} + \frac{\partial}{\partial y}\mathbf{\hat{j}} + \frac{\partial}{\partial z}\mathbf{\hat{k}} $$ Using the chain rule I can re-write each of the derivative operators in terms of $t$: \begin{align} \mathbf{\nabla} &= \frac{\partial t}{\partial x}\frac{\partial}{\partial t}\mathbf{\hat{i}} + \frac{\partial t}{\partial y}\frac{\partial}{\partial t}\mathbf{\hat{j}} + \frac{\partial t}{\partial z}\frac{\partial}{\partial t}\mathbf{\hat{k}} \\ \mathbf{\nabla} &= \left(\frac{\partial t}{\partial x}\mathbf{\hat{i}} + \frac{\partial t}{\partial y}\mathbf{\hat{j}} + \frac{\partial t}{\partial z}\mathbf{\hat{k}}\right) \frac{\partial}{\partial t} \\ \mathbf{\nabla} &= \left(\frac{1}{X'(t)}\mathbf{\hat{i}} + \frac{1}{Y'(t)}\mathbf{\hat{j}} + \frac{1}{Z'(t)}\mathbf{\hat{k}}\right) \frac{\partial}{\partial t} \end{align} The vector in the above equation should be parallel to $\mathbf{r}'(t)$. I can test this by checking the cross product of these two vectors (should be $\mathbf{0}$ if they are parallel): \begin{align} \left(\frac{1}{X'(t)}\mathbf{\hat{i}} + \frac{1}{Y'(t)}\mathbf{\hat{j}} + \frac{1}{Z'(t)}\mathbf{\hat{k}}\right) \times \left( X(t)\mathbf{\hat{i}} + Y(t)\mathbf{\hat{j}} + Z(t)\mathbf{\hat{k}} \right) \\ = \left( \frac{Z'}{Y'} - \frac{Y'}{Z'} \right)\mathbf{\hat{i}} +  \left( \frac{X'}{Z'} - \frac{Z'}{X'} \right)\mathbf{\hat{j}} + \left( \frac{Y'}{X'} - \frac{X'}{Y'} \right)\mathbf{\hat{k}} \end{align} So it appears these vectors are parallel only when $X'^2 = Y'^2 = Z'^2$, which seems like a really unlikely restriction. What's wrong here: my intuition, my math, or both? Also please keep in mind what I'm really interested in is the correct transformation of the gradient operator. This question is just the most concise way to explain what I'm struggling with. I realize that similar questions on this topic have been asked previously: characteristic curves tangent and gradient Gradient vector of parametric curve This may seem like a duplicate of one of those, but from what I understand about those questions neither of them address the case where the shape of the domain is confined to a parametric curve , which is the focus here.",,"['multivariable-calculus', 'vector-analysis', 'parametric']"
95,"Find the maximum of $U (x,y) = x^\alpha y^\beta$ subject to $I = px + qy$",Find the maximum of  subject to,"U (x,y) = x^\alpha y^\beta I = px + qy","Let be $U (x,y) = x^\alpha y^\beta$. Find the maximum of the function $U(x,y)$ subject to the equality constraint $I = px + qy$. I have tried to use the Lagrangian function to find the solution for the problem, with the equation $$\nabla\mathscr{L}=\vec{0}$$ where $\mathscr{L}$ is the Lagrangian function and $\vec{0}=\pmatrix{0,0}$. Using this method I have a system of $3$ equations with $3$ variables, but I can't simplify this system: $$ax^{\alpha-1}y^\beta-p\lambda=0$$ $$\beta y^{\beta-1}x^\alpha-q\lambda=0$$ $$I=px+qx$$","Let be $U (x,y) = x^\alpha y^\beta$. Find the maximum of the function $U(x,y)$ subject to the equality constraint $I = px + qy$. I have tried to use the Lagrangian function to find the solution for the problem, with the equation $$\nabla\mathscr{L}=\vec{0}$$ where $\mathscr{L}$ is the Lagrangian function and $\vec{0}=\pmatrix{0,0}$. Using this method I have a system of $3$ equations with $3$ variables, but I can't simplify this system: $$ax^{\alpha-1}y^\beta-p\lambda=0$$ $$\beta y^{\beta-1}x^\alpha-q\lambda=0$$ $$I=px+qx$$",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'economics']"
96,$\iint_{\mathbb R^2}\sqrt{\frac{x^2}{a^2}+\frac{x^2}{b^2}}e^{-\frac{x^2}{a^2}+\frac{y^2}{b^2}}dxdy$,,\iint_{\mathbb R^2}\sqrt{\frac{x^2}{a^2}+\frac{x^2}{b^2}}e^{-\frac{x^2}{a^2}+\frac{y^2}{b^2}}dxdy,"$$\iint_{\mathbb R^2}\sqrt{\frac{x^2}{a^2}+\frac{y^2}{b^2}}\,e^{-\left(\frac{x^2}{a^2}+\frac{y^2}{b^2}\right)}\,dx\,dy$$ Basically I have done problems similar to this, using the theorem that if $f>0$ then if $$\bigcup_{n=1}^{\infty} D_n=D$$ then $$\exists\int_{D}f(x)dx \Leftrightarrow \exists \lim_{n\to \infty}\int_{D_n}f(x)dx$$ So basically if I had the integral $$\iiint_{x^2+y^2+z^2\geq1}\frac{e^{-x^2-y^2-z^2}}{\sqrt{x^2+y^2+z^2}}$$ I would use the spherical coordinates substitution and have in this particular case $D_n=\{(x,y,z)|1<x^2+y^2+z^2<n^2\}$ and the radius in the mapping $(r,\phi,\theta)\mapsto (r\cos\varphi \sin\theta, r\sin \varphi \sin \theta,r\cos\theta)$ the ranges of the angles are clear as day, and the range of $r$ is from $1$ to $n$. Here in the main problem that I give, it seems like a could Idea to have $D_n$ be rising ellipses  and the substitution be $x=a\cos\phi, y=b\sin \phi$. The problem I have is setting up how this would actually look, could $D_{a,b}=\{(x,y)|\frac{x^2}{a^2}+\frac{y^2}{b^2}\leq 1\}$? I would appreciate if someone could bring this to a finished point, because I am not sure how to solve this one.","$$\iint_{\mathbb R^2}\sqrt{\frac{x^2}{a^2}+\frac{y^2}{b^2}}\,e^{-\left(\frac{x^2}{a^2}+\frac{y^2}{b^2}\right)}\,dx\,dy$$ Basically I have done problems similar to this, using the theorem that if $f>0$ then if $$\bigcup_{n=1}^{\infty} D_n=D$$ then $$\exists\int_{D}f(x)dx \Leftrightarrow \exists \lim_{n\to \infty}\int_{D_n}f(x)dx$$ So basically if I had the integral $$\iiint_{x^2+y^2+z^2\geq1}\frac{e^{-x^2-y^2-z^2}}{\sqrt{x^2+y^2+z^2}}$$ I would use the spherical coordinates substitution and have in this particular case $D_n=\{(x,y,z)|1<x^2+y^2+z^2<n^2\}$ and the radius in the mapping $(r,\phi,\theta)\mapsto (r\cos\varphi \sin\theta, r\sin \varphi \sin \theta,r\cos\theta)$ the ranges of the angles are clear as day, and the range of $r$ is from $1$ to $n$. Here in the main problem that I give, it seems like a could Idea to have $D_n$ be rising ellipses  and the substitution be $x=a\cos\phi, y=b\sin \phi$. The problem I have is setting up how this would actually look, could $D_{a,b}=\{(x,y)|\frac{x^2}{a^2}+\frac{y^2}{b^2}\leq 1\}$? I would appreciate if someone could bring this to a finished point, because I am not sure how to solve this one.",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
97,Estimating the curvature of a discretized curve in 3d with cubic splines,Estimating the curvature of a discretized curve in 3d with cubic splines,,"I have a computer simulation in which I'm modeling a physical curve by discretizing it and updating the locations of these points. I want to find/estimate the location of the maximum curvature of the curve and it seems like a way to approach this is to try and calculate which discretized point has maximum curvature. Note, I never have an actual curve, just the discretized points that are suppose to be representing the curve. So far, the approach I have been thinking about is to estimate the curvature by interpolationg the points with cubic splines and then finding the curvature of the spline at each point. If you think there is a better approach, please let me know, but my questions below will be assuming this approach. My first question is in setting up the cubic splines. Previously, I have only done it for scalar functions of one variable, but the points in question lie on a 3d space curve. The approach I would be inclined to take is to treat the curve parametrically as $\langle x(t), y(t), z(t) \rangle$ and then apply the 1d cubic spline algorithm I know to each component function. Finally, I would apply the curvature formula $\frac{\sqrt{(z''y'-y''z')^2 + (x''z' - z''x')^2 + (y''x'-x''y')^2}}{(x'^2+y'^2+z'^2)^{3/2}}$ from wikipedia ( https://en.wikipedia.org/wiki/Curvature#Curvature_of_space_curves ) at each of the discretized point and find the maximum. Does this seem like a valid approach? Do you think there is an approach that would be better?","I have a computer simulation in which I'm modeling a physical curve by discretizing it and updating the locations of these points. I want to find/estimate the location of the maximum curvature of the curve and it seems like a way to approach this is to try and calculate which discretized point has maximum curvature. Note, I never have an actual curve, just the discretized points that are suppose to be representing the curve. So far, the approach I have been thinking about is to estimate the curvature by interpolationg the points with cubic splines and then finding the curvature of the spline at each point. If you think there is a better approach, please let me know, but my questions below will be assuming this approach. My first question is in setting up the cubic splines. Previously, I have only done it for scalar functions of one variable, but the points in question lie on a 3d space curve. The approach I would be inclined to take is to treat the curve parametrically as $\langle x(t), y(t), z(t) \rangle$ and then apply the 1d cubic spline algorithm I know to each component function. Finally, I would apply the curvature formula $\frac{\sqrt{(z''y'-y''z')^2 + (x''z' - z''x')^2 + (y''x'-x''y')^2}}{(x'^2+y'^2+z'^2)^{3/2}}$ from wikipedia ( https://en.wikipedia.org/wiki/Curvature#Curvature_of_space_curves ) at each of the discretized point and find the maximum. Does this seem like a valid approach? Do you think there is an approach that would be better?",,"['multivariable-calculus', 'spline']"
98,Relation between chain rule and implicit differentiation derivation in multi variable calculus,Relation between chain rule and implicit differentiation derivation in multi variable calculus,,"So my question is on the derivation of the implicit differentiation (taken from here ). The general chain rule, from here , it says that if we have a function $z$ of $n$ variables, $x_1, x_2,\ldots,x_n$ and each of these variables are in turn a function of $m$ variables, $t_1, t_2,\ldots, t_m$.  Then for any $t_i, i=1, 2, \ldots, m$ we have (1)  $$ \frac{\partial z}{\partial t_i}=\frac{\partial z}{\partial x_1}\frac{\partial x_1}{\partial t_i}+\frac{\partial z}{\partial x_2}\frac{\partial x_2}{\partial t_i}+\cdots+\frac{\partial z}{\partial x_n}\frac{\partial x_n}{\partial t_i}$$ My question is how does did the differentiation of $F$ with respect to $x$ come up as it did.  How did it come up as (2) $$\frac{\partial F}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial F}{\partial y}\frac{\partial y}{\partial x}+\cdots+\frac{\partial F}{\partial z}\frac{\partial z}{\partial x}=0$$ If I match (2) with (1), then it seems that the left hand side is $\frac{\partial F}{\partial x}$ because $F=z, x=x_1, y=x_2, z=x_3$.  As far as the initial condition of $F$ having three variables $(x, y, z)$ that in turn are each supposed to be functions of more variables, can it be assumed that $x$ and $y$ are constants; this would be mean effectively that $x=g(t_1, t_2, t_3)=x$ of some function g and $y=y(t_1, t_2, t_3)=y$.  So is the left hand side of (2) $\frac{\partial F}{\partial x}$ or some different notation.  What does the differentiation of $F$ with respect to $x$ on the left hand side look like in symbols then?","So my question is on the derivation of the implicit differentiation (taken from here ). The general chain rule, from here , it says that if we have a function $z$ of $n$ variables, $x_1, x_2,\ldots,x_n$ and each of these variables are in turn a function of $m$ variables, $t_1, t_2,\ldots, t_m$.  Then for any $t_i, i=1, 2, \ldots, m$ we have (1)  $$ \frac{\partial z}{\partial t_i}=\frac{\partial z}{\partial x_1}\frac{\partial x_1}{\partial t_i}+\frac{\partial z}{\partial x_2}\frac{\partial x_2}{\partial t_i}+\cdots+\frac{\partial z}{\partial x_n}\frac{\partial x_n}{\partial t_i}$$ My question is how does did the differentiation of $F$ with respect to $x$ come up as it did.  How did it come up as (2) $$\frac{\partial F}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial F}{\partial y}\frac{\partial y}{\partial x}+\cdots+\frac{\partial F}{\partial z}\frac{\partial z}{\partial x}=0$$ If I match (2) with (1), then it seems that the left hand side is $\frac{\partial F}{\partial x}$ because $F=z, x=x_1, y=x_2, z=x_3$.  As far as the initial condition of $F$ having three variables $(x, y, z)$ that in turn are each supposed to be functions of more variables, can it be assumed that $x$ and $y$ are constants; this would be mean effectively that $x=g(t_1, t_2, t_3)=x$ of some function g and $y=y(t_1, t_2, t_3)=y$.  So is the left hand side of (2) $\frac{\partial F}{\partial x}$ or some different notation.  What does the differentiation of $F$ with respect to $x$ on the left hand side look like in symbols then?",,"['multivariable-calculus', 'proof-explanation', 'implicit-differentiation']"
99,Conditional Extremes/Lagrange multipliers: proving: $\frac{1}{x_1}+...+\frac{1}{x_n} \geq \frac{n^2}{x_1+...+x_n}$,Conditional Extremes/Lagrange multipliers: proving:,\frac{1}{x_1}+...+\frac{1}{x_n} \geq \frac{n^2}{x_1+...+x_n},"$$\frac{1}{x_1}+...+\frac{1}{x_n} \geq \frac{n^2}{x_1+...+x_n};x_i>0.$$ This is supposed to be proven using conditional extremes. I tried the main function being $f(x_1,...,x_n)=\frac{1}{x_1}+...+\frac{1}{x_n} $and the condition:$x_1+...+x_n=s$ but when setting up the Lagrange equation I cannot get $x_i$ to be dependant upon $s$ and $n$ for reasons which are not clear. Help is appreciated.","$$\frac{1}{x_1}+...+\frac{1}{x_n} \geq \frac{n^2}{x_1+...+x_n};x_i>0.$$ This is supposed to be proven using conditional extremes. I tried the main function being $f(x_1,...,x_n)=\frac{1}{x_1}+...+\frac{1}{x_n} $and the condition:$x_1+...+x_n=s$ but when setting up the Lagrange equation I cannot get $x_i$ to be dependant upon $s$ and $n$ for reasons which are not clear. Help is appreciated.",,"['calculus', 'analysis', 'multivariable-calculus', 'lagrange-multiplier']"
